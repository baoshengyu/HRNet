/home/baoshengyu/anaconda3/lib/python3.7/site-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 4
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 6
Added key: store_based_barrier_key:1 to store for rank: 7
Added key: store_based_barrier_key:1 to store for rank: 5
Added key: store_based_barrier_key:1 to store for rank: 3
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 8.
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 4, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 7, total 8.
Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 5, total 8.
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 8.
Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 8.
Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 6, total 8.
HRNet(
  (conv0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
  )
  (conv1): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): Bottleneck(
              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (downsample): Sequential(
                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
          )
        )
      )
    )
  )
  (pool1): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(256, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Conv2d(256, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv2): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
          )
        )
      )
    )
  )
  (pool2): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Identity()
      (1): Identity()
      (2): Sequential(
        (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv3): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (1): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (2): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (3): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
  )
  (pool3): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Identity()
      (1): Identity()
      (2): Identity()
      (3): Sequential(
        (0): Conv2d(72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv4): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(18, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(36, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
    (1): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(18, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(36, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
    (2): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(18, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(36, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
  )
  (classifier): CLSHead(
    (smooth_layers): ModuleList(
      (0): Bottleneck(
        (conv1): Conv2d(18, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(18, 128, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(36, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(36, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): Bottleneck(
        (conv1): Conv2d(72, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(72, 512, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): Bottleneck(
        (conv1): Conv2d(144, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(144, 1024, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (downsample_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (final_layer): Sequential(
      (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): AdaptiveAvgPool2d(output_size=1)
      (4): Conv2d(2048, 1000, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
HRNet(
  (conv0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
  )
  (conv1): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): Bottleneck(
              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (downsample): Sequential(
                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
          )
        )
      )
    )
  )
  (pool1): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(256, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Conv2d(256, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv2): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
          )
        )
      )
    )
  )
  (pool2): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Identity()
      (1): Identity()
      (2): Sequential(
        (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv3): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (1): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (2): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (3): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
  )
  (pool3): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Identity()
      (1): Identity()
      (2): Identity()
      (3): Sequential(
        (0): Conv2d(72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv4): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(18, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(36, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
    (1): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(18, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(36, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
    (2): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(18, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(36, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
  )
  (classifier): CLSHead(
    (smooth_layers): ModuleList(
      (0): Bottleneck(
        (conv1): Conv2d(18, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(18, 128, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(36, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(36, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): Bottleneck(
        (conv1): Conv2d(72, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(72, 512, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): Bottleneck(
        (conv1): Conv2d(144, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(144, 1024, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (downsample_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (final_layer): Sequential(
      (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): AdaptiveAvgPool2d(output_size=1)
      (4): Conv2d(2048, 1000, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
HRNet(
  (conv0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
  )
  (conv1): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): Bottleneck(
              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (downsample): Sequential(
                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
          )
        )
      )
    )
  )
  (pool1): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(256, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Conv2d(256, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv2): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
          )
        )
      )
    )
  )
  (pool2): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Identity()
      (1): Identity()
      (2): Sequential(
        (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv3): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (1): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (2): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (3): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
  )
  (pool3): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Identity()
      (1): Identity()
      (2): Identity()
      (3): Sequential(
        (0): Conv2d(72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv4): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(18, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(36, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
    (1): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(18, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(36, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
    (2): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(18, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(36, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
  )
  (classifier): CLSHead(
    (smooth_layers): ModuleList(
      (0): Bottleneck(
        (conv1): Conv2d(18, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(18, 128, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(36, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(36, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): Bottleneck(
        (conv1): Conv2d(72, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(72, 512, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): Bottleneck(
        (conv1): Conv2d(144, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(144, 1024, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (downsample_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (final_layer): Sequential(
      (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): AdaptiveAvgPool2d(output_size=1)
      (4): Conv2d(2048, 1000, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
HRNet(
  (conv0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
  )
  (conv1): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): Bottleneck(
              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (downsample): Sequential(
                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
          )
        )
      )
    )
  )
  (pool1): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(256, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Conv2d(256, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv2): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
          )
        )
      )
    )
  )
  (pool2): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Identity()
      (1): Identity()
      (2): Sequential(
        (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv3): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (1): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (2): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (3): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
  )
  (pool3): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Identity()
      (1): Identity()
      (2): Identity()
      (3): Sequential(
        (0): Conv2d(72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv4): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(18, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(36, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
    (1): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(18, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(36, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
    (2): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(18, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(36, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
  )
  (classifier): CLSHead(
    (smooth_layers): ModuleList(
      (0): Bottleneck(
        (conv1): Conv2d(18, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(18, 128, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(36, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(36, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): Bottleneck(
        (conv1): Conv2d(72, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(72, 512, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): Bottleneck(
        (conv1): Conv2d(144, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(144, 1024, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (downsample_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (final_layer): Sequential(
      (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): AdaptiveAvgPool2d(output_size=1)
      (4): Conv2d(2048, 1000, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
HRNet(
  (conv0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
  )
  (conv1): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): Bottleneck(
              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (downsample): Sequential(
                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
          )
        )
      )
    )
  )
  (pool1): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(256, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Conv2d(256, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv2): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
          )
        )
      )
    )
  )
  (pool2): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Identity()
      (1): Identity()
      (2): Sequential(
        (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv3): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (1): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (2): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (3): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
  )
  (pool3): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Identity()
      (1): Identity()
      (2): Identity()
      (3): Sequential(
        (0): Conv2d(72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv4): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(18, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(36, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
    (1): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(18, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(36, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
    (2): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(18, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(36, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
  )
  (classifier): CLSHead(
    (smooth_layers): ModuleList(
      (0): Bottleneck(
        (conv1): Conv2d(18, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(18, 128, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(36, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(36, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): Bottleneck(
        (conv1): Conv2d(72, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(72, 512, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): Bottleneck(
        (conv1): Conv2d(144, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(144, 1024, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (downsample_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (final_layer): Sequential(
      (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): AdaptiveAvgPool2d(output_size=1)
      (4): Conv2d(2048, 1000, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
HRNet(
  (conv0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
  )
  (conv1): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): Bottleneck(
              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (downsample): Sequential(
                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
          )
        )
      )
    )
  )
  (pool1): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(256, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Conv2d(256, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv2): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
          )
        )
      )
    )
  )
  (pool2): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Identity()
      (1): Identity()
      (2): Sequential(
        (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv3): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (1): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (2): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (3): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
  )
  (pool3): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Identity()
      (1): Identity()
      (2): Identity()
      (3): Sequential(
        (0): Conv2d(72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv4): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(18, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(36, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
    (1): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(18, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(36, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
    (2): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(18, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(36, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
  )
  (classifier): CLSHead(
    (smooth_layers): ModuleList(
      (0): Bottleneck(
        (conv1): Conv2d(18, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(18, 128, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(36, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(36, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): Bottleneck(
        (conv1): Conv2d(72, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(72, 512, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): Bottleneck(
        (conv1): Conv2d(144, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(144, 1024, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (downsample_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (final_layer): Sequential(
      (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): AdaptiveAvgPool2d(output_size=1)
      (4): Conv2d(2048, 1000, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
HRNet(
  (conv0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
  )
  (conv1): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): Bottleneck(
              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (downsample): Sequential(
                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
          )
        )
      )
    )
  )
  (pool1): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(256, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Conv2d(256, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv2): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
          )
        )
      )
    )
  )
  (pool2): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Identity()
      (1): Identity()
      (2): Sequential(
        (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv3): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (1): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (2): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (3): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
  )
  (pool3): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Identity()
      (1): Identity()
      (2): Identity()
      (3): Sequential(
        (0): Conv2d(72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv4): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(18, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(36, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
    (1): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(18, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(36, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
    (2): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(18, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(36, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
  )
  (classifier): CLSHead(
    (smooth_layers): ModuleList(
      (0): Bottleneck(
        (conv1): Conv2d(18, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(18, 128, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(36, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(36, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): Bottleneck(
        (conv1): Conv2d(72, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(72, 512, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): Bottleneck(
        (conv1): Conv2d(144, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(144, 1024, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (downsample_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (final_layer): Sequential(
      (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): AdaptiveAvgPool2d(output_size=1)
      (4): Conv2d(2048, 1000, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
HRNet(
  (conv0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
  )
  (conv1): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): Bottleneck(
              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (downsample): Sequential(
                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
          )
        )
      )
    )
  )
  (pool1): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(256, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Conv2d(256, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv2): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
          )
        )
      )
    )
  )
  (pool2): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Identity()
      (1): Identity()
      (2): Sequential(
        (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv3): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (1): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (2): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (3): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
  )
  (pool3): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Identity()
      (1): Identity()
      (2): Identity()
      (3): Sequential(
        (0): Conv2d(72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv4): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(18, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(36, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
    (1): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(18, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(36, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
    (2): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 18, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(144, 36, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(18, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(36, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(36, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
  )
  (classifier): CLSHead(
    (smooth_layers): ModuleList(
      (0): Bottleneck(
        (conv1): Conv2d(18, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(18, 128, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(36, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(36, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): Bottleneck(
        (conv1): Conv2d(72, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(72, 512, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): Bottleneck(
        (conv1): Conv2d(144, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(144, 1024, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (downsample_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (final_layer): Sequential(
      (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): AdaptiveAvgPool2d(output_size=1)
      (4): Conv2d(2048, 1000, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
Model hrnet18 created, param count:21301174
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.875
Using native Torch AMP. Training in mixed precision.
Using native Torch DistributedDataParallel.
Scheduled epochs: 610
Train: 0 [   0/1251 (  0%)]  Loss: 6.949 (6.95)  Time: 9.941s,  103.01/s  (9.941s,  103.01/s)  LR: 1.000e-06  Data: 1.820 (1.820)
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Train: 0 [  50/1251 (  4%)]  Loss: 6.944 (6.95)  Time: 0.692s, 1479.62/s  (0.917s, 1117.26/s)  LR: 1.000e-06  Data: 0.010 (0.047)
Train: 0 [ 100/1251 (  8%)]  Loss: 6.921 (6.94)  Time: 0.678s, 1509.37/s  (0.807s, 1269.04/s)  LR: 1.000e-06  Data: 0.010 (0.029)
Train: 0 [ 150/1251 ( 12%)]  Loss: 6.929 (6.94)  Time: 0.680s, 1505.20/s  (0.771s, 1327.69/s)  LR: 1.000e-06  Data: 0.009 (0.023)
Train: 0 [ 200/1251 ( 16%)]  Loss: 6.924 (6.93)  Time: 0.674s, 1519.78/s  (0.750s, 1364.77/s)  LR: 1.000e-06  Data: 0.009 (0.020)
Train: 0 [ 250/1251 ( 20%)]  Loss: 6.928 (6.93)  Time: 0.673s, 1520.84/s  (0.739s, 1386.47/s)  LR: 1.000e-06  Data: 0.010 (0.018)
Train: 0 [ 300/1251 ( 24%)]  Loss: 6.938 (6.93)  Time: 0.739s, 1385.35/s  (0.731s, 1401.48/s)  LR: 1.000e-06  Data: 0.009 (0.016)
Train: 0 [ 350/1251 ( 28%)]  Loss: 6.923 (6.93)  Time: 0.675s, 1517.74/s  (0.724s, 1413.50/s)  LR: 1.000e-06  Data: 0.009 (0.015)
Train: 0 [ 400/1251 ( 32%)]  Loss: 6.917 (6.93)  Time: 0.686s, 1491.63/s  (0.721s, 1420.15/s)  LR: 1.000e-06  Data: 0.015 (0.015)
Train: 0 [ 450/1251 ( 36%)]  Loss: 6.924 (6.93)  Time: 0.683s, 1499.30/s  (0.718s, 1425.84/s)  LR: 1.000e-06  Data: 0.009 (0.014)
Train: 0 [ 500/1251 ( 40%)]  Loss: 6.934 (6.93)  Time: 0.675s, 1516.22/s  (0.715s, 1431.98/s)  LR: 1.000e-06  Data: 0.010 (0.014)
Train: 0 [ 550/1251 ( 44%)]  Loss: 6.915 (6.93)  Time: 0.680s, 1505.36/s  (0.713s, 1435.99/s)  LR: 1.000e-06  Data: 0.009 (0.013)
Train: 0 [ 600/1251 ( 48%)]  Loss: 6.915 (6.93)  Time: 0.688s, 1488.21/s  (0.711s, 1439.71/s)  LR: 1.000e-06  Data: 0.009 (0.013)
Train: 0 [ 650/1251 ( 52%)]  Loss: 6.927 (6.93)  Time: 0.729s, 1404.23/s  (0.710s, 1442.65/s)  LR: 1.000e-06  Data: 0.010 (0.013)
Train: 0 [ 700/1251 ( 56%)]  Loss: 6.923 (6.93)  Time: 0.697s, 1469.80/s  (0.709s, 1445.06/s)  LR: 1.000e-06  Data: 0.010 (0.013)
Train: 0 [ 750/1251 ( 60%)]  Loss: 6.914 (6.93)  Time: 0.684s, 1496.34/s  (0.707s, 1447.60/s)  LR: 1.000e-06  Data: 0.009 (0.012)
Train: 0 [ 800/1251 ( 64%)]  Loss: 6.915 (6.93)  Time: 0.672s, 1524.06/s  (0.706s, 1449.68/s)  LR: 1.000e-06  Data: 0.010 (0.012)
Train: 0 [ 850/1251 ( 68%)]  Loss: 6.912 (6.92)  Time: 0.673s, 1522.12/s  (0.705s, 1451.70/s)  LR: 1.000e-06  Data: 0.010 (0.012)
Train: 0 [ 900/1251 ( 72%)]  Loss: 6.915 (6.92)  Time: 0.680s, 1505.61/s  (0.705s, 1453.45/s)  LR: 1.000e-06  Data: 0.010 (0.012)
Train: 0 [ 950/1251 ( 76%)]  Loss: 6.918 (6.92)  Time: 0.706s, 1450.15/s  (0.704s, 1454.71/s)  LR: 1.000e-06  Data: 0.010 (0.012)
Train: 0 [1000/1251 ( 80%)]  Loss: 6.907 (6.92)  Time: 0.712s, 1437.40/s  (0.704s, 1455.12/s)  LR: 1.000e-06  Data: 0.011 (0.012)
Train: 0 [1050/1251 ( 84%)]  Loss: 6.911 (6.92)  Time: 0.676s, 1513.88/s  (0.703s, 1455.98/s)  LR: 1.000e-06  Data: 0.010 (0.012)
Train: 0 [1100/1251 ( 88%)]  Loss: 6.913 (6.92)  Time: 0.703s, 1456.47/s  (0.703s, 1456.98/s)  LR: 1.000e-06  Data: 0.009 (0.012)
Train: 0 [1150/1251 ( 92%)]  Loss: 6.916 (6.92)  Time: 0.674s, 1519.34/s  (0.702s, 1458.06/s)  LR: 1.000e-06  Data: 0.010 (0.012)
Train: 0 [1200/1251 ( 96%)]  Loss: 6.921 (6.92)  Time: 0.704s, 1454.00/s  (0.702s, 1458.95/s)  LR: 1.000e-06  Data: 0.011 (0.012)
Train: 0 [1250/1251 (100%)]  Loss: 6.900 (6.92)  Time: 0.658s, 1555.64/s  (0.702s, 1459.61/s)  LR: 1.000e-06  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.964 (1.964)  Loss:  6.8086 (6.8086)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  1.6602 ( 1.6602)
Test: [  48/48]  Time: 2.094 (0.631)  Loss:  6.8281 (6.9027)  Acc@1:  1.1792 ( 0.1860)  Acc@5:  2.5943 ( 0.6960)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-0.pth.tar', 0.18600000118255616)

Train: 1 [   0/1251 (  0%)]  Loss: 6.910 (6.91)  Time: 2.503s,  409.11/s  (2.503s,  409.11/s)  LR: 2.008e-04  Data: 1.603 (1.603)
Train: 1 [  50/1251 (  4%)]  Loss: 6.907 (6.91)  Time: 0.705s, 1451.51/s  (0.730s, 1402.70/s)  LR: 2.008e-04  Data: 0.009 (0.043)
Train: 1 [ 100/1251 (  8%)]  Loss: 6.913 (6.91)  Time: 0.707s, 1448.41/s  (0.711s, 1440.89/s)  LR: 2.008e-04  Data: 0.009 (0.027)
Train: 1 [ 150/1251 ( 12%)]  Loss: 6.872 (6.90)  Time: 0.674s, 1520.07/s  (0.705s, 1451.58/s)  LR: 2.008e-04  Data: 0.010 (0.021)
Train: 1 [ 200/1251 ( 16%)]  Loss: 6.880 (6.90)  Time: 0.671s, 1526.99/s  (0.703s, 1455.94/s)  LR: 2.008e-04  Data: 0.010 (0.018)
Train: 1 [ 250/1251 ( 20%)]  Loss: 6.853 (6.89)  Time: 0.709s, 1445.24/s  (0.701s, 1461.38/s)  LR: 2.008e-04  Data: 0.009 (0.017)
Train: 1 [ 300/1251 ( 24%)]  Loss: 6.840 (6.88)  Time: 0.720s, 1423.02/s  (0.699s, 1464.62/s)  LR: 2.008e-04  Data: 0.010 (0.015)
Train: 1 [ 350/1251 ( 28%)]  Loss: 6.766 (6.87)  Time: 0.708s, 1445.36/s  (0.700s, 1463.51/s)  LR: 2.008e-04  Data: 0.009 (0.015)
Train: 1 [ 400/1251 ( 32%)]  Loss: 6.737 (6.85)  Time: 0.695s, 1473.44/s  (0.699s, 1465.88/s)  LR: 2.008e-04  Data: 0.009 (0.014)
Train: 1 [ 450/1251 ( 36%)]  Loss: 6.696 (6.84)  Time: 0.672s, 1524.24/s  (0.698s, 1466.03/s)  LR: 2.008e-04  Data: 0.011 (0.014)
Train: 1 [ 500/1251 ( 40%)]  Loss: 6.758 (6.83)  Time: 0.672s, 1523.46/s  (0.698s, 1466.62/s)  LR: 2.008e-04  Data: 0.010 (0.013)
Train: 1 [ 550/1251 ( 44%)]  Loss: 6.740 (6.82)  Time: 0.684s, 1496.66/s  (0.698s, 1468.04/s)  LR: 2.008e-04  Data: 0.011 (0.013)
Train: 1 [ 600/1251 ( 48%)]  Loss: 6.725 (6.82)  Time: 0.720s, 1422.89/s  (0.699s, 1465.14/s)  LR: 2.008e-04  Data: 0.014 (0.013)
Train: 1 [ 650/1251 ( 52%)]  Loss: 6.662 (6.80)  Time: 0.753s, 1359.70/s  (0.700s, 1462.31/s)  LR: 2.008e-04  Data: 0.010 (0.013)
Train: 1 [ 700/1251 ( 56%)]  Loss: 6.613 (6.79)  Time: 0.674s, 1518.91/s  (0.701s, 1460.33/s)  LR: 2.008e-04  Data: 0.011 (0.013)
Train: 1 [ 750/1251 ( 60%)]  Loss: 6.703 (6.79)  Time: 0.709s, 1444.84/s  (0.701s, 1460.44/s)  LR: 2.008e-04  Data: 0.009 (0.012)
Train: 1 [ 800/1251 ( 64%)]  Loss: 6.613 (6.78)  Time: 0.683s, 1499.60/s  (0.700s, 1462.39/s)  LR: 2.008e-04  Data: 0.009 (0.012)
Train: 1 [ 850/1251 ( 68%)]  Loss: 6.635 (6.77)  Time: 0.705s, 1452.82/s  (0.700s, 1463.22/s)  LR: 2.008e-04  Data: 0.009 (0.012)
Train: 1 [ 900/1251 ( 72%)]  Loss: 6.635 (6.76)  Time: 0.708s, 1446.59/s  (0.700s, 1462.87/s)  LR: 2.008e-04  Data: 0.009 (0.012)
Train: 1 [ 950/1251 ( 76%)]  Loss: 6.653 (6.76)  Time: 0.677s, 1512.98/s  (0.700s, 1463.27/s)  LR: 2.008e-04  Data: 0.010 (0.012)
Train: 1 [1000/1251 ( 80%)]  Loss: 6.588 (6.75)  Time: 0.704s, 1454.23/s  (0.699s, 1464.32/s)  LR: 2.008e-04  Data: 0.009 (0.012)
Train: 1 [1050/1251 ( 84%)]  Loss: 6.616 (6.74)  Time: 0.716s, 1430.77/s  (0.699s, 1464.85/s)  LR: 2.008e-04  Data: 0.009 (0.012)
Train: 1 [1100/1251 ( 88%)]  Loss: 6.458 (6.73)  Time: 0.683s, 1499.52/s  (0.699s, 1464.89/s)  LR: 2.008e-04  Data: 0.010 (0.012)
Train: 1 [1150/1251 ( 92%)]  Loss: 6.414 (6.72)  Time: 0.671s, 1526.41/s  (0.699s, 1464.62/s)  LR: 2.008e-04  Data: 0.010 (0.012)
Train: 1 [1200/1251 ( 96%)]  Loss: 6.618 (6.71)  Time: 0.711s, 1439.74/s  (0.699s, 1464.86/s)  LR: 2.008e-04  Data: 0.009 (0.011)
Train: 1 [1250/1251 (100%)]  Loss: 6.404 (6.70)  Time: 0.660s, 1552.19/s  (0.699s, 1464.98/s)  LR: 2.008e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.448 (1.448)  Loss:  5.7891 (5.7891)  Acc@1:  0.5859 ( 0.5859)  Acc@5: 13.8672 (13.8672)
Test: [  48/48]  Time: 0.137 (0.584)  Loss:  5.5312 (5.9487)  Acc@1: 10.4953 ( 2.2620)  Acc@5: 21.5802 ( 8.6020)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-1.pth.tar', 2.262000018005371)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-0.pth.tar', 0.18600000118255616)

Train: 2 [   0/1251 (  0%)]  Loss: 6.588 (6.59)  Time: 2.198s,  465.84/s  (2.198s,  465.84/s)  LR: 4.006e-04  Data: 1.582 (1.582)
Train: 2 [  50/1251 (  4%)]  Loss: 6.512 (6.55)  Time: 0.703s, 1455.95/s  (0.735s, 1393.54/s)  LR: 4.006e-04  Data: 0.009 (0.051)
Train: 2 [ 100/1251 (  8%)]  Loss: 6.496 (6.53)  Time: 0.708s, 1445.41/s  (0.718s, 1425.75/s)  LR: 4.006e-04  Data: 0.009 (0.031)
Train: 2 [ 150/1251 ( 12%)]  Loss: 6.635 (6.56)  Time: 0.673s, 1522.15/s  (0.710s, 1441.39/s)  LR: 4.006e-04  Data: 0.009 (0.024)
Train: 2 [ 200/1251 ( 16%)]  Loss: 6.492 (6.54)  Time: 0.725s, 1412.05/s  (0.708s, 1445.52/s)  LR: 4.006e-04  Data: 0.010 (0.020)
Train: 2 [ 250/1251 ( 20%)]  Loss: 6.433 (6.53)  Time: 0.674s, 1520.08/s  (0.705s, 1451.96/s)  LR: 4.006e-04  Data: 0.010 (0.018)
Train: 2 [ 300/1251 ( 24%)]  Loss: 6.526 (6.53)  Time: 0.727s, 1409.24/s  (0.705s, 1453.37/s)  LR: 4.006e-04  Data: 0.009 (0.017)
Train: 2 [ 350/1251 ( 28%)]  Loss: 6.499 (6.52)  Time: 0.713s, 1436.15/s  (0.702s, 1457.74/s)  LR: 4.006e-04  Data: 0.009 (0.016)
Train: 2 [ 400/1251 ( 32%)]  Loss: 6.564 (6.53)  Time: 0.674s, 1519.69/s  (0.703s, 1457.60/s)  LR: 4.006e-04  Data: 0.010 (0.015)
Train: 2 [ 450/1251 ( 36%)]  Loss: 6.465 (6.52)  Time: 0.703s, 1457.45/s  (0.702s, 1459.61/s)  LR: 4.006e-04  Data: 0.009 (0.015)
Train: 2 [ 500/1251 ( 40%)]  Loss: 6.352 (6.51)  Time: 0.707s, 1448.74/s  (0.703s, 1456.98/s)  LR: 4.006e-04  Data: 0.009 (0.014)
Train: 2 [ 550/1251 ( 44%)]  Loss: 6.305 (6.49)  Time: 0.713s, 1435.27/s  (0.703s, 1457.21/s)  LR: 4.006e-04  Data: 0.010 (0.014)
Train: 2 [ 600/1251 ( 48%)]  Loss: 6.317 (6.48)  Time: 0.704s, 1455.30/s  (0.702s, 1458.93/s)  LR: 4.006e-04  Data: 0.011 (0.013)
Train: 2 [ 650/1251 ( 52%)]  Loss: 6.284 (6.46)  Time: 0.703s, 1456.62/s  (0.702s, 1458.80/s)  LR: 4.006e-04  Data: 0.009 (0.013)
Train: 2 [ 700/1251 ( 56%)]  Loss: 6.119 (6.44)  Time: 0.692s, 1480.63/s  (0.702s, 1458.51/s)  LR: 4.006e-04  Data: 0.009 (0.013)
Train: 2 [ 750/1251 ( 60%)]  Loss: 6.320 (6.43)  Time: 0.693s, 1478.62/s  (0.702s, 1458.97/s)  LR: 4.006e-04  Data: 0.009 (0.013)
Train: 2 [ 800/1251 ( 64%)]  Loss: 6.316 (6.42)  Time: 0.691s, 1481.63/s  (0.702s, 1459.59/s)  LR: 4.006e-04  Data: 0.010 (0.013)
Train: 2 [ 850/1251 ( 68%)]  Loss: 6.336 (6.42)  Time: 0.672s, 1524.76/s  (0.702s, 1459.19/s)  LR: 4.006e-04  Data: 0.009 (0.013)
Train: 2 [ 900/1251 ( 72%)]  Loss: 6.268 (6.41)  Time: 0.712s, 1438.45/s  (0.702s, 1459.60/s)  LR: 4.006e-04  Data: 0.009 (0.012)
Train: 2 [ 950/1251 ( 76%)]  Loss: 6.201 (6.40)  Time: 0.715s, 1432.42/s  (0.701s, 1460.16/s)  LR: 4.006e-04  Data: 0.009 (0.012)
Train: 2 [1000/1251 ( 80%)]  Loss: 6.196 (6.39)  Time: 0.673s, 1520.93/s  (0.701s, 1460.39/s)  LR: 4.006e-04  Data: 0.010 (0.012)
Train: 2 [1050/1251 ( 84%)]  Loss: 6.360 (6.39)  Time: 0.711s, 1440.33/s  (0.701s, 1460.58/s)  LR: 4.006e-04  Data: 0.010 (0.012)
Train: 2 [1100/1251 ( 88%)]  Loss: 6.297 (6.39)  Time: 0.701s, 1460.91/s  (0.701s, 1461.05/s)  LR: 4.006e-04  Data: 0.009 (0.012)
Train: 2 [1150/1251 ( 92%)]  Loss: 6.158 (6.38)  Time: 0.757s, 1352.96/s  (0.701s, 1461.66/s)  LR: 4.006e-04  Data: 0.014 (0.012)
Train: 2 [1200/1251 ( 96%)]  Loss: 6.123 (6.37)  Time: 0.676s, 1514.39/s  (0.700s, 1462.17/s)  LR: 4.006e-04  Data: 0.009 (0.012)
Train: 2 [1250/1251 (100%)]  Loss: 6.002 (6.35)  Time: 0.707s, 1449.21/s  (0.700s, 1462.67/s)  LR: 4.006e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.625 (1.625)  Loss:  4.9336 (4.9336)  Acc@1:  7.1289 ( 7.1289)  Acc@5: 24.7070 (24.7070)
Test: [  48/48]  Time: 0.140 (0.578)  Loss:  4.4297 (5.0706)  Acc@1: 22.5236 ( 8.4880)  Acc@5: 37.8538 (23.0940)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-2.pth.tar', 8.487999958496093)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-1.pth.tar', 2.262000018005371)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-0.pth.tar', 0.18600000118255616)

Train: 3 [   0/1251 (  0%)]  Loss: 6.210 (6.21)  Time: 2.273s,  450.56/s  (2.273s,  450.56/s)  LR: 6.004e-04  Data: 1.650 (1.650)
Train: 3 [  50/1251 (  4%)]  Loss: 6.290 (6.25)  Time: 0.669s, 1531.16/s  (0.738s, 1387.05/s)  LR: 6.004e-04  Data: 0.010 (0.048)
Train: 3 [ 100/1251 (  8%)]  Loss: 6.240 (6.25)  Time: 0.750s, 1365.77/s  (0.720s, 1423.16/s)  LR: 6.004e-04  Data: 0.009 (0.030)
Train: 3 [ 150/1251 ( 12%)]  Loss: 6.217 (6.24)  Time: 0.704s, 1454.65/s  (0.712s, 1437.99/s)  LR: 6.004e-04  Data: 0.009 (0.023)
Train: 3 [ 200/1251 ( 16%)]  Loss: 6.180 (6.23)  Time: 0.676s, 1514.90/s  (0.708s, 1446.64/s)  LR: 6.004e-04  Data: 0.010 (0.020)
Train: 3 [ 250/1251 ( 20%)]  Loss: 6.189 (6.22)  Time: 0.708s, 1447.34/s  (0.705s, 1453.07/s)  LR: 6.004e-04  Data: 0.009 (0.018)
Train: 3 [ 300/1251 ( 24%)]  Loss: 5.944 (6.18)  Time: 0.703s, 1456.66/s  (0.704s, 1454.19/s)  LR: 6.004e-04  Data: 0.010 (0.017)
Train: 3 [ 350/1251 ( 28%)]  Loss: 6.281 (6.19)  Time: 0.679s, 1508.88/s  (0.703s, 1456.48/s)  LR: 6.004e-04  Data: 0.010 (0.016)
Train: 3 [ 400/1251 ( 32%)]  Loss: 6.169 (6.19)  Time: 0.712s, 1437.32/s  (0.702s, 1458.63/s)  LR: 6.004e-04  Data: 0.009 (0.015)
Train: 3 [ 450/1251 ( 36%)]  Loss: 6.051 (6.18)  Time: 0.711s, 1440.38/s  (0.702s, 1458.96/s)  LR: 6.004e-04  Data: 0.009 (0.014)
Train: 3 [ 500/1251 ( 40%)]  Loss: 6.084 (6.17)  Time: 0.711s, 1440.31/s  (0.702s, 1459.16/s)  LR: 6.004e-04  Data: 0.009 (0.014)
Train: 3 [ 550/1251 ( 44%)]  Loss: 6.159 (6.17)  Time: 0.681s, 1503.90/s  (0.702s, 1459.20/s)  LR: 6.004e-04  Data: 0.010 (0.014)
Train: 3 [ 600/1251 ( 48%)]  Loss: 6.270 (6.18)  Time: 0.730s, 1402.73/s  (0.702s, 1459.28/s)  LR: 6.004e-04  Data: 0.009 (0.013)
Train: 3 [ 650/1251 ( 52%)]  Loss: 6.217 (6.18)  Time: 0.684s, 1497.51/s  (0.702s, 1459.33/s)  LR: 6.004e-04  Data: 0.013 (0.013)
Train: 3 [ 700/1251 ( 56%)]  Loss: 5.920 (6.16)  Time: 0.706s, 1450.14/s  (0.702s, 1459.04/s)  LR: 6.004e-04  Data: 0.010 (0.013)
Train: 3 [ 750/1251 ( 60%)]  Loss: 5.991 (6.15)  Time: 0.686s, 1493.19/s  (0.702s, 1459.69/s)  LR: 6.004e-04  Data: 0.009 (0.013)
Train: 3 [ 800/1251 ( 64%)]  Loss: 5.839 (6.13)  Time: 0.674s, 1518.76/s  (0.701s, 1461.29/s)  LR: 6.004e-04  Data: 0.010 (0.013)
Train: 3 [ 850/1251 ( 68%)]  Loss: 6.085 (6.13)  Time: 0.676s, 1514.48/s  (0.700s, 1463.00/s)  LR: 6.004e-04  Data: 0.009 (0.012)
Train: 3 [ 900/1251 ( 72%)]  Loss: 5.966 (6.12)  Time: 0.677s, 1513.05/s  (0.700s, 1462.96/s)  LR: 6.004e-04  Data: 0.010 (0.012)
Train: 3 [ 950/1251 ( 76%)]  Loss: 5.668 (6.10)  Time: 0.711s, 1439.93/s  (0.700s, 1462.40/s)  LR: 6.004e-04  Data: 0.010 (0.012)
Train: 3 [1000/1251 ( 80%)]  Loss: 6.043 (6.10)  Time: 0.672s, 1523.41/s  (0.700s, 1462.61/s)  LR: 6.004e-04  Data: 0.009 (0.012)
Train: 3 [1050/1251 ( 84%)]  Loss: 6.175 (6.10)  Time: 0.694s, 1475.27/s  (0.700s, 1462.67/s)  LR: 6.004e-04  Data: 0.010 (0.012)
Train: 3 [1100/1251 ( 88%)]  Loss: 5.725 (6.08)  Time: 0.674s, 1520.25/s  (0.700s, 1462.82/s)  LR: 6.004e-04  Data: 0.010 (0.012)
Train: 3 [1150/1251 ( 92%)]  Loss: 5.590 (6.06)  Time: 0.672s, 1523.99/s  (0.700s, 1462.94/s)  LR: 6.004e-04  Data: 0.009 (0.012)
Train: 3 [1200/1251 ( 96%)]  Loss: 5.959 (6.06)  Time: 0.729s, 1405.23/s  (0.700s, 1463.04/s)  LR: 6.004e-04  Data: 0.012 (0.012)
Train: 3 [1250/1251 (100%)]  Loss: 5.954 (6.05)  Time: 0.661s, 1550.27/s  (0.700s, 1462.89/s)  LR: 6.004e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.503 (1.503)  Loss:  4.2930 (4.2930)  Acc@1: 15.7227 (15.7227)  Acc@5: 39.7461 (39.7461)
Test: [  48/48]  Time: 0.136 (0.592)  Loss:  3.3887 (4.4654)  Acc@1: 38.0896 (15.0800)  Acc@5: 58.4906 (35.8660)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-3.pth.tar', 15.08000006225586)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-2.pth.tar', 8.487999958496093)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-1.pth.tar', 2.262000018005371)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-0.pth.tar', 0.18600000118255616)

Train: 4 [   0/1251 (  0%)]  Loss: 5.881 (5.88)  Time: 2.031s,  504.24/s  (2.031s,  504.24/s)  LR: 8.002e-04  Data: 1.389 (1.389)
Train: 4 [  50/1251 (  4%)]  Loss: 5.993 (5.94)  Time: 0.674s, 1518.56/s  (0.736s, 1390.89/s)  LR: 8.002e-04  Data: 0.010 (0.047)
Train: 4 [ 100/1251 (  8%)]  Loss: 5.918 (5.93)  Time: 0.698s, 1466.63/s  (0.720s, 1422.73/s)  LR: 8.002e-04  Data: 0.013 (0.029)
Train: 4 [ 150/1251 ( 12%)]  Loss: 5.966 (5.94)  Time: 0.794s, 1289.56/s  (0.712s, 1437.53/s)  LR: 8.002e-04  Data: 0.009 (0.023)
Train: 4 [ 200/1251 ( 16%)]  Loss: 5.712 (5.89)  Time: 0.703s, 1457.12/s  (0.709s, 1443.28/s)  LR: 8.002e-04  Data: 0.009 (0.020)
Train: 4 [ 250/1251 ( 20%)]  Loss: 5.886 (5.89)  Time: 0.718s, 1426.48/s  (0.707s, 1449.13/s)  LR: 8.002e-04  Data: 0.009 (0.018)
Train: 4 [ 300/1251 ( 24%)]  Loss: 6.004 (5.91)  Time: 0.712s, 1438.45/s  (0.705s, 1452.42/s)  LR: 8.002e-04  Data: 0.010 (0.016)
Train: 4 [ 350/1251 ( 28%)]  Loss: 5.896 (5.91)  Time: 0.709s, 1443.99/s  (0.704s, 1453.75/s)  LR: 8.002e-04  Data: 0.009 (0.015)
Train: 4 [ 400/1251 ( 32%)]  Loss: 5.892 (5.91)  Time: 0.702s, 1458.18/s  (0.704s, 1455.33/s)  LR: 8.002e-04  Data: 0.009 (0.015)
Train: 4 [ 450/1251 ( 36%)]  Loss: 6.030 (5.92)  Time: 0.707s, 1447.98/s  (0.703s, 1455.73/s)  LR: 8.002e-04  Data: 0.015 (0.014)
Train: 4 [ 500/1251 ( 40%)]  Loss: 5.720 (5.90)  Time: 0.726s, 1410.48/s  (0.703s, 1456.12/s)  LR: 8.002e-04  Data: 0.010 (0.014)
Train: 4 [ 550/1251 ( 44%)]  Loss: 5.790 (5.89)  Time: 0.704s, 1454.99/s  (0.703s, 1457.56/s)  LR: 8.002e-04  Data: 0.009 (0.013)
Train: 4 [ 600/1251 ( 48%)]  Loss: 5.692 (5.88)  Time: 0.706s, 1449.52/s  (0.702s, 1459.01/s)  LR: 8.002e-04  Data: 0.013 (0.013)
Train: 4 [ 650/1251 ( 52%)]  Loss: 5.713 (5.86)  Time: 0.670s, 1528.26/s  (0.702s, 1459.46/s)  LR: 8.002e-04  Data: 0.010 (0.013)
Train: 4 [ 700/1251 ( 56%)]  Loss: 5.870 (5.86)  Time: 0.680s, 1506.98/s  (0.701s, 1460.75/s)  LR: 8.002e-04  Data: 0.009 (0.013)
Train: 4 [ 750/1251 ( 60%)]  Loss: 5.764 (5.86)  Time: 0.674s, 1519.57/s  (0.700s, 1462.07/s)  LR: 8.002e-04  Data: 0.010 (0.012)
Train: 4 [ 800/1251 ( 64%)]  Loss: 5.761 (5.85)  Time: 0.700s, 1462.12/s  (0.700s, 1462.81/s)  LR: 8.002e-04  Data: 0.010 (0.012)
Train: 4 [ 850/1251 ( 68%)]  Loss: 5.719 (5.84)  Time: 0.714s, 1434.00/s  (0.700s, 1463.09/s)  LR: 8.002e-04  Data: 0.013 (0.012)
Train: 4 [ 900/1251 ( 72%)]  Loss: 5.950 (5.85)  Time: 0.675s, 1516.92/s  (0.700s, 1462.87/s)  LR: 8.002e-04  Data: 0.010 (0.012)
Train: 4 [ 950/1251 ( 76%)]  Loss: 5.675 (5.84)  Time: 0.706s, 1450.93/s  (0.700s, 1462.77/s)  LR: 8.002e-04  Data: 0.009 (0.012)
Train: 4 [1000/1251 ( 80%)]  Loss: 5.474 (5.82)  Time: 0.711s, 1440.67/s  (0.700s, 1463.52/s)  LR: 8.002e-04  Data: 0.010 (0.012)
Train: 4 [1050/1251 ( 84%)]  Loss: 5.581 (5.81)  Time: 0.711s, 1440.14/s  (0.700s, 1463.14/s)  LR: 8.002e-04  Data: 0.011 (0.012)
Train: 4 [1100/1251 ( 88%)]  Loss: 5.772 (5.81)  Time: 0.673s, 1521.91/s  (0.700s, 1463.41/s)  LR: 8.002e-04  Data: 0.009 (0.012)
Train: 4 [1150/1251 ( 92%)]  Loss: 5.719 (5.81)  Time: 0.689s, 1485.71/s  (0.699s, 1464.48/s)  LR: 8.002e-04  Data: 0.010 (0.012)
Train: 4 [1200/1251 ( 96%)]  Loss: 5.710 (5.80)  Time: 0.768s, 1334.03/s  (0.699s, 1464.20/s)  LR: 8.002e-04  Data: 0.009 (0.012)
Train: 4 [1250/1251 (100%)]  Loss: 5.712 (5.80)  Time: 0.660s, 1552.30/s  (0.699s, 1464.55/s)  LR: 8.002e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.458 (1.458)  Loss:  3.3750 (3.3750)  Acc@1: 32.6172 (32.6172)  Acc@5: 61.0352 (61.0352)
Test: [  48/48]  Time: 0.136 (0.588)  Loss:  3.0039 (4.0509)  Acc@1: 47.5236 (20.3660)  Acc@5: 66.0377 (43.5520)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-4.pth.tar', 20.366000023193358)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-3.pth.tar', 15.08000006225586)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-2.pth.tar', 8.487999958496093)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-1.pth.tar', 2.262000018005371)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-0.pth.tar', 0.18600000118255616)

Train: 5 [   0/1251 (  0%)]  Loss: 5.615 (5.62)  Time: 2.177s,  470.46/s  (2.177s,  470.46/s)  LR: 9.998e-04  Data: 1.561 (1.561)
Train: 5 [  50/1251 (  4%)]  Loss: 5.718 (5.67)  Time: 0.681s, 1504.24/s  (0.729s, 1404.63/s)  LR: 9.998e-04  Data: 0.009 (0.043)
Train: 5 [ 100/1251 (  8%)]  Loss: 5.659 (5.66)  Time: 0.684s, 1497.84/s  (0.711s, 1440.49/s)  LR: 9.998e-04  Data: 0.009 (0.027)
Train: 5 [ 150/1251 ( 12%)]  Loss: 5.687 (5.67)  Time: 0.709s, 1444.13/s  (0.706s, 1449.87/s)  LR: 9.998e-04  Data: 0.015 (0.021)
Train: 5 [ 200/1251 ( 16%)]  Loss: 5.516 (5.64)  Time: 0.673s, 1522.15/s  (0.703s, 1455.58/s)  LR: 9.998e-04  Data: 0.009 (0.018)
Train: 5 [ 250/1251 ( 20%)]  Loss: 5.511 (5.62)  Time: 0.682s, 1502.24/s  (0.701s, 1460.80/s)  LR: 9.998e-04  Data: 0.010 (0.017)
Train: 5 [ 300/1251 ( 24%)]  Loss: 5.442 (5.59)  Time: 0.831s, 1232.83/s  (0.701s, 1460.57/s)  LR: 9.998e-04  Data: 0.009 (0.015)
Train: 5 [ 350/1251 ( 28%)]  Loss: 5.581 (5.59)  Time: 0.704s, 1455.16/s  (0.701s, 1460.24/s)  LR: 9.998e-04  Data: 0.009 (0.015)
Train: 5 [ 400/1251 ( 32%)]  Loss: 5.784 (5.61)  Time: 0.734s, 1394.64/s  (0.700s, 1462.35/s)  LR: 9.998e-04  Data: 0.017 (0.014)
Train: 5 [ 450/1251 ( 36%)]  Loss: 5.601 (5.61)  Time: 0.684s, 1497.79/s  (0.700s, 1463.09/s)  LR: 9.998e-04  Data: 0.010 (0.014)
Train: 5 [ 500/1251 ( 40%)]  Loss: 5.509 (5.60)  Time: 0.676s, 1515.79/s  (0.700s, 1462.26/s)  LR: 9.998e-04  Data: 0.013 (0.013)
Train: 5 [ 550/1251 ( 44%)]  Loss: 5.579 (5.60)  Time: 0.707s, 1449.31/s  (0.700s, 1462.01/s)  LR: 9.998e-04  Data: 0.010 (0.013)
Train: 5 [ 600/1251 ( 48%)]  Loss: 5.449 (5.59)  Time: 0.710s, 1441.58/s  (0.700s, 1462.45/s)  LR: 9.998e-04  Data: 0.009 (0.013)
Train: 5 [ 650/1251 ( 52%)]  Loss: 5.686 (5.60)  Time: 0.730s, 1402.59/s  (0.700s, 1461.97/s)  LR: 9.998e-04  Data: 0.011 (0.013)
Train: 5 [ 700/1251 ( 56%)]  Loss: 5.358 (5.58)  Time: 0.693s, 1477.71/s  (0.701s, 1460.66/s)  LR: 9.998e-04  Data: 0.012 (0.012)
Train: 5 [ 750/1251 ( 60%)]  Loss: 5.861 (5.60)  Time: 0.709s, 1443.42/s  (0.701s, 1460.95/s)  LR: 9.998e-04  Data: 0.009 (0.012)
Train: 5 [ 800/1251 ( 64%)]  Loss: 5.004 (5.56)  Time: 0.709s, 1443.64/s  (0.701s, 1461.77/s)  LR: 9.998e-04  Data: 0.011 (0.012)
Train: 5 [ 850/1251 ( 68%)]  Loss: 5.707 (5.57)  Time: 0.679s, 1508.21/s  (0.700s, 1463.00/s)  LR: 9.998e-04  Data: 0.011 (0.012)
Train: 5 [ 900/1251 ( 72%)]  Loss: 5.649 (5.57)  Time: 0.721s, 1420.43/s  (0.700s, 1463.27/s)  LR: 9.998e-04  Data: 0.010 (0.012)
Train: 5 [ 950/1251 ( 76%)]  Loss: 5.564 (5.57)  Time: 0.710s, 1442.03/s  (0.700s, 1463.86/s)  LR: 9.998e-04  Data: 0.010 (0.012)
Train: 5 [1000/1251 ( 80%)]  Loss: 5.205 (5.56)  Time: 0.710s, 1441.91/s  (0.700s, 1463.01/s)  LR: 9.998e-04  Data: 0.009 (0.012)
Train: 5 [1050/1251 ( 84%)]  Loss: 5.396 (5.55)  Time: 0.702s, 1459.64/s  (0.700s, 1462.68/s)  LR: 9.998e-04  Data: 0.009 (0.012)
Train: 5 [1100/1251 ( 88%)]  Loss: 5.593 (5.55)  Time: 0.706s, 1449.96/s  (0.700s, 1463.63/s)  LR: 9.998e-04  Data: 0.009 (0.012)
Train: 5 [1150/1251 ( 92%)]  Loss: 5.251 (5.54)  Time: 0.737s, 1390.22/s  (0.700s, 1463.01/s)  LR: 9.998e-04  Data: 0.012 (0.011)
Train: 5 [1200/1251 ( 96%)]  Loss: 5.458 (5.54)  Time: 0.693s, 1476.77/s  (0.700s, 1463.28/s)  LR: 9.998e-04  Data: 0.010 (0.011)
Train: 5 [1250/1251 (100%)]  Loss: 5.072 (5.52)  Time: 0.725s, 1413.38/s  (0.700s, 1463.33/s)  LR: 9.998e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.495 (1.495)  Loss:  3.0078 (3.0078)  Acc@1: 35.2539 (35.2539)  Acc@5: 65.7227 (65.7227)
Test: [  48/48]  Time: 0.136 (0.585)  Loss:  2.7109 (3.6413)  Acc@1: 46.9340 (25.9820)  Acc@5: 67.6887 (51.4140)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-5.pth.tar', 25.982000090332033)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-4.pth.tar', 20.366000023193358)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-3.pth.tar', 15.08000006225586)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-2.pth.tar', 8.487999958496093)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-1.pth.tar', 2.262000018005371)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-0.pth.tar', 0.18600000118255616)

Train: 6 [   0/1251 (  0%)]  Loss: 5.253 (5.25)  Time: 2.130s,  480.80/s  (2.130s,  480.80/s)  LR: 9.998e-04  Data: 1.514 (1.514)
Train: 6 [  50/1251 (  4%)]  Loss: 5.578 (5.42)  Time: 0.672s, 1523.82/s  (0.734s, 1396.02/s)  LR: 9.998e-04  Data: 0.009 (0.046)
Train: 6 [ 100/1251 (  8%)]  Loss: 5.505 (5.45)  Time: 0.679s, 1507.57/s  (0.715s, 1431.56/s)  LR: 9.998e-04  Data: 0.009 (0.028)
Train: 6 [ 150/1251 ( 12%)]  Loss: 5.265 (5.40)  Time: 0.672s, 1523.22/s  (0.709s, 1445.03/s)  LR: 9.998e-04  Data: 0.009 (0.022)
Train: 6 [ 200/1251 ( 16%)]  Loss: 5.476 (5.42)  Time: 0.693s, 1478.44/s  (0.706s, 1450.43/s)  LR: 9.998e-04  Data: 0.009 (0.019)
Train: 6 [ 250/1251 ( 20%)]  Loss: 5.440 (5.42)  Time: 0.687s, 1489.87/s  (0.704s, 1454.07/s)  LR: 9.998e-04  Data: 0.011 (0.017)
Train: 6 [ 300/1251 ( 24%)]  Loss: 5.556 (5.44)  Time: 0.687s, 1489.75/s  (0.704s, 1453.92/s)  LR: 9.998e-04  Data: 0.017 (0.016)
Train: 6 [ 350/1251 ( 28%)]  Loss: 5.243 (5.41)  Time: 0.669s, 1530.62/s  (0.704s, 1453.87/s)  LR: 9.998e-04  Data: 0.011 (0.015)
Train: 6 [ 400/1251 ( 32%)]  Loss: 5.157 (5.39)  Time: 0.673s, 1521.39/s  (0.703s, 1456.47/s)  LR: 9.998e-04  Data: 0.011 (0.015)
Train: 6 [ 450/1251 ( 36%)]  Loss: 5.315 (5.38)  Time: 0.675s, 1516.83/s  (0.702s, 1459.48/s)  LR: 9.998e-04  Data: 0.010 (0.014)
Train: 6 [ 500/1251 ( 40%)]  Loss: 5.415 (5.38)  Time: 0.673s, 1520.59/s  (0.702s, 1459.62/s)  LR: 9.998e-04  Data: 0.010 (0.014)
Train: 6 [ 550/1251 ( 44%)]  Loss: 5.584 (5.40)  Time: 0.672s, 1523.33/s  (0.701s, 1459.84/s)  LR: 9.998e-04  Data: 0.010 (0.013)
Train: 6 [ 600/1251 ( 48%)]  Loss: 5.543 (5.41)  Time: 0.703s, 1455.88/s  (0.701s, 1459.85/s)  LR: 9.998e-04  Data: 0.011 (0.013)
Train: 6 [ 650/1251 ( 52%)]  Loss: 5.347 (5.41)  Time: 0.665s, 1539.78/s  (0.701s, 1460.05/s)  LR: 9.998e-04  Data: 0.010 (0.013)
Train: 6 [ 700/1251 ( 56%)]  Loss: 5.284 (5.40)  Time: 0.698s, 1467.76/s  (0.701s, 1460.36/s)  LR: 9.998e-04  Data: 0.009 (0.013)
Train: 6 [ 750/1251 ( 60%)]  Loss: 5.242 (5.39)  Time: 0.673s, 1521.01/s  (0.701s, 1460.68/s)  LR: 9.998e-04  Data: 0.009 (0.012)
Train: 6 [ 800/1251 ( 64%)]  Loss: 4.975 (5.36)  Time: 0.673s, 1520.69/s  (0.701s, 1460.70/s)  LR: 9.998e-04  Data: 0.010 (0.012)
Train: 6 [ 850/1251 ( 68%)]  Loss: 5.269 (5.36)  Time: 0.674s, 1518.66/s  (0.701s, 1460.78/s)  LR: 9.998e-04  Data: 0.010 (0.012)
Train: 6 [ 900/1251 ( 72%)]  Loss: 5.349 (5.36)  Time: 0.668s, 1531.98/s  (0.701s, 1461.54/s)  LR: 9.998e-04  Data: 0.012 (0.012)
Train: 6 [ 950/1251 ( 76%)]  Loss: 5.382 (5.36)  Time: 0.720s, 1422.72/s  (0.700s, 1461.82/s)  LR: 9.998e-04  Data: 0.010 (0.012)
Train: 6 [1000/1251 ( 80%)]  Loss: 5.328 (5.36)  Time: 0.723s, 1417.25/s  (0.700s, 1461.87/s)  LR: 9.998e-04  Data: 0.010 (0.012)
Train: 6 [1050/1251 ( 84%)]  Loss: 5.001 (5.34)  Time: 0.675s, 1516.46/s  (0.700s, 1462.13/s)  LR: 9.998e-04  Data: 0.009 (0.012)
Train: 6 [1100/1251 ( 88%)]  Loss: 5.166 (5.33)  Time: 0.717s, 1428.64/s  (0.700s, 1462.67/s)  LR: 9.998e-04  Data: 0.009 (0.012)
Train: 6 [1150/1251 ( 92%)]  Loss: 5.088 (5.32)  Time: 0.707s, 1448.53/s  (0.700s, 1462.91/s)  LR: 9.998e-04  Data: 0.010 (0.012)
Train: 6 [1200/1251 ( 96%)]  Loss: 5.377 (5.33)  Time: 0.708s, 1447.27/s  (0.700s, 1461.97/s)  LR: 9.998e-04  Data: 0.010 (0.012)
Train: 6 [1250/1251 (100%)]  Loss: 5.274 (5.32)  Time: 0.656s, 1560.11/s  (0.701s, 1461.59/s)  LR: 9.998e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.495 (1.495)  Loss:  2.6445 (2.6445)  Acc@1: 50.0977 (50.0977)  Acc@5: 77.8320 (77.8320)
Test: [  48/48]  Time: 0.136 (0.577)  Loss:  2.3672 (3.2641)  Acc@1: 56.8396 (34.5620)  Acc@5: 77.7123 (61.3760)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-6.pth.tar', 34.561999997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-5.pth.tar', 25.982000090332033)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-4.pth.tar', 20.366000023193358)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-3.pth.tar', 15.08000006225586)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-2.pth.tar', 8.487999958496093)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-1.pth.tar', 2.262000018005371)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-0.pth.tar', 0.18600000118255616)

Train: 7 [   0/1251 (  0%)]  Loss: 5.224 (5.22)  Time: 2.081s,  492.15/s  (2.081s,  492.15/s)  LR: 9.997e-04  Data: 1.466 (1.466)
Train: 7 [  50/1251 (  4%)]  Loss: 5.016 (5.12)  Time: 0.679s, 1509.16/s  (0.735s, 1392.30/s)  LR: 9.997e-04  Data: 0.010 (0.049)
Train: 7 [ 100/1251 (  8%)]  Loss: 5.286 (5.18)  Time: 0.674s, 1518.22/s  (0.715s, 1431.51/s)  LR: 9.997e-04  Data: 0.009 (0.030)
Train: 7 [ 150/1251 ( 12%)]  Loss: 5.022 (5.14)  Time: 0.672s, 1523.99/s  (0.707s, 1449.16/s)  LR: 9.997e-04  Data: 0.011 (0.023)
Train: 7 [ 200/1251 ( 16%)]  Loss: 4.969 (5.10)  Time: 0.672s, 1523.71/s  (0.704s, 1454.66/s)  LR: 9.997e-04  Data: 0.011 (0.020)
Train: 7 [ 250/1251 ( 20%)]  Loss: 4.998 (5.09)  Time: 0.678s, 1510.81/s  (0.702s, 1459.14/s)  LR: 9.997e-04  Data: 0.010 (0.018)
Train: 7 [ 300/1251 ( 24%)]  Loss: 5.404 (5.13)  Time: 0.703s, 1455.85/s  (0.700s, 1462.87/s)  LR: 9.997e-04  Data: 0.009 (0.017)
Train: 7 [ 350/1251 ( 28%)]  Loss: 5.304 (5.15)  Time: 0.724s, 1413.72/s  (0.699s, 1464.83/s)  LR: 9.997e-04  Data: 0.013 (0.016)
Train: 7 [ 400/1251 ( 32%)]  Loss: 5.317 (5.17)  Time: 0.717s, 1427.60/s  (0.699s, 1465.50/s)  LR: 9.997e-04  Data: 0.012 (0.015)
Train: 7 [ 450/1251 ( 36%)]  Loss: 4.961 (5.15)  Time: 0.693s, 1477.98/s  (0.698s, 1466.10/s)  LR: 9.997e-04  Data: 0.010 (0.014)
Train: 7 [ 500/1251 ( 40%)]  Loss: 5.494 (5.18)  Time: 0.685s, 1494.62/s  (0.698s, 1466.30/s)  LR: 9.997e-04  Data: 0.009 (0.014)
Train: 7 [ 550/1251 ( 44%)]  Loss: 5.461 (5.20)  Time: 0.678s, 1509.51/s  (0.698s, 1466.99/s)  LR: 9.997e-04  Data: 0.009 (0.014)
Train: 7 [ 600/1251 ( 48%)]  Loss: 5.196 (5.20)  Time: 0.683s, 1499.30/s  (0.698s, 1466.04/s)  LR: 9.997e-04  Data: 0.009 (0.013)
Train: 7 [ 650/1251 ( 52%)]  Loss: 5.000 (5.19)  Time: 0.693s, 1477.51/s  (0.698s, 1466.08/s)  LR: 9.997e-04  Data: 0.009 (0.013)
Train: 7 [ 700/1251 ( 56%)]  Loss: 5.305 (5.20)  Time: 0.717s, 1427.28/s  (0.698s, 1466.64/s)  LR: 9.997e-04  Data: 0.010 (0.013)
Train: 7 [ 750/1251 ( 60%)]  Loss: 4.905 (5.18)  Time: 0.703s, 1457.34/s  (0.698s, 1466.97/s)  LR: 9.997e-04  Data: 0.009 (0.013)
Train: 7 [ 800/1251 ( 64%)]  Loss: 5.224 (5.18)  Time: 0.680s, 1506.40/s  (0.698s, 1467.26/s)  LR: 9.997e-04  Data: 0.010 (0.012)
Train: 7 [ 850/1251 ( 68%)]  Loss: 5.021 (5.17)  Time: 0.699s, 1464.12/s  (0.698s, 1467.36/s)  LR: 9.997e-04  Data: 0.010 (0.012)
Train: 7 [ 900/1251 ( 72%)]  Loss: 5.026 (5.16)  Time: 0.668s, 1531.90/s  (0.698s, 1467.13/s)  LR: 9.997e-04  Data: 0.012 (0.012)
Train: 7 [ 950/1251 ( 76%)]  Loss: 5.401 (5.18)  Time: 0.673s, 1522.53/s  (0.698s, 1467.65/s)  LR: 9.997e-04  Data: 0.009 (0.012)
Train: 7 [1000/1251 ( 80%)]  Loss: 5.244 (5.18)  Time: 0.702s, 1457.79/s  (0.698s, 1467.41/s)  LR: 9.997e-04  Data: 0.009 (0.012)
Train: 7 [1050/1251 ( 84%)]  Loss: 5.169 (5.18)  Time: 0.788s, 1299.91/s  (0.698s, 1466.44/s)  LR: 9.997e-04  Data: 0.009 (0.012)
Train: 7 [1100/1251 ( 88%)]  Loss: 5.294 (5.18)  Time: 0.680s, 1506.01/s  (0.698s, 1466.47/s)  LR: 9.997e-04  Data: 0.010 (0.012)
Train: 7 [1150/1251 ( 92%)]  Loss: 5.028 (5.18)  Time: 0.680s, 1506.77/s  (0.698s, 1466.75/s)  LR: 9.997e-04  Data: 0.011 (0.012)
Train: 7 [1200/1251 ( 96%)]  Loss: 5.632 (5.20)  Time: 0.682s, 1501.59/s  (0.698s, 1466.19/s)  LR: 9.997e-04  Data: 0.009 (0.012)
Train: 7 [1250/1251 (100%)]  Loss: 5.483 (5.21)  Time: 0.661s, 1549.34/s  (0.698s, 1466.25/s)  LR: 9.997e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.606 (1.606)  Loss:  2.1230 (2.1230)  Acc@1: 58.2031 (58.2031)  Acc@5: 83.8867 (83.8867)
Test: [  48/48]  Time: 0.136 (0.589)  Loss:  1.9678 (3.0131)  Acc@1: 63.2076 (37.6880)  Acc@5: 81.4859 (64.5540)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-7.pth.tar', 37.68800004882812)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-6.pth.tar', 34.561999997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-5.pth.tar', 25.982000090332033)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-4.pth.tar', 20.366000023193358)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-3.pth.tar', 15.08000006225586)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-2.pth.tar', 8.487999958496093)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-1.pth.tar', 2.262000018005371)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-0.pth.tar', 0.18600000118255616)

Train: 8 [   0/1251 (  0%)]  Loss: 5.282 (5.28)  Time: 2.271s,  450.91/s  (2.271s,  450.91/s)  LR: 9.996e-04  Data: 1.623 (1.623)
Train: 8 [  50/1251 (  4%)]  Loss: 4.925 (5.10)  Time: 0.704s, 1454.70/s  (0.730s, 1403.39/s)  LR: 9.996e-04  Data: 0.009 (0.050)
Train: 8 [ 100/1251 (  8%)]  Loss: 5.366 (5.19)  Time: 0.705s, 1453.45/s  (0.714s, 1434.45/s)  LR: 9.996e-04  Data: 0.012 (0.030)
Train: 8 [ 150/1251 ( 12%)]  Loss: 4.921 (5.12)  Time: 0.671s, 1527.01/s  (0.709s, 1443.90/s)  LR: 9.996e-04  Data: 0.011 (0.024)
Train: 8 [ 200/1251 ( 16%)]  Loss: 5.110 (5.12)  Time: 0.708s, 1445.65/s  (0.707s, 1449.24/s)  LR: 9.996e-04  Data: 0.010 (0.020)
Train: 8 [ 250/1251 ( 20%)]  Loss: 4.772 (5.06)  Time: 0.676s, 1515.90/s  (0.705s, 1452.59/s)  LR: 9.996e-04  Data: 0.010 (0.018)
Train: 8 [ 300/1251 ( 24%)]  Loss: 5.382 (5.11)  Time: 0.698s, 1467.12/s  (0.705s, 1453.29/s)  LR: 9.996e-04  Data: 0.009 (0.017)
Train: 8 [ 350/1251 ( 28%)]  Loss: 4.959 (5.09)  Time: 0.674s, 1519.17/s  (0.704s, 1453.54/s)  LR: 9.996e-04  Data: 0.009 (0.016)
Train: 8 [ 400/1251 ( 32%)]  Loss: 4.908 (5.07)  Time: 0.688s, 1488.58/s  (0.703s, 1456.45/s)  LR: 9.996e-04  Data: 0.013 (0.015)
Train: 8 [ 450/1251 ( 36%)]  Loss: 5.012 (5.06)  Time: 0.673s, 1521.76/s  (0.702s, 1458.66/s)  LR: 9.996e-04  Data: 0.010 (0.015)
Train: 8 [ 500/1251 ( 40%)]  Loss: 5.143 (5.07)  Time: 0.672s, 1524.11/s  (0.702s, 1458.55/s)  LR: 9.996e-04  Data: 0.010 (0.014)
Train: 8 [ 550/1251 ( 44%)]  Loss: 4.863 (5.05)  Time: 0.672s, 1522.94/s  (0.702s, 1459.01/s)  LR: 9.996e-04  Data: 0.009 (0.014)
Train: 8 [ 600/1251 ( 48%)]  Loss: 5.093 (5.06)  Time: 0.676s, 1514.42/s  (0.702s, 1459.15/s)  LR: 9.996e-04  Data: 0.009 (0.014)
Train: 8 [ 650/1251 ( 52%)]  Loss: 5.325 (5.08)  Time: 0.723s, 1416.80/s  (0.702s, 1458.32/s)  LR: 9.996e-04  Data: 0.009 (0.013)
Train: 8 [ 700/1251 ( 56%)]  Loss: 5.195 (5.08)  Time: 0.673s, 1521.49/s  (0.702s, 1459.03/s)  LR: 9.996e-04  Data: 0.009 (0.013)
Train: 8 [ 750/1251 ( 60%)]  Loss: 4.939 (5.07)  Time: 0.701s, 1460.17/s  (0.702s, 1458.69/s)  LR: 9.996e-04  Data: 0.009 (0.013)
Train: 8 [ 800/1251 ( 64%)]  Loss: 5.154 (5.08)  Time: 0.673s, 1521.01/s  (0.702s, 1458.74/s)  LR: 9.996e-04  Data: 0.010 (0.013)
Train: 8 [ 850/1251 ( 68%)]  Loss: 5.277 (5.09)  Time: 0.679s, 1507.59/s  (0.701s, 1459.96/s)  LR: 9.996e-04  Data: 0.010 (0.013)
Train: 8 [ 900/1251 ( 72%)]  Loss: 4.830 (5.08)  Time: 0.679s, 1507.98/s  (0.701s, 1459.76/s)  LR: 9.996e-04  Data: 0.010 (0.012)
Train: 8 [ 950/1251 ( 76%)]  Loss: 4.612 (5.05)  Time: 0.705s, 1453.34/s  (0.702s, 1459.65/s)  LR: 9.996e-04  Data: 0.010 (0.012)
Train: 8 [1000/1251 ( 80%)]  Loss: 4.954 (5.05)  Time: 0.673s, 1522.26/s  (0.702s, 1459.50/s)  LR: 9.996e-04  Data: 0.009 (0.012)
Train: 8 [1050/1251 ( 84%)]  Loss: 4.900 (5.04)  Time: 0.674s, 1519.80/s  (0.701s, 1459.85/s)  LR: 9.996e-04  Data: 0.010 (0.012)
Train: 8 [1100/1251 ( 88%)]  Loss: 5.029 (5.04)  Time: 0.675s, 1516.75/s  (0.702s, 1459.67/s)  LR: 9.996e-04  Data: 0.010 (0.012)
Train: 8 [1150/1251 ( 92%)]  Loss: 4.694 (5.03)  Time: 0.708s, 1445.87/s  (0.702s, 1459.56/s)  LR: 9.996e-04  Data: 0.009 (0.012)
Train: 8 [1200/1251 ( 96%)]  Loss: 5.360 (5.04)  Time: 0.677s, 1511.96/s  (0.702s, 1459.67/s)  LR: 9.996e-04  Data: 0.010 (0.012)
Train: 8 [1250/1251 (100%)]  Loss: 5.234 (5.05)  Time: 0.701s, 1460.57/s  (0.702s, 1458.91/s)  LR: 9.996e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.583 (1.583)  Loss:  2.1582 (2.1582)  Acc@1: 54.6875 (54.6875)  Acc@5: 80.3711 (80.3711)
Test: [  48/48]  Time: 0.136 (0.583)  Loss:  2.1113 (2.8308)  Acc@1: 59.1981 (40.4600)  Acc@5: 77.2406 (67.4540)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-8.pth.tar', 40.45999998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-7.pth.tar', 37.68800004882812)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-6.pth.tar', 34.561999997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-5.pth.tar', 25.982000090332033)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-4.pth.tar', 20.366000023193358)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-3.pth.tar', 15.08000006225586)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-2.pth.tar', 8.487999958496093)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-1.pth.tar', 2.262000018005371)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-0.pth.tar', 0.18600000118255616)

Train: 9 [   0/1251 (  0%)]  Loss: 4.736 (4.74)  Time: 2.371s,  431.96/s  (2.371s,  431.96/s)  LR: 9.995e-04  Data: 1.712 (1.712)
Train: 9 [  50/1251 (  4%)]  Loss: 4.932 (4.83)  Time: 0.667s, 1536.25/s  (0.741s, 1381.86/s)  LR: 9.995e-04  Data: 0.010 (0.053)
Train: 9 [ 100/1251 (  8%)]  Loss: 4.366 (4.68)  Time: 0.718s, 1426.59/s  (0.719s, 1423.56/s)  LR: 9.995e-04  Data: 0.010 (0.032)
Train: 9 [ 150/1251 ( 12%)]  Loss: 5.094 (4.78)  Time: 0.694s, 1474.80/s  (0.711s, 1440.83/s)  LR: 9.995e-04  Data: 0.009 (0.024)
Train: 9 [ 200/1251 ( 16%)]  Loss: 5.013 (4.83)  Time: 0.675s, 1516.97/s  (0.707s, 1447.85/s)  LR: 9.995e-04  Data: 0.011 (0.021)
Train: 9 [ 250/1251 ( 20%)]  Loss: 4.871 (4.84)  Time: 0.675s, 1517.28/s  (0.704s, 1453.56/s)  LR: 9.995e-04  Data: 0.009 (0.019)
Train: 9 [ 300/1251 ( 24%)]  Loss: 4.853 (4.84)  Time: 0.789s, 1297.14/s  (0.703s, 1455.66/s)  LR: 9.995e-04  Data: 0.010 (0.017)
Train: 9 [ 350/1251 ( 28%)]  Loss: 4.625 (4.81)  Time: 0.702s, 1457.98/s  (0.704s, 1455.38/s)  LR: 9.995e-04  Data: 0.009 (0.016)
Train: 9 [ 400/1251 ( 32%)]  Loss: 4.998 (4.83)  Time: 0.720s, 1422.10/s  (0.704s, 1454.79/s)  LR: 9.995e-04  Data: 0.015 (0.016)
Train: 9 [ 450/1251 ( 36%)]  Loss: 5.078 (4.86)  Time: 0.714s, 1433.86/s  (0.704s, 1455.18/s)  LR: 9.995e-04  Data: 0.011 (0.015)
Train: 9 [ 500/1251 ( 40%)]  Loss: 4.737 (4.85)  Time: 0.682s, 1502.45/s  (0.704s, 1455.23/s)  LR: 9.995e-04  Data: 0.010 (0.014)
Train: 9 [ 550/1251 ( 44%)]  Loss: 5.154 (4.87)  Time: 0.684s, 1497.57/s  (0.703s, 1457.37/s)  LR: 9.995e-04  Data: 0.010 (0.014)
Train: 9 [ 600/1251 ( 48%)]  Loss: 4.772 (4.86)  Time: 0.706s, 1449.67/s  (0.702s, 1458.83/s)  LR: 9.995e-04  Data: 0.010 (0.014)
Train: 9 [ 650/1251 ( 52%)]  Loss: 4.472 (4.84)  Time: 0.705s, 1452.19/s  (0.702s, 1457.96/s)  LR: 9.995e-04  Data: 0.009 (0.013)
Train: 9 [ 700/1251 ( 56%)]  Loss: 5.052 (4.85)  Time: 0.673s, 1521.14/s  (0.702s, 1458.09/s)  LR: 9.995e-04  Data: 0.010 (0.013)
Train: 9 [ 750/1251 ( 60%)]  Loss: 5.170 (4.87)  Time: 0.721s, 1420.45/s  (0.702s, 1459.18/s)  LR: 9.995e-04  Data: 0.010 (0.013)
Train: 9 [ 800/1251 ( 64%)]  Loss: 4.776 (4.86)  Time: 0.704s, 1454.22/s  (0.701s, 1460.26/s)  LR: 9.995e-04  Data: 0.010 (0.013)
Train: 9 [ 850/1251 ( 68%)]  Loss: 4.665 (4.85)  Time: 0.724s, 1414.51/s  (0.701s, 1460.66/s)  LR: 9.995e-04  Data: 0.013 (0.013)
Train: 9 [ 900/1251 ( 72%)]  Loss: 4.963 (4.86)  Time: 0.693s, 1478.60/s  (0.701s, 1460.74/s)  LR: 9.995e-04  Data: 0.010 (0.013)
Train: 9 [ 950/1251 ( 76%)]  Loss: 5.185 (4.88)  Time: 0.739s, 1386.58/s  (0.701s, 1460.99/s)  LR: 9.995e-04  Data: 0.010 (0.012)
Train: 9 [1000/1251 ( 80%)]  Loss: 4.932 (4.88)  Time: 0.704s, 1454.94/s  (0.701s, 1460.99/s)  LR: 9.995e-04  Data: 0.010 (0.012)
Train: 9 [1050/1251 ( 84%)]  Loss: 4.917 (4.88)  Time: 0.666s, 1537.75/s  (0.701s, 1461.34/s)  LR: 9.995e-04  Data: 0.011 (0.012)
Train: 9 [1100/1251 ( 88%)]  Loss: 4.730 (4.87)  Time: 0.700s, 1463.73/s  (0.701s, 1461.71/s)  LR: 9.995e-04  Data: 0.010 (0.012)
Train: 9 [1150/1251 ( 92%)]  Loss: 5.016 (4.88)  Time: 0.673s, 1522.09/s  (0.700s, 1462.01/s)  LR: 9.995e-04  Data: 0.010 (0.012)
Train: 9 [1200/1251 ( 96%)]  Loss: 4.719 (4.87)  Time: 0.711s, 1439.48/s  (0.701s, 1461.78/s)  LR: 9.995e-04  Data: 0.010 (0.012)
Train: 9 [1250/1251 (100%)]  Loss: 4.854 (4.87)  Time: 0.696s, 1470.81/s  (0.701s, 1461.75/s)  LR: 9.995e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.567 (1.567)  Loss:  2.0742 (2.0742)  Acc@1: 62.5000 (62.5000)  Acc@5: 86.2305 (86.2305)
Test: [  48/48]  Time: 0.136 (0.577)  Loss:  1.8496 (2.6881)  Acc@1: 65.8019 (45.0060)  Acc@5: 82.4292 (71.8440)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-9.pth.tar', 45.00600001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-8.pth.tar', 40.45999998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-7.pth.tar', 37.68800004882812)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-6.pth.tar', 34.561999997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-5.pth.tar', 25.982000090332033)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-4.pth.tar', 20.366000023193358)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-3.pth.tar', 15.08000006225586)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-2.pth.tar', 8.487999958496093)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-1.pth.tar', 2.262000018005371)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-0.pth.tar', 0.18600000118255616)

Train: 10 [   0/1251 (  0%)]  Loss: 4.839 (4.84)  Time: 2.264s,  452.26/s  (2.264s,  452.26/s)  LR: 9.993e-04  Data: 1.605 (1.605)
Train: 10 [  50/1251 (  4%)]  Loss: 4.722 (4.78)  Time: 0.721s, 1421.15/s  (0.728s, 1406.59/s)  LR: 9.993e-04  Data: 0.009 (0.046)
Train: 10 [ 100/1251 (  8%)]  Loss: 4.844 (4.80)  Time: 0.703s, 1456.70/s  (0.711s, 1440.32/s)  LR: 9.993e-04  Data: 0.010 (0.028)
Train: 10 [ 150/1251 ( 12%)]  Loss: 4.610 (4.75)  Time: 0.707s, 1447.49/s  (0.709s, 1443.68/s)  LR: 9.993e-04  Data: 0.010 (0.022)
Train: 10 [ 200/1251 ( 16%)]  Loss: 4.934 (4.79)  Time: 0.692s, 1480.81/s  (0.706s, 1449.66/s)  LR: 9.993e-04  Data: 0.009 (0.019)
Train: 10 [ 250/1251 ( 20%)]  Loss: 4.635 (4.76)  Time: 0.676s, 1514.79/s  (0.706s, 1450.76/s)  LR: 9.993e-04  Data: 0.014 (0.017)
Train: 10 [ 300/1251 ( 24%)]  Loss: 4.704 (4.76)  Time: 0.674s, 1518.67/s  (0.705s, 1452.11/s)  LR: 9.993e-04  Data: 0.010 (0.016)
Train: 10 [ 350/1251 ( 28%)]  Loss: 4.932 (4.78)  Time: 0.675s, 1517.05/s  (0.705s, 1453.20/s)  LR: 9.993e-04  Data: 0.010 (0.015)
Train: 10 [ 400/1251 ( 32%)]  Loss: 4.903 (4.79)  Time: 0.695s, 1473.89/s  (0.703s, 1455.65/s)  LR: 9.993e-04  Data: 0.010 (0.015)
Train: 10 [ 450/1251 ( 36%)]  Loss: 4.836 (4.80)  Time: 0.700s, 1462.71/s  (0.702s, 1459.25/s)  LR: 9.993e-04  Data: 0.009 (0.014)
Train: 10 [ 500/1251 ( 40%)]  Loss: 5.116 (4.82)  Time: 0.674s, 1518.50/s  (0.701s, 1460.35/s)  LR: 9.993e-04  Data: 0.010 (0.014)
Train: 10 [ 550/1251 ( 44%)]  Loss: 4.970 (4.84)  Time: 0.703s, 1456.99/s  (0.701s, 1460.57/s)  LR: 9.993e-04  Data: 0.009 (0.013)
Train: 10 [ 600/1251 ( 48%)]  Loss: 5.077 (4.86)  Time: 0.727s, 1408.73/s  (0.701s, 1460.60/s)  LR: 9.993e-04  Data: 0.012 (0.013)
Train: 10 [ 650/1251 ( 52%)]  Loss: 4.739 (4.85)  Time: 0.723s, 1416.20/s  (0.702s, 1459.08/s)  LR: 9.993e-04  Data: 0.009 (0.013)
Train: 10 [ 700/1251 ( 56%)]  Loss: 4.720 (4.84)  Time: 0.732s, 1399.76/s  (0.702s, 1457.97/s)  LR: 9.993e-04  Data: 0.010 (0.013)
Train: 10 [ 750/1251 ( 60%)]  Loss: 4.622 (4.83)  Time: 0.718s, 1425.91/s  (0.703s, 1456.63/s)  LR: 9.993e-04  Data: 0.009 (0.013)
Train: 10 [ 800/1251 ( 64%)]  Loss: 4.866 (4.83)  Time: 0.707s, 1448.39/s  (0.702s, 1459.26/s)  LR: 9.993e-04  Data: 0.009 (0.013)
Train: 10 [ 850/1251 ( 68%)]  Loss: 4.793 (4.83)  Time: 0.707s, 1449.15/s  (0.701s, 1459.85/s)  LR: 9.993e-04  Data: 0.010 (0.012)
Train: 10 [ 900/1251 ( 72%)]  Loss: 4.674 (4.82)  Time: 0.709s, 1443.90/s  (0.701s, 1460.29/s)  LR: 9.993e-04  Data: 0.010 (0.012)
Train: 10 [ 950/1251 ( 76%)]  Loss: 4.602 (4.81)  Time: 0.709s, 1444.22/s  (0.701s, 1461.37/s)  LR: 9.993e-04  Data: 0.010 (0.012)
Train: 10 [1000/1251 ( 80%)]  Loss: 4.940 (4.81)  Time: 0.677s, 1513.11/s  (0.700s, 1462.16/s)  LR: 9.993e-04  Data: 0.010 (0.012)
Train: 10 [1050/1251 ( 84%)]  Loss: 4.797 (4.81)  Time: 0.709s, 1444.98/s  (0.700s, 1462.78/s)  LR: 9.993e-04  Data: 0.010 (0.012)
Train: 10 [1100/1251 ( 88%)]  Loss: 4.743 (4.81)  Time: 0.694s, 1476.30/s  (0.700s, 1463.37/s)  LR: 9.993e-04  Data: 0.012 (0.012)
Train: 10 [1150/1251 ( 92%)]  Loss: 4.844 (4.81)  Time: 0.762s, 1344.56/s  (0.700s, 1462.59/s)  LR: 9.993e-04  Data: 0.010 (0.012)
Train: 10 [1200/1251 ( 96%)]  Loss: 4.606 (4.80)  Time: 0.744s, 1376.26/s  (0.700s, 1462.75/s)  LR: 9.993e-04  Data: 0.010 (0.012)
Train: 10 [1250/1251 (100%)]  Loss: 4.761 (4.80)  Time: 0.654s, 1566.78/s  (0.700s, 1462.15/s)  LR: 9.993e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.581 (1.581)  Loss:  1.6270 (1.6270)  Acc@1: 70.7031 (70.7031)  Acc@5: 88.9648 (88.9648)
Test: [  48/48]  Time: 0.137 (0.576)  Loss:  1.5693 (2.4433)  Acc@1: 69.6934 (49.5620)  Acc@5: 85.6132 (75.7020)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-10.pth.tar', 49.56200002197266)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-9.pth.tar', 45.00600001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-8.pth.tar', 40.45999998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-7.pth.tar', 37.68800004882812)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-6.pth.tar', 34.561999997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-5.pth.tar', 25.982000090332033)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-4.pth.tar', 20.366000023193358)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-3.pth.tar', 15.08000006225586)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-2.pth.tar', 8.487999958496093)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-1.pth.tar', 2.262000018005371)

Train: 11 [   0/1251 (  0%)]  Loss: 4.391 (4.39)  Time: 2.295s,  446.16/s  (2.295s,  446.16/s)  LR: 9.992e-04  Data: 1.639 (1.639)
Train: 11 [  50/1251 (  4%)]  Loss: 4.717 (4.55)  Time: 0.708s, 1445.65/s  (0.736s, 1391.38/s)  LR: 9.992e-04  Data: 0.010 (0.048)
Train: 11 [ 100/1251 (  8%)]  Loss: 4.736 (4.61)  Time: 0.674s, 1519.51/s  (0.715s, 1432.52/s)  LR: 9.992e-04  Data: 0.010 (0.029)
Train: 11 [ 150/1251 ( 12%)]  Loss: 4.503 (4.59)  Time: 0.701s, 1460.05/s  (0.707s, 1448.72/s)  LR: 9.992e-04  Data: 0.009 (0.023)
Train: 11 [ 200/1251 ( 16%)]  Loss: 4.641 (4.60)  Time: 0.752s, 1362.48/s  (0.705s, 1453.08/s)  LR: 9.992e-04  Data: 0.014 (0.020)
Train: 11 [ 250/1251 ( 20%)]  Loss: 4.693 (4.61)  Time: 0.681s, 1503.08/s  (0.702s, 1458.84/s)  LR: 9.992e-04  Data: 0.009 (0.018)
Train: 11 [ 300/1251 ( 24%)]  Loss: 4.574 (4.61)  Time: 0.671s, 1526.48/s  (0.701s, 1459.97/s)  LR: 9.992e-04  Data: 0.010 (0.016)
Train: 11 [ 350/1251 ( 28%)]  Loss: 4.720 (4.62)  Time: 0.668s, 1533.96/s  (0.702s, 1457.79/s)  LR: 9.992e-04  Data: 0.010 (0.016)
Train: 11 [ 400/1251 ( 32%)]  Loss: 4.693 (4.63)  Time: 0.706s, 1449.98/s  (0.702s, 1459.09/s)  LR: 9.992e-04  Data: 0.010 (0.015)
Train: 11 [ 450/1251 ( 36%)]  Loss: 4.512 (4.62)  Time: 0.672s, 1523.78/s  (0.702s, 1459.64/s)  LR: 9.992e-04  Data: 0.009 (0.014)
Train: 11 [ 500/1251 ( 40%)]  Loss: 4.865 (4.64)  Time: 0.732s, 1399.69/s  (0.701s, 1460.57/s)  LR: 9.992e-04  Data: 0.009 (0.014)
Train: 11 [ 550/1251 ( 44%)]  Loss: 4.480 (4.63)  Time: 0.713s, 1436.06/s  (0.701s, 1461.46/s)  LR: 9.992e-04  Data: 0.010 (0.014)
Train: 11 [ 600/1251 ( 48%)]  Loss: 4.795 (4.64)  Time: 0.727s, 1409.09/s  (0.701s, 1461.68/s)  LR: 9.992e-04  Data: 0.014 (0.013)
Train: 11 [ 650/1251 ( 52%)]  Loss: 4.602 (4.64)  Time: 0.716s, 1429.33/s  (0.700s, 1462.13/s)  LR: 9.992e-04  Data: 0.010 (0.013)
Train: 11 [ 700/1251 ( 56%)]  Loss: 4.576 (4.63)  Time: 0.705s, 1451.61/s  (0.701s, 1461.51/s)  LR: 9.992e-04  Data: 0.010 (0.013)
Train: 11 [ 750/1251 ( 60%)]  Loss: 5.025 (4.66)  Time: 0.721s, 1420.07/s  (0.701s, 1461.10/s)  LR: 9.992e-04  Data: 0.015 (0.013)
Train: 11 [ 800/1251 ( 64%)]  Loss: 4.485 (4.65)  Time: 0.675s, 1517.03/s  (0.700s, 1461.92/s)  LR: 9.992e-04  Data: 0.010 (0.013)
Train: 11 [ 850/1251 ( 68%)]  Loss: 4.696 (4.65)  Time: 0.721s, 1420.22/s  (0.701s, 1461.79/s)  LR: 9.992e-04  Data: 0.010 (0.012)
Train: 11 [ 900/1251 ( 72%)]  Loss: 4.408 (4.64)  Time: 0.685s, 1494.29/s  (0.700s, 1461.82/s)  LR: 9.992e-04  Data: 0.009 (0.012)
Train: 11 [ 950/1251 ( 76%)]  Loss: 4.896 (4.65)  Time: 0.671s, 1525.82/s  (0.700s, 1462.68/s)  LR: 9.992e-04  Data: 0.009 (0.012)
Train: 11 [1000/1251 ( 80%)]  Loss: 4.737 (4.65)  Time: 0.703s, 1456.29/s  (0.700s, 1463.03/s)  LR: 9.992e-04  Data: 0.009 (0.012)
Train: 11 [1050/1251 ( 84%)]  Loss: 4.828 (4.66)  Time: 0.771s, 1327.53/s  (0.700s, 1462.22/s)  LR: 9.992e-04  Data: 0.010 (0.012)
Train: 11 [1100/1251 ( 88%)]  Loss: 4.553 (4.66)  Time: 0.707s, 1448.66/s  (0.700s, 1462.31/s)  LR: 9.992e-04  Data: 0.009 (0.012)
Train: 11 [1150/1251 ( 92%)]  Loss: 4.654 (4.66)  Time: 0.687s, 1491.32/s  (0.700s, 1463.02/s)  LR: 9.992e-04  Data: 0.010 (0.012)
Train: 11 [1200/1251 ( 96%)]  Loss: 4.394 (4.65)  Time: 0.671s, 1527.05/s  (0.700s, 1462.91/s)  LR: 9.992e-04  Data: 0.009 (0.012)
Train: 11 [1250/1251 (100%)]  Loss: 4.628 (4.65)  Time: 0.688s, 1487.99/s  (0.700s, 1463.03/s)  LR: 9.992e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.568 (1.568)  Loss:  1.3682 (1.3682)  Acc@1: 74.0234 (74.0234)  Acc@5: 92.5781 (92.5781)
Test: [  48/48]  Time: 0.138 (0.593)  Loss:  1.6670 (2.4194)  Acc@1: 70.1651 (50.5760)  Acc@5: 85.8491 (76.9080)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-11.pth.tar', 50.57599996826172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-10.pth.tar', 49.56200002197266)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-9.pth.tar', 45.00600001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-8.pth.tar', 40.45999998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-7.pth.tar', 37.68800004882812)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-6.pth.tar', 34.561999997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-5.pth.tar', 25.982000090332033)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-4.pth.tar', 20.366000023193358)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-3.pth.tar', 15.08000006225586)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-2.pth.tar', 8.487999958496093)

Train: 12 [   0/1251 (  0%)]  Loss: 4.714 (4.71)  Time: 2.276s,  450.00/s  (2.276s,  450.00/s)  LR: 9.990e-04  Data: 1.660 (1.660)
Train: 12 [  50/1251 (  4%)]  Loss: 4.436 (4.58)  Time: 0.673s, 1522.68/s  (0.729s, 1403.80/s)  LR: 9.990e-04  Data: 0.010 (0.050)
Train: 12 [ 100/1251 (  8%)]  Loss: 4.483 (4.54)  Time: 0.668s, 1533.26/s  (0.711s, 1440.08/s)  LR: 9.990e-04  Data: 0.013 (0.030)
Train: 12 [ 150/1251 ( 12%)]  Loss: 4.753 (4.60)  Time: 0.671s, 1525.32/s  (0.706s, 1449.48/s)  LR: 9.990e-04  Data: 0.010 (0.024)
Train: 12 [ 200/1251 ( 16%)]  Loss: 4.640 (4.61)  Time: 0.700s, 1462.00/s  (0.706s, 1449.96/s)  LR: 9.990e-04  Data: 0.009 (0.020)
Train: 12 [ 250/1251 ( 20%)]  Loss: 4.754 (4.63)  Time: 0.706s, 1449.87/s  (0.705s, 1452.70/s)  LR: 9.990e-04  Data: 0.010 (0.018)
Train: 12 [ 300/1251 ( 24%)]  Loss: 4.914 (4.67)  Time: 0.709s, 1445.07/s  (0.704s, 1455.23/s)  LR: 9.990e-04  Data: 0.009 (0.017)
Train: 12 [ 350/1251 ( 28%)]  Loss: 4.777 (4.68)  Time: 0.672s, 1523.82/s  (0.703s, 1456.49/s)  LR: 9.990e-04  Data: 0.010 (0.016)
Train: 12 [ 400/1251 ( 32%)]  Loss: 4.559 (4.67)  Time: 0.702s, 1458.29/s  (0.704s, 1455.02/s)  LR: 9.990e-04  Data: 0.010 (0.015)
Train: 12 [ 450/1251 ( 36%)]  Loss: 4.556 (4.66)  Time: 0.703s, 1455.71/s  (0.704s, 1454.49/s)  LR: 9.990e-04  Data: 0.010 (0.015)
Train: 12 [ 500/1251 ( 40%)]  Loss: 4.554 (4.65)  Time: 0.707s, 1449.06/s  (0.704s, 1453.89/s)  LR: 9.990e-04  Data: 0.011 (0.014)
Train: 12 [ 550/1251 ( 44%)]  Loss: 4.680 (4.65)  Time: 0.673s, 1521.37/s  (0.704s, 1454.92/s)  LR: 9.990e-04  Data: 0.010 (0.014)
Train: 12 [ 600/1251 ( 48%)]  Loss: 4.785 (4.66)  Time: 0.692s, 1480.36/s  (0.703s, 1455.68/s)  LR: 9.990e-04  Data: 0.011 (0.014)
Train: 12 [ 650/1251 ( 52%)]  Loss: 4.773 (4.67)  Time: 0.687s, 1491.28/s  (0.703s, 1456.66/s)  LR: 9.990e-04  Data: 0.010 (0.013)
Train: 12 [ 700/1251 ( 56%)]  Loss: 4.451 (4.66)  Time: 0.722s, 1418.68/s  (0.703s, 1456.97/s)  LR: 9.990e-04  Data: 0.009 (0.013)
Train: 12 [ 750/1251 ( 60%)]  Loss: 4.580 (4.65)  Time: 0.681s, 1502.76/s  (0.703s, 1457.35/s)  LR: 9.990e-04  Data: 0.009 (0.013)
Train: 12 [ 800/1251 ( 64%)]  Loss: 4.850 (4.66)  Time: 0.758s, 1350.47/s  (0.702s, 1458.08/s)  LR: 9.990e-04  Data: 0.014 (0.013)
Train: 12 [ 850/1251 ( 68%)]  Loss: 4.554 (4.66)  Time: 0.675s, 1517.40/s  (0.702s, 1458.30/s)  LR: 9.990e-04  Data: 0.010 (0.013)
Train: 12 [ 900/1251 ( 72%)]  Loss: 4.379 (4.64)  Time: 0.700s, 1462.42/s  (0.702s, 1458.18/s)  LR: 9.990e-04  Data: 0.010 (0.012)
Train: 12 [ 950/1251 ( 76%)]  Loss: 4.602 (4.64)  Time: 0.731s, 1401.08/s  (0.702s, 1458.63/s)  LR: 9.990e-04  Data: 0.010 (0.012)
Train: 12 [1000/1251 ( 80%)]  Loss: 4.796 (4.65)  Time: 0.669s, 1531.71/s  (0.702s, 1458.80/s)  LR: 9.990e-04  Data: 0.010 (0.012)
Train: 12 [1050/1251 ( 84%)]  Loss: 4.474 (4.64)  Time: 0.759s, 1349.82/s  (0.702s, 1459.34/s)  LR: 9.990e-04  Data: 0.010 (0.012)
Train: 12 [1100/1251 ( 88%)]  Loss: 4.227 (4.62)  Time: 0.696s, 1472.19/s  (0.702s, 1459.08/s)  LR: 9.990e-04  Data: 0.010 (0.012)
Train: 12 [1150/1251 ( 92%)]  Loss: 4.876 (4.63)  Time: 0.675s, 1518.11/s  (0.702s, 1459.32/s)  LR: 9.990e-04  Data: 0.009 (0.012)
Train: 12 [1200/1251 ( 96%)]  Loss: 4.836 (4.64)  Time: 0.706s, 1450.51/s  (0.702s, 1459.44/s)  LR: 9.990e-04  Data: 0.011 (0.012)
Train: 12 [1250/1251 (100%)]  Loss: 4.431 (4.63)  Time: 0.701s, 1460.24/s  (0.702s, 1458.67/s)  LR: 9.990e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.449 (1.449)  Loss:  1.7480 (1.7480)  Acc@1: 70.8984 (70.8984)  Acc@5: 90.2344 (90.2344)
Test: [  48/48]  Time: 0.136 (0.580)  Loss:  1.7432 (2.5201)  Acc@1: 72.0519 (51.6220)  Acc@5: 87.6179 (77.1440)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-12.pth.tar', 51.62200001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-11.pth.tar', 50.57599996826172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-10.pth.tar', 49.56200002197266)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-9.pth.tar', 45.00600001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-8.pth.tar', 40.45999998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-7.pth.tar', 37.68800004882812)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-6.pth.tar', 34.561999997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-5.pth.tar', 25.982000090332033)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-4.pth.tar', 20.366000023193358)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-3.pth.tar', 15.08000006225586)

Train: 13 [   0/1251 (  0%)]  Loss: 4.764 (4.76)  Time: 2.242s,  456.68/s  (2.242s,  456.68/s)  LR: 9.989e-04  Data: 1.626 (1.626)
Train: 13 [  50/1251 (  4%)]  Loss: 4.733 (4.75)  Time: 0.670s, 1529.35/s  (0.727s, 1408.98/s)  LR: 9.989e-04  Data: 0.010 (0.047)
Train: 13 [ 100/1251 (  8%)]  Loss: 4.566 (4.69)  Time: 0.722s, 1418.88/s  (0.714s, 1434.87/s)  LR: 9.989e-04  Data: 0.009 (0.029)
Train: 13 [ 150/1251 ( 12%)]  Loss: 4.325 (4.60)  Time: 0.713s, 1435.82/s  (0.709s, 1443.39/s)  LR: 9.989e-04  Data: 0.009 (0.023)
Train: 13 [ 200/1251 ( 16%)]  Loss: 4.689 (4.62)  Time: 0.677s, 1512.25/s  (0.705s, 1452.28/s)  LR: 9.989e-04  Data: 0.010 (0.020)
Train: 13 [ 250/1251 ( 20%)]  Loss: 4.601 (4.61)  Time: 0.708s, 1445.52/s  (0.705s, 1453.37/s)  LR: 9.989e-04  Data: 0.009 (0.018)
Train: 13 [ 300/1251 ( 24%)]  Loss: 4.531 (4.60)  Time: 0.760s, 1348.00/s  (0.703s, 1456.34/s)  LR: 9.989e-04  Data: 0.010 (0.016)
Train: 13 [ 350/1251 ( 28%)]  Loss: 4.858 (4.63)  Time: 0.666s, 1537.21/s  (0.703s, 1455.67/s)  LR: 9.989e-04  Data: 0.010 (0.016)
Train: 13 [ 400/1251 ( 32%)]  Loss: 4.541 (4.62)  Time: 0.678s, 1510.76/s  (0.704s, 1455.03/s)  LR: 9.989e-04  Data: 0.009 (0.015)
Train: 13 [ 450/1251 ( 36%)]  Loss: 4.573 (4.62)  Time: 0.685s, 1494.91/s  (0.704s, 1455.50/s)  LR: 9.989e-04  Data: 0.011 (0.014)
Train: 13 [ 500/1251 ( 40%)]  Loss: 4.567 (4.61)  Time: 0.685s, 1494.95/s  (0.703s, 1456.81/s)  LR: 9.989e-04  Data: 0.029 (0.014)
Train: 13 [ 550/1251 ( 44%)]  Loss: 4.680 (4.62)  Time: 0.677s, 1511.47/s  (0.703s, 1456.67/s)  LR: 9.989e-04  Data: 0.010 (0.014)
Train: 13 [ 600/1251 ( 48%)]  Loss: 4.350 (4.60)  Time: 0.707s, 1447.80/s  (0.703s, 1456.95/s)  LR: 9.989e-04  Data: 0.009 (0.013)
Train: 13 [ 650/1251 ( 52%)]  Loss: 4.353 (4.58)  Time: 0.686s, 1493.43/s  (0.703s, 1456.72/s)  LR: 9.989e-04  Data: 0.010 (0.013)
Train: 13 [ 700/1251 ( 56%)]  Loss: 4.753 (4.59)  Time: 0.672s, 1524.67/s  (0.703s, 1456.13/s)  LR: 9.989e-04  Data: 0.013 (0.013)
Train: 13 [ 750/1251 ( 60%)]  Loss: 4.786 (4.60)  Time: 0.673s, 1522.07/s  (0.703s, 1456.56/s)  LR: 9.989e-04  Data: 0.010 (0.013)
Train: 13 [ 800/1251 ( 64%)]  Loss: 4.221 (4.58)  Time: 0.680s, 1505.71/s  (0.702s, 1457.89/s)  LR: 9.989e-04  Data: 0.009 (0.013)
Train: 13 [ 850/1251 ( 68%)]  Loss: 4.884 (4.60)  Time: 0.679s, 1507.24/s  (0.702s, 1458.29/s)  LR: 9.989e-04  Data: 0.010 (0.012)
Train: 13 [ 900/1251 ( 72%)]  Loss: 4.701 (4.60)  Time: 0.700s, 1463.28/s  (0.702s, 1458.63/s)  LR: 9.989e-04  Data: 0.009 (0.012)
Train: 13 [ 950/1251 ( 76%)]  Loss: 4.726 (4.61)  Time: 0.745s, 1374.06/s  (0.702s, 1458.66/s)  LR: 9.989e-04  Data: 0.010 (0.012)
Train: 13 [1000/1251 ( 80%)]  Loss: 4.539 (4.61)  Time: 0.707s, 1448.46/s  (0.702s, 1459.67/s)  LR: 9.989e-04  Data: 0.009 (0.012)
Train: 13 [1050/1251 ( 84%)]  Loss: 4.568 (4.60)  Time: 0.711s, 1439.66/s  (0.702s, 1459.22/s)  LR: 9.989e-04  Data: 0.011 (0.012)
Train: 13 [1100/1251 ( 88%)]  Loss: 4.770 (4.61)  Time: 0.713s, 1436.51/s  (0.702s, 1458.90/s)  LR: 9.989e-04  Data: 0.017 (0.012)
Train: 13 [1150/1251 ( 92%)]  Loss: 4.692 (4.62)  Time: 0.681s, 1504.66/s  (0.702s, 1458.77/s)  LR: 9.989e-04  Data: 0.011 (0.012)
Train: 13 [1200/1251 ( 96%)]  Loss: 4.390 (4.61)  Time: 0.666s, 1538.07/s  (0.702s, 1458.63/s)  LR: 9.989e-04  Data: 0.011 (0.012)
Train: 13 [1250/1251 (100%)]  Loss: 4.140 (4.59)  Time: 0.690s, 1483.06/s  (0.702s, 1458.57/s)  LR: 9.989e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.575 (1.575)  Loss:  1.6328 (1.6328)  Acc@1: 71.6797 (71.6797)  Acc@5: 88.5742 (88.5742)
Test: [  48/48]  Time: 0.136 (0.591)  Loss:  1.5479 (2.3234)  Acc@1: 72.2877 (52.8320)  Acc@5: 89.2689 (78.1160)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-13.pth.tar', 52.831999985351565)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-12.pth.tar', 51.62200001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-11.pth.tar', 50.57599996826172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-10.pth.tar', 49.56200002197266)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-9.pth.tar', 45.00600001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-8.pth.tar', 40.45999998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-7.pth.tar', 37.68800004882812)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-6.pth.tar', 34.561999997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-5.pth.tar', 25.982000090332033)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-4.pth.tar', 20.366000023193358)

Train: 14 [   0/1251 (  0%)]  Loss: 4.563 (4.56)  Time: 2.131s,  480.60/s  (2.131s,  480.60/s)  LR: 9.987e-04  Data: 1.514 (1.514)
Train: 14 [  50/1251 (  4%)]  Loss: 4.571 (4.57)  Time: 0.723s, 1415.53/s  (0.745s, 1374.60/s)  LR: 9.987e-04  Data: 0.009 (0.054)
Train: 14 [ 100/1251 (  8%)]  Loss: 4.525 (4.55)  Time: 0.693s, 1477.53/s  (0.722s, 1417.75/s)  LR: 9.987e-04  Data: 0.011 (0.032)
Train: 14 [ 150/1251 ( 12%)]  Loss: 4.661 (4.58)  Time: 0.698s, 1466.70/s  (0.719s, 1424.11/s)  LR: 9.987e-04  Data: 0.016 (0.025)
Train: 14 [ 200/1251 ( 16%)]  Loss: 4.637 (4.59)  Time: 0.693s, 1477.80/s  (0.714s, 1433.18/s)  LR: 9.987e-04  Data: 0.016 (0.021)
Train: 14 [ 250/1251 ( 20%)]  Loss: 4.566 (4.59)  Time: 0.730s, 1402.30/s  (0.710s, 1441.38/s)  LR: 9.987e-04  Data: 0.011 (0.019)
Train: 14 [ 300/1251 ( 24%)]  Loss: 4.326 (4.55)  Time: 0.695s, 1474.36/s  (0.708s, 1445.64/s)  LR: 9.987e-04  Data: 0.010 (0.018)
Train: 14 [ 350/1251 ( 28%)]  Loss: 4.821 (4.58)  Time: 0.723s, 1416.09/s  (0.706s, 1449.40/s)  LR: 9.987e-04  Data: 0.010 (0.017)
Train: 14 [ 400/1251 ( 32%)]  Loss: 4.636 (4.59)  Time: 0.672s, 1522.84/s  (0.705s, 1452.55/s)  LR: 9.987e-04  Data: 0.009 (0.016)
Train: 14 [ 450/1251 ( 36%)]  Loss: 4.210 (4.55)  Time: 0.703s, 1457.02/s  (0.704s, 1453.85/s)  LR: 9.987e-04  Data: 0.009 (0.015)
Train: 14 [ 500/1251 ( 40%)]  Loss: 4.609 (4.56)  Time: 0.724s, 1414.60/s  (0.704s, 1454.93/s)  LR: 9.987e-04  Data: 0.009 (0.015)
Train: 14 [ 550/1251 ( 44%)]  Loss: 4.792 (4.58)  Time: 0.701s, 1459.79/s  (0.704s, 1454.16/s)  LR: 9.987e-04  Data: 0.010 (0.014)
Train: 14 [ 600/1251 ( 48%)]  Loss: 4.627 (4.58)  Time: 0.672s, 1524.44/s  (0.703s, 1455.71/s)  LR: 9.987e-04  Data: 0.013 (0.014)
Train: 14 [ 650/1251 ( 52%)]  Loss: 4.662 (4.59)  Time: 0.730s, 1403.02/s  (0.703s, 1457.03/s)  LR: 9.987e-04  Data: 0.009 (0.014)
Train: 14 [ 700/1251 ( 56%)]  Loss: 4.565 (4.58)  Time: 0.700s, 1462.44/s  (0.703s, 1457.02/s)  LR: 9.987e-04  Data: 0.010 (0.013)
Train: 14 [ 750/1251 ( 60%)]  Loss: 4.821 (4.60)  Time: 0.709s, 1444.82/s  (0.702s, 1458.30/s)  LR: 9.987e-04  Data: 0.009 (0.013)
Train: 14 [ 800/1251 ( 64%)]  Loss: 4.194 (4.58)  Time: 0.705s, 1453.08/s  (0.702s, 1458.92/s)  LR: 9.987e-04  Data: 0.010 (0.013)
Train: 14 [ 850/1251 ( 68%)]  Loss: 4.607 (4.58)  Time: 0.712s, 1438.04/s  (0.702s, 1458.39/s)  LR: 9.987e-04  Data: 0.010 (0.013)
Train: 14 [ 900/1251 ( 72%)]  Loss: 4.940 (4.60)  Time: 0.726s, 1409.74/s  (0.702s, 1457.88/s)  LR: 9.987e-04  Data: 0.009 (0.013)
Train: 14 [ 950/1251 ( 76%)]  Loss: 4.781 (4.61)  Time: 0.743s, 1377.82/s  (0.702s, 1458.16/s)  LR: 9.987e-04  Data: 0.009 (0.013)
Train: 14 [1000/1251 ( 80%)]  Loss: 4.406 (4.60)  Time: 0.701s, 1460.65/s  (0.702s, 1458.59/s)  LR: 9.987e-04  Data: 0.010 (0.012)
Train: 14 [1050/1251 ( 84%)]  Loss: 4.447 (4.59)  Time: 0.704s, 1454.31/s  (0.702s, 1459.67/s)  LR: 9.987e-04  Data: 0.009 (0.012)
Train: 14 [1100/1251 ( 88%)]  Loss: 4.253 (4.57)  Time: 0.666s, 1538.26/s  (0.701s, 1460.88/s)  LR: 9.987e-04  Data: 0.010 (0.012)
Train: 14 [1150/1251 ( 92%)]  Loss: 4.611 (4.58)  Time: 0.674s, 1518.66/s  (0.701s, 1461.60/s)  LR: 9.987e-04  Data: 0.010 (0.012)
Train: 14 [1200/1251 ( 96%)]  Loss: 4.902 (4.59)  Time: 0.728s, 1407.05/s  (0.700s, 1462.11/s)  LR: 9.987e-04  Data: 0.009 (0.012)
Train: 14 [1250/1251 (100%)]  Loss: 4.529 (4.59)  Time: 0.666s, 1538.68/s  (0.701s, 1461.46/s)  LR: 9.987e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.552 (1.552)  Loss:  1.4297 (1.4297)  Acc@1: 74.2188 (74.2188)  Acc@5: 89.8438 (89.8438)
Test: [  48/48]  Time: 0.135 (0.572)  Loss:  1.4639 (2.1891)  Acc@1: 74.1745 (55.2820)  Acc@5: 88.4434 (80.1780)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-14.pth.tar', 55.282000029296874)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-13.pth.tar', 52.831999985351565)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-12.pth.tar', 51.62200001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-11.pth.tar', 50.57599996826172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-10.pth.tar', 49.56200002197266)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-9.pth.tar', 45.00600001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-8.pth.tar', 40.45999998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-7.pth.tar', 37.68800004882812)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-6.pth.tar', 34.561999997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-5.pth.tar', 25.982000090332033)

Train: 15 [   0/1251 (  0%)]  Loss: 4.509 (4.51)  Time: 2.313s,  442.74/s  (2.313s,  442.74/s)  LR: 9.985e-04  Data: 1.699 (1.699)
Train: 15 [  50/1251 (  4%)]  Loss: 4.002 (4.26)  Time: 0.758s, 1350.63/s  (0.732s, 1398.11/s)  LR: 9.985e-04  Data: 0.009 (0.049)
Train: 15 [ 100/1251 (  8%)]  Loss: 4.644 (4.38)  Time: 0.716s, 1430.41/s  (0.715s, 1433.16/s)  LR: 9.985e-04  Data: 0.011 (0.030)
Train: 15 [ 150/1251 ( 12%)]  Loss: 4.151 (4.33)  Time: 0.704s, 1455.13/s  (0.707s, 1448.83/s)  LR: 9.985e-04  Data: 0.009 (0.023)
Train: 15 [ 200/1251 ( 16%)]  Loss: 4.747 (4.41)  Time: 0.674s, 1520.40/s  (0.703s, 1457.22/s)  LR: 9.985e-04  Data: 0.010 (0.020)
Train: 15 [ 250/1251 ( 20%)]  Loss: 4.588 (4.44)  Time: 0.673s, 1521.87/s  (0.701s, 1461.22/s)  LR: 9.985e-04  Data: 0.010 (0.018)
Train: 15 [ 300/1251 ( 24%)]  Loss: 4.732 (4.48)  Time: 0.736s, 1391.64/s  (0.702s, 1459.12/s)  LR: 9.985e-04  Data: 0.022 (0.017)
Train: 15 [ 350/1251 ( 28%)]  Loss: 4.507 (4.48)  Time: 0.712s, 1438.32/s  (0.703s, 1457.40/s)  LR: 9.985e-04  Data: 0.010 (0.016)
Train: 15 [ 400/1251 ( 32%)]  Loss: 4.176 (4.45)  Time: 0.703s, 1457.15/s  (0.702s, 1457.96/s)  LR: 9.985e-04  Data: 0.009 (0.015)
Train: 15 [ 450/1251 ( 36%)]  Loss: 4.481 (4.45)  Time: 0.706s, 1449.84/s  (0.703s, 1457.30/s)  LR: 9.985e-04  Data: 0.011 (0.015)
Train: 15 [ 500/1251 ( 40%)]  Loss: 4.452 (4.45)  Time: 0.699s, 1465.87/s  (0.702s, 1457.73/s)  LR: 9.985e-04  Data: 0.015 (0.014)
Train: 15 [ 550/1251 ( 44%)]  Loss: 4.424 (4.45)  Time: 0.667s, 1534.98/s  (0.703s, 1457.47/s)  LR: 9.985e-04  Data: 0.011 (0.014)
Train: 15 [ 600/1251 ( 48%)]  Loss: 4.461 (4.45)  Time: 0.679s, 1508.72/s  (0.703s, 1457.42/s)  LR: 9.985e-04  Data: 0.009 (0.013)
Train: 15 [ 650/1251 ( 52%)]  Loss: 4.555 (4.46)  Time: 0.709s, 1443.77/s  (0.702s, 1458.14/s)  LR: 9.985e-04  Data: 0.009 (0.013)
Train: 15 [ 700/1251 ( 56%)]  Loss: 4.780 (4.48)  Time: 0.702s, 1458.67/s  (0.702s, 1457.92/s)  LR: 9.985e-04  Data: 0.010 (0.013)
Train: 15 [ 750/1251 ( 60%)]  Loss: 4.438 (4.48)  Time: 0.710s, 1442.86/s  (0.703s, 1456.81/s)  LR: 9.985e-04  Data: 0.010 (0.013)
Train: 15 [ 800/1251 ( 64%)]  Loss: 4.593 (4.48)  Time: 0.718s, 1426.62/s  (0.703s, 1456.87/s)  LR: 9.985e-04  Data: 0.010 (0.013)
Train: 15 [ 850/1251 ( 68%)]  Loss: 4.411 (4.48)  Time: 0.717s, 1428.66/s  (0.703s, 1457.14/s)  LR: 9.985e-04  Data: 0.010 (0.013)
Train: 15 [ 900/1251 ( 72%)]  Loss: 4.666 (4.49)  Time: 0.715s, 1431.76/s  (0.703s, 1457.57/s)  LR: 9.985e-04  Data: 0.010 (0.012)
Train: 15 [ 950/1251 ( 76%)]  Loss: 4.312 (4.48)  Time: 0.729s, 1405.46/s  (0.703s, 1457.37/s)  LR: 9.985e-04  Data: 0.009 (0.012)
Train: 15 [1000/1251 ( 80%)]  Loss: 4.532 (4.48)  Time: 0.704s, 1454.24/s  (0.702s, 1458.39/s)  LR: 9.985e-04  Data: 0.009 (0.012)
Train: 15 [1050/1251 ( 84%)]  Loss: 4.493 (4.48)  Time: 0.778s, 1316.71/s  (0.702s, 1457.84/s)  LR: 9.985e-04  Data: 0.010 (0.012)
Train: 15 [1100/1251 ( 88%)]  Loss: 4.450 (4.48)  Time: 0.677s, 1511.82/s  (0.702s, 1457.83/s)  LR: 9.985e-04  Data: 0.009 (0.012)
Train: 15 [1150/1251 ( 92%)]  Loss: 4.933 (4.50)  Time: 0.674s, 1519.93/s  (0.702s, 1458.59/s)  LR: 9.985e-04  Data: 0.009 (0.012)
Train: 15 [1200/1251 ( 96%)]  Loss: 4.731 (4.51)  Time: 0.681s, 1502.79/s  (0.702s, 1459.20/s)  LR: 9.985e-04  Data: 0.011 (0.012)
Train: 15 [1250/1251 (100%)]  Loss: 4.593 (4.51)  Time: 0.662s, 1547.28/s  (0.702s, 1459.69/s)  LR: 9.985e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.549 (1.549)  Loss:  1.4512 (1.4512)  Acc@1: 74.9023 (74.9023)  Acc@5: 90.6250 (90.6250)
Test: [  48/48]  Time: 0.136 (0.588)  Loss:  1.5156 (2.1928)  Acc@1: 72.5236 (55.3340)  Acc@5: 89.6226 (79.9540)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-15.pth.tar', 55.33400008789062)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-14.pth.tar', 55.282000029296874)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-13.pth.tar', 52.831999985351565)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-12.pth.tar', 51.62200001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-11.pth.tar', 50.57599996826172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-10.pth.tar', 49.56200002197266)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-9.pth.tar', 45.00600001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-8.pth.tar', 40.45999998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-7.pth.tar', 37.68800004882812)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-6.pth.tar', 34.561999997558594)

Train: 16 [   0/1251 (  0%)]  Loss: 4.242 (4.24)  Time: 2.258s,  453.46/s  (2.258s,  453.46/s)  LR: 9.983e-04  Data: 1.591 (1.591)
Train: 16 [  50/1251 (  4%)]  Loss: 4.577 (4.41)  Time: 0.711s, 1440.62/s  (0.741s, 1382.35/s)  LR: 9.983e-04  Data: 0.011 (0.048)
Train: 16 [ 100/1251 (  8%)]  Loss: 4.455 (4.42)  Time: 0.732s, 1399.05/s  (0.720s, 1422.16/s)  LR: 9.983e-04  Data: 0.009 (0.029)
Train: 16 [ 150/1251 ( 12%)]  Loss: 4.503 (4.44)  Time: 0.703s, 1455.99/s  (0.716s, 1429.47/s)  LR: 9.983e-04  Data: 0.009 (0.023)
Train: 16 [ 200/1251 ( 16%)]  Loss: 4.525 (4.46)  Time: 0.702s, 1458.60/s  (0.712s, 1438.37/s)  LR: 9.983e-04  Data: 0.009 (0.020)
Train: 16 [ 250/1251 ( 20%)]  Loss: 4.510 (4.47)  Time: 0.703s, 1457.31/s  (0.709s, 1444.61/s)  LR: 9.983e-04  Data: 0.009 (0.018)
Train: 16 [ 300/1251 ( 24%)]  Loss: 4.028 (4.41)  Time: 0.689s, 1485.17/s  (0.707s, 1449.01/s)  LR: 9.983e-04  Data: 0.009 (0.017)
Train: 16 [ 350/1251 ( 28%)]  Loss: 4.763 (4.45)  Time: 0.708s, 1445.72/s  (0.705s, 1452.16/s)  LR: 9.983e-04  Data: 0.009 (0.016)
Train: 16 [ 400/1251 ( 32%)]  Loss: 4.422 (4.45)  Time: 0.676s, 1514.29/s  (0.705s, 1452.08/s)  LR: 9.983e-04  Data: 0.012 (0.015)
Train: 16 [ 450/1251 ( 36%)]  Loss: 4.447 (4.45)  Time: 0.675s, 1517.53/s  (0.704s, 1454.13/s)  LR: 9.983e-04  Data: 0.010 (0.014)
Train: 16 [ 500/1251 ( 40%)]  Loss: 3.954 (4.40)  Time: 0.720s, 1421.41/s  (0.704s, 1455.52/s)  LR: 9.983e-04  Data: 0.016 (0.014)
Train: 16 [ 550/1251 ( 44%)]  Loss: 4.599 (4.42)  Time: 0.671s, 1525.43/s  (0.703s, 1456.13/s)  LR: 9.983e-04  Data: 0.012 (0.014)
Train: 16 [ 600/1251 ( 48%)]  Loss: 4.321 (4.41)  Time: 0.694s, 1475.68/s  (0.702s, 1457.77/s)  LR: 9.983e-04  Data: 0.011 (0.013)
Train: 16 [ 650/1251 ( 52%)]  Loss: 4.178 (4.39)  Time: 0.670s, 1527.35/s  (0.702s, 1459.70/s)  LR: 9.983e-04  Data: 0.013 (0.013)
Train: 16 [ 700/1251 ( 56%)]  Loss: 4.176 (4.38)  Time: 0.678s, 1509.29/s  (0.701s, 1459.99/s)  LR: 9.983e-04  Data: 0.010 (0.013)
Train: 16 [ 750/1251 ( 60%)]  Loss: 4.174 (4.37)  Time: 0.705s, 1453.30/s  (0.701s, 1461.15/s)  LR: 9.983e-04  Data: 0.010 (0.013)
Train: 16 [ 800/1251 ( 64%)]  Loss: 4.748 (4.39)  Time: 0.672s, 1523.48/s  (0.701s, 1460.79/s)  LR: 9.983e-04  Data: 0.010 (0.012)
Train: 16 [ 850/1251 ( 68%)]  Loss: 4.699 (4.41)  Time: 0.678s, 1509.69/s  (0.701s, 1461.32/s)  LR: 9.983e-04  Data: 0.010 (0.012)
Train: 16 [ 900/1251 ( 72%)]  Loss: 4.478 (4.41)  Time: 0.704s, 1454.94/s  (0.701s, 1461.36/s)  LR: 9.983e-04  Data: 0.011 (0.012)
Train: 16 [ 950/1251 ( 76%)]  Loss: 4.467 (4.41)  Time: 0.684s, 1496.53/s  (0.701s, 1461.21/s)  LR: 9.983e-04  Data: 0.012 (0.012)
Train: 16 [1000/1251 ( 80%)]  Loss: 4.171 (4.40)  Time: 0.721s, 1421.16/s  (0.701s, 1460.98/s)  LR: 9.983e-04  Data: 0.017 (0.012)
Train: 16 [1050/1251 ( 84%)]  Loss: 4.659 (4.41)  Time: 0.720s, 1422.82/s  (0.701s, 1461.02/s)  LR: 9.983e-04  Data: 0.011 (0.012)
Train: 16 [1100/1251 ( 88%)]  Loss: 4.243 (4.41)  Time: 0.705s, 1453.09/s  (0.701s, 1461.14/s)  LR: 9.983e-04  Data: 0.009 (0.012)
Train: 16 [1150/1251 ( 92%)]  Loss: 4.628 (4.42)  Time: 0.725s, 1413.20/s  (0.701s, 1460.94/s)  LR: 9.983e-04  Data: 0.015 (0.012)
Train: 16 [1200/1251 ( 96%)]  Loss: 4.246 (4.41)  Time: 0.705s, 1453.04/s  (0.701s, 1461.59/s)  LR: 9.983e-04  Data: 0.010 (0.012)
Train: 16 [1250/1251 (100%)]  Loss: 4.775 (4.42)  Time: 0.696s, 1472.00/s  (0.701s, 1461.71/s)  LR: 9.983e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.640 (1.640)  Loss:  1.3633 (1.3633)  Acc@1: 75.3906 (75.3906)  Acc@5: 91.4062 (91.4062)
Test: [  48/48]  Time: 0.136 (0.585)  Loss:  1.3838 (2.2103)  Acc@1: 75.9434 (56.3980)  Acc@5: 90.4481 (80.5340)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-16.pth.tar', 56.398000021972656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-15.pth.tar', 55.33400008789062)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-14.pth.tar', 55.282000029296874)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-13.pth.tar', 52.831999985351565)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-12.pth.tar', 51.62200001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-11.pth.tar', 50.57599996826172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-10.pth.tar', 49.56200002197266)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-9.pth.tar', 45.00600001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-8.pth.tar', 40.45999998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-7.pth.tar', 37.68800004882812)

Train: 17 [   0/1251 (  0%)]  Loss: 4.482 (4.48)  Time: 2.228s,  459.56/s  (2.228s,  459.56/s)  LR: 9.980e-04  Data: 1.612 (1.612)
Train: 17 [  50/1251 (  4%)]  Loss: 4.101 (4.29)  Time: 0.690s, 1483.72/s  (0.734s, 1394.89/s)  LR: 9.980e-04  Data: 0.012 (0.049)
Train: 17 [ 100/1251 (  8%)]  Loss: 4.509 (4.36)  Time: 0.724s, 1413.80/s  (0.713s, 1436.49/s)  LR: 9.980e-04  Data: 0.010 (0.030)
Train: 17 [ 150/1251 ( 12%)]  Loss: 4.622 (4.43)  Time: 0.671s, 1525.81/s  (0.708s, 1446.12/s)  LR: 9.980e-04  Data: 0.010 (0.023)
Train: 17 [ 200/1251 ( 16%)]  Loss: 4.616 (4.47)  Time: 0.670s, 1529.42/s  (0.705s, 1453.26/s)  LR: 9.980e-04  Data: 0.009 (0.020)
Train: 17 [ 250/1251 ( 20%)]  Loss: 4.474 (4.47)  Time: 0.704s, 1454.49/s  (0.702s, 1459.72/s)  LR: 9.980e-04  Data: 0.009 (0.018)
Train: 17 [ 300/1251 ( 24%)]  Loss: 4.299 (4.44)  Time: 0.704s, 1454.43/s  (0.702s, 1459.58/s)  LR: 9.980e-04  Data: 0.009 (0.017)
Train: 17 [ 350/1251 ( 28%)]  Loss: 4.161 (4.41)  Time: 0.705s, 1452.71/s  (0.702s, 1459.58/s)  LR: 9.980e-04  Data: 0.010 (0.016)
Train: 17 [ 400/1251 ( 32%)]  Loss: 4.132 (4.38)  Time: 0.715s, 1432.20/s  (0.701s, 1461.01/s)  LR: 9.980e-04  Data: 0.012 (0.015)
Train: 17 [ 450/1251 ( 36%)]  Loss: 4.590 (4.40)  Time: 0.671s, 1526.47/s  (0.701s, 1461.39/s)  LR: 9.980e-04  Data: 0.011 (0.015)
Train: 17 [ 500/1251 ( 40%)]  Loss: 4.768 (4.43)  Time: 0.719s, 1424.55/s  (0.701s, 1460.50/s)  LR: 9.980e-04  Data: 0.009 (0.014)
Train: 17 [ 550/1251 ( 44%)]  Loss: 4.278 (4.42)  Time: 0.676s, 1515.57/s  (0.701s, 1459.99/s)  LR: 9.980e-04  Data: 0.015 (0.014)
Train: 17 [ 600/1251 ( 48%)]  Loss: 4.559 (4.43)  Time: 0.718s, 1426.98/s  (0.701s, 1460.72/s)  LR: 9.980e-04  Data: 0.009 (0.014)
Train: 17 [ 650/1251 ( 52%)]  Loss: 4.248 (4.42)  Time: 0.683s, 1498.50/s  (0.701s, 1461.70/s)  LR: 9.980e-04  Data: 0.010 (0.013)
Train: 17 [ 700/1251 ( 56%)]  Loss: 4.681 (4.43)  Time: 0.708s, 1445.52/s  (0.701s, 1461.26/s)  LR: 9.980e-04  Data: 0.010 (0.013)
Train: 17 [ 750/1251 ( 60%)]  Loss: 4.760 (4.46)  Time: 0.677s, 1513.19/s  (0.701s, 1461.67/s)  LR: 9.980e-04  Data: 0.010 (0.013)
Train: 17 [ 800/1251 ( 64%)]  Loss: 4.459 (4.46)  Time: 0.705s, 1453.28/s  (0.701s, 1461.24/s)  LR: 9.980e-04  Data: 0.010 (0.013)
Train: 17 [ 850/1251 ( 68%)]  Loss: 4.469 (4.46)  Time: 0.674s, 1520.26/s  (0.701s, 1460.44/s)  LR: 9.980e-04  Data: 0.010 (0.013)
Train: 17 [ 900/1251 ( 72%)]  Loss: 4.825 (4.48)  Time: 0.767s, 1335.37/s  (0.702s, 1459.61/s)  LR: 9.980e-04  Data: 0.009 (0.012)
Train: 17 [ 950/1251 ( 76%)]  Loss: 4.414 (4.47)  Time: 0.681s, 1503.40/s  (0.701s, 1459.85/s)  LR: 9.980e-04  Data: 0.013 (0.012)
Train: 17 [1000/1251 ( 80%)]  Loss: 4.315 (4.46)  Time: 0.719s, 1424.87/s  (0.701s, 1460.35/s)  LR: 9.980e-04  Data: 0.010 (0.012)
Train: 17 [1050/1251 ( 84%)]  Loss: 4.512 (4.47)  Time: 0.672s, 1523.96/s  (0.701s, 1460.19/s)  LR: 9.980e-04  Data: 0.010 (0.012)
Train: 17 [1100/1251 ( 88%)]  Loss: 4.190 (4.45)  Time: 0.672s, 1524.81/s  (0.702s, 1459.40/s)  LR: 9.980e-04  Data: 0.009 (0.012)
Train: 17 [1150/1251 ( 92%)]  Loss: 4.365 (4.45)  Time: 0.679s, 1508.78/s  (0.701s, 1459.83/s)  LR: 9.980e-04  Data: 0.010 (0.012)
Train: 17 [1200/1251 ( 96%)]  Loss: 4.203 (4.44)  Time: 0.673s, 1520.57/s  (0.702s, 1459.29/s)  LR: 9.980e-04  Data: 0.010 (0.012)
Train: 17 [1250/1251 (100%)]  Loss: 4.799 (4.46)  Time: 0.657s, 1559.31/s  (0.702s, 1459.14/s)  LR: 9.980e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.608 (1.608)  Loss:  1.2803 (1.2803)  Acc@1: 76.7578 (76.7578)  Acc@5: 93.1641 (93.1641)
Test: [  48/48]  Time: 0.136 (0.589)  Loss:  1.3037 (2.0289)  Acc@1: 75.0000 (58.2080)  Acc@5: 89.7406 (82.1200)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-17.pth.tar', 58.208)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-16.pth.tar', 56.398000021972656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-15.pth.tar', 55.33400008789062)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-14.pth.tar', 55.282000029296874)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-13.pth.tar', 52.831999985351565)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-12.pth.tar', 51.62200001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-11.pth.tar', 50.57599996826172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-10.pth.tar', 49.56200002197266)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-9.pth.tar', 45.00600001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-8.pth.tar', 40.45999998779297)

Train: 18 [   0/1251 (  0%)]  Loss: 4.301 (4.30)  Time: 2.425s,  422.34/s  (2.425s,  422.34/s)  LR: 9.978e-04  Data: 1.790 (1.790)
Train: 18 [  50/1251 (  4%)]  Loss: 4.426 (4.36)  Time: 0.706s, 1450.72/s  (0.734s, 1395.66/s)  LR: 9.978e-04  Data: 0.010 (0.052)
Train: 18 [ 100/1251 (  8%)]  Loss: 4.521 (4.42)  Time: 0.708s, 1446.21/s  (0.714s, 1433.97/s)  LR: 9.978e-04  Data: 0.010 (0.031)
Train: 18 [ 150/1251 ( 12%)]  Loss: 4.474 (4.43)  Time: 0.671s, 1526.81/s  (0.708s, 1447.09/s)  LR: 9.978e-04  Data: 0.010 (0.024)
Train: 18 [ 200/1251 ( 16%)]  Loss: 3.973 (4.34)  Time: 0.669s, 1530.66/s  (0.706s, 1451.25/s)  LR: 9.978e-04  Data: 0.010 (0.021)
Train: 18 [ 250/1251 ( 20%)]  Loss: 4.617 (4.39)  Time: 0.714s, 1433.32/s  (0.704s, 1454.77/s)  LR: 9.978e-04  Data: 0.010 (0.019)
Train: 18 [ 300/1251 ( 24%)]  Loss: 4.492 (4.40)  Time: 0.701s, 1460.23/s  (0.702s, 1458.61/s)  LR: 9.978e-04  Data: 0.009 (0.017)
Train: 18 [ 350/1251 ( 28%)]  Loss: 4.245 (4.38)  Time: 0.708s, 1446.73/s  (0.701s, 1461.13/s)  LR: 9.978e-04  Data: 0.010 (0.016)
Train: 18 [ 400/1251 ( 32%)]  Loss: 4.517 (4.40)  Time: 0.672s, 1524.61/s  (0.701s, 1461.18/s)  LR: 9.978e-04  Data: 0.011 (0.015)
Train: 18 [ 450/1251 ( 36%)]  Loss: 4.649 (4.42)  Time: 0.700s, 1463.43/s  (0.701s, 1460.42/s)  LR: 9.978e-04  Data: 0.009 (0.015)
Train: 18 [ 500/1251 ( 40%)]  Loss: 4.033 (4.39)  Time: 0.673s, 1521.62/s  (0.700s, 1462.88/s)  LR: 9.978e-04  Data: 0.012 (0.014)
Train: 18 [ 550/1251 ( 44%)]  Loss: 4.281 (4.38)  Time: 0.705s, 1453.25/s  (0.701s, 1461.67/s)  LR: 9.978e-04  Data: 0.010 (0.014)
Train: 18 [ 600/1251 ( 48%)]  Loss: 4.205 (4.36)  Time: 0.681s, 1502.80/s  (0.700s, 1462.52/s)  LR: 9.978e-04  Data: 0.010 (0.014)
Train: 18 [ 650/1251 ( 52%)]  Loss: 4.656 (4.38)  Time: 0.711s, 1440.51/s  (0.700s, 1462.29/s)  LR: 9.978e-04  Data: 0.009 (0.013)
Train: 18 [ 700/1251 ( 56%)]  Loss: 3.949 (4.36)  Time: 0.703s, 1457.08/s  (0.700s, 1462.84/s)  LR: 9.978e-04  Data: 0.010 (0.013)
Train: 18 [ 750/1251 ( 60%)]  Loss: 4.354 (4.36)  Time: 0.707s, 1449.28/s  (0.699s, 1464.19/s)  LR: 9.978e-04  Data: 0.009 (0.013)
Train: 18 [ 800/1251 ( 64%)]  Loss: 4.649 (4.37)  Time: 0.671s, 1526.52/s  (0.699s, 1465.06/s)  LR: 9.978e-04  Data: 0.010 (0.013)
Train: 18 [ 850/1251 ( 68%)]  Loss: 4.386 (4.37)  Time: 0.703s, 1456.07/s  (0.699s, 1465.33/s)  LR: 9.978e-04  Data: 0.009 (0.013)
Train: 18 [ 900/1251 ( 72%)]  Loss: 4.151 (4.36)  Time: 0.700s, 1462.39/s  (0.699s, 1465.41/s)  LR: 9.978e-04  Data: 0.011 (0.012)
Train: 18 [ 950/1251 ( 76%)]  Loss: 4.335 (4.36)  Time: 0.694s, 1476.14/s  (0.699s, 1465.05/s)  LR: 9.978e-04  Data: 0.019 (0.012)
Train: 18 [1000/1251 ( 80%)]  Loss: 4.274 (4.36)  Time: 0.675s, 1516.58/s  (0.699s, 1465.18/s)  LR: 9.978e-04  Data: 0.011 (0.012)
Train: 18 [1050/1251 ( 84%)]  Loss: 4.619 (4.37)  Time: 0.707s, 1448.16/s  (0.699s, 1464.58/s)  LR: 9.978e-04  Data: 0.009 (0.012)
Train: 18 [1100/1251 ( 88%)]  Loss: 4.733 (4.38)  Time: 0.705s, 1452.22/s  (0.699s, 1464.06/s)  LR: 9.978e-04  Data: 0.009 (0.012)
Train: 18 [1150/1251 ( 92%)]  Loss: 4.451 (4.39)  Time: 0.674s, 1519.82/s  (0.700s, 1463.56/s)  LR: 9.978e-04  Data: 0.010 (0.012)
Train: 18 [1200/1251 ( 96%)]  Loss: 4.169 (4.38)  Time: 0.715s, 1432.13/s  (0.700s, 1463.50/s)  LR: 9.978e-04  Data: 0.010 (0.012)
Train: 18 [1250/1251 (100%)]  Loss: 4.184 (4.37)  Time: 0.696s, 1471.59/s  (0.700s, 1462.82/s)  LR: 9.978e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.509 (1.509)  Loss:  1.3965 (1.3965)  Acc@1: 77.7344 (77.7344)  Acc@5: 92.8711 (92.8711)
Test: [  48/48]  Time: 0.136 (0.583)  Loss:  1.3047 (2.1977)  Acc@1: 76.1792 (57.3120)  Acc@5: 91.7453 (81.5520)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-17.pth.tar', 58.208)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-18.pth.tar', 57.31199999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-16.pth.tar', 56.398000021972656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-15.pth.tar', 55.33400008789062)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-14.pth.tar', 55.282000029296874)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-13.pth.tar', 52.831999985351565)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-12.pth.tar', 51.62200001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-11.pth.tar', 50.57599996826172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-10.pth.tar', 49.56200002197266)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-9.pth.tar', 45.00600001220703)

Train: 19 [   0/1251 (  0%)]  Loss: 4.869 (4.87)  Time: 2.147s,  477.05/s  (2.147s,  477.05/s)  LR: 9.976e-04  Data: 1.532 (1.532)
Train: 19 [  50/1251 (  4%)]  Loss: 4.526 (4.70)  Time: 0.705s, 1452.94/s  (0.736s, 1390.52/s)  LR: 9.976e-04  Data: 0.011 (0.048)
Train: 19 [ 100/1251 (  8%)]  Loss: 4.219 (4.54)  Time: 0.762s, 1343.73/s  (0.717s, 1427.61/s)  LR: 9.976e-04  Data: 0.009 (0.029)
Train: 19 [ 150/1251 ( 12%)]  Loss: 4.425 (4.51)  Time: 0.731s, 1399.91/s  (0.710s, 1441.86/s)  LR: 9.976e-04  Data: 0.010 (0.023)
Train: 19 [ 200/1251 ( 16%)]  Loss: 4.765 (4.56)  Time: 0.676s, 1513.69/s  (0.706s, 1449.42/s)  LR: 9.976e-04  Data: 0.009 (0.020)
Train: 19 [ 250/1251 ( 20%)]  Loss: 4.521 (4.55)  Time: 0.676s, 1515.01/s  (0.705s, 1453.21/s)  LR: 9.976e-04  Data: 0.012 (0.018)
Train: 19 [ 300/1251 ( 24%)]  Loss: 4.162 (4.50)  Time: 0.669s, 1530.30/s  (0.704s, 1455.16/s)  LR: 9.976e-04  Data: 0.010 (0.017)
Train: 19 [ 350/1251 ( 28%)]  Loss: 4.700 (4.52)  Time: 0.683s, 1499.15/s  (0.703s, 1456.34/s)  LR: 9.976e-04  Data: 0.010 (0.016)
Train: 19 [ 400/1251 ( 32%)]  Loss: 4.067 (4.47)  Time: 0.710s, 1442.67/s  (0.703s, 1457.21/s)  LR: 9.976e-04  Data: 0.010 (0.015)
Train: 19 [ 450/1251 ( 36%)]  Loss: 4.225 (4.45)  Time: 0.737s, 1389.17/s  (0.703s, 1456.13/s)  LR: 9.976e-04  Data: 0.009 (0.014)
Train: 19 [ 500/1251 ( 40%)]  Loss: 4.383 (4.44)  Time: 0.749s, 1366.72/s  (0.703s, 1456.59/s)  LR: 9.976e-04  Data: 0.009 (0.014)
Train: 19 [ 550/1251 ( 44%)]  Loss: 4.071 (4.41)  Time: 0.685s, 1495.44/s  (0.703s, 1457.29/s)  LR: 9.976e-04  Data: 0.009 (0.014)
Train: 19 [ 600/1251 ( 48%)]  Loss: 4.605 (4.43)  Time: 0.705s, 1452.81/s  (0.703s, 1457.57/s)  LR: 9.976e-04  Data: 0.009 (0.013)
Train: 19 [ 650/1251 ( 52%)]  Loss: 4.582 (4.44)  Time: 0.671s, 1525.03/s  (0.702s, 1458.57/s)  LR: 9.976e-04  Data: 0.010 (0.013)
Train: 19 [ 700/1251 ( 56%)]  Loss: 4.551 (4.44)  Time: 0.727s, 1409.01/s  (0.702s, 1457.75/s)  LR: 9.976e-04  Data: 0.009 (0.013)
Train: 19 [ 750/1251 ( 60%)]  Loss: 4.761 (4.46)  Time: 0.686s, 1492.73/s  (0.702s, 1458.49/s)  LR: 9.976e-04  Data: 0.010 (0.013)
Train: 19 [ 800/1251 ( 64%)]  Loss: 4.273 (4.45)  Time: 0.699s, 1464.03/s  (0.702s, 1458.99/s)  LR: 9.976e-04  Data: 0.009 (0.013)
Train: 19 [ 850/1251 ( 68%)]  Loss: 4.827 (4.47)  Time: 0.712s, 1438.30/s  (0.702s, 1459.58/s)  LR: 9.976e-04  Data: 0.010 (0.013)
Train: 19 [ 900/1251 ( 72%)]  Loss: 4.231 (4.46)  Time: 0.723s, 1417.30/s  (0.701s, 1460.82/s)  LR: 9.976e-04  Data: 0.010 (0.012)
Train: 19 [ 950/1251 ( 76%)]  Loss: 4.352 (4.46)  Time: 0.691s, 1481.48/s  (0.702s, 1459.21/s)  LR: 9.976e-04  Data: 0.009 (0.012)
Train: 19 [1000/1251 ( 80%)]  Loss: 4.627 (4.46)  Time: 0.707s, 1448.91/s  (0.702s, 1457.87/s)  LR: 9.976e-04  Data: 0.012 (0.012)
Train: 19 [1050/1251 ( 84%)]  Loss: 4.364 (4.46)  Time: 0.806s, 1271.23/s  (0.703s, 1455.64/s)  LR: 9.976e-04  Data: 0.012 (0.012)
Train: 19 [1100/1251 ( 88%)]  Loss: 4.558 (4.46)  Time: 0.726s, 1409.70/s  (0.704s, 1455.44/s)  LR: 9.976e-04  Data: 0.009 (0.012)
Train: 19 [1150/1251 ( 92%)]  Loss: 4.325 (4.46)  Time: 0.676s, 1515.75/s  (0.703s, 1456.43/s)  LR: 9.976e-04  Data: 0.009 (0.012)
Train: 19 [1200/1251 ( 96%)]  Loss: 4.130 (4.44)  Time: 0.671s, 1526.11/s  (0.703s, 1456.65/s)  LR: 9.976e-04  Data: 0.009 (0.012)
Train: 19 [1250/1251 (100%)]  Loss: 4.336 (4.44)  Time: 0.658s, 1556.74/s  (0.703s, 1456.49/s)  LR: 9.976e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.589 (1.589)  Loss:  1.2617 (1.2617)  Acc@1: 78.8086 (78.8086)  Acc@5: 95.0195 (95.0195)
Test: [  48/48]  Time: 0.136 (0.576)  Loss:  1.4668 (2.0170)  Acc@1: 73.4670 (60.2160)  Acc@5: 90.0943 (83.5180)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-19.pth.tar', 60.21599998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-17.pth.tar', 58.208)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-18.pth.tar', 57.31199999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-16.pth.tar', 56.398000021972656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-15.pth.tar', 55.33400008789062)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-14.pth.tar', 55.282000029296874)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-13.pth.tar', 52.831999985351565)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-12.pth.tar', 51.62200001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-11.pth.tar', 50.57599996826172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-10.pth.tar', 49.56200002197266)

Train: 20 [   0/1251 (  0%)]  Loss: 4.519 (4.52)  Time: 2.373s,  431.56/s  (2.373s,  431.56/s)  LR: 9.973e-04  Data: 1.718 (1.718)
Train: 20 [  50/1251 (  4%)]  Loss: 4.502 (4.51)  Time: 0.728s, 1406.48/s  (0.724s, 1414.83/s)  LR: 9.973e-04  Data: 0.009 (0.044)
Train: 20 [ 100/1251 (  8%)]  Loss: 4.455 (4.49)  Time: 0.715s, 1431.47/s  (0.709s, 1444.44/s)  LR: 9.973e-04  Data: 0.010 (0.027)
Train: 20 [ 150/1251 ( 12%)]  Loss: 4.092 (4.39)  Time: 0.679s, 1507.62/s  (0.706s, 1451.09/s)  LR: 9.973e-04  Data: 0.010 (0.021)
Train: 20 [ 200/1251 ( 16%)]  Loss: 4.313 (4.38)  Time: 0.666s, 1537.85/s  (0.703s, 1457.19/s)  LR: 9.973e-04  Data: 0.008 (0.018)
Train: 20 [ 250/1251 ( 20%)]  Loss: 4.288 (4.36)  Time: 0.711s, 1441.13/s  (0.702s, 1458.61/s)  LR: 9.973e-04  Data: 0.009 (0.017)
Train: 20 [ 300/1251 ( 24%)]  Loss: 4.380 (4.36)  Time: 0.719s, 1423.28/s  (0.701s, 1461.46/s)  LR: 9.973e-04  Data: 0.010 (0.016)
Train: 20 [ 350/1251 ( 28%)]  Loss: 4.362 (4.36)  Time: 0.711s, 1440.70/s  (0.700s, 1463.88/s)  LR: 9.973e-04  Data: 0.009 (0.015)
Train: 20 [ 400/1251 ( 32%)]  Loss: 4.295 (4.36)  Time: 0.706s, 1450.67/s  (0.699s, 1465.76/s)  LR: 9.973e-04  Data: 0.010 (0.014)
Train: 20 [ 450/1251 ( 36%)]  Loss: 4.648 (4.39)  Time: 0.684s, 1496.56/s  (0.698s, 1466.01/s)  LR: 9.973e-04  Data: 0.009 (0.014)
Train: 20 [ 500/1251 ( 40%)]  Loss: 4.546 (4.40)  Time: 0.703s, 1456.30/s  (0.698s, 1466.65/s)  LR: 9.973e-04  Data: 0.009 (0.013)
Train: 20 [ 550/1251 ( 44%)]  Loss: 4.426 (4.40)  Time: 0.718s, 1426.42/s  (0.699s, 1465.91/s)  LR: 9.973e-04  Data: 0.010 (0.013)
Train: 20 [ 600/1251 ( 48%)]  Loss: 4.589 (4.42)  Time: 0.726s, 1411.35/s  (0.699s, 1465.53/s)  LR: 9.973e-04  Data: 0.018 (0.013)
Train: 20 [ 650/1251 ( 52%)]  Loss: 4.032 (4.39)  Time: 0.680s, 1505.38/s  (0.699s, 1465.32/s)  LR: 9.973e-04  Data: 0.009 (0.013)
Train: 20 [ 700/1251 ( 56%)]  Loss: 4.567 (4.40)  Time: 0.705s, 1452.81/s  (0.699s, 1465.26/s)  LR: 9.973e-04  Data: 0.010 (0.013)
Train: 20 [ 750/1251 ( 60%)]  Loss: 4.388 (4.40)  Time: 0.686s, 1491.86/s  (0.698s, 1466.36/s)  LR: 9.973e-04  Data: 0.010 (0.012)
Train: 20 [ 800/1251 ( 64%)]  Loss: 4.401 (4.40)  Time: 0.673s, 1520.65/s  (0.698s, 1467.58/s)  LR: 9.973e-04  Data: 0.010 (0.012)
Train: 20 [ 850/1251 ( 68%)]  Loss: 4.419 (4.40)  Time: 0.706s, 1449.49/s  (0.698s, 1467.15/s)  LR: 9.973e-04  Data: 0.011 (0.012)
Train: 20 [ 900/1251 ( 72%)]  Loss: 4.135 (4.39)  Time: 0.702s, 1458.47/s  (0.698s, 1466.86/s)  LR: 9.973e-04  Data: 0.012 (0.012)
Train: 20 [ 950/1251 ( 76%)]  Loss: 4.579 (4.40)  Time: 0.670s, 1528.05/s  (0.698s, 1466.93/s)  LR: 9.973e-04  Data: 0.009 (0.012)
Train: 20 [1000/1251 ( 80%)]  Loss: 4.458 (4.40)  Time: 0.716s, 1429.83/s  (0.698s, 1466.67/s)  LR: 9.973e-04  Data: 0.009 (0.012)
Train: 20 [1050/1251 ( 84%)]  Loss: 4.275 (4.39)  Time: 0.703s, 1456.02/s  (0.698s, 1467.00/s)  LR: 9.973e-04  Data: 0.009 (0.012)
Train: 20 [1100/1251 ( 88%)]  Loss: 4.415 (4.39)  Time: 0.673s, 1522.35/s  (0.698s, 1467.17/s)  LR: 9.973e-04  Data: 0.010 (0.012)
Train: 20 [1150/1251 ( 92%)]  Loss: 4.457 (4.40)  Time: 0.668s, 1533.32/s  (0.698s, 1467.50/s)  LR: 9.973e-04  Data: 0.009 (0.012)
Train: 20 [1200/1251 ( 96%)]  Loss: 4.392 (4.40)  Time: 0.730s, 1402.56/s  (0.698s, 1467.88/s)  LR: 9.973e-04  Data: 0.009 (0.011)
Train: 20 [1250/1251 (100%)]  Loss: 4.152 (4.39)  Time: 0.691s, 1481.35/s  (0.698s, 1467.53/s)  LR: 9.973e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.566 (1.566)  Loss:  1.2002 (1.2002)  Acc@1: 78.8086 (78.8086)  Acc@5: 94.4336 (94.4336)
Test: [  48/48]  Time: 0.137 (0.581)  Loss:  1.3887 (2.0685)  Acc@1: 75.5896 (58.6440)  Acc@5: 92.5708 (82.7700)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-19.pth.tar', 60.21599998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-20.pth.tar', 58.643999997558595)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-17.pth.tar', 58.208)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-18.pth.tar', 57.31199999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-16.pth.tar', 56.398000021972656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-15.pth.tar', 55.33400008789062)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-14.pth.tar', 55.282000029296874)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-13.pth.tar', 52.831999985351565)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-12.pth.tar', 51.62200001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-11.pth.tar', 50.57599996826172)

Train: 21 [   0/1251 (  0%)]  Loss: 4.448 (4.45)  Time: 2.175s,  470.86/s  (2.175s,  470.86/s)  LR: 9.970e-04  Data: 1.561 (1.561)
Train: 21 [  50/1251 (  4%)]  Loss: 3.990 (4.22)  Time: 0.707s, 1449.26/s  (0.737s, 1389.25/s)  LR: 9.970e-04  Data: 0.009 (0.049)
Train: 21 [ 100/1251 (  8%)]  Loss: 4.110 (4.18)  Time: 0.708s, 1446.05/s  (0.714s, 1434.38/s)  LR: 9.970e-04  Data: 0.011 (0.030)
Train: 21 [ 150/1251 ( 12%)]  Loss: 4.515 (4.27)  Time: 0.708s, 1445.79/s  (0.709s, 1444.86/s)  LR: 9.970e-04  Data: 0.010 (0.023)
Train: 21 [ 200/1251 ( 16%)]  Loss: 4.530 (4.32)  Time: 0.670s, 1527.39/s  (0.707s, 1448.41/s)  LR: 9.970e-04  Data: 0.009 (0.020)
Train: 21 [ 250/1251 ( 20%)]  Loss: 4.373 (4.33)  Time: 0.673s, 1522.21/s  (0.705s, 1452.41/s)  LR: 9.970e-04  Data: 0.009 (0.018)
Train: 21 [ 300/1251 ( 24%)]  Loss: 4.631 (4.37)  Time: 0.773s, 1324.36/s  (0.703s, 1456.13/s)  LR: 9.970e-04  Data: 0.010 (0.017)
Train: 21 [ 350/1251 ( 28%)]  Loss: 4.285 (4.36)  Time: 0.722s, 1418.79/s  (0.704s, 1454.87/s)  LR: 9.970e-04  Data: 0.009 (0.016)
Train: 21 [ 400/1251 ( 32%)]  Loss: 4.315 (4.36)  Time: 0.672s, 1523.77/s  (0.704s, 1454.38/s)  LR: 9.970e-04  Data: 0.010 (0.015)
Train: 21 [ 450/1251 ( 36%)]  Loss: 4.601 (4.38)  Time: 0.708s, 1447.04/s  (0.704s, 1455.49/s)  LR: 9.970e-04  Data: 0.009 (0.015)
Train: 21 [ 500/1251 ( 40%)]  Loss: 4.307 (4.37)  Time: 0.713s, 1436.78/s  (0.703s, 1456.08/s)  LR: 9.970e-04  Data: 0.009 (0.014)
Train: 21 [ 550/1251 ( 44%)]  Loss: 4.465 (4.38)  Time: 0.669s, 1530.82/s  (0.703s, 1456.57/s)  LR: 9.970e-04  Data: 0.009 (0.014)
Train: 21 [ 600/1251 ( 48%)]  Loss: 4.328 (4.38)  Time: 0.705s, 1452.63/s  (0.703s, 1457.46/s)  LR: 9.970e-04  Data: 0.013 (0.014)
Train: 21 [ 650/1251 ( 52%)]  Loss: 4.435 (4.38)  Time: 0.722s, 1417.71/s  (0.702s, 1457.75/s)  LR: 9.970e-04  Data: 0.009 (0.013)
Train: 21 [ 700/1251 ( 56%)]  Loss: 4.317 (4.38)  Time: 0.676s, 1513.88/s  (0.703s, 1456.51/s)  LR: 9.970e-04  Data: 0.010 (0.013)
Train: 21 [ 750/1251 ( 60%)]  Loss: 4.646 (4.39)  Time: 0.669s, 1530.27/s  (0.703s, 1456.42/s)  LR: 9.970e-04  Data: 0.009 (0.013)
Train: 21 [ 800/1251 ( 64%)]  Loss: 4.590 (4.41)  Time: 0.685s, 1494.22/s  (0.702s, 1457.68/s)  LR: 9.970e-04  Data: 0.009 (0.013)
Train: 21 [ 850/1251 ( 68%)]  Loss: 4.086 (4.39)  Time: 0.672s, 1523.93/s  (0.702s, 1458.43/s)  LR: 9.970e-04  Data: 0.010 (0.012)
Train: 21 [ 900/1251 ( 72%)]  Loss: 4.326 (4.38)  Time: 0.702s, 1459.71/s  (0.702s, 1458.10/s)  LR: 9.970e-04  Data: 0.009 (0.012)
Train: 21 [ 950/1251 ( 76%)]  Loss: 4.036 (4.37)  Time: 0.697s, 1468.48/s  (0.702s, 1458.33/s)  LR: 9.970e-04  Data: 0.010 (0.012)
Train: 21 [1000/1251 ( 80%)]  Loss: 4.240 (4.36)  Time: 0.700s, 1462.07/s  (0.702s, 1459.19/s)  LR: 9.970e-04  Data: 0.009 (0.012)
Train: 21 [1050/1251 ( 84%)]  Loss: 4.467 (4.37)  Time: 0.674s, 1518.25/s  (0.702s, 1458.63/s)  LR: 9.970e-04  Data: 0.011 (0.012)
Train: 21 [1100/1251 ( 88%)]  Loss: 4.138 (4.36)  Time: 0.716s, 1429.23/s  (0.702s, 1458.80/s)  LR: 9.970e-04  Data: 0.009 (0.012)
Train: 21 [1150/1251 ( 92%)]  Loss: 4.312 (4.35)  Time: 0.707s, 1449.07/s  (0.702s, 1458.95/s)  LR: 9.970e-04  Data: 0.011 (0.012)
Train: 21 [1200/1251 ( 96%)]  Loss: 4.481 (4.36)  Time: 0.680s, 1506.03/s  (0.702s, 1458.59/s)  LR: 9.970e-04  Data: 0.010 (0.012)
Train: 21 [1250/1251 (100%)]  Loss: 4.437 (4.36)  Time: 0.654s, 1565.72/s  (0.702s, 1458.43/s)  LR: 9.970e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.557 (1.557)  Loss:  1.3379 (1.3379)  Acc@1: 78.8086 (78.8086)  Acc@5: 94.6289 (94.6289)
Test: [  48/48]  Time: 0.138 (0.584)  Loss:  1.5225 (2.0191)  Acc@1: 75.8255 (60.6780)  Acc@5: 91.1557 (83.7800)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-21.pth.tar', 60.678000100097655)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-19.pth.tar', 60.21599998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-20.pth.tar', 58.643999997558595)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-17.pth.tar', 58.208)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-18.pth.tar', 57.31199999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-16.pth.tar', 56.398000021972656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-15.pth.tar', 55.33400008789062)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-14.pth.tar', 55.282000029296874)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-13.pth.tar', 52.831999985351565)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-12.pth.tar', 51.62200001220703)

Train: 22 [   0/1251 (  0%)]  Loss: 4.194 (4.19)  Time: 2.815s,  363.70/s  (2.815s,  363.70/s)  LR: 9.967e-04  Data: 2.141 (2.141)
Train: 22 [  50/1251 (  4%)]  Loss: 4.004 (4.10)  Time: 0.701s, 1461.17/s  (0.745s, 1374.74/s)  LR: 9.967e-04  Data: 0.010 (0.058)
Train: 22 [ 100/1251 (  8%)]  Loss: 3.999 (4.07)  Time: 0.706s, 1451.08/s  (0.727s, 1407.61/s)  LR: 9.967e-04  Data: 0.011 (0.034)
Train: 22 [ 150/1251 ( 12%)]  Loss: 4.333 (4.13)  Time: 0.711s, 1441.14/s  (0.720s, 1421.41/s)  LR: 9.967e-04  Data: 0.009 (0.026)
Train: 22 [ 200/1251 ( 16%)]  Loss: 4.425 (4.19)  Time: 0.704s, 1454.30/s  (0.717s, 1428.60/s)  LR: 9.967e-04  Data: 0.009 (0.022)
Train: 22 [ 250/1251 ( 20%)]  Loss: 4.669 (4.27)  Time: 0.708s, 1447.19/s  (0.715s, 1431.99/s)  LR: 9.967e-04  Data: 0.010 (0.020)
Train: 22 [ 300/1251 ( 24%)]  Loss: 4.312 (4.28)  Time: 0.705s, 1452.61/s  (0.711s, 1439.50/s)  LR: 9.967e-04  Data: 0.010 (0.018)
Train: 22 [ 350/1251 ( 28%)]  Loss: 4.576 (4.31)  Time: 0.679s, 1507.84/s  (0.709s, 1443.66/s)  LR: 9.967e-04  Data: 0.010 (0.017)
Train: 22 [ 400/1251 ( 32%)]  Loss: 4.659 (4.35)  Time: 0.695s, 1472.71/s  (0.708s, 1446.53/s)  LR: 9.967e-04  Data: 0.010 (0.016)
Train: 22 [ 450/1251 ( 36%)]  Loss: 4.624 (4.38)  Time: 0.711s, 1439.24/s  (0.707s, 1449.24/s)  LR: 9.967e-04  Data: 0.009 (0.016)
Train: 22 [ 500/1251 ( 40%)]  Loss: 4.200 (4.36)  Time: 0.671s, 1526.02/s  (0.706s, 1450.14/s)  LR: 9.967e-04  Data: 0.010 (0.015)
Train: 22 [ 550/1251 ( 44%)]  Loss: 4.593 (4.38)  Time: 0.675s, 1517.41/s  (0.706s, 1450.50/s)  LR: 9.967e-04  Data: 0.010 (0.015)
Train: 22 [ 600/1251 ( 48%)]  Loss: 4.392 (4.38)  Time: 0.712s, 1438.49/s  (0.705s, 1452.30/s)  LR: 9.967e-04  Data: 0.009 (0.014)
Train: 22 [ 650/1251 ( 52%)]  Loss: 4.300 (4.38)  Time: 0.674s, 1519.77/s  (0.705s, 1452.79/s)  LR: 9.967e-04  Data: 0.010 (0.014)
Train: 22 [ 700/1251 ( 56%)]  Loss: 4.360 (4.38)  Time: 0.764s, 1340.46/s  (0.704s, 1454.52/s)  LR: 9.967e-04  Data: 0.009 (0.014)
Train: 22 [ 750/1251 ( 60%)]  Loss: 4.624 (4.39)  Time: 0.720s, 1422.51/s  (0.704s, 1454.84/s)  LR: 9.967e-04  Data: 0.010 (0.014)
Train: 22 [ 800/1251 ( 64%)]  Loss: 4.204 (4.38)  Time: 0.702s, 1459.32/s  (0.704s, 1454.59/s)  LR: 9.967e-04  Data: 0.009 (0.013)
Train: 22 [ 850/1251 ( 68%)]  Loss: 4.161 (4.37)  Time: 0.674s, 1519.62/s  (0.704s, 1454.80/s)  LR: 9.967e-04  Data: 0.010 (0.013)
Train: 22 [ 900/1251 ( 72%)]  Loss: 4.533 (4.38)  Time: 0.693s, 1477.24/s  (0.704s, 1454.61/s)  LR: 9.967e-04  Data: 0.009 (0.013)
Train: 22 [ 950/1251 ( 76%)]  Loss: 4.342 (4.38)  Time: 0.703s, 1456.72/s  (0.704s, 1455.43/s)  LR: 9.967e-04  Data: 0.009 (0.013)
Train: 22 [1000/1251 ( 80%)]  Loss: 4.322 (4.37)  Time: 0.702s, 1457.69/s  (0.703s, 1456.13/s)  LR: 9.967e-04  Data: 0.009 (0.013)
Train: 22 [1050/1251 ( 84%)]  Loss: 4.288 (4.37)  Time: 0.689s, 1485.16/s  (0.703s, 1457.24/s)  LR: 9.967e-04  Data: 0.009 (0.013)
Train: 22 [1100/1251 ( 88%)]  Loss: 4.473 (4.37)  Time: 0.709s, 1444.97/s  (0.703s, 1457.46/s)  LR: 9.967e-04  Data: 0.009 (0.012)
Train: 22 [1150/1251 ( 92%)]  Loss: 4.588 (4.38)  Time: 0.702s, 1458.22/s  (0.703s, 1457.14/s)  LR: 9.967e-04  Data: 0.010 (0.012)
Train: 22 [1200/1251 ( 96%)]  Loss: 4.468 (4.39)  Time: 0.729s, 1404.48/s  (0.703s, 1457.20/s)  LR: 9.967e-04  Data: 0.014 (0.012)
Train: 22 [1250/1251 (100%)]  Loss: 4.237 (4.38)  Time: 0.660s, 1551.99/s  (0.703s, 1457.16/s)  LR: 9.967e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.526 (1.526)  Loss:  1.3623 (1.3623)  Acc@1: 80.0781 (80.0781)  Acc@5: 93.1641 (93.1641)
Test: [  48/48]  Time: 0.136 (0.586)  Loss:  1.2949 (1.9809)  Acc@1: 77.7123 (61.0360)  Acc@5: 92.0991 (84.0500)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-22.pth.tar', 61.03600001464844)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-21.pth.tar', 60.678000100097655)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-19.pth.tar', 60.21599998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-20.pth.tar', 58.643999997558595)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-17.pth.tar', 58.208)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-18.pth.tar', 57.31199999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-16.pth.tar', 56.398000021972656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-15.pth.tar', 55.33400008789062)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-14.pth.tar', 55.282000029296874)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-13.pth.tar', 52.831999985351565)

Train: 23 [   0/1251 (  0%)]  Loss: 4.499 (4.50)  Time: 2.160s,  474.14/s  (2.160s,  474.14/s)  LR: 9.964e-04  Data: 1.521 (1.521)
Train: 23 [  50/1251 (  4%)]  Loss: 3.914 (4.21)  Time: 0.710s, 1443.00/s  (0.743s, 1377.42/s)  LR: 9.964e-04  Data: 0.009 (0.047)
Train: 23 [ 100/1251 (  8%)]  Loss: 4.425 (4.28)  Time: 0.671s, 1525.71/s  (0.725s, 1412.52/s)  LR: 9.964e-04  Data: 0.010 (0.029)
Train: 23 [ 150/1251 ( 12%)]  Loss: 4.241 (4.27)  Time: 0.736s, 1391.01/s  (0.718s, 1425.45/s)  LR: 9.964e-04  Data: 0.009 (0.023)
Train: 23 [ 200/1251 ( 16%)]  Loss: 4.387 (4.29)  Time: 0.684s, 1496.92/s  (0.713s, 1437.08/s)  LR: 9.964e-04  Data: 0.009 (0.020)
Train: 23 [ 250/1251 ( 20%)]  Loss: 4.261 (4.29)  Time: 0.675s, 1517.59/s  (0.710s, 1441.84/s)  LR: 9.964e-04  Data: 0.010 (0.018)
Train: 23 [ 300/1251 ( 24%)]  Loss: 4.062 (4.26)  Time: 0.690s, 1484.63/s  (0.710s, 1443.18/s)  LR: 9.964e-04  Data: 0.010 (0.017)
Train: 23 [ 350/1251 ( 28%)]  Loss: 4.148 (4.24)  Time: 0.690s, 1484.17/s  (0.710s, 1441.41/s)  LR: 9.964e-04  Data: 0.012 (0.016)
Train: 23 [ 400/1251 ( 32%)]  Loss: 4.144 (4.23)  Time: 0.704s, 1453.76/s  (0.710s, 1441.38/s)  LR: 9.964e-04  Data: 0.009 (0.015)
Train: 23 [ 450/1251 ( 36%)]  Loss: 4.167 (4.22)  Time: 0.670s, 1527.78/s  (0.709s, 1444.22/s)  LR: 9.964e-04  Data: 0.010 (0.014)
Train: 23 [ 500/1251 ( 40%)]  Loss: 4.188 (4.22)  Time: 0.703s, 1456.82/s  (0.708s, 1446.11/s)  LR: 9.964e-04  Data: 0.009 (0.014)
Train: 23 [ 550/1251 ( 44%)]  Loss: 4.739 (4.26)  Time: 0.719s, 1423.26/s  (0.708s, 1447.24/s)  LR: 9.964e-04  Data: 0.009 (0.014)
Train: 23 [ 600/1251 ( 48%)]  Loss: 4.555 (4.29)  Time: 0.714s, 1433.47/s  (0.707s, 1448.19/s)  LR: 9.964e-04  Data: 0.009 (0.013)
Train: 23 [ 650/1251 ( 52%)]  Loss: 4.463 (4.30)  Time: 0.704s, 1454.67/s  (0.706s, 1449.76/s)  LR: 9.964e-04  Data: 0.016 (0.013)
Train: 23 [ 700/1251 ( 56%)]  Loss: 4.525 (4.31)  Time: 0.705s, 1452.82/s  (0.706s, 1449.49/s)  LR: 9.964e-04  Data: 0.009 (0.013)
Train: 23 [ 750/1251 ( 60%)]  Loss: 4.542 (4.33)  Time: 0.843s, 1215.22/s  (0.706s, 1449.49/s)  LR: 9.964e-04  Data: 0.016 (0.013)
Train: 23 [ 800/1251 ( 64%)]  Loss: 4.282 (4.33)  Time: 0.705s, 1453.28/s  (0.706s, 1450.37/s)  LR: 9.964e-04  Data: 0.009 (0.013)
Train: 23 [ 850/1251 ( 68%)]  Loss: 4.657 (4.34)  Time: 0.688s, 1487.94/s  (0.705s, 1451.82/s)  LR: 9.964e-04  Data: 0.010 (0.012)
Train: 23 [ 900/1251 ( 72%)]  Loss: 4.245 (4.34)  Time: 0.666s, 1536.86/s  (0.705s, 1452.39/s)  LR: 9.964e-04  Data: 0.010 (0.012)
Train: 23 [ 950/1251 ( 76%)]  Loss: 4.454 (4.34)  Time: 0.668s, 1532.27/s  (0.704s, 1453.73/s)  LR: 9.964e-04  Data: 0.009 (0.012)
Train: 23 [1000/1251 ( 80%)]  Loss: 4.562 (4.36)  Time: 0.709s, 1445.06/s  (0.704s, 1453.91/s)  LR: 9.964e-04  Data: 0.009 (0.012)
Train: 23 [1050/1251 ( 84%)]  Loss: 4.337 (4.35)  Time: 0.715s, 1431.78/s  (0.704s, 1453.71/s)  LR: 9.964e-04  Data: 0.010 (0.012)
Train: 23 [1100/1251 ( 88%)]  Loss: 4.701 (4.37)  Time: 0.735s, 1393.61/s  (0.704s, 1453.64/s)  LR: 9.964e-04  Data: 0.010 (0.012)
Train: 23 [1150/1251 ( 92%)]  Loss: 4.273 (4.37)  Time: 0.716s, 1430.76/s  (0.704s, 1453.90/s)  LR: 9.964e-04  Data: 0.010 (0.012)
Train: 23 [1200/1251 ( 96%)]  Loss: 3.692 (4.34)  Time: 0.671s, 1525.52/s  (0.704s, 1453.87/s)  LR: 9.964e-04  Data: 0.010 (0.012)
Train: 23 [1250/1251 (100%)]  Loss: 4.201 (4.33)  Time: 0.659s, 1553.01/s  (0.704s, 1454.39/s)  LR: 9.964e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.509 (1.509)  Loss:  1.1895 (1.1895)  Acc@1: 80.3711 (80.3711)  Acc@5: 95.1172 (95.1172)
Test: [  48/48]  Time: 0.136 (0.586)  Loss:  1.1934 (1.9547)  Acc@1: 75.9434 (60.7700)  Acc@5: 92.3349 (84.0500)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-22.pth.tar', 61.03600001464844)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-23.pth.tar', 60.770000021972656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-21.pth.tar', 60.678000100097655)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-19.pth.tar', 60.21599998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-20.pth.tar', 58.643999997558595)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-17.pth.tar', 58.208)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-18.pth.tar', 57.31199999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-16.pth.tar', 56.398000021972656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-15.pth.tar', 55.33400008789062)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-14.pth.tar', 55.282000029296874)

Train: 24 [   0/1251 (  0%)]  Loss: 4.219 (4.22)  Time: 2.118s,  483.47/s  (2.118s,  483.47/s)  LR: 9.961e-04  Data: 1.501 (1.501)
Train: 24 [  50/1251 (  4%)]  Loss: 3.886 (4.05)  Time: 0.704s, 1455.24/s  (0.738s, 1387.65/s)  LR: 9.961e-04  Data: 0.009 (0.049)
Train: 24 [ 100/1251 (  8%)]  Loss: 4.187 (4.10)  Time: 0.723s, 1417.00/s  (0.719s, 1425.05/s)  LR: 9.961e-04  Data: 0.009 (0.030)
Train: 24 [ 150/1251 ( 12%)]  Loss: 4.133 (4.11)  Time: 0.707s, 1448.31/s  (0.713s, 1435.23/s)  LR: 9.961e-04  Data: 0.011 (0.024)
Train: 24 [ 200/1251 ( 16%)]  Loss: 4.427 (4.17)  Time: 0.674s, 1519.14/s  (0.710s, 1441.37/s)  LR: 9.961e-04  Data: 0.010 (0.020)
Train: 24 [ 250/1251 ( 20%)]  Loss: 4.033 (4.15)  Time: 0.727s, 1409.03/s  (0.709s, 1445.25/s)  LR: 9.961e-04  Data: 0.009 (0.018)
Train: 24 [ 300/1251 ( 24%)]  Loss: 4.292 (4.17)  Time: 0.673s, 1521.33/s  (0.708s, 1445.99/s)  LR: 9.961e-04  Data: 0.010 (0.017)
Train: 24 [ 350/1251 ( 28%)]  Loss: 4.364 (4.19)  Time: 0.703s, 1456.56/s  (0.706s, 1449.60/s)  LR: 9.961e-04  Data: 0.010 (0.016)
Train: 24 [ 400/1251 ( 32%)]  Loss: 4.314 (4.21)  Time: 0.674s, 1520.06/s  (0.705s, 1451.60/s)  LR: 9.961e-04  Data: 0.009 (0.015)
Train: 24 [ 450/1251 ( 36%)]  Loss: 4.437 (4.23)  Time: 0.673s, 1520.67/s  (0.705s, 1451.69/s)  LR: 9.961e-04  Data: 0.011 (0.015)
Train: 24 [ 500/1251 ( 40%)]  Loss: 4.521 (4.26)  Time: 0.675s, 1517.75/s  (0.706s, 1451.23/s)  LR: 9.961e-04  Data: 0.010 (0.014)
Train: 24 [ 550/1251 ( 44%)]  Loss: 3.885 (4.22)  Time: 0.672s, 1522.94/s  (0.705s, 1451.83/s)  LR: 9.961e-04  Data: 0.010 (0.014)
Train: 24 [ 600/1251 ( 48%)]  Loss: 4.232 (4.23)  Time: 0.707s, 1448.69/s  (0.705s, 1453.03/s)  LR: 9.961e-04  Data: 0.009 (0.014)
Train: 24 [ 650/1251 ( 52%)]  Loss: 4.306 (4.23)  Time: 0.676s, 1515.26/s  (0.704s, 1454.68/s)  LR: 9.961e-04  Data: 0.010 (0.013)
Train: 24 [ 700/1251 ( 56%)]  Loss: 4.154 (4.23)  Time: 0.677s, 1512.85/s  (0.703s, 1456.27/s)  LR: 9.961e-04  Data: 0.010 (0.013)
Train: 24 [ 750/1251 ( 60%)]  Loss: 4.111 (4.22)  Time: 0.672s, 1523.20/s  (0.703s, 1457.58/s)  LR: 9.961e-04  Data: 0.010 (0.013)
Train: 24 [ 800/1251 ( 64%)]  Loss: 4.299 (4.22)  Time: 0.675s, 1516.14/s  (0.702s, 1457.80/s)  LR: 9.961e-04  Data: 0.010 (0.013)
Train: 24 [ 850/1251 ( 68%)]  Loss: 3.883 (4.20)  Time: 0.708s, 1445.85/s  (0.702s, 1458.58/s)  LR: 9.961e-04  Data: 0.010 (0.013)
Train: 24 [ 900/1251 ( 72%)]  Loss: 4.367 (4.21)  Time: 0.684s, 1497.52/s  (0.702s, 1458.14/s)  LR: 9.961e-04  Data: 0.009 (0.012)
Train: 24 [ 950/1251 ( 76%)]  Loss: 4.550 (4.23)  Time: 0.733s, 1397.47/s  (0.702s, 1457.78/s)  LR: 9.961e-04  Data: 0.009 (0.012)
Train: 24 [1000/1251 ( 80%)]  Loss: 4.387 (4.24)  Time: 0.672s, 1523.81/s  (0.702s, 1458.20/s)  LR: 9.961e-04  Data: 0.010 (0.012)
Train: 24 [1050/1251 ( 84%)]  Loss: 4.468 (4.25)  Time: 0.705s, 1453.17/s  (0.702s, 1459.24/s)  LR: 9.961e-04  Data: 0.009 (0.012)
Train: 24 [1100/1251 ( 88%)]  Loss: 4.371 (4.25)  Time: 0.668s, 1532.36/s  (0.702s, 1459.58/s)  LR: 9.961e-04  Data: 0.009 (0.012)
Train: 24 [1150/1251 ( 92%)]  Loss: 4.444 (4.26)  Time: 0.703s, 1456.69/s  (0.701s, 1460.48/s)  LR: 9.961e-04  Data: 0.010 (0.012)
Train: 24 [1200/1251 ( 96%)]  Loss: 4.637 (4.28)  Time: 0.722s, 1417.32/s  (0.701s, 1460.08/s)  LR: 9.961e-04  Data: 0.009 (0.012)
Train: 24 [1250/1251 (100%)]  Loss: 4.104 (4.27)  Time: 0.681s, 1504.49/s  (0.701s, 1459.88/s)  LR: 9.961e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.488 (1.488)  Loss:  1.1953 (1.1953)  Acc@1: 81.7383 (81.7383)  Acc@5: 93.9453 (93.9453)
Test: [  48/48]  Time: 0.136 (0.566)  Loss:  1.2754 (1.9136)  Acc@1: 77.9481 (61.2700)  Acc@5: 91.0377 (84.2120)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-24.pth.tar', 61.2700001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-22.pth.tar', 61.03600001464844)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-23.pth.tar', 60.770000021972656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-21.pth.tar', 60.678000100097655)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-19.pth.tar', 60.21599998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-20.pth.tar', 58.643999997558595)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-17.pth.tar', 58.208)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-18.pth.tar', 57.31199999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-16.pth.tar', 56.398000021972656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-15.pth.tar', 55.33400008789062)

Train: 25 [   0/1251 (  0%)]  Loss: 4.441 (4.44)  Time: 2.110s,  485.32/s  (2.110s,  485.32/s)  LR: 9.958e-04  Data: 1.455 (1.455)
Train: 25 [  50/1251 (  4%)]  Loss: 4.408 (4.42)  Time: 0.725s, 1412.96/s  (0.735s, 1392.34/s)  LR: 9.958e-04  Data: 0.009 (0.045)
Train: 25 [ 100/1251 (  8%)]  Loss: 3.950 (4.27)  Time: 0.704s, 1453.96/s  (0.718s, 1425.64/s)  LR: 9.958e-04  Data: 0.009 (0.028)
Train: 25 [ 150/1251 ( 12%)]  Loss: 4.006 (4.20)  Time: 0.705s, 1452.82/s  (0.711s, 1439.23/s)  LR: 9.958e-04  Data: 0.009 (0.022)
Train: 25 [ 200/1251 ( 16%)]  Loss: 4.088 (4.18)  Time: 0.750s, 1364.63/s  (0.708s, 1445.99/s)  LR: 9.958e-04  Data: 0.010 (0.019)
Train: 25 [ 250/1251 ( 20%)]  Loss: 4.114 (4.17)  Time: 0.675s, 1517.85/s  (0.706s, 1450.48/s)  LR: 9.958e-04  Data: 0.014 (0.017)
Train: 25 [ 300/1251 ( 24%)]  Loss: 3.842 (4.12)  Time: 0.809s, 1265.13/s  (0.705s, 1452.42/s)  LR: 9.958e-04  Data: 0.009 (0.016)
Train: 25 [ 350/1251 ( 28%)]  Loss: 4.049 (4.11)  Time: 0.709s, 1444.85/s  (0.706s, 1450.64/s)  LR: 9.958e-04  Data: 0.013 (0.015)
Train: 25 [ 400/1251 ( 32%)]  Loss: 3.853 (4.08)  Time: 0.704s, 1454.60/s  (0.707s, 1447.71/s)  LR: 9.958e-04  Data: 0.009 (0.015)
Train: 25 [ 450/1251 ( 36%)]  Loss: 4.533 (4.13)  Time: 0.732s, 1398.42/s  (0.707s, 1448.78/s)  LR: 9.958e-04  Data: 0.009 (0.014)
Train: 25 [ 500/1251 ( 40%)]  Loss: 4.122 (4.13)  Time: 0.674s, 1520.01/s  (0.705s, 1451.55/s)  LR: 9.958e-04  Data: 0.013 (0.014)
Train: 25 [ 550/1251 ( 44%)]  Loss: 4.330 (4.14)  Time: 0.706s, 1451.13/s  (0.705s, 1452.84/s)  LR: 9.958e-04  Data: 0.010 (0.013)
Train: 25 [ 600/1251 ( 48%)]  Loss: 4.284 (4.16)  Time: 0.718s, 1425.58/s  (0.704s, 1454.96/s)  LR: 9.958e-04  Data: 0.010 (0.013)
Train: 25 [ 650/1251 ( 52%)]  Loss: 4.358 (4.17)  Time: 0.684s, 1496.90/s  (0.703s, 1455.98/s)  LR: 9.958e-04  Data: 0.010 (0.013)
Train: 25 [ 700/1251 ( 56%)]  Loss: 4.435 (4.19)  Time: 0.691s, 1481.58/s  (0.704s, 1455.54/s)  LR: 9.958e-04  Data: 0.010 (0.013)
Train: 25 [ 750/1251 ( 60%)]  Loss: 4.257 (4.19)  Time: 0.667s, 1535.62/s  (0.703s, 1455.66/s)  LR: 9.958e-04  Data: 0.010 (0.013)
Train: 25 [ 800/1251 ( 64%)]  Loss: 4.184 (4.19)  Time: 0.667s, 1534.36/s  (0.703s, 1456.75/s)  LR: 9.958e-04  Data: 0.011 (0.012)
Train: 25 [ 850/1251 ( 68%)]  Loss: 4.400 (4.20)  Time: 0.706s, 1450.19/s  (0.702s, 1458.02/s)  LR: 9.958e-04  Data: 0.009 (0.012)
Train: 25 [ 900/1251 ( 72%)]  Loss: 4.063 (4.20)  Time: 0.670s, 1528.57/s  (0.702s, 1458.64/s)  LR: 9.958e-04  Data: 0.010 (0.012)
Train: 25 [ 950/1251 ( 76%)]  Loss: 4.124 (4.19)  Time: 0.707s, 1447.78/s  (0.702s, 1458.53/s)  LR: 9.958e-04  Data: 0.011 (0.012)
Train: 25 [1000/1251 ( 80%)]  Loss: 4.419 (4.20)  Time: 0.705s, 1452.47/s  (0.702s, 1458.30/s)  LR: 9.958e-04  Data: 0.009 (0.012)
Train: 25 [1050/1251 ( 84%)]  Loss: 4.019 (4.19)  Time: 0.685s, 1495.68/s  (0.702s, 1458.51/s)  LR: 9.958e-04  Data: 0.010 (0.012)
Train: 25 [1100/1251 ( 88%)]  Loss: 4.410 (4.20)  Time: 0.737s, 1390.30/s  (0.702s, 1458.15/s)  LR: 9.958e-04  Data: 0.009 (0.012)
Train: 25 [1150/1251 ( 92%)]  Loss: 4.581 (4.22)  Time: 0.718s, 1425.46/s  (0.702s, 1458.15/s)  LR: 9.958e-04  Data: 0.012 (0.012)
Train: 25 [1200/1251 ( 96%)]  Loss: 4.037 (4.21)  Time: 0.674s, 1519.27/s  (0.702s, 1459.32/s)  LR: 9.958e-04  Data: 0.010 (0.012)
Train: 25 [1250/1251 (100%)]  Loss: 4.308 (4.22)  Time: 0.674s, 1518.68/s  (0.701s, 1459.82/s)  LR: 9.958e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.547 (1.547)  Loss:  1.1006 (1.1006)  Acc@1: 82.2266 (82.2266)  Acc@5: 95.2148 (95.2148)
Test: [  48/48]  Time: 0.136 (0.584)  Loss:  1.3184 (1.8902)  Acc@1: 76.4151 (62.0560)  Acc@5: 92.2170 (84.9740)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-25.pth.tar', 62.055999968261716)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-24.pth.tar', 61.2700001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-22.pth.tar', 61.03600001464844)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-23.pth.tar', 60.770000021972656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-21.pth.tar', 60.678000100097655)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-19.pth.tar', 60.21599998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-20.pth.tar', 58.643999997558595)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-17.pth.tar', 58.208)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-18.pth.tar', 57.31199999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-16.pth.tar', 56.398000021972656)

Train: 26 [   0/1251 (  0%)]  Loss: 4.255 (4.26)  Time: 2.149s,  476.43/s  (2.149s,  476.43/s)  LR: 9.954e-04  Data: 1.533 (1.533)
Train: 26 [  50/1251 (  4%)]  Loss: 4.391 (4.32)  Time: 0.754s, 1358.78/s  (0.726s, 1410.11/s)  LR: 9.954e-04  Data: 0.009 (0.046)
Train: 26 [ 100/1251 (  8%)]  Loss: 4.031 (4.23)  Time: 0.679s, 1509.04/s  (0.714s, 1435.17/s)  LR: 9.954e-04  Data: 0.012 (0.028)
Train: 26 [ 150/1251 ( 12%)]  Loss: 3.923 (4.15)  Time: 0.703s, 1456.50/s  (0.707s, 1447.65/s)  LR: 9.954e-04  Data: 0.010 (0.022)
Train: 26 [ 200/1251 ( 16%)]  Loss: 4.251 (4.17)  Time: 0.676s, 1515.68/s  (0.706s, 1450.38/s)  LR: 9.954e-04  Data: 0.010 (0.019)
Train: 26 [ 250/1251 ( 20%)]  Loss: 4.187 (4.17)  Time: 0.701s, 1460.98/s  (0.704s, 1453.55/s)  LR: 9.954e-04  Data: 0.011 (0.017)
Train: 26 [ 300/1251 ( 24%)]  Loss: 4.245 (4.18)  Time: 0.721s, 1420.30/s  (0.704s, 1454.03/s)  LR: 9.954e-04  Data: 0.011 (0.016)
Train: 26 [ 350/1251 ( 28%)]  Loss: 4.120 (4.18)  Time: 0.708s, 1446.92/s  (0.704s, 1455.40/s)  LR: 9.954e-04  Data: 0.012 (0.016)
Train: 26 [ 400/1251 ( 32%)]  Loss: 4.327 (4.19)  Time: 0.682s, 1502.06/s  (0.703s, 1456.91/s)  LR: 9.954e-04  Data: 0.010 (0.015)
Train: 26 [ 450/1251 ( 36%)]  Loss: 4.487 (4.22)  Time: 0.725s, 1412.81/s  (0.702s, 1457.70/s)  LR: 9.954e-04  Data: 0.009 (0.014)
Train: 26 [ 500/1251 ( 40%)]  Loss: 3.994 (4.20)  Time: 0.706s, 1450.27/s  (0.703s, 1456.30/s)  LR: 9.954e-04  Data: 0.009 (0.014)
Train: 26 [ 550/1251 ( 44%)]  Loss: 4.640 (4.24)  Time: 0.708s, 1446.47/s  (0.704s, 1455.41/s)  LR: 9.954e-04  Data: 0.010 (0.014)
Train: 26 [ 600/1251 ( 48%)]  Loss: 4.652 (4.27)  Time: 0.671s, 1525.76/s  (0.703s, 1455.68/s)  LR: 9.954e-04  Data: 0.009 (0.014)
Train: 26 [ 650/1251 ( 52%)]  Loss: 4.292 (4.27)  Time: 0.702s, 1458.43/s  (0.703s, 1456.95/s)  LR: 9.954e-04  Data: 0.009 (0.013)
Train: 26 [ 700/1251 ( 56%)]  Loss: 4.093 (4.26)  Time: 0.703s, 1455.84/s  (0.702s, 1457.90/s)  LR: 9.954e-04  Data: 0.009 (0.013)
Train: 26 [ 750/1251 ( 60%)]  Loss: 4.475 (4.27)  Time: 0.675s, 1516.03/s  (0.702s, 1458.44/s)  LR: 9.954e-04  Data: 0.010 (0.013)
Train: 26 [ 800/1251 ( 64%)]  Loss: 4.585 (4.29)  Time: 0.706s, 1450.84/s  (0.702s, 1458.24/s)  LR: 9.954e-04  Data: 0.009 (0.013)
Train: 26 [ 850/1251 ( 68%)]  Loss: 4.375 (4.30)  Time: 0.674s, 1519.53/s  (0.702s, 1458.00/s)  LR: 9.954e-04  Data: 0.010 (0.013)
Train: 26 [ 900/1251 ( 72%)]  Loss: 3.861 (4.27)  Time: 0.674s, 1519.78/s  (0.702s, 1457.79/s)  LR: 9.954e-04  Data: 0.010 (0.012)
Train: 26 [ 950/1251 ( 76%)]  Loss: 4.494 (4.28)  Time: 0.677s, 1513.47/s  (0.702s, 1458.19/s)  LR: 9.954e-04  Data: 0.011 (0.012)
Train: 26 [1000/1251 ( 80%)]  Loss: 4.281 (4.28)  Time: 0.698s, 1467.08/s  (0.702s, 1458.42/s)  LR: 9.954e-04  Data: 0.012 (0.012)
Train: 26 [1050/1251 ( 84%)]  Loss: 4.501 (4.29)  Time: 0.696s, 1471.20/s  (0.702s, 1459.40/s)  LR: 9.954e-04  Data: 0.009 (0.012)
Train: 26 [1100/1251 ( 88%)]  Loss: 4.133 (4.29)  Time: 0.702s, 1458.37/s  (0.701s, 1460.14/s)  LR: 9.954e-04  Data: 0.010 (0.012)
Train: 26 [1150/1251 ( 92%)]  Loss: 4.529 (4.30)  Time: 0.706s, 1450.36/s  (0.701s, 1460.41/s)  LR: 9.954e-04  Data: 0.010 (0.012)
Train: 26 [1200/1251 ( 96%)]  Loss: 4.323 (4.30)  Time: 0.756s, 1354.77/s  (0.701s, 1460.37/s)  LR: 9.954e-04  Data: 0.009 (0.012)
Train: 26 [1250/1251 (100%)]  Loss: 4.384 (4.30)  Time: 0.700s, 1462.76/s  (0.701s, 1460.42/s)  LR: 9.954e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.567 (1.567)  Loss:  1.2109 (1.2109)  Acc@1: 81.1523 (81.1523)  Acc@5: 94.2383 (94.2383)
Test: [  48/48]  Time: 0.136 (0.582)  Loss:  1.2676 (1.9001)  Acc@1: 77.5943 (62.4700)  Acc@5: 92.9245 (85.1020)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-26.pth.tar', 62.47000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-25.pth.tar', 62.055999968261716)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-24.pth.tar', 61.2700001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-22.pth.tar', 61.03600001464844)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-23.pth.tar', 60.770000021972656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-21.pth.tar', 60.678000100097655)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-19.pth.tar', 60.21599998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-20.pth.tar', 58.643999997558595)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-17.pth.tar', 58.208)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-18.pth.tar', 57.31199999511719)

Train: 27 [   0/1251 (  0%)]  Loss: 4.036 (4.04)  Time: 2.217s,  461.93/s  (2.217s,  461.93/s)  LR: 9.951e-04  Data: 1.577 (1.577)
Train: 27 [  50/1251 (  4%)]  Loss: 4.073 (4.05)  Time: 0.729s, 1404.83/s  (0.740s, 1383.52/s)  LR: 9.951e-04  Data: 0.009 (0.046)
Train: 27 [ 100/1251 (  8%)]  Loss: 4.529 (4.21)  Time: 0.699s, 1465.25/s  (0.719s, 1424.03/s)  LR: 9.951e-04  Data: 0.010 (0.028)
Train: 27 [ 150/1251 ( 12%)]  Loss: 4.365 (4.25)  Time: 0.669s, 1531.62/s  (0.710s, 1441.27/s)  LR: 9.951e-04  Data: 0.012 (0.022)
Train: 27 [ 200/1251 ( 16%)]  Loss: 4.234 (4.25)  Time: 0.674s, 1519.20/s  (0.708s, 1447.33/s)  LR: 9.951e-04  Data: 0.010 (0.019)
Train: 27 [ 250/1251 ( 20%)]  Loss: 4.049 (4.21)  Time: 0.727s, 1409.09/s  (0.708s, 1446.83/s)  LR: 9.951e-04  Data: 0.010 (0.017)
Train: 27 [ 300/1251 ( 24%)]  Loss: 4.413 (4.24)  Time: 0.679s, 1508.02/s  (0.706s, 1449.53/s)  LR: 9.951e-04  Data: 0.009 (0.016)
Train: 27 [ 350/1251 ( 28%)]  Loss: 3.779 (4.18)  Time: 0.729s, 1405.02/s  (0.706s, 1449.54/s)  LR: 9.951e-04  Data: 0.009 (0.015)
Train: 27 [ 400/1251 ( 32%)]  Loss: 4.208 (4.19)  Time: 0.665s, 1538.87/s  (0.707s, 1449.04/s)  LR: 9.951e-04  Data: 0.009 (0.015)
Train: 27 [ 450/1251 ( 36%)]  Loss: 4.705 (4.24)  Time: 0.680s, 1506.98/s  (0.706s, 1449.75/s)  LR: 9.951e-04  Data: 0.010 (0.014)
Train: 27 [ 500/1251 ( 40%)]  Loss: 4.256 (4.24)  Time: 0.725s, 1412.31/s  (0.706s, 1449.51/s)  LR: 9.951e-04  Data: 0.010 (0.014)
Train: 27 [ 550/1251 ( 44%)]  Loss: 4.075 (4.23)  Time: 0.671s, 1526.50/s  (0.706s, 1450.82/s)  LR: 9.951e-04  Data: 0.009 (0.014)
Train: 27 [ 600/1251 ( 48%)]  Loss: 4.055 (4.21)  Time: 0.709s, 1444.29/s  (0.704s, 1453.98/s)  LR: 9.951e-04  Data: 0.010 (0.013)
Train: 27 [ 650/1251 ( 52%)]  Loss: 4.129 (4.21)  Time: 0.670s, 1528.14/s  (0.704s, 1455.03/s)  LR: 9.951e-04  Data: 0.009 (0.013)
Train: 27 [ 700/1251 ( 56%)]  Loss: 4.513 (4.23)  Time: 0.694s, 1475.93/s  (0.704s, 1455.44/s)  LR: 9.951e-04  Data: 0.013 (0.013)
Train: 27 [ 750/1251 ( 60%)]  Loss: 4.416 (4.24)  Time: 0.834s, 1227.76/s  (0.704s, 1455.50/s)  LR: 9.951e-04  Data: 0.009 (0.013)
Train: 27 [ 800/1251 ( 64%)]  Loss: 4.405 (4.25)  Time: 0.699s, 1464.17/s  (0.703s, 1455.96/s)  LR: 9.951e-04  Data: 0.009 (0.012)
Train: 27 [ 850/1251 ( 68%)]  Loss: 4.723 (4.28)  Time: 0.735s, 1392.66/s  (0.703s, 1456.32/s)  LR: 9.951e-04  Data: 0.009 (0.012)
Train: 27 [ 900/1251 ( 72%)]  Loss: 4.297 (4.28)  Time: 0.675s, 1516.46/s  (0.702s, 1457.92/s)  LR: 9.951e-04  Data: 0.010 (0.012)
Train: 27 [ 950/1251 ( 76%)]  Loss: 4.348 (4.28)  Time: 0.674s, 1520.08/s  (0.702s, 1458.40/s)  LR: 9.951e-04  Data: 0.009 (0.012)
Train: 27 [1000/1251 ( 80%)]  Loss: 4.297 (4.28)  Time: 0.694s, 1476.30/s  (0.702s, 1458.28/s)  LR: 9.951e-04  Data: 0.009 (0.012)
Train: 27 [1050/1251 ( 84%)]  Loss: 4.140 (4.27)  Time: 0.769s, 1331.93/s  (0.702s, 1457.91/s)  LR: 9.951e-04  Data: 0.010 (0.012)
Train: 27 [1100/1251 ( 88%)]  Loss: 4.379 (4.28)  Time: 0.676s, 1515.70/s  (0.702s, 1458.34/s)  LR: 9.951e-04  Data: 0.009 (0.012)
Train: 27 [1150/1251 ( 92%)]  Loss: 4.283 (4.28)  Time: 0.694s, 1474.76/s  (0.702s, 1458.40/s)  LR: 9.951e-04  Data: 0.009 (0.012)
Train: 27 [1200/1251 ( 96%)]  Loss: 4.267 (4.28)  Time: 0.702s, 1458.04/s  (0.702s, 1458.50/s)  LR: 9.951e-04  Data: 0.022 (0.012)
Train: 27 [1250/1251 (100%)]  Loss: 4.055 (4.27)  Time: 0.673s, 1522.57/s  (0.702s, 1458.53/s)  LR: 9.951e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.587 (1.587)  Loss:  0.9980 (0.9980)  Acc@1: 82.3242 (82.3242)  Acc@5: 95.0195 (95.0195)
Test: [  48/48]  Time: 0.136 (0.586)  Loss:  1.1914 (1.7907)  Acc@1: 76.6509 (63.5480)  Acc@5: 92.0991 (85.7440)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-27.pth.tar', 63.54799994140625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-26.pth.tar', 62.47000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-25.pth.tar', 62.055999968261716)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-24.pth.tar', 61.2700001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-22.pth.tar', 61.03600001464844)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-23.pth.tar', 60.770000021972656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-21.pth.tar', 60.678000100097655)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-19.pth.tar', 60.21599998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-20.pth.tar', 58.643999997558595)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-17.pth.tar', 58.208)

Train: 28 [   0/1251 (  0%)]  Loss: 4.166 (4.17)  Time: 2.354s,  434.99/s  (2.354s,  434.99/s)  LR: 9.947e-04  Data: 1.707 (1.707)
Train: 28 [  50/1251 (  4%)]  Loss: 3.826 (4.00)  Time: 0.731s, 1400.63/s  (0.732s, 1398.63/s)  LR: 9.947e-04  Data: 0.009 (0.047)
Train: 28 [ 100/1251 (  8%)]  Loss: 4.322 (4.10)  Time: 0.672s, 1524.23/s  (0.716s, 1430.24/s)  LR: 9.947e-04  Data: 0.010 (0.029)
Train: 28 [ 150/1251 ( 12%)]  Loss: 4.212 (4.13)  Time: 0.681s, 1504.46/s  (0.712s, 1437.50/s)  LR: 9.947e-04  Data: 0.011 (0.023)
Train: 28 [ 200/1251 ( 16%)]  Loss: 4.121 (4.13)  Time: 0.736s, 1390.94/s  (0.710s, 1443.20/s)  LR: 9.947e-04  Data: 0.010 (0.020)
Train: 28 [ 250/1251 ( 20%)]  Loss: 3.966 (4.10)  Time: 0.706s, 1449.77/s  (0.707s, 1447.65/s)  LR: 9.947e-04  Data: 0.010 (0.018)
Train: 28 [ 300/1251 ( 24%)]  Loss: 4.333 (4.14)  Time: 0.675s, 1516.03/s  (0.707s, 1449.32/s)  LR: 9.947e-04  Data: 0.009 (0.016)
Train: 28 [ 350/1251 ( 28%)]  Loss: 4.282 (4.15)  Time: 0.684s, 1496.46/s  (0.706s, 1449.65/s)  LR: 9.947e-04  Data: 0.009 (0.016)
Train: 28 [ 400/1251 ( 32%)]  Loss: 4.035 (4.14)  Time: 0.674s, 1519.52/s  (0.705s, 1451.78/s)  LR: 9.947e-04  Data: 0.009 (0.015)
Train: 28 [ 450/1251 ( 36%)]  Loss: 4.307 (4.16)  Time: 0.668s, 1533.48/s  (0.705s, 1453.29/s)  LR: 9.947e-04  Data: 0.009 (0.014)
Train: 28 [ 500/1251 ( 40%)]  Loss: 3.675 (4.11)  Time: 0.809s, 1266.31/s  (0.705s, 1452.66/s)  LR: 9.947e-04  Data: 0.009 (0.014)
Train: 28 [ 550/1251 ( 44%)]  Loss: 3.974 (4.10)  Time: 0.716s, 1429.87/s  (0.705s, 1452.11/s)  LR: 9.947e-04  Data: 0.010 (0.014)
Train: 28 [ 600/1251 ( 48%)]  Loss: 4.516 (4.13)  Time: 0.701s, 1461.60/s  (0.704s, 1454.58/s)  LR: 9.947e-04  Data: 0.011 (0.013)
Train: 28 [ 650/1251 ( 52%)]  Loss: 4.320 (4.15)  Time: 0.687s, 1489.79/s  (0.703s, 1456.75/s)  LR: 9.947e-04  Data: 0.010 (0.013)
Train: 28 [ 700/1251 ( 56%)]  Loss: 4.476 (4.17)  Time: 0.695s, 1472.93/s  (0.702s, 1457.86/s)  LR: 9.947e-04  Data: 0.010 (0.013)
Train: 28 [ 750/1251 ( 60%)]  Loss: 4.088 (4.16)  Time: 0.675s, 1517.68/s  (0.702s, 1458.67/s)  LR: 9.947e-04  Data: 0.013 (0.013)
Train: 28 [ 800/1251 ( 64%)]  Loss: 4.357 (4.18)  Time: 0.710s, 1441.55/s  (0.702s, 1458.01/s)  LR: 9.947e-04  Data: 0.009 (0.013)
Train: 28 [ 850/1251 ( 68%)]  Loss: 3.984 (4.16)  Time: 0.681s, 1504.26/s  (0.702s, 1457.86/s)  LR: 9.947e-04  Data: 0.010 (0.012)
Train: 28 [ 900/1251 ( 72%)]  Loss: 3.960 (4.15)  Time: 0.683s, 1499.69/s  (0.702s, 1458.60/s)  LR: 9.947e-04  Data: 0.011 (0.012)
Train: 28 [ 950/1251 ( 76%)]  Loss: 4.415 (4.17)  Time: 0.707s, 1448.30/s  (0.702s, 1458.15/s)  LR: 9.947e-04  Data: 0.009 (0.012)
Train: 28 [1000/1251 ( 80%)]  Loss: 4.188 (4.17)  Time: 0.674s, 1518.89/s  (0.702s, 1458.70/s)  LR: 9.947e-04  Data: 0.010 (0.012)
Train: 28 [1050/1251 ( 84%)]  Loss: 3.942 (4.16)  Time: 0.763s, 1342.28/s  (0.702s, 1459.24/s)  LR: 9.947e-04  Data: 0.009 (0.012)
Train: 28 [1100/1251 ( 88%)]  Loss: 4.453 (4.17)  Time: 0.733s, 1397.24/s  (0.702s, 1459.32/s)  LR: 9.947e-04  Data: 0.009 (0.012)
Train: 28 [1150/1251 ( 92%)]  Loss: 3.878 (4.16)  Time: 0.704s, 1454.45/s  (0.702s, 1459.69/s)  LR: 9.947e-04  Data: 0.011 (0.012)
Train: 28 [1200/1251 ( 96%)]  Loss: 4.268 (4.16)  Time: 0.674s, 1519.80/s  (0.701s, 1460.10/s)  LR: 9.947e-04  Data: 0.010 (0.012)
Train: 28 [1250/1251 (100%)]  Loss: 3.917 (4.15)  Time: 0.691s, 1482.42/s  (0.702s, 1459.41/s)  LR: 9.947e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.487 (1.487)  Loss:  1.3506 (1.3506)  Acc@1: 79.5898 (79.5898)  Acc@5: 94.7266 (94.7266)
Test: [  48/48]  Time: 0.136 (0.611)  Loss:  1.2383 (1.9206)  Acc@1: 77.5943 (62.9780)  Acc@5: 92.4528 (85.2740)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-27.pth.tar', 63.54799994140625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-28.pth.tar', 62.978000092773435)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-26.pth.tar', 62.47000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-25.pth.tar', 62.055999968261716)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-24.pth.tar', 61.2700001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-22.pth.tar', 61.03600001464844)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-23.pth.tar', 60.770000021972656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-21.pth.tar', 60.678000100097655)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-19.pth.tar', 60.21599998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-20.pth.tar', 58.643999997558595)

Train: 29 [   0/1251 (  0%)]  Loss: 4.563 (4.56)  Time: 2.486s,  411.86/s  (2.486s,  411.86/s)  LR: 9.943e-04  Data: 1.846 (1.846)
Train: 29 [  50/1251 (  4%)]  Loss: 4.292 (4.43)  Time: 0.764s, 1340.05/s  (0.768s, 1333.92/s)  LR: 9.943e-04  Data: 0.010 (0.056)
Train: 29 [ 100/1251 (  8%)]  Loss: 4.166 (4.34)  Time: 0.735s, 1393.06/s  (0.740s, 1383.54/s)  LR: 9.943e-04  Data: 0.011 (0.034)
Train: 29 [ 150/1251 ( 12%)]  Loss: 4.148 (4.29)  Time: 0.673s, 1521.54/s  (0.729s, 1404.50/s)  LR: 9.943e-04  Data: 0.010 (0.026)
Train: 29 [ 200/1251 ( 16%)]  Loss: 4.603 (4.35)  Time: 0.698s, 1466.02/s  (0.720s, 1422.61/s)  LR: 9.943e-04  Data: 0.009 (0.022)
Train: 29 [ 250/1251 ( 20%)]  Loss: 4.261 (4.34)  Time: 0.701s, 1461.12/s  (0.715s, 1431.61/s)  LR: 9.943e-04  Data: 0.009 (0.020)
Train: 29 [ 300/1251 ( 24%)]  Loss: 3.895 (4.28)  Time: 0.721s, 1420.58/s  (0.713s, 1435.77/s)  LR: 9.943e-04  Data: 0.010 (0.018)
Train: 29 [ 350/1251 ( 28%)]  Loss: 4.139 (4.26)  Time: 0.701s, 1461.54/s  (0.713s, 1435.63/s)  LR: 9.943e-04  Data: 0.009 (0.017)
Train: 29 [ 400/1251 ( 32%)]  Loss: 4.034 (4.23)  Time: 0.727s, 1409.25/s  (0.711s, 1439.26/s)  LR: 9.943e-04  Data: 0.008 (0.016)
Train: 29 [ 450/1251 ( 36%)]  Loss: 3.925 (4.20)  Time: 0.667s, 1534.49/s  (0.711s, 1439.89/s)  LR: 9.943e-04  Data: 0.010 (0.015)
Train: 29 [ 500/1251 ( 40%)]  Loss: 4.189 (4.20)  Time: 0.712s, 1439.21/s  (0.710s, 1443.01/s)  LR: 9.943e-04  Data: 0.011 (0.015)
Train: 29 [ 550/1251 ( 44%)]  Loss: 4.254 (4.21)  Time: 0.673s, 1522.44/s  (0.708s, 1445.32/s)  LR: 9.943e-04  Data: 0.012 (0.014)
Train: 29 [ 600/1251 ( 48%)]  Loss: 4.394 (4.22)  Time: 0.690s, 1483.65/s  (0.708s, 1447.13/s)  LR: 9.943e-04  Data: 0.009 (0.014)
Train: 29 [ 650/1251 ( 52%)]  Loss: 4.000 (4.20)  Time: 0.679s, 1508.49/s  (0.707s, 1447.89/s)  LR: 9.943e-04  Data: 0.011 (0.014)
Train: 29 [ 700/1251 ( 56%)]  Loss: 3.851 (4.18)  Time: 0.695s, 1472.92/s  (0.707s, 1448.55/s)  LR: 9.943e-04  Data: 0.011 (0.014)
Train: 29 [ 750/1251 ( 60%)]  Loss: 4.525 (4.20)  Time: 0.704s, 1454.47/s  (0.707s, 1448.52/s)  LR: 9.943e-04  Data: 0.009 (0.013)
Train: 29 [ 800/1251 ( 64%)]  Loss: 4.417 (4.22)  Time: 0.672s, 1523.92/s  (0.706s, 1449.95/s)  LR: 9.943e-04  Data: 0.010 (0.013)
Train: 29 [ 850/1251 ( 68%)]  Loss: 4.223 (4.22)  Time: 0.716s, 1431.03/s  (0.706s, 1450.48/s)  LR: 9.943e-04  Data: 0.010 (0.013)
Train: 29 [ 900/1251 ( 72%)]  Loss: 4.504 (4.23)  Time: 0.703s, 1456.20/s  (0.706s, 1451.29/s)  LR: 9.943e-04  Data: 0.010 (0.013)
Train: 29 [ 950/1251 ( 76%)]  Loss: 4.319 (4.24)  Time: 0.739s, 1385.75/s  (0.706s, 1450.75/s)  LR: 9.943e-04  Data: 0.009 (0.013)
Train: 29 [1000/1251 ( 80%)]  Loss: 4.019 (4.22)  Time: 0.710s, 1443.07/s  (0.705s, 1451.50/s)  LR: 9.943e-04  Data: 0.009 (0.012)
Train: 29 [1050/1251 ( 84%)]  Loss: 4.719 (4.25)  Time: 0.704s, 1453.79/s  (0.705s, 1452.04/s)  LR: 9.943e-04  Data: 0.009 (0.012)
Train: 29 [1100/1251 ( 88%)]  Loss: 4.010 (4.24)  Time: 0.684s, 1496.13/s  (0.705s, 1451.99/s)  LR: 9.943e-04  Data: 0.009 (0.012)
Train: 29 [1150/1251 ( 92%)]  Loss: 4.495 (4.25)  Time: 0.703s, 1457.40/s  (0.705s, 1451.80/s)  LR: 9.943e-04  Data: 0.009 (0.012)
Train: 29 [1200/1251 ( 96%)]  Loss: 3.952 (4.24)  Time: 0.697s, 1468.67/s  (0.705s, 1452.26/s)  LR: 9.943e-04  Data: 0.013 (0.012)
Train: 29 [1250/1251 (100%)]  Loss: 4.145 (4.23)  Time: 0.671s, 1527.02/s  (0.705s, 1453.41/s)  LR: 9.943e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.484 (1.484)  Loss:  1.1191 (1.1191)  Acc@1: 81.4453 (81.4453)  Acc@5: 95.0195 (95.0195)
Test: [  48/48]  Time: 0.138 (0.593)  Loss:  1.3506 (1.9468)  Acc@1: 79.5991 (62.9500)  Acc@5: 93.7500 (85.4280)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-27.pth.tar', 63.54799994140625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-28.pth.tar', 62.978000092773435)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-29.pth.tar', 62.95000005859375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-26.pth.tar', 62.47000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-25.pth.tar', 62.055999968261716)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-24.pth.tar', 61.2700001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-22.pth.tar', 61.03600001464844)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-23.pth.tar', 60.770000021972656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-21.pth.tar', 60.678000100097655)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-19.pth.tar', 60.21599998046875)

Train: 30 [   0/1251 (  0%)]  Loss: 4.383 (4.38)  Time: 2.293s,  446.64/s  (2.293s,  446.64/s)  LR: 9.939e-04  Data: 1.636 (1.636)
Train: 30 [  50/1251 (  4%)]  Loss: 4.275 (4.33)  Time: 0.677s, 1513.63/s  (0.736s, 1390.54/s)  LR: 9.939e-04  Data: 0.009 (0.053)
Train: 30 [ 100/1251 (  8%)]  Loss: 3.952 (4.20)  Time: 0.671s, 1525.80/s  (0.716s, 1430.98/s)  LR: 9.939e-04  Data: 0.010 (0.032)
Train: 30 [ 150/1251 ( 12%)]  Loss: 3.904 (4.13)  Time: 0.742s, 1379.96/s  (0.709s, 1444.70/s)  LR: 9.939e-04  Data: 0.009 (0.024)
Train: 30 [ 200/1251 ( 16%)]  Loss: 4.427 (4.19)  Time: 0.667s, 1536.05/s  (0.709s, 1444.97/s)  LR: 9.939e-04  Data: 0.011 (0.021)
Train: 30 [ 250/1251 ( 20%)]  Loss: 4.208 (4.19)  Time: 0.672s, 1522.81/s  (0.708s, 1447.27/s)  LR: 9.939e-04  Data: 0.010 (0.019)
Train: 30 [ 300/1251 ( 24%)]  Loss: 4.295 (4.21)  Time: 0.706s, 1450.33/s  (0.705s, 1452.01/s)  LR: 9.939e-04  Data: 0.011 (0.017)
Train: 30 [ 350/1251 ( 28%)]  Loss: 4.341 (4.22)  Time: 0.709s, 1444.80/s  (0.704s, 1454.04/s)  LR: 9.939e-04  Data: 0.011 (0.016)
Train: 30 [ 400/1251 ( 32%)]  Loss: 4.108 (4.21)  Time: 0.706s, 1449.99/s  (0.704s, 1453.69/s)  LR: 9.939e-04  Data: 0.009 (0.015)
Train: 30 [ 450/1251 ( 36%)]  Loss: 4.764 (4.27)  Time: 0.687s, 1490.20/s  (0.704s, 1454.55/s)  LR: 9.939e-04  Data: 0.009 (0.015)
Train: 30 [ 500/1251 ( 40%)]  Loss: 4.206 (4.26)  Time: 0.679s, 1507.47/s  (0.704s, 1454.37/s)  LR: 9.939e-04  Data: 0.009 (0.014)
Train: 30 [ 550/1251 ( 44%)]  Loss: 3.989 (4.24)  Time: 0.706s, 1450.59/s  (0.704s, 1454.09/s)  LR: 9.939e-04  Data: 0.014 (0.014)
Train: 30 [ 600/1251 ( 48%)]  Loss: 4.182 (4.23)  Time: 0.681s, 1504.41/s  (0.705s, 1453.48/s)  LR: 9.939e-04  Data: 0.013 (0.014)
Train: 30 [ 650/1251 ( 52%)]  Loss: 4.345 (4.24)  Time: 0.672s, 1523.48/s  (0.704s, 1455.38/s)  LR: 9.939e-04  Data: 0.011 (0.013)
Train: 30 [ 700/1251 ( 56%)]  Loss: 4.309 (4.25)  Time: 0.696s, 1471.23/s  (0.703s, 1456.24/s)  LR: 9.939e-04  Data: 0.009 (0.013)
Train: 30 [ 750/1251 ( 60%)]  Loss: 4.139 (4.24)  Time: 0.688s, 1488.75/s  (0.703s, 1457.06/s)  LR: 9.939e-04  Data: 0.012 (0.013)
Train: 30 [ 800/1251 ( 64%)]  Loss: 4.248 (4.24)  Time: 0.680s, 1506.48/s  (0.703s, 1457.62/s)  LR: 9.939e-04  Data: 0.010 (0.013)
Train: 30 [ 850/1251 ( 68%)]  Loss: 4.101 (4.23)  Time: 0.720s, 1422.87/s  (0.703s, 1456.97/s)  LR: 9.939e-04  Data: 0.016 (0.013)
Train: 30 [ 900/1251 ( 72%)]  Loss: 4.320 (4.24)  Time: 0.674s, 1520.28/s  (0.703s, 1456.82/s)  LR: 9.939e-04  Data: 0.010 (0.013)
Train: 30 [ 950/1251 ( 76%)]  Loss: 4.283 (4.24)  Time: 0.680s, 1505.82/s  (0.703s, 1457.09/s)  LR: 9.939e-04  Data: 0.010 (0.012)
Train: 30 [1000/1251 ( 80%)]  Loss: 3.939 (4.22)  Time: 0.707s, 1447.37/s  (0.703s, 1457.36/s)  LR: 9.939e-04  Data: 0.009 (0.012)
Train: 30 [1050/1251 ( 84%)]  Loss: 3.871 (4.21)  Time: 0.705s, 1453.38/s  (0.702s, 1458.09/s)  LR: 9.939e-04  Data: 0.009 (0.012)
Train: 30 [1100/1251 ( 88%)]  Loss: 4.463 (4.22)  Time: 0.720s, 1422.67/s  (0.702s, 1458.70/s)  LR: 9.939e-04  Data: 0.009 (0.012)
Train: 30 [1150/1251 ( 92%)]  Loss: 4.130 (4.22)  Time: 0.723s, 1416.70/s  (0.702s, 1458.71/s)  LR: 9.939e-04  Data: 0.011 (0.012)
Train: 30 [1200/1251 ( 96%)]  Loss: 4.103 (4.21)  Time: 0.728s, 1406.03/s  (0.702s, 1458.56/s)  LR: 9.939e-04  Data: 0.009 (0.012)
Train: 30 [1250/1251 (100%)]  Loss: 4.520 (4.22)  Time: 0.697s, 1469.97/s  (0.702s, 1458.63/s)  LR: 9.939e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.555 (1.555)  Loss:  1.1836 (1.1836)  Acc@1: 82.7148 (82.7148)  Acc@5: 94.9219 (94.9219)
Test: [  48/48]  Time: 0.136 (0.573)  Loss:  1.2061 (1.9677)  Acc@1: 78.7736 (61.9380)  Acc@5: 93.5142 (84.5220)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-27.pth.tar', 63.54799994140625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-28.pth.tar', 62.978000092773435)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-29.pth.tar', 62.95000005859375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-26.pth.tar', 62.47000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-25.pth.tar', 62.055999968261716)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-30.pth.tar', 61.93799995849609)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-24.pth.tar', 61.2700001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-22.pth.tar', 61.03600001464844)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-23.pth.tar', 60.770000021972656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-21.pth.tar', 60.678000100097655)

Train: 31 [   0/1251 (  0%)]  Loss: 3.994 (3.99)  Time: 2.083s,  491.60/s  (2.083s,  491.60/s)  LR: 9.935e-04  Data: 1.467 (1.467)
Train: 31 [  50/1251 (  4%)]  Loss: 4.457 (4.23)  Time: 0.719s, 1424.60/s  (0.744s, 1375.66/s)  LR: 9.935e-04  Data: 0.009 (0.050)
Train: 31 [ 100/1251 (  8%)]  Loss: 3.878 (4.11)  Time: 0.707s, 1448.67/s  (0.722s, 1417.63/s)  LR: 9.935e-04  Data: 0.012 (0.030)
Train: 31 [ 150/1251 ( 12%)]  Loss: 4.651 (4.24)  Time: 0.703s, 1455.72/s  (0.714s, 1434.86/s)  LR: 9.935e-04  Data: 0.010 (0.024)
Train: 31 [ 200/1251 ( 16%)]  Loss: 4.447 (4.29)  Time: 0.672s, 1524.79/s  (0.711s, 1440.58/s)  LR: 9.935e-04  Data: 0.009 (0.020)
Train: 31 [ 250/1251 ( 20%)]  Loss: 4.618 (4.34)  Time: 0.720s, 1422.77/s  (0.707s, 1448.36/s)  LR: 9.935e-04  Data: 0.011 (0.018)
Train: 31 [ 300/1251 ( 24%)]  Loss: 4.252 (4.33)  Time: 0.671s, 1525.36/s  (0.706s, 1451.07/s)  LR: 9.935e-04  Data: 0.010 (0.017)
Train: 31 [ 350/1251 ( 28%)]  Loss: 4.475 (4.35)  Time: 0.725s, 1412.79/s  (0.705s, 1452.29/s)  LR: 9.935e-04  Data: 0.009 (0.016)
Train: 31 [ 400/1251 ( 32%)]  Loss: 3.986 (4.31)  Time: 0.675s, 1517.68/s  (0.705s, 1452.09/s)  LR: 9.935e-04  Data: 0.009 (0.015)
Train: 31 [ 450/1251 ( 36%)]  Loss: 4.192 (4.29)  Time: 0.728s, 1407.39/s  (0.704s, 1454.09/s)  LR: 9.935e-04  Data: 0.010 (0.015)
Train: 31 [ 500/1251 ( 40%)]  Loss: 4.252 (4.29)  Time: 0.676s, 1514.71/s  (0.703s, 1456.49/s)  LR: 9.935e-04  Data: 0.012 (0.014)
Train: 31 [ 550/1251 ( 44%)]  Loss: 4.231 (4.29)  Time: 0.674s, 1519.04/s  (0.703s, 1456.65/s)  LR: 9.935e-04  Data: 0.010 (0.014)
Train: 31 [ 600/1251 ( 48%)]  Loss: 3.888 (4.26)  Time: 0.704s, 1454.59/s  (0.702s, 1458.00/s)  LR: 9.935e-04  Data: 0.009 (0.013)
Train: 31 [ 650/1251 ( 52%)]  Loss: 3.898 (4.23)  Time: 0.724s, 1414.54/s  (0.702s, 1458.67/s)  LR: 9.935e-04  Data: 0.010 (0.013)
Train: 31 [ 700/1251 ( 56%)]  Loss: 4.344 (4.24)  Time: 0.689s, 1486.34/s  (0.703s, 1457.40/s)  LR: 9.935e-04  Data: 0.010 (0.013)
Train: 31 [ 750/1251 ( 60%)]  Loss: 3.979 (4.22)  Time: 0.796s, 1286.68/s  (0.703s, 1456.88/s)  LR: 9.935e-04  Data: 0.011 (0.013)
Train: 31 [ 800/1251 ( 64%)]  Loss: 3.656 (4.19)  Time: 0.702s, 1457.67/s  (0.703s, 1457.22/s)  LR: 9.935e-04  Data: 0.009 (0.013)
Train: 31 [ 850/1251 ( 68%)]  Loss: 4.368 (4.20)  Time: 0.704s, 1454.16/s  (0.702s, 1458.18/s)  LR: 9.935e-04  Data: 0.009 (0.013)
Train: 31 [ 900/1251 ( 72%)]  Loss: 4.334 (4.21)  Time: 0.709s, 1443.86/s  (0.702s, 1458.39/s)  LR: 9.935e-04  Data: 0.009 (0.012)
Train: 31 [ 950/1251 ( 76%)]  Loss: 4.276 (4.21)  Time: 0.690s, 1483.93/s  (0.702s, 1458.44/s)  LR: 9.935e-04  Data: 0.017 (0.012)
Train: 31 [1000/1251 ( 80%)]  Loss: 4.087 (4.20)  Time: 0.681s, 1503.27/s  (0.702s, 1458.91/s)  LR: 9.935e-04  Data: 0.009 (0.012)
Train: 31 [1050/1251 ( 84%)]  Loss: 4.030 (4.20)  Time: 0.750s, 1365.93/s  (0.702s, 1459.02/s)  LR: 9.935e-04  Data: 0.009 (0.012)
Train: 31 [1100/1251 ( 88%)]  Loss: 4.147 (4.19)  Time: 0.683s, 1498.50/s  (0.702s, 1458.56/s)  LR: 9.935e-04  Data: 0.009 (0.012)
Train: 31 [1150/1251 ( 92%)]  Loss: 4.169 (4.19)  Time: 0.673s, 1520.64/s  (0.702s, 1458.22/s)  LR: 9.935e-04  Data: 0.009 (0.012)
Train: 31 [1200/1251 ( 96%)]  Loss: 4.235 (4.19)  Time: 0.714s, 1434.14/s  (0.702s, 1458.27/s)  LR: 9.935e-04  Data: 0.010 (0.012)
Train: 31 [1250/1251 (100%)]  Loss: 4.353 (4.20)  Time: 0.710s, 1442.21/s  (0.702s, 1458.26/s)  LR: 9.935e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.466 (1.466)  Loss:  1.0205 (1.0205)  Acc@1: 82.2266 (82.2266)  Acc@5: 95.6055 (95.6055)
Test: [  48/48]  Time: 0.138 (0.585)  Loss:  1.0762 (1.6710)  Acc@1: 79.1274 (64.4860)  Acc@5: 93.8679 (86.1180)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-31.pth.tar', 64.4860001123047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-27.pth.tar', 63.54799994140625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-28.pth.tar', 62.978000092773435)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-29.pth.tar', 62.95000005859375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-26.pth.tar', 62.47000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-25.pth.tar', 62.055999968261716)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-30.pth.tar', 61.93799995849609)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-24.pth.tar', 61.2700001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-22.pth.tar', 61.03600001464844)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-23.pth.tar', 60.770000021972656)

Train: 32 [   0/1251 (  0%)]  Loss: 4.029 (4.03)  Time: 2.112s,  484.79/s  (2.112s,  484.79/s)  LR: 9.931e-04  Data: 1.497 (1.497)
Train: 32 [  50/1251 (  4%)]  Loss: 4.351 (4.19)  Time: 0.675s, 1517.91/s  (0.741s, 1382.10/s)  LR: 9.931e-04  Data: 0.012 (0.049)
Train: 32 [ 100/1251 (  8%)]  Loss: 4.452 (4.28)  Time: 0.673s, 1520.75/s  (0.721s, 1420.43/s)  LR: 9.931e-04  Data: 0.010 (0.030)
Train: 32 [ 150/1251 ( 12%)]  Loss: 3.917 (4.19)  Time: 0.703s, 1456.95/s  (0.713s, 1436.01/s)  LR: 9.931e-04  Data: 0.010 (0.023)
Train: 32 [ 200/1251 ( 16%)]  Loss: 4.075 (4.16)  Time: 0.739s, 1385.16/s  (0.709s, 1444.48/s)  LR: 9.931e-04  Data: 0.009 (0.020)
Train: 32 [ 250/1251 ( 20%)]  Loss: 4.189 (4.17)  Time: 0.749s, 1367.60/s  (0.708s, 1447.04/s)  LR: 9.931e-04  Data: 0.009 (0.018)
Train: 32 [ 300/1251 ( 24%)]  Loss: 4.184 (4.17)  Time: 0.674s, 1518.43/s  (0.706s, 1451.31/s)  LR: 9.931e-04  Data: 0.011 (0.017)
Train: 32 [ 350/1251 ( 28%)]  Loss: 4.532 (4.22)  Time: 0.708s, 1446.04/s  (0.704s, 1453.90/s)  LR: 9.931e-04  Data: 0.009 (0.016)
Train: 32 [ 400/1251 ( 32%)]  Loss: 4.658 (4.27)  Time: 0.710s, 1443.27/s  (0.705s, 1453.35/s)  LR: 9.931e-04  Data: 0.010 (0.015)
Train: 32 [ 450/1251 ( 36%)]  Loss: 3.552 (4.19)  Time: 0.673s, 1521.21/s  (0.704s, 1453.60/s)  LR: 9.931e-04  Data: 0.009 (0.014)
Train: 32 [ 500/1251 ( 40%)]  Loss: 4.284 (4.20)  Time: 0.729s, 1404.17/s  (0.704s, 1454.13/s)  LR: 9.931e-04  Data: 0.009 (0.014)
Train: 32 [ 550/1251 ( 44%)]  Loss: 4.128 (4.20)  Time: 0.711s, 1439.78/s  (0.704s, 1454.66/s)  LR: 9.931e-04  Data: 0.009 (0.014)
Train: 32 [ 600/1251 ( 48%)]  Loss: 4.388 (4.21)  Time: 0.671s, 1525.65/s  (0.704s, 1454.69/s)  LR: 9.931e-04  Data: 0.010 (0.013)
Train: 32 [ 650/1251 ( 52%)]  Loss: 4.169 (4.21)  Time: 0.672s, 1524.24/s  (0.704s, 1455.20/s)  LR: 9.931e-04  Data: 0.010 (0.013)
Train: 32 [ 700/1251 ( 56%)]  Loss: 4.067 (4.20)  Time: 0.724s, 1413.79/s  (0.703s, 1456.05/s)  LR: 9.931e-04  Data: 0.009 (0.013)
Train: 32 [ 750/1251 ( 60%)]  Loss: 4.431 (4.21)  Time: 0.698s, 1466.28/s  (0.703s, 1456.38/s)  LR: 9.931e-04  Data: 0.009 (0.013)
Train: 32 [ 800/1251 ( 64%)]  Loss: 3.979 (4.20)  Time: 0.676s, 1515.83/s  (0.703s, 1456.52/s)  LR: 9.931e-04  Data: 0.010 (0.013)
Train: 32 [ 850/1251 ( 68%)]  Loss: 4.280 (4.20)  Time: 0.672s, 1523.52/s  (0.702s, 1457.99/s)  LR: 9.931e-04  Data: 0.010 (0.012)
Train: 32 [ 900/1251 ( 72%)]  Loss: 4.055 (4.20)  Time: 0.703s, 1457.33/s  (0.702s, 1458.74/s)  LR: 9.931e-04  Data: 0.009 (0.012)
Train: 32 [ 950/1251 ( 76%)]  Loss: 4.059 (4.19)  Time: 0.719s, 1423.88/s  (0.702s, 1459.29/s)  LR: 9.931e-04  Data: 0.009 (0.012)
Train: 32 [1000/1251 ( 80%)]  Loss: 4.216 (4.19)  Time: 0.674s, 1519.32/s  (0.701s, 1459.96/s)  LR: 9.931e-04  Data: 0.010 (0.012)
Train: 32 [1050/1251 ( 84%)]  Loss: 4.072 (4.18)  Time: 0.702s, 1457.79/s  (0.701s, 1460.79/s)  LR: 9.931e-04  Data: 0.009 (0.012)
Train: 32 [1100/1251 ( 88%)]  Loss: 4.146 (4.18)  Time: 0.674s, 1518.48/s  (0.701s, 1461.56/s)  LR: 9.931e-04  Data: 0.010 (0.012)
Train: 32 [1150/1251 ( 92%)]  Loss: 4.390 (4.19)  Time: 0.710s, 1442.34/s  (0.701s, 1461.37/s)  LR: 9.931e-04  Data: 0.009 (0.012)
Train: 32 [1200/1251 ( 96%)]  Loss: 4.104 (4.19)  Time: 0.706s, 1451.44/s  (0.701s, 1461.30/s)  LR: 9.931e-04  Data: 0.010 (0.012)
Train: 32 [1250/1251 (100%)]  Loss: 3.943 (4.18)  Time: 0.690s, 1483.20/s  (0.701s, 1460.89/s)  LR: 9.931e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.513 (1.513)  Loss:  1.1562 (1.1562)  Acc@1: 81.9336 (81.9336)  Acc@5: 95.0195 (95.0195)
Test: [  48/48]  Time: 0.137 (0.578)  Loss:  1.3066 (1.9860)  Acc@1: 81.7217 (63.5220)  Acc@5: 94.5755 (85.7000)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-31.pth.tar', 64.4860001123047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-27.pth.tar', 63.54799994140625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-32.pth.tar', 63.521999946289064)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-28.pth.tar', 62.978000092773435)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-29.pth.tar', 62.95000005859375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-26.pth.tar', 62.47000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-25.pth.tar', 62.055999968261716)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-30.pth.tar', 61.93799995849609)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-24.pth.tar', 61.2700001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-22.pth.tar', 61.03600001464844)

Train: 33 [   0/1251 (  0%)]  Loss: 3.993 (3.99)  Time: 2.156s,  474.91/s  (2.156s,  474.91/s)  LR: 9.926e-04  Data: 1.539 (1.539)
Train: 33 [  50/1251 (  4%)]  Loss: 4.196 (4.09)  Time: 0.693s, 1477.99/s  (0.742s, 1380.84/s)  LR: 9.926e-04  Data: 0.010 (0.045)
Train: 33 [ 100/1251 (  8%)]  Loss: 4.439 (4.21)  Time: 0.673s, 1522.35/s  (0.722s, 1417.48/s)  LR: 9.926e-04  Data: 0.010 (0.028)
Train: 33 [ 150/1251 ( 12%)]  Loss: 4.440 (4.27)  Time: 0.710s, 1442.61/s  (0.713s, 1436.50/s)  LR: 9.926e-04  Data: 0.010 (0.022)
Train: 33 [ 200/1251 ( 16%)]  Loss: 3.966 (4.21)  Time: 0.678s, 1510.18/s  (0.710s, 1441.51/s)  LR: 9.926e-04  Data: 0.014 (0.019)
Train: 33 [ 250/1251 ( 20%)]  Loss: 4.149 (4.20)  Time: 0.759s, 1349.48/s  (0.708s, 1446.98/s)  LR: 9.926e-04  Data: 0.012 (0.017)
Train: 33 [ 300/1251 ( 24%)]  Loss: 4.010 (4.17)  Time: 0.815s, 1255.77/s  (0.708s, 1446.65/s)  LR: 9.926e-04  Data: 0.010 (0.016)
Train: 33 [ 350/1251 ( 28%)]  Loss: 4.240 (4.18)  Time: 0.700s, 1462.99/s  (0.707s, 1449.31/s)  LR: 9.926e-04  Data: 0.009 (0.015)
Train: 33 [ 400/1251 ( 32%)]  Loss: 4.072 (4.17)  Time: 0.686s, 1491.81/s  (0.706s, 1449.53/s)  LR: 9.926e-04  Data: 0.013 (0.015)
Train: 33 [ 450/1251 ( 36%)]  Loss: 4.310 (4.18)  Time: 0.708s, 1446.61/s  (0.707s, 1449.34/s)  LR: 9.926e-04  Data: 0.009 (0.014)
Train: 33 [ 500/1251 ( 40%)]  Loss: 4.055 (4.17)  Time: 0.690s, 1484.81/s  (0.706s, 1450.84/s)  LR: 9.926e-04  Data: 0.009 (0.014)
Train: 33 [ 550/1251 ( 44%)]  Loss: 4.412 (4.19)  Time: 0.672s, 1523.95/s  (0.706s, 1451.24/s)  LR: 9.926e-04  Data: 0.012 (0.013)
Train: 33 [ 600/1251 ( 48%)]  Loss: 4.443 (4.21)  Time: 0.729s, 1404.99/s  (0.706s, 1451.41/s)  LR: 9.926e-04  Data: 0.009 (0.013)
Train: 33 [ 650/1251 ( 52%)]  Loss: 4.314 (4.22)  Time: 0.675s, 1516.81/s  (0.705s, 1452.71/s)  LR: 9.926e-04  Data: 0.009 (0.013)
Train: 33 [ 700/1251 ( 56%)]  Loss: 4.275 (4.22)  Time: 0.703s, 1457.17/s  (0.705s, 1453.00/s)  LR: 9.926e-04  Data: 0.009 (0.013)
Train: 33 [ 750/1251 ( 60%)]  Loss: 4.201 (4.22)  Time: 0.721s, 1420.43/s  (0.705s, 1453.00/s)  LR: 9.926e-04  Data: 0.011 (0.013)
Train: 33 [ 800/1251 ( 64%)]  Loss: 4.083 (4.21)  Time: 0.672s, 1524.66/s  (0.705s, 1453.35/s)  LR: 9.926e-04  Data: 0.010 (0.012)
Train: 33 [ 850/1251 ( 68%)]  Loss: 3.940 (4.20)  Time: 0.698s, 1466.49/s  (0.704s, 1454.11/s)  LR: 9.926e-04  Data: 0.010 (0.012)
Train: 33 [ 900/1251 ( 72%)]  Loss: 4.024 (4.19)  Time: 0.711s, 1440.40/s  (0.704s, 1453.73/s)  LR: 9.926e-04  Data: 0.011 (0.012)
Train: 33 [ 950/1251 ( 76%)]  Loss: 4.430 (4.20)  Time: 0.706s, 1450.46/s  (0.705s, 1453.14/s)  LR: 9.926e-04  Data: 0.011 (0.012)
Train: 33 [1000/1251 ( 80%)]  Loss: 4.149 (4.20)  Time: 0.705s, 1452.12/s  (0.705s, 1452.83/s)  LR: 9.926e-04  Data: 0.009 (0.012)
Train: 33 [1050/1251 ( 84%)]  Loss: 3.813 (4.18)  Time: 0.671s, 1525.98/s  (0.704s, 1454.16/s)  LR: 9.926e-04  Data: 0.009 (0.012)
Train: 33 [1100/1251 ( 88%)]  Loss: 4.346 (4.19)  Time: 0.705s, 1453.11/s  (0.704s, 1455.41/s)  LR: 9.926e-04  Data: 0.010 (0.012)
Train: 33 [1150/1251 ( 92%)]  Loss: 4.445 (4.20)  Time: 0.673s, 1522.50/s  (0.704s, 1455.29/s)  LR: 9.926e-04  Data: 0.010 (0.012)
Train: 33 [1200/1251 ( 96%)]  Loss: 4.158 (4.20)  Time: 0.729s, 1404.23/s  (0.704s, 1455.41/s)  LR: 9.926e-04  Data: 0.010 (0.012)
Train: 33 [1250/1251 (100%)]  Loss: 4.129 (4.19)  Time: 0.708s, 1446.38/s  (0.703s, 1455.77/s)  LR: 9.926e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.494 (1.494)  Loss:  1.0303 (1.0303)  Acc@1: 82.0312 (82.0312)  Acc@5: 94.7266 (94.7266)
Test: [  48/48]  Time: 0.137 (0.592)  Loss:  1.2266 (1.8145)  Acc@1: 79.0094 (63.9340)  Acc@5: 92.6887 (86.0180)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-31.pth.tar', 64.4860001123047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-33.pth.tar', 63.93400006103516)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-27.pth.tar', 63.54799994140625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-32.pth.tar', 63.521999946289064)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-28.pth.tar', 62.978000092773435)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-29.pth.tar', 62.95000005859375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-26.pth.tar', 62.47000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-25.pth.tar', 62.055999968261716)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-30.pth.tar', 61.93799995849609)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-24.pth.tar', 61.2700001171875)

Train: 34 [   0/1251 (  0%)]  Loss: 4.249 (4.25)  Time: 2.176s,  470.68/s  (2.176s,  470.68/s)  LR: 9.922e-04  Data: 1.560 (1.560)
Train: 34 [  50/1251 (  4%)]  Loss: 4.240 (4.24)  Time: 0.718s, 1425.95/s  (0.732s, 1398.61/s)  LR: 9.922e-04  Data: 0.011 (0.048)
Train: 34 [ 100/1251 (  8%)]  Loss: 4.039 (4.18)  Time: 0.704s, 1455.10/s  (0.716s, 1429.44/s)  LR: 9.922e-04  Data: 0.010 (0.030)
Train: 34 [ 150/1251 ( 12%)]  Loss: 4.259 (4.20)  Time: 0.711s, 1440.07/s  (0.711s, 1440.81/s)  LR: 9.922e-04  Data: 0.009 (0.023)
Train: 34 [ 200/1251 ( 16%)]  Loss: 4.387 (4.23)  Time: 0.718s, 1425.95/s  (0.707s, 1447.51/s)  LR: 9.922e-04  Data: 0.010 (0.020)
Train: 34 [ 250/1251 ( 20%)]  Loss: 4.095 (4.21)  Time: 0.732s, 1399.80/s  (0.708s, 1446.27/s)  LR: 9.922e-04  Data: 0.011 (0.018)
Train: 34 [ 300/1251 ( 24%)]  Loss: 4.395 (4.24)  Time: 0.721s, 1419.46/s  (0.706s, 1451.32/s)  LR: 9.922e-04  Data: 0.010 (0.017)
Train: 34 [ 350/1251 ( 28%)]  Loss: 4.282 (4.24)  Time: 0.717s, 1427.52/s  (0.705s, 1453.25/s)  LR: 9.922e-04  Data: 0.009 (0.016)
Train: 34 [ 400/1251 ( 32%)]  Loss: 3.670 (4.18)  Time: 0.748s, 1368.59/s  (0.704s, 1453.64/s)  LR: 9.922e-04  Data: 0.009 (0.015)
Train: 34 [ 450/1251 ( 36%)]  Loss: 4.451 (4.21)  Time: 0.707s, 1448.93/s  (0.704s, 1453.89/s)  LR: 9.922e-04  Data: 0.009 (0.015)
Train: 34 [ 500/1251 ( 40%)]  Loss: 4.315 (4.22)  Time: 0.668s, 1533.94/s  (0.703s, 1456.34/s)  LR: 9.922e-04  Data: 0.010 (0.014)
Train: 34 [ 550/1251 ( 44%)]  Loss: 4.111 (4.21)  Time: 0.719s, 1424.19/s  (0.703s, 1456.55/s)  LR: 9.922e-04  Data: 0.008 (0.014)
Train: 34 [ 600/1251 ( 48%)]  Loss: 4.201 (4.21)  Time: 0.704s, 1454.27/s  (0.703s, 1457.20/s)  LR: 9.922e-04  Data: 0.009 (0.013)
Train: 34 [ 650/1251 ( 52%)]  Loss: 4.313 (4.21)  Time: 0.677s, 1513.50/s  (0.703s, 1457.47/s)  LR: 9.922e-04  Data: 0.010 (0.013)
Train: 34 [ 700/1251 ( 56%)]  Loss: 4.193 (4.21)  Time: 0.673s, 1520.85/s  (0.702s, 1458.97/s)  LR: 9.922e-04  Data: 0.010 (0.013)
Train: 34 [ 750/1251 ( 60%)]  Loss: 3.918 (4.19)  Time: 0.711s, 1440.31/s  (0.702s, 1459.23/s)  LR: 9.922e-04  Data: 0.011 (0.013)
Train: 34 [ 800/1251 ( 64%)]  Loss: 4.071 (4.19)  Time: 0.693s, 1476.96/s  (0.701s, 1459.85/s)  LR: 9.922e-04  Data: 0.009 (0.013)
Train: 34 [ 850/1251 ( 68%)]  Loss: 4.048 (4.18)  Time: 0.704s, 1454.72/s  (0.701s, 1461.06/s)  LR: 9.922e-04  Data: 0.009 (0.012)
Train: 34 [ 900/1251 ( 72%)]  Loss: 4.315 (4.19)  Time: 0.706s, 1450.68/s  (0.701s, 1461.49/s)  LR: 9.922e-04  Data: 0.010 (0.012)
Train: 34 [ 950/1251 ( 76%)]  Loss: 3.976 (4.18)  Time: 0.712s, 1437.54/s  (0.701s, 1461.08/s)  LR: 9.922e-04  Data: 0.014 (0.012)
Train: 34 [1000/1251 ( 80%)]  Loss: 3.661 (4.15)  Time: 0.694s, 1475.13/s  (0.701s, 1461.38/s)  LR: 9.922e-04  Data: 0.009 (0.012)
Train: 34 [1050/1251 ( 84%)]  Loss: 4.010 (4.15)  Time: 0.739s, 1385.26/s  (0.701s, 1461.54/s)  LR: 9.922e-04  Data: 0.009 (0.012)
Train: 34 [1100/1251 ( 88%)]  Loss: 4.342 (4.15)  Time: 0.679s, 1508.48/s  (0.701s, 1461.47/s)  LR: 9.922e-04  Data: 0.010 (0.012)
Train: 34 [1150/1251 ( 92%)]  Loss: 4.283 (4.16)  Time: 0.707s, 1448.94/s  (0.701s, 1461.13/s)  LR: 9.922e-04  Data: 0.009 (0.012)
Train: 34 [1200/1251 ( 96%)]  Loss: 4.188 (4.16)  Time: 0.713s, 1436.30/s  (0.701s, 1460.60/s)  LR: 9.922e-04  Data: 0.010 (0.012)
Train: 34 [1250/1251 (100%)]  Loss: 3.658 (4.14)  Time: 0.657s, 1557.94/s  (0.701s, 1460.56/s)  LR: 9.922e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.585 (1.585)  Loss:  1.1201 (1.1201)  Acc@1: 80.6641 (80.6641)  Acc@5: 93.5547 (93.5547)
Test: [  48/48]  Time: 0.136 (0.582)  Loss:  1.3350 (1.8346)  Acc@1: 78.4198 (63.4880)  Acc@5: 91.7453 (85.5660)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-31.pth.tar', 64.4860001123047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-33.pth.tar', 63.93400006103516)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-27.pth.tar', 63.54799994140625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-32.pth.tar', 63.521999946289064)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-34.pth.tar', 63.48799993408203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-28.pth.tar', 62.978000092773435)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-29.pth.tar', 62.95000005859375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-26.pth.tar', 62.47000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-25.pth.tar', 62.055999968261716)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-30.pth.tar', 61.93799995849609)

Train: 35 [   0/1251 (  0%)]  Loss: 4.293 (4.29)  Time: 2.349s,  435.87/s  (2.349s,  435.87/s)  LR: 9.917e-04  Data: 1.677 (1.677)
Train: 35 [  50/1251 (  4%)]  Loss: 4.163 (4.23)  Time: 0.673s, 1522.17/s  (0.742s, 1379.38/s)  LR: 9.917e-04  Data: 0.010 (0.046)
Train: 35 [ 100/1251 (  8%)]  Loss: 4.457 (4.30)  Time: 0.671s, 1526.99/s  (0.718s, 1426.33/s)  LR: 9.917e-04  Data: 0.010 (0.028)
Train: 35 [ 150/1251 ( 12%)]  Loss: 3.820 (4.18)  Time: 0.679s, 1509.07/s  (0.711s, 1440.56/s)  LR: 9.917e-04  Data: 0.011 (0.022)
Train: 35 [ 200/1251 ( 16%)]  Loss: 3.891 (4.12)  Time: 0.673s, 1520.92/s  (0.707s, 1448.55/s)  LR: 9.917e-04  Data: 0.012 (0.019)
Train: 35 [ 250/1251 ( 20%)]  Loss: 4.104 (4.12)  Time: 0.703s, 1455.84/s  (0.706s, 1451.24/s)  LR: 9.917e-04  Data: 0.009 (0.017)
Train: 35 [ 300/1251 ( 24%)]  Loss: 4.227 (4.14)  Time: 0.701s, 1460.04/s  (0.705s, 1453.21/s)  LR: 9.917e-04  Data: 0.011 (0.016)
Train: 35 [ 350/1251 ( 28%)]  Loss: 4.256 (4.15)  Time: 0.708s, 1445.94/s  (0.702s, 1458.35/s)  LR: 9.917e-04  Data: 0.010 (0.015)
Train: 35 [ 400/1251 ( 32%)]  Loss: 4.705 (4.21)  Time: 0.739s, 1386.58/s  (0.703s, 1456.55/s)  LR: 9.917e-04  Data: 0.009 (0.015)
Train: 35 [ 450/1251 ( 36%)]  Loss: 3.932 (4.18)  Time: 0.702s, 1459.02/s  (0.703s, 1456.95/s)  LR: 9.917e-04  Data: 0.010 (0.014)
Train: 35 [ 500/1251 ( 40%)]  Loss: 4.614 (4.22)  Time: 0.718s, 1426.53/s  (0.703s, 1457.25/s)  LR: 9.917e-04  Data: 0.010 (0.014)
Train: 35 [ 550/1251 ( 44%)]  Loss: 4.032 (4.21)  Time: 0.713s, 1436.82/s  (0.703s, 1457.25/s)  LR: 9.917e-04  Data: 0.010 (0.013)
Train: 35 [ 600/1251 ( 48%)]  Loss: 4.113 (4.20)  Time: 0.672s, 1524.30/s  (0.703s, 1457.38/s)  LR: 9.917e-04  Data: 0.010 (0.013)
Train: 35 [ 650/1251 ( 52%)]  Loss: 4.217 (4.20)  Time: 0.695s, 1473.59/s  (0.702s, 1458.87/s)  LR: 9.917e-04  Data: 0.010 (0.013)
Train: 35 [ 700/1251 ( 56%)]  Loss: 4.101 (4.19)  Time: 0.707s, 1447.88/s  (0.702s, 1458.54/s)  LR: 9.917e-04  Data: 0.009 (0.013)
Train: 35 [ 750/1251 ( 60%)]  Loss: 4.114 (4.19)  Time: 0.812s, 1260.69/s  (0.702s, 1458.36/s)  LR: 9.917e-04  Data: 0.017 (0.013)
Train: 35 [ 800/1251 ( 64%)]  Loss: 4.568 (4.21)  Time: 0.682s, 1500.80/s  (0.702s, 1458.49/s)  LR: 9.917e-04  Data: 0.010 (0.012)
Train: 35 [ 850/1251 ( 68%)]  Loss: 4.326 (4.22)  Time: 0.680s, 1505.95/s  (0.701s, 1459.82/s)  LR: 9.917e-04  Data: 0.009 (0.012)
Train: 35 [ 900/1251 ( 72%)]  Loss: 4.441 (4.23)  Time: 0.706s, 1450.04/s  (0.701s, 1460.12/s)  LR: 9.917e-04  Data: 0.010 (0.012)
Train: 35 [ 950/1251 ( 76%)]  Loss: 4.123 (4.22)  Time: 0.717s, 1428.58/s  (0.701s, 1460.10/s)  LR: 9.917e-04  Data: 0.010 (0.012)
Train: 35 [1000/1251 ( 80%)]  Loss: 4.515 (4.24)  Time: 0.707s, 1447.89/s  (0.701s, 1460.97/s)  LR: 9.917e-04  Data: 0.009 (0.012)
Train: 35 [1050/1251 ( 84%)]  Loss: 4.465 (4.25)  Time: 0.752s, 1360.90/s  (0.701s, 1460.68/s)  LR: 9.917e-04  Data: 0.009 (0.012)
Train: 35 [1100/1251 ( 88%)]  Loss: 3.967 (4.24)  Time: 0.706s, 1450.08/s  (0.701s, 1459.91/s)  LR: 9.917e-04  Data: 0.009 (0.012)
Train: 35 [1150/1251 ( 92%)]  Loss: 3.778 (4.22)  Time: 0.731s, 1401.14/s  (0.702s, 1459.19/s)  LR: 9.917e-04  Data: 0.009 (0.012)
Train: 35 [1200/1251 ( 96%)]  Loss: 4.456 (4.23)  Time: 0.671s, 1525.52/s  (0.702s, 1458.90/s)  LR: 9.917e-04  Data: 0.009 (0.012)
Train: 35 [1250/1251 (100%)]  Loss: 4.100 (4.22)  Time: 0.659s, 1554.91/s  (0.702s, 1458.90/s)  LR: 9.917e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.483 (1.483)  Loss:  1.2920 (1.2920)  Acc@1: 82.0312 (82.0312)  Acc@5: 95.2148 (95.2148)
Test: [  48/48]  Time: 0.136 (0.586)  Loss:  1.3027 (1.9396)  Acc@1: 80.7783 (64.2400)  Acc@5: 93.0425 (86.4460)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-31.pth.tar', 64.4860001123047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-35.pth.tar', 64.24000018310547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-33.pth.tar', 63.93400006103516)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-27.pth.tar', 63.54799994140625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-32.pth.tar', 63.521999946289064)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-34.pth.tar', 63.48799993408203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-28.pth.tar', 62.978000092773435)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-29.pth.tar', 62.95000005859375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-26.pth.tar', 62.47000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-25.pth.tar', 62.055999968261716)

Train: 36 [   0/1251 (  0%)]  Loss: 4.150 (4.15)  Time: 2.080s,  492.20/s  (2.080s,  492.20/s)  LR: 9.912e-04  Data: 1.465 (1.465)
Train: 36 [  50/1251 (  4%)]  Loss: 3.840 (4.00)  Time: 0.687s, 1491.28/s  (0.727s, 1408.76/s)  LR: 9.912e-04  Data: 0.013 (0.044)
Train: 36 [ 100/1251 (  8%)]  Loss: 4.165 (4.05)  Time: 0.704s, 1455.09/s  (0.714s, 1433.32/s)  LR: 9.912e-04  Data: 0.009 (0.027)
Train: 36 [ 150/1251 ( 12%)]  Loss: 4.137 (4.07)  Time: 0.706s, 1450.00/s  (0.707s, 1447.48/s)  LR: 9.912e-04  Data: 0.009 (0.022)
Train: 36 [ 200/1251 ( 16%)]  Loss: 3.974 (4.05)  Time: 0.786s, 1303.10/s  (0.706s, 1450.61/s)  LR: 9.912e-04  Data: 0.010 (0.019)
Train: 36 [ 250/1251 ( 20%)]  Loss: 4.024 (4.05)  Time: 0.666s, 1537.11/s  (0.707s, 1447.79/s)  LR: 9.912e-04  Data: 0.010 (0.017)
Train: 36 [ 300/1251 ( 24%)]  Loss: 4.285 (4.08)  Time: 0.669s, 1530.05/s  (0.707s, 1447.91/s)  LR: 9.912e-04  Data: 0.013 (0.016)
Train: 36 [ 350/1251 ( 28%)]  Loss: 4.072 (4.08)  Time: 0.705s, 1452.09/s  (0.705s, 1451.56/s)  LR: 9.912e-04  Data: 0.012 (0.015)
Train: 36 [ 400/1251 ( 32%)]  Loss: 3.814 (4.05)  Time: 0.675s, 1516.09/s  (0.706s, 1451.39/s)  LR: 9.912e-04  Data: 0.011 (0.015)
Train: 36 [ 450/1251 ( 36%)]  Loss: 3.993 (4.05)  Time: 0.712s, 1438.00/s  (0.705s, 1451.82/s)  LR: 9.912e-04  Data: 0.009 (0.014)
Train: 36 [ 500/1251 ( 40%)]  Loss: 4.383 (4.08)  Time: 0.689s, 1485.51/s  (0.705s, 1453.37/s)  LR: 9.912e-04  Data: 0.010 (0.014)
Train: 36 [ 550/1251 ( 44%)]  Loss: 3.938 (4.06)  Time: 0.704s, 1455.21/s  (0.705s, 1452.76/s)  LR: 9.912e-04  Data: 0.009 (0.013)
Train: 36 [ 600/1251 ( 48%)]  Loss: 3.968 (4.06)  Time: 0.708s, 1445.41/s  (0.704s, 1454.10/s)  LR: 9.912e-04  Data: 0.009 (0.013)
Train: 36 [ 650/1251 ( 52%)]  Loss: 4.151 (4.06)  Time: 0.731s, 1401.17/s  (0.704s, 1454.23/s)  LR: 9.912e-04  Data: 0.013 (0.013)
Train: 36 [ 700/1251 ( 56%)]  Loss: 3.912 (4.05)  Time: 0.689s, 1487.29/s  (0.704s, 1454.21/s)  LR: 9.912e-04  Data: 0.013 (0.013)
Train: 36 [ 750/1251 ( 60%)]  Loss: 3.983 (4.05)  Time: 0.712s, 1438.65/s  (0.703s, 1456.03/s)  LR: 9.912e-04  Data: 0.010 (0.013)
Train: 36 [ 800/1251 ( 64%)]  Loss: 4.369 (4.07)  Time: 0.676s, 1515.63/s  (0.703s, 1456.77/s)  LR: 9.912e-04  Data: 0.010 (0.012)
Train: 36 [ 850/1251 ( 68%)]  Loss: 4.099 (4.07)  Time: 0.694s, 1474.66/s  (0.703s, 1456.98/s)  LR: 9.912e-04  Data: 0.010 (0.012)
Train: 36 [ 900/1251 ( 72%)]  Loss: 3.933 (4.06)  Time: 0.704s, 1454.64/s  (0.702s, 1458.06/s)  LR: 9.912e-04  Data: 0.010 (0.012)
Train: 36 [ 950/1251 ( 76%)]  Loss: 4.292 (4.07)  Time: 0.676s, 1515.10/s  (0.703s, 1457.49/s)  LR: 9.912e-04  Data: 0.010 (0.012)
Train: 36 [1000/1251 ( 80%)]  Loss: 3.861 (4.06)  Time: 0.677s, 1511.72/s  (0.702s, 1458.02/s)  LR: 9.912e-04  Data: 0.011 (0.012)
Train: 36 [1050/1251 ( 84%)]  Loss: 4.335 (4.08)  Time: 0.709s, 1444.14/s  (0.702s, 1458.13/s)  LR: 9.912e-04  Data: 0.017 (0.012)
Train: 36 [1100/1251 ( 88%)]  Loss: 4.222 (4.08)  Time: 0.747s, 1371.03/s  (0.702s, 1457.71/s)  LR: 9.912e-04  Data: 0.010 (0.012)
Train: 36 [1150/1251 ( 92%)]  Loss: 4.175 (4.09)  Time: 0.752s, 1362.20/s  (0.703s, 1457.44/s)  LR: 9.912e-04  Data: 0.009 (0.012)
Train: 36 [1200/1251 ( 96%)]  Loss: 3.988 (4.08)  Time: 0.674s, 1518.44/s  (0.702s, 1457.68/s)  LR: 9.912e-04  Data: 0.010 (0.012)
Train: 36 [1250/1251 (100%)]  Loss: 4.494 (4.10)  Time: 0.662s, 1546.33/s  (0.702s, 1457.97/s)  LR: 9.912e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.562 (1.562)  Loss:  1.2363 (1.2363)  Acc@1: 83.2031 (83.2031)  Acc@5: 94.5312 (94.5312)
Test: [  48/48]  Time: 0.136 (0.578)  Loss:  1.0742 (1.8180)  Acc@1: 82.6651 (65.0940)  Acc@5: 94.5755 (86.5720)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-36.pth.tar', 65.09400009765625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-31.pth.tar', 64.4860001123047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-35.pth.tar', 64.24000018310547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-33.pth.tar', 63.93400006103516)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-27.pth.tar', 63.54799994140625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-32.pth.tar', 63.521999946289064)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-34.pth.tar', 63.48799993408203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-28.pth.tar', 62.978000092773435)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-29.pth.tar', 62.95000005859375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-26.pth.tar', 62.47000009277344)

Train: 37 [   0/1251 (  0%)]  Loss: 4.610 (4.61)  Time: 2.196s,  466.32/s  (2.196s,  466.32/s)  LR: 9.907e-04  Data: 1.578 (1.578)
Train: 37 [  50/1251 (  4%)]  Loss: 4.564 (4.59)  Time: 0.690s, 1483.95/s  (0.742s, 1379.71/s)  LR: 9.907e-04  Data: 0.010 (0.048)
Train: 37 [ 100/1251 (  8%)]  Loss: 4.138 (4.44)  Time: 0.731s, 1400.00/s  (0.724s, 1414.92/s)  LR: 9.907e-04  Data: 0.009 (0.029)
Train: 37 [ 150/1251 ( 12%)]  Loss: 4.045 (4.34)  Time: 0.704s, 1453.87/s  (0.715s, 1432.46/s)  LR: 9.907e-04  Data: 0.009 (0.023)
Train: 37 [ 200/1251 ( 16%)]  Loss: 4.235 (4.32)  Time: 0.696s, 1470.98/s  (0.710s, 1442.51/s)  LR: 9.907e-04  Data: 0.010 (0.020)
Train: 37 [ 250/1251 ( 20%)]  Loss: 4.014 (4.27)  Time: 0.731s, 1400.19/s  (0.706s, 1449.63/s)  LR: 9.907e-04  Data: 0.012 (0.018)
Train: 37 [ 300/1251 ( 24%)]  Loss: 4.229 (4.26)  Time: 0.789s, 1298.13/s  (0.705s, 1453.19/s)  LR: 9.907e-04  Data: 0.009 (0.017)
Train: 37 [ 350/1251 ( 28%)]  Loss: 3.745 (4.20)  Time: 0.720s, 1421.34/s  (0.703s, 1456.73/s)  LR: 9.907e-04  Data: 0.016 (0.016)
Train: 37 [ 400/1251 ( 32%)]  Loss: 3.950 (4.17)  Time: 0.724s, 1414.04/s  (0.704s, 1453.84/s)  LR: 9.907e-04  Data: 0.009 (0.015)
Train: 37 [ 450/1251 ( 36%)]  Loss: 4.121 (4.17)  Time: 0.737s, 1390.34/s  (0.705s, 1452.69/s)  LR: 9.907e-04  Data: 0.010 (0.015)
Train: 37 [ 500/1251 ( 40%)]  Loss: 3.869 (4.14)  Time: 0.703s, 1457.23/s  (0.704s, 1454.61/s)  LR: 9.907e-04  Data: 0.009 (0.014)
Train: 37 [ 550/1251 ( 44%)]  Loss: 4.338 (4.15)  Time: 0.711s, 1440.65/s  (0.704s, 1455.02/s)  LR: 9.907e-04  Data: 0.009 (0.014)
Train: 37 [ 600/1251 ( 48%)]  Loss: 4.250 (4.16)  Time: 0.695s, 1472.72/s  (0.703s, 1455.94/s)  LR: 9.907e-04  Data: 0.012 (0.014)
Train: 37 [ 650/1251 ( 52%)]  Loss: 3.972 (4.15)  Time: 0.708s, 1446.67/s  (0.703s, 1456.36/s)  LR: 9.907e-04  Data: 0.010 (0.013)
Train: 37 [ 700/1251 ( 56%)]  Loss: 3.895 (4.13)  Time: 0.704s, 1454.93/s  (0.702s, 1457.72/s)  LR: 9.907e-04  Data: 0.009 (0.013)
Train: 37 [ 750/1251 ( 60%)]  Loss: 3.886 (4.12)  Time: 0.684s, 1496.32/s  (0.703s, 1457.61/s)  LR: 9.907e-04  Data: 0.010 (0.013)
Train: 37 [ 800/1251 ( 64%)]  Loss: 4.127 (4.12)  Time: 0.786s, 1302.93/s  (0.703s, 1457.29/s)  LR: 9.907e-04  Data: 0.009 (0.013)
Train: 37 [ 850/1251 ( 68%)]  Loss: 4.370 (4.13)  Time: 0.669s, 1530.24/s  (0.703s, 1457.22/s)  LR: 9.907e-04  Data: 0.009 (0.013)
Train: 37 [ 900/1251 ( 72%)]  Loss: 3.742 (4.11)  Time: 0.684s, 1496.83/s  (0.702s, 1458.75/s)  LR: 9.907e-04  Data: 0.010 (0.012)
Train: 37 [ 950/1251 ( 76%)]  Loss: 4.317 (4.12)  Time: 0.672s, 1523.89/s  (0.702s, 1459.15/s)  LR: 9.907e-04  Data: 0.010 (0.012)
Train: 37 [1000/1251 ( 80%)]  Loss: 4.119 (4.12)  Time: 0.674s, 1518.54/s  (0.701s, 1460.17/s)  LR: 9.907e-04  Data: 0.010 (0.012)
Train: 37 [1050/1251 ( 84%)]  Loss: 3.992 (4.12)  Time: 0.700s, 1462.44/s  (0.701s, 1460.00/s)  LR: 9.907e-04  Data: 0.009 (0.012)
Train: 37 [1100/1251 ( 88%)]  Loss: 4.371 (4.13)  Time: 0.674s, 1520.37/s  (0.701s, 1461.00/s)  LR: 9.907e-04  Data: 0.010 (0.012)
Train: 37 [1150/1251 ( 92%)]  Loss: 4.302 (4.13)  Time: 0.677s, 1513.11/s  (0.701s, 1461.34/s)  LR: 9.907e-04  Data: 0.010 (0.012)
Train: 37 [1200/1251 ( 96%)]  Loss: 4.521 (4.15)  Time: 0.700s, 1462.22/s  (0.701s, 1461.03/s)  LR: 9.907e-04  Data: 0.009 (0.012)
Train: 37 [1250/1251 (100%)]  Loss: 4.511 (4.16)  Time: 0.684s, 1497.33/s  (0.701s, 1461.16/s)  LR: 9.907e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.540 (1.540)  Loss:  1.0586 (1.0586)  Acc@1: 83.4961 (83.4961)  Acc@5: 94.9219 (94.9219)
Test: [  48/48]  Time: 0.136 (0.585)  Loss:  1.2520 (1.7684)  Acc@1: 79.2453 (64.6200)  Acc@5: 92.4528 (86.1840)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-36.pth.tar', 65.09400009765625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-37.pth.tar', 64.62000003417968)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-31.pth.tar', 64.4860001123047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-35.pth.tar', 64.24000018310547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-33.pth.tar', 63.93400006103516)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-27.pth.tar', 63.54799994140625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-32.pth.tar', 63.521999946289064)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-34.pth.tar', 63.48799993408203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-28.pth.tar', 62.978000092773435)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-29.pth.tar', 62.95000005859375)

Train: 38 [   0/1251 (  0%)]  Loss: 4.235 (4.24)  Time: 2.177s,  470.27/s  (2.177s,  470.27/s)  LR: 9.902e-04  Data: 1.562 (1.562)
Train: 38 [  50/1251 (  4%)]  Loss: 4.413 (4.32)  Time: 0.707s, 1449.11/s  (0.734s, 1395.01/s)  LR: 9.902e-04  Data: 0.009 (0.047)
Train: 38 [ 100/1251 (  8%)]  Loss: 4.340 (4.33)  Time: 0.673s, 1521.61/s  (0.715s, 1431.52/s)  LR: 9.902e-04  Data: 0.010 (0.029)
Train: 38 [ 150/1251 ( 12%)]  Loss: 4.420 (4.35)  Time: 0.680s, 1504.97/s  (0.713s, 1436.85/s)  LR: 9.902e-04  Data: 0.015 (0.023)
Train: 38 [ 200/1251 ( 16%)]  Loss: 3.623 (4.21)  Time: 0.675s, 1517.74/s  (0.710s, 1441.31/s)  LR: 9.902e-04  Data: 0.010 (0.020)
Train: 38 [ 250/1251 ( 20%)]  Loss: 4.225 (4.21)  Time: 0.674s, 1520.09/s  (0.710s, 1442.64/s)  LR: 9.902e-04  Data: 0.010 (0.018)
Train: 38 [ 300/1251 ( 24%)]  Loss: 3.593 (4.12)  Time: 0.711s, 1439.79/s  (0.707s, 1448.01/s)  LR: 9.902e-04  Data: 0.010 (0.017)
Train: 38 [ 350/1251 ( 28%)]  Loss: 3.877 (4.09)  Time: 0.671s, 1526.30/s  (0.705s, 1451.51/s)  LR: 9.902e-04  Data: 0.010 (0.016)
Train: 38 [ 400/1251 ( 32%)]  Loss: 3.901 (4.07)  Time: 0.696s, 1471.40/s  (0.705s, 1452.95/s)  LR: 9.902e-04  Data: 0.010 (0.015)
Train: 38 [ 450/1251 ( 36%)]  Loss: 4.010 (4.06)  Time: 0.719s, 1424.46/s  (0.703s, 1455.91/s)  LR: 9.902e-04  Data: 0.009 (0.015)
Train: 38 [ 500/1251 ( 40%)]  Loss: 4.134 (4.07)  Time: 0.739s, 1386.26/s  (0.703s, 1457.55/s)  LR: 9.902e-04  Data: 0.009 (0.014)
Train: 38 [ 550/1251 ( 44%)]  Loss: 4.395 (4.10)  Time: 0.686s, 1493.21/s  (0.704s, 1455.47/s)  LR: 9.902e-04  Data: 0.009 (0.014)
Train: 38 [ 600/1251 ( 48%)]  Loss: 4.257 (4.11)  Time: 0.744s, 1376.70/s  (0.705s, 1452.67/s)  LR: 9.902e-04  Data: 0.009 (0.014)
Train: 38 [ 650/1251 ( 52%)]  Loss: 4.343 (4.13)  Time: 0.736s, 1391.12/s  (0.706s, 1450.56/s)  LR: 9.902e-04  Data: 0.014 (0.014)
Train: 38 [ 700/1251 ( 56%)]  Loss: 4.061 (4.12)  Time: 0.702s, 1457.99/s  (0.705s, 1452.91/s)  LR: 9.902e-04  Data: 0.010 (0.013)
Train: 38 [ 750/1251 ( 60%)]  Loss: 4.164 (4.12)  Time: 0.678s, 1510.02/s  (0.703s, 1456.81/s)  LR: 9.902e-04  Data: 0.010 (0.013)
Train: 38 [ 800/1251 ( 64%)]  Loss: 4.101 (4.12)  Time: 0.672s, 1523.20/s  (0.701s, 1460.69/s)  LR: 9.902e-04  Data: 0.009 (0.013)
Train: 38 [ 850/1251 ( 68%)]  Loss: 3.951 (4.11)  Time: 0.677s, 1512.35/s  (0.700s, 1462.06/s)  LR: 9.902e-04  Data: 0.010 (0.013)
Train: 38 [ 900/1251 ( 72%)]  Loss: 4.196 (4.12)  Time: 0.723s, 1417.13/s  (0.700s, 1462.56/s)  LR: 9.902e-04  Data: 0.010 (0.013)
Train: 38 [ 950/1251 ( 76%)]  Loss: 3.913 (4.11)  Time: 0.707s, 1447.50/s  (0.700s, 1462.96/s)  LR: 9.902e-04  Data: 0.009 (0.012)
Train: 38 [1000/1251 ( 80%)]  Loss: 3.769 (4.09)  Time: 0.680s, 1506.82/s  (0.700s, 1463.35/s)  LR: 9.902e-04  Data: 0.010 (0.012)
Train: 38 [1050/1251 ( 84%)]  Loss: 4.142 (4.09)  Time: 0.673s, 1520.78/s  (0.700s, 1463.17/s)  LR: 9.902e-04  Data: 0.010 (0.012)
Train: 38 [1100/1251 ( 88%)]  Loss: 3.947 (4.09)  Time: 0.719s, 1424.37/s  (0.700s, 1463.71/s)  LR: 9.902e-04  Data: 0.015 (0.012)
Train: 38 [1150/1251 ( 92%)]  Loss: 4.564 (4.11)  Time: 0.704s, 1453.97/s  (0.700s, 1463.31/s)  LR: 9.902e-04  Data: 0.011 (0.012)
Train: 38 [1200/1251 ( 96%)]  Loss: 3.666 (4.09)  Time: 0.735s, 1393.63/s  (0.700s, 1463.37/s)  LR: 9.902e-04  Data: 0.009 (0.012)
Train: 38 [1250/1251 (100%)]  Loss: 3.835 (4.08)  Time: 0.693s, 1477.83/s  (0.700s, 1462.56/s)  LR: 9.902e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.594 (1.594)  Loss:  1.0889 (1.0889)  Acc@1: 83.7891 (83.7891)  Acc@5: 95.0195 (95.0195)
Test: [  48/48]  Time: 0.136 (0.581)  Loss:  1.2480 (1.7588)  Acc@1: 79.8349 (65.5080)  Acc@5: 93.2783 (86.8220)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-38.pth.tar', 65.50800003173828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-36.pth.tar', 65.09400009765625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-37.pth.tar', 64.62000003417968)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-31.pth.tar', 64.4860001123047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-35.pth.tar', 64.24000018310547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-33.pth.tar', 63.93400006103516)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-27.pth.tar', 63.54799994140625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-32.pth.tar', 63.521999946289064)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-34.pth.tar', 63.48799993408203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-28.pth.tar', 62.978000092773435)

Train: 39 [   0/1251 (  0%)]  Loss: 4.226 (4.23)  Time: 2.097s,  488.22/s  (2.097s,  488.22/s)  LR: 9.897e-04  Data: 1.481 (1.481)
Train: 39 [  50/1251 (  4%)]  Loss: 4.313 (4.27)  Time: 0.705s, 1452.02/s  (0.733s, 1396.56/s)  LR: 9.897e-04  Data: 0.010 (0.046)
Train: 39 [ 100/1251 (  8%)]  Loss: 4.439 (4.33)  Time: 0.669s, 1531.46/s  (0.715s, 1432.25/s)  LR: 9.897e-04  Data: 0.013 (0.028)
Train: 39 [ 150/1251 ( 12%)]  Loss: 3.890 (4.22)  Time: 0.752s, 1362.32/s  (0.711s, 1440.95/s)  LR: 9.897e-04  Data: 0.010 (0.022)
Train: 39 [ 200/1251 ( 16%)]  Loss: 4.237 (4.22)  Time: 0.712s, 1438.68/s  (0.707s, 1448.37/s)  LR: 9.897e-04  Data: 0.009 (0.019)
Train: 39 [ 250/1251 ( 20%)]  Loss: 3.564 (4.11)  Time: 0.707s, 1447.39/s  (0.706s, 1449.64/s)  LR: 9.897e-04  Data: 0.011 (0.017)
Train: 39 [ 300/1251 ( 24%)]  Loss: 3.881 (4.08)  Time: 0.720s, 1421.99/s  (0.706s, 1450.01/s)  LR: 9.897e-04  Data: 0.010 (0.016)
Train: 39 [ 350/1251 ( 28%)]  Loss: 4.520 (4.13)  Time: 0.672s, 1522.74/s  (0.705s, 1452.07/s)  LR: 9.897e-04  Data: 0.009 (0.015)
Train: 39 [ 400/1251 ( 32%)]  Loss: 4.237 (4.15)  Time: 0.705s, 1452.53/s  (0.705s, 1453.26/s)  LR: 9.897e-04  Data: 0.009 (0.015)
Train: 39 [ 450/1251 ( 36%)]  Loss: 3.696 (4.10)  Time: 0.904s, 1133.12/s  (0.704s, 1454.58/s)  LR: 9.897e-04  Data: 0.010 (0.014)
Train: 39 [ 500/1251 ( 40%)]  Loss: 3.551 (4.05)  Time: 0.719s, 1424.71/s  (0.704s, 1453.61/s)  LR: 9.897e-04  Data: 0.010 (0.014)
Train: 39 [ 550/1251 ( 44%)]  Loss: 4.147 (4.06)  Time: 0.721s, 1420.80/s  (0.703s, 1455.69/s)  LR: 9.897e-04  Data: 0.011 (0.013)
Train: 39 [ 600/1251 ( 48%)]  Loss: 4.199 (4.07)  Time: 0.712s, 1438.73/s  (0.703s, 1456.42/s)  LR: 9.897e-04  Data: 0.011 (0.013)
Train: 39 [ 650/1251 ( 52%)]  Loss: 4.066 (4.07)  Time: 0.731s, 1400.81/s  (0.703s, 1455.90/s)  LR: 9.897e-04  Data: 0.010 (0.013)
Train: 39 [ 700/1251 ( 56%)]  Loss: 4.019 (4.07)  Time: 0.681s, 1502.81/s  (0.703s, 1456.42/s)  LR: 9.897e-04  Data: 0.011 (0.013)
Train: 39 [ 750/1251 ( 60%)]  Loss: 3.987 (4.06)  Time: 0.773s, 1325.19/s  (0.703s, 1457.20/s)  LR: 9.897e-04  Data: 0.010 (0.013)
Train: 39 [ 800/1251 ( 64%)]  Loss: 4.065 (4.06)  Time: 0.684s, 1497.12/s  (0.703s, 1457.63/s)  LR: 9.897e-04  Data: 0.010 (0.012)
Train: 39 [ 850/1251 ( 68%)]  Loss: 4.012 (4.06)  Time: 0.703s, 1456.90/s  (0.702s, 1458.56/s)  LR: 9.897e-04  Data: 0.009 (0.012)
Train: 39 [ 900/1251 ( 72%)]  Loss: 4.119 (4.06)  Time: 0.702s, 1459.52/s  (0.702s, 1458.46/s)  LR: 9.897e-04  Data: 0.014 (0.012)
Train: 39 [ 950/1251 ( 76%)]  Loss: 4.309 (4.07)  Time: 0.701s, 1461.11/s  (0.702s, 1458.35/s)  LR: 9.897e-04  Data: 0.010 (0.012)
Train: 39 [1000/1251 ( 80%)]  Loss: 4.393 (4.09)  Time: 0.729s, 1405.00/s  (0.702s, 1457.77/s)  LR: 9.897e-04  Data: 0.009 (0.012)
Train: 39 [1050/1251 ( 84%)]  Loss: 4.210 (4.09)  Time: 0.681s, 1504.44/s  (0.702s, 1458.09/s)  LR: 9.897e-04  Data: 0.010 (0.012)
Train: 39 [1100/1251 ( 88%)]  Loss: 4.191 (4.10)  Time: 0.705s, 1451.62/s  (0.702s, 1458.88/s)  LR: 9.897e-04  Data: 0.011 (0.012)
Train: 39 [1150/1251 ( 92%)]  Loss: 3.787 (4.09)  Time: 0.710s, 1441.33/s  (0.702s, 1458.94/s)  LR: 9.897e-04  Data: 0.010 (0.012)
Train: 39 [1200/1251 ( 96%)]  Loss: 4.014 (4.08)  Time: 0.700s, 1462.47/s  (0.702s, 1458.78/s)  LR: 9.897e-04  Data: 0.009 (0.012)
Train: 39 [1250/1251 (100%)]  Loss: 4.460 (4.10)  Time: 0.683s, 1499.26/s  (0.702s, 1459.11/s)  LR: 9.897e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.638 (1.638)  Loss:  1.1055 (1.1055)  Acc@1: 81.7383 (81.7383)  Acc@5: 94.7266 (94.7266)
Test: [  48/48]  Time: 0.136 (0.570)  Loss:  1.3975 (1.8561)  Acc@1: 76.7689 (63.9640)  Acc@5: 92.4528 (85.9360)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-38.pth.tar', 65.50800003173828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-36.pth.tar', 65.09400009765625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-37.pth.tar', 64.62000003417968)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-31.pth.tar', 64.4860001123047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-35.pth.tar', 64.24000018310547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-39.pth.tar', 63.96400012207031)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-33.pth.tar', 63.93400006103516)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-27.pth.tar', 63.54799994140625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-32.pth.tar', 63.521999946289064)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-34.pth.tar', 63.48799993408203)

Train: 40 [   0/1251 (  0%)]  Loss: 4.278 (4.28)  Time: 2.081s,  492.04/s  (2.081s,  492.04/s)  LR: 9.892e-04  Data: 1.463 (1.463)
Train: 40 [  50/1251 (  4%)]  Loss: 3.854 (4.07)  Time: 0.702s, 1458.59/s  (0.728s, 1406.67/s)  LR: 9.892e-04  Data: 0.009 (0.046)
Train: 40 [ 100/1251 (  8%)]  Loss: 4.071 (4.07)  Time: 0.667s, 1535.63/s  (0.712s, 1437.52/s)  LR: 9.892e-04  Data: 0.010 (0.028)
Train: 40 [ 150/1251 ( 12%)]  Loss: 4.310 (4.13)  Time: 0.699s, 1465.23/s  (0.707s, 1448.89/s)  LR: 9.892e-04  Data: 0.009 (0.022)
Train: 40 [ 200/1251 ( 16%)]  Loss: 4.138 (4.13)  Time: 0.703s, 1456.40/s  (0.704s, 1454.23/s)  LR: 9.892e-04  Data: 0.011 (0.019)
Train: 40 [ 250/1251 ( 20%)]  Loss: 4.198 (4.14)  Time: 0.705s, 1453.37/s  (0.704s, 1455.47/s)  LR: 9.892e-04  Data: 0.009 (0.017)
Train: 40 [ 300/1251 ( 24%)]  Loss: 4.397 (4.18)  Time: 0.680s, 1506.64/s  (0.703s, 1457.18/s)  LR: 9.892e-04  Data: 0.011 (0.016)
Train: 40 [ 350/1251 ( 28%)]  Loss: 4.453 (4.21)  Time: 0.671s, 1525.86/s  (0.702s, 1459.03/s)  LR: 9.892e-04  Data: 0.009 (0.015)
Train: 40 [ 400/1251 ( 32%)]  Loss: 4.132 (4.20)  Time: 0.704s, 1454.41/s  (0.701s, 1460.11/s)  LR: 9.892e-04  Data: 0.009 (0.015)
Train: 40 [ 450/1251 ( 36%)]  Loss: 4.435 (4.23)  Time: 0.712s, 1437.40/s  (0.701s, 1460.19/s)  LR: 9.892e-04  Data: 0.009 (0.014)
Train: 40 [ 500/1251 ( 40%)]  Loss: 4.060 (4.21)  Time: 0.690s, 1483.15/s  (0.701s, 1460.46/s)  LR: 9.892e-04  Data: 0.011 (0.014)
Train: 40 [ 550/1251 ( 44%)]  Loss: 4.177 (4.21)  Time: 0.716s, 1429.59/s  (0.701s, 1461.42/s)  LR: 9.892e-04  Data: 0.009 (0.014)
Train: 40 [ 600/1251 ( 48%)]  Loss: 4.111 (4.20)  Time: 0.672s, 1524.68/s  (0.701s, 1460.15/s)  LR: 9.892e-04  Data: 0.011 (0.013)
Train: 40 [ 650/1251 ( 52%)]  Loss: 3.928 (4.18)  Time: 0.680s, 1505.02/s  (0.701s, 1460.72/s)  LR: 9.892e-04  Data: 0.010 (0.013)
Train: 40 [ 700/1251 ( 56%)]  Loss: 3.923 (4.16)  Time: 0.675s, 1516.78/s  (0.701s, 1460.59/s)  LR: 9.892e-04  Data: 0.011 (0.013)
Train: 40 [ 750/1251 ( 60%)]  Loss: 4.214 (4.17)  Time: 0.710s, 1442.42/s  (0.701s, 1461.81/s)  LR: 9.892e-04  Data: 0.009 (0.013)
Train: 40 [ 800/1251 ( 64%)]  Loss: 4.165 (4.17)  Time: 0.730s, 1403.06/s  (0.700s, 1462.37/s)  LR: 9.892e-04  Data: 0.013 (0.012)
Train: 40 [ 850/1251 ( 68%)]  Loss: 4.033 (4.16)  Time: 0.694s, 1475.17/s  (0.700s, 1462.28/s)  LR: 9.892e-04  Data: 0.009 (0.012)
Train: 40 [ 900/1251 ( 72%)]  Loss: 3.616 (4.13)  Time: 0.718s, 1425.46/s  (0.700s, 1462.40/s)  LR: 9.892e-04  Data: 0.013 (0.012)
Train: 40 [ 950/1251 ( 76%)]  Loss: 4.330 (4.14)  Time: 0.675s, 1516.34/s  (0.701s, 1461.77/s)  LR: 9.892e-04  Data: 0.010 (0.012)
Train: 40 [1000/1251 ( 80%)]  Loss: 4.199 (4.14)  Time: 0.705s, 1452.05/s  (0.701s, 1461.58/s)  LR: 9.892e-04  Data: 0.011 (0.012)
Train: 40 [1050/1251 ( 84%)]  Loss: 4.406 (4.16)  Time: 0.673s, 1521.40/s  (0.700s, 1461.93/s)  LR: 9.892e-04  Data: 0.010 (0.012)
Train: 40 [1100/1251 ( 88%)]  Loss: 4.060 (4.15)  Time: 0.711s, 1439.36/s  (0.700s, 1462.19/s)  LR: 9.892e-04  Data: 0.010 (0.012)
Train: 40 [1150/1251 ( 92%)]  Loss: 4.183 (4.15)  Time: 0.705s, 1452.74/s  (0.700s, 1462.67/s)  LR: 9.892e-04  Data: 0.010 (0.012)
Train: 40 [1200/1251 ( 96%)]  Loss: 4.292 (4.16)  Time: 0.702s, 1458.46/s  (0.700s, 1463.47/s)  LR: 9.892e-04  Data: 0.009 (0.012)
Train: 40 [1250/1251 (100%)]  Loss: 3.960 (4.15)  Time: 0.692s, 1478.93/s  (0.700s, 1462.77/s)  LR: 9.892e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.489 (1.489)  Loss:  1.0947 (1.0947)  Acc@1: 83.6914 (83.6914)  Acc@5: 93.9453 (93.9453)
Test: [  48/48]  Time: 0.136 (0.571)  Loss:  1.3086 (1.7710)  Acc@1: 78.7736 (65.1680)  Acc@5: 93.0424 (86.7460)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-38.pth.tar', 65.50800003173828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-40.pth.tar', 65.16800008789062)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-36.pth.tar', 65.09400009765625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-37.pth.tar', 64.62000003417968)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-31.pth.tar', 64.4860001123047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-35.pth.tar', 64.24000018310547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-39.pth.tar', 63.96400012207031)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-33.pth.tar', 63.93400006103516)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-27.pth.tar', 63.54799994140625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-32.pth.tar', 63.521999946289064)

Train: 41 [   0/1251 (  0%)]  Loss: 4.200 (4.20)  Time: 2.153s,  475.67/s  (2.153s,  475.67/s)  LR: 9.886e-04  Data: 1.537 (1.537)
Train: 41 [  50/1251 (  4%)]  Loss: 4.322 (4.26)  Time: 0.703s, 1456.07/s  (0.743s, 1378.95/s)  LR: 9.886e-04  Data: 0.010 (0.054)
Train: 41 [ 100/1251 (  8%)]  Loss: 4.004 (4.18)  Time: 0.700s, 1462.04/s  (0.723s, 1415.57/s)  LR: 9.886e-04  Data: 0.009 (0.033)
Train: 41 [ 150/1251 ( 12%)]  Loss: 4.392 (4.23)  Time: 0.678s, 1511.10/s  (0.717s, 1429.03/s)  LR: 9.886e-04  Data: 0.012 (0.025)
Train: 41 [ 200/1251 ( 16%)]  Loss: 4.333 (4.25)  Time: 0.678s, 1509.26/s  (0.711s, 1439.26/s)  LR: 9.886e-04  Data: 0.014 (0.021)
Train: 41 [ 250/1251 ( 20%)]  Loss: 4.111 (4.23)  Time: 0.712s, 1438.19/s  (0.709s, 1444.09/s)  LR: 9.886e-04  Data: 0.011 (0.019)
Train: 41 [ 300/1251 ( 24%)]  Loss: 4.126 (4.21)  Time: 0.708s, 1445.51/s  (0.707s, 1448.18/s)  LR: 9.886e-04  Data: 0.010 (0.018)
Train: 41 [ 350/1251 ( 28%)]  Loss: 4.432 (4.24)  Time: 0.708s, 1447.34/s  (0.706s, 1451.37/s)  LR: 9.886e-04  Data: 0.009 (0.017)
Train: 41 [ 400/1251 ( 32%)]  Loss: 3.878 (4.20)  Time: 0.671s, 1525.85/s  (0.704s, 1454.21/s)  LR: 9.886e-04  Data: 0.010 (0.016)
Train: 41 [ 450/1251 ( 36%)]  Loss: 4.429 (4.22)  Time: 0.710s, 1441.46/s  (0.704s, 1454.57/s)  LR: 9.886e-04  Data: 0.009 (0.015)
Train: 41 [ 500/1251 ( 40%)]  Loss: 4.174 (4.22)  Time: 0.673s, 1521.12/s  (0.703s, 1456.92/s)  LR: 9.886e-04  Data: 0.009 (0.015)
Train: 41 [ 550/1251 ( 44%)]  Loss: 4.421 (4.24)  Time: 0.744s, 1376.52/s  (0.702s, 1458.55/s)  LR: 9.886e-04  Data: 0.035 (0.014)
Train: 41 [ 600/1251 ( 48%)]  Loss: 4.281 (4.24)  Time: 0.704s, 1455.23/s  (0.702s, 1458.37/s)  LR: 9.886e-04  Data: 0.010 (0.014)
Train: 41 [ 650/1251 ( 52%)]  Loss: 3.955 (4.22)  Time: 0.673s, 1521.39/s  (0.702s, 1459.33/s)  LR: 9.886e-04  Data: 0.009 (0.014)
Train: 41 [ 700/1251 ( 56%)]  Loss: 4.077 (4.21)  Time: 0.711s, 1440.34/s  (0.701s, 1459.86/s)  LR: 9.886e-04  Data: 0.009 (0.013)
Train: 41 [ 750/1251 ( 60%)]  Loss: 4.237 (4.21)  Time: 0.711s, 1439.31/s  (0.702s, 1459.43/s)  LR: 9.886e-04  Data: 0.013 (0.013)
Train: 41 [ 800/1251 ( 64%)]  Loss: 3.912 (4.19)  Time: 0.673s, 1521.98/s  (0.701s, 1459.86/s)  LR: 9.886e-04  Data: 0.011 (0.013)
Train: 41 [ 850/1251 ( 68%)]  Loss: 3.955 (4.18)  Time: 0.673s, 1522.03/s  (0.702s, 1458.83/s)  LR: 9.886e-04  Data: 0.010 (0.013)
Train: 41 [ 900/1251 ( 72%)]  Loss: 4.282 (4.19)  Time: 0.700s, 1463.71/s  (0.702s, 1459.42/s)  LR: 9.886e-04  Data: 0.009 (0.013)
Train: 41 [ 950/1251 ( 76%)]  Loss: 4.195 (4.19)  Time: 0.676s, 1514.29/s  (0.701s, 1460.26/s)  LR: 9.886e-04  Data: 0.010 (0.013)
Train: 41 [1000/1251 ( 80%)]  Loss: 4.202 (4.19)  Time: 0.708s, 1446.70/s  (0.701s, 1459.98/s)  LR: 9.886e-04  Data: 0.013 (0.012)
Train: 41 [1050/1251 ( 84%)]  Loss: 4.325 (4.19)  Time: 0.745s, 1375.02/s  (0.701s, 1459.98/s)  LR: 9.886e-04  Data: 0.009 (0.012)
Train: 41 [1100/1251 ( 88%)]  Loss: 3.873 (4.18)  Time: 0.719s, 1424.79/s  (0.701s, 1460.35/s)  LR: 9.886e-04  Data: 0.009 (0.012)
Train: 41 [1150/1251 ( 92%)]  Loss: 4.157 (4.18)  Time: 0.691s, 1482.54/s  (0.701s, 1460.50/s)  LR: 9.886e-04  Data: 0.010 (0.012)
Train: 41 [1200/1251 ( 96%)]  Loss: 4.070 (4.17)  Time: 0.801s, 1277.90/s  (0.701s, 1460.43/s)  LR: 9.886e-04  Data: 0.009 (0.012)
Train: 41 [1250/1251 (100%)]  Loss: 3.943 (4.16)  Time: 0.660s, 1551.62/s  (0.701s, 1461.17/s)  LR: 9.886e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.470 (1.470)  Loss:  0.9985 (0.9985)  Acc@1: 83.8867 (83.8867)  Acc@5: 96.1914 (96.1914)
Test: [  48/48]  Time: 0.136 (0.594)  Loss:  1.0918 (1.6999)  Acc@1: 80.8962 (66.1520)  Acc@5: 94.5755 (87.4260)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-41.pth.tar', 66.15200010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-38.pth.tar', 65.50800003173828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-40.pth.tar', 65.16800008789062)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-36.pth.tar', 65.09400009765625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-37.pth.tar', 64.62000003417968)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-31.pth.tar', 64.4860001123047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-35.pth.tar', 64.24000018310547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-39.pth.tar', 63.96400012207031)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-33.pth.tar', 63.93400006103516)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-27.pth.tar', 63.54799994140625)

Train: 42 [   0/1251 (  0%)]  Loss: 4.182 (4.18)  Time: 2.324s,  440.55/s  (2.324s,  440.55/s)  LR: 9.881e-04  Data: 1.656 (1.656)
Train: 42 [  50/1251 (  4%)]  Loss: 3.904 (4.04)  Time: 0.681s, 1503.76/s  (0.743s, 1378.59/s)  LR: 9.881e-04  Data: 0.010 (0.055)
Train: 42 [ 100/1251 (  8%)]  Loss: 4.387 (4.16)  Time: 0.717s, 1427.22/s  (0.723s, 1417.28/s)  LR: 9.881e-04  Data: 0.009 (0.033)
Train: 42 [ 150/1251 ( 12%)]  Loss: 3.864 (4.08)  Time: 0.734s, 1394.29/s  (0.717s, 1428.44/s)  LR: 9.881e-04  Data: 0.009 (0.025)
Train: 42 [ 200/1251 ( 16%)]  Loss: 4.256 (4.12)  Time: 0.733s, 1396.30/s  (0.714s, 1434.68/s)  LR: 9.881e-04  Data: 0.009 (0.022)
Train: 42 [ 250/1251 ( 20%)]  Loss: 3.893 (4.08)  Time: 0.704s, 1454.39/s  (0.712s, 1437.44/s)  LR: 9.881e-04  Data: 0.011 (0.019)
Train: 42 [ 300/1251 ( 24%)]  Loss: 3.897 (4.05)  Time: 0.706s, 1449.48/s  (0.711s, 1440.97/s)  LR: 9.881e-04  Data: 0.011 (0.018)
Train: 42 [ 350/1251 ( 28%)]  Loss: 4.156 (4.07)  Time: 0.674s, 1518.67/s  (0.709s, 1444.40/s)  LR: 9.881e-04  Data: 0.010 (0.017)
Train: 42 [ 400/1251 ( 32%)]  Loss: 3.707 (4.03)  Time: 0.706s, 1451.31/s  (0.708s, 1447.29/s)  LR: 9.881e-04  Data: 0.011 (0.016)
Train: 42 [ 450/1251 ( 36%)]  Loss: 4.032 (4.03)  Time: 0.727s, 1409.18/s  (0.706s, 1450.41/s)  LR: 9.881e-04  Data: 0.010 (0.015)
Train: 42 [ 500/1251 ( 40%)]  Loss: 4.186 (4.04)  Time: 0.677s, 1513.14/s  (0.705s, 1451.46/s)  LR: 9.881e-04  Data: 0.010 (0.015)
Train: 42 [ 550/1251 ( 44%)]  Loss: 3.941 (4.03)  Time: 0.703s, 1455.75/s  (0.705s, 1452.97/s)  LR: 9.881e-04  Data: 0.009 (0.014)
Train: 42 [ 600/1251 ( 48%)]  Loss: 4.173 (4.04)  Time: 0.682s, 1501.98/s  (0.705s, 1453.17/s)  LR: 9.881e-04  Data: 0.013 (0.014)
Train: 42 [ 650/1251 ( 52%)]  Loss: 3.924 (4.04)  Time: 0.704s, 1454.50/s  (0.705s, 1452.98/s)  LR: 9.881e-04  Data: 0.009 (0.014)
Train: 42 [ 700/1251 ( 56%)]  Loss: 4.434 (4.06)  Time: 0.675s, 1516.73/s  (0.704s, 1454.09/s)  LR: 9.881e-04  Data: 0.009 (0.014)
Train: 42 [ 750/1251 ( 60%)]  Loss: 4.513 (4.09)  Time: 0.699s, 1465.58/s  (0.703s, 1455.67/s)  LR: 9.881e-04  Data: 0.013 (0.013)
Train: 42 [ 800/1251 ( 64%)]  Loss: 4.252 (4.10)  Time: 0.675s, 1517.43/s  (0.703s, 1456.65/s)  LR: 9.881e-04  Data: 0.011 (0.013)
Train: 42 [ 850/1251 ( 68%)]  Loss: 4.024 (4.10)  Time: 0.672s, 1524.47/s  (0.703s, 1456.20/s)  LR: 9.881e-04  Data: 0.010 (0.013)
Train: 42 [ 900/1251 ( 72%)]  Loss: 3.870 (4.08)  Time: 0.709s, 1444.06/s  (0.703s, 1456.89/s)  LR: 9.881e-04  Data: 0.009 (0.013)
Train: 42 [ 950/1251 ( 76%)]  Loss: 4.020 (4.08)  Time: 0.672s, 1523.54/s  (0.703s, 1457.25/s)  LR: 9.881e-04  Data: 0.011 (0.013)
Train: 42 [1000/1251 ( 80%)]  Loss: 3.980 (4.08)  Time: 0.721s, 1421.23/s  (0.703s, 1457.56/s)  LR: 9.881e-04  Data: 0.009 (0.013)
Train: 42 [1050/1251 ( 84%)]  Loss: 4.148 (4.08)  Time: 0.672s, 1523.74/s  (0.702s, 1458.45/s)  LR: 9.881e-04  Data: 0.010 (0.012)
Train: 42 [1100/1251 ( 88%)]  Loss: 4.397 (4.09)  Time: 0.678s, 1510.08/s  (0.702s, 1458.72/s)  LR: 9.881e-04  Data: 0.010 (0.012)
Train: 42 [1150/1251 ( 92%)]  Loss: 3.961 (4.09)  Time: 0.681s, 1503.07/s  (0.702s, 1459.72/s)  LR: 9.881e-04  Data: 0.010 (0.012)
Train: 42 [1200/1251 ( 96%)]  Loss: 4.315 (4.10)  Time: 0.701s, 1461.15/s  (0.701s, 1459.77/s)  LR: 9.881e-04  Data: 0.011 (0.012)
Train: 42 [1250/1251 (100%)]  Loss: 4.082 (4.10)  Time: 0.722s, 1417.73/s  (0.702s, 1459.57/s)  LR: 9.881e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.481 (1.481)  Loss:  0.9463 (0.9463)  Acc@1: 84.4727 (84.4727)  Acc@5: 95.1172 (95.1172)
Test: [  48/48]  Time: 0.136 (0.577)  Loss:  1.0947 (1.6732)  Acc@1: 81.0142 (65.5860)  Acc@5: 93.5142 (86.9680)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-41.pth.tar', 66.15200010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-42.pth.tar', 65.58600002685547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-38.pth.tar', 65.50800003173828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-40.pth.tar', 65.16800008789062)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-36.pth.tar', 65.09400009765625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-37.pth.tar', 64.62000003417968)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-31.pth.tar', 64.4860001123047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-35.pth.tar', 64.24000018310547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-39.pth.tar', 63.96400012207031)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-33.pth.tar', 63.93400006103516)

Train: 43 [   0/1251 (  0%)]  Loss: 4.368 (4.37)  Time: 2.401s,  426.51/s  (2.401s,  426.51/s)  LR: 9.875e-04  Data: 1.769 (1.769)
Train: 43 [  50/1251 (  4%)]  Loss: 4.493 (4.43)  Time: 0.703s, 1455.63/s  (0.750s, 1364.67/s)  LR: 9.875e-04  Data: 0.013 (0.052)
Train: 43 [ 100/1251 (  8%)]  Loss: 4.391 (4.42)  Time: 0.670s, 1527.97/s  (0.726s, 1410.74/s)  LR: 9.875e-04  Data: 0.010 (0.031)
Train: 43 [ 150/1251 ( 12%)]  Loss: 4.266 (4.38)  Time: 0.669s, 1531.60/s  (0.719s, 1423.42/s)  LR: 9.875e-04  Data: 0.010 (0.025)
Train: 43 [ 200/1251 ( 16%)]  Loss: 4.302 (4.36)  Time: 0.704s, 1455.30/s  (0.715s, 1432.61/s)  LR: 9.875e-04  Data: 0.009 (0.021)
Train: 43 [ 250/1251 ( 20%)]  Loss: 3.926 (4.29)  Time: 0.719s, 1423.57/s  (0.711s, 1440.84/s)  LR: 9.875e-04  Data: 0.009 (0.019)
Train: 43 [ 300/1251 ( 24%)]  Loss: 4.063 (4.26)  Time: 0.698s, 1466.30/s  (0.709s, 1444.19/s)  LR: 9.875e-04  Data: 0.010 (0.017)
Train: 43 [ 350/1251 ( 28%)]  Loss: 3.772 (4.20)  Time: 0.704s, 1454.80/s  (0.708s, 1445.90/s)  LR: 9.875e-04  Data: 0.009 (0.016)
Train: 43 [ 400/1251 ( 32%)]  Loss: 4.351 (4.21)  Time: 0.693s, 1476.57/s  (0.707s, 1448.55/s)  LR: 9.875e-04  Data: 0.016 (0.016)
Train: 43 [ 450/1251 ( 36%)]  Loss: 4.260 (4.22)  Time: 0.800s, 1280.09/s  (0.706s, 1450.21/s)  LR: 9.875e-04  Data: 0.009 (0.015)
Train: 43 [ 500/1251 ( 40%)]  Loss: 4.293 (4.23)  Time: 0.668s, 1533.96/s  (0.705s, 1451.72/s)  LR: 9.875e-04  Data: 0.009 (0.015)
Train: 43 [ 550/1251 ( 44%)]  Loss: 3.631 (4.18)  Time: 0.703s, 1457.22/s  (0.705s, 1451.80/s)  LR: 9.875e-04  Data: 0.009 (0.014)
Train: 43 [ 600/1251 ( 48%)]  Loss: 4.190 (4.18)  Time: 0.688s, 1487.55/s  (0.705s, 1452.31/s)  LR: 9.875e-04  Data: 0.009 (0.014)
Train: 43 [ 650/1251 ( 52%)]  Loss: 4.351 (4.19)  Time: 0.693s, 1477.93/s  (0.705s, 1452.55/s)  LR: 9.875e-04  Data: 0.014 (0.014)
Train: 43 [ 700/1251 ( 56%)]  Loss: 3.830 (4.17)  Time: 0.711s, 1440.38/s  (0.705s, 1452.64/s)  LR: 9.875e-04  Data: 0.010 (0.013)
Train: 43 [ 750/1251 ( 60%)]  Loss: 4.175 (4.17)  Time: 0.692s, 1479.26/s  (0.705s, 1453.50/s)  LR: 9.875e-04  Data: 0.010 (0.013)
Train: 43 [ 800/1251 ( 64%)]  Loss: 4.242 (4.17)  Time: 0.679s, 1508.28/s  (0.704s, 1454.83/s)  LR: 9.875e-04  Data: 0.011 (0.013)
Train: 43 [ 850/1251 ( 68%)]  Loss: 4.201 (4.17)  Time: 0.671s, 1525.09/s  (0.704s, 1455.14/s)  LR: 9.875e-04  Data: 0.011 (0.013)
Train: 43 [ 900/1251 ( 72%)]  Loss: 4.306 (4.18)  Time: 0.671s, 1525.85/s  (0.704s, 1454.99/s)  LR: 9.875e-04  Data: 0.010 (0.013)
Train: 43 [ 950/1251 ( 76%)]  Loss: 4.017 (4.17)  Time: 0.712s, 1439.01/s  (0.703s, 1455.63/s)  LR: 9.875e-04  Data: 0.009 (0.013)
Train: 43 [1000/1251 ( 80%)]  Loss: 4.077 (4.17)  Time: 0.724s, 1414.48/s  (0.703s, 1455.98/s)  LR: 9.875e-04  Data: 0.009 (0.013)
Train: 43 [1050/1251 ( 84%)]  Loss: 3.906 (4.16)  Time: 0.674s, 1518.86/s  (0.703s, 1456.93/s)  LR: 9.875e-04  Data: 0.011 (0.012)
Train: 43 [1100/1251 ( 88%)]  Loss: 3.819 (4.14)  Time: 0.740s, 1384.16/s  (0.703s, 1457.34/s)  LR: 9.875e-04  Data: 0.009 (0.012)
Train: 43 [1150/1251 ( 92%)]  Loss: 4.187 (4.14)  Time: 0.678s, 1509.38/s  (0.703s, 1457.04/s)  LR: 9.875e-04  Data: 0.011 (0.012)
Train: 43 [1200/1251 ( 96%)]  Loss: 4.236 (4.15)  Time: 0.741s, 1382.20/s  (0.703s, 1456.95/s)  LR: 9.875e-04  Data: 0.010 (0.012)
Train: 43 [1250/1251 (100%)]  Loss: 3.927 (4.14)  Time: 0.656s, 1560.31/s  (0.703s, 1457.55/s)  LR: 9.875e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.580 (1.580)  Loss:  1.1201 (1.1201)  Acc@1: 83.2031 (83.2031)  Acc@5: 94.3359 (94.3359)
Test: [  48/48]  Time: 0.140 (0.589)  Loss:  1.2012 (1.7971)  Acc@1: 79.9528 (65.3140)  Acc@5: 91.9811 (86.6680)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-41.pth.tar', 66.15200010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-42.pth.tar', 65.58600002685547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-38.pth.tar', 65.50800003173828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-43.pth.tar', 65.31400008300781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-40.pth.tar', 65.16800008789062)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-36.pth.tar', 65.09400009765625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-37.pth.tar', 64.62000003417968)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-31.pth.tar', 64.4860001123047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-35.pth.tar', 64.24000018310547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-39.pth.tar', 63.96400012207031)

Train: 44 [   0/1251 (  0%)]  Loss: 4.083 (4.08)  Time: 2.129s,  481.01/s  (2.129s,  481.01/s)  LR: 9.869e-04  Data: 1.511 (1.511)
Train: 44 [  50/1251 (  4%)]  Loss: 3.986 (4.03)  Time: 0.673s, 1521.49/s  (0.732s, 1398.36/s)  LR: 9.869e-04  Data: 0.011 (0.048)
Train: 44 [ 100/1251 (  8%)]  Loss: 3.610 (3.89)  Time: 0.672s, 1524.91/s  (0.714s, 1433.96/s)  LR: 9.869e-04  Data: 0.009 (0.029)
Train: 44 [ 150/1251 ( 12%)]  Loss: 4.202 (3.97)  Time: 0.705s, 1451.86/s  (0.709s, 1444.28/s)  LR: 9.869e-04  Data: 0.011 (0.023)
Train: 44 [ 200/1251 ( 16%)]  Loss: 4.235 (4.02)  Time: 0.671s, 1525.38/s  (0.708s, 1446.79/s)  LR: 9.869e-04  Data: 0.010 (0.020)
Train: 44 [ 250/1251 ( 20%)]  Loss: 4.401 (4.09)  Time: 0.748s, 1369.56/s  (0.707s, 1447.88/s)  LR: 9.869e-04  Data: 0.009 (0.018)
Train: 44 [ 300/1251 ( 24%)]  Loss: 4.134 (4.09)  Time: 0.696s, 1471.94/s  (0.705s, 1451.66/s)  LR: 9.869e-04  Data: 0.010 (0.017)
Train: 44 [ 350/1251 ( 28%)]  Loss: 4.398 (4.13)  Time: 0.673s, 1520.52/s  (0.704s, 1454.23/s)  LR: 9.869e-04  Data: 0.010 (0.016)
Train: 44 [ 400/1251 ( 32%)]  Loss: 4.457 (4.17)  Time: 0.674s, 1519.54/s  (0.703s, 1455.79/s)  LR: 9.869e-04  Data: 0.010 (0.015)
Train: 44 [ 450/1251 ( 36%)]  Loss: 4.409 (4.19)  Time: 0.675s, 1516.11/s  (0.703s, 1456.96/s)  LR: 9.869e-04  Data: 0.009 (0.015)
Train: 44 [ 500/1251 ( 40%)]  Loss: 4.154 (4.19)  Time: 0.704s, 1454.95/s  (0.702s, 1458.94/s)  LR: 9.869e-04  Data: 0.010 (0.014)
Train: 44 [ 550/1251 ( 44%)]  Loss: 4.139 (4.18)  Time: 0.717s, 1427.46/s  (0.702s, 1458.49/s)  LR: 9.869e-04  Data: 0.010 (0.014)
Train: 44 [ 600/1251 ( 48%)]  Loss: 4.181 (4.18)  Time: 0.704s, 1454.28/s  (0.702s, 1457.99/s)  LR: 9.869e-04  Data: 0.009 (0.013)
Train: 44 [ 650/1251 ( 52%)]  Loss: 4.382 (4.20)  Time: 0.698s, 1467.26/s  (0.702s, 1458.37/s)  LR: 9.869e-04  Data: 0.010 (0.013)
Train: 44 [ 700/1251 ( 56%)]  Loss: 4.316 (4.21)  Time: 0.673s, 1522.19/s  (0.702s, 1458.63/s)  LR: 9.869e-04  Data: 0.010 (0.013)
Train: 44 [ 750/1251 ( 60%)]  Loss: 3.715 (4.18)  Time: 0.717s, 1428.29/s  (0.702s, 1459.38/s)  LR: 9.869e-04  Data: 0.012 (0.013)
Train: 44 [ 800/1251 ( 64%)]  Loss: 3.945 (4.16)  Time: 0.724s, 1414.05/s  (0.702s, 1459.29/s)  LR: 9.869e-04  Data: 0.010 (0.013)
Train: 44 [ 850/1251 ( 68%)]  Loss: 3.812 (4.14)  Time: 0.719s, 1423.74/s  (0.702s, 1459.33/s)  LR: 9.869e-04  Data: 0.009 (0.013)
Train: 44 [ 900/1251 ( 72%)]  Loss: 4.061 (4.14)  Time: 0.704s, 1454.38/s  (0.702s, 1459.35/s)  LR: 9.869e-04  Data: 0.012 (0.012)
Train: 44 [ 950/1251 ( 76%)]  Loss: 4.013 (4.13)  Time: 0.704s, 1455.38/s  (0.701s, 1460.09/s)  LR: 9.869e-04  Data: 0.009 (0.012)
Train: 44 [1000/1251 ( 80%)]  Loss: 3.637 (4.11)  Time: 0.703s, 1456.77/s  (0.702s, 1459.61/s)  LR: 9.869e-04  Data: 0.009 (0.012)
Train: 44 [1050/1251 ( 84%)]  Loss: 3.869 (4.10)  Time: 0.691s, 1482.20/s  (0.702s, 1459.59/s)  LR: 9.869e-04  Data: 0.011 (0.012)
Train: 44 [1100/1251 ( 88%)]  Loss: 4.213 (4.10)  Time: 0.685s, 1494.55/s  (0.701s, 1460.11/s)  LR: 9.869e-04  Data: 0.010 (0.012)
Train: 44 [1150/1251 ( 92%)]  Loss: 4.010 (4.10)  Time: 0.698s, 1467.84/s  (0.701s, 1460.44/s)  LR: 9.869e-04  Data: 0.009 (0.012)
Train: 44 [1200/1251 ( 96%)]  Loss: 3.675 (4.08)  Time: 0.700s, 1463.09/s  (0.701s, 1460.88/s)  LR: 9.869e-04  Data: 0.012 (0.012)
Train: 44 [1250/1251 (100%)]  Loss: 3.614 (4.06)  Time: 0.659s, 1554.51/s  (0.701s, 1461.44/s)  LR: 9.869e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.467 (1.467)  Loss:  1.1836 (1.1836)  Acc@1: 82.4219 (82.4219)  Acc@5: 94.8242 (94.8242)
Test: [  48/48]  Time: 0.136 (0.578)  Loss:  1.1201 (1.7588)  Acc@1: 80.7783 (64.4120)  Acc@5: 94.2217 (86.4100)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-41.pth.tar', 66.15200010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-42.pth.tar', 65.58600002685547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-38.pth.tar', 65.50800003173828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-43.pth.tar', 65.31400008300781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-40.pth.tar', 65.16800008789062)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-36.pth.tar', 65.09400009765625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-37.pth.tar', 64.62000003417968)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-31.pth.tar', 64.4860001123047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-44.pth.tar', 64.41200018310546)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-35.pth.tar', 64.24000018310547)

Train: 45 [   0/1251 (  0%)]  Loss: 3.817 (3.82)  Time: 2.218s,  461.76/s  (2.218s,  461.76/s)  LR: 9.863e-04  Data: 1.603 (1.603)
Train: 45 [  50/1251 (  4%)]  Loss: 3.989 (3.90)  Time: 0.705s, 1452.52/s  (0.736s, 1390.71/s)  LR: 9.863e-04  Data: 0.011 (0.048)
Train: 45 [ 100/1251 (  8%)]  Loss: 4.394 (4.07)  Time: 0.674s, 1519.01/s  (0.719s, 1423.31/s)  LR: 9.863e-04  Data: 0.010 (0.029)
Train: 45 [ 150/1251 ( 12%)]  Loss: 4.246 (4.11)  Time: 0.707s, 1449.31/s  (0.713s, 1435.39/s)  LR: 9.863e-04  Data: 0.009 (0.023)
Train: 45 [ 200/1251 ( 16%)]  Loss: 3.860 (4.06)  Time: 0.671s, 1525.38/s  (0.710s, 1442.27/s)  LR: 9.863e-04  Data: 0.010 (0.020)
Train: 45 [ 250/1251 ( 20%)]  Loss: 4.402 (4.12)  Time: 0.725s, 1412.88/s  (0.708s, 1446.06/s)  LR: 9.863e-04  Data: 0.010 (0.018)
Train: 45 [ 300/1251 ( 24%)]  Loss: 3.807 (4.07)  Time: 0.704s, 1453.98/s  (0.705s, 1453.12/s)  LR: 9.863e-04  Data: 0.009 (0.016)
Train: 45 [ 350/1251 ( 28%)]  Loss: 3.825 (4.04)  Time: 0.672s, 1523.02/s  (0.704s, 1454.37/s)  LR: 9.863e-04  Data: 0.010 (0.016)
Train: 45 [ 400/1251 ( 32%)]  Loss: 3.864 (4.02)  Time: 0.676s, 1514.57/s  (0.702s, 1459.20/s)  LR: 9.863e-04  Data: 0.010 (0.015)
Train: 45 [ 450/1251 ( 36%)]  Loss: 4.234 (4.04)  Time: 0.671s, 1526.36/s  (0.702s, 1458.99/s)  LR: 9.863e-04  Data: 0.010 (0.014)
Train: 45 [ 500/1251 ( 40%)]  Loss: 4.193 (4.06)  Time: 0.712s, 1438.23/s  (0.701s, 1459.82/s)  LR: 9.863e-04  Data: 0.009 (0.014)
Train: 45 [ 550/1251 ( 44%)]  Loss: 4.164 (4.07)  Time: 0.708s, 1445.40/s  (0.702s, 1459.39/s)  LR: 9.863e-04  Data: 0.010 (0.014)
Train: 45 [ 600/1251 ( 48%)]  Loss: 3.840 (4.05)  Time: 0.718s, 1426.36/s  (0.701s, 1459.98/s)  LR: 9.863e-04  Data: 0.009 (0.013)
Train: 45 [ 650/1251 ( 52%)]  Loss: 3.631 (4.02)  Time: 0.716s, 1429.40/s  (0.701s, 1460.69/s)  LR: 9.863e-04  Data: 0.009 (0.013)
Train: 45 [ 700/1251 ( 56%)]  Loss: 4.019 (4.02)  Time: 0.719s, 1425.19/s  (0.701s, 1461.18/s)  LR: 9.863e-04  Data: 0.010 (0.013)
Train: 45 [ 750/1251 ( 60%)]  Loss: 3.891 (4.01)  Time: 0.708s, 1446.45/s  (0.701s, 1461.74/s)  LR: 9.863e-04  Data: 0.009 (0.013)
Train: 45 [ 800/1251 ( 64%)]  Loss: 4.118 (4.02)  Time: 0.705s, 1453.34/s  (0.701s, 1461.19/s)  LR: 9.863e-04  Data: 0.011 (0.013)
Train: 45 [ 850/1251 ( 68%)]  Loss: 3.962 (4.01)  Time: 0.673s, 1521.92/s  (0.701s, 1461.05/s)  LR: 9.863e-04  Data: 0.010 (0.012)
Train: 45 [ 900/1251 ( 72%)]  Loss: 4.111 (4.02)  Time: 0.676s, 1513.93/s  (0.700s, 1462.24/s)  LR: 9.863e-04  Data: 0.010 (0.012)
Train: 45 [ 950/1251 ( 76%)]  Loss: 4.455 (4.04)  Time: 0.726s, 1410.14/s  (0.700s, 1463.04/s)  LR: 9.863e-04  Data: 0.009 (0.012)
Train: 45 [1000/1251 ( 80%)]  Loss: 4.231 (4.05)  Time: 0.706s, 1450.19/s  (0.700s, 1462.89/s)  LR: 9.863e-04  Data: 0.010 (0.012)
Train: 45 [1050/1251 ( 84%)]  Loss: 4.196 (4.06)  Time: 0.742s, 1379.46/s  (0.700s, 1462.66/s)  LR: 9.863e-04  Data: 0.009 (0.012)
Train: 45 [1100/1251 ( 88%)]  Loss: 3.859 (4.05)  Time: 0.684s, 1497.70/s  (0.700s, 1462.84/s)  LR: 9.863e-04  Data: 0.010 (0.012)
Train: 45 [1150/1251 ( 92%)]  Loss: 3.911 (4.04)  Time: 0.712s, 1438.47/s  (0.700s, 1463.14/s)  LR: 9.863e-04  Data: 0.009 (0.012)
Train: 45 [1200/1251 ( 96%)]  Loss: 4.552 (4.06)  Time: 0.751s, 1364.39/s  (0.700s, 1463.22/s)  LR: 9.863e-04  Data: 0.010 (0.012)
Train: 45 [1250/1251 (100%)]  Loss: 4.281 (4.07)  Time: 0.661s, 1549.08/s  (0.700s, 1462.58/s)  LR: 9.863e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.538 (1.538)  Loss:  1.2617 (1.2617)  Acc@1: 80.1758 (80.1758)  Acc@5: 93.2617 (93.2617)
Test: [  48/48]  Time: 0.136 (0.585)  Loss:  1.3223 (1.7942)  Acc@1: 80.1887 (65.7780)  Acc@5: 93.2783 (87.0200)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-41.pth.tar', 66.15200010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-45.pth.tar', 65.77800005615235)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-42.pth.tar', 65.58600002685547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-38.pth.tar', 65.50800003173828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-43.pth.tar', 65.31400008300781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-40.pth.tar', 65.16800008789062)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-36.pth.tar', 65.09400009765625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-37.pth.tar', 64.62000003417968)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-31.pth.tar', 64.4860001123047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-44.pth.tar', 64.41200018310546)

Train: 46 [   0/1251 (  0%)]  Loss: 3.969 (3.97)  Time: 2.195s,  466.59/s  (2.195s,  466.59/s)  LR: 9.857e-04  Data: 1.579 (1.579)
Train: 46 [  50/1251 (  4%)]  Loss: 4.332 (4.15)  Time: 0.674s, 1520.11/s  (0.730s, 1403.19/s)  LR: 9.857e-04  Data: 0.009 (0.047)
Train: 46 [ 100/1251 (  8%)]  Loss: 4.267 (4.19)  Time: 0.729s, 1404.79/s  (0.718s, 1426.37/s)  LR: 9.857e-04  Data: 0.010 (0.029)
Train: 46 [ 150/1251 ( 12%)]  Loss: 3.610 (4.04)  Time: 0.717s, 1427.97/s  (0.712s, 1438.19/s)  LR: 9.857e-04  Data: 0.009 (0.023)
Train: 46 [ 200/1251 ( 16%)]  Loss: 4.170 (4.07)  Time: 0.717s, 1427.30/s  (0.708s, 1445.35/s)  LR: 9.857e-04  Data: 0.009 (0.020)
Train: 46 [ 250/1251 ( 20%)]  Loss: 4.040 (4.06)  Time: 0.671s, 1525.05/s  (0.708s, 1446.26/s)  LR: 9.857e-04  Data: 0.010 (0.018)
Train: 46 [ 300/1251 ( 24%)]  Loss: 4.117 (4.07)  Time: 0.672s, 1524.00/s  (0.708s, 1447.29/s)  LR: 9.857e-04  Data: 0.010 (0.017)
Train: 46 [ 350/1251 ( 28%)]  Loss: 3.686 (4.02)  Time: 0.706s, 1450.98/s  (0.707s, 1448.55/s)  LR: 9.857e-04  Data: 0.009 (0.016)
Train: 46 [ 400/1251 ( 32%)]  Loss: 4.039 (4.03)  Time: 0.707s, 1448.43/s  (0.706s, 1450.54/s)  LR: 9.857e-04  Data: 0.010 (0.015)
Train: 46 [ 450/1251 ( 36%)]  Loss: 4.150 (4.04)  Time: 0.705s, 1453.35/s  (0.705s, 1453.01/s)  LR: 9.857e-04  Data: 0.010 (0.014)
Train: 46 [ 500/1251 ( 40%)]  Loss: 3.681 (4.01)  Time: 0.717s, 1427.48/s  (0.704s, 1453.59/s)  LR: 9.857e-04  Data: 0.010 (0.014)
Train: 46 [ 550/1251 ( 44%)]  Loss: 4.249 (4.03)  Time: 0.714s, 1434.75/s  (0.704s, 1453.71/s)  LR: 9.857e-04  Data: 0.013 (0.014)
Train: 46 [ 600/1251 ( 48%)]  Loss: 4.311 (4.05)  Time: 0.680s, 1506.05/s  (0.705s, 1453.30/s)  LR: 9.857e-04  Data: 0.009 (0.013)
Train: 46 [ 650/1251 ( 52%)]  Loss: 4.397 (4.07)  Time: 0.703s, 1457.43/s  (0.704s, 1454.72/s)  LR: 9.857e-04  Data: 0.009 (0.013)
Train: 46 [ 700/1251 ( 56%)]  Loss: 4.440 (4.10)  Time: 0.704s, 1454.24/s  (0.704s, 1454.43/s)  LR: 9.857e-04  Data: 0.009 (0.013)
Train: 46 [ 750/1251 ( 60%)]  Loss: 4.218 (4.10)  Time: 0.706s, 1449.96/s  (0.704s, 1454.83/s)  LR: 9.857e-04  Data: 0.009 (0.013)
Train: 46 [ 800/1251 ( 64%)]  Loss: 4.238 (4.11)  Time: 0.698s, 1467.71/s  (0.704s, 1455.01/s)  LR: 9.857e-04  Data: 0.009 (0.013)
Train: 46 [ 850/1251 ( 68%)]  Loss: 4.141 (4.11)  Time: 0.668s, 1533.86/s  (0.704s, 1455.56/s)  LR: 9.857e-04  Data: 0.012 (0.013)
Train: 46 [ 900/1251 ( 72%)]  Loss: 4.462 (4.13)  Time: 0.712s, 1437.20/s  (0.703s, 1456.35/s)  LR: 9.857e-04  Data: 0.009 (0.012)
Train: 46 [ 950/1251 ( 76%)]  Loss: 3.767 (4.11)  Time: 0.726s, 1410.62/s  (0.703s, 1456.45/s)  LR: 9.857e-04  Data: 0.011 (0.012)
Train: 46 [1000/1251 ( 80%)]  Loss: 4.132 (4.12)  Time: 0.705s, 1453.27/s  (0.703s, 1456.37/s)  LR: 9.857e-04  Data: 0.010 (0.012)
Train: 46 [1050/1251 ( 84%)]  Loss: 4.003 (4.11)  Time: 0.766s, 1336.81/s  (0.703s, 1456.30/s)  LR: 9.857e-04  Data: 0.009 (0.012)
Train: 46 [1100/1251 ( 88%)]  Loss: 4.164 (4.11)  Time: 0.707s, 1448.43/s  (0.703s, 1456.83/s)  LR: 9.857e-04  Data: 0.010 (0.012)
Train: 46 [1150/1251 ( 92%)]  Loss: 4.033 (4.11)  Time: 0.671s, 1526.51/s  (0.703s, 1456.93/s)  LR: 9.857e-04  Data: 0.010 (0.012)
Train: 46 [1200/1251 ( 96%)]  Loss: 3.899 (4.10)  Time: 0.677s, 1512.32/s  (0.702s, 1458.06/s)  LR: 9.857e-04  Data: 0.011 (0.012)
Train: 46 [1250/1251 (100%)]  Loss: 4.211 (4.10)  Time: 0.711s, 1440.10/s  (0.702s, 1458.37/s)  LR: 9.857e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.516 (1.516)  Loss:  1.0684 (1.0684)  Acc@1: 84.2773 (84.2773)  Acc@5: 96.0938 (96.0938)
Test: [  48/48]  Time: 0.136 (0.583)  Loss:  1.2002 (1.8836)  Acc@1: 81.4858 (65.1420)  Acc@5: 93.6321 (86.5160)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-41.pth.tar', 66.15200010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-45.pth.tar', 65.77800005615235)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-42.pth.tar', 65.58600002685547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-38.pth.tar', 65.50800003173828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-43.pth.tar', 65.31400008300781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-40.pth.tar', 65.16800008789062)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-46.pth.tar', 65.14199997314454)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-36.pth.tar', 65.09400009765625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-37.pth.tar', 64.62000003417968)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-31.pth.tar', 64.4860001123047)

Train: 47 [   0/1251 (  0%)]  Loss: 4.110 (4.11)  Time: 2.125s,  481.95/s  (2.125s,  481.95/s)  LR: 9.851e-04  Data: 1.509 (1.509)
Train: 47 [  50/1251 (  4%)]  Loss: 4.160 (4.14)  Time: 0.717s, 1428.95/s  (0.735s, 1393.61/s)  LR: 9.851e-04  Data: 0.014 (0.048)
Train: 47 [ 100/1251 (  8%)]  Loss: 3.949 (4.07)  Time: 0.678s, 1509.79/s  (0.717s, 1429.17/s)  LR: 9.851e-04  Data: 0.010 (0.030)
Train: 47 [ 150/1251 ( 12%)]  Loss: 4.216 (4.11)  Time: 0.776s, 1319.51/s  (0.710s, 1442.84/s)  LR: 9.851e-04  Data: 0.009 (0.023)
Train: 47 [ 200/1251 ( 16%)]  Loss: 3.539 (3.99)  Time: 0.684s, 1496.18/s  (0.705s, 1452.72/s)  LR: 9.851e-04  Data: 0.009 (0.020)
Train: 47 [ 250/1251 ( 20%)]  Loss: 4.514 (4.08)  Time: 0.704s, 1453.81/s  (0.704s, 1453.59/s)  LR: 9.851e-04  Data: 0.010 (0.018)
Train: 47 [ 300/1251 ( 24%)]  Loss: 3.793 (4.04)  Time: 0.705s, 1451.90/s  (0.703s, 1457.37/s)  LR: 9.851e-04  Data: 0.011 (0.017)
Train: 47 [ 350/1251 ( 28%)]  Loss: 3.887 (4.02)  Time: 0.710s, 1442.08/s  (0.701s, 1460.18/s)  LR: 9.851e-04  Data: 0.010 (0.016)
Train: 47 [ 400/1251 ( 32%)]  Loss: 4.021 (4.02)  Time: 0.704s, 1455.51/s  (0.700s, 1462.42/s)  LR: 9.851e-04  Data: 0.009 (0.015)
Train: 47 [ 450/1251 ( 36%)]  Loss: 3.832 (4.00)  Time: 0.674s, 1519.95/s  (0.700s, 1462.98/s)  LR: 9.851e-04  Data: 0.009 (0.014)
Train: 47 [ 500/1251 ( 40%)]  Loss: 4.461 (4.04)  Time: 0.704s, 1454.55/s  (0.700s, 1463.24/s)  LR: 9.851e-04  Data: 0.009 (0.014)
Train: 47 [ 550/1251 ( 44%)]  Loss: 3.863 (4.03)  Time: 0.705s, 1451.67/s  (0.700s, 1462.52/s)  LR: 9.851e-04  Data: 0.009 (0.014)
Train: 47 [ 600/1251 ( 48%)]  Loss: 4.269 (4.05)  Time: 0.706s, 1450.87/s  (0.700s, 1463.67/s)  LR: 9.851e-04  Data: 0.009 (0.013)
Train: 47 [ 650/1251 ( 52%)]  Loss: 4.324 (4.07)  Time: 0.670s, 1527.54/s  (0.699s, 1464.97/s)  LR: 9.851e-04  Data: 0.012 (0.013)
Train: 47 [ 700/1251 ( 56%)]  Loss: 3.448 (4.03)  Time: 0.699s, 1465.07/s  (0.699s, 1464.24/s)  LR: 9.851e-04  Data: 0.009 (0.013)
Train: 47 [ 750/1251 ( 60%)]  Loss: 3.576 (4.00)  Time: 0.676s, 1515.34/s  (0.699s, 1464.12/s)  LR: 9.851e-04  Data: 0.015 (0.013)
Train: 47 [ 800/1251 ( 64%)]  Loss: 4.155 (4.01)  Time: 0.772s, 1326.81/s  (0.700s, 1463.17/s)  LR: 9.851e-04  Data: 0.009 (0.012)
Train: 47 [ 850/1251 ( 68%)]  Loss: 3.990 (4.01)  Time: 0.698s, 1466.42/s  (0.700s, 1463.55/s)  LR: 9.851e-04  Data: 0.010 (0.012)
Train: 47 [ 900/1251 ( 72%)]  Loss: 4.251 (4.02)  Time: 0.672s, 1524.54/s  (0.699s, 1464.07/s)  LR: 9.851e-04  Data: 0.010 (0.012)
Train: 47 [ 950/1251 ( 76%)]  Loss: 3.724 (4.00)  Time: 0.728s, 1406.93/s  (0.700s, 1463.83/s)  LR: 9.851e-04  Data: 0.010 (0.012)
Train: 47 [1000/1251 ( 80%)]  Loss: 4.115 (4.01)  Time: 0.705s, 1453.17/s  (0.699s, 1464.00/s)  LR: 9.851e-04  Data: 0.010 (0.012)
Train: 47 [1050/1251 ( 84%)]  Loss: 4.162 (4.02)  Time: 0.765s, 1338.92/s  (0.699s, 1464.02/s)  LR: 9.851e-04  Data: 0.009 (0.012)
Train: 47 [1100/1251 ( 88%)]  Loss: 4.300 (4.03)  Time: 0.673s, 1522.58/s  (0.699s, 1463.92/s)  LR: 9.851e-04  Data: 0.010 (0.012)
Train: 47 [1150/1251 ( 92%)]  Loss: 4.046 (4.03)  Time: 0.666s, 1538.20/s  (0.700s, 1463.59/s)  LR: 9.851e-04  Data: 0.010 (0.012)
Train: 47 [1200/1251 ( 96%)]  Loss: 3.954 (4.03)  Time: 0.729s, 1404.84/s  (0.700s, 1462.61/s)  LR: 9.851e-04  Data: 0.009 (0.012)
Train: 47 [1250/1251 (100%)]  Loss: 4.107 (4.03)  Time: 0.677s, 1511.73/s  (0.701s, 1461.45/s)  LR: 9.851e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.584 (1.584)  Loss:  1.0557 (1.0557)  Acc@1: 81.4453 (81.4453)  Acc@5: 94.7266 (94.7266)
Test: [  48/48]  Time: 0.136 (0.660)  Loss:  1.1875 (1.7603)  Acc@1: 80.1887 (65.5640)  Acc@5: 92.6887 (87.1440)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-41.pth.tar', 66.15200010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-45.pth.tar', 65.77800005615235)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-42.pth.tar', 65.58600002685547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-47.pth.tar', 65.56399992675782)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-38.pth.tar', 65.50800003173828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-43.pth.tar', 65.31400008300781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-40.pth.tar', 65.16800008789062)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-46.pth.tar', 65.14199997314454)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-36.pth.tar', 65.09400009765625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-37.pth.tar', 64.62000003417968)

Train: 48 [   0/1251 (  0%)]  Loss: 4.308 (4.31)  Time: 2.448s,  418.37/s  (2.448s,  418.37/s)  LR: 9.844e-04  Data: 1.773 (1.773)
Train: 48 [  50/1251 (  4%)]  Loss: 3.992 (4.15)  Time: 0.674s, 1519.12/s  (0.744s, 1377.17/s)  LR: 9.844e-04  Data: 0.009 (0.054)
Train: 48 [ 100/1251 (  8%)]  Loss: 4.053 (4.12)  Time: 0.703s, 1456.94/s  (0.720s, 1422.14/s)  LR: 9.844e-04  Data: 0.009 (0.032)
Train: 48 [ 150/1251 ( 12%)]  Loss: 4.400 (4.19)  Time: 0.707s, 1447.92/s  (0.713s, 1436.14/s)  LR: 9.844e-04  Data: 0.009 (0.024)
Train: 48 [ 200/1251 ( 16%)]  Loss: 4.250 (4.20)  Time: 0.708s, 1445.48/s  (0.711s, 1440.14/s)  LR: 9.844e-04  Data: 0.010 (0.021)
Train: 48 [ 250/1251 ( 20%)]  Loss: 3.946 (4.16)  Time: 0.723s, 1415.37/s  (0.709s, 1445.03/s)  LR: 9.844e-04  Data: 0.009 (0.018)
Train: 48 [ 300/1251 ( 24%)]  Loss: 4.113 (4.15)  Time: 0.666s, 1537.95/s  (0.708s, 1445.47/s)  LR: 9.844e-04  Data: 0.008 (0.017)
Train: 48 [ 350/1251 ( 28%)]  Loss: 4.251 (4.16)  Time: 0.709s, 1444.85/s  (0.707s, 1448.72/s)  LR: 9.844e-04  Data: 0.010 (0.016)
Train: 48 [ 400/1251 ( 32%)]  Loss: 3.959 (4.14)  Time: 0.688s, 1487.37/s  (0.706s, 1449.87/s)  LR: 9.844e-04  Data: 0.014 (0.015)
Train: 48 [ 450/1251 ( 36%)]  Loss: 3.957 (4.12)  Time: 0.690s, 1484.45/s  (0.706s, 1450.97/s)  LR: 9.844e-04  Data: 0.009 (0.015)
Train: 48 [ 500/1251 ( 40%)]  Loss: 3.938 (4.11)  Time: 0.696s, 1472.28/s  (0.705s, 1452.91/s)  LR: 9.844e-04  Data: 0.012 (0.014)
Train: 48 [ 550/1251 ( 44%)]  Loss: 3.826 (4.08)  Time: 0.746s, 1372.23/s  (0.705s, 1452.98/s)  LR: 9.844e-04  Data: 0.009 (0.014)
Train: 48 [ 600/1251 ( 48%)]  Loss: 4.024 (4.08)  Time: 0.704s, 1455.36/s  (0.704s, 1453.98/s)  LR: 9.844e-04  Data: 0.010 (0.014)
Train: 48 [ 650/1251 ( 52%)]  Loss: 4.212 (4.09)  Time: 0.709s, 1444.73/s  (0.704s, 1454.52/s)  LR: 9.844e-04  Data: 0.009 (0.013)
Train: 48 [ 700/1251 ( 56%)]  Loss: 4.214 (4.10)  Time: 0.703s, 1456.70/s  (0.704s, 1454.11/s)  LR: 9.844e-04  Data: 0.009 (0.013)
Train: 48 [ 750/1251 ( 60%)]  Loss: 3.723 (4.07)  Time: 0.703s, 1456.47/s  (0.704s, 1454.30/s)  LR: 9.844e-04  Data: 0.009 (0.013)
Train: 48 [ 800/1251 ( 64%)]  Loss: 3.929 (4.06)  Time: 0.703s, 1455.80/s  (0.704s, 1454.44/s)  LR: 9.844e-04  Data: 0.009 (0.013)
Train: 48 [ 850/1251 ( 68%)]  Loss: 4.224 (4.07)  Time: 0.704s, 1454.78/s  (0.704s, 1455.06/s)  LR: 9.844e-04  Data: 0.010 (0.012)
Train: 48 [ 900/1251 ( 72%)]  Loss: 3.952 (4.07)  Time: 0.671s, 1525.07/s  (0.704s, 1455.30/s)  LR: 9.844e-04  Data: 0.010 (0.012)
Train: 48 [ 950/1251 ( 76%)]  Loss: 4.016 (4.06)  Time: 0.672s, 1522.94/s  (0.703s, 1457.59/s)  LR: 9.844e-04  Data: 0.010 (0.012)
Train: 48 [1000/1251 ( 80%)]  Loss: 4.526 (4.09)  Time: 0.700s, 1463.25/s  (0.702s, 1457.90/s)  LR: 9.844e-04  Data: 0.010 (0.012)
Train: 48 [1050/1251 ( 84%)]  Loss: 3.515 (4.06)  Time: 0.671s, 1525.13/s  (0.701s, 1460.60/s)  LR: 9.844e-04  Data: 0.010 (0.012)
Train: 48 [1100/1251 ( 88%)]  Loss: 4.035 (4.06)  Time: 0.672s, 1524.67/s  (0.700s, 1463.11/s)  LR: 9.844e-04  Data: 0.010 (0.012)
Train: 48 [1150/1251 ( 92%)]  Loss: 3.758 (4.05)  Time: 0.671s, 1525.67/s  (0.699s, 1465.67/s)  LR: 9.844e-04  Data: 0.010 (0.012)
Train: 48 [1200/1251 ( 96%)]  Loss: 4.122 (4.05)  Time: 0.701s, 1460.45/s  (0.698s, 1466.11/s)  LR: 9.844e-04  Data: 0.009 (0.012)
Train: 48 [1250/1251 (100%)]  Loss: 4.335 (4.06)  Time: 0.688s, 1489.42/s  (0.698s, 1466.04/s)  LR: 9.844e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.474 (1.474)  Loss:  1.1816 (1.1816)  Acc@1: 83.0078 (83.0078)  Acc@5: 95.8984 (95.8984)
Test: [  48/48]  Time: 0.136 (0.549)  Loss:  1.3936 (1.8899)  Acc@1: 81.3679 (66.1520)  Acc@5: 94.5755 (87.4160)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-41.pth.tar', 66.15200010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-48.pth.tar', 66.15200005126952)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-45.pth.tar', 65.77800005615235)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-42.pth.tar', 65.58600002685547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-47.pth.tar', 65.56399992675782)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-38.pth.tar', 65.50800003173828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-43.pth.tar', 65.31400008300781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-40.pth.tar', 65.16800008789062)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-46.pth.tar', 65.14199997314454)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-36.pth.tar', 65.09400009765625)

Train: 49 [   0/1251 (  0%)]  Loss: 4.100 (4.10)  Time: 2.088s,  490.33/s  (2.088s,  490.33/s)  LR: 9.838e-04  Data: 1.472 (1.472)
Train: 49 [  50/1251 (  4%)]  Loss: 4.391 (4.25)  Time: 0.672s, 1523.71/s  (0.719s, 1425.09/s)  LR: 9.838e-04  Data: 0.010 (0.044)
Train: 49 [ 100/1251 (  8%)]  Loss: 3.879 (4.12)  Time: 0.700s, 1462.65/s  (0.709s, 1443.31/s)  LR: 9.838e-04  Data: 0.009 (0.027)
Train: 49 [ 150/1251 ( 12%)]  Loss: 4.344 (4.18)  Time: 0.703s, 1456.75/s  (0.708s, 1447.06/s)  LR: 9.838e-04  Data: 0.009 (0.021)
Train: 49 [ 200/1251 ( 16%)]  Loss: 4.008 (4.14)  Time: 0.702s, 1458.75/s  (0.707s, 1448.07/s)  LR: 9.838e-04  Data: 0.009 (0.018)
Train: 49 [ 250/1251 ( 20%)]  Loss: 4.004 (4.12)  Time: 0.703s, 1455.99/s  (0.705s, 1452.71/s)  LR: 9.838e-04  Data: 0.011 (0.016)
Train: 49 [ 300/1251 ( 24%)]  Loss: 3.707 (4.06)  Time: 0.672s, 1524.74/s  (0.702s, 1458.40/s)  LR: 9.838e-04  Data: 0.010 (0.015)
Train: 49 [ 350/1251 ( 28%)]  Loss: 3.884 (4.04)  Time: 0.702s, 1458.87/s  (0.702s, 1457.91/s)  LR: 9.838e-04  Data: 0.009 (0.015)
Train: 49 [ 400/1251 ( 32%)]  Loss: 4.126 (4.05)  Time: 0.671s, 1525.54/s  (0.702s, 1459.64/s)  LR: 9.838e-04  Data: 0.010 (0.014)
Train: 49 [ 450/1251 ( 36%)]  Loss: 3.666 (4.01)  Time: 0.707s, 1448.95/s  (0.701s, 1461.49/s)  LR: 9.838e-04  Data: 0.009 (0.013)
Train: 49 [ 500/1251 ( 40%)]  Loss: 3.964 (4.01)  Time: 0.702s, 1458.67/s  (0.701s, 1460.78/s)  LR: 9.838e-04  Data: 0.010 (0.013)
Train: 49 [ 550/1251 ( 44%)]  Loss: 4.267 (4.03)  Time: 0.703s, 1456.61/s  (0.701s, 1461.48/s)  LR: 9.838e-04  Data: 0.009 (0.013)
Train: 49 [ 600/1251 ( 48%)]  Loss: 3.893 (4.02)  Time: 0.703s, 1456.49/s  (0.701s, 1461.14/s)  LR: 9.838e-04  Data: 0.009 (0.012)
Train: 49 [ 650/1251 ( 52%)]  Loss: 4.114 (4.02)  Time: 0.706s, 1449.66/s  (0.701s, 1460.76/s)  LR: 9.838e-04  Data: 0.009 (0.012)
Train: 49 [ 700/1251 ( 56%)]  Loss: 4.109 (4.03)  Time: 0.707s, 1447.99/s  (0.701s, 1460.51/s)  LR: 9.838e-04  Data: 0.010 (0.012)
Train: 49 [ 750/1251 ( 60%)]  Loss: 4.582 (4.06)  Time: 0.704s, 1455.02/s  (0.701s, 1460.01/s)  LR: 9.838e-04  Data: 0.009 (0.012)
Train: 49 [ 800/1251 ( 64%)]  Loss: 4.077 (4.07)  Time: 0.702s, 1457.94/s  (0.702s, 1459.54/s)  LR: 9.838e-04  Data: 0.010 (0.012)
Train: 49 [ 850/1251 ( 68%)]  Loss: 4.176 (4.07)  Time: 0.703s, 1457.54/s  (0.702s, 1458.74/s)  LR: 9.838e-04  Data: 0.009 (0.012)
Train: 49 [ 900/1251 ( 72%)]  Loss: 4.241 (4.08)  Time: 0.671s, 1526.20/s  (0.702s, 1458.97/s)  LR: 9.838e-04  Data: 0.010 (0.011)
Train: 49 [ 950/1251 ( 76%)]  Loss: 4.260 (4.09)  Time: 0.708s, 1446.19/s  (0.702s, 1459.01/s)  LR: 9.838e-04  Data: 0.010 (0.011)
Train: 49 [1000/1251 ( 80%)]  Loss: 4.146 (4.09)  Time: 0.702s, 1457.93/s  (0.702s, 1459.06/s)  LR: 9.838e-04  Data: 0.010 (0.011)
Train: 49 [1050/1251 ( 84%)]  Loss: 4.023 (4.09)  Time: 0.671s, 1526.94/s  (0.702s, 1459.46/s)  LR: 9.838e-04  Data: 0.010 (0.011)
Train: 49 [1100/1251 ( 88%)]  Loss: 3.896 (4.08)  Time: 0.703s, 1457.25/s  (0.702s, 1459.45/s)  LR: 9.838e-04  Data: 0.009 (0.011)
Train: 49 [1150/1251 ( 92%)]  Loss: 4.159 (4.08)  Time: 0.671s, 1526.22/s  (0.701s, 1460.17/s)  LR: 9.838e-04  Data: 0.010 (0.011)
Train: 49 [1200/1251 ( 96%)]  Loss: 4.090 (4.08)  Time: 0.785s, 1304.54/s  (0.701s, 1460.41/s)  LR: 9.838e-04  Data: 0.009 (0.011)
Train: 49 [1250/1251 (100%)]  Loss: 4.047 (4.08)  Time: 0.694s, 1474.68/s  (0.700s, 1461.98/s)  LR: 9.838e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.447 (1.447)  Loss:  1.1719 (1.1719)  Acc@1: 83.3984 (83.3984)  Acc@5: 94.9219 (94.9219)
Test: [  48/48]  Time: 0.137 (0.557)  Loss:  1.2412 (1.7953)  Acc@1: 79.5991 (65.7680)  Acc@5: 93.0424 (87.0440)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-41.pth.tar', 66.15200010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-48.pth.tar', 66.15200005126952)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-45.pth.tar', 65.77800005615235)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-49.pth.tar', 65.76800005859376)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-42.pth.tar', 65.58600002685547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-47.pth.tar', 65.56399992675782)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-38.pth.tar', 65.50800003173828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-43.pth.tar', 65.31400008300781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-40.pth.tar', 65.16800008789062)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-46.pth.tar', 65.14199997314454)

Train: 50 [   0/1251 (  0%)]  Loss: 3.634 (3.63)  Time: 2.090s,  489.90/s  (2.090s,  489.90/s)  LR: 9.831e-04  Data: 1.474 (1.474)
Train: 50 [  50/1251 (  4%)]  Loss: 4.244 (3.94)  Time: 0.704s, 1454.64/s  (0.725s, 1413.22/s)  LR: 9.831e-04  Data: 0.010 (0.044)
Train: 50 [ 100/1251 (  8%)]  Loss: 4.157 (4.01)  Time: 0.700s, 1462.30/s  (0.713s, 1435.47/s)  LR: 9.831e-04  Data: 0.010 (0.027)
Train: 50 [ 150/1251 ( 12%)]  Loss: 4.337 (4.09)  Time: 0.700s, 1462.51/s  (0.709s, 1443.62/s)  LR: 9.831e-04  Data: 0.010 (0.021)
Train: 50 [ 200/1251 ( 16%)]  Loss: 4.428 (4.16)  Time: 0.701s, 1461.53/s  (0.704s, 1455.11/s)  LR: 9.831e-04  Data: 0.010 (0.019)
Train: 50 [ 250/1251 ( 20%)]  Loss: 4.206 (4.17)  Time: 0.703s, 1456.33/s  (0.702s, 1457.85/s)  LR: 9.831e-04  Data: 0.009 (0.017)
Train: 50 [ 300/1251 ( 24%)]  Loss: 4.307 (4.19)  Time: 0.701s, 1461.66/s  (0.702s, 1459.64/s)  LR: 9.831e-04  Data: 0.010 (0.016)
Train: 50 [ 350/1251 ( 28%)]  Loss: 3.785 (4.14)  Time: 0.704s, 1454.50/s  (0.702s, 1459.33/s)  LR: 9.831e-04  Data: 0.010 (0.015)
Train: 50 [ 400/1251 ( 32%)]  Loss: 3.970 (4.12)  Time: 0.701s, 1461.62/s  (0.702s, 1458.96/s)  LR: 9.831e-04  Data: 0.010 (0.014)
Train: 50 [ 450/1251 ( 36%)]  Loss: 3.906 (4.10)  Time: 0.701s, 1461.21/s  (0.701s, 1459.76/s)  LR: 9.831e-04  Data: 0.010 (0.014)
Train: 50 [ 500/1251 ( 40%)]  Loss: 4.294 (4.12)  Time: 0.709s, 1443.34/s  (0.701s, 1461.63/s)  LR: 9.831e-04  Data: 0.009 (0.013)
Train: 50 [ 550/1251 ( 44%)]  Loss: 3.811 (4.09)  Time: 0.704s, 1454.35/s  (0.700s, 1462.60/s)  LR: 9.831e-04  Data: 0.010 (0.013)
Train: 50 [ 600/1251 ( 48%)]  Loss: 4.101 (4.09)  Time: 0.701s, 1460.32/s  (0.700s, 1462.83/s)  LR: 9.831e-04  Data: 0.011 (0.013)
Train: 50 [ 650/1251 ( 52%)]  Loss: 4.332 (4.11)  Time: 0.673s, 1522.67/s  (0.700s, 1462.06/s)  LR: 9.831e-04  Data: 0.010 (0.013)
Train: 50 [ 700/1251 ( 56%)]  Loss: 3.745 (4.08)  Time: 0.703s, 1457.44/s  (0.700s, 1461.82/s)  LR: 9.831e-04  Data: 0.009 (0.012)
Train: 50 [ 750/1251 ( 60%)]  Loss: 4.239 (4.09)  Time: 0.708s, 1446.55/s  (0.701s, 1461.20/s)  LR: 9.831e-04  Data: 0.009 (0.012)
Train: 50 [ 800/1251 ( 64%)]  Loss: 4.169 (4.10)  Time: 0.702s, 1458.76/s  (0.701s, 1460.83/s)  LR: 9.831e-04  Data: 0.010 (0.012)
Train: 50 [ 850/1251 ( 68%)]  Loss: 4.008 (4.09)  Time: 0.703s, 1456.42/s  (0.701s, 1460.52/s)  LR: 9.831e-04  Data: 0.009 (0.012)
Train: 50 [ 900/1251 ( 72%)]  Loss: 4.002 (4.09)  Time: 0.704s, 1455.28/s  (0.701s, 1460.04/s)  LR: 9.831e-04  Data: 0.009 (0.012)
Train: 50 [ 950/1251 ( 76%)]  Loss: 4.113 (4.09)  Time: 0.703s, 1456.81/s  (0.702s, 1459.70/s)  LR: 9.831e-04  Data: 0.009 (0.012)
Train: 50 [1000/1251 ( 80%)]  Loss: 3.862 (4.08)  Time: 0.673s, 1522.15/s  (0.701s, 1461.00/s)  LR: 9.831e-04  Data: 0.011 (0.011)
Train: 50 [1050/1251 ( 84%)]  Loss: 4.441 (4.10)  Time: 0.703s, 1455.59/s  (0.701s, 1460.63/s)  LR: 9.831e-04  Data: 0.009 (0.011)
Train: 50 [1100/1251 ( 88%)]  Loss: 4.027 (4.09)  Time: 0.705s, 1453.08/s  (0.701s, 1460.58/s)  LR: 9.831e-04  Data: 0.009 (0.011)
Train: 50 [1150/1251 ( 92%)]  Loss: 4.138 (4.09)  Time: 0.706s, 1450.16/s  (0.701s, 1461.69/s)  LR: 9.831e-04  Data: 0.009 (0.011)
Train: 50 [1200/1251 ( 96%)]  Loss: 3.931 (4.09)  Time: 0.703s, 1455.78/s  (0.701s, 1461.37/s)  LR: 9.831e-04  Data: 0.009 (0.011)
Train: 50 [1250/1251 (100%)]  Loss: 3.879 (4.08)  Time: 0.677s, 1513.11/s  (0.700s, 1463.70/s)  LR: 9.831e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.443 (1.443)  Loss:  1.0020 (1.0020)  Acc@1: 84.3750 (84.3750)  Acc@5: 95.3125 (95.3125)
Test: [  48/48]  Time: 0.136 (0.547)  Loss:  1.0879 (1.6897)  Acc@1: 81.0142 (66.2900)  Acc@5: 93.3962 (87.2620)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-50.pth.tar', 66.29000002685547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-41.pth.tar', 66.15200010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-48.pth.tar', 66.15200005126952)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-45.pth.tar', 65.77800005615235)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-49.pth.tar', 65.76800005859376)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-42.pth.tar', 65.58600002685547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-47.pth.tar', 65.56399992675782)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-38.pth.tar', 65.50800003173828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-43.pth.tar', 65.31400008300781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-40.pth.tar', 65.16800008789062)

Train: 51 [   0/1251 (  0%)]  Loss: 4.756 (4.76)  Time: 2.245s,  456.20/s  (2.245s,  456.20/s)  LR: 9.825e-04  Data: 1.630 (1.630)
Train: 51 [  50/1251 (  4%)]  Loss: 4.119 (4.44)  Time: 0.703s, 1457.37/s  (0.729s, 1405.16/s)  LR: 9.825e-04  Data: 0.010 (0.052)
Train: 51 [ 100/1251 (  8%)]  Loss: 4.310 (4.39)  Time: 0.671s, 1525.88/s  (0.715s, 1432.13/s)  LR: 9.825e-04  Data: 0.010 (0.031)
Train: 51 [ 150/1251 ( 12%)]  Loss: 4.223 (4.35)  Time: 0.702s, 1458.34/s  (0.704s, 1454.64/s)  LR: 9.825e-04  Data: 0.010 (0.024)
Train: 51 [ 200/1251 ( 16%)]  Loss: 3.929 (4.27)  Time: 0.672s, 1524.66/s  (0.699s, 1465.31/s)  LR: 9.825e-04  Data: 0.010 (0.021)
Train: 51 [ 250/1251 ( 20%)]  Loss: 3.784 (4.19)  Time: 0.703s, 1457.44/s  (0.695s, 1472.46/s)  LR: 9.825e-04  Data: 0.010 (0.018)
Train: 51 [ 300/1251 ( 24%)]  Loss: 4.042 (4.17)  Time: 0.707s, 1448.08/s  (0.697s, 1470.04/s)  LR: 9.825e-04  Data: 0.010 (0.017)
Train: 51 [ 350/1251 ( 28%)]  Loss: 4.616 (4.22)  Time: 0.702s, 1458.28/s  (0.697s, 1468.31/s)  LR: 9.825e-04  Data: 0.010 (0.016)
Train: 51 [ 400/1251 ( 32%)]  Loss: 3.939 (4.19)  Time: 0.702s, 1459.32/s  (0.698s, 1467.04/s)  LR: 9.825e-04  Data: 0.011 (0.015)
Train: 51 [ 450/1251 ( 36%)]  Loss: 4.217 (4.19)  Time: 0.672s, 1523.74/s  (0.698s, 1466.83/s)  LR: 9.825e-04  Data: 0.009 (0.015)
Train: 51 [ 500/1251 ( 40%)]  Loss: 4.362 (4.21)  Time: 0.673s, 1521.65/s  (0.697s, 1470.18/s)  LR: 9.825e-04  Data: 0.010 (0.014)
Train: 51 [ 550/1251 ( 44%)]  Loss: 3.728 (4.17)  Time: 0.672s, 1523.86/s  (0.695s, 1474.27/s)  LR: 9.825e-04  Data: 0.010 (0.014)
Train: 51 [ 600/1251 ( 48%)]  Loss: 3.479 (4.12)  Time: 0.671s, 1526.49/s  (0.693s, 1478.19/s)  LR: 9.825e-04  Data: 0.009 (0.014)
Train: 51 [ 650/1251 ( 52%)]  Loss: 4.136 (4.12)  Time: 0.710s, 1442.16/s  (0.692s, 1479.42/s)  LR: 9.825e-04  Data: 0.009 (0.013)
Train: 51 [ 700/1251 ( 56%)]  Loss: 4.027 (4.11)  Time: 0.708s, 1445.82/s  (0.693s, 1477.66/s)  LR: 9.825e-04  Data: 0.009 (0.013)
Train: 51 [ 750/1251 ( 60%)]  Loss: 3.985 (4.10)  Time: 0.702s, 1457.84/s  (0.694s, 1476.13/s)  LR: 9.825e-04  Data: 0.009 (0.013)
Train: 51 [ 800/1251 ( 64%)]  Loss: 4.224 (4.11)  Time: 0.673s, 1522.22/s  (0.694s, 1475.45/s)  LR: 9.825e-04  Data: 0.010 (0.013)
Train: 51 [ 850/1251 ( 68%)]  Loss: 3.985 (4.10)  Time: 0.703s, 1456.67/s  (0.693s, 1476.90/s)  LR: 9.825e-04  Data: 0.009 (0.012)
Train: 51 [ 900/1251 ( 72%)]  Loss: 4.178 (4.11)  Time: 0.703s, 1456.06/s  (0.694s, 1475.55/s)  LR: 9.825e-04  Data: 0.009 (0.012)
Train: 51 [ 950/1251 ( 76%)]  Loss: 4.324 (4.12)  Time: 0.703s, 1455.97/s  (0.695s, 1474.16/s)  LR: 9.825e-04  Data: 0.009 (0.012)
Train: 51 [1000/1251 ( 80%)]  Loss: 4.397 (4.13)  Time: 0.703s, 1456.75/s  (0.695s, 1473.27/s)  LR: 9.825e-04  Data: 0.009 (0.012)
Train: 51 [1050/1251 ( 84%)]  Loss: 4.002 (4.13)  Time: 0.702s, 1458.38/s  (0.695s, 1472.45/s)  LR: 9.825e-04  Data: 0.009 (0.012)
Train: 51 [1100/1251 ( 88%)]  Loss: 4.132 (4.13)  Time: 0.703s, 1456.82/s  (0.696s, 1472.21/s)  LR: 9.825e-04  Data: 0.010 (0.012)
Train: 51 [1150/1251 ( 92%)]  Loss: 3.892 (4.12)  Time: 0.703s, 1456.70/s  (0.696s, 1471.52/s)  LR: 9.825e-04  Data: 0.010 (0.012)
Train: 51 [1200/1251 ( 96%)]  Loss: 4.133 (4.12)  Time: 0.702s, 1457.83/s  (0.696s, 1470.37/s)  LR: 9.825e-04  Data: 0.010 (0.011)
Train: 51 [1250/1251 (100%)]  Loss: 4.231 (4.12)  Time: 0.694s, 1475.22/s  (0.697s, 1469.69/s)  LR: 9.825e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.522 (1.522)  Loss:  1.0664 (1.0664)  Acc@1: 85.1562 (85.1562)  Acc@5: 95.9961 (95.9961)
Test: [  48/48]  Time: 0.137 (0.559)  Loss:  1.2646 (1.7892)  Acc@1: 79.4811 (65.6620)  Acc@5: 93.1604 (87.2500)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-50.pth.tar', 66.29000002685547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-41.pth.tar', 66.15200010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-48.pth.tar', 66.15200005126952)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-45.pth.tar', 65.77800005615235)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-49.pth.tar', 65.76800005859376)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-51.pth.tar', 65.66200000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-42.pth.tar', 65.58600002685547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-47.pth.tar', 65.56399992675782)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-38.pth.tar', 65.50800003173828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-43.pth.tar', 65.31400008300781)

Train: 52 [   0/1251 (  0%)]  Loss: 4.238 (4.24)  Time: 2.101s,  487.33/s  (2.101s,  487.33/s)  LR: 9.818e-04  Data: 1.486 (1.486)
Train: 52 [  50/1251 (  4%)]  Loss: 3.964 (4.10)  Time: 0.705s, 1452.94/s  (0.721s, 1420.97/s)  LR: 9.818e-04  Data: 0.009 (0.043)
Train: 52 [ 100/1251 (  8%)]  Loss: 3.849 (4.02)  Time: 0.673s, 1522.41/s  (0.705s, 1452.10/s)  LR: 9.818e-04  Data: 0.010 (0.026)
Train: 52 [ 150/1251 ( 12%)]  Loss: 4.037 (4.02)  Time: 0.701s, 1461.63/s  (0.704s, 1455.32/s)  LR: 9.818e-04  Data: 0.009 (0.021)
Train: 52 [ 200/1251 ( 16%)]  Loss: 3.983 (4.01)  Time: 0.701s, 1460.48/s  (0.704s, 1455.05/s)  LR: 9.818e-04  Data: 0.010 (0.018)
Train: 52 [ 250/1251 ( 20%)]  Loss: 4.246 (4.05)  Time: 0.704s, 1453.84/s  (0.703s, 1456.91/s)  LR: 9.818e-04  Data: 0.010 (0.016)
Train: 52 [ 300/1251 ( 24%)]  Loss: 4.041 (4.05)  Time: 0.700s, 1462.34/s  (0.703s, 1455.95/s)  LR: 9.818e-04  Data: 0.009 (0.015)
Train: 52 [ 350/1251 ( 28%)]  Loss: 3.879 (4.03)  Time: 0.672s, 1523.78/s  (0.699s, 1464.15/s)  LR: 9.818e-04  Data: 0.010 (0.014)
Train: 52 [ 400/1251 ( 32%)]  Loss: 3.663 (3.99)  Time: 0.671s, 1525.38/s  (0.699s, 1465.39/s)  LR: 9.818e-04  Data: 0.010 (0.014)
Train: 52 [ 450/1251 ( 36%)]  Loss: 4.218 (4.01)  Time: 0.673s, 1522.65/s  (0.696s, 1470.71/s)  LR: 9.818e-04  Data: 0.010 (0.013)
Train: 52 [ 500/1251 ( 40%)]  Loss: 4.253 (4.03)  Time: 0.703s, 1457.19/s  (0.697s, 1469.42/s)  LR: 9.818e-04  Data: 0.009 (0.013)
Train: 52 [ 550/1251 ( 44%)]  Loss: 4.007 (4.03)  Time: 0.703s, 1455.79/s  (0.698s, 1467.99/s)  LR: 9.818e-04  Data: 0.009 (0.013)
Train: 52 [ 600/1251 ( 48%)]  Loss: 3.935 (4.02)  Time: 0.705s, 1451.64/s  (0.698s, 1466.44/s)  LR: 9.818e-04  Data: 0.009 (0.012)
Train: 52 [ 650/1251 ( 52%)]  Loss: 3.661 (4.00)  Time: 0.703s, 1457.14/s  (0.698s, 1466.32/s)  LR: 9.818e-04  Data: 0.009 (0.012)
Train: 52 [ 700/1251 ( 56%)]  Loss: 3.838 (3.99)  Time: 0.700s, 1462.66/s  (0.699s, 1465.06/s)  LR: 9.818e-04  Data: 0.010 (0.012)
Train: 52 [ 750/1251 ( 60%)]  Loss: 4.094 (3.99)  Time: 0.701s, 1461.15/s  (0.699s, 1464.54/s)  LR: 9.818e-04  Data: 0.010 (0.012)
Train: 52 [ 800/1251 ( 64%)]  Loss: 4.183 (4.01)  Time: 0.703s, 1457.46/s  (0.698s, 1466.41/s)  LR: 9.818e-04  Data: 0.009 (0.012)
Train: 52 [ 850/1251 ( 68%)]  Loss: 3.888 (4.00)  Time: 0.672s, 1523.26/s  (0.697s, 1469.23/s)  LR: 9.818e-04  Data: 0.010 (0.012)
Train: 52 [ 900/1251 ( 72%)]  Loss: 3.932 (4.00)  Time: 0.702s, 1458.36/s  (0.697s, 1469.93/s)  LR: 9.818e-04  Data: 0.009 (0.012)
Train: 52 [ 950/1251 ( 76%)]  Loss: 4.106 (4.00)  Time: 0.672s, 1523.57/s  (0.697s, 1469.02/s)  LR: 9.818e-04  Data: 0.010 (0.011)
Train: 52 [1000/1251 ( 80%)]  Loss: 4.034 (4.00)  Time: 0.701s, 1460.95/s  (0.697s, 1469.52/s)  LR: 9.818e-04  Data: 0.009 (0.011)
Train: 52 [1050/1251 ( 84%)]  Loss: 3.856 (4.00)  Time: 0.702s, 1457.73/s  (0.697s, 1468.78/s)  LR: 9.818e-04  Data: 0.009 (0.011)
Train: 52 [1100/1251 ( 88%)]  Loss: 4.202 (4.00)  Time: 0.706s, 1449.93/s  (0.698s, 1468.01/s)  LR: 9.818e-04  Data: 0.010 (0.011)
Train: 52 [1150/1251 ( 92%)]  Loss: 3.927 (4.00)  Time: 0.703s, 1456.00/s  (0.698s, 1467.73/s)  LR: 9.818e-04  Data: 0.009 (0.011)
Train: 52 [1200/1251 ( 96%)]  Loss: 4.199 (4.01)  Time: 0.707s, 1449.09/s  (0.698s, 1467.24/s)  LR: 9.818e-04  Data: 0.009 (0.011)
Train: 52 [1250/1251 (100%)]  Loss: 4.112 (4.01)  Time: 0.691s, 1481.14/s  (0.698s, 1466.85/s)  LR: 9.818e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.535 (1.535)  Loss:  1.0410 (1.0410)  Acc@1: 84.9609 (84.9609)  Acc@5: 95.3125 (95.3125)
Test: [  48/48]  Time: 0.140 (0.551)  Loss:  0.8989 (1.7030)  Acc@1: 83.1368 (66.3460)  Acc@5: 95.7547 (87.6740)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-52.pth.tar', 66.34600004394531)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-50.pth.tar', 66.29000002685547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-41.pth.tar', 66.15200010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-48.pth.tar', 66.15200005126952)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-45.pth.tar', 65.77800005615235)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-49.pth.tar', 65.76800005859376)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-51.pth.tar', 65.66200000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-42.pth.tar', 65.58600002685547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-47.pth.tar', 65.56399992675782)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-38.pth.tar', 65.50800003173828)

Train: 53 [   0/1251 (  0%)]  Loss: 3.701 (3.70)  Time: 2.307s,  443.78/s  (2.307s,  443.78/s)  LR: 9.811e-04  Data: 1.693 (1.693)
Train: 53 [  50/1251 (  4%)]  Loss: 4.120 (3.91)  Time: 0.671s, 1526.24/s  (0.734s, 1394.23/s)  LR: 9.811e-04  Data: 0.010 (0.051)
Train: 53 [ 100/1251 (  8%)]  Loss: 4.266 (4.03)  Time: 0.702s, 1458.83/s  (0.714s, 1434.73/s)  LR: 9.811e-04  Data: 0.009 (0.030)
Train: 53 [ 150/1251 ( 12%)]  Loss: 4.332 (4.10)  Time: 0.702s, 1457.91/s  (0.711s, 1439.45/s)  LR: 9.811e-04  Data: 0.010 (0.023)
Train: 53 [ 200/1251 ( 16%)]  Loss: 3.796 (4.04)  Time: 0.703s, 1457.56/s  (0.710s, 1442.29/s)  LR: 9.811e-04  Data: 0.009 (0.020)
Train: 53 [ 250/1251 ( 20%)]  Loss: 3.569 (3.96)  Time: 0.702s, 1458.34/s  (0.709s, 1444.40/s)  LR: 9.811e-04  Data: 0.009 (0.018)
Train: 53 [ 300/1251 ( 24%)]  Loss: 4.318 (4.01)  Time: 0.702s, 1459.33/s  (0.708s, 1446.64/s)  LR: 9.811e-04  Data: 0.009 (0.016)
Train: 53 [ 350/1251 ( 28%)]  Loss: 3.623 (3.97)  Time: 0.706s, 1449.48/s  (0.707s, 1448.31/s)  LR: 9.811e-04  Data: 0.009 (0.015)
Train: 53 [ 400/1251 ( 32%)]  Loss: 3.588 (3.92)  Time: 0.702s, 1458.78/s  (0.707s, 1449.04/s)  LR: 9.811e-04  Data: 0.010 (0.015)
Train: 53 [ 450/1251 ( 36%)]  Loss: 3.916 (3.92)  Time: 0.703s, 1456.19/s  (0.706s, 1449.90/s)  LR: 9.811e-04  Data: 0.009 (0.014)
Train: 53 [ 500/1251 ( 40%)]  Loss: 4.118 (3.94)  Time: 0.670s, 1528.01/s  (0.706s, 1450.04/s)  LR: 9.811e-04  Data: 0.010 (0.014)
Train: 53 [ 550/1251 ( 44%)]  Loss: 4.152 (3.96)  Time: 0.700s, 1462.68/s  (0.705s, 1452.10/s)  LR: 9.811e-04  Data: 0.009 (0.013)
Train: 53 [ 600/1251 ( 48%)]  Loss: 3.904 (3.95)  Time: 0.671s, 1526.89/s  (0.703s, 1456.51/s)  LR: 9.811e-04  Data: 0.010 (0.013)
Train: 53 [ 650/1251 ( 52%)]  Loss: 4.041 (3.96)  Time: 0.702s, 1458.29/s  (0.703s, 1456.48/s)  LR: 9.811e-04  Data: 0.010 (0.013)
Train: 53 [ 700/1251 ( 56%)]  Loss: 4.332 (3.99)  Time: 0.708s, 1446.87/s  (0.703s, 1456.49/s)  LR: 9.811e-04  Data: 0.009 (0.012)
Train: 53 [ 750/1251 ( 60%)]  Loss: 4.373 (4.01)  Time: 0.703s, 1455.78/s  (0.703s, 1456.20/s)  LR: 9.811e-04  Data: 0.010 (0.012)
Train: 53 [ 800/1251 ( 64%)]  Loss: 4.070 (4.01)  Time: 0.671s, 1525.46/s  (0.702s, 1457.99/s)  LR: 9.811e-04  Data: 0.010 (0.012)
Train: 53 [ 850/1251 ( 68%)]  Loss: 3.655 (3.99)  Time: 0.699s, 1465.86/s  (0.702s, 1458.44/s)  LR: 9.811e-04  Data: 0.009 (0.012)
Train: 53 [ 900/1251 ( 72%)]  Loss: 4.126 (4.00)  Time: 0.670s, 1527.65/s  (0.701s, 1460.07/s)  LR: 9.811e-04  Data: 0.010 (0.012)
Train: 53 [ 950/1251 ( 76%)]  Loss: 4.231 (4.01)  Time: 0.671s, 1525.23/s  (0.701s, 1460.27/s)  LR: 9.811e-04  Data: 0.010 (0.012)
Train: 53 [1000/1251 ( 80%)]  Loss: 3.888 (4.01)  Time: 0.674s, 1518.24/s  (0.700s, 1463.34/s)  LR: 9.811e-04  Data: 0.010 (0.012)
Train: 53 [1050/1251 ( 84%)]  Loss: 3.782 (4.00)  Time: 0.703s, 1456.78/s  (0.699s, 1464.00/s)  LR: 9.811e-04  Data: 0.010 (0.012)
Train: 53 [1100/1251 ( 88%)]  Loss: 4.028 (4.00)  Time: 0.672s, 1524.16/s  (0.699s, 1464.63/s)  LR: 9.811e-04  Data: 0.011 (0.011)
Train: 53 [1150/1251 ( 92%)]  Loss: 3.710 (3.98)  Time: 0.675s, 1516.80/s  (0.698s, 1466.93/s)  LR: 9.811e-04  Data: 0.010 (0.011)
Train: 53 [1200/1251 ( 96%)]  Loss: 3.930 (3.98)  Time: 0.791s, 1294.21/s  (0.698s, 1467.00/s)  LR: 9.811e-04  Data: 0.009 (0.011)
Train: 53 [1250/1251 (100%)]  Loss: 4.138 (3.99)  Time: 0.688s, 1489.38/s  (0.698s, 1466.36/s)  LR: 9.811e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.528 (1.528)  Loss:  1.2441 (1.2441)  Acc@1: 82.6172 (82.6172)  Acc@5: 94.9219 (94.9219)
Test: [  48/48]  Time: 0.137 (0.563)  Loss:  1.2041 (1.7720)  Acc@1: 81.4858 (67.1460)  Acc@5: 94.1038 (87.9420)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-53.pth.tar', 67.14599997314453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-52.pth.tar', 66.34600004394531)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-50.pth.tar', 66.29000002685547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-41.pth.tar', 66.15200010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-48.pth.tar', 66.15200005126952)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-45.pth.tar', 65.77800005615235)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-49.pth.tar', 65.76800005859376)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-51.pth.tar', 65.66200000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-42.pth.tar', 65.58600002685547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-47.pth.tar', 65.56399992675782)

Train: 54 [   0/1251 (  0%)]  Loss: 3.928 (3.93)  Time: 2.165s,  473.05/s  (2.165s,  473.05/s)  LR: 9.803e-04  Data: 1.548 (1.548)
Train: 54 [  50/1251 (  4%)]  Loss: 4.411 (4.17)  Time: 0.672s, 1522.89/s  (0.710s, 1442.45/s)  LR: 9.803e-04  Data: 0.010 (0.046)
Train: 54 [ 100/1251 (  8%)]  Loss: 4.016 (4.12)  Time: 0.673s, 1521.16/s  (0.691s, 1481.91/s)  LR: 9.803e-04  Data: 0.010 (0.028)
Train: 54 [ 150/1251 ( 12%)]  Loss: 4.043 (4.10)  Time: 0.703s, 1457.03/s  (0.693s, 1477.66/s)  LR: 9.803e-04  Data: 0.010 (0.022)
Train: 54 [ 200/1251 ( 16%)]  Loss: 4.238 (4.13)  Time: 0.671s, 1525.07/s  (0.695s, 1473.70/s)  LR: 9.803e-04  Data: 0.010 (0.019)
Train: 54 [ 250/1251 ( 20%)]  Loss: 3.819 (4.08)  Time: 0.672s, 1522.76/s  (0.691s, 1482.87/s)  LR: 9.803e-04  Data: 0.010 (0.017)
Train: 54 [ 300/1251 ( 24%)]  Loss: 4.194 (4.09)  Time: 0.672s, 1524.58/s  (0.688s, 1488.99/s)  LR: 9.803e-04  Data: 0.010 (0.016)
Train: 54 [ 350/1251 ( 28%)]  Loss: 4.035 (4.09)  Time: 0.672s, 1523.82/s  (0.686s, 1492.24/s)  LR: 9.803e-04  Data: 0.010 (0.015)
Train: 54 [ 400/1251 ( 32%)]  Loss: 4.087 (4.09)  Time: 0.702s, 1459.15/s  (0.688s, 1487.61/s)  LR: 9.803e-04  Data: 0.009 (0.015)
Train: 54 [ 450/1251 ( 36%)]  Loss: 3.816 (4.06)  Time: 0.703s, 1456.90/s  (0.690s, 1484.20/s)  LR: 9.803e-04  Data: 0.010 (0.014)
Train: 54 [ 500/1251 ( 40%)]  Loss: 4.040 (4.06)  Time: 0.700s, 1463.83/s  (0.690s, 1484.35/s)  LR: 9.803e-04  Data: 0.010 (0.014)
Train: 54 [ 550/1251 ( 44%)]  Loss: 3.807 (4.04)  Time: 0.673s, 1522.50/s  (0.689s, 1485.19/s)  LR: 9.803e-04  Data: 0.010 (0.013)
Train: 54 [ 600/1251 ( 48%)]  Loss: 4.261 (4.05)  Time: 0.703s, 1456.64/s  (0.690s, 1484.37/s)  LR: 9.803e-04  Data: 0.009 (0.013)
Train: 54 [ 650/1251 ( 52%)]  Loss: 4.006 (4.05)  Time: 0.702s, 1458.51/s  (0.691s, 1481.66/s)  LR: 9.803e-04  Data: 0.009 (0.013)
Train: 54 [ 700/1251 ( 56%)]  Loss: 4.129 (4.06)  Time: 0.705s, 1452.04/s  (0.692s, 1479.47/s)  LR: 9.803e-04  Data: 0.010 (0.012)
Train: 54 [ 750/1251 ( 60%)]  Loss: 3.990 (4.05)  Time: 0.704s, 1455.36/s  (0.693s, 1477.36/s)  LR: 9.803e-04  Data: 0.009 (0.012)
Train: 54 [ 800/1251 ( 64%)]  Loss: 4.092 (4.05)  Time: 0.705s, 1453.36/s  (0.694s, 1475.56/s)  LR: 9.803e-04  Data: 0.010 (0.012)
Train: 54 [ 850/1251 ( 68%)]  Loss: 3.915 (4.05)  Time: 0.705s, 1452.17/s  (0.695s, 1474.23/s)  LR: 9.803e-04  Data: 0.011 (0.012)
Train: 54 [ 900/1251 ( 72%)]  Loss: 4.234 (4.06)  Time: 0.704s, 1455.24/s  (0.695s, 1473.14/s)  LR: 9.803e-04  Data: 0.009 (0.012)
Train: 54 [ 950/1251 ( 76%)]  Loss: 4.456 (4.08)  Time: 0.703s, 1457.21/s  (0.696s, 1471.89/s)  LR: 9.803e-04  Data: 0.009 (0.012)
Train: 54 [1000/1251 ( 80%)]  Loss: 4.356 (4.09)  Time: 0.703s, 1457.54/s  (0.696s, 1471.07/s)  LR: 9.803e-04  Data: 0.009 (0.012)
Train: 54 [1050/1251 ( 84%)]  Loss: 4.119 (4.09)  Time: 0.706s, 1449.86/s  (0.697s, 1470.03/s)  LR: 9.803e-04  Data: 0.009 (0.012)
Train: 54 [1100/1251 ( 88%)]  Loss: 4.089 (4.09)  Time: 0.707s, 1449.06/s  (0.697s, 1469.29/s)  LR: 9.803e-04  Data: 0.009 (0.011)
Train: 54 [1150/1251 ( 92%)]  Loss: 4.020 (4.09)  Time: 0.702s, 1457.99/s  (0.697s, 1469.14/s)  LR: 9.803e-04  Data: 0.009 (0.011)
Train: 54 [1200/1251 ( 96%)]  Loss: 4.085 (4.09)  Time: 0.703s, 1457.51/s  (0.697s, 1468.65/s)  LR: 9.803e-04  Data: 0.010 (0.011)
Train: 54 [1250/1251 (100%)]  Loss: 3.830 (4.08)  Time: 0.651s, 1572.52/s  (0.697s, 1468.35/s)  LR: 9.803e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.464 (1.464)  Loss:  1.1523 (1.1523)  Acc@1: 83.4961 (83.4961)  Acc@5: 94.7266 (94.7266)
Test: [  48/48]  Time: 0.136 (0.552)  Loss:  1.1035 (1.7448)  Acc@1: 81.7217 (66.0060)  Acc@5: 93.6321 (87.3740)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-53.pth.tar', 67.14599997314453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-52.pth.tar', 66.34600004394531)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-50.pth.tar', 66.29000002685547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-41.pth.tar', 66.15200010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-48.pth.tar', 66.15200005126952)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-54.pth.tar', 66.00599994628907)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-45.pth.tar', 65.77800005615235)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-49.pth.tar', 65.76800005859376)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-51.pth.tar', 65.66200000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-42.pth.tar', 65.58600002685547)

Train: 55 [   0/1251 (  0%)]  Loss: 3.771 (3.77)  Time: 2.177s,  470.36/s  (2.177s,  470.36/s)  LR: 9.796e-04  Data: 1.561 (1.561)
Train: 55 [  50/1251 (  4%)]  Loss: 3.943 (3.86)  Time: 0.670s, 1528.40/s  (0.726s, 1410.49/s)  LR: 9.796e-04  Data: 0.010 (0.044)
Train: 55 [ 100/1251 (  8%)]  Loss: 4.125 (3.95)  Time: 0.672s, 1524.58/s  (0.709s, 1444.25/s)  LR: 9.796e-04  Data: 0.010 (0.027)
Train: 55 [ 150/1251 ( 12%)]  Loss: 3.958 (3.95)  Time: 0.784s, 1305.38/s  (0.700s, 1462.81/s)  LR: 9.796e-04  Data: 0.009 (0.022)
Train: 55 [ 200/1251 ( 16%)]  Loss: 3.637 (3.89)  Time: 0.672s, 1522.97/s  (0.697s, 1469.54/s)  LR: 9.796e-04  Data: 0.010 (0.019)
Train: 55 [ 250/1251 ( 20%)]  Loss: 4.337 (3.96)  Time: 0.671s, 1525.04/s  (0.692s, 1479.47/s)  LR: 9.796e-04  Data: 0.010 (0.017)
Train: 55 [ 300/1251 ( 24%)]  Loss: 4.409 (4.03)  Time: 0.702s, 1458.24/s  (0.693s, 1477.88/s)  LR: 9.796e-04  Data: 0.010 (0.016)
Train: 55 [ 350/1251 ( 28%)]  Loss: 3.605 (3.97)  Time: 0.706s, 1450.54/s  (0.694s, 1475.00/s)  LR: 9.796e-04  Data: 0.009 (0.015)
Train: 55 [ 400/1251 ( 32%)]  Loss: 3.801 (3.95)  Time: 0.672s, 1523.95/s  (0.693s, 1476.63/s)  LR: 9.796e-04  Data: 0.010 (0.014)
Train: 55 [ 450/1251 ( 36%)]  Loss: 3.728 (3.93)  Time: 0.672s, 1524.42/s  (0.691s, 1481.11/s)  LR: 9.796e-04  Data: 0.010 (0.014)
Train: 55 [ 500/1251 ( 40%)]  Loss: 4.213 (3.96)  Time: 0.702s, 1458.16/s  (0.692s, 1479.49/s)  LR: 9.796e-04  Data: 0.011 (0.013)
Train: 55 [ 550/1251 ( 44%)]  Loss: 4.076 (3.97)  Time: 0.704s, 1453.98/s  (0.693s, 1476.57/s)  LR: 9.796e-04  Data: 0.010 (0.013)
Train: 55 [ 600/1251 ( 48%)]  Loss: 4.065 (3.97)  Time: 0.703s, 1457.29/s  (0.694s, 1474.95/s)  LR: 9.796e-04  Data: 0.010 (0.013)
Train: 55 [ 650/1251 ( 52%)]  Loss: 4.116 (3.98)  Time: 0.702s, 1459.35/s  (0.695s, 1473.69/s)  LR: 9.796e-04  Data: 0.010 (0.013)
Train: 55 [ 700/1251 ( 56%)]  Loss: 4.259 (4.00)  Time: 0.708s, 1445.59/s  (0.694s, 1475.34/s)  LR: 9.796e-04  Data: 0.009 (0.012)
Train: 55 [ 750/1251 ( 60%)]  Loss: 4.397 (4.03)  Time: 0.670s, 1527.44/s  (0.694s, 1475.69/s)  LR: 9.796e-04  Data: 0.010 (0.012)
Train: 55 [ 800/1251 ( 64%)]  Loss: 4.423 (4.05)  Time: 0.706s, 1449.95/s  (0.694s, 1476.20/s)  LR: 9.796e-04  Data: 0.011 (0.012)
Train: 55 [ 850/1251 ( 68%)]  Loss: 3.772 (4.04)  Time: 0.710s, 1442.97/s  (0.694s, 1475.17/s)  LR: 9.796e-04  Data: 0.009 (0.012)
Train: 55 [ 900/1251 ( 72%)]  Loss: 3.975 (4.03)  Time: 0.702s, 1457.68/s  (0.695s, 1473.59/s)  LR: 9.796e-04  Data: 0.009 (0.012)
Train: 55 [ 950/1251 ( 76%)]  Loss: 4.200 (4.04)  Time: 0.671s, 1524.98/s  (0.695s, 1472.70/s)  LR: 9.796e-04  Data: 0.010 (0.012)
Train: 55 [1000/1251 ( 80%)]  Loss: 3.990 (4.04)  Time: 0.704s, 1454.98/s  (0.695s, 1473.49/s)  LR: 9.796e-04  Data: 0.010 (0.012)
Train: 55 [1050/1251 ( 84%)]  Loss: 4.008 (4.04)  Time: 0.702s, 1458.82/s  (0.695s, 1472.83/s)  LR: 9.796e-04  Data: 0.010 (0.012)
Train: 55 [1100/1251 ( 88%)]  Loss: 4.324 (4.05)  Time: 0.670s, 1527.44/s  (0.695s, 1473.96/s)  LR: 9.796e-04  Data: 0.010 (0.011)
Train: 55 [1150/1251 ( 92%)]  Loss: 4.371 (4.06)  Time: 0.699s, 1464.08/s  (0.695s, 1473.71/s)  LR: 9.796e-04  Data: 0.009 (0.011)
Train: 55 [1200/1251 ( 96%)]  Loss: 4.174 (4.07)  Time: 0.703s, 1457.27/s  (0.695s, 1473.18/s)  LR: 9.796e-04  Data: 0.010 (0.011)
Train: 55 [1250/1251 (100%)]  Loss: 3.847 (4.06)  Time: 0.686s, 1492.07/s  (0.695s, 1472.39/s)  LR: 9.796e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.434 (1.434)  Loss:  1.0283 (1.0283)  Acc@1: 84.7656 (84.7656)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.138 (0.556)  Loss:  1.4287 (1.7933)  Acc@1: 78.3019 (66.8140)  Acc@5: 91.8632 (87.7900)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-53.pth.tar', 67.14599997314453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-55.pth.tar', 66.81400001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-52.pth.tar', 66.34600004394531)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-50.pth.tar', 66.29000002685547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-41.pth.tar', 66.15200010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-48.pth.tar', 66.15200005126952)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-54.pth.tar', 66.00599994628907)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-45.pth.tar', 65.77800005615235)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-49.pth.tar', 65.76800005859376)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-51.pth.tar', 65.66200000732422)

Train: 56 [   0/1251 (  0%)]  Loss: 4.003 (4.00)  Time: 2.040s,  502.07/s  (2.040s,  502.07/s)  LR: 9.789e-04  Data: 1.424 (1.424)
Train: 56 [  50/1251 (  4%)]  Loss: 3.958 (3.98)  Time: 0.672s, 1523.13/s  (0.709s, 1443.41/s)  LR: 9.789e-04  Data: 0.010 (0.044)
Train: 56 [ 100/1251 (  8%)]  Loss: 4.189 (4.05)  Time: 0.674s, 1520.12/s  (0.692s, 1479.31/s)  LR: 9.789e-04  Data: 0.010 (0.027)
Train: 56 [ 150/1251 ( 12%)]  Loss: 3.943 (4.02)  Time: 0.673s, 1521.29/s  (0.686s, 1493.56/s)  LR: 9.789e-04  Data: 0.010 (0.022)
Train: 56 [ 200/1251 ( 16%)]  Loss: 4.017 (4.02)  Time: 0.702s, 1458.69/s  (0.689s, 1486.61/s)  LR: 9.789e-04  Data: 0.010 (0.019)
Train: 56 [ 250/1251 ( 20%)]  Loss: 3.793 (3.98)  Time: 0.701s, 1460.55/s  (0.692s, 1479.97/s)  LR: 9.789e-04  Data: 0.009 (0.017)
Train: 56 [ 300/1251 ( 24%)]  Loss: 3.904 (3.97)  Time: 0.706s, 1451.36/s  (0.693s, 1476.85/s)  LR: 9.789e-04  Data: 0.009 (0.015)
Train: 56 [ 350/1251 ( 28%)]  Loss: 3.978 (3.97)  Time: 0.696s, 1471.84/s  (0.695s, 1472.90/s)  LR: 9.789e-04  Data: 0.009 (0.015)
Train: 56 [ 400/1251 ( 32%)]  Loss: 3.598 (3.93)  Time: 0.704s, 1454.93/s  (0.696s, 1470.96/s)  LR: 9.789e-04  Data: 0.009 (0.014)
Train: 56 [ 450/1251 ( 36%)]  Loss: 3.627 (3.90)  Time: 0.701s, 1461.35/s  (0.697s, 1469.52/s)  LR: 9.789e-04  Data: 0.009 (0.013)
Train: 56 [ 500/1251 ( 40%)]  Loss: 3.858 (3.90)  Time: 0.701s, 1461.00/s  (0.697s, 1468.62/s)  LR: 9.789e-04  Data: 0.009 (0.013)
Train: 56 [ 550/1251 ( 44%)]  Loss: 3.603 (3.87)  Time: 0.700s, 1462.12/s  (0.698s, 1467.65/s)  LR: 9.789e-04  Data: 0.010 (0.013)
Train: 56 [ 600/1251 ( 48%)]  Loss: 4.119 (3.89)  Time: 0.704s, 1454.76/s  (0.698s, 1466.86/s)  LR: 9.789e-04  Data: 0.009 (0.012)
Train: 56 [ 650/1251 ( 52%)]  Loss: 4.034 (3.90)  Time: 0.702s, 1459.67/s  (0.698s, 1466.36/s)  LR: 9.789e-04  Data: 0.009 (0.012)
Train: 56 [ 700/1251 ( 56%)]  Loss: 4.151 (3.92)  Time: 0.701s, 1460.78/s  (0.699s, 1465.13/s)  LR: 9.789e-04  Data: 0.010 (0.012)
Train: 56 [ 750/1251 ( 60%)]  Loss: 3.719 (3.91)  Time: 0.701s, 1460.45/s  (0.699s, 1464.66/s)  LR: 9.789e-04  Data: 0.009 (0.012)
Train: 56 [ 800/1251 ( 64%)]  Loss: 3.982 (3.91)  Time: 0.706s, 1449.63/s  (0.699s, 1464.30/s)  LR: 9.789e-04  Data: 0.009 (0.012)
Train: 56 [ 850/1251 ( 68%)]  Loss: 3.885 (3.91)  Time: 0.701s, 1460.52/s  (0.699s, 1464.13/s)  LR: 9.789e-04  Data: 0.009 (0.011)
Train: 56 [ 900/1251 ( 72%)]  Loss: 4.131 (3.92)  Time: 0.702s, 1458.96/s  (0.699s, 1463.94/s)  LR: 9.789e-04  Data: 0.009 (0.011)
Train: 56 [ 950/1251 ( 76%)]  Loss: 3.660 (3.91)  Time: 0.701s, 1460.18/s  (0.700s, 1463.67/s)  LR: 9.789e-04  Data: 0.009 (0.011)
Train: 56 [1000/1251 ( 80%)]  Loss: 4.417 (3.93)  Time: 0.703s, 1456.49/s  (0.700s, 1463.37/s)  LR: 9.789e-04  Data: 0.009 (0.011)
Train: 56 [1050/1251 ( 84%)]  Loss: 4.289 (3.95)  Time: 0.672s, 1524.01/s  (0.700s, 1463.77/s)  LR: 9.789e-04  Data: 0.009 (0.011)
Train: 56 [1100/1251 ( 88%)]  Loss: 3.859 (3.94)  Time: 0.705s, 1452.33/s  (0.699s, 1464.01/s)  LR: 9.789e-04  Data: 0.009 (0.011)
Train: 56 [1150/1251 ( 92%)]  Loss: 3.875 (3.94)  Time: 0.704s, 1453.61/s  (0.699s, 1463.92/s)  LR: 9.789e-04  Data: 0.009 (0.011)
Train: 56 [1200/1251 ( 96%)]  Loss: 4.209 (3.95)  Time: 0.672s, 1523.96/s  (0.700s, 1463.85/s)  LR: 9.789e-04  Data: 0.010 (0.011)
Train: 56 [1250/1251 (100%)]  Loss: 3.916 (3.95)  Time: 0.692s, 1479.69/s  (0.699s, 1464.11/s)  LR: 9.789e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.560 (1.560)  Loss:  0.9951 (0.9951)  Acc@1: 85.0586 (85.0586)  Acc@5: 96.1914 (96.1914)
Test: [  48/48]  Time: 0.136 (0.552)  Loss:  1.0557 (1.6828)  Acc@1: 81.4859 (67.0880)  Acc@5: 94.8113 (88.1780)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-53.pth.tar', 67.14599997314453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-56.pth.tar', 67.08800010253906)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-55.pth.tar', 66.81400001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-52.pth.tar', 66.34600004394531)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-50.pth.tar', 66.29000002685547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-41.pth.tar', 66.15200010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-48.pth.tar', 66.15200005126952)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-54.pth.tar', 66.00599994628907)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-45.pth.tar', 65.77800005615235)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-49.pth.tar', 65.76800005859376)

Train: 57 [   0/1251 (  0%)]  Loss: 3.853 (3.85)  Time: 2.084s,  491.33/s  (2.084s,  491.33/s)  LR: 9.781e-04  Data: 1.469 (1.469)
Train: 57 [  50/1251 (  4%)]  Loss: 3.725 (3.79)  Time: 0.705s, 1451.92/s  (0.728s, 1407.05/s)  LR: 9.781e-04  Data: 0.011 (0.043)
Train: 57 [ 100/1251 (  8%)]  Loss: 4.292 (3.96)  Time: 0.671s, 1526.78/s  (0.715s, 1433.07/s)  LR: 9.781e-04  Data: 0.010 (0.027)
Train: 57 [ 150/1251 ( 12%)]  Loss: 4.121 (4.00)  Time: 0.701s, 1460.59/s  (0.706s, 1450.67/s)  LR: 9.781e-04  Data: 0.011 (0.021)
Train: 57 [ 200/1251 ( 16%)]  Loss: 4.196 (4.04)  Time: 0.707s, 1448.27/s  (0.706s, 1449.53/s)  LR: 9.781e-04  Data: 0.011 (0.019)
Train: 57 [ 250/1251 ( 20%)]  Loss: 4.019 (4.03)  Time: 0.704s, 1453.69/s  (0.701s, 1460.47/s)  LR: 9.781e-04  Data: 0.010 (0.017)
Train: 57 [ 300/1251 ( 24%)]  Loss: 4.055 (4.04)  Time: 0.704s, 1453.79/s  (0.702s, 1458.95/s)  LR: 9.781e-04  Data: 0.010 (0.016)
Train: 57 [ 350/1251 ( 28%)]  Loss: 3.848 (4.01)  Time: 0.705s, 1452.66/s  (0.702s, 1458.03/s)  LR: 9.781e-04  Data: 0.009 (0.015)
Train: 57 [ 400/1251 ( 32%)]  Loss: 4.130 (4.03)  Time: 0.705s, 1453.03/s  (0.703s, 1457.37/s)  LR: 9.781e-04  Data: 0.010 (0.014)
Train: 57 [ 450/1251 ( 36%)]  Loss: 3.713 (4.00)  Time: 0.705s, 1453.49/s  (0.703s, 1456.57/s)  LR: 9.781e-04  Data: 0.010 (0.014)
Train: 57 [ 500/1251 ( 40%)]  Loss: 4.048 (4.00)  Time: 0.705s, 1452.73/s  (0.703s, 1455.93/s)  LR: 9.781e-04  Data: 0.011 (0.014)
Train: 57 [ 550/1251 ( 44%)]  Loss: 3.954 (4.00)  Time: 0.704s, 1454.57/s  (0.704s, 1454.89/s)  LR: 9.781e-04  Data: 0.010 (0.013)
Train: 57 [ 600/1251 ( 48%)]  Loss: 3.886 (3.99)  Time: 0.762s, 1343.61/s  (0.704s, 1454.33/s)  LR: 9.781e-04  Data: 0.011 (0.013)
Train: 57 [ 650/1251 ( 52%)]  Loss: 4.197 (4.00)  Time: 0.704s, 1455.11/s  (0.704s, 1454.29/s)  LR: 9.781e-04  Data: 0.010 (0.013)
Train: 57 [ 700/1251 ( 56%)]  Loss: 4.364 (4.03)  Time: 0.705s, 1452.83/s  (0.704s, 1454.20/s)  LR: 9.781e-04  Data: 0.010 (0.013)
Train: 57 [ 750/1251 ( 60%)]  Loss: 4.243 (4.04)  Time: 0.703s, 1456.23/s  (0.704s, 1453.89/s)  LR: 9.781e-04  Data: 0.010 (0.012)
Train: 57 [ 800/1251 ( 64%)]  Loss: 4.121 (4.04)  Time: 0.704s, 1454.18/s  (0.704s, 1453.76/s)  LR: 9.781e-04  Data: 0.010 (0.012)
Train: 57 [ 850/1251 ( 68%)]  Loss: 4.215 (4.05)  Time: 0.704s, 1454.08/s  (0.704s, 1453.72/s)  LR: 9.781e-04  Data: 0.010 (0.012)
Train: 57 [ 900/1251 ( 72%)]  Loss: 4.065 (4.05)  Time: 0.704s, 1455.03/s  (0.705s, 1453.35/s)  LR: 9.781e-04  Data: 0.010 (0.012)
Train: 57 [ 950/1251 ( 76%)]  Loss: 3.901 (4.05)  Time: 0.705s, 1453.38/s  (0.705s, 1453.01/s)  LR: 9.781e-04  Data: 0.010 (0.012)
Train: 57 [1000/1251 ( 80%)]  Loss: 3.916 (4.04)  Time: 0.708s, 1446.20/s  (0.704s, 1453.71/s)  LR: 9.781e-04  Data: 0.009 (0.012)
Train: 57 [1050/1251 ( 84%)]  Loss: 3.773 (4.03)  Time: 0.702s, 1459.44/s  (0.704s, 1454.50/s)  LR: 9.781e-04  Data: 0.010 (0.012)
Train: 57 [1100/1251 ( 88%)]  Loss: 3.781 (4.02)  Time: 0.702s, 1458.07/s  (0.704s, 1454.74/s)  LR: 9.781e-04  Data: 0.010 (0.012)
Train: 57 [1150/1251 ( 92%)]  Loss: 4.310 (4.03)  Time: 0.702s, 1457.88/s  (0.704s, 1454.83/s)  LR: 9.781e-04  Data: 0.011 (0.012)
Train: 57 [1200/1251 ( 96%)]  Loss: 3.842 (4.02)  Time: 0.702s, 1457.89/s  (0.704s, 1455.00/s)  LR: 9.781e-04  Data: 0.010 (0.011)
Train: 57 [1250/1251 (100%)]  Loss: 3.929 (4.02)  Time: 0.692s, 1479.65/s  (0.704s, 1455.09/s)  LR: 9.781e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.501 (1.501)  Loss:  1.1221 (1.1221)  Acc@1: 83.6914 (83.6914)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.136 (0.550)  Loss:  1.3086 (1.7768)  Acc@1: 79.1274 (66.6160)  Acc@5: 92.6887 (87.5840)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-53.pth.tar', 67.14599997314453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-56.pth.tar', 67.08800010253906)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-55.pth.tar', 66.81400001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-57.pth.tar', 66.61599998291015)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-52.pth.tar', 66.34600004394531)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-50.pth.tar', 66.29000002685547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-41.pth.tar', 66.15200010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-48.pth.tar', 66.15200005126952)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-54.pth.tar', 66.00599994628907)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-45.pth.tar', 65.77800005615235)

Train: 58 [   0/1251 (  0%)]  Loss: 4.091 (4.09)  Time: 2.073s,  493.93/s  (2.073s,  493.93/s)  LR: 9.773e-04  Data: 1.457 (1.457)
Train: 58 [  50/1251 (  4%)]  Loss: 4.056 (4.07)  Time: 0.701s, 1461.40/s  (0.722s, 1418.68/s)  LR: 9.773e-04  Data: 0.009 (0.045)
Train: 58 [ 100/1251 (  8%)]  Loss: 3.934 (4.03)  Time: 0.704s, 1454.25/s  (0.712s, 1438.23/s)  LR: 9.773e-04  Data: 0.009 (0.027)
Train: 58 [ 150/1251 ( 12%)]  Loss: 4.167 (4.06)  Time: 0.701s, 1460.32/s  (0.708s, 1445.56/s)  LR: 9.773e-04  Data: 0.009 (0.021)
Train: 58 [ 200/1251 ( 16%)]  Loss: 4.078 (4.07)  Time: 0.706s, 1449.91/s  (0.706s, 1450.36/s)  LR: 9.773e-04  Data: 0.009 (0.018)
Train: 58 [ 250/1251 ( 20%)]  Loss: 4.037 (4.06)  Time: 0.707s, 1447.58/s  (0.706s, 1450.85/s)  LR: 9.773e-04  Data: 0.010 (0.017)
Train: 58 [ 300/1251 ( 24%)]  Loss: 3.734 (4.01)  Time: 0.702s, 1458.42/s  (0.705s, 1452.10/s)  LR: 9.773e-04  Data: 0.009 (0.015)
Train: 58 [ 350/1251 ( 28%)]  Loss: 3.894 (4.00)  Time: 0.703s, 1457.55/s  (0.705s, 1451.64/s)  LR: 9.773e-04  Data: 0.009 (0.014)
Train: 58 [ 400/1251 ( 32%)]  Loss: 4.043 (4.00)  Time: 0.703s, 1455.99/s  (0.706s, 1451.45/s)  LR: 9.773e-04  Data: 0.009 (0.014)
Train: 58 [ 450/1251 ( 36%)]  Loss: 4.033 (4.01)  Time: 0.703s, 1457.63/s  (0.705s, 1451.71/s)  LR: 9.773e-04  Data: 0.009 (0.013)
Train: 58 [ 500/1251 ( 40%)]  Loss: 4.336 (4.04)  Time: 0.707s, 1448.64/s  (0.705s, 1452.34/s)  LR: 9.773e-04  Data: 0.009 (0.013)
Train: 58 [ 550/1251 ( 44%)]  Loss: 3.925 (4.03)  Time: 0.704s, 1455.56/s  (0.705s, 1452.64/s)  LR: 9.773e-04  Data: 0.009 (0.013)
Train: 58 [ 600/1251 ( 48%)]  Loss: 4.246 (4.04)  Time: 0.704s, 1455.19/s  (0.704s, 1455.09/s)  LR: 9.773e-04  Data: 0.009 (0.012)
Train: 58 [ 650/1251 ( 52%)]  Loss: 3.893 (4.03)  Time: 0.705s, 1453.36/s  (0.704s, 1454.78/s)  LR: 9.773e-04  Data: 0.009 (0.012)
Train: 58 [ 700/1251 ( 56%)]  Loss: 3.888 (4.02)  Time: 0.702s, 1458.90/s  (0.703s, 1456.57/s)  LR: 9.773e-04  Data: 0.010 (0.012)
Train: 58 [ 750/1251 ( 60%)]  Loss: 3.836 (4.01)  Time: 0.672s, 1522.77/s  (0.703s, 1456.51/s)  LR: 9.773e-04  Data: 0.010 (0.012)
Train: 58 [ 800/1251 ( 64%)]  Loss: 3.971 (4.01)  Time: 0.703s, 1457.16/s  (0.703s, 1456.73/s)  LR: 9.773e-04  Data: 0.009 (0.012)
Train: 58 [ 850/1251 ( 68%)]  Loss: 3.886 (4.00)  Time: 0.704s, 1455.47/s  (0.703s, 1456.65/s)  LR: 9.773e-04  Data: 0.009 (0.012)
Train: 58 [ 900/1251 ( 72%)]  Loss: 3.810 (3.99)  Time: 0.701s, 1460.72/s  (0.702s, 1459.38/s)  LR: 9.773e-04  Data: 0.010 (0.011)
Train: 58 [ 950/1251 ( 76%)]  Loss: 3.775 (3.98)  Time: 0.701s, 1461.40/s  (0.702s, 1459.30/s)  LR: 9.773e-04  Data: 0.010 (0.011)
Train: 58 [1000/1251 ( 80%)]  Loss: 3.951 (3.98)  Time: 0.703s, 1457.08/s  (0.702s, 1459.23/s)  LR: 9.773e-04  Data: 0.010 (0.011)
Train: 58 [1050/1251 ( 84%)]  Loss: 4.229 (3.99)  Time: 0.701s, 1460.80/s  (0.702s, 1459.16/s)  LR: 9.773e-04  Data: 0.010 (0.011)
Train: 58 [1100/1251 ( 88%)]  Loss: 4.138 (4.00)  Time: 0.706s, 1451.44/s  (0.701s, 1459.84/s)  LR: 9.773e-04  Data: 0.009 (0.011)
Train: 58 [1150/1251 ( 92%)]  Loss: 4.020 (4.00)  Time: 0.702s, 1459.38/s  (0.702s, 1459.58/s)  LR: 9.773e-04  Data: 0.009 (0.011)
Train: 58 [1200/1251 ( 96%)]  Loss: 4.168 (4.01)  Time: 0.708s, 1445.36/s  (0.701s, 1459.81/s)  LR: 9.773e-04  Data: 0.009 (0.011)
Train: 58 [1250/1251 (100%)]  Loss: 3.761 (4.00)  Time: 0.692s, 1479.71/s  (0.702s, 1459.66/s)  LR: 9.773e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.439 (1.439)  Loss:  1.1025 (1.1025)  Acc@1: 83.4961 (83.4961)  Acc@5: 95.8984 (95.8984)
Test: [  48/48]  Time: 0.136 (0.548)  Loss:  1.1172 (1.7130)  Acc@1: 80.8962 (67.3300)  Acc@5: 94.2217 (88.0260)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-58.pth.tar', 67.33000010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-53.pth.tar', 67.14599997314453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-56.pth.tar', 67.08800010253906)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-55.pth.tar', 66.81400001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-57.pth.tar', 66.61599998291015)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-52.pth.tar', 66.34600004394531)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-50.pth.tar', 66.29000002685547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-41.pth.tar', 66.15200010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-48.pth.tar', 66.15200005126952)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-54.pth.tar', 66.00599994628907)

Train: 59 [   0/1251 (  0%)]  Loss: 4.194 (4.19)  Time: 2.057s,  497.71/s  (2.057s,  497.71/s)  LR: 9.766e-04  Data: 1.442 (1.442)
Train: 59 [  50/1251 (  4%)]  Loss: 4.029 (4.11)  Time: 0.703s, 1456.91/s  (0.732s, 1398.81/s)  LR: 9.766e-04  Data: 0.009 (0.042)
Train: 59 [ 100/1251 (  8%)]  Loss: 4.428 (4.22)  Time: 0.706s, 1449.72/s  (0.719s, 1424.94/s)  LR: 9.766e-04  Data: 0.009 (0.026)
Train: 59 [ 150/1251 ( 12%)]  Loss: 4.261 (4.23)  Time: 0.801s, 1278.00/s  (0.714s, 1433.55/s)  LR: 9.766e-04  Data: 0.009 (0.021)
Train: 59 [ 200/1251 ( 16%)]  Loss: 4.012 (4.18)  Time: 0.671s, 1526.79/s  (0.711s, 1439.37/s)  LR: 9.766e-04  Data: 0.010 (0.018)
Train: 59 [ 250/1251 ( 20%)]  Loss: 3.915 (4.14)  Time: 0.702s, 1457.69/s  (0.709s, 1444.11/s)  LR: 9.766e-04  Data: 0.009 (0.016)
Train: 59 [ 300/1251 ( 24%)]  Loss: 3.891 (4.10)  Time: 0.706s, 1451.01/s  (0.707s, 1448.85/s)  LR: 9.766e-04  Data: 0.010 (0.015)
Train: 59 [ 350/1251 ( 28%)]  Loss: 3.914 (4.08)  Time: 0.675s, 1517.44/s  (0.706s, 1451.00/s)  LR: 9.766e-04  Data: 0.009 (0.014)
Train: 59 [ 400/1251 ( 32%)]  Loss: 3.842 (4.05)  Time: 0.707s, 1449.31/s  (0.705s, 1451.65/s)  LR: 9.766e-04  Data: 0.009 (0.014)
Train: 59 [ 450/1251 ( 36%)]  Loss: 4.405 (4.09)  Time: 0.702s, 1459.29/s  (0.704s, 1453.57/s)  LR: 9.766e-04  Data: 0.009 (0.013)
Train: 59 [ 500/1251 ( 40%)]  Loss: 3.909 (4.07)  Time: 0.703s, 1457.15/s  (0.704s, 1453.82/s)  LR: 9.766e-04  Data: 0.009 (0.013)
Train: 59 [ 550/1251 ( 44%)]  Loss: 4.321 (4.09)  Time: 0.709s, 1444.70/s  (0.705s, 1452.87/s)  LR: 9.766e-04  Data: 0.009 (0.013)
Train: 59 [ 600/1251 ( 48%)]  Loss: 4.058 (4.09)  Time: 0.704s, 1454.95/s  (0.704s, 1454.13/s)  LR: 9.766e-04  Data: 0.009 (0.012)
Train: 59 [ 650/1251 ( 52%)]  Loss: 4.137 (4.09)  Time: 0.706s, 1450.05/s  (0.704s, 1454.00/s)  LR: 9.766e-04  Data: 0.009 (0.012)
Train: 59 [ 700/1251 ( 56%)]  Loss: 3.862 (4.08)  Time: 0.707s, 1449.26/s  (0.704s, 1454.24/s)  LR: 9.766e-04  Data: 0.009 (0.012)
Train: 59 [ 750/1251 ( 60%)]  Loss: 4.157 (4.08)  Time: 0.709s, 1444.26/s  (0.704s, 1454.37/s)  LR: 9.766e-04  Data: 0.010 (0.012)
Train: 59 [ 800/1251 ( 64%)]  Loss: 3.978 (4.08)  Time: 0.672s, 1523.51/s  (0.702s, 1458.20/s)  LR: 9.766e-04  Data: 0.010 (0.012)
Train: 59 [ 850/1251 ( 68%)]  Loss: 3.723 (4.06)  Time: 0.671s, 1525.17/s  (0.701s, 1460.76/s)  LR: 9.766e-04  Data: 0.010 (0.012)
Train: 59 [ 900/1251 ( 72%)]  Loss: 3.893 (4.05)  Time: 0.707s, 1448.29/s  (0.700s, 1462.09/s)  LR: 9.766e-04  Data: 0.010 (0.011)
Train: 59 [ 950/1251 ( 76%)]  Loss: 4.127 (4.05)  Time: 0.672s, 1524.13/s  (0.700s, 1462.95/s)  LR: 9.766e-04  Data: 0.010 (0.011)
Train: 59 [1000/1251 ( 80%)]  Loss: 4.323 (4.07)  Time: 0.671s, 1526.65/s  (0.699s, 1465.70/s)  LR: 9.766e-04  Data: 0.010 (0.011)
Train: 59 [1050/1251 ( 84%)]  Loss: 3.936 (4.06)  Time: 0.703s, 1457.06/s  (0.699s, 1465.43/s)  LR: 9.766e-04  Data: 0.010 (0.011)
Train: 59 [1100/1251 ( 88%)]  Loss: 4.674 (4.09)  Time: 0.707s, 1448.09/s  (0.699s, 1465.26/s)  LR: 9.766e-04  Data: 0.009 (0.011)
Train: 59 [1150/1251 ( 92%)]  Loss: 3.821 (4.08)  Time: 0.702s, 1457.89/s  (0.699s, 1464.88/s)  LR: 9.766e-04  Data: 0.009 (0.011)
Train: 59 [1200/1251 ( 96%)]  Loss: 3.901 (4.07)  Time: 0.707s, 1448.50/s  (0.699s, 1464.40/s)  LR: 9.766e-04  Data: 0.009 (0.011)
Train: 59 [1250/1251 (100%)]  Loss: 3.761 (4.06)  Time: 0.688s, 1489.23/s  (0.699s, 1464.63/s)  LR: 9.766e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.510 (1.510)  Loss:  1.0469 (1.0469)  Acc@1: 85.7422 (85.7422)  Acc@5: 96.0938 (96.0938)
Test: [  48/48]  Time: 0.137 (0.553)  Loss:  1.1387 (1.7079)  Acc@1: 80.8962 (66.8200)  Acc@5: 93.0425 (87.7980)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-58.pth.tar', 67.33000010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-53.pth.tar', 67.14599997314453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-56.pth.tar', 67.08800010253906)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-59.pth.tar', 66.81999997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-55.pth.tar', 66.81400001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-57.pth.tar', 66.61599998291015)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-52.pth.tar', 66.34600004394531)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-50.pth.tar', 66.29000002685547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-41.pth.tar', 66.15200010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-48.pth.tar', 66.15200005126952)

Train: 60 [   0/1251 (  0%)]  Loss: 4.268 (4.27)  Time: 2.207s,  463.96/s  (2.207s,  463.96/s)  LR: 9.758e-04  Data: 1.593 (1.593)
Train: 60 [  50/1251 (  4%)]  Loss: 3.898 (4.08)  Time: 0.671s, 1525.54/s  (0.737s, 1389.15/s)  LR: 9.758e-04  Data: 0.010 (0.048)
Train: 60 [ 100/1251 (  8%)]  Loss: 3.677 (3.95)  Time: 0.672s, 1524.50/s  (0.717s, 1427.90/s)  LR: 9.758e-04  Data: 0.010 (0.029)
Train: 60 [ 150/1251 ( 12%)]  Loss: 4.339 (4.05)  Time: 0.700s, 1462.10/s  (0.711s, 1439.48/s)  LR: 9.758e-04  Data: 0.010 (0.023)
Train: 60 [ 200/1251 ( 16%)]  Loss: 3.914 (4.02)  Time: 0.700s, 1461.91/s  (0.709s, 1443.58/s)  LR: 9.758e-04  Data: 0.010 (0.019)
Train: 60 [ 250/1251 ( 20%)]  Loss: 3.801 (3.98)  Time: 0.673s, 1522.50/s  (0.707s, 1448.38/s)  LR: 9.758e-04  Data: 0.010 (0.018)
Train: 60 [ 300/1251 ( 24%)]  Loss: 4.062 (3.99)  Time: 0.710s, 1443.22/s  (0.706s, 1450.37/s)  LR: 9.758e-04  Data: 0.010 (0.016)
Train: 60 [ 350/1251 ( 28%)]  Loss: 4.164 (4.02)  Time: 0.702s, 1459.73/s  (0.704s, 1454.22/s)  LR: 9.758e-04  Data: 0.010 (0.015)
Train: 60 [ 400/1251 ( 32%)]  Loss: 4.494 (4.07)  Time: 0.703s, 1456.95/s  (0.704s, 1454.91/s)  LR: 9.758e-04  Data: 0.009 (0.015)
Train: 60 [ 450/1251 ( 36%)]  Loss: 4.007 (4.06)  Time: 0.703s, 1457.26/s  (0.704s, 1455.25/s)  LR: 9.758e-04  Data: 0.010 (0.014)
Train: 60 [ 500/1251 ( 40%)]  Loss: 3.546 (4.02)  Time: 0.672s, 1523.17/s  (0.701s, 1460.31/s)  LR: 9.758e-04  Data: 0.010 (0.014)
Train: 60 [ 550/1251 ( 44%)]  Loss: 3.741 (3.99)  Time: 0.702s, 1458.28/s  (0.700s, 1463.68/s)  LR: 9.758e-04  Data: 0.010 (0.013)
Train: 60 [ 600/1251 ( 48%)]  Loss: 3.845 (3.98)  Time: 0.701s, 1461.70/s  (0.700s, 1463.61/s)  LR: 9.758e-04  Data: 0.010 (0.013)
Train: 60 [ 650/1251 ( 52%)]  Loss: 3.666 (3.96)  Time: 0.700s, 1462.62/s  (0.700s, 1463.06/s)  LR: 9.758e-04  Data: 0.010 (0.013)
Train: 60 [ 700/1251 ( 56%)]  Loss: 3.921 (3.96)  Time: 0.701s, 1460.44/s  (0.700s, 1462.57/s)  LR: 9.758e-04  Data: 0.010 (0.013)
Train: 60 [ 750/1251 ( 60%)]  Loss: 4.103 (3.97)  Time: 0.704s, 1455.05/s  (0.700s, 1461.88/s)  LR: 9.758e-04  Data: 0.010 (0.012)
Train: 60 [ 800/1251 ( 64%)]  Loss: 4.291 (3.98)  Time: 0.707s, 1448.68/s  (0.699s, 1465.31/s)  LR: 9.758e-04  Data: 0.011 (0.012)
Train: 60 [ 850/1251 ( 68%)]  Loss: 4.139 (3.99)  Time: 0.700s, 1462.32/s  (0.699s, 1464.83/s)  LR: 9.758e-04  Data: 0.010 (0.012)
Train: 60 [ 900/1251 ( 72%)]  Loss: 4.000 (3.99)  Time: 0.700s, 1462.06/s  (0.699s, 1464.48/s)  LR: 9.758e-04  Data: 0.010 (0.012)
Train: 60 [ 950/1251 ( 76%)]  Loss: 4.145 (4.00)  Time: 0.701s, 1461.39/s  (0.700s, 1463.85/s)  LR: 9.758e-04  Data: 0.009 (0.012)
Train: 60 [1000/1251 ( 80%)]  Loss: 4.196 (4.01)  Time: 0.701s, 1460.04/s  (0.700s, 1463.41/s)  LR: 9.758e-04  Data: 0.009 (0.012)
Train: 60 [1050/1251 ( 84%)]  Loss: 4.339 (4.03)  Time: 0.673s, 1521.95/s  (0.699s, 1464.60/s)  LR: 9.758e-04  Data: 0.010 (0.012)
Train: 60 [1100/1251 ( 88%)]  Loss: 3.580 (4.01)  Time: 0.702s, 1458.47/s  (0.699s, 1464.04/s)  LR: 9.758e-04  Data: 0.010 (0.012)
Train: 60 [1150/1251 ( 92%)]  Loss: 3.997 (4.01)  Time: 0.700s, 1461.87/s  (0.700s, 1463.77/s)  LR: 9.758e-04  Data: 0.010 (0.012)
Train: 60 [1200/1251 ( 96%)]  Loss: 4.174 (4.01)  Time: 0.701s, 1459.96/s  (0.700s, 1463.51/s)  LR: 9.758e-04  Data: 0.010 (0.011)
Train: 60 [1250/1251 (100%)]  Loss: 3.842 (4.01)  Time: 0.656s, 1561.94/s  (0.700s, 1463.42/s)  LR: 9.758e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.438 (1.438)  Loss:  1.0850 (1.0850)  Acc@1: 85.1562 (85.1562)  Acc@5: 95.2148 (95.2148)
Test: [  48/48]  Time: 0.136 (0.548)  Loss:  1.3193 (1.7826)  Acc@1: 79.3632 (66.6160)  Acc@5: 91.9811 (87.6420)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-58.pth.tar', 67.33000010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-53.pth.tar', 67.14599997314453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-56.pth.tar', 67.08800010253906)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-59.pth.tar', 66.81999997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-55.pth.tar', 66.81400001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-60.pth.tar', 66.61600008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-57.pth.tar', 66.61599998291015)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-52.pth.tar', 66.34600004394531)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-50.pth.tar', 66.29000002685547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-41.pth.tar', 66.15200010498047)

Train: 61 [   0/1251 (  0%)]  Loss: 4.034 (4.03)  Time: 2.146s,  477.23/s  (2.146s,  477.23/s)  LR: 9.750e-04  Data: 1.529 (1.529)
Train: 61 [  50/1251 (  4%)]  Loss: 4.099 (4.07)  Time: 0.671s, 1526.80/s  (0.706s, 1450.18/s)  LR: 9.750e-04  Data: 0.010 (0.045)
Train: 61 [ 100/1251 (  8%)]  Loss: 3.992 (4.04)  Time: 0.672s, 1524.77/s  (0.690s, 1484.12/s)  LR: 9.750e-04  Data: 0.010 (0.028)
Train: 61 [ 150/1251 ( 12%)]  Loss: 3.989 (4.03)  Time: 0.672s, 1523.09/s  (0.684s, 1496.82/s)  LR: 9.750e-04  Data: 0.010 (0.022)
Train: 61 [ 200/1251 ( 16%)]  Loss: 4.403 (4.10)  Time: 0.672s, 1524.49/s  (0.683s, 1500.33/s)  LR: 9.750e-04  Data: 0.010 (0.019)
Train: 61 [ 250/1251 ( 20%)]  Loss: 4.048 (4.09)  Time: 0.672s, 1523.27/s  (0.681s, 1503.48/s)  LR: 9.750e-04  Data: 0.010 (0.017)
Train: 61 [ 300/1251 ( 24%)]  Loss: 3.805 (4.05)  Time: 0.672s, 1523.58/s  (0.680s, 1506.07/s)  LR: 9.750e-04  Data: 0.010 (0.016)
Train: 61 [ 350/1251 ( 28%)]  Loss: 4.094 (4.06)  Time: 0.674s, 1520.05/s  (0.679s, 1508.37/s)  LR: 9.750e-04  Data: 0.009 (0.015)
Train: 61 [ 400/1251 ( 32%)]  Loss: 4.154 (4.07)  Time: 0.676s, 1513.91/s  (0.678s, 1509.87/s)  LR: 9.750e-04  Data: 0.009 (0.014)
Train: 61 [ 450/1251 ( 36%)]  Loss: 3.804 (4.04)  Time: 0.673s, 1521.00/s  (0.678s, 1510.67/s)  LR: 9.750e-04  Data: 0.010 (0.014)
Train: 61 [ 500/1251 ( 40%)]  Loss: 4.092 (4.05)  Time: 0.670s, 1529.33/s  (0.677s, 1511.98/s)  LR: 9.750e-04  Data: 0.009 (0.013)
Train: 61 [ 550/1251 ( 44%)]  Loss: 4.251 (4.06)  Time: 0.702s, 1459.67/s  (0.679s, 1508.67/s)  LR: 9.750e-04  Data: 0.009 (0.013)
Train: 61 [ 600/1251 ( 48%)]  Loss: 3.853 (4.05)  Time: 0.679s, 1509.02/s  (0.681s, 1504.42/s)  LR: 9.750e-04  Data: 0.010 (0.013)
Train: 61 [ 650/1251 ( 52%)]  Loss: 3.974 (4.04)  Time: 0.703s, 1456.38/s  (0.682s, 1502.14/s)  LR: 9.750e-04  Data: 0.010 (0.013)
Train: 61 [ 700/1251 ( 56%)]  Loss: 4.189 (4.05)  Time: 0.703s, 1457.28/s  (0.683s, 1499.01/s)  LR: 9.750e-04  Data: 0.010 (0.012)
Train: 61 [ 750/1251 ( 60%)]  Loss: 3.755 (4.03)  Time: 0.701s, 1460.38/s  (0.684s, 1496.33/s)  LR: 9.750e-04  Data: 0.010 (0.012)
Train: 61 [ 800/1251 ( 64%)]  Loss: 3.778 (4.02)  Time: 0.701s, 1460.56/s  (0.685s, 1494.05/s)  LR: 9.750e-04  Data: 0.010 (0.012)
Train: 61 [ 850/1251 ( 68%)]  Loss: 4.135 (4.02)  Time: 0.707s, 1449.39/s  (0.686s, 1491.65/s)  LR: 9.750e-04  Data: 0.010 (0.012)
Train: 61 [ 900/1251 ( 72%)]  Loss: 4.368 (4.04)  Time: 0.702s, 1458.91/s  (0.687s, 1489.80/s)  LR: 9.750e-04  Data: 0.009 (0.012)
Train: 61 [ 950/1251 ( 76%)]  Loss: 3.722 (4.03)  Time: 0.672s, 1522.90/s  (0.688s, 1487.76/s)  LR: 9.750e-04  Data: 0.010 (0.012)
Train: 61 [1000/1251 ( 80%)]  Loss: 4.117 (4.03)  Time: 0.702s, 1459.35/s  (0.689s, 1487.18/s)  LR: 9.750e-04  Data: 0.010 (0.012)
Train: 61 [1050/1251 ( 84%)]  Loss: 3.950 (4.03)  Time: 0.705s, 1452.21/s  (0.689s, 1485.79/s)  LR: 9.750e-04  Data: 0.010 (0.012)
Train: 61 [1100/1251 ( 88%)]  Loss: 3.581 (4.01)  Time: 0.702s, 1457.87/s  (0.690s, 1484.57/s)  LR: 9.750e-04  Data: 0.010 (0.012)
Train: 61 [1150/1251 ( 92%)]  Loss: 3.926 (4.00)  Time: 0.702s, 1458.95/s  (0.690s, 1483.44/s)  LR: 9.750e-04  Data: 0.010 (0.011)
Train: 61 [1200/1251 ( 96%)]  Loss: 3.755 (3.99)  Time: 0.704s, 1453.66/s  (0.690s, 1483.10/s)  LR: 9.750e-04  Data: 0.009 (0.011)
Train: 61 [1250/1251 (100%)]  Loss: 4.008 (4.00)  Time: 0.698s, 1468.00/s  (0.690s, 1483.18/s)  LR: 9.750e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.487 (1.487)  Loss:  1.0996 (1.0996)  Acc@1: 84.2773 (84.2773)  Acc@5: 96.1914 (96.1914)
Test: [  48/48]  Time: 0.136 (0.549)  Loss:  1.1924 (1.7531)  Acc@1: 80.5425 (66.9540)  Acc@5: 93.8679 (87.8100)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-58.pth.tar', 67.33000010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-53.pth.tar', 67.14599997314453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-56.pth.tar', 67.08800010253906)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-61.pth.tar', 66.9540000805664)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-59.pth.tar', 66.81999997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-55.pth.tar', 66.81400001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-60.pth.tar', 66.61600008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-57.pth.tar', 66.61599998291015)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-52.pth.tar', 66.34600004394531)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-50.pth.tar', 66.29000002685547)

Train: 62 [   0/1251 (  0%)]  Loss: 3.965 (3.97)  Time: 2.092s,  489.38/s  (2.092s,  489.38/s)  LR: 9.741e-04  Data: 1.476 (1.476)
Train: 62 [  50/1251 (  4%)]  Loss: 3.989 (3.98)  Time: 0.673s, 1521.69/s  (0.729s, 1405.03/s)  LR: 9.741e-04  Data: 0.010 (0.043)
Train: 62 [ 100/1251 (  8%)]  Loss: 3.921 (3.96)  Time: 0.702s, 1458.71/s  (0.716s, 1430.25/s)  LR: 9.741e-04  Data: 0.010 (0.026)
Train: 62 [ 150/1251 ( 12%)]  Loss: 3.920 (3.95)  Time: 0.704s, 1455.12/s  (0.712s, 1438.85/s)  LR: 9.741e-04  Data: 0.010 (0.021)
Train: 62 [ 200/1251 ( 16%)]  Loss: 3.977 (3.95)  Time: 0.676s, 1515.34/s  (0.705s, 1452.59/s)  LR: 9.741e-04  Data: 0.010 (0.018)
Train: 62 [ 250/1251 ( 20%)]  Loss: 3.758 (3.92)  Time: 0.704s, 1455.57/s  (0.701s, 1461.64/s)  LR: 9.741e-04  Data: 0.009 (0.017)
Train: 62 [ 300/1251 ( 24%)]  Loss: 4.148 (3.95)  Time: 0.703s, 1456.17/s  (0.701s, 1460.36/s)  LR: 9.741e-04  Data: 0.010 (0.015)
Train: 62 [ 350/1251 ( 28%)]  Loss: 4.261 (3.99)  Time: 0.703s, 1455.73/s  (0.702s, 1459.69/s)  LR: 9.741e-04  Data: 0.009 (0.015)
Train: 62 [ 400/1251 ( 32%)]  Loss: 4.275 (4.02)  Time: 0.672s, 1523.78/s  (0.700s, 1462.71/s)  LR: 9.741e-04  Data: 0.010 (0.014)
Train: 62 [ 450/1251 ( 36%)]  Loss: 3.880 (4.01)  Time: 0.671s, 1524.98/s  (0.699s, 1464.57/s)  LR: 9.741e-04  Data: 0.009 (0.014)
Train: 62 [ 500/1251 ( 40%)]  Loss: 4.088 (4.02)  Time: 0.703s, 1455.80/s  (0.697s, 1468.23/s)  LR: 9.741e-04  Data: 0.009 (0.013)
Train: 62 [ 550/1251 ( 44%)]  Loss: 3.904 (4.01)  Time: 0.703s, 1456.37/s  (0.698s, 1466.88/s)  LR: 9.741e-04  Data: 0.010 (0.013)
Train: 62 [ 600/1251 ( 48%)]  Loss: 4.262 (4.03)  Time: 0.672s, 1524.77/s  (0.698s, 1467.83/s)  LR: 9.741e-04  Data: 0.010 (0.013)
Train: 62 [ 650/1251 ( 52%)]  Loss: 4.452 (4.06)  Time: 0.700s, 1462.05/s  (0.698s, 1467.86/s)  LR: 9.741e-04  Data: 0.009 (0.012)
Train: 62 [ 700/1251 ( 56%)]  Loss: 3.902 (4.05)  Time: 0.701s, 1460.74/s  (0.698s, 1467.31/s)  LR: 9.741e-04  Data: 0.010 (0.012)
Train: 62 [ 750/1251 ( 60%)]  Loss: 3.944 (4.04)  Time: 0.672s, 1524.72/s  (0.697s, 1468.97/s)  LR: 9.741e-04  Data: 0.010 (0.012)
Train: 62 [ 800/1251 ( 64%)]  Loss: 3.941 (4.03)  Time: 0.701s, 1460.98/s  (0.696s, 1471.13/s)  LR: 9.741e-04  Data: 0.010 (0.012)
Train: 62 [ 850/1251 ( 68%)]  Loss: 3.757 (4.02)  Time: 0.700s, 1461.96/s  (0.696s, 1470.25/s)  LR: 9.741e-04  Data: 0.010 (0.012)
Train: 62 [ 900/1251 ( 72%)]  Loss: 4.100 (4.02)  Time: 0.701s, 1461.42/s  (0.697s, 1469.63/s)  LR: 9.741e-04  Data: 0.010 (0.012)
Train: 62 [ 950/1251 ( 76%)]  Loss: 3.943 (4.02)  Time: 0.703s, 1456.59/s  (0.697s, 1468.81/s)  LR: 9.741e-04  Data: 0.009 (0.012)
Train: 62 [1000/1251 ( 80%)]  Loss: 3.682 (4.00)  Time: 0.700s, 1463.14/s  (0.697s, 1469.01/s)  LR: 9.741e-04  Data: 0.011 (0.011)
Train: 62 [1050/1251 ( 84%)]  Loss: 3.932 (4.00)  Time: 0.702s, 1458.32/s  (0.697s, 1468.45/s)  LR: 9.741e-04  Data: 0.009 (0.011)
Train: 62 [1100/1251 ( 88%)]  Loss: 4.049 (4.00)  Time: 0.703s, 1457.34/s  (0.697s, 1468.11/s)  LR: 9.741e-04  Data: 0.009 (0.011)
Train: 62 [1150/1251 ( 92%)]  Loss: 4.357 (4.02)  Time: 0.702s, 1458.54/s  (0.698s, 1467.13/s)  LR: 9.741e-04  Data: 0.009 (0.011)
Train: 62 [1200/1251 ( 96%)]  Loss: 4.189 (4.02)  Time: 0.710s, 1442.28/s  (0.698s, 1466.56/s)  LR: 9.741e-04  Data: 0.009 (0.011)
Train: 62 [1250/1251 (100%)]  Loss: 4.113 (4.03)  Time: 0.693s, 1477.38/s  (0.698s, 1466.71/s)  LR: 9.741e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.433 (1.433)  Loss:  0.9390 (0.9390)  Acc@1: 85.7422 (85.7422)  Acc@5: 95.1172 (95.1172)
Test: [  48/48]  Time: 0.136 (0.553)  Loss:  0.9927 (1.6045)  Acc@1: 81.6038 (67.8200)  Acc@5: 94.5755 (88.3820)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-62.pth.tar', 67.82000002441406)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-58.pth.tar', 67.33000010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-53.pth.tar', 67.14599997314453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-56.pth.tar', 67.08800010253906)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-61.pth.tar', 66.9540000805664)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-59.pth.tar', 66.81999997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-55.pth.tar', 66.81400001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-60.pth.tar', 66.61600008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-57.pth.tar', 66.61599998291015)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-52.pth.tar', 66.34600004394531)

Train: 63 [   0/1251 (  0%)]  Loss: 4.003 (4.00)  Time: 2.147s,  476.87/s  (2.147s,  476.87/s)  LR: 9.733e-04  Data: 1.480 (1.480)
Train: 63 [  50/1251 (  4%)]  Loss: 4.032 (4.02)  Time: 0.704s, 1455.05/s  (0.722s, 1418.73/s)  LR: 9.733e-04  Data: 0.009 (0.044)
Train: 63 [ 100/1251 (  8%)]  Loss: 4.030 (4.02)  Time: 0.703s, 1456.00/s  (0.712s, 1437.68/s)  LR: 9.733e-04  Data: 0.009 (0.027)
Train: 63 [ 150/1251 ( 12%)]  Loss: 4.036 (4.03)  Time: 0.702s, 1457.77/s  (0.710s, 1441.82/s)  LR: 9.733e-04  Data: 0.010 (0.021)
Train: 63 [ 200/1251 ( 16%)]  Loss: 4.160 (4.05)  Time: 0.703s, 1456.90/s  (0.707s, 1448.07/s)  LR: 9.733e-04  Data: 0.009 (0.018)
Train: 63 [ 250/1251 ( 20%)]  Loss: 3.938 (4.03)  Time: 0.703s, 1457.02/s  (0.706s, 1450.22/s)  LR: 9.733e-04  Data: 0.010 (0.017)
Train: 63 [ 300/1251 ( 24%)]  Loss: 4.138 (4.05)  Time: 0.671s, 1526.57/s  (0.705s, 1451.83/s)  LR: 9.733e-04  Data: 0.010 (0.015)
Train: 63 [ 350/1251 ( 28%)]  Loss: 4.093 (4.05)  Time: 0.702s, 1459.18/s  (0.705s, 1452.61/s)  LR: 9.733e-04  Data: 0.011 (0.015)
Train: 63 [ 400/1251 ( 32%)]  Loss: 4.354 (4.09)  Time: 0.703s, 1457.57/s  (0.705s, 1453.40/s)  LR: 9.733e-04  Data: 0.010 (0.014)
Train: 63 [ 450/1251 ( 36%)]  Loss: 4.343 (4.11)  Time: 0.703s, 1456.03/s  (0.704s, 1455.38/s)  LR: 9.733e-04  Data: 0.009 (0.014)
Train: 63 [ 500/1251 ( 40%)]  Loss: 4.377 (4.14)  Time: 0.702s, 1458.07/s  (0.704s, 1455.18/s)  LR: 9.733e-04  Data: 0.010 (0.013)
Train: 63 [ 550/1251 ( 44%)]  Loss: 3.673 (4.10)  Time: 0.699s, 1464.49/s  (0.703s, 1456.50/s)  LR: 9.733e-04  Data: 0.009 (0.013)
Train: 63 [ 600/1251 ( 48%)]  Loss: 4.154 (4.10)  Time: 0.699s, 1465.39/s  (0.703s, 1456.34/s)  LR: 9.733e-04  Data: 0.009 (0.013)
Train: 63 [ 650/1251 ( 52%)]  Loss: 3.775 (4.08)  Time: 0.701s, 1461.37/s  (0.702s, 1457.98/s)  LR: 9.733e-04  Data: 0.010 (0.012)
Train: 63 [ 700/1251 ( 56%)]  Loss: 3.784 (4.06)  Time: 0.702s, 1457.83/s  (0.702s, 1458.99/s)  LR: 9.733e-04  Data: 0.010 (0.012)
Train: 63 [ 750/1251 ( 60%)]  Loss: 4.001 (4.06)  Time: 0.672s, 1524.45/s  (0.702s, 1458.83/s)  LR: 9.733e-04  Data: 0.010 (0.012)
Train: 63 [ 800/1251 ( 64%)]  Loss: 3.842 (4.04)  Time: 0.672s, 1523.33/s  (0.700s, 1462.12/s)  LR: 9.733e-04  Data: 0.011 (0.012)
Train: 63 [ 850/1251 ( 68%)]  Loss: 3.959 (4.04)  Time: 0.702s, 1459.52/s  (0.700s, 1462.64/s)  LR: 9.733e-04  Data: 0.010 (0.012)
Train: 63 [ 900/1251 ( 72%)]  Loss: 4.125 (4.04)  Time: 0.705s, 1451.62/s  (0.700s, 1462.44/s)  LR: 9.733e-04  Data: 0.010 (0.012)
Train: 63 [ 950/1251 ( 76%)]  Loss: 4.195 (4.05)  Time: 0.704s, 1455.22/s  (0.701s, 1461.69/s)  LR: 9.733e-04  Data: 0.009 (0.012)
Train: 63 [1000/1251 ( 80%)]  Loss: 3.879 (4.04)  Time: 0.702s, 1457.66/s  (0.701s, 1461.32/s)  LR: 9.733e-04  Data: 0.009 (0.011)
Train: 63 [1050/1251 ( 84%)]  Loss: 3.954 (4.04)  Time: 0.706s, 1450.85/s  (0.701s, 1460.92/s)  LR: 9.733e-04  Data: 0.010 (0.011)
Train: 63 [1100/1251 ( 88%)]  Loss: 3.701 (4.02)  Time: 0.703s, 1457.42/s  (0.701s, 1461.49/s)  LR: 9.733e-04  Data: 0.013 (0.011)
Train: 63 [1150/1251 ( 92%)]  Loss: 4.257 (4.03)  Time: 0.702s, 1458.91/s  (0.701s, 1461.38/s)  LR: 9.733e-04  Data: 0.010 (0.011)
Train: 63 [1200/1251 ( 96%)]  Loss: 3.579 (4.02)  Time: 0.705s, 1452.08/s  (0.701s, 1461.25/s)  LR: 9.733e-04  Data: 0.009 (0.011)
Train: 63 [1250/1251 (100%)]  Loss: 4.312 (4.03)  Time: 0.690s, 1484.86/s  (0.701s, 1461.75/s)  LR: 9.733e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.490 (1.490)  Loss:  1.0088 (1.0088)  Acc@1: 86.2305 (86.2305)  Acc@5: 95.9961 (95.9961)
Test: [  48/48]  Time: 0.136 (0.552)  Loss:  1.1631 (1.7224)  Acc@1: 80.6604 (67.5660)  Acc@5: 94.6934 (88.4780)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-62.pth.tar', 67.82000002441406)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-63.pth.tar', 67.56600013183593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-58.pth.tar', 67.33000010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-53.pth.tar', 67.14599997314453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-56.pth.tar', 67.08800010253906)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-61.pth.tar', 66.9540000805664)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-59.pth.tar', 66.81999997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-55.pth.tar', 66.81400001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-60.pth.tar', 66.61600008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-57.pth.tar', 66.61599998291015)

Train: 64 [   0/1251 (  0%)]  Loss: 3.861 (3.86)  Time: 2.044s,  501.09/s  (2.044s,  501.09/s)  LR: 9.725e-04  Data: 1.428 (1.428)
Train: 64 [  50/1251 (  4%)]  Loss: 3.910 (3.89)  Time: 0.672s, 1522.71/s  (0.735s, 1393.52/s)  LR: 9.725e-04  Data: 0.010 (0.043)
Train: 64 [ 100/1251 (  8%)]  Loss: 3.735 (3.84)  Time: 0.710s, 1442.27/s  (0.716s, 1429.26/s)  LR: 9.725e-04  Data: 0.009 (0.027)
Train: 64 [ 150/1251 ( 12%)]  Loss: 3.829 (3.83)  Time: 0.704s, 1453.94/s  (0.707s, 1449.00/s)  LR: 9.725e-04  Data: 0.009 (0.021)
Train: 64 [ 200/1251 ( 16%)]  Loss: 4.294 (3.93)  Time: 0.703s, 1457.56/s  (0.705s, 1453.02/s)  LR: 9.725e-04  Data: 0.009 (0.018)
Train: 64 [ 250/1251 ( 20%)]  Loss: 3.955 (3.93)  Time: 0.702s, 1459.16/s  (0.704s, 1454.12/s)  LR: 9.725e-04  Data: 0.010 (0.017)
Train: 64 [ 300/1251 ( 24%)]  Loss: 3.813 (3.91)  Time: 0.701s, 1461.02/s  (0.701s, 1459.80/s)  LR: 9.725e-04  Data: 0.009 (0.015)
Train: 64 [ 350/1251 ( 28%)]  Loss: 3.990 (3.92)  Time: 0.703s, 1457.55/s  (0.702s, 1459.44/s)  LR: 9.725e-04  Data: 0.009 (0.015)
Train: 64 [ 400/1251 ( 32%)]  Loss: 4.049 (3.94)  Time: 0.708s, 1445.59/s  (0.702s, 1458.20/s)  LR: 9.725e-04  Data: 0.009 (0.014)
Train: 64 [ 450/1251 ( 36%)]  Loss: 3.663 (3.91)  Time: 0.672s, 1523.28/s  (0.700s, 1462.20/s)  LR: 9.725e-04  Data: 0.010 (0.013)
Train: 64 [ 500/1251 ( 40%)]  Loss: 4.064 (3.92)  Time: 0.702s, 1458.60/s  (0.698s, 1467.08/s)  LR: 9.725e-04  Data: 0.009 (0.013)
Train: 64 [ 550/1251 ( 44%)]  Loss: 3.613 (3.90)  Time: 0.703s, 1456.59/s  (0.699s, 1465.88/s)  LR: 9.725e-04  Data: 0.009 (0.013)
Train: 64 [ 600/1251 ( 48%)]  Loss: 4.252 (3.93)  Time: 0.700s, 1462.21/s  (0.698s, 1467.08/s)  LR: 9.725e-04  Data: 0.010 (0.013)
Train: 64 [ 650/1251 ( 52%)]  Loss: 3.903 (3.92)  Time: 0.703s, 1456.92/s  (0.698s, 1466.42/s)  LR: 9.725e-04  Data: 0.010 (0.012)
Train: 64 [ 700/1251 ( 56%)]  Loss: 4.471 (3.96)  Time: 0.700s, 1463.22/s  (0.699s, 1465.65/s)  LR: 9.725e-04  Data: 0.011 (0.012)
Train: 64 [ 750/1251 ( 60%)]  Loss: 4.003 (3.96)  Time: 0.703s, 1457.57/s  (0.698s, 1467.72/s)  LR: 9.725e-04  Data: 0.009 (0.012)
Train: 64 [ 800/1251 ( 64%)]  Loss: 3.765 (3.95)  Time: 0.700s, 1462.00/s  (0.697s, 1468.52/s)  LR: 9.725e-04  Data: 0.010 (0.012)
Train: 64 [ 850/1251 ( 68%)]  Loss: 3.915 (3.95)  Time: 0.704s, 1454.78/s  (0.696s, 1470.83/s)  LR: 9.725e-04  Data: 0.010 (0.012)
Train: 64 [ 900/1251 ( 72%)]  Loss: 3.883 (3.95)  Time: 0.672s, 1523.19/s  (0.695s, 1472.90/s)  LR: 9.725e-04  Data: 0.009 (0.012)
Train: 64 [ 950/1251 ( 76%)]  Loss: 4.283 (3.96)  Time: 0.708s, 1445.51/s  (0.696s, 1471.94/s)  LR: 9.725e-04  Data: 0.010 (0.012)
Train: 64 [1000/1251 ( 80%)]  Loss: 4.083 (3.97)  Time: 0.701s, 1461.38/s  (0.695s, 1473.08/s)  LR: 9.725e-04  Data: 0.009 (0.011)
Train: 64 [1050/1251 ( 84%)]  Loss: 3.936 (3.97)  Time: 0.704s, 1454.03/s  (0.696s, 1472.27/s)  LR: 9.725e-04  Data: 0.009 (0.011)
Train: 64 [1100/1251 ( 88%)]  Loss: 3.891 (3.96)  Time: 0.701s, 1461.73/s  (0.696s, 1471.44/s)  LR: 9.725e-04  Data: 0.010 (0.011)
Train: 64 [1150/1251 ( 92%)]  Loss: 3.993 (3.96)  Time: 0.704s, 1455.03/s  (0.696s, 1470.53/s)  LR: 9.725e-04  Data: 0.010 (0.011)
Train: 64 [1200/1251 ( 96%)]  Loss: 4.109 (3.97)  Time: 0.704s, 1455.12/s  (0.696s, 1470.31/s)  LR: 9.725e-04  Data: 0.010 (0.011)
Train: 64 [1250/1251 (100%)]  Loss: 3.749 (3.96)  Time: 0.690s, 1484.78/s  (0.697s, 1469.63/s)  LR: 9.725e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.487 (1.487)  Loss:  1.1465 (1.1465)  Acc@1: 84.3750 (84.3750)  Acc@5: 95.6055 (95.6055)
Test: [  48/48]  Time: 0.136 (0.546)  Loss:  1.1191 (1.7042)  Acc@1: 81.0142 (66.6640)  Acc@5: 94.9293 (87.6560)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-62.pth.tar', 67.82000002441406)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-63.pth.tar', 67.56600013183593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-58.pth.tar', 67.33000010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-53.pth.tar', 67.14599997314453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-56.pth.tar', 67.08800010253906)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-61.pth.tar', 66.9540000805664)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-59.pth.tar', 66.81999997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-55.pth.tar', 66.81400001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-64.pth.tar', 66.66400002685548)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-60.pth.tar', 66.61600008544922)

Train: 65 [   0/1251 (  0%)]  Loss: 3.993 (3.99)  Time: 2.048s,  499.91/s  (2.048s,  499.91/s)  LR: 9.716e-04  Data: 1.432 (1.432)
Train: 65 [  50/1251 (  4%)]  Loss: 3.983 (3.99)  Time: 0.703s, 1456.16/s  (0.720s, 1422.19/s)  LR: 9.716e-04  Data: 0.009 (0.045)
Train: 65 [ 100/1251 (  8%)]  Loss: 4.037 (4.00)  Time: 0.703s, 1456.95/s  (0.711s, 1439.31/s)  LR: 9.716e-04  Data: 0.009 (0.027)
Train: 65 [ 150/1251 ( 12%)]  Loss: 3.887 (3.97)  Time: 0.703s, 1456.69/s  (0.702s, 1459.52/s)  LR: 9.716e-04  Data: 0.009 (0.021)
Train: 65 [ 200/1251 ( 16%)]  Loss: 3.538 (3.89)  Time: 0.703s, 1456.08/s  (0.700s, 1463.32/s)  LR: 9.716e-04  Data: 0.009 (0.018)
Train: 65 [ 250/1251 ( 20%)]  Loss: 4.163 (3.93)  Time: 0.702s, 1458.10/s  (0.700s, 1462.65/s)  LR: 9.716e-04  Data: 0.009 (0.017)
Train: 65 [ 300/1251 ( 24%)]  Loss: 3.861 (3.92)  Time: 0.704s, 1454.48/s  (0.701s, 1460.94/s)  LR: 9.716e-04  Data: 0.009 (0.015)
Train: 65 [ 350/1251 ( 28%)]  Loss: 4.739 (4.03)  Time: 0.709s, 1444.42/s  (0.699s, 1465.83/s)  LR: 9.716e-04  Data: 0.009 (0.015)
Train: 65 [ 400/1251 ( 32%)]  Loss: 3.944 (4.02)  Time: 0.675s, 1516.37/s  (0.695s, 1472.79/s)  LR: 9.716e-04  Data: 0.010 (0.014)
Train: 65 [ 450/1251 ( 36%)]  Loss: 3.920 (4.01)  Time: 0.701s, 1460.15/s  (0.694s, 1474.80/s)  LR: 9.716e-04  Data: 0.009 (0.014)
Train: 65 [ 500/1251 ( 40%)]  Loss: 3.925 (4.00)  Time: 0.708s, 1447.05/s  (0.695s, 1472.94/s)  LR: 9.716e-04  Data: 0.010 (0.013)
Train: 65 [ 550/1251 ( 44%)]  Loss: 4.052 (4.00)  Time: 0.702s, 1459.00/s  (0.696s, 1471.46/s)  LR: 9.716e-04  Data: 0.010 (0.013)
Train: 65 [ 600/1251 ( 48%)]  Loss: 4.024 (4.00)  Time: 0.760s, 1347.60/s  (0.696s, 1470.73/s)  LR: 9.716e-04  Data: 0.009 (0.013)
Train: 65 [ 650/1251 ( 52%)]  Loss: 3.757 (3.99)  Time: 0.700s, 1463.09/s  (0.695s, 1474.34/s)  LR: 9.716e-04  Data: 0.009 (0.012)
Train: 65 [ 700/1251 ( 56%)]  Loss: 3.941 (3.98)  Time: 0.702s, 1457.88/s  (0.694s, 1474.85/s)  LR: 9.716e-04  Data: 0.010 (0.012)
Train: 65 [ 750/1251 ( 60%)]  Loss: 4.214 (4.00)  Time: 0.703s, 1456.39/s  (0.694s, 1474.59/s)  LR: 9.716e-04  Data: 0.009 (0.012)
Train: 65 [ 800/1251 ( 64%)]  Loss: 4.057 (4.00)  Time: 0.702s, 1458.31/s  (0.694s, 1475.80/s)  LR: 9.716e-04  Data: 0.009 (0.012)
Train: 65 [ 850/1251 ( 68%)]  Loss: 3.896 (4.00)  Time: 0.704s, 1454.48/s  (0.694s, 1475.07/s)  LR: 9.716e-04  Data: 0.010 (0.012)
Train: 65 [ 900/1251 ( 72%)]  Loss: 4.202 (4.01)  Time: 0.704s, 1454.79/s  (0.695s, 1473.73/s)  LR: 9.716e-04  Data: 0.009 (0.012)
Train: 65 [ 950/1251 ( 76%)]  Loss: 4.364 (4.02)  Time: 0.704s, 1454.12/s  (0.696s, 1472.22/s)  LR: 9.716e-04  Data: 0.010 (0.012)
Train: 65 [1000/1251 ( 80%)]  Loss: 3.945 (4.02)  Time: 0.705s, 1452.77/s  (0.696s, 1471.05/s)  LR: 9.716e-04  Data: 0.010 (0.012)
Train: 65 [1050/1251 ( 84%)]  Loss: 4.044 (4.02)  Time: 0.706s, 1451.36/s  (0.697s, 1470.05/s)  LR: 9.716e-04  Data: 0.010 (0.011)
Train: 65 [1100/1251 ( 88%)]  Loss: 3.829 (4.01)  Time: 0.704s, 1453.82/s  (0.697s, 1469.26/s)  LR: 9.716e-04  Data: 0.010 (0.011)
Train: 65 [1150/1251 ( 92%)]  Loss: 4.123 (4.02)  Time: 0.706s, 1450.91/s  (0.697s, 1468.34/s)  LR: 9.716e-04  Data: 0.011 (0.011)
Train: 65 [1200/1251 ( 96%)]  Loss: 4.176 (4.02)  Time: 0.794s, 1290.28/s  (0.697s, 1468.34/s)  LR: 9.716e-04  Data: 0.009 (0.011)
Train: 65 [1250/1251 (100%)]  Loss: 3.917 (4.02)  Time: 0.687s, 1491.24/s  (0.698s, 1467.94/s)  LR: 9.716e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.430 (1.430)  Loss:  0.8657 (0.8657)  Acc@1: 86.0352 (86.0352)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.136 (0.546)  Loss:  1.0566 (1.6730)  Acc@1: 81.3679 (66.8880)  Acc@5: 94.5755 (87.7780)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-62.pth.tar', 67.82000002441406)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-63.pth.tar', 67.56600013183593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-58.pth.tar', 67.33000010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-53.pth.tar', 67.14599997314453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-56.pth.tar', 67.08800010253906)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-61.pth.tar', 66.9540000805664)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-65.pth.tar', 66.887999921875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-59.pth.tar', 66.81999997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-55.pth.tar', 66.81400001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-64.pth.tar', 66.66400002685548)

Train: 66 [   0/1251 (  0%)]  Loss: 3.680 (3.68)  Time: 2.097s,  488.24/s  (2.097s,  488.24/s)  LR: 9.707e-04  Data: 1.479 (1.479)
Train: 66 [  50/1251 (  4%)]  Loss: 3.794 (3.74)  Time: 0.704s, 1455.52/s  (0.712s, 1437.21/s)  LR: 9.707e-04  Data: 0.009 (0.043)
Train: 66 [ 100/1251 (  8%)]  Loss: 3.925 (3.80)  Time: 0.702s, 1458.25/s  (0.700s, 1462.20/s)  LR: 9.707e-04  Data: 0.009 (0.027)
Train: 66 [ 150/1251 ( 12%)]  Loss: 3.901 (3.83)  Time: 0.700s, 1462.41/s  (0.700s, 1463.03/s)  LR: 9.707e-04  Data: 0.010 (0.021)
Train: 66 [ 200/1251 ( 16%)]  Loss: 3.703 (3.80)  Time: 0.706s, 1449.64/s  (0.700s, 1462.03/s)  LR: 9.707e-04  Data: 0.009 (0.018)
Train: 66 [ 250/1251 ( 20%)]  Loss: 4.070 (3.85)  Time: 0.702s, 1459.52/s  (0.701s, 1461.09/s)  LR: 9.707e-04  Data: 0.009 (0.016)
Train: 66 [ 300/1251 ( 24%)]  Loss: 3.726 (3.83)  Time: 0.672s, 1524.61/s  (0.696s, 1470.21/s)  LR: 9.707e-04  Data: 0.009 (0.015)
Train: 66 [ 350/1251 ( 28%)]  Loss: 3.977 (3.85)  Time: 0.703s, 1456.69/s  (0.696s, 1470.27/s)  LR: 9.707e-04  Data: 0.010 (0.015)
Train: 66 [ 400/1251 ( 32%)]  Loss: 3.764 (3.84)  Time: 0.673s, 1521.84/s  (0.696s, 1470.38/s)  LR: 9.707e-04  Data: 0.010 (0.014)
Train: 66 [ 450/1251 ( 36%)]  Loss: 4.207 (3.87)  Time: 0.700s, 1462.98/s  (0.695s, 1472.38/s)  LR: 9.707e-04  Data: 0.009 (0.014)
Train: 66 [ 500/1251 ( 40%)]  Loss: 4.047 (3.89)  Time: 0.705s, 1453.33/s  (0.696s, 1472.00/s)  LR: 9.707e-04  Data: 0.010 (0.013)
Train: 66 [ 550/1251 ( 44%)]  Loss: 4.102 (3.91)  Time: 0.673s, 1521.33/s  (0.696s, 1470.32/s)  LR: 9.707e-04  Data: 0.010 (0.013)
Train: 66 [ 600/1251 ( 48%)]  Loss: 4.016 (3.92)  Time: 0.703s, 1457.61/s  (0.696s, 1471.38/s)  LR: 9.707e-04  Data: 0.009 (0.013)
Train: 66 [ 650/1251 ( 52%)]  Loss: 4.225 (3.94)  Time: 0.672s, 1524.40/s  (0.694s, 1475.24/s)  LR: 9.707e-04  Data: 0.010 (0.012)
Train: 66 [ 700/1251 ( 56%)]  Loss: 3.910 (3.94)  Time: 0.702s, 1459.71/s  (0.695s, 1474.06/s)  LR: 9.707e-04  Data: 0.010 (0.012)
Train: 66 [ 750/1251 ( 60%)]  Loss: 4.220 (3.95)  Time: 0.702s, 1457.87/s  (0.695s, 1473.58/s)  LR: 9.707e-04  Data: 0.009 (0.012)
Train: 66 [ 800/1251 ( 64%)]  Loss: 4.025 (3.96)  Time: 0.703s, 1457.10/s  (0.695s, 1472.86/s)  LR: 9.707e-04  Data: 0.009 (0.012)
Train: 66 [ 850/1251 ( 68%)]  Loss: 3.893 (3.95)  Time: 0.672s, 1523.91/s  (0.694s, 1474.45/s)  LR: 9.707e-04  Data: 0.010 (0.012)
Train: 66 [ 900/1251 ( 72%)]  Loss: 3.749 (3.94)  Time: 0.702s, 1458.17/s  (0.695s, 1473.98/s)  LR: 9.707e-04  Data: 0.009 (0.012)
Train: 66 [ 950/1251 ( 76%)]  Loss: 4.045 (3.95)  Time: 0.702s, 1457.89/s  (0.695s, 1472.93/s)  LR: 9.707e-04  Data: 0.009 (0.012)
Train: 66 [1000/1251 ( 80%)]  Loss: 4.157 (3.96)  Time: 0.699s, 1464.27/s  (0.695s, 1473.81/s)  LR: 9.707e-04  Data: 0.011 (0.011)
Train: 66 [1050/1251 ( 84%)]  Loss: 4.409 (3.98)  Time: 0.672s, 1524.21/s  (0.695s, 1474.35/s)  LR: 9.707e-04  Data: 0.010 (0.011)
Train: 66 [1100/1251 ( 88%)]  Loss: 4.211 (3.99)  Time: 0.700s, 1462.50/s  (0.695s, 1473.75/s)  LR: 9.707e-04  Data: 0.010 (0.011)
Train: 66 [1150/1251 ( 92%)]  Loss: 3.900 (3.99)  Time: 0.700s, 1462.16/s  (0.695s, 1472.67/s)  LR: 9.707e-04  Data: 0.010 (0.011)
Train: 66 [1200/1251 ( 96%)]  Loss: 4.168 (3.99)  Time: 0.701s, 1461.60/s  (0.695s, 1473.13/s)  LR: 9.707e-04  Data: 0.010 (0.011)
Train: 66 [1250/1251 (100%)]  Loss: 4.451 (4.01)  Time: 0.687s, 1490.10/s  (0.695s, 1472.47/s)  LR: 9.707e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.456 (1.456)  Loss:  1.0391 (1.0391)  Acc@1: 86.0352 (86.0352)  Acc@5: 95.4102 (95.4102)
Test: [  48/48]  Time: 0.137 (0.551)  Loss:  0.9512 (1.6697)  Acc@1: 82.7830 (67.3200)  Acc@5: 95.5189 (88.1920)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-62.pth.tar', 67.82000002441406)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-63.pth.tar', 67.56600013183593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-58.pth.tar', 67.33000010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-66.pth.tar', 67.32000001953125)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-53.pth.tar', 67.14599997314453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-56.pth.tar', 67.08800010253906)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-61.pth.tar', 66.9540000805664)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-65.pth.tar', 66.887999921875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-59.pth.tar', 66.81999997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-55.pth.tar', 66.81400001220703)

Train: 67 [   0/1251 (  0%)]  Loss: 4.227 (4.23)  Time: 2.039s,  502.27/s  (2.039s,  502.27/s)  LR: 9.699e-04  Data: 1.423 (1.423)
Train: 67 [  50/1251 (  4%)]  Loss: 3.865 (4.05)  Time: 0.701s, 1459.88/s  (0.723s, 1416.01/s)  LR: 9.699e-04  Data: 0.010 (0.047)
Train: 67 [ 100/1251 (  8%)]  Loss: 3.840 (3.98)  Time: 0.704s, 1454.50/s  (0.713s, 1436.86/s)  LR: 9.699e-04  Data: 0.010 (0.029)
Train: 67 [ 150/1251 ( 12%)]  Loss: 3.960 (3.97)  Time: 0.703s, 1456.58/s  (0.709s, 1443.38/s)  LR: 9.699e-04  Data: 0.010 (0.023)
Train: 67 [ 200/1251 ( 16%)]  Loss: 3.745 (3.93)  Time: 0.704s, 1453.85/s  (0.708s, 1446.67/s)  LR: 9.699e-04  Data: 0.010 (0.019)
Train: 67 [ 250/1251 ( 20%)]  Loss: 4.109 (3.96)  Time: 0.703s, 1457.40/s  (0.707s, 1447.79/s)  LR: 9.699e-04  Data: 0.010 (0.017)
Train: 67 [ 300/1251 ( 24%)]  Loss: 3.528 (3.90)  Time: 0.707s, 1449.26/s  (0.706s, 1450.93/s)  LR: 9.699e-04  Data: 0.010 (0.016)
Train: 67 [ 350/1251 ( 28%)]  Loss: 4.378 (3.96)  Time: 0.701s, 1460.17/s  (0.705s, 1451.83/s)  LR: 9.699e-04  Data: 0.010 (0.015)
Train: 67 [ 400/1251 ( 32%)]  Loss: 4.085 (3.97)  Time: 0.702s, 1457.65/s  (0.705s, 1452.66/s)  LR: 9.699e-04  Data: 0.010 (0.015)
Train: 67 [ 450/1251 ( 36%)]  Loss: 4.393 (4.01)  Time: 0.702s, 1459.05/s  (0.705s, 1453.48/s)  LR: 9.699e-04  Data: 0.010 (0.014)
Train: 67 [ 500/1251 ( 40%)]  Loss: 3.819 (4.00)  Time: 0.671s, 1526.36/s  (0.702s, 1457.68/s)  LR: 9.699e-04  Data: 0.010 (0.014)
Train: 67 [ 550/1251 ( 44%)]  Loss: 4.329 (4.02)  Time: 0.702s, 1459.47/s  (0.701s, 1460.73/s)  LR: 9.699e-04  Data: 0.009 (0.013)
Train: 67 [ 600/1251 ( 48%)]  Loss: 3.823 (4.01)  Time: 0.703s, 1456.47/s  (0.701s, 1460.20/s)  LR: 9.699e-04  Data: 0.010 (0.013)
Train: 67 [ 650/1251 ( 52%)]  Loss: 3.919 (4.00)  Time: 0.705s, 1453.24/s  (0.702s, 1459.31/s)  LR: 9.699e-04  Data: 0.011 (0.013)
Train: 67 [ 700/1251 ( 56%)]  Loss: 4.054 (4.00)  Time: 0.703s, 1456.88/s  (0.702s, 1459.16/s)  LR: 9.699e-04  Data: 0.010 (0.013)
Train: 67 [ 750/1251 ( 60%)]  Loss: 3.831 (3.99)  Time: 0.673s, 1522.41/s  (0.701s, 1460.22/s)  LR: 9.699e-04  Data: 0.009 (0.012)
Train: 67 [ 800/1251 ( 64%)]  Loss: 3.965 (3.99)  Time: 0.700s, 1463.24/s  (0.700s, 1463.26/s)  LR: 9.699e-04  Data: 0.009 (0.012)
Train: 67 [ 850/1251 ( 68%)]  Loss: 3.857 (3.98)  Time: 0.702s, 1458.28/s  (0.699s, 1464.65/s)  LR: 9.699e-04  Data: 0.009 (0.012)
Train: 67 [ 900/1251 ( 72%)]  Loss: 4.334 (4.00)  Time: 0.702s, 1458.88/s  (0.699s, 1463.98/s)  LR: 9.699e-04  Data: 0.009 (0.012)
Train: 67 [ 950/1251 ( 76%)]  Loss: 3.780 (3.99)  Time: 0.705s, 1452.67/s  (0.700s, 1463.20/s)  LR: 9.699e-04  Data: 0.009 (0.012)
Train: 67 [1000/1251 ( 80%)]  Loss: 4.233 (4.00)  Time: 0.702s, 1457.87/s  (0.700s, 1462.62/s)  LR: 9.699e-04  Data: 0.010 (0.012)
Train: 67 [1050/1251 ( 84%)]  Loss: 3.858 (4.00)  Time: 0.792s, 1293.38/s  (0.700s, 1462.63/s)  LR: 9.699e-04  Data: 0.010 (0.012)
Train: 67 [1100/1251 ( 88%)]  Loss: 3.844 (3.99)  Time: 0.701s, 1460.13/s  (0.700s, 1463.60/s)  LR: 9.699e-04  Data: 0.009 (0.012)
Train: 67 [1150/1251 ( 92%)]  Loss: 4.019 (3.99)  Time: 0.699s, 1464.87/s  (0.700s, 1463.50/s)  LR: 9.699e-04  Data: 0.010 (0.011)
Train: 67 [1200/1251 ( 96%)]  Loss: 3.781 (3.98)  Time: 0.703s, 1457.55/s  (0.699s, 1464.03/s)  LR: 9.699e-04  Data: 0.010 (0.011)
Train: 67 [1250/1251 (100%)]  Loss: 4.346 (4.00)  Time: 0.656s, 1560.70/s  (0.699s, 1464.82/s)  LR: 9.699e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.442 (1.442)  Loss:  1.1689 (1.1689)  Acc@1: 84.4727 (84.4727)  Acc@5: 94.8242 (94.8242)
Test: [  48/48]  Time: 0.136 (0.550)  Loss:  1.2051 (1.8171)  Acc@1: 81.2500 (66.6360)  Acc@5: 94.6934 (87.6980)
Train: 68 [   0/1251 (  0%)]  Loss: 4.009 (4.01)  Time: 2.210s,  463.44/s  (2.210s,  463.44/s)  LR: 9.690e-04  Data: 1.594 (1.594)
Train: 68 [  50/1251 (  4%)]  Loss: 3.973 (3.99)  Time: 0.702s, 1457.96/s  (0.727s, 1408.77/s)  LR: 9.690e-04  Data: 0.009 (0.045)
Train: 68 [ 100/1251 (  8%)]  Loss: 3.625 (3.87)  Time: 0.720s, 1421.91/s  (0.719s, 1423.51/s)  LR: 9.690e-04  Data: 0.016 (0.028)
Train: 68 [ 150/1251 ( 12%)]  Loss: 4.261 (3.97)  Time: 0.672s, 1523.61/s  (0.714s, 1433.89/s)  LR: 9.690e-04  Data: 0.010 (0.022)
Train: 68 [ 200/1251 ( 16%)]  Loss: 3.935 (3.96)  Time: 0.702s, 1457.78/s  (0.706s, 1450.29/s)  LR: 9.690e-04  Data: 0.009 (0.019)
Train: 68 [ 250/1251 ( 20%)]  Loss: 3.807 (3.94)  Time: 0.671s, 1525.47/s  (0.705s, 1452.41/s)  LR: 9.690e-04  Data: 0.009 (0.017)
Train: 68 [ 300/1251 ( 24%)]  Loss: 3.525 (3.88)  Time: 0.702s, 1459.36/s  (0.704s, 1454.72/s)  LR: 9.690e-04  Data: 0.009 (0.016)
Train: 68 [ 350/1251 ( 28%)]  Loss: 3.781 (3.86)  Time: 0.701s, 1461.79/s  (0.702s, 1459.44/s)  LR: 9.690e-04  Data: 0.010 (0.015)
Train: 68 [ 400/1251 ( 32%)]  Loss: 4.039 (3.88)  Time: 0.702s, 1457.70/s  (0.701s, 1460.49/s)  LR: 9.690e-04  Data: 0.009 (0.014)
Train: 68 [ 450/1251 ( 36%)]  Loss: 4.282 (3.92)  Time: 0.700s, 1462.18/s  (0.701s, 1461.16/s)  LR: 9.690e-04  Data: 0.009 (0.014)
Train: 68 [ 500/1251 ( 40%)]  Loss: 4.445 (3.97)  Time: 0.706s, 1451.25/s  (0.700s, 1462.41/s)  LR: 9.690e-04  Data: 0.009 (0.013)
Train: 68 [ 550/1251 ( 44%)]  Loss: 4.119 (3.98)  Time: 0.712s, 1437.33/s  (0.698s, 1466.32/s)  LR: 9.690e-04  Data: 0.009 (0.013)
Train: 68 [ 600/1251 ( 48%)]  Loss: 3.891 (3.98)  Time: 0.701s, 1461.26/s  (0.698s, 1467.96/s)  LR: 9.690e-04  Data: 0.009 (0.013)
Train: 68 [ 650/1251 ( 52%)]  Loss: 3.560 (3.95)  Time: 0.672s, 1523.33/s  (0.697s, 1470.12/s)  LR: 9.690e-04  Data: 0.010 (0.012)
Train: 68 [ 700/1251 ( 56%)]  Loss: 4.032 (3.95)  Time: 0.706s, 1450.24/s  (0.696s, 1471.25/s)  LR: 9.690e-04  Data: 0.009 (0.012)
Train: 68 [ 750/1251 ( 60%)]  Loss: 4.026 (3.96)  Time: 0.703s, 1456.14/s  (0.696s, 1470.22/s)  LR: 9.690e-04  Data: 0.009 (0.012)
Train: 68 [ 800/1251 ( 64%)]  Loss: 4.044 (3.96)  Time: 0.702s, 1459.30/s  (0.697s, 1470.01/s)  LR: 9.690e-04  Data: 0.010 (0.012)
Train: 68 [ 850/1251 ( 68%)]  Loss: 4.016 (3.97)  Time: 0.702s, 1458.84/s  (0.697s, 1468.75/s)  LR: 9.690e-04  Data: 0.010 (0.012)
Train: 68 [ 900/1251 ( 72%)]  Loss: 3.788 (3.96)  Time: 0.672s, 1523.44/s  (0.697s, 1469.19/s)  LR: 9.690e-04  Data: 0.010 (0.012)
Train: 68 [ 950/1251 ( 76%)]  Loss: 3.923 (3.95)  Time: 0.707s, 1448.29/s  (0.697s, 1469.44/s)  LR: 9.690e-04  Data: 0.009 (0.012)
Train: 68 [1000/1251 ( 80%)]  Loss: 4.241 (3.97)  Time: 0.673s, 1521.65/s  (0.696s, 1472.04/s)  LR: 9.690e-04  Data: 0.010 (0.011)
Train: 68 [1050/1251 ( 84%)]  Loss: 4.044 (3.97)  Time: 0.678s, 1510.41/s  (0.696s, 1472.07/s)  LR: 9.690e-04  Data: 0.009 (0.011)
Train: 68 [1100/1251 ( 88%)]  Loss: 4.144 (3.98)  Time: 0.701s, 1461.61/s  (0.695s, 1473.44/s)  LR: 9.690e-04  Data: 0.010 (0.011)
Train: 68 [1150/1251 ( 92%)]  Loss: 3.878 (3.97)  Time: 0.704s, 1454.91/s  (0.695s, 1473.55/s)  LR: 9.690e-04  Data: 0.010 (0.011)
Train: 68 [1200/1251 ( 96%)]  Loss: 4.071 (3.98)  Time: 0.707s, 1448.45/s  (0.695s, 1472.38/s)  LR: 9.690e-04  Data: 0.009 (0.011)
Train: 68 [1250/1251 (100%)]  Loss: 4.140 (3.98)  Time: 0.688s, 1488.84/s  (0.696s, 1472.11/s)  LR: 9.690e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.472 (1.472)  Loss:  0.9961 (0.9961)  Acc@1: 85.1562 (85.1562)  Acc@5: 96.2891 (96.2891)
Test: [  48/48]  Time: 0.141 (0.546)  Loss:  1.2441 (1.7143)  Acc@1: 78.7736 (67.3380)  Acc@5: 90.9198 (88.4400)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-62.pth.tar', 67.82000002441406)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-63.pth.tar', 67.56600013183593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-68.pth.tar', 67.3379999584961)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-58.pth.tar', 67.33000010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-66.pth.tar', 67.32000001953125)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-53.pth.tar', 67.14599997314453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-56.pth.tar', 67.08800010253906)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-61.pth.tar', 66.9540000805664)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-65.pth.tar', 66.887999921875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-59.pth.tar', 66.81999997558594)

Train: 69 [   0/1251 (  0%)]  Loss: 4.493 (4.49)  Time: 2.073s,  493.90/s  (2.073s,  493.90/s)  LR: 9.680e-04  Data: 1.457 (1.457)
Train: 69 [  50/1251 (  4%)]  Loss: 3.777 (4.14)  Time: 0.703s, 1456.89/s  (0.737s, 1388.87/s)  LR: 9.680e-04  Data: 0.010 (0.048)
Train: 69 [ 100/1251 (  8%)]  Loss: 3.917 (4.06)  Time: 0.699s, 1465.18/s  (0.720s, 1422.89/s)  LR: 9.680e-04  Data: 0.009 (0.029)
Train: 69 [ 150/1251 ( 12%)]  Loss: 3.848 (4.01)  Time: 0.703s, 1456.66/s  (0.710s, 1441.44/s)  LR: 9.680e-04  Data: 0.009 (0.023)
Train: 69 [ 200/1251 ( 16%)]  Loss: 4.162 (4.04)  Time: 0.702s, 1458.85/s  (0.709s, 1443.61/s)  LR: 9.680e-04  Data: 0.009 (0.019)
Train: 69 [ 250/1251 ( 20%)]  Loss: 3.986 (4.03)  Time: 0.686s, 1491.69/s  (0.706s, 1451.13/s)  LR: 9.680e-04  Data: 0.009 (0.017)
Train: 69 [ 300/1251 ( 24%)]  Loss: 3.885 (4.01)  Time: 0.753s, 1359.19/s  (0.703s, 1455.99/s)  LR: 9.680e-04  Data: 0.010 (0.016)
Train: 69 [ 350/1251 ( 28%)]  Loss: 3.728 (3.97)  Time: 0.686s, 1492.16/s  (0.702s, 1457.98/s)  LR: 9.680e-04  Data: 0.012 (0.015)
Train: 69 [ 400/1251 ( 32%)]  Loss: 4.178 (4.00)  Time: 0.665s, 1540.69/s  (0.702s, 1457.90/s)  LR: 9.680e-04  Data: 0.008 (0.015)
Train: 69 [ 450/1251 ( 36%)]  Loss: 3.738 (3.97)  Time: 0.720s, 1421.88/s  (0.701s, 1460.47/s)  LR: 9.680e-04  Data: 0.009 (0.014)
Train: 69 [ 500/1251 ( 40%)]  Loss: 4.121 (3.98)  Time: 0.667s, 1535.28/s  (0.701s, 1461.76/s)  LR: 9.680e-04  Data: 0.011 (0.014)
Train: 69 [ 550/1251 ( 44%)]  Loss: 4.262 (4.01)  Time: 0.685s, 1493.92/s  (0.701s, 1461.04/s)  LR: 9.680e-04  Data: 0.013 (0.014)
Train: 69 [ 600/1251 ( 48%)]  Loss: 3.786 (3.99)  Time: 0.865s, 1183.58/s  (0.701s, 1461.71/s)  LR: 9.680e-04  Data: 0.010 (0.013)
Train: 69 [ 650/1251 ( 52%)]  Loss: 4.095 (4.00)  Time: 0.708s, 1445.60/s  (0.701s, 1460.57/s)  LR: 9.680e-04  Data: 0.009 (0.013)
Train: 69 [ 700/1251 ( 56%)]  Loss: 4.128 (4.01)  Time: 0.689s, 1485.19/s  (0.700s, 1461.87/s)  LR: 9.680e-04  Data: 0.013 (0.013)
Train: 69 [ 750/1251 ( 60%)]  Loss: 4.226 (4.02)  Time: 0.795s, 1288.72/s  (0.700s, 1462.28/s)  LR: 9.680e-04  Data: 0.009 (0.013)
Train: 69 [ 800/1251 ( 64%)]  Loss: 4.218 (4.03)  Time: 0.671s, 1525.82/s  (0.700s, 1463.32/s)  LR: 9.680e-04  Data: 0.009 (0.012)
Train: 69 [ 850/1251 ( 68%)]  Loss: 3.875 (4.02)  Time: 0.715s, 1431.86/s  (0.699s, 1463.97/s)  LR: 9.680e-04  Data: 0.010 (0.012)
Train: 69 [ 900/1251 ( 72%)]  Loss: 4.199 (4.03)  Time: 0.695s, 1473.79/s  (0.699s, 1464.18/s)  LR: 9.680e-04  Data: 0.011 (0.012)
Train: 69 [ 950/1251 ( 76%)]  Loss: 3.516 (4.01)  Time: 0.702s, 1458.77/s  (0.699s, 1464.57/s)  LR: 9.680e-04  Data: 0.011 (0.012)
Train: 69 [1000/1251 ( 80%)]  Loss: 3.894 (4.00)  Time: 0.674s, 1518.62/s  (0.699s, 1465.38/s)  LR: 9.680e-04  Data: 0.010 (0.012)
Train: 69 [1050/1251 ( 84%)]  Loss: 4.042 (4.00)  Time: 0.677s, 1511.94/s  (0.699s, 1465.77/s)  LR: 9.680e-04  Data: 0.011 (0.012)
Train: 69 [1100/1251 ( 88%)]  Loss: 4.166 (4.01)  Time: 0.705s, 1452.01/s  (0.698s, 1466.03/s)  LR: 9.680e-04  Data: 0.010 (0.012)
Train: 69 [1150/1251 ( 92%)]  Loss: 3.996 (4.01)  Time: 0.708s, 1445.72/s  (0.698s, 1466.39/s)  LR: 9.680e-04  Data: 0.009 (0.012)
Train: 69 [1200/1251 ( 96%)]  Loss: 4.322 (4.02)  Time: 0.707s, 1448.52/s  (0.698s, 1466.82/s)  LR: 9.680e-04  Data: 0.009 (0.012)
Train: 69 [1250/1251 (100%)]  Loss: 3.598 (4.01)  Time: 0.690s, 1484.61/s  (0.698s, 1466.46/s)  LR: 9.680e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.505 (1.505)  Loss:  0.9448 (0.9448)  Acc@1: 84.1797 (84.1797)  Acc@5: 96.0938 (96.0938)
Test: [  48/48]  Time: 0.137 (0.586)  Loss:  1.1064 (1.6283)  Acc@1: 80.6604 (67.9180)  Acc@5: 93.6321 (88.5680)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-69.pth.tar', 67.9180000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-62.pth.tar', 67.82000002441406)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-63.pth.tar', 67.56600013183593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-68.pth.tar', 67.3379999584961)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-58.pth.tar', 67.33000010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-66.pth.tar', 67.32000001953125)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-53.pth.tar', 67.14599997314453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-56.pth.tar', 67.08800010253906)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-61.pth.tar', 66.9540000805664)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-65.pth.tar', 66.887999921875)

Train: 70 [   0/1251 (  0%)]  Loss: 3.794 (3.79)  Time: 2.165s,  473.07/s  (2.165s,  473.07/s)  LR: 9.671e-04  Data: 1.549 (1.549)
Train: 70 [  50/1251 (  4%)]  Loss: 4.403 (4.10)  Time: 0.675s, 1517.22/s  (0.743s, 1378.57/s)  LR: 9.671e-04  Data: 0.010 (0.055)
Train: 70 [ 100/1251 (  8%)]  Loss: 3.907 (4.03)  Time: 0.711s, 1441.14/s  (0.724s, 1414.31/s)  LR: 9.671e-04  Data: 0.010 (0.033)
Train: 70 [ 150/1251 ( 12%)]  Loss: 4.077 (4.05)  Time: 0.685s, 1494.49/s  (0.716s, 1429.72/s)  LR: 9.671e-04  Data: 0.009 (0.025)
Train: 70 [ 200/1251 ( 16%)]  Loss: 4.019 (4.04)  Time: 0.673s, 1521.19/s  (0.711s, 1440.61/s)  LR: 9.671e-04  Data: 0.010 (0.022)
Train: 70 [ 250/1251 ( 20%)]  Loss: 3.479 (3.95)  Time: 0.674s, 1518.68/s  (0.707s, 1448.15/s)  LR: 9.671e-04  Data: 0.010 (0.019)
Train: 70 [ 300/1251 ( 24%)]  Loss: 3.882 (3.94)  Time: 0.673s, 1520.62/s  (0.704s, 1454.95/s)  LR: 9.671e-04  Data: 0.012 (0.018)
Train: 70 [ 350/1251 ( 28%)]  Loss: 4.100 (3.96)  Time: 0.685s, 1495.62/s  (0.703s, 1456.94/s)  LR: 9.671e-04  Data: 0.011 (0.017)
Train: 70 [ 400/1251 ( 32%)]  Loss: 4.083 (3.97)  Time: 0.674s, 1519.24/s  (0.701s, 1460.14/s)  LR: 9.671e-04  Data: 0.011 (0.016)
Train: 70 [ 450/1251 ( 36%)]  Loss: 4.185 (3.99)  Time: 0.717s, 1427.73/s  (0.700s, 1462.17/s)  LR: 9.671e-04  Data: 0.010 (0.015)
Train: 70 [ 500/1251 ( 40%)]  Loss: 4.322 (4.02)  Time: 0.730s, 1403.05/s  (0.701s, 1460.48/s)  LR: 9.671e-04  Data: 0.010 (0.015)
Train: 70 [ 550/1251 ( 44%)]  Loss: 3.762 (4.00)  Time: 0.690s, 1484.08/s  (0.701s, 1459.99/s)  LR: 9.671e-04  Data: 0.010 (0.014)
Train: 70 [ 600/1251 ( 48%)]  Loss: 3.969 (4.00)  Time: 0.686s, 1492.55/s  (0.701s, 1460.51/s)  LR: 9.671e-04  Data: 0.011 (0.014)
Train: 70 [ 650/1251 ( 52%)]  Loss: 4.098 (4.01)  Time: 0.666s, 1537.84/s  (0.701s, 1461.42/s)  LR: 9.671e-04  Data: 0.010 (0.014)
Train: 70 [ 700/1251 ( 56%)]  Loss: 3.909 (4.00)  Time: 0.674s, 1519.56/s  (0.701s, 1461.52/s)  LR: 9.671e-04  Data: 0.011 (0.013)
Train: 70 [ 750/1251 ( 60%)]  Loss: 3.821 (3.99)  Time: 0.672s, 1523.56/s  (0.700s, 1462.85/s)  LR: 9.671e-04  Data: 0.011 (0.013)
Train: 70 [ 800/1251 ( 64%)]  Loss: 4.326 (4.01)  Time: 0.700s, 1463.68/s  (0.700s, 1463.81/s)  LR: 9.671e-04  Data: 0.010 (0.013)
Train: 70 [ 850/1251 ( 68%)]  Loss: 4.000 (4.01)  Time: 0.673s, 1521.57/s  (0.699s, 1464.65/s)  LR: 9.671e-04  Data: 0.010 (0.013)
Train: 70 [ 900/1251 ( 72%)]  Loss: 4.179 (4.02)  Time: 0.733s, 1397.30/s  (0.699s, 1464.95/s)  LR: 9.671e-04  Data: 0.010 (0.013)
Train: 70 [ 950/1251 ( 76%)]  Loss: 4.088 (4.02)  Time: 0.698s, 1467.93/s  (0.698s, 1466.13/s)  LR: 9.671e-04  Data: 0.009 (0.013)
Train: 70 [1000/1251 ( 80%)]  Loss: 3.699 (4.00)  Time: 0.692s, 1480.42/s  (0.698s, 1466.72/s)  LR: 9.671e-04  Data: 0.011 (0.012)
Train: 70 [1050/1251 ( 84%)]  Loss: 4.094 (4.01)  Time: 0.673s, 1521.10/s  (0.698s, 1467.55/s)  LR: 9.671e-04  Data: 0.010 (0.012)
Train: 70 [1100/1251 ( 88%)]  Loss: 3.610 (3.99)  Time: 0.673s, 1521.19/s  (0.697s, 1468.34/s)  LR: 9.671e-04  Data: 0.012 (0.012)
Train: 70 [1150/1251 ( 92%)]  Loss: 3.967 (3.99)  Time: 0.729s, 1404.23/s  (0.698s, 1467.91/s)  LR: 9.671e-04  Data: 0.009 (0.012)
Train: 70 [1200/1251 ( 96%)]  Loss: 4.245 (4.00)  Time: 0.672s, 1523.05/s  (0.697s, 1468.12/s)  LR: 9.671e-04  Data: 0.013 (0.012)
Train: 70 [1250/1251 (100%)]  Loss: 4.453 (4.02)  Time: 0.695s, 1473.28/s  (0.698s, 1467.75/s)  LR: 9.671e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.548 (1.548)  Loss:  0.9971 (0.9971)  Acc@1: 83.8867 (83.8867)  Acc@5: 96.0938 (96.0938)
Test: [  48/48]  Time: 0.136 (0.581)  Loss:  1.0908 (1.6727)  Acc@1: 81.6038 (67.7480)  Acc@5: 93.7500 (88.3460)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-69.pth.tar', 67.9180000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-62.pth.tar', 67.82000002441406)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-70.pth.tar', 67.74800002441407)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-63.pth.tar', 67.56600013183593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-68.pth.tar', 67.3379999584961)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-58.pth.tar', 67.33000010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-66.pth.tar', 67.32000001953125)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-53.pth.tar', 67.14599997314453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-56.pth.tar', 67.08800010253906)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-61.pth.tar', 66.9540000805664)

Train: 71 [   0/1251 (  0%)]  Loss: 3.761 (3.76)  Time: 2.353s,  435.22/s  (2.353s,  435.22/s)  LR: 9.662e-04  Data: 1.684 (1.684)
Train: 71 [  50/1251 (  4%)]  Loss: 4.083 (3.92)  Time: 0.671s, 1527.20/s  (0.738s, 1388.25/s)  LR: 9.662e-04  Data: 0.010 (0.053)
Train: 71 [ 100/1251 (  8%)]  Loss: 3.931 (3.92)  Time: 0.711s, 1440.09/s  (0.712s, 1439.07/s)  LR: 9.662e-04  Data: 0.009 (0.032)
Train: 71 [ 150/1251 ( 12%)]  Loss: 3.959 (3.93)  Time: 0.699s, 1464.76/s  (0.706s, 1450.73/s)  LR: 9.662e-04  Data: 0.011 (0.025)
Train: 71 [ 200/1251 ( 16%)]  Loss: 3.616 (3.87)  Time: 0.666s, 1537.08/s  (0.703s, 1457.04/s)  LR: 9.662e-04  Data: 0.010 (0.021)
Train: 71 [ 250/1251 ( 20%)]  Loss: 4.048 (3.90)  Time: 0.701s, 1461.62/s  (0.701s, 1461.45/s)  LR: 9.662e-04  Data: 0.010 (0.019)
Train: 71 [ 300/1251 ( 24%)]  Loss: 3.786 (3.88)  Time: 0.727s, 1409.15/s  (0.702s, 1459.30/s)  LR: 9.662e-04  Data: 0.010 (0.017)
Train: 71 [ 350/1251 ( 28%)]  Loss: 3.683 (3.86)  Time: 0.707s, 1449.21/s  (0.701s, 1461.51/s)  LR: 9.662e-04  Data: 0.009 (0.016)
Train: 71 [ 400/1251 ( 32%)]  Loss: 4.084 (3.88)  Time: 0.674s, 1520.06/s  (0.699s, 1465.15/s)  LR: 9.662e-04  Data: 0.010 (0.016)
Train: 71 [ 450/1251 ( 36%)]  Loss: 3.870 (3.88)  Time: 0.731s, 1401.52/s  (0.698s, 1467.83/s)  LR: 9.662e-04  Data: 0.010 (0.015)
Train: 71 [ 500/1251 ( 40%)]  Loss: 3.690 (3.86)  Time: 0.703s, 1455.73/s  (0.697s, 1468.26/s)  LR: 9.662e-04  Data: 0.010 (0.015)
Train: 71 [ 550/1251 ( 44%)]  Loss: 3.842 (3.86)  Time: 0.708s, 1446.95/s  (0.698s, 1467.38/s)  LR: 9.662e-04  Data: 0.009 (0.014)
Train: 71 [ 600/1251 ( 48%)]  Loss: 3.784 (3.86)  Time: 0.670s, 1528.44/s  (0.697s, 1468.58/s)  LR: 9.662e-04  Data: 0.010 (0.014)
Train: 71 [ 650/1251 ( 52%)]  Loss: 3.875 (3.86)  Time: 0.671s, 1526.94/s  (0.698s, 1466.97/s)  LR: 9.662e-04  Data: 0.010 (0.014)
Train: 71 [ 700/1251 ( 56%)]  Loss: 4.050 (3.87)  Time: 0.677s, 1511.89/s  (0.698s, 1467.02/s)  LR: 9.662e-04  Data: 0.010 (0.013)
Train: 71 [ 750/1251 ( 60%)]  Loss: 4.088 (3.88)  Time: 0.667s, 1536.07/s  (0.698s, 1467.31/s)  LR: 9.662e-04  Data: 0.010 (0.013)
Train: 71 [ 800/1251 ( 64%)]  Loss: 4.037 (3.89)  Time: 0.734s, 1396.05/s  (0.698s, 1466.82/s)  LR: 9.662e-04  Data: 0.010 (0.013)
Train: 71 [ 850/1251 ( 68%)]  Loss: 4.256 (3.91)  Time: 0.708s, 1446.31/s  (0.698s, 1467.59/s)  LR: 9.662e-04  Data: 0.011 (0.013)
Train: 71 [ 900/1251 ( 72%)]  Loss: 3.699 (3.90)  Time: 0.705s, 1453.44/s  (0.698s, 1468.05/s)  LR: 9.662e-04  Data: 0.009 (0.013)
Train: 71 [ 950/1251 ( 76%)]  Loss: 3.635 (3.89)  Time: 0.747s, 1370.36/s  (0.697s, 1468.36/s)  LR: 9.662e-04  Data: 0.009 (0.012)
Train: 71 [1000/1251 ( 80%)]  Loss: 3.913 (3.89)  Time: 0.686s, 1493.57/s  (0.698s, 1467.51/s)  LR: 9.662e-04  Data: 0.012 (0.012)
Train: 71 [1050/1251 ( 84%)]  Loss: 3.938 (3.89)  Time: 0.673s, 1521.83/s  (0.698s, 1468.05/s)  LR: 9.662e-04  Data: 0.010 (0.012)
Train: 71 [1100/1251 ( 88%)]  Loss: 4.245 (3.91)  Time: 0.672s, 1523.50/s  (0.698s, 1467.77/s)  LR: 9.662e-04  Data: 0.011 (0.012)
Train: 71 [1150/1251 ( 92%)]  Loss: 3.835 (3.90)  Time: 0.700s, 1463.00/s  (0.697s, 1468.57/s)  LR: 9.662e-04  Data: 0.009 (0.012)
Train: 71 [1200/1251 ( 96%)]  Loss: 4.176 (3.92)  Time: 0.691s, 1482.52/s  (0.697s, 1469.82/s)  LR: 9.662e-04  Data: 0.010 (0.012)
Train: 71 [1250/1251 (100%)]  Loss: 3.954 (3.92)  Time: 0.692s, 1478.91/s  (0.697s, 1469.80/s)  LR: 9.662e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.605 (1.605)  Loss:  0.8906 (0.8906)  Acc@1: 85.4492 (85.4492)  Acc@5: 95.6055 (95.6055)
Test: [  48/48]  Time: 0.136 (0.591)  Loss:  1.0420 (1.5898)  Acc@1: 80.1887 (67.8560)  Acc@5: 93.8679 (88.4580)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-69.pth.tar', 67.9180000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-71.pth.tar', 67.85599992675782)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-62.pth.tar', 67.82000002441406)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-70.pth.tar', 67.74800002441407)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-63.pth.tar', 67.56600013183593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-68.pth.tar', 67.3379999584961)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-58.pth.tar', 67.33000010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-66.pth.tar', 67.32000001953125)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-53.pth.tar', 67.14599997314453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-56.pth.tar', 67.08800010253906)

Train: 72 [   0/1251 (  0%)]  Loss: 3.887 (3.89)  Time: 2.097s,  488.28/s  (2.097s,  488.28/s)  LR: 9.652e-04  Data: 1.486 (1.486)
Train: 72 [  50/1251 (  4%)]  Loss: 4.268 (4.08)  Time: 0.674s, 1519.08/s  (0.733s, 1397.57/s)  LR: 9.652e-04  Data: 0.010 (0.048)
Train: 72 [ 100/1251 (  8%)]  Loss: 3.879 (4.01)  Time: 0.672s, 1524.63/s  (0.712s, 1438.76/s)  LR: 9.652e-04  Data: 0.010 (0.030)
Train: 72 [ 150/1251 ( 12%)]  Loss: 3.923 (3.99)  Time: 0.729s, 1405.01/s  (0.705s, 1453.30/s)  LR: 9.652e-04  Data: 0.014 (0.023)
Train: 72 [ 200/1251 ( 16%)]  Loss: 4.195 (4.03)  Time: 0.722s, 1418.36/s  (0.704s, 1454.54/s)  LR: 9.652e-04  Data: 0.009 (0.020)
Train: 72 [ 250/1251 ( 20%)]  Loss: 3.627 (3.96)  Time: 0.707s, 1448.31/s  (0.701s, 1460.19/s)  LR: 9.652e-04  Data: 0.010 (0.018)
Train: 72 [ 300/1251 ( 24%)]  Loss: 4.092 (3.98)  Time: 0.719s, 1424.98/s  (0.700s, 1462.62/s)  LR: 9.652e-04  Data: 0.011 (0.017)
Train: 72 [ 350/1251 ( 28%)]  Loss: 4.031 (3.99)  Time: 0.672s, 1523.62/s  (0.700s, 1462.97/s)  LR: 9.652e-04  Data: 0.010 (0.016)
Train: 72 [ 400/1251 ( 32%)]  Loss: 4.133 (4.00)  Time: 0.702s, 1458.08/s  (0.699s, 1465.11/s)  LR: 9.652e-04  Data: 0.010 (0.015)
Train: 72 [ 450/1251 ( 36%)]  Loss: 4.137 (4.02)  Time: 0.693s, 1478.60/s  (0.699s, 1464.73/s)  LR: 9.652e-04  Data: 0.014 (0.015)
Train: 72 [ 500/1251 ( 40%)]  Loss: 4.536 (4.06)  Time: 0.708s, 1446.80/s  (0.699s, 1465.81/s)  LR: 9.652e-04  Data: 0.011 (0.014)
Train: 72 [ 550/1251 ( 44%)]  Loss: 3.833 (4.05)  Time: 0.715s, 1432.40/s  (0.699s, 1464.40/s)  LR: 9.652e-04  Data: 0.010 (0.014)
Train: 72 [ 600/1251 ( 48%)]  Loss: 4.310 (4.07)  Time: 0.669s, 1530.74/s  (0.699s, 1465.86/s)  LR: 9.652e-04  Data: 0.011 (0.014)
Train: 72 [ 650/1251 ( 52%)]  Loss: 4.070 (4.07)  Time: 0.719s, 1425.05/s  (0.698s, 1467.10/s)  LR: 9.652e-04  Data: 0.011 (0.013)
Train: 72 [ 700/1251 ( 56%)]  Loss: 3.873 (4.05)  Time: 0.747s, 1370.54/s  (0.698s, 1466.87/s)  LR: 9.652e-04  Data: 0.010 (0.013)
Train: 72 [ 750/1251 ( 60%)]  Loss: 3.681 (4.03)  Time: 0.683s, 1498.29/s  (0.698s, 1467.42/s)  LR: 9.652e-04  Data: 0.012 (0.013)
Train: 72 [ 800/1251 ( 64%)]  Loss: 4.229 (4.04)  Time: 0.676s, 1515.08/s  (0.698s, 1467.82/s)  LR: 9.652e-04  Data: 0.011 (0.013)
Train: 72 [ 850/1251 ( 68%)]  Loss: 3.650 (4.02)  Time: 0.689s, 1485.50/s  (0.697s, 1468.17/s)  LR: 9.652e-04  Data: 0.011 (0.013)
Train: 72 [ 900/1251 ( 72%)]  Loss: 3.930 (4.01)  Time: 0.707s, 1449.12/s  (0.698s, 1468.03/s)  LR: 9.652e-04  Data: 0.009 (0.013)
Train: 72 [ 950/1251 ( 76%)]  Loss: 3.983 (4.01)  Time: 0.711s, 1440.22/s  (0.697s, 1469.18/s)  LR: 9.652e-04  Data: 0.009 (0.012)
Train: 72 [1000/1251 ( 80%)]  Loss: 3.917 (4.01)  Time: 0.684s, 1496.79/s  (0.697s, 1469.73/s)  LR: 9.652e-04  Data: 0.010 (0.012)
Train: 72 [1050/1251 ( 84%)]  Loss: 3.928 (4.01)  Time: 0.673s, 1521.64/s  (0.696s, 1470.37/s)  LR: 9.652e-04  Data: 0.010 (0.012)
Train: 72 [1100/1251 ( 88%)]  Loss: 3.847 (4.00)  Time: 0.704s, 1454.27/s  (0.696s, 1470.77/s)  LR: 9.652e-04  Data: 0.011 (0.012)
Train: 72 [1150/1251 ( 92%)]  Loss: 3.497 (3.98)  Time: 0.695s, 1472.81/s  (0.696s, 1470.69/s)  LR: 9.652e-04  Data: 0.010 (0.012)
Train: 72 [1200/1251 ( 96%)]  Loss: 4.225 (3.99)  Time: 0.694s, 1475.17/s  (0.697s, 1470.01/s)  LR: 9.652e-04  Data: 0.010 (0.012)
Train: 72 [1250/1251 (100%)]  Loss: 4.280 (4.00)  Time: 0.692s, 1480.65/s  (0.696s, 1470.40/s)  LR: 9.652e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.478 (1.478)  Loss:  0.9346 (0.9346)  Acc@1: 87.1094 (87.1094)  Acc@5: 95.8984 (95.8984)
Test: [  48/48]  Time: 0.136 (0.581)  Loss:  1.0420 (1.6554)  Acc@1: 81.6038 (67.7120)  Acc@5: 94.6934 (88.3460)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-69.pth.tar', 67.9180000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-71.pth.tar', 67.85599992675782)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-62.pth.tar', 67.82000002441406)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-70.pth.tar', 67.74800002441407)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-72.pth.tar', 67.71200002441407)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-63.pth.tar', 67.56600013183593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-68.pth.tar', 67.3379999584961)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-58.pth.tar', 67.33000010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-66.pth.tar', 67.32000001953125)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-53.pth.tar', 67.14599997314453)

Train: 73 [   0/1251 (  0%)]  Loss: 4.025 (4.02)  Time: 2.223s,  460.69/s  (2.223s,  460.69/s)  LR: 9.643e-04  Data: 1.606 (1.606)
Train: 73 [  50/1251 (  4%)]  Loss: 3.863 (3.94)  Time: 0.699s, 1465.14/s  (0.734s, 1394.61/s)  LR: 9.643e-04  Data: 0.009 (0.049)
Train: 73 [ 100/1251 (  8%)]  Loss: 3.895 (3.93)  Time: 0.705s, 1452.26/s  (0.716s, 1430.94/s)  LR: 9.643e-04  Data: 0.010 (0.030)
Train: 73 [ 150/1251 ( 12%)]  Loss: 4.206 (4.00)  Time: 0.682s, 1501.89/s  (0.708s, 1446.60/s)  LR: 9.643e-04  Data: 0.013 (0.023)
Train: 73 [ 200/1251 ( 16%)]  Loss: 3.997 (4.00)  Time: 0.674s, 1519.89/s  (0.706s, 1450.04/s)  LR: 9.643e-04  Data: 0.012 (0.020)
Train: 73 [ 250/1251 ( 20%)]  Loss: 4.466 (4.08)  Time: 0.709s, 1444.95/s  (0.703s, 1456.99/s)  LR: 9.643e-04  Data: 0.011 (0.018)
Train: 73 [ 300/1251 ( 24%)]  Loss: 3.786 (4.03)  Time: 0.758s, 1350.24/s  (0.703s, 1455.87/s)  LR: 9.643e-04  Data: 0.010 (0.017)
Train: 73 [ 350/1251 ( 28%)]  Loss: 4.042 (4.04)  Time: 0.707s, 1448.15/s  (0.702s, 1458.61/s)  LR: 9.643e-04  Data: 0.010 (0.016)
Train: 73 [ 400/1251 ( 32%)]  Loss: 3.957 (4.03)  Time: 0.705s, 1452.12/s  (0.702s, 1459.61/s)  LR: 9.643e-04  Data: 0.010 (0.015)
Train: 73 [ 450/1251 ( 36%)]  Loss: 4.124 (4.04)  Time: 0.670s, 1527.85/s  (0.701s, 1461.15/s)  LR: 9.643e-04  Data: 0.009 (0.015)
Train: 73 [ 500/1251 ( 40%)]  Loss: 4.013 (4.03)  Time: 0.677s, 1512.12/s  (0.700s, 1462.52/s)  LR: 9.643e-04  Data: 0.010 (0.014)
Train: 73 [ 550/1251 ( 44%)]  Loss: 3.830 (4.02)  Time: 0.670s, 1527.51/s  (0.700s, 1462.71/s)  LR: 9.643e-04  Data: 0.011 (0.014)
Train: 73 [ 600/1251 ( 48%)]  Loss: 3.934 (4.01)  Time: 0.677s, 1512.88/s  (0.699s, 1464.48/s)  LR: 9.643e-04  Data: 0.012 (0.014)
Train: 73 [ 650/1251 ( 52%)]  Loss: 3.950 (4.01)  Time: 0.706s, 1450.14/s  (0.700s, 1463.61/s)  LR: 9.643e-04  Data: 0.014 (0.013)
Train: 73 [ 700/1251 ( 56%)]  Loss: 4.195 (4.02)  Time: 0.686s, 1493.69/s  (0.700s, 1463.02/s)  LR: 9.643e-04  Data: 0.015 (0.013)
Train: 73 [ 750/1251 ( 60%)]  Loss: 3.969 (4.02)  Time: 0.704s, 1454.70/s  (0.700s, 1462.81/s)  LR: 9.643e-04  Data: 0.010 (0.013)
Train: 73 [ 800/1251 ( 64%)]  Loss: 4.078 (4.02)  Time: 0.671s, 1526.10/s  (0.699s, 1464.10/s)  LR: 9.643e-04  Data: 0.010 (0.013)
Train: 73 [ 850/1251 ( 68%)]  Loss: 3.778 (4.01)  Time: 0.680s, 1506.03/s  (0.699s, 1465.57/s)  LR: 9.643e-04  Data: 0.010 (0.013)
Train: 73 [ 900/1251 ( 72%)]  Loss: 4.122 (4.01)  Time: 0.775s, 1321.45/s  (0.699s, 1465.16/s)  LR: 9.643e-04  Data: 0.013 (0.013)
Train: 73 [ 950/1251 ( 76%)]  Loss: 4.071 (4.02)  Time: 0.718s, 1425.79/s  (0.699s, 1465.06/s)  LR: 9.643e-04  Data: 0.009 (0.012)
Train: 73 [1000/1251 ( 80%)]  Loss: 3.766 (4.00)  Time: 0.701s, 1461.48/s  (0.699s, 1464.59/s)  LR: 9.643e-04  Data: 0.009 (0.012)
Train: 73 [1050/1251 ( 84%)]  Loss: 4.207 (4.01)  Time: 0.671s, 1526.59/s  (0.699s, 1464.32/s)  LR: 9.643e-04  Data: 0.012 (0.012)
Train: 73 [1100/1251 ( 88%)]  Loss: 4.013 (4.01)  Time: 0.670s, 1528.79/s  (0.699s, 1465.21/s)  LR: 9.643e-04  Data: 0.010 (0.012)
Train: 73 [1150/1251 ( 92%)]  Loss: 3.975 (4.01)  Time: 0.718s, 1425.59/s  (0.699s, 1465.30/s)  LR: 9.643e-04  Data: 0.009 (0.012)
Train: 73 [1200/1251 ( 96%)]  Loss: 3.799 (4.00)  Time: 0.699s, 1465.18/s  (0.698s, 1466.05/s)  LR: 9.643e-04  Data: 0.010 (0.012)
Train: 73 [1250/1251 (100%)]  Loss: 4.031 (4.00)  Time: 0.660s, 1551.15/s  (0.698s, 1466.66/s)  LR: 9.643e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.511 (1.511)  Loss:  1.0156 (1.0156)  Acc@1: 83.8867 (83.8867)  Acc@5: 95.2148 (95.2148)
Test: [  48/48]  Time: 0.136 (0.587)  Loss:  1.1104 (1.6546)  Acc@1: 79.8349 (66.6520)  Acc@5: 94.4575 (87.9180)
Train: 74 [   0/1251 (  0%)]  Loss: 4.054 (4.05)  Time: 2.222s,  460.87/s  (2.222s,  460.87/s)  LR: 9.633e-04  Data: 1.555 (1.555)
Train: 74 [  50/1251 (  4%)]  Loss: 3.761 (3.91)  Time: 0.674s, 1519.06/s  (0.731s, 1401.61/s)  LR: 9.633e-04  Data: 0.014 (0.048)
Train: 74 [ 100/1251 (  8%)]  Loss: 3.725 (3.85)  Time: 0.667s, 1535.39/s  (0.710s, 1441.88/s)  LR: 9.633e-04  Data: 0.011 (0.029)
Train: 74 [ 150/1251 ( 12%)]  Loss: 3.806 (3.84)  Time: 0.708s, 1446.72/s  (0.702s, 1458.38/s)  LR: 9.633e-04  Data: 0.010 (0.023)
Train: 74 [ 200/1251 ( 16%)]  Loss: 3.679 (3.80)  Time: 0.705s, 1452.28/s  (0.702s, 1458.36/s)  LR: 9.633e-04  Data: 0.010 (0.020)
Train: 74 [ 250/1251 ( 20%)]  Loss: 3.621 (3.77)  Time: 0.705s, 1452.20/s  (0.700s, 1463.09/s)  LR: 9.633e-04  Data: 0.010 (0.018)
Train: 74 [ 300/1251 ( 24%)]  Loss: 3.542 (3.74)  Time: 0.692s, 1480.03/s  (0.699s, 1465.48/s)  LR: 9.633e-04  Data: 0.011 (0.016)
Train: 74 [ 350/1251 ( 28%)]  Loss: 4.023 (3.78)  Time: 0.682s, 1501.76/s  (0.698s, 1467.91/s)  LR: 9.633e-04  Data: 0.015 (0.016)
Train: 74 [ 400/1251 ( 32%)]  Loss: 4.097 (3.81)  Time: 0.677s, 1512.63/s  (0.697s, 1468.42/s)  LR: 9.633e-04  Data: 0.009 (0.015)
Train: 74 [ 450/1251 ( 36%)]  Loss: 3.783 (3.81)  Time: 0.706s, 1449.80/s  (0.697s, 1468.68/s)  LR: 9.633e-04  Data: 0.011 (0.014)
Train: 74 [ 500/1251 ( 40%)]  Loss: 3.842 (3.81)  Time: 0.693s, 1476.62/s  (0.697s, 1468.83/s)  LR: 9.633e-04  Data: 0.011 (0.014)
Train: 74 [ 550/1251 ( 44%)]  Loss: 4.247 (3.85)  Time: 0.674s, 1519.69/s  (0.697s, 1468.15/s)  LR: 9.633e-04  Data: 0.011 (0.014)
Train: 74 [ 600/1251 ( 48%)]  Loss: 4.026 (3.86)  Time: 0.691s, 1481.67/s  (0.697s, 1469.02/s)  LR: 9.633e-04  Data: 0.011 (0.013)
Train: 74 [ 650/1251 ( 52%)]  Loss: 3.804 (3.86)  Time: 0.674s, 1520.17/s  (0.696s, 1471.31/s)  LR: 9.633e-04  Data: 0.010 (0.013)
Train: 74 [ 700/1251 ( 56%)]  Loss: 4.087 (3.87)  Time: 0.771s, 1328.24/s  (0.696s, 1471.86/s)  LR: 9.633e-04  Data: 0.010 (0.013)
Train: 74 [ 750/1251 ( 60%)]  Loss: 4.377 (3.90)  Time: 0.668s, 1533.84/s  (0.696s, 1472.24/s)  LR: 9.633e-04  Data: 0.009 (0.013)
Train: 74 [ 800/1251 ( 64%)]  Loss: 3.952 (3.91)  Time: 0.673s, 1520.63/s  (0.695s, 1472.87/s)  LR: 9.633e-04  Data: 0.010 (0.012)
Train: 74 [ 850/1251 ( 68%)]  Loss: 4.088 (3.92)  Time: 0.673s, 1520.64/s  (0.696s, 1471.12/s)  LR: 9.633e-04  Data: 0.012 (0.012)
Train: 74 [ 900/1251 ( 72%)]  Loss: 3.851 (3.91)  Time: 0.674s, 1518.57/s  (0.696s, 1471.88/s)  LR: 9.633e-04  Data: 0.010 (0.012)
Train: 74 [ 950/1251 ( 76%)]  Loss: 3.875 (3.91)  Time: 0.707s, 1447.78/s  (0.696s, 1472.01/s)  LR: 9.633e-04  Data: 0.011 (0.012)
Train: 74 [1000/1251 ( 80%)]  Loss: 4.108 (3.92)  Time: 0.741s, 1382.74/s  (0.695s, 1472.61/s)  LR: 9.633e-04  Data: 0.010 (0.012)
Train: 74 [1050/1251 ( 84%)]  Loss: 4.540 (3.95)  Time: 0.706s, 1450.66/s  (0.695s, 1472.70/s)  LR: 9.633e-04  Data: 0.010 (0.012)
Train: 74 [1100/1251 ( 88%)]  Loss: 3.772 (3.94)  Time: 0.674s, 1520.12/s  (0.695s, 1472.85/s)  LR: 9.633e-04  Data: 0.011 (0.012)
Train: 74 [1150/1251 ( 92%)]  Loss: 3.885 (3.94)  Time: 0.676s, 1515.86/s  (0.695s, 1472.90/s)  LR: 9.633e-04  Data: 0.011 (0.012)
Train: 74 [1200/1251 ( 96%)]  Loss: 3.964 (3.94)  Time: 0.665s, 1539.66/s  (0.695s, 1472.53/s)  LR: 9.633e-04  Data: 0.009 (0.012)
Train: 74 [1250/1251 (100%)]  Loss: 4.196 (3.95)  Time: 0.655s, 1562.20/s  (0.695s, 1472.48/s)  LR: 9.633e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.499 (1.499)  Loss:  0.9902 (0.9902)  Acc@1: 84.1797 (84.1797)  Acc@5: 95.1172 (95.1172)
Test: [  48/48]  Time: 0.136 (0.579)  Loss:  1.0498 (1.5894)  Acc@1: 82.0755 (68.1100)  Acc@5: 94.8113 (88.2280)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-74.pth.tar', 68.11000010009765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-69.pth.tar', 67.9180000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-71.pth.tar', 67.85599992675782)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-62.pth.tar', 67.82000002441406)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-70.pth.tar', 67.74800002441407)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-72.pth.tar', 67.71200002441407)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-63.pth.tar', 67.56600013183593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-68.pth.tar', 67.3379999584961)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-58.pth.tar', 67.33000010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-66.pth.tar', 67.32000001953125)

Train: 75 [   0/1251 (  0%)]  Loss: 4.335 (4.33)  Time: 2.278s,  449.56/s  (2.278s,  449.56/s)  LR: 9.623e-04  Data: 1.663 (1.663)
Train: 75 [  50/1251 (  4%)]  Loss: 3.852 (4.09)  Time: 0.682s, 1502.30/s  (0.737s, 1389.71/s)  LR: 9.623e-04  Data: 0.010 (0.050)
Train: 75 [ 100/1251 (  8%)]  Loss: 3.994 (4.06)  Time: 0.730s, 1403.66/s  (0.717s, 1428.49/s)  LR: 9.623e-04  Data: 0.010 (0.030)
Train: 75 [ 150/1251 ( 12%)]  Loss: 4.184 (4.09)  Time: 0.713s, 1436.66/s  (0.709s, 1443.36/s)  LR: 9.623e-04  Data: 0.010 (0.024)
Train: 75 [ 200/1251 ( 16%)]  Loss: 3.839 (4.04)  Time: 0.672s, 1523.82/s  (0.704s, 1454.27/s)  LR: 9.623e-04  Data: 0.009 (0.020)
Train: 75 [ 250/1251 ( 20%)]  Loss: 4.089 (4.05)  Time: 0.705s, 1452.26/s  (0.701s, 1459.82/s)  LR: 9.623e-04  Data: 0.010 (0.018)
Train: 75 [ 300/1251 ( 24%)]  Loss: 3.779 (4.01)  Time: 0.706s, 1451.41/s  (0.700s, 1461.85/s)  LR: 9.623e-04  Data: 0.009 (0.017)
Train: 75 [ 350/1251 ( 28%)]  Loss: 3.705 (3.97)  Time: 0.697s, 1468.56/s  (0.699s, 1464.30/s)  LR: 9.623e-04  Data: 0.010 (0.016)
Train: 75 [ 400/1251 ( 32%)]  Loss: 4.019 (3.98)  Time: 0.705s, 1452.68/s  (0.699s, 1463.90/s)  LR: 9.623e-04  Data: 0.011 (0.015)
Train: 75 [ 450/1251 ( 36%)]  Loss: 4.143 (3.99)  Time: 0.675s, 1517.67/s  (0.699s, 1465.40/s)  LR: 9.623e-04  Data: 0.010 (0.015)
Train: 75 [ 500/1251 ( 40%)]  Loss: 4.062 (4.00)  Time: 0.690s, 1484.46/s  (0.698s, 1467.64/s)  LR: 9.623e-04  Data: 0.009 (0.014)
Train: 75 [ 550/1251 ( 44%)]  Loss: 4.125 (4.01)  Time: 0.672s, 1522.78/s  (0.697s, 1468.22/s)  LR: 9.623e-04  Data: 0.012 (0.014)
Train: 75 [ 600/1251 ( 48%)]  Loss: 4.101 (4.02)  Time: 0.671s, 1526.77/s  (0.698s, 1467.44/s)  LR: 9.623e-04  Data: 0.013 (0.014)
Train: 75 [ 650/1251 ( 52%)]  Loss: 3.841 (4.00)  Time: 0.707s, 1447.66/s  (0.698s, 1467.70/s)  LR: 9.623e-04  Data: 0.009 (0.013)
Train: 75 [ 700/1251 ( 56%)]  Loss: 3.656 (3.98)  Time: 0.674s, 1518.46/s  (0.698s, 1467.70/s)  LR: 9.623e-04  Data: 0.010 (0.013)
Train: 75 [ 750/1251 ( 60%)]  Loss: 3.843 (3.97)  Time: 0.710s, 1441.97/s  (0.698s, 1467.56/s)  LR: 9.623e-04  Data: 0.010 (0.013)
Train: 75 [ 800/1251 ( 64%)]  Loss: 3.419 (3.94)  Time: 0.710s, 1442.58/s  (0.698s, 1467.75/s)  LR: 9.623e-04  Data: 0.009 (0.013)
Train: 75 [ 850/1251 ( 68%)]  Loss: 4.193 (3.95)  Time: 0.685s, 1495.08/s  (0.697s, 1469.27/s)  LR: 9.623e-04  Data: 0.011 (0.013)
Train: 75 [ 900/1251 ( 72%)]  Loss: 4.013 (3.96)  Time: 0.675s, 1517.25/s  (0.697s, 1469.71/s)  LR: 9.623e-04  Data: 0.009 (0.013)
Train: 75 [ 950/1251 ( 76%)]  Loss: 3.995 (3.96)  Time: 0.671s, 1526.20/s  (0.697s, 1468.90/s)  LR: 9.623e-04  Data: 0.009 (0.012)
Train: 75 [1000/1251 ( 80%)]  Loss: 3.986 (3.96)  Time: 0.677s, 1512.59/s  (0.697s, 1468.66/s)  LR: 9.623e-04  Data: 0.011 (0.012)
Train: 75 [1050/1251 ( 84%)]  Loss: 4.089 (3.97)  Time: 0.685s, 1494.33/s  (0.697s, 1469.08/s)  LR: 9.623e-04  Data: 0.009 (0.012)
Train: 75 [1100/1251 ( 88%)]  Loss: 4.436 (3.99)  Time: 0.684s, 1498.07/s  (0.697s, 1468.95/s)  LR: 9.623e-04  Data: 0.009 (0.012)
Train: 75 [1150/1251 ( 92%)]  Loss: 4.172 (3.99)  Time: 0.689s, 1486.77/s  (0.697s, 1469.56/s)  LR: 9.623e-04  Data: 0.009 (0.012)
Train: 75 [1200/1251 ( 96%)]  Loss: 4.154 (4.00)  Time: 0.725s, 1411.87/s  (0.697s, 1469.78/s)  LR: 9.623e-04  Data: 0.010 (0.012)
Train: 75 [1250/1251 (100%)]  Loss: 4.214 (4.01)  Time: 0.657s, 1558.02/s  (0.696s, 1470.45/s)  LR: 9.623e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.525 (1.525)  Loss:  0.8940 (0.8940)  Acc@1: 85.7422 (85.7422)  Acc@5: 95.2148 (95.2148)
Test: [  48/48]  Time: 0.136 (0.579)  Loss:  1.0264 (1.6085)  Acc@1: 80.8962 (68.3060)  Acc@5: 93.7500 (88.5900)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-75.pth.tar', 68.30599997558593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-74.pth.tar', 68.11000010009765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-69.pth.tar', 67.9180000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-71.pth.tar', 67.85599992675782)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-62.pth.tar', 67.82000002441406)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-70.pth.tar', 67.74800002441407)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-72.pth.tar', 67.71200002441407)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-63.pth.tar', 67.56600013183593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-68.pth.tar', 67.3379999584961)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-58.pth.tar', 67.33000010498047)

Train: 76 [   0/1251 (  0%)]  Loss: 3.845 (3.84)  Time: 2.455s,  417.18/s  (2.455s,  417.18/s)  LR: 9.613e-04  Data: 1.820 (1.820)
Train: 76 [  50/1251 (  4%)]  Loss: 4.010 (3.93)  Time: 0.682s, 1500.79/s  (0.725s, 1412.57/s)  LR: 9.613e-04  Data: 0.013 (0.046)
Train: 76 [ 100/1251 (  8%)]  Loss: 4.363 (4.07)  Time: 0.712s, 1437.52/s  (0.713s, 1436.46/s)  LR: 9.613e-04  Data: 0.010 (0.029)
Train: 76 [ 150/1251 ( 12%)]  Loss: 3.933 (4.04)  Time: 0.676s, 1515.72/s  (0.709s, 1443.91/s)  LR: 9.613e-04  Data: 0.010 (0.023)
Train: 76 [ 200/1251 ( 16%)]  Loss: 4.011 (4.03)  Time: 0.668s, 1533.23/s  (0.705s, 1451.66/s)  LR: 9.613e-04  Data: 0.010 (0.020)
Train: 76 [ 250/1251 ( 20%)]  Loss: 3.666 (3.97)  Time: 0.676s, 1515.21/s  (0.703s, 1455.63/s)  LR: 9.613e-04  Data: 0.013 (0.018)
Train: 76 [ 300/1251 ( 24%)]  Loss: 3.774 (3.94)  Time: 0.714s, 1434.02/s  (0.701s, 1459.85/s)  LR: 9.613e-04  Data: 0.011 (0.017)
Train: 76 [ 350/1251 ( 28%)]  Loss: 3.808 (3.93)  Time: 0.728s, 1406.19/s  (0.701s, 1460.44/s)  LR: 9.613e-04  Data: 0.012 (0.016)
Train: 76 [ 400/1251 ( 32%)]  Loss: 3.766 (3.91)  Time: 0.706s, 1451.24/s  (0.701s, 1461.35/s)  LR: 9.613e-04  Data: 0.010 (0.015)
Train: 76 [ 450/1251 ( 36%)]  Loss: 3.863 (3.90)  Time: 0.718s, 1426.95/s  (0.700s, 1461.91/s)  LR: 9.613e-04  Data: 0.009 (0.015)
Train: 76 [ 500/1251 ( 40%)]  Loss: 3.739 (3.89)  Time: 0.697s, 1468.22/s  (0.702s, 1459.49/s)  LR: 9.613e-04  Data: 0.013 (0.014)
Train: 76 [ 550/1251 ( 44%)]  Loss: 3.782 (3.88)  Time: 0.722s, 1419.18/s  (0.701s, 1460.17/s)  LR: 9.613e-04  Data: 0.010 (0.014)
Train: 76 [ 600/1251 ( 48%)]  Loss: 4.027 (3.89)  Time: 0.708s, 1446.56/s  (0.700s, 1462.12/s)  LR: 9.613e-04  Data: 0.010 (0.014)
Train: 76 [ 650/1251 ( 52%)]  Loss: 3.979 (3.90)  Time: 0.703s, 1456.28/s  (0.700s, 1463.83/s)  LR: 9.613e-04  Data: 0.010 (0.013)
Train: 76 [ 700/1251 ( 56%)]  Loss: 4.045 (3.91)  Time: 0.673s, 1522.36/s  (0.699s, 1464.89/s)  LR: 9.613e-04  Data: 0.010 (0.013)
Train: 76 [ 750/1251 ( 60%)]  Loss: 4.181 (3.92)  Time: 0.708s, 1446.16/s  (0.698s, 1466.28/s)  LR: 9.613e-04  Data: 0.010 (0.013)
Train: 76 [ 800/1251 ( 64%)]  Loss: 4.080 (3.93)  Time: 0.700s, 1462.97/s  (0.699s, 1465.67/s)  LR: 9.613e-04  Data: 0.010 (0.013)
Train: 76 [ 850/1251 ( 68%)]  Loss: 3.391 (3.90)  Time: 0.723s, 1416.53/s  (0.699s, 1465.92/s)  LR: 9.613e-04  Data: 0.010 (0.013)
Train: 76 [ 900/1251 ( 72%)]  Loss: 3.739 (3.89)  Time: 0.672s, 1523.55/s  (0.698s, 1466.79/s)  LR: 9.613e-04  Data: 0.013 (0.013)
Train: 76 [ 950/1251 ( 76%)]  Loss: 3.880 (3.89)  Time: 0.717s, 1427.46/s  (0.698s, 1467.61/s)  LR: 9.613e-04  Data: 0.010 (0.012)
Train: 76 [1000/1251 ( 80%)]  Loss: 4.209 (3.91)  Time: 0.678s, 1510.28/s  (0.697s, 1468.48/s)  LR: 9.613e-04  Data: 0.014 (0.012)
Train: 76 [1050/1251 ( 84%)]  Loss: 3.965 (3.91)  Time: 0.742s, 1380.69/s  (0.697s, 1468.61/s)  LR: 9.613e-04  Data: 0.013 (0.012)
Train: 76 [1100/1251 ( 88%)]  Loss: 4.411 (3.93)  Time: 0.723s, 1416.72/s  (0.697s, 1468.71/s)  LR: 9.613e-04  Data: 0.010 (0.012)
Train: 76 [1150/1251 ( 92%)]  Loss: 3.573 (3.92)  Time: 0.750s, 1366.12/s  (0.697s, 1469.00/s)  LR: 9.613e-04  Data: 0.012 (0.012)
Train: 76 [1200/1251 ( 96%)]  Loss: 4.228 (3.93)  Time: 0.717s, 1428.48/s  (0.697s, 1469.36/s)  LR: 9.613e-04  Data: 0.009 (0.012)
Train: 76 [1250/1251 (100%)]  Loss: 3.584 (3.92)  Time: 0.654s, 1565.67/s  (0.697s, 1469.73/s)  LR: 9.613e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.568 (1.568)  Loss:  1.0195 (1.0195)  Acc@1: 84.9609 (84.9609)  Acc@5: 94.6289 (94.6289)
Test: [  48/48]  Time: 0.138 (0.583)  Loss:  0.9497 (1.5217)  Acc@1: 80.3066 (68.4940)  Acc@5: 94.5755 (88.8900)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-76.pth.tar', 68.49400010742187)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-75.pth.tar', 68.30599997558593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-74.pth.tar', 68.11000010009765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-69.pth.tar', 67.9180000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-71.pth.tar', 67.85599992675782)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-62.pth.tar', 67.82000002441406)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-70.pth.tar', 67.74800002441407)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-72.pth.tar', 67.71200002441407)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-63.pth.tar', 67.56600013183593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-68.pth.tar', 67.3379999584961)

Train: 77 [   0/1251 (  0%)]  Loss: 3.684 (3.68)  Time: 2.339s,  437.81/s  (2.339s,  437.81/s)  LR: 9.603e-04  Data: 1.723 (1.723)
Train: 77 [  50/1251 (  4%)]  Loss: 3.684 (3.68)  Time: 0.672s, 1524.38/s  (0.733s, 1396.57/s)  LR: 9.603e-04  Data: 0.011 (0.051)
Train: 77 [ 100/1251 (  8%)]  Loss: 3.942 (3.77)  Time: 0.670s, 1527.93/s  (0.717s, 1428.73/s)  LR: 9.603e-04  Data: 0.009 (0.031)
Train: 77 [ 150/1251 ( 12%)]  Loss: 3.679 (3.75)  Time: 0.666s, 1536.99/s  (0.708s, 1445.97/s)  LR: 9.603e-04  Data: 0.011 (0.024)
Train: 77 [ 200/1251 ( 16%)]  Loss: 4.060 (3.81)  Time: 0.714s, 1434.15/s  (0.705s, 1452.65/s)  LR: 9.603e-04  Data: 0.012 (0.021)
Train: 77 [ 250/1251 ( 20%)]  Loss: 3.864 (3.82)  Time: 0.707s, 1447.98/s  (0.703s, 1455.91/s)  LR: 9.603e-04  Data: 0.010 (0.019)
Train: 77 [ 300/1251 ( 24%)]  Loss: 3.648 (3.79)  Time: 0.691s, 1482.20/s  (0.702s, 1458.15/s)  LR: 9.603e-04  Data: 0.010 (0.017)
Train: 77 [ 350/1251 ( 28%)]  Loss: 4.203 (3.85)  Time: 0.707s, 1448.39/s  (0.701s, 1460.20/s)  LR: 9.603e-04  Data: 0.010 (0.016)
Train: 77 [ 400/1251 ( 32%)]  Loss: 3.929 (3.85)  Time: 0.727s, 1408.92/s  (0.699s, 1463.99/s)  LR: 9.603e-04  Data: 0.010 (0.016)
Train: 77 [ 450/1251 ( 36%)]  Loss: 3.951 (3.86)  Time: 0.693s, 1476.87/s  (0.699s, 1465.17/s)  LR: 9.603e-04  Data: 0.013 (0.015)
Train: 77 [ 500/1251 ( 40%)]  Loss: 3.657 (3.85)  Time: 0.706s, 1450.37/s  (0.698s, 1466.45/s)  LR: 9.603e-04  Data: 0.011 (0.015)
Train: 77 [ 550/1251 ( 44%)]  Loss: 3.731 (3.84)  Time: 0.704s, 1454.84/s  (0.698s, 1466.86/s)  LR: 9.603e-04  Data: 0.011 (0.014)
Train: 77 [ 600/1251 ( 48%)]  Loss: 4.269 (3.87)  Time: 0.673s, 1522.63/s  (0.698s, 1467.93/s)  LR: 9.603e-04  Data: 0.009 (0.014)
Train: 77 [ 650/1251 ( 52%)]  Loss: 3.979 (3.88)  Time: 0.671s, 1526.51/s  (0.698s, 1466.99/s)  LR: 9.603e-04  Data: 0.009 (0.014)
Train: 77 [ 700/1251 ( 56%)]  Loss: 4.027 (3.89)  Time: 0.672s, 1523.48/s  (0.698s, 1466.81/s)  LR: 9.603e-04  Data: 0.009 (0.013)
Train: 77 [ 750/1251 ( 60%)]  Loss: 3.738 (3.88)  Time: 0.677s, 1512.98/s  (0.698s, 1466.78/s)  LR: 9.603e-04  Data: 0.010 (0.013)
Train: 77 [ 800/1251 ( 64%)]  Loss: 4.176 (3.90)  Time: 0.741s, 1382.63/s  (0.698s, 1466.95/s)  LR: 9.603e-04  Data: 0.009 (0.013)
Train: 77 [ 850/1251 ( 68%)]  Loss: 4.136 (3.91)  Time: 0.684s, 1496.22/s  (0.698s, 1467.00/s)  LR: 9.603e-04  Data: 0.015 (0.013)
Train: 77 [ 900/1251 ( 72%)]  Loss: 4.115 (3.92)  Time: 0.666s, 1536.92/s  (0.698s, 1467.55/s)  LR: 9.603e-04  Data: 0.011 (0.013)
Train: 77 [ 950/1251 ( 76%)]  Loss: 3.961 (3.92)  Time: 0.706s, 1451.25/s  (0.698s, 1467.16/s)  LR: 9.603e-04  Data: 0.011 (0.013)
Train: 77 [1000/1251 ( 80%)]  Loss: 3.817 (3.92)  Time: 0.752s, 1361.90/s  (0.698s, 1467.70/s)  LR: 9.603e-04  Data: 0.009 (0.013)
Train: 77 [1050/1251 ( 84%)]  Loss: 4.293 (3.93)  Time: 0.709s, 1444.05/s  (0.698s, 1467.75/s)  LR: 9.603e-04  Data: 0.010 (0.012)
Train: 77 [1100/1251 ( 88%)]  Loss: 3.954 (3.93)  Time: 0.688s, 1489.02/s  (0.698s, 1467.57/s)  LR: 9.603e-04  Data: 0.010 (0.012)
Train: 77 [1150/1251 ( 92%)]  Loss: 3.708 (3.93)  Time: 0.703s, 1455.99/s  (0.697s, 1468.40/s)  LR: 9.603e-04  Data: 0.010 (0.012)
Train: 77 [1200/1251 ( 96%)]  Loss: 4.237 (3.94)  Time: 0.688s, 1489.26/s  (0.697s, 1468.83/s)  LR: 9.603e-04  Data: 0.009 (0.012)
Train: 77 [1250/1251 (100%)]  Loss: 4.141 (3.95)  Time: 0.669s, 1529.87/s  (0.697s, 1468.49/s)  LR: 9.603e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.771 (1.771)  Loss:  0.9468 (0.9468)  Acc@1: 85.2539 (85.2539)  Acc@5: 95.8984 (95.8984)
Test: [  48/48]  Time: 0.138 (0.656)  Loss:  0.9463 (1.5632)  Acc@1: 83.2547 (68.5960)  Acc@5: 95.4009 (88.9940)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-77.pth.tar', 68.59599996582031)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-76.pth.tar', 68.49400010742187)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-75.pth.tar', 68.30599997558593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-74.pth.tar', 68.11000010009765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-69.pth.tar', 67.9180000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-71.pth.tar', 67.85599992675782)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-62.pth.tar', 67.82000002441406)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-70.pth.tar', 67.74800002441407)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-72.pth.tar', 67.71200002441407)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-63.pth.tar', 67.56600013183593)

Train: 78 [   0/1251 (  0%)]  Loss: 3.645 (3.65)  Time: 2.314s,  442.61/s  (2.314s,  442.61/s)  LR: 9.593e-04  Data: 1.697 (1.697)
Train: 78 [  50/1251 (  4%)]  Loss: 4.372 (4.01)  Time: 0.737s, 1389.60/s  (0.763s, 1341.68/s)  LR: 9.593e-04  Data: 0.010 (0.058)
Train: 78 [ 100/1251 (  8%)]  Loss: 3.703 (3.91)  Time: 0.680s, 1504.94/s  (0.729s, 1404.83/s)  LR: 9.593e-04  Data: 0.010 (0.034)
Train: 78 [ 150/1251 ( 12%)]  Loss: 4.120 (3.96)  Time: 0.679s, 1508.24/s  (0.716s, 1430.46/s)  LR: 9.593e-04  Data: 0.011 (0.026)
Train: 78 [ 200/1251 ( 16%)]  Loss: 3.803 (3.93)  Time: 0.672s, 1523.18/s  (0.711s, 1440.15/s)  LR: 9.593e-04  Data: 0.010 (0.022)
Train: 78 [ 250/1251 ( 20%)]  Loss: 4.385 (4.00)  Time: 0.702s, 1458.53/s  (0.706s, 1450.93/s)  LR: 9.593e-04  Data: 0.009 (0.020)
Train: 78 [ 300/1251 ( 24%)]  Loss: 4.270 (4.04)  Time: 0.707s, 1448.98/s  (0.703s, 1456.89/s)  LR: 9.593e-04  Data: 0.010 (0.018)
Train: 78 [ 350/1251 ( 28%)]  Loss: 4.146 (4.06)  Time: 0.675s, 1515.93/s  (0.701s, 1460.93/s)  LR: 9.593e-04  Data: 0.009 (0.017)
Train: 78 [ 400/1251 ( 32%)]  Loss: 3.989 (4.05)  Time: 0.673s, 1521.74/s  (0.700s, 1463.26/s)  LR: 9.593e-04  Data: 0.011 (0.016)
Train: 78 [ 450/1251 ( 36%)]  Loss: 3.751 (4.02)  Time: 0.710s, 1441.98/s  (0.701s, 1461.61/s)  LR: 9.593e-04  Data: 0.010 (0.016)
Train: 78 [ 500/1251 ( 40%)]  Loss: 4.073 (4.02)  Time: 0.674s, 1519.77/s  (0.701s, 1461.01/s)  LR: 9.593e-04  Data: 0.011 (0.015)
Train: 78 [ 550/1251 ( 44%)]  Loss: 4.108 (4.03)  Time: 0.710s, 1442.83/s  (0.701s, 1460.78/s)  LR: 9.593e-04  Data: 0.012 (0.015)
Train: 78 [ 600/1251 ( 48%)]  Loss: 4.035 (4.03)  Time: 0.694s, 1475.07/s  (0.701s, 1461.64/s)  LR: 9.593e-04  Data: 0.010 (0.014)
Train: 78 [ 650/1251 ( 52%)]  Loss: 3.828 (4.02)  Time: 0.693s, 1478.00/s  (0.700s, 1463.19/s)  LR: 9.593e-04  Data: 0.010 (0.014)
Train: 78 [ 700/1251 ( 56%)]  Loss: 4.232 (4.03)  Time: 0.671s, 1525.84/s  (0.700s, 1463.26/s)  LR: 9.593e-04  Data: 0.011 (0.014)
Train: 78 [ 750/1251 ( 60%)]  Loss: 3.912 (4.02)  Time: 0.673s, 1522.16/s  (0.699s, 1464.21/s)  LR: 9.593e-04  Data: 0.012 (0.014)
Train: 78 [ 800/1251 ( 64%)]  Loss: 4.153 (4.03)  Time: 0.697s, 1469.79/s  (0.699s, 1464.94/s)  LR: 9.593e-04  Data: 0.013 (0.013)
Train: 78 [ 850/1251 ( 68%)]  Loss: 3.952 (4.03)  Time: 0.672s, 1522.87/s  (0.699s, 1465.49/s)  LR: 9.593e-04  Data: 0.010 (0.013)
Train: 78 [ 900/1251 ( 72%)]  Loss: 3.753 (4.01)  Time: 0.665s, 1539.32/s  (0.698s, 1466.22/s)  LR: 9.593e-04  Data: 0.010 (0.013)
Train: 78 [ 950/1251 ( 76%)]  Loss: 4.050 (4.01)  Time: 0.692s, 1480.69/s  (0.698s, 1467.02/s)  LR: 9.593e-04  Data: 0.013 (0.013)
Train: 78 [1000/1251 ( 80%)]  Loss: 4.070 (4.02)  Time: 0.674s, 1519.40/s  (0.698s, 1467.71/s)  LR: 9.593e-04  Data: 0.010 (0.013)
Train: 78 [1050/1251 ( 84%)]  Loss: 3.697 (4.00)  Time: 0.680s, 1506.75/s  (0.697s, 1468.39/s)  LR: 9.593e-04  Data: 0.009 (0.013)
Train: 78 [1100/1251 ( 88%)]  Loss: 4.044 (4.00)  Time: 0.667s, 1535.44/s  (0.697s, 1468.94/s)  LR: 9.593e-04  Data: 0.011 (0.012)
Train: 78 [1150/1251 ( 92%)]  Loss: 3.445 (3.98)  Time: 0.716s, 1429.49/s  (0.697s, 1468.46/s)  LR: 9.593e-04  Data: 0.010 (0.012)
Train: 78 [1200/1251 ( 96%)]  Loss: 3.805 (3.97)  Time: 0.695s, 1473.44/s  (0.698s, 1467.91/s)  LR: 9.593e-04  Data: 0.011 (0.012)
Train: 78 [1250/1251 (100%)]  Loss: 4.359 (3.99)  Time: 0.656s, 1560.17/s  (0.697s, 1468.13/s)  LR: 9.593e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.556 (1.556)  Loss:  0.9814 (0.9814)  Acc@1: 85.1562 (85.1562)  Acc@5: 95.8008 (95.8008)
Test: [  48/48]  Time: 0.136 (0.593)  Loss:  1.1113 (1.6807)  Acc@1: 81.4858 (67.8780)  Acc@5: 93.9859 (88.4340)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-77.pth.tar', 68.59599996582031)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-76.pth.tar', 68.49400010742187)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-75.pth.tar', 68.30599997558593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-74.pth.tar', 68.11000010009765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-69.pth.tar', 67.9180000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-78.pth.tar', 67.87799997314453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-71.pth.tar', 67.85599992675782)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-62.pth.tar', 67.82000002441406)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-70.pth.tar', 67.74800002441407)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-72.pth.tar', 67.71200002441407)

Train: 79 [   0/1251 (  0%)]  Loss: 4.005 (4.01)  Time: 2.353s,  435.18/s  (2.353s,  435.18/s)  LR: 9.583e-04  Data: 1.679 (1.679)
Train: 79 [  50/1251 (  4%)]  Loss: 3.856 (3.93)  Time: 0.698s, 1466.43/s  (0.733s, 1397.11/s)  LR: 9.583e-04  Data: 0.010 (0.046)
Train: 79 [ 100/1251 (  8%)]  Loss: 3.844 (3.90)  Time: 0.706s, 1451.21/s  (0.714s, 1433.77/s)  LR: 9.583e-04  Data: 0.010 (0.029)
Train: 79 [ 150/1251 ( 12%)]  Loss: 3.650 (3.84)  Time: 0.678s, 1510.22/s  (0.708s, 1446.79/s)  LR: 9.583e-04  Data: 0.009 (0.023)
Train: 79 [ 200/1251 ( 16%)]  Loss: 4.029 (3.88)  Time: 0.670s, 1528.66/s  (0.704s, 1453.92/s)  LR: 9.583e-04  Data: 0.009 (0.019)
Train: 79 [ 250/1251 ( 20%)]  Loss: 3.712 (3.85)  Time: 0.700s, 1462.15/s  (0.701s, 1460.04/s)  LR: 9.583e-04  Data: 0.008 (0.018)
Train: 79 [ 300/1251 ( 24%)]  Loss: 4.282 (3.91)  Time: 0.701s, 1460.69/s  (0.702s, 1459.20/s)  LR: 9.583e-04  Data: 0.010 (0.016)
Train: 79 [ 350/1251 ( 28%)]  Loss: 3.870 (3.91)  Time: 0.702s, 1458.40/s  (0.700s, 1462.36/s)  LR: 9.583e-04  Data: 0.010 (0.016)
Train: 79 [ 400/1251 ( 32%)]  Loss: 3.797 (3.89)  Time: 0.709s, 1444.38/s  (0.699s, 1465.11/s)  LR: 9.583e-04  Data: 0.011 (0.015)
Train: 79 [ 450/1251 ( 36%)]  Loss: 4.038 (3.91)  Time: 0.674s, 1518.41/s  (0.699s, 1464.18/s)  LR: 9.583e-04  Data: 0.010 (0.014)
Train: 79 [ 500/1251 ( 40%)]  Loss: 4.501 (3.96)  Time: 0.673s, 1521.75/s  (0.699s, 1464.40/s)  LR: 9.583e-04  Data: 0.009 (0.014)
Train: 79 [ 550/1251 ( 44%)]  Loss: 3.926 (3.96)  Time: 0.701s, 1460.55/s  (0.699s, 1465.06/s)  LR: 9.583e-04  Data: 0.009 (0.014)
Train: 79 [ 600/1251 ( 48%)]  Loss: 4.191 (3.98)  Time: 0.676s, 1515.70/s  (0.698s, 1466.54/s)  LR: 9.583e-04  Data: 0.010 (0.013)
Train: 79 [ 650/1251 ( 52%)]  Loss: 3.710 (3.96)  Time: 0.708s, 1446.03/s  (0.698s, 1467.50/s)  LR: 9.583e-04  Data: 0.010 (0.013)
Train: 79 [ 700/1251 ( 56%)]  Loss: 3.804 (3.95)  Time: 0.712s, 1438.28/s  (0.698s, 1467.26/s)  LR: 9.583e-04  Data: 0.009 (0.013)
Train: 79 [ 750/1251 ( 60%)]  Loss: 3.957 (3.95)  Time: 0.672s, 1523.71/s  (0.698s, 1466.52/s)  LR: 9.583e-04  Data: 0.010 (0.013)
Train: 79 [ 800/1251 ( 64%)]  Loss: 4.075 (3.96)  Time: 0.706s, 1450.61/s  (0.698s, 1467.39/s)  LR: 9.583e-04  Data: 0.012 (0.013)
Train: 79 [ 850/1251 ( 68%)]  Loss: 4.120 (3.96)  Time: 0.666s, 1537.13/s  (0.697s, 1468.51/s)  LR: 9.583e-04  Data: 0.010 (0.012)
Train: 79 [ 900/1251 ( 72%)]  Loss: 3.665 (3.95)  Time: 0.712s, 1438.78/s  (0.697s, 1469.27/s)  LR: 9.583e-04  Data: 0.009 (0.012)
Train: 79 [ 950/1251 ( 76%)]  Loss: 4.060 (3.95)  Time: 0.677s, 1511.95/s  (0.697s, 1468.57/s)  LR: 9.583e-04  Data: 0.009 (0.012)
Train: 79 [1000/1251 ( 80%)]  Loss: 3.847 (3.95)  Time: 0.684s, 1496.09/s  (0.697s, 1468.63/s)  LR: 9.583e-04  Data: 0.012 (0.012)
Train: 79 [1050/1251 ( 84%)]  Loss: 3.865 (3.95)  Time: 0.815s, 1256.69/s  (0.697s, 1468.50/s)  LR: 9.583e-04  Data: 0.011 (0.012)
Train: 79 [1100/1251 ( 88%)]  Loss: 3.782 (3.94)  Time: 0.670s, 1529.41/s  (0.697s, 1469.11/s)  LR: 9.583e-04  Data: 0.011 (0.012)
Train: 79 [1150/1251 ( 92%)]  Loss: 3.644 (3.93)  Time: 0.676s, 1515.43/s  (0.697s, 1469.40/s)  LR: 9.583e-04  Data: 0.009 (0.012)
Train: 79 [1200/1251 ( 96%)]  Loss: 4.000 (3.93)  Time: 0.679s, 1508.24/s  (0.696s, 1470.26/s)  LR: 9.583e-04  Data: 0.011 (0.012)
Train: 79 [1250/1251 (100%)]  Loss: 4.056 (3.93)  Time: 0.703s, 1457.39/s  (0.696s, 1470.67/s)  LR: 9.583e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.522 (1.522)  Loss:  1.0732 (1.0732)  Acc@1: 87.1094 (87.1094)  Acc@5: 96.0938 (96.0938)
Test: [  48/48]  Time: 0.136 (0.575)  Loss:  1.0137 (1.6521)  Acc@1: 82.1934 (68.0920)  Acc@5: 95.1651 (88.7060)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-77.pth.tar', 68.59599996582031)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-76.pth.tar', 68.49400010742187)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-75.pth.tar', 68.30599997558593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-74.pth.tar', 68.11000010009765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-79.pth.tar', 68.09200002197265)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-69.pth.tar', 67.9180000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-78.pth.tar', 67.87799997314453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-71.pth.tar', 67.85599992675782)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-62.pth.tar', 67.82000002441406)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-70.pth.tar', 67.74800002441407)

Train: 80 [   0/1251 (  0%)]  Loss: 3.899 (3.90)  Time: 2.349s,  435.96/s  (2.349s,  435.96/s)  LR: 9.572e-04  Data: 1.706 (1.706)
Train: 80 [  50/1251 (  4%)]  Loss: 3.655 (3.78)  Time: 0.709s, 1444.46/s  (0.742s, 1379.66/s)  LR: 9.572e-04  Data: 0.011 (0.047)
Train: 80 [ 100/1251 (  8%)]  Loss: 3.813 (3.79)  Time: 0.710s, 1441.34/s  (0.718s, 1426.00/s)  LR: 9.572e-04  Data: 0.010 (0.029)
Train: 80 [ 150/1251 ( 12%)]  Loss: 4.073 (3.86)  Time: 0.692s, 1479.12/s  (0.713s, 1436.73/s)  LR: 9.572e-04  Data: 0.009 (0.023)
Train: 80 [ 200/1251 ( 16%)]  Loss: 4.111 (3.91)  Time: 0.731s, 1400.41/s  (0.708s, 1446.83/s)  LR: 9.572e-04  Data: 0.013 (0.020)
Train: 80 [ 250/1251 ( 20%)]  Loss: 4.157 (3.95)  Time: 0.672s, 1523.50/s  (0.705s, 1453.27/s)  LR: 9.572e-04  Data: 0.009 (0.018)
Train: 80 [ 300/1251 ( 24%)]  Loss: 3.728 (3.92)  Time: 0.796s, 1285.77/s  (0.703s, 1455.81/s)  LR: 9.572e-04  Data: 0.009 (0.016)
Train: 80 [ 350/1251 ( 28%)]  Loss: 3.770 (3.90)  Time: 0.704s, 1453.95/s  (0.702s, 1458.72/s)  LR: 9.572e-04  Data: 0.009 (0.016)
Train: 80 [ 400/1251 ( 32%)]  Loss: 3.965 (3.91)  Time: 0.707s, 1449.03/s  (0.701s, 1460.48/s)  LR: 9.572e-04  Data: 0.010 (0.015)
Train: 80 [ 450/1251 ( 36%)]  Loss: 3.961 (3.91)  Time: 0.668s, 1532.38/s  (0.701s, 1460.31/s)  LR: 9.572e-04  Data: 0.012 (0.014)
Train: 80 [ 500/1251 ( 40%)]  Loss: 4.179 (3.94)  Time: 0.688s, 1487.33/s  (0.701s, 1460.62/s)  LR: 9.572e-04  Data: 0.010 (0.014)
Train: 80 [ 550/1251 ( 44%)]  Loss: 3.951 (3.94)  Time: 0.692s, 1480.05/s  (0.700s, 1463.47/s)  LR: 9.572e-04  Data: 0.009 (0.014)
Train: 80 [ 600/1251 ( 48%)]  Loss: 4.093 (3.95)  Time: 0.673s, 1521.17/s  (0.699s, 1464.92/s)  LR: 9.572e-04  Data: 0.010 (0.013)
Train: 80 [ 650/1251 ( 52%)]  Loss: 4.265 (3.97)  Time: 0.672s, 1522.79/s  (0.698s, 1466.42/s)  LR: 9.572e-04  Data: 0.011 (0.013)
Train: 80 [ 700/1251 ( 56%)]  Loss: 4.122 (3.98)  Time: 0.672s, 1523.24/s  (0.698s, 1467.94/s)  LR: 9.572e-04  Data: 0.010 (0.013)
Train: 80 [ 750/1251 ( 60%)]  Loss: 4.172 (3.99)  Time: 0.684s, 1497.50/s  (0.697s, 1468.49/s)  LR: 9.572e-04  Data: 0.009 (0.013)
Train: 80 [ 800/1251 ( 64%)]  Loss: 3.640 (3.97)  Time: 0.705s, 1452.78/s  (0.697s, 1469.20/s)  LR: 9.572e-04  Data: 0.010 (0.013)
Train: 80 [ 850/1251 ( 68%)]  Loss: 3.759 (3.96)  Time: 0.696s, 1472.06/s  (0.697s, 1468.81/s)  LR: 9.572e-04  Data: 0.011 (0.012)
Train: 80 [ 900/1251 ( 72%)]  Loss: 4.335 (3.98)  Time: 0.709s, 1444.86/s  (0.697s, 1468.47/s)  LR: 9.572e-04  Data: 0.009 (0.012)
Train: 80 [ 950/1251 ( 76%)]  Loss: 4.242 (3.99)  Time: 0.672s, 1524.85/s  (0.697s, 1468.38/s)  LR: 9.572e-04  Data: 0.011 (0.012)
Train: 80 [1000/1251 ( 80%)]  Loss: 4.153 (4.00)  Time: 0.673s, 1520.44/s  (0.697s, 1468.69/s)  LR: 9.572e-04  Data: 0.011 (0.012)
Train: 80 [1050/1251 ( 84%)]  Loss: 4.034 (4.00)  Time: 0.674s, 1518.25/s  (0.697s, 1469.24/s)  LR: 9.572e-04  Data: 0.010 (0.012)
Train: 80 [1100/1251 ( 88%)]  Loss: 3.706 (3.99)  Time: 0.673s, 1521.33/s  (0.697s, 1469.62/s)  LR: 9.572e-04  Data: 0.009 (0.012)
Train: 80 [1150/1251 ( 92%)]  Loss: 4.061 (3.99)  Time: 0.676s, 1515.18/s  (0.697s, 1469.69/s)  LR: 9.572e-04  Data: 0.010 (0.012)
Train: 80 [1200/1251 ( 96%)]  Loss: 4.017 (3.99)  Time: 0.706s, 1449.60/s  (0.697s, 1470.11/s)  LR: 9.572e-04  Data: 0.009 (0.012)
Train: 80 [1250/1251 (100%)]  Loss: 3.684 (3.98)  Time: 0.656s, 1560.48/s  (0.696s, 1470.73/s)  LR: 9.572e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.554 (1.554)  Loss:  0.8936 (0.8936)  Acc@1: 85.4492 (85.4492)  Acc@5: 96.1914 (96.1914)
Test: [  48/48]  Time: 0.136 (0.589)  Loss:  1.0479 (1.6178)  Acc@1: 81.4859 (68.6060)  Acc@5: 94.2217 (88.8720)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-80.pth.tar', 68.60600010253906)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-77.pth.tar', 68.59599996582031)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-76.pth.tar', 68.49400010742187)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-75.pth.tar', 68.30599997558593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-74.pth.tar', 68.11000010009765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-79.pth.tar', 68.09200002197265)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-69.pth.tar', 67.9180000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-78.pth.tar', 67.87799997314453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-71.pth.tar', 67.85599992675782)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-62.pth.tar', 67.82000002441406)

Train: 81 [   0/1251 (  0%)]  Loss: 3.704 (3.70)  Time: 2.111s,  485.11/s  (2.111s,  485.11/s)  LR: 9.561e-04  Data: 1.495 (1.495)
Train: 81 [  50/1251 (  4%)]  Loss: 3.413 (3.56)  Time: 0.673s, 1521.63/s  (0.729s, 1405.17/s)  LR: 9.561e-04  Data: 0.010 (0.045)
Train: 81 [ 100/1251 (  8%)]  Loss: 3.656 (3.59)  Time: 0.672s, 1523.13/s  (0.715s, 1431.79/s)  LR: 9.561e-04  Data: 0.011 (0.028)
Train: 81 [ 150/1251 ( 12%)]  Loss: 4.184 (3.74)  Time: 0.695s, 1474.26/s  (0.708s, 1445.43/s)  LR: 9.561e-04  Data: 0.010 (0.022)
Train: 81 [ 200/1251 ( 16%)]  Loss: 3.975 (3.79)  Time: 0.710s, 1442.67/s  (0.706s, 1450.75/s)  LR: 9.561e-04  Data: 0.010 (0.019)
Train: 81 [ 250/1251 ( 20%)]  Loss: 3.540 (3.75)  Time: 0.704s, 1454.29/s  (0.703s, 1456.59/s)  LR: 9.561e-04  Data: 0.010 (0.017)
Train: 81 [ 300/1251 ( 24%)]  Loss: 3.808 (3.75)  Time: 0.693s, 1477.64/s  (0.702s, 1457.93/s)  LR: 9.561e-04  Data: 0.010 (0.016)
Train: 81 [ 350/1251 ( 28%)]  Loss: 4.104 (3.80)  Time: 0.675s, 1517.47/s  (0.701s, 1461.31/s)  LR: 9.561e-04  Data: 0.010 (0.015)
Train: 81 [ 400/1251 ( 32%)]  Loss: 3.949 (3.81)  Time: 0.699s, 1465.53/s  (0.699s, 1464.25/s)  LR: 9.561e-04  Data: 0.009 (0.015)
Train: 81 [ 450/1251 ( 36%)]  Loss: 3.743 (3.81)  Time: 0.727s, 1408.49/s  (0.698s, 1466.56/s)  LR: 9.561e-04  Data: 0.011 (0.014)
Train: 81 [ 500/1251 ( 40%)]  Loss: 3.841 (3.81)  Time: 0.672s, 1524.01/s  (0.698s, 1466.72/s)  LR: 9.561e-04  Data: 0.013 (0.014)
Train: 81 [ 550/1251 ( 44%)]  Loss: 4.165 (3.84)  Time: 0.731s, 1399.95/s  (0.698s, 1467.99/s)  LR: 9.561e-04  Data: 0.009 (0.014)
Train: 81 [ 600/1251 ( 48%)]  Loss: 3.659 (3.83)  Time: 0.670s, 1528.55/s  (0.698s, 1467.98/s)  LR: 9.561e-04  Data: 0.010 (0.013)
Train: 81 [ 650/1251 ( 52%)]  Loss: 4.093 (3.85)  Time: 0.687s, 1489.90/s  (0.697s, 1468.29/s)  LR: 9.561e-04  Data: 0.012 (0.013)
Train: 81 [ 700/1251 ( 56%)]  Loss: 3.793 (3.84)  Time: 0.708s, 1445.60/s  (0.698s, 1467.61/s)  LR: 9.561e-04  Data: 0.010 (0.013)
Train: 81 [ 750/1251 ( 60%)]  Loss: 4.109 (3.86)  Time: 0.671s, 1526.85/s  (0.698s, 1467.51/s)  LR: 9.561e-04  Data: 0.010 (0.013)
Train: 81 [ 800/1251 ( 64%)]  Loss: 3.865 (3.86)  Time: 0.671s, 1525.51/s  (0.697s, 1468.24/s)  LR: 9.561e-04  Data: 0.010 (0.013)
Train: 81 [ 850/1251 ( 68%)]  Loss: 4.138 (3.87)  Time: 0.706s, 1450.78/s  (0.697s, 1468.68/s)  LR: 9.561e-04  Data: 0.011 (0.012)
Train: 81 [ 900/1251 ( 72%)]  Loss: 3.626 (3.86)  Time: 0.704s, 1454.66/s  (0.697s, 1469.29/s)  LR: 9.561e-04  Data: 0.009 (0.012)
Train: 81 [ 950/1251 ( 76%)]  Loss: 3.950 (3.87)  Time: 0.731s, 1401.63/s  (0.697s, 1468.35/s)  LR: 9.561e-04  Data: 0.009 (0.012)
Train: 81 [1000/1251 ( 80%)]  Loss: 4.066 (3.88)  Time: 0.675s, 1516.75/s  (0.697s, 1468.85/s)  LR: 9.561e-04  Data: 0.010 (0.012)
Train: 81 [1050/1251 ( 84%)]  Loss: 3.947 (3.88)  Time: 0.678s, 1509.22/s  (0.697s, 1469.13/s)  LR: 9.561e-04  Data: 0.013 (0.012)
Train: 81 [1100/1251 ( 88%)]  Loss: 4.194 (3.89)  Time: 0.690s, 1483.17/s  (0.697s, 1469.63/s)  LR: 9.561e-04  Data: 0.015 (0.012)
Train: 81 [1150/1251 ( 92%)]  Loss: 3.992 (3.90)  Time: 0.672s, 1524.28/s  (0.697s, 1470.03/s)  LR: 9.561e-04  Data: 0.012 (0.012)
Train: 81 [1200/1251 ( 96%)]  Loss: 3.770 (3.89)  Time: 0.703s, 1457.58/s  (0.696s, 1470.40/s)  LR: 9.561e-04  Data: 0.010 (0.012)
Train: 81 [1250/1251 (100%)]  Loss: 4.297 (3.91)  Time: 0.664s, 1541.36/s  (0.696s, 1471.01/s)  LR: 9.561e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.442 (1.442)  Loss:  1.1113 (1.1113)  Acc@1: 86.1328 (86.1328)  Acc@5: 96.0938 (96.0938)
Test: [  48/48]  Time: 0.136 (0.588)  Loss:  1.1514 (1.7064)  Acc@1: 82.4292 (67.4360)  Acc@5: 94.1038 (88.2440)
Train: 82 [   0/1251 (  0%)]  Loss: 4.126 (4.13)  Time: 2.263s,  452.40/s  (2.263s,  452.40/s)  LR: 9.551e-04  Data: 1.647 (1.647)
Train: 82 [  50/1251 (  4%)]  Loss: 3.776 (3.95)  Time: 0.710s, 1441.97/s  (0.733s, 1396.72/s)  LR: 9.551e-04  Data: 0.010 (0.049)
Train: 82 [ 100/1251 (  8%)]  Loss: 3.947 (3.95)  Time: 0.684s, 1497.48/s  (0.716s, 1430.52/s)  LR: 9.551e-04  Data: 0.009 (0.030)
Train: 82 [ 150/1251 ( 12%)]  Loss: 3.633 (3.87)  Time: 0.705s, 1452.11/s  (0.710s, 1441.72/s)  LR: 9.551e-04  Data: 0.009 (0.023)
Train: 82 [ 200/1251 ( 16%)]  Loss: 4.386 (3.97)  Time: 0.680s, 1506.15/s  (0.706s, 1449.92/s)  LR: 9.551e-04  Data: 0.010 (0.020)
Train: 82 [ 250/1251 ( 20%)]  Loss: 4.090 (3.99)  Time: 0.705s, 1451.49/s  (0.704s, 1454.95/s)  LR: 9.551e-04  Data: 0.009 (0.018)
Train: 82 [ 300/1251 ( 24%)]  Loss: 3.989 (3.99)  Time: 0.724s, 1414.61/s  (0.702s, 1458.35/s)  LR: 9.551e-04  Data: 0.010 (0.017)
Train: 82 [ 350/1251 ( 28%)]  Loss: 3.655 (3.95)  Time: 0.715s, 1431.66/s  (0.701s, 1461.62/s)  LR: 9.551e-04  Data: 0.009 (0.016)
Train: 82 [ 400/1251 ( 32%)]  Loss: 3.823 (3.94)  Time: 0.679s, 1507.50/s  (0.700s, 1461.95/s)  LR: 9.551e-04  Data: 0.009 (0.015)
Train: 82 [ 450/1251 ( 36%)]  Loss: 3.900 (3.93)  Time: 0.706s, 1451.19/s  (0.699s, 1463.93/s)  LR: 9.551e-04  Data: 0.009 (0.014)
Train: 82 [ 500/1251 ( 40%)]  Loss: 3.945 (3.93)  Time: 0.733s, 1397.89/s  (0.700s, 1463.37/s)  LR: 9.551e-04  Data: 0.010 (0.014)
Train: 82 [ 550/1251 ( 44%)]  Loss: 4.183 (3.95)  Time: 0.715s, 1433.13/s  (0.700s, 1463.38/s)  LR: 9.551e-04  Data: 0.009 (0.014)
Train: 82 [ 600/1251 ( 48%)]  Loss: 4.060 (3.96)  Time: 0.703s, 1457.11/s  (0.699s, 1464.02/s)  LR: 9.551e-04  Data: 0.009 (0.013)
Train: 82 [ 650/1251 ( 52%)]  Loss: 4.193 (3.98)  Time: 0.697s, 1469.93/s  (0.699s, 1465.52/s)  LR: 9.551e-04  Data: 0.009 (0.013)
Train: 82 [ 700/1251 ( 56%)]  Loss: 3.983 (3.98)  Time: 0.688s, 1488.68/s  (0.698s, 1467.02/s)  LR: 9.551e-04  Data: 0.011 (0.013)
Train: 82 [ 750/1251 ( 60%)]  Loss: 4.136 (3.99)  Time: 0.706s, 1450.62/s  (0.698s, 1466.95/s)  LR: 9.551e-04  Data: 0.009 (0.013)
Train: 82 [ 800/1251 ( 64%)]  Loss: 4.115 (4.00)  Time: 0.681s, 1504.19/s  (0.698s, 1467.78/s)  LR: 9.551e-04  Data: 0.010 (0.012)
Train: 82 [ 850/1251 ( 68%)]  Loss: 3.789 (3.98)  Time: 0.670s, 1528.96/s  (0.698s, 1467.52/s)  LR: 9.551e-04  Data: 0.009 (0.012)
Train: 82 [ 900/1251 ( 72%)]  Loss: 3.893 (3.98)  Time: 0.672s, 1523.12/s  (0.697s, 1468.62/s)  LR: 9.551e-04  Data: 0.009 (0.012)
Train: 82 [ 950/1251 ( 76%)]  Loss: 3.896 (3.98)  Time: 0.704s, 1454.47/s  (0.697s, 1468.15/s)  LR: 9.551e-04  Data: 0.011 (0.012)
Train: 82 [1000/1251 ( 80%)]  Loss: 4.209 (3.99)  Time: 0.730s, 1402.24/s  (0.697s, 1468.12/s)  LR: 9.551e-04  Data: 0.009 (0.012)
Train: 82 [1050/1251 ( 84%)]  Loss: 3.788 (3.98)  Time: 0.677s, 1513.58/s  (0.697s, 1468.95/s)  LR: 9.551e-04  Data: 0.014 (0.012)
Train: 82 [1100/1251 ( 88%)]  Loss: 4.309 (3.99)  Time: 0.727s, 1408.24/s  (0.697s, 1469.69/s)  LR: 9.551e-04  Data: 0.010 (0.012)
Train: 82 [1150/1251 ( 92%)]  Loss: 3.924 (3.99)  Time: 0.669s, 1530.81/s  (0.697s, 1469.99/s)  LR: 9.551e-04  Data: 0.013 (0.012)
Train: 82 [1200/1251 ( 96%)]  Loss: 3.936 (3.99)  Time: 0.704s, 1453.53/s  (0.697s, 1469.95/s)  LR: 9.551e-04  Data: 0.009 (0.012)
Train: 82 [1250/1251 (100%)]  Loss: 4.059 (3.99)  Time: 0.657s, 1558.28/s  (0.697s, 1469.83/s)  LR: 9.551e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.586 (1.586)  Loss:  0.9072 (0.9072)  Acc@1: 83.4961 (83.4961)  Acc@5: 95.2148 (95.2148)
Test: [  48/48]  Time: 0.136 (0.591)  Loss:  0.9883 (1.5600)  Acc@1: 84.0802 (68.6400)  Acc@5: 95.2830 (88.9420)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-82.pth.tar', 68.63999993652344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-80.pth.tar', 68.60600010253906)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-77.pth.tar', 68.59599996582031)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-76.pth.tar', 68.49400010742187)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-75.pth.tar', 68.30599997558593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-74.pth.tar', 68.11000010009765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-79.pth.tar', 68.09200002197265)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-69.pth.tar', 67.9180000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-78.pth.tar', 67.87799997314453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-71.pth.tar', 67.85599992675782)

Train: 83 [   0/1251 (  0%)]  Loss: 3.777 (3.78)  Time: 2.182s,  469.26/s  (2.182s,  469.26/s)  LR: 9.540e-04  Data: 1.542 (1.542)
Train: 83 [  50/1251 (  4%)]  Loss: 4.155 (3.97)  Time: 0.688s, 1488.65/s  (0.730s, 1402.35/s)  LR: 9.540e-04  Data: 0.010 (0.047)
Train: 83 [ 100/1251 (  8%)]  Loss: 4.022 (3.98)  Time: 0.714s, 1433.89/s  (0.712s, 1438.35/s)  LR: 9.540e-04  Data: 0.011 (0.029)
Train: 83 [ 150/1251 ( 12%)]  Loss: 3.916 (3.97)  Time: 0.673s, 1521.46/s  (0.706s, 1450.08/s)  LR: 9.540e-04  Data: 0.009 (0.023)
Train: 83 [ 200/1251 ( 16%)]  Loss: 3.821 (3.94)  Time: 0.677s, 1511.73/s  (0.702s, 1457.89/s)  LR: 9.540e-04  Data: 0.013 (0.019)
Train: 83 [ 250/1251 ( 20%)]  Loss: 4.007 (3.95)  Time: 0.674s, 1519.88/s  (0.701s, 1460.70/s)  LR: 9.540e-04  Data: 0.010 (0.018)
Train: 83 [ 300/1251 ( 24%)]  Loss: 4.356 (4.01)  Time: 0.716s, 1429.63/s  (0.700s, 1461.90/s)  LR: 9.540e-04  Data: 0.009 (0.016)
Train: 83 [ 350/1251 ( 28%)]  Loss: 4.159 (4.03)  Time: 0.672s, 1522.71/s  (0.700s, 1462.64/s)  LR: 9.540e-04  Data: 0.011 (0.015)
Train: 83 [ 400/1251 ( 32%)]  Loss: 4.337 (4.06)  Time: 0.671s, 1525.07/s  (0.699s, 1464.72/s)  LR: 9.540e-04  Data: 0.010 (0.015)
Train: 83 [ 450/1251 ( 36%)]  Loss: 4.120 (4.07)  Time: 0.676s, 1514.61/s  (0.698s, 1467.47/s)  LR: 9.540e-04  Data: 0.010 (0.014)
Train: 83 [ 500/1251 ( 40%)]  Loss: 3.821 (4.04)  Time: 0.674s, 1518.39/s  (0.697s, 1469.13/s)  LR: 9.540e-04  Data: 0.012 (0.014)
Train: 83 [ 550/1251 ( 44%)]  Loss: 3.970 (4.04)  Time: 0.668s, 1532.45/s  (0.697s, 1469.08/s)  LR: 9.540e-04  Data: 0.010 (0.014)
Train: 83 [ 600/1251 ( 48%)]  Loss: 4.031 (4.04)  Time: 0.764s, 1340.58/s  (0.696s, 1470.76/s)  LR: 9.540e-04  Data: 0.009 (0.013)
Train: 83 [ 650/1251 ( 52%)]  Loss: 3.905 (4.03)  Time: 0.762s, 1344.17/s  (0.696s, 1471.49/s)  LR: 9.540e-04  Data: 0.008 (0.013)
Train: 83 [ 700/1251 ( 56%)]  Loss: 3.798 (4.01)  Time: 0.701s, 1459.78/s  (0.696s, 1471.45/s)  LR: 9.540e-04  Data: 0.009 (0.013)
Train: 83 [ 750/1251 ( 60%)]  Loss: 3.868 (4.00)  Time: 0.669s, 1531.77/s  (0.696s, 1471.11/s)  LR: 9.540e-04  Data: 0.009 (0.013)
Train: 83 [ 800/1251 ( 64%)]  Loss: 3.979 (4.00)  Time: 0.703s, 1457.38/s  (0.696s, 1471.69/s)  LR: 9.540e-04  Data: 0.010 (0.012)
Train: 83 [ 850/1251 ( 68%)]  Loss: 3.974 (4.00)  Time: 0.757s, 1353.47/s  (0.696s, 1472.02/s)  LR: 9.540e-04  Data: 0.009 (0.012)
Train: 83 [ 900/1251 ( 72%)]  Loss: 4.041 (4.00)  Time: 0.695s, 1472.97/s  (0.695s, 1472.39/s)  LR: 9.540e-04  Data: 0.011 (0.012)
Train: 83 [ 950/1251 ( 76%)]  Loss: 3.722 (3.99)  Time: 0.722s, 1417.46/s  (0.696s, 1472.06/s)  LR: 9.540e-04  Data: 0.010 (0.012)
Train: 83 [1000/1251 ( 80%)]  Loss: 4.329 (4.01)  Time: 0.704s, 1454.22/s  (0.696s, 1471.63/s)  LR: 9.540e-04  Data: 0.010 (0.012)
Train: 83 [1050/1251 ( 84%)]  Loss: 3.888 (4.00)  Time: 0.715s, 1431.61/s  (0.696s, 1470.67/s)  LR: 9.540e-04  Data: 0.009 (0.012)
Train: 83 [1100/1251 ( 88%)]  Loss: 3.902 (4.00)  Time: 0.668s, 1533.90/s  (0.696s, 1470.40/s)  LR: 9.540e-04  Data: 0.011 (0.012)
Train: 83 [1150/1251 ( 92%)]  Loss: 3.809 (3.99)  Time: 0.695s, 1473.23/s  (0.696s, 1470.53/s)  LR: 9.540e-04  Data: 0.010 (0.012)
Train: 83 [1200/1251 ( 96%)]  Loss: 3.868 (3.98)  Time: 0.673s, 1521.67/s  (0.696s, 1471.09/s)  LR: 9.540e-04  Data: 0.010 (0.012)
Train: 83 [1250/1251 (100%)]  Loss: 4.298 (4.00)  Time: 0.657s, 1558.34/s  (0.696s, 1471.40/s)  LR: 9.540e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.517 (1.517)  Loss:  1.1572 (1.1572)  Acc@1: 84.8633 (84.8633)  Acc@5: 95.5078 (95.5078)
Test: [  48/48]  Time: 0.136 (0.580)  Loss:  1.3516 (1.7155)  Acc@1: 78.1840 (68.6420)  Acc@5: 92.5708 (88.9020)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-83.pth.tar', 68.6419999609375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-82.pth.tar', 68.63999993652344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-80.pth.tar', 68.60600010253906)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-77.pth.tar', 68.59599996582031)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-76.pth.tar', 68.49400010742187)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-75.pth.tar', 68.30599997558593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-74.pth.tar', 68.11000010009765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-79.pth.tar', 68.09200002197265)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-69.pth.tar', 67.9180000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-78.pth.tar', 67.87799997314453)

Train: 84 [   0/1251 (  0%)]  Loss: 3.839 (3.84)  Time: 1.950s,  525.10/s  (1.950s,  525.10/s)  LR: 9.529e-04  Data: 1.332 (1.332)
Train: 84 [  50/1251 (  4%)]  Loss: 3.825 (3.83)  Time: 0.710s, 1441.36/s  (0.740s, 1384.62/s)  LR: 9.529e-04  Data: 0.010 (0.045)
Train: 84 [ 100/1251 (  8%)]  Loss: 4.044 (3.90)  Time: 0.673s, 1521.17/s  (0.720s, 1422.66/s)  LR: 9.529e-04  Data: 0.011 (0.028)
Train: 84 [ 150/1251 ( 12%)]  Loss: 4.008 (3.93)  Time: 0.676s, 1514.84/s  (0.713s, 1436.10/s)  LR: 9.529e-04  Data: 0.013 (0.022)
Train: 84 [ 200/1251 ( 16%)]  Loss: 4.272 (4.00)  Time: 0.710s, 1442.28/s  (0.710s, 1441.92/s)  LR: 9.529e-04  Data: 0.010 (0.019)
Train: 84 [ 250/1251 ( 20%)]  Loss: 3.759 (3.96)  Time: 0.672s, 1523.62/s  (0.709s, 1445.06/s)  LR: 9.529e-04  Data: 0.010 (0.017)
Train: 84 [ 300/1251 ( 24%)]  Loss: 4.010 (3.97)  Time: 0.708s, 1446.68/s  (0.705s, 1453.40/s)  LR: 9.529e-04  Data: 0.011 (0.016)
Train: 84 [ 350/1251 ( 28%)]  Loss: 4.169 (3.99)  Time: 0.668s, 1533.54/s  (0.703s, 1457.27/s)  LR: 9.529e-04  Data: 0.009 (0.015)
Train: 84 [ 400/1251 ( 32%)]  Loss: 3.896 (3.98)  Time: 0.671s, 1526.70/s  (0.701s, 1461.19/s)  LR: 9.529e-04  Data: 0.010 (0.015)
Train: 84 [ 450/1251 ( 36%)]  Loss: 4.076 (3.99)  Time: 0.723s, 1415.58/s  (0.700s, 1462.89/s)  LR: 9.529e-04  Data: 0.010 (0.014)
Train: 84 [ 500/1251 ( 40%)]  Loss: 3.995 (3.99)  Time: 0.675s, 1516.80/s  (0.700s, 1463.48/s)  LR: 9.529e-04  Data: 0.009 (0.014)
Train: 84 [ 550/1251 ( 44%)]  Loss: 4.245 (4.01)  Time: 0.737s, 1390.11/s  (0.699s, 1464.33/s)  LR: 9.529e-04  Data: 0.010 (0.013)
Train: 84 [ 600/1251 ( 48%)]  Loss: 3.964 (4.01)  Time: 0.694s, 1475.29/s  (0.698s, 1466.45/s)  LR: 9.529e-04  Data: 0.011 (0.013)
Train: 84 [ 650/1251 ( 52%)]  Loss: 3.956 (4.00)  Time: 0.670s, 1528.61/s  (0.697s, 1468.33/s)  LR: 9.529e-04  Data: 0.010 (0.013)
Train: 84 [ 700/1251 ( 56%)]  Loss: 3.854 (3.99)  Time: 0.674s, 1518.90/s  (0.697s, 1469.54/s)  LR: 9.529e-04  Data: 0.010 (0.013)
Train: 84 [ 750/1251 ( 60%)]  Loss: 3.760 (3.98)  Time: 0.706s, 1450.67/s  (0.697s, 1470.05/s)  LR: 9.529e-04  Data: 0.010 (0.013)
Train: 84 [ 800/1251 ( 64%)]  Loss: 3.891 (3.97)  Time: 0.720s, 1421.90/s  (0.696s, 1470.29/s)  LR: 9.529e-04  Data: 0.009 (0.012)
Train: 84 [ 850/1251 ( 68%)]  Loss: 4.085 (3.98)  Time: 0.708s, 1446.88/s  (0.697s, 1469.73/s)  LR: 9.529e-04  Data: 0.009 (0.012)
Train: 84 [ 900/1251 ( 72%)]  Loss: 4.330 (4.00)  Time: 0.673s, 1520.49/s  (0.696s, 1470.32/s)  LR: 9.529e-04  Data: 0.010 (0.012)
Train: 84 [ 950/1251 ( 76%)]  Loss: 4.178 (4.01)  Time: 0.684s, 1497.19/s  (0.697s, 1469.83/s)  LR: 9.529e-04  Data: 0.011 (0.012)
Train: 84 [1000/1251 ( 80%)]  Loss: 3.601 (3.99)  Time: 0.710s, 1442.22/s  (0.696s, 1470.87/s)  LR: 9.529e-04  Data: 0.009 (0.012)
Train: 84 [1050/1251 ( 84%)]  Loss: 4.050 (3.99)  Time: 0.681s, 1504.31/s  (0.696s, 1471.40/s)  LR: 9.529e-04  Data: 0.009 (0.012)
Train: 84 [1100/1251 ( 88%)]  Loss: 4.198 (4.00)  Time: 0.675s, 1517.33/s  (0.696s, 1471.41/s)  LR: 9.529e-04  Data: 0.010 (0.012)
Train: 84 [1150/1251 ( 92%)]  Loss: 4.214 (4.01)  Time: 0.674s, 1519.99/s  (0.696s, 1471.52/s)  LR: 9.529e-04  Data: 0.011 (0.012)
Train: 84 [1200/1251 ( 96%)]  Loss: 3.971 (4.01)  Time: 0.702s, 1459.07/s  (0.696s, 1471.98/s)  LR: 9.529e-04  Data: 0.011 (0.012)
Train: 84 [1250/1251 (100%)]  Loss: 3.963 (4.01)  Time: 0.656s, 1560.08/s  (0.696s, 1471.78/s)  LR: 9.529e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.585 (1.585)  Loss:  1.0547 (1.0547)  Acc@1: 85.0586 (85.0586)  Acc@5: 95.6055 (95.6055)
Test: [  48/48]  Time: 0.138 (0.587)  Loss:  1.0488 (1.6836)  Acc@1: 82.3113 (68.1900)  Acc@5: 95.4009 (88.6960)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-83.pth.tar', 68.6419999609375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-82.pth.tar', 68.63999993652344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-80.pth.tar', 68.60600010253906)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-77.pth.tar', 68.59599996582031)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-76.pth.tar', 68.49400010742187)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-75.pth.tar', 68.30599997558593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-84.pth.tar', 68.18999994384765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-74.pth.tar', 68.11000010009765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-79.pth.tar', 68.09200002197265)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-69.pth.tar', 67.9180000024414)

Train: 85 [   0/1251 (  0%)]  Loss: 4.345 (4.34)  Time: 2.184s,  468.91/s  (2.184s,  468.91/s)  LR: 9.518e-04  Data: 1.568 (1.568)
Train: 85 [  50/1251 (  4%)]  Loss: 4.422 (4.38)  Time: 0.693s, 1477.12/s  (0.740s, 1383.99/s)  LR: 9.518e-04  Data: 0.009 (0.055)
Train: 85 [ 100/1251 (  8%)]  Loss: 3.957 (4.24)  Time: 0.707s, 1448.63/s  (0.719s, 1425.00/s)  LR: 9.518e-04  Data: 0.011 (0.033)
Train: 85 [ 150/1251 ( 12%)]  Loss: 3.974 (4.17)  Time: 0.727s, 1408.90/s  (0.714s, 1434.41/s)  LR: 9.518e-04  Data: 0.009 (0.025)
Train: 85 [ 200/1251 ( 16%)]  Loss: 3.644 (4.07)  Time: 0.665s, 1538.79/s  (0.709s, 1443.34/s)  LR: 9.518e-04  Data: 0.010 (0.022)
Train: 85 [ 250/1251 ( 20%)]  Loss: 4.156 (4.08)  Time: 0.705s, 1452.63/s  (0.706s, 1450.44/s)  LR: 9.518e-04  Data: 0.009 (0.019)
Train: 85 [ 300/1251 ( 24%)]  Loss: 4.048 (4.08)  Time: 0.822s, 1245.41/s  (0.705s, 1452.25/s)  LR: 9.518e-04  Data: 0.009 (0.018)
Train: 85 [ 350/1251 ( 28%)]  Loss: 4.331 (4.11)  Time: 0.674s, 1519.86/s  (0.705s, 1452.59/s)  LR: 9.518e-04  Data: 0.009 (0.017)
Train: 85 [ 400/1251 ( 32%)]  Loss: 3.869 (4.08)  Time: 0.669s, 1530.08/s  (0.705s, 1453.34/s)  LR: 9.518e-04  Data: 0.009 (0.016)
Train: 85 [ 450/1251 ( 36%)]  Loss: 3.824 (4.06)  Time: 0.670s, 1529.47/s  (0.703s, 1456.46/s)  LR: 9.518e-04  Data: 0.011 (0.015)
Train: 85 [ 500/1251 ( 40%)]  Loss: 4.080 (4.06)  Time: 0.706s, 1449.99/s  (0.703s, 1457.61/s)  LR: 9.518e-04  Data: 0.011 (0.015)
Train: 85 [ 550/1251 ( 44%)]  Loss: 3.837 (4.04)  Time: 0.676s, 1514.79/s  (0.702s, 1458.88/s)  LR: 9.518e-04  Data: 0.010 (0.014)
Train: 85 [ 600/1251 ( 48%)]  Loss: 4.318 (4.06)  Time: 0.706s, 1451.08/s  (0.701s, 1460.91/s)  LR: 9.518e-04  Data: 0.010 (0.014)
Train: 85 [ 650/1251 ( 52%)]  Loss: 4.124 (4.07)  Time: 0.750s, 1365.05/s  (0.700s, 1461.86/s)  LR: 9.518e-04  Data: 0.010 (0.014)
Train: 85 [ 700/1251 ( 56%)]  Loss: 4.033 (4.06)  Time: 0.695s, 1474.20/s  (0.701s, 1461.12/s)  LR: 9.518e-04  Data: 0.009 (0.013)
Train: 85 [ 750/1251 ( 60%)]  Loss: 3.999 (4.06)  Time: 0.672s, 1523.85/s  (0.701s, 1461.19/s)  LR: 9.518e-04  Data: 0.010 (0.013)
Train: 85 [ 800/1251 ( 64%)]  Loss: 3.846 (4.05)  Time: 0.671s, 1526.14/s  (0.700s, 1462.71/s)  LR: 9.518e-04  Data: 0.009 (0.013)
Train: 85 [ 850/1251 ( 68%)]  Loss: 4.048 (4.05)  Time: 0.707s, 1448.07/s  (0.700s, 1463.88/s)  LR: 9.518e-04  Data: 0.009 (0.013)
Train: 85 [ 900/1251 ( 72%)]  Loss: 3.924 (4.04)  Time: 0.720s, 1421.69/s  (0.699s, 1464.94/s)  LR: 9.518e-04  Data: 0.010 (0.013)
Train: 85 [ 950/1251 ( 76%)]  Loss: 4.013 (4.04)  Time: 0.696s, 1471.69/s  (0.699s, 1464.37/s)  LR: 9.518e-04  Data: 0.010 (0.013)
Train: 85 [1000/1251 ( 80%)]  Loss: 4.066 (4.04)  Time: 0.671s, 1525.07/s  (0.699s, 1464.96/s)  LR: 9.518e-04  Data: 0.011 (0.012)
Train: 85 [1050/1251 ( 84%)]  Loss: 3.754 (4.03)  Time: 0.685s, 1494.22/s  (0.699s, 1464.51/s)  LR: 9.518e-04  Data: 0.010 (0.012)
Train: 85 [1100/1251 ( 88%)]  Loss: 4.309 (4.04)  Time: 0.694s, 1475.04/s  (0.699s, 1465.27/s)  LR: 9.518e-04  Data: 0.010 (0.012)
Train: 85 [1150/1251 ( 92%)]  Loss: 4.176 (4.05)  Time: 0.671s, 1525.77/s  (0.698s, 1466.10/s)  LR: 9.518e-04  Data: 0.010 (0.012)
Train: 85 [1200/1251 ( 96%)]  Loss: 4.050 (4.05)  Time: 0.665s, 1540.67/s  (0.698s, 1466.31/s)  LR: 9.518e-04  Data: 0.009 (0.012)
Train: 85 [1250/1251 (100%)]  Loss: 3.895 (4.04)  Time: 0.663s, 1545.43/s  (0.698s, 1466.96/s)  LR: 9.518e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.471 (1.471)  Loss:  1.0586 (1.0586)  Acc@1: 84.2773 (84.2773)  Acc@5: 95.3125 (95.3125)
Test: [  48/48]  Time: 0.136 (0.590)  Loss:  1.2598 (1.7314)  Acc@1: 81.2500 (67.0520)  Acc@5: 93.5142 (87.9920)
Train: 86 [   0/1251 (  0%)]  Loss: 4.236 (4.24)  Time: 2.251s,  454.91/s  (2.251s,  454.91/s)  LR: 9.507e-04  Data: 1.605 (1.605)
Train: 86 [  50/1251 (  4%)]  Loss: 4.011 (4.12)  Time: 0.672s, 1524.72/s  (0.735s, 1394.07/s)  LR: 9.507e-04  Data: 0.008 (0.048)
Train: 86 [ 100/1251 (  8%)]  Loss: 4.102 (4.12)  Time: 0.668s, 1533.44/s  (0.714s, 1434.09/s)  LR: 9.507e-04  Data: 0.008 (0.029)
Train: 86 [ 150/1251 ( 12%)]  Loss: 4.322 (4.17)  Time: 0.676s, 1515.35/s  (0.709s, 1444.45/s)  LR: 9.507e-04  Data: 0.008 (0.023)
Train: 86 [ 200/1251 ( 16%)]  Loss: 3.877 (4.11)  Time: 0.680s, 1505.90/s  (0.705s, 1453.32/s)  LR: 9.507e-04  Data: 0.011 (0.020)
Train: 86 [ 250/1251 ( 20%)]  Loss: 3.981 (4.09)  Time: 0.695s, 1472.85/s  (0.703s, 1455.90/s)  LR: 9.507e-04  Data: 0.012 (0.018)
Train: 86 [ 300/1251 ( 24%)]  Loss: 4.120 (4.09)  Time: 0.676s, 1514.11/s  (0.701s, 1460.50/s)  LR: 9.507e-04  Data: 0.010 (0.017)
Train: 86 [ 350/1251 ( 28%)]  Loss: 4.118 (4.10)  Time: 0.707s, 1448.41/s  (0.700s, 1462.17/s)  LR: 9.507e-04  Data: 0.014 (0.016)
Train: 86 [ 400/1251 ( 32%)]  Loss: 3.403 (4.02)  Time: 0.664s, 1543.24/s  (0.700s, 1462.44/s)  LR: 9.507e-04  Data: 0.008 (0.015)
Train: 86 [ 450/1251 ( 36%)]  Loss: 3.403 (3.96)  Time: 0.680s, 1506.54/s  (0.699s, 1464.43/s)  LR: 9.507e-04  Data: 0.010 (0.014)
Train: 86 [ 500/1251 ( 40%)]  Loss: 4.139 (3.97)  Time: 0.707s, 1448.94/s  (0.700s, 1463.10/s)  LR: 9.507e-04  Data: 0.011 (0.014)
Train: 86 [ 550/1251 ( 44%)]  Loss: 3.944 (3.97)  Time: 0.673s, 1521.16/s  (0.700s, 1462.94/s)  LR: 9.507e-04  Data: 0.010 (0.014)
Train: 86 [ 600/1251 ( 48%)]  Loss: 3.879 (3.96)  Time: 0.678s, 1510.64/s  (0.700s, 1463.63/s)  LR: 9.507e-04  Data: 0.010 (0.014)
Train: 86 [ 650/1251 ( 52%)]  Loss: 4.339 (3.99)  Time: 0.706s, 1451.09/s  (0.699s, 1464.17/s)  LR: 9.507e-04  Data: 0.010 (0.013)
Train: 86 [ 700/1251 ( 56%)]  Loss: 4.285 (4.01)  Time: 0.742s, 1379.48/s  (0.699s, 1464.30/s)  LR: 9.507e-04  Data: 0.013 (0.013)
Train: 86 [ 750/1251 ( 60%)]  Loss: 3.622 (3.99)  Time: 0.675s, 1517.43/s  (0.699s, 1465.52/s)  LR: 9.507e-04  Data: 0.013 (0.013)
Train: 86 [ 800/1251 ( 64%)]  Loss: 3.810 (3.98)  Time: 0.673s, 1520.57/s  (0.699s, 1465.79/s)  LR: 9.507e-04  Data: 0.010 (0.013)
Train: 86 [ 850/1251 ( 68%)]  Loss: 3.924 (3.97)  Time: 0.758s, 1350.43/s  (0.699s, 1465.73/s)  LR: 9.507e-04  Data: 0.009 (0.013)
Train: 86 [ 900/1251 ( 72%)]  Loss: 3.900 (3.97)  Time: 0.726s, 1410.17/s  (0.699s, 1465.32/s)  LR: 9.507e-04  Data: 0.009 (0.012)
Train: 86 [ 950/1251 ( 76%)]  Loss: 4.037 (3.97)  Time: 0.674s, 1519.07/s  (0.698s, 1466.13/s)  LR: 9.507e-04  Data: 0.011 (0.012)
Train: 86 [1000/1251 ( 80%)]  Loss: 3.527 (3.95)  Time: 0.669s, 1530.00/s  (0.698s, 1466.84/s)  LR: 9.507e-04  Data: 0.010 (0.012)
Train: 86 [1050/1251 ( 84%)]  Loss: 3.925 (3.95)  Time: 0.674s, 1519.29/s  (0.698s, 1467.79/s)  LR: 9.507e-04  Data: 0.014 (0.012)
Train: 86 [1100/1251 ( 88%)]  Loss: 4.095 (3.96)  Time: 0.683s, 1499.23/s  (0.698s, 1467.95/s)  LR: 9.507e-04  Data: 0.010 (0.012)
Train: 86 [1150/1251 ( 92%)]  Loss: 3.880 (3.95)  Time: 0.751s, 1364.06/s  (0.698s, 1467.73/s)  LR: 9.507e-04  Data: 0.010 (0.012)
Train: 86 [1200/1251 ( 96%)]  Loss: 4.271 (3.97)  Time: 0.717s, 1428.98/s  (0.698s, 1467.41/s)  LR: 9.507e-04  Data: 0.010 (0.012)
Train: 86 [1250/1251 (100%)]  Loss: 3.957 (3.97)  Time: 0.703s, 1456.90/s  (0.698s, 1467.52/s)  LR: 9.507e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.626 (1.626)  Loss:  1.2158 (1.2158)  Acc@1: 83.6914 (83.6914)  Acc@5: 95.2148 (95.2148)
Test: [  48/48]  Time: 0.136 (0.566)  Loss:  1.0986 (1.6857)  Acc@1: 81.0142 (67.2500)  Acc@5: 95.4009 (88.3180)
Train: 87 [   0/1251 (  0%)]  Loss: 3.749 (3.75)  Time: 2.093s,  489.31/s  (2.093s,  489.31/s)  LR: 9.495e-04  Data: 1.481 (1.481)
Train: 87 [  50/1251 (  4%)]  Loss: 4.200 (3.97)  Time: 0.728s, 1405.99/s  (0.756s, 1354.41/s)  LR: 9.495e-04  Data: 0.011 (0.047)
Train: 87 [ 100/1251 (  8%)]  Loss: 3.921 (3.96)  Time: 0.677s, 1511.92/s  (0.736s, 1391.55/s)  LR: 9.495e-04  Data: 0.015 (0.030)
Train: 87 [ 150/1251 ( 12%)]  Loss: 3.909 (3.94)  Time: 0.743s, 1378.46/s  (0.728s, 1405.85/s)  LR: 9.495e-04  Data: 0.014 (0.024)
Train: 87 [ 200/1251 ( 16%)]  Loss: 3.792 (3.91)  Time: 0.703s, 1455.67/s  (0.723s, 1416.47/s)  LR: 9.495e-04  Data: 0.010 (0.021)
Train: 87 [ 250/1251 ( 20%)]  Loss: 3.828 (3.90)  Time: 0.672s, 1524.01/s  (0.716s, 1429.44/s)  LR: 9.495e-04  Data: 0.010 (0.019)
Train: 87 [ 300/1251 ( 24%)]  Loss: 3.827 (3.89)  Time: 0.672s, 1523.58/s  (0.713s, 1436.71/s)  LR: 9.495e-04  Data: 0.010 (0.017)
Train: 87 [ 350/1251 ( 28%)]  Loss: 4.182 (3.93)  Time: 0.699s, 1464.54/s  (0.709s, 1444.03/s)  LR: 9.495e-04  Data: 0.010 (0.016)
Train: 87 [ 400/1251 ( 32%)]  Loss: 3.540 (3.88)  Time: 0.672s, 1524.31/s  (0.707s, 1449.34/s)  LR: 9.495e-04  Data: 0.010 (0.016)
Train: 87 [ 450/1251 ( 36%)]  Loss: 3.547 (3.85)  Time: 0.672s, 1523.23/s  (0.704s, 1453.92/s)  LR: 9.495e-04  Data: 0.011 (0.015)
Train: 87 [ 500/1251 ( 40%)]  Loss: 3.757 (3.84)  Time: 0.710s, 1441.88/s  (0.703s, 1456.13/s)  LR: 9.495e-04  Data: 0.009 (0.014)
Train: 87 [ 550/1251 ( 44%)]  Loss: 3.808 (3.84)  Time: 0.708s, 1446.54/s  (0.702s, 1458.82/s)  LR: 9.495e-04  Data: 0.009 (0.014)
Train: 87 [ 600/1251 ( 48%)]  Loss: 3.923 (3.84)  Time: 0.671s, 1525.00/s  (0.701s, 1460.90/s)  LR: 9.495e-04  Data: 0.010 (0.014)
Train: 87 [ 650/1251 ( 52%)]  Loss: 4.060 (3.86)  Time: 0.678s, 1510.97/s  (0.701s, 1461.33/s)  LR: 9.495e-04  Data: 0.014 (0.013)
Train: 87 [ 700/1251 ( 56%)]  Loss: 3.928 (3.86)  Time: 0.677s, 1513.22/s  (0.700s, 1462.22/s)  LR: 9.495e-04  Data: 0.014 (0.013)
Train: 87 [ 750/1251 ( 60%)]  Loss: 4.006 (3.87)  Time: 0.726s, 1409.87/s  (0.700s, 1462.54/s)  LR: 9.495e-04  Data: 0.017 (0.013)
Train: 87 [ 800/1251 ( 64%)]  Loss: 3.882 (3.87)  Time: 0.690s, 1484.81/s  (0.700s, 1462.16/s)  LR: 9.495e-04  Data: 0.012 (0.013)
Train: 87 [ 850/1251 ( 68%)]  Loss: 3.729 (3.87)  Time: 0.673s, 1522.28/s  (0.700s, 1463.55/s)  LR: 9.495e-04  Data: 0.010 (0.013)
Train: 87 [ 900/1251 ( 72%)]  Loss: 4.053 (3.88)  Time: 0.705s, 1452.69/s  (0.699s, 1464.46/s)  LR: 9.495e-04  Data: 0.011 (0.013)
Train: 87 [ 950/1251 ( 76%)]  Loss: 4.058 (3.88)  Time: 0.673s, 1520.98/s  (0.699s, 1465.50/s)  LR: 9.495e-04  Data: 0.010 (0.012)
Train: 87 [1000/1251 ( 80%)]  Loss: 3.914 (3.89)  Time: 0.673s, 1520.77/s  (0.698s, 1466.11/s)  LR: 9.495e-04  Data: 0.010 (0.012)
Train: 87 [1050/1251 ( 84%)]  Loss: 4.145 (3.90)  Time: 0.719s, 1423.60/s  (0.698s, 1466.42/s)  LR: 9.495e-04  Data: 0.010 (0.012)
Train: 87 [1100/1251 ( 88%)]  Loss: 3.707 (3.89)  Time: 0.708s, 1446.28/s  (0.698s, 1467.08/s)  LR: 9.495e-04  Data: 0.010 (0.012)
Train: 87 [1150/1251 ( 92%)]  Loss: 3.676 (3.88)  Time: 0.710s, 1442.34/s  (0.698s, 1466.94/s)  LR: 9.495e-04  Data: 0.009 (0.012)
Train: 87 [1200/1251 ( 96%)]  Loss: 4.046 (3.89)  Time: 0.741s, 1382.47/s  (0.698s, 1467.93/s)  LR: 9.495e-04  Data: 0.010 (0.012)
Train: 87 [1250/1251 (100%)]  Loss: 4.180 (3.90)  Time: 0.699s, 1465.97/s  (0.698s, 1467.80/s)  LR: 9.495e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.554 (1.554)  Loss:  1.0391 (1.0391)  Acc@1: 85.4492 (85.4492)  Acc@5: 95.8008 (95.8008)
Test: [  48/48]  Time: 0.136 (0.581)  Loss:  1.2471 (1.6334)  Acc@1: 79.0094 (68.3540)  Acc@5: 93.1604 (88.9780)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-83.pth.tar', 68.6419999609375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-82.pth.tar', 68.63999993652344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-80.pth.tar', 68.60600010253906)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-77.pth.tar', 68.59599996582031)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-76.pth.tar', 68.49400010742187)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-87.pth.tar', 68.35399993164063)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-75.pth.tar', 68.30599997558593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-84.pth.tar', 68.18999994384765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-74.pth.tar', 68.11000010009765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-79.pth.tar', 68.09200002197265)

Train: 88 [   0/1251 (  0%)]  Loss: 3.645 (3.65)  Time: 2.317s,  441.95/s  (2.317s,  441.95/s)  LR: 9.484e-04  Data: 1.662 (1.662)
Train: 88 [  50/1251 (  4%)]  Loss: 3.689 (3.67)  Time: 0.709s, 1443.43/s  (0.727s, 1409.13/s)  LR: 9.484e-04  Data: 0.009 (0.044)
Train: 88 [ 100/1251 (  8%)]  Loss: 3.990 (3.77)  Time: 0.697s, 1469.06/s  (0.710s, 1442.24/s)  LR: 9.484e-04  Data: 0.010 (0.027)
Train: 88 [ 150/1251 ( 12%)]  Loss: 3.597 (3.73)  Time: 0.699s, 1465.31/s  (0.705s, 1451.53/s)  LR: 9.484e-04  Data: 0.010 (0.022)
Train: 88 [ 200/1251 ( 16%)]  Loss: 4.138 (3.81)  Time: 0.704s, 1454.81/s  (0.702s, 1458.27/s)  LR: 9.484e-04  Data: 0.009 (0.019)
Train: 88 [ 250/1251 ( 20%)]  Loss: 3.920 (3.83)  Time: 0.703s, 1456.44/s  (0.700s, 1462.26/s)  LR: 9.484e-04  Data: 0.010 (0.017)
Train: 88 [ 300/1251 ( 24%)]  Loss: 4.180 (3.88)  Time: 0.671s, 1526.18/s  (0.699s, 1465.93/s)  LR: 9.484e-04  Data: 0.010 (0.016)
Train: 88 [ 350/1251 ( 28%)]  Loss: 4.413 (3.95)  Time: 0.687s, 1491.12/s  (0.698s, 1467.62/s)  LR: 9.484e-04  Data: 0.013 (0.015)
Train: 88 [ 400/1251 ( 32%)]  Loss: 4.175 (3.97)  Time: 0.766s, 1337.15/s  (0.698s, 1467.36/s)  LR: 9.484e-04  Data: 0.013 (0.015)
Train: 88 [ 450/1251 ( 36%)]  Loss: 4.151 (3.99)  Time: 0.676s, 1515.51/s  (0.698s, 1467.07/s)  LR: 9.484e-04  Data: 0.014 (0.014)
Train: 88 [ 500/1251 ( 40%)]  Loss: 3.892 (3.98)  Time: 0.691s, 1482.39/s  (0.698s, 1466.55/s)  LR: 9.484e-04  Data: 0.009 (0.014)
Train: 88 [ 550/1251 ( 44%)]  Loss: 4.034 (3.99)  Time: 0.679s, 1509.04/s  (0.698s, 1466.70/s)  LR: 9.484e-04  Data: 0.011 (0.014)
Train: 88 [ 600/1251 ( 48%)]  Loss: 4.285 (4.01)  Time: 0.684s, 1496.81/s  (0.698s, 1466.88/s)  LR: 9.484e-04  Data: 0.014 (0.013)
Train: 88 [ 650/1251 ( 52%)]  Loss: 4.056 (4.01)  Time: 0.696s, 1470.63/s  (0.698s, 1466.63/s)  LR: 9.484e-04  Data: 0.010 (0.013)
Train: 88 [ 700/1251 ( 56%)]  Loss: 3.647 (3.99)  Time: 0.674s, 1518.65/s  (0.698s, 1468.03/s)  LR: 9.484e-04  Data: 0.011 (0.013)
Train: 88 [ 750/1251 ( 60%)]  Loss: 3.991 (3.99)  Time: 0.705s, 1452.91/s  (0.698s, 1467.53/s)  LR: 9.484e-04  Data: 0.010 (0.013)
Train: 88 [ 800/1251 ( 64%)]  Loss: 3.854 (3.98)  Time: 0.666s, 1537.26/s  (0.697s, 1468.65/s)  LR: 9.484e-04  Data: 0.010 (0.013)
Train: 88 [ 850/1251 ( 68%)]  Loss: 3.631 (3.96)  Time: 0.674s, 1519.63/s  (0.697s, 1468.59/s)  LR: 9.484e-04  Data: 0.011 (0.012)
Train: 88 [ 900/1251 ( 72%)]  Loss: 4.180 (3.97)  Time: 0.677s, 1511.92/s  (0.697s, 1469.46/s)  LR: 9.484e-04  Data: 0.013 (0.012)
Train: 88 [ 950/1251 ( 76%)]  Loss: 4.041 (3.98)  Time: 0.673s, 1522.61/s  (0.696s, 1470.36/s)  LR: 9.484e-04  Data: 0.010 (0.012)
Train: 88 [1000/1251 ( 80%)]  Loss: 3.409 (3.95)  Time: 0.672s, 1523.48/s  (0.696s, 1471.08/s)  LR: 9.484e-04  Data: 0.011 (0.012)
Train: 88 [1050/1251 ( 84%)]  Loss: 4.060 (3.95)  Time: 0.709s, 1445.28/s  (0.696s, 1470.76/s)  LR: 9.484e-04  Data: 0.010 (0.012)
Train: 88 [1100/1251 ( 88%)]  Loss: 3.898 (3.95)  Time: 0.694s, 1475.82/s  (0.696s, 1471.13/s)  LR: 9.484e-04  Data: 0.009 (0.012)
Train: 88 [1150/1251 ( 92%)]  Loss: 3.881 (3.95)  Time: 0.681s, 1504.35/s  (0.696s, 1471.35/s)  LR: 9.484e-04  Data: 0.010 (0.012)
Train: 88 [1200/1251 ( 96%)]  Loss: 3.969 (3.95)  Time: 0.666s, 1538.36/s  (0.696s, 1471.14/s)  LR: 9.484e-04  Data: 0.010 (0.012)
Train: 88 [1250/1251 (100%)]  Loss: 4.057 (3.95)  Time: 0.692s, 1480.77/s  (0.696s, 1471.32/s)  LR: 9.484e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.491 (1.491)  Loss:  1.2568 (1.2568)  Acc@1: 83.4961 (83.4961)  Acc@5: 95.8984 (95.8984)
Test: [  48/48]  Time: 0.136 (0.579)  Loss:  1.2764 (1.7327)  Acc@1: 80.8962 (67.4440)  Acc@5: 93.8679 (88.3060)
Train: 89 [   0/1251 (  0%)]  Loss: 3.804 (3.80)  Time: 2.508s,  408.34/s  (2.508s,  408.34/s)  LR: 9.472e-04  Data: 1.888 (1.888)
Train: 89 [  50/1251 (  4%)]  Loss: 4.347 (4.08)  Time: 0.671s, 1525.28/s  (0.737s, 1390.08/s)  LR: 9.472e-04  Data: 0.010 (0.052)
Train: 89 [ 100/1251 (  8%)]  Loss: 3.835 (4.00)  Time: 0.703s, 1455.86/s  (0.713s, 1435.65/s)  LR: 9.472e-04  Data: 0.009 (0.031)
Train: 89 [ 150/1251 ( 12%)]  Loss: 3.696 (3.92)  Time: 0.666s, 1536.88/s  (0.706s, 1450.84/s)  LR: 9.472e-04  Data: 0.009 (0.024)
Train: 89 [ 200/1251 ( 16%)]  Loss: 3.920 (3.92)  Time: 0.675s, 1516.09/s  (0.703s, 1456.09/s)  LR: 9.472e-04  Data: 0.010 (0.021)
Train: 89 [ 250/1251 ( 20%)]  Loss: 3.749 (3.89)  Time: 0.676s, 1515.58/s  (0.700s, 1462.35/s)  LR: 9.472e-04  Data: 0.010 (0.019)
Train: 89 [ 300/1251 ( 24%)]  Loss: 3.945 (3.90)  Time: 0.802s, 1276.61/s  (0.699s, 1465.79/s)  LR: 9.472e-04  Data: 0.009 (0.017)
Train: 89 [ 350/1251 ( 28%)]  Loss: 4.009 (3.91)  Time: 0.676s, 1515.52/s  (0.699s, 1465.35/s)  LR: 9.472e-04  Data: 0.011 (0.016)
Train: 89 [ 400/1251 ( 32%)]  Loss: 4.272 (3.95)  Time: 0.706s, 1449.63/s  (0.699s, 1465.81/s)  LR: 9.472e-04  Data: 0.013 (0.015)
Train: 89 [ 450/1251 ( 36%)]  Loss: 3.368 (3.89)  Time: 0.751s, 1363.03/s  (0.697s, 1468.54/s)  LR: 9.472e-04  Data: 0.010 (0.015)
Train: 89 [ 500/1251 ( 40%)]  Loss: 3.568 (3.86)  Time: 0.675s, 1515.96/s  (0.697s, 1469.92/s)  LR: 9.472e-04  Data: 0.011 (0.014)
Train: 89 [ 550/1251 ( 44%)]  Loss: 4.254 (3.90)  Time: 0.675s, 1516.59/s  (0.696s, 1471.15/s)  LR: 9.472e-04  Data: 0.010 (0.014)
Train: 89 [ 600/1251 ( 48%)]  Loss: 4.012 (3.91)  Time: 0.702s, 1457.74/s  (0.695s, 1472.37/s)  LR: 9.472e-04  Data: 0.011 (0.014)
Train: 89 [ 650/1251 ( 52%)]  Loss: 3.946 (3.91)  Time: 0.675s, 1518.12/s  (0.695s, 1473.19/s)  LR: 9.472e-04  Data: 0.010 (0.013)
Train: 89 [ 700/1251 ( 56%)]  Loss: 3.589 (3.89)  Time: 0.714s, 1434.71/s  (0.695s, 1472.49/s)  LR: 9.472e-04  Data: 0.012 (0.013)
Train: 89 [ 750/1251 ( 60%)]  Loss: 4.286 (3.91)  Time: 0.670s, 1527.69/s  (0.696s, 1472.30/s)  LR: 9.472e-04  Data: 0.010 (0.013)
Train: 89 [ 800/1251 ( 64%)]  Loss: 4.054 (3.92)  Time: 0.708s, 1445.51/s  (0.695s, 1473.05/s)  LR: 9.472e-04  Data: 0.011 (0.013)
Train: 89 [ 850/1251 ( 68%)]  Loss: 3.610 (3.90)  Time: 0.679s, 1507.26/s  (0.695s, 1473.00/s)  LR: 9.472e-04  Data: 0.010 (0.013)
Train: 89 [ 900/1251 ( 72%)]  Loss: 3.909 (3.90)  Time: 0.700s, 1461.97/s  (0.695s, 1473.75/s)  LR: 9.472e-04  Data: 0.009 (0.013)
Train: 89 [ 950/1251 ( 76%)]  Loss: 3.790 (3.90)  Time: 0.720s, 1422.54/s  (0.695s, 1473.23/s)  LR: 9.472e-04  Data: 0.010 (0.012)
Train: 89 [1000/1251 ( 80%)]  Loss: 4.131 (3.91)  Time: 0.678s, 1510.03/s  (0.695s, 1473.42/s)  LR: 9.472e-04  Data: 0.009 (0.012)
Train: 89 [1050/1251 ( 84%)]  Loss: 3.948 (3.91)  Time: 0.685s, 1494.56/s  (0.695s, 1473.56/s)  LR: 9.472e-04  Data: 0.009 (0.012)
Train: 89 [1100/1251 ( 88%)]  Loss: 4.114 (3.92)  Time: 0.672s, 1524.40/s  (0.695s, 1473.71/s)  LR: 9.472e-04  Data: 0.010 (0.012)
Train: 89 [1150/1251 ( 92%)]  Loss: 3.681 (3.91)  Time: 0.673s, 1521.81/s  (0.695s, 1473.80/s)  LR: 9.472e-04  Data: 0.011 (0.012)
Train: 89 [1200/1251 ( 96%)]  Loss: 3.889 (3.91)  Time: 0.672s, 1523.18/s  (0.695s, 1474.06/s)  LR: 9.472e-04  Data: 0.010 (0.012)
Train: 89 [1250/1251 (100%)]  Loss: 3.995 (3.91)  Time: 0.657s, 1559.60/s  (0.695s, 1474.08/s)  LR: 9.472e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.588 (1.588)  Loss:  0.9971 (0.9971)  Acc@1: 85.1562 (85.1562)  Acc@5: 96.2891 (96.2891)
Test: [  48/48]  Time: 0.136 (0.588)  Loss:  0.9873 (1.6498)  Acc@1: 81.6038 (68.5120)  Acc@5: 95.1651 (88.8620)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-83.pth.tar', 68.6419999609375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-82.pth.tar', 68.63999993652344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-80.pth.tar', 68.60600010253906)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-77.pth.tar', 68.59599996582031)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-89.pth.tar', 68.51200002441406)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-76.pth.tar', 68.49400010742187)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-87.pth.tar', 68.35399993164063)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-75.pth.tar', 68.30599997558593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-84.pth.tar', 68.18999994384765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-74.pth.tar', 68.11000010009765)

Train: 90 [   0/1251 (  0%)]  Loss: 3.948 (3.95)  Time: 2.150s,  476.30/s  (2.150s,  476.30/s)  LR: 9.460e-04  Data: 1.534 (1.534)
Train: 90 [  50/1251 (  4%)]  Loss: 3.830 (3.89)  Time: 0.682s, 1502.44/s  (0.734s, 1394.57/s)  LR: 9.460e-04  Data: 0.009 (0.049)
Train: 90 [ 100/1251 (  8%)]  Loss: 3.450 (3.74)  Time: 0.691s, 1481.24/s  (0.714s, 1433.67/s)  LR: 9.460e-04  Data: 0.011 (0.030)
Train: 90 [ 150/1251 ( 12%)]  Loss: 4.062 (3.82)  Time: 0.672s, 1524.09/s  (0.708s, 1446.60/s)  LR: 9.460e-04  Data: 0.009 (0.023)
Train: 90 [ 200/1251 ( 16%)]  Loss: 4.238 (3.91)  Time: 0.704s, 1454.82/s  (0.706s, 1451.15/s)  LR: 9.460e-04  Data: 0.009 (0.020)
Train: 90 [ 250/1251 ( 20%)]  Loss: 4.080 (3.93)  Time: 0.673s, 1521.68/s  (0.701s, 1460.54/s)  LR: 9.460e-04  Data: 0.011 (0.018)
Train: 90 [ 300/1251 ( 24%)]  Loss: 4.015 (3.95)  Time: 0.673s, 1522.62/s  (0.699s, 1464.89/s)  LR: 9.460e-04  Data: 0.010 (0.017)
Train: 90 [ 350/1251 ( 28%)]  Loss: 4.193 (3.98)  Time: 0.696s, 1470.31/s  (0.697s, 1469.32/s)  LR: 9.460e-04  Data: 0.009 (0.016)
Train: 90 [ 400/1251 ( 32%)]  Loss: 4.370 (4.02)  Time: 0.705s, 1451.73/s  (0.696s, 1470.36/s)  LR: 9.460e-04  Data: 0.011 (0.015)
Train: 90 [ 450/1251 ( 36%)]  Loss: 4.087 (4.03)  Time: 0.717s, 1428.14/s  (0.696s, 1471.70/s)  LR: 9.460e-04  Data: 0.009 (0.015)
Train: 90 [ 500/1251 ( 40%)]  Loss: 3.910 (4.02)  Time: 0.753s, 1359.34/s  (0.696s, 1472.18/s)  LR: 9.460e-04  Data: 0.014 (0.014)
Train: 90 [ 550/1251 ( 44%)]  Loss: 4.198 (4.03)  Time: 0.672s, 1523.17/s  (0.695s, 1472.76/s)  LR: 9.460e-04  Data: 0.010 (0.014)
Train: 90 [ 600/1251 ( 48%)]  Loss: 3.848 (4.02)  Time: 0.673s, 1522.52/s  (0.695s, 1472.84/s)  LR: 9.460e-04  Data: 0.011 (0.013)
Train: 90 [ 650/1251 ( 52%)]  Loss: 3.999 (4.02)  Time: 0.672s, 1523.95/s  (0.695s, 1474.25/s)  LR: 9.460e-04  Data: 0.010 (0.013)
Train: 90 [ 700/1251 ( 56%)]  Loss: 4.052 (4.02)  Time: 0.691s, 1481.83/s  (0.694s, 1474.74/s)  LR: 9.460e-04  Data: 0.010 (0.013)
Train: 90 [ 750/1251 ( 60%)]  Loss: 4.135 (4.03)  Time: 0.703s, 1455.68/s  (0.695s, 1473.62/s)  LR: 9.460e-04  Data: 0.009 (0.013)
Train: 90 [ 800/1251 ( 64%)]  Loss: 3.760 (4.01)  Time: 0.673s, 1521.78/s  (0.694s, 1475.06/s)  LR: 9.460e-04  Data: 0.011 (0.013)
Train: 90 [ 850/1251 ( 68%)]  Loss: 3.548 (3.98)  Time: 0.677s, 1511.92/s  (0.694s, 1474.80/s)  LR: 9.460e-04  Data: 0.009 (0.013)
Train: 90 [ 900/1251 ( 72%)]  Loss: 4.096 (3.99)  Time: 0.696s, 1472.22/s  (0.694s, 1475.53/s)  LR: 9.460e-04  Data: 0.010 (0.012)
Train: 90 [ 950/1251 ( 76%)]  Loss: 3.808 (3.98)  Time: 0.742s, 1380.82/s  (0.694s, 1474.76/s)  LR: 9.460e-04  Data: 0.009 (0.012)
Train: 90 [1000/1251 ( 80%)]  Loss: 4.077 (3.99)  Time: 0.717s, 1428.02/s  (0.694s, 1475.09/s)  LR: 9.460e-04  Data: 0.010 (0.012)
Train: 90 [1050/1251 ( 84%)]  Loss: 3.679 (3.97)  Time: 0.681s, 1503.31/s  (0.695s, 1474.15/s)  LR: 9.460e-04  Data: 0.011 (0.012)
Train: 90 [1100/1251 ( 88%)]  Loss: 4.019 (3.97)  Time: 0.735s, 1393.98/s  (0.695s, 1474.36/s)  LR: 9.460e-04  Data: 0.011 (0.012)
Train: 90 [1150/1251 ( 92%)]  Loss: 3.839 (3.97)  Time: 0.703s, 1456.60/s  (0.695s, 1474.23/s)  LR: 9.460e-04  Data: 0.010 (0.012)
Train: 90 [1200/1251 ( 96%)]  Loss: 3.924 (3.97)  Time: 0.669s, 1529.87/s  (0.695s, 1474.13/s)  LR: 9.460e-04  Data: 0.010 (0.012)
Train: 90 [1250/1251 (100%)]  Loss: 3.795 (3.96)  Time: 0.654s, 1566.16/s  (0.695s, 1474.25/s)  LR: 9.460e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.650 (1.650)  Loss:  1.0459 (1.0459)  Acc@1: 84.3750 (84.3750)  Acc@5: 95.8008 (95.8008)
Test: [  48/48]  Time: 0.136 (0.579)  Loss:  1.0557 (1.6237)  Acc@1: 82.1934 (68.2860)  Acc@5: 94.9293 (88.6340)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-83.pth.tar', 68.6419999609375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-82.pth.tar', 68.63999993652344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-80.pth.tar', 68.60600010253906)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-77.pth.tar', 68.59599996582031)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-89.pth.tar', 68.51200002441406)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-76.pth.tar', 68.49400010742187)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-87.pth.tar', 68.35399993164063)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-75.pth.tar', 68.30599997558593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-90.pth.tar', 68.28600002197265)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-84.pth.tar', 68.18999994384765)

Train: 91 [   0/1251 (  0%)]  Loss: 3.942 (3.94)  Time: 2.208s,  463.70/s  (2.208s,  463.70/s)  LR: 9.449e-04  Data: 1.592 (1.592)
Train: 91 [  50/1251 (  4%)]  Loss: 3.969 (3.96)  Time: 0.677s, 1511.88/s  (0.733s, 1396.93/s)  LR: 9.449e-04  Data: 0.010 (0.048)
Train: 91 [ 100/1251 (  8%)]  Loss: 4.086 (4.00)  Time: 0.676s, 1514.05/s  (0.715s, 1431.47/s)  LR: 9.449e-04  Data: 0.010 (0.029)
Train: 91 [ 150/1251 ( 12%)]  Loss: 3.847 (3.96)  Time: 0.709s, 1444.84/s  (0.706s, 1449.46/s)  LR: 9.449e-04  Data: 0.012 (0.023)
Train: 91 [ 200/1251 ( 16%)]  Loss: 3.856 (3.94)  Time: 0.671s, 1526.51/s  (0.702s, 1457.75/s)  LR: 9.449e-04  Data: 0.010 (0.020)
Train: 91 [ 250/1251 ( 20%)]  Loss: 4.242 (3.99)  Time: 0.714s, 1433.53/s  (0.700s, 1463.60/s)  LR: 9.449e-04  Data: 0.009 (0.018)
Train: 91 [ 300/1251 ( 24%)]  Loss: 3.419 (3.91)  Time: 0.678s, 1509.79/s  (0.697s, 1468.52/s)  LR: 9.449e-04  Data: 0.010 (0.017)
Train: 91 [ 350/1251 ( 28%)]  Loss: 3.795 (3.89)  Time: 0.673s, 1521.76/s  (0.696s, 1470.47/s)  LR: 9.449e-04  Data: 0.010 (0.016)
Train: 91 [ 400/1251 ( 32%)]  Loss: 4.111 (3.92)  Time: 0.672s, 1524.86/s  (0.697s, 1469.87/s)  LR: 9.449e-04  Data: 0.010 (0.015)
Train: 91 [ 450/1251 ( 36%)]  Loss: 3.922 (3.92)  Time: 0.748s, 1368.14/s  (0.697s, 1469.30/s)  LR: 9.449e-04  Data: 0.012 (0.014)
Train: 91 [ 500/1251 ( 40%)]  Loss: 4.282 (3.95)  Time: 0.667s, 1534.25/s  (0.696s, 1470.66/s)  LR: 9.449e-04  Data: 0.012 (0.014)
Train: 91 [ 550/1251 ( 44%)]  Loss: 4.010 (3.96)  Time: 0.670s, 1529.40/s  (0.696s, 1471.73/s)  LR: 9.449e-04  Data: 0.009 (0.014)
Train: 91 [ 600/1251 ( 48%)]  Loss: 3.987 (3.96)  Time: 0.670s, 1527.92/s  (0.695s, 1472.75/s)  LR: 9.449e-04  Data: 0.009 (0.013)
Train: 91 [ 650/1251 ( 52%)]  Loss: 3.945 (3.96)  Time: 0.702s, 1458.35/s  (0.696s, 1472.22/s)  LR: 9.449e-04  Data: 0.009 (0.013)
Train: 91 [ 700/1251 ( 56%)]  Loss: 4.165 (3.97)  Time: 0.724s, 1414.17/s  (0.696s, 1470.92/s)  LR: 9.449e-04  Data: 0.013 (0.013)
Train: 91 [ 750/1251 ( 60%)]  Loss: 3.926 (3.97)  Time: 0.831s, 1231.85/s  (0.696s, 1471.53/s)  LR: 9.449e-04  Data: 0.010 (0.013)
Train: 91 [ 800/1251 ( 64%)]  Loss: 3.900 (3.96)  Time: 0.722s, 1417.84/s  (0.695s, 1472.37/s)  LR: 9.449e-04  Data: 0.010 (0.013)
Train: 91 [ 850/1251 ( 68%)]  Loss: 4.159 (3.98)  Time: 0.680s, 1506.17/s  (0.695s, 1472.77/s)  LR: 9.449e-04  Data: 0.010 (0.013)
Train: 91 [ 900/1251 ( 72%)]  Loss: 4.081 (3.98)  Time: 0.713s, 1435.22/s  (0.696s, 1472.12/s)  LR: 9.449e-04  Data: 0.011 (0.012)
Train: 91 [ 950/1251 ( 76%)]  Loss: 3.804 (3.97)  Time: 0.712s, 1438.56/s  (0.695s, 1472.46/s)  LR: 9.449e-04  Data: 0.016 (0.012)
Train: 91 [1000/1251 ( 80%)]  Loss: 3.985 (3.97)  Time: 0.673s, 1521.54/s  (0.695s, 1472.68/s)  LR: 9.449e-04  Data: 0.011 (0.012)
Train: 91 [1050/1251 ( 84%)]  Loss: 3.986 (3.97)  Time: 0.672s, 1524.58/s  (0.695s, 1473.07/s)  LR: 9.449e-04  Data: 0.010 (0.012)
Train: 91 [1100/1251 ( 88%)]  Loss: 4.006 (3.97)  Time: 0.705s, 1452.93/s  (0.695s, 1473.12/s)  LR: 9.449e-04  Data: 0.010 (0.012)
Train: 91 [1150/1251 ( 92%)]  Loss: 3.832 (3.97)  Time: 0.695s, 1473.49/s  (0.695s, 1473.16/s)  LR: 9.449e-04  Data: 0.010 (0.012)
Train: 91 [1200/1251 ( 96%)]  Loss: 4.214 (3.98)  Time: 0.704s, 1455.50/s  (0.695s, 1473.68/s)  LR: 9.449e-04  Data: 0.010 (0.012)
Train: 91 [1250/1251 (100%)]  Loss: 3.712 (3.97)  Time: 0.660s, 1551.94/s  (0.695s, 1473.53/s)  LR: 9.449e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.451 (1.451)  Loss:  1.0645 (1.0645)  Acc@1: 85.0586 (85.0586)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.136 (0.582)  Loss:  1.1240 (1.6417)  Acc@1: 80.5425 (68.3760)  Acc@5: 94.2217 (88.6840)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-83.pth.tar', 68.6419999609375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-82.pth.tar', 68.63999993652344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-80.pth.tar', 68.60600010253906)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-77.pth.tar', 68.59599996582031)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-89.pth.tar', 68.51200002441406)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-76.pth.tar', 68.49400010742187)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-91.pth.tar', 68.3760000805664)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-87.pth.tar', 68.35399993164063)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-75.pth.tar', 68.30599997558593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-90.pth.tar', 68.28600002197265)

Train: 92 [   0/1251 (  0%)]  Loss: 3.753 (3.75)  Time: 2.355s,  434.73/s  (2.355s,  434.73/s)  LR: 9.437e-04  Data: 1.723 (1.723)
Train: 92 [  50/1251 (  4%)]  Loss: 4.137 (3.95)  Time: 0.675s, 1515.96/s  (0.729s, 1404.90/s)  LR: 9.437e-04  Data: 0.010 (0.047)
Train: 92 [ 100/1251 (  8%)]  Loss: 3.614 (3.83)  Time: 0.673s, 1521.13/s  (0.709s, 1443.43/s)  LR: 9.437e-04  Data: 0.011 (0.029)
Train: 92 [ 150/1251 ( 12%)]  Loss: 4.080 (3.90)  Time: 0.695s, 1473.23/s  (0.707s, 1448.99/s)  LR: 9.437e-04  Data: 0.009 (0.023)
Train: 92 [ 200/1251 ( 16%)]  Loss: 3.827 (3.88)  Time: 0.724s, 1414.52/s  (0.704s, 1453.92/s)  LR: 9.437e-04  Data: 0.010 (0.020)
Train: 92 [ 250/1251 ( 20%)]  Loss: 4.053 (3.91)  Time: 0.672s, 1524.80/s  (0.702s, 1457.86/s)  LR: 9.437e-04  Data: 0.010 (0.018)
Train: 92 [ 300/1251 ( 24%)]  Loss: 3.904 (3.91)  Time: 0.672s, 1523.88/s  (0.700s, 1462.90/s)  LR: 9.437e-04  Data: 0.010 (0.017)
Train: 92 [ 350/1251 ( 28%)]  Loss: 4.027 (3.92)  Time: 0.710s, 1441.41/s  (0.699s, 1465.45/s)  LR: 9.437e-04  Data: 0.011 (0.016)
Train: 92 [ 400/1251 ( 32%)]  Loss: 3.964 (3.93)  Time: 0.700s, 1462.91/s  (0.698s, 1467.23/s)  LR: 9.437e-04  Data: 0.010 (0.015)
Train: 92 [ 450/1251 ( 36%)]  Loss: 3.885 (3.92)  Time: 0.672s, 1523.09/s  (0.697s, 1469.17/s)  LR: 9.437e-04  Data: 0.010 (0.015)
Train: 92 [ 500/1251 ( 40%)]  Loss: 3.621 (3.90)  Time: 0.695s, 1472.60/s  (0.697s, 1469.59/s)  LR: 9.437e-04  Data: 0.010 (0.014)
Train: 92 [ 550/1251 ( 44%)]  Loss: 3.916 (3.90)  Time: 0.704s, 1454.94/s  (0.696s, 1470.81/s)  LR: 9.437e-04  Data: 0.009 (0.014)
Train: 92 [ 600/1251 ( 48%)]  Loss: 4.238 (3.92)  Time: 0.678s, 1510.90/s  (0.697s, 1469.42/s)  LR: 9.437e-04  Data: 0.011 (0.013)
Train: 92 [ 650/1251 ( 52%)]  Loss: 4.275 (3.95)  Time: 0.681s, 1502.84/s  (0.697s, 1470.20/s)  LR: 9.437e-04  Data: 0.011 (0.013)
Train: 92 [ 700/1251 ( 56%)]  Loss: 3.885 (3.95)  Time: 0.703s, 1457.30/s  (0.696s, 1471.22/s)  LR: 9.437e-04  Data: 0.009 (0.013)
Train: 92 [ 750/1251 ( 60%)]  Loss: 3.647 (3.93)  Time: 0.669s, 1529.64/s  (0.696s, 1472.30/s)  LR: 9.437e-04  Data: 0.010 (0.013)
Train: 92 [ 800/1251 ( 64%)]  Loss: 3.666 (3.91)  Time: 0.705s, 1452.32/s  (0.695s, 1473.02/s)  LR: 9.437e-04  Data: 0.010 (0.013)
Train: 92 [ 850/1251 ( 68%)]  Loss: 3.771 (3.90)  Time: 0.711s, 1440.75/s  (0.695s, 1473.72/s)  LR: 9.437e-04  Data: 0.010 (0.013)
Train: 92 [ 900/1251 ( 72%)]  Loss: 4.085 (3.91)  Time: 0.706s, 1449.94/s  (0.695s, 1472.66/s)  LR: 9.437e-04  Data: 0.009 (0.012)
Train: 92 [ 950/1251 ( 76%)]  Loss: 3.815 (3.91)  Time: 0.688s, 1488.10/s  (0.696s, 1471.88/s)  LR: 9.437e-04  Data: 0.009 (0.012)
Train: 92 [1000/1251 ( 80%)]  Loss: 3.930 (3.91)  Time: 0.667s, 1536.31/s  (0.696s, 1472.23/s)  LR: 9.437e-04  Data: 0.011 (0.012)
Train: 92 [1050/1251 ( 84%)]  Loss: 3.869 (3.91)  Time: 0.719s, 1424.67/s  (0.695s, 1472.60/s)  LR: 9.437e-04  Data: 0.010 (0.012)
Train: 92 [1100/1251 ( 88%)]  Loss: 4.167 (3.92)  Time: 0.718s, 1427.04/s  (0.696s, 1471.92/s)  LR: 9.437e-04  Data: 0.010 (0.012)
Train: 92 [1150/1251 ( 92%)]  Loss: 3.797 (3.91)  Time: 0.703s, 1456.96/s  (0.695s, 1472.72/s)  LR: 9.437e-04  Data: 0.009 (0.012)
Train: 92 [1200/1251 ( 96%)]  Loss: 4.381 (3.93)  Time: 0.681s, 1503.78/s  (0.695s, 1472.72/s)  LR: 9.437e-04  Data: 0.009 (0.012)
Train: 92 [1250/1251 (100%)]  Loss: 4.019 (3.94)  Time: 0.705s, 1452.12/s  (0.696s, 1472.28/s)  LR: 9.437e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.566 (1.566)  Loss:  0.9824 (0.9824)  Acc@1: 85.4492 (85.4492)  Acc@5: 95.8008 (95.8008)
Test: [  48/48]  Time: 0.137 (0.585)  Loss:  1.0186 (1.5701)  Acc@1: 81.9576 (69.2280)  Acc@5: 94.2217 (89.4380)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-92.pth.tar', 69.22800004882812)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-83.pth.tar', 68.6419999609375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-82.pth.tar', 68.63999993652344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-80.pth.tar', 68.60600010253906)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-77.pth.tar', 68.59599996582031)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-89.pth.tar', 68.51200002441406)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-76.pth.tar', 68.49400010742187)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-91.pth.tar', 68.3760000805664)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-87.pth.tar', 68.35399993164063)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-75.pth.tar', 68.30599997558593)

Train: 93 [   0/1251 (  0%)]  Loss: 3.986 (3.99)  Time: 2.287s,  447.74/s  (2.287s,  447.74/s)  LR: 9.425e-04  Data: 1.671 (1.671)
Train: 93 [  50/1251 (  4%)]  Loss: 3.791 (3.89)  Time: 0.691s, 1482.02/s  (0.729s, 1404.06/s)  LR: 9.425e-04  Data: 0.010 (0.048)
Train: 93 [ 100/1251 (  8%)]  Loss: 3.727 (3.83)  Time: 0.671s, 1525.76/s  (0.715s, 1432.30/s)  LR: 9.425e-04  Data: 0.010 (0.029)
Train: 93 [ 150/1251 ( 12%)]  Loss: 4.065 (3.89)  Time: 0.672s, 1523.33/s  (0.706s, 1449.87/s)  LR: 9.425e-04  Data: 0.011 (0.023)
Train: 93 [ 200/1251 ( 16%)]  Loss: 3.778 (3.87)  Time: 0.672s, 1523.37/s  (0.703s, 1457.64/s)  LR: 9.425e-04  Data: 0.009 (0.020)
Train: 93 [ 250/1251 ( 20%)]  Loss: 3.999 (3.89)  Time: 0.671s, 1526.95/s  (0.702s, 1459.42/s)  LR: 9.425e-04  Data: 0.009 (0.018)
Train: 93 [ 300/1251 ( 24%)]  Loss: 3.591 (3.85)  Time: 0.684s, 1496.84/s  (0.701s, 1459.97/s)  LR: 9.425e-04  Data: 0.009 (0.017)
Train: 93 [ 350/1251 ( 28%)]  Loss: 3.614 (3.82)  Time: 0.705s, 1451.76/s  (0.700s, 1463.75/s)  LR: 9.425e-04  Data: 0.009 (0.016)
Train: 93 [ 400/1251 ( 32%)]  Loss: 3.833 (3.82)  Time: 0.676s, 1514.84/s  (0.698s, 1466.49/s)  LR: 9.425e-04  Data: 0.010 (0.015)
Train: 93 [ 450/1251 ( 36%)]  Loss: 3.789 (3.82)  Time: 0.720s, 1421.60/s  (0.697s, 1468.34/s)  LR: 9.425e-04  Data: 0.010 (0.015)
Train: 93 [ 500/1251 ( 40%)]  Loss: 4.291 (3.86)  Time: 0.672s, 1524.42/s  (0.696s, 1470.78/s)  LR: 9.425e-04  Data: 0.010 (0.014)
Train: 93 [ 550/1251 ( 44%)]  Loss: 4.030 (3.87)  Time: 0.714s, 1433.45/s  (0.696s, 1471.09/s)  LR: 9.425e-04  Data: 0.010 (0.014)
Train: 93 [ 600/1251 ( 48%)]  Loss: 3.940 (3.88)  Time: 0.704s, 1455.16/s  (0.696s, 1471.85/s)  LR: 9.425e-04  Data: 0.009 (0.013)
Train: 93 [ 650/1251 ( 52%)]  Loss: 3.688 (3.87)  Time: 0.697s, 1469.39/s  (0.696s, 1471.29/s)  LR: 9.425e-04  Data: 0.011 (0.013)
Train: 93 [ 700/1251 ( 56%)]  Loss: 4.102 (3.88)  Time: 0.718s, 1426.19/s  (0.696s, 1471.21/s)  LR: 9.425e-04  Data: 0.009 (0.013)
Train: 93 [ 750/1251 ( 60%)]  Loss: 3.954 (3.89)  Time: 0.700s, 1463.58/s  (0.697s, 1470.18/s)  LR: 9.425e-04  Data: 0.009 (0.013)
Train: 93 [ 800/1251 ( 64%)]  Loss: 3.889 (3.89)  Time: 0.678s, 1509.99/s  (0.696s, 1471.42/s)  LR: 9.425e-04  Data: 0.010 (0.013)
Train: 93 [ 850/1251 ( 68%)]  Loss: 4.247 (3.91)  Time: 0.673s, 1520.67/s  (0.695s, 1472.49/s)  LR: 9.425e-04  Data: 0.010 (0.013)
Train: 93 [ 900/1251 ( 72%)]  Loss: 4.071 (3.92)  Time: 0.681s, 1504.01/s  (0.695s, 1473.06/s)  LR: 9.425e-04  Data: 0.009 (0.012)
Train: 93 [ 950/1251 ( 76%)]  Loss: 3.959 (3.92)  Time: 0.683s, 1498.89/s  (0.695s, 1473.58/s)  LR: 9.425e-04  Data: 0.009 (0.012)
Train: 93 [1000/1251 ( 80%)]  Loss: 3.978 (3.92)  Time: 0.761s, 1345.36/s  (0.695s, 1473.59/s)  LR: 9.425e-04  Data: 0.010 (0.012)
Train: 93 [1050/1251 ( 84%)]  Loss: 4.106 (3.93)  Time: 0.694s, 1475.81/s  (0.695s, 1473.99/s)  LR: 9.425e-04  Data: 0.010 (0.012)
Train: 93 [1100/1251 ( 88%)]  Loss: 4.058 (3.93)  Time: 0.674s, 1519.33/s  (0.695s, 1474.08/s)  LR: 9.425e-04  Data: 0.010 (0.012)
Train: 93 [1150/1251 ( 92%)]  Loss: 3.451 (3.91)  Time: 0.710s, 1443.18/s  (0.695s, 1474.09/s)  LR: 9.425e-04  Data: 0.009 (0.012)
Train: 93 [1200/1251 ( 96%)]  Loss: 4.049 (3.92)  Time: 0.699s, 1465.92/s  (0.694s, 1474.72/s)  LR: 9.425e-04  Data: 0.010 (0.012)
Train: 93 [1250/1251 (100%)]  Loss: 3.958 (3.92)  Time: 0.660s, 1551.13/s  (0.694s, 1475.22/s)  LR: 9.425e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.486 (1.486)  Loss:  1.0771 (1.0771)  Acc@1: 86.0352 (86.0352)  Acc@5: 95.3125 (95.3125)
Test: [  48/48]  Time: 0.136 (0.587)  Loss:  0.9727 (1.5775)  Acc@1: 81.4859 (68.9620)  Acc@5: 94.5755 (88.9420)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-92.pth.tar', 69.22800004882812)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-93.pth.tar', 68.96200010253906)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-83.pth.tar', 68.6419999609375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-82.pth.tar', 68.63999993652344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-80.pth.tar', 68.60600010253906)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-77.pth.tar', 68.59599996582031)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-89.pth.tar', 68.51200002441406)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-76.pth.tar', 68.49400010742187)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-91.pth.tar', 68.3760000805664)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-87.pth.tar', 68.35399993164063)

Train: 94 [   0/1251 (  0%)]  Loss: 3.961 (3.96)  Time: 2.253s,  454.51/s  (2.253s,  454.51/s)  LR: 9.412e-04  Data: 1.636 (1.636)
Train: 94 [  50/1251 (  4%)]  Loss: 3.750 (3.86)  Time: 0.674s, 1518.60/s  (0.734s, 1395.05/s)  LR: 9.412e-04  Data: 0.012 (0.053)
Train: 94 [ 100/1251 (  8%)]  Loss: 3.674 (3.79)  Time: 0.672s, 1522.76/s  (0.711s, 1440.55/s)  LR: 9.412e-04  Data: 0.010 (0.031)
Train: 94 [ 150/1251 ( 12%)]  Loss: 3.890 (3.82)  Time: 0.668s, 1533.44/s  (0.704s, 1453.81/s)  LR: 9.412e-04  Data: 0.011 (0.024)
Train: 94 [ 200/1251 ( 16%)]  Loss: 4.210 (3.90)  Time: 0.714s, 1433.96/s  (0.703s, 1455.63/s)  LR: 9.412e-04  Data: 0.010 (0.021)
Train: 94 [ 250/1251 ( 20%)]  Loss: 3.892 (3.90)  Time: 0.692s, 1479.05/s  (0.701s, 1460.17/s)  LR: 9.412e-04  Data: 0.011 (0.019)
Train: 94 [ 300/1251 ( 24%)]  Loss: 3.837 (3.89)  Time: 0.672s, 1523.85/s  (0.699s, 1464.17/s)  LR: 9.412e-04  Data: 0.009 (0.017)
Train: 94 [ 350/1251 ( 28%)]  Loss: 3.820 (3.88)  Time: 0.697s, 1470.01/s  (0.698s, 1466.79/s)  LR: 9.412e-04  Data: 0.010 (0.016)
Train: 94 [ 400/1251 ( 32%)]  Loss: 4.096 (3.90)  Time: 0.708s, 1445.83/s  (0.697s, 1468.73/s)  LR: 9.412e-04  Data: 0.010 (0.015)
Train: 94 [ 450/1251 ( 36%)]  Loss: 3.910 (3.90)  Time: 0.712s, 1437.54/s  (0.696s, 1470.82/s)  LR: 9.412e-04  Data: 0.012 (0.015)
Train: 94 [ 500/1251 ( 40%)]  Loss: 4.007 (3.91)  Time: 0.675s, 1517.29/s  (0.696s, 1471.02/s)  LR: 9.412e-04  Data: 0.010 (0.014)
Train: 94 [ 550/1251 ( 44%)]  Loss: 3.745 (3.90)  Time: 0.698s, 1468.00/s  (0.696s, 1471.91/s)  LR: 9.412e-04  Data: 0.011 (0.014)
Train: 94 [ 600/1251 ( 48%)]  Loss: 3.965 (3.90)  Time: 0.693s, 1476.57/s  (0.696s, 1471.83/s)  LR: 9.412e-04  Data: 0.009 (0.014)
Train: 94 [ 650/1251 ( 52%)]  Loss: 3.987 (3.91)  Time: 0.673s, 1520.56/s  (0.695s, 1473.23/s)  LR: 9.412e-04  Data: 0.011 (0.013)
Train: 94 [ 700/1251 ( 56%)]  Loss: 4.015 (3.92)  Time: 0.673s, 1522.57/s  (0.695s, 1474.28/s)  LR: 9.412e-04  Data: 0.011 (0.013)
Train: 94 [ 750/1251 ( 60%)]  Loss: 3.499 (3.89)  Time: 0.676s, 1515.26/s  (0.695s, 1473.27/s)  LR: 9.412e-04  Data: 0.010 (0.013)
Train: 94 [ 800/1251 ( 64%)]  Loss: 4.097 (3.90)  Time: 0.671s, 1525.27/s  (0.695s, 1473.74/s)  LR: 9.412e-04  Data: 0.010 (0.013)
Train: 94 [ 850/1251 ( 68%)]  Loss: 4.031 (3.91)  Time: 0.674s, 1520.29/s  (0.695s, 1473.90/s)  LR: 9.412e-04  Data: 0.010 (0.013)
Train: 94 [ 900/1251 ( 72%)]  Loss: 4.077 (3.92)  Time: 0.723s, 1415.49/s  (0.695s, 1473.18/s)  LR: 9.412e-04  Data: 0.010 (0.013)
Train: 94 [ 950/1251 ( 76%)]  Loss: 3.769 (3.91)  Time: 0.689s, 1486.76/s  (0.695s, 1473.01/s)  LR: 9.412e-04  Data: 0.011 (0.012)
Train: 94 [1000/1251 ( 80%)]  Loss: 3.964 (3.91)  Time: 0.672s, 1523.19/s  (0.695s, 1473.55/s)  LR: 9.412e-04  Data: 0.010 (0.012)
Train: 94 [1050/1251 ( 84%)]  Loss: 3.881 (3.91)  Time: 0.673s, 1520.95/s  (0.695s, 1473.08/s)  LR: 9.412e-04  Data: 0.010 (0.012)
Train: 94 [1100/1251 ( 88%)]  Loss: 3.762 (3.91)  Time: 0.700s, 1463.15/s  (0.695s, 1473.07/s)  LR: 9.412e-04  Data: 0.009 (0.012)
Train: 94 [1150/1251 ( 92%)]  Loss: 3.534 (3.89)  Time: 0.695s, 1472.93/s  (0.695s, 1473.65/s)  LR: 9.412e-04  Data: 0.009 (0.012)
Train: 94 [1200/1251 ( 96%)]  Loss: 4.160 (3.90)  Time: 0.675s, 1517.14/s  (0.695s, 1473.97/s)  LR: 9.412e-04  Data: 0.009 (0.012)
Train: 94 [1250/1251 (100%)]  Loss: 3.896 (3.90)  Time: 0.728s, 1407.51/s  (0.695s, 1472.96/s)  LR: 9.412e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.698 (1.698)  Loss:  0.8911 (0.8911)  Acc@1: 87.1094 (87.1094)  Acc@5: 96.0938 (96.0938)
Test: [  48/48]  Time: 0.136 (0.586)  Loss:  0.8999 (1.5636)  Acc@1: 83.3727 (68.9440)  Acc@5: 96.4623 (89.1680)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-92.pth.tar', 69.22800004882812)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-93.pth.tar', 68.96200010253906)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-94.pth.tar', 68.94400014648437)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-83.pth.tar', 68.6419999609375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-82.pth.tar', 68.63999993652344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-80.pth.tar', 68.60600010253906)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-77.pth.tar', 68.59599996582031)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-89.pth.tar', 68.51200002441406)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-76.pth.tar', 68.49400010742187)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-91.pth.tar', 68.3760000805664)

Train: 95 [   0/1251 (  0%)]  Loss: 4.049 (4.05)  Time: 2.606s,  392.99/s  (2.606s,  392.99/s)  LR: 9.400e-04  Data: 1.901 (1.901)
Train: 95 [  50/1251 (  4%)]  Loss: 4.077 (4.06)  Time: 0.701s, 1459.77/s  (0.741s, 1382.33/s)  LR: 9.400e-04  Data: 0.010 (0.053)
Train: 95 [ 100/1251 (  8%)]  Loss: 3.652 (3.93)  Time: 0.671s, 1525.22/s  (0.714s, 1434.49/s)  LR: 9.400e-04  Data: 0.011 (0.032)
Train: 95 [ 150/1251 ( 12%)]  Loss: 3.732 (3.88)  Time: 0.673s, 1522.28/s  (0.706s, 1451.10/s)  LR: 9.400e-04  Data: 0.010 (0.025)
Train: 95 [ 200/1251 ( 16%)]  Loss: 3.965 (3.89)  Time: 0.674s, 1519.11/s  (0.702s, 1458.95/s)  LR: 9.400e-04  Data: 0.010 (0.021)
Train: 95 [ 250/1251 ( 20%)]  Loss: 3.977 (3.91)  Time: 0.737s, 1389.33/s  (0.700s, 1462.30/s)  LR: 9.400e-04  Data: 0.010 (0.019)
Train: 95 [ 300/1251 ( 24%)]  Loss: 3.792 (3.89)  Time: 0.705s, 1452.85/s  (0.699s, 1463.99/s)  LR: 9.400e-04  Data: 0.010 (0.017)
Train: 95 [ 350/1251 ( 28%)]  Loss: 4.127 (3.92)  Time: 0.711s, 1440.53/s  (0.699s, 1465.34/s)  LR: 9.400e-04  Data: 0.009 (0.016)
Train: 95 [ 400/1251 ( 32%)]  Loss: 3.987 (3.93)  Time: 0.674s, 1520.14/s  (0.698s, 1467.32/s)  LR: 9.400e-04  Data: 0.009 (0.016)
Train: 95 [ 450/1251 ( 36%)]  Loss: 3.814 (3.92)  Time: 0.722s, 1418.61/s  (0.698s, 1467.98/s)  LR: 9.400e-04  Data: 0.009 (0.015)
Train: 95 [ 500/1251 ( 40%)]  Loss: 4.052 (3.93)  Time: 0.675s, 1516.76/s  (0.697s, 1469.77/s)  LR: 9.400e-04  Data: 0.009 (0.015)
Train: 95 [ 550/1251 ( 44%)]  Loss: 3.605 (3.90)  Time: 0.712s, 1438.09/s  (0.697s, 1469.53/s)  LR: 9.400e-04  Data: 0.009 (0.014)
Train: 95 [ 600/1251 ( 48%)]  Loss: 4.117 (3.92)  Time: 0.671s, 1525.22/s  (0.697s, 1470.17/s)  LR: 9.400e-04  Data: 0.009 (0.014)
Train: 95 [ 650/1251 ( 52%)]  Loss: 3.572 (3.89)  Time: 0.691s, 1482.58/s  (0.696s, 1470.38/s)  LR: 9.400e-04  Data: 0.012 (0.014)
Train: 95 [ 700/1251 ( 56%)]  Loss: 4.247 (3.92)  Time: 0.724s, 1414.59/s  (0.696s, 1470.49/s)  LR: 9.400e-04  Data: 0.010 (0.013)
Train: 95 [ 750/1251 ( 60%)]  Loss: 4.095 (3.93)  Time: 0.678s, 1510.64/s  (0.697s, 1469.84/s)  LR: 9.400e-04  Data: 0.009 (0.013)
Train: 95 [ 800/1251 ( 64%)]  Loss: 4.203 (3.94)  Time: 0.674s, 1519.62/s  (0.697s, 1469.86/s)  LR: 9.400e-04  Data: 0.010 (0.013)
Train: 95 [ 850/1251 ( 68%)]  Loss: 4.207 (3.96)  Time: 0.668s, 1532.42/s  (0.696s, 1470.34/s)  LR: 9.400e-04  Data: 0.010 (0.013)
Train: 95 [ 900/1251 ( 72%)]  Loss: 3.733 (3.95)  Time: 0.706s, 1449.93/s  (0.696s, 1471.04/s)  LR: 9.400e-04  Data: 0.009 (0.013)
Train: 95 [ 950/1251 ( 76%)]  Loss: 3.885 (3.94)  Time: 0.749s, 1367.10/s  (0.696s, 1471.49/s)  LR: 9.400e-04  Data: 0.009 (0.013)
Train: 95 [1000/1251 ( 80%)]  Loss: 3.949 (3.94)  Time: 0.708s, 1445.62/s  (0.696s, 1471.94/s)  LR: 9.400e-04  Data: 0.010 (0.012)
Train: 95 [1050/1251 ( 84%)]  Loss: 3.440 (3.92)  Time: 0.711s, 1440.59/s  (0.696s, 1472.03/s)  LR: 9.400e-04  Data: 0.010 (0.012)
Train: 95 [1100/1251 ( 88%)]  Loss: 4.004 (3.93)  Time: 0.701s, 1461.81/s  (0.696s, 1471.46/s)  LR: 9.400e-04  Data: 0.009 (0.012)
Train: 95 [1150/1251 ( 92%)]  Loss: 4.172 (3.94)  Time: 0.713s, 1435.75/s  (0.696s, 1471.47/s)  LR: 9.400e-04  Data: 0.009 (0.012)
Train: 95 [1200/1251 ( 96%)]  Loss: 4.132 (3.94)  Time: 0.704s, 1454.00/s  (0.696s, 1472.00/s)  LR: 9.400e-04  Data: 0.009 (0.012)
Train: 95 [1250/1251 (100%)]  Loss: 3.676 (3.93)  Time: 0.657s, 1558.40/s  (0.696s, 1472.18/s)  LR: 9.400e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.514 (1.514)  Loss:  1.0059 (1.0059)  Acc@1: 86.4258 (86.4258)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.136 (0.589)  Loss:  1.0146 (1.6242)  Acc@1: 83.8443 (68.7940)  Acc@5: 94.4576 (88.9960)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-92.pth.tar', 69.22800004882812)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-93.pth.tar', 68.96200010253906)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-94.pth.tar', 68.94400014648437)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-95.pth.tar', 68.79399996337891)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-83.pth.tar', 68.6419999609375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-82.pth.tar', 68.63999993652344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-80.pth.tar', 68.60600010253906)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-77.pth.tar', 68.59599996582031)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-89.pth.tar', 68.51200002441406)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-76.pth.tar', 68.49400010742187)

Train: 96 [   0/1251 (  0%)]  Loss: 4.063 (4.06)  Time: 2.186s,  468.36/s  (2.186s,  468.36/s)  LR: 9.388e-04  Data: 1.572 (1.572)
Train: 96 [  50/1251 (  4%)]  Loss: 3.977 (4.02)  Time: 0.677s, 1511.63/s  (0.733s, 1397.16/s)  LR: 9.388e-04  Data: 0.010 (0.050)
Train: 96 [ 100/1251 (  8%)]  Loss: 4.152 (4.06)  Time: 0.729s, 1404.84/s  (0.712s, 1437.79/s)  LR: 9.388e-04  Data: 0.011 (0.030)
Train: 96 [ 150/1251 ( 12%)]  Loss: 4.145 (4.08)  Time: 0.701s, 1461.26/s  (0.708s, 1446.90/s)  LR: 9.388e-04  Data: 0.011 (0.024)
Train: 96 [ 200/1251 ( 16%)]  Loss: 3.996 (4.07)  Time: 0.672s, 1522.79/s  (0.703s, 1456.13/s)  LR: 9.388e-04  Data: 0.010 (0.020)
Train: 96 [ 250/1251 ( 20%)]  Loss: 4.125 (4.08)  Time: 0.695s, 1473.42/s  (0.706s, 1449.66/s)  LR: 9.388e-04  Data: 0.011 (0.019)
Train: 96 [ 300/1251 ( 24%)]  Loss: 3.888 (4.05)  Time: 0.727s, 1409.18/s  (0.707s, 1447.39/s)  LR: 9.388e-04  Data: 0.012 (0.018)
Train: 96 [ 350/1251 ( 28%)]  Loss: 3.984 (4.04)  Time: 0.721s, 1419.92/s  (0.708s, 1446.31/s)  LR: 9.388e-04  Data: 0.011 (0.017)
Train: 96 [ 400/1251 ( 32%)]  Loss: 4.319 (4.07)  Time: 0.706s, 1451.07/s  (0.707s, 1448.98/s)  LR: 9.388e-04  Data: 0.010 (0.016)
Train: 96 [ 450/1251 ( 36%)]  Loss: 3.523 (4.02)  Time: 0.675s, 1518.04/s  (0.704s, 1454.74/s)  LR: 9.388e-04  Data: 0.010 (0.016)
Train: 96 [ 500/1251 ( 40%)]  Loss: 3.943 (4.01)  Time: 0.672s, 1524.55/s  (0.701s, 1460.48/s)  LR: 9.388e-04  Data: 0.011 (0.015)
Train: 96 [ 550/1251 ( 44%)]  Loss: 3.821 (3.99)  Time: 0.681s, 1504.17/s  (0.700s, 1463.07/s)  LR: 9.388e-04  Data: 0.010 (0.015)
Train: 96 [ 600/1251 ( 48%)]  Loss: 3.985 (3.99)  Time: 0.674s, 1519.96/s  (0.699s, 1464.10/s)  LR: 9.388e-04  Data: 0.011 (0.014)
Train: 96 [ 650/1251 ( 52%)]  Loss: 4.060 (4.00)  Time: 0.707s, 1448.55/s  (0.699s, 1465.50/s)  LR: 9.388e-04  Data: 0.009 (0.014)
Train: 96 [ 700/1251 ( 56%)]  Loss: 3.840 (3.99)  Time: 0.691s, 1481.30/s  (0.698s, 1467.36/s)  LR: 9.388e-04  Data: 0.009 (0.014)
Train: 96 [ 750/1251 ( 60%)]  Loss: 3.719 (3.97)  Time: 0.786s, 1302.85/s  (0.697s, 1468.27/s)  LR: 9.388e-04  Data: 0.013 (0.013)
Train: 96 [ 800/1251 ( 64%)]  Loss: 3.351 (3.93)  Time: 0.722s, 1417.51/s  (0.697s, 1469.00/s)  LR: 9.388e-04  Data: 0.010 (0.013)
Train: 96 [ 850/1251 ( 68%)]  Loss: 4.181 (3.95)  Time: 0.672s, 1524.23/s  (0.697s, 1469.39/s)  LR: 9.388e-04  Data: 0.010 (0.013)
Train: 96 [ 900/1251 ( 72%)]  Loss: 3.524 (3.93)  Time: 0.722s, 1417.87/s  (0.697s, 1468.88/s)  LR: 9.388e-04  Data: 0.015 (0.013)
Train: 96 [ 950/1251 ( 76%)]  Loss: 3.825 (3.92)  Time: 0.708s, 1446.70/s  (0.697s, 1468.79/s)  LR: 9.388e-04  Data: 0.010 (0.013)
Train: 96 [1000/1251 ( 80%)]  Loss: 3.999 (3.92)  Time: 0.734s, 1395.36/s  (0.697s, 1469.70/s)  LR: 9.388e-04  Data: 0.010 (0.013)
Train: 96 [1050/1251 ( 84%)]  Loss: 4.106 (3.93)  Time: 0.681s, 1504.51/s  (0.697s, 1470.09/s)  LR: 9.388e-04  Data: 0.010 (0.013)
Train: 96 [1100/1251 ( 88%)]  Loss: 3.716 (3.92)  Time: 0.708s, 1447.19/s  (0.696s, 1471.15/s)  LR: 9.388e-04  Data: 0.009 (0.012)
Train: 96 [1150/1251 ( 92%)]  Loss: 3.594 (3.91)  Time: 0.671s, 1525.25/s  (0.696s, 1470.71/s)  LR: 9.388e-04  Data: 0.011 (0.012)
Train: 96 [1200/1251 ( 96%)]  Loss: 4.116 (3.92)  Time: 0.675s, 1517.38/s  (0.696s, 1471.60/s)  LR: 9.388e-04  Data: 0.013 (0.012)
Train: 96 [1250/1251 (100%)]  Loss: 4.152 (3.93)  Time: 0.656s, 1560.23/s  (0.696s, 1471.92/s)  LR: 9.388e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.595 (1.595)  Loss:  0.8916 (0.8916)  Acc@1: 85.8398 (85.8398)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.136 (0.590)  Loss:  1.0303 (1.6258)  Acc@1: 81.0142 (68.6500)  Acc@5: 93.6321 (88.9020)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-92.pth.tar', 69.22800004882812)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-93.pth.tar', 68.96200010253906)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-94.pth.tar', 68.94400014648437)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-95.pth.tar', 68.79399996337891)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-96.pth.tar', 68.65000002685547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-83.pth.tar', 68.6419999609375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-82.pth.tar', 68.63999993652344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-80.pth.tar', 68.60600010253906)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-77.pth.tar', 68.59599996582031)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-89.pth.tar', 68.51200002441406)

Train: 97 [   0/1251 (  0%)]  Loss: 3.512 (3.51)  Time: 2.254s,  454.28/s  (2.254s,  454.28/s)  LR: 9.375e-04  Data: 1.635 (1.635)
Train: 97 [  50/1251 (  4%)]  Loss: 4.165 (3.84)  Time: 0.681s, 1503.14/s  (0.723s, 1416.79/s)  LR: 9.375e-04  Data: 0.009 (0.048)
Train: 97 [ 100/1251 (  8%)]  Loss: 3.490 (3.72)  Time: 0.671s, 1526.23/s  (0.713s, 1436.59/s)  LR: 9.375e-04  Data: 0.010 (0.029)
Train: 97 [ 150/1251 ( 12%)]  Loss: 3.909 (3.77)  Time: 0.666s, 1538.67/s  (0.706s, 1450.09/s)  LR: 9.375e-04  Data: 0.010 (0.023)
Train: 97 [ 200/1251 ( 16%)]  Loss: 3.952 (3.81)  Time: 0.706s, 1450.76/s  (0.704s, 1455.54/s)  LR: 9.375e-04  Data: 0.011 (0.020)
Train: 97 [ 250/1251 ( 20%)]  Loss: 3.909 (3.82)  Time: 0.673s, 1520.89/s  (0.703s, 1456.20/s)  LR: 9.375e-04  Data: 0.014 (0.018)
Train: 97 [ 300/1251 ( 24%)]  Loss: 4.020 (3.85)  Time: 0.794s, 1289.73/s  (0.702s, 1459.07/s)  LR: 9.375e-04  Data: 0.009 (0.017)
Train: 97 [ 350/1251 ( 28%)]  Loss: 3.505 (3.81)  Time: 0.672s, 1523.17/s  (0.701s, 1461.72/s)  LR: 9.375e-04  Data: 0.010 (0.016)
Train: 97 [ 400/1251 ( 32%)]  Loss: 3.988 (3.83)  Time: 0.677s, 1513.35/s  (0.700s, 1463.40/s)  LR: 9.375e-04  Data: 0.010 (0.015)
Train: 97 [ 450/1251 ( 36%)]  Loss: 4.182 (3.86)  Time: 0.672s, 1524.42/s  (0.699s, 1465.68/s)  LR: 9.375e-04  Data: 0.011 (0.015)
Train: 97 [ 500/1251 ( 40%)]  Loss: 3.969 (3.87)  Time: 0.678s, 1510.19/s  (0.698s, 1467.12/s)  LR: 9.375e-04  Data: 0.009 (0.014)
Train: 97 [ 550/1251 ( 44%)]  Loss: 4.010 (3.88)  Time: 0.674s, 1519.10/s  (0.698s, 1466.00/s)  LR: 9.375e-04  Data: 0.010 (0.014)
Train: 97 [ 600/1251 ( 48%)]  Loss: 4.439 (3.93)  Time: 0.672s, 1523.70/s  (0.698s, 1467.50/s)  LR: 9.375e-04  Data: 0.009 (0.014)
Train: 97 [ 650/1251 ( 52%)]  Loss: 3.590 (3.90)  Time: 0.690s, 1483.57/s  (0.697s, 1469.07/s)  LR: 9.375e-04  Data: 0.009 (0.013)
Train: 97 [ 700/1251 ( 56%)]  Loss: 3.420 (3.87)  Time: 0.682s, 1502.20/s  (0.697s, 1469.66/s)  LR: 9.375e-04  Data: 0.009 (0.013)
Train: 97 [ 750/1251 ( 60%)]  Loss: 4.018 (3.88)  Time: 0.671s, 1525.29/s  (0.697s, 1469.98/s)  LR: 9.375e-04  Data: 0.010 (0.013)
Train: 97 [ 800/1251 ( 64%)]  Loss: 3.551 (3.86)  Time: 0.705s, 1453.43/s  (0.696s, 1470.67/s)  LR: 9.375e-04  Data: 0.010 (0.013)
Train: 97 [ 850/1251 ( 68%)]  Loss: 3.724 (3.85)  Time: 0.674s, 1518.18/s  (0.696s, 1471.42/s)  LR: 9.375e-04  Data: 0.009 (0.013)
Train: 97 [ 900/1251 ( 72%)]  Loss: 3.753 (3.85)  Time: 0.680s, 1506.69/s  (0.696s, 1472.31/s)  LR: 9.375e-04  Data: 0.009 (0.012)
Train: 97 [ 950/1251 ( 76%)]  Loss: 4.005 (3.86)  Time: 0.700s, 1461.87/s  (0.695s, 1472.72/s)  LR: 9.375e-04  Data: 0.009 (0.012)
Train: 97 [1000/1251 ( 80%)]  Loss: 4.102 (3.87)  Time: 0.669s, 1530.88/s  (0.696s, 1472.13/s)  LR: 9.375e-04  Data: 0.010 (0.012)
Train: 97 [1050/1251 ( 84%)]  Loss: 3.550 (3.85)  Time: 0.672s, 1523.48/s  (0.696s, 1471.70/s)  LR: 9.375e-04  Data: 0.013 (0.012)
Train: 97 [1100/1251 ( 88%)]  Loss: 4.167 (3.87)  Time: 0.690s, 1484.52/s  (0.696s, 1471.86/s)  LR: 9.375e-04  Data: 0.011 (0.012)
Train: 97 [1150/1251 ( 92%)]  Loss: 3.580 (3.85)  Time: 0.679s, 1507.42/s  (0.696s, 1472.15/s)  LR: 9.375e-04  Data: 0.010 (0.012)
Train: 97 [1200/1251 ( 96%)]  Loss: 4.174 (3.87)  Time: 0.730s, 1402.88/s  (0.696s, 1472.10/s)  LR: 9.375e-04  Data: 0.009 (0.012)
Train: 97 [1250/1251 (100%)]  Loss: 4.102 (3.88)  Time: 0.694s, 1474.63/s  (0.695s, 1472.56/s)  LR: 9.375e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.594 (1.594)  Loss:  0.9219 (0.9219)  Acc@1: 85.3516 (85.3516)  Acc@5: 95.8008 (95.8008)
Test: [  48/48]  Time: 0.136 (0.588)  Loss:  1.1748 (1.6025)  Acc@1: 80.8962 (68.8300)  Acc@5: 93.9859 (89.1340)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-92.pth.tar', 69.22800004882812)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-93.pth.tar', 68.96200010253906)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-94.pth.tar', 68.94400014648437)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-97.pth.tar', 68.82999997558593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-95.pth.tar', 68.79399996337891)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-96.pth.tar', 68.65000002685547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-83.pth.tar', 68.6419999609375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-82.pth.tar', 68.63999993652344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-80.pth.tar', 68.60600010253906)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-77.pth.tar', 68.59599996582031)

Train: 98 [   0/1251 (  0%)]  Loss: 4.052 (4.05)  Time: 2.055s,  498.38/s  (2.055s,  498.38/s)  LR: 9.363e-04  Data: 1.440 (1.440)
Train: 98 [  50/1251 (  4%)]  Loss: 4.074 (4.06)  Time: 0.705s, 1451.85/s  (0.730s, 1403.67/s)  LR: 9.363e-04  Data: 0.009 (0.048)
Train: 98 [ 100/1251 (  8%)]  Loss: 4.322 (4.15)  Time: 0.672s, 1524.88/s  (0.710s, 1443.15/s)  LR: 9.363e-04  Data: 0.010 (0.030)
Train: 98 [ 150/1251 ( 12%)]  Loss: 3.709 (4.04)  Time: 0.673s, 1520.44/s  (0.704s, 1454.14/s)  LR: 9.363e-04  Data: 0.010 (0.023)
Train: 98 [ 200/1251 ( 16%)]  Loss: 4.112 (4.05)  Time: 0.704s, 1455.43/s  (0.702s, 1459.25/s)  LR: 9.363e-04  Data: 0.010 (0.020)
Train: 98 [ 250/1251 ( 20%)]  Loss: 4.101 (4.06)  Time: 0.713s, 1436.72/s  (0.699s, 1465.37/s)  LR: 9.363e-04  Data: 0.011 (0.018)
Train: 98 [ 300/1251 ( 24%)]  Loss: 3.777 (4.02)  Time: 0.706s, 1450.93/s  (0.697s, 1469.29/s)  LR: 9.363e-04  Data: 0.009 (0.017)
Train: 98 [ 350/1251 ( 28%)]  Loss: 4.436 (4.07)  Time: 0.685s, 1495.75/s  (0.696s, 1470.53/s)  LR: 9.363e-04  Data: 0.010 (0.016)
Train: 98 [ 400/1251 ( 32%)]  Loss: 3.971 (4.06)  Time: 0.681s, 1504.09/s  (0.696s, 1472.12/s)  LR: 9.363e-04  Data: 0.011 (0.015)
Train: 98 [ 450/1251 ( 36%)]  Loss: 4.036 (4.06)  Time: 0.732s, 1398.76/s  (0.695s, 1472.55/s)  LR: 9.363e-04  Data: 0.010 (0.015)
Train: 98 [ 500/1251 ( 40%)]  Loss: 3.930 (4.05)  Time: 0.714s, 1435.16/s  (0.696s, 1471.94/s)  LR: 9.363e-04  Data: 0.011 (0.014)
Train: 98 [ 550/1251 ( 44%)]  Loss: 4.033 (4.05)  Time: 0.700s, 1463.04/s  (0.696s, 1470.75/s)  LR: 9.363e-04  Data: 0.010 (0.014)
Train: 98 [ 600/1251 ( 48%)]  Loss: 3.389 (4.00)  Time: 0.682s, 1501.56/s  (0.697s, 1469.64/s)  LR: 9.363e-04  Data: 0.010 (0.013)
Train: 98 [ 650/1251 ( 52%)]  Loss: 3.810 (3.98)  Time: 0.679s, 1507.99/s  (0.696s, 1471.14/s)  LR: 9.363e-04  Data: 0.011 (0.013)
Train: 98 [ 700/1251 ( 56%)]  Loss: 4.258 (4.00)  Time: 0.679s, 1507.04/s  (0.696s, 1471.85/s)  LR: 9.363e-04  Data: 0.009 (0.013)
Train: 98 [ 750/1251 ( 60%)]  Loss: 4.102 (4.01)  Time: 0.712s, 1438.19/s  (0.697s, 1469.95/s)  LR: 9.363e-04  Data: 0.011 (0.013)
Train: 98 [ 800/1251 ( 64%)]  Loss: 4.108 (4.01)  Time: 0.727s, 1407.93/s  (0.696s, 1470.69/s)  LR: 9.363e-04  Data: 0.010 (0.013)
Train: 98 [ 850/1251 ( 68%)]  Loss: 3.756 (4.00)  Time: 0.712s, 1437.87/s  (0.697s, 1469.98/s)  LR: 9.363e-04  Data: 0.010 (0.013)
Train: 98 [ 900/1251 ( 72%)]  Loss: 4.016 (4.00)  Time: 0.705s, 1452.21/s  (0.697s, 1469.99/s)  LR: 9.363e-04  Data: 0.011 (0.012)
Train: 98 [ 950/1251 ( 76%)]  Loss: 3.743 (3.99)  Time: 0.668s, 1532.87/s  (0.697s, 1469.93/s)  LR: 9.363e-04  Data: 0.010 (0.012)
Train: 98 [1000/1251 ( 80%)]  Loss: 3.999 (3.99)  Time: 0.679s, 1507.97/s  (0.696s, 1471.14/s)  LR: 9.363e-04  Data: 0.011 (0.012)
Train: 98 [1050/1251 ( 84%)]  Loss: 3.966 (3.99)  Time: 0.776s, 1320.11/s  (0.696s, 1471.81/s)  LR: 9.363e-04  Data: 0.010 (0.012)
Train: 98 [1100/1251 ( 88%)]  Loss: 4.114 (3.99)  Time: 0.676s, 1515.29/s  (0.695s, 1472.38/s)  LR: 9.363e-04  Data: 0.011 (0.012)
Train: 98 [1150/1251 ( 92%)]  Loss: 3.771 (3.98)  Time: 0.712s, 1437.65/s  (0.695s, 1472.85/s)  LR: 9.363e-04  Data: 0.009 (0.012)
Train: 98 [1200/1251 ( 96%)]  Loss: 3.713 (3.97)  Time: 0.672s, 1524.59/s  (0.695s, 1473.64/s)  LR: 9.363e-04  Data: 0.010 (0.012)
Train: 98 [1250/1251 (100%)]  Loss: 3.930 (3.97)  Time: 0.715s, 1432.41/s  (0.695s, 1472.98/s)  LR: 9.363e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.485 (1.485)  Loss:  1.1729 (1.1729)  Acc@1: 87.8906 (87.8906)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.136 (0.584)  Loss:  1.3027 (1.7611)  Acc@1: 81.4858 (68.9000)  Acc@5: 93.9858 (88.8560)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-92.pth.tar', 69.22800004882812)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-93.pth.tar', 68.96200010253906)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-94.pth.tar', 68.94400014648437)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-98.pth.tar', 68.89999997314453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-97.pth.tar', 68.82999997558593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-95.pth.tar', 68.79399996337891)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-96.pth.tar', 68.65000002685547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-83.pth.tar', 68.6419999609375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-82.pth.tar', 68.63999993652344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-80.pth.tar', 68.60600010253906)

Train: 99 [   0/1251 (  0%)]  Loss: 4.092 (4.09)  Time: 2.283s,  448.60/s  (2.283s,  448.60/s)  LR: 9.350e-04  Data: 1.641 (1.641)
Train: 99 [  50/1251 (  4%)]  Loss: 3.997 (4.04)  Time: 0.703s, 1457.50/s  (0.735s, 1392.57/s)  LR: 9.350e-04  Data: 0.010 (0.046)
Train: 99 [ 100/1251 (  8%)]  Loss: 3.803 (3.96)  Time: 0.680s, 1506.85/s  (0.715s, 1431.57/s)  LR: 9.350e-04  Data: 0.009 (0.028)
Train: 99 [ 150/1251 ( 12%)]  Loss: 4.188 (4.02)  Time: 0.703s, 1457.32/s  (0.709s, 1444.18/s)  LR: 9.350e-04  Data: 0.010 (0.022)
Train: 99 [ 200/1251 ( 16%)]  Loss: 3.859 (3.99)  Time: 0.674s, 1519.11/s  (0.705s, 1452.67/s)  LR: 9.350e-04  Data: 0.010 (0.019)
Train: 99 [ 250/1251 ( 20%)]  Loss: 3.877 (3.97)  Time: 0.665s, 1539.26/s  (0.702s, 1457.83/s)  LR: 9.350e-04  Data: 0.010 (0.018)
Train: 99 [ 300/1251 ( 24%)]  Loss: 3.915 (3.96)  Time: 0.708s, 1446.92/s  (0.702s, 1459.03/s)  LR: 9.350e-04  Data: 0.009 (0.016)
Train: 99 [ 350/1251 ( 28%)]  Loss: 3.654 (3.92)  Time: 0.695s, 1472.90/s  (0.701s, 1460.93/s)  LR: 9.350e-04  Data: 0.009 (0.016)
Train: 99 [ 400/1251 ( 32%)]  Loss: 3.978 (3.93)  Time: 0.705s, 1451.60/s  (0.700s, 1462.30/s)  LR: 9.350e-04  Data: 0.012 (0.015)
Train: 99 [ 450/1251 ( 36%)]  Loss: 3.609 (3.90)  Time: 0.668s, 1532.10/s  (0.701s, 1461.49/s)  LR: 9.350e-04  Data: 0.010 (0.014)
Train: 99 [ 500/1251 ( 40%)]  Loss: 3.764 (3.89)  Time: 0.673s, 1521.80/s  (0.699s, 1464.02/s)  LR: 9.350e-04  Data: 0.010 (0.014)
Train: 99 [ 550/1251 ( 44%)]  Loss: 3.812 (3.88)  Time: 0.720s, 1421.84/s  (0.699s, 1465.43/s)  LR: 9.350e-04  Data: 0.010 (0.014)
Train: 99 [ 600/1251 ( 48%)]  Loss: 3.934 (3.88)  Time: 0.730s, 1403.62/s  (0.698s, 1467.57/s)  LR: 9.350e-04  Data: 0.009 (0.013)
Train: 99 [ 650/1251 ( 52%)]  Loss: 4.055 (3.90)  Time: 0.693s, 1478.35/s  (0.697s, 1469.03/s)  LR: 9.350e-04  Data: 0.009 (0.013)
Train: 99 [ 700/1251 ( 56%)]  Loss: 3.806 (3.89)  Time: 0.717s, 1428.58/s  (0.697s, 1468.82/s)  LR: 9.350e-04  Data: 0.009 (0.013)
Train: 99 [ 750/1251 ( 60%)]  Loss: 3.886 (3.89)  Time: 0.747s, 1371.18/s  (0.698s, 1467.92/s)  LR: 9.350e-04  Data: 0.017 (0.013)
Train: 99 [ 800/1251 ( 64%)]  Loss: 4.475 (3.92)  Time: 0.672s, 1524.28/s  (0.698s, 1467.86/s)  LR: 9.350e-04  Data: 0.009 (0.013)
Train: 99 [ 850/1251 ( 68%)]  Loss: 3.964 (3.93)  Time: 0.744s, 1376.99/s  (0.697s, 1468.63/s)  LR: 9.350e-04  Data: 0.009 (0.012)
Train: 99 [ 900/1251 ( 72%)]  Loss: 3.859 (3.92)  Time: 0.709s, 1443.79/s  (0.697s, 1468.22/s)  LR: 9.350e-04  Data: 0.010 (0.012)
Train: 99 [ 950/1251 ( 76%)]  Loss: 3.960 (3.92)  Time: 0.726s, 1409.92/s  (0.698s, 1467.56/s)  LR: 9.350e-04  Data: 0.028 (0.012)
Train: 99 [1000/1251 ( 80%)]  Loss: 3.801 (3.92)  Time: 0.693s, 1476.79/s  (0.697s, 1468.28/s)  LR: 9.350e-04  Data: 0.010 (0.012)
Train: 99 [1050/1251 ( 84%)]  Loss: 3.947 (3.92)  Time: 0.674s, 1520.12/s  (0.697s, 1469.01/s)  LR: 9.350e-04  Data: 0.009 (0.012)
Train: 99 [1100/1251 ( 88%)]  Loss: 3.789 (3.91)  Time: 0.674s, 1519.73/s  (0.697s, 1469.33/s)  LR: 9.350e-04  Data: 0.010 (0.012)
Train: 99 [1150/1251 ( 92%)]  Loss: 3.857 (3.91)  Time: 0.672s, 1522.77/s  (0.697s, 1468.76/s)  LR: 9.350e-04  Data: 0.010 (0.012)
Train: 99 [1200/1251 ( 96%)]  Loss: 3.988 (3.91)  Time: 0.670s, 1528.21/s  (0.697s, 1469.85/s)  LR: 9.350e-04  Data: 0.010 (0.012)
Train: 99 [1250/1251 (100%)]  Loss: 3.878 (3.91)  Time: 0.696s, 1471.30/s  (0.696s, 1470.51/s)  LR: 9.350e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.578 (1.578)  Loss:  1.0117 (1.0117)  Acc@1: 86.8164 (86.8164)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.136 (0.585)  Loss:  1.0625 (1.6577)  Acc@1: 82.5472 (68.8740)  Acc@5: 94.2217 (89.1620)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-92.pth.tar', 69.22800004882812)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-93.pth.tar', 68.96200010253906)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-94.pth.tar', 68.94400014648437)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-98.pth.tar', 68.89999997314453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-99.pth.tar', 68.87400004638671)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-97.pth.tar', 68.82999997558593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-95.pth.tar', 68.79399996337891)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-96.pth.tar', 68.65000002685547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-83.pth.tar', 68.6419999609375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-82.pth.tar', 68.63999993652344)

Train: 100 [   0/1251 (  0%)]  Loss: 3.958 (3.96)  Time: 2.262s,  452.60/s  (2.262s,  452.60/s)  LR: 9.337e-04  Data: 1.647 (1.647)
Train: 100 [  50/1251 (  4%)]  Loss: 3.895 (3.93)  Time: 0.753s, 1359.56/s  (0.728s, 1407.51/s)  LR: 9.337e-04  Data: 0.009 (0.050)
Train: 100 [ 100/1251 (  8%)]  Loss: 3.799 (3.88)  Time: 0.722s, 1419.09/s  (0.710s, 1442.85/s)  LR: 9.337e-04  Data: 0.009 (0.030)
Train: 100 [ 150/1251 ( 12%)]  Loss: 3.770 (3.86)  Time: 0.690s, 1484.52/s  (0.702s, 1459.09/s)  LR: 9.337e-04  Data: 0.012 (0.024)
Train: 100 [ 200/1251 ( 16%)]  Loss: 4.266 (3.94)  Time: 0.708s, 1446.88/s  (0.701s, 1461.12/s)  LR: 9.337e-04  Data: 0.010 (0.020)
Train: 100 [ 250/1251 ( 20%)]  Loss: 3.995 (3.95)  Time: 0.696s, 1472.05/s  (0.701s, 1460.71/s)  LR: 9.337e-04  Data: 0.010 (0.018)
Train: 100 [ 300/1251 ( 24%)]  Loss: 3.593 (3.90)  Time: 0.707s, 1448.20/s  (0.701s, 1461.53/s)  LR: 9.337e-04  Data: 0.009 (0.017)
Train: 100 [ 350/1251 ( 28%)]  Loss: 4.168 (3.93)  Time: 0.704s, 1453.80/s  (0.700s, 1462.32/s)  LR: 9.337e-04  Data: 0.010 (0.016)
Train: 100 [ 400/1251 ( 32%)]  Loss: 4.163 (3.96)  Time: 0.671s, 1526.09/s  (0.699s, 1465.51/s)  LR: 9.337e-04  Data: 0.009 (0.015)
Train: 100 [ 450/1251 ( 36%)]  Loss: 4.115 (3.97)  Time: 0.701s, 1461.72/s  (0.697s, 1468.39/s)  LR: 9.337e-04  Data: 0.009 (0.015)
Train: 100 [ 500/1251 ( 40%)]  Loss: 3.524 (3.93)  Time: 0.717s, 1428.00/s  (0.697s, 1468.27/s)  LR: 9.337e-04  Data: 0.009 (0.014)
Train: 100 [ 550/1251 ( 44%)]  Loss: 4.274 (3.96)  Time: 0.667s, 1534.32/s  (0.698s, 1466.81/s)  LR: 9.337e-04  Data: 0.012 (0.014)
Train: 100 [ 600/1251 ( 48%)]  Loss: 3.968 (3.96)  Time: 0.673s, 1522.27/s  (0.698s, 1466.33/s)  LR: 9.337e-04  Data: 0.011 (0.014)
Train: 100 [ 650/1251 ( 52%)]  Loss: 3.893 (3.96)  Time: 0.673s, 1520.61/s  (0.698s, 1466.56/s)  LR: 9.337e-04  Data: 0.010 (0.013)
Train: 100 [ 700/1251 ( 56%)]  Loss: 4.177 (3.97)  Time: 0.729s, 1404.67/s  (0.698s, 1466.04/s)  LR: 9.337e-04  Data: 0.016 (0.013)
Train: 100 [ 750/1251 ( 60%)]  Loss: 3.705 (3.95)  Time: 0.710s, 1441.49/s  (0.698s, 1467.00/s)  LR: 9.337e-04  Data: 0.013 (0.013)
Train: 100 [ 800/1251 ( 64%)]  Loss: 3.658 (3.94)  Time: 0.674s, 1519.77/s  (0.697s, 1468.29/s)  LR: 9.337e-04  Data: 0.009 (0.013)
Train: 100 [ 850/1251 ( 68%)]  Loss: 4.238 (3.95)  Time: 0.675s, 1516.39/s  (0.697s, 1468.28/s)  LR: 9.337e-04  Data: 0.010 (0.013)
Train: 100 [ 900/1251 ( 72%)]  Loss: 4.014 (3.96)  Time: 0.673s, 1522.60/s  (0.698s, 1468.05/s)  LR: 9.337e-04  Data: 0.010 (0.012)
Train: 100 [ 950/1251 ( 76%)]  Loss: 3.774 (3.95)  Time: 0.674s, 1520.35/s  (0.697s, 1468.33/s)  LR: 9.337e-04  Data: 0.011 (0.012)
Train: 100 [1000/1251 ( 80%)]  Loss: 4.059 (3.95)  Time: 0.681s, 1504.52/s  (0.697s, 1468.37/s)  LR: 9.337e-04  Data: 0.016 (0.012)
Train: 100 [1050/1251 ( 84%)]  Loss: 4.054 (3.96)  Time: 0.679s, 1509.17/s  (0.697s, 1468.51/s)  LR: 9.337e-04  Data: 0.009 (0.012)
Train: 100 [1100/1251 ( 88%)]  Loss: 4.137 (3.97)  Time: 0.694s, 1476.10/s  (0.697s, 1469.18/s)  LR: 9.337e-04  Data: 0.012 (0.012)
Train: 100 [1150/1251 ( 92%)]  Loss: 3.984 (3.97)  Time: 0.672s, 1522.99/s  (0.697s, 1469.61/s)  LR: 9.337e-04  Data: 0.010 (0.012)
Train: 100 [1200/1251 ( 96%)]  Loss: 3.469 (3.95)  Time: 0.695s, 1474.28/s  (0.697s, 1469.75/s)  LR: 9.337e-04  Data: 0.009 (0.012)
Train: 100 [1250/1251 (100%)]  Loss: 3.894 (3.94)  Time: 0.677s, 1512.47/s  (0.697s, 1469.90/s)  LR: 9.337e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.547 (1.547)  Loss:  1.0488 (1.0488)  Acc@1: 82.7148 (82.7148)  Acc@5: 95.0195 (95.0195)
Test: [  48/48]  Time: 0.142 (0.584)  Loss:  1.1582 (1.5544)  Acc@1: 80.8962 (68.2880)  Acc@5: 95.2830 (88.9080)
Train: 101 [   0/1251 (  0%)]  Loss: 3.961 (3.96)  Time: 2.368s,  432.48/s  (2.368s,  432.48/s)  LR: 9.324e-04  Data: 1.702 (1.702)
Train: 101 [  50/1251 (  4%)]  Loss: 3.909 (3.93)  Time: 0.709s, 1444.34/s  (0.724s, 1414.67/s)  LR: 9.324e-04  Data: 0.011 (0.044)
Train: 101 [ 100/1251 (  8%)]  Loss: 4.043 (3.97)  Time: 0.733s, 1396.73/s  (0.709s, 1444.22/s)  LR: 9.324e-04  Data: 0.009 (0.027)
Train: 101 [ 150/1251 ( 12%)]  Loss: 3.861 (3.94)  Time: 0.780s, 1313.08/s  (0.705s, 1452.69/s)  LR: 9.324e-04  Data: 0.009 (0.022)
Train: 101 [ 200/1251 ( 16%)]  Loss: 3.701 (3.89)  Time: 0.669s, 1531.39/s  (0.701s, 1460.81/s)  LR: 9.324e-04  Data: 0.012 (0.019)
Train: 101 [ 250/1251 ( 20%)]  Loss: 3.486 (3.83)  Time: 0.706s, 1450.34/s  (0.699s, 1465.86/s)  LR: 9.324e-04  Data: 0.010 (0.017)
Train: 101 [ 300/1251 ( 24%)]  Loss: 4.169 (3.88)  Time: 0.741s, 1382.57/s  (0.697s, 1468.19/s)  LR: 9.324e-04  Data: 0.009 (0.016)
Train: 101 [ 350/1251 ( 28%)]  Loss: 3.793 (3.87)  Time: 0.672s, 1522.77/s  (0.697s, 1469.63/s)  LR: 9.324e-04  Data: 0.010 (0.015)
Train: 101 [ 400/1251 ( 32%)]  Loss: 3.656 (3.84)  Time: 0.677s, 1512.23/s  (0.697s, 1468.72/s)  LR: 9.324e-04  Data: 0.010 (0.015)
Train: 101 [ 450/1251 ( 36%)]  Loss: 3.690 (3.83)  Time: 0.686s, 1492.68/s  (0.696s, 1470.47/s)  LR: 9.324e-04  Data: 0.009 (0.014)
Train: 101 [ 500/1251 ( 40%)]  Loss: 3.715 (3.82)  Time: 0.702s, 1458.56/s  (0.696s, 1471.18/s)  LR: 9.324e-04  Data: 0.009 (0.014)
Train: 101 [ 550/1251 ( 44%)]  Loss: 3.940 (3.83)  Time: 0.675s, 1516.32/s  (0.696s, 1471.38/s)  LR: 9.324e-04  Data: 0.010 (0.013)
Train: 101 [ 600/1251 ( 48%)]  Loss: 3.817 (3.83)  Time: 0.788s, 1299.84/s  (0.695s, 1472.62/s)  LR: 9.324e-04  Data: 0.009 (0.013)
Train: 101 [ 650/1251 ( 52%)]  Loss: 3.549 (3.81)  Time: 0.749s, 1366.59/s  (0.696s, 1470.84/s)  LR: 9.324e-04  Data: 0.009 (0.013)
Train: 101 [ 700/1251 ( 56%)]  Loss: 4.142 (3.83)  Time: 0.696s, 1472.30/s  (0.696s, 1470.26/s)  LR: 9.324e-04  Data: 0.009 (0.013)
Train: 101 [ 750/1251 ( 60%)]  Loss: 3.979 (3.84)  Time: 0.670s, 1528.96/s  (0.696s, 1471.03/s)  LR: 9.324e-04  Data: 0.009 (0.013)
Train: 101 [ 800/1251 ( 64%)]  Loss: 3.893 (3.84)  Time: 0.674s, 1519.85/s  (0.696s, 1471.38/s)  LR: 9.324e-04  Data: 0.010 (0.012)
Train: 101 [ 850/1251 ( 68%)]  Loss: 4.128 (3.86)  Time: 0.700s, 1461.92/s  (0.696s, 1471.11/s)  LR: 9.324e-04  Data: 0.010 (0.012)
Train: 101 [ 900/1251 ( 72%)]  Loss: 4.227 (3.88)  Time: 0.724s, 1413.93/s  (0.696s, 1471.76/s)  LR: 9.324e-04  Data: 0.010 (0.012)
Train: 101 [ 950/1251 ( 76%)]  Loss: 4.166 (3.89)  Time: 0.671s, 1527.13/s  (0.696s, 1472.26/s)  LR: 9.324e-04  Data: 0.011 (0.012)
Train: 101 [1000/1251 ( 80%)]  Loss: 4.033 (3.90)  Time: 0.690s, 1483.64/s  (0.695s, 1472.75/s)  LR: 9.324e-04  Data: 0.011 (0.012)
Train: 101 [1050/1251 ( 84%)]  Loss: 3.805 (3.89)  Time: 0.706s, 1451.13/s  (0.695s, 1472.94/s)  LR: 9.324e-04  Data: 0.010 (0.012)
Train: 101 [1100/1251 ( 88%)]  Loss: 3.863 (3.89)  Time: 0.699s, 1463.95/s  (0.695s, 1472.89/s)  LR: 9.324e-04  Data: 0.010 (0.012)
Train: 101 [1150/1251 ( 92%)]  Loss: 4.047 (3.90)  Time: 0.695s, 1474.41/s  (0.696s, 1472.07/s)  LR: 9.324e-04  Data: 0.015 (0.012)
Train: 101 [1200/1251 ( 96%)]  Loss: 3.819 (3.90)  Time: 0.670s, 1528.49/s  (0.696s, 1471.92/s)  LR: 9.324e-04  Data: 0.011 (0.012)
Train: 101 [1250/1251 (100%)]  Loss: 4.033 (3.90)  Time: 0.688s, 1487.33/s  (0.695s, 1472.32/s)  LR: 9.324e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.613 (1.613)  Loss:  1.0518 (1.0518)  Acc@1: 84.5703 (84.5703)  Acc@5: 95.2148 (95.2148)
Test: [  48/48]  Time: 0.136 (0.596)  Loss:  1.2744 (1.7149)  Acc@1: 79.1274 (68.4320)  Acc@5: 93.8679 (88.7480)
Train: 102 [   0/1251 (  0%)]  Loss: 4.114 (4.11)  Time: 2.197s,  466.02/s  (2.197s,  466.02/s)  LR: 9.311e-04  Data: 1.581 (1.581)
Train: 102 [  50/1251 (  4%)]  Loss: 3.806 (3.96)  Time: 0.704s, 1453.76/s  (0.742s, 1379.18/s)  LR: 9.311e-04  Data: 0.009 (0.050)
Train: 102 [ 100/1251 (  8%)]  Loss: 3.629 (3.85)  Time: 0.708s, 1445.53/s  (0.715s, 1431.33/s)  LR: 9.311e-04  Data: 0.012 (0.030)
Train: 102 [ 150/1251 ( 12%)]  Loss: 3.940 (3.87)  Time: 0.715s, 1432.93/s  (0.710s, 1442.62/s)  LR: 9.311e-04  Data: 0.010 (0.024)
Train: 102 [ 200/1251 ( 16%)]  Loss: 4.065 (3.91)  Time: 0.710s, 1441.87/s  (0.709s, 1444.97/s)  LR: 9.311e-04  Data: 0.010 (0.021)
Train: 102 [ 250/1251 ( 20%)]  Loss: 3.982 (3.92)  Time: 0.709s, 1444.25/s  (0.707s, 1447.90/s)  LR: 9.311e-04  Data: 0.010 (0.018)
Train: 102 [ 300/1251 ( 24%)]  Loss: 4.126 (3.95)  Time: 0.710s, 1442.12/s  (0.703s, 1455.59/s)  LR: 9.311e-04  Data: 0.008 (0.017)
Train: 102 [ 350/1251 ( 28%)]  Loss: 3.746 (3.93)  Time: 0.728s, 1406.99/s  (0.702s, 1458.21/s)  LR: 9.311e-04  Data: 0.011 (0.016)
Train: 102 [ 400/1251 ( 32%)]  Loss: 3.972 (3.93)  Time: 0.703s, 1455.80/s  (0.701s, 1461.61/s)  LR: 9.311e-04  Data: 0.009 (0.015)
Train: 102 [ 450/1251 ( 36%)]  Loss: 4.038 (3.94)  Time: 0.672s, 1523.15/s  (0.700s, 1461.98/s)  LR: 9.311e-04  Data: 0.010 (0.015)
Train: 102 [ 500/1251 ( 40%)]  Loss: 4.222 (3.97)  Time: 0.673s, 1522.37/s  (0.699s, 1464.16/s)  LR: 9.311e-04  Data: 0.011 (0.014)
Train: 102 [ 550/1251 ( 44%)]  Loss: 4.015 (3.97)  Time: 0.670s, 1527.50/s  (0.699s, 1464.06/s)  LR: 9.311e-04  Data: 0.010 (0.014)
Train: 102 [ 600/1251 ( 48%)]  Loss: 3.741 (3.95)  Time: 0.677s, 1511.89/s  (0.699s, 1464.20/s)  LR: 9.311e-04  Data: 0.010 (0.014)
Train: 102 [ 650/1251 ( 52%)]  Loss: 4.104 (3.96)  Time: 0.673s, 1521.41/s  (0.699s, 1465.73/s)  LR: 9.311e-04  Data: 0.011 (0.013)
Train: 102 [ 700/1251 ( 56%)]  Loss: 4.021 (3.97)  Time: 0.680s, 1505.66/s  (0.698s, 1466.60/s)  LR: 9.311e-04  Data: 0.009 (0.013)
Train: 102 [ 750/1251 ( 60%)]  Loss: 4.071 (3.97)  Time: 0.704s, 1454.44/s  (0.698s, 1467.29/s)  LR: 9.311e-04  Data: 0.009 (0.013)
Train: 102 [ 800/1251 ( 64%)]  Loss: 3.642 (3.95)  Time: 0.672s, 1522.68/s  (0.698s, 1467.54/s)  LR: 9.311e-04  Data: 0.010 (0.013)
Train: 102 [ 850/1251 ( 68%)]  Loss: 3.969 (3.96)  Time: 0.731s, 1401.43/s  (0.698s, 1467.18/s)  LR: 9.311e-04  Data: 0.011 (0.013)
Train: 102 [ 900/1251 ( 72%)]  Loss: 3.920 (3.95)  Time: 0.672s, 1524.43/s  (0.698s, 1467.21/s)  LR: 9.311e-04  Data: 0.010 (0.013)
Train: 102 [ 950/1251 ( 76%)]  Loss: 3.577 (3.94)  Time: 0.726s, 1409.74/s  (0.698s, 1467.72/s)  LR: 9.311e-04  Data: 0.010 (0.012)
Train: 102 [1000/1251 ( 80%)]  Loss: 3.538 (3.92)  Time: 0.668s, 1533.26/s  (0.698s, 1467.72/s)  LR: 9.311e-04  Data: 0.008 (0.012)
Train: 102 [1050/1251 ( 84%)]  Loss: 4.079 (3.92)  Time: 0.702s, 1458.64/s  (0.698s, 1467.97/s)  LR: 9.311e-04  Data: 0.009 (0.012)
Train: 102 [1100/1251 ( 88%)]  Loss: 4.004 (3.93)  Time: 0.675s, 1517.01/s  (0.697s, 1468.30/s)  LR: 9.311e-04  Data: 0.010 (0.012)
Train: 102 [1150/1251 ( 92%)]  Loss: 4.033 (3.93)  Time: 0.710s, 1442.41/s  (0.697s, 1468.67/s)  LR: 9.311e-04  Data: 0.010 (0.012)
Train: 102 [1200/1251 ( 96%)]  Loss: 3.608 (3.92)  Time: 0.672s, 1524.30/s  (0.697s, 1468.73/s)  LR: 9.311e-04  Data: 0.011 (0.012)
Train: 102 [1250/1251 (100%)]  Loss: 4.151 (3.93)  Time: 0.656s, 1560.83/s  (0.697s, 1469.51/s)  LR: 9.311e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.623 (1.623)  Loss:  0.9521 (0.9521)  Acc@1: 86.6211 (86.6211)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.136 (0.590)  Loss:  1.0488 (1.5977)  Acc@1: 83.1368 (69.5460)  Acc@5: 95.0472 (89.3660)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-102.pth.tar', 69.54600004394531)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-92.pth.tar', 69.22800004882812)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-93.pth.tar', 68.96200010253906)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-94.pth.tar', 68.94400014648437)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-98.pth.tar', 68.89999997314453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-99.pth.tar', 68.87400004638671)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-97.pth.tar', 68.82999997558593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-95.pth.tar', 68.79399996337891)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-96.pth.tar', 68.65000002685547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-83.pth.tar', 68.6419999609375)

Train: 103 [   0/1251 (  0%)]  Loss: 3.768 (3.77)  Time: 2.365s,  432.97/s  (2.365s,  432.97/s)  LR: 9.297e-04  Data: 1.728 (1.728)
Train: 103 [  50/1251 (  4%)]  Loss: 3.804 (3.79)  Time: 0.672s, 1523.47/s  (0.734s, 1394.71/s)  LR: 9.297e-04  Data: 0.009 (0.049)
Train: 103 [ 100/1251 (  8%)]  Loss: 3.641 (3.74)  Time: 0.672s, 1523.19/s  (0.715s, 1432.41/s)  LR: 9.297e-04  Data: 0.011 (0.030)
Train: 103 [ 150/1251 ( 12%)]  Loss: 3.953 (3.79)  Time: 0.732s, 1399.33/s  (0.709s, 1443.73/s)  LR: 9.297e-04  Data: 0.009 (0.024)
Train: 103 [ 200/1251 ( 16%)]  Loss: 3.692 (3.77)  Time: 0.672s, 1524.44/s  (0.705s, 1451.60/s)  LR: 9.297e-04  Data: 0.009 (0.020)
Train: 103 [ 250/1251 ( 20%)]  Loss: 4.053 (3.82)  Time: 0.756s, 1355.12/s  (0.702s, 1458.53/s)  LR: 9.297e-04  Data: 0.009 (0.018)
Train: 103 [ 300/1251 ( 24%)]  Loss: 3.842 (3.82)  Time: 0.696s, 1471.35/s  (0.701s, 1460.37/s)  LR: 9.297e-04  Data: 0.009 (0.017)
Train: 103 [ 350/1251 ( 28%)]  Loss: 3.645 (3.80)  Time: 0.682s, 1500.99/s  (0.700s, 1463.10/s)  LR: 9.297e-04  Data: 0.013 (0.016)
Train: 103 [ 400/1251 ( 32%)]  Loss: 3.839 (3.80)  Time: 0.693s, 1477.42/s  (0.699s, 1465.19/s)  LR: 9.297e-04  Data: 0.012 (0.015)
Train: 103 [ 450/1251 ( 36%)]  Loss: 3.657 (3.79)  Time: 0.699s, 1465.10/s  (0.698s, 1467.20/s)  LR: 9.297e-04  Data: 0.010 (0.015)
Train: 103 [ 500/1251 ( 40%)]  Loss: 3.793 (3.79)  Time: 0.703s, 1456.96/s  (0.697s, 1468.36/s)  LR: 9.297e-04  Data: 0.009 (0.014)
Train: 103 [ 550/1251 ( 44%)]  Loss: 3.873 (3.80)  Time: 0.679s, 1508.95/s  (0.697s, 1468.82/s)  LR: 9.297e-04  Data: 0.009 (0.014)
Train: 103 [ 600/1251 ( 48%)]  Loss: 3.659 (3.79)  Time: 0.744s, 1376.84/s  (0.697s, 1470.01/s)  LR: 9.297e-04  Data: 0.010 (0.014)
Train: 103 [ 650/1251 ( 52%)]  Loss: 3.715 (3.78)  Time: 0.668s, 1532.59/s  (0.696s, 1471.16/s)  LR: 9.297e-04  Data: 0.010 (0.013)
Train: 103 [ 700/1251 ( 56%)]  Loss: 3.951 (3.79)  Time: 0.696s, 1471.78/s  (0.696s, 1471.00/s)  LR: 9.297e-04  Data: 0.010 (0.013)
Train: 103 [ 750/1251 ( 60%)]  Loss: 3.653 (3.78)  Time: 0.753s, 1359.08/s  (0.696s, 1471.32/s)  LR: 9.297e-04  Data: 0.010 (0.013)
Train: 103 [ 800/1251 ( 64%)]  Loss: 3.864 (3.79)  Time: 0.670s, 1528.97/s  (0.696s, 1471.99/s)  LR: 9.297e-04  Data: 0.010 (0.013)
Train: 103 [ 850/1251 ( 68%)]  Loss: 3.778 (3.79)  Time: 0.691s, 1481.65/s  (0.696s, 1471.43/s)  LR: 9.297e-04  Data: 0.010 (0.013)
Train: 103 [ 900/1251 ( 72%)]  Loss: 4.089 (3.80)  Time: 0.712s, 1438.52/s  (0.696s, 1472.21/s)  LR: 9.297e-04  Data: 0.009 (0.012)
Train: 103 [ 950/1251 ( 76%)]  Loss: 4.148 (3.82)  Time: 0.684s, 1496.34/s  (0.695s, 1473.05/s)  LR: 9.297e-04  Data: 0.011 (0.012)
Train: 103 [1000/1251 ( 80%)]  Loss: 4.191 (3.84)  Time: 0.707s, 1448.98/s  (0.695s, 1473.32/s)  LR: 9.297e-04  Data: 0.010 (0.012)
Train: 103 [1050/1251 ( 84%)]  Loss: 4.076 (3.85)  Time: 0.674s, 1518.65/s  (0.695s, 1474.12/s)  LR: 9.297e-04  Data: 0.010 (0.012)
Train: 103 [1100/1251 ( 88%)]  Loss: 3.869 (3.85)  Time: 0.670s, 1527.70/s  (0.695s, 1474.33/s)  LR: 9.297e-04  Data: 0.010 (0.012)
Train: 103 [1150/1251 ( 92%)]  Loss: 3.956 (3.85)  Time: 0.672s, 1522.72/s  (0.694s, 1474.71/s)  LR: 9.297e-04  Data: 0.010 (0.012)
Train: 103 [1200/1251 ( 96%)]  Loss: 4.104 (3.86)  Time: 0.670s, 1528.90/s  (0.694s, 1475.65/s)  LR: 9.297e-04  Data: 0.009 (0.012)
Train: 103 [1250/1251 (100%)]  Loss: 3.985 (3.87)  Time: 0.677s, 1512.08/s  (0.694s, 1475.92/s)  LR: 9.297e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.607 (1.607)  Loss:  0.8843 (0.8843)  Acc@1: 85.9375 (85.9375)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.136 (0.582)  Loss:  1.0615 (1.5341)  Acc@1: 81.9575 (69.3120)  Acc@5: 93.5142 (89.5400)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-102.pth.tar', 69.54600004394531)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-103.pth.tar', 69.31199991943359)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-92.pth.tar', 69.22800004882812)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-93.pth.tar', 68.96200010253906)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-94.pth.tar', 68.94400014648437)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-98.pth.tar', 68.89999997314453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-99.pth.tar', 68.87400004638671)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-97.pth.tar', 68.82999997558593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-95.pth.tar', 68.79399996337891)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-96.pth.tar', 68.65000002685547)

Train: 104 [   0/1251 (  0%)]  Loss: 3.749 (3.75)  Time: 2.245s,  456.20/s  (2.245s,  456.20/s)  LR: 9.284e-04  Data: 1.619 (1.619)
Train: 104 [  50/1251 (  4%)]  Loss: 3.838 (3.79)  Time: 0.668s, 1533.42/s  (0.733s, 1397.75/s)  LR: 9.284e-04  Data: 0.010 (0.050)
Train: 104 [ 100/1251 (  8%)]  Loss: 4.055 (3.88)  Time: 0.667s, 1535.77/s  (0.712s, 1437.73/s)  LR: 9.284e-04  Data: 0.008 (0.030)
Train: 104 [ 150/1251 ( 12%)]  Loss: 3.831 (3.87)  Time: 0.703s, 1456.54/s  (0.706s, 1450.84/s)  LR: 9.284e-04  Data: 0.009 (0.023)
Train: 104 [ 200/1251 ( 16%)]  Loss: 3.530 (3.80)  Time: 0.726s, 1410.03/s  (0.705s, 1453.11/s)  LR: 9.284e-04  Data: 0.014 (0.020)
Train: 104 [ 250/1251 ( 20%)]  Loss: 4.124 (3.85)  Time: 0.676s, 1515.83/s  (0.702s, 1459.43/s)  LR: 9.284e-04  Data: 0.010 (0.018)
Train: 104 [ 300/1251 ( 24%)]  Loss: 4.059 (3.88)  Time: 0.671s, 1526.10/s  (0.699s, 1464.35/s)  LR: 9.284e-04  Data: 0.010 (0.017)
Train: 104 [ 350/1251 ( 28%)]  Loss: 3.682 (3.86)  Time: 0.700s, 1463.61/s  (0.699s, 1465.02/s)  LR: 9.284e-04  Data: 0.010 (0.016)
Train: 104 [ 400/1251 ( 32%)]  Loss: 4.295 (3.91)  Time: 0.716s, 1429.33/s  (0.698s, 1466.86/s)  LR: 9.284e-04  Data: 0.011 (0.015)
Train: 104 [ 450/1251 ( 36%)]  Loss: 3.579 (3.87)  Time: 0.681s, 1503.93/s  (0.699s, 1465.96/s)  LR: 9.284e-04  Data: 0.013 (0.015)
Train: 104 [ 500/1251 ( 40%)]  Loss: 3.815 (3.87)  Time: 0.705s, 1453.51/s  (0.698s, 1466.63/s)  LR: 9.284e-04  Data: 0.010 (0.014)
Train: 104 [ 550/1251 ( 44%)]  Loss: 4.005 (3.88)  Time: 0.719s, 1424.40/s  (0.698s, 1466.12/s)  LR: 9.284e-04  Data: 0.010 (0.014)
Train: 104 [ 600/1251 ( 48%)]  Loss: 4.056 (3.89)  Time: 0.720s, 1422.87/s  (0.699s, 1464.93/s)  LR: 9.284e-04  Data: 0.015 (0.014)
Train: 104 [ 650/1251 ( 52%)]  Loss: 4.287 (3.92)  Time: 0.671s, 1525.54/s  (0.699s, 1465.41/s)  LR: 9.284e-04  Data: 0.010 (0.013)
Train: 104 [ 700/1251 ( 56%)]  Loss: 4.024 (3.93)  Time: 0.696s, 1471.31/s  (0.698s, 1466.41/s)  LR: 9.284e-04  Data: 0.010 (0.013)
Train: 104 [ 750/1251 ( 60%)]  Loss: 3.443 (3.90)  Time: 0.668s, 1533.00/s  (0.698s, 1467.93/s)  LR: 9.284e-04  Data: 0.009 (0.013)
Train: 104 [ 800/1251 ( 64%)]  Loss: 4.030 (3.91)  Time: 0.672s, 1524.86/s  (0.697s, 1469.10/s)  LR: 9.284e-04  Data: 0.010 (0.013)
Train: 104 [ 850/1251 ( 68%)]  Loss: 4.047 (3.91)  Time: 0.675s, 1517.92/s  (0.697s, 1469.99/s)  LR: 9.284e-04  Data: 0.010 (0.013)
Train: 104 [ 900/1251 ( 72%)]  Loss: 3.675 (3.90)  Time: 0.673s, 1522.49/s  (0.696s, 1470.69/s)  LR: 9.284e-04  Data: 0.010 (0.012)
Train: 104 [ 950/1251 ( 76%)]  Loss: 3.968 (3.90)  Time: 0.721s, 1419.82/s  (0.696s, 1470.25/s)  LR: 9.284e-04  Data: 0.016 (0.012)
Train: 104 [1000/1251 ( 80%)]  Loss: 3.942 (3.91)  Time: 0.685s, 1495.79/s  (0.696s, 1470.47/s)  LR: 9.284e-04  Data: 0.029 (0.012)
Train: 104 [1050/1251 ( 84%)]  Loss: 4.075 (3.91)  Time: 0.673s, 1521.13/s  (0.696s, 1471.09/s)  LR: 9.284e-04  Data: 0.010 (0.012)
Train: 104 [1100/1251 ( 88%)]  Loss: 4.318 (3.93)  Time: 0.676s, 1513.77/s  (0.696s, 1471.83/s)  LR: 9.284e-04  Data: 0.011 (0.012)
Train: 104 [1150/1251 ( 92%)]  Loss: 4.014 (3.93)  Time: 0.673s, 1521.00/s  (0.696s, 1472.12/s)  LR: 9.284e-04  Data: 0.011 (0.012)
Train: 104 [1200/1251 ( 96%)]  Loss: 3.851 (3.93)  Time: 0.671s, 1525.57/s  (0.695s, 1472.46/s)  LR: 9.284e-04  Data: 0.010 (0.012)
Train: 104 [1250/1251 (100%)]  Loss: 4.031 (3.94)  Time: 0.663s, 1543.90/s  (0.695s, 1472.63/s)  LR: 9.284e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.574 (1.574)  Loss:  0.9561 (0.9561)  Acc@1: 85.6445 (85.6445)  Acc@5: 95.8008 (95.8008)
Test: [  48/48]  Time: 0.136 (0.581)  Loss:  1.0312 (1.5943)  Acc@1: 82.3113 (69.4340)  Acc@5: 94.1038 (89.3620)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-102.pth.tar', 69.54600004394531)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-104.pth.tar', 69.43399994384765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-103.pth.tar', 69.31199991943359)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-92.pth.tar', 69.22800004882812)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-93.pth.tar', 68.96200010253906)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-94.pth.tar', 68.94400014648437)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-98.pth.tar', 68.89999997314453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-99.pth.tar', 68.87400004638671)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-97.pth.tar', 68.82999997558593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-95.pth.tar', 68.79399996337891)

Train: 105 [   0/1251 (  0%)]  Loss: 3.956 (3.96)  Time: 2.337s,  438.10/s  (2.337s,  438.10/s)  LR: 9.271e-04  Data: 1.669 (1.669)
Train: 105 [  50/1251 (  4%)]  Loss: 3.997 (3.98)  Time: 0.668s, 1532.38/s  (0.733s, 1396.69/s)  LR: 9.271e-04  Data: 0.011 (0.051)
Train: 105 [ 100/1251 (  8%)]  Loss: 3.585 (3.85)  Time: 0.671s, 1526.85/s  (0.713s, 1436.00/s)  LR: 9.271e-04  Data: 0.010 (0.030)
Train: 105 [ 150/1251 ( 12%)]  Loss: 3.597 (3.78)  Time: 0.686s, 1493.17/s  (0.705s, 1452.08/s)  LR: 9.271e-04  Data: 0.010 (0.024)
Train: 105 [ 200/1251 ( 16%)]  Loss: 4.265 (3.88)  Time: 0.699s, 1464.65/s  (0.700s, 1462.85/s)  LR: 9.271e-04  Data: 0.009 (0.020)
Train: 105 [ 250/1251 ( 20%)]  Loss: 3.992 (3.90)  Time: 0.709s, 1445.21/s  (0.700s, 1463.47/s)  LR: 9.271e-04  Data: 0.010 (0.018)
Train: 105 [ 300/1251 ( 24%)]  Loss: 4.017 (3.92)  Time: 0.757s, 1352.49/s  (0.700s, 1463.26/s)  LR: 9.271e-04  Data: 0.011 (0.017)
Train: 105 [ 350/1251 ( 28%)]  Loss: 4.133 (3.94)  Time: 0.705s, 1452.87/s  (0.698s, 1466.75/s)  LR: 9.271e-04  Data: 0.009 (0.016)
Train: 105 [ 400/1251 ( 32%)]  Loss: 4.130 (3.96)  Time: 0.672s, 1524.81/s  (0.697s, 1469.48/s)  LR: 9.271e-04  Data: 0.010 (0.015)
Train: 105 [ 450/1251 ( 36%)]  Loss: 3.911 (3.96)  Time: 0.732s, 1398.57/s  (0.697s, 1470.05/s)  LR: 9.271e-04  Data: 0.015 (0.015)
Train: 105 [ 500/1251 ( 40%)]  Loss: 3.871 (3.95)  Time: 0.732s, 1398.77/s  (0.698s, 1466.83/s)  LR: 9.271e-04  Data: 0.011 (0.015)
Train: 105 [ 550/1251 ( 44%)]  Loss: 3.902 (3.95)  Time: 0.693s, 1477.29/s  (0.700s, 1463.86/s)  LR: 9.271e-04  Data: 0.011 (0.014)
Train: 105 [ 600/1251 ( 48%)]  Loss: 4.245 (3.97)  Time: 0.740s, 1383.41/s  (0.700s, 1463.00/s)  LR: 9.271e-04  Data: 0.014 (0.014)
Train: 105 [ 650/1251 ( 52%)]  Loss: 3.834 (3.96)  Time: 0.673s, 1522.65/s  (0.699s, 1464.91/s)  LR: 9.271e-04  Data: 0.009 (0.014)
Train: 105 [ 700/1251 ( 56%)]  Loss: 4.305 (3.98)  Time: 0.671s, 1526.19/s  (0.698s, 1467.34/s)  LR: 9.271e-04  Data: 0.010 (0.014)
Train: 105 [ 750/1251 ( 60%)]  Loss: 3.832 (3.97)  Time: 0.705s, 1452.98/s  (0.698s, 1468.09/s)  LR: 9.271e-04  Data: 0.010 (0.013)
Train: 105 [ 800/1251 ( 64%)]  Loss: 4.079 (3.98)  Time: 0.712s, 1438.41/s  (0.697s, 1468.57/s)  LR: 9.271e-04  Data: 0.009 (0.013)
Train: 105 [ 850/1251 ( 68%)]  Loss: 4.076 (3.98)  Time: 0.702s, 1459.11/s  (0.697s, 1469.13/s)  LR: 9.271e-04  Data: 0.009 (0.013)
Train: 105 [ 900/1251 ( 72%)]  Loss: 3.777 (3.97)  Time: 0.687s, 1490.93/s  (0.697s, 1469.75/s)  LR: 9.271e-04  Data: 0.010 (0.013)
Train: 105 [ 950/1251 ( 76%)]  Loss: 3.949 (3.97)  Time: 0.705s, 1452.13/s  (0.697s, 1470.03/s)  LR: 9.271e-04  Data: 0.009 (0.013)
Train: 105 [1000/1251 ( 80%)]  Loss: 3.924 (3.97)  Time: 0.671s, 1527.06/s  (0.697s, 1470.06/s)  LR: 9.271e-04  Data: 0.009 (0.013)
Train: 105 [1050/1251 ( 84%)]  Loss: 4.074 (3.98)  Time: 0.671s, 1527.21/s  (0.697s, 1469.99/s)  LR: 9.271e-04  Data: 0.011 (0.012)
Train: 105 [1100/1251 ( 88%)]  Loss: 4.013 (3.98)  Time: 0.673s, 1522.13/s  (0.696s, 1470.70/s)  LR: 9.271e-04  Data: 0.010 (0.012)
Train: 105 [1150/1251 ( 92%)]  Loss: 3.968 (3.98)  Time: 0.709s, 1443.95/s  (0.697s, 1470.12/s)  LR: 9.271e-04  Data: 0.009 (0.012)
Train: 105 [1200/1251 ( 96%)]  Loss: 3.529 (3.96)  Time: 0.696s, 1470.45/s  (0.696s, 1470.57/s)  LR: 9.271e-04  Data: 0.009 (0.012)
Train: 105 [1250/1251 (100%)]  Loss: 4.207 (3.97)  Time: 0.674s, 1519.40/s  (0.696s, 1471.02/s)  LR: 9.271e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.642 (1.642)  Loss:  1.0703 (1.0703)  Acc@1: 85.8398 (85.8398)  Acc@5: 95.8984 (95.8984)
Test: [  48/48]  Time: 0.136 (0.586)  Loss:  1.1934 (1.6555)  Acc@1: 82.4292 (69.8780)  Acc@5: 94.6934 (89.7340)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-105.pth.tar', 69.8779999951172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-102.pth.tar', 69.54600004394531)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-104.pth.tar', 69.43399994384765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-103.pth.tar', 69.31199991943359)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-92.pth.tar', 69.22800004882812)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-93.pth.tar', 68.96200010253906)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-94.pth.tar', 68.94400014648437)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-98.pth.tar', 68.89999997314453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-99.pth.tar', 68.87400004638671)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-97.pth.tar', 68.82999997558593)

Train: 106 [   0/1251 (  0%)]  Loss: 3.778 (3.78)  Time: 2.346s,  436.40/s  (2.346s,  436.40/s)  LR: 9.257e-04  Data: 1.713 (1.713)
Train: 106 [  50/1251 (  4%)]  Loss: 3.703 (3.74)  Time: 0.708s, 1447.02/s  (0.735s, 1393.40/s)  LR: 9.257e-04  Data: 0.009 (0.057)
Train: 106 [ 100/1251 (  8%)]  Loss: 4.026 (3.84)  Time: 0.731s, 1401.27/s  (0.714s, 1434.28/s)  LR: 9.257e-04  Data: 0.010 (0.034)
Train: 106 [ 150/1251 ( 12%)]  Loss: 3.689 (3.80)  Time: 0.715s, 1431.73/s  (0.710s, 1442.10/s)  LR: 9.257e-04  Data: 0.010 (0.026)
Train: 106 [ 200/1251 ( 16%)]  Loss: 3.911 (3.82)  Time: 0.670s, 1529.25/s  (0.706s, 1451.00/s)  LR: 9.257e-04  Data: 0.011 (0.022)
Train: 106 [ 250/1251 ( 20%)]  Loss: 3.775 (3.81)  Time: 0.715s, 1431.58/s  (0.704s, 1454.84/s)  LR: 9.257e-04  Data: 0.010 (0.020)
Train: 106 [ 300/1251 ( 24%)]  Loss: 3.924 (3.83)  Time: 0.676s, 1514.32/s  (0.702s, 1458.38/s)  LR: 9.257e-04  Data: 0.009 (0.018)
Train: 106 [ 350/1251 ( 28%)]  Loss: 3.648 (3.81)  Time: 0.667s, 1534.52/s  (0.700s, 1462.56/s)  LR: 9.257e-04  Data: 0.011 (0.017)
Train: 106 [ 400/1251 ( 32%)]  Loss: 3.614 (3.79)  Time: 0.725s, 1413.13/s  (0.699s, 1465.97/s)  LR: 9.257e-04  Data: 0.011 (0.016)
Train: 106 [ 450/1251 ( 36%)]  Loss: 4.024 (3.81)  Time: 0.679s, 1508.29/s  (0.698s, 1467.41/s)  LR: 9.257e-04  Data: 0.009 (0.015)
Train: 106 [ 500/1251 ( 40%)]  Loss: 3.731 (3.80)  Time: 0.671s, 1527.06/s  (0.698s, 1467.56/s)  LR: 9.257e-04  Data: 0.009 (0.015)
Train: 106 [ 550/1251 ( 44%)]  Loss: 3.999 (3.82)  Time: 0.701s, 1461.54/s  (0.697s, 1468.62/s)  LR: 9.257e-04  Data: 0.010 (0.015)
Train: 106 [ 600/1251 ( 48%)]  Loss: 3.634 (3.80)  Time: 0.723s, 1416.18/s  (0.697s, 1469.81/s)  LR: 9.257e-04  Data: 0.010 (0.014)
Train: 106 [ 650/1251 ( 52%)]  Loss: 3.984 (3.82)  Time: 0.720s, 1422.72/s  (0.697s, 1469.80/s)  LR: 9.257e-04  Data: 0.010 (0.014)
Train: 106 [ 700/1251 ( 56%)]  Loss: 3.493 (3.80)  Time: 0.705s, 1452.85/s  (0.696s, 1470.54/s)  LR: 9.257e-04  Data: 0.009 (0.014)
Train: 106 [ 750/1251 ( 60%)]  Loss: 3.677 (3.79)  Time: 0.709s, 1444.59/s  (0.696s, 1471.90/s)  LR: 9.257e-04  Data: 0.010 (0.013)
Train: 106 [ 800/1251 ( 64%)]  Loss: 4.148 (3.81)  Time: 0.706s, 1449.93/s  (0.695s, 1472.82/s)  LR: 9.257e-04  Data: 0.010 (0.013)
Train: 106 [ 850/1251 ( 68%)]  Loss: 4.240 (3.83)  Time: 0.718s, 1425.54/s  (0.696s, 1471.95/s)  LR: 9.257e-04  Data: 0.010 (0.013)
Train: 106 [ 900/1251 ( 72%)]  Loss: 3.855 (3.83)  Time: 0.712s, 1438.15/s  (0.696s, 1471.89/s)  LR: 9.257e-04  Data: 0.010 (0.013)
Train: 106 [ 950/1251 ( 76%)]  Loss: 3.931 (3.84)  Time: 0.672s, 1524.43/s  (0.696s, 1471.74/s)  LR: 9.257e-04  Data: 0.010 (0.013)
Train: 106 [1000/1251 ( 80%)]  Loss: 3.896 (3.84)  Time: 0.671s, 1526.49/s  (0.696s, 1472.02/s)  LR: 9.257e-04  Data: 0.012 (0.013)
Train: 106 [1050/1251 ( 84%)]  Loss: 3.977 (3.85)  Time: 0.684s, 1498.08/s  (0.695s, 1472.82/s)  LR: 9.257e-04  Data: 0.009 (0.013)
Train: 106 [1100/1251 ( 88%)]  Loss: 4.020 (3.86)  Time: 0.717s, 1427.42/s  (0.695s, 1473.07/s)  LR: 9.257e-04  Data: 0.010 (0.012)
Train: 106 [1150/1251 ( 92%)]  Loss: 3.369 (3.84)  Time: 0.670s, 1528.73/s  (0.695s, 1473.46/s)  LR: 9.257e-04  Data: 0.010 (0.012)
Train: 106 [1200/1251 ( 96%)]  Loss: 3.835 (3.84)  Time: 0.676s, 1514.95/s  (0.695s, 1473.87/s)  LR: 9.257e-04  Data: 0.010 (0.012)
Train: 106 [1250/1251 (100%)]  Loss: 3.950 (3.84)  Time: 0.657s, 1559.56/s  (0.695s, 1474.40/s)  LR: 9.257e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.608 (1.608)  Loss:  1.0049 (1.0049)  Acc@1: 84.6680 (84.6680)  Acc@5: 95.7031 (95.7031)
Test: [  48/48]  Time: 0.136 (0.585)  Loss:  1.1328 (1.6267)  Acc@1: 79.5991 (68.7040)  Acc@5: 94.5755 (88.9720)
Train: 107 [   0/1251 (  0%)]  Loss: 3.766 (3.77)  Time: 2.181s,  469.57/s  (2.181s,  469.57/s)  LR: 9.243e-04  Data: 1.563 (1.563)
Train: 107 [  50/1251 (  4%)]  Loss: 4.155 (3.96)  Time: 0.695s, 1474.01/s  (0.731s, 1400.18/s)  LR: 9.243e-04  Data: 0.010 (0.047)
Train: 107 [ 100/1251 (  8%)]  Loss: 3.988 (3.97)  Time: 0.731s, 1400.03/s  (0.713s, 1435.26/s)  LR: 9.243e-04  Data: 0.009 (0.029)
Train: 107 [ 150/1251 ( 12%)]  Loss: 3.809 (3.93)  Time: 0.673s, 1521.58/s  (0.708s, 1446.75/s)  LR: 9.243e-04  Data: 0.010 (0.023)
Train: 107 [ 200/1251 ( 16%)]  Loss: 3.722 (3.89)  Time: 0.668s, 1533.16/s  (0.703s, 1457.03/s)  LR: 9.243e-04  Data: 0.010 (0.020)
Train: 107 [ 250/1251 ( 20%)]  Loss: 3.963 (3.90)  Time: 0.715s, 1431.39/s  (0.701s, 1461.18/s)  LR: 9.243e-04  Data: 0.010 (0.018)
Train: 107 [ 300/1251 ( 24%)]  Loss: 4.130 (3.93)  Time: 0.706s, 1449.55/s  (0.700s, 1461.82/s)  LR: 9.243e-04  Data: 0.010 (0.017)
Train: 107 [ 350/1251 ( 28%)]  Loss: 3.986 (3.94)  Time: 0.722s, 1418.98/s  (0.700s, 1462.97/s)  LR: 9.243e-04  Data: 0.009 (0.016)
Train: 107 [ 400/1251 ( 32%)]  Loss: 3.874 (3.93)  Time: 0.672s, 1523.83/s  (0.700s, 1463.08/s)  LR: 9.243e-04  Data: 0.010 (0.015)
Train: 107 [ 450/1251 ( 36%)]  Loss: 3.553 (3.89)  Time: 0.706s, 1450.97/s  (0.699s, 1464.28/s)  LR: 9.243e-04  Data: 0.010 (0.015)
Train: 107 [ 500/1251 ( 40%)]  Loss: 3.820 (3.89)  Time: 0.701s, 1460.98/s  (0.699s, 1464.25/s)  LR: 9.243e-04  Data: 0.009 (0.014)
Train: 107 [ 550/1251 ( 44%)]  Loss: 3.297 (3.84)  Time: 0.697s, 1469.63/s  (0.699s, 1465.11/s)  LR: 9.243e-04  Data: 0.010 (0.014)
Train: 107 [ 600/1251 ( 48%)]  Loss: 3.653 (3.82)  Time: 0.688s, 1488.65/s  (0.698s, 1466.05/s)  LR: 9.243e-04  Data: 0.009 (0.014)
Train: 107 [ 650/1251 ( 52%)]  Loss: 4.274 (3.86)  Time: 0.685s, 1494.50/s  (0.698s, 1466.11/s)  LR: 9.243e-04  Data: 0.010 (0.013)
Train: 107 [ 700/1251 ( 56%)]  Loss: 3.749 (3.85)  Time: 0.712s, 1438.40/s  (0.698s, 1466.45/s)  LR: 9.243e-04  Data: 0.010 (0.013)
Train: 107 [ 750/1251 ( 60%)]  Loss: 4.004 (3.86)  Time: 0.710s, 1442.48/s  (0.698s, 1467.02/s)  LR: 9.243e-04  Data: 0.010 (0.013)
Train: 107 [ 800/1251 ( 64%)]  Loss: 4.143 (3.88)  Time: 0.673s, 1522.26/s  (0.697s, 1468.15/s)  LR: 9.243e-04  Data: 0.011 (0.013)
Train: 107 [ 850/1251 ( 68%)]  Loss: 3.702 (3.87)  Time: 0.713s, 1437.14/s  (0.697s, 1469.25/s)  LR: 9.243e-04  Data: 0.009 (0.013)
Train: 107 [ 900/1251 ( 72%)]  Loss: 3.610 (3.85)  Time: 0.699s, 1465.60/s  (0.697s, 1470.16/s)  LR: 9.243e-04  Data: 0.011 (0.012)
Train: 107 [ 950/1251 ( 76%)]  Loss: 3.630 (3.84)  Time: 0.710s, 1441.89/s  (0.697s, 1469.45/s)  LR: 9.243e-04  Data: 0.010 (0.012)
Train: 107 [1000/1251 ( 80%)]  Loss: 4.007 (3.85)  Time: 0.722s, 1418.41/s  (0.697s, 1469.16/s)  LR: 9.243e-04  Data: 0.010 (0.012)
Train: 107 [1050/1251 ( 84%)]  Loss: 4.085 (3.86)  Time: 0.761s, 1345.80/s  (0.697s, 1468.47/s)  LR: 9.243e-04  Data: 0.010 (0.012)
Train: 107 [1100/1251 ( 88%)]  Loss: 4.108 (3.87)  Time: 0.678s, 1509.46/s  (0.697s, 1469.29/s)  LR: 9.243e-04  Data: 0.009 (0.012)
Train: 107 [1150/1251 ( 92%)]  Loss: 3.720 (3.86)  Time: 0.678s, 1510.98/s  (0.697s, 1468.76/s)  LR: 9.243e-04  Data: 0.009 (0.012)
Train: 107 [1200/1251 ( 96%)]  Loss: 3.525 (3.85)  Time: 0.695s, 1472.67/s  (0.697s, 1469.34/s)  LR: 9.243e-04  Data: 0.009 (0.012)
Train: 107 [1250/1251 (100%)]  Loss: 3.916 (3.85)  Time: 0.656s, 1562.00/s  (0.696s, 1470.47/s)  LR: 9.243e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.454 (1.454)  Loss:  0.8828 (0.8828)  Acc@1: 86.9141 (86.9141)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.136 (0.585)  Loss:  0.8955 (1.5666)  Acc@1: 83.0189 (69.1480)  Acc@5: 95.7547 (89.1680)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-105.pth.tar', 69.8779999951172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-102.pth.tar', 69.54600004394531)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-104.pth.tar', 69.43399994384765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-103.pth.tar', 69.31199991943359)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-92.pth.tar', 69.22800004882812)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-107.pth.tar', 69.14799999267578)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-93.pth.tar', 68.96200010253906)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-94.pth.tar', 68.94400014648437)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-98.pth.tar', 68.89999997314453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-99.pth.tar', 68.87400004638671)

Train: 108 [   0/1251 (  0%)]  Loss: 4.218 (4.22)  Time: 2.152s,  475.74/s  (2.152s,  475.74/s)  LR: 9.229e-04  Data: 1.536 (1.536)
Train: 108 [  50/1251 (  4%)]  Loss: 3.907 (4.06)  Time: 0.667s, 1534.82/s  (0.732s, 1398.15/s)  LR: 9.229e-04  Data: 0.010 (0.047)
Train: 108 [ 100/1251 (  8%)]  Loss: 4.093 (4.07)  Time: 0.678s, 1511.40/s  (0.711s, 1439.93/s)  LR: 9.229e-04  Data: 0.011 (0.029)
Train: 108 [ 150/1251 ( 12%)]  Loss: 4.031 (4.06)  Time: 0.693s, 1478.31/s  (0.705s, 1452.64/s)  LR: 9.229e-04  Data: 0.010 (0.023)
Train: 108 [ 200/1251 ( 16%)]  Loss: 3.999 (4.05)  Time: 0.695s, 1473.85/s  (0.705s, 1452.90/s)  LR: 9.229e-04  Data: 0.009 (0.020)
Train: 108 [ 250/1251 ( 20%)]  Loss: 3.565 (3.97)  Time: 0.715s, 1431.89/s  (0.703s, 1457.05/s)  LR: 9.229e-04  Data: 0.010 (0.018)
Train: 108 [ 300/1251 ( 24%)]  Loss: 3.813 (3.95)  Time: 0.677s, 1512.42/s  (0.702s, 1458.82/s)  LR: 9.229e-04  Data: 0.013 (0.017)
Train: 108 [ 350/1251 ( 28%)]  Loss: 4.035 (3.96)  Time: 0.675s, 1517.23/s  (0.700s, 1462.36/s)  LR: 9.229e-04  Data: 0.011 (0.016)
Train: 108 [ 400/1251 ( 32%)]  Loss: 3.663 (3.92)  Time: 0.679s, 1507.95/s  (0.700s, 1463.42/s)  LR: 9.229e-04  Data: 0.010 (0.015)
Train: 108 [ 450/1251 ( 36%)]  Loss: 4.127 (3.95)  Time: 0.671s, 1526.54/s  (0.700s, 1462.73/s)  LR: 9.229e-04  Data: 0.011 (0.015)
Train: 108 [ 500/1251 ( 40%)]  Loss: 3.677 (3.92)  Time: 0.677s, 1512.18/s  (0.699s, 1464.12/s)  LR: 9.229e-04  Data: 0.010 (0.014)
Train: 108 [ 550/1251 ( 44%)]  Loss: 4.021 (3.93)  Time: 0.674s, 1519.90/s  (0.699s, 1465.41/s)  LR: 9.229e-04  Data: 0.011 (0.014)
Train: 108 [ 600/1251 ( 48%)]  Loss: 3.688 (3.91)  Time: 0.668s, 1532.40/s  (0.698s, 1467.35/s)  LR: 9.229e-04  Data: 0.010 (0.013)
Train: 108 [ 650/1251 ( 52%)]  Loss: 4.056 (3.92)  Time: 0.701s, 1460.72/s  (0.697s, 1468.60/s)  LR: 9.229e-04  Data: 0.010 (0.013)
Train: 108 [ 700/1251 ( 56%)]  Loss: 4.158 (3.94)  Time: 0.670s, 1527.51/s  (0.697s, 1469.29/s)  LR: 9.229e-04  Data: 0.012 (0.013)
Train: 108 [ 750/1251 ( 60%)]  Loss: 3.987 (3.94)  Time: 0.682s, 1501.75/s  (0.697s, 1470.20/s)  LR: 9.229e-04  Data: 0.010 (0.013)
Train: 108 [ 800/1251 ( 64%)]  Loss: 3.798 (3.93)  Time: 0.671s, 1527.21/s  (0.696s, 1471.07/s)  LR: 9.229e-04  Data: 0.009 (0.013)
Train: 108 [ 850/1251 ( 68%)]  Loss: 3.668 (3.92)  Time: 0.693s, 1477.11/s  (0.696s, 1472.08/s)  LR: 9.229e-04  Data: 0.010 (0.013)
Train: 108 [ 900/1251 ( 72%)]  Loss: 3.939 (3.92)  Time: 0.673s, 1520.58/s  (0.696s, 1472.22/s)  LR: 9.229e-04  Data: 0.010 (0.012)
Train: 108 [ 950/1251 ( 76%)]  Loss: 3.884 (3.92)  Time: 0.704s, 1454.57/s  (0.695s, 1472.58/s)  LR: 9.229e-04  Data: 0.010 (0.012)
Train: 108 [1000/1251 ( 80%)]  Loss: 3.594 (3.90)  Time: 0.678s, 1509.62/s  (0.695s, 1472.65/s)  LR: 9.229e-04  Data: 0.016 (0.012)
Train: 108 [1050/1251 ( 84%)]  Loss: 3.909 (3.90)  Time: 0.679s, 1507.16/s  (0.695s, 1473.11/s)  LR: 9.229e-04  Data: 0.010 (0.012)
Train: 108 [1100/1251 ( 88%)]  Loss: 3.852 (3.90)  Time: 0.725s, 1413.35/s  (0.695s, 1473.46/s)  LR: 9.229e-04  Data: 0.009 (0.012)
Train: 108 [1150/1251 ( 92%)]  Loss: 4.105 (3.91)  Time: 0.697s, 1468.96/s  (0.695s, 1474.06/s)  LR: 9.229e-04  Data: 0.010 (0.012)
Train: 108 [1200/1251 ( 96%)]  Loss: 3.987 (3.91)  Time: 0.674s, 1518.67/s  (0.694s, 1474.56/s)  LR: 9.229e-04  Data: 0.011 (0.012)
Train: 108 [1250/1251 (100%)]  Loss: 3.802 (3.91)  Time: 0.660s, 1551.64/s  (0.695s, 1474.35/s)  LR: 9.229e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.630 (1.630)  Loss:  1.1797 (1.1797)  Acc@1: 85.1562 (85.1562)  Acc@5: 95.9961 (95.9961)
Test: [  48/48]  Time: 0.136 (0.583)  Loss:  1.0771 (1.6195)  Acc@1: 82.7830 (69.1020)  Acc@5: 94.6934 (89.0860)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-105.pth.tar', 69.8779999951172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-102.pth.tar', 69.54600004394531)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-104.pth.tar', 69.43399994384765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-103.pth.tar', 69.31199991943359)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-92.pth.tar', 69.22800004882812)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-107.pth.tar', 69.14799999267578)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-108.pth.tar', 69.10200001953125)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-93.pth.tar', 68.96200010253906)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-94.pth.tar', 68.94400014648437)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-98.pth.tar', 68.89999997314453)

Train: 109 [   0/1251 (  0%)]  Loss: 3.728 (3.73)  Time: 2.164s,  473.15/s  (2.164s,  473.15/s)  LR: 9.215e-04  Data: 1.548 (1.548)
Train: 109 [  50/1251 (  4%)]  Loss: 4.080 (3.90)  Time: 0.700s, 1462.22/s  (0.740s, 1383.73/s)  LR: 9.215e-04  Data: 0.009 (0.049)
Train: 109 [ 100/1251 (  8%)]  Loss: 4.096 (3.97)  Time: 0.672s, 1524.53/s  (0.714s, 1433.74/s)  LR: 9.215e-04  Data: 0.011 (0.030)
Train: 109 [ 150/1251 ( 12%)]  Loss: 3.340 (3.81)  Time: 0.704s, 1453.80/s  (0.706s, 1450.26/s)  LR: 9.215e-04  Data: 0.010 (0.023)
Train: 109 [ 200/1251 ( 16%)]  Loss: 3.950 (3.84)  Time: 0.690s, 1484.46/s  (0.704s, 1455.35/s)  LR: 9.215e-04  Data: 0.014 (0.020)
Train: 109 [ 250/1251 ( 20%)]  Loss: 3.897 (3.85)  Time: 0.666s, 1537.95/s  (0.701s, 1460.78/s)  LR: 9.215e-04  Data: 0.011 (0.018)
Train: 109 [ 300/1251 ( 24%)]  Loss: 3.662 (3.82)  Time: 0.672s, 1524.45/s  (0.701s, 1461.52/s)  LR: 9.215e-04  Data: 0.010 (0.017)
Train: 109 [ 350/1251 ( 28%)]  Loss: 4.206 (3.87)  Time: 0.709s, 1444.75/s  (0.701s, 1461.40/s)  LR: 9.215e-04  Data: 0.009 (0.016)
Train: 109 [ 400/1251 ( 32%)]  Loss: 3.877 (3.87)  Time: 0.723s, 1417.05/s  (0.700s, 1463.10/s)  LR: 9.215e-04  Data: 0.010 (0.015)
Train: 109 [ 450/1251 ( 36%)]  Loss: 4.109 (3.89)  Time: 0.671s, 1527.05/s  (0.699s, 1464.42/s)  LR: 9.215e-04  Data: 0.010 (0.015)
Train: 109 [ 500/1251 ( 40%)]  Loss: 4.005 (3.90)  Time: 0.708s, 1446.00/s  (0.699s, 1464.32/s)  LR: 9.215e-04  Data: 0.009 (0.014)
Train: 109 [ 550/1251 ( 44%)]  Loss: 3.709 (3.89)  Time: 0.723s, 1417.16/s  (0.699s, 1465.01/s)  LR: 9.215e-04  Data: 0.009 (0.014)
Train: 109 [ 600/1251 ( 48%)]  Loss: 3.687 (3.87)  Time: 0.749s, 1366.31/s  (0.699s, 1465.44/s)  LR: 9.215e-04  Data: 0.009 (0.014)
Train: 109 [ 650/1251 ( 52%)]  Loss: 4.047 (3.89)  Time: 0.675s, 1517.83/s  (0.698s, 1466.94/s)  LR: 9.215e-04  Data: 0.010 (0.013)
Train: 109 [ 700/1251 ( 56%)]  Loss: 3.692 (3.87)  Time: 0.705s, 1451.82/s  (0.698s, 1467.67/s)  LR: 9.215e-04  Data: 0.010 (0.013)
Train: 109 [ 750/1251 ( 60%)]  Loss: 3.736 (3.86)  Time: 0.671s, 1525.36/s  (0.697s, 1468.25/s)  LR: 9.215e-04  Data: 0.009 (0.013)
Train: 109 [ 800/1251 ( 64%)]  Loss: 4.455 (3.90)  Time: 0.673s, 1521.59/s  (0.697s, 1469.08/s)  LR: 9.215e-04  Data: 0.011 (0.013)
Train: 109 [ 850/1251 ( 68%)]  Loss: 3.913 (3.90)  Time: 0.688s, 1487.73/s  (0.696s, 1470.37/s)  LR: 9.215e-04  Data: 0.010 (0.013)
Train: 109 [ 900/1251 ( 72%)]  Loss: 4.081 (3.91)  Time: 0.705s, 1453.41/s  (0.696s, 1470.69/s)  LR: 9.215e-04  Data: 0.011 (0.012)
Train: 109 [ 950/1251 ( 76%)]  Loss: 4.233 (3.93)  Time: 0.666s, 1537.57/s  (0.696s, 1471.39/s)  LR: 9.215e-04  Data: 0.010 (0.012)
Train: 109 [1000/1251 ( 80%)]  Loss: 3.888 (3.92)  Time: 0.675s, 1516.69/s  (0.696s, 1470.99/s)  LR: 9.215e-04  Data: 0.009 (0.012)
Train: 109 [1050/1251 ( 84%)]  Loss: 4.150 (3.93)  Time: 0.727s, 1408.49/s  (0.696s, 1470.90/s)  LR: 9.215e-04  Data: 0.011 (0.012)
Train: 109 [1100/1251 ( 88%)]  Loss: 3.749 (3.93)  Time: 0.671s, 1525.97/s  (0.696s, 1470.80/s)  LR: 9.215e-04  Data: 0.009 (0.012)
Train: 109 [1150/1251 ( 92%)]  Loss: 4.064 (3.93)  Time: 0.727s, 1407.93/s  (0.696s, 1471.08/s)  LR: 9.215e-04  Data: 0.009 (0.012)
Train: 109 [1200/1251 ( 96%)]  Loss: 3.630 (3.92)  Time: 0.700s, 1462.47/s  (0.696s, 1471.61/s)  LR: 9.215e-04  Data: 0.009 (0.012)
Train: 109 [1250/1251 (100%)]  Loss: 3.876 (3.92)  Time: 0.656s, 1561.48/s  (0.696s, 1472.31/s)  LR: 9.215e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.473 (1.473)  Loss:  1.0635 (1.0635)  Acc@1: 85.8398 (85.8398)  Acc@5: 95.9961 (95.9961)
Test: [  48/48]  Time: 0.136 (0.580)  Loss:  0.9844 (1.6214)  Acc@1: 83.1368 (69.2300)  Acc@5: 95.8726 (89.2140)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-105.pth.tar', 69.8779999951172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-102.pth.tar', 69.54600004394531)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-104.pth.tar', 69.43399994384765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-103.pth.tar', 69.31199991943359)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-109.pth.tar', 69.22999991455079)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-92.pth.tar', 69.22800004882812)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-107.pth.tar', 69.14799999267578)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-108.pth.tar', 69.10200001953125)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-93.pth.tar', 68.96200010253906)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-94.pth.tar', 68.94400014648437)

Train: 110 [   0/1251 (  0%)]  Loss: 4.234 (4.23)  Time: 2.094s,  489.09/s  (2.094s,  489.09/s)  LR: 9.201e-04  Data: 1.465 (1.465)
Train: 110 [  50/1251 (  4%)]  Loss: 3.470 (3.85)  Time: 0.675s, 1515.92/s  (0.731s, 1400.61/s)  LR: 9.201e-04  Data: 0.011 (0.047)
Train: 110 [ 100/1251 (  8%)]  Loss: 4.073 (3.93)  Time: 0.707s, 1448.69/s  (0.712s, 1438.34/s)  LR: 9.201e-04  Data: 0.010 (0.029)
Train: 110 [ 150/1251 ( 12%)]  Loss: 4.151 (3.98)  Time: 0.674s, 1519.73/s  (0.705s, 1452.52/s)  LR: 9.201e-04  Data: 0.011 (0.023)
Train: 110 [ 200/1251 ( 16%)]  Loss: 4.111 (4.01)  Time: 0.671s, 1525.45/s  (0.702s, 1458.43/s)  LR: 9.201e-04  Data: 0.010 (0.020)
Train: 110 [ 250/1251 ( 20%)]  Loss: 3.752 (3.96)  Time: 0.721s, 1420.45/s  (0.701s, 1460.75/s)  LR: 9.201e-04  Data: 0.010 (0.018)
Train: 110 [ 300/1251 ( 24%)]  Loss: 3.792 (3.94)  Time: 0.718s, 1425.26/s  (0.700s, 1461.95/s)  LR: 9.201e-04  Data: 0.009 (0.016)
Train: 110 [ 350/1251 ( 28%)]  Loss: 3.854 (3.93)  Time: 0.711s, 1441.13/s  (0.699s, 1464.89/s)  LR: 9.201e-04  Data: 0.012 (0.016)
Train: 110 [ 400/1251 ( 32%)]  Loss: 4.151 (3.95)  Time: 0.711s, 1439.52/s  (0.699s, 1465.02/s)  LR: 9.201e-04  Data: 0.009 (0.015)
Train: 110 [ 450/1251 ( 36%)]  Loss: 3.908 (3.95)  Time: 0.716s, 1429.37/s  (0.698s, 1467.89/s)  LR: 9.201e-04  Data: 0.010 (0.014)
Train: 110 [ 500/1251 ( 40%)]  Loss: 3.922 (3.95)  Time: 0.667s, 1534.85/s  (0.697s, 1469.02/s)  LR: 9.201e-04  Data: 0.009 (0.014)
Train: 110 [ 550/1251 ( 44%)]  Loss: 3.981 (3.95)  Time: 0.701s, 1460.89/s  (0.698s, 1467.49/s)  LR: 9.201e-04  Data: 0.010 (0.014)
Train: 110 [ 600/1251 ( 48%)]  Loss: 3.931 (3.95)  Time: 0.678s, 1510.95/s  (0.697s, 1468.59/s)  LR: 9.201e-04  Data: 0.010 (0.013)
Train: 110 [ 650/1251 ( 52%)]  Loss: 3.984 (3.95)  Time: 0.720s, 1421.89/s  (0.697s, 1468.66/s)  LR: 9.201e-04  Data: 0.011 (0.013)
Train: 110 [ 700/1251 ( 56%)]  Loss: 4.425 (3.98)  Time: 0.720s, 1422.68/s  (0.697s, 1468.87/s)  LR: 9.201e-04  Data: 0.009 (0.013)
Train: 110 [ 750/1251 ( 60%)]  Loss: 3.911 (3.98)  Time: 0.707s, 1447.43/s  (0.697s, 1469.70/s)  LR: 9.201e-04  Data: 0.009 (0.013)
Train: 110 [ 800/1251 ( 64%)]  Loss: 3.626 (3.96)  Time: 0.696s, 1470.29/s  (0.696s, 1470.59/s)  LR: 9.201e-04  Data: 0.010 (0.013)
Train: 110 [ 850/1251 ( 68%)]  Loss: 4.355 (3.98)  Time: 0.671s, 1526.28/s  (0.696s, 1470.82/s)  LR: 9.201e-04  Data: 0.010 (0.012)
Train: 110 [ 900/1251 ( 72%)]  Loss: 3.547 (3.96)  Time: 0.672s, 1523.32/s  (0.696s, 1471.39/s)  LR: 9.201e-04  Data: 0.010 (0.012)
Train: 110 [ 950/1251 ( 76%)]  Loss: 3.673 (3.94)  Time: 0.672s, 1523.80/s  (0.696s, 1471.75/s)  LR: 9.201e-04  Data: 0.010 (0.012)
Train: 110 [1000/1251 ( 80%)]  Loss: 3.805 (3.94)  Time: 0.673s, 1521.30/s  (0.696s, 1472.29/s)  LR: 9.201e-04  Data: 0.010 (0.012)
Train: 110 [1050/1251 ( 84%)]  Loss: 3.862 (3.93)  Time: 0.686s, 1491.81/s  (0.696s, 1472.21/s)  LR: 9.201e-04  Data: 0.011 (0.012)
Train: 110 [1100/1251 ( 88%)]  Loss: 3.966 (3.93)  Time: 0.698s, 1466.53/s  (0.695s, 1472.54/s)  LR: 9.201e-04  Data: 0.009 (0.012)
Train: 110 [1150/1251 ( 92%)]  Loss: 4.175 (3.94)  Time: 0.688s, 1488.23/s  (0.695s, 1473.18/s)  LR: 9.201e-04  Data: 0.009 (0.012)
Train: 110 [1200/1251 ( 96%)]  Loss: 4.151 (3.95)  Time: 0.706s, 1451.41/s  (0.695s, 1473.40/s)  LR: 9.201e-04  Data: 0.010 (0.012)
Train: 110 [1250/1251 (100%)]  Loss: 3.925 (3.95)  Time: 0.657s, 1557.70/s  (0.695s, 1473.81/s)  LR: 9.201e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.614 (1.614)  Loss:  0.9585 (0.9585)  Acc@1: 84.8633 (84.8633)  Acc@5: 95.9961 (95.9961)
Test: [  48/48]  Time: 0.136 (0.587)  Loss:  1.0518 (1.6129)  Acc@1: 82.4292 (69.2180)  Acc@5: 95.1651 (89.1640)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-105.pth.tar', 69.8779999951172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-102.pth.tar', 69.54600004394531)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-104.pth.tar', 69.43399994384765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-103.pth.tar', 69.31199991943359)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-109.pth.tar', 69.22999991455079)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-92.pth.tar', 69.22800004882812)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-110.pth.tar', 69.21799999511718)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-107.pth.tar', 69.14799999267578)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-108.pth.tar', 69.10200001953125)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-93.pth.tar', 68.96200010253906)

Train: 111 [   0/1251 (  0%)]  Loss: 4.191 (4.19)  Time: 2.500s,  409.58/s  (2.500s,  409.58/s)  LR: 9.187e-04  Data: 1.862 (1.862)
Train: 111 [  50/1251 (  4%)]  Loss: 4.000 (4.10)  Time: 0.713s, 1436.64/s  (0.743s, 1378.23/s)  LR: 9.187e-04  Data: 0.016 (0.052)
Train: 111 [ 100/1251 (  8%)]  Loss: 4.113 (4.10)  Time: 0.722s, 1418.80/s  (0.719s, 1424.63/s)  LR: 9.187e-04  Data: 0.010 (0.031)
Train: 111 [ 150/1251 ( 12%)]  Loss: 3.742 (4.01)  Time: 0.682s, 1501.07/s  (0.710s, 1441.45/s)  LR: 9.187e-04  Data: 0.009 (0.024)
Train: 111 [ 200/1251 ( 16%)]  Loss: 3.890 (3.99)  Time: 0.676s, 1515.13/s  (0.706s, 1450.73/s)  LR: 9.187e-04  Data: 0.014 (0.021)
Train: 111 [ 250/1251 ( 20%)]  Loss: 3.865 (3.97)  Time: 0.672s, 1523.46/s  (0.703s, 1456.81/s)  LR: 9.187e-04  Data: 0.010 (0.019)
Train: 111 [ 300/1251 ( 24%)]  Loss: 4.163 (3.99)  Time: 0.698s, 1466.35/s  (0.701s, 1460.70/s)  LR: 9.187e-04  Data: 0.009 (0.017)
Train: 111 [ 350/1251 ( 28%)]  Loss: 3.901 (3.98)  Time: 0.683s, 1500.05/s  (0.700s, 1463.90/s)  LR: 9.187e-04  Data: 0.010 (0.016)
Train: 111 [ 400/1251 ( 32%)]  Loss: 3.784 (3.96)  Time: 0.719s, 1423.67/s  (0.699s, 1464.60/s)  LR: 9.187e-04  Data: 0.009 (0.015)
Train: 111 [ 450/1251 ( 36%)]  Loss: 3.876 (3.95)  Time: 0.768s, 1332.86/s  (0.699s, 1464.63/s)  LR: 9.187e-04  Data: 0.014 (0.015)
Train: 111 [ 500/1251 ( 40%)]  Loss: 3.930 (3.95)  Time: 0.671s, 1526.89/s  (0.699s, 1464.57/s)  LR: 9.187e-04  Data: 0.010 (0.014)
Train: 111 [ 550/1251 ( 44%)]  Loss: 3.788 (3.94)  Time: 0.706s, 1451.32/s  (0.699s, 1465.37/s)  LR: 9.187e-04  Data: 0.011 (0.014)
Train: 111 [ 600/1251 ( 48%)]  Loss: 3.726 (3.92)  Time: 0.697s, 1468.55/s  (0.699s, 1465.47/s)  LR: 9.187e-04  Data: 0.009 (0.014)
Train: 111 [ 650/1251 ( 52%)]  Loss: 4.074 (3.93)  Time: 0.714s, 1434.53/s  (0.699s, 1465.38/s)  LR: 9.187e-04  Data: 0.008 (0.013)
Train: 111 [ 700/1251 ( 56%)]  Loss: 4.211 (3.95)  Time: 0.707s, 1448.68/s  (0.699s, 1465.87/s)  LR: 9.187e-04  Data: 0.009 (0.013)
Train: 111 [ 750/1251 ( 60%)]  Loss: 3.599 (3.93)  Time: 0.673s, 1521.46/s  (0.698s, 1466.63/s)  LR: 9.187e-04  Data: 0.010 (0.013)
Train: 111 [ 800/1251 ( 64%)]  Loss: 3.662 (3.91)  Time: 0.712s, 1438.04/s  (0.699s, 1465.96/s)  LR: 9.187e-04  Data: 0.009 (0.013)
Train: 111 [ 850/1251 ( 68%)]  Loss: 3.872 (3.91)  Time: 0.670s, 1528.51/s  (0.698s, 1466.16/s)  LR: 9.187e-04  Data: 0.010 (0.013)
Train: 111 [ 900/1251 ( 72%)]  Loss: 4.052 (3.92)  Time: 0.677s, 1512.40/s  (0.698s, 1466.80/s)  LR: 9.187e-04  Data: 0.009 (0.013)
Train: 111 [ 950/1251 ( 76%)]  Loss: 3.977 (3.92)  Time: 0.693s, 1477.04/s  (0.698s, 1467.41/s)  LR: 9.187e-04  Data: 0.009 (0.012)
Train: 111 [1000/1251 ( 80%)]  Loss: 4.000 (3.92)  Time: 0.686s, 1492.70/s  (0.697s, 1468.36/s)  LR: 9.187e-04  Data: 0.009 (0.012)
Train: 111 [1050/1251 ( 84%)]  Loss: 3.643 (3.91)  Time: 0.760s, 1347.37/s  (0.697s, 1468.72/s)  LR: 9.187e-04  Data: 0.009 (0.012)
Train: 111 [1100/1251 ( 88%)]  Loss: 4.237 (3.93)  Time: 0.703s, 1457.58/s  (0.697s, 1469.52/s)  LR: 9.187e-04  Data: 0.009 (0.012)
Train: 111 [1150/1251 ( 92%)]  Loss: 4.053 (3.93)  Time: 0.694s, 1475.82/s  (0.697s, 1469.48/s)  LR: 9.187e-04  Data: 0.012 (0.012)
Train: 111 [1200/1251 ( 96%)]  Loss: 4.066 (3.94)  Time: 0.670s, 1527.98/s  (0.696s, 1470.25/s)  LR: 9.187e-04  Data: 0.010 (0.012)
Train: 111 [1250/1251 (100%)]  Loss: 3.848 (3.93)  Time: 0.658s, 1556.54/s  (0.696s, 1470.70/s)  LR: 9.187e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.472 (1.472)  Loss:  0.9121 (0.9121)  Acc@1: 87.5000 (87.5000)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.136 (0.598)  Loss:  0.8501 (1.4991)  Acc@1: 82.6651 (69.9600)  Acc@5: 95.4009 (89.7880)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-111.pth.tar', 69.95999996826171)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-105.pth.tar', 69.8779999951172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-102.pth.tar', 69.54600004394531)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-104.pth.tar', 69.43399994384765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-103.pth.tar', 69.31199991943359)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-109.pth.tar', 69.22999991455079)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-92.pth.tar', 69.22800004882812)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-110.pth.tar', 69.21799999511718)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-107.pth.tar', 69.14799999267578)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-108.pth.tar', 69.10200001953125)

Train: 112 [   0/1251 (  0%)]  Loss: 3.994 (3.99)  Time: 2.199s,  465.76/s  (2.199s,  465.76/s)  LR: 9.173e-04  Data: 1.583 (1.583)
Train: 112 [  50/1251 (  4%)]  Loss: 3.730 (3.86)  Time: 0.676s, 1515.59/s  (0.732s, 1398.07/s)  LR: 9.173e-04  Data: 0.013 (0.051)
Train: 112 [ 100/1251 (  8%)]  Loss: 4.174 (3.97)  Time: 0.727s, 1409.10/s  (0.718s, 1426.85/s)  LR: 9.173e-04  Data: 0.009 (0.031)
Train: 112 [ 150/1251 ( 12%)]  Loss: 3.768 (3.92)  Time: 0.691s, 1482.37/s  (0.708s, 1447.28/s)  LR: 9.173e-04  Data: 0.011 (0.024)
Train: 112 [ 200/1251 ( 16%)]  Loss: 3.720 (3.88)  Time: 0.679s, 1508.22/s  (0.704s, 1454.19/s)  LR: 9.173e-04  Data: 0.009 (0.021)
Train: 112 [ 250/1251 ( 20%)]  Loss: 3.962 (3.89)  Time: 0.671s, 1525.08/s  (0.703s, 1457.59/s)  LR: 9.173e-04  Data: 0.010 (0.018)
Train: 112 [ 300/1251 ( 24%)]  Loss: 3.959 (3.90)  Time: 0.705s, 1451.47/s  (0.702s, 1459.72/s)  LR: 9.173e-04  Data: 0.009 (0.017)
Train: 112 [ 350/1251 ( 28%)]  Loss: 3.178 (3.81)  Time: 0.672s, 1524.08/s  (0.699s, 1464.58/s)  LR: 9.173e-04  Data: 0.010 (0.016)
Train: 112 [ 400/1251 ( 32%)]  Loss: 3.590 (3.79)  Time: 0.668s, 1533.46/s  (0.698s, 1467.49/s)  LR: 9.173e-04  Data: 0.010 (0.015)
Train: 112 [ 450/1251 ( 36%)]  Loss: 3.742 (3.78)  Time: 0.704s, 1454.18/s  (0.698s, 1467.90/s)  LR: 9.173e-04  Data: 0.009 (0.015)
Train: 112 [ 500/1251 ( 40%)]  Loss: 3.890 (3.79)  Time: 0.683s, 1498.83/s  (0.697s, 1470.18/s)  LR: 9.173e-04  Data: 0.011 (0.014)
Train: 112 [ 550/1251 ( 44%)]  Loss: 3.926 (3.80)  Time: 0.676s, 1514.01/s  (0.697s, 1468.66/s)  LR: 9.173e-04  Data: 0.011 (0.014)
Train: 112 [ 600/1251 ( 48%)]  Loss: 3.755 (3.80)  Time: 0.708s, 1447.21/s  (0.697s, 1468.67/s)  LR: 9.173e-04  Data: 0.010 (0.014)
Train: 112 [ 650/1251 ( 52%)]  Loss: 3.802 (3.80)  Time: 0.674s, 1519.68/s  (0.697s, 1468.24/s)  LR: 9.173e-04  Data: 0.011 (0.013)
Train: 112 [ 700/1251 ( 56%)]  Loss: 4.187 (3.83)  Time: 0.672s, 1524.76/s  (0.697s, 1469.80/s)  LR: 9.173e-04  Data: 0.010 (0.013)
Train: 112 [ 750/1251 ( 60%)]  Loss: 3.649 (3.81)  Time: 0.673s, 1521.72/s  (0.696s, 1471.21/s)  LR: 9.173e-04  Data: 0.010 (0.013)
Train: 112 [ 800/1251 ( 64%)]  Loss: 3.940 (3.82)  Time: 0.704s, 1454.52/s  (0.696s, 1471.64/s)  LR: 9.173e-04  Data: 0.010 (0.013)
Train: 112 [ 850/1251 ( 68%)]  Loss: 3.878 (3.82)  Time: 0.677s, 1513.34/s  (0.695s, 1472.67/s)  LR: 9.173e-04  Data: 0.010 (0.013)
Train: 112 [ 900/1251 ( 72%)]  Loss: 3.702 (3.82)  Time: 0.736s, 1391.73/s  (0.695s, 1472.86/s)  LR: 9.173e-04  Data: 0.010 (0.013)
Train: 112 [ 950/1251 ( 76%)]  Loss: 4.217 (3.84)  Time: 0.674s, 1519.69/s  (0.695s, 1472.87/s)  LR: 9.173e-04  Data: 0.010 (0.012)
Train: 112 [1000/1251 ( 80%)]  Loss: 4.086 (3.85)  Time: 0.671s, 1524.99/s  (0.695s, 1473.58/s)  LR: 9.173e-04  Data: 0.010 (0.012)
Train: 112 [1050/1251 ( 84%)]  Loss: 4.080 (3.86)  Time: 0.707s, 1449.38/s  (0.695s, 1474.03/s)  LR: 9.173e-04  Data: 0.011 (0.012)
Train: 112 [1100/1251 ( 88%)]  Loss: 3.889 (3.86)  Time: 0.674s, 1519.03/s  (0.695s, 1473.74/s)  LR: 9.173e-04  Data: 0.010 (0.012)
Train: 112 [1150/1251 ( 92%)]  Loss: 3.885 (3.86)  Time: 0.705s, 1453.46/s  (0.695s, 1473.59/s)  LR: 9.173e-04  Data: 0.010 (0.012)
Train: 112 [1200/1251 ( 96%)]  Loss: 3.857 (3.86)  Time: 0.681s, 1503.44/s  (0.695s, 1473.88/s)  LR: 9.173e-04  Data: 0.010 (0.012)
Train: 112 [1250/1251 (100%)]  Loss: 3.902 (3.86)  Time: 0.693s, 1477.28/s  (0.695s, 1474.09/s)  LR: 9.173e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.583 (1.583)  Loss:  0.8901 (0.8901)  Acc@1: 85.9375 (85.9375)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.136 (0.585)  Loss:  1.0713 (1.5986)  Acc@1: 83.2547 (69.0060)  Acc@5: 95.4009 (89.1900)
Train: 113 [   0/1251 (  0%)]  Loss: 4.020 (4.02)  Time: 2.234s,  458.45/s  (2.234s,  458.45/s)  LR: 9.159e-04  Data: 1.588 (1.588)
Train: 113 [  50/1251 (  4%)]  Loss: 3.944 (3.98)  Time: 0.672s, 1524.84/s  (0.734s, 1394.42/s)  LR: 9.159e-04  Data: 0.009 (0.047)
Train: 113 [ 100/1251 (  8%)]  Loss: 4.235 (4.07)  Time: 0.726s, 1410.55/s  (0.713s, 1435.56/s)  LR: 9.159e-04  Data: 0.010 (0.029)
Train: 113 [ 150/1251 ( 12%)]  Loss: 4.230 (4.11)  Time: 0.671s, 1525.95/s  (0.705s, 1451.76/s)  LR: 9.159e-04  Data: 0.010 (0.023)
Train: 113 [ 200/1251 ( 16%)]  Loss: 3.702 (4.03)  Time: 0.721s, 1420.17/s  (0.703s, 1457.49/s)  LR: 9.159e-04  Data: 0.009 (0.020)
Train: 113 [ 250/1251 ( 20%)]  Loss: 4.035 (4.03)  Time: 0.672s, 1522.69/s  (0.700s, 1463.12/s)  LR: 9.159e-04  Data: 0.011 (0.018)
Train: 113 [ 300/1251 ( 24%)]  Loss: 3.786 (3.99)  Time: 0.672s, 1523.94/s  (0.699s, 1465.53/s)  LR: 9.159e-04  Data: 0.010 (0.017)
Train: 113 [ 350/1251 ( 28%)]  Loss: 3.902 (3.98)  Time: 0.694s, 1474.96/s  (0.699s, 1465.37/s)  LR: 9.159e-04  Data: 0.009 (0.016)
Train: 113 [ 400/1251 ( 32%)]  Loss: 4.099 (3.99)  Time: 0.710s, 1441.72/s  (0.699s, 1465.72/s)  LR: 9.159e-04  Data: 0.009 (0.015)
Train: 113 [ 450/1251 ( 36%)]  Loss: 4.081 (4.00)  Time: 0.702s, 1459.31/s  (0.698s, 1466.68/s)  LR: 9.159e-04  Data: 0.009 (0.014)
Train: 113 [ 500/1251 ( 40%)]  Loss: 3.618 (3.97)  Time: 0.671s, 1524.97/s  (0.698s, 1468.04/s)  LR: 9.159e-04  Data: 0.011 (0.014)
Train: 113 [ 550/1251 ( 44%)]  Loss: 3.843 (3.96)  Time: 0.673s, 1520.52/s  (0.697s, 1469.07/s)  LR: 9.159e-04  Data: 0.009 (0.014)
Train: 113 [ 600/1251 ( 48%)]  Loss: 3.967 (3.96)  Time: 0.704s, 1455.54/s  (0.697s, 1469.20/s)  LR: 9.159e-04  Data: 0.010 (0.013)
Train: 113 [ 650/1251 ( 52%)]  Loss: 3.615 (3.93)  Time: 0.672s, 1524.40/s  (0.697s, 1470.13/s)  LR: 9.159e-04  Data: 0.011 (0.013)
Train: 113 [ 700/1251 ( 56%)]  Loss: 3.912 (3.93)  Time: 0.709s, 1443.68/s  (0.697s, 1469.36/s)  LR: 9.159e-04  Data: 0.013 (0.013)
Train: 113 [ 750/1251 ( 60%)]  Loss: 4.104 (3.94)  Time: 0.680s, 1505.26/s  (0.697s, 1469.79/s)  LR: 9.159e-04  Data: 0.010 (0.013)
Train: 113 [ 800/1251 ( 64%)]  Loss: 4.335 (3.97)  Time: 0.705s, 1453.04/s  (0.697s, 1469.89/s)  LR: 9.159e-04  Data: 0.009 (0.013)
Train: 113 [ 850/1251 ( 68%)]  Loss: 3.746 (3.95)  Time: 0.674s, 1519.29/s  (0.696s, 1470.24/s)  LR: 9.159e-04  Data: 0.009 (0.013)
Train: 113 [ 900/1251 ( 72%)]  Loss: 4.125 (3.96)  Time: 0.675s, 1517.81/s  (0.696s, 1471.12/s)  LR: 9.159e-04  Data: 0.010 (0.012)
Train: 113 [ 950/1251 ( 76%)]  Loss: 3.829 (3.96)  Time: 0.715s, 1432.45/s  (0.696s, 1471.60/s)  LR: 9.159e-04  Data: 0.009 (0.012)
Train: 113 [1000/1251 ( 80%)]  Loss: 4.246 (3.97)  Time: 0.667s, 1536.05/s  (0.696s, 1471.76/s)  LR: 9.159e-04  Data: 0.011 (0.012)
Train: 113 [1050/1251 ( 84%)]  Loss: 4.233 (3.98)  Time: 0.675s, 1517.13/s  (0.696s, 1471.98/s)  LR: 9.159e-04  Data: 0.010 (0.012)
Train: 113 [1100/1251 ( 88%)]  Loss: 3.731 (3.97)  Time: 0.673s, 1522.39/s  (0.696s, 1472.19/s)  LR: 9.159e-04  Data: 0.010 (0.012)
Train: 113 [1150/1251 ( 92%)]  Loss: 4.060 (3.97)  Time: 0.709s, 1443.54/s  (0.695s, 1472.56/s)  LR: 9.159e-04  Data: 0.009 (0.012)
Train: 113 [1200/1251 ( 96%)]  Loss: 4.135 (3.98)  Time: 0.767s, 1334.44/s  (0.695s, 1472.32/s)  LR: 9.159e-04  Data: 0.010 (0.012)
Train: 113 [1250/1251 (100%)]  Loss: 3.690 (3.97)  Time: 0.657s, 1557.77/s  (0.695s, 1473.39/s)  LR: 9.159e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.466 (1.466)  Loss:  1.0723 (1.0723)  Acc@1: 86.0352 (86.0352)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.136 (0.582)  Loss:  1.1045 (1.6650)  Acc@1: 80.4245 (69.1900)  Acc@5: 94.4576 (89.3060)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-111.pth.tar', 69.95999996826171)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-105.pth.tar', 69.8779999951172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-102.pth.tar', 69.54600004394531)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-104.pth.tar', 69.43399994384765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-103.pth.tar', 69.31199991943359)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-109.pth.tar', 69.22999991455079)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-92.pth.tar', 69.22800004882812)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-110.pth.tar', 69.21799999511718)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-113.pth.tar', 69.19000002929687)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-107.pth.tar', 69.14799999267578)

Train: 114 [   0/1251 (  0%)]  Loss: 3.871 (3.87)  Time: 2.179s,  469.96/s  (2.179s,  469.96/s)  LR: 9.144e-04  Data: 1.554 (1.554)
Train: 114 [  50/1251 (  4%)]  Loss: 3.916 (3.89)  Time: 0.696s, 1471.62/s  (0.735s, 1393.20/s)  LR: 9.144e-04  Data: 0.009 (0.049)
Train: 114 [ 100/1251 (  8%)]  Loss: 3.705 (3.83)  Time: 0.674s, 1519.74/s  (0.712s, 1437.31/s)  LR: 9.144e-04  Data: 0.011 (0.030)
Train: 114 [ 150/1251 ( 12%)]  Loss: 4.191 (3.92)  Time: 0.708s, 1445.62/s  (0.705s, 1451.89/s)  LR: 9.144e-04  Data: 0.009 (0.023)
Train: 114 [ 200/1251 ( 16%)]  Loss: 3.947 (3.93)  Time: 0.673s, 1522.04/s  (0.702s, 1459.52/s)  LR: 9.144e-04  Data: 0.010 (0.020)
Train: 114 [ 250/1251 ( 20%)]  Loss: 3.926 (3.93)  Time: 0.683s, 1499.06/s  (0.702s, 1459.25/s)  LR: 9.144e-04  Data: 0.010 (0.018)
Train: 114 [ 300/1251 ( 24%)]  Loss: 3.758 (3.90)  Time: 0.673s, 1522.08/s  (0.701s, 1461.48/s)  LR: 9.144e-04  Data: 0.009 (0.017)
Train: 114 [ 350/1251 ( 28%)]  Loss: 3.908 (3.90)  Time: 0.697s, 1468.34/s  (0.700s, 1462.52/s)  LR: 9.144e-04  Data: 0.014 (0.016)
Train: 114 [ 400/1251 ( 32%)]  Loss: 3.908 (3.90)  Time: 0.674s, 1518.86/s  (0.699s, 1465.21/s)  LR: 9.144e-04  Data: 0.010 (0.015)
Train: 114 [ 450/1251 ( 36%)]  Loss: 4.185 (3.93)  Time: 0.718s, 1427.01/s  (0.698s, 1467.18/s)  LR: 9.144e-04  Data: 0.009 (0.015)
Train: 114 [ 500/1251 ( 40%)]  Loss: 4.031 (3.94)  Time: 0.703s, 1455.61/s  (0.698s, 1467.05/s)  LR: 9.144e-04  Data: 0.010 (0.014)
Train: 114 [ 550/1251 ( 44%)]  Loss: 3.665 (3.92)  Time: 0.684s, 1496.47/s  (0.698s, 1467.68/s)  LR: 9.144e-04  Data: 0.013 (0.014)
Train: 114 [ 600/1251 ( 48%)]  Loss: 3.894 (3.92)  Time: 0.752s, 1362.42/s  (0.698s, 1467.75/s)  LR: 9.144e-04  Data: 0.009 (0.014)
Train: 114 [ 650/1251 ( 52%)]  Loss: 3.761 (3.90)  Time: 0.673s, 1521.21/s  (0.697s, 1468.60/s)  LR: 9.144e-04  Data: 0.011 (0.013)
Train: 114 [ 700/1251 ( 56%)]  Loss: 4.146 (3.92)  Time: 0.672s, 1522.70/s  (0.697s, 1469.70/s)  LR: 9.144e-04  Data: 0.010 (0.013)
Train: 114 [ 750/1251 ( 60%)]  Loss: 4.029 (3.93)  Time: 0.705s, 1452.18/s  (0.697s, 1470.03/s)  LR: 9.144e-04  Data: 0.010 (0.013)
Train: 114 [ 800/1251 ( 64%)]  Loss: 3.916 (3.93)  Time: 0.721s, 1419.79/s  (0.696s, 1470.56/s)  LR: 9.144e-04  Data: 0.010 (0.013)
Train: 114 [ 850/1251 ( 68%)]  Loss: 3.862 (3.92)  Time: 0.679s, 1508.58/s  (0.697s, 1468.91/s)  LR: 9.144e-04  Data: 0.011 (0.013)
Train: 114 [ 900/1251 ( 72%)]  Loss: 4.174 (3.94)  Time: 0.725s, 1413.17/s  (0.699s, 1465.94/s)  LR: 9.144e-04  Data: 0.011 (0.013)
Train: 114 [ 950/1251 ( 76%)]  Loss: 3.745 (3.93)  Time: 0.694s, 1476.01/s  (0.699s, 1464.49/s)  LR: 9.144e-04  Data: 0.010 (0.013)
Train: 114 [1000/1251 ( 80%)]  Loss: 3.569 (3.91)  Time: 0.676s, 1513.88/s  (0.699s, 1465.63/s)  LR: 9.144e-04  Data: 0.011 (0.013)
Train: 114 [1050/1251 ( 84%)]  Loss: 3.546 (3.89)  Time: 0.704s, 1453.67/s  (0.698s, 1466.92/s)  LR: 9.144e-04  Data: 0.010 (0.012)
Train: 114 [1100/1251 ( 88%)]  Loss: 3.967 (3.90)  Time: 0.716s, 1429.87/s  (0.698s, 1467.71/s)  LR: 9.144e-04  Data: 0.011 (0.012)
Train: 114 [1150/1251 ( 92%)]  Loss: 3.577 (3.88)  Time: 0.683s, 1498.65/s  (0.697s, 1468.47/s)  LR: 9.144e-04  Data: 0.009 (0.012)
Train: 114 [1200/1251 ( 96%)]  Loss: 3.918 (3.88)  Time: 0.754s, 1358.34/s  (0.697s, 1469.09/s)  LR: 9.144e-04  Data: 0.010 (0.012)
Train: 114 [1250/1251 (100%)]  Loss: 3.683 (3.88)  Time: 0.655s, 1563.85/s  (0.697s, 1468.94/s)  LR: 9.144e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.469 (1.469)  Loss:  0.8696 (0.8696)  Acc@1: 85.4492 (85.4492)  Acc@5: 96.0938 (96.0938)
Test: [  48/48]  Time: 0.136 (0.572)  Loss:  0.9829 (1.5233)  Acc@1: 83.0189 (69.4900)  Acc@5: 95.1651 (89.3580)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-111.pth.tar', 69.95999996826171)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-105.pth.tar', 69.8779999951172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-102.pth.tar', 69.54600004394531)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-114.pth.tar', 69.48999999267578)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-104.pth.tar', 69.43399994384765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-103.pth.tar', 69.31199991943359)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-109.pth.tar', 69.22999991455079)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-92.pth.tar', 69.22800004882812)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-110.pth.tar', 69.21799999511718)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-113.pth.tar', 69.19000002929687)

Train: 115 [   0/1251 (  0%)]  Loss: 3.910 (3.91)  Time: 2.040s,  502.02/s  (2.040s,  502.02/s)  LR: 9.129e-04  Data: 1.426 (1.426)
Train: 115 [  50/1251 (  4%)]  Loss: 3.801 (3.86)  Time: 0.676s, 1514.72/s  (0.728s, 1406.26/s)  LR: 9.129e-04  Data: 0.009 (0.047)
Train: 115 [ 100/1251 (  8%)]  Loss: 3.717 (3.81)  Time: 0.699s, 1464.82/s  (0.711s, 1439.80/s)  LR: 9.129e-04  Data: 0.010 (0.029)
Train: 115 [ 150/1251 ( 12%)]  Loss: 4.096 (3.88)  Time: 0.699s, 1464.36/s  (0.709s, 1443.78/s)  LR: 9.129e-04  Data: 0.009 (0.023)
Train: 115 [ 200/1251 ( 16%)]  Loss: 3.759 (3.86)  Time: 0.730s, 1402.46/s  (0.706s, 1451.44/s)  LR: 9.129e-04  Data: 0.009 (0.019)
Train: 115 [ 250/1251 ( 20%)]  Loss: 4.116 (3.90)  Time: 0.670s, 1528.33/s  (0.705s, 1453.46/s)  LR: 9.129e-04  Data: 0.009 (0.018)
Train: 115 [ 300/1251 ( 24%)]  Loss: 3.823 (3.89)  Time: 0.673s, 1522.23/s  (0.702s, 1457.79/s)  LR: 9.129e-04  Data: 0.011 (0.016)
Train: 115 [ 350/1251 ( 28%)]  Loss: 3.654 (3.86)  Time: 0.721s, 1420.36/s  (0.701s, 1460.12/s)  LR: 9.129e-04  Data: 0.010 (0.016)
Train: 115 [ 400/1251 ( 32%)]  Loss: 4.165 (3.89)  Time: 0.703s, 1456.95/s  (0.701s, 1460.39/s)  LR: 9.129e-04  Data: 0.010 (0.015)
Train: 115 [ 450/1251 ( 36%)]  Loss: 3.772 (3.88)  Time: 0.674s, 1518.41/s  (0.700s, 1462.08/s)  LR: 9.129e-04  Data: 0.009 (0.014)
Train: 115 [ 500/1251 ( 40%)]  Loss: 3.773 (3.87)  Time: 0.673s, 1521.68/s  (0.700s, 1463.82/s)  LR: 9.129e-04  Data: 0.010 (0.014)
Train: 115 [ 550/1251 ( 44%)]  Loss: 3.719 (3.86)  Time: 0.740s, 1383.67/s  (0.699s, 1464.69/s)  LR: 9.129e-04  Data: 0.011 (0.014)
Train: 115 [ 600/1251 ( 48%)]  Loss: 3.717 (3.85)  Time: 0.700s, 1463.59/s  (0.699s, 1465.81/s)  LR: 9.129e-04  Data: 0.009 (0.013)
Train: 115 [ 650/1251 ( 52%)]  Loss: 3.820 (3.85)  Time: 0.674s, 1518.96/s  (0.698s, 1467.32/s)  LR: 9.129e-04  Data: 0.009 (0.013)
Train: 115 [ 700/1251 ( 56%)]  Loss: 3.837 (3.85)  Time: 0.734s, 1394.49/s  (0.698s, 1467.88/s)  LR: 9.129e-04  Data: 0.010 (0.013)
Train: 115 [ 750/1251 ( 60%)]  Loss: 4.061 (3.86)  Time: 0.682s, 1502.35/s  (0.698s, 1467.78/s)  LR: 9.129e-04  Data: 0.011 (0.013)
Train: 115 [ 800/1251 ( 64%)]  Loss: 4.135 (3.87)  Time: 0.700s, 1462.63/s  (0.698s, 1468.09/s)  LR: 9.129e-04  Data: 0.009 (0.013)
Train: 115 [ 850/1251 ( 68%)]  Loss: 3.976 (3.88)  Time: 0.673s, 1520.68/s  (0.697s, 1469.38/s)  LR: 9.129e-04  Data: 0.010 (0.012)
Train: 115 [ 900/1251 ( 72%)]  Loss: 3.984 (3.89)  Time: 0.670s, 1527.24/s  (0.696s, 1470.30/s)  LR: 9.129e-04  Data: 0.009 (0.012)
Train: 115 [ 950/1251 ( 76%)]  Loss: 4.051 (3.89)  Time: 0.676s, 1515.55/s  (0.696s, 1470.64/s)  LR: 9.129e-04  Data: 0.010 (0.012)
Train: 115 [1000/1251 ( 80%)]  Loss: 3.863 (3.89)  Time: 0.705s, 1451.61/s  (0.696s, 1471.41/s)  LR: 9.129e-04  Data: 0.009 (0.012)
Train: 115 [1050/1251 ( 84%)]  Loss: 3.864 (3.89)  Time: 0.679s, 1508.81/s  (0.696s, 1471.03/s)  LR: 9.129e-04  Data: 0.010 (0.012)
Train: 115 [1100/1251 ( 88%)]  Loss: 3.722 (3.88)  Time: 0.674s, 1520.25/s  (0.696s, 1471.04/s)  LR: 9.129e-04  Data: 0.009 (0.012)
Train: 115 [1150/1251 ( 92%)]  Loss: 4.105 (3.89)  Time: 0.689s, 1487.01/s  (0.696s, 1471.71/s)  LR: 9.129e-04  Data: 0.009 (0.012)
Train: 115 [1200/1251 ( 96%)]  Loss: 4.157 (3.90)  Time: 0.718s, 1427.15/s  (0.696s, 1472.01/s)  LR: 9.129e-04  Data: 0.010 (0.012)
Train: 115 [1250/1251 (100%)]  Loss: 4.325 (3.92)  Time: 0.654s, 1565.16/s  (0.696s, 1472.09/s)  LR: 9.129e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.614 (1.614)  Loss:  0.8809 (0.8809)  Acc@1: 87.6953 (87.6953)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.137 (0.607)  Loss:  1.0586 (1.6037)  Acc@1: 83.0189 (69.2760)  Acc@5: 95.0472 (89.2480)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-111.pth.tar', 69.95999996826171)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-105.pth.tar', 69.8779999951172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-102.pth.tar', 69.54600004394531)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-114.pth.tar', 69.48999999267578)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-104.pth.tar', 69.43399994384765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-103.pth.tar', 69.31199991943359)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-115.pth.tar', 69.27600012207031)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-109.pth.tar', 69.22999991455079)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-92.pth.tar', 69.22800004882812)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-110.pth.tar', 69.21799999511718)

Train: 116 [   0/1251 (  0%)]  Loss: 4.063 (4.06)  Time: 2.246s,  455.87/s  (2.246s,  455.87/s)  LR: 9.115e-04  Data: 1.631 (1.631)
Train: 116 [  50/1251 (  4%)]  Loss: 3.891 (3.98)  Time: 0.707s, 1448.22/s  (0.736s, 1391.52/s)  LR: 9.115e-04  Data: 0.010 (0.048)
Train: 116 [ 100/1251 (  8%)]  Loss: 3.760 (3.90)  Time: 0.677s, 1511.93/s  (0.714s, 1434.94/s)  LR: 9.115e-04  Data: 0.011 (0.029)
Train: 116 [ 150/1251 ( 12%)]  Loss: 3.834 (3.89)  Time: 0.709s, 1444.24/s  (0.706s, 1449.61/s)  LR: 9.115e-04  Data: 0.011 (0.023)
Train: 116 [ 200/1251 ( 16%)]  Loss: 3.745 (3.86)  Time: 0.701s, 1460.70/s  (0.704s, 1453.63/s)  LR: 9.115e-04  Data: 0.010 (0.020)
Train: 116 [ 250/1251 ( 20%)]  Loss: 3.861 (3.86)  Time: 0.683s, 1499.47/s  (0.702s, 1459.70/s)  LR: 9.115e-04  Data: 0.011 (0.018)
Train: 116 [ 300/1251 ( 24%)]  Loss: 4.033 (3.88)  Time: 0.672s, 1524.56/s  (0.700s, 1463.62/s)  LR: 9.115e-04  Data: 0.009 (0.017)
Train: 116 [ 350/1251 ( 28%)]  Loss: 3.696 (3.86)  Time: 0.666s, 1537.66/s  (0.699s, 1465.75/s)  LR: 9.115e-04  Data: 0.011 (0.016)
Train: 116 [ 400/1251 ( 32%)]  Loss: 3.878 (3.86)  Time: 0.665s, 1540.26/s  (0.697s, 1468.17/s)  LR: 9.115e-04  Data: 0.009 (0.015)
Train: 116 [ 450/1251 ( 36%)]  Loss: 3.655 (3.84)  Time: 0.705s, 1452.41/s  (0.698s, 1468.04/s)  LR: 9.115e-04  Data: 0.010 (0.015)
Train: 116 [ 500/1251 ( 40%)]  Loss: 4.245 (3.88)  Time: 0.673s, 1520.54/s  (0.696s, 1470.27/s)  LR: 9.115e-04  Data: 0.011 (0.014)
Train: 116 [ 550/1251 ( 44%)]  Loss: 3.660 (3.86)  Time: 0.702s, 1458.88/s  (0.696s, 1470.40/s)  LR: 9.115e-04  Data: 0.009 (0.014)
Train: 116 [ 600/1251 ( 48%)]  Loss: 3.602 (3.84)  Time: 0.689s, 1486.87/s  (0.697s, 1470.06/s)  LR: 9.115e-04  Data: 0.010 (0.013)
Train: 116 [ 650/1251 ( 52%)]  Loss: 3.942 (3.85)  Time: 0.673s, 1521.84/s  (0.696s, 1471.71/s)  LR: 9.115e-04  Data: 0.011 (0.013)
Train: 116 [ 700/1251 ( 56%)]  Loss: 4.119 (3.87)  Time: 0.698s, 1467.87/s  (0.696s, 1471.89/s)  LR: 9.115e-04  Data: 0.009 (0.013)
Train: 116 [ 750/1251 ( 60%)]  Loss: 3.651 (3.85)  Time: 0.719s, 1424.56/s  (0.695s, 1472.85/s)  LR: 9.115e-04  Data: 0.011 (0.013)
Train: 116 [ 800/1251 ( 64%)]  Loss: 3.809 (3.85)  Time: 0.666s, 1537.64/s  (0.695s, 1473.36/s)  LR: 9.115e-04  Data: 0.010 (0.013)
Train: 116 [ 850/1251 ( 68%)]  Loss: 3.719 (3.84)  Time: 0.704s, 1455.48/s  (0.695s, 1472.69/s)  LR: 9.115e-04  Data: 0.009 (0.012)
Train: 116 [ 900/1251 ( 72%)]  Loss: 4.045 (3.85)  Time: 0.668s, 1532.84/s  (0.696s, 1471.81/s)  LR: 9.115e-04  Data: 0.012 (0.012)
Train: 116 [ 950/1251 ( 76%)]  Loss: 3.854 (3.85)  Time: 0.679s, 1508.67/s  (0.696s, 1471.49/s)  LR: 9.115e-04  Data: 0.011 (0.012)
Train: 116 [1000/1251 ( 80%)]  Loss: 3.930 (3.86)  Time: 0.671s, 1525.10/s  (0.695s, 1472.34/s)  LR: 9.115e-04  Data: 0.009 (0.012)
Train: 116 [1050/1251 ( 84%)]  Loss: 4.217 (3.87)  Time: 0.671s, 1526.08/s  (0.696s, 1472.07/s)  LR: 9.115e-04  Data: 0.010 (0.012)
Train: 116 [1100/1251 ( 88%)]  Loss: 3.858 (3.87)  Time: 0.713s, 1436.26/s  (0.695s, 1472.48/s)  LR: 9.115e-04  Data: 0.010 (0.012)
Train: 116 [1150/1251 ( 92%)]  Loss: 4.132 (3.88)  Time: 0.671s, 1525.06/s  (0.695s, 1472.91/s)  LR: 9.115e-04  Data: 0.010 (0.012)
Train: 116 [1200/1251 ( 96%)]  Loss: 3.752 (3.88)  Time: 0.705s, 1453.32/s  (0.695s, 1473.04/s)  LR: 9.115e-04  Data: 0.009 (0.012)
Train: 116 [1250/1251 (100%)]  Loss: 4.255 (3.89)  Time: 0.657s, 1559.57/s  (0.695s, 1473.71/s)  LR: 9.115e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.490 (1.490)  Loss:  0.9448 (0.9448)  Acc@1: 86.5234 (86.5234)  Acc@5: 96.0938 (96.0938)
Test: [  48/48]  Time: 0.136 (0.583)  Loss:  1.1094 (1.5945)  Acc@1: 81.8396 (69.3360)  Acc@5: 94.5755 (89.3660)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-111.pth.tar', 69.95999996826171)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-105.pth.tar', 69.8779999951172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-102.pth.tar', 69.54600004394531)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-114.pth.tar', 69.48999999267578)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-104.pth.tar', 69.43399994384765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-116.pth.tar', 69.33600012695312)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-103.pth.tar', 69.31199991943359)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-115.pth.tar', 69.27600012207031)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-109.pth.tar', 69.22999991455079)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-92.pth.tar', 69.22800004882812)

Train: 117 [   0/1251 (  0%)]  Loss: 3.974 (3.97)  Time: 2.306s,  443.96/s  (2.306s,  443.96/s)  LR: 9.100e-04  Data: 1.692 (1.692)
Train: 117 [  50/1251 (  4%)]  Loss: 3.830 (3.90)  Time: 0.671s, 1525.57/s  (0.729s, 1404.00/s)  LR: 9.100e-04  Data: 0.010 (0.048)
Train: 117 [ 100/1251 (  8%)]  Loss: 3.988 (3.93)  Time: 0.666s, 1537.55/s  (0.710s, 1442.75/s)  LR: 9.100e-04  Data: 0.010 (0.029)
Train: 117 [ 150/1251 ( 12%)]  Loss: 3.389 (3.80)  Time: 0.706s, 1449.54/s  (0.706s, 1449.86/s)  LR: 9.100e-04  Data: 0.010 (0.023)
Train: 117 [ 200/1251 ( 16%)]  Loss: 3.324 (3.70)  Time: 0.673s, 1522.51/s  (0.702s, 1459.71/s)  LR: 9.100e-04  Data: 0.010 (0.020)
Train: 117 [ 250/1251 ( 20%)]  Loss: 3.749 (3.71)  Time: 0.674s, 1518.72/s  (0.699s, 1464.26/s)  LR: 9.100e-04  Data: 0.010 (0.018)
Train: 117 [ 300/1251 ( 24%)]  Loss: 3.922 (3.74)  Time: 0.708s, 1445.44/s  (0.699s, 1465.42/s)  LR: 9.100e-04  Data: 0.011 (0.017)
Train: 117 [ 350/1251 ( 28%)]  Loss: 3.631 (3.73)  Time: 0.671s, 1526.12/s  (0.697s, 1468.27/s)  LR: 9.100e-04  Data: 0.010 (0.016)
Train: 117 [ 400/1251 ( 32%)]  Loss: 3.930 (3.75)  Time: 0.680s, 1504.82/s  (0.697s, 1469.15/s)  LR: 9.100e-04  Data: 0.016 (0.015)
Train: 117 [ 450/1251 ( 36%)]  Loss: 3.891 (3.76)  Time: 0.674s, 1518.78/s  (0.697s, 1469.94/s)  LR: 9.100e-04  Data: 0.010 (0.014)
Train: 117 [ 500/1251 ( 40%)]  Loss: 3.935 (3.78)  Time: 0.672s, 1523.33/s  (0.695s, 1473.25/s)  LR: 9.100e-04  Data: 0.009 (0.014)
Train: 117 [ 550/1251 ( 44%)]  Loss: 3.881 (3.79)  Time: 0.680s, 1505.03/s  (0.695s, 1473.34/s)  LR: 9.100e-04  Data: 0.010 (0.014)
Train: 117 [ 600/1251 ( 48%)]  Loss: 3.893 (3.80)  Time: 0.671s, 1526.42/s  (0.695s, 1473.83/s)  LR: 9.100e-04  Data: 0.010 (0.013)
Train: 117 [ 650/1251 ( 52%)]  Loss: 3.548 (3.78)  Time: 0.680s, 1505.15/s  (0.695s, 1473.03/s)  LR: 9.100e-04  Data: 0.009 (0.013)
Train: 117 [ 700/1251 ( 56%)]  Loss: 4.045 (3.80)  Time: 0.664s, 1541.40/s  (0.695s, 1474.28/s)  LR: 9.100e-04  Data: 0.009 (0.013)
Train: 117 [ 750/1251 ( 60%)]  Loss: 3.742 (3.79)  Time: 0.697s, 1469.55/s  (0.694s, 1474.92/s)  LR: 9.100e-04  Data: 0.010 (0.013)
Train: 117 [ 800/1251 ( 64%)]  Loss: 3.884 (3.80)  Time: 0.700s, 1462.77/s  (0.694s, 1475.54/s)  LR: 9.100e-04  Data: 0.010 (0.013)
Train: 117 [ 850/1251 ( 68%)]  Loss: 4.183 (3.82)  Time: 0.705s, 1451.52/s  (0.694s, 1475.24/s)  LR: 9.100e-04  Data: 0.010 (0.012)
Train: 117 [ 900/1251 ( 72%)]  Loss: 3.766 (3.82)  Time: 0.706s, 1450.71/s  (0.694s, 1475.93/s)  LR: 9.100e-04  Data: 0.012 (0.012)
Train: 117 [ 950/1251 ( 76%)]  Loss: 4.001 (3.83)  Time: 0.717s, 1428.80/s  (0.694s, 1475.98/s)  LR: 9.100e-04  Data: 0.010 (0.012)
Train: 117 [1000/1251 ( 80%)]  Loss: 3.897 (3.83)  Time: 0.673s, 1521.55/s  (0.694s, 1476.56/s)  LR: 9.100e-04  Data: 0.010 (0.012)
Train: 117 [1050/1251 ( 84%)]  Loss: 4.195 (3.85)  Time: 0.671s, 1526.53/s  (0.693s, 1477.25/s)  LR: 9.100e-04  Data: 0.011 (0.012)
Train: 117 [1100/1251 ( 88%)]  Loss: 3.923 (3.85)  Time: 0.671s, 1526.37/s  (0.693s, 1477.21/s)  LR: 9.100e-04  Data: 0.013 (0.012)
Train: 117 [1150/1251 ( 92%)]  Loss: 4.100 (3.86)  Time: 0.707s, 1448.15/s  (0.693s, 1476.80/s)  LR: 9.100e-04  Data: 0.009 (0.012)
Train: 117 [1200/1251 ( 96%)]  Loss: 4.028 (3.87)  Time: 0.846s, 1210.93/s  (0.694s, 1476.00/s)  LR: 9.100e-04  Data: 0.009 (0.012)
Train: 117 [1250/1251 (100%)]  Loss: 3.960 (3.87)  Time: 0.689s, 1487.21/s  (0.694s, 1475.97/s)  LR: 9.100e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.597 (1.597)  Loss:  0.9502 (0.9502)  Acc@1: 86.1328 (86.1328)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.136 (0.586)  Loss:  1.0254 (1.6344)  Acc@1: 82.6651 (68.8860)  Acc@5: 94.2217 (89.1180)
Train: 118 [   0/1251 (  0%)]  Loss: 3.892 (3.89)  Time: 2.288s,  447.63/s  (2.288s,  447.63/s)  LR: 9.085e-04  Data: 1.619 (1.619)
Train: 118 [  50/1251 (  4%)]  Loss: 3.849 (3.87)  Time: 0.673s, 1521.54/s  (0.744s, 1376.85/s)  LR: 9.085e-04  Data: 0.010 (0.056)
Train: 118 [ 100/1251 (  8%)]  Loss: 3.934 (3.89)  Time: 0.670s, 1527.41/s  (0.716s, 1429.41/s)  LR: 9.085e-04  Data: 0.010 (0.033)
Train: 118 [ 150/1251 ( 12%)]  Loss: 3.767 (3.86)  Time: 0.694s, 1476.51/s  (0.708s, 1445.89/s)  LR: 9.085e-04  Data: 0.013 (0.026)
Train: 118 [ 200/1251 ( 16%)]  Loss: 3.642 (3.82)  Time: 0.671s, 1525.70/s  (0.705s, 1451.60/s)  LR: 9.085e-04  Data: 0.009 (0.022)
Train: 118 [ 250/1251 ( 20%)]  Loss: 3.745 (3.80)  Time: 0.730s, 1403.36/s  (0.703s, 1456.38/s)  LR: 9.085e-04  Data: 0.009 (0.019)
Train: 118 [ 300/1251 ( 24%)]  Loss: 4.033 (3.84)  Time: 0.711s, 1439.97/s  (0.702s, 1459.20/s)  LR: 9.085e-04  Data: 0.009 (0.018)
Train: 118 [ 350/1251 ( 28%)]  Loss: 3.446 (3.79)  Time: 0.673s, 1522.40/s  (0.700s, 1462.38/s)  LR: 9.085e-04  Data: 0.010 (0.017)
Train: 118 [ 400/1251 ( 32%)]  Loss: 3.632 (3.77)  Time: 0.669s, 1531.04/s  (0.699s, 1464.94/s)  LR: 9.085e-04  Data: 0.010 (0.016)
Train: 118 [ 450/1251 ( 36%)]  Loss: 3.994 (3.79)  Time: 0.718s, 1425.39/s  (0.698s, 1467.44/s)  LR: 9.085e-04  Data: 0.009 (0.015)
Train: 118 [ 500/1251 ( 40%)]  Loss: 3.830 (3.80)  Time: 0.686s, 1492.47/s  (0.697s, 1468.50/s)  LR: 9.085e-04  Data: 0.013 (0.015)
Train: 118 [ 550/1251 ( 44%)]  Loss: 4.271 (3.84)  Time: 0.692s, 1479.58/s  (0.697s, 1469.81/s)  LR: 9.085e-04  Data: 0.009 (0.014)
Train: 118 [ 600/1251 ( 48%)]  Loss: 3.937 (3.84)  Time: 0.677s, 1512.54/s  (0.697s, 1469.50/s)  LR: 9.085e-04  Data: 0.010 (0.014)
Train: 118 [ 650/1251 ( 52%)]  Loss: 3.980 (3.85)  Time: 0.675s, 1517.29/s  (0.696s, 1471.27/s)  LR: 9.085e-04  Data: 0.010 (0.014)
Train: 118 [ 700/1251 ( 56%)]  Loss: 4.081 (3.87)  Time: 0.667s, 1535.38/s  (0.696s, 1471.42/s)  LR: 9.085e-04  Data: 0.009 (0.013)
Train: 118 [ 750/1251 ( 60%)]  Loss: 3.670 (3.86)  Time: 0.710s, 1443.11/s  (0.695s, 1472.43/s)  LR: 9.085e-04  Data: 0.010 (0.013)
Train: 118 [ 800/1251 ( 64%)]  Loss: 3.621 (3.84)  Time: 0.672s, 1523.81/s  (0.695s, 1474.00/s)  LR: 9.085e-04  Data: 0.010 (0.013)
Train: 118 [ 850/1251 ( 68%)]  Loss: 3.636 (3.83)  Time: 0.678s, 1509.98/s  (0.695s, 1474.36/s)  LR: 9.085e-04  Data: 0.011 (0.013)
Train: 118 [ 900/1251 ( 72%)]  Loss: 3.679 (3.82)  Time: 0.716s, 1431.14/s  (0.694s, 1474.74/s)  LR: 9.085e-04  Data: 0.010 (0.013)
Train: 118 [ 950/1251 ( 76%)]  Loss: 4.191 (3.84)  Time: 0.714s, 1435.11/s  (0.695s, 1473.94/s)  LR: 9.085e-04  Data: 0.009 (0.013)
Train: 118 [1000/1251 ( 80%)]  Loss: 3.722 (3.84)  Time: 0.672s, 1523.36/s  (0.694s, 1474.94/s)  LR: 9.085e-04  Data: 0.010 (0.012)
Train: 118 [1050/1251 ( 84%)]  Loss: 3.730 (3.83)  Time: 0.679s, 1508.53/s  (0.694s, 1475.65/s)  LR: 9.085e-04  Data: 0.010 (0.012)
Train: 118 [1100/1251 ( 88%)]  Loss: 3.807 (3.83)  Time: 0.708s, 1446.67/s  (0.694s, 1476.08/s)  LR: 9.085e-04  Data: 0.010 (0.012)
Train: 118 [1150/1251 ( 92%)]  Loss: 4.223 (3.85)  Time: 0.667s, 1536.25/s  (0.694s, 1476.24/s)  LR: 9.085e-04  Data: 0.009 (0.012)
Train: 118 [1200/1251 ( 96%)]  Loss: 3.996 (3.85)  Time: 0.672s, 1522.78/s  (0.693s, 1476.58/s)  LR: 9.085e-04  Data: 0.010 (0.012)
Train: 118 [1250/1251 (100%)]  Loss: 3.865 (3.85)  Time: 0.664s, 1541.80/s  (0.693s, 1476.90/s)  LR: 9.085e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.467 (1.467)  Loss:  1.0352 (1.0352)  Acc@1: 86.6211 (86.6211)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.136 (0.582)  Loss:  1.1113 (1.6650)  Acc@1: 80.5425 (69.1160)  Acc@5: 94.9293 (89.1180)
Train: 119 [   0/1251 (  0%)]  Loss: 3.682 (3.68)  Time: 2.132s,  480.23/s  (2.132s,  480.23/s)  LR: 9.070e-04  Data: 1.516 (1.516)
Train: 119 [  50/1251 (  4%)]  Loss: 3.964 (3.82)  Time: 0.705s, 1452.96/s  (0.738s, 1388.13/s)  LR: 9.070e-04  Data: 0.008 (0.045)
Train: 119 [ 100/1251 (  8%)]  Loss: 4.066 (3.90)  Time: 0.710s, 1443.09/s  (0.719s, 1423.42/s)  LR: 9.070e-04  Data: 0.008 (0.028)
Train: 119 [ 150/1251 ( 12%)]  Loss: 3.664 (3.84)  Time: 0.761s, 1345.47/s  (0.712s, 1439.00/s)  LR: 9.070e-04  Data: 0.010 (0.022)
Train: 119 [ 200/1251 ( 16%)]  Loss: 3.820 (3.84)  Time: 0.706s, 1450.80/s  (0.707s, 1448.70/s)  LR: 9.070e-04  Data: 0.011 (0.019)
Train: 119 [ 250/1251 ( 20%)]  Loss: 3.915 (3.85)  Time: 0.704s, 1455.47/s  (0.704s, 1454.63/s)  LR: 9.070e-04  Data: 0.009 (0.017)
Train: 119 [ 300/1251 ( 24%)]  Loss: 3.643 (3.82)  Time: 0.701s, 1460.61/s  (0.703s, 1457.18/s)  LR: 9.070e-04  Data: 0.011 (0.016)
Train: 119 [ 350/1251 ( 28%)]  Loss: 4.309 (3.88)  Time: 0.670s, 1528.49/s  (0.701s, 1460.55/s)  LR: 9.070e-04  Data: 0.010 (0.015)
Train: 119 [ 400/1251 ( 32%)]  Loss: 3.728 (3.87)  Time: 0.672s, 1524.28/s  (0.699s, 1465.00/s)  LR: 9.070e-04  Data: 0.010 (0.015)
Train: 119 [ 450/1251 ( 36%)]  Loss: 3.909 (3.87)  Time: 0.680s, 1505.23/s  (0.699s, 1463.94/s)  LR: 9.070e-04  Data: 0.017 (0.014)
Train: 119 [ 500/1251 ( 40%)]  Loss: 3.597 (3.85)  Time: 0.674s, 1520.03/s  (0.699s, 1464.09/s)  LR: 9.070e-04  Data: 0.009 (0.014)
Train: 119 [ 550/1251 ( 44%)]  Loss: 3.756 (3.84)  Time: 0.672s, 1524.94/s  (0.699s, 1465.64/s)  LR: 9.070e-04  Data: 0.009 (0.013)
Train: 119 [ 600/1251 ( 48%)]  Loss: 3.913 (3.84)  Time: 0.714s, 1434.29/s  (0.698s, 1467.49/s)  LR: 9.070e-04  Data: 0.010 (0.013)
Train: 119 [ 650/1251 ( 52%)]  Loss: 3.706 (3.83)  Time: 0.671s, 1526.11/s  (0.697s, 1468.63/s)  LR: 9.070e-04  Data: 0.010 (0.013)
Train: 119 [ 700/1251 ( 56%)]  Loss: 3.859 (3.84)  Time: 0.674s, 1519.39/s  (0.697s, 1468.96/s)  LR: 9.070e-04  Data: 0.014 (0.013)
Train: 119 [ 750/1251 ( 60%)]  Loss: 4.147 (3.85)  Time: 0.699s, 1465.97/s  (0.697s, 1469.18/s)  LR: 9.070e-04  Data: 0.011 (0.013)
Train: 119 [ 800/1251 ( 64%)]  Loss: 4.161 (3.87)  Time: 0.668s, 1533.83/s  (0.697s, 1469.33/s)  LR: 9.070e-04  Data: 0.009 (0.012)
Train: 119 [ 850/1251 ( 68%)]  Loss: 3.707 (3.86)  Time: 0.706s, 1449.95/s  (0.697s, 1469.70/s)  LR: 9.070e-04  Data: 0.010 (0.012)
Train: 119 [ 900/1251 ( 72%)]  Loss: 3.834 (3.86)  Time: 0.704s, 1453.81/s  (0.697s, 1469.88/s)  LR: 9.070e-04  Data: 0.009 (0.012)
Train: 119 [ 950/1251 ( 76%)]  Loss: 3.871 (3.86)  Time: 0.703s, 1456.03/s  (0.696s, 1470.31/s)  LR: 9.070e-04  Data: 0.009 (0.012)
Train: 119 [1000/1251 ( 80%)]  Loss: 4.104 (3.87)  Time: 0.701s, 1459.78/s  (0.696s, 1470.36/s)  LR: 9.070e-04  Data: 0.009 (0.012)
Train: 119 [1050/1251 ( 84%)]  Loss: 3.516 (3.86)  Time: 0.699s, 1463.91/s  (0.696s, 1470.77/s)  LR: 9.070e-04  Data: 0.010 (0.012)
Train: 119 [1100/1251 ( 88%)]  Loss: 3.752 (3.85)  Time: 0.710s, 1443.13/s  (0.696s, 1471.35/s)  LR: 9.070e-04  Data: 0.009 (0.012)
Train: 119 [1150/1251 ( 92%)]  Loss: 3.923 (3.86)  Time: 0.671s, 1526.69/s  (0.696s, 1471.43/s)  LR: 9.070e-04  Data: 0.011 (0.012)
Train: 119 [1200/1251 ( 96%)]  Loss: 3.601 (3.85)  Time: 0.734s, 1394.42/s  (0.696s, 1471.22/s)  LR: 9.070e-04  Data: 0.010 (0.012)
Train: 119 [1250/1251 (100%)]  Loss: 3.561 (3.83)  Time: 0.656s, 1561.57/s  (0.696s, 1470.61/s)  LR: 9.070e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.617 (1.617)  Loss:  0.9209 (0.9209)  Acc@1: 86.3281 (86.3281)  Acc@5: 95.8008 (95.8008)
Test: [  48/48]  Time: 0.136 (0.596)  Loss:  1.0713 (1.5851)  Acc@1: 79.7170 (69.4540)  Acc@5: 94.6934 (89.4040)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-111.pth.tar', 69.95999996826171)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-105.pth.tar', 69.8779999951172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-102.pth.tar', 69.54600004394531)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-114.pth.tar', 69.48999999267578)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-119.pth.tar', 69.45400010986329)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-104.pth.tar', 69.43399994384765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-116.pth.tar', 69.33600012695312)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-103.pth.tar', 69.31199991943359)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-115.pth.tar', 69.27600012207031)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-109.pth.tar', 69.22999991455079)

Train: 120 [   0/1251 (  0%)]  Loss: 4.006 (4.01)  Time: 2.399s,  426.87/s  (2.399s,  426.87/s)  LR: 9.055e-04  Data: 1.766 (1.766)
Train: 120 [  50/1251 (  4%)]  Loss: 3.611 (3.81)  Time: 0.673s, 1521.85/s  (0.734s, 1395.99/s)  LR: 9.055e-04  Data: 0.010 (0.047)
Train: 120 [ 100/1251 (  8%)]  Loss: 3.869 (3.83)  Time: 0.717s, 1427.37/s  (0.715s, 1431.68/s)  LR: 9.055e-04  Data: 0.009 (0.029)
Train: 120 [ 150/1251 ( 12%)]  Loss: 3.705 (3.80)  Time: 0.701s, 1461.62/s  (0.708s, 1446.10/s)  LR: 9.055e-04  Data: 0.009 (0.023)
Train: 120 [ 200/1251 ( 16%)]  Loss: 3.936 (3.83)  Time: 0.708s, 1446.32/s  (0.704s, 1454.70/s)  LR: 9.055e-04  Data: 0.010 (0.020)
Train: 120 [ 250/1251 ( 20%)]  Loss: 3.928 (3.84)  Time: 0.684s, 1496.06/s  (0.703s, 1456.02/s)  LR: 9.055e-04  Data: 0.010 (0.018)
Train: 120 [ 300/1251 ( 24%)]  Loss: 3.939 (3.86)  Time: 0.702s, 1459.26/s  (0.700s, 1462.36/s)  LR: 9.055e-04  Data: 0.011 (0.016)
Train: 120 [ 350/1251 ( 28%)]  Loss: 4.063 (3.88)  Time: 0.706s, 1449.47/s  (0.700s, 1463.15/s)  LR: 9.055e-04  Data: 0.010 (0.016)
Train: 120 [ 400/1251 ( 32%)]  Loss: 3.962 (3.89)  Time: 0.679s, 1507.60/s  (0.699s, 1464.03/s)  LR: 9.055e-04  Data: 0.011 (0.015)
Train: 120 [ 450/1251 ( 36%)]  Loss: 3.532 (3.86)  Time: 0.687s, 1489.72/s  (0.698s, 1466.65/s)  LR: 9.055e-04  Data: 0.010 (0.014)
Train: 120 [ 500/1251 ( 40%)]  Loss: 4.085 (3.88)  Time: 0.691s, 1481.84/s  (0.697s, 1468.61/s)  LR: 9.055e-04  Data: 0.009 (0.014)
Train: 120 [ 550/1251 ( 44%)]  Loss: 4.131 (3.90)  Time: 0.665s, 1538.96/s  (0.697s, 1468.98/s)  LR: 9.055e-04  Data: 0.011 (0.014)
Train: 120 [ 600/1251 ( 48%)]  Loss: 4.161 (3.92)  Time: 0.706s, 1449.53/s  (0.698s, 1468.01/s)  LR: 9.055e-04  Data: 0.012 (0.013)
Train: 120 [ 650/1251 ( 52%)]  Loss: 3.820 (3.91)  Time: 0.673s, 1521.50/s  (0.697s, 1468.63/s)  LR: 9.055e-04  Data: 0.011 (0.013)
Train: 120 [ 700/1251 ( 56%)]  Loss: 4.099 (3.92)  Time: 0.666s, 1538.25/s  (0.697s, 1469.08/s)  LR: 9.055e-04  Data: 0.011 (0.013)
Train: 120 [ 750/1251 ( 60%)]  Loss: 3.723 (3.91)  Time: 0.720s, 1421.24/s  (0.697s, 1470.12/s)  LR: 9.055e-04  Data: 0.013 (0.013)
Train: 120 [ 800/1251 ( 64%)]  Loss: 3.849 (3.91)  Time: 0.694s, 1476.23/s  (0.696s, 1470.41/s)  LR: 9.055e-04  Data: 0.009 (0.013)
Train: 120 [ 850/1251 ( 68%)]  Loss: 3.606 (3.89)  Time: 0.684s, 1496.80/s  (0.697s, 1470.21/s)  LR: 9.055e-04  Data: 0.010 (0.012)
Train: 120 [ 900/1251 ( 72%)]  Loss: 3.934 (3.89)  Time: 0.671s, 1526.22/s  (0.696s, 1471.44/s)  LR: 9.055e-04  Data: 0.010 (0.012)
Train: 120 [ 950/1251 ( 76%)]  Loss: 3.958 (3.90)  Time: 0.672s, 1524.16/s  (0.696s, 1471.23/s)  LR: 9.055e-04  Data: 0.010 (0.012)
Train: 120 [1000/1251 ( 80%)]  Loss: 4.029 (3.90)  Time: 0.671s, 1527.12/s  (0.696s, 1471.27/s)  LR: 9.055e-04  Data: 0.011 (0.012)
Train: 120 [1050/1251 ( 84%)]  Loss: 4.065 (3.91)  Time: 0.696s, 1470.95/s  (0.696s, 1470.81/s)  LR: 9.055e-04  Data: 0.009 (0.012)
Train: 120 [1100/1251 ( 88%)]  Loss: 3.853 (3.91)  Time: 0.674s, 1520.10/s  (0.696s, 1471.71/s)  LR: 9.055e-04  Data: 0.010 (0.012)
Train: 120 [1150/1251 ( 92%)]  Loss: 3.647 (3.90)  Time: 0.673s, 1521.29/s  (0.695s, 1472.43/s)  LR: 9.055e-04  Data: 0.010 (0.012)
Train: 120 [1200/1251 ( 96%)]  Loss: 3.658 (3.89)  Time: 0.673s, 1522.42/s  (0.696s, 1472.11/s)  LR: 9.055e-04  Data: 0.010 (0.012)
Train: 120 [1250/1251 (100%)]  Loss: 3.578 (3.87)  Time: 0.659s, 1554.67/s  (0.695s, 1472.55/s)  LR: 9.055e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.562 (1.562)  Loss:  1.0840 (1.0840)  Acc@1: 85.9375 (85.9375)  Acc@5: 96.0938 (96.0938)
Test: [  48/48]  Time: 0.136 (0.571)  Loss:  1.1426 (1.6257)  Acc@1: 83.1368 (69.9220)  Acc@5: 95.0472 (89.5540)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-111.pth.tar', 69.95999996826171)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-120.pth.tar', 69.92200004394532)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-105.pth.tar', 69.8779999951172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-102.pth.tar', 69.54600004394531)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-114.pth.tar', 69.48999999267578)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-119.pth.tar', 69.45400010986329)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-104.pth.tar', 69.43399994384765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-116.pth.tar', 69.33600012695312)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-103.pth.tar', 69.31199991943359)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-115.pth.tar', 69.27600012207031)

Train: 121 [   0/1251 (  0%)]  Loss: 4.020 (4.02)  Time: 2.320s,  441.39/s  (2.320s,  441.39/s)  LR: 9.039e-04  Data: 1.704 (1.704)
Train: 121 [  50/1251 (  4%)]  Loss: 3.869 (3.94)  Time: 0.687s, 1490.12/s  (0.738s, 1387.51/s)  LR: 9.039e-04  Data: 0.010 (0.055)
Train: 121 [ 100/1251 (  8%)]  Loss: 3.426 (3.77)  Time: 0.677s, 1512.52/s  (0.718s, 1425.23/s)  LR: 9.039e-04  Data: 0.010 (0.033)
Train: 121 [ 150/1251 ( 12%)]  Loss: 4.308 (3.91)  Time: 0.678s, 1510.42/s  (0.711s, 1440.84/s)  LR: 9.039e-04  Data: 0.009 (0.026)
Train: 121 [ 200/1251 ( 16%)]  Loss: 4.035 (3.93)  Time: 0.690s, 1483.64/s  (0.708s, 1446.79/s)  LR: 9.039e-04  Data: 0.009 (0.022)
Train: 121 [ 250/1251 ( 20%)]  Loss: 3.757 (3.90)  Time: 0.672s, 1523.00/s  (0.704s, 1455.16/s)  LR: 9.039e-04  Data: 0.013 (0.019)
Train: 121 [ 300/1251 ( 24%)]  Loss: 3.695 (3.87)  Time: 0.671s, 1525.17/s  (0.701s, 1460.91/s)  LR: 9.039e-04  Data: 0.010 (0.018)
Train: 121 [ 350/1251 ( 28%)]  Loss: 4.349 (3.93)  Time: 0.699s, 1464.87/s  (0.699s, 1464.34/s)  LR: 9.039e-04  Data: 0.009 (0.017)
Train: 121 [ 400/1251 ( 32%)]  Loss: 3.605 (3.90)  Time: 0.708s, 1445.98/s  (0.698s, 1466.36/s)  LR: 9.039e-04  Data: 0.009 (0.016)
Train: 121 [ 450/1251 ( 36%)]  Loss: 3.950 (3.90)  Time: 0.670s, 1528.22/s  (0.698s, 1466.70/s)  LR: 9.039e-04  Data: 0.009 (0.015)
Train: 121 [ 500/1251 ( 40%)]  Loss: 3.992 (3.91)  Time: 0.666s, 1537.93/s  (0.697s, 1470.02/s)  LR: 9.039e-04  Data: 0.010 (0.015)
Train: 121 [ 550/1251 ( 44%)]  Loss: 3.885 (3.91)  Time: 0.681s, 1502.75/s  (0.697s, 1469.70/s)  LR: 9.039e-04  Data: 0.012 (0.014)
Train: 121 [ 600/1251 ( 48%)]  Loss: 4.220 (3.93)  Time: 0.672s, 1523.45/s  (0.696s, 1471.62/s)  LR: 9.039e-04  Data: 0.012 (0.014)
Train: 121 [ 650/1251 ( 52%)]  Loss: 3.991 (3.94)  Time: 0.721s, 1420.22/s  (0.695s, 1472.83/s)  LR: 9.039e-04  Data: 0.009 (0.014)
Train: 121 [ 700/1251 ( 56%)]  Loss: 3.797 (3.93)  Time: 0.697s, 1469.26/s  (0.695s, 1473.40/s)  LR: 9.039e-04  Data: 0.009 (0.014)
Train: 121 [ 750/1251 ( 60%)]  Loss: 3.937 (3.93)  Time: 0.688s, 1488.41/s  (0.695s, 1473.74/s)  LR: 9.039e-04  Data: 0.009 (0.013)
Train: 121 [ 800/1251 ( 64%)]  Loss: 3.761 (3.92)  Time: 0.695s, 1472.44/s  (0.695s, 1473.98/s)  LR: 9.039e-04  Data: 0.011 (0.013)
Train: 121 [ 850/1251 ( 68%)]  Loss: 3.856 (3.91)  Time: 0.715s, 1432.32/s  (0.695s, 1473.90/s)  LR: 9.039e-04  Data: 0.009 (0.013)
Train: 121 [ 900/1251 ( 72%)]  Loss: 4.054 (3.92)  Time: 0.670s, 1527.94/s  (0.695s, 1473.83/s)  LR: 9.039e-04  Data: 0.010 (0.013)
Train: 121 [ 950/1251 ( 76%)]  Loss: 4.021 (3.93)  Time: 0.730s, 1402.31/s  (0.694s, 1474.92/s)  LR: 9.039e-04  Data: 0.010 (0.013)
Train: 121 [1000/1251 ( 80%)]  Loss: 3.554 (3.91)  Time: 0.727s, 1408.21/s  (0.694s, 1475.77/s)  LR: 9.039e-04  Data: 0.009 (0.013)
Train: 121 [1050/1251 ( 84%)]  Loss: 4.348 (3.93)  Time: 0.673s, 1522.32/s  (0.694s, 1476.16/s)  LR: 9.039e-04  Data: 0.009 (0.012)
Train: 121 [1100/1251 ( 88%)]  Loss: 4.171 (3.94)  Time: 0.680s, 1506.05/s  (0.693s, 1476.82/s)  LR: 9.039e-04  Data: 0.009 (0.012)
Train: 121 [1150/1251 ( 92%)]  Loss: 4.238 (3.95)  Time: 0.685s, 1495.60/s  (0.693s, 1477.01/s)  LR: 9.039e-04  Data: 0.009 (0.012)
Train: 121 [1200/1251 ( 96%)]  Loss: 4.475 (3.97)  Time: 0.673s, 1522.16/s  (0.693s, 1477.07/s)  LR: 9.039e-04  Data: 0.009 (0.012)
Train: 121 [1250/1251 (100%)]  Loss: 3.862 (3.97)  Time: 0.659s, 1554.18/s  (0.693s, 1477.35/s)  LR: 9.039e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.546 (1.546)  Loss:  1.0596 (1.0596)  Acc@1: 86.3281 (86.3281)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.136 (0.583)  Loss:  1.1680 (1.7576)  Acc@1: 82.7830 (68.7220)  Acc@5: 95.1651 (88.8900)
Train: 122 [   0/1251 (  0%)]  Loss: 3.694 (3.69)  Time: 2.210s,  463.30/s  (2.210s,  463.30/s)  LR: 9.024e-04  Data: 1.594 (1.594)
Train: 122 [  50/1251 (  4%)]  Loss: 4.048 (3.87)  Time: 0.671s, 1525.01/s  (0.730s, 1402.64/s)  LR: 9.024e-04  Data: 0.010 (0.051)
Train: 122 [ 100/1251 (  8%)]  Loss: 3.889 (3.88)  Time: 0.672s, 1523.69/s  (0.710s, 1441.61/s)  LR: 9.024e-04  Data: 0.010 (0.031)
Train: 122 [ 150/1251 ( 12%)]  Loss: 3.981 (3.90)  Time: 0.674s, 1519.68/s  (0.705s, 1452.83/s)  LR: 9.024e-04  Data: 0.010 (0.024)
Train: 122 [ 200/1251 ( 16%)]  Loss: 3.518 (3.83)  Time: 0.775s, 1321.75/s  (0.703s, 1455.86/s)  LR: 9.024e-04  Data: 0.014 (0.021)
Train: 122 [ 250/1251 ( 20%)]  Loss: 4.056 (3.86)  Time: 0.725s, 1411.71/s  (0.701s, 1460.68/s)  LR: 9.024e-04  Data: 0.009 (0.019)
Train: 122 [ 300/1251 ( 24%)]  Loss: 4.297 (3.93)  Time: 0.707s, 1448.07/s  (0.699s, 1465.35/s)  LR: 9.024e-04  Data: 0.009 (0.017)
Train: 122 [ 350/1251 ( 28%)]  Loss: 3.555 (3.88)  Time: 0.674s, 1519.13/s  (0.697s, 1469.01/s)  LR: 9.024e-04  Data: 0.011 (0.016)
Train: 122 [ 400/1251 ( 32%)]  Loss: 3.801 (3.87)  Time: 0.667s, 1535.16/s  (0.696s, 1470.84/s)  LR: 9.024e-04  Data: 0.010 (0.015)
Train: 122 [ 450/1251 ( 36%)]  Loss: 4.054 (3.89)  Time: 0.703s, 1455.84/s  (0.695s, 1473.52/s)  LR: 9.024e-04  Data: 0.010 (0.015)
Train: 122 [ 500/1251 ( 40%)]  Loss: 4.047 (3.90)  Time: 0.726s, 1410.27/s  (0.696s, 1472.31/s)  LR: 9.024e-04  Data: 0.009 (0.014)
Train: 122 [ 550/1251 ( 44%)]  Loss: 3.781 (3.89)  Time: 0.697s, 1468.78/s  (0.696s, 1471.91/s)  LR: 9.024e-04  Data: 0.009 (0.014)
Train: 122 [ 600/1251 ( 48%)]  Loss: 3.865 (3.89)  Time: 0.673s, 1522.28/s  (0.697s, 1470.06/s)  LR: 9.024e-04  Data: 0.010 (0.014)
Train: 122 [ 650/1251 ( 52%)]  Loss: 3.541 (3.87)  Time: 0.712s, 1437.41/s  (0.696s, 1471.34/s)  LR: 9.024e-04  Data: 0.011 (0.013)
Train: 122 [ 700/1251 ( 56%)]  Loss: 3.987 (3.87)  Time: 0.673s, 1521.86/s  (0.696s, 1472.02/s)  LR: 9.024e-04  Data: 0.010 (0.013)
Train: 122 [ 750/1251 ( 60%)]  Loss: 3.308 (3.84)  Time: 0.720s, 1421.45/s  (0.695s, 1472.37/s)  LR: 9.024e-04  Data: 0.009 (0.013)
Train: 122 [ 800/1251 ( 64%)]  Loss: 3.670 (3.83)  Time: 0.676s, 1514.91/s  (0.695s, 1472.83/s)  LR: 9.024e-04  Data: 0.011 (0.013)
Train: 122 [ 850/1251 ( 68%)]  Loss: 3.697 (3.82)  Time: 0.674s, 1519.43/s  (0.695s, 1473.25/s)  LR: 9.024e-04  Data: 0.009 (0.013)
Train: 122 [ 900/1251 ( 72%)]  Loss: 4.018 (3.83)  Time: 0.692s, 1478.78/s  (0.695s, 1473.97/s)  LR: 9.024e-04  Data: 0.010 (0.012)
Train: 122 [ 950/1251 ( 76%)]  Loss: 4.147 (3.85)  Time: 0.686s, 1492.03/s  (0.695s, 1474.06/s)  LR: 9.024e-04  Data: 0.010 (0.012)
Train: 122 [1000/1251 ( 80%)]  Loss: 4.002 (3.86)  Time: 0.702s, 1458.02/s  (0.695s, 1474.30/s)  LR: 9.024e-04  Data: 0.009 (0.012)
Train: 122 [1050/1251 ( 84%)]  Loss: 3.936 (3.86)  Time: 0.708s, 1446.61/s  (0.695s, 1474.19/s)  LR: 9.024e-04  Data: 0.009 (0.012)
Train: 122 [1100/1251 ( 88%)]  Loss: 3.881 (3.86)  Time: 0.674s, 1520.14/s  (0.695s, 1474.22/s)  LR: 9.024e-04  Data: 0.013 (0.012)
Train: 122 [1150/1251 ( 92%)]  Loss: 3.881 (3.86)  Time: 0.672s, 1522.79/s  (0.694s, 1474.79/s)  LR: 9.024e-04  Data: 0.013 (0.012)
Train: 122 [1200/1251 ( 96%)]  Loss: 4.298 (3.88)  Time: 0.675s, 1516.49/s  (0.694s, 1475.16/s)  LR: 9.024e-04  Data: 0.010 (0.012)
Train: 122 [1250/1251 (100%)]  Loss: 3.863 (3.88)  Time: 0.687s, 1491.38/s  (0.694s, 1475.32/s)  LR: 9.024e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.460 (1.460)  Loss:  1.0029 (1.0029)  Acc@1: 86.1328 (86.1328)  Acc@5: 95.7031 (95.7031)
Test: [  48/48]  Time: 0.136 (0.582)  Loss:  1.1318 (1.6780)  Acc@1: 81.0142 (69.7180)  Acc@5: 94.3396 (89.5180)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-111.pth.tar', 69.95999996826171)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-120.pth.tar', 69.92200004394532)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-105.pth.tar', 69.8779999951172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-122.pth.tar', 69.71800015625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-102.pth.tar', 69.54600004394531)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-114.pth.tar', 69.48999999267578)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-119.pth.tar', 69.45400010986329)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-104.pth.tar', 69.43399994384765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-116.pth.tar', 69.33600012695312)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-103.pth.tar', 69.31199991943359)

Train: 123 [   0/1251 (  0%)]  Loss: 3.548 (3.55)  Time: 2.263s,  452.53/s  (2.263s,  452.53/s)  LR: 9.008e-04  Data: 1.646 (1.646)
Train: 123 [  50/1251 (  4%)]  Loss: 3.758 (3.65)  Time: 0.672s, 1524.43/s  (0.737s, 1390.04/s)  LR: 9.008e-04  Data: 0.010 (0.047)
Train: 123 [ 100/1251 (  8%)]  Loss: 3.749 (3.69)  Time: 0.703s, 1457.27/s  (0.715s, 1432.22/s)  LR: 9.008e-04  Data: 0.009 (0.029)
Train: 123 [ 150/1251 ( 12%)]  Loss: 3.626 (3.67)  Time: 0.809s, 1265.22/s  (0.707s, 1448.79/s)  LR: 9.008e-04  Data: 0.010 (0.023)
Train: 123 [ 200/1251 ( 16%)]  Loss: 3.959 (3.73)  Time: 0.674s, 1519.40/s  (0.707s, 1448.97/s)  LR: 9.008e-04  Data: 0.010 (0.020)
Train: 123 [ 250/1251 ( 20%)]  Loss: 3.981 (3.77)  Time: 0.699s, 1465.85/s  (0.704s, 1455.11/s)  LR: 9.008e-04  Data: 0.008 (0.018)
Train: 123 [ 300/1251 ( 24%)]  Loss: 3.837 (3.78)  Time: 0.673s, 1522.06/s  (0.701s, 1460.39/s)  LR: 9.008e-04  Data: 0.010 (0.016)
Train: 123 [ 350/1251 ( 28%)]  Loss: 3.971 (3.80)  Time: 0.717s, 1428.69/s  (0.699s, 1464.23/s)  LR: 9.008e-04  Data: 0.009 (0.015)
Train: 123 [ 400/1251 ( 32%)]  Loss: 3.992 (3.82)  Time: 0.680s, 1506.77/s  (0.698s, 1468.04/s)  LR: 9.008e-04  Data: 0.009 (0.015)
Train: 123 [ 450/1251 ( 36%)]  Loss: 3.899 (3.83)  Time: 0.701s, 1459.95/s  (0.698s, 1467.90/s)  LR: 9.008e-04  Data: 0.014 (0.014)
Train: 123 [ 500/1251 ( 40%)]  Loss: 3.915 (3.84)  Time: 0.675s, 1516.03/s  (0.697s, 1470.01/s)  LR: 9.008e-04  Data: 0.010 (0.014)
Train: 123 [ 550/1251 ( 44%)]  Loss: 3.984 (3.85)  Time: 0.675s, 1517.93/s  (0.696s, 1470.43/s)  LR: 9.008e-04  Data: 0.010 (0.013)
Train: 123 [ 600/1251 ( 48%)]  Loss: 3.740 (3.84)  Time: 0.683s, 1498.20/s  (0.696s, 1471.58/s)  LR: 9.008e-04  Data: 0.009 (0.013)
Train: 123 [ 650/1251 ( 52%)]  Loss: 3.760 (3.84)  Time: 0.672s, 1523.28/s  (0.695s, 1472.61/s)  LR: 9.008e-04  Data: 0.010 (0.013)
Train: 123 [ 700/1251 ( 56%)]  Loss: 3.688 (3.83)  Time: 0.705s, 1452.08/s  (0.695s, 1473.74/s)  LR: 9.008e-04  Data: 0.011 (0.013)
Train: 123 [ 750/1251 ( 60%)]  Loss: 3.867 (3.83)  Time: 0.771s, 1328.05/s  (0.695s, 1473.47/s)  LR: 9.008e-04  Data: 0.012 (0.013)
Train: 123 [ 800/1251 ( 64%)]  Loss: 3.991 (3.84)  Time: 0.665s, 1539.47/s  (0.695s, 1473.56/s)  LR: 9.008e-04  Data: 0.011 (0.012)
Train: 123 [ 850/1251 ( 68%)]  Loss: 3.620 (3.83)  Time: 0.707s, 1448.39/s  (0.694s, 1474.90/s)  LR: 9.008e-04  Data: 0.009 (0.012)
Train: 123 [ 900/1251 ( 72%)]  Loss: 3.943 (3.83)  Time: 0.703s, 1456.36/s  (0.694s, 1475.04/s)  LR: 9.008e-04  Data: 0.010 (0.012)
Train: 123 [ 950/1251 ( 76%)]  Loss: 4.212 (3.85)  Time: 0.668s, 1533.01/s  (0.694s, 1475.63/s)  LR: 9.008e-04  Data: 0.012 (0.012)
Train: 123 [1000/1251 ( 80%)]  Loss: 4.042 (3.86)  Time: 0.707s, 1448.50/s  (0.694s, 1475.76/s)  LR: 9.008e-04  Data: 0.014 (0.012)
Train: 123 [1050/1251 ( 84%)]  Loss: 3.863 (3.86)  Time: 0.670s, 1527.56/s  (0.694s, 1475.94/s)  LR: 9.008e-04  Data: 0.009 (0.012)
Train: 123 [1100/1251 ( 88%)]  Loss: 3.755 (3.86)  Time: 0.672s, 1522.93/s  (0.694s, 1475.66/s)  LR: 9.008e-04  Data: 0.009 (0.012)
Train: 123 [1150/1251 ( 92%)]  Loss: 3.936 (3.86)  Time: 0.709s, 1444.29/s  (0.694s, 1475.65/s)  LR: 9.008e-04  Data: 0.009 (0.012)
Train: 123 [1200/1251 ( 96%)]  Loss: 3.847 (3.86)  Time: 0.670s, 1527.39/s  (0.694s, 1476.01/s)  LR: 9.008e-04  Data: 0.010 (0.012)
Train: 123 [1250/1251 (100%)]  Loss: 4.325 (3.88)  Time: 0.659s, 1553.69/s  (0.694s, 1476.51/s)  LR: 9.008e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.498 (1.498)  Loss:  1.0049 (1.0049)  Acc@1: 85.4492 (85.4492)  Acc@5: 95.9961 (95.9961)
Test: [  48/48]  Time: 0.136 (0.618)  Loss:  1.0771 (1.5787)  Acc@1: 82.0755 (69.7020)  Acc@5: 94.3396 (89.5820)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-111.pth.tar', 69.95999996826171)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-120.pth.tar', 69.92200004394532)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-105.pth.tar', 69.8779999951172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-122.pth.tar', 69.71800015625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-123.pth.tar', 69.70200010009765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-102.pth.tar', 69.54600004394531)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-114.pth.tar', 69.48999999267578)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-119.pth.tar', 69.45400010986329)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-104.pth.tar', 69.43399994384765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-116.pth.tar', 69.33600012695312)

Train: 124 [   0/1251 (  0%)]  Loss: 3.364 (3.36)  Time: 2.569s,  398.57/s  (2.569s,  398.57/s)  LR: 8.993e-04  Data: 1.834 (1.834)
Train: 124 [  50/1251 (  4%)]  Loss: 3.403 (3.38)  Time: 0.712s, 1438.04/s  (0.762s, 1343.46/s)  LR: 8.993e-04  Data: 0.013 (0.061)
Train: 124 [ 100/1251 (  8%)]  Loss: 3.952 (3.57)  Time: 0.729s, 1405.47/s  (0.740s, 1383.07/s)  LR: 8.993e-04  Data: 0.010 (0.037)
Train: 124 [ 150/1251 ( 12%)]  Loss: 4.169 (3.72)  Time: 0.673s, 1521.12/s  (0.727s, 1407.69/s)  LR: 8.993e-04  Data: 0.011 (0.028)
Train: 124 [ 200/1251 ( 16%)]  Loss: 3.941 (3.77)  Time: 0.673s, 1520.91/s  (0.718s, 1425.95/s)  LR: 8.993e-04  Data: 0.011 (0.024)
Train: 124 [ 250/1251 ( 20%)]  Loss: 3.717 (3.76)  Time: 0.673s, 1521.11/s  (0.713s, 1435.33/s)  LR: 8.993e-04  Data: 0.010 (0.021)
Train: 124 [ 300/1251 ( 24%)]  Loss: 3.818 (3.77)  Time: 0.675s, 1518.03/s  (0.710s, 1442.16/s)  LR: 8.993e-04  Data: 0.011 (0.019)
Train: 124 [ 350/1251 ( 28%)]  Loss: 4.319 (3.84)  Time: 0.670s, 1527.58/s  (0.707s, 1449.27/s)  LR: 8.993e-04  Data: 0.012 (0.018)
Train: 124 [ 400/1251 ( 32%)]  Loss: 3.890 (3.84)  Time: 0.683s, 1500.37/s  (0.705s, 1452.60/s)  LR: 8.993e-04  Data: 0.010 (0.017)
Train: 124 [ 450/1251 ( 36%)]  Loss: 3.909 (3.85)  Time: 0.696s, 1471.17/s  (0.703s, 1455.76/s)  LR: 8.993e-04  Data: 0.012 (0.016)
Train: 124 [ 500/1251 ( 40%)]  Loss: 3.758 (3.84)  Time: 0.671s, 1525.95/s  (0.702s, 1457.86/s)  LR: 8.993e-04  Data: 0.009 (0.016)
Train: 124 [ 550/1251 ( 44%)]  Loss: 3.867 (3.84)  Time: 0.751s, 1364.35/s  (0.702s, 1457.67/s)  LR: 8.993e-04  Data: 0.009 (0.015)
Train: 124 [ 600/1251 ( 48%)]  Loss: 3.913 (3.85)  Time: 0.707s, 1449.31/s  (0.702s, 1458.40/s)  LR: 8.993e-04  Data: 0.011 (0.015)
Train: 124 [ 650/1251 ( 52%)]  Loss: 4.091 (3.87)  Time: 0.685s, 1495.02/s  (0.702s, 1458.42/s)  LR: 8.993e-04  Data: 0.010 (0.014)
Train: 124 [ 700/1251 ( 56%)]  Loss: 3.934 (3.87)  Time: 0.673s, 1520.87/s  (0.701s, 1459.99/s)  LR: 8.993e-04  Data: 0.011 (0.014)
Train: 124 [ 750/1251 ( 60%)]  Loss: 3.976 (3.88)  Time: 0.669s, 1531.37/s  (0.701s, 1461.29/s)  LR: 8.993e-04  Data: 0.010 (0.014)
Train: 124 [ 800/1251 ( 64%)]  Loss: 3.987 (3.88)  Time: 0.673s, 1520.86/s  (0.700s, 1462.37/s)  LR: 8.993e-04  Data: 0.010 (0.014)
Train: 124 [ 850/1251 ( 68%)]  Loss: 3.713 (3.87)  Time: 0.704s, 1455.32/s  (0.699s, 1464.09/s)  LR: 8.993e-04  Data: 0.009 (0.013)
Train: 124 [ 900/1251 ( 72%)]  Loss: 3.788 (3.87)  Time: 0.696s, 1470.82/s  (0.700s, 1463.66/s)  LR: 8.993e-04  Data: 0.012 (0.013)
Train: 124 [ 950/1251 ( 76%)]  Loss: 3.618 (3.86)  Time: 0.674s, 1520.01/s  (0.699s, 1463.97/s)  LR: 8.993e-04  Data: 0.010 (0.013)
Train: 124 [1000/1251 ( 80%)]  Loss: 4.270 (3.88)  Time: 0.673s, 1520.98/s  (0.699s, 1465.14/s)  LR: 8.993e-04  Data: 0.011 (0.013)
Train: 124 [1050/1251 ( 84%)]  Loss: 4.047 (3.88)  Time: 0.704s, 1455.04/s  (0.699s, 1465.90/s)  LR: 8.993e-04  Data: 0.009 (0.013)
Train: 124 [1100/1251 ( 88%)]  Loss: 3.668 (3.87)  Time: 0.713s, 1435.83/s  (0.698s, 1466.45/s)  LR: 8.993e-04  Data: 0.009 (0.013)
Train: 124 [1150/1251 ( 92%)]  Loss: 3.585 (3.86)  Time: 0.670s, 1527.56/s  (0.698s, 1466.36/s)  LR: 8.993e-04  Data: 0.010 (0.013)
Train: 124 [1200/1251 ( 96%)]  Loss: 3.604 (3.85)  Time: 0.676s, 1515.20/s  (0.698s, 1467.03/s)  LR: 8.993e-04  Data: 0.010 (0.013)
Train: 124 [1250/1251 (100%)]  Loss: 3.545 (3.84)  Time: 0.689s, 1487.11/s  (0.698s, 1467.11/s)  LR: 8.993e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.496 (1.496)  Loss:  1.0859 (1.0859)  Acc@1: 85.1562 (85.1562)  Acc@5: 95.5078 (95.5078)
Test: [  48/48]  Time: 0.136 (0.585)  Loss:  0.9414 (1.5492)  Acc@1: 82.6651 (70.1420)  Acc@5: 95.5189 (89.6460)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-124.pth.tar', 70.14199996826171)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-111.pth.tar', 69.95999996826171)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-120.pth.tar', 69.92200004394532)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-105.pth.tar', 69.8779999951172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-122.pth.tar', 69.71800015625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-123.pth.tar', 69.70200010009765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-102.pth.tar', 69.54600004394531)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-114.pth.tar', 69.48999999267578)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-119.pth.tar', 69.45400010986329)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-104.pth.tar', 69.43399994384765)

Train: 125 [   0/1251 (  0%)]  Loss: 3.801 (3.80)  Time: 2.192s,  467.23/s  (2.192s,  467.23/s)  LR: 8.977e-04  Data: 1.575 (1.575)
Train: 125 [  50/1251 (  4%)]  Loss: 3.855 (3.83)  Time: 0.672s, 1524.48/s  (0.734s, 1395.95/s)  LR: 8.977e-04  Data: 0.010 (0.050)
Train: 125 [ 100/1251 (  8%)]  Loss: 4.002 (3.89)  Time: 0.704s, 1454.85/s  (0.716s, 1429.46/s)  LR: 8.977e-04  Data: 0.009 (0.030)
Train: 125 [ 150/1251 ( 12%)]  Loss: 3.967 (3.91)  Time: 0.709s, 1444.28/s  (0.708s, 1447.34/s)  LR: 8.977e-04  Data: 0.011 (0.024)
Train: 125 [ 200/1251 ( 16%)]  Loss: 3.419 (3.81)  Time: 0.685s, 1494.42/s  (0.704s, 1454.73/s)  LR: 8.977e-04  Data: 0.010 (0.020)
Train: 125 [ 250/1251 ( 20%)]  Loss: 3.695 (3.79)  Time: 0.723s, 1416.26/s  (0.702s, 1459.70/s)  LR: 8.977e-04  Data: 0.010 (0.018)
Train: 125 [ 300/1251 ( 24%)]  Loss: 4.055 (3.83)  Time: 0.715s, 1431.70/s  (0.699s, 1463.96/s)  LR: 8.977e-04  Data: 0.012 (0.017)
Train: 125 [ 350/1251 ( 28%)]  Loss: 4.001 (3.85)  Time: 0.740s, 1383.38/s  (0.697s, 1468.45/s)  LR: 8.977e-04  Data: 0.009 (0.016)
Train: 125 [ 400/1251 ( 32%)]  Loss: 3.791 (3.84)  Time: 0.678s, 1509.29/s  (0.697s, 1469.89/s)  LR: 8.977e-04  Data: 0.011 (0.015)
Train: 125 [ 450/1251 ( 36%)]  Loss: 3.635 (3.82)  Time: 0.701s, 1461.31/s  (0.697s, 1469.35/s)  LR: 8.977e-04  Data: 0.009 (0.015)
Train: 125 [ 500/1251 ( 40%)]  Loss: 3.877 (3.83)  Time: 0.674s, 1519.86/s  (0.696s, 1470.50/s)  LR: 8.977e-04  Data: 0.010 (0.014)
Train: 125 [ 550/1251 ( 44%)]  Loss: 3.835 (3.83)  Time: 0.745s, 1373.87/s  (0.696s, 1470.99/s)  LR: 8.977e-04  Data: 0.009 (0.014)
Train: 125 [ 600/1251 ( 48%)]  Loss: 4.263 (3.86)  Time: 0.671s, 1526.55/s  (0.696s, 1471.38/s)  LR: 8.977e-04  Data: 0.010 (0.014)
Train: 125 [ 650/1251 ( 52%)]  Loss: 3.841 (3.86)  Time: 0.743s, 1378.04/s  (0.696s, 1471.37/s)  LR: 8.977e-04  Data: 0.011 (0.013)
Train: 125 [ 700/1251 ( 56%)]  Loss: 4.137 (3.88)  Time: 0.715s, 1432.86/s  (0.696s, 1470.95/s)  LR: 8.977e-04  Data: 0.014 (0.013)
Train: 125 [ 750/1251 ( 60%)]  Loss: 4.223 (3.90)  Time: 0.673s, 1521.58/s  (0.696s, 1471.79/s)  LR: 8.977e-04  Data: 0.010 (0.013)
Train: 125 [ 800/1251 ( 64%)]  Loss: 3.567 (3.88)  Time: 0.707s, 1448.53/s  (0.696s, 1470.56/s)  LR: 8.977e-04  Data: 0.011 (0.013)
Train: 125 [ 850/1251 ( 68%)]  Loss: 3.997 (3.89)  Time: 0.707s, 1448.26/s  (0.696s, 1470.42/s)  LR: 8.977e-04  Data: 0.011 (0.013)
Train: 125 [ 900/1251 ( 72%)]  Loss: 3.957 (3.89)  Time: 0.795s, 1287.90/s  (0.696s, 1470.61/s)  LR: 8.977e-04  Data: 0.011 (0.013)
Train: 125 [ 950/1251 ( 76%)]  Loss: 3.988 (3.90)  Time: 0.671s, 1527.18/s  (0.696s, 1470.63/s)  LR: 8.977e-04  Data: 0.009 (0.012)
Train: 125 [1000/1251 ( 80%)]  Loss: 4.176 (3.91)  Time: 0.670s, 1528.36/s  (0.696s, 1471.31/s)  LR: 8.977e-04  Data: 0.010 (0.012)
Train: 125 [1050/1251 ( 84%)]  Loss: 4.002 (3.91)  Time: 0.675s, 1517.69/s  (0.696s, 1471.68/s)  LR: 8.977e-04  Data: 0.010 (0.012)
Train: 125 [1100/1251 ( 88%)]  Loss: 4.067 (3.92)  Time: 0.742s, 1379.89/s  (0.696s, 1471.52/s)  LR: 8.977e-04  Data: 0.008 (0.012)
Train: 125 [1150/1251 ( 92%)]  Loss: 3.712 (3.91)  Time: 0.670s, 1528.68/s  (0.696s, 1471.26/s)  LR: 8.977e-04  Data: 0.010 (0.012)
Train: 125 [1200/1251 ( 96%)]  Loss: 3.926 (3.91)  Time: 0.671s, 1525.74/s  (0.696s, 1471.50/s)  LR: 8.977e-04  Data: 0.011 (0.012)
Train: 125 [1250/1251 (100%)]  Loss: 4.194 (3.92)  Time: 0.691s, 1481.30/s  (0.696s, 1471.07/s)  LR: 8.977e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.642 (1.642)  Loss:  0.9922 (0.9922)  Acc@1: 86.8164 (86.8164)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.136 (0.583)  Loss:  1.1152 (1.6069)  Acc@1: 81.2500 (69.5120)  Acc@5: 93.9858 (89.6360)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-124.pth.tar', 70.14199996826171)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-111.pth.tar', 69.95999996826171)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-120.pth.tar', 69.92200004394532)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-105.pth.tar', 69.8779999951172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-122.pth.tar', 69.71800015625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-123.pth.tar', 69.70200010009765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-102.pth.tar', 69.54600004394531)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-125.pth.tar', 69.512)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-114.pth.tar', 69.48999999267578)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-119.pth.tar', 69.45400010986329)

Train: 126 [   0/1251 (  0%)]  Loss: 3.727 (3.73)  Time: 2.142s,  477.97/s  (2.142s,  477.97/s)  LR: 8.961e-04  Data: 1.527 (1.527)
Train: 126 [  50/1251 (  4%)]  Loss: 3.906 (3.82)  Time: 0.678s, 1510.26/s  (0.733s, 1396.57/s)  LR: 8.961e-04  Data: 0.014 (0.046)
Train: 126 [ 100/1251 (  8%)]  Loss: 3.866 (3.83)  Time: 0.678s, 1510.59/s  (0.712s, 1438.69/s)  LR: 8.961e-04  Data: 0.010 (0.028)
Train: 126 [ 150/1251 ( 12%)]  Loss: 3.610 (3.78)  Time: 0.674s, 1520.36/s  (0.705s, 1453.37/s)  LR: 8.961e-04  Data: 0.011 (0.022)
Train: 126 [ 200/1251 ( 16%)]  Loss: 4.048 (3.83)  Time: 0.672s, 1522.72/s  (0.700s, 1462.77/s)  LR: 8.961e-04  Data: 0.010 (0.019)
Train: 126 [ 250/1251 ( 20%)]  Loss: 3.688 (3.81)  Time: 0.711s, 1440.83/s  (0.700s, 1462.14/s)  LR: 8.961e-04  Data: 0.009 (0.017)
Train: 126 [ 300/1251 ( 24%)]  Loss: 3.541 (3.77)  Time: 0.701s, 1460.88/s  (0.699s, 1465.21/s)  LR: 8.961e-04  Data: 0.012 (0.016)
Train: 126 [ 350/1251 ( 28%)]  Loss: 4.046 (3.80)  Time: 0.681s, 1503.95/s  (0.697s, 1468.66/s)  LR: 8.961e-04  Data: 0.011 (0.015)
Train: 126 [ 400/1251 ( 32%)]  Loss: 3.606 (3.78)  Time: 0.672s, 1524.27/s  (0.697s, 1468.72/s)  LR: 8.961e-04  Data: 0.010 (0.015)
Train: 126 [ 450/1251 ( 36%)]  Loss: 3.958 (3.80)  Time: 0.687s, 1491.47/s  (0.697s, 1469.32/s)  LR: 8.961e-04  Data: 0.010 (0.014)
Train: 126 [ 500/1251 ( 40%)]  Loss: 3.810 (3.80)  Time: 0.781s, 1311.92/s  (0.697s, 1468.17/s)  LR: 8.961e-04  Data: 0.010 (0.014)
Train: 126 [ 550/1251 ( 44%)]  Loss: 4.276 (3.84)  Time: 0.691s, 1482.41/s  (0.698s, 1466.60/s)  LR: 8.961e-04  Data: 0.009 (0.014)
Train: 126 [ 600/1251 ( 48%)]  Loss: 3.630 (3.82)  Time: 0.713s, 1436.37/s  (0.698s, 1467.81/s)  LR: 8.961e-04  Data: 0.010 (0.013)
Train: 126 [ 650/1251 ( 52%)]  Loss: 3.749 (3.82)  Time: 0.709s, 1444.75/s  (0.697s, 1468.86/s)  LR: 8.961e-04  Data: 0.010 (0.013)
Train: 126 [ 700/1251 ( 56%)]  Loss: 3.999 (3.83)  Time: 0.702s, 1459.53/s  (0.697s, 1468.73/s)  LR: 8.961e-04  Data: 0.009 (0.013)
Train: 126 [ 750/1251 ( 60%)]  Loss: 3.325 (3.80)  Time: 0.710s, 1442.07/s  (0.697s, 1469.25/s)  LR: 8.961e-04  Data: 0.010 (0.013)
Train: 126 [ 800/1251 ( 64%)]  Loss: 3.861 (3.80)  Time: 0.676s, 1514.85/s  (0.696s, 1470.37/s)  LR: 8.961e-04  Data: 0.011 (0.012)
Train: 126 [ 850/1251 ( 68%)]  Loss: 3.660 (3.79)  Time: 0.724s, 1414.75/s  (0.697s, 1469.95/s)  LR: 8.961e-04  Data: 0.010 (0.012)
Train: 126 [ 900/1251 ( 72%)]  Loss: 3.850 (3.80)  Time: 0.673s, 1521.17/s  (0.696s, 1470.29/s)  LR: 8.961e-04  Data: 0.014 (0.012)
Train: 126 [ 950/1251 ( 76%)]  Loss: 3.764 (3.80)  Time: 0.680s, 1506.30/s  (0.696s, 1470.42/s)  LR: 8.961e-04  Data: 0.010 (0.012)
Train: 126 [1000/1251 ( 80%)]  Loss: 3.966 (3.80)  Time: 0.706s, 1450.94/s  (0.697s, 1469.62/s)  LR: 8.961e-04  Data: 0.010 (0.012)
Train: 126 [1050/1251 ( 84%)]  Loss: 3.936 (3.81)  Time: 0.724s, 1414.06/s  (0.697s, 1469.85/s)  LR: 8.961e-04  Data: 0.010 (0.012)
Train: 126 [1100/1251 ( 88%)]  Loss: 3.647 (3.80)  Time: 0.717s, 1428.94/s  (0.697s, 1469.18/s)  LR: 8.961e-04  Data: 0.010 (0.012)
Train: 126 [1150/1251 ( 92%)]  Loss: 3.939 (3.81)  Time: 0.705s, 1453.48/s  (0.697s, 1470.12/s)  LR: 8.961e-04  Data: 0.010 (0.012)
Train: 126 [1200/1251 ( 96%)]  Loss: 3.571 (3.80)  Time: 0.682s, 1501.96/s  (0.696s, 1470.71/s)  LR: 8.961e-04  Data: 0.010 (0.012)
Train: 126 [1250/1251 (100%)]  Loss: 3.867 (3.80)  Time: 0.696s, 1472.01/s  (0.696s, 1471.03/s)  LR: 8.961e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.627 (1.627)  Loss:  1.1484 (1.1484)  Acc@1: 83.7891 (83.7891)  Acc@5: 95.5078 (95.5078)
Test: [  48/48]  Time: 0.136 (0.588)  Loss:  1.1914 (1.7119)  Acc@1: 81.8396 (69.3920)  Acc@5: 95.9906 (89.4640)
Train: 127 [   0/1251 (  0%)]  Loss: 3.821 (3.82)  Time: 2.232s,  458.81/s  (2.232s,  458.81/s)  LR: 8.945e-04  Data: 1.589 (1.589)
Train: 127 [  50/1251 (  4%)]  Loss: 3.983 (3.90)  Time: 0.671s, 1524.96/s  (0.735s, 1394.06/s)  LR: 8.945e-04  Data: 0.010 (0.054)
Train: 127 [ 100/1251 (  8%)]  Loss: 3.874 (3.89)  Time: 0.682s, 1502.44/s  (0.714s, 1434.43/s)  LR: 8.945e-04  Data: 0.009 (0.032)
Train: 127 [ 150/1251 ( 12%)]  Loss: 3.921 (3.90)  Time: 0.704s, 1454.41/s  (0.706s, 1449.60/s)  LR: 8.945e-04  Data: 0.009 (0.025)
Train: 127 [ 200/1251 ( 16%)]  Loss: 3.854 (3.89)  Time: 0.711s, 1440.97/s  (0.703s, 1457.10/s)  LR: 8.945e-04  Data: 0.010 (0.021)
Train: 127 [ 250/1251 ( 20%)]  Loss: 3.745 (3.87)  Time: 0.710s, 1441.82/s  (0.701s, 1461.36/s)  LR: 8.945e-04  Data: 0.010 (0.019)
Train: 127 [ 300/1251 ( 24%)]  Loss: 4.265 (3.92)  Time: 0.677s, 1511.57/s  (0.699s, 1464.01/s)  LR: 8.945e-04  Data: 0.010 (0.018)
Train: 127 [ 350/1251 ( 28%)]  Loss: 3.784 (3.91)  Time: 0.700s, 1463.17/s  (0.700s, 1461.89/s)  LR: 8.945e-04  Data: 0.009 (0.017)
Train: 127 [ 400/1251 ( 32%)]  Loss: 4.299 (3.95)  Time: 0.677s, 1513.40/s  (0.699s, 1464.20/s)  LR: 8.945e-04  Data: 0.014 (0.016)
Train: 127 [ 450/1251 ( 36%)]  Loss: 4.060 (3.96)  Time: 0.704s, 1453.83/s  (0.699s, 1465.51/s)  LR: 8.945e-04  Data: 0.009 (0.015)
Train: 127 [ 500/1251 ( 40%)]  Loss: 4.064 (3.97)  Time: 0.669s, 1530.08/s  (0.698s, 1467.64/s)  LR: 8.945e-04  Data: 0.009 (0.015)
Train: 127 [ 550/1251 ( 44%)]  Loss: 3.741 (3.95)  Time: 0.719s, 1423.46/s  (0.697s, 1468.24/s)  LR: 8.945e-04  Data: 0.009 (0.014)
Train: 127 [ 600/1251 ( 48%)]  Loss: 3.870 (3.94)  Time: 0.676s, 1515.66/s  (0.697s, 1469.48/s)  LR: 8.945e-04  Data: 0.009 (0.014)
Train: 127 [ 650/1251 ( 52%)]  Loss: 3.723 (3.93)  Time: 0.670s, 1527.46/s  (0.696s, 1471.15/s)  LR: 8.945e-04  Data: 0.010 (0.014)
Train: 127 [ 700/1251 ( 56%)]  Loss: 3.737 (3.92)  Time: 0.741s, 1381.55/s  (0.696s, 1471.58/s)  LR: 8.945e-04  Data: 0.010 (0.013)
Train: 127 [ 750/1251 ( 60%)]  Loss: 3.957 (3.92)  Time: 0.706s, 1450.85/s  (0.696s, 1471.66/s)  LR: 8.945e-04  Data: 0.010 (0.013)
Train: 127 [ 800/1251 ( 64%)]  Loss: 4.083 (3.93)  Time: 0.671s, 1526.31/s  (0.695s, 1472.53/s)  LR: 8.945e-04  Data: 0.012 (0.013)
Train: 127 [ 850/1251 ( 68%)]  Loss: 4.087 (3.94)  Time: 0.696s, 1471.99/s  (0.695s, 1473.75/s)  LR: 8.945e-04  Data: 0.010 (0.013)
Train: 127 [ 900/1251 ( 72%)]  Loss: 4.067 (3.94)  Time: 0.673s, 1522.46/s  (0.695s, 1474.39/s)  LR: 8.945e-04  Data: 0.010 (0.013)
Train: 127 [ 950/1251 ( 76%)]  Loss: 3.672 (3.93)  Time: 0.674s, 1519.05/s  (0.695s, 1473.34/s)  LR: 8.945e-04  Data: 0.010 (0.013)
Train: 127 [1000/1251 ( 80%)]  Loss: 3.569 (3.91)  Time: 0.673s, 1521.14/s  (0.695s, 1473.47/s)  LR: 8.945e-04  Data: 0.009 (0.012)
Train: 127 [1050/1251 ( 84%)]  Loss: 3.968 (3.92)  Time: 0.677s, 1511.92/s  (0.695s, 1474.06/s)  LR: 8.945e-04  Data: 0.010 (0.012)
Train: 127 [1100/1251 ( 88%)]  Loss: 3.611 (3.90)  Time: 0.674s, 1520.25/s  (0.694s, 1474.55/s)  LR: 8.945e-04  Data: 0.010 (0.012)
Train: 127 [1150/1251 ( 92%)]  Loss: 3.856 (3.90)  Time: 0.706s, 1451.31/s  (0.694s, 1475.18/s)  LR: 8.945e-04  Data: 0.009 (0.012)
Train: 127 [1200/1251 ( 96%)]  Loss: 3.931 (3.90)  Time: 0.673s, 1520.79/s  (0.694s, 1475.34/s)  LR: 8.945e-04  Data: 0.010 (0.012)
Train: 127 [1250/1251 (100%)]  Loss: 3.555 (3.89)  Time: 0.694s, 1475.25/s  (0.694s, 1474.92/s)  LR: 8.945e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.468 (1.468)  Loss:  1.1289 (1.1289)  Acc@1: 85.7422 (85.7422)  Acc@5: 95.9961 (95.9961)
Test: [  48/48]  Time: 0.136 (0.588)  Loss:  1.2012 (1.6472)  Acc@1: 80.4245 (69.6420)  Acc@5: 94.5755 (89.6620)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-124.pth.tar', 70.14199996826171)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-111.pth.tar', 69.95999996826171)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-120.pth.tar', 69.92200004394532)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-105.pth.tar', 69.8779999951172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-122.pth.tar', 69.71800015625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-123.pth.tar', 69.70200010009765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-127.pth.tar', 69.64200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-102.pth.tar', 69.54600004394531)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-125.pth.tar', 69.512)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-114.pth.tar', 69.48999999267578)

Train: 128 [   0/1251 (  0%)]  Loss: 4.274 (4.27)  Time: 2.150s,  476.22/s  (2.150s,  476.22/s)  LR: 8.929e-04  Data: 1.534 (1.534)
Train: 128 [  50/1251 (  4%)]  Loss: 3.770 (4.02)  Time: 0.713s, 1436.58/s  (0.723s, 1415.71/s)  LR: 8.929e-04  Data: 0.010 (0.045)
Train: 128 [ 100/1251 (  8%)]  Loss: 3.654 (3.90)  Time: 0.674s, 1519.08/s  (0.710s, 1443.24/s)  LR: 8.929e-04  Data: 0.015 (0.028)
Train: 128 [ 150/1251 ( 12%)]  Loss: 3.411 (3.78)  Time: 0.666s, 1537.42/s  (0.704s, 1454.31/s)  LR: 8.929e-04  Data: 0.010 (0.022)
Train: 128 [ 200/1251 ( 16%)]  Loss: 3.781 (3.78)  Time: 0.673s, 1522.64/s  (0.702s, 1458.20/s)  LR: 8.929e-04  Data: 0.010 (0.019)
Train: 128 [ 250/1251 ( 20%)]  Loss: 3.767 (3.78)  Time: 0.672s, 1523.02/s  (0.702s, 1458.76/s)  LR: 8.929e-04  Data: 0.010 (0.017)
Train: 128 [ 300/1251 ( 24%)]  Loss: 4.181 (3.83)  Time: 0.673s, 1522.41/s  (0.701s, 1461.21/s)  LR: 8.929e-04  Data: 0.011 (0.016)
Train: 128 [ 350/1251 ( 28%)]  Loss: 3.837 (3.83)  Time: 0.705s, 1451.77/s  (0.699s, 1464.73/s)  LR: 8.929e-04  Data: 0.010 (0.015)
Train: 128 [ 400/1251 ( 32%)]  Loss: 4.220 (3.88)  Time: 0.743s, 1377.53/s  (0.699s, 1465.69/s)  LR: 8.929e-04  Data: 0.017 (0.015)
Train: 128 [ 450/1251 ( 36%)]  Loss: 3.853 (3.87)  Time: 0.692s, 1478.77/s  (0.698s, 1466.84/s)  LR: 8.929e-04  Data: 0.010 (0.014)
Train: 128 [ 500/1251 ( 40%)]  Loss: 3.843 (3.87)  Time: 0.671s, 1525.54/s  (0.698s, 1466.94/s)  LR: 8.929e-04  Data: 0.009 (0.014)
Train: 128 [ 550/1251 ( 44%)]  Loss: 4.104 (3.89)  Time: 0.671s, 1526.97/s  (0.697s, 1469.12/s)  LR: 8.929e-04  Data: 0.010 (0.014)
Train: 128 [ 600/1251 ( 48%)]  Loss: 3.713 (3.88)  Time: 0.691s, 1481.44/s  (0.697s, 1468.51/s)  LR: 8.929e-04  Data: 0.009 (0.013)
Train: 128 [ 650/1251 ( 52%)]  Loss: 4.065 (3.89)  Time: 0.672s, 1522.68/s  (0.698s, 1468.10/s)  LR: 8.929e-04  Data: 0.012 (0.013)
Train: 128 [ 700/1251 ( 56%)]  Loss: 4.102 (3.90)  Time: 0.705s, 1453.06/s  (0.697s, 1469.18/s)  LR: 8.929e-04  Data: 0.009 (0.013)
Train: 128 [ 750/1251 ( 60%)]  Loss: 4.111 (3.92)  Time: 0.712s, 1438.31/s  (0.697s, 1468.85/s)  LR: 8.929e-04  Data: 0.010 (0.013)
Train: 128 [ 800/1251 ( 64%)]  Loss: 3.852 (3.91)  Time: 0.696s, 1470.65/s  (0.697s, 1469.39/s)  LR: 8.929e-04  Data: 0.009 (0.013)
Train: 128 [ 850/1251 ( 68%)]  Loss: 3.648 (3.90)  Time: 0.705s, 1452.63/s  (0.697s, 1468.62/s)  LR: 8.929e-04  Data: 0.010 (0.012)
Train: 128 [ 900/1251 ( 72%)]  Loss: 3.714 (3.89)  Time: 0.668s, 1534.05/s  (0.697s, 1469.70/s)  LR: 8.929e-04  Data: 0.009 (0.012)
Train: 128 [ 950/1251 ( 76%)]  Loss: 3.909 (3.89)  Time: 0.727s, 1408.31/s  (0.697s, 1469.78/s)  LR: 8.929e-04  Data: 0.009 (0.012)
Train: 128 [1000/1251 ( 80%)]  Loss: 3.898 (3.89)  Time: 0.682s, 1502.45/s  (0.697s, 1469.58/s)  LR: 8.929e-04  Data: 0.010 (0.012)
Train: 128 [1050/1251 ( 84%)]  Loss: 4.007 (3.90)  Time: 0.671s, 1526.48/s  (0.697s, 1470.16/s)  LR: 8.929e-04  Data: 0.010 (0.012)
Train: 128 [1100/1251 ( 88%)]  Loss: 3.951 (3.90)  Time: 0.691s, 1480.99/s  (0.696s, 1470.34/s)  LR: 8.929e-04  Data: 0.010 (0.012)
Train: 128 [1150/1251 ( 92%)]  Loss: 3.713 (3.89)  Time: 0.730s, 1402.59/s  (0.696s, 1470.74/s)  LR: 8.929e-04  Data: 0.009 (0.012)
Train: 128 [1200/1251 ( 96%)]  Loss: 3.668 (3.88)  Time: 0.711s, 1439.92/s  (0.696s, 1470.47/s)  LR: 8.929e-04  Data: 0.011 (0.012)
Train: 128 [1250/1251 (100%)]  Loss: 4.042 (3.89)  Time: 0.657s, 1557.64/s  (0.696s, 1471.30/s)  LR: 8.929e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.630 (1.630)  Loss:  1.0732 (1.0732)  Acc@1: 86.8164 (86.8164)  Acc@5: 96.1914 (96.1914)
Test: [  48/48]  Time: 0.136 (0.589)  Loss:  1.0293 (1.6227)  Acc@1: 83.7264 (69.4840)  Acc@5: 94.9292 (89.4680)
Train: 129 [   0/1251 (  0%)]  Loss: 4.087 (4.09)  Time: 2.233s,  458.50/s  (2.233s,  458.50/s)  LR: 8.913e-04  Data: 1.618 (1.618)
Train: 129 [  50/1251 (  4%)]  Loss: 4.037 (4.06)  Time: 0.743s, 1377.90/s  (0.735s, 1393.49/s)  LR: 8.913e-04  Data: 0.011 (0.048)
Train: 129 [ 100/1251 (  8%)]  Loss: 3.982 (4.04)  Time: 0.672s, 1524.33/s  (0.719s, 1424.66/s)  LR: 8.913e-04  Data: 0.010 (0.030)
Train: 129 [ 150/1251 ( 12%)]  Loss: 3.947 (4.01)  Time: 0.717s, 1427.44/s  (0.711s, 1440.16/s)  LR: 8.913e-04  Data: 0.010 (0.023)
Train: 129 [ 200/1251 ( 16%)]  Loss: 4.002 (4.01)  Time: 0.719s, 1424.85/s  (0.707s, 1447.62/s)  LR: 8.913e-04  Data: 0.009 (0.020)
Train: 129 [ 250/1251 ( 20%)]  Loss: 3.837 (3.98)  Time: 0.665s, 1539.12/s  (0.707s, 1448.72/s)  LR: 8.913e-04  Data: 0.009 (0.018)
Train: 129 [ 300/1251 ( 24%)]  Loss: 4.179 (4.01)  Time: 0.672s, 1523.15/s  (0.704s, 1454.10/s)  LR: 8.913e-04  Data: 0.010 (0.017)
Train: 129 [ 350/1251 ( 28%)]  Loss: 3.836 (3.99)  Time: 0.672s, 1524.72/s  (0.702s, 1458.45/s)  LR: 8.913e-04  Data: 0.010 (0.016)
Train: 129 [ 400/1251 ( 32%)]  Loss: 3.606 (3.95)  Time: 0.667s, 1536.08/s  (0.701s, 1460.01/s)  LR: 8.913e-04  Data: 0.010 (0.015)
Train: 129 [ 450/1251 ( 36%)]  Loss: 4.070 (3.96)  Time: 0.715s, 1432.42/s  (0.700s, 1462.07/s)  LR: 8.913e-04  Data: 0.013 (0.015)
Train: 129 [ 500/1251 ( 40%)]  Loss: 3.819 (3.95)  Time: 0.737s, 1389.31/s  (0.700s, 1462.23/s)  LR: 8.913e-04  Data: 0.009 (0.014)
Train: 129 [ 550/1251 ( 44%)]  Loss: 4.247 (3.97)  Time: 0.671s, 1526.23/s  (0.700s, 1463.30/s)  LR: 8.913e-04  Data: 0.011 (0.014)
Train: 129 [ 600/1251 ( 48%)]  Loss: 3.793 (3.96)  Time: 0.708s, 1446.09/s  (0.699s, 1464.11/s)  LR: 8.913e-04  Data: 0.010 (0.014)
Train: 129 [ 650/1251 ( 52%)]  Loss: 4.000 (3.96)  Time: 0.682s, 1502.49/s  (0.699s, 1465.09/s)  LR: 8.913e-04  Data: 0.011 (0.013)
Train: 129 [ 700/1251 ( 56%)]  Loss: 4.140 (3.97)  Time: 0.677s, 1512.73/s  (0.699s, 1465.19/s)  LR: 8.913e-04  Data: 0.010 (0.013)
Train: 129 [ 750/1251 ( 60%)]  Loss: 4.161 (3.98)  Time: 0.672s, 1522.86/s  (0.699s, 1465.66/s)  LR: 8.913e-04  Data: 0.010 (0.013)
Train: 129 [ 800/1251 ( 64%)]  Loss: 3.963 (3.98)  Time: 0.670s, 1528.84/s  (0.699s, 1465.86/s)  LR: 8.913e-04  Data: 0.010 (0.013)
Train: 129 [ 850/1251 ( 68%)]  Loss: 3.981 (3.98)  Time: 0.674s, 1520.17/s  (0.698s, 1466.87/s)  LR: 8.913e-04  Data: 0.010 (0.013)
Train: 129 [ 900/1251 ( 72%)]  Loss: 3.758 (3.97)  Time: 0.676s, 1515.53/s  (0.698s, 1466.98/s)  LR: 8.913e-04  Data: 0.009 (0.013)
Train: 129 [ 950/1251 ( 76%)]  Loss: 3.787 (3.96)  Time: 0.705s, 1452.57/s  (0.698s, 1466.67/s)  LR: 8.913e-04  Data: 0.010 (0.012)
Train: 129 [1000/1251 ( 80%)]  Loss: 3.978 (3.96)  Time: 0.670s, 1529.14/s  (0.698s, 1467.63/s)  LR: 8.913e-04  Data: 0.010 (0.012)
Train: 129 [1050/1251 ( 84%)]  Loss: 3.587 (3.95)  Time: 0.669s, 1531.30/s  (0.697s, 1468.75/s)  LR: 8.913e-04  Data: 0.011 (0.012)
Train: 129 [1100/1251 ( 88%)]  Loss: 4.081 (3.95)  Time: 0.672s, 1524.64/s  (0.697s, 1469.48/s)  LR: 8.913e-04  Data: 0.010 (0.012)
Train: 129 [1150/1251 ( 92%)]  Loss: 3.915 (3.95)  Time: 0.704s, 1454.31/s  (0.697s, 1468.99/s)  LR: 8.913e-04  Data: 0.010 (0.012)
Train: 129 [1200/1251 ( 96%)]  Loss: 3.787 (3.94)  Time: 0.672s, 1524.62/s  (0.697s, 1469.68/s)  LR: 8.913e-04  Data: 0.011 (0.012)
Train: 129 [1250/1251 (100%)]  Loss: 3.712 (3.93)  Time: 0.657s, 1559.63/s  (0.697s, 1469.81/s)  LR: 8.913e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.488 (1.488)  Loss:  0.7900 (0.7900)  Acc@1: 87.4023 (87.4023)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.137 (0.590)  Loss:  0.8896 (1.5026)  Acc@1: 82.6651 (69.7220)  Acc@5: 95.4009 (89.4680)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-124.pth.tar', 70.14199996826171)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-111.pth.tar', 69.95999996826171)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-120.pth.tar', 69.92200004394532)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-105.pth.tar', 69.8779999951172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-129.pth.tar', 69.72199996826171)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-122.pth.tar', 69.71800015625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-123.pth.tar', 69.70200010009765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-127.pth.tar', 69.64200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-102.pth.tar', 69.54600004394531)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-125.pth.tar', 69.512)

Train: 130 [   0/1251 (  0%)]  Loss: 3.886 (3.89)  Time: 2.328s,  439.82/s  (2.328s,  439.82/s)  LR: 8.897e-04  Data: 1.665 (1.665)
Train: 130 [  50/1251 (  4%)]  Loss: 3.560 (3.72)  Time: 0.678s, 1511.25/s  (0.740s, 1383.68/s)  LR: 8.897e-04  Data: 0.010 (0.053)
Train: 130 [ 100/1251 (  8%)]  Loss: 3.765 (3.74)  Time: 0.713s, 1436.82/s  (0.715s, 1431.45/s)  LR: 8.897e-04  Data: 0.009 (0.032)
Train: 130 [ 150/1251 ( 12%)]  Loss: 4.039 (3.81)  Time: 0.679s, 1508.95/s  (0.707s, 1447.50/s)  LR: 8.897e-04  Data: 0.010 (0.024)
Train: 130 [ 200/1251 ( 16%)]  Loss: 3.702 (3.79)  Time: 0.716s, 1431.16/s  (0.702s, 1458.48/s)  LR: 8.897e-04  Data: 0.010 (0.021)
Train: 130 [ 250/1251 ( 20%)]  Loss: 3.777 (3.79)  Time: 0.693s, 1477.13/s  (0.703s, 1457.23/s)  LR: 8.897e-04  Data: 0.010 (0.019)
Train: 130 [ 300/1251 ( 24%)]  Loss: 3.627 (3.77)  Time: 0.704s, 1455.15/s  (0.701s, 1461.20/s)  LR: 8.897e-04  Data: 0.009 (0.017)
Train: 130 [ 350/1251 ( 28%)]  Loss: 3.770 (3.77)  Time: 0.673s, 1521.54/s  (0.699s, 1465.80/s)  LR: 8.897e-04  Data: 0.010 (0.016)
Train: 130 [ 400/1251 ( 32%)]  Loss: 3.554 (3.74)  Time: 0.720s, 1422.17/s  (0.698s, 1466.98/s)  LR: 8.897e-04  Data: 0.011 (0.016)
Train: 130 [ 450/1251 ( 36%)]  Loss: 3.876 (3.76)  Time: 0.670s, 1528.03/s  (0.698s, 1468.04/s)  LR: 8.897e-04  Data: 0.010 (0.015)
Train: 130 [ 500/1251 ( 40%)]  Loss: 3.736 (3.75)  Time: 0.709s, 1444.17/s  (0.697s, 1469.67/s)  LR: 8.897e-04  Data: 0.010 (0.015)
Train: 130 [ 550/1251 ( 44%)]  Loss: 4.046 (3.78)  Time: 0.705s, 1452.95/s  (0.696s, 1470.76/s)  LR: 8.897e-04  Data: 0.009 (0.014)
Train: 130 [ 600/1251 ( 48%)]  Loss: 4.145 (3.81)  Time: 0.674s, 1518.47/s  (0.697s, 1469.86/s)  LR: 8.897e-04  Data: 0.010 (0.014)
Train: 130 [ 650/1251 ( 52%)]  Loss: 3.857 (3.81)  Time: 0.663s, 1543.96/s  (0.696s, 1470.83/s)  LR: 8.897e-04  Data: 0.008 (0.014)
Train: 130 [ 700/1251 ( 56%)]  Loss: 3.921 (3.82)  Time: 0.700s, 1463.28/s  (0.696s, 1471.85/s)  LR: 8.897e-04  Data: 0.009 (0.013)
Train: 130 [ 750/1251 ( 60%)]  Loss: 3.901 (3.82)  Time: 0.705s, 1452.93/s  (0.695s, 1472.72/s)  LR: 8.897e-04  Data: 0.009 (0.013)
Train: 130 [ 800/1251 ( 64%)]  Loss: 4.055 (3.84)  Time: 0.696s, 1470.46/s  (0.695s, 1473.59/s)  LR: 8.897e-04  Data: 0.009 (0.013)
Train: 130 [ 850/1251 ( 68%)]  Loss: 3.874 (3.84)  Time: 0.692s, 1479.03/s  (0.694s, 1474.81/s)  LR: 8.897e-04  Data: 0.010 (0.013)
Train: 130 [ 900/1251 ( 72%)]  Loss: 3.712 (3.83)  Time: 0.675s, 1516.24/s  (0.694s, 1474.50/s)  LR: 8.897e-04  Data: 0.011 (0.013)
Train: 130 [ 950/1251 ( 76%)]  Loss: 3.816 (3.83)  Time: 0.707s, 1449.28/s  (0.695s, 1473.86/s)  LR: 8.897e-04  Data: 0.009 (0.012)
Train: 130 [1000/1251 ( 80%)]  Loss: 3.907 (3.83)  Time: 0.702s, 1457.84/s  (0.695s, 1472.98/s)  LR: 8.897e-04  Data: 0.009 (0.012)
Train: 130 [1050/1251 ( 84%)]  Loss: 4.123 (3.85)  Time: 0.663s, 1544.10/s  (0.695s, 1473.04/s)  LR: 8.897e-04  Data: 0.008 (0.012)
Train: 130 [1100/1251 ( 88%)]  Loss: 3.284 (3.82)  Time: 0.675s, 1516.41/s  (0.695s, 1473.66/s)  LR: 8.897e-04  Data: 0.011 (0.012)
Train: 130 [1150/1251 ( 92%)]  Loss: 4.211 (3.84)  Time: 0.704s, 1454.15/s  (0.695s, 1474.40/s)  LR: 8.897e-04  Data: 0.010 (0.012)
Train: 130 [1200/1251 ( 96%)]  Loss: 3.967 (3.84)  Time: 0.669s, 1531.17/s  (0.694s, 1474.90/s)  LR: 8.897e-04  Data: 0.009 (0.012)
Train: 130 [1250/1251 (100%)]  Loss: 3.869 (3.85)  Time: 0.699s, 1465.50/s  (0.694s, 1474.80/s)  LR: 8.897e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.542 (1.542)  Loss:  1.1289 (1.1289)  Acc@1: 85.2539 (85.2539)  Acc@5: 95.3125 (95.3125)
Test: [  48/48]  Time: 0.136 (0.592)  Loss:  1.1777 (1.6531)  Acc@1: 82.0755 (69.9760)  Acc@5: 94.5755 (89.8200)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-124.pth.tar', 70.14199996826171)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-130.pth.tar', 69.97599997070313)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-111.pth.tar', 69.95999996826171)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-120.pth.tar', 69.92200004394532)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-105.pth.tar', 69.8779999951172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-129.pth.tar', 69.72199996826171)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-122.pth.tar', 69.71800015625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-123.pth.tar', 69.70200010009765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-127.pth.tar', 69.64200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-102.pth.tar', 69.54600004394531)

Train: 131 [   0/1251 (  0%)]  Loss: 3.821 (3.82)  Time: 2.241s,  456.87/s  (2.241s,  456.87/s)  LR: 8.881e-04  Data: 1.626 (1.626)
Train: 131 [  50/1251 (  4%)]  Loss: 3.875 (3.85)  Time: 0.720s, 1421.63/s  (0.730s, 1402.74/s)  LR: 8.881e-04  Data: 0.009 (0.051)
Train: 131 [ 100/1251 (  8%)]  Loss: 4.151 (3.95)  Time: 0.671s, 1526.31/s  (0.715s, 1432.44/s)  LR: 8.881e-04  Data: 0.010 (0.031)
Train: 131 [ 150/1251 ( 12%)]  Loss: 3.897 (3.94)  Time: 0.690s, 1484.89/s  (0.706s, 1449.55/s)  LR: 8.881e-04  Data: 0.009 (0.024)
Train: 131 [ 200/1251 ( 16%)]  Loss: 3.555 (3.86)  Time: 0.717s, 1427.51/s  (0.702s, 1459.62/s)  LR: 8.881e-04  Data: 0.009 (0.021)
Train: 131 [ 250/1251 ( 20%)]  Loss: 3.681 (3.83)  Time: 0.705s, 1452.97/s  (0.700s, 1462.39/s)  LR: 8.881e-04  Data: 0.010 (0.019)
Train: 131 [ 300/1251 ( 24%)]  Loss: 3.872 (3.84)  Time: 0.713s, 1436.57/s  (0.700s, 1463.27/s)  LR: 8.881e-04  Data: 0.010 (0.017)
Train: 131 [ 350/1251 ( 28%)]  Loss: 3.670 (3.82)  Time: 0.719s, 1423.65/s  (0.698s, 1466.71/s)  LR: 8.881e-04  Data: 0.011 (0.016)
Train: 131 [ 400/1251 ( 32%)]  Loss: 3.769 (3.81)  Time: 0.672s, 1524.80/s  (0.697s, 1468.12/s)  LR: 8.881e-04  Data: 0.010 (0.015)
Train: 131 [ 450/1251 ( 36%)]  Loss: 3.738 (3.80)  Time: 0.679s, 1507.05/s  (0.698s, 1467.57/s)  LR: 8.881e-04  Data: 0.012 (0.015)
Train: 131 [ 500/1251 ( 40%)]  Loss: 3.657 (3.79)  Time: 0.704s, 1455.24/s  (0.697s, 1468.58/s)  LR: 8.881e-04  Data: 0.009 (0.014)
Train: 131 [ 550/1251 ( 44%)]  Loss: 3.887 (3.80)  Time: 0.712s, 1438.21/s  (0.697s, 1468.25/s)  LR: 8.881e-04  Data: 0.010 (0.014)
Train: 131 [ 600/1251 ( 48%)]  Loss: 3.511 (3.78)  Time: 0.673s, 1522.40/s  (0.697s, 1469.22/s)  LR: 8.881e-04  Data: 0.011 (0.014)
Train: 131 [ 650/1251 ( 52%)]  Loss: 4.158 (3.80)  Time: 0.665s, 1539.03/s  (0.696s, 1470.77/s)  LR: 8.881e-04  Data: 0.010 (0.013)
Train: 131 [ 700/1251 ( 56%)]  Loss: 3.449 (3.78)  Time: 0.701s, 1461.55/s  (0.696s, 1471.94/s)  LR: 8.881e-04  Data: 0.010 (0.013)
Train: 131 [ 750/1251 ( 60%)]  Loss: 3.554 (3.77)  Time: 0.697s, 1468.47/s  (0.696s, 1471.47/s)  LR: 8.881e-04  Data: 0.010 (0.013)
Train: 131 [ 800/1251 ( 64%)]  Loss: 3.795 (3.77)  Time: 0.673s, 1521.93/s  (0.696s, 1472.23/s)  LR: 8.881e-04  Data: 0.010 (0.013)
Train: 131 [ 850/1251 ( 68%)]  Loss: 4.173 (3.79)  Time: 0.705s, 1453.44/s  (0.696s, 1471.43/s)  LR: 8.881e-04  Data: 0.009 (0.013)
Train: 131 [ 900/1251 ( 72%)]  Loss: 4.165 (3.81)  Time: 0.672s, 1523.21/s  (0.696s, 1472.16/s)  LR: 8.881e-04  Data: 0.010 (0.013)
Train: 131 [ 950/1251 ( 76%)]  Loss: 3.807 (3.81)  Time: 0.695s, 1472.79/s  (0.696s, 1471.89/s)  LR: 8.881e-04  Data: 0.012 (0.012)
Train: 131 [1000/1251 ( 80%)]  Loss: 3.994 (3.82)  Time: 0.674s, 1519.74/s  (0.696s, 1472.20/s)  LR: 8.881e-04  Data: 0.009 (0.012)
Train: 131 [1050/1251 ( 84%)]  Loss: 4.164 (3.83)  Time: 0.671s, 1526.25/s  (0.695s, 1472.41/s)  LR: 8.881e-04  Data: 0.009 (0.012)
Train: 131 [1100/1251 ( 88%)]  Loss: 3.978 (3.84)  Time: 0.747s, 1370.09/s  (0.696s, 1471.56/s)  LR: 8.881e-04  Data: 0.009 (0.012)
Train: 131 [1150/1251 ( 92%)]  Loss: 4.030 (3.85)  Time: 0.674s, 1519.32/s  (0.696s, 1472.06/s)  LR: 8.881e-04  Data: 0.010 (0.012)
Train: 131 [1200/1251 ( 96%)]  Loss: 3.860 (3.85)  Time: 0.677s, 1513.67/s  (0.695s, 1472.77/s)  LR: 8.881e-04  Data: 0.010 (0.012)
Train: 131 [1250/1251 (100%)]  Loss: 3.649 (3.84)  Time: 0.656s, 1562.00/s  (0.695s, 1473.26/s)  LR: 8.881e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.627 (1.627)  Loss:  0.8164 (0.8164)  Acc@1: 89.0625 (89.0625)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.136 (0.589)  Loss:  0.9229 (1.5522)  Acc@1: 82.5472 (70.2100)  Acc@5: 94.9293 (89.8240)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-131.pth.tar', 70.21000004638672)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-124.pth.tar', 70.14199996826171)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-130.pth.tar', 69.97599997070313)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-111.pth.tar', 69.95999996826171)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-120.pth.tar', 69.92200004394532)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-105.pth.tar', 69.8779999951172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-129.pth.tar', 69.72199996826171)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-122.pth.tar', 69.71800015625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-123.pth.tar', 69.70200010009765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-127.pth.tar', 69.64200002929688)

Train: 132 [   0/1251 (  0%)]  Loss: 3.773 (3.77)  Time: 2.310s,  443.23/s  (2.310s,  443.23/s)  LR: 8.864e-04  Data: 1.680 (1.680)
Train: 132 [  50/1251 (  4%)]  Loss: 4.130 (3.95)  Time: 0.674s, 1519.79/s  (0.728s, 1407.19/s)  LR: 8.864e-04  Data: 0.011 (0.048)
Train: 132 [ 100/1251 (  8%)]  Loss: 3.517 (3.81)  Time: 0.704s, 1454.16/s  (0.710s, 1441.61/s)  LR: 8.864e-04  Data: 0.008 (0.029)
Train: 132 [ 150/1251 ( 12%)]  Loss: 3.637 (3.76)  Time: 0.705s, 1452.03/s  (0.702s, 1459.42/s)  LR: 8.864e-04  Data: 0.011 (0.023)
Train: 132 [ 200/1251 ( 16%)]  Loss: 3.400 (3.69)  Time: 0.684s, 1497.74/s  (0.699s, 1465.74/s)  LR: 8.864e-04  Data: 0.010 (0.020)
Train: 132 [ 250/1251 ( 20%)]  Loss: 3.634 (3.68)  Time: 0.673s, 1521.56/s  (0.697s, 1468.26/s)  LR: 8.864e-04  Data: 0.011 (0.018)
Train: 132 [ 300/1251 ( 24%)]  Loss: 4.119 (3.74)  Time: 0.674s, 1518.77/s  (0.697s, 1469.56/s)  LR: 8.864e-04  Data: 0.009 (0.017)
Train: 132 [ 350/1251 ( 28%)]  Loss: 3.902 (3.76)  Time: 0.738s, 1387.89/s  (0.697s, 1468.97/s)  LR: 8.864e-04  Data: 0.010 (0.016)
Train: 132 [ 400/1251 ( 32%)]  Loss: 3.754 (3.76)  Time: 0.690s, 1483.72/s  (0.698s, 1467.50/s)  LR: 8.864e-04  Data: 0.018 (0.015)
Train: 132 [ 450/1251 ( 36%)]  Loss: 4.122 (3.80)  Time: 0.728s, 1405.98/s  (0.698s, 1467.62/s)  LR: 8.864e-04  Data: 0.009 (0.015)
Train: 132 [ 500/1251 ( 40%)]  Loss: 3.409 (3.76)  Time: 0.667s, 1536.35/s  (0.698s, 1468.03/s)  LR: 8.864e-04  Data: 0.010 (0.014)
Train: 132 [ 550/1251 ( 44%)]  Loss: 3.875 (3.77)  Time: 0.694s, 1474.62/s  (0.698s, 1468.01/s)  LR: 8.864e-04  Data: 0.010 (0.014)
Train: 132 [ 600/1251 ( 48%)]  Loss: 4.342 (3.82)  Time: 0.729s, 1404.60/s  (0.698s, 1467.04/s)  LR: 8.864e-04  Data: 0.009 (0.014)
Train: 132 [ 650/1251 ( 52%)]  Loss: 4.021 (3.83)  Time: 0.725s, 1412.32/s  (0.698s, 1467.78/s)  LR: 8.864e-04  Data: 0.010 (0.013)
Train: 132 [ 700/1251 ( 56%)]  Loss: 4.139 (3.85)  Time: 0.679s, 1508.39/s  (0.698s, 1467.51/s)  LR: 8.864e-04  Data: 0.010 (0.013)
Train: 132 [ 750/1251 ( 60%)]  Loss: 3.796 (3.85)  Time: 0.679s, 1507.89/s  (0.697s, 1468.13/s)  LR: 8.864e-04  Data: 0.010 (0.013)
Train: 132 [ 800/1251 ( 64%)]  Loss: 4.046 (3.86)  Time: 0.677s, 1512.70/s  (0.697s, 1469.11/s)  LR: 8.864e-04  Data: 0.015 (0.013)
Train: 132 [ 850/1251 ( 68%)]  Loss: 4.005 (3.87)  Time: 0.676s, 1515.60/s  (0.697s, 1469.92/s)  LR: 8.864e-04  Data: 0.010 (0.013)
Train: 132 [ 900/1251 ( 72%)]  Loss: 3.786 (3.86)  Time: 0.711s, 1440.67/s  (0.696s, 1470.34/s)  LR: 8.864e-04  Data: 0.009 (0.012)
Train: 132 [ 950/1251 ( 76%)]  Loss: 4.130 (3.88)  Time: 0.729s, 1405.13/s  (0.696s, 1470.82/s)  LR: 8.864e-04  Data: 0.010 (0.012)
Train: 132 [1000/1251 ( 80%)]  Loss: 3.932 (3.88)  Time: 0.699s, 1465.87/s  (0.696s, 1470.23/s)  LR: 8.864e-04  Data: 0.010 (0.012)
Train: 132 [1050/1251 ( 84%)]  Loss: 4.239 (3.90)  Time: 0.673s, 1521.63/s  (0.696s, 1470.76/s)  LR: 8.864e-04  Data: 0.009 (0.012)
Train: 132 [1100/1251 ( 88%)]  Loss: 3.975 (3.90)  Time: 0.719s, 1424.79/s  (0.696s, 1470.97/s)  LR: 8.864e-04  Data: 0.009 (0.012)
Train: 132 [1150/1251 ( 92%)]  Loss: 4.162 (3.91)  Time: 0.682s, 1502.57/s  (0.696s, 1471.29/s)  LR: 8.864e-04  Data: 0.010 (0.012)
Train: 132 [1200/1251 ( 96%)]  Loss: 3.592 (3.90)  Time: 0.775s, 1321.49/s  (0.696s, 1471.20/s)  LR: 8.864e-04  Data: 0.012 (0.012)
Train: 132 [1250/1251 (100%)]  Loss: 4.086 (3.90)  Time: 0.657s, 1558.21/s  (0.696s, 1471.95/s)  LR: 8.864e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.567 (1.567)  Loss:  0.9155 (0.9155)  Acc@1: 85.6445 (85.6445)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.136 (0.585)  Loss:  1.0020 (1.6490)  Acc@1: 81.8396 (69.4820)  Acc@5: 95.9906 (89.2560)
Train: 133 [   0/1251 (  0%)]  Loss: 3.880 (3.88)  Time: 2.260s,  453.01/s  (2.260s,  453.01/s)  LR: 8.847e-04  Data: 1.645 (1.645)
Train: 133 [  50/1251 (  4%)]  Loss: 3.884 (3.88)  Time: 0.700s, 1462.16/s  (0.723s, 1416.11/s)  LR: 8.847e-04  Data: 0.010 (0.046)
Train: 133 [ 100/1251 (  8%)]  Loss: 3.579 (3.78)  Time: 0.747s, 1371.68/s  (0.710s, 1441.29/s)  LR: 8.847e-04  Data: 0.017 (0.028)
Train: 133 [ 150/1251 ( 12%)]  Loss: 3.988 (3.83)  Time: 0.680s, 1506.94/s  (0.705s, 1451.49/s)  LR: 8.847e-04  Data: 0.010 (0.022)
Train: 133 [ 200/1251 ( 16%)]  Loss: 3.780 (3.82)  Time: 0.674s, 1519.45/s  (0.701s, 1461.36/s)  LR: 8.847e-04  Data: 0.010 (0.019)
Train: 133 [ 250/1251 ( 20%)]  Loss: 3.987 (3.85)  Time: 0.751s, 1363.95/s  (0.701s, 1461.41/s)  LR: 8.847e-04  Data: 0.010 (0.018)
Train: 133 [ 300/1251 ( 24%)]  Loss: 4.001 (3.87)  Time: 0.708s, 1447.11/s  (0.700s, 1462.09/s)  LR: 8.847e-04  Data: 0.011 (0.016)
Train: 133 [ 350/1251 ( 28%)]  Loss: 3.587 (3.84)  Time: 0.670s, 1527.58/s  (0.699s, 1464.81/s)  LR: 8.847e-04  Data: 0.009 (0.015)
Train: 133 [ 400/1251 ( 32%)]  Loss: 3.541 (3.80)  Time: 0.670s, 1527.59/s  (0.698s, 1467.14/s)  LR: 8.847e-04  Data: 0.010 (0.015)
Train: 133 [ 450/1251 ( 36%)]  Loss: 3.519 (3.77)  Time: 0.703s, 1457.02/s  (0.697s, 1469.33/s)  LR: 8.847e-04  Data: 0.010 (0.014)
Train: 133 [ 500/1251 ( 40%)]  Loss: 3.395 (3.74)  Time: 0.694s, 1475.93/s  (0.698s, 1467.08/s)  LR: 8.847e-04  Data: 0.011 (0.014)
Train: 133 [ 550/1251 ( 44%)]  Loss: 4.000 (3.76)  Time: 0.690s, 1484.35/s  (0.700s, 1463.89/s)  LR: 8.847e-04  Data: 0.013 (0.014)
Train: 133 [ 600/1251 ( 48%)]  Loss: 3.782 (3.76)  Time: 0.686s, 1491.84/s  (0.701s, 1460.04/s)  LR: 8.847e-04  Data: 0.013 (0.014)
Train: 133 [ 650/1251 ( 52%)]  Loss: 3.770 (3.76)  Time: 0.704s, 1453.95/s  (0.702s, 1459.42/s)  LR: 8.847e-04  Data: 0.009 (0.014)
Train: 133 [ 700/1251 ( 56%)]  Loss: 3.957 (3.78)  Time: 0.671s, 1525.15/s  (0.700s, 1461.89/s)  LR: 8.847e-04  Data: 0.010 (0.013)
Train: 133 [ 750/1251 ( 60%)]  Loss: 3.795 (3.78)  Time: 0.700s, 1462.52/s  (0.700s, 1463.06/s)  LR: 8.847e-04  Data: 0.009 (0.013)
Train: 133 [ 800/1251 ( 64%)]  Loss: 3.802 (3.78)  Time: 0.669s, 1531.36/s  (0.699s, 1463.95/s)  LR: 8.847e-04  Data: 0.011 (0.013)
Train: 133 [ 850/1251 ( 68%)]  Loss: 3.557 (3.77)  Time: 0.672s, 1522.90/s  (0.699s, 1464.48/s)  LR: 8.847e-04  Data: 0.010 (0.013)
Train: 133 [ 900/1251 ( 72%)]  Loss: 3.870 (3.77)  Time: 0.672s, 1523.79/s  (0.699s, 1465.92/s)  LR: 8.847e-04  Data: 0.010 (0.013)
Train: 133 [ 950/1251 ( 76%)]  Loss: 3.925 (3.78)  Time: 0.717s, 1428.12/s  (0.698s, 1466.67/s)  LR: 8.847e-04  Data: 0.010 (0.012)
Train: 133 [1000/1251 ( 80%)]  Loss: 3.754 (3.78)  Time: 0.668s, 1532.54/s  (0.698s, 1466.63/s)  LR: 8.847e-04  Data: 0.009 (0.012)
Train: 133 [1050/1251 ( 84%)]  Loss: 3.648 (3.77)  Time: 0.721s, 1420.47/s  (0.698s, 1467.45/s)  LR: 8.847e-04  Data: 0.011 (0.012)
Train: 133 [1100/1251 ( 88%)]  Loss: 3.702 (3.77)  Time: 0.702s, 1459.44/s  (0.698s, 1467.92/s)  LR: 8.847e-04  Data: 0.009 (0.012)
Train: 133 [1150/1251 ( 92%)]  Loss: 4.233 (3.79)  Time: 0.735s, 1393.73/s  (0.697s, 1468.42/s)  LR: 8.847e-04  Data: 0.009 (0.012)
Train: 133 [1200/1251 ( 96%)]  Loss: 3.538 (3.78)  Time: 0.673s, 1522.38/s  (0.697s, 1468.82/s)  LR: 8.847e-04  Data: 0.010 (0.012)
Train: 133 [1250/1251 (100%)]  Loss: 3.903 (3.78)  Time: 0.693s, 1476.95/s  (0.697s, 1469.21/s)  LR: 8.847e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.663 (1.663)  Loss:  1.0801 (1.0801)  Acc@1: 86.4258 (86.4258)  Acc@5: 95.6055 (95.6055)
Test: [  48/48]  Time: 0.136 (0.589)  Loss:  1.1260 (1.6099)  Acc@1: 82.4292 (70.1900)  Acc@5: 94.9293 (89.9200)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-131.pth.tar', 70.21000004638672)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-133.pth.tar', 70.18999999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-124.pth.tar', 70.14199996826171)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-130.pth.tar', 69.97599997070313)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-111.pth.tar', 69.95999996826171)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-120.pth.tar', 69.92200004394532)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-105.pth.tar', 69.8779999951172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-129.pth.tar', 69.72199996826171)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-122.pth.tar', 69.71800015625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-123.pth.tar', 69.70200010009765)

Train: 134 [   0/1251 (  0%)]  Loss: 4.062 (4.06)  Time: 2.403s,  426.09/s  (2.403s,  426.09/s)  LR: 8.831e-04  Data: 1.755 (1.755)
Train: 134 [  50/1251 (  4%)]  Loss: 3.384 (3.72)  Time: 0.673s, 1521.77/s  (0.736s, 1392.13/s)  LR: 8.831e-04  Data: 0.011 (0.050)
Train: 134 [ 100/1251 (  8%)]  Loss: 3.790 (3.75)  Time: 0.673s, 1521.52/s  (0.719s, 1425.02/s)  LR: 8.831e-04  Data: 0.009 (0.030)
Train: 134 [ 150/1251 ( 12%)]  Loss: 3.905 (3.79)  Time: 0.670s, 1528.49/s  (0.708s, 1446.32/s)  LR: 8.831e-04  Data: 0.010 (0.024)
Train: 134 [ 200/1251 ( 16%)]  Loss: 3.968 (3.82)  Time: 0.693s, 1477.78/s  (0.706s, 1450.23/s)  LR: 8.831e-04  Data: 0.009 (0.020)
Train: 134 [ 250/1251 ( 20%)]  Loss: 4.118 (3.87)  Time: 0.672s, 1524.27/s  (0.704s, 1454.83/s)  LR: 8.831e-04  Data: 0.010 (0.018)
Train: 134 [ 300/1251 ( 24%)]  Loss: 3.729 (3.85)  Time: 0.678s, 1510.45/s  (0.703s, 1455.98/s)  LR: 8.831e-04  Data: 0.009 (0.017)
Train: 134 [ 350/1251 ( 28%)]  Loss: 3.838 (3.85)  Time: 0.672s, 1523.88/s  (0.701s, 1460.33/s)  LR: 8.831e-04  Data: 0.011 (0.016)
Train: 134 [ 400/1251 ( 32%)]  Loss: 3.509 (3.81)  Time: 0.670s, 1527.66/s  (0.699s, 1464.22/s)  LR: 8.831e-04  Data: 0.013 (0.015)
Train: 134 [ 450/1251 ( 36%)]  Loss: 3.514 (3.78)  Time: 0.672s, 1523.76/s  (0.699s, 1465.30/s)  LR: 8.831e-04  Data: 0.010 (0.015)
Train: 134 [ 500/1251 ( 40%)]  Loss: 3.778 (3.78)  Time: 0.670s, 1528.09/s  (0.698s, 1467.54/s)  LR: 8.831e-04  Data: 0.010 (0.014)
Train: 134 [ 550/1251 ( 44%)]  Loss: 4.201 (3.82)  Time: 0.706s, 1451.27/s  (0.697s, 1469.11/s)  LR: 8.831e-04  Data: 0.012 (0.014)
Train: 134 [ 600/1251 ( 48%)]  Loss: 3.886 (3.82)  Time: 0.672s, 1524.00/s  (0.696s, 1470.97/s)  LR: 8.831e-04  Data: 0.010 (0.014)
Train: 134 [ 650/1251 ( 52%)]  Loss: 3.994 (3.83)  Time: 0.709s, 1444.77/s  (0.696s, 1471.87/s)  LR: 8.831e-04  Data: 0.009 (0.013)
Train: 134 [ 700/1251 ( 56%)]  Loss: 4.143 (3.85)  Time: 0.673s, 1520.95/s  (0.695s, 1473.33/s)  LR: 8.831e-04  Data: 0.010 (0.013)
Train: 134 [ 750/1251 ( 60%)]  Loss: 3.896 (3.86)  Time: 0.671s, 1526.63/s  (0.695s, 1474.03/s)  LR: 8.831e-04  Data: 0.010 (0.013)
Train: 134 [ 800/1251 ( 64%)]  Loss: 4.057 (3.87)  Time: 0.707s, 1447.93/s  (0.695s, 1474.19/s)  LR: 8.831e-04  Data: 0.010 (0.013)
Train: 134 [ 850/1251 ( 68%)]  Loss: 3.965 (3.87)  Time: 0.719s, 1424.44/s  (0.695s, 1473.58/s)  LR: 8.831e-04  Data: 0.009 (0.013)
Train: 134 [ 900/1251 ( 72%)]  Loss: 3.953 (3.88)  Time: 0.713s, 1437.03/s  (0.695s, 1472.87/s)  LR: 8.831e-04  Data: 0.009 (0.012)
Train: 134 [ 950/1251 ( 76%)]  Loss: 4.031 (3.89)  Time: 0.711s, 1440.02/s  (0.695s, 1472.82/s)  LR: 8.831e-04  Data: 0.010 (0.012)
Train: 134 [1000/1251 ( 80%)]  Loss: 3.731 (3.88)  Time: 0.681s, 1503.74/s  (0.695s, 1472.45/s)  LR: 8.831e-04  Data: 0.010 (0.012)
Train: 134 [1050/1251 ( 84%)]  Loss: 4.109 (3.89)  Time: 0.667s, 1536.15/s  (0.695s, 1472.81/s)  LR: 8.831e-04  Data: 0.011 (0.012)
Train: 134 [1100/1251 ( 88%)]  Loss: 4.103 (3.90)  Time: 0.692s, 1479.34/s  (0.695s, 1472.33/s)  LR: 8.831e-04  Data: 0.009 (0.012)
Train: 134 [1150/1251 ( 92%)]  Loss: 3.707 (3.89)  Time: 0.710s, 1442.36/s  (0.695s, 1473.11/s)  LR: 8.831e-04  Data: 0.012 (0.012)
Train: 134 [1200/1251 ( 96%)]  Loss: 3.975 (3.89)  Time: 0.676s, 1514.06/s  (0.695s, 1473.53/s)  LR: 8.831e-04  Data: 0.009 (0.012)
Train: 134 [1250/1251 (100%)]  Loss: 3.966 (3.90)  Time: 0.656s, 1561.64/s  (0.695s, 1473.97/s)  LR: 8.831e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.582 (1.582)  Loss:  0.9336 (0.9336)  Acc@1: 86.9141 (86.9141)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.136 (0.586)  Loss:  1.1895 (1.6132)  Acc@1: 81.0141 (69.2720)  Acc@5: 93.3962 (89.3200)
Train: 135 [   0/1251 (  0%)]  Loss: 4.093 (4.09)  Time: 2.211s,  463.11/s  (2.211s,  463.11/s)  LR: 8.814e-04  Data: 1.595 (1.595)
Train: 135 [  50/1251 (  4%)]  Loss: 4.063 (4.08)  Time: 0.678s, 1510.80/s  (0.737s, 1389.68/s)  LR: 8.814e-04  Data: 0.009 (0.054)
Train: 135 [ 100/1251 (  8%)]  Loss: 3.587 (3.91)  Time: 0.699s, 1465.32/s  (0.716s, 1429.28/s)  LR: 8.814e-04  Data: 0.010 (0.033)
Train: 135 [ 150/1251 ( 12%)]  Loss: 3.288 (3.76)  Time: 0.711s, 1440.50/s  (0.710s, 1443.25/s)  LR: 8.814e-04  Data: 0.010 (0.026)
Train: 135 [ 200/1251 ( 16%)]  Loss: 3.761 (3.76)  Time: 0.672s, 1524.03/s  (0.705s, 1451.58/s)  LR: 8.814e-04  Data: 0.009 (0.022)
Train: 135 [ 250/1251 ( 20%)]  Loss: 4.013 (3.80)  Time: 0.671s, 1525.07/s  (0.704s, 1454.99/s)  LR: 8.814e-04  Data: 0.010 (0.019)
Train: 135 [ 300/1251 ( 24%)]  Loss: 3.707 (3.79)  Time: 0.676s, 1514.12/s  (0.702s, 1458.45/s)  LR: 8.814e-04  Data: 0.010 (0.018)
Train: 135 [ 350/1251 ( 28%)]  Loss: 3.732 (3.78)  Time: 0.708s, 1446.77/s  (0.700s, 1462.18/s)  LR: 8.814e-04  Data: 0.010 (0.017)
Train: 135 [ 400/1251 ( 32%)]  Loss: 4.216 (3.83)  Time: 0.674s, 1519.20/s  (0.699s, 1464.23/s)  LR: 8.814e-04  Data: 0.009 (0.016)
Train: 135 [ 450/1251 ( 36%)]  Loss: 3.671 (3.81)  Time: 0.712s, 1438.56/s  (0.698s, 1467.05/s)  LR: 8.814e-04  Data: 0.010 (0.015)
Train: 135 [ 500/1251 ( 40%)]  Loss: 4.158 (3.84)  Time: 0.702s, 1458.52/s  (0.697s, 1468.48/s)  LR: 8.814e-04  Data: 0.009 (0.015)
Train: 135 [ 550/1251 ( 44%)]  Loss: 3.733 (3.84)  Time: 0.700s, 1463.60/s  (0.697s, 1468.57/s)  LR: 8.814e-04  Data: 0.010 (0.014)
Train: 135 [ 600/1251 ( 48%)]  Loss: 3.921 (3.84)  Time: 0.670s, 1529.22/s  (0.697s, 1468.95/s)  LR: 8.814e-04  Data: 0.011 (0.014)
Train: 135 [ 650/1251 ( 52%)]  Loss: 3.733 (3.83)  Time: 0.706s, 1451.37/s  (0.697s, 1470.15/s)  LR: 8.814e-04  Data: 0.009 (0.014)
Train: 135 [ 700/1251 ( 56%)]  Loss: 3.768 (3.83)  Time: 0.750s, 1364.52/s  (0.696s, 1470.67/s)  LR: 8.814e-04  Data: 0.010 (0.013)
Train: 135 [ 750/1251 ( 60%)]  Loss: 4.086 (3.85)  Time: 0.794s, 1289.41/s  (0.696s, 1470.59/s)  LR: 8.814e-04  Data: 0.010 (0.013)
Train: 135 [ 800/1251 ( 64%)]  Loss: 3.555 (3.83)  Time: 0.697s, 1468.79/s  (0.696s, 1471.26/s)  LR: 8.814e-04  Data: 0.009 (0.013)
Train: 135 [ 850/1251 ( 68%)]  Loss: 3.836 (3.83)  Time: 0.677s, 1512.04/s  (0.695s, 1472.39/s)  LR: 8.814e-04  Data: 0.010 (0.013)
Train: 135 [ 900/1251 ( 72%)]  Loss: 3.886 (3.83)  Time: 0.671s, 1526.41/s  (0.695s, 1472.75/s)  LR: 8.814e-04  Data: 0.012 (0.013)
Train: 135 [ 950/1251 ( 76%)]  Loss: 3.701 (3.83)  Time: 0.704s, 1454.33/s  (0.696s, 1471.85/s)  LR: 8.814e-04  Data: 0.009 (0.013)
Train: 135 [1000/1251 ( 80%)]  Loss: 3.662 (3.82)  Time: 0.739s, 1386.22/s  (0.696s, 1471.87/s)  LR: 8.814e-04  Data: 0.016 (0.012)
Train: 135 [1050/1251 ( 84%)]  Loss: 3.981 (3.83)  Time: 0.705s, 1451.66/s  (0.696s, 1471.82/s)  LR: 8.814e-04  Data: 0.010 (0.012)
Train: 135 [1100/1251 ( 88%)]  Loss: 4.075 (3.84)  Time: 0.719s, 1423.92/s  (0.696s, 1472.14/s)  LR: 8.814e-04  Data: 0.010 (0.012)
Train: 135 [1150/1251 ( 92%)]  Loss: 4.209 (3.85)  Time: 0.673s, 1521.30/s  (0.695s, 1472.52/s)  LR: 8.814e-04  Data: 0.010 (0.012)
Train: 135 [1200/1251 ( 96%)]  Loss: 4.128 (3.86)  Time: 0.672s, 1523.40/s  (0.695s, 1472.89/s)  LR: 8.814e-04  Data: 0.011 (0.012)
Train: 135 [1250/1251 (100%)]  Loss: 3.553 (3.85)  Time: 0.658s, 1555.97/s  (0.696s, 1472.16/s)  LR: 8.814e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.500 (1.500)  Loss:  1.0020 (1.0020)  Acc@1: 84.2773 (84.2773)  Acc@5: 96.0938 (96.0938)
Test: [  48/48]  Time: 0.136 (0.574)  Loss:  1.0518 (1.5454)  Acc@1: 81.7217 (69.6480)  Acc@5: 94.2217 (89.5800)
Train: 136 [   0/1251 (  0%)]  Loss: 3.728 (3.73)  Time: 2.188s,  467.99/s  (2.188s,  467.99/s)  LR: 8.797e-04  Data: 1.575 (1.575)
Train: 136 [  50/1251 (  4%)]  Loss: 4.146 (3.94)  Time: 0.677s, 1511.55/s  (0.729s, 1404.15/s)  LR: 8.797e-04  Data: 0.010 (0.051)
Train: 136 [ 100/1251 (  8%)]  Loss: 3.887 (3.92)  Time: 0.685s, 1495.36/s  (0.712s, 1437.92/s)  LR: 8.797e-04  Data: 0.009 (0.031)
Train: 136 [ 150/1251 ( 12%)]  Loss: 4.357 (4.03)  Time: 0.704s, 1454.47/s  (0.704s, 1454.93/s)  LR: 8.797e-04  Data: 0.011 (0.024)
Train: 136 [ 200/1251 ( 16%)]  Loss: 4.134 (4.05)  Time: 0.668s, 1533.23/s  (0.700s, 1462.59/s)  LR: 8.797e-04  Data: 0.011 (0.021)
Train: 136 [ 250/1251 ( 20%)]  Loss: 3.671 (3.99)  Time: 0.672s, 1524.48/s  (0.698s, 1466.33/s)  LR: 8.797e-04  Data: 0.010 (0.018)
Train: 136 [ 300/1251 ( 24%)]  Loss: 3.942 (3.98)  Time: 0.705s, 1453.47/s  (0.697s, 1469.19/s)  LR: 8.797e-04  Data: 0.009 (0.017)
Train: 136 [ 350/1251 ( 28%)]  Loss: 3.933 (3.97)  Time: 0.672s, 1523.09/s  (0.696s, 1470.88/s)  LR: 8.797e-04  Data: 0.010 (0.016)
Train: 136 [ 400/1251 ( 32%)]  Loss: 3.746 (3.95)  Time: 0.723s, 1415.78/s  (0.696s, 1471.80/s)  LR: 8.797e-04  Data: 0.010 (0.015)
Train: 136 [ 450/1251 ( 36%)]  Loss: 4.283 (3.98)  Time: 0.672s, 1523.71/s  (0.696s, 1471.95/s)  LR: 8.797e-04  Data: 0.010 (0.015)
Train: 136 [ 500/1251 ( 40%)]  Loss: 3.718 (3.96)  Time: 0.690s, 1483.14/s  (0.695s, 1473.42/s)  LR: 8.797e-04  Data: 0.013 (0.014)
Train: 136 [ 550/1251 ( 44%)]  Loss: 3.588 (3.93)  Time: 0.669s, 1531.55/s  (0.695s, 1474.02/s)  LR: 8.797e-04  Data: 0.009 (0.014)
Train: 136 [ 600/1251 ( 48%)]  Loss: 4.165 (3.95)  Time: 0.681s, 1503.73/s  (0.695s, 1474.39/s)  LR: 8.797e-04  Data: 0.011 (0.014)
Train: 136 [ 650/1251 ( 52%)]  Loss: 3.956 (3.95)  Time: 0.702s, 1457.95/s  (0.694s, 1474.87/s)  LR: 8.797e-04  Data: 0.010 (0.013)
Train: 136 [ 700/1251 ( 56%)]  Loss: 3.836 (3.94)  Time: 0.674s, 1519.71/s  (0.694s, 1476.11/s)  LR: 8.797e-04  Data: 0.013 (0.013)
Train: 136 [ 750/1251 ( 60%)]  Loss: 3.839 (3.93)  Time: 0.705s, 1453.10/s  (0.693s, 1476.74/s)  LR: 8.797e-04  Data: 0.013 (0.013)
Train: 136 [ 800/1251 ( 64%)]  Loss: 4.124 (3.94)  Time: 0.711s, 1439.65/s  (0.694s, 1476.38/s)  LR: 8.797e-04  Data: 0.009 (0.013)
Train: 136 [ 850/1251 ( 68%)]  Loss: 3.800 (3.94)  Time: 0.674s, 1518.94/s  (0.693s, 1476.70/s)  LR: 8.797e-04  Data: 0.011 (0.013)
Train: 136 [ 900/1251 ( 72%)]  Loss: 3.928 (3.94)  Time: 0.670s, 1527.97/s  (0.693s, 1477.32/s)  LR: 8.797e-04  Data: 0.010 (0.013)
Train: 136 [ 950/1251 ( 76%)]  Loss: 3.617 (3.92)  Time: 0.716s, 1430.77/s  (0.693s, 1477.33/s)  LR: 8.797e-04  Data: 0.009 (0.012)
Train: 136 [1000/1251 ( 80%)]  Loss: 4.210 (3.93)  Time: 0.672s, 1522.85/s  (0.693s, 1477.31/s)  LR: 8.797e-04  Data: 0.010 (0.012)
Train: 136 [1050/1251 ( 84%)]  Loss: 3.966 (3.94)  Time: 0.687s, 1490.82/s  (0.693s, 1477.82/s)  LR: 8.797e-04  Data: 0.010 (0.012)
Train: 136 [1100/1251 ( 88%)]  Loss: 4.137 (3.94)  Time: 0.675s, 1517.57/s  (0.693s, 1477.82/s)  LR: 8.797e-04  Data: 0.010 (0.012)
Train: 136 [1150/1251 ( 92%)]  Loss: 4.075 (3.95)  Time: 0.694s, 1474.92/s  (0.693s, 1477.36/s)  LR: 8.797e-04  Data: 0.010 (0.012)
Train: 136 [1200/1251 ( 96%)]  Loss: 3.876 (3.95)  Time: 0.677s, 1513.46/s  (0.693s, 1477.52/s)  LR: 8.797e-04  Data: 0.011 (0.012)
Train: 136 [1250/1251 (100%)]  Loss: 3.929 (3.95)  Time: 0.670s, 1528.40/s  (0.693s, 1477.71/s)  LR: 8.797e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.537 (1.537)  Loss:  1.0146 (1.0146)  Acc@1: 84.6680 (84.6680)  Acc@5: 96.0938 (96.0938)
Test: [  48/48]  Time: 0.139 (0.592)  Loss:  1.0039 (1.5664)  Acc@1: 81.7217 (69.5540)  Acc@5: 94.6934 (89.3400)
Train: 137 [   0/1251 (  0%)]  Loss: 3.970 (3.97)  Time: 2.284s,  448.25/s  (2.284s,  448.25/s)  LR: 8.780e-04  Data: 1.669 (1.669)
Train: 137 [  50/1251 (  4%)]  Loss: 3.697 (3.83)  Time: 0.727s, 1408.87/s  (0.727s, 1408.13/s)  LR: 8.780e-04  Data: 0.011 (0.047)
Train: 137 [ 100/1251 (  8%)]  Loss: 3.601 (3.76)  Time: 0.682s, 1501.92/s  (0.711s, 1440.13/s)  LR: 8.780e-04  Data: 0.013 (0.029)
Train: 137 [ 150/1251 ( 12%)]  Loss: 3.711 (3.74)  Time: 0.670s, 1529.47/s  (0.703s, 1455.85/s)  LR: 8.780e-04  Data: 0.009 (0.023)
Train: 137 [ 200/1251 ( 16%)]  Loss: 3.995 (3.79)  Time: 0.685s, 1494.11/s  (0.702s, 1459.67/s)  LR: 8.780e-04  Data: 0.012 (0.020)
Train: 137 [ 250/1251 ( 20%)]  Loss: 4.069 (3.84)  Time: 0.668s, 1533.68/s  (0.701s, 1460.18/s)  LR: 8.780e-04  Data: 0.010 (0.018)
Train: 137 [ 300/1251 ( 24%)]  Loss: 3.967 (3.86)  Time: 0.776s, 1319.94/s  (0.700s, 1462.17/s)  LR: 8.780e-04  Data: 0.011 (0.017)
Train: 137 [ 350/1251 ( 28%)]  Loss: 3.591 (3.83)  Time: 0.737s, 1388.66/s  (0.699s, 1465.51/s)  LR: 8.780e-04  Data: 0.018 (0.016)
Train: 137 [ 400/1251 ( 32%)]  Loss: 3.623 (3.80)  Time: 0.674s, 1518.66/s  (0.698s, 1467.29/s)  LR: 8.780e-04  Data: 0.011 (0.015)
Train: 137 [ 450/1251 ( 36%)]  Loss: 3.709 (3.79)  Time: 0.715s, 1433.05/s  (0.697s, 1468.61/s)  LR: 8.780e-04  Data: 0.011 (0.014)
Train: 137 [ 500/1251 ( 40%)]  Loss: 4.248 (3.83)  Time: 0.692s, 1480.04/s  (0.697s, 1468.52/s)  LR: 8.780e-04  Data: 0.009 (0.014)
Train: 137 [ 550/1251 ( 44%)]  Loss: 3.966 (3.85)  Time: 0.670s, 1527.75/s  (0.698s, 1467.36/s)  LR: 8.780e-04  Data: 0.009 (0.014)
Train: 137 [ 600/1251 ( 48%)]  Loss: 3.861 (3.85)  Time: 0.791s, 1294.70/s  (0.698s, 1466.86/s)  LR: 8.780e-04  Data: 0.009 (0.013)
Train: 137 [ 650/1251 ( 52%)]  Loss: 3.955 (3.85)  Time: 0.701s, 1459.93/s  (0.698s, 1467.37/s)  LR: 8.780e-04  Data: 0.010 (0.013)
Train: 137 [ 700/1251 ( 56%)]  Loss: 3.867 (3.86)  Time: 0.702s, 1457.69/s  (0.697s, 1468.34/s)  LR: 8.780e-04  Data: 0.010 (0.013)
Train: 137 [ 750/1251 ( 60%)]  Loss: 3.667 (3.84)  Time: 0.669s, 1530.87/s  (0.698s, 1467.79/s)  LR: 8.780e-04  Data: 0.009 (0.013)
Train: 137 [ 800/1251 ( 64%)]  Loss: 3.517 (3.82)  Time: 0.704s, 1453.58/s  (0.697s, 1468.55/s)  LR: 8.780e-04  Data: 0.010 (0.013)
Train: 137 [ 850/1251 ( 68%)]  Loss: 3.650 (3.81)  Time: 0.673s, 1520.59/s  (0.697s, 1469.69/s)  LR: 8.780e-04  Data: 0.010 (0.013)
Train: 137 [ 900/1251 ( 72%)]  Loss: 4.083 (3.83)  Time: 0.671s, 1525.53/s  (0.697s, 1468.99/s)  LR: 8.780e-04  Data: 0.010 (0.012)
Train: 137 [ 950/1251 ( 76%)]  Loss: 3.862 (3.83)  Time: 0.703s, 1456.05/s  (0.697s, 1469.74/s)  LR: 8.780e-04  Data: 0.010 (0.012)
Train: 137 [1000/1251 ( 80%)]  Loss: 3.995 (3.84)  Time: 0.685s, 1495.88/s  (0.697s, 1469.35/s)  LR: 8.780e-04  Data: 0.009 (0.012)
Train: 137 [1050/1251 ( 84%)]  Loss: 3.946 (3.84)  Time: 0.672s, 1524.83/s  (0.696s, 1470.46/s)  LR: 8.780e-04  Data: 0.009 (0.012)
Train: 137 [1100/1251 ( 88%)]  Loss: 3.540 (3.83)  Time: 0.677s, 1512.34/s  (0.696s, 1470.88/s)  LR: 8.780e-04  Data: 0.010 (0.012)
Train: 137 [1150/1251 ( 92%)]  Loss: 3.999 (3.84)  Time: 0.705s, 1452.01/s  (0.696s, 1470.98/s)  LR: 8.780e-04  Data: 0.042 (0.012)
Train: 137 [1200/1251 ( 96%)]  Loss: 3.754 (3.83)  Time: 0.669s, 1530.47/s  (0.696s, 1471.65/s)  LR: 8.780e-04  Data: 0.010 (0.012)
Train: 137 [1250/1251 (100%)]  Loss: 4.256 (3.85)  Time: 0.657s, 1559.21/s  (0.696s, 1471.95/s)  LR: 8.780e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.660 (1.660)  Loss:  1.0508 (1.0508)  Acc@1: 87.2070 (87.2070)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.136 (0.586)  Loss:  1.0479 (1.6282)  Acc@1: 83.3726 (70.0180)  Acc@5: 94.9292 (89.5560)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-131.pth.tar', 70.21000004638672)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-133.pth.tar', 70.18999999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-124.pth.tar', 70.14199996826171)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-137.pth.tar', 70.01800001708985)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-130.pth.tar', 69.97599997070313)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-111.pth.tar', 69.95999996826171)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-120.pth.tar', 69.92200004394532)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-105.pth.tar', 69.8779999951172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-129.pth.tar', 69.72199996826171)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-122.pth.tar', 69.71800015625)

Train: 138 [   0/1251 (  0%)]  Loss: 3.898 (3.90)  Time: 2.358s,  434.24/s  (2.358s,  434.24/s)  LR: 8.763e-04  Data: 1.727 (1.727)
Train: 138 [  50/1251 (  4%)]  Loss: 4.060 (3.98)  Time: 0.724s, 1414.56/s  (0.745s, 1375.20/s)  LR: 8.763e-04  Data: 0.015 (0.057)
Train: 138 [ 100/1251 (  8%)]  Loss: 4.228 (4.06)  Time: 0.684s, 1497.49/s  (0.722s, 1419.03/s)  LR: 8.763e-04  Data: 0.014 (0.034)
Train: 138 [ 150/1251 ( 12%)]  Loss: 3.779 (3.99)  Time: 0.709s, 1444.35/s  (0.713s, 1436.03/s)  LR: 8.763e-04  Data: 0.010 (0.026)
Train: 138 [ 200/1251 ( 16%)]  Loss: 3.825 (3.96)  Time: 0.674s, 1520.22/s  (0.707s, 1448.03/s)  LR: 8.763e-04  Data: 0.011 (0.022)
Train: 138 [ 250/1251 ( 20%)]  Loss: 3.876 (3.94)  Time: 0.673s, 1522.36/s  (0.704s, 1453.85/s)  LR: 8.763e-04  Data: 0.010 (0.020)
Train: 138 [ 300/1251 ( 24%)]  Loss: 3.965 (3.95)  Time: 0.674s, 1518.53/s  (0.703s, 1455.85/s)  LR: 8.763e-04  Data: 0.010 (0.018)
Train: 138 [ 350/1251 ( 28%)]  Loss: 3.950 (3.95)  Time: 0.672s, 1524.02/s  (0.703s, 1456.86/s)  LR: 8.763e-04  Data: 0.010 (0.017)
Train: 138 [ 400/1251 ( 32%)]  Loss: 3.705 (3.92)  Time: 0.706s, 1450.30/s  (0.702s, 1459.05/s)  LR: 8.763e-04  Data: 0.009 (0.016)
Train: 138 [ 450/1251 ( 36%)]  Loss: 4.125 (3.94)  Time: 0.673s, 1522.04/s  (0.701s, 1460.17/s)  LR: 8.763e-04  Data: 0.011 (0.016)
Train: 138 [ 500/1251 ( 40%)]  Loss: 3.656 (3.92)  Time: 0.699s, 1465.15/s  (0.701s, 1461.24/s)  LR: 8.763e-04  Data: 0.010 (0.015)
Train: 138 [ 550/1251 ( 44%)]  Loss: 4.003 (3.92)  Time: 0.701s, 1459.95/s  (0.701s, 1461.29/s)  LR: 8.763e-04  Data: 0.009 (0.015)
Train: 138 [ 600/1251 ( 48%)]  Loss: 3.918 (3.92)  Time: 0.667s, 1535.92/s  (0.700s, 1462.35/s)  LR: 8.763e-04  Data: 0.012 (0.014)
Train: 138 [ 650/1251 ( 52%)]  Loss: 4.231 (3.94)  Time: 0.675s, 1517.02/s  (0.699s, 1464.03/s)  LR: 8.763e-04  Data: 0.010 (0.014)
Train: 138 [ 700/1251 ( 56%)]  Loss: 4.067 (3.95)  Time: 0.673s, 1520.89/s  (0.700s, 1463.64/s)  LR: 8.763e-04  Data: 0.011 (0.014)
Train: 138 [ 750/1251 ( 60%)]  Loss: 3.912 (3.95)  Time: 0.702s, 1457.67/s  (0.699s, 1464.84/s)  LR: 8.763e-04  Data: 0.009 (0.013)
Train: 138 [ 800/1251 ( 64%)]  Loss: 3.925 (3.95)  Time: 0.712s, 1439.08/s  (0.699s, 1465.41/s)  LR: 8.763e-04  Data: 0.009 (0.013)
Train: 138 [ 850/1251 ( 68%)]  Loss: 3.867 (3.94)  Time: 0.778s, 1316.24/s  (0.698s, 1466.09/s)  LR: 8.763e-04  Data: 0.010 (0.013)
Train: 138 [ 900/1251 ( 72%)]  Loss: 3.892 (3.94)  Time: 0.672s, 1524.01/s  (0.698s, 1467.40/s)  LR: 8.763e-04  Data: 0.010 (0.013)
Train: 138 [ 950/1251 ( 76%)]  Loss: 4.123 (3.95)  Time: 0.691s, 1481.01/s  (0.698s, 1467.56/s)  LR: 8.763e-04  Data: 0.011 (0.013)
Train: 138 [1000/1251 ( 80%)]  Loss: 3.862 (3.95)  Time: 0.672s, 1524.56/s  (0.697s, 1468.45/s)  LR: 8.763e-04  Data: 0.009 (0.013)
Train: 138 [1050/1251 ( 84%)]  Loss: 4.213 (3.96)  Time: 0.696s, 1471.48/s  (0.697s, 1469.13/s)  LR: 8.763e-04  Data: 0.009 (0.013)
Train: 138 [1100/1251 ( 88%)]  Loss: 3.949 (3.96)  Time: 0.689s, 1485.95/s  (0.697s, 1470.09/s)  LR: 8.763e-04  Data: 0.011 (0.012)
Train: 138 [1150/1251 ( 92%)]  Loss: 3.834 (3.95)  Time: 0.685s, 1495.43/s  (0.696s, 1470.33/s)  LR: 8.763e-04  Data: 0.011 (0.012)
Train: 138 [1200/1251 ( 96%)]  Loss: 4.134 (3.96)  Time: 0.665s, 1538.73/s  (0.696s, 1470.73/s)  LR: 8.763e-04  Data: 0.010 (0.012)
Train: 138 [1250/1251 (100%)]  Loss: 4.051 (3.96)  Time: 0.659s, 1554.44/s  (0.696s, 1471.24/s)  LR: 8.763e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.495 (1.495)  Loss:  0.8154 (0.8154)  Acc@1: 86.2305 (86.2305)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.136 (0.584)  Loss:  0.9595 (1.5231)  Acc@1: 83.8443 (70.3320)  Acc@5: 95.2830 (89.9560)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-138.pth.tar', 70.33200009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-131.pth.tar', 70.21000004638672)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-133.pth.tar', 70.18999999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-124.pth.tar', 70.14199996826171)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-137.pth.tar', 70.01800001708985)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-130.pth.tar', 69.97599997070313)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-111.pth.tar', 69.95999996826171)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-120.pth.tar', 69.92200004394532)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-105.pth.tar', 69.8779999951172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-129.pth.tar', 69.72199996826171)

Train: 139 [   0/1251 (  0%)]  Loss: 3.814 (3.81)  Time: 2.266s,  451.97/s  (2.266s,  451.97/s)  LR: 8.746e-04  Data: 1.650 (1.650)
Train: 139 [  50/1251 (  4%)]  Loss: 3.897 (3.86)  Time: 0.672s, 1523.36/s  (0.732s, 1398.11/s)  LR: 8.746e-04  Data: 0.010 (0.049)
Train: 139 [ 100/1251 (  8%)]  Loss: 3.683 (3.80)  Time: 0.707s, 1448.56/s  (0.714s, 1434.42/s)  LR: 8.746e-04  Data: 0.008 (0.030)
Train: 139 [ 150/1251 ( 12%)]  Loss: 4.205 (3.90)  Time: 0.700s, 1463.82/s  (0.709s, 1444.25/s)  LR: 8.746e-04  Data: 0.009 (0.023)
Train: 139 [ 200/1251 ( 16%)]  Loss: 4.087 (3.94)  Time: 0.703s, 1457.53/s  (0.706s, 1450.84/s)  LR: 8.746e-04  Data: 0.010 (0.020)
Train: 139 [ 250/1251 ( 20%)]  Loss: 4.179 (3.98)  Time: 0.675s, 1517.83/s  (0.703s, 1457.40/s)  LR: 8.746e-04  Data: 0.010 (0.018)
Train: 139 [ 300/1251 ( 24%)]  Loss: 3.597 (3.92)  Time: 0.673s, 1522.37/s  (0.700s, 1462.06/s)  LR: 8.746e-04  Data: 0.010 (0.017)
Train: 139 [ 350/1251 ( 28%)]  Loss: 4.099 (3.95)  Time: 0.706s, 1449.59/s  (0.699s, 1465.29/s)  LR: 8.746e-04  Data: 0.010 (0.016)
Train: 139 [ 400/1251 ( 32%)]  Loss: 3.890 (3.94)  Time: 0.706s, 1450.29/s  (0.698s, 1467.06/s)  LR: 8.746e-04  Data: 0.009 (0.015)
Train: 139 [ 450/1251 ( 36%)]  Loss: 4.025 (3.95)  Time: 0.771s, 1328.35/s  (0.698s, 1466.05/s)  LR: 8.746e-04  Data: 0.009 (0.015)
Train: 139 [ 500/1251 ( 40%)]  Loss: 3.601 (3.92)  Time: 0.744s, 1376.69/s  (0.699s, 1465.72/s)  LR: 8.746e-04  Data: 0.014 (0.014)
Train: 139 [ 550/1251 ( 44%)]  Loss: 3.656 (3.89)  Time: 0.702s, 1457.82/s  (0.698s, 1466.77/s)  LR: 8.746e-04  Data: 0.009 (0.014)
Train: 139 [ 600/1251 ( 48%)]  Loss: 4.073 (3.91)  Time: 0.670s, 1527.90/s  (0.697s, 1469.21/s)  LR: 8.746e-04  Data: 0.010 (0.014)
Train: 139 [ 650/1251 ( 52%)]  Loss: 3.637 (3.89)  Time: 0.706s, 1451.31/s  (0.696s, 1471.02/s)  LR: 8.746e-04  Data: 0.009 (0.013)
Train: 139 [ 700/1251 ( 56%)]  Loss: 4.314 (3.92)  Time: 0.703s, 1456.64/s  (0.696s, 1471.90/s)  LR: 8.746e-04  Data: 0.009 (0.013)
Train: 139 [ 750/1251 ( 60%)]  Loss: 3.617 (3.90)  Time: 0.675s, 1517.58/s  (0.696s, 1472.31/s)  LR: 8.746e-04  Data: 0.011 (0.013)
Train: 139 [ 800/1251 ( 64%)]  Loss: 4.026 (3.91)  Time: 0.668s, 1533.06/s  (0.695s, 1472.34/s)  LR: 8.746e-04  Data: 0.009 (0.013)
Train: 139 [ 850/1251 ( 68%)]  Loss: 3.688 (3.89)  Time: 0.667s, 1535.37/s  (0.695s, 1472.95/s)  LR: 8.746e-04  Data: 0.010 (0.013)
Train: 139 [ 900/1251 ( 72%)]  Loss: 3.967 (3.90)  Time: 0.705s, 1453.04/s  (0.695s, 1472.74/s)  LR: 8.746e-04  Data: 0.010 (0.012)
Train: 139 [ 950/1251 ( 76%)]  Loss: 3.987 (3.90)  Time: 0.707s, 1449.08/s  (0.696s, 1472.15/s)  LR: 8.746e-04  Data: 0.011 (0.012)
Train: 139 [1000/1251 ( 80%)]  Loss: 3.689 (3.89)  Time: 0.671s, 1526.99/s  (0.696s, 1471.30/s)  LR: 8.746e-04  Data: 0.010 (0.012)
Train: 139 [1050/1251 ( 84%)]  Loss: 3.852 (3.89)  Time: 0.707s, 1447.55/s  (0.696s, 1471.97/s)  LR: 8.746e-04  Data: 0.009 (0.012)
Train: 139 [1100/1251 ( 88%)]  Loss: 3.897 (3.89)  Time: 0.675s, 1516.37/s  (0.695s, 1472.76/s)  LR: 8.746e-04  Data: 0.015 (0.012)
Train: 139 [1150/1251 ( 92%)]  Loss: 3.636 (3.88)  Time: 0.705s, 1451.70/s  (0.696s, 1471.87/s)  LR: 8.746e-04  Data: 0.011 (0.012)
Train: 139 [1200/1251 ( 96%)]  Loss: 4.574 (3.91)  Time: 0.684s, 1497.30/s  (0.696s, 1471.56/s)  LR: 8.746e-04  Data: 0.009 (0.012)
Train: 139 [1250/1251 (100%)]  Loss: 3.986 (3.91)  Time: 0.658s, 1557.24/s  (0.696s, 1472.06/s)  LR: 8.746e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.480 (1.480)  Loss:  0.8604 (0.8604)  Acc@1: 86.8164 (86.8164)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.136 (0.592)  Loss:  1.1387 (1.5396)  Acc@1: 82.0755 (70.2640)  Acc@5: 95.0472 (89.8500)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-138.pth.tar', 70.33200009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-139.pth.tar', 70.26399997070313)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-131.pth.tar', 70.21000004638672)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-133.pth.tar', 70.18999999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-124.pth.tar', 70.14199996826171)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-137.pth.tar', 70.01800001708985)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-130.pth.tar', 69.97599997070313)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-111.pth.tar', 69.95999996826171)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-120.pth.tar', 69.92200004394532)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-105.pth.tar', 69.8779999951172)

Train: 140 [   0/1251 (  0%)]  Loss: 4.084 (4.08)  Time: 2.324s,  440.57/s  (2.324s,  440.57/s)  LR: 8.729e-04  Data: 1.710 (1.710)
Train: 140 [  50/1251 (  4%)]  Loss: 3.937 (4.01)  Time: 0.686s, 1491.82/s  (0.728s, 1406.02/s)  LR: 8.729e-04  Data: 0.011 (0.050)
Train: 140 [ 100/1251 (  8%)]  Loss: 3.452 (3.82)  Time: 0.673s, 1520.98/s  (0.713s, 1435.22/s)  LR: 8.729e-04  Data: 0.010 (0.030)
Train: 140 [ 150/1251 ( 12%)]  Loss: 3.901 (3.84)  Time: 0.696s, 1470.98/s  (0.707s, 1447.69/s)  LR: 8.729e-04  Data: 0.009 (0.024)
Train: 140 [ 200/1251 ( 16%)]  Loss: 3.899 (3.85)  Time: 0.699s, 1464.77/s  (0.703s, 1456.28/s)  LR: 8.729e-04  Data: 0.011 (0.020)
Train: 140 [ 250/1251 ( 20%)]  Loss: 4.243 (3.92)  Time: 0.672s, 1523.33/s  (0.701s, 1460.58/s)  LR: 8.729e-04  Data: 0.011 (0.018)
Train: 140 [ 300/1251 ( 24%)]  Loss: 3.892 (3.92)  Time: 0.711s, 1440.66/s  (0.700s, 1463.45/s)  LR: 8.729e-04  Data: 0.009 (0.017)
Train: 140 [ 350/1251 ( 28%)]  Loss: 4.170 (3.95)  Time: 0.679s, 1508.53/s  (0.699s, 1464.11/s)  LR: 8.729e-04  Data: 0.014 (0.016)
Train: 140 [ 400/1251 ( 32%)]  Loss: 3.959 (3.95)  Time: 0.702s, 1458.78/s  (0.698s, 1467.46/s)  LR: 8.729e-04  Data: 0.010 (0.015)
Train: 140 [ 450/1251 ( 36%)]  Loss: 4.048 (3.96)  Time: 0.753s, 1360.67/s  (0.698s, 1467.08/s)  LR: 8.729e-04  Data: 0.010 (0.015)
Train: 140 [ 500/1251 ( 40%)]  Loss: 3.646 (3.93)  Time: 0.672s, 1522.89/s  (0.697s, 1468.72/s)  LR: 8.729e-04  Data: 0.011 (0.014)
Train: 140 [ 550/1251 ( 44%)]  Loss: 3.618 (3.90)  Time: 0.673s, 1520.76/s  (0.697s, 1470.06/s)  LR: 8.729e-04  Data: 0.011 (0.014)
Train: 140 [ 600/1251 ( 48%)]  Loss: 3.558 (3.88)  Time: 0.673s, 1521.31/s  (0.696s, 1471.66/s)  LR: 8.729e-04  Data: 0.008 (0.014)
Train: 140 [ 650/1251 ( 52%)]  Loss: 4.108 (3.89)  Time: 0.731s, 1401.57/s  (0.696s, 1472.32/s)  LR: 8.729e-04  Data: 0.011 (0.013)
Train: 140 [ 700/1251 ( 56%)]  Loss: 3.850 (3.89)  Time: 0.716s, 1431.01/s  (0.696s, 1472.22/s)  LR: 8.729e-04  Data: 0.009 (0.013)
Train: 140 [ 750/1251 ( 60%)]  Loss: 3.919 (3.89)  Time: 0.714s, 1434.26/s  (0.695s, 1472.80/s)  LR: 8.729e-04  Data: 0.010 (0.013)
Train: 140 [ 800/1251 ( 64%)]  Loss: 3.833 (3.89)  Time: 0.673s, 1522.18/s  (0.695s, 1472.57/s)  LR: 8.729e-04  Data: 0.010 (0.013)
Train: 140 [ 850/1251 ( 68%)]  Loss: 3.682 (3.88)  Time: 0.676s, 1515.03/s  (0.695s, 1473.70/s)  LR: 8.729e-04  Data: 0.010 (0.013)
Train: 140 [ 900/1251 ( 72%)]  Loss: 3.965 (3.88)  Time: 0.704s, 1454.21/s  (0.695s, 1474.11/s)  LR: 8.729e-04  Data: 0.011 (0.012)
Train: 140 [ 950/1251 ( 76%)]  Loss: 3.306 (3.85)  Time: 0.708s, 1445.31/s  (0.695s, 1473.53/s)  LR: 8.729e-04  Data: 0.009 (0.012)
Train: 140 [1000/1251 ( 80%)]  Loss: 3.811 (3.85)  Time: 0.677s, 1512.29/s  (0.695s, 1473.54/s)  LR: 8.729e-04  Data: 0.011 (0.012)
Train: 140 [1050/1251 ( 84%)]  Loss: 3.985 (3.86)  Time: 0.706s, 1450.55/s  (0.695s, 1472.62/s)  LR: 8.729e-04  Data: 0.011 (0.012)
Train: 140 [1100/1251 ( 88%)]  Loss: 3.763 (3.85)  Time: 0.684s, 1496.68/s  (0.695s, 1473.14/s)  LR: 8.729e-04  Data: 0.011 (0.012)
Train: 140 [1150/1251 ( 92%)]  Loss: 4.058 (3.86)  Time: 0.711s, 1439.99/s  (0.695s, 1473.25/s)  LR: 8.729e-04  Data: 0.010 (0.012)
Train: 140 [1200/1251 ( 96%)]  Loss: 3.734 (3.86)  Time: 0.685s, 1495.90/s  (0.695s, 1473.23/s)  LR: 8.729e-04  Data: 0.014 (0.012)
Train: 140 [1250/1251 (100%)]  Loss: 4.221 (3.87)  Time: 0.660s, 1552.59/s  (0.695s, 1473.75/s)  LR: 8.729e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.560 (1.560)  Loss:  0.9106 (0.9106)  Acc@1: 87.8906 (87.8906)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.136 (0.588)  Loss:  0.9824 (1.5641)  Acc@1: 81.3679 (69.6940)  Acc@5: 94.4576 (89.5940)
Train: 141 [   0/1251 (  0%)]  Loss: 3.725 (3.72)  Time: 2.071s,  494.48/s  (2.071s,  494.48/s)  LR: 8.711e-04  Data: 1.437 (1.437)
Train: 141 [  50/1251 (  4%)]  Loss: 3.388 (3.56)  Time: 0.666s, 1537.05/s  (0.731s, 1399.90/s)  LR: 8.711e-04  Data: 0.009 (0.049)
Train: 141 [ 100/1251 (  8%)]  Loss: 4.023 (3.71)  Time: 0.698s, 1467.67/s  (0.710s, 1442.68/s)  LR: 8.711e-04  Data: 0.013 (0.030)
Train: 141 [ 150/1251 ( 12%)]  Loss: 3.752 (3.72)  Time: 0.672s, 1524.52/s  (0.707s, 1448.66/s)  LR: 8.711e-04  Data: 0.012 (0.024)
Train: 141 [ 200/1251 ( 16%)]  Loss: 3.808 (3.74)  Time: 0.671s, 1526.61/s  (0.703s, 1456.89/s)  LR: 8.711e-04  Data: 0.010 (0.020)
Train: 141 [ 250/1251 ( 20%)]  Loss: 4.183 (3.81)  Time: 0.671s, 1526.56/s  (0.701s, 1460.05/s)  LR: 8.711e-04  Data: 0.010 (0.018)
Train: 141 [ 300/1251 ( 24%)]  Loss: 3.463 (3.76)  Time: 0.757s, 1352.90/s  (0.701s, 1461.52/s)  LR: 8.711e-04  Data: 0.011 (0.017)
Train: 141 [ 350/1251 ( 28%)]  Loss: 4.114 (3.81)  Time: 0.706s, 1449.47/s  (0.699s, 1465.50/s)  LR: 8.711e-04  Data: 0.009 (0.016)
Train: 141 [ 400/1251 ( 32%)]  Loss: 3.769 (3.80)  Time: 0.678s, 1510.86/s  (0.697s, 1469.61/s)  LR: 8.711e-04  Data: 0.010 (0.015)
Train: 141 [ 450/1251 ( 36%)]  Loss: 3.484 (3.77)  Time: 0.753s, 1360.27/s  (0.697s, 1468.78/s)  LR: 8.711e-04  Data: 0.009 (0.015)
Train: 141 [ 500/1251 ( 40%)]  Loss: 3.942 (3.79)  Time: 0.716s, 1430.07/s  (0.697s, 1469.24/s)  LR: 8.711e-04  Data: 0.010 (0.014)
Train: 141 [ 550/1251 ( 44%)]  Loss: 3.608 (3.77)  Time: 0.673s, 1521.04/s  (0.697s, 1469.97/s)  LR: 8.711e-04  Data: 0.010 (0.014)
Train: 141 [ 600/1251 ( 48%)]  Loss: 3.943 (3.78)  Time: 0.762s, 1343.79/s  (0.696s, 1470.81/s)  LR: 8.711e-04  Data: 0.011 (0.014)
Train: 141 [ 650/1251 ( 52%)]  Loss: 3.755 (3.78)  Time: 0.671s, 1527.11/s  (0.696s, 1471.08/s)  LR: 8.711e-04  Data: 0.010 (0.013)
Train: 141 [ 700/1251 ( 56%)]  Loss: 3.915 (3.79)  Time: 0.673s, 1520.45/s  (0.696s, 1471.21/s)  LR: 8.711e-04  Data: 0.010 (0.013)
Train: 141 [ 750/1251 ( 60%)]  Loss: 3.912 (3.80)  Time: 0.674s, 1519.19/s  (0.696s, 1471.86/s)  LR: 8.711e-04  Data: 0.011 (0.013)
Train: 141 [ 800/1251 ( 64%)]  Loss: 3.807 (3.80)  Time: 0.716s, 1431.00/s  (0.696s, 1471.13/s)  LR: 8.711e-04  Data: 0.010 (0.013)
Train: 141 [ 850/1251 ( 68%)]  Loss: 3.240 (3.77)  Time: 0.719s, 1424.85/s  (0.697s, 1470.02/s)  LR: 8.711e-04  Data: 0.010 (0.013)
Train: 141 [ 900/1251 ( 72%)]  Loss: 3.820 (3.77)  Time: 0.697s, 1468.52/s  (0.697s, 1469.75/s)  LR: 8.711e-04  Data: 0.010 (0.013)
Train: 141 [ 950/1251 ( 76%)]  Loss: 4.187 (3.79)  Time: 0.693s, 1478.42/s  (0.696s, 1470.29/s)  LR: 8.711e-04  Data: 0.010 (0.012)
Train: 141 [1000/1251 ( 80%)]  Loss: 3.451 (3.78)  Time: 0.686s, 1492.65/s  (0.696s, 1471.16/s)  LR: 8.711e-04  Data: 0.010 (0.012)
Train: 141 [1050/1251 ( 84%)]  Loss: 4.077 (3.79)  Time: 0.703s, 1455.61/s  (0.696s, 1471.96/s)  LR: 8.711e-04  Data: 0.009 (0.012)
Train: 141 [1100/1251 ( 88%)]  Loss: 3.716 (3.79)  Time: 0.674s, 1518.38/s  (0.695s, 1472.46/s)  LR: 8.711e-04  Data: 0.010 (0.012)
Train: 141 [1150/1251 ( 92%)]  Loss: 4.122 (3.80)  Time: 0.672s, 1523.64/s  (0.696s, 1472.22/s)  LR: 8.711e-04  Data: 0.010 (0.012)
Train: 141 [1200/1251 ( 96%)]  Loss: 4.219 (3.82)  Time: 0.706s, 1451.01/s  (0.696s, 1471.75/s)  LR: 8.711e-04  Data: 0.009 (0.012)
Train: 141 [1250/1251 (100%)]  Loss: 4.056 (3.83)  Time: 0.670s, 1528.87/s  (0.696s, 1471.49/s)  LR: 8.711e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.557 (1.557)  Loss:  1.0469 (1.0469)  Acc@1: 87.6953 (87.6953)  Acc@5: 96.2891 (96.2891)
Test: [  48/48]  Time: 0.136 (0.587)  Loss:  0.9346 (1.5739)  Acc@1: 83.4906 (70.2820)  Acc@5: 95.5189 (89.6960)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-138.pth.tar', 70.33200009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-141.pth.tar', 70.28199993896484)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-139.pth.tar', 70.26399997070313)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-131.pth.tar', 70.21000004638672)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-133.pth.tar', 70.18999999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-124.pth.tar', 70.14199996826171)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-137.pth.tar', 70.01800001708985)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-130.pth.tar', 69.97599997070313)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-111.pth.tar', 69.95999996826171)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-120.pth.tar', 69.92200004394532)

Train: 142 [   0/1251 (  0%)]  Loss: 3.963 (3.96)  Time: 2.111s,  485.13/s  (2.111s,  485.13/s)  LR: 8.694e-04  Data: 1.496 (1.496)
Train: 142 [  50/1251 (  4%)]  Loss: 3.533 (3.75)  Time: 0.676s, 1514.73/s  (0.737s, 1388.66/s)  LR: 8.694e-04  Data: 0.010 (0.052)
Train: 142 [ 100/1251 (  8%)]  Loss: 4.039 (3.84)  Time: 0.690s, 1484.76/s  (0.715s, 1433.02/s)  LR: 8.694e-04  Data: 0.009 (0.031)
Train: 142 [ 150/1251 ( 12%)]  Loss: 3.657 (3.80)  Time: 0.693s, 1477.29/s  (0.710s, 1442.00/s)  LR: 8.694e-04  Data: 0.010 (0.025)
Train: 142 [ 200/1251 ( 16%)]  Loss: 4.178 (3.87)  Time: 0.707s, 1448.82/s  (0.708s, 1446.73/s)  LR: 8.694e-04  Data: 0.011 (0.021)
Train: 142 [ 250/1251 ( 20%)]  Loss: 3.823 (3.87)  Time: 0.707s, 1448.06/s  (0.705s, 1452.06/s)  LR: 8.694e-04  Data: 0.011 (0.019)
Train: 142 [ 300/1251 ( 24%)]  Loss: 3.884 (3.87)  Time: 0.688s, 1487.74/s  (0.703s, 1456.25/s)  LR: 8.694e-04  Data: 0.009 (0.018)
Train: 142 [ 350/1251 ( 28%)]  Loss: 4.277 (3.92)  Time: 0.672s, 1523.51/s  (0.701s, 1460.47/s)  LR: 8.694e-04  Data: 0.010 (0.016)
Train: 142 [ 400/1251 ( 32%)]  Loss: 3.822 (3.91)  Time: 0.705s, 1452.43/s  (0.700s, 1462.75/s)  LR: 8.694e-04  Data: 0.010 (0.016)
Train: 142 [ 450/1251 ( 36%)]  Loss: 4.153 (3.93)  Time: 0.683s, 1499.14/s  (0.700s, 1462.85/s)  LR: 8.694e-04  Data: 0.013 (0.015)
Train: 142 [ 500/1251 ( 40%)]  Loss: 3.533 (3.90)  Time: 0.715s, 1431.60/s  (0.700s, 1463.82/s)  LR: 8.694e-04  Data: 0.010 (0.015)
Train: 142 [ 550/1251 ( 44%)]  Loss: 3.093 (3.83)  Time: 0.676s, 1514.28/s  (0.699s, 1465.74/s)  LR: 8.694e-04  Data: 0.009 (0.014)
Train: 142 [ 600/1251 ( 48%)]  Loss: 4.202 (3.86)  Time: 0.703s, 1455.82/s  (0.698s, 1466.44/s)  LR: 8.694e-04  Data: 0.009 (0.014)
Train: 142 [ 650/1251 ( 52%)]  Loss: 3.933 (3.86)  Time: 0.691s, 1481.14/s  (0.698s, 1466.70/s)  LR: 8.694e-04  Data: 0.009 (0.014)
Train: 142 [ 700/1251 ( 56%)]  Loss: 3.836 (3.86)  Time: 0.709s, 1443.36/s  (0.698s, 1467.16/s)  LR: 8.694e-04  Data: 0.010 (0.013)
Train: 142 [ 750/1251 ( 60%)]  Loss: 4.025 (3.87)  Time: 0.673s, 1521.99/s  (0.697s, 1469.02/s)  LR: 8.694e-04  Data: 0.011 (0.013)
Train: 142 [ 800/1251 ( 64%)]  Loss: 3.982 (3.88)  Time: 0.685s, 1495.29/s  (0.697s, 1468.31/s)  LR: 8.694e-04  Data: 0.011 (0.013)
Train: 142 [ 850/1251 ( 68%)]  Loss: 4.036 (3.89)  Time: 0.672s, 1523.87/s  (0.697s, 1469.20/s)  LR: 8.694e-04  Data: 0.010 (0.013)
Train: 142 [ 900/1251 ( 72%)]  Loss: 3.581 (3.87)  Time: 0.695s, 1474.18/s  (0.696s, 1470.34/s)  LR: 8.694e-04  Data: 0.010 (0.013)
Train: 142 [ 950/1251 ( 76%)]  Loss: 4.101 (3.88)  Time: 0.721s, 1420.74/s  (0.696s, 1470.42/s)  LR: 8.694e-04  Data: 0.011 (0.013)
Train: 142 [1000/1251 ( 80%)]  Loss: 3.779 (3.88)  Time: 0.735s, 1393.77/s  (0.698s, 1468.09/s)  LR: 8.694e-04  Data: 0.010 (0.012)
Train: 142 [1050/1251 ( 84%)]  Loss: 4.052 (3.89)  Time: 0.683s, 1500.20/s  (0.699s, 1465.80/s)  LR: 8.694e-04  Data: 0.011 (0.012)
Train: 142 [1100/1251 ( 88%)]  Loss: 4.298 (3.90)  Time: 0.743s, 1378.04/s  (0.699s, 1464.75/s)  LR: 8.694e-04  Data: 0.015 (0.012)
Train: 142 [1150/1251 ( 92%)]  Loss: 3.923 (3.90)  Time: 0.709s, 1444.87/s  (0.699s, 1464.76/s)  LR: 8.694e-04  Data: 0.009 (0.012)
Train: 142 [1200/1251 ( 96%)]  Loss: 3.592 (3.89)  Time: 0.705s, 1453.20/s  (0.699s, 1464.91/s)  LR: 8.694e-04  Data: 0.010 (0.012)
Train: 142 [1250/1251 (100%)]  Loss: 3.639 (3.88)  Time: 0.694s, 1476.21/s  (0.698s, 1466.35/s)  LR: 8.694e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.547 (1.547)  Loss:  0.8184 (0.8184)  Acc@1: 87.5977 (87.5977)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.136 (0.563)  Loss:  0.9062 (1.5134)  Acc@1: 83.7264 (70.6500)  Acc@5: 95.6368 (89.9880)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-142.pth.tar', 70.64999991210938)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-138.pth.tar', 70.33200009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-141.pth.tar', 70.28199993896484)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-139.pth.tar', 70.26399997070313)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-131.pth.tar', 70.21000004638672)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-133.pth.tar', 70.18999999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-124.pth.tar', 70.14199996826171)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-137.pth.tar', 70.01800001708985)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-130.pth.tar', 69.97599997070313)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-111.pth.tar', 69.95999996826171)

Train: 143 [   0/1251 (  0%)]  Loss: 4.149 (4.15)  Time: 2.154s,  475.38/s  (2.154s,  475.38/s)  LR: 8.676e-04  Data: 1.487 (1.487)
Train: 143 [  50/1251 (  4%)]  Loss: 3.827 (3.99)  Time: 0.677s, 1511.71/s  (0.721s, 1419.30/s)  LR: 8.676e-04  Data: 0.010 (0.044)
Train: 143 [ 100/1251 (  8%)]  Loss: 3.759 (3.91)  Time: 0.680s, 1505.53/s  (0.704s, 1453.87/s)  LR: 8.676e-04  Data: 0.009 (0.027)
Train: 143 [ 150/1251 ( 12%)]  Loss: 4.162 (3.97)  Time: 0.710s, 1442.33/s  (0.699s, 1465.56/s)  LR: 8.676e-04  Data: 0.009 (0.021)
Train: 143 [ 200/1251 ( 16%)]  Loss: 4.410 (4.06)  Time: 0.705s, 1452.26/s  (0.696s, 1471.92/s)  LR: 8.676e-04  Data: 0.010 (0.019)
Train: 143 [ 250/1251 ( 20%)]  Loss: 3.657 (3.99)  Time: 0.697s, 1469.92/s  (0.696s, 1470.42/s)  LR: 8.676e-04  Data: 0.010 (0.017)
Train: 143 [ 300/1251 ( 24%)]  Loss: 3.667 (3.95)  Time: 0.711s, 1439.43/s  (0.696s, 1471.81/s)  LR: 8.676e-04  Data: 0.009 (0.016)
Train: 143 [ 350/1251 ( 28%)]  Loss: 3.923 (3.94)  Time: 0.670s, 1528.53/s  (0.696s, 1470.71/s)  LR: 8.676e-04  Data: 0.010 (0.015)
Train: 143 [ 400/1251 ( 32%)]  Loss: 3.571 (3.90)  Time: 0.674s, 1518.44/s  (0.695s, 1472.62/s)  LR: 8.676e-04  Data: 0.010 (0.014)
Train: 143 [ 450/1251 ( 36%)]  Loss: 3.959 (3.91)  Time: 0.673s, 1521.02/s  (0.695s, 1473.53/s)  LR: 8.676e-04  Data: 0.010 (0.014)
Train: 143 [ 500/1251 ( 40%)]  Loss: 3.911 (3.91)  Time: 0.675s, 1518.11/s  (0.695s, 1473.66/s)  LR: 8.676e-04  Data: 0.010 (0.014)
Train: 143 [ 550/1251 ( 44%)]  Loss: 3.816 (3.90)  Time: 0.707s, 1449.21/s  (0.694s, 1475.37/s)  LR: 8.676e-04  Data: 0.009 (0.013)
Train: 143 [ 600/1251 ( 48%)]  Loss: 3.955 (3.91)  Time: 0.701s, 1459.94/s  (0.693s, 1476.69/s)  LR: 8.676e-04  Data: 0.010 (0.013)
Train: 143 [ 650/1251 ( 52%)]  Loss: 3.983 (3.91)  Time: 0.674s, 1519.26/s  (0.693s, 1477.78/s)  LR: 8.676e-04  Data: 0.010 (0.013)
Train: 143 [ 700/1251 ( 56%)]  Loss: 3.675 (3.90)  Time: 0.762s, 1344.59/s  (0.693s, 1478.18/s)  LR: 8.676e-04  Data: 0.009 (0.013)
Train: 143 [ 750/1251 ( 60%)]  Loss: 3.920 (3.90)  Time: 0.672s, 1523.10/s  (0.693s, 1478.23/s)  LR: 8.676e-04  Data: 0.009 (0.012)
Train: 143 [ 800/1251 ( 64%)]  Loss: 3.705 (3.89)  Time: 0.671s, 1525.26/s  (0.693s, 1478.31/s)  LR: 8.676e-04  Data: 0.009 (0.012)
Train: 143 [ 850/1251 ( 68%)]  Loss: 4.053 (3.89)  Time: 0.672s, 1524.24/s  (0.692s, 1479.11/s)  LR: 8.676e-04  Data: 0.010 (0.012)
Train: 143 [ 900/1251 ( 72%)]  Loss: 3.806 (3.89)  Time: 0.679s, 1507.99/s  (0.692s, 1479.02/s)  LR: 8.676e-04  Data: 0.012 (0.012)
Train: 143 [ 950/1251 ( 76%)]  Loss: 3.799 (3.89)  Time: 0.706s, 1449.44/s  (0.693s, 1478.01/s)  LR: 8.676e-04  Data: 0.011 (0.012)
Train: 143 [1000/1251 ( 80%)]  Loss: 4.341 (3.91)  Time: 0.705s, 1452.20/s  (0.693s, 1477.96/s)  LR: 8.676e-04  Data: 0.010 (0.012)
Train: 143 [1050/1251 ( 84%)]  Loss: 3.846 (3.90)  Time: 0.707s, 1449.30/s  (0.693s, 1478.20/s)  LR: 8.676e-04  Data: 0.012 (0.012)
Train: 143 [1100/1251 ( 88%)]  Loss: 4.097 (3.91)  Time: 0.673s, 1520.88/s  (0.693s, 1478.55/s)  LR: 8.676e-04  Data: 0.010 (0.012)
Train: 143 [1150/1251 ( 92%)]  Loss: 4.174 (3.92)  Time: 0.676s, 1513.99/s  (0.693s, 1478.33/s)  LR: 8.676e-04  Data: 0.010 (0.012)
Train: 143 [1200/1251 ( 96%)]  Loss: 3.578 (3.91)  Time: 0.677s, 1511.73/s  (0.693s, 1477.71/s)  LR: 8.676e-04  Data: 0.011 (0.012)
Train: 143 [1250/1251 (100%)]  Loss: 3.802 (3.91)  Time: 0.671s, 1527.12/s  (0.693s, 1477.63/s)  LR: 8.676e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.532 (1.532)  Loss:  1.0742 (1.0742)  Acc@1: 86.2305 (86.2305)  Acc@5: 96.0938 (96.0938)
Test: [  48/48]  Time: 0.137 (0.574)  Loss:  1.1270 (1.6078)  Acc@1: 83.1368 (68.9020)  Acc@5: 95.2830 (88.9860)
Train: 144 [   0/1251 (  0%)]  Loss: 3.706 (3.71)  Time: 2.410s,  424.88/s  (2.410s,  424.88/s)  LR: 8.658e-04  Data: 1.748 (1.748)
Train: 144 [  50/1251 (  4%)]  Loss: 3.663 (3.68)  Time: 0.672s, 1524.67/s  (0.726s, 1410.31/s)  LR: 8.658e-04  Data: 0.009 (0.048)
Train: 144 [ 100/1251 (  8%)]  Loss: 3.749 (3.71)  Time: 0.667s, 1535.73/s  (0.712s, 1437.48/s)  LR: 8.658e-04  Data: 0.010 (0.029)
Train: 144 [ 150/1251 ( 12%)]  Loss: 4.252 (3.84)  Time: 0.693s, 1476.95/s  (0.706s, 1450.61/s)  LR: 8.658e-04  Data: 0.009 (0.023)
Train: 144 [ 200/1251 ( 16%)]  Loss: 3.757 (3.83)  Time: 0.748s, 1368.09/s  (0.702s, 1458.68/s)  LR: 8.658e-04  Data: 0.010 (0.020)
Train: 144 [ 250/1251 ( 20%)]  Loss: 4.149 (3.88)  Time: 0.676s, 1515.83/s  (0.701s, 1461.28/s)  LR: 8.658e-04  Data: 0.011 (0.018)
Train: 144 [ 300/1251 ( 24%)]  Loss: 3.972 (3.89)  Time: 0.723s, 1416.36/s  (0.700s, 1463.59/s)  LR: 8.658e-04  Data: 0.012 (0.017)
Train: 144 [ 350/1251 ( 28%)]  Loss: 3.659 (3.86)  Time: 0.674s, 1518.56/s  (0.700s, 1463.37/s)  LR: 8.658e-04  Data: 0.011 (0.016)
Train: 144 [ 400/1251 ( 32%)]  Loss: 3.768 (3.85)  Time: 0.707s, 1447.72/s  (0.699s, 1465.72/s)  LR: 8.658e-04  Data: 0.010 (0.015)
Train: 144 [ 450/1251 ( 36%)]  Loss: 4.137 (3.88)  Time: 0.703s, 1457.40/s  (0.698s, 1466.27/s)  LR: 8.658e-04  Data: 0.009 (0.014)
Train: 144 [ 500/1251 ( 40%)]  Loss: 3.923 (3.89)  Time: 0.707s, 1449.22/s  (0.698s, 1467.96/s)  LR: 8.658e-04  Data: 0.011 (0.014)
Train: 144 [ 550/1251 ( 44%)]  Loss: 3.578 (3.86)  Time: 0.669s, 1530.74/s  (0.697s, 1469.45/s)  LR: 8.658e-04  Data: 0.010 (0.014)
Train: 144 [ 600/1251 ( 48%)]  Loss: 4.212 (3.89)  Time: 0.698s, 1466.70/s  (0.696s, 1471.24/s)  LR: 8.658e-04  Data: 0.010 (0.013)
Train: 144 [ 650/1251 ( 52%)]  Loss: 3.362 (3.85)  Time: 0.673s, 1520.97/s  (0.695s, 1472.91/s)  LR: 8.658e-04  Data: 0.011 (0.013)
Train: 144 [ 700/1251 ( 56%)]  Loss: 3.729 (3.84)  Time: 0.672s, 1524.30/s  (0.695s, 1472.36/s)  LR: 8.658e-04  Data: 0.010 (0.013)
Train: 144 [ 750/1251 ( 60%)]  Loss: 3.418 (3.81)  Time: 0.694s, 1475.56/s  (0.695s, 1473.16/s)  LR: 8.658e-04  Data: 0.010 (0.013)
Train: 144 [ 800/1251 ( 64%)]  Loss: 4.133 (3.83)  Time: 0.715s, 1431.83/s  (0.695s, 1474.34/s)  LR: 8.658e-04  Data: 0.011 (0.013)
Train: 144 [ 850/1251 ( 68%)]  Loss: 3.947 (3.84)  Time: 0.672s, 1523.26/s  (0.694s, 1474.51/s)  LR: 8.658e-04  Data: 0.010 (0.012)
Train: 144 [ 900/1251 ( 72%)]  Loss: 3.880 (3.84)  Time: 0.699s, 1464.12/s  (0.694s, 1474.67/s)  LR: 8.658e-04  Data: 0.008 (0.012)
Train: 144 [ 950/1251 ( 76%)]  Loss: 3.988 (3.85)  Time: 0.697s, 1468.33/s  (0.694s, 1474.81/s)  LR: 8.658e-04  Data: 0.009 (0.012)
Train: 144 [1000/1251 ( 80%)]  Loss: 3.958 (3.85)  Time: 0.673s, 1522.44/s  (0.694s, 1475.04/s)  LR: 8.658e-04  Data: 0.010 (0.012)
Train: 144 [1050/1251 ( 84%)]  Loss: 3.792 (3.85)  Time: 0.672s, 1524.65/s  (0.694s, 1475.40/s)  LR: 8.658e-04  Data: 0.010 (0.012)
Train: 144 [1100/1251 ( 88%)]  Loss: 3.755 (3.85)  Time: 0.666s, 1536.52/s  (0.694s, 1475.91/s)  LR: 8.658e-04  Data: 0.011 (0.012)
Train: 144 [1150/1251 ( 92%)]  Loss: 4.090 (3.86)  Time: 0.688s, 1488.13/s  (0.694s, 1475.50/s)  LR: 8.658e-04  Data: 0.009 (0.012)
Train: 144 [1200/1251 ( 96%)]  Loss: 3.952 (3.86)  Time: 0.671s, 1526.62/s  (0.694s, 1475.36/s)  LR: 8.658e-04  Data: 0.010 (0.012)
Train: 144 [1250/1251 (100%)]  Loss: 3.885 (3.86)  Time: 0.721s, 1419.29/s  (0.694s, 1475.95/s)  LR: 8.658e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.584 (1.584)  Loss:  0.9946 (0.9946)  Acc@1: 85.4492 (85.4492)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.136 (0.579)  Loss:  0.9927 (1.5457)  Acc@1: 82.5472 (70.0060)  Acc@5: 95.4009 (89.6260)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-142.pth.tar', 70.64999991210938)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-138.pth.tar', 70.33200009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-141.pth.tar', 70.28199993896484)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-139.pth.tar', 70.26399997070313)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-131.pth.tar', 70.21000004638672)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-133.pth.tar', 70.18999999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-124.pth.tar', 70.14199996826171)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-137.pth.tar', 70.01800001708985)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-144.pth.tar', 70.00600004638672)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-130.pth.tar', 69.97599997070313)

Train: 145 [   0/1251 (  0%)]  Loss: 3.795 (3.80)  Time: 2.390s,  428.45/s  (2.390s,  428.45/s)  LR: 8.641e-04  Data: 1.759 (1.759)
Train: 145 [  50/1251 (  4%)]  Loss: 3.629 (3.71)  Time: 0.694s, 1475.99/s  (0.733s, 1396.56/s)  LR: 8.641e-04  Data: 0.010 (0.053)
Train: 145 [ 100/1251 (  8%)]  Loss: 3.755 (3.73)  Time: 0.676s, 1513.75/s  (0.711s, 1440.23/s)  LR: 8.641e-04  Data: 0.010 (0.032)
Train: 145 [ 150/1251 ( 12%)]  Loss: 3.740 (3.73)  Time: 0.677s, 1511.55/s  (0.703s, 1455.94/s)  LR: 8.641e-04  Data: 0.012 (0.025)
Train: 145 [ 200/1251 ( 16%)]  Loss: 3.420 (3.67)  Time: 0.669s, 1529.71/s  (0.702s, 1459.51/s)  LR: 8.641e-04  Data: 0.011 (0.021)
Train: 145 [ 250/1251 ( 20%)]  Loss: 3.903 (3.71)  Time: 0.672s, 1524.30/s  (0.699s, 1464.48/s)  LR: 8.641e-04  Data: 0.009 (0.019)
Train: 145 [ 300/1251 ( 24%)]  Loss: 4.285 (3.79)  Time: 0.786s, 1302.06/s  (0.699s, 1464.80/s)  LR: 8.641e-04  Data: 0.015 (0.017)
Train: 145 [ 350/1251 ( 28%)]  Loss: 3.908 (3.80)  Time: 0.719s, 1423.87/s  (0.698s, 1466.39/s)  LR: 8.641e-04  Data: 0.009 (0.016)
Train: 145 [ 400/1251 ( 32%)]  Loss: 3.903 (3.82)  Time: 0.701s, 1461.51/s  (0.699s, 1465.53/s)  LR: 8.641e-04  Data: 0.009 (0.016)
Train: 145 [ 450/1251 ( 36%)]  Loss: 3.837 (3.82)  Time: 0.693s, 1477.40/s  (0.698s, 1467.34/s)  LR: 8.641e-04  Data: 0.009 (0.015)
Train: 145 [ 500/1251 ( 40%)]  Loss: 3.919 (3.83)  Time: 0.708s, 1445.95/s  (0.699s, 1465.70/s)  LR: 8.641e-04  Data: 0.010 (0.015)
Train: 145 [ 550/1251 ( 44%)]  Loss: 3.697 (3.82)  Time: 0.703s, 1456.63/s  (0.699s, 1465.97/s)  LR: 8.641e-04  Data: 0.013 (0.014)
Train: 145 [ 600/1251 ( 48%)]  Loss: 3.743 (3.81)  Time: 0.798s, 1283.47/s  (0.698s, 1466.94/s)  LR: 8.641e-04  Data: 0.009 (0.014)
Train: 145 [ 650/1251 ( 52%)]  Loss: 4.149 (3.83)  Time: 0.712s, 1437.40/s  (0.698s, 1466.39/s)  LR: 8.641e-04  Data: 0.010 (0.014)
Train: 145 [ 700/1251 ( 56%)]  Loss: 3.912 (3.84)  Time: 0.709s, 1444.49/s  (0.698s, 1467.44/s)  LR: 8.641e-04  Data: 0.010 (0.013)
Train: 145 [ 750/1251 ( 60%)]  Loss: 4.024 (3.85)  Time: 0.683s, 1498.46/s  (0.697s, 1468.98/s)  LR: 8.641e-04  Data: 0.009 (0.013)
Train: 145 [ 800/1251 ( 64%)]  Loss: 3.664 (3.84)  Time: 0.755s, 1356.33/s  (0.697s, 1469.43/s)  LR: 8.641e-04  Data: 0.009 (0.013)
Train: 145 [ 850/1251 ( 68%)]  Loss: 3.893 (3.84)  Time: 0.725s, 1413.12/s  (0.696s, 1470.45/s)  LR: 8.641e-04  Data: 0.010 (0.013)
Train: 145 [ 900/1251 ( 72%)]  Loss: 3.899 (3.85)  Time: 0.673s, 1522.47/s  (0.696s, 1470.91/s)  LR: 8.641e-04  Data: 0.009 (0.013)
Train: 145 [ 950/1251 ( 76%)]  Loss: 4.265 (3.87)  Time: 0.672s, 1522.77/s  (0.696s, 1470.95/s)  LR: 8.641e-04  Data: 0.009 (0.012)
Train: 145 [1000/1251 ( 80%)]  Loss: 3.786 (3.86)  Time: 0.673s, 1522.52/s  (0.696s, 1470.84/s)  LR: 8.641e-04  Data: 0.012 (0.012)
Train: 145 [1050/1251 ( 84%)]  Loss: 3.765 (3.86)  Time: 0.672s, 1524.43/s  (0.696s, 1471.88/s)  LR: 8.641e-04  Data: 0.009 (0.012)
Train: 145 [1100/1251 ( 88%)]  Loss: 4.059 (3.87)  Time: 0.672s, 1524.15/s  (0.695s, 1472.35/s)  LR: 8.641e-04  Data: 0.010 (0.012)
Train: 145 [1150/1251 ( 92%)]  Loss: 4.226 (3.88)  Time: 0.672s, 1524.25/s  (0.696s, 1471.57/s)  LR: 8.641e-04  Data: 0.010 (0.012)
Train: 145 [1200/1251 ( 96%)]  Loss: 3.930 (3.88)  Time: 0.681s, 1503.81/s  (0.696s, 1471.65/s)  LR: 8.641e-04  Data: 0.010 (0.012)
Train: 145 [1250/1251 (100%)]  Loss: 4.164 (3.90)  Time: 0.656s, 1560.88/s  (0.696s, 1471.78/s)  LR: 8.641e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.617 (1.617)  Loss:  0.9771 (0.9771)  Acc@1: 85.2539 (85.2539)  Acc@5: 95.8984 (95.8984)
Test: [  48/48]  Time: 0.136 (0.586)  Loss:  0.9961 (1.5872)  Acc@1: 83.1368 (69.4280)  Acc@5: 94.9293 (89.3480)
Train: 146 [   0/1251 (  0%)]  Loss: 3.232 (3.23)  Time: 2.184s,  468.88/s  (2.184s,  468.88/s)  LR: 8.623e-04  Data: 1.565 (1.565)
Train: 146 [  50/1251 (  4%)]  Loss: 4.126 (3.68)  Time: 0.710s, 1441.78/s  (0.718s, 1425.88/s)  LR: 8.623e-04  Data: 0.010 (0.045)
Train: 146 [ 100/1251 (  8%)]  Loss: 3.969 (3.78)  Time: 0.730s, 1403.60/s  (0.710s, 1441.82/s)  LR: 8.623e-04  Data: 0.011 (0.028)
Train: 146 [ 150/1251 ( 12%)]  Loss: 3.880 (3.80)  Time: 0.709s, 1444.88/s  (0.704s, 1453.71/s)  LR: 8.623e-04  Data: 0.009 (0.022)
Train: 146 [ 200/1251 ( 16%)]  Loss: 4.136 (3.87)  Time: 0.693s, 1477.95/s  (0.701s, 1460.38/s)  LR: 8.623e-04  Data: 0.009 (0.019)
Train: 146 [ 250/1251 ( 20%)]  Loss: 4.164 (3.92)  Time: 0.705s, 1452.20/s  (0.700s, 1462.95/s)  LR: 8.623e-04  Data: 0.008 (0.017)
Train: 146 [ 300/1251 ( 24%)]  Loss: 3.802 (3.90)  Time: 0.702s, 1458.94/s  (0.701s, 1460.04/s)  LR: 8.623e-04  Data: 0.010 (0.016)
Train: 146 [ 350/1251 ( 28%)]  Loss: 3.768 (3.88)  Time: 0.723s, 1416.12/s  (0.702s, 1458.98/s)  LR: 8.623e-04  Data: 0.010 (0.015)
Train: 146 [ 400/1251 ( 32%)]  Loss: 3.774 (3.87)  Time: 0.676s, 1514.76/s  (0.700s, 1461.84/s)  LR: 8.623e-04  Data: 0.009 (0.015)
Train: 146 [ 450/1251 ( 36%)]  Loss: 3.825 (3.87)  Time: 0.683s, 1498.89/s  (0.699s, 1464.28/s)  LR: 8.623e-04  Data: 0.011 (0.014)
Train: 146 [ 500/1251 ( 40%)]  Loss: 3.942 (3.87)  Time: 0.687s, 1490.80/s  (0.699s, 1465.63/s)  LR: 8.623e-04  Data: 0.010 (0.014)
Train: 146 [ 550/1251 ( 44%)]  Loss: 4.139 (3.90)  Time: 0.672s, 1523.97/s  (0.698s, 1466.25/s)  LR: 8.623e-04  Data: 0.010 (0.013)
Train: 146 [ 600/1251 ( 48%)]  Loss: 3.786 (3.89)  Time: 0.672s, 1524.51/s  (0.699s, 1465.67/s)  LR: 8.623e-04  Data: 0.012 (0.013)
Train: 146 [ 650/1251 ( 52%)]  Loss: 4.226 (3.91)  Time: 0.687s, 1491.02/s  (0.698s, 1466.34/s)  LR: 8.623e-04  Data: 0.011 (0.013)
Train: 146 [ 700/1251 ( 56%)]  Loss: 3.601 (3.89)  Time: 0.673s, 1522.30/s  (0.698s, 1467.61/s)  LR: 8.623e-04  Data: 0.010 (0.013)
Train: 146 [ 750/1251 ( 60%)]  Loss: 3.356 (3.86)  Time: 0.674s, 1520.31/s  (0.697s, 1468.98/s)  LR: 8.623e-04  Data: 0.012 (0.013)
Train: 146 [ 800/1251 ( 64%)]  Loss: 3.631 (3.84)  Time: 0.767s, 1334.50/s  (0.697s, 1469.45/s)  LR: 8.623e-04  Data: 0.010 (0.012)
Train: 146 [ 850/1251 ( 68%)]  Loss: 3.849 (3.84)  Time: 0.671s, 1526.42/s  (0.697s, 1469.74/s)  LR: 8.623e-04  Data: 0.011 (0.012)
Train: 146 [ 900/1251 ( 72%)]  Loss: 3.357 (3.82)  Time: 0.675s, 1516.82/s  (0.696s, 1470.39/s)  LR: 8.623e-04  Data: 0.010 (0.012)
Train: 146 [ 950/1251 ( 76%)]  Loss: 4.127 (3.83)  Time: 0.743s, 1377.67/s  (0.696s, 1470.28/s)  LR: 8.623e-04  Data: 0.008 (0.012)
Train: 146 [1000/1251 ( 80%)]  Loss: 3.543 (3.82)  Time: 0.672s, 1524.79/s  (0.696s, 1470.33/s)  LR: 8.623e-04  Data: 0.010 (0.012)
Train: 146 [1050/1251 ( 84%)]  Loss: 4.024 (3.83)  Time: 0.735s, 1393.60/s  (0.696s, 1470.50/s)  LR: 8.623e-04  Data: 0.010 (0.012)
Train: 146 [1100/1251 ( 88%)]  Loss: 3.697 (3.82)  Time: 0.673s, 1522.37/s  (0.696s, 1471.36/s)  LR: 8.623e-04  Data: 0.009 (0.012)
Train: 146 [1150/1251 ( 92%)]  Loss: 4.088 (3.84)  Time: 0.672s, 1524.10/s  (0.696s, 1471.48/s)  LR: 8.623e-04  Data: 0.010 (0.012)
Train: 146 [1200/1251 ( 96%)]  Loss: 3.986 (3.84)  Time: 0.691s, 1480.86/s  (0.695s, 1472.51/s)  LR: 8.623e-04  Data: 0.009 (0.012)
Train: 146 [1250/1251 (100%)]  Loss: 4.071 (3.85)  Time: 0.700s, 1463.21/s  (0.695s, 1473.44/s)  LR: 8.623e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.550 (1.550)  Loss:  0.8999 (0.8999)  Acc@1: 85.4492 (85.4492)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.136 (0.589)  Loss:  1.1436 (1.5490)  Acc@1: 82.7830 (70.3000)  Acc@5: 94.6934 (89.7640)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-142.pth.tar', 70.64999991210938)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-138.pth.tar', 70.33200009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-146.pth.tar', 70.30000001953125)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-141.pth.tar', 70.28199993896484)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-139.pth.tar', 70.26399997070313)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-131.pth.tar', 70.21000004638672)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-133.pth.tar', 70.18999999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-124.pth.tar', 70.14199996826171)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-137.pth.tar', 70.01800001708985)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-144.pth.tar', 70.00600004638672)

Train: 147 [   0/1251 (  0%)]  Loss: 3.629 (3.63)  Time: 2.131s,  480.48/s  (2.131s,  480.48/s)  LR: 8.605e-04  Data: 1.517 (1.517)
Train: 147 [  50/1251 (  4%)]  Loss: 3.674 (3.65)  Time: 0.673s, 1522.67/s  (0.728s, 1407.27/s)  LR: 8.605e-04  Data: 0.010 (0.047)
Train: 147 [ 100/1251 (  8%)]  Loss: 3.521 (3.61)  Time: 0.671s, 1525.08/s  (0.709s, 1443.51/s)  LR: 8.605e-04  Data: 0.009 (0.029)
Train: 147 [ 150/1251 ( 12%)]  Loss: 3.190 (3.50)  Time: 0.791s, 1294.39/s  (0.705s, 1451.89/s)  LR: 8.605e-04  Data: 0.009 (0.022)
Train: 147 [ 200/1251 ( 16%)]  Loss: 4.279 (3.66)  Time: 0.670s, 1528.01/s  (0.702s, 1459.21/s)  LR: 8.605e-04  Data: 0.009 (0.019)
Train: 147 [ 250/1251 ( 20%)]  Loss: 3.704 (3.67)  Time: 0.731s, 1400.27/s  (0.699s, 1464.57/s)  LR: 8.605e-04  Data: 0.009 (0.017)
Train: 147 [ 300/1251 ( 24%)]  Loss: 3.589 (3.66)  Time: 0.708s, 1445.51/s  (0.698s, 1466.33/s)  LR: 8.605e-04  Data: 0.010 (0.016)
Train: 147 [ 350/1251 ( 28%)]  Loss: 4.120 (3.71)  Time: 0.673s, 1520.87/s  (0.699s, 1464.72/s)  LR: 8.605e-04  Data: 0.009 (0.015)
Train: 147 [ 400/1251 ( 32%)]  Loss: 3.880 (3.73)  Time: 0.691s, 1482.45/s  (0.699s, 1465.15/s)  LR: 8.605e-04  Data: 0.009 (0.015)
Train: 147 [ 450/1251 ( 36%)]  Loss: 3.622 (3.72)  Time: 0.706s, 1450.78/s  (0.698s, 1467.02/s)  LR: 8.605e-04  Data: 0.009 (0.014)
Train: 147 [ 500/1251 ( 40%)]  Loss: 3.722 (3.72)  Time: 0.672s, 1523.83/s  (0.698s, 1467.04/s)  LR: 8.605e-04  Data: 0.009 (0.014)
Train: 147 [ 550/1251 ( 44%)]  Loss: 3.777 (3.73)  Time: 0.672s, 1524.34/s  (0.698s, 1467.76/s)  LR: 8.605e-04  Data: 0.009 (0.013)
Train: 147 [ 600/1251 ( 48%)]  Loss: 3.670 (3.72)  Time: 0.700s, 1462.82/s  (0.697s, 1469.56/s)  LR: 8.605e-04  Data: 0.009 (0.013)
Train: 147 [ 650/1251 ( 52%)]  Loss: 3.936 (3.74)  Time: 0.709s, 1444.23/s  (0.697s, 1469.78/s)  LR: 8.605e-04  Data: 0.009 (0.013)
Train: 147 [ 700/1251 ( 56%)]  Loss: 3.438 (3.72)  Time: 0.702s, 1459.31/s  (0.697s, 1470.19/s)  LR: 8.605e-04  Data: 0.009 (0.013)
Train: 147 [ 750/1251 ( 60%)]  Loss: 3.657 (3.71)  Time: 0.701s, 1461.49/s  (0.696s, 1470.38/s)  LR: 8.605e-04  Data: 0.009 (0.013)
Train: 147 [ 800/1251 ( 64%)]  Loss: 3.608 (3.71)  Time: 0.691s, 1481.59/s  (0.696s, 1471.37/s)  LR: 8.605e-04  Data: 0.009 (0.012)
Train: 147 [ 850/1251 ( 68%)]  Loss: 3.776 (3.71)  Time: 0.684s, 1496.06/s  (0.695s, 1472.63/s)  LR: 8.605e-04  Data: 0.009 (0.012)
Train: 147 [ 900/1251 ( 72%)]  Loss: 4.310 (3.74)  Time: 0.672s, 1524.27/s  (0.695s, 1472.63/s)  LR: 8.605e-04  Data: 0.010 (0.012)
Train: 147 [ 950/1251 ( 76%)]  Loss: 3.868 (3.75)  Time: 0.699s, 1464.50/s  (0.695s, 1472.46/s)  LR: 8.605e-04  Data: 0.010 (0.012)
Train: 147 [1000/1251 ( 80%)]  Loss: 3.898 (3.76)  Time: 0.671s, 1526.38/s  (0.695s, 1472.90/s)  LR: 8.605e-04  Data: 0.011 (0.012)
Train: 147 [1050/1251 ( 84%)]  Loss: 3.943 (3.76)  Time: 0.675s, 1516.69/s  (0.695s, 1473.40/s)  LR: 8.605e-04  Data: 0.011 (0.012)
Train: 147 [1100/1251 ( 88%)]  Loss: 3.355 (3.75)  Time: 0.672s, 1523.02/s  (0.695s, 1473.45/s)  LR: 8.605e-04  Data: 0.009 (0.012)
Train: 147 [1150/1251 ( 92%)]  Loss: 3.949 (3.75)  Time: 0.690s, 1484.54/s  (0.695s, 1473.52/s)  LR: 8.605e-04  Data: 0.012 (0.012)
Train: 147 [1200/1251 ( 96%)]  Loss: 3.969 (3.76)  Time: 0.673s, 1522.40/s  (0.695s, 1474.02/s)  LR: 8.605e-04  Data: 0.011 (0.012)
Train: 147 [1250/1251 (100%)]  Loss: 3.650 (3.76)  Time: 0.709s, 1443.57/s  (0.695s, 1473.27/s)  LR: 8.605e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.485 (1.485)  Loss:  0.9609 (0.9609)  Acc@1: 87.0117 (87.0117)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.137 (0.588)  Loss:  1.0059 (1.5621)  Acc@1: 82.6651 (70.4080)  Acc@5: 96.2264 (89.9460)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-142.pth.tar', 70.64999991210938)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-147.pth.tar', 70.40799996826172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-138.pth.tar', 70.33200009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-146.pth.tar', 70.30000001953125)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-141.pth.tar', 70.28199993896484)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-139.pth.tar', 70.26399997070313)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-131.pth.tar', 70.21000004638672)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-133.pth.tar', 70.18999999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-124.pth.tar', 70.14199996826171)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-137.pth.tar', 70.01800001708985)

Train: 148 [   0/1251 (  0%)]  Loss: 3.984 (3.98)  Time: 2.367s,  432.69/s  (2.367s,  432.69/s)  LR: 8.587e-04  Data: 1.753 (1.753)
Train: 148 [  50/1251 (  4%)]  Loss: 3.905 (3.94)  Time: 0.688s, 1488.38/s  (0.726s, 1410.59/s)  LR: 8.587e-04  Data: 0.011 (0.053)
Train: 148 [ 100/1251 (  8%)]  Loss: 4.037 (3.98)  Time: 0.699s, 1465.29/s  (0.707s, 1447.95/s)  LR: 8.587e-04  Data: 0.010 (0.032)
Train: 148 [ 150/1251 ( 12%)]  Loss: 4.208 (4.03)  Time: 0.682s, 1500.46/s  (0.703s, 1457.64/s)  LR: 8.587e-04  Data: 0.011 (0.024)
Train: 148 [ 200/1251 ( 16%)]  Loss: 3.738 (3.97)  Time: 0.714s, 1434.96/s  (0.700s, 1463.68/s)  LR: 8.587e-04  Data: 0.009 (0.021)
Train: 148 [ 250/1251 ( 20%)]  Loss: 3.772 (3.94)  Time: 0.759s, 1349.56/s  (0.699s, 1464.02/s)  LR: 8.587e-04  Data: 0.009 (0.019)
Train: 148 [ 300/1251 ( 24%)]  Loss: 3.828 (3.92)  Time: 0.712s, 1438.49/s  (0.699s, 1464.26/s)  LR: 8.587e-04  Data: 0.010 (0.017)
Train: 148 [ 350/1251 ( 28%)]  Loss: 3.863 (3.92)  Time: 0.706s, 1450.34/s  (0.699s, 1465.97/s)  LR: 8.587e-04  Data: 0.012 (0.016)
Train: 148 [ 400/1251 ( 32%)]  Loss: 3.537 (3.87)  Time: 0.700s, 1463.14/s  (0.697s, 1468.61/s)  LR: 8.587e-04  Data: 0.009 (0.016)
Train: 148 [ 450/1251 ( 36%)]  Loss: 3.936 (3.88)  Time: 0.675s, 1517.29/s  (0.696s, 1470.92/s)  LR: 8.587e-04  Data: 0.010 (0.015)
Train: 148 [ 500/1251 ( 40%)]  Loss: 3.795 (3.87)  Time: 0.712s, 1438.79/s  (0.697s, 1469.12/s)  LR: 8.587e-04  Data: 0.009 (0.015)
Train: 148 [ 550/1251 ( 44%)]  Loss: 3.857 (3.87)  Time: 0.670s, 1528.09/s  (0.697s, 1469.16/s)  LR: 8.587e-04  Data: 0.009 (0.014)
Train: 148 [ 600/1251 ( 48%)]  Loss: 3.557 (3.85)  Time: 0.676s, 1515.20/s  (0.697s, 1468.80/s)  LR: 8.587e-04  Data: 0.011 (0.014)
Train: 148 [ 650/1251 ( 52%)]  Loss: 3.941 (3.85)  Time: 0.692s, 1480.33/s  (0.697s, 1469.68/s)  LR: 8.587e-04  Data: 0.011 (0.014)
Train: 148 [ 700/1251 ( 56%)]  Loss: 3.909 (3.86)  Time: 0.673s, 1522.13/s  (0.697s, 1469.57/s)  LR: 8.587e-04  Data: 0.011 (0.013)
Train: 148 [ 750/1251 ( 60%)]  Loss: 3.598 (3.84)  Time: 0.723s, 1417.09/s  (0.696s, 1470.64/s)  LR: 8.587e-04  Data: 0.009 (0.013)
Train: 148 [ 800/1251 ( 64%)]  Loss: 3.583 (3.83)  Time: 0.673s, 1521.46/s  (0.696s, 1471.34/s)  LR: 8.587e-04  Data: 0.011 (0.013)
Train: 148 [ 850/1251 ( 68%)]  Loss: 3.399 (3.80)  Time: 0.666s, 1536.51/s  (0.696s, 1471.73/s)  LR: 8.587e-04  Data: 0.011 (0.013)
Train: 148 [ 900/1251 ( 72%)]  Loss: 3.994 (3.81)  Time: 0.715s, 1432.91/s  (0.695s, 1472.57/s)  LR: 8.587e-04  Data: 0.010 (0.013)
Train: 148 [ 950/1251 ( 76%)]  Loss: 3.557 (3.80)  Time: 0.680s, 1505.40/s  (0.696s, 1472.07/s)  LR: 8.587e-04  Data: 0.019 (0.013)
Train: 148 [1000/1251 ( 80%)]  Loss: 3.672 (3.79)  Time: 0.675s, 1516.72/s  (0.696s, 1472.11/s)  LR: 8.587e-04  Data: 0.010 (0.012)
Train: 148 [1050/1251 ( 84%)]  Loss: 3.882 (3.80)  Time: 0.693s, 1477.44/s  (0.695s, 1472.68/s)  LR: 8.587e-04  Data: 0.009 (0.012)
Train: 148 [1100/1251 ( 88%)]  Loss: 3.579 (3.79)  Time: 0.673s, 1522.37/s  (0.695s, 1473.45/s)  LR: 8.587e-04  Data: 0.010 (0.012)
Train: 148 [1150/1251 ( 92%)]  Loss: 3.802 (3.79)  Time: 0.705s, 1452.99/s  (0.695s, 1473.84/s)  LR: 8.587e-04  Data: 0.009 (0.012)
Train: 148 [1200/1251 ( 96%)]  Loss: 3.836 (3.79)  Time: 0.676s, 1514.11/s  (0.695s, 1474.36/s)  LR: 8.587e-04  Data: 0.010 (0.012)
Train: 148 [1250/1251 (100%)]  Loss: 3.880 (3.79)  Time: 0.695s, 1472.74/s  (0.695s, 1473.92/s)  LR: 8.587e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.623 (1.623)  Loss:  0.8301 (0.8301)  Acc@1: 86.9141 (86.9141)  Acc@5: 96.2891 (96.2891)
Test: [  48/48]  Time: 0.136 (0.580)  Loss:  0.9023 (1.4799)  Acc@1: 83.3726 (70.2180)  Acc@5: 95.8727 (89.7580)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-142.pth.tar', 70.64999991210938)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-147.pth.tar', 70.40799996826172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-138.pth.tar', 70.33200009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-146.pth.tar', 70.30000001953125)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-141.pth.tar', 70.28199993896484)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-139.pth.tar', 70.26399997070313)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-148.pth.tar', 70.21800001708985)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-131.pth.tar', 70.21000004638672)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-133.pth.tar', 70.18999999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-124.pth.tar', 70.14199996826171)

Train: 149 [   0/1251 (  0%)]  Loss: 4.262 (4.26)  Time: 2.297s,  445.82/s  (2.297s,  445.82/s)  LR: 8.568e-04  Data: 1.636 (1.636)
Train: 149 [  50/1251 (  4%)]  Loss: 3.794 (4.03)  Time: 0.707s, 1448.28/s  (0.731s, 1400.68/s)  LR: 8.568e-04  Data: 0.009 (0.049)
Train: 149 [ 100/1251 (  8%)]  Loss: 3.604 (3.89)  Time: 0.672s, 1524.87/s  (0.709s, 1445.11/s)  LR: 8.568e-04  Data: 0.009 (0.030)
Train: 149 [ 150/1251 ( 12%)]  Loss: 4.099 (3.94)  Time: 0.671s, 1525.09/s  (0.706s, 1450.47/s)  LR: 8.568e-04  Data: 0.012 (0.023)
Train: 149 [ 200/1251 ( 16%)]  Loss: 3.771 (3.91)  Time: 0.693s, 1477.95/s  (0.702s, 1458.01/s)  LR: 8.568e-04  Data: 0.009 (0.020)
Train: 149 [ 250/1251 ( 20%)]  Loss: 4.040 (3.93)  Time: 0.712s, 1437.87/s  (0.703s, 1457.50/s)  LR: 8.568e-04  Data: 0.010 (0.018)
Train: 149 [ 300/1251 ( 24%)]  Loss: 4.187 (3.97)  Time: 0.773s, 1325.30/s  (0.702s, 1458.25/s)  LR: 8.568e-04  Data: 0.009 (0.017)
Train: 149 [ 350/1251 ( 28%)]  Loss: 3.991 (3.97)  Time: 0.672s, 1522.72/s  (0.701s, 1461.59/s)  LR: 8.568e-04  Data: 0.010 (0.016)
Train: 149 [ 400/1251 ( 32%)]  Loss: 3.808 (3.95)  Time: 0.700s, 1462.28/s  (0.699s, 1464.13/s)  LR: 8.568e-04  Data: 0.009 (0.015)
Train: 149 [ 450/1251 ( 36%)]  Loss: 3.979 (3.95)  Time: 0.718s, 1426.21/s  (0.698s, 1466.61/s)  LR: 8.568e-04  Data: 0.009 (0.015)
Train: 149 [ 500/1251 ( 40%)]  Loss: 4.253 (3.98)  Time: 0.671s, 1525.50/s  (0.698s, 1467.67/s)  LR: 8.568e-04  Data: 0.010 (0.014)
Train: 149 [ 550/1251 ( 44%)]  Loss: 3.655 (3.95)  Time: 0.672s, 1523.75/s  (0.698s, 1466.48/s)  LR: 8.568e-04  Data: 0.009 (0.014)
Train: 149 [ 600/1251 ( 48%)]  Loss: 3.336 (3.91)  Time: 0.667s, 1535.79/s  (0.698s, 1466.27/s)  LR: 8.568e-04  Data: 0.008 (0.014)
Train: 149 [ 650/1251 ( 52%)]  Loss: 4.173 (3.93)  Time: 0.701s, 1461.34/s  (0.698s, 1468.02/s)  LR: 8.568e-04  Data: 0.009 (0.013)
Train: 149 [ 700/1251 ( 56%)]  Loss: 4.213 (3.94)  Time: 0.669s, 1531.09/s  (0.697s, 1469.54/s)  LR: 8.568e-04  Data: 0.010 (0.013)
Train: 149 [ 750/1251 ( 60%)]  Loss: 3.780 (3.93)  Time: 0.690s, 1483.15/s  (0.696s, 1471.23/s)  LR: 8.568e-04  Data: 0.010 (0.013)
Train: 149 [ 800/1251 ( 64%)]  Loss: 4.134 (3.95)  Time: 0.728s, 1406.78/s  (0.696s, 1472.00/s)  LR: 8.568e-04  Data: 0.009 (0.013)
Train: 149 [ 850/1251 ( 68%)]  Loss: 3.801 (3.94)  Time: 0.671s, 1525.49/s  (0.695s, 1472.53/s)  LR: 8.568e-04  Data: 0.011 (0.012)
Train: 149 [ 900/1251 ( 72%)]  Loss: 3.748 (3.93)  Time: 0.675s, 1517.55/s  (0.695s, 1473.35/s)  LR: 8.568e-04  Data: 0.010 (0.012)
Train: 149 [ 950/1251 ( 76%)]  Loss: 3.905 (3.93)  Time: 0.670s, 1529.31/s  (0.695s, 1474.37/s)  LR: 8.568e-04  Data: 0.008 (0.012)
Train: 149 [1000/1251 ( 80%)]  Loss: 3.705 (3.92)  Time: 0.699s, 1464.23/s  (0.694s, 1474.58/s)  LR: 8.568e-04  Data: 0.009 (0.012)
Train: 149 [1050/1251 ( 84%)]  Loss: 4.203 (3.93)  Time: 0.673s, 1521.18/s  (0.694s, 1474.72/s)  LR: 8.568e-04  Data: 0.009 (0.012)
Train: 149 [1100/1251 ( 88%)]  Loss: 4.171 (3.94)  Time: 0.707s, 1447.96/s  (0.694s, 1474.89/s)  LR: 8.568e-04  Data: 0.010 (0.012)
Train: 149 [1150/1251 ( 92%)]  Loss: 4.110 (3.95)  Time: 0.705s, 1452.44/s  (0.694s, 1474.94/s)  LR: 8.568e-04  Data: 0.011 (0.012)
Train: 149 [1200/1251 ( 96%)]  Loss: 3.669 (3.94)  Time: 0.769s, 1331.45/s  (0.694s, 1474.96/s)  LR: 8.568e-04  Data: 0.010 (0.012)
Train: 149 [1250/1251 (100%)]  Loss: 3.995 (3.94)  Time: 0.698s, 1466.93/s  (0.695s, 1474.44/s)  LR: 8.568e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.588 (1.588)  Loss:  0.9629 (0.9629)  Acc@1: 87.7930 (87.7930)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.136 (0.589)  Loss:  0.9697 (1.5533)  Acc@1: 84.1981 (70.2740)  Acc@5: 95.5189 (89.9200)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-142.pth.tar', 70.64999991210938)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-147.pth.tar', 70.40799996826172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-138.pth.tar', 70.33200009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-146.pth.tar', 70.30000001953125)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-141.pth.tar', 70.28199993896484)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-149.pth.tar', 70.2740001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-139.pth.tar', 70.26399997070313)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-148.pth.tar', 70.21800001708985)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-131.pth.tar', 70.21000004638672)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-133.pth.tar', 70.18999999511719)

Train: 150 [   0/1251 (  0%)]  Loss: 3.982 (3.98)  Time: 2.131s,  480.51/s  (2.131s,  480.51/s)  LR: 8.550e-04  Data: 1.510 (1.510)
Train: 150 [  50/1251 (  4%)]  Loss: 4.002 (3.99)  Time: 0.674s, 1519.99/s  (0.726s, 1411.06/s)  LR: 8.550e-04  Data: 0.010 (0.049)
Train: 150 [ 100/1251 (  8%)]  Loss: 3.953 (3.98)  Time: 0.706s, 1450.86/s  (0.711s, 1439.34/s)  LR: 8.550e-04  Data: 0.010 (0.030)
Train: 150 [ 150/1251 ( 12%)]  Loss: 3.807 (3.94)  Time: 0.700s, 1463.49/s  (0.706s, 1451.09/s)  LR: 8.550e-04  Data: 0.013 (0.024)
Train: 150 [ 200/1251 ( 16%)]  Loss: 3.772 (3.90)  Time: 0.757s, 1352.86/s  (0.701s, 1460.60/s)  LR: 8.550e-04  Data: 0.010 (0.020)
Train: 150 [ 250/1251 ( 20%)]  Loss: 4.054 (3.93)  Time: 0.672s, 1522.73/s  (0.699s, 1465.33/s)  LR: 8.550e-04  Data: 0.011 (0.018)
Train: 150 [ 300/1251 ( 24%)]  Loss: 3.848 (3.92)  Time: 0.720s, 1422.16/s  (0.698s, 1466.68/s)  LR: 8.550e-04  Data: 0.009 (0.017)
Train: 150 [ 350/1251 ( 28%)]  Loss: 4.265 (3.96)  Time: 0.666s, 1537.09/s  (0.697s, 1470.14/s)  LR: 8.550e-04  Data: 0.010 (0.016)
Train: 150 [ 400/1251 ( 32%)]  Loss: 3.875 (3.95)  Time: 0.694s, 1474.59/s  (0.696s, 1471.92/s)  LR: 8.550e-04  Data: 0.011 (0.015)
Train: 150 [ 450/1251 ( 36%)]  Loss: 4.148 (3.97)  Time: 0.676s, 1515.34/s  (0.696s, 1472.12/s)  LR: 8.550e-04  Data: 0.011 (0.015)
Train: 150 [ 500/1251 ( 40%)]  Loss: 3.608 (3.94)  Time: 0.701s, 1461.10/s  (0.695s, 1474.02/s)  LR: 8.550e-04  Data: 0.010 (0.014)
Train: 150 [ 550/1251 ( 44%)]  Loss: 3.546 (3.90)  Time: 0.667s, 1536.23/s  (0.694s, 1474.86/s)  LR: 8.550e-04  Data: 0.010 (0.014)
Train: 150 [ 600/1251 ( 48%)]  Loss: 3.925 (3.91)  Time: 0.675s, 1517.58/s  (0.694s, 1475.23/s)  LR: 8.550e-04  Data: 0.010 (0.014)
Train: 150 [ 650/1251 ( 52%)]  Loss: 3.778 (3.90)  Time: 0.672s, 1524.63/s  (0.694s, 1475.64/s)  LR: 8.550e-04  Data: 0.010 (0.013)
Train: 150 [ 700/1251 ( 56%)]  Loss: 3.964 (3.90)  Time: 0.688s, 1487.57/s  (0.694s, 1476.41/s)  LR: 8.550e-04  Data: 0.019 (0.013)
Train: 150 [ 750/1251 ( 60%)]  Loss: 3.912 (3.90)  Time: 0.727s, 1409.04/s  (0.694s, 1475.74/s)  LR: 8.550e-04  Data: 0.009 (0.013)
Train: 150 [ 800/1251 ( 64%)]  Loss: 3.926 (3.90)  Time: 0.675s, 1517.71/s  (0.694s, 1476.30/s)  LR: 8.550e-04  Data: 0.011 (0.013)
Train: 150 [ 850/1251 ( 68%)]  Loss: 3.839 (3.90)  Time: 0.672s, 1523.81/s  (0.693s, 1476.68/s)  LR: 8.550e-04  Data: 0.009 (0.013)
Train: 150 [ 900/1251 ( 72%)]  Loss: 3.796 (3.89)  Time: 0.721s, 1419.36/s  (0.693s, 1477.01/s)  LR: 8.550e-04  Data: 0.009 (0.012)
Train: 150 [ 950/1251 ( 76%)]  Loss: 3.691 (3.88)  Time: 0.675s, 1517.17/s  (0.693s, 1477.22/s)  LR: 8.550e-04  Data: 0.010 (0.012)
Train: 150 [1000/1251 ( 80%)]  Loss: 3.949 (3.89)  Time: 0.673s, 1522.23/s  (0.693s, 1477.58/s)  LR: 8.550e-04  Data: 0.009 (0.012)
Train: 150 [1050/1251 ( 84%)]  Loss: 3.917 (3.89)  Time: 0.673s, 1520.77/s  (0.693s, 1477.78/s)  LR: 8.550e-04  Data: 0.010 (0.012)
Train: 150 [1100/1251 ( 88%)]  Loss: 4.205 (3.90)  Time: 0.671s, 1525.24/s  (0.693s, 1478.06/s)  LR: 8.550e-04  Data: 0.010 (0.012)
Train: 150 [1150/1251 ( 92%)]  Loss: 4.073 (3.91)  Time: 0.672s, 1524.81/s  (0.693s, 1478.28/s)  LR: 8.550e-04  Data: 0.009 (0.012)
Train: 150 [1200/1251 ( 96%)]  Loss: 3.671 (3.90)  Time: 0.746s, 1372.92/s  (0.693s, 1477.97/s)  LR: 8.550e-04  Data: 0.009 (0.012)
Train: 150 [1250/1251 (100%)]  Loss: 3.814 (3.90)  Time: 0.711s, 1440.96/s  (0.693s, 1478.49/s)  LR: 8.550e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.465 (1.465)  Loss:  0.9038 (0.9038)  Acc@1: 88.1836 (88.1836)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.136 (0.580)  Loss:  0.9409 (1.5784)  Acc@1: 84.6698 (70.7140)  Acc@5: 95.8726 (90.2360)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-150.pth.tar', 70.71399993408203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-142.pth.tar', 70.64999991210938)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-147.pth.tar', 70.40799996826172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-138.pth.tar', 70.33200009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-146.pth.tar', 70.30000001953125)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-141.pth.tar', 70.28199993896484)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-149.pth.tar', 70.2740001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-139.pth.tar', 70.26399997070313)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-148.pth.tar', 70.21800001708985)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-131.pth.tar', 70.21000004638672)

Train: 151 [   0/1251 (  0%)]  Loss: 3.982 (3.98)  Time: 2.058s,  497.61/s  (2.058s,  497.61/s)  LR: 8.532e-04  Data: 1.442 (1.442)
Train: 151 [  50/1251 (  4%)]  Loss: 3.486 (3.73)  Time: 0.701s, 1461.28/s  (0.726s, 1411.35/s)  LR: 8.532e-04  Data: 0.009 (0.045)
Train: 151 [ 100/1251 (  8%)]  Loss: 3.787 (3.75)  Time: 0.728s, 1405.93/s  (0.712s, 1437.23/s)  LR: 8.532e-04  Data: 0.008 (0.028)
Train: 151 [ 150/1251 ( 12%)]  Loss: 3.644 (3.72)  Time: 0.783s, 1307.67/s  (0.709s, 1445.09/s)  LR: 8.532e-04  Data: 0.011 (0.022)
Train: 151 [ 200/1251 ( 16%)]  Loss: 4.124 (3.80)  Time: 0.673s, 1521.76/s  (0.707s, 1448.91/s)  LR: 8.532e-04  Data: 0.011 (0.019)
Train: 151 [ 250/1251 ( 20%)]  Loss: 3.890 (3.82)  Time: 0.671s, 1525.89/s  (0.703s, 1456.07/s)  LR: 8.532e-04  Data: 0.010 (0.018)
Train: 151 [ 300/1251 ( 24%)]  Loss: 3.811 (3.82)  Time: 0.666s, 1538.18/s  (0.701s, 1460.45/s)  LR: 8.532e-04  Data: 0.010 (0.016)
Train: 151 [ 350/1251 ( 28%)]  Loss: 4.088 (3.85)  Time: 0.707s, 1448.59/s  (0.700s, 1462.31/s)  LR: 8.532e-04  Data: 0.009 (0.015)
Train: 151 [ 400/1251 ( 32%)]  Loss: 3.948 (3.86)  Time: 0.670s, 1528.85/s  (0.698s, 1466.11/s)  LR: 8.532e-04  Data: 0.008 (0.015)
Train: 151 [ 450/1251 ( 36%)]  Loss: 4.138 (3.89)  Time: 0.707s, 1449.35/s  (0.697s, 1468.55/s)  LR: 8.532e-04  Data: 0.009 (0.014)
Train: 151 [ 500/1251 ( 40%)]  Loss: 3.504 (3.85)  Time: 0.721s, 1421.07/s  (0.696s, 1470.69/s)  LR: 8.532e-04  Data: 0.017 (0.014)
Train: 151 [ 550/1251 ( 44%)]  Loss: 3.781 (3.85)  Time: 0.672s, 1523.94/s  (0.697s, 1469.23/s)  LR: 8.532e-04  Data: 0.010 (0.014)
Train: 151 [ 600/1251 ( 48%)]  Loss: 4.063 (3.87)  Time: 0.665s, 1538.96/s  (0.696s, 1470.97/s)  LR: 8.532e-04  Data: 0.011 (0.013)
Train: 151 [ 650/1251 ( 52%)]  Loss: 3.861 (3.86)  Time: 0.672s, 1523.42/s  (0.696s, 1472.15/s)  LR: 8.532e-04  Data: 0.010 (0.013)
Train: 151 [ 700/1251 ( 56%)]  Loss: 4.171 (3.89)  Time: 0.672s, 1523.44/s  (0.695s, 1472.56/s)  LR: 8.532e-04  Data: 0.010 (0.013)
Train: 151 [ 750/1251 ( 60%)]  Loss: 4.066 (3.90)  Time: 0.691s, 1482.18/s  (0.695s, 1472.81/s)  LR: 8.532e-04  Data: 0.009 (0.013)
Train: 151 [ 800/1251 ( 64%)]  Loss: 3.939 (3.90)  Time: 0.711s, 1439.53/s  (0.695s, 1473.54/s)  LR: 8.532e-04  Data: 0.009 (0.013)
Train: 151 [ 850/1251 ( 68%)]  Loss: 3.630 (3.88)  Time: 0.672s, 1523.40/s  (0.695s, 1473.84/s)  LR: 8.532e-04  Data: 0.010 (0.012)
Train: 151 [ 900/1251 ( 72%)]  Loss: 3.828 (3.88)  Time: 0.671s, 1526.48/s  (0.695s, 1473.56/s)  LR: 8.532e-04  Data: 0.010 (0.012)
Train: 151 [ 950/1251 ( 76%)]  Loss: 3.729 (3.87)  Time: 0.695s, 1472.45/s  (0.695s, 1472.95/s)  LR: 8.532e-04  Data: 0.009 (0.012)
Train: 151 [1000/1251 ( 80%)]  Loss: 4.160 (3.89)  Time: 0.731s, 1401.71/s  (0.695s, 1472.52/s)  LR: 8.532e-04  Data: 0.010 (0.012)
Train: 151 [1050/1251 ( 84%)]  Loss: 3.893 (3.89)  Time: 0.696s, 1471.34/s  (0.696s, 1472.28/s)  LR: 8.532e-04  Data: 0.009 (0.012)
Train: 151 [1100/1251 ( 88%)]  Loss: 4.017 (3.89)  Time: 0.706s, 1450.37/s  (0.695s, 1472.83/s)  LR: 8.532e-04  Data: 0.010 (0.012)
Train: 151 [1150/1251 ( 92%)]  Loss: 3.337 (3.87)  Time: 0.671s, 1525.83/s  (0.695s, 1472.77/s)  LR: 8.532e-04  Data: 0.010 (0.012)
Train: 151 [1200/1251 ( 96%)]  Loss: 3.867 (3.87)  Time: 0.699s, 1464.05/s  (0.695s, 1473.12/s)  LR: 8.532e-04  Data: 0.009 (0.012)
Train: 151 [1250/1251 (100%)]  Loss: 3.638 (3.86)  Time: 0.658s, 1555.20/s  (0.695s, 1473.40/s)  LR: 8.532e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.484 (1.484)  Loss:  0.8125 (0.8125)  Acc@1: 86.5234 (86.5234)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.136 (0.569)  Loss:  1.0039 (1.5638)  Acc@1: 81.6038 (69.5080)  Acc@5: 95.2830 (89.6500)
Train: 152 [   0/1251 (  0%)]  Loss: 4.072 (4.07)  Time: 2.312s,  442.95/s  (2.312s,  442.95/s)  LR: 8.513e-04  Data: 1.671 (1.671)
Train: 152 [  50/1251 (  4%)]  Loss: 4.009 (4.04)  Time: 0.714s, 1434.47/s  (0.729s, 1404.13/s)  LR: 8.513e-04  Data: 0.013 (0.050)
Train: 152 [ 100/1251 (  8%)]  Loss: 4.146 (4.08)  Time: 0.708s, 1445.75/s  (0.713s, 1435.62/s)  LR: 8.513e-04  Data: 0.009 (0.031)
Train: 152 [ 150/1251 ( 12%)]  Loss: 3.902 (4.03)  Time: 0.699s, 1464.08/s  (0.706s, 1450.92/s)  LR: 8.513e-04  Data: 0.011 (0.024)
Train: 152 [ 200/1251 ( 16%)]  Loss: 3.847 (4.00)  Time: 0.672s, 1524.38/s  (0.701s, 1460.85/s)  LR: 8.513e-04  Data: 0.010 (0.020)
Train: 152 [ 250/1251 ( 20%)]  Loss: 4.203 (4.03)  Time: 0.769s, 1330.94/s  (0.700s, 1462.36/s)  LR: 8.513e-04  Data: 0.010 (0.019)
Train: 152 [ 300/1251 ( 24%)]  Loss: 3.412 (3.94)  Time: 0.715s, 1432.97/s  (0.702s, 1458.31/s)  LR: 8.513e-04  Data: 0.011 (0.018)
Train: 152 [ 350/1251 ( 28%)]  Loss: 3.672 (3.91)  Time: 0.706s, 1450.94/s  (0.704s, 1454.87/s)  LR: 8.513e-04  Data: 0.009 (0.017)
Train: 152 [ 400/1251 ( 32%)]  Loss: 3.630 (3.88)  Time: 0.673s, 1522.00/s  (0.705s, 1452.65/s)  LR: 8.513e-04  Data: 0.010 (0.016)
Train: 152 [ 450/1251 ( 36%)]  Loss: 4.332 (3.92)  Time: 0.666s, 1537.81/s  (0.702s, 1458.18/s)  LR: 8.513e-04  Data: 0.010 (0.016)
Train: 152 [ 500/1251 ( 40%)]  Loss: 3.891 (3.92)  Time: 0.672s, 1524.32/s  (0.701s, 1461.24/s)  LR: 8.513e-04  Data: 0.011 (0.015)
Train: 152 [ 550/1251 ( 44%)]  Loss: 3.963 (3.92)  Time: 0.702s, 1459.41/s  (0.700s, 1463.84/s)  LR: 8.513e-04  Data: 0.009 (0.015)
Train: 152 [ 600/1251 ( 48%)]  Loss: 3.797 (3.91)  Time: 0.703s, 1455.96/s  (0.699s, 1464.67/s)  LR: 8.513e-04  Data: 0.010 (0.014)
Train: 152 [ 650/1251 ( 52%)]  Loss: 4.037 (3.92)  Time: 0.713s, 1436.03/s  (0.699s, 1464.99/s)  LR: 8.513e-04  Data: 0.010 (0.014)
Train: 152 [ 700/1251 ( 56%)]  Loss: 3.744 (3.91)  Time: 0.677s, 1511.48/s  (0.698s, 1466.66/s)  LR: 8.513e-04  Data: 0.010 (0.014)
Train: 152 [ 750/1251 ( 60%)]  Loss: 3.807 (3.90)  Time: 0.701s, 1461.79/s  (0.698s, 1467.26/s)  LR: 8.513e-04  Data: 0.009 (0.013)
Train: 152 [ 800/1251 ( 64%)]  Loss: 4.203 (3.92)  Time: 0.679s, 1508.29/s  (0.698s, 1466.92/s)  LR: 8.513e-04  Data: 0.015 (0.013)
Train: 152 [ 850/1251 ( 68%)]  Loss: 4.214 (3.94)  Time: 0.688s, 1488.68/s  (0.698s, 1467.55/s)  LR: 8.513e-04  Data: 0.011 (0.013)
Train: 152 [ 900/1251 ( 72%)]  Loss: 4.210 (3.95)  Time: 0.697s, 1469.38/s  (0.697s, 1468.26/s)  LR: 8.513e-04  Data: 0.010 (0.013)
Train: 152 [ 950/1251 ( 76%)]  Loss: 3.668 (3.94)  Time: 0.691s, 1481.40/s  (0.697s, 1469.52/s)  LR: 8.513e-04  Data: 0.009 (0.013)
Train: 152 [1000/1251 ( 80%)]  Loss: 4.028 (3.94)  Time: 0.695s, 1474.17/s  (0.697s, 1470.19/s)  LR: 8.513e-04  Data: 0.011 (0.013)
Train: 152 [1050/1251 ( 84%)]  Loss: 4.192 (3.95)  Time: 0.700s, 1463.18/s  (0.696s, 1470.24/s)  LR: 8.513e-04  Data: 0.009 (0.012)
Train: 152 [1100/1251 ( 88%)]  Loss: 3.761 (3.95)  Time: 0.709s, 1444.73/s  (0.696s, 1470.35/s)  LR: 8.513e-04  Data: 0.009 (0.012)
Train: 152 [1150/1251 ( 92%)]  Loss: 3.970 (3.95)  Time: 0.670s, 1529.33/s  (0.696s, 1470.71/s)  LR: 8.513e-04  Data: 0.012 (0.012)
Train: 152 [1200/1251 ( 96%)]  Loss: 3.771 (3.94)  Time: 0.666s, 1538.11/s  (0.696s, 1471.27/s)  LR: 8.513e-04  Data: 0.011 (0.012)
Train: 152 [1250/1251 (100%)]  Loss: 3.817 (3.93)  Time: 0.677s, 1513.44/s  (0.696s, 1471.87/s)  LR: 8.513e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.693 (1.693)  Loss:  0.8960 (0.8960)  Acc@1: 87.5000 (87.5000)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.136 (0.574)  Loss:  0.9165 (1.4778)  Acc@1: 82.4292 (70.3700)  Acc@5: 95.5189 (89.8980)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-150.pth.tar', 70.71399993408203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-142.pth.tar', 70.64999991210938)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-147.pth.tar', 70.40799996826172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-152.pth.tar', 70.36999999511718)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-138.pth.tar', 70.33200009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-146.pth.tar', 70.30000001953125)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-141.pth.tar', 70.28199993896484)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-149.pth.tar', 70.2740001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-139.pth.tar', 70.26399997070313)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-148.pth.tar', 70.21800001708985)

Train: 153 [   0/1251 (  0%)]  Loss: 3.673 (3.67)  Time: 2.261s,  452.94/s  (2.261s,  452.94/s)  LR: 8.495e-04  Data: 1.643 (1.643)
Train: 153 [  50/1251 (  4%)]  Loss: 4.089 (3.88)  Time: 0.677s, 1513.27/s  (0.734s, 1395.29/s)  LR: 8.495e-04  Data: 0.009 (0.051)
Train: 153 [ 100/1251 (  8%)]  Loss: 3.848 (3.87)  Time: 0.716s, 1429.98/s  (0.712s, 1437.64/s)  LR: 8.495e-04  Data: 0.010 (0.030)
Train: 153 [ 150/1251 ( 12%)]  Loss: 3.447 (3.76)  Time: 0.705s, 1453.39/s  (0.705s, 1452.32/s)  LR: 8.495e-04  Data: 0.009 (0.024)
Train: 153 [ 200/1251 ( 16%)]  Loss: 3.807 (3.77)  Time: 0.706s, 1451.27/s  (0.702s, 1459.00/s)  LR: 8.495e-04  Data: 0.009 (0.020)
Train: 153 [ 250/1251 ( 20%)]  Loss: 4.132 (3.83)  Time: 0.705s, 1451.45/s  (0.700s, 1462.47/s)  LR: 8.495e-04  Data: 0.009 (0.018)
Train: 153 [ 300/1251 ( 24%)]  Loss: 4.152 (3.88)  Time: 0.678s, 1509.85/s  (0.699s, 1465.19/s)  LR: 8.495e-04  Data: 0.010 (0.017)
Train: 153 [ 350/1251 ( 28%)]  Loss: 3.625 (3.85)  Time: 0.670s, 1529.50/s  (0.698s, 1467.93/s)  LR: 8.495e-04  Data: 0.009 (0.016)
Train: 153 [ 400/1251 ( 32%)]  Loss: 3.941 (3.86)  Time: 0.689s, 1485.38/s  (0.697s, 1469.09/s)  LR: 8.495e-04  Data: 0.010 (0.015)
Train: 153 [ 450/1251 ( 36%)]  Loss: 4.047 (3.88)  Time: 0.706s, 1449.62/s  (0.697s, 1469.71/s)  LR: 8.495e-04  Data: 0.010 (0.015)
Train: 153 [ 500/1251 ( 40%)]  Loss: 3.675 (3.86)  Time: 0.706s, 1449.82/s  (0.696s, 1470.65/s)  LR: 8.495e-04  Data: 0.009 (0.014)
Train: 153 [ 550/1251 ( 44%)]  Loss: 4.166 (3.88)  Time: 0.669s, 1531.14/s  (0.697s, 1470.07/s)  LR: 8.495e-04  Data: 0.009 (0.014)
Train: 153 [ 600/1251 ( 48%)]  Loss: 3.879 (3.88)  Time: 0.777s, 1317.93/s  (0.696s, 1470.67/s)  LR: 8.495e-04  Data: 0.009 (0.014)
Train: 153 [ 650/1251 ( 52%)]  Loss: 4.174 (3.90)  Time: 0.670s, 1527.58/s  (0.697s, 1469.58/s)  LR: 8.495e-04  Data: 0.010 (0.013)
Train: 153 [ 700/1251 ( 56%)]  Loss: 3.944 (3.91)  Time: 0.697s, 1469.27/s  (0.696s, 1470.40/s)  LR: 8.495e-04  Data: 0.011 (0.013)
Train: 153 [ 750/1251 ( 60%)]  Loss: 3.675 (3.89)  Time: 0.674s, 1519.21/s  (0.696s, 1470.95/s)  LR: 8.495e-04  Data: 0.010 (0.013)
Train: 153 [ 800/1251 ( 64%)]  Loss: 3.991 (3.90)  Time: 0.736s, 1391.33/s  (0.696s, 1471.23/s)  LR: 8.495e-04  Data: 0.010 (0.013)
Train: 153 [ 850/1251 ( 68%)]  Loss: 3.765 (3.89)  Time: 0.692s, 1479.35/s  (0.696s, 1470.76/s)  LR: 8.495e-04  Data: 0.009 (0.013)
Train: 153 [ 900/1251 ( 72%)]  Loss: 3.835 (3.89)  Time: 0.788s, 1299.24/s  (0.696s, 1471.36/s)  LR: 8.495e-04  Data: 0.010 (0.012)
Train: 153 [ 950/1251 ( 76%)]  Loss: 3.757 (3.88)  Time: 0.672s, 1524.69/s  (0.696s, 1471.00/s)  LR: 8.495e-04  Data: 0.010 (0.012)
Train: 153 [1000/1251 ( 80%)]  Loss: 3.595 (3.87)  Time: 0.683s, 1500.19/s  (0.696s, 1471.51/s)  LR: 8.495e-04  Data: 0.010 (0.012)
Train: 153 [1050/1251 ( 84%)]  Loss: 3.445 (3.85)  Time: 0.733s, 1397.61/s  (0.696s, 1470.85/s)  LR: 8.495e-04  Data: 0.010 (0.012)
Train: 153 [1100/1251 ( 88%)]  Loss: 4.058 (3.86)  Time: 0.712s, 1438.43/s  (0.696s, 1470.42/s)  LR: 8.495e-04  Data: 0.010 (0.012)
Train: 153 [1150/1251 ( 92%)]  Loss: 3.866 (3.86)  Time: 0.675s, 1516.65/s  (0.696s, 1470.75/s)  LR: 8.495e-04  Data: 0.009 (0.012)
Train: 153 [1200/1251 ( 96%)]  Loss: 3.919 (3.86)  Time: 0.769s, 1331.81/s  (0.696s, 1471.42/s)  LR: 8.495e-04  Data: 0.008 (0.012)
Train: 153 [1250/1251 (100%)]  Loss: 3.878 (3.86)  Time: 0.656s, 1561.88/s  (0.696s, 1471.34/s)  LR: 8.495e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.463 (1.463)  Loss:  1.0400 (1.0400)  Acc@1: 85.7422 (85.7422)  Acc@5: 96.1914 (96.1914)
Test: [  48/48]  Time: 0.137 (0.588)  Loss:  1.0918 (1.5783)  Acc@1: 81.2500 (69.9200)  Acc@5: 96.1085 (89.7160)
Train: 154 [   0/1251 (  0%)]  Loss: 3.630 (3.63)  Time: 2.257s,  453.67/s  (2.257s,  453.67/s)  LR: 8.476e-04  Data: 1.643 (1.643)
Train: 154 [  50/1251 (  4%)]  Loss: 3.976 (3.80)  Time: 0.675s, 1518.10/s  (0.735s, 1393.50/s)  LR: 8.476e-04  Data: 0.011 (0.045)
Train: 154 [ 100/1251 (  8%)]  Loss: 4.074 (3.89)  Time: 0.720s, 1422.76/s  (0.714s, 1434.04/s)  LR: 8.476e-04  Data: 0.010 (0.028)
Train: 154 [ 150/1251 ( 12%)]  Loss: 4.111 (3.95)  Time: 0.721s, 1420.58/s  (0.707s, 1448.34/s)  LR: 8.476e-04  Data: 0.009 (0.022)
Train: 154 [ 200/1251 ( 16%)]  Loss: 3.587 (3.88)  Time: 0.730s, 1403.03/s  (0.703s, 1456.28/s)  LR: 8.476e-04  Data: 0.009 (0.019)
Train: 154 [ 250/1251 ( 20%)]  Loss: 3.972 (3.89)  Time: 0.738s, 1387.80/s  (0.701s, 1460.62/s)  LR: 8.476e-04  Data: 0.009 (0.017)
Train: 154 [ 300/1251 ( 24%)]  Loss: 3.691 (3.86)  Time: 0.672s, 1523.63/s  (0.699s, 1465.54/s)  LR: 8.476e-04  Data: 0.010 (0.016)
Train: 154 [ 350/1251 ( 28%)]  Loss: 3.516 (3.82)  Time: 0.705s, 1452.02/s  (0.698s, 1467.72/s)  LR: 8.476e-04  Data: 0.009 (0.015)
Train: 154 [ 400/1251 ( 32%)]  Loss: 3.894 (3.83)  Time: 0.673s, 1520.60/s  (0.697s, 1469.28/s)  LR: 8.476e-04  Data: 0.010 (0.015)
Train: 154 [ 450/1251 ( 36%)]  Loss: 3.982 (3.84)  Time: 0.685s, 1495.75/s  (0.696s, 1470.36/s)  LR: 8.476e-04  Data: 0.012 (0.014)
Train: 154 [ 500/1251 ( 40%)]  Loss: 3.813 (3.84)  Time: 0.673s, 1522.63/s  (0.696s, 1471.91/s)  LR: 8.476e-04  Data: 0.011 (0.014)
Train: 154 [ 550/1251 ( 44%)]  Loss: 3.744 (3.83)  Time: 0.674s, 1519.10/s  (0.697s, 1469.27/s)  LR: 8.476e-04  Data: 0.011 (0.013)
Train: 154 [ 600/1251 ( 48%)]  Loss: 3.674 (3.82)  Time: 0.724s, 1413.50/s  (0.696s, 1470.61/s)  LR: 8.476e-04  Data: 0.010 (0.013)
Train: 154 [ 650/1251 ( 52%)]  Loss: 3.842 (3.82)  Time: 0.682s, 1500.60/s  (0.696s, 1471.45/s)  LR: 8.476e-04  Data: 0.009 (0.013)
Train: 154 [ 700/1251 ( 56%)]  Loss: 3.287 (3.79)  Time: 0.682s, 1500.70/s  (0.696s, 1471.94/s)  LR: 8.476e-04  Data: 0.011 (0.013)
Train: 154 [ 750/1251 ( 60%)]  Loss: 3.938 (3.80)  Time: 0.680s, 1505.57/s  (0.695s, 1472.77/s)  LR: 8.476e-04  Data: 0.018 (0.013)
Train: 154 [ 800/1251 ( 64%)]  Loss: 3.740 (3.79)  Time: 0.703s, 1456.53/s  (0.695s, 1472.60/s)  LR: 8.476e-04  Data: 0.010 (0.012)
Train: 154 [ 850/1251 ( 68%)]  Loss: 3.869 (3.80)  Time: 0.676s, 1515.89/s  (0.695s, 1473.54/s)  LR: 8.476e-04  Data: 0.011 (0.012)
Train: 154 [ 900/1251 ( 72%)]  Loss: 3.828 (3.80)  Time: 0.678s, 1510.27/s  (0.695s, 1473.63/s)  LR: 8.476e-04  Data: 0.011 (0.012)
Train: 154 [ 950/1251 ( 76%)]  Loss: 3.830 (3.80)  Time: 0.707s, 1448.53/s  (0.695s, 1474.22/s)  LR: 8.476e-04  Data: 0.013 (0.012)
Train: 154 [1000/1251 ( 80%)]  Loss: 3.931 (3.81)  Time: 0.684s, 1496.81/s  (0.695s, 1474.31/s)  LR: 8.476e-04  Data: 0.011 (0.012)
Train: 154 [1050/1251 ( 84%)]  Loss: 3.708 (3.80)  Time: 0.703s, 1456.88/s  (0.694s, 1474.59/s)  LR: 8.476e-04  Data: 0.009 (0.012)
Train: 154 [1100/1251 ( 88%)]  Loss: 4.023 (3.81)  Time: 0.674s, 1520.29/s  (0.694s, 1474.88/s)  LR: 8.476e-04  Data: 0.011 (0.012)
Train: 154 [1150/1251 ( 92%)]  Loss: 4.070 (3.82)  Time: 0.726s, 1409.89/s  (0.694s, 1475.06/s)  LR: 8.476e-04  Data: 0.011 (0.012)
Train: 154 [1200/1251 ( 96%)]  Loss: 3.773 (3.82)  Time: 0.676s, 1514.55/s  (0.694s, 1475.16/s)  LR: 8.476e-04  Data: 0.010 (0.012)
Train: 154 [1250/1251 (100%)]  Loss: 4.107 (3.83)  Time: 0.656s, 1560.66/s  (0.694s, 1475.11/s)  LR: 8.476e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.588 (1.588)  Loss:  0.9448 (0.9448)  Acc@1: 87.6953 (87.6953)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.136 (0.588)  Loss:  1.0635 (1.6207)  Acc@1: 81.6038 (70.4100)  Acc@5: 94.8113 (90.0920)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-150.pth.tar', 70.71399993408203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-142.pth.tar', 70.64999991210938)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-154.pth.tar', 70.41000002441406)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-147.pth.tar', 70.40799996826172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-152.pth.tar', 70.36999999511718)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-138.pth.tar', 70.33200009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-146.pth.tar', 70.30000001953125)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-141.pth.tar', 70.28199993896484)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-149.pth.tar', 70.2740001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-139.pth.tar', 70.26399997070313)

Train: 155 [   0/1251 (  0%)]  Loss: 3.770 (3.77)  Time: 2.201s,  465.16/s  (2.201s,  465.16/s)  LR: 8.457e-04  Data: 1.540 (1.540)
Train: 155 [  50/1251 (  4%)]  Loss: 3.876 (3.82)  Time: 0.668s, 1533.88/s  (0.740s, 1384.21/s)  LR: 8.457e-04  Data: 0.009 (0.051)
Train: 155 [ 100/1251 (  8%)]  Loss: 3.604 (3.75)  Time: 0.721s, 1421.10/s  (0.722s, 1419.02/s)  LR: 8.457e-04  Data: 0.011 (0.031)
Train: 155 [ 150/1251 ( 12%)]  Loss: 3.720 (3.74)  Time: 0.785s, 1304.64/s  (0.712s, 1437.96/s)  LR: 8.457e-04  Data: 0.009 (0.024)
Train: 155 [ 200/1251 ( 16%)]  Loss: 4.086 (3.81)  Time: 0.750s, 1364.44/s  (0.710s, 1441.57/s)  LR: 8.457e-04  Data: 0.009 (0.020)
Train: 155 [ 250/1251 ( 20%)]  Loss: 4.120 (3.86)  Time: 0.710s, 1442.30/s  (0.707s, 1448.18/s)  LR: 8.457e-04  Data: 0.013 (0.018)
Train: 155 [ 300/1251 ( 24%)]  Loss: 3.954 (3.88)  Time: 0.701s, 1460.98/s  (0.705s, 1453.37/s)  LR: 8.457e-04  Data: 0.009 (0.017)
Train: 155 [ 350/1251 ( 28%)]  Loss: 3.826 (3.87)  Time: 0.678s, 1510.47/s  (0.704s, 1454.59/s)  LR: 8.457e-04  Data: 0.010 (0.016)
Train: 155 [ 400/1251 ( 32%)]  Loss: 3.872 (3.87)  Time: 0.693s, 1478.35/s  (0.702s, 1458.52/s)  LR: 8.457e-04  Data: 0.010 (0.015)
Train: 155 [ 450/1251 ( 36%)]  Loss: 3.756 (3.86)  Time: 0.717s, 1427.38/s  (0.701s, 1460.99/s)  LR: 8.457e-04  Data: 0.010 (0.015)
Train: 155 [ 500/1251 ( 40%)]  Loss: 3.980 (3.87)  Time: 0.706s, 1450.02/s  (0.700s, 1461.90/s)  LR: 8.457e-04  Data: 0.010 (0.014)
Train: 155 [ 550/1251 ( 44%)]  Loss: 3.836 (3.87)  Time: 0.711s, 1439.32/s  (0.700s, 1462.98/s)  LR: 8.457e-04  Data: 0.010 (0.014)
Train: 155 [ 600/1251 ( 48%)]  Loss: 4.028 (3.88)  Time: 0.666s, 1537.75/s  (0.699s, 1464.11/s)  LR: 8.457e-04  Data: 0.010 (0.014)
Train: 155 [ 650/1251 ( 52%)]  Loss: 3.709 (3.87)  Time: 0.670s, 1528.12/s  (0.699s, 1464.84/s)  LR: 8.457e-04  Data: 0.011 (0.013)
Train: 155 [ 700/1251 ( 56%)]  Loss: 3.664 (3.85)  Time: 0.678s, 1509.85/s  (0.699s, 1465.10/s)  LR: 8.457e-04  Data: 0.010 (0.013)
Train: 155 [ 750/1251 ( 60%)]  Loss: 3.873 (3.85)  Time: 0.671s, 1526.97/s  (0.699s, 1464.81/s)  LR: 8.457e-04  Data: 0.010 (0.013)
Train: 155 [ 800/1251 ( 64%)]  Loss: 3.972 (3.86)  Time: 0.705s, 1451.71/s  (0.699s, 1464.61/s)  LR: 8.457e-04  Data: 0.010 (0.013)
Train: 155 [ 850/1251 ( 68%)]  Loss: 4.007 (3.87)  Time: 0.676s, 1514.39/s  (0.699s, 1465.22/s)  LR: 8.457e-04  Data: 0.010 (0.013)
Train: 155 [ 900/1251 ( 72%)]  Loss: 3.881 (3.87)  Time: 0.681s, 1504.24/s  (0.699s, 1465.85/s)  LR: 8.457e-04  Data: 0.015 (0.012)
Train: 155 [ 950/1251 ( 76%)]  Loss: 3.887 (3.87)  Time: 0.693s, 1478.08/s  (0.699s, 1465.92/s)  LR: 8.457e-04  Data: 0.009 (0.012)
Train: 155 [1000/1251 ( 80%)]  Loss: 4.062 (3.88)  Time: 0.722s, 1419.12/s  (0.698s, 1466.54/s)  LR: 8.457e-04  Data: 0.009 (0.012)
Train: 155 [1050/1251 ( 84%)]  Loss: 3.808 (3.88)  Time: 0.707s, 1448.09/s  (0.698s, 1466.89/s)  LR: 8.457e-04  Data: 0.009 (0.012)
Train: 155 [1100/1251 ( 88%)]  Loss: 4.092 (3.89)  Time: 0.673s, 1521.41/s  (0.698s, 1466.93/s)  LR: 8.457e-04  Data: 0.009 (0.012)
Train: 155 [1150/1251 ( 92%)]  Loss: 3.845 (3.88)  Time: 0.670s, 1527.37/s  (0.698s, 1466.78/s)  LR: 8.457e-04  Data: 0.010 (0.012)
Train: 155 [1200/1251 ( 96%)]  Loss: 3.951 (3.89)  Time: 0.723s, 1416.30/s  (0.698s, 1467.01/s)  LR: 8.457e-04  Data: 0.008 (0.012)
Train: 155 [1250/1251 (100%)]  Loss: 3.715 (3.88)  Time: 0.708s, 1446.90/s  (0.698s, 1467.35/s)  LR: 8.457e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.566 (1.566)  Loss:  0.9189 (0.9189)  Acc@1: 86.3281 (86.3281)  Acc@5: 96.1914 (96.1914)
Test: [  48/48]  Time: 0.136 (0.590)  Loss:  1.1035 (1.5858)  Acc@1: 81.7217 (69.7180)  Acc@5: 94.8113 (89.4200)
Train: 156 [   0/1251 (  0%)]  Loss: 3.708 (3.71)  Time: 2.260s,  453.17/s  (2.260s,  453.17/s)  LR: 8.439e-04  Data: 1.568 (1.568)
Train: 156 [  50/1251 (  4%)]  Loss: 3.816 (3.76)  Time: 0.671s, 1526.00/s  (0.729s, 1405.18/s)  LR: 8.439e-04  Data: 0.010 (0.048)
Train: 156 [ 100/1251 (  8%)]  Loss: 3.857 (3.79)  Time: 0.724s, 1413.65/s  (0.710s, 1442.10/s)  LR: 8.439e-04  Data: 0.010 (0.030)
Train: 156 [ 150/1251 ( 12%)]  Loss: 3.751 (3.78)  Time: 0.673s, 1521.68/s  (0.705s, 1452.19/s)  LR: 8.439e-04  Data: 0.011 (0.023)
Train: 156 [ 200/1251 ( 16%)]  Loss: 3.591 (3.74)  Time: 0.675s, 1516.74/s  (0.703s, 1456.75/s)  LR: 8.439e-04  Data: 0.012 (0.020)
Train: 156 [ 250/1251 ( 20%)]  Loss: 3.738 (3.74)  Time: 0.671s, 1526.07/s  (0.702s, 1459.04/s)  LR: 8.439e-04  Data: 0.011 (0.018)
Train: 156 [ 300/1251 ( 24%)]  Loss: 3.632 (3.73)  Time: 0.672s, 1522.71/s  (0.701s, 1460.90/s)  LR: 8.439e-04  Data: 0.010 (0.017)
Train: 156 [ 350/1251 ( 28%)]  Loss: 3.544 (3.70)  Time: 0.671s, 1526.87/s  (0.700s, 1463.42/s)  LR: 8.439e-04  Data: 0.010 (0.016)
Train: 156 [ 400/1251 ( 32%)]  Loss: 3.629 (3.70)  Time: 0.673s, 1520.73/s  (0.700s, 1463.79/s)  LR: 8.439e-04  Data: 0.008 (0.015)
Train: 156 [ 450/1251 ( 36%)]  Loss: 3.654 (3.69)  Time: 0.693s, 1478.58/s  (0.700s, 1463.46/s)  LR: 8.439e-04  Data: 0.009 (0.015)
Train: 156 [ 500/1251 ( 40%)]  Loss: 4.129 (3.73)  Time: 0.707s, 1448.24/s  (0.699s, 1464.89/s)  LR: 8.439e-04  Data: 0.010 (0.014)
Train: 156 [ 550/1251 ( 44%)]  Loss: 4.031 (3.76)  Time: 0.696s, 1471.98/s  (0.698s, 1466.22/s)  LR: 8.439e-04  Data: 0.010 (0.014)
Train: 156 [ 600/1251 ( 48%)]  Loss: 3.872 (3.77)  Time: 0.673s, 1521.90/s  (0.698s, 1467.64/s)  LR: 8.439e-04  Data: 0.011 (0.013)
Train: 156 [ 650/1251 ( 52%)]  Loss: 3.632 (3.76)  Time: 0.704s, 1453.98/s  (0.697s, 1468.78/s)  LR: 8.439e-04  Data: 0.009 (0.013)
Train: 156 [ 700/1251 ( 56%)]  Loss: 3.728 (3.75)  Time: 0.671s, 1525.16/s  (0.697s, 1469.08/s)  LR: 8.439e-04  Data: 0.010 (0.013)
Train: 156 [ 750/1251 ( 60%)]  Loss: 3.994 (3.77)  Time: 0.673s, 1520.73/s  (0.696s, 1470.82/s)  LR: 8.439e-04  Data: 0.010 (0.013)
Train: 156 [ 800/1251 ( 64%)]  Loss: 3.354 (3.74)  Time: 0.684s, 1496.55/s  (0.695s, 1472.33/s)  LR: 8.439e-04  Data: 0.014 (0.013)
Train: 156 [ 850/1251 ( 68%)]  Loss: 3.843 (3.75)  Time: 0.712s, 1438.93/s  (0.696s, 1471.75/s)  LR: 8.439e-04  Data: 0.010 (0.012)
Train: 156 [ 900/1251 ( 72%)]  Loss: 3.851 (3.76)  Time: 0.673s, 1522.30/s  (0.695s, 1472.37/s)  LR: 8.439e-04  Data: 0.011 (0.012)
Train: 156 [ 950/1251 ( 76%)]  Loss: 3.722 (3.75)  Time: 0.668s, 1532.91/s  (0.695s, 1473.19/s)  LR: 8.439e-04  Data: 0.011 (0.012)
Train: 156 [1000/1251 ( 80%)]  Loss: 3.995 (3.77)  Time: 0.675s, 1517.75/s  (0.695s, 1473.77/s)  LR: 8.439e-04  Data: 0.012 (0.012)
Train: 156 [1050/1251 ( 84%)]  Loss: 3.394 (3.75)  Time: 0.674s, 1520.02/s  (0.695s, 1474.16/s)  LR: 8.439e-04  Data: 0.010 (0.012)
Train: 156 [1100/1251 ( 88%)]  Loss: 3.806 (3.75)  Time: 0.667s, 1535.89/s  (0.694s, 1474.53/s)  LR: 8.439e-04  Data: 0.010 (0.012)
Train: 156 [1150/1251 ( 92%)]  Loss: 3.915 (3.76)  Time: 0.677s, 1511.82/s  (0.694s, 1475.24/s)  LR: 8.439e-04  Data: 0.011 (0.012)
Train: 156 [1200/1251 ( 96%)]  Loss: 3.463 (3.75)  Time: 0.704s, 1455.02/s  (0.694s, 1475.26/s)  LR: 8.439e-04  Data: 0.011 (0.012)
Train: 156 [1250/1251 (100%)]  Loss: 4.302 (3.77)  Time: 0.690s, 1483.65/s  (0.694s, 1475.18/s)  LR: 8.439e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.476 (1.476)  Loss:  1.0117 (1.0117)  Acc@1: 87.9883 (87.9883)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.137 (0.581)  Loss:  1.0293 (1.5850)  Acc@1: 83.0189 (70.5100)  Acc@5: 96.1085 (90.0500)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-150.pth.tar', 70.71399993408203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-142.pth.tar', 70.64999991210938)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-156.pth.tar', 70.51000012207031)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-154.pth.tar', 70.41000002441406)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-147.pth.tar', 70.40799996826172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-152.pth.tar', 70.36999999511718)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-138.pth.tar', 70.33200009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-146.pth.tar', 70.30000001953125)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-141.pth.tar', 70.28199993896484)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-149.pth.tar', 70.2740001171875)

Train: 157 [   0/1251 (  0%)]  Loss: 3.476 (3.48)  Time: 2.198s,  465.96/s  (2.198s,  465.96/s)  LR: 8.420e-04  Data: 1.586 (1.586)
Train: 157 [  50/1251 (  4%)]  Loss: 3.848 (3.66)  Time: 0.701s, 1459.90/s  (0.730s, 1403.04/s)  LR: 8.420e-04  Data: 0.009 (0.051)
Train: 157 [ 100/1251 (  8%)]  Loss: 4.087 (3.80)  Time: 0.677s, 1511.64/s  (0.709s, 1443.44/s)  LR: 8.420e-04  Data: 0.010 (0.030)
Train: 157 [ 150/1251 ( 12%)]  Loss: 3.597 (3.75)  Time: 0.725s, 1412.74/s  (0.704s, 1454.52/s)  LR: 8.420e-04  Data: 0.015 (0.024)
Train: 157 [ 200/1251 ( 16%)]  Loss: 3.766 (3.75)  Time: 0.689s, 1486.83/s  (0.703s, 1456.93/s)  LR: 8.420e-04  Data: 0.017 (0.020)
Train: 157 [ 250/1251 ( 20%)]  Loss: 4.110 (3.81)  Time: 0.672s, 1524.00/s  (0.701s, 1460.83/s)  LR: 8.420e-04  Data: 0.010 (0.018)
Train: 157 [ 300/1251 ( 24%)]  Loss: 3.943 (3.83)  Time: 0.704s, 1454.16/s  (0.701s, 1461.49/s)  LR: 8.420e-04  Data: 0.011 (0.017)
Train: 157 [ 350/1251 ( 28%)]  Loss: 3.822 (3.83)  Time: 0.671s, 1526.59/s  (0.700s, 1462.58/s)  LR: 8.420e-04  Data: 0.009 (0.016)
Train: 157 [ 400/1251 ( 32%)]  Loss: 4.052 (3.86)  Time: 0.714s, 1435.17/s  (0.700s, 1462.60/s)  LR: 8.420e-04  Data: 0.011 (0.015)
Train: 157 [ 450/1251 ( 36%)]  Loss: 3.377 (3.81)  Time: 0.668s, 1532.97/s  (0.700s, 1463.72/s)  LR: 8.420e-04  Data: 0.009 (0.015)
Train: 157 [ 500/1251 ( 40%)]  Loss: 3.372 (3.77)  Time: 0.706s, 1450.04/s  (0.699s, 1465.62/s)  LR: 8.420e-04  Data: 0.029 (0.014)
Train: 157 [ 550/1251 ( 44%)]  Loss: 3.849 (3.77)  Time: 0.673s, 1520.70/s  (0.698s, 1466.28/s)  LR: 8.420e-04  Data: 0.010 (0.014)
Train: 157 [ 600/1251 ( 48%)]  Loss: 3.981 (3.79)  Time: 0.681s, 1504.03/s  (0.698s, 1466.05/s)  LR: 8.420e-04  Data: 0.010 (0.014)
Train: 157 [ 650/1251 ( 52%)]  Loss: 3.333 (3.76)  Time: 0.674s, 1519.85/s  (0.698s, 1466.93/s)  LR: 8.420e-04  Data: 0.011 (0.013)
Train: 157 [ 700/1251 ( 56%)]  Loss: 3.573 (3.75)  Time: 0.706s, 1451.08/s  (0.698s, 1467.43/s)  LR: 8.420e-04  Data: 0.010 (0.013)
Train: 157 [ 750/1251 ( 60%)]  Loss: 3.649 (3.74)  Time: 0.675s, 1515.96/s  (0.697s, 1468.62/s)  LR: 8.420e-04  Data: 0.010 (0.013)
Train: 157 [ 800/1251 ( 64%)]  Loss: 3.842 (3.75)  Time: 0.672s, 1523.05/s  (0.697s, 1468.44/s)  LR: 8.420e-04  Data: 0.010 (0.013)
Train: 157 [ 850/1251 ( 68%)]  Loss: 4.142 (3.77)  Time: 0.682s, 1501.18/s  (0.697s, 1469.93/s)  LR: 8.420e-04  Data: 0.011 (0.013)
Train: 157 [ 900/1251 ( 72%)]  Loss: 3.715 (3.76)  Time: 0.674s, 1519.71/s  (0.696s, 1470.89/s)  LR: 8.420e-04  Data: 0.009 (0.012)
Train: 157 [ 950/1251 ( 76%)]  Loss: 3.620 (3.76)  Time: 0.672s, 1524.65/s  (0.696s, 1471.09/s)  LR: 8.420e-04  Data: 0.009 (0.012)
Train: 157 [1000/1251 ( 80%)]  Loss: 3.683 (3.75)  Time: 0.702s, 1458.73/s  (0.696s, 1471.93/s)  LR: 8.420e-04  Data: 0.010 (0.012)
Train: 157 [1050/1251 ( 84%)]  Loss: 4.124 (3.77)  Time: 0.715s, 1431.59/s  (0.696s, 1472.08/s)  LR: 8.420e-04  Data: 0.010 (0.012)
Train: 157 [1100/1251 ( 88%)]  Loss: 3.657 (3.77)  Time: 0.723s, 1415.44/s  (0.696s, 1472.27/s)  LR: 8.420e-04  Data: 0.010 (0.012)
Train: 157 [1150/1251 ( 92%)]  Loss: 3.485 (3.75)  Time: 0.709s, 1443.66/s  (0.695s, 1472.61/s)  LR: 8.420e-04  Data: 0.011 (0.012)
Train: 157 [1200/1251 ( 96%)]  Loss: 3.808 (3.76)  Time: 0.714s, 1433.98/s  (0.695s, 1472.85/s)  LR: 8.420e-04  Data: 0.010 (0.012)
Train: 157 [1250/1251 (100%)]  Loss: 3.845 (3.76)  Time: 0.657s, 1557.96/s  (0.695s, 1472.90/s)  LR: 8.420e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.662 (1.662)  Loss:  1.0049 (1.0049)  Acc@1: 86.5234 (86.5234)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.137 (0.595)  Loss:  1.0898 (1.6241)  Acc@1: 82.3113 (70.6980)  Acc@5: 96.6981 (90.1140)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-150.pth.tar', 70.71399993408203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-157.pth.tar', 70.69799994384766)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-142.pth.tar', 70.64999991210938)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-156.pth.tar', 70.51000012207031)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-154.pth.tar', 70.41000002441406)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-147.pth.tar', 70.40799996826172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-152.pth.tar', 70.36999999511718)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-138.pth.tar', 70.33200009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-146.pth.tar', 70.30000001953125)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-141.pth.tar', 70.28199993896484)

Train: 158 [   0/1251 (  0%)]  Loss: 3.410 (3.41)  Time: 2.274s,  450.26/s  (2.274s,  450.26/s)  LR: 8.401e-04  Data: 1.583 (1.583)
Train: 158 [  50/1251 (  4%)]  Loss: 3.946 (3.68)  Time: 0.678s, 1511.39/s  (0.724s, 1414.30/s)  LR: 8.401e-04  Data: 0.010 (0.046)
Train: 158 [ 100/1251 (  8%)]  Loss: 3.957 (3.77)  Time: 0.718s, 1425.38/s  (0.709s, 1445.17/s)  LR: 8.401e-04  Data: 0.009 (0.028)
Train: 158 [ 150/1251 ( 12%)]  Loss: 3.935 (3.81)  Time: 0.679s, 1507.23/s  (0.706s, 1450.47/s)  LR: 8.401e-04  Data: 0.009 (0.022)
Train: 158 [ 200/1251 ( 16%)]  Loss: 3.915 (3.83)  Time: 0.696s, 1471.84/s  (0.703s, 1456.25/s)  LR: 8.401e-04  Data: 0.009 (0.019)
Train: 158 [ 250/1251 ( 20%)]  Loss: 4.195 (3.89)  Time: 0.674s, 1518.56/s  (0.704s, 1454.85/s)  LR: 8.401e-04  Data: 0.009 (0.017)
Train: 158 [ 300/1251 ( 24%)]  Loss: 3.909 (3.90)  Time: 0.671s, 1526.49/s  (0.701s, 1460.85/s)  LR: 8.401e-04  Data: 0.010 (0.016)
Train: 158 [ 350/1251 ( 28%)]  Loss: 3.584 (3.86)  Time: 0.717s, 1428.20/s  (0.702s, 1458.74/s)  LR: 8.401e-04  Data: 0.010 (0.015)
Train: 158 [ 400/1251 ( 32%)]  Loss: 3.220 (3.79)  Time: 0.674s, 1518.58/s  (0.701s, 1460.53/s)  LR: 8.401e-04  Data: 0.014 (0.015)
Train: 158 [ 450/1251 ( 36%)]  Loss: 3.511 (3.76)  Time: 0.676s, 1514.75/s  (0.700s, 1462.66/s)  LR: 8.401e-04  Data: 0.009 (0.014)
Train: 158 [ 500/1251 ( 40%)]  Loss: 3.894 (3.77)  Time: 0.706s, 1450.88/s  (0.699s, 1464.65/s)  LR: 8.401e-04  Data: 0.010 (0.014)
Train: 158 [ 550/1251 ( 44%)]  Loss: 3.617 (3.76)  Time: 0.702s, 1459.68/s  (0.698s, 1466.73/s)  LR: 8.401e-04  Data: 0.010 (0.014)
Train: 158 [ 600/1251 ( 48%)]  Loss: 3.861 (3.77)  Time: 0.671s, 1525.48/s  (0.698s, 1467.82/s)  LR: 8.401e-04  Data: 0.010 (0.013)
Train: 158 [ 650/1251 ( 52%)]  Loss: 3.989 (3.78)  Time: 0.696s, 1471.16/s  (0.697s, 1468.76/s)  LR: 8.401e-04  Data: 0.010 (0.013)
Train: 158 [ 700/1251 ( 56%)]  Loss: 3.877 (3.79)  Time: 0.713s, 1435.77/s  (0.697s, 1468.86/s)  LR: 8.401e-04  Data: 0.010 (0.013)
Train: 158 [ 750/1251 ( 60%)]  Loss: 4.053 (3.80)  Time: 0.666s, 1538.06/s  (0.697s, 1469.28/s)  LR: 8.401e-04  Data: 0.010 (0.013)
Train: 158 [ 800/1251 ( 64%)]  Loss: 4.190 (3.83)  Time: 0.705s, 1452.67/s  (0.697s, 1469.24/s)  LR: 8.401e-04  Data: 0.009 (0.012)
Train: 158 [ 850/1251 ( 68%)]  Loss: 3.913 (3.83)  Time: 0.715s, 1433.04/s  (0.697s, 1469.73/s)  LR: 8.401e-04  Data: 0.011 (0.012)
Train: 158 [ 900/1251 ( 72%)]  Loss: 4.244 (3.85)  Time: 0.675s, 1516.12/s  (0.696s, 1470.41/s)  LR: 8.401e-04  Data: 0.010 (0.012)
Train: 158 [ 950/1251 ( 76%)]  Loss: 3.948 (3.86)  Time: 0.685s, 1495.63/s  (0.696s, 1471.61/s)  LR: 8.401e-04  Data: 0.011 (0.012)
Train: 158 [1000/1251 ( 80%)]  Loss: 3.865 (3.86)  Time: 0.673s, 1521.67/s  (0.696s, 1471.90/s)  LR: 8.401e-04  Data: 0.010 (0.012)
Train: 158 [1050/1251 ( 84%)]  Loss: 4.057 (3.87)  Time: 0.717s, 1428.70/s  (0.696s, 1472.26/s)  LR: 8.401e-04  Data: 0.015 (0.012)
Train: 158 [1100/1251 ( 88%)]  Loss: 3.532 (3.85)  Time: 0.676s, 1514.71/s  (0.696s, 1472.17/s)  LR: 8.401e-04  Data: 0.012 (0.012)
Train: 158 [1150/1251 ( 92%)]  Loss: 3.678 (3.85)  Time: 0.677s, 1512.47/s  (0.695s, 1472.39/s)  LR: 8.401e-04  Data: 0.011 (0.012)
Train: 158 [1200/1251 ( 96%)]  Loss: 4.153 (3.86)  Time: 0.674s, 1519.49/s  (0.695s, 1472.34/s)  LR: 8.401e-04  Data: 0.010 (0.012)
Train: 158 [1250/1251 (100%)]  Loss: 3.668 (3.85)  Time: 0.735s, 1392.65/s  (0.695s, 1472.55/s)  LR: 8.401e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.548 (1.548)  Loss:  0.8091 (0.8091)  Acc@1: 87.3047 (87.3047)  Acc@5: 95.9961 (95.9961)
Test: [  48/48]  Time: 0.136 (0.583)  Loss:  1.0791 (1.5629)  Acc@1: 80.3066 (70.3020)  Acc@5: 95.0472 (89.8540)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-150.pth.tar', 70.71399993408203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-157.pth.tar', 70.69799994384766)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-142.pth.tar', 70.64999991210938)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-156.pth.tar', 70.51000012207031)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-154.pth.tar', 70.41000002441406)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-147.pth.tar', 70.40799996826172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-152.pth.tar', 70.36999999511718)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-138.pth.tar', 70.33200009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-158.pth.tar', 70.30200010742188)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-146.pth.tar', 70.30000001953125)

Train: 159 [   0/1251 (  0%)]  Loss: 3.719 (3.72)  Time: 2.315s,  442.39/s  (2.315s,  442.39/s)  LR: 8.381e-04  Data: 1.660 (1.660)
Train: 159 [  50/1251 (  4%)]  Loss: 3.691 (3.70)  Time: 0.710s, 1442.94/s  (0.734s, 1395.53/s)  LR: 8.381e-04  Data: 0.009 (0.050)
Train: 159 [ 100/1251 (  8%)]  Loss: 4.125 (3.84)  Time: 0.702s, 1457.97/s  (0.714s, 1434.21/s)  LR: 8.381e-04  Data: 0.009 (0.030)
Train: 159 [ 150/1251 ( 12%)]  Loss: 4.114 (3.91)  Time: 0.766s, 1337.48/s  (0.706s, 1449.68/s)  LR: 8.381e-04  Data: 0.010 (0.024)
Train: 159 [ 200/1251 ( 16%)]  Loss: 3.714 (3.87)  Time: 0.685s, 1494.15/s  (0.706s, 1450.28/s)  LR: 8.381e-04  Data: 0.012 (0.020)
Train: 159 [ 250/1251 ( 20%)]  Loss: 3.664 (3.84)  Time: 0.710s, 1442.39/s  (0.703s, 1457.31/s)  LR: 8.381e-04  Data: 0.011 (0.018)
Train: 159 [ 300/1251 ( 24%)]  Loss: 3.921 (3.85)  Time: 0.722s, 1418.08/s  (0.702s, 1458.24/s)  LR: 8.381e-04  Data: 0.016 (0.017)
Train: 159 [ 350/1251 ( 28%)]  Loss: 3.998 (3.87)  Time: 0.709s, 1444.73/s  (0.701s, 1459.95/s)  LR: 8.381e-04  Data: 0.011 (0.016)
Train: 159 [ 400/1251 ( 32%)]  Loss: 3.916 (3.87)  Time: 0.670s, 1528.54/s  (0.701s, 1461.30/s)  LR: 8.381e-04  Data: 0.009 (0.015)
Train: 159 [ 450/1251 ( 36%)]  Loss: 4.039 (3.89)  Time: 0.665s, 1539.04/s  (0.700s, 1463.04/s)  LR: 8.381e-04  Data: 0.011 (0.015)
Train: 159 [ 500/1251 ( 40%)]  Loss: 4.121 (3.91)  Time: 0.674s, 1519.90/s  (0.699s, 1465.12/s)  LR: 8.381e-04  Data: 0.013 (0.014)
Train: 159 [ 550/1251 ( 44%)]  Loss: 3.691 (3.89)  Time: 0.673s, 1521.90/s  (0.699s, 1465.64/s)  LR: 8.381e-04  Data: 0.011 (0.014)
Train: 159 [ 600/1251 ( 48%)]  Loss: 3.952 (3.90)  Time: 0.716s, 1431.14/s  (0.699s, 1465.88/s)  LR: 8.381e-04  Data: 0.010 (0.014)
Train: 159 [ 650/1251 ( 52%)]  Loss: 3.824 (3.89)  Time: 0.674s, 1519.44/s  (0.698s, 1467.45/s)  LR: 8.381e-04  Data: 0.013 (0.013)
Train: 159 [ 700/1251 ( 56%)]  Loss: 3.853 (3.89)  Time: 0.726s, 1409.55/s  (0.698s, 1467.42/s)  LR: 8.381e-04  Data: 0.011 (0.013)
Train: 159 [ 750/1251 ( 60%)]  Loss: 3.860 (3.89)  Time: 0.719s, 1423.35/s  (0.698s, 1467.59/s)  LR: 8.381e-04  Data: 0.009 (0.013)
Train: 159 [ 800/1251 ( 64%)]  Loss: 3.646 (3.87)  Time: 0.703s, 1457.08/s  (0.699s, 1465.99/s)  LR: 8.381e-04  Data: 0.012 (0.013)
Train: 159 [ 850/1251 ( 68%)]  Loss: 3.402 (3.85)  Time: 0.672s, 1524.83/s  (0.698s, 1466.89/s)  LR: 8.381e-04  Data: 0.009 (0.013)
Train: 159 [ 900/1251 ( 72%)]  Loss: 3.760 (3.84)  Time: 0.674s, 1520.04/s  (0.698s, 1467.35/s)  LR: 8.381e-04  Data: 0.009 (0.012)
Train: 159 [ 950/1251 ( 76%)]  Loss: 3.919 (3.85)  Time: 0.708s, 1446.52/s  (0.698s, 1467.81/s)  LR: 8.381e-04  Data: 0.009 (0.012)
Train: 159 [1000/1251 ( 80%)]  Loss: 4.032 (3.86)  Time: 0.689s, 1486.16/s  (0.698s, 1467.97/s)  LR: 8.381e-04  Data: 0.011 (0.012)
Train: 159 [1050/1251 ( 84%)]  Loss: 4.068 (3.86)  Time: 0.820s, 1248.33/s  (0.697s, 1468.12/s)  LR: 8.381e-04  Data: 0.009 (0.012)
Train: 159 [1100/1251 ( 88%)]  Loss: 3.676 (3.86)  Time: 0.673s, 1521.54/s  (0.697s, 1468.55/s)  LR: 8.381e-04  Data: 0.010 (0.012)
Train: 159 [1150/1251 ( 92%)]  Loss: 3.674 (3.85)  Time: 0.696s, 1471.98/s  (0.697s, 1469.43/s)  LR: 8.381e-04  Data: 0.009 (0.012)
Train: 159 [1200/1251 ( 96%)]  Loss: 3.657 (3.84)  Time: 0.673s, 1521.44/s  (0.697s, 1470.15/s)  LR: 8.381e-04  Data: 0.009 (0.012)
Train: 159 [1250/1251 (100%)]  Loss: 3.643 (3.83)  Time: 0.656s, 1561.80/s  (0.696s, 1470.64/s)  LR: 8.381e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.590 (1.590)  Loss:  0.9448 (0.9448)  Acc@1: 87.5977 (87.5977)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.136 (0.575)  Loss:  1.1221 (1.5849)  Acc@1: 82.3113 (70.0360)  Acc@5: 95.0472 (89.6160)
Train: 160 [   0/1251 (  0%)]  Loss: 3.518 (3.52)  Time: 2.248s,  455.55/s  (2.248s,  455.55/s)  LR: 8.362e-04  Data: 1.632 (1.632)
Train: 160 [  50/1251 (  4%)]  Loss: 4.176 (3.85)  Time: 0.674s, 1520.10/s  (0.723s, 1416.60/s)  LR: 8.362e-04  Data: 0.010 (0.045)
Train: 160 [ 100/1251 (  8%)]  Loss: 3.822 (3.84)  Time: 0.712s, 1439.12/s  (0.710s, 1442.34/s)  LR: 8.362e-04  Data: 0.009 (0.028)
Train: 160 [ 150/1251 ( 12%)]  Loss: 3.729 (3.81)  Time: 0.729s, 1404.50/s  (0.708s, 1446.91/s)  LR: 8.362e-04  Data: 0.011 (0.022)
Train: 160 [ 200/1251 ( 16%)]  Loss: 3.757 (3.80)  Time: 0.751s, 1362.61/s  (0.705s, 1452.67/s)  LR: 8.362e-04  Data: 0.009 (0.019)
Train: 160 [ 250/1251 ( 20%)]  Loss: 3.966 (3.83)  Time: 0.706s, 1450.42/s  (0.702s, 1459.59/s)  LR: 8.362e-04  Data: 0.011 (0.017)
Train: 160 [ 300/1251 ( 24%)]  Loss: 3.823 (3.83)  Time: 0.717s, 1427.33/s  (0.699s, 1464.02/s)  LR: 8.362e-04  Data: 0.009 (0.016)
Train: 160 [ 350/1251 ( 28%)]  Loss: 4.107 (3.86)  Time: 0.675s, 1517.43/s  (0.699s, 1464.25/s)  LR: 8.362e-04  Data: 0.010 (0.015)
Train: 160 [ 400/1251 ( 32%)]  Loss: 3.716 (3.85)  Time: 0.701s, 1459.92/s  (0.699s, 1465.04/s)  LR: 8.362e-04  Data: 0.009 (0.015)
Train: 160 [ 450/1251 ( 36%)]  Loss: 3.842 (3.85)  Time: 0.675s, 1517.21/s  (0.697s, 1468.73/s)  LR: 8.362e-04  Data: 0.009 (0.014)
Train: 160 [ 500/1251 ( 40%)]  Loss: 3.819 (3.84)  Time: 0.690s, 1483.07/s  (0.698s, 1467.20/s)  LR: 8.362e-04  Data: 0.011 (0.014)
Train: 160 [ 550/1251 ( 44%)]  Loss: 4.238 (3.88)  Time: 0.706s, 1450.36/s  (0.698s, 1468.00/s)  LR: 8.362e-04  Data: 0.009 (0.013)
Train: 160 [ 600/1251 ( 48%)]  Loss: 3.894 (3.88)  Time: 0.701s, 1459.91/s  (0.697s, 1469.85/s)  LR: 8.362e-04  Data: 0.009 (0.013)
Train: 160 [ 650/1251 ( 52%)]  Loss: 3.592 (3.86)  Time: 0.674s, 1519.65/s  (0.696s, 1470.37/s)  LR: 8.362e-04  Data: 0.011 (0.013)
Train: 160 [ 700/1251 ( 56%)]  Loss: 4.184 (3.88)  Time: 0.699s, 1464.90/s  (0.696s, 1471.28/s)  LR: 8.362e-04  Data: 0.010 (0.013)
Train: 160 [ 750/1251 ( 60%)]  Loss: 3.752 (3.87)  Time: 0.726s, 1409.57/s  (0.695s, 1472.34/s)  LR: 8.362e-04  Data: 0.010 (0.012)
Train: 160 [ 800/1251 ( 64%)]  Loss: 3.358 (3.84)  Time: 0.710s, 1441.46/s  (0.695s, 1473.29/s)  LR: 8.362e-04  Data: 0.009 (0.012)
Train: 160 [ 850/1251 ( 68%)]  Loss: 4.251 (3.86)  Time: 0.672s, 1523.15/s  (0.695s, 1474.35/s)  LR: 8.362e-04  Data: 0.012 (0.012)
Train: 160 [ 900/1251 ( 72%)]  Loss: 3.996 (3.87)  Time: 0.671s, 1525.60/s  (0.695s, 1473.12/s)  LR: 8.362e-04  Data: 0.010 (0.012)
Train: 160 [ 950/1251 ( 76%)]  Loss: 4.207 (3.89)  Time: 0.673s, 1522.05/s  (0.695s, 1473.49/s)  LR: 8.362e-04  Data: 0.009 (0.012)
Train: 160 [1000/1251 ( 80%)]  Loss: 4.020 (3.89)  Time: 0.677s, 1513.54/s  (0.695s, 1474.37/s)  LR: 8.362e-04  Data: 0.010 (0.012)
Train: 160 [1050/1251 ( 84%)]  Loss: 3.648 (3.88)  Time: 0.671s, 1525.08/s  (0.694s, 1474.65/s)  LR: 8.362e-04  Data: 0.009 (0.012)
Train: 160 [1100/1251 ( 88%)]  Loss: 4.017 (3.89)  Time: 0.672s, 1523.45/s  (0.694s, 1474.95/s)  LR: 8.362e-04  Data: 0.011 (0.012)
Train: 160 [1150/1251 ( 92%)]  Loss: 4.028 (3.89)  Time: 0.688s, 1488.76/s  (0.694s, 1475.60/s)  LR: 8.362e-04  Data: 0.009 (0.012)
Train: 160 [1200/1251 ( 96%)]  Loss: 3.761 (3.89)  Time: 0.674s, 1518.16/s  (0.694s, 1475.89/s)  LR: 8.362e-04  Data: 0.010 (0.012)
Train: 160 [1250/1251 (100%)]  Loss: 3.786 (3.88)  Time: 0.657s, 1559.49/s  (0.694s, 1475.89/s)  LR: 8.362e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.462 (1.462)  Loss:  1.0205 (1.0205)  Acc@1: 87.5000 (87.5000)  Acc@5: 95.4102 (95.4102)
Test: [  48/48]  Time: 0.136 (0.575)  Loss:  1.1416 (1.5841)  Acc@1: 82.4292 (70.7960)  Acc@5: 95.5189 (89.9800)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-160.pth.tar', 70.79599999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-150.pth.tar', 70.71399993408203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-157.pth.tar', 70.69799994384766)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-142.pth.tar', 70.64999991210938)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-156.pth.tar', 70.51000012207031)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-154.pth.tar', 70.41000002441406)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-147.pth.tar', 70.40799996826172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-152.pth.tar', 70.36999999511718)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-138.pth.tar', 70.33200009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-158.pth.tar', 70.30200010742188)

Train: 161 [   0/1251 (  0%)]  Loss: 4.002 (4.00)  Time: 2.208s,  463.76/s  (2.208s,  463.76/s)  LR: 8.343e-04  Data: 1.593 (1.593)
Train: 161 [  50/1251 (  4%)]  Loss: 4.097 (4.05)  Time: 0.666s, 1537.31/s  (0.729s, 1403.86/s)  LR: 8.343e-04  Data: 0.010 (0.049)
Train: 161 [ 100/1251 (  8%)]  Loss: 3.946 (4.01)  Time: 0.694s, 1475.84/s  (0.708s, 1446.83/s)  LR: 8.343e-04  Data: 0.009 (0.030)
Train: 161 [ 150/1251 ( 12%)]  Loss: 3.582 (3.91)  Time: 0.671s, 1526.54/s  (0.704s, 1454.56/s)  LR: 8.343e-04  Data: 0.010 (0.023)
Train: 161 [ 200/1251 ( 16%)]  Loss: 3.674 (3.86)  Time: 0.705s, 1451.57/s  (0.700s, 1462.80/s)  LR: 8.343e-04  Data: 0.009 (0.020)
Train: 161 [ 250/1251 ( 20%)]  Loss: 3.634 (3.82)  Time: 0.724s, 1414.53/s  (0.700s, 1463.25/s)  LR: 8.343e-04  Data: 0.009 (0.018)
Train: 161 [ 300/1251 ( 24%)]  Loss: 4.036 (3.85)  Time: 0.717s, 1428.57/s  (0.700s, 1463.35/s)  LR: 8.343e-04  Data: 0.013 (0.017)
Train: 161 [ 350/1251 ( 28%)]  Loss: 3.920 (3.86)  Time: 0.680s, 1506.24/s  (0.699s, 1464.18/s)  LR: 8.343e-04  Data: 0.011 (0.016)
Train: 161 [ 400/1251 ( 32%)]  Loss: 3.778 (3.85)  Time: 0.706s, 1450.34/s  (0.698s, 1466.41/s)  LR: 8.343e-04  Data: 0.009 (0.015)
Train: 161 [ 450/1251 ( 36%)]  Loss: 3.923 (3.86)  Time: 0.702s, 1458.44/s  (0.698s, 1467.15/s)  LR: 8.343e-04  Data: 0.009 (0.015)
Train: 161 [ 500/1251 ( 40%)]  Loss: 3.762 (3.85)  Time: 0.689s, 1485.16/s  (0.698s, 1467.81/s)  LR: 8.343e-04  Data: 0.010 (0.014)
Train: 161 [ 550/1251 ( 44%)]  Loss: 4.041 (3.87)  Time: 0.724s, 1414.27/s  (0.698s, 1467.68/s)  LR: 8.343e-04  Data: 0.009 (0.014)
Train: 161 [ 600/1251 ( 48%)]  Loss: 3.902 (3.87)  Time: 0.706s, 1450.74/s  (0.698s, 1467.67/s)  LR: 8.343e-04  Data: 0.009 (0.014)
Train: 161 [ 650/1251 ( 52%)]  Loss: 3.950 (3.87)  Time: 0.673s, 1521.72/s  (0.698s, 1467.54/s)  LR: 8.343e-04  Data: 0.013 (0.013)
Train: 161 [ 700/1251 ( 56%)]  Loss: 4.235 (3.90)  Time: 0.722s, 1418.05/s  (0.698s, 1467.90/s)  LR: 8.343e-04  Data: 0.011 (0.013)
Train: 161 [ 750/1251 ( 60%)]  Loss: 3.915 (3.90)  Time: 0.672s, 1524.81/s  (0.697s, 1469.25/s)  LR: 8.343e-04  Data: 0.010 (0.013)
Train: 161 [ 800/1251 ( 64%)]  Loss: 3.945 (3.90)  Time: 0.754s, 1358.50/s  (0.698s, 1467.31/s)  LR: 8.343e-04  Data: 0.010 (0.013)
Train: 161 [ 850/1251 ( 68%)]  Loss: 3.739 (3.89)  Time: 0.680s, 1506.76/s  (0.699s, 1465.36/s)  LR: 8.343e-04  Data: 0.013 (0.013)
Train: 161 [ 900/1251 ( 72%)]  Loss: 3.635 (3.88)  Time: 0.864s, 1184.99/s  (0.700s, 1463.36/s)  LR: 8.343e-04  Data: 0.011 (0.013)
Train: 161 [ 950/1251 ( 76%)]  Loss: 3.780 (3.87)  Time: 0.705s, 1451.63/s  (0.700s, 1463.18/s)  LR: 8.343e-04  Data: 0.011 (0.013)
Train: 161 [1000/1251 ( 80%)]  Loss: 3.900 (3.88)  Time: 0.671s, 1526.84/s  (0.699s, 1465.12/s)  LR: 8.343e-04  Data: 0.010 (0.013)
Train: 161 [1050/1251 ( 84%)]  Loss: 3.962 (3.88)  Time: 0.705s, 1453.02/s  (0.698s, 1466.02/s)  LR: 8.343e-04  Data: 0.010 (0.012)
Train: 161 [1100/1251 ( 88%)]  Loss: 3.749 (3.87)  Time: 0.713s, 1435.26/s  (0.698s, 1466.67/s)  LR: 8.343e-04  Data: 0.009 (0.012)
Train: 161 [1150/1251 ( 92%)]  Loss: 3.685 (3.87)  Time: 0.673s, 1522.60/s  (0.698s, 1467.08/s)  LR: 8.343e-04  Data: 0.010 (0.012)
Train: 161 [1200/1251 ( 96%)]  Loss: 3.956 (3.87)  Time: 0.672s, 1524.89/s  (0.698s, 1467.99/s)  LR: 8.343e-04  Data: 0.009 (0.012)
Train: 161 [1250/1251 (100%)]  Loss: 4.090 (3.88)  Time: 0.660s, 1551.71/s  (0.698s, 1467.61/s)  LR: 8.343e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.564 (1.564)  Loss:  0.8940 (0.8940)  Acc@1: 86.5234 (86.5234)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.136 (0.587)  Loss:  0.9717 (1.4921)  Acc@1: 80.8962 (70.4660)  Acc@5: 95.1651 (90.0500)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-160.pth.tar', 70.79599999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-150.pth.tar', 70.71399993408203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-157.pth.tar', 70.69799994384766)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-142.pth.tar', 70.64999991210938)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-156.pth.tar', 70.51000012207031)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-161.pth.tar', 70.46599997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-154.pth.tar', 70.41000002441406)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-147.pth.tar', 70.40799996826172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-152.pth.tar', 70.36999999511718)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-138.pth.tar', 70.33200009277344)

Train: 162 [   0/1251 (  0%)]  Loss: 3.908 (3.91)  Time: 2.240s,  457.05/s  (2.240s,  457.05/s)  LR: 8.323e-04  Data: 1.624 (1.624)
Train: 162 [  50/1251 (  4%)]  Loss: 3.517 (3.71)  Time: 0.722s, 1418.32/s  (0.741s, 1381.71/s)  LR: 8.323e-04  Data: 0.017 (0.050)
Train: 162 [ 100/1251 (  8%)]  Loss: 3.847 (3.76)  Time: 0.673s, 1521.35/s  (0.718s, 1427.18/s)  LR: 8.323e-04  Data: 0.010 (0.030)
Train: 162 [ 150/1251 ( 12%)]  Loss: 3.723 (3.75)  Time: 0.685s, 1494.34/s  (0.709s, 1445.10/s)  LR: 8.323e-04  Data: 0.009 (0.024)
Train: 162 [ 200/1251 ( 16%)]  Loss: 3.890 (3.78)  Time: 0.676s, 1515.73/s  (0.704s, 1455.09/s)  LR: 8.323e-04  Data: 0.010 (0.020)
Train: 162 [ 250/1251 ( 20%)]  Loss: 3.482 (3.73)  Time: 0.728s, 1406.67/s  (0.703s, 1457.61/s)  LR: 8.323e-04  Data: 0.009 (0.018)
Train: 162 [ 300/1251 ( 24%)]  Loss: 4.013 (3.77)  Time: 0.674s, 1519.13/s  (0.700s, 1462.39/s)  LR: 8.323e-04  Data: 0.011 (0.017)
Train: 162 [ 350/1251 ( 28%)]  Loss: 4.141 (3.81)  Time: 0.712s, 1438.68/s  (0.700s, 1462.68/s)  LR: 8.323e-04  Data: 0.016 (0.016)
Train: 162 [ 400/1251 ( 32%)]  Loss: 3.650 (3.80)  Time: 0.693s, 1477.31/s  (0.700s, 1462.46/s)  LR: 8.323e-04  Data: 0.009 (0.015)
Train: 162 [ 450/1251 ( 36%)]  Loss: 3.416 (3.76)  Time: 0.705s, 1451.55/s  (0.700s, 1463.59/s)  LR: 8.323e-04  Data: 0.010 (0.015)
Train: 162 [ 500/1251 ( 40%)]  Loss: 3.381 (3.72)  Time: 0.667s, 1535.88/s  (0.699s, 1464.24/s)  LR: 8.323e-04  Data: 0.011 (0.014)
Train: 162 [ 550/1251 ( 44%)]  Loss: 4.018 (3.75)  Time: 0.680s, 1505.03/s  (0.698s, 1466.36/s)  LR: 8.323e-04  Data: 0.009 (0.014)
Train: 162 [ 600/1251 ( 48%)]  Loss: 3.505 (3.73)  Time: 0.671s, 1526.72/s  (0.698s, 1467.79/s)  LR: 8.323e-04  Data: 0.010 (0.013)
Train: 162 [ 650/1251 ( 52%)]  Loss: 4.288 (3.77)  Time: 0.744s, 1375.57/s  (0.697s, 1469.28/s)  LR: 8.323e-04  Data: 0.009 (0.013)
Train: 162 [ 700/1251 ( 56%)]  Loss: 3.950 (3.78)  Time: 0.730s, 1403.33/s  (0.697s, 1468.46/s)  LR: 8.323e-04  Data: 0.010 (0.013)
Train: 162 [ 750/1251 ( 60%)]  Loss: 3.809 (3.78)  Time: 0.670s, 1527.65/s  (0.697s, 1469.76/s)  LR: 8.323e-04  Data: 0.009 (0.013)
Train: 162 [ 800/1251 ( 64%)]  Loss: 3.628 (3.77)  Time: 0.674s, 1520.39/s  (0.696s, 1470.48/s)  LR: 8.323e-04  Data: 0.011 (0.013)
Train: 162 [ 850/1251 ( 68%)]  Loss: 3.778 (3.77)  Time: 0.729s, 1405.60/s  (0.696s, 1470.22/s)  LR: 8.323e-04  Data: 0.010 (0.013)
Train: 162 [ 900/1251 ( 72%)]  Loss: 3.853 (3.78)  Time: 0.713s, 1435.24/s  (0.696s, 1470.45/s)  LR: 8.323e-04  Data: 0.010 (0.012)
Train: 162 [ 950/1251 ( 76%)]  Loss: 4.056 (3.79)  Time: 0.708s, 1446.95/s  (0.696s, 1471.33/s)  LR: 8.323e-04  Data: 0.009 (0.012)
Train: 162 [1000/1251 ( 80%)]  Loss: 3.930 (3.80)  Time: 0.729s, 1404.90/s  (0.696s, 1470.89/s)  LR: 8.323e-04  Data: 0.013 (0.012)
Train: 162 [1050/1251 ( 84%)]  Loss: 4.045 (3.81)  Time: 0.693s, 1476.71/s  (0.696s, 1471.03/s)  LR: 8.323e-04  Data: 0.010 (0.012)
Train: 162 [1100/1251 ( 88%)]  Loss: 3.770 (3.81)  Time: 0.706s, 1450.64/s  (0.696s, 1470.53/s)  LR: 8.323e-04  Data: 0.011 (0.012)
Train: 162 [1150/1251 ( 92%)]  Loss: 4.115 (3.82)  Time: 0.669s, 1529.72/s  (0.696s, 1471.36/s)  LR: 8.323e-04  Data: 0.008 (0.012)
Train: 162 [1200/1251 ( 96%)]  Loss: 4.154 (3.83)  Time: 0.677s, 1513.22/s  (0.696s, 1472.25/s)  LR: 8.323e-04  Data: 0.011 (0.012)
Train: 162 [1250/1251 (100%)]  Loss: 4.215 (3.85)  Time: 0.695s, 1473.89/s  (0.695s, 1472.62/s)  LR: 8.323e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.541 (1.541)  Loss:  1.0557 (1.0557)  Acc@1: 85.2539 (85.2539)  Acc@5: 96.1914 (96.1914)
Test: [  48/48]  Time: 0.136 (0.580)  Loss:  1.1387 (1.6881)  Acc@1: 83.0189 (69.7520)  Acc@5: 95.4009 (89.7180)
Train: 163 [   0/1251 (  0%)]  Loss: 3.904 (3.90)  Time: 2.224s,  460.40/s  (2.224s,  460.40/s)  LR: 8.304e-04  Data: 1.587 (1.587)
Train: 163 [  50/1251 (  4%)]  Loss: 3.805 (3.85)  Time: 0.704s, 1454.95/s  (0.730s, 1402.67/s)  LR: 8.304e-04  Data: 0.010 (0.048)
Train: 163 [ 100/1251 (  8%)]  Loss: 3.988 (3.90)  Time: 0.671s, 1525.14/s  (0.710s, 1442.22/s)  LR: 8.304e-04  Data: 0.009 (0.029)
Train: 163 [ 150/1251 ( 12%)]  Loss: 3.673 (3.84)  Time: 0.768s, 1333.10/s  (0.705s, 1452.01/s)  LR: 8.304e-04  Data: 0.011 (0.023)
Train: 163 [ 200/1251 ( 16%)]  Loss: 3.621 (3.80)  Time: 0.725s, 1411.53/s  (0.702s, 1458.02/s)  LR: 8.304e-04  Data: 0.009 (0.020)
Train: 163 [ 250/1251 ( 20%)]  Loss: 3.861 (3.81)  Time: 0.671s, 1525.55/s  (0.701s, 1461.62/s)  LR: 8.304e-04  Data: 0.010 (0.018)
Train: 163 [ 300/1251 ( 24%)]  Loss: 3.566 (3.77)  Time: 0.688s, 1488.28/s  (0.699s, 1464.18/s)  LR: 8.304e-04  Data: 0.011 (0.016)
Train: 163 [ 350/1251 ( 28%)]  Loss: 3.670 (3.76)  Time: 0.672s, 1524.63/s  (0.700s, 1462.92/s)  LR: 8.304e-04  Data: 0.012 (0.016)
Train: 163 [ 400/1251 ( 32%)]  Loss: 3.971 (3.78)  Time: 0.699s, 1465.31/s  (0.700s, 1463.52/s)  LR: 8.304e-04  Data: 0.019 (0.015)
Train: 163 [ 450/1251 ( 36%)]  Loss: 3.901 (3.80)  Time: 0.703s, 1456.44/s  (0.699s, 1464.96/s)  LR: 8.304e-04  Data: 0.010 (0.014)
Train: 163 [ 500/1251 ( 40%)]  Loss: 3.773 (3.79)  Time: 0.677s, 1513.00/s  (0.698s, 1466.57/s)  LR: 8.304e-04  Data: 0.010 (0.014)
Train: 163 [ 550/1251 ( 44%)]  Loss: 3.725 (3.79)  Time: 0.706s, 1450.65/s  (0.698s, 1467.21/s)  LR: 8.304e-04  Data: 0.009 (0.014)
Train: 163 [ 600/1251 ( 48%)]  Loss: 3.820 (3.79)  Time: 0.752s, 1361.58/s  (0.698s, 1467.02/s)  LR: 8.304e-04  Data: 0.010 (0.013)
Train: 163 [ 650/1251 ( 52%)]  Loss: 4.056 (3.81)  Time: 0.707s, 1448.63/s  (0.698s, 1466.95/s)  LR: 8.304e-04  Data: 0.010 (0.013)
Train: 163 [ 700/1251 ( 56%)]  Loss: 4.152 (3.83)  Time: 0.673s, 1522.67/s  (0.698s, 1467.19/s)  LR: 8.304e-04  Data: 0.009 (0.013)
Train: 163 [ 750/1251 ( 60%)]  Loss: 3.698 (3.82)  Time: 0.716s, 1429.80/s  (0.698s, 1467.45/s)  LR: 8.304e-04  Data: 0.010 (0.013)
Train: 163 [ 800/1251 ( 64%)]  Loss: 3.626 (3.81)  Time: 0.671s, 1526.34/s  (0.698s, 1467.98/s)  LR: 8.304e-04  Data: 0.009 (0.013)
Train: 163 [ 850/1251 ( 68%)]  Loss: 3.922 (3.82)  Time: 0.669s, 1529.61/s  (0.697s, 1468.99/s)  LR: 8.304e-04  Data: 0.013 (0.012)
Train: 163 [ 900/1251 ( 72%)]  Loss: 3.991 (3.83)  Time: 0.713s, 1436.02/s  (0.697s, 1469.76/s)  LR: 8.304e-04  Data: 0.010 (0.012)
Train: 163 [ 950/1251 ( 76%)]  Loss: 3.773 (3.82)  Time: 0.671s, 1526.07/s  (0.697s, 1469.93/s)  LR: 8.304e-04  Data: 0.013 (0.012)
Train: 163 [1000/1251 ( 80%)]  Loss: 3.822 (3.82)  Time: 0.672s, 1524.79/s  (0.697s, 1469.93/s)  LR: 8.304e-04  Data: 0.010 (0.012)
Train: 163 [1050/1251 ( 84%)]  Loss: 3.745 (3.82)  Time: 0.671s, 1525.40/s  (0.697s, 1469.84/s)  LR: 8.304e-04  Data: 0.009 (0.012)
Train: 163 [1100/1251 ( 88%)]  Loss: 3.705 (3.82)  Time: 0.721s, 1420.93/s  (0.696s, 1470.63/s)  LR: 8.304e-04  Data: 0.010 (0.012)
Train: 163 [1150/1251 ( 92%)]  Loss: 3.648 (3.81)  Time: 0.671s, 1525.95/s  (0.696s, 1470.87/s)  LR: 8.304e-04  Data: 0.011 (0.012)
Train: 163 [1200/1251 ( 96%)]  Loss: 4.224 (3.83)  Time: 0.676s, 1513.90/s  (0.696s, 1470.93/s)  LR: 8.304e-04  Data: 0.011 (0.012)
Train: 163 [1250/1251 (100%)]  Loss: 3.592 (3.82)  Time: 0.692s, 1480.53/s  (0.696s, 1470.58/s)  LR: 8.304e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.479 (1.479)  Loss:  1.0254 (1.0254)  Acc@1: 86.2305 (86.2305)  Acc@5: 96.1914 (96.1914)
Test: [  48/48]  Time: 0.137 (0.591)  Loss:  1.1865 (1.6056)  Acc@1: 80.1887 (70.3200)  Acc@5: 94.5755 (89.8600)
Train: 164 [   0/1251 (  0%)]  Loss: 4.040 (4.04)  Time: 2.536s,  403.80/s  (2.536s,  403.80/s)  LR: 8.284e-04  Data: 1.859 (1.859)
Train: 164 [  50/1251 (  4%)]  Loss: 3.956 (4.00)  Time: 0.719s, 1424.06/s  (0.744s, 1376.53/s)  LR: 8.284e-04  Data: 0.009 (0.059)
Train: 164 [ 100/1251 (  8%)]  Loss: 3.887 (3.96)  Time: 0.712s, 1437.67/s  (0.720s, 1422.18/s)  LR: 8.284e-04  Data: 0.010 (0.035)
Train: 164 [ 150/1251 ( 12%)]  Loss: 3.870 (3.94)  Time: 0.694s, 1475.21/s  (0.714s, 1434.08/s)  LR: 8.284e-04  Data: 0.009 (0.027)
Train: 164 [ 200/1251 ( 16%)]  Loss: 3.937 (3.94)  Time: 0.671s, 1525.97/s  (0.709s, 1443.36/s)  LR: 8.284e-04  Data: 0.009 (0.023)
Train: 164 [ 250/1251 ( 20%)]  Loss: 3.991 (3.95)  Time: 0.739s, 1386.51/s  (0.707s, 1449.08/s)  LR: 8.284e-04  Data: 0.011 (0.020)
Train: 164 [ 300/1251 ( 24%)]  Loss: 3.803 (3.93)  Time: 0.702s, 1458.59/s  (0.703s, 1455.99/s)  LR: 8.284e-04  Data: 0.010 (0.019)
Train: 164 [ 350/1251 ( 28%)]  Loss: 3.991 (3.93)  Time: 0.707s, 1448.05/s  (0.702s, 1458.59/s)  LR: 8.284e-04  Data: 0.010 (0.017)
Train: 164 [ 400/1251 ( 32%)]  Loss: 3.920 (3.93)  Time: 0.675s, 1516.69/s  (0.701s, 1459.85/s)  LR: 8.284e-04  Data: 0.010 (0.016)
Train: 164 [ 450/1251 ( 36%)]  Loss: 3.547 (3.89)  Time: 0.673s, 1521.74/s  (0.701s, 1460.75/s)  LR: 8.284e-04  Data: 0.010 (0.016)
Train: 164 [ 500/1251 ( 40%)]  Loss: 3.497 (3.86)  Time: 0.673s, 1520.68/s  (0.701s, 1461.68/s)  LR: 8.284e-04  Data: 0.010 (0.015)
Train: 164 [ 550/1251 ( 44%)]  Loss: 3.673 (3.84)  Time: 0.724s, 1414.99/s  (0.701s, 1461.64/s)  LR: 8.284e-04  Data: 0.009 (0.015)
Train: 164 [ 600/1251 ( 48%)]  Loss: 3.772 (3.84)  Time: 0.668s, 1532.35/s  (0.700s, 1463.27/s)  LR: 8.284e-04  Data: 0.011 (0.014)
Train: 164 [ 650/1251 ( 52%)]  Loss: 3.510 (3.81)  Time: 0.708s, 1447.09/s  (0.700s, 1463.26/s)  LR: 8.284e-04  Data: 0.011 (0.014)
Train: 164 [ 700/1251 ( 56%)]  Loss: 3.873 (3.82)  Time: 0.675s, 1517.71/s  (0.699s, 1463.97/s)  LR: 8.284e-04  Data: 0.010 (0.014)
Train: 164 [ 750/1251 ( 60%)]  Loss: 3.606 (3.80)  Time: 0.707s, 1449.18/s  (0.699s, 1464.64/s)  LR: 8.284e-04  Data: 0.009 (0.014)
Train: 164 [ 800/1251 ( 64%)]  Loss: 3.909 (3.81)  Time: 0.669s, 1531.08/s  (0.699s, 1465.77/s)  LR: 8.284e-04  Data: 0.010 (0.013)
Train: 164 [ 850/1251 ( 68%)]  Loss: 3.725 (3.81)  Time: 0.667s, 1536.15/s  (0.698s, 1466.53/s)  LR: 8.284e-04  Data: 0.009 (0.013)
Train: 164 [ 900/1251 ( 72%)]  Loss: 3.824 (3.81)  Time: 0.693s, 1477.78/s  (0.698s, 1467.82/s)  LR: 8.284e-04  Data: 0.009 (0.013)
Train: 164 [ 950/1251 ( 76%)]  Loss: 3.489 (3.79)  Time: 0.670s, 1528.81/s  (0.697s, 1468.50/s)  LR: 8.284e-04  Data: 0.009 (0.013)
Train: 164 [1000/1251 ( 80%)]  Loss: 3.985 (3.80)  Time: 0.720s, 1421.57/s  (0.697s, 1468.58/s)  LR: 8.284e-04  Data: 0.012 (0.013)
Train: 164 [1050/1251 ( 84%)]  Loss: 4.080 (3.81)  Time: 0.675s, 1517.66/s  (0.697s, 1468.79/s)  LR: 8.284e-04  Data: 0.010 (0.013)
Train: 164 [1100/1251 ( 88%)]  Loss: 3.959 (3.82)  Time: 0.673s, 1522.16/s  (0.697s, 1468.99/s)  LR: 8.284e-04  Data: 0.010 (0.013)
Train: 164 [1150/1251 ( 92%)]  Loss: 3.703 (3.81)  Time: 0.671s, 1525.43/s  (0.697s, 1469.79/s)  LR: 8.284e-04  Data: 0.010 (0.012)
Train: 164 [1200/1251 ( 96%)]  Loss: 3.853 (3.82)  Time: 0.673s, 1521.85/s  (0.697s, 1470.15/s)  LR: 8.284e-04  Data: 0.011 (0.012)
Train: 164 [1250/1251 (100%)]  Loss: 3.901 (3.82)  Time: 0.695s, 1474.36/s  (0.696s, 1470.62/s)  LR: 8.284e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.498 (1.498)  Loss:  0.8623 (0.8623)  Acc@1: 87.4023 (87.4023)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.136 (0.592)  Loss:  0.8838 (1.4664)  Acc@1: 84.1981 (71.1100)  Acc@5: 95.7547 (90.4240)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-164.pth.tar', 71.10999998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-160.pth.tar', 70.79599999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-150.pth.tar', 70.71399993408203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-157.pth.tar', 70.69799994384766)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-142.pth.tar', 70.64999991210938)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-156.pth.tar', 70.51000012207031)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-161.pth.tar', 70.46599997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-154.pth.tar', 70.41000002441406)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-147.pth.tar', 70.40799996826172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-152.pth.tar', 70.36999999511718)

Train: 165 [   0/1251 (  0%)]  Loss: 3.971 (3.97)  Time: 2.227s,  459.76/s  (2.227s,  459.76/s)  LR: 8.265e-04  Data: 1.612 (1.612)
Train: 165 [  50/1251 (  4%)]  Loss: 3.607 (3.79)  Time: 0.700s, 1462.50/s  (0.732s, 1398.21/s)  LR: 8.265e-04  Data: 0.009 (0.046)
Train: 165 [ 100/1251 (  8%)]  Loss: 3.731 (3.77)  Time: 0.710s, 1441.45/s  (0.713s, 1436.40/s)  LR: 8.265e-04  Data: 0.010 (0.028)
Train: 165 [ 150/1251 ( 12%)]  Loss: 3.974 (3.82)  Time: 0.674s, 1520.13/s  (0.707s, 1448.45/s)  LR: 8.265e-04  Data: 0.011 (0.022)
Train: 165 [ 200/1251 ( 16%)]  Loss: 4.105 (3.88)  Time: 0.710s, 1443.24/s  (0.705s, 1452.68/s)  LR: 8.265e-04  Data: 0.009 (0.019)
Train: 165 [ 250/1251 ( 20%)]  Loss: 4.072 (3.91)  Time: 0.714s, 1434.19/s  (0.704s, 1454.59/s)  LR: 8.265e-04  Data: 0.010 (0.017)
Train: 165 [ 300/1251 ( 24%)]  Loss: 3.960 (3.92)  Time: 0.760s, 1347.62/s  (0.702s, 1459.70/s)  LR: 8.265e-04  Data: 0.010 (0.016)
Train: 165 [ 350/1251 ( 28%)]  Loss: 3.960 (3.92)  Time: 0.706s, 1451.44/s  (0.701s, 1461.35/s)  LR: 8.265e-04  Data: 0.011 (0.015)
Train: 165 [ 400/1251 ( 32%)]  Loss: 4.161 (3.95)  Time: 0.749s, 1366.48/s  (0.700s, 1462.78/s)  LR: 8.265e-04  Data: 0.009 (0.015)
Train: 165 [ 450/1251 ( 36%)]  Loss: 3.731 (3.93)  Time: 0.690s, 1483.70/s  (0.700s, 1463.30/s)  LR: 8.265e-04  Data: 0.010 (0.014)
Train: 165 [ 500/1251 ( 40%)]  Loss: 4.052 (3.94)  Time: 0.701s, 1460.77/s  (0.699s, 1464.45/s)  LR: 8.265e-04  Data: 0.010 (0.014)
Train: 165 [ 550/1251 ( 44%)]  Loss: 4.042 (3.95)  Time: 0.705s, 1452.72/s  (0.699s, 1465.20/s)  LR: 8.265e-04  Data: 0.009 (0.014)
Train: 165 [ 600/1251 ( 48%)]  Loss: 3.692 (3.93)  Time: 0.683s, 1500.35/s  (0.698s, 1467.35/s)  LR: 8.265e-04  Data: 0.010 (0.013)
Train: 165 [ 650/1251 ( 52%)]  Loss: 4.028 (3.93)  Time: 0.672s, 1523.16/s  (0.698s, 1467.44/s)  LR: 8.265e-04  Data: 0.010 (0.013)
Train: 165 [ 700/1251 ( 56%)]  Loss: 3.882 (3.93)  Time: 0.716s, 1430.40/s  (0.698s, 1466.65/s)  LR: 8.265e-04  Data: 0.010 (0.013)
Train: 165 [ 750/1251 ( 60%)]  Loss: 3.682 (3.92)  Time: 0.674s, 1519.81/s  (0.698s, 1467.55/s)  LR: 8.265e-04  Data: 0.011 (0.013)
Train: 165 [ 800/1251 ( 64%)]  Loss: 3.877 (3.91)  Time: 0.669s, 1530.60/s  (0.697s, 1468.61/s)  LR: 8.265e-04  Data: 0.009 (0.013)
Train: 165 [ 850/1251 ( 68%)]  Loss: 3.920 (3.91)  Time: 0.668s, 1532.49/s  (0.697s, 1469.05/s)  LR: 8.265e-04  Data: 0.010 (0.012)
Train: 165 [ 900/1251 ( 72%)]  Loss: 3.926 (3.91)  Time: 0.707s, 1448.86/s  (0.697s, 1469.79/s)  LR: 8.265e-04  Data: 0.009 (0.012)
Train: 165 [ 950/1251 ( 76%)]  Loss: 3.701 (3.90)  Time: 0.677s, 1513.22/s  (0.697s, 1470.15/s)  LR: 8.265e-04  Data: 0.010 (0.012)
Train: 165 [1000/1251 ( 80%)]  Loss: 3.522 (3.89)  Time: 0.710s, 1441.81/s  (0.696s, 1470.80/s)  LR: 8.265e-04  Data: 0.011 (0.012)
Train: 165 [1050/1251 ( 84%)]  Loss: 4.278 (3.90)  Time: 0.702s, 1459.00/s  (0.696s, 1470.86/s)  LR: 8.265e-04  Data: 0.011 (0.012)
Train: 165 [1100/1251 ( 88%)]  Loss: 3.941 (3.91)  Time: 0.683s, 1500.17/s  (0.696s, 1471.45/s)  LR: 8.265e-04  Data: 0.008 (0.012)
Train: 165 [1150/1251 ( 92%)]  Loss: 4.154 (3.92)  Time: 0.692s, 1480.61/s  (0.696s, 1471.60/s)  LR: 8.265e-04  Data: 0.010 (0.012)
Train: 165 [1200/1251 ( 96%)]  Loss: 3.839 (3.91)  Time: 0.673s, 1522.37/s  (0.696s, 1471.73/s)  LR: 8.265e-04  Data: 0.010 (0.012)
Train: 165 [1250/1251 (100%)]  Loss: 4.127 (3.92)  Time: 0.658s, 1556.50/s  (0.696s, 1470.99/s)  LR: 8.265e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.486 (1.486)  Loss:  0.9634 (0.9634)  Acc@1: 88.4766 (88.4766)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.139 (0.582)  Loss:  1.2432 (1.5918)  Acc@1: 81.1321 (70.7620)  Acc@5: 94.4575 (90.0740)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-164.pth.tar', 71.10999998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-160.pth.tar', 70.79599999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-165.pth.tar', 70.76199994873046)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-150.pth.tar', 70.71399993408203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-157.pth.tar', 70.69799994384766)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-142.pth.tar', 70.64999991210938)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-156.pth.tar', 70.51000012207031)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-161.pth.tar', 70.46599997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-154.pth.tar', 70.41000002441406)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-147.pth.tar', 70.40799996826172)

Train: 166 [   0/1251 (  0%)]  Loss: 4.039 (4.04)  Time: 2.192s,  467.21/s  (2.192s,  467.21/s)  LR: 8.245e-04  Data: 1.576 (1.576)
Train: 166 [  50/1251 (  4%)]  Loss: 3.960 (4.00)  Time: 0.669s, 1529.66/s  (0.724s, 1414.36/s)  LR: 8.245e-04  Data: 0.010 (0.048)
Train: 166 [ 100/1251 (  8%)]  Loss: 3.473 (3.82)  Time: 0.692s, 1480.31/s  (0.708s, 1446.64/s)  LR: 8.245e-04  Data: 0.010 (0.029)
Train: 166 [ 150/1251 ( 12%)]  Loss: 3.602 (3.77)  Time: 0.672s, 1524.87/s  (0.702s, 1459.34/s)  LR: 8.245e-04  Data: 0.010 (0.023)
Train: 166 [ 200/1251 ( 16%)]  Loss: 3.695 (3.75)  Time: 0.670s, 1527.65/s  (0.701s, 1459.92/s)  LR: 8.245e-04  Data: 0.009 (0.020)
Train: 166 [ 250/1251 ( 20%)]  Loss: 3.668 (3.74)  Time: 0.704s, 1454.53/s  (0.699s, 1464.55/s)  LR: 8.245e-04  Data: 0.010 (0.018)
Train: 166 [ 300/1251 ( 24%)]  Loss: 3.993 (3.78)  Time: 0.675s, 1518.07/s  (0.698s, 1467.51/s)  LR: 8.245e-04  Data: 0.010 (0.017)
Train: 166 [ 350/1251 ( 28%)]  Loss: 3.731 (3.77)  Time: 0.675s, 1516.99/s  (0.697s, 1469.36/s)  LR: 8.245e-04  Data: 0.010 (0.016)
Train: 166 [ 400/1251 ( 32%)]  Loss: 3.764 (3.77)  Time: 0.676s, 1514.87/s  (0.696s, 1470.63/s)  LR: 8.245e-04  Data: 0.010 (0.015)
Train: 166 [ 450/1251 ( 36%)]  Loss: 3.725 (3.76)  Time: 0.673s, 1522.61/s  (0.695s, 1472.61/s)  LR: 8.245e-04  Data: 0.011 (0.014)
Train: 166 [ 500/1251 ( 40%)]  Loss: 3.312 (3.72)  Time: 0.714s, 1433.40/s  (0.696s, 1472.16/s)  LR: 8.245e-04  Data: 0.009 (0.014)
Train: 166 [ 550/1251 ( 44%)]  Loss: 3.791 (3.73)  Time: 0.706s, 1449.73/s  (0.695s, 1472.88/s)  LR: 8.245e-04  Data: 0.009 (0.014)
Train: 166 [ 600/1251 ( 48%)]  Loss: 3.961 (3.75)  Time: 0.666s, 1536.76/s  (0.695s, 1474.39/s)  LR: 8.245e-04  Data: 0.011 (0.013)
Train: 166 [ 650/1251 ( 52%)]  Loss: 3.740 (3.75)  Time: 0.674s, 1518.21/s  (0.694s, 1474.87/s)  LR: 8.245e-04  Data: 0.010 (0.013)
Train: 166 [ 700/1251 ( 56%)]  Loss: 3.786 (3.75)  Time: 0.691s, 1481.95/s  (0.694s, 1475.46/s)  LR: 8.245e-04  Data: 0.010 (0.013)
Train: 166 [ 750/1251 ( 60%)]  Loss: 3.758 (3.75)  Time: 0.673s, 1521.04/s  (0.694s, 1475.77/s)  LR: 8.245e-04  Data: 0.011 (0.013)
Train: 166 [ 800/1251 ( 64%)]  Loss: 3.938 (3.76)  Time: 0.675s, 1516.53/s  (0.694s, 1475.99/s)  LR: 8.245e-04  Data: 0.013 (0.013)
Train: 166 [ 850/1251 ( 68%)]  Loss: 3.243 (3.73)  Time: 0.678s, 1509.36/s  (0.694s, 1475.66/s)  LR: 8.245e-04  Data: 0.010 (0.012)
Train: 166 [ 900/1251 ( 72%)]  Loss: 3.665 (3.73)  Time: 0.673s, 1521.79/s  (0.694s, 1476.33/s)  LR: 8.245e-04  Data: 0.010 (0.012)
Train: 166 [ 950/1251 ( 76%)]  Loss: 3.958 (3.74)  Time: 0.748s, 1369.13/s  (0.694s, 1475.31/s)  LR: 8.245e-04  Data: 0.010 (0.012)
Train: 166 [1000/1251 ( 80%)]  Loss: 3.680 (3.74)  Time: 0.675s, 1517.20/s  (0.694s, 1474.77/s)  LR: 8.245e-04  Data: 0.010 (0.012)
Train: 166 [1050/1251 ( 84%)]  Loss: 3.796 (3.74)  Time: 0.693s, 1476.64/s  (0.694s, 1475.16/s)  LR: 8.245e-04  Data: 0.009 (0.012)
Train: 166 [1100/1251 ( 88%)]  Loss: 3.424 (3.73)  Time: 0.673s, 1521.74/s  (0.694s, 1475.64/s)  LR: 8.245e-04  Data: 0.010 (0.012)
Train: 166 [1150/1251 ( 92%)]  Loss: 3.776 (3.73)  Time: 0.696s, 1471.85/s  (0.694s, 1476.20/s)  LR: 8.245e-04  Data: 0.009 (0.012)
Train: 166 [1200/1251 ( 96%)]  Loss: 3.709 (3.73)  Time: 0.697s, 1468.50/s  (0.694s, 1475.44/s)  LR: 8.245e-04  Data: 0.010 (0.012)
Train: 166 [1250/1251 (100%)]  Loss: 4.044 (3.74)  Time: 0.693s, 1478.69/s  (0.694s, 1475.13/s)  LR: 8.245e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.450 (1.450)  Loss:  0.9443 (0.9443)  Acc@1: 87.6953 (87.6953)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.136 (0.579)  Loss:  1.0342 (1.5138)  Acc@1: 81.2500 (70.7540)  Acc@5: 95.0472 (89.9420)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-164.pth.tar', 71.10999998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-160.pth.tar', 70.79599999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-165.pth.tar', 70.76199994873046)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-166.pth.tar', 70.754)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-150.pth.tar', 70.71399993408203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-157.pth.tar', 70.69799994384766)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-142.pth.tar', 70.64999991210938)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-156.pth.tar', 70.51000012207031)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-161.pth.tar', 70.46599997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-154.pth.tar', 70.41000002441406)

Train: 167 [   0/1251 (  0%)]  Loss: 3.834 (3.83)  Time: 2.412s,  424.46/s  (2.412s,  424.46/s)  LR: 8.225e-04  Data: 1.778 (1.778)
Train: 167 [  50/1251 (  4%)]  Loss: 3.716 (3.77)  Time: 0.760s, 1347.89/s  (0.731s, 1401.74/s)  LR: 8.225e-04  Data: 0.009 (0.053)
Train: 167 [ 100/1251 (  8%)]  Loss: 3.891 (3.81)  Time: 0.721s, 1420.99/s  (0.717s, 1428.13/s)  LR: 8.225e-04  Data: 0.010 (0.032)
Train: 167 [ 150/1251 ( 12%)]  Loss: 3.602 (3.76)  Time: 0.712s, 1437.41/s  (0.711s, 1440.68/s)  LR: 8.225e-04  Data: 0.010 (0.025)
Train: 167 [ 200/1251 ( 16%)]  Loss: 4.018 (3.81)  Time: 0.675s, 1517.57/s  (0.706s, 1449.50/s)  LR: 8.225e-04  Data: 0.009 (0.021)
Train: 167 [ 250/1251 ( 20%)]  Loss: 4.156 (3.87)  Time: 0.705s, 1452.81/s  (0.705s, 1451.96/s)  LR: 8.225e-04  Data: 0.009 (0.019)
Train: 167 [ 300/1251 ( 24%)]  Loss: 3.929 (3.88)  Time: 0.717s, 1428.34/s  (0.703s, 1456.35/s)  LR: 8.225e-04  Data: 0.010 (0.017)
Train: 167 [ 350/1251 ( 28%)]  Loss: 3.716 (3.86)  Time: 0.671s, 1525.67/s  (0.702s, 1459.38/s)  LR: 8.225e-04  Data: 0.009 (0.016)
Train: 167 [ 400/1251 ( 32%)]  Loss: 3.756 (3.85)  Time: 0.672s, 1524.58/s  (0.701s, 1460.99/s)  LR: 8.225e-04  Data: 0.010 (0.016)
Train: 167 [ 450/1251 ( 36%)]  Loss: 3.548 (3.82)  Time: 0.673s, 1522.40/s  (0.700s, 1462.53/s)  LR: 8.225e-04  Data: 0.009 (0.015)
Train: 167 [ 500/1251 ( 40%)]  Loss: 3.811 (3.82)  Time: 0.719s, 1424.21/s  (0.700s, 1463.74/s)  LR: 8.225e-04  Data: 0.012 (0.015)
Train: 167 [ 550/1251 ( 44%)]  Loss: 3.834 (3.82)  Time: 0.710s, 1443.00/s  (0.699s, 1464.64/s)  LR: 8.225e-04  Data: 0.009 (0.014)
Train: 167 [ 600/1251 ( 48%)]  Loss: 3.885 (3.82)  Time: 0.745s, 1373.83/s  (0.700s, 1463.82/s)  LR: 8.225e-04  Data: 0.009 (0.014)
Train: 167 [ 650/1251 ( 52%)]  Loss: 3.406 (3.79)  Time: 0.672s, 1522.70/s  (0.699s, 1464.93/s)  LR: 8.225e-04  Data: 0.009 (0.014)
Train: 167 [ 700/1251 ( 56%)]  Loss: 3.798 (3.79)  Time: 0.672s, 1523.66/s  (0.699s, 1465.45/s)  LR: 8.225e-04  Data: 0.009 (0.013)
Train: 167 [ 750/1251 ( 60%)]  Loss: 3.779 (3.79)  Time: 0.709s, 1445.19/s  (0.698s, 1466.11/s)  LR: 8.225e-04  Data: 0.011 (0.013)
Train: 167 [ 800/1251 ( 64%)]  Loss: 4.010 (3.81)  Time: 0.672s, 1523.69/s  (0.699s, 1465.83/s)  LR: 8.225e-04  Data: 0.009 (0.013)
Train: 167 [ 850/1251 ( 68%)]  Loss: 3.622 (3.80)  Time: 0.726s, 1411.02/s  (0.698s, 1466.61/s)  LR: 8.225e-04  Data: 0.009 (0.013)
Train: 167 [ 900/1251 ( 72%)]  Loss: 3.551 (3.78)  Time: 0.666s, 1537.75/s  (0.698s, 1466.82/s)  LR: 8.225e-04  Data: 0.010 (0.013)
Train: 167 [ 950/1251 ( 76%)]  Loss: 3.556 (3.77)  Time: 0.699s, 1464.44/s  (0.698s, 1466.64/s)  LR: 8.225e-04  Data: 0.008 (0.013)
Train: 167 [1000/1251 ( 80%)]  Loss: 3.689 (3.77)  Time: 0.700s, 1462.13/s  (0.698s, 1467.08/s)  LR: 8.225e-04  Data: 0.010 (0.012)
Train: 167 [1050/1251 ( 84%)]  Loss: 3.754 (3.77)  Time: 0.673s, 1521.40/s  (0.698s, 1467.42/s)  LR: 8.225e-04  Data: 0.009 (0.012)
Train: 167 [1100/1251 ( 88%)]  Loss: 3.924 (3.77)  Time: 0.672s, 1523.09/s  (0.698s, 1467.33/s)  LR: 8.225e-04  Data: 0.011 (0.012)
Train: 167 [1150/1251 ( 92%)]  Loss: 3.824 (3.78)  Time: 0.729s, 1403.74/s  (0.698s, 1468.02/s)  LR: 8.225e-04  Data: 0.009 (0.012)
Train: 167 [1200/1251 ( 96%)]  Loss: 3.410 (3.76)  Time: 0.687s, 1490.64/s  (0.697s, 1468.69/s)  LR: 8.225e-04  Data: 0.013 (0.012)
Train: 167 [1250/1251 (100%)]  Loss: 3.870 (3.76)  Time: 0.656s, 1561.57/s  (0.697s, 1469.09/s)  LR: 8.225e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.525 (1.525)  Loss:  0.9438 (0.9438)  Acc@1: 86.3281 (86.3281)  Acc@5: 96.1914 (96.1914)
Test: [  48/48]  Time: 0.136 (0.573)  Loss:  0.9673 (1.5663)  Acc@1: 82.3113 (71.0380)  Acc@5: 95.2830 (90.1600)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-164.pth.tar', 71.10999998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-167.pth.tar', 71.03800007324219)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-160.pth.tar', 70.79599999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-165.pth.tar', 70.76199994873046)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-166.pth.tar', 70.754)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-150.pth.tar', 70.71399993408203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-157.pth.tar', 70.69799994384766)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-142.pth.tar', 70.64999991210938)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-156.pth.tar', 70.51000012207031)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-161.pth.tar', 70.46599997558594)

Train: 168 [   0/1251 (  0%)]  Loss: 3.727 (3.73)  Time: 2.273s,  450.46/s  (2.273s,  450.46/s)  LR: 8.205e-04  Data: 1.643 (1.643)
Train: 168 [  50/1251 (  4%)]  Loss: 3.927 (3.83)  Time: 0.675s, 1518.14/s  (0.734s, 1395.61/s)  LR: 8.205e-04  Data: 0.011 (0.052)
Train: 168 [ 100/1251 (  8%)]  Loss: 3.438 (3.70)  Time: 0.700s, 1463.22/s  (0.711s, 1439.92/s)  LR: 8.205e-04  Data: 0.009 (0.031)
Train: 168 [ 150/1251 ( 12%)]  Loss: 4.079 (3.79)  Time: 0.667s, 1535.97/s  (0.705s, 1451.91/s)  LR: 8.205e-04  Data: 0.010 (0.024)
Train: 168 [ 200/1251 ( 16%)]  Loss: 3.725 (3.78)  Time: 0.709s, 1444.42/s  (0.703s, 1455.97/s)  LR: 8.205e-04  Data: 0.009 (0.021)
Train: 168 [ 250/1251 ( 20%)]  Loss: 3.775 (3.78)  Time: 0.680s, 1505.58/s  (0.702s, 1459.21/s)  LR: 8.205e-04  Data: 0.010 (0.019)
Train: 168 [ 300/1251 ( 24%)]  Loss: 3.428 (3.73)  Time: 0.743s, 1378.03/s  (0.701s, 1460.42/s)  LR: 8.205e-04  Data: 0.008 (0.017)
Train: 168 [ 350/1251 ( 28%)]  Loss: 3.883 (3.75)  Time: 0.686s, 1492.28/s  (0.701s, 1460.08/s)  LR: 8.205e-04  Data: 0.012 (0.016)
Train: 168 [ 400/1251 ( 32%)]  Loss: 3.361 (3.70)  Time: 0.673s, 1521.88/s  (0.700s, 1463.35/s)  LR: 8.205e-04  Data: 0.011 (0.016)
Train: 168 [ 450/1251 ( 36%)]  Loss: 3.957 (3.73)  Time: 0.694s, 1475.84/s  (0.699s, 1465.29/s)  LR: 8.205e-04  Data: 0.009 (0.015)
Train: 168 [ 500/1251 ( 40%)]  Loss: 4.310 (3.78)  Time: 0.733s, 1396.05/s  (0.698s, 1467.73/s)  LR: 8.205e-04  Data: 0.009 (0.014)
Train: 168 [ 550/1251 ( 44%)]  Loss: 3.636 (3.77)  Time: 0.672s, 1523.79/s  (0.697s, 1469.53/s)  LR: 8.205e-04  Data: 0.010 (0.014)
Train: 168 [ 600/1251 ( 48%)]  Loss: 3.837 (3.78)  Time: 0.666s, 1537.86/s  (0.697s, 1469.66/s)  LR: 8.205e-04  Data: 0.010 (0.014)
Train: 168 [ 650/1251 ( 52%)]  Loss: 4.226 (3.81)  Time: 0.707s, 1448.00/s  (0.696s, 1471.12/s)  LR: 8.205e-04  Data: 0.012 (0.013)
Train: 168 [ 700/1251 ( 56%)]  Loss: 3.615 (3.79)  Time: 0.672s, 1522.93/s  (0.696s, 1472.18/s)  LR: 8.205e-04  Data: 0.010 (0.013)
Train: 168 [ 750/1251 ( 60%)]  Loss: 3.673 (3.79)  Time: 0.669s, 1530.73/s  (0.695s, 1473.42/s)  LR: 8.205e-04  Data: 0.011 (0.013)
Train: 168 [ 800/1251 ( 64%)]  Loss: 3.593 (3.78)  Time: 0.717s, 1427.48/s  (0.695s, 1473.33/s)  LR: 8.205e-04  Data: 0.010 (0.013)
Train: 168 [ 850/1251 ( 68%)]  Loss: 4.267 (3.80)  Time: 0.674s, 1520.32/s  (0.695s, 1473.48/s)  LR: 8.205e-04  Data: 0.010 (0.013)
Train: 168 [ 900/1251 ( 72%)]  Loss: 3.795 (3.80)  Time: 0.703s, 1456.71/s  (0.695s, 1473.52/s)  LR: 8.205e-04  Data: 0.009 (0.012)
Train: 168 [ 950/1251 ( 76%)]  Loss: 3.634 (3.79)  Time: 0.720s, 1422.78/s  (0.695s, 1473.78/s)  LR: 8.205e-04  Data: 0.016 (0.012)
Train: 168 [1000/1251 ( 80%)]  Loss: 4.005 (3.80)  Time: 0.673s, 1520.92/s  (0.695s, 1474.32/s)  LR: 8.205e-04  Data: 0.011 (0.012)
Train: 168 [1050/1251 ( 84%)]  Loss: 3.568 (3.79)  Time: 0.679s, 1507.96/s  (0.694s, 1474.82/s)  LR: 8.205e-04  Data: 0.021 (0.012)
Train: 168 [1100/1251 ( 88%)]  Loss: 3.745 (3.79)  Time: 0.672s, 1522.88/s  (0.694s, 1475.51/s)  LR: 8.205e-04  Data: 0.010 (0.012)
Train: 168 [1150/1251 ( 92%)]  Loss: 4.143 (3.81)  Time: 0.720s, 1421.42/s  (0.694s, 1475.47/s)  LR: 8.205e-04  Data: 0.010 (0.012)
Train: 168 [1200/1251 ( 96%)]  Loss: 3.873 (3.81)  Time: 0.673s, 1522.60/s  (0.694s, 1476.35/s)  LR: 8.205e-04  Data: 0.010 (0.012)
Train: 168 [1250/1251 (100%)]  Loss: 3.911 (3.81)  Time: 0.657s, 1558.46/s  (0.693s, 1476.82/s)  LR: 8.205e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.549 (1.549)  Loss:  0.9287 (0.9287)  Acc@1: 86.4258 (86.4258)  Acc@5: 96.2891 (96.2891)
Test: [  48/48]  Time: 0.136 (0.579)  Loss:  0.9521 (1.5404)  Acc@1: 83.7264 (70.5260)  Acc@5: 94.8113 (89.9920)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-164.pth.tar', 71.10999998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-167.pth.tar', 71.03800007324219)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-160.pth.tar', 70.79599999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-165.pth.tar', 70.76199994873046)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-166.pth.tar', 70.754)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-150.pth.tar', 70.71399993408203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-157.pth.tar', 70.69799994384766)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-142.pth.tar', 70.64999991210938)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-168.pth.tar', 70.5260000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-156.pth.tar', 70.51000012207031)

Train: 169 [   0/1251 (  0%)]  Loss: 3.728 (3.73)  Time: 2.306s,  444.08/s  (2.306s,  444.08/s)  LR: 8.185e-04  Data: 1.673 (1.673)
Train: 169 [  50/1251 (  4%)]  Loss: 3.648 (3.69)  Time: 0.707s, 1447.61/s  (0.733s, 1397.82/s)  LR: 8.185e-04  Data: 0.012 (0.049)
Train: 169 [ 100/1251 (  8%)]  Loss: 3.655 (3.68)  Time: 0.668s, 1532.68/s  (0.709s, 1443.60/s)  LR: 8.185e-04  Data: 0.010 (0.030)
Train: 169 [ 150/1251 ( 12%)]  Loss: 3.766 (3.70)  Time: 0.673s, 1521.48/s  (0.702s, 1458.35/s)  LR: 8.185e-04  Data: 0.009 (0.023)
Train: 169 [ 200/1251 ( 16%)]  Loss: 3.695 (3.70)  Time: 0.675s, 1516.58/s  (0.700s, 1463.15/s)  LR: 8.185e-04  Data: 0.011 (0.020)
Train: 169 [ 250/1251 ( 20%)]  Loss: 3.616 (3.68)  Time: 0.723s, 1415.43/s  (0.698s, 1467.29/s)  LR: 8.185e-04  Data: 0.010 (0.018)
Train: 169 [ 300/1251 ( 24%)]  Loss: 3.567 (3.67)  Time: 0.776s, 1318.97/s  (0.699s, 1466.00/s)  LR: 8.185e-04  Data: 0.011 (0.017)
Train: 169 [ 350/1251 ( 28%)]  Loss: 3.862 (3.69)  Time: 0.678s, 1511.14/s  (0.697s, 1469.79/s)  LR: 8.185e-04  Data: 0.010 (0.016)
Train: 169 [ 400/1251 ( 32%)]  Loss: 3.883 (3.71)  Time: 0.671s, 1525.14/s  (0.697s, 1469.37/s)  LR: 8.185e-04  Data: 0.009 (0.015)
Train: 169 [ 450/1251 ( 36%)]  Loss: 3.869 (3.73)  Time: 0.673s, 1521.74/s  (0.697s, 1468.66/s)  LR: 8.185e-04  Data: 0.010 (0.015)
Train: 169 [ 500/1251 ( 40%)]  Loss: 3.872 (3.74)  Time: 0.693s, 1477.74/s  (0.696s, 1470.71/s)  LR: 8.185e-04  Data: 0.010 (0.014)
Train: 169 [ 550/1251 ( 44%)]  Loss: 3.508 (3.72)  Time: 0.671s, 1526.19/s  (0.696s, 1470.52/s)  LR: 8.185e-04  Data: 0.009 (0.014)
Train: 169 [ 600/1251 ( 48%)]  Loss: 3.929 (3.74)  Time: 0.672s, 1523.44/s  (0.697s, 1469.96/s)  LR: 8.185e-04  Data: 0.010 (0.013)
Train: 169 [ 650/1251 ( 52%)]  Loss: 3.468 (3.72)  Time: 0.672s, 1523.27/s  (0.696s, 1470.48/s)  LR: 8.185e-04  Data: 0.009 (0.013)
Train: 169 [ 700/1251 ( 56%)]  Loss: 3.940 (3.73)  Time: 0.699s, 1464.62/s  (0.696s, 1471.03/s)  LR: 8.185e-04  Data: 0.010 (0.013)
Train: 169 [ 750/1251 ( 60%)]  Loss: 4.119 (3.76)  Time: 0.696s, 1471.90/s  (0.696s, 1471.57/s)  LR: 8.185e-04  Data: 0.010 (0.013)
Train: 169 [ 800/1251 ( 64%)]  Loss: 3.831 (3.76)  Time: 0.710s, 1442.05/s  (0.696s, 1470.72/s)  LR: 8.185e-04  Data: 0.010 (0.013)
Train: 169 [ 850/1251 ( 68%)]  Loss: 3.846 (3.77)  Time: 0.672s, 1523.77/s  (0.696s, 1470.28/s)  LR: 8.185e-04  Data: 0.010 (0.012)
Train: 169 [ 900/1251 ( 72%)]  Loss: 3.735 (3.77)  Time: 0.713s, 1436.70/s  (0.696s, 1471.21/s)  LR: 8.185e-04  Data: 0.009 (0.012)
Train: 169 [ 950/1251 ( 76%)]  Loss: 3.962 (3.78)  Time: 0.716s, 1429.25/s  (0.696s, 1471.75/s)  LR: 8.185e-04  Data: 0.010 (0.012)
Train: 169 [1000/1251 ( 80%)]  Loss: 4.252 (3.80)  Time: 0.719s, 1424.55/s  (0.696s, 1472.21/s)  LR: 8.185e-04  Data: 0.010 (0.012)
Train: 169 [1050/1251 ( 84%)]  Loss: 4.259 (3.82)  Time: 0.673s, 1522.13/s  (0.696s, 1472.24/s)  LR: 8.185e-04  Data: 0.011 (0.012)
Train: 169 [1100/1251 ( 88%)]  Loss: 3.728 (3.81)  Time: 0.676s, 1515.32/s  (0.695s, 1472.63/s)  LR: 8.185e-04  Data: 0.010 (0.012)
Train: 169 [1150/1251 ( 92%)]  Loss: 3.705 (3.81)  Time: 0.689s, 1486.12/s  (0.696s, 1472.21/s)  LR: 8.185e-04  Data: 0.010 (0.012)
Train: 169 [1200/1251 ( 96%)]  Loss: 4.064 (3.82)  Time: 0.671s, 1525.37/s  (0.695s, 1472.81/s)  LR: 8.185e-04  Data: 0.010 (0.012)
Train: 169 [1250/1251 (100%)]  Loss: 3.566 (3.81)  Time: 0.697s, 1469.41/s  (0.696s, 1472.02/s)  LR: 8.185e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.597 (1.597)  Loss:  0.8701 (0.8701)  Acc@1: 87.0117 (87.0117)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.136 (0.588)  Loss:  1.1133 (1.5571)  Acc@1: 81.3679 (70.1540)  Acc@5: 94.9293 (89.9760)
Train: 170 [   0/1251 (  0%)]  Loss: 4.046 (4.05)  Time: 2.142s,  478.08/s  (2.142s,  478.08/s)  LR: 8.165e-04  Data: 1.527 (1.527)
Train: 170 [  50/1251 (  4%)]  Loss: 3.904 (3.97)  Time: 0.679s, 1508.49/s  (0.723s, 1415.98/s)  LR: 8.165e-04  Data: 0.010 (0.046)
Train: 170 [ 100/1251 (  8%)]  Loss: 3.607 (3.85)  Time: 0.710s, 1441.95/s  (0.710s, 1442.93/s)  LR: 8.165e-04  Data: 0.014 (0.028)
Train: 170 [ 150/1251 ( 12%)]  Loss: 3.729 (3.82)  Time: 0.703s, 1456.25/s  (0.704s, 1455.05/s)  LR: 8.165e-04  Data: 0.010 (0.022)
Train: 170 [ 200/1251 ( 16%)]  Loss: 3.630 (3.78)  Time: 0.705s, 1452.80/s  (0.701s, 1461.23/s)  LR: 8.165e-04  Data: 0.010 (0.019)
Train: 170 [ 250/1251 ( 20%)]  Loss: 3.678 (3.77)  Time: 0.705s, 1451.51/s  (0.699s, 1464.35/s)  LR: 8.165e-04  Data: 0.010 (0.017)
Train: 170 [ 300/1251 ( 24%)]  Loss: 3.843 (3.78)  Time: 0.689s, 1486.71/s  (0.698s, 1466.91/s)  LR: 8.165e-04  Data: 0.012 (0.016)
Train: 170 [ 350/1251 ( 28%)]  Loss: 3.793 (3.78)  Time: 0.680s, 1505.95/s  (0.697s, 1468.68/s)  LR: 8.165e-04  Data: 0.011 (0.015)
Train: 170 [ 400/1251 ( 32%)]  Loss: 3.913 (3.79)  Time: 0.669s, 1530.50/s  (0.697s, 1469.80/s)  LR: 8.165e-04  Data: 0.009 (0.015)
Train: 170 [ 450/1251 ( 36%)]  Loss: 3.974 (3.81)  Time: 0.692s, 1480.21/s  (0.696s, 1470.77/s)  LR: 8.165e-04  Data: 0.010 (0.014)
Train: 170 [ 500/1251 ( 40%)]  Loss: 3.634 (3.80)  Time: 0.672s, 1523.78/s  (0.695s, 1472.48/s)  LR: 8.165e-04  Data: 0.011 (0.014)
Train: 170 [ 550/1251 ( 44%)]  Loss: 4.310 (3.84)  Time: 0.673s, 1520.72/s  (0.696s, 1472.26/s)  LR: 8.165e-04  Data: 0.010 (0.013)
Train: 170 [ 600/1251 ( 48%)]  Loss: 4.059 (3.86)  Time: 0.672s, 1523.23/s  (0.696s, 1472.03/s)  LR: 8.165e-04  Data: 0.011 (0.013)
Train: 170 [ 650/1251 ( 52%)]  Loss: 3.697 (3.84)  Time: 0.671s, 1525.87/s  (0.695s, 1473.95/s)  LR: 8.165e-04  Data: 0.010 (0.013)
Train: 170 [ 700/1251 ( 56%)]  Loss: 4.064 (3.86)  Time: 0.729s, 1403.75/s  (0.695s, 1473.83/s)  LR: 8.165e-04  Data: 0.009 (0.013)
Train: 170 [ 750/1251 ( 60%)]  Loss: 3.854 (3.86)  Time: 0.690s, 1484.92/s  (0.695s, 1474.31/s)  LR: 8.165e-04  Data: 0.011 (0.013)
Train: 170 [ 800/1251 ( 64%)]  Loss: 4.140 (3.87)  Time: 0.676s, 1515.84/s  (0.694s, 1474.63/s)  LR: 8.165e-04  Data: 0.010 (0.012)
Train: 170 [ 850/1251 ( 68%)]  Loss: 4.044 (3.88)  Time: 0.670s, 1528.05/s  (0.694s, 1475.35/s)  LR: 8.165e-04  Data: 0.010 (0.012)
Train: 170 [ 900/1251 ( 72%)]  Loss: 3.578 (3.87)  Time: 0.684s, 1497.00/s  (0.694s, 1475.42/s)  LR: 8.165e-04  Data: 0.010 (0.012)
Train: 170 [ 950/1251 ( 76%)]  Loss: 4.087 (3.88)  Time: 0.726s, 1409.87/s  (0.694s, 1475.18/s)  LR: 8.165e-04  Data: 0.009 (0.012)
Train: 170 [1000/1251 ( 80%)]  Loss: 3.720 (3.87)  Time: 0.671s, 1525.10/s  (0.694s, 1475.44/s)  LR: 8.165e-04  Data: 0.011 (0.012)
Train: 170 [1050/1251 ( 84%)]  Loss: 3.420 (3.85)  Time: 0.706s, 1449.55/s  (0.694s, 1475.78/s)  LR: 8.165e-04  Data: 0.010 (0.012)
Train: 170 [1100/1251 ( 88%)]  Loss: 3.906 (3.85)  Time: 0.712s, 1438.11/s  (0.694s, 1475.60/s)  LR: 8.165e-04  Data: 0.010 (0.012)
Train: 170 [1150/1251 ( 92%)]  Loss: 3.906 (3.86)  Time: 0.704s, 1454.85/s  (0.694s, 1475.08/s)  LR: 8.165e-04  Data: 0.009 (0.012)
Train: 170 [1200/1251 ( 96%)]  Loss: 3.699 (3.85)  Time: 0.677s, 1512.56/s  (0.694s, 1474.75/s)  LR: 8.165e-04  Data: 0.010 (0.012)
Train: 170 [1250/1251 (100%)]  Loss: 4.121 (3.86)  Time: 0.689s, 1486.01/s  (0.694s, 1475.05/s)  LR: 8.165e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.566 (1.566)  Loss:  0.9233 (0.9233)  Acc@1: 87.2070 (87.2070)  Acc@5: 95.8984 (95.8984)
Test: [  48/48]  Time: 0.136 (0.570)  Loss:  0.9785 (1.5893)  Acc@1: 81.2500 (70.1620)  Acc@5: 94.6934 (90.0380)
Train: 171 [   0/1251 (  0%)]  Loss: 3.919 (3.92)  Time: 2.554s,  400.95/s  (2.554s,  400.95/s)  LR: 8.145e-04  Data: 1.907 (1.907)
Train: 171 [  50/1251 (  4%)]  Loss: 3.774 (3.85)  Time: 0.697s, 1469.44/s  (0.752s, 1362.03/s)  LR: 8.145e-04  Data: 0.011 (0.051)
Train: 171 [ 100/1251 (  8%)]  Loss: 4.111 (3.93)  Time: 0.691s, 1481.26/s  (0.731s, 1399.94/s)  LR: 8.145e-04  Data: 0.014 (0.032)
Train: 171 [ 150/1251 ( 12%)]  Loss: 3.460 (3.82)  Time: 0.839s, 1221.13/s  (0.726s, 1411.14/s)  LR: 8.145e-04  Data: 0.015 (0.025)
Train: 171 [ 200/1251 ( 16%)]  Loss: 3.867 (3.83)  Time: 0.671s, 1525.40/s  (0.716s, 1430.62/s)  LR: 8.145e-04  Data: 0.009 (0.022)
Train: 171 [ 250/1251 ( 20%)]  Loss: 3.774 (3.82)  Time: 0.704s, 1454.62/s  (0.712s, 1438.91/s)  LR: 8.145e-04  Data: 0.010 (0.019)
Train: 171 [ 300/1251 ( 24%)]  Loss: 3.674 (3.80)  Time: 0.670s, 1529.06/s  (0.711s, 1440.20/s)  LR: 8.145e-04  Data: 0.009 (0.018)
Train: 171 [ 350/1251 ( 28%)]  Loss: 3.567 (3.77)  Time: 0.707s, 1448.69/s  (0.708s, 1445.74/s)  LR: 8.145e-04  Data: 0.009 (0.017)
Train: 171 [ 400/1251 ( 32%)]  Loss: 3.839 (3.78)  Time: 0.667s, 1535.98/s  (0.707s, 1448.67/s)  LR: 8.145e-04  Data: 0.011 (0.016)
Train: 171 [ 450/1251 ( 36%)]  Loss: 3.211 (3.72)  Time: 0.711s, 1440.81/s  (0.706s, 1450.57/s)  LR: 8.145e-04  Data: 0.008 (0.015)
Train: 171 [ 500/1251 ( 40%)]  Loss: 3.959 (3.74)  Time: 0.728s, 1407.40/s  (0.704s, 1453.96/s)  LR: 8.145e-04  Data: 0.009 (0.015)
Train: 171 [ 550/1251 ( 44%)]  Loss: 3.809 (3.75)  Time: 0.690s, 1484.37/s  (0.704s, 1454.92/s)  LR: 8.145e-04  Data: 0.010 (0.014)
Train: 171 [ 600/1251 ( 48%)]  Loss: 3.634 (3.74)  Time: 0.755s, 1356.04/s  (0.703s, 1457.56/s)  LR: 8.145e-04  Data: 0.012 (0.014)
Train: 171 [ 650/1251 ( 52%)]  Loss: 3.888 (3.75)  Time: 0.666s, 1536.69/s  (0.702s, 1458.15/s)  LR: 8.145e-04  Data: 0.009 (0.014)
Train: 171 [ 700/1251 ( 56%)]  Loss: 3.431 (3.73)  Time: 0.753s, 1359.56/s  (0.701s, 1459.85/s)  LR: 8.145e-04  Data: 0.010 (0.014)
Train: 171 [ 750/1251 ( 60%)]  Loss: 3.826 (3.73)  Time: 0.704s, 1454.33/s  (0.701s, 1460.13/s)  LR: 8.145e-04  Data: 0.010 (0.013)
Train: 171 [ 800/1251 ( 64%)]  Loss: 3.463 (3.72)  Time: 0.745s, 1373.61/s  (0.701s, 1460.10/s)  LR: 8.145e-04  Data: 0.009 (0.013)
Train: 171 [ 850/1251 ( 68%)]  Loss: 3.681 (3.72)  Time: 0.692s, 1480.03/s  (0.701s, 1460.44/s)  LR: 8.145e-04  Data: 0.009 (0.013)
Train: 171 [ 900/1251 ( 72%)]  Loss: 4.120 (3.74)  Time: 0.713s, 1435.63/s  (0.701s, 1461.15/s)  LR: 8.145e-04  Data: 0.009 (0.013)
Train: 171 [ 950/1251 ( 76%)]  Loss: 3.668 (3.73)  Time: 0.715s, 1431.89/s  (0.700s, 1462.22/s)  LR: 8.145e-04  Data: 0.010 (0.013)
Train: 171 [1000/1251 ( 80%)]  Loss: 3.983 (3.75)  Time: 0.689s, 1487.22/s  (0.700s, 1463.70/s)  LR: 8.145e-04  Data: 0.013 (0.013)
Train: 171 [1050/1251 ( 84%)]  Loss: 4.028 (3.76)  Time: 0.676s, 1514.64/s  (0.699s, 1464.73/s)  LR: 8.145e-04  Data: 0.013 (0.013)
Train: 171 [1100/1251 ( 88%)]  Loss: 3.970 (3.77)  Time: 0.701s, 1461.56/s  (0.699s, 1465.26/s)  LR: 8.145e-04  Data: 0.011 (0.012)
Train: 171 [1150/1251 ( 92%)]  Loss: 4.252 (3.79)  Time: 0.666s, 1537.27/s  (0.698s, 1466.48/s)  LR: 8.145e-04  Data: 0.008 (0.012)
Train: 171 [1200/1251 ( 96%)]  Loss: 3.992 (3.80)  Time: 0.677s, 1512.61/s  (0.698s, 1466.87/s)  LR: 8.145e-04  Data: 0.010 (0.012)
Train: 171 [1250/1251 (100%)]  Loss: 3.930 (3.80)  Time: 0.657s, 1559.57/s  (0.698s, 1467.91/s)  LR: 8.145e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.489 (1.489)  Loss:  0.9585 (0.9585)  Acc@1: 87.3047 (87.3047)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.136 (0.593)  Loss:  1.1113 (1.5954)  Acc@1: 84.3160 (70.3380)  Acc@5: 96.2264 (89.9760)
Train: 172 [   0/1251 (  0%)]  Loss: 3.692 (3.69)  Time: 2.188s,  467.99/s  (2.188s,  467.99/s)  LR: 8.125e-04  Data: 1.545 (1.545)
Train: 172 [  50/1251 (  4%)]  Loss: 3.931 (3.81)  Time: 0.694s, 1475.13/s  (0.728s, 1406.51/s)  LR: 8.125e-04  Data: 0.010 (0.045)
Train: 172 [ 100/1251 (  8%)]  Loss: 4.228 (3.95)  Time: 0.688s, 1488.87/s  (0.711s, 1440.40/s)  LR: 8.125e-04  Data: 0.009 (0.028)
Train: 172 [ 150/1251 ( 12%)]  Loss: 3.761 (3.90)  Time: 0.711s, 1439.31/s  (0.706s, 1450.84/s)  LR: 8.125e-04  Data: 0.008 (0.022)
Train: 172 [ 200/1251 ( 16%)]  Loss: 3.849 (3.89)  Time: 0.706s, 1450.38/s  (0.705s, 1452.16/s)  LR: 8.125e-04  Data: 0.011 (0.019)
Train: 172 [ 250/1251 ( 20%)]  Loss: 4.304 (3.96)  Time: 0.692s, 1480.77/s  (0.703s, 1457.56/s)  LR: 8.125e-04  Data: 0.010 (0.017)
Train: 172 [ 300/1251 ( 24%)]  Loss: 3.781 (3.94)  Time: 0.753s, 1359.26/s  (0.700s, 1463.30/s)  LR: 8.125e-04  Data: 0.010 (0.016)
Train: 172 [ 350/1251 ( 28%)]  Loss: 3.784 (3.92)  Time: 0.755s, 1355.63/s  (0.700s, 1462.52/s)  LR: 8.125e-04  Data: 0.010 (0.015)
Train: 172 [ 400/1251 ( 32%)]  Loss: 4.078 (3.93)  Time: 0.675s, 1516.53/s  (0.699s, 1464.14/s)  LR: 8.125e-04  Data: 0.009 (0.015)
Train: 172 [ 450/1251 ( 36%)]  Loss: 3.717 (3.91)  Time: 0.673s, 1521.77/s  (0.698s, 1466.79/s)  LR: 8.125e-04  Data: 0.010 (0.014)
Train: 172 [ 500/1251 ( 40%)]  Loss: 3.961 (3.92)  Time: 0.694s, 1475.86/s  (0.698s, 1467.73/s)  LR: 8.125e-04  Data: 0.009 (0.014)
Train: 172 [ 550/1251 ( 44%)]  Loss: 3.865 (3.91)  Time: 0.724s, 1415.02/s  (0.698s, 1467.31/s)  LR: 8.125e-04  Data: 0.010 (0.014)
Train: 172 [ 600/1251 ( 48%)]  Loss: 3.637 (3.89)  Time: 0.673s, 1522.12/s  (0.697s, 1468.75/s)  LR: 8.125e-04  Data: 0.010 (0.013)
Train: 172 [ 650/1251 ( 52%)]  Loss: 3.669 (3.88)  Time: 0.709s, 1443.48/s  (0.697s, 1469.32/s)  LR: 8.125e-04  Data: 0.009 (0.013)
Train: 172 [ 700/1251 ( 56%)]  Loss: 3.328 (3.84)  Time: 0.686s, 1492.80/s  (0.696s, 1470.48/s)  LR: 8.125e-04  Data: 0.011 (0.013)
Train: 172 [ 750/1251 ( 60%)]  Loss: 3.906 (3.84)  Time: 0.708s, 1445.69/s  (0.696s, 1470.64/s)  LR: 8.125e-04  Data: 0.010 (0.013)
Train: 172 [ 800/1251 ( 64%)]  Loss: 3.774 (3.84)  Time: 0.691s, 1481.26/s  (0.697s, 1469.93/s)  LR: 8.125e-04  Data: 0.014 (0.013)
Train: 172 [ 850/1251 ( 68%)]  Loss: 4.022 (3.85)  Time: 0.667s, 1534.09/s  (0.696s, 1470.37/s)  LR: 8.125e-04  Data: 0.009 (0.012)
Train: 172 [ 900/1251 ( 72%)]  Loss: 3.868 (3.85)  Time: 0.674s, 1519.35/s  (0.696s, 1471.04/s)  LR: 8.125e-04  Data: 0.009 (0.012)
Train: 172 [ 950/1251 ( 76%)]  Loss: 3.825 (3.85)  Time: 0.673s, 1521.49/s  (0.696s, 1471.65/s)  LR: 8.125e-04  Data: 0.010 (0.012)
Train: 172 [1000/1251 ( 80%)]  Loss: 3.690 (3.84)  Time: 0.678s, 1510.76/s  (0.696s, 1472.06/s)  LR: 8.125e-04  Data: 0.009 (0.012)
Train: 172 [1050/1251 ( 84%)]  Loss: 3.737 (3.84)  Time: 0.664s, 1541.10/s  (0.696s, 1472.28/s)  LR: 8.125e-04  Data: 0.009 (0.012)
Train: 172 [1100/1251 ( 88%)]  Loss: 3.715 (3.83)  Time: 0.692s, 1480.66/s  (0.695s, 1472.48/s)  LR: 8.125e-04  Data: 0.009 (0.012)
Train: 172 [1150/1251 ( 92%)]  Loss: 3.477 (3.82)  Time: 0.708s, 1445.91/s  (0.696s, 1471.94/s)  LR: 8.125e-04  Data: 0.009 (0.012)
Train: 172 [1200/1251 ( 96%)]  Loss: 3.852 (3.82)  Time: 0.717s, 1428.78/s  (0.696s, 1471.85/s)  LR: 8.125e-04  Data: 0.009 (0.012)
Train: 172 [1250/1251 (100%)]  Loss: 3.484 (3.81)  Time: 0.662s, 1547.94/s  (0.696s, 1472.24/s)  LR: 8.125e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.599 (1.599)  Loss:  0.9292 (0.9292)  Acc@1: 85.9375 (85.9375)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.136 (0.579)  Loss:  1.0234 (1.5582)  Acc@1: 82.5472 (70.6240)  Acc@5: 94.8113 (90.1720)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-164.pth.tar', 71.10999998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-167.pth.tar', 71.03800007324219)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-160.pth.tar', 70.79599999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-165.pth.tar', 70.76199994873046)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-166.pth.tar', 70.754)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-150.pth.tar', 70.71399993408203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-157.pth.tar', 70.69799994384766)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-142.pth.tar', 70.64999991210938)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-172.pth.tar', 70.62400004638671)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-168.pth.tar', 70.5260000415039)

Train: 173 [   0/1251 (  0%)]  Loss: 3.762 (3.76)  Time: 2.140s,  478.43/s  (2.140s,  478.43/s)  LR: 8.104e-04  Data: 1.509 (1.509)
Train: 173 [  50/1251 (  4%)]  Loss: 3.741 (3.75)  Time: 0.674s, 1520.20/s  (0.731s, 1400.05/s)  LR: 8.104e-04  Data: 0.009 (0.051)
Train: 173 [ 100/1251 (  8%)]  Loss: 3.804 (3.77)  Time: 0.706s, 1451.31/s  (0.713s, 1435.85/s)  LR: 8.104e-04  Data: 0.009 (0.031)
Train: 173 [ 150/1251 ( 12%)]  Loss: 3.889 (3.80)  Time: 0.670s, 1529.42/s  (0.709s, 1445.26/s)  LR: 8.104e-04  Data: 0.009 (0.024)
Train: 173 [ 200/1251 ( 16%)]  Loss: 3.782 (3.80)  Time: 0.675s, 1518.02/s  (0.703s, 1457.63/s)  LR: 8.104e-04  Data: 0.011 (0.021)
Train: 173 [ 250/1251 ( 20%)]  Loss: 4.135 (3.85)  Time: 0.716s, 1429.90/s  (0.701s, 1460.23/s)  LR: 8.104e-04  Data: 0.010 (0.019)
Train: 173 [ 300/1251 ( 24%)]  Loss: 3.440 (3.79)  Time: 0.697s, 1469.08/s  (0.701s, 1461.25/s)  LR: 8.104e-04  Data: 0.011 (0.017)
Train: 173 [ 350/1251 ( 28%)]  Loss: 3.418 (3.75)  Time: 0.686s, 1491.98/s  (0.701s, 1461.15/s)  LR: 8.104e-04  Data: 0.016 (0.016)
Train: 173 [ 400/1251 ( 32%)]  Loss: 3.646 (3.74)  Time: 0.680s, 1506.87/s  (0.700s, 1463.63/s)  LR: 8.104e-04  Data: 0.010 (0.016)
Train: 173 [ 450/1251 ( 36%)]  Loss: 4.077 (3.77)  Time: 0.674s, 1520.34/s  (0.699s, 1464.53/s)  LR: 8.104e-04  Data: 0.011 (0.015)
Train: 173 [ 500/1251 ( 40%)]  Loss: 3.927 (3.78)  Time: 0.696s, 1471.48/s  (0.699s, 1465.73/s)  LR: 8.104e-04  Data: 0.010 (0.015)
Train: 173 [ 550/1251 ( 44%)]  Loss: 3.748 (3.78)  Time: 0.679s, 1507.82/s  (0.699s, 1465.84/s)  LR: 8.104e-04  Data: 0.010 (0.014)
Train: 173 [ 600/1251 ( 48%)]  Loss: 4.008 (3.80)  Time: 0.706s, 1450.82/s  (0.698s, 1467.64/s)  LR: 8.104e-04  Data: 0.010 (0.014)
Train: 173 [ 650/1251 ( 52%)]  Loss: 3.702 (3.79)  Time: 0.706s, 1449.94/s  (0.698s, 1467.35/s)  LR: 8.104e-04  Data: 0.010 (0.014)
Train: 173 [ 700/1251 ( 56%)]  Loss: 4.365 (3.83)  Time: 0.671s, 1526.01/s  (0.697s, 1469.02/s)  LR: 8.104e-04  Data: 0.010 (0.013)
Train: 173 [ 750/1251 ( 60%)]  Loss: 3.748 (3.82)  Time: 0.670s, 1529.09/s  (0.697s, 1469.89/s)  LR: 8.104e-04  Data: 0.009 (0.013)
Train: 173 [ 800/1251 ( 64%)]  Loss: 3.871 (3.83)  Time: 0.700s, 1463.68/s  (0.697s, 1470.06/s)  LR: 8.104e-04  Data: 0.009 (0.013)
Train: 173 [ 850/1251 ( 68%)]  Loss: 3.992 (3.84)  Time: 0.699s, 1463.99/s  (0.696s, 1471.70/s)  LR: 8.104e-04  Data: 0.009 (0.013)
Train: 173 [ 900/1251 ( 72%)]  Loss: 4.277 (3.86)  Time: 0.669s, 1530.34/s  (0.695s, 1472.73/s)  LR: 8.104e-04  Data: 0.009 (0.013)
Train: 173 [ 950/1251 ( 76%)]  Loss: 3.345 (3.83)  Time: 0.716s, 1429.17/s  (0.695s, 1472.71/s)  LR: 8.104e-04  Data: 0.010 (0.012)
Train: 173 [1000/1251 ( 80%)]  Loss: 4.143 (3.85)  Time: 0.671s, 1525.38/s  (0.696s, 1472.08/s)  LR: 8.104e-04  Data: 0.010 (0.012)
Train: 173 [1050/1251 ( 84%)]  Loss: 4.053 (3.86)  Time: 0.671s, 1526.62/s  (0.696s, 1471.93/s)  LR: 8.104e-04  Data: 0.010 (0.012)
Train: 173 [1100/1251 ( 88%)]  Loss: 3.860 (3.86)  Time: 0.735s, 1392.94/s  (0.695s, 1472.84/s)  LR: 8.104e-04  Data: 0.009 (0.012)
Train: 173 [1150/1251 ( 92%)]  Loss: 3.957 (3.86)  Time: 0.703s, 1456.51/s  (0.695s, 1473.10/s)  LR: 8.104e-04  Data: 0.012 (0.012)
Train: 173 [1200/1251 ( 96%)]  Loss: 3.714 (3.86)  Time: 0.683s, 1500.15/s  (0.695s, 1473.89/s)  LR: 8.104e-04  Data: 0.010 (0.012)
Train: 173 [1250/1251 (100%)]  Loss: 3.774 (3.85)  Time: 0.655s, 1562.91/s  (0.695s, 1473.53/s)  LR: 8.104e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.619 (1.619)  Loss:  0.8867 (0.8867)  Acc@1: 86.8164 (86.8164)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.136 (0.589)  Loss:  0.9976 (1.5556)  Acc@1: 84.1981 (70.8580)  Acc@5: 95.1651 (90.2240)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-164.pth.tar', 71.10999998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-167.pth.tar', 71.03800007324219)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-173.pth.tar', 70.8580001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-160.pth.tar', 70.79599999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-165.pth.tar', 70.76199994873046)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-166.pth.tar', 70.754)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-150.pth.tar', 70.71399993408203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-157.pth.tar', 70.69799994384766)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-142.pth.tar', 70.64999991210938)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-172.pth.tar', 70.62400004638671)

Train: 174 [   0/1251 (  0%)]  Loss: 3.856 (3.86)  Time: 2.095s,  488.71/s  (2.095s,  488.71/s)  LR: 8.084e-04  Data: 1.482 (1.482)
Train: 174 [  50/1251 (  4%)]  Loss: 4.159 (4.01)  Time: 0.676s, 1515.39/s  (0.725s, 1411.78/s)  LR: 8.084e-04  Data: 0.010 (0.050)
Train: 174 [ 100/1251 (  8%)]  Loss: 3.838 (3.95)  Time: 0.672s, 1523.32/s  (0.709s, 1444.98/s)  LR: 8.084e-04  Data: 0.011 (0.030)
Train: 174 [ 150/1251 ( 12%)]  Loss: 3.548 (3.85)  Time: 0.671s, 1526.75/s  (0.702s, 1458.09/s)  LR: 8.084e-04  Data: 0.009 (0.023)
Train: 174 [ 200/1251 ( 16%)]  Loss: 3.848 (3.85)  Time: 0.693s, 1477.01/s  (0.699s, 1464.73/s)  LR: 8.084e-04  Data: 0.011 (0.020)
Train: 174 [ 250/1251 ( 20%)]  Loss: 3.903 (3.86)  Time: 0.673s, 1520.49/s  (0.697s, 1470.06/s)  LR: 8.084e-04  Data: 0.010 (0.018)
Train: 174 [ 300/1251 ( 24%)]  Loss: 3.649 (3.83)  Time: 0.727s, 1408.63/s  (0.696s, 1471.99/s)  LR: 8.084e-04  Data: 0.012 (0.017)
Train: 174 [ 350/1251 ( 28%)]  Loss: 3.909 (3.84)  Time: 0.674s, 1518.26/s  (0.696s, 1470.81/s)  LR: 8.084e-04  Data: 0.010 (0.016)
Train: 174 [ 400/1251 ( 32%)]  Loss: 3.772 (3.83)  Time: 0.677s, 1513.64/s  (0.696s, 1471.72/s)  LR: 8.084e-04  Data: 0.010 (0.015)
Train: 174 [ 450/1251 ( 36%)]  Loss: 3.804 (3.83)  Time: 0.703s, 1456.90/s  (0.696s, 1470.88/s)  LR: 8.084e-04  Data: 0.009 (0.015)
Train: 174 [ 500/1251 ( 40%)]  Loss: 4.023 (3.85)  Time: 0.674s, 1518.22/s  (0.696s, 1471.67/s)  LR: 8.084e-04  Data: 0.010 (0.014)
Train: 174 [ 550/1251 ( 44%)]  Loss: 3.655 (3.83)  Time: 0.674s, 1518.23/s  (0.696s, 1470.51/s)  LR: 8.084e-04  Data: 0.012 (0.014)
Train: 174 [ 600/1251 ( 48%)]  Loss: 3.701 (3.82)  Time: 0.672s, 1524.74/s  (0.696s, 1471.93/s)  LR: 8.084e-04  Data: 0.009 (0.014)
Train: 174 [ 650/1251 ( 52%)]  Loss: 3.972 (3.83)  Time: 0.700s, 1463.10/s  (0.696s, 1472.31/s)  LR: 8.084e-04  Data: 0.011 (0.013)
Train: 174 [ 700/1251 ( 56%)]  Loss: 3.871 (3.83)  Time: 0.721s, 1420.39/s  (0.696s, 1472.30/s)  LR: 8.084e-04  Data: 0.010 (0.013)
Train: 174 [ 750/1251 ( 60%)]  Loss: 3.952 (3.84)  Time: 0.673s, 1521.24/s  (0.696s, 1472.05/s)  LR: 8.084e-04  Data: 0.010 (0.013)
Train: 174 [ 800/1251 ( 64%)]  Loss: 3.469 (3.82)  Time: 0.669s, 1531.51/s  (0.696s, 1471.70/s)  LR: 8.084e-04  Data: 0.009 (0.013)
Train: 174 [ 850/1251 ( 68%)]  Loss: 3.934 (3.83)  Time: 0.685s, 1494.67/s  (0.696s, 1471.56/s)  LR: 8.084e-04  Data: 0.010 (0.013)
Train: 174 [ 900/1251 ( 72%)]  Loss: 3.754 (3.82)  Time: 0.666s, 1537.84/s  (0.696s, 1471.61/s)  LR: 8.084e-04  Data: 0.010 (0.012)
Train: 174 [ 950/1251 ( 76%)]  Loss: 3.697 (3.82)  Time: 0.667s, 1534.96/s  (0.696s, 1471.94/s)  LR: 8.084e-04  Data: 0.011 (0.012)
Train: 174 [1000/1251 ( 80%)]  Loss: 3.619 (3.81)  Time: 0.705s, 1451.89/s  (0.696s, 1472.23/s)  LR: 8.084e-04  Data: 0.010 (0.012)
Train: 174 [1050/1251 ( 84%)]  Loss: 4.144 (3.82)  Time: 0.753s, 1359.37/s  (0.696s, 1470.93/s)  LR: 8.084e-04  Data: 0.009 (0.012)
Train: 174 [1100/1251 ( 88%)]  Loss: 4.179 (3.84)  Time: 0.671s, 1525.39/s  (0.696s, 1470.55/s)  LR: 8.084e-04  Data: 0.009 (0.012)
Train: 174 [1150/1251 ( 92%)]  Loss: 3.551 (3.83)  Time: 0.675s, 1517.61/s  (0.696s, 1470.79/s)  LR: 8.084e-04  Data: 0.011 (0.012)
Train: 174 [1200/1251 ( 96%)]  Loss: 3.783 (3.82)  Time: 0.713s, 1435.96/s  (0.696s, 1470.82/s)  LR: 8.084e-04  Data: 0.010 (0.012)
Train: 174 [1250/1251 (100%)]  Loss: 3.848 (3.82)  Time: 0.691s, 1480.92/s  (0.696s, 1470.29/s)  LR: 8.084e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.597 (1.597)  Loss:  1.0225 (1.0225)  Acc@1: 87.3047 (87.3047)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.137 (0.584)  Loss:  1.1094 (1.5845)  Acc@1: 83.2547 (70.7220)  Acc@5: 95.0472 (90.1680)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-164.pth.tar', 71.10999998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-167.pth.tar', 71.03800007324219)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-173.pth.tar', 70.8580001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-160.pth.tar', 70.79599999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-165.pth.tar', 70.76199994873046)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-166.pth.tar', 70.754)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-174.pth.tar', 70.72199996582032)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-150.pth.tar', 70.71399993408203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-157.pth.tar', 70.69799994384766)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-142.pth.tar', 70.64999991210938)

Train: 175 [   0/1251 (  0%)]  Loss: 4.063 (4.06)  Time: 2.254s,  454.21/s  (2.254s,  454.21/s)  LR: 8.063e-04  Data: 1.604 (1.604)
Train: 175 [  50/1251 (  4%)]  Loss: 4.029 (4.05)  Time: 0.730s, 1403.55/s  (0.735s, 1393.14/s)  LR: 8.063e-04  Data: 0.010 (0.050)
Train: 175 [ 100/1251 (  8%)]  Loss: 3.757 (3.95)  Time: 0.737s, 1388.93/s  (0.717s, 1428.93/s)  LR: 8.063e-04  Data: 0.010 (0.030)
Train: 175 [ 150/1251 ( 12%)]  Loss: 3.949 (3.95)  Time: 0.670s, 1528.92/s  (0.707s, 1448.86/s)  LR: 8.063e-04  Data: 0.010 (0.023)
Train: 175 [ 200/1251 ( 16%)]  Loss: 3.956 (3.95)  Time: 0.678s, 1509.51/s  (0.703s, 1455.75/s)  LR: 8.063e-04  Data: 0.009 (0.020)
Train: 175 [ 250/1251 ( 20%)]  Loss: 3.852 (3.93)  Time: 0.670s, 1528.03/s  (0.701s, 1460.75/s)  LR: 8.063e-04  Data: 0.010 (0.018)
Train: 175 [ 300/1251 ( 24%)]  Loss: 3.780 (3.91)  Time: 0.723s, 1416.35/s  (0.700s, 1463.51/s)  LR: 8.063e-04  Data: 0.009 (0.017)
Train: 175 [ 350/1251 ( 28%)]  Loss: 4.233 (3.95)  Time: 0.672s, 1524.55/s  (0.699s, 1465.39/s)  LR: 8.063e-04  Data: 0.010 (0.016)
Train: 175 [ 400/1251 ( 32%)]  Loss: 3.794 (3.93)  Time: 0.702s, 1457.97/s  (0.699s, 1465.95/s)  LR: 8.063e-04  Data: 0.009 (0.015)
Train: 175 [ 450/1251 ( 36%)]  Loss: 3.778 (3.92)  Time: 0.787s, 1301.09/s  (0.698s, 1466.26/s)  LR: 8.063e-04  Data: 0.010 (0.015)
Train: 175 [ 500/1251 ( 40%)]  Loss: 3.826 (3.91)  Time: 0.671s, 1525.37/s  (0.698s, 1467.59/s)  LR: 8.063e-04  Data: 0.010 (0.014)
Train: 175 [ 550/1251 ( 44%)]  Loss: 3.675 (3.89)  Time: 0.705s, 1452.48/s  (0.698s, 1467.74/s)  LR: 8.063e-04  Data: 0.009 (0.014)
Train: 175 [ 600/1251 ( 48%)]  Loss: 3.704 (3.88)  Time: 0.742s, 1380.51/s  (0.697s, 1469.18/s)  LR: 8.063e-04  Data: 0.009 (0.014)
Train: 175 [ 650/1251 ( 52%)]  Loss: 3.961 (3.88)  Time: 0.703s, 1456.58/s  (0.697s, 1469.55/s)  LR: 8.063e-04  Data: 0.009 (0.013)
Train: 175 [ 700/1251 ( 56%)]  Loss: 3.735 (3.87)  Time: 0.725s, 1412.28/s  (0.697s, 1469.55/s)  LR: 8.063e-04  Data: 0.010 (0.013)
Train: 175 [ 750/1251 ( 60%)]  Loss: 3.723 (3.86)  Time: 0.713s, 1435.86/s  (0.697s, 1469.74/s)  LR: 8.063e-04  Data: 0.009 (0.013)
Train: 175 [ 800/1251 ( 64%)]  Loss: 4.249 (3.89)  Time: 0.672s, 1524.48/s  (0.696s, 1470.49/s)  LR: 8.063e-04  Data: 0.010 (0.013)
Train: 175 [ 850/1251 ( 68%)]  Loss: 3.916 (3.89)  Time: 0.710s, 1441.78/s  (0.696s, 1471.18/s)  LR: 8.063e-04  Data: 0.010 (0.013)
Train: 175 [ 900/1251 ( 72%)]  Loss: 3.952 (3.89)  Time: 0.672s, 1522.87/s  (0.696s, 1471.89/s)  LR: 8.063e-04  Data: 0.010 (0.012)
Train: 175 [ 950/1251 ( 76%)]  Loss: 3.783 (3.89)  Time: 0.682s, 1501.96/s  (0.695s, 1472.35/s)  LR: 8.063e-04  Data: 0.010 (0.012)
Train: 175 [1000/1251 ( 80%)]  Loss: 3.874 (3.89)  Time: 0.701s, 1459.90/s  (0.695s, 1472.39/s)  LR: 8.063e-04  Data: 0.009 (0.012)
Train: 175 [1050/1251 ( 84%)]  Loss: 3.941 (3.89)  Time: 0.758s, 1350.15/s  (0.696s, 1471.79/s)  LR: 8.063e-04  Data: 0.009 (0.012)
Train: 175 [1100/1251 ( 88%)]  Loss: 3.805 (3.88)  Time: 0.671s, 1526.81/s  (0.696s, 1472.17/s)  LR: 8.063e-04  Data: 0.009 (0.012)
Train: 175 [1150/1251 ( 92%)]  Loss: 3.698 (3.88)  Time: 0.705s, 1453.02/s  (0.695s, 1472.41/s)  LR: 8.063e-04  Data: 0.010 (0.012)
Train: 175 [1200/1251 ( 96%)]  Loss: 3.910 (3.88)  Time: 0.704s, 1454.02/s  (0.695s, 1473.30/s)  LR: 8.063e-04  Data: 0.010 (0.012)
Train: 175 [1250/1251 (100%)]  Loss: 4.175 (3.89)  Time: 0.692s, 1479.10/s  (0.695s, 1474.02/s)  LR: 8.063e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.585 (1.585)  Loss:  0.8940 (0.8940)  Acc@1: 87.5000 (87.5000)  Acc@5: 96.1914 (96.1914)
Test: [  48/48]  Time: 0.136 (0.572)  Loss:  0.9790 (1.4734)  Acc@1: 83.7264 (71.1080)  Acc@5: 95.0472 (90.3620)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-164.pth.tar', 71.10999998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-175.pth.tar', 71.1080000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-167.pth.tar', 71.03800007324219)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-173.pth.tar', 70.8580001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-160.pth.tar', 70.79599999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-165.pth.tar', 70.76199994873046)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-166.pth.tar', 70.754)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-174.pth.tar', 70.72199996582032)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-150.pth.tar', 70.71399993408203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-157.pth.tar', 70.69799994384766)

Train: 176 [   0/1251 (  0%)]  Loss: 4.030 (4.03)  Time: 2.434s,  420.69/s  (2.434s,  420.69/s)  LR: 8.043e-04  Data: 1.819 (1.819)
Train: 176 [  50/1251 (  4%)]  Loss: 3.855 (3.94)  Time: 0.684s, 1496.65/s  (0.730s, 1402.29/s)  LR: 8.043e-04  Data: 0.028 (0.055)
Train: 176 [ 100/1251 (  8%)]  Loss: 3.797 (3.89)  Time: 0.673s, 1521.58/s  (0.714s, 1435.10/s)  LR: 8.043e-04  Data: 0.008 (0.033)
Train: 176 [ 150/1251 ( 12%)]  Loss: 3.289 (3.74)  Time: 0.690s, 1483.34/s  (0.709s, 1444.69/s)  LR: 8.043e-04  Data: 0.012 (0.025)
Train: 176 [ 200/1251 ( 16%)]  Loss: 3.726 (3.74)  Time: 0.672s, 1524.76/s  (0.708s, 1446.07/s)  LR: 8.043e-04  Data: 0.010 (0.021)
Train: 176 [ 250/1251 ( 20%)]  Loss: 3.854 (3.76)  Time: 0.675s, 1518.02/s  (0.704s, 1454.14/s)  LR: 8.043e-04  Data: 0.010 (0.019)
Train: 176 [ 300/1251 ( 24%)]  Loss: 3.706 (3.75)  Time: 0.680s, 1506.60/s  (0.702s, 1457.90/s)  LR: 8.043e-04  Data: 0.018 (0.018)
Train: 176 [ 350/1251 ( 28%)]  Loss: 3.642 (3.74)  Time: 0.666s, 1537.41/s  (0.702s, 1458.80/s)  LR: 8.043e-04  Data: 0.010 (0.017)
Train: 176 [ 400/1251 ( 32%)]  Loss: 3.831 (3.75)  Time: 0.673s, 1522.52/s  (0.700s, 1461.90/s)  LR: 8.043e-04  Data: 0.010 (0.016)
Train: 176 [ 450/1251 ( 36%)]  Loss: 4.097 (3.78)  Time: 0.700s, 1462.50/s  (0.700s, 1463.67/s)  LR: 8.043e-04  Data: 0.010 (0.015)
Train: 176 [ 500/1251 ( 40%)]  Loss: 3.716 (3.78)  Time: 0.679s, 1508.33/s  (0.699s, 1463.98/s)  LR: 8.043e-04  Data: 0.012 (0.015)
Train: 176 [ 550/1251 ( 44%)]  Loss: 4.033 (3.80)  Time: 0.671s, 1526.08/s  (0.700s, 1463.16/s)  LR: 8.043e-04  Data: 0.010 (0.014)
Train: 176 [ 600/1251 ( 48%)]  Loss: 3.925 (3.81)  Time: 0.720s, 1421.56/s  (0.699s, 1464.56/s)  LR: 8.043e-04  Data: 0.011 (0.014)
Train: 176 [ 650/1251 ( 52%)]  Loss: 3.430 (3.78)  Time: 0.673s, 1521.94/s  (0.699s, 1464.91/s)  LR: 8.043e-04  Data: 0.010 (0.014)
Train: 176 [ 700/1251 ( 56%)]  Loss: 3.460 (3.76)  Time: 0.691s, 1480.97/s  (0.698s, 1466.32/s)  LR: 8.043e-04  Data: 0.009 (0.013)
Train: 176 [ 750/1251 ( 60%)]  Loss: 3.733 (3.76)  Time: 0.706s, 1450.41/s  (0.698s, 1468.02/s)  LR: 8.043e-04  Data: 0.010 (0.013)
Train: 176 [ 800/1251 ( 64%)]  Loss: 3.453 (3.74)  Time: 0.691s, 1481.01/s  (0.697s, 1468.39/s)  LR: 8.043e-04  Data: 0.010 (0.013)
Train: 176 [ 850/1251 ( 68%)]  Loss: 3.955 (3.75)  Time: 0.698s, 1467.92/s  (0.697s, 1468.46/s)  LR: 8.043e-04  Data: 0.010 (0.013)
Train: 176 [ 900/1251 ( 72%)]  Loss: 3.762 (3.75)  Time: 0.672s, 1524.75/s  (0.698s, 1468.09/s)  LR: 8.043e-04  Data: 0.010 (0.013)
Train: 176 [ 950/1251 ( 76%)]  Loss: 3.717 (3.75)  Time: 0.711s, 1440.10/s  (0.697s, 1468.17/s)  LR: 8.043e-04  Data: 0.016 (0.013)
Train: 176 [1000/1251 ( 80%)]  Loss: 4.070 (3.77)  Time: 0.674s, 1519.14/s  (0.698s, 1468.01/s)  LR: 8.043e-04  Data: 0.010 (0.012)
Train: 176 [1050/1251 ( 84%)]  Loss: 3.758 (3.77)  Time: 0.669s, 1530.52/s  (0.697s, 1469.24/s)  LR: 8.043e-04  Data: 0.010 (0.012)
Train: 176 [1100/1251 ( 88%)]  Loss: 3.999 (3.78)  Time: 0.703s, 1457.15/s  (0.697s, 1469.43/s)  LR: 8.043e-04  Data: 0.009 (0.012)
Train: 176 [1150/1251 ( 92%)]  Loss: 4.003 (3.79)  Time: 0.707s, 1449.33/s  (0.697s, 1469.68/s)  LR: 8.043e-04  Data: 0.010 (0.012)
Train: 176 [1200/1251 ( 96%)]  Loss: 3.576 (3.78)  Time: 0.724s, 1415.25/s  (0.697s, 1469.74/s)  LR: 8.043e-04  Data: 0.009 (0.012)
Train: 176 [1250/1251 (100%)]  Loss: 3.928 (3.78)  Time: 0.655s, 1564.02/s  (0.697s, 1470.07/s)  LR: 8.043e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.530 (1.530)  Loss:  1.0342 (1.0342)  Acc@1: 84.9609 (84.9609)  Acc@5: 95.0195 (95.0195)
Test: [  48/48]  Time: 0.137 (0.583)  Loss:  1.0781 (1.5338)  Acc@1: 81.6038 (70.1880)  Acc@5: 95.6368 (89.8920)
Train: 177 [   0/1251 (  0%)]  Loss: 3.276 (3.28)  Time: 2.268s,  451.55/s  (2.268s,  451.55/s)  LR: 8.022e-04  Data: 1.652 (1.652)
Train: 177 [  50/1251 (  4%)]  Loss: 3.606 (3.44)  Time: 0.706s, 1449.42/s  (0.735s, 1392.49/s)  LR: 8.022e-04  Data: 0.009 (0.052)
Train: 177 [ 100/1251 (  8%)]  Loss: 3.933 (3.61)  Time: 0.672s, 1524.47/s  (0.712s, 1438.06/s)  LR: 8.022e-04  Data: 0.010 (0.031)
Train: 177 [ 150/1251 ( 12%)]  Loss: 3.627 (3.61)  Time: 0.672s, 1523.89/s  (0.703s, 1456.43/s)  LR: 8.022e-04  Data: 0.010 (0.024)
Train: 177 [ 200/1251 ( 16%)]  Loss: 3.931 (3.67)  Time: 0.683s, 1499.24/s  (0.701s, 1461.34/s)  LR: 8.022e-04  Data: 0.009 (0.021)
Train: 177 [ 250/1251 ( 20%)]  Loss: 3.650 (3.67)  Time: 0.694s, 1476.36/s  (0.698s, 1467.60/s)  LR: 8.022e-04  Data: 0.010 (0.018)
Train: 177 [ 300/1251 ( 24%)]  Loss: 4.114 (3.73)  Time: 0.671s, 1526.48/s  (0.697s, 1468.50/s)  LR: 8.022e-04  Data: 0.009 (0.017)
Train: 177 [ 350/1251 ( 28%)]  Loss: 3.853 (3.75)  Time: 0.694s, 1474.55/s  (0.697s, 1469.45/s)  LR: 8.022e-04  Data: 0.010 (0.016)
Train: 177 [ 400/1251 ( 32%)]  Loss: 4.065 (3.78)  Time: 0.718s, 1426.45/s  (0.696s, 1471.54/s)  LR: 8.022e-04  Data: 0.011 (0.015)
Train: 177 [ 450/1251 ( 36%)]  Loss: 4.064 (3.81)  Time: 0.682s, 1502.07/s  (0.696s, 1471.40/s)  LR: 8.022e-04  Data: 0.010 (0.015)
Train: 177 [ 500/1251 ( 40%)]  Loss: 3.983 (3.83)  Time: 0.670s, 1528.65/s  (0.695s, 1472.33/s)  LR: 8.022e-04  Data: 0.010 (0.014)
Train: 177 [ 550/1251 ( 44%)]  Loss: 3.838 (3.83)  Time: 0.672s, 1524.58/s  (0.696s, 1471.21/s)  LR: 8.022e-04  Data: 0.011 (0.014)
Train: 177 [ 600/1251 ( 48%)]  Loss: 3.894 (3.83)  Time: 0.755s, 1356.11/s  (0.696s, 1471.46/s)  LR: 8.022e-04  Data: 0.009 (0.014)
Train: 177 [ 650/1251 ( 52%)]  Loss: 3.813 (3.83)  Time: 0.672s, 1524.00/s  (0.696s, 1472.04/s)  LR: 8.022e-04  Data: 0.010 (0.013)
Train: 177 [ 700/1251 ( 56%)]  Loss: 3.971 (3.84)  Time: 0.671s, 1526.77/s  (0.695s, 1472.83/s)  LR: 8.022e-04  Data: 0.011 (0.013)
Train: 177 [ 750/1251 ( 60%)]  Loss: 3.727 (3.83)  Time: 0.676s, 1514.30/s  (0.695s, 1472.75/s)  LR: 8.022e-04  Data: 0.009 (0.013)
Train: 177 [ 800/1251 ( 64%)]  Loss: 3.437 (3.81)  Time: 0.709s, 1443.85/s  (0.696s, 1471.74/s)  LR: 8.022e-04  Data: 0.010 (0.013)
Train: 177 [ 850/1251 ( 68%)]  Loss: 3.875 (3.81)  Time: 0.671s, 1525.28/s  (0.696s, 1472.14/s)  LR: 8.022e-04  Data: 0.011 (0.013)
Train: 177 [ 900/1251 ( 72%)]  Loss: 4.111 (3.83)  Time: 0.706s, 1449.63/s  (0.695s, 1472.44/s)  LR: 8.022e-04  Data: 0.010 (0.012)
Train: 177 [ 950/1251 ( 76%)]  Loss: 3.647 (3.82)  Time: 0.715s, 1432.95/s  (0.695s, 1472.48/s)  LR: 8.022e-04  Data: 0.010 (0.012)
Train: 177 [1000/1251 ( 80%)]  Loss: 3.720 (3.82)  Time: 0.703s, 1456.10/s  (0.695s, 1472.79/s)  LR: 8.022e-04  Data: 0.010 (0.012)
Train: 177 [1050/1251 ( 84%)]  Loss: 3.609 (3.81)  Time: 0.676s, 1514.56/s  (0.695s, 1473.37/s)  LR: 8.022e-04  Data: 0.009 (0.012)
Train: 177 [1100/1251 ( 88%)]  Loss: 3.601 (3.80)  Time: 0.709s, 1445.30/s  (0.695s, 1472.62/s)  LR: 8.022e-04  Data: 0.009 (0.012)
Train: 177 [1150/1251 ( 92%)]  Loss: 3.960 (3.80)  Time: 0.705s, 1451.83/s  (0.695s, 1472.83/s)  LR: 8.022e-04  Data: 0.008 (0.012)
Train: 177 [1200/1251 ( 96%)]  Loss: 3.868 (3.81)  Time: 0.709s, 1443.31/s  (0.695s, 1473.48/s)  LR: 8.022e-04  Data: 0.009 (0.012)
Train: 177 [1250/1251 (100%)]  Loss: 3.677 (3.80)  Time: 0.657s, 1559.23/s  (0.694s, 1474.68/s)  LR: 8.022e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.541 (1.541)  Loss:  0.7607 (0.7607)  Acc@1: 87.7930 (87.7930)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.136 (0.585)  Loss:  0.9614 (1.4571)  Acc@1: 82.3113 (71.0820)  Acc@5: 95.0472 (90.3500)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-164.pth.tar', 71.10999998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-175.pth.tar', 71.1080000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-177.pth.tar', 71.08200007324218)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-167.pth.tar', 71.03800007324219)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-173.pth.tar', 70.8580001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-160.pth.tar', 70.79599999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-165.pth.tar', 70.76199994873046)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-166.pth.tar', 70.754)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-174.pth.tar', 70.72199996582032)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-150.pth.tar', 70.71399993408203)

Train: 178 [   0/1251 (  0%)]  Loss: 3.675 (3.67)  Time: 2.214s,  462.60/s  (2.214s,  462.60/s)  LR: 8.001e-04  Data: 1.602 (1.602)
Train: 178 [  50/1251 (  4%)]  Loss: 3.774 (3.72)  Time: 0.697s, 1468.90/s  (0.724s, 1413.47/s)  LR: 8.001e-04  Data: 0.010 (0.042)
Train: 178 [ 100/1251 (  8%)]  Loss: 3.878 (3.78)  Time: 0.696s, 1471.56/s  (0.710s, 1441.89/s)  LR: 8.001e-04  Data: 0.010 (0.026)
Train: 178 [ 150/1251 ( 12%)]  Loss: 3.654 (3.75)  Time: 0.674s, 1518.62/s  (0.708s, 1446.44/s)  LR: 8.001e-04  Data: 0.010 (0.021)
Train: 178 [ 200/1251 ( 16%)]  Loss: 3.837 (3.76)  Time: 0.673s, 1521.01/s  (0.702s, 1457.66/s)  LR: 8.001e-04  Data: 0.010 (0.018)
Train: 178 [ 250/1251 ( 20%)]  Loss: 4.074 (3.82)  Time: 0.697s, 1468.19/s  (0.701s, 1461.10/s)  LR: 8.001e-04  Data: 0.009 (0.017)
Train: 178 [ 300/1251 ( 24%)]  Loss: 3.973 (3.84)  Time: 0.699s, 1464.53/s  (0.699s, 1465.17/s)  LR: 8.001e-04  Data: 0.010 (0.016)
Train: 178 [ 350/1251 ( 28%)]  Loss: 3.747 (3.83)  Time: 0.707s, 1449.34/s  (0.698s, 1466.49/s)  LR: 8.001e-04  Data: 0.012 (0.015)
Train: 178 [ 400/1251 ( 32%)]  Loss: 3.756 (3.82)  Time: 0.675s, 1518.12/s  (0.699s, 1465.23/s)  LR: 8.001e-04  Data: 0.009 (0.014)
Train: 178 [ 450/1251 ( 36%)]  Loss: 3.571 (3.79)  Time: 0.704s, 1455.02/s  (0.699s, 1465.38/s)  LR: 8.001e-04  Data: 0.010 (0.014)
Train: 178 [ 500/1251 ( 40%)]  Loss: 3.697 (3.79)  Time: 0.704s, 1453.61/s  (0.699s, 1464.54/s)  LR: 8.001e-04  Data: 0.010 (0.014)
Train: 178 [ 550/1251 ( 44%)]  Loss: 4.121 (3.81)  Time: 0.672s, 1523.97/s  (0.699s, 1464.72/s)  LR: 8.001e-04  Data: 0.011 (0.013)
Train: 178 [ 600/1251 ( 48%)]  Loss: 3.867 (3.82)  Time: 0.681s, 1502.60/s  (0.699s, 1465.16/s)  LR: 8.001e-04  Data: 0.010 (0.013)
Train: 178 [ 650/1251 ( 52%)]  Loss: 3.978 (3.83)  Time: 0.718s, 1427.13/s  (0.698s, 1467.00/s)  LR: 8.001e-04  Data: 0.010 (0.013)
Train: 178 [ 700/1251 ( 56%)]  Loss: 4.075 (3.85)  Time: 0.708s, 1446.97/s  (0.698s, 1466.47/s)  LR: 8.001e-04  Data: 0.010 (0.013)
Train: 178 [ 750/1251 ( 60%)]  Loss: 3.686 (3.84)  Time: 0.724s, 1414.85/s  (0.698s, 1466.75/s)  LR: 8.001e-04  Data: 0.009 (0.012)
Train: 178 [ 800/1251 ( 64%)]  Loss: 3.548 (3.82)  Time: 0.721s, 1420.45/s  (0.698s, 1467.30/s)  LR: 8.001e-04  Data: 0.009 (0.012)
Train: 178 [ 850/1251 ( 68%)]  Loss: 3.739 (3.81)  Time: 0.705s, 1451.89/s  (0.698s, 1467.07/s)  LR: 8.001e-04  Data: 0.010 (0.012)
Train: 178 [ 900/1251 ( 72%)]  Loss: 4.075 (3.83)  Time: 0.677s, 1512.64/s  (0.698s, 1467.64/s)  LR: 8.001e-04  Data: 0.011 (0.012)
Train: 178 [ 950/1251 ( 76%)]  Loss: 3.672 (3.82)  Time: 0.677s, 1512.84/s  (0.697s, 1468.36/s)  LR: 8.001e-04  Data: 0.011 (0.012)
Train: 178 [1000/1251 ( 80%)]  Loss: 3.880 (3.82)  Time: 0.709s, 1443.33/s  (0.698s, 1467.99/s)  LR: 8.001e-04  Data: 0.010 (0.012)
Train: 178 [1050/1251 ( 84%)]  Loss: 3.963 (3.83)  Time: 0.693s, 1478.46/s  (0.698s, 1468.06/s)  LR: 8.001e-04  Data: 0.016 (0.012)
Train: 178 [1100/1251 ( 88%)]  Loss: 3.724 (3.82)  Time: 0.675s, 1517.42/s  (0.697s, 1468.63/s)  LR: 8.001e-04  Data: 0.010 (0.012)
Train: 178 [1150/1251 ( 92%)]  Loss: 3.540 (3.81)  Time: 0.691s, 1482.68/s  (0.697s, 1469.15/s)  LR: 8.001e-04  Data: 0.009 (0.012)
Train: 178 [1200/1251 ( 96%)]  Loss: 3.609 (3.80)  Time: 0.671s, 1527.02/s  (0.697s, 1469.42/s)  LR: 8.001e-04  Data: 0.009 (0.012)
Train: 178 [1250/1251 (100%)]  Loss: 4.166 (3.82)  Time: 0.697s, 1469.10/s  (0.697s, 1469.53/s)  LR: 8.001e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.531 (1.531)  Loss:  0.9692 (0.9692)  Acc@1: 86.2305 (86.2305)  Acc@5: 95.8008 (95.8008)
Test: [  48/48]  Time: 0.136 (0.586)  Loss:  0.9668 (1.5256)  Acc@1: 84.0802 (70.3060)  Acc@5: 95.7547 (89.6700)
Train: 179 [   0/1251 (  0%)]  Loss: 3.970 (3.97)  Time: 2.214s,  462.46/s  (2.214s,  462.46/s)  LR: 7.980e-04  Data: 1.599 (1.599)
Train: 179 [  50/1251 (  4%)]  Loss: 3.670 (3.82)  Time: 0.670s, 1527.54/s  (0.728s, 1407.45/s)  LR: 7.980e-04  Data: 0.010 (0.048)
Train: 179 [ 100/1251 (  8%)]  Loss: 3.630 (3.76)  Time: 0.689s, 1485.43/s  (0.714s, 1433.28/s)  LR: 7.980e-04  Data: 0.012 (0.030)
Train: 179 [ 150/1251 ( 12%)]  Loss: 4.195 (3.87)  Time: 0.761s, 1345.96/s  (0.707s, 1448.44/s)  LR: 7.980e-04  Data: 0.010 (0.023)
Train: 179 [ 200/1251 ( 16%)]  Loss: 3.548 (3.80)  Time: 0.692s, 1479.30/s  (0.702s, 1458.92/s)  LR: 7.980e-04  Data: 0.009 (0.020)
Train: 179 [ 250/1251 ( 20%)]  Loss: 3.320 (3.72)  Time: 0.700s, 1462.10/s  (0.700s, 1461.98/s)  LR: 7.980e-04  Data: 0.009 (0.018)
Train: 179 [ 300/1251 ( 24%)]  Loss: 3.811 (3.73)  Time: 0.746s, 1372.59/s  (0.699s, 1465.01/s)  LR: 7.980e-04  Data: 0.010 (0.017)
Train: 179 [ 350/1251 ( 28%)]  Loss: 3.680 (3.73)  Time: 0.742s, 1380.55/s  (0.701s, 1461.74/s)  LR: 7.980e-04  Data: 0.010 (0.016)
Train: 179 [ 400/1251 ( 32%)]  Loss: 3.832 (3.74)  Time: 0.672s, 1524.19/s  (0.700s, 1462.97/s)  LR: 7.980e-04  Data: 0.010 (0.015)
Train: 179 [ 450/1251 ( 36%)]  Loss: 3.282 (3.69)  Time: 0.672s, 1522.76/s  (0.699s, 1465.51/s)  LR: 7.980e-04  Data: 0.009 (0.015)
Train: 179 [ 500/1251 ( 40%)]  Loss: 3.651 (3.69)  Time: 0.674s, 1520.13/s  (0.697s, 1468.12/s)  LR: 7.980e-04  Data: 0.010 (0.014)
Train: 179 [ 550/1251 ( 44%)]  Loss: 3.844 (3.70)  Time: 0.683s, 1499.27/s  (0.698s, 1468.04/s)  LR: 7.980e-04  Data: 0.013 (0.014)
Train: 179 [ 600/1251 ( 48%)]  Loss: 3.710 (3.70)  Time: 0.670s, 1528.27/s  (0.698s, 1467.32/s)  LR: 7.980e-04  Data: 0.010 (0.014)
Train: 179 [ 650/1251 ( 52%)]  Loss: 3.517 (3.69)  Time: 0.671s, 1526.80/s  (0.698s, 1467.50/s)  LR: 7.980e-04  Data: 0.010 (0.013)
Train: 179 [ 700/1251 ( 56%)]  Loss: 3.673 (3.69)  Time: 0.705s, 1452.48/s  (0.697s, 1468.12/s)  LR: 7.980e-04  Data: 0.011 (0.013)
Train: 179 [ 750/1251 ( 60%)]  Loss: 3.854 (3.70)  Time: 0.674s, 1519.49/s  (0.697s, 1469.19/s)  LR: 7.980e-04  Data: 0.010 (0.013)
Train: 179 [ 800/1251 ( 64%)]  Loss: 3.240 (3.67)  Time: 0.666s, 1536.67/s  (0.696s, 1470.23/s)  LR: 7.980e-04  Data: 0.009 (0.013)
Train: 179 [ 850/1251 ( 68%)]  Loss: 4.009 (3.69)  Time: 0.692s, 1480.35/s  (0.696s, 1471.15/s)  LR: 7.980e-04  Data: 0.012 (0.013)
Train: 179 [ 900/1251 ( 72%)]  Loss: 3.892 (3.70)  Time: 0.695s, 1473.83/s  (0.696s, 1471.60/s)  LR: 7.980e-04  Data: 0.010 (0.012)
Train: 179 [ 950/1251 ( 76%)]  Loss: 3.889 (3.71)  Time: 0.742s, 1380.84/s  (0.696s, 1471.54/s)  LR: 7.980e-04  Data: 0.009 (0.012)
Train: 179 [1000/1251 ( 80%)]  Loss: 3.592 (3.71)  Time: 0.670s, 1528.65/s  (0.696s, 1471.52/s)  LR: 7.980e-04  Data: 0.010 (0.012)
Train: 179 [1050/1251 ( 84%)]  Loss: 3.432 (3.69)  Time: 0.710s, 1441.37/s  (0.696s, 1472.00/s)  LR: 7.980e-04  Data: 0.010 (0.012)
Train: 179 [1100/1251 ( 88%)]  Loss: 3.965 (3.70)  Time: 0.672s, 1523.83/s  (0.695s, 1472.50/s)  LR: 7.980e-04  Data: 0.012 (0.012)
Train: 179 [1150/1251 ( 92%)]  Loss: 4.265 (3.73)  Time: 0.673s, 1521.54/s  (0.695s, 1472.97/s)  LR: 7.980e-04  Data: 0.013 (0.012)
Train: 179 [1200/1251 ( 96%)]  Loss: 3.573 (3.72)  Time: 0.679s, 1508.30/s  (0.695s, 1472.99/s)  LR: 7.980e-04  Data: 0.010 (0.012)
Train: 179 [1250/1251 (100%)]  Loss: 3.842 (3.73)  Time: 0.656s, 1559.93/s  (0.695s, 1473.10/s)  LR: 7.980e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.563 (1.563)  Loss:  0.8516 (0.8516)  Acc@1: 87.5000 (87.5000)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.136 (0.586)  Loss:  0.8862 (1.4715)  Acc@1: 84.5519 (71.2360)  Acc@5: 96.3443 (90.4000)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-179.pth.tar', 71.23600001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-164.pth.tar', 71.10999998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-175.pth.tar', 71.1080000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-177.pth.tar', 71.08200007324218)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-167.pth.tar', 71.03800007324219)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-173.pth.tar', 70.8580001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-160.pth.tar', 70.79599999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-165.pth.tar', 70.76199994873046)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-166.pth.tar', 70.754)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-174.pth.tar', 70.72199996582032)

Train: 180 [   0/1251 (  0%)]  Loss: 3.670 (3.67)  Time: 2.172s,  471.54/s  (2.172s,  471.54/s)  LR: 7.960e-04  Data: 1.556 (1.556)
Train: 180 [  50/1251 (  4%)]  Loss: 3.378 (3.52)  Time: 0.712s, 1437.31/s  (0.725s, 1412.53/s)  LR: 7.960e-04  Data: 0.008 (0.047)
Train: 180 [ 100/1251 (  8%)]  Loss: 4.157 (3.73)  Time: 0.679s, 1508.68/s  (0.713s, 1435.31/s)  LR: 7.960e-04  Data: 0.010 (0.029)
Train: 180 [ 150/1251 ( 12%)]  Loss: 3.591 (3.70)  Time: 0.726s, 1410.71/s  (0.708s, 1447.30/s)  LR: 7.960e-04  Data: 0.013 (0.023)
Train: 180 [ 200/1251 ( 16%)]  Loss: 3.864 (3.73)  Time: 0.670s, 1527.30/s  (0.706s, 1450.82/s)  LR: 7.960e-04  Data: 0.010 (0.020)
Train: 180 [ 250/1251 ( 20%)]  Loss: 3.912 (3.76)  Time: 0.675s, 1516.22/s  (0.704s, 1453.53/s)  LR: 7.960e-04  Data: 0.010 (0.018)
Train: 180 [ 300/1251 ( 24%)]  Loss: 3.551 (3.73)  Time: 0.675s, 1516.01/s  (0.703s, 1456.21/s)  LR: 7.960e-04  Data: 0.011 (0.017)
Train: 180 [ 350/1251 ( 28%)]  Loss: 4.154 (3.78)  Time: 0.694s, 1476.46/s  (0.702s, 1458.59/s)  LR: 7.960e-04  Data: 0.020 (0.016)
Train: 180 [ 400/1251 ( 32%)]  Loss: 3.856 (3.79)  Time: 0.710s, 1441.41/s  (0.701s, 1461.11/s)  LR: 7.960e-04  Data: 0.011 (0.015)
Train: 180 [ 450/1251 ( 36%)]  Loss: 3.916 (3.80)  Time: 0.710s, 1442.82/s  (0.700s, 1462.08/s)  LR: 7.960e-04  Data: 0.009 (0.014)
Train: 180 [ 500/1251 ( 40%)]  Loss: 3.962 (3.82)  Time: 0.685s, 1494.07/s  (0.700s, 1463.87/s)  LR: 7.960e-04  Data: 0.013 (0.014)
Train: 180 [ 550/1251 ( 44%)]  Loss: 3.662 (3.81)  Time: 0.740s, 1382.89/s  (0.700s, 1463.84/s)  LR: 7.960e-04  Data: 0.012 (0.014)
Train: 180 [ 600/1251 ( 48%)]  Loss: 3.584 (3.79)  Time: 0.701s, 1460.54/s  (0.701s, 1461.43/s)  LR: 7.960e-04  Data: 0.013 (0.014)
Train: 180 [ 650/1251 ( 52%)]  Loss: 4.192 (3.82)  Time: 0.698s, 1466.04/s  (0.702s, 1459.23/s)  LR: 7.960e-04  Data: 0.015 (0.013)
Train: 180 [ 700/1251 ( 56%)]  Loss: 4.143 (3.84)  Time: 0.685s, 1494.53/s  (0.703s, 1457.01/s)  LR: 7.960e-04  Data: 0.014 (0.013)
Train: 180 [ 750/1251 ( 60%)]  Loss: 3.773 (3.84)  Time: 0.699s, 1463.93/s  (0.701s, 1460.04/s)  LR: 7.960e-04  Data: 0.011 (0.013)
Train: 180 [ 800/1251 ( 64%)]  Loss: 4.084 (3.85)  Time: 0.672s, 1522.71/s  (0.700s, 1463.35/s)  LR: 7.960e-04  Data: 0.011 (0.013)
Train: 180 [ 850/1251 ( 68%)]  Loss: 3.487 (3.83)  Time: 0.709s, 1444.57/s  (0.699s, 1465.66/s)  LR: 7.960e-04  Data: 0.010 (0.013)
Train: 180 [ 900/1251 ( 72%)]  Loss: 4.060 (3.84)  Time: 0.668s, 1532.56/s  (0.699s, 1465.75/s)  LR: 7.960e-04  Data: 0.010 (0.013)
Train: 180 [ 950/1251 ( 76%)]  Loss: 3.676 (3.83)  Time: 0.669s, 1530.05/s  (0.699s, 1465.51/s)  LR: 7.960e-04  Data: 0.009 (0.013)
Train: 180 [1000/1251 ( 80%)]  Loss: 4.100 (3.85)  Time: 0.672s, 1522.68/s  (0.699s, 1465.85/s)  LR: 7.960e-04  Data: 0.010 (0.012)
Train: 180 [1050/1251 ( 84%)]  Loss: 3.675 (3.84)  Time: 0.754s, 1357.51/s  (0.698s, 1466.09/s)  LR: 7.960e-04  Data: 0.010 (0.012)
Train: 180 [1100/1251 ( 88%)]  Loss: 3.573 (3.83)  Time: 0.673s, 1521.92/s  (0.698s, 1466.38/s)  LR: 7.960e-04  Data: 0.010 (0.012)
Train: 180 [1150/1251 ( 92%)]  Loss: 3.992 (3.83)  Time: 0.673s, 1520.62/s  (0.698s, 1467.60/s)  LR: 7.960e-04  Data: 0.009 (0.012)
Train: 180 [1200/1251 ( 96%)]  Loss: 4.023 (3.84)  Time: 0.675s, 1517.05/s  (0.698s, 1466.95/s)  LR: 7.960e-04  Data: 0.011 (0.012)
Train: 180 [1250/1251 (100%)]  Loss: 3.501 (3.83)  Time: 0.695s, 1474.10/s  (0.698s, 1466.84/s)  LR: 7.960e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.463 (1.463)  Loss:  1.0039 (1.0039)  Acc@1: 87.8906 (87.8906)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.136 (0.583)  Loss:  1.0400 (1.5497)  Acc@1: 82.0755 (71.2640)  Acc@5: 94.8113 (90.4000)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-180.pth.tar', 71.26400010009766)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-179.pth.tar', 71.23600001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-164.pth.tar', 71.10999998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-175.pth.tar', 71.1080000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-177.pth.tar', 71.08200007324218)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-167.pth.tar', 71.03800007324219)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-173.pth.tar', 70.8580001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-160.pth.tar', 70.79599999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-165.pth.tar', 70.76199994873046)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-166.pth.tar', 70.754)

Train: 181 [   0/1251 (  0%)]  Loss: 3.586 (3.59)  Time: 2.167s,  472.55/s  (2.167s,  472.55/s)  LR: 7.939e-04  Data: 1.552 (1.552)
Train: 181 [  50/1251 (  4%)]  Loss: 3.280 (3.43)  Time: 0.672s, 1522.88/s  (0.729s, 1404.00/s)  LR: 7.939e-04  Data: 0.010 (0.049)
Train: 181 [ 100/1251 (  8%)]  Loss: 3.463 (3.44)  Time: 0.694s, 1476.38/s  (0.712s, 1438.44/s)  LR: 7.939e-04  Data: 0.010 (0.030)
Train: 181 [ 150/1251 ( 12%)]  Loss: 3.787 (3.53)  Time: 0.666s, 1538.63/s  (0.706s, 1449.84/s)  LR: 7.939e-04  Data: 0.009 (0.023)
Train: 181 [ 200/1251 ( 16%)]  Loss: 3.775 (3.58)  Time: 0.706s, 1451.04/s  (0.705s, 1452.47/s)  LR: 7.939e-04  Data: 0.011 (0.020)
Train: 181 [ 250/1251 ( 20%)]  Loss: 3.794 (3.61)  Time: 0.679s, 1507.46/s  (0.702s, 1458.77/s)  LR: 7.939e-04  Data: 0.010 (0.018)
Train: 181 [ 300/1251 ( 24%)]  Loss: 3.573 (3.61)  Time: 0.751s, 1363.51/s  (0.701s, 1460.71/s)  LR: 7.939e-04  Data: 0.010 (0.017)
Train: 181 [ 350/1251 ( 28%)]  Loss: 3.680 (3.62)  Time: 0.704s, 1455.49/s  (0.699s, 1464.23/s)  LR: 7.939e-04  Data: 0.009 (0.016)
Train: 181 [ 400/1251 ( 32%)]  Loss: 3.474 (3.60)  Time: 0.681s, 1503.88/s  (0.698s, 1466.46/s)  LR: 7.939e-04  Data: 0.014 (0.015)
Train: 181 [ 450/1251 ( 36%)]  Loss: 3.621 (3.60)  Time: 0.667s, 1535.79/s  (0.697s, 1469.06/s)  LR: 7.939e-04  Data: 0.009 (0.015)
Train: 181 [ 500/1251 ( 40%)]  Loss: 3.642 (3.61)  Time: 0.709s, 1444.38/s  (0.697s, 1469.54/s)  LR: 7.939e-04  Data: 0.010 (0.014)
Train: 181 [ 550/1251 ( 44%)]  Loss: 3.664 (3.61)  Time: 0.701s, 1461.80/s  (0.696s, 1471.00/s)  LR: 7.939e-04  Data: 0.009 (0.014)
Train: 181 [ 600/1251 ( 48%)]  Loss: 3.522 (3.60)  Time: 0.675s, 1515.93/s  (0.696s, 1471.55/s)  LR: 7.939e-04  Data: 0.010 (0.013)
Train: 181 [ 650/1251 ( 52%)]  Loss: 3.821 (3.62)  Time: 0.673s, 1521.97/s  (0.696s, 1471.56/s)  LR: 7.939e-04  Data: 0.010 (0.013)
Train: 181 [ 700/1251 ( 56%)]  Loss: 3.828 (3.63)  Time: 0.712s, 1438.58/s  (0.696s, 1471.38/s)  LR: 7.939e-04  Data: 0.009 (0.013)
Train: 181 [ 750/1251 ( 60%)]  Loss: 3.685 (3.64)  Time: 0.674s, 1519.54/s  (0.696s, 1472.13/s)  LR: 7.939e-04  Data: 0.010 (0.013)
Train: 181 [ 800/1251 ( 64%)]  Loss: 3.940 (3.66)  Time: 0.670s, 1528.71/s  (0.696s, 1471.95/s)  LR: 7.939e-04  Data: 0.012 (0.013)
Train: 181 [ 850/1251 ( 68%)]  Loss: 3.947 (3.67)  Time: 0.667s, 1535.01/s  (0.696s, 1471.75/s)  LR: 7.939e-04  Data: 0.011 (0.013)
Train: 181 [ 900/1251 ( 72%)]  Loss: 3.792 (3.68)  Time: 0.694s, 1475.15/s  (0.696s, 1471.76/s)  LR: 7.939e-04  Data: 0.011 (0.012)
Train: 181 [ 950/1251 ( 76%)]  Loss: 3.847 (3.69)  Time: 0.671s, 1526.41/s  (0.695s, 1472.41/s)  LR: 7.939e-04  Data: 0.010 (0.012)
Train: 181 [1000/1251 ( 80%)]  Loss: 3.624 (3.68)  Time: 0.671s, 1525.23/s  (0.695s, 1472.52/s)  LR: 7.939e-04  Data: 0.010 (0.012)
Train: 181 [1050/1251 ( 84%)]  Loss: 3.811 (3.69)  Time: 0.672s, 1524.25/s  (0.695s, 1473.05/s)  LR: 7.939e-04  Data: 0.010 (0.012)
Train: 181 [1100/1251 ( 88%)]  Loss: 3.644 (3.69)  Time: 0.671s, 1526.21/s  (0.695s, 1472.59/s)  LR: 7.939e-04  Data: 0.010 (0.012)
Train: 181 [1150/1251 ( 92%)]  Loss: 4.042 (3.70)  Time: 0.706s, 1450.48/s  (0.695s, 1473.29/s)  LR: 7.939e-04  Data: 0.009 (0.012)
Train: 181 [1200/1251 ( 96%)]  Loss: 3.673 (3.70)  Time: 0.797s, 1284.13/s  (0.695s, 1472.92/s)  LR: 7.939e-04  Data: 0.010 (0.012)
Train: 181 [1250/1251 (100%)]  Loss: 4.001 (3.71)  Time: 0.663s, 1543.84/s  (0.695s, 1472.86/s)  LR: 7.939e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.575 (1.575)  Loss:  0.9990 (0.9990)  Acc@1: 87.3047 (87.3047)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.136 (0.579)  Loss:  1.0039 (1.5969)  Acc@1: 83.1368 (70.7660)  Acc@5: 94.6934 (90.1920)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-180.pth.tar', 71.26400010009766)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-179.pth.tar', 71.23600001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-164.pth.tar', 71.10999998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-175.pth.tar', 71.1080000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-177.pth.tar', 71.08200007324218)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-167.pth.tar', 71.03800007324219)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-173.pth.tar', 70.8580001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-160.pth.tar', 70.79599999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-181.pth.tar', 70.76599991455078)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-165.pth.tar', 70.76199994873046)

Train: 182 [   0/1251 (  0%)]  Loss: 3.477 (3.48)  Time: 2.095s,  488.84/s  (2.095s,  488.84/s)  LR: 7.917e-04  Data: 1.471 (1.471)
Train: 182 [  50/1251 (  4%)]  Loss: 3.826 (3.65)  Time: 0.672s, 1523.58/s  (0.732s, 1399.05/s)  LR: 7.917e-04  Data: 0.010 (0.049)
Train: 182 [ 100/1251 (  8%)]  Loss: 3.990 (3.76)  Time: 0.707s, 1449.21/s  (0.710s, 1441.24/s)  LR: 7.917e-04  Data: 0.010 (0.030)
Train: 182 [ 150/1251 ( 12%)]  Loss: 3.978 (3.82)  Time: 0.687s, 1491.48/s  (0.705s, 1451.61/s)  LR: 7.917e-04  Data: 0.009 (0.023)
Train: 182 [ 200/1251 ( 16%)]  Loss: 4.008 (3.86)  Time: 0.677s, 1512.58/s  (0.702s, 1459.62/s)  LR: 7.917e-04  Data: 0.010 (0.020)
Train: 182 [ 250/1251 ( 20%)]  Loss: 3.682 (3.83)  Time: 0.713s, 1435.82/s  (0.700s, 1462.24/s)  LR: 7.917e-04  Data: 0.013 (0.018)
Train: 182 [ 300/1251 ( 24%)]  Loss: 3.836 (3.83)  Time: 0.760s, 1347.45/s  (0.699s, 1464.67/s)  LR: 7.917e-04  Data: 0.010 (0.017)
Train: 182 [ 350/1251 ( 28%)]  Loss: 4.130 (3.87)  Time: 0.677s, 1513.07/s  (0.699s, 1464.38/s)  LR: 7.917e-04  Data: 0.010 (0.016)
Train: 182 [ 400/1251 ( 32%)]  Loss: 3.807 (3.86)  Time: 0.695s, 1472.42/s  (0.698s, 1467.40/s)  LR: 7.917e-04  Data: 0.013 (0.015)
Train: 182 [ 450/1251 ( 36%)]  Loss: 3.919 (3.87)  Time: 0.715s, 1431.37/s  (0.698s, 1466.97/s)  LR: 7.917e-04  Data: 0.011 (0.015)
Train: 182 [ 500/1251 ( 40%)]  Loss: 3.693 (3.85)  Time: 0.673s, 1521.43/s  (0.697s, 1468.86/s)  LR: 7.917e-04  Data: 0.011 (0.014)
Train: 182 [ 550/1251 ( 44%)]  Loss: 4.027 (3.86)  Time: 0.674s, 1518.88/s  (0.697s, 1470.03/s)  LR: 7.917e-04  Data: 0.011 (0.014)
Train: 182 [ 600/1251 ( 48%)]  Loss: 4.134 (3.89)  Time: 0.703s, 1455.80/s  (0.696s, 1472.14/s)  LR: 7.917e-04  Data: 0.009 (0.013)
Train: 182 [ 650/1251 ( 52%)]  Loss: 3.549 (3.86)  Time: 0.710s, 1443.11/s  (0.695s, 1472.56/s)  LR: 7.917e-04  Data: 0.010 (0.013)
Train: 182 [ 700/1251 ( 56%)]  Loss: 3.473 (3.84)  Time: 0.702s, 1459.33/s  (0.695s, 1473.50/s)  LR: 7.917e-04  Data: 0.009 (0.013)
Train: 182 [ 750/1251 ( 60%)]  Loss: 3.632 (3.82)  Time: 0.720s, 1422.10/s  (0.695s, 1474.15/s)  LR: 7.917e-04  Data: 0.010 (0.013)
Train: 182 [ 800/1251 ( 64%)]  Loss: 3.676 (3.81)  Time: 0.690s, 1485.07/s  (0.695s, 1472.84/s)  LR: 7.917e-04  Data: 0.011 (0.013)
Train: 182 [ 850/1251 ( 68%)]  Loss: 3.979 (3.82)  Time: 0.734s, 1394.45/s  (0.696s, 1472.23/s)  LR: 7.917e-04  Data: 0.010 (0.013)
Train: 182 [ 900/1251 ( 72%)]  Loss: 3.746 (3.82)  Time: 0.675s, 1516.75/s  (0.695s, 1472.36/s)  LR: 7.917e-04  Data: 0.009 (0.012)
Train: 182 [ 950/1251 ( 76%)]  Loss: 3.432 (3.80)  Time: 0.707s, 1448.05/s  (0.695s, 1472.45/s)  LR: 7.917e-04  Data: 0.009 (0.012)
Train: 182 [1000/1251 ( 80%)]  Loss: 3.429 (3.78)  Time: 0.709s, 1444.40/s  (0.695s, 1472.71/s)  LR: 7.917e-04  Data: 0.010 (0.012)
Train: 182 [1050/1251 ( 84%)]  Loss: 3.846 (3.78)  Time: 0.694s, 1474.87/s  (0.695s, 1472.60/s)  LR: 7.917e-04  Data: 0.009 (0.012)
Train: 182 [1100/1251 ( 88%)]  Loss: 3.412 (3.77)  Time: 0.672s, 1524.78/s  (0.695s, 1473.30/s)  LR: 7.917e-04  Data: 0.012 (0.012)
Train: 182 [1150/1251 ( 92%)]  Loss: 4.164 (3.79)  Time: 0.672s, 1522.75/s  (0.695s, 1473.70/s)  LR: 7.917e-04  Data: 0.010 (0.012)
Train: 182 [1200/1251 ( 96%)]  Loss: 3.546 (3.78)  Time: 0.762s, 1343.71/s  (0.695s, 1472.77/s)  LR: 7.917e-04  Data: 0.010 (0.012)
Train: 182 [1250/1251 (100%)]  Loss: 3.576 (3.77)  Time: 0.691s, 1482.84/s  (0.695s, 1472.62/s)  LR: 7.917e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.468 (1.468)  Loss:  0.8672 (0.8672)  Acc@1: 87.9883 (87.9883)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.136 (0.586)  Loss:  1.0010 (1.5475)  Acc@1: 82.0755 (70.3600)  Acc@5: 94.8113 (90.2180)
Train: 183 [   0/1251 (  0%)]  Loss: 3.266 (3.27)  Time: 2.867s,  357.21/s  (2.867s,  357.21/s)  LR: 7.896e-04  Data: 2.194 (2.194)
Train: 183 [  50/1251 (  4%)]  Loss: 3.970 (3.62)  Time: 0.671s, 1526.26/s  (0.744s, 1377.21/s)  LR: 7.896e-04  Data: 0.010 (0.060)
Train: 183 [ 100/1251 (  8%)]  Loss: 3.709 (3.65)  Time: 0.705s, 1452.08/s  (0.721s, 1420.61/s)  LR: 7.896e-04  Data: 0.010 (0.035)
Train: 183 [ 150/1251 ( 12%)]  Loss: 3.818 (3.69)  Time: 0.700s, 1463.77/s  (0.711s, 1439.45/s)  LR: 7.896e-04  Data: 0.012 (0.027)
Train: 183 [ 200/1251 ( 16%)]  Loss: 3.998 (3.75)  Time: 0.672s, 1523.08/s  (0.708s, 1447.32/s)  LR: 7.896e-04  Data: 0.010 (0.023)
Train: 183 [ 250/1251 ( 20%)]  Loss: 3.867 (3.77)  Time: 0.723s, 1416.35/s  (0.704s, 1453.92/s)  LR: 7.896e-04  Data: 0.009 (0.020)
Train: 183 [ 300/1251 ( 24%)]  Loss: 3.755 (3.77)  Time: 0.677s, 1513.20/s  (0.702s, 1458.42/s)  LR: 7.896e-04  Data: 0.010 (0.018)
Train: 183 [ 350/1251 ( 28%)]  Loss: 3.444 (3.73)  Time: 0.673s, 1521.46/s  (0.701s, 1461.43/s)  LR: 7.896e-04  Data: 0.010 (0.017)
Train: 183 [ 400/1251 ( 32%)]  Loss: 3.801 (3.74)  Time: 0.715s, 1432.68/s  (0.700s, 1461.99/s)  LR: 7.896e-04  Data: 0.010 (0.016)
Train: 183 [ 450/1251 ( 36%)]  Loss: 4.034 (3.77)  Time: 0.673s, 1521.98/s  (0.700s, 1463.26/s)  LR: 7.896e-04  Data: 0.010 (0.016)
Train: 183 [ 500/1251 ( 40%)]  Loss: 3.719 (3.76)  Time: 0.673s, 1521.90/s  (0.699s, 1465.84/s)  LR: 7.896e-04  Data: 0.012 (0.015)
Train: 183 [ 550/1251 ( 44%)]  Loss: 3.571 (3.75)  Time: 0.701s, 1459.74/s  (0.699s, 1465.70/s)  LR: 7.896e-04  Data: 0.009 (0.015)
Train: 183 [ 600/1251 ( 48%)]  Loss: 3.625 (3.74)  Time: 0.699s, 1464.23/s  (0.699s, 1464.70/s)  LR: 7.896e-04  Data: 0.013 (0.014)
Train: 183 [ 650/1251 ( 52%)]  Loss: 3.497 (3.72)  Time: 0.670s, 1528.81/s  (0.699s, 1465.35/s)  LR: 7.896e-04  Data: 0.009 (0.014)
Train: 183 [ 700/1251 ( 56%)]  Loss: 3.591 (3.71)  Time: 0.672s, 1524.31/s  (0.698s, 1466.74/s)  LR: 7.896e-04  Data: 0.008 (0.014)
Train: 183 [ 750/1251 ( 60%)]  Loss: 4.183 (3.74)  Time: 0.707s, 1447.70/s  (0.698s, 1466.34/s)  LR: 7.896e-04  Data: 0.009 (0.013)
Train: 183 [ 800/1251 ( 64%)]  Loss: 4.042 (3.76)  Time: 0.700s, 1463.21/s  (0.698s, 1467.35/s)  LR: 7.896e-04  Data: 0.009 (0.013)
Train: 183 [ 850/1251 ( 68%)]  Loss: 3.966 (3.77)  Time: 0.691s, 1482.31/s  (0.697s, 1468.26/s)  LR: 7.896e-04  Data: 0.010 (0.013)
Train: 183 [ 900/1251 ( 72%)]  Loss: 4.024 (3.78)  Time: 0.715s, 1431.27/s  (0.697s, 1468.10/s)  LR: 7.896e-04  Data: 0.011 (0.013)
Train: 183 [ 950/1251 ( 76%)]  Loss: 3.881 (3.79)  Time: 0.701s, 1460.36/s  (0.697s, 1468.17/s)  LR: 7.896e-04  Data: 0.012 (0.013)
Train: 183 [1000/1251 ( 80%)]  Loss: 3.772 (3.79)  Time: 0.671s, 1527.01/s  (0.697s, 1469.38/s)  LR: 7.896e-04  Data: 0.012 (0.013)
Train: 183 [1050/1251 ( 84%)]  Loss: 3.996 (3.80)  Time: 0.682s, 1502.48/s  (0.697s, 1469.10/s)  LR: 7.896e-04  Data: 0.016 (0.013)
Train: 183 [1100/1251 ( 88%)]  Loss: 3.746 (3.79)  Time: 0.665s, 1538.70/s  (0.697s, 1469.25/s)  LR: 7.896e-04  Data: 0.009 (0.012)
Train: 183 [1150/1251 ( 92%)]  Loss: 3.974 (3.80)  Time: 0.711s, 1439.42/s  (0.697s, 1470.12/s)  LR: 7.896e-04  Data: 0.010 (0.012)
Train: 183 [1200/1251 ( 96%)]  Loss: 4.016 (3.81)  Time: 0.691s, 1481.30/s  (0.696s, 1470.23/s)  LR: 7.896e-04  Data: 0.010 (0.012)
Train: 183 [1250/1251 (100%)]  Loss: 3.527 (3.80)  Time: 0.656s, 1561.44/s  (0.696s, 1470.81/s)  LR: 7.896e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.556 (1.556)  Loss:  0.9741 (0.9741)  Acc@1: 87.7930 (87.7930)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.136 (0.586)  Loss:  1.1328 (1.5580)  Acc@1: 82.3113 (70.7900)  Acc@5: 94.2217 (90.1880)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-180.pth.tar', 71.26400010009766)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-179.pth.tar', 71.23600001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-164.pth.tar', 71.10999998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-175.pth.tar', 71.1080000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-177.pth.tar', 71.08200007324218)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-167.pth.tar', 71.03800007324219)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-173.pth.tar', 70.8580001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-160.pth.tar', 70.79599999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-183.pth.tar', 70.79000007324218)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-181.pth.tar', 70.76599991455078)

Train: 184 [   0/1251 (  0%)]  Loss: 3.865 (3.87)  Time: 2.099s,  487.80/s  (2.099s,  487.80/s)  LR: 7.875e-04  Data: 1.440 (1.440)
Train: 184 [  50/1251 (  4%)]  Loss: 3.562 (3.71)  Time: 0.696s, 1471.48/s  (0.722s, 1419.14/s)  LR: 7.875e-04  Data: 0.010 (0.044)
Train: 184 [ 100/1251 (  8%)]  Loss: 3.862 (3.76)  Time: 0.715s, 1431.98/s  (0.706s, 1450.91/s)  LR: 7.875e-04  Data: 0.010 (0.027)
Train: 184 [ 150/1251 ( 12%)]  Loss: 3.766 (3.76)  Time: 0.717s, 1428.67/s  (0.701s, 1461.58/s)  LR: 7.875e-04  Data: 0.009 (0.022)
Train: 184 [ 200/1251 ( 16%)]  Loss: 3.787 (3.77)  Time: 0.674s, 1518.79/s  (0.698s, 1467.34/s)  LR: 7.875e-04  Data: 0.010 (0.019)
Train: 184 [ 250/1251 ( 20%)]  Loss: 4.128 (3.83)  Time: 0.671s, 1525.98/s  (0.696s, 1470.42/s)  LR: 7.875e-04  Data: 0.010 (0.017)
Train: 184 [ 300/1251 ( 24%)]  Loss: 3.632 (3.80)  Time: 0.687s, 1489.81/s  (0.697s, 1470.12/s)  LR: 7.875e-04  Data: 0.009 (0.016)
Train: 184 [ 350/1251 ( 28%)]  Loss: 3.813 (3.80)  Time: 0.688s, 1488.16/s  (0.696s, 1470.78/s)  LR: 7.875e-04  Data: 0.009 (0.015)
Train: 184 [ 400/1251 ( 32%)]  Loss: 3.745 (3.80)  Time: 0.670s, 1528.76/s  (0.696s, 1471.26/s)  LR: 7.875e-04  Data: 0.014 (0.014)
Train: 184 [ 450/1251 ( 36%)]  Loss: 3.854 (3.80)  Time: 0.711s, 1440.02/s  (0.696s, 1472.30/s)  LR: 7.875e-04  Data: 0.009 (0.014)
Train: 184 [ 500/1251 ( 40%)]  Loss: 3.859 (3.81)  Time: 0.679s, 1507.79/s  (0.695s, 1473.33/s)  LR: 7.875e-04  Data: 0.011 (0.014)
Train: 184 [ 550/1251 ( 44%)]  Loss: 4.011 (3.82)  Time: 0.664s, 1542.42/s  (0.695s, 1473.89/s)  LR: 7.875e-04  Data: 0.008 (0.013)
Train: 184 [ 600/1251 ( 48%)]  Loss: 3.954 (3.83)  Time: 0.671s, 1526.37/s  (0.695s, 1473.25/s)  LR: 7.875e-04  Data: 0.011 (0.013)
Train: 184 [ 650/1251 ( 52%)]  Loss: 4.043 (3.85)  Time: 0.715s, 1431.22/s  (0.694s, 1474.48/s)  LR: 7.875e-04  Data: 0.009 (0.013)
Train: 184 [ 700/1251 ( 56%)]  Loss: 4.184 (3.87)  Time: 0.672s, 1523.29/s  (0.695s, 1472.86/s)  LR: 7.875e-04  Data: 0.011 (0.013)
Train: 184 [ 750/1251 ( 60%)]  Loss: 3.639 (3.86)  Time: 0.704s, 1454.77/s  (0.695s, 1473.52/s)  LR: 7.875e-04  Data: 0.009 (0.012)
Train: 184 [ 800/1251 ( 64%)]  Loss: 3.889 (3.86)  Time: 0.673s, 1522.47/s  (0.695s, 1474.30/s)  LR: 7.875e-04  Data: 0.009 (0.012)
Train: 184 [ 850/1251 ( 68%)]  Loss: 4.117 (3.87)  Time: 0.672s, 1522.69/s  (0.694s, 1474.72/s)  LR: 7.875e-04  Data: 0.010 (0.012)
Train: 184 [ 900/1251 ( 72%)]  Loss: 3.617 (3.86)  Time: 0.725s, 1413.10/s  (0.695s, 1474.20/s)  LR: 7.875e-04  Data: 0.009 (0.012)
Train: 184 [ 950/1251 ( 76%)]  Loss: 4.170 (3.87)  Time: 0.693s, 1478.51/s  (0.695s, 1474.30/s)  LR: 7.875e-04  Data: 0.014 (0.012)
Train: 184 [1000/1251 ( 80%)]  Loss: 3.874 (3.87)  Time: 0.682s, 1500.91/s  (0.695s, 1474.00/s)  LR: 7.875e-04  Data: 0.011 (0.012)
Train: 184 [1050/1251 ( 84%)]  Loss: 3.641 (3.86)  Time: 0.676s, 1514.67/s  (0.695s, 1474.01/s)  LR: 7.875e-04  Data: 0.010 (0.012)
Train: 184 [1100/1251 ( 88%)]  Loss: 3.561 (3.85)  Time: 0.719s, 1424.34/s  (0.695s, 1473.86/s)  LR: 7.875e-04  Data: 0.009 (0.012)
Train: 184 [1150/1251 ( 92%)]  Loss: 3.792 (3.85)  Time: 0.673s, 1521.85/s  (0.694s, 1474.47/s)  LR: 7.875e-04  Data: 0.011 (0.012)
Train: 184 [1200/1251 ( 96%)]  Loss: 3.782 (3.85)  Time: 0.694s, 1474.59/s  (0.695s, 1474.43/s)  LR: 7.875e-04  Data: 0.009 (0.012)
Train: 184 [1250/1251 (100%)]  Loss: 3.637 (3.84)  Time: 0.661s, 1550.12/s  (0.694s, 1475.02/s)  LR: 7.875e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.465 (1.465)  Loss:  0.9336 (0.9336)  Acc@1: 87.0117 (87.0117)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.136 (0.577)  Loss:  1.0938 (1.5944)  Acc@1: 83.9623 (70.2900)  Acc@5: 94.9293 (89.7140)
Train: 185 [   0/1251 (  0%)]  Loss: 3.770 (3.77)  Time: 2.144s,  477.52/s  (2.144s,  477.52/s)  LR: 7.854e-04  Data: 1.529 (1.529)
Train: 185 [  50/1251 (  4%)]  Loss: 4.002 (3.89)  Time: 0.703s, 1456.90/s  (0.723s, 1416.43/s)  LR: 7.854e-04  Data: 0.010 (0.046)
Train: 185 [ 100/1251 (  8%)]  Loss: 3.349 (3.71)  Time: 0.708s, 1445.85/s  (0.708s, 1446.37/s)  LR: 7.854e-04  Data: 0.013 (0.028)
Train: 185 [ 150/1251 ( 12%)]  Loss: 4.047 (3.79)  Time: 0.708s, 1445.78/s  (0.704s, 1454.43/s)  LR: 7.854e-04  Data: 0.009 (0.022)
Train: 185 [ 200/1251 ( 16%)]  Loss: 3.762 (3.79)  Time: 0.702s, 1457.89/s  (0.702s, 1458.80/s)  LR: 7.854e-04  Data: 0.012 (0.019)
Train: 185 [ 250/1251 ( 20%)]  Loss: 3.909 (3.81)  Time: 0.671s, 1525.99/s  (0.700s, 1462.48/s)  LR: 7.854e-04  Data: 0.011 (0.017)
Train: 185 [ 300/1251 ( 24%)]  Loss: 3.895 (3.82)  Time: 0.788s, 1299.98/s  (0.700s, 1463.30/s)  LR: 7.854e-04  Data: 0.009 (0.016)
Train: 185 [ 350/1251 ( 28%)]  Loss: 3.925 (3.83)  Time: 0.671s, 1526.66/s  (0.699s, 1465.22/s)  LR: 7.854e-04  Data: 0.009 (0.015)
Train: 185 [ 400/1251 ( 32%)]  Loss: 3.727 (3.82)  Time: 0.718s, 1425.32/s  (0.700s, 1463.60/s)  LR: 7.854e-04  Data: 0.009 (0.015)
Train: 185 [ 450/1251 ( 36%)]  Loss: 3.973 (3.84)  Time: 0.698s, 1466.74/s  (0.699s, 1464.69/s)  LR: 7.854e-04  Data: 0.010 (0.014)
Train: 185 [ 500/1251 ( 40%)]  Loss: 4.031 (3.85)  Time: 0.760s, 1346.73/s  (0.699s, 1465.38/s)  LR: 7.854e-04  Data: 0.012 (0.014)
Train: 185 [ 550/1251 ( 44%)]  Loss: 3.765 (3.85)  Time: 0.696s, 1470.38/s  (0.699s, 1465.91/s)  LR: 7.854e-04  Data: 0.010 (0.014)
Train: 185 [ 600/1251 ( 48%)]  Loss: 4.107 (3.87)  Time: 0.667s, 1534.79/s  (0.698s, 1467.31/s)  LR: 7.854e-04  Data: 0.011 (0.013)
Train: 185 [ 650/1251 ( 52%)]  Loss: 4.026 (3.88)  Time: 0.690s, 1483.71/s  (0.697s, 1468.23/s)  LR: 7.854e-04  Data: 0.009 (0.013)
Train: 185 [ 700/1251 ( 56%)]  Loss: 4.076 (3.89)  Time: 0.705s, 1452.24/s  (0.697s, 1469.14/s)  LR: 7.854e-04  Data: 0.010 (0.013)
Train: 185 [ 750/1251 ( 60%)]  Loss: 3.882 (3.89)  Time: 0.671s, 1525.28/s  (0.697s, 1470.16/s)  LR: 7.854e-04  Data: 0.011 (0.013)
Train: 185 [ 800/1251 ( 64%)]  Loss: 3.423 (3.86)  Time: 0.714s, 1433.68/s  (0.696s, 1471.22/s)  LR: 7.854e-04  Data: 0.010 (0.013)
Train: 185 [ 850/1251 ( 68%)]  Loss: 3.721 (3.86)  Time: 0.667s, 1535.75/s  (0.696s, 1471.29/s)  LR: 7.854e-04  Data: 0.011 (0.012)
Train: 185 [ 900/1251 ( 72%)]  Loss: 3.765 (3.85)  Time: 0.671s, 1525.44/s  (0.696s, 1470.92/s)  LR: 7.854e-04  Data: 0.010 (0.012)
Train: 185 [ 950/1251 ( 76%)]  Loss: 4.126 (3.86)  Time: 0.706s, 1451.14/s  (0.696s, 1471.38/s)  LR: 7.854e-04  Data: 0.010 (0.012)
Train: 185 [1000/1251 ( 80%)]  Loss: 3.685 (3.86)  Time: 0.670s, 1527.89/s  (0.696s, 1471.88/s)  LR: 7.854e-04  Data: 0.010 (0.012)
Train: 185 [1050/1251 ( 84%)]  Loss: 3.751 (3.85)  Time: 0.688s, 1489.15/s  (0.696s, 1472.15/s)  LR: 7.854e-04  Data: 0.011 (0.012)
Train: 185 [1100/1251 ( 88%)]  Loss: 3.590 (3.84)  Time: 0.697s, 1469.44/s  (0.695s, 1472.42/s)  LR: 7.854e-04  Data: 0.012 (0.012)
Train: 185 [1150/1251 ( 92%)]  Loss: 3.749 (3.84)  Time: 0.723s, 1415.75/s  (0.695s, 1472.83/s)  LR: 7.854e-04  Data: 0.010 (0.012)
Train: 185 [1200/1251 ( 96%)]  Loss: 3.536 (3.82)  Time: 0.671s, 1526.33/s  (0.695s, 1473.37/s)  LR: 7.854e-04  Data: 0.010 (0.012)
Train: 185 [1250/1251 (100%)]  Loss: 3.942 (3.83)  Time: 0.655s, 1563.15/s  (0.695s, 1473.72/s)  LR: 7.854e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.581 (1.581)  Loss:  0.9146 (0.9146)  Acc@1: 87.8906 (87.8906)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.137 (0.595)  Loss:  1.0215 (1.5742)  Acc@1: 83.1368 (70.8880)  Acc@5: 94.8113 (90.2420)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-180.pth.tar', 71.26400010009766)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-179.pth.tar', 71.23600001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-164.pth.tar', 71.10999998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-175.pth.tar', 71.1080000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-177.pth.tar', 71.08200007324218)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-167.pth.tar', 71.03800007324219)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-185.pth.tar', 70.88799991455078)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-173.pth.tar', 70.8580001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-160.pth.tar', 70.79599999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-183.pth.tar', 70.79000007324218)

Train: 186 [   0/1251 (  0%)]  Loss: 3.647 (3.65)  Time: 2.438s,  420.06/s  (2.438s,  420.06/s)  LR: 7.832e-04  Data: 1.763 (1.763)
Train: 186 [  50/1251 (  4%)]  Loss: 3.832 (3.74)  Time: 0.677s, 1512.78/s  (0.723s, 1416.63/s)  LR: 7.832e-04  Data: 0.011 (0.050)
Train: 186 [ 100/1251 (  8%)]  Loss: 3.940 (3.81)  Time: 0.687s, 1491.57/s  (0.707s, 1447.86/s)  LR: 7.832e-04  Data: 0.012 (0.030)
Train: 186 [ 150/1251 ( 12%)]  Loss: 4.040 (3.86)  Time: 0.679s, 1507.54/s  (0.704s, 1455.53/s)  LR: 7.832e-04  Data: 0.010 (0.024)
Train: 186 [ 200/1251 ( 16%)]  Loss: 4.046 (3.90)  Time: 0.702s, 1459.43/s  (0.702s, 1459.53/s)  LR: 7.832e-04  Data: 0.009 (0.020)
Train: 186 [ 250/1251 ( 20%)]  Loss: 4.034 (3.92)  Time: 0.705s, 1451.50/s  (0.701s, 1460.53/s)  LR: 7.832e-04  Data: 0.011 (0.018)
Train: 186 [ 300/1251 ( 24%)]  Loss: 3.976 (3.93)  Time: 0.689s, 1485.28/s  (0.701s, 1461.75/s)  LR: 7.832e-04  Data: 0.013 (0.017)
Train: 186 [ 350/1251 ( 28%)]  Loss: 4.008 (3.94)  Time: 0.676s, 1514.44/s  (0.701s, 1461.47/s)  LR: 7.832e-04  Data: 0.009 (0.016)
Train: 186 [ 400/1251 ( 32%)]  Loss: 4.122 (3.96)  Time: 0.696s, 1471.85/s  (0.700s, 1461.83/s)  LR: 7.832e-04  Data: 0.013 (0.016)
Train: 186 [ 450/1251 ( 36%)]  Loss: 4.011 (3.97)  Time: 0.713s, 1437.00/s  (0.700s, 1463.43/s)  LR: 7.832e-04  Data: 0.011 (0.015)
Train: 186 [ 500/1251 ( 40%)]  Loss: 3.599 (3.93)  Time: 0.674s, 1519.65/s  (0.699s, 1464.34/s)  LR: 7.832e-04  Data: 0.010 (0.015)
Train: 186 [ 550/1251 ( 44%)]  Loss: 3.861 (3.93)  Time: 0.752s, 1362.11/s  (0.699s, 1465.73/s)  LR: 7.832e-04  Data: 0.009 (0.014)
Train: 186 [ 600/1251 ( 48%)]  Loss: 3.704 (3.91)  Time: 0.674s, 1518.64/s  (0.699s, 1465.94/s)  LR: 7.832e-04  Data: 0.010 (0.014)
Train: 186 [ 650/1251 ( 52%)]  Loss: 3.717 (3.90)  Time: 0.672s, 1523.15/s  (0.698s, 1466.50/s)  LR: 7.832e-04  Data: 0.014 (0.014)
Train: 186 [ 700/1251 ( 56%)]  Loss: 3.816 (3.89)  Time: 0.672s, 1523.47/s  (0.698s, 1467.89/s)  LR: 7.832e-04  Data: 0.010 (0.013)
Train: 186 [ 750/1251 ( 60%)]  Loss: 3.768 (3.88)  Time: 0.676s, 1515.48/s  (0.697s, 1468.57/s)  LR: 7.832e-04  Data: 0.010 (0.013)
Train: 186 [ 800/1251 ( 64%)]  Loss: 4.023 (3.89)  Time: 0.703s, 1456.96/s  (0.697s, 1468.91/s)  LR: 7.832e-04  Data: 0.010 (0.013)
Train: 186 [ 850/1251 ( 68%)]  Loss: 3.875 (3.89)  Time: 0.763s, 1342.59/s  (0.697s, 1469.35/s)  LR: 7.832e-04  Data: 0.013 (0.013)
Train: 186 [ 900/1251 ( 72%)]  Loss: 3.890 (3.89)  Time: 0.702s, 1457.89/s  (0.697s, 1469.17/s)  LR: 7.832e-04  Data: 0.009 (0.013)
Train: 186 [ 950/1251 ( 76%)]  Loss: 3.775 (3.88)  Time: 0.675s, 1518.15/s  (0.697s, 1469.03/s)  LR: 7.832e-04  Data: 0.009 (0.012)
Train: 186 [1000/1251 ( 80%)]  Loss: 4.108 (3.89)  Time: 0.678s, 1509.86/s  (0.696s, 1470.35/s)  LR: 7.832e-04  Data: 0.010 (0.012)
Train: 186 [1050/1251 ( 84%)]  Loss: 3.649 (3.88)  Time: 0.668s, 1533.30/s  (0.696s, 1471.06/s)  LR: 7.832e-04  Data: 0.009 (0.012)
Train: 186 [1100/1251 ( 88%)]  Loss: 3.725 (3.88)  Time: 0.702s, 1459.49/s  (0.696s, 1470.37/s)  LR: 7.832e-04  Data: 0.013 (0.012)
Train: 186 [1150/1251 ( 92%)]  Loss: 3.783 (3.87)  Time: 0.673s, 1522.03/s  (0.696s, 1470.43/s)  LR: 7.832e-04  Data: 0.010 (0.012)
Train: 186 [1200/1251 ( 96%)]  Loss: 3.731 (3.87)  Time: 0.708s, 1446.73/s  (0.696s, 1471.06/s)  LR: 7.832e-04  Data: 0.009 (0.012)
Train: 186 [1250/1251 (100%)]  Loss: 3.830 (3.87)  Time: 0.666s, 1537.72/s  (0.696s, 1471.50/s)  LR: 7.832e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.651 (1.651)  Loss:  0.8433 (0.8433)  Acc@1: 88.0859 (88.0859)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.137 (0.584)  Loss:  0.9863 (1.5256)  Acc@1: 82.7830 (70.6480)  Acc@5: 95.4009 (90.0660)
Train: 187 [   0/1251 (  0%)]  Loss: 3.791 (3.79)  Time: 2.375s,  431.22/s  (2.375s,  431.22/s)  LR: 7.811e-04  Data: 1.724 (1.724)
Train: 187 [  50/1251 (  4%)]  Loss: 3.656 (3.72)  Time: 0.671s, 1525.33/s  (0.738s, 1386.68/s)  LR: 7.811e-04  Data: 0.013 (0.052)
Train: 187 [ 100/1251 (  8%)]  Loss: 3.725 (3.72)  Time: 0.696s, 1471.46/s  (0.716s, 1429.77/s)  LR: 7.811e-04  Data: 0.009 (0.031)
Train: 187 [ 150/1251 ( 12%)]  Loss: 3.855 (3.76)  Time: 0.767s, 1335.50/s  (0.710s, 1442.80/s)  LR: 7.811e-04  Data: 0.011 (0.024)
Train: 187 [ 200/1251 ( 16%)]  Loss: 3.673 (3.74)  Time: 0.714s, 1434.12/s  (0.705s, 1452.47/s)  LR: 7.811e-04  Data: 0.009 (0.021)
Train: 187 [ 250/1251 ( 20%)]  Loss: 3.434 (3.69)  Time: 0.666s, 1538.09/s  (0.703s, 1456.74/s)  LR: 7.811e-04  Data: 0.010 (0.019)
Train: 187 [ 300/1251 ( 24%)]  Loss: 4.345 (3.78)  Time: 0.709s, 1444.30/s  (0.702s, 1458.45/s)  LR: 7.811e-04  Data: 0.015 (0.017)
Train: 187 [ 350/1251 ( 28%)]  Loss: 3.753 (3.78)  Time: 0.714s, 1434.81/s  (0.702s, 1459.13/s)  LR: 7.811e-04  Data: 0.009 (0.016)
Train: 187 [ 400/1251 ( 32%)]  Loss: 3.630 (3.76)  Time: 0.699s, 1464.82/s  (0.701s, 1461.15/s)  LR: 7.811e-04  Data: 0.009 (0.015)
Train: 187 [ 450/1251 ( 36%)]  Loss: 3.636 (3.75)  Time: 0.703s, 1455.72/s  (0.700s, 1463.09/s)  LR: 7.811e-04  Data: 0.009 (0.015)
Train: 187 [ 500/1251 ( 40%)]  Loss: 3.686 (3.74)  Time: 0.666s, 1537.56/s  (0.699s, 1465.03/s)  LR: 7.811e-04  Data: 0.010 (0.014)
Train: 187 [ 550/1251 ( 44%)]  Loss: 3.308 (3.71)  Time: 0.693s, 1477.53/s  (0.698s, 1466.36/s)  LR: 7.811e-04  Data: 0.009 (0.014)
Train: 187 [ 600/1251 ( 48%)]  Loss: 4.046 (3.73)  Time: 0.702s, 1459.72/s  (0.698s, 1467.30/s)  LR: 7.811e-04  Data: 0.009 (0.014)
Train: 187 [ 650/1251 ( 52%)]  Loss: 3.563 (3.72)  Time: 0.672s, 1523.73/s  (0.698s, 1467.99/s)  LR: 7.811e-04  Data: 0.011 (0.013)
Train: 187 [ 700/1251 ( 56%)]  Loss: 3.637 (3.72)  Time: 0.710s, 1442.30/s  (0.697s, 1468.41/s)  LR: 7.811e-04  Data: 0.011 (0.013)
Train: 187 [ 750/1251 ( 60%)]  Loss: 3.654 (3.71)  Time: 0.671s, 1527.11/s  (0.697s, 1469.35/s)  LR: 7.811e-04  Data: 0.011 (0.013)
Train: 187 [ 800/1251 ( 64%)]  Loss: 3.364 (3.69)  Time: 0.700s, 1463.69/s  (0.696s, 1470.64/s)  LR: 7.811e-04  Data: 0.009 (0.013)
Train: 187 [ 850/1251 ( 68%)]  Loss: 3.767 (3.70)  Time: 0.686s, 1492.90/s  (0.696s, 1472.32/s)  LR: 7.811e-04  Data: 0.011 (0.013)
Train: 187 [ 900/1251 ( 72%)]  Loss: 3.631 (3.69)  Time: 0.709s, 1443.56/s  (0.695s, 1472.63/s)  LR: 7.811e-04  Data: 0.010 (0.012)
Train: 187 [ 950/1251 ( 76%)]  Loss: 3.889 (3.70)  Time: 0.705s, 1452.73/s  (0.695s, 1472.51/s)  LR: 7.811e-04  Data: 0.009 (0.012)
Train: 187 [1000/1251 ( 80%)]  Loss: 4.042 (3.72)  Time: 0.669s, 1531.42/s  (0.695s, 1473.74/s)  LR: 7.811e-04  Data: 0.009 (0.012)
Train: 187 [1050/1251 ( 84%)]  Loss: 3.945 (3.73)  Time: 0.705s, 1451.75/s  (0.695s, 1473.40/s)  LR: 7.811e-04  Data: 0.009 (0.012)
Train: 187 [1100/1251 ( 88%)]  Loss: 3.801 (3.73)  Time: 0.671s, 1526.72/s  (0.695s, 1472.36/s)  LR: 7.811e-04  Data: 0.010 (0.012)
Train: 187 [1150/1251 ( 92%)]  Loss: 3.736 (3.73)  Time: 0.682s, 1500.95/s  (0.695s, 1472.83/s)  LR: 7.811e-04  Data: 0.009 (0.012)
Train: 187 [1200/1251 ( 96%)]  Loss: 3.864 (3.74)  Time: 0.704s, 1454.90/s  (0.695s, 1473.08/s)  LR: 7.811e-04  Data: 0.010 (0.012)
Train: 187 [1250/1251 (100%)]  Loss: 3.592 (3.73)  Time: 0.666s, 1536.80/s  (0.696s, 1472.19/s)  LR: 7.811e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.448 (1.448)  Loss:  0.8457 (0.8457)  Acc@1: 86.6211 (86.6211)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.137 (0.592)  Loss:  0.9463 (1.4079)  Acc@1: 83.8443 (71.8700)  Acc@5: 95.2830 (90.6800)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-187.pth.tar', 71.87000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-180.pth.tar', 71.26400010009766)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-179.pth.tar', 71.23600001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-164.pth.tar', 71.10999998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-175.pth.tar', 71.1080000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-177.pth.tar', 71.08200007324218)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-167.pth.tar', 71.03800007324219)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-185.pth.tar', 70.88799991455078)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-173.pth.tar', 70.8580001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-160.pth.tar', 70.79599999511719)

Train: 188 [   0/1251 (  0%)]  Loss: 3.560 (3.56)  Time: 2.310s,  443.37/s  (2.310s,  443.37/s)  LR: 7.789e-04  Data: 1.694 (1.694)
Train: 188 [  50/1251 (  4%)]  Loss: 3.902 (3.73)  Time: 0.675s, 1516.89/s  (0.732s, 1399.26/s)  LR: 7.789e-04  Data: 0.010 (0.049)
Train: 188 [ 100/1251 (  8%)]  Loss: 3.905 (3.79)  Time: 0.728s, 1406.41/s  (0.715s, 1431.43/s)  LR: 7.789e-04  Data: 0.014 (0.030)
Train: 188 [ 150/1251 ( 12%)]  Loss: 3.769 (3.78)  Time: 0.669s, 1530.87/s  (0.709s, 1443.98/s)  LR: 7.789e-04  Data: 0.009 (0.024)
Train: 188 [ 200/1251 ( 16%)]  Loss: 3.735 (3.77)  Time: 0.723s, 1416.93/s  (0.709s, 1444.31/s)  LR: 7.789e-04  Data: 0.009 (0.021)
Train: 188 [ 250/1251 ( 20%)]  Loss: 3.783 (3.78)  Time: 0.704s, 1453.86/s  (0.708s, 1445.43/s)  LR: 7.789e-04  Data: 0.009 (0.019)
Train: 188 [ 300/1251 ( 24%)]  Loss: 3.794 (3.78)  Time: 0.667s, 1535.44/s  (0.707s, 1449.29/s)  LR: 7.789e-04  Data: 0.010 (0.017)
Train: 188 [ 350/1251 ( 28%)]  Loss: 3.655 (3.76)  Time: 0.672s, 1522.71/s  (0.704s, 1453.90/s)  LR: 7.789e-04  Data: 0.011 (0.016)
Train: 188 [ 400/1251 ( 32%)]  Loss: 3.771 (3.76)  Time: 0.671s, 1525.12/s  (0.702s, 1458.08/s)  LR: 7.789e-04  Data: 0.011 (0.015)
Train: 188 [ 450/1251 ( 36%)]  Loss: 3.904 (3.78)  Time: 0.677s, 1513.11/s  (0.700s, 1462.33/s)  LR: 7.789e-04  Data: 0.009 (0.015)
Train: 188 [ 500/1251 ( 40%)]  Loss: 3.846 (3.78)  Time: 0.715s, 1431.76/s  (0.699s, 1464.20/s)  LR: 7.789e-04  Data: 0.010 (0.014)
Train: 188 [ 550/1251 ( 44%)]  Loss: 3.845 (3.79)  Time: 0.666s, 1537.62/s  (0.699s, 1465.77/s)  LR: 7.789e-04  Data: 0.010 (0.014)
Train: 188 [ 600/1251 ( 48%)]  Loss: 3.701 (3.78)  Time: 0.673s, 1521.90/s  (0.698s, 1467.90/s)  LR: 7.789e-04  Data: 0.011 (0.014)
Train: 188 [ 650/1251 ( 52%)]  Loss: 3.946 (3.79)  Time: 0.706s, 1451.06/s  (0.697s, 1469.09/s)  LR: 7.789e-04  Data: 0.010 (0.013)
Train: 188 [ 700/1251 ( 56%)]  Loss: 3.643 (3.78)  Time: 0.674s, 1519.70/s  (0.697s, 1469.35/s)  LR: 7.789e-04  Data: 0.011 (0.013)
Train: 188 [ 750/1251 ( 60%)]  Loss: 3.636 (3.77)  Time: 0.704s, 1455.23/s  (0.697s, 1469.99/s)  LR: 7.789e-04  Data: 0.009 (0.013)
Train: 188 [ 800/1251 ( 64%)]  Loss: 3.647 (3.77)  Time: 0.667s, 1536.01/s  (0.696s, 1471.22/s)  LR: 7.789e-04  Data: 0.012 (0.013)
Train: 188 [ 850/1251 ( 68%)]  Loss: 3.886 (3.77)  Time: 0.672s, 1524.06/s  (0.696s, 1472.26/s)  LR: 7.789e-04  Data: 0.010 (0.013)
Train: 188 [ 900/1251 ( 72%)]  Loss: 4.005 (3.79)  Time: 0.708s, 1446.79/s  (0.695s, 1473.18/s)  LR: 7.789e-04  Data: 0.010 (0.012)
Train: 188 [ 950/1251 ( 76%)]  Loss: 3.705 (3.78)  Time: 0.686s, 1492.94/s  (0.695s, 1473.04/s)  LR: 7.789e-04  Data: 0.011 (0.012)
Train: 188 [1000/1251 ( 80%)]  Loss: 4.058 (3.80)  Time: 0.674s, 1519.47/s  (0.695s, 1473.07/s)  LR: 7.789e-04  Data: 0.014 (0.012)
Train: 188 [1050/1251 ( 84%)]  Loss: 3.431 (3.78)  Time: 0.729s, 1405.28/s  (0.695s, 1473.01/s)  LR: 7.789e-04  Data: 0.009 (0.012)
Train: 188 [1100/1251 ( 88%)]  Loss: 3.864 (3.78)  Time: 0.669s, 1531.66/s  (0.695s, 1473.31/s)  LR: 7.789e-04  Data: 0.011 (0.012)
Train: 188 [1150/1251 ( 92%)]  Loss: 4.095 (3.80)  Time: 0.704s, 1454.68/s  (0.695s, 1473.34/s)  LR: 7.789e-04  Data: 0.010 (0.012)
Train: 188 [1200/1251 ( 96%)]  Loss: 3.758 (3.79)  Time: 0.698s, 1466.14/s  (0.695s, 1473.89/s)  LR: 7.789e-04  Data: 0.010 (0.012)
Train: 188 [1250/1251 (100%)]  Loss: 3.930 (3.80)  Time: 0.684s, 1497.65/s  (0.695s, 1473.69/s)  LR: 7.789e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.507 (1.507)  Loss:  0.9912 (0.9912)  Acc@1: 87.0117 (87.0117)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.136 (0.579)  Loss:  0.9341 (1.5400)  Acc@1: 84.5519 (70.6240)  Acc@5: 96.2264 (90.2280)
Train: 189 [   0/1251 (  0%)]  Loss: 3.479 (3.48)  Time: 2.398s,  426.95/s  (2.398s,  426.95/s)  LR: 7.768e-04  Data: 1.744 (1.744)
Train: 189 [  50/1251 (  4%)]  Loss: 4.042 (3.76)  Time: 0.680s, 1506.41/s  (0.729s, 1404.00/s)  LR: 7.768e-04  Data: 0.010 (0.050)
Train: 189 [ 100/1251 (  8%)]  Loss: 3.964 (3.83)  Time: 0.672s, 1522.86/s  (0.710s, 1442.13/s)  LR: 7.768e-04  Data: 0.010 (0.030)
Train: 189 [ 150/1251 ( 12%)]  Loss: 3.647 (3.78)  Time: 0.669s, 1531.54/s  (0.707s, 1449.25/s)  LR: 7.768e-04  Data: 0.010 (0.024)
Train: 189 [ 200/1251 ( 16%)]  Loss: 3.892 (3.80)  Time: 0.670s, 1527.59/s  (0.706s, 1451.28/s)  LR: 7.768e-04  Data: 0.010 (0.021)
Train: 189 [ 250/1251 ( 20%)]  Loss: 3.333 (3.73)  Time: 0.708s, 1445.90/s  (0.704s, 1454.81/s)  LR: 7.768e-04  Data: 0.010 (0.019)
Train: 189 [ 300/1251 ( 24%)]  Loss: 3.766 (3.73)  Time: 0.774s, 1322.99/s  (0.702s, 1459.52/s)  LR: 7.768e-04  Data: 0.015 (0.017)
Train: 189 [ 350/1251 ( 28%)]  Loss: 3.696 (3.73)  Time: 0.670s, 1527.61/s  (0.700s, 1462.16/s)  LR: 7.768e-04  Data: 0.011 (0.016)
Train: 189 [ 400/1251 ( 32%)]  Loss: 3.571 (3.71)  Time: 0.672s, 1524.58/s  (0.699s, 1464.77/s)  LR: 7.768e-04  Data: 0.010 (0.015)
Train: 189 [ 450/1251 ( 36%)]  Loss: 3.904 (3.73)  Time: 0.744s, 1376.59/s  (0.698s, 1466.35/s)  LR: 7.768e-04  Data: 0.009 (0.015)
Train: 189 [ 500/1251 ( 40%)]  Loss: 3.675 (3.72)  Time: 0.722s, 1418.13/s  (0.698s, 1467.35/s)  LR: 7.768e-04  Data: 0.009 (0.014)
Train: 189 [ 550/1251 ( 44%)]  Loss: 3.499 (3.71)  Time: 0.705s, 1451.63/s  (0.698s, 1466.53/s)  LR: 7.768e-04  Data: 0.009 (0.014)
Train: 189 [ 600/1251 ( 48%)]  Loss: 3.497 (3.69)  Time: 0.666s, 1536.60/s  (0.699s, 1465.94/s)  LR: 7.768e-04  Data: 0.011 (0.014)
Train: 189 [ 650/1251 ( 52%)]  Loss: 3.700 (3.69)  Time: 0.672s, 1523.21/s  (0.698s, 1467.46/s)  LR: 7.768e-04  Data: 0.010 (0.013)
Train: 189 [ 700/1251 ( 56%)]  Loss: 3.557 (3.68)  Time: 0.711s, 1441.20/s  (0.698s, 1467.21/s)  LR: 7.768e-04  Data: 0.009 (0.013)
Train: 189 [ 750/1251 ( 60%)]  Loss: 3.445 (3.67)  Time: 0.671s, 1527.02/s  (0.698s, 1467.73/s)  LR: 7.768e-04  Data: 0.009 (0.013)
Train: 189 [ 800/1251 ( 64%)]  Loss: 3.575 (3.66)  Time: 0.681s, 1503.22/s  (0.698s, 1468.01/s)  LR: 7.768e-04  Data: 0.010 (0.013)
Train: 189 [ 850/1251 ( 68%)]  Loss: 3.669 (3.66)  Time: 0.702s, 1458.46/s  (0.697s, 1469.54/s)  LR: 7.768e-04  Data: 0.010 (0.013)
Train: 189 [ 900/1251 ( 72%)]  Loss: 3.965 (3.68)  Time: 0.705s, 1451.77/s  (0.697s, 1468.66/s)  LR: 7.768e-04  Data: 0.009 (0.013)
Train: 189 [ 950/1251 ( 76%)]  Loss: 3.736 (3.68)  Time: 0.691s, 1482.25/s  (0.697s, 1468.49/s)  LR: 7.768e-04  Data: 0.011 (0.012)
Train: 189 [1000/1251 ( 80%)]  Loss: 3.595 (3.68)  Time: 0.671s, 1525.47/s  (0.697s, 1469.27/s)  LR: 7.768e-04  Data: 0.010 (0.012)
Train: 189 [1050/1251 ( 84%)]  Loss: 3.244 (3.66)  Time: 0.672s, 1524.04/s  (0.697s, 1469.64/s)  LR: 7.768e-04  Data: 0.010 (0.012)
Train: 189 [1100/1251 ( 88%)]  Loss: 3.866 (3.67)  Time: 0.773s, 1325.28/s  (0.697s, 1468.15/s)  LR: 7.768e-04  Data: 0.009 (0.012)
Train: 189 [1150/1251 ( 92%)]  Loss: 3.600 (3.66)  Time: 0.709s, 1445.19/s  (0.698s, 1467.13/s)  LR: 7.768e-04  Data: 0.010 (0.012)
Train: 189 [1200/1251 ( 96%)]  Loss: 3.699 (3.66)  Time: 0.743s, 1377.87/s  (0.699s, 1465.66/s)  LR: 7.768e-04  Data: 0.009 (0.012)
Train: 189 [1250/1251 (100%)]  Loss: 3.836 (3.67)  Time: 0.694s, 1475.30/s  (0.699s, 1465.06/s)  LR: 7.768e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.523 (1.523)  Loss:  0.9097 (0.9097)  Acc@1: 88.1836 (88.1836)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.136 (0.569)  Loss:  0.8955 (1.5274)  Acc@1: 82.1934 (70.7840)  Acc@5: 96.1085 (90.2180)
Train: 190 [   0/1251 (  0%)]  Loss: 3.990 (3.99)  Time: 2.227s,  459.90/s  (2.227s,  459.90/s)  LR: 7.746e-04  Data: 1.559 (1.559)
Train: 190 [  50/1251 (  4%)]  Loss: 3.939 (3.96)  Time: 0.699s, 1464.29/s  (0.713s, 1435.62/s)  LR: 7.746e-04  Data: 0.010 (0.046)
Train: 190 [ 100/1251 (  8%)]  Loss: 3.597 (3.84)  Time: 0.675s, 1517.69/s  (0.702s, 1459.62/s)  LR: 7.746e-04  Data: 0.011 (0.028)
Train: 190 [ 150/1251 ( 12%)]  Loss: 3.634 (3.79)  Time: 0.681s, 1502.99/s  (0.702s, 1459.45/s)  LR: 7.746e-04  Data: 0.014 (0.022)
Train: 190 [ 200/1251 ( 16%)]  Loss: 3.644 (3.76)  Time: 0.678s, 1510.53/s  (0.699s, 1464.77/s)  LR: 7.746e-04  Data: 0.011 (0.019)
Train: 190 [ 250/1251 ( 20%)]  Loss: 4.040 (3.81)  Time: 0.743s, 1377.48/s  (0.699s, 1465.81/s)  LR: 7.746e-04  Data: 0.010 (0.018)
Train: 190 [ 300/1251 ( 24%)]  Loss: 3.841 (3.81)  Time: 0.691s, 1482.30/s  (0.697s, 1469.57/s)  LR: 7.746e-04  Data: 0.009 (0.016)
Train: 190 [ 350/1251 ( 28%)]  Loss: 4.032 (3.84)  Time: 0.691s, 1481.44/s  (0.698s, 1467.89/s)  LR: 7.746e-04  Data: 0.014 (0.016)
Train: 190 [ 400/1251 ( 32%)]  Loss: 3.830 (3.84)  Time: 0.671s, 1525.10/s  (0.697s, 1468.92/s)  LR: 7.746e-04  Data: 0.010 (0.015)
Train: 190 [ 450/1251 ( 36%)]  Loss: 3.749 (3.83)  Time: 0.711s, 1439.53/s  (0.697s, 1468.57/s)  LR: 7.746e-04  Data: 0.010 (0.014)
Train: 190 [ 500/1251 ( 40%)]  Loss: 3.837 (3.83)  Time: 0.691s, 1482.52/s  (0.698s, 1467.90/s)  LR: 7.746e-04  Data: 0.009 (0.014)
Train: 190 [ 550/1251 ( 44%)]  Loss: 3.944 (3.84)  Time: 0.673s, 1521.64/s  (0.698s, 1466.87/s)  LR: 7.746e-04  Data: 0.011 (0.014)
Train: 190 [ 600/1251 ( 48%)]  Loss: 3.578 (3.82)  Time: 0.682s, 1500.83/s  (0.697s, 1468.49/s)  LR: 7.746e-04  Data: 0.010 (0.013)
Train: 190 [ 650/1251 ( 52%)]  Loss: 3.748 (3.81)  Time: 0.702s, 1458.87/s  (0.697s, 1470.13/s)  LR: 7.746e-04  Data: 0.011 (0.013)
Train: 190 [ 700/1251 ( 56%)]  Loss: 3.859 (3.82)  Time: 0.683s, 1500.22/s  (0.696s, 1470.72/s)  LR: 7.746e-04  Data: 0.010 (0.013)
Train: 190 [ 750/1251 ( 60%)]  Loss: 3.806 (3.82)  Time: 0.690s, 1484.64/s  (0.697s, 1469.71/s)  LR: 7.746e-04  Data: 0.010 (0.013)
Train: 190 [ 800/1251 ( 64%)]  Loss: 3.471 (3.80)  Time: 0.673s, 1522.04/s  (0.696s, 1470.82/s)  LR: 7.746e-04  Data: 0.010 (0.013)
Train: 190 [ 850/1251 ( 68%)]  Loss: 3.598 (3.79)  Time: 0.672s, 1523.40/s  (0.696s, 1471.81/s)  LR: 7.746e-04  Data: 0.010 (0.012)
Train: 190 [ 900/1251 ( 72%)]  Loss: 3.810 (3.79)  Time: 0.696s, 1470.71/s  (0.696s, 1472.10/s)  LR: 7.746e-04  Data: 0.009 (0.012)
Train: 190 [ 950/1251 ( 76%)]  Loss: 3.792 (3.79)  Time: 0.677s, 1512.44/s  (0.695s, 1472.51/s)  LR: 7.746e-04  Data: 0.011 (0.012)
Train: 190 [1000/1251 ( 80%)]  Loss: 3.563 (3.78)  Time: 0.673s, 1521.63/s  (0.695s, 1473.44/s)  LR: 7.746e-04  Data: 0.010 (0.012)
Train: 190 [1050/1251 ( 84%)]  Loss: 3.818 (3.78)  Time: 0.672s, 1522.86/s  (0.695s, 1473.51/s)  LR: 7.746e-04  Data: 0.010 (0.012)
Train: 190 [1100/1251 ( 88%)]  Loss: 4.006 (3.79)  Time: 0.705s, 1452.00/s  (0.695s, 1473.61/s)  LR: 7.746e-04  Data: 0.010 (0.012)
Train: 190 [1150/1251 ( 92%)]  Loss: 3.624 (3.78)  Time: 0.703s, 1457.11/s  (0.695s, 1473.84/s)  LR: 7.746e-04  Data: 0.009 (0.012)
Train: 190 [1200/1251 ( 96%)]  Loss: 3.918 (3.79)  Time: 0.671s, 1525.27/s  (0.694s, 1474.80/s)  LR: 7.746e-04  Data: 0.009 (0.012)
Train: 190 [1250/1251 (100%)]  Loss: 4.075 (3.80)  Time: 0.693s, 1478.56/s  (0.694s, 1474.59/s)  LR: 7.746e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.574 (1.574)  Loss:  0.7661 (0.7661)  Acc@1: 88.1836 (88.1836)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.136 (0.583)  Loss:  0.9263 (1.4302)  Acc@1: 83.9623 (71.3260)  Acc@5: 95.1651 (90.4740)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-187.pth.tar', 71.87000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-190.pth.tar', 71.32600001464844)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-180.pth.tar', 71.26400010009766)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-179.pth.tar', 71.23600001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-164.pth.tar', 71.10999998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-175.pth.tar', 71.1080000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-177.pth.tar', 71.08200007324218)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-167.pth.tar', 71.03800007324219)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-185.pth.tar', 70.88799991455078)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-173.pth.tar', 70.8580001171875)

Train: 191 [   0/1251 (  0%)]  Loss: 3.956 (3.96)  Time: 2.181s,  469.46/s  (2.181s,  469.46/s)  LR: 7.724e-04  Data: 1.566 (1.566)
Train: 191 [  50/1251 (  4%)]  Loss: 3.464 (3.71)  Time: 0.704s, 1453.65/s  (0.732s, 1398.80/s)  LR: 7.724e-04  Data: 0.009 (0.046)
Train: 191 [ 100/1251 (  8%)]  Loss: 3.503 (3.64)  Time: 0.702s, 1457.66/s  (0.714s, 1433.72/s)  LR: 7.724e-04  Data: 0.009 (0.029)
Train: 191 [ 150/1251 ( 12%)]  Loss: 3.936 (3.72)  Time: 0.666s, 1537.65/s  (0.707s, 1448.95/s)  LR: 7.724e-04  Data: 0.010 (0.022)
Train: 191 [ 200/1251 ( 16%)]  Loss: 3.545 (3.68)  Time: 0.701s, 1459.84/s  (0.706s, 1449.99/s)  LR: 7.724e-04  Data: 0.009 (0.019)
Train: 191 [ 250/1251 ( 20%)]  Loss: 4.216 (3.77)  Time: 0.673s, 1522.63/s  (0.702s, 1457.65/s)  LR: 7.724e-04  Data: 0.009 (0.017)
Train: 191 [ 300/1251 ( 24%)]  Loss: 3.484 (3.73)  Time: 0.701s, 1459.74/s  (0.701s, 1461.49/s)  LR: 7.724e-04  Data: 0.009 (0.016)
Train: 191 [ 350/1251 ( 28%)]  Loss: 3.612 (3.71)  Time: 0.773s, 1324.26/s  (0.701s, 1461.67/s)  LR: 7.724e-04  Data: 0.009 (0.015)
Train: 191 [ 400/1251 ( 32%)]  Loss: 3.490 (3.69)  Time: 0.672s, 1524.81/s  (0.701s, 1461.50/s)  LR: 7.724e-04  Data: 0.010 (0.015)
Train: 191 [ 450/1251 ( 36%)]  Loss: 3.872 (3.71)  Time: 0.687s, 1490.72/s  (0.700s, 1463.28/s)  LR: 7.724e-04  Data: 0.010 (0.014)
Train: 191 [ 500/1251 ( 40%)]  Loss: 4.023 (3.74)  Time: 0.673s, 1522.14/s  (0.699s, 1464.91/s)  LR: 7.724e-04  Data: 0.010 (0.014)
Train: 191 [ 550/1251 ( 44%)]  Loss: 3.951 (3.75)  Time: 0.672s, 1523.32/s  (0.698s, 1466.54/s)  LR: 7.724e-04  Data: 0.010 (0.014)
Train: 191 [ 600/1251 ( 48%)]  Loss: 3.379 (3.73)  Time: 0.712s, 1438.87/s  (0.698s, 1466.76/s)  LR: 7.724e-04  Data: 0.009 (0.013)
Train: 191 [ 650/1251 ( 52%)]  Loss: 3.699 (3.72)  Time: 0.672s, 1523.20/s  (0.698s, 1467.45/s)  LR: 7.724e-04  Data: 0.010 (0.013)
Train: 191 [ 700/1251 ( 56%)]  Loss: 3.858 (3.73)  Time: 0.672s, 1524.64/s  (0.698s, 1467.09/s)  LR: 7.724e-04  Data: 0.010 (0.013)
Train: 191 [ 750/1251 ( 60%)]  Loss: 3.583 (3.72)  Time: 0.798s, 1282.75/s  (0.698s, 1467.42/s)  LR: 7.724e-04  Data: 0.009 (0.013)
Train: 191 [ 800/1251 ( 64%)]  Loss: 3.745 (3.72)  Time: 0.691s, 1481.58/s  (0.698s, 1467.54/s)  LR: 7.724e-04  Data: 0.019 (0.013)
Train: 191 [ 850/1251 ( 68%)]  Loss: 3.679 (3.72)  Time: 0.673s, 1522.04/s  (0.698s, 1467.86/s)  LR: 7.724e-04  Data: 0.011 (0.012)
Train: 191 [ 900/1251 ( 72%)]  Loss: 3.477 (3.71)  Time: 0.738s, 1388.18/s  (0.698s, 1467.43/s)  LR: 7.724e-04  Data: 0.010 (0.012)
Train: 191 [ 950/1251 ( 76%)]  Loss: 3.858 (3.72)  Time: 0.688s, 1488.40/s  (0.698s, 1467.73/s)  LR: 7.724e-04  Data: 0.010 (0.012)
Train: 191 [1000/1251 ( 80%)]  Loss: 4.023 (3.73)  Time: 0.667s, 1534.58/s  (0.697s, 1468.28/s)  LR: 7.724e-04  Data: 0.010 (0.012)
Train: 191 [1050/1251 ( 84%)]  Loss: 4.142 (3.75)  Time: 0.702s, 1459.12/s  (0.697s, 1469.12/s)  LR: 7.724e-04  Data: 0.010 (0.012)
Train: 191 [1100/1251 ( 88%)]  Loss: 3.887 (3.76)  Time: 0.672s, 1523.63/s  (0.697s, 1470.15/s)  LR: 7.724e-04  Data: 0.010 (0.012)
Train: 191 [1150/1251 ( 92%)]  Loss: 3.918 (3.76)  Time: 0.713s, 1436.23/s  (0.696s, 1470.58/s)  LR: 7.724e-04  Data: 0.010 (0.012)
Train: 191 [1200/1251 ( 96%)]  Loss: 3.811 (3.76)  Time: 0.669s, 1529.78/s  (0.696s, 1470.98/s)  LR: 7.724e-04  Data: 0.009 (0.012)
Train: 191 [1250/1251 (100%)]  Loss: 3.838 (3.77)  Time: 0.666s, 1537.36/s  (0.696s, 1471.43/s)  LR: 7.724e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.542 (1.542)  Loss:  1.0098 (1.0098)  Acc@1: 86.3281 (86.3281)  Acc@5: 96.2891 (96.2891)
Test: [  48/48]  Time: 0.136 (0.579)  Loss:  1.2969 (1.6355)  Acc@1: 82.0755 (70.2680)  Acc@5: 95.2830 (89.9460)
Train: 192 [   0/1251 (  0%)]  Loss: 3.373 (3.37)  Time: 2.677s,  382.47/s  (2.677s,  382.47/s)  LR: 7.702e-04  Data: 2.048 (2.048)
Train: 192 [  50/1251 (  4%)]  Loss: 3.687 (3.53)  Time: 0.713s, 1435.38/s  (0.736s, 1391.88/s)  LR: 7.702e-04  Data: 0.009 (0.060)
Train: 192 [ 100/1251 (  8%)]  Loss: 3.897 (3.65)  Time: 0.665s, 1539.22/s  (0.712s, 1438.93/s)  LR: 7.702e-04  Data: 0.010 (0.035)
Train: 192 [ 150/1251 ( 12%)]  Loss: 4.025 (3.75)  Time: 0.673s, 1521.58/s  (0.705s, 1451.96/s)  LR: 7.702e-04  Data: 0.011 (0.027)
Train: 192 [ 200/1251 ( 16%)]  Loss: 4.118 (3.82)  Time: 0.697s, 1469.13/s  (0.701s, 1460.48/s)  LR: 7.702e-04  Data: 0.010 (0.023)
Train: 192 [ 250/1251 ( 20%)]  Loss: 3.531 (3.77)  Time: 0.676s, 1515.03/s  (0.700s, 1463.67/s)  LR: 7.702e-04  Data: 0.010 (0.020)
Train: 192 [ 300/1251 ( 24%)]  Loss: 3.812 (3.78)  Time: 0.666s, 1537.20/s  (0.698s, 1466.13/s)  LR: 7.702e-04  Data: 0.011 (0.019)
Train: 192 [ 350/1251 ( 28%)]  Loss: 3.783 (3.78)  Time: 0.673s, 1520.61/s  (0.699s, 1465.02/s)  LR: 7.702e-04  Data: 0.010 (0.018)
Train: 192 [ 400/1251 ( 32%)]  Loss: 3.696 (3.77)  Time: 0.686s, 1492.66/s  (0.699s, 1464.86/s)  LR: 7.702e-04  Data: 0.010 (0.017)
Train: 192 [ 450/1251 ( 36%)]  Loss: 3.913 (3.78)  Time: 0.691s, 1481.57/s  (0.698s, 1467.03/s)  LR: 7.702e-04  Data: 0.009 (0.016)
Train: 192 [ 500/1251 ( 40%)]  Loss: 3.861 (3.79)  Time: 0.756s, 1355.16/s  (0.698s, 1467.31/s)  LR: 7.702e-04  Data: 0.010 (0.015)
Train: 192 [ 550/1251 ( 44%)]  Loss: 3.619 (3.78)  Time: 0.674s, 1519.61/s  (0.697s, 1468.88/s)  LR: 7.702e-04  Data: 0.011 (0.015)
Train: 192 [ 600/1251 ( 48%)]  Loss: 4.044 (3.80)  Time: 0.673s, 1522.02/s  (0.697s, 1469.60/s)  LR: 7.702e-04  Data: 0.010 (0.014)
Train: 192 [ 650/1251 ( 52%)]  Loss: 3.536 (3.78)  Time: 0.708s, 1445.36/s  (0.697s, 1468.53/s)  LR: 7.702e-04  Data: 0.010 (0.014)
Train: 192 [ 700/1251 ( 56%)]  Loss: 3.742 (3.78)  Time: 0.678s, 1509.80/s  (0.697s, 1469.22/s)  LR: 7.702e-04  Data: 0.011 (0.014)
Train: 192 [ 750/1251 ( 60%)]  Loss: 3.835 (3.78)  Time: 0.722s, 1418.93/s  (0.697s, 1469.94/s)  LR: 7.702e-04  Data: 0.011 (0.014)
Train: 192 [ 800/1251 ( 64%)]  Loss: 3.977 (3.79)  Time: 0.706s, 1451.40/s  (0.696s, 1470.26/s)  LR: 7.702e-04  Data: 0.009 (0.013)
Train: 192 [ 850/1251 ( 68%)]  Loss: 3.702 (3.79)  Time: 0.699s, 1465.73/s  (0.697s, 1470.01/s)  LR: 7.702e-04  Data: 0.009 (0.013)
Train: 192 [ 900/1251 ( 72%)]  Loss: 3.855 (3.79)  Time: 0.711s, 1441.12/s  (0.696s, 1470.27/s)  LR: 7.702e-04  Data: 0.010 (0.013)
Train: 192 [ 950/1251 ( 76%)]  Loss: 4.024 (3.80)  Time: 0.716s, 1430.56/s  (0.697s, 1469.64/s)  LR: 7.702e-04  Data: 0.010 (0.013)
Train: 192 [1000/1251 ( 80%)]  Loss: 3.634 (3.79)  Time: 0.672s, 1523.02/s  (0.697s, 1470.13/s)  LR: 7.702e-04  Data: 0.010 (0.013)
Train: 192 [1050/1251 ( 84%)]  Loss: 3.805 (3.79)  Time: 0.669s, 1530.81/s  (0.696s, 1471.02/s)  LR: 7.702e-04  Data: 0.009 (0.013)
Train: 192 [1100/1251 ( 88%)]  Loss: 3.634 (3.79)  Time: 0.675s, 1517.43/s  (0.696s, 1471.03/s)  LR: 7.702e-04  Data: 0.011 (0.013)
Train: 192 [1150/1251 ( 92%)]  Loss: 3.490 (3.77)  Time: 0.671s, 1525.40/s  (0.696s, 1471.71/s)  LR: 7.702e-04  Data: 0.009 (0.012)
Train: 192 [1200/1251 ( 96%)]  Loss: 3.755 (3.77)  Time: 0.701s, 1459.77/s  (0.696s, 1472.14/s)  LR: 7.702e-04  Data: 0.010 (0.012)
Train: 192 [1250/1251 (100%)]  Loss: 3.943 (3.78)  Time: 0.656s, 1560.36/s  (0.695s, 1472.58/s)  LR: 7.702e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.445 (1.445)  Loss:  1.0127 (1.0127)  Acc@1: 87.3047 (87.3047)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.136 (0.576)  Loss:  1.0176 (1.5712)  Acc@1: 82.4293 (71.3500)  Acc@5: 95.8726 (90.4960)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-187.pth.tar', 71.87000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-192.pth.tar', 71.35000012451172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-190.pth.tar', 71.32600001464844)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-180.pth.tar', 71.26400010009766)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-179.pth.tar', 71.23600001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-164.pth.tar', 71.10999998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-175.pth.tar', 71.1080000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-177.pth.tar', 71.08200007324218)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-167.pth.tar', 71.03800007324219)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-185.pth.tar', 70.88799991455078)

Train: 193 [   0/1251 (  0%)]  Loss: 3.682 (3.68)  Time: 2.348s,  436.05/s  (2.348s,  436.05/s)  LR: 7.680e-04  Data: 1.718 (1.718)
Train: 193 [  50/1251 (  4%)]  Loss: 3.479 (3.58)  Time: 0.702s, 1458.15/s  (0.726s, 1410.99/s)  LR: 7.680e-04  Data: 0.009 (0.051)
Train: 193 [ 100/1251 (  8%)]  Loss: 3.710 (3.62)  Time: 0.708s, 1445.63/s  (0.711s, 1440.54/s)  LR: 7.680e-04  Data: 0.011 (0.031)
Train: 193 [ 150/1251 ( 12%)]  Loss: 3.690 (3.64)  Time: 0.725s, 1411.94/s  (0.709s, 1444.95/s)  LR: 7.680e-04  Data: 0.009 (0.024)
Train: 193 [ 200/1251 ( 16%)]  Loss: 3.314 (3.57)  Time: 0.692s, 1480.58/s  (0.707s, 1449.18/s)  LR: 7.680e-04  Data: 0.010 (0.021)
Train: 193 [ 250/1251 ( 20%)]  Loss: 3.476 (3.56)  Time: 0.723s, 1417.06/s  (0.705s, 1453.25/s)  LR: 7.680e-04  Data: 0.009 (0.019)
Train: 193 [ 300/1251 ( 24%)]  Loss: 3.782 (3.59)  Time: 0.676s, 1513.70/s  (0.704s, 1454.78/s)  LR: 7.680e-04  Data: 0.012 (0.017)
Train: 193 [ 350/1251 ( 28%)]  Loss: 3.649 (3.60)  Time: 0.707s, 1447.61/s  (0.703s, 1455.70/s)  LR: 7.680e-04  Data: 0.009 (0.016)
Train: 193 [ 400/1251 ( 32%)]  Loss: 3.874 (3.63)  Time: 0.674s, 1518.97/s  (0.702s, 1458.37/s)  LR: 7.680e-04  Data: 0.009 (0.016)
Train: 193 [ 450/1251 ( 36%)]  Loss: 3.721 (3.64)  Time: 0.771s, 1328.61/s  (0.701s, 1460.21/s)  LR: 7.680e-04  Data: 0.010 (0.015)
Train: 193 [ 500/1251 ( 40%)]  Loss: 3.898 (3.66)  Time: 0.673s, 1522.43/s  (0.700s, 1461.99/s)  LR: 7.680e-04  Data: 0.010 (0.014)
Train: 193 [ 550/1251 ( 44%)]  Loss: 3.433 (3.64)  Time: 0.706s, 1451.18/s  (0.699s, 1463.97/s)  LR: 7.680e-04  Data: 0.009 (0.014)
Train: 193 [ 600/1251 ( 48%)]  Loss: 3.643 (3.64)  Time: 0.709s, 1444.33/s  (0.699s, 1464.15/s)  LR: 7.680e-04  Data: 0.009 (0.014)
Train: 193 [ 650/1251 ( 52%)]  Loss: 3.641 (3.64)  Time: 0.672s, 1523.75/s  (0.699s, 1465.85/s)  LR: 7.680e-04  Data: 0.010 (0.013)
Train: 193 [ 700/1251 ( 56%)]  Loss: 3.976 (3.66)  Time: 0.688s, 1487.33/s  (0.698s, 1466.13/s)  LR: 7.680e-04  Data: 0.009 (0.013)
Train: 193 [ 750/1251 ( 60%)]  Loss: 3.419 (3.65)  Time: 0.679s, 1508.75/s  (0.698s, 1466.00/s)  LR: 7.680e-04  Data: 0.010 (0.013)
Train: 193 [ 800/1251 ( 64%)]  Loss: 3.937 (3.67)  Time: 0.683s, 1499.42/s  (0.698s, 1466.78/s)  LR: 7.680e-04  Data: 0.010 (0.013)
Train: 193 [ 850/1251 ( 68%)]  Loss: 3.631 (3.66)  Time: 0.667s, 1536.01/s  (0.697s, 1468.15/s)  LR: 7.680e-04  Data: 0.010 (0.013)
Train: 193 [ 900/1251 ( 72%)]  Loss: 3.584 (3.66)  Time: 0.804s, 1273.71/s  (0.697s, 1468.82/s)  LR: 7.680e-04  Data: 0.010 (0.013)
Train: 193 [ 950/1251 ( 76%)]  Loss: 4.072 (3.68)  Time: 0.692s, 1480.01/s  (0.696s, 1470.33/s)  LR: 7.680e-04  Data: 0.009 (0.012)
Train: 193 [1000/1251 ( 80%)]  Loss: 3.972 (3.69)  Time: 0.672s, 1523.67/s  (0.696s, 1470.78/s)  LR: 7.680e-04  Data: 0.010 (0.012)
Train: 193 [1050/1251 ( 84%)]  Loss: 3.908 (3.70)  Time: 0.721s, 1419.68/s  (0.696s, 1470.93/s)  LR: 7.680e-04  Data: 0.009 (0.012)
Train: 193 [1100/1251 ( 88%)]  Loss: 3.500 (3.70)  Time: 0.667s, 1535.91/s  (0.696s, 1471.56/s)  LR: 7.680e-04  Data: 0.011 (0.012)
Train: 193 [1150/1251 ( 92%)]  Loss: 3.662 (3.69)  Time: 0.704s, 1453.77/s  (0.696s, 1472.07/s)  LR: 7.680e-04  Data: 0.009 (0.012)
Train: 193 [1200/1251 ( 96%)]  Loss: 3.985 (3.71)  Time: 0.685s, 1494.88/s  (0.696s, 1472.25/s)  LR: 7.680e-04  Data: 0.015 (0.012)
Train: 193 [1250/1251 (100%)]  Loss: 3.658 (3.70)  Time: 0.694s, 1475.57/s  (0.695s, 1472.56/s)  LR: 7.680e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.625 (1.625)  Loss:  0.9453 (0.9453)  Acc@1: 87.0117 (87.0117)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.136 (0.587)  Loss:  1.2676 (1.5468)  Acc@1: 78.8915 (70.4800)  Acc@5: 93.6321 (89.9260)
Train: 194 [   0/1251 (  0%)]  Loss: 3.546 (3.55)  Time: 2.261s,  452.94/s  (2.261s,  452.94/s)  LR: 7.658e-04  Data: 1.646 (1.646)
Train: 194 [  50/1251 (  4%)]  Loss: 3.993 (3.77)  Time: 0.681s, 1502.98/s  (0.728s, 1406.05/s)  LR: 7.658e-04  Data: 0.010 (0.051)
Train: 194 [ 100/1251 (  8%)]  Loss: 3.982 (3.84)  Time: 0.690s, 1483.87/s  (0.711s, 1440.79/s)  LR: 7.658e-04  Data: 0.009 (0.031)
Train: 194 [ 150/1251 ( 12%)]  Loss: 3.636 (3.79)  Time: 0.685s, 1495.55/s  (0.706s, 1449.59/s)  LR: 7.658e-04  Data: 0.013 (0.024)
Train: 194 [ 200/1251 ( 16%)]  Loss: 3.770 (3.79)  Time: 0.694s, 1474.75/s  (0.704s, 1454.17/s)  LR: 7.658e-04  Data: 0.010 (0.021)
Train: 194 [ 250/1251 ( 20%)]  Loss: 3.387 (3.72)  Time: 0.674s, 1520.28/s  (0.703s, 1457.41/s)  LR: 7.658e-04  Data: 0.011 (0.018)
Train: 194 [ 300/1251 ( 24%)]  Loss: 4.051 (3.77)  Time: 0.672s, 1524.15/s  (0.700s, 1463.67/s)  LR: 7.658e-04  Data: 0.011 (0.017)
Train: 194 [ 350/1251 ( 28%)]  Loss: 3.731 (3.76)  Time: 0.715s, 1432.48/s  (0.697s, 1468.15/s)  LR: 7.658e-04  Data: 0.009 (0.016)
Train: 194 [ 400/1251 ( 32%)]  Loss: 3.888 (3.78)  Time: 0.692s, 1478.76/s  (0.697s, 1469.86/s)  LR: 7.658e-04  Data: 0.010 (0.015)
Train: 194 [ 450/1251 ( 36%)]  Loss: 4.280 (3.83)  Time: 0.707s, 1448.85/s  (0.696s, 1471.85/s)  LR: 7.658e-04  Data: 0.010 (0.015)
Train: 194 [ 500/1251 ( 40%)]  Loss: 4.050 (3.85)  Time: 0.704s, 1454.22/s  (0.695s, 1473.33/s)  LR: 7.658e-04  Data: 0.010 (0.014)
Train: 194 [ 550/1251 ( 44%)]  Loss: 3.550 (3.82)  Time: 0.688s, 1487.31/s  (0.695s, 1473.67/s)  LR: 7.658e-04  Data: 0.010 (0.014)
Train: 194 [ 600/1251 ( 48%)]  Loss: 3.537 (3.80)  Time: 0.761s, 1346.05/s  (0.695s, 1474.14/s)  LR: 7.658e-04  Data: 0.009 (0.014)
Train: 194 [ 650/1251 ( 52%)]  Loss: 4.088 (3.82)  Time: 0.698s, 1467.27/s  (0.695s, 1474.43/s)  LR: 7.658e-04  Data: 0.010 (0.013)
Train: 194 [ 700/1251 ( 56%)]  Loss: 3.781 (3.82)  Time: 0.721s, 1420.28/s  (0.695s, 1474.13/s)  LR: 7.658e-04  Data: 0.010 (0.013)
Train: 194 [ 750/1251 ( 60%)]  Loss: 3.879 (3.82)  Time: 0.708s, 1446.10/s  (0.695s, 1473.23/s)  LR: 7.658e-04  Data: 0.011 (0.013)
Train: 194 [ 800/1251 ( 64%)]  Loss: 3.850 (3.82)  Time: 0.700s, 1463.37/s  (0.695s, 1474.42/s)  LR: 7.658e-04  Data: 0.010 (0.013)
Train: 194 [ 850/1251 ( 68%)]  Loss: 3.483 (3.80)  Time: 0.694s, 1474.59/s  (0.695s, 1474.01/s)  LR: 7.658e-04  Data: 0.010 (0.013)
Train: 194 [ 900/1251 ( 72%)]  Loss: 3.773 (3.80)  Time: 0.680s, 1506.90/s  (0.695s, 1474.21/s)  LR: 7.658e-04  Data: 0.009 (0.012)
Train: 194 [ 950/1251 ( 76%)]  Loss: 4.060 (3.82)  Time: 0.672s, 1522.91/s  (0.694s, 1474.51/s)  LR: 7.658e-04  Data: 0.012 (0.012)
Train: 194 [1000/1251 ( 80%)]  Loss: 4.040 (3.83)  Time: 0.728s, 1406.24/s  (0.695s, 1473.68/s)  LR: 7.658e-04  Data: 0.009 (0.012)
Train: 194 [1050/1251 ( 84%)]  Loss: 3.674 (3.82)  Time: 0.715s, 1431.78/s  (0.695s, 1473.67/s)  LR: 7.658e-04  Data: 0.009 (0.012)
Train: 194 [1100/1251 ( 88%)]  Loss: 3.622 (3.81)  Time: 0.702s, 1458.05/s  (0.695s, 1474.21/s)  LR: 7.658e-04  Data: 0.010 (0.012)
Train: 194 [1150/1251 ( 92%)]  Loss: 3.644 (3.80)  Time: 0.669s, 1530.15/s  (0.694s, 1474.88/s)  LR: 7.658e-04  Data: 0.008 (0.012)
Train: 194 [1200/1251 ( 96%)]  Loss: 3.891 (3.81)  Time: 0.712s, 1438.25/s  (0.694s, 1475.11/s)  LR: 7.658e-04  Data: 0.010 (0.012)
Train: 194 [1250/1251 (100%)]  Loss: 3.792 (3.81)  Time: 0.656s, 1559.99/s  (0.694s, 1475.34/s)  LR: 7.658e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.617 (1.617)  Loss:  1.1309 (1.1309)  Acc@1: 87.6953 (87.6953)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.136 (0.588)  Loss:  1.1768 (1.5527)  Acc@1: 81.8396 (71.2260)  Acc@5: 95.2830 (90.6220)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-187.pth.tar', 71.87000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-192.pth.tar', 71.35000012451172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-190.pth.tar', 71.32600001464844)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-180.pth.tar', 71.26400010009766)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-179.pth.tar', 71.23600001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-194.pth.tar', 71.22599999755859)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-164.pth.tar', 71.10999998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-175.pth.tar', 71.1080000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-177.pth.tar', 71.08200007324218)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-167.pth.tar', 71.03800007324219)

Train: 195 [   0/1251 (  0%)]  Loss: 3.490 (3.49)  Time: 2.075s,  493.59/s  (2.075s,  493.59/s)  LR: 7.636e-04  Data: 1.454 (1.454)
Train: 195 [  50/1251 (  4%)]  Loss: 3.960 (3.72)  Time: 0.673s, 1520.64/s  (0.729s, 1405.61/s)  LR: 7.636e-04  Data: 0.010 (0.046)
Train: 195 [ 100/1251 (  8%)]  Loss: 3.743 (3.73)  Time: 0.703s, 1457.27/s  (0.711s, 1439.77/s)  LR: 7.636e-04  Data: 0.010 (0.028)
Train: 195 [ 150/1251 ( 12%)]  Loss: 4.065 (3.81)  Time: 0.672s, 1523.55/s  (0.708s, 1446.95/s)  LR: 7.636e-04  Data: 0.010 (0.023)
Train: 195 [ 200/1251 ( 16%)]  Loss: 3.782 (3.81)  Time: 0.712s, 1438.11/s  (0.705s, 1452.78/s)  LR: 7.636e-04  Data: 0.010 (0.019)
Train: 195 [ 250/1251 ( 20%)]  Loss: 4.162 (3.87)  Time: 0.673s, 1522.45/s  (0.702s, 1458.31/s)  LR: 7.636e-04  Data: 0.010 (0.018)
Train: 195 [ 300/1251 ( 24%)]  Loss: 4.050 (3.89)  Time: 0.670s, 1528.21/s  (0.700s, 1463.39/s)  LR: 7.636e-04  Data: 0.009 (0.016)
Train: 195 [ 350/1251 ( 28%)]  Loss: 4.081 (3.92)  Time: 0.690s, 1484.93/s  (0.699s, 1463.91/s)  LR: 7.636e-04  Data: 0.009 (0.015)
Train: 195 [ 400/1251 ( 32%)]  Loss: 3.577 (3.88)  Time: 0.679s, 1507.51/s  (0.699s, 1465.24/s)  LR: 7.636e-04  Data: 0.010 (0.015)
Train: 195 [ 450/1251 ( 36%)]  Loss: 3.699 (3.86)  Time: 0.698s, 1467.74/s  (0.697s, 1468.15/s)  LR: 7.636e-04  Data: 0.009 (0.014)
Train: 195 [ 500/1251 ( 40%)]  Loss: 3.474 (3.83)  Time: 0.751s, 1364.15/s  (0.697s, 1468.43/s)  LR: 7.636e-04  Data: 0.009 (0.014)
Train: 195 [ 550/1251 ( 44%)]  Loss: 4.127 (3.85)  Time: 0.672s, 1522.88/s  (0.697s, 1468.62/s)  LR: 7.636e-04  Data: 0.010 (0.013)
Train: 195 [ 600/1251 ( 48%)]  Loss: 4.077 (3.87)  Time: 0.674s, 1519.24/s  (0.696s, 1470.21/s)  LR: 7.636e-04  Data: 0.009 (0.013)
Train: 195 [ 650/1251 ( 52%)]  Loss: 3.471 (3.84)  Time: 0.671s, 1527.15/s  (0.696s, 1471.73/s)  LR: 7.636e-04  Data: 0.010 (0.013)
Train: 195 [ 700/1251 ( 56%)]  Loss: 3.924 (3.85)  Time: 0.712s, 1437.76/s  (0.696s, 1471.22/s)  LR: 7.636e-04  Data: 0.009 (0.013)
Train: 195 [ 750/1251 ( 60%)]  Loss: 3.946 (3.85)  Time: 0.673s, 1522.35/s  (0.696s, 1472.19/s)  LR: 7.636e-04  Data: 0.010 (0.013)
Train: 195 [ 800/1251 ( 64%)]  Loss: 3.823 (3.85)  Time: 0.671s, 1525.86/s  (0.695s, 1472.99/s)  LR: 7.636e-04  Data: 0.009 (0.012)
Train: 195 [ 850/1251 ( 68%)]  Loss: 3.543 (3.83)  Time: 0.666s, 1537.54/s  (0.695s, 1473.52/s)  LR: 7.636e-04  Data: 0.010 (0.012)
Train: 195 [ 900/1251 ( 72%)]  Loss: 3.712 (3.83)  Time: 0.672s, 1523.77/s  (0.695s, 1473.29/s)  LR: 7.636e-04  Data: 0.010 (0.012)
Train: 195 [ 950/1251 ( 76%)]  Loss: 3.683 (3.82)  Time: 0.670s, 1528.43/s  (0.695s, 1472.91/s)  LR: 7.636e-04  Data: 0.010 (0.012)
Train: 195 [1000/1251 ( 80%)]  Loss: 3.324 (3.80)  Time: 0.711s, 1440.91/s  (0.695s, 1473.30/s)  LR: 7.636e-04  Data: 0.010 (0.012)
Train: 195 [1050/1251 ( 84%)]  Loss: 3.981 (3.80)  Time: 0.778s, 1315.86/s  (0.695s, 1472.65/s)  LR: 7.636e-04  Data: 0.009 (0.012)
Train: 195 [1100/1251 ( 88%)]  Loss: 3.507 (3.79)  Time: 0.742s, 1379.64/s  (0.695s, 1473.27/s)  LR: 7.636e-04  Data: 0.010 (0.012)
Train: 195 [1150/1251 ( 92%)]  Loss: 3.871 (3.79)  Time: 0.673s, 1520.94/s  (0.695s, 1473.40/s)  LR: 7.636e-04  Data: 0.010 (0.012)
Train: 195 [1200/1251 ( 96%)]  Loss: 3.734 (3.79)  Time: 0.706s, 1450.90/s  (0.695s, 1473.65/s)  LR: 7.636e-04  Data: 0.009 (0.012)
Train: 195 [1250/1251 (100%)]  Loss: 3.930 (3.80)  Time: 0.736s, 1391.84/s  (0.695s, 1473.71/s)  LR: 7.636e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.603 (1.603)  Loss:  0.9170 (0.9170)  Acc@1: 89.0625 (89.0625)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.136 (0.586)  Loss:  1.0117 (1.5337)  Acc@1: 84.5519 (71.1180)  Acc@5: 95.6368 (90.2400)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-187.pth.tar', 71.87000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-192.pth.tar', 71.35000012451172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-190.pth.tar', 71.32600001464844)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-180.pth.tar', 71.26400010009766)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-179.pth.tar', 71.23600001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-194.pth.tar', 71.22599999755859)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-195.pth.tar', 71.11800001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-164.pth.tar', 71.10999998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-175.pth.tar', 71.1080000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-177.pth.tar', 71.08200007324218)

Train: 196 [   0/1251 (  0%)]  Loss: 4.076 (4.08)  Time: 2.512s,  407.64/s  (2.512s,  407.64/s)  LR: 7.614e-04  Data: 1.830 (1.830)
Train: 196 [  50/1251 (  4%)]  Loss: 3.778 (3.93)  Time: 0.674s, 1518.52/s  (0.729s, 1404.31/s)  LR: 7.614e-04  Data: 0.010 (0.047)
Train: 196 [ 100/1251 (  8%)]  Loss: 3.608 (3.82)  Time: 0.678s, 1511.12/s  (0.710s, 1442.81/s)  LR: 7.614e-04  Data: 0.009 (0.029)
Train: 196 [ 150/1251 ( 12%)]  Loss: 3.422 (3.72)  Time: 0.671s, 1526.56/s  (0.708s, 1446.93/s)  LR: 7.614e-04  Data: 0.011 (0.023)
Train: 196 [ 200/1251 ( 16%)]  Loss: 3.810 (3.74)  Time: 0.677s, 1512.30/s  (0.705s, 1453.25/s)  LR: 7.614e-04  Data: 0.008 (0.020)
Train: 196 [ 250/1251 ( 20%)]  Loss: 3.978 (3.78)  Time: 0.706s, 1449.68/s  (0.702s, 1457.99/s)  LR: 7.614e-04  Data: 0.009 (0.018)
Train: 196 [ 300/1251 ( 24%)]  Loss: 3.751 (3.77)  Time: 0.715s, 1432.42/s  (0.702s, 1459.29/s)  LR: 7.614e-04  Data: 0.010 (0.017)
Train: 196 [ 350/1251 ( 28%)]  Loss: 3.447 (3.73)  Time: 0.741s, 1382.31/s  (0.701s, 1460.91/s)  LR: 7.614e-04  Data: 0.010 (0.016)
Train: 196 [ 400/1251 ( 32%)]  Loss: 3.522 (3.71)  Time: 0.672s, 1524.66/s  (0.700s, 1462.05/s)  LR: 7.614e-04  Data: 0.010 (0.015)
Train: 196 [ 450/1251 ( 36%)]  Loss: 3.851 (3.72)  Time: 0.682s, 1501.74/s  (0.700s, 1463.77/s)  LR: 7.614e-04  Data: 0.010 (0.015)
Train: 196 [ 500/1251 ( 40%)]  Loss: 3.988 (3.75)  Time: 0.672s, 1524.49/s  (0.699s, 1465.00/s)  LR: 7.614e-04  Data: 0.010 (0.014)
Train: 196 [ 550/1251 ( 44%)]  Loss: 4.092 (3.78)  Time: 0.675s, 1517.27/s  (0.699s, 1465.69/s)  LR: 7.614e-04  Data: 0.010 (0.014)
Train: 196 [ 600/1251 ( 48%)]  Loss: 3.971 (3.79)  Time: 0.713s, 1436.04/s  (0.698s, 1467.58/s)  LR: 7.614e-04  Data: 0.010 (0.013)
Train: 196 [ 650/1251 ( 52%)]  Loss: 3.506 (3.77)  Time: 0.717s, 1427.56/s  (0.698s, 1467.51/s)  LR: 7.614e-04  Data: 0.011 (0.013)
Train: 196 [ 700/1251 ( 56%)]  Loss: 3.824 (3.78)  Time: 0.672s, 1522.75/s  (0.698s, 1467.59/s)  LR: 7.614e-04  Data: 0.011 (0.013)
Train: 196 [ 750/1251 ( 60%)]  Loss: 3.983 (3.79)  Time: 0.672s, 1523.85/s  (0.697s, 1468.74/s)  LR: 7.614e-04  Data: 0.010 (0.013)
Train: 196 [ 800/1251 ( 64%)]  Loss: 3.530 (3.77)  Time: 0.708s, 1446.36/s  (0.697s, 1468.91/s)  LR: 7.614e-04  Data: 0.009 (0.013)
Train: 196 [ 850/1251 ( 68%)]  Loss: 3.807 (3.77)  Time: 0.682s, 1501.47/s  (0.697s, 1469.57/s)  LR: 7.614e-04  Data: 0.009 (0.012)
Train: 196 [ 900/1251 ( 72%)]  Loss: 3.434 (3.76)  Time: 0.669s, 1529.77/s  (0.697s, 1469.12/s)  LR: 7.614e-04  Data: 0.009 (0.012)
Train: 196 [ 950/1251 ( 76%)]  Loss: 4.035 (3.77)  Time: 0.674s, 1519.79/s  (0.697s, 1470.15/s)  LR: 7.614e-04  Data: 0.010 (0.012)
Train: 196 [1000/1251 ( 80%)]  Loss: 3.997 (3.78)  Time: 0.709s, 1443.94/s  (0.696s, 1470.70/s)  LR: 7.614e-04  Data: 0.009 (0.012)
Train: 196 [1050/1251 ( 84%)]  Loss: 4.037 (3.79)  Time: 0.674s, 1520.13/s  (0.696s, 1471.21/s)  LR: 7.614e-04  Data: 0.011 (0.012)
Train: 196 [1100/1251 ( 88%)]  Loss: 3.640 (3.79)  Time: 0.674s, 1520.09/s  (0.696s, 1471.51/s)  LR: 7.614e-04  Data: 0.010 (0.012)
Train: 196 [1150/1251 ( 92%)]  Loss: 3.802 (3.79)  Time: 0.666s, 1536.70/s  (0.696s, 1472.07/s)  LR: 7.614e-04  Data: 0.009 (0.012)
Train: 196 [1200/1251 ( 96%)]  Loss: 3.873 (3.79)  Time: 0.711s, 1440.46/s  (0.696s, 1472.31/s)  LR: 7.614e-04  Data: 0.010 (0.012)
Train: 196 [1250/1251 (100%)]  Loss: 3.918 (3.80)  Time: 0.681s, 1504.41/s  (0.696s, 1471.90/s)  LR: 7.614e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.561 (1.561)  Loss:  0.8667 (0.8667)  Acc@1: 87.5000 (87.5000)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.136 (0.589)  Loss:  1.0479 (1.5556)  Acc@1: 83.2547 (71.2200)  Acc@5: 95.8727 (90.3560)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-187.pth.tar', 71.87000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-192.pth.tar', 71.35000012451172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-190.pth.tar', 71.32600001464844)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-180.pth.tar', 71.26400010009766)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-179.pth.tar', 71.23600001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-194.pth.tar', 71.22599999755859)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-196.pth.tar', 71.2199999658203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-195.pth.tar', 71.11800001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-164.pth.tar', 71.10999998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-175.pth.tar', 71.1080000415039)

Train: 197 [   0/1251 (  0%)]  Loss: 3.588 (3.59)  Time: 2.219s,  461.55/s  (2.219s,  461.55/s)  LR: 7.592e-04  Data: 1.562 (1.562)
Train: 197 [  50/1251 (  4%)]  Loss: 3.615 (3.60)  Time: 0.674s, 1519.28/s  (0.723s, 1417.17/s)  LR: 7.592e-04  Data: 0.009 (0.047)
Train: 197 [ 100/1251 (  8%)]  Loss: 3.409 (3.54)  Time: 0.700s, 1462.05/s  (0.710s, 1441.37/s)  LR: 7.592e-04  Data: 0.010 (0.029)
Train: 197 [ 150/1251 ( 12%)]  Loss: 3.697 (3.58)  Time: 0.713s, 1437.03/s  (0.705s, 1452.59/s)  LR: 7.592e-04  Data: 0.009 (0.023)
Train: 197 [ 200/1251 ( 16%)]  Loss: 3.616 (3.58)  Time: 0.670s, 1528.69/s  (0.702s, 1459.00/s)  LR: 7.592e-04  Data: 0.010 (0.020)
Train: 197 [ 250/1251 ( 20%)]  Loss: 3.882 (3.63)  Time: 0.700s, 1462.74/s  (0.699s, 1464.52/s)  LR: 7.592e-04  Data: 0.009 (0.018)
Train: 197 [ 300/1251 ( 24%)]  Loss: 3.926 (3.68)  Time: 0.673s, 1522.18/s  (0.698s, 1467.08/s)  LR: 7.592e-04  Data: 0.011 (0.016)
Train: 197 [ 350/1251 ( 28%)]  Loss: 3.615 (3.67)  Time: 0.755s, 1355.80/s  (0.698s, 1467.16/s)  LR: 7.592e-04  Data: 0.009 (0.016)
Train: 197 [ 400/1251 ( 32%)]  Loss: 3.267 (3.62)  Time: 0.672s, 1524.21/s  (0.698s, 1467.75/s)  LR: 7.592e-04  Data: 0.009 (0.015)
Train: 197 [ 450/1251 ( 36%)]  Loss: 3.600 (3.62)  Time: 0.721s, 1420.20/s  (0.697s, 1468.72/s)  LR: 7.592e-04  Data: 0.010 (0.014)
Train: 197 [ 500/1251 ( 40%)]  Loss: 3.776 (3.64)  Time: 0.704s, 1454.57/s  (0.696s, 1470.52/s)  LR: 7.592e-04  Data: 0.009 (0.014)
Train: 197 [ 550/1251 ( 44%)]  Loss: 3.746 (3.64)  Time: 0.671s, 1525.23/s  (0.696s, 1471.53/s)  LR: 7.592e-04  Data: 0.011 (0.014)
Train: 197 [ 600/1251 ( 48%)]  Loss: 4.141 (3.68)  Time: 0.671s, 1526.55/s  (0.696s, 1470.35/s)  LR: 7.592e-04  Data: 0.013 (0.013)
Train: 197 [ 650/1251 ( 52%)]  Loss: 4.087 (3.71)  Time: 0.696s, 1471.52/s  (0.696s, 1471.44/s)  LR: 7.592e-04  Data: 0.010 (0.013)
Train: 197 [ 700/1251 ( 56%)]  Loss: 3.942 (3.73)  Time: 0.738s, 1387.67/s  (0.696s, 1470.86/s)  LR: 7.592e-04  Data: 0.010 (0.013)
Train: 197 [ 750/1251 ( 60%)]  Loss: 4.100 (3.75)  Time: 0.675s, 1517.52/s  (0.696s, 1471.12/s)  LR: 7.592e-04  Data: 0.013 (0.013)
Train: 197 [ 800/1251 ( 64%)]  Loss: 4.051 (3.77)  Time: 0.703s, 1455.62/s  (0.696s, 1470.90/s)  LR: 7.592e-04  Data: 0.011 (0.013)
Train: 197 [ 850/1251 ( 68%)]  Loss: 3.653 (3.76)  Time: 0.725s, 1411.56/s  (0.696s, 1471.85/s)  LR: 7.592e-04  Data: 0.010 (0.012)
Train: 197 [ 900/1251 ( 72%)]  Loss: 3.459 (3.75)  Time: 0.671s, 1526.18/s  (0.695s, 1472.69/s)  LR: 7.592e-04  Data: 0.010 (0.012)
Train: 197 [ 950/1251 ( 76%)]  Loss: 3.837 (3.75)  Time: 0.707s, 1448.25/s  (0.695s, 1472.65/s)  LR: 7.592e-04  Data: 0.010 (0.012)
Train: 197 [1000/1251 ( 80%)]  Loss: 3.811 (3.75)  Time: 0.671s, 1525.49/s  (0.696s, 1472.24/s)  LR: 7.592e-04  Data: 0.010 (0.012)
Train: 197 [1050/1251 ( 84%)]  Loss: 3.094 (3.72)  Time: 0.673s, 1521.39/s  (0.696s, 1471.65/s)  LR: 7.592e-04  Data: 0.010 (0.012)
Train: 197 [1100/1251 ( 88%)]  Loss: 3.485 (3.71)  Time: 0.668s, 1532.60/s  (0.696s, 1471.90/s)  LR: 7.592e-04  Data: 0.010 (0.012)
Train: 197 [1150/1251 ( 92%)]  Loss: 3.208 (3.69)  Time: 0.686s, 1493.57/s  (0.696s, 1471.91/s)  LR: 7.592e-04  Data: 0.010 (0.012)
Train: 197 [1200/1251 ( 96%)]  Loss: 3.956 (3.70)  Time: 0.717s, 1428.76/s  (0.696s, 1472.32/s)  LR: 7.592e-04  Data: 0.011 (0.012)
Train: 197 [1250/1251 (100%)]  Loss: 3.560 (3.70)  Time: 0.703s, 1457.07/s  (0.695s, 1472.86/s)  LR: 7.592e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.608 (1.608)  Loss:  1.1074 (1.1074)  Acc@1: 88.0859 (88.0859)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.136 (0.584)  Loss:  1.0850 (1.6273)  Acc@1: 82.5472 (71.5100)  Acc@5: 96.9340 (90.5960)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-187.pth.tar', 71.87000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-197.pth.tar', 71.51000004638672)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-192.pth.tar', 71.35000012451172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-190.pth.tar', 71.32600001464844)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-180.pth.tar', 71.26400010009766)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-179.pth.tar', 71.23600001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-194.pth.tar', 71.22599999755859)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-196.pth.tar', 71.2199999658203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-195.pth.tar', 71.11800001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-164.pth.tar', 71.10999998779297)

Train: 198 [   0/1251 (  0%)]  Loss: 3.564 (3.56)  Time: 2.192s,  467.26/s  (2.192s,  467.26/s)  LR: 7.570e-04  Data: 1.519 (1.519)
Train: 198 [  50/1251 (  4%)]  Loss: 3.381 (3.47)  Time: 0.673s, 1521.97/s  (0.732s, 1398.94/s)  LR: 7.570e-04  Data: 0.011 (0.045)
Train: 198 [ 100/1251 (  8%)]  Loss: 3.747 (3.56)  Time: 0.726s, 1410.74/s  (0.718s, 1426.30/s)  LR: 7.570e-04  Data: 0.010 (0.028)
Train: 198 [ 150/1251 ( 12%)]  Loss: 3.734 (3.61)  Time: 0.677s, 1513.19/s  (0.709s, 1445.09/s)  LR: 7.570e-04  Data: 0.010 (0.022)
Train: 198 [ 200/1251 ( 16%)]  Loss: 3.529 (3.59)  Time: 0.676s, 1514.77/s  (0.704s, 1453.90/s)  LR: 7.570e-04  Data: 0.013 (0.019)
Train: 198 [ 250/1251 ( 20%)]  Loss: 3.559 (3.59)  Time: 0.748s, 1369.00/s  (0.702s, 1459.50/s)  LR: 7.570e-04  Data: 0.010 (0.017)
Train: 198 [ 300/1251 ( 24%)]  Loss: 4.039 (3.65)  Time: 0.706s, 1449.92/s  (0.700s, 1462.75/s)  LR: 7.570e-04  Data: 0.009 (0.016)
Train: 198 [ 350/1251 ( 28%)]  Loss: 3.502 (3.63)  Time: 0.672s, 1524.28/s  (0.699s, 1465.75/s)  LR: 7.570e-04  Data: 0.010 (0.015)
Train: 198 [ 400/1251 ( 32%)]  Loss: 3.459 (3.61)  Time: 0.691s, 1482.96/s  (0.698s, 1467.42/s)  LR: 7.570e-04  Data: 0.012 (0.015)
Train: 198 [ 450/1251 ( 36%)]  Loss: 4.121 (3.66)  Time: 0.675s, 1516.82/s  (0.697s, 1468.77/s)  LR: 7.570e-04  Data: 0.009 (0.014)
Train: 198 [ 500/1251 ( 40%)]  Loss: 3.485 (3.65)  Time: 0.707s, 1448.91/s  (0.696s, 1470.45/s)  LR: 7.570e-04  Data: 0.012 (0.014)
Train: 198 [ 550/1251 ( 44%)]  Loss: 3.761 (3.66)  Time: 0.703s, 1457.04/s  (0.696s, 1470.58/s)  LR: 7.570e-04  Data: 0.014 (0.013)
Train: 198 [ 600/1251 ( 48%)]  Loss: 3.665 (3.66)  Time: 0.672s, 1523.14/s  (0.697s, 1469.69/s)  LR: 7.570e-04  Data: 0.014 (0.013)
Train: 198 [ 650/1251 ( 52%)]  Loss: 3.352 (3.64)  Time: 0.672s, 1524.90/s  (0.696s, 1470.42/s)  LR: 7.570e-04  Data: 0.010 (0.013)
Train: 198 [ 700/1251 ( 56%)]  Loss: 3.591 (3.63)  Time: 0.672s, 1522.80/s  (0.696s, 1470.97/s)  LR: 7.570e-04  Data: 0.010 (0.013)
Train: 198 [ 750/1251 ( 60%)]  Loss: 3.924 (3.65)  Time: 0.699s, 1465.82/s  (0.696s, 1471.71/s)  LR: 7.570e-04  Data: 0.010 (0.013)
Train: 198 [ 800/1251 ( 64%)]  Loss: 3.911 (3.67)  Time: 0.714s, 1433.49/s  (0.696s, 1470.94/s)  LR: 7.570e-04  Data: 0.013 (0.013)
Train: 198 [ 850/1251 ( 68%)]  Loss: 4.166 (3.69)  Time: 0.677s, 1512.72/s  (0.697s, 1469.84/s)  LR: 7.570e-04  Data: 0.012 (0.012)
Train: 198 [ 900/1251 ( 72%)]  Loss: 4.097 (3.72)  Time: 0.673s, 1522.59/s  (0.697s, 1469.96/s)  LR: 7.570e-04  Data: 0.011 (0.012)
Train: 198 [ 950/1251 ( 76%)]  Loss: 4.243 (3.74)  Time: 0.704s, 1454.52/s  (0.696s, 1471.00/s)  LR: 7.570e-04  Data: 0.009 (0.012)
Train: 198 [1000/1251 ( 80%)]  Loss: 3.956 (3.75)  Time: 0.711s, 1440.53/s  (0.696s, 1471.15/s)  LR: 7.570e-04  Data: 0.010 (0.012)
Train: 198 [1050/1251 ( 84%)]  Loss: 4.116 (3.77)  Time: 0.677s, 1512.75/s  (0.696s, 1471.70/s)  LR: 7.570e-04  Data: 0.010 (0.012)
Train: 198 [1100/1251 ( 88%)]  Loss: 3.793 (3.77)  Time: 0.679s, 1508.29/s  (0.696s, 1472.08/s)  LR: 7.570e-04  Data: 0.010 (0.012)
Train: 198 [1150/1251 ( 92%)]  Loss: 3.800 (3.77)  Time: 0.741s, 1381.58/s  (0.696s, 1471.35/s)  LR: 7.570e-04  Data: 0.010 (0.012)
Train: 198 [1200/1251 ( 96%)]  Loss: 3.746 (3.77)  Time: 0.693s, 1477.69/s  (0.696s, 1471.44/s)  LR: 7.570e-04  Data: 0.012 (0.012)
Train: 198 [1250/1251 (100%)]  Loss: 3.823 (3.77)  Time: 0.672s, 1524.83/s  (0.696s, 1471.57/s)  LR: 7.570e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.471 (1.471)  Loss:  0.9053 (0.9053)  Acc@1: 87.4023 (87.4023)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.136 (0.573)  Loss:  1.0908 (1.5399)  Acc@1: 81.6038 (70.4960)  Acc@5: 93.8679 (90.0940)
Train: 199 [   0/1251 (  0%)]  Loss: 3.682 (3.68)  Time: 2.305s,  444.23/s  (2.305s,  444.23/s)  LR: 7.547e-04  Data: 1.641 (1.641)
Train: 199 [  50/1251 (  4%)]  Loss: 4.116 (3.90)  Time: 0.715s, 1431.65/s  (0.730s, 1402.82/s)  LR: 7.547e-04  Data: 0.009 (0.048)
Train: 199 [ 100/1251 (  8%)]  Loss: 3.895 (3.90)  Time: 0.673s, 1522.50/s  (0.710s, 1441.74/s)  LR: 7.547e-04  Data: 0.010 (0.029)
Train: 199 [ 150/1251 ( 12%)]  Loss: 3.934 (3.91)  Time: 0.710s, 1442.93/s  (0.702s, 1458.13/s)  LR: 7.547e-04  Data: 0.010 (0.023)
Train: 199 [ 200/1251 ( 16%)]  Loss: 3.556 (3.84)  Time: 0.679s, 1507.75/s  (0.699s, 1464.11/s)  LR: 7.547e-04  Data: 0.009 (0.020)
Train: 199 [ 250/1251 ( 20%)]  Loss: 4.036 (3.87)  Time: 0.745s, 1373.93/s  (0.700s, 1463.22/s)  LR: 7.547e-04  Data: 0.012 (0.018)
Train: 199 [ 300/1251 ( 24%)]  Loss: 3.452 (3.81)  Time: 0.763s, 1341.56/s  (0.702s, 1458.05/s)  LR: 7.547e-04  Data: 0.011 (0.017)
Train: 199 [ 350/1251 ( 28%)]  Loss: 3.872 (3.82)  Time: 0.738s, 1387.32/s  (0.704s, 1455.30/s)  LR: 7.547e-04  Data: 0.013 (0.016)
Train: 199 [ 400/1251 ( 32%)]  Loss: 3.883 (3.83)  Time: 0.679s, 1507.70/s  (0.704s, 1454.87/s)  LR: 7.547e-04  Data: 0.011 (0.016)
Train: 199 [ 450/1251 ( 36%)]  Loss: 3.402 (3.78)  Time: 0.756s, 1353.62/s  (0.702s, 1457.69/s)  LR: 7.547e-04  Data: 0.010 (0.015)
Train: 199 [ 500/1251 ( 40%)]  Loss: 4.075 (3.81)  Time: 0.701s, 1461.15/s  (0.701s, 1460.20/s)  LR: 7.547e-04  Data: 0.009 (0.015)
Train: 199 [ 550/1251 ( 44%)]  Loss: 3.642 (3.80)  Time: 0.684s, 1497.64/s  (0.700s, 1462.25/s)  LR: 7.547e-04  Data: 0.015 (0.014)
Train: 199 [ 600/1251 ( 48%)]  Loss: 3.826 (3.80)  Time: 0.672s, 1523.95/s  (0.699s, 1463.98/s)  LR: 7.547e-04  Data: 0.009 (0.014)
Train: 199 [ 650/1251 ( 52%)]  Loss: 3.700 (3.79)  Time: 0.701s, 1461.60/s  (0.699s, 1465.40/s)  LR: 7.547e-04  Data: 0.010 (0.014)
Train: 199 [ 700/1251 ( 56%)]  Loss: 3.926 (3.80)  Time: 0.678s, 1509.71/s  (0.699s, 1465.26/s)  LR: 7.547e-04  Data: 0.010 (0.013)
Train: 199 [ 750/1251 ( 60%)]  Loss: 3.777 (3.80)  Time: 0.678s, 1510.35/s  (0.699s, 1465.78/s)  LR: 7.547e-04  Data: 0.009 (0.013)
Train: 199 [ 800/1251 ( 64%)]  Loss: 3.866 (3.80)  Time: 0.704s, 1453.90/s  (0.698s, 1467.15/s)  LR: 7.547e-04  Data: 0.009 (0.013)
Train: 199 [ 850/1251 ( 68%)]  Loss: 3.789 (3.80)  Time: 0.705s, 1452.68/s  (0.697s, 1468.23/s)  LR: 7.547e-04  Data: 0.009 (0.013)
Train: 199 [ 900/1251 ( 72%)]  Loss: 3.746 (3.80)  Time: 0.701s, 1460.41/s  (0.697s, 1468.36/s)  LR: 7.547e-04  Data: 0.009 (0.013)
Train: 199 [ 950/1251 ( 76%)]  Loss: 3.578 (3.79)  Time: 0.672s, 1523.33/s  (0.697s, 1468.38/s)  LR: 7.547e-04  Data: 0.010 (0.013)
Train: 199 [1000/1251 ( 80%)]  Loss: 3.977 (3.80)  Time: 0.694s, 1475.58/s  (0.697s, 1468.36/s)  LR: 7.547e-04  Data: 0.009 (0.012)
Train: 199 [1050/1251 ( 84%)]  Loss: 3.937 (3.80)  Time: 0.762s, 1344.54/s  (0.697s, 1468.96/s)  LR: 7.547e-04  Data: 0.010 (0.012)
Train: 199 [1100/1251 ( 88%)]  Loss: 4.008 (3.81)  Time: 0.711s, 1440.84/s  (0.697s, 1469.28/s)  LR: 7.547e-04  Data: 0.010 (0.012)
Train: 199 [1150/1251 ( 92%)]  Loss: 3.751 (3.81)  Time: 0.669s, 1529.94/s  (0.697s, 1469.93/s)  LR: 7.547e-04  Data: 0.010 (0.012)
Train: 199 [1200/1251 ( 96%)]  Loss: 3.752 (3.81)  Time: 0.672s, 1523.76/s  (0.697s, 1469.76/s)  LR: 7.547e-04  Data: 0.013 (0.012)
Train: 199 [1250/1251 (100%)]  Loss: 3.876 (3.81)  Time: 0.785s, 1303.98/s  (0.697s, 1469.77/s)  LR: 7.547e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.538 (1.538)  Loss:  1.0049 (1.0049)  Acc@1: 86.7188 (86.7188)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.137 (0.589)  Loss:  0.9990 (1.5117)  Acc@1: 81.2500 (70.6260)  Acc@5: 95.1651 (90.2300)
Train: 200 [   0/1251 (  0%)]  Loss: 3.595 (3.60)  Time: 2.369s,  432.28/s  (2.369s,  432.28/s)  LR: 7.525e-04  Data: 1.737 (1.737)
Train: 200 [  50/1251 (  4%)]  Loss: 3.654 (3.62)  Time: 0.708s, 1446.17/s  (0.736s, 1392.00/s)  LR: 7.525e-04  Data: 0.010 (0.050)
Train: 200 [ 100/1251 (  8%)]  Loss: 3.799 (3.68)  Time: 0.708s, 1446.94/s  (0.714s, 1434.43/s)  LR: 7.525e-04  Data: 0.010 (0.030)
Train: 200 [ 150/1251 ( 12%)]  Loss: 3.849 (3.72)  Time: 0.672s, 1524.01/s  (0.707s, 1448.64/s)  LR: 7.525e-04  Data: 0.010 (0.024)
Train: 200 [ 200/1251 ( 16%)]  Loss: 3.372 (3.65)  Time: 0.736s, 1390.42/s  (0.704s, 1454.43/s)  LR: 7.525e-04  Data: 0.009 (0.020)
Train: 200 [ 250/1251 ( 20%)]  Loss: 4.073 (3.72)  Time: 0.706s, 1451.28/s  (0.702s, 1459.26/s)  LR: 7.525e-04  Data: 0.010 (0.018)
Train: 200 [ 300/1251 ( 24%)]  Loss: 3.485 (3.69)  Time: 0.671s, 1525.14/s  (0.701s, 1461.44/s)  LR: 7.525e-04  Data: 0.010 (0.017)
Train: 200 [ 350/1251 ( 28%)]  Loss: 3.659 (3.69)  Time: 0.675s, 1516.62/s  (0.699s, 1465.32/s)  LR: 7.525e-04  Data: 0.010 (0.016)
Train: 200 [ 400/1251 ( 32%)]  Loss: 3.913 (3.71)  Time: 0.684s, 1496.15/s  (0.698s, 1466.64/s)  LR: 7.525e-04  Data: 0.013 (0.015)
Train: 200 [ 450/1251 ( 36%)]  Loss: 3.425 (3.68)  Time: 0.668s, 1534.01/s  (0.697s, 1468.37/s)  LR: 7.525e-04  Data: 0.010 (0.015)
Train: 200 [ 500/1251 ( 40%)]  Loss: 3.686 (3.68)  Time: 0.699s, 1464.82/s  (0.697s, 1469.93/s)  LR: 7.525e-04  Data: 0.010 (0.014)
Train: 200 [ 550/1251 ( 44%)]  Loss: 3.673 (3.68)  Time: 0.702s, 1458.05/s  (0.698s, 1467.28/s)  LR: 7.525e-04  Data: 0.010 (0.014)
Train: 200 [ 600/1251 ( 48%)]  Loss: 3.943 (3.70)  Time: 0.678s, 1509.59/s  (0.697s, 1468.40/s)  LR: 7.525e-04  Data: 0.009 (0.014)
Train: 200 [ 650/1251 ( 52%)]  Loss: 3.965 (3.72)  Time: 0.674s, 1519.03/s  (0.697s, 1468.64/s)  LR: 7.525e-04  Data: 0.009 (0.013)
Train: 200 [ 700/1251 ( 56%)]  Loss: 3.834 (3.73)  Time: 0.672s, 1524.77/s  (0.697s, 1469.53/s)  LR: 7.525e-04  Data: 0.010 (0.013)
Train: 200 [ 750/1251 ( 60%)]  Loss: 3.982 (3.74)  Time: 0.679s, 1508.14/s  (0.696s, 1470.73/s)  LR: 7.525e-04  Data: 0.010 (0.013)
Train: 200 [ 800/1251 ( 64%)]  Loss: 4.173 (3.77)  Time: 0.689s, 1485.27/s  (0.696s, 1471.52/s)  LR: 7.525e-04  Data: 0.010 (0.013)
Train: 200 [ 850/1251 ( 68%)]  Loss: 3.663 (3.76)  Time: 0.672s, 1523.30/s  (0.696s, 1471.98/s)  LR: 7.525e-04  Data: 0.010 (0.013)
Train: 200 [ 900/1251 ( 72%)]  Loss: 3.370 (3.74)  Time: 0.672s, 1523.37/s  (0.696s, 1471.67/s)  LR: 7.525e-04  Data: 0.010 (0.012)
Train: 200 [ 950/1251 ( 76%)]  Loss: 3.587 (3.73)  Time: 0.673s, 1521.67/s  (0.696s, 1471.49/s)  LR: 7.525e-04  Data: 0.010 (0.012)
Train: 200 [1000/1251 ( 80%)]  Loss: 3.410 (3.72)  Time: 0.673s, 1520.73/s  (0.696s, 1472.13/s)  LR: 7.525e-04  Data: 0.010 (0.012)
Train: 200 [1050/1251 ( 84%)]  Loss: 3.873 (3.73)  Time: 0.711s, 1439.60/s  (0.696s, 1472.06/s)  LR: 7.525e-04  Data: 0.012 (0.012)
Train: 200 [1100/1251 ( 88%)]  Loss: 3.960 (3.74)  Time: 0.681s, 1504.29/s  (0.695s, 1472.69/s)  LR: 7.525e-04  Data: 0.010 (0.012)
Train: 200 [1150/1251 ( 92%)]  Loss: 3.794 (3.74)  Time: 0.710s, 1441.66/s  (0.696s, 1472.22/s)  LR: 7.525e-04  Data: 0.010 (0.012)
Train: 200 [1200/1251 ( 96%)]  Loss: 3.922 (3.75)  Time: 0.678s, 1510.54/s  (0.695s, 1472.58/s)  LR: 7.525e-04  Data: 0.009 (0.012)
Train: 200 [1250/1251 (100%)]  Loss: 3.805 (3.75)  Time: 0.656s, 1561.53/s  (0.695s, 1473.06/s)  LR: 7.525e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.514 (1.514)  Loss:  0.9170 (0.9170)  Acc@1: 87.8906 (87.8906)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.137 (0.595)  Loss:  0.9521 (1.4492)  Acc@1: 82.4292 (71.5580)  Acc@5: 96.1085 (90.5720)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-187.pth.tar', 71.87000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-200.pth.tar', 71.55799999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-197.pth.tar', 71.51000004638672)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-192.pth.tar', 71.35000012451172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-190.pth.tar', 71.32600001464844)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-180.pth.tar', 71.26400010009766)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-179.pth.tar', 71.23600001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-194.pth.tar', 71.22599999755859)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-196.pth.tar', 71.2199999658203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-195.pth.tar', 71.11800001220703)

Train: 201 [   0/1251 (  0%)]  Loss: 3.798 (3.80)  Time: 2.366s,  432.86/s  (2.366s,  432.86/s)  LR: 7.503e-04  Data: 1.717 (1.717)
Train: 201 [  50/1251 (  4%)]  Loss: 3.912 (3.86)  Time: 0.675s, 1517.27/s  (0.744s, 1375.51/s)  LR: 7.503e-04  Data: 0.010 (0.057)
Train: 201 [ 100/1251 (  8%)]  Loss: 3.261 (3.66)  Time: 0.669s, 1530.61/s  (0.718s, 1425.29/s)  LR: 7.503e-04  Data: 0.010 (0.034)
Train: 201 [ 150/1251 ( 12%)]  Loss: 3.816 (3.70)  Time: 0.673s, 1521.93/s  (0.712s, 1438.07/s)  LR: 7.503e-04  Data: 0.011 (0.026)
Train: 201 [ 200/1251 ( 16%)]  Loss: 3.866 (3.73)  Time: 0.676s, 1515.80/s  (0.708s, 1445.32/s)  LR: 7.503e-04  Data: 0.011 (0.022)
Train: 201 [ 250/1251 ( 20%)]  Loss: 3.929 (3.76)  Time: 0.675s, 1518.05/s  (0.706s, 1449.67/s)  LR: 7.503e-04  Data: 0.011 (0.020)
Train: 201 [ 300/1251 ( 24%)]  Loss: 3.676 (3.75)  Time: 0.670s, 1527.53/s  (0.703s, 1455.89/s)  LR: 7.503e-04  Data: 0.010 (0.018)
Train: 201 [ 350/1251 ( 28%)]  Loss: 3.709 (3.75)  Time: 0.677s, 1511.99/s  (0.703s, 1456.95/s)  LR: 7.503e-04  Data: 0.012 (0.017)
Train: 201 [ 400/1251 ( 32%)]  Loss: 4.258 (3.80)  Time: 0.676s, 1514.47/s  (0.702s, 1459.08/s)  LR: 7.503e-04  Data: 0.009 (0.016)
Train: 201 [ 450/1251 ( 36%)]  Loss: 3.859 (3.81)  Time: 0.672s, 1523.40/s  (0.700s, 1461.92/s)  LR: 7.503e-04  Data: 0.013 (0.015)
Train: 201 [ 500/1251 ( 40%)]  Loss: 3.900 (3.82)  Time: 0.706s, 1451.05/s  (0.699s, 1464.38/s)  LR: 7.503e-04  Data: 0.010 (0.015)
Train: 201 [ 550/1251 ( 44%)]  Loss: 3.911 (3.82)  Time: 0.672s, 1523.99/s  (0.699s, 1465.23/s)  LR: 7.503e-04  Data: 0.009 (0.014)
Train: 201 [ 600/1251 ( 48%)]  Loss: 3.890 (3.83)  Time: 0.740s, 1383.89/s  (0.699s, 1465.33/s)  LR: 7.503e-04  Data: 0.009 (0.014)
Train: 201 [ 650/1251 ( 52%)]  Loss: 4.188 (3.86)  Time: 0.674s, 1520.38/s  (0.698s, 1467.08/s)  LR: 7.503e-04  Data: 0.011 (0.014)
Train: 201 [ 700/1251 ( 56%)]  Loss: 3.414 (3.83)  Time: 0.684s, 1497.66/s  (0.698s, 1467.51/s)  LR: 7.503e-04  Data: 0.011 (0.013)
Train: 201 [ 750/1251 ( 60%)]  Loss: 3.798 (3.82)  Time: 0.671s, 1526.02/s  (0.697s, 1468.68/s)  LR: 7.503e-04  Data: 0.010 (0.013)
Train: 201 [ 800/1251 ( 64%)]  Loss: 3.616 (3.81)  Time: 0.675s, 1516.10/s  (0.697s, 1469.27/s)  LR: 7.503e-04  Data: 0.009 (0.013)
Train: 201 [ 850/1251 ( 68%)]  Loss: 3.619 (3.80)  Time: 0.669s, 1531.19/s  (0.697s, 1469.85/s)  LR: 7.503e-04  Data: 0.009 (0.013)
Train: 201 [ 900/1251 ( 72%)]  Loss: 4.191 (3.82)  Time: 0.676s, 1514.89/s  (0.696s, 1470.48/s)  LR: 7.503e-04  Data: 0.010 (0.013)
Train: 201 [ 950/1251 ( 76%)]  Loss: 3.836 (3.82)  Time: 0.699s, 1465.72/s  (0.696s, 1470.31/s)  LR: 7.503e-04  Data: 0.015 (0.013)
Train: 201 [1000/1251 ( 80%)]  Loss: 3.483 (3.81)  Time: 0.673s, 1521.13/s  (0.696s, 1470.77/s)  LR: 7.503e-04  Data: 0.010 (0.013)
Train: 201 [1050/1251 ( 84%)]  Loss: 3.506 (3.79)  Time: 0.718s, 1426.06/s  (0.696s, 1471.30/s)  LR: 7.503e-04  Data: 0.009 (0.012)
Train: 201 [1100/1251 ( 88%)]  Loss: 3.642 (3.79)  Time: 0.705s, 1453.49/s  (0.696s, 1471.98/s)  LR: 7.503e-04  Data: 0.009 (0.012)
Train: 201 [1150/1251 ( 92%)]  Loss: 3.562 (3.78)  Time: 0.711s, 1439.93/s  (0.696s, 1472.20/s)  LR: 7.503e-04  Data: 0.012 (0.012)
Train: 201 [1200/1251 ( 96%)]  Loss: 3.919 (3.78)  Time: 0.701s, 1461.66/s  (0.696s, 1472.11/s)  LR: 7.503e-04  Data: 0.009 (0.012)
Train: 201 [1250/1251 (100%)]  Loss: 3.707 (3.78)  Time: 0.656s, 1559.98/s  (0.695s, 1472.68/s)  LR: 7.503e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.609 (1.609)  Loss:  0.9160 (0.9160)  Acc@1: 86.8164 (86.8164)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.136 (0.594)  Loss:  1.0137 (1.5894)  Acc@1: 83.7264 (71.4920)  Acc@5: 95.5189 (90.4620)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-187.pth.tar', 71.87000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-200.pth.tar', 71.55799999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-197.pth.tar', 71.51000004638672)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-201.pth.tar', 71.4920000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-192.pth.tar', 71.35000012451172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-190.pth.tar', 71.32600001464844)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-180.pth.tar', 71.26400010009766)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-179.pth.tar', 71.23600001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-194.pth.tar', 71.22599999755859)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-196.pth.tar', 71.2199999658203)

Train: 202 [   0/1251 (  0%)]  Loss: 4.007 (4.01)  Time: 2.171s,  471.59/s  (2.171s,  471.59/s)  LR: 7.480e-04  Data: 1.545 (1.545)
Train: 202 [  50/1251 (  4%)]  Loss: 3.627 (3.82)  Time: 0.676s, 1515.31/s  (0.733s, 1396.80/s)  LR: 7.480e-04  Data: 0.012 (0.052)
Train: 202 [ 100/1251 (  8%)]  Loss: 4.069 (3.90)  Time: 0.695s, 1473.51/s  (0.717s, 1428.75/s)  LR: 7.480e-04  Data: 0.009 (0.031)
Train: 202 [ 150/1251 ( 12%)]  Loss: 3.728 (3.86)  Time: 0.673s, 1521.51/s  (0.709s, 1443.96/s)  LR: 7.480e-04  Data: 0.011 (0.024)
Train: 202 [ 200/1251 ( 16%)]  Loss: 3.896 (3.87)  Time: 0.681s, 1504.77/s  (0.707s, 1449.04/s)  LR: 7.480e-04  Data: 0.017 (0.021)
Train: 202 [ 250/1251 ( 20%)]  Loss: 3.681 (3.83)  Time: 0.710s, 1442.07/s  (0.704s, 1453.93/s)  LR: 7.480e-04  Data: 0.010 (0.019)
Train: 202 [ 300/1251 ( 24%)]  Loss: 3.667 (3.81)  Time: 0.673s, 1521.45/s  (0.702s, 1457.90/s)  LR: 7.480e-04  Data: 0.010 (0.017)
Train: 202 [ 350/1251 ( 28%)]  Loss: 3.677 (3.79)  Time: 0.707s, 1449.27/s  (0.701s, 1460.99/s)  LR: 7.480e-04  Data: 0.009 (0.016)
Train: 202 [ 400/1251 ( 32%)]  Loss: 3.802 (3.79)  Time: 0.675s, 1516.72/s  (0.699s, 1464.45/s)  LR: 7.480e-04  Data: 0.010 (0.016)
Train: 202 [ 450/1251 ( 36%)]  Loss: 3.407 (3.76)  Time: 0.699s, 1465.91/s  (0.698s, 1466.35/s)  LR: 7.480e-04  Data: 0.010 (0.015)
Train: 202 [ 500/1251 ( 40%)]  Loss: 3.906 (3.77)  Time: 0.679s, 1508.04/s  (0.697s, 1468.48/s)  LR: 7.480e-04  Data: 0.011 (0.014)
Train: 202 [ 550/1251 ( 44%)]  Loss: 3.608 (3.76)  Time: 0.672s, 1524.79/s  (0.698s, 1467.64/s)  LR: 7.480e-04  Data: 0.010 (0.014)
Train: 202 [ 600/1251 ( 48%)]  Loss: 3.724 (3.75)  Time: 0.692s, 1480.46/s  (0.697s, 1468.99/s)  LR: 7.480e-04  Data: 0.009 (0.014)
Train: 202 [ 650/1251 ( 52%)]  Loss: 3.649 (3.75)  Time: 0.693s, 1478.58/s  (0.697s, 1469.23/s)  LR: 7.480e-04  Data: 0.010 (0.014)
Train: 202 [ 700/1251 ( 56%)]  Loss: 3.656 (3.74)  Time: 0.674s, 1518.72/s  (0.697s, 1469.80/s)  LR: 7.480e-04  Data: 0.011 (0.013)
Train: 202 [ 750/1251 ( 60%)]  Loss: 4.050 (3.76)  Time: 0.673s, 1520.86/s  (0.696s, 1470.98/s)  LR: 7.480e-04  Data: 0.010 (0.013)
Train: 202 [ 800/1251 ( 64%)]  Loss: 4.016 (3.77)  Time: 0.701s, 1461.07/s  (0.696s, 1472.01/s)  LR: 7.480e-04  Data: 0.010 (0.013)
Train: 202 [ 850/1251 ( 68%)]  Loss: 3.836 (3.78)  Time: 0.673s, 1522.32/s  (0.696s, 1472.21/s)  LR: 7.480e-04  Data: 0.010 (0.013)
Train: 202 [ 900/1251 ( 72%)]  Loss: 3.823 (3.78)  Time: 0.704s, 1453.63/s  (0.696s, 1471.45/s)  LR: 7.480e-04  Data: 0.009 (0.013)
Train: 202 [ 950/1251 ( 76%)]  Loss: 3.489 (3.77)  Time: 0.675s, 1517.07/s  (0.695s, 1472.40/s)  LR: 7.480e-04  Data: 0.010 (0.013)
Train: 202 [1000/1251 ( 80%)]  Loss: 3.859 (3.77)  Time: 0.672s, 1524.19/s  (0.695s, 1472.54/s)  LR: 7.480e-04  Data: 0.010 (0.012)
Train: 202 [1050/1251 ( 84%)]  Loss: 3.610 (3.76)  Time: 0.724s, 1414.65/s  (0.695s, 1472.43/s)  LR: 7.480e-04  Data: 0.009 (0.012)
Train: 202 [1100/1251 ( 88%)]  Loss: 3.754 (3.76)  Time: 0.671s, 1525.29/s  (0.695s, 1472.61/s)  LR: 7.480e-04  Data: 0.011 (0.012)
Train: 202 [1150/1251 ( 92%)]  Loss: 3.365 (3.75)  Time: 0.686s, 1493.45/s  (0.695s, 1473.13/s)  LR: 7.480e-04  Data: 0.009 (0.012)
Train: 202 [1200/1251 ( 96%)]  Loss: 3.879 (3.75)  Time: 0.680s, 1506.46/s  (0.695s, 1473.61/s)  LR: 7.480e-04  Data: 0.011 (0.012)
Train: 202 [1250/1251 (100%)]  Loss: 3.593 (3.75)  Time: 0.659s, 1554.67/s  (0.695s, 1474.11/s)  LR: 7.480e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.604 (1.604)  Loss:  1.1719 (1.1719)  Acc@1: 87.6953 (87.6953)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.136 (0.587)  Loss:  1.2871 (1.7006)  Acc@1: 82.3113 (71.6240)  Acc@5: 95.7547 (90.5140)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-187.pth.tar', 71.87000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-202.pth.tar', 71.62399994384765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-200.pth.tar', 71.55799999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-197.pth.tar', 71.51000004638672)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-201.pth.tar', 71.4920000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-192.pth.tar', 71.35000012451172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-190.pth.tar', 71.32600001464844)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-180.pth.tar', 71.26400010009766)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-179.pth.tar', 71.23600001220703)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-194.pth.tar', 71.22599999755859)

Train: 203 [   0/1251 (  0%)]  Loss: 3.534 (3.53)  Time: 2.575s,  397.71/s  (2.575s,  397.71/s)  LR: 7.457e-04  Data: 1.942 (1.942)
Train: 203 [  50/1251 (  4%)]  Loss: 3.946 (3.74)  Time: 0.671s, 1526.91/s  (0.738s, 1386.82/s)  LR: 7.457e-04  Data: 0.009 (0.054)
Train: 203 [ 100/1251 (  8%)]  Loss: 3.319 (3.60)  Time: 0.671s, 1525.73/s  (0.714s, 1434.11/s)  LR: 7.457e-04  Data: 0.010 (0.032)
Train: 203 [ 150/1251 ( 12%)]  Loss: 3.423 (3.56)  Time: 0.695s, 1474.30/s  (0.708s, 1446.93/s)  LR: 7.457e-04  Data: 0.012 (0.025)
Train: 203 [ 200/1251 ( 16%)]  Loss: 3.727 (3.59)  Time: 0.711s, 1440.96/s  (0.704s, 1453.63/s)  LR: 7.457e-04  Data: 0.010 (0.022)
Train: 203 [ 250/1251 ( 20%)]  Loss: 3.579 (3.59)  Time: 0.696s, 1472.17/s  (0.702s, 1459.32/s)  LR: 7.457e-04  Data: 0.010 (0.019)
Train: 203 [ 300/1251 ( 24%)]  Loss: 3.518 (3.58)  Time: 0.670s, 1529.32/s  (0.699s, 1464.39/s)  LR: 7.457e-04  Data: 0.010 (0.018)
Train: 203 [ 350/1251 ( 28%)]  Loss: 3.812 (3.61)  Time: 0.668s, 1533.06/s  (0.698s, 1466.95/s)  LR: 7.457e-04  Data: 0.009 (0.017)
Train: 203 [ 400/1251 ( 32%)]  Loss: 4.071 (3.66)  Time: 0.702s, 1457.76/s  (0.697s, 1469.15/s)  LR: 7.457e-04  Data: 0.010 (0.016)
Train: 203 [ 450/1251 ( 36%)]  Loss: 3.839 (3.68)  Time: 0.671s, 1526.25/s  (0.696s, 1471.84/s)  LR: 7.457e-04  Data: 0.010 (0.015)
Train: 203 [ 500/1251 ( 40%)]  Loss: 3.634 (3.67)  Time: 0.676s, 1515.64/s  (0.696s, 1471.29/s)  LR: 7.457e-04  Data: 0.010 (0.015)
Train: 203 [ 550/1251 ( 44%)]  Loss: 3.731 (3.68)  Time: 0.705s, 1453.15/s  (0.696s, 1472.06/s)  LR: 7.457e-04  Data: 0.009 (0.014)
Train: 203 [ 600/1251 ( 48%)]  Loss: 3.635 (3.67)  Time: 0.705s, 1451.84/s  (0.695s, 1472.42/s)  LR: 7.457e-04  Data: 0.011 (0.014)
Train: 203 [ 650/1251 ( 52%)]  Loss: 3.528 (3.66)  Time: 0.673s, 1520.61/s  (0.695s, 1473.41/s)  LR: 7.457e-04  Data: 0.010 (0.014)
Train: 203 [ 700/1251 ( 56%)]  Loss: 3.712 (3.67)  Time: 0.672s, 1523.65/s  (0.694s, 1474.48/s)  LR: 7.457e-04  Data: 0.009 (0.013)
Train: 203 [ 750/1251 ( 60%)]  Loss: 4.077 (3.69)  Time: 0.671s, 1525.90/s  (0.695s, 1474.08/s)  LR: 7.457e-04  Data: 0.010 (0.013)
Train: 203 [ 800/1251 ( 64%)]  Loss: 3.690 (3.69)  Time: 0.706s, 1450.18/s  (0.695s, 1474.44/s)  LR: 7.457e-04  Data: 0.009 (0.013)
Train: 203 [ 850/1251 ( 68%)]  Loss: 3.675 (3.69)  Time: 0.685s, 1495.76/s  (0.694s, 1474.93/s)  LR: 7.457e-04  Data: 0.009 (0.013)
Train: 203 [ 900/1251 ( 72%)]  Loss: 3.431 (3.68)  Time: 0.678s, 1510.48/s  (0.694s, 1475.06/s)  LR: 7.457e-04  Data: 0.011 (0.013)
Train: 203 [ 950/1251 ( 76%)]  Loss: 4.228 (3.71)  Time: 0.676s, 1514.57/s  (0.695s, 1474.08/s)  LR: 7.457e-04  Data: 0.010 (0.013)
Train: 203 [1000/1251 ( 80%)]  Loss: 3.617 (3.70)  Time: 0.671s, 1526.75/s  (0.694s, 1474.47/s)  LR: 7.457e-04  Data: 0.010 (0.012)
Train: 203 [1050/1251 ( 84%)]  Loss: 3.765 (3.70)  Time: 0.757s, 1352.00/s  (0.695s, 1474.24/s)  LR: 7.457e-04  Data: 0.012 (0.012)
Train: 203 [1100/1251 ( 88%)]  Loss: 3.916 (3.71)  Time: 0.673s, 1521.55/s  (0.694s, 1474.79/s)  LR: 7.457e-04  Data: 0.010 (0.012)
Train: 203 [1150/1251 ( 92%)]  Loss: 3.528 (3.71)  Time: 0.672s, 1523.46/s  (0.694s, 1475.27/s)  LR: 7.457e-04  Data: 0.009 (0.012)
Train: 203 [1200/1251 ( 96%)]  Loss: 4.151 (3.72)  Time: 0.685s, 1494.51/s  (0.694s, 1475.10/s)  LR: 7.457e-04  Data: 0.010 (0.012)
Train: 203 [1250/1251 (100%)]  Loss: 4.007 (3.73)  Time: 0.691s, 1481.86/s  (0.694s, 1475.80/s)  LR: 7.457e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.561 (1.561)  Loss:  0.8833 (0.8833)  Acc@1: 88.0859 (88.0859)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.137 (0.587)  Loss:  1.0127 (1.5092)  Acc@1: 83.8443 (71.4920)  Acc@5: 95.2830 (90.5420)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-187.pth.tar', 71.87000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-202.pth.tar', 71.62399994384765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-200.pth.tar', 71.55799999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-197.pth.tar', 71.51000004638672)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-201.pth.tar', 71.4920000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-203.pth.tar', 71.4919999633789)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-192.pth.tar', 71.35000012451172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-190.pth.tar', 71.32600001464844)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-180.pth.tar', 71.26400010009766)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-179.pth.tar', 71.23600001220703)

Train: 204 [   0/1251 (  0%)]  Loss: 3.821 (3.82)  Time: 2.340s,  437.69/s  (2.340s,  437.69/s)  LR: 7.435e-04  Data: 1.672 (1.672)
Train: 204 [  50/1251 (  4%)]  Loss: 3.601 (3.71)  Time: 0.675s, 1517.85/s  (0.738s, 1387.37/s)  LR: 7.435e-04  Data: 0.011 (0.054)
Train: 204 [ 100/1251 (  8%)]  Loss: 3.857 (3.76)  Time: 0.671s, 1525.54/s  (0.718s, 1425.64/s)  LR: 7.435e-04  Data: 0.009 (0.032)
Train: 204 [ 150/1251 ( 12%)]  Loss: 3.910 (3.80)  Time: 0.672s, 1523.27/s  (0.711s, 1440.01/s)  LR: 7.435e-04  Data: 0.013 (0.025)
Train: 204 [ 200/1251 ( 16%)]  Loss: 3.467 (3.73)  Time: 0.717s, 1428.90/s  (0.709s, 1444.29/s)  LR: 7.435e-04  Data: 0.011 (0.021)
Train: 204 [ 250/1251 ( 20%)]  Loss: 3.647 (3.72)  Time: 0.693s, 1477.19/s  (0.706s, 1450.23/s)  LR: 7.435e-04  Data: 0.009 (0.019)
Train: 204 [ 300/1251 ( 24%)]  Loss: 3.974 (3.75)  Time: 0.693s, 1477.34/s  (0.705s, 1452.36/s)  LR: 7.435e-04  Data: 0.010 (0.018)
Train: 204 [ 350/1251 ( 28%)]  Loss: 3.831 (3.76)  Time: 0.668s, 1532.35/s  (0.703s, 1456.63/s)  LR: 7.435e-04  Data: 0.011 (0.017)
Train: 204 [ 400/1251 ( 32%)]  Loss: 4.132 (3.80)  Time: 0.705s, 1452.34/s  (0.702s, 1458.65/s)  LR: 7.435e-04  Data: 0.014 (0.016)
Train: 204 [ 450/1251 ( 36%)]  Loss: 3.799 (3.80)  Time: 0.671s, 1525.31/s  (0.701s, 1460.89/s)  LR: 7.435e-04  Data: 0.010 (0.015)
Train: 204 [ 500/1251 ( 40%)]  Loss: 3.466 (3.77)  Time: 0.704s, 1455.45/s  (0.700s, 1462.90/s)  LR: 7.435e-04  Data: 0.009 (0.015)
Train: 204 [ 550/1251 ( 44%)]  Loss: 3.554 (3.76)  Time: 0.674s, 1520.23/s  (0.699s, 1464.45/s)  LR: 7.435e-04  Data: 0.011 (0.014)
Train: 204 [ 600/1251 ( 48%)]  Loss: 3.682 (3.75)  Time: 0.692s, 1480.53/s  (0.699s, 1465.50/s)  LR: 7.435e-04  Data: 0.011 (0.014)
Train: 204 [ 650/1251 ( 52%)]  Loss: 4.056 (3.77)  Time: 0.707s, 1448.87/s  (0.698s, 1466.27/s)  LR: 7.435e-04  Data: 0.010 (0.014)
Train: 204 [ 700/1251 ( 56%)]  Loss: 3.526 (3.76)  Time: 0.705s, 1452.36/s  (0.698s, 1467.00/s)  LR: 7.435e-04  Data: 0.010 (0.013)
Train: 204 [ 750/1251 ( 60%)]  Loss: 3.879 (3.76)  Time: 0.699s, 1465.08/s  (0.698s, 1467.95/s)  LR: 7.435e-04  Data: 0.011 (0.013)
Train: 204 [ 800/1251 ( 64%)]  Loss: 3.805 (3.77)  Time: 0.673s, 1521.21/s  (0.697s, 1468.60/s)  LR: 7.435e-04  Data: 0.010 (0.013)
Train: 204 [ 850/1251 ( 68%)]  Loss: 4.013 (3.78)  Time: 0.708s, 1446.71/s  (0.697s, 1469.16/s)  LR: 7.435e-04  Data: 0.009 (0.013)
Train: 204 [ 900/1251 ( 72%)]  Loss: 4.010 (3.79)  Time: 0.676s, 1514.56/s  (0.697s, 1469.19/s)  LR: 7.435e-04  Data: 0.010 (0.013)
Train: 204 [ 950/1251 ( 76%)]  Loss: 3.992 (3.80)  Time: 0.706s, 1449.64/s  (0.697s, 1469.71/s)  LR: 7.435e-04  Data: 0.010 (0.013)
Train: 204 [1000/1251 ( 80%)]  Loss: 3.861 (3.80)  Time: 0.718s, 1426.71/s  (0.697s, 1468.82/s)  LR: 7.435e-04  Data: 0.011 (0.012)
Train: 204 [1050/1251 ( 84%)]  Loss: 3.732 (3.80)  Time: 0.693s, 1478.27/s  (0.697s, 1469.08/s)  LR: 7.435e-04  Data: 0.010 (0.012)
Train: 204 [1100/1251 ( 88%)]  Loss: 3.491 (3.79)  Time: 0.707s, 1448.88/s  (0.697s, 1469.33/s)  LR: 7.435e-04  Data: 0.010 (0.012)
Train: 204 [1150/1251 ( 92%)]  Loss: 3.747 (3.79)  Time: 0.712s, 1437.72/s  (0.697s, 1469.29/s)  LR: 7.435e-04  Data: 0.011 (0.012)
Train: 204 [1200/1251 ( 96%)]  Loss: 3.876 (3.79)  Time: 0.719s, 1424.35/s  (0.697s, 1469.87/s)  LR: 7.435e-04  Data: 0.011 (0.012)
Train: 204 [1250/1251 (100%)]  Loss: 3.863 (3.79)  Time: 0.693s, 1478.29/s  (0.697s, 1469.88/s)  LR: 7.435e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.526 (1.526)  Loss:  0.8965 (0.8965)  Acc@1: 86.1328 (86.1328)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.136 (0.588)  Loss:  1.0010 (1.4744)  Acc@1: 81.9575 (71.1240)  Acc@5: 95.4009 (90.4380)
Train: 205 [   0/1251 (  0%)]  Loss: 3.941 (3.94)  Time: 2.195s,  466.59/s  (2.195s,  466.59/s)  LR: 7.412e-04  Data: 1.575 (1.575)
Train: 205 [  50/1251 (  4%)]  Loss: 3.975 (3.96)  Time: 0.704s, 1453.90/s  (0.722s, 1418.37/s)  LR: 7.412e-04  Data: 0.009 (0.047)
Train: 205 [ 100/1251 (  8%)]  Loss: 3.666 (3.86)  Time: 0.724s, 1414.07/s  (0.711s, 1440.43/s)  LR: 7.412e-04  Data: 0.010 (0.029)
Train: 205 [ 150/1251 ( 12%)]  Loss: 3.689 (3.82)  Time: 0.701s, 1461.12/s  (0.704s, 1454.25/s)  LR: 7.412e-04  Data: 0.009 (0.023)
Train: 205 [ 200/1251 ( 16%)]  Loss: 3.596 (3.77)  Time: 0.673s, 1522.55/s  (0.701s, 1461.12/s)  LR: 7.412e-04  Data: 0.010 (0.020)
Train: 205 [ 250/1251 ( 20%)]  Loss: 3.907 (3.80)  Time: 0.713s, 1436.88/s  (0.700s, 1462.66/s)  LR: 7.412e-04  Data: 0.010 (0.018)
Train: 205 [ 300/1251 ( 24%)]  Loss: 3.482 (3.75)  Time: 0.686s, 1491.99/s  (0.701s, 1460.08/s)  LR: 7.412e-04  Data: 0.012 (0.016)
Train: 205 [ 350/1251 ( 28%)]  Loss: 3.731 (3.75)  Time: 0.733s, 1396.58/s  (0.701s, 1460.45/s)  LR: 7.412e-04  Data: 0.012 (0.016)
Train: 205 [ 400/1251 ( 32%)]  Loss: 3.939 (3.77)  Time: 0.670s, 1529.19/s  (0.700s, 1463.61/s)  LR: 7.412e-04  Data: 0.009 (0.015)
Train: 205 [ 450/1251 ( 36%)]  Loss: 3.454 (3.74)  Time: 0.676s, 1515.40/s  (0.699s, 1465.27/s)  LR: 7.412e-04  Data: 0.010 (0.014)
Train: 205 [ 500/1251 ( 40%)]  Loss: 3.866 (3.75)  Time: 0.671s, 1525.33/s  (0.698s, 1467.63/s)  LR: 7.412e-04  Data: 0.011 (0.014)
Train: 205 [ 550/1251 ( 44%)]  Loss: 4.115 (3.78)  Time: 0.719s, 1424.34/s  (0.697s, 1469.03/s)  LR: 7.412e-04  Data: 0.009 (0.014)
Train: 205 [ 600/1251 ( 48%)]  Loss: 3.621 (3.77)  Time: 0.802s, 1276.08/s  (0.697s, 1470.14/s)  LR: 7.412e-04  Data: 0.009 (0.013)
Train: 205 [ 650/1251 ( 52%)]  Loss: 3.765 (3.77)  Time: 0.693s, 1476.99/s  (0.696s, 1471.16/s)  LR: 7.412e-04  Data: 0.012 (0.013)
Train: 205 [ 700/1251 ( 56%)]  Loss: 3.555 (3.75)  Time: 0.713s, 1436.38/s  (0.696s, 1471.79/s)  LR: 7.412e-04  Data: 0.009 (0.013)
Train: 205 [ 750/1251 ( 60%)]  Loss: 3.602 (3.74)  Time: 0.672s, 1524.69/s  (0.695s, 1472.69/s)  LR: 7.412e-04  Data: 0.010 (0.013)
Train: 205 [ 800/1251 ( 64%)]  Loss: 3.611 (3.74)  Time: 0.673s, 1520.45/s  (0.695s, 1473.62/s)  LR: 7.412e-04  Data: 0.010 (0.013)
Train: 205 [ 850/1251 ( 68%)]  Loss: 3.660 (3.73)  Time: 0.702s, 1458.70/s  (0.695s, 1474.01/s)  LR: 7.412e-04  Data: 0.009 (0.012)
Train: 205 [ 900/1251 ( 72%)]  Loss: 3.695 (3.73)  Time: 0.699s, 1465.12/s  (0.695s, 1474.42/s)  LR: 7.412e-04  Data: 0.010 (0.012)
Train: 205 [ 950/1251 ( 76%)]  Loss: 4.031 (3.75)  Time: 0.669s, 1530.77/s  (0.695s, 1473.52/s)  LR: 7.412e-04  Data: 0.010 (0.012)
Train: 205 [1000/1251 ( 80%)]  Loss: 4.063 (3.76)  Time: 0.701s, 1461.67/s  (0.695s, 1473.56/s)  LR: 7.412e-04  Data: 0.009 (0.012)
Train: 205 [1050/1251 ( 84%)]  Loss: 4.036 (3.77)  Time: 0.678s, 1510.74/s  (0.695s, 1474.10/s)  LR: 7.412e-04  Data: 0.010 (0.012)
Train: 205 [1100/1251 ( 88%)]  Loss: 3.778 (3.77)  Time: 0.673s, 1521.48/s  (0.694s, 1474.46/s)  LR: 7.412e-04  Data: 0.010 (0.012)
Train: 205 [1150/1251 ( 92%)]  Loss: 3.480 (3.76)  Time: 0.671s, 1526.34/s  (0.694s, 1475.20/s)  LR: 7.412e-04  Data: 0.010 (0.012)
Train: 205 [1200/1251 ( 96%)]  Loss: 3.903 (3.77)  Time: 0.717s, 1428.99/s  (0.694s, 1475.37/s)  LR: 7.412e-04  Data: 0.009 (0.012)
Train: 205 [1250/1251 (100%)]  Loss: 3.880 (3.77)  Time: 0.661s, 1549.46/s  (0.694s, 1475.76/s)  LR: 7.412e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.469 (1.469)  Loss:  1.1104 (1.1104)  Acc@1: 88.2812 (88.2812)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.136 (0.578)  Loss:  1.2578 (1.6945)  Acc@1: 81.7217 (70.8240)  Acc@5: 95.5189 (90.2580)
Train: 206 [   0/1251 (  0%)]  Loss: 3.699 (3.70)  Time: 2.220s,  461.28/s  (2.220s,  461.28/s)  LR: 7.389e-04  Data: 1.603 (1.603)
Train: 206 [  50/1251 (  4%)]  Loss: 3.931 (3.81)  Time: 0.674s, 1519.62/s  (0.725s, 1411.95/s)  LR: 7.389e-04  Data: 0.011 (0.048)
Train: 206 [ 100/1251 (  8%)]  Loss: 4.008 (3.88)  Time: 0.672s, 1523.15/s  (0.708s, 1446.64/s)  LR: 7.389e-04  Data: 0.010 (0.029)
Train: 206 [ 150/1251 ( 12%)]  Loss: 4.034 (3.92)  Time: 0.709s, 1445.05/s  (0.702s, 1457.78/s)  LR: 7.389e-04  Data: 0.010 (0.023)
Train: 206 [ 200/1251 ( 16%)]  Loss: 3.497 (3.83)  Time: 0.672s, 1524.87/s  (0.700s, 1463.53/s)  LR: 7.389e-04  Data: 0.010 (0.020)
Train: 206 [ 250/1251 ( 20%)]  Loss: 3.674 (3.81)  Time: 0.707s, 1447.76/s  (0.698s, 1466.79/s)  LR: 7.389e-04  Data: 0.009 (0.018)
Train: 206 [ 300/1251 ( 24%)]  Loss: 3.713 (3.79)  Time: 0.706s, 1450.29/s  (0.697s, 1468.95/s)  LR: 7.389e-04  Data: 0.009 (0.017)
Train: 206 [ 350/1251 ( 28%)]  Loss: 4.271 (3.85)  Time: 0.707s, 1448.88/s  (0.695s, 1472.83/s)  LR: 7.389e-04  Data: 0.014 (0.016)
Train: 206 [ 400/1251 ( 32%)]  Loss: 3.977 (3.87)  Time: 0.674s, 1519.91/s  (0.695s, 1473.48/s)  LR: 7.389e-04  Data: 0.010 (0.015)
Train: 206 [ 450/1251 ( 36%)]  Loss: 3.514 (3.83)  Time: 0.711s, 1439.63/s  (0.696s, 1471.96/s)  LR: 7.389e-04  Data: 0.009 (0.015)
Train: 206 [ 500/1251 ( 40%)]  Loss: 3.444 (3.80)  Time: 0.706s, 1450.19/s  (0.696s, 1470.55/s)  LR: 7.389e-04  Data: 0.009 (0.014)
Train: 206 [ 550/1251 ( 44%)]  Loss: 4.066 (3.82)  Time: 0.673s, 1521.94/s  (0.696s, 1471.03/s)  LR: 7.389e-04  Data: 0.011 (0.014)
Train: 206 [ 600/1251 ( 48%)]  Loss: 3.820 (3.82)  Time: 0.669s, 1530.49/s  (0.696s, 1472.28/s)  LR: 7.389e-04  Data: 0.009 (0.014)
Train: 206 [ 650/1251 ( 52%)]  Loss: 3.556 (3.80)  Time: 0.720s, 1422.99/s  (0.696s, 1471.72/s)  LR: 7.389e-04  Data: 0.009 (0.013)
Train: 206 [ 700/1251 ( 56%)]  Loss: 3.729 (3.80)  Time: 0.676s, 1514.47/s  (0.696s, 1471.86/s)  LR: 7.389e-04  Data: 0.011 (0.013)
Train: 206 [ 750/1251 ( 60%)]  Loss: 3.476 (3.78)  Time: 0.711s, 1439.48/s  (0.695s, 1472.58/s)  LR: 7.389e-04  Data: 0.010 (0.013)
Train: 206 [ 800/1251 ( 64%)]  Loss: 4.147 (3.80)  Time: 0.709s, 1443.64/s  (0.695s, 1472.89/s)  LR: 7.389e-04  Data: 0.010 (0.013)
Train: 206 [ 850/1251 ( 68%)]  Loss: 3.937 (3.81)  Time: 0.678s, 1510.32/s  (0.695s, 1473.01/s)  LR: 7.389e-04  Data: 0.009 (0.013)
Train: 206 [ 900/1251 ( 72%)]  Loss: 3.943 (3.81)  Time: 0.674s, 1520.17/s  (0.695s, 1473.69/s)  LR: 7.389e-04  Data: 0.010 (0.012)
Train: 206 [ 950/1251 ( 76%)]  Loss: 3.681 (3.81)  Time: 0.688s, 1488.23/s  (0.695s, 1474.30/s)  LR: 7.389e-04  Data: 0.011 (0.012)
Train: 206 [1000/1251 ( 80%)]  Loss: 4.027 (3.82)  Time: 0.703s, 1456.00/s  (0.695s, 1473.99/s)  LR: 7.389e-04  Data: 0.009 (0.012)
Train: 206 [1050/1251 ( 84%)]  Loss: 3.966 (3.82)  Time: 0.705s, 1453.38/s  (0.695s, 1473.21/s)  LR: 7.389e-04  Data: 0.009 (0.012)
Train: 206 [1100/1251 ( 88%)]  Loss: 3.985 (3.83)  Time: 0.684s, 1497.48/s  (0.695s, 1473.86/s)  LR: 7.389e-04  Data: 0.009 (0.012)
Train: 206 [1150/1251 ( 92%)]  Loss: 3.745 (3.83)  Time: 0.705s, 1452.25/s  (0.695s, 1473.69/s)  LR: 7.389e-04  Data: 0.011 (0.012)
Train: 206 [1200/1251 ( 96%)]  Loss: 3.788 (3.83)  Time: 0.725s, 1412.36/s  (0.695s, 1472.81/s)  LR: 7.389e-04  Data: 0.010 (0.012)
Train: 206 [1250/1251 (100%)]  Loss: 3.909 (3.83)  Time: 0.673s, 1521.05/s  (0.695s, 1472.66/s)  LR: 7.389e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.531 (1.531)  Loss:  0.9390 (0.9390)  Acc@1: 87.0117 (87.0117)  Acc@5: 96.2891 (96.2891)
Test: [  48/48]  Time: 0.136 (0.574)  Loss:  1.1514 (1.5511)  Acc@1: 82.5472 (71.2000)  Acc@5: 96.2264 (90.3420)
Train: 207 [   0/1251 (  0%)]  Loss: 3.943 (3.94)  Time: 2.154s,  475.37/s  (2.154s,  475.37/s)  LR: 7.366e-04  Data: 1.537 (1.537)
Train: 207 [  50/1251 (  4%)]  Loss: 4.082 (4.01)  Time: 0.702s, 1459.54/s  (0.735s, 1392.97/s)  LR: 7.366e-04  Data: 0.010 (0.047)
Train: 207 [ 100/1251 (  8%)]  Loss: 3.817 (3.95)  Time: 0.754s, 1358.07/s  (0.713s, 1436.16/s)  LR: 7.366e-04  Data: 0.009 (0.029)
Train: 207 [ 150/1251 ( 12%)]  Loss: 3.913 (3.94)  Time: 0.671s, 1526.57/s  (0.705s, 1451.60/s)  LR: 7.366e-04  Data: 0.010 (0.023)
Train: 207 [ 200/1251 ( 16%)]  Loss: 3.942 (3.94)  Time: 0.676s, 1515.62/s  (0.702s, 1457.66/s)  LR: 7.366e-04  Data: 0.009 (0.020)
Train: 207 [ 250/1251 ( 20%)]  Loss: 3.641 (3.89)  Time: 0.671s, 1526.46/s  (0.701s, 1460.53/s)  LR: 7.366e-04  Data: 0.010 (0.018)
Train: 207 [ 300/1251 ( 24%)]  Loss: 3.705 (3.86)  Time: 0.671s, 1525.46/s  (0.699s, 1464.13/s)  LR: 7.366e-04  Data: 0.013 (0.017)
Train: 207 [ 350/1251 ( 28%)]  Loss: 3.604 (3.83)  Time: 0.672s, 1522.91/s  (0.699s, 1465.09/s)  LR: 7.366e-04  Data: 0.011 (0.016)
Train: 207 [ 400/1251 ( 32%)]  Loss: 3.594 (3.80)  Time: 0.671s, 1525.07/s  (0.698s, 1466.96/s)  LR: 7.366e-04  Data: 0.011 (0.015)
Train: 207 [ 450/1251 ( 36%)]  Loss: 3.635 (3.79)  Time: 0.676s, 1514.30/s  (0.697s, 1468.80/s)  LR: 7.366e-04  Data: 0.009 (0.015)
Train: 207 [ 500/1251 ( 40%)]  Loss: 3.513 (3.76)  Time: 0.675s, 1516.98/s  (0.697s, 1470.04/s)  LR: 7.366e-04  Data: 0.013 (0.014)
Train: 207 [ 550/1251 ( 44%)]  Loss: 3.772 (3.76)  Time: 0.671s, 1525.62/s  (0.696s, 1470.64/s)  LR: 7.366e-04  Data: 0.011 (0.014)
Train: 207 [ 600/1251 ( 48%)]  Loss: 3.919 (3.78)  Time: 0.680s, 1505.77/s  (0.696s, 1471.51/s)  LR: 7.366e-04  Data: 0.010 (0.013)
Train: 207 [ 650/1251 ( 52%)]  Loss: 4.073 (3.80)  Time: 0.707s, 1449.14/s  (0.695s, 1472.58/s)  LR: 7.366e-04  Data: 0.010 (0.013)
Train: 207 [ 700/1251 ( 56%)]  Loss: 3.626 (3.79)  Time: 0.715s, 1431.44/s  (0.695s, 1473.49/s)  LR: 7.366e-04  Data: 0.011 (0.013)
Train: 207 [ 750/1251 ( 60%)]  Loss: 4.090 (3.80)  Time: 0.682s, 1501.30/s  (0.695s, 1472.46/s)  LR: 7.366e-04  Data: 0.010 (0.013)
Train: 207 [ 800/1251 ( 64%)]  Loss: 3.994 (3.82)  Time: 0.704s, 1454.06/s  (0.696s, 1472.25/s)  LR: 7.366e-04  Data: 0.009 (0.013)
Train: 207 [ 850/1251 ( 68%)]  Loss: 3.800 (3.81)  Time: 0.670s, 1527.61/s  (0.695s, 1472.58/s)  LR: 7.366e-04  Data: 0.009 (0.013)
Train: 207 [ 900/1251 ( 72%)]  Loss: 3.879 (3.82)  Time: 0.672s, 1524.52/s  (0.695s, 1473.38/s)  LR: 7.366e-04  Data: 0.010 (0.012)
Train: 207 [ 950/1251 ( 76%)]  Loss: 3.901 (3.82)  Time: 0.673s, 1522.54/s  (0.695s, 1472.55/s)  LR: 7.366e-04  Data: 0.010 (0.012)
Train: 207 [1000/1251 ( 80%)]  Loss: 3.633 (3.81)  Time: 0.672s, 1523.47/s  (0.695s, 1472.54/s)  LR: 7.366e-04  Data: 0.011 (0.012)
Train: 207 [1050/1251 ( 84%)]  Loss: 3.765 (3.81)  Time: 0.769s, 1331.30/s  (0.695s, 1473.20/s)  LR: 7.366e-04  Data: 0.010 (0.012)
Train: 207 [1100/1251 ( 88%)]  Loss: 4.141 (3.83)  Time: 0.704s, 1453.94/s  (0.695s, 1473.60/s)  LR: 7.366e-04  Data: 0.010 (0.012)
Train: 207 [1150/1251 ( 92%)]  Loss: 4.316 (3.85)  Time: 0.687s, 1490.65/s  (0.695s, 1474.39/s)  LR: 7.366e-04  Data: 0.009 (0.012)
Train: 207 [1200/1251 ( 96%)]  Loss: 3.691 (3.84)  Time: 0.677s, 1513.12/s  (0.694s, 1474.54/s)  LR: 7.366e-04  Data: 0.009 (0.012)
Train: 207 [1250/1251 (100%)]  Loss: 3.671 (3.83)  Time: 0.695s, 1473.39/s  (0.695s, 1474.27/s)  LR: 7.366e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.606 (1.606)  Loss:  0.9233 (0.9233)  Acc@1: 87.5977 (87.5977)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.136 (0.578)  Loss:  1.2402 (1.5565)  Acc@1: 80.4245 (70.6060)  Acc@5: 94.1038 (90.1880)
Train: 208 [   0/1251 (  0%)]  Loss: 3.670 (3.67)  Time: 2.123s,  482.24/s  (2.123s,  482.24/s)  LR: 7.343e-04  Data: 1.510 (1.510)
Train: 208 [  50/1251 (  4%)]  Loss: 3.724 (3.70)  Time: 0.702s, 1457.93/s  (0.730s, 1403.07/s)  LR: 7.343e-04  Data: 0.009 (0.049)
Train: 208 [ 100/1251 (  8%)]  Loss: 3.778 (3.72)  Time: 0.669s, 1529.69/s  (0.712s, 1438.34/s)  LR: 7.343e-04  Data: 0.010 (0.030)
Train: 208 [ 150/1251 ( 12%)]  Loss: 3.787 (3.74)  Time: 0.665s, 1539.98/s  (0.703s, 1457.43/s)  LR: 7.343e-04  Data: 0.010 (0.023)
Train: 208 [ 200/1251 ( 16%)]  Loss: 3.847 (3.76)  Time: 0.673s, 1520.78/s  (0.700s, 1462.41/s)  LR: 7.343e-04  Data: 0.011 (0.020)
Train: 208 [ 250/1251 ( 20%)]  Loss: 3.723 (3.76)  Time: 0.688s, 1488.10/s  (0.699s, 1465.97/s)  LR: 7.343e-04  Data: 0.009 (0.018)
Train: 208 [ 300/1251 ( 24%)]  Loss: 3.773 (3.76)  Time: 0.691s, 1481.82/s  (0.698s, 1466.53/s)  LR: 7.343e-04  Data: 0.009 (0.017)
Train: 208 [ 350/1251 ( 28%)]  Loss: 4.026 (3.79)  Time: 0.705s, 1452.68/s  (0.697s, 1468.59/s)  LR: 7.343e-04  Data: 0.010 (0.016)
Train: 208 [ 400/1251 ( 32%)]  Loss: 3.959 (3.81)  Time: 0.673s, 1522.53/s  (0.697s, 1469.93/s)  LR: 7.343e-04  Data: 0.012 (0.015)
Train: 208 [ 450/1251 ( 36%)]  Loss: 3.801 (3.81)  Time: 0.673s, 1521.28/s  (0.696s, 1470.82/s)  LR: 7.343e-04  Data: 0.011 (0.015)
Train: 208 [ 500/1251 ( 40%)]  Loss: 3.817 (3.81)  Time: 0.674s, 1519.65/s  (0.696s, 1472.17/s)  LR: 7.343e-04  Data: 0.010 (0.014)
Train: 208 [ 550/1251 ( 44%)]  Loss: 4.401 (3.86)  Time: 0.673s, 1520.84/s  (0.695s, 1472.96/s)  LR: 7.343e-04  Data: 0.011 (0.014)
Train: 208 [ 600/1251 ( 48%)]  Loss: 4.220 (3.89)  Time: 0.674s, 1518.90/s  (0.695s, 1472.49/s)  LR: 7.343e-04  Data: 0.011 (0.014)
Train: 208 [ 650/1251 ( 52%)]  Loss: 3.715 (3.87)  Time: 0.680s, 1505.89/s  (0.695s, 1473.41/s)  LR: 7.343e-04  Data: 0.010 (0.013)
Train: 208 [ 700/1251 ( 56%)]  Loss: 4.037 (3.89)  Time: 0.710s, 1441.54/s  (0.696s, 1472.14/s)  LR: 7.343e-04  Data: 0.010 (0.013)
Train: 208 [ 750/1251 ( 60%)]  Loss: 3.414 (3.86)  Time: 0.718s, 1425.82/s  (0.695s, 1472.51/s)  LR: 7.343e-04  Data: 0.010 (0.013)
Train: 208 [ 800/1251 ( 64%)]  Loss: 3.833 (3.85)  Time: 0.728s, 1407.41/s  (0.695s, 1473.26/s)  LR: 7.343e-04  Data: 0.011 (0.013)
Train: 208 [ 850/1251 ( 68%)]  Loss: 3.746 (3.85)  Time: 0.677s, 1512.55/s  (0.696s, 1470.37/s)  LR: 7.343e-04  Data: 0.017 (0.013)
Train: 208 [ 900/1251 ( 72%)]  Loss: 4.051 (3.86)  Time: 0.676s, 1513.83/s  (0.697s, 1468.17/s)  LR: 7.343e-04  Data: 0.010 (0.013)
Train: 208 [ 950/1251 ( 76%)]  Loss: 4.116 (3.87)  Time: 0.737s, 1389.40/s  (0.699s, 1465.88/s)  LR: 7.343e-04  Data: 0.013 (0.013)
Train: 208 [1000/1251 ( 80%)]  Loss: 4.160 (3.89)  Time: 0.676s, 1515.05/s  (0.698s, 1466.78/s)  LR: 7.343e-04  Data: 0.011 (0.013)
Train: 208 [1050/1251 ( 84%)]  Loss: 3.677 (3.88)  Time: 0.673s, 1522.55/s  (0.698s, 1467.91/s)  LR: 7.343e-04  Data: 0.010 (0.012)
Train: 208 [1100/1251 ( 88%)]  Loss: 4.060 (3.88)  Time: 0.672s, 1524.13/s  (0.697s, 1469.39/s)  LR: 7.343e-04  Data: 0.011 (0.012)
Train: 208 [1150/1251 ( 92%)]  Loss: 4.132 (3.89)  Time: 0.700s, 1462.06/s  (0.697s, 1469.52/s)  LR: 7.343e-04  Data: 0.011 (0.012)
Train: 208 [1200/1251 ( 96%)]  Loss: 4.097 (3.90)  Time: 0.673s, 1522.38/s  (0.697s, 1469.46/s)  LR: 7.343e-04  Data: 0.012 (0.012)
Train: 208 [1250/1251 (100%)]  Loss: 4.057 (3.91)  Time: 0.655s, 1563.80/s  (0.697s, 1469.53/s)  LR: 7.343e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.623 (1.623)  Loss:  0.9731 (0.9731)  Acc@1: 87.2070 (87.2070)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.136 (0.580)  Loss:  1.1113 (1.5440)  Acc@1: 80.5424 (70.9940)  Acc@5: 95.6368 (90.2320)
Train: 209 [   0/1251 (  0%)]  Loss: 3.798 (3.80)  Time: 2.410s,  424.87/s  (2.410s,  424.87/s)  LR: 7.320e-04  Data: 1.733 (1.733)
Train: 209 [  50/1251 (  4%)]  Loss: 3.996 (3.90)  Time: 0.673s, 1520.61/s  (0.726s, 1410.83/s)  LR: 7.320e-04  Data: 0.010 (0.044)
Train: 209 [ 100/1251 (  8%)]  Loss: 3.552 (3.78)  Time: 0.676s, 1515.78/s  (0.707s, 1447.95/s)  LR: 7.320e-04  Data: 0.015 (0.027)
Train: 209 [ 150/1251 ( 12%)]  Loss: 4.152 (3.87)  Time: 0.665s, 1539.21/s  (0.705s, 1452.20/s)  LR: 7.320e-04  Data: 0.010 (0.022)
Train: 209 [ 200/1251 ( 16%)]  Loss: 3.808 (3.86)  Time: 0.719s, 1424.91/s  (0.702s, 1459.27/s)  LR: 7.320e-04  Data: 0.009 (0.019)
Train: 209 [ 250/1251 ( 20%)]  Loss: 3.731 (3.84)  Time: 0.696s, 1471.30/s  (0.700s, 1462.62/s)  LR: 7.320e-04  Data: 0.010 (0.017)
Train: 209 [ 300/1251 ( 24%)]  Loss: 3.812 (3.84)  Time: 0.672s, 1524.91/s  (0.700s, 1463.67/s)  LR: 7.320e-04  Data: 0.010 (0.016)
Train: 209 [ 350/1251 ( 28%)]  Loss: 3.759 (3.83)  Time: 0.672s, 1524.09/s  (0.699s, 1465.75/s)  LR: 7.320e-04  Data: 0.010 (0.015)
Train: 209 [ 400/1251 ( 32%)]  Loss: 3.464 (3.79)  Time: 0.668s, 1532.84/s  (0.699s, 1464.13/s)  LR: 7.320e-04  Data: 0.010 (0.015)
Train: 209 [ 450/1251 ( 36%)]  Loss: 3.669 (3.77)  Time: 0.677s, 1513.40/s  (0.698s, 1466.34/s)  LR: 7.320e-04  Data: 0.010 (0.014)
Train: 209 [ 500/1251 ( 40%)]  Loss: 3.483 (3.75)  Time: 0.705s, 1452.40/s  (0.698s, 1467.73/s)  LR: 7.320e-04  Data: 0.010 (0.014)
Train: 209 [ 550/1251 ( 44%)]  Loss: 3.805 (3.75)  Time: 0.672s, 1524.10/s  (0.697s, 1468.50/s)  LR: 7.320e-04  Data: 0.010 (0.013)
Train: 209 [ 600/1251 ( 48%)]  Loss: 3.955 (3.77)  Time: 0.713s, 1436.63/s  (0.697s, 1468.17/s)  LR: 7.320e-04  Data: 0.010 (0.013)
Train: 209 [ 650/1251 ( 52%)]  Loss: 3.628 (3.76)  Time: 0.673s, 1521.13/s  (0.697s, 1469.62/s)  LR: 7.320e-04  Data: 0.010 (0.013)
Train: 209 [ 700/1251 ( 56%)]  Loss: 3.891 (3.77)  Time: 0.672s, 1523.36/s  (0.696s, 1470.76/s)  LR: 7.320e-04  Data: 0.010 (0.013)
Train: 209 [ 750/1251 ( 60%)]  Loss: 4.012 (3.78)  Time: 0.674s, 1519.88/s  (0.696s, 1470.66/s)  LR: 7.320e-04  Data: 0.010 (0.013)
Train: 209 [ 800/1251 ( 64%)]  Loss: 3.785 (3.78)  Time: 0.723s, 1417.13/s  (0.696s, 1471.10/s)  LR: 7.320e-04  Data: 0.011 (0.012)
Train: 209 [ 850/1251 ( 68%)]  Loss: 3.585 (3.77)  Time: 0.674s, 1518.99/s  (0.696s, 1472.00/s)  LR: 7.320e-04  Data: 0.010 (0.012)
Train: 209 [ 900/1251 ( 72%)]  Loss: 3.563 (3.76)  Time: 0.672s, 1523.09/s  (0.695s, 1472.38/s)  LR: 7.320e-04  Data: 0.010 (0.012)
Train: 209 [ 950/1251 ( 76%)]  Loss: 3.991 (3.77)  Time: 0.673s, 1522.25/s  (0.696s, 1472.25/s)  LR: 7.320e-04  Data: 0.010 (0.012)
Train: 209 [1000/1251 ( 80%)]  Loss: 3.863 (3.78)  Time: 0.667s, 1534.84/s  (0.695s, 1472.70/s)  LR: 7.320e-04  Data: 0.009 (0.012)
Train: 209 [1050/1251 ( 84%)]  Loss: 4.133 (3.79)  Time: 0.699s, 1465.37/s  (0.695s, 1473.35/s)  LR: 7.320e-04  Data: 0.009 (0.012)
Train: 209 [1100/1251 ( 88%)]  Loss: 3.599 (3.78)  Time: 0.668s, 1533.18/s  (0.695s, 1473.80/s)  LR: 7.320e-04  Data: 0.010 (0.012)
Train: 209 [1150/1251 ( 92%)]  Loss: 3.696 (3.78)  Time: 0.740s, 1383.20/s  (0.695s, 1474.10/s)  LR: 7.320e-04  Data: 0.013 (0.012)
Train: 209 [1200/1251 ( 96%)]  Loss: 3.993 (3.79)  Time: 0.783s, 1307.19/s  (0.695s, 1474.12/s)  LR: 7.320e-04  Data: 0.010 (0.012)
Train: 209 [1250/1251 (100%)]  Loss: 3.941 (3.79)  Time: 0.657s, 1559.60/s  (0.695s, 1474.31/s)  LR: 7.320e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.630 (1.630)  Loss:  0.9512 (0.9512)  Acc@1: 86.9141 (86.9141)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.136 (0.590)  Loss:  0.9722 (1.4936)  Acc@1: 83.9623 (71.1080)  Acc@5: 96.5802 (90.4340)
Train: 210 [   0/1251 (  0%)]  Loss: 3.718 (3.72)  Time: 2.526s,  405.46/s  (2.526s,  405.46/s)  LR: 7.297e-04  Data: 1.854 (1.854)
Train: 210 [  50/1251 (  4%)]  Loss: 3.944 (3.83)  Time: 0.699s, 1464.29/s  (0.736s, 1391.33/s)  LR: 7.297e-04  Data: 0.009 (0.054)
Train: 210 [ 100/1251 (  8%)]  Loss: 3.765 (3.81)  Time: 0.669s, 1531.78/s  (0.712s, 1438.03/s)  LR: 7.297e-04  Data: 0.010 (0.032)
Train: 210 [ 150/1251 ( 12%)]  Loss: 4.031 (3.86)  Time: 0.704s, 1454.04/s  (0.705s, 1451.56/s)  LR: 7.297e-04  Data: 0.010 (0.025)
Train: 210 [ 200/1251 ( 16%)]  Loss: 3.591 (3.81)  Time: 0.697s, 1469.51/s  (0.704s, 1455.20/s)  LR: 7.297e-04  Data: 0.009 (0.021)
Train: 210 [ 250/1251 ( 20%)]  Loss: 3.962 (3.84)  Time: 0.672s, 1522.73/s  (0.702s, 1459.26/s)  LR: 7.297e-04  Data: 0.010 (0.019)
Train: 210 [ 300/1251 ( 24%)]  Loss: 3.655 (3.81)  Time: 0.671s, 1524.97/s  (0.699s, 1463.93/s)  LR: 7.297e-04  Data: 0.010 (0.017)
Train: 210 [ 350/1251 ( 28%)]  Loss: 3.629 (3.79)  Time: 0.718s, 1427.00/s  (0.699s, 1465.25/s)  LR: 7.297e-04  Data: 0.010 (0.016)
Train: 210 [ 400/1251 ( 32%)]  Loss: 3.800 (3.79)  Time: 0.673s, 1521.52/s  (0.700s, 1463.64/s)  LR: 7.297e-04  Data: 0.010 (0.016)
Train: 210 [ 450/1251 ( 36%)]  Loss: 3.841 (3.79)  Time: 0.670s, 1528.63/s  (0.698s, 1466.14/s)  LR: 7.297e-04  Data: 0.011 (0.015)
Train: 210 [ 500/1251 ( 40%)]  Loss: 3.615 (3.78)  Time: 0.733s, 1396.90/s  (0.698s, 1467.68/s)  LR: 7.297e-04  Data: 0.016 (0.015)
Train: 210 [ 550/1251 ( 44%)]  Loss: 4.277 (3.82)  Time: 0.672s, 1524.33/s  (0.698s, 1467.75/s)  LR: 7.297e-04  Data: 0.010 (0.014)
Train: 210 [ 600/1251 ( 48%)]  Loss: 3.394 (3.79)  Time: 0.671s, 1525.15/s  (0.698s, 1468.07/s)  LR: 7.297e-04  Data: 0.009 (0.014)
Train: 210 [ 650/1251 ( 52%)]  Loss: 4.049 (3.81)  Time: 0.711s, 1439.98/s  (0.697s, 1469.78/s)  LR: 7.297e-04  Data: 0.009 (0.014)
Train: 210 [ 700/1251 ( 56%)]  Loss: 3.894 (3.81)  Time: 0.719s, 1424.79/s  (0.697s, 1470.00/s)  LR: 7.297e-04  Data: 0.010 (0.013)
Train: 210 [ 750/1251 ( 60%)]  Loss: 3.664 (3.80)  Time: 0.680s, 1505.68/s  (0.697s, 1469.96/s)  LR: 7.297e-04  Data: 0.009 (0.013)
Train: 210 [ 800/1251 ( 64%)]  Loss: 3.778 (3.80)  Time: 0.675s, 1516.18/s  (0.696s, 1470.76/s)  LR: 7.297e-04  Data: 0.010 (0.013)
Train: 210 [ 850/1251 ( 68%)]  Loss: 3.675 (3.79)  Time: 0.674s, 1519.67/s  (0.696s, 1471.87/s)  LR: 7.297e-04  Data: 0.010 (0.013)
Train: 210 [ 900/1251 ( 72%)]  Loss: 3.772 (3.79)  Time: 0.695s, 1473.72/s  (0.696s, 1472.13/s)  LR: 7.297e-04  Data: 0.010 (0.013)
Train: 210 [ 950/1251 ( 76%)]  Loss: 3.748 (3.79)  Time: 0.696s, 1471.49/s  (0.695s, 1472.75/s)  LR: 7.297e-04  Data: 0.009 (0.012)
Train: 210 [1000/1251 ( 80%)]  Loss: 3.729 (3.79)  Time: 0.671s, 1525.18/s  (0.695s, 1473.25/s)  LR: 7.297e-04  Data: 0.010 (0.012)
Train: 210 [1050/1251 ( 84%)]  Loss: 3.355 (3.77)  Time: 0.673s, 1520.96/s  (0.695s, 1473.50/s)  LR: 7.297e-04  Data: 0.011 (0.012)
Train: 210 [1100/1251 ( 88%)]  Loss: 3.829 (3.77)  Time: 0.672s, 1524.44/s  (0.695s, 1473.94/s)  LR: 7.297e-04  Data: 0.010 (0.012)
Train: 210 [1150/1251 ( 92%)]  Loss: 3.706 (3.77)  Time: 0.718s, 1426.00/s  (0.695s, 1474.04/s)  LR: 7.297e-04  Data: 0.009 (0.012)
Train: 210 [1200/1251 ( 96%)]  Loss: 3.810 (3.77)  Time: 0.694s, 1475.68/s  (0.695s, 1474.19/s)  LR: 7.297e-04  Data: 0.010 (0.012)
Train: 210 [1250/1251 (100%)]  Loss: 3.904 (3.77)  Time: 0.658s, 1557.15/s  (0.694s, 1474.52/s)  LR: 7.297e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.659 (1.659)  Loss:  0.8389 (0.8389)  Acc@1: 88.2812 (88.2812)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.136 (0.578)  Loss:  0.9360 (1.4874)  Acc@1: 84.4340 (71.7100)  Acc@5: 95.9906 (90.5520)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-187.pth.tar', 71.87000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-210.pth.tar', 71.71000009033203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-202.pth.tar', 71.62399994384765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-200.pth.tar', 71.55799999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-197.pth.tar', 71.51000004638672)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-201.pth.tar', 71.4920000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-203.pth.tar', 71.4919999633789)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-192.pth.tar', 71.35000012451172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-190.pth.tar', 71.32600001464844)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-180.pth.tar', 71.26400010009766)

Train: 211 [   0/1251 (  0%)]  Loss: 3.730 (3.73)  Time: 2.494s,  410.65/s  (2.494s,  410.65/s)  LR: 7.274e-04  Data: 1.814 (1.814)
Train: 211 [  50/1251 (  4%)]  Loss: 3.693 (3.71)  Time: 0.675s, 1516.77/s  (0.739s, 1385.35/s)  LR: 7.274e-04  Data: 0.010 (0.053)
Train: 211 [ 100/1251 (  8%)]  Loss: 4.048 (3.82)  Time: 0.669s, 1530.09/s  (0.715s, 1431.22/s)  LR: 7.274e-04  Data: 0.010 (0.032)
Train: 211 [ 150/1251 ( 12%)]  Loss: 3.328 (3.70)  Time: 0.672s, 1524.46/s  (0.707s, 1447.60/s)  LR: 7.274e-04  Data: 0.010 (0.025)
Train: 211 [ 200/1251 ( 16%)]  Loss: 3.676 (3.70)  Time: 0.699s, 1463.94/s  (0.705s, 1452.67/s)  LR: 7.274e-04  Data: 0.009 (0.021)
Train: 211 [ 250/1251 ( 20%)]  Loss: 3.614 (3.68)  Time: 0.680s, 1506.72/s  (0.703s, 1456.16/s)  LR: 7.274e-04  Data: 0.011 (0.019)
Train: 211 [ 300/1251 ( 24%)]  Loss: 3.722 (3.69)  Time: 0.735s, 1394.03/s  (0.701s, 1460.11/s)  LR: 7.274e-04  Data: 0.010 (0.017)
Train: 211 [ 350/1251 ( 28%)]  Loss: 3.560 (3.67)  Time: 0.671s, 1525.47/s  (0.700s, 1463.59/s)  LR: 7.274e-04  Data: 0.010 (0.016)
Train: 211 [ 400/1251 ( 32%)]  Loss: 3.922 (3.70)  Time: 0.673s, 1522.62/s  (0.698s, 1466.26/s)  LR: 7.274e-04  Data: 0.010 (0.016)
Train: 211 [ 450/1251 ( 36%)]  Loss: 3.958 (3.73)  Time: 0.671s, 1526.43/s  (0.697s, 1468.66/s)  LR: 7.274e-04  Data: 0.011 (0.015)
Train: 211 [ 500/1251 ( 40%)]  Loss: 3.181 (3.68)  Time: 0.716s, 1429.90/s  (0.697s, 1469.04/s)  LR: 7.274e-04  Data: 0.009 (0.015)
Train: 211 [ 550/1251 ( 44%)]  Loss: 3.748 (3.68)  Time: 0.708s, 1447.21/s  (0.698s, 1467.42/s)  LR: 7.274e-04  Data: 0.010 (0.014)
Train: 211 [ 600/1251 ( 48%)]  Loss: 3.808 (3.69)  Time: 0.704s, 1454.39/s  (0.698s, 1467.69/s)  LR: 7.274e-04  Data: 0.014 (0.014)
Train: 211 [ 650/1251 ( 52%)]  Loss: 3.884 (3.71)  Time: 0.716s, 1430.17/s  (0.698s, 1467.73/s)  LR: 7.274e-04  Data: 0.010 (0.014)
Train: 211 [ 700/1251 ( 56%)]  Loss: 3.957 (3.72)  Time: 0.676s, 1515.38/s  (0.697s, 1468.87/s)  LR: 7.274e-04  Data: 0.013 (0.013)
Train: 211 [ 750/1251 ( 60%)]  Loss: 3.820 (3.73)  Time: 0.754s, 1358.87/s  (0.697s, 1468.32/s)  LR: 7.274e-04  Data: 0.010 (0.013)
Train: 211 [ 800/1251 ( 64%)]  Loss: 3.622 (3.72)  Time: 0.710s, 1442.84/s  (0.697s, 1468.57/s)  LR: 7.274e-04  Data: 0.009 (0.013)
Train: 211 [ 850/1251 ( 68%)]  Loss: 4.020 (3.74)  Time: 0.670s, 1528.37/s  (0.697s, 1468.66/s)  LR: 7.274e-04  Data: 0.010 (0.013)
Train: 211 [ 900/1251 ( 72%)]  Loss: 3.578 (3.73)  Time: 0.693s, 1477.93/s  (0.697s, 1469.84/s)  LR: 7.274e-04  Data: 0.010 (0.013)
Train: 211 [ 950/1251 ( 76%)]  Loss: 3.878 (3.74)  Time: 0.676s, 1513.85/s  (0.697s, 1469.74/s)  LR: 7.274e-04  Data: 0.013 (0.013)
Train: 211 [1000/1251 ( 80%)]  Loss: 3.994 (3.75)  Time: 0.686s, 1491.71/s  (0.697s, 1470.10/s)  LR: 7.274e-04  Data: 0.011 (0.012)
Train: 211 [1050/1251 ( 84%)]  Loss: 3.379 (3.73)  Time: 0.666s, 1537.18/s  (0.696s, 1470.49/s)  LR: 7.274e-04  Data: 0.011 (0.012)
Train: 211 [1100/1251 ( 88%)]  Loss: 3.964 (3.74)  Time: 0.707s, 1448.17/s  (0.696s, 1470.76/s)  LR: 7.274e-04  Data: 0.010 (0.012)
Train: 211 [1150/1251 ( 92%)]  Loss: 4.031 (3.75)  Time: 0.712s, 1437.81/s  (0.696s, 1471.15/s)  LR: 7.274e-04  Data: 0.010 (0.012)
Train: 211 [1200/1251 ( 96%)]  Loss: 3.828 (3.76)  Time: 0.709s, 1443.63/s  (0.696s, 1471.63/s)  LR: 7.274e-04  Data: 0.010 (0.012)
Train: 211 [1250/1251 (100%)]  Loss: 3.496 (3.75)  Time: 0.694s, 1474.63/s  (0.696s, 1471.49/s)  LR: 7.274e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.596 (1.596)  Loss:  0.9092 (0.9092)  Acc@1: 88.7695 (88.7695)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.136 (0.590)  Loss:  1.0039 (1.5327)  Acc@1: 83.8443 (71.2300)  Acc@5: 95.6368 (90.3500)
Train: 212 [   0/1251 (  0%)]  Loss: 3.689 (3.69)  Time: 2.057s,  497.90/s  (2.057s,  497.90/s)  LR: 7.251e-04  Data: 1.441 (1.441)
Train: 212 [  50/1251 (  4%)]  Loss: 3.450 (3.57)  Time: 0.671s, 1525.50/s  (0.727s, 1408.41/s)  LR: 7.251e-04  Data: 0.010 (0.044)
Train: 212 [ 100/1251 (  8%)]  Loss: 3.674 (3.60)  Time: 0.677s, 1512.46/s  (0.710s, 1441.76/s)  LR: 7.251e-04  Data: 0.011 (0.027)
Train: 212 [ 150/1251 ( 12%)]  Loss: 3.928 (3.69)  Time: 0.672s, 1524.73/s  (0.706s, 1450.17/s)  LR: 7.251e-04  Data: 0.010 (0.022)
Train: 212 [ 200/1251 ( 16%)]  Loss: 3.452 (3.64)  Time: 0.677s, 1511.58/s  (0.702s, 1459.03/s)  LR: 7.251e-04  Data: 0.011 (0.019)
Train: 212 [ 250/1251 ( 20%)]  Loss: 3.831 (3.67)  Time: 0.717s, 1428.27/s  (0.701s, 1460.56/s)  LR: 7.251e-04  Data: 0.011 (0.017)
Train: 212 [ 300/1251 ( 24%)]  Loss: 3.488 (3.64)  Time: 0.671s, 1525.41/s  (0.699s, 1464.13/s)  LR: 7.251e-04  Data: 0.010 (0.016)
Train: 212 [ 350/1251 ( 28%)]  Loss: 3.778 (3.66)  Time: 0.673s, 1520.42/s  (0.698s, 1466.03/s)  LR: 7.251e-04  Data: 0.011 (0.015)
Train: 212 [ 400/1251 ( 32%)]  Loss: 4.094 (3.71)  Time: 0.717s, 1428.26/s  (0.698s, 1467.77/s)  LR: 7.251e-04  Data: 0.009 (0.014)
Train: 212 [ 450/1251 ( 36%)]  Loss: 3.763 (3.71)  Time: 0.696s, 1470.33/s  (0.697s, 1468.90/s)  LR: 7.251e-04  Data: 0.009 (0.014)
Train: 212 [ 500/1251 ( 40%)]  Loss: 3.729 (3.72)  Time: 0.707s, 1447.96/s  (0.698s, 1467.91/s)  LR: 7.251e-04  Data: 0.008 (0.014)
Train: 212 [ 550/1251 ( 44%)]  Loss: 3.626 (3.71)  Time: 0.674s, 1519.61/s  (0.698s, 1467.60/s)  LR: 7.251e-04  Data: 0.013 (0.013)
Train: 212 [ 600/1251 ( 48%)]  Loss: 3.980 (3.73)  Time: 0.672s, 1523.87/s  (0.698s, 1467.78/s)  LR: 7.251e-04  Data: 0.011 (0.013)
Train: 212 [ 650/1251 ( 52%)]  Loss: 3.751 (3.73)  Time: 0.668s, 1533.87/s  (0.697s, 1469.04/s)  LR: 7.251e-04  Data: 0.011 (0.013)
Train: 212 [ 700/1251 ( 56%)]  Loss: 3.744 (3.73)  Time: 0.731s, 1401.40/s  (0.697s, 1469.58/s)  LR: 7.251e-04  Data: 0.010 (0.013)
Train: 212 [ 750/1251 ( 60%)]  Loss: 3.321 (3.71)  Time: 0.738s, 1387.77/s  (0.697s, 1469.59/s)  LR: 7.251e-04  Data: 0.010 (0.013)
Train: 212 [ 800/1251 ( 64%)]  Loss: 3.705 (3.71)  Time: 0.673s, 1522.49/s  (0.697s, 1469.43/s)  LR: 7.251e-04  Data: 0.011 (0.012)
Train: 212 [ 850/1251 ( 68%)]  Loss: 3.777 (3.71)  Time: 0.707s, 1449.00/s  (0.696s, 1470.30/s)  LR: 7.251e-04  Data: 0.010 (0.012)
Train: 212 [ 900/1251 ( 72%)]  Loss: 3.698 (3.71)  Time: 0.672s, 1523.93/s  (0.696s, 1471.10/s)  LR: 7.251e-04  Data: 0.013 (0.012)
Train: 212 [ 950/1251 ( 76%)]  Loss: 4.146 (3.73)  Time: 0.715s, 1432.56/s  (0.696s, 1471.31/s)  LR: 7.251e-04  Data: 0.010 (0.012)
Train: 212 [1000/1251 ( 80%)]  Loss: 4.267 (3.76)  Time: 0.673s, 1521.65/s  (0.696s, 1471.95/s)  LR: 7.251e-04  Data: 0.011 (0.012)
Train: 212 [1050/1251 ( 84%)]  Loss: 3.302 (3.74)  Time: 0.688s, 1489.18/s  (0.695s, 1472.47/s)  LR: 7.251e-04  Data: 0.011 (0.012)
Train: 212 [1100/1251 ( 88%)]  Loss: 3.408 (3.72)  Time: 0.673s, 1521.43/s  (0.695s, 1472.63/s)  LR: 7.251e-04  Data: 0.010 (0.012)
Train: 212 [1150/1251 ( 92%)]  Loss: 3.644 (3.72)  Time: 0.703s, 1456.71/s  (0.695s, 1473.09/s)  LR: 7.251e-04  Data: 0.009 (0.012)
Train: 212 [1200/1251 ( 96%)]  Loss: 4.109 (3.73)  Time: 0.693s, 1477.45/s  (0.695s, 1473.37/s)  LR: 7.251e-04  Data: 0.010 (0.012)
Train: 212 [1250/1251 (100%)]  Loss: 3.685 (3.73)  Time: 0.655s, 1563.44/s  (0.695s, 1473.81/s)  LR: 7.251e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.592 (1.592)  Loss:  0.8442 (0.8442)  Acc@1: 87.7930 (87.7930)  Acc@5: 96.2891 (96.2891)
Test: [  48/48]  Time: 0.137 (0.581)  Loss:  0.9546 (1.4379)  Acc@1: 82.4292 (71.5060)  Acc@5: 94.9292 (90.5140)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-187.pth.tar', 71.87000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-210.pth.tar', 71.71000009033203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-202.pth.tar', 71.62399994384765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-200.pth.tar', 71.55799999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-197.pth.tar', 71.51000004638672)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-212.pth.tar', 71.50599986572266)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-201.pth.tar', 71.4920000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-203.pth.tar', 71.4919999633789)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-192.pth.tar', 71.35000012451172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-190.pth.tar', 71.32600001464844)

Train: 213 [   0/1251 (  0%)]  Loss: 3.915 (3.91)  Time: 2.190s,  467.65/s  (2.190s,  467.65/s)  LR: 7.228e-04  Data: 1.572 (1.572)
Train: 213 [  50/1251 (  4%)]  Loss: 3.299 (3.61)  Time: 0.672s, 1524.20/s  (0.728s, 1407.16/s)  LR: 7.228e-04  Data: 0.010 (0.048)
Train: 213 [ 100/1251 (  8%)]  Loss: 3.612 (3.61)  Time: 0.690s, 1483.26/s  (0.707s, 1449.31/s)  LR: 7.228e-04  Data: 0.009 (0.029)
Train: 213 [ 150/1251 ( 12%)]  Loss: 3.717 (3.64)  Time: 0.683s, 1498.99/s  (0.703s, 1457.11/s)  LR: 7.228e-04  Data: 0.009 (0.023)
Train: 213 [ 200/1251 ( 16%)]  Loss: 3.677 (3.64)  Time: 0.706s, 1450.02/s  (0.701s, 1460.58/s)  LR: 7.228e-04  Data: 0.010 (0.020)
Train: 213 [ 250/1251 ( 20%)]  Loss: 3.985 (3.70)  Time: 0.689s, 1485.42/s  (0.700s, 1462.17/s)  LR: 7.228e-04  Data: 0.011 (0.018)
Train: 213 [ 300/1251 ( 24%)]  Loss: 3.739 (3.71)  Time: 0.774s, 1322.62/s  (0.700s, 1461.96/s)  LR: 7.228e-04  Data: 0.009 (0.017)
Train: 213 [ 350/1251 ( 28%)]  Loss: 3.683 (3.70)  Time: 0.691s, 1482.38/s  (0.699s, 1465.69/s)  LR: 7.228e-04  Data: 0.012 (0.016)
Train: 213 [ 400/1251 ( 32%)]  Loss: 3.679 (3.70)  Time: 0.666s, 1537.34/s  (0.699s, 1465.64/s)  LR: 7.228e-04  Data: 0.010 (0.015)
Train: 213 [ 450/1251 ( 36%)]  Loss: 3.373 (3.67)  Time: 0.739s, 1385.95/s  (0.698s, 1466.10/s)  LR: 7.228e-04  Data: 0.009 (0.014)
Train: 213 [ 500/1251 ( 40%)]  Loss: 3.992 (3.70)  Time: 0.668s, 1532.60/s  (0.698s, 1466.84/s)  LR: 7.228e-04  Data: 0.009 (0.014)
Train: 213 [ 550/1251 ( 44%)]  Loss: 3.647 (3.69)  Time: 0.685s, 1495.85/s  (0.698s, 1467.80/s)  LR: 7.228e-04  Data: 0.009 (0.014)
Train: 213 [ 600/1251 ( 48%)]  Loss: 3.864 (3.71)  Time: 0.720s, 1422.20/s  (0.698s, 1467.83/s)  LR: 7.228e-04  Data: 0.009 (0.013)
Train: 213 [ 650/1251 ( 52%)]  Loss: 4.123 (3.74)  Time: 0.688s, 1488.41/s  (0.697s, 1468.12/s)  LR: 7.228e-04  Data: 0.009 (0.013)
Train: 213 [ 700/1251 ( 56%)]  Loss: 3.518 (3.72)  Time: 0.699s, 1465.59/s  (0.697s, 1469.24/s)  LR: 7.228e-04  Data: 0.009 (0.013)
Train: 213 [ 750/1251 ( 60%)]  Loss: 3.486 (3.71)  Time: 0.699s, 1464.69/s  (0.696s, 1470.39/s)  LR: 7.228e-04  Data: 0.009 (0.013)
Train: 213 [ 800/1251 ( 64%)]  Loss: 3.689 (3.71)  Time: 0.706s, 1450.32/s  (0.696s, 1470.78/s)  LR: 7.228e-04  Data: 0.010 (0.013)
Train: 213 [ 850/1251 ( 68%)]  Loss: 3.908 (3.72)  Time: 0.693s, 1476.67/s  (0.696s, 1471.30/s)  LR: 7.228e-04  Data: 0.009 (0.013)
Train: 213 [ 900/1251 ( 72%)]  Loss: 3.466 (3.70)  Time: 0.689s, 1486.24/s  (0.696s, 1472.29/s)  LR: 7.228e-04  Data: 0.010 (0.012)
Train: 213 [ 950/1251 ( 76%)]  Loss: 3.552 (3.70)  Time: 0.672s, 1524.48/s  (0.695s, 1473.10/s)  LR: 7.228e-04  Data: 0.010 (0.012)
Train: 213 [1000/1251 ( 80%)]  Loss: 3.412 (3.68)  Time: 0.767s, 1335.02/s  (0.695s, 1472.42/s)  LR: 7.228e-04  Data: 0.009 (0.012)
Train: 213 [1050/1251 ( 84%)]  Loss: 3.750 (3.69)  Time: 0.716s, 1430.60/s  (0.696s, 1472.15/s)  LR: 7.228e-04  Data: 0.011 (0.012)
Train: 213 [1100/1251 ( 88%)]  Loss: 3.710 (3.69)  Time: 0.708s, 1446.67/s  (0.695s, 1472.75/s)  LR: 7.228e-04  Data: 0.009 (0.012)
Train: 213 [1150/1251 ( 92%)]  Loss: 3.619 (3.68)  Time: 0.598s, 1712.21/s  (0.695s, 1473.37/s)  LR: 7.228e-04  Data: 0.009 (0.012)
Train: 213 [1200/1251 ( 96%)]  Loss: 4.098 (3.70)  Time: 0.762s, 1344.32/s  (0.695s, 1473.58/s)  LR: 7.228e-04  Data: 0.010 (0.012)
Train: 213 [1250/1251 (100%)]  Loss: 3.812 (3.70)  Time: 0.679s, 1508.30/s  (0.695s, 1473.54/s)  LR: 7.228e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.578 (1.578)  Loss:  0.9531 (0.9531)  Acc@1: 87.2070 (87.2070)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.136 (0.592)  Loss:  1.0127 (1.5577)  Acc@1: 83.0189 (71.3400)  Acc@5: 95.9906 (90.5720)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-187.pth.tar', 71.87000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-210.pth.tar', 71.71000009033203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-202.pth.tar', 71.62399994384765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-200.pth.tar', 71.55799999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-197.pth.tar', 71.51000004638672)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-212.pth.tar', 71.50599986572266)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-201.pth.tar', 71.4920000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-203.pth.tar', 71.4919999633789)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-192.pth.tar', 71.35000012451172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-213.pth.tar', 71.33999999267579)

Train: 214 [   0/1251 (  0%)]  Loss: 3.832 (3.83)  Time: 2.288s,  447.47/s  (2.288s,  447.47/s)  LR: 7.204e-04  Data: 1.660 (1.660)
Train: 214 [  50/1251 (  4%)]  Loss: 3.609 (3.72)  Time: 0.707s, 1448.11/s  (0.730s, 1401.84/s)  LR: 7.204e-04  Data: 0.010 (0.044)
Train: 214 [ 100/1251 (  8%)]  Loss: 4.124 (3.86)  Time: 0.686s, 1492.73/s  (0.712s, 1437.96/s)  LR: 7.204e-04  Data: 0.011 (0.027)
Train: 214 [ 150/1251 ( 12%)]  Loss: 3.980 (3.89)  Time: 0.672s, 1524.05/s  (0.706s, 1449.50/s)  LR: 7.204e-04  Data: 0.009 (0.022)
Train: 214 [ 200/1251 ( 16%)]  Loss: 4.148 (3.94)  Time: 0.698s, 1467.09/s  (0.704s, 1455.55/s)  LR: 7.204e-04  Data: 0.011 (0.019)
Train: 214 [ 250/1251 ( 20%)]  Loss: 3.833 (3.92)  Time: 0.711s, 1440.76/s  (0.701s, 1460.29/s)  LR: 7.204e-04  Data: 0.009 (0.017)
Train: 214 [ 300/1251 ( 24%)]  Loss: 3.398 (3.85)  Time: 0.708s, 1446.98/s  (0.699s, 1463.96/s)  LR: 7.204e-04  Data: 0.011 (0.016)
Train: 214 [ 350/1251 ( 28%)]  Loss: 3.389 (3.79)  Time: 0.721s, 1419.56/s  (0.698s, 1466.33/s)  LR: 7.204e-04  Data: 0.017 (0.015)
Train: 214 [ 400/1251 ( 32%)]  Loss: 3.717 (3.78)  Time: 0.691s, 1481.15/s  (0.697s, 1468.11/s)  LR: 7.204e-04  Data: 0.010 (0.014)
Train: 214 [ 450/1251 ( 36%)]  Loss: 3.703 (3.77)  Time: 0.671s, 1525.07/s  (0.696s, 1470.35/s)  LR: 7.204e-04  Data: 0.010 (0.014)
Train: 214 [ 500/1251 ( 40%)]  Loss: 3.522 (3.75)  Time: 0.708s, 1447.21/s  (0.697s, 1469.98/s)  LR: 7.204e-04  Data: 0.010 (0.014)
Train: 214 [ 550/1251 ( 44%)]  Loss: 3.925 (3.76)  Time: 0.731s, 1400.54/s  (0.696s, 1470.53/s)  LR: 7.204e-04  Data: 0.009 (0.013)
Train: 214 [ 600/1251 ( 48%)]  Loss: 3.731 (3.76)  Time: 0.679s, 1508.36/s  (0.696s, 1470.57/s)  LR: 7.204e-04  Data: 0.010 (0.013)
Train: 214 [ 650/1251 ( 52%)]  Loss: 3.673 (3.76)  Time: 0.674s, 1519.79/s  (0.696s, 1471.96/s)  LR: 7.204e-04  Data: 0.011 (0.013)
Train: 214 [ 700/1251 ( 56%)]  Loss: 3.375 (3.73)  Time: 0.706s, 1451.02/s  (0.695s, 1473.36/s)  LR: 7.204e-04  Data: 0.011 (0.013)
Train: 214 [ 750/1251 ( 60%)]  Loss: 4.084 (3.75)  Time: 0.670s, 1527.50/s  (0.695s, 1473.97/s)  LR: 7.204e-04  Data: 0.010 (0.012)
Train: 214 [ 800/1251 ( 64%)]  Loss: 3.792 (3.76)  Time: 0.733s, 1397.88/s  (0.695s, 1473.49/s)  LR: 7.204e-04  Data: 0.009 (0.012)
Train: 214 [ 850/1251 ( 68%)]  Loss: 3.790 (3.76)  Time: 0.708s, 1446.38/s  (0.695s, 1473.89/s)  LR: 7.204e-04  Data: 0.010 (0.012)
Train: 214 [ 900/1251 ( 72%)]  Loss: 3.666 (3.75)  Time: 0.689s, 1486.90/s  (0.694s, 1474.65/s)  LR: 7.204e-04  Data: 0.010 (0.012)
Train: 214 [ 950/1251 ( 76%)]  Loss: 3.485 (3.74)  Time: 0.675s, 1517.00/s  (0.695s, 1473.48/s)  LR: 7.204e-04  Data: 0.010 (0.012)
Train: 214 [1000/1251 ( 80%)]  Loss: 3.356 (3.72)  Time: 0.673s, 1522.06/s  (0.695s, 1473.53/s)  LR: 7.204e-04  Data: 0.011 (0.012)
Train: 214 [1050/1251 ( 84%)]  Loss: 3.751 (3.72)  Time: 0.704s, 1454.51/s  (0.695s, 1473.75/s)  LR: 7.204e-04  Data: 0.009 (0.012)
Train: 214 [1100/1251 ( 88%)]  Loss: 3.751 (3.72)  Time: 0.678s, 1509.66/s  (0.694s, 1474.83/s)  LR: 7.204e-04  Data: 0.011 (0.012)
Train: 214 [1150/1251 ( 92%)]  Loss: 3.864 (3.73)  Time: 0.716s, 1429.88/s  (0.694s, 1474.74/s)  LR: 7.204e-04  Data: 0.010 (0.012)
Train: 214 [1200/1251 ( 96%)]  Loss: 4.081 (3.74)  Time: 0.755s, 1355.82/s  (0.695s, 1473.94/s)  LR: 7.204e-04  Data: 0.010 (0.012)
Train: 214 [1250/1251 (100%)]  Loss: 3.711 (3.74)  Time: 0.660s, 1551.78/s  (0.695s, 1474.27/s)  LR: 7.204e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.500 (1.500)  Loss:  0.9194 (0.9194)  Acc@1: 87.4023 (87.4023)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.136 (0.579)  Loss:  0.9395 (1.5235)  Acc@1: 83.0189 (71.8860)  Acc@5: 95.2830 (90.8660)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-214.pth.tar', 71.88599999267578)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-187.pth.tar', 71.87000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-210.pth.tar', 71.71000009033203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-202.pth.tar', 71.62399994384765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-200.pth.tar', 71.55799999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-197.pth.tar', 71.51000004638672)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-212.pth.tar', 71.50599986572266)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-201.pth.tar', 71.4920000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-203.pth.tar', 71.4919999633789)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-192.pth.tar', 71.35000012451172)

Train: 215 [   0/1251 (  0%)]  Loss: 4.110 (4.11)  Time: 2.091s,  489.61/s  (2.091s,  489.61/s)  LR: 7.181e-04  Data: 1.477 (1.477)
Train: 215 [  50/1251 (  4%)]  Loss: 3.767 (3.94)  Time: 0.682s, 1501.96/s  (0.728s, 1405.97/s)  LR: 7.181e-04  Data: 0.010 (0.044)
Train: 215 [ 100/1251 (  8%)]  Loss: 3.551 (3.81)  Time: 0.671s, 1526.27/s  (0.708s, 1445.54/s)  LR: 7.181e-04  Data: 0.010 (0.027)
Train: 215 [ 150/1251 ( 12%)]  Loss: 3.567 (3.75)  Time: 0.684s, 1496.12/s  (0.701s, 1459.89/s)  LR: 7.181e-04  Data: 0.011 (0.022)
Train: 215 [ 200/1251 ( 16%)]  Loss: 3.387 (3.68)  Time: 0.702s, 1457.90/s  (0.699s, 1464.59/s)  LR: 7.181e-04  Data: 0.009 (0.019)
Train: 215 [ 250/1251 ( 20%)]  Loss: 3.835 (3.70)  Time: 0.703s, 1455.90/s  (0.698s, 1467.90/s)  LR: 7.181e-04  Data: 0.009 (0.017)
Train: 215 [ 300/1251 ( 24%)]  Loss: 3.740 (3.71)  Time: 0.722s, 1418.59/s  (0.697s, 1469.36/s)  LR: 7.181e-04  Data: 0.009 (0.016)
Train: 215 [ 350/1251 ( 28%)]  Loss: 3.984 (3.74)  Time: 0.706s, 1450.56/s  (0.697s, 1469.37/s)  LR: 7.181e-04  Data: 0.009 (0.015)
Train: 215 [ 400/1251 ( 32%)]  Loss: 3.906 (3.76)  Time: 0.703s, 1456.24/s  (0.698s, 1467.54/s)  LR: 7.181e-04  Data: 0.009 (0.015)
Train: 215 [ 450/1251 ( 36%)]  Loss: 3.686 (3.75)  Time: 0.689s, 1486.85/s  (0.698s, 1467.22/s)  LR: 7.181e-04  Data: 0.010 (0.014)
Train: 215 [ 500/1251 ( 40%)]  Loss: 3.875 (3.76)  Time: 0.700s, 1461.90/s  (0.697s, 1469.30/s)  LR: 7.181e-04  Data: 0.009 (0.014)
Train: 215 [ 550/1251 ( 44%)]  Loss: 4.000 (3.78)  Time: 0.672s, 1524.00/s  (0.697s, 1470.02/s)  LR: 7.181e-04  Data: 0.010 (0.013)
Train: 215 [ 600/1251 ( 48%)]  Loss: 3.696 (3.78)  Time: 0.689s, 1486.92/s  (0.696s, 1471.22/s)  LR: 7.181e-04  Data: 0.010 (0.013)
Train: 215 [ 650/1251 ( 52%)]  Loss: 3.822 (3.78)  Time: 0.717s, 1427.59/s  (0.696s, 1472.11/s)  LR: 7.181e-04  Data: 0.009 (0.013)
Train: 215 [ 700/1251 ( 56%)]  Loss: 3.574 (3.77)  Time: 0.672s, 1523.18/s  (0.695s, 1473.02/s)  LR: 7.181e-04  Data: 0.010 (0.013)
Train: 215 [ 750/1251 ( 60%)]  Loss: 3.702 (3.76)  Time: 0.843s, 1214.76/s  (0.696s, 1472.30/s)  LR: 7.181e-04  Data: 0.010 (0.013)
Train: 215 [ 800/1251 ( 64%)]  Loss: 3.575 (3.75)  Time: 0.671s, 1527.17/s  (0.695s, 1473.25/s)  LR: 7.181e-04  Data: 0.010 (0.012)
Train: 215 [ 850/1251 ( 68%)]  Loss: 3.708 (3.75)  Time: 0.669s, 1531.27/s  (0.695s, 1473.62/s)  LR: 7.181e-04  Data: 0.010 (0.012)
Train: 215 [ 900/1251 ( 72%)]  Loss: 3.689 (3.75)  Time: 0.699s, 1465.78/s  (0.695s, 1474.34/s)  LR: 7.181e-04  Data: 0.009 (0.012)
Train: 215 [ 950/1251 ( 76%)]  Loss: 3.597 (3.74)  Time: 0.687s, 1491.32/s  (0.695s, 1474.27/s)  LR: 7.181e-04  Data: 0.016 (0.012)
Train: 215 [1000/1251 ( 80%)]  Loss: 3.297 (3.72)  Time: 0.710s, 1442.31/s  (0.695s, 1474.02/s)  LR: 7.181e-04  Data: 0.009 (0.012)
Train: 215 [1050/1251 ( 84%)]  Loss: 3.916 (3.73)  Time: 0.682s, 1502.19/s  (0.695s, 1474.05/s)  LR: 7.181e-04  Data: 0.010 (0.012)
Train: 215 [1100/1251 ( 88%)]  Loss: 3.751 (3.73)  Time: 0.700s, 1462.20/s  (0.694s, 1474.45/s)  LR: 7.181e-04  Data: 0.010 (0.012)
Train: 215 [1150/1251 ( 92%)]  Loss: 3.629 (3.72)  Time: 0.671s, 1526.00/s  (0.694s, 1474.91/s)  LR: 7.181e-04  Data: 0.010 (0.012)
Train: 215 [1200/1251 ( 96%)]  Loss: 3.760 (3.73)  Time: 0.664s, 1543.24/s  (0.694s, 1475.15/s)  LR: 7.181e-04  Data: 0.009 (0.012)
Train: 215 [1250/1251 (100%)]  Loss: 3.823 (3.73)  Time: 0.670s, 1528.63/s  (0.694s, 1475.40/s)  LR: 7.181e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.495 (1.495)  Loss:  0.8530 (0.8530)  Acc@1: 88.5742 (88.5742)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.136 (0.578)  Loss:  0.9678 (1.5120)  Acc@1: 82.9009 (71.1720)  Acc@5: 94.9292 (90.3040)
Train: 216 [   0/1251 (  0%)]  Loss: 3.588 (3.59)  Time: 2.134s,  479.74/s  (2.134s,  479.74/s)  LR: 7.158e-04  Data: 1.515 (1.515)
Train: 216 [  50/1251 (  4%)]  Loss: 3.558 (3.57)  Time: 0.680s, 1506.60/s  (0.730s, 1403.08/s)  LR: 7.158e-04  Data: 0.010 (0.052)
Train: 216 [ 100/1251 (  8%)]  Loss: 3.748 (3.63)  Time: 0.670s, 1528.74/s  (0.709s, 1443.66/s)  LR: 7.158e-04  Data: 0.010 (0.031)
Train: 216 [ 150/1251 ( 12%)]  Loss: 3.788 (3.67)  Time: 0.677s, 1512.49/s  (0.705s, 1453.49/s)  LR: 7.158e-04  Data: 0.010 (0.024)
Train: 216 [ 200/1251 ( 16%)]  Loss: 4.018 (3.74)  Time: 0.666s, 1538.47/s  (0.702s, 1458.99/s)  LR: 7.158e-04  Data: 0.010 (0.020)
Train: 216 [ 250/1251 ( 20%)]  Loss: 3.448 (3.69)  Time: 0.671s, 1525.90/s  (0.701s, 1460.20/s)  LR: 7.158e-04  Data: 0.010 (0.018)
Train: 216 [ 300/1251 ( 24%)]  Loss: 3.995 (3.73)  Time: 0.746s, 1372.54/s  (0.700s, 1463.80/s)  LR: 7.158e-04  Data: 0.013 (0.017)
Train: 216 [ 350/1251 ( 28%)]  Loss: 3.971 (3.76)  Time: 0.683s, 1498.92/s  (0.699s, 1464.13/s)  LR: 7.158e-04  Data: 0.010 (0.016)
Train: 216 [ 400/1251 ( 32%)]  Loss: 3.521 (3.74)  Time: 0.694s, 1475.01/s  (0.698s, 1467.77/s)  LR: 7.158e-04  Data: 0.009 (0.015)
Train: 216 [ 450/1251 ( 36%)]  Loss: 3.893 (3.75)  Time: 0.672s, 1524.27/s  (0.697s, 1468.92/s)  LR: 7.158e-04  Data: 0.011 (0.015)
Train: 216 [ 500/1251 ( 40%)]  Loss: 3.264 (3.71)  Time: 0.723s, 1416.10/s  (0.696s, 1470.68/s)  LR: 7.158e-04  Data: 0.009 (0.014)
Train: 216 [ 550/1251 ( 44%)]  Loss: 3.650 (3.70)  Time: 0.692s, 1479.01/s  (0.696s, 1470.64/s)  LR: 7.158e-04  Data: 0.010 (0.014)
Train: 216 [ 600/1251 ( 48%)]  Loss: 3.644 (3.70)  Time: 0.666s, 1536.51/s  (0.697s, 1470.18/s)  LR: 7.158e-04  Data: 0.011 (0.014)
Train: 216 [ 650/1251 ( 52%)]  Loss: 3.616 (3.69)  Time: 0.707s, 1449.00/s  (0.697s, 1470.08/s)  LR: 7.158e-04  Data: 0.015 (0.013)
Train: 216 [ 700/1251 ( 56%)]  Loss: 3.553 (3.68)  Time: 0.674s, 1520.24/s  (0.696s, 1471.05/s)  LR: 7.158e-04  Data: 0.010 (0.013)
Train: 216 [ 750/1251 ( 60%)]  Loss: 3.838 (3.69)  Time: 0.687s, 1489.96/s  (0.696s, 1471.84/s)  LR: 7.158e-04  Data: 0.010 (0.013)
Train: 216 [ 800/1251 ( 64%)]  Loss: 3.905 (3.71)  Time: 0.710s, 1442.33/s  (0.696s, 1472.15/s)  LR: 7.158e-04  Data: 0.011 (0.013)
Train: 216 [ 850/1251 ( 68%)]  Loss: 3.647 (3.70)  Time: 0.713s, 1436.41/s  (0.695s, 1472.52/s)  LR: 7.158e-04  Data: 0.009 (0.013)
Train: 216 [ 900/1251 ( 72%)]  Loss: 4.201 (3.73)  Time: 0.672s, 1524.45/s  (0.695s, 1473.01/s)  LR: 7.158e-04  Data: 0.011 (0.012)
Train: 216 [ 950/1251 ( 76%)]  Loss: 3.814 (3.73)  Time: 0.775s, 1321.34/s  (0.695s, 1472.35/s)  LR: 7.158e-04  Data: 0.009 (0.012)
Train: 216 [1000/1251 ( 80%)]  Loss: 3.692 (3.73)  Time: 0.675s, 1516.67/s  (0.695s, 1472.40/s)  LR: 7.158e-04  Data: 0.010 (0.012)
Train: 216 [1050/1251 ( 84%)]  Loss: 3.594 (3.72)  Time: 0.739s, 1386.53/s  (0.695s, 1472.88/s)  LR: 7.158e-04  Data: 0.010 (0.012)
Train: 216 [1100/1251 ( 88%)]  Loss: 3.444 (3.71)  Time: 0.672s, 1524.26/s  (0.695s, 1473.68/s)  LR: 7.158e-04  Data: 0.009 (0.012)
Train: 216 [1150/1251 ( 92%)]  Loss: 3.272 (3.69)  Time: 0.684s, 1497.51/s  (0.695s, 1473.17/s)  LR: 7.158e-04  Data: 0.013 (0.012)
Train: 216 [1200/1251 ( 96%)]  Loss: 3.929 (3.70)  Time: 0.673s, 1521.74/s  (0.695s, 1472.66/s)  LR: 7.158e-04  Data: 0.011 (0.012)
Train: 216 [1250/1251 (100%)]  Loss: 3.512 (3.70)  Time: 0.659s, 1553.76/s  (0.695s, 1472.73/s)  LR: 7.158e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.559 (1.559)  Loss:  1.0107 (1.0107)  Acc@1: 89.2578 (89.2578)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.136 (0.581)  Loss:  1.1562 (1.6140)  Acc@1: 83.8443 (71.8980)  Acc@5: 95.9906 (90.7080)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-216.pth.tar', 71.89799996337891)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-214.pth.tar', 71.88599999267578)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-187.pth.tar', 71.87000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-210.pth.tar', 71.71000009033203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-202.pth.tar', 71.62399994384765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-200.pth.tar', 71.55799999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-197.pth.tar', 71.51000004638672)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-212.pth.tar', 71.50599986572266)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-201.pth.tar', 71.4920000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-203.pth.tar', 71.4919999633789)

Train: 217 [   0/1251 (  0%)]  Loss: 3.527 (3.53)  Time: 2.142s,  478.06/s  (2.142s,  478.06/s)  LR: 7.134e-04  Data: 1.476 (1.476)
Train: 217 [  50/1251 (  4%)]  Loss: 3.568 (3.55)  Time: 0.704s, 1455.46/s  (0.735s, 1393.58/s)  LR: 7.134e-04  Data: 0.012 (0.047)
Train: 217 [ 100/1251 (  8%)]  Loss: 3.927 (3.67)  Time: 0.717s, 1427.49/s  (0.719s, 1423.90/s)  LR: 7.134e-04  Data: 0.011 (0.029)
Train: 217 [ 150/1251 ( 12%)]  Loss: 3.717 (3.68)  Time: 0.675s, 1518.12/s  (0.713s, 1436.76/s)  LR: 7.134e-04  Data: 0.011 (0.023)
Train: 217 [ 200/1251 ( 16%)]  Loss: 3.679 (3.68)  Time: 0.695s, 1474.22/s  (0.706s, 1449.88/s)  LR: 7.134e-04  Data: 0.009 (0.020)
Train: 217 [ 250/1251 ( 20%)]  Loss: 3.745 (3.69)  Time: 0.672s, 1524.70/s  (0.703s, 1457.27/s)  LR: 7.134e-04  Data: 0.010 (0.018)
Train: 217 [ 300/1251 ( 24%)]  Loss: 3.665 (3.69)  Time: 0.671s, 1525.43/s  (0.702s, 1459.08/s)  LR: 7.134e-04  Data: 0.010 (0.017)
Train: 217 [ 350/1251 ( 28%)]  Loss: 3.798 (3.70)  Time: 0.714s, 1434.32/s  (0.701s, 1460.94/s)  LR: 7.134e-04  Data: 0.011 (0.016)
Train: 217 [ 400/1251 ( 32%)]  Loss: 4.177 (3.76)  Time: 0.671s, 1525.71/s  (0.700s, 1463.22/s)  LR: 7.134e-04  Data: 0.010 (0.015)
Train: 217 [ 450/1251 ( 36%)]  Loss: 4.160 (3.80)  Time: 0.683s, 1499.37/s  (0.700s, 1462.16/s)  LR: 7.134e-04  Data: 0.011 (0.015)
Train: 217 [ 500/1251 ( 40%)]  Loss: 3.771 (3.79)  Time: 0.694s, 1474.46/s  (0.700s, 1463.14/s)  LR: 7.134e-04  Data: 0.010 (0.014)
Train: 217 [ 550/1251 ( 44%)]  Loss: 3.379 (3.76)  Time: 0.673s, 1522.11/s  (0.699s, 1465.45/s)  LR: 7.134e-04  Data: 0.010 (0.014)
Train: 217 [ 600/1251 ( 48%)]  Loss: 3.981 (3.78)  Time: 0.775s, 1321.88/s  (0.699s, 1465.68/s)  LR: 7.134e-04  Data: 0.009 (0.014)
Train: 217 [ 650/1251 ( 52%)]  Loss: 3.996 (3.79)  Time: 0.675s, 1516.67/s  (0.698s, 1466.69/s)  LR: 7.134e-04  Data: 0.009 (0.013)
Train: 217 [ 700/1251 ( 56%)]  Loss: 4.014 (3.81)  Time: 0.668s, 1534.02/s  (0.698s, 1466.37/s)  LR: 7.134e-04  Data: 0.009 (0.013)
Train: 217 [ 750/1251 ( 60%)]  Loss: 4.023 (3.82)  Time: 0.667s, 1534.41/s  (0.699s, 1465.28/s)  LR: 7.134e-04  Data: 0.009 (0.013)
Train: 217 [ 800/1251 ( 64%)]  Loss: 4.164 (3.84)  Time: 0.670s, 1527.58/s  (0.699s, 1465.68/s)  LR: 7.134e-04  Data: 0.011 (0.013)
Train: 217 [ 850/1251 ( 68%)]  Loss: 3.716 (3.83)  Time: 0.704s, 1454.09/s  (0.698s, 1466.02/s)  LR: 7.134e-04  Data: 0.009 (0.013)
Train: 217 [ 900/1251 ( 72%)]  Loss: 3.988 (3.84)  Time: 0.705s, 1452.96/s  (0.698s, 1466.83/s)  LR: 7.134e-04  Data: 0.009 (0.013)
Train: 217 [ 950/1251 ( 76%)]  Loss: 3.710 (3.84)  Time: 0.674s, 1518.63/s  (0.698s, 1467.44/s)  LR: 7.134e-04  Data: 0.010 (0.012)
Train: 217 [1000/1251 ( 80%)]  Loss: 3.797 (3.83)  Time: 0.722s, 1417.32/s  (0.698s, 1468.02/s)  LR: 7.134e-04  Data: 0.010 (0.012)
Train: 217 [1050/1251 ( 84%)]  Loss: 3.467 (3.82)  Time: 0.706s, 1451.10/s  (0.697s, 1468.92/s)  LR: 7.134e-04  Data: 0.011 (0.012)
Train: 217 [1100/1251 ( 88%)]  Loss: 4.141 (3.83)  Time: 0.700s, 1463.64/s  (0.697s, 1468.75/s)  LR: 7.134e-04  Data: 0.010 (0.012)
Train: 217 [1150/1251 ( 92%)]  Loss: 3.611 (3.82)  Time: 0.670s, 1527.56/s  (0.698s, 1468.03/s)  LR: 7.134e-04  Data: 0.009 (0.012)
Train: 217 [1200/1251 ( 96%)]  Loss: 3.673 (3.82)  Time: 0.713s, 1436.38/s  (0.697s, 1468.52/s)  LR: 7.134e-04  Data: 0.010 (0.012)
Train: 217 [1250/1251 (100%)]  Loss: 3.786 (3.81)  Time: 0.691s, 1482.54/s  (0.697s, 1468.81/s)  LR: 7.134e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.495 (1.495)  Loss:  0.7876 (0.7876)  Acc@1: 87.4023 (87.4023)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.136 (0.592)  Loss:  0.8813 (1.4243)  Acc@1: 83.7264 (71.9660)  Acc@5: 96.4623 (90.8240)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-217.pth.tar', 71.9660000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-216.pth.tar', 71.89799996337891)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-214.pth.tar', 71.88599999267578)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-187.pth.tar', 71.87000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-210.pth.tar', 71.71000009033203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-202.pth.tar', 71.62399994384765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-200.pth.tar', 71.55799999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-197.pth.tar', 71.51000004638672)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-212.pth.tar', 71.50599986572266)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-201.pth.tar', 71.4920000415039)

Train: 218 [   0/1251 (  0%)]  Loss: 3.958 (3.96)  Time: 2.173s,  471.28/s  (2.173s,  471.28/s)  LR: 7.111e-04  Data: 1.557 (1.557)
Train: 218 [  50/1251 (  4%)]  Loss: 3.778 (3.87)  Time: 0.706s, 1449.49/s  (0.734s, 1394.41/s)  LR: 7.111e-04  Data: 0.012 (0.047)
Train: 218 [ 100/1251 (  8%)]  Loss: 3.591 (3.78)  Time: 0.688s, 1488.08/s  (0.720s, 1422.20/s)  LR: 7.111e-04  Data: 0.013 (0.029)
Train: 218 [ 150/1251 ( 12%)]  Loss: 3.527 (3.71)  Time: 0.734s, 1395.25/s  (0.718s, 1426.53/s)  LR: 7.111e-04  Data: 0.015 (0.023)
Train: 218 [ 200/1251 ( 16%)]  Loss: 3.864 (3.74)  Time: 0.697s, 1468.47/s  (0.717s, 1427.63/s)  LR: 7.111e-04  Data: 0.013 (0.021)
Train: 218 [ 250/1251 ( 20%)]  Loss: 3.506 (3.70)  Time: 0.667s, 1534.91/s  (0.717s, 1428.90/s)  LR: 7.111e-04  Data: 0.009 (0.019)
Train: 218 [ 300/1251 ( 24%)]  Loss: 4.129 (3.76)  Time: 0.671s, 1525.13/s  (0.712s, 1437.96/s)  LR: 7.111e-04  Data: 0.010 (0.017)
Train: 218 [ 350/1251 ( 28%)]  Loss: 3.573 (3.74)  Time: 0.729s, 1405.56/s  (0.707s, 1448.61/s)  LR: 7.111e-04  Data: 0.009 (0.016)
Train: 218 [ 400/1251 ( 32%)]  Loss: 3.889 (3.76)  Time: 0.685s, 1494.34/s  (0.703s, 1456.31/s)  LR: 7.111e-04  Data: 0.010 (0.015)
Train: 218 [ 450/1251 ( 36%)]  Loss: 3.846 (3.77)  Time: 0.708s, 1445.71/s  (0.702s, 1458.93/s)  LR: 7.111e-04  Data: 0.010 (0.015)
Train: 218 [ 500/1251 ( 40%)]  Loss: 3.930 (3.78)  Time: 0.671s, 1525.97/s  (0.701s, 1461.03/s)  LR: 7.111e-04  Data: 0.010 (0.014)
Train: 218 [ 550/1251 ( 44%)]  Loss: 3.709 (3.78)  Time: 0.673s, 1522.58/s  (0.700s, 1463.17/s)  LR: 7.111e-04  Data: 0.009 (0.014)
Train: 218 [ 600/1251 ( 48%)]  Loss: 3.902 (3.78)  Time: 0.709s, 1444.77/s  (0.700s, 1462.55/s)  LR: 7.111e-04  Data: 0.011 (0.014)
Train: 218 [ 650/1251 ( 52%)]  Loss: 3.851 (3.79)  Time: 0.666s, 1538.13/s  (0.700s, 1463.54/s)  LR: 7.111e-04  Data: 0.011 (0.013)
Train: 218 [ 700/1251 ( 56%)]  Loss: 3.711 (3.78)  Time: 0.708s, 1447.15/s  (0.699s, 1465.64/s)  LR: 7.111e-04  Data: 0.011 (0.013)
Train: 218 [ 750/1251 ( 60%)]  Loss: 3.555 (3.77)  Time: 0.706s, 1450.88/s  (0.698s, 1466.48/s)  LR: 7.111e-04  Data: 0.009 (0.013)
Train: 218 [ 800/1251 ( 64%)]  Loss: 3.410 (3.75)  Time: 0.684s, 1497.90/s  (0.698s, 1467.80/s)  LR: 7.111e-04  Data: 0.011 (0.013)
Train: 218 [ 850/1251 ( 68%)]  Loss: 3.609 (3.74)  Time: 0.672s, 1524.70/s  (0.698s, 1467.68/s)  LR: 7.111e-04  Data: 0.009 (0.013)
Train: 218 [ 900/1251 ( 72%)]  Loss: 3.838 (3.75)  Time: 0.700s, 1462.61/s  (0.698s, 1467.45/s)  LR: 7.111e-04  Data: 0.013 (0.013)
Train: 218 [ 950/1251 ( 76%)]  Loss: 3.748 (3.75)  Time: 0.686s, 1492.22/s  (0.698s, 1467.30/s)  LR: 7.111e-04  Data: 0.015 (0.012)
Train: 218 [1000/1251 ( 80%)]  Loss: 3.812 (3.75)  Time: 0.676s, 1514.03/s  (0.698s, 1467.62/s)  LR: 7.111e-04  Data: 0.009 (0.012)
Train: 218 [1050/1251 ( 84%)]  Loss: 3.690 (3.75)  Time: 0.680s, 1505.08/s  (0.698s, 1468.05/s)  LR: 7.111e-04  Data: 0.011 (0.012)
Train: 218 [1100/1251 ( 88%)]  Loss: 3.739 (3.75)  Time: 0.704s, 1454.42/s  (0.697s, 1468.94/s)  LR: 7.111e-04  Data: 0.010 (0.012)
Train: 218 [1150/1251 ( 92%)]  Loss: 3.587 (3.74)  Time: 0.673s, 1522.64/s  (0.697s, 1468.79/s)  LR: 7.111e-04  Data: 0.010 (0.012)
Train: 218 [1200/1251 ( 96%)]  Loss: 3.633 (3.74)  Time: 0.672s, 1523.87/s  (0.697s, 1468.64/s)  LR: 7.111e-04  Data: 0.011 (0.012)
Train: 218 [1250/1251 (100%)]  Loss: 3.507 (3.73)  Time: 0.692s, 1479.84/s  (0.697s, 1469.00/s)  LR: 7.111e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.581 (1.581)  Loss:  1.0010 (1.0010)  Acc@1: 86.4258 (86.4258)  Acc@5: 96.2891 (96.2891)
Test: [  48/48]  Time: 0.136 (0.587)  Loss:  1.0283 (1.5370)  Acc@1: 83.6085 (71.2440)  Acc@5: 95.9906 (90.2520)
Train: 219 [   0/1251 (  0%)]  Loss: 3.634 (3.63)  Time: 2.327s,  440.04/s  (2.327s,  440.04/s)  LR: 7.087e-04  Data: 1.659 (1.659)
Train: 219 [  50/1251 (  4%)]  Loss: 3.739 (3.69)  Time: 0.674s, 1519.64/s  (0.730s, 1402.33/s)  LR: 7.087e-04  Data: 0.009 (0.049)
Train: 219 [ 100/1251 (  8%)]  Loss: 3.546 (3.64)  Time: 0.702s, 1457.66/s  (0.712s, 1437.85/s)  LR: 7.087e-04  Data: 0.009 (0.030)
Train: 219 [ 150/1251 ( 12%)]  Loss: 3.865 (3.70)  Time: 0.709s, 1445.16/s  (0.707s, 1448.48/s)  LR: 7.087e-04  Data: 0.009 (0.023)
Train: 219 [ 200/1251 ( 16%)]  Loss: 3.152 (3.59)  Time: 0.712s, 1437.35/s  (0.704s, 1455.39/s)  LR: 7.087e-04  Data: 0.010 (0.020)
Train: 219 [ 250/1251 ( 20%)]  Loss: 3.841 (3.63)  Time: 0.674s, 1520.40/s  (0.701s, 1459.98/s)  LR: 7.087e-04  Data: 0.010 (0.018)
Train: 219 [ 300/1251 ( 24%)]  Loss: 3.680 (3.64)  Time: 0.761s, 1345.73/s  (0.701s, 1461.27/s)  LR: 7.087e-04  Data: 0.009 (0.017)
Train: 219 [ 350/1251 ( 28%)]  Loss: 3.924 (3.67)  Time: 0.681s, 1504.12/s  (0.701s, 1461.40/s)  LR: 7.087e-04  Data: 0.012 (0.016)
Train: 219 [ 400/1251 ( 32%)]  Loss: 3.652 (3.67)  Time: 0.707s, 1448.24/s  (0.700s, 1463.11/s)  LR: 7.087e-04  Data: 0.009 (0.015)
Train: 219 [ 450/1251 ( 36%)]  Loss: 3.773 (3.68)  Time: 0.711s, 1439.75/s  (0.699s, 1464.60/s)  LR: 7.087e-04  Data: 0.010 (0.015)
Train: 219 [ 500/1251 ( 40%)]  Loss: 3.731 (3.69)  Time: 0.707s, 1448.07/s  (0.698s, 1467.43/s)  LR: 7.087e-04  Data: 0.009 (0.014)
Train: 219 [ 550/1251 ( 44%)]  Loss: 3.324 (3.66)  Time: 0.703s, 1457.32/s  (0.697s, 1468.51/s)  LR: 7.087e-04  Data: 0.009 (0.014)
Train: 219 [ 600/1251 ( 48%)]  Loss: 3.719 (3.66)  Time: 0.672s, 1524.36/s  (0.697s, 1469.73/s)  LR: 7.087e-04  Data: 0.010 (0.013)
Train: 219 [ 650/1251 ( 52%)]  Loss: 3.766 (3.67)  Time: 0.674s, 1518.70/s  (0.696s, 1470.64/s)  LR: 7.087e-04  Data: 0.013 (0.013)
Train: 219 [ 700/1251 ( 56%)]  Loss: 4.032 (3.69)  Time: 0.715s, 1431.82/s  (0.696s, 1471.39/s)  LR: 7.087e-04  Data: 0.009 (0.013)
Train: 219 [ 750/1251 ( 60%)]  Loss: 4.067 (3.72)  Time: 0.840s, 1219.62/s  (0.697s, 1469.86/s)  LR: 7.087e-04  Data: 0.011 (0.013)
Train: 219 [ 800/1251 ( 64%)]  Loss: 3.537 (3.70)  Time: 0.707s, 1448.50/s  (0.697s, 1468.69/s)  LR: 7.087e-04  Data: 0.010 (0.013)
Train: 219 [ 850/1251 ( 68%)]  Loss: 3.765 (3.71)  Time: 0.699s, 1464.18/s  (0.697s, 1468.23/s)  LR: 7.087e-04  Data: 0.009 (0.012)
Train: 219 [ 900/1251 ( 72%)]  Loss: 3.603 (3.70)  Time: 0.709s, 1443.88/s  (0.697s, 1468.51/s)  LR: 7.087e-04  Data: 0.010 (0.012)
Train: 219 [ 950/1251 ( 76%)]  Loss: 3.587 (3.70)  Time: 0.673s, 1521.64/s  (0.697s, 1469.49/s)  LR: 7.087e-04  Data: 0.011 (0.012)
Train: 219 [1000/1251 ( 80%)]  Loss: 3.739 (3.70)  Time: 0.670s, 1529.37/s  (0.697s, 1469.39/s)  LR: 7.087e-04  Data: 0.010 (0.012)
Train: 219 [1050/1251 ( 84%)]  Loss: 3.612 (3.69)  Time: 0.676s, 1515.77/s  (0.697s, 1469.91/s)  LR: 7.087e-04  Data: 0.009 (0.012)
Train: 219 [1100/1251 ( 88%)]  Loss: 3.824 (3.70)  Time: 0.684s, 1498.02/s  (0.696s, 1470.74/s)  LR: 7.087e-04  Data: 0.010 (0.012)
Train: 219 [1150/1251 ( 92%)]  Loss: 4.237 (3.72)  Time: 0.721s, 1420.75/s  (0.696s, 1470.57/s)  LR: 7.087e-04  Data: 0.009 (0.012)
Train: 219 [1200/1251 ( 96%)]  Loss: 3.936 (3.73)  Time: 0.676s, 1515.69/s  (0.696s, 1471.47/s)  LR: 7.087e-04  Data: 0.010 (0.012)
Train: 219 [1250/1251 (100%)]  Loss: 4.114 (3.75)  Time: 0.742s, 1380.85/s  (0.696s, 1471.53/s)  LR: 7.087e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.608 (1.608)  Loss:  0.8120 (0.8120)  Acc@1: 87.9883 (87.9883)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.136 (0.584)  Loss:  0.9727 (1.4277)  Acc@1: 83.8443 (71.9800)  Acc@5: 95.6368 (90.8420)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-219.pth.tar', 71.98000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-217.pth.tar', 71.9660000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-216.pth.tar', 71.89799996337891)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-214.pth.tar', 71.88599999267578)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-187.pth.tar', 71.87000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-210.pth.tar', 71.71000009033203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-202.pth.tar', 71.62399994384765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-200.pth.tar', 71.55799999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-197.pth.tar', 71.51000004638672)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-212.pth.tar', 71.50599986572266)

Train: 220 [   0/1251 (  0%)]  Loss: 3.526 (3.53)  Time: 2.239s,  457.38/s  (2.239s,  457.38/s)  LR: 7.063e-04  Data: 1.579 (1.579)
Train: 220 [  50/1251 (  4%)]  Loss: 4.080 (3.80)  Time: 0.722s, 1417.98/s  (0.732s, 1398.39/s)  LR: 7.063e-04  Data: 0.009 (0.047)
Train: 220 [ 100/1251 (  8%)]  Loss: 3.537 (3.71)  Time: 0.705s, 1452.43/s  (0.712s, 1437.50/s)  LR: 7.063e-04  Data: 0.010 (0.029)
Train: 220 [ 150/1251 ( 12%)]  Loss: 3.577 (3.68)  Time: 0.710s, 1442.84/s  (0.706s, 1450.55/s)  LR: 7.063e-04  Data: 0.010 (0.023)
Train: 220 [ 200/1251 ( 16%)]  Loss: 3.860 (3.72)  Time: 0.673s, 1521.24/s  (0.701s, 1460.52/s)  LR: 7.063e-04  Data: 0.009 (0.020)
Train: 220 [ 250/1251 ( 20%)]  Loss: 4.018 (3.77)  Time: 0.672s, 1523.39/s  (0.699s, 1464.61/s)  LR: 7.063e-04  Data: 0.010 (0.018)
Train: 220 [ 300/1251 ( 24%)]  Loss: 3.979 (3.80)  Time: 0.667s, 1534.86/s  (0.698s, 1467.98/s)  LR: 7.063e-04  Data: 0.010 (0.016)
Train: 220 [ 350/1251 ( 28%)]  Loss: 3.848 (3.80)  Time: 0.697s, 1470.03/s  (0.697s, 1468.34/s)  LR: 7.063e-04  Data: 0.012 (0.016)
Train: 220 [ 400/1251 ( 32%)]  Loss: 4.059 (3.83)  Time: 0.691s, 1481.07/s  (0.698s, 1467.61/s)  LR: 7.063e-04  Data: 0.012 (0.015)
Train: 220 [ 450/1251 ( 36%)]  Loss: 3.869 (3.84)  Time: 0.671s, 1526.83/s  (0.697s, 1468.39/s)  LR: 7.063e-04  Data: 0.014 (0.014)
Train: 220 [ 500/1251 ( 40%)]  Loss: 4.090 (3.86)  Time: 0.691s, 1482.82/s  (0.697s, 1469.44/s)  LR: 7.063e-04  Data: 0.013 (0.014)
Train: 220 [ 550/1251 ( 44%)]  Loss: 3.711 (3.85)  Time: 0.670s, 1528.82/s  (0.697s, 1470.19/s)  LR: 7.063e-04  Data: 0.009 (0.014)
Train: 220 [ 600/1251 ( 48%)]  Loss: 4.010 (3.86)  Time: 0.698s, 1466.51/s  (0.697s, 1469.93/s)  LR: 7.063e-04  Data: 0.010 (0.013)
Train: 220 [ 650/1251 ( 52%)]  Loss: 3.516 (3.83)  Time: 0.705s, 1451.72/s  (0.697s, 1469.76/s)  LR: 7.063e-04  Data: 0.011 (0.013)
Train: 220 [ 700/1251 ( 56%)]  Loss: 3.684 (3.82)  Time: 0.705s, 1452.90/s  (0.697s, 1470.15/s)  LR: 7.063e-04  Data: 0.010 (0.013)
Train: 220 [ 750/1251 ( 60%)]  Loss: 3.696 (3.82)  Time: 0.674s, 1519.81/s  (0.696s, 1471.92/s)  LR: 7.063e-04  Data: 0.010 (0.013)
Train: 220 [ 800/1251 ( 64%)]  Loss: 3.594 (3.80)  Time: 0.684s, 1496.40/s  (0.695s, 1473.65/s)  LR: 7.063e-04  Data: 0.013 (0.013)
Train: 220 [ 850/1251 ( 68%)]  Loss: 3.954 (3.81)  Time: 0.674s, 1518.30/s  (0.695s, 1473.61/s)  LR: 7.063e-04  Data: 0.009 (0.012)
Train: 220 [ 900/1251 ( 72%)]  Loss: 3.791 (3.81)  Time: 0.691s, 1482.96/s  (0.695s, 1473.92/s)  LR: 7.063e-04  Data: 0.009 (0.012)
Train: 220 [ 950/1251 ( 76%)]  Loss: 3.820 (3.81)  Time: 0.699s, 1464.94/s  (0.694s, 1474.56/s)  LR: 7.063e-04  Data: 0.011 (0.012)
Train: 220 [1000/1251 ( 80%)]  Loss: 3.809 (3.81)  Time: 0.704s, 1453.82/s  (0.694s, 1475.01/s)  LR: 7.063e-04  Data: 0.011 (0.012)
Train: 220 [1050/1251 ( 84%)]  Loss: 3.979 (3.82)  Time: 0.706s, 1450.37/s  (0.694s, 1474.75/s)  LR: 7.063e-04  Data: 0.011 (0.012)
Train: 220 [1100/1251 ( 88%)]  Loss: 3.583 (3.81)  Time: 0.673s, 1520.87/s  (0.695s, 1474.20/s)  LR: 7.063e-04  Data: 0.010 (0.012)
Train: 220 [1150/1251 ( 92%)]  Loss: 3.883 (3.81)  Time: 0.714s, 1433.37/s  (0.695s, 1474.25/s)  LR: 7.063e-04  Data: 0.009 (0.012)
Train: 220 [1200/1251 ( 96%)]  Loss: 4.112 (3.82)  Time: 0.706s, 1450.14/s  (0.694s, 1474.53/s)  LR: 7.063e-04  Data: 0.011 (0.012)
Train: 220 [1250/1251 (100%)]  Loss: 3.352 (3.81)  Time: 0.701s, 1461.49/s  (0.694s, 1474.99/s)  LR: 7.063e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.540 (1.540)  Loss:  0.8604 (0.8604)  Acc@1: 88.0859 (88.0859)  Acc@5: 96.2891 (96.2891)
Test: [  48/48]  Time: 0.136 (0.585)  Loss:  1.0137 (1.5273)  Acc@1: 83.1368 (72.0120)  Acc@5: 95.4009 (90.8180)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-220.pth.tar', 72.0120000439453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-219.pth.tar', 71.98000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-217.pth.tar', 71.9660000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-216.pth.tar', 71.89799996337891)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-214.pth.tar', 71.88599999267578)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-187.pth.tar', 71.87000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-210.pth.tar', 71.71000009033203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-202.pth.tar', 71.62399994384765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-200.pth.tar', 71.55799999511719)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-197.pth.tar', 71.51000004638672)

Train: 221 [   0/1251 (  0%)]  Loss: 3.805 (3.80)  Time: 2.410s,  424.96/s  (2.410s,  424.96/s)  LR: 7.040e-04  Data: 1.746 (1.746)
Train: 221 [  50/1251 (  4%)]  Loss: 3.557 (3.68)  Time: 0.706s, 1450.87/s  (0.738s, 1386.69/s)  LR: 7.040e-04  Data: 0.010 (0.047)
Train: 221 [ 100/1251 (  8%)]  Loss: 4.064 (3.81)  Time: 0.707s, 1447.95/s  (0.715s, 1431.60/s)  LR: 7.040e-04  Data: 0.009 (0.029)
Train: 221 [ 150/1251 ( 12%)]  Loss: 3.635 (3.76)  Time: 0.679s, 1507.86/s  (0.707s, 1447.49/s)  LR: 7.040e-04  Data: 0.013 (0.023)
Train: 221 [ 200/1251 ( 16%)]  Loss: 3.644 (3.74)  Time: 0.687s, 1490.59/s  (0.704s, 1453.72/s)  LR: 7.040e-04  Data: 0.016 (0.020)
Train: 221 [ 250/1251 ( 20%)]  Loss: 3.564 (3.71)  Time: 0.679s, 1508.27/s  (0.702s, 1458.46/s)  LR: 7.040e-04  Data: 0.010 (0.018)
Train: 221 [ 300/1251 ( 24%)]  Loss: 3.489 (3.68)  Time: 0.705s, 1453.39/s  (0.701s, 1461.45/s)  LR: 7.040e-04  Data: 0.011 (0.017)
Train: 221 [ 350/1251 ( 28%)]  Loss: 3.340 (3.64)  Time: 0.702s, 1458.24/s  (0.700s, 1461.92/s)  LR: 7.040e-04  Data: 0.009 (0.016)
Train: 221 [ 400/1251 ( 32%)]  Loss: 3.720 (3.65)  Time: 0.672s, 1523.40/s  (0.698s, 1466.58/s)  LR: 7.040e-04  Data: 0.010 (0.015)
Train: 221 [ 450/1251 ( 36%)]  Loss: 3.717 (3.65)  Time: 0.669s, 1531.63/s  (0.698s, 1466.30/s)  LR: 7.040e-04  Data: 0.012 (0.014)
Train: 221 [ 500/1251 ( 40%)]  Loss: 3.888 (3.67)  Time: 0.715s, 1432.27/s  (0.699s, 1465.84/s)  LR: 7.040e-04  Data: 0.010 (0.014)
Train: 221 [ 550/1251 ( 44%)]  Loss: 3.507 (3.66)  Time: 0.704s, 1453.61/s  (0.698s, 1466.72/s)  LR: 7.040e-04  Data: 0.010 (0.014)
Train: 221 [ 600/1251 ( 48%)]  Loss: 3.856 (3.68)  Time: 0.699s, 1464.84/s  (0.698s, 1468.01/s)  LR: 7.040e-04  Data: 0.009 (0.013)
Train: 221 [ 650/1251 ( 52%)]  Loss: 3.665 (3.67)  Time: 0.715s, 1431.22/s  (0.698s, 1467.06/s)  LR: 7.040e-04  Data: 0.010 (0.013)
Train: 221 [ 700/1251 ( 56%)]  Loss: 3.377 (3.66)  Time: 0.681s, 1503.61/s  (0.698s, 1466.68/s)  LR: 7.040e-04  Data: 0.010 (0.013)
Train: 221 [ 750/1251 ( 60%)]  Loss: 3.625 (3.65)  Time: 0.703s, 1455.76/s  (0.698s, 1468.05/s)  LR: 7.040e-04  Data: 0.009 (0.013)
Train: 221 [ 800/1251 ( 64%)]  Loss: 3.990 (3.67)  Time: 0.668s, 1533.32/s  (0.697s, 1468.43/s)  LR: 7.040e-04  Data: 0.010 (0.013)
Train: 221 [ 850/1251 ( 68%)]  Loss: 4.093 (3.70)  Time: 0.706s, 1451.02/s  (0.697s, 1469.53/s)  LR: 7.040e-04  Data: 0.009 (0.013)
Train: 221 [ 900/1251 ( 72%)]  Loss: 4.164 (3.72)  Time: 0.684s, 1496.31/s  (0.696s, 1470.46/s)  LR: 7.040e-04  Data: 0.011 (0.012)
Train: 221 [ 950/1251 ( 76%)]  Loss: 3.832 (3.73)  Time: 0.679s, 1508.43/s  (0.696s, 1470.83/s)  LR: 7.040e-04  Data: 0.009 (0.012)
Train: 221 [1000/1251 ( 80%)]  Loss: 3.503 (3.72)  Time: 0.705s, 1452.96/s  (0.696s, 1471.40/s)  LR: 7.040e-04  Data: 0.009 (0.012)
Train: 221 [1050/1251 ( 84%)]  Loss: 3.912 (3.72)  Time: 0.671s, 1527.04/s  (0.696s, 1471.22/s)  LR: 7.040e-04  Data: 0.010 (0.012)
Train: 221 [1100/1251 ( 88%)]  Loss: 3.518 (3.72)  Time: 0.705s, 1452.55/s  (0.696s, 1471.61/s)  LR: 7.040e-04  Data: 0.011 (0.012)
Train: 221 [1150/1251 ( 92%)]  Loss: 3.695 (3.71)  Time: 0.706s, 1450.00/s  (0.696s, 1471.35/s)  LR: 7.040e-04  Data: 0.012 (0.012)
Train: 221 [1200/1251 ( 96%)]  Loss: 3.893 (3.72)  Time: 0.674s, 1520.23/s  (0.696s, 1471.93/s)  LR: 7.040e-04  Data: 0.010 (0.012)
Train: 221 [1250/1251 (100%)]  Loss: 3.424 (3.71)  Time: 0.693s, 1478.45/s  (0.695s, 1472.52/s)  LR: 7.040e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.499 (1.499)  Loss:  0.8613 (0.8613)  Acc@1: 89.0625 (89.0625)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.136 (0.591)  Loss:  1.0029 (1.5209)  Acc@1: 81.8396 (71.7600)  Acc@5: 94.8113 (90.7420)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-220.pth.tar', 72.0120000439453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-219.pth.tar', 71.98000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-217.pth.tar', 71.9660000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-216.pth.tar', 71.89799996337891)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-214.pth.tar', 71.88599999267578)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-187.pth.tar', 71.87000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-221.pth.tar', 71.7599999975586)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-210.pth.tar', 71.71000009033203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-202.pth.tar', 71.62399994384765)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-200.pth.tar', 71.55799999511719)

Train: 222 [   0/1251 (  0%)]  Loss: 4.046 (4.05)  Time: 2.178s,  470.07/s  (2.178s,  470.07/s)  LR: 7.016e-04  Data: 1.562 (1.562)
Train: 222 [  50/1251 (  4%)]  Loss: 3.394 (3.72)  Time: 0.674s, 1518.64/s  (0.725s, 1412.11/s)  LR: 7.016e-04  Data: 0.010 (0.049)
Train: 222 [ 100/1251 (  8%)]  Loss: 4.171 (3.87)  Time: 0.716s, 1429.43/s  (0.712s, 1439.16/s)  LR: 7.016e-04  Data: 0.009 (0.030)
Train: 222 [ 150/1251 ( 12%)]  Loss: 3.678 (3.82)  Time: 0.716s, 1429.87/s  (0.705s, 1453.49/s)  LR: 7.016e-04  Data: 0.011 (0.023)
Train: 222 [ 200/1251 ( 16%)]  Loss: 3.474 (3.75)  Time: 0.709s, 1444.66/s  (0.704s, 1454.16/s)  LR: 7.016e-04  Data: 0.011 (0.020)
Train: 222 [ 250/1251 ( 20%)]  Loss: 3.293 (3.68)  Time: 0.683s, 1499.96/s  (0.702s, 1459.08/s)  LR: 7.016e-04  Data: 0.009 (0.018)
Train: 222 [ 300/1251 ( 24%)]  Loss: 3.868 (3.70)  Time: 0.674s, 1518.66/s  (0.699s, 1464.82/s)  LR: 7.016e-04  Data: 0.011 (0.017)
Train: 222 [ 350/1251 ( 28%)]  Loss: 3.721 (3.71)  Time: 0.712s, 1438.69/s  (0.699s, 1464.99/s)  LR: 7.016e-04  Data: 0.010 (0.016)
Train: 222 [ 400/1251 ( 32%)]  Loss: 3.759 (3.71)  Time: 0.700s, 1463.79/s  (0.698s, 1467.16/s)  LR: 7.016e-04  Data: 0.009 (0.015)
Train: 222 [ 450/1251 ( 36%)]  Loss: 3.557 (3.70)  Time: 0.710s, 1442.16/s  (0.697s, 1469.31/s)  LR: 7.016e-04  Data: 0.008 (0.015)
Train: 222 [ 500/1251 ( 40%)]  Loss: 3.882 (3.71)  Time: 0.704s, 1453.61/s  (0.696s, 1470.46/s)  LR: 7.016e-04  Data: 0.009 (0.014)
Train: 222 [ 550/1251 ( 44%)]  Loss: 3.784 (3.72)  Time: 0.672s, 1524.36/s  (0.696s, 1471.43/s)  LR: 7.016e-04  Data: 0.011 (0.014)
Train: 222 [ 600/1251 ( 48%)]  Loss: 4.149 (3.75)  Time: 0.762s, 1343.89/s  (0.696s, 1471.58/s)  LR: 7.016e-04  Data: 0.028 (0.013)
Train: 222 [ 650/1251 ( 52%)]  Loss: 3.773 (3.75)  Time: 0.671s, 1526.12/s  (0.696s, 1471.76/s)  LR: 7.016e-04  Data: 0.009 (0.013)
Train: 222 [ 700/1251 ( 56%)]  Loss: 3.730 (3.75)  Time: 0.721s, 1420.04/s  (0.697s, 1470.10/s)  LR: 7.016e-04  Data: 0.012 (0.013)
Train: 222 [ 750/1251 ( 60%)]  Loss: 3.264 (3.72)  Time: 0.673s, 1521.81/s  (0.696s, 1470.71/s)  LR: 7.016e-04  Data: 0.015 (0.013)
Train: 222 [ 800/1251 ( 64%)]  Loss: 3.812 (3.73)  Time: 0.677s, 1512.27/s  (0.696s, 1471.28/s)  LR: 7.016e-04  Data: 0.010 (0.013)
Train: 222 [ 850/1251 ( 68%)]  Loss: 3.627 (3.72)  Time: 0.678s, 1511.42/s  (0.696s, 1471.96/s)  LR: 7.016e-04  Data: 0.015 (0.012)
Train: 222 [ 900/1251 ( 72%)]  Loss: 3.721 (3.72)  Time: 0.717s, 1428.86/s  (0.696s, 1471.88/s)  LR: 7.016e-04  Data: 0.014 (0.012)
Train: 222 [ 950/1251 ( 76%)]  Loss: 3.777 (3.72)  Time: 0.672s, 1523.38/s  (0.696s, 1470.83/s)  LR: 7.016e-04  Data: 0.012 (0.012)
Train: 222 [1000/1251 ( 80%)]  Loss: 3.748 (3.73)  Time: 0.711s, 1439.35/s  (0.696s, 1471.74/s)  LR: 7.016e-04  Data: 0.010 (0.012)
Train: 222 [1050/1251 ( 84%)]  Loss: 3.632 (3.72)  Time: 0.673s, 1521.57/s  (0.696s, 1472.08/s)  LR: 7.016e-04  Data: 0.011 (0.012)
Train: 222 [1100/1251 ( 88%)]  Loss: 3.613 (3.72)  Time: 0.704s, 1454.80/s  (0.696s, 1472.17/s)  LR: 7.016e-04  Data: 0.009 (0.012)
Train: 222 [1150/1251 ( 92%)]  Loss: 3.665 (3.71)  Time: 0.676s, 1515.88/s  (0.695s, 1473.14/s)  LR: 7.016e-04  Data: 0.010 (0.012)
Train: 222 [1200/1251 ( 96%)]  Loss: 3.376 (3.70)  Time: 0.676s, 1514.90/s  (0.695s, 1473.23/s)  LR: 7.016e-04  Data: 0.009 (0.012)
Train: 222 [1250/1251 (100%)]  Loss: 3.983 (3.71)  Time: 0.653s, 1568.65/s  (0.695s, 1473.46/s)  LR: 7.016e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.550 (1.550)  Loss:  0.8228 (0.8228)  Acc@1: 87.4023 (87.4023)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.141 (0.582)  Loss:  0.9541 (1.4678)  Acc@1: 81.6038 (71.5220)  Acc@5: 95.4009 (90.6960)
Train: 223 [   0/1251 (  0%)]  Loss: 3.967 (3.97)  Time: 2.311s,  443.13/s  (2.311s,  443.13/s)  LR: 6.992e-04  Data: 1.695 (1.695)
Train: 223 [  50/1251 (  4%)]  Loss: 3.436 (3.70)  Time: 0.704s, 1455.43/s  (0.728s, 1407.30/s)  LR: 6.992e-04  Data: 0.009 (0.050)
Train: 223 [ 100/1251 (  8%)]  Loss: 3.922 (3.77)  Time: 0.699s, 1465.34/s  (0.711s, 1439.86/s)  LR: 6.992e-04  Data: 0.010 (0.030)
Train: 223 [ 150/1251 ( 12%)]  Loss: 3.982 (3.83)  Time: 0.671s, 1526.49/s  (0.703s, 1457.11/s)  LR: 6.992e-04  Data: 0.009 (0.024)
Train: 223 [ 200/1251 ( 16%)]  Loss: 3.461 (3.75)  Time: 0.679s, 1508.97/s  (0.700s, 1463.46/s)  LR: 6.992e-04  Data: 0.009 (0.020)
Train: 223 [ 250/1251 ( 20%)]  Loss: 3.966 (3.79)  Time: 0.672s, 1523.29/s  (0.698s, 1467.39/s)  LR: 6.992e-04  Data: 0.010 (0.018)
Train: 223 [ 300/1251 ( 24%)]  Loss: 3.811 (3.79)  Time: 0.664s, 1542.06/s  (0.696s, 1471.48/s)  LR: 6.992e-04  Data: 0.008 (0.017)
Train: 223 [ 350/1251 ( 28%)]  Loss: 3.733 (3.78)  Time: 0.671s, 1526.26/s  (0.695s, 1473.41/s)  LR: 6.992e-04  Data: 0.010 (0.016)
Train: 223 [ 400/1251 ( 32%)]  Loss: 3.794 (3.79)  Time: 0.679s, 1507.66/s  (0.696s, 1471.91/s)  LR: 6.992e-04  Data: 0.009 (0.015)
Train: 223 [ 450/1251 ( 36%)]  Loss: 3.450 (3.75)  Time: 0.696s, 1471.42/s  (0.696s, 1471.48/s)  LR: 6.992e-04  Data: 0.011 (0.015)
Train: 223 [ 500/1251 ( 40%)]  Loss: 3.481 (3.73)  Time: 0.674s, 1520.31/s  (0.695s, 1473.66/s)  LR: 6.992e-04  Data: 0.011 (0.014)
Train: 223 [ 550/1251 ( 44%)]  Loss: 3.078 (3.67)  Time: 0.672s, 1524.71/s  (0.694s, 1474.45/s)  LR: 6.992e-04  Data: 0.010 (0.014)
Train: 223 [ 600/1251 ( 48%)]  Loss: 3.676 (3.67)  Time: 0.709s, 1444.80/s  (0.694s, 1475.18/s)  LR: 6.992e-04  Data: 0.009 (0.013)
Train: 223 [ 650/1251 ( 52%)]  Loss: 3.376 (3.65)  Time: 0.674s, 1519.44/s  (0.695s, 1473.75/s)  LR: 6.992e-04  Data: 0.012 (0.013)
Train: 223 [ 700/1251 ( 56%)]  Loss: 3.762 (3.66)  Time: 0.711s, 1439.88/s  (0.695s, 1472.94/s)  LR: 6.992e-04  Data: 0.009 (0.013)
Train: 223 [ 750/1251 ( 60%)]  Loss: 3.426 (3.64)  Time: 0.793s, 1290.86/s  (0.695s, 1473.05/s)  LR: 6.992e-04  Data: 0.011 (0.013)
Train: 223 [ 800/1251 ( 64%)]  Loss: 3.743 (3.65)  Time: 0.746s, 1371.80/s  (0.696s, 1471.45/s)  LR: 6.992e-04  Data: 0.009 (0.013)
Train: 223 [ 850/1251 ( 68%)]  Loss: 3.817 (3.66)  Time: 0.723s, 1415.86/s  (0.695s, 1472.76/s)  LR: 6.992e-04  Data: 0.009 (0.012)
Train: 223 [ 900/1251 ( 72%)]  Loss: 3.863 (3.67)  Time: 0.693s, 1477.09/s  (0.695s, 1473.80/s)  LR: 6.992e-04  Data: 0.009 (0.012)
Train: 223 [ 950/1251 ( 76%)]  Loss: 3.941 (3.68)  Time: 0.672s, 1524.41/s  (0.695s, 1474.31/s)  LR: 6.992e-04  Data: 0.009 (0.012)
Train: 223 [1000/1251 ( 80%)]  Loss: 3.684 (3.68)  Time: 0.705s, 1452.68/s  (0.694s, 1474.83/s)  LR: 6.992e-04  Data: 0.011 (0.012)
Train: 223 [1050/1251 ( 84%)]  Loss: 3.513 (3.68)  Time: 0.798s, 1283.97/s  (0.694s, 1475.06/s)  LR: 6.992e-04  Data: 0.009 (0.012)
Train: 223 [1100/1251 ( 88%)]  Loss: 3.761 (3.68)  Time: 0.667s, 1535.83/s  (0.694s, 1474.86/s)  LR: 6.992e-04  Data: 0.011 (0.012)
Train: 223 [1150/1251 ( 92%)]  Loss: 3.178 (3.66)  Time: 0.682s, 1500.60/s  (0.694s, 1475.35/s)  LR: 6.992e-04  Data: 0.010 (0.012)
Train: 223 [1200/1251 ( 96%)]  Loss: 3.542 (3.65)  Time: 0.683s, 1500.03/s  (0.694s, 1475.42/s)  LR: 6.992e-04  Data: 0.009 (0.012)
Train: 223 [1250/1251 (100%)]  Loss: 3.900 (3.66)  Time: 0.658s, 1555.63/s  (0.694s, 1475.81/s)  LR: 6.992e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.545 (1.545)  Loss:  0.8247 (0.8247)  Acc@1: 87.6953 (87.6953)  Acc@5: 95.8984 (95.8984)
Test: [  48/48]  Time: 0.136 (0.580)  Loss:  0.9688 (1.4641)  Acc@1: 83.8443 (71.7500)  Acc@5: 94.8113 (90.4940)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-220.pth.tar', 72.0120000439453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-219.pth.tar', 71.98000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-217.pth.tar', 71.9660000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-216.pth.tar', 71.89799996337891)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-214.pth.tar', 71.88599999267578)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-187.pth.tar', 71.87000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-221.pth.tar', 71.7599999975586)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-223.pth.tar', 71.75000009277343)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-210.pth.tar', 71.71000009033203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-202.pth.tar', 71.62399994384765)

Train: 224 [   0/1251 (  0%)]  Loss: 3.849 (3.85)  Time: 2.298s,  445.62/s  (2.298s,  445.62/s)  LR: 6.968e-04  Data: 1.665 (1.665)
Train: 224 [  50/1251 (  4%)]  Loss: 3.706 (3.78)  Time: 0.710s, 1442.30/s  (0.730s, 1403.17/s)  LR: 6.968e-04  Data: 0.010 (0.051)
Train: 224 [ 100/1251 (  8%)]  Loss: 3.904 (3.82)  Time: 0.681s, 1503.17/s  (0.711s, 1441.17/s)  LR: 6.968e-04  Data: 0.011 (0.031)
Train: 224 [ 150/1251 ( 12%)]  Loss: 3.548 (3.75)  Time: 0.726s, 1409.60/s  (0.708s, 1445.80/s)  LR: 6.968e-04  Data: 0.009 (0.024)
Train: 224 [ 200/1251 ( 16%)]  Loss: 3.741 (3.75)  Time: 0.678s, 1510.60/s  (0.707s, 1448.50/s)  LR: 6.968e-04  Data: 0.010 (0.021)
Train: 224 [ 250/1251 ( 20%)]  Loss: 3.487 (3.71)  Time: 0.675s, 1517.64/s  (0.704s, 1454.21/s)  LR: 6.968e-04  Data: 0.011 (0.019)
Train: 224 [ 300/1251 ( 24%)]  Loss: 4.104 (3.76)  Time: 0.739s, 1384.85/s  (0.702s, 1457.83/s)  LR: 6.968e-04  Data: 0.010 (0.017)
Train: 224 [ 350/1251 ( 28%)]  Loss: 3.978 (3.79)  Time: 0.699s, 1465.40/s  (0.701s, 1460.35/s)  LR: 6.968e-04  Data: 0.011 (0.016)
Train: 224 [ 400/1251 ( 32%)]  Loss: 3.761 (3.79)  Time: 0.713s, 1436.67/s  (0.700s, 1463.50/s)  LR: 6.968e-04  Data: 0.012 (0.016)
Train: 224 [ 450/1251 ( 36%)]  Loss: 3.918 (3.80)  Time: 0.731s, 1400.62/s  (0.698s, 1466.17/s)  LR: 6.968e-04  Data: 0.009 (0.015)
Train: 224 [ 500/1251 ( 40%)]  Loss: 3.732 (3.79)  Time: 0.722s, 1418.74/s  (0.698s, 1467.06/s)  LR: 6.968e-04  Data: 0.009 (0.015)
Train: 224 [ 550/1251 ( 44%)]  Loss: 3.880 (3.80)  Time: 0.704s, 1455.33/s  (0.698s, 1467.00/s)  LR: 6.968e-04  Data: 0.010 (0.014)
Train: 224 [ 600/1251 ( 48%)]  Loss: 3.929 (3.81)  Time: 0.694s, 1474.90/s  (0.697s, 1468.30/s)  LR: 6.968e-04  Data: 0.011 (0.014)
Train: 224 [ 650/1251 ( 52%)]  Loss: 3.672 (3.80)  Time: 0.674s, 1519.68/s  (0.697s, 1469.21/s)  LR: 6.968e-04  Data: 0.010 (0.014)
Train: 224 [ 700/1251 ( 56%)]  Loss: 4.023 (3.82)  Time: 0.676s, 1514.44/s  (0.697s, 1469.61/s)  LR: 6.968e-04  Data: 0.010 (0.013)
Train: 224 [ 750/1251 ( 60%)]  Loss: 3.568 (3.80)  Time: 0.677s, 1513.46/s  (0.696s, 1470.73/s)  LR: 6.968e-04  Data: 0.010 (0.013)
Train: 224 [ 800/1251 ( 64%)]  Loss: 3.598 (3.79)  Time: 0.673s, 1521.14/s  (0.696s, 1470.55/s)  LR: 6.968e-04  Data: 0.010 (0.013)
Train: 224 [ 850/1251 ( 68%)]  Loss: 3.593 (3.78)  Time: 0.713s, 1436.04/s  (0.696s, 1471.77/s)  LR: 6.968e-04  Data: 0.009 (0.013)
Train: 224 [ 900/1251 ( 72%)]  Loss: 3.629 (3.77)  Time: 0.711s, 1440.89/s  (0.696s, 1471.47/s)  LR: 6.968e-04  Data: 0.014 (0.013)
Train: 224 [ 950/1251 ( 76%)]  Loss: 3.620 (3.76)  Time: 0.674s, 1518.72/s  (0.696s, 1471.76/s)  LR: 6.968e-04  Data: 0.010 (0.013)
Train: 224 [1000/1251 ( 80%)]  Loss: 3.930 (3.77)  Time: 0.698s, 1466.30/s  (0.696s, 1472.28/s)  LR: 6.968e-04  Data: 0.009 (0.012)
Train: 224 [1050/1251 ( 84%)]  Loss: 3.789 (3.77)  Time: 0.669s, 1529.84/s  (0.695s, 1472.59/s)  LR: 6.968e-04  Data: 0.010 (0.012)
Train: 224 [1100/1251 ( 88%)]  Loss: 3.809 (3.77)  Time: 0.670s, 1528.36/s  (0.695s, 1473.53/s)  LR: 6.968e-04  Data: 0.010 (0.012)
Train: 224 [1150/1251 ( 92%)]  Loss: 3.613 (3.77)  Time: 0.700s, 1463.07/s  (0.695s, 1473.98/s)  LR: 6.968e-04  Data: 0.011 (0.012)
Train: 224 [1200/1251 ( 96%)]  Loss: 3.414 (3.75)  Time: 0.678s, 1510.27/s  (0.695s, 1473.94/s)  LR: 6.968e-04  Data: 0.011 (0.012)
Train: 224 [1250/1251 (100%)]  Loss: 4.232 (3.77)  Time: 0.656s, 1562.02/s  (0.694s, 1474.71/s)  LR: 6.968e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.590 (1.590)  Loss:  0.8320 (0.8320)  Acc@1: 89.8438 (89.8438)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.136 (0.573)  Loss:  0.9287 (1.5190)  Acc@1: 84.4340 (71.9740)  Acc@5: 96.6981 (90.8580)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-220.pth.tar', 72.0120000439453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-219.pth.tar', 71.98000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-224.pth.tar', 71.97400009033203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-217.pth.tar', 71.9660000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-216.pth.tar', 71.89799996337891)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-214.pth.tar', 71.88599999267578)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-187.pth.tar', 71.87000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-221.pth.tar', 71.7599999975586)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-223.pth.tar', 71.75000009277343)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-210.pth.tar', 71.71000009033203)

Train: 225 [   0/1251 (  0%)]  Loss: 3.607 (3.61)  Time: 2.371s,  431.80/s  (2.371s,  431.80/s)  LR: 6.944e-04  Data: 1.755 (1.755)
Train: 225 [  50/1251 (  4%)]  Loss: 3.550 (3.58)  Time: 0.671s, 1526.55/s  (0.738s, 1387.59/s)  LR: 6.944e-04  Data: 0.010 (0.047)
Train: 225 [ 100/1251 (  8%)]  Loss: 3.684 (3.61)  Time: 0.706s, 1450.80/s  (0.719s, 1423.50/s)  LR: 6.944e-04  Data: 0.009 (0.029)
Train: 225 [ 150/1251 ( 12%)]  Loss: 3.808 (3.66)  Time: 0.700s, 1463.01/s  (0.713s, 1435.83/s)  LR: 6.944e-04  Data: 0.009 (0.023)
Train: 225 [ 200/1251 ( 16%)]  Loss: 4.014 (3.73)  Time: 0.685s, 1494.15/s  (0.709s, 1443.87/s)  LR: 6.944e-04  Data: 0.008 (0.020)
Train: 225 [ 250/1251 ( 20%)]  Loss: 3.474 (3.69)  Time: 0.710s, 1441.65/s  (0.705s, 1452.92/s)  LR: 6.944e-04  Data: 0.010 (0.018)
Train: 225 [ 300/1251 ( 24%)]  Loss: 3.830 (3.71)  Time: 0.836s, 1224.70/s  (0.704s, 1454.68/s)  LR: 6.944e-04  Data: 0.010 (0.017)
Train: 225 [ 350/1251 ( 28%)]  Loss: 3.674 (3.71)  Time: 0.707s, 1448.27/s  (0.703s, 1455.75/s)  LR: 6.944e-04  Data: 0.011 (0.016)
Train: 225 [ 400/1251 ( 32%)]  Loss: 3.991 (3.74)  Time: 0.702s, 1458.45/s  (0.702s, 1459.26/s)  LR: 6.944e-04  Data: 0.010 (0.015)
Train: 225 [ 450/1251 ( 36%)]  Loss: 3.802 (3.74)  Time: 0.707s, 1447.79/s  (0.701s, 1459.93/s)  LR: 6.944e-04  Data: 0.009 (0.015)
Train: 225 [ 500/1251 ( 40%)]  Loss: 3.926 (3.76)  Time: 0.677s, 1512.81/s  (0.701s, 1461.72/s)  LR: 6.944e-04  Data: 0.010 (0.014)
Train: 225 [ 550/1251 ( 44%)]  Loss: 3.909 (3.77)  Time: 0.672s, 1523.84/s  (0.699s, 1464.34/s)  LR: 6.944e-04  Data: 0.009 (0.014)
Train: 225 [ 600/1251 ( 48%)]  Loss: 3.858 (3.78)  Time: 0.720s, 1421.91/s  (0.699s, 1465.91/s)  LR: 6.944e-04  Data: 0.009 (0.014)
Train: 225 [ 650/1251 ( 52%)]  Loss: 3.636 (3.77)  Time: 0.726s, 1409.90/s  (0.699s, 1465.37/s)  LR: 6.944e-04  Data: 0.014 (0.013)
Train: 225 [ 700/1251 ( 56%)]  Loss: 3.974 (3.78)  Time: 0.707s, 1449.06/s  (0.698s, 1466.05/s)  LR: 6.944e-04  Data: 0.009 (0.013)
Train: 225 [ 750/1251 ( 60%)]  Loss: 3.387 (3.76)  Time: 0.716s, 1429.42/s  (0.699s, 1464.91/s)  LR: 6.944e-04  Data: 0.008 (0.013)
Train: 225 [ 800/1251 ( 64%)]  Loss: 3.429 (3.74)  Time: 0.715s, 1432.33/s  (0.699s, 1465.35/s)  LR: 6.944e-04  Data: 0.009 (0.013)
Train: 225 [ 850/1251 ( 68%)]  Loss: 3.985 (3.75)  Time: 0.680s, 1505.57/s  (0.698s, 1466.62/s)  LR: 6.944e-04  Data: 0.010 (0.013)
Train: 225 [ 900/1251 ( 72%)]  Loss: 3.700 (3.75)  Time: 0.668s, 1533.47/s  (0.698s, 1467.30/s)  LR: 6.944e-04  Data: 0.009 (0.012)
Train: 225 [ 950/1251 ( 76%)]  Loss: 3.760 (3.75)  Time: 0.666s, 1538.33/s  (0.698s, 1467.47/s)  LR: 6.944e-04  Data: 0.011 (0.012)
Train: 225 [1000/1251 ( 80%)]  Loss: 3.986 (3.76)  Time: 0.734s, 1394.23/s  (0.697s, 1468.22/s)  LR: 6.944e-04  Data: 0.008 (0.012)
Train: 225 [1050/1251 ( 84%)]  Loss: 3.915 (3.77)  Time: 0.686s, 1492.19/s  (0.697s, 1468.57/s)  LR: 6.944e-04  Data: 0.010 (0.012)
Train: 225 [1100/1251 ( 88%)]  Loss: 3.901 (3.77)  Time: 0.694s, 1475.64/s  (0.697s, 1468.50/s)  LR: 6.944e-04  Data: 0.009 (0.012)
Train: 225 [1150/1251 ( 92%)]  Loss: 3.710 (3.77)  Time: 0.698s, 1466.44/s  (0.697s, 1469.18/s)  LR: 6.944e-04  Data: 0.010 (0.012)
Train: 225 [1200/1251 ( 96%)]  Loss: 3.954 (3.78)  Time: 0.671s, 1525.31/s  (0.697s, 1470.14/s)  LR: 6.944e-04  Data: 0.010 (0.012)
Train: 225 [1250/1251 (100%)]  Loss: 3.766 (3.78)  Time: 0.658s, 1556.18/s  (0.696s, 1470.91/s)  LR: 6.944e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.510 (1.510)  Loss:  0.7905 (0.7905)  Acc@1: 87.5000 (87.5000)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.136 (0.584)  Loss:  0.9580 (1.4660)  Acc@1: 83.2547 (71.5960)  Acc@5: 96.9340 (90.5300)
Train: 226 [   0/1251 (  0%)]  Loss: 3.581 (3.58)  Time: 2.372s,  431.64/s  (2.372s,  431.64/s)  LR: 6.920e-04  Data: 1.739 (1.739)
Train: 226 [  50/1251 (  4%)]  Loss: 3.942 (3.76)  Time: 0.695s, 1473.42/s  (0.734s, 1395.38/s)  LR: 6.920e-04  Data: 0.012 (0.050)
Train: 226 [ 100/1251 (  8%)]  Loss: 3.809 (3.78)  Time: 0.707s, 1449.01/s  (0.712s, 1437.23/s)  LR: 6.920e-04  Data: 0.010 (0.030)
Train: 226 [ 150/1251 ( 12%)]  Loss: 3.909 (3.81)  Time: 0.671s, 1526.56/s  (0.707s, 1448.93/s)  LR: 6.920e-04  Data: 0.010 (0.024)
Train: 226 [ 200/1251 ( 16%)]  Loss: 3.929 (3.83)  Time: 0.705s, 1451.86/s  (0.702s, 1458.07/s)  LR: 6.920e-04  Data: 0.009 (0.020)
Train: 226 [ 250/1251 ( 20%)]  Loss: 3.842 (3.84)  Time: 0.665s, 1540.36/s  (0.702s, 1457.73/s)  LR: 6.920e-04  Data: 0.010 (0.018)
Train: 226 [ 300/1251 ( 24%)]  Loss: 3.636 (3.81)  Time: 0.700s, 1462.60/s  (0.702s, 1458.78/s)  LR: 6.920e-04  Data: 0.009 (0.017)
Train: 226 [ 350/1251 ( 28%)]  Loss: 3.963 (3.83)  Time: 0.730s, 1402.44/s  (0.700s, 1463.21/s)  LR: 6.920e-04  Data: 0.010 (0.016)
Train: 226 [ 400/1251 ( 32%)]  Loss: 3.795 (3.82)  Time: 0.718s, 1426.30/s  (0.699s, 1465.03/s)  LR: 6.920e-04  Data: 0.009 (0.015)
Train: 226 [ 450/1251 ( 36%)]  Loss: 3.513 (3.79)  Time: 0.689s, 1486.27/s  (0.698s, 1467.34/s)  LR: 6.920e-04  Data: 0.009 (0.015)
Train: 226 [ 500/1251 ( 40%)]  Loss: 3.710 (3.78)  Time: 0.704s, 1455.41/s  (0.698s, 1467.58/s)  LR: 6.920e-04  Data: 0.009 (0.014)
Train: 226 [ 550/1251 ( 44%)]  Loss: 3.707 (3.78)  Time: 0.704s, 1453.96/s  (0.698s, 1467.74/s)  LR: 6.920e-04  Data: 0.010 (0.014)
Train: 226 [ 600/1251 ( 48%)]  Loss: 3.629 (3.77)  Time: 0.723s, 1416.88/s  (0.698s, 1467.27/s)  LR: 6.920e-04  Data: 0.016 (0.014)
Train: 226 [ 650/1251 ( 52%)]  Loss: 3.741 (3.76)  Time: 0.680s, 1504.96/s  (0.698s, 1467.90/s)  LR: 6.920e-04  Data: 0.011 (0.013)
Train: 226 [ 700/1251 ( 56%)]  Loss: 3.619 (3.75)  Time: 0.671s, 1526.32/s  (0.697s, 1469.60/s)  LR: 6.920e-04  Data: 0.009 (0.013)
Train: 226 [ 750/1251 ( 60%)]  Loss: 3.851 (3.76)  Time: 0.704s, 1454.62/s  (0.696s, 1470.29/s)  LR: 6.920e-04  Data: 0.009 (0.013)
Train: 226 [ 800/1251 ( 64%)]  Loss: 4.104 (3.78)  Time: 0.702s, 1457.90/s  (0.696s, 1471.70/s)  LR: 6.920e-04  Data: 0.010 (0.013)
Train: 226 [ 850/1251 ( 68%)]  Loss: 3.440 (3.76)  Time: 0.707s, 1448.61/s  (0.695s, 1472.43/s)  LR: 6.920e-04  Data: 0.010 (0.012)
Train: 226 [ 900/1251 ( 72%)]  Loss: 3.719 (3.76)  Time: 0.722s, 1418.45/s  (0.696s, 1472.17/s)  LR: 6.920e-04  Data: 0.010 (0.012)
Train: 226 [ 950/1251 ( 76%)]  Loss: 3.710 (3.76)  Time: 0.679s, 1507.10/s  (0.696s, 1471.75/s)  LR: 6.920e-04  Data: 0.009 (0.012)
Train: 226 [1000/1251 ( 80%)]  Loss: 3.433 (3.74)  Time: 0.704s, 1455.34/s  (0.696s, 1471.88/s)  LR: 6.920e-04  Data: 0.010 (0.012)
Train: 226 [1050/1251 ( 84%)]  Loss: 3.701 (3.74)  Time: 0.736s, 1391.68/s  (0.696s, 1471.82/s)  LR: 6.920e-04  Data: 0.010 (0.012)
Train: 226 [1100/1251 ( 88%)]  Loss: 4.003 (3.75)  Time: 0.667s, 1536.32/s  (0.696s, 1471.80/s)  LR: 6.920e-04  Data: 0.010 (0.012)
Train: 226 [1150/1251 ( 92%)]  Loss: 3.449 (3.74)  Time: 0.670s, 1528.23/s  (0.696s, 1472.06/s)  LR: 6.920e-04  Data: 0.010 (0.012)
Train: 226 [1200/1251 ( 96%)]  Loss: 3.843 (3.74)  Time: 0.671s, 1526.35/s  (0.696s, 1472.04/s)  LR: 6.920e-04  Data: 0.009 (0.012)
Train: 226 [1250/1251 (100%)]  Loss: 3.762 (3.74)  Time: 0.657s, 1557.81/s  (0.695s, 1472.32/s)  LR: 6.920e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.633 (1.633)  Loss:  0.8232 (0.8232)  Acc@1: 88.3789 (88.3789)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.136 (0.584)  Loss:  0.9131 (1.4759)  Acc@1: 84.3160 (71.9100)  Acc@5: 96.8160 (90.9320)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-220.pth.tar', 72.0120000439453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-219.pth.tar', 71.98000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-224.pth.tar', 71.97400009033203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-217.pth.tar', 71.9660000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-226.pth.tar', 71.9100000390625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-216.pth.tar', 71.89799996337891)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-214.pth.tar', 71.88599999267578)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-187.pth.tar', 71.87000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-221.pth.tar', 71.7599999975586)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-223.pth.tar', 71.75000009277343)

Train: 227 [   0/1251 (  0%)]  Loss: 3.708 (3.71)  Time: 2.310s,  443.37/s  (2.310s,  443.37/s)  LR: 6.896e-04  Data: 1.640 (1.640)
Train: 227 [  50/1251 (  4%)]  Loss: 3.709 (3.71)  Time: 0.707s, 1448.84/s  (0.735s, 1393.08/s)  LR: 6.896e-04  Data: 0.009 (0.049)
Train: 227 [ 100/1251 (  8%)]  Loss: 3.548 (3.65)  Time: 0.708s, 1446.04/s  (0.714s, 1433.62/s)  LR: 6.896e-04  Data: 0.010 (0.030)
Train: 227 [ 150/1251 ( 12%)]  Loss: 3.477 (3.61)  Time: 0.803s, 1274.73/s  (0.708s, 1445.67/s)  LR: 6.896e-04  Data: 0.009 (0.023)
Train: 227 [ 200/1251 ( 16%)]  Loss: 3.985 (3.69)  Time: 0.714s, 1434.81/s  (0.706s, 1450.60/s)  LR: 6.896e-04  Data: 0.009 (0.020)
Train: 227 [ 250/1251 ( 20%)]  Loss: 3.773 (3.70)  Time: 0.694s, 1475.71/s  (0.703s, 1457.43/s)  LR: 6.896e-04  Data: 0.010 (0.018)
Train: 227 [ 300/1251 ( 24%)]  Loss: 3.622 (3.69)  Time: 0.702s, 1458.94/s  (0.701s, 1460.04/s)  LR: 6.896e-04  Data: 0.010 (0.017)
Train: 227 [ 350/1251 ( 28%)]  Loss: 3.634 (3.68)  Time: 0.699s, 1465.41/s  (0.700s, 1462.57/s)  LR: 6.896e-04  Data: 0.009 (0.016)
Train: 227 [ 400/1251 ( 32%)]  Loss: 3.752 (3.69)  Time: 0.673s, 1521.45/s  (0.699s, 1465.01/s)  LR: 6.896e-04  Data: 0.014 (0.015)
Train: 227 [ 450/1251 ( 36%)]  Loss: 3.505 (3.67)  Time: 0.673s, 1521.23/s  (0.698s, 1466.20/s)  LR: 6.896e-04  Data: 0.011 (0.015)
Train: 227 [ 500/1251 ( 40%)]  Loss: 3.648 (3.67)  Time: 0.678s, 1510.78/s  (0.698s, 1467.69/s)  LR: 6.896e-04  Data: 0.009 (0.014)
Train: 227 [ 550/1251 ( 44%)]  Loss: 3.790 (3.68)  Time: 0.672s, 1524.31/s  (0.697s, 1468.76/s)  LR: 6.896e-04  Data: 0.010 (0.014)
Train: 227 [ 600/1251 ( 48%)]  Loss: 3.767 (3.69)  Time: 0.679s, 1507.50/s  (0.697s, 1469.71/s)  LR: 6.896e-04  Data: 0.011 (0.014)
Train: 227 [ 650/1251 ( 52%)]  Loss: 4.146 (3.72)  Time: 0.772s, 1325.71/s  (0.698s, 1466.70/s)  LR: 6.896e-04  Data: 0.015 (0.014)
Train: 227 [ 700/1251 ( 56%)]  Loss: 3.400 (3.70)  Time: 0.688s, 1487.39/s  (0.700s, 1463.48/s)  LR: 6.896e-04  Data: 0.012 (0.013)
Train: 227 [ 750/1251 ( 60%)]  Loss: 3.740 (3.70)  Time: 0.706s, 1450.57/s  (0.701s, 1461.04/s)  LR: 6.896e-04  Data: 0.010 (0.013)
Train: 227 [ 800/1251 ( 64%)]  Loss: 3.931 (3.71)  Time: 0.671s, 1526.57/s  (0.700s, 1462.41/s)  LR: 6.896e-04  Data: 0.010 (0.013)
Train: 227 [ 850/1251 ( 68%)]  Loss: 3.684 (3.71)  Time: 0.666s, 1536.54/s  (0.699s, 1465.73/s)  LR: 6.896e-04  Data: 0.009 (0.013)
Train: 227 [ 900/1251 ( 72%)]  Loss: 3.347 (3.69)  Time: 0.671s, 1525.87/s  (0.697s, 1468.20/s)  LR: 6.896e-04  Data: 0.010 (0.013)
Train: 227 [ 950/1251 ( 76%)]  Loss: 4.005 (3.71)  Time: 0.673s, 1522.52/s  (0.697s, 1468.68/s)  LR: 6.896e-04  Data: 0.011 (0.013)
Train: 227 [1000/1251 ( 80%)]  Loss: 3.905 (3.72)  Time: 0.676s, 1513.96/s  (0.697s, 1469.65/s)  LR: 6.896e-04  Data: 0.009 (0.013)
Train: 227 [1050/1251 ( 84%)]  Loss: 3.602 (3.71)  Time: 0.729s, 1405.62/s  (0.697s, 1469.83/s)  LR: 6.896e-04  Data: 0.009 (0.013)
Train: 227 [1100/1251 ( 88%)]  Loss: 3.491 (3.70)  Time: 0.669s, 1531.49/s  (0.696s, 1470.23/s)  LR: 6.896e-04  Data: 0.009 (0.012)
Train: 227 [1150/1251 ( 92%)]  Loss: 3.668 (3.70)  Time: 0.675s, 1516.48/s  (0.696s, 1470.85/s)  LR: 6.896e-04  Data: 0.009 (0.012)
Train: 227 [1200/1251 ( 96%)]  Loss: 3.379 (3.69)  Time: 0.705s, 1451.99/s  (0.696s, 1471.15/s)  LR: 6.896e-04  Data: 0.010 (0.012)
Train: 227 [1250/1251 (100%)]  Loss: 3.434 (3.68)  Time: 0.656s, 1560.16/s  (0.696s, 1471.19/s)  LR: 6.896e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.556 (1.556)  Loss:  0.8433 (0.8433)  Acc@1: 88.1836 (88.1836)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.136 (0.591)  Loss:  0.9966 (1.4614)  Acc@1: 84.3160 (71.7060)  Acc@5: 96.3443 (90.7840)
Train: 228 [   0/1251 (  0%)]  Loss: 3.524 (3.52)  Time: 2.248s,  455.45/s  (2.248s,  455.45/s)  LR: 6.872e-04  Data: 1.621 (1.621)
Train: 228 [  50/1251 (  4%)]  Loss: 3.831 (3.68)  Time: 0.753s, 1359.31/s  (0.724s, 1415.24/s)  LR: 6.872e-04  Data: 0.010 (0.046)
Train: 228 [ 100/1251 (  8%)]  Loss: 3.710 (3.69)  Time: 0.698s, 1467.05/s  (0.708s, 1445.60/s)  LR: 6.872e-04  Data: 0.010 (0.028)
Train: 228 [ 150/1251 ( 12%)]  Loss: 3.689 (3.69)  Time: 0.690s, 1484.39/s  (0.705s, 1452.92/s)  LR: 6.872e-04  Data: 0.011 (0.022)
Train: 228 [ 200/1251 ( 16%)]  Loss: 3.567 (3.66)  Time: 0.673s, 1522.21/s  (0.702s, 1458.15/s)  LR: 6.872e-04  Data: 0.011 (0.019)
Train: 228 [ 250/1251 ( 20%)]  Loss: 3.502 (3.64)  Time: 0.673s, 1520.98/s  (0.700s, 1463.13/s)  LR: 6.872e-04  Data: 0.011 (0.018)
Train: 228 [ 300/1251 ( 24%)]  Loss: 3.774 (3.66)  Time: 0.727s, 1408.12/s  (0.698s, 1468.01/s)  LR: 6.872e-04  Data: 0.010 (0.016)
Train: 228 [ 350/1251 ( 28%)]  Loss: 3.700 (3.66)  Time: 0.703s, 1456.27/s  (0.697s, 1469.41/s)  LR: 6.872e-04  Data: 0.009 (0.015)
Train: 228 [ 400/1251 ( 32%)]  Loss: 3.851 (3.68)  Time: 0.673s, 1520.69/s  (0.696s, 1471.63/s)  LR: 6.872e-04  Data: 0.011 (0.015)
Train: 228 [ 450/1251 ( 36%)]  Loss: 3.377 (3.65)  Time: 0.707s, 1447.91/s  (0.695s, 1472.34/s)  LR: 6.872e-04  Data: 0.010 (0.014)
Train: 228 [ 500/1251 ( 40%)]  Loss: 3.608 (3.65)  Time: 0.723s, 1416.87/s  (0.696s, 1472.20/s)  LR: 6.872e-04  Data: 0.009 (0.014)
Train: 228 [ 550/1251 ( 44%)]  Loss: 4.248 (3.70)  Time: 0.676s, 1515.09/s  (0.695s, 1472.74/s)  LR: 6.872e-04  Data: 0.012 (0.013)
Train: 228 [ 600/1251 ( 48%)]  Loss: 3.436 (3.68)  Time: 0.671s, 1526.27/s  (0.695s, 1473.39/s)  LR: 6.872e-04  Data: 0.011 (0.013)
Train: 228 [ 650/1251 ( 52%)]  Loss: 3.971 (3.70)  Time: 0.672s, 1524.01/s  (0.695s, 1472.91/s)  LR: 6.872e-04  Data: 0.014 (0.013)
Train: 228 [ 700/1251 ( 56%)]  Loss: 3.222 (3.67)  Time: 0.671s, 1527.13/s  (0.695s, 1473.15/s)  LR: 6.872e-04  Data: 0.010 (0.013)
Train: 228 [ 750/1251 ( 60%)]  Loss: 3.196 (3.64)  Time: 0.696s, 1471.24/s  (0.695s, 1472.47/s)  LR: 6.872e-04  Data: 0.009 (0.013)
Train: 228 [ 800/1251 ( 64%)]  Loss: 3.737 (3.64)  Time: 0.669s, 1531.08/s  (0.695s, 1472.71/s)  LR: 6.872e-04  Data: 0.010 (0.012)
Train: 228 [ 850/1251 ( 68%)]  Loss: 3.590 (3.64)  Time: 0.673s, 1522.13/s  (0.695s, 1473.87/s)  LR: 6.872e-04  Data: 0.010 (0.012)
Train: 228 [ 900/1251 ( 72%)]  Loss: 3.860 (3.65)  Time: 0.679s, 1507.24/s  (0.695s, 1474.38/s)  LR: 6.872e-04  Data: 0.011 (0.012)
Train: 228 [ 950/1251 ( 76%)]  Loss: 3.668 (3.65)  Time: 0.678s, 1511.26/s  (0.694s, 1474.84/s)  LR: 6.872e-04  Data: 0.010 (0.012)
Train: 228 [1000/1251 ( 80%)]  Loss: 3.959 (3.67)  Time: 0.698s, 1467.99/s  (0.694s, 1475.05/s)  LR: 6.872e-04  Data: 0.010 (0.012)
Train: 228 [1050/1251 ( 84%)]  Loss: 3.911 (3.68)  Time: 0.674s, 1519.96/s  (0.694s, 1475.22/s)  LR: 6.872e-04  Data: 0.011 (0.012)
Train: 228 [1100/1251 ( 88%)]  Loss: 4.104 (3.70)  Time: 0.679s, 1508.81/s  (0.694s, 1476.30/s)  LR: 6.872e-04  Data: 0.010 (0.012)
Train: 228 [1150/1251 ( 92%)]  Loss: 3.748 (3.70)  Time: 0.673s, 1521.09/s  (0.694s, 1476.44/s)  LR: 6.872e-04  Data: 0.010 (0.012)
Train: 228 [1200/1251 ( 96%)]  Loss: 3.732 (3.70)  Time: 0.708s, 1445.77/s  (0.694s, 1476.29/s)  LR: 6.872e-04  Data: 0.010 (0.012)
Train: 228 [1250/1251 (100%)]  Loss: 3.889 (3.71)  Time: 0.658s, 1556.81/s  (0.694s, 1475.95/s)  LR: 6.872e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.542 (1.542)  Loss:  0.8828 (0.8828)  Acc@1: 88.1836 (88.1836)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.136 (0.584)  Loss:  0.9688 (1.4681)  Acc@1: 85.9670 (72.0320)  Acc@5: 96.1085 (90.6700)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-228.pth.tar', 72.03200010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-220.pth.tar', 72.0120000439453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-219.pth.tar', 71.98000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-224.pth.tar', 71.97400009033203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-217.pth.tar', 71.9660000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-226.pth.tar', 71.9100000390625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-216.pth.tar', 71.89799996337891)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-214.pth.tar', 71.88599999267578)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-187.pth.tar', 71.87000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-221.pth.tar', 71.7599999975586)

Train: 229 [   0/1251 (  0%)]  Loss: 3.690 (3.69)  Time: 2.312s,  442.82/s  (2.312s,  442.82/s)  LR: 6.848e-04  Data: 1.696 (1.696)
Train: 229 [  50/1251 (  4%)]  Loss: 3.673 (3.68)  Time: 0.706s, 1450.03/s  (0.736s, 1391.80/s)  LR: 6.848e-04  Data: 0.009 (0.052)
Train: 229 [ 100/1251 (  8%)]  Loss: 3.772 (3.71)  Time: 0.700s, 1462.42/s  (0.711s, 1440.62/s)  LR: 6.848e-04  Data: 0.009 (0.031)
Train: 229 [ 150/1251 ( 12%)]  Loss: 3.839 (3.74)  Time: 0.678s, 1509.52/s  (0.705s, 1452.69/s)  LR: 6.848e-04  Data: 0.009 (0.024)
Train: 229 [ 200/1251 ( 16%)]  Loss: 3.410 (3.68)  Time: 0.699s, 1464.99/s  (0.703s, 1457.59/s)  LR: 6.848e-04  Data: 0.010 (0.021)
Train: 229 [ 250/1251 ( 20%)]  Loss: 3.733 (3.69)  Time: 0.671s, 1526.83/s  (0.701s, 1460.97/s)  LR: 6.848e-04  Data: 0.012 (0.019)
Train: 229 [ 300/1251 ( 24%)]  Loss: 3.671 (3.68)  Time: 0.705s, 1452.16/s  (0.700s, 1463.47/s)  LR: 6.848e-04  Data: 0.011 (0.017)
Train: 229 [ 350/1251 ( 28%)]  Loss: 3.958 (3.72)  Time: 0.670s, 1527.80/s  (0.700s, 1463.04/s)  LR: 6.848e-04  Data: 0.010 (0.016)
Train: 229 [ 400/1251 ( 32%)]  Loss: 3.947 (3.74)  Time: 0.704s, 1454.83/s  (0.699s, 1465.60/s)  LR: 6.848e-04  Data: 0.009 (0.016)
Train: 229 [ 450/1251 ( 36%)]  Loss: 3.668 (3.74)  Time: 0.706s, 1450.99/s  (0.698s, 1466.69/s)  LR: 6.848e-04  Data: 0.010 (0.015)
Train: 229 [ 500/1251 ( 40%)]  Loss: 3.851 (3.75)  Time: 0.705s, 1453.04/s  (0.698s, 1467.23/s)  LR: 6.848e-04  Data: 0.010 (0.015)
Train: 229 [ 550/1251 ( 44%)]  Loss: 4.170 (3.78)  Time: 0.691s, 1480.98/s  (0.698s, 1467.99/s)  LR: 6.848e-04  Data: 0.009 (0.014)
Train: 229 [ 600/1251 ( 48%)]  Loss: 3.619 (3.77)  Time: 0.666s, 1537.97/s  (0.698s, 1467.44/s)  LR: 6.848e-04  Data: 0.010 (0.014)
Train: 229 [ 650/1251 ( 52%)]  Loss: 3.528 (3.75)  Time: 0.668s, 1532.00/s  (0.698s, 1467.91/s)  LR: 6.848e-04  Data: 0.010 (0.014)
Train: 229 [ 700/1251 ( 56%)]  Loss: 3.838 (3.76)  Time: 0.712s, 1438.89/s  (0.697s, 1468.17/s)  LR: 6.848e-04  Data: 0.011 (0.013)
Train: 229 [ 750/1251 ( 60%)]  Loss: 3.355 (3.73)  Time: 0.694s, 1476.55/s  (0.697s, 1469.21/s)  LR: 6.848e-04  Data: 0.011 (0.013)
Train: 229 [ 800/1251 ( 64%)]  Loss: 3.661 (3.73)  Time: 0.669s, 1530.77/s  (0.697s, 1468.92/s)  LR: 6.848e-04  Data: 0.012 (0.013)
Train: 229 [ 850/1251 ( 68%)]  Loss: 3.854 (3.74)  Time: 0.707s, 1448.45/s  (0.697s, 1469.77/s)  LR: 6.848e-04  Data: 0.010 (0.013)
Train: 229 [ 900/1251 ( 72%)]  Loss: 3.860 (3.74)  Time: 0.721s, 1421.03/s  (0.697s, 1469.70/s)  LR: 6.848e-04  Data: 0.010 (0.013)
Train: 229 [ 950/1251 ( 76%)]  Loss: 3.688 (3.74)  Time: 0.667s, 1534.72/s  (0.697s, 1470.17/s)  LR: 6.848e-04  Data: 0.011 (0.012)
Train: 229 [1000/1251 ( 80%)]  Loss: 3.736 (3.74)  Time: 0.676s, 1515.37/s  (0.697s, 1470.12/s)  LR: 6.848e-04  Data: 0.009 (0.012)
Train: 229 [1050/1251 ( 84%)]  Loss: 3.616 (3.73)  Time: 0.671s, 1526.59/s  (0.697s, 1470.20/s)  LR: 6.848e-04  Data: 0.011 (0.012)
Train: 229 [1100/1251 ( 88%)]  Loss: 3.517 (3.72)  Time: 0.673s, 1522.06/s  (0.697s, 1470.12/s)  LR: 6.848e-04  Data: 0.010 (0.012)
Train: 229 [1150/1251 ( 92%)]  Loss: 3.741 (3.72)  Time: 0.695s, 1472.51/s  (0.697s, 1469.87/s)  LR: 6.848e-04  Data: 0.009 (0.012)
Train: 229 [1200/1251 ( 96%)]  Loss: 3.642 (3.72)  Time: 0.701s, 1461.35/s  (0.696s, 1470.35/s)  LR: 6.848e-04  Data: 0.009 (0.012)
Train: 229 [1250/1251 (100%)]  Loss: 3.802 (3.72)  Time: 0.654s, 1565.71/s  (0.696s, 1471.18/s)  LR: 6.848e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.629 (1.629)  Loss:  0.9355 (0.9355)  Acc@1: 88.5742 (88.5742)  Acc@5: 96.0938 (96.0938)
Test: [  48/48]  Time: 0.138 (0.586)  Loss:  0.8452 (1.4519)  Acc@1: 85.3774 (72.2420)  Acc@5: 96.3443 (90.8800)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-229.pth.tar', 72.24199998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-228.pth.tar', 72.03200010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-220.pth.tar', 72.0120000439453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-219.pth.tar', 71.98000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-224.pth.tar', 71.97400009033203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-217.pth.tar', 71.9660000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-226.pth.tar', 71.9100000390625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-216.pth.tar', 71.89799996337891)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-214.pth.tar', 71.88599999267578)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-187.pth.tar', 71.87000009277344)

Train: 230 [   0/1251 (  0%)]  Loss: 3.411 (3.41)  Time: 2.030s,  504.46/s  (2.030s,  504.46/s)  LR: 6.824e-04  Data: 1.418 (1.418)
Train: 230 [  50/1251 (  4%)]  Loss: 3.609 (3.51)  Time: 0.705s, 1452.06/s  (0.726s, 1410.61/s)  LR: 6.824e-04  Data: 0.010 (0.049)
Train: 230 [ 100/1251 (  8%)]  Loss: 3.660 (3.56)  Time: 0.685s, 1495.61/s  (0.711s, 1440.21/s)  LR: 6.824e-04  Data: 0.015 (0.030)
Train: 230 [ 150/1251 ( 12%)]  Loss: 3.366 (3.51)  Time: 0.719s, 1424.07/s  (0.707s, 1447.94/s)  LR: 6.824e-04  Data: 0.010 (0.023)
Train: 230 [ 200/1251 ( 16%)]  Loss: 3.564 (3.52)  Time: 0.682s, 1502.08/s  (0.705s, 1452.18/s)  LR: 6.824e-04  Data: 0.010 (0.020)
Train: 230 [ 250/1251 ( 20%)]  Loss: 3.707 (3.55)  Time: 0.672s, 1523.51/s  (0.703s, 1456.92/s)  LR: 6.824e-04  Data: 0.013 (0.018)
Train: 230 [ 300/1251 ( 24%)]  Loss: 3.520 (3.55)  Time: 0.689s, 1487.17/s  (0.700s, 1462.19/s)  LR: 6.824e-04  Data: 0.010 (0.017)
Train: 230 [ 350/1251 ( 28%)]  Loss: 3.805 (3.58)  Time: 0.675s, 1517.21/s  (0.699s, 1464.03/s)  LR: 6.824e-04  Data: 0.011 (0.016)
Train: 230 [ 400/1251 ( 32%)]  Loss: 3.835 (3.61)  Time: 0.706s, 1451.45/s  (0.698s, 1466.50/s)  LR: 6.824e-04  Data: 0.010 (0.015)
Train: 230 [ 450/1251 ( 36%)]  Loss: 3.512 (3.60)  Time: 0.667s, 1534.10/s  (0.698s, 1467.44/s)  LR: 6.824e-04  Data: 0.008 (0.015)
Train: 230 [ 500/1251 ( 40%)]  Loss: 3.759 (3.61)  Time: 0.671s, 1525.33/s  (0.698s, 1466.74/s)  LR: 6.824e-04  Data: 0.010 (0.014)
Train: 230 [ 550/1251 ( 44%)]  Loss: 3.723 (3.62)  Time: 0.671s, 1526.23/s  (0.698s, 1467.37/s)  LR: 6.824e-04  Data: 0.010 (0.014)
Train: 230 [ 600/1251 ( 48%)]  Loss: 3.659 (3.63)  Time: 0.671s, 1526.33/s  (0.697s, 1468.67/s)  LR: 6.824e-04  Data: 0.010 (0.014)
Train: 230 [ 650/1251 ( 52%)]  Loss: 3.807 (3.64)  Time: 0.728s, 1406.91/s  (0.697s, 1469.18/s)  LR: 6.824e-04  Data: 0.009 (0.013)
Train: 230 [ 700/1251 ( 56%)]  Loss: 3.764 (3.65)  Time: 0.665s, 1539.00/s  (0.697s, 1469.92/s)  LR: 6.824e-04  Data: 0.010 (0.013)
Train: 230 [ 750/1251 ( 60%)]  Loss: 3.927 (3.66)  Time: 0.718s, 1425.46/s  (0.697s, 1469.15/s)  LR: 6.824e-04  Data: 0.015 (0.013)
Train: 230 [ 800/1251 ( 64%)]  Loss: 3.351 (3.65)  Time: 0.681s, 1503.17/s  (0.697s, 1469.76/s)  LR: 6.824e-04  Data: 0.010 (0.013)
Train: 230 [ 850/1251 ( 68%)]  Loss: 4.056 (3.67)  Time: 0.715s, 1432.51/s  (0.697s, 1469.81/s)  LR: 6.824e-04  Data: 0.009 (0.013)
Train: 230 [ 900/1251 ( 72%)]  Loss: 3.912 (3.68)  Time: 0.684s, 1496.16/s  (0.696s, 1470.46/s)  LR: 6.824e-04  Data: 0.009 (0.012)
Train: 230 [ 950/1251 ( 76%)]  Loss: 3.533 (3.67)  Time: 0.666s, 1537.50/s  (0.697s, 1470.05/s)  LR: 6.824e-04  Data: 0.011 (0.012)
Train: 230 [1000/1251 ( 80%)]  Loss: 3.791 (3.68)  Time: 0.722s, 1419.12/s  (0.696s, 1470.74/s)  LR: 6.824e-04  Data: 0.009 (0.012)
Train: 230 [1050/1251 ( 84%)]  Loss: 3.840 (3.69)  Time: 0.699s, 1464.68/s  (0.696s, 1471.81/s)  LR: 6.824e-04  Data: 0.012 (0.012)
Train: 230 [1100/1251 ( 88%)]  Loss: 3.912 (3.70)  Time: 0.708s, 1446.10/s  (0.696s, 1471.67/s)  LR: 6.824e-04  Data: 0.011 (0.012)
Train: 230 [1150/1251 ( 92%)]  Loss: 3.757 (3.70)  Time: 0.669s, 1530.65/s  (0.696s, 1472.03/s)  LR: 6.824e-04  Data: 0.010 (0.012)
Train: 230 [1200/1251 ( 96%)]  Loss: 3.347 (3.69)  Time: 0.676s, 1515.88/s  (0.695s, 1472.38/s)  LR: 6.824e-04  Data: 0.011 (0.012)
Train: 230 [1250/1251 (100%)]  Loss: 3.959 (3.70)  Time: 0.658s, 1555.94/s  (0.695s, 1472.93/s)  LR: 6.824e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.488 (1.488)  Loss:  0.9302 (0.9302)  Acc@1: 88.5742 (88.5742)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.136 (0.590)  Loss:  1.0039 (1.5503)  Acc@1: 83.8443 (71.4860)  Acc@5: 95.0472 (90.5000)
Train: 231 [   0/1251 (  0%)]  Loss: 3.693 (3.69)  Time: 2.058s,  497.56/s  (2.058s,  497.56/s)  LR: 6.800e-04  Data: 1.443 (1.443)
Train: 231 [  50/1251 (  4%)]  Loss: 3.729 (3.71)  Time: 0.704s, 1454.09/s  (0.727s, 1409.11/s)  LR: 6.800e-04  Data: 0.009 (0.048)
Train: 231 [ 100/1251 (  8%)]  Loss: 3.393 (3.60)  Time: 0.704s, 1454.07/s  (0.709s, 1443.45/s)  LR: 6.800e-04  Data: 0.010 (0.029)
Train: 231 [ 150/1251 ( 12%)]  Loss: 3.658 (3.62)  Time: 0.700s, 1462.73/s  (0.703s, 1456.13/s)  LR: 6.800e-04  Data: 0.009 (0.023)
Train: 231 [ 200/1251 ( 16%)]  Loss: 3.823 (3.66)  Time: 0.724s, 1415.05/s  (0.702s, 1457.82/s)  LR: 6.800e-04  Data: 0.009 (0.020)
Train: 231 [ 250/1251 ( 20%)]  Loss: 4.041 (3.72)  Time: 0.706s, 1449.43/s  (0.701s, 1460.48/s)  LR: 6.800e-04  Data: 0.009 (0.018)
Train: 231 [ 300/1251 ( 24%)]  Loss: 3.438 (3.68)  Time: 0.674s, 1518.45/s  (0.700s, 1463.57/s)  LR: 6.800e-04  Data: 0.010 (0.016)
Train: 231 [ 350/1251 ( 28%)]  Loss: 3.752 (3.69)  Time: 0.672s, 1524.50/s  (0.699s, 1465.66/s)  LR: 6.800e-04  Data: 0.010 (0.016)
Train: 231 [ 400/1251 ( 32%)]  Loss: 3.698 (3.69)  Time: 0.690s, 1484.79/s  (0.697s, 1469.50/s)  LR: 6.800e-04  Data: 0.010 (0.015)
Train: 231 [ 450/1251 ( 36%)]  Loss: 3.497 (3.67)  Time: 0.672s, 1523.31/s  (0.697s, 1470.00/s)  LR: 6.800e-04  Data: 0.010 (0.014)
Train: 231 [ 500/1251 ( 40%)]  Loss: 3.806 (3.68)  Time: 0.702s, 1457.74/s  (0.696s, 1471.23/s)  LR: 6.800e-04  Data: 0.009 (0.014)
Train: 231 [ 550/1251 ( 44%)]  Loss: 3.823 (3.70)  Time: 0.674s, 1519.56/s  (0.695s, 1472.47/s)  LR: 6.800e-04  Data: 0.011 (0.014)
Train: 231 [ 600/1251 ( 48%)]  Loss: 3.758 (3.70)  Time: 0.674s, 1519.83/s  (0.695s, 1473.28/s)  LR: 6.800e-04  Data: 0.010 (0.013)
Train: 231 [ 650/1251 ( 52%)]  Loss: 3.490 (3.69)  Time: 0.673s, 1522.28/s  (0.694s, 1474.66/s)  LR: 6.800e-04  Data: 0.012 (0.013)
Train: 231 [ 700/1251 ( 56%)]  Loss: 3.685 (3.69)  Time: 0.670s, 1528.71/s  (0.694s, 1474.61/s)  LR: 6.800e-04  Data: 0.009 (0.013)
Train: 231 [ 750/1251 ( 60%)]  Loss: 3.974 (3.70)  Time: 0.675s, 1517.42/s  (0.695s, 1473.17/s)  LR: 6.800e-04  Data: 0.014 (0.013)
Train: 231 [ 800/1251 ( 64%)]  Loss: 3.443 (3.69)  Time: 0.672s, 1524.84/s  (0.695s, 1473.02/s)  LR: 6.800e-04  Data: 0.010 (0.013)
Train: 231 [ 850/1251 ( 68%)]  Loss: 3.505 (3.68)  Time: 0.722s, 1418.00/s  (0.695s, 1472.46/s)  LR: 6.800e-04  Data: 0.009 (0.012)
Train: 231 [ 900/1251 ( 72%)]  Loss: 3.904 (3.69)  Time: 0.675s, 1517.69/s  (0.695s, 1472.72/s)  LR: 6.800e-04  Data: 0.011 (0.012)
Train: 231 [ 950/1251 ( 76%)]  Loss: 4.119 (3.71)  Time: 0.675s, 1517.16/s  (0.695s, 1473.41/s)  LR: 6.800e-04  Data: 0.011 (0.012)
Train: 231 [1000/1251 ( 80%)]  Loss: 3.592 (3.71)  Time: 0.671s, 1526.01/s  (0.695s, 1474.26/s)  LR: 6.800e-04  Data: 0.010 (0.012)
Train: 231 [1050/1251 ( 84%)]  Loss: 3.789 (3.71)  Time: 0.703s, 1456.58/s  (0.695s, 1473.60/s)  LR: 6.800e-04  Data: 0.010 (0.012)
Train: 231 [1100/1251 ( 88%)]  Loss: 3.138 (3.68)  Time: 0.696s, 1470.50/s  (0.695s, 1473.75/s)  LR: 6.800e-04  Data: 0.009 (0.012)
Train: 231 [1150/1251 ( 92%)]  Loss: 4.063 (3.70)  Time: 0.670s, 1529.14/s  (0.695s, 1473.34/s)  LR: 6.800e-04  Data: 0.011 (0.012)
Train: 231 [1200/1251 ( 96%)]  Loss: 3.740 (3.70)  Time: 0.673s, 1520.45/s  (0.695s, 1473.71/s)  LR: 6.800e-04  Data: 0.009 (0.012)
Train: 231 [1250/1251 (100%)]  Loss: 4.047 (3.72)  Time: 0.691s, 1482.85/s  (0.695s, 1474.18/s)  LR: 6.800e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.574 (1.574)  Loss:  0.7520 (0.7520)  Acc@1: 88.9648 (88.9648)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.136 (0.581)  Loss:  0.8711 (1.4621)  Acc@1: 84.1981 (71.6300)  Acc@5: 96.6981 (90.7280)
Train: 232 [   0/1251 (  0%)]  Loss: 3.663 (3.66)  Time: 2.130s,  480.68/s  (2.130s,  480.68/s)  LR: 6.775e-04  Data: 1.514 (1.514)
Train: 232 [  50/1251 (  4%)]  Loss: 3.597 (3.63)  Time: 0.717s, 1428.90/s  (0.725s, 1413.01/s)  LR: 6.775e-04  Data: 0.009 (0.046)
Train: 232 [ 100/1251 (  8%)]  Loss: 3.533 (3.60)  Time: 0.732s, 1398.65/s  (0.712s, 1437.36/s)  LR: 6.775e-04  Data: 0.009 (0.028)
Train: 232 [ 150/1251 ( 12%)]  Loss: 3.626 (3.60)  Time: 0.706s, 1450.47/s  (0.707s, 1447.71/s)  LR: 6.775e-04  Data: 0.009 (0.022)
Train: 232 [ 200/1251 ( 16%)]  Loss: 3.531 (3.59)  Time: 0.717s, 1427.97/s  (0.703s, 1455.94/s)  LR: 6.775e-04  Data: 0.010 (0.019)
Train: 232 [ 250/1251 ( 20%)]  Loss: 3.903 (3.64)  Time: 0.674s, 1519.45/s  (0.703s, 1456.99/s)  LR: 6.775e-04  Data: 0.013 (0.017)
Train: 232 [ 300/1251 ( 24%)]  Loss: 3.561 (3.63)  Time: 0.668s, 1532.15/s  (0.701s, 1460.84/s)  LR: 6.775e-04  Data: 0.009 (0.016)
Train: 232 [ 350/1251 ( 28%)]  Loss: 3.978 (3.67)  Time: 0.748s, 1368.71/s  (0.699s, 1464.33/s)  LR: 6.775e-04  Data: 0.010 (0.015)
Train: 232 [ 400/1251 ( 32%)]  Loss: 3.796 (3.69)  Time: 0.695s, 1473.54/s  (0.699s, 1465.37/s)  LR: 6.775e-04  Data: 0.010 (0.015)
Train: 232 [ 450/1251 ( 36%)]  Loss: 3.878 (3.71)  Time: 0.720s, 1421.57/s  (0.698s, 1466.59/s)  LR: 6.775e-04  Data: 0.009 (0.014)
Train: 232 [ 500/1251 ( 40%)]  Loss: 3.625 (3.70)  Time: 0.709s, 1445.13/s  (0.698s, 1467.48/s)  LR: 6.775e-04  Data: 0.009 (0.014)
Train: 232 [ 550/1251 ( 44%)]  Loss: 3.863 (3.71)  Time: 0.675s, 1517.58/s  (0.697s, 1468.96/s)  LR: 6.775e-04  Data: 0.012 (0.013)
Train: 232 [ 600/1251 ( 48%)]  Loss: 3.810 (3.72)  Time: 0.693s, 1478.37/s  (0.697s, 1469.70/s)  LR: 6.775e-04  Data: 0.010 (0.013)
Train: 232 [ 650/1251 ( 52%)]  Loss: 3.788 (3.73)  Time: 0.669s, 1531.66/s  (0.696s, 1470.47/s)  LR: 6.775e-04  Data: 0.010 (0.013)
Train: 232 [ 700/1251 ( 56%)]  Loss: 3.923 (3.74)  Time: 0.687s, 1490.30/s  (0.696s, 1471.35/s)  LR: 6.775e-04  Data: 0.016 (0.013)
Train: 232 [ 750/1251 ( 60%)]  Loss: 3.857 (3.75)  Time: 0.707s, 1448.54/s  (0.696s, 1471.44/s)  LR: 6.775e-04  Data: 0.009 (0.013)
Train: 232 [ 800/1251 ( 64%)]  Loss: 4.041 (3.76)  Time: 0.673s, 1522.63/s  (0.696s, 1472.12/s)  LR: 6.775e-04  Data: 0.010 (0.012)
Train: 232 [ 850/1251 ( 68%)]  Loss: 3.639 (3.76)  Time: 0.675s, 1516.23/s  (0.695s, 1473.36/s)  LR: 6.775e-04  Data: 0.009 (0.012)
Train: 232 [ 900/1251 ( 72%)]  Loss: 3.858 (3.76)  Time: 0.682s, 1502.03/s  (0.695s, 1473.36/s)  LR: 6.775e-04  Data: 0.015 (0.012)
Train: 232 [ 950/1251 ( 76%)]  Loss: 3.764 (3.76)  Time: 0.679s, 1507.50/s  (0.695s, 1473.53/s)  LR: 6.775e-04  Data: 0.010 (0.012)
Train: 232 [1000/1251 ( 80%)]  Loss: 3.643 (3.76)  Time: 0.712s, 1437.69/s  (0.695s, 1473.70/s)  LR: 6.775e-04  Data: 0.011 (0.012)
Train: 232 [1050/1251 ( 84%)]  Loss: 3.438 (3.74)  Time: 0.678s, 1509.55/s  (0.695s, 1473.72/s)  LR: 6.775e-04  Data: 0.010 (0.012)
Train: 232 [1100/1251 ( 88%)]  Loss: 4.003 (3.75)  Time: 0.717s, 1427.38/s  (0.695s, 1473.89/s)  LR: 6.775e-04  Data: 0.008 (0.012)
Train: 232 [1150/1251 ( 92%)]  Loss: 3.945 (3.76)  Time: 0.706s, 1449.56/s  (0.695s, 1473.72/s)  LR: 6.775e-04  Data: 0.009 (0.012)
Train: 232 [1200/1251 ( 96%)]  Loss: 3.798 (3.76)  Time: 0.689s, 1485.84/s  (0.695s, 1473.45/s)  LR: 6.775e-04  Data: 0.009 (0.012)
Train: 232 [1250/1251 (100%)]  Loss: 3.806 (3.76)  Time: 0.690s, 1484.22/s  (0.695s, 1473.38/s)  LR: 6.775e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.539 (1.539)  Loss:  0.9844 (0.9844)  Acc@1: 87.5000 (87.5000)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.137 (0.580)  Loss:  1.0811 (1.5248)  Acc@1: 83.2547 (71.9660)  Acc@5: 94.5755 (91.1280)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-229.pth.tar', 72.24199998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-228.pth.tar', 72.03200010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-220.pth.tar', 72.0120000439453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-219.pth.tar', 71.98000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-224.pth.tar', 71.97400009033203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-232.pth.tar', 71.96600009521484)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-217.pth.tar', 71.9660000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-226.pth.tar', 71.9100000390625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-216.pth.tar', 71.89799996337891)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-214.pth.tar', 71.88599999267578)

Train: 233 [   0/1251 (  0%)]  Loss: 3.807 (3.81)  Time: 2.176s,  470.51/s  (2.176s,  470.51/s)  LR: 6.751e-04  Data: 1.560 (1.560)
Train: 233 [  50/1251 (  4%)]  Loss: 4.049 (3.93)  Time: 0.668s, 1533.20/s  (0.736s, 1390.71/s)  LR: 6.751e-04  Data: 0.009 (0.048)
Train: 233 [ 100/1251 (  8%)]  Loss: 3.798 (3.88)  Time: 0.671s, 1525.39/s  (0.714s, 1433.78/s)  LR: 6.751e-04  Data: 0.009 (0.029)
Train: 233 [ 150/1251 ( 12%)]  Loss: 3.361 (3.75)  Time: 0.691s, 1481.84/s  (0.706s, 1451.33/s)  LR: 6.751e-04  Data: 0.010 (0.023)
Train: 233 [ 200/1251 ( 16%)]  Loss: 3.440 (3.69)  Time: 0.706s, 1450.13/s  (0.703s, 1456.57/s)  LR: 6.751e-04  Data: 0.010 (0.020)
Train: 233 [ 250/1251 ( 20%)]  Loss: 3.843 (3.72)  Time: 0.673s, 1522.38/s  (0.700s, 1462.20/s)  LR: 6.751e-04  Data: 0.010 (0.018)
Train: 233 [ 300/1251 ( 24%)]  Loss: 3.606 (3.70)  Time: 0.754s, 1358.41/s  (0.701s, 1461.37/s)  LR: 6.751e-04  Data: 0.010 (0.017)
Train: 233 [ 350/1251 ( 28%)]  Loss: 3.852 (3.72)  Time: 0.672s, 1524.73/s  (0.700s, 1462.75/s)  LR: 6.751e-04  Data: 0.009 (0.016)
Train: 233 [ 400/1251 ( 32%)]  Loss: 3.595 (3.71)  Time: 0.676s, 1515.24/s  (0.700s, 1463.76/s)  LR: 6.751e-04  Data: 0.009 (0.015)
Train: 233 [ 450/1251 ( 36%)]  Loss: 3.770 (3.71)  Time: 0.673s, 1520.94/s  (0.699s, 1465.32/s)  LR: 6.751e-04  Data: 0.010 (0.015)
Train: 233 [ 500/1251 ( 40%)]  Loss: 3.475 (3.69)  Time: 0.722s, 1419.01/s  (0.699s, 1465.63/s)  LR: 6.751e-04  Data: 0.010 (0.014)
Train: 233 [ 550/1251 ( 44%)]  Loss: 3.840 (3.70)  Time: 0.726s, 1411.13/s  (0.699s, 1464.26/s)  LR: 6.751e-04  Data: 0.010 (0.014)
Train: 233 [ 600/1251 ( 48%)]  Loss: 3.684 (3.70)  Time: 0.704s, 1454.78/s  (0.699s, 1464.63/s)  LR: 6.751e-04  Data: 0.010 (0.013)
Train: 233 [ 650/1251 ( 52%)]  Loss: 3.627 (3.70)  Time: 0.693s, 1477.85/s  (0.699s, 1464.86/s)  LR: 6.751e-04  Data: 0.009 (0.013)
Train: 233 [ 700/1251 ( 56%)]  Loss: 3.905 (3.71)  Time: 0.739s, 1386.15/s  (0.699s, 1465.80/s)  LR: 6.751e-04  Data: 0.009 (0.013)
Train: 233 [ 750/1251 ( 60%)]  Loss: 3.727 (3.71)  Time: 0.666s, 1537.11/s  (0.698s, 1466.80/s)  LR: 6.751e-04  Data: 0.010 (0.013)
Train: 233 [ 800/1251 ( 64%)]  Loss: 3.859 (3.72)  Time: 0.671s, 1525.19/s  (0.698s, 1466.68/s)  LR: 6.751e-04  Data: 0.010 (0.013)
Train: 233 [ 850/1251 ( 68%)]  Loss: 3.846 (3.73)  Time: 0.672s, 1524.49/s  (0.698s, 1467.49/s)  LR: 6.751e-04  Data: 0.011 (0.013)
Train: 233 [ 900/1251 ( 72%)]  Loss: 3.611 (3.72)  Time: 0.778s, 1316.00/s  (0.698s, 1467.32/s)  LR: 6.751e-04  Data: 0.009 (0.012)
Train: 233 [ 950/1251 ( 76%)]  Loss: 3.731 (3.72)  Time: 0.676s, 1514.24/s  (0.698s, 1466.10/s)  LR: 6.751e-04  Data: 0.010 (0.012)
Train: 233 [1000/1251 ( 80%)]  Loss: 3.793 (3.72)  Time: 0.727s, 1408.40/s  (0.698s, 1466.79/s)  LR: 6.751e-04  Data: 0.009 (0.012)
Train: 233 [1050/1251 ( 84%)]  Loss: 3.784 (3.73)  Time: 0.704s, 1453.60/s  (0.698s, 1466.99/s)  LR: 6.751e-04  Data: 0.009 (0.012)
Train: 233 [1100/1251 ( 88%)]  Loss: 3.759 (3.73)  Time: 0.679s, 1507.15/s  (0.698s, 1466.99/s)  LR: 6.751e-04  Data: 0.010 (0.012)
Train: 233 [1150/1251 ( 92%)]  Loss: 3.866 (3.73)  Time: 0.672s, 1524.42/s  (0.697s, 1468.14/s)  LR: 6.751e-04  Data: 0.009 (0.012)
Train: 233 [1200/1251 ( 96%)]  Loss: 3.620 (3.73)  Time: 0.668s, 1533.72/s  (0.697s, 1468.81/s)  LR: 6.751e-04  Data: 0.009 (0.012)
Train: 233 [1250/1251 (100%)]  Loss: 3.686 (3.73)  Time: 0.666s, 1537.40/s  (0.697s, 1469.20/s)  LR: 6.751e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.525 (1.525)  Loss:  0.9570 (0.9570)  Acc@1: 87.8906 (87.8906)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.136 (0.575)  Loss:  0.9839 (1.4763)  Acc@1: 82.1934 (72.1200)  Acc@5: 95.4009 (90.9260)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-229.pth.tar', 72.24199998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-233.pth.tar', 72.12000002197266)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-228.pth.tar', 72.03200010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-220.pth.tar', 72.0120000439453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-219.pth.tar', 71.98000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-224.pth.tar', 71.97400009033203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-232.pth.tar', 71.96600009521484)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-217.pth.tar', 71.9660000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-226.pth.tar', 71.9100000390625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-216.pth.tar', 71.89799996337891)

Train: 234 [   0/1251 (  0%)]  Loss: 3.780 (3.78)  Time: 2.250s,  455.11/s  (2.250s,  455.11/s)  LR: 6.727e-04  Data: 1.634 (1.634)
Train: 234 [  50/1251 (  4%)]  Loss: 3.361 (3.57)  Time: 0.672s, 1522.78/s  (0.730s, 1403.28/s)  LR: 6.727e-04  Data: 0.010 (0.050)
Train: 234 [ 100/1251 (  8%)]  Loss: 3.429 (3.52)  Time: 0.687s, 1490.60/s  (0.713s, 1436.26/s)  LR: 6.727e-04  Data: 0.010 (0.030)
Train: 234 [ 150/1251 ( 12%)]  Loss: 3.511 (3.52)  Time: 0.729s, 1405.49/s  (0.707s, 1447.69/s)  LR: 6.727e-04  Data: 0.010 (0.024)
Train: 234 [ 200/1251 ( 16%)]  Loss: 3.828 (3.58)  Time: 0.674s, 1518.77/s  (0.706s, 1450.06/s)  LR: 6.727e-04  Data: 0.010 (0.021)
Train: 234 [ 250/1251 ( 20%)]  Loss: 3.996 (3.65)  Time: 0.671s, 1526.04/s  (0.705s, 1452.97/s)  LR: 6.727e-04  Data: 0.009 (0.019)
Train: 234 [ 300/1251 ( 24%)]  Loss: 3.581 (3.64)  Time: 0.681s, 1503.15/s  (0.702s, 1457.69/s)  LR: 6.727e-04  Data: 0.010 (0.017)
Train: 234 [ 350/1251 ( 28%)]  Loss: 3.758 (3.66)  Time: 0.674s, 1518.82/s  (0.702s, 1459.70/s)  LR: 6.727e-04  Data: 0.009 (0.016)
Train: 234 [ 400/1251 ( 32%)]  Loss: 3.785 (3.67)  Time: 0.697s, 1469.57/s  (0.700s, 1462.49/s)  LR: 6.727e-04  Data: 0.010 (0.015)
Train: 234 [ 450/1251 ( 36%)]  Loss: 3.808 (3.68)  Time: 0.671s, 1526.18/s  (0.700s, 1463.90/s)  LR: 6.727e-04  Data: 0.011 (0.015)
Train: 234 [ 500/1251 ( 40%)]  Loss: 3.762 (3.69)  Time: 0.666s, 1537.05/s  (0.699s, 1465.32/s)  LR: 6.727e-04  Data: 0.010 (0.014)
Train: 234 [ 550/1251 ( 44%)]  Loss: 3.751 (3.70)  Time: 0.701s, 1460.90/s  (0.698s, 1466.64/s)  LR: 6.727e-04  Data: 0.010 (0.014)
Train: 234 [ 600/1251 ( 48%)]  Loss: 3.796 (3.70)  Time: 0.673s, 1521.69/s  (0.698s, 1468.06/s)  LR: 6.727e-04  Data: 0.011 (0.014)
Train: 234 [ 650/1251 ( 52%)]  Loss: 3.719 (3.70)  Time: 0.670s, 1527.86/s  (0.697s, 1468.80/s)  LR: 6.727e-04  Data: 0.010 (0.013)
Train: 234 [ 700/1251 ( 56%)]  Loss: 3.526 (3.69)  Time: 0.706s, 1449.74/s  (0.697s, 1470.13/s)  LR: 6.727e-04  Data: 0.009 (0.013)
Train: 234 [ 750/1251 ( 60%)]  Loss: 3.504 (3.68)  Time: 0.720s, 1421.60/s  (0.696s, 1470.65/s)  LR: 6.727e-04  Data: 0.012 (0.013)
Train: 234 [ 800/1251 ( 64%)]  Loss: 3.815 (3.69)  Time: 0.687s, 1491.49/s  (0.696s, 1470.60/s)  LR: 6.727e-04  Data: 0.011 (0.013)
Train: 234 [ 850/1251 ( 68%)]  Loss: 3.896 (3.70)  Time: 0.687s, 1490.95/s  (0.696s, 1470.86/s)  LR: 6.727e-04  Data: 0.010 (0.013)
Train: 234 [ 900/1251 ( 72%)]  Loss: 3.659 (3.70)  Time: 0.717s, 1427.50/s  (0.696s, 1471.71/s)  LR: 6.727e-04  Data: 0.009 (0.013)
Train: 234 [ 950/1251 ( 76%)]  Loss: 3.765 (3.70)  Time: 0.701s, 1461.20/s  (0.696s, 1471.45/s)  LR: 6.727e-04  Data: 0.010 (0.012)
Train: 234 [1000/1251 ( 80%)]  Loss: 3.496 (3.69)  Time: 0.672s, 1522.94/s  (0.696s, 1471.66/s)  LR: 6.727e-04  Data: 0.010 (0.012)
Train: 234 [1050/1251 ( 84%)]  Loss: 4.183 (3.71)  Time: 0.710s, 1442.27/s  (0.696s, 1472.04/s)  LR: 6.727e-04  Data: 0.010 (0.012)
Train: 234 [1100/1251 ( 88%)]  Loss: 3.756 (3.72)  Time: 0.706s, 1451.35/s  (0.696s, 1471.79/s)  LR: 6.727e-04  Data: 0.011 (0.012)
Train: 234 [1150/1251 ( 92%)]  Loss: 3.347 (3.70)  Time: 0.678s, 1511.13/s  (0.696s, 1471.60/s)  LR: 6.727e-04  Data: 0.010 (0.012)
Train: 234 [1200/1251 ( 96%)]  Loss: 3.784 (3.70)  Time: 0.717s, 1428.89/s  (0.696s, 1471.96/s)  LR: 6.727e-04  Data: 0.010 (0.012)
Train: 234 [1250/1251 (100%)]  Loss: 4.052 (3.72)  Time: 0.656s, 1561.04/s  (0.696s, 1472.14/s)  LR: 6.727e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.565 (1.565)  Loss:  0.8081 (0.8081)  Acc@1: 88.8672 (88.8672)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.137 (0.582)  Loss:  0.9253 (1.4573)  Acc@1: 84.4340 (72.2320)  Acc@5: 95.9906 (91.0380)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-229.pth.tar', 72.24199998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-234.pth.tar', 72.23200009033204)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-233.pth.tar', 72.12000002197266)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-228.pth.tar', 72.03200010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-220.pth.tar', 72.0120000439453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-219.pth.tar', 71.98000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-224.pth.tar', 71.97400009033203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-232.pth.tar', 71.96600009521484)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-217.pth.tar', 71.9660000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-226.pth.tar', 71.9100000390625)

Train: 235 [   0/1251 (  0%)]  Loss: 3.972 (3.97)  Time: 2.259s,  453.36/s  (2.259s,  453.36/s)  LR: 6.702e-04  Data: 1.593 (1.593)
Train: 235 [  50/1251 (  4%)]  Loss: 3.343 (3.66)  Time: 0.703s, 1456.84/s  (0.735s, 1393.82/s)  LR: 6.702e-04  Data: 0.009 (0.047)
Train: 235 [ 100/1251 (  8%)]  Loss: 3.779 (3.70)  Time: 0.671s, 1525.72/s  (0.713s, 1436.46/s)  LR: 6.702e-04  Data: 0.010 (0.029)
Train: 235 [ 150/1251 ( 12%)]  Loss: 3.849 (3.74)  Time: 0.698s, 1466.28/s  (0.705s, 1452.91/s)  LR: 6.702e-04  Data: 0.009 (0.023)
Train: 235 [ 200/1251 ( 16%)]  Loss: 3.596 (3.71)  Time: 0.702s, 1457.79/s  (0.703s, 1455.80/s)  LR: 6.702e-04  Data: 0.009 (0.020)
Train: 235 [ 250/1251 ( 20%)]  Loss: 3.869 (3.73)  Time: 0.717s, 1428.20/s  (0.701s, 1461.55/s)  LR: 6.702e-04  Data: 0.010 (0.018)
Train: 235 [ 300/1251 ( 24%)]  Loss: 3.720 (3.73)  Time: 0.670s, 1528.95/s  (0.700s, 1462.74/s)  LR: 6.702e-04  Data: 0.011 (0.016)
Train: 235 [ 350/1251 ( 28%)]  Loss: 3.909 (3.75)  Time: 0.748s, 1368.95/s  (0.698s, 1466.02/s)  LR: 6.702e-04  Data: 0.009 (0.016)
Train: 235 [ 400/1251 ( 32%)]  Loss: 3.784 (3.76)  Time: 0.707s, 1447.77/s  (0.698s, 1467.57/s)  LR: 6.702e-04  Data: 0.009 (0.015)
Train: 235 [ 450/1251 ( 36%)]  Loss: 3.822 (3.76)  Time: 0.819s, 1250.78/s  (0.698s, 1466.75/s)  LR: 6.702e-04  Data: 0.009 (0.014)
Train: 235 [ 500/1251 ( 40%)]  Loss: 3.665 (3.76)  Time: 0.705s, 1453.48/s  (0.697s, 1469.87/s)  LR: 6.702e-04  Data: 0.010 (0.014)
Train: 235 [ 550/1251 ( 44%)]  Loss: 4.000 (3.78)  Time: 0.681s, 1502.70/s  (0.696s, 1470.43/s)  LR: 6.702e-04  Data: 0.009 (0.014)
Train: 235 [ 600/1251 ( 48%)]  Loss: 3.704 (3.77)  Time: 0.692s, 1480.06/s  (0.696s, 1470.56/s)  LR: 6.702e-04  Data: 0.012 (0.013)
Train: 235 [ 650/1251 ( 52%)]  Loss: 3.385 (3.74)  Time: 0.691s, 1481.20/s  (0.697s, 1470.14/s)  LR: 6.702e-04  Data: 0.009 (0.013)
Train: 235 [ 700/1251 ( 56%)]  Loss: 3.644 (3.74)  Time: 0.705s, 1451.46/s  (0.696s, 1470.22/s)  LR: 6.702e-04  Data: 0.009 (0.013)
Train: 235 [ 750/1251 ( 60%)]  Loss: 3.477 (3.72)  Time: 0.782s, 1310.23/s  (0.697s, 1469.75/s)  LR: 6.702e-04  Data: 0.011 (0.013)
Train: 235 [ 800/1251 ( 64%)]  Loss: 3.623 (3.71)  Time: 0.674s, 1518.95/s  (0.696s, 1470.52/s)  LR: 6.702e-04  Data: 0.010 (0.013)
Train: 235 [ 850/1251 ( 68%)]  Loss: 3.893 (3.72)  Time: 0.722s, 1418.91/s  (0.696s, 1470.73/s)  LR: 6.702e-04  Data: 0.009 (0.012)
Train: 235 [ 900/1251 ( 72%)]  Loss: 3.674 (3.72)  Time: 0.719s, 1423.25/s  (0.696s, 1471.18/s)  LR: 6.702e-04  Data: 0.009 (0.012)
Train: 235 [ 950/1251 ( 76%)]  Loss: 3.408 (3.71)  Time: 0.672s, 1524.03/s  (0.696s, 1471.55/s)  LR: 6.702e-04  Data: 0.009 (0.012)
Train: 235 [1000/1251 ( 80%)]  Loss: 3.734 (3.71)  Time: 0.674s, 1519.96/s  (0.696s, 1472.06/s)  LR: 6.702e-04  Data: 0.009 (0.012)
Train: 235 [1050/1251 ( 84%)]  Loss: 4.283 (3.73)  Time: 0.669s, 1529.77/s  (0.696s, 1471.63/s)  LR: 6.702e-04  Data: 0.010 (0.012)
Train: 235 [1100/1251 ( 88%)]  Loss: 3.509 (3.72)  Time: 0.673s, 1522.25/s  (0.696s, 1471.35/s)  LR: 6.702e-04  Data: 0.010 (0.012)
Train: 235 [1150/1251 ( 92%)]  Loss: 3.586 (3.72)  Time: 0.674s, 1518.48/s  (0.696s, 1471.28/s)  LR: 6.702e-04  Data: 0.010 (0.012)
Train: 235 [1200/1251 ( 96%)]  Loss: 3.483 (3.71)  Time: 0.677s, 1513.11/s  (0.696s, 1471.58/s)  LR: 6.702e-04  Data: 0.011 (0.012)
Train: 235 [1250/1251 (100%)]  Loss: 3.539 (3.70)  Time: 0.656s, 1561.27/s  (0.695s, 1472.60/s)  LR: 6.702e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.690 (1.690)  Loss:  0.9092 (0.9092)  Acc@1: 88.1836 (88.1836)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.136 (0.581)  Loss:  1.0693 (1.5034)  Acc@1: 84.0802 (71.6380)  Acc@5: 95.7547 (90.7400)
Train: 236 [   0/1251 (  0%)]  Loss: 3.793 (3.79)  Time: 2.312s,  442.92/s  (2.312s,  442.92/s)  LR: 6.678e-04  Data: 1.642 (1.642)
Train: 236 [  50/1251 (  4%)]  Loss: 3.877 (3.84)  Time: 0.738s, 1387.70/s  (0.734s, 1395.75/s)  LR: 6.678e-04  Data: 0.009 (0.051)
Train: 236 [ 100/1251 (  8%)]  Loss: 3.620 (3.76)  Time: 0.672s, 1523.02/s  (0.710s, 1441.50/s)  LR: 6.678e-04  Data: 0.010 (0.031)
Train: 236 [ 150/1251 ( 12%)]  Loss: 3.792 (3.77)  Time: 0.692s, 1480.16/s  (0.707s, 1449.38/s)  LR: 6.678e-04  Data: 0.010 (0.024)
Train: 236 [ 200/1251 ( 16%)]  Loss: 3.893 (3.80)  Time: 0.672s, 1523.55/s  (0.702s, 1457.96/s)  LR: 6.678e-04  Data: 0.010 (0.021)
Train: 236 [ 250/1251 ( 20%)]  Loss: 3.634 (3.77)  Time: 0.709s, 1445.23/s  (0.700s, 1462.84/s)  LR: 6.678e-04  Data: 0.009 (0.019)
Train: 236 [ 300/1251 ( 24%)]  Loss: 3.315 (3.70)  Time: 0.703s, 1457.62/s  (0.701s, 1461.16/s)  LR: 6.678e-04  Data: 0.010 (0.017)
Train: 236 [ 350/1251 ( 28%)]  Loss: 3.566 (3.69)  Time: 0.676s, 1515.58/s  (0.699s, 1464.76/s)  LR: 6.678e-04  Data: 0.010 (0.016)
Train: 236 [ 400/1251 ( 32%)]  Loss: 3.792 (3.70)  Time: 0.671s, 1527.07/s  (0.698s, 1467.19/s)  LR: 6.678e-04  Data: 0.011 (0.015)
Train: 236 [ 450/1251 ( 36%)]  Loss: 3.515 (3.68)  Time: 0.689s, 1486.25/s  (0.697s, 1469.87/s)  LR: 6.678e-04  Data: 0.009 (0.015)
Train: 236 [ 500/1251 ( 40%)]  Loss: 3.850 (3.70)  Time: 0.721s, 1419.92/s  (0.696s, 1471.02/s)  LR: 6.678e-04  Data: 0.010 (0.014)
Train: 236 [ 550/1251 ( 44%)]  Loss: 3.713 (3.70)  Time: 0.698s, 1468.10/s  (0.696s, 1472.20/s)  LR: 6.678e-04  Data: 0.009 (0.014)
Train: 236 [ 600/1251 ( 48%)]  Loss: 3.488 (3.68)  Time: 0.673s, 1521.30/s  (0.696s, 1471.21/s)  LR: 6.678e-04  Data: 0.011 (0.014)
Train: 236 [ 650/1251 ( 52%)]  Loss: 3.906 (3.70)  Time: 0.710s, 1442.52/s  (0.696s, 1470.46/s)  LR: 6.678e-04  Data: 0.014 (0.013)
Train: 236 [ 700/1251 ( 56%)]  Loss: 3.428 (3.68)  Time: 0.675s, 1516.64/s  (0.696s, 1470.79/s)  LR: 6.678e-04  Data: 0.010 (0.013)
Train: 236 [ 750/1251 ( 60%)]  Loss: 4.010 (3.70)  Time: 0.669s, 1531.11/s  (0.696s, 1471.08/s)  LR: 6.678e-04  Data: 0.010 (0.013)
Train: 236 [ 800/1251 ( 64%)]  Loss: 3.826 (3.71)  Time: 0.693s, 1477.83/s  (0.696s, 1471.47/s)  LR: 6.678e-04  Data: 0.010 (0.013)
Train: 236 [ 850/1251 ( 68%)]  Loss: 3.468 (3.69)  Time: 0.666s, 1538.10/s  (0.696s, 1472.31/s)  LR: 6.678e-04  Data: 0.010 (0.013)
Train: 236 [ 900/1251 ( 72%)]  Loss: 3.530 (3.69)  Time: 0.735s, 1392.44/s  (0.696s, 1471.71/s)  LR: 6.678e-04  Data: 0.009 (0.012)
Train: 236 [ 950/1251 ( 76%)]  Loss: 3.959 (3.70)  Time: 0.711s, 1440.94/s  (0.696s, 1472.15/s)  LR: 6.678e-04  Data: 0.009 (0.012)
Train: 236 [1000/1251 ( 80%)]  Loss: 3.635 (3.70)  Time: 0.724s, 1414.28/s  (0.696s, 1472.06/s)  LR: 6.678e-04  Data: 0.010 (0.012)
Train: 236 [1050/1251 ( 84%)]  Loss: 3.646 (3.69)  Time: 0.703s, 1457.51/s  (0.696s, 1472.23/s)  LR: 6.678e-04  Data: 0.010 (0.012)
Train: 236 [1100/1251 ( 88%)]  Loss: 3.499 (3.68)  Time: 0.719s, 1423.60/s  (0.695s, 1472.67/s)  LR: 6.678e-04  Data: 0.011 (0.012)
Train: 236 [1150/1251 ( 92%)]  Loss: 3.327 (3.67)  Time: 0.710s, 1442.82/s  (0.695s, 1473.34/s)  LR: 6.678e-04  Data: 0.010 (0.012)
Train: 236 [1200/1251 ( 96%)]  Loss: 3.731 (3.67)  Time: 0.705s, 1453.31/s  (0.695s, 1473.87/s)  LR: 6.678e-04  Data: 0.009 (0.012)
Train: 236 [1250/1251 (100%)]  Loss: 3.943 (3.68)  Time: 0.686s, 1492.84/s  (0.695s, 1473.85/s)  LR: 6.678e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.500 (1.500)  Loss:  1.0625 (1.0625)  Acc@1: 88.2812 (88.2812)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.138 (0.573)  Loss:  1.2705 (1.5820)  Acc@1: 81.6038 (71.9280)  Acc@5: 94.8113 (90.8220)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-229.pth.tar', 72.24199998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-234.pth.tar', 72.23200009033204)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-233.pth.tar', 72.12000002197266)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-228.pth.tar', 72.03200010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-220.pth.tar', 72.0120000439453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-219.pth.tar', 71.98000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-224.pth.tar', 71.97400009033203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-232.pth.tar', 71.96600009521484)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-217.pth.tar', 71.9660000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-236.pth.tar', 71.92800002441406)

Train: 237 [   0/1251 (  0%)]  Loss: 3.896 (3.90)  Time: 2.483s,  412.36/s  (2.483s,  412.36/s)  LR: 6.653e-04  Data: 1.842 (1.842)
Train: 237 [  50/1251 (  4%)]  Loss: 3.868 (3.88)  Time: 0.727s, 1408.53/s  (0.747s, 1370.82/s)  LR: 6.653e-04  Data: 0.012 (0.051)
Train: 237 [ 100/1251 (  8%)]  Loss: 3.767 (3.84)  Time: 0.695s, 1473.33/s  (0.730s, 1402.71/s)  LR: 6.653e-04  Data: 0.010 (0.032)
Train: 237 [ 150/1251 ( 12%)]  Loss: 3.596 (3.78)  Time: 0.739s, 1384.75/s  (0.723s, 1415.37/s)  LR: 6.653e-04  Data: 0.010 (0.025)
Train: 237 [ 200/1251 ( 16%)]  Loss: 3.793 (3.78)  Time: 0.706s, 1450.05/s  (0.717s, 1428.23/s)  LR: 6.653e-04  Data: 0.009 (0.022)
Train: 237 [ 250/1251 ( 20%)]  Loss: 3.496 (3.74)  Time: 0.672s, 1523.56/s  (0.711s, 1440.68/s)  LR: 6.653e-04  Data: 0.010 (0.019)
Train: 237 [ 300/1251 ( 24%)]  Loss: 3.615 (3.72)  Time: 0.704s, 1453.75/s  (0.706s, 1450.10/s)  LR: 6.653e-04  Data: 0.009 (0.018)
Train: 237 [ 350/1251 ( 28%)]  Loss: 3.664 (3.71)  Time: 0.673s, 1522.44/s  (0.704s, 1455.08/s)  LR: 6.653e-04  Data: 0.010 (0.017)
Train: 237 [ 400/1251 ( 32%)]  Loss: 4.008 (3.74)  Time: 0.672s, 1524.79/s  (0.702s, 1458.88/s)  LR: 6.653e-04  Data: 0.009 (0.016)
Train: 237 [ 450/1251 ( 36%)]  Loss: 3.415 (3.71)  Time: 0.719s, 1423.66/s  (0.700s, 1462.20/s)  LR: 6.653e-04  Data: 0.010 (0.015)
Train: 237 [ 500/1251 ( 40%)]  Loss: 3.956 (3.73)  Time: 0.674s, 1520.28/s  (0.700s, 1463.77/s)  LR: 6.653e-04  Data: 0.015 (0.015)
Train: 237 [ 550/1251 ( 44%)]  Loss: 3.201 (3.69)  Time: 0.701s, 1461.53/s  (0.699s, 1464.66/s)  LR: 6.653e-04  Data: 0.009 (0.014)
Train: 237 [ 600/1251 ( 48%)]  Loss: 3.933 (3.71)  Time: 0.673s, 1520.63/s  (0.699s, 1465.14/s)  LR: 6.653e-04  Data: 0.009 (0.014)
Train: 237 [ 650/1251 ( 52%)]  Loss: 3.596 (3.70)  Time: 0.686s, 1492.78/s  (0.698s, 1466.94/s)  LR: 6.653e-04  Data: 0.010 (0.014)
Train: 237 [ 700/1251 ( 56%)]  Loss: 3.552 (3.69)  Time: 0.676s, 1515.39/s  (0.697s, 1468.53/s)  LR: 6.653e-04  Data: 0.010 (0.013)
Train: 237 [ 750/1251 ( 60%)]  Loss: 3.988 (3.71)  Time: 0.701s, 1460.79/s  (0.697s, 1468.99/s)  LR: 6.653e-04  Data: 0.012 (0.013)
Train: 237 [ 800/1251 ( 64%)]  Loss: 3.960 (3.72)  Time: 0.672s, 1523.29/s  (0.697s, 1469.47/s)  LR: 6.653e-04  Data: 0.010 (0.013)
Train: 237 [ 850/1251 ( 68%)]  Loss: 3.989 (3.74)  Time: 0.705s, 1453.11/s  (0.697s, 1469.75/s)  LR: 6.653e-04  Data: 0.009 (0.013)
Train: 237 [ 900/1251 ( 72%)]  Loss: 3.535 (3.73)  Time: 0.664s, 1542.44/s  (0.697s, 1469.81/s)  LR: 6.653e-04  Data: 0.010 (0.013)
Train: 237 [ 950/1251 ( 76%)]  Loss: 3.813 (3.73)  Time: 0.675s, 1517.30/s  (0.697s, 1469.69/s)  LR: 6.653e-04  Data: 0.008 (0.013)
Train: 237 [1000/1251 ( 80%)]  Loss: 4.067 (3.75)  Time: 0.716s, 1430.22/s  (0.696s, 1470.94/s)  LR: 6.653e-04  Data: 0.010 (0.013)
Train: 237 [1050/1251 ( 84%)]  Loss: 3.566 (3.74)  Time: 0.680s, 1506.84/s  (0.696s, 1471.45/s)  LR: 6.653e-04  Data: 0.010 (0.012)
Train: 237 [1100/1251 ( 88%)]  Loss: 3.981 (3.75)  Time: 0.702s, 1458.56/s  (0.696s, 1471.69/s)  LR: 6.653e-04  Data: 0.009 (0.012)
Train: 237 [1150/1251 ( 92%)]  Loss: 3.663 (3.75)  Time: 0.710s, 1443.10/s  (0.696s, 1472.15/s)  LR: 6.653e-04  Data: 0.010 (0.012)
Train: 237 [1200/1251 ( 96%)]  Loss: 3.813 (3.75)  Time: 0.703s, 1456.16/s  (0.696s, 1472.04/s)  LR: 6.653e-04  Data: 0.009 (0.012)
Train: 237 [1250/1251 (100%)]  Loss: 3.658 (3.75)  Time: 0.653s, 1567.67/s  (0.695s, 1472.53/s)  LR: 6.653e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.551 (1.551)  Loss:  1.0400 (1.0400)  Acc@1: 85.8398 (85.8398)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.136 (0.588)  Loss:  1.2402 (1.6431)  Acc@1: 81.9576 (71.5440)  Acc@5: 95.0472 (90.6760)
Train: 238 [   0/1251 (  0%)]  Loss: 3.723 (3.72)  Time: 2.421s,  422.95/s  (2.421s,  422.95/s)  LR: 6.629e-04  Data: 1.746 (1.746)
Train: 238 [  50/1251 (  4%)]  Loss: 3.736 (3.73)  Time: 0.693s, 1477.82/s  (0.738s, 1387.66/s)  LR: 6.629e-04  Data: 0.010 (0.051)
Train: 238 [ 100/1251 (  8%)]  Loss: 3.772 (3.74)  Time: 0.673s, 1521.02/s  (0.715s, 1432.16/s)  LR: 6.629e-04  Data: 0.010 (0.031)
Train: 238 [ 150/1251 ( 12%)]  Loss: 3.858 (3.77)  Time: 0.673s, 1521.66/s  (0.709s, 1444.01/s)  LR: 6.629e-04  Data: 0.009 (0.024)
Train: 238 [ 200/1251 ( 16%)]  Loss: 3.980 (3.81)  Time: 0.674s, 1519.27/s  (0.709s, 1445.16/s)  LR: 6.629e-04  Data: 0.011 (0.021)
Train: 238 [ 250/1251 ( 20%)]  Loss: 3.819 (3.81)  Time: 0.673s, 1521.62/s  (0.705s, 1452.16/s)  LR: 6.629e-04  Data: 0.010 (0.019)
Train: 238 [ 300/1251 ( 24%)]  Loss: 3.480 (3.77)  Time: 0.668s, 1533.98/s  (0.703s, 1456.06/s)  LR: 6.629e-04  Data: 0.012 (0.017)
Train: 238 [ 350/1251 ( 28%)]  Loss: 3.815 (3.77)  Time: 0.674s, 1518.24/s  (0.701s, 1460.66/s)  LR: 6.629e-04  Data: 0.012 (0.016)
Train: 238 [ 400/1251 ( 32%)]  Loss: 3.644 (3.76)  Time: 0.699s, 1465.20/s  (0.700s, 1463.87/s)  LR: 6.629e-04  Data: 0.009 (0.015)
Train: 238 [ 450/1251 ( 36%)]  Loss: 3.986 (3.78)  Time: 0.712s, 1438.88/s  (0.698s, 1466.32/s)  LR: 6.629e-04  Data: 0.010 (0.015)
Train: 238 [ 500/1251 ( 40%)]  Loss: 3.589 (3.76)  Time: 0.701s, 1460.48/s  (0.697s, 1468.54/s)  LR: 6.629e-04  Data: 0.010 (0.014)
Train: 238 [ 550/1251 ( 44%)]  Loss: 3.627 (3.75)  Time: 0.706s, 1451.24/s  (0.697s, 1468.32/s)  LR: 6.629e-04  Data: 0.010 (0.014)
Train: 238 [ 600/1251 ( 48%)]  Loss: 3.222 (3.71)  Time: 0.676s, 1515.12/s  (0.697s, 1469.34/s)  LR: 6.629e-04  Data: 0.010 (0.014)
Train: 238 [ 650/1251 ( 52%)]  Loss: 4.201 (3.75)  Time: 0.716s, 1429.58/s  (0.697s, 1468.73/s)  LR: 6.629e-04  Data: 0.009 (0.013)
Train: 238 [ 700/1251 ( 56%)]  Loss: 3.939 (3.76)  Time: 0.679s, 1508.94/s  (0.697s, 1469.14/s)  LR: 6.629e-04  Data: 0.010 (0.013)
Train: 238 [ 750/1251 ( 60%)]  Loss: 3.849 (3.76)  Time: 0.717s, 1427.98/s  (0.697s, 1469.13/s)  LR: 6.629e-04  Data: 0.010 (0.013)
Train: 238 [ 800/1251 ( 64%)]  Loss: 3.850 (3.77)  Time: 0.679s, 1508.10/s  (0.697s, 1468.84/s)  LR: 6.629e-04  Data: 0.010 (0.013)
Train: 238 [ 850/1251 ( 68%)]  Loss: 3.652 (3.76)  Time: 0.685s, 1494.26/s  (0.697s, 1470.19/s)  LR: 6.629e-04  Data: 0.010 (0.013)
Train: 238 [ 900/1251 ( 72%)]  Loss: 4.007 (3.78)  Time: 0.732s, 1399.71/s  (0.697s, 1469.72/s)  LR: 6.629e-04  Data: 0.011 (0.013)
Train: 238 [ 950/1251 ( 76%)]  Loss: 3.793 (3.78)  Time: 0.700s, 1463.37/s  (0.697s, 1469.33/s)  LR: 6.629e-04  Data: 0.010 (0.012)
Train: 238 [1000/1251 ( 80%)]  Loss: 3.929 (3.78)  Time: 0.673s, 1521.80/s  (0.696s, 1470.35/s)  LR: 6.629e-04  Data: 0.009 (0.012)
Train: 238 [1050/1251 ( 84%)]  Loss: 3.942 (3.79)  Time: 0.673s, 1521.91/s  (0.696s, 1471.06/s)  LR: 6.629e-04  Data: 0.011 (0.012)
Train: 238 [1100/1251 ( 88%)]  Loss: 3.737 (3.79)  Time: 0.674s, 1519.39/s  (0.696s, 1471.81/s)  LR: 6.629e-04  Data: 0.010 (0.012)
Train: 238 [1150/1251 ( 92%)]  Loss: 3.483 (3.78)  Time: 0.710s, 1441.75/s  (0.696s, 1472.02/s)  LR: 6.629e-04  Data: 0.010 (0.012)
Train: 238 [1200/1251 ( 96%)]  Loss: 3.342 (3.76)  Time: 0.720s, 1422.17/s  (0.695s, 1472.34/s)  LR: 6.629e-04  Data: 0.010 (0.012)
Train: 238 [1250/1251 (100%)]  Loss: 3.834 (3.76)  Time: 0.658s, 1556.98/s  (0.695s, 1472.76/s)  LR: 6.629e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.483 (1.483)  Loss:  0.8931 (0.8931)  Acc@1: 88.9648 (88.9648)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.136 (0.587)  Loss:  0.9570 (1.5027)  Acc@1: 85.1415 (71.9880)  Acc@5: 95.8727 (91.0480)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-229.pth.tar', 72.24199998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-234.pth.tar', 72.23200009033204)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-233.pth.tar', 72.12000002197266)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-228.pth.tar', 72.03200010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-220.pth.tar', 72.0120000439453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-238.pth.tar', 71.98800000976563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-219.pth.tar', 71.98000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-224.pth.tar', 71.97400009033203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-232.pth.tar', 71.96600009521484)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-217.pth.tar', 71.9660000415039)

Train: 239 [   0/1251 (  0%)]  Loss: 3.302 (3.30)  Time: 2.257s,  453.66/s  (2.257s,  453.66/s)  LR: 6.604e-04  Data: 1.590 (1.590)
Train: 239 [  50/1251 (  4%)]  Loss: 4.098 (3.70)  Time: 0.688s, 1488.90/s  (0.733s, 1396.94/s)  LR: 6.604e-04  Data: 0.013 (0.047)
Train: 239 [ 100/1251 (  8%)]  Loss: 3.469 (3.62)  Time: 0.706s, 1449.74/s  (0.713s, 1436.75/s)  LR: 6.604e-04  Data: 0.009 (0.029)
Train: 239 [ 150/1251 ( 12%)]  Loss: 3.546 (3.60)  Time: 0.677s, 1511.76/s  (0.705s, 1453.31/s)  LR: 6.604e-04  Data: 0.013 (0.023)
Train: 239 [ 200/1251 ( 16%)]  Loss: 3.495 (3.58)  Time: 0.708s, 1446.76/s  (0.702s, 1458.67/s)  LR: 6.604e-04  Data: 0.009 (0.019)
Train: 239 [ 250/1251 ( 20%)]  Loss: 3.704 (3.60)  Time: 0.719s, 1423.38/s  (0.701s, 1461.61/s)  LR: 6.604e-04  Data: 0.010 (0.018)
Train: 239 [ 300/1251 ( 24%)]  Loss: 3.575 (3.60)  Time: 0.701s, 1461.69/s  (0.699s, 1464.96/s)  LR: 6.604e-04  Data: 0.009 (0.016)
Train: 239 [ 350/1251 ( 28%)]  Loss: 3.982 (3.65)  Time: 0.717s, 1428.42/s  (0.698s, 1467.62/s)  LR: 6.604e-04  Data: 0.009 (0.015)
Train: 239 [ 400/1251 ( 32%)]  Loss: 3.880 (3.67)  Time: 0.672s, 1524.41/s  (0.697s, 1469.84/s)  LR: 6.604e-04  Data: 0.010 (0.015)
Train: 239 [ 450/1251 ( 36%)]  Loss: 3.648 (3.67)  Time: 0.791s, 1294.77/s  (0.698s, 1467.57/s)  LR: 6.604e-04  Data: 0.012 (0.014)
Train: 239 [ 500/1251 ( 40%)]  Loss: 3.690 (3.67)  Time: 0.672s, 1524.29/s  (0.697s, 1468.70/s)  LR: 6.604e-04  Data: 0.010 (0.014)
Train: 239 [ 550/1251 ( 44%)]  Loss: 3.656 (3.67)  Time: 0.666s, 1538.60/s  (0.697s, 1469.87/s)  LR: 6.604e-04  Data: 0.009 (0.014)
Train: 239 [ 600/1251 ( 48%)]  Loss: 3.646 (3.67)  Time: 0.666s, 1536.42/s  (0.697s, 1469.14/s)  LR: 6.604e-04  Data: 0.011 (0.013)
Train: 239 [ 650/1251 ( 52%)]  Loss: 3.857 (3.68)  Time: 0.685s, 1495.07/s  (0.697s, 1469.63/s)  LR: 6.604e-04  Data: 0.009 (0.013)
Train: 239 [ 700/1251 ( 56%)]  Loss: 3.799 (3.69)  Time: 0.703s, 1457.38/s  (0.696s, 1470.84/s)  LR: 6.604e-04  Data: 0.009 (0.013)
Train: 239 [ 750/1251 ( 60%)]  Loss: 4.014 (3.71)  Time: 0.674s, 1518.26/s  (0.696s, 1470.75/s)  LR: 6.604e-04  Data: 0.010 (0.013)
Train: 239 [ 800/1251 ( 64%)]  Loss: 3.747 (3.71)  Time: 0.722s, 1418.19/s  (0.696s, 1470.92/s)  LR: 6.604e-04  Data: 0.015 (0.013)
Train: 239 [ 850/1251 ( 68%)]  Loss: 3.655 (3.71)  Time: 0.735s, 1393.55/s  (0.696s, 1470.44/s)  LR: 6.604e-04  Data: 0.010 (0.012)
Train: 239 [ 900/1251 ( 72%)]  Loss: 3.570 (3.70)  Time: 0.699s, 1464.17/s  (0.696s, 1470.68/s)  LR: 6.604e-04  Data: 0.009 (0.012)
Train: 239 [ 950/1251 ( 76%)]  Loss: 3.770 (3.71)  Time: 0.706s, 1450.38/s  (0.696s, 1471.48/s)  LR: 6.604e-04  Data: 0.010 (0.012)
Train: 239 [1000/1251 ( 80%)]  Loss: 3.541 (3.70)  Time: 0.675s, 1516.32/s  (0.695s, 1472.56/s)  LR: 6.604e-04  Data: 0.010 (0.012)
Train: 239 [1050/1251 ( 84%)]  Loss: 3.787 (3.70)  Time: 0.669s, 1530.36/s  (0.695s, 1472.74/s)  LR: 6.604e-04  Data: 0.009 (0.012)
Train: 239 [1100/1251 ( 88%)]  Loss: 3.673 (3.70)  Time: 0.713s, 1436.04/s  (0.695s, 1472.94/s)  LR: 6.604e-04  Data: 0.010 (0.012)
Train: 239 [1150/1251 ( 92%)]  Loss: 3.941 (3.71)  Time: 0.673s, 1521.25/s  (0.695s, 1473.41/s)  LR: 6.604e-04  Data: 0.011 (0.012)
Train: 239 [1200/1251 ( 96%)]  Loss: 3.795 (3.71)  Time: 0.700s, 1462.20/s  (0.695s, 1472.93/s)  LR: 6.604e-04  Data: 0.009 (0.012)
Train: 239 [1250/1251 (100%)]  Loss: 3.733 (3.71)  Time: 0.713s, 1436.69/s  (0.695s, 1472.41/s)  LR: 6.604e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.510 (1.510)  Loss:  0.9468 (0.9468)  Acc@1: 88.4766 (88.4766)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.136 (0.585)  Loss:  1.2246 (1.5850)  Acc@1: 81.7217 (72.2240)  Acc@5: 95.1651 (90.9980)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-229.pth.tar', 72.24199998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-234.pth.tar', 72.23200009033204)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-239.pth.tar', 72.22400007568359)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-233.pth.tar', 72.12000002197266)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-228.pth.tar', 72.03200010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-220.pth.tar', 72.0120000439453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-238.pth.tar', 71.98800000976563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-219.pth.tar', 71.98000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-224.pth.tar', 71.97400009033203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-232.pth.tar', 71.96600009521484)

Train: 240 [   0/1251 (  0%)]  Loss: 3.388 (3.39)  Time: 2.399s,  426.85/s  (2.399s,  426.85/s)  LR: 6.580e-04  Data: 1.763 (1.763)
Train: 240 [  50/1251 (  4%)]  Loss: 3.566 (3.48)  Time: 0.701s, 1461.69/s  (0.731s, 1401.35/s)  LR: 6.580e-04  Data: 0.009 (0.051)
Train: 240 [ 100/1251 (  8%)]  Loss: 3.911 (3.62)  Time: 0.711s, 1441.19/s  (0.711s, 1440.83/s)  LR: 6.580e-04  Data: 0.010 (0.031)
Train: 240 [ 150/1251 ( 12%)]  Loss: 3.303 (3.54)  Time: 0.694s, 1476.13/s  (0.704s, 1454.13/s)  LR: 6.580e-04  Data: 0.010 (0.024)
Train: 240 [ 200/1251 ( 16%)]  Loss: 3.515 (3.54)  Time: 0.685s, 1495.74/s  (0.703s, 1457.15/s)  LR: 6.580e-04  Data: 0.010 (0.021)
Train: 240 [ 250/1251 ( 20%)]  Loss: 3.823 (3.58)  Time: 0.706s, 1451.26/s  (0.701s, 1460.07/s)  LR: 6.580e-04  Data: 0.011 (0.019)
Train: 240 [ 300/1251 ( 24%)]  Loss: 3.470 (3.57)  Time: 0.671s, 1525.57/s  (0.700s, 1462.27/s)  LR: 6.580e-04  Data: 0.011 (0.017)
Train: 240 [ 350/1251 ( 28%)]  Loss: 3.396 (3.55)  Time: 0.713s, 1436.53/s  (0.698s, 1466.85/s)  LR: 6.580e-04  Data: 0.011 (0.016)
Train: 240 [ 400/1251 ( 32%)]  Loss: 3.499 (3.54)  Time: 0.675s, 1516.99/s  (0.698s, 1467.67/s)  LR: 6.580e-04  Data: 0.011 (0.015)
Train: 240 [ 450/1251 ( 36%)]  Loss: 3.697 (3.56)  Time: 0.676s, 1515.14/s  (0.698s, 1466.50/s)  LR: 6.580e-04  Data: 0.010 (0.015)
Train: 240 [ 500/1251 ( 40%)]  Loss: 3.744 (3.57)  Time: 0.684s, 1497.91/s  (0.698s, 1467.53/s)  LR: 6.580e-04  Data: 0.010 (0.014)
Train: 240 [ 550/1251 ( 44%)]  Loss: 3.386 (3.56)  Time: 0.709s, 1444.62/s  (0.698s, 1467.96/s)  LR: 6.580e-04  Data: 0.009 (0.014)
Train: 240 [ 600/1251 ( 48%)]  Loss: 3.483 (3.55)  Time: 0.672s, 1523.01/s  (0.697s, 1469.50/s)  LR: 6.580e-04  Data: 0.010 (0.014)
Train: 240 [ 650/1251 ( 52%)]  Loss: 3.585 (3.55)  Time: 0.725s, 1412.81/s  (0.696s, 1470.93/s)  LR: 6.580e-04  Data: 0.010 (0.013)
Train: 240 [ 700/1251 ( 56%)]  Loss: 3.904 (3.58)  Time: 0.735s, 1392.40/s  (0.696s, 1471.82/s)  LR: 6.580e-04  Data: 0.015 (0.013)
Train: 240 [ 750/1251 ( 60%)]  Loss: 3.579 (3.58)  Time: 0.702s, 1459.64/s  (0.696s, 1471.74/s)  LR: 6.580e-04  Data: 0.010 (0.013)
Train: 240 [ 800/1251 ( 64%)]  Loss: 3.653 (3.58)  Time: 0.680s, 1504.93/s  (0.695s, 1472.82/s)  LR: 6.580e-04  Data: 0.011 (0.013)
Train: 240 [ 850/1251 ( 68%)]  Loss: 3.755 (3.59)  Time: 0.749s, 1366.71/s  (0.695s, 1472.64/s)  LR: 6.580e-04  Data: 0.009 (0.013)
Train: 240 [ 900/1251 ( 72%)]  Loss: 3.785 (3.60)  Time: 0.725s, 1413.06/s  (0.696s, 1472.16/s)  LR: 6.580e-04  Data: 0.010 (0.013)
Train: 240 [ 950/1251 ( 76%)]  Loss: 3.434 (3.59)  Time: 0.664s, 1541.55/s  (0.695s, 1472.66/s)  LR: 6.580e-04  Data: 0.009 (0.012)
Train: 240 [1000/1251 ( 80%)]  Loss: 3.656 (3.60)  Time: 0.697s, 1469.65/s  (0.695s, 1473.45/s)  LR: 6.580e-04  Data: 0.010 (0.012)
Train: 240 [1050/1251 ( 84%)]  Loss: 3.554 (3.59)  Time: 0.726s, 1411.23/s  (0.695s, 1474.03/s)  LR: 6.580e-04  Data: 0.010 (0.012)
Train: 240 [1100/1251 ( 88%)]  Loss: 3.424 (3.59)  Time: 0.719s, 1424.99/s  (0.695s, 1473.83/s)  LR: 6.580e-04  Data: 0.015 (0.012)
Train: 240 [1150/1251 ( 92%)]  Loss: 3.664 (3.59)  Time: 0.710s, 1441.45/s  (0.695s, 1473.38/s)  LR: 6.580e-04  Data: 0.011 (0.012)
Train: 240 [1200/1251 ( 96%)]  Loss: 3.585 (3.59)  Time: 0.671s, 1525.33/s  (0.695s, 1473.51/s)  LR: 6.580e-04  Data: 0.011 (0.012)
Train: 240 [1250/1251 (100%)]  Loss: 3.467 (3.59)  Time: 0.657s, 1557.71/s  (0.695s, 1473.79/s)  LR: 6.580e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.557 (1.557)  Loss:  0.8584 (0.8584)  Acc@1: 87.7930 (87.7930)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.137 (0.588)  Loss:  0.9941 (1.4571)  Acc@1: 83.1368 (71.9880)  Acc@5: 95.0472 (90.7500)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-229.pth.tar', 72.24199998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-234.pth.tar', 72.23200009033204)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-239.pth.tar', 72.22400007568359)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-233.pth.tar', 72.12000002197266)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-228.pth.tar', 72.03200010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-220.pth.tar', 72.0120000439453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-240.pth.tar', 71.9880000439453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-238.pth.tar', 71.98800000976563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-219.pth.tar', 71.98000009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-224.pth.tar', 71.97400009033203)

Train: 241 [   0/1251 (  0%)]  Loss: 3.756 (3.76)  Time: 2.589s,  395.46/s  (2.589s,  395.46/s)  LR: 6.555e-04  Data: 1.904 (1.904)
Train: 241 [  50/1251 (  4%)]  Loss: 3.627 (3.69)  Time: 0.673s, 1521.66/s  (0.729s, 1404.66/s)  LR: 6.555e-04  Data: 0.010 (0.047)
Train: 241 [ 100/1251 (  8%)]  Loss: 3.686 (3.69)  Time: 0.670s, 1529.35/s  (0.713s, 1436.07/s)  LR: 6.555e-04  Data: 0.010 (0.029)
Train: 241 [ 150/1251 ( 12%)]  Loss: 3.340 (3.60)  Time: 0.711s, 1440.66/s  (0.707s, 1447.51/s)  LR: 6.555e-04  Data: 0.009 (0.023)
Train: 241 [ 200/1251 ( 16%)]  Loss: 3.892 (3.66)  Time: 0.692s, 1479.87/s  (0.703s, 1455.97/s)  LR: 6.555e-04  Data: 0.010 (0.020)
Train: 241 [ 250/1251 ( 20%)]  Loss: 3.694 (3.67)  Time: 0.675s, 1517.20/s  (0.701s, 1460.42/s)  LR: 6.555e-04  Data: 0.010 (0.018)
Train: 241 [ 300/1251 ( 24%)]  Loss: 3.666 (3.67)  Time: 0.746s, 1373.21/s  (0.701s, 1460.55/s)  LR: 6.555e-04  Data: 0.009 (0.017)
Train: 241 [ 350/1251 ( 28%)]  Loss: 3.351 (3.63)  Time: 0.698s, 1467.74/s  (0.700s, 1463.22/s)  LR: 6.555e-04  Data: 0.009 (0.016)
Train: 241 [ 400/1251 ( 32%)]  Loss: 3.562 (3.62)  Time: 0.669s, 1529.54/s  (0.700s, 1463.64/s)  LR: 6.555e-04  Data: 0.010 (0.015)
Train: 241 [ 450/1251 ( 36%)]  Loss: 3.627 (3.62)  Time: 0.717s, 1427.81/s  (0.699s, 1464.20/s)  LR: 6.555e-04  Data: 0.009 (0.014)
Train: 241 [ 500/1251 ( 40%)]  Loss: 3.570 (3.62)  Time: 0.665s, 1539.71/s  (0.700s, 1463.84/s)  LR: 6.555e-04  Data: 0.010 (0.014)
Train: 241 [ 550/1251 ( 44%)]  Loss: 3.508 (3.61)  Time: 0.712s, 1438.66/s  (0.700s, 1462.54/s)  LR: 6.555e-04  Data: 0.009 (0.014)
Train: 241 [ 600/1251 ( 48%)]  Loss: 4.142 (3.65)  Time: 0.698s, 1467.36/s  (0.699s, 1464.15/s)  LR: 6.555e-04  Data: 0.008 (0.013)
Train: 241 [ 650/1251 ( 52%)]  Loss: 3.643 (3.65)  Time: 0.675s, 1516.35/s  (0.698s, 1466.19/s)  LR: 6.555e-04  Data: 0.010 (0.013)
Train: 241 [ 700/1251 ( 56%)]  Loss: 3.738 (3.65)  Time: 0.702s, 1459.64/s  (0.697s, 1468.30/s)  LR: 6.555e-04  Data: 0.010 (0.013)
Train: 241 [ 750/1251 ( 60%)]  Loss: 3.673 (3.65)  Time: 0.704s, 1453.92/s  (0.698s, 1467.67/s)  LR: 6.555e-04  Data: 0.009 (0.013)
Train: 241 [ 800/1251 ( 64%)]  Loss: 3.835 (3.67)  Time: 0.686s, 1491.88/s  (0.698s, 1466.55/s)  LR: 6.555e-04  Data: 0.013 (0.013)
Train: 241 [ 850/1251 ( 68%)]  Loss: 3.837 (3.67)  Time: 0.712s, 1437.60/s  (0.698s, 1467.07/s)  LR: 6.555e-04  Data: 0.009 (0.012)
Train: 241 [ 900/1251 ( 72%)]  Loss: 3.528 (3.67)  Time: 0.674s, 1518.35/s  (0.697s, 1468.37/s)  LR: 6.555e-04  Data: 0.009 (0.012)
Train: 241 [ 950/1251 ( 76%)]  Loss: 3.597 (3.66)  Time: 0.708s, 1447.33/s  (0.697s, 1468.96/s)  LR: 6.555e-04  Data: 0.011 (0.012)
Train: 241 [1000/1251 ( 80%)]  Loss: 3.430 (3.65)  Time: 0.711s, 1441.13/s  (0.697s, 1469.55/s)  LR: 6.555e-04  Data: 0.010 (0.012)
Train: 241 [1050/1251 ( 84%)]  Loss: 3.940 (3.67)  Time: 0.715s, 1432.95/s  (0.697s, 1469.51/s)  LR: 6.555e-04  Data: 0.011 (0.012)
Train: 241 [1100/1251 ( 88%)]  Loss: 3.340 (3.65)  Time: 0.673s, 1521.92/s  (0.696s, 1470.31/s)  LR: 6.555e-04  Data: 0.009 (0.012)
Train: 241 [1150/1251 ( 92%)]  Loss: 3.921 (3.66)  Time: 0.732s, 1399.81/s  (0.696s, 1471.06/s)  LR: 6.555e-04  Data: 0.009 (0.012)
Train: 241 [1200/1251 ( 96%)]  Loss: 3.879 (3.67)  Time: 0.672s, 1523.62/s  (0.696s, 1471.19/s)  LR: 6.555e-04  Data: 0.010 (0.012)
Train: 241 [1250/1251 (100%)]  Loss: 3.888 (3.68)  Time: 0.654s, 1565.66/s  (0.696s, 1470.99/s)  LR: 6.555e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.536 (1.536)  Loss:  0.8467 (0.8467)  Acc@1: 88.9648 (88.9648)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.136 (0.591)  Loss:  0.9785 (1.4571)  Acc@1: 82.6651 (72.5400)  Acc@5: 95.6368 (91.2420)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-241.pth.tar', 72.53999996826172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-229.pth.tar', 72.24199998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-234.pth.tar', 72.23200009033204)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-239.pth.tar', 72.22400007568359)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-233.pth.tar', 72.12000002197266)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-228.pth.tar', 72.03200010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-220.pth.tar', 72.0120000439453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-240.pth.tar', 71.9880000439453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-238.pth.tar', 71.98800000976563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-219.pth.tar', 71.98000009277344)

Train: 242 [   0/1251 (  0%)]  Loss: 3.963 (3.96)  Time: 2.055s,  498.37/s  (2.055s,  498.37/s)  LR: 6.530e-04  Data: 1.438 (1.438)
Train: 242 [  50/1251 (  4%)]  Loss: 3.565 (3.76)  Time: 0.728s, 1405.96/s  (0.729s, 1405.22/s)  LR: 6.530e-04  Data: 0.011 (0.045)
Train: 242 [ 100/1251 (  8%)]  Loss: 3.719 (3.75)  Time: 0.713s, 1435.75/s  (0.710s, 1441.65/s)  LR: 6.530e-04  Data: 0.013 (0.028)
Train: 242 [ 150/1251 ( 12%)]  Loss: 3.507 (3.69)  Time: 0.711s, 1440.20/s  (0.706s, 1451.28/s)  LR: 6.530e-04  Data: 0.009 (0.022)
Train: 242 [ 200/1251 ( 16%)]  Loss: 3.984 (3.75)  Time: 0.671s, 1526.31/s  (0.702s, 1458.37/s)  LR: 6.530e-04  Data: 0.009 (0.019)
Train: 242 [ 250/1251 ( 20%)]  Loss: 3.689 (3.74)  Time: 0.674s, 1519.16/s  (0.701s, 1460.50/s)  LR: 6.530e-04  Data: 0.010 (0.017)
Train: 242 [ 300/1251 ( 24%)]  Loss: 3.710 (3.73)  Time: 0.706s, 1451.28/s  (0.701s, 1460.43/s)  LR: 6.530e-04  Data: 0.009 (0.016)
Train: 242 [ 350/1251 ( 28%)]  Loss: 3.757 (3.74)  Time: 0.705s, 1452.95/s  (0.699s, 1464.03/s)  LR: 6.530e-04  Data: 0.010 (0.015)
Train: 242 [ 400/1251 ( 32%)]  Loss: 3.473 (3.71)  Time: 0.728s, 1406.09/s  (0.698s, 1466.08/s)  LR: 6.530e-04  Data: 0.011 (0.015)
Train: 242 [ 450/1251 ( 36%)]  Loss: 3.636 (3.70)  Time: 0.674s, 1520.16/s  (0.697s, 1468.87/s)  LR: 6.530e-04  Data: 0.010 (0.014)
Train: 242 [ 500/1251 ( 40%)]  Loss: 4.017 (3.73)  Time: 0.690s, 1484.51/s  (0.697s, 1469.34/s)  LR: 6.530e-04  Data: 0.010 (0.014)
Train: 242 [ 550/1251 ( 44%)]  Loss: 3.476 (3.71)  Time: 0.673s, 1520.84/s  (0.696s, 1471.00/s)  LR: 6.530e-04  Data: 0.011 (0.013)
Train: 242 [ 600/1251 ( 48%)]  Loss: 3.606 (3.70)  Time: 0.696s, 1470.75/s  (0.696s, 1472.18/s)  LR: 6.530e-04  Data: 0.009 (0.013)
Train: 242 [ 650/1251 ( 52%)]  Loss: 3.836 (3.71)  Time: 0.672s, 1524.00/s  (0.695s, 1473.22/s)  LR: 6.530e-04  Data: 0.009 (0.013)
Train: 242 [ 700/1251 ( 56%)]  Loss: 3.806 (3.72)  Time: 0.672s, 1523.26/s  (0.694s, 1474.79/s)  LR: 6.530e-04  Data: 0.010 (0.013)
Train: 242 [ 750/1251 ( 60%)]  Loss: 3.542 (3.71)  Time: 0.720s, 1422.14/s  (0.694s, 1475.59/s)  LR: 6.530e-04  Data: 0.010 (0.012)
Train: 242 [ 800/1251 ( 64%)]  Loss: 3.977 (3.72)  Time: 0.677s, 1513.06/s  (0.694s, 1475.64/s)  LR: 6.530e-04  Data: 0.010 (0.012)
Train: 242 [ 850/1251 ( 68%)]  Loss: 3.794 (3.73)  Time: 0.667s, 1535.70/s  (0.694s, 1475.60/s)  LR: 6.530e-04  Data: 0.009 (0.012)
Train: 242 [ 900/1251 ( 72%)]  Loss: 3.614 (3.72)  Time: 0.673s, 1521.64/s  (0.694s, 1475.31/s)  LR: 6.530e-04  Data: 0.009 (0.012)
Train: 242 [ 950/1251 ( 76%)]  Loss: 3.876 (3.73)  Time: 0.733s, 1396.06/s  (0.694s, 1475.65/s)  LR: 6.530e-04  Data: 0.011 (0.012)
Train: 242 [1000/1251 ( 80%)]  Loss: 3.832 (3.73)  Time: 0.679s, 1508.59/s  (0.694s, 1475.21/s)  LR: 6.530e-04  Data: 0.009 (0.012)
Train: 242 [1050/1251 ( 84%)]  Loss: 4.135 (3.75)  Time: 0.669s, 1531.08/s  (0.694s, 1475.70/s)  LR: 6.530e-04  Data: 0.012 (0.012)
Train: 242 [1100/1251 ( 88%)]  Loss: 3.944 (3.76)  Time: 0.704s, 1454.21/s  (0.694s, 1475.57/s)  LR: 6.530e-04  Data: 0.009 (0.012)
Train: 242 [1150/1251 ( 92%)]  Loss: 3.703 (3.76)  Time: 0.705s, 1453.05/s  (0.694s, 1475.70/s)  LR: 6.530e-04  Data: 0.009 (0.012)
Train: 242 [1200/1251 ( 96%)]  Loss: 3.757 (3.76)  Time: 0.723s, 1416.34/s  (0.694s, 1475.99/s)  LR: 6.530e-04  Data: 0.010 (0.012)
Train: 242 [1250/1251 (100%)]  Loss: 3.555 (3.75)  Time: 0.657s, 1558.78/s  (0.694s, 1476.00/s)  LR: 6.530e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.628 (1.628)  Loss:  0.6963 (0.6963)  Acc@1: 89.1602 (89.1602)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.136 (0.591)  Loss:  0.8691 (1.3640)  Acc@1: 83.1368 (72.2440)  Acc@5: 95.6368 (90.9580)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-241.pth.tar', 72.53999996826172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-242.pth.tar', 72.2440000439453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-229.pth.tar', 72.24199998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-234.pth.tar', 72.23200009033204)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-239.pth.tar', 72.22400007568359)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-233.pth.tar', 72.12000002197266)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-228.pth.tar', 72.03200010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-220.pth.tar', 72.0120000439453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-240.pth.tar', 71.9880000439453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-238.pth.tar', 71.98800000976563)

Train: 243 [   0/1251 (  0%)]  Loss: 3.804 (3.80)  Time: 2.064s,  496.14/s  (2.064s,  496.14/s)  LR: 6.505e-04  Data: 1.449 (1.449)
Train: 243 [  50/1251 (  4%)]  Loss: 3.969 (3.89)  Time: 0.705s, 1452.71/s  (0.729s, 1404.43/s)  LR: 6.505e-04  Data: 0.012 (0.048)
Train: 243 [ 100/1251 (  8%)]  Loss: 3.607 (3.79)  Time: 0.714s, 1434.20/s  (0.713s, 1436.15/s)  LR: 6.505e-04  Data: 0.009 (0.030)
Train: 243 [ 150/1251 ( 12%)]  Loss: 3.698 (3.77)  Time: 0.665s, 1539.55/s  (0.705s, 1451.50/s)  LR: 6.505e-04  Data: 0.009 (0.023)
Train: 243 [ 200/1251 ( 16%)]  Loss: 3.968 (3.81)  Time: 0.682s, 1501.74/s  (0.704s, 1454.71/s)  LR: 6.505e-04  Data: 0.009 (0.020)
Train: 243 [ 250/1251 ( 20%)]  Loss: 3.738 (3.80)  Time: 0.669s, 1530.31/s  (0.703s, 1457.02/s)  LR: 6.505e-04  Data: 0.011 (0.018)
Train: 243 [ 300/1251 ( 24%)]  Loss: 3.721 (3.79)  Time: 0.717s, 1428.40/s  (0.700s, 1463.24/s)  LR: 6.505e-04  Data: 0.009 (0.017)
Train: 243 [ 350/1251 ( 28%)]  Loss: 3.579 (3.76)  Time: 0.670s, 1528.83/s  (0.700s, 1462.76/s)  LR: 6.505e-04  Data: 0.010 (0.016)
Train: 243 [ 400/1251 ( 32%)]  Loss: 3.505 (3.73)  Time: 0.707s, 1448.52/s  (0.699s, 1464.91/s)  LR: 6.505e-04  Data: 0.009 (0.015)
Train: 243 [ 450/1251 ( 36%)]  Loss: 3.958 (3.75)  Time: 0.840s, 1219.37/s  (0.698s, 1466.50/s)  LR: 6.505e-04  Data: 0.009 (0.015)
Train: 243 [ 500/1251 ( 40%)]  Loss: 3.765 (3.76)  Time: 0.670s, 1529.14/s  (0.698s, 1468.04/s)  LR: 6.505e-04  Data: 0.010 (0.014)
Train: 243 [ 550/1251 ( 44%)]  Loss: 3.730 (3.75)  Time: 0.672s, 1523.14/s  (0.697s, 1468.64/s)  LR: 6.505e-04  Data: 0.010 (0.014)
Train: 243 [ 600/1251 ( 48%)]  Loss: 3.774 (3.76)  Time: 0.692s, 1479.36/s  (0.697s, 1468.18/s)  LR: 6.505e-04  Data: 0.015 (0.014)
Train: 243 [ 650/1251 ( 52%)]  Loss: 3.863 (3.76)  Time: 0.704s, 1454.29/s  (0.697s, 1468.84/s)  LR: 6.505e-04  Data: 0.014 (0.013)
Train: 243 [ 700/1251 ( 56%)]  Loss: 3.759 (3.76)  Time: 0.727s, 1407.81/s  (0.697s, 1469.03/s)  LR: 6.505e-04  Data: 0.012 (0.013)
Train: 243 [ 750/1251 ( 60%)]  Loss: 3.677 (3.76)  Time: 0.690s, 1484.88/s  (0.697s, 1469.35/s)  LR: 6.505e-04  Data: 0.010 (0.013)
Train: 243 [ 800/1251 ( 64%)]  Loss: 3.529 (3.74)  Time: 0.674s, 1519.98/s  (0.696s, 1470.29/s)  LR: 6.505e-04  Data: 0.011 (0.013)
Train: 243 [ 850/1251 ( 68%)]  Loss: 3.798 (3.75)  Time: 0.716s, 1429.40/s  (0.696s, 1470.53/s)  LR: 6.505e-04  Data: 0.009 (0.013)
Train: 243 [ 900/1251 ( 72%)]  Loss: 3.692 (3.74)  Time: 0.719s, 1425.19/s  (0.696s, 1471.41/s)  LR: 6.505e-04  Data: 0.009 (0.012)
Train: 243 [ 950/1251 ( 76%)]  Loss: 3.722 (3.74)  Time: 0.704s, 1455.47/s  (0.696s, 1471.73/s)  LR: 6.505e-04  Data: 0.009 (0.012)
Train: 243 [1000/1251 ( 80%)]  Loss: 3.885 (3.75)  Time: 0.668s, 1532.90/s  (0.695s, 1472.86/s)  LR: 6.505e-04  Data: 0.009 (0.012)
Train: 243 [1050/1251 ( 84%)]  Loss: 3.806 (3.75)  Time: 0.722s, 1419.03/s  (0.695s, 1474.11/s)  LR: 6.505e-04  Data: 0.009 (0.012)
Train: 243 [1100/1251 ( 88%)]  Loss: 3.606 (3.75)  Time: 0.705s, 1452.06/s  (0.695s, 1473.26/s)  LR: 6.505e-04  Data: 0.011 (0.012)
Train: 243 [1150/1251 ( 92%)]  Loss: 3.949 (3.75)  Time: 0.672s, 1524.27/s  (0.695s, 1473.64/s)  LR: 6.505e-04  Data: 0.012 (0.012)
Train: 243 [1200/1251 ( 96%)]  Loss: 4.022 (3.76)  Time: 0.673s, 1520.55/s  (0.695s, 1473.24/s)  LR: 6.505e-04  Data: 0.010 (0.012)
Train: 243 [1250/1251 (100%)]  Loss: 3.616 (3.76)  Time: 0.658s, 1556.71/s  (0.695s, 1473.88/s)  LR: 6.505e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.483 (1.483)  Loss:  1.1426 (1.1426)  Acc@1: 87.5000 (87.5000)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.136 (0.588)  Loss:  1.1875 (1.6403)  Acc@1: 81.7217 (71.7220)  Acc@5: 95.2830 (90.8840)
Train: 244 [   0/1251 (  0%)]  Loss: 3.996 (4.00)  Time: 2.260s,  453.18/s  (2.260s,  453.18/s)  LR: 6.481e-04  Data: 1.591 (1.591)
Train: 244 [  50/1251 (  4%)]  Loss: 4.264 (4.13)  Time: 0.673s, 1522.31/s  (0.729s, 1403.78/s)  LR: 6.481e-04  Data: 0.010 (0.047)
Train: 244 [ 100/1251 (  8%)]  Loss: 3.832 (4.03)  Time: 0.681s, 1503.58/s  (0.711s, 1439.85/s)  LR: 6.481e-04  Data: 0.011 (0.029)
Train: 244 [ 150/1251 ( 12%)]  Loss: 3.624 (3.93)  Time: 0.703s, 1457.35/s  (0.704s, 1453.71/s)  LR: 6.481e-04  Data: 0.011 (0.023)
Train: 244 [ 200/1251 ( 16%)]  Loss: 3.720 (3.89)  Time: 0.684s, 1496.32/s  (0.704s, 1455.52/s)  LR: 6.481e-04  Data: 0.011 (0.020)
Train: 244 [ 250/1251 ( 20%)]  Loss: 3.677 (3.85)  Time: 0.672s, 1524.13/s  (0.701s, 1461.40/s)  LR: 6.481e-04  Data: 0.010 (0.018)
Train: 244 [ 300/1251 ( 24%)]  Loss: 3.877 (3.86)  Time: 0.709s, 1444.85/s  (0.701s, 1461.48/s)  LR: 6.481e-04  Data: 0.009 (0.016)
Train: 244 [ 350/1251 ( 28%)]  Loss: 3.691 (3.84)  Time: 0.687s, 1489.59/s  (0.699s, 1464.27/s)  LR: 6.481e-04  Data: 0.011 (0.016)
Train: 244 [ 400/1251 ( 32%)]  Loss: 3.936 (3.85)  Time: 0.673s, 1522.48/s  (0.699s, 1465.33/s)  LR: 6.481e-04  Data: 0.011 (0.015)
Train: 244 [ 450/1251 ( 36%)]  Loss: 3.697 (3.83)  Time: 0.672s, 1523.36/s  (0.698s, 1467.51/s)  LR: 6.481e-04  Data: 0.012 (0.014)
Train: 244 [ 500/1251 ( 40%)]  Loss: 3.560 (3.81)  Time: 0.716s, 1431.14/s  (0.697s, 1468.82/s)  LR: 6.481e-04  Data: 0.010 (0.014)
Train: 244 [ 550/1251 ( 44%)]  Loss: 3.401 (3.77)  Time: 0.669s, 1530.44/s  (0.697s, 1469.55/s)  LR: 6.481e-04  Data: 0.009 (0.014)
Train: 244 [ 600/1251 ( 48%)]  Loss: 3.731 (3.77)  Time: 0.704s, 1454.53/s  (0.696s, 1470.69/s)  LR: 6.481e-04  Data: 0.009 (0.013)
Train: 244 [ 650/1251 ( 52%)]  Loss: 3.845 (3.78)  Time: 0.708s, 1445.40/s  (0.697s, 1470.09/s)  LR: 6.481e-04  Data: 0.010 (0.013)
Train: 244 [ 700/1251 ( 56%)]  Loss: 3.361 (3.75)  Time: 0.673s, 1520.57/s  (0.696s, 1471.57/s)  LR: 6.481e-04  Data: 0.010 (0.013)
Train: 244 [ 750/1251 ( 60%)]  Loss: 3.563 (3.74)  Time: 0.705s, 1452.79/s  (0.696s, 1471.27/s)  LR: 6.481e-04  Data: 0.011 (0.013)
Train: 244 [ 800/1251 ( 64%)]  Loss: 3.540 (3.72)  Time: 0.673s, 1522.34/s  (0.696s, 1471.37/s)  LR: 6.481e-04  Data: 0.010 (0.013)
Train: 244 [ 850/1251 ( 68%)]  Loss: 3.605 (3.72)  Time: 0.681s, 1503.51/s  (0.696s, 1471.89/s)  LR: 6.481e-04  Data: 0.010 (0.012)
Train: 244 [ 900/1251 ( 72%)]  Loss: 3.716 (3.72)  Time: 0.705s, 1451.94/s  (0.695s, 1472.69/s)  LR: 6.481e-04  Data: 0.009 (0.012)
Train: 244 [ 950/1251 ( 76%)]  Loss: 3.466 (3.71)  Time: 0.672s, 1524.51/s  (0.695s, 1473.42/s)  LR: 6.481e-04  Data: 0.009 (0.012)
Train: 244 [1000/1251 ( 80%)]  Loss: 3.809 (3.71)  Time: 0.672s, 1524.73/s  (0.695s, 1472.34/s)  LR: 6.481e-04  Data: 0.011 (0.012)
Train: 244 [1050/1251 ( 84%)]  Loss: 3.749 (3.71)  Time: 0.691s, 1481.21/s  (0.696s, 1471.73/s)  LR: 6.481e-04  Data: 0.010 (0.012)
Train: 244 [1100/1251 ( 88%)]  Loss: 3.493 (3.70)  Time: 0.706s, 1450.38/s  (0.696s, 1471.15/s)  LR: 6.481e-04  Data: 0.014 (0.012)
Train: 244 [1150/1251 ( 92%)]  Loss: 3.881 (3.71)  Time: 0.706s, 1449.99/s  (0.696s, 1471.47/s)  LR: 6.481e-04  Data: 0.011 (0.012)
Train: 244 [1200/1251 ( 96%)]  Loss: 4.177 (3.73)  Time: 0.717s, 1428.05/s  (0.696s, 1471.73/s)  LR: 6.481e-04  Data: 0.009 (0.012)
Train: 244 [1250/1251 (100%)]  Loss: 3.954 (3.74)  Time: 0.655s, 1564.03/s  (0.696s, 1471.74/s)  LR: 6.481e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.581 (1.581)  Loss:  0.8755 (0.8755)  Acc@1: 87.9883 (87.9883)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.136 (0.578)  Loss:  0.9229 (1.4954)  Acc@1: 83.4906 (71.8040)  Acc@5: 96.5802 (90.6880)
Train: 245 [   0/1251 (  0%)]  Loss: 3.701 (3.70)  Time: 2.236s,  457.86/s  (2.236s,  457.86/s)  LR: 6.456e-04  Data: 1.597 (1.597)
Train: 245 [  50/1251 (  4%)]  Loss: 3.394 (3.55)  Time: 0.709s, 1444.75/s  (0.738s, 1387.67/s)  LR: 6.456e-04  Data: 0.009 (0.048)
Train: 245 [ 100/1251 (  8%)]  Loss: 3.830 (3.64)  Time: 0.768s, 1333.69/s  (0.718s, 1426.25/s)  LR: 6.456e-04  Data: 0.009 (0.029)
Train: 245 [ 150/1251 ( 12%)]  Loss: 3.888 (3.70)  Time: 0.669s, 1530.46/s  (0.711s, 1439.60/s)  LR: 6.456e-04  Data: 0.009 (0.023)
Train: 245 [ 200/1251 ( 16%)]  Loss: 3.443 (3.65)  Time: 0.671s, 1526.24/s  (0.708s, 1445.90/s)  LR: 6.456e-04  Data: 0.011 (0.020)
Train: 245 [ 250/1251 ( 20%)]  Loss: 3.691 (3.66)  Time: 0.698s, 1466.99/s  (0.705s, 1453.06/s)  LR: 6.456e-04  Data: 0.009 (0.018)
Train: 245 [ 300/1251 ( 24%)]  Loss: 3.524 (3.64)  Time: 0.673s, 1522.07/s  (0.703s, 1457.13/s)  LR: 6.456e-04  Data: 0.009 (0.017)
Train: 245 [ 350/1251 ( 28%)]  Loss: 3.750 (3.65)  Time: 0.671s, 1527.08/s  (0.700s, 1462.74/s)  LR: 6.456e-04  Data: 0.012 (0.016)
Train: 245 [ 400/1251 ( 32%)]  Loss: 3.916 (3.68)  Time: 0.671s, 1525.79/s  (0.700s, 1463.17/s)  LR: 6.456e-04  Data: 0.012 (0.015)
Train: 245 [ 450/1251 ( 36%)]  Loss: 4.154 (3.73)  Time: 0.706s, 1449.99/s  (0.700s, 1463.58/s)  LR: 6.456e-04  Data: 0.009 (0.014)
Train: 245 [ 500/1251 ( 40%)]  Loss: 3.547 (3.71)  Time: 0.672s, 1523.87/s  (0.699s, 1465.73/s)  LR: 6.456e-04  Data: 0.009 (0.014)
Train: 245 [ 550/1251 ( 44%)]  Loss: 3.853 (3.72)  Time: 0.688s, 1487.30/s  (0.699s, 1465.59/s)  LR: 6.456e-04  Data: 0.016 (0.014)
Train: 245 [ 600/1251 ( 48%)]  Loss: 3.746 (3.73)  Time: 0.759s, 1348.77/s  (0.698s, 1466.01/s)  LR: 6.456e-04  Data: 0.009 (0.013)
Train: 245 [ 650/1251 ( 52%)]  Loss: 3.974 (3.74)  Time: 0.670s, 1527.87/s  (0.697s, 1468.15/s)  LR: 6.456e-04  Data: 0.010 (0.013)
Train: 245 [ 700/1251 ( 56%)]  Loss: 3.889 (3.75)  Time: 0.671s, 1526.85/s  (0.697s, 1468.54/s)  LR: 6.456e-04  Data: 0.009 (0.013)
Train: 245 [ 750/1251 ( 60%)]  Loss: 3.915 (3.76)  Time: 0.671s, 1526.10/s  (0.697s, 1468.74/s)  LR: 6.456e-04  Data: 0.009 (0.013)
Train: 245 [ 800/1251 ( 64%)]  Loss: 3.423 (3.74)  Time: 0.674s, 1518.23/s  (0.696s, 1470.35/s)  LR: 6.456e-04  Data: 0.010 (0.013)
Train: 245 [ 850/1251 ( 68%)]  Loss: 3.705 (3.74)  Time: 0.724s, 1414.74/s  (0.697s, 1469.81/s)  LR: 6.456e-04  Data: 0.010 (0.012)
Train: 245 [ 900/1251 ( 72%)]  Loss: 4.149 (3.76)  Time: 0.749s, 1368.03/s  (0.696s, 1470.57/s)  LR: 6.456e-04  Data: 0.009 (0.012)
Train: 245 [ 950/1251 ( 76%)]  Loss: 3.526 (3.75)  Time: 0.703s, 1457.52/s  (0.696s, 1470.81/s)  LR: 6.456e-04  Data: 0.010 (0.012)
Train: 245 [1000/1251 ( 80%)]  Loss: 3.834 (3.75)  Time: 0.683s, 1499.01/s  (0.696s, 1471.20/s)  LR: 6.456e-04  Data: 0.014 (0.012)
Train: 245 [1050/1251 ( 84%)]  Loss: 3.324 (3.74)  Time: 0.703s, 1457.34/s  (0.696s, 1471.45/s)  LR: 6.456e-04  Data: 0.009 (0.012)
Train: 245 [1100/1251 ( 88%)]  Loss: 3.653 (3.73)  Time: 0.748s, 1368.62/s  (0.696s, 1471.04/s)  LR: 6.456e-04  Data: 0.010 (0.012)
Train: 245 [1150/1251 ( 92%)]  Loss: 4.001 (3.74)  Time: 0.676s, 1513.93/s  (0.696s, 1471.61/s)  LR: 6.456e-04  Data: 0.009 (0.012)
Train: 245 [1200/1251 ( 96%)]  Loss: 3.962 (3.75)  Time: 0.674s, 1519.72/s  (0.696s, 1472.04/s)  LR: 6.456e-04  Data: 0.009 (0.012)
Train: 245 [1250/1251 (100%)]  Loss: 4.194 (3.77)  Time: 0.693s, 1478.16/s  (0.695s, 1472.53/s)  LR: 6.456e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.521 (1.521)  Loss:  1.1035 (1.1035)  Acc@1: 87.5000 (87.5000)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.136 (0.589)  Loss:  1.0488 (1.5508)  Acc@1: 84.0802 (71.7680)  Acc@5: 96.1085 (90.7580)
Train: 246 [   0/1251 (  0%)]  Loss: 3.769 (3.77)  Time: 2.124s,  482.19/s  (2.124s,  482.19/s)  LR: 6.431e-04  Data: 1.509 (1.509)
Train: 246 [  50/1251 (  4%)]  Loss: 3.768 (3.77)  Time: 0.672s, 1522.75/s  (0.726s, 1409.77/s)  LR: 6.431e-04  Data: 0.010 (0.045)
Train: 246 [ 100/1251 (  8%)]  Loss: 3.369 (3.64)  Time: 0.671s, 1525.69/s  (0.713s, 1436.69/s)  LR: 6.431e-04  Data: 0.010 (0.028)
Train: 246 [ 150/1251 ( 12%)]  Loss: 4.062 (3.74)  Time: 0.694s, 1475.35/s  (0.705s, 1451.69/s)  LR: 6.431e-04  Data: 0.009 (0.022)
Train: 246 [ 200/1251 ( 16%)]  Loss: 3.732 (3.74)  Time: 0.734s, 1394.44/s  (0.702s, 1459.51/s)  LR: 6.431e-04  Data: 0.010 (0.019)
Train: 246 [ 250/1251 ( 20%)]  Loss: 3.868 (3.76)  Time: 0.668s, 1533.00/s  (0.700s, 1462.59/s)  LR: 6.431e-04  Data: 0.011 (0.017)
Train: 246 [ 300/1251 ( 24%)]  Loss: 3.775 (3.76)  Time: 0.672s, 1524.69/s  (0.700s, 1462.90/s)  LR: 6.431e-04  Data: 0.010 (0.016)
Train: 246 [ 350/1251 ( 28%)]  Loss: 3.438 (3.72)  Time: 0.676s, 1513.73/s  (0.699s, 1465.00/s)  LR: 6.431e-04  Data: 0.010 (0.015)
Train: 246 [ 400/1251 ( 32%)]  Loss: 4.084 (3.76)  Time: 0.738s, 1387.94/s  (0.698s, 1466.11/s)  LR: 6.431e-04  Data: 0.009 (0.015)
Train: 246 [ 450/1251 ( 36%)]  Loss: 3.847 (3.77)  Time: 0.709s, 1444.88/s  (0.698s, 1466.33/s)  LR: 6.431e-04  Data: 0.010 (0.014)
Train: 246 [ 500/1251 ( 40%)]  Loss: 3.862 (3.78)  Time: 0.711s, 1440.12/s  (0.698s, 1468.05/s)  LR: 6.431e-04  Data: 0.009 (0.014)
Train: 246 [ 550/1251 ( 44%)]  Loss: 3.386 (3.75)  Time: 0.692s, 1480.81/s  (0.698s, 1466.20/s)  LR: 6.431e-04  Data: 0.010 (0.014)
Train: 246 [ 600/1251 ( 48%)]  Loss: 3.471 (3.73)  Time: 0.697s, 1469.61/s  (0.698s, 1467.54/s)  LR: 6.431e-04  Data: 0.010 (0.013)
Train: 246 [ 650/1251 ( 52%)]  Loss: 4.166 (3.76)  Time: 0.674s, 1519.90/s  (0.697s, 1468.21/s)  LR: 6.431e-04  Data: 0.011 (0.013)
Train: 246 [ 700/1251 ( 56%)]  Loss: 3.752 (3.76)  Time: 0.686s, 1492.62/s  (0.698s, 1466.21/s)  LR: 6.431e-04  Data: 0.013 (0.013)
Train: 246 [ 750/1251 ( 60%)]  Loss: 3.715 (3.75)  Time: 0.718s, 1426.92/s  (0.699s, 1464.19/s)  LR: 6.431e-04  Data: 0.010 (0.013)
Train: 246 [ 800/1251 ( 64%)]  Loss: 3.607 (3.75)  Time: 0.677s, 1513.09/s  (0.700s, 1462.40/s)  LR: 6.431e-04  Data: 0.010 (0.013)
Train: 246 [ 850/1251 ( 68%)]  Loss: 3.668 (3.74)  Time: 0.673s, 1521.92/s  (0.700s, 1462.46/s)  LR: 6.431e-04  Data: 0.009 (0.013)
Train: 246 [ 900/1251 ( 72%)]  Loss: 3.612 (3.73)  Time: 0.669s, 1530.17/s  (0.699s, 1464.39/s)  LR: 6.431e-04  Data: 0.009 (0.013)
Train: 246 [ 950/1251 ( 76%)]  Loss: 3.498 (3.72)  Time: 0.673s, 1522.29/s  (0.699s, 1465.46/s)  LR: 6.431e-04  Data: 0.011 (0.013)
Train: 246 [1000/1251 ( 80%)]  Loss: 3.791 (3.73)  Time: 0.713s, 1436.11/s  (0.698s, 1467.13/s)  LR: 6.431e-04  Data: 0.009 (0.012)
Train: 246 [1050/1251 ( 84%)]  Loss: 3.622 (3.72)  Time: 0.682s, 1500.98/s  (0.698s, 1467.70/s)  LR: 6.431e-04  Data: 0.009 (0.012)
Train: 246 [1100/1251 ( 88%)]  Loss: 3.861 (3.73)  Time: 0.672s, 1524.25/s  (0.698s, 1467.58/s)  LR: 6.431e-04  Data: 0.009 (0.012)
Train: 246 [1150/1251 ( 92%)]  Loss: 3.552 (3.72)  Time: 0.713s, 1437.06/s  (0.697s, 1468.12/s)  LR: 6.431e-04  Data: 0.009 (0.012)
Train: 246 [1200/1251 ( 96%)]  Loss: 3.965 (3.73)  Time: 0.722s, 1419.21/s  (0.697s, 1468.77/s)  LR: 6.431e-04  Data: 0.010 (0.012)
Train: 246 [1250/1251 (100%)]  Loss: 3.423 (3.72)  Time: 0.654s, 1565.06/s  (0.697s, 1469.31/s)  LR: 6.431e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.592 (1.592)  Loss:  0.9673 (0.9673)  Acc@1: 87.5977 (87.5977)  Acc@5: 96.1914 (96.1914)
Test: [  48/48]  Time: 0.136 (0.583)  Loss:  0.9507 (1.4845)  Acc@1: 85.2594 (72.1200)  Acc@5: 96.9340 (90.7420)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-241.pth.tar', 72.53999996826172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-242.pth.tar', 72.2440000439453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-229.pth.tar', 72.24199998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-234.pth.tar', 72.23200009033204)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-239.pth.tar', 72.22400007568359)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-246.pth.tar', 72.12000006103516)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-233.pth.tar', 72.12000002197266)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-228.pth.tar', 72.03200010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-220.pth.tar', 72.0120000439453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-240.pth.tar', 71.9880000439453)

Train: 247 [   0/1251 (  0%)]  Loss: 3.968 (3.97)  Time: 2.151s,  476.14/s  (2.151s,  476.14/s)  LR: 6.406e-04  Data: 1.534 (1.534)
Train: 247 [  50/1251 (  4%)]  Loss: 3.873 (3.92)  Time: 0.674s, 1519.11/s  (0.742s, 1380.32/s)  LR: 6.406e-04  Data: 0.013 (0.051)
Train: 247 [ 100/1251 (  8%)]  Loss: 3.791 (3.88)  Time: 0.668s, 1533.59/s  (0.720s, 1422.05/s)  LR: 6.406e-04  Data: 0.009 (0.031)
Train: 247 [ 150/1251 ( 12%)]  Loss: 3.676 (3.83)  Time: 0.676s, 1513.93/s  (0.713s, 1435.88/s)  LR: 6.406e-04  Data: 0.009 (0.024)
Train: 247 [ 200/1251 ( 16%)]  Loss: 3.652 (3.79)  Time: 0.712s, 1438.14/s  (0.708s, 1446.08/s)  LR: 6.406e-04  Data: 0.009 (0.020)
Train: 247 [ 250/1251 ( 20%)]  Loss: 3.675 (3.77)  Time: 0.672s, 1523.82/s  (0.705s, 1452.65/s)  LR: 6.406e-04  Data: 0.010 (0.018)
Train: 247 [ 300/1251 ( 24%)]  Loss: 3.523 (3.74)  Time: 0.704s, 1453.82/s  (0.702s, 1458.68/s)  LR: 6.406e-04  Data: 0.009 (0.017)
Train: 247 [ 350/1251 ( 28%)]  Loss: 3.897 (3.76)  Time: 0.704s, 1454.77/s  (0.701s, 1460.95/s)  LR: 6.406e-04  Data: 0.013 (0.016)
Train: 247 [ 400/1251 ( 32%)]  Loss: 4.195 (3.81)  Time: 0.769s, 1331.90/s  (0.701s, 1460.93/s)  LR: 6.406e-04  Data: 0.010 (0.015)
Train: 247 [ 450/1251 ( 36%)]  Loss: 3.490 (3.77)  Time: 0.754s, 1358.79/s  (0.700s, 1463.18/s)  LR: 6.406e-04  Data: 0.010 (0.015)
Train: 247 [ 500/1251 ( 40%)]  Loss: 3.611 (3.76)  Time: 0.736s, 1391.03/s  (0.700s, 1462.88/s)  LR: 6.406e-04  Data: 0.009 (0.014)
Train: 247 [ 550/1251 ( 44%)]  Loss: 3.617 (3.75)  Time: 0.671s, 1526.33/s  (0.699s, 1464.82/s)  LR: 6.406e-04  Data: 0.009 (0.014)
Train: 247 [ 600/1251 ( 48%)]  Loss: 3.628 (3.74)  Time: 0.700s, 1461.91/s  (0.699s, 1465.91/s)  LR: 6.406e-04  Data: 0.009 (0.014)
Train: 247 [ 650/1251 ( 52%)]  Loss: 3.592 (3.73)  Time: 0.683s, 1498.67/s  (0.698s, 1466.53/s)  LR: 6.406e-04  Data: 0.009 (0.013)
Train: 247 [ 700/1251 ( 56%)]  Loss: 3.931 (3.74)  Time: 0.695s, 1472.87/s  (0.698s, 1466.75/s)  LR: 6.406e-04  Data: 0.010 (0.013)
Train: 247 [ 750/1251 ( 60%)]  Loss: 3.456 (3.72)  Time: 0.682s, 1502.16/s  (0.698s, 1467.58/s)  LR: 6.406e-04  Data: 0.010 (0.013)
Train: 247 [ 800/1251 ( 64%)]  Loss: 4.060 (3.74)  Time: 0.674s, 1518.31/s  (0.697s, 1468.72/s)  LR: 6.406e-04  Data: 0.010 (0.013)
Train: 247 [ 850/1251 ( 68%)]  Loss: 3.389 (3.72)  Time: 0.675s, 1516.88/s  (0.697s, 1468.81/s)  LR: 6.406e-04  Data: 0.010 (0.013)
Train: 247 [ 900/1251 ( 72%)]  Loss: 3.512 (3.71)  Time: 0.670s, 1528.90/s  (0.697s, 1470.04/s)  LR: 6.406e-04  Data: 0.010 (0.012)
Train: 247 [ 950/1251 ( 76%)]  Loss: 3.473 (3.70)  Time: 0.708s, 1445.82/s  (0.696s, 1470.89/s)  LR: 6.406e-04  Data: 0.008 (0.012)
Train: 247 [1000/1251 ( 80%)]  Loss: 3.524 (3.69)  Time: 0.708s, 1445.77/s  (0.696s, 1470.74/s)  LR: 6.406e-04  Data: 0.010 (0.012)
Train: 247 [1050/1251 ( 84%)]  Loss: 3.768 (3.70)  Time: 0.681s, 1503.13/s  (0.696s, 1471.23/s)  LR: 6.406e-04  Data: 0.013 (0.012)
Train: 247 [1100/1251 ( 88%)]  Loss: 3.869 (3.70)  Time: 0.736s, 1391.16/s  (0.696s, 1471.63/s)  LR: 6.406e-04  Data: 0.008 (0.012)
Train: 247 [1150/1251 ( 92%)]  Loss: 3.517 (3.70)  Time: 0.677s, 1511.95/s  (0.696s, 1472.23/s)  LR: 6.406e-04  Data: 0.015 (0.012)
Train: 247 [1200/1251 ( 96%)]  Loss: 4.013 (3.71)  Time: 0.672s, 1523.30/s  (0.695s, 1472.75/s)  LR: 6.406e-04  Data: 0.010 (0.012)
Train: 247 [1250/1251 (100%)]  Loss: 3.456 (3.70)  Time: 0.655s, 1562.75/s  (0.695s, 1472.52/s)  LR: 6.406e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.594 (1.594)  Loss:  0.9004 (0.9004)  Acc@1: 89.0625 (89.0625)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.136 (0.577)  Loss:  1.0166 (1.4614)  Acc@1: 83.8443 (72.3560)  Acc@5: 96.4623 (90.9260)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-241.pth.tar', 72.53999996826172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-247.pth.tar', 72.35600009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-242.pth.tar', 72.2440000439453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-229.pth.tar', 72.24199998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-234.pth.tar', 72.23200009033204)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-239.pth.tar', 72.22400007568359)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-246.pth.tar', 72.12000006103516)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-233.pth.tar', 72.12000002197266)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-228.pth.tar', 72.03200010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-220.pth.tar', 72.0120000439453)

Train: 248 [   0/1251 (  0%)]  Loss: 3.848 (3.85)  Time: 2.427s,  421.96/s  (2.427s,  421.96/s)  LR: 6.381e-04  Data: 1.759 (1.759)
Train: 248 [  50/1251 (  4%)]  Loss: 3.435 (3.64)  Time: 0.678s, 1510.62/s  (0.732s, 1399.82/s)  LR: 6.381e-04  Data: 0.010 (0.052)
Train: 248 [ 100/1251 (  8%)]  Loss: 3.411 (3.56)  Time: 0.680s, 1505.96/s  (0.711s, 1439.42/s)  LR: 6.381e-04  Data: 0.010 (0.031)
Train: 248 [ 150/1251 ( 12%)]  Loss: 3.814 (3.63)  Time: 0.676s, 1515.00/s  (0.704s, 1453.86/s)  LR: 6.381e-04  Data: 0.010 (0.024)
Train: 248 [ 200/1251 ( 16%)]  Loss: 3.573 (3.62)  Time: 0.674s, 1520.38/s  (0.701s, 1461.18/s)  LR: 6.381e-04  Data: 0.010 (0.021)
Train: 248 [ 250/1251 ( 20%)]  Loss: 3.857 (3.66)  Time: 0.712s, 1438.87/s  (0.699s, 1465.23/s)  LR: 6.381e-04  Data: 0.011 (0.019)
Train: 248 [ 300/1251 ( 24%)]  Loss: 3.945 (3.70)  Time: 0.671s, 1526.34/s  (0.699s, 1464.65/s)  LR: 6.381e-04  Data: 0.010 (0.017)
Train: 248 [ 350/1251 ( 28%)]  Loss: 3.875 (3.72)  Time: 0.704s, 1454.81/s  (0.698s, 1467.40/s)  LR: 6.381e-04  Data: 0.011 (0.016)
Train: 248 [ 400/1251 ( 32%)]  Loss: 3.803 (3.73)  Time: 0.677s, 1513.59/s  (0.697s, 1468.58/s)  LR: 6.381e-04  Data: 0.009 (0.016)
Train: 248 [ 450/1251 ( 36%)]  Loss: 3.858 (3.74)  Time: 0.739s, 1386.47/s  (0.698s, 1467.99/s)  LR: 6.381e-04  Data: 0.017 (0.015)
Train: 248 [ 500/1251 ( 40%)]  Loss: 3.634 (3.73)  Time: 0.665s, 1540.01/s  (0.697s, 1468.36/s)  LR: 6.381e-04  Data: 0.009 (0.014)
Train: 248 [ 550/1251 ( 44%)]  Loss: 3.625 (3.72)  Time: 0.673s, 1522.13/s  (0.697s, 1469.52/s)  LR: 6.381e-04  Data: 0.011 (0.014)
Train: 248 [ 600/1251 ( 48%)]  Loss: 3.940 (3.74)  Time: 0.717s, 1427.91/s  (0.697s, 1470.01/s)  LR: 6.381e-04  Data: 0.010 (0.014)
Train: 248 [ 650/1251 ( 52%)]  Loss: 3.545 (3.73)  Time: 0.735s, 1393.81/s  (0.697s, 1469.37/s)  LR: 6.381e-04  Data: 0.011 (0.013)
Train: 248 [ 700/1251 ( 56%)]  Loss: 3.476 (3.71)  Time: 0.666s, 1536.99/s  (0.697s, 1469.06/s)  LR: 6.381e-04  Data: 0.010 (0.013)
Train: 248 [ 750/1251 ( 60%)]  Loss: 3.459 (3.69)  Time: 0.671s, 1525.98/s  (0.697s, 1469.25/s)  LR: 6.381e-04  Data: 0.010 (0.013)
Train: 248 [ 800/1251 ( 64%)]  Loss: 3.875 (3.70)  Time: 0.667s, 1534.25/s  (0.696s, 1470.21/s)  LR: 6.381e-04  Data: 0.011 (0.013)
Train: 248 [ 850/1251 ( 68%)]  Loss: 3.693 (3.70)  Time: 0.700s, 1462.47/s  (0.696s, 1471.01/s)  LR: 6.381e-04  Data: 0.009 (0.013)
Train: 248 [ 900/1251 ( 72%)]  Loss: 3.839 (3.71)  Time: 0.727s, 1408.75/s  (0.696s, 1471.99/s)  LR: 6.381e-04  Data: 0.010 (0.013)
Train: 248 [ 950/1251 ( 76%)]  Loss: 3.899 (3.72)  Time: 0.689s, 1487.13/s  (0.696s, 1472.25/s)  LR: 6.381e-04  Data: 0.010 (0.012)
Train: 248 [1000/1251 ( 80%)]  Loss: 3.537 (3.71)  Time: 0.708s, 1446.45/s  (0.695s, 1472.35/s)  LR: 6.381e-04  Data: 0.010 (0.012)
Train: 248 [1050/1251 ( 84%)]  Loss: 3.584 (3.71)  Time: 0.674s, 1519.87/s  (0.695s, 1472.81/s)  LR: 6.381e-04  Data: 0.010 (0.012)
Train: 248 [1100/1251 ( 88%)]  Loss: 3.708 (3.71)  Time: 0.727s, 1409.25/s  (0.695s, 1473.40/s)  LR: 6.381e-04  Data: 0.010 (0.012)
Train: 248 [1150/1251 ( 92%)]  Loss: 3.118 (3.68)  Time: 0.672s, 1523.49/s  (0.695s, 1473.49/s)  LR: 6.381e-04  Data: 0.012 (0.012)
Train: 248 [1200/1251 ( 96%)]  Loss: 3.511 (3.67)  Time: 0.714s, 1434.09/s  (0.695s, 1474.19/s)  LR: 6.381e-04  Data: 0.011 (0.012)
Train: 248 [1250/1251 (100%)]  Loss: 3.655 (3.67)  Time: 0.655s, 1562.62/s  (0.694s, 1475.08/s)  LR: 6.381e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.436 (1.436)  Loss:  0.9976 (0.9976)  Acc@1: 87.4023 (87.4023)  Acc@5: 96.0938 (96.0938)
Test: [  48/48]  Time: 0.136 (0.576)  Loss:  0.9487 (1.4897)  Acc@1: 85.3774 (72.2000)  Acc@5: 97.0519 (90.9760)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-241.pth.tar', 72.53999996826172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-247.pth.tar', 72.35600009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-242.pth.tar', 72.2440000439453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-229.pth.tar', 72.24199998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-234.pth.tar', 72.23200009033204)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-239.pth.tar', 72.22400007568359)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-248.pth.tar', 72.19999998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-246.pth.tar', 72.12000006103516)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-233.pth.tar', 72.12000002197266)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-228.pth.tar', 72.03200010986328)

Train: 249 [   0/1251 (  0%)]  Loss: 3.933 (3.93)  Time: 2.115s,  484.24/s  (2.115s,  484.24/s)  LR: 6.356e-04  Data: 1.501 (1.501)
Train: 249 [  50/1251 (  4%)]  Loss: 3.624 (3.78)  Time: 0.675s, 1516.23/s  (0.728s, 1406.81/s)  LR: 6.356e-04  Data: 0.013 (0.049)
Train: 249 [ 100/1251 (  8%)]  Loss: 3.658 (3.74)  Time: 0.672s, 1523.95/s  (0.714s, 1434.82/s)  LR: 6.356e-04  Data: 0.009 (0.030)
Train: 249 [ 150/1251 ( 12%)]  Loss: 3.626 (3.71)  Time: 0.703s, 1457.33/s  (0.708s, 1445.98/s)  LR: 6.356e-04  Data: 0.010 (0.023)
Train: 249 [ 200/1251 ( 16%)]  Loss: 3.975 (3.76)  Time: 0.703s, 1457.36/s  (0.704s, 1455.29/s)  LR: 6.356e-04  Data: 0.009 (0.020)
Train: 249 [ 250/1251 ( 20%)]  Loss: 3.857 (3.78)  Time: 0.682s, 1500.90/s  (0.701s, 1461.36/s)  LR: 6.356e-04  Data: 0.015 (0.018)
Train: 249 [ 300/1251 ( 24%)]  Loss: 3.664 (3.76)  Time: 0.672s, 1523.72/s  (0.698s, 1466.42/s)  LR: 6.356e-04  Data: 0.010 (0.017)
Train: 249 [ 350/1251 ( 28%)]  Loss: 3.822 (3.77)  Time: 0.704s, 1454.14/s  (0.697s, 1469.50/s)  LR: 6.356e-04  Data: 0.010 (0.016)
Train: 249 [ 400/1251 ( 32%)]  Loss: 3.711 (3.76)  Time: 0.675s, 1516.65/s  (0.696s, 1471.29/s)  LR: 6.356e-04  Data: 0.011 (0.015)
Train: 249 [ 450/1251 ( 36%)]  Loss: 3.407 (3.73)  Time: 0.682s, 1502.34/s  (0.696s, 1471.78/s)  LR: 6.356e-04  Data: 0.010 (0.015)
Train: 249 [ 500/1251 ( 40%)]  Loss: 3.537 (3.71)  Time: 0.673s, 1522.49/s  (0.695s, 1472.64/s)  LR: 6.356e-04  Data: 0.011 (0.014)
Train: 249 [ 550/1251 ( 44%)]  Loss: 3.950 (3.73)  Time: 0.692s, 1478.81/s  (0.695s, 1473.53/s)  LR: 6.356e-04  Data: 0.009 (0.014)
Train: 249 [ 600/1251 ( 48%)]  Loss: 3.836 (3.74)  Time: 0.673s, 1522.59/s  (0.694s, 1475.25/s)  LR: 6.356e-04  Data: 0.009 (0.014)
Train: 249 [ 650/1251 ( 52%)]  Loss: 4.029 (3.76)  Time: 0.671s, 1525.17/s  (0.694s, 1475.96/s)  LR: 6.356e-04  Data: 0.009 (0.013)
Train: 249 [ 700/1251 ( 56%)]  Loss: 3.797 (3.76)  Time: 0.721s, 1419.82/s  (0.694s, 1475.61/s)  LR: 6.356e-04  Data: 0.011 (0.013)
Train: 249 [ 750/1251 ( 60%)]  Loss: 3.571 (3.75)  Time: 0.673s, 1521.47/s  (0.694s, 1476.46/s)  LR: 6.356e-04  Data: 0.010 (0.013)
Train: 249 [ 800/1251 ( 64%)]  Loss: 3.770 (3.75)  Time: 0.680s, 1504.93/s  (0.694s, 1475.72/s)  LR: 6.356e-04  Data: 0.013 (0.013)
Train: 249 [ 850/1251 ( 68%)]  Loss: 3.847 (3.76)  Time: 0.762s, 1343.49/s  (0.694s, 1475.85/s)  LR: 6.356e-04  Data: 0.009 (0.013)
Train: 249 [ 900/1251 ( 72%)]  Loss: 3.877 (3.76)  Time: 0.702s, 1457.67/s  (0.694s, 1476.36/s)  LR: 6.356e-04  Data: 0.011 (0.013)
Train: 249 [ 950/1251 ( 76%)]  Loss: 3.802 (3.76)  Time: 0.670s, 1528.48/s  (0.693s, 1476.78/s)  LR: 6.356e-04  Data: 0.010 (0.013)
Train: 249 [1000/1251 ( 80%)]  Loss: 3.688 (3.76)  Time: 0.679s, 1508.49/s  (0.693s, 1476.79/s)  LR: 6.356e-04  Data: 0.015 (0.012)
Train: 249 [1050/1251 ( 84%)]  Loss: 3.152 (3.73)  Time: 0.695s, 1473.32/s  (0.693s, 1476.79/s)  LR: 6.356e-04  Data: 0.009 (0.012)
Train: 249 [1100/1251 ( 88%)]  Loss: 4.075 (3.75)  Time: 0.689s, 1485.32/s  (0.693s, 1476.79/s)  LR: 6.356e-04  Data: 0.009 (0.012)
Train: 249 [1150/1251 ( 92%)]  Loss: 3.549 (3.74)  Time: 0.673s, 1521.72/s  (0.693s, 1477.08/s)  LR: 6.356e-04  Data: 0.010 (0.012)
Train: 249 [1200/1251 ( 96%)]  Loss: 3.715 (3.74)  Time: 0.792s, 1292.23/s  (0.693s, 1477.21/s)  LR: 6.356e-04  Data: 0.009 (0.012)
Train: 249 [1250/1251 (100%)]  Loss: 4.051 (3.75)  Time: 0.691s, 1482.81/s  (0.693s, 1478.18/s)  LR: 6.356e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.474 (1.474)  Loss:  1.0840 (1.0840)  Acc@1: 85.9375 (85.9375)  Acc@5: 96.2891 (96.2891)
Test: [  48/48]  Time: 0.136 (0.585)  Loss:  1.1104 (1.5653)  Acc@1: 82.4292 (71.1840)  Acc@5: 94.9292 (90.2700)
Train: 250 [   0/1251 (  0%)]  Loss: 3.576 (3.58)  Time: 2.368s,  432.41/s  (2.368s,  432.41/s)  LR: 6.331e-04  Data: 1.752 (1.752)
Train: 250 [  50/1251 (  4%)]  Loss: 3.773 (3.67)  Time: 0.706s, 1451.11/s  (0.726s, 1410.93/s)  LR: 6.331e-04  Data: 0.010 (0.046)
Train: 250 [ 100/1251 (  8%)]  Loss: 3.718 (3.69)  Time: 0.676s, 1514.42/s  (0.709s, 1443.32/s)  LR: 6.331e-04  Data: 0.011 (0.028)
Train: 250 [ 150/1251 ( 12%)]  Loss: 3.978 (3.76)  Time: 0.668s, 1532.52/s  (0.704s, 1453.76/s)  LR: 6.331e-04  Data: 0.011 (0.022)
Train: 250 [ 200/1251 ( 16%)]  Loss: 3.945 (3.80)  Time: 0.710s, 1441.48/s  (0.702s, 1459.01/s)  LR: 6.331e-04  Data: 0.011 (0.019)
Train: 250 [ 250/1251 ( 20%)]  Loss: 3.813 (3.80)  Time: 0.730s, 1403.39/s  (0.700s, 1462.26/s)  LR: 6.331e-04  Data: 0.015 (0.018)
Train: 250 [ 300/1251 ( 24%)]  Loss: 3.810 (3.80)  Time: 0.672s, 1522.84/s  (0.700s, 1463.46/s)  LR: 6.331e-04  Data: 0.011 (0.017)
Train: 250 [ 350/1251 ( 28%)]  Loss: 3.830 (3.81)  Time: 0.716s, 1429.75/s  (0.698s, 1466.27/s)  LR: 6.331e-04  Data: 0.013 (0.016)
Train: 250 [ 400/1251 ( 32%)]  Loss: 3.711 (3.79)  Time: 0.674s, 1519.21/s  (0.698s, 1466.91/s)  LR: 6.331e-04  Data: 0.010 (0.015)
Train: 250 [ 450/1251 ( 36%)]  Loss: 4.014 (3.82)  Time: 0.708s, 1446.23/s  (0.698s, 1467.23/s)  LR: 6.331e-04  Data: 0.011 (0.015)
Train: 250 [ 500/1251 ( 40%)]  Loss: 3.701 (3.81)  Time: 0.672s, 1524.34/s  (0.697s, 1468.84/s)  LR: 6.331e-04  Data: 0.010 (0.014)
Train: 250 [ 550/1251 ( 44%)]  Loss: 3.714 (3.80)  Time: 0.690s, 1483.96/s  (0.697s, 1469.08/s)  LR: 6.331e-04  Data: 0.016 (0.014)
Train: 250 [ 600/1251 ( 48%)]  Loss: 3.800 (3.80)  Time: 0.681s, 1504.45/s  (0.696s, 1470.73/s)  LR: 6.331e-04  Data: 0.011 (0.014)
Train: 250 [ 650/1251 ( 52%)]  Loss: 3.824 (3.80)  Time: 0.720s, 1422.20/s  (0.696s, 1470.61/s)  LR: 6.331e-04  Data: 0.010 (0.013)
Train: 250 [ 700/1251 ( 56%)]  Loss: 3.751 (3.80)  Time: 0.719s, 1424.33/s  (0.696s, 1470.42/s)  LR: 6.331e-04  Data: 0.011 (0.013)
Train: 250 [ 750/1251 ( 60%)]  Loss: 3.543 (3.78)  Time: 0.708s, 1445.59/s  (0.696s, 1471.18/s)  LR: 6.331e-04  Data: 0.012 (0.013)
Train: 250 [ 800/1251 ( 64%)]  Loss: 3.773 (3.78)  Time: 0.676s, 1514.95/s  (0.696s, 1471.78/s)  LR: 6.331e-04  Data: 0.011 (0.013)
Train: 250 [ 850/1251 ( 68%)]  Loss: 3.520 (3.77)  Time: 0.710s, 1441.79/s  (0.695s, 1472.45/s)  LR: 6.331e-04  Data: 0.009 (0.013)
Train: 250 [ 900/1251 ( 72%)]  Loss: 4.017 (3.78)  Time: 0.686s, 1491.92/s  (0.695s, 1473.69/s)  LR: 6.331e-04  Data: 0.012 (0.012)
Train: 250 [ 950/1251 ( 76%)]  Loss: 3.663 (3.77)  Time: 0.669s, 1530.96/s  (0.695s, 1473.31/s)  LR: 6.331e-04  Data: 0.011 (0.012)
Train: 250 [1000/1251 ( 80%)]  Loss: 3.446 (3.76)  Time: 0.748s, 1369.12/s  (0.695s, 1473.24/s)  LR: 6.331e-04  Data: 0.013 (0.012)
Train: 250 [1050/1251 ( 84%)]  Loss: 3.619 (3.75)  Time: 0.671s, 1525.05/s  (0.695s, 1474.17/s)  LR: 6.331e-04  Data: 0.013 (0.012)
Train: 250 [1100/1251 ( 88%)]  Loss: 4.058 (3.77)  Time: 0.699s, 1464.57/s  (0.694s, 1474.46/s)  LR: 6.331e-04  Data: 0.011 (0.012)
Train: 250 [1150/1251 ( 92%)]  Loss: 3.762 (3.77)  Time: 0.673s, 1522.51/s  (0.694s, 1475.32/s)  LR: 6.331e-04  Data: 0.010 (0.012)
Train: 250 [1200/1251 ( 96%)]  Loss: 3.993 (3.77)  Time: 0.720s, 1421.96/s  (0.694s, 1475.89/s)  LR: 6.331e-04  Data: 0.010 (0.012)
Train: 250 [1250/1251 (100%)]  Loss: 3.777 (3.77)  Time: 0.659s, 1552.75/s  (0.694s, 1475.71/s)  LR: 6.331e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.494 (1.494)  Loss:  0.8647 (0.8647)  Acc@1: 88.8672 (88.8672)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.135 (0.580)  Loss:  0.9858 (1.4548)  Acc@1: 85.7311 (72.4620)  Acc@5: 96.2264 (90.8780)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-241.pth.tar', 72.53999996826172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-250.pth.tar', 72.46200000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-247.pth.tar', 72.35600009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-242.pth.tar', 72.2440000439453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-229.pth.tar', 72.24199998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-234.pth.tar', 72.23200009033204)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-239.pth.tar', 72.22400007568359)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-248.pth.tar', 72.19999998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-246.pth.tar', 72.12000006103516)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-233.pth.tar', 72.12000002197266)

Train: 251 [   0/1251 (  0%)]  Loss: 3.592 (3.59)  Time: 2.276s,  449.99/s  (2.276s,  449.99/s)  LR: 6.306e-04  Data: 1.612 (1.612)
Train: 251 [  50/1251 (  4%)]  Loss: 3.957 (3.77)  Time: 0.677s, 1512.78/s  (0.734s, 1394.47/s)  LR: 6.306e-04  Data: 0.009 (0.053)
Train: 251 [ 100/1251 (  8%)]  Loss: 3.245 (3.60)  Time: 0.714s, 1433.84/s  (0.712s, 1438.02/s)  LR: 6.306e-04  Data: 0.010 (0.032)
Train: 251 [ 150/1251 ( 12%)]  Loss: 3.697 (3.62)  Time: 0.711s, 1440.17/s  (0.707s, 1448.79/s)  LR: 6.306e-04  Data: 0.011 (0.025)
Train: 251 [ 200/1251 ( 16%)]  Loss: 3.944 (3.69)  Time: 0.671s, 1525.87/s  (0.703s, 1455.97/s)  LR: 6.306e-04  Data: 0.010 (0.021)
Train: 251 [ 250/1251 ( 20%)]  Loss: 3.411 (3.64)  Time: 0.664s, 1541.66/s  (0.700s, 1463.26/s)  LR: 6.306e-04  Data: 0.009 (0.019)
Train: 251 [ 300/1251 ( 24%)]  Loss: 4.029 (3.70)  Time: 0.672s, 1523.60/s  (0.699s, 1465.44/s)  LR: 6.306e-04  Data: 0.010 (0.018)
Train: 251 [ 350/1251 ( 28%)]  Loss: 3.669 (3.69)  Time: 0.713s, 1435.96/s  (0.697s, 1469.32/s)  LR: 6.306e-04  Data: 0.010 (0.017)
Train: 251 [ 400/1251 ( 32%)]  Loss: 3.762 (3.70)  Time: 0.673s, 1520.77/s  (0.698s, 1468.03/s)  LR: 6.306e-04  Data: 0.010 (0.016)
Train: 251 [ 450/1251 ( 36%)]  Loss: 3.679 (3.70)  Time: 0.710s, 1442.10/s  (0.697s, 1468.79/s)  LR: 6.306e-04  Data: 0.011 (0.015)
Train: 251 [ 500/1251 ( 40%)]  Loss: 3.448 (3.68)  Time: 0.672s, 1523.09/s  (0.696s, 1470.46/s)  LR: 6.306e-04  Data: 0.010 (0.015)
Train: 251 [ 550/1251 ( 44%)]  Loss: 3.430 (3.66)  Time: 0.723s, 1416.01/s  (0.696s, 1471.88/s)  LR: 6.306e-04  Data: 0.009 (0.014)
Train: 251 [ 600/1251 ( 48%)]  Loss: 3.937 (3.68)  Time: 0.724s, 1414.46/s  (0.695s, 1473.52/s)  LR: 6.306e-04  Data: 0.016 (0.014)
Train: 251 [ 650/1251 ( 52%)]  Loss: 3.742 (3.68)  Time: 0.668s, 1532.89/s  (0.695s, 1473.78/s)  LR: 6.306e-04  Data: 0.009 (0.014)
Train: 251 [ 700/1251 ( 56%)]  Loss: 4.041 (3.71)  Time: 0.702s, 1458.59/s  (0.695s, 1474.24/s)  LR: 6.306e-04  Data: 0.009 (0.014)
Train: 251 [ 750/1251 ( 60%)]  Loss: 3.723 (3.71)  Time: 0.673s, 1522.21/s  (0.695s, 1474.12/s)  LR: 6.306e-04  Data: 0.011 (0.013)
Train: 251 [ 800/1251 ( 64%)]  Loss: 3.465 (3.69)  Time: 0.720s, 1422.86/s  (0.695s, 1474.41/s)  LR: 6.306e-04  Data: 0.009 (0.013)
Train: 251 [ 850/1251 ( 68%)]  Loss: 3.948 (3.71)  Time: 0.707s, 1448.19/s  (0.695s, 1473.73/s)  LR: 6.306e-04  Data: 0.012 (0.013)
Train: 251 [ 900/1251 ( 72%)]  Loss: 3.928 (3.72)  Time: 0.672s, 1524.02/s  (0.695s, 1474.23/s)  LR: 6.306e-04  Data: 0.011 (0.013)
Train: 251 [ 950/1251 ( 76%)]  Loss: 4.039 (3.73)  Time: 0.671s, 1525.49/s  (0.695s, 1474.01/s)  LR: 6.306e-04  Data: 0.009 (0.013)
Train: 251 [1000/1251 ( 80%)]  Loss: 3.580 (3.73)  Time: 0.672s, 1523.43/s  (0.695s, 1474.34/s)  LR: 6.306e-04  Data: 0.013 (0.013)
Train: 251 [1050/1251 ( 84%)]  Loss: 3.742 (3.73)  Time: 0.712s, 1438.51/s  (0.694s, 1475.20/s)  LR: 6.306e-04  Data: 0.009 (0.013)
Train: 251 [1100/1251 ( 88%)]  Loss: 3.942 (3.74)  Time: 0.667s, 1534.26/s  (0.694s, 1474.46/s)  LR: 6.306e-04  Data: 0.009 (0.012)
Train: 251 [1150/1251 ( 92%)]  Loss: 3.734 (3.74)  Time: 0.672s, 1523.06/s  (0.694s, 1475.01/s)  LR: 6.306e-04  Data: 0.009 (0.012)
Train: 251 [1200/1251 ( 96%)]  Loss: 3.683 (3.73)  Time: 0.671s, 1525.69/s  (0.694s, 1474.92/s)  LR: 6.306e-04  Data: 0.011 (0.012)
Train: 251 [1250/1251 (100%)]  Loss: 3.720 (3.73)  Time: 0.658s, 1556.14/s  (0.694s, 1475.48/s)  LR: 6.306e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.587 (1.587)  Loss:  0.9341 (0.9341)  Acc@1: 89.4531 (89.4531)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.136 (0.585)  Loss:  1.0859 (1.4976)  Acc@1: 82.3113 (71.7520)  Acc@5: 95.1651 (90.7000)
Train: 252 [   0/1251 (  0%)]  Loss: 3.445 (3.45)  Time: 2.367s,  432.55/s  (2.367s,  432.55/s)  LR: 6.281e-04  Data: 1.751 (1.751)
Train: 252 [  50/1251 (  4%)]  Loss: 3.633 (3.54)  Time: 0.676s, 1513.89/s  (0.736s, 1391.46/s)  LR: 6.281e-04  Data: 0.011 (0.054)
Train: 252 [ 100/1251 (  8%)]  Loss: 3.727 (3.60)  Time: 0.668s, 1532.47/s  (0.714s, 1433.53/s)  LR: 6.281e-04  Data: 0.009 (0.033)
Train: 252 [ 150/1251 ( 12%)]  Loss: 3.330 (3.53)  Time: 0.670s, 1529.49/s  (0.708s, 1447.08/s)  LR: 6.281e-04  Data: 0.009 (0.025)
Train: 252 [ 200/1251 ( 16%)]  Loss: 3.538 (3.53)  Time: 0.671s, 1527.12/s  (0.704s, 1453.57/s)  LR: 6.281e-04  Data: 0.011 (0.022)
Train: 252 [ 250/1251 ( 20%)]  Loss: 3.664 (3.56)  Time: 0.678s, 1509.80/s  (0.701s, 1460.02/s)  LR: 6.281e-04  Data: 0.011 (0.019)
Train: 252 [ 300/1251 ( 24%)]  Loss: 3.426 (3.54)  Time: 0.683s, 1498.60/s  (0.700s, 1462.47/s)  LR: 6.281e-04  Data: 0.011 (0.018)
Train: 252 [ 350/1251 ( 28%)]  Loss: 3.823 (3.57)  Time: 0.612s, 1673.99/s  (0.699s, 1465.16/s)  LR: 6.281e-04  Data: 0.011 (0.017)
Train: 252 [ 400/1251 ( 32%)]  Loss: 3.402 (3.55)  Time: 0.673s, 1520.59/s  (0.697s, 1468.22/s)  LR: 6.281e-04  Data: 0.012 (0.016)
Train: 252 [ 450/1251 ( 36%)]  Loss: 3.781 (3.58)  Time: 0.671s, 1526.79/s  (0.696s, 1471.16/s)  LR: 6.281e-04  Data: 0.011 (0.015)
Train: 252 [ 500/1251 ( 40%)]  Loss: 3.535 (3.57)  Time: 0.671s, 1526.64/s  (0.696s, 1470.49/s)  LR: 6.281e-04  Data: 0.011 (0.015)
Train: 252 [ 550/1251 ( 44%)]  Loss: 3.482 (3.57)  Time: 0.672s, 1523.38/s  (0.696s, 1471.14/s)  LR: 6.281e-04  Data: 0.011 (0.014)
Train: 252 [ 600/1251 ( 48%)]  Loss: 4.047 (3.60)  Time: 0.673s, 1521.91/s  (0.696s, 1471.53/s)  LR: 6.281e-04  Data: 0.010 (0.014)
Train: 252 [ 650/1251 ( 52%)]  Loss: 3.415 (3.59)  Time: 0.673s, 1521.92/s  (0.696s, 1471.04/s)  LR: 6.281e-04  Data: 0.011 (0.014)
Train: 252 [ 700/1251 ( 56%)]  Loss: 3.765 (3.60)  Time: 0.718s, 1427.06/s  (0.696s, 1472.01/s)  LR: 6.281e-04  Data: 0.011 (0.014)
Train: 252 [ 750/1251 ( 60%)]  Loss: 3.780 (3.61)  Time: 0.677s, 1511.66/s  (0.695s, 1473.19/s)  LR: 6.281e-04  Data: 0.011 (0.013)
Train: 252 [ 800/1251 ( 64%)]  Loss: 3.680 (3.62)  Time: 0.685s, 1493.95/s  (0.695s, 1474.04/s)  LR: 6.281e-04  Data: 0.011 (0.013)
Train: 252 [ 850/1251 ( 68%)]  Loss: 3.728 (3.62)  Time: 0.675s, 1517.95/s  (0.695s, 1474.43/s)  LR: 6.281e-04  Data: 0.011 (0.013)
Train: 252 [ 900/1251 ( 72%)]  Loss: 3.646 (3.62)  Time: 0.668s, 1531.80/s  (0.694s, 1475.24/s)  LR: 6.281e-04  Data: 0.011 (0.013)
Train: 252 [ 950/1251 ( 76%)]  Loss: 3.489 (3.62)  Time: 0.672s, 1523.54/s  (0.695s, 1474.40/s)  LR: 6.281e-04  Data: 0.011 (0.013)
Train: 252 [1000/1251 ( 80%)]  Loss: 3.644 (3.62)  Time: 0.673s, 1522.53/s  (0.694s, 1475.04/s)  LR: 6.281e-04  Data: 0.011 (0.013)
Train: 252 [1050/1251 ( 84%)]  Loss: 3.495 (3.61)  Time: 0.672s, 1524.04/s  (0.694s, 1475.59/s)  LR: 6.281e-04  Data: 0.011 (0.013)
Train: 252 [1100/1251 ( 88%)]  Loss: 3.090 (3.59)  Time: 0.709s, 1444.04/s  (0.693s, 1476.68/s)  LR: 6.281e-04  Data: 0.010 (0.013)
Train: 252 [1150/1251 ( 92%)]  Loss: 3.738 (3.60)  Time: 0.701s, 1460.77/s  (0.693s, 1477.50/s)  LR: 6.281e-04  Data: 0.011 (0.012)
Train: 252 [1200/1251 ( 96%)]  Loss: 3.931 (3.61)  Time: 0.710s, 1441.90/s  (0.693s, 1477.13/s)  LR: 6.281e-04  Data: 0.017 (0.012)
Train: 252 [1250/1251 (100%)]  Loss: 3.766 (3.62)  Time: 0.659s, 1554.08/s  (0.693s, 1477.50/s)  LR: 6.281e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.444 (1.444)  Loss:  0.9512 (0.9512)  Acc@1: 87.8906 (87.8906)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.136 (0.581)  Loss:  1.0410 (1.5288)  Acc@1: 83.7264 (72.1300)  Acc@5: 96.1085 (90.9580)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-241.pth.tar', 72.53999996826172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-250.pth.tar', 72.46200000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-247.pth.tar', 72.35600009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-242.pth.tar', 72.2440000439453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-229.pth.tar', 72.24199998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-234.pth.tar', 72.23200009033204)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-239.pth.tar', 72.22400007568359)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-248.pth.tar', 72.19999998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-252.pth.tar', 72.12999991210937)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-246.pth.tar', 72.12000006103516)

Train: 253 [   0/1251 (  0%)]  Loss: 3.529 (3.53)  Time: 2.376s,  430.96/s  (2.376s,  430.96/s)  LR: 6.256e-04  Data: 1.744 (1.744)
Train: 253 [  50/1251 (  4%)]  Loss: 3.498 (3.51)  Time: 0.672s, 1523.13/s  (0.732s, 1398.36/s)  LR: 6.256e-04  Data: 0.010 (0.050)
Train: 253 [ 100/1251 (  8%)]  Loss: 3.777 (3.60)  Time: 0.671s, 1526.64/s  (0.714s, 1435.02/s)  LR: 6.256e-04  Data: 0.010 (0.030)
Train: 253 [ 150/1251 ( 12%)]  Loss: 3.540 (3.59)  Time: 0.712s, 1438.19/s  (0.708s, 1445.86/s)  LR: 6.256e-04  Data: 0.010 (0.024)
Train: 253 [ 200/1251 ( 16%)]  Loss: 3.270 (3.52)  Time: 0.671s, 1527.22/s  (0.705s, 1451.55/s)  LR: 6.256e-04  Data: 0.011 (0.020)
Train: 253 [ 250/1251 ( 20%)]  Loss: 3.813 (3.57)  Time: 0.706s, 1451.27/s  (0.702s, 1459.69/s)  LR: 6.256e-04  Data: 0.010 (0.018)
Train: 253 [ 300/1251 ( 24%)]  Loss: 3.683 (3.59)  Time: 0.674s, 1519.12/s  (0.701s, 1461.79/s)  LR: 6.256e-04  Data: 0.010 (0.017)
Train: 253 [ 350/1251 ( 28%)]  Loss: 3.193 (3.54)  Time: 0.676s, 1515.71/s  (0.699s, 1464.98/s)  LR: 6.256e-04  Data: 0.009 (0.016)
Train: 253 [ 400/1251 ( 32%)]  Loss: 3.560 (3.54)  Time: 0.705s, 1453.38/s  (0.699s, 1465.14/s)  LR: 6.256e-04  Data: 0.010 (0.015)
Train: 253 [ 450/1251 ( 36%)]  Loss: 3.564 (3.54)  Time: 0.673s, 1522.27/s  (0.698s, 1466.14/s)  LR: 6.256e-04  Data: 0.010 (0.015)
Train: 253 [ 500/1251 ( 40%)]  Loss: 3.420 (3.53)  Time: 0.701s, 1460.25/s  (0.697s, 1468.24/s)  LR: 6.256e-04  Data: 0.009 (0.014)
Train: 253 [ 550/1251 ( 44%)]  Loss: 4.143 (3.58)  Time: 0.689s, 1486.10/s  (0.697s, 1468.13/s)  LR: 6.256e-04  Data: 0.012 (0.014)
Train: 253 [ 600/1251 ( 48%)]  Loss: 3.603 (3.58)  Time: 0.750s, 1365.38/s  (0.698s, 1467.53/s)  LR: 6.256e-04  Data: 0.015 (0.014)
Train: 253 [ 650/1251 ( 52%)]  Loss: 3.824 (3.60)  Time: 0.708s, 1446.35/s  (0.698s, 1466.75/s)  LR: 6.256e-04  Data: 0.015 (0.014)
Train: 253 [ 700/1251 ( 56%)]  Loss: 3.821 (3.62)  Time: 0.717s, 1428.74/s  (0.698s, 1466.90/s)  LR: 6.256e-04  Data: 0.010 (0.013)
Train: 253 [ 750/1251 ( 60%)]  Loss: 3.832 (3.63)  Time: 0.709s, 1445.00/s  (0.698s, 1467.70/s)  LR: 6.256e-04  Data: 0.009 (0.013)
Train: 253 [ 800/1251 ( 64%)]  Loss: 3.674 (3.63)  Time: 0.665s, 1539.10/s  (0.698s, 1467.81/s)  LR: 6.256e-04  Data: 0.010 (0.013)
Train: 253 [ 850/1251 ( 68%)]  Loss: 3.697 (3.64)  Time: 0.672s, 1522.80/s  (0.698s, 1467.21/s)  LR: 6.256e-04  Data: 0.011 (0.013)
Train: 253 [ 900/1251 ( 72%)]  Loss: 3.557 (3.63)  Time: 0.706s, 1451.00/s  (0.698s, 1467.48/s)  LR: 6.256e-04  Data: 0.009 (0.013)
Train: 253 [ 950/1251 ( 76%)]  Loss: 4.081 (3.65)  Time: 0.699s, 1465.98/s  (0.698s, 1468.01/s)  LR: 6.256e-04  Data: 0.009 (0.013)
Train: 253 [1000/1251 ( 80%)]  Loss: 3.777 (3.66)  Time: 0.670s, 1527.61/s  (0.697s, 1469.03/s)  LR: 6.256e-04  Data: 0.010 (0.012)
Train: 253 [1050/1251 ( 84%)]  Loss: 3.868 (3.67)  Time: 0.700s, 1463.12/s  (0.697s, 1469.47/s)  LR: 6.256e-04  Data: 0.009 (0.012)
Train: 253 [1100/1251 ( 88%)]  Loss: 3.549 (3.66)  Time: 0.676s, 1514.42/s  (0.697s, 1470.02/s)  LR: 6.256e-04  Data: 0.010 (0.012)
Train: 253 [1150/1251 ( 92%)]  Loss: 3.272 (3.65)  Time: 0.735s, 1392.50/s  (0.696s, 1470.78/s)  LR: 6.256e-04  Data: 0.012 (0.012)
Train: 253 [1200/1251 ( 96%)]  Loss: 3.964 (3.66)  Time: 0.691s, 1480.90/s  (0.696s, 1470.82/s)  LR: 6.256e-04  Data: 0.009 (0.012)
Train: 253 [1250/1251 (100%)]  Loss: 3.424 (3.65)  Time: 0.657s, 1557.60/s  (0.696s, 1471.16/s)  LR: 6.256e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.585 (1.585)  Loss:  0.7554 (0.7554)  Acc@1: 88.4766 (88.4766)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.136 (0.587)  Loss:  0.8145 (1.3776)  Acc@1: 84.1981 (71.9860)  Acc@5: 96.5802 (90.8280)
Train: 254 [   0/1251 (  0%)]  Loss: 3.298 (3.30)  Time: 2.126s,  481.62/s  (2.126s,  481.62/s)  LR: 6.231e-04  Data: 1.514 (1.514)
Train: 254 [  50/1251 (  4%)]  Loss: 3.232 (3.26)  Time: 0.672s, 1522.81/s  (0.727s, 1409.46/s)  LR: 6.231e-04  Data: 0.011 (0.049)
Train: 254 [ 100/1251 (  8%)]  Loss: 3.300 (3.28)  Time: 0.686s, 1492.50/s  (0.711s, 1441.12/s)  LR: 6.231e-04  Data: 0.011 (0.030)
Train: 254 [ 150/1251 ( 12%)]  Loss: 3.645 (3.37)  Time: 0.674s, 1519.87/s  (0.704s, 1454.49/s)  LR: 6.231e-04  Data: 0.013 (0.023)
Train: 254 [ 200/1251 ( 16%)]  Loss: 3.846 (3.46)  Time: 0.674s, 1520.12/s  (0.700s, 1462.47/s)  LR: 6.231e-04  Data: 0.011 (0.020)
Train: 254 [ 250/1251 ( 20%)]  Loss: 3.825 (3.52)  Time: 0.687s, 1490.00/s  (0.700s, 1462.44/s)  LR: 6.231e-04  Data: 0.009 (0.018)
Train: 254 [ 300/1251 ( 24%)]  Loss: 3.746 (3.56)  Time: 0.673s, 1520.62/s  (0.699s, 1464.82/s)  LR: 6.231e-04  Data: 0.011 (0.017)
Train: 254 [ 350/1251 ( 28%)]  Loss: 4.102 (3.62)  Time: 0.706s, 1450.35/s  (0.698s, 1467.48/s)  LR: 6.231e-04  Data: 0.010 (0.016)
Train: 254 [ 400/1251 ( 32%)]  Loss: 3.639 (3.63)  Time: 0.682s, 1501.51/s  (0.697s, 1470.11/s)  LR: 6.231e-04  Data: 0.010 (0.015)
Train: 254 [ 450/1251 ( 36%)]  Loss: 3.790 (3.64)  Time: 0.676s, 1514.83/s  (0.696s, 1471.93/s)  LR: 6.231e-04  Data: 0.015 (0.015)
Train: 254 [ 500/1251 ( 40%)]  Loss: 3.786 (3.66)  Time: 0.674s, 1520.19/s  (0.696s, 1472.14/s)  LR: 6.231e-04  Data: 0.011 (0.014)
Train: 254 [ 550/1251 ( 44%)]  Loss: 3.325 (3.63)  Time: 0.673s, 1521.95/s  (0.696s, 1471.77/s)  LR: 6.231e-04  Data: 0.010 (0.014)
Train: 254 [ 600/1251 ( 48%)]  Loss: 3.982 (3.66)  Time: 0.671s, 1525.91/s  (0.695s, 1473.02/s)  LR: 6.231e-04  Data: 0.011 (0.014)
Train: 254 [ 650/1251 ( 52%)]  Loss: 3.640 (3.65)  Time: 0.670s, 1527.95/s  (0.696s, 1471.93/s)  LR: 6.231e-04  Data: 0.009 (0.013)
Train: 254 [ 700/1251 ( 56%)]  Loss: 3.910 (3.67)  Time: 0.672s, 1522.99/s  (0.695s, 1472.64/s)  LR: 6.231e-04  Data: 0.011 (0.013)
Train: 254 [ 750/1251 ( 60%)]  Loss: 3.786 (3.68)  Time: 0.676s, 1515.87/s  (0.695s, 1472.72/s)  LR: 6.231e-04  Data: 0.010 (0.013)
Train: 254 [ 800/1251 ( 64%)]  Loss: 4.242 (3.71)  Time: 0.688s, 1489.28/s  (0.695s, 1473.77/s)  LR: 6.231e-04  Data: 0.013 (0.013)
Train: 254 [ 850/1251 ( 68%)]  Loss: 3.964 (3.73)  Time: 0.673s, 1521.93/s  (0.695s, 1472.75/s)  LR: 6.231e-04  Data: 0.011 (0.013)
Train: 254 [ 900/1251 ( 72%)]  Loss: 3.643 (3.72)  Time: 0.669s, 1530.94/s  (0.695s, 1473.15/s)  LR: 6.231e-04  Data: 0.010 (0.013)
Train: 254 [ 950/1251 ( 76%)]  Loss: 3.452 (3.71)  Time: 0.666s, 1537.56/s  (0.695s, 1472.87/s)  LR: 6.231e-04  Data: 0.010 (0.013)
Train: 254 [1000/1251 ( 80%)]  Loss: 3.520 (3.70)  Time: 0.709s, 1445.23/s  (0.695s, 1473.60/s)  LR: 6.231e-04  Data: 0.011 (0.012)
Train: 254 [1050/1251 ( 84%)]  Loss: 3.586 (3.69)  Time: 0.680s, 1506.63/s  (0.695s, 1473.70/s)  LR: 6.231e-04  Data: 0.010 (0.012)
Train: 254 [1100/1251 ( 88%)]  Loss: 3.867 (3.70)  Time: 0.672s, 1523.01/s  (0.695s, 1473.76/s)  LR: 6.231e-04  Data: 0.011 (0.012)
Train: 254 [1150/1251 ( 92%)]  Loss: 3.751 (3.70)  Time: 0.703s, 1457.43/s  (0.695s, 1474.09/s)  LR: 6.231e-04  Data: 0.011 (0.012)
Train: 254 [1200/1251 ( 96%)]  Loss: 3.989 (3.71)  Time: 0.708s, 1447.25/s  (0.695s, 1473.67/s)  LR: 6.231e-04  Data: 0.011 (0.012)
Train: 254 [1250/1251 (100%)]  Loss: 3.291 (3.70)  Time: 0.658s, 1557.33/s  (0.695s, 1473.63/s)  LR: 6.231e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.561 (1.561)  Loss:  0.7617 (0.7617)  Acc@1: 88.5742 (88.5742)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.136 (0.586)  Loss:  0.7964 (1.3859)  Acc@1: 83.4906 (72.6860)  Acc@5: 95.5189 (91.2160)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-254.pth.tar', 72.68600006835938)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-241.pth.tar', 72.53999996826172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-250.pth.tar', 72.46200000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-247.pth.tar', 72.35600009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-242.pth.tar', 72.2440000439453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-229.pth.tar', 72.24199998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-234.pth.tar', 72.23200009033204)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-239.pth.tar', 72.22400007568359)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-248.pth.tar', 72.19999998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-252.pth.tar', 72.12999991210937)

Train: 255 [   0/1251 (  0%)]  Loss: 3.632 (3.63)  Time: 2.098s,  487.98/s  (2.098s,  487.98/s)  LR: 6.206e-04  Data: 1.483 (1.483)
Train: 255 [  50/1251 (  4%)]  Loss: 3.958 (3.80)  Time: 0.721s, 1419.29/s  (0.725s, 1412.64/s)  LR: 6.206e-04  Data: 0.010 (0.046)
Train: 255 [ 100/1251 (  8%)]  Loss: 3.661 (3.75)  Time: 0.672s, 1524.08/s  (0.708s, 1445.54/s)  LR: 6.206e-04  Data: 0.010 (0.028)
Train: 255 [ 150/1251 ( 12%)]  Loss: 3.679 (3.73)  Time: 0.789s, 1298.43/s  (0.703s, 1456.55/s)  LR: 6.206e-04  Data: 0.010 (0.023)
Train: 255 [ 200/1251 ( 16%)]  Loss: 3.797 (3.75)  Time: 0.666s, 1537.73/s  (0.699s, 1465.83/s)  LR: 6.206e-04  Data: 0.010 (0.020)
Train: 255 [ 250/1251 ( 20%)]  Loss: 3.391 (3.69)  Time: 0.706s, 1450.65/s  (0.696s, 1470.46/s)  LR: 6.206e-04  Data: 0.010 (0.018)
Train: 255 [ 300/1251 ( 24%)]  Loss: 3.762 (3.70)  Time: 0.699s, 1465.46/s  (0.697s, 1469.88/s)  LR: 6.206e-04  Data: 0.010 (0.017)
Train: 255 [ 350/1251 ( 28%)]  Loss: 3.851 (3.72)  Time: 0.672s, 1523.55/s  (0.696s, 1471.46/s)  LR: 6.206e-04  Data: 0.010 (0.016)
Train: 255 [ 400/1251 ( 32%)]  Loss: 3.855 (3.73)  Time: 0.712s, 1437.35/s  (0.695s, 1472.75/s)  LR: 6.206e-04  Data: 0.010 (0.015)
Train: 255 [ 450/1251 ( 36%)]  Loss: 3.530 (3.71)  Time: 0.672s, 1523.47/s  (0.695s, 1472.82/s)  LR: 6.206e-04  Data: 0.012 (0.014)
Train: 255 [ 500/1251 ( 40%)]  Loss: 3.795 (3.72)  Time: 0.730s, 1403.05/s  (0.695s, 1473.48/s)  LR: 6.206e-04  Data: 0.010 (0.014)
Train: 255 [ 550/1251 ( 44%)]  Loss: 3.557 (3.71)  Time: 0.669s, 1531.04/s  (0.695s, 1473.40/s)  LR: 6.206e-04  Data: 0.011 (0.014)
Train: 255 [ 600/1251 ( 48%)]  Loss: 3.542 (3.69)  Time: 0.664s, 1542.40/s  (0.694s, 1474.96/s)  LR: 6.206e-04  Data: 0.008 (0.013)
Train: 255 [ 650/1251 ( 52%)]  Loss: 3.862 (3.71)  Time: 0.745s, 1374.96/s  (0.694s, 1475.99/s)  LR: 6.206e-04  Data: 0.010 (0.013)
Train: 255 [ 700/1251 ( 56%)]  Loss: 3.576 (3.70)  Time: 0.673s, 1520.44/s  (0.693s, 1476.84/s)  LR: 6.206e-04  Data: 0.009 (0.013)
Train: 255 [ 750/1251 ( 60%)]  Loss: 4.029 (3.72)  Time: 0.764s, 1339.98/s  (0.693s, 1476.80/s)  LR: 6.206e-04  Data: 0.009 (0.013)
Train: 255 [ 800/1251 ( 64%)]  Loss: 3.833 (3.72)  Time: 0.672s, 1523.32/s  (0.693s, 1477.60/s)  LR: 6.206e-04  Data: 0.009 (0.013)
Train: 255 [ 850/1251 ( 68%)]  Loss: 3.589 (3.72)  Time: 0.730s, 1403.65/s  (0.693s, 1477.35/s)  LR: 6.206e-04  Data: 0.009 (0.013)
Train: 255 [ 900/1251 ( 72%)]  Loss: 3.645 (3.71)  Time: 0.682s, 1500.74/s  (0.693s, 1477.90/s)  LR: 6.206e-04  Data: 0.010 (0.012)
Train: 255 [ 950/1251 ( 76%)]  Loss: 4.068 (3.73)  Time: 0.672s, 1524.86/s  (0.693s, 1478.43/s)  LR: 6.206e-04  Data: 0.010 (0.012)
Train: 255 [1000/1251 ( 80%)]  Loss: 3.850 (3.74)  Time: 0.692s, 1480.54/s  (0.692s, 1478.88/s)  LR: 6.206e-04  Data: 0.010 (0.012)
Train: 255 [1050/1251 ( 84%)]  Loss: 3.882 (3.74)  Time: 0.670s, 1527.52/s  (0.692s, 1479.32/s)  LR: 6.206e-04  Data: 0.010 (0.012)
Train: 255 [1100/1251 ( 88%)]  Loss: 3.895 (3.75)  Time: 0.704s, 1454.50/s  (0.692s, 1479.33/s)  LR: 6.206e-04  Data: 0.010 (0.012)
Train: 255 [1150/1251 ( 92%)]  Loss: 3.609 (3.74)  Time: 0.708s, 1445.43/s  (0.692s, 1479.74/s)  LR: 6.206e-04  Data: 0.011 (0.012)
Train: 255 [1200/1251 ( 96%)]  Loss: 3.720 (3.74)  Time: 0.669s, 1529.64/s  (0.692s, 1478.90/s)  LR: 6.206e-04  Data: 0.011 (0.012)
Train: 255 [1250/1251 (100%)]  Loss: 3.665 (3.74)  Time: 0.657s, 1558.72/s  (0.692s, 1478.98/s)  LR: 6.206e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.549 (1.549)  Loss:  0.8994 (0.8994)  Acc@1: 87.3047 (87.3047)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.136 (0.590)  Loss:  0.9863 (1.4869)  Acc@1: 83.2547 (71.8400)  Acc@5: 95.9906 (90.7720)
Train: 256 [   0/1251 (  0%)]  Loss: 3.708 (3.71)  Time: 2.172s,  471.39/s  (2.172s,  471.39/s)  LR: 6.180e-04  Data: 1.556 (1.556)
Train: 256 [  50/1251 (  4%)]  Loss: 3.911 (3.81)  Time: 0.705s, 1453.38/s  (0.731s, 1401.45/s)  LR: 6.180e-04  Data: 0.010 (0.046)
Train: 256 [ 100/1251 (  8%)]  Loss: 3.823 (3.81)  Time: 0.736s, 1391.85/s  (0.714s, 1434.69/s)  LR: 6.180e-04  Data: 0.016 (0.029)
Train: 256 [ 150/1251 ( 12%)]  Loss: 3.736 (3.79)  Time: 0.739s, 1385.96/s  (0.714s, 1434.96/s)  LR: 6.180e-04  Data: 0.011 (0.023)
Train: 256 [ 200/1251 ( 16%)]  Loss: 3.468 (3.73)  Time: 0.719s, 1423.44/s  (0.713s, 1436.95/s)  LR: 6.180e-04  Data: 0.013 (0.020)
Train: 256 [ 250/1251 ( 20%)]  Loss: 3.759 (3.73)  Time: 0.706s, 1449.88/s  (0.712s, 1437.40/s)  LR: 6.180e-04  Data: 0.010 (0.018)
Train: 256 [ 300/1251 ( 24%)]  Loss: 3.468 (3.70)  Time: 0.695s, 1473.29/s  (0.708s, 1445.67/s)  LR: 6.180e-04  Data: 0.009 (0.017)
Train: 256 [ 350/1251 ( 28%)]  Loss: 3.868 (3.72)  Time: 0.672s, 1522.86/s  (0.704s, 1455.36/s)  LR: 6.180e-04  Data: 0.011 (0.016)
Train: 256 [ 400/1251 ( 32%)]  Loss: 3.336 (3.68)  Time: 0.672s, 1524.42/s  (0.700s, 1463.23/s)  LR: 6.180e-04  Data: 0.010 (0.015)
Train: 256 [ 450/1251 ( 36%)]  Loss: 3.311 (3.64)  Time: 0.711s, 1440.67/s  (0.699s, 1465.99/s)  LR: 6.180e-04  Data: 0.011 (0.015)
Train: 256 [ 500/1251 ( 40%)]  Loss: 3.580 (3.63)  Time: 0.735s, 1392.69/s  (0.698s, 1466.22/s)  LR: 6.180e-04  Data: 0.009 (0.014)
Train: 256 [ 550/1251 ( 44%)]  Loss: 3.713 (3.64)  Time: 0.673s, 1521.85/s  (0.698s, 1467.97/s)  LR: 6.180e-04  Data: 0.011 (0.014)
Train: 256 [ 600/1251 ( 48%)]  Loss: 3.582 (3.64)  Time: 0.692s, 1480.47/s  (0.697s, 1469.69/s)  LR: 6.180e-04  Data: 0.013 (0.014)
Train: 256 [ 650/1251 ( 52%)]  Loss: 3.948 (3.66)  Time: 0.673s, 1520.56/s  (0.696s, 1470.51/s)  LR: 6.180e-04  Data: 0.010 (0.013)
Train: 256 [ 700/1251 ( 56%)]  Loss: 3.929 (3.68)  Time: 0.667s, 1536.32/s  (0.696s, 1471.34/s)  LR: 6.180e-04  Data: 0.009 (0.013)
Train: 256 [ 750/1251 ( 60%)]  Loss: 3.698 (3.68)  Time: 0.731s, 1401.40/s  (0.696s, 1471.79/s)  LR: 6.180e-04  Data: 0.012 (0.013)
Train: 256 [ 800/1251 ( 64%)]  Loss: 3.916 (3.69)  Time: 0.710s, 1441.48/s  (0.696s, 1472.30/s)  LR: 6.180e-04  Data: 0.010 (0.013)
Train: 256 [ 850/1251 ( 68%)]  Loss: 3.330 (3.67)  Time: 0.675s, 1517.57/s  (0.695s, 1472.50/s)  LR: 6.180e-04  Data: 0.010 (0.013)
Train: 256 [ 900/1251 ( 72%)]  Loss: 3.669 (3.67)  Time: 0.710s, 1442.92/s  (0.696s, 1471.99/s)  LR: 6.180e-04  Data: 0.009 (0.013)
Train: 256 [ 950/1251 ( 76%)]  Loss: 3.583 (3.67)  Time: 0.672s, 1524.03/s  (0.696s, 1471.33/s)  LR: 6.180e-04  Data: 0.010 (0.013)
Train: 256 [1000/1251 ( 80%)]  Loss: 4.057 (3.69)  Time: 0.702s, 1459.46/s  (0.696s, 1470.83/s)  LR: 6.180e-04  Data: 0.010 (0.012)
Train: 256 [1050/1251 ( 84%)]  Loss: 4.022 (3.70)  Time: 0.673s, 1521.74/s  (0.696s, 1471.21/s)  LR: 6.180e-04  Data: 0.011 (0.012)
Train: 256 [1100/1251 ( 88%)]  Loss: 3.732 (3.70)  Time: 0.681s, 1503.06/s  (0.696s, 1472.03/s)  LR: 6.180e-04  Data: 0.009 (0.012)
Train: 256 [1150/1251 ( 92%)]  Loss: 3.957 (3.71)  Time: 0.685s, 1494.50/s  (0.695s, 1472.88/s)  LR: 6.180e-04  Data: 0.010 (0.012)
Train: 256 [1200/1251 ( 96%)]  Loss: 3.379 (3.70)  Time: 0.686s, 1493.36/s  (0.695s, 1472.63/s)  LR: 6.180e-04  Data: 0.015 (0.012)
Train: 256 [1250/1251 (100%)]  Loss: 4.150 (3.72)  Time: 0.659s, 1552.85/s  (0.696s, 1472.32/s)  LR: 6.180e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.512 (1.512)  Loss:  0.8564 (0.8564)  Acc@1: 87.6953 (87.6953)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.136 (0.590)  Loss:  0.9697 (1.4797)  Acc@1: 83.9623 (72.6300)  Acc@5: 95.4009 (91.0800)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-254.pth.tar', 72.68600006835938)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-256.pth.tar', 72.62999988525391)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-241.pth.tar', 72.53999996826172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-250.pth.tar', 72.46200000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-247.pth.tar', 72.35600009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-242.pth.tar', 72.2440000439453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-229.pth.tar', 72.24199998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-234.pth.tar', 72.23200009033204)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-239.pth.tar', 72.22400007568359)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-248.pth.tar', 72.19999998291016)

Train: 257 [   0/1251 (  0%)]  Loss: 3.753 (3.75)  Time: 2.200s,  465.41/s  (2.200s,  465.41/s)  LR: 6.155e-04  Data: 1.584 (1.584)
Train: 257 [  50/1251 (  4%)]  Loss: 3.798 (3.78)  Time: 0.671s, 1526.74/s  (0.728s, 1406.69/s)  LR: 6.155e-04  Data: 0.010 (0.046)
Train: 257 [ 100/1251 (  8%)]  Loss: 3.880 (3.81)  Time: 0.684s, 1497.54/s  (0.708s, 1446.73/s)  LR: 6.155e-04  Data: 0.014 (0.028)
Train: 257 [ 150/1251 ( 12%)]  Loss: 3.763 (3.80)  Time: 0.671s, 1525.12/s  (0.705s, 1453.29/s)  LR: 6.155e-04  Data: 0.010 (0.022)
Train: 257 [ 200/1251 ( 16%)]  Loss: 3.902 (3.82)  Time: 0.702s, 1458.39/s  (0.702s, 1457.87/s)  LR: 6.155e-04  Data: 0.009 (0.019)
Train: 257 [ 250/1251 ( 20%)]  Loss: 3.432 (3.75)  Time: 0.671s, 1526.46/s  (0.699s, 1465.29/s)  LR: 6.155e-04  Data: 0.010 (0.018)
Train: 257 [ 300/1251 ( 24%)]  Loss: 3.977 (3.79)  Time: 0.699s, 1464.18/s  (0.698s, 1466.91/s)  LR: 6.155e-04  Data: 0.009 (0.016)
Train: 257 [ 350/1251 ( 28%)]  Loss: 3.510 (3.75)  Time: 0.672s, 1524.05/s  (0.697s, 1469.75/s)  LR: 6.155e-04  Data: 0.010 (0.016)
Train: 257 [ 400/1251 ( 32%)]  Loss: 3.644 (3.74)  Time: 0.680s, 1506.85/s  (0.696s, 1471.52/s)  LR: 6.155e-04  Data: 0.012 (0.015)
Train: 257 [ 450/1251 ( 36%)]  Loss: 4.017 (3.77)  Time: 0.673s, 1521.47/s  (0.695s, 1472.65/s)  LR: 6.155e-04  Data: 0.010 (0.014)
Train: 257 [ 500/1251 ( 40%)]  Loss: 3.612 (3.75)  Time: 0.696s, 1470.21/s  (0.696s, 1471.32/s)  LR: 6.155e-04  Data: 0.014 (0.014)
Train: 257 [ 550/1251 ( 44%)]  Loss: 4.166 (3.79)  Time: 0.683s, 1499.30/s  (0.696s, 1470.72/s)  LR: 6.155e-04  Data: 0.013 (0.014)
Train: 257 [ 600/1251 ( 48%)]  Loss: 3.453 (3.76)  Time: 0.672s, 1523.87/s  (0.697s, 1469.92/s)  LR: 6.155e-04  Data: 0.011 (0.013)
Train: 257 [ 650/1251 ( 52%)]  Loss: 3.800 (3.76)  Time: 0.706s, 1449.50/s  (0.696s, 1471.25/s)  LR: 6.155e-04  Data: 0.010 (0.013)
Train: 257 [ 700/1251 ( 56%)]  Loss: 3.549 (3.75)  Time: 0.756s, 1354.84/s  (0.696s, 1472.10/s)  LR: 6.155e-04  Data: 0.009 (0.013)
Train: 257 [ 750/1251 ( 60%)]  Loss: 3.468 (3.73)  Time: 0.713s, 1436.44/s  (0.695s, 1472.82/s)  LR: 6.155e-04  Data: 0.010 (0.013)
Train: 257 [ 800/1251 ( 64%)]  Loss: 3.395 (3.71)  Time: 0.672s, 1523.58/s  (0.695s, 1472.80/s)  LR: 6.155e-04  Data: 0.010 (0.013)
Train: 257 [ 850/1251 ( 68%)]  Loss: 3.554 (3.70)  Time: 0.666s, 1538.12/s  (0.695s, 1473.08/s)  LR: 6.155e-04  Data: 0.009 (0.013)
Train: 257 [ 900/1251 ( 72%)]  Loss: 3.719 (3.70)  Time: 0.666s, 1537.60/s  (0.695s, 1473.60/s)  LR: 6.155e-04  Data: 0.011 (0.012)
Train: 257 [ 950/1251 ( 76%)]  Loss: 3.579 (3.70)  Time: 0.714s, 1433.97/s  (0.695s, 1474.03/s)  LR: 6.155e-04  Data: 0.010 (0.012)
Train: 257 [1000/1251 ( 80%)]  Loss: 3.411 (3.68)  Time: 0.681s, 1504.54/s  (0.694s, 1474.78/s)  LR: 6.155e-04  Data: 0.010 (0.012)
Train: 257 [1050/1251 ( 84%)]  Loss: 3.936 (3.70)  Time: 0.702s, 1458.11/s  (0.694s, 1475.00/s)  LR: 6.155e-04  Data: 0.010 (0.012)
Train: 257 [1100/1251 ( 88%)]  Loss: 3.724 (3.70)  Time: 0.679s, 1508.46/s  (0.694s, 1475.22/s)  LR: 6.155e-04  Data: 0.011 (0.012)
Train: 257 [1150/1251 ( 92%)]  Loss: 3.404 (3.69)  Time: 0.699s, 1464.72/s  (0.694s, 1475.17/s)  LR: 6.155e-04  Data: 0.010 (0.012)
Train: 257 [1200/1251 ( 96%)]  Loss: 3.513 (3.68)  Time: 0.753s, 1359.49/s  (0.694s, 1475.02/s)  LR: 6.155e-04  Data: 0.010 (0.012)
Train: 257 [1250/1251 (100%)]  Loss: 3.800 (3.68)  Time: 0.714s, 1434.75/s  (0.694s, 1475.15/s)  LR: 6.155e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.566 (1.566)  Loss:  0.8193 (0.8193)  Acc@1: 90.0391 (90.0391)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.136 (0.586)  Loss:  0.8730 (1.4312)  Acc@1: 84.9057 (73.0720)  Acc@5: 96.5802 (91.2080)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-257.pth.tar', 73.07200016601563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-254.pth.tar', 72.68600006835938)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-256.pth.tar', 72.62999988525391)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-241.pth.tar', 72.53999996826172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-250.pth.tar', 72.46200000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-247.pth.tar', 72.35600009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-242.pth.tar', 72.2440000439453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-229.pth.tar', 72.24199998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-234.pth.tar', 72.23200009033204)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-239.pth.tar', 72.22400007568359)

Train: 258 [   0/1251 (  0%)]  Loss: 3.714 (3.71)  Time: 2.191s,  467.47/s  (2.191s,  467.47/s)  LR: 6.130e-04  Data: 1.554 (1.554)
Train: 258 [  50/1251 (  4%)]  Loss: 3.679 (3.70)  Time: 0.705s, 1451.76/s  (0.727s, 1408.70/s)  LR: 6.130e-04  Data: 0.009 (0.047)
Train: 258 [ 100/1251 (  8%)]  Loss: 3.734 (3.71)  Time: 0.692s, 1478.90/s  (0.707s, 1447.75/s)  LR: 6.130e-04  Data: 0.011 (0.029)
Train: 258 [ 150/1251 ( 12%)]  Loss: 3.396 (3.63)  Time: 0.671s, 1525.98/s  (0.703s, 1456.86/s)  LR: 6.130e-04  Data: 0.011 (0.023)
Train: 258 [ 200/1251 ( 16%)]  Loss: 3.673 (3.64)  Time: 0.678s, 1510.97/s  (0.701s, 1461.56/s)  LR: 6.130e-04  Data: 0.011 (0.020)
Train: 258 [ 250/1251 ( 20%)]  Loss: 3.537 (3.62)  Time: 0.683s, 1498.57/s  (0.699s, 1464.95/s)  LR: 6.130e-04  Data: 0.009 (0.018)
Train: 258 [ 300/1251 ( 24%)]  Loss: 4.123 (3.69)  Time: 0.674s, 1519.87/s  (0.698s, 1467.32/s)  LR: 6.130e-04  Data: 0.010 (0.017)
Train: 258 [ 350/1251 ( 28%)]  Loss: 3.857 (3.71)  Time: 0.683s, 1498.47/s  (0.697s, 1468.19/s)  LR: 6.130e-04  Data: 0.023 (0.016)
Train: 258 [ 400/1251 ( 32%)]  Loss: 3.885 (3.73)  Time: 0.690s, 1483.71/s  (0.697s, 1469.30/s)  LR: 6.130e-04  Data: 0.010 (0.015)
Train: 258 [ 450/1251 ( 36%)]  Loss: 3.637 (3.72)  Time: 0.704s, 1454.35/s  (0.696s, 1471.52/s)  LR: 6.130e-04  Data: 0.010 (0.015)
Train: 258 [ 500/1251 ( 40%)]  Loss: 3.846 (3.73)  Time: 0.688s, 1489.06/s  (0.696s, 1472.23/s)  LR: 6.130e-04  Data: 0.009 (0.014)
Train: 258 [ 550/1251 ( 44%)]  Loss: 3.997 (3.76)  Time: 0.679s, 1508.32/s  (0.695s, 1472.39/s)  LR: 6.130e-04  Data: 0.011 (0.014)
Train: 258 [ 600/1251 ( 48%)]  Loss: 3.734 (3.75)  Time: 0.706s, 1450.89/s  (0.695s, 1473.06/s)  LR: 6.130e-04  Data: 0.011 (0.014)
Train: 258 [ 650/1251 ( 52%)]  Loss: 3.771 (3.76)  Time: 0.709s, 1443.40/s  (0.695s, 1473.95/s)  LR: 6.130e-04  Data: 0.011 (0.013)
Train: 258 [ 700/1251 ( 56%)]  Loss: 3.824 (3.76)  Time: 0.681s, 1503.53/s  (0.694s, 1474.47/s)  LR: 6.130e-04  Data: 0.011 (0.013)
Train: 258 [ 750/1251 ( 60%)]  Loss: 3.807 (3.76)  Time: 0.673s, 1520.46/s  (0.694s, 1474.97/s)  LR: 6.130e-04  Data: 0.011 (0.013)
Train: 258 [ 800/1251 ( 64%)]  Loss: 3.828 (3.77)  Time: 0.673s, 1522.34/s  (0.694s, 1475.92/s)  LR: 6.130e-04  Data: 0.011 (0.013)
Train: 258 [ 850/1251 ( 68%)]  Loss: 3.836 (3.77)  Time: 0.675s, 1517.65/s  (0.694s, 1475.80/s)  LR: 6.130e-04  Data: 0.014 (0.013)
Train: 258 [ 900/1251 ( 72%)]  Loss: 3.644 (3.76)  Time: 0.717s, 1427.66/s  (0.694s, 1475.49/s)  LR: 6.130e-04  Data: 0.011 (0.013)
Train: 258 [ 950/1251 ( 76%)]  Loss: 3.722 (3.76)  Time: 0.716s, 1429.86/s  (0.694s, 1475.32/s)  LR: 6.130e-04  Data: 0.011 (0.012)
Train: 258 [1000/1251 ( 80%)]  Loss: 3.607 (3.75)  Time: 0.721s, 1421.07/s  (0.694s, 1475.52/s)  LR: 6.130e-04  Data: 0.016 (0.012)
Train: 258 [1050/1251 ( 84%)]  Loss: 3.601 (3.75)  Time: 0.674s, 1520.32/s  (0.694s, 1475.21/s)  LR: 6.130e-04  Data: 0.011 (0.012)
Train: 258 [1100/1251 ( 88%)]  Loss: 3.317 (3.73)  Time: 0.706s, 1449.78/s  (0.694s, 1474.99/s)  LR: 6.130e-04  Data: 0.011 (0.012)
Train: 258 [1150/1251 ( 92%)]  Loss: 3.734 (3.73)  Time: 0.716s, 1429.56/s  (0.694s, 1475.16/s)  LR: 6.130e-04  Data: 0.009 (0.012)
Train: 258 [1200/1251 ( 96%)]  Loss: 3.801 (3.73)  Time: 0.673s, 1521.84/s  (0.694s, 1475.47/s)  LR: 6.130e-04  Data: 0.011 (0.012)
Train: 258 [1250/1251 (100%)]  Loss: 3.364 (3.72)  Time: 0.653s, 1568.81/s  (0.694s, 1476.17/s)  LR: 6.130e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.598 (1.598)  Loss:  0.9077 (0.9077)  Acc@1: 88.3789 (88.3789)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.136 (0.583)  Loss:  0.9873 (1.4836)  Acc@1: 83.0189 (72.5580)  Acc@5: 95.4009 (91.1840)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-257.pth.tar', 73.07200016601563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-254.pth.tar', 72.68600006835938)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-256.pth.tar', 72.62999988525391)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-258.pth.tar', 72.55800012207031)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-241.pth.tar', 72.53999996826172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-250.pth.tar', 72.46200000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-247.pth.tar', 72.35600009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-242.pth.tar', 72.2440000439453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-229.pth.tar', 72.24199998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-234.pth.tar', 72.23200009033204)

Train: 259 [   0/1251 (  0%)]  Loss: 3.657 (3.66)  Time: 2.230s,  459.12/s  (2.230s,  459.12/s)  LR: 6.105e-04  Data: 1.614 (1.614)
Train: 259 [  50/1251 (  4%)]  Loss: 3.456 (3.56)  Time: 0.666s, 1536.70/s  (0.725s, 1413.04/s)  LR: 6.105e-04  Data: 0.011 (0.045)
Train: 259 [ 100/1251 (  8%)]  Loss: 3.547 (3.55)  Time: 0.670s, 1527.89/s  (0.709s, 1444.25/s)  LR: 6.105e-04  Data: 0.009 (0.028)
Train: 259 [ 150/1251 ( 12%)]  Loss: 3.463 (3.53)  Time: 0.713s, 1435.77/s  (0.707s, 1447.88/s)  LR: 6.105e-04  Data: 0.011 (0.022)
Train: 259 [ 200/1251 ( 16%)]  Loss: 3.768 (3.58)  Time: 0.704s, 1455.24/s  (0.702s, 1458.23/s)  LR: 6.105e-04  Data: 0.009 (0.019)
Train: 259 [ 250/1251 ( 20%)]  Loss: 3.651 (3.59)  Time: 0.705s, 1451.82/s  (0.699s, 1464.30/s)  LR: 6.105e-04  Data: 0.009 (0.018)
Train: 259 [ 300/1251 ( 24%)]  Loss: 3.941 (3.64)  Time: 0.666s, 1537.51/s  (0.697s, 1469.20/s)  LR: 6.105e-04  Data: 0.010 (0.016)
Train: 259 [ 350/1251 ( 28%)]  Loss: 3.989 (3.68)  Time: 0.673s, 1522.27/s  (0.695s, 1473.00/s)  LR: 6.105e-04  Data: 0.010 (0.016)
Train: 259 [ 400/1251 ( 32%)]  Loss: 3.741 (3.69)  Time: 0.671s, 1525.32/s  (0.695s, 1474.36/s)  LR: 6.105e-04  Data: 0.010 (0.015)
Train: 259 [ 450/1251 ( 36%)]  Loss: 4.104 (3.73)  Time: 0.672s, 1523.47/s  (0.693s, 1476.80/s)  LR: 6.105e-04  Data: 0.010 (0.015)
Train: 259 [ 500/1251 ( 40%)]  Loss: 3.957 (3.75)  Time: 0.710s, 1443.11/s  (0.694s, 1475.86/s)  LR: 6.105e-04  Data: 0.010 (0.014)
Train: 259 [ 550/1251 ( 44%)]  Loss: 3.735 (3.75)  Time: 0.673s, 1521.34/s  (0.694s, 1475.26/s)  LR: 6.105e-04  Data: 0.010 (0.014)
Train: 259 [ 600/1251 ( 48%)]  Loss: 3.575 (3.74)  Time: 0.712s, 1439.14/s  (0.694s, 1475.65/s)  LR: 6.105e-04  Data: 0.010 (0.014)
Train: 259 [ 650/1251 ( 52%)]  Loss: 4.005 (3.76)  Time: 0.673s, 1522.67/s  (0.694s, 1476.39/s)  LR: 6.105e-04  Data: 0.010 (0.013)
Train: 259 [ 700/1251 ( 56%)]  Loss: 3.043 (3.71)  Time: 0.678s, 1509.78/s  (0.693s, 1477.56/s)  LR: 6.105e-04  Data: 0.009 (0.013)
Train: 259 [ 750/1251 ( 60%)]  Loss: 3.746 (3.71)  Time: 0.675s, 1515.95/s  (0.693s, 1477.87/s)  LR: 6.105e-04  Data: 0.010 (0.013)
Train: 259 [ 800/1251 ( 64%)]  Loss: 3.752 (3.71)  Time: 0.669s, 1530.01/s  (0.693s, 1477.44/s)  LR: 6.105e-04  Data: 0.009 (0.013)
Train: 259 [ 850/1251 ( 68%)]  Loss: 3.636 (3.71)  Time: 0.675s, 1516.94/s  (0.693s, 1477.67/s)  LR: 6.105e-04  Data: 0.010 (0.013)
Train: 259 [ 900/1251 ( 72%)]  Loss: 3.784 (3.71)  Time: 0.685s, 1494.30/s  (0.693s, 1478.38/s)  LR: 6.105e-04  Data: 0.025 (0.013)
Train: 259 [ 950/1251 ( 76%)]  Loss: 3.608 (3.71)  Time: 0.716s, 1429.63/s  (0.693s, 1478.50/s)  LR: 6.105e-04  Data: 0.009 (0.012)
Train: 259 [1000/1251 ( 80%)]  Loss: 3.946 (3.72)  Time: 0.679s, 1508.51/s  (0.692s, 1478.73/s)  LR: 6.105e-04  Data: 0.010 (0.012)
Train: 259 [1050/1251 ( 84%)]  Loss: 3.116 (3.69)  Time: 0.670s, 1527.82/s  (0.692s, 1478.90/s)  LR: 6.105e-04  Data: 0.009 (0.012)
Train: 259 [1100/1251 ( 88%)]  Loss: 3.784 (3.70)  Time: 0.715s, 1433.01/s  (0.692s, 1479.09/s)  LR: 6.105e-04  Data: 0.009 (0.012)
Train: 259 [1150/1251 ( 92%)]  Loss: 3.879 (3.70)  Time: 0.697s, 1469.31/s  (0.692s, 1479.31/s)  LR: 6.105e-04  Data: 0.010 (0.012)
Train: 259 [1200/1251 ( 96%)]  Loss: 4.075 (3.72)  Time: 0.666s, 1537.96/s  (0.693s, 1478.22/s)  LR: 6.105e-04  Data: 0.009 (0.012)
Train: 259 [1250/1251 (100%)]  Loss: 3.525 (3.71)  Time: 0.659s, 1552.86/s  (0.693s, 1477.74/s)  LR: 6.105e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.561 (1.561)  Loss:  0.8442 (0.8442)  Acc@1: 87.2070 (87.2070)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.136 (0.583)  Loss:  0.9478 (1.4546)  Acc@1: 83.2547 (72.2000)  Acc@5: 96.6981 (90.6700)
Train: 260 [   0/1251 (  0%)]  Loss: 3.686 (3.69)  Time: 2.204s,  464.63/s  (2.204s,  464.63/s)  LR: 6.079e-04  Data: 1.588 (1.588)
Train: 260 [  50/1251 (  4%)]  Loss: 3.731 (3.71)  Time: 0.755s, 1357.17/s  (0.732s, 1399.67/s)  LR: 6.079e-04  Data: 0.011 (0.049)
Train: 260 [ 100/1251 (  8%)]  Loss: 3.992 (3.80)  Time: 0.719s, 1423.95/s  (0.711s, 1440.11/s)  LR: 6.079e-04  Data: 0.011 (0.030)
Train: 260 [ 150/1251 ( 12%)]  Loss: 3.705 (3.78)  Time: 0.672s, 1524.40/s  (0.704s, 1454.36/s)  LR: 6.079e-04  Data: 0.012 (0.023)
Train: 260 [ 200/1251 ( 16%)]  Loss: 3.878 (3.80)  Time: 0.705s, 1452.91/s  (0.701s, 1459.91/s)  LR: 6.079e-04  Data: 0.014 (0.020)
Train: 260 [ 250/1251 ( 20%)]  Loss: 3.765 (3.79)  Time: 0.686s, 1492.23/s  (0.701s, 1461.14/s)  LR: 6.079e-04  Data: 0.010 (0.018)
Train: 260 [ 300/1251 ( 24%)]  Loss: 3.752 (3.79)  Time: 0.672s, 1524.62/s  (0.699s, 1464.18/s)  LR: 6.079e-04  Data: 0.012 (0.017)
Train: 260 [ 350/1251 ( 28%)]  Loss: 3.635 (3.77)  Time: 0.718s, 1425.51/s  (0.700s, 1463.37/s)  LR: 6.079e-04  Data: 0.015 (0.016)
Train: 260 [ 400/1251 ( 32%)]  Loss: 3.632 (3.75)  Time: 0.702s, 1457.87/s  (0.699s, 1464.70/s)  LR: 6.079e-04  Data: 0.011 (0.016)
Train: 260 [ 450/1251 ( 36%)]  Loss: 3.369 (3.71)  Time: 0.672s, 1524.31/s  (0.698s, 1466.54/s)  LR: 6.079e-04  Data: 0.011 (0.015)
Train: 260 [ 500/1251 ( 40%)]  Loss: 3.861 (3.73)  Time: 0.695s, 1474.04/s  (0.697s, 1468.59/s)  LR: 6.079e-04  Data: 0.009 (0.015)
Train: 260 [ 550/1251 ( 44%)]  Loss: 3.670 (3.72)  Time: 0.695s, 1472.54/s  (0.697s, 1470.07/s)  LR: 6.079e-04  Data: 0.011 (0.014)
Train: 260 [ 600/1251 ( 48%)]  Loss: 3.683 (3.72)  Time: 0.669s, 1530.27/s  (0.696s, 1471.23/s)  LR: 6.079e-04  Data: 0.010 (0.014)
Train: 260 [ 650/1251 ( 52%)]  Loss: 3.544 (3.71)  Time: 0.674s, 1518.49/s  (0.696s, 1472.18/s)  LR: 6.079e-04  Data: 0.011 (0.014)
Train: 260 [ 700/1251 ( 56%)]  Loss: 3.753 (3.71)  Time: 0.704s, 1455.08/s  (0.696s, 1471.84/s)  LR: 6.079e-04  Data: 0.011 (0.013)
Train: 260 [ 750/1251 ( 60%)]  Loss: 3.598 (3.70)  Time: 0.715s, 1431.17/s  (0.695s, 1472.51/s)  LR: 6.079e-04  Data: 0.010 (0.013)
Train: 260 [ 800/1251 ( 64%)]  Loss: 3.940 (3.72)  Time: 0.673s, 1521.19/s  (0.695s, 1473.14/s)  LR: 6.079e-04  Data: 0.011 (0.013)
Train: 260 [ 850/1251 ( 68%)]  Loss: 3.669 (3.71)  Time: 0.712s, 1438.76/s  (0.695s, 1474.07/s)  LR: 6.079e-04  Data: 0.010 (0.013)
Train: 260 [ 900/1251 ( 72%)]  Loss: 3.681 (3.71)  Time: 0.675s, 1515.99/s  (0.695s, 1473.97/s)  LR: 6.079e-04  Data: 0.011 (0.013)
Train: 260 [ 950/1251 ( 76%)]  Loss: 3.803 (3.72)  Time: 0.690s, 1484.61/s  (0.694s, 1475.01/s)  LR: 6.079e-04  Data: 0.010 (0.013)
Train: 260 [1000/1251 ( 80%)]  Loss: 3.592 (3.71)  Time: 0.670s, 1527.29/s  (0.694s, 1475.26/s)  LR: 6.079e-04  Data: 0.010 (0.013)
Train: 260 [1050/1251 ( 84%)]  Loss: 3.733 (3.71)  Time: 0.671s, 1526.30/s  (0.694s, 1474.52/s)  LR: 6.079e-04  Data: 0.011 (0.012)
Train: 260 [1100/1251 ( 88%)]  Loss: 3.566 (3.71)  Time: 0.675s, 1516.99/s  (0.694s, 1475.19/s)  LR: 6.079e-04  Data: 0.011 (0.012)
Train: 260 [1150/1251 ( 92%)]  Loss: 4.028 (3.72)  Time: 0.671s, 1525.60/s  (0.694s, 1475.95/s)  LR: 6.079e-04  Data: 0.010 (0.012)
Train: 260 [1200/1251 ( 96%)]  Loss: 3.568 (3.71)  Time: 0.667s, 1535.79/s  (0.694s, 1475.74/s)  LR: 6.079e-04  Data: 0.009 (0.012)
Train: 260 [1250/1251 (100%)]  Loss: 4.005 (3.72)  Time: 0.688s, 1488.71/s  (0.694s, 1476.30/s)  LR: 6.079e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.544 (1.544)  Loss:  0.8926 (0.8926)  Acc@1: 88.5742 (88.5742)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.136 (0.586)  Loss:  1.0166 (1.5214)  Acc@1: 83.3726 (72.5240)  Acc@5: 95.7547 (91.1160)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-257.pth.tar', 73.07200016601563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-254.pth.tar', 72.68600006835938)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-256.pth.tar', 72.62999988525391)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-258.pth.tar', 72.55800012207031)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-241.pth.tar', 72.53999996826172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-260.pth.tar', 72.52400001708985)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-250.pth.tar', 72.46200000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-247.pth.tar', 72.35600009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-242.pth.tar', 72.2440000439453)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-229.pth.tar', 72.24199998291016)

Train: 261 [   0/1251 (  0%)]  Loss: 3.373 (3.37)  Time: 2.182s,  469.29/s  (2.182s,  469.29/s)  LR: 6.054e-04  Data: 1.559 (1.559)
Train: 261 [  50/1251 (  4%)]  Loss: 3.641 (3.51)  Time: 0.723s, 1415.69/s  (0.730s, 1402.39/s)  LR: 6.054e-04  Data: 0.012 (0.050)
Train: 261 [ 100/1251 (  8%)]  Loss: 3.482 (3.50)  Time: 0.707s, 1448.39/s  (0.713s, 1436.34/s)  LR: 6.054e-04  Data: 0.009 (0.031)
Train: 261 [ 150/1251 ( 12%)]  Loss: 3.656 (3.54)  Time: 0.673s, 1522.19/s  (0.707s, 1447.79/s)  LR: 6.054e-04  Data: 0.009 (0.024)
Train: 261 [ 200/1251 ( 16%)]  Loss: 3.704 (3.57)  Time: 0.671s, 1525.69/s  (0.703s, 1457.02/s)  LR: 6.054e-04  Data: 0.010 (0.020)
Train: 261 [ 250/1251 ( 20%)]  Loss: 3.937 (3.63)  Time: 0.692s, 1480.27/s  (0.700s, 1462.42/s)  LR: 6.054e-04  Data: 0.009 (0.019)
Train: 261 [ 300/1251 ( 24%)]  Loss: 3.949 (3.68)  Time: 0.795s, 1288.45/s  (0.700s, 1463.12/s)  LR: 6.054e-04  Data: 0.008 (0.017)
Train: 261 [ 350/1251 ( 28%)]  Loss: 3.443 (3.65)  Time: 0.672s, 1524.37/s  (0.698s, 1467.42/s)  LR: 6.054e-04  Data: 0.010 (0.016)
Train: 261 [ 400/1251 ( 32%)]  Loss: 3.738 (3.66)  Time: 0.748s, 1369.56/s  (0.697s, 1468.61/s)  LR: 6.054e-04  Data: 0.009 (0.015)
Train: 261 [ 450/1251 ( 36%)]  Loss: 3.401 (3.63)  Time: 0.715s, 1432.05/s  (0.697s, 1469.12/s)  LR: 6.054e-04  Data: 0.018 (0.015)
Train: 261 [ 500/1251 ( 40%)]  Loss: 3.570 (3.63)  Time: 0.705s, 1452.81/s  (0.697s, 1469.69/s)  LR: 6.054e-04  Data: 0.010 (0.015)
Train: 261 [ 550/1251 ( 44%)]  Loss: 3.818 (3.64)  Time: 0.671s, 1526.90/s  (0.696s, 1470.39/s)  LR: 6.054e-04  Data: 0.011 (0.014)
Train: 261 [ 600/1251 ( 48%)]  Loss: 3.764 (3.65)  Time: 0.701s, 1461.63/s  (0.696s, 1471.87/s)  LR: 6.054e-04  Data: 0.009 (0.014)
Train: 261 [ 650/1251 ( 52%)]  Loss: 3.628 (3.65)  Time: 0.699s, 1464.18/s  (0.696s, 1472.13/s)  LR: 6.054e-04  Data: 0.010 (0.014)
Train: 261 [ 700/1251 ( 56%)]  Loss: 3.812 (3.66)  Time: 0.710s, 1443.04/s  (0.695s, 1473.31/s)  LR: 6.054e-04  Data: 0.011 (0.013)
Train: 261 [ 750/1251 ( 60%)]  Loss: 4.191 (3.69)  Time: 0.666s, 1537.80/s  (0.695s, 1473.49/s)  LR: 6.054e-04  Data: 0.010 (0.013)
Train: 261 [ 800/1251 ( 64%)]  Loss: 3.700 (3.69)  Time: 0.665s, 1540.02/s  (0.695s, 1473.91/s)  LR: 6.054e-04  Data: 0.009 (0.013)
Train: 261 [ 850/1251 ( 68%)]  Loss: 3.387 (3.68)  Time: 0.700s, 1463.87/s  (0.694s, 1474.54/s)  LR: 6.054e-04  Data: 0.009 (0.013)
Train: 261 [ 900/1251 ( 72%)]  Loss: 3.775 (3.68)  Time: 0.671s, 1525.99/s  (0.694s, 1475.14/s)  LR: 6.054e-04  Data: 0.010 (0.013)
Train: 261 [ 950/1251 ( 76%)]  Loss: 3.780 (3.69)  Time: 0.674s, 1519.46/s  (0.694s, 1474.90/s)  LR: 6.054e-04  Data: 0.010 (0.013)
Train: 261 [1000/1251 ( 80%)]  Loss: 3.808 (3.69)  Time: 0.673s, 1522.13/s  (0.694s, 1476.14/s)  LR: 6.054e-04  Data: 0.011 (0.012)
Train: 261 [1050/1251 ( 84%)]  Loss: 3.829 (3.70)  Time: 0.684s, 1497.07/s  (0.694s, 1476.44/s)  LR: 6.054e-04  Data: 0.013 (0.012)
Train: 261 [1100/1251 ( 88%)]  Loss: 3.663 (3.70)  Time: 0.703s, 1456.12/s  (0.694s, 1475.94/s)  LR: 6.054e-04  Data: 0.009 (0.012)
Train: 261 [1150/1251 ( 92%)]  Loss: 3.786 (3.70)  Time: 0.671s, 1526.98/s  (0.694s, 1475.62/s)  LR: 6.054e-04  Data: 0.010 (0.012)
Train: 261 [1200/1251 ( 96%)]  Loss: 3.438 (3.69)  Time: 0.706s, 1450.97/s  (0.694s, 1475.75/s)  LR: 6.054e-04  Data: 0.010 (0.012)
Train: 261 [1250/1251 (100%)]  Loss: 3.547 (3.69)  Time: 0.661s, 1549.17/s  (0.694s, 1476.35/s)  LR: 6.054e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.563 (1.563)  Loss:  0.8779 (0.8779)  Acc@1: 89.3555 (89.3555)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.136 (0.586)  Loss:  0.9756 (1.4983)  Acc@1: 84.1981 (72.5700)  Acc@5: 96.2264 (91.0080)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-257.pth.tar', 73.07200016601563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-254.pth.tar', 72.68600006835938)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-256.pth.tar', 72.62999988525391)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-261.pth.tar', 72.5700001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-258.pth.tar', 72.55800012207031)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-241.pth.tar', 72.53999996826172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-260.pth.tar', 72.52400001708985)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-250.pth.tar', 72.46200000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-247.pth.tar', 72.35600009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-242.pth.tar', 72.2440000439453)

Train: 262 [   0/1251 (  0%)]  Loss: 3.650 (3.65)  Time: 2.246s,  455.89/s  (2.246s,  455.89/s)  LR: 6.028e-04  Data: 1.576 (1.576)
Train: 262 [  50/1251 (  4%)]  Loss: 4.029 (3.84)  Time: 0.759s, 1348.61/s  (0.724s, 1415.26/s)  LR: 6.028e-04  Data: 0.009 (0.047)
Train: 262 [ 100/1251 (  8%)]  Loss: 3.797 (3.83)  Time: 0.706s, 1450.02/s  (0.710s, 1442.98/s)  LR: 6.028e-04  Data: 0.010 (0.029)
Train: 262 [ 150/1251 ( 12%)]  Loss: 3.691 (3.79)  Time: 0.667s, 1535.31/s  (0.705s, 1451.61/s)  LR: 6.028e-04  Data: 0.011 (0.023)
Train: 262 [ 200/1251 ( 16%)]  Loss: 3.207 (3.67)  Time: 0.707s, 1448.45/s  (0.702s, 1459.06/s)  LR: 6.028e-04  Data: 0.010 (0.020)
Train: 262 [ 250/1251 ( 20%)]  Loss: 3.390 (3.63)  Time: 0.673s, 1522.08/s  (0.700s, 1463.69/s)  LR: 6.028e-04  Data: 0.010 (0.018)
Train: 262 [ 300/1251 ( 24%)]  Loss: 4.138 (3.70)  Time: 0.705s, 1452.31/s  (0.698s, 1466.35/s)  LR: 6.028e-04  Data: 0.011 (0.016)
Train: 262 [ 350/1251 ( 28%)]  Loss: 3.779 (3.71)  Time: 0.672s, 1523.39/s  (0.698s, 1467.91/s)  LR: 6.028e-04  Data: 0.012 (0.016)
Train: 262 [ 400/1251 ( 32%)]  Loss: 3.664 (3.71)  Time: 0.681s, 1504.21/s  (0.697s, 1468.45/s)  LR: 6.028e-04  Data: 0.011 (0.015)
Train: 262 [ 450/1251 ( 36%)]  Loss: 3.910 (3.73)  Time: 0.723s, 1415.85/s  (0.696s, 1470.60/s)  LR: 6.028e-04  Data: 0.011 (0.014)
Train: 262 [ 500/1251 ( 40%)]  Loss: 3.647 (3.72)  Time: 0.673s, 1521.87/s  (0.696s, 1472.22/s)  LR: 6.028e-04  Data: 0.011 (0.014)
Train: 262 [ 550/1251 ( 44%)]  Loss: 3.777 (3.72)  Time: 0.705s, 1452.38/s  (0.696s, 1472.11/s)  LR: 6.028e-04  Data: 0.011 (0.014)
Train: 262 [ 600/1251 ( 48%)]  Loss: 3.916 (3.74)  Time: 0.713s, 1436.88/s  (0.696s, 1471.51/s)  LR: 6.028e-04  Data: 0.009 (0.013)
Train: 262 [ 650/1251 ( 52%)]  Loss: 3.288 (3.71)  Time: 0.702s, 1458.10/s  (0.696s, 1472.30/s)  LR: 6.028e-04  Data: 0.009 (0.013)
Train: 262 [ 700/1251 ( 56%)]  Loss: 3.528 (3.69)  Time: 0.667s, 1535.65/s  (0.696s, 1471.67/s)  LR: 6.028e-04  Data: 0.010 (0.013)
Train: 262 [ 750/1251 ( 60%)]  Loss: 3.535 (3.68)  Time: 0.677s, 1512.93/s  (0.696s, 1472.19/s)  LR: 6.028e-04  Data: 0.010 (0.013)
Train: 262 [ 800/1251 ( 64%)]  Loss: 3.580 (3.68)  Time: 0.714s, 1435.03/s  (0.696s, 1472.19/s)  LR: 6.028e-04  Data: 0.010 (0.013)
Train: 262 [ 850/1251 ( 68%)]  Loss: 3.696 (3.68)  Time: 0.713s, 1436.17/s  (0.696s, 1472.28/s)  LR: 6.028e-04  Data: 0.009 (0.013)
Train: 262 [ 900/1251 ( 72%)]  Loss: 3.725 (3.68)  Time: 0.671s, 1526.30/s  (0.696s, 1472.24/s)  LR: 6.028e-04  Data: 0.011 (0.012)
Train: 262 [ 950/1251 ( 76%)]  Loss: 3.629 (3.68)  Time: 0.704s, 1454.70/s  (0.696s, 1471.05/s)  LR: 6.028e-04  Data: 0.011 (0.012)
Train: 262 [1000/1251 ( 80%)]  Loss: 4.059 (3.70)  Time: 0.672s, 1523.90/s  (0.696s, 1472.27/s)  LR: 6.028e-04  Data: 0.011 (0.012)
Train: 262 [1050/1251 ( 84%)]  Loss: 3.682 (3.70)  Time: 0.709s, 1444.83/s  (0.696s, 1472.18/s)  LR: 6.028e-04  Data: 0.009 (0.012)
Train: 262 [1100/1251 ( 88%)]  Loss: 3.881 (3.70)  Time: 0.722s, 1418.39/s  (0.696s, 1471.79/s)  LR: 6.028e-04  Data: 0.013 (0.012)
Train: 262 [1150/1251 ( 92%)]  Loss: 3.736 (3.71)  Time: 0.701s, 1460.88/s  (0.696s, 1472.00/s)  LR: 6.028e-04  Data: 0.009 (0.012)
Train: 262 [1200/1251 ( 96%)]  Loss: 3.357 (3.69)  Time: 0.728s, 1405.95/s  (0.696s, 1472.08/s)  LR: 6.028e-04  Data: 0.010 (0.012)
Train: 262 [1250/1251 (100%)]  Loss: 3.639 (3.69)  Time: 0.656s, 1561.08/s  (0.695s, 1472.52/s)  LR: 6.028e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.560 (1.560)  Loss:  0.8589 (0.8589)  Acc@1: 88.0859 (88.0859)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.136 (0.574)  Loss:  0.9155 (1.4664)  Acc@1: 85.3774 (72.7680)  Acc@5: 97.2877 (91.1980)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-257.pth.tar', 73.07200016601563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-262.pth.tar', 72.76800011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-254.pth.tar', 72.68600006835938)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-256.pth.tar', 72.62999988525391)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-261.pth.tar', 72.5700001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-258.pth.tar', 72.55800012207031)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-241.pth.tar', 72.53999996826172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-260.pth.tar', 72.52400001708985)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-250.pth.tar', 72.46200000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-247.pth.tar', 72.35600009277344)

Train: 263 [   0/1251 (  0%)]  Loss: 3.630 (3.63)  Time: 2.262s,  452.78/s  (2.262s,  452.78/s)  LR: 6.003e-04  Data: 1.590 (1.590)
Train: 263 [  50/1251 (  4%)]  Loss: 3.693 (3.66)  Time: 0.681s, 1503.90/s  (0.728s, 1406.46/s)  LR: 6.003e-04  Data: 0.014 (0.047)
Train: 263 [ 100/1251 (  8%)]  Loss: 3.633 (3.65)  Time: 0.691s, 1482.30/s  (0.714s, 1434.83/s)  LR: 6.003e-04  Data: 0.008 (0.029)
Train: 263 [ 150/1251 ( 12%)]  Loss: 4.012 (3.74)  Time: 0.805s, 1271.74/s  (0.710s, 1441.35/s)  LR: 6.003e-04  Data: 0.009 (0.023)
Train: 263 [ 200/1251 ( 16%)]  Loss: 3.534 (3.70)  Time: 0.680s, 1505.33/s  (0.707s, 1447.79/s)  LR: 6.003e-04  Data: 0.010 (0.020)
Train: 263 [ 250/1251 ( 20%)]  Loss: 4.107 (3.77)  Time: 0.672s, 1523.92/s  (0.705s, 1452.04/s)  LR: 6.003e-04  Data: 0.009 (0.018)
Train: 263 [ 300/1251 ( 24%)]  Loss: 3.661 (3.75)  Time: 0.671s, 1525.50/s  (0.703s, 1456.63/s)  LR: 6.003e-04  Data: 0.010 (0.017)
Train: 263 [ 350/1251 ( 28%)]  Loss: 3.699 (3.75)  Time: 0.676s, 1515.50/s  (0.702s, 1457.84/s)  LR: 6.003e-04  Data: 0.015 (0.016)
Train: 263 [ 400/1251 ( 32%)]  Loss: 3.353 (3.70)  Time: 0.703s, 1456.24/s  (0.700s, 1462.01/s)  LR: 6.003e-04  Data: 0.010 (0.015)
Train: 263 [ 450/1251 ( 36%)]  Loss: 3.678 (3.70)  Time: 0.668s, 1532.32/s  (0.699s, 1464.54/s)  LR: 6.003e-04  Data: 0.010 (0.015)
Train: 263 [ 500/1251 ( 40%)]  Loss: 3.276 (3.66)  Time: 0.695s, 1472.81/s  (0.699s, 1465.63/s)  LR: 6.003e-04  Data: 0.009 (0.014)
Train: 263 [ 550/1251 ( 44%)]  Loss: 3.492 (3.65)  Time: 0.697s, 1469.39/s  (0.699s, 1465.17/s)  LR: 6.003e-04  Data: 0.010 (0.014)
Train: 263 [ 600/1251 ( 48%)]  Loss: 3.746 (3.65)  Time: 0.667s, 1535.86/s  (0.698s, 1467.35/s)  LR: 6.003e-04  Data: 0.011 (0.014)
Train: 263 [ 650/1251 ( 52%)]  Loss: 3.875 (3.67)  Time: 0.691s, 1481.17/s  (0.697s, 1469.59/s)  LR: 6.003e-04  Data: 0.011 (0.013)
Train: 263 [ 700/1251 ( 56%)]  Loss: 4.002 (3.69)  Time: 0.673s, 1521.11/s  (0.696s, 1470.52/s)  LR: 6.003e-04  Data: 0.009 (0.013)
Train: 263 [ 750/1251 ( 60%)]  Loss: 3.611 (3.69)  Time: 0.774s, 1322.46/s  (0.696s, 1470.77/s)  LR: 6.003e-04  Data: 0.010 (0.013)
Train: 263 [ 800/1251 ( 64%)]  Loss: 3.724 (3.69)  Time: 0.671s, 1526.43/s  (0.696s, 1471.16/s)  LR: 6.003e-04  Data: 0.010 (0.013)
Train: 263 [ 850/1251 ( 68%)]  Loss: 3.511 (3.68)  Time: 0.693s, 1477.32/s  (0.696s, 1471.64/s)  LR: 6.003e-04  Data: 0.009 (0.013)
Train: 263 [ 900/1251 ( 72%)]  Loss: 3.565 (3.67)  Time: 0.733s, 1397.93/s  (0.696s, 1471.60/s)  LR: 6.003e-04  Data: 0.009 (0.013)
Train: 263 [ 950/1251 ( 76%)]  Loss: 3.873 (3.68)  Time: 0.673s, 1521.22/s  (0.696s, 1472.25/s)  LR: 6.003e-04  Data: 0.008 (0.012)
Train: 263 [1000/1251 ( 80%)]  Loss: 3.598 (3.68)  Time: 0.701s, 1460.91/s  (0.695s, 1473.31/s)  LR: 6.003e-04  Data: 0.010 (0.012)
Train: 263 [1050/1251 ( 84%)]  Loss: 3.527 (3.67)  Time: 0.702s, 1458.55/s  (0.695s, 1473.43/s)  LR: 6.003e-04  Data: 0.009 (0.012)
Train: 263 [1100/1251 ( 88%)]  Loss: 3.810 (3.68)  Time: 0.704s, 1454.55/s  (0.695s, 1474.24/s)  LR: 6.003e-04  Data: 0.011 (0.012)
Train: 263 [1150/1251 ( 92%)]  Loss: 3.630 (3.68)  Time: 0.710s, 1441.87/s  (0.694s, 1474.87/s)  LR: 6.003e-04  Data: 0.011 (0.012)
Train: 263 [1200/1251 ( 96%)]  Loss: 3.874 (3.68)  Time: 0.673s, 1521.35/s  (0.694s, 1475.12/s)  LR: 6.003e-04  Data: 0.010 (0.012)
Train: 263 [1250/1251 (100%)]  Loss: 3.528 (3.68)  Time: 0.655s, 1563.17/s  (0.694s, 1475.55/s)  LR: 6.003e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.531 (1.531)  Loss:  0.9009 (0.9009)  Acc@1: 89.5508 (89.5508)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.137 (0.581)  Loss:  1.0361 (1.5360)  Acc@1: 84.5519 (72.2700)  Acc@5: 97.1698 (90.8740)
Train: 264 [   0/1251 (  0%)]  Loss: 3.709 (3.71)  Time: 2.237s,  457.73/s  (2.237s,  457.73/s)  LR: 5.978e-04  Data: 1.582 (1.582)
Train: 264 [  50/1251 (  4%)]  Loss: 3.842 (3.78)  Time: 0.699s, 1464.29/s  (0.731s, 1401.15/s)  LR: 5.978e-04  Data: 0.016 (0.048)
Train: 264 [ 100/1251 (  8%)]  Loss: 3.635 (3.73)  Time: 0.674s, 1519.29/s  (0.712s, 1438.30/s)  LR: 5.978e-04  Data: 0.010 (0.030)
Train: 264 [ 150/1251 ( 12%)]  Loss: 3.743 (3.73)  Time: 0.672s, 1522.85/s  (0.705s, 1452.90/s)  LR: 5.978e-04  Data: 0.010 (0.023)
Train: 264 [ 200/1251 ( 16%)]  Loss: 3.492 (3.68)  Time: 0.703s, 1456.17/s  (0.703s, 1456.40/s)  LR: 5.978e-04  Data: 0.009 (0.020)
Train: 264 [ 250/1251 ( 20%)]  Loss: 3.531 (3.66)  Time: 0.672s, 1522.84/s  (0.702s, 1458.26/s)  LR: 5.978e-04  Data: 0.011 (0.018)
Train: 264 [ 300/1251 ( 24%)]  Loss: 4.052 (3.71)  Time: 0.682s, 1500.97/s  (0.699s, 1464.43/s)  LR: 5.978e-04  Data: 0.010 (0.017)
Train: 264 [ 350/1251 ( 28%)]  Loss: 3.598 (3.70)  Time: 0.708s, 1446.41/s  (0.700s, 1463.29/s)  LR: 5.978e-04  Data: 0.010 (0.016)
Train: 264 [ 400/1251 ( 32%)]  Loss: 3.985 (3.73)  Time: 0.673s, 1522.17/s  (0.699s, 1465.64/s)  LR: 5.978e-04  Data: 0.011 (0.015)
Train: 264 [ 450/1251 ( 36%)]  Loss: 3.723 (3.73)  Time: 0.674s, 1519.68/s  (0.698s, 1466.89/s)  LR: 5.978e-04  Data: 0.011 (0.015)
Train: 264 [ 500/1251 ( 40%)]  Loss: 3.897 (3.75)  Time: 0.673s, 1521.01/s  (0.697s, 1469.39/s)  LR: 5.978e-04  Data: 0.013 (0.014)
Train: 264 [ 550/1251 ( 44%)]  Loss: 3.581 (3.73)  Time: 0.711s, 1440.72/s  (0.697s, 1469.86/s)  LR: 5.978e-04  Data: 0.010 (0.014)
Train: 264 [ 600/1251 ( 48%)]  Loss: 3.755 (3.73)  Time: 0.675s, 1517.94/s  (0.697s, 1469.88/s)  LR: 5.978e-04  Data: 0.011 (0.014)
Train: 264 [ 650/1251 ( 52%)]  Loss: 3.577 (3.72)  Time: 0.682s, 1502.22/s  (0.697s, 1470.07/s)  LR: 5.978e-04  Data: 0.011 (0.013)
Train: 264 [ 700/1251 ( 56%)]  Loss: 3.795 (3.73)  Time: 0.699s, 1465.97/s  (0.696s, 1470.86/s)  LR: 5.978e-04  Data: 0.009 (0.013)
Train: 264 [ 750/1251 ( 60%)]  Loss: 3.735 (3.73)  Time: 0.673s, 1522.20/s  (0.696s, 1471.45/s)  LR: 5.978e-04  Data: 0.011 (0.013)
Train: 264 [ 800/1251 ( 64%)]  Loss: 3.758 (3.73)  Time: 0.707s, 1447.51/s  (0.696s, 1472.12/s)  LR: 5.978e-04  Data: 0.011 (0.013)
Train: 264 [ 850/1251 ( 68%)]  Loss: 3.796 (3.73)  Time: 0.681s, 1504.64/s  (0.695s, 1472.94/s)  LR: 5.978e-04  Data: 0.010 (0.013)
Train: 264 [ 900/1251 ( 72%)]  Loss: 3.496 (3.72)  Time: 0.673s, 1521.33/s  (0.695s, 1472.81/s)  LR: 5.978e-04  Data: 0.010 (0.013)
Train: 264 [ 950/1251 ( 76%)]  Loss: 3.696 (3.72)  Time: 0.675s, 1517.93/s  (0.695s, 1473.30/s)  LR: 5.978e-04  Data: 0.010 (0.012)
Train: 264 [1000/1251 ( 80%)]  Loss: 3.399 (3.70)  Time: 0.693s, 1476.67/s  (0.695s, 1473.23/s)  LR: 5.978e-04  Data: 0.015 (0.012)
Train: 264 [1050/1251 ( 84%)]  Loss: 3.815 (3.71)  Time: 0.673s, 1521.36/s  (0.695s, 1472.56/s)  LR: 5.978e-04  Data: 0.013 (0.012)
Train: 264 [1100/1251 ( 88%)]  Loss: 3.552 (3.70)  Time: 0.705s, 1451.46/s  (0.695s, 1473.14/s)  LR: 5.978e-04  Data: 0.011 (0.012)
Train: 264 [1150/1251 ( 92%)]  Loss: 3.663 (3.70)  Time: 0.754s, 1357.63/s  (0.695s, 1473.11/s)  LR: 5.978e-04  Data: 0.010 (0.012)
Train: 264 [1200/1251 ( 96%)]  Loss: 4.070 (3.72)  Time: 0.719s, 1424.61/s  (0.695s, 1473.15/s)  LR: 5.978e-04  Data: 0.009 (0.012)
Train: 264 [1250/1251 (100%)]  Loss: 3.908 (3.72)  Time: 0.653s, 1568.20/s  (0.695s, 1473.58/s)  LR: 5.978e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.572 (1.572)  Loss:  0.9785 (0.9785)  Acc@1: 88.1836 (88.1836)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.136 (0.574)  Loss:  1.0498 (1.4549)  Acc@1: 83.2547 (72.4040)  Acc@5: 95.7547 (91.3160)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-257.pth.tar', 73.07200016601563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-262.pth.tar', 72.76800011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-254.pth.tar', 72.68600006835938)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-256.pth.tar', 72.62999988525391)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-261.pth.tar', 72.5700001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-258.pth.tar', 72.55800012207031)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-241.pth.tar', 72.53999996826172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-260.pth.tar', 72.52400001708985)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-250.pth.tar', 72.46200000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-264.pth.tar', 72.40399996582032)

Train: 265 [   0/1251 (  0%)]  Loss: 3.417 (3.42)  Time: 2.281s,  449.01/s  (2.281s,  449.01/s)  LR: 5.952e-04  Data: 1.665 (1.665)
Train: 265 [  50/1251 (  4%)]  Loss: 3.793 (3.61)  Time: 0.718s, 1426.46/s  (0.727s, 1408.85/s)  LR: 5.952e-04  Data: 0.010 (0.047)
Train: 265 [ 100/1251 (  8%)]  Loss: 3.530 (3.58)  Time: 0.738s, 1387.08/s  (0.711s, 1440.67/s)  LR: 5.952e-04  Data: 0.011 (0.029)
Train: 265 [ 150/1251 ( 12%)]  Loss: 3.402 (3.54)  Time: 0.669s, 1530.25/s  (0.705s, 1453.15/s)  LR: 5.952e-04  Data: 0.010 (0.023)
Train: 265 [ 200/1251 ( 16%)]  Loss: 3.616 (3.55)  Time: 0.709s, 1445.22/s  (0.701s, 1461.57/s)  LR: 5.952e-04  Data: 0.010 (0.020)
Train: 265 [ 250/1251 ( 20%)]  Loss: 3.669 (3.57)  Time: 0.718s, 1426.73/s  (0.699s, 1465.30/s)  LR: 5.952e-04  Data: 0.009 (0.018)
Train: 265 [ 300/1251 ( 24%)]  Loss: 3.425 (3.55)  Time: 0.671s, 1526.53/s  (0.696s, 1470.31/s)  LR: 5.952e-04  Data: 0.010 (0.017)
Train: 265 [ 350/1251 ( 28%)]  Loss: 3.965 (3.60)  Time: 0.681s, 1503.76/s  (0.695s, 1472.58/s)  LR: 5.952e-04  Data: 0.010 (0.016)
Train: 265 [ 400/1251 ( 32%)]  Loss: 3.430 (3.58)  Time: 0.670s, 1527.34/s  (0.694s, 1474.81/s)  LR: 5.952e-04  Data: 0.011 (0.015)
Train: 265 [ 450/1251 ( 36%)]  Loss: 3.771 (3.60)  Time: 0.705s, 1452.56/s  (0.695s, 1472.58/s)  LR: 5.952e-04  Data: 0.011 (0.015)
Train: 265 [ 500/1251 ( 40%)]  Loss: 3.447 (3.59)  Time: 0.672s, 1524.76/s  (0.695s, 1472.92/s)  LR: 5.952e-04  Data: 0.009 (0.014)
Train: 265 [ 550/1251 ( 44%)]  Loss: 3.597 (3.59)  Time: 0.712s, 1438.99/s  (0.696s, 1472.15/s)  LR: 5.952e-04  Data: 0.010 (0.014)
Train: 265 [ 600/1251 ( 48%)]  Loss: 3.496 (3.58)  Time: 0.712s, 1438.13/s  (0.695s, 1472.63/s)  LR: 5.952e-04  Data: 0.012 (0.014)
Train: 265 [ 650/1251 ( 52%)]  Loss: 3.584 (3.58)  Time: 0.702s, 1459.72/s  (0.696s, 1471.95/s)  LR: 5.952e-04  Data: 0.009 (0.013)
Train: 265 [ 700/1251 ( 56%)]  Loss: 3.552 (3.58)  Time: 0.668s, 1533.03/s  (0.696s, 1471.53/s)  LR: 5.952e-04  Data: 0.009 (0.013)
Train: 265 [ 750/1251 ( 60%)]  Loss: 3.687 (3.59)  Time: 0.688s, 1488.14/s  (0.696s, 1471.13/s)  LR: 5.952e-04  Data: 0.010 (0.013)
Train: 265 [ 800/1251 ( 64%)]  Loss: 3.961 (3.61)  Time: 0.670s, 1527.39/s  (0.696s, 1470.52/s)  LR: 5.952e-04  Data: 0.009 (0.013)
Train: 265 [ 850/1251 ( 68%)]  Loss: 3.867 (3.62)  Time: 0.735s, 1392.89/s  (0.696s, 1470.49/s)  LR: 5.952e-04  Data: 0.010 (0.013)
Train: 265 [ 900/1251 ( 72%)]  Loss: 3.861 (3.64)  Time: 0.740s, 1384.07/s  (0.698s, 1467.83/s)  LR: 5.952e-04  Data: 0.010 (0.013)
Train: 265 [ 950/1251 ( 76%)]  Loss: 3.602 (3.63)  Time: 0.698s, 1467.78/s  (0.698s, 1466.40/s)  LR: 5.952e-04  Data: 0.012 (0.013)
Train: 265 [1000/1251 ( 80%)]  Loss: 3.667 (3.64)  Time: 0.672s, 1524.43/s  (0.699s, 1465.33/s)  LR: 5.952e-04  Data: 0.010 (0.013)
Train: 265 [1050/1251 ( 84%)]  Loss: 3.542 (3.63)  Time: 0.673s, 1522.58/s  (0.698s, 1466.35/s)  LR: 5.952e-04  Data: 0.010 (0.012)
Train: 265 [1100/1251 ( 88%)]  Loss: 4.011 (3.65)  Time: 0.705s, 1452.92/s  (0.697s, 1468.24/s)  LR: 5.952e-04  Data: 0.009 (0.012)
Train: 265 [1150/1251 ( 92%)]  Loss: 3.621 (3.65)  Time: 0.665s, 1539.62/s  (0.697s, 1469.07/s)  LR: 5.952e-04  Data: 0.009 (0.012)
Train: 265 [1200/1251 ( 96%)]  Loss: 3.483 (3.64)  Time: 0.792s, 1292.68/s  (0.697s, 1469.30/s)  LR: 5.952e-04  Data: 0.009 (0.012)
Train: 265 [1250/1251 (100%)]  Loss: 3.602 (3.64)  Time: 0.698s, 1466.94/s  (0.697s, 1469.61/s)  LR: 5.952e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.469 (1.469)  Loss:  0.8701 (0.8701)  Acc@1: 89.2578 (89.2578)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.136 (0.575)  Loss:  1.0381 (1.4404)  Acc@1: 83.8443 (72.2840)  Acc@5: 95.6368 (90.9280)
Train: 266 [   0/1251 (  0%)]  Loss: 3.989 (3.99)  Time: 2.062s,  496.54/s  (2.062s,  496.54/s)  LR: 5.927e-04  Data: 1.443 (1.443)
Train: 266 [  50/1251 (  4%)]  Loss: 3.400 (3.69)  Time: 0.699s, 1464.54/s  (0.726s, 1410.13/s)  LR: 5.927e-04  Data: 0.012 (0.049)
Train: 266 [ 100/1251 (  8%)]  Loss: 3.795 (3.73)  Time: 0.730s, 1403.67/s  (0.708s, 1445.90/s)  LR: 5.927e-04  Data: 0.011 (0.030)
Train: 266 [ 150/1251 ( 12%)]  Loss: 3.592 (3.69)  Time: 0.689s, 1486.01/s  (0.702s, 1458.02/s)  LR: 5.927e-04  Data: 0.011 (0.023)
Train: 266 [ 200/1251 ( 16%)]  Loss: 3.537 (3.66)  Time: 0.725s, 1412.45/s  (0.698s, 1466.01/s)  LR: 5.927e-04  Data: 0.010 (0.020)
Train: 266 [ 250/1251 ( 20%)]  Loss: 3.856 (3.69)  Time: 0.727s, 1407.66/s  (0.698s, 1467.34/s)  LR: 5.927e-04  Data: 0.010 (0.018)
Train: 266 [ 300/1251 ( 24%)]  Loss: 3.749 (3.70)  Time: 0.695s, 1474.22/s  (0.696s, 1471.63/s)  LR: 5.927e-04  Data: 0.009 (0.017)
Train: 266 [ 350/1251 ( 28%)]  Loss: 3.661 (3.70)  Time: 0.707s, 1448.72/s  (0.698s, 1468.08/s)  LR: 5.927e-04  Data: 0.011 (0.016)
Train: 266 [ 400/1251 ( 32%)]  Loss: 3.852 (3.71)  Time: 0.689s, 1485.33/s  (0.697s, 1470.11/s)  LR: 5.927e-04  Data: 0.009 (0.015)
Train: 266 [ 450/1251 ( 36%)]  Loss: 3.648 (3.71)  Time: 0.672s, 1524.64/s  (0.695s, 1472.87/s)  LR: 5.927e-04  Data: 0.011 (0.015)
Train: 266 [ 500/1251 ( 40%)]  Loss: 3.835 (3.72)  Time: 0.715s, 1432.93/s  (0.696s, 1472.04/s)  LR: 5.927e-04  Data: 0.010 (0.014)
Train: 266 [ 550/1251 ( 44%)]  Loss: 3.736 (3.72)  Time: 0.706s, 1450.59/s  (0.696s, 1472.21/s)  LR: 5.927e-04  Data: 0.010 (0.014)
Train: 266 [ 600/1251 ( 48%)]  Loss: 3.937 (3.74)  Time: 0.671s, 1526.88/s  (0.695s, 1472.96/s)  LR: 5.927e-04  Data: 0.010 (0.014)
Train: 266 [ 650/1251 ( 52%)]  Loss: 3.705 (3.74)  Time: 0.673s, 1522.29/s  (0.694s, 1474.76/s)  LR: 5.927e-04  Data: 0.011 (0.013)
Train: 266 [ 700/1251 ( 56%)]  Loss: 3.717 (3.73)  Time: 0.676s, 1514.81/s  (0.694s, 1474.57/s)  LR: 5.927e-04  Data: 0.010 (0.013)
Train: 266 [ 750/1251 ( 60%)]  Loss: 3.816 (3.74)  Time: 0.673s, 1521.05/s  (0.695s, 1473.98/s)  LR: 5.927e-04  Data: 0.011 (0.013)
Train: 266 [ 800/1251 ( 64%)]  Loss: 3.756 (3.74)  Time: 0.684s, 1497.36/s  (0.694s, 1474.90/s)  LR: 5.927e-04  Data: 0.011 (0.013)
Train: 266 [ 850/1251 ( 68%)]  Loss: 4.096 (3.76)  Time: 0.672s, 1523.41/s  (0.694s, 1475.67/s)  LR: 5.927e-04  Data: 0.010 (0.013)
Train: 266 [ 900/1251 ( 72%)]  Loss: 3.871 (3.77)  Time: 0.672s, 1523.83/s  (0.694s, 1474.97/s)  LR: 5.927e-04  Data: 0.011 (0.013)
Train: 266 [ 950/1251 ( 76%)]  Loss: 3.612 (3.76)  Time: 0.671s, 1526.98/s  (0.694s, 1474.58/s)  LR: 5.927e-04  Data: 0.010 (0.013)
Train: 266 [1000/1251 ( 80%)]  Loss: 3.951 (3.77)  Time: 0.707s, 1447.88/s  (0.694s, 1474.74/s)  LR: 5.927e-04  Data: 0.010 (0.012)
Train: 266 [1050/1251 ( 84%)]  Loss: 3.591 (3.76)  Time: 0.675s, 1515.95/s  (0.694s, 1474.87/s)  LR: 5.927e-04  Data: 0.011 (0.012)
Train: 266 [1100/1251 ( 88%)]  Loss: 3.671 (3.76)  Time: 0.670s, 1529.32/s  (0.694s, 1475.12/s)  LR: 5.927e-04  Data: 0.011 (0.012)
Train: 266 [1150/1251 ( 92%)]  Loss: 3.581 (3.75)  Time: 0.679s, 1507.93/s  (0.694s, 1475.76/s)  LR: 5.927e-04  Data: 0.012 (0.012)
Train: 266 [1200/1251 ( 96%)]  Loss: 3.450 (3.74)  Time: 0.708s, 1447.26/s  (0.694s, 1476.35/s)  LR: 5.927e-04  Data: 0.011 (0.012)
Train: 266 [1250/1251 (100%)]  Loss: 4.100 (3.75)  Time: 0.656s, 1561.23/s  (0.693s, 1477.09/s)  LR: 5.927e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.561 (1.561)  Loss:  0.8115 (0.8115)  Acc@1: 89.5508 (89.5508)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.137 (0.569)  Loss:  0.9351 (1.4820)  Acc@1: 84.0802 (72.5160)  Acc@5: 96.3443 (91.1240)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-257.pth.tar', 73.07200016601563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-262.pth.tar', 72.76800011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-254.pth.tar', 72.68600006835938)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-256.pth.tar', 72.62999988525391)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-261.pth.tar', 72.5700001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-258.pth.tar', 72.55800012207031)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-241.pth.tar', 72.53999996826172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-260.pth.tar', 72.52400001708985)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-266.pth.tar', 72.51600006591796)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-250.pth.tar', 72.46200000732422)

Train: 267 [   0/1251 (  0%)]  Loss: 4.045 (4.05)  Time: 2.244s,  456.42/s  (2.244s,  456.42/s)  LR: 5.901e-04  Data: 1.627 (1.627)
Train: 267 [  50/1251 (  4%)]  Loss: 3.330 (3.69)  Time: 0.670s, 1529.23/s  (0.731s, 1400.35/s)  LR: 5.901e-04  Data: 0.009 (0.049)
Train: 267 [ 100/1251 (  8%)]  Loss: 3.623 (3.67)  Time: 0.709s, 1443.54/s  (0.713s, 1435.35/s)  LR: 5.901e-04  Data: 0.010 (0.030)
Train: 267 [ 150/1251 ( 12%)]  Loss: 3.237 (3.56)  Time: 0.793s, 1290.91/s  (0.707s, 1449.31/s)  LR: 5.901e-04  Data: 0.009 (0.023)
Train: 267 [ 200/1251 ( 16%)]  Loss: 3.899 (3.63)  Time: 0.677s, 1513.10/s  (0.705s, 1452.48/s)  LR: 5.901e-04  Data: 0.010 (0.020)
Train: 267 [ 250/1251 ( 20%)]  Loss: 3.894 (3.67)  Time: 0.671s, 1524.97/s  (0.702s, 1459.41/s)  LR: 5.901e-04  Data: 0.010 (0.018)
Train: 267 [ 300/1251 ( 24%)]  Loss: 3.698 (3.68)  Time: 0.708s, 1446.57/s  (0.701s, 1461.18/s)  LR: 5.901e-04  Data: 0.014 (0.017)
Train: 267 [ 350/1251 ( 28%)]  Loss: 3.472 (3.65)  Time: 0.676s, 1515.35/s  (0.701s, 1461.10/s)  LR: 5.901e-04  Data: 0.009 (0.016)
Train: 267 [ 400/1251 ( 32%)]  Loss: 3.660 (3.65)  Time: 0.705s, 1453.26/s  (0.700s, 1462.43/s)  LR: 5.901e-04  Data: 0.009 (0.015)
Train: 267 [ 450/1251 ( 36%)]  Loss: 3.509 (3.64)  Time: 0.712s, 1438.53/s  (0.699s, 1464.07/s)  LR: 5.901e-04  Data: 0.011 (0.015)
Train: 267 [ 500/1251 ( 40%)]  Loss: 3.611 (3.63)  Time: 0.695s, 1472.62/s  (0.698s, 1466.65/s)  LR: 5.901e-04  Data: 0.009 (0.014)
Train: 267 [ 550/1251 ( 44%)]  Loss: 3.739 (3.64)  Time: 0.675s, 1517.26/s  (0.698s, 1467.66/s)  LR: 5.901e-04  Data: 0.011 (0.014)
Train: 267 [ 600/1251 ( 48%)]  Loss: 3.805 (3.66)  Time: 0.675s, 1516.33/s  (0.697s, 1469.82/s)  LR: 5.901e-04  Data: 0.011 (0.014)
Train: 267 [ 650/1251 ( 52%)]  Loss: 3.556 (3.65)  Time: 0.679s, 1508.26/s  (0.696s, 1471.11/s)  LR: 5.901e-04  Data: 0.010 (0.013)
Train: 267 [ 700/1251 ( 56%)]  Loss: 3.742 (3.65)  Time: 0.707s, 1447.52/s  (0.697s, 1469.79/s)  LR: 5.901e-04  Data: 0.011 (0.013)
Train: 267 [ 750/1251 ( 60%)]  Loss: 3.650 (3.65)  Time: 0.707s, 1448.67/s  (0.696s, 1471.26/s)  LR: 5.901e-04  Data: 0.009 (0.013)
Train: 267 [ 800/1251 ( 64%)]  Loss: 3.802 (3.66)  Time: 0.671s, 1527.08/s  (0.696s, 1471.67/s)  LR: 5.901e-04  Data: 0.010 (0.013)
Train: 267 [ 850/1251 ( 68%)]  Loss: 3.645 (3.66)  Time: 0.682s, 1501.42/s  (0.696s, 1471.36/s)  LR: 5.901e-04  Data: 0.010 (0.013)
Train: 267 [ 900/1251 ( 72%)]  Loss: 3.770 (3.67)  Time: 0.672s, 1524.57/s  (0.696s, 1471.44/s)  LR: 5.901e-04  Data: 0.010 (0.013)
Train: 267 [ 950/1251 ( 76%)]  Loss: 3.736 (3.67)  Time: 0.671s, 1526.37/s  (0.696s, 1471.91/s)  LR: 5.901e-04  Data: 0.010 (0.013)
Train: 267 [1000/1251 ( 80%)]  Loss: 3.729 (3.67)  Time: 0.666s, 1536.95/s  (0.695s, 1472.41/s)  LR: 5.901e-04  Data: 0.010 (0.012)
Train: 267 [1050/1251 ( 84%)]  Loss: 3.755 (3.68)  Time: 0.706s, 1449.56/s  (0.695s, 1472.34/s)  LR: 5.901e-04  Data: 0.009 (0.012)
Train: 267 [1100/1251 ( 88%)]  Loss: 3.801 (3.68)  Time: 0.716s, 1431.07/s  (0.695s, 1473.05/s)  LR: 5.901e-04  Data: 0.010 (0.012)
Train: 267 [1150/1251 ( 92%)]  Loss: 3.621 (3.68)  Time: 0.697s, 1468.66/s  (0.695s, 1472.44/s)  LR: 5.901e-04  Data: 0.009 (0.012)
Train: 267 [1200/1251 ( 96%)]  Loss: 3.830 (3.69)  Time: 0.706s, 1451.45/s  (0.696s, 1472.25/s)  LR: 5.901e-04  Data: 0.011 (0.012)
Train: 267 [1250/1251 (100%)]  Loss: 3.496 (3.68)  Time: 0.671s, 1525.92/s  (0.696s, 1472.04/s)  LR: 5.901e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.481 (1.481)  Loss:  0.8174 (0.8174)  Acc@1: 88.9648 (88.9648)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.136 (0.582)  Loss:  1.0049 (1.4665)  Acc@1: 82.3113 (72.7660)  Acc@5: 94.4576 (91.2880)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-257.pth.tar', 73.07200016601563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-262.pth.tar', 72.76800011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-267.pth.tar', 72.76599994384766)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-254.pth.tar', 72.68600006835938)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-256.pth.tar', 72.62999988525391)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-261.pth.tar', 72.5700001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-258.pth.tar', 72.55800012207031)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-241.pth.tar', 72.53999996826172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-260.pth.tar', 72.52400001708985)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-266.pth.tar', 72.51600006591796)

Train: 268 [   0/1251 (  0%)]  Loss: 3.549 (3.55)  Time: 2.149s,  476.39/s  (2.149s,  476.39/s)  LR: 5.876e-04  Data: 1.535 (1.535)
Train: 268 [  50/1251 (  4%)]  Loss: 3.641 (3.60)  Time: 0.677s, 1512.60/s  (0.739s, 1385.02/s)  LR: 5.876e-04  Data: 0.011 (0.049)
Train: 268 [ 100/1251 (  8%)]  Loss: 3.573 (3.59)  Time: 0.666s, 1538.27/s  (0.717s, 1427.57/s)  LR: 5.876e-04  Data: 0.010 (0.030)
Train: 268 [ 150/1251 ( 12%)]  Loss: 3.688 (3.61)  Time: 0.688s, 1489.38/s  (0.710s, 1443.10/s)  LR: 5.876e-04  Data: 0.011 (0.024)
Train: 268 [ 200/1251 ( 16%)]  Loss: 3.700 (3.63)  Time: 0.686s, 1492.41/s  (0.704s, 1454.00/s)  LR: 5.876e-04  Data: 0.010 (0.020)
Train: 268 [ 250/1251 ( 20%)]  Loss: 3.438 (3.60)  Time: 0.706s, 1450.73/s  (0.703s, 1456.60/s)  LR: 5.876e-04  Data: 0.010 (0.018)
Train: 268 [ 300/1251 ( 24%)]  Loss: 3.703 (3.61)  Time: 0.724s, 1414.40/s  (0.702s, 1458.86/s)  LR: 5.876e-04  Data: 0.016 (0.017)
Train: 268 [ 350/1251 ( 28%)]  Loss: 3.919 (3.65)  Time: 0.710s, 1442.61/s  (0.701s, 1460.57/s)  LR: 5.876e-04  Data: 0.010 (0.016)
Train: 268 [ 400/1251 ( 32%)]  Loss: 3.809 (3.67)  Time: 0.677s, 1513.33/s  (0.699s, 1463.91/s)  LR: 5.876e-04  Data: 0.011 (0.015)
Train: 268 [ 450/1251 ( 36%)]  Loss: 3.520 (3.65)  Time: 0.668s, 1533.92/s  (0.699s, 1465.80/s)  LR: 5.876e-04  Data: 0.010 (0.015)
Train: 268 [ 500/1251 ( 40%)]  Loss: 4.019 (3.69)  Time: 0.719s, 1424.96/s  (0.697s, 1468.59/s)  LR: 5.876e-04  Data: 0.010 (0.014)
Train: 268 [ 550/1251 ( 44%)]  Loss: 3.762 (3.69)  Time: 0.671s, 1526.21/s  (0.696s, 1470.32/s)  LR: 5.876e-04  Data: 0.010 (0.014)
Train: 268 [ 600/1251 ( 48%)]  Loss: 3.834 (3.70)  Time: 0.672s, 1523.71/s  (0.696s, 1471.72/s)  LR: 5.876e-04  Data: 0.009 (0.014)
Train: 268 [ 650/1251 ( 52%)]  Loss: 3.454 (3.69)  Time: 0.701s, 1460.12/s  (0.696s, 1471.81/s)  LR: 5.876e-04  Data: 0.017 (0.014)
Train: 268 [ 700/1251 ( 56%)]  Loss: 3.931 (3.70)  Time: 0.672s, 1524.38/s  (0.696s, 1471.87/s)  LR: 5.876e-04  Data: 0.010 (0.013)
Train: 268 [ 750/1251 ( 60%)]  Loss: 4.145 (3.73)  Time: 0.719s, 1423.82/s  (0.696s, 1471.39/s)  LR: 5.876e-04  Data: 0.016 (0.013)
Train: 268 [ 800/1251 ( 64%)]  Loss: 3.926 (3.74)  Time: 0.672s, 1522.74/s  (0.695s, 1473.01/s)  LR: 5.876e-04  Data: 0.011 (0.013)
Train: 268 [ 850/1251 ( 68%)]  Loss: 3.784 (3.74)  Time: 0.692s, 1480.78/s  (0.695s, 1472.91/s)  LR: 5.876e-04  Data: 0.013 (0.013)
Train: 268 [ 900/1251 ( 72%)]  Loss: 3.513 (3.73)  Time: 0.673s, 1521.38/s  (0.695s, 1472.87/s)  LR: 5.876e-04  Data: 0.011 (0.013)
Train: 268 [ 950/1251 ( 76%)]  Loss: 3.963 (3.74)  Time: 0.681s, 1502.98/s  (0.695s, 1473.70/s)  LR: 5.876e-04  Data: 0.012 (0.013)
Train: 268 [1000/1251 ( 80%)]  Loss: 3.472 (3.73)  Time: 0.677s, 1512.77/s  (0.695s, 1474.07/s)  LR: 5.876e-04  Data: 0.010 (0.012)
Train: 268 [1050/1251 ( 84%)]  Loss: 3.749 (3.73)  Time: 0.702s, 1459.29/s  (0.695s, 1474.28/s)  LR: 5.876e-04  Data: 0.011 (0.012)
Train: 268 [1100/1251 ( 88%)]  Loss: 3.869 (3.74)  Time: 0.696s, 1472.13/s  (0.694s, 1474.49/s)  LR: 5.876e-04  Data: 0.013 (0.012)
Train: 268 [1150/1251 ( 92%)]  Loss: 3.277 (3.72)  Time: 0.732s, 1398.75/s  (0.694s, 1475.00/s)  LR: 5.876e-04  Data: 0.011 (0.012)
Train: 268 [1200/1251 ( 96%)]  Loss: 4.044 (3.73)  Time: 0.708s, 1446.74/s  (0.694s, 1475.36/s)  LR: 5.876e-04  Data: 0.009 (0.012)
Train: 268 [1250/1251 (100%)]  Loss: 3.998 (3.74)  Time: 0.663s, 1545.46/s  (0.694s, 1475.32/s)  LR: 5.876e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.450 (1.450)  Loss:  0.8994 (0.8994)  Acc@1: 89.1602 (89.1602)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.136 (0.568)  Loss:  0.9688 (1.4499)  Acc@1: 85.1415 (72.8700)  Acc@5: 96.3443 (91.2360)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-257.pth.tar', 73.07200016601563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-268.pth.tar', 72.87000000976562)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-262.pth.tar', 72.76800011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-267.pth.tar', 72.76599994384766)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-254.pth.tar', 72.68600006835938)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-256.pth.tar', 72.62999988525391)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-261.pth.tar', 72.5700001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-258.pth.tar', 72.55800012207031)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-241.pth.tar', 72.53999996826172)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-260.pth.tar', 72.52400001708985)

Train: 269 [   0/1251 (  0%)]  Loss: 3.366 (3.37)  Time: 2.259s,  453.24/s  (2.259s,  453.24/s)  LR: 5.850e-04  Data: 1.643 (1.643)
Train: 269 [  50/1251 (  4%)]  Loss: 3.866 (3.62)  Time: 0.693s, 1478.03/s  (0.732s, 1399.50/s)  LR: 5.850e-04  Data: 0.010 (0.048)
Train: 269 [ 100/1251 (  8%)]  Loss: 3.927 (3.72)  Time: 0.671s, 1527.07/s  (0.712s, 1437.88/s)  LR: 5.850e-04  Data: 0.009 (0.029)
Train: 269 [ 150/1251 ( 12%)]  Loss: 3.379 (3.63)  Time: 0.690s, 1484.10/s  (0.709s, 1445.22/s)  LR: 5.850e-04  Data: 0.009 (0.023)
Train: 269 [ 200/1251 ( 16%)]  Loss: 3.735 (3.65)  Time: 0.670s, 1527.39/s  (0.705s, 1453.32/s)  LR: 5.850e-04  Data: 0.009 (0.020)
Train: 269 [ 250/1251 ( 20%)]  Loss: 3.378 (3.61)  Time: 0.716s, 1429.66/s  (0.702s, 1459.05/s)  LR: 5.850e-04  Data: 0.011 (0.018)
Train: 269 [ 300/1251 ( 24%)]  Loss: 3.561 (3.60)  Time: 0.676s, 1515.71/s  (0.701s, 1460.90/s)  LR: 5.850e-04  Data: 0.009 (0.017)
Train: 269 [ 350/1251 ( 28%)]  Loss: 3.761 (3.62)  Time: 0.703s, 1457.05/s  (0.699s, 1464.79/s)  LR: 5.850e-04  Data: 0.009 (0.016)
Train: 269 [ 400/1251 ( 32%)]  Loss: 3.697 (3.63)  Time: 0.702s, 1459.71/s  (0.698s, 1468.04/s)  LR: 5.850e-04  Data: 0.009 (0.015)
Train: 269 [ 450/1251 ( 36%)]  Loss: 3.966 (3.66)  Time: 0.723s, 1416.20/s  (0.697s, 1468.84/s)  LR: 5.850e-04  Data: 0.009 (0.015)
Train: 269 [ 500/1251 ( 40%)]  Loss: 3.618 (3.66)  Time: 0.728s, 1406.45/s  (0.696s, 1470.67/s)  LR: 5.850e-04  Data: 0.009 (0.014)
Train: 269 [ 550/1251 ( 44%)]  Loss: 3.637 (3.66)  Time: 0.701s, 1461.12/s  (0.696s, 1471.70/s)  LR: 5.850e-04  Data: 0.009 (0.014)
Train: 269 [ 600/1251 ( 48%)]  Loss: 3.592 (3.65)  Time: 0.704s, 1455.55/s  (0.696s, 1470.68/s)  LR: 5.850e-04  Data: 0.009 (0.014)
Train: 269 [ 650/1251 ( 52%)]  Loss: 3.810 (3.66)  Time: 0.673s, 1522.61/s  (0.696s, 1472.13/s)  LR: 5.850e-04  Data: 0.013 (0.013)
Train: 269 [ 700/1251 ( 56%)]  Loss: 4.027 (3.69)  Time: 0.702s, 1458.83/s  (0.696s, 1472.09/s)  LR: 5.850e-04  Data: 0.013 (0.013)
Train: 269 [ 750/1251 ( 60%)]  Loss: 4.069 (3.71)  Time: 0.673s, 1520.54/s  (0.695s, 1473.26/s)  LR: 5.850e-04  Data: 0.011 (0.013)
Train: 269 [ 800/1251 ( 64%)]  Loss: 3.682 (3.71)  Time: 0.670s, 1528.30/s  (0.695s, 1472.88/s)  LR: 5.850e-04  Data: 0.010 (0.013)
Train: 269 [ 850/1251 ( 68%)]  Loss: 3.749 (3.71)  Time: 0.711s, 1440.05/s  (0.695s, 1472.96/s)  LR: 5.850e-04  Data: 0.011 (0.013)
Train: 269 [ 900/1251 ( 72%)]  Loss: 3.899 (3.72)  Time: 0.671s, 1526.07/s  (0.695s, 1472.93/s)  LR: 5.850e-04  Data: 0.010 (0.013)
Train: 269 [ 950/1251 ( 76%)]  Loss: 3.936 (3.73)  Time: 0.670s, 1528.63/s  (0.695s, 1472.83/s)  LR: 5.850e-04  Data: 0.010 (0.012)
Train: 269 [1000/1251 ( 80%)]  Loss: 3.619 (3.73)  Time: 0.726s, 1409.55/s  (0.695s, 1473.43/s)  LR: 5.850e-04  Data: 0.010 (0.012)
Train: 269 [1050/1251 ( 84%)]  Loss: 3.443 (3.71)  Time: 0.707s, 1449.19/s  (0.695s, 1472.97/s)  LR: 5.850e-04  Data: 0.009 (0.012)
Train: 269 [1100/1251 ( 88%)]  Loss: 3.508 (3.71)  Time: 0.723s, 1416.83/s  (0.695s, 1473.27/s)  LR: 5.850e-04  Data: 0.010 (0.012)
Train: 269 [1150/1251 ( 92%)]  Loss: 3.803 (3.71)  Time: 0.667s, 1535.39/s  (0.695s, 1474.07/s)  LR: 5.850e-04  Data: 0.011 (0.012)
Train: 269 [1200/1251 ( 96%)]  Loss: 3.815 (3.71)  Time: 0.674s, 1519.85/s  (0.694s, 1474.46/s)  LR: 5.850e-04  Data: 0.011 (0.012)
Train: 269 [1250/1251 (100%)]  Loss: 3.346 (3.70)  Time: 0.656s, 1561.89/s  (0.695s, 1473.95/s)  LR: 5.850e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.480 (1.480)  Loss:  0.7573 (0.7573)  Acc@1: 89.6484 (89.6484)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.137 (0.582)  Loss:  0.9521 (1.4685)  Acc@1: 85.8491 (73.0160)  Acc@5: 95.9906 (91.2680)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-257.pth.tar', 73.07200016601563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-269.pth.tar', 73.01599992919922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-268.pth.tar', 72.87000000976562)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-262.pth.tar', 72.76800011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-267.pth.tar', 72.76599994384766)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-254.pth.tar', 72.68600006835938)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-256.pth.tar', 72.62999988525391)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-261.pth.tar', 72.5700001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-258.pth.tar', 72.55800012207031)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-241.pth.tar', 72.53999996826172)

Train: 270 [   0/1251 (  0%)]  Loss: 3.769 (3.77)  Time: 2.158s,  474.45/s  (2.158s,  474.45/s)  LR: 5.824e-04  Data: 1.542 (1.542)
Train: 270 [  50/1251 (  4%)]  Loss: 3.642 (3.71)  Time: 0.706s, 1451.00/s  (0.725s, 1411.67/s)  LR: 5.824e-04  Data: 0.009 (0.045)
Train: 270 [ 100/1251 (  8%)]  Loss: 3.554 (3.66)  Time: 0.686s, 1491.91/s  (0.706s, 1450.11/s)  LR: 5.824e-04  Data: 0.009 (0.027)
Train: 270 [ 150/1251 ( 12%)]  Loss: 3.225 (3.55)  Time: 0.705s, 1452.39/s  (0.703s, 1455.98/s)  LR: 5.824e-04  Data: 0.010 (0.022)
Train: 270 [ 200/1251 ( 16%)]  Loss: 3.618 (3.56)  Time: 0.675s, 1517.53/s  (0.699s, 1464.01/s)  LR: 5.824e-04  Data: 0.009 (0.019)
Train: 270 [ 250/1251 ( 20%)]  Loss: 3.749 (3.59)  Time: 0.675s, 1516.35/s  (0.699s, 1465.16/s)  LR: 5.824e-04  Data: 0.010 (0.017)
Train: 270 [ 300/1251 ( 24%)]  Loss: 3.707 (3.61)  Time: 0.672s, 1523.82/s  (0.698s, 1466.96/s)  LR: 5.824e-04  Data: 0.010 (0.016)
Train: 270 [ 350/1251 ( 28%)]  Loss: 3.736 (3.63)  Time: 0.717s, 1429.13/s  (0.698s, 1466.51/s)  LR: 5.824e-04  Data: 0.009 (0.015)
Train: 270 [ 400/1251 ( 32%)]  Loss: 3.859 (3.65)  Time: 0.672s, 1523.82/s  (0.697s, 1468.95/s)  LR: 5.824e-04  Data: 0.010 (0.015)
Train: 270 [ 450/1251 ( 36%)]  Loss: 3.178 (3.60)  Time: 0.674s, 1520.34/s  (0.696s, 1471.83/s)  LR: 5.824e-04  Data: 0.011 (0.014)
Train: 270 [ 500/1251 ( 40%)]  Loss: 3.910 (3.63)  Time: 0.686s, 1492.85/s  (0.695s, 1473.19/s)  LR: 5.824e-04  Data: 0.016 (0.014)
Train: 270 [ 550/1251 ( 44%)]  Loss: 3.829 (3.65)  Time: 0.678s, 1509.47/s  (0.695s, 1473.92/s)  LR: 5.824e-04  Data: 0.011 (0.014)
Train: 270 [ 600/1251 ( 48%)]  Loss: 3.915 (3.67)  Time: 0.673s, 1522.28/s  (0.695s, 1474.25/s)  LR: 5.824e-04  Data: 0.011 (0.013)
Train: 270 [ 650/1251 ( 52%)]  Loss: 3.589 (3.66)  Time: 0.720s, 1422.15/s  (0.694s, 1474.86/s)  LR: 5.824e-04  Data: 0.017 (0.013)
Train: 270 [ 700/1251 ( 56%)]  Loss: 3.433 (3.65)  Time: 0.706s, 1451.42/s  (0.694s, 1475.40/s)  LR: 5.824e-04  Data: 0.010 (0.013)
Train: 270 [ 750/1251 ( 60%)]  Loss: 3.904 (3.66)  Time: 0.673s, 1521.95/s  (0.694s, 1476.40/s)  LR: 5.824e-04  Data: 0.011 (0.013)
Train: 270 [ 800/1251 ( 64%)]  Loss: 3.627 (3.66)  Time: 0.675s, 1517.72/s  (0.694s, 1476.29/s)  LR: 5.824e-04  Data: 0.011 (0.013)
Train: 270 [ 850/1251 ( 68%)]  Loss: 3.884 (3.67)  Time: 0.764s, 1340.47/s  (0.694s, 1476.50/s)  LR: 5.824e-04  Data: 0.010 (0.012)
Train: 270 [ 900/1251 ( 72%)]  Loss: 3.578 (3.67)  Time: 0.742s, 1380.05/s  (0.694s, 1476.38/s)  LR: 5.824e-04  Data: 0.010 (0.012)
Train: 270 [ 950/1251 ( 76%)]  Loss: 4.032 (3.69)  Time: 0.672s, 1524.20/s  (0.694s, 1476.05/s)  LR: 5.824e-04  Data: 0.011 (0.012)
Train: 270 [1000/1251 ( 80%)]  Loss: 3.562 (3.68)  Time: 0.727s, 1408.84/s  (0.693s, 1476.71/s)  LR: 5.824e-04  Data: 0.010 (0.012)
Train: 270 [1050/1251 ( 84%)]  Loss: 3.520 (3.67)  Time: 0.671s, 1525.53/s  (0.693s, 1477.81/s)  LR: 5.824e-04  Data: 0.010 (0.012)
Train: 270 [1100/1251 ( 88%)]  Loss: 3.838 (3.68)  Time: 0.676s, 1514.17/s  (0.693s, 1478.35/s)  LR: 5.824e-04  Data: 0.011 (0.012)
Train: 270 [1150/1251 ( 92%)]  Loss: 3.950 (3.69)  Time: 0.676s, 1515.61/s  (0.693s, 1478.38/s)  LR: 5.824e-04  Data: 0.011 (0.012)
Train: 270 [1200/1251 ( 96%)]  Loss: 3.659 (3.69)  Time: 0.703s, 1456.69/s  (0.692s, 1479.02/s)  LR: 5.824e-04  Data: 0.010 (0.012)
Train: 270 [1250/1251 (100%)]  Loss: 3.743 (3.69)  Time: 0.682s, 1501.16/s  (0.692s, 1479.42/s)  LR: 5.824e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.440 (1.440)  Loss:  0.8857 (0.8857)  Acc@1: 88.4766 (88.4766)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.136 (0.575)  Loss:  0.9341 (1.4499)  Acc@1: 84.3160 (72.8340)  Acc@5: 96.3443 (91.4300)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-257.pth.tar', 73.07200016601563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-269.pth.tar', 73.01599992919922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-268.pth.tar', 72.87000000976562)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-270.pth.tar', 72.8340000390625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-262.pth.tar', 72.76800011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-267.pth.tar', 72.76599994384766)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-254.pth.tar', 72.68600006835938)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-256.pth.tar', 72.62999988525391)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-261.pth.tar', 72.5700001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-258.pth.tar', 72.55800012207031)

Train: 271 [   0/1251 (  0%)]  Loss: 3.579 (3.58)  Time: 2.132s,  480.34/s  (2.132s,  480.34/s)  LR: 5.799e-04  Data: 1.484 (1.484)
Train: 271 [  50/1251 (  4%)]  Loss: 3.656 (3.62)  Time: 0.681s, 1504.42/s  (0.724s, 1414.09/s)  LR: 5.799e-04  Data: 0.010 (0.049)
Train: 271 [ 100/1251 (  8%)]  Loss: 3.949 (3.73)  Time: 0.706s, 1451.32/s  (0.709s, 1444.71/s)  LR: 5.799e-04  Data: 0.010 (0.030)
Train: 271 [ 150/1251 ( 12%)]  Loss: 3.312 (3.62)  Time: 0.735s, 1392.63/s  (0.702s, 1459.69/s)  LR: 5.799e-04  Data: 0.011 (0.023)
Train: 271 [ 200/1251 ( 16%)]  Loss: 3.967 (3.69)  Time: 0.671s, 1526.82/s  (0.699s, 1465.19/s)  LR: 5.799e-04  Data: 0.010 (0.020)
Train: 271 [ 250/1251 ( 20%)]  Loss: 4.058 (3.75)  Time: 0.670s, 1528.63/s  (0.697s, 1468.54/s)  LR: 5.799e-04  Data: 0.010 (0.018)
Train: 271 [ 300/1251 ( 24%)]  Loss: 3.487 (3.72)  Time: 0.671s, 1525.47/s  (0.696s, 1470.63/s)  LR: 5.799e-04  Data: 0.010 (0.017)
Train: 271 [ 350/1251 ( 28%)]  Loss: 3.773 (3.72)  Time: 0.682s, 1502.29/s  (0.697s, 1468.92/s)  LR: 5.799e-04  Data: 0.010 (0.016)
Train: 271 [ 400/1251 ( 32%)]  Loss: 3.669 (3.72)  Time: 0.673s, 1521.72/s  (0.696s, 1470.92/s)  LR: 5.799e-04  Data: 0.009 (0.015)
Train: 271 [ 450/1251 ( 36%)]  Loss: 3.926 (3.74)  Time: 0.674s, 1518.59/s  (0.696s, 1471.44/s)  LR: 5.799e-04  Data: 0.010 (0.015)
Train: 271 [ 500/1251 ( 40%)]  Loss: 3.884 (3.75)  Time: 0.733s, 1396.54/s  (0.695s, 1472.52/s)  LR: 5.799e-04  Data: 0.009 (0.014)
Train: 271 [ 550/1251 ( 44%)]  Loss: 3.727 (3.75)  Time: 0.672s, 1524.81/s  (0.695s, 1473.16/s)  LR: 5.799e-04  Data: 0.010 (0.014)
Train: 271 [ 600/1251 ( 48%)]  Loss: 3.443 (3.73)  Time: 0.706s, 1450.96/s  (0.695s, 1472.83/s)  LR: 5.799e-04  Data: 0.010 (0.014)
Train: 271 [ 650/1251 ( 52%)]  Loss: 3.457 (3.71)  Time: 0.666s, 1537.22/s  (0.695s, 1472.91/s)  LR: 5.799e-04  Data: 0.011 (0.013)
Train: 271 [ 700/1251 ( 56%)]  Loss: 3.651 (3.70)  Time: 0.676s, 1515.25/s  (0.695s, 1474.34/s)  LR: 5.799e-04  Data: 0.011 (0.013)
Train: 271 [ 750/1251 ( 60%)]  Loss: 3.768 (3.71)  Time: 0.710s, 1441.76/s  (0.695s, 1473.38/s)  LR: 5.799e-04  Data: 0.010 (0.013)
Train: 271 [ 800/1251 ( 64%)]  Loss: 3.492 (3.69)  Time: 0.680s, 1505.43/s  (0.695s, 1472.75/s)  LR: 5.799e-04  Data: 0.009 (0.013)
Train: 271 [ 850/1251 ( 68%)]  Loss: 3.593 (3.69)  Time: 0.671s, 1524.96/s  (0.695s, 1473.46/s)  LR: 5.799e-04  Data: 0.011 (0.013)
Train: 271 [ 900/1251 ( 72%)]  Loss: 3.410 (3.67)  Time: 0.673s, 1520.82/s  (0.695s, 1473.66/s)  LR: 5.799e-04  Data: 0.009 (0.013)
Train: 271 [ 950/1251 ( 76%)]  Loss: 3.768 (3.68)  Time: 0.706s, 1450.41/s  (0.695s, 1474.35/s)  LR: 5.799e-04  Data: 0.010 (0.013)
Train: 271 [1000/1251 ( 80%)]  Loss: 3.822 (3.69)  Time: 0.733s, 1396.40/s  (0.694s, 1474.77/s)  LR: 5.799e-04  Data: 0.010 (0.012)
Train: 271 [1050/1251 ( 84%)]  Loss: 3.926 (3.70)  Time: 0.703s, 1457.22/s  (0.694s, 1475.63/s)  LR: 5.799e-04  Data: 0.010 (0.012)
Train: 271 [1100/1251 ( 88%)]  Loss: 3.853 (3.70)  Time: 0.675s, 1518.12/s  (0.694s, 1475.55/s)  LR: 5.799e-04  Data: 0.009 (0.012)
Train: 271 [1150/1251 ( 92%)]  Loss: 3.700 (3.70)  Time: 0.684s, 1497.27/s  (0.694s, 1476.00/s)  LR: 5.799e-04  Data: 0.010 (0.012)
Train: 271 [1200/1251 ( 96%)]  Loss: 3.365 (3.69)  Time: 0.684s, 1498.15/s  (0.694s, 1475.73/s)  LR: 5.799e-04  Data: 0.014 (0.012)
Train: 271 [1250/1251 (100%)]  Loss: 3.910 (3.70)  Time: 0.684s, 1496.12/s  (0.694s, 1476.31/s)  LR: 5.799e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.463 (1.463)  Loss:  0.7754 (0.7754)  Acc@1: 88.5742 (88.5742)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.136 (0.580)  Loss:  0.9360 (1.4329)  Acc@1: 83.8443 (72.7200)  Acc@5: 96.1085 (91.0980)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-257.pth.tar', 73.07200016601563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-269.pth.tar', 73.01599992919922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-268.pth.tar', 72.87000000976562)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-270.pth.tar', 72.8340000390625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-262.pth.tar', 72.76800011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-267.pth.tar', 72.76599994384766)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-271.pth.tar', 72.72000009277343)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-254.pth.tar', 72.68600006835938)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-256.pth.tar', 72.62999988525391)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-261.pth.tar', 72.5700001171875)

Train: 272 [   0/1251 (  0%)]  Loss: 3.493 (3.49)  Time: 2.238s,  457.55/s  (2.238s,  457.55/s)  LR: 5.773e-04  Data: 1.621 (1.621)
Train: 272 [  50/1251 (  4%)]  Loss: 4.079 (3.79)  Time: 0.673s, 1520.73/s  (0.722s, 1417.89/s)  LR: 5.773e-04  Data: 0.011 (0.048)
Train: 272 [ 100/1251 (  8%)]  Loss: 3.871 (3.81)  Time: 0.670s, 1527.34/s  (0.714s, 1434.94/s)  LR: 5.773e-04  Data: 0.011 (0.030)
Train: 272 [ 150/1251 ( 12%)]  Loss: 3.913 (3.84)  Time: 0.709s, 1443.80/s  (0.710s, 1442.56/s)  LR: 5.773e-04  Data: 0.013 (0.024)
Train: 272 [ 200/1251 ( 16%)]  Loss: 3.819 (3.84)  Time: 0.725s, 1412.52/s  (0.705s, 1452.96/s)  LR: 5.773e-04  Data: 0.010 (0.020)
Train: 272 [ 250/1251 ( 20%)]  Loss: 3.555 (3.79)  Time: 0.701s, 1460.23/s  (0.703s, 1456.89/s)  LR: 5.773e-04  Data: 0.010 (0.018)
Train: 272 [ 300/1251 ( 24%)]  Loss: 3.643 (3.77)  Time: 0.666s, 1537.39/s  (0.700s, 1463.08/s)  LR: 5.773e-04  Data: 0.010 (0.017)
Train: 272 [ 350/1251 ( 28%)]  Loss: 3.740 (3.76)  Time: 0.674s, 1520.33/s  (0.699s, 1465.02/s)  LR: 5.773e-04  Data: 0.011 (0.016)
Train: 272 [ 400/1251 ( 32%)]  Loss: 3.976 (3.79)  Time: 0.672s, 1524.10/s  (0.698s, 1467.90/s)  LR: 5.773e-04  Data: 0.010 (0.015)
Train: 272 [ 450/1251 ( 36%)]  Loss: 3.538 (3.76)  Time: 0.673s, 1520.94/s  (0.697s, 1469.76/s)  LR: 5.773e-04  Data: 0.011 (0.015)
Train: 272 [ 500/1251 ( 40%)]  Loss: 4.030 (3.79)  Time: 0.670s, 1527.39/s  (0.696s, 1471.04/s)  LR: 5.773e-04  Data: 0.011 (0.014)
Train: 272 [ 550/1251 ( 44%)]  Loss: 3.734 (3.78)  Time: 0.696s, 1472.13/s  (0.696s, 1471.94/s)  LR: 5.773e-04  Data: 0.014 (0.014)
Train: 272 [ 600/1251 ( 48%)]  Loss: 3.665 (3.77)  Time: 0.671s, 1525.56/s  (0.695s, 1473.22/s)  LR: 5.773e-04  Data: 0.011 (0.014)
Train: 272 [ 650/1251 ( 52%)]  Loss: 3.163 (3.73)  Time: 0.677s, 1512.28/s  (0.695s, 1473.44/s)  LR: 5.773e-04  Data: 0.010 (0.013)
Train: 272 [ 700/1251 ( 56%)]  Loss: 3.465 (3.71)  Time: 0.670s, 1529.21/s  (0.695s, 1474.10/s)  LR: 5.773e-04  Data: 0.010 (0.013)
Train: 272 [ 750/1251 ( 60%)]  Loss: 3.736 (3.71)  Time: 0.733s, 1396.33/s  (0.695s, 1473.04/s)  LR: 5.773e-04  Data: 0.013 (0.013)
Train: 272 [ 800/1251 ( 64%)]  Loss: 3.729 (3.71)  Time: 0.674s, 1519.87/s  (0.695s, 1473.89/s)  LR: 5.773e-04  Data: 0.011 (0.013)
Train: 272 [ 850/1251 ( 68%)]  Loss: 3.946 (3.73)  Time: 0.708s, 1445.92/s  (0.694s, 1475.07/s)  LR: 5.773e-04  Data: 0.011 (0.013)
Train: 272 [ 900/1251 ( 72%)]  Loss: 3.831 (3.73)  Time: 0.676s, 1514.61/s  (0.694s, 1474.46/s)  LR: 5.773e-04  Data: 0.013 (0.013)
Train: 272 [ 950/1251 ( 76%)]  Loss: 3.193 (3.71)  Time: 0.673s, 1520.59/s  (0.694s, 1474.62/s)  LR: 5.773e-04  Data: 0.011 (0.012)
Train: 272 [1000/1251 ( 80%)]  Loss: 3.727 (3.71)  Time: 0.688s, 1488.18/s  (0.694s, 1474.95/s)  LR: 5.773e-04  Data: 0.010 (0.012)
Train: 272 [1050/1251 ( 84%)]  Loss: 3.754 (3.71)  Time: 0.686s, 1492.47/s  (0.694s, 1475.76/s)  LR: 5.773e-04  Data: 0.008 (0.012)
Train: 272 [1100/1251 ( 88%)]  Loss: 4.219 (3.73)  Time: 0.675s, 1516.45/s  (0.694s, 1475.51/s)  LR: 5.773e-04  Data: 0.011 (0.012)
Train: 272 [1150/1251 ( 92%)]  Loss: 3.790 (3.73)  Time: 0.665s, 1539.18/s  (0.694s, 1475.96/s)  LR: 5.773e-04  Data: 0.010 (0.012)
Train: 272 [1200/1251 ( 96%)]  Loss: 3.702 (3.73)  Time: 0.705s, 1452.42/s  (0.694s, 1476.46/s)  LR: 5.773e-04  Data: 0.010 (0.012)
Train: 272 [1250/1251 (100%)]  Loss: 3.872 (3.74)  Time: 0.692s, 1480.52/s  (0.693s, 1476.95/s)  LR: 5.773e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.518 (1.518)  Loss:  0.6851 (0.6851)  Acc@1: 88.8672 (88.8672)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.136 (0.581)  Loss:  0.9229 (1.3860)  Acc@1: 83.3726 (72.4640)  Acc@5: 95.7547 (91.1320)
Train: 273 [   0/1251 (  0%)]  Loss: 3.784 (3.78)  Time: 2.174s,  470.97/s  (2.174s,  470.97/s)  LR: 5.747e-04  Data: 1.560 (1.560)
Train: 273 [  50/1251 (  4%)]  Loss: 3.198 (3.49)  Time: 0.674s, 1518.27/s  (0.726s, 1410.74/s)  LR: 5.747e-04  Data: 0.010 (0.046)
Train: 273 [ 100/1251 (  8%)]  Loss: 3.933 (3.64)  Time: 0.700s, 1462.63/s  (0.709s, 1444.93/s)  LR: 5.747e-04  Data: 0.009 (0.028)
Train: 273 [ 150/1251 ( 12%)]  Loss: 3.854 (3.69)  Time: 0.712s, 1438.32/s  (0.702s, 1459.33/s)  LR: 5.747e-04  Data: 0.009 (0.022)
Train: 273 [ 200/1251 ( 16%)]  Loss: 3.553 (3.66)  Time: 0.706s, 1450.25/s  (0.700s, 1463.49/s)  LR: 5.747e-04  Data: 0.009 (0.019)
Train: 273 [ 250/1251 ( 20%)]  Loss: 3.573 (3.65)  Time: 0.670s, 1529.16/s  (0.699s, 1465.61/s)  LR: 5.747e-04  Data: 0.009 (0.018)
Train: 273 [ 300/1251 ( 24%)]  Loss: 3.918 (3.69)  Time: 0.674s, 1520.30/s  (0.698s, 1467.54/s)  LR: 5.747e-04  Data: 0.009 (0.016)
Train: 273 [ 350/1251 ( 28%)]  Loss: 3.946 (3.72)  Time: 0.672s, 1524.35/s  (0.695s, 1472.63/s)  LR: 5.747e-04  Data: 0.010 (0.016)
Train: 273 [ 400/1251 ( 32%)]  Loss: 3.597 (3.71)  Time: 0.697s, 1468.52/s  (0.695s, 1473.98/s)  LR: 5.747e-04  Data: 0.009 (0.015)
Train: 273 [ 450/1251 ( 36%)]  Loss: 3.572 (3.69)  Time: 0.671s, 1526.74/s  (0.694s, 1474.47/s)  LR: 5.747e-04  Data: 0.009 (0.015)
Train: 273 [ 500/1251 ( 40%)]  Loss: 3.528 (3.68)  Time: 0.700s, 1463.36/s  (0.694s, 1475.04/s)  LR: 5.747e-04  Data: 0.009 (0.014)
Train: 273 [ 550/1251 ( 44%)]  Loss: 3.834 (3.69)  Time: 0.703s, 1455.61/s  (0.694s, 1475.25/s)  LR: 5.747e-04  Data: 0.010 (0.014)
Train: 273 [ 600/1251 ( 48%)]  Loss: 3.854 (3.70)  Time: 0.760s, 1347.21/s  (0.694s, 1475.95/s)  LR: 5.747e-04  Data: 0.013 (0.013)
Train: 273 [ 650/1251 ( 52%)]  Loss: 4.035 (3.73)  Time: 0.673s, 1521.93/s  (0.693s, 1476.92/s)  LR: 5.747e-04  Data: 0.010 (0.013)
Train: 273 [ 700/1251 ( 56%)]  Loss: 3.662 (3.72)  Time: 0.669s, 1530.65/s  (0.693s, 1477.98/s)  LR: 5.747e-04  Data: 0.009 (0.013)
Train: 273 [ 750/1251 ( 60%)]  Loss: 3.837 (3.73)  Time: 0.736s, 1390.46/s  (0.693s, 1477.93/s)  LR: 5.747e-04  Data: 0.010 (0.013)
Train: 273 [ 800/1251 ( 64%)]  Loss: 3.841 (3.74)  Time: 0.670s, 1528.01/s  (0.693s, 1478.59/s)  LR: 5.747e-04  Data: 0.010 (0.013)
Train: 273 [ 850/1251 ( 68%)]  Loss: 3.803 (3.74)  Time: 0.670s, 1528.50/s  (0.692s, 1479.05/s)  LR: 5.747e-04  Data: 0.010 (0.013)
Train: 273 [ 900/1251 ( 72%)]  Loss: 3.293 (3.72)  Time: 0.779s, 1315.02/s  (0.693s, 1478.64/s)  LR: 5.747e-04  Data: 0.011 (0.012)
Train: 273 [ 950/1251 ( 76%)]  Loss: 3.992 (3.73)  Time: 0.693s, 1477.80/s  (0.692s, 1479.05/s)  LR: 5.747e-04  Data: 0.013 (0.012)
Train: 273 [1000/1251 ( 80%)]  Loss: 3.808 (3.73)  Time: 0.705s, 1452.85/s  (0.693s, 1478.48/s)  LR: 5.747e-04  Data: 0.009 (0.012)
Train: 273 [1050/1251 ( 84%)]  Loss: 3.776 (3.74)  Time: 0.679s, 1508.53/s  (0.692s, 1478.76/s)  LR: 5.747e-04  Data: 0.008 (0.012)
Train: 273 [1100/1251 ( 88%)]  Loss: 3.961 (3.75)  Time: 0.704s, 1454.07/s  (0.693s, 1478.62/s)  LR: 5.747e-04  Data: 0.010 (0.012)
Train: 273 [1150/1251 ( 92%)]  Loss: 3.360 (3.73)  Time: 0.667s, 1536.00/s  (0.693s, 1478.69/s)  LR: 5.747e-04  Data: 0.012 (0.012)
Train: 273 [1200/1251 ( 96%)]  Loss: 3.891 (3.74)  Time: 0.671s, 1525.03/s  (0.692s, 1478.70/s)  LR: 5.747e-04  Data: 0.010 (0.012)
Train: 273 [1250/1251 (100%)]  Loss: 3.682 (3.73)  Time: 0.710s, 1441.92/s  (0.693s, 1478.56/s)  LR: 5.747e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.506 (1.506)  Loss:  0.9199 (0.9199)  Acc@1: 88.1836 (88.1836)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.136 (0.585)  Loss:  0.9219 (1.4444)  Acc@1: 83.4906 (72.6400)  Acc@5: 96.2264 (91.2380)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-257.pth.tar', 73.07200016601563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-269.pth.tar', 73.01599992919922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-268.pth.tar', 72.87000000976562)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-270.pth.tar', 72.8340000390625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-262.pth.tar', 72.76800011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-267.pth.tar', 72.76599994384766)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-271.pth.tar', 72.72000009277343)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-254.pth.tar', 72.68600006835938)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-273.pth.tar', 72.64000006835937)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-256.pth.tar', 72.62999988525391)

Train: 274 [   0/1251 (  0%)]  Loss: 3.455 (3.46)  Time: 2.143s,  477.82/s  (2.143s,  477.82/s)  LR: 5.722e-04  Data: 1.518 (1.518)
Train: 274 [  50/1251 (  4%)]  Loss: 3.838 (3.65)  Time: 0.718s, 1426.38/s  (0.730s, 1403.03/s)  LR: 5.722e-04  Data: 0.014 (0.050)
Train: 274 [ 100/1251 (  8%)]  Loss: 3.524 (3.61)  Time: 0.711s, 1439.26/s  (0.711s, 1440.94/s)  LR: 5.722e-04  Data: 0.009 (0.030)
Train: 274 [ 150/1251 ( 12%)]  Loss: 3.580 (3.60)  Time: 0.666s, 1538.25/s  (0.707s, 1448.66/s)  LR: 5.722e-04  Data: 0.010 (0.024)
Train: 274 [ 200/1251 ( 16%)]  Loss: 3.749 (3.63)  Time: 0.673s, 1521.01/s  (0.703s, 1457.51/s)  LR: 5.722e-04  Data: 0.011 (0.020)
Train: 274 [ 250/1251 ( 20%)]  Loss: 3.260 (3.57)  Time: 0.672s, 1523.76/s  (0.702s, 1459.66/s)  LR: 5.722e-04  Data: 0.010 (0.018)
Train: 274 [ 300/1251 ( 24%)]  Loss: 3.381 (3.54)  Time: 0.727s, 1409.42/s  (0.700s, 1463.45/s)  LR: 5.722e-04  Data: 0.009 (0.017)
Train: 274 [ 350/1251 ( 28%)]  Loss: 3.304 (3.51)  Time: 0.672s, 1522.98/s  (0.698s, 1466.58/s)  LR: 5.722e-04  Data: 0.014 (0.016)
Train: 274 [ 400/1251 ( 32%)]  Loss: 3.440 (3.50)  Time: 0.691s, 1481.36/s  (0.697s, 1469.46/s)  LR: 5.722e-04  Data: 0.009 (0.015)
Train: 274 [ 450/1251 ( 36%)]  Loss: 3.648 (3.52)  Time: 0.671s, 1525.16/s  (0.697s, 1469.24/s)  LR: 5.722e-04  Data: 0.011 (0.015)
Train: 274 [ 500/1251 ( 40%)]  Loss: 3.854 (3.55)  Time: 0.676s, 1513.91/s  (0.696s, 1471.47/s)  LR: 5.722e-04  Data: 0.012 (0.014)
Train: 274 [ 550/1251 ( 44%)]  Loss: 3.801 (3.57)  Time: 0.704s, 1455.37/s  (0.696s, 1471.80/s)  LR: 5.722e-04  Data: 0.012 (0.014)
Train: 274 [ 600/1251 ( 48%)]  Loss: 3.771 (3.59)  Time: 0.672s, 1524.06/s  (0.695s, 1472.97/s)  LR: 5.722e-04  Data: 0.011 (0.014)
Train: 274 [ 650/1251 ( 52%)]  Loss: 3.265 (3.56)  Time: 0.674s, 1520.04/s  (0.695s, 1473.06/s)  LR: 5.722e-04  Data: 0.011 (0.014)
Train: 274 [ 700/1251 ( 56%)]  Loss: 3.909 (3.59)  Time: 0.674s, 1518.94/s  (0.694s, 1474.58/s)  LR: 5.722e-04  Data: 0.010 (0.013)
Train: 274 [ 750/1251 ( 60%)]  Loss: 3.531 (3.58)  Time: 0.704s, 1454.90/s  (0.694s, 1474.60/s)  LR: 5.722e-04  Data: 0.010 (0.013)
Train: 274 [ 800/1251 ( 64%)]  Loss: 3.558 (3.58)  Time: 0.667s, 1535.07/s  (0.694s, 1475.53/s)  LR: 5.722e-04  Data: 0.009 (0.013)
Train: 274 [ 850/1251 ( 68%)]  Loss: 3.426 (3.57)  Time: 0.677s, 1512.37/s  (0.694s, 1475.70/s)  LR: 5.722e-04  Data: 0.015 (0.013)
Train: 274 [ 900/1251 ( 72%)]  Loss: 3.750 (3.58)  Time: 0.668s, 1534.08/s  (0.694s, 1475.02/s)  LR: 5.722e-04  Data: 0.011 (0.013)
Train: 274 [ 950/1251 ( 76%)]  Loss: 3.757 (3.59)  Time: 0.678s, 1509.74/s  (0.695s, 1474.37/s)  LR: 5.722e-04  Data: 0.012 (0.013)
Train: 274 [1000/1251 ( 80%)]  Loss: 3.546 (3.59)  Time: 0.674s, 1518.79/s  (0.695s, 1473.87/s)  LR: 5.722e-04  Data: 0.012 (0.012)
Train: 274 [1050/1251 ( 84%)]  Loss: 3.692 (3.59)  Time: 0.709s, 1445.28/s  (0.694s, 1474.90/s)  LR: 5.722e-04  Data: 0.015 (0.012)
Train: 274 [1100/1251 ( 88%)]  Loss: 3.501 (3.59)  Time: 0.674s, 1520.30/s  (0.694s, 1475.11/s)  LR: 5.722e-04  Data: 0.011 (0.012)
Train: 274 [1150/1251 ( 92%)]  Loss: 3.651 (3.59)  Time: 0.674s, 1519.41/s  (0.694s, 1475.52/s)  LR: 5.722e-04  Data: 0.010 (0.012)
Train: 274 [1200/1251 ( 96%)]  Loss: 3.693 (3.60)  Time: 0.673s, 1522.47/s  (0.694s, 1476.48/s)  LR: 5.722e-04  Data: 0.010 (0.012)
Train: 274 [1250/1251 (100%)]  Loss: 3.677 (3.60)  Time: 0.671s, 1525.27/s  (0.694s, 1476.35/s)  LR: 5.722e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.645 (1.645)  Loss:  0.7861 (0.7861)  Acc@1: 88.5742 (88.5742)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.136 (0.585)  Loss:  0.8936 (1.5044)  Acc@1: 85.9670 (72.9120)  Acc@5: 96.1085 (91.2900)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-257.pth.tar', 73.07200016601563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-269.pth.tar', 73.01599992919922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-274.pth.tar', 72.91199998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-268.pth.tar', 72.87000000976562)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-270.pth.tar', 72.8340000390625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-262.pth.tar', 72.76800011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-267.pth.tar', 72.76599994384766)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-271.pth.tar', 72.72000009277343)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-254.pth.tar', 72.68600006835938)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-273.pth.tar', 72.64000006835937)

Train: 275 [   0/1251 (  0%)]  Loss: 3.745 (3.74)  Time: 2.207s,  463.98/s  (2.207s,  463.98/s)  LR: 5.696e-04  Data: 1.591 (1.591)
Train: 275 [  50/1251 (  4%)]  Loss: 3.731 (3.74)  Time: 0.672s, 1523.30/s  (0.721s, 1420.33/s)  LR: 5.696e-04  Data: 0.011 (0.045)
Train: 275 [ 100/1251 (  8%)]  Loss: 3.844 (3.77)  Time: 0.715s, 1433.12/s  (0.707s, 1448.82/s)  LR: 5.696e-04  Data: 0.009 (0.028)
Train: 275 [ 150/1251 ( 12%)]  Loss: 3.812 (3.78)  Time: 0.669s, 1530.81/s  (0.699s, 1464.55/s)  LR: 5.696e-04  Data: 0.010 (0.022)
Train: 275 [ 200/1251 ( 16%)]  Loss: 3.821 (3.79)  Time: 0.717s, 1428.14/s  (0.697s, 1468.27/s)  LR: 5.696e-04  Data: 0.014 (0.019)
Train: 275 [ 250/1251 ( 20%)]  Loss: 3.907 (3.81)  Time: 0.687s, 1491.45/s  (0.697s, 1468.69/s)  LR: 5.696e-04  Data: 0.012 (0.018)
Train: 275 [ 300/1251 ( 24%)]  Loss: 3.720 (3.80)  Time: 0.668s, 1533.39/s  (0.696s, 1471.61/s)  LR: 5.696e-04  Data: 0.010 (0.016)
Train: 275 [ 350/1251 ( 28%)]  Loss: 3.584 (3.77)  Time: 0.704s, 1454.08/s  (0.694s, 1474.45/s)  LR: 5.696e-04  Data: 0.009 (0.015)
Train: 275 [ 400/1251 ( 32%)]  Loss: 3.604 (3.75)  Time: 0.700s, 1462.70/s  (0.694s, 1476.10/s)  LR: 5.696e-04  Data: 0.009 (0.015)
Train: 275 [ 450/1251 ( 36%)]  Loss: 3.893 (3.77)  Time: 0.699s, 1464.69/s  (0.695s, 1472.94/s)  LR: 5.696e-04  Data: 0.011 (0.015)
Train: 275 [ 500/1251 ( 40%)]  Loss: 3.772 (3.77)  Time: 0.686s, 1493.47/s  (0.697s, 1469.98/s)  LR: 5.696e-04  Data: 0.013 (0.014)
Train: 275 [ 550/1251 ( 44%)]  Loss: 3.467 (3.74)  Time: 0.748s, 1369.77/s  (0.698s, 1467.28/s)  LR: 5.696e-04  Data: 0.014 (0.014)
Train: 275 [ 600/1251 ( 48%)]  Loss: 3.852 (3.75)  Time: 0.674s, 1520.29/s  (0.698s, 1466.41/s)  LR: 5.696e-04  Data: 0.010 (0.014)
Train: 275 [ 650/1251 ( 52%)]  Loss: 3.692 (3.75)  Time: 0.671s, 1525.33/s  (0.697s, 1469.23/s)  LR: 5.696e-04  Data: 0.010 (0.014)
Train: 275 [ 700/1251 ( 56%)]  Loss: 3.533 (3.73)  Time: 0.670s, 1528.32/s  (0.696s, 1471.63/s)  LR: 5.696e-04  Data: 0.010 (0.013)
Train: 275 [ 750/1251 ( 60%)]  Loss: 3.882 (3.74)  Time: 0.702s, 1457.81/s  (0.695s, 1473.26/s)  LR: 5.696e-04  Data: 0.009 (0.013)
Train: 275 [ 800/1251 ( 64%)]  Loss: 3.726 (3.74)  Time: 0.678s, 1509.43/s  (0.695s, 1473.86/s)  LR: 5.696e-04  Data: 0.010 (0.013)
Train: 275 [ 850/1251 ( 68%)]  Loss: 3.589 (3.73)  Time: 0.679s, 1508.31/s  (0.695s, 1473.67/s)  LR: 5.696e-04  Data: 0.013 (0.013)
Train: 275 [ 900/1251 ( 72%)]  Loss: 3.952 (3.74)  Time: 0.670s, 1528.14/s  (0.695s, 1474.02/s)  LR: 5.696e-04  Data: 0.010 (0.013)
Train: 275 [ 950/1251 ( 76%)]  Loss: 3.514 (3.73)  Time: 0.673s, 1521.59/s  (0.695s, 1473.93/s)  LR: 5.696e-04  Data: 0.011 (0.013)
Train: 275 [1000/1251 ( 80%)]  Loss: 3.720 (3.73)  Time: 0.672s, 1524.57/s  (0.695s, 1473.98/s)  LR: 5.696e-04  Data: 0.011 (0.012)
Train: 275 [1050/1251 ( 84%)]  Loss: 3.935 (3.74)  Time: 0.668s, 1533.16/s  (0.695s, 1473.74/s)  LR: 5.696e-04  Data: 0.010 (0.012)
Train: 275 [1100/1251 ( 88%)]  Loss: 3.543 (3.73)  Time: 0.699s, 1464.26/s  (0.695s, 1474.27/s)  LR: 5.696e-04  Data: 0.009 (0.012)
Train: 275 [1150/1251 ( 92%)]  Loss: 3.908 (3.74)  Time: 0.685s, 1494.75/s  (0.695s, 1474.34/s)  LR: 5.696e-04  Data: 0.017 (0.012)
Train: 275 [1200/1251 ( 96%)]  Loss: 3.825 (3.74)  Time: 0.685s, 1495.52/s  (0.694s, 1474.68/s)  LR: 5.696e-04  Data: 0.014 (0.012)
Train: 275 [1250/1251 (100%)]  Loss: 3.378 (3.73)  Time: 0.702s, 1458.88/s  (0.694s, 1475.45/s)  LR: 5.696e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.579 (1.579)  Loss:  0.8140 (0.8140)  Acc@1: 89.2578 (89.2578)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.136 (0.582)  Loss:  0.9189 (1.4596)  Acc@1: 83.9623 (72.5080)  Acc@5: 96.4623 (91.1300)
Train: 276 [   0/1251 (  0%)]  Loss: 3.646 (3.65)  Time: 2.151s,  475.98/s  (2.151s,  475.98/s)  LR: 5.670e-04  Data: 1.536 (1.536)
Train: 276 [  50/1251 (  4%)]  Loss: 3.205 (3.43)  Time: 0.674s, 1519.88/s  (0.731s, 1401.71/s)  LR: 5.670e-04  Data: 0.011 (0.052)
Train: 276 [ 100/1251 (  8%)]  Loss: 3.677 (3.51)  Time: 0.668s, 1532.38/s  (0.707s, 1448.56/s)  LR: 5.670e-04  Data: 0.009 (0.031)
Train: 276 [ 150/1251 ( 12%)]  Loss: 3.693 (3.56)  Time: 0.692s, 1480.48/s  (0.702s, 1459.07/s)  LR: 5.670e-04  Data: 0.010 (0.024)
Train: 276 [ 200/1251 ( 16%)]  Loss: 3.362 (3.52)  Time: 0.706s, 1449.98/s  (0.700s, 1463.09/s)  LR: 5.670e-04  Data: 0.011 (0.021)
Train: 276 [ 250/1251 ( 20%)]  Loss: 3.807 (3.56)  Time: 0.704s, 1454.89/s  (0.699s, 1464.71/s)  LR: 5.670e-04  Data: 0.011 (0.019)
Train: 276 [ 300/1251 ( 24%)]  Loss: 3.624 (3.57)  Time: 0.696s, 1471.20/s  (0.699s, 1465.26/s)  LR: 5.670e-04  Data: 0.011 (0.017)
Train: 276 [ 350/1251 ( 28%)]  Loss: 3.916 (3.62)  Time: 0.671s, 1527.12/s  (0.697s, 1469.15/s)  LR: 5.670e-04  Data: 0.011 (0.017)
Train: 276 [ 400/1251 ( 32%)]  Loss: 3.586 (3.61)  Time: 0.702s, 1458.15/s  (0.696s, 1471.22/s)  LR: 5.670e-04  Data: 0.010 (0.016)
Train: 276 [ 450/1251 ( 36%)]  Loss: 3.025 (3.55)  Time: 0.680s, 1505.54/s  (0.696s, 1471.44/s)  LR: 5.670e-04  Data: 0.014 (0.015)
Train: 276 [ 500/1251 ( 40%)]  Loss: 3.519 (3.55)  Time: 0.728s, 1406.39/s  (0.695s, 1472.57/s)  LR: 5.670e-04  Data: 0.011 (0.015)
Train: 276 [ 550/1251 ( 44%)]  Loss: 3.089 (3.51)  Time: 0.675s, 1517.36/s  (0.695s, 1473.05/s)  LR: 5.670e-04  Data: 0.011 (0.014)
Train: 276 [ 600/1251 ( 48%)]  Loss: 4.006 (3.55)  Time: 0.702s, 1457.99/s  (0.695s, 1473.01/s)  LR: 5.670e-04  Data: 0.010 (0.014)
Train: 276 [ 650/1251 ( 52%)]  Loss: 3.826 (3.57)  Time: 0.700s, 1463.71/s  (0.695s, 1473.21/s)  LR: 5.670e-04  Data: 0.009 (0.014)
Train: 276 [ 700/1251 ( 56%)]  Loss: 3.638 (3.57)  Time: 0.672s, 1523.41/s  (0.694s, 1474.72/s)  LR: 5.670e-04  Data: 0.009 (0.013)
Train: 276 [ 750/1251 ( 60%)]  Loss: 3.746 (3.59)  Time: 0.674s, 1520.22/s  (0.694s, 1475.87/s)  LR: 5.670e-04  Data: 0.011 (0.013)
Train: 276 [ 800/1251 ( 64%)]  Loss: 3.442 (3.58)  Time: 0.711s, 1440.52/s  (0.694s, 1475.56/s)  LR: 5.670e-04  Data: 0.010 (0.013)
Train: 276 [ 850/1251 ( 68%)]  Loss: 3.136 (3.55)  Time: 0.688s, 1488.79/s  (0.694s, 1476.06/s)  LR: 5.670e-04  Data: 0.009 (0.013)
Train: 276 [ 900/1251 ( 72%)]  Loss: 4.039 (3.58)  Time: 0.672s, 1524.80/s  (0.694s, 1475.93/s)  LR: 5.670e-04  Data: 0.009 (0.013)
Train: 276 [ 950/1251 ( 76%)]  Loss: 3.783 (3.59)  Time: 0.696s, 1470.48/s  (0.694s, 1475.61/s)  LR: 5.670e-04  Data: 0.010 (0.013)
Train: 276 [1000/1251 ( 80%)]  Loss: 3.675 (3.59)  Time: 0.706s, 1449.96/s  (0.694s, 1475.54/s)  LR: 5.670e-04  Data: 0.011 (0.013)
Train: 276 [1050/1251 ( 84%)]  Loss: 3.819 (3.60)  Time: 0.672s, 1524.94/s  (0.694s, 1476.30/s)  LR: 5.670e-04  Data: 0.010 (0.012)
Train: 276 [1100/1251 ( 88%)]  Loss: 3.538 (3.60)  Time: 0.674s, 1520.19/s  (0.693s, 1476.72/s)  LR: 5.670e-04  Data: 0.011 (0.012)
Train: 276 [1150/1251 ( 92%)]  Loss: 3.484 (3.60)  Time: 0.718s, 1425.23/s  (0.694s, 1475.92/s)  LR: 5.670e-04  Data: 0.011 (0.012)
Train: 276 [1200/1251 ( 96%)]  Loss: 4.023 (3.61)  Time: 0.689s, 1486.15/s  (0.694s, 1476.09/s)  LR: 5.670e-04  Data: 0.010 (0.012)
Train: 276 [1250/1251 (100%)]  Loss: 3.828 (3.62)  Time: 0.657s, 1558.10/s  (0.694s, 1476.01/s)  LR: 5.670e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.531 (1.531)  Loss:  0.8584 (0.8584)  Acc@1: 89.4531 (89.4531)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.136 (0.581)  Loss:  1.1045 (1.5058)  Acc@1: 83.2547 (72.8800)  Acc@5: 95.7547 (91.2860)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-257.pth.tar', 73.07200016601563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-269.pth.tar', 73.01599992919922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-274.pth.tar', 72.91199998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-276.pth.tar', 72.87999996582032)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-268.pth.tar', 72.87000000976562)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-270.pth.tar', 72.8340000390625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-262.pth.tar', 72.76800011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-267.pth.tar', 72.76599994384766)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-271.pth.tar', 72.72000009277343)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-254.pth.tar', 72.68600006835938)

Train: 277 [   0/1251 (  0%)]  Loss: 3.603 (3.60)  Time: 2.191s,  467.28/s  (2.191s,  467.28/s)  LR: 5.645e-04  Data: 1.527 (1.527)
Train: 277 [  50/1251 (  4%)]  Loss: 3.467 (3.53)  Time: 0.710s, 1442.67/s  (0.731s, 1401.60/s)  LR: 5.645e-04  Data: 0.010 (0.049)
Train: 277 [ 100/1251 (  8%)]  Loss: 3.741 (3.60)  Time: 0.724s, 1414.50/s  (0.709s, 1444.63/s)  LR: 5.645e-04  Data: 0.009 (0.030)
Train: 277 [ 150/1251 ( 12%)]  Loss: 3.675 (3.62)  Time: 0.702s, 1459.16/s  (0.700s, 1461.87/s)  LR: 5.645e-04  Data: 0.010 (0.023)
Train: 277 [ 200/1251 ( 16%)]  Loss: 3.804 (3.66)  Time: 0.671s, 1525.69/s  (0.699s, 1465.51/s)  LR: 5.645e-04  Data: 0.009 (0.020)
Train: 277 [ 250/1251 ( 20%)]  Loss: 3.781 (3.68)  Time: 0.671s, 1525.66/s  (0.697s, 1468.36/s)  LR: 5.645e-04  Data: 0.010 (0.018)
Train: 277 [ 300/1251 ( 24%)]  Loss: 4.169 (3.75)  Time: 0.670s, 1527.64/s  (0.697s, 1469.18/s)  LR: 5.645e-04  Data: 0.010 (0.017)
Train: 277 [ 350/1251 ( 28%)]  Loss: 3.961 (3.78)  Time: 0.709s, 1445.29/s  (0.696s, 1470.59/s)  LR: 5.645e-04  Data: 0.009 (0.016)
Train: 277 [ 400/1251 ( 32%)]  Loss: 3.731 (3.77)  Time: 0.676s, 1515.24/s  (0.695s, 1473.40/s)  LR: 5.645e-04  Data: 0.010 (0.015)
Train: 277 [ 450/1251 ( 36%)]  Loss: 3.501 (3.74)  Time: 0.700s, 1463.39/s  (0.695s, 1473.90/s)  LR: 5.645e-04  Data: 0.012 (0.015)
Train: 277 [ 500/1251 ( 40%)]  Loss: 3.966 (3.76)  Time: 0.672s, 1524.13/s  (0.694s, 1475.30/s)  LR: 5.645e-04  Data: 0.012 (0.014)
Train: 277 [ 550/1251 ( 44%)]  Loss: 3.692 (3.76)  Time: 0.672s, 1524.23/s  (0.694s, 1476.20/s)  LR: 5.645e-04  Data: 0.010 (0.014)
Train: 277 [ 600/1251 ( 48%)]  Loss: 3.581 (3.74)  Time: 0.717s, 1427.62/s  (0.694s, 1476.14/s)  LR: 5.645e-04  Data: 0.010 (0.014)
Train: 277 [ 650/1251 ( 52%)]  Loss: 3.490 (3.73)  Time: 0.671s, 1526.14/s  (0.694s, 1475.84/s)  LR: 5.645e-04  Data: 0.011 (0.013)
Train: 277 [ 700/1251 ( 56%)]  Loss: 3.680 (3.72)  Time: 0.718s, 1426.46/s  (0.694s, 1476.43/s)  LR: 5.645e-04  Data: 0.010 (0.013)
Train: 277 [ 750/1251 ( 60%)]  Loss: 3.774 (3.73)  Time: 0.671s, 1526.43/s  (0.694s, 1476.32/s)  LR: 5.645e-04  Data: 0.011 (0.013)
Train: 277 [ 800/1251 ( 64%)]  Loss: 3.743 (3.73)  Time: 0.702s, 1459.56/s  (0.694s, 1475.76/s)  LR: 5.645e-04  Data: 0.009 (0.013)
Train: 277 [ 850/1251 ( 68%)]  Loss: 3.414 (3.71)  Time: 0.705s, 1452.96/s  (0.694s, 1476.20/s)  LR: 5.645e-04  Data: 0.009 (0.013)
Train: 277 [ 900/1251 ( 72%)]  Loss: 3.712 (3.71)  Time: 0.707s, 1449.16/s  (0.693s, 1476.98/s)  LR: 5.645e-04  Data: 0.010 (0.013)
Train: 277 [ 950/1251 ( 76%)]  Loss: 3.707 (3.71)  Time: 0.674s, 1520.03/s  (0.694s, 1475.50/s)  LR: 5.645e-04  Data: 0.009 (0.012)
Train: 277 [1000/1251 ( 80%)]  Loss: 3.604 (3.70)  Time: 0.706s, 1451.15/s  (0.693s, 1476.77/s)  LR: 5.645e-04  Data: 0.009 (0.012)
Train: 277 [1050/1251 ( 84%)]  Loss: 3.517 (3.70)  Time: 0.678s, 1511.40/s  (0.693s, 1477.08/s)  LR: 5.645e-04  Data: 0.009 (0.012)
Train: 277 [1100/1251 ( 88%)]  Loss: 3.736 (3.70)  Time: 0.705s, 1452.13/s  (0.693s, 1477.07/s)  LR: 5.645e-04  Data: 0.010 (0.012)
Train: 277 [1150/1251 ( 92%)]  Loss: 3.553 (3.69)  Time: 0.672s, 1523.78/s  (0.693s, 1477.87/s)  LR: 5.645e-04  Data: 0.010 (0.012)
Train: 277 [1200/1251 ( 96%)]  Loss: 3.732 (3.69)  Time: 0.672s, 1524.36/s  (0.693s, 1478.26/s)  LR: 5.645e-04  Data: 0.011 (0.012)
Train: 277 [1250/1251 (100%)]  Loss: 3.442 (3.68)  Time: 0.657s, 1559.27/s  (0.693s, 1477.49/s)  LR: 5.645e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.608 (1.608)  Loss:  0.8853 (0.8853)  Acc@1: 88.5742 (88.5742)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.137 (0.586)  Loss:  1.0146 (1.4868)  Acc@1: 83.8443 (73.2660)  Acc@5: 96.6981 (91.4580)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-277.pth.tar', 73.26600009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-257.pth.tar', 73.07200016601563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-269.pth.tar', 73.01599992919922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-274.pth.tar', 72.91199998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-276.pth.tar', 72.87999996582032)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-268.pth.tar', 72.87000000976562)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-270.pth.tar', 72.8340000390625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-262.pth.tar', 72.76800011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-267.pth.tar', 72.76599994384766)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-271.pth.tar', 72.72000009277343)

Train: 278 [   0/1251 (  0%)]  Loss: 3.389 (3.39)  Time: 2.293s,  446.63/s  (2.293s,  446.63/s)  LR: 5.619e-04  Data: 1.676 (1.676)
Train: 278 [  50/1251 (  4%)]  Loss: 3.624 (3.51)  Time: 0.673s, 1521.68/s  (0.734s, 1394.83/s)  LR: 5.619e-04  Data: 0.011 (0.051)
Train: 278 [ 100/1251 (  8%)]  Loss: 3.538 (3.52)  Time: 0.703s, 1457.23/s  (0.709s, 1443.63/s)  LR: 5.619e-04  Data: 0.010 (0.031)
Train: 278 [ 150/1251 ( 12%)]  Loss: 3.723 (3.57)  Time: 0.714s, 1434.34/s  (0.704s, 1453.95/s)  LR: 5.619e-04  Data: 0.009 (0.024)
Train: 278 [ 200/1251 ( 16%)]  Loss: 3.689 (3.59)  Time: 0.676s, 1515.81/s  (0.702s, 1459.42/s)  LR: 5.619e-04  Data: 0.010 (0.020)
Train: 278 [ 250/1251 ( 20%)]  Loss: 3.502 (3.58)  Time: 0.689s, 1485.84/s  (0.699s, 1465.55/s)  LR: 5.619e-04  Data: 0.012 (0.018)
Train: 278 [ 300/1251 ( 24%)]  Loss: 3.802 (3.61)  Time: 0.669s, 1530.42/s  (0.698s, 1467.10/s)  LR: 5.619e-04  Data: 0.009 (0.017)
Train: 278 [ 350/1251 ( 28%)]  Loss: 3.767 (3.63)  Time: 0.672s, 1524.10/s  (0.697s, 1469.44/s)  LR: 5.619e-04  Data: 0.011 (0.016)
Train: 278 [ 400/1251 ( 32%)]  Loss: 3.758 (3.64)  Time: 0.675s, 1517.73/s  (0.696s, 1471.27/s)  LR: 5.619e-04  Data: 0.011 (0.015)
Train: 278 [ 450/1251 ( 36%)]  Loss: 3.455 (3.62)  Time: 0.671s, 1526.48/s  (0.695s, 1472.45/s)  LR: 5.619e-04  Data: 0.011 (0.015)
Train: 278 [ 500/1251 ( 40%)]  Loss: 3.815 (3.64)  Time: 0.685s, 1494.07/s  (0.695s, 1474.31/s)  LR: 5.619e-04  Data: 0.011 (0.014)
Train: 278 [ 550/1251 ( 44%)]  Loss: 3.482 (3.63)  Time: 0.668s, 1532.81/s  (0.694s, 1475.40/s)  LR: 5.619e-04  Data: 0.010 (0.014)
Train: 278 [ 600/1251 ( 48%)]  Loss: 3.557 (3.62)  Time: 0.717s, 1427.77/s  (0.694s, 1475.43/s)  LR: 5.619e-04  Data: 0.011 (0.014)
Train: 278 [ 650/1251 ( 52%)]  Loss: 3.471 (3.61)  Time: 0.710s, 1442.50/s  (0.694s, 1475.56/s)  LR: 5.619e-04  Data: 0.011 (0.014)
Train: 278 [ 700/1251 ( 56%)]  Loss: 3.536 (3.61)  Time: 0.737s, 1389.43/s  (0.694s, 1476.54/s)  LR: 5.619e-04  Data: 0.010 (0.013)
Train: 278 [ 750/1251 ( 60%)]  Loss: 3.604 (3.61)  Time: 0.673s, 1522.50/s  (0.694s, 1476.55/s)  LR: 5.619e-04  Data: 0.010 (0.013)
Train: 278 [ 800/1251 ( 64%)]  Loss: 3.450 (3.60)  Time: 0.687s, 1489.87/s  (0.693s, 1476.73/s)  LR: 5.619e-04  Data: 0.011 (0.013)
Train: 278 [ 850/1251 ( 68%)]  Loss: 3.661 (3.60)  Time: 0.666s, 1538.22/s  (0.693s, 1476.63/s)  LR: 5.619e-04  Data: 0.011 (0.013)
Train: 278 [ 900/1251 ( 72%)]  Loss: 3.473 (3.59)  Time: 0.672s, 1524.13/s  (0.693s, 1477.25/s)  LR: 5.619e-04  Data: 0.010 (0.013)
Train: 278 [ 950/1251 ( 76%)]  Loss: 3.454 (3.59)  Time: 0.679s, 1508.09/s  (0.693s, 1476.85/s)  LR: 5.619e-04  Data: 0.011 (0.013)
Train: 278 [1000/1251 ( 80%)]  Loss: 3.697 (3.59)  Time: 0.713s, 1435.49/s  (0.693s, 1476.61/s)  LR: 5.619e-04  Data: 0.009 (0.012)
Train: 278 [1050/1251 ( 84%)]  Loss: 3.903 (3.61)  Time: 0.690s, 1484.98/s  (0.693s, 1476.96/s)  LR: 5.619e-04  Data: 0.011 (0.012)
Train: 278 [1100/1251 ( 88%)]  Loss: 4.093 (3.63)  Time: 0.676s, 1515.01/s  (0.693s, 1476.77/s)  LR: 5.619e-04  Data: 0.010 (0.012)
Train: 278 [1150/1251 ( 92%)]  Loss: 3.834 (3.64)  Time: 0.671s, 1525.07/s  (0.693s, 1477.02/s)  LR: 5.619e-04  Data: 0.011 (0.012)
Train: 278 [1200/1251 ( 96%)]  Loss: 3.747 (3.64)  Time: 0.673s, 1521.57/s  (0.693s, 1477.24/s)  LR: 5.619e-04  Data: 0.010 (0.012)
Train: 278 [1250/1251 (100%)]  Loss: 3.476 (3.63)  Time: 0.656s, 1560.93/s  (0.693s, 1477.91/s)  LR: 5.619e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.568 (1.568)  Loss:  0.9478 (0.9478)  Acc@1: 87.5977 (87.5977)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.136 (0.581)  Loss:  1.0977 (1.4989)  Acc@1: 83.7264 (72.8640)  Acc@5: 95.8726 (91.2760)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-277.pth.tar', 73.26600009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-257.pth.tar', 73.07200016601563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-269.pth.tar', 73.01599992919922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-274.pth.tar', 72.91199998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-276.pth.tar', 72.87999996582032)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-268.pth.tar', 72.87000000976562)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-278.pth.tar', 72.8640000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-270.pth.tar', 72.8340000390625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-262.pth.tar', 72.76800011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-267.pth.tar', 72.76599994384766)

Train: 279 [   0/1251 (  0%)]  Loss: 3.382 (3.38)  Time: 2.219s,  461.38/s  (2.219s,  461.38/s)  LR: 5.593e-04  Data: 1.605 (1.605)
Train: 279 [  50/1251 (  4%)]  Loss: 3.848 (3.62)  Time: 0.699s, 1465.38/s  (0.733s, 1396.23/s)  LR: 5.593e-04  Data: 0.009 (0.048)
Train: 279 [ 100/1251 (  8%)]  Loss: 3.733 (3.65)  Time: 0.719s, 1423.43/s  (0.710s, 1441.85/s)  LR: 5.593e-04  Data: 0.010 (0.029)
Train: 279 [ 150/1251 ( 12%)]  Loss: 3.448 (3.60)  Time: 0.689s, 1486.75/s  (0.706s, 1450.71/s)  LR: 5.593e-04  Data: 0.010 (0.023)
Train: 279 [ 200/1251 ( 16%)]  Loss: 3.352 (3.55)  Time: 0.704s, 1454.01/s  (0.701s, 1460.84/s)  LR: 5.593e-04  Data: 0.009 (0.020)
Train: 279 [ 250/1251 ( 20%)]  Loss: 3.610 (3.56)  Time: 0.671s, 1525.70/s  (0.699s, 1465.39/s)  LR: 5.593e-04  Data: 0.010 (0.018)
Train: 279 [ 300/1251 ( 24%)]  Loss: 3.791 (3.60)  Time: 0.693s, 1477.60/s  (0.698s, 1467.88/s)  LR: 5.593e-04  Data: 0.009 (0.017)
Train: 279 [ 350/1251 ( 28%)]  Loss: 3.691 (3.61)  Time: 0.706s, 1450.01/s  (0.696s, 1471.10/s)  LR: 5.593e-04  Data: 0.010 (0.016)
Train: 279 [ 400/1251 ( 32%)]  Loss: 3.359 (3.58)  Time: 0.719s, 1425.07/s  (0.696s, 1471.34/s)  LR: 5.593e-04  Data: 0.010 (0.015)
Train: 279 [ 450/1251 ( 36%)]  Loss: 3.574 (3.58)  Time: 0.670s, 1527.33/s  (0.695s, 1472.67/s)  LR: 5.593e-04  Data: 0.010 (0.015)
Train: 279 [ 500/1251 ( 40%)]  Loss: 3.508 (3.57)  Time: 0.670s, 1527.63/s  (0.694s, 1475.02/s)  LR: 5.593e-04  Data: 0.009 (0.014)
Train: 279 [ 550/1251 ( 44%)]  Loss: 3.079 (3.53)  Time: 0.698s, 1467.62/s  (0.695s, 1474.34/s)  LR: 5.593e-04  Data: 0.012 (0.014)
Train: 279 [ 600/1251 ( 48%)]  Loss: 3.742 (3.55)  Time: 0.719s, 1423.89/s  (0.695s, 1473.52/s)  LR: 5.593e-04  Data: 0.011 (0.014)
Train: 279 [ 650/1251 ( 52%)]  Loss: 3.155 (3.52)  Time: 0.719s, 1424.60/s  (0.695s, 1473.42/s)  LR: 5.593e-04  Data: 0.014 (0.013)
Train: 279 [ 700/1251 ( 56%)]  Loss: 3.897 (3.54)  Time: 0.678s, 1511.32/s  (0.695s, 1474.40/s)  LR: 5.593e-04  Data: 0.010 (0.013)
Train: 279 [ 750/1251 ( 60%)]  Loss: 3.541 (3.54)  Time: 0.782s, 1308.93/s  (0.695s, 1473.43/s)  LR: 5.593e-04  Data: 0.011 (0.013)
Train: 279 [ 800/1251 ( 64%)]  Loss: 3.875 (3.56)  Time: 0.730s, 1402.07/s  (0.695s, 1473.28/s)  LR: 5.593e-04  Data: 0.009 (0.013)
Train: 279 [ 850/1251 ( 68%)]  Loss: 4.125 (3.60)  Time: 0.714s, 1434.71/s  (0.695s, 1472.58/s)  LR: 5.593e-04  Data: 0.009 (0.013)
Train: 279 [ 900/1251 ( 72%)]  Loss: 3.578 (3.59)  Time: 0.672s, 1524.81/s  (0.695s, 1472.40/s)  LR: 5.593e-04  Data: 0.009 (0.013)
Train: 279 [ 950/1251 ( 76%)]  Loss: 3.664 (3.60)  Time: 0.712s, 1439.13/s  (0.695s, 1472.92/s)  LR: 5.593e-04  Data: 0.009 (0.012)
Train: 279 [1000/1251 ( 80%)]  Loss: 3.616 (3.60)  Time: 0.672s, 1524.12/s  (0.695s, 1473.72/s)  LR: 5.593e-04  Data: 0.009 (0.012)
Train: 279 [1050/1251 ( 84%)]  Loss: 3.326 (3.59)  Time: 0.669s, 1531.33/s  (0.694s, 1474.50/s)  LR: 5.593e-04  Data: 0.010 (0.012)
Train: 279 [1100/1251 ( 88%)]  Loss: 3.823 (3.60)  Time: 0.719s, 1423.26/s  (0.694s, 1474.95/s)  LR: 5.593e-04  Data: 0.009 (0.012)
Train: 279 [1150/1251 ( 92%)]  Loss: 3.404 (3.59)  Time: 0.706s, 1450.09/s  (0.694s, 1474.56/s)  LR: 5.593e-04  Data: 0.010 (0.012)
Train: 279 [1200/1251 ( 96%)]  Loss: 3.934 (3.60)  Time: 0.680s, 1506.25/s  (0.695s, 1474.44/s)  LR: 5.593e-04  Data: 0.010 (0.012)
Train: 279 [1250/1251 (100%)]  Loss: 3.885 (3.61)  Time: 0.656s, 1561.12/s  (0.694s, 1475.15/s)  LR: 5.593e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.517 (1.517)  Loss:  0.8813 (0.8813)  Acc@1: 88.3789 (88.3789)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.136 (0.591)  Loss:  1.0283 (1.5364)  Acc@1: 83.4906 (72.5720)  Acc@5: 95.4009 (91.1080)
Train: 280 [   0/1251 (  0%)]  Loss: 3.672 (3.67)  Time: 2.136s,  479.29/s  (2.136s,  479.29/s)  LR: 5.567e-04  Data: 1.498 (1.498)
Train: 280 [  50/1251 (  4%)]  Loss: 3.927 (3.80)  Time: 0.674s, 1520.08/s  (0.725s, 1412.97/s)  LR: 5.567e-04  Data: 0.011 (0.048)
Train: 280 [ 100/1251 (  8%)]  Loss: 3.467 (3.69)  Time: 0.704s, 1455.52/s  (0.708s, 1446.89/s)  LR: 5.567e-04  Data: 0.010 (0.029)
Train: 280 [ 150/1251 ( 12%)]  Loss: 3.169 (3.56)  Time: 0.672s, 1523.85/s  (0.702s, 1459.32/s)  LR: 5.567e-04  Data: 0.011 (0.023)
Train: 280 [ 200/1251 ( 16%)]  Loss: 3.709 (3.59)  Time: 0.701s, 1460.48/s  (0.699s, 1464.13/s)  LR: 5.567e-04  Data: 0.009 (0.020)
Train: 280 [ 250/1251 ( 20%)]  Loss: 3.396 (3.56)  Time: 0.703s, 1456.82/s  (0.697s, 1468.57/s)  LR: 5.567e-04  Data: 0.013 (0.018)
Train: 280 [ 300/1251 ( 24%)]  Loss: 3.511 (3.55)  Time: 0.714s, 1434.30/s  (0.698s, 1467.26/s)  LR: 5.567e-04  Data: 0.011 (0.017)
Train: 280 [ 350/1251 ( 28%)]  Loss: 3.874 (3.59)  Time: 0.718s, 1426.12/s  (0.696s, 1471.00/s)  LR: 5.567e-04  Data: 0.009 (0.016)
Train: 280 [ 400/1251 ( 32%)]  Loss: 3.656 (3.60)  Time: 0.671s, 1526.79/s  (0.695s, 1472.78/s)  LR: 5.567e-04  Data: 0.011 (0.015)
Train: 280 [ 450/1251 ( 36%)]  Loss: 3.750 (3.61)  Time: 0.670s, 1527.26/s  (0.695s, 1472.65/s)  LR: 5.567e-04  Data: 0.011 (0.015)
Train: 280 [ 500/1251 ( 40%)]  Loss: 3.689 (3.62)  Time: 0.701s, 1460.09/s  (0.695s, 1474.22/s)  LR: 5.567e-04  Data: 0.009 (0.014)
Train: 280 [ 550/1251 ( 44%)]  Loss: 3.721 (3.63)  Time: 0.676s, 1514.01/s  (0.694s, 1475.74/s)  LR: 5.567e-04  Data: 0.013 (0.014)
Train: 280 [ 600/1251 ( 48%)]  Loss: 3.468 (3.62)  Time: 0.706s, 1451.38/s  (0.695s, 1474.39/s)  LR: 5.567e-04  Data: 0.011 (0.014)
Train: 280 [ 650/1251 ( 52%)]  Loss: 3.799 (3.63)  Time: 0.672s, 1523.88/s  (0.694s, 1475.06/s)  LR: 5.567e-04  Data: 0.010 (0.013)
Train: 280 [ 700/1251 ( 56%)]  Loss: 3.488 (3.62)  Time: 0.704s, 1455.56/s  (0.694s, 1475.78/s)  LR: 5.567e-04  Data: 0.009 (0.013)
Train: 280 [ 750/1251 ( 60%)]  Loss: 3.601 (3.62)  Time: 0.672s, 1522.85/s  (0.694s, 1475.69/s)  LR: 5.567e-04  Data: 0.010 (0.013)
Train: 280 [ 800/1251 ( 64%)]  Loss: 3.358 (3.60)  Time: 0.672s, 1523.03/s  (0.694s, 1474.66/s)  LR: 5.567e-04  Data: 0.012 (0.013)
Train: 280 [ 850/1251 ( 68%)]  Loss: 3.584 (3.60)  Time: 0.671s, 1524.98/s  (0.694s, 1476.27/s)  LR: 5.567e-04  Data: 0.011 (0.013)
Train: 280 [ 900/1251 ( 72%)]  Loss: 3.799 (3.61)  Time: 0.691s, 1482.35/s  (0.693s, 1477.76/s)  LR: 5.567e-04  Data: 0.011 (0.013)
Train: 280 [ 950/1251 ( 76%)]  Loss: 3.342 (3.60)  Time: 0.702s, 1457.88/s  (0.693s, 1477.94/s)  LR: 5.567e-04  Data: 0.009 (0.012)
Train: 280 [1000/1251 ( 80%)]  Loss: 3.826 (3.61)  Time: 0.671s, 1525.18/s  (0.693s, 1478.02/s)  LR: 5.567e-04  Data: 0.011 (0.012)
Train: 280 [1050/1251 ( 84%)]  Loss: 3.940 (3.62)  Time: 0.718s, 1426.24/s  (0.693s, 1477.54/s)  LR: 5.567e-04  Data: 0.009 (0.012)
Train: 280 [1100/1251 ( 88%)]  Loss: 4.102 (3.65)  Time: 0.732s, 1399.04/s  (0.693s, 1477.88/s)  LR: 5.567e-04  Data: 0.011 (0.012)
Train: 280 [1150/1251 ( 92%)]  Loss: 3.569 (3.64)  Time: 0.673s, 1522.07/s  (0.693s, 1477.49/s)  LR: 5.567e-04  Data: 0.013 (0.012)
Train: 280 [1200/1251 ( 96%)]  Loss: 4.084 (3.66)  Time: 0.710s, 1442.69/s  (0.693s, 1477.29/s)  LR: 5.567e-04  Data: 0.011 (0.012)
Train: 280 [1250/1251 (100%)]  Loss: 3.693 (3.66)  Time: 0.675s, 1517.31/s  (0.693s, 1478.48/s)  LR: 5.567e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.589 (1.589)  Loss:  0.9204 (0.9204)  Acc@1: 87.9883 (87.9883)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.136 (0.585)  Loss:  0.9463 (1.5424)  Acc@1: 83.7264 (71.6780)  Acc@5: 96.5802 (90.7740)
Train: 281 [   0/1251 (  0%)]  Loss: 3.625 (3.62)  Time: 2.176s,  470.50/s  (2.176s,  470.50/s)  LR: 5.542e-04  Data: 1.560 (1.560)
Train: 281 [  50/1251 (  4%)]  Loss: 3.709 (3.67)  Time: 0.670s, 1528.92/s  (0.735s, 1392.99/s)  LR: 5.542e-04  Data: 0.009 (0.051)
Train: 281 [ 100/1251 (  8%)]  Loss: 3.391 (3.58)  Time: 0.692s, 1479.67/s  (0.715s, 1432.42/s)  LR: 5.542e-04  Data: 0.010 (0.031)
Train: 281 [ 150/1251 ( 12%)]  Loss: 3.560 (3.57)  Time: 0.719s, 1424.58/s  (0.707s, 1448.00/s)  LR: 5.542e-04  Data: 0.012 (0.024)
Train: 281 [ 200/1251 ( 16%)]  Loss: 3.587 (3.57)  Time: 0.671s, 1526.45/s  (0.703s, 1457.37/s)  LR: 5.542e-04  Data: 0.010 (0.021)
Train: 281 [ 250/1251 ( 20%)]  Loss: 3.842 (3.62)  Time: 0.675s, 1517.47/s  (0.699s, 1464.19/s)  LR: 5.542e-04  Data: 0.011 (0.018)
Train: 281 [ 300/1251 ( 24%)]  Loss: 3.774 (3.64)  Time: 0.667s, 1534.95/s  (0.698s, 1466.82/s)  LR: 5.542e-04  Data: 0.010 (0.017)
Train: 281 [ 350/1251 ( 28%)]  Loss: 3.517 (3.63)  Time: 0.671s, 1525.08/s  (0.697s, 1470.20/s)  LR: 5.542e-04  Data: 0.009 (0.016)
Train: 281 [ 400/1251 ( 32%)]  Loss: 3.652 (3.63)  Time: 0.672s, 1522.71/s  (0.696s, 1471.35/s)  LR: 5.542e-04  Data: 0.010 (0.015)
Train: 281 [ 450/1251 ( 36%)]  Loss: 3.504 (3.62)  Time: 0.695s, 1473.58/s  (0.695s, 1473.09/s)  LR: 5.542e-04  Data: 0.009 (0.015)
Train: 281 [ 500/1251 ( 40%)]  Loss: 3.309 (3.59)  Time: 0.704s, 1453.99/s  (0.695s, 1473.08/s)  LR: 5.542e-04  Data: 0.010 (0.015)
Train: 281 [ 550/1251 ( 44%)]  Loss: 3.396 (3.57)  Time: 0.687s, 1489.91/s  (0.695s, 1472.90/s)  LR: 5.542e-04  Data: 0.010 (0.014)
Train: 281 [ 600/1251 ( 48%)]  Loss: 4.183 (3.62)  Time: 0.717s, 1428.43/s  (0.696s, 1471.86/s)  LR: 5.542e-04  Data: 0.014 (0.014)
Train: 281 [ 650/1251 ( 52%)]  Loss: 3.700 (3.62)  Time: 0.672s, 1523.24/s  (0.696s, 1471.49/s)  LR: 5.542e-04  Data: 0.010 (0.014)
Train: 281 [ 700/1251 ( 56%)]  Loss: 3.996 (3.65)  Time: 0.671s, 1525.36/s  (0.695s, 1472.76/s)  LR: 5.542e-04  Data: 0.009 (0.013)
Train: 281 [ 750/1251 ( 60%)]  Loss: 3.814 (3.66)  Time: 0.674s, 1519.22/s  (0.695s, 1473.45/s)  LR: 5.542e-04  Data: 0.014 (0.013)
Train: 281 [ 800/1251 ( 64%)]  Loss: 3.467 (3.65)  Time: 0.709s, 1445.05/s  (0.695s, 1472.52/s)  LR: 5.542e-04  Data: 0.009 (0.013)
Train: 281 [ 850/1251 ( 68%)]  Loss: 3.861 (3.66)  Time: 0.727s, 1408.62/s  (0.695s, 1473.18/s)  LR: 5.542e-04  Data: 0.010 (0.013)
Train: 281 [ 900/1251 ( 72%)]  Loss: 3.512 (3.65)  Time: 0.672s, 1522.88/s  (0.695s, 1472.49/s)  LR: 5.542e-04  Data: 0.010 (0.013)
Train: 281 [ 950/1251 ( 76%)]  Loss: 3.931 (3.67)  Time: 0.703s, 1456.39/s  (0.696s, 1472.19/s)  LR: 5.542e-04  Data: 0.009 (0.013)
Train: 281 [1000/1251 ( 80%)]  Loss: 3.927 (3.68)  Time: 0.698s, 1466.27/s  (0.695s, 1472.87/s)  LR: 5.542e-04  Data: 0.011 (0.012)
Train: 281 [1050/1251 ( 84%)]  Loss: 3.728 (3.68)  Time: 0.673s, 1522.57/s  (0.695s, 1473.84/s)  LR: 5.542e-04  Data: 0.010 (0.012)
Train: 281 [1100/1251 ( 88%)]  Loss: 3.681 (3.68)  Time: 0.697s, 1468.75/s  (0.694s, 1474.96/s)  LR: 5.542e-04  Data: 0.009 (0.012)
Train: 281 [1150/1251 ( 92%)]  Loss: 3.823 (3.69)  Time: 0.709s, 1444.82/s  (0.694s, 1475.40/s)  LR: 5.542e-04  Data: 0.013 (0.012)
Train: 281 [1200/1251 ( 96%)]  Loss: 3.478 (3.68)  Time: 0.700s, 1462.30/s  (0.694s, 1474.92/s)  LR: 5.542e-04  Data: 0.008 (0.012)
Train: 281 [1250/1251 (100%)]  Loss: 3.694 (3.68)  Time: 0.659s, 1555.01/s  (0.694s, 1475.07/s)  LR: 5.542e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.475 (1.475)  Loss:  0.8188 (0.8188)  Acc@1: 89.1602 (89.1602)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.137 (0.592)  Loss:  0.9517 (1.4285)  Acc@1: 84.0802 (73.1920)  Acc@5: 95.6368 (91.3840)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-277.pth.tar', 73.26600009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-281.pth.tar', 73.19199993652344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-257.pth.tar', 73.07200016601563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-269.pth.tar', 73.01599992919922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-274.pth.tar', 72.91199998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-276.pth.tar', 72.87999996582032)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-268.pth.tar', 72.87000000976562)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-278.pth.tar', 72.8640000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-270.pth.tar', 72.8340000390625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-262.pth.tar', 72.76800011230469)

Train: 282 [   0/1251 (  0%)]  Loss: 3.644 (3.64)  Time: 2.215s,  462.34/s  (2.215s,  462.34/s)  LR: 5.516e-04  Data: 1.599 (1.599)
Train: 282 [  50/1251 (  4%)]  Loss: 3.699 (3.67)  Time: 0.666s, 1536.97/s  (0.730s, 1403.28/s)  LR: 5.516e-04  Data: 0.010 (0.055)
Train: 282 [ 100/1251 (  8%)]  Loss: 3.293 (3.55)  Time: 0.678s, 1509.87/s  (0.713s, 1436.84/s)  LR: 5.516e-04  Data: 0.011 (0.033)
Train: 282 [ 150/1251 ( 12%)]  Loss: 3.583 (3.55)  Time: 0.676s, 1514.35/s  (0.705s, 1452.40/s)  LR: 5.516e-04  Data: 0.011 (0.026)
Train: 282 [ 200/1251 ( 16%)]  Loss: 3.692 (3.58)  Time: 0.674s, 1520.14/s  (0.700s, 1462.98/s)  LR: 5.516e-04  Data: 0.011 (0.022)
Train: 282 [ 250/1251 ( 20%)]  Loss: 3.699 (3.60)  Time: 0.677s, 1513.43/s  (0.698s, 1468.07/s)  LR: 5.516e-04  Data: 0.012 (0.020)
Train: 282 [ 300/1251 ( 24%)]  Loss: 3.686 (3.61)  Time: 0.673s, 1520.49/s  (0.697s, 1469.22/s)  LR: 5.516e-04  Data: 0.010 (0.018)
Train: 282 [ 350/1251 ( 28%)]  Loss: 3.612 (3.61)  Time: 0.712s, 1437.89/s  (0.696s, 1471.13/s)  LR: 5.516e-04  Data: 0.011 (0.017)
Train: 282 [ 400/1251 ( 32%)]  Loss: 3.849 (3.64)  Time: 0.674s, 1518.22/s  (0.696s, 1470.92/s)  LR: 5.516e-04  Data: 0.012 (0.016)
Train: 282 [ 450/1251 ( 36%)]  Loss: 4.108 (3.69)  Time: 0.673s, 1521.36/s  (0.696s, 1470.91/s)  LR: 5.516e-04  Data: 0.011 (0.016)
Train: 282 [ 500/1251 ( 40%)]  Loss: 3.578 (3.68)  Time: 0.731s, 1400.60/s  (0.695s, 1473.07/s)  LR: 5.516e-04  Data: 0.009 (0.015)
Train: 282 [ 550/1251 ( 44%)]  Loss: 3.933 (3.70)  Time: 0.672s, 1524.42/s  (0.695s, 1473.35/s)  LR: 5.516e-04  Data: 0.011 (0.015)
Train: 282 [ 600/1251 ( 48%)]  Loss: 3.619 (3.69)  Time: 0.707s, 1447.49/s  (0.695s, 1474.23/s)  LR: 5.516e-04  Data: 0.011 (0.014)
Train: 282 [ 650/1251 ( 52%)]  Loss: 3.235 (3.66)  Time: 0.674s, 1519.15/s  (0.695s, 1474.13/s)  LR: 5.516e-04  Data: 0.010 (0.014)
Train: 282 [ 700/1251 ( 56%)]  Loss: 3.583 (3.65)  Time: 0.673s, 1522.66/s  (0.694s, 1475.88/s)  LR: 5.516e-04  Data: 0.010 (0.014)
Train: 282 [ 750/1251 ( 60%)]  Loss: 3.915 (3.67)  Time: 0.689s, 1486.66/s  (0.694s, 1476.52/s)  LR: 5.516e-04  Data: 0.009 (0.014)
Train: 282 [ 800/1251 ( 64%)]  Loss: 3.756 (3.68)  Time: 0.711s, 1441.20/s  (0.693s, 1477.52/s)  LR: 5.516e-04  Data: 0.009 (0.013)
Train: 282 [ 850/1251 ( 68%)]  Loss: 3.727 (3.68)  Time: 0.707s, 1447.75/s  (0.693s, 1477.77/s)  LR: 5.516e-04  Data: 0.009 (0.013)
Train: 282 [ 900/1251 ( 72%)]  Loss: 3.529 (3.67)  Time: 0.672s, 1524.16/s  (0.692s, 1478.82/s)  LR: 5.516e-04  Data: 0.011 (0.013)
Train: 282 [ 950/1251 ( 76%)]  Loss: 3.635 (3.67)  Time: 0.705s, 1451.56/s  (0.692s, 1478.79/s)  LR: 5.516e-04  Data: 0.010 (0.013)
Train: 282 [1000/1251 ( 80%)]  Loss: 3.726 (3.67)  Time: 0.693s, 1477.90/s  (0.692s, 1479.29/s)  LR: 5.516e-04  Data: 0.011 (0.013)
Train: 282 [1050/1251 ( 84%)]  Loss: 3.803 (3.68)  Time: 0.704s, 1453.91/s  (0.692s, 1479.06/s)  LR: 5.516e-04  Data: 0.010 (0.013)
Train: 282 [1100/1251 ( 88%)]  Loss: 4.051 (3.69)  Time: 0.674s, 1518.25/s  (0.692s, 1479.26/s)  LR: 5.516e-04  Data: 0.010 (0.013)
Train: 282 [1150/1251 ( 92%)]  Loss: 3.732 (3.70)  Time: 0.703s, 1456.80/s  (0.692s, 1479.14/s)  LR: 5.516e-04  Data: 0.010 (0.012)
Train: 282 [1200/1251 ( 96%)]  Loss: 3.397 (3.68)  Time: 0.711s, 1439.30/s  (0.692s, 1479.44/s)  LR: 5.516e-04  Data: 0.028 (0.012)
Train: 282 [1250/1251 (100%)]  Loss: 3.729 (3.69)  Time: 0.659s, 1554.02/s  (0.692s, 1479.67/s)  LR: 5.516e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.443 (1.443)  Loss:  0.8394 (0.8394)  Acc@1: 88.8672 (88.8672)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.136 (0.580)  Loss:  0.9019 (1.3796)  Acc@1: 82.4292 (72.8820)  Acc@5: 95.6368 (91.3600)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-277.pth.tar', 73.26600009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-281.pth.tar', 73.19199993652344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-257.pth.tar', 73.07200016601563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-269.pth.tar', 73.01599992919922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-274.pth.tar', 72.91199998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-282.pth.tar', 72.88199999511718)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-276.pth.tar', 72.87999996582032)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-268.pth.tar', 72.87000000976562)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-278.pth.tar', 72.8640000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-270.pth.tar', 72.8340000390625)

Train: 283 [   0/1251 (  0%)]  Loss: 3.468 (3.47)  Time: 2.327s,  440.07/s  (2.327s,  440.07/s)  LR: 5.490e-04  Data: 1.693 (1.693)
Train: 283 [  50/1251 (  4%)]  Loss: 4.085 (3.78)  Time: 0.706s, 1451.41/s  (0.732s, 1399.44/s)  LR: 5.490e-04  Data: 0.009 (0.051)
Train: 283 [ 100/1251 (  8%)]  Loss: 3.949 (3.83)  Time: 0.672s, 1522.99/s  (0.713s, 1436.74/s)  LR: 5.490e-04  Data: 0.010 (0.031)
Train: 283 [ 150/1251 ( 12%)]  Loss: 3.437 (3.73)  Time: 0.704s, 1454.19/s  (0.707s, 1447.96/s)  LR: 5.490e-04  Data: 0.010 (0.024)
Train: 283 [ 200/1251 ( 16%)]  Loss: 4.348 (3.86)  Time: 0.674s, 1518.46/s  (0.703s, 1457.27/s)  LR: 5.490e-04  Data: 0.010 (0.021)
Train: 283 [ 250/1251 ( 20%)]  Loss: 3.562 (3.81)  Time: 0.675s, 1516.07/s  (0.701s, 1461.46/s)  LR: 5.490e-04  Data: 0.009 (0.019)
Train: 283 [ 300/1251 ( 24%)]  Loss: 3.824 (3.81)  Time: 0.717s, 1428.45/s  (0.699s, 1464.51/s)  LR: 5.490e-04  Data: 0.010 (0.017)
Train: 283 [ 350/1251 ( 28%)]  Loss: 3.427 (3.76)  Time: 0.671s, 1526.23/s  (0.698s, 1467.85/s)  LR: 5.490e-04  Data: 0.012 (0.016)
Train: 283 [ 400/1251 ( 32%)]  Loss: 3.716 (3.76)  Time: 0.673s, 1522.46/s  (0.697s, 1469.20/s)  LR: 5.490e-04  Data: 0.010 (0.016)
Train: 283 [ 450/1251 ( 36%)]  Loss: 3.968 (3.78)  Time: 0.758s, 1351.02/s  (0.697s, 1468.32/s)  LR: 5.490e-04  Data: 0.009 (0.015)
Train: 283 [ 500/1251 ( 40%)]  Loss: 3.500 (3.75)  Time: 0.774s, 1322.35/s  (0.697s, 1468.13/s)  LR: 5.490e-04  Data: 0.011 (0.015)
Train: 283 [ 550/1251 ( 44%)]  Loss: 3.307 (3.72)  Time: 0.666s, 1537.64/s  (0.698s, 1467.80/s)  LR: 5.490e-04  Data: 0.011 (0.014)
Train: 283 [ 600/1251 ( 48%)]  Loss: 3.618 (3.71)  Time: 0.721s, 1419.57/s  (0.697s, 1468.88/s)  LR: 5.490e-04  Data: 0.010 (0.014)
Train: 283 [ 650/1251 ( 52%)]  Loss: 3.818 (3.72)  Time: 0.672s, 1523.68/s  (0.697s, 1470.09/s)  LR: 5.490e-04  Data: 0.011 (0.014)
Train: 283 [ 700/1251 ( 56%)]  Loss: 3.390 (3.69)  Time: 0.695s, 1474.29/s  (0.696s, 1471.38/s)  LR: 5.490e-04  Data: 0.009 (0.013)
Train: 283 [ 750/1251 ( 60%)]  Loss: 3.432 (3.68)  Time: 0.710s, 1442.51/s  (0.695s, 1472.42/s)  LR: 5.490e-04  Data: 0.010 (0.013)
Train: 283 [ 800/1251 ( 64%)]  Loss: 3.905 (3.69)  Time: 0.691s, 1481.55/s  (0.695s, 1472.76/s)  LR: 5.490e-04  Data: 0.010 (0.013)
Train: 283 [ 850/1251 ( 68%)]  Loss: 3.822 (3.70)  Time: 0.689s, 1486.80/s  (0.695s, 1472.91/s)  LR: 5.490e-04  Data: 0.009 (0.013)
Train: 283 [ 900/1251 ( 72%)]  Loss: 4.066 (3.72)  Time: 0.729s, 1404.36/s  (0.695s, 1473.06/s)  LR: 5.490e-04  Data: 0.010 (0.013)
Train: 283 [ 950/1251 ( 76%)]  Loss: 3.265 (3.70)  Time: 0.669s, 1531.30/s  (0.695s, 1473.42/s)  LR: 5.490e-04  Data: 0.010 (0.013)
Train: 283 [1000/1251 ( 80%)]  Loss: 3.869 (3.70)  Time: 0.682s, 1502.05/s  (0.695s, 1474.01/s)  LR: 5.490e-04  Data: 0.009 (0.012)
Train: 283 [1050/1251 ( 84%)]  Loss: 3.601 (3.70)  Time: 0.673s, 1521.67/s  (0.694s, 1474.48/s)  LR: 5.490e-04  Data: 0.010 (0.012)
Train: 283 [1100/1251 ( 88%)]  Loss: 3.237 (3.68)  Time: 0.666s, 1537.42/s  (0.694s, 1474.84/s)  LR: 5.490e-04  Data: 0.009 (0.012)
Train: 283 [1150/1251 ( 92%)]  Loss: 3.700 (3.68)  Time: 0.677s, 1513.59/s  (0.694s, 1474.71/s)  LR: 5.490e-04  Data: 0.012 (0.012)
Train: 283 [1200/1251 ( 96%)]  Loss: 3.517 (3.67)  Time: 0.705s, 1452.07/s  (0.694s, 1474.71/s)  LR: 5.490e-04  Data: 0.011 (0.012)
Train: 283 [1250/1251 (100%)]  Loss: 3.586 (3.67)  Time: 0.659s, 1554.34/s  (0.694s, 1475.57/s)  LR: 5.490e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.520 (1.520)  Loss:  0.9634 (0.9634)  Acc@1: 88.3789 (88.3789)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.136 (0.586)  Loss:  1.1680 (1.5564)  Acc@1: 83.9623 (72.8360)  Acc@5: 95.9906 (91.4620)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-277.pth.tar', 73.26600009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-281.pth.tar', 73.19199993652344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-257.pth.tar', 73.07200016601563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-269.pth.tar', 73.01599992919922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-274.pth.tar', 72.91199998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-282.pth.tar', 72.88199999511718)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-276.pth.tar', 72.87999996582032)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-268.pth.tar', 72.87000000976562)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-278.pth.tar', 72.8640000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-283.pth.tar', 72.83600014404297)

Train: 284 [   0/1251 (  0%)]  Loss: 3.810 (3.81)  Time: 2.206s,  464.19/s  (2.206s,  464.19/s)  LR: 5.464e-04  Data: 1.590 (1.590)
Train: 284 [  50/1251 (  4%)]  Loss: 3.729 (3.77)  Time: 0.724s, 1414.79/s  (0.732s, 1399.77/s)  LR: 5.464e-04  Data: 0.010 (0.048)
Train: 284 [ 100/1251 (  8%)]  Loss: 3.876 (3.80)  Time: 0.675s, 1517.53/s  (0.711s, 1440.78/s)  LR: 5.464e-04  Data: 0.010 (0.029)
Train: 284 [ 150/1251 ( 12%)]  Loss: 4.000 (3.85)  Time: 0.699s, 1465.32/s  (0.705s, 1453.51/s)  LR: 5.464e-04  Data: 0.010 (0.023)
Train: 284 [ 200/1251 ( 16%)]  Loss: 3.776 (3.84)  Time: 0.700s, 1462.73/s  (0.701s, 1460.75/s)  LR: 5.464e-04  Data: 0.011 (0.020)
Train: 284 [ 250/1251 ( 20%)]  Loss: 3.554 (3.79)  Time: 0.700s, 1462.62/s  (0.701s, 1461.32/s)  LR: 5.464e-04  Data: 0.011 (0.018)
Train: 284 [ 300/1251 ( 24%)]  Loss: 3.681 (3.78)  Time: 0.672s, 1522.92/s  (0.699s, 1464.10/s)  LR: 5.464e-04  Data: 0.011 (0.017)
Train: 284 [ 350/1251 ( 28%)]  Loss: 3.580 (3.75)  Time: 0.678s, 1509.78/s  (0.698s, 1467.79/s)  LR: 5.464e-04  Data: 0.015 (0.016)
Train: 284 [ 400/1251 ( 32%)]  Loss: 3.743 (3.75)  Time: 0.683s, 1499.88/s  (0.698s, 1467.99/s)  LR: 5.464e-04  Data: 0.009 (0.015)
Train: 284 [ 450/1251 ( 36%)]  Loss: 3.823 (3.76)  Time: 0.677s, 1512.98/s  (0.696s, 1470.47/s)  LR: 5.464e-04  Data: 0.010 (0.015)
Train: 284 [ 500/1251 ( 40%)]  Loss: 3.615 (3.74)  Time: 0.693s, 1476.64/s  (0.696s, 1471.26/s)  LR: 5.464e-04  Data: 0.012 (0.014)
Train: 284 [ 550/1251 ( 44%)]  Loss: 3.578 (3.73)  Time: 0.679s, 1508.33/s  (0.695s, 1473.29/s)  LR: 5.464e-04  Data: 0.009 (0.014)
Train: 284 [ 600/1251 ( 48%)]  Loss: 3.722 (3.73)  Time: 0.687s, 1491.59/s  (0.695s, 1473.20/s)  LR: 5.464e-04  Data: 0.010 (0.014)
Train: 284 [ 650/1251 ( 52%)]  Loss: 3.884 (3.74)  Time: 0.721s, 1420.34/s  (0.694s, 1474.61/s)  LR: 5.464e-04  Data: 0.009 (0.013)
Train: 284 [ 700/1251 ( 56%)]  Loss: 3.847 (3.75)  Time: 0.705s, 1451.61/s  (0.694s, 1475.35/s)  LR: 5.464e-04  Data: 0.010 (0.013)
Train: 284 [ 750/1251 ( 60%)]  Loss: 3.623 (3.74)  Time: 0.674s, 1518.28/s  (0.694s, 1476.50/s)  LR: 5.464e-04  Data: 0.011 (0.013)
Train: 284 [ 800/1251 ( 64%)]  Loss: 3.846 (3.75)  Time: 0.706s, 1450.42/s  (0.693s, 1476.65/s)  LR: 5.464e-04  Data: 0.011 (0.013)
Train: 284 [ 850/1251 ( 68%)]  Loss: 4.107 (3.77)  Time: 0.709s, 1444.28/s  (0.693s, 1476.89/s)  LR: 5.464e-04  Data: 0.009 (0.013)
Train: 284 [ 900/1251 ( 72%)]  Loss: 3.708 (3.76)  Time: 0.675s, 1516.23/s  (0.694s, 1476.21/s)  LR: 5.464e-04  Data: 0.011 (0.013)
Train: 284 [ 950/1251 ( 76%)]  Loss: 3.800 (3.76)  Time: 0.665s, 1539.72/s  (0.693s, 1476.68/s)  LR: 5.464e-04  Data: 0.010 (0.012)
Train: 284 [1000/1251 ( 80%)]  Loss: 3.737 (3.76)  Time: 0.711s, 1441.12/s  (0.694s, 1476.24/s)  LR: 5.464e-04  Data: 0.012 (0.012)
Train: 284 [1050/1251 ( 84%)]  Loss: 3.400 (3.75)  Time: 0.723s, 1415.67/s  (0.693s, 1476.99/s)  LR: 5.464e-04  Data: 0.015 (0.012)
Train: 284 [1100/1251 ( 88%)]  Loss: 3.674 (3.74)  Time: 0.672s, 1524.29/s  (0.693s, 1477.58/s)  LR: 5.464e-04  Data: 0.011 (0.012)
Train: 284 [1150/1251 ( 92%)]  Loss: 3.925 (3.75)  Time: 0.672s, 1524.69/s  (0.693s, 1477.26/s)  LR: 5.464e-04  Data: 0.010 (0.012)
Train: 284 [1200/1251 ( 96%)]  Loss: 3.905 (3.76)  Time: 0.672s, 1522.97/s  (0.693s, 1476.90/s)  LR: 5.464e-04  Data: 0.010 (0.012)
Train: 284 [1250/1251 (100%)]  Loss: 3.301 (3.74)  Time: 0.715s, 1432.77/s  (0.693s, 1477.42/s)  LR: 5.464e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.501 (1.501)  Loss:  0.8140 (0.8140)  Acc@1: 89.7461 (89.7461)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.137 (0.597)  Loss:  0.8906 (1.3884)  Acc@1: 83.6085 (73.2420)  Acc@5: 96.2264 (91.5160)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-277.pth.tar', 73.26600009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-284.pth.tar', 73.24199999023438)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-281.pth.tar', 73.19199993652344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-257.pth.tar', 73.07200016601563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-269.pth.tar', 73.01599992919922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-274.pth.tar', 72.91199998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-282.pth.tar', 72.88199999511718)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-276.pth.tar', 72.87999996582032)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-268.pth.tar', 72.87000000976562)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-278.pth.tar', 72.8640000415039)

Train: 285 [   0/1251 (  0%)]  Loss: 3.533 (3.53)  Time: 2.262s,  452.71/s  (2.262s,  452.71/s)  LR: 5.438e-04  Data: 1.634 (1.634)
Train: 285 [  50/1251 (  4%)]  Loss: 3.226 (3.38)  Time: 0.729s, 1405.17/s  (0.751s, 1362.70/s)  LR: 5.438e-04  Data: 0.009 (0.052)
Train: 285 [ 100/1251 (  8%)]  Loss: 3.594 (3.45)  Time: 0.701s, 1461.27/s  (0.731s, 1401.32/s)  LR: 5.438e-04  Data: 0.033 (0.033)
Train: 285 [ 150/1251 ( 12%)]  Loss: 3.398 (3.44)  Time: 0.670s, 1528.92/s  (0.721s, 1419.97/s)  LR: 5.438e-04  Data: 0.013 (0.026)
Train: 285 [ 200/1251 ( 16%)]  Loss: 3.949 (3.54)  Time: 0.671s, 1526.52/s  (0.712s, 1437.49/s)  LR: 5.438e-04  Data: 0.010 (0.022)
Train: 285 [ 250/1251 ( 20%)]  Loss: 3.712 (3.57)  Time: 0.672s, 1524.75/s  (0.705s, 1451.63/s)  LR: 5.438e-04  Data: 0.010 (0.020)
Train: 285 [ 300/1251 ( 24%)]  Loss: 3.701 (3.59)  Time: 0.692s, 1478.76/s  (0.700s, 1462.27/s)  LR: 5.438e-04  Data: 0.011 (0.018)
Train: 285 [ 350/1251 ( 28%)]  Loss: 3.125 (3.53)  Time: 0.703s, 1457.00/s  (0.698s, 1466.02/s)  LR: 5.438e-04  Data: 0.010 (0.017)
Train: 285 [ 400/1251 ( 32%)]  Loss: 3.988 (3.58)  Time: 0.703s, 1456.30/s  (0.697s, 1468.44/s)  LR: 5.438e-04  Data: 0.009 (0.016)
Train: 285 [ 450/1251 ( 36%)]  Loss: 3.884 (3.61)  Time: 0.681s, 1502.78/s  (0.696s, 1470.61/s)  LR: 5.438e-04  Data: 0.013 (0.016)
Train: 285 [ 500/1251 ( 40%)]  Loss: 3.962 (3.64)  Time: 0.700s, 1463.86/s  (0.696s, 1471.20/s)  LR: 5.438e-04  Data: 0.008 (0.015)
Train: 285 [ 550/1251 ( 44%)]  Loss: 3.923 (3.67)  Time: 0.675s, 1516.71/s  (0.695s, 1473.26/s)  LR: 5.438e-04  Data: 0.010 (0.015)
Train: 285 [ 600/1251 ( 48%)]  Loss: 3.755 (3.67)  Time: 0.751s, 1363.50/s  (0.695s, 1473.78/s)  LR: 5.438e-04  Data: 0.012 (0.014)
Train: 285 [ 650/1251 ( 52%)]  Loss: 3.917 (3.69)  Time: 0.676s, 1514.38/s  (0.694s, 1475.18/s)  LR: 5.438e-04  Data: 0.010 (0.014)
Train: 285 [ 700/1251 ( 56%)]  Loss: 3.370 (3.67)  Time: 0.706s, 1450.74/s  (0.694s, 1475.07/s)  LR: 5.438e-04  Data: 0.009 (0.014)
Train: 285 [ 750/1251 ( 60%)]  Loss: 3.682 (3.67)  Time: 0.675s, 1516.90/s  (0.694s, 1475.44/s)  LR: 5.438e-04  Data: 0.010 (0.014)
Train: 285 [ 800/1251 ( 64%)]  Loss: 3.811 (3.68)  Time: 0.706s, 1450.35/s  (0.694s, 1476.20/s)  LR: 5.438e-04  Data: 0.009 (0.013)
Train: 285 [ 850/1251 ( 68%)]  Loss: 3.962 (3.69)  Time: 0.671s, 1524.96/s  (0.693s, 1476.94/s)  LR: 5.438e-04  Data: 0.011 (0.013)
Train: 285 [ 900/1251 ( 72%)]  Loss: 4.077 (3.71)  Time: 0.727s, 1409.22/s  (0.693s, 1477.49/s)  LR: 5.438e-04  Data: 0.010 (0.013)
Train: 285 [ 950/1251 ( 76%)]  Loss: 4.023 (3.73)  Time: 0.669s, 1530.55/s  (0.693s, 1476.73/s)  LR: 5.438e-04  Data: 0.010 (0.013)
Train: 285 [1000/1251 ( 80%)]  Loss: 3.588 (3.72)  Time: 0.671s, 1526.84/s  (0.693s, 1476.94/s)  LR: 5.438e-04  Data: 0.009 (0.013)
Train: 285 [1050/1251 ( 84%)]  Loss: 3.717 (3.72)  Time: 0.758s, 1350.33/s  (0.693s, 1477.01/s)  LR: 5.438e-04  Data: 0.009 (0.013)
Train: 285 [1100/1251 ( 88%)]  Loss: 3.256 (3.70)  Time: 0.671s, 1526.04/s  (0.693s, 1477.09/s)  LR: 5.438e-04  Data: 0.010 (0.013)
Train: 285 [1150/1251 ( 92%)]  Loss: 3.832 (3.71)  Time: 0.675s, 1517.30/s  (0.693s, 1477.02/s)  LR: 5.438e-04  Data: 0.011 (0.012)
Train: 285 [1200/1251 ( 96%)]  Loss: 3.705 (3.71)  Time: 0.671s, 1526.18/s  (0.693s, 1477.35/s)  LR: 5.438e-04  Data: 0.010 (0.012)
Train: 285 [1250/1251 (100%)]  Loss: 3.717 (3.71)  Time: 0.655s, 1562.71/s  (0.693s, 1477.59/s)  LR: 5.438e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.610 (1.610)  Loss:  0.7827 (0.7827)  Acc@1: 88.7695 (88.7695)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.136 (0.589)  Loss:  0.9072 (1.4093)  Acc@1: 83.6085 (72.6480)  Acc@5: 96.2264 (91.3140)
Train: 286 [   0/1251 (  0%)]  Loss: 3.621 (3.62)  Time: 2.183s,  469.16/s  (2.183s,  469.16/s)  LR: 5.413e-04  Data: 1.566 (1.566)
Train: 286 [  50/1251 (  4%)]  Loss: 3.744 (3.68)  Time: 0.664s, 1541.02/s  (0.726s, 1409.53/s)  LR: 5.413e-04  Data: 0.009 (0.052)
Train: 286 [ 100/1251 (  8%)]  Loss: 3.404 (3.59)  Time: 0.678s, 1511.31/s  (0.707s, 1447.64/s)  LR: 5.413e-04  Data: 0.011 (0.031)
Train: 286 [ 150/1251 ( 12%)]  Loss: 3.574 (3.59)  Time: 0.671s, 1526.16/s  (0.701s, 1461.11/s)  LR: 5.413e-04  Data: 0.011 (0.024)
Train: 286 [ 200/1251 ( 16%)]  Loss: 3.711 (3.61)  Time: 0.727s, 1408.21/s  (0.700s, 1463.05/s)  LR: 5.413e-04  Data: 0.010 (0.021)
Train: 286 [ 250/1251 ( 20%)]  Loss: 3.155 (3.53)  Time: 0.713s, 1436.25/s  (0.701s, 1461.61/s)  LR: 5.413e-04  Data: 0.012 (0.019)
Train: 286 [ 300/1251 ( 24%)]  Loss: 3.491 (3.53)  Time: 0.712s, 1437.61/s  (0.700s, 1463.77/s)  LR: 5.413e-04  Data: 0.010 (0.017)
Train: 286 [ 350/1251 ( 28%)]  Loss: 3.582 (3.54)  Time: 0.672s, 1523.85/s  (0.698s, 1467.34/s)  LR: 5.413e-04  Data: 0.011 (0.016)
Train: 286 [ 400/1251 ( 32%)]  Loss: 3.671 (3.55)  Time: 0.713s, 1437.07/s  (0.696s, 1470.41/s)  LR: 5.413e-04  Data: 0.009 (0.016)
Train: 286 [ 450/1251 ( 36%)]  Loss: 3.614 (3.56)  Time: 0.677s, 1513.23/s  (0.695s, 1473.04/s)  LR: 5.413e-04  Data: 0.011 (0.015)
Train: 286 [ 500/1251 ( 40%)]  Loss: 3.824 (3.58)  Time: 0.700s, 1463.05/s  (0.695s, 1474.14/s)  LR: 5.413e-04  Data: 0.015 (0.015)
Train: 286 [ 550/1251 ( 44%)]  Loss: 3.854 (3.60)  Time: 0.674s, 1518.80/s  (0.694s, 1475.14/s)  LR: 5.413e-04  Data: 0.011 (0.014)
Train: 286 [ 600/1251 ( 48%)]  Loss: 3.800 (3.62)  Time: 0.678s, 1510.04/s  (0.694s, 1474.83/s)  LR: 5.413e-04  Data: 0.010 (0.014)
Train: 286 [ 650/1251 ( 52%)]  Loss: 3.821 (3.63)  Time: 0.674s, 1518.56/s  (0.694s, 1474.54/s)  LR: 5.413e-04  Data: 0.013 (0.014)
Train: 286 [ 700/1251 ( 56%)]  Loss: 3.495 (3.62)  Time: 0.702s, 1458.17/s  (0.694s, 1474.94/s)  LR: 5.413e-04  Data: 0.009 (0.013)
Train: 286 [ 750/1251 ( 60%)]  Loss: 3.209 (3.60)  Time: 0.707s, 1447.80/s  (0.694s, 1475.48/s)  LR: 5.413e-04  Data: 0.009 (0.013)
Train: 286 [ 800/1251 ( 64%)]  Loss: 3.885 (3.62)  Time: 0.672s, 1523.70/s  (0.694s, 1475.36/s)  LR: 5.413e-04  Data: 0.011 (0.013)
Train: 286 [ 850/1251 ( 68%)]  Loss: 3.611 (3.61)  Time: 0.673s, 1522.46/s  (0.694s, 1475.32/s)  LR: 5.413e-04  Data: 0.010 (0.013)
Train: 286 [ 900/1251 ( 72%)]  Loss: 3.063 (3.59)  Time: 0.673s, 1520.98/s  (0.694s, 1476.31/s)  LR: 5.413e-04  Data: 0.011 (0.013)
Train: 286 [ 950/1251 ( 76%)]  Loss: 3.558 (3.58)  Time: 0.671s, 1525.36/s  (0.693s, 1476.99/s)  LR: 5.413e-04  Data: 0.010 (0.013)
Train: 286 [1000/1251 ( 80%)]  Loss: 3.939 (3.60)  Time: 0.672s, 1524.15/s  (0.694s, 1476.54/s)  LR: 5.413e-04  Data: 0.010 (0.012)
Train: 286 [1050/1251 ( 84%)]  Loss: 3.504 (3.60)  Time: 0.671s, 1525.67/s  (0.694s, 1476.48/s)  LR: 5.413e-04  Data: 0.010 (0.012)
Train: 286 [1100/1251 ( 88%)]  Loss: 3.229 (3.58)  Time: 0.679s, 1507.67/s  (0.694s, 1476.51/s)  LR: 5.413e-04  Data: 0.015 (0.012)
Train: 286 [1150/1251 ( 92%)]  Loss: 3.723 (3.59)  Time: 0.682s, 1501.72/s  (0.694s, 1475.96/s)  LR: 5.413e-04  Data: 0.009 (0.012)
Train: 286 [1200/1251 ( 96%)]  Loss: 3.729 (3.59)  Time: 0.675s, 1517.70/s  (0.694s, 1475.91/s)  LR: 5.413e-04  Data: 0.010 (0.012)
Train: 286 [1250/1251 (100%)]  Loss: 3.645 (3.59)  Time: 0.658s, 1556.84/s  (0.693s, 1476.79/s)  LR: 5.413e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.573 (1.573)  Loss:  0.8384 (0.8384)  Acc@1: 89.2578 (89.2578)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.137 (0.588)  Loss:  0.9004 (1.4053)  Acc@1: 83.8443 (73.4720)  Acc@5: 96.2264 (91.5380)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-286.pth.tar', 73.4719999633789)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-277.pth.tar', 73.26600009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-284.pth.tar', 73.24199999023438)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-281.pth.tar', 73.19199993652344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-257.pth.tar', 73.07200016601563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-269.pth.tar', 73.01599992919922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-274.pth.tar', 72.91199998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-282.pth.tar', 72.88199999511718)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-276.pth.tar', 72.87999996582032)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-268.pth.tar', 72.87000000976562)

Train: 287 [   0/1251 (  0%)]  Loss: 3.742 (3.74)  Time: 2.383s,  429.63/s  (2.383s,  429.63/s)  LR: 5.387e-04  Data: 1.750 (1.750)
Train: 287 [  50/1251 (  4%)]  Loss: 3.247 (3.49)  Time: 0.721s, 1420.04/s  (0.735s, 1393.32/s)  LR: 5.387e-04  Data: 0.012 (0.054)
Train: 287 [ 100/1251 (  8%)]  Loss: 3.814 (3.60)  Time: 0.679s, 1509.18/s  (0.716s, 1429.81/s)  LR: 5.387e-04  Data: 0.009 (0.033)
Train: 287 [ 150/1251 ( 12%)]  Loss: 3.489 (3.57)  Time: 0.671s, 1525.09/s  (0.707s, 1449.02/s)  LR: 5.387e-04  Data: 0.013 (0.025)
Train: 287 [ 200/1251 ( 16%)]  Loss: 3.636 (3.59)  Time: 0.702s, 1459.17/s  (0.703s, 1457.42/s)  LR: 5.387e-04  Data: 0.009 (0.021)
Train: 287 [ 250/1251 ( 20%)]  Loss: 3.274 (3.53)  Time: 0.670s, 1528.38/s  (0.701s, 1461.12/s)  LR: 5.387e-04  Data: 0.012 (0.019)
Train: 287 [ 300/1251 ( 24%)]  Loss: 3.694 (3.56)  Time: 0.712s, 1439.16/s  (0.698s, 1466.34/s)  LR: 5.387e-04  Data: 0.011 (0.018)
Train: 287 [ 350/1251 ( 28%)]  Loss: 3.731 (3.58)  Time: 0.673s, 1521.81/s  (0.697s, 1468.91/s)  LR: 5.387e-04  Data: 0.009 (0.017)
Train: 287 [ 400/1251 ( 32%)]  Loss: 3.531 (3.57)  Time: 0.711s, 1440.89/s  (0.696s, 1472.01/s)  LR: 5.387e-04  Data: 0.009 (0.016)
Train: 287 [ 450/1251 ( 36%)]  Loss: 3.593 (3.58)  Time: 0.724s, 1414.50/s  (0.695s, 1473.70/s)  LR: 5.387e-04  Data: 0.009 (0.015)
Train: 287 [ 500/1251 ( 40%)]  Loss: 3.410 (3.56)  Time: 0.671s, 1526.98/s  (0.694s, 1475.20/s)  LR: 5.387e-04  Data: 0.010 (0.015)
Train: 287 [ 550/1251 ( 44%)]  Loss: 3.937 (3.59)  Time: 0.668s, 1532.45/s  (0.694s, 1476.34/s)  LR: 5.387e-04  Data: 0.009 (0.014)
Train: 287 [ 600/1251 ( 48%)]  Loss: 3.907 (3.62)  Time: 0.690s, 1485.08/s  (0.693s, 1476.96/s)  LR: 5.387e-04  Data: 0.009 (0.014)
Train: 287 [ 650/1251 ( 52%)]  Loss: 3.862 (3.63)  Time: 0.674s, 1520.15/s  (0.693s, 1476.73/s)  LR: 5.387e-04  Data: 0.009 (0.014)
Train: 287 [ 700/1251 ( 56%)]  Loss: 3.596 (3.63)  Time: 0.667s, 1536.14/s  (0.693s, 1477.83/s)  LR: 5.387e-04  Data: 0.008 (0.014)
Train: 287 [ 750/1251 ( 60%)]  Loss: 3.291 (3.61)  Time: 0.698s, 1466.70/s  (0.693s, 1478.18/s)  LR: 5.387e-04  Data: 0.009 (0.013)
Train: 287 [ 800/1251 ( 64%)]  Loss: 3.469 (3.60)  Time: 0.687s, 1489.90/s  (0.693s, 1477.94/s)  LR: 5.387e-04  Data: 0.009 (0.013)
Train: 287 [ 850/1251 ( 68%)]  Loss: 3.605 (3.60)  Time: 0.685s, 1495.77/s  (0.693s, 1478.59/s)  LR: 5.387e-04  Data: 0.009 (0.013)
Train: 287 [ 900/1251 ( 72%)]  Loss: 3.516 (3.60)  Time: 0.703s, 1456.20/s  (0.693s, 1478.43/s)  LR: 5.387e-04  Data: 0.010 (0.013)
Train: 287 [ 950/1251 ( 76%)]  Loss: 3.605 (3.60)  Time: 0.724s, 1414.93/s  (0.693s, 1477.73/s)  LR: 5.387e-04  Data: 0.010 (0.013)
Train: 287 [1000/1251 ( 80%)]  Loss: 3.702 (3.60)  Time: 0.671s, 1525.60/s  (0.693s, 1477.72/s)  LR: 5.387e-04  Data: 0.010 (0.013)
Train: 287 [1050/1251 ( 84%)]  Loss: 3.694 (3.61)  Time: 0.695s, 1472.62/s  (0.693s, 1478.09/s)  LR: 5.387e-04  Data: 0.010 (0.012)
Train: 287 [1100/1251 ( 88%)]  Loss: 3.694 (3.61)  Time: 0.673s, 1521.01/s  (0.692s, 1479.10/s)  LR: 5.387e-04  Data: 0.010 (0.012)
Train: 287 [1150/1251 ( 92%)]  Loss: 3.507 (3.61)  Time: 0.671s, 1526.35/s  (0.693s, 1478.08/s)  LR: 5.387e-04  Data: 0.010 (0.012)
Train: 287 [1200/1251 ( 96%)]  Loss: 3.897 (3.62)  Time: 0.674s, 1518.20/s  (0.693s, 1477.13/s)  LR: 5.387e-04  Data: 0.010 (0.012)
Train: 287 [1250/1251 (100%)]  Loss: 3.774 (3.62)  Time: 0.655s, 1562.76/s  (0.693s, 1477.26/s)  LR: 5.387e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.633 (1.633)  Loss:  0.8604 (0.8604)  Acc@1: 88.4766 (88.4766)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.136 (0.587)  Loss:  0.9976 (1.4680)  Acc@1: 84.3160 (72.3880)  Acc@5: 95.1651 (91.1760)
Train: 288 [   0/1251 (  0%)]  Loss: 3.128 (3.13)  Time: 2.214s,  462.52/s  (2.214s,  462.52/s)  LR: 5.361e-04  Data: 1.598 (1.598)
Train: 288 [  50/1251 (  4%)]  Loss: 3.591 (3.36)  Time: 0.671s, 1525.92/s  (0.726s, 1411.07/s)  LR: 5.361e-04  Data: 0.010 (0.052)
Train: 288 [ 100/1251 (  8%)]  Loss: 3.813 (3.51)  Time: 0.697s, 1468.74/s  (0.708s, 1445.66/s)  LR: 5.361e-04  Data: 0.009 (0.031)
Train: 288 [ 150/1251 ( 12%)]  Loss: 3.680 (3.55)  Time: 0.671s, 1525.78/s  (0.702s, 1459.21/s)  LR: 5.361e-04  Data: 0.011 (0.025)
Train: 288 [ 200/1251 ( 16%)]  Loss: 3.126 (3.47)  Time: 0.671s, 1525.77/s  (0.700s, 1463.49/s)  LR: 5.361e-04  Data: 0.010 (0.021)
Train: 288 [ 250/1251 ( 20%)]  Loss: 3.985 (3.55)  Time: 0.688s, 1488.26/s  (0.697s, 1468.60/s)  LR: 5.361e-04  Data: 0.010 (0.019)
Train: 288 [ 300/1251 ( 24%)]  Loss: 3.997 (3.62)  Time: 0.723s, 1416.80/s  (0.696s, 1470.54/s)  LR: 5.361e-04  Data: 0.010 (0.017)
Train: 288 [ 350/1251 ( 28%)]  Loss: 4.193 (3.69)  Time: 0.707s, 1448.64/s  (0.696s, 1471.07/s)  LR: 5.361e-04  Data: 0.011 (0.016)
Train: 288 [ 400/1251 ( 32%)]  Loss: 2.896 (3.60)  Time: 0.738s, 1386.61/s  (0.696s, 1471.38/s)  LR: 5.361e-04  Data: 0.010 (0.016)
Train: 288 [ 450/1251 ( 36%)]  Loss: 3.389 (3.58)  Time: 0.686s, 1493.03/s  (0.695s, 1472.70/s)  LR: 5.361e-04  Data: 0.010 (0.015)
Train: 288 [ 500/1251 ( 40%)]  Loss: 3.522 (3.57)  Time: 0.673s, 1522.14/s  (0.695s, 1473.95/s)  LR: 5.361e-04  Data: 0.011 (0.015)
Train: 288 [ 550/1251 ( 44%)]  Loss: 3.788 (3.59)  Time: 0.708s, 1447.10/s  (0.695s, 1473.40/s)  LR: 5.361e-04  Data: 0.014 (0.014)
Train: 288 [ 600/1251 ( 48%)]  Loss: 3.761 (3.61)  Time: 0.672s, 1524.87/s  (0.695s, 1472.66/s)  LR: 5.361e-04  Data: 0.013 (0.014)
Train: 288 [ 650/1251 ( 52%)]  Loss: 3.927 (3.63)  Time: 0.680s, 1505.54/s  (0.695s, 1473.82/s)  LR: 5.361e-04  Data: 0.011 (0.014)
Train: 288 [ 700/1251 ( 56%)]  Loss: 3.688 (3.63)  Time: 0.671s, 1525.70/s  (0.694s, 1475.45/s)  LR: 5.361e-04  Data: 0.010 (0.014)
Train: 288 [ 750/1251 ( 60%)]  Loss: 3.800 (3.64)  Time: 0.706s, 1449.47/s  (0.694s, 1475.24/s)  LR: 5.361e-04  Data: 0.009 (0.013)
Train: 288 [ 800/1251 ( 64%)]  Loss: 3.602 (3.64)  Time: 0.674s, 1518.64/s  (0.695s, 1474.22/s)  LR: 5.361e-04  Data: 0.009 (0.013)
Train: 288 [ 850/1251 ( 68%)]  Loss: 3.615 (3.64)  Time: 0.759s, 1350.01/s  (0.694s, 1474.61/s)  LR: 5.361e-04  Data: 0.018 (0.013)
Train: 288 [ 900/1251 ( 72%)]  Loss: 4.169 (3.67)  Time: 0.672s, 1524.41/s  (0.694s, 1474.70/s)  LR: 5.361e-04  Data: 0.011 (0.013)
Train: 288 [ 950/1251 ( 76%)]  Loss: 3.705 (3.67)  Time: 0.673s, 1521.02/s  (0.694s, 1475.01/s)  LR: 5.361e-04  Data: 0.010 (0.013)
Train: 288 [1000/1251 ( 80%)]  Loss: 3.657 (3.67)  Time: 0.674s, 1520.05/s  (0.694s, 1474.94/s)  LR: 5.361e-04  Data: 0.010 (0.013)
Train: 288 [1050/1251 ( 84%)]  Loss: 3.944 (3.68)  Time: 0.730s, 1403.58/s  (0.695s, 1474.21/s)  LR: 5.361e-04  Data: 0.011 (0.013)
Train: 288 [1100/1251 ( 88%)]  Loss: 3.957 (3.69)  Time: 0.703s, 1455.58/s  (0.694s, 1474.77/s)  LR: 5.361e-04  Data: 0.013 (0.012)
Train: 288 [1150/1251 ( 92%)]  Loss: 3.716 (3.69)  Time: 0.672s, 1524.61/s  (0.694s, 1474.61/s)  LR: 5.361e-04  Data: 0.010 (0.012)
Train: 288 [1200/1251 ( 96%)]  Loss: 3.912 (3.70)  Time: 0.673s, 1521.42/s  (0.694s, 1474.80/s)  LR: 5.361e-04  Data: 0.009 (0.012)
Train: 288 [1250/1251 (100%)]  Loss: 3.887 (3.71)  Time: 0.657s, 1559.60/s  (0.694s, 1475.24/s)  LR: 5.361e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.561 (1.561)  Loss:  0.7769 (0.7769)  Acc@1: 89.0625 (89.0625)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.137 (0.582)  Loss:  0.9194 (1.3723)  Acc@1: 82.9009 (72.8740)  Acc@5: 95.4009 (91.4760)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-286.pth.tar', 73.4719999633789)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-277.pth.tar', 73.26600009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-284.pth.tar', 73.24199999023438)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-281.pth.tar', 73.19199993652344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-257.pth.tar', 73.07200016601563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-269.pth.tar', 73.01599992919922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-274.pth.tar', 72.91199998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-282.pth.tar', 72.88199999511718)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-276.pth.tar', 72.87999996582032)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-288.pth.tar', 72.87400007080078)

Train: 289 [   0/1251 (  0%)]  Loss: 3.757 (3.76)  Time: 2.195s,  466.51/s  (2.195s,  466.51/s)  LR: 5.335e-04  Data: 1.559 (1.559)
Train: 289 [  50/1251 (  4%)]  Loss: 3.488 (3.62)  Time: 0.678s, 1509.63/s  (0.730s, 1401.87/s)  LR: 5.335e-04  Data: 0.009 (0.049)
Train: 289 [ 100/1251 (  8%)]  Loss: 3.915 (3.72)  Time: 0.670s, 1527.54/s  (0.710s, 1443.03/s)  LR: 5.335e-04  Data: 0.010 (0.030)
Train: 289 [ 150/1251 ( 12%)]  Loss: 3.236 (3.60)  Time: 0.702s, 1458.94/s  (0.703s, 1456.47/s)  LR: 5.335e-04  Data: 0.010 (0.023)
Train: 289 [ 200/1251 ( 16%)]  Loss: 3.743 (3.63)  Time: 0.666s, 1536.81/s  (0.701s, 1461.22/s)  LR: 5.335e-04  Data: 0.011 (0.020)
Train: 289 [ 250/1251 ( 20%)]  Loss: 3.040 (3.53)  Time: 0.746s, 1372.97/s  (0.701s, 1460.55/s)  LR: 5.335e-04  Data: 0.010 (0.018)
Train: 289 [ 300/1251 ( 24%)]  Loss: 3.409 (3.51)  Time: 0.686s, 1493.60/s  (0.701s, 1461.23/s)  LR: 5.335e-04  Data: 0.009 (0.017)
Train: 289 [ 350/1251 ( 28%)]  Loss: 3.871 (3.56)  Time: 0.717s, 1427.47/s  (0.701s, 1461.66/s)  LR: 5.335e-04  Data: 0.009 (0.016)
Train: 289 [ 400/1251 ( 32%)]  Loss: 3.695 (3.57)  Time: 0.753s, 1359.41/s  (0.699s, 1464.25/s)  LR: 5.335e-04  Data: 0.009 (0.015)
Train: 289 [ 450/1251 ( 36%)]  Loss: 3.445 (3.56)  Time: 0.705s, 1452.44/s  (0.698s, 1466.19/s)  LR: 5.335e-04  Data: 0.009 (0.015)
Train: 289 [ 500/1251 ( 40%)]  Loss: 3.851 (3.59)  Time: 0.700s, 1462.78/s  (0.698s, 1466.12/s)  LR: 5.335e-04  Data: 0.014 (0.014)
Train: 289 [ 550/1251 ( 44%)]  Loss: 3.683 (3.59)  Time: 0.706s, 1450.48/s  (0.698s, 1467.98/s)  LR: 5.335e-04  Data: 0.010 (0.014)
Train: 289 [ 600/1251 ( 48%)]  Loss: 3.842 (3.61)  Time: 0.763s, 1341.36/s  (0.697s, 1469.49/s)  LR: 5.335e-04  Data: 0.009 (0.013)
Train: 289 [ 650/1251 ( 52%)]  Loss: 3.763 (3.62)  Time: 0.672s, 1524.01/s  (0.697s, 1469.32/s)  LR: 5.335e-04  Data: 0.009 (0.013)
Train: 289 [ 700/1251 ( 56%)]  Loss: 3.544 (3.62)  Time: 0.673s, 1521.03/s  (0.696s, 1470.45/s)  LR: 5.335e-04  Data: 0.010 (0.013)
Train: 289 [ 750/1251 ( 60%)]  Loss: 3.672 (3.62)  Time: 0.672s, 1524.48/s  (0.696s, 1471.44/s)  LR: 5.335e-04  Data: 0.009 (0.013)
Train: 289 [ 800/1251 ( 64%)]  Loss: 3.787 (3.63)  Time: 0.672s, 1524.01/s  (0.696s, 1471.21/s)  LR: 5.335e-04  Data: 0.010 (0.013)
Train: 289 [ 850/1251 ( 68%)]  Loss: 3.592 (3.63)  Time: 0.699s, 1464.81/s  (0.696s, 1471.91/s)  LR: 5.335e-04  Data: 0.009 (0.013)
Train: 289 [ 900/1251 ( 72%)]  Loss: 3.762 (3.64)  Time: 0.674s, 1519.49/s  (0.695s, 1472.71/s)  LR: 5.335e-04  Data: 0.010 (0.012)
Train: 289 [ 950/1251 ( 76%)]  Loss: 3.693 (3.64)  Time: 0.676s, 1514.67/s  (0.695s, 1473.01/s)  LR: 5.335e-04  Data: 0.012 (0.012)
Train: 289 [1000/1251 ( 80%)]  Loss: 3.398 (3.63)  Time: 0.701s, 1461.42/s  (0.695s, 1473.45/s)  LR: 5.335e-04  Data: 0.010 (0.012)
Train: 289 [1050/1251 ( 84%)]  Loss: 3.385 (3.62)  Time: 0.705s, 1453.26/s  (0.695s, 1473.88/s)  LR: 5.335e-04  Data: 0.010 (0.012)
Train: 289 [1100/1251 ( 88%)]  Loss: 3.875 (3.63)  Time: 0.704s, 1455.07/s  (0.695s, 1474.27/s)  LR: 5.335e-04  Data: 0.010 (0.012)
Train: 289 [1150/1251 ( 92%)]  Loss: 3.641 (3.63)  Time: 0.683s, 1499.48/s  (0.695s, 1474.20/s)  LR: 5.335e-04  Data: 0.014 (0.012)
Train: 289 [1200/1251 ( 96%)]  Loss: 3.862 (3.64)  Time: 0.792s, 1293.20/s  (0.695s, 1474.21/s)  LR: 5.335e-04  Data: 0.011 (0.012)
Train: 289 [1250/1251 (100%)]  Loss: 3.796 (3.64)  Time: 0.656s, 1561.63/s  (0.694s, 1474.94/s)  LR: 5.335e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.561 (1.561)  Loss:  0.8384 (0.8384)  Acc@1: 89.3555 (89.3555)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.138 (0.587)  Loss:  0.9927 (1.4849)  Acc@1: 84.6698 (73.2360)  Acc@5: 96.1085 (91.5660)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-286.pth.tar', 73.4719999633789)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-277.pth.tar', 73.26600009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-284.pth.tar', 73.24199999023438)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-289.pth.tar', 73.23600006347657)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-281.pth.tar', 73.19199993652344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-257.pth.tar', 73.07200016601563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-269.pth.tar', 73.01599992919922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-274.pth.tar', 72.91199998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-282.pth.tar', 72.88199999511718)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-276.pth.tar', 72.87999996582032)

Train: 290 [   0/1251 (  0%)]  Loss: 3.484 (3.48)  Time: 2.159s,  474.36/s  (2.159s,  474.36/s)  LR: 5.309e-04  Data: 1.537 (1.537)
Train: 290 [  50/1251 (  4%)]  Loss: 3.819 (3.65)  Time: 0.733s, 1396.61/s  (0.732s, 1399.15/s)  LR: 5.309e-04  Data: 0.009 (0.047)
Train: 290 [ 100/1251 (  8%)]  Loss: 3.981 (3.76)  Time: 0.690s, 1484.96/s  (0.712s, 1437.84/s)  LR: 5.309e-04  Data: 0.009 (0.029)
Train: 290 [ 150/1251 ( 12%)]  Loss: 3.519 (3.70)  Time: 0.670s, 1527.43/s  (0.705s, 1453.15/s)  LR: 5.309e-04  Data: 0.010 (0.023)
Train: 290 [ 200/1251 ( 16%)]  Loss: 3.531 (3.67)  Time: 0.680s, 1506.46/s  (0.702s, 1459.46/s)  LR: 5.309e-04  Data: 0.009 (0.020)
Train: 290 [ 250/1251 ( 20%)]  Loss: 3.795 (3.69)  Time: 0.715s, 1432.24/s  (0.700s, 1462.04/s)  LR: 5.309e-04  Data: 0.011 (0.018)
Train: 290 [ 300/1251 ( 24%)]  Loss: 3.817 (3.71)  Time: 0.671s, 1526.40/s  (0.697s, 1468.82/s)  LR: 5.309e-04  Data: 0.011 (0.017)
Train: 290 [ 350/1251 ( 28%)]  Loss: 3.782 (3.72)  Time: 0.715s, 1432.07/s  (0.696s, 1471.93/s)  LR: 5.309e-04  Data: 0.011 (0.016)
Train: 290 [ 400/1251 ( 32%)]  Loss: 3.484 (3.69)  Time: 0.693s, 1477.66/s  (0.695s, 1472.73/s)  LR: 5.309e-04  Data: 0.010 (0.015)
Train: 290 [ 450/1251 ( 36%)]  Loss: 3.162 (3.64)  Time: 0.671s, 1525.40/s  (0.694s, 1474.65/s)  LR: 5.309e-04  Data: 0.010 (0.014)
Train: 290 [ 500/1251 ( 40%)]  Loss: 3.727 (3.65)  Time: 0.703s, 1457.12/s  (0.694s, 1474.56/s)  LR: 5.309e-04  Data: 0.010 (0.014)
Train: 290 [ 550/1251 ( 44%)]  Loss: 3.393 (3.62)  Time: 0.695s, 1472.78/s  (0.694s, 1475.21/s)  LR: 5.309e-04  Data: 0.009 (0.014)
Train: 290 [ 600/1251 ( 48%)]  Loss: 3.864 (3.64)  Time: 0.672s, 1523.68/s  (0.694s, 1475.96/s)  LR: 5.309e-04  Data: 0.010 (0.013)
Train: 290 [ 650/1251 ( 52%)]  Loss: 3.531 (3.63)  Time: 0.671s, 1527.06/s  (0.694s, 1475.33/s)  LR: 5.309e-04  Data: 0.010 (0.013)
Train: 290 [ 700/1251 ( 56%)]  Loss: 3.641 (3.64)  Time: 0.689s, 1486.82/s  (0.694s, 1474.71/s)  LR: 5.309e-04  Data: 0.010 (0.013)
Train: 290 [ 750/1251 ( 60%)]  Loss: 3.341 (3.62)  Time: 0.696s, 1471.07/s  (0.694s, 1475.89/s)  LR: 5.309e-04  Data: 0.010 (0.013)
Train: 290 [ 800/1251 ( 64%)]  Loss: 3.710 (3.62)  Time: 0.708s, 1445.38/s  (0.694s, 1475.48/s)  LR: 5.309e-04  Data: 0.012 (0.013)
Train: 290 [ 850/1251 ( 68%)]  Loss: 3.557 (3.62)  Time: 0.727s, 1409.24/s  (0.694s, 1474.89/s)  LR: 5.309e-04  Data: 0.010 (0.013)
Train: 290 [ 900/1251 ( 72%)]  Loss: 3.685 (3.62)  Time: 0.731s, 1401.45/s  (0.694s, 1475.56/s)  LR: 5.309e-04  Data: 0.011 (0.012)
Train: 290 [ 950/1251 ( 76%)]  Loss: 3.506 (3.62)  Time: 0.712s, 1437.92/s  (0.694s, 1475.64/s)  LR: 5.309e-04  Data: 0.010 (0.012)
Train: 290 [1000/1251 ( 80%)]  Loss: 3.510 (3.61)  Time: 0.722s, 1418.48/s  (0.694s, 1474.95/s)  LR: 5.309e-04  Data: 0.011 (0.012)
Train: 290 [1050/1251 ( 84%)]  Loss: 3.865 (3.62)  Time: 0.707s, 1448.94/s  (0.694s, 1474.78/s)  LR: 5.309e-04  Data: 0.011 (0.012)
Train: 290 [1100/1251 ( 88%)]  Loss: 3.977 (3.64)  Time: 0.715s, 1432.54/s  (0.694s, 1475.26/s)  LR: 5.309e-04  Data: 0.014 (0.012)
Train: 290 [1150/1251 ( 92%)]  Loss: 3.879 (3.65)  Time: 0.721s, 1419.93/s  (0.694s, 1474.67/s)  LR: 5.309e-04  Data: 0.010 (0.012)
Train: 290 [1200/1251 ( 96%)]  Loss: 3.567 (3.64)  Time: 0.702s, 1458.54/s  (0.694s, 1475.33/s)  LR: 5.309e-04  Data: 0.009 (0.012)
Train: 290 [1250/1251 (100%)]  Loss: 3.830 (3.65)  Time: 0.656s, 1561.30/s  (0.694s, 1476.17/s)  LR: 5.309e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.614 (1.614)  Loss:  0.9575 (0.9575)  Acc@1: 88.5742 (88.5742)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.136 (0.583)  Loss:  0.9683 (1.4917)  Acc@1: 84.5519 (72.9400)  Acc@5: 96.4623 (91.2140)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-286.pth.tar', 73.4719999633789)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-277.pth.tar', 73.26600009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-284.pth.tar', 73.24199999023438)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-289.pth.tar', 73.23600006347657)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-281.pth.tar', 73.19199993652344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-257.pth.tar', 73.07200016601563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-269.pth.tar', 73.01599992919922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-290.pth.tar', 72.9399998828125)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-274.pth.tar', 72.91199998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-282.pth.tar', 72.88199999511718)

Train: 291 [   0/1251 (  0%)]  Loss: 3.529 (3.53)  Time: 2.225s,  460.28/s  (2.225s,  460.28/s)  LR: 5.283e-04  Data: 1.609 (1.609)
Train: 291 [  50/1251 (  4%)]  Loss: 3.575 (3.55)  Time: 0.666s, 1538.02/s  (0.733s, 1396.22/s)  LR: 5.283e-04  Data: 0.009 (0.053)
Train: 291 [ 100/1251 (  8%)]  Loss: 3.607 (3.57)  Time: 0.673s, 1521.06/s  (0.711s, 1440.58/s)  LR: 5.283e-04  Data: 0.012 (0.032)
Train: 291 [ 150/1251 ( 12%)]  Loss: 3.458 (3.54)  Time: 0.707s, 1447.89/s  (0.704s, 1455.09/s)  LR: 5.283e-04  Data: 0.011 (0.025)
Train: 291 [ 200/1251 ( 16%)]  Loss: 3.940 (3.62)  Time: 0.672s, 1524.27/s  (0.701s, 1460.39/s)  LR: 5.283e-04  Data: 0.010 (0.021)
Train: 291 [ 250/1251 ( 20%)]  Loss: 3.690 (3.63)  Time: 0.694s, 1475.74/s  (0.700s, 1462.00/s)  LR: 5.283e-04  Data: 0.010 (0.019)
Train: 291 [ 300/1251 ( 24%)]  Loss: 3.625 (3.63)  Time: 0.691s, 1482.39/s  (0.698s, 1466.68/s)  LR: 5.283e-04  Data: 0.010 (0.018)
Train: 291 [ 350/1251 ( 28%)]  Loss: 3.496 (3.61)  Time: 0.727s, 1408.55/s  (0.697s, 1469.64/s)  LR: 5.283e-04  Data: 0.011 (0.017)
Train: 291 [ 400/1251 ( 32%)]  Loss: 3.656 (3.62)  Time: 0.671s, 1525.32/s  (0.695s, 1473.14/s)  LR: 5.283e-04  Data: 0.011 (0.016)
Train: 291 [ 450/1251 ( 36%)]  Loss: 3.679 (3.63)  Time: 0.699s, 1464.03/s  (0.695s, 1473.48/s)  LR: 5.283e-04  Data: 0.009 (0.015)
Train: 291 [ 500/1251 ( 40%)]  Loss: 3.263 (3.59)  Time: 0.674s, 1519.83/s  (0.695s, 1474.02/s)  LR: 5.283e-04  Data: 0.009 (0.015)
Train: 291 [ 550/1251 ( 44%)]  Loss: 3.535 (3.59)  Time: 0.670s, 1528.36/s  (0.695s, 1473.89/s)  LR: 5.283e-04  Data: 0.010 (0.014)
Train: 291 [ 600/1251 ( 48%)]  Loss: 3.844 (3.61)  Time: 0.669s, 1531.61/s  (0.694s, 1475.27/s)  LR: 5.283e-04  Data: 0.010 (0.014)
Train: 291 [ 650/1251 ( 52%)]  Loss: 3.617 (3.61)  Time: 0.683s, 1499.87/s  (0.695s, 1473.83/s)  LR: 5.283e-04  Data: 0.016 (0.014)
Train: 291 [ 700/1251 ( 56%)]  Loss: 3.469 (3.60)  Time: 0.726s, 1410.59/s  (0.695s, 1473.88/s)  LR: 5.283e-04  Data: 0.010 (0.013)
Train: 291 [ 750/1251 ( 60%)]  Loss: 3.539 (3.60)  Time: 0.666s, 1536.62/s  (0.694s, 1475.67/s)  LR: 5.283e-04  Data: 0.010 (0.013)
Train: 291 [ 800/1251 ( 64%)]  Loss: 3.605 (3.60)  Time: 0.698s, 1466.07/s  (0.694s, 1476.47/s)  LR: 5.283e-04  Data: 0.009 (0.013)
Train: 291 [ 850/1251 ( 68%)]  Loss: 3.708 (3.60)  Time: 0.680s, 1505.07/s  (0.693s, 1477.33/s)  LR: 5.283e-04  Data: 0.010 (0.013)
Train: 291 [ 900/1251 ( 72%)]  Loss: 3.590 (3.60)  Time: 0.700s, 1463.38/s  (0.693s, 1477.40/s)  LR: 5.283e-04  Data: 0.009 (0.013)
Train: 291 [ 950/1251 ( 76%)]  Loss: 3.274 (3.58)  Time: 0.724s, 1414.79/s  (0.693s, 1478.29/s)  LR: 5.283e-04  Data: 0.009 (0.013)
Train: 291 [1000/1251 ( 80%)]  Loss: 3.425 (3.58)  Time: 0.727s, 1408.90/s  (0.693s, 1478.36/s)  LR: 5.283e-04  Data: 0.011 (0.012)
Train: 291 [1050/1251 ( 84%)]  Loss: 3.924 (3.59)  Time: 0.670s, 1528.93/s  (0.693s, 1478.32/s)  LR: 5.283e-04  Data: 0.010 (0.012)
Train: 291 [1100/1251 ( 88%)]  Loss: 3.398 (3.58)  Time: 0.668s, 1532.50/s  (0.693s, 1478.48/s)  LR: 5.283e-04  Data: 0.010 (0.012)
Train: 291 [1150/1251 ( 92%)]  Loss: 3.837 (3.60)  Time: 0.665s, 1539.38/s  (0.692s, 1478.92/s)  LR: 5.283e-04  Data: 0.010 (0.012)
Train: 291 [1200/1251 ( 96%)]  Loss: 3.887 (3.61)  Time: 0.696s, 1471.48/s  (0.692s, 1479.54/s)  LR: 5.283e-04  Data: 0.010 (0.012)
Train: 291 [1250/1251 (100%)]  Loss: 3.322 (3.60)  Time: 0.659s, 1552.73/s  (0.692s, 1479.76/s)  LR: 5.283e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.518 (1.518)  Loss:  0.8506 (0.8506)  Acc@1: 88.0859 (88.0859)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.136 (0.589)  Loss:  0.8843 (1.4049)  Acc@1: 85.2594 (72.9860)  Acc@5: 95.7547 (91.4160)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-286.pth.tar', 73.4719999633789)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-277.pth.tar', 73.26600009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-284.pth.tar', 73.24199999023438)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-289.pth.tar', 73.23600006347657)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-281.pth.tar', 73.19199993652344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-257.pth.tar', 73.07200016601563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-269.pth.tar', 73.01599992919922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-291.pth.tar', 72.98600006103516)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-290.pth.tar', 72.9399998828125)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-274.pth.tar', 72.91199998046875)

Train: 292 [   0/1251 (  0%)]  Loss: 3.579 (3.58)  Time: 2.684s,  381.55/s  (2.684s,  381.55/s)  LR: 5.257e-04  Data: 2.047 (2.047)
Train: 292 [  50/1251 (  4%)]  Loss: 3.904 (3.74)  Time: 0.691s, 1482.51/s  (0.739s, 1385.56/s)  LR: 5.257e-04  Data: 0.011 (0.057)
Train: 292 [ 100/1251 (  8%)]  Loss: 3.819 (3.77)  Time: 0.671s, 1525.21/s  (0.721s, 1420.01/s)  LR: 5.257e-04  Data: 0.010 (0.034)
Train: 292 [ 150/1251 ( 12%)]  Loss: 3.783 (3.77)  Time: 0.676s, 1513.78/s  (0.713s, 1436.51/s)  LR: 5.257e-04  Data: 0.011 (0.026)
Train: 292 [ 200/1251 ( 16%)]  Loss: 3.399 (3.70)  Time: 0.665s, 1539.34/s  (0.708s, 1445.31/s)  LR: 5.257e-04  Data: 0.009 (0.022)
Train: 292 [ 250/1251 ( 20%)]  Loss: 3.529 (3.67)  Time: 0.725s, 1412.04/s  (0.705s, 1451.64/s)  LR: 5.257e-04  Data: 0.010 (0.020)
Train: 292 [ 300/1251 ( 24%)]  Loss: 3.307 (3.62)  Time: 0.692s, 1480.52/s  (0.704s, 1455.48/s)  LR: 5.257e-04  Data: 0.009 (0.018)
Train: 292 [ 350/1251 ( 28%)]  Loss: 3.369 (3.59)  Time: 0.677s, 1511.89/s  (0.702s, 1459.10/s)  LR: 5.257e-04  Data: 0.010 (0.017)
Train: 292 [ 400/1251 ( 32%)]  Loss: 4.093 (3.64)  Time: 0.673s, 1520.43/s  (0.700s, 1462.30/s)  LR: 5.257e-04  Data: 0.011 (0.016)
Train: 292 [ 450/1251 ( 36%)]  Loss: 3.382 (3.62)  Time: 0.761s, 1345.30/s  (0.700s, 1463.82/s)  LR: 5.257e-04  Data: 0.009 (0.016)
Train: 292 [ 500/1251 ( 40%)]  Loss: 3.644 (3.62)  Time: 0.689s, 1487.01/s  (0.699s, 1464.24/s)  LR: 5.257e-04  Data: 0.018 (0.015)
Train: 292 [ 550/1251 ( 44%)]  Loss: 4.002 (3.65)  Time: 0.673s, 1521.38/s  (0.699s, 1465.61/s)  LR: 5.257e-04  Data: 0.011 (0.015)
Train: 292 [ 600/1251 ( 48%)]  Loss: 3.600 (3.65)  Time: 0.718s, 1425.38/s  (0.699s, 1465.52/s)  LR: 5.257e-04  Data: 0.009 (0.014)
Train: 292 [ 650/1251 ( 52%)]  Loss: 3.945 (3.67)  Time: 0.676s, 1515.28/s  (0.699s, 1465.87/s)  LR: 5.257e-04  Data: 0.009 (0.014)
Train: 292 [ 700/1251 ( 56%)]  Loss: 3.215 (3.64)  Time: 0.686s, 1493.33/s  (0.698s, 1467.40/s)  LR: 5.257e-04  Data: 0.009 (0.014)
Train: 292 [ 750/1251 ( 60%)]  Loss: 3.558 (3.63)  Time: 0.678s, 1511.40/s  (0.697s, 1468.76/s)  LR: 5.257e-04  Data: 0.010 (0.014)
Train: 292 [ 800/1251 ( 64%)]  Loss: 3.479 (3.62)  Time: 0.676s, 1515.73/s  (0.697s, 1469.86/s)  LR: 5.257e-04  Data: 0.014 (0.013)
Train: 292 [ 850/1251 ( 68%)]  Loss: 3.909 (3.64)  Time: 0.708s, 1446.05/s  (0.697s, 1469.71/s)  LR: 5.257e-04  Data: 0.010 (0.013)
Train: 292 [ 900/1251 ( 72%)]  Loss: 4.121 (3.67)  Time: 0.730s, 1403.45/s  (0.697s, 1469.70/s)  LR: 5.257e-04  Data: 0.011 (0.013)
Train: 292 [ 950/1251 ( 76%)]  Loss: 3.436 (3.65)  Time: 0.709s, 1444.72/s  (0.696s, 1470.45/s)  LR: 5.257e-04  Data: 0.010 (0.013)
Train: 292 [1000/1251 ( 80%)]  Loss: 3.857 (3.66)  Time: 0.702s, 1458.68/s  (0.697s, 1469.98/s)  LR: 5.257e-04  Data: 0.009 (0.013)
Train: 292 [1050/1251 ( 84%)]  Loss: 3.643 (3.66)  Time: 0.669s, 1530.87/s  (0.696s, 1470.39/s)  LR: 5.257e-04  Data: 0.009 (0.013)
Train: 292 [1100/1251 ( 88%)]  Loss: 3.548 (3.66)  Time: 0.679s, 1508.86/s  (0.697s, 1470.19/s)  LR: 5.257e-04  Data: 0.011 (0.013)
Train: 292 [1150/1251 ( 92%)]  Loss: 3.675 (3.66)  Time: 0.697s, 1469.63/s  (0.696s, 1471.07/s)  LR: 5.257e-04  Data: 0.012 (0.013)
Train: 292 [1200/1251 ( 96%)]  Loss: 3.893 (3.67)  Time: 0.750s, 1365.03/s  (0.696s, 1471.08/s)  LR: 5.257e-04  Data: 0.009 (0.012)
Train: 292 [1250/1251 (100%)]  Loss: 3.689 (3.67)  Time: 0.662s, 1547.76/s  (0.696s, 1471.24/s)  LR: 5.257e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.530 (1.530)  Loss:  0.7822 (0.7822)  Acc@1: 88.5742 (88.5742)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.136 (0.586)  Loss:  0.9512 (1.3750)  Acc@1: 83.1368 (73.8120)  Acc@5: 95.4009 (91.7100)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-292.pth.tar', 73.81200004394532)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-286.pth.tar', 73.4719999633789)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-277.pth.tar', 73.26600009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-284.pth.tar', 73.24199999023438)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-289.pth.tar', 73.23600006347657)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-281.pth.tar', 73.19199993652344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-257.pth.tar', 73.07200016601563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-269.pth.tar', 73.01599992919922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-291.pth.tar', 72.98600006103516)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-290.pth.tar', 72.9399998828125)

Train: 293 [   0/1251 (  0%)]  Loss: 3.505 (3.50)  Time: 2.268s,  451.59/s  (2.268s,  451.59/s)  LR: 5.231e-04  Data: 1.614 (1.614)
Train: 293 [  50/1251 (  4%)]  Loss: 3.790 (3.65)  Time: 0.671s, 1526.80/s  (0.725s, 1411.56/s)  LR: 5.231e-04  Data: 0.010 (0.047)
Train: 293 [ 100/1251 (  8%)]  Loss: 3.551 (3.62)  Time: 0.670s, 1528.16/s  (0.714s, 1435.15/s)  LR: 5.231e-04  Data: 0.010 (0.029)
Train: 293 [ 150/1251 ( 12%)]  Loss: 3.219 (3.52)  Time: 0.667s, 1534.32/s  (0.705s, 1453.13/s)  LR: 5.231e-04  Data: 0.009 (0.023)
Train: 293 [ 200/1251 ( 16%)]  Loss: 3.820 (3.58)  Time: 0.681s, 1504.30/s  (0.700s, 1461.92/s)  LR: 5.231e-04  Data: 0.009 (0.020)
Train: 293 [ 250/1251 ( 20%)]  Loss: 3.669 (3.59)  Time: 0.739s, 1385.16/s  (0.698s, 1467.32/s)  LR: 5.231e-04  Data: 0.011 (0.018)
Train: 293 [ 300/1251 ( 24%)]  Loss: 3.874 (3.63)  Time: 0.671s, 1525.87/s  (0.697s, 1468.75/s)  LR: 5.231e-04  Data: 0.010 (0.016)
Train: 293 [ 350/1251 ( 28%)]  Loss: 3.446 (3.61)  Time: 0.721s, 1420.97/s  (0.697s, 1470.04/s)  LR: 5.231e-04  Data: 0.015 (0.016)
Train: 293 [ 400/1251 ( 32%)]  Loss: 3.547 (3.60)  Time: 0.683s, 1499.76/s  (0.696s, 1472.30/s)  LR: 5.231e-04  Data: 0.010 (0.015)
Train: 293 [ 450/1251 ( 36%)]  Loss: 3.849 (3.63)  Time: 0.672s, 1524.66/s  (0.696s, 1471.50/s)  LR: 5.231e-04  Data: 0.010 (0.015)
Train: 293 [ 500/1251 ( 40%)]  Loss: 3.691 (3.63)  Time: 0.672s, 1524.84/s  (0.695s, 1473.15/s)  LR: 5.231e-04  Data: 0.010 (0.014)
Train: 293 [ 550/1251 ( 44%)]  Loss: 3.677 (3.64)  Time: 0.701s, 1460.89/s  (0.695s, 1474.11/s)  LR: 5.231e-04  Data: 0.010 (0.014)
Train: 293 [ 600/1251 ( 48%)]  Loss: 3.433 (3.62)  Time: 0.766s, 1337.67/s  (0.694s, 1475.08/s)  LR: 5.231e-04  Data: 0.009 (0.013)
Train: 293 [ 650/1251 ( 52%)]  Loss: 3.752 (3.63)  Time: 0.676s, 1515.44/s  (0.694s, 1475.78/s)  LR: 5.231e-04  Data: 0.011 (0.013)
Train: 293 [ 700/1251 ( 56%)]  Loss: 3.369 (3.61)  Time: 0.672s, 1523.19/s  (0.693s, 1477.21/s)  LR: 5.231e-04  Data: 0.010 (0.013)
Train: 293 [ 750/1251 ( 60%)]  Loss: 3.737 (3.62)  Time: 0.702s, 1459.58/s  (0.693s, 1478.33/s)  LR: 5.231e-04  Data: 0.009 (0.013)
Train: 293 [ 800/1251 ( 64%)]  Loss: 3.393 (3.61)  Time: 0.706s, 1450.80/s  (0.693s, 1477.73/s)  LR: 5.231e-04  Data: 0.011 (0.013)
Train: 293 [ 850/1251 ( 68%)]  Loss: 3.468 (3.60)  Time: 0.673s, 1522.56/s  (0.693s, 1477.15/s)  LR: 5.231e-04  Data: 0.010 (0.013)
Train: 293 [ 900/1251 ( 72%)]  Loss: 3.563 (3.60)  Time: 0.684s, 1497.98/s  (0.693s, 1477.40/s)  LR: 5.231e-04  Data: 0.009 (0.012)
Train: 293 [ 950/1251 ( 76%)]  Loss: 3.747 (3.61)  Time: 0.701s, 1460.67/s  (0.693s, 1477.89/s)  LR: 5.231e-04  Data: 0.009 (0.012)
Train: 293 [1000/1251 ( 80%)]  Loss: 3.547 (3.60)  Time: 0.674s, 1518.92/s  (0.693s, 1478.67/s)  LR: 5.231e-04  Data: 0.010 (0.012)
Train: 293 [1050/1251 ( 84%)]  Loss: 3.608 (3.60)  Time: 0.667s, 1534.81/s  (0.692s, 1478.77/s)  LR: 5.231e-04  Data: 0.010 (0.012)
Train: 293 [1100/1251 ( 88%)]  Loss: 3.900 (3.62)  Time: 0.673s, 1522.05/s  (0.692s, 1479.21/s)  LR: 5.231e-04  Data: 0.009 (0.012)
Train: 293 [1150/1251 ( 92%)]  Loss: 3.799 (3.62)  Time: 0.672s, 1523.38/s  (0.693s, 1478.46/s)  LR: 5.231e-04  Data: 0.009 (0.012)
Train: 293 [1200/1251 ( 96%)]  Loss: 3.889 (3.63)  Time: 0.672s, 1523.15/s  (0.693s, 1478.52/s)  LR: 5.231e-04  Data: 0.014 (0.012)
Train: 293 [1250/1251 (100%)]  Loss: 3.783 (3.64)  Time: 0.680s, 1506.30/s  (0.693s, 1478.20/s)  LR: 5.231e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.569 (1.569)  Loss:  0.7900 (0.7900)  Acc@1: 89.7461 (89.7461)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.137 (0.586)  Loss:  1.0449 (1.4454)  Acc@1: 83.6085 (73.5220)  Acc@5: 95.4009 (91.7140)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-292.pth.tar', 73.81200004394532)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-293.pth.tar', 73.52199999023438)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-286.pth.tar', 73.4719999633789)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-277.pth.tar', 73.26600009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-284.pth.tar', 73.24199999023438)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-289.pth.tar', 73.23600006347657)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-281.pth.tar', 73.19199993652344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-257.pth.tar', 73.07200016601563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-269.pth.tar', 73.01599992919922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-291.pth.tar', 72.98600006103516)

Train: 294 [   0/1251 (  0%)]  Loss: 3.671 (3.67)  Time: 2.042s,  501.55/s  (2.042s,  501.55/s)  LR: 5.205e-04  Data: 1.424 (1.424)
Train: 294 [  50/1251 (  4%)]  Loss: 3.570 (3.62)  Time: 0.723s, 1416.36/s  (0.731s, 1401.49/s)  LR: 5.205e-04  Data: 0.011 (0.050)
Train: 294 [ 100/1251 (  8%)]  Loss: 3.546 (3.60)  Time: 0.722s, 1418.49/s  (0.711s, 1440.08/s)  LR: 5.205e-04  Data: 0.011 (0.030)
Train: 294 [ 150/1251 ( 12%)]  Loss: 3.780 (3.64)  Time: 0.707s, 1449.08/s  (0.704s, 1454.07/s)  LR: 5.205e-04  Data: 0.010 (0.023)
Train: 294 [ 200/1251 ( 16%)]  Loss: 3.741 (3.66)  Time: 0.705s, 1451.77/s  (0.702s, 1458.55/s)  LR: 5.205e-04  Data: 0.010 (0.020)
Train: 294 [ 250/1251 ( 20%)]  Loss: 3.526 (3.64)  Time: 0.730s, 1402.48/s  (0.701s, 1460.95/s)  LR: 5.205e-04  Data: 0.009 (0.018)
Train: 294 [ 300/1251 ( 24%)]  Loss: 3.340 (3.60)  Time: 0.715s, 1433.06/s  (0.701s, 1461.16/s)  LR: 5.205e-04  Data: 0.010 (0.017)
Train: 294 [ 350/1251 ( 28%)]  Loss: 3.743 (3.61)  Time: 0.708s, 1446.26/s  (0.700s, 1463.28/s)  LR: 5.205e-04  Data: 0.011 (0.016)
Train: 294 [ 400/1251 ( 32%)]  Loss: 3.538 (3.61)  Time: 0.701s, 1459.92/s  (0.698s, 1466.38/s)  LR: 5.205e-04  Data: 0.012 (0.015)
Train: 294 [ 450/1251 ( 36%)]  Loss: 3.731 (3.62)  Time: 0.702s, 1459.71/s  (0.698s, 1467.73/s)  LR: 5.205e-04  Data: 0.009 (0.015)
Train: 294 [ 500/1251 ( 40%)]  Loss: 4.090 (3.66)  Time: 0.715s, 1431.26/s  (0.697s, 1468.48/s)  LR: 5.205e-04  Data: 0.010 (0.014)
Train: 294 [ 550/1251 ( 44%)]  Loss: 3.847 (3.68)  Time: 0.687s, 1490.24/s  (0.697s, 1468.28/s)  LR: 5.205e-04  Data: 0.009 (0.014)
Train: 294 [ 600/1251 ( 48%)]  Loss: 3.563 (3.67)  Time: 0.714s, 1434.20/s  (0.698s, 1467.65/s)  LR: 5.205e-04  Data: 0.011 (0.014)
Train: 294 [ 650/1251 ( 52%)]  Loss: 4.133 (3.70)  Time: 0.677s, 1513.32/s  (0.698s, 1467.29/s)  LR: 5.205e-04  Data: 0.010 (0.014)
Train: 294 [ 700/1251 ( 56%)]  Loss: 3.853 (3.71)  Time: 0.706s, 1450.19/s  (0.697s, 1468.83/s)  LR: 5.205e-04  Data: 0.011 (0.013)
Train: 294 [ 750/1251 ( 60%)]  Loss: 3.918 (3.72)  Time: 0.672s, 1522.71/s  (0.696s, 1470.40/s)  LR: 5.205e-04  Data: 0.011 (0.013)
Train: 294 [ 800/1251 ( 64%)]  Loss: 3.537 (3.71)  Time: 0.707s, 1447.65/s  (0.696s, 1470.35/s)  LR: 5.205e-04  Data: 0.011 (0.013)
Train: 294 [ 850/1251 ( 68%)]  Loss: 3.536 (3.70)  Time: 0.699s, 1465.78/s  (0.697s, 1469.07/s)  LR: 5.205e-04  Data: 0.012 (0.013)
Train: 294 [ 900/1251 ( 72%)]  Loss: 3.824 (3.71)  Time: 0.685s, 1495.65/s  (0.698s, 1467.81/s)  LR: 5.205e-04  Data: 0.010 (0.013)
Train: 294 [ 950/1251 ( 76%)]  Loss: 3.427 (3.70)  Time: 0.725s, 1412.05/s  (0.699s, 1465.83/s)  LR: 5.205e-04  Data: 0.011 (0.013)
Train: 294 [1000/1251 ( 80%)]  Loss: 3.567 (3.69)  Time: 0.678s, 1509.37/s  (0.699s, 1465.20/s)  LR: 5.205e-04  Data: 0.011 (0.013)
Train: 294 [1050/1251 ( 84%)]  Loss: 3.751 (3.69)  Time: 0.672s, 1523.83/s  (0.698s, 1466.70/s)  LR: 5.205e-04  Data: 0.011 (0.013)
Train: 294 [1100/1251 ( 88%)]  Loss: 2.985 (3.66)  Time: 0.672s, 1524.37/s  (0.697s, 1469.10/s)  LR: 5.205e-04  Data: 0.011 (0.013)
Train: 294 [1150/1251 ( 92%)]  Loss: 3.484 (3.65)  Time: 0.676s, 1514.49/s  (0.696s, 1471.17/s)  LR: 5.205e-04  Data: 0.011 (0.012)
Train: 294 [1200/1251 ( 96%)]  Loss: 3.832 (3.66)  Time: 0.669s, 1530.40/s  (0.696s, 1471.78/s)  LR: 5.205e-04  Data: 0.010 (0.012)
Train: 294 [1250/1251 (100%)]  Loss: 3.769 (3.67)  Time: 0.657s, 1558.66/s  (0.696s, 1472.16/s)  LR: 5.205e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.560 (1.560)  Loss:  1.0010 (1.0010)  Acc@1: 87.9883 (87.9883)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.136 (0.586)  Loss:  1.0527 (1.5492)  Acc@1: 83.6085 (73.0340)  Acc@5: 95.7547 (91.2600)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-292.pth.tar', 73.81200004394532)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-293.pth.tar', 73.52199999023438)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-286.pth.tar', 73.4719999633789)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-277.pth.tar', 73.26600009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-284.pth.tar', 73.24199999023438)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-289.pth.tar', 73.23600006347657)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-281.pth.tar', 73.19199993652344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-257.pth.tar', 73.07200016601563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-294.pth.tar', 73.03399999023438)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-269.pth.tar', 73.01599992919922)

Train: 295 [   0/1251 (  0%)]  Loss: 3.822 (3.82)  Time: 2.111s,  485.06/s  (2.111s,  485.06/s)  LR: 5.180e-04  Data: 1.497 (1.497)
Train: 295 [  50/1251 (  4%)]  Loss: 3.742 (3.78)  Time: 0.674s, 1519.60/s  (0.721s, 1419.80/s)  LR: 5.180e-04  Data: 0.010 (0.049)
Train: 295 [ 100/1251 (  8%)]  Loss: 3.694 (3.75)  Time: 0.725s, 1411.67/s  (0.711s, 1440.10/s)  LR: 5.180e-04  Data: 0.009 (0.030)
Train: 295 [ 150/1251 ( 12%)]  Loss: 3.822 (3.77)  Time: 0.756s, 1355.34/s  (0.703s, 1456.11/s)  LR: 5.180e-04  Data: 0.010 (0.024)
Train: 295 [ 200/1251 ( 16%)]  Loss: 3.606 (3.74)  Time: 0.675s, 1517.39/s  (0.701s, 1460.44/s)  LR: 5.180e-04  Data: 0.010 (0.020)
Train: 295 [ 250/1251 ( 20%)]  Loss: 3.687 (3.73)  Time: 0.749s, 1367.84/s  (0.699s, 1464.64/s)  LR: 5.180e-04  Data: 0.009 (0.018)
Train: 295 [ 300/1251 ( 24%)]  Loss: 3.758 (3.73)  Time: 0.708s, 1446.41/s  (0.698s, 1467.81/s)  LR: 5.180e-04  Data: 0.011 (0.017)
Train: 295 [ 350/1251 ( 28%)]  Loss: 3.330 (3.68)  Time: 0.720s, 1421.59/s  (0.696s, 1470.65/s)  LR: 5.180e-04  Data: 0.009 (0.016)
Train: 295 [ 400/1251 ( 32%)]  Loss: 3.408 (3.65)  Time: 0.711s, 1440.48/s  (0.696s, 1471.58/s)  LR: 5.180e-04  Data: 0.010 (0.015)
Train: 295 [ 450/1251 ( 36%)]  Loss: 3.557 (3.64)  Time: 0.781s, 1311.78/s  (0.695s, 1472.93/s)  LR: 5.180e-04  Data: 0.009 (0.015)
Train: 295 [ 500/1251 ( 40%)]  Loss: 3.302 (3.61)  Time: 0.673s, 1521.02/s  (0.694s, 1474.52/s)  LR: 5.180e-04  Data: 0.010 (0.014)
Train: 295 [ 550/1251 ( 44%)]  Loss: 3.488 (3.60)  Time: 0.672s, 1524.11/s  (0.695s, 1473.02/s)  LR: 5.180e-04  Data: 0.012 (0.014)
Train: 295 [ 600/1251 ( 48%)]  Loss: 3.744 (3.61)  Time: 0.706s, 1451.38/s  (0.695s, 1473.84/s)  LR: 5.180e-04  Data: 0.009 (0.014)
Train: 295 [ 650/1251 ( 52%)]  Loss: 3.676 (3.62)  Time: 0.692s, 1480.45/s  (0.695s, 1473.93/s)  LR: 5.180e-04  Data: 0.010 (0.014)
Train: 295 [ 700/1251 ( 56%)]  Loss: 3.596 (3.62)  Time: 0.693s, 1477.66/s  (0.695s, 1474.12/s)  LR: 5.180e-04  Data: 0.009 (0.013)
Train: 295 [ 750/1251 ( 60%)]  Loss: 3.737 (3.62)  Time: 0.727s, 1408.72/s  (0.694s, 1474.93/s)  LR: 5.180e-04  Data: 0.010 (0.013)
Train: 295 [ 800/1251 ( 64%)]  Loss: 3.771 (3.63)  Time: 0.670s, 1527.43/s  (0.694s, 1475.21/s)  LR: 5.180e-04  Data: 0.010 (0.013)
Train: 295 [ 850/1251 ( 68%)]  Loss: 4.053 (3.66)  Time: 0.677s, 1511.63/s  (0.694s, 1475.88/s)  LR: 5.180e-04  Data: 0.010 (0.013)
Train: 295 [ 900/1251 ( 72%)]  Loss: 3.834 (3.66)  Time: 0.671s, 1525.37/s  (0.694s, 1475.54/s)  LR: 5.180e-04  Data: 0.010 (0.013)
Train: 295 [ 950/1251 ( 76%)]  Loss: 3.659 (3.66)  Time: 0.684s, 1496.26/s  (0.694s, 1475.85/s)  LR: 5.180e-04  Data: 0.010 (0.013)
Train: 295 [1000/1251 ( 80%)]  Loss: 3.661 (3.66)  Time: 0.741s, 1381.55/s  (0.694s, 1475.82/s)  LR: 5.180e-04  Data: 0.015 (0.012)
Train: 295 [1050/1251 ( 84%)]  Loss: 3.786 (3.67)  Time: 0.670s, 1527.62/s  (0.694s, 1476.05/s)  LR: 5.180e-04  Data: 0.010 (0.012)
Train: 295 [1100/1251 ( 88%)]  Loss: 3.613 (3.67)  Time: 0.672s, 1522.84/s  (0.694s, 1476.24/s)  LR: 5.180e-04  Data: 0.010 (0.012)
Train: 295 [1150/1251 ( 92%)]  Loss: 4.047 (3.68)  Time: 0.671s, 1526.91/s  (0.694s, 1476.37/s)  LR: 5.180e-04  Data: 0.010 (0.012)
Train: 295 [1200/1251 ( 96%)]  Loss: 3.784 (3.69)  Time: 0.689s, 1485.14/s  (0.694s, 1476.19/s)  LR: 5.180e-04  Data: 0.009 (0.012)
Train: 295 [1250/1251 (100%)]  Loss: 3.397 (3.68)  Time: 0.656s, 1560.63/s  (0.693s, 1476.86/s)  LR: 5.180e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.589 (1.589)  Loss:  0.7388 (0.7388)  Acc@1: 89.3555 (89.3555)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.136 (0.589)  Loss:  0.8770 (1.3879)  Acc@1: 84.4340 (73.2880)  Acc@5: 95.7547 (91.5300)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-292.pth.tar', 73.81200004394532)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-293.pth.tar', 73.52199999023438)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-286.pth.tar', 73.4719999633789)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-295.pth.tar', 73.28800009033203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-277.pth.tar', 73.26600009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-284.pth.tar', 73.24199999023438)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-289.pth.tar', 73.23600006347657)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-281.pth.tar', 73.19199993652344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-257.pth.tar', 73.07200016601563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-294.pth.tar', 73.03399999023438)

Train: 296 [   0/1251 (  0%)]  Loss: 3.623 (3.62)  Time: 2.224s,  460.46/s  (2.224s,  460.46/s)  LR: 5.154e-04  Data: 1.608 (1.608)
Train: 296 [  50/1251 (  4%)]  Loss: 3.599 (3.61)  Time: 0.676s, 1515.54/s  (0.732s, 1399.41/s)  LR: 5.154e-04  Data: 0.010 (0.052)
Train: 296 [ 100/1251 (  8%)]  Loss: 3.753 (3.66)  Time: 0.709s, 1443.45/s  (0.712s, 1438.16/s)  LR: 5.154e-04  Data: 0.011 (0.032)
Train: 296 [ 150/1251 ( 12%)]  Loss: 3.592 (3.64)  Time: 0.716s, 1429.50/s  (0.707s, 1447.57/s)  LR: 5.154e-04  Data: 0.010 (0.025)
Train: 296 [ 200/1251 ( 16%)]  Loss: 3.679 (3.65)  Time: 0.673s, 1522.30/s  (0.702s, 1458.57/s)  LR: 5.154e-04  Data: 0.010 (0.021)
Train: 296 [ 250/1251 ( 20%)]  Loss: 3.939 (3.70)  Time: 0.675s, 1516.96/s  (0.700s, 1463.83/s)  LR: 5.154e-04  Data: 0.011 (0.019)
Train: 296 [ 300/1251 ( 24%)]  Loss: 3.179 (3.62)  Time: 0.670s, 1527.31/s  (0.698s, 1467.99/s)  LR: 5.154e-04  Data: 0.011 (0.018)
Train: 296 [ 350/1251 ( 28%)]  Loss: 3.900 (3.66)  Time: 0.668s, 1532.48/s  (0.696s, 1470.91/s)  LR: 5.154e-04  Data: 0.009 (0.017)
Train: 296 [ 400/1251 ( 32%)]  Loss: 3.641 (3.66)  Time: 0.676s, 1515.03/s  (0.696s, 1471.76/s)  LR: 5.154e-04  Data: 0.010 (0.016)
Train: 296 [ 450/1251 ( 36%)]  Loss: 3.532 (3.64)  Time: 0.675s, 1517.56/s  (0.695s, 1472.97/s)  LR: 5.154e-04  Data: 0.010 (0.015)
Train: 296 [ 500/1251 ( 40%)]  Loss: 3.384 (3.62)  Time: 0.666s, 1537.43/s  (0.695s, 1473.60/s)  LR: 5.154e-04  Data: 0.010 (0.015)
Train: 296 [ 550/1251 ( 44%)]  Loss: 3.849 (3.64)  Time: 0.678s, 1509.62/s  (0.694s, 1475.22/s)  LR: 5.154e-04  Data: 0.011 (0.014)
Train: 296 [ 600/1251 ( 48%)]  Loss: 3.608 (3.64)  Time: 0.673s, 1520.43/s  (0.694s, 1475.97/s)  LR: 5.154e-04  Data: 0.011 (0.014)
Train: 296 [ 650/1251 ( 52%)]  Loss: 3.691 (3.64)  Time: 0.702s, 1458.39/s  (0.694s, 1475.13/s)  LR: 5.154e-04  Data: 0.013 (0.014)
Train: 296 [ 700/1251 ( 56%)]  Loss: 3.656 (3.64)  Time: 0.674s, 1519.99/s  (0.694s, 1475.81/s)  LR: 5.154e-04  Data: 0.011 (0.014)
Train: 296 [ 750/1251 ( 60%)]  Loss: 3.661 (3.64)  Time: 0.676s, 1513.93/s  (0.694s, 1476.34/s)  LR: 5.154e-04  Data: 0.011 (0.013)
Train: 296 [ 800/1251 ( 64%)]  Loss: 3.491 (3.63)  Time: 0.673s, 1522.37/s  (0.693s, 1477.37/s)  LR: 5.154e-04  Data: 0.010 (0.013)
Train: 296 [ 850/1251 ( 68%)]  Loss: 3.765 (3.64)  Time: 0.673s, 1520.72/s  (0.693s, 1477.67/s)  LR: 5.154e-04  Data: 0.011 (0.013)
Train: 296 [ 900/1251 ( 72%)]  Loss: 3.923 (3.66)  Time: 0.672s, 1524.09/s  (0.693s, 1478.02/s)  LR: 5.154e-04  Data: 0.011 (0.013)
Train: 296 [ 950/1251 ( 76%)]  Loss: 3.822 (3.66)  Time: 0.673s, 1522.22/s  (0.693s, 1478.08/s)  LR: 5.154e-04  Data: 0.011 (0.013)
Train: 296 [1000/1251 ( 80%)]  Loss: 3.918 (3.68)  Time: 0.711s, 1439.74/s  (0.693s, 1478.21/s)  LR: 5.154e-04  Data: 0.010 (0.013)
Train: 296 [1050/1251 ( 84%)]  Loss: 3.720 (3.68)  Time: 0.676s, 1515.14/s  (0.693s, 1478.00/s)  LR: 5.154e-04  Data: 0.010 (0.013)
Train: 296 [1100/1251 ( 88%)]  Loss: 3.564 (3.67)  Time: 0.675s, 1515.95/s  (0.693s, 1478.65/s)  LR: 5.154e-04  Data: 0.011 (0.013)
Train: 296 [1150/1251 ( 92%)]  Loss: 3.382 (3.66)  Time: 0.706s, 1449.91/s  (0.692s, 1478.76/s)  LR: 5.154e-04  Data: 0.011 (0.012)
Train: 296 [1200/1251 ( 96%)]  Loss: 3.948 (3.67)  Time: 0.669s, 1531.41/s  (0.692s, 1479.00/s)  LR: 5.154e-04  Data: 0.011 (0.012)
Train: 296 [1250/1251 (100%)]  Loss: 3.545 (3.67)  Time: 0.660s, 1550.71/s  (0.692s, 1479.53/s)  LR: 5.154e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.506 (1.506)  Loss:  0.7515 (0.7515)  Acc@1: 88.5742 (88.5742)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.136 (0.594)  Loss:  0.9028 (1.3403)  Acc@1: 83.7264 (73.6020)  Acc@5: 95.4009 (91.6520)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-292.pth.tar', 73.81200004394532)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-296.pth.tar', 73.6020000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-293.pth.tar', 73.52199999023438)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-286.pth.tar', 73.4719999633789)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-295.pth.tar', 73.28800009033203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-277.pth.tar', 73.26600009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-284.pth.tar', 73.24199999023438)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-289.pth.tar', 73.23600006347657)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-281.pth.tar', 73.19199993652344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-257.pth.tar', 73.07200016601563)

Train: 297 [   0/1251 (  0%)]  Loss: 3.536 (3.54)  Time: 2.196s,  466.30/s  (2.196s,  466.30/s)  LR: 5.128e-04  Data: 1.570 (1.570)
Train: 297 [  50/1251 (  4%)]  Loss: 3.362 (3.45)  Time: 0.671s, 1525.50/s  (0.722s, 1418.25/s)  LR: 5.128e-04  Data: 0.010 (0.047)
Train: 297 [ 100/1251 (  8%)]  Loss: 3.233 (3.38)  Time: 0.720s, 1421.91/s  (0.709s, 1444.86/s)  LR: 5.128e-04  Data: 0.009 (0.029)
Train: 297 [ 150/1251 ( 12%)]  Loss: 3.581 (3.43)  Time: 0.671s, 1526.81/s  (0.702s, 1458.37/s)  LR: 5.128e-04  Data: 0.011 (0.023)
Train: 297 [ 200/1251 ( 16%)]  Loss: 3.740 (3.49)  Time: 0.713s, 1435.40/s  (0.701s, 1461.04/s)  LR: 5.128e-04  Data: 0.009 (0.020)
Train: 297 [ 250/1251 ( 20%)]  Loss: 3.563 (3.50)  Time: 0.724s, 1414.83/s  (0.698s, 1466.46/s)  LR: 5.128e-04  Data: 0.009 (0.018)
Train: 297 [ 300/1251 ( 24%)]  Loss: 3.646 (3.52)  Time: 0.670s, 1527.74/s  (0.699s, 1464.99/s)  LR: 5.128e-04  Data: 0.010 (0.017)
Train: 297 [ 350/1251 ( 28%)]  Loss: 3.756 (3.55)  Time: 0.728s, 1406.53/s  (0.697s, 1469.24/s)  LR: 5.128e-04  Data: 0.023 (0.016)
Train: 297 [ 400/1251 ( 32%)]  Loss: 3.657 (3.56)  Time: 0.705s, 1453.03/s  (0.696s, 1471.08/s)  LR: 5.128e-04  Data: 0.014 (0.015)
Train: 297 [ 450/1251 ( 36%)]  Loss: 3.680 (3.58)  Time: 0.671s, 1525.83/s  (0.695s, 1472.43/s)  LR: 5.128e-04  Data: 0.011 (0.015)
Train: 297 [ 500/1251 ( 40%)]  Loss: 3.156 (3.54)  Time: 0.734s, 1394.24/s  (0.694s, 1474.97/s)  LR: 5.128e-04  Data: 0.009 (0.014)
Train: 297 [ 550/1251 ( 44%)]  Loss: 3.384 (3.52)  Time: 0.708s, 1447.23/s  (0.694s, 1474.54/s)  LR: 5.128e-04  Data: 0.011 (0.014)
Train: 297 [ 600/1251 ( 48%)]  Loss: 3.510 (3.52)  Time: 0.666s, 1538.24/s  (0.694s, 1475.04/s)  LR: 5.128e-04  Data: 0.010 (0.013)
Train: 297 [ 650/1251 ( 52%)]  Loss: 3.721 (3.54)  Time: 0.667s, 1535.08/s  (0.695s, 1473.70/s)  LR: 5.128e-04  Data: 0.009 (0.013)
Train: 297 [ 700/1251 ( 56%)]  Loss: 3.932 (3.56)  Time: 0.683s, 1500.19/s  (0.695s, 1472.77/s)  LR: 5.128e-04  Data: 0.015 (0.013)
Train: 297 [ 750/1251 ( 60%)]  Loss: 3.509 (3.56)  Time: 0.673s, 1521.69/s  (0.695s, 1472.37/s)  LR: 5.128e-04  Data: 0.010 (0.013)
Train: 297 [ 800/1251 ( 64%)]  Loss: 3.974 (3.58)  Time: 0.710s, 1441.33/s  (0.695s, 1473.16/s)  LR: 5.128e-04  Data: 0.009 (0.013)
Train: 297 [ 850/1251 ( 68%)]  Loss: 3.880 (3.60)  Time: 0.699s, 1464.23/s  (0.695s, 1472.67/s)  LR: 5.128e-04  Data: 0.010 (0.013)
Train: 297 [ 900/1251 ( 72%)]  Loss: 3.515 (3.60)  Time: 0.706s, 1450.88/s  (0.695s, 1472.76/s)  LR: 5.128e-04  Data: 0.012 (0.013)
Train: 297 [ 950/1251 ( 76%)]  Loss: 3.566 (3.60)  Time: 0.718s, 1426.75/s  (0.695s, 1473.38/s)  LR: 5.128e-04  Data: 0.010 (0.012)
Train: 297 [1000/1251 ( 80%)]  Loss: 3.905 (3.61)  Time: 0.672s, 1524.50/s  (0.695s, 1473.52/s)  LR: 5.128e-04  Data: 0.011 (0.012)
Train: 297 [1050/1251 ( 84%)]  Loss: 3.867 (3.62)  Time: 0.711s, 1440.30/s  (0.695s, 1472.95/s)  LR: 5.128e-04  Data: 0.011 (0.012)
Train: 297 [1100/1251 ( 88%)]  Loss: 3.827 (3.63)  Time: 0.672s, 1524.85/s  (0.695s, 1473.76/s)  LR: 5.128e-04  Data: 0.009 (0.012)
Train: 297 [1150/1251 ( 92%)]  Loss: 3.737 (3.63)  Time: 0.677s, 1511.70/s  (0.695s, 1473.63/s)  LR: 5.128e-04  Data: 0.009 (0.012)
Train: 297 [1200/1251 ( 96%)]  Loss: 3.582 (3.63)  Time: 0.690s, 1483.84/s  (0.695s, 1473.12/s)  LR: 5.128e-04  Data: 0.009 (0.012)
Train: 297 [1250/1251 (100%)]  Loss: 3.461 (3.63)  Time: 0.657s, 1557.97/s  (0.695s, 1472.96/s)  LR: 5.128e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.636 (1.636)  Loss:  0.9336 (0.9336)  Acc@1: 88.5742 (88.5742)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.136 (0.585)  Loss:  0.8887 (1.4077)  Acc@1: 84.7877 (73.4760)  Acc@5: 95.6368 (91.6980)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-292.pth.tar', 73.81200004394532)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-296.pth.tar', 73.6020000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-293.pth.tar', 73.52199999023438)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-297.pth.tar', 73.4760001147461)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-286.pth.tar', 73.4719999633789)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-295.pth.tar', 73.28800009033203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-277.pth.tar', 73.26600009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-284.pth.tar', 73.24199999023438)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-289.pth.tar', 73.23600006347657)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-281.pth.tar', 73.19199993652344)

Train: 298 [   0/1251 (  0%)]  Loss: 3.657 (3.66)  Time: 2.100s,  487.65/s  (2.100s,  487.65/s)  LR: 5.102e-04  Data: 1.486 (1.486)
Train: 298 [  50/1251 (  4%)]  Loss: 3.714 (3.69)  Time: 0.683s, 1500.10/s  (0.724s, 1413.44/s)  LR: 5.102e-04  Data: 0.011 (0.048)
Train: 298 [ 100/1251 (  8%)]  Loss: 3.457 (3.61)  Time: 0.721s, 1419.35/s  (0.711s, 1439.63/s)  LR: 5.102e-04  Data: 0.012 (0.029)
Train: 298 [ 150/1251 ( 12%)]  Loss: 3.742 (3.64)  Time: 0.681s, 1503.58/s  (0.706s, 1450.61/s)  LR: 5.102e-04  Data: 0.012 (0.023)
Train: 298 [ 200/1251 ( 16%)]  Loss: 3.783 (3.67)  Time: 0.668s, 1532.40/s  (0.704s, 1455.37/s)  LR: 5.102e-04  Data: 0.009 (0.020)
Train: 298 [ 250/1251 ( 20%)]  Loss: 3.349 (3.62)  Time: 0.683s, 1499.42/s  (0.702s, 1459.31/s)  LR: 5.102e-04  Data: 0.010 (0.018)
Train: 298 [ 300/1251 ( 24%)]  Loss: 3.825 (3.65)  Time: 0.674s, 1520.41/s  (0.700s, 1463.33/s)  LR: 5.102e-04  Data: 0.010 (0.017)
Train: 298 [ 350/1251 ( 28%)]  Loss: 3.995 (3.69)  Time: 0.673s, 1521.81/s  (0.698s, 1466.37/s)  LR: 5.102e-04  Data: 0.011 (0.016)
Train: 298 [ 400/1251 ( 32%)]  Loss: 3.871 (3.71)  Time: 0.688s, 1489.38/s  (0.697s, 1468.32/s)  LR: 5.102e-04  Data: 0.017 (0.015)
Train: 298 [ 450/1251 ( 36%)]  Loss: 3.437 (3.68)  Time: 0.676s, 1513.99/s  (0.697s, 1469.61/s)  LR: 5.102e-04  Data: 0.008 (0.015)
Train: 298 [ 500/1251 ( 40%)]  Loss: 3.831 (3.70)  Time: 0.748s, 1369.14/s  (0.697s, 1468.97/s)  LR: 5.102e-04  Data: 0.010 (0.014)
Train: 298 [ 550/1251 ( 44%)]  Loss: 3.696 (3.70)  Time: 0.732s, 1398.98/s  (0.696s, 1470.26/s)  LR: 5.102e-04  Data: 0.012 (0.014)
Train: 298 [ 600/1251 ( 48%)]  Loss: 3.910 (3.71)  Time: 0.671s, 1526.62/s  (0.696s, 1471.82/s)  LR: 5.102e-04  Data: 0.013 (0.014)
Train: 298 [ 650/1251 ( 52%)]  Loss: 3.791 (3.72)  Time: 0.705s, 1453.13/s  (0.696s, 1472.18/s)  LR: 5.102e-04  Data: 0.010 (0.013)
Train: 298 [ 700/1251 ( 56%)]  Loss: 3.713 (3.72)  Time: 0.703s, 1455.59/s  (0.695s, 1472.54/s)  LR: 5.102e-04  Data: 0.010 (0.013)
Train: 298 [ 750/1251 ( 60%)]  Loss: 3.048 (3.68)  Time: 0.671s, 1525.55/s  (0.695s, 1472.53/s)  LR: 5.102e-04  Data: 0.011 (0.013)
Train: 298 [ 800/1251 ( 64%)]  Loss: 3.624 (3.67)  Time: 0.706s, 1451.03/s  (0.695s, 1473.32/s)  LR: 5.102e-04  Data: 0.011 (0.013)
Train: 298 [ 850/1251 ( 68%)]  Loss: 3.781 (3.68)  Time: 0.670s, 1527.67/s  (0.695s, 1473.39/s)  LR: 5.102e-04  Data: 0.011 (0.013)
Train: 298 [ 900/1251 ( 72%)]  Loss: 3.598 (3.67)  Time: 0.675s, 1517.76/s  (0.695s, 1473.72/s)  LR: 5.102e-04  Data: 0.011 (0.013)
Train: 298 [ 950/1251 ( 76%)]  Loss: 3.700 (3.68)  Time: 0.668s, 1533.49/s  (0.695s, 1474.41/s)  LR: 5.102e-04  Data: 0.009 (0.012)
Train: 298 [1000/1251 ( 80%)]  Loss: 3.899 (3.69)  Time: 0.676s, 1515.61/s  (0.694s, 1474.60/s)  LR: 5.102e-04  Data: 0.010 (0.012)
Train: 298 [1050/1251 ( 84%)]  Loss: 3.443 (3.68)  Time: 0.692s, 1478.90/s  (0.695s, 1473.86/s)  LR: 5.102e-04  Data: 0.011 (0.012)
Train: 298 [1100/1251 ( 88%)]  Loss: 3.744 (3.68)  Time: 0.702s, 1459.30/s  (0.694s, 1474.45/s)  LR: 5.102e-04  Data: 0.008 (0.012)
Train: 298 [1150/1251 ( 92%)]  Loss: 3.216 (3.66)  Time: 0.727s, 1408.30/s  (0.694s, 1474.81/s)  LR: 5.102e-04  Data: 0.010 (0.012)
Train: 298 [1200/1251 ( 96%)]  Loss: 3.737 (3.66)  Time: 0.700s, 1463.40/s  (0.694s, 1475.18/s)  LR: 5.102e-04  Data: 0.011 (0.012)
Train: 298 [1250/1251 (100%)]  Loss: 3.303 (3.65)  Time: 0.656s, 1559.81/s  (0.694s, 1475.47/s)  LR: 5.102e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.581 (1.581)  Loss:  0.8750 (0.8750)  Acc@1: 88.6719 (88.6719)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.136 (0.586)  Loss:  0.9653 (1.4080)  Acc@1: 83.3727 (72.9180)  Acc@5: 95.8726 (91.3600)
Train: 299 [   0/1251 (  0%)]  Loss: 3.637 (3.64)  Time: 2.218s,  461.68/s  (2.218s,  461.68/s)  LR: 5.076e-04  Data: 1.605 (1.605)
Train: 299 [  50/1251 (  4%)]  Loss: 3.792 (3.71)  Time: 0.669s, 1530.39/s  (0.724s, 1414.43/s)  LR: 5.076e-04  Data: 0.010 (0.050)
Train: 299 [ 100/1251 (  8%)]  Loss: 3.526 (3.65)  Time: 0.751s, 1362.78/s  (0.704s, 1453.81/s)  LR: 5.076e-04  Data: 0.017 (0.030)
Train: 299 [ 150/1251 ( 12%)]  Loss: 3.509 (3.62)  Time: 0.707s, 1448.77/s  (0.703s, 1456.54/s)  LR: 5.076e-04  Data: 0.009 (0.023)
Train: 299 [ 200/1251 ( 16%)]  Loss: 3.861 (3.67)  Time: 0.675s, 1517.24/s  (0.701s, 1460.49/s)  LR: 5.076e-04  Data: 0.011 (0.020)
Train: 299 [ 250/1251 ( 20%)]  Loss: 3.639 (3.66)  Time: 0.676s, 1515.09/s  (0.699s, 1464.52/s)  LR: 5.076e-04  Data: 0.010 (0.018)
Train: 299 [ 300/1251 ( 24%)]  Loss: 3.889 (3.69)  Time: 0.689s, 1485.99/s  (0.697s, 1469.04/s)  LR: 5.076e-04  Data: 0.008 (0.017)
Train: 299 [ 350/1251 ( 28%)]  Loss: 3.654 (3.69)  Time: 0.680s, 1506.01/s  (0.696s, 1471.88/s)  LR: 5.076e-04  Data: 0.009 (0.016)
Train: 299 [ 400/1251 ( 32%)]  Loss: 3.580 (3.68)  Time: 0.681s, 1502.75/s  (0.695s, 1473.35/s)  LR: 5.076e-04  Data: 0.010 (0.015)
Train: 299 [ 450/1251 ( 36%)]  Loss: 3.619 (3.67)  Time: 0.672s, 1522.87/s  (0.695s, 1474.32/s)  LR: 5.076e-04  Data: 0.010 (0.015)
Train: 299 [ 500/1251 ( 40%)]  Loss: 3.717 (3.67)  Time: 0.728s, 1406.12/s  (0.695s, 1473.06/s)  LR: 5.076e-04  Data: 0.009 (0.014)
Train: 299 [ 550/1251 ( 44%)]  Loss: 3.468 (3.66)  Time: 0.673s, 1521.00/s  (0.694s, 1475.24/s)  LR: 5.076e-04  Data: 0.013 (0.014)
Train: 299 [ 600/1251 ( 48%)]  Loss: 3.676 (3.66)  Time: 0.688s, 1487.35/s  (0.694s, 1476.49/s)  LR: 5.076e-04  Data: 0.009 (0.014)
Train: 299 [ 650/1251 ( 52%)]  Loss: 3.748 (3.67)  Time: 0.673s, 1521.57/s  (0.693s, 1477.00/s)  LR: 5.076e-04  Data: 0.009 (0.013)
Train: 299 [ 700/1251 ( 56%)]  Loss: 3.870 (3.68)  Time: 0.707s, 1449.34/s  (0.693s, 1477.99/s)  LR: 5.076e-04  Data: 0.009 (0.013)
Train: 299 [ 750/1251 ( 60%)]  Loss: 3.807 (3.69)  Time: 0.717s, 1427.37/s  (0.693s, 1478.21/s)  LR: 5.076e-04  Data: 0.010 (0.013)
Train: 299 [ 800/1251 ( 64%)]  Loss: 3.566 (3.68)  Time: 0.676s, 1515.76/s  (0.692s, 1478.75/s)  LR: 5.076e-04  Data: 0.010 (0.013)
Train: 299 [ 850/1251 ( 68%)]  Loss: 3.429 (3.67)  Time: 0.718s, 1426.98/s  (0.692s, 1479.41/s)  LR: 5.076e-04  Data: 0.010 (0.013)
Train: 299 [ 900/1251 ( 72%)]  Loss: 3.312 (3.65)  Time: 0.672s, 1524.00/s  (0.692s, 1479.17/s)  LR: 5.076e-04  Data: 0.010 (0.013)
Train: 299 [ 950/1251 ( 76%)]  Loss: 3.480 (3.64)  Time: 0.731s, 1401.54/s  (0.693s, 1477.99/s)  LR: 5.076e-04  Data: 0.009 (0.012)
Train: 299 [1000/1251 ( 80%)]  Loss: 2.931 (3.61)  Time: 0.669s, 1530.69/s  (0.693s, 1478.25/s)  LR: 5.076e-04  Data: 0.009 (0.012)
Train: 299 [1050/1251 ( 84%)]  Loss: 3.688 (3.61)  Time: 0.673s, 1522.42/s  (0.692s, 1479.04/s)  LR: 5.076e-04  Data: 0.010 (0.012)
Train: 299 [1100/1251 ( 88%)]  Loss: 3.968 (3.62)  Time: 0.674s, 1520.30/s  (0.692s, 1479.14/s)  LR: 5.076e-04  Data: 0.010 (0.012)
Train: 299 [1150/1251 ( 92%)]  Loss: 3.468 (3.62)  Time: 0.668s, 1531.87/s  (0.692s, 1479.85/s)  LR: 5.076e-04  Data: 0.009 (0.012)
Train: 299 [1200/1251 ( 96%)]  Loss: 3.656 (3.62)  Time: 0.671s, 1525.29/s  (0.692s, 1480.08/s)  LR: 5.076e-04  Data: 0.010 (0.012)
Train: 299 [1250/1251 (100%)]  Loss: 3.596 (3.62)  Time: 0.656s, 1559.85/s  (0.692s, 1480.41/s)  LR: 5.076e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.634 (1.634)  Loss:  0.8774 (0.8774)  Acc@1: 89.0625 (89.0625)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.136 (0.586)  Loss:  0.9556 (1.4182)  Acc@1: 83.3726 (73.4120)  Acc@5: 96.2264 (91.6280)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-292.pth.tar', 73.81200004394532)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-296.pth.tar', 73.6020000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-293.pth.tar', 73.52199999023438)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-297.pth.tar', 73.4760001147461)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-286.pth.tar', 73.4719999633789)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-299.pth.tar', 73.41200001708984)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-295.pth.tar', 73.28800009033203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-277.pth.tar', 73.26600009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-284.pth.tar', 73.24199999023438)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-289.pth.tar', 73.23600006347657)

Train: 300 [   0/1251 (  0%)]  Loss: 3.696 (3.70)  Time: 2.253s,  454.48/s  (2.253s,  454.48/s)  LR: 5.050e-04  Data: 1.569 (1.569)
Train: 300 [  50/1251 (  4%)]  Loss: 3.840 (3.77)  Time: 0.747s, 1371.51/s  (0.733s, 1397.40/s)  LR: 5.050e-04  Data: 0.010 (0.050)
Train: 300 [ 100/1251 (  8%)]  Loss: 3.616 (3.72)  Time: 0.677s, 1512.13/s  (0.713s, 1435.25/s)  LR: 5.050e-04  Data: 0.018 (0.030)
Train: 300 [ 150/1251 ( 12%)]  Loss: 3.421 (3.64)  Time: 0.736s, 1391.67/s  (0.706s, 1450.98/s)  LR: 5.050e-04  Data: 0.011 (0.024)
Train: 300 [ 200/1251 ( 16%)]  Loss: 3.654 (3.65)  Time: 0.671s, 1526.02/s  (0.702s, 1459.49/s)  LR: 5.050e-04  Data: 0.009 (0.020)
Train: 300 [ 250/1251 ( 20%)]  Loss: 3.377 (3.60)  Time: 0.674s, 1519.72/s  (0.699s, 1464.66/s)  LR: 5.050e-04  Data: 0.009 (0.018)
Train: 300 [ 300/1251 ( 24%)]  Loss: 3.644 (3.61)  Time: 0.668s, 1533.78/s  (0.697s, 1468.45/s)  LR: 5.050e-04  Data: 0.012 (0.017)
Train: 300 [ 350/1251 ( 28%)]  Loss: 3.462 (3.59)  Time: 0.702s, 1457.89/s  (0.696s, 1471.70/s)  LR: 5.050e-04  Data: 0.014 (0.016)
Train: 300 [ 400/1251 ( 32%)]  Loss: 3.624 (3.59)  Time: 0.711s, 1441.12/s  (0.695s, 1474.38/s)  LR: 5.050e-04  Data: 0.011 (0.015)
Train: 300 [ 450/1251 ( 36%)]  Loss: 3.483 (3.58)  Time: 0.709s, 1443.67/s  (0.695s, 1473.47/s)  LR: 5.050e-04  Data: 0.011 (0.015)
Train: 300 [ 500/1251 ( 40%)]  Loss: 4.041 (3.62)  Time: 0.704s, 1454.68/s  (0.695s, 1472.53/s)  LR: 5.050e-04  Data: 0.009 (0.014)
Train: 300 [ 550/1251 ( 44%)]  Loss: 3.824 (3.64)  Time: 0.671s, 1525.39/s  (0.694s, 1474.45/s)  LR: 5.050e-04  Data: 0.011 (0.014)
Train: 300 [ 600/1251 ( 48%)]  Loss: 3.576 (3.64)  Time: 0.676s, 1514.75/s  (0.694s, 1475.69/s)  LR: 5.050e-04  Data: 0.010 (0.014)
Train: 300 [ 650/1251 ( 52%)]  Loss: 3.972 (3.66)  Time: 0.676s, 1513.89/s  (0.693s, 1476.75/s)  LR: 5.050e-04  Data: 0.011 (0.013)
Train: 300 [ 700/1251 ( 56%)]  Loss: 3.663 (3.66)  Time: 0.673s, 1521.54/s  (0.693s, 1477.07/s)  LR: 5.050e-04  Data: 0.010 (0.013)
Train: 300 [ 750/1251 ( 60%)]  Loss: 3.512 (3.65)  Time: 0.723s, 1416.08/s  (0.694s, 1476.36/s)  LR: 5.050e-04  Data: 0.010 (0.013)
Train: 300 [ 800/1251 ( 64%)]  Loss: 3.437 (3.64)  Time: 0.671s, 1525.78/s  (0.694s, 1475.81/s)  LR: 5.050e-04  Data: 0.011 (0.013)
Train: 300 [ 850/1251 ( 68%)]  Loss: 3.750 (3.64)  Time: 0.685s, 1495.58/s  (0.694s, 1475.16/s)  LR: 5.050e-04  Data: 0.011 (0.013)
Train: 300 [ 900/1251 ( 72%)]  Loss: 3.652 (3.64)  Time: 0.672s, 1522.96/s  (0.694s, 1475.67/s)  LR: 5.050e-04  Data: 0.010 (0.013)
Train: 300 [ 950/1251 ( 76%)]  Loss: 3.977 (3.66)  Time: 0.671s, 1525.60/s  (0.694s, 1475.94/s)  LR: 5.050e-04  Data: 0.010 (0.012)
Train: 300 [1000/1251 ( 80%)]  Loss: 3.709 (3.66)  Time: 0.734s, 1394.93/s  (0.694s, 1476.11/s)  LR: 5.050e-04  Data: 0.010 (0.012)
Train: 300 [1050/1251 ( 84%)]  Loss: 3.757 (3.67)  Time: 0.719s, 1425.14/s  (0.694s, 1476.43/s)  LR: 5.050e-04  Data: 0.009 (0.012)
Train: 300 [1100/1251 ( 88%)]  Loss: 3.967 (3.68)  Time: 0.673s, 1521.48/s  (0.693s, 1476.60/s)  LR: 5.050e-04  Data: 0.010 (0.012)
Train: 300 [1150/1251 ( 92%)]  Loss: 3.698 (3.68)  Time: 0.672s, 1522.83/s  (0.694s, 1476.43/s)  LR: 5.050e-04  Data: 0.010 (0.012)
Train: 300 [1200/1251 ( 96%)]  Loss: 3.802 (3.69)  Time: 0.672s, 1524.60/s  (0.693s, 1476.94/s)  LR: 5.050e-04  Data: 0.011 (0.012)
Train: 300 [1250/1251 (100%)]  Loss: 3.370 (3.67)  Time: 0.659s, 1553.53/s  (0.693s, 1477.57/s)  LR: 5.050e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.534 (1.534)  Loss:  0.8262 (0.8262)  Acc@1: 89.0625 (89.0625)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.136 (0.580)  Loss:  0.9106 (1.3950)  Acc@1: 84.0802 (73.6480)  Acc@5: 95.4009 (91.7160)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-292.pth.tar', 73.81200004394532)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-300.pth.tar', 73.64799993652343)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-296.pth.tar', 73.6020000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-293.pth.tar', 73.52199999023438)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-297.pth.tar', 73.4760001147461)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-286.pth.tar', 73.4719999633789)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-299.pth.tar', 73.41200001708984)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-295.pth.tar', 73.28800009033203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-277.pth.tar', 73.26600009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-284.pth.tar', 73.24199999023438)

Train: 301 [   0/1251 (  0%)]  Loss: 3.646 (3.65)  Time: 2.209s,  463.46/s  (2.209s,  463.46/s)  LR: 5.024e-04  Data: 1.548 (1.548)
Train: 301 [  50/1251 (  4%)]  Loss: 3.951 (3.80)  Time: 0.700s, 1463.04/s  (0.724s, 1413.71/s)  LR: 5.024e-04  Data: 0.009 (0.047)
Train: 301 [ 100/1251 (  8%)]  Loss: 3.986 (3.86)  Time: 0.696s, 1470.58/s  (0.713s, 1435.98/s)  LR: 5.024e-04  Data: 0.008 (0.029)
Train: 301 [ 150/1251 ( 12%)]  Loss: 3.586 (3.79)  Time: 0.704s, 1454.28/s  (0.704s, 1454.79/s)  LR: 5.024e-04  Data: 0.009 (0.023)
Train: 301 [ 200/1251 ( 16%)]  Loss: 4.004 (3.83)  Time: 0.672s, 1523.03/s  (0.700s, 1462.52/s)  LR: 5.024e-04  Data: 0.011 (0.019)
Train: 301 [ 250/1251 ( 20%)]  Loss: 4.117 (3.88)  Time: 0.668s, 1533.51/s  (0.698s, 1466.99/s)  LR: 5.024e-04  Data: 0.009 (0.018)
Train: 301 [ 300/1251 ( 24%)]  Loss: 3.972 (3.89)  Time: 0.758s, 1350.98/s  (0.698s, 1466.88/s)  LR: 5.024e-04  Data: 0.012 (0.016)
Train: 301 [ 350/1251 ( 28%)]  Loss: 3.358 (3.83)  Time: 0.774s, 1323.25/s  (0.697s, 1468.96/s)  LR: 5.024e-04  Data: 0.010 (0.015)
Train: 301 [ 400/1251 ( 32%)]  Loss: 3.999 (3.85)  Time: 0.700s, 1463.52/s  (0.697s, 1468.42/s)  LR: 5.024e-04  Data: 0.009 (0.015)
Train: 301 [ 450/1251 ( 36%)]  Loss: 3.565 (3.82)  Time: 0.700s, 1462.66/s  (0.698s, 1468.03/s)  LR: 5.024e-04  Data: 0.010 (0.014)
Train: 301 [ 500/1251 ( 40%)]  Loss: 3.351 (3.78)  Time: 0.707s, 1449.10/s  (0.697s, 1468.11/s)  LR: 5.024e-04  Data: 0.008 (0.014)
Train: 301 [ 550/1251 ( 44%)]  Loss: 3.576 (3.76)  Time: 0.716s, 1430.61/s  (0.697s, 1469.48/s)  LR: 5.024e-04  Data: 0.010 (0.014)
Train: 301 [ 600/1251 ( 48%)]  Loss: 3.819 (3.76)  Time: 0.743s, 1378.10/s  (0.697s, 1469.74/s)  LR: 5.024e-04  Data: 0.010 (0.013)
Train: 301 [ 650/1251 ( 52%)]  Loss: 3.491 (3.74)  Time: 0.689s, 1487.01/s  (0.696s, 1470.30/s)  LR: 5.024e-04  Data: 0.010 (0.013)
Train: 301 [ 700/1251 ( 56%)]  Loss: 3.776 (3.75)  Time: 0.705s, 1452.16/s  (0.697s, 1470.00/s)  LR: 5.024e-04  Data: 0.010 (0.013)
Train: 301 [ 750/1251 ( 60%)]  Loss: 3.997 (3.76)  Time: 0.709s, 1444.43/s  (0.696s, 1470.61/s)  LR: 5.024e-04  Data: 0.015 (0.013)
Train: 301 [ 800/1251 ( 64%)]  Loss: 3.905 (3.77)  Time: 0.676s, 1515.81/s  (0.696s, 1471.88/s)  LR: 5.024e-04  Data: 0.009 (0.013)
Train: 301 [ 850/1251 ( 68%)]  Loss: 3.872 (3.78)  Time: 0.702s, 1458.26/s  (0.696s, 1472.32/s)  LR: 5.024e-04  Data: 0.010 (0.013)
Train: 301 [ 900/1251 ( 72%)]  Loss: 3.649 (3.77)  Time: 0.673s, 1522.36/s  (0.695s, 1473.40/s)  LR: 5.024e-04  Data: 0.010 (0.012)
Train: 301 [ 950/1251 ( 76%)]  Loss: 3.525 (3.76)  Time: 0.709s, 1444.04/s  (0.695s, 1473.75/s)  LR: 5.024e-04  Data: 0.009 (0.012)
Train: 301 [1000/1251 ( 80%)]  Loss: 3.751 (3.76)  Time: 0.666s, 1536.88/s  (0.695s, 1473.77/s)  LR: 5.024e-04  Data: 0.010 (0.012)
Train: 301 [1050/1251 ( 84%)]  Loss: 3.652 (3.75)  Time: 0.699s, 1463.94/s  (0.695s, 1473.67/s)  LR: 5.024e-04  Data: 0.010 (0.012)
Train: 301 [1100/1251 ( 88%)]  Loss: 3.999 (3.76)  Time: 0.672s, 1522.92/s  (0.695s, 1474.09/s)  LR: 5.024e-04  Data: 0.010 (0.012)
Train: 301 [1150/1251 ( 92%)]  Loss: 3.556 (3.75)  Time: 0.708s, 1445.75/s  (0.694s, 1475.01/s)  LR: 5.024e-04  Data: 0.009 (0.012)
Train: 301 [1200/1251 ( 96%)]  Loss: 3.928 (3.76)  Time: 0.678s, 1510.56/s  (0.694s, 1475.46/s)  LR: 5.024e-04  Data: 0.009 (0.012)
Train: 301 [1250/1251 (100%)]  Loss: 3.807 (3.76)  Time: 0.717s, 1427.78/s  (0.694s, 1475.84/s)  LR: 5.024e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.545 (1.545)  Loss:  0.7856 (0.7856)  Acc@1: 89.0625 (89.0625)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.137 (0.588)  Loss:  0.9775 (1.4479)  Acc@1: 83.8443 (73.5620)  Acc@5: 95.6368 (91.7000)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-292.pth.tar', 73.81200004394532)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-300.pth.tar', 73.64799993652343)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-296.pth.tar', 73.6020000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-301.pth.tar', 73.56200009277343)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-293.pth.tar', 73.52199999023438)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-297.pth.tar', 73.4760001147461)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-286.pth.tar', 73.4719999633789)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-299.pth.tar', 73.41200001708984)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-295.pth.tar', 73.28800009033203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-277.pth.tar', 73.26600009277344)

Train: 302 [   0/1251 (  0%)]  Loss: 3.270 (3.27)  Time: 2.374s,  431.41/s  (2.374s,  431.41/s)  LR: 4.998e-04  Data: 1.743 (1.743)
Train: 302 [  50/1251 (  4%)]  Loss: 3.528 (3.40)  Time: 0.672s, 1524.78/s  (0.725s, 1412.98/s)  LR: 4.998e-04  Data: 0.010 (0.047)
Train: 302 [ 100/1251 (  8%)]  Loss: 3.519 (3.44)  Time: 0.673s, 1522.39/s  (0.707s, 1449.33/s)  LR: 4.998e-04  Data: 0.011 (0.029)
Train: 302 [ 150/1251 ( 12%)]  Loss: 3.930 (3.56)  Time: 0.675s, 1516.57/s  (0.703s, 1457.05/s)  LR: 4.998e-04  Data: 0.011 (0.023)
Train: 302 [ 200/1251 ( 16%)]  Loss: 3.486 (3.55)  Time: 0.670s, 1529.32/s  (0.698s, 1467.71/s)  LR: 4.998e-04  Data: 0.011 (0.020)
Train: 302 [ 250/1251 ( 20%)]  Loss: 3.477 (3.53)  Time: 0.673s, 1520.98/s  (0.696s, 1470.58/s)  LR: 4.998e-04  Data: 0.013 (0.018)
Train: 302 [ 300/1251 ( 24%)]  Loss: 3.946 (3.59)  Time: 0.725s, 1412.56/s  (0.696s, 1471.57/s)  LR: 4.998e-04  Data: 0.012 (0.016)
Train: 302 [ 350/1251 ( 28%)]  Loss: 4.062 (3.65)  Time: 0.722s, 1417.74/s  (0.696s, 1471.56/s)  LR: 4.998e-04  Data: 0.009 (0.016)
Train: 302 [ 400/1251 ( 32%)]  Loss: 3.770 (3.67)  Time: 0.708s, 1445.46/s  (0.696s, 1471.48/s)  LR: 4.998e-04  Data: 0.010 (0.015)
Train: 302 [ 450/1251 ( 36%)]  Loss: 3.696 (3.67)  Time: 0.705s, 1452.45/s  (0.695s, 1473.65/s)  LR: 4.998e-04  Data: 0.012 (0.015)
Train: 302 [ 500/1251 ( 40%)]  Loss: 3.265 (3.63)  Time: 0.671s, 1526.61/s  (0.694s, 1475.06/s)  LR: 4.998e-04  Data: 0.010 (0.014)
Train: 302 [ 550/1251 ( 44%)]  Loss: 3.971 (3.66)  Time: 0.723s, 1416.34/s  (0.695s, 1474.41/s)  LR: 4.998e-04  Data: 0.012 (0.014)
Train: 302 [ 600/1251 ( 48%)]  Loss: 3.687 (3.66)  Time: 0.721s, 1420.17/s  (0.695s, 1474.02/s)  LR: 4.998e-04  Data: 0.012 (0.014)
Train: 302 [ 650/1251 ( 52%)]  Loss: 3.735 (3.67)  Time: 0.667s, 1534.99/s  (0.695s, 1473.82/s)  LR: 4.998e-04  Data: 0.012 (0.013)
Train: 302 [ 700/1251 ( 56%)]  Loss: 3.681 (3.67)  Time: 0.674s, 1518.57/s  (0.695s, 1472.88/s)  LR: 4.998e-04  Data: 0.013 (0.013)
Train: 302 [ 750/1251 ( 60%)]  Loss: 3.822 (3.68)  Time: 0.674s, 1520.33/s  (0.695s, 1474.17/s)  LR: 4.998e-04  Data: 0.010 (0.013)
Train: 302 [ 800/1251 ( 64%)]  Loss: 3.277 (3.65)  Time: 0.677s, 1513.57/s  (0.694s, 1475.37/s)  LR: 4.998e-04  Data: 0.012 (0.013)
Train: 302 [ 850/1251 ( 68%)]  Loss: 3.429 (3.64)  Time: 0.672s, 1524.72/s  (0.694s, 1476.04/s)  LR: 4.998e-04  Data: 0.012 (0.013)
Train: 302 [ 900/1251 ( 72%)]  Loss: 3.396 (3.63)  Time: 0.703s, 1456.56/s  (0.694s, 1476.26/s)  LR: 4.998e-04  Data: 0.010 (0.013)
Train: 302 [ 950/1251 ( 76%)]  Loss: 3.687 (3.63)  Time: 0.735s, 1393.86/s  (0.694s, 1475.69/s)  LR: 4.998e-04  Data: 0.013 (0.013)
Train: 302 [1000/1251 ( 80%)]  Loss: 3.703 (3.64)  Time: 0.678s, 1510.87/s  (0.694s, 1475.24/s)  LR: 4.998e-04  Data: 0.010 (0.012)
Train: 302 [1050/1251 ( 84%)]  Loss: 3.515 (3.63)  Time: 0.671s, 1525.89/s  (0.694s, 1475.92/s)  LR: 4.998e-04  Data: 0.010 (0.012)
Train: 302 [1100/1251 ( 88%)]  Loss: 3.654 (3.63)  Time: 0.681s, 1503.31/s  (0.694s, 1476.02/s)  LR: 4.998e-04  Data: 0.009 (0.012)
Train: 302 [1150/1251 ( 92%)]  Loss: 3.627 (3.63)  Time: 0.672s, 1523.58/s  (0.694s, 1475.80/s)  LR: 4.998e-04  Data: 0.010 (0.012)
Train: 302 [1200/1251 ( 96%)]  Loss: 3.813 (3.64)  Time: 0.673s, 1522.20/s  (0.694s, 1476.17/s)  LR: 4.998e-04  Data: 0.011 (0.012)
Train: 302 [1250/1251 (100%)]  Loss: 3.466 (3.63)  Time: 0.705s, 1452.14/s  (0.694s, 1476.28/s)  LR: 4.998e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.613 (1.613)  Loss:  0.9053 (0.9053)  Acc@1: 89.8438 (89.8438)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.137 (0.585)  Loss:  1.0996 (1.4793)  Acc@1: 82.6651 (73.6680)  Acc@5: 96.2264 (91.7540)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-292.pth.tar', 73.81200004394532)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-302.pth.tar', 73.66800009765625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-300.pth.tar', 73.64799993652343)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-296.pth.tar', 73.6020000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-301.pth.tar', 73.56200009277343)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-293.pth.tar', 73.52199999023438)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-297.pth.tar', 73.4760001147461)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-286.pth.tar', 73.4719999633789)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-299.pth.tar', 73.41200001708984)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-295.pth.tar', 73.28800009033203)

Train: 303 [   0/1251 (  0%)]  Loss: 3.484 (3.48)  Time: 2.360s,  433.99/s  (2.360s,  433.99/s)  LR: 4.972e-04  Data: 1.708 (1.708)
Train: 303 [  50/1251 (  4%)]  Loss: 3.505 (3.49)  Time: 0.671s, 1525.88/s  (0.721s, 1420.49/s)  LR: 4.972e-04  Data: 0.010 (0.048)
Train: 303 [ 100/1251 (  8%)]  Loss: 3.880 (3.62)  Time: 0.684s, 1496.22/s  (0.710s, 1442.86/s)  LR: 4.972e-04  Data: 0.013 (0.029)
Train: 303 [ 150/1251 ( 12%)]  Loss: 3.576 (3.61)  Time: 0.671s, 1525.95/s  (0.705s, 1453.12/s)  LR: 4.972e-04  Data: 0.011 (0.023)
Train: 303 [ 200/1251 ( 16%)]  Loss: 3.824 (3.65)  Time: 0.723s, 1415.43/s  (0.701s, 1461.53/s)  LR: 4.972e-04  Data: 0.010 (0.020)
Train: 303 [ 250/1251 ( 20%)]  Loss: 3.548 (3.64)  Time: 0.727s, 1409.03/s  (0.699s, 1464.39/s)  LR: 4.972e-04  Data: 0.009 (0.018)
Train: 303 [ 300/1251 ( 24%)]  Loss: 3.671 (3.64)  Time: 0.678s, 1509.42/s  (0.698s, 1467.46/s)  LR: 4.972e-04  Data: 0.010 (0.017)
Train: 303 [ 350/1251 ( 28%)]  Loss: 3.418 (3.61)  Time: 0.671s, 1525.35/s  (0.697s, 1469.92/s)  LR: 4.972e-04  Data: 0.010 (0.016)
Train: 303 [ 400/1251 ( 32%)]  Loss: 3.800 (3.63)  Time: 0.681s, 1504.77/s  (0.696s, 1471.27/s)  LR: 4.972e-04  Data: 0.011 (0.015)
Train: 303 [ 450/1251 ( 36%)]  Loss: 3.376 (3.61)  Time: 0.703s, 1456.94/s  (0.695s, 1473.02/s)  LR: 4.972e-04  Data: 0.009 (0.015)
Train: 303 [ 500/1251 ( 40%)]  Loss: 3.858 (3.63)  Time: 0.673s, 1520.46/s  (0.694s, 1474.70/s)  LR: 4.972e-04  Data: 0.010 (0.014)
Train: 303 [ 550/1251 ( 44%)]  Loss: 3.476 (3.62)  Time: 0.671s, 1525.88/s  (0.694s, 1474.61/s)  LR: 4.972e-04  Data: 0.011 (0.014)
Train: 303 [ 600/1251 ( 48%)]  Loss: 3.498 (3.61)  Time: 0.702s, 1457.86/s  (0.694s, 1476.20/s)  LR: 4.972e-04  Data: 0.010 (0.014)
Train: 303 [ 650/1251 ( 52%)]  Loss: 2.858 (3.56)  Time: 0.695s, 1472.57/s  (0.694s, 1476.31/s)  LR: 4.972e-04  Data: 0.010 (0.013)
Train: 303 [ 700/1251 ( 56%)]  Loss: 3.329 (3.54)  Time: 0.726s, 1411.21/s  (0.694s, 1475.13/s)  LR: 4.972e-04  Data: 0.010 (0.013)
Train: 303 [ 750/1251 ( 60%)]  Loss: 3.779 (3.56)  Time: 0.708s, 1446.44/s  (0.694s, 1474.90/s)  LR: 4.972e-04  Data: 0.011 (0.013)
Train: 303 [ 800/1251 ( 64%)]  Loss: 3.702 (3.56)  Time: 0.672s, 1522.91/s  (0.695s, 1474.32/s)  LR: 4.972e-04  Data: 0.011 (0.013)
Train: 303 [ 850/1251 ( 68%)]  Loss: 3.532 (3.56)  Time: 0.689s, 1485.65/s  (0.695s, 1473.90/s)  LR: 4.972e-04  Data: 0.013 (0.013)
Train: 303 [ 900/1251 ( 72%)]  Loss: 3.675 (3.57)  Time: 0.726s, 1411.18/s  (0.695s, 1474.17/s)  LR: 4.972e-04  Data: 0.009 (0.013)
Train: 303 [ 950/1251 ( 76%)]  Loss: 3.546 (3.57)  Time: 0.671s, 1525.67/s  (0.695s, 1474.16/s)  LR: 4.972e-04  Data: 0.011 (0.012)
Train: 303 [1000/1251 ( 80%)]  Loss: 3.892 (3.58)  Time: 0.713s, 1436.80/s  (0.694s, 1474.48/s)  LR: 4.972e-04  Data: 0.010 (0.012)
Train: 303 [1050/1251 ( 84%)]  Loss: 3.838 (3.59)  Time: 0.700s, 1463.79/s  (0.695s, 1473.84/s)  LR: 4.972e-04  Data: 0.010 (0.012)
Train: 303 [1100/1251 ( 88%)]  Loss: 3.949 (3.61)  Time: 0.687s, 1491.53/s  (0.695s, 1473.72/s)  LR: 4.972e-04  Data: 0.010 (0.012)
Train: 303 [1150/1251 ( 92%)]  Loss: 3.749 (3.62)  Time: 0.674s, 1520.39/s  (0.694s, 1474.54/s)  LR: 4.972e-04  Data: 0.011 (0.012)
Train: 303 [1200/1251 ( 96%)]  Loss: 3.346 (3.60)  Time: 0.668s, 1532.02/s  (0.694s, 1474.71/s)  LR: 4.972e-04  Data: 0.012 (0.012)
Train: 303 [1250/1251 (100%)]  Loss: 3.972 (3.62)  Time: 0.660s, 1551.88/s  (0.694s, 1475.59/s)  LR: 4.972e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.450 (1.450)  Loss:  1.0117 (1.0117)  Acc@1: 88.4766 (88.4766)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.136 (0.586)  Loss:  1.1113 (1.4967)  Acc@1: 83.8443 (73.6240)  Acc@5: 95.6368 (91.7160)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-292.pth.tar', 73.81200004394532)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-302.pth.tar', 73.66800009765625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-300.pth.tar', 73.64799993652343)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-303.pth.tar', 73.62400009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-296.pth.tar', 73.6020000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-301.pth.tar', 73.56200009277343)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-293.pth.tar', 73.52199999023438)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-297.pth.tar', 73.4760001147461)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-286.pth.tar', 73.4719999633789)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-299.pth.tar', 73.41200001708984)

Train: 304 [   0/1251 (  0%)]  Loss: 3.579 (3.58)  Time: 2.292s,  446.71/s  (2.292s,  446.71/s)  LR: 4.946e-04  Data: 1.630 (1.630)
Train: 304 [  50/1251 (  4%)]  Loss: 3.630 (3.60)  Time: 0.675s, 1518.01/s  (0.727s, 1407.87/s)  LR: 4.946e-04  Data: 0.009 (0.049)
Train: 304 [ 100/1251 (  8%)]  Loss: 3.715 (3.64)  Time: 0.675s, 1518.08/s  (0.712s, 1438.15/s)  LR: 4.946e-04  Data: 0.010 (0.030)
Train: 304 [ 150/1251 ( 12%)]  Loss: 3.328 (3.56)  Time: 0.676s, 1513.69/s  (0.707s, 1448.48/s)  LR: 4.946e-04  Data: 0.015 (0.023)
Train: 304 [ 200/1251 ( 16%)]  Loss: 3.672 (3.58)  Time: 0.670s, 1528.30/s  (0.703s, 1456.01/s)  LR: 4.946e-04  Data: 0.011 (0.020)
Train: 304 [ 250/1251 ( 20%)]  Loss: 3.568 (3.58)  Time: 0.685s, 1494.52/s  (0.700s, 1462.93/s)  LR: 4.946e-04  Data: 0.011 (0.018)
Train: 304 [ 300/1251 ( 24%)]  Loss: 3.529 (3.57)  Time: 0.679s, 1509.13/s  (0.700s, 1461.98/s)  LR: 4.946e-04  Data: 0.010 (0.017)
Train: 304 [ 350/1251 ( 28%)]  Loss: 4.081 (3.64)  Time: 0.672s, 1523.71/s  (0.700s, 1462.39/s)  LR: 4.946e-04  Data: 0.011 (0.016)
Train: 304 [ 400/1251 ( 32%)]  Loss: 3.696 (3.64)  Time: 0.727s, 1407.90/s  (0.702s, 1458.78/s)  LR: 4.946e-04  Data: 0.013 (0.016)
Train: 304 [ 450/1251 ( 36%)]  Loss: 3.692 (3.65)  Time: 0.702s, 1459.34/s  (0.703s, 1456.92/s)  LR: 4.946e-04  Data: 0.010 (0.015)
Train: 304 [ 500/1251 ( 40%)]  Loss: 3.429 (3.63)  Time: 0.734s, 1395.19/s  (0.704s, 1454.70/s)  LR: 4.946e-04  Data: 0.011 (0.015)
Train: 304 [ 550/1251 ( 44%)]  Loss: 3.812 (3.64)  Time: 0.674s, 1519.37/s  (0.703s, 1457.59/s)  LR: 4.946e-04  Data: 0.010 (0.015)
Train: 304 [ 600/1251 ( 48%)]  Loss: 3.697 (3.65)  Time: 0.673s, 1521.93/s  (0.700s, 1461.86/s)  LR: 4.946e-04  Data: 0.011 (0.014)
Train: 304 [ 650/1251 ( 52%)]  Loss: 3.591 (3.64)  Time: 0.706s, 1450.75/s  (0.699s, 1465.64/s)  LR: 4.946e-04  Data: 0.010 (0.014)
Train: 304 [ 700/1251 ( 56%)]  Loss: 3.612 (3.64)  Time: 0.671s, 1525.02/s  (0.698s, 1467.45/s)  LR: 4.946e-04  Data: 0.011 (0.014)
Train: 304 [ 750/1251 ( 60%)]  Loss: 3.328 (3.62)  Time: 0.722s, 1418.30/s  (0.697s, 1469.44/s)  LR: 4.946e-04  Data: 0.011 (0.013)
Train: 304 [ 800/1251 ( 64%)]  Loss: 3.677 (3.63)  Time: 0.682s, 1500.76/s  (0.697s, 1469.11/s)  LR: 4.946e-04  Data: 0.010 (0.013)
Train: 304 [ 850/1251 ( 68%)]  Loss: 3.440 (3.62)  Time: 0.708s, 1446.44/s  (0.697s, 1468.44/s)  LR: 4.946e-04  Data: 0.011 (0.013)
Train: 304 [ 900/1251 ( 72%)]  Loss: 3.542 (3.61)  Time: 0.707s, 1448.33/s  (0.697s, 1468.31/s)  LR: 4.946e-04  Data: 0.011 (0.013)
Train: 304 [ 950/1251 ( 76%)]  Loss: 3.703 (3.62)  Time: 0.672s, 1523.60/s  (0.697s, 1469.68/s)  LR: 4.946e-04  Data: 0.012 (0.013)
Train: 304 [1000/1251 ( 80%)]  Loss: 3.425 (3.61)  Time: 0.667s, 1534.23/s  (0.697s, 1469.98/s)  LR: 4.946e-04  Data: 0.008 (0.013)
Train: 304 [1050/1251 ( 84%)]  Loss: 3.538 (3.60)  Time: 0.680s, 1506.15/s  (0.696s, 1470.48/s)  LR: 4.946e-04  Data: 0.010 (0.013)
Train: 304 [1100/1251 ( 88%)]  Loss: 3.680 (3.61)  Time: 0.706s, 1449.42/s  (0.696s, 1471.21/s)  LR: 4.946e-04  Data: 0.009 (0.012)
Train: 304 [1150/1251 ( 92%)]  Loss: 3.872 (3.62)  Time: 0.671s, 1527.18/s  (0.696s, 1471.09/s)  LR: 4.946e-04  Data: 0.009 (0.012)
Train: 304 [1200/1251 ( 96%)]  Loss: 3.756 (3.62)  Time: 0.673s, 1521.78/s  (0.696s, 1471.14/s)  LR: 4.946e-04  Data: 0.011 (0.012)
Train: 304 [1250/1251 (100%)]  Loss: 4.112 (3.64)  Time: 0.656s, 1560.76/s  (0.696s, 1471.47/s)  LR: 4.946e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.547 (1.547)  Loss:  0.6997 (0.6997)  Acc@1: 88.8672 (88.8672)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.136 (0.582)  Loss:  0.7681 (1.2868)  Acc@1: 84.6698 (73.8900)  Acc@5: 96.5802 (91.8500)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-304.pth.tar', 73.88999993408203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-292.pth.tar', 73.81200004394532)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-302.pth.tar', 73.66800009765625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-300.pth.tar', 73.64799993652343)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-303.pth.tar', 73.62400009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-296.pth.tar', 73.6020000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-301.pth.tar', 73.56200009277343)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-293.pth.tar', 73.52199999023438)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-297.pth.tar', 73.4760001147461)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-286.pth.tar', 73.4719999633789)

Train: 305 [   0/1251 (  0%)]  Loss: 3.786 (3.79)  Time: 2.166s,  472.81/s  (2.166s,  472.81/s)  LR: 4.920e-04  Data: 1.510 (1.510)
Train: 305 [  50/1251 (  4%)]  Loss: 3.377 (3.58)  Time: 0.675s, 1516.59/s  (0.734s, 1395.51/s)  LR: 4.920e-04  Data: 0.011 (0.051)
Train: 305 [ 100/1251 (  8%)]  Loss: 3.644 (3.60)  Time: 0.706s, 1450.08/s  (0.712s, 1439.21/s)  LR: 4.920e-04  Data: 0.010 (0.031)
Train: 305 [ 150/1251 ( 12%)]  Loss: 3.950 (3.69)  Time: 0.669s, 1530.87/s  (0.706s, 1449.72/s)  LR: 4.920e-04  Data: 0.011 (0.024)
Train: 305 [ 200/1251 ( 16%)]  Loss: 3.667 (3.68)  Time: 0.681s, 1503.09/s  (0.706s, 1449.85/s)  LR: 4.920e-04  Data: 0.015 (0.021)
Train: 305 [ 250/1251 ( 20%)]  Loss: 3.903 (3.72)  Time: 0.673s, 1520.86/s  (0.703s, 1456.15/s)  LR: 4.920e-04  Data: 0.010 (0.019)
Train: 305 [ 300/1251 ( 24%)]  Loss: 3.377 (3.67)  Time: 0.689s, 1485.36/s  (0.702s, 1457.69/s)  LR: 4.920e-04  Data: 0.010 (0.017)
Train: 305 [ 350/1251 ( 28%)]  Loss: 3.935 (3.70)  Time: 0.672s, 1523.82/s  (0.702s, 1458.64/s)  LR: 4.920e-04  Data: 0.009 (0.016)
Train: 305 [ 400/1251 ( 32%)]  Loss: 3.366 (3.67)  Time: 0.671s, 1527.12/s  (0.700s, 1463.74/s)  LR: 4.920e-04  Data: 0.010 (0.016)
Train: 305 [ 450/1251 ( 36%)]  Loss: 3.521 (3.65)  Time: 0.665s, 1539.52/s  (0.698s, 1466.15/s)  LR: 4.920e-04  Data: 0.009 (0.015)
Train: 305 [ 500/1251 ( 40%)]  Loss: 3.583 (3.65)  Time: 0.675s, 1516.29/s  (0.698s, 1468.01/s)  LR: 4.920e-04  Data: 0.012 (0.015)
Train: 305 [ 550/1251 ( 44%)]  Loss: 3.554 (3.64)  Time: 0.672s, 1523.71/s  (0.697s, 1469.22/s)  LR: 4.920e-04  Data: 0.010 (0.014)
Train: 305 [ 600/1251 ( 48%)]  Loss: 3.765 (3.65)  Time: 0.810s, 1264.12/s  (0.697s, 1470.02/s)  LR: 4.920e-04  Data: 0.010 (0.014)
Train: 305 [ 650/1251 ( 52%)]  Loss: 3.753 (3.66)  Time: 0.670s, 1529.14/s  (0.696s, 1470.63/s)  LR: 4.920e-04  Data: 0.010 (0.014)
Train: 305 [ 700/1251 ( 56%)]  Loss: 3.499 (3.65)  Time: 0.685s, 1494.39/s  (0.695s, 1472.53/s)  LR: 4.920e-04  Data: 0.012 (0.013)
Train: 305 [ 750/1251 ( 60%)]  Loss: 3.645 (3.65)  Time: 0.674s, 1520.35/s  (0.695s, 1473.78/s)  LR: 4.920e-04  Data: 0.010 (0.013)
Train: 305 [ 800/1251 ( 64%)]  Loss: 3.386 (3.63)  Time: 0.700s, 1462.47/s  (0.695s, 1474.15/s)  LR: 4.920e-04  Data: 0.009 (0.013)
Train: 305 [ 850/1251 ( 68%)]  Loss: 3.688 (3.63)  Time: 0.665s, 1538.78/s  (0.694s, 1474.62/s)  LR: 4.920e-04  Data: 0.010 (0.013)
Train: 305 [ 900/1251 ( 72%)]  Loss: 3.887 (3.65)  Time: 0.678s, 1511.07/s  (0.694s, 1475.28/s)  LR: 4.920e-04  Data: 0.010 (0.013)
Train: 305 [ 950/1251 ( 76%)]  Loss: 3.368 (3.63)  Time: 0.709s, 1443.63/s  (0.694s, 1475.54/s)  LR: 4.920e-04  Data: 0.009 (0.013)
Train: 305 [1000/1251 ( 80%)]  Loss: 3.370 (3.62)  Time: 0.733s, 1397.37/s  (0.694s, 1475.48/s)  LR: 4.920e-04  Data: 0.014 (0.013)
Train: 305 [1050/1251 ( 84%)]  Loss: 3.714 (3.62)  Time: 0.701s, 1460.85/s  (0.694s, 1475.52/s)  LR: 4.920e-04  Data: 0.014 (0.013)
Train: 305 [1100/1251 ( 88%)]  Loss: 3.608 (3.62)  Time: 0.754s, 1357.55/s  (0.694s, 1475.87/s)  LR: 4.920e-04  Data: 0.009 (0.012)
Train: 305 [1150/1251 ( 92%)]  Loss: 4.089 (3.64)  Time: 0.710s, 1441.33/s  (0.694s, 1476.33/s)  LR: 4.920e-04  Data: 0.010 (0.012)
Train: 305 [1200/1251 ( 96%)]  Loss: 3.818 (3.65)  Time: 0.708s, 1447.15/s  (0.694s, 1475.52/s)  LR: 4.920e-04  Data: 0.009 (0.012)
Train: 305 [1250/1251 (100%)]  Loss: 3.666 (3.65)  Time: 0.700s, 1462.05/s  (0.694s, 1475.49/s)  LR: 4.920e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.476 (1.476)  Loss:  0.8335 (0.8335)  Acc@1: 89.6484 (89.6484)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.136 (0.594)  Loss:  0.9688 (1.4512)  Acc@1: 83.6085 (73.2780)  Acc@5: 95.1651 (91.6160)
Train: 306 [   0/1251 (  0%)]  Loss: 3.775 (3.78)  Time: 2.243s,  456.52/s  (2.243s,  456.52/s)  LR: 4.895e-04  Data: 1.633 (1.633)
Train: 306 [  50/1251 (  4%)]  Loss: 3.462 (3.62)  Time: 0.669s, 1530.13/s  (0.730s, 1403.59/s)  LR: 4.895e-04  Data: 0.010 (0.051)
Train: 306 [ 100/1251 (  8%)]  Loss: 3.778 (3.67)  Time: 0.702s, 1458.22/s  (0.713s, 1435.96/s)  LR: 4.895e-04  Data: 0.009 (0.031)
Train: 306 [ 150/1251 ( 12%)]  Loss: 3.433 (3.61)  Time: 0.670s, 1528.72/s  (0.706s, 1450.95/s)  LR: 4.895e-04  Data: 0.013 (0.024)
Train: 306 [ 200/1251 ( 16%)]  Loss: 3.840 (3.66)  Time: 0.756s, 1355.28/s  (0.704s, 1454.06/s)  LR: 4.895e-04  Data: 0.009 (0.021)
Train: 306 [ 250/1251 ( 20%)]  Loss: 3.731 (3.67)  Time: 0.672s, 1523.21/s  (0.702s, 1458.61/s)  LR: 4.895e-04  Data: 0.011 (0.019)
Train: 306 [ 300/1251 ( 24%)]  Loss: 3.793 (3.69)  Time: 0.682s, 1501.35/s  (0.702s, 1459.35/s)  LR: 4.895e-04  Data: 0.010 (0.018)
Train: 306 [ 350/1251 ( 28%)]  Loss: 3.811 (3.70)  Time: 0.685s, 1494.45/s  (0.701s, 1461.24/s)  LR: 4.895e-04  Data: 0.009 (0.017)
Train: 306 [ 400/1251 ( 32%)]  Loss: 4.066 (3.74)  Time: 0.672s, 1523.60/s  (0.699s, 1464.13/s)  LR: 4.895e-04  Data: 0.013 (0.016)
Train: 306 [ 450/1251 ( 36%)]  Loss: 3.646 (3.73)  Time: 0.700s, 1462.19/s  (0.699s, 1465.05/s)  LR: 4.895e-04  Data: 0.009 (0.015)
Train: 306 [ 500/1251 ( 40%)]  Loss: 3.509 (3.71)  Time: 0.706s, 1451.27/s  (0.699s, 1463.95/s)  LR: 4.895e-04  Data: 0.010 (0.015)
Train: 306 [ 550/1251 ( 44%)]  Loss: 3.512 (3.70)  Time: 0.673s, 1522.49/s  (0.699s, 1465.48/s)  LR: 4.895e-04  Data: 0.011 (0.014)
Train: 306 [ 600/1251 ( 48%)]  Loss: 3.605 (3.69)  Time: 0.715s, 1433.14/s  (0.698s, 1467.59/s)  LR: 4.895e-04  Data: 0.008 (0.014)
Train: 306 [ 650/1251 ( 52%)]  Loss: 3.938 (3.71)  Time: 0.697s, 1468.15/s  (0.698s, 1467.24/s)  LR: 4.895e-04  Data: 0.010 (0.014)
Train: 306 [ 700/1251 ( 56%)]  Loss: 3.123 (3.67)  Time: 0.715s, 1431.76/s  (0.697s, 1468.31/s)  LR: 4.895e-04  Data: 0.014 (0.014)
Train: 306 [ 750/1251 ( 60%)]  Loss: 3.639 (3.67)  Time: 0.703s, 1457.06/s  (0.697s, 1468.99/s)  LR: 4.895e-04  Data: 0.010 (0.013)
Train: 306 [ 800/1251 ( 64%)]  Loss: 3.765 (3.67)  Time: 0.701s, 1461.65/s  (0.697s, 1470.17/s)  LR: 4.895e-04  Data: 0.009 (0.013)
Train: 306 [ 850/1251 ( 68%)]  Loss: 4.030 (3.69)  Time: 0.714s, 1433.59/s  (0.697s, 1469.83/s)  LR: 4.895e-04  Data: 0.011 (0.013)
Train: 306 [ 900/1251 ( 72%)]  Loss: 3.867 (3.70)  Time: 0.702s, 1458.38/s  (0.697s, 1469.18/s)  LR: 4.895e-04  Data: 0.011 (0.013)
Train: 306 [ 950/1251 ( 76%)]  Loss: 3.853 (3.71)  Time: 0.677s, 1511.66/s  (0.697s, 1469.71/s)  LR: 4.895e-04  Data: 0.012 (0.013)
Train: 306 [1000/1251 ( 80%)]  Loss: 3.502 (3.70)  Time: 0.668s, 1531.87/s  (0.697s, 1469.83/s)  LR: 4.895e-04  Data: 0.011 (0.013)
Train: 306 [1050/1251 ( 84%)]  Loss: 3.458 (3.69)  Time: 0.679s, 1508.15/s  (0.697s, 1469.98/s)  LR: 4.895e-04  Data: 0.010 (0.013)
Train: 306 [1100/1251 ( 88%)]  Loss: 3.727 (3.69)  Time: 0.670s, 1528.87/s  (0.696s, 1470.61/s)  LR: 4.895e-04  Data: 0.011 (0.012)
Train: 306 [1150/1251 ( 92%)]  Loss: 3.420 (3.68)  Time: 0.704s, 1455.26/s  (0.696s, 1471.12/s)  LR: 4.895e-04  Data: 0.009 (0.012)
Train: 306 [1200/1251 ( 96%)]  Loss: 3.776 (3.68)  Time: 0.739s, 1386.10/s  (0.696s, 1470.95/s)  LR: 4.895e-04  Data: 0.010 (0.012)
Train: 306 [1250/1251 (100%)]  Loss: 3.324 (3.67)  Time: 0.659s, 1554.78/s  (0.696s, 1471.41/s)  LR: 4.895e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.633 (1.633)  Loss:  0.7129 (0.7129)  Acc@1: 90.1367 (90.1367)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.136 (0.578)  Loss:  0.8379 (1.3341)  Acc@1: 85.0236 (73.7580)  Acc@5: 96.5802 (91.7520)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-304.pth.tar', 73.88999993408203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-292.pth.tar', 73.81200004394532)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-306.pth.tar', 73.75800008789062)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-302.pth.tar', 73.66800009765625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-300.pth.tar', 73.64799993652343)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-303.pth.tar', 73.62400009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-296.pth.tar', 73.6020000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-301.pth.tar', 73.56200009277343)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-293.pth.tar', 73.52199999023438)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-297.pth.tar', 73.4760001147461)

Train: 307 [   0/1251 (  0%)]  Loss: 3.470 (3.47)  Time: 2.216s,  462.13/s  (2.216s,  462.13/s)  LR: 4.869e-04  Data: 1.575 (1.575)
Train: 307 [  50/1251 (  4%)]  Loss: 3.434 (3.45)  Time: 0.673s, 1521.48/s  (0.726s, 1410.73/s)  LR: 4.869e-04  Data: 0.010 (0.046)
Train: 307 [ 100/1251 (  8%)]  Loss: 3.617 (3.51)  Time: 0.670s, 1528.18/s  (0.709s, 1444.56/s)  LR: 4.869e-04  Data: 0.009 (0.028)
Train: 307 [ 150/1251 ( 12%)]  Loss: 3.448 (3.49)  Time: 0.757s, 1353.33/s  (0.704s, 1454.87/s)  LR: 4.869e-04  Data: 0.010 (0.022)
Train: 307 [ 200/1251 ( 16%)]  Loss: 3.476 (3.49)  Time: 0.717s, 1428.93/s  (0.701s, 1460.41/s)  LR: 4.869e-04  Data: 0.009 (0.019)
Train: 307 [ 250/1251 ( 20%)]  Loss: 3.754 (3.53)  Time: 0.671s, 1527.14/s  (0.699s, 1464.08/s)  LR: 4.869e-04  Data: 0.010 (0.018)
Train: 307 [ 300/1251 ( 24%)]  Loss: 4.084 (3.61)  Time: 0.722s, 1417.60/s  (0.698s, 1467.77/s)  LR: 4.869e-04  Data: 0.010 (0.016)
Train: 307 [ 350/1251 ( 28%)]  Loss: 3.815 (3.64)  Time: 0.694s, 1474.63/s  (0.696s, 1470.49/s)  LR: 4.869e-04  Data: 0.010 (0.016)
Train: 307 [ 400/1251 ( 32%)]  Loss: 3.620 (3.64)  Time: 0.690s, 1483.97/s  (0.695s, 1472.83/s)  LR: 4.869e-04  Data: 0.011 (0.015)
Train: 307 [ 450/1251 ( 36%)]  Loss: 3.824 (3.65)  Time: 0.672s, 1524.67/s  (0.694s, 1475.16/s)  LR: 4.869e-04  Data: 0.010 (0.014)
Train: 307 [ 500/1251 ( 40%)]  Loss: 3.578 (3.65)  Time: 0.726s, 1410.14/s  (0.694s, 1475.83/s)  LR: 4.869e-04  Data: 0.013 (0.014)
Train: 307 [ 550/1251 ( 44%)]  Loss: 3.984 (3.68)  Time: 0.672s, 1524.66/s  (0.694s, 1476.10/s)  LR: 4.869e-04  Data: 0.011 (0.014)
Train: 307 [ 600/1251 ( 48%)]  Loss: 3.182 (3.64)  Time: 0.711s, 1441.24/s  (0.693s, 1477.36/s)  LR: 4.869e-04  Data: 0.009 (0.013)
Train: 307 [ 650/1251 ( 52%)]  Loss: 3.520 (3.63)  Time: 0.761s, 1344.93/s  (0.693s, 1478.42/s)  LR: 4.869e-04  Data: 0.010 (0.013)
Train: 307 [ 700/1251 ( 56%)]  Loss: 3.452 (3.62)  Time: 0.674s, 1519.38/s  (0.693s, 1478.58/s)  LR: 4.869e-04  Data: 0.010 (0.013)
Train: 307 [ 750/1251 ( 60%)]  Loss: 3.195 (3.59)  Time: 0.738s, 1386.84/s  (0.692s, 1479.17/s)  LR: 4.869e-04  Data: 0.010 (0.013)
Train: 307 [ 800/1251 ( 64%)]  Loss: 3.701 (3.60)  Time: 0.670s, 1527.82/s  (0.692s, 1479.57/s)  LR: 4.869e-04  Data: 0.010 (0.013)
Train: 307 [ 850/1251 ( 68%)]  Loss: 3.844 (3.61)  Time: 0.690s, 1485.08/s  (0.693s, 1478.30/s)  LR: 4.869e-04  Data: 0.014 (0.012)
Train: 307 [ 900/1251 ( 72%)]  Loss: 3.716 (3.62)  Time: 0.723s, 1415.81/s  (0.693s, 1478.11/s)  LR: 4.869e-04  Data: 0.010 (0.012)
Train: 307 [ 950/1251 ( 76%)]  Loss: 3.647 (3.62)  Time: 0.745s, 1374.51/s  (0.693s, 1477.26/s)  LR: 4.869e-04  Data: 0.010 (0.012)
Train: 307 [1000/1251 ( 80%)]  Loss: 3.623 (3.62)  Time: 0.672s, 1523.00/s  (0.693s, 1476.99/s)  LR: 4.869e-04  Data: 0.010 (0.012)
Train: 307 [1050/1251 ( 84%)]  Loss: 3.418 (3.61)  Time: 0.793s, 1290.69/s  (0.693s, 1477.27/s)  LR: 4.869e-04  Data: 0.009 (0.012)
Train: 307 [1100/1251 ( 88%)]  Loss: 3.701 (3.61)  Time: 0.673s, 1520.82/s  (0.693s, 1477.70/s)  LR: 4.869e-04  Data: 0.010 (0.012)
Train: 307 [1150/1251 ( 92%)]  Loss: 3.776 (3.62)  Time: 0.670s, 1529.15/s  (0.693s, 1477.59/s)  LR: 4.869e-04  Data: 0.011 (0.012)
Train: 307 [1200/1251 ( 96%)]  Loss: 3.185 (3.60)  Time: 0.705s, 1453.20/s  (0.693s, 1477.81/s)  LR: 4.869e-04  Data: 0.012 (0.012)
Train: 307 [1250/1251 (100%)]  Loss: 3.438 (3.60)  Time: 0.660s, 1552.05/s  (0.693s, 1478.23/s)  LR: 4.869e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.464 (1.464)  Loss:  0.7871 (0.7871)  Acc@1: 89.6484 (89.6484)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.136 (0.580)  Loss:  1.0098 (1.4690)  Acc@1: 83.1368 (73.3780)  Acc@5: 95.4009 (91.5980)
Train: 308 [   0/1251 (  0%)]  Loss: 3.532 (3.53)  Time: 2.164s,  473.24/s  (2.164s,  473.24/s)  LR: 4.843e-04  Data: 1.548 (1.548)
Train: 308 [  50/1251 (  4%)]  Loss: 3.432 (3.48)  Time: 0.673s, 1521.39/s  (0.721s, 1420.26/s)  LR: 4.843e-04  Data: 0.010 (0.046)
Train: 308 [ 100/1251 (  8%)]  Loss: 3.522 (3.50)  Time: 0.701s, 1460.10/s  (0.706s, 1449.80/s)  LR: 4.843e-04  Data: 0.013 (0.028)
Train: 308 [ 150/1251 ( 12%)]  Loss: 4.025 (3.63)  Time: 0.713s, 1436.09/s  (0.704s, 1453.97/s)  LR: 4.843e-04  Data: 0.010 (0.022)
Train: 308 [ 200/1251 ( 16%)]  Loss: 3.576 (3.62)  Time: 0.673s, 1521.82/s  (0.701s, 1461.72/s)  LR: 4.843e-04  Data: 0.011 (0.020)
Train: 308 [ 250/1251 ( 20%)]  Loss: 3.433 (3.59)  Time: 0.671s, 1526.18/s  (0.698s, 1466.87/s)  LR: 4.843e-04  Data: 0.010 (0.018)
Train: 308 [ 300/1251 ( 24%)]  Loss: 3.514 (3.58)  Time: 0.701s, 1460.41/s  (0.697s, 1469.60/s)  LR: 4.843e-04  Data: 0.010 (0.016)
Train: 308 [ 350/1251 ( 28%)]  Loss: 3.913 (3.62)  Time: 0.675s, 1516.35/s  (0.696s, 1471.75/s)  LR: 4.843e-04  Data: 0.016 (0.016)
Train: 308 [ 400/1251 ( 32%)]  Loss: 3.358 (3.59)  Time: 0.672s, 1524.21/s  (0.694s, 1475.53/s)  LR: 4.843e-04  Data: 0.012 (0.015)
Train: 308 [ 450/1251 ( 36%)]  Loss: 3.443 (3.57)  Time: 0.673s, 1522.40/s  (0.694s, 1474.88/s)  LR: 4.843e-04  Data: 0.010 (0.014)
Train: 308 [ 500/1251 ( 40%)]  Loss: 3.813 (3.60)  Time: 0.671s, 1525.20/s  (0.695s, 1473.88/s)  LR: 4.843e-04  Data: 0.012 (0.014)
Train: 308 [ 550/1251 ( 44%)]  Loss: 3.910 (3.62)  Time: 0.673s, 1520.98/s  (0.695s, 1473.04/s)  LR: 4.843e-04  Data: 0.010 (0.014)
Train: 308 [ 600/1251 ( 48%)]  Loss: 3.663 (3.63)  Time: 0.674s, 1519.63/s  (0.695s, 1474.40/s)  LR: 4.843e-04  Data: 0.010 (0.013)
Train: 308 [ 650/1251 ( 52%)]  Loss: 3.803 (3.64)  Time: 0.665s, 1540.21/s  (0.695s, 1473.45/s)  LR: 4.843e-04  Data: 0.010 (0.013)
Train: 308 [ 700/1251 ( 56%)]  Loss: 3.613 (3.64)  Time: 0.673s, 1520.46/s  (0.695s, 1473.97/s)  LR: 4.843e-04  Data: 0.011 (0.013)
Train: 308 [ 750/1251 ( 60%)]  Loss: 3.602 (3.63)  Time: 0.674s, 1519.86/s  (0.694s, 1474.57/s)  LR: 4.843e-04  Data: 0.011 (0.013)
Train: 308 [ 800/1251 ( 64%)]  Loss: 3.428 (3.62)  Time: 0.695s, 1474.20/s  (0.694s, 1475.10/s)  LR: 4.843e-04  Data: 0.011 (0.013)
Train: 308 [ 850/1251 ( 68%)]  Loss: 3.440 (3.61)  Time: 0.692s, 1480.62/s  (0.694s, 1475.29/s)  LR: 4.843e-04  Data: 0.010 (0.013)
Train: 308 [ 900/1251 ( 72%)]  Loss: 3.832 (3.62)  Time: 0.700s, 1463.79/s  (0.694s, 1475.82/s)  LR: 4.843e-04  Data: 0.010 (0.012)
Train: 308 [ 950/1251 ( 76%)]  Loss: 3.610 (3.62)  Time: 0.673s, 1521.81/s  (0.693s, 1476.59/s)  LR: 4.843e-04  Data: 0.011 (0.012)
Train: 308 [1000/1251 ( 80%)]  Loss: 3.614 (3.62)  Time: 0.737s, 1388.77/s  (0.693s, 1476.62/s)  LR: 4.843e-04  Data: 0.015 (0.012)
Train: 308 [1050/1251 ( 84%)]  Loss: 3.429 (3.61)  Time: 0.708s, 1446.79/s  (0.694s, 1476.27/s)  LR: 4.843e-04  Data: 0.011 (0.012)
Train: 308 [1100/1251 ( 88%)]  Loss: 3.649 (3.62)  Time: 0.725s, 1411.84/s  (0.694s, 1475.71/s)  LR: 4.843e-04  Data: 0.010 (0.012)
Train: 308 [1150/1251 ( 92%)]  Loss: 3.680 (3.62)  Time: 0.709s, 1444.02/s  (0.694s, 1475.77/s)  LR: 4.843e-04  Data: 0.009 (0.012)
Train: 308 [1200/1251 ( 96%)]  Loss: 3.373 (3.61)  Time: 0.677s, 1513.54/s  (0.694s, 1475.99/s)  LR: 4.843e-04  Data: 0.011 (0.012)
Train: 308 [1250/1251 (100%)]  Loss: 3.867 (3.62)  Time: 0.659s, 1554.59/s  (0.694s, 1476.52/s)  LR: 4.843e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.444 (1.444)  Loss:  0.8691 (0.8691)  Acc@1: 89.4531 (89.4531)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.136 (0.576)  Loss:  0.9873 (1.4646)  Acc@1: 84.9057 (73.1120)  Acc@5: 96.1085 (91.2660)
Train: 309 [   0/1251 (  0%)]  Loss: 3.593 (3.59)  Time: 2.112s,  484.76/s  (2.112s,  484.76/s)  LR: 4.817e-04  Data: 1.467 (1.467)
Train: 309 [  50/1251 (  4%)]  Loss: 3.643 (3.62)  Time: 0.674s, 1519.12/s  (0.721s, 1420.36/s)  LR: 4.817e-04  Data: 0.013 (0.045)
Train: 309 [ 100/1251 (  8%)]  Loss: 3.773 (3.67)  Time: 0.693s, 1477.50/s  (0.707s, 1447.93/s)  LR: 4.817e-04  Data: 0.009 (0.028)
Train: 309 [ 150/1251 ( 12%)]  Loss: 3.798 (3.70)  Time: 0.672s, 1522.81/s  (0.701s, 1460.10/s)  LR: 4.817e-04  Data: 0.010 (0.022)
Train: 309 [ 200/1251 ( 16%)]  Loss: 3.692 (3.70)  Time: 0.680s, 1505.76/s  (0.698s, 1468.03/s)  LR: 4.817e-04  Data: 0.009 (0.019)
Train: 309 [ 250/1251 ( 20%)]  Loss: 3.580 (3.68)  Time: 0.670s, 1527.71/s  (0.697s, 1469.76/s)  LR: 4.817e-04  Data: 0.012 (0.018)
Train: 309 [ 300/1251 ( 24%)]  Loss: 3.600 (3.67)  Time: 0.795s, 1288.02/s  (0.698s, 1467.98/s)  LR: 4.817e-04  Data: 0.010 (0.016)
Train: 309 [ 350/1251 ( 28%)]  Loss: 3.995 (3.71)  Time: 0.711s, 1439.27/s  (0.697s, 1469.52/s)  LR: 4.817e-04  Data: 0.010 (0.015)
Train: 309 [ 400/1251 ( 32%)]  Loss: 3.998 (3.74)  Time: 0.673s, 1520.87/s  (0.697s, 1468.55/s)  LR: 4.817e-04  Data: 0.011 (0.015)
Train: 309 [ 450/1251 ( 36%)]  Loss: 3.386 (3.71)  Time: 0.671s, 1526.77/s  (0.696s, 1470.63/s)  LR: 4.817e-04  Data: 0.012 (0.014)
Train: 309 [ 500/1251 ( 40%)]  Loss: 3.724 (3.71)  Time: 0.697s, 1468.16/s  (0.695s, 1472.85/s)  LR: 4.817e-04  Data: 0.010 (0.014)
Train: 309 [ 550/1251 ( 44%)]  Loss: 3.437 (3.68)  Time: 0.671s, 1526.33/s  (0.695s, 1473.56/s)  LR: 4.817e-04  Data: 0.011 (0.014)
Train: 309 [ 600/1251 ( 48%)]  Loss: 3.516 (3.67)  Time: 0.670s, 1527.81/s  (0.695s, 1473.12/s)  LR: 4.817e-04  Data: 0.010 (0.013)
Train: 309 [ 650/1251 ( 52%)]  Loss: 3.415 (3.65)  Time: 0.705s, 1451.78/s  (0.695s, 1472.59/s)  LR: 4.817e-04  Data: 0.009 (0.013)
Train: 309 [ 700/1251 ( 56%)]  Loss: 3.839 (3.67)  Time: 0.714s, 1434.71/s  (0.695s, 1473.34/s)  LR: 4.817e-04  Data: 0.010 (0.013)
Train: 309 [ 750/1251 ( 60%)]  Loss: 3.844 (3.68)  Time: 0.670s, 1527.63/s  (0.694s, 1474.78/s)  LR: 4.817e-04  Data: 0.010 (0.013)
Train: 309 [ 800/1251 ( 64%)]  Loss: 3.665 (3.68)  Time: 0.672s, 1524.34/s  (0.694s, 1474.80/s)  LR: 4.817e-04  Data: 0.010 (0.013)
Train: 309 [ 850/1251 ( 68%)]  Loss: 3.946 (3.69)  Time: 0.670s, 1527.84/s  (0.694s, 1475.45/s)  LR: 4.817e-04  Data: 0.009 (0.013)
Train: 309 [ 900/1251 ( 72%)]  Loss: 3.355 (3.67)  Time: 0.672s, 1523.92/s  (0.694s, 1476.15/s)  LR: 4.817e-04  Data: 0.010 (0.012)
Train: 309 [ 950/1251 ( 76%)]  Loss: 3.523 (3.67)  Time: 0.672s, 1524.05/s  (0.694s, 1475.86/s)  LR: 4.817e-04  Data: 0.011 (0.012)
Train: 309 [1000/1251 ( 80%)]  Loss: 3.871 (3.68)  Time: 0.698s, 1467.39/s  (0.694s, 1475.96/s)  LR: 4.817e-04  Data: 0.009 (0.012)
Train: 309 [1050/1251 ( 84%)]  Loss: 3.597 (3.67)  Time: 0.677s, 1512.36/s  (0.694s, 1476.29/s)  LR: 4.817e-04  Data: 0.010 (0.012)
Train: 309 [1100/1251 ( 88%)]  Loss: 3.537 (3.67)  Time: 0.710s, 1441.85/s  (0.693s, 1476.91/s)  LR: 4.817e-04  Data: 0.014 (0.012)
Train: 309 [1150/1251 ( 92%)]  Loss: 3.492 (3.66)  Time: 0.678s, 1510.92/s  (0.693s, 1477.24/s)  LR: 4.817e-04  Data: 0.011 (0.012)
Train: 309 [1200/1251 ( 96%)]  Loss: 3.569 (3.66)  Time: 0.670s, 1527.69/s  (0.693s, 1477.09/s)  LR: 4.817e-04  Data: 0.010 (0.012)
Train: 309 [1250/1251 (100%)]  Loss: 3.581 (3.65)  Time: 0.661s, 1549.86/s  (0.693s, 1477.52/s)  LR: 4.817e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.515 (1.515)  Loss:  0.8647 (0.8647)  Acc@1: 89.9414 (89.9414)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.136 (0.577)  Loss:  0.9575 (1.4358)  Acc@1: 83.4906 (73.5280)  Acc@5: 95.9906 (91.7660)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-304.pth.tar', 73.88999993408203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-292.pth.tar', 73.81200004394532)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-306.pth.tar', 73.75800008789062)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-302.pth.tar', 73.66800009765625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-300.pth.tar', 73.64799993652343)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-303.pth.tar', 73.62400009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-296.pth.tar', 73.6020000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-301.pth.tar', 73.56200009277343)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-309.pth.tar', 73.52800006835938)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-293.pth.tar', 73.52199999023438)

Train: 310 [   0/1251 (  0%)]  Loss: 3.466 (3.47)  Time: 2.140s,  478.51/s  (2.140s,  478.51/s)  LR: 4.791e-04  Data: 1.524 (1.524)
Train: 310 [  50/1251 (  4%)]  Loss: 3.963 (3.71)  Time: 0.704s, 1454.41/s  (0.727s, 1408.99/s)  LR: 4.791e-04  Data: 0.011 (0.049)
Train: 310 [ 100/1251 (  8%)]  Loss: 3.507 (3.64)  Time: 0.673s, 1521.73/s  (0.712s, 1437.76/s)  LR: 4.791e-04  Data: 0.011 (0.030)
Train: 310 [ 150/1251 ( 12%)]  Loss: 3.589 (3.63)  Time: 0.671s, 1526.03/s  (0.707s, 1448.45/s)  LR: 4.791e-04  Data: 0.011 (0.024)
Train: 310 [ 200/1251 ( 16%)]  Loss: 4.077 (3.72)  Time: 0.673s, 1521.29/s  (0.702s, 1458.26/s)  LR: 4.791e-04  Data: 0.011 (0.020)
Train: 310 [ 250/1251 ( 20%)]  Loss: 3.467 (3.68)  Time: 0.673s, 1521.82/s  (0.699s, 1464.80/s)  LR: 4.791e-04  Data: 0.010 (0.018)
Train: 310 [ 300/1251 ( 24%)]  Loss: 3.525 (3.66)  Time: 0.676s, 1515.23/s  (0.698s, 1467.39/s)  LR: 4.791e-04  Data: 0.009 (0.017)
Train: 310 [ 350/1251 ( 28%)]  Loss: 3.792 (3.67)  Time: 0.730s, 1403.14/s  (0.697s, 1469.68/s)  LR: 4.791e-04  Data: 0.009 (0.016)
Train: 310 [ 400/1251 ( 32%)]  Loss: 3.782 (3.69)  Time: 0.682s, 1502.17/s  (0.696s, 1472.15/s)  LR: 4.791e-04  Data: 0.009 (0.015)
Train: 310 [ 450/1251 ( 36%)]  Loss: 3.611 (3.68)  Time: 0.676s, 1514.78/s  (0.695s, 1472.40/s)  LR: 4.791e-04  Data: 0.011 (0.015)
Train: 310 [ 500/1251 ( 40%)]  Loss: 3.778 (3.69)  Time: 0.671s, 1525.40/s  (0.695s, 1473.70/s)  LR: 4.791e-04  Data: 0.013 (0.014)
Train: 310 [ 550/1251 ( 44%)]  Loss: 3.248 (3.65)  Time: 0.683s, 1500.31/s  (0.695s, 1473.97/s)  LR: 4.791e-04  Data: 0.011 (0.014)
Train: 310 [ 600/1251 ( 48%)]  Loss: 3.766 (3.66)  Time: 0.698s, 1466.96/s  (0.695s, 1474.21/s)  LR: 4.791e-04  Data: 0.010 (0.014)
Train: 310 [ 650/1251 ( 52%)]  Loss: 3.617 (3.66)  Time: 0.703s, 1456.89/s  (0.695s, 1473.84/s)  LR: 4.791e-04  Data: 0.010 (0.014)
Train: 310 [ 700/1251 ( 56%)]  Loss: 3.674 (3.66)  Time: 0.731s, 1400.40/s  (0.695s, 1474.43/s)  LR: 4.791e-04  Data: 0.009 (0.013)
Train: 310 [ 750/1251 ( 60%)]  Loss: 3.953 (3.68)  Time: 0.673s, 1521.06/s  (0.694s, 1475.29/s)  LR: 4.791e-04  Data: 0.011 (0.013)
Train: 310 [ 800/1251 ( 64%)]  Loss: 3.443 (3.66)  Time: 0.686s, 1492.03/s  (0.694s, 1475.62/s)  LR: 4.791e-04  Data: 0.009 (0.013)
Train: 310 [ 850/1251 ( 68%)]  Loss: 3.903 (3.68)  Time: 0.673s, 1521.77/s  (0.694s, 1475.68/s)  LR: 4.791e-04  Data: 0.011 (0.013)
Train: 310 [ 900/1251 ( 72%)]  Loss: 3.997 (3.69)  Time: 0.689s, 1486.55/s  (0.694s, 1476.54/s)  LR: 4.791e-04  Data: 0.010 (0.013)
Train: 310 [ 950/1251 ( 76%)]  Loss: 3.551 (3.69)  Time: 0.673s, 1521.05/s  (0.694s, 1476.51/s)  LR: 4.791e-04  Data: 0.010 (0.013)
Train: 310 [1000/1251 ( 80%)]  Loss: 3.649 (3.68)  Time: 0.721s, 1419.65/s  (0.694s, 1476.44/s)  LR: 4.791e-04  Data: 0.010 (0.013)
Train: 310 [1050/1251 ( 84%)]  Loss: 3.681 (3.68)  Time: 0.707s, 1448.44/s  (0.693s, 1476.76/s)  LR: 4.791e-04  Data: 0.009 (0.012)
Train: 310 [1100/1251 ( 88%)]  Loss: 3.803 (3.69)  Time: 0.707s, 1448.40/s  (0.693s, 1477.37/s)  LR: 4.791e-04  Data: 0.040 (0.012)
Train: 310 [1150/1251 ( 92%)]  Loss: 3.516 (3.68)  Time: 0.700s, 1462.87/s  (0.693s, 1477.54/s)  LR: 4.791e-04  Data: 0.011 (0.012)
Train: 310 [1200/1251 ( 96%)]  Loss: 3.635 (3.68)  Time: 0.708s, 1446.79/s  (0.693s, 1477.39/s)  LR: 4.791e-04  Data: 0.011 (0.012)
Train: 310 [1250/1251 (100%)]  Loss: 3.739 (3.68)  Time: 0.656s, 1561.79/s  (0.693s, 1477.76/s)  LR: 4.791e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.584 (1.584)  Loss:  0.8511 (0.8511)  Acc@1: 89.2578 (89.2578)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.136 (0.571)  Loss:  0.9863 (1.4680)  Acc@1: 83.9623 (73.2680)  Acc@5: 96.2264 (91.5120)
Train: 311 [   0/1251 (  0%)]  Loss: 3.285 (3.29)  Time: 2.224s,  460.53/s  (2.224s,  460.53/s)  LR: 4.765e-04  Data: 1.563 (1.563)
Train: 311 [  50/1251 (  4%)]  Loss: 3.637 (3.46)  Time: 0.702s, 1459.11/s  (0.733s, 1396.70/s)  LR: 4.765e-04  Data: 0.010 (0.047)
Train: 311 [ 100/1251 (  8%)]  Loss: 3.927 (3.62)  Time: 0.708s, 1446.82/s  (0.714s, 1434.75/s)  LR: 4.765e-04  Data: 0.013 (0.029)
Train: 311 [ 150/1251 ( 12%)]  Loss: 3.626 (3.62)  Time: 0.761s, 1346.15/s  (0.707s, 1449.38/s)  LR: 4.765e-04  Data: 0.010 (0.023)
Train: 311 [ 200/1251 ( 16%)]  Loss: 3.647 (3.62)  Time: 0.673s, 1522.28/s  (0.701s, 1459.90/s)  LR: 4.765e-04  Data: 0.010 (0.020)
Train: 311 [ 250/1251 ( 20%)]  Loss: 3.655 (3.63)  Time: 0.719s, 1423.70/s  (0.700s, 1462.82/s)  LR: 4.765e-04  Data: 0.010 (0.018)
Train: 311 [ 300/1251 ( 24%)]  Loss: 3.570 (3.62)  Time: 0.707s, 1448.27/s  (0.698s, 1467.87/s)  LR: 4.765e-04  Data: 0.009 (0.017)
Train: 311 [ 350/1251 ( 28%)]  Loss: 3.673 (3.63)  Time: 0.707s, 1449.12/s  (0.697s, 1469.79/s)  LR: 4.765e-04  Data: 0.010 (0.016)
Train: 311 [ 400/1251 ( 32%)]  Loss: 3.893 (3.66)  Time: 0.671s, 1527.16/s  (0.697s, 1469.65/s)  LR: 4.765e-04  Data: 0.010 (0.015)
Train: 311 [ 450/1251 ( 36%)]  Loss: 3.595 (3.65)  Time: 0.670s, 1527.90/s  (0.697s, 1469.98/s)  LR: 4.765e-04  Data: 0.010 (0.015)
Train: 311 [ 500/1251 ( 40%)]  Loss: 3.213 (3.61)  Time: 0.673s, 1521.91/s  (0.696s, 1471.56/s)  LR: 4.765e-04  Data: 0.010 (0.014)
Train: 311 [ 550/1251 ( 44%)]  Loss: 3.601 (3.61)  Time: 0.709s, 1443.78/s  (0.696s, 1471.27/s)  LR: 4.765e-04  Data: 0.010 (0.014)
Train: 311 [ 600/1251 ( 48%)]  Loss: 3.251 (3.58)  Time: 0.701s, 1460.67/s  (0.696s, 1471.74/s)  LR: 4.765e-04  Data: 0.010 (0.014)
Train: 311 [ 650/1251 ( 52%)]  Loss: 3.396 (3.57)  Time: 0.713s, 1435.81/s  (0.695s, 1472.43/s)  LR: 4.765e-04  Data: 0.010 (0.013)
Train: 311 [ 700/1251 ( 56%)]  Loss: 3.623 (3.57)  Time: 0.669s, 1529.88/s  (0.695s, 1472.85/s)  LR: 4.765e-04  Data: 0.009 (0.013)
Train: 311 [ 750/1251 ( 60%)]  Loss: 3.722 (3.58)  Time: 0.686s, 1493.50/s  (0.695s, 1473.36/s)  LR: 4.765e-04  Data: 0.012 (0.013)
Train: 311 [ 800/1251 ( 64%)]  Loss: 4.041 (3.61)  Time: 0.681s, 1504.41/s  (0.695s, 1474.00/s)  LR: 4.765e-04  Data: 0.010 (0.013)
Train: 311 [ 850/1251 ( 68%)]  Loss: 3.909 (3.63)  Time: 0.670s, 1528.08/s  (0.695s, 1473.35/s)  LR: 4.765e-04  Data: 0.010 (0.013)
Train: 311 [ 900/1251 ( 72%)]  Loss: 3.771 (3.63)  Time: 0.675s, 1517.96/s  (0.695s, 1473.43/s)  LR: 4.765e-04  Data: 0.010 (0.013)
Train: 311 [ 950/1251 ( 76%)]  Loss: 3.608 (3.63)  Time: 0.672s, 1524.36/s  (0.695s, 1473.88/s)  LR: 4.765e-04  Data: 0.012 (0.012)
Train: 311 [1000/1251 ( 80%)]  Loss: 3.763 (3.64)  Time: 0.690s, 1485.00/s  (0.695s, 1474.19/s)  LR: 4.765e-04  Data: 0.011 (0.012)
Train: 311 [1050/1251 ( 84%)]  Loss: 3.642 (3.64)  Time: 0.671s, 1527.07/s  (0.694s, 1474.77/s)  LR: 4.765e-04  Data: 0.010 (0.012)
Train: 311 [1100/1251 ( 88%)]  Loss: 3.664 (3.64)  Time: 0.672s, 1523.91/s  (0.694s, 1475.05/s)  LR: 4.765e-04  Data: 0.013 (0.012)
Train: 311 [1150/1251 ( 92%)]  Loss: 3.482 (3.63)  Time: 0.727s, 1408.86/s  (0.694s, 1475.58/s)  LR: 4.765e-04  Data: 0.011 (0.012)
Train: 311 [1200/1251 ( 96%)]  Loss: 3.719 (3.64)  Time: 0.755s, 1356.90/s  (0.694s, 1475.80/s)  LR: 4.765e-04  Data: 0.008 (0.012)
Train: 311 [1250/1251 (100%)]  Loss: 3.225 (3.62)  Time: 0.668s, 1532.28/s  (0.694s, 1475.25/s)  LR: 4.765e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.491 (1.491)  Loss:  0.7695 (0.7695)  Acc@1: 89.1602 (89.1602)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.136 (0.584)  Loss:  0.8506 (1.3314)  Acc@1: 83.3727 (73.4960)  Acc@5: 96.3443 (91.6780)
Train: 312 [   0/1251 (  0%)]  Loss: 3.939 (3.94)  Time: 2.334s,  438.64/s  (2.334s,  438.64/s)  LR: 4.739e-04  Data: 1.720 (1.720)
Train: 312 [  50/1251 (  4%)]  Loss: 3.605 (3.77)  Time: 0.671s, 1525.06/s  (0.733s, 1397.01/s)  LR: 4.739e-04  Data: 0.011 (0.057)
Train: 312 [ 100/1251 (  8%)]  Loss: 3.925 (3.82)  Time: 0.692s, 1479.26/s  (0.711s, 1440.02/s)  LR: 4.739e-04  Data: 0.010 (0.034)
Train: 312 [ 150/1251 ( 12%)]  Loss: 3.312 (3.69)  Time: 0.717s, 1428.73/s  (0.704s, 1455.33/s)  LR: 4.739e-04  Data: 0.010 (0.026)
Train: 312 [ 200/1251 ( 16%)]  Loss: 3.569 (3.67)  Time: 0.704s, 1454.93/s  (0.700s, 1462.06/s)  LR: 4.739e-04  Data: 0.010 (0.022)
Train: 312 [ 250/1251 ( 20%)]  Loss: 3.546 (3.65)  Time: 0.674s, 1519.89/s  (0.697s, 1469.13/s)  LR: 4.739e-04  Data: 0.013 (0.020)
Train: 312 [ 300/1251 ( 24%)]  Loss: 3.463 (3.62)  Time: 0.675s, 1517.52/s  (0.695s, 1472.40/s)  LR: 4.739e-04  Data: 0.011 (0.018)
Train: 312 [ 350/1251 ( 28%)]  Loss: 3.446 (3.60)  Time: 0.757s, 1351.97/s  (0.697s, 1470.12/s)  LR: 4.739e-04  Data: 0.012 (0.017)
Train: 312 [ 400/1251 ( 32%)]  Loss: 3.517 (3.59)  Time: 0.701s, 1461.49/s  (0.696s, 1471.65/s)  LR: 4.739e-04  Data: 0.009 (0.016)
Train: 312 [ 450/1251 ( 36%)]  Loss: 3.575 (3.59)  Time: 0.673s, 1520.78/s  (0.695s, 1472.39/s)  LR: 4.739e-04  Data: 0.011 (0.016)
Train: 312 [ 500/1251 ( 40%)]  Loss: 3.435 (3.58)  Time: 0.668s, 1532.05/s  (0.695s, 1473.22/s)  LR: 4.739e-04  Data: 0.010 (0.015)
Train: 312 [ 550/1251 ( 44%)]  Loss: 3.714 (3.59)  Time: 0.674s, 1519.47/s  (0.695s, 1472.60/s)  LR: 4.739e-04  Data: 0.009 (0.015)
Train: 312 [ 600/1251 ( 48%)]  Loss: 3.462 (3.58)  Time: 0.667s, 1534.24/s  (0.695s, 1473.65/s)  LR: 4.739e-04  Data: 0.010 (0.014)
Train: 312 [ 650/1251 ( 52%)]  Loss: 3.633 (3.58)  Time: 0.675s, 1517.42/s  (0.694s, 1475.16/s)  LR: 4.739e-04  Data: 0.011 (0.014)
Train: 312 [ 700/1251 ( 56%)]  Loss: 4.072 (3.61)  Time: 0.692s, 1479.71/s  (0.694s, 1474.54/s)  LR: 4.739e-04  Data: 0.009 (0.014)
Train: 312 [ 750/1251 ( 60%)]  Loss: 3.447 (3.60)  Time: 0.777s, 1317.54/s  (0.694s, 1475.08/s)  LR: 4.739e-04  Data: 0.010 (0.014)
Train: 312 [ 800/1251 ( 64%)]  Loss: 3.523 (3.60)  Time: 0.674s, 1519.13/s  (0.694s, 1475.87/s)  LR: 4.739e-04  Data: 0.013 (0.013)
Train: 312 [ 850/1251 ( 68%)]  Loss: 3.845 (3.61)  Time: 0.703s, 1457.42/s  (0.693s, 1476.80/s)  LR: 4.739e-04  Data: 0.009 (0.013)
Train: 312 [ 900/1251 ( 72%)]  Loss: 3.480 (3.61)  Time: 0.672s, 1524.43/s  (0.693s, 1477.54/s)  LR: 4.739e-04  Data: 0.011 (0.013)
Train: 312 [ 950/1251 ( 76%)]  Loss: 3.535 (3.60)  Time: 0.673s, 1521.84/s  (0.693s, 1477.65/s)  LR: 4.739e-04  Data: 0.011 (0.013)
Train: 312 [1000/1251 ( 80%)]  Loss: 3.826 (3.61)  Time: 0.676s, 1515.72/s  (0.693s, 1477.82/s)  LR: 4.739e-04  Data: 0.010 (0.013)
Train: 312 [1050/1251 ( 84%)]  Loss: 3.768 (3.62)  Time: 0.742s, 1380.44/s  (0.693s, 1477.32/s)  LR: 4.739e-04  Data: 0.013 (0.013)
Train: 312 [1100/1251 ( 88%)]  Loss: 3.844 (3.63)  Time: 0.705s, 1452.18/s  (0.693s, 1477.55/s)  LR: 4.739e-04  Data: 0.010 (0.013)
Train: 312 [1150/1251 ( 92%)]  Loss: 3.604 (3.63)  Time: 0.673s, 1521.15/s  (0.693s, 1477.80/s)  LR: 4.739e-04  Data: 0.010 (0.013)
Train: 312 [1200/1251 ( 96%)]  Loss: 3.794 (3.64)  Time: 0.719s, 1423.82/s  (0.693s, 1477.47/s)  LR: 4.739e-04  Data: 0.016 (0.012)
Train: 312 [1250/1251 (100%)]  Loss: 3.841 (3.64)  Time: 0.656s, 1561.28/s  (0.693s, 1477.35/s)  LR: 4.739e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.560 (1.560)  Loss:  0.8604 (0.8604)  Acc@1: 90.0391 (90.0391)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.136 (0.578)  Loss:  0.9990 (1.4682)  Acc@1: 82.9009 (73.6720)  Acc@5: 96.3443 (91.7680)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-304.pth.tar', 73.88999993408203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-292.pth.tar', 73.81200004394532)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-306.pth.tar', 73.75800008789062)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-312.pth.tar', 73.67200007080078)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-302.pth.tar', 73.66800009765625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-300.pth.tar', 73.64799993652343)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-303.pth.tar', 73.62400009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-296.pth.tar', 73.6020000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-301.pth.tar', 73.56200009277343)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-309.pth.tar', 73.52800006835938)

Train: 313 [   0/1251 (  0%)]  Loss: 3.883 (3.88)  Time: 2.084s,  491.41/s  (2.084s,  491.41/s)  LR: 4.713e-04  Data: 1.471 (1.471)
Train: 313 [  50/1251 (  4%)]  Loss: 3.654 (3.77)  Time: 0.673s, 1521.44/s  (0.732s, 1399.61/s)  LR: 4.713e-04  Data: 0.010 (0.048)
Train: 313 [ 100/1251 (  8%)]  Loss: 3.407 (3.65)  Time: 0.676s, 1513.79/s  (0.712s, 1437.72/s)  LR: 4.713e-04  Data: 0.011 (0.029)
Train: 313 [ 150/1251 ( 12%)]  Loss: 3.471 (3.60)  Time: 0.707s, 1449.29/s  (0.705s, 1452.93/s)  LR: 4.713e-04  Data: 0.011 (0.023)
Train: 313 [ 200/1251 ( 16%)]  Loss: 3.457 (3.57)  Time: 0.716s, 1429.64/s  (0.701s, 1460.56/s)  LR: 4.713e-04  Data: 0.010 (0.020)
Train: 313 [ 250/1251 ( 20%)]  Loss: 3.253 (3.52)  Time: 0.697s, 1469.00/s  (0.699s, 1464.02/s)  LR: 4.713e-04  Data: 0.014 (0.018)
Train: 313 [ 300/1251 ( 24%)]  Loss: 3.602 (3.53)  Time: 0.830s, 1234.10/s  (0.699s, 1463.94/s)  LR: 4.713e-04  Data: 0.010 (0.017)
Train: 313 [ 350/1251 ( 28%)]  Loss: 3.667 (3.55)  Time: 0.707s, 1448.98/s  (0.699s, 1464.75/s)  LR: 4.713e-04  Data: 0.011 (0.016)
Train: 313 [ 400/1251 ( 32%)]  Loss: 3.659 (3.56)  Time: 0.723s, 1416.66/s  (0.698s, 1466.42/s)  LR: 4.713e-04  Data: 0.009 (0.015)
Train: 313 [ 450/1251 ( 36%)]  Loss: 3.430 (3.55)  Time: 0.681s, 1503.31/s  (0.697s, 1469.37/s)  LR: 4.713e-04  Data: 0.011 (0.015)
Train: 313 [ 500/1251 ( 40%)]  Loss: 3.568 (3.55)  Time: 0.709s, 1444.53/s  (0.696s, 1471.03/s)  LR: 4.713e-04  Data: 0.010 (0.014)
Train: 313 [ 550/1251 ( 44%)]  Loss: 3.846 (3.57)  Time: 0.704s, 1454.90/s  (0.696s, 1470.38/s)  LR: 4.713e-04  Data: 0.009 (0.014)
Train: 313 [ 600/1251 ( 48%)]  Loss: 3.739 (3.59)  Time: 0.707s, 1447.63/s  (0.696s, 1470.84/s)  LR: 4.713e-04  Data: 0.009 (0.014)
Train: 313 [ 650/1251 ( 52%)]  Loss: 3.749 (3.60)  Time: 0.677s, 1512.78/s  (0.695s, 1472.55/s)  LR: 4.713e-04  Data: 0.011 (0.013)
Train: 313 [ 700/1251 ( 56%)]  Loss: 3.317 (3.58)  Time: 0.676s, 1515.78/s  (0.695s, 1473.45/s)  LR: 4.713e-04  Data: 0.011 (0.013)
Train: 313 [ 750/1251 ( 60%)]  Loss: 3.807 (3.59)  Time: 0.669s, 1529.70/s  (0.695s, 1473.96/s)  LR: 4.713e-04  Data: 0.009 (0.013)
Train: 313 [ 800/1251 ( 64%)]  Loss: 3.942 (3.61)  Time: 0.719s, 1424.19/s  (0.694s, 1475.36/s)  LR: 4.713e-04  Data: 0.010 (0.013)
Train: 313 [ 850/1251 ( 68%)]  Loss: 3.524 (3.61)  Time: 0.672s, 1523.45/s  (0.694s, 1476.46/s)  LR: 4.713e-04  Data: 0.010 (0.013)
Train: 313 [ 900/1251 ( 72%)]  Loss: 3.841 (3.62)  Time: 0.742s, 1380.02/s  (0.694s, 1475.95/s)  LR: 4.713e-04  Data: 0.010 (0.013)
Train: 313 [ 950/1251 ( 76%)]  Loss: 3.770 (3.63)  Time: 0.712s, 1437.41/s  (0.693s, 1476.59/s)  LR: 4.713e-04  Data: 0.022 (0.013)
Train: 313 [1000/1251 ( 80%)]  Loss: 3.753 (3.64)  Time: 0.674s, 1518.38/s  (0.693s, 1477.28/s)  LR: 4.713e-04  Data: 0.011 (0.012)
Train: 313 [1050/1251 ( 84%)]  Loss: 3.892 (3.65)  Time: 0.668s, 1532.79/s  (0.693s, 1477.35/s)  LR: 4.713e-04  Data: 0.010 (0.012)
Train: 313 [1100/1251 ( 88%)]  Loss: 3.968 (3.66)  Time: 0.725s, 1412.04/s  (0.693s, 1476.81/s)  LR: 4.713e-04  Data: 0.013 (0.012)
Train: 313 [1150/1251 ( 92%)]  Loss: 3.677 (3.66)  Time: 0.706s, 1450.55/s  (0.694s, 1475.44/s)  LR: 4.713e-04  Data: 0.012 (0.012)
Train: 313 [1200/1251 ( 96%)]  Loss: 3.932 (3.67)  Time: 0.788s, 1299.57/s  (0.695s, 1474.34/s)  LR: 4.713e-04  Data: 0.011 (0.012)
Train: 313 [1250/1251 (100%)]  Loss: 3.754 (3.68)  Time: 0.708s, 1445.34/s  (0.695s, 1472.38/s)  LR: 4.713e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.568 (1.568)  Loss:  0.8657 (0.8657)  Acc@1: 88.6719 (88.6719)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.136 (0.565)  Loss:  0.9443 (1.4617)  Acc@1: 84.4340 (73.4480)  Acc@5: 95.7547 (91.6140)
Train: 314 [   0/1251 (  0%)]  Loss: 3.934 (3.93)  Time: 2.210s,  463.26/s  (2.210s,  463.26/s)  LR: 4.687e-04  Data: 1.585 (1.585)
Train: 314 [  50/1251 (  4%)]  Loss: 3.457 (3.70)  Time: 0.672s, 1523.22/s  (0.715s, 1431.64/s)  LR: 4.687e-04  Data: 0.011 (0.050)
Train: 314 [ 100/1251 (  8%)]  Loss: 3.457 (3.62)  Time: 0.673s, 1522.37/s  (0.703s, 1455.99/s)  LR: 4.687e-04  Data: 0.010 (0.030)
Train: 314 [ 150/1251 ( 12%)]  Loss: 3.755 (3.65)  Time: 0.724s, 1413.42/s  (0.702s, 1459.72/s)  LR: 4.687e-04  Data: 0.010 (0.024)
Train: 314 [ 200/1251 ( 16%)]  Loss: 3.853 (3.69)  Time: 0.707s, 1448.43/s  (0.699s, 1464.42/s)  LR: 4.687e-04  Data: 0.010 (0.020)
Train: 314 [ 250/1251 ( 20%)]  Loss: 3.781 (3.71)  Time: 0.697s, 1469.87/s  (0.697s, 1470.04/s)  LR: 4.687e-04  Data: 0.013 (0.018)
Train: 314 [ 300/1251 ( 24%)]  Loss: 3.625 (3.69)  Time: 0.706s, 1450.95/s  (0.695s, 1472.90/s)  LR: 4.687e-04  Data: 0.010 (0.017)
Train: 314 [ 350/1251 ( 28%)]  Loss: 3.696 (3.69)  Time: 0.709s, 1444.88/s  (0.697s, 1469.40/s)  LR: 4.687e-04  Data: 0.009 (0.016)
Train: 314 [ 400/1251 ( 32%)]  Loss: 4.007 (3.73)  Time: 0.666s, 1537.42/s  (0.696s, 1471.76/s)  LR: 4.687e-04  Data: 0.011 (0.015)
Train: 314 [ 450/1251 ( 36%)]  Loss: 3.665 (3.72)  Time: 0.673s, 1520.67/s  (0.696s, 1470.92/s)  LR: 4.687e-04  Data: 0.011 (0.015)
Train: 314 [ 500/1251 ( 40%)]  Loss: 3.460 (3.70)  Time: 0.739s, 1384.99/s  (0.696s, 1471.86/s)  LR: 4.687e-04  Data: 0.010 (0.014)
Train: 314 [ 550/1251 ( 44%)]  Loss: 3.574 (3.69)  Time: 0.704s, 1454.42/s  (0.696s, 1472.13/s)  LR: 4.687e-04  Data: 0.009 (0.014)
Train: 314 [ 600/1251 ( 48%)]  Loss: 3.676 (3.69)  Time: 0.674s, 1519.59/s  (0.695s, 1473.63/s)  LR: 4.687e-04  Data: 0.011 (0.014)
Train: 314 [ 650/1251 ( 52%)]  Loss: 3.625 (3.68)  Time: 0.670s, 1527.31/s  (0.694s, 1474.85/s)  LR: 4.687e-04  Data: 0.010 (0.013)
Train: 314 [ 700/1251 ( 56%)]  Loss: 3.481 (3.67)  Time: 0.698s, 1467.97/s  (0.694s, 1475.35/s)  LR: 4.687e-04  Data: 0.009 (0.013)
Train: 314 [ 750/1251 ( 60%)]  Loss: 3.997 (3.69)  Time: 0.667s, 1536.35/s  (0.694s, 1476.38/s)  LR: 4.687e-04  Data: 0.011 (0.013)
Train: 314 [ 800/1251 ( 64%)]  Loss: 3.702 (3.69)  Time: 0.707s, 1448.25/s  (0.693s, 1476.77/s)  LR: 4.687e-04  Data: 0.010 (0.013)
Train: 314 [ 850/1251 ( 68%)]  Loss: 3.421 (3.68)  Time: 0.684s, 1497.13/s  (0.693s, 1476.58/s)  LR: 4.687e-04  Data: 0.010 (0.013)
Train: 314 [ 900/1251 ( 72%)]  Loss: 3.656 (3.67)  Time: 0.669s, 1530.36/s  (0.693s, 1476.89/s)  LR: 4.687e-04  Data: 0.010 (0.013)
Train: 314 [ 950/1251 ( 76%)]  Loss: 3.545 (3.67)  Time: 0.716s, 1431.10/s  (0.693s, 1477.08/s)  LR: 4.687e-04  Data: 0.011 (0.012)
Train: 314 [1000/1251 ( 80%)]  Loss: 3.709 (3.67)  Time: 0.687s, 1491.51/s  (0.693s, 1477.65/s)  LR: 4.687e-04  Data: 0.010 (0.012)
Train: 314 [1050/1251 ( 84%)]  Loss: 3.470 (3.66)  Time: 0.685s, 1493.86/s  (0.693s, 1478.01/s)  LR: 4.687e-04  Data: 0.008 (0.012)
Train: 314 [1100/1251 ( 88%)]  Loss: 3.519 (3.66)  Time: 0.680s, 1505.50/s  (0.692s, 1478.75/s)  LR: 4.687e-04  Data: 0.010 (0.012)
Train: 314 [1150/1251 ( 92%)]  Loss: 3.518 (3.65)  Time: 0.674s, 1518.25/s  (0.692s, 1478.71/s)  LR: 4.687e-04  Data: 0.014 (0.012)
Train: 314 [1200/1251 ( 96%)]  Loss: 3.873 (3.66)  Time: 0.717s, 1427.40/s  (0.692s, 1479.15/s)  LR: 4.687e-04  Data: 0.012 (0.012)
Train: 314 [1250/1251 (100%)]  Loss: 3.440 (3.65)  Time: 0.657s, 1558.28/s  (0.693s, 1478.58/s)  LR: 4.687e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.633 (1.633)  Loss:  0.9082 (0.9082)  Acc@1: 88.7695 (88.7695)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.137 (0.584)  Loss:  0.9341 (1.4498)  Acc@1: 85.3774 (73.9040)  Acc@5: 96.6981 (91.9200)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-314.pth.tar', 73.90399998291015)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-304.pth.tar', 73.88999993408203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-292.pth.tar', 73.81200004394532)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-306.pth.tar', 73.75800008789062)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-312.pth.tar', 73.67200007080078)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-302.pth.tar', 73.66800009765625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-300.pth.tar', 73.64799993652343)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-303.pth.tar', 73.62400009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-296.pth.tar', 73.6020000415039)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-301.pth.tar', 73.56200009277343)

Train: 315 [   0/1251 (  0%)]  Loss: 3.461 (3.46)  Time: 2.349s,  435.95/s  (2.349s,  435.95/s)  LR: 4.662e-04  Data: 1.678 (1.678)
Train: 315 [  50/1251 (  4%)]  Loss: 3.762 (3.61)  Time: 0.677s, 1512.82/s  (0.724s, 1414.99/s)  LR: 4.662e-04  Data: 0.010 (0.048)
Train: 315 [ 100/1251 (  8%)]  Loss: 3.656 (3.63)  Time: 0.674s, 1520.08/s  (0.708s, 1446.78/s)  LR: 4.662e-04  Data: 0.012 (0.029)
Train: 315 [ 150/1251 ( 12%)]  Loss: 3.474 (3.59)  Time: 0.754s, 1357.29/s  (0.700s, 1463.68/s)  LR: 4.662e-04  Data: 0.010 (0.023)
Train: 315 [ 200/1251 ( 16%)]  Loss: 3.624 (3.60)  Time: 0.679s, 1507.72/s  (0.698s, 1467.52/s)  LR: 4.662e-04  Data: 0.010 (0.020)
Train: 315 [ 250/1251 ( 20%)]  Loss: 3.711 (3.61)  Time: 0.706s, 1451.38/s  (0.698s, 1467.81/s)  LR: 4.662e-04  Data: 0.010 (0.018)
Train: 315 [ 300/1251 ( 24%)]  Loss: 3.803 (3.64)  Time: 0.674s, 1520.25/s  (0.696s, 1471.28/s)  LR: 4.662e-04  Data: 0.010 (0.017)
Train: 315 [ 350/1251 ( 28%)]  Loss: 3.719 (3.65)  Time: 0.694s, 1476.11/s  (0.696s, 1472.00/s)  LR: 4.662e-04  Data: 0.009 (0.016)
Train: 315 [ 400/1251 ( 32%)]  Loss: 3.361 (3.62)  Time: 0.706s, 1451.43/s  (0.695s, 1473.71/s)  LR: 4.662e-04  Data: 0.009 (0.015)
Train: 315 [ 450/1251 ( 36%)]  Loss: 3.852 (3.64)  Time: 0.711s, 1441.22/s  (0.694s, 1474.80/s)  LR: 4.662e-04  Data: 0.012 (0.015)
Train: 315 [ 500/1251 ( 40%)]  Loss: 3.470 (3.63)  Time: 0.683s, 1499.48/s  (0.694s, 1475.26/s)  LR: 4.662e-04  Data: 0.010 (0.014)
Train: 315 [ 550/1251 ( 44%)]  Loss: 3.762 (3.64)  Time: 0.714s, 1433.66/s  (0.694s, 1475.03/s)  LR: 4.662e-04  Data: 0.010 (0.014)
Train: 315 [ 600/1251 ( 48%)]  Loss: 3.305 (3.61)  Time: 0.671s, 1525.29/s  (0.694s, 1475.95/s)  LR: 4.662e-04  Data: 0.011 (0.014)
Train: 315 [ 650/1251 ( 52%)]  Loss: 3.537 (3.61)  Time: 0.728s, 1406.33/s  (0.693s, 1476.87/s)  LR: 4.662e-04  Data: 0.010 (0.013)
Train: 315 [ 700/1251 ( 56%)]  Loss: 3.601 (3.61)  Time: 0.671s, 1525.70/s  (0.694s, 1476.36/s)  LR: 4.662e-04  Data: 0.010 (0.013)
Train: 315 [ 750/1251 ( 60%)]  Loss: 3.454 (3.60)  Time: 0.665s, 1539.80/s  (0.693s, 1476.75/s)  LR: 4.662e-04  Data: 0.010 (0.013)
Train: 315 [ 800/1251 ( 64%)]  Loss: 3.486 (3.59)  Time: 0.676s, 1515.35/s  (0.693s, 1477.87/s)  LR: 4.662e-04  Data: 0.010 (0.013)
Train: 315 [ 850/1251 ( 68%)]  Loss: 3.376 (3.58)  Time: 0.671s, 1525.25/s  (0.693s, 1477.86/s)  LR: 4.662e-04  Data: 0.010 (0.013)
Train: 315 [ 900/1251 ( 72%)]  Loss: 3.476 (3.57)  Time: 0.671s, 1526.12/s  (0.693s, 1478.42/s)  LR: 4.662e-04  Data: 0.010 (0.013)
Train: 315 [ 950/1251 ( 76%)]  Loss: 3.663 (3.58)  Time: 0.706s, 1451.00/s  (0.693s, 1477.95/s)  LR: 4.662e-04  Data: 0.010 (0.012)
Train: 315 [1000/1251 ( 80%)]  Loss: 3.415 (3.57)  Time: 0.679s, 1508.35/s  (0.693s, 1477.60/s)  LR: 4.662e-04  Data: 0.010 (0.012)
Train: 315 [1050/1251 ( 84%)]  Loss: 3.831 (3.58)  Time: 0.669s, 1531.62/s  (0.693s, 1477.60/s)  LR: 4.662e-04  Data: 0.010 (0.012)
Train: 315 [1100/1251 ( 88%)]  Loss: 3.687 (3.59)  Time: 0.757s, 1352.24/s  (0.693s, 1478.11/s)  LR: 4.662e-04  Data: 0.011 (0.012)
Train: 315 [1150/1251 ( 92%)]  Loss: 3.512 (3.58)  Time: 0.672s, 1524.73/s  (0.693s, 1478.48/s)  LR: 4.662e-04  Data: 0.013 (0.012)
Train: 315 [1200/1251 ( 96%)]  Loss: 3.702 (3.59)  Time: 0.671s, 1525.90/s  (0.692s, 1479.16/s)  LR: 4.662e-04  Data: 0.010 (0.012)
Train: 315 [1250/1251 (100%)]  Loss: 4.173 (3.61)  Time: 0.655s, 1562.17/s  (0.692s, 1478.95/s)  LR: 4.662e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.567 (1.567)  Loss:  0.7524 (0.7524)  Acc@1: 89.7461 (89.7461)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.136 (0.585)  Loss:  0.9038 (1.3502)  Acc@1: 84.1981 (73.7900)  Acc@5: 96.1085 (91.9260)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-314.pth.tar', 73.90399998291015)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-304.pth.tar', 73.88999993408203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-292.pth.tar', 73.81200004394532)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-315.pth.tar', 73.7900001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-306.pth.tar', 73.75800008789062)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-312.pth.tar', 73.67200007080078)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-302.pth.tar', 73.66800009765625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-300.pth.tar', 73.64799993652343)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-303.pth.tar', 73.62400009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-296.pth.tar', 73.6020000415039)

Train: 316 [   0/1251 (  0%)]  Loss: 3.423 (3.42)  Time: 2.360s,  433.94/s  (2.360s,  433.94/s)  LR: 4.636e-04  Data: 1.709 (1.709)
Train: 316 [  50/1251 (  4%)]  Loss: 3.808 (3.62)  Time: 0.718s, 1425.40/s  (0.737s, 1389.83/s)  LR: 4.636e-04  Data: 0.016 (0.054)
Train: 316 [ 100/1251 (  8%)]  Loss: 3.814 (3.68)  Time: 0.691s, 1481.90/s  (0.718s, 1426.77/s)  LR: 4.636e-04  Data: 0.011 (0.033)
Train: 316 [ 150/1251 ( 12%)]  Loss: 3.779 (3.71)  Time: 0.688s, 1488.03/s  (0.708s, 1445.92/s)  LR: 4.636e-04  Data: 0.009 (0.025)
Train: 316 [ 200/1251 ( 16%)]  Loss: 3.600 (3.68)  Time: 0.724s, 1415.05/s  (0.703s, 1455.88/s)  LR: 4.636e-04  Data: 0.009 (0.022)
Train: 316 [ 250/1251 ( 20%)]  Loss: 3.828 (3.71)  Time: 0.676s, 1514.20/s  (0.700s, 1462.68/s)  LR: 4.636e-04  Data: 0.010 (0.019)
Train: 316 [ 300/1251 ( 24%)]  Loss: 3.469 (3.67)  Time: 0.671s, 1526.53/s  (0.698s, 1466.59/s)  LR: 4.636e-04  Data: 0.010 (0.018)
Train: 316 [ 350/1251 ( 28%)]  Loss: 3.545 (3.66)  Time: 0.683s, 1499.01/s  (0.698s, 1467.05/s)  LR: 4.636e-04  Data: 0.010 (0.017)
Train: 316 [ 400/1251 ( 32%)]  Loss: 3.289 (3.62)  Time: 0.707s, 1447.76/s  (0.698s, 1467.13/s)  LR: 4.636e-04  Data: 0.009 (0.016)
Train: 316 [ 450/1251 ( 36%)]  Loss: 3.609 (3.62)  Time: 0.672s, 1522.98/s  (0.697s, 1470.15/s)  LR: 4.636e-04  Data: 0.011 (0.015)
Train: 316 [ 500/1251 ( 40%)]  Loss: 3.848 (3.64)  Time: 0.698s, 1467.62/s  (0.696s, 1471.00/s)  LR: 4.636e-04  Data: 0.009 (0.015)
Train: 316 [ 550/1251 ( 44%)]  Loss: 4.008 (3.67)  Time: 0.672s, 1523.12/s  (0.696s, 1471.75/s)  LR: 4.636e-04  Data: 0.010 (0.014)
Train: 316 [ 600/1251 ( 48%)]  Loss: 3.760 (3.68)  Time: 0.668s, 1531.90/s  (0.696s, 1471.46/s)  LR: 4.636e-04  Data: 0.009 (0.014)
Train: 316 [ 650/1251 ( 52%)]  Loss: 3.641 (3.67)  Time: 0.702s, 1459.57/s  (0.695s, 1473.63/s)  LR: 4.636e-04  Data: 0.009 (0.014)
Train: 316 [ 700/1251 ( 56%)]  Loss: 3.403 (3.65)  Time: 0.714s, 1434.37/s  (0.695s, 1473.15/s)  LR: 4.636e-04  Data: 0.011 (0.014)
Train: 316 [ 750/1251 ( 60%)]  Loss: 3.319 (3.63)  Time: 0.668s, 1532.05/s  (0.694s, 1474.50/s)  LR: 4.636e-04  Data: 0.010 (0.013)
Train: 316 [ 800/1251 ( 64%)]  Loss: 3.796 (3.64)  Time: 0.711s, 1439.32/s  (0.694s, 1475.18/s)  LR: 4.636e-04  Data: 0.010 (0.013)
Train: 316 [ 850/1251 ( 68%)]  Loss: 3.617 (3.64)  Time: 0.671s, 1526.14/s  (0.694s, 1475.21/s)  LR: 4.636e-04  Data: 0.011 (0.013)
Train: 316 [ 900/1251 ( 72%)]  Loss: 3.779 (3.65)  Time: 0.720s, 1422.51/s  (0.694s, 1475.82/s)  LR: 4.636e-04  Data: 0.008 (0.013)
Train: 316 [ 950/1251 ( 76%)]  Loss: 3.622 (3.65)  Time: 0.673s, 1520.72/s  (0.693s, 1477.15/s)  LR: 4.636e-04  Data: 0.010 (0.013)
Train: 316 [1000/1251 ( 80%)]  Loss: 3.924 (3.66)  Time: 0.669s, 1531.16/s  (0.693s, 1477.64/s)  LR: 4.636e-04  Data: 0.010 (0.013)
Train: 316 [1050/1251 ( 84%)]  Loss: 3.968 (3.67)  Time: 0.729s, 1404.26/s  (0.693s, 1477.76/s)  LR: 4.636e-04  Data: 0.016 (0.012)
Train: 316 [1100/1251 ( 88%)]  Loss: 3.669 (3.67)  Time: 0.703s, 1457.19/s  (0.693s, 1478.45/s)  LR: 4.636e-04  Data: 0.013 (0.012)
Train: 316 [1150/1251 ( 92%)]  Loss: 3.791 (3.68)  Time: 0.672s, 1523.33/s  (0.692s, 1479.11/s)  LR: 4.636e-04  Data: 0.009 (0.012)
Train: 316 [1200/1251 ( 96%)]  Loss: 3.601 (3.68)  Time: 0.699s, 1465.10/s  (0.692s, 1479.53/s)  LR: 4.636e-04  Data: 0.009 (0.012)
Train: 316 [1250/1251 (100%)]  Loss: 3.916 (3.69)  Time: 0.697s, 1469.79/s  (0.692s, 1479.34/s)  LR: 4.636e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.596 (1.596)  Loss:  0.8813 (0.8813)  Acc@1: 88.2812 (88.2812)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.136 (0.581)  Loss:  0.9507 (1.4325)  Acc@1: 83.2547 (73.4280)  Acc@5: 95.6368 (91.6840)
Train: 317 [   0/1251 (  0%)]  Loss: 3.480 (3.48)  Time: 2.498s,  409.90/s  (2.498s,  409.90/s)  LR: 4.610e-04  Data: 1.815 (1.815)
Train: 317 [  50/1251 (  4%)]  Loss: 3.673 (3.58)  Time: 0.706s, 1450.09/s  (0.737s, 1388.71/s)  LR: 4.610e-04  Data: 0.009 (0.053)
Train: 317 [ 100/1251 (  8%)]  Loss: 3.741 (3.63)  Time: 0.670s, 1528.99/s  (0.718s, 1426.96/s)  LR: 4.610e-04  Data: 0.009 (0.032)
Train: 317 [ 150/1251 ( 12%)]  Loss: 4.026 (3.73)  Time: 0.706s, 1451.27/s  (0.711s, 1440.01/s)  LR: 4.610e-04  Data: 0.009 (0.025)
Train: 317 [ 200/1251 ( 16%)]  Loss: 3.183 (3.62)  Time: 0.705s, 1452.56/s  (0.706s, 1450.25/s)  LR: 4.610e-04  Data: 0.011 (0.021)
Train: 317 [ 250/1251 ( 20%)]  Loss: 3.546 (3.61)  Time: 0.666s, 1537.33/s  (0.703s, 1455.58/s)  LR: 4.610e-04  Data: 0.011 (0.019)
Train: 317 [ 300/1251 ( 24%)]  Loss: 3.348 (3.57)  Time: 0.686s, 1493.29/s  (0.701s, 1461.57/s)  LR: 4.610e-04  Data: 0.010 (0.018)
Train: 317 [ 350/1251 ( 28%)]  Loss: 3.414 (3.55)  Time: 0.702s, 1458.02/s  (0.699s, 1464.15/s)  LR: 4.610e-04  Data: 0.010 (0.017)
Train: 317 [ 400/1251 ( 32%)]  Loss: 3.936 (3.59)  Time: 0.669s, 1530.98/s  (0.699s, 1465.98/s)  LR: 4.610e-04  Data: 0.011 (0.016)
Train: 317 [ 450/1251 ( 36%)]  Loss: 3.987 (3.63)  Time: 0.669s, 1529.52/s  (0.697s, 1469.48/s)  LR: 4.610e-04  Data: 0.010 (0.015)
Train: 317 [ 500/1251 ( 40%)]  Loss: 3.502 (3.62)  Time: 0.711s, 1439.32/s  (0.696s, 1470.51/s)  LR: 4.610e-04  Data: 0.010 (0.015)
Train: 317 [ 550/1251 ( 44%)]  Loss: 3.568 (3.62)  Time: 0.672s, 1524.45/s  (0.696s, 1470.97/s)  LR: 4.610e-04  Data: 0.010 (0.014)
Train: 317 [ 600/1251 ( 48%)]  Loss: 3.160 (3.58)  Time: 0.666s, 1538.24/s  (0.695s, 1473.18/s)  LR: 4.610e-04  Data: 0.011 (0.014)
Train: 317 [ 650/1251 ( 52%)]  Loss: 3.629 (3.59)  Time: 0.674s, 1520.23/s  (0.695s, 1472.95/s)  LR: 4.610e-04  Data: 0.010 (0.014)
Train: 317 [ 700/1251 ( 56%)]  Loss: 3.363 (3.57)  Time: 0.678s, 1509.98/s  (0.695s, 1472.86/s)  LR: 4.610e-04  Data: 0.013 (0.013)
Train: 317 [ 750/1251 ( 60%)]  Loss: 3.592 (3.57)  Time: 0.665s, 1539.06/s  (0.695s, 1473.43/s)  LR: 4.610e-04  Data: 0.010 (0.013)
Train: 317 [ 800/1251 ( 64%)]  Loss: 3.379 (3.56)  Time: 0.671s, 1527.10/s  (0.694s, 1474.91/s)  LR: 4.610e-04  Data: 0.010 (0.013)
Train: 317 [ 850/1251 ( 68%)]  Loss: 3.770 (3.57)  Time: 0.672s, 1522.73/s  (0.695s, 1474.34/s)  LR: 4.610e-04  Data: 0.010 (0.013)
Train: 317 [ 900/1251 ( 72%)]  Loss: 3.541 (3.57)  Time: 0.700s, 1462.02/s  (0.694s, 1474.77/s)  LR: 4.610e-04  Data: 0.009 (0.013)
Train: 317 [ 950/1251 ( 76%)]  Loss: 3.845 (3.58)  Time: 0.685s, 1495.13/s  (0.694s, 1475.51/s)  LR: 4.610e-04  Data: 0.014 (0.013)
Train: 317 [1000/1251 ( 80%)]  Loss: 3.753 (3.59)  Time: 0.710s, 1441.88/s  (0.694s, 1475.71/s)  LR: 4.610e-04  Data: 0.011 (0.013)
Train: 317 [1050/1251 ( 84%)]  Loss: 3.466 (3.59)  Time: 0.700s, 1462.93/s  (0.694s, 1476.10/s)  LR: 4.610e-04  Data: 0.009 (0.012)
Train: 317 [1100/1251 ( 88%)]  Loss: 3.079 (3.56)  Time: 0.681s, 1502.96/s  (0.694s, 1475.76/s)  LR: 4.610e-04  Data: 0.011 (0.012)
Train: 317 [1150/1251 ( 92%)]  Loss: 3.529 (3.56)  Time: 0.675s, 1518.02/s  (0.694s, 1475.60/s)  LR: 4.610e-04  Data: 0.014 (0.012)
Train: 317 [1200/1251 ( 96%)]  Loss: 3.443 (3.56)  Time: 0.680s, 1505.44/s  (0.694s, 1476.14/s)  LR: 4.610e-04  Data: 0.015 (0.012)
Train: 317 [1250/1251 (100%)]  Loss: 3.566 (3.56)  Time: 0.698s, 1466.65/s  (0.694s, 1475.68/s)  LR: 4.610e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.567 (1.567)  Loss:  0.7373 (0.7373)  Acc@1: 88.1836 (88.1836)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.136 (0.585)  Loss:  0.8271 (1.2993)  Acc@1: 83.6085 (73.7340)  Acc@5: 96.2264 (91.8780)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-314.pth.tar', 73.90399998291015)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-304.pth.tar', 73.88999993408203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-292.pth.tar', 73.81200004394532)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-315.pth.tar', 73.7900001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-306.pth.tar', 73.75800008789062)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-317.pth.tar', 73.73399999023438)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-312.pth.tar', 73.67200007080078)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-302.pth.tar', 73.66800009765625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-300.pth.tar', 73.64799993652343)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-303.pth.tar', 73.62400009277344)

Train: 318 [   0/1251 (  0%)]  Loss: 3.644 (3.64)  Time: 2.165s,  472.90/s  (2.165s,  472.90/s)  LR: 4.584e-04  Data: 1.550 (1.550)
Train: 318 [  50/1251 (  4%)]  Loss: 3.611 (3.63)  Time: 0.672s, 1523.80/s  (0.729s, 1404.08/s)  LR: 4.584e-04  Data: 0.012 (0.049)
Train: 318 [ 100/1251 (  8%)]  Loss: 3.390 (3.55)  Time: 0.672s, 1524.42/s  (0.710s, 1441.66/s)  LR: 4.584e-04  Data: 0.012 (0.030)
Train: 318 [ 150/1251 ( 12%)]  Loss: 3.693 (3.58)  Time: 0.696s, 1471.24/s  (0.706s, 1450.80/s)  LR: 4.584e-04  Data: 0.014 (0.024)
Train: 318 [ 200/1251 ( 16%)]  Loss: 3.656 (3.60)  Time: 0.733s, 1397.82/s  (0.704s, 1455.38/s)  LR: 4.584e-04  Data: 0.014 (0.020)
Train: 318 [ 250/1251 ( 20%)]  Loss: 3.475 (3.58)  Time: 0.705s, 1453.07/s  (0.701s, 1461.71/s)  LR: 4.584e-04  Data: 0.011 (0.018)
Train: 318 [ 300/1251 ( 24%)]  Loss: 3.909 (3.63)  Time: 0.672s, 1523.80/s  (0.698s, 1466.18/s)  LR: 4.584e-04  Data: 0.012 (0.017)
Train: 318 [ 350/1251 ( 28%)]  Loss: 3.825 (3.65)  Time: 0.702s, 1457.94/s  (0.698s, 1467.40/s)  LR: 4.584e-04  Data: 0.010 (0.016)
Train: 318 [ 400/1251 ( 32%)]  Loss: 3.540 (3.64)  Time: 0.671s, 1525.53/s  (0.697s, 1469.88/s)  LR: 4.584e-04  Data: 0.011 (0.015)
Train: 318 [ 450/1251 ( 36%)]  Loss: 3.654 (3.64)  Time: 0.673s, 1522.67/s  (0.697s, 1469.92/s)  LR: 4.584e-04  Data: 0.010 (0.015)
Train: 318 [ 500/1251 ( 40%)]  Loss: 3.510 (3.63)  Time: 0.693s, 1477.15/s  (0.695s, 1473.62/s)  LR: 4.584e-04  Data: 0.011 (0.014)
Train: 318 [ 550/1251 ( 44%)]  Loss: 4.002 (3.66)  Time: 0.672s, 1522.72/s  (0.695s, 1474.38/s)  LR: 4.584e-04  Data: 0.011 (0.014)
Train: 318 [ 600/1251 ( 48%)]  Loss: 3.484 (3.65)  Time: 0.729s, 1404.53/s  (0.694s, 1474.94/s)  LR: 4.584e-04  Data: 0.010 (0.014)
Train: 318 [ 650/1251 ( 52%)]  Loss: 3.901 (3.66)  Time: 0.669s, 1531.74/s  (0.694s, 1475.20/s)  LR: 4.584e-04  Data: 0.011 (0.013)
Train: 318 [ 700/1251 ( 56%)]  Loss: 3.482 (3.65)  Time: 0.672s, 1524.14/s  (0.694s, 1475.52/s)  LR: 4.584e-04  Data: 0.013 (0.013)
Train: 318 [ 750/1251 ( 60%)]  Loss: 3.587 (3.65)  Time: 0.672s, 1524.10/s  (0.694s, 1476.04/s)  LR: 4.584e-04  Data: 0.011 (0.013)
Train: 318 [ 800/1251 ( 64%)]  Loss: 3.870 (3.66)  Time: 0.686s, 1493.28/s  (0.694s, 1475.30/s)  LR: 4.584e-04  Data: 0.014 (0.013)
Train: 318 [ 850/1251 ( 68%)]  Loss: 3.509 (3.65)  Time: 0.724s, 1414.90/s  (0.694s, 1475.12/s)  LR: 4.584e-04  Data: 0.016 (0.013)
Train: 318 [ 900/1251 ( 72%)]  Loss: 3.697 (3.65)  Time: 0.692s, 1479.55/s  (0.694s, 1475.09/s)  LR: 4.584e-04  Data: 0.011 (0.013)
Train: 318 [ 950/1251 ( 76%)]  Loss: 3.785 (3.66)  Time: 0.674s, 1518.83/s  (0.694s, 1474.82/s)  LR: 4.584e-04  Data: 0.011 (0.013)
Train: 318 [1000/1251 ( 80%)]  Loss: 3.341 (3.65)  Time: 0.681s, 1503.66/s  (0.694s, 1475.76/s)  LR: 4.584e-04  Data: 0.011 (0.012)
Train: 318 [1050/1251 ( 84%)]  Loss: 3.877 (3.66)  Time: 0.711s, 1439.37/s  (0.694s, 1475.81/s)  LR: 4.584e-04  Data: 0.011 (0.012)
Train: 318 [1100/1251 ( 88%)]  Loss: 3.582 (3.65)  Time: 0.672s, 1523.81/s  (0.694s, 1476.27/s)  LR: 4.584e-04  Data: 0.010 (0.012)
Train: 318 [1150/1251 ( 92%)]  Loss: 3.746 (3.66)  Time: 0.687s, 1489.95/s  (0.694s, 1476.40/s)  LR: 4.584e-04  Data: 0.010 (0.012)
Train: 318 [1200/1251 ( 96%)]  Loss: 4.017 (3.67)  Time: 0.707s, 1448.79/s  (0.693s, 1476.70/s)  LR: 4.584e-04  Data: 0.010 (0.012)
Train: 318 [1250/1251 (100%)]  Loss: 3.490 (3.66)  Time: 0.657s, 1559.32/s  (0.693s, 1477.16/s)  LR: 4.584e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.561 (1.561)  Loss:  0.7856 (0.7856)  Acc@1: 89.9414 (89.9414)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.136 (0.581)  Loss:  0.8721 (1.3686)  Acc@1: 84.7877 (73.7480)  Acc@5: 96.2264 (91.8380)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-314.pth.tar', 73.90399998291015)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-304.pth.tar', 73.88999993408203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-292.pth.tar', 73.81200004394532)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-315.pth.tar', 73.7900001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-306.pth.tar', 73.75800008789062)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-318.pth.tar', 73.74799998535157)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-317.pth.tar', 73.73399999023438)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-312.pth.tar', 73.67200007080078)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-302.pth.tar', 73.66800009765625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-300.pth.tar', 73.64799993652343)

Train: 319 [   0/1251 (  0%)]  Loss: 3.311 (3.31)  Time: 2.226s,  459.96/s  (2.226s,  459.96/s)  LR: 4.558e-04  Data: 1.569 (1.569)
Train: 319 [  50/1251 (  4%)]  Loss: 3.545 (3.43)  Time: 0.676s, 1513.91/s  (0.734s, 1394.21/s)  LR: 4.558e-04  Data: 0.011 (0.055)
Train: 319 [ 100/1251 (  8%)]  Loss: 3.538 (3.46)  Time: 0.701s, 1461.09/s  (0.714s, 1433.99/s)  LR: 4.558e-04  Data: 0.009 (0.033)
Train: 319 [ 150/1251 ( 12%)]  Loss: 3.478 (3.47)  Time: 0.766s, 1336.39/s  (0.705s, 1453.40/s)  LR: 4.558e-04  Data: 0.010 (0.025)
Train: 319 [ 200/1251 ( 16%)]  Loss: 3.802 (3.53)  Time: 0.668s, 1533.20/s  (0.704s, 1455.13/s)  LR: 4.558e-04  Data: 0.013 (0.022)
Train: 319 [ 250/1251 ( 20%)]  Loss: 3.965 (3.61)  Time: 0.685s, 1495.80/s  (0.701s, 1460.71/s)  LR: 4.558e-04  Data: 0.010 (0.020)
Train: 319 [ 300/1251 ( 24%)]  Loss: 3.784 (3.63)  Time: 0.715s, 1432.94/s  (0.700s, 1463.90/s)  LR: 4.558e-04  Data: 0.010 (0.018)
Train: 319 [ 350/1251 ( 28%)]  Loss: 3.816 (3.65)  Time: 0.719s, 1423.50/s  (0.700s, 1463.38/s)  LR: 4.558e-04  Data: 0.009 (0.017)
Train: 319 [ 400/1251 ( 32%)]  Loss: 3.949 (3.69)  Time: 0.671s, 1525.54/s  (0.699s, 1464.23/s)  LR: 4.558e-04  Data: 0.010 (0.016)
Train: 319 [ 450/1251 ( 36%)]  Loss: 3.625 (3.68)  Time: 0.710s, 1442.53/s  (0.699s, 1465.83/s)  LR: 4.558e-04  Data: 0.010 (0.016)
Train: 319 [ 500/1251 ( 40%)]  Loss: 3.388 (3.65)  Time: 0.689s, 1486.67/s  (0.698s, 1466.69/s)  LR: 4.558e-04  Data: 0.011 (0.015)
Train: 319 [ 550/1251 ( 44%)]  Loss: 3.543 (3.65)  Time: 0.716s, 1430.65/s  (0.698s, 1467.58/s)  LR: 4.558e-04  Data: 0.011 (0.015)
Train: 319 [ 600/1251 ( 48%)]  Loss: 3.825 (3.66)  Time: 0.701s, 1461.70/s  (0.697s, 1469.95/s)  LR: 4.558e-04  Data: 0.009 (0.014)
Train: 319 [ 650/1251 ( 52%)]  Loss: 3.448 (3.64)  Time: 0.714s, 1435.16/s  (0.696s, 1471.13/s)  LR: 4.558e-04  Data: 0.010 (0.014)
Train: 319 [ 700/1251 ( 56%)]  Loss: 3.669 (3.65)  Time: 0.674s, 1519.82/s  (0.695s, 1472.56/s)  LR: 4.558e-04  Data: 0.010 (0.014)
Train: 319 [ 750/1251 ( 60%)]  Loss: 3.662 (3.65)  Time: 0.691s, 1482.68/s  (0.695s, 1473.30/s)  LR: 4.558e-04  Data: 0.010 (0.013)
Train: 319 [ 800/1251 ( 64%)]  Loss: 3.914 (3.66)  Time: 0.672s, 1524.16/s  (0.695s, 1473.83/s)  LR: 4.558e-04  Data: 0.010 (0.013)
Train: 319 [ 850/1251 ( 68%)]  Loss: 3.743 (3.67)  Time: 0.665s, 1540.78/s  (0.694s, 1474.86/s)  LR: 4.558e-04  Data: 0.009 (0.013)
Train: 319 [ 900/1251 ( 72%)]  Loss: 3.726 (3.67)  Time: 0.716s, 1430.51/s  (0.694s, 1475.21/s)  LR: 4.558e-04  Data: 0.014 (0.013)
Train: 319 [ 950/1251 ( 76%)]  Loss: 3.745 (3.67)  Time: 0.707s, 1448.30/s  (0.694s, 1475.85/s)  LR: 4.558e-04  Data: 0.010 (0.013)
Train: 319 [1000/1251 ( 80%)]  Loss: 3.592 (3.67)  Time: 0.712s, 1439.13/s  (0.694s, 1476.42/s)  LR: 4.558e-04  Data: 0.009 (0.013)
Train: 319 [1050/1251 ( 84%)]  Loss: 3.471 (3.66)  Time: 0.758s, 1351.02/s  (0.694s, 1475.83/s)  LR: 4.558e-04  Data: 0.010 (0.013)
Train: 319 [1100/1251 ( 88%)]  Loss: 3.677 (3.66)  Time: 0.731s, 1400.07/s  (0.694s, 1475.46/s)  LR: 4.558e-04  Data: 0.011 (0.012)
Train: 319 [1150/1251 ( 92%)]  Loss: 3.631 (3.66)  Time: 0.721s, 1419.37/s  (0.694s, 1475.91/s)  LR: 4.558e-04  Data: 0.009 (0.012)
Train: 319 [1200/1251 ( 96%)]  Loss: 3.758 (3.66)  Time: 0.671s, 1525.68/s  (0.694s, 1476.21/s)  LR: 4.558e-04  Data: 0.010 (0.012)
Train: 319 [1250/1251 (100%)]  Loss: 3.305 (3.65)  Time: 0.685s, 1494.32/s  (0.694s, 1475.97/s)  LR: 4.558e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.610 (1.610)  Loss:  0.9956 (0.9956)  Acc@1: 87.4023 (87.4023)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.136 (0.583)  Loss:  1.0254 (1.4556)  Acc@1: 84.1981 (73.3120)  Acc@5: 96.5802 (91.4500)
Train: 320 [   0/1251 (  0%)]  Loss: 3.563 (3.56)  Time: 2.258s,  453.54/s  (2.258s,  453.54/s)  LR: 4.533e-04  Data: 1.597 (1.597)
Train: 320 [  50/1251 (  4%)]  Loss: 3.464 (3.51)  Time: 0.690s, 1483.88/s  (0.738s, 1388.22/s)  LR: 4.533e-04  Data: 0.010 (0.050)
Train: 320 [ 100/1251 (  8%)]  Loss: 3.778 (3.60)  Time: 0.709s, 1444.05/s  (0.712s, 1438.60/s)  LR: 4.533e-04  Data: 0.010 (0.030)
Train: 320 [ 150/1251 ( 12%)]  Loss: 4.067 (3.72)  Time: 0.672s, 1524.77/s  (0.708s, 1446.89/s)  LR: 4.533e-04  Data: 0.014 (0.024)
Train: 320 [ 200/1251 ( 16%)]  Loss: 3.537 (3.68)  Time: 0.682s, 1502.16/s  (0.702s, 1458.46/s)  LR: 4.533e-04  Data: 0.009 (0.020)
Train: 320 [ 250/1251 ( 20%)]  Loss: 4.010 (3.74)  Time: 0.673s, 1520.81/s  (0.700s, 1463.90/s)  LR: 4.533e-04  Data: 0.010 (0.018)
Train: 320 [ 300/1251 ( 24%)]  Loss: 3.755 (3.74)  Time: 0.689s, 1486.82/s  (0.697s, 1468.48/s)  LR: 4.533e-04  Data: 0.010 (0.017)
Train: 320 [ 350/1251 ( 28%)]  Loss: 3.573 (3.72)  Time: 0.671s, 1525.34/s  (0.696s, 1470.25/s)  LR: 4.533e-04  Data: 0.010 (0.016)
Train: 320 [ 400/1251 ( 32%)]  Loss: 3.562 (3.70)  Time: 0.667s, 1536.12/s  (0.697s, 1469.84/s)  LR: 4.533e-04  Data: 0.009 (0.015)
Train: 320 [ 450/1251 ( 36%)]  Loss: 3.636 (3.69)  Time: 0.682s, 1501.54/s  (0.697s, 1470.12/s)  LR: 4.533e-04  Data: 0.013 (0.015)
Train: 320 [ 500/1251 ( 40%)]  Loss: 3.375 (3.67)  Time: 0.673s, 1520.51/s  (0.696s, 1471.69/s)  LR: 4.533e-04  Data: 0.011 (0.014)
Train: 320 [ 550/1251 ( 44%)]  Loss: 3.644 (3.66)  Time: 0.706s, 1449.59/s  (0.695s, 1473.06/s)  LR: 4.533e-04  Data: 0.009 (0.014)
Train: 320 [ 600/1251 ( 48%)]  Loss: 3.892 (3.68)  Time: 0.723s, 1417.26/s  (0.695s, 1474.11/s)  LR: 4.533e-04  Data: 0.010 (0.014)
Train: 320 [ 650/1251 ( 52%)]  Loss: 3.774 (3.69)  Time: 0.679s, 1507.01/s  (0.694s, 1475.10/s)  LR: 4.533e-04  Data: 0.011 (0.013)
Train: 320 [ 700/1251 ( 56%)]  Loss: 3.744 (3.69)  Time: 0.674s, 1519.23/s  (0.694s, 1476.48/s)  LR: 4.533e-04  Data: 0.011 (0.013)
Train: 320 [ 750/1251 ( 60%)]  Loss: 3.356 (3.67)  Time: 0.669s, 1529.87/s  (0.694s, 1475.99/s)  LR: 4.533e-04  Data: 0.010 (0.013)
Train: 320 [ 800/1251 ( 64%)]  Loss: 3.792 (3.68)  Time: 0.675s, 1516.52/s  (0.694s, 1475.78/s)  LR: 4.533e-04  Data: 0.011 (0.013)
Train: 320 [ 850/1251 ( 68%)]  Loss: 3.848 (3.69)  Time: 0.694s, 1476.30/s  (0.694s, 1476.22/s)  LR: 4.533e-04  Data: 0.009 (0.013)
Train: 320 [ 900/1251 ( 72%)]  Loss: 3.274 (3.67)  Time: 0.674s, 1519.16/s  (0.693s, 1476.75/s)  LR: 4.533e-04  Data: 0.013 (0.013)
Train: 320 [ 950/1251 ( 76%)]  Loss: 3.830 (3.67)  Time: 0.718s, 1425.41/s  (0.694s, 1475.44/s)  LR: 4.533e-04  Data: 0.013 (0.012)
Train: 320 [1000/1251 ( 80%)]  Loss: 4.012 (3.69)  Time: 0.665s, 1540.95/s  (0.694s, 1474.86/s)  LR: 4.533e-04  Data: 0.011 (0.012)
Train: 320 [1050/1251 ( 84%)]  Loss: 3.517 (3.68)  Time: 0.673s, 1521.58/s  (0.694s, 1475.07/s)  LR: 4.533e-04  Data: 0.011 (0.012)
Train: 320 [1100/1251 ( 88%)]  Loss: 3.768 (3.69)  Time: 0.683s, 1499.08/s  (0.694s, 1475.64/s)  LR: 4.533e-04  Data: 0.010 (0.012)
Train: 320 [1150/1251 ( 92%)]  Loss: 3.286 (3.67)  Time: 0.674s, 1519.45/s  (0.694s, 1476.17/s)  LR: 4.533e-04  Data: 0.011 (0.012)
Train: 320 [1200/1251 ( 96%)]  Loss: 3.522 (3.66)  Time: 0.706s, 1449.50/s  (0.694s, 1476.21/s)  LR: 4.533e-04  Data: 0.010 (0.012)
Train: 320 [1250/1251 (100%)]  Loss: 3.513 (3.66)  Time: 0.656s, 1561.89/s  (0.693s, 1476.64/s)  LR: 4.533e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.526 (1.526)  Loss:  0.8335 (0.8335)  Acc@1: 89.3555 (89.3555)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.136 (0.584)  Loss:  1.0205 (1.3800)  Acc@1: 83.6085 (73.8840)  Acc@5: 95.5189 (91.6660)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-314.pth.tar', 73.90399998291015)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-304.pth.tar', 73.88999993408203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-320.pth.tar', 73.88399999023437)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-292.pth.tar', 73.81200004394532)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-315.pth.tar', 73.7900001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-306.pth.tar', 73.75800008789062)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-318.pth.tar', 73.74799998535157)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-317.pth.tar', 73.73399999023438)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-312.pth.tar', 73.67200007080078)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-302.pth.tar', 73.66800009765625)

Train: 321 [   0/1251 (  0%)]  Loss: 3.319 (3.32)  Time: 2.132s,  480.34/s  (2.132s,  480.34/s)  LR: 4.507e-04  Data: 1.516 (1.516)
Train: 321 [  50/1251 (  4%)]  Loss: 3.766 (3.54)  Time: 0.671s, 1526.44/s  (0.730s, 1403.62/s)  LR: 4.507e-04  Data: 0.010 (0.051)
Train: 321 [ 100/1251 (  8%)]  Loss: 3.446 (3.51)  Time: 0.677s, 1511.82/s  (0.712s, 1438.03/s)  LR: 4.507e-04  Data: 0.010 (0.031)
Train: 321 [ 150/1251 ( 12%)]  Loss: 3.325 (3.46)  Time: 0.672s, 1523.40/s  (0.705s, 1452.98/s)  LR: 4.507e-04  Data: 0.009 (0.024)
Train: 321 [ 200/1251 ( 16%)]  Loss: 3.787 (3.53)  Time: 0.694s, 1474.83/s  (0.701s, 1461.27/s)  LR: 4.507e-04  Data: 0.009 (0.021)
Train: 321 [ 250/1251 ( 20%)]  Loss: 3.685 (3.55)  Time: 0.703s, 1457.60/s  (0.699s, 1464.66/s)  LR: 4.507e-04  Data: 0.009 (0.019)
Train: 321 [ 300/1251 ( 24%)]  Loss: 3.418 (3.54)  Time: 0.678s, 1510.24/s  (0.697s, 1468.69/s)  LR: 4.507e-04  Data: 0.009 (0.017)
Train: 321 [ 350/1251 ( 28%)]  Loss: 3.827 (3.57)  Time: 0.699s, 1465.94/s  (0.696s, 1471.33/s)  LR: 4.507e-04  Data: 0.010 (0.016)
Train: 321 [ 400/1251 ( 32%)]  Loss: 3.058 (3.51)  Time: 0.706s, 1450.65/s  (0.697s, 1469.28/s)  LR: 4.507e-04  Data: 0.011 (0.016)
Train: 321 [ 450/1251 ( 36%)]  Loss: 4.035 (3.57)  Time: 0.708s, 1446.91/s  (0.696s, 1471.91/s)  LR: 4.507e-04  Data: 0.010 (0.015)
Train: 321 [ 500/1251 ( 40%)]  Loss: 3.778 (3.59)  Time: 0.672s, 1523.30/s  (0.694s, 1474.82/s)  LR: 4.507e-04  Data: 0.010 (0.015)
Train: 321 [ 550/1251 ( 44%)]  Loss: 3.528 (3.58)  Time: 0.693s, 1478.18/s  (0.695s, 1473.78/s)  LR: 4.507e-04  Data: 0.010 (0.014)
Train: 321 [ 600/1251 ( 48%)]  Loss: 3.851 (3.60)  Time: 0.754s, 1357.66/s  (0.694s, 1474.67/s)  LR: 4.507e-04  Data: 0.010 (0.014)
Train: 321 [ 650/1251 ( 52%)]  Loss: 3.731 (3.61)  Time: 0.680s, 1505.42/s  (0.694s, 1475.83/s)  LR: 4.507e-04  Data: 0.010 (0.014)
Train: 321 [ 700/1251 ( 56%)]  Loss: 3.384 (3.60)  Time: 0.671s, 1526.93/s  (0.694s, 1476.45/s)  LR: 4.507e-04  Data: 0.009 (0.013)
Train: 321 [ 750/1251 ( 60%)]  Loss: 3.898 (3.61)  Time: 0.673s, 1521.27/s  (0.694s, 1476.48/s)  LR: 4.507e-04  Data: 0.010 (0.013)
Train: 321 [ 800/1251 ( 64%)]  Loss: 3.492 (3.61)  Time: 0.704s, 1454.26/s  (0.694s, 1476.49/s)  LR: 4.507e-04  Data: 0.010 (0.013)
Train: 321 [ 850/1251 ( 68%)]  Loss: 3.489 (3.60)  Time: 0.666s, 1536.96/s  (0.693s, 1476.65/s)  LR: 4.507e-04  Data: 0.008 (0.013)
Train: 321 [ 900/1251 ( 72%)]  Loss: 3.684 (3.61)  Time: 0.758s, 1350.24/s  (0.694s, 1475.36/s)  LR: 4.507e-04  Data: 0.010 (0.013)
Train: 321 [ 950/1251 ( 76%)]  Loss: 3.409 (3.60)  Time: 0.670s, 1528.28/s  (0.694s, 1475.78/s)  LR: 4.507e-04  Data: 0.010 (0.013)
Train: 321 [1000/1251 ( 80%)]  Loss: 3.904 (3.61)  Time: 0.679s, 1508.29/s  (0.694s, 1476.00/s)  LR: 4.507e-04  Data: 0.010 (0.012)
Train: 321 [1050/1251 ( 84%)]  Loss: 3.708 (3.61)  Time: 0.680s, 1506.06/s  (0.694s, 1475.95/s)  LR: 4.507e-04  Data: 0.015 (0.012)
Train: 321 [1100/1251 ( 88%)]  Loss: 3.781 (3.62)  Time: 0.672s, 1524.07/s  (0.693s, 1476.58/s)  LR: 4.507e-04  Data: 0.010 (0.012)
Train: 321 [1150/1251 ( 92%)]  Loss: 3.660 (3.62)  Time: 0.673s, 1521.15/s  (0.693s, 1476.84/s)  LR: 4.507e-04  Data: 0.012 (0.012)
Train: 321 [1200/1251 ( 96%)]  Loss: 3.683 (3.63)  Time: 0.675s, 1516.45/s  (0.693s, 1477.00/s)  LR: 4.507e-04  Data: 0.010 (0.012)
Train: 321 [1250/1251 (100%)]  Loss: 3.710 (3.63)  Time: 0.658s, 1555.52/s  (0.693s, 1477.92/s)  LR: 4.507e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.495 (1.495)  Loss:  0.7651 (0.7651)  Acc@1: 89.6484 (89.6484)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.136 (0.572)  Loss:  0.9346 (1.3768)  Acc@1: 83.9623 (73.7680)  Acc@5: 96.1085 (91.8420)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-314.pth.tar', 73.90399998291015)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-304.pth.tar', 73.88999993408203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-320.pth.tar', 73.88399999023437)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-292.pth.tar', 73.81200004394532)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-315.pth.tar', 73.7900001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-321.pth.tar', 73.76800001464844)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-306.pth.tar', 73.75800008789062)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-318.pth.tar', 73.74799998535157)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-317.pth.tar', 73.73399999023438)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-312.pth.tar', 73.67200007080078)

Train: 322 [   0/1251 (  0%)]  Loss: 4.144 (4.14)  Time: 2.311s,  443.13/s  (2.311s,  443.13/s)  LR: 4.481e-04  Data: 1.658 (1.658)
Train: 322 [  50/1251 (  4%)]  Loss: 3.496 (3.82)  Time: 0.666s, 1537.83/s  (0.725s, 1411.54/s)  LR: 4.481e-04  Data: 0.009 (0.051)
Train: 322 [ 100/1251 (  8%)]  Loss: 3.640 (3.76)  Time: 0.674s, 1518.37/s  (0.707s, 1448.68/s)  LR: 4.481e-04  Data: 0.011 (0.031)
Train: 322 [ 150/1251 ( 12%)]  Loss: 3.798 (3.77)  Time: 0.672s, 1523.56/s  (0.701s, 1460.29/s)  LR: 4.481e-04  Data: 0.010 (0.024)
Train: 322 [ 200/1251 ( 16%)]  Loss: 3.637 (3.74)  Time: 0.716s, 1429.58/s  (0.699s, 1464.79/s)  LR: 4.481e-04  Data: 0.011 (0.021)
Train: 322 [ 250/1251 ( 20%)]  Loss: 4.011 (3.79)  Time: 0.703s, 1456.27/s  (0.697s, 1468.69/s)  LR: 4.481e-04  Data: 0.010 (0.019)
Train: 322 [ 300/1251 ( 24%)]  Loss: 3.622 (3.76)  Time: 0.690s, 1483.03/s  (0.696s, 1471.96/s)  LR: 4.481e-04  Data: 0.010 (0.017)
Train: 322 [ 350/1251 ( 28%)]  Loss: 3.481 (3.73)  Time: 0.729s, 1403.91/s  (0.694s, 1474.52/s)  LR: 4.481e-04  Data: 0.009 (0.016)
Train: 322 [ 400/1251 ( 32%)]  Loss: 3.620 (3.72)  Time: 0.679s, 1507.51/s  (0.695s, 1474.38/s)  LR: 4.481e-04  Data: 0.010 (0.015)
Train: 322 [ 450/1251 ( 36%)]  Loss: 3.180 (3.66)  Time: 0.707s, 1449.34/s  (0.694s, 1474.60/s)  LR: 4.481e-04  Data: 0.009 (0.015)
Train: 322 [ 500/1251 ( 40%)]  Loss: 3.546 (3.65)  Time: 0.673s, 1521.21/s  (0.694s, 1475.11/s)  LR: 4.481e-04  Data: 0.010 (0.014)
Train: 322 [ 550/1251 ( 44%)]  Loss: 3.547 (3.64)  Time: 0.778s, 1316.71/s  (0.695s, 1474.07/s)  LR: 4.481e-04  Data: 0.009 (0.014)
Train: 322 [ 600/1251 ( 48%)]  Loss: 3.778 (3.65)  Time: 0.690s, 1484.78/s  (0.694s, 1476.07/s)  LR: 4.481e-04  Data: 0.022 (0.014)
Train: 322 [ 650/1251 ( 52%)]  Loss: 3.197 (3.62)  Time: 0.672s, 1523.04/s  (0.693s, 1477.83/s)  LR: 4.481e-04  Data: 0.010 (0.013)
Train: 322 [ 700/1251 ( 56%)]  Loss: 3.571 (3.62)  Time: 0.700s, 1463.62/s  (0.693s, 1477.44/s)  LR: 4.481e-04  Data: 0.009 (0.013)
Train: 322 [ 750/1251 ( 60%)]  Loss: 3.898 (3.64)  Time: 0.719s, 1423.57/s  (0.693s, 1477.62/s)  LR: 4.481e-04  Data: 0.009 (0.013)
Train: 322 [ 800/1251 ( 64%)]  Loss: 3.631 (3.64)  Time: 0.693s, 1478.56/s  (0.693s, 1476.58/s)  LR: 4.481e-04  Data: 0.012 (0.013)
Train: 322 [ 850/1251 ( 68%)]  Loss: 3.416 (3.62)  Time: 0.710s, 1442.36/s  (0.693s, 1477.42/s)  LR: 4.481e-04  Data: 0.009 (0.013)
Train: 322 [ 900/1251 ( 72%)]  Loss: 3.474 (3.62)  Time: 0.743s, 1377.86/s  (0.693s, 1477.66/s)  LR: 4.481e-04  Data: 0.011 (0.013)
Train: 322 [ 950/1251 ( 76%)]  Loss: 3.524 (3.61)  Time: 0.679s, 1508.17/s  (0.693s, 1478.29/s)  LR: 4.481e-04  Data: 0.011 (0.013)
Train: 322 [1000/1251 ( 80%)]  Loss: 3.642 (3.61)  Time: 0.678s, 1509.45/s  (0.693s, 1478.27/s)  LR: 4.481e-04  Data: 0.010 (0.012)
Train: 322 [1050/1251 ( 84%)]  Loss: 3.640 (3.61)  Time: 0.729s, 1404.95/s  (0.693s, 1478.12/s)  LR: 4.481e-04  Data: 0.010 (0.012)
Train: 322 [1100/1251 ( 88%)]  Loss: 3.727 (3.62)  Time: 0.666s, 1537.44/s  (0.693s, 1478.00/s)  LR: 4.481e-04  Data: 0.009 (0.012)
Train: 322 [1150/1251 ( 92%)]  Loss: 3.590 (3.62)  Time: 0.707s, 1449.38/s  (0.693s, 1478.34/s)  LR: 4.481e-04  Data: 0.009 (0.012)
Train: 322 [1200/1251 ( 96%)]  Loss: 3.698 (3.62)  Time: 0.705s, 1451.96/s  (0.693s, 1478.36/s)  LR: 4.481e-04  Data: 0.010 (0.012)
Train: 322 [1250/1251 (100%)]  Loss: 3.137 (3.60)  Time: 0.658s, 1555.72/s  (0.692s, 1479.33/s)  LR: 4.481e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.487 (1.487)  Loss:  0.8032 (0.8032)  Acc@1: 88.6719 (88.6719)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.137 (0.582)  Loss:  0.8799 (1.3606)  Acc@1: 85.3774 (73.8960)  Acc@5: 97.5236 (91.8080)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-314.pth.tar', 73.90399998291015)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-322.pth.tar', 73.89600011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-304.pth.tar', 73.88999993408203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-320.pth.tar', 73.88399999023437)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-292.pth.tar', 73.81200004394532)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-315.pth.tar', 73.7900001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-321.pth.tar', 73.76800001464844)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-306.pth.tar', 73.75800008789062)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-318.pth.tar', 73.74799998535157)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-317.pth.tar', 73.73399999023438)

Train: 323 [   0/1251 (  0%)]  Loss: 3.381 (3.38)  Time: 2.061s,  496.77/s  (2.061s,  496.77/s)  LR: 4.455e-04  Data: 1.444 (1.444)
Train: 323 [  50/1251 (  4%)]  Loss: 4.041 (3.71)  Time: 0.671s, 1526.85/s  (0.725s, 1411.88/s)  LR: 4.455e-04  Data: 0.010 (0.044)
Train: 323 [ 100/1251 (  8%)]  Loss: 3.922 (3.78)  Time: 0.702s, 1458.56/s  (0.710s, 1442.76/s)  LR: 4.455e-04  Data: 0.009 (0.027)
Train: 323 [ 150/1251 ( 12%)]  Loss: 3.580 (3.73)  Time: 0.729s, 1404.01/s  (0.707s, 1448.04/s)  LR: 4.455e-04  Data: 0.013 (0.022)
Train: 323 [ 200/1251 ( 16%)]  Loss: 3.519 (3.69)  Time: 0.674s, 1519.19/s  (0.703s, 1457.13/s)  LR: 4.455e-04  Data: 0.010 (0.019)
Train: 323 [ 250/1251 ( 20%)]  Loss: 3.768 (3.70)  Time: 0.706s, 1450.19/s  (0.702s, 1459.03/s)  LR: 4.455e-04  Data: 0.009 (0.017)
Train: 323 [ 300/1251 ( 24%)]  Loss: 3.622 (3.69)  Time: 0.676s, 1515.90/s  (0.701s, 1461.53/s)  LR: 4.455e-04  Data: 0.009 (0.016)
Train: 323 [ 350/1251 ( 28%)]  Loss: 3.792 (3.70)  Time: 0.674s, 1519.16/s  (0.700s, 1463.26/s)  LR: 4.455e-04  Data: 0.009 (0.015)
Train: 323 [ 400/1251 ( 32%)]  Loss: 3.679 (3.70)  Time: 0.671s, 1526.30/s  (0.698s, 1467.15/s)  LR: 4.455e-04  Data: 0.010 (0.015)
Train: 323 [ 450/1251 ( 36%)]  Loss: 3.408 (3.67)  Time: 0.665s, 1538.87/s  (0.698s, 1467.65/s)  LR: 4.455e-04  Data: 0.009 (0.014)
Train: 323 [ 500/1251 ( 40%)]  Loss: 3.950 (3.70)  Time: 0.708s, 1447.18/s  (0.698s, 1467.72/s)  LR: 4.455e-04  Data: 0.012 (0.014)
Train: 323 [ 550/1251 ( 44%)]  Loss: 3.554 (3.68)  Time: 0.751s, 1364.33/s  (0.700s, 1463.58/s)  LR: 4.455e-04  Data: 0.016 (0.014)
Train: 323 [ 600/1251 ( 48%)]  Loss: 3.494 (3.67)  Time: 0.686s, 1493.67/s  (0.700s, 1462.43/s)  LR: 4.455e-04  Data: 0.009 (0.014)
Train: 323 [ 650/1251 ( 52%)]  Loss: 3.532 (3.66)  Time: 0.670s, 1529.14/s  (0.700s, 1462.07/s)  LR: 4.455e-04  Data: 0.010 (0.014)
Train: 323 [ 700/1251 ( 56%)]  Loss: 3.681 (3.66)  Time: 0.671s, 1525.34/s  (0.699s, 1465.67/s)  LR: 4.455e-04  Data: 0.011 (0.013)
Train: 323 [ 750/1251 ( 60%)]  Loss: 3.917 (3.68)  Time: 0.751s, 1363.65/s  (0.697s, 1468.77/s)  LR: 4.455e-04  Data: 0.010 (0.013)
Train: 323 [ 800/1251 ( 64%)]  Loss: 3.874 (3.69)  Time: 0.696s, 1472.14/s  (0.696s, 1471.18/s)  LR: 4.455e-04  Data: 0.009 (0.013)
Train: 323 [ 850/1251 ( 68%)]  Loss: 3.446 (3.68)  Time: 0.690s, 1485.08/s  (0.696s, 1471.51/s)  LR: 4.455e-04  Data: 0.009 (0.013)
Train: 323 [ 900/1251 ( 72%)]  Loss: 3.561 (3.67)  Time: 0.692s, 1479.24/s  (0.696s, 1471.99/s)  LR: 4.455e-04  Data: 0.010 (0.013)
Train: 323 [ 950/1251 ( 76%)]  Loss: 3.567 (3.66)  Time: 0.706s, 1450.16/s  (0.696s, 1471.95/s)  LR: 4.455e-04  Data: 0.009 (0.013)
Train: 323 [1000/1251 ( 80%)]  Loss: 3.740 (3.67)  Time: 0.673s, 1521.34/s  (0.695s, 1472.43/s)  LR: 4.455e-04  Data: 0.012 (0.013)
Train: 323 [1050/1251 ( 84%)]  Loss: 3.692 (3.67)  Time: 0.672s, 1524.51/s  (0.695s, 1473.18/s)  LR: 4.455e-04  Data: 0.009 (0.012)
Train: 323 [1100/1251 ( 88%)]  Loss: 3.618 (3.67)  Time: 0.681s, 1504.32/s  (0.695s, 1473.19/s)  LR: 4.455e-04  Data: 0.009 (0.012)
Train: 323 [1150/1251 ( 92%)]  Loss: 3.648 (3.67)  Time: 0.705s, 1452.35/s  (0.695s, 1473.24/s)  LR: 4.455e-04  Data: 0.010 (0.012)
Train: 323 [1200/1251 ( 96%)]  Loss: 3.795 (3.67)  Time: 0.691s, 1480.91/s  (0.695s, 1474.36/s)  LR: 4.455e-04  Data: 0.009 (0.012)
Train: 323 [1250/1251 (100%)]  Loss: 3.377 (3.66)  Time: 0.693s, 1477.74/s  (0.694s, 1474.83/s)  LR: 4.455e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.438 (1.438)  Loss:  0.9126 (0.9126)  Acc@1: 88.7695 (88.7695)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.136 (0.577)  Loss:  0.8862 (1.3563)  Acc@1: 84.6698 (74.1000)  Acc@5: 97.1698 (92.0700)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-323.pth.tar', 74.10000006347656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-314.pth.tar', 73.90399998291015)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-322.pth.tar', 73.89600011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-304.pth.tar', 73.88999993408203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-320.pth.tar', 73.88399999023437)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-292.pth.tar', 73.81200004394532)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-315.pth.tar', 73.7900001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-321.pth.tar', 73.76800001464844)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-306.pth.tar', 73.75800008789062)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-318.pth.tar', 73.74799998535157)

Train: 324 [   0/1251 (  0%)]  Loss: 3.941 (3.94)  Time: 2.016s,  507.96/s  (2.016s,  507.96/s)  LR: 4.430e-04  Data: 1.402 (1.402)
Train: 324 [  50/1251 (  4%)]  Loss: 3.781 (3.86)  Time: 0.682s, 1500.82/s  (0.726s, 1411.26/s)  LR: 4.430e-04  Data: 0.012 (0.044)
Train: 324 [ 100/1251 (  8%)]  Loss: 3.960 (3.89)  Time: 0.672s, 1523.85/s  (0.708s, 1445.67/s)  LR: 4.430e-04  Data: 0.012 (0.027)
Train: 324 [ 150/1251 ( 12%)]  Loss: 3.622 (3.83)  Time: 0.697s, 1468.21/s  (0.705s, 1452.51/s)  LR: 4.430e-04  Data: 0.010 (0.021)
Train: 324 [ 200/1251 ( 16%)]  Loss: 3.172 (3.70)  Time: 0.694s, 1475.57/s  (0.703s, 1455.91/s)  LR: 4.430e-04  Data: 0.010 (0.019)
Train: 324 [ 250/1251 ( 20%)]  Loss: 3.514 (3.67)  Time: 0.726s, 1410.09/s  (0.701s, 1460.48/s)  LR: 4.430e-04  Data: 0.010 (0.017)
Train: 324 [ 300/1251 ( 24%)]  Loss: 3.450 (3.63)  Time: 0.693s, 1476.83/s  (0.700s, 1463.79/s)  LR: 4.430e-04  Data: 0.011 (0.016)
Train: 324 [ 350/1251 ( 28%)]  Loss: 3.624 (3.63)  Time: 0.733s, 1397.15/s  (0.699s, 1464.50/s)  LR: 4.430e-04  Data: 0.016 (0.015)
Train: 324 [ 400/1251 ( 32%)]  Loss: 3.202 (3.59)  Time: 0.697s, 1469.33/s  (0.699s, 1465.26/s)  LR: 4.430e-04  Data: 0.011 (0.015)
Train: 324 [ 450/1251 ( 36%)]  Loss: 3.550 (3.58)  Time: 0.722s, 1418.90/s  (0.699s, 1464.14/s)  LR: 4.430e-04  Data: 0.010 (0.014)
Train: 324 [ 500/1251 ( 40%)]  Loss: 3.656 (3.59)  Time: 0.680s, 1505.35/s  (0.698s, 1466.58/s)  LR: 4.430e-04  Data: 0.011 (0.014)
Train: 324 [ 550/1251 ( 44%)]  Loss: 3.341 (3.57)  Time: 0.704s, 1454.00/s  (0.698s, 1468.10/s)  LR: 4.430e-04  Data: 0.009 (0.014)
Train: 324 [ 600/1251 ( 48%)]  Loss: 3.957 (3.60)  Time: 0.696s, 1472.23/s  (0.698s, 1467.42/s)  LR: 4.430e-04  Data: 0.008 (0.013)
Train: 324 [ 650/1251 ( 52%)]  Loss: 4.105 (3.63)  Time: 0.710s, 1442.66/s  (0.698s, 1466.87/s)  LR: 4.430e-04  Data: 0.010 (0.013)
Train: 324 [ 700/1251 ( 56%)]  Loss: 3.860 (3.65)  Time: 0.710s, 1441.99/s  (0.697s, 1468.38/s)  LR: 4.430e-04  Data: 0.009 (0.013)
Train: 324 [ 750/1251 ( 60%)]  Loss: 3.601 (3.65)  Time: 0.707s, 1447.81/s  (0.697s, 1469.43/s)  LR: 4.430e-04  Data: 0.011 (0.013)
Train: 324 [ 800/1251 ( 64%)]  Loss: 3.845 (3.66)  Time: 0.674s, 1518.77/s  (0.697s, 1469.85/s)  LR: 4.430e-04  Data: 0.011 (0.013)
Train: 324 [ 850/1251 ( 68%)]  Loss: 3.914 (3.67)  Time: 0.711s, 1440.24/s  (0.696s, 1470.72/s)  LR: 4.430e-04  Data: 0.009 (0.012)
Train: 324 [ 900/1251 ( 72%)]  Loss: 3.976 (3.69)  Time: 0.680s, 1504.88/s  (0.697s, 1469.87/s)  LR: 4.430e-04  Data: 0.012 (0.012)
Train: 324 [ 950/1251 ( 76%)]  Loss: 3.725 (3.69)  Time: 0.693s, 1478.12/s  (0.697s, 1469.72/s)  LR: 4.430e-04  Data: 0.011 (0.012)
Train: 324 [1000/1251 ( 80%)]  Loss: 3.720 (3.69)  Time: 0.710s, 1442.59/s  (0.696s, 1470.34/s)  LR: 4.430e-04  Data: 0.011 (0.012)
Train: 324 [1050/1251 ( 84%)]  Loss: 3.456 (3.68)  Time: 0.699s, 1464.87/s  (0.696s, 1471.13/s)  LR: 4.430e-04  Data: 0.010 (0.012)
Train: 324 [1100/1251 ( 88%)]  Loss: 3.404 (3.67)  Time: 0.717s, 1428.60/s  (0.696s, 1470.86/s)  LR: 4.430e-04  Data: 0.011 (0.012)
Train: 324 [1150/1251 ( 92%)]  Loss: 3.759 (3.67)  Time: 0.707s, 1448.06/s  (0.696s, 1471.58/s)  LR: 4.430e-04  Data: 0.010 (0.012)
Train: 324 [1200/1251 ( 96%)]  Loss: 3.673 (3.67)  Time: 0.671s, 1524.96/s  (0.695s, 1472.35/s)  LR: 4.430e-04  Data: 0.011 (0.012)
Train: 324 [1250/1251 (100%)]  Loss: 3.378 (3.66)  Time: 0.668s, 1533.97/s  (0.696s, 1472.27/s)  LR: 4.430e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.545 (1.545)  Loss:  0.7954 (0.7954)  Acc@1: 89.2578 (89.2578)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.136 (0.575)  Loss:  0.8535 (1.3314)  Acc@1: 85.0236 (74.2580)  Acc@5: 96.3443 (91.9360)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-324.pth.tar', 74.2579999584961)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-323.pth.tar', 74.10000006347656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-314.pth.tar', 73.90399998291015)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-322.pth.tar', 73.89600011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-304.pth.tar', 73.88999993408203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-320.pth.tar', 73.88399999023437)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-292.pth.tar', 73.81200004394532)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-315.pth.tar', 73.7900001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-321.pth.tar', 73.76800001464844)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-306.pth.tar', 73.75800008789062)

Train: 325 [   0/1251 (  0%)]  Loss: 3.332 (3.33)  Time: 2.291s,  446.96/s  (2.291s,  446.96/s)  LR: 4.404e-04  Data: 1.675 (1.675)
Train: 325 [  50/1251 (  4%)]  Loss: 3.717 (3.52)  Time: 0.671s, 1527.08/s  (0.723s, 1416.62/s)  LR: 4.404e-04  Data: 0.010 (0.051)
Train: 325 [ 100/1251 (  8%)]  Loss: 3.958 (3.67)  Time: 0.675s, 1517.83/s  (0.705s, 1453.48/s)  LR: 4.404e-04  Data: 0.009 (0.031)
Train: 325 [ 150/1251 ( 12%)]  Loss: 3.902 (3.73)  Time: 0.674s, 1519.36/s  (0.700s, 1462.13/s)  LR: 4.404e-04  Data: 0.010 (0.024)
Train: 325 [ 200/1251 ( 16%)]  Loss: 3.455 (3.67)  Time: 0.720s, 1422.52/s  (0.700s, 1463.72/s)  LR: 4.404e-04  Data: 0.010 (0.021)
Train: 325 [ 250/1251 ( 20%)]  Loss: 3.535 (3.65)  Time: 0.706s, 1450.29/s  (0.699s, 1463.91/s)  LR: 4.404e-04  Data: 0.011 (0.019)
Train: 325 [ 300/1251 ( 24%)]  Loss: 3.364 (3.61)  Time: 0.672s, 1523.44/s  (0.699s, 1465.83/s)  LR: 4.404e-04  Data: 0.010 (0.017)
Train: 325 [ 350/1251 ( 28%)]  Loss: 3.802 (3.63)  Time: 0.703s, 1456.16/s  (0.697s, 1468.16/s)  LR: 4.404e-04  Data: 0.010 (0.017)
Train: 325 [ 400/1251 ( 32%)]  Loss: 3.565 (3.63)  Time: 0.713s, 1436.46/s  (0.698s, 1467.75/s)  LR: 4.404e-04  Data: 0.010 (0.016)
Train: 325 [ 450/1251 ( 36%)]  Loss: 3.613 (3.62)  Time: 0.672s, 1524.91/s  (0.697s, 1469.56/s)  LR: 4.404e-04  Data: 0.011 (0.015)
Train: 325 [ 500/1251 ( 40%)]  Loss: 3.597 (3.62)  Time: 0.672s, 1523.77/s  (0.696s, 1471.72/s)  LR: 4.404e-04  Data: 0.010 (0.015)
Train: 325 [ 550/1251 ( 44%)]  Loss: 3.924 (3.65)  Time: 0.671s, 1526.50/s  (0.696s, 1472.03/s)  LR: 4.404e-04  Data: 0.010 (0.014)
Train: 325 [ 600/1251 ( 48%)]  Loss: 3.816 (3.66)  Time: 0.677s, 1513.06/s  (0.695s, 1473.67/s)  LR: 4.404e-04  Data: 0.010 (0.014)
Train: 325 [ 650/1251 ( 52%)]  Loss: 3.311 (3.64)  Time: 0.671s, 1525.51/s  (0.695s, 1473.68/s)  LR: 4.404e-04  Data: 0.009 (0.014)
Train: 325 [ 700/1251 ( 56%)]  Loss: 3.278 (3.61)  Time: 0.678s, 1510.22/s  (0.695s, 1474.43/s)  LR: 4.404e-04  Data: 0.010 (0.013)
Train: 325 [ 750/1251 ( 60%)]  Loss: 3.643 (3.61)  Time: 0.705s, 1451.69/s  (0.694s, 1474.97/s)  LR: 4.404e-04  Data: 0.011 (0.013)
Train: 325 [ 800/1251 ( 64%)]  Loss: 3.497 (3.61)  Time: 0.669s, 1530.34/s  (0.694s, 1475.42/s)  LR: 4.404e-04  Data: 0.011 (0.013)
Train: 325 [ 850/1251 ( 68%)]  Loss: 3.408 (3.60)  Time: 0.676s, 1514.91/s  (0.693s, 1476.68/s)  LR: 4.404e-04  Data: 0.010 (0.013)
Train: 325 [ 900/1251 ( 72%)]  Loss: 3.702 (3.60)  Time: 0.756s, 1353.93/s  (0.693s, 1477.07/s)  LR: 4.404e-04  Data: 0.010 (0.013)
Train: 325 [ 950/1251 ( 76%)]  Loss: 3.788 (3.61)  Time: 0.708s, 1445.76/s  (0.693s, 1477.28/s)  LR: 4.404e-04  Data: 0.012 (0.013)
Train: 325 [1000/1251 ( 80%)]  Loss: 3.392 (3.60)  Time: 0.704s, 1454.94/s  (0.693s, 1477.78/s)  LR: 4.404e-04  Data: 0.010 (0.013)
Train: 325 [1050/1251 ( 84%)]  Loss: 3.509 (3.60)  Time: 0.712s, 1437.59/s  (0.693s, 1478.13/s)  LR: 4.404e-04  Data: 0.010 (0.012)
Train: 325 [1100/1251 ( 88%)]  Loss: 3.327 (3.58)  Time: 0.672s, 1523.78/s  (0.692s, 1478.95/s)  LR: 4.404e-04  Data: 0.009 (0.012)
Train: 325 [1150/1251 ( 92%)]  Loss: 3.748 (3.59)  Time: 0.672s, 1523.13/s  (0.692s, 1479.26/s)  LR: 4.404e-04  Data: 0.011 (0.012)
Train: 325 [1200/1251 ( 96%)]  Loss: 3.726 (3.60)  Time: 0.705s, 1452.10/s  (0.692s, 1479.17/s)  LR: 4.404e-04  Data: 0.010 (0.012)
Train: 325 [1250/1251 (100%)]  Loss: 3.244 (3.58)  Time: 0.657s, 1558.29/s  (0.692s, 1479.29/s)  LR: 4.404e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.639 (1.639)  Loss:  0.7808 (0.7808)  Acc@1: 89.6484 (89.6484)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.136 (0.576)  Loss:  0.9634 (1.3717)  Acc@1: 84.9057 (73.9360)  Acc@5: 96.2264 (92.0040)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-324.pth.tar', 74.2579999584961)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-323.pth.tar', 74.10000006347656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-325.pth.tar', 73.93599990722656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-314.pth.tar', 73.90399998291015)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-322.pth.tar', 73.89600011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-304.pth.tar', 73.88999993408203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-320.pth.tar', 73.88399999023437)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-292.pth.tar', 73.81200004394532)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-315.pth.tar', 73.7900001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-321.pth.tar', 73.76800001464844)

Train: 326 [   0/1251 (  0%)]  Loss: 3.298 (3.30)  Time: 2.243s,  456.46/s  (2.243s,  456.46/s)  LR: 4.378e-04  Data: 1.628 (1.628)
Train: 326 [  50/1251 (  4%)]  Loss: 3.664 (3.48)  Time: 0.710s, 1441.36/s  (0.733s, 1396.87/s)  LR: 4.378e-04  Data: 0.009 (0.051)
Train: 326 [ 100/1251 (  8%)]  Loss: 3.365 (3.44)  Time: 0.675s, 1515.99/s  (0.710s, 1442.14/s)  LR: 4.378e-04  Data: 0.011 (0.031)
Train: 326 [ 150/1251 ( 12%)]  Loss: 3.243 (3.39)  Time: 0.669s, 1529.69/s  (0.704s, 1454.14/s)  LR: 4.378e-04  Data: 0.010 (0.024)
Train: 326 [ 200/1251 ( 16%)]  Loss: 3.675 (3.45)  Time: 0.675s, 1517.09/s  (0.702s, 1459.30/s)  LR: 4.378e-04  Data: 0.011 (0.020)
Train: 326 [ 250/1251 ( 20%)]  Loss: 3.611 (3.48)  Time: 0.730s, 1403.38/s  (0.700s, 1462.01/s)  LR: 4.378e-04  Data: 0.011 (0.019)
Train: 326 [ 300/1251 ( 24%)]  Loss: 3.830 (3.53)  Time: 0.715s, 1431.31/s  (0.699s, 1465.08/s)  LR: 4.378e-04  Data: 0.010 (0.017)
Train: 326 [ 350/1251 ( 28%)]  Loss: 3.915 (3.58)  Time: 0.703s, 1457.02/s  (0.698s, 1466.63/s)  LR: 4.378e-04  Data: 0.008 (0.016)
Train: 326 [ 400/1251 ( 32%)]  Loss: 3.627 (3.58)  Time: 0.716s, 1430.84/s  (0.697s, 1468.93/s)  LR: 4.378e-04  Data: 0.011 (0.016)
Train: 326 [ 450/1251 ( 36%)]  Loss: 3.448 (3.57)  Time: 0.676s, 1515.60/s  (0.697s, 1469.49/s)  LR: 4.378e-04  Data: 0.010 (0.015)
Train: 326 [ 500/1251 ( 40%)]  Loss: 3.448 (3.56)  Time: 0.666s, 1537.73/s  (0.696s, 1470.75/s)  LR: 4.378e-04  Data: 0.010 (0.015)
Train: 326 [ 550/1251 ( 44%)]  Loss: 3.506 (3.55)  Time: 0.674s, 1519.01/s  (0.696s, 1470.75/s)  LR: 4.378e-04  Data: 0.010 (0.014)
Train: 326 [ 600/1251 ( 48%)]  Loss: 3.561 (3.55)  Time: 0.670s, 1527.54/s  (0.696s, 1470.32/s)  LR: 4.378e-04  Data: 0.011 (0.014)
Train: 326 [ 650/1251 ( 52%)]  Loss: 3.479 (3.55)  Time: 0.691s, 1482.49/s  (0.696s, 1470.66/s)  LR: 4.378e-04  Data: 0.010 (0.014)
Train: 326 [ 700/1251 ( 56%)]  Loss: 3.254 (3.53)  Time: 0.712s, 1437.95/s  (0.696s, 1470.27/s)  LR: 4.378e-04  Data: 0.012 (0.013)
Train: 326 [ 750/1251 ( 60%)]  Loss: 3.463 (3.52)  Time: 0.731s, 1400.49/s  (0.697s, 1469.33/s)  LR: 4.378e-04  Data: 0.011 (0.013)
Train: 326 [ 800/1251 ( 64%)]  Loss: 3.475 (3.52)  Time: 0.726s, 1410.83/s  (0.697s, 1469.74/s)  LR: 4.378e-04  Data: 0.009 (0.013)
Train: 326 [ 850/1251 ( 68%)]  Loss: 3.581 (3.52)  Time: 0.680s, 1505.20/s  (0.697s, 1469.88/s)  LR: 4.378e-04  Data: 0.008 (0.013)
Train: 326 [ 900/1251 ( 72%)]  Loss: 3.716 (3.53)  Time: 0.702s, 1458.51/s  (0.696s, 1470.24/s)  LR: 4.378e-04  Data: 0.009 (0.013)
Train: 326 [ 950/1251 ( 76%)]  Loss: 3.804 (3.55)  Time: 0.673s, 1521.36/s  (0.696s, 1470.36/s)  LR: 4.378e-04  Data: 0.011 (0.013)
Train: 326 [1000/1251 ( 80%)]  Loss: 3.527 (3.55)  Time: 0.700s, 1463.09/s  (0.696s, 1471.43/s)  LR: 4.378e-04  Data: 0.011 (0.013)
Train: 326 [1050/1251 ( 84%)]  Loss: 3.590 (3.55)  Time: 0.715s, 1432.14/s  (0.696s, 1471.43/s)  LR: 4.378e-04  Data: 0.013 (0.013)
Train: 326 [1100/1251 ( 88%)]  Loss: 3.905 (3.56)  Time: 0.691s, 1482.34/s  (0.696s, 1471.23/s)  LR: 4.378e-04  Data: 0.010 (0.012)
Train: 326 [1150/1251 ( 92%)]  Loss: 3.788 (3.57)  Time: 0.706s, 1450.32/s  (0.696s, 1471.37/s)  LR: 4.378e-04  Data: 0.011 (0.012)
Train: 326 [1200/1251 ( 96%)]  Loss: 3.553 (3.57)  Time: 0.722s, 1418.07/s  (0.696s, 1471.05/s)  LR: 4.378e-04  Data: 0.010 (0.012)
Train: 326 [1250/1251 (100%)]  Loss: 4.001 (3.59)  Time: 0.672s, 1524.39/s  (0.696s, 1471.43/s)  LR: 4.378e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.618 (1.618)  Loss:  0.9170 (0.9170)  Acc@1: 90.0391 (90.0391)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.136 (0.571)  Loss:  1.0117 (1.4750)  Acc@1: 86.0849 (73.9620)  Acc@5: 96.4623 (91.8560)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-324.pth.tar', 74.2579999584961)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-323.pth.tar', 74.10000006347656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-326.pth.tar', 73.96200003173828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-325.pth.tar', 73.93599990722656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-314.pth.tar', 73.90399998291015)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-322.pth.tar', 73.89600011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-304.pth.tar', 73.88999993408203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-320.pth.tar', 73.88399999023437)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-292.pth.tar', 73.81200004394532)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-315.pth.tar', 73.7900001171875)

Train: 327 [   0/1251 (  0%)]  Loss: 3.463 (3.46)  Time: 2.193s,  466.87/s  (2.193s,  466.87/s)  LR: 4.353e-04  Data: 1.578 (1.578)
Train: 327 [  50/1251 (  4%)]  Loss: 3.388 (3.43)  Time: 0.675s, 1516.33/s  (0.728s, 1406.38/s)  LR: 4.353e-04  Data: 0.010 (0.049)
Train: 327 [ 100/1251 (  8%)]  Loss: 3.547 (3.47)  Time: 0.702s, 1459.61/s  (0.707s, 1447.39/s)  LR: 4.353e-04  Data: 0.010 (0.030)
Train: 327 [ 150/1251 ( 12%)]  Loss: 3.395 (3.45)  Time: 0.675s, 1517.31/s  (0.701s, 1461.37/s)  LR: 4.353e-04  Data: 0.010 (0.023)
Train: 327 [ 200/1251 ( 16%)]  Loss: 3.500 (3.46)  Time: 0.669s, 1530.11/s  (0.699s, 1464.18/s)  LR: 4.353e-04  Data: 0.009 (0.020)
Train: 327 [ 250/1251 ( 20%)]  Loss: 3.422 (3.45)  Time: 0.681s, 1503.78/s  (0.697s, 1468.93/s)  LR: 4.353e-04  Data: 0.016 (0.018)
Train: 327 [ 300/1251 ( 24%)]  Loss: 3.424 (3.45)  Time: 0.669s, 1529.69/s  (0.696s, 1471.83/s)  LR: 4.353e-04  Data: 0.010 (0.017)
Train: 327 [ 350/1251 ( 28%)]  Loss: 3.886 (3.50)  Time: 0.699s, 1465.88/s  (0.694s, 1474.70/s)  LR: 4.353e-04  Data: 0.009 (0.016)
Train: 327 [ 400/1251 ( 32%)]  Loss: 3.613 (3.52)  Time: 0.692s, 1480.78/s  (0.694s, 1475.79/s)  LR: 4.353e-04  Data: 0.010 (0.015)
Train: 327 [ 450/1251 ( 36%)]  Loss: 3.170 (3.48)  Time: 0.671s, 1526.59/s  (0.694s, 1476.10/s)  LR: 4.353e-04  Data: 0.009 (0.015)
Train: 327 [ 500/1251 ( 40%)]  Loss: 3.571 (3.49)  Time: 0.720s, 1421.65/s  (0.693s, 1476.92/s)  LR: 4.353e-04  Data: 0.009 (0.014)
Train: 327 [ 550/1251 ( 44%)]  Loss: 3.856 (3.52)  Time: 0.671s, 1526.45/s  (0.693s, 1478.52/s)  LR: 4.353e-04  Data: 0.011 (0.014)
Train: 327 [ 600/1251 ( 48%)]  Loss: 4.066 (3.56)  Time: 0.675s, 1517.91/s  (0.693s, 1477.46/s)  LR: 4.353e-04  Data: 0.010 (0.014)
Train: 327 [ 650/1251 ( 52%)]  Loss: 3.691 (3.57)  Time: 0.698s, 1466.51/s  (0.692s, 1478.81/s)  LR: 4.353e-04  Data: 0.011 (0.013)
Train: 327 [ 700/1251 ( 56%)]  Loss: 3.418 (3.56)  Time: 0.695s, 1474.29/s  (0.693s, 1477.53/s)  LR: 4.353e-04  Data: 0.010 (0.013)
Train: 327 [ 750/1251 ( 60%)]  Loss: 3.933 (3.58)  Time: 0.764s, 1340.04/s  (0.693s, 1477.52/s)  LR: 4.353e-04  Data: 0.010 (0.013)
Train: 327 [ 800/1251 ( 64%)]  Loss: 3.633 (3.59)  Time: 0.705s, 1452.96/s  (0.693s, 1477.57/s)  LR: 4.353e-04  Data: 0.011 (0.013)
Train: 327 [ 850/1251 ( 68%)]  Loss: 3.518 (3.58)  Time: 0.705s, 1451.56/s  (0.693s, 1477.74/s)  LR: 4.353e-04  Data: 0.010 (0.013)
Train: 327 [ 900/1251 ( 72%)]  Loss: 3.635 (3.59)  Time: 0.693s, 1477.22/s  (0.693s, 1477.22/s)  LR: 4.353e-04  Data: 0.012 (0.013)
Train: 327 [ 950/1251 ( 76%)]  Loss: 3.533 (3.58)  Time: 0.669s, 1531.70/s  (0.693s, 1476.65/s)  LR: 4.353e-04  Data: 0.009 (0.013)
Train: 327 [1000/1251 ( 80%)]  Loss: 3.675 (3.59)  Time: 0.674s, 1518.74/s  (0.693s, 1476.80/s)  LR: 4.353e-04  Data: 0.014 (0.012)
Train: 327 [1050/1251 ( 84%)]  Loss: 3.811 (3.60)  Time: 0.684s, 1497.32/s  (0.693s, 1477.15/s)  LR: 4.353e-04  Data: 0.010 (0.012)
Train: 327 [1100/1251 ( 88%)]  Loss: 3.308 (3.59)  Time: 0.671s, 1525.30/s  (0.693s, 1477.47/s)  LR: 4.353e-04  Data: 0.010 (0.012)
Train: 327 [1150/1251 ( 92%)]  Loss: 3.787 (3.59)  Time: 0.668s, 1533.95/s  (0.693s, 1477.30/s)  LR: 4.353e-04  Data: 0.010 (0.012)
Train: 327 [1200/1251 ( 96%)]  Loss: 4.171 (3.62)  Time: 0.672s, 1524.57/s  (0.693s, 1478.05/s)  LR: 4.353e-04  Data: 0.009 (0.012)
Train: 327 [1250/1251 (100%)]  Loss: 3.520 (3.61)  Time: 0.687s, 1489.71/s  (0.693s, 1478.36/s)  LR: 4.353e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.437 (1.437)  Loss:  0.8672 (0.8672)  Acc@1: 89.3555 (89.3555)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.136 (0.575)  Loss:  0.8872 (1.4054)  Acc@1: 85.1415 (74.0380)  Acc@5: 96.9340 (91.9620)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-324.pth.tar', 74.2579999584961)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-323.pth.tar', 74.10000006347656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-327.pth.tar', 74.03800000976563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-326.pth.tar', 73.96200003173828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-325.pth.tar', 73.93599990722656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-314.pth.tar', 73.90399998291015)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-322.pth.tar', 73.89600011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-304.pth.tar', 73.88999993408203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-320.pth.tar', 73.88399999023437)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-292.pth.tar', 73.81200004394532)

Train: 328 [   0/1251 (  0%)]  Loss: 3.619 (3.62)  Time: 2.252s,  454.62/s  (2.252s,  454.62/s)  LR: 4.327e-04  Data: 1.634 (1.634)
Train: 328 [  50/1251 (  4%)]  Loss: 3.965 (3.79)  Time: 0.705s, 1451.62/s  (0.731s, 1400.11/s)  LR: 4.327e-04  Data: 0.011 (0.049)
Train: 328 [ 100/1251 (  8%)]  Loss: 3.709 (3.76)  Time: 0.674s, 1518.72/s  (0.713s, 1435.29/s)  LR: 4.327e-04  Data: 0.013 (0.030)
Train: 328 [ 150/1251 ( 12%)]  Loss: 3.764 (3.76)  Time: 0.672s, 1524.81/s  (0.706s, 1450.81/s)  LR: 4.327e-04  Data: 0.011 (0.024)
Train: 328 [ 200/1251 ( 16%)]  Loss: 3.308 (3.67)  Time: 0.674s, 1519.23/s  (0.704s, 1454.07/s)  LR: 4.327e-04  Data: 0.011 (0.020)
Train: 328 [ 250/1251 ( 20%)]  Loss: 3.711 (3.68)  Time: 0.712s, 1437.81/s  (0.703s, 1457.21/s)  LR: 4.327e-04  Data: 0.010 (0.018)
Train: 328 [ 300/1251 ( 24%)]  Loss: 3.687 (3.68)  Time: 0.705s, 1452.76/s  (0.701s, 1460.15/s)  LR: 4.327e-04  Data: 0.011 (0.017)
Train: 328 [ 350/1251 ( 28%)]  Loss: 3.182 (3.62)  Time: 0.698s, 1466.70/s  (0.701s, 1461.38/s)  LR: 4.327e-04  Data: 0.010 (0.016)
Train: 328 [ 400/1251 ( 32%)]  Loss: 3.488 (3.60)  Time: 0.672s, 1523.35/s  (0.700s, 1462.84/s)  LR: 4.327e-04  Data: 0.011 (0.015)
Train: 328 [ 450/1251 ( 36%)]  Loss: 3.589 (3.60)  Time: 0.675s, 1518.10/s  (0.699s, 1463.91/s)  LR: 4.327e-04  Data: 0.009 (0.015)
Train: 328 [ 500/1251 ( 40%)]  Loss: 3.618 (3.60)  Time: 0.753s, 1360.71/s  (0.698s, 1466.32/s)  LR: 4.327e-04  Data: 0.011 (0.014)
Train: 328 [ 550/1251 ( 44%)]  Loss: 3.833 (3.62)  Time: 0.672s, 1523.57/s  (0.698s, 1466.99/s)  LR: 4.327e-04  Data: 0.011 (0.014)
Train: 328 [ 600/1251 ( 48%)]  Loss: 3.603 (3.62)  Time: 0.726s, 1410.47/s  (0.698s, 1466.45/s)  LR: 4.327e-04  Data: 0.010 (0.014)
Train: 328 [ 650/1251 ( 52%)]  Loss: 3.364 (3.60)  Time: 0.676s, 1515.90/s  (0.698s, 1467.68/s)  LR: 4.327e-04  Data: 0.015 (0.013)
Train: 328 [ 700/1251 ( 56%)]  Loss: 3.516 (3.60)  Time: 0.675s, 1515.96/s  (0.697s, 1468.20/s)  LR: 4.327e-04  Data: 0.013 (0.013)
Train: 328 [ 750/1251 ( 60%)]  Loss: 3.340 (3.58)  Time: 0.675s, 1518.04/s  (0.697s, 1468.81/s)  LR: 4.327e-04  Data: 0.010 (0.013)
Train: 328 [ 800/1251 ( 64%)]  Loss: 3.788 (3.59)  Time: 0.666s, 1537.88/s  (0.697s, 1470.04/s)  LR: 4.327e-04  Data: 0.009 (0.013)
Train: 328 [ 850/1251 ( 68%)]  Loss: 3.824 (3.61)  Time: 0.711s, 1440.81/s  (0.696s, 1470.48/s)  LR: 4.327e-04  Data: 0.011 (0.013)
Train: 328 [ 900/1251 ( 72%)]  Loss: 3.978 (3.63)  Time: 0.672s, 1524.68/s  (0.696s, 1470.40/s)  LR: 4.327e-04  Data: 0.011 (0.013)
Train: 328 [ 950/1251 ( 76%)]  Loss: 3.839 (3.64)  Time: 0.729s, 1404.76/s  (0.697s, 1470.01/s)  LR: 4.327e-04  Data: 0.009 (0.013)
Train: 328 [1000/1251 ( 80%)]  Loss: 3.708 (3.64)  Time: 0.675s, 1516.09/s  (0.696s, 1471.21/s)  LR: 4.327e-04  Data: 0.017 (0.012)
Train: 328 [1050/1251 ( 84%)]  Loss: 3.530 (3.63)  Time: 0.677s, 1512.68/s  (0.696s, 1472.16/s)  LR: 4.327e-04  Data: 0.009 (0.012)
Train: 328 [1100/1251 ( 88%)]  Loss: 3.427 (3.63)  Time: 0.730s, 1403.09/s  (0.696s, 1471.86/s)  LR: 4.327e-04  Data: 0.013 (0.012)
Train: 328 [1150/1251 ( 92%)]  Loss: 3.531 (3.62)  Time: 0.698s, 1466.06/s  (0.696s, 1471.40/s)  LR: 4.327e-04  Data: 0.016 (0.012)
Train: 328 [1200/1251 ( 96%)]  Loss: 3.923 (3.63)  Time: 0.673s, 1522.66/s  (0.696s, 1472.01/s)  LR: 4.327e-04  Data: 0.011 (0.012)
Train: 328 [1250/1251 (100%)]  Loss: 3.278 (3.62)  Time: 0.674s, 1519.46/s  (0.695s, 1472.61/s)  LR: 4.327e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.433 (1.433)  Loss:  0.8286 (0.8286)  Acc@1: 89.5508 (89.5508)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.136 (0.581)  Loss:  1.0342 (1.4927)  Acc@1: 83.8443 (74.0460)  Acc@5: 95.9906 (91.9000)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-324.pth.tar', 74.2579999584961)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-323.pth.tar', 74.10000006347656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-328.pth.tar', 74.04600009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-327.pth.tar', 74.03800000976563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-326.pth.tar', 73.96200003173828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-325.pth.tar', 73.93599990722656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-314.pth.tar', 73.90399998291015)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-322.pth.tar', 73.89600011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-304.pth.tar', 73.88999993408203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-320.pth.tar', 73.88399999023437)

Train: 329 [   0/1251 (  0%)]  Loss: 3.881 (3.88)  Time: 2.183s,  469.11/s  (2.183s,  469.11/s)  LR: 4.301e-04  Data: 1.567 (1.567)
Train: 329 [  50/1251 (  4%)]  Loss: 3.461 (3.67)  Time: 0.697s, 1468.18/s  (0.744s, 1376.51/s)  LR: 4.301e-04  Data: 0.010 (0.049)
Train: 329 [ 100/1251 (  8%)]  Loss: 3.713 (3.69)  Time: 0.695s, 1473.41/s  (0.718s, 1426.73/s)  LR: 4.301e-04  Data: 0.009 (0.030)
Train: 329 [ 150/1251 ( 12%)]  Loss: 3.772 (3.71)  Time: 0.704s, 1454.55/s  (0.708s, 1445.46/s)  LR: 4.301e-04  Data: 0.009 (0.023)
Train: 329 [ 200/1251 ( 16%)]  Loss: 2.998 (3.57)  Time: 0.712s, 1438.21/s  (0.704s, 1455.13/s)  LR: 4.301e-04  Data: 0.010 (0.020)
Train: 329 [ 250/1251 ( 20%)]  Loss: 3.502 (3.55)  Time: 0.712s, 1438.07/s  (0.702s, 1458.47/s)  LR: 4.301e-04  Data: 0.010 (0.018)
Train: 329 [ 300/1251 ( 24%)]  Loss: 3.775 (3.59)  Time: 0.710s, 1441.93/s  (0.700s, 1462.32/s)  LR: 4.301e-04  Data: 0.009 (0.017)
Train: 329 [ 350/1251 ( 28%)]  Loss: 3.633 (3.59)  Time: 0.666s, 1537.62/s  (0.700s, 1462.69/s)  LR: 4.301e-04  Data: 0.010 (0.016)
Train: 329 [ 400/1251 ( 32%)]  Loss: 3.761 (3.61)  Time: 0.708s, 1445.45/s  (0.699s, 1464.48/s)  LR: 4.301e-04  Data: 0.010 (0.015)
Train: 329 [ 450/1251 ( 36%)]  Loss: 3.856 (3.64)  Time: 0.670s, 1527.54/s  (0.698s, 1467.02/s)  LR: 4.301e-04  Data: 0.010 (0.015)
Train: 329 [ 500/1251 ( 40%)]  Loss: 3.459 (3.62)  Time: 0.737s, 1390.15/s  (0.697s, 1468.34/s)  LR: 4.301e-04  Data: 0.010 (0.014)
Train: 329 [ 550/1251 ( 44%)]  Loss: 3.748 (3.63)  Time: 0.672s, 1524.86/s  (0.697s, 1468.66/s)  LR: 4.301e-04  Data: 0.009 (0.014)
Train: 329 [ 600/1251 ( 48%)]  Loss: 3.738 (3.64)  Time: 0.849s, 1206.79/s  (0.697s, 1468.25/s)  LR: 4.301e-04  Data: 0.009 (0.014)
Train: 329 [ 650/1251 ( 52%)]  Loss: 3.574 (3.63)  Time: 0.725s, 1413.13/s  (0.697s, 1468.44/s)  LR: 4.301e-04  Data: 0.011 (0.013)
Train: 329 [ 700/1251 ( 56%)]  Loss: 3.133 (3.60)  Time: 0.670s, 1529.35/s  (0.697s, 1469.72/s)  LR: 4.301e-04  Data: 0.011 (0.013)
Train: 329 [ 750/1251 ( 60%)]  Loss: 3.982 (3.62)  Time: 0.668s, 1533.92/s  (0.697s, 1469.98/s)  LR: 4.301e-04  Data: 0.011 (0.013)
Train: 329 [ 800/1251 ( 64%)]  Loss: 3.508 (3.62)  Time: 0.688s, 1487.38/s  (0.696s, 1470.95/s)  LR: 4.301e-04  Data: 0.010 (0.013)
Train: 329 [ 850/1251 ( 68%)]  Loss: 3.750 (3.62)  Time: 0.746s, 1373.31/s  (0.696s, 1471.56/s)  LR: 4.301e-04  Data: 0.009 (0.013)
Train: 329 [ 900/1251 ( 72%)]  Loss: 3.760 (3.63)  Time: 0.671s, 1525.07/s  (0.696s, 1471.67/s)  LR: 4.301e-04  Data: 0.010 (0.013)
Train: 329 [ 950/1251 ( 76%)]  Loss: 3.534 (3.63)  Time: 0.685s, 1494.53/s  (0.696s, 1471.56/s)  LR: 4.301e-04  Data: 0.010 (0.012)
Train: 329 [1000/1251 ( 80%)]  Loss: 3.561 (3.62)  Time: 0.725s, 1413.04/s  (0.696s, 1471.65/s)  LR: 4.301e-04  Data: 0.009 (0.012)
Train: 329 [1050/1251 ( 84%)]  Loss: 3.766 (3.63)  Time: 0.702s, 1459.08/s  (0.696s, 1472.21/s)  LR: 4.301e-04  Data: 0.009 (0.012)
Train: 329 [1100/1251 ( 88%)]  Loss: 3.656 (3.63)  Time: 0.683s, 1499.23/s  (0.695s, 1472.63/s)  LR: 4.301e-04  Data: 0.009 (0.012)
Train: 329 [1150/1251 ( 92%)]  Loss: 3.415 (3.62)  Time: 0.709s, 1443.53/s  (0.695s, 1472.66/s)  LR: 4.301e-04  Data: 0.010 (0.012)
Train: 329 [1200/1251 ( 96%)]  Loss: 3.720 (3.63)  Time: 0.706s, 1450.01/s  (0.695s, 1472.87/s)  LR: 4.301e-04  Data: 0.009 (0.012)
Train: 329 [1250/1251 (100%)]  Loss: 4.047 (3.64)  Time: 0.661s, 1550.32/s  (0.695s, 1473.05/s)  LR: 4.301e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.500 (1.500)  Loss:  0.8403 (0.8403)  Acc@1: 88.1836 (88.1836)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.136 (0.586)  Loss:  1.0059 (1.4089)  Acc@1: 83.0189 (73.9220)  Acc@5: 95.8726 (91.7720)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-324.pth.tar', 74.2579999584961)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-323.pth.tar', 74.10000006347656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-328.pth.tar', 74.04600009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-327.pth.tar', 74.03800000976563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-326.pth.tar', 73.96200003173828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-325.pth.tar', 73.93599990722656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-329.pth.tar', 73.92200012207032)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-314.pth.tar', 73.90399998291015)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-322.pth.tar', 73.89600011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-304.pth.tar', 73.88999993408203)

Train: 330 [   0/1251 (  0%)]  Loss: 3.660 (3.66)  Time: 2.255s,  454.11/s  (2.255s,  454.11/s)  LR: 4.276e-04  Data: 1.639 (1.639)
Train: 330 [  50/1251 (  4%)]  Loss: 3.567 (3.61)  Time: 0.712s, 1439.14/s  (0.726s, 1410.04/s)  LR: 4.276e-04  Data: 0.009 (0.045)
Train: 330 [ 100/1251 (  8%)]  Loss: 3.728 (3.65)  Time: 0.720s, 1422.28/s  (0.708s, 1446.93/s)  LR: 4.276e-04  Data: 0.011 (0.028)
Train: 330 [ 150/1251 ( 12%)]  Loss: 3.501 (3.61)  Time: 0.670s, 1528.40/s  (0.701s, 1460.38/s)  LR: 4.276e-04  Data: 0.012 (0.022)
Train: 330 [ 200/1251 ( 16%)]  Loss: 3.750 (3.64)  Time: 0.673s, 1522.33/s  (0.699s, 1465.69/s)  LR: 4.276e-04  Data: 0.011 (0.019)
Train: 330 [ 250/1251 ( 20%)]  Loss: 3.668 (3.65)  Time: 0.671s, 1525.41/s  (0.697s, 1469.29/s)  LR: 4.276e-04  Data: 0.011 (0.017)
Train: 330 [ 300/1251 ( 24%)]  Loss: 3.772 (3.66)  Time: 0.671s, 1526.04/s  (0.695s, 1472.44/s)  LR: 4.276e-04  Data: 0.010 (0.016)
Train: 330 [ 350/1251 ( 28%)]  Loss: 3.380 (3.63)  Time: 0.729s, 1404.67/s  (0.694s, 1475.26/s)  LR: 4.276e-04  Data: 0.010 (0.015)
Train: 330 [ 400/1251 ( 32%)]  Loss: 3.394 (3.60)  Time: 0.739s, 1386.13/s  (0.694s, 1474.57/s)  LR: 4.276e-04  Data: 0.017 (0.015)
Train: 330 [ 450/1251 ( 36%)]  Loss: 3.574 (3.60)  Time: 0.682s, 1500.77/s  (0.694s, 1475.32/s)  LR: 4.276e-04  Data: 0.011 (0.014)
Train: 330 [ 500/1251 ( 40%)]  Loss: 3.407 (3.58)  Time: 0.689s, 1485.34/s  (0.694s, 1475.77/s)  LR: 4.276e-04  Data: 0.013 (0.014)
Train: 330 [ 550/1251 ( 44%)]  Loss: 3.472 (3.57)  Time: 0.692s, 1480.26/s  (0.694s, 1475.25/s)  LR: 4.276e-04  Data: 0.011 (0.014)
Train: 330 [ 600/1251 ( 48%)]  Loss: 3.208 (3.54)  Time: 0.673s, 1521.40/s  (0.694s, 1476.20/s)  LR: 4.276e-04  Data: 0.010 (0.013)
Train: 330 [ 650/1251 ( 52%)]  Loss: 3.745 (3.56)  Time: 0.735s, 1393.23/s  (0.693s, 1476.81/s)  LR: 4.276e-04  Data: 0.011 (0.013)
Train: 330 [ 700/1251 ( 56%)]  Loss: 3.659 (3.57)  Time: 0.667s, 1535.63/s  (0.693s, 1477.32/s)  LR: 4.276e-04  Data: 0.010 (0.013)
Train: 330 [ 750/1251 ( 60%)]  Loss: 3.135 (3.54)  Time: 0.671s, 1524.97/s  (0.693s, 1477.80/s)  LR: 4.276e-04  Data: 0.009 (0.013)
Train: 330 [ 800/1251 ( 64%)]  Loss: 3.729 (3.55)  Time: 0.706s, 1451.31/s  (0.693s, 1477.61/s)  LR: 4.276e-04  Data: 0.011 (0.013)
Train: 330 [ 850/1251 ( 68%)]  Loss: 3.727 (3.56)  Time: 0.669s, 1530.52/s  (0.693s, 1477.58/s)  LR: 4.276e-04  Data: 0.010 (0.013)
Train: 330 [ 900/1251 ( 72%)]  Loss: 3.665 (3.57)  Time: 0.673s, 1522.51/s  (0.693s, 1478.14/s)  LR: 4.276e-04  Data: 0.011 (0.012)
Train: 330 [ 950/1251 ( 76%)]  Loss: 3.649 (3.57)  Time: 0.673s, 1520.89/s  (0.693s, 1478.68/s)  LR: 4.276e-04  Data: 0.011 (0.012)
Train: 330 [1000/1251 ( 80%)]  Loss: 3.801 (3.58)  Time: 0.672s, 1523.14/s  (0.692s, 1479.07/s)  LR: 4.276e-04  Data: 0.010 (0.012)
Train: 330 [1050/1251 ( 84%)]  Loss: 3.548 (3.58)  Time: 0.704s, 1455.18/s  (0.692s, 1479.26/s)  LR: 4.276e-04  Data: 0.010 (0.012)
Train: 330 [1100/1251 ( 88%)]  Loss: 3.403 (3.57)  Time: 0.666s, 1538.53/s  (0.693s, 1478.69/s)  LR: 4.276e-04  Data: 0.010 (0.012)
Train: 330 [1150/1251 ( 92%)]  Loss: 3.565 (3.57)  Time: 0.673s, 1521.33/s  (0.692s, 1479.13/s)  LR: 4.276e-04  Data: 0.011 (0.012)
Train: 330 [1200/1251 ( 96%)]  Loss: 3.741 (3.58)  Time: 0.678s, 1510.40/s  (0.692s, 1479.03/s)  LR: 4.276e-04  Data: 0.013 (0.012)
Train: 330 [1250/1251 (100%)]  Loss: 3.523 (3.58)  Time: 0.656s, 1560.45/s  (0.692s, 1479.39/s)  LR: 4.276e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.600 (1.600)  Loss:  0.7964 (0.7964)  Acc@1: 88.6719 (88.6719)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.136 (0.581)  Loss:  0.9141 (1.3429)  Acc@1: 84.9057 (73.8960)  Acc@5: 96.6981 (92.0160)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-324.pth.tar', 74.2579999584961)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-323.pth.tar', 74.10000006347656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-328.pth.tar', 74.04600009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-327.pth.tar', 74.03800000976563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-326.pth.tar', 73.96200003173828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-325.pth.tar', 73.93599990722656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-329.pth.tar', 73.92200012207032)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-314.pth.tar', 73.90399998291015)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-322.pth.tar', 73.89600011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-330.pth.tar', 73.89600003662109)

Train: 331 [   0/1251 (  0%)]  Loss: 3.685 (3.69)  Time: 2.328s,  439.83/s  (2.328s,  439.83/s)  LR: 4.250e-04  Data: 1.713 (1.713)
Train: 331 [  50/1251 (  4%)]  Loss: 3.555 (3.62)  Time: 0.673s, 1521.58/s  (0.740s, 1383.04/s)  LR: 4.250e-04  Data: 0.010 (0.055)
Train: 331 [ 100/1251 (  8%)]  Loss: 3.655 (3.63)  Time: 0.702s, 1459.23/s  (0.716s, 1430.67/s)  LR: 4.250e-04  Data: 0.009 (0.033)
Train: 331 [ 150/1251 ( 12%)]  Loss: 3.202 (3.52)  Time: 0.672s, 1524.02/s  (0.711s, 1439.76/s)  LR: 4.250e-04  Data: 0.012 (0.025)
Train: 331 [ 200/1251 ( 16%)]  Loss: 3.986 (3.62)  Time: 0.678s, 1509.36/s  (0.708s, 1446.30/s)  LR: 4.250e-04  Data: 0.009 (0.022)
Train: 331 [ 250/1251 ( 20%)]  Loss: 3.507 (3.60)  Time: 0.708s, 1447.29/s  (0.705s, 1452.35/s)  LR: 4.250e-04  Data: 0.010 (0.020)
Train: 331 [ 300/1251 ( 24%)]  Loss: 3.726 (3.62)  Time: 0.673s, 1520.77/s  (0.701s, 1460.14/s)  LR: 4.250e-04  Data: 0.010 (0.018)
Train: 331 [ 350/1251 ( 28%)]  Loss: 3.729 (3.63)  Time: 0.671s, 1525.78/s  (0.699s, 1464.27/s)  LR: 4.250e-04  Data: 0.010 (0.017)
Train: 331 [ 400/1251 ( 32%)]  Loss: 3.733 (3.64)  Time: 0.675s, 1516.40/s  (0.699s, 1464.62/s)  LR: 4.250e-04  Data: 0.009 (0.016)
Train: 331 [ 450/1251 ( 36%)]  Loss: 3.751 (3.65)  Time: 0.689s, 1487.20/s  (0.699s, 1465.63/s)  LR: 4.250e-04  Data: 0.011 (0.016)
Train: 331 [ 500/1251 ( 40%)]  Loss: 3.709 (3.66)  Time: 0.671s, 1525.74/s  (0.698s, 1466.79/s)  LR: 4.250e-04  Data: 0.010 (0.015)
Train: 331 [ 550/1251 ( 44%)]  Loss: 3.649 (3.66)  Time: 0.674s, 1519.74/s  (0.697s, 1468.48/s)  LR: 4.250e-04  Data: 0.009 (0.015)
Train: 331 [ 600/1251 ( 48%)]  Loss: 3.729 (3.66)  Time: 0.703s, 1455.94/s  (0.697s, 1468.60/s)  LR: 4.250e-04  Data: 0.009 (0.014)
Train: 331 [ 650/1251 ( 52%)]  Loss: 3.852 (3.68)  Time: 0.669s, 1531.25/s  (0.697s, 1469.17/s)  LR: 4.250e-04  Data: 0.013 (0.014)
Train: 331 [ 700/1251 ( 56%)]  Loss: 3.303 (3.65)  Time: 0.690s, 1484.24/s  (0.696s, 1471.18/s)  LR: 4.250e-04  Data: 0.009 (0.014)
Train: 331 [ 750/1251 ( 60%)]  Loss: 3.723 (3.66)  Time: 0.760s, 1346.93/s  (0.696s, 1472.31/s)  LR: 4.250e-04  Data: 0.010 (0.014)
Train: 331 [ 800/1251 ( 64%)]  Loss: 3.288 (3.63)  Time: 0.672s, 1524.92/s  (0.696s, 1471.73/s)  LR: 4.250e-04  Data: 0.010 (0.013)
Train: 331 [ 850/1251 ( 68%)]  Loss: 3.537 (3.63)  Time: 0.705s, 1453.24/s  (0.695s, 1472.54/s)  LR: 4.250e-04  Data: 0.009 (0.013)
Train: 331 [ 900/1251 ( 72%)]  Loss: 3.590 (3.63)  Time: 0.728s, 1406.49/s  (0.695s, 1472.94/s)  LR: 4.250e-04  Data: 0.009 (0.013)
Train: 331 [ 950/1251 ( 76%)]  Loss: 3.463 (3.62)  Time: 0.667s, 1536.34/s  (0.695s, 1473.34/s)  LR: 4.250e-04  Data: 0.011 (0.013)
Train: 331 [1000/1251 ( 80%)]  Loss: 3.888 (3.63)  Time: 0.673s, 1520.58/s  (0.695s, 1472.69/s)  LR: 4.250e-04  Data: 0.011 (0.013)
Train: 331 [1050/1251 ( 84%)]  Loss: 3.305 (3.62)  Time: 0.672s, 1523.83/s  (0.695s, 1472.85/s)  LR: 4.250e-04  Data: 0.011 (0.013)
Train: 331 [1100/1251 ( 88%)]  Loss: 3.945 (3.63)  Time: 0.676s, 1515.15/s  (0.695s, 1473.26/s)  LR: 4.250e-04  Data: 0.010 (0.013)
Train: 331 [1150/1251 ( 92%)]  Loss: 3.641 (3.63)  Time: 0.670s, 1527.85/s  (0.695s, 1473.57/s)  LR: 4.250e-04  Data: 0.010 (0.013)
Train: 331 [1200/1251 ( 96%)]  Loss: 3.442 (3.62)  Time: 0.670s, 1528.09/s  (0.695s, 1474.12/s)  LR: 4.250e-04  Data: 0.010 (0.012)
Train: 331 [1250/1251 (100%)]  Loss: 3.711 (3.63)  Time: 0.653s, 1566.95/s  (0.694s, 1474.97/s)  LR: 4.250e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.597 (1.597)  Loss:  0.8203 (0.8203)  Acc@1: 89.5508 (89.5508)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.136 (0.587)  Loss:  0.9043 (1.3198)  Acc@1: 83.8443 (74.1680)  Acc@5: 95.5189 (91.9100)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-324.pth.tar', 74.2579999584961)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-331.pth.tar', 74.16800009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-323.pth.tar', 74.10000006347656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-328.pth.tar', 74.04600009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-327.pth.tar', 74.03800000976563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-326.pth.tar', 73.96200003173828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-325.pth.tar', 73.93599990722656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-329.pth.tar', 73.92200012207032)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-314.pth.tar', 73.90399998291015)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-322.pth.tar', 73.89600011230469)

Train: 332 [   0/1251 (  0%)]  Loss: 3.749 (3.75)  Time: 2.235s,  458.18/s  (2.235s,  458.18/s)  LR: 4.224e-04  Data: 1.578 (1.578)
Train: 332 [  50/1251 (  4%)]  Loss: 3.412 (3.58)  Time: 0.671s, 1525.48/s  (0.729s, 1405.44/s)  LR: 4.224e-04  Data: 0.011 (0.046)
Train: 332 [ 100/1251 (  8%)]  Loss: 3.647 (3.60)  Time: 0.680s, 1505.94/s  (0.714s, 1434.56/s)  LR: 4.224e-04  Data: 0.010 (0.029)
Train: 332 [ 150/1251 ( 12%)]  Loss: 3.251 (3.51)  Time: 0.672s, 1523.03/s  (0.705s, 1452.00/s)  LR: 4.224e-04  Data: 0.010 (0.023)
Train: 332 [ 200/1251 ( 16%)]  Loss: 3.509 (3.51)  Time: 0.692s, 1479.65/s  (0.703s, 1457.20/s)  LR: 4.224e-04  Data: 0.012 (0.020)
Train: 332 [ 250/1251 ( 20%)]  Loss: 3.813 (3.56)  Time: 0.667s, 1534.24/s  (0.701s, 1461.18/s)  LR: 4.224e-04  Data: 0.008 (0.018)
Train: 332 [ 300/1251 ( 24%)]  Loss: 3.307 (3.53)  Time: 0.735s, 1393.70/s  (0.698s, 1467.17/s)  LR: 4.224e-04  Data: 0.010 (0.017)
Train: 332 [ 350/1251 ( 28%)]  Loss: 3.474 (3.52)  Time: 0.716s, 1430.50/s  (0.697s, 1470.17/s)  LR: 4.224e-04  Data: 0.010 (0.016)
Train: 332 [ 400/1251 ( 32%)]  Loss: 3.532 (3.52)  Time: 0.698s, 1467.60/s  (0.697s, 1468.63/s)  LR: 4.224e-04  Data: 0.009 (0.015)
Train: 332 [ 450/1251 ( 36%)]  Loss: 3.657 (3.54)  Time: 0.677s, 1511.93/s  (0.697s, 1469.97/s)  LR: 4.224e-04  Data: 0.009 (0.014)
Train: 332 [ 500/1251 ( 40%)]  Loss: 3.553 (3.54)  Time: 0.672s, 1523.61/s  (0.696s, 1472.11/s)  LR: 4.224e-04  Data: 0.013 (0.014)
Train: 332 [ 550/1251 ( 44%)]  Loss: 3.723 (3.55)  Time: 0.684s, 1496.12/s  (0.696s, 1471.99/s)  LR: 4.224e-04  Data: 0.011 (0.014)
Train: 332 [ 600/1251 ( 48%)]  Loss: 3.946 (3.58)  Time: 0.707s, 1447.59/s  (0.695s, 1472.97/s)  LR: 4.224e-04  Data: 0.009 (0.013)
Train: 332 [ 650/1251 ( 52%)]  Loss: 3.540 (3.58)  Time: 0.669s, 1531.33/s  (0.695s, 1474.17/s)  LR: 4.224e-04  Data: 0.010 (0.013)
Train: 332 [ 700/1251 ( 56%)]  Loss: 3.846 (3.60)  Time: 0.679s, 1508.54/s  (0.694s, 1474.80/s)  LR: 4.224e-04  Data: 0.011 (0.013)
Train: 332 [ 750/1251 ( 60%)]  Loss: 3.767 (3.61)  Time: 0.699s, 1463.98/s  (0.694s, 1474.94/s)  LR: 4.224e-04  Data: 0.009 (0.013)
Train: 332 [ 800/1251 ( 64%)]  Loss: 3.627 (3.61)  Time: 0.721s, 1421.12/s  (0.694s, 1475.56/s)  LR: 4.224e-04  Data: 0.010 (0.013)
Train: 332 [ 850/1251 ( 68%)]  Loss: 4.096 (3.64)  Time: 0.722s, 1417.63/s  (0.694s, 1475.73/s)  LR: 4.224e-04  Data: 0.010 (0.013)
Train: 332 [ 900/1251 ( 72%)]  Loss: 3.945 (3.65)  Time: 0.673s, 1521.25/s  (0.694s, 1476.04/s)  LR: 4.224e-04  Data: 0.014 (0.012)
Train: 332 [ 950/1251 ( 76%)]  Loss: 3.097 (3.62)  Time: 0.672s, 1523.39/s  (0.694s, 1476.09/s)  LR: 4.224e-04  Data: 0.010 (0.012)
Train: 332 [1000/1251 ( 80%)]  Loss: 3.530 (3.62)  Time: 0.704s, 1454.34/s  (0.694s, 1476.23/s)  LR: 4.224e-04  Data: 0.010 (0.012)
Train: 332 [1050/1251 ( 84%)]  Loss: 3.721 (3.62)  Time: 0.703s, 1457.24/s  (0.693s, 1476.99/s)  LR: 4.224e-04  Data: 0.011 (0.012)
Train: 332 [1100/1251 ( 88%)]  Loss: 3.568 (3.62)  Time: 0.717s, 1428.06/s  (0.694s, 1476.29/s)  LR: 4.224e-04  Data: 0.009 (0.012)
Train: 332 [1150/1251 ( 92%)]  Loss: 3.558 (3.62)  Time: 0.685s, 1495.15/s  (0.694s, 1476.16/s)  LR: 4.224e-04  Data: 0.012 (0.012)
Train: 332 [1200/1251 ( 96%)]  Loss: 3.644 (3.62)  Time: 0.710s, 1441.81/s  (0.694s, 1476.35/s)  LR: 4.224e-04  Data: 0.009 (0.012)
Train: 332 [1250/1251 (100%)]  Loss: 3.330 (3.61)  Time: 0.657s, 1558.81/s  (0.693s, 1477.11/s)  LR: 4.224e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.569 (1.569)  Loss:  0.8369 (0.8369)  Acc@1: 89.9414 (89.9414)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.137 (0.575)  Loss:  0.9507 (1.3889)  Acc@1: 85.0236 (74.4460)  Acc@5: 96.5802 (92.0960)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-332.pth.tar', 74.44599995849609)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-324.pth.tar', 74.2579999584961)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-331.pth.tar', 74.16800009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-323.pth.tar', 74.10000006347656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-328.pth.tar', 74.04600009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-327.pth.tar', 74.03800000976563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-326.pth.tar', 73.96200003173828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-325.pth.tar', 73.93599990722656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-329.pth.tar', 73.92200012207032)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-314.pth.tar', 73.90399998291015)

Train: 333 [   0/1251 (  0%)]  Loss: 3.508 (3.51)  Time: 2.017s,  507.69/s  (2.017s,  507.69/s)  LR: 4.199e-04  Data: 1.402 (1.402)
Train: 333 [  50/1251 (  4%)]  Loss: 3.410 (3.46)  Time: 0.737s, 1388.51/s  (0.745s, 1374.64/s)  LR: 4.199e-04  Data: 0.009 (0.048)
Train: 333 [ 100/1251 (  8%)]  Loss: 3.230 (3.38)  Time: 0.684s, 1496.40/s  (0.730s, 1403.67/s)  LR: 4.199e-04  Data: 0.010 (0.031)
Train: 333 [ 150/1251 ( 12%)]  Loss: 3.738 (3.47)  Time: 0.767s, 1334.55/s  (0.723s, 1415.85/s)  LR: 4.199e-04  Data: 0.010 (0.024)
Train: 333 [ 200/1251 ( 16%)]  Loss: 3.652 (3.51)  Time: 0.712s, 1438.80/s  (0.719s, 1423.47/s)  LR: 4.199e-04  Data: 0.010 (0.021)
Train: 333 [ 250/1251 ( 20%)]  Loss: 3.567 (3.52)  Time: 0.671s, 1525.96/s  (0.713s, 1436.97/s)  LR: 4.199e-04  Data: 0.011 (0.019)
Train: 333 [ 300/1251 ( 24%)]  Loss: 3.151 (3.47)  Time: 0.700s, 1463.11/s  (0.708s, 1446.78/s)  LR: 4.199e-04  Data: 0.010 (0.017)
Train: 333 [ 350/1251 ( 28%)]  Loss: 3.134 (3.42)  Time: 0.677s, 1512.93/s  (0.706s, 1449.93/s)  LR: 4.199e-04  Data: 0.013 (0.016)
Train: 333 [ 400/1251 ( 32%)]  Loss: 3.587 (3.44)  Time: 0.696s, 1472.18/s  (0.706s, 1451.38/s)  LR: 4.199e-04  Data: 0.010 (0.016)
Train: 333 [ 450/1251 ( 36%)]  Loss: 3.640 (3.46)  Time: 0.673s, 1521.79/s  (0.704s, 1454.31/s)  LR: 4.199e-04  Data: 0.013 (0.015)
Train: 333 [ 500/1251 ( 40%)]  Loss: 3.907 (3.50)  Time: 0.694s, 1476.15/s  (0.703s, 1457.01/s)  LR: 4.199e-04  Data: 0.016 (0.015)
Train: 333 [ 550/1251 ( 44%)]  Loss: 3.593 (3.51)  Time: 0.674s, 1519.28/s  (0.702s, 1458.40/s)  LR: 4.199e-04  Data: 0.011 (0.014)
Train: 333 [ 600/1251 ( 48%)]  Loss: 3.779 (3.53)  Time: 0.761s, 1345.26/s  (0.701s, 1461.00/s)  LR: 4.199e-04  Data: 0.009 (0.014)
Train: 333 [ 650/1251 ( 52%)]  Loss: 3.566 (3.53)  Time: 0.700s, 1461.90/s  (0.700s, 1462.23/s)  LR: 4.199e-04  Data: 0.009 (0.014)
Train: 333 [ 700/1251 ( 56%)]  Loss: 3.501 (3.53)  Time: 0.665s, 1539.58/s  (0.699s, 1463.91/s)  LR: 4.199e-04  Data: 0.010 (0.013)
Train: 333 [ 750/1251 ( 60%)]  Loss: 3.522 (3.53)  Time: 0.708s, 1446.75/s  (0.699s, 1465.00/s)  LR: 4.199e-04  Data: 0.010 (0.013)
Train: 333 [ 800/1251 ( 64%)]  Loss: 3.696 (3.54)  Time: 0.671s, 1527.18/s  (0.699s, 1464.77/s)  LR: 4.199e-04  Data: 0.010 (0.013)
Train: 333 [ 850/1251 ( 68%)]  Loss: 3.548 (3.54)  Time: 0.672s, 1524.44/s  (0.698s, 1466.08/s)  LR: 4.199e-04  Data: 0.010 (0.013)
Train: 333 [ 900/1251 ( 72%)]  Loss: 3.876 (3.56)  Time: 0.805s, 1271.56/s  (0.698s, 1466.76/s)  LR: 4.199e-04  Data: 0.010 (0.013)
Train: 333 [ 950/1251 ( 76%)]  Loss: 3.196 (3.54)  Time: 0.666s, 1537.71/s  (0.698s, 1466.74/s)  LR: 4.199e-04  Data: 0.009 (0.013)
Train: 333 [1000/1251 ( 80%)]  Loss: 3.600 (3.54)  Time: 0.733s, 1397.65/s  (0.698s, 1467.22/s)  LR: 4.199e-04  Data: 0.008 (0.012)
Train: 333 [1050/1251 ( 84%)]  Loss: 3.547 (3.54)  Time: 0.739s, 1386.33/s  (0.697s, 1468.43/s)  LR: 4.199e-04  Data: 0.009 (0.012)
Train: 333 [1100/1251 ( 88%)]  Loss: 3.921 (3.56)  Time: 0.673s, 1521.49/s  (0.697s, 1469.88/s)  LR: 4.199e-04  Data: 0.010 (0.012)
Train: 333 [1150/1251 ( 92%)]  Loss: 3.451 (3.55)  Time: 0.738s, 1388.35/s  (0.697s, 1469.25/s)  LR: 4.199e-04  Data: 0.014 (0.012)
Train: 333 [1200/1251 ( 96%)]  Loss: 3.405 (3.55)  Time: 0.706s, 1450.52/s  (0.697s, 1469.27/s)  LR: 4.199e-04  Data: 0.009 (0.012)
Train: 333 [1250/1251 (100%)]  Loss: 3.035 (3.53)  Time: 0.657s, 1558.70/s  (0.697s, 1469.33/s)  LR: 4.199e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.588 (1.588)  Loss:  0.8379 (0.8379)  Acc@1: 89.5508 (89.5508)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.137 (0.589)  Loss:  0.9932 (1.4259)  Acc@1: 83.8443 (73.9760)  Acc@5: 95.6368 (91.8980)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-332.pth.tar', 74.44599995849609)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-324.pth.tar', 74.2579999584961)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-331.pth.tar', 74.16800009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-323.pth.tar', 74.10000006347656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-328.pth.tar', 74.04600009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-327.pth.tar', 74.03800000976563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-333.pth.tar', 73.97599996337891)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-326.pth.tar', 73.96200003173828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-325.pth.tar', 73.93599990722656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-329.pth.tar', 73.92200012207032)

Train: 334 [   0/1251 (  0%)]  Loss: 3.423 (3.42)  Time: 2.211s,  463.18/s  (2.211s,  463.18/s)  LR: 4.173e-04  Data: 1.597 (1.597)
Train: 334 [  50/1251 (  4%)]  Loss: 3.691 (3.56)  Time: 0.666s, 1538.12/s  (0.735s, 1393.52/s)  LR: 4.173e-04  Data: 0.010 (0.048)
Train: 334 [ 100/1251 (  8%)]  Loss: 3.465 (3.53)  Time: 0.675s, 1516.07/s  (0.713s, 1436.77/s)  LR: 4.173e-04  Data: 0.011 (0.030)
Train: 334 [ 150/1251 ( 12%)]  Loss: 3.781 (3.59)  Time: 0.709s, 1443.68/s  (0.706s, 1450.25/s)  LR: 4.173e-04  Data: 0.011 (0.023)
Train: 334 [ 200/1251 ( 16%)]  Loss: 3.853 (3.64)  Time: 0.684s, 1498.05/s  (0.704s, 1453.67/s)  LR: 4.173e-04  Data: 0.010 (0.020)
Train: 334 [ 250/1251 ( 20%)]  Loss: 3.268 (3.58)  Time: 0.735s, 1392.74/s  (0.701s, 1460.20/s)  LR: 4.173e-04  Data: 0.010 (0.018)
Train: 334 [ 300/1251 ( 24%)]  Loss: 3.382 (3.55)  Time: 0.672s, 1523.33/s  (0.699s, 1463.97/s)  LR: 4.173e-04  Data: 0.010 (0.017)
Train: 334 [ 350/1251 ( 28%)]  Loss: 3.735 (3.57)  Time: 0.707s, 1448.24/s  (0.698s, 1467.59/s)  LR: 4.173e-04  Data: 0.010 (0.016)
Train: 334 [ 400/1251 ( 32%)]  Loss: 3.297 (3.54)  Time: 0.673s, 1521.61/s  (0.697s, 1470.06/s)  LR: 4.173e-04  Data: 0.011 (0.015)
Train: 334 [ 450/1251 ( 36%)]  Loss: 3.562 (3.55)  Time: 0.747s, 1371.32/s  (0.696s, 1470.83/s)  LR: 4.173e-04  Data: 0.013 (0.015)
Train: 334 [ 500/1251 ( 40%)]  Loss: 3.714 (3.56)  Time: 0.669s, 1530.15/s  (0.695s, 1472.48/s)  LR: 4.173e-04  Data: 0.009 (0.014)
Train: 334 [ 550/1251 ( 44%)]  Loss: 3.693 (3.57)  Time: 0.763s, 1342.94/s  (0.695s, 1472.59/s)  LR: 4.173e-04  Data: 0.009 (0.014)
Train: 334 [ 600/1251 ( 48%)]  Loss: 3.352 (3.56)  Time: 0.740s, 1384.09/s  (0.695s, 1473.20/s)  LR: 4.173e-04  Data: 0.018 (0.014)
Train: 334 [ 650/1251 ( 52%)]  Loss: 3.711 (3.57)  Time: 0.687s, 1491.52/s  (0.695s, 1473.17/s)  LR: 4.173e-04  Data: 0.010 (0.013)
Train: 334 [ 700/1251 ( 56%)]  Loss: 3.578 (3.57)  Time: 0.700s, 1463.20/s  (0.695s, 1474.27/s)  LR: 4.173e-04  Data: 0.010 (0.013)
Train: 334 [ 750/1251 ( 60%)]  Loss: 3.529 (3.56)  Time: 0.708s, 1445.69/s  (0.694s, 1474.79/s)  LR: 4.173e-04  Data: 0.010 (0.013)
Train: 334 [ 800/1251 ( 64%)]  Loss: 3.633 (3.57)  Time: 0.674s, 1519.30/s  (0.694s, 1475.08/s)  LR: 4.173e-04  Data: 0.010 (0.013)
Train: 334 [ 850/1251 ( 68%)]  Loss: 3.782 (3.58)  Time: 0.714s, 1434.66/s  (0.694s, 1475.62/s)  LR: 4.173e-04  Data: 0.011 (0.013)
Train: 334 [ 900/1251 ( 72%)]  Loss: 3.259 (3.56)  Time: 0.682s, 1501.52/s  (0.694s, 1476.13/s)  LR: 4.173e-04  Data: 0.011 (0.012)
Train: 334 [ 950/1251 ( 76%)]  Loss: 3.489 (3.56)  Time: 0.709s, 1444.27/s  (0.693s, 1476.78/s)  LR: 4.173e-04  Data: 0.010 (0.012)
Train: 334 [1000/1251 ( 80%)]  Loss: 3.806 (3.57)  Time: 0.766s, 1337.06/s  (0.694s, 1476.07/s)  LR: 4.173e-04  Data: 0.010 (0.012)
Train: 334 [1050/1251 ( 84%)]  Loss: 3.399 (3.56)  Time: 0.684s, 1497.02/s  (0.693s, 1476.98/s)  LR: 4.173e-04  Data: 0.012 (0.012)
Train: 334 [1100/1251 ( 88%)]  Loss: 3.537 (3.56)  Time: 0.676s, 1514.78/s  (0.693s, 1477.36/s)  LR: 4.173e-04  Data: 0.011 (0.012)
Train: 334 [1150/1251 ( 92%)]  Loss: 3.380 (3.55)  Time: 0.706s, 1449.41/s  (0.693s, 1478.46/s)  LR: 4.173e-04  Data: 0.011 (0.012)
Train: 334 [1200/1251 ( 96%)]  Loss: 3.881 (3.57)  Time: 0.703s, 1457.22/s  (0.692s, 1478.90/s)  LR: 4.173e-04  Data: 0.009 (0.012)
Train: 334 [1250/1251 (100%)]  Loss: 3.653 (3.57)  Time: 0.686s, 1492.01/s  (0.692s, 1478.76/s)  LR: 4.173e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.454 (1.454)  Loss:  0.8403 (0.8403)  Acc@1: 89.5508 (89.5508)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.136 (0.582)  Loss:  0.8745 (1.3821)  Acc@1: 84.1981 (73.5700)  Acc@5: 96.3443 (91.5940)
Train: 335 [   0/1251 (  0%)]  Loss: 3.442 (3.44)  Time: 2.217s,  461.83/s  (2.217s,  461.83/s)  LR: 4.148e-04  Data: 1.602 (1.602)
Train: 335 [  50/1251 (  4%)]  Loss: 3.226 (3.33)  Time: 0.670s, 1527.77/s  (0.736s, 1391.12/s)  LR: 4.148e-04  Data: 0.010 (0.053)
Train: 335 [ 100/1251 (  8%)]  Loss: 3.379 (3.35)  Time: 0.701s, 1459.84/s  (0.713s, 1436.63/s)  LR: 4.148e-04  Data: 0.011 (0.032)
Train: 335 [ 150/1251 ( 12%)]  Loss: 3.650 (3.42)  Time: 0.706s, 1450.39/s  (0.706s, 1451.21/s)  LR: 4.148e-04  Data: 0.011 (0.025)
Train: 335 [ 200/1251 ( 16%)]  Loss: 3.718 (3.48)  Time: 0.676s, 1515.84/s  (0.704s, 1453.97/s)  LR: 4.148e-04  Data: 0.010 (0.021)
Train: 335 [ 250/1251 ( 20%)]  Loss: 3.680 (3.52)  Time: 0.670s, 1527.74/s  (0.701s, 1459.84/s)  LR: 4.148e-04  Data: 0.009 (0.019)
Train: 335 [ 300/1251 ( 24%)]  Loss: 3.485 (3.51)  Time: 0.676s, 1514.18/s  (0.701s, 1461.62/s)  LR: 4.148e-04  Data: 0.011 (0.018)
Train: 335 [ 350/1251 ( 28%)]  Loss: 3.342 (3.49)  Time: 0.702s, 1459.18/s  (0.699s, 1464.11/s)  LR: 4.148e-04  Data: 0.008 (0.017)
Train: 335 [ 400/1251 ( 32%)]  Loss: 3.507 (3.49)  Time: 0.674s, 1519.94/s  (0.699s, 1465.78/s)  LR: 4.148e-04  Data: 0.010 (0.016)
Train: 335 [ 450/1251 ( 36%)]  Loss: 3.427 (3.49)  Time: 0.673s, 1522.54/s  (0.698s, 1466.52/s)  LR: 4.148e-04  Data: 0.012 (0.015)
Train: 335 [ 500/1251 ( 40%)]  Loss: 3.723 (3.51)  Time: 0.695s, 1473.05/s  (0.698s, 1467.99/s)  LR: 4.148e-04  Data: 0.010 (0.015)
Train: 335 [ 550/1251 ( 44%)]  Loss: 3.783 (3.53)  Time: 0.676s, 1514.66/s  (0.697s, 1468.55/s)  LR: 4.148e-04  Data: 0.011 (0.014)
Train: 335 [ 600/1251 ( 48%)]  Loss: 2.944 (3.49)  Time: 0.727s, 1408.77/s  (0.697s, 1470.12/s)  LR: 4.148e-04  Data: 0.009 (0.014)
Train: 335 [ 650/1251 ( 52%)]  Loss: 3.468 (3.48)  Time: 0.667s, 1534.18/s  (0.697s, 1469.07/s)  LR: 4.148e-04  Data: 0.011 (0.014)
Train: 335 [ 700/1251 ( 56%)]  Loss: 3.648 (3.49)  Time: 0.670s, 1528.08/s  (0.696s, 1470.37/s)  LR: 4.148e-04  Data: 0.010 (0.014)
Train: 335 [ 750/1251 ( 60%)]  Loss: 3.720 (3.51)  Time: 0.676s, 1513.79/s  (0.696s, 1470.78/s)  LR: 4.148e-04  Data: 0.015 (0.013)
Train: 335 [ 800/1251 ( 64%)]  Loss: 3.379 (3.50)  Time: 0.673s, 1521.98/s  (0.696s, 1471.98/s)  LR: 4.148e-04  Data: 0.010 (0.013)
Train: 335 [ 850/1251 ( 68%)]  Loss: 3.773 (3.52)  Time: 0.675s, 1517.31/s  (0.696s, 1472.17/s)  LR: 4.148e-04  Data: 0.011 (0.013)
Train: 335 [ 900/1251 ( 72%)]  Loss: 3.064 (3.49)  Time: 0.673s, 1520.44/s  (0.695s, 1472.93/s)  LR: 4.148e-04  Data: 0.010 (0.013)
Train: 335 [ 950/1251 ( 76%)]  Loss: 3.668 (3.50)  Time: 0.752s, 1361.47/s  (0.696s, 1472.24/s)  LR: 4.148e-04  Data: 0.010 (0.013)
Train: 335 [1000/1251 ( 80%)]  Loss: 3.956 (3.52)  Time: 0.707s, 1449.27/s  (0.696s, 1471.86/s)  LR: 4.148e-04  Data: 0.010 (0.013)
Train: 335 [1050/1251 ( 84%)]  Loss: 3.427 (3.52)  Time: 0.757s, 1353.56/s  (0.696s, 1472.09/s)  LR: 4.148e-04  Data: 0.011 (0.013)
Train: 335 [1100/1251 ( 88%)]  Loss: 3.668 (3.53)  Time: 0.689s, 1486.55/s  (0.695s, 1473.25/s)  LR: 4.148e-04  Data: 0.009 (0.012)
Train: 335 [1150/1251 ( 92%)]  Loss: 3.758 (3.53)  Time: 0.671s, 1526.44/s  (0.695s, 1473.90/s)  LR: 4.148e-04  Data: 0.010 (0.012)
Train: 335 [1200/1251 ( 96%)]  Loss: 3.614 (3.54)  Time: 0.703s, 1456.47/s  (0.694s, 1474.74/s)  LR: 4.148e-04  Data: 0.014 (0.012)
Train: 335 [1250/1251 (100%)]  Loss: 3.283 (3.53)  Time: 0.660s, 1551.97/s  (0.694s, 1474.72/s)  LR: 4.148e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.469 (1.469)  Loss:  0.8022 (0.8022)  Acc@1: 88.7695 (88.7695)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.136 (0.583)  Loss:  0.8618 (1.3451)  Acc@1: 84.0802 (73.7540)  Acc@5: 95.6368 (91.7640)
Train: 336 [   0/1251 (  0%)]  Loss: 3.760 (3.76)  Time: 2.213s,  462.63/s  (2.213s,  462.63/s)  LR: 4.122e-04  Data: 1.599 (1.599)
Train: 336 [  50/1251 (  4%)]  Loss: 3.625 (3.69)  Time: 0.715s, 1432.42/s  (0.731s, 1400.77/s)  LR: 4.122e-04  Data: 0.014 (0.046)
Train: 336 [ 100/1251 (  8%)]  Loss: 3.697 (3.69)  Time: 0.731s, 1400.91/s  (0.714s, 1433.85/s)  LR: 4.122e-04  Data: 0.009 (0.029)
Train: 336 [ 150/1251 ( 12%)]  Loss: 3.299 (3.60)  Time: 0.706s, 1450.75/s  (0.706s, 1449.51/s)  LR: 4.122e-04  Data: 0.011 (0.023)
Train: 336 [ 200/1251 ( 16%)]  Loss: 3.155 (3.51)  Time: 0.726s, 1410.25/s  (0.703s, 1457.36/s)  LR: 4.122e-04  Data: 0.011 (0.020)
Train: 336 [ 250/1251 ( 20%)]  Loss: 3.444 (3.50)  Time: 0.696s, 1470.65/s  (0.699s, 1463.91/s)  LR: 4.122e-04  Data: 0.010 (0.018)
Train: 336 [ 300/1251 ( 24%)]  Loss: 3.214 (3.46)  Time: 0.705s, 1452.67/s  (0.697s, 1469.10/s)  LR: 4.122e-04  Data: 0.009 (0.017)
Train: 336 [ 350/1251 ( 28%)]  Loss: 3.646 (3.48)  Time: 0.707s, 1448.49/s  (0.697s, 1469.73/s)  LR: 4.122e-04  Data: 0.013 (0.016)
Train: 336 [ 400/1251 ( 32%)]  Loss: 3.505 (3.48)  Time: 0.711s, 1439.68/s  (0.696s, 1470.55/s)  LR: 4.122e-04  Data: 0.010 (0.015)
Train: 336 [ 450/1251 ( 36%)]  Loss: 3.302 (3.46)  Time: 0.672s, 1524.71/s  (0.695s, 1472.55/s)  LR: 4.122e-04  Data: 0.010 (0.014)
Train: 336 [ 500/1251 ( 40%)]  Loss: 3.553 (3.47)  Time: 0.708s, 1445.60/s  (0.695s, 1474.21/s)  LR: 4.122e-04  Data: 0.011 (0.014)
Train: 336 [ 550/1251 ( 44%)]  Loss: 3.616 (3.48)  Time: 0.702s, 1458.95/s  (0.694s, 1475.25/s)  LR: 4.122e-04  Data: 0.009 (0.014)
Train: 336 [ 600/1251 ( 48%)]  Loss: 3.559 (3.49)  Time: 0.706s, 1450.23/s  (0.694s, 1476.52/s)  LR: 4.122e-04  Data: 0.009 (0.013)
Train: 336 [ 650/1251 ( 52%)]  Loss: 3.689 (3.50)  Time: 0.670s, 1527.61/s  (0.693s, 1477.24/s)  LR: 4.122e-04  Data: 0.011 (0.013)
Train: 336 [ 700/1251 ( 56%)]  Loss: 3.636 (3.51)  Time: 0.667s, 1534.09/s  (0.693s, 1478.14/s)  LR: 4.122e-04  Data: 0.010 (0.013)
Train: 336 [ 750/1251 ( 60%)]  Loss: 4.011 (3.54)  Time: 0.674s, 1519.67/s  (0.692s, 1478.89/s)  LR: 4.122e-04  Data: 0.009 (0.013)
Train: 336 [ 800/1251 ( 64%)]  Loss: 3.494 (3.54)  Time: 0.718s, 1426.60/s  (0.693s, 1478.62/s)  LR: 4.122e-04  Data: 0.009 (0.013)
Train: 336 [ 850/1251 ( 68%)]  Loss: 3.712 (3.55)  Time: 0.701s, 1460.04/s  (0.693s, 1476.92/s)  LR: 4.122e-04  Data: 0.010 (0.012)
Train: 336 [ 900/1251 ( 72%)]  Loss: 2.882 (3.52)  Time: 0.672s, 1522.92/s  (0.693s, 1476.76/s)  LR: 4.122e-04  Data: 0.011 (0.012)
Train: 336 [ 950/1251 ( 76%)]  Loss: 3.605 (3.52)  Time: 0.678s, 1510.74/s  (0.693s, 1477.10/s)  LR: 4.122e-04  Data: 0.014 (0.012)
Train: 336 [1000/1251 ( 80%)]  Loss: 3.459 (3.52)  Time: 0.666s, 1538.66/s  (0.693s, 1477.23/s)  LR: 4.122e-04  Data: 0.010 (0.012)
Train: 336 [1050/1251 ( 84%)]  Loss: 3.706 (3.53)  Time: 0.667s, 1534.52/s  (0.693s, 1477.65/s)  LR: 4.122e-04  Data: 0.010 (0.012)
Train: 336 [1100/1251 ( 88%)]  Loss: 3.358 (3.52)  Time: 0.672s, 1524.24/s  (0.693s, 1477.70/s)  LR: 4.122e-04  Data: 0.012 (0.012)
Train: 336 [1150/1251 ( 92%)]  Loss: 3.869 (3.53)  Time: 0.696s, 1470.61/s  (0.693s, 1477.99/s)  LR: 4.122e-04  Data: 0.009 (0.012)
Train: 336 [1200/1251 ( 96%)]  Loss: 3.641 (3.54)  Time: 0.678s, 1509.50/s  (0.693s, 1477.36/s)  LR: 4.122e-04  Data: 0.009 (0.012)
Train: 336 [1250/1251 (100%)]  Loss: 3.417 (3.53)  Time: 0.659s, 1553.43/s  (0.693s, 1477.46/s)  LR: 4.122e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.638 (1.638)  Loss:  0.8101 (0.8101)  Acc@1: 89.2578 (89.2578)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.136 (0.581)  Loss:  0.9829 (1.3872)  Acc@1: 84.6698 (74.0140)  Acc@5: 95.6368 (91.8020)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-332.pth.tar', 74.44599995849609)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-324.pth.tar', 74.2579999584961)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-331.pth.tar', 74.16800009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-323.pth.tar', 74.10000006347656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-328.pth.tar', 74.04600009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-327.pth.tar', 74.03800000976563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-336.pth.tar', 74.01399993408204)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-333.pth.tar', 73.97599996337891)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-326.pth.tar', 73.96200003173828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-325.pth.tar', 73.93599990722656)

Train: 337 [   0/1251 (  0%)]  Loss: 3.142 (3.14)  Time: 2.363s,  433.37/s  (2.363s,  433.37/s)  LR: 4.097e-04  Data: 1.697 (1.697)
Train: 337 [  50/1251 (  4%)]  Loss: 3.530 (3.34)  Time: 0.668s, 1533.72/s  (0.724s, 1414.62/s)  LR: 4.097e-04  Data: 0.009 (0.046)
Train: 337 [ 100/1251 (  8%)]  Loss: 3.792 (3.49)  Time: 0.666s, 1538.61/s  (0.708s, 1445.91/s)  LR: 4.097e-04  Data: 0.009 (0.028)
Train: 337 [ 150/1251 ( 12%)]  Loss: 3.307 (3.44)  Time: 0.671s, 1525.41/s  (0.700s, 1462.61/s)  LR: 4.097e-04  Data: 0.010 (0.022)
Train: 337 [ 200/1251 ( 16%)]  Loss: 3.763 (3.51)  Time: 0.697s, 1469.96/s  (0.699s, 1465.75/s)  LR: 4.097e-04  Data: 0.010 (0.019)
Train: 337 [ 250/1251 ( 20%)]  Loss: 3.556 (3.52)  Time: 0.668s, 1533.03/s  (0.697s, 1468.30/s)  LR: 4.097e-04  Data: 0.009 (0.018)
Train: 337 [ 300/1251 ( 24%)]  Loss: 3.639 (3.53)  Time: 0.683s, 1498.87/s  (0.696s, 1470.38/s)  LR: 4.097e-04  Data: 0.013 (0.016)
Train: 337 [ 350/1251 ( 28%)]  Loss: 3.629 (3.54)  Time: 0.688s, 1489.34/s  (0.695s, 1473.29/s)  LR: 4.097e-04  Data: 0.010 (0.016)
Train: 337 [ 400/1251 ( 32%)]  Loss: 3.353 (3.52)  Time: 0.671s, 1524.98/s  (0.694s, 1474.55/s)  LR: 4.097e-04  Data: 0.012 (0.015)
Train: 337 [ 450/1251 ( 36%)]  Loss: 3.466 (3.52)  Time: 0.670s, 1528.47/s  (0.694s, 1475.39/s)  LR: 4.097e-04  Data: 0.010 (0.014)
Train: 337 [ 500/1251 ( 40%)]  Loss: 3.960 (3.56)  Time: 0.670s, 1527.28/s  (0.694s, 1476.47/s)  LR: 4.097e-04  Data: 0.011 (0.014)
Train: 337 [ 550/1251 ( 44%)]  Loss: 3.344 (3.54)  Time: 0.667s, 1535.82/s  (0.693s, 1477.85/s)  LR: 4.097e-04  Data: 0.009 (0.014)
Train: 337 [ 600/1251 ( 48%)]  Loss: 3.644 (3.55)  Time: 0.703s, 1456.48/s  (0.693s, 1478.51/s)  LR: 4.097e-04  Data: 0.009 (0.013)
Train: 337 [ 650/1251 ( 52%)]  Loss: 3.643 (3.56)  Time: 0.691s, 1481.16/s  (0.693s, 1477.10/s)  LR: 4.097e-04  Data: 0.015 (0.013)
Train: 337 [ 700/1251 ( 56%)]  Loss: 3.733 (3.57)  Time: 0.707s, 1449.11/s  (0.693s, 1477.07/s)  LR: 4.097e-04  Data: 0.010 (0.013)
Train: 337 [ 750/1251 ( 60%)]  Loss: 3.721 (3.58)  Time: 0.668s, 1533.40/s  (0.693s, 1477.48/s)  LR: 4.097e-04  Data: 0.012 (0.013)
Train: 337 [ 800/1251 ( 64%)]  Loss: 3.628 (3.58)  Time: 0.670s, 1529.36/s  (0.693s, 1477.47/s)  LR: 4.097e-04  Data: 0.010 (0.013)
Train: 337 [ 850/1251 ( 68%)]  Loss: 3.665 (3.58)  Time: 0.671s, 1526.70/s  (0.694s, 1476.44/s)  LR: 4.097e-04  Data: 0.009 (0.013)
Train: 337 [ 900/1251 ( 72%)]  Loss: 3.407 (3.57)  Time: 0.776s, 1320.07/s  (0.694s, 1475.77/s)  LR: 4.097e-04  Data: 0.011 (0.012)
Train: 337 [ 950/1251 ( 76%)]  Loss: 3.788 (3.59)  Time: 0.705s, 1451.73/s  (0.694s, 1476.29/s)  LR: 4.097e-04  Data: 0.011 (0.012)
Train: 337 [1000/1251 ( 80%)]  Loss: 3.581 (3.59)  Time: 0.671s, 1525.22/s  (0.694s, 1476.03/s)  LR: 4.097e-04  Data: 0.010 (0.012)
Train: 337 [1050/1251 ( 84%)]  Loss: 3.692 (3.59)  Time: 0.691s, 1482.07/s  (0.693s, 1476.67/s)  LR: 4.097e-04  Data: 0.010 (0.012)
Train: 337 [1100/1251 ( 88%)]  Loss: 3.540 (3.59)  Time: 0.705s, 1451.50/s  (0.693s, 1476.92/s)  LR: 4.097e-04  Data: 0.009 (0.012)
Train: 337 [1150/1251 ( 92%)]  Loss: 3.544 (3.59)  Time: 0.679s, 1507.17/s  (0.693s, 1476.92/s)  LR: 4.097e-04  Data: 0.013 (0.012)
Train: 337 [1200/1251 ( 96%)]  Loss: 3.708 (3.59)  Time: 0.673s, 1520.78/s  (0.693s, 1477.07/s)  LR: 4.097e-04  Data: 0.012 (0.012)
Train: 337 [1250/1251 (100%)]  Loss: 3.620 (3.59)  Time: 0.694s, 1475.44/s  (0.693s, 1478.05/s)  LR: 4.097e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.656 (1.656)  Loss:  0.9004 (0.9004)  Acc@1: 88.7695 (88.7695)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.136 (0.582)  Loss:  0.9941 (1.4374)  Acc@1: 85.2594 (74.3100)  Acc@5: 96.3443 (92.1320)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-332.pth.tar', 74.44599995849609)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-337.pth.tar', 74.31000006103515)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-324.pth.tar', 74.2579999584961)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-331.pth.tar', 74.16800009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-323.pth.tar', 74.10000006347656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-328.pth.tar', 74.04600009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-327.pth.tar', 74.03800000976563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-336.pth.tar', 74.01399993408204)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-333.pth.tar', 73.97599996337891)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-326.pth.tar', 73.96200003173828)

Train: 338 [   0/1251 (  0%)]  Loss: 3.738 (3.74)  Time: 2.238s,  457.56/s  (2.238s,  457.56/s)  LR: 4.072e-04  Data: 1.622 (1.622)
Train: 338 [  50/1251 (  4%)]  Loss: 3.187 (3.46)  Time: 0.673s, 1521.60/s  (0.732s, 1398.61/s)  LR: 4.072e-04  Data: 0.010 (0.048)
Train: 338 [ 100/1251 (  8%)]  Loss: 3.755 (3.56)  Time: 0.705s, 1452.34/s  (0.714s, 1435.01/s)  LR: 4.072e-04  Data: 0.009 (0.029)
Train: 338 [ 150/1251 ( 12%)]  Loss: 3.458 (3.53)  Time: 0.713s, 1436.62/s  (0.708s, 1446.61/s)  LR: 4.072e-04  Data: 0.011 (0.023)
Train: 338 [ 200/1251 ( 16%)]  Loss: 3.461 (3.52)  Time: 0.675s, 1517.61/s  (0.704s, 1454.47/s)  LR: 4.072e-04  Data: 0.011 (0.020)
Train: 338 [ 250/1251 ( 20%)]  Loss: 3.582 (3.53)  Time: 0.711s, 1439.43/s  (0.702s, 1457.95/s)  LR: 4.072e-04  Data: 0.010 (0.018)
Train: 338 [ 300/1251 ( 24%)]  Loss: 3.793 (3.57)  Time: 0.706s, 1451.03/s  (0.700s, 1462.99/s)  LR: 4.072e-04  Data: 0.009 (0.017)
Train: 338 [ 350/1251 ( 28%)]  Loss: 3.681 (3.58)  Time: 0.674s, 1519.00/s  (0.698s, 1466.86/s)  LR: 4.072e-04  Data: 0.010 (0.016)
Train: 338 [ 400/1251 ( 32%)]  Loss: 3.376 (3.56)  Time: 0.683s, 1499.33/s  (0.698s, 1466.97/s)  LR: 4.072e-04  Data: 0.010 (0.015)
Train: 338 [ 450/1251 ( 36%)]  Loss: 3.179 (3.52)  Time: 0.702s, 1458.96/s  (0.697s, 1469.24/s)  LR: 4.072e-04  Data: 0.010 (0.015)
Train: 338 [ 500/1251 ( 40%)]  Loss: 3.542 (3.52)  Time: 0.690s, 1483.50/s  (0.696s, 1471.35/s)  LR: 4.072e-04  Data: 0.010 (0.014)
Train: 338 [ 550/1251 ( 44%)]  Loss: 3.021 (3.48)  Time: 0.680s, 1506.96/s  (0.695s, 1472.36/s)  LR: 4.072e-04  Data: 0.011 (0.014)
Train: 338 [ 600/1251 ( 48%)]  Loss: 3.854 (3.51)  Time: 0.671s, 1526.63/s  (0.695s, 1474.04/s)  LR: 4.072e-04  Data: 0.009 (0.014)
Train: 338 [ 650/1251 ( 52%)]  Loss: 3.436 (3.50)  Time: 0.666s, 1536.87/s  (0.694s, 1474.77/s)  LR: 4.072e-04  Data: 0.011 (0.013)
Train: 338 [ 700/1251 ( 56%)]  Loss: 3.382 (3.50)  Time: 0.707s, 1448.64/s  (0.694s, 1475.74/s)  LR: 4.072e-04  Data: 0.011 (0.013)
Train: 338 [ 750/1251 ( 60%)]  Loss: 3.652 (3.51)  Time: 0.679s, 1508.30/s  (0.694s, 1475.89/s)  LR: 4.072e-04  Data: 0.011 (0.013)
Train: 338 [ 800/1251 ( 64%)]  Loss: 3.539 (3.51)  Time: 0.719s, 1424.67/s  (0.694s, 1475.17/s)  LR: 4.072e-04  Data: 0.012 (0.013)
Train: 338 [ 850/1251 ( 68%)]  Loss: 3.184 (3.49)  Time: 0.670s, 1527.39/s  (0.694s, 1475.38/s)  LR: 4.072e-04  Data: 0.010 (0.013)
Train: 338 [ 900/1251 ( 72%)]  Loss: 3.619 (3.50)  Time: 0.672s, 1522.74/s  (0.694s, 1476.13/s)  LR: 4.072e-04  Data: 0.011 (0.012)
Train: 338 [ 950/1251 ( 76%)]  Loss: 3.431 (3.49)  Time: 0.671s, 1525.96/s  (0.693s, 1476.60/s)  LR: 4.072e-04  Data: 0.012 (0.012)
Train: 338 [1000/1251 ( 80%)]  Loss: 4.058 (3.52)  Time: 0.674s, 1520.33/s  (0.693s, 1477.26/s)  LR: 4.072e-04  Data: 0.011 (0.012)
Train: 338 [1050/1251 ( 84%)]  Loss: 3.447 (3.52)  Time: 0.722s, 1418.39/s  (0.693s, 1477.84/s)  LR: 4.072e-04  Data: 0.009 (0.012)
Train: 338 [1100/1251 ( 88%)]  Loss: 4.085 (3.54)  Time: 0.677s, 1512.03/s  (0.693s, 1478.36/s)  LR: 4.072e-04  Data: 0.010 (0.012)
Train: 338 [1150/1251 ( 92%)]  Loss: 3.657 (3.55)  Time: 0.672s, 1524.66/s  (0.693s, 1478.28/s)  LR: 4.072e-04  Data: 0.011 (0.012)
Train: 338 [1200/1251 ( 96%)]  Loss: 3.436 (3.54)  Time: 0.678s, 1509.85/s  (0.692s, 1478.77/s)  LR: 4.072e-04  Data: 0.010 (0.012)
Train: 338 [1250/1251 (100%)]  Loss: 3.495 (3.54)  Time: 0.709s, 1443.62/s  (0.692s, 1479.32/s)  LR: 4.072e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.483 (1.483)  Loss:  0.8442 (0.8442)  Acc@1: 88.7695 (88.7695)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.136 (0.581)  Loss:  0.9204 (1.4180)  Acc@1: 84.0802 (74.4160)  Acc@5: 96.8160 (92.2580)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-332.pth.tar', 74.44599995849609)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-338.pth.tar', 74.41599993652343)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-337.pth.tar', 74.31000006103515)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-324.pth.tar', 74.2579999584961)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-331.pth.tar', 74.16800009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-323.pth.tar', 74.10000006347656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-328.pth.tar', 74.04600009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-327.pth.tar', 74.03800000976563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-336.pth.tar', 74.01399993408204)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-333.pth.tar', 73.97599996337891)

Train: 339 [   0/1251 (  0%)]  Loss: 3.810 (3.81)  Time: 2.257s,  453.74/s  (2.257s,  453.74/s)  LR: 4.046e-04  Data: 1.640 (1.640)
Train: 339 [  50/1251 (  4%)]  Loss: 3.557 (3.68)  Time: 0.720s, 1421.45/s  (0.727s, 1408.90/s)  LR: 4.046e-04  Data: 0.010 (0.047)
Train: 339 [ 100/1251 (  8%)]  Loss: 3.665 (3.68)  Time: 0.672s, 1524.38/s  (0.709s, 1444.21/s)  LR: 4.046e-04  Data: 0.010 (0.029)
Train: 339 [ 150/1251 ( 12%)]  Loss: 3.894 (3.73)  Time: 0.717s, 1428.51/s  (0.705s, 1452.80/s)  LR: 4.046e-04  Data: 0.009 (0.023)
Train: 339 [ 200/1251 ( 16%)]  Loss: 3.731 (3.73)  Time: 0.671s, 1525.61/s  (0.703s, 1457.43/s)  LR: 4.046e-04  Data: 0.010 (0.020)
Train: 339 [ 250/1251 ( 20%)]  Loss: 3.643 (3.72)  Time: 0.721s, 1419.70/s  (0.700s, 1463.44/s)  LR: 4.046e-04  Data: 0.009 (0.018)
Train: 339 [ 300/1251 ( 24%)]  Loss: 3.386 (3.67)  Time: 0.674s, 1519.06/s  (0.698s, 1466.12/s)  LR: 4.046e-04  Data: 0.010 (0.017)
Train: 339 [ 350/1251 ( 28%)]  Loss: 3.750 (3.68)  Time: 0.669s, 1530.34/s  (0.697s, 1468.98/s)  LR: 4.046e-04  Data: 0.010 (0.016)
Train: 339 [ 400/1251 ( 32%)]  Loss: 3.276 (3.63)  Time: 0.693s, 1477.75/s  (0.696s, 1472.09/s)  LR: 4.046e-04  Data: 0.009 (0.015)
Train: 339 [ 450/1251 ( 36%)]  Loss: 3.712 (3.64)  Time: 0.762s, 1343.50/s  (0.696s, 1472.14/s)  LR: 4.046e-04  Data: 0.010 (0.015)
Train: 339 [ 500/1251 ( 40%)]  Loss: 3.566 (3.64)  Time: 0.707s, 1447.46/s  (0.695s, 1473.83/s)  LR: 4.046e-04  Data: 0.012 (0.014)
Train: 339 [ 550/1251 ( 44%)]  Loss: 3.813 (3.65)  Time: 0.718s, 1426.52/s  (0.695s, 1472.68/s)  LR: 4.046e-04  Data: 0.009 (0.014)
Train: 339 [ 600/1251 ( 48%)]  Loss: 3.622 (3.65)  Time: 0.672s, 1524.03/s  (0.695s, 1473.76/s)  LR: 4.046e-04  Data: 0.011 (0.014)
Train: 339 [ 650/1251 ( 52%)]  Loss: 4.042 (3.68)  Time: 0.710s, 1443.08/s  (0.695s, 1473.26/s)  LR: 4.046e-04  Data: 0.010 (0.013)
Train: 339 [ 700/1251 ( 56%)]  Loss: 3.511 (3.67)  Time: 0.739s, 1384.94/s  (0.695s, 1472.92/s)  LR: 4.046e-04  Data: 0.011 (0.013)
Train: 339 [ 750/1251 ( 60%)]  Loss: 3.860 (3.68)  Time: 0.670s, 1528.18/s  (0.695s, 1473.19/s)  LR: 4.046e-04  Data: 0.010 (0.013)
Train: 339 [ 800/1251 ( 64%)]  Loss: 3.820 (3.69)  Time: 0.728s, 1405.86/s  (0.695s, 1473.74/s)  LR: 4.046e-04  Data: 0.010 (0.013)
Train: 339 [ 850/1251 ( 68%)]  Loss: 3.802 (3.69)  Time: 0.680s, 1504.94/s  (0.695s, 1474.21/s)  LR: 4.046e-04  Data: 0.010 (0.013)
Train: 339 [ 900/1251 ( 72%)]  Loss: 3.365 (3.68)  Time: 0.674s, 1519.49/s  (0.694s, 1475.42/s)  LR: 4.046e-04  Data: 0.009 (0.013)
Train: 339 [ 950/1251 ( 76%)]  Loss: 3.673 (3.67)  Time: 0.699s, 1464.84/s  (0.695s, 1474.10/s)  LR: 4.046e-04  Data: 0.009 (0.013)
Train: 339 [1000/1251 ( 80%)]  Loss: 3.953 (3.69)  Time: 0.672s, 1522.97/s  (0.694s, 1475.02/s)  LR: 4.046e-04  Data: 0.011 (0.012)
Train: 339 [1050/1251 ( 84%)]  Loss: 3.928 (3.70)  Time: 0.719s, 1424.26/s  (0.694s, 1474.84/s)  LR: 4.046e-04  Data: 0.011 (0.012)
Train: 339 [1100/1251 ( 88%)]  Loss: 3.523 (3.69)  Time: 0.682s, 1500.72/s  (0.694s, 1475.35/s)  LR: 4.046e-04  Data: 0.010 (0.012)
Train: 339 [1150/1251 ( 92%)]  Loss: 3.477 (3.68)  Time: 0.700s, 1463.14/s  (0.694s, 1475.52/s)  LR: 4.046e-04  Data: 0.009 (0.012)
Train: 339 [1200/1251 ( 96%)]  Loss: 3.544 (3.68)  Time: 0.674s, 1518.71/s  (0.694s, 1475.83/s)  LR: 4.046e-04  Data: 0.010 (0.012)
Train: 339 [1250/1251 (100%)]  Loss: 3.710 (3.68)  Time: 0.746s, 1372.91/s  (0.694s, 1475.66/s)  LR: 4.046e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.490 (1.490)  Loss:  0.8135 (0.8135)  Acc@1: 89.3555 (89.3555)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.136 (0.593)  Loss:  0.8716 (1.4141)  Acc@1: 85.1415 (74.2500)  Acc@5: 96.2264 (92.0760)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-332.pth.tar', 74.44599995849609)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-338.pth.tar', 74.41599993652343)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-337.pth.tar', 74.31000006103515)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-324.pth.tar', 74.2579999584961)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-339.pth.tar', 74.25000000976563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-331.pth.tar', 74.16800009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-323.pth.tar', 74.10000006347656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-328.pth.tar', 74.04600009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-327.pth.tar', 74.03800000976563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-336.pth.tar', 74.01399993408204)

Train: 340 [   0/1251 (  0%)]  Loss: 3.493 (3.49)  Time: 2.196s,  466.32/s  (2.196s,  466.32/s)  LR: 4.021e-04  Data: 1.580 (1.580)
Train: 340 [  50/1251 (  4%)]  Loss: 3.624 (3.56)  Time: 0.737s, 1388.83/s  (0.737s, 1388.81/s)  LR: 4.021e-04  Data: 0.009 (0.047)
Train: 340 [ 100/1251 (  8%)]  Loss: 3.811 (3.64)  Time: 0.676s, 1514.48/s  (0.713s, 1436.22/s)  LR: 4.021e-04  Data: 0.017 (0.029)
Train: 340 [ 150/1251 ( 12%)]  Loss: 3.295 (3.56)  Time: 0.672s, 1523.98/s  (0.705s, 1451.68/s)  LR: 4.021e-04  Data: 0.011 (0.023)
Train: 340 [ 200/1251 ( 16%)]  Loss: 3.550 (3.55)  Time: 0.704s, 1455.21/s  (0.702s, 1458.68/s)  LR: 4.021e-04  Data: 0.010 (0.020)
Train: 340 [ 250/1251 ( 20%)]  Loss: 3.866 (3.61)  Time: 0.690s, 1484.35/s  (0.699s, 1464.03/s)  LR: 4.021e-04  Data: 0.010 (0.018)
Train: 340 [ 300/1251 ( 24%)]  Loss: 3.403 (3.58)  Time: 0.764s, 1339.69/s  (0.697s, 1469.56/s)  LR: 4.021e-04  Data: 0.009 (0.017)
Train: 340 [ 350/1251 ( 28%)]  Loss: 3.343 (3.55)  Time: 0.677s, 1512.77/s  (0.695s, 1472.89/s)  LR: 4.021e-04  Data: 0.011 (0.016)
Train: 340 [ 400/1251 ( 32%)]  Loss: 3.619 (3.56)  Time: 0.672s, 1523.84/s  (0.694s, 1475.09/s)  LR: 4.021e-04  Data: 0.010 (0.015)
Train: 340 [ 450/1251 ( 36%)]  Loss: 3.283 (3.53)  Time: 0.741s, 1381.19/s  (0.694s, 1474.58/s)  LR: 4.021e-04  Data: 0.009 (0.014)
Train: 340 [ 500/1251 ( 40%)]  Loss: 3.470 (3.52)  Time: 0.721s, 1420.47/s  (0.695s, 1474.21/s)  LR: 4.021e-04  Data: 0.011 (0.014)
Train: 340 [ 550/1251 ( 44%)]  Loss: 3.477 (3.52)  Time: 0.671s, 1525.19/s  (0.694s, 1474.76/s)  LR: 4.021e-04  Data: 0.011 (0.014)
Train: 340 [ 600/1251 ( 48%)]  Loss: 3.354 (3.51)  Time: 0.672s, 1522.98/s  (0.694s, 1475.82/s)  LR: 4.021e-04  Data: 0.010 (0.013)
Train: 340 [ 650/1251 ( 52%)]  Loss: 3.101 (3.48)  Time: 0.725s, 1412.73/s  (0.694s, 1474.98/s)  LR: 4.021e-04  Data: 0.010 (0.013)
Train: 340 [ 700/1251 ( 56%)]  Loss: 3.626 (3.49)  Time: 0.674s, 1518.26/s  (0.694s, 1474.74/s)  LR: 4.021e-04  Data: 0.011 (0.013)
Train: 340 [ 750/1251 ( 60%)]  Loss: 3.795 (3.51)  Time: 0.672s, 1523.89/s  (0.694s, 1474.88/s)  LR: 4.021e-04  Data: 0.011 (0.013)
Train: 340 [ 800/1251 ( 64%)]  Loss: 3.534 (3.51)  Time: 0.665s, 1538.72/s  (0.694s, 1475.14/s)  LR: 4.021e-04  Data: 0.010 (0.013)
Train: 340 [ 850/1251 ( 68%)]  Loss: 3.625 (3.51)  Time: 0.673s, 1521.76/s  (0.694s, 1475.64/s)  LR: 4.021e-04  Data: 0.010 (0.013)
Train: 340 [ 900/1251 ( 72%)]  Loss: 3.901 (3.54)  Time: 0.725s, 1411.46/s  (0.694s, 1476.13/s)  LR: 4.021e-04  Data: 0.011 (0.012)
Train: 340 [ 950/1251 ( 76%)]  Loss: 3.826 (3.55)  Time: 0.673s, 1520.43/s  (0.694s, 1476.30/s)  LR: 4.021e-04  Data: 0.010 (0.012)
Train: 340 [1000/1251 ( 80%)]  Loss: 3.500 (3.55)  Time: 0.688s, 1488.21/s  (0.694s, 1475.31/s)  LR: 4.021e-04  Data: 0.009 (0.012)
Train: 340 [1050/1251 ( 84%)]  Loss: 3.727 (3.56)  Time: 0.719s, 1424.95/s  (0.694s, 1476.10/s)  LR: 4.021e-04  Data: 0.011 (0.012)
Train: 340 [1100/1251 ( 88%)]  Loss: 3.521 (3.55)  Time: 0.671s, 1526.22/s  (0.694s, 1476.35/s)  LR: 4.021e-04  Data: 0.011 (0.012)
Train: 340 [1150/1251 ( 92%)]  Loss: 3.381 (3.55)  Time: 0.707s, 1447.51/s  (0.694s, 1475.37/s)  LR: 4.021e-04  Data: 0.012 (0.012)
Train: 340 [1200/1251 ( 96%)]  Loss: 3.623 (3.55)  Time: 0.707s, 1447.78/s  (0.694s, 1475.37/s)  LR: 4.021e-04  Data: 0.011 (0.012)
Train: 340 [1250/1251 (100%)]  Loss: 3.393 (3.54)  Time: 0.656s, 1560.10/s  (0.694s, 1475.83/s)  LR: 4.021e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.623 (1.623)  Loss:  0.7295 (0.7295)  Acc@1: 89.9414 (89.9414)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.136 (0.585)  Loss:  0.8965 (1.3009)  Acc@1: 82.9009 (74.0900)  Acc@5: 95.2830 (92.0940)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-332.pth.tar', 74.44599995849609)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-338.pth.tar', 74.41599993652343)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-337.pth.tar', 74.31000006103515)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-324.pth.tar', 74.2579999584961)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-339.pth.tar', 74.25000000976563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-331.pth.tar', 74.16800009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-323.pth.tar', 74.10000006347656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-340.pth.tar', 74.09000007080078)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-328.pth.tar', 74.04600009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-327.pth.tar', 74.03800000976563)

Train: 341 [   0/1251 (  0%)]  Loss: 3.563 (3.56)  Time: 2.103s,  486.90/s  (2.103s,  486.90/s)  LR: 3.995e-04  Data: 1.488 (1.488)
Train: 341 [  50/1251 (  4%)]  Loss: 3.421 (3.49)  Time: 0.672s, 1524.07/s  (0.723s, 1415.84/s)  LR: 3.995e-04  Data: 0.010 (0.050)
Train: 341 [ 100/1251 (  8%)]  Loss: 3.555 (3.51)  Time: 0.701s, 1459.97/s  (0.709s, 1444.84/s)  LR: 3.995e-04  Data: 0.009 (0.030)
Train: 341 [ 150/1251 ( 12%)]  Loss: 3.357 (3.47)  Time: 0.671s, 1526.97/s  (0.699s, 1465.48/s)  LR: 3.995e-04  Data: 0.010 (0.024)
Train: 341 [ 200/1251 ( 16%)]  Loss: 3.316 (3.44)  Time: 0.671s, 1527.10/s  (0.696s, 1470.21/s)  LR: 3.995e-04  Data: 0.010 (0.020)
Train: 341 [ 250/1251 ( 20%)]  Loss: 3.522 (3.46)  Time: 0.683s, 1498.28/s  (0.696s, 1470.54/s)  LR: 3.995e-04  Data: 0.011 (0.018)
Train: 341 [ 300/1251 ( 24%)]  Loss: 3.632 (3.48)  Time: 0.682s, 1502.54/s  (0.696s, 1471.02/s)  LR: 3.995e-04  Data: 0.010 (0.017)
Train: 341 [ 350/1251 ( 28%)]  Loss: 3.593 (3.49)  Time: 0.674s, 1518.64/s  (0.695s, 1473.74/s)  LR: 3.995e-04  Data: 0.010 (0.016)
Train: 341 [ 400/1251 ( 32%)]  Loss: 3.219 (3.46)  Time: 0.703s, 1457.23/s  (0.695s, 1473.79/s)  LR: 3.995e-04  Data: 0.008 (0.015)
Train: 341 [ 450/1251 ( 36%)]  Loss: 3.616 (3.48)  Time: 0.671s, 1527.10/s  (0.694s, 1475.71/s)  LR: 3.995e-04  Data: 0.010 (0.015)
Train: 341 [ 500/1251 ( 40%)]  Loss: 3.554 (3.49)  Time: 0.671s, 1525.49/s  (0.693s, 1476.99/s)  LR: 3.995e-04  Data: 0.009 (0.014)
Train: 341 [ 550/1251 ( 44%)]  Loss: 3.344 (3.47)  Time: 0.700s, 1462.58/s  (0.692s, 1479.36/s)  LR: 3.995e-04  Data: 0.010 (0.014)
Train: 341 [ 600/1251 ( 48%)]  Loss: 3.232 (3.46)  Time: 0.684s, 1497.76/s  (0.692s, 1479.92/s)  LR: 3.995e-04  Data: 0.012 (0.014)
Train: 341 [ 650/1251 ( 52%)]  Loss: 3.877 (3.49)  Time: 0.673s, 1522.34/s  (0.692s, 1480.37/s)  LR: 3.995e-04  Data: 0.010 (0.013)
Train: 341 [ 700/1251 ( 56%)]  Loss: 3.974 (3.52)  Time: 0.678s, 1510.56/s  (0.691s, 1481.39/s)  LR: 3.995e-04  Data: 0.010 (0.013)
Train: 341 [ 750/1251 ( 60%)]  Loss: 3.246 (3.50)  Time: 0.730s, 1402.47/s  (0.691s, 1481.94/s)  LR: 3.995e-04  Data: 0.010 (0.013)
Train: 341 [ 800/1251 ( 64%)]  Loss: 3.546 (3.50)  Time: 0.676s, 1515.77/s  (0.691s, 1481.25/s)  LR: 3.995e-04  Data: 0.014 (0.013)
Train: 341 [ 850/1251 ( 68%)]  Loss: 3.726 (3.52)  Time: 0.704s, 1454.92/s  (0.692s, 1480.53/s)  LR: 3.995e-04  Data: 0.009 (0.013)
Train: 341 [ 900/1251 ( 72%)]  Loss: 3.747 (3.53)  Time: 0.671s, 1524.98/s  (0.691s, 1481.05/s)  LR: 3.995e-04  Data: 0.010 (0.012)
Train: 341 [ 950/1251 ( 76%)]  Loss: 3.464 (3.53)  Time: 0.673s, 1522.66/s  (0.691s, 1481.40/s)  LR: 3.995e-04  Data: 0.009 (0.012)
Train: 341 [1000/1251 ( 80%)]  Loss: 3.386 (3.52)  Time: 0.702s, 1459.72/s  (0.691s, 1481.39/s)  LR: 3.995e-04  Data: 0.009 (0.012)
Train: 341 [1050/1251 ( 84%)]  Loss: 3.690 (3.53)  Time: 0.700s, 1462.25/s  (0.692s, 1480.33/s)  LR: 3.995e-04  Data: 0.009 (0.012)
Train: 341 [1100/1251 ( 88%)]  Loss: 3.863 (3.54)  Time: 0.714s, 1434.71/s  (0.692s, 1480.49/s)  LR: 3.995e-04  Data: 0.009 (0.012)
Train: 341 [1150/1251 ( 92%)]  Loss: 3.458 (3.54)  Time: 0.674s, 1520.22/s  (0.692s, 1480.47/s)  LR: 3.995e-04  Data: 0.011 (0.012)
Train: 341 [1200/1251 ( 96%)]  Loss: 3.494 (3.54)  Time: 0.672s, 1524.08/s  (0.692s, 1480.78/s)  LR: 3.995e-04  Data: 0.013 (0.012)
Train: 341 [1250/1251 (100%)]  Loss: 3.884 (3.55)  Time: 0.658s, 1556.88/s  (0.691s, 1481.32/s)  LR: 3.995e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.579 (1.579)  Loss:  0.6992 (0.6992)  Acc@1: 89.0625 (89.0625)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.137 (0.589)  Loss:  0.8784 (1.2871)  Acc@1: 83.4906 (74.2460)  Acc@5: 95.4009 (92.0280)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-332.pth.tar', 74.44599995849609)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-338.pth.tar', 74.41599993652343)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-337.pth.tar', 74.31000006103515)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-324.pth.tar', 74.2579999584961)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-339.pth.tar', 74.25000000976563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-341.pth.tar', 74.24599993896484)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-331.pth.tar', 74.16800009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-323.pth.tar', 74.10000006347656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-340.pth.tar', 74.09000007080078)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-328.pth.tar', 74.04600009277344)

Train: 342 [   0/1251 (  0%)]  Loss: 3.405 (3.41)  Time: 2.234s,  458.27/s  (2.234s,  458.27/s)  LR: 3.970e-04  Data: 1.617 (1.617)
Train: 342 [  50/1251 (  4%)]  Loss: 3.886 (3.65)  Time: 0.676s, 1514.52/s  (0.728s, 1406.33/s)  LR: 3.970e-04  Data: 0.011 (0.050)
Train: 342 [ 100/1251 (  8%)]  Loss: 3.777 (3.69)  Time: 0.670s, 1527.84/s  (0.709s, 1444.98/s)  LR: 3.970e-04  Data: 0.009 (0.031)
Train: 342 [ 150/1251 ( 12%)]  Loss: 3.438 (3.63)  Time: 0.701s, 1460.19/s  (0.703s, 1456.92/s)  LR: 3.970e-04  Data: 0.010 (0.024)
Train: 342 [ 200/1251 ( 16%)]  Loss: 3.869 (3.68)  Time: 0.672s, 1523.24/s  (0.700s, 1462.50/s)  LR: 3.970e-04  Data: 0.011 (0.020)
Train: 342 [ 250/1251 ( 20%)]  Loss: 3.816 (3.70)  Time: 0.717s, 1428.15/s  (0.700s, 1463.71/s)  LR: 3.970e-04  Data: 0.011 (0.019)
Train: 342 [ 300/1251 ( 24%)]  Loss: 3.751 (3.71)  Time: 0.719s, 1424.04/s  (0.699s, 1465.92/s)  LR: 3.970e-04  Data: 0.010 (0.017)
Train: 342 [ 350/1251 ( 28%)]  Loss: 3.516 (3.68)  Time: 0.745s, 1374.24/s  (0.698s, 1467.63/s)  LR: 3.970e-04  Data: 0.009 (0.016)
Train: 342 [ 400/1251 ( 32%)]  Loss: 3.647 (3.68)  Time: 0.702s, 1458.62/s  (0.697s, 1468.99/s)  LR: 3.970e-04  Data: 0.009 (0.016)
Train: 342 [ 450/1251 ( 36%)]  Loss: 3.838 (3.69)  Time: 0.673s, 1522.68/s  (0.696s, 1471.64/s)  LR: 3.970e-04  Data: 0.012 (0.015)
Train: 342 [ 500/1251 ( 40%)]  Loss: 3.680 (3.69)  Time: 0.681s, 1503.18/s  (0.695s, 1473.36/s)  LR: 3.970e-04  Data: 0.009 (0.015)
Train: 342 [ 550/1251 ( 44%)]  Loss: 3.865 (3.71)  Time: 0.676s, 1514.00/s  (0.694s, 1475.11/s)  LR: 3.970e-04  Data: 0.012 (0.014)
Train: 342 [ 600/1251 ( 48%)]  Loss: 3.762 (3.71)  Time: 0.667s, 1535.28/s  (0.694s, 1474.82/s)  LR: 3.970e-04  Data: 0.010 (0.014)
Train: 342 [ 650/1251 ( 52%)]  Loss: 4.101 (3.74)  Time: 0.673s, 1522.38/s  (0.694s, 1475.51/s)  LR: 3.970e-04  Data: 0.010 (0.014)
Train: 342 [ 700/1251 ( 56%)]  Loss: 3.793 (3.74)  Time: 0.679s, 1508.07/s  (0.693s, 1476.76/s)  LR: 3.970e-04  Data: 0.011 (0.013)
Train: 342 [ 750/1251 ( 60%)]  Loss: 3.798 (3.75)  Time: 0.673s, 1521.05/s  (0.693s, 1476.75/s)  LR: 3.970e-04  Data: 0.011 (0.013)
Train: 342 [ 800/1251 ( 64%)]  Loss: 3.149 (3.71)  Time: 0.691s, 1482.62/s  (0.693s, 1476.59/s)  LR: 3.970e-04  Data: 0.014 (0.013)
Train: 342 [ 850/1251 ( 68%)]  Loss: 3.640 (3.71)  Time: 0.677s, 1513.60/s  (0.694s, 1476.10/s)  LR: 3.970e-04  Data: 0.012 (0.013)
Train: 342 [ 900/1251 ( 72%)]  Loss: 3.553 (3.70)  Time: 0.697s, 1469.81/s  (0.695s, 1473.54/s)  LR: 3.970e-04  Data: 0.011 (0.013)
Train: 342 [ 950/1251 ( 76%)]  Loss: 3.566 (3.69)  Time: 0.709s, 1444.28/s  (0.696s, 1471.68/s)  LR: 3.970e-04  Data: 0.011 (0.013)
Train: 342 [1000/1251 ( 80%)]  Loss: 3.184 (3.67)  Time: 0.690s, 1483.53/s  (0.697s, 1469.46/s)  LR: 3.970e-04  Data: 0.010 (0.013)
Train: 342 [1050/1251 ( 84%)]  Loss: 3.566 (3.66)  Time: 0.672s, 1522.95/s  (0.696s, 1471.52/s)  LR: 3.970e-04  Data: 0.011 (0.013)
Train: 342 [1100/1251 ( 88%)]  Loss: 3.647 (3.66)  Time: 0.672s, 1522.68/s  (0.695s, 1473.87/s)  LR: 3.970e-04  Data: 0.011 (0.013)
Train: 342 [1150/1251 ( 92%)]  Loss: 3.928 (3.67)  Time: 0.672s, 1523.29/s  (0.694s, 1475.70/s)  LR: 3.970e-04  Data: 0.010 (0.013)
Train: 342 [1200/1251 ( 96%)]  Loss: 3.239 (3.66)  Time: 0.706s, 1450.87/s  (0.694s, 1475.58/s)  LR: 3.970e-04  Data: 0.010 (0.013)
Train: 342 [1250/1251 (100%)]  Loss: 3.213 (3.64)  Time: 0.777s, 1317.88/s  (0.694s, 1475.39/s)  LR: 3.970e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.521 (1.521)  Loss:  0.8486 (0.8486)  Acc@1: 89.0625 (89.0625)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.137 (0.588)  Loss:  0.9590 (1.4034)  Acc@1: 84.3160 (74.2620)  Acc@5: 96.1085 (91.9220)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-332.pth.tar', 74.44599995849609)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-338.pth.tar', 74.41599993652343)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-337.pth.tar', 74.31000006103515)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-342.pth.tar', 74.2620000390625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-324.pth.tar', 74.2579999584961)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-339.pth.tar', 74.25000000976563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-341.pth.tar', 74.24599993896484)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-331.pth.tar', 74.16800009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-323.pth.tar', 74.10000006347656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-340.pth.tar', 74.09000007080078)

Train: 343 [   0/1251 (  0%)]  Loss: 3.634 (3.63)  Time: 2.194s,  466.65/s  (2.194s,  466.65/s)  LR: 3.945e-04  Data: 1.580 (1.580)
Train: 343 [  50/1251 (  4%)]  Loss: 3.652 (3.64)  Time: 0.671s, 1525.62/s  (0.734s, 1394.44/s)  LR: 3.945e-04  Data: 0.011 (0.050)
Train: 343 [ 100/1251 (  8%)]  Loss: 3.358 (3.55)  Time: 0.702s, 1457.83/s  (0.717s, 1428.55/s)  LR: 3.945e-04  Data: 0.009 (0.030)
Train: 343 [ 150/1251 ( 12%)]  Loss: 3.411 (3.51)  Time: 0.700s, 1461.98/s  (0.709s, 1444.67/s)  LR: 3.945e-04  Data: 0.009 (0.024)
Train: 343 [ 200/1251 ( 16%)]  Loss: 3.475 (3.51)  Time: 0.670s, 1527.83/s  (0.704s, 1455.32/s)  LR: 3.945e-04  Data: 0.011 (0.020)
Train: 343 [ 250/1251 ( 20%)]  Loss: 3.605 (3.52)  Time: 0.697s, 1469.65/s  (0.702s, 1459.35/s)  LR: 3.945e-04  Data: 0.009 (0.018)
Train: 343 [ 300/1251 ( 24%)]  Loss: 3.542 (3.53)  Time: 0.673s, 1522.00/s  (0.700s, 1463.20/s)  LR: 3.945e-04  Data: 0.009 (0.017)
Train: 343 [ 350/1251 ( 28%)]  Loss: 3.962 (3.58)  Time: 0.708s, 1446.13/s  (0.698s, 1466.64/s)  LR: 3.945e-04  Data: 0.010 (0.016)
Train: 343 [ 400/1251 ( 32%)]  Loss: 3.352 (3.55)  Time: 0.671s, 1525.35/s  (0.698s, 1467.68/s)  LR: 3.945e-04  Data: 0.009 (0.015)
Train: 343 [ 450/1251 ( 36%)]  Loss: 3.441 (3.54)  Time: 0.701s, 1460.63/s  (0.697s, 1468.15/s)  LR: 3.945e-04  Data: 0.010 (0.015)
Train: 343 [ 500/1251 ( 40%)]  Loss: 3.820 (3.57)  Time: 0.668s, 1532.01/s  (0.697s, 1468.83/s)  LR: 3.945e-04  Data: 0.011 (0.014)
Train: 343 [ 550/1251 ( 44%)]  Loss: 3.360 (3.55)  Time: 0.701s, 1460.81/s  (0.697s, 1469.50/s)  LR: 3.945e-04  Data: 0.009 (0.014)
Train: 343 [ 600/1251 ( 48%)]  Loss: 3.794 (3.57)  Time: 0.694s, 1475.49/s  (0.696s, 1471.62/s)  LR: 3.945e-04  Data: 0.010 (0.014)
Train: 343 [ 650/1251 ( 52%)]  Loss: 3.802 (3.59)  Time: 0.672s, 1524.68/s  (0.695s, 1473.86/s)  LR: 3.945e-04  Data: 0.010 (0.013)
Train: 343 [ 700/1251 ( 56%)]  Loss: 3.704 (3.59)  Time: 0.670s, 1528.17/s  (0.695s, 1474.39/s)  LR: 3.945e-04  Data: 0.010 (0.013)
Train: 343 [ 750/1251 ( 60%)]  Loss: 3.597 (3.59)  Time: 0.674s, 1520.11/s  (0.694s, 1475.62/s)  LR: 3.945e-04  Data: 0.010 (0.013)
Train: 343 [ 800/1251 ( 64%)]  Loss: 3.667 (3.60)  Time: 0.708s, 1446.72/s  (0.694s, 1476.07/s)  LR: 3.945e-04  Data: 0.011 (0.013)
Train: 343 [ 850/1251 ( 68%)]  Loss: 3.528 (3.59)  Time: 0.702s, 1458.74/s  (0.694s, 1475.71/s)  LR: 3.945e-04  Data: 0.010 (0.013)
Train: 343 [ 900/1251 ( 72%)]  Loss: 3.677 (3.60)  Time: 0.729s, 1405.44/s  (0.694s, 1476.11/s)  LR: 3.945e-04  Data: 0.010 (0.013)
Train: 343 [ 950/1251 ( 76%)]  Loss: 3.945 (3.62)  Time: 0.670s, 1528.87/s  (0.694s, 1475.13/s)  LR: 3.945e-04  Data: 0.009 (0.012)
Train: 343 [1000/1251 ( 80%)]  Loss: 3.724 (3.62)  Time: 0.688s, 1489.27/s  (0.694s, 1475.44/s)  LR: 3.945e-04  Data: 0.010 (0.012)
Train: 343 [1050/1251 ( 84%)]  Loss: 3.785 (3.63)  Time: 0.674s, 1518.91/s  (0.694s, 1475.83/s)  LR: 3.945e-04  Data: 0.011 (0.012)
Train: 343 [1100/1251 ( 88%)]  Loss: 3.690 (3.63)  Time: 0.681s, 1503.29/s  (0.694s, 1475.81/s)  LR: 3.945e-04  Data: 0.012 (0.012)
Train: 343 [1150/1251 ( 92%)]  Loss: 3.351 (3.62)  Time: 0.705s, 1452.85/s  (0.693s, 1476.77/s)  LR: 3.945e-04  Data: 0.010 (0.012)
Train: 343 [1200/1251 ( 96%)]  Loss: 3.764 (3.63)  Time: 0.666s, 1537.83/s  (0.694s, 1476.43/s)  LR: 3.945e-04  Data: 0.010 (0.012)
Train: 343 [1250/1251 (100%)]  Loss: 3.793 (3.63)  Time: 0.658s, 1555.66/s  (0.693s, 1477.13/s)  LR: 3.945e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.599 (1.599)  Loss:  0.8521 (0.8521)  Acc@1: 89.0625 (89.0625)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.136 (0.583)  Loss:  0.9106 (1.4198)  Acc@1: 86.0849 (74.0720)  Acc@5: 96.6981 (92.0000)
Train: 344 [   0/1251 (  0%)]  Loss: 3.512 (3.51)  Time: 2.399s,  426.88/s  (2.399s,  426.88/s)  LR: 3.920e-04  Data: 1.707 (1.707)
Train: 344 [  50/1251 (  4%)]  Loss: 3.768 (3.64)  Time: 0.676s, 1513.85/s  (0.723s, 1416.93/s)  LR: 3.920e-04  Data: 0.009 (0.047)
Train: 344 [ 100/1251 (  8%)]  Loss: 3.885 (3.72)  Time: 0.673s, 1521.13/s  (0.709s, 1444.84/s)  LR: 3.920e-04  Data: 0.011 (0.029)
Train: 344 [ 150/1251 ( 12%)]  Loss: 3.418 (3.65)  Time: 0.704s, 1454.37/s  (0.702s, 1458.76/s)  LR: 3.920e-04  Data: 0.010 (0.023)
Train: 344 [ 200/1251 ( 16%)]  Loss: 3.947 (3.71)  Time: 0.705s, 1452.82/s  (0.700s, 1463.23/s)  LR: 3.920e-04  Data: 0.012 (0.020)
Train: 344 [ 250/1251 ( 20%)]  Loss: 3.626 (3.69)  Time: 0.672s, 1523.06/s  (0.698s, 1467.80/s)  LR: 3.920e-04  Data: 0.011 (0.018)
Train: 344 [ 300/1251 ( 24%)]  Loss: 3.690 (3.69)  Time: 0.672s, 1522.98/s  (0.697s, 1468.78/s)  LR: 3.920e-04  Data: 0.010 (0.017)
Train: 344 [ 350/1251 ( 28%)]  Loss: 3.614 (3.68)  Time: 0.714s, 1433.70/s  (0.696s, 1471.04/s)  LR: 3.920e-04  Data: 0.011 (0.016)
Train: 344 [ 400/1251 ( 32%)]  Loss: 3.239 (3.63)  Time: 0.671s, 1525.42/s  (0.696s, 1471.56/s)  LR: 3.920e-04  Data: 0.011 (0.015)
Train: 344 [ 450/1251 ( 36%)]  Loss: 3.620 (3.63)  Time: 0.699s, 1464.05/s  (0.696s, 1471.92/s)  LR: 3.920e-04  Data: 0.009 (0.015)
Train: 344 [ 500/1251 ( 40%)]  Loss: 3.875 (3.65)  Time: 0.705s, 1451.77/s  (0.696s, 1470.88/s)  LR: 3.920e-04  Data: 0.010 (0.014)
Train: 344 [ 550/1251 ( 44%)]  Loss: 3.858 (3.67)  Time: 0.672s, 1523.87/s  (0.695s, 1472.37/s)  LR: 3.920e-04  Data: 0.011 (0.014)
Train: 344 [ 600/1251 ( 48%)]  Loss: 3.616 (3.67)  Time: 0.674s, 1518.18/s  (0.695s, 1473.69/s)  LR: 3.920e-04  Data: 0.011 (0.014)
Train: 344 [ 650/1251 ( 52%)]  Loss: 3.661 (3.67)  Time: 0.676s, 1515.79/s  (0.695s, 1473.06/s)  LR: 3.920e-04  Data: 0.010 (0.013)
Train: 344 [ 700/1251 ( 56%)]  Loss: 3.489 (3.65)  Time: 0.680s, 1505.07/s  (0.695s, 1474.18/s)  LR: 3.920e-04  Data: 0.011 (0.013)
Train: 344 [ 750/1251 ( 60%)]  Loss: 3.398 (3.64)  Time: 0.670s, 1528.04/s  (0.695s, 1473.84/s)  LR: 3.920e-04  Data: 0.011 (0.013)
Train: 344 [ 800/1251 ( 64%)]  Loss: 3.667 (3.64)  Time: 0.713s, 1436.00/s  (0.695s, 1473.73/s)  LR: 3.920e-04  Data: 0.010 (0.013)
Train: 344 [ 850/1251 ( 68%)]  Loss: 3.723 (3.64)  Time: 0.665s, 1539.57/s  (0.695s, 1473.74/s)  LR: 3.920e-04  Data: 0.009 (0.013)
Train: 344 [ 900/1251 ( 72%)]  Loss: 3.827 (3.65)  Time: 0.672s, 1524.45/s  (0.695s, 1474.30/s)  LR: 3.920e-04  Data: 0.010 (0.012)
Train: 344 [ 950/1251 ( 76%)]  Loss: 3.540 (3.65)  Time: 0.681s, 1502.62/s  (0.694s, 1474.93/s)  LR: 3.920e-04  Data: 0.009 (0.012)
Train: 344 [1000/1251 ( 80%)]  Loss: 3.495 (3.64)  Time: 0.701s, 1461.48/s  (0.694s, 1475.31/s)  LR: 3.920e-04  Data: 0.008 (0.012)
Train: 344 [1050/1251 ( 84%)]  Loss: 3.537 (3.64)  Time: 0.673s, 1522.33/s  (0.694s, 1476.53/s)  LR: 3.920e-04  Data: 0.010 (0.012)
Train: 344 [1100/1251 ( 88%)]  Loss: 3.379 (3.63)  Time: 0.706s, 1450.63/s  (0.693s, 1477.09/s)  LR: 3.920e-04  Data: 0.010 (0.012)
Train: 344 [1150/1251 ( 92%)]  Loss: 3.567 (3.62)  Time: 0.731s, 1400.60/s  (0.694s, 1475.95/s)  LR: 3.920e-04  Data: 0.009 (0.012)
Train: 344 [1200/1251 ( 96%)]  Loss: 3.438 (3.62)  Time: 0.705s, 1453.46/s  (0.694s, 1476.36/s)  LR: 3.920e-04  Data: 0.009 (0.012)
Train: 344 [1250/1251 (100%)]  Loss: 3.697 (3.62)  Time: 0.659s, 1553.29/s  (0.693s, 1476.86/s)  LR: 3.920e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.543 (1.543)  Loss:  0.8486 (0.8486)  Acc@1: 88.5742 (88.5742)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.136 (0.575)  Loss:  0.8862 (1.3977)  Acc@1: 84.1981 (74.1660)  Acc@5: 96.5802 (91.9120)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-332.pth.tar', 74.44599995849609)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-338.pth.tar', 74.41599993652343)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-337.pth.tar', 74.31000006103515)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-342.pth.tar', 74.2620000390625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-324.pth.tar', 74.2579999584961)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-339.pth.tar', 74.25000000976563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-341.pth.tar', 74.24599993896484)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-331.pth.tar', 74.16800009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-344.pth.tar', 74.16599998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-323.pth.tar', 74.10000006347656)

Train: 345 [   0/1251 (  0%)]  Loss: 3.573 (3.57)  Time: 2.056s,  497.95/s  (2.056s,  497.95/s)  LR: 3.894e-04  Data: 1.442 (1.442)
Train: 345 [  50/1251 (  4%)]  Loss: 3.525 (3.55)  Time: 0.695s, 1473.53/s  (0.725s, 1412.79/s)  LR: 3.894e-04  Data: 0.009 (0.047)
Train: 345 [ 100/1251 (  8%)]  Loss: 3.767 (3.62)  Time: 0.672s, 1523.56/s  (0.708s, 1446.26/s)  LR: 3.894e-04  Data: 0.012 (0.029)
Train: 345 [ 150/1251 ( 12%)]  Loss: 3.257 (3.53)  Time: 0.684s, 1497.46/s  (0.701s, 1460.12/s)  LR: 3.894e-04  Data: 0.011 (0.023)
Train: 345 [ 200/1251 ( 16%)]  Loss: 3.873 (3.60)  Time: 0.746s, 1372.91/s  (0.699s, 1464.94/s)  LR: 3.894e-04  Data: 0.009 (0.020)
Train: 345 [ 250/1251 ( 20%)]  Loss: 3.877 (3.65)  Time: 0.716s, 1430.58/s  (0.698s, 1467.67/s)  LR: 3.894e-04  Data: 0.010 (0.018)
Train: 345 [ 300/1251 ( 24%)]  Loss: 3.555 (3.63)  Time: 0.689s, 1485.21/s  (0.696s, 1472.25/s)  LR: 3.894e-04  Data: 0.010 (0.017)
Train: 345 [ 350/1251 ( 28%)]  Loss: 3.560 (3.62)  Time: 0.716s, 1429.81/s  (0.695s, 1474.31/s)  LR: 3.894e-04  Data: 0.009 (0.016)
Train: 345 [ 400/1251 ( 32%)]  Loss: 3.586 (3.62)  Time: 0.684s, 1496.34/s  (0.694s, 1475.47/s)  LR: 3.894e-04  Data: 0.009 (0.015)
Train: 345 [ 450/1251 ( 36%)]  Loss: 3.515 (3.61)  Time: 0.672s, 1523.96/s  (0.693s, 1476.69/s)  LR: 3.894e-04  Data: 0.012 (0.014)
Train: 345 [ 500/1251 ( 40%)]  Loss: 3.284 (3.58)  Time: 0.674s, 1519.40/s  (0.694s, 1476.45/s)  LR: 3.894e-04  Data: 0.009 (0.014)
Train: 345 [ 550/1251 ( 44%)]  Loss: 3.956 (3.61)  Time: 0.672s, 1523.42/s  (0.693s, 1478.03/s)  LR: 3.894e-04  Data: 0.011 (0.014)
Train: 345 [ 600/1251 ( 48%)]  Loss: 3.586 (3.61)  Time: 0.701s, 1461.70/s  (0.693s, 1477.17/s)  LR: 3.894e-04  Data: 0.011 (0.013)
Train: 345 [ 650/1251 ( 52%)]  Loss: 3.919 (3.63)  Time: 0.672s, 1524.75/s  (0.693s, 1477.46/s)  LR: 3.894e-04  Data: 0.010 (0.013)
Train: 345 [ 700/1251 ( 56%)]  Loss: 3.470 (3.62)  Time: 0.700s, 1462.79/s  (0.693s, 1477.88/s)  LR: 3.894e-04  Data: 0.009 (0.013)
Train: 345 [ 750/1251 ( 60%)]  Loss: 3.934 (3.64)  Time: 0.704s, 1454.61/s  (0.693s, 1477.99/s)  LR: 3.894e-04  Data: 0.009 (0.013)
Train: 345 [ 800/1251 ( 64%)]  Loss: 3.654 (3.64)  Time: 0.682s, 1501.82/s  (0.693s, 1477.03/s)  LR: 3.894e-04  Data: 0.010 (0.013)
Train: 345 [ 850/1251 ( 68%)]  Loss: 3.654 (3.64)  Time: 0.671s, 1526.87/s  (0.693s, 1477.06/s)  LR: 3.894e-04  Data: 0.010 (0.012)
Train: 345 [ 900/1251 ( 72%)]  Loss: 3.758 (3.65)  Time: 0.702s, 1459.69/s  (0.693s, 1477.66/s)  LR: 3.894e-04  Data: 0.008 (0.012)
Train: 345 [ 950/1251 ( 76%)]  Loss: 3.913 (3.66)  Time: 0.703s, 1455.93/s  (0.694s, 1476.57/s)  LR: 3.894e-04  Data: 0.009 (0.012)
Train: 345 [1000/1251 ( 80%)]  Loss: 3.783 (3.67)  Time: 0.730s, 1403.58/s  (0.693s, 1476.76/s)  LR: 3.894e-04  Data: 0.009 (0.012)
Train: 345 [1050/1251 ( 84%)]  Loss: 3.827 (3.67)  Time: 0.681s, 1504.58/s  (0.693s, 1477.09/s)  LR: 3.894e-04  Data: 0.009 (0.012)
Train: 345 [1100/1251 ( 88%)]  Loss: 3.517 (3.67)  Time: 0.674s, 1519.69/s  (0.693s, 1477.21/s)  LR: 3.894e-04  Data: 0.009 (0.012)
Train: 345 [1150/1251 ( 92%)]  Loss: 3.990 (3.68)  Time: 0.673s, 1520.83/s  (0.693s, 1477.45/s)  LR: 3.894e-04  Data: 0.010 (0.012)
Train: 345 [1200/1251 ( 96%)]  Loss: 3.689 (3.68)  Time: 0.775s, 1321.29/s  (0.693s, 1476.99/s)  LR: 3.894e-04  Data: 0.010 (0.012)
Train: 345 [1250/1251 (100%)]  Loss: 3.148 (3.66)  Time: 0.692s, 1478.87/s  (0.693s, 1477.35/s)  LR: 3.894e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.577 (1.577)  Loss:  0.8530 (0.8530)  Acc@1: 89.8438 (89.8438)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.136 (0.590)  Loss:  0.8472 (1.3712)  Acc@1: 84.5519 (74.8180)  Acc@5: 95.9906 (92.2800)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-345.pth.tar', 74.81800001220704)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-332.pth.tar', 74.44599995849609)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-338.pth.tar', 74.41599993652343)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-337.pth.tar', 74.31000006103515)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-342.pth.tar', 74.2620000390625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-324.pth.tar', 74.2579999584961)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-339.pth.tar', 74.25000000976563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-341.pth.tar', 74.24599993896484)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-331.pth.tar', 74.16800009277344)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-344.pth.tar', 74.16599998779297)

Train: 346 [   0/1251 (  0%)]  Loss: 3.585 (3.58)  Time: 2.300s,  445.18/s  (2.300s,  445.18/s)  LR: 3.869e-04  Data: 1.689 (1.689)
Train: 346 [  50/1251 (  4%)]  Loss: 3.523 (3.55)  Time: 0.707s, 1448.02/s  (0.725s, 1412.66/s)  LR: 3.869e-04  Data: 0.011 (0.046)
Train: 346 [ 100/1251 (  8%)]  Loss: 3.626 (3.58)  Time: 0.671s, 1525.38/s  (0.707s, 1449.39/s)  LR: 3.869e-04  Data: 0.010 (0.028)
Train: 346 [ 150/1251 ( 12%)]  Loss: 3.144 (3.47)  Time: 0.731s, 1400.10/s  (0.701s, 1460.73/s)  LR: 3.869e-04  Data: 0.012 (0.023)
Train: 346 [ 200/1251 ( 16%)]  Loss: 3.631 (3.50)  Time: 0.667s, 1534.53/s  (0.700s, 1461.84/s)  LR: 3.869e-04  Data: 0.010 (0.020)
Train: 346 [ 250/1251 ( 20%)]  Loss: 3.856 (3.56)  Time: 0.675s, 1517.07/s  (0.698s, 1466.30/s)  LR: 3.869e-04  Data: 0.010 (0.018)
Train: 346 [ 300/1251 ( 24%)]  Loss: 3.372 (3.53)  Time: 0.678s, 1510.52/s  (0.698s, 1467.41/s)  LR: 3.869e-04  Data: 0.010 (0.017)
Train: 346 [ 350/1251 ( 28%)]  Loss: 3.908 (3.58)  Time: 0.713s, 1435.88/s  (0.696s, 1470.44/s)  LR: 3.869e-04  Data: 0.009 (0.016)
Train: 346 [ 400/1251 ( 32%)]  Loss: 3.649 (3.59)  Time: 0.711s, 1440.10/s  (0.696s, 1471.80/s)  LR: 3.869e-04  Data: 0.011 (0.015)
Train: 346 [ 450/1251 ( 36%)]  Loss: 3.700 (3.60)  Time: 0.673s, 1521.03/s  (0.695s, 1474.01/s)  LR: 3.869e-04  Data: 0.011 (0.014)
Train: 346 [ 500/1251 ( 40%)]  Loss: 3.793 (3.62)  Time: 0.676s, 1515.90/s  (0.694s, 1474.93/s)  LR: 3.869e-04  Data: 0.009 (0.014)
Train: 346 [ 550/1251 ( 44%)]  Loss: 3.621 (3.62)  Time: 0.683s, 1498.55/s  (0.694s, 1475.36/s)  LR: 3.869e-04  Data: 0.010 (0.014)
Train: 346 [ 600/1251 ( 48%)]  Loss: 3.845 (3.63)  Time: 0.669s, 1531.48/s  (0.693s, 1476.65/s)  LR: 3.869e-04  Data: 0.010 (0.013)
Train: 346 [ 650/1251 ( 52%)]  Loss: 3.364 (3.62)  Time: 0.712s, 1438.95/s  (0.693s, 1477.18/s)  LR: 3.869e-04  Data: 0.011 (0.013)
Train: 346 [ 700/1251 ( 56%)]  Loss: 3.410 (3.60)  Time: 0.686s, 1492.45/s  (0.693s, 1477.50/s)  LR: 3.869e-04  Data: 0.013 (0.013)
Train: 346 [ 750/1251 ( 60%)]  Loss: 3.292 (3.58)  Time: 0.770s, 1329.25/s  (0.693s, 1477.01/s)  LR: 3.869e-04  Data: 0.009 (0.013)
Train: 346 [ 800/1251 ( 64%)]  Loss: 3.506 (3.58)  Time: 0.749s, 1368.05/s  (0.693s, 1477.47/s)  LR: 3.869e-04  Data: 0.010 (0.013)
Train: 346 [ 850/1251 ( 68%)]  Loss: 3.110 (3.55)  Time: 0.673s, 1521.46/s  (0.693s, 1477.77/s)  LR: 3.869e-04  Data: 0.011 (0.013)
Train: 346 [ 900/1251 ( 72%)]  Loss: 3.579 (3.55)  Time: 0.716s, 1430.23/s  (0.693s, 1478.11/s)  LR: 3.869e-04  Data: 0.011 (0.012)
Train: 346 [ 950/1251 ( 76%)]  Loss: 3.302 (3.54)  Time: 0.702s, 1459.50/s  (0.693s, 1478.48/s)  LR: 3.869e-04  Data: 0.010 (0.012)
Train: 346 [1000/1251 ( 80%)]  Loss: 3.530 (3.54)  Time: 0.679s, 1507.20/s  (0.693s, 1478.68/s)  LR: 3.869e-04  Data: 0.013 (0.012)
Train: 346 [1050/1251 ( 84%)]  Loss: 3.028 (3.52)  Time: 0.683s, 1498.32/s  (0.692s, 1479.20/s)  LR: 3.869e-04  Data: 0.011 (0.012)
Train: 346 [1100/1251 ( 88%)]  Loss: 3.313 (3.51)  Time: 0.671s, 1525.87/s  (0.692s, 1479.98/s)  LR: 3.869e-04  Data: 0.010 (0.012)
Train: 346 [1150/1251 ( 92%)]  Loss: 3.873 (3.52)  Time: 0.670s, 1528.71/s  (0.692s, 1480.70/s)  LR: 3.869e-04  Data: 0.011 (0.012)
Train: 346 [1200/1251 ( 96%)]  Loss: 3.678 (3.53)  Time: 0.681s, 1502.88/s  (0.691s, 1480.91/s)  LR: 3.869e-04  Data: 0.009 (0.012)
Train: 346 [1250/1251 (100%)]  Loss: 3.445 (3.53)  Time: 0.659s, 1555.04/s  (0.692s, 1480.78/s)  LR: 3.869e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.437 (1.437)  Loss:  0.7773 (0.7773)  Acc@1: 89.3555 (89.3555)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.136 (0.578)  Loss:  0.9219 (1.3517)  Acc@1: 84.1981 (74.5860)  Acc@5: 96.4623 (92.2620)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-345.pth.tar', 74.81800001220704)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-346.pth.tar', 74.58599998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-332.pth.tar', 74.44599995849609)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-338.pth.tar', 74.41599993652343)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-337.pth.tar', 74.31000006103515)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-342.pth.tar', 74.2620000390625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-324.pth.tar', 74.2579999584961)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-339.pth.tar', 74.25000000976563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-341.pth.tar', 74.24599993896484)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-331.pth.tar', 74.16800009277344)

Train: 347 [   0/1251 (  0%)]  Loss: 3.220 (3.22)  Time: 2.166s,  472.68/s  (2.166s,  472.68/s)  LR: 3.844e-04  Data: 1.524 (1.524)
Train: 347 [  50/1251 (  4%)]  Loss: 3.695 (3.46)  Time: 0.716s, 1429.82/s  (0.730s, 1401.83/s)  LR: 3.844e-04  Data: 0.010 (0.050)
Train: 347 [ 100/1251 (  8%)]  Loss: 3.397 (3.44)  Time: 0.700s, 1463.08/s  (0.714s, 1434.97/s)  LR: 3.844e-04  Data: 0.009 (0.031)
Train: 347 [ 150/1251 ( 12%)]  Loss: 3.413 (3.43)  Time: 0.707s, 1448.00/s  (0.707s, 1447.71/s)  LR: 3.844e-04  Data: 0.009 (0.024)
Train: 347 [ 200/1251 ( 16%)]  Loss: 3.856 (3.52)  Time: 0.699s, 1465.38/s  (0.702s, 1457.75/s)  LR: 3.844e-04  Data: 0.009 (0.021)
Train: 347 [ 250/1251 ( 20%)]  Loss: 3.626 (3.53)  Time: 0.698s, 1467.54/s  (0.701s, 1461.52/s)  LR: 3.844e-04  Data: 0.010 (0.018)
Train: 347 [ 300/1251 ( 24%)]  Loss: 3.703 (3.56)  Time: 0.701s, 1460.47/s  (0.699s, 1464.09/s)  LR: 3.844e-04  Data: 0.012 (0.017)
Train: 347 [ 350/1251 ( 28%)]  Loss: 3.686 (3.57)  Time: 0.704s, 1454.63/s  (0.698s, 1467.25/s)  LR: 3.844e-04  Data: 0.010 (0.016)
Train: 347 [ 400/1251 ( 32%)]  Loss: 3.661 (3.58)  Time: 0.671s, 1526.50/s  (0.697s, 1469.93/s)  LR: 3.844e-04  Data: 0.010 (0.015)
Train: 347 [ 450/1251 ( 36%)]  Loss: 3.192 (3.54)  Time: 0.697s, 1468.92/s  (0.697s, 1469.67/s)  LR: 3.844e-04  Data: 0.013 (0.015)
Train: 347 [ 500/1251 ( 40%)]  Loss: 3.554 (3.55)  Time: 0.707s, 1448.28/s  (0.696s, 1471.13/s)  LR: 3.844e-04  Data: 0.010 (0.014)
Train: 347 [ 550/1251 ( 44%)]  Loss: 3.598 (3.55)  Time: 0.712s, 1437.35/s  (0.696s, 1470.96/s)  LR: 3.844e-04  Data: 0.011 (0.014)
Train: 347 [ 600/1251 ( 48%)]  Loss: 3.605 (3.55)  Time: 0.732s, 1398.68/s  (0.696s, 1470.55/s)  LR: 3.844e-04  Data: 0.012 (0.014)
Train: 347 [ 650/1251 ( 52%)]  Loss: 4.095 (3.59)  Time: 0.699s, 1464.86/s  (0.696s, 1471.50/s)  LR: 3.844e-04  Data: 0.009 (0.014)
Train: 347 [ 700/1251 ( 56%)]  Loss: 3.569 (3.59)  Time: 0.671s, 1526.46/s  (0.695s, 1472.34/s)  LR: 3.844e-04  Data: 0.010 (0.013)
Train: 347 [ 750/1251 ( 60%)]  Loss: 3.126 (3.56)  Time: 0.723s, 1417.30/s  (0.695s, 1472.55/s)  LR: 3.844e-04  Data: 0.012 (0.013)
Train: 347 [ 800/1251 ( 64%)]  Loss: 3.513 (3.56)  Time: 0.673s, 1521.09/s  (0.695s, 1473.68/s)  LR: 3.844e-04  Data: 0.009 (0.013)
Train: 347 [ 850/1251 ( 68%)]  Loss: 3.874 (3.58)  Time: 0.705s, 1452.67/s  (0.695s, 1473.89/s)  LR: 3.844e-04  Data: 0.011 (0.013)
Train: 347 [ 900/1251 ( 72%)]  Loss: 3.358 (3.57)  Time: 0.700s, 1462.26/s  (0.695s, 1474.10/s)  LR: 3.844e-04  Data: 0.010 (0.013)
Train: 347 [ 950/1251 ( 76%)]  Loss: 3.434 (3.56)  Time: 0.673s, 1520.77/s  (0.695s, 1474.04/s)  LR: 3.844e-04  Data: 0.010 (0.013)
Train: 347 [1000/1251 ( 80%)]  Loss: 3.853 (3.57)  Time: 0.705s, 1451.55/s  (0.695s, 1473.99/s)  LR: 3.844e-04  Data: 0.010 (0.012)
Train: 347 [1050/1251 ( 84%)]  Loss: 3.238 (3.56)  Time: 0.763s, 1342.30/s  (0.695s, 1473.56/s)  LR: 3.844e-04  Data: 0.011 (0.012)
Train: 347 [1100/1251 ( 88%)]  Loss: 3.506 (3.56)  Time: 0.745s, 1374.08/s  (0.695s, 1473.37/s)  LR: 3.844e-04  Data: 0.008 (0.012)
Train: 347 [1150/1251 ( 92%)]  Loss: 3.942 (3.57)  Time: 0.706s, 1449.58/s  (0.695s, 1473.82/s)  LR: 3.844e-04  Data: 0.010 (0.012)
Train: 347 [1200/1251 ( 96%)]  Loss: 3.728 (3.58)  Time: 0.671s, 1525.71/s  (0.695s, 1474.23/s)  LR: 3.844e-04  Data: 0.010 (0.012)
Train: 347 [1250/1251 (100%)]  Loss: 3.239 (3.56)  Time: 0.695s, 1474.32/s  (0.695s, 1473.86/s)  LR: 3.844e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.527 (1.527)  Loss:  0.8052 (0.8052)  Acc@1: 89.6484 (89.6484)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.136 (0.588)  Loss:  0.8960 (1.3693)  Acc@1: 84.5519 (74.5380)  Acc@5: 97.1698 (92.1580)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-345.pth.tar', 74.81800001220704)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-346.pth.tar', 74.58599998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-347.pth.tar', 74.53800001220704)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-332.pth.tar', 74.44599995849609)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-338.pth.tar', 74.41599993652343)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-337.pth.tar', 74.31000006103515)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-342.pth.tar', 74.2620000390625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-324.pth.tar', 74.2579999584961)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-339.pth.tar', 74.25000000976563)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-341.pth.tar', 74.24599993896484)

Train: 348 [   0/1251 (  0%)]  Loss: 3.825 (3.82)  Time: 2.138s,  479.00/s  (2.138s,  479.00/s)  LR: 3.819e-04  Data: 1.522 (1.522)
Train: 348 [  50/1251 (  4%)]  Loss: 3.532 (3.68)  Time: 0.672s, 1523.67/s  (0.737s, 1389.46/s)  LR: 3.819e-04  Data: 0.011 (0.058)
Train: 348 [ 100/1251 (  8%)]  Loss: 3.949 (3.77)  Time: 0.720s, 1421.50/s  (0.714s, 1435.06/s)  LR: 3.819e-04  Data: 0.010 (0.035)
Train: 348 [ 150/1251 ( 12%)]  Loss: 3.682 (3.75)  Time: 0.684s, 1496.28/s  (0.708s, 1445.64/s)  LR: 3.819e-04  Data: 0.015 (0.027)
Train: 348 [ 200/1251 ( 16%)]  Loss: 3.837 (3.77)  Time: 0.675s, 1516.17/s  (0.707s, 1448.79/s)  LR: 3.819e-04  Data: 0.009 (0.023)
Train: 348 [ 250/1251 ( 20%)]  Loss: 3.438 (3.71)  Time: 0.668s, 1534.00/s  (0.703s, 1456.51/s)  LR: 3.819e-04  Data: 0.011 (0.020)
Train: 348 [ 300/1251 ( 24%)]  Loss: 3.789 (3.72)  Time: 0.707s, 1449.30/s  (0.701s, 1460.52/s)  LR: 3.819e-04  Data: 0.012 (0.019)
Train: 348 [ 350/1251 ( 28%)]  Loss: 3.420 (3.68)  Time: 0.673s, 1521.01/s  (0.700s, 1463.54/s)  LR: 3.819e-04  Data: 0.011 (0.017)
Train: 348 [ 400/1251 ( 32%)]  Loss: 3.634 (3.68)  Time: 0.671s, 1526.13/s  (0.699s, 1464.65/s)  LR: 3.819e-04  Data: 0.011 (0.016)
Train: 348 [ 450/1251 ( 36%)]  Loss: 3.608 (3.67)  Time: 0.668s, 1533.66/s  (0.698s, 1466.57/s)  LR: 3.819e-04  Data: 0.011 (0.016)
Train: 348 [ 500/1251 ( 40%)]  Loss: 3.556 (3.66)  Time: 0.700s, 1462.59/s  (0.697s, 1469.15/s)  LR: 3.819e-04  Data: 0.010 (0.015)
Train: 348 [ 550/1251 ( 44%)]  Loss: 3.458 (3.64)  Time: 0.701s, 1460.06/s  (0.697s, 1470.15/s)  LR: 3.819e-04  Data: 0.011 (0.015)
Train: 348 [ 600/1251 ( 48%)]  Loss: 3.591 (3.64)  Time: 0.671s, 1526.42/s  (0.696s, 1470.94/s)  LR: 3.819e-04  Data: 0.010 (0.014)
Train: 348 [ 650/1251 ( 52%)]  Loss: 3.865 (3.66)  Time: 0.674s, 1520.01/s  (0.696s, 1471.76/s)  LR: 3.819e-04  Data: 0.011 (0.014)
Train: 348 [ 700/1251 ( 56%)]  Loss: 3.649 (3.66)  Time: 0.679s, 1507.51/s  (0.695s, 1472.53/s)  LR: 3.819e-04  Data: 0.014 (0.014)
Train: 348 [ 750/1251 ( 60%)]  Loss: 3.501 (3.65)  Time: 0.672s, 1523.33/s  (0.695s, 1473.72/s)  LR: 3.819e-04  Data: 0.010 (0.014)
Train: 348 [ 800/1251 ( 64%)]  Loss: 3.914 (3.66)  Time: 0.703s, 1456.98/s  (0.695s, 1474.25/s)  LR: 3.819e-04  Data: 0.010 (0.013)
Train: 348 [ 850/1251 ( 68%)]  Loss: 3.363 (3.65)  Time: 0.670s, 1528.12/s  (0.694s, 1475.06/s)  LR: 3.819e-04  Data: 0.009 (0.013)
Train: 348 [ 900/1251 ( 72%)]  Loss: 3.673 (3.65)  Time: 0.707s, 1448.76/s  (0.694s, 1475.74/s)  LR: 3.819e-04  Data: 0.009 (0.013)
Train: 348 [ 950/1251 ( 76%)]  Loss: 3.733 (3.65)  Time: 0.735s, 1392.73/s  (0.694s, 1475.23/s)  LR: 3.819e-04  Data: 0.009 (0.013)
Train: 348 [1000/1251 ( 80%)]  Loss: 3.294 (3.63)  Time: 0.719s, 1425.02/s  (0.694s, 1475.20/s)  LR: 3.819e-04  Data: 0.015 (0.013)
Train: 348 [1050/1251 ( 84%)]  Loss: 3.761 (3.64)  Time: 0.685s, 1494.20/s  (0.695s, 1473.91/s)  LR: 3.819e-04  Data: 0.009 (0.013)
Train: 348 [1100/1251 ( 88%)]  Loss: 3.272 (3.62)  Time: 0.672s, 1524.76/s  (0.695s, 1474.38/s)  LR: 3.819e-04  Data: 0.010 (0.013)
Train: 348 [1150/1251 ( 92%)]  Loss: 3.841 (3.63)  Time: 0.667s, 1535.00/s  (0.695s, 1473.50/s)  LR: 3.819e-04  Data: 0.010 (0.013)
Train: 348 [1200/1251 ( 96%)]  Loss: 3.744 (3.64)  Time: 0.699s, 1465.38/s  (0.695s, 1473.86/s)  LR: 3.819e-04  Data: 0.012 (0.012)
Train: 348 [1250/1251 (100%)]  Loss: 3.790 (3.64)  Time: 0.695s, 1472.99/s  (0.695s, 1473.55/s)  LR: 3.819e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.453 (1.453)  Loss:  0.8062 (0.8062)  Acc@1: 89.7461 (89.7461)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.136 (0.578)  Loss:  0.8545 (1.3720)  Acc@1: 84.1981 (74.7060)  Acc@5: 97.5236 (92.2780)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-345.pth.tar', 74.81800001220704)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-348.pth.tar', 74.7060001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-346.pth.tar', 74.58599998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-347.pth.tar', 74.53800001220704)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-332.pth.tar', 74.44599995849609)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-338.pth.tar', 74.41599993652343)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-337.pth.tar', 74.31000006103515)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-342.pth.tar', 74.2620000390625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-324.pth.tar', 74.2579999584961)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-339.pth.tar', 74.25000000976563)

Train: 349 [   0/1251 (  0%)]  Loss: 3.862 (3.86)  Time: 2.478s,  413.24/s  (2.478s,  413.24/s)  LR: 3.794e-04  Data: 1.846 (1.846)
Train: 349 [  50/1251 (  4%)]  Loss: 3.722 (3.79)  Time: 0.704s, 1455.54/s  (0.734s, 1394.33/s)  LR: 3.794e-04  Data: 0.010 (0.053)
Train: 349 [ 100/1251 (  8%)]  Loss: 3.611 (3.73)  Time: 0.701s, 1460.20/s  (0.713s, 1436.81/s)  LR: 3.794e-04  Data: 0.009 (0.032)
Train: 349 [ 150/1251 ( 12%)]  Loss: 3.685 (3.72)  Time: 0.672s, 1523.55/s  (0.703s, 1456.06/s)  LR: 3.794e-04  Data: 0.011 (0.025)
Train: 349 [ 200/1251 ( 16%)]  Loss: 3.778 (3.73)  Time: 0.698s, 1466.90/s  (0.699s, 1464.32/s)  LR: 3.794e-04  Data: 0.009 (0.021)
Train: 349 [ 250/1251 ( 20%)]  Loss: 3.779 (3.74)  Time: 0.689s, 1487.16/s  (0.697s, 1468.80/s)  LR: 3.794e-04  Data: 0.009 (0.019)
Train: 349 [ 300/1251 ( 24%)]  Loss: 3.661 (3.73)  Time: 0.705s, 1452.67/s  (0.697s, 1469.92/s)  LR: 3.794e-04  Data: 0.011 (0.018)
Train: 349 [ 350/1251 ( 28%)]  Loss: 3.239 (3.67)  Time: 0.701s, 1460.82/s  (0.696s, 1471.65/s)  LR: 3.794e-04  Data: 0.009 (0.017)
Train: 349 [ 400/1251 ( 32%)]  Loss: 3.646 (3.66)  Time: 0.720s, 1423.19/s  (0.695s, 1473.99/s)  LR: 3.794e-04  Data: 0.009 (0.016)
Train: 349 [ 450/1251 ( 36%)]  Loss: 3.371 (3.64)  Time: 0.689s, 1485.86/s  (0.695s, 1473.19/s)  LR: 3.794e-04  Data: 0.010 (0.015)
Train: 349 [ 500/1251 ( 40%)]  Loss: 3.988 (3.67)  Time: 0.705s, 1453.01/s  (0.695s, 1473.67/s)  LR: 3.794e-04  Data: 0.009 (0.015)
Train: 349 [ 550/1251 ( 44%)]  Loss: 3.274 (3.63)  Time: 0.703s, 1457.57/s  (0.694s, 1474.65/s)  LR: 3.794e-04  Data: 0.009 (0.014)
Train: 349 [ 600/1251 ( 48%)]  Loss: 3.691 (3.64)  Time: 0.674s, 1519.33/s  (0.694s, 1474.93/s)  LR: 3.794e-04  Data: 0.010 (0.014)
Train: 349 [ 650/1251 ( 52%)]  Loss: 3.701 (3.64)  Time: 0.702s, 1458.76/s  (0.694s, 1474.77/s)  LR: 3.794e-04  Data: 0.009 (0.014)
Train: 349 [ 700/1251 ( 56%)]  Loss: 3.347 (3.62)  Time: 0.707s, 1447.74/s  (0.694s, 1474.83/s)  LR: 3.794e-04  Data: 0.012 (0.013)
Train: 349 [ 750/1251 ( 60%)]  Loss: 3.605 (3.62)  Time: 0.696s, 1471.28/s  (0.694s, 1475.15/s)  LR: 3.794e-04  Data: 0.016 (0.013)
Train: 349 [ 800/1251 ( 64%)]  Loss: 3.597 (3.62)  Time: 0.675s, 1516.69/s  (0.694s, 1475.81/s)  LR: 3.794e-04  Data: 0.010 (0.013)
Train: 349 [ 850/1251 ( 68%)]  Loss: 3.391 (3.61)  Time: 0.711s, 1440.90/s  (0.694s, 1475.95/s)  LR: 3.794e-04  Data: 0.010 (0.013)
Train: 349 [ 900/1251 ( 72%)]  Loss: 3.467 (3.60)  Time: 0.672s, 1524.38/s  (0.693s, 1477.01/s)  LR: 3.794e-04  Data: 0.011 (0.013)
Train: 349 [ 950/1251 ( 76%)]  Loss: 3.839 (3.61)  Time: 0.675s, 1516.82/s  (0.693s, 1477.13/s)  LR: 3.794e-04  Data: 0.009 (0.013)
Train: 349 [1000/1251 ( 80%)]  Loss: 3.709 (3.62)  Time: 0.712s, 1438.40/s  (0.693s, 1476.92/s)  LR: 3.794e-04  Data: 0.009 (0.013)
Train: 349 [1050/1251 ( 84%)]  Loss: 3.781 (3.62)  Time: 0.680s, 1505.01/s  (0.693s, 1477.43/s)  LR: 3.794e-04  Data: 0.009 (0.012)
Train: 349 [1100/1251 ( 88%)]  Loss: 3.517 (3.62)  Time: 0.672s, 1522.82/s  (0.693s, 1477.70/s)  LR: 3.794e-04  Data: 0.011 (0.012)
Train: 349 [1150/1251 ( 92%)]  Loss: 3.734 (3.62)  Time: 0.706s, 1450.39/s  (0.693s, 1478.54/s)  LR: 3.794e-04  Data: 0.010 (0.012)
Train: 349 [1200/1251 ( 96%)]  Loss: 3.542 (3.62)  Time: 0.671s, 1525.99/s  (0.692s, 1479.18/s)  LR: 3.794e-04  Data: 0.009 (0.012)
Train: 349 [1250/1251 (100%)]  Loss: 3.537 (3.62)  Time: 0.690s, 1483.06/s  (0.692s, 1479.58/s)  LR: 3.794e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.489 (1.489)  Loss:  0.9199 (0.9199)  Acc@1: 89.7461 (89.7461)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.136 (0.587)  Loss:  0.9033 (1.4596)  Acc@1: 85.1415 (74.4340)  Acc@5: 96.9340 (92.0060)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-345.pth.tar', 74.81800001220704)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-348.pth.tar', 74.7060001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-346.pth.tar', 74.58599998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-347.pth.tar', 74.53800001220704)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-332.pth.tar', 74.44599995849609)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-349.pth.tar', 74.43400013916016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-338.pth.tar', 74.41599993652343)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-337.pth.tar', 74.31000006103515)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-342.pth.tar', 74.2620000390625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-324.pth.tar', 74.2579999584961)

Train: 350 [   0/1251 (  0%)]  Loss: 2.842 (2.84)  Time: 2.274s,  450.22/s  (2.274s,  450.22/s)  LR: 3.769e-04  Data: 1.660 (1.660)
Train: 350 [  50/1251 (  4%)]  Loss: 3.447 (3.14)  Time: 0.668s, 1534.07/s  (0.732s, 1399.37/s)  LR: 3.769e-04  Data: 0.009 (0.049)
Train: 350 [ 100/1251 (  8%)]  Loss: 3.545 (3.28)  Time: 0.706s, 1451.41/s  (0.712s, 1437.55/s)  LR: 3.769e-04  Data: 0.010 (0.030)
Train: 350 [ 150/1251 ( 12%)]  Loss: 3.834 (3.42)  Time: 0.693s, 1478.40/s  (0.704s, 1454.16/s)  LR: 3.769e-04  Data: 0.011 (0.023)
Train: 350 [ 200/1251 ( 16%)]  Loss: 3.777 (3.49)  Time: 0.690s, 1484.36/s  (0.701s, 1461.62/s)  LR: 3.769e-04  Data: 0.010 (0.020)
Train: 350 [ 250/1251 ( 20%)]  Loss: 4.018 (3.58)  Time: 0.672s, 1523.37/s  (0.698s, 1467.99/s)  LR: 3.769e-04  Data: 0.010 (0.018)
Train: 350 [ 300/1251 ( 24%)]  Loss: 3.557 (3.57)  Time: 0.727s, 1407.98/s  (0.697s, 1468.25/s)  LR: 3.769e-04  Data: 0.011 (0.017)
Train: 350 [ 350/1251 ( 28%)]  Loss: 3.507 (3.57)  Time: 0.671s, 1527.17/s  (0.696s, 1470.97/s)  LR: 3.769e-04  Data: 0.010 (0.016)
Train: 350 [ 400/1251 ( 32%)]  Loss: 3.642 (3.57)  Time: 0.679s, 1507.78/s  (0.696s, 1471.32/s)  LR: 3.769e-04  Data: 0.010 (0.015)
Train: 350 [ 450/1251 ( 36%)]  Loss: 3.500 (3.57)  Time: 0.673s, 1521.17/s  (0.695s, 1473.72/s)  LR: 3.769e-04  Data: 0.011 (0.015)
Train: 350 [ 500/1251 ( 40%)]  Loss: 3.723 (3.58)  Time: 0.677s, 1513.37/s  (0.694s, 1476.51/s)  LR: 3.769e-04  Data: 0.011 (0.014)
Train: 350 [ 550/1251 ( 44%)]  Loss: 3.883 (3.61)  Time: 0.711s, 1439.39/s  (0.694s, 1475.82/s)  LR: 3.769e-04  Data: 0.010 (0.014)
Train: 350 [ 600/1251 ( 48%)]  Loss: 3.757 (3.62)  Time: 0.676s, 1514.45/s  (0.694s, 1475.91/s)  LR: 3.769e-04  Data: 0.011 (0.014)
Train: 350 [ 650/1251 ( 52%)]  Loss: 2.926 (3.57)  Time: 0.707s, 1448.77/s  (0.694s, 1476.15/s)  LR: 3.769e-04  Data: 0.010 (0.013)
Train: 350 [ 700/1251 ( 56%)]  Loss: 3.459 (3.56)  Time: 0.721s, 1419.44/s  (0.694s, 1475.71/s)  LR: 3.769e-04  Data: 0.011 (0.013)
Train: 350 [ 750/1251 ( 60%)]  Loss: 3.727 (3.57)  Time: 0.681s, 1504.75/s  (0.694s, 1475.83/s)  LR: 3.769e-04  Data: 0.010 (0.013)
Train: 350 [ 800/1251 ( 64%)]  Loss: 3.273 (3.55)  Time: 0.672s, 1524.64/s  (0.694s, 1475.92/s)  LR: 3.769e-04  Data: 0.010 (0.013)
Train: 350 [ 850/1251 ( 68%)]  Loss: 3.406 (3.55)  Time: 0.722s, 1419.08/s  (0.694s, 1476.39/s)  LR: 3.769e-04  Data: 0.010 (0.013)
Train: 350 [ 900/1251 ( 72%)]  Loss: 3.545 (3.55)  Time: 0.669s, 1530.79/s  (0.694s, 1476.17/s)  LR: 3.769e-04  Data: 0.010 (0.013)
Train: 350 [ 950/1251 ( 76%)]  Loss: 3.507 (3.54)  Time: 0.677s, 1513.45/s  (0.693s, 1476.78/s)  LR: 3.769e-04  Data: 0.011 (0.012)
Train: 350 [1000/1251 ( 80%)]  Loss: 3.647 (3.55)  Time: 0.706s, 1451.14/s  (0.694s, 1476.50/s)  LR: 3.769e-04  Data: 0.009 (0.012)
Train: 350 [1050/1251 ( 84%)]  Loss: 4.023 (3.57)  Time: 0.693s, 1476.61/s  (0.694s, 1476.36/s)  LR: 3.769e-04  Data: 0.009 (0.012)
Train: 350 [1100/1251 ( 88%)]  Loss: 3.219 (3.55)  Time: 0.672s, 1524.68/s  (0.693s, 1476.97/s)  LR: 3.769e-04  Data: 0.011 (0.012)
Train: 350 [1150/1251 ( 92%)]  Loss: 3.694 (3.56)  Time: 0.701s, 1459.81/s  (0.694s, 1476.50/s)  LR: 3.769e-04  Data: 0.010 (0.012)
Train: 350 [1200/1251 ( 96%)]  Loss: 3.357 (3.55)  Time: 0.728s, 1406.97/s  (0.694s, 1476.08/s)  LR: 3.769e-04  Data: 0.018 (0.012)
Train: 350 [1250/1251 (100%)]  Loss: 3.585 (3.55)  Time: 0.693s, 1477.19/s  (0.694s, 1476.43/s)  LR: 3.769e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.668 (1.668)  Loss:  0.9102 (0.9102)  Acc@1: 88.7695 (88.7695)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.136 (0.586)  Loss:  0.9492 (1.4054)  Acc@1: 85.6132 (74.9240)  Acc@5: 97.1698 (92.4040)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-350.pth.tar', 74.92400008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-345.pth.tar', 74.81800001220704)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-348.pth.tar', 74.7060001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-346.pth.tar', 74.58599998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-347.pth.tar', 74.53800001220704)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-332.pth.tar', 74.44599995849609)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-349.pth.tar', 74.43400013916016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-338.pth.tar', 74.41599993652343)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-337.pth.tar', 74.31000006103515)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-342.pth.tar', 74.2620000390625)

Train: 351 [   0/1251 (  0%)]  Loss: 3.841 (3.84)  Time: 2.343s,  436.98/s  (2.343s,  436.98/s)  LR: 3.744e-04  Data: 1.711 (1.711)
Train: 351 [  50/1251 (  4%)]  Loss: 3.522 (3.68)  Time: 0.674s, 1520.04/s  (0.733s, 1396.43/s)  LR: 3.744e-04  Data: 0.010 (0.049)
Train: 351 [ 100/1251 (  8%)]  Loss: 3.416 (3.59)  Time: 0.665s, 1540.32/s  (0.713s, 1435.63/s)  LR: 3.744e-04  Data: 0.008 (0.030)
Train: 351 [ 150/1251 ( 12%)]  Loss: 3.892 (3.67)  Time: 0.727s, 1409.18/s  (0.707s, 1448.08/s)  LR: 3.744e-04  Data: 0.009 (0.024)
Train: 351 [ 200/1251 ( 16%)]  Loss: 3.569 (3.65)  Time: 0.726s, 1410.38/s  (0.705s, 1452.17/s)  LR: 3.744e-04  Data: 0.009 (0.020)
Train: 351 [ 250/1251 ( 20%)]  Loss: 3.673 (3.65)  Time: 0.671s, 1525.74/s  (0.703s, 1456.60/s)  LR: 3.744e-04  Data: 0.010 (0.018)
Train: 351 [ 300/1251 ( 24%)]  Loss: 3.791 (3.67)  Time: 0.698s, 1467.75/s  (0.701s, 1460.70/s)  LR: 3.744e-04  Data: 0.010 (0.017)
Train: 351 [ 350/1251 ( 28%)]  Loss: 3.324 (3.63)  Time: 0.671s, 1525.89/s  (0.701s, 1461.36/s)  LR: 3.744e-04  Data: 0.010 (0.016)
Train: 351 [ 400/1251 ( 32%)]  Loss: 3.491 (3.61)  Time: 0.715s, 1431.62/s  (0.699s, 1464.91/s)  LR: 3.744e-04  Data: 0.010 (0.015)
Train: 351 [ 450/1251 ( 36%)]  Loss: 3.878 (3.64)  Time: 0.674s, 1518.82/s  (0.699s, 1465.80/s)  LR: 3.744e-04  Data: 0.009 (0.015)
Train: 351 [ 500/1251 ( 40%)]  Loss: 3.816 (3.66)  Time: 0.703s, 1455.61/s  (0.699s, 1465.05/s)  LR: 3.744e-04  Data: 0.009 (0.015)
Train: 351 [ 550/1251 ( 44%)]  Loss: 3.676 (3.66)  Time: 0.674s, 1518.79/s  (0.698s, 1467.26/s)  LR: 3.744e-04  Data: 0.010 (0.014)
Train: 351 [ 600/1251 ( 48%)]  Loss: 3.389 (3.64)  Time: 0.701s, 1461.05/s  (0.697s, 1468.46/s)  LR: 3.744e-04  Data: 0.010 (0.014)
Train: 351 [ 650/1251 ( 52%)]  Loss: 3.596 (3.63)  Time: 0.670s, 1528.54/s  (0.697s, 1469.62/s)  LR: 3.744e-04  Data: 0.010 (0.014)
Train: 351 [ 700/1251 ( 56%)]  Loss: 3.547 (3.63)  Time: 0.671s, 1525.52/s  (0.696s, 1470.52/s)  LR: 3.744e-04  Data: 0.010 (0.013)
Train: 351 [ 750/1251 ( 60%)]  Loss: 3.429 (3.62)  Time: 0.673s, 1521.18/s  (0.696s, 1470.52/s)  LR: 3.744e-04  Data: 0.010 (0.013)
Train: 351 [ 800/1251 ( 64%)]  Loss: 3.907 (3.63)  Time: 0.718s, 1426.14/s  (0.697s, 1469.90/s)  LR: 3.744e-04  Data: 0.010 (0.013)
Train: 351 [ 850/1251 ( 68%)]  Loss: 3.633 (3.63)  Time: 0.669s, 1529.58/s  (0.696s, 1470.89/s)  LR: 3.744e-04  Data: 0.009 (0.013)
Train: 351 [ 900/1251 ( 72%)]  Loss: 3.323 (3.62)  Time: 0.734s, 1394.21/s  (0.696s, 1471.47/s)  LR: 3.744e-04  Data: 0.009 (0.013)
Train: 351 [ 950/1251 ( 76%)]  Loss: 3.717 (3.62)  Time: 0.672s, 1524.70/s  (0.695s, 1472.50/s)  LR: 3.744e-04  Data: 0.011 (0.013)
Train: 351 [1000/1251 ( 80%)]  Loss: 3.628 (3.62)  Time: 0.701s, 1461.64/s  (0.695s, 1472.55/s)  LR: 3.744e-04  Data: 0.009 (0.012)
Train: 351 [1050/1251 ( 84%)]  Loss: 3.564 (3.62)  Time: 0.672s, 1524.91/s  (0.695s, 1473.42/s)  LR: 3.744e-04  Data: 0.011 (0.012)
Train: 351 [1100/1251 ( 88%)]  Loss: 3.589 (3.62)  Time: 0.670s, 1527.91/s  (0.695s, 1474.19/s)  LR: 3.744e-04  Data: 0.009 (0.012)
Train: 351 [1150/1251 ( 92%)]  Loss: 3.223 (3.60)  Time: 0.700s, 1462.09/s  (0.694s, 1474.81/s)  LR: 3.744e-04  Data: 0.009 (0.012)
Train: 351 [1200/1251 ( 96%)]  Loss: 3.517 (3.60)  Time: 0.673s, 1522.40/s  (0.694s, 1475.32/s)  LR: 3.744e-04  Data: 0.010 (0.012)
Train: 351 [1250/1251 (100%)]  Loss: 3.686 (3.60)  Time: 0.682s, 1502.07/s  (0.694s, 1476.29/s)  LR: 3.744e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.574 (1.574)  Loss:  0.8447 (0.8447)  Acc@1: 89.1602 (89.1602)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.136 (0.586)  Loss:  0.9126 (1.3648)  Acc@1: 84.9057 (74.6800)  Acc@5: 96.8160 (92.2360)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-350.pth.tar', 74.92400008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-345.pth.tar', 74.81800001220704)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-348.pth.tar', 74.7060001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-351.pth.tar', 74.67999990722656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-346.pth.tar', 74.58599998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-347.pth.tar', 74.53800001220704)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-332.pth.tar', 74.44599995849609)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-349.pth.tar', 74.43400013916016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-338.pth.tar', 74.41599993652343)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-337.pth.tar', 74.31000006103515)

Train: 352 [   0/1251 (  0%)]  Loss: 3.429 (3.43)  Time: 2.150s,  476.17/s  (2.150s,  476.17/s)  LR: 3.719e-04  Data: 1.534 (1.534)
Train: 352 [  50/1251 (  4%)]  Loss: 3.776 (3.60)  Time: 0.672s, 1523.06/s  (0.730s, 1401.99/s)  LR: 3.719e-04  Data: 0.010 (0.050)
Train: 352 [ 100/1251 (  8%)]  Loss: 3.459 (3.55)  Time: 0.727s, 1408.38/s  (0.710s, 1442.01/s)  LR: 3.719e-04  Data: 0.010 (0.030)
Train: 352 [ 150/1251 ( 12%)]  Loss: 4.083 (3.69)  Time: 0.678s, 1511.20/s  (0.708s, 1447.17/s)  LR: 3.719e-04  Data: 0.010 (0.024)
Train: 352 [ 200/1251 ( 16%)]  Loss: 3.492 (3.65)  Time: 0.681s, 1504.55/s  (0.703s, 1455.63/s)  LR: 3.719e-04  Data: 0.012 (0.020)
Train: 352 [ 250/1251 ( 20%)]  Loss: 3.425 (3.61)  Time: 0.701s, 1459.82/s  (0.700s, 1462.05/s)  LR: 3.719e-04  Data: 0.014 (0.018)
Train: 352 [ 300/1251 ( 24%)]  Loss: 3.746 (3.63)  Time: 0.676s, 1514.57/s  (0.699s, 1464.40/s)  LR: 3.719e-04  Data: 0.009 (0.017)
Train: 352 [ 350/1251 ( 28%)]  Loss: 3.737 (3.64)  Time: 0.672s, 1524.48/s  (0.698s, 1467.42/s)  LR: 3.719e-04  Data: 0.011 (0.016)
Train: 352 [ 400/1251 ( 32%)]  Loss: 3.684 (3.65)  Time: 0.674s, 1518.62/s  (0.697s, 1469.98/s)  LR: 3.719e-04  Data: 0.010 (0.015)
Train: 352 [ 450/1251 ( 36%)]  Loss: 3.368 (3.62)  Time: 0.693s, 1476.65/s  (0.699s, 1464.89/s)  LR: 3.719e-04  Data: 0.012 (0.015)
Train: 352 [ 500/1251 ( 40%)]  Loss: 3.791 (3.64)  Time: 0.740s, 1383.22/s  (0.700s, 1462.46/s)  LR: 3.719e-04  Data: 0.015 (0.015)
Train: 352 [ 550/1251 ( 44%)]  Loss: 3.490 (3.62)  Time: 0.693s, 1478.62/s  (0.702s, 1459.27/s)  LR: 3.719e-04  Data: 0.013 (0.014)
Train: 352 [ 600/1251 ( 48%)]  Loss: 3.347 (3.60)  Time: 0.673s, 1520.79/s  (0.700s, 1463.05/s)  LR: 3.719e-04  Data: 0.011 (0.014)
Train: 352 [ 650/1251 ( 52%)]  Loss: 3.324 (3.58)  Time: 0.703s, 1456.91/s  (0.698s, 1466.25/s)  LR: 3.719e-04  Data: 0.010 (0.014)
Train: 352 [ 700/1251 ( 56%)]  Loss: 3.834 (3.60)  Time: 0.673s, 1522.09/s  (0.698s, 1467.59/s)  LR: 3.719e-04  Data: 0.010 (0.014)
Train: 352 [ 750/1251 ( 60%)]  Loss: 3.567 (3.60)  Time: 0.681s, 1502.75/s  (0.697s, 1469.26/s)  LR: 3.719e-04  Data: 0.012 (0.013)
Train: 352 [ 800/1251 ( 64%)]  Loss: 4.001 (3.62)  Time: 0.673s, 1521.69/s  (0.697s, 1469.15/s)  LR: 3.719e-04  Data: 0.011 (0.013)
Train: 352 [ 850/1251 ( 68%)]  Loss: 3.423 (3.61)  Time: 0.679s, 1508.18/s  (0.697s, 1469.99/s)  LR: 3.719e-04  Data: 0.009 (0.013)
Train: 352 [ 900/1251 ( 72%)]  Loss: 4.008 (3.63)  Time: 0.677s, 1512.38/s  (0.696s, 1470.46/s)  LR: 3.719e-04  Data: 0.010 (0.013)
Train: 352 [ 950/1251 ( 76%)]  Loss: 3.625 (3.63)  Time: 0.693s, 1476.74/s  (0.697s, 1469.43/s)  LR: 3.719e-04  Data: 0.010 (0.013)
Train: 352 [1000/1251 ( 80%)]  Loss: 3.487 (3.62)  Time: 0.672s, 1523.97/s  (0.697s, 1469.80/s)  LR: 3.719e-04  Data: 0.011 (0.013)
Train: 352 [1050/1251 ( 84%)]  Loss: 3.849 (3.63)  Time: 0.673s, 1520.77/s  (0.697s, 1470.18/s)  LR: 3.719e-04  Data: 0.011 (0.013)
Train: 352 [1100/1251 ( 88%)]  Loss: 3.803 (3.64)  Time: 0.709s, 1444.84/s  (0.696s, 1471.35/s)  LR: 3.719e-04  Data: 0.010 (0.012)
Train: 352 [1150/1251 ( 92%)]  Loss: 3.506 (3.64)  Time: 0.742s, 1380.05/s  (0.696s, 1471.45/s)  LR: 3.719e-04  Data: 0.010 (0.012)
Train: 352 [1200/1251 ( 96%)]  Loss: 3.572 (3.63)  Time: 0.671s, 1526.76/s  (0.696s, 1472.11/s)  LR: 3.719e-04  Data: 0.010 (0.012)
Train: 352 [1250/1251 (100%)]  Loss: 3.680 (3.64)  Time: 0.657s, 1558.55/s  (0.695s, 1472.48/s)  LR: 3.719e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.590 (1.590)  Loss:  0.6821 (0.6821)  Acc@1: 89.6484 (89.6484)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.136 (0.581)  Loss:  0.8008 (1.2768)  Acc@1: 84.5519 (74.8240)  Acc@5: 96.8160 (92.3780)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-350.pth.tar', 74.92400008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-352.pth.tar', 74.82400001220704)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-345.pth.tar', 74.81800001220704)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-348.pth.tar', 74.7060001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-351.pth.tar', 74.67999990722656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-346.pth.tar', 74.58599998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-347.pth.tar', 74.53800001220704)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-332.pth.tar', 74.44599995849609)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-349.pth.tar', 74.43400013916016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-338.pth.tar', 74.41599993652343)

Train: 353 [   0/1251 (  0%)]  Loss: 3.723 (3.72)  Time: 2.259s,  453.31/s  (2.259s,  453.31/s)  LR: 3.694e-04  Data: 1.643 (1.643)
Train: 353 [  50/1251 (  4%)]  Loss: 3.908 (3.82)  Time: 0.704s, 1454.92/s  (0.724s, 1414.26/s)  LR: 3.694e-04  Data: 0.011 (0.051)
Train: 353 [ 100/1251 (  8%)]  Loss: 3.533 (3.72)  Time: 0.700s, 1463.76/s  (0.709s, 1443.74/s)  LR: 3.694e-04  Data: 0.009 (0.031)
Train: 353 [ 150/1251 ( 12%)]  Loss: 3.404 (3.64)  Time: 0.711s, 1440.43/s  (0.707s, 1447.73/s)  LR: 3.694e-04  Data: 0.010 (0.024)
Train: 353 [ 200/1251 ( 16%)]  Loss: 3.316 (3.58)  Time: 0.672s, 1522.82/s  (0.704s, 1454.99/s)  LR: 3.694e-04  Data: 0.010 (0.021)
Train: 353 [ 250/1251 ( 20%)]  Loss: 3.503 (3.56)  Time: 0.697s, 1468.22/s  (0.701s, 1460.40/s)  LR: 3.694e-04  Data: 0.011 (0.019)
Train: 353 [ 300/1251 ( 24%)]  Loss: 3.910 (3.61)  Time: 0.673s, 1520.59/s  (0.700s, 1463.03/s)  LR: 3.694e-04  Data: 0.011 (0.017)
Train: 353 [ 350/1251 ( 28%)]  Loss: 3.244 (3.57)  Time: 0.701s, 1460.15/s  (0.699s, 1464.34/s)  LR: 3.694e-04  Data: 0.009 (0.016)
Train: 353 [ 400/1251 ( 32%)]  Loss: 3.345 (3.54)  Time: 0.671s, 1525.00/s  (0.699s, 1464.98/s)  LR: 3.694e-04  Data: 0.009 (0.016)
Train: 353 [ 450/1251 ( 36%)]  Loss: 3.868 (3.58)  Time: 0.690s, 1484.30/s  (0.698s, 1466.83/s)  LR: 3.694e-04  Data: 0.011 (0.015)
Train: 353 [ 500/1251 ( 40%)]  Loss: 3.429 (3.56)  Time: 0.691s, 1482.30/s  (0.697s, 1468.70/s)  LR: 3.694e-04  Data: 0.010 (0.015)
Train: 353 [ 550/1251 ( 44%)]  Loss: 3.286 (3.54)  Time: 0.670s, 1528.24/s  (0.697s, 1470.12/s)  LR: 3.694e-04  Data: 0.010 (0.014)
Train: 353 [ 600/1251 ( 48%)]  Loss: 3.659 (3.55)  Time: 0.670s, 1527.41/s  (0.697s, 1469.51/s)  LR: 3.694e-04  Data: 0.010 (0.014)
Train: 353 [ 650/1251 ( 52%)]  Loss: 3.306 (3.53)  Time: 0.671s, 1525.50/s  (0.696s, 1470.75/s)  LR: 3.694e-04  Data: 0.010 (0.014)
Train: 353 [ 700/1251 ( 56%)]  Loss: 3.232 (3.51)  Time: 0.672s, 1524.29/s  (0.696s, 1470.45/s)  LR: 3.694e-04  Data: 0.010 (0.013)
Train: 353 [ 750/1251 ( 60%)]  Loss: 3.401 (3.50)  Time: 0.676s, 1514.89/s  (0.696s, 1470.47/s)  LR: 3.694e-04  Data: 0.014 (0.013)
Train: 353 [ 800/1251 ( 64%)]  Loss: 3.362 (3.50)  Time: 0.666s, 1537.01/s  (0.696s, 1470.89/s)  LR: 3.694e-04  Data: 0.011 (0.013)
Train: 353 [ 850/1251 ( 68%)]  Loss: 3.436 (3.49)  Time: 0.676s, 1514.13/s  (0.697s, 1470.21/s)  LR: 3.694e-04  Data: 0.010 (0.013)
Train: 353 [ 900/1251 ( 72%)]  Loss: 3.618 (3.50)  Time: 0.672s, 1522.93/s  (0.696s, 1471.29/s)  LR: 3.694e-04  Data: 0.010 (0.013)
Train: 353 [ 950/1251 ( 76%)]  Loss: 3.640 (3.51)  Time: 0.705s, 1451.96/s  (0.696s, 1471.19/s)  LR: 3.694e-04  Data: 0.011 (0.013)
Train: 353 [1000/1251 ( 80%)]  Loss: 3.347 (3.50)  Time: 0.666s, 1537.76/s  (0.696s, 1470.45/s)  LR: 3.694e-04  Data: 0.010 (0.013)
Train: 353 [1050/1251 ( 84%)]  Loss: 3.909 (3.52)  Time: 0.672s, 1524.37/s  (0.696s, 1470.78/s)  LR: 3.694e-04  Data: 0.009 (0.012)
Train: 353 [1100/1251 ( 88%)]  Loss: 3.426 (3.51)  Time: 0.675s, 1518.00/s  (0.696s, 1470.61/s)  LR: 3.694e-04  Data: 0.009 (0.012)
Train: 353 [1150/1251 ( 92%)]  Loss: 3.926 (3.53)  Time: 0.706s, 1449.81/s  (0.696s, 1471.47/s)  LR: 3.694e-04  Data: 0.010 (0.012)
Train: 353 [1200/1251 ( 96%)]  Loss: 3.566 (3.53)  Time: 0.699s, 1465.76/s  (0.696s, 1471.71/s)  LR: 3.694e-04  Data: 0.010 (0.012)
Train: 353 [1250/1251 (100%)]  Loss: 3.781 (3.54)  Time: 0.658s, 1556.62/s  (0.696s, 1472.17/s)  LR: 3.694e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.623 (1.623)  Loss:  1.0469 (1.0469)  Acc@1: 89.2578 (89.2578)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.136 (0.589)  Loss:  1.0254 (1.5184)  Acc@1: 85.0236 (73.8280)  Acc@5: 96.9340 (91.9720)
Train: 354 [   0/1251 (  0%)]  Loss: 3.762 (3.76)  Time: 2.174s,  471.01/s  (2.174s,  471.01/s)  LR: 3.669e-04  Data: 1.509 (1.509)
Train: 354 [  50/1251 (  4%)]  Loss: 3.252 (3.51)  Time: 0.695s, 1474.27/s  (0.727s, 1408.92/s)  LR: 3.669e-04  Data: 0.011 (0.050)
Train: 354 [ 100/1251 (  8%)]  Loss: 3.785 (3.60)  Time: 0.687s, 1490.64/s  (0.706s, 1450.92/s)  LR: 3.669e-04  Data: 0.010 (0.030)
Train: 354 [ 150/1251 ( 12%)]  Loss: 3.710 (3.63)  Time: 0.674s, 1519.70/s  (0.701s, 1460.07/s)  LR: 3.669e-04  Data: 0.011 (0.024)
Train: 354 [ 200/1251 ( 16%)]  Loss: 3.627 (3.63)  Time: 0.681s, 1504.05/s  (0.699s, 1464.60/s)  LR: 3.669e-04  Data: 0.010 (0.020)
Train: 354 [ 250/1251 ( 20%)]  Loss: 3.000 (3.52)  Time: 0.685s, 1494.79/s  (0.698s, 1467.55/s)  LR: 3.669e-04  Data: 0.010 (0.018)
Train: 354 [ 300/1251 ( 24%)]  Loss: 3.705 (3.55)  Time: 0.717s, 1427.74/s  (0.697s, 1469.85/s)  LR: 3.669e-04  Data: 0.010 (0.017)
Train: 354 [ 350/1251 ( 28%)]  Loss: 3.607 (3.56)  Time: 0.688s, 1487.94/s  (0.696s, 1471.16/s)  LR: 3.669e-04  Data: 0.014 (0.016)
Train: 354 [ 400/1251 ( 32%)]  Loss: 3.929 (3.60)  Time: 0.723s, 1415.69/s  (0.695s, 1472.51/s)  LR: 3.669e-04  Data: 0.012 (0.016)
Train: 354 [ 450/1251 ( 36%)]  Loss: 3.247 (3.56)  Time: 0.722s, 1417.35/s  (0.695s, 1472.86/s)  LR: 3.669e-04  Data: 0.011 (0.015)
Train: 354 [ 500/1251 ( 40%)]  Loss: 3.541 (3.56)  Time: 0.675s, 1517.27/s  (0.694s, 1474.60/s)  LR: 3.669e-04  Data: 0.010 (0.015)
Train: 354 [ 550/1251 ( 44%)]  Loss: 3.825 (3.58)  Time: 0.705s, 1452.77/s  (0.695s, 1473.45/s)  LR: 3.669e-04  Data: 0.011 (0.014)
Train: 354 [ 600/1251 ( 48%)]  Loss: 3.869 (3.60)  Time: 0.666s, 1536.68/s  (0.695s, 1473.31/s)  LR: 3.669e-04  Data: 0.011 (0.014)
Train: 354 [ 650/1251 ( 52%)]  Loss: 3.290 (3.58)  Time: 0.666s, 1536.82/s  (0.695s, 1473.81/s)  LR: 3.669e-04  Data: 0.010 (0.014)
Train: 354 [ 700/1251 ( 56%)]  Loss: 3.016 (3.54)  Time: 0.730s, 1403.04/s  (0.695s, 1473.30/s)  LR: 3.669e-04  Data: 0.009 (0.013)
Train: 354 [ 750/1251 ( 60%)]  Loss: 3.334 (3.53)  Time: 0.691s, 1480.90/s  (0.695s, 1473.95/s)  LR: 3.669e-04  Data: 0.011 (0.013)
Train: 354 [ 800/1251 ( 64%)]  Loss: 3.797 (3.55)  Time: 0.692s, 1480.46/s  (0.695s, 1473.85/s)  LR: 3.669e-04  Data: 0.009 (0.013)
Train: 354 [ 850/1251 ( 68%)]  Loss: 3.640 (3.55)  Time: 0.726s, 1410.80/s  (0.694s, 1474.56/s)  LR: 3.669e-04  Data: 0.010 (0.013)
Train: 354 [ 900/1251 ( 72%)]  Loss: 3.696 (3.56)  Time: 0.709s, 1443.99/s  (0.694s, 1474.57/s)  LR: 3.669e-04  Data: 0.010 (0.013)
Train: 354 [ 950/1251 ( 76%)]  Loss: 3.860 (3.57)  Time: 0.710s, 1442.03/s  (0.694s, 1474.74/s)  LR: 3.669e-04  Data: 0.008 (0.013)
Train: 354 [1000/1251 ( 80%)]  Loss: 3.777 (3.58)  Time: 0.703s, 1457.10/s  (0.694s, 1475.19/s)  LR: 3.669e-04  Data: 0.009 (0.013)
Train: 354 [1050/1251 ( 84%)]  Loss: 3.615 (3.59)  Time: 0.694s, 1474.89/s  (0.694s, 1475.00/s)  LR: 3.669e-04  Data: 0.011 (0.012)
Train: 354 [1100/1251 ( 88%)]  Loss: 3.107 (3.56)  Time: 0.676s, 1514.19/s  (0.694s, 1475.67/s)  LR: 3.669e-04  Data: 0.011 (0.012)
Train: 354 [1150/1251 ( 92%)]  Loss: 3.460 (3.56)  Time: 0.705s, 1452.37/s  (0.694s, 1475.72/s)  LR: 3.669e-04  Data: 0.010 (0.012)
Train: 354 [1200/1251 ( 96%)]  Loss: 3.458 (3.56)  Time: 0.712s, 1437.76/s  (0.694s, 1476.04/s)  LR: 3.669e-04  Data: 0.010 (0.012)
Train: 354 [1250/1251 (100%)]  Loss: 3.790 (3.57)  Time: 0.657s, 1558.86/s  (0.694s, 1476.55/s)  LR: 3.669e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.590 (1.590)  Loss:  0.7832 (0.7832)  Acc@1: 89.2578 (89.2578)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.136 (0.579)  Loss:  0.9790 (1.3822)  Acc@1: 83.9623 (73.9500)  Acc@5: 95.6368 (91.9240)
Train: 355 [   0/1251 (  0%)]  Loss: 3.964 (3.96)  Time: 2.145s,  477.49/s  (2.145s,  477.49/s)  LR: 3.644e-04  Data: 1.528 (1.528)
Train: 355 [  50/1251 (  4%)]  Loss: 3.478 (3.72)  Time: 0.681s, 1504.73/s  (0.734s, 1395.88/s)  LR: 3.644e-04  Data: 0.010 (0.049)
Train: 355 [ 100/1251 (  8%)]  Loss: 3.427 (3.62)  Time: 0.671s, 1526.14/s  (0.712s, 1438.78/s)  LR: 3.644e-04  Data: 0.010 (0.030)
Train: 355 [ 150/1251 ( 12%)]  Loss: 3.594 (3.62)  Time: 0.777s, 1317.90/s  (0.703s, 1455.70/s)  LR: 3.644e-04  Data: 0.013 (0.023)
Train: 355 [ 200/1251 ( 16%)]  Loss: 3.446 (3.58)  Time: 0.670s, 1527.92/s  (0.700s, 1463.44/s)  LR: 3.644e-04  Data: 0.012 (0.020)
Train: 355 [ 250/1251 ( 20%)]  Loss: 3.478 (3.56)  Time: 0.697s, 1468.56/s  (0.697s, 1468.56/s)  LR: 3.644e-04  Data: 0.011 (0.018)
Train: 355 [ 300/1251 ( 24%)]  Loss: 3.621 (3.57)  Time: 0.693s, 1477.57/s  (0.698s, 1467.13/s)  LR: 3.644e-04  Data: 0.012 (0.017)
Train: 355 [ 350/1251 ( 28%)]  Loss: 3.215 (3.53)  Time: 0.673s, 1522.19/s  (0.697s, 1469.34/s)  LR: 3.644e-04  Data: 0.010 (0.016)
Train: 355 [ 400/1251 ( 32%)]  Loss: 3.755 (3.55)  Time: 0.694s, 1476.43/s  (0.697s, 1469.91/s)  LR: 3.644e-04  Data: 0.009 (0.015)
Train: 355 [ 450/1251 ( 36%)]  Loss: 3.677 (3.57)  Time: 0.759s, 1348.51/s  (0.697s, 1468.46/s)  LR: 3.644e-04  Data: 0.009 (0.015)
Train: 355 [ 500/1251 ( 40%)]  Loss: 3.269 (3.54)  Time: 0.673s, 1521.60/s  (0.697s, 1470.16/s)  LR: 3.644e-04  Data: 0.009 (0.014)
Train: 355 [ 550/1251 ( 44%)]  Loss: 3.455 (3.53)  Time: 0.677s, 1511.89/s  (0.696s, 1471.76/s)  LR: 3.644e-04  Data: 0.010 (0.014)
Train: 355 [ 600/1251 ( 48%)]  Loss: 4.015 (3.57)  Time: 0.691s, 1482.01/s  (0.695s, 1473.01/s)  LR: 3.644e-04  Data: 0.011 (0.014)
Train: 355 [ 650/1251 ( 52%)]  Loss: 3.539 (3.57)  Time: 0.687s, 1491.59/s  (0.695s, 1473.43/s)  LR: 3.644e-04  Data: 0.012 (0.013)
Train: 355 [ 700/1251 ( 56%)]  Loss: 3.762 (3.58)  Time: 0.704s, 1455.28/s  (0.695s, 1473.14/s)  LR: 3.644e-04  Data: 0.009 (0.013)
Train: 355 [ 750/1251 ( 60%)]  Loss: 3.561 (3.58)  Time: 0.687s, 1491.06/s  (0.695s, 1472.58/s)  LR: 3.644e-04  Data: 0.009 (0.013)
Train: 355 [ 800/1251 ( 64%)]  Loss: 3.736 (3.59)  Time: 0.677s, 1511.80/s  (0.695s, 1473.46/s)  LR: 3.644e-04  Data: 0.010 (0.013)
Train: 355 [ 850/1251 ( 68%)]  Loss: 3.251 (3.57)  Time: 0.674s, 1520.10/s  (0.695s, 1473.21/s)  LR: 3.644e-04  Data: 0.009 (0.013)
Train: 355 [ 900/1251 ( 72%)]  Loss: 3.487 (3.56)  Time: 0.711s, 1440.44/s  (0.695s, 1473.53/s)  LR: 3.644e-04  Data: 0.011 (0.013)
Train: 355 [ 950/1251 ( 76%)]  Loss: 3.410 (3.56)  Time: 0.672s, 1523.03/s  (0.695s, 1472.46/s)  LR: 3.644e-04  Data: 0.009 (0.013)
Train: 355 [1000/1251 ( 80%)]  Loss: 3.647 (3.56)  Time: 0.700s, 1462.23/s  (0.695s, 1472.86/s)  LR: 3.644e-04  Data: 0.009 (0.012)
Train: 355 [1050/1251 ( 84%)]  Loss: 3.601 (3.56)  Time: 0.760s, 1347.65/s  (0.695s, 1472.42/s)  LR: 3.644e-04  Data: 0.010 (0.012)
Train: 355 [1100/1251 ( 88%)]  Loss: 3.415 (3.56)  Time: 0.680s, 1505.28/s  (0.696s, 1471.85/s)  LR: 3.644e-04  Data: 0.013 (0.012)
Train: 355 [1150/1251 ( 92%)]  Loss: 3.719 (3.56)  Time: 0.701s, 1461.56/s  (0.696s, 1471.98/s)  LR: 3.644e-04  Data: 0.011 (0.012)
Train: 355 [1200/1251 ( 96%)]  Loss: 3.489 (3.56)  Time: 0.672s, 1524.46/s  (0.696s, 1471.33/s)  LR: 3.644e-04  Data: 0.010 (0.012)
Train: 355 [1250/1251 (100%)]  Loss: 2.999 (3.54)  Time: 0.655s, 1562.75/s  (0.696s, 1471.89/s)  LR: 3.644e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.447 (1.447)  Loss:  0.8242 (0.8242)  Acc@1: 89.4531 (89.4531)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.136 (0.583)  Loss:  0.9771 (1.3641)  Acc@1: 84.3160 (74.6800)  Acc@5: 96.1085 (92.1760)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-350.pth.tar', 74.92400008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-352.pth.tar', 74.82400001220704)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-345.pth.tar', 74.81800001220704)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-348.pth.tar', 74.7060001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-355.pth.tar', 74.6800000390625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-351.pth.tar', 74.67999990722656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-346.pth.tar', 74.58599998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-347.pth.tar', 74.53800001220704)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-332.pth.tar', 74.44599995849609)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-349.pth.tar', 74.43400013916016)

Train: 356 [   0/1251 (  0%)]  Loss: 3.324 (3.32)  Time: 2.482s,  412.52/s  (2.482s,  412.52/s)  LR: 3.619e-04  Data: 1.813 (1.813)
Train: 356 [  50/1251 (  4%)]  Loss: 4.089 (3.71)  Time: 0.673s, 1521.90/s  (0.735s, 1393.59/s)  LR: 3.619e-04  Data: 0.011 (0.060)
Train: 356 [ 100/1251 (  8%)]  Loss: 3.226 (3.55)  Time: 0.704s, 1453.97/s  (0.716s, 1429.55/s)  LR: 3.619e-04  Data: 0.010 (0.035)
Train: 356 [ 150/1251 ( 12%)]  Loss: 3.692 (3.58)  Time: 0.672s, 1523.82/s  (0.710s, 1441.81/s)  LR: 3.619e-04  Data: 0.011 (0.027)
Train: 356 [ 200/1251 ( 16%)]  Loss: 3.793 (3.62)  Time: 0.672s, 1522.98/s  (0.705s, 1452.54/s)  LR: 3.619e-04  Data: 0.011 (0.023)
Train: 356 [ 250/1251 ( 20%)]  Loss: 3.496 (3.60)  Time: 0.673s, 1521.18/s  (0.702s, 1458.90/s)  LR: 3.619e-04  Data: 0.012 (0.021)
Train: 356 [ 300/1251 ( 24%)]  Loss: 3.529 (3.59)  Time: 0.670s, 1527.84/s  (0.701s, 1461.25/s)  LR: 3.619e-04  Data: 0.010 (0.019)
Train: 356 [ 350/1251 ( 28%)]  Loss: 3.490 (3.58)  Time: 0.666s, 1537.97/s  (0.701s, 1461.61/s)  LR: 3.619e-04  Data: 0.010 (0.018)
Train: 356 [ 400/1251 ( 32%)]  Loss: 3.229 (3.54)  Time: 0.671s, 1525.86/s  (0.700s, 1462.22/s)  LR: 3.619e-04  Data: 0.010 (0.017)
Train: 356 [ 450/1251 ( 36%)]  Loss: 3.765 (3.56)  Time: 0.666s, 1536.41/s  (0.700s, 1463.20/s)  LR: 3.619e-04  Data: 0.010 (0.016)
Train: 356 [ 500/1251 ( 40%)]  Loss: 3.548 (3.56)  Time: 0.758s, 1351.39/s  (0.699s, 1465.65/s)  LR: 3.619e-04  Data: 0.010 (0.016)
Train: 356 [ 550/1251 ( 44%)]  Loss: 3.431 (3.55)  Time: 0.674s, 1519.55/s  (0.698s, 1466.55/s)  LR: 3.619e-04  Data: 0.011 (0.015)
Train: 356 [ 600/1251 ( 48%)]  Loss: 3.576 (3.55)  Time: 0.672s, 1524.60/s  (0.697s, 1468.25/s)  LR: 3.619e-04  Data: 0.011 (0.015)
Train: 356 [ 650/1251 ( 52%)]  Loss: 3.569 (3.55)  Time: 0.721s, 1420.74/s  (0.697s, 1468.52/s)  LR: 3.619e-04  Data: 0.012 (0.014)
Train: 356 [ 700/1251 ( 56%)]  Loss: 3.209 (3.53)  Time: 0.696s, 1470.45/s  (0.698s, 1467.72/s)  LR: 3.619e-04  Data: 0.009 (0.014)
Train: 356 [ 750/1251 ( 60%)]  Loss: 3.632 (3.54)  Time: 0.677s, 1512.12/s  (0.698s, 1467.77/s)  LR: 3.619e-04  Data: 0.014 (0.014)
Train: 356 [ 800/1251 ( 64%)]  Loss: 3.643 (3.54)  Time: 0.674s, 1520.40/s  (0.697s, 1468.12/s)  LR: 3.619e-04  Data: 0.011 (0.014)
Train: 356 [ 850/1251 ( 68%)]  Loss: 3.596 (3.55)  Time: 0.671s, 1525.65/s  (0.697s, 1469.49/s)  LR: 3.619e-04  Data: 0.012 (0.013)
Train: 356 [ 900/1251 ( 72%)]  Loss: 3.532 (3.55)  Time: 0.699s, 1465.77/s  (0.697s, 1469.83/s)  LR: 3.619e-04  Data: 0.015 (0.013)
Train: 356 [ 950/1251 ( 76%)]  Loss: 3.364 (3.54)  Time: 0.692s, 1478.85/s  (0.697s, 1469.94/s)  LR: 3.619e-04  Data: 0.010 (0.013)
Train: 356 [1000/1251 ( 80%)]  Loss: 3.835 (3.55)  Time: 0.776s, 1319.98/s  (0.696s, 1470.75/s)  LR: 3.619e-04  Data: 0.010 (0.013)
Train: 356 [1050/1251 ( 84%)]  Loss: 3.640 (3.55)  Time: 0.672s, 1523.97/s  (0.696s, 1471.29/s)  LR: 3.619e-04  Data: 0.009 (0.013)
Train: 356 [1100/1251 ( 88%)]  Loss: 3.463 (3.55)  Time: 0.715s, 1432.24/s  (0.696s, 1470.63/s)  LR: 3.619e-04  Data: 0.013 (0.013)
Train: 356 [1150/1251 ( 92%)]  Loss: 3.608 (3.55)  Time: 0.674s, 1519.17/s  (0.696s, 1470.94/s)  LR: 3.619e-04  Data: 0.010 (0.013)
Train: 356 [1200/1251 ( 96%)]  Loss: 3.876 (3.57)  Time: 0.687s, 1490.44/s  (0.696s, 1471.53/s)  LR: 3.619e-04  Data: 0.011 (0.013)
Train: 356 [1250/1251 (100%)]  Loss: 3.457 (3.56)  Time: 0.660s, 1552.04/s  (0.696s, 1471.49/s)  LR: 3.619e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.568 (1.568)  Loss:  0.8213 (0.8213)  Acc@1: 89.1602 (89.1602)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.136 (0.581)  Loss:  0.8799 (1.3760)  Acc@1: 83.8443 (74.3120)  Acc@5: 96.4623 (92.2460)
Train: 357 [   0/1251 (  0%)]  Loss: 3.681 (3.68)  Time: 2.300s,  445.15/s  (2.300s,  445.15/s)  LR: 3.595e-04  Data: 1.685 (1.685)
Train: 357 [  50/1251 (  4%)]  Loss: 3.537 (3.61)  Time: 0.708s, 1446.15/s  (0.722s, 1419.00/s)  LR: 3.595e-04  Data: 0.009 (0.045)
Train: 357 [ 100/1251 (  8%)]  Loss: 3.026 (3.41)  Time: 0.671s, 1525.75/s  (0.705s, 1451.71/s)  LR: 3.595e-04  Data: 0.010 (0.027)
Train: 357 [ 150/1251 ( 12%)]  Loss: 3.534 (3.44)  Time: 0.672s, 1523.02/s  (0.699s, 1465.85/s)  LR: 3.595e-04  Data: 0.010 (0.022)
Train: 357 [ 200/1251 ( 16%)]  Loss: 3.382 (3.43)  Time: 0.664s, 1541.61/s  (0.697s, 1470.01/s)  LR: 3.595e-04  Data: 0.009 (0.019)
Train: 357 [ 250/1251 ( 20%)]  Loss: 3.521 (3.45)  Time: 0.671s, 1526.78/s  (0.697s, 1468.35/s)  LR: 3.595e-04  Data: 0.010 (0.017)
Train: 357 [ 300/1251 ( 24%)]  Loss: 3.144 (3.40)  Time: 0.697s, 1468.76/s  (0.696s, 1471.38/s)  LR: 3.595e-04  Data: 0.010 (0.016)
Train: 357 [ 350/1251 ( 28%)]  Loss: 3.749 (3.45)  Time: 0.671s, 1526.30/s  (0.695s, 1473.56/s)  LR: 3.595e-04  Data: 0.009 (0.015)
Train: 357 [ 400/1251 ( 32%)]  Loss: 3.358 (3.44)  Time: 0.671s, 1526.46/s  (0.696s, 1472.11/s)  LR: 3.595e-04  Data: 0.010 (0.015)
Train: 357 [ 450/1251 ( 36%)]  Loss: 3.619 (3.46)  Time: 0.700s, 1462.51/s  (0.696s, 1471.83/s)  LR: 3.595e-04  Data: 0.009 (0.014)
Train: 357 [ 500/1251 ( 40%)]  Loss: 3.601 (3.47)  Time: 0.701s, 1460.93/s  (0.695s, 1473.35/s)  LR: 3.595e-04  Data: 0.010 (0.014)
Train: 357 [ 550/1251 ( 44%)]  Loss: 3.345 (3.46)  Time: 0.681s, 1504.05/s  (0.695s, 1474.36/s)  LR: 3.595e-04  Data: 0.010 (0.014)
Train: 357 [ 600/1251 ( 48%)]  Loss: 3.561 (3.47)  Time: 0.837s, 1222.82/s  (0.695s, 1473.84/s)  LR: 3.595e-04  Data: 0.009 (0.013)
Train: 357 [ 650/1251 ( 52%)]  Loss: 3.588 (3.47)  Time: 0.684s, 1497.21/s  (0.694s, 1475.32/s)  LR: 3.595e-04  Data: 0.009 (0.013)
Train: 357 [ 700/1251 ( 56%)]  Loss: 3.308 (3.46)  Time: 0.705s, 1453.43/s  (0.694s, 1475.27/s)  LR: 3.595e-04  Data: 0.009 (0.013)
Train: 357 [ 750/1251 ( 60%)]  Loss: 3.416 (3.46)  Time: 0.673s, 1521.93/s  (0.694s, 1475.58/s)  LR: 3.595e-04  Data: 0.011 (0.013)
Train: 357 [ 800/1251 ( 64%)]  Loss: 3.452 (3.46)  Time: 0.668s, 1533.50/s  (0.693s, 1476.82/s)  LR: 3.595e-04  Data: 0.010 (0.013)
Train: 357 [ 850/1251 ( 68%)]  Loss: 3.631 (3.47)  Time: 0.701s, 1461.48/s  (0.693s, 1476.94/s)  LR: 3.595e-04  Data: 0.009 (0.012)
Train: 357 [ 900/1251 ( 72%)]  Loss: 2.939 (3.44)  Time: 0.670s, 1528.10/s  (0.693s, 1477.00/s)  LR: 3.595e-04  Data: 0.010 (0.012)
Train: 357 [ 950/1251 ( 76%)]  Loss: 4.018 (3.47)  Time: 0.701s, 1461.40/s  (0.694s, 1476.32/s)  LR: 3.595e-04  Data: 0.009 (0.012)
Train: 357 [1000/1251 ( 80%)]  Loss: 3.476 (3.47)  Time: 0.705s, 1453.20/s  (0.694s, 1476.32/s)  LR: 3.595e-04  Data: 0.010 (0.012)
Train: 357 [1050/1251 ( 84%)]  Loss: 3.621 (3.48)  Time: 0.684s, 1496.77/s  (0.693s, 1476.62/s)  LR: 3.595e-04  Data: 0.009 (0.012)
Train: 357 [1100/1251 ( 88%)]  Loss: 3.310 (3.47)  Time: 0.678s, 1510.64/s  (0.693s, 1476.84/s)  LR: 3.595e-04  Data: 0.008 (0.012)
Train: 357 [1150/1251 ( 92%)]  Loss: 3.402 (3.47)  Time: 0.669s, 1531.32/s  (0.693s, 1477.29/s)  LR: 3.595e-04  Data: 0.009 (0.012)
Train: 357 [1200/1251 ( 96%)]  Loss: 3.012 (3.45)  Time: 0.678s, 1511.28/s  (0.693s, 1476.65/s)  LR: 3.595e-04  Data: 0.009 (0.012)
Train: 357 [1250/1251 (100%)]  Loss: 3.633 (3.46)  Time: 0.689s, 1485.23/s  (0.694s, 1476.34/s)  LR: 3.595e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.486 (1.486)  Loss:  0.9580 (0.9580)  Acc@1: 88.5742 (88.5742)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.136 (0.575)  Loss:  1.0674 (1.4840)  Acc@1: 84.7877 (74.1880)  Acc@5: 96.6981 (91.8640)
Train: 358 [   0/1251 (  0%)]  Loss: 3.298 (3.30)  Time: 2.242s,  456.68/s  (2.242s,  456.68/s)  LR: 3.570e-04  Data: 1.628 (1.628)
Train: 358 [  50/1251 (  4%)]  Loss: 3.459 (3.38)  Time: 0.672s, 1522.80/s  (0.722s, 1417.89/s)  LR: 3.570e-04  Data: 0.010 (0.046)
Train: 358 [ 100/1251 (  8%)]  Loss: 3.470 (3.41)  Time: 0.684s, 1496.38/s  (0.705s, 1452.11/s)  LR: 3.570e-04  Data: 0.011 (0.029)
Train: 358 [ 150/1251 ( 12%)]  Loss: 3.259 (3.37)  Time: 0.673s, 1521.51/s  (0.701s, 1459.93/s)  LR: 3.570e-04  Data: 0.010 (0.023)
Train: 358 [ 200/1251 ( 16%)]  Loss: 3.541 (3.41)  Time: 0.675s, 1516.57/s  (0.699s, 1465.81/s)  LR: 3.570e-04  Data: 0.012 (0.020)
Train: 358 [ 250/1251 ( 20%)]  Loss: 3.419 (3.41)  Time: 0.737s, 1390.33/s  (0.698s, 1466.82/s)  LR: 3.570e-04  Data: 0.010 (0.018)
Train: 358 [ 300/1251 ( 24%)]  Loss: 3.242 (3.38)  Time: 0.705s, 1451.62/s  (0.696s, 1471.08/s)  LR: 3.570e-04  Data: 0.009 (0.016)
Train: 358 [ 350/1251 ( 28%)]  Loss: 3.575 (3.41)  Time: 0.709s, 1444.53/s  (0.695s, 1473.58/s)  LR: 3.570e-04  Data: 0.009 (0.015)
Train: 358 [ 400/1251 ( 32%)]  Loss: 3.447 (3.41)  Time: 0.674s, 1519.49/s  (0.694s, 1475.38/s)  LR: 3.570e-04  Data: 0.011 (0.015)
Train: 358 [ 450/1251 ( 36%)]  Loss: 3.505 (3.42)  Time: 0.671s, 1526.96/s  (0.693s, 1476.69/s)  LR: 3.570e-04  Data: 0.010 (0.014)
Train: 358 [ 500/1251 ( 40%)]  Loss: 3.536 (3.43)  Time: 0.689s, 1485.20/s  (0.693s, 1477.33/s)  LR: 3.570e-04  Data: 0.017 (0.014)
Train: 358 [ 550/1251 ( 44%)]  Loss: 3.323 (3.42)  Time: 0.700s, 1463.20/s  (0.693s, 1477.76/s)  LR: 3.570e-04  Data: 0.011 (0.014)
Train: 358 [ 600/1251 ( 48%)]  Loss: 3.662 (3.44)  Time: 0.673s, 1521.85/s  (0.693s, 1476.85/s)  LR: 3.570e-04  Data: 0.009 (0.013)
Train: 358 [ 650/1251 ( 52%)]  Loss: 3.479 (3.44)  Time: 0.708s, 1446.89/s  (0.693s, 1476.80/s)  LR: 3.570e-04  Data: 0.010 (0.013)
Train: 358 [ 700/1251 ( 56%)]  Loss: 3.528 (3.45)  Time: 0.672s, 1523.08/s  (0.693s, 1477.29/s)  LR: 3.570e-04  Data: 0.011 (0.013)
Train: 358 [ 750/1251 ( 60%)]  Loss: 3.811 (3.47)  Time: 0.712s, 1438.63/s  (0.693s, 1477.04/s)  LR: 3.570e-04  Data: 0.009 (0.013)
Train: 358 [ 800/1251 ( 64%)]  Loss: 3.399 (3.47)  Time: 0.718s, 1425.56/s  (0.693s, 1477.27/s)  LR: 3.570e-04  Data: 0.009 (0.013)
Train: 358 [ 850/1251 ( 68%)]  Loss: 3.292 (3.46)  Time: 0.705s, 1451.60/s  (0.693s, 1478.04/s)  LR: 3.570e-04  Data: 0.011 (0.012)
Train: 358 [ 900/1251 ( 72%)]  Loss: 3.571 (3.46)  Time: 0.729s, 1404.43/s  (0.693s, 1477.76/s)  LR: 3.570e-04  Data: 0.011 (0.012)
Train: 358 [ 950/1251 ( 76%)]  Loss: 3.520 (3.47)  Time: 0.703s, 1456.06/s  (0.693s, 1478.22/s)  LR: 3.570e-04  Data: 0.009 (0.012)
Train: 358 [1000/1251 ( 80%)]  Loss: 3.380 (3.46)  Time: 0.684s, 1497.62/s  (0.693s, 1478.09/s)  LR: 3.570e-04  Data: 0.010 (0.012)
Train: 358 [1050/1251 ( 84%)]  Loss: 3.427 (3.46)  Time: 0.678s, 1509.84/s  (0.693s, 1477.64/s)  LR: 3.570e-04  Data: 0.011 (0.012)
Train: 358 [1100/1251 ( 88%)]  Loss: 3.528 (3.46)  Time: 0.674s, 1519.94/s  (0.693s, 1477.39/s)  LR: 3.570e-04  Data: 0.011 (0.012)
Train: 358 [1150/1251 ( 92%)]  Loss: 3.283 (3.46)  Time: 0.671s, 1525.15/s  (0.693s, 1477.30/s)  LR: 3.570e-04  Data: 0.011 (0.012)
Train: 358 [1200/1251 ( 96%)]  Loss: 3.179 (3.45)  Time: 0.705s, 1452.20/s  (0.693s, 1477.10/s)  LR: 3.570e-04  Data: 0.010 (0.012)
Train: 358 [1250/1251 (100%)]  Loss: 3.820 (3.46)  Time: 0.656s, 1561.32/s  (0.693s, 1477.77/s)  LR: 3.570e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.659 (1.659)  Loss:  0.8457 (0.8457)  Acc@1: 90.0391 (90.0391)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.136 (0.571)  Loss:  1.0488 (1.4770)  Acc@1: 82.9009 (74.4100)  Acc@5: 95.2830 (92.0940)
Train: 359 [   0/1251 (  0%)]  Loss: 3.340 (3.34)  Time: 2.444s,  418.94/s  (2.444s,  418.94/s)  LR: 3.545e-04  Data: 1.813 (1.813)
Train: 359 [  50/1251 (  4%)]  Loss: 3.134 (3.24)  Time: 0.738s, 1387.88/s  (0.732s, 1399.41/s)  LR: 3.545e-04  Data: 0.010 (0.048)
Train: 359 [ 100/1251 (  8%)]  Loss: 3.546 (3.34)  Time: 0.669s, 1530.92/s  (0.713s, 1436.08/s)  LR: 3.545e-04  Data: 0.009 (0.030)
Train: 359 [ 150/1251 ( 12%)]  Loss: 3.569 (3.40)  Time: 0.748s, 1368.93/s  (0.707s, 1448.61/s)  LR: 3.545e-04  Data: 0.010 (0.023)
Train: 359 [ 200/1251 ( 16%)]  Loss: 3.783 (3.47)  Time: 0.717s, 1427.39/s  (0.703s, 1457.44/s)  LR: 3.545e-04  Data: 0.010 (0.020)
Train: 359 [ 250/1251 ( 20%)]  Loss: 3.671 (3.51)  Time: 0.673s, 1521.80/s  (0.701s, 1461.19/s)  LR: 3.545e-04  Data: 0.012 (0.018)
Train: 359 [ 300/1251 ( 24%)]  Loss: 3.662 (3.53)  Time: 0.700s, 1462.40/s  (0.699s, 1465.20/s)  LR: 3.545e-04  Data: 0.009 (0.017)
Train: 359 [ 350/1251 ( 28%)]  Loss: 3.461 (3.52)  Time: 0.691s, 1480.99/s  (0.698s, 1466.80/s)  LR: 3.545e-04  Data: 0.010 (0.016)
Train: 359 [ 400/1251 ( 32%)]  Loss: 3.653 (3.54)  Time: 0.674s, 1518.57/s  (0.697s, 1468.67/s)  LR: 3.545e-04  Data: 0.011 (0.015)
Train: 359 [ 450/1251 ( 36%)]  Loss: 3.585 (3.54)  Time: 0.764s, 1339.50/s  (0.697s, 1470.05/s)  LR: 3.545e-04  Data: 0.022 (0.015)
Train: 359 [ 500/1251 ( 40%)]  Loss: 3.508 (3.54)  Time: 0.695s, 1472.33/s  (0.696s, 1471.92/s)  LR: 3.545e-04  Data: 0.009 (0.014)
Train: 359 [ 550/1251 ( 44%)]  Loss: 3.769 (3.56)  Time: 0.710s, 1442.81/s  (0.695s, 1472.43/s)  LR: 3.545e-04  Data: 0.023 (0.014)
Train: 359 [ 600/1251 ( 48%)]  Loss: 3.451 (3.55)  Time: 0.685s, 1495.50/s  (0.695s, 1473.21/s)  LR: 3.545e-04  Data: 0.029 (0.014)
Train: 359 [ 650/1251 ( 52%)]  Loss: 3.427 (3.54)  Time: 0.757s, 1353.25/s  (0.695s, 1473.02/s)  LR: 3.545e-04  Data: 0.009 (0.013)
Train: 359 [ 700/1251 ( 56%)]  Loss: 3.438 (3.53)  Time: 0.679s, 1508.11/s  (0.695s, 1473.29/s)  LR: 3.545e-04  Data: 0.009 (0.013)
Train: 359 [ 750/1251 ( 60%)]  Loss: 3.617 (3.54)  Time: 0.758s, 1351.81/s  (0.695s, 1473.89/s)  LR: 3.545e-04  Data: 0.009 (0.013)
Train: 359 [ 800/1251 ( 64%)]  Loss: 3.580 (3.54)  Time: 0.674s, 1518.87/s  (0.694s, 1474.62/s)  LR: 3.545e-04  Data: 0.010 (0.013)
Train: 359 [ 850/1251 ( 68%)]  Loss: 3.088 (3.52)  Time: 0.673s, 1520.63/s  (0.694s, 1475.43/s)  LR: 3.545e-04  Data: 0.010 (0.013)
Train: 359 [ 900/1251 ( 72%)]  Loss: 3.437 (3.51)  Time: 0.696s, 1470.49/s  (0.694s, 1475.92/s)  LR: 3.545e-04  Data: 0.011 (0.013)
Train: 359 [ 950/1251 ( 76%)]  Loss: 3.529 (3.51)  Time: 0.705s, 1453.15/s  (0.694s, 1475.91/s)  LR: 3.545e-04  Data: 0.010 (0.012)
Train: 359 [1000/1251 ( 80%)]  Loss: 3.500 (3.51)  Time: 0.682s, 1501.61/s  (0.693s, 1476.66/s)  LR: 3.545e-04  Data: 0.010 (0.012)
Train: 359 [1050/1251 ( 84%)]  Loss: 3.606 (3.52)  Time: 0.678s, 1511.29/s  (0.693s, 1477.31/s)  LR: 3.545e-04  Data: 0.009 (0.012)
Train: 359 [1100/1251 ( 88%)]  Loss: 3.797 (3.53)  Time: 0.688s, 1487.78/s  (0.694s, 1476.10/s)  LR: 3.545e-04  Data: 0.009 (0.012)
Train: 359 [1150/1251 ( 92%)]  Loss: 3.508 (3.53)  Time: 0.677s, 1511.45/s  (0.694s, 1476.41/s)  LR: 3.545e-04  Data: 0.011 (0.012)
Train: 359 [1200/1251 ( 96%)]  Loss: 3.582 (3.53)  Time: 0.678s, 1509.46/s  (0.693s, 1476.65/s)  LR: 3.545e-04  Data: 0.011 (0.012)
Train: 359 [1250/1251 (100%)]  Loss: 3.510 (3.53)  Time: 0.655s, 1563.47/s  (0.693s, 1476.87/s)  LR: 3.545e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.540 (1.540)  Loss:  0.7671 (0.7671)  Acc@1: 89.3555 (89.3555)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.136 (0.585)  Loss:  0.8394 (1.3789)  Acc@1: 84.6698 (74.7860)  Acc@5: 96.6981 (92.1940)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-350.pth.tar', 74.92400008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-352.pth.tar', 74.82400001220704)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-345.pth.tar', 74.81800001220704)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-359.pth.tar', 74.78600006347656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-348.pth.tar', 74.7060001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-355.pth.tar', 74.6800000390625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-351.pth.tar', 74.67999990722656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-346.pth.tar', 74.58599998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-347.pth.tar', 74.53800001220704)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-332.pth.tar', 74.44599995849609)

Train: 360 [   0/1251 (  0%)]  Loss: 3.216 (3.22)  Time: 2.447s,  418.51/s  (2.447s,  418.51/s)  LR: 3.520e-04  Data: 1.815 (1.815)
Train: 360 [  50/1251 (  4%)]  Loss: 3.617 (3.42)  Time: 0.666s, 1538.22/s  (0.736s, 1390.75/s)  LR: 3.520e-04  Data: 0.010 (0.050)
Train: 360 [ 100/1251 (  8%)]  Loss: 3.539 (3.46)  Time: 0.676s, 1513.83/s  (0.713s, 1436.10/s)  LR: 3.520e-04  Data: 0.010 (0.031)
Train: 360 [ 150/1251 ( 12%)]  Loss: 3.299 (3.42)  Time: 0.709s, 1444.35/s  (0.706s, 1450.51/s)  LR: 3.520e-04  Data: 0.010 (0.024)
Train: 360 [ 200/1251 ( 16%)]  Loss: 3.271 (3.39)  Time: 0.668s, 1532.97/s  (0.703s, 1455.75/s)  LR: 3.520e-04  Data: 0.011 (0.021)
Train: 360 [ 250/1251 ( 20%)]  Loss: 3.274 (3.37)  Time: 0.674s, 1519.37/s  (0.701s, 1460.64/s)  LR: 3.520e-04  Data: 0.010 (0.019)
Train: 360 [ 300/1251 ( 24%)]  Loss: 3.943 (3.45)  Time: 0.718s, 1426.07/s  (0.700s, 1462.40/s)  LR: 3.520e-04  Data: 0.011 (0.017)
Train: 360 [ 350/1251 ( 28%)]  Loss: 3.494 (3.46)  Time: 0.673s, 1521.42/s  (0.699s, 1465.86/s)  LR: 3.520e-04  Data: 0.010 (0.016)
Train: 360 [ 400/1251 ( 32%)]  Loss: 3.533 (3.47)  Time: 0.690s, 1484.96/s  (0.698s, 1466.41/s)  LR: 3.520e-04  Data: 0.011 (0.016)
Train: 360 [ 450/1251 ( 36%)]  Loss: 3.674 (3.49)  Time: 0.678s, 1509.27/s  (0.697s, 1468.42/s)  LR: 3.520e-04  Data: 0.011 (0.015)
Train: 360 [ 500/1251 ( 40%)]  Loss: 3.585 (3.49)  Time: 0.671s, 1525.54/s  (0.697s, 1469.60/s)  LR: 3.520e-04  Data: 0.011 (0.015)
Train: 360 [ 550/1251 ( 44%)]  Loss: 3.230 (3.47)  Time: 0.673s, 1521.08/s  (0.697s, 1469.76/s)  LR: 3.520e-04  Data: 0.011 (0.014)
Train: 360 [ 600/1251 ( 48%)]  Loss: 3.586 (3.48)  Time: 0.713s, 1436.48/s  (0.696s, 1470.48/s)  LR: 3.520e-04  Data: 0.011 (0.014)
Train: 360 [ 650/1251 ( 52%)]  Loss: 3.383 (3.47)  Time: 0.674s, 1519.37/s  (0.696s, 1470.96/s)  LR: 3.520e-04  Data: 0.010 (0.014)
Train: 360 [ 700/1251 ( 56%)]  Loss: 3.626 (3.48)  Time: 0.681s, 1503.61/s  (0.696s, 1471.96/s)  LR: 3.520e-04  Data: 0.011 (0.013)
Train: 360 [ 750/1251 ( 60%)]  Loss: 3.845 (3.51)  Time: 0.688s, 1487.50/s  (0.695s, 1472.69/s)  LR: 3.520e-04  Data: 0.012 (0.013)
Train: 360 [ 800/1251 ( 64%)]  Loss: 3.633 (3.51)  Time: 0.735s, 1393.88/s  (0.695s, 1474.29/s)  LR: 3.520e-04  Data: 0.010 (0.013)
Train: 360 [ 850/1251 ( 68%)]  Loss: 3.504 (3.51)  Time: 0.708s, 1446.36/s  (0.695s, 1473.77/s)  LR: 3.520e-04  Data: 0.011 (0.013)
Train: 360 [ 900/1251 ( 72%)]  Loss: 3.361 (3.51)  Time: 0.671s, 1525.70/s  (0.694s, 1474.49/s)  LR: 3.520e-04  Data: 0.011 (0.013)
Train: 360 [ 950/1251 ( 76%)]  Loss: 3.682 (3.51)  Time: 0.677s, 1512.97/s  (0.694s, 1474.63/s)  LR: 3.520e-04  Data: 0.008 (0.013)
Train: 360 [1000/1251 ( 80%)]  Loss: 3.931 (3.53)  Time: 0.687s, 1491.39/s  (0.694s, 1475.10/s)  LR: 3.520e-04  Data: 0.010 (0.012)
Train: 360 [1050/1251 ( 84%)]  Loss: 3.761 (3.54)  Time: 0.722s, 1418.63/s  (0.694s, 1475.79/s)  LR: 3.520e-04  Data: 0.011 (0.012)
Train: 360 [1100/1251 ( 88%)]  Loss: 3.225 (3.53)  Time: 0.754s, 1357.21/s  (0.694s, 1476.11/s)  LR: 3.520e-04  Data: 0.010 (0.012)
Train: 360 [1150/1251 ( 92%)]  Loss: 3.524 (3.53)  Time: 0.673s, 1521.64/s  (0.694s, 1476.28/s)  LR: 3.520e-04  Data: 0.011 (0.012)
Train: 360 [1200/1251 ( 96%)]  Loss: 3.443 (3.53)  Time: 0.678s, 1509.85/s  (0.694s, 1476.00/s)  LR: 3.520e-04  Data: 0.011 (0.012)
Train: 360 [1250/1251 (100%)]  Loss: 3.442 (3.52)  Time: 0.662s, 1547.80/s  (0.694s, 1475.87/s)  LR: 3.520e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.565 (1.565)  Loss:  0.7578 (0.7578)  Acc@1: 90.7227 (90.7227)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.136 (0.580)  Loss:  0.9326 (1.3669)  Acc@1: 84.9057 (74.8940)  Acc@5: 96.6981 (92.4060)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-350.pth.tar', 74.92400008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-360.pth.tar', 74.8940000366211)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-352.pth.tar', 74.82400001220704)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-345.pth.tar', 74.81800001220704)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-359.pth.tar', 74.78600006347656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-348.pth.tar', 74.7060001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-355.pth.tar', 74.6800000390625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-351.pth.tar', 74.67999990722656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-346.pth.tar', 74.58599998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-347.pth.tar', 74.53800001220704)

Train: 361 [   0/1251 (  0%)]  Loss: 3.734 (3.73)  Time: 2.394s,  427.73/s  (2.394s,  427.73/s)  LR: 3.496e-04  Data: 1.780 (1.780)
Train: 361 [  50/1251 (  4%)]  Loss: 3.505 (3.62)  Time: 0.707s, 1448.44/s  (0.738s, 1387.95/s)  LR: 3.496e-04  Data: 0.009 (0.051)
Train: 361 [ 100/1251 (  8%)]  Loss: 3.713 (3.65)  Time: 0.701s, 1460.77/s  (0.714s, 1433.92/s)  LR: 3.496e-04  Data: 0.009 (0.031)
Train: 361 [ 150/1251 ( 12%)]  Loss: 3.793 (3.69)  Time: 0.672s, 1524.85/s  (0.706s, 1451.13/s)  LR: 3.496e-04  Data: 0.009 (0.024)
Train: 361 [ 200/1251 ( 16%)]  Loss: 3.662 (3.68)  Time: 0.671s, 1526.53/s  (0.701s, 1459.81/s)  LR: 3.496e-04  Data: 0.011 (0.020)
Train: 361 [ 250/1251 ( 20%)]  Loss: 3.617 (3.67)  Time: 0.690s, 1483.26/s  (0.698s, 1466.02/s)  LR: 3.496e-04  Data: 0.014 (0.018)
Train: 361 [ 300/1251 ( 24%)]  Loss: 3.526 (3.65)  Time: 0.691s, 1482.41/s  (0.698s, 1467.69/s)  LR: 3.496e-04  Data: 0.009 (0.017)
Train: 361 [ 350/1251 ( 28%)]  Loss: 3.507 (3.63)  Time: 0.718s, 1426.47/s  (0.696s, 1470.79/s)  LR: 3.496e-04  Data: 0.009 (0.016)
Train: 361 [ 400/1251 ( 32%)]  Loss: 3.720 (3.64)  Time: 0.664s, 1542.22/s  (0.695s, 1472.37/s)  LR: 3.496e-04  Data: 0.009 (0.016)
Train: 361 [ 450/1251 ( 36%)]  Loss: 3.434 (3.62)  Time: 0.745s, 1373.72/s  (0.696s, 1471.66/s)  LR: 3.496e-04  Data: 0.009 (0.015)
Train: 361 [ 500/1251 ( 40%)]  Loss: 3.628 (3.62)  Time: 0.702s, 1459.07/s  (0.696s, 1471.18/s)  LR: 3.496e-04  Data: 0.009 (0.015)
Train: 361 [ 550/1251 ( 44%)]  Loss: 3.804 (3.64)  Time: 0.672s, 1524.18/s  (0.695s, 1472.37/s)  LR: 3.496e-04  Data: 0.010 (0.014)
Train: 361 [ 600/1251 ( 48%)]  Loss: 3.633 (3.64)  Time: 0.718s, 1426.97/s  (0.695s, 1472.98/s)  LR: 3.496e-04  Data: 0.009 (0.014)
Train: 361 [ 650/1251 ( 52%)]  Loss: 3.632 (3.64)  Time: 0.709s, 1444.27/s  (0.695s, 1472.93/s)  LR: 3.496e-04  Data: 0.011 (0.014)
Train: 361 [ 700/1251 ( 56%)]  Loss: 3.255 (3.61)  Time: 0.672s, 1524.33/s  (0.695s, 1473.95/s)  LR: 3.496e-04  Data: 0.010 (0.013)
Train: 361 [ 750/1251 ( 60%)]  Loss: 3.463 (3.60)  Time: 0.719s, 1424.41/s  (0.695s, 1474.10/s)  LR: 3.496e-04  Data: 0.010 (0.013)
Train: 361 [ 800/1251 ( 64%)]  Loss: 3.689 (3.61)  Time: 0.715s, 1432.24/s  (0.694s, 1474.56/s)  LR: 3.496e-04  Data: 0.009 (0.013)
Train: 361 [ 850/1251 ( 68%)]  Loss: 3.358 (3.59)  Time: 0.700s, 1462.81/s  (0.695s, 1473.73/s)  LR: 3.496e-04  Data: 0.009 (0.013)
Train: 361 [ 900/1251 ( 72%)]  Loss: 3.625 (3.59)  Time: 0.710s, 1441.75/s  (0.695s, 1474.27/s)  LR: 3.496e-04  Data: 0.010 (0.013)
Train: 361 [ 950/1251 ( 76%)]  Loss: 3.548 (3.59)  Time: 0.676s, 1514.03/s  (0.694s, 1474.64/s)  LR: 3.496e-04  Data: 0.010 (0.013)
Train: 361 [1000/1251 ( 80%)]  Loss: 3.367 (3.58)  Time: 0.682s, 1500.76/s  (0.694s, 1475.15/s)  LR: 3.496e-04  Data: 0.010 (0.012)
Train: 361 [1050/1251 ( 84%)]  Loss: 3.781 (3.59)  Time: 0.699s, 1464.65/s  (0.694s, 1475.62/s)  LR: 3.496e-04  Data: 0.013 (0.012)
Train: 361 [1100/1251 ( 88%)]  Loss: 3.705 (3.60)  Time: 0.700s, 1463.72/s  (0.694s, 1476.08/s)  LR: 3.496e-04  Data: 0.009 (0.012)
Train: 361 [1150/1251 ( 92%)]  Loss: 3.462 (3.59)  Time: 0.769s, 1332.38/s  (0.694s, 1475.30/s)  LR: 3.496e-04  Data: 0.010 (0.012)
Train: 361 [1200/1251 ( 96%)]  Loss: 3.462 (3.59)  Time: 0.684s, 1497.80/s  (0.695s, 1473.22/s)  LR: 3.496e-04  Data: 0.015 (0.012)
Train: 361 [1250/1251 (100%)]  Loss: 3.209 (3.57)  Time: 0.683s, 1499.12/s  (0.695s, 1472.34/s)  LR: 3.496e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.639 (1.639)  Loss:  0.7285 (0.7285)  Acc@1: 89.8438 (89.8438)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.136 (0.665)  Loss:  0.9331 (1.3589)  Acc@1: 85.1415 (74.4900)  Acc@5: 96.5802 (92.0480)
Train: 362 [   0/1251 (  0%)]  Loss: 3.462 (3.46)  Time: 2.059s,  497.33/s  (2.059s,  497.33/s)  LR: 3.471e-04  Data: 1.443 (1.443)
Train: 362 [  50/1251 (  4%)]  Loss: 3.502 (3.48)  Time: 0.674s, 1518.58/s  (0.712s, 1439.15/s)  LR: 3.471e-04  Data: 0.011 (0.044)
Train: 362 [ 100/1251 (  8%)]  Loss: 3.577 (3.51)  Time: 0.701s, 1460.63/s  (0.704s, 1455.04/s)  LR: 3.471e-04  Data: 0.009 (0.027)
Train: 362 [ 150/1251 ( 12%)]  Loss: 3.712 (3.56)  Time: 0.718s, 1426.02/s  (0.697s, 1468.51/s)  LR: 3.471e-04  Data: 0.011 (0.021)
Train: 362 [ 200/1251 ( 16%)]  Loss: 3.186 (3.49)  Time: 0.714s, 1433.61/s  (0.697s, 1468.54/s)  LR: 3.471e-04  Data: 0.011 (0.019)
Train: 362 [ 250/1251 ( 20%)]  Loss: 3.585 (3.50)  Time: 0.680s, 1505.81/s  (0.696s, 1471.69/s)  LR: 3.471e-04  Data: 0.011 (0.017)
Train: 362 [ 300/1251 ( 24%)]  Loss: 3.854 (3.55)  Time: 0.673s, 1520.88/s  (0.697s, 1470.06/s)  LR: 3.471e-04  Data: 0.011 (0.016)
Train: 362 [ 350/1251 ( 28%)]  Loss: 3.548 (3.55)  Time: 0.694s, 1476.06/s  (0.696s, 1470.84/s)  LR: 3.471e-04  Data: 0.009 (0.015)
Train: 362 [ 400/1251 ( 32%)]  Loss: 3.555 (3.55)  Time: 0.702s, 1458.29/s  (0.696s, 1471.64/s)  LR: 3.471e-04  Data: 0.009 (0.015)
Train: 362 [ 450/1251 ( 36%)]  Loss: 3.498 (3.55)  Time: 0.707s, 1448.97/s  (0.695s, 1472.93/s)  LR: 3.471e-04  Data: 0.011 (0.014)
Train: 362 [ 500/1251 ( 40%)]  Loss: 3.601 (3.55)  Time: 0.672s, 1523.21/s  (0.695s, 1474.04/s)  LR: 3.471e-04  Data: 0.011 (0.014)
Train: 362 [ 550/1251 ( 44%)]  Loss: 3.074 (3.51)  Time: 0.672s, 1523.87/s  (0.695s, 1473.79/s)  LR: 3.471e-04  Data: 0.011 (0.014)
Train: 362 [ 600/1251 ( 48%)]  Loss: 3.791 (3.53)  Time: 0.669s, 1530.35/s  (0.695s, 1472.63/s)  LR: 3.471e-04  Data: 0.010 (0.013)
Train: 362 [ 650/1251 ( 52%)]  Loss: 3.604 (3.54)  Time: 0.674s, 1519.17/s  (0.696s, 1472.22/s)  LR: 3.471e-04  Data: 0.009 (0.013)
Train: 362 [ 700/1251 ( 56%)]  Loss: 3.491 (3.54)  Time: 0.735s, 1394.06/s  (0.696s, 1471.53/s)  LR: 3.471e-04  Data: 0.012 (0.013)
Train: 362 [ 750/1251 ( 60%)]  Loss: 3.818 (3.55)  Time: 0.684s, 1496.90/s  (0.696s, 1470.87/s)  LR: 3.471e-04  Data: 0.010 (0.013)
Train: 362 [ 800/1251 ( 64%)]  Loss: 3.842 (3.57)  Time: 0.674s, 1519.56/s  (0.696s, 1471.34/s)  LR: 3.471e-04  Data: 0.011 (0.013)
Train: 362 [ 850/1251 ( 68%)]  Loss: 3.783 (3.58)  Time: 0.695s, 1473.66/s  (0.696s, 1471.34/s)  LR: 3.471e-04  Data: 0.010 (0.013)
Train: 362 [ 900/1251 ( 72%)]  Loss: 3.445 (3.58)  Time: 0.672s, 1523.66/s  (0.696s, 1472.26/s)  LR: 3.471e-04  Data: 0.011 (0.012)
Train: 362 [ 950/1251 ( 76%)]  Loss: 3.482 (3.57)  Time: 0.668s, 1533.81/s  (0.696s, 1472.01/s)  LR: 3.471e-04  Data: 0.010 (0.012)
Train: 362 [1000/1251 ( 80%)]  Loss: 3.415 (3.56)  Time: 0.674s, 1519.75/s  (0.695s, 1472.36/s)  LR: 3.471e-04  Data: 0.010 (0.012)
Train: 362 [1050/1251 ( 84%)]  Loss: 3.770 (3.57)  Time: 0.676s, 1513.73/s  (0.696s, 1471.97/s)  LR: 3.471e-04  Data: 0.010 (0.012)
Train: 362 [1100/1251 ( 88%)]  Loss: 3.487 (3.57)  Time: 0.666s, 1538.58/s  (0.695s, 1472.44/s)  LR: 3.471e-04  Data: 0.010 (0.012)
Train: 362 [1150/1251 ( 92%)]  Loss: 4.057 (3.59)  Time: 0.668s, 1532.06/s  (0.695s, 1472.46/s)  LR: 3.471e-04  Data: 0.012 (0.012)
Train: 362 [1200/1251 ( 96%)]  Loss: 3.320 (3.58)  Time: 0.680s, 1505.18/s  (0.695s, 1472.84/s)  LR: 3.471e-04  Data: 0.013 (0.012)
Train: 362 [1250/1251 (100%)]  Loss: 3.119 (3.56)  Time: 0.698s, 1466.83/s  (0.695s, 1473.39/s)  LR: 3.471e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.481 (1.481)  Loss:  0.7563 (0.7563)  Acc@1: 89.7461 (89.7461)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.136 (0.586)  Loss:  0.9185 (1.3489)  Acc@1: 84.9057 (74.8520)  Acc@5: 96.4623 (92.3680)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-350.pth.tar', 74.92400008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-360.pth.tar', 74.8940000366211)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-362.pth.tar', 74.85200003662109)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-352.pth.tar', 74.82400001220704)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-345.pth.tar', 74.81800001220704)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-359.pth.tar', 74.78600006347656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-348.pth.tar', 74.7060001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-355.pth.tar', 74.6800000390625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-351.pth.tar', 74.67999990722656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-346.pth.tar', 74.58599998779297)

Train: 363 [   0/1251 (  0%)]  Loss: 3.676 (3.68)  Time: 2.346s,  436.56/s  (2.346s,  436.56/s)  LR: 3.447e-04  Data: 1.712 (1.712)
Train: 363 [  50/1251 (  4%)]  Loss: 3.622 (3.65)  Time: 0.682s, 1501.61/s  (0.740s, 1383.53/s)  LR: 3.447e-04  Data: 0.010 (0.057)
Train: 363 [ 100/1251 (  8%)]  Loss: 3.719 (3.67)  Time: 0.672s, 1523.85/s  (0.715s, 1432.32/s)  LR: 3.447e-04  Data: 0.011 (0.034)
Train: 363 [ 150/1251 ( 12%)]  Loss: 3.474 (3.62)  Time: 0.672s, 1524.51/s  (0.706s, 1449.96/s)  LR: 3.447e-04  Data: 0.010 (0.026)
Train: 363 [ 200/1251 ( 16%)]  Loss: 3.307 (3.56)  Time: 0.689s, 1485.75/s  (0.704s, 1455.26/s)  LR: 3.447e-04  Data: 0.012 (0.023)
Train: 363 [ 250/1251 ( 20%)]  Loss: 3.292 (3.52)  Time: 0.722s, 1417.71/s  (0.702s, 1457.82/s)  LR: 3.447e-04  Data: 0.010 (0.020)
Train: 363 [ 300/1251 ( 24%)]  Loss: 2.929 (3.43)  Time: 0.668s, 1533.05/s  (0.701s, 1461.35/s)  LR: 3.447e-04  Data: 0.010 (0.019)
Train: 363 [ 350/1251 ( 28%)]  Loss: 3.693 (3.46)  Time: 0.680s, 1504.99/s  (0.699s, 1464.45/s)  LR: 3.447e-04  Data: 0.009 (0.018)
Train: 363 [ 400/1251 ( 32%)]  Loss: 3.601 (3.48)  Time: 0.719s, 1423.52/s  (0.698s, 1466.48/s)  LR: 3.447e-04  Data: 0.010 (0.017)
Train: 363 [ 450/1251 ( 36%)]  Loss: 3.788 (3.51)  Time: 0.669s, 1531.17/s  (0.698s, 1468.00/s)  LR: 3.447e-04  Data: 0.010 (0.016)
Train: 363 [ 500/1251 ( 40%)]  Loss: 3.563 (3.51)  Time: 0.753s, 1360.75/s  (0.697s, 1468.79/s)  LR: 3.447e-04  Data: 0.009 (0.015)
Train: 363 [ 550/1251 ( 44%)]  Loss: 3.719 (3.53)  Time: 0.711s, 1441.02/s  (0.699s, 1465.93/s)  LR: 3.447e-04  Data: 0.009 (0.015)
Train: 363 [ 600/1251 ( 48%)]  Loss: 3.592 (3.54)  Time: 0.676s, 1514.43/s  (0.698s, 1466.89/s)  LR: 3.447e-04  Data: 0.008 (0.015)
Train: 363 [ 650/1251 ( 52%)]  Loss: 3.437 (3.53)  Time: 0.673s, 1521.33/s  (0.697s, 1468.60/s)  LR: 3.447e-04  Data: 0.010 (0.014)
Train: 363 [ 700/1251 ( 56%)]  Loss: 3.484 (3.53)  Time: 0.748s, 1369.89/s  (0.697s, 1468.72/s)  LR: 3.447e-04  Data: 0.011 (0.014)
Train: 363 [ 750/1251 ( 60%)]  Loss: 3.314 (3.51)  Time: 0.763s, 1341.80/s  (0.697s, 1468.89/s)  LR: 3.447e-04  Data: 0.009 (0.014)
Train: 363 [ 800/1251 ( 64%)]  Loss: 3.681 (3.52)  Time: 0.703s, 1457.29/s  (0.697s, 1469.72/s)  LR: 3.447e-04  Data: 0.011 (0.014)
Train: 363 [ 850/1251 ( 68%)]  Loss: 3.537 (3.52)  Time: 0.699s, 1464.87/s  (0.697s, 1470.08/s)  LR: 3.447e-04  Data: 0.009 (0.013)
Train: 363 [ 900/1251 ( 72%)]  Loss: 3.554 (3.53)  Time: 0.674s, 1518.17/s  (0.696s, 1470.26/s)  LR: 3.447e-04  Data: 0.011 (0.013)
Train: 363 [ 950/1251 ( 76%)]  Loss: 3.242 (3.51)  Time: 0.671s, 1525.02/s  (0.696s, 1470.81/s)  LR: 3.447e-04  Data: 0.011 (0.013)
Train: 363 [1000/1251 ( 80%)]  Loss: 3.667 (3.52)  Time: 0.690s, 1483.08/s  (0.696s, 1471.34/s)  LR: 3.447e-04  Data: 0.009 (0.013)
Train: 363 [1050/1251 ( 84%)]  Loss: 3.198 (3.50)  Time: 0.671s, 1526.64/s  (0.696s, 1471.25/s)  LR: 3.447e-04  Data: 0.010 (0.013)
Train: 363 [1100/1251 ( 88%)]  Loss: 3.529 (3.51)  Time: 0.763s, 1342.47/s  (0.696s, 1471.28/s)  LR: 3.447e-04  Data: 0.009 (0.013)
Train: 363 [1150/1251 ( 92%)]  Loss: 3.244 (3.49)  Time: 0.703s, 1457.17/s  (0.696s, 1471.12/s)  LR: 3.447e-04  Data: 0.017 (0.013)
Train: 363 [1200/1251 ( 96%)]  Loss: 3.905 (3.51)  Time: 0.677s, 1513.31/s  (0.696s, 1471.69/s)  LR: 3.447e-04  Data: 0.009 (0.013)
Train: 363 [1250/1251 (100%)]  Loss: 3.495 (3.51)  Time: 0.659s, 1552.87/s  (0.696s, 1472.02/s)  LR: 3.447e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.587 (1.587)  Loss:  0.8677 (0.8677)  Acc@1: 89.6484 (89.6484)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.136 (0.590)  Loss:  0.9902 (1.3810)  Acc@1: 85.3774 (74.8860)  Acc@5: 96.3443 (92.3280)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-350.pth.tar', 74.92400008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-360.pth.tar', 74.8940000366211)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-363.pth.tar', 74.88600011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-362.pth.tar', 74.85200003662109)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-352.pth.tar', 74.82400001220704)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-345.pth.tar', 74.81800001220704)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-359.pth.tar', 74.78600006347656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-348.pth.tar', 74.7060001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-355.pth.tar', 74.6800000390625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-351.pth.tar', 74.67999990722656)

Train: 364 [   0/1251 (  0%)]  Loss: 3.633 (3.63)  Time: 2.173s,  471.19/s  (2.173s,  471.19/s)  LR: 3.422e-04  Data: 1.557 (1.557)
Train: 364 [  50/1251 (  4%)]  Loss: 3.686 (3.66)  Time: 0.676s, 1515.89/s  (0.727s, 1408.78/s)  LR: 3.422e-04  Data: 0.010 (0.049)
Train: 364 [ 100/1251 (  8%)]  Loss: 3.561 (3.63)  Time: 0.703s, 1457.45/s  (0.710s, 1441.79/s)  LR: 3.422e-04  Data: 0.012 (0.030)
Train: 364 [ 150/1251 ( 12%)]  Loss: 3.347 (3.56)  Time: 0.696s, 1470.75/s  (0.704s, 1453.88/s)  LR: 3.422e-04  Data: 0.015 (0.023)
Train: 364 [ 200/1251 ( 16%)]  Loss: 3.715 (3.59)  Time: 0.676s, 1514.89/s  (0.702s, 1459.14/s)  LR: 3.422e-04  Data: 0.011 (0.020)
Train: 364 [ 250/1251 ( 20%)]  Loss: 3.345 (3.55)  Time: 0.673s, 1520.89/s  (0.700s, 1462.54/s)  LR: 3.422e-04  Data: 0.009 (0.018)
Train: 364 [ 300/1251 ( 24%)]  Loss: 3.374 (3.52)  Time: 0.675s, 1516.49/s  (0.698s, 1466.80/s)  LR: 3.422e-04  Data: 0.012 (0.017)
Train: 364 [ 350/1251 ( 28%)]  Loss: 3.143 (3.48)  Time: 0.672s, 1523.41/s  (0.698s, 1467.97/s)  LR: 3.422e-04  Data: 0.011 (0.016)
Train: 364 [ 400/1251 ( 32%)]  Loss: 3.726 (3.50)  Time: 0.702s, 1458.86/s  (0.696s, 1470.89/s)  LR: 3.422e-04  Data: 0.009 (0.015)
Train: 364 [ 450/1251 ( 36%)]  Loss: 3.626 (3.52)  Time: 0.672s, 1524.40/s  (0.695s, 1473.39/s)  LR: 3.422e-04  Data: 0.011 (0.015)
Train: 364 [ 500/1251 ( 40%)]  Loss: 3.589 (3.52)  Time: 0.715s, 1431.63/s  (0.694s, 1474.98/s)  LR: 3.422e-04  Data: 0.011 (0.014)
Train: 364 [ 550/1251 ( 44%)]  Loss: 3.158 (3.49)  Time: 0.720s, 1423.02/s  (0.694s, 1476.12/s)  LR: 3.422e-04  Data: 0.011 (0.014)
Train: 364 [ 600/1251 ( 48%)]  Loss: 3.671 (3.51)  Time: 0.679s, 1509.04/s  (0.694s, 1475.92/s)  LR: 3.422e-04  Data: 0.011 (0.014)
Train: 364 [ 650/1251 ( 52%)]  Loss: 3.803 (3.53)  Time: 0.671s, 1526.95/s  (0.694s, 1475.13/s)  LR: 3.422e-04  Data: 0.011 (0.013)
Train: 364 [ 700/1251 ( 56%)]  Loss: 3.699 (3.54)  Time: 0.715s, 1431.85/s  (0.694s, 1475.40/s)  LR: 3.422e-04  Data: 0.011 (0.013)
Train: 364 [ 750/1251 ( 60%)]  Loss: 3.694 (3.55)  Time: 0.673s, 1521.35/s  (0.694s, 1476.30/s)  LR: 3.422e-04  Data: 0.011 (0.013)
Train: 364 [ 800/1251 ( 64%)]  Loss: 3.692 (3.56)  Time: 0.720s, 1422.98/s  (0.694s, 1476.52/s)  LR: 3.422e-04  Data: 0.012 (0.013)
Train: 364 [ 850/1251 ( 68%)]  Loss: 3.701 (3.56)  Time: 0.728s, 1407.32/s  (0.694s, 1475.56/s)  LR: 3.422e-04  Data: 0.011 (0.013)
Train: 364 [ 900/1251 ( 72%)]  Loss: 3.425 (3.56)  Time: 0.702s, 1457.81/s  (0.694s, 1476.12/s)  LR: 3.422e-04  Data: 0.010 (0.013)
Train: 364 [ 950/1251 ( 76%)]  Loss: 3.722 (3.57)  Time: 0.732s, 1399.27/s  (0.694s, 1476.34/s)  LR: 3.422e-04  Data: 0.010 (0.012)
Train: 364 [1000/1251 ( 80%)]  Loss: 3.741 (3.57)  Time: 0.715s, 1432.61/s  (0.694s, 1474.83/s)  LR: 3.422e-04  Data: 0.014 (0.012)
Train: 364 [1050/1251 ( 84%)]  Loss: 3.602 (3.58)  Time: 0.709s, 1443.47/s  (0.694s, 1475.61/s)  LR: 3.422e-04  Data: 0.009 (0.012)
Train: 364 [1100/1251 ( 88%)]  Loss: 3.351 (3.57)  Time: 0.722s, 1418.29/s  (0.694s, 1475.80/s)  LR: 3.422e-04  Data: 0.014 (0.012)
Train: 364 [1150/1251 ( 92%)]  Loss: 3.244 (3.55)  Time: 0.729s, 1404.07/s  (0.694s, 1475.31/s)  LR: 3.422e-04  Data: 0.011 (0.012)
Train: 364 [1200/1251 ( 96%)]  Loss: 4.002 (3.57)  Time: 0.707s, 1448.13/s  (0.694s, 1475.02/s)  LR: 3.422e-04  Data: 0.011 (0.012)
Train: 364 [1250/1251 (100%)]  Loss: 3.282 (3.56)  Time: 0.656s, 1560.68/s  (0.694s, 1475.47/s)  LR: 3.422e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.618 (1.618)  Loss:  0.8213 (0.8213)  Acc@1: 89.3555 (89.3555)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.136 (0.587)  Loss:  0.8984 (1.3323)  Acc@1: 84.1981 (74.9580)  Acc@5: 96.6981 (92.3620)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-364.pth.tar', 74.9580001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-350.pth.tar', 74.92400008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-360.pth.tar', 74.8940000366211)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-363.pth.tar', 74.88600011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-362.pth.tar', 74.85200003662109)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-352.pth.tar', 74.82400001220704)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-345.pth.tar', 74.81800001220704)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-359.pth.tar', 74.78600006347656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-348.pth.tar', 74.7060001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-355.pth.tar', 74.6800000390625)

Train: 365 [   0/1251 (  0%)]  Loss: 3.729 (3.73)  Time: 2.340s,  437.64/s  (2.340s,  437.64/s)  LR: 3.398e-04  Data: 1.723 (1.723)
Train: 365 [  50/1251 (  4%)]  Loss: 3.726 (3.73)  Time: 0.707s, 1448.82/s  (0.727s, 1407.57/s)  LR: 3.398e-04  Data: 0.011 (0.046)
Train: 365 [ 100/1251 (  8%)]  Loss: 3.799 (3.75)  Time: 0.671s, 1527.00/s  (0.710s, 1443.27/s)  LR: 3.398e-04  Data: 0.009 (0.028)
Train: 365 [ 150/1251 ( 12%)]  Loss: 3.583 (3.71)  Time: 0.739s, 1386.21/s  (0.704s, 1454.66/s)  LR: 3.398e-04  Data: 0.009 (0.023)
Train: 365 [ 200/1251 ( 16%)]  Loss: 3.338 (3.63)  Time: 0.703s, 1456.20/s  (0.703s, 1456.54/s)  LR: 3.398e-04  Data: 0.009 (0.020)
Train: 365 [ 250/1251 ( 20%)]  Loss: 3.326 (3.58)  Time: 0.678s, 1510.12/s  (0.700s, 1462.68/s)  LR: 3.398e-04  Data: 0.019 (0.018)
Train: 365 [ 300/1251 ( 24%)]  Loss: 3.406 (3.56)  Time: 0.713s, 1436.97/s  (0.701s, 1461.41/s)  LR: 3.398e-04  Data: 0.013 (0.017)
Train: 365 [ 350/1251 ( 28%)]  Loss: 3.668 (3.57)  Time: 0.673s, 1521.75/s  (0.699s, 1464.73/s)  LR: 3.398e-04  Data: 0.010 (0.016)
Train: 365 [ 400/1251 ( 32%)]  Loss: 3.272 (3.54)  Time: 0.668s, 1532.02/s  (0.699s, 1465.94/s)  LR: 3.398e-04  Data: 0.009 (0.015)
Train: 365 [ 450/1251 ( 36%)]  Loss: 3.387 (3.52)  Time: 0.726s, 1410.68/s  (0.698s, 1467.50/s)  LR: 3.398e-04  Data: 0.009 (0.015)
Train: 365 [ 500/1251 ( 40%)]  Loss: 3.854 (3.55)  Time: 0.670s, 1528.14/s  (0.696s, 1470.69/s)  LR: 3.398e-04  Data: 0.010 (0.014)
Train: 365 [ 550/1251 ( 44%)]  Loss: 3.650 (3.56)  Time: 0.710s, 1441.94/s  (0.696s, 1470.64/s)  LR: 3.398e-04  Data: 0.010 (0.014)
Train: 365 [ 600/1251 ( 48%)]  Loss: 3.512 (3.56)  Time: 0.701s, 1460.79/s  (0.696s, 1470.97/s)  LR: 3.398e-04  Data: 0.009 (0.014)
Train: 365 [ 650/1251 ( 52%)]  Loss: 3.509 (3.55)  Time: 0.705s, 1451.73/s  (0.696s, 1471.57/s)  LR: 3.398e-04  Data: 0.010 (0.013)
Train: 365 [ 700/1251 ( 56%)]  Loss: 3.504 (3.55)  Time: 0.710s, 1442.64/s  (0.696s, 1471.98/s)  LR: 3.398e-04  Data: 0.010 (0.013)
Train: 365 [ 750/1251 ( 60%)]  Loss: 3.469 (3.55)  Time: 0.701s, 1461.76/s  (0.695s, 1473.23/s)  LR: 3.398e-04  Data: 0.009 (0.013)
Train: 365 [ 800/1251 ( 64%)]  Loss: 3.949 (3.57)  Time: 0.665s, 1539.22/s  (0.695s, 1472.93/s)  LR: 3.398e-04  Data: 0.010 (0.013)
Train: 365 [ 850/1251 ( 68%)]  Loss: 3.726 (3.58)  Time: 0.675s, 1517.24/s  (0.694s, 1474.49/s)  LR: 3.398e-04  Data: 0.009 (0.013)
Train: 365 [ 900/1251 ( 72%)]  Loss: 3.420 (3.57)  Time: 0.673s, 1520.45/s  (0.694s, 1474.57/s)  LR: 3.398e-04  Data: 0.010 (0.013)
Train: 365 [ 950/1251 ( 76%)]  Loss: 3.588 (3.57)  Time: 0.704s, 1455.13/s  (0.694s, 1474.73/s)  LR: 3.398e-04  Data: 0.009 (0.012)
Train: 365 [1000/1251 ( 80%)]  Loss: 3.584 (3.57)  Time: 0.665s, 1539.96/s  (0.694s, 1475.45/s)  LR: 3.398e-04  Data: 0.010 (0.012)
Train: 365 [1050/1251 ( 84%)]  Loss: 3.303 (3.56)  Time: 0.689s, 1485.53/s  (0.694s, 1475.77/s)  LR: 3.398e-04  Data: 0.010 (0.012)
Train: 365 [1100/1251 ( 88%)]  Loss: 3.602 (3.56)  Time: 0.694s, 1474.89/s  (0.694s, 1475.74/s)  LR: 3.398e-04  Data: 0.009 (0.012)
Train: 365 [1150/1251 ( 92%)]  Loss: 3.904 (3.58)  Time: 0.670s, 1528.17/s  (0.694s, 1476.14/s)  LR: 3.398e-04  Data: 0.010 (0.012)
Train: 365 [1200/1251 ( 96%)]  Loss: 3.233 (3.56)  Time: 0.676s, 1515.65/s  (0.694s, 1476.05/s)  LR: 3.398e-04  Data: 0.009 (0.012)
Train: 365 [1250/1251 (100%)]  Loss: 3.758 (3.57)  Time: 0.655s, 1563.62/s  (0.694s, 1476.31/s)  LR: 3.398e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.484 (1.484)  Loss:  0.9438 (0.9438)  Acc@1: 89.1602 (89.1602)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.136 (0.587)  Loss:  1.0479 (1.4924)  Acc@1: 83.4906 (74.8360)  Acc@5: 95.9906 (92.3340)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-364.pth.tar', 74.9580001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-350.pth.tar', 74.92400008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-360.pth.tar', 74.8940000366211)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-363.pth.tar', 74.88600011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-362.pth.tar', 74.85200003662109)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-365.pth.tar', 74.83600006835937)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-352.pth.tar', 74.82400001220704)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-345.pth.tar', 74.81800001220704)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-359.pth.tar', 74.78600006347656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-348.pth.tar', 74.7060001171875)

Train: 366 [   0/1251 (  0%)]  Loss: 3.276 (3.28)  Time: 2.238s,  457.54/s  (2.238s,  457.54/s)  LR: 3.373e-04  Data: 1.623 (1.623)
Train: 366 [  50/1251 (  4%)]  Loss: 3.568 (3.42)  Time: 0.719s, 1425.00/s  (0.736s, 1390.58/s)  LR: 3.373e-04  Data: 0.009 (0.049)
Train: 366 [ 100/1251 (  8%)]  Loss: 3.309 (3.38)  Time: 0.674s, 1519.82/s  (0.715s, 1432.56/s)  LR: 3.373e-04  Data: 0.011 (0.030)
Train: 366 [ 150/1251 ( 12%)]  Loss: 3.638 (3.45)  Time: 0.675s, 1516.38/s  (0.709s, 1445.05/s)  LR: 3.373e-04  Data: 0.011 (0.024)
Train: 366 [ 200/1251 ( 16%)]  Loss: 3.510 (3.46)  Time: 0.672s, 1524.76/s  (0.703s, 1456.48/s)  LR: 3.373e-04  Data: 0.011 (0.020)
Train: 366 [ 250/1251 ( 20%)]  Loss: 3.618 (3.49)  Time: 0.674s, 1520.41/s  (0.702s, 1459.20/s)  LR: 3.373e-04  Data: 0.011 (0.018)
Train: 366 [ 300/1251 ( 24%)]  Loss: 3.813 (3.53)  Time: 0.708s, 1445.80/s  (0.699s, 1464.41/s)  LR: 3.373e-04  Data: 0.010 (0.017)
Train: 366 [ 350/1251 ( 28%)]  Loss: 3.203 (3.49)  Time: 0.688s, 1488.51/s  (0.700s, 1463.59/s)  LR: 3.373e-04  Data: 0.014 (0.016)
Train: 366 [ 400/1251 ( 32%)]  Loss: 3.586 (3.50)  Time: 0.673s, 1520.85/s  (0.699s, 1464.53/s)  LR: 3.373e-04  Data: 0.009 (0.015)
Train: 366 [ 450/1251 ( 36%)]  Loss: 3.522 (3.50)  Time: 0.695s, 1473.23/s  (0.698s, 1466.01/s)  LR: 3.373e-04  Data: 0.012 (0.015)
Train: 366 [ 500/1251 ( 40%)]  Loss: 3.609 (3.51)  Time: 0.668s, 1533.83/s  (0.698s, 1467.91/s)  LR: 3.373e-04  Data: 0.009 (0.014)
Train: 366 [ 550/1251 ( 44%)]  Loss: 3.674 (3.53)  Time: 0.671s, 1525.42/s  (0.697s, 1469.58/s)  LR: 3.373e-04  Data: 0.011 (0.014)
Train: 366 [ 600/1251 ( 48%)]  Loss: 3.953 (3.56)  Time: 0.672s, 1522.75/s  (0.697s, 1469.80/s)  LR: 3.373e-04  Data: 0.011 (0.014)
Train: 366 [ 650/1251 ( 52%)]  Loss: 3.775 (3.58)  Time: 0.680s, 1506.24/s  (0.696s, 1470.56/s)  LR: 3.373e-04  Data: 0.009 (0.014)
Train: 366 [ 700/1251 ( 56%)]  Loss: 3.743 (3.59)  Time: 0.675s, 1517.29/s  (0.696s, 1471.76/s)  LR: 3.373e-04  Data: 0.011 (0.013)
Train: 366 [ 750/1251 ( 60%)]  Loss: 3.771 (3.60)  Time: 0.717s, 1429.16/s  (0.696s, 1471.36/s)  LR: 3.373e-04  Data: 0.014 (0.013)
Train: 366 [ 800/1251 ( 64%)]  Loss: 3.348 (3.58)  Time: 0.672s, 1522.69/s  (0.696s, 1472.18/s)  LR: 3.373e-04  Data: 0.011 (0.013)
Train: 366 [ 850/1251 ( 68%)]  Loss: 3.747 (3.59)  Time: 0.717s, 1427.42/s  (0.695s, 1473.38/s)  LR: 3.373e-04  Data: 0.011 (0.013)
Train: 366 [ 900/1251 ( 72%)]  Loss: 3.623 (3.59)  Time: 0.671s, 1525.74/s  (0.695s, 1474.05/s)  LR: 3.373e-04  Data: 0.010 (0.013)
Train: 366 [ 950/1251 ( 76%)]  Loss: 3.579 (3.59)  Time: 0.697s, 1469.17/s  (0.694s, 1475.11/s)  LR: 3.373e-04  Data: 0.009 (0.012)
Train: 366 [1000/1251 ( 80%)]  Loss: 3.236 (3.58)  Time: 0.672s, 1524.31/s  (0.694s, 1475.27/s)  LR: 3.373e-04  Data: 0.010 (0.012)
Train: 366 [1050/1251 ( 84%)]  Loss: 3.469 (3.57)  Time: 0.666s, 1538.66/s  (0.694s, 1475.53/s)  LR: 3.373e-04  Data: 0.011 (0.012)
Train: 366 [1100/1251 ( 88%)]  Loss: 3.699 (3.58)  Time: 0.737s, 1388.52/s  (0.694s, 1475.79/s)  LR: 3.373e-04  Data: 0.009 (0.012)
Train: 366 [1150/1251 ( 92%)]  Loss: 3.512 (3.57)  Time: 0.698s, 1466.65/s  (0.694s, 1476.02/s)  LR: 3.373e-04  Data: 0.013 (0.012)
Train: 366 [1200/1251 ( 96%)]  Loss: 3.394 (3.57)  Time: 0.708s, 1447.16/s  (0.694s, 1475.78/s)  LR: 3.373e-04  Data: 0.010 (0.012)
Train: 366 [1250/1251 (100%)]  Loss: 3.184 (3.55)  Time: 0.671s, 1526.67/s  (0.694s, 1476.16/s)  LR: 3.373e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.624 (1.624)  Loss:  0.8262 (0.8262)  Acc@1: 89.4531 (89.4531)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.136 (0.582)  Loss:  0.9854 (1.3281)  Acc@1: 83.2547 (74.9660)  Acc@5: 95.9906 (92.3060)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-366.pth.tar', 74.96599996582032)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-364.pth.tar', 74.9580001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-350.pth.tar', 74.92400008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-360.pth.tar', 74.8940000366211)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-363.pth.tar', 74.88600011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-362.pth.tar', 74.85200003662109)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-365.pth.tar', 74.83600006835937)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-352.pth.tar', 74.82400001220704)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-345.pth.tar', 74.81800001220704)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-359.pth.tar', 74.78600006347656)

Train: 367 [   0/1251 (  0%)]  Loss: 3.518 (3.52)  Time: 2.179s,  469.89/s  (2.179s,  469.89/s)  LR: 3.349e-04  Data: 1.564 (1.564)
Train: 367 [  50/1251 (  4%)]  Loss: 3.431 (3.47)  Time: 0.670s, 1528.32/s  (0.732s, 1398.74/s)  LR: 3.349e-04  Data: 0.010 (0.050)
Train: 367 [ 100/1251 (  8%)]  Loss: 3.393 (3.45)  Time: 0.671s, 1524.98/s  (0.711s, 1440.77/s)  LR: 3.349e-04  Data: 0.009 (0.030)
Train: 367 [ 150/1251 ( 12%)]  Loss: 3.267 (3.40)  Time: 0.673s, 1522.63/s  (0.705s, 1451.48/s)  LR: 3.349e-04  Data: 0.011 (0.024)
Train: 367 [ 200/1251 ( 16%)]  Loss: 3.156 (3.35)  Time: 0.694s, 1474.74/s  (0.704s, 1453.96/s)  LR: 3.349e-04  Data: 0.010 (0.020)
Train: 367 [ 250/1251 ( 20%)]  Loss: 3.456 (3.37)  Time: 0.671s, 1526.21/s  (0.703s, 1457.20/s)  LR: 3.349e-04  Data: 0.010 (0.019)
Train: 367 [ 300/1251 ( 24%)]  Loss: 3.476 (3.39)  Time: 0.700s, 1462.22/s  (0.702s, 1459.42/s)  LR: 3.349e-04  Data: 0.010 (0.017)
Train: 367 [ 350/1251 ( 28%)]  Loss: 3.514 (3.40)  Time: 0.692s, 1478.70/s  (0.701s, 1460.33/s)  LR: 3.349e-04  Data: 0.010 (0.016)
Train: 367 [ 400/1251 ( 32%)]  Loss: 3.493 (3.41)  Time: 0.688s, 1489.13/s  (0.699s, 1464.06/s)  LR: 3.349e-04  Data: 0.010 (0.016)
Train: 367 [ 450/1251 ( 36%)]  Loss: 3.324 (3.40)  Time: 0.718s, 1425.58/s  (0.699s, 1465.66/s)  LR: 3.349e-04  Data: 0.010 (0.015)
Train: 367 [ 500/1251 ( 40%)]  Loss: 3.696 (3.43)  Time: 0.683s, 1498.85/s  (0.698s, 1467.76/s)  LR: 3.349e-04  Data: 0.009 (0.015)
Train: 367 [ 550/1251 ( 44%)]  Loss: 3.339 (3.42)  Time: 0.692s, 1480.40/s  (0.698s, 1467.70/s)  LR: 3.349e-04  Data: 0.010 (0.014)
Train: 367 [ 600/1251 ( 48%)]  Loss: 3.587 (3.43)  Time: 0.740s, 1383.57/s  (0.698s, 1467.08/s)  LR: 3.349e-04  Data: 0.009 (0.014)
Train: 367 [ 650/1251 ( 52%)]  Loss: 3.665 (3.45)  Time: 0.671s, 1526.31/s  (0.698s, 1467.12/s)  LR: 3.349e-04  Data: 0.011 (0.014)
Train: 367 [ 700/1251 ( 56%)]  Loss: 3.473 (3.45)  Time: 0.671s, 1526.21/s  (0.697s, 1468.51/s)  LR: 3.349e-04  Data: 0.010 (0.013)
Train: 367 [ 750/1251 ( 60%)]  Loss: 3.705 (3.47)  Time: 0.671s, 1526.37/s  (0.697s, 1469.81/s)  LR: 3.349e-04  Data: 0.012 (0.013)
Train: 367 [ 800/1251 ( 64%)]  Loss: 3.697 (3.48)  Time: 0.670s, 1528.15/s  (0.697s, 1470.15/s)  LR: 3.349e-04  Data: 0.010 (0.013)
Train: 367 [ 850/1251 ( 68%)]  Loss: 3.330 (3.47)  Time: 0.674s, 1519.98/s  (0.696s, 1470.91/s)  LR: 3.349e-04  Data: 0.011 (0.013)
Train: 367 [ 900/1251 ( 72%)]  Loss: 3.444 (3.47)  Time: 0.738s, 1386.73/s  (0.697s, 1469.61/s)  LR: 3.349e-04  Data: 0.011 (0.013)
Train: 367 [ 950/1251 ( 76%)]  Loss: 2.996 (3.45)  Time: 0.669s, 1529.66/s  (0.697s, 1469.76/s)  LR: 3.349e-04  Data: 0.009 (0.013)
Train: 367 [1000/1251 ( 80%)]  Loss: 3.516 (3.45)  Time: 0.671s, 1527.18/s  (0.696s, 1470.45/s)  LR: 3.349e-04  Data: 0.010 (0.013)
Train: 367 [1050/1251 ( 84%)]  Loss: 3.372 (3.45)  Time: 0.703s, 1457.11/s  (0.696s, 1470.73/s)  LR: 3.349e-04  Data: 0.009 (0.013)
Train: 367 [1100/1251 ( 88%)]  Loss: 3.469 (3.45)  Time: 0.714s, 1434.66/s  (0.696s, 1471.77/s)  LR: 3.349e-04  Data: 0.009 (0.012)
Train: 367 [1150/1251 ( 92%)]  Loss: 3.570 (3.45)  Time: 0.747s, 1371.53/s  (0.696s, 1472.18/s)  LR: 3.349e-04  Data: 0.009 (0.012)
Train: 367 [1200/1251 ( 96%)]  Loss: 3.716 (3.46)  Time: 0.703s, 1455.90/s  (0.696s, 1472.28/s)  LR: 3.349e-04  Data: 0.009 (0.012)
Train: 367 [1250/1251 (100%)]  Loss: 3.798 (3.48)  Time: 0.658s, 1556.02/s  (0.695s, 1472.44/s)  LR: 3.349e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.677 (1.677)  Loss:  0.8237 (0.8237)  Acc@1: 89.7461 (89.7461)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.136 (0.583)  Loss:  0.9136 (1.4030)  Acc@1: 83.3726 (74.5460)  Acc@5: 95.9906 (92.1780)
Train: 368 [   0/1251 (  0%)]  Loss: 3.614 (3.61)  Time: 2.248s,  455.43/s  (2.248s,  455.43/s)  LR: 3.325e-04  Data: 1.633 (1.633)
Train: 368 [  50/1251 (  4%)]  Loss: 3.475 (3.54)  Time: 0.673s, 1522.16/s  (0.728s, 1407.16/s)  LR: 3.325e-04  Data: 0.013 (0.048)
Train: 368 [ 100/1251 (  8%)]  Loss: 3.671 (3.59)  Time: 0.729s, 1405.59/s  (0.711s, 1439.39/s)  LR: 3.325e-04  Data: 0.010 (0.030)
Train: 368 [ 150/1251 ( 12%)]  Loss: 3.711 (3.62)  Time: 0.671s, 1527.07/s  (0.705s, 1452.38/s)  LR: 3.325e-04  Data: 0.012 (0.023)
Train: 368 [ 200/1251 ( 16%)]  Loss: 3.582 (3.61)  Time: 0.734s, 1395.35/s  (0.704s, 1454.08/s)  LR: 3.325e-04  Data: 0.010 (0.020)
Train: 368 [ 250/1251 ( 20%)]  Loss: 3.418 (3.58)  Time: 0.671s, 1526.47/s  (0.702s, 1458.40/s)  LR: 3.325e-04  Data: 0.010 (0.018)
Train: 368 [ 300/1251 ( 24%)]  Loss: 3.156 (3.52)  Time: 0.676s, 1515.78/s  (0.701s, 1461.79/s)  LR: 3.325e-04  Data: 0.011 (0.017)
Train: 368 [ 350/1251 ( 28%)]  Loss: 3.455 (3.51)  Time: 0.725s, 1413.04/s  (0.699s, 1464.81/s)  LR: 3.325e-04  Data: 0.010 (0.016)
Train: 368 [ 400/1251 ( 32%)]  Loss: 3.906 (3.55)  Time: 0.673s, 1521.80/s  (0.698s, 1467.87/s)  LR: 3.325e-04  Data: 0.012 (0.015)
Train: 368 [ 450/1251 ( 36%)]  Loss: 3.702 (3.57)  Time: 0.672s, 1523.72/s  (0.697s, 1469.84/s)  LR: 3.325e-04  Data: 0.011 (0.015)
Train: 368 [ 500/1251 ( 40%)]  Loss: 3.514 (3.56)  Time: 0.719s, 1424.09/s  (0.696s, 1471.62/s)  LR: 3.325e-04  Data: 0.010 (0.014)
Train: 368 [ 550/1251 ( 44%)]  Loss: 3.792 (3.58)  Time: 0.672s, 1524.88/s  (0.696s, 1472.12/s)  LR: 3.325e-04  Data: 0.012 (0.014)
Train: 368 [ 600/1251 ( 48%)]  Loss: 3.700 (3.59)  Time: 0.675s, 1516.43/s  (0.695s, 1472.58/s)  LR: 3.325e-04  Data: 0.011 (0.014)
Train: 368 [ 650/1251 ( 52%)]  Loss: 3.393 (3.58)  Time: 0.706s, 1451.31/s  (0.695s, 1472.42/s)  LR: 3.325e-04  Data: 0.011 (0.013)
Train: 368 [ 700/1251 ( 56%)]  Loss: 3.974 (3.60)  Time: 0.704s, 1454.00/s  (0.695s, 1473.77/s)  LR: 3.325e-04  Data: 0.009 (0.013)
Train: 368 [ 750/1251 ( 60%)]  Loss: 3.209 (3.58)  Time: 0.673s, 1522.67/s  (0.694s, 1475.12/s)  LR: 3.325e-04  Data: 0.011 (0.013)
Train: 368 [ 800/1251 ( 64%)]  Loss: 3.789 (3.59)  Time: 0.719s, 1423.84/s  (0.694s, 1476.00/s)  LR: 3.325e-04  Data: 0.014 (0.013)
Train: 368 [ 850/1251 ( 68%)]  Loss: 3.684 (3.60)  Time: 0.683s, 1500.30/s  (0.694s, 1476.22/s)  LR: 3.325e-04  Data: 0.010 (0.013)
Train: 368 [ 900/1251 ( 72%)]  Loss: 3.379 (3.59)  Time: 0.682s, 1502.36/s  (0.693s, 1476.83/s)  LR: 3.325e-04  Data: 0.011 (0.013)
Train: 368 [ 950/1251 ( 76%)]  Loss: 3.528 (3.58)  Time: 0.666s, 1538.68/s  (0.693s, 1476.86/s)  LR: 3.325e-04  Data: 0.010 (0.012)
Train: 368 [1000/1251 ( 80%)]  Loss: 3.434 (3.58)  Time: 0.672s, 1524.52/s  (0.694s, 1476.23/s)  LR: 3.325e-04  Data: 0.011 (0.012)
Train: 368 [1050/1251 ( 84%)]  Loss: 3.720 (3.58)  Time: 0.671s, 1525.08/s  (0.693s, 1476.99/s)  LR: 3.325e-04  Data: 0.010 (0.012)
Train: 368 [1100/1251 ( 88%)]  Loss: 3.709 (3.59)  Time: 0.671s, 1525.58/s  (0.693s, 1476.75/s)  LR: 3.325e-04  Data: 0.010 (0.012)
Train: 368 [1150/1251 ( 92%)]  Loss: 3.689 (3.59)  Time: 0.681s, 1503.04/s  (0.693s, 1477.17/s)  LR: 3.325e-04  Data: 0.009 (0.012)
Train: 368 [1200/1251 ( 96%)]  Loss: 3.253 (3.58)  Time: 0.674s, 1519.80/s  (0.693s, 1477.08/s)  LR: 3.325e-04  Data: 0.010 (0.012)
Train: 368 [1250/1251 (100%)]  Loss: 3.372 (3.57)  Time: 0.660s, 1550.52/s  (0.693s, 1477.57/s)  LR: 3.325e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.547 (1.547)  Loss:  0.9023 (0.9023)  Acc@1: 89.2578 (89.2578)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.137 (0.587)  Loss:  0.8906 (1.3461)  Acc@1: 85.6132 (74.7780)  Acc@5: 96.9340 (92.3140)
Train: 369 [   0/1251 (  0%)]  Loss: 3.320 (3.32)  Time: 2.273s,  450.55/s  (2.273s,  450.55/s)  LR: 3.300e-04  Data: 1.605 (1.605)
Train: 369 [  50/1251 (  4%)]  Loss: 3.584 (3.45)  Time: 0.718s, 1426.59/s  (0.734s, 1395.24/s)  LR: 3.300e-04  Data: 0.009 (0.048)
Train: 369 [ 100/1251 (  8%)]  Loss: 3.602 (3.50)  Time: 0.687s, 1491.04/s  (0.713s, 1435.78/s)  LR: 3.300e-04  Data: 0.010 (0.029)
Train: 369 [ 150/1251 ( 12%)]  Loss: 3.559 (3.52)  Time: 0.700s, 1462.16/s  (0.706s, 1451.31/s)  LR: 3.300e-04  Data: 0.010 (0.023)
Train: 369 [ 200/1251 ( 16%)]  Loss: 3.485 (3.51)  Time: 0.671s, 1525.88/s  (0.704s, 1454.87/s)  LR: 3.300e-04  Data: 0.011 (0.020)
Train: 369 [ 250/1251 ( 20%)]  Loss: 3.465 (3.50)  Time: 0.706s, 1450.65/s  (0.702s, 1457.85/s)  LR: 3.300e-04  Data: 0.012 (0.018)
Train: 369 [ 300/1251 ( 24%)]  Loss: 3.876 (3.56)  Time: 0.706s, 1450.38/s  (0.702s, 1457.68/s)  LR: 3.300e-04  Data: 0.011 (0.017)
Train: 369 [ 350/1251 ( 28%)]  Loss: 3.485 (3.55)  Time: 0.686s, 1493.71/s  (0.700s, 1462.35/s)  LR: 3.300e-04  Data: 0.009 (0.016)
Train: 369 [ 400/1251 ( 32%)]  Loss: 3.398 (3.53)  Time: 0.664s, 1541.25/s  (0.699s, 1465.85/s)  LR: 3.300e-04  Data: 0.009 (0.015)
Train: 369 [ 450/1251 ( 36%)]  Loss: 3.404 (3.52)  Time: 0.673s, 1521.30/s  (0.698s, 1467.60/s)  LR: 3.300e-04  Data: 0.010 (0.015)
Train: 369 [ 500/1251 ( 40%)]  Loss: 3.668 (3.53)  Time: 0.709s, 1444.46/s  (0.697s, 1468.22/s)  LR: 3.300e-04  Data: 0.013 (0.014)
Train: 369 [ 550/1251 ( 44%)]  Loss: 3.761 (3.55)  Time: 0.686s, 1493.76/s  (0.697s, 1468.62/s)  LR: 3.300e-04  Data: 0.010 (0.014)
Train: 369 [ 600/1251 ( 48%)]  Loss: 2.983 (3.51)  Time: 0.704s, 1454.28/s  (0.697s, 1469.46/s)  LR: 3.300e-04  Data: 0.010 (0.014)
Train: 369 [ 650/1251 ( 52%)]  Loss: 3.687 (3.52)  Time: 0.714s, 1433.89/s  (0.696s, 1470.61/s)  LR: 3.300e-04  Data: 0.012 (0.013)
Train: 369 [ 700/1251 ( 56%)]  Loss: 3.915 (3.55)  Time: 0.671s, 1525.02/s  (0.696s, 1471.17/s)  LR: 3.300e-04  Data: 0.010 (0.013)
Train: 369 [ 750/1251 ( 60%)]  Loss: 3.535 (3.55)  Time: 0.672s, 1523.30/s  (0.696s, 1471.76/s)  LR: 3.300e-04  Data: 0.012 (0.013)
Train: 369 [ 800/1251 ( 64%)]  Loss: 3.521 (3.54)  Time: 0.678s, 1511.21/s  (0.696s, 1471.84/s)  LR: 3.300e-04  Data: 0.013 (0.013)
Train: 369 [ 850/1251 ( 68%)]  Loss: 3.480 (3.54)  Time: 0.707s, 1447.71/s  (0.695s, 1472.77/s)  LR: 3.300e-04  Data: 0.009 (0.013)
Train: 369 [ 900/1251 ( 72%)]  Loss: 3.797 (3.55)  Time: 0.782s, 1309.68/s  (0.695s, 1472.73/s)  LR: 3.300e-04  Data: 0.010 (0.013)
Train: 369 [ 950/1251 ( 76%)]  Loss: 3.640 (3.56)  Time: 0.670s, 1529.38/s  (0.695s, 1473.65/s)  LR: 3.300e-04  Data: 0.010 (0.012)
Train: 369 [1000/1251 ( 80%)]  Loss: 3.647 (3.56)  Time: 0.700s, 1463.07/s  (0.695s, 1473.95/s)  LR: 3.300e-04  Data: 0.009 (0.012)
Train: 369 [1050/1251 ( 84%)]  Loss: 4.117 (3.59)  Time: 0.673s, 1522.54/s  (0.695s, 1474.31/s)  LR: 3.300e-04  Data: 0.011 (0.012)
Train: 369 [1100/1251 ( 88%)]  Loss: 3.558 (3.59)  Time: 0.765s, 1338.74/s  (0.695s, 1474.33/s)  LR: 3.300e-04  Data: 0.009 (0.012)
Train: 369 [1150/1251 ( 92%)]  Loss: 3.623 (3.59)  Time: 0.692s, 1479.32/s  (0.695s, 1474.14/s)  LR: 3.300e-04  Data: 0.013 (0.012)
Train: 369 [1200/1251 ( 96%)]  Loss: 3.482 (3.58)  Time: 0.695s, 1472.99/s  (0.694s, 1474.86/s)  LR: 3.300e-04  Data: 0.011 (0.012)
Train: 369 [1250/1251 (100%)]  Loss: 3.323 (3.57)  Time: 0.657s, 1557.49/s  (0.694s, 1475.66/s)  LR: 3.300e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.578 (1.578)  Loss:  0.7637 (0.7637)  Acc@1: 90.1367 (90.1367)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.136 (0.586)  Loss:  0.9829 (1.3759)  Acc@1: 85.7311 (74.7800)  Acc@5: 96.1085 (92.2120)
Train: 370 [   0/1251 (  0%)]  Loss: 3.274 (3.27)  Time: 2.118s,  483.47/s  (2.118s,  483.47/s)  LR: 3.276e-04  Data: 1.503 (1.503)
Train: 370 [  50/1251 (  4%)]  Loss: 3.514 (3.39)  Time: 0.676s, 1515.71/s  (0.740s, 1384.06/s)  LR: 3.276e-04  Data: 0.010 (0.050)
Train: 370 [ 100/1251 (  8%)]  Loss: 3.546 (3.44)  Time: 0.672s, 1523.59/s  (0.719s, 1423.44/s)  LR: 3.276e-04  Data: 0.011 (0.031)
Train: 370 [ 150/1251 ( 12%)]  Loss: 3.830 (3.54)  Time: 0.706s, 1451.20/s  (0.709s, 1445.16/s)  LR: 3.276e-04  Data: 0.010 (0.024)
Train: 370 [ 200/1251 ( 16%)]  Loss: 3.762 (3.59)  Time: 0.678s, 1510.92/s  (0.703s, 1457.52/s)  LR: 3.276e-04  Data: 0.009 (0.021)
Train: 370 [ 250/1251 ( 20%)]  Loss: 3.240 (3.53)  Time: 0.695s, 1473.33/s  (0.701s, 1460.74/s)  LR: 3.276e-04  Data: 0.011 (0.019)
Train: 370 [ 300/1251 ( 24%)]  Loss: 3.311 (3.50)  Time: 0.706s, 1451.11/s  (0.699s, 1464.69/s)  LR: 3.276e-04  Data: 0.011 (0.017)
Train: 370 [ 350/1251 ( 28%)]  Loss: 3.016 (3.44)  Time: 0.671s, 1526.03/s  (0.700s, 1463.32/s)  LR: 3.276e-04  Data: 0.010 (0.016)
Train: 370 [ 400/1251 ( 32%)]  Loss: 3.581 (3.45)  Time: 0.677s, 1511.61/s  (0.699s, 1464.46/s)  LR: 3.276e-04  Data: 0.011 (0.016)
Train: 370 [ 450/1251 ( 36%)]  Loss: 3.618 (3.47)  Time: 0.671s, 1526.72/s  (0.698s, 1467.44/s)  LR: 3.276e-04  Data: 0.010 (0.015)
Train: 370 [ 500/1251 ( 40%)]  Loss: 3.648 (3.49)  Time: 0.672s, 1523.47/s  (0.697s, 1469.46/s)  LR: 3.276e-04  Data: 0.011 (0.015)
Train: 370 [ 550/1251 ( 44%)]  Loss: 3.779 (3.51)  Time: 0.695s, 1474.09/s  (0.696s, 1470.90/s)  LR: 3.276e-04  Data: 0.009 (0.014)
Train: 370 [ 600/1251 ( 48%)]  Loss: 3.405 (3.50)  Time: 0.691s, 1481.00/s  (0.696s, 1470.34/s)  LR: 3.276e-04  Data: 0.013 (0.014)
Train: 370 [ 650/1251 ( 52%)]  Loss: 3.437 (3.50)  Time: 0.716s, 1430.76/s  (0.696s, 1470.61/s)  LR: 3.276e-04  Data: 0.011 (0.014)
Train: 370 [ 700/1251 ( 56%)]  Loss: 3.470 (3.50)  Time: 0.672s, 1522.96/s  (0.696s, 1471.77/s)  LR: 3.276e-04  Data: 0.010 (0.013)
Train: 370 [ 750/1251 ( 60%)]  Loss: 3.409 (3.49)  Time: 0.676s, 1515.61/s  (0.695s, 1473.72/s)  LR: 3.276e-04  Data: 0.012 (0.013)
Train: 370 [ 800/1251 ( 64%)]  Loss: 3.795 (3.51)  Time: 0.676s, 1514.91/s  (0.695s, 1473.73/s)  LR: 3.276e-04  Data: 0.011 (0.013)
Train: 370 [ 850/1251 ( 68%)]  Loss: 3.528 (3.51)  Time: 0.724s, 1414.00/s  (0.695s, 1473.30/s)  LR: 3.276e-04  Data: 0.011 (0.013)
Train: 370 [ 900/1251 ( 72%)]  Loss: 3.184 (3.49)  Time: 0.668s, 1532.02/s  (0.695s, 1473.63/s)  LR: 3.276e-04  Data: 0.009 (0.013)
Train: 370 [ 950/1251 ( 76%)]  Loss: 3.435 (3.49)  Time: 0.683s, 1499.64/s  (0.695s, 1474.05/s)  LR: 3.276e-04  Data: 0.009 (0.013)
Train: 370 [1000/1251 ( 80%)]  Loss: 3.832 (3.51)  Time: 0.705s, 1451.87/s  (0.694s, 1474.83/s)  LR: 3.276e-04  Data: 0.010 (0.013)
Train: 370 [1050/1251 ( 84%)]  Loss: 3.753 (3.52)  Time: 0.676s, 1514.96/s  (0.694s, 1475.73/s)  LR: 3.276e-04  Data: 0.009 (0.012)
Train: 370 [1100/1251 ( 88%)]  Loss: 3.568 (3.52)  Time: 0.708s, 1445.80/s  (0.694s, 1474.75/s)  LR: 3.276e-04  Data: 0.010 (0.012)
Train: 370 [1150/1251 ( 92%)]  Loss: 3.443 (3.52)  Time: 0.702s, 1458.98/s  (0.694s, 1475.19/s)  LR: 3.276e-04  Data: 0.011 (0.012)
Train: 370 [1200/1251 ( 96%)]  Loss: 3.573 (3.52)  Time: 0.726s, 1410.87/s  (0.694s, 1475.02/s)  LR: 3.276e-04  Data: 0.009 (0.012)
Train: 370 [1250/1251 (100%)]  Loss: 3.181 (3.51)  Time: 0.658s, 1557.19/s  (0.694s, 1475.85/s)  LR: 3.276e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.522 (1.522)  Loss:  0.7515 (0.7515)  Acc@1: 90.2344 (90.2344)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.136 (0.584)  Loss:  0.8877 (1.2930)  Acc@1: 85.4953 (75.0360)  Acc@5: 96.2264 (92.5620)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-370.pth.tar', 75.0360000341797)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-366.pth.tar', 74.96599996582032)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-364.pth.tar', 74.9580001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-350.pth.tar', 74.92400008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-360.pth.tar', 74.8940000366211)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-363.pth.tar', 74.88600011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-362.pth.tar', 74.85200003662109)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-365.pth.tar', 74.83600006835937)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-352.pth.tar', 74.82400001220704)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-345.pth.tar', 74.81800001220704)

Train: 371 [   0/1251 (  0%)]  Loss: 3.800 (3.80)  Time: 2.241s,  456.90/s  (2.241s,  456.90/s)  LR: 3.252e-04  Data: 1.626 (1.626)
Train: 371 [  50/1251 (  4%)]  Loss: 3.049 (3.42)  Time: 0.671s, 1525.94/s  (0.733s, 1396.59/s)  LR: 3.252e-04  Data: 0.010 (0.050)
Train: 371 [ 100/1251 (  8%)]  Loss: 3.675 (3.51)  Time: 0.672s, 1524.31/s  (0.711s, 1439.69/s)  LR: 3.252e-04  Data: 0.012 (0.031)
Train: 371 [ 150/1251 ( 12%)]  Loss: 3.854 (3.59)  Time: 0.673s, 1521.84/s  (0.703s, 1456.41/s)  LR: 3.252e-04  Data: 0.011 (0.024)
Train: 371 [ 200/1251 ( 16%)]  Loss: 3.488 (3.57)  Time: 0.672s, 1523.74/s  (0.701s, 1461.66/s)  LR: 3.252e-04  Data: 0.011 (0.021)
Train: 371 [ 250/1251 ( 20%)]  Loss: 3.598 (3.58)  Time: 0.704s, 1454.61/s  (0.699s, 1465.65/s)  LR: 3.252e-04  Data: 0.009 (0.019)
Train: 371 [ 300/1251 ( 24%)]  Loss: 3.307 (3.54)  Time: 0.712s, 1437.90/s  (0.698s, 1466.90/s)  LR: 3.252e-04  Data: 0.009 (0.017)
Train: 371 [ 350/1251 ( 28%)]  Loss: 3.419 (3.52)  Time: 0.682s, 1502.31/s  (0.697s, 1468.36/s)  LR: 3.252e-04  Data: 0.011 (0.016)
Train: 371 [ 400/1251 ( 32%)]  Loss: 3.662 (3.54)  Time: 0.707s, 1447.85/s  (0.697s, 1469.98/s)  LR: 3.252e-04  Data: 0.011 (0.016)
Train: 371 [ 450/1251 ( 36%)]  Loss: 3.806 (3.57)  Time: 0.755s, 1355.41/s  (0.696s, 1470.69/s)  LR: 3.252e-04  Data: 0.009 (0.015)
Train: 371 [ 500/1251 ( 40%)]  Loss: 3.619 (3.57)  Time: 0.673s, 1521.07/s  (0.695s, 1472.59/s)  LR: 3.252e-04  Data: 0.012 (0.015)
Train: 371 [ 550/1251 ( 44%)]  Loss: 3.066 (3.53)  Time: 0.682s, 1500.48/s  (0.695s, 1473.09/s)  LR: 3.252e-04  Data: 0.009 (0.014)
Train: 371 [ 600/1251 ( 48%)]  Loss: 3.567 (3.53)  Time: 0.673s, 1520.62/s  (0.695s, 1473.99/s)  LR: 3.252e-04  Data: 0.011 (0.014)
Train: 371 [ 650/1251 ( 52%)]  Loss: 3.431 (3.52)  Time: 0.729s, 1403.89/s  (0.694s, 1475.39/s)  LR: 3.252e-04  Data: 0.013 (0.014)
Train: 371 [ 700/1251 ( 56%)]  Loss: 3.476 (3.52)  Time: 0.722s, 1418.82/s  (0.695s, 1472.42/s)  LR: 3.252e-04  Data: 0.009 (0.014)
Train: 371 [ 750/1251 ( 60%)]  Loss: 2.933 (3.48)  Time: 0.674s, 1519.13/s  (0.697s, 1470.07/s)  LR: 3.252e-04  Data: 0.009 (0.013)
Train: 371 [ 800/1251 ( 64%)]  Loss: 3.754 (3.50)  Time: 0.679s, 1508.14/s  (0.697s, 1468.13/s)  LR: 3.252e-04  Data: 0.013 (0.013)
Train: 371 [ 850/1251 ( 68%)]  Loss: 3.012 (3.47)  Time: 0.671s, 1525.86/s  (0.697s, 1468.84/s)  LR: 3.252e-04  Data: 0.010 (0.013)
Train: 371 [ 900/1251 ( 72%)]  Loss: 3.811 (3.49)  Time: 0.670s, 1527.56/s  (0.696s, 1471.07/s)  LR: 3.252e-04  Data: 0.009 (0.013)
Train: 371 [ 950/1251 ( 76%)]  Loss: 3.897 (3.51)  Time: 0.672s, 1524.42/s  (0.695s, 1473.18/s)  LR: 3.252e-04  Data: 0.010 (0.013)
Train: 371 [1000/1251 ( 80%)]  Loss: 3.900 (3.53)  Time: 0.690s, 1484.49/s  (0.695s, 1474.42/s)  LR: 3.252e-04  Data: 0.010 (0.013)
Train: 371 [1050/1251 ( 84%)]  Loss: 3.688 (3.54)  Time: 0.689s, 1486.25/s  (0.694s, 1474.94/s)  LR: 3.252e-04  Data: 0.010 (0.013)
Train: 371 [1100/1251 ( 88%)]  Loss: 3.789 (3.55)  Time: 0.677s, 1512.02/s  (0.694s, 1474.70/s)  LR: 3.252e-04  Data: 0.013 (0.013)
Train: 371 [1150/1251 ( 92%)]  Loss: 3.647 (3.55)  Time: 0.700s, 1463.39/s  (0.694s, 1475.00/s)  LR: 3.252e-04  Data: 0.009 (0.012)
Train: 371 [1200/1251 ( 96%)]  Loss: 3.576 (3.55)  Time: 0.709s, 1444.99/s  (0.694s, 1475.55/s)  LR: 3.252e-04  Data: 0.009 (0.012)
Train: 371 [1250/1251 (100%)]  Loss: 4.006 (3.57)  Time: 0.686s, 1492.25/s  (0.694s, 1476.20/s)  LR: 3.252e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.611 (1.611)  Loss:  0.7510 (0.7510)  Acc@1: 89.7461 (89.7461)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.136 (0.581)  Loss:  0.8481 (1.2969)  Acc@1: 84.5519 (75.1060)  Acc@5: 96.6981 (92.4540)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-371.pth.tar', 75.10600014160157)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-370.pth.tar', 75.0360000341797)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-366.pth.tar', 74.96599996582032)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-364.pth.tar', 74.9580001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-350.pth.tar', 74.92400008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-360.pth.tar', 74.8940000366211)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-363.pth.tar', 74.88600011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-362.pth.tar', 74.85200003662109)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-365.pth.tar', 74.83600006835937)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-352.pth.tar', 74.82400001220704)

Train: 372 [   0/1251 (  0%)]  Loss: 3.433 (3.43)  Time: 2.323s,  440.83/s  (2.323s,  440.83/s)  LR: 3.228e-04  Data: 1.669 (1.669)
Train: 372 [  50/1251 (  4%)]  Loss: 3.502 (3.47)  Time: 0.690s, 1484.17/s  (0.724s, 1415.25/s)  LR: 3.228e-04  Data: 0.011 (0.050)
Train: 372 [ 100/1251 (  8%)]  Loss: 3.542 (3.49)  Time: 0.698s, 1467.10/s  (0.712s, 1437.36/s)  LR: 3.228e-04  Data: 0.011 (0.030)
Train: 372 [ 150/1251 ( 12%)]  Loss: 3.510 (3.50)  Time: 0.707s, 1448.58/s  (0.704s, 1454.76/s)  LR: 3.228e-04  Data: 0.012 (0.024)
Train: 372 [ 200/1251 ( 16%)]  Loss: 3.407 (3.48)  Time: 0.678s, 1510.30/s  (0.701s, 1461.54/s)  LR: 3.228e-04  Data: 0.011 (0.020)
Train: 372 [ 250/1251 ( 20%)]  Loss: 3.447 (3.47)  Time: 0.673s, 1520.86/s  (0.699s, 1465.33/s)  LR: 3.228e-04  Data: 0.009 (0.018)
Train: 372 [ 300/1251 ( 24%)]  Loss: 3.291 (3.45)  Time: 0.679s, 1508.44/s  (0.697s, 1468.76/s)  LR: 3.228e-04  Data: 0.013 (0.017)
Train: 372 [ 350/1251 ( 28%)]  Loss: 3.666 (3.47)  Time: 0.673s, 1521.98/s  (0.696s, 1470.35/s)  LR: 3.228e-04  Data: 0.014 (0.016)
Train: 372 [ 400/1251 ( 32%)]  Loss: 3.708 (3.50)  Time: 0.710s, 1442.99/s  (0.695s, 1472.64/s)  LR: 3.228e-04  Data: 0.010 (0.015)
Train: 372 [ 450/1251 ( 36%)]  Loss: 3.655 (3.52)  Time: 0.734s, 1395.70/s  (0.695s, 1474.00/s)  LR: 3.228e-04  Data: 0.009 (0.015)
Train: 372 [ 500/1251 ( 40%)]  Loss: 3.296 (3.50)  Time: 0.691s, 1481.79/s  (0.694s, 1475.85/s)  LR: 3.228e-04  Data: 0.010 (0.014)
Train: 372 [ 550/1251 ( 44%)]  Loss: 3.447 (3.49)  Time: 0.680s, 1506.85/s  (0.693s, 1477.02/s)  LR: 3.228e-04  Data: 0.011 (0.014)
Train: 372 [ 600/1251 ( 48%)]  Loss: 3.706 (3.51)  Time: 0.707s, 1448.27/s  (0.693s, 1477.39/s)  LR: 3.228e-04  Data: 0.011 (0.014)
Train: 372 [ 650/1251 ( 52%)]  Loss: 3.486 (3.51)  Time: 0.708s, 1445.93/s  (0.693s, 1478.17/s)  LR: 3.228e-04  Data: 0.011 (0.014)
Train: 372 [ 700/1251 ( 56%)]  Loss: 4.040 (3.54)  Time: 0.701s, 1460.77/s  (0.693s, 1477.15/s)  LR: 3.228e-04  Data: 0.009 (0.013)
Train: 372 [ 750/1251 ( 60%)]  Loss: 3.494 (3.54)  Time: 0.672s, 1522.78/s  (0.693s, 1477.59/s)  LR: 3.228e-04  Data: 0.010 (0.013)
Train: 372 [ 800/1251 ( 64%)]  Loss: 3.153 (3.52)  Time: 0.747s, 1370.06/s  (0.693s, 1476.96/s)  LR: 3.228e-04  Data: 0.009 (0.013)
Train: 372 [ 850/1251 ( 68%)]  Loss: 3.402 (3.51)  Time: 0.707s, 1447.40/s  (0.694s, 1476.25/s)  LR: 3.228e-04  Data: 0.011 (0.013)
Train: 372 [ 900/1251 ( 72%)]  Loss: 3.503 (3.51)  Time: 0.669s, 1530.42/s  (0.693s, 1477.08/s)  LR: 3.228e-04  Data: 0.011 (0.013)
Train: 372 [ 950/1251 ( 76%)]  Loss: 3.807 (3.52)  Time: 0.675s, 1517.91/s  (0.693s, 1476.87/s)  LR: 3.228e-04  Data: 0.011 (0.013)
Train: 372 [1000/1251 ( 80%)]  Loss: 3.340 (3.52)  Time: 0.711s, 1440.12/s  (0.693s, 1477.69/s)  LR: 3.228e-04  Data: 0.011 (0.012)
Train: 372 [1050/1251 ( 84%)]  Loss: 3.601 (3.52)  Time: 0.736s, 1391.45/s  (0.693s, 1478.01/s)  LR: 3.228e-04  Data: 0.015 (0.012)
Train: 372 [1100/1251 ( 88%)]  Loss: 3.478 (3.52)  Time: 0.706s, 1449.51/s  (0.693s, 1477.83/s)  LR: 3.228e-04  Data: 0.011 (0.012)
Train: 372 [1150/1251 ( 92%)]  Loss: 3.456 (3.52)  Time: 0.674s, 1519.60/s  (0.693s, 1478.20/s)  LR: 3.228e-04  Data: 0.010 (0.012)
Train: 372 [1200/1251 ( 96%)]  Loss: 3.765 (3.53)  Time: 0.707s, 1448.08/s  (0.693s, 1477.77/s)  LR: 3.228e-04  Data: 0.012 (0.012)
Train: 372 [1250/1251 (100%)]  Loss: 3.710 (3.53)  Time: 0.657s, 1557.71/s  (0.693s, 1477.53/s)  LR: 3.228e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.662 (1.662)  Loss:  0.8784 (0.8784)  Acc@1: 89.5508 (89.5508)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.136 (0.581)  Loss:  1.0371 (1.4248)  Acc@1: 84.1981 (74.7400)  Acc@5: 96.5802 (92.4120)
Train: 373 [   0/1251 (  0%)]  Loss: 3.667 (3.67)  Time: 2.202s,  465.14/s  (2.202s,  465.14/s)  LR: 3.204e-04  Data: 1.556 (1.556)
Train: 373 [  50/1251 (  4%)]  Loss: 3.257 (3.46)  Time: 0.699s, 1464.07/s  (0.736s, 1391.35/s)  LR: 3.204e-04  Data: 0.008 (0.054)
Train: 373 [ 100/1251 (  8%)]  Loss: 3.310 (3.41)  Time: 0.671s, 1525.83/s  (0.714s, 1434.59/s)  LR: 3.204e-04  Data: 0.010 (0.033)
Train: 373 [ 150/1251 ( 12%)]  Loss: 3.842 (3.52)  Time: 0.710s, 1442.50/s  (0.706s, 1450.08/s)  LR: 3.204e-04  Data: 0.011 (0.025)
Train: 373 [ 200/1251 ( 16%)]  Loss: 3.616 (3.54)  Time: 0.671s, 1525.09/s  (0.704s, 1455.55/s)  LR: 3.204e-04  Data: 0.010 (0.022)
Train: 373 [ 250/1251 ( 20%)]  Loss: 3.426 (3.52)  Time: 0.671s, 1525.53/s  (0.700s, 1462.31/s)  LR: 3.204e-04  Data: 0.010 (0.019)
Train: 373 [ 300/1251 ( 24%)]  Loss: 3.202 (3.47)  Time: 0.701s, 1459.93/s  (0.699s, 1464.86/s)  LR: 3.204e-04  Data: 0.009 (0.018)
Train: 373 [ 350/1251 ( 28%)]  Loss: 3.481 (3.48)  Time: 0.672s, 1523.75/s  (0.697s, 1468.63/s)  LR: 3.204e-04  Data: 0.009 (0.017)
Train: 373 [ 400/1251 ( 32%)]  Loss: 3.391 (3.47)  Time: 0.676s, 1515.74/s  (0.696s, 1470.77/s)  LR: 3.204e-04  Data: 0.010 (0.016)
Train: 373 [ 450/1251 ( 36%)]  Loss: 3.775 (3.50)  Time: 0.704s, 1454.68/s  (0.696s, 1471.53/s)  LR: 3.204e-04  Data: 0.009 (0.015)
Train: 373 [ 500/1251 ( 40%)]  Loss: 3.317 (3.48)  Time: 0.671s, 1525.52/s  (0.696s, 1471.35/s)  LR: 3.204e-04  Data: 0.010 (0.015)
Train: 373 [ 550/1251 ( 44%)]  Loss: 3.753 (3.50)  Time: 0.704s, 1454.31/s  (0.696s, 1471.27/s)  LR: 3.204e-04  Data: 0.010 (0.014)
Train: 373 [ 600/1251 ( 48%)]  Loss: 3.484 (3.50)  Time: 0.721s, 1420.34/s  (0.696s, 1472.23/s)  LR: 3.204e-04  Data: 0.010 (0.014)
Train: 373 [ 650/1251 ( 52%)]  Loss: 3.553 (3.51)  Time: 0.671s, 1525.41/s  (0.695s, 1473.47/s)  LR: 3.204e-04  Data: 0.010 (0.014)
Train: 373 [ 700/1251 ( 56%)]  Loss: 3.653 (3.52)  Time: 0.702s, 1458.43/s  (0.694s, 1475.33/s)  LR: 3.204e-04  Data: 0.010 (0.014)
Train: 373 [ 750/1251 ( 60%)]  Loss: 3.533 (3.52)  Time: 0.722s, 1418.29/s  (0.694s, 1475.22/s)  LR: 3.204e-04  Data: 0.010 (0.013)
Train: 373 [ 800/1251 ( 64%)]  Loss: 3.099 (3.49)  Time: 0.673s, 1520.93/s  (0.694s, 1475.84/s)  LR: 3.204e-04  Data: 0.012 (0.013)
Train: 373 [ 850/1251 ( 68%)]  Loss: 3.587 (3.50)  Time: 0.686s, 1493.77/s  (0.694s, 1475.61/s)  LR: 3.204e-04  Data: 0.010 (0.013)
Train: 373 [ 900/1251 ( 72%)]  Loss: 3.370 (3.49)  Time: 0.752s, 1362.31/s  (0.695s, 1474.16/s)  LR: 3.204e-04  Data: 0.009 (0.013)
Train: 373 [ 950/1251 ( 76%)]  Loss: 3.366 (3.48)  Time: 0.672s, 1523.62/s  (0.695s, 1474.28/s)  LR: 3.204e-04  Data: 0.012 (0.013)
Train: 373 [1000/1251 ( 80%)]  Loss: 3.668 (3.49)  Time: 0.672s, 1524.42/s  (0.694s, 1474.87/s)  LR: 3.204e-04  Data: 0.010 (0.013)
Train: 373 [1050/1251 ( 84%)]  Loss: 3.417 (3.49)  Time: 0.720s, 1423.07/s  (0.694s, 1474.79/s)  LR: 3.204e-04  Data: 0.024 (0.013)
Train: 373 [1100/1251 ( 88%)]  Loss: 3.418 (3.49)  Time: 0.672s, 1523.34/s  (0.694s, 1475.60/s)  LR: 3.204e-04  Data: 0.009 (0.012)
Train: 373 [1150/1251 ( 92%)]  Loss: 3.645 (3.49)  Time: 0.674s, 1519.94/s  (0.694s, 1476.02/s)  LR: 3.204e-04  Data: 0.009 (0.012)
Train: 373 [1200/1251 ( 96%)]  Loss: 3.482 (3.49)  Time: 0.785s, 1304.72/s  (0.694s, 1475.00/s)  LR: 3.204e-04  Data: 0.009 (0.012)
Train: 373 [1250/1251 (100%)]  Loss: 3.590 (3.50)  Time: 0.689s, 1485.58/s  (0.694s, 1475.67/s)  LR: 3.204e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.557 (1.557)  Loss:  0.7046 (0.7046)  Acc@1: 90.1367 (90.1367)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.136 (0.589)  Loss:  0.9849 (1.3436)  Acc@1: 85.3774 (74.9180)  Acc@5: 96.6981 (92.3760)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-371.pth.tar', 75.10600014160157)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-370.pth.tar', 75.0360000341797)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-366.pth.tar', 74.96599996582032)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-364.pth.tar', 74.9580001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-350.pth.tar', 74.92400008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-373.pth.tar', 74.91800011230468)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-360.pth.tar', 74.8940000366211)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-363.pth.tar', 74.88600011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-362.pth.tar', 74.85200003662109)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-365.pth.tar', 74.83600006835937)

Train: 374 [   0/1251 (  0%)]  Loss: 3.589 (3.59)  Time: 2.400s,  426.75/s  (2.400s,  426.75/s)  LR: 3.180e-04  Data: 1.770 (1.770)
Train: 374 [  50/1251 (  4%)]  Loss: 3.349 (3.47)  Time: 0.702s, 1458.67/s  (0.733s, 1397.35/s)  LR: 3.180e-04  Data: 0.009 (0.053)
Train: 374 [ 100/1251 (  8%)]  Loss: 3.257 (3.40)  Time: 0.672s, 1523.92/s  (0.712s, 1437.80/s)  LR: 3.180e-04  Data: 0.011 (0.032)
Train: 374 [ 150/1251 ( 12%)]  Loss: 3.726 (3.48)  Time: 0.701s, 1460.08/s  (0.708s, 1445.72/s)  LR: 3.180e-04  Data: 0.009 (0.025)
Train: 374 [ 200/1251 ( 16%)]  Loss: 3.489 (3.48)  Time: 0.674s, 1520.14/s  (0.702s, 1458.75/s)  LR: 3.180e-04  Data: 0.012 (0.021)
Train: 374 [ 250/1251 ( 20%)]  Loss: 3.514 (3.49)  Time: 0.682s, 1501.25/s  (0.699s, 1464.88/s)  LR: 3.180e-04  Data: 0.010 (0.019)
Train: 374 [ 300/1251 ( 24%)]  Loss: 3.762 (3.53)  Time: 0.710s, 1441.81/s  (0.700s, 1463.64/s)  LR: 3.180e-04  Data: 0.011 (0.017)
Train: 374 [ 350/1251 ( 28%)]  Loss: 3.695 (3.55)  Time: 0.702s, 1458.79/s  (0.699s, 1464.76/s)  LR: 3.180e-04  Data: 0.010 (0.016)
Train: 374 [ 400/1251 ( 32%)]  Loss: 3.638 (3.56)  Time: 0.705s, 1452.18/s  (0.698s, 1466.27/s)  LR: 3.180e-04  Data: 0.010 (0.016)
Train: 374 [ 450/1251 ( 36%)]  Loss: 3.555 (3.56)  Time: 0.712s, 1438.66/s  (0.698s, 1467.75/s)  LR: 3.180e-04  Data: 0.011 (0.015)
Train: 374 [ 500/1251 ( 40%)]  Loss: 3.765 (3.58)  Time: 0.709s, 1443.29/s  (0.697s, 1468.88/s)  LR: 3.180e-04  Data: 0.014 (0.015)
Train: 374 [ 550/1251 ( 44%)]  Loss: 3.415 (3.56)  Time: 0.665s, 1538.77/s  (0.697s, 1469.97/s)  LR: 3.180e-04  Data: 0.010 (0.014)
Train: 374 [ 600/1251 ( 48%)]  Loss: 3.320 (3.54)  Time: 0.674s, 1518.22/s  (0.696s, 1471.32/s)  LR: 3.180e-04  Data: 0.011 (0.014)
Train: 374 [ 650/1251 ( 52%)]  Loss: 3.680 (3.55)  Time: 0.711s, 1441.19/s  (0.695s, 1472.63/s)  LR: 3.180e-04  Data: 0.009 (0.014)
Train: 374 [ 700/1251 ( 56%)]  Loss: 3.376 (3.54)  Time: 0.697s, 1469.11/s  (0.695s, 1473.49/s)  LR: 3.180e-04  Data: 0.013 (0.013)
Train: 374 [ 750/1251 ( 60%)]  Loss: 3.639 (3.55)  Time: 0.674s, 1520.05/s  (0.695s, 1473.34/s)  LR: 3.180e-04  Data: 0.010 (0.013)
Train: 374 [ 800/1251 ( 64%)]  Loss: 3.234 (3.53)  Time: 0.699s, 1465.60/s  (0.695s, 1474.06/s)  LR: 3.180e-04  Data: 0.011 (0.013)
Train: 374 [ 850/1251 ( 68%)]  Loss: 3.424 (3.52)  Time: 0.675s, 1516.95/s  (0.695s, 1473.77/s)  LR: 3.180e-04  Data: 0.008 (0.013)
Train: 374 [ 900/1251 ( 72%)]  Loss: 3.487 (3.52)  Time: 0.721s, 1421.04/s  (0.694s, 1474.57/s)  LR: 3.180e-04  Data: 0.009 (0.013)
Train: 374 [ 950/1251 ( 76%)]  Loss: 3.722 (3.53)  Time: 0.679s, 1509.07/s  (0.694s, 1475.44/s)  LR: 3.180e-04  Data: 0.010 (0.013)
Train: 374 [1000/1251 ( 80%)]  Loss: 3.836 (3.55)  Time: 0.671s, 1525.54/s  (0.694s, 1475.12/s)  LR: 3.180e-04  Data: 0.011 (0.013)
Train: 374 [1050/1251 ( 84%)]  Loss: 3.601 (3.55)  Time: 0.713s, 1436.27/s  (0.694s, 1475.57/s)  LR: 3.180e-04  Data: 0.010 (0.012)
Train: 374 [1100/1251 ( 88%)]  Loss: 3.614 (3.55)  Time: 0.688s, 1489.05/s  (0.694s, 1475.77/s)  LR: 3.180e-04  Data: 0.009 (0.012)
Train: 374 [1150/1251 ( 92%)]  Loss: 3.338 (3.54)  Time: 0.671s, 1525.23/s  (0.694s, 1476.32/s)  LR: 3.180e-04  Data: 0.011 (0.012)
Train: 374 [1200/1251 ( 96%)]  Loss: 3.303 (3.53)  Time: 0.694s, 1476.16/s  (0.694s, 1476.30/s)  LR: 3.180e-04  Data: 0.011 (0.012)
Train: 374 [1250/1251 (100%)]  Loss: 3.340 (3.53)  Time: 0.701s, 1460.12/s  (0.694s, 1476.18/s)  LR: 3.180e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.585 (1.585)  Loss:  0.7285 (0.7285)  Acc@1: 89.6484 (89.6484)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.136 (0.583)  Loss:  0.8765 (1.3192)  Acc@1: 85.3774 (75.0200)  Acc@5: 96.9340 (92.4600)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-371.pth.tar', 75.10600014160157)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-370.pth.tar', 75.0360000341797)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-374.pth.tar', 75.01999998291015)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-366.pth.tar', 74.96599996582032)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-364.pth.tar', 74.9580001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-350.pth.tar', 74.92400008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-373.pth.tar', 74.91800011230468)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-360.pth.tar', 74.8940000366211)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-363.pth.tar', 74.88600011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-362.pth.tar', 74.85200003662109)

Train: 375 [   0/1251 (  0%)]  Loss: 3.835 (3.84)  Time: 2.173s,  471.30/s  (2.173s,  471.30/s)  LR: 3.156e-04  Data: 1.557 (1.557)
Train: 375 [  50/1251 (  4%)]  Loss: 3.679 (3.76)  Time: 0.690s, 1483.47/s  (0.722s, 1419.04/s)  LR: 3.156e-04  Data: 0.011 (0.046)
Train: 375 [ 100/1251 (  8%)]  Loss: 3.511 (3.68)  Time: 0.704s, 1453.51/s  (0.707s, 1448.95/s)  LR: 3.156e-04  Data: 0.011 (0.028)
Train: 375 [ 150/1251 ( 12%)]  Loss: 3.452 (3.62)  Time: 0.704s, 1453.61/s  (0.703s, 1455.77/s)  LR: 3.156e-04  Data: 0.010 (0.022)
Train: 375 [ 200/1251 ( 16%)]  Loss: 3.507 (3.60)  Time: 0.701s, 1460.83/s  (0.700s, 1463.77/s)  LR: 3.156e-04  Data: 0.009 (0.019)
Train: 375 [ 250/1251 ( 20%)]  Loss: 3.607 (3.60)  Time: 0.697s, 1469.60/s  (0.700s, 1463.08/s)  LR: 3.156e-04  Data: 0.010 (0.018)
Train: 375 [ 300/1251 ( 24%)]  Loss: 3.003 (3.51)  Time: 0.722s, 1417.73/s  (0.699s, 1463.96/s)  LR: 3.156e-04  Data: 0.010 (0.016)
Train: 375 [ 350/1251 ( 28%)]  Loss: 3.253 (3.48)  Time: 0.724s, 1414.63/s  (0.698s, 1466.34/s)  LR: 3.156e-04  Data: 0.009 (0.016)
Train: 375 [ 400/1251 ( 32%)]  Loss: 3.592 (3.49)  Time: 0.700s, 1463.04/s  (0.696s, 1470.33/s)  LR: 3.156e-04  Data: 0.010 (0.015)
Train: 375 [ 450/1251 ( 36%)]  Loss: 3.561 (3.50)  Time: 0.758s, 1351.27/s  (0.697s, 1468.87/s)  LR: 3.156e-04  Data: 0.010 (0.014)
Train: 375 [ 500/1251 ( 40%)]  Loss: 3.800 (3.53)  Time: 0.718s, 1426.40/s  (0.697s, 1469.80/s)  LR: 3.156e-04  Data: 0.009 (0.014)
Train: 375 [ 550/1251 ( 44%)]  Loss: 3.607 (3.53)  Time: 0.671s, 1526.36/s  (0.696s, 1471.83/s)  LR: 3.156e-04  Data: 0.011 (0.014)
Train: 375 [ 600/1251 ( 48%)]  Loss: 3.336 (3.52)  Time: 0.681s, 1504.29/s  (0.695s, 1472.68/s)  LR: 3.156e-04  Data: 0.016 (0.013)
Train: 375 [ 650/1251 ( 52%)]  Loss: 3.616 (3.53)  Time: 0.685s, 1494.79/s  (0.695s, 1472.98/s)  LR: 3.156e-04  Data: 0.010 (0.013)
Train: 375 [ 700/1251 ( 56%)]  Loss: 3.641 (3.53)  Time: 0.671s, 1525.11/s  (0.695s, 1473.58/s)  LR: 3.156e-04  Data: 0.011 (0.013)
Train: 375 [ 750/1251 ( 60%)]  Loss: 3.656 (3.54)  Time: 0.690s, 1483.70/s  (0.694s, 1474.48/s)  LR: 3.156e-04  Data: 0.013 (0.013)
Train: 375 [ 800/1251 ( 64%)]  Loss: 3.345 (3.53)  Time: 0.709s, 1444.46/s  (0.695s, 1474.04/s)  LR: 3.156e-04  Data: 0.011 (0.013)
Train: 375 [ 850/1251 ( 68%)]  Loss: 3.162 (3.51)  Time: 0.706s, 1450.21/s  (0.694s, 1474.68/s)  LR: 3.156e-04  Data: 0.008 (0.013)
Train: 375 [ 900/1251 ( 72%)]  Loss: 3.645 (3.52)  Time: 0.674s, 1518.51/s  (0.695s, 1474.13/s)  LR: 3.156e-04  Data: 0.010 (0.012)
Train: 375 [ 950/1251 ( 76%)]  Loss: 3.390 (3.51)  Time: 0.688s, 1487.32/s  (0.694s, 1474.71/s)  LR: 3.156e-04  Data: 0.009 (0.012)
Train: 375 [1000/1251 ( 80%)]  Loss: 3.710 (3.52)  Time: 0.704s, 1455.02/s  (0.694s, 1474.93/s)  LR: 3.156e-04  Data: 0.009 (0.012)
Train: 375 [1050/1251 ( 84%)]  Loss: 3.589 (3.52)  Time: 0.700s, 1463.35/s  (0.694s, 1475.52/s)  LR: 3.156e-04  Data: 0.009 (0.012)
Train: 375 [1100/1251 ( 88%)]  Loss: 3.225 (3.51)  Time: 0.713s, 1436.18/s  (0.694s, 1476.33/s)  LR: 3.156e-04  Data: 0.009 (0.012)
Train: 375 [1150/1251 ( 92%)]  Loss: 3.738 (3.52)  Time: 0.673s, 1522.27/s  (0.694s, 1476.42/s)  LR: 3.156e-04  Data: 0.009 (0.012)
Train: 375 [1200/1251 ( 96%)]  Loss: 3.534 (3.52)  Time: 0.679s, 1507.32/s  (0.693s, 1476.78/s)  LR: 3.156e-04  Data: 0.010 (0.012)
Train: 375 [1250/1251 (100%)]  Loss: 3.598 (3.52)  Time: 0.656s, 1560.10/s  (0.694s, 1476.20/s)  LR: 3.156e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.682 (1.682)  Loss:  0.7441 (0.7441)  Acc@1: 89.2578 (89.2578)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.136 (0.592)  Loss:  0.9199 (1.3951)  Acc@1: 83.9623 (74.4180)  Acc@5: 95.5189 (92.1860)
Train: 376 [   0/1251 (  0%)]  Loss: 3.420 (3.42)  Time: 2.366s,  432.86/s  (2.366s,  432.86/s)  LR: 3.132e-04  Data: 1.734 (1.734)
Train: 376 [  50/1251 (  4%)]  Loss: 3.669 (3.54)  Time: 0.710s, 1443.22/s  (0.726s, 1410.94/s)  LR: 3.132e-04  Data: 0.010 (0.047)
Train: 376 [ 100/1251 (  8%)]  Loss: 3.372 (3.49)  Time: 0.681s, 1504.05/s  (0.706s, 1450.13/s)  LR: 3.132e-04  Data: 0.011 (0.029)
Train: 376 [ 150/1251 ( 12%)]  Loss: 3.470 (3.48)  Time: 0.669s, 1530.80/s  (0.702s, 1459.37/s)  LR: 3.132e-04  Data: 0.009 (0.023)
Train: 376 [ 200/1251 ( 16%)]  Loss: 3.484 (3.48)  Time: 0.674s, 1520.26/s  (0.699s, 1465.31/s)  LR: 3.132e-04  Data: 0.010 (0.019)
Train: 376 [ 250/1251 ( 20%)]  Loss: 3.331 (3.46)  Time: 0.688s, 1488.32/s  (0.697s, 1469.52/s)  LR: 3.132e-04  Data: 0.011 (0.018)
Train: 376 [ 300/1251 ( 24%)]  Loss: 3.565 (3.47)  Time: 0.672s, 1524.77/s  (0.695s, 1472.80/s)  LR: 3.132e-04  Data: 0.011 (0.016)
Train: 376 [ 350/1251 ( 28%)]  Loss: 3.458 (3.47)  Time: 0.691s, 1481.94/s  (0.695s, 1473.27/s)  LR: 3.132e-04  Data: 0.010 (0.016)
Train: 376 [ 400/1251 ( 32%)]  Loss: 3.231 (3.44)  Time: 0.703s, 1455.61/s  (0.695s, 1473.45/s)  LR: 3.132e-04  Data: 0.011 (0.015)
Train: 376 [ 450/1251 ( 36%)]  Loss: 3.326 (3.43)  Time: 0.713s, 1435.44/s  (0.695s, 1473.54/s)  LR: 3.132e-04  Data: 0.014 (0.014)
Train: 376 [ 500/1251 ( 40%)]  Loss: 3.474 (3.44)  Time: 0.699s, 1465.23/s  (0.695s, 1472.42/s)  LR: 3.132e-04  Data: 0.009 (0.014)
Train: 376 [ 550/1251 ( 44%)]  Loss: 3.633 (3.45)  Time: 0.678s, 1509.98/s  (0.695s, 1473.91/s)  LR: 3.132e-04  Data: 0.009 (0.014)
Train: 376 [ 600/1251 ( 48%)]  Loss: 3.668 (3.47)  Time: 0.674s, 1518.99/s  (0.695s, 1473.62/s)  LR: 3.132e-04  Data: 0.011 (0.013)
Train: 376 [ 650/1251 ( 52%)]  Loss: 4.004 (3.51)  Time: 0.667s, 1535.89/s  (0.695s, 1474.12/s)  LR: 3.132e-04  Data: 0.009 (0.013)
Train: 376 [ 700/1251 ( 56%)]  Loss: 3.795 (3.53)  Time: 0.671s, 1525.78/s  (0.695s, 1474.05/s)  LR: 3.132e-04  Data: 0.011 (0.013)
Train: 376 [ 750/1251 ( 60%)]  Loss: 3.620 (3.53)  Time: 0.672s, 1523.42/s  (0.695s, 1474.43/s)  LR: 3.132e-04  Data: 0.010 (0.013)
Train: 376 [ 800/1251 ( 64%)]  Loss: 3.701 (3.54)  Time: 0.686s, 1492.36/s  (0.694s, 1475.32/s)  LR: 3.132e-04  Data: 0.011 (0.013)
Train: 376 [ 850/1251 ( 68%)]  Loss: 3.938 (3.56)  Time: 0.673s, 1521.51/s  (0.694s, 1476.08/s)  LR: 3.132e-04  Data: 0.010 (0.013)
Train: 376 [ 900/1251 ( 72%)]  Loss: 3.460 (3.56)  Time: 0.709s, 1445.25/s  (0.693s, 1477.02/s)  LR: 3.132e-04  Data: 0.008 (0.012)
Train: 376 [ 950/1251 ( 76%)]  Loss: 3.183 (3.54)  Time: 0.710s, 1443.07/s  (0.693s, 1477.29/s)  LR: 3.132e-04  Data: 0.011 (0.012)
Train: 376 [1000/1251 ( 80%)]  Loss: 3.693 (3.55)  Time: 0.704s, 1453.51/s  (0.693s, 1477.36/s)  LR: 3.132e-04  Data: 0.009 (0.012)
Train: 376 [1050/1251 ( 84%)]  Loss: 3.499 (3.55)  Time: 0.673s, 1520.93/s  (0.693s, 1477.59/s)  LR: 3.132e-04  Data: 0.010 (0.012)
Train: 376 [1100/1251 ( 88%)]  Loss: 3.167 (3.53)  Time: 0.673s, 1522.47/s  (0.693s, 1477.90/s)  LR: 3.132e-04  Data: 0.011 (0.012)
Train: 376 [1150/1251 ( 92%)]  Loss: 3.548 (3.53)  Time: 0.671s, 1525.48/s  (0.692s, 1478.72/s)  LR: 3.132e-04  Data: 0.011 (0.012)
Train: 376 [1200/1251 ( 96%)]  Loss: 3.439 (3.53)  Time: 0.687s, 1490.29/s  (0.692s, 1478.90/s)  LR: 3.132e-04  Data: 0.012 (0.012)
Train: 376 [1250/1251 (100%)]  Loss: 3.481 (3.52)  Time: 0.659s, 1553.99/s  (0.692s, 1479.06/s)  LR: 3.132e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.469 (1.469)  Loss:  0.8711 (0.8711)  Acc@1: 90.1367 (90.1367)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.136 (0.579)  Loss:  0.8691 (1.4246)  Acc@1: 86.4387 (75.0420)  Acc@5: 97.4057 (92.4540)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-371.pth.tar', 75.10600014160157)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-376.pth.tar', 75.04199992675781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-370.pth.tar', 75.0360000341797)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-374.pth.tar', 75.01999998291015)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-366.pth.tar', 74.96599996582032)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-364.pth.tar', 74.9580001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-350.pth.tar', 74.92400008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-373.pth.tar', 74.91800011230468)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-360.pth.tar', 74.8940000366211)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-363.pth.tar', 74.88600011230469)

Train: 377 [   0/1251 (  0%)]  Loss: 3.572 (3.57)  Time: 2.101s,  487.37/s  (2.101s,  487.37/s)  LR: 3.108e-04  Data: 1.484 (1.484)
Train: 377 [  50/1251 (  4%)]  Loss: 3.336 (3.45)  Time: 0.673s, 1522.28/s  (0.723s, 1415.54/s)  LR: 3.108e-04  Data: 0.010 (0.050)
Train: 377 [ 100/1251 (  8%)]  Loss: 3.590 (3.50)  Time: 0.670s, 1527.83/s  (0.713s, 1436.90/s)  LR: 3.108e-04  Data: 0.013 (0.031)
Train: 377 [ 150/1251 ( 12%)]  Loss: 3.741 (3.56)  Time: 0.673s, 1522.34/s  (0.704s, 1455.35/s)  LR: 3.108e-04  Data: 0.013 (0.024)
Train: 377 [ 200/1251 ( 16%)]  Loss: 3.428 (3.53)  Time: 0.672s, 1523.87/s  (0.701s, 1461.00/s)  LR: 3.108e-04  Data: 0.010 (0.020)
Train: 377 [ 250/1251 ( 20%)]  Loss: 3.766 (3.57)  Time: 0.672s, 1524.93/s  (0.699s, 1465.89/s)  LR: 3.108e-04  Data: 0.011 (0.018)
Train: 377 [ 300/1251 ( 24%)]  Loss: 3.560 (3.57)  Time: 0.782s, 1309.28/s  (0.697s, 1469.20/s)  LR: 3.108e-04  Data: 0.010 (0.017)
Train: 377 [ 350/1251 ( 28%)]  Loss: 3.294 (3.54)  Time: 0.694s, 1476.12/s  (0.696s, 1472.32/s)  LR: 3.108e-04  Data: 0.009 (0.016)
Train: 377 [ 400/1251 ( 32%)]  Loss: 3.678 (3.55)  Time: 0.672s, 1524.77/s  (0.693s, 1476.57/s)  LR: 3.108e-04  Data: 0.010 (0.015)
Train: 377 [ 450/1251 ( 36%)]  Loss: 3.364 (3.53)  Time: 0.722s, 1418.19/s  (0.694s, 1475.55/s)  LR: 3.108e-04  Data: 0.011 (0.015)
Train: 377 [ 500/1251 ( 40%)]  Loss: 3.206 (3.50)  Time: 0.671s, 1526.38/s  (0.693s, 1476.78/s)  LR: 3.108e-04  Data: 0.010 (0.014)
Train: 377 [ 550/1251 ( 44%)]  Loss: 3.157 (3.47)  Time: 0.670s, 1529.34/s  (0.693s, 1477.33/s)  LR: 3.108e-04  Data: 0.010 (0.014)
Train: 377 [ 600/1251 ( 48%)]  Loss: 3.380 (3.47)  Time: 0.694s, 1474.89/s  (0.693s, 1476.73/s)  LR: 3.108e-04  Data: 0.017 (0.014)
Train: 377 [ 650/1251 ( 52%)]  Loss: 3.576 (3.47)  Time: 0.671s, 1525.83/s  (0.693s, 1476.60/s)  LR: 3.108e-04  Data: 0.010 (0.014)
Train: 377 [ 700/1251 ( 56%)]  Loss: 3.470 (3.47)  Time: 0.699s, 1464.58/s  (0.694s, 1475.47/s)  LR: 3.108e-04  Data: 0.009 (0.013)
Train: 377 [ 750/1251 ( 60%)]  Loss: 3.382 (3.47)  Time: 0.702s, 1459.25/s  (0.693s, 1477.02/s)  LR: 3.108e-04  Data: 0.009 (0.013)
Train: 377 [ 800/1251 ( 64%)]  Loss: 3.638 (3.48)  Time: 0.673s, 1522.68/s  (0.693s, 1477.45/s)  LR: 3.108e-04  Data: 0.010 (0.013)
Train: 377 [ 850/1251 ( 68%)]  Loss: 3.232 (3.46)  Time: 0.704s, 1454.46/s  (0.693s, 1478.02/s)  LR: 3.108e-04  Data: 0.009 (0.013)
Train: 377 [ 900/1251 ( 72%)]  Loss: 3.921 (3.49)  Time: 0.766s, 1337.20/s  (0.693s, 1478.55/s)  LR: 3.108e-04  Data: 0.010 (0.013)
Train: 377 [ 950/1251 ( 76%)]  Loss: 3.303 (3.48)  Time: 0.700s, 1462.16/s  (0.693s, 1478.69/s)  LR: 3.108e-04  Data: 0.009 (0.013)
Train: 377 [1000/1251 ( 80%)]  Loss: 3.459 (3.48)  Time: 0.721s, 1420.15/s  (0.693s, 1478.65/s)  LR: 3.108e-04  Data: 0.010 (0.012)
Train: 377 [1050/1251 ( 84%)]  Loss: 3.437 (3.48)  Time: 0.671s, 1525.65/s  (0.692s, 1479.12/s)  LR: 3.108e-04  Data: 0.010 (0.012)
Train: 377 [1100/1251 ( 88%)]  Loss: 3.479 (3.48)  Time: 0.711s, 1440.42/s  (0.692s, 1479.60/s)  LR: 3.108e-04  Data: 0.009 (0.012)
Train: 377 [1150/1251 ( 92%)]  Loss: 3.772 (3.49)  Time: 0.715s, 1431.50/s  (0.692s, 1480.07/s)  LR: 3.108e-04  Data: 0.012 (0.012)
Train: 377 [1200/1251 ( 96%)]  Loss: 3.217 (3.48)  Time: 0.681s, 1502.73/s  (0.692s, 1480.10/s)  LR: 3.108e-04  Data: 0.010 (0.012)
Train: 377 [1250/1251 (100%)]  Loss: 3.634 (3.48)  Time: 0.738s, 1386.98/s  (0.692s, 1479.64/s)  LR: 3.108e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.470 (1.470)  Loss:  0.9395 (0.9395)  Acc@1: 89.2578 (89.2578)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.136 (0.579)  Loss:  1.0078 (1.3949)  Acc@1: 85.7311 (75.2320)  Acc@5: 96.4623 (92.5340)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-377.pth.tar', 75.23200000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-371.pth.tar', 75.10600014160157)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-376.pth.tar', 75.04199992675781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-370.pth.tar', 75.0360000341797)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-374.pth.tar', 75.01999998291015)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-366.pth.tar', 74.96599996582032)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-364.pth.tar', 74.9580001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-350.pth.tar', 74.92400008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-373.pth.tar', 74.91800011230468)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-360.pth.tar', 74.8940000366211)

Train: 378 [   0/1251 (  0%)]  Loss: 3.421 (3.42)  Time: 2.072s,  494.31/s  (2.072s,  494.31/s)  LR: 3.084e-04  Data: 1.455 (1.455)
Train: 378 [  50/1251 (  4%)]  Loss: 3.692 (3.56)  Time: 0.672s, 1524.92/s  (0.726s, 1409.62/s)  LR: 3.084e-04  Data: 0.011 (0.047)
Train: 378 [ 100/1251 (  8%)]  Loss: 3.542 (3.55)  Time: 0.672s, 1524.22/s  (0.712s, 1439.19/s)  LR: 3.084e-04  Data: 0.010 (0.029)
Train: 378 [ 150/1251 ( 12%)]  Loss: 3.411 (3.52)  Time: 0.687s, 1489.85/s  (0.704s, 1454.47/s)  LR: 3.084e-04  Data: 0.011 (0.023)
Train: 378 [ 200/1251 ( 16%)]  Loss: 3.371 (3.49)  Time: 0.695s, 1474.42/s  (0.702s, 1458.00/s)  LR: 3.084e-04  Data: 0.009 (0.020)
Train: 378 [ 250/1251 ( 20%)]  Loss: 3.476 (3.49)  Time: 0.672s, 1523.44/s  (0.700s, 1463.22/s)  LR: 3.084e-04  Data: 0.011 (0.018)
Train: 378 [ 300/1251 ( 24%)]  Loss: 3.595 (3.50)  Time: 0.748s, 1368.20/s  (0.700s, 1462.94/s)  LR: 3.084e-04  Data: 0.016 (0.017)
Train: 378 [ 350/1251 ( 28%)]  Loss: 3.720 (3.53)  Time: 0.700s, 1462.18/s  (0.698s, 1467.70/s)  LR: 3.084e-04  Data: 0.009 (0.016)
Train: 378 [ 400/1251 ( 32%)]  Loss: 3.734 (3.55)  Time: 0.672s, 1522.75/s  (0.696s, 1470.58/s)  LR: 3.084e-04  Data: 0.010 (0.015)
Train: 378 [ 450/1251 ( 36%)]  Loss: 3.638 (3.56)  Time: 0.673s, 1522.23/s  (0.696s, 1471.97/s)  LR: 3.084e-04  Data: 0.010 (0.015)
Train: 378 [ 500/1251 ( 40%)]  Loss: 3.368 (3.54)  Time: 0.701s, 1460.40/s  (0.697s, 1469.64/s)  LR: 3.084e-04  Data: 0.009 (0.014)
Train: 378 [ 550/1251 ( 44%)]  Loss: 3.770 (3.56)  Time: 0.679s, 1508.67/s  (0.697s, 1468.56/s)  LR: 3.084e-04  Data: 0.009 (0.014)
Train: 378 [ 600/1251 ( 48%)]  Loss: 3.663 (3.57)  Time: 0.699s, 1464.72/s  (0.696s, 1470.80/s)  LR: 3.084e-04  Data: 0.012 (0.014)
Train: 378 [ 650/1251 ( 52%)]  Loss: 3.405 (3.56)  Time: 0.666s, 1538.18/s  (0.696s, 1471.37/s)  LR: 3.084e-04  Data: 0.010 (0.013)
Train: 378 [ 700/1251 ( 56%)]  Loss: 3.204 (3.53)  Time: 0.688s, 1487.69/s  (0.696s, 1470.83/s)  LR: 3.084e-04  Data: 0.016 (0.013)
Train: 378 [ 750/1251 ( 60%)]  Loss: 3.608 (3.54)  Time: 0.676s, 1515.51/s  (0.696s, 1472.20/s)  LR: 3.084e-04  Data: 0.010 (0.013)
Train: 378 [ 800/1251 ( 64%)]  Loss: 3.305 (3.52)  Time: 0.683s, 1499.95/s  (0.696s, 1471.62/s)  LR: 3.084e-04  Data: 0.011 (0.013)
Train: 378 [ 850/1251 ( 68%)]  Loss: 3.472 (3.52)  Time: 0.707s, 1448.09/s  (0.695s, 1472.34/s)  LR: 3.084e-04  Data: 0.009 (0.013)
Train: 378 [ 900/1251 ( 72%)]  Loss: 3.225 (3.51)  Time: 0.673s, 1522.04/s  (0.695s, 1473.56/s)  LR: 3.084e-04  Data: 0.011 (0.013)
Train: 378 [ 950/1251 ( 76%)]  Loss: 3.760 (3.52)  Time: 0.707s, 1447.81/s  (0.695s, 1473.93/s)  LR: 3.084e-04  Data: 0.010 (0.012)
Train: 378 [1000/1251 ( 80%)]  Loss: 3.686 (3.53)  Time: 0.713s, 1436.35/s  (0.695s, 1473.29/s)  LR: 3.084e-04  Data: 0.012 (0.012)
Train: 378 [1050/1251 ( 84%)]  Loss: 3.733 (3.54)  Time: 0.684s, 1496.47/s  (0.695s, 1474.26/s)  LR: 3.084e-04  Data: 0.011 (0.012)
Train: 378 [1100/1251 ( 88%)]  Loss: 3.186 (3.52)  Time: 0.680s, 1505.22/s  (0.695s, 1474.09/s)  LR: 3.084e-04  Data: 0.015 (0.012)
Train: 378 [1150/1251 ( 92%)]  Loss: 3.180 (3.51)  Time: 0.722s, 1418.46/s  (0.695s, 1474.40/s)  LR: 3.084e-04  Data: 0.011 (0.012)
Train: 378 [1200/1251 ( 96%)]  Loss: 3.582 (3.51)  Time: 0.672s, 1524.12/s  (0.695s, 1474.13/s)  LR: 3.084e-04  Data: 0.011 (0.012)
Train: 378 [1250/1251 (100%)]  Loss: 3.610 (3.51)  Time: 0.656s, 1560.89/s  (0.694s, 1474.97/s)  LR: 3.084e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.610 (1.610)  Loss:  0.8633 (0.8633)  Acc@1: 89.6484 (89.6484)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.136 (0.574)  Loss:  0.9712 (1.4021)  Acc@1: 86.2028 (75.2780)  Acc@5: 96.9340 (92.5300)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-378.pth.tar', 75.27799995361327)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-377.pth.tar', 75.23200000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-371.pth.tar', 75.10600014160157)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-376.pth.tar', 75.04199992675781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-370.pth.tar', 75.0360000341797)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-374.pth.tar', 75.01999998291015)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-366.pth.tar', 74.96599996582032)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-364.pth.tar', 74.9580001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-350.pth.tar', 74.92400008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-373.pth.tar', 74.91800011230468)

Train: 379 [   0/1251 (  0%)]  Loss: 3.478 (3.48)  Time: 2.042s,  501.52/s  (2.042s,  501.52/s)  LR: 3.060e-04  Data: 1.429 (1.429)
Train: 379 [  50/1251 (  4%)]  Loss: 3.788 (3.63)  Time: 0.690s, 1484.32/s  (0.731s, 1401.52/s)  LR: 3.060e-04  Data: 0.013 (0.048)
Train: 379 [ 100/1251 (  8%)]  Loss: 3.347 (3.54)  Time: 0.724s, 1414.47/s  (0.709s, 1444.46/s)  LR: 3.060e-04  Data: 0.011 (0.030)
Train: 379 [ 150/1251 ( 12%)]  Loss: 3.644 (3.56)  Time: 0.760s, 1347.21/s  (0.703s, 1457.10/s)  LR: 3.060e-04  Data: 0.010 (0.023)
Train: 379 [ 200/1251 ( 16%)]  Loss: 3.439 (3.54)  Time: 0.671s, 1527.07/s  (0.699s, 1464.34/s)  LR: 3.060e-04  Data: 0.010 (0.020)
Train: 379 [ 250/1251 ( 20%)]  Loss: 3.503 (3.53)  Time: 0.715s, 1431.44/s  (0.697s, 1468.76/s)  LR: 3.060e-04  Data: 0.009 (0.018)
Train: 379 [ 300/1251 ( 24%)]  Loss: 3.935 (3.59)  Time: 0.703s, 1457.55/s  (0.695s, 1472.84/s)  LR: 3.060e-04  Data: 0.009 (0.017)
Train: 379 [ 350/1251 ( 28%)]  Loss: 4.087 (3.65)  Time: 0.671s, 1527.01/s  (0.695s, 1474.30/s)  LR: 3.060e-04  Data: 0.010 (0.016)
Train: 379 [ 400/1251 ( 32%)]  Loss: 3.533 (3.64)  Time: 0.679s, 1507.42/s  (0.694s, 1475.20/s)  LR: 3.060e-04  Data: 0.009 (0.015)
Train: 379 [ 450/1251 ( 36%)]  Loss: 3.257 (3.60)  Time: 0.824s, 1242.73/s  (0.694s, 1475.92/s)  LR: 3.060e-04  Data: 0.010 (0.015)
Train: 379 [ 500/1251 ( 40%)]  Loss: 3.738 (3.61)  Time: 0.696s, 1470.90/s  (0.694s, 1475.33/s)  LR: 3.060e-04  Data: 0.011 (0.014)
Train: 379 [ 550/1251 ( 44%)]  Loss: 3.594 (3.61)  Time: 0.675s, 1517.16/s  (0.694s, 1475.61/s)  LR: 3.060e-04  Data: 0.010 (0.014)
Train: 379 [ 600/1251 ( 48%)]  Loss: 3.734 (3.62)  Time: 0.670s, 1527.23/s  (0.694s, 1474.97/s)  LR: 3.060e-04  Data: 0.010 (0.014)
Train: 379 [ 650/1251 ( 52%)]  Loss: 3.469 (3.61)  Time: 0.712s, 1437.36/s  (0.694s, 1474.80/s)  LR: 3.060e-04  Data: 0.010 (0.013)
Train: 379 [ 700/1251 ( 56%)]  Loss: 3.791 (3.62)  Time: 0.681s, 1502.82/s  (0.694s, 1475.95/s)  LR: 3.060e-04  Data: 0.013 (0.013)
Train: 379 [ 750/1251 ( 60%)]  Loss: 3.726 (3.63)  Time: 0.701s, 1461.27/s  (0.694s, 1476.24/s)  LR: 3.060e-04  Data: 0.009 (0.013)
Train: 379 [ 800/1251 ( 64%)]  Loss: 3.397 (3.62)  Time: 0.681s, 1503.60/s  (0.693s, 1477.21/s)  LR: 3.060e-04  Data: 0.011 (0.013)
Train: 379 [ 850/1251 ( 68%)]  Loss: 3.754 (3.62)  Time: 0.699s, 1464.83/s  (0.693s, 1477.57/s)  LR: 3.060e-04  Data: 0.009 (0.013)
Train: 379 [ 900/1251 ( 72%)]  Loss: 3.234 (3.60)  Time: 0.666s, 1536.85/s  (0.693s, 1477.55/s)  LR: 3.060e-04  Data: 0.010 (0.012)
Train: 379 [ 950/1251 ( 76%)]  Loss: 3.465 (3.60)  Time: 0.699s, 1463.91/s  (0.693s, 1477.58/s)  LR: 3.060e-04  Data: 0.009 (0.012)
Train: 379 [1000/1251 ( 80%)]  Loss: 3.677 (3.60)  Time: 0.671s, 1525.49/s  (0.693s, 1477.28/s)  LR: 3.060e-04  Data: 0.010 (0.012)
Train: 379 [1050/1251 ( 84%)]  Loss: 3.584 (3.60)  Time: 0.674s, 1518.65/s  (0.693s, 1477.18/s)  LR: 3.060e-04  Data: 0.010 (0.012)
Train: 379 [1100/1251 ( 88%)]  Loss: 3.704 (3.60)  Time: 0.679s, 1508.06/s  (0.693s, 1478.07/s)  LR: 3.060e-04  Data: 0.010 (0.012)
Train: 379 [1150/1251 ( 92%)]  Loss: 3.721 (3.61)  Time: 0.671s, 1526.96/s  (0.693s, 1477.49/s)  LR: 3.060e-04  Data: 0.013 (0.012)
Train: 379 [1200/1251 ( 96%)]  Loss: 3.845 (3.62)  Time: 0.674s, 1518.51/s  (0.693s, 1477.88/s)  LR: 3.060e-04  Data: 0.009 (0.012)
Train: 379 [1250/1251 (100%)]  Loss: 3.512 (3.61)  Time: 0.707s, 1448.10/s  (0.693s, 1477.92/s)  LR: 3.060e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.547 (1.547)  Loss:  0.7725 (0.7725)  Acc@1: 88.6719 (88.6719)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.136 (0.579)  Loss:  0.8501 (1.3540)  Acc@1: 85.3774 (74.6340)  Acc@5: 96.4623 (92.0220)
Train: 380 [   0/1251 (  0%)]  Loss: 3.474 (3.47)  Time: 2.223s,  460.59/s  (2.223s,  460.59/s)  LR: 3.037e-04  Data: 1.563 (1.563)
Train: 380 [  50/1251 (  4%)]  Loss: 3.493 (3.48)  Time: 0.711s, 1441.16/s  (0.735s, 1393.68/s)  LR: 3.037e-04  Data: 0.009 (0.050)
Train: 380 [ 100/1251 (  8%)]  Loss: 3.540 (3.50)  Time: 0.671s, 1526.03/s  (0.710s, 1443.01/s)  LR: 3.037e-04  Data: 0.010 (0.030)
Train: 380 [ 150/1251 ( 12%)]  Loss: 3.457 (3.49)  Time: 0.684s, 1497.20/s  (0.706s, 1450.12/s)  LR: 3.037e-04  Data: 0.009 (0.024)
Train: 380 [ 200/1251 ( 16%)]  Loss: 3.297 (3.45)  Time: 0.731s, 1401.13/s  (0.703s, 1456.13/s)  LR: 3.037e-04  Data: 0.011 (0.020)
Train: 380 [ 250/1251 ( 20%)]  Loss: 3.368 (3.44)  Time: 0.705s, 1451.47/s  (0.701s, 1460.91/s)  LR: 3.037e-04  Data: 0.009 (0.018)
Train: 380 [ 300/1251 ( 24%)]  Loss: 3.937 (3.51)  Time: 0.672s, 1524.86/s  (0.698s, 1466.63/s)  LR: 3.037e-04  Data: 0.010 (0.017)
Train: 380 [ 350/1251 ( 28%)]  Loss: 3.831 (3.55)  Time: 0.712s, 1438.01/s  (0.697s, 1469.10/s)  LR: 3.037e-04  Data: 0.010 (0.016)
Train: 380 [ 400/1251 ( 32%)]  Loss: 3.633 (3.56)  Time: 0.674s, 1518.88/s  (0.697s, 1468.95/s)  LR: 3.037e-04  Data: 0.011 (0.015)
Train: 380 [ 450/1251 ( 36%)]  Loss: 3.621 (3.57)  Time: 0.673s, 1521.66/s  (0.696s, 1470.36/s)  LR: 3.037e-04  Data: 0.010 (0.015)
Train: 380 [ 500/1251 ( 40%)]  Loss: 3.496 (3.56)  Time: 0.681s, 1504.30/s  (0.697s, 1470.07/s)  LR: 3.037e-04  Data: 0.011 (0.014)
Train: 380 [ 550/1251 ( 44%)]  Loss: 3.651 (3.57)  Time: 0.671s, 1527.19/s  (0.696s, 1471.54/s)  LR: 3.037e-04  Data: 0.009 (0.014)
Train: 380 [ 600/1251 ( 48%)]  Loss: 3.433 (3.56)  Time: 0.735s, 1393.52/s  (0.695s, 1473.48/s)  LR: 3.037e-04  Data: 0.010 (0.014)
Train: 380 [ 650/1251 ( 52%)]  Loss: 3.458 (3.55)  Time: 0.700s, 1463.17/s  (0.695s, 1473.34/s)  LR: 3.037e-04  Data: 0.011 (0.013)
Train: 380 [ 700/1251 ( 56%)]  Loss: 3.560 (3.55)  Time: 0.673s, 1522.49/s  (0.695s, 1473.81/s)  LR: 3.037e-04  Data: 0.010 (0.013)
Train: 380 [ 750/1251 ( 60%)]  Loss: 3.928 (3.57)  Time: 0.732s, 1399.16/s  (0.694s, 1474.54/s)  LR: 3.037e-04  Data: 0.009 (0.013)
Train: 380 [ 800/1251 ( 64%)]  Loss: 3.731 (3.58)  Time: 0.674s, 1519.03/s  (0.694s, 1474.67/s)  LR: 3.037e-04  Data: 0.011 (0.013)
Train: 380 [ 850/1251 ( 68%)]  Loss: 3.521 (3.58)  Time: 0.673s, 1522.67/s  (0.694s, 1475.48/s)  LR: 3.037e-04  Data: 0.011 (0.013)
Train: 380 [ 900/1251 ( 72%)]  Loss: 3.475 (3.57)  Time: 0.720s, 1421.74/s  (0.694s, 1475.99/s)  LR: 3.037e-04  Data: 0.011 (0.013)
Train: 380 [ 950/1251 ( 76%)]  Loss: 3.417 (3.57)  Time: 0.709s, 1444.77/s  (0.693s, 1477.28/s)  LR: 3.037e-04  Data: 0.011 (0.013)
Train: 380 [1000/1251 ( 80%)]  Loss: 3.668 (3.57)  Time: 0.734s, 1395.21/s  (0.693s, 1477.31/s)  LR: 3.037e-04  Data: 0.009 (0.012)
Train: 380 [1050/1251 ( 84%)]  Loss: 3.758 (3.58)  Time: 0.703s, 1457.52/s  (0.693s, 1476.76/s)  LR: 3.037e-04  Data: 0.009 (0.012)
Train: 380 [1100/1251 ( 88%)]  Loss: 3.532 (3.58)  Time: 0.724s, 1415.23/s  (0.694s, 1475.94/s)  LR: 3.037e-04  Data: 0.009 (0.012)
Train: 380 [1150/1251 ( 92%)]  Loss: 3.573 (3.58)  Time: 0.672s, 1524.30/s  (0.694s, 1476.34/s)  LR: 3.037e-04  Data: 0.012 (0.012)
Train: 380 [1200/1251 ( 96%)]  Loss: 3.421 (3.57)  Time: 0.695s, 1474.08/s  (0.694s, 1476.17/s)  LR: 3.037e-04  Data: 0.009 (0.012)
Train: 380 [1250/1251 (100%)]  Loss: 3.614 (3.57)  Time: 0.656s, 1561.50/s  (0.694s, 1476.35/s)  LR: 3.037e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.477 (1.477)  Loss:  0.7310 (0.7310)  Acc@1: 89.4531 (89.4531)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.136 (0.578)  Loss:  0.8379 (1.3014)  Acc@1: 83.7264 (75.3120)  Acc@5: 96.8160 (92.6660)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-380.pth.tar', 75.31200004150391)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-378.pth.tar', 75.27799995361327)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-377.pth.tar', 75.23200000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-371.pth.tar', 75.10600014160157)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-376.pth.tar', 75.04199992675781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-370.pth.tar', 75.0360000341797)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-374.pth.tar', 75.01999998291015)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-366.pth.tar', 74.96599996582032)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-364.pth.tar', 74.9580001171875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-350.pth.tar', 74.92400008544922)

Train: 381 [   0/1251 (  0%)]  Loss: 3.159 (3.16)  Time: 2.124s,  482.07/s  (2.124s,  482.07/s)  LR: 3.013e-04  Data: 1.510 (1.510)
Train: 381 [  50/1251 (  4%)]  Loss: 3.288 (3.22)  Time: 0.690s, 1483.63/s  (0.738s, 1386.81/s)  LR: 3.013e-04  Data: 0.010 (0.051)
Train: 381 [ 100/1251 (  8%)]  Loss: 2.996 (3.15)  Time: 0.708s, 1446.61/s  (0.723s, 1416.62/s)  LR: 3.013e-04  Data: 0.009 (0.031)
Train: 381 [ 150/1251 ( 12%)]  Loss: 3.746 (3.30)  Time: 0.751s, 1363.77/s  (0.712s, 1437.40/s)  LR: 3.013e-04  Data: 0.011 (0.024)
Train: 381 [ 200/1251 ( 16%)]  Loss: 3.557 (3.35)  Time: 0.699s, 1465.97/s  (0.713s, 1436.08/s)  LR: 3.013e-04  Data: 0.009 (0.021)
Train: 381 [ 250/1251 ( 20%)]  Loss: 3.516 (3.38)  Time: 0.722s, 1419.24/s  (0.712s, 1437.65/s)  LR: 3.013e-04  Data: 0.013 (0.020)
Train: 381 [ 300/1251 ( 24%)]  Loss: 3.387 (3.38)  Time: 0.793s, 1291.66/s  (0.712s, 1437.92/s)  LR: 3.013e-04  Data: 0.015 (0.018)
Train: 381 [ 350/1251 ( 28%)]  Loss: 3.593 (3.41)  Time: 0.703s, 1456.22/s  (0.708s, 1445.89/s)  LR: 3.013e-04  Data: 0.009 (0.017)
Train: 381 [ 400/1251 ( 32%)]  Loss: 3.311 (3.39)  Time: 0.671s, 1526.89/s  (0.704s, 1453.82/s)  LR: 3.013e-04  Data: 0.010 (0.016)
Train: 381 [ 450/1251 ( 36%)]  Loss: 3.540 (3.41)  Time: 0.671s, 1526.33/s  (0.701s, 1461.32/s)  LR: 3.013e-04  Data: 0.010 (0.016)
Train: 381 [ 500/1251 ( 40%)]  Loss: 3.380 (3.41)  Time: 0.715s, 1432.43/s  (0.699s, 1464.01/s)  LR: 3.013e-04  Data: 0.011 (0.015)
Train: 381 [ 550/1251 ( 44%)]  Loss: 3.375 (3.40)  Time: 0.706s, 1449.93/s  (0.699s, 1464.29/s)  LR: 3.013e-04  Data: 0.010 (0.015)
Train: 381 [ 600/1251 ( 48%)]  Loss: 3.592 (3.42)  Time: 0.669s, 1529.63/s  (0.698s, 1466.85/s)  LR: 3.013e-04  Data: 0.011 (0.014)
Train: 381 [ 650/1251 ( 52%)]  Loss: 2.911 (3.38)  Time: 0.690s, 1484.92/s  (0.698s, 1467.54/s)  LR: 3.013e-04  Data: 0.009 (0.014)
Train: 381 [ 700/1251 ( 56%)]  Loss: 3.565 (3.39)  Time: 0.676s, 1514.75/s  (0.697s, 1469.28/s)  LR: 3.013e-04  Data: 0.013 (0.014)
Train: 381 [ 750/1251 ( 60%)]  Loss: 3.613 (3.41)  Time: 0.713s, 1435.40/s  (0.697s, 1469.56/s)  LR: 3.013e-04  Data: 0.009 (0.014)
Train: 381 [ 800/1251 ( 64%)]  Loss: 3.827 (3.43)  Time: 0.670s, 1529.35/s  (0.696s, 1471.11/s)  LR: 3.013e-04  Data: 0.010 (0.013)
Train: 381 [ 850/1251 ( 68%)]  Loss: 3.411 (3.43)  Time: 0.672s, 1522.77/s  (0.696s, 1471.58/s)  LR: 3.013e-04  Data: 0.010 (0.013)
Train: 381 [ 900/1251 ( 72%)]  Loss: 3.415 (3.43)  Time: 0.750s, 1364.64/s  (0.696s, 1472.23/s)  LR: 3.013e-04  Data: 0.011 (0.013)
Train: 381 [ 950/1251 ( 76%)]  Loss: 3.171 (3.42)  Time: 0.678s, 1510.09/s  (0.695s, 1473.20/s)  LR: 3.013e-04  Data: 0.014 (0.013)
Train: 381 [1000/1251 ( 80%)]  Loss: 3.569 (3.42)  Time: 0.665s, 1539.28/s  (0.695s, 1473.14/s)  LR: 3.013e-04  Data: 0.009 (0.013)
Train: 381 [1050/1251 ( 84%)]  Loss: 3.719 (3.44)  Time: 0.676s, 1514.11/s  (0.695s, 1473.51/s)  LR: 3.013e-04  Data: 0.011 (0.013)
Train: 381 [1100/1251 ( 88%)]  Loss: 3.389 (3.44)  Time: 0.707s, 1448.84/s  (0.695s, 1473.39/s)  LR: 3.013e-04  Data: 0.009 (0.013)
Train: 381 [1150/1251 ( 92%)]  Loss: 3.284 (3.43)  Time: 0.678s, 1509.36/s  (0.695s, 1473.85/s)  LR: 3.013e-04  Data: 0.011 (0.013)
Train: 381 [1200/1251 ( 96%)]  Loss: 3.599 (3.44)  Time: 0.668s, 1533.93/s  (0.695s, 1474.41/s)  LR: 3.013e-04  Data: 0.009 (0.012)
Train: 381 [1250/1251 (100%)]  Loss: 3.306 (3.43)  Time: 0.719s, 1424.63/s  (0.694s, 1475.28/s)  LR: 3.013e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.481 (1.481)  Loss:  0.6904 (0.6904)  Acc@1: 89.1602 (89.1602)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.136 (0.584)  Loss:  0.7305 (1.2219)  Acc@1: 85.7311 (75.4460)  Acc@5: 96.9340 (92.7140)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-381.pth.tar', 75.44600000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-380.pth.tar', 75.31200004150391)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-378.pth.tar', 75.27799995361327)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-377.pth.tar', 75.23200000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-371.pth.tar', 75.10600014160157)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-376.pth.tar', 75.04199992675781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-370.pth.tar', 75.0360000341797)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-374.pth.tar', 75.01999998291015)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-366.pth.tar', 74.96599996582032)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-364.pth.tar', 74.9580001171875)

Train: 382 [   0/1251 (  0%)]  Loss: 3.501 (3.50)  Time: 2.185s,  468.58/s  (2.185s,  468.58/s)  LR: 2.989e-04  Data: 1.569 (1.569)
Train: 382 [  50/1251 (  4%)]  Loss: 3.189 (3.35)  Time: 0.743s, 1377.64/s  (0.734s, 1394.95/s)  LR: 2.989e-04  Data: 0.009 (0.048)
Train: 382 [ 100/1251 (  8%)]  Loss: 3.457 (3.38)  Time: 0.694s, 1475.58/s  (0.715s, 1431.84/s)  LR: 2.989e-04  Data: 0.011 (0.029)
Train: 382 [ 150/1251 ( 12%)]  Loss: 3.553 (3.43)  Time: 0.675s, 1517.52/s  (0.707s, 1448.67/s)  LR: 2.989e-04  Data: 0.011 (0.023)
Train: 382 [ 200/1251 ( 16%)]  Loss: 3.727 (3.49)  Time: 0.673s, 1520.76/s  (0.701s, 1459.98/s)  LR: 2.989e-04  Data: 0.011 (0.020)
Train: 382 [ 250/1251 ( 20%)]  Loss: 3.553 (3.50)  Time: 0.703s, 1457.20/s  (0.699s, 1464.74/s)  LR: 2.989e-04  Data: 0.010 (0.018)
Train: 382 [ 300/1251 ( 24%)]  Loss: 3.930 (3.56)  Time: 0.758s, 1351.20/s  (0.698s, 1466.97/s)  LR: 2.989e-04  Data: 0.009 (0.017)
Train: 382 [ 350/1251 ( 28%)]  Loss: 3.251 (3.52)  Time: 0.670s, 1527.84/s  (0.696s, 1470.44/s)  LR: 2.989e-04  Data: 0.010 (0.016)
Train: 382 [ 400/1251 ( 32%)]  Loss: 3.288 (3.49)  Time: 0.673s, 1521.68/s  (0.697s, 1469.63/s)  LR: 2.989e-04  Data: 0.010 (0.015)
Train: 382 [ 450/1251 ( 36%)]  Loss: 3.083 (3.45)  Time: 0.701s, 1461.61/s  (0.697s, 1469.67/s)  LR: 2.989e-04  Data: 0.010 (0.015)
Train: 382 [ 500/1251 ( 40%)]  Loss: 3.592 (3.47)  Time: 0.706s, 1450.93/s  (0.696s, 1471.74/s)  LR: 2.989e-04  Data: 0.011 (0.014)
Train: 382 [ 550/1251 ( 44%)]  Loss: 3.142 (3.44)  Time: 0.671s, 1526.93/s  (0.696s, 1472.18/s)  LR: 2.989e-04  Data: 0.010 (0.014)
Train: 382 [ 600/1251 ( 48%)]  Loss: 3.796 (3.47)  Time: 0.733s, 1397.78/s  (0.695s, 1473.67/s)  LR: 2.989e-04  Data: 0.010 (0.014)
Train: 382 [ 650/1251 ( 52%)]  Loss: 3.586 (3.47)  Time: 0.732s, 1399.05/s  (0.694s, 1474.79/s)  LR: 2.989e-04  Data: 0.011 (0.013)
Train: 382 [ 700/1251 ( 56%)]  Loss: 3.204 (3.46)  Time: 0.673s, 1521.97/s  (0.694s, 1475.20/s)  LR: 2.989e-04  Data: 0.011 (0.013)
Train: 382 [ 750/1251 ( 60%)]  Loss: 3.645 (3.47)  Time: 0.674s, 1519.79/s  (0.694s, 1475.83/s)  LR: 2.989e-04  Data: 0.013 (0.013)
Train: 382 [ 800/1251 ( 64%)]  Loss: 3.496 (3.47)  Time: 0.673s, 1520.53/s  (0.694s, 1475.87/s)  LR: 2.989e-04  Data: 0.011 (0.013)
Train: 382 [ 850/1251 ( 68%)]  Loss: 3.371 (3.46)  Time: 0.671s, 1526.17/s  (0.694s, 1476.18/s)  LR: 2.989e-04  Data: 0.011 (0.013)
Train: 382 [ 900/1251 ( 72%)]  Loss: 3.633 (3.47)  Time: 0.673s, 1522.62/s  (0.694s, 1476.17/s)  LR: 2.989e-04  Data: 0.013 (0.013)
Train: 382 [ 950/1251 ( 76%)]  Loss: 3.505 (3.48)  Time: 0.717s, 1427.92/s  (0.693s, 1476.69/s)  LR: 2.989e-04  Data: 0.016 (0.013)
Train: 382 [1000/1251 ( 80%)]  Loss: 3.440 (3.47)  Time: 0.720s, 1422.87/s  (0.693s, 1476.89/s)  LR: 2.989e-04  Data: 0.010 (0.012)
Train: 382 [1050/1251 ( 84%)]  Loss: 3.467 (3.47)  Time: 0.717s, 1428.80/s  (0.693s, 1477.85/s)  LR: 2.989e-04  Data: 0.009 (0.012)
Train: 382 [1100/1251 ( 88%)]  Loss: 3.444 (3.47)  Time: 0.704s, 1454.87/s  (0.693s, 1478.08/s)  LR: 2.989e-04  Data: 0.009 (0.012)
Train: 382 [1150/1251 ( 92%)]  Loss: 3.178 (3.46)  Time: 0.673s, 1521.52/s  (0.693s, 1477.70/s)  LR: 2.989e-04  Data: 0.011 (0.012)
Train: 382 [1200/1251 ( 96%)]  Loss: 3.478 (3.46)  Time: 0.675s, 1517.62/s  (0.693s, 1478.42/s)  LR: 2.989e-04  Data: 0.011 (0.012)
Train: 382 [1250/1251 (100%)]  Loss: 3.550 (3.46)  Time: 0.676s, 1515.00/s  (0.693s, 1477.88/s)  LR: 2.989e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.474 (1.474)  Loss:  0.8184 (0.8184)  Acc@1: 89.5508 (89.5508)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.136 (0.585)  Loss:  1.0381 (1.3531)  Acc@1: 84.1981 (75.3620)  Acc@5: 95.9906 (92.6480)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-381.pth.tar', 75.44600000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-382.pth.tar', 75.36199998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-380.pth.tar', 75.31200004150391)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-378.pth.tar', 75.27799995361327)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-377.pth.tar', 75.23200000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-371.pth.tar', 75.10600014160157)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-376.pth.tar', 75.04199992675781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-370.pth.tar', 75.0360000341797)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-374.pth.tar', 75.01999998291015)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-366.pth.tar', 74.96599996582032)

Train: 383 [   0/1251 (  0%)]  Loss: 3.381 (3.38)  Time: 2.196s,  466.29/s  (2.196s,  466.29/s)  LR: 2.966e-04  Data: 1.535 (1.535)
Train: 383 [  50/1251 (  4%)]  Loss: 3.224 (3.30)  Time: 0.674s, 1520.15/s  (0.721s, 1419.56/s)  LR: 2.966e-04  Data: 0.010 (0.048)
Train: 383 [ 100/1251 (  8%)]  Loss: 3.586 (3.40)  Time: 0.709s, 1443.52/s  (0.707s, 1447.89/s)  LR: 2.966e-04  Data: 0.010 (0.029)
Train: 383 [ 150/1251 ( 12%)]  Loss: 3.769 (3.49)  Time: 0.674s, 1520.36/s  (0.700s, 1462.52/s)  LR: 2.966e-04  Data: 0.010 (0.023)
Train: 383 [ 200/1251 ( 16%)]  Loss: 3.672 (3.53)  Time: 0.729s, 1404.38/s  (0.699s, 1464.86/s)  LR: 2.966e-04  Data: 0.009 (0.020)
Train: 383 [ 250/1251 ( 20%)]  Loss: 3.164 (3.47)  Time: 0.698s, 1468.07/s  (0.699s, 1465.74/s)  LR: 2.966e-04  Data: 0.011 (0.018)
Train: 383 [ 300/1251 ( 24%)]  Loss: 3.675 (3.50)  Time: 0.737s, 1388.74/s  (0.697s, 1468.53/s)  LR: 2.966e-04  Data: 0.011 (0.017)
Train: 383 [ 350/1251 ( 28%)]  Loss: 3.451 (3.49)  Time: 0.688s, 1489.27/s  (0.696s, 1470.90/s)  LR: 2.966e-04  Data: 0.013 (0.016)
Train: 383 [ 400/1251 ( 32%)]  Loss: 3.538 (3.50)  Time: 0.701s, 1461.69/s  (0.695s, 1473.02/s)  LR: 2.966e-04  Data: 0.009 (0.015)
Train: 383 [ 450/1251 ( 36%)]  Loss: 3.557 (3.50)  Time: 0.761s, 1346.33/s  (0.695s, 1473.95/s)  LR: 2.966e-04  Data: 0.010 (0.015)
Train: 383 [ 500/1251 ( 40%)]  Loss: 3.519 (3.50)  Time: 0.670s, 1528.12/s  (0.694s, 1475.76/s)  LR: 2.966e-04  Data: 0.010 (0.014)
Train: 383 [ 550/1251 ( 44%)]  Loss: 3.679 (3.52)  Time: 0.673s, 1522.30/s  (0.693s, 1477.20/s)  LR: 2.966e-04  Data: 0.010 (0.014)
Train: 383 [ 600/1251 ( 48%)]  Loss: 3.702 (3.53)  Time: 0.671s, 1525.42/s  (0.694s, 1476.47/s)  LR: 2.966e-04  Data: 0.010 (0.014)
Train: 383 [ 650/1251 ( 52%)]  Loss: 3.659 (3.54)  Time: 0.691s, 1481.05/s  (0.694s, 1474.91/s)  LR: 2.966e-04  Data: 0.009 (0.013)
Train: 383 [ 700/1251 ( 56%)]  Loss: 3.613 (3.55)  Time: 0.673s, 1520.68/s  (0.694s, 1475.20/s)  LR: 2.966e-04  Data: 0.009 (0.013)
Train: 383 [ 750/1251 ( 60%)]  Loss: 3.545 (3.55)  Time: 0.706s, 1451.36/s  (0.694s, 1475.28/s)  LR: 2.966e-04  Data: 0.010 (0.013)
Train: 383 [ 800/1251 ( 64%)]  Loss: 3.416 (3.54)  Time: 0.676s, 1515.38/s  (0.694s, 1475.56/s)  LR: 2.966e-04  Data: 0.009 (0.013)
Train: 383 [ 850/1251 ( 68%)]  Loss: 3.465 (3.53)  Time: 0.680s, 1505.64/s  (0.694s, 1475.85/s)  LR: 2.966e-04  Data: 0.011 (0.013)
Train: 383 [ 900/1251 ( 72%)]  Loss: 3.429 (3.53)  Time: 0.666s, 1537.91/s  (0.694s, 1476.19/s)  LR: 2.966e-04  Data: 0.010 (0.012)
Train: 383 [ 950/1251 ( 76%)]  Loss: 3.641 (3.53)  Time: 0.673s, 1520.78/s  (0.693s, 1476.67/s)  LR: 2.966e-04  Data: 0.009 (0.012)
Train: 383 [1000/1251 ( 80%)]  Loss: 3.480 (3.53)  Time: 0.671s, 1525.62/s  (0.693s, 1477.92/s)  LR: 2.966e-04  Data: 0.009 (0.012)
Train: 383 [1050/1251 ( 84%)]  Loss: 3.416 (3.53)  Time: 0.696s, 1471.51/s  (0.693s, 1477.61/s)  LR: 2.966e-04  Data: 0.011 (0.012)
Train: 383 [1100/1251 ( 88%)]  Loss: 3.040 (3.51)  Time: 0.711s, 1439.87/s  (0.693s, 1477.89/s)  LR: 2.966e-04  Data: 0.010 (0.012)
Train: 383 [1150/1251 ( 92%)]  Loss: 3.782 (3.52)  Time: 0.673s, 1522.28/s  (0.693s, 1478.28/s)  LR: 2.966e-04  Data: 0.009 (0.012)
Train: 383 [1200/1251 ( 96%)]  Loss: 3.226 (3.51)  Time: 0.671s, 1525.58/s  (0.693s, 1478.48/s)  LR: 2.966e-04  Data: 0.009 (0.012)
Train: 383 [1250/1251 (100%)]  Loss: 3.668 (3.51)  Time: 0.691s, 1482.72/s  (0.693s, 1478.41/s)  LR: 2.966e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.530 (1.530)  Loss:  0.9053 (0.9053)  Acc@1: 90.5273 (90.5273)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.136 (0.584)  Loss:  1.0518 (1.4558)  Acc@1: 84.7877 (75.0000)  Acc@5: 95.8726 (92.5320)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-381.pth.tar', 75.44600000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-382.pth.tar', 75.36199998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-380.pth.tar', 75.31200004150391)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-378.pth.tar', 75.27799995361327)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-377.pth.tar', 75.23200000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-371.pth.tar', 75.10600014160157)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-376.pth.tar', 75.04199992675781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-370.pth.tar', 75.0360000341797)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-374.pth.tar', 75.01999998291015)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-383.pth.tar', 74.99999998535156)

Train: 384 [   0/1251 (  0%)]  Loss: 3.335 (3.34)  Time: 2.355s,  434.82/s  (2.355s,  434.82/s)  LR: 2.942e-04  Data: 1.739 (1.739)
Train: 384 [  50/1251 (  4%)]  Loss: 3.780 (3.56)  Time: 0.876s, 1168.43/s  (0.738s, 1387.50/s)  LR: 2.942e-04  Data: 0.011 (0.050)
Train: 384 [ 100/1251 (  8%)]  Loss: 3.689 (3.60)  Time: 0.680s, 1506.47/s  (0.714s, 1433.61/s)  LR: 2.942e-04  Data: 0.011 (0.030)
Train: 384 [ 150/1251 ( 12%)]  Loss: 3.928 (3.68)  Time: 0.708s, 1445.60/s  (0.706s, 1449.59/s)  LR: 2.942e-04  Data: 0.011 (0.024)
Train: 384 [ 200/1251 ( 16%)]  Loss: 3.682 (3.68)  Time: 0.705s, 1452.51/s  (0.702s, 1458.32/s)  LR: 2.942e-04  Data: 0.012 (0.020)
Train: 384 [ 250/1251 ( 20%)]  Loss: 3.740 (3.69)  Time: 0.708s, 1446.00/s  (0.700s, 1463.84/s)  LR: 2.942e-04  Data: 0.010 (0.018)
Train: 384 [ 300/1251 ( 24%)]  Loss: 3.334 (3.64)  Time: 0.729s, 1404.89/s  (0.699s, 1465.44/s)  LR: 2.942e-04  Data: 0.011 (0.017)
Train: 384 [ 350/1251 ( 28%)]  Loss: 3.782 (3.66)  Time: 0.704s, 1455.42/s  (0.699s, 1465.65/s)  LR: 2.942e-04  Data: 0.010 (0.016)
Train: 384 [ 400/1251 ( 32%)]  Loss: 3.393 (3.63)  Time: 0.704s, 1454.61/s  (0.698s, 1466.42/s)  LR: 2.942e-04  Data: 0.010 (0.015)
Train: 384 [ 450/1251 ( 36%)]  Loss: 3.625 (3.63)  Time: 0.666s, 1538.68/s  (0.698s, 1467.65/s)  LR: 2.942e-04  Data: 0.009 (0.015)
Train: 384 [ 500/1251 ( 40%)]  Loss: 3.287 (3.60)  Time: 0.674s, 1520.14/s  (0.697s, 1469.50/s)  LR: 2.942e-04  Data: 0.011 (0.014)
Train: 384 [ 550/1251 ( 44%)]  Loss: 3.698 (3.61)  Time: 0.671s, 1525.93/s  (0.696s, 1471.51/s)  LR: 2.942e-04  Data: 0.010 (0.014)
Train: 384 [ 600/1251 ( 48%)]  Loss: 3.607 (3.61)  Time: 0.675s, 1516.98/s  (0.695s, 1472.78/s)  LR: 2.942e-04  Data: 0.011 (0.014)
Train: 384 [ 650/1251 ( 52%)]  Loss: 3.574 (3.60)  Time: 0.674s, 1518.96/s  (0.695s, 1473.58/s)  LR: 2.942e-04  Data: 0.010 (0.013)
Train: 384 [ 700/1251 ( 56%)]  Loss: 3.782 (3.62)  Time: 0.671s, 1525.32/s  (0.695s, 1473.21/s)  LR: 2.942e-04  Data: 0.013 (0.013)
Train: 384 [ 750/1251 ( 60%)]  Loss: 3.855 (3.63)  Time: 0.683s, 1499.23/s  (0.695s, 1473.35/s)  LR: 2.942e-04  Data: 0.011 (0.013)
Train: 384 [ 800/1251 ( 64%)]  Loss: 3.427 (3.62)  Time: 0.676s, 1515.08/s  (0.695s, 1474.24/s)  LR: 2.942e-04  Data: 0.011 (0.013)
Train: 384 [ 850/1251 ( 68%)]  Loss: 3.479 (3.61)  Time: 0.725s, 1413.22/s  (0.694s, 1474.89/s)  LR: 2.942e-04  Data: 0.009 (0.013)
Train: 384 [ 900/1251 ( 72%)]  Loss: 3.564 (3.61)  Time: 0.673s, 1522.65/s  (0.694s, 1474.58/s)  LR: 2.942e-04  Data: 0.014 (0.013)
Train: 384 [ 950/1251 ( 76%)]  Loss: 3.241 (3.59)  Time: 0.691s, 1481.90/s  (0.694s, 1475.95/s)  LR: 2.942e-04  Data: 0.011 (0.012)
Train: 384 [1000/1251 ( 80%)]  Loss: 3.525 (3.59)  Time: 0.683s, 1498.83/s  (0.693s, 1476.83/s)  LR: 2.942e-04  Data: 0.012 (0.012)
Train: 384 [1050/1251 ( 84%)]  Loss: 2.951 (3.56)  Time: 0.698s, 1466.93/s  (0.693s, 1476.60/s)  LR: 2.942e-04  Data: 0.011 (0.012)
Train: 384 [1100/1251 ( 88%)]  Loss: 3.595 (3.56)  Time: 0.689s, 1486.45/s  (0.694s, 1475.86/s)  LR: 2.942e-04  Data: 0.012 (0.012)
Train: 384 [1150/1251 ( 92%)]  Loss: 3.572 (3.56)  Time: 0.673s, 1521.07/s  (0.694s, 1475.73/s)  LR: 2.942e-04  Data: 0.011 (0.012)
Train: 384 [1200/1251 ( 96%)]  Loss: 3.416 (3.55)  Time: 0.713s, 1436.62/s  (0.694s, 1476.41/s)  LR: 2.942e-04  Data: 0.008 (0.012)
Train: 384 [1250/1251 (100%)]  Loss: 3.491 (3.55)  Time: 0.656s, 1561.85/s  (0.694s, 1476.29/s)  LR: 2.942e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.782 (1.782)  Loss:  0.7998 (0.7998)  Acc@1: 91.0156 (91.0156)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.136 (0.583)  Loss:  0.8574 (1.3525)  Acc@1: 85.1415 (74.9900)  Acc@5: 97.2877 (92.6300)
Train: 385 [   0/1251 (  0%)]  Loss: 3.225 (3.23)  Time: 2.196s,  466.31/s  (2.196s,  466.31/s)  LR: 2.919e-04  Data: 1.583 (1.583)
Train: 385 [  50/1251 (  4%)]  Loss: 3.455 (3.34)  Time: 0.721s, 1420.66/s  (0.730s, 1402.11/s)  LR: 2.919e-04  Data: 0.010 (0.051)
Train: 385 [ 100/1251 (  8%)]  Loss: 3.541 (3.41)  Time: 0.686s, 1491.78/s  (0.710s, 1442.70/s)  LR: 2.919e-04  Data: 0.010 (0.030)
Train: 385 [ 150/1251 ( 12%)]  Loss: 3.684 (3.48)  Time: 0.667s, 1536.32/s  (0.702s, 1457.83/s)  LR: 2.919e-04  Data: 0.009 (0.024)
Train: 385 [ 200/1251 ( 16%)]  Loss: 3.541 (3.49)  Time: 0.711s, 1439.97/s  (0.700s, 1463.48/s)  LR: 2.919e-04  Data: 0.009 (0.020)
Train: 385 [ 250/1251 ( 20%)]  Loss: 3.417 (3.48)  Time: 0.674s, 1519.76/s  (0.697s, 1468.17/s)  LR: 2.919e-04  Data: 0.011 (0.018)
Train: 385 [ 300/1251 ( 24%)]  Loss: 3.593 (3.49)  Time: 0.795s, 1288.33/s  (0.697s, 1469.59/s)  LR: 2.919e-04  Data: 0.009 (0.017)
Train: 385 [ 350/1251 ( 28%)]  Loss: 3.521 (3.50)  Time: 0.671s, 1527.09/s  (0.696s, 1472.16/s)  LR: 2.919e-04  Data: 0.010 (0.016)
Train: 385 [ 400/1251 ( 32%)]  Loss: 3.725 (3.52)  Time: 0.685s, 1493.83/s  (0.696s, 1471.58/s)  LR: 2.919e-04  Data: 0.013 (0.015)
Train: 385 [ 450/1251 ( 36%)]  Loss: 3.437 (3.51)  Time: 0.700s, 1462.49/s  (0.697s, 1469.64/s)  LR: 2.919e-04  Data: 0.009 (0.015)
Train: 385 [ 500/1251 ( 40%)]  Loss: 3.503 (3.51)  Time: 0.669s, 1529.95/s  (0.697s, 1469.09/s)  LR: 2.919e-04  Data: 0.010 (0.014)
Train: 385 [ 550/1251 ( 44%)]  Loss: 3.199 (3.49)  Time: 0.673s, 1521.33/s  (0.696s, 1470.56/s)  LR: 2.919e-04  Data: 0.010 (0.014)
Train: 385 [ 600/1251 ( 48%)]  Loss: 3.239 (3.47)  Time: 0.671s, 1525.87/s  (0.696s, 1471.45/s)  LR: 2.919e-04  Data: 0.011 (0.014)
Train: 385 [ 650/1251 ( 52%)]  Loss: 3.072 (3.44)  Time: 0.671s, 1526.64/s  (0.695s, 1472.60/s)  LR: 2.919e-04  Data: 0.009 (0.013)
Train: 385 [ 700/1251 ( 56%)]  Loss: 3.715 (3.46)  Time: 0.672s, 1524.71/s  (0.696s, 1471.60/s)  LR: 2.919e-04  Data: 0.011 (0.013)
Train: 385 [ 750/1251 ( 60%)]  Loss: 3.649 (3.47)  Time: 0.701s, 1460.44/s  (0.696s, 1471.49/s)  LR: 2.919e-04  Data: 0.013 (0.013)
Train: 385 [ 800/1251 ( 64%)]  Loss: 3.602 (3.48)  Time: 0.765s, 1339.37/s  (0.696s, 1471.11/s)  LR: 2.919e-04  Data: 0.011 (0.013)
Train: 385 [ 850/1251 ( 68%)]  Loss: 3.587 (3.48)  Time: 0.671s, 1525.46/s  (0.696s, 1472.20/s)  LR: 2.919e-04  Data: 0.009 (0.013)
Train: 385 [ 900/1251 ( 72%)]  Loss: 3.612 (3.49)  Time: 0.695s, 1473.53/s  (0.695s, 1472.75/s)  LR: 2.919e-04  Data: 0.010 (0.013)
Train: 385 [ 950/1251 ( 76%)]  Loss: 3.648 (3.50)  Time: 0.739s, 1384.92/s  (0.695s, 1472.43/s)  LR: 2.919e-04  Data: 0.010 (0.013)
Train: 385 [1000/1251 ( 80%)]  Loss: 3.636 (3.50)  Time: 0.686s, 1493.62/s  (0.695s, 1472.50/s)  LR: 2.919e-04  Data: 0.009 (0.012)
Train: 385 [1050/1251 ( 84%)]  Loss: 3.340 (3.50)  Time: 0.668s, 1533.25/s  (0.695s, 1473.26/s)  LR: 2.919e-04  Data: 0.009 (0.012)
Train: 385 [1100/1251 ( 88%)]  Loss: 3.438 (3.49)  Time: 0.674s, 1520.23/s  (0.695s, 1473.85/s)  LR: 2.919e-04  Data: 0.010 (0.012)
Train: 385 [1150/1251 ( 92%)]  Loss: 3.785 (3.51)  Time: 0.674s, 1519.97/s  (0.695s, 1473.67/s)  LR: 2.919e-04  Data: 0.011 (0.012)
Train: 385 [1200/1251 ( 96%)]  Loss: 3.782 (3.52)  Time: 0.672s, 1524.16/s  (0.695s, 1473.94/s)  LR: 2.919e-04  Data: 0.009 (0.012)
Train: 385 [1250/1251 (100%)]  Loss: 3.492 (3.52)  Time: 0.701s, 1461.37/s  (0.695s, 1474.23/s)  LR: 2.919e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.564 (1.564)  Loss:  0.7744 (0.7744)  Acc@1: 88.9648 (88.9648)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.136 (0.580)  Loss:  0.9150 (1.2911)  Acc@1: 83.9623 (75.1160)  Acc@5: 96.5802 (92.5900)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-381.pth.tar', 75.44600000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-382.pth.tar', 75.36199998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-380.pth.tar', 75.31200004150391)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-378.pth.tar', 75.27799995361327)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-377.pth.tar', 75.23200000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-385.pth.tar', 75.11600001464844)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-371.pth.tar', 75.10600014160157)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-376.pth.tar', 75.04199992675781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-370.pth.tar', 75.0360000341797)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-374.pth.tar', 75.01999998291015)

Train: 386 [   0/1251 (  0%)]  Loss: 3.542 (3.54)  Time: 2.134s,  479.92/s  (2.134s,  479.92/s)  LR: 2.896e-04  Data: 1.519 (1.519)
Train: 386 [  50/1251 (  4%)]  Loss: 3.324 (3.43)  Time: 0.707s, 1448.99/s  (0.738s, 1387.09/s)  LR: 2.896e-04  Data: 0.010 (0.048)
Train: 386 [ 100/1251 (  8%)]  Loss: 3.480 (3.45)  Time: 0.695s, 1474.05/s  (0.715s, 1431.89/s)  LR: 2.896e-04  Data: 0.009 (0.030)
Train: 386 [ 150/1251 ( 12%)]  Loss: 3.967 (3.58)  Time: 0.678s, 1509.90/s  (0.707s, 1448.89/s)  LR: 2.896e-04  Data: 0.011 (0.023)
Train: 386 [ 200/1251 ( 16%)]  Loss: 3.637 (3.59)  Time: 0.704s, 1455.39/s  (0.702s, 1458.17/s)  LR: 2.896e-04  Data: 0.009 (0.020)
Train: 386 [ 250/1251 ( 20%)]  Loss: 3.760 (3.62)  Time: 0.672s, 1524.82/s  (0.699s, 1465.12/s)  LR: 2.896e-04  Data: 0.009 (0.018)
Train: 386 [ 300/1251 ( 24%)]  Loss: 3.014 (3.53)  Time: 0.667s, 1534.73/s  (0.698s, 1467.21/s)  LR: 2.896e-04  Data: 0.010 (0.017)
Train: 386 [ 350/1251 ( 28%)]  Loss: 3.272 (3.50)  Time: 0.673s, 1520.95/s  (0.697s, 1468.84/s)  LR: 2.896e-04  Data: 0.010 (0.016)
Train: 386 [ 400/1251 ( 32%)]  Loss: 3.573 (3.51)  Time: 0.675s, 1516.43/s  (0.697s, 1469.37/s)  LR: 2.896e-04  Data: 0.011 (0.015)
Train: 386 [ 450/1251 ( 36%)]  Loss: 3.523 (3.51)  Time: 0.698s, 1466.83/s  (0.696s, 1471.04/s)  LR: 2.896e-04  Data: 0.011 (0.015)
Train: 386 [ 500/1251 ( 40%)]  Loss: 3.453 (3.50)  Time: 0.693s, 1476.83/s  (0.695s, 1472.46/s)  LR: 2.896e-04  Data: 0.010 (0.014)
Train: 386 [ 550/1251 ( 44%)]  Loss: 3.502 (3.50)  Time: 0.674s, 1518.65/s  (0.695s, 1474.27/s)  LR: 2.896e-04  Data: 0.011 (0.014)
Train: 386 [ 600/1251 ( 48%)]  Loss: 3.355 (3.49)  Time: 0.691s, 1481.22/s  (0.694s, 1475.72/s)  LR: 2.896e-04  Data: 0.010 (0.014)
Train: 386 [ 650/1251 ( 52%)]  Loss: 3.608 (3.50)  Time: 0.700s, 1463.60/s  (0.694s, 1476.14/s)  LR: 2.896e-04  Data: 0.013 (0.013)
Train: 386 [ 700/1251 ( 56%)]  Loss: 3.244 (3.48)  Time: 0.706s, 1450.19/s  (0.694s, 1476.31/s)  LR: 2.896e-04  Data: 0.012 (0.013)
Train: 386 [ 750/1251 ( 60%)]  Loss: 3.275 (3.47)  Time: 0.701s, 1461.58/s  (0.694s, 1475.71/s)  LR: 2.896e-04  Data: 0.010 (0.013)
Train: 386 [ 800/1251 ( 64%)]  Loss: 3.831 (3.49)  Time: 0.674s, 1519.79/s  (0.693s, 1476.75/s)  LR: 2.896e-04  Data: 0.011 (0.013)
Train: 386 [ 850/1251 ( 68%)]  Loss: 3.660 (3.50)  Time: 0.679s, 1507.41/s  (0.693s, 1477.84/s)  LR: 2.896e-04  Data: 0.009 (0.013)
Train: 386 [ 900/1251 ( 72%)]  Loss: 3.680 (3.51)  Time: 0.671s, 1525.03/s  (0.693s, 1478.49/s)  LR: 2.896e-04  Data: 0.011 (0.013)
Train: 386 [ 950/1251 ( 76%)]  Loss: 3.313 (3.50)  Time: 0.717s, 1427.82/s  (0.693s, 1478.60/s)  LR: 2.896e-04  Data: 0.010 (0.012)
Train: 386 [1000/1251 ( 80%)]  Loss: 3.558 (3.50)  Time: 0.671s, 1526.13/s  (0.692s, 1479.41/s)  LR: 2.896e-04  Data: 0.011 (0.012)
Train: 386 [1050/1251 ( 84%)]  Loss: 3.830 (3.52)  Time: 0.708s, 1446.14/s  (0.692s, 1479.39/s)  LR: 2.896e-04  Data: 0.010 (0.012)
Train: 386 [1100/1251 ( 88%)]  Loss: 3.696 (3.53)  Time: 0.668s, 1533.50/s  (0.692s, 1479.62/s)  LR: 2.896e-04  Data: 0.009 (0.012)
Train: 386 [1150/1251 ( 92%)]  Loss: 3.827 (3.54)  Time: 0.718s, 1426.48/s  (0.692s, 1480.08/s)  LR: 2.896e-04  Data: 0.011 (0.012)
Train: 386 [1200/1251 ( 96%)]  Loss: 3.734 (3.55)  Time: 0.672s, 1522.72/s  (0.692s, 1480.26/s)  LR: 2.896e-04  Data: 0.011 (0.012)
Train: 386 [1250/1251 (100%)]  Loss: 3.579 (3.55)  Time: 0.656s, 1559.98/s  (0.692s, 1480.27/s)  LR: 2.896e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.618 (1.618)  Loss:  0.8677 (0.8677)  Acc@1: 88.5742 (88.5742)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.138 (0.577)  Loss:  0.8301 (1.3395)  Acc@1: 86.5566 (75.4640)  Acc@5: 97.4057 (92.7420)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-386.pth.tar', 75.46400010742188)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-381.pth.tar', 75.44600000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-382.pth.tar', 75.36199998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-380.pth.tar', 75.31200004150391)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-378.pth.tar', 75.27799995361327)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-377.pth.tar', 75.23200000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-385.pth.tar', 75.11600001464844)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-371.pth.tar', 75.10600014160157)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-376.pth.tar', 75.04199992675781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-370.pth.tar', 75.0360000341797)

Train: 387 [   0/1251 (  0%)]  Loss: 3.561 (3.56)  Time: 2.024s,  506.00/s  (2.024s,  506.00/s)  LR: 2.872e-04  Data: 1.407 (1.407)
Train: 387 [  50/1251 (  4%)]  Loss: 3.302 (3.43)  Time: 0.678s, 1509.54/s  (0.722s, 1419.11/s)  LR: 2.872e-04  Data: 0.010 (0.044)
Train: 387 [ 100/1251 (  8%)]  Loss: 3.826 (3.56)  Time: 0.675s, 1517.68/s  (0.707s, 1448.72/s)  LR: 2.872e-04  Data: 0.009 (0.027)
Train: 387 [ 150/1251 ( 12%)]  Loss: 3.356 (3.51)  Time: 0.751s, 1362.78/s  (0.705s, 1452.80/s)  LR: 2.872e-04  Data: 0.009 (0.022)
Train: 387 [ 200/1251 ( 16%)]  Loss: 3.547 (3.52)  Time: 0.704s, 1453.67/s  (0.701s, 1461.27/s)  LR: 2.872e-04  Data: 0.009 (0.019)
Train: 387 [ 250/1251 ( 20%)]  Loss: 3.517 (3.52)  Time: 0.671s, 1526.78/s  (0.699s, 1465.01/s)  LR: 2.872e-04  Data: 0.009 (0.017)
Train: 387 [ 300/1251 ( 24%)]  Loss: 3.682 (3.54)  Time: 0.671s, 1526.92/s  (0.698s, 1467.70/s)  LR: 2.872e-04  Data: 0.010 (0.016)
Train: 387 [ 350/1251 ( 28%)]  Loss: 3.429 (3.53)  Time: 0.740s, 1383.73/s  (0.698s, 1466.85/s)  LR: 2.872e-04  Data: 0.009 (0.015)
Train: 387 [ 400/1251 ( 32%)]  Loss: 3.462 (3.52)  Time: 0.706s, 1450.89/s  (0.697s, 1468.95/s)  LR: 2.872e-04  Data: 0.009 (0.015)
Train: 387 [ 450/1251 ( 36%)]  Loss: 3.732 (3.54)  Time: 0.763s, 1342.12/s  (0.696s, 1471.00/s)  LR: 2.872e-04  Data: 0.011 (0.014)
Train: 387 [ 500/1251 ( 40%)]  Loss: 3.757 (3.56)  Time: 0.703s, 1456.59/s  (0.696s, 1470.99/s)  LR: 2.872e-04  Data: 0.009 (0.014)
Train: 387 [ 550/1251 ( 44%)]  Loss: 3.422 (3.55)  Time: 0.670s, 1528.36/s  (0.696s, 1472.30/s)  LR: 2.872e-04  Data: 0.010 (0.013)
Train: 387 [ 600/1251 ( 48%)]  Loss: 3.428 (3.54)  Time: 0.672s, 1524.69/s  (0.695s, 1472.90/s)  LR: 2.872e-04  Data: 0.011 (0.013)
Train: 387 [ 650/1251 ( 52%)]  Loss: 3.639 (3.55)  Time: 0.706s, 1449.91/s  (0.695s, 1473.06/s)  LR: 2.872e-04  Data: 0.009 (0.013)
Train: 387 [ 700/1251 ( 56%)]  Loss: 3.116 (3.52)  Time: 0.689s, 1486.23/s  (0.695s, 1473.28/s)  LR: 2.872e-04  Data: 0.009 (0.013)
Train: 387 [ 750/1251 ( 60%)]  Loss: 3.484 (3.52)  Time: 0.672s, 1524.47/s  (0.695s, 1473.08/s)  LR: 2.872e-04  Data: 0.010 (0.013)
Train: 387 [ 800/1251 ( 64%)]  Loss: 3.979 (3.54)  Time: 0.699s, 1464.58/s  (0.695s, 1474.24/s)  LR: 2.872e-04  Data: 0.010 (0.013)
Train: 387 [ 850/1251 ( 68%)]  Loss: 3.252 (3.53)  Time: 0.671s, 1526.38/s  (0.695s, 1474.41/s)  LR: 2.872e-04  Data: 0.010 (0.012)
Train: 387 [ 900/1251 ( 72%)]  Loss: 3.442 (3.52)  Time: 0.714s, 1435.13/s  (0.694s, 1474.60/s)  LR: 2.872e-04  Data: 0.010 (0.012)
Train: 387 [ 950/1251 ( 76%)]  Loss: 3.438 (3.52)  Time: 0.671s, 1526.22/s  (0.694s, 1474.99/s)  LR: 2.872e-04  Data: 0.010 (0.012)
Train: 387 [1000/1251 ( 80%)]  Loss: 3.642 (3.52)  Time: 0.671s, 1526.03/s  (0.694s, 1475.67/s)  LR: 2.872e-04  Data: 0.010 (0.012)
Train: 387 [1050/1251 ( 84%)]  Loss: 3.714 (3.53)  Time: 0.673s, 1521.32/s  (0.694s, 1476.07/s)  LR: 2.872e-04  Data: 0.010 (0.012)
Train: 387 [1100/1251 ( 88%)]  Loss: 3.733 (3.54)  Time: 0.725s, 1412.99/s  (0.693s, 1476.60/s)  LR: 2.872e-04  Data: 0.009 (0.012)
Train: 387 [1150/1251 ( 92%)]  Loss: 3.612 (3.54)  Time: 0.706s, 1449.61/s  (0.693s, 1476.95/s)  LR: 2.872e-04  Data: 0.009 (0.012)
Train: 387 [1200/1251 ( 96%)]  Loss: 3.227 (3.53)  Time: 0.692s, 1480.24/s  (0.693s, 1477.05/s)  LR: 2.872e-04  Data: 0.011 (0.012)
Train: 387 [1250/1251 (100%)]  Loss: 3.306 (3.52)  Time: 0.655s, 1562.61/s  (0.693s, 1477.22/s)  LR: 2.872e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.589 (1.589)  Loss:  0.8467 (0.8467)  Acc@1: 89.6484 (89.6484)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.136 (0.578)  Loss:  0.9399 (1.3658)  Acc@1: 83.7264 (75.2380)  Acc@5: 96.5802 (92.5640)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-386.pth.tar', 75.46400010742188)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-381.pth.tar', 75.44600000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-382.pth.tar', 75.36199998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-380.pth.tar', 75.31200004150391)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-378.pth.tar', 75.27799995361327)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-387.pth.tar', 75.23799991210937)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-377.pth.tar', 75.23200000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-385.pth.tar', 75.11600001464844)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-371.pth.tar', 75.10600014160157)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-376.pth.tar', 75.04199992675781)

Train: 388 [   0/1251 (  0%)]  Loss: 3.585 (3.58)  Time: 2.200s,  465.43/s  (2.200s,  465.43/s)  LR: 2.849e-04  Data: 1.583 (1.583)
Train: 388 [  50/1251 (  4%)]  Loss: 3.775 (3.68)  Time: 0.699s, 1465.15/s  (0.727s, 1407.89/s)  LR: 2.849e-04  Data: 0.010 (0.048)
Train: 388 [ 100/1251 (  8%)]  Loss: 3.711 (3.69)  Time: 0.718s, 1426.20/s  (0.714s, 1434.52/s)  LR: 2.849e-04  Data: 0.009 (0.029)
Train: 388 [ 150/1251 ( 12%)]  Loss: 3.594 (3.67)  Time: 0.674s, 1518.22/s  (0.710s, 1442.31/s)  LR: 2.849e-04  Data: 0.011 (0.023)
Train: 388 [ 200/1251 ( 16%)]  Loss: 3.625 (3.66)  Time: 0.696s, 1471.17/s  (0.704s, 1453.59/s)  LR: 2.849e-04  Data: 0.011 (0.020)
Train: 388 [ 250/1251 ( 20%)]  Loss: 3.478 (3.63)  Time: 0.705s, 1451.58/s  (0.701s, 1461.38/s)  LR: 2.849e-04  Data: 0.011 (0.018)
Train: 388 [ 300/1251 ( 24%)]  Loss: 2.984 (3.54)  Time: 0.677s, 1511.84/s  (0.700s, 1462.28/s)  LR: 2.849e-04  Data: 0.010 (0.017)
Train: 388 [ 350/1251 ( 28%)]  Loss: 3.429 (3.52)  Time: 0.668s, 1534.01/s  (0.700s, 1463.59/s)  LR: 2.849e-04  Data: 0.012 (0.016)
Train: 388 [ 400/1251 ( 32%)]  Loss: 3.190 (3.49)  Time: 0.678s, 1509.55/s  (0.699s, 1464.64/s)  LR: 2.849e-04  Data: 0.011 (0.015)
Train: 388 [ 450/1251 ( 36%)]  Loss: 3.487 (3.49)  Time: 0.707s, 1447.38/s  (0.698s, 1466.02/s)  LR: 2.849e-04  Data: 0.012 (0.015)
Train: 388 [ 500/1251 ( 40%)]  Loss: 3.545 (3.49)  Time: 0.671s, 1526.25/s  (0.698s, 1466.47/s)  LR: 2.849e-04  Data: 0.011 (0.014)
Train: 388 [ 550/1251 ( 44%)]  Loss: 3.562 (3.50)  Time: 0.724s, 1415.26/s  (0.698s, 1466.78/s)  LR: 2.849e-04  Data: 0.010 (0.014)
Train: 388 [ 600/1251 ( 48%)]  Loss: 3.952 (3.53)  Time: 0.721s, 1420.96/s  (0.698s, 1467.52/s)  LR: 2.849e-04  Data: 0.010 (0.014)
Train: 388 [ 650/1251 ( 52%)]  Loss: 3.363 (3.52)  Time: 0.691s, 1482.70/s  (0.697s, 1468.40/s)  LR: 2.849e-04  Data: 0.013 (0.013)
Train: 388 [ 700/1251 ( 56%)]  Loss: 3.481 (3.52)  Time: 0.674s, 1518.89/s  (0.697s, 1469.34/s)  LR: 2.849e-04  Data: 0.012 (0.013)
Train: 388 [ 750/1251 ( 60%)]  Loss: 3.511 (3.52)  Time: 0.681s, 1504.13/s  (0.696s, 1470.49/s)  LR: 2.849e-04  Data: 0.010 (0.013)
Train: 388 [ 800/1251 ( 64%)]  Loss: 3.589 (3.52)  Time: 0.671s, 1525.18/s  (0.696s, 1471.39/s)  LR: 2.849e-04  Data: 0.011 (0.013)
Train: 388 [ 850/1251 ( 68%)]  Loss: 3.444 (3.52)  Time: 0.680s, 1506.40/s  (0.696s, 1472.16/s)  LR: 2.849e-04  Data: 0.011 (0.013)
Train: 388 [ 900/1251 ( 72%)]  Loss: 3.287 (3.50)  Time: 0.702s, 1457.97/s  (0.695s, 1472.47/s)  LR: 2.849e-04  Data: 0.009 (0.012)
Train: 388 [ 950/1251 ( 76%)]  Loss: 3.339 (3.50)  Time: 0.672s, 1523.51/s  (0.695s, 1473.32/s)  LR: 2.849e-04  Data: 0.011 (0.012)
Train: 388 [1000/1251 ( 80%)]  Loss: 3.481 (3.50)  Time: 0.734s, 1395.87/s  (0.695s, 1473.00/s)  LR: 2.849e-04  Data: 0.011 (0.012)
Train: 388 [1050/1251 ( 84%)]  Loss: 3.874 (3.51)  Time: 0.707s, 1448.85/s  (0.695s, 1473.83/s)  LR: 2.849e-04  Data: 0.010 (0.012)
Train: 388 [1100/1251 ( 88%)]  Loss: 3.626 (3.52)  Time: 0.676s, 1514.10/s  (0.694s, 1474.72/s)  LR: 2.849e-04  Data: 0.010 (0.012)
Train: 388 [1150/1251 ( 92%)]  Loss: 3.275 (3.51)  Time: 0.671s, 1527.09/s  (0.694s, 1475.43/s)  LR: 2.849e-04  Data: 0.010 (0.012)
Train: 388 [1200/1251 ( 96%)]  Loss: 3.360 (3.50)  Time: 0.706s, 1450.88/s  (0.694s, 1475.13/s)  LR: 2.849e-04  Data: 0.009 (0.012)
Train: 388 [1250/1251 (100%)]  Loss: 3.182 (3.49)  Time: 0.658s, 1555.53/s  (0.694s, 1475.43/s)  LR: 2.849e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.449 (1.449)  Loss:  0.7031 (0.7031)  Acc@1: 90.2344 (90.2344)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.136 (0.569)  Loss:  0.9019 (1.3065)  Acc@1: 84.7877 (75.0960)  Acc@5: 96.5802 (92.5220)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-386.pth.tar', 75.46400010742188)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-381.pth.tar', 75.44600000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-382.pth.tar', 75.36199998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-380.pth.tar', 75.31200004150391)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-378.pth.tar', 75.27799995361327)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-387.pth.tar', 75.23799991210937)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-377.pth.tar', 75.23200000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-385.pth.tar', 75.11600001464844)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-371.pth.tar', 75.10600014160157)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-388.pth.tar', 75.09599998535157)

Train: 389 [   0/1251 (  0%)]  Loss: 3.664 (3.66)  Time: 2.197s,  466.09/s  (2.197s,  466.09/s)  LR: 2.826e-04  Data: 1.581 (1.581)
Train: 389 [  50/1251 (  4%)]  Loss: 3.646 (3.66)  Time: 0.709s, 1444.44/s  (0.727s, 1409.28/s)  LR: 2.826e-04  Data: 0.011 (0.045)
Train: 389 [ 100/1251 (  8%)]  Loss: 3.112 (3.47)  Time: 0.665s, 1540.21/s  (0.709s, 1444.93/s)  LR: 2.826e-04  Data: 0.008 (0.028)
Train: 389 [ 150/1251 ( 12%)]  Loss: 3.602 (3.51)  Time: 0.694s, 1476.06/s  (0.703s, 1456.65/s)  LR: 2.826e-04  Data: 0.009 (0.022)
Train: 389 [ 200/1251 ( 16%)]  Loss: 3.506 (3.51)  Time: 0.702s, 1459.12/s  (0.699s, 1464.76/s)  LR: 2.826e-04  Data: 0.009 (0.019)
Train: 389 [ 250/1251 ( 20%)]  Loss: 3.538 (3.51)  Time: 0.672s, 1524.46/s  (0.696s, 1472.14/s)  LR: 2.826e-04  Data: 0.010 (0.017)
Train: 389 [ 300/1251 ( 24%)]  Loss: 2.951 (3.43)  Time: 0.695s, 1472.76/s  (0.694s, 1475.07/s)  LR: 2.826e-04  Data: 0.013 (0.016)
Train: 389 [ 350/1251 ( 28%)]  Loss: 3.217 (3.40)  Time: 0.713s, 1436.63/s  (0.693s, 1477.25/s)  LR: 2.826e-04  Data: 0.009 (0.015)
Train: 389 [ 400/1251 ( 32%)]  Loss: 3.600 (3.43)  Time: 0.673s, 1521.93/s  (0.693s, 1476.57/s)  LR: 2.826e-04  Data: 0.010 (0.015)
Train: 389 [ 450/1251 ( 36%)]  Loss: 3.711 (3.45)  Time: 0.683s, 1500.23/s  (0.694s, 1476.38/s)  LR: 2.826e-04  Data: 0.010 (0.014)
Train: 389 [ 500/1251 ( 40%)]  Loss: 3.459 (3.46)  Time: 0.670s, 1528.58/s  (0.693s, 1477.11/s)  LR: 2.826e-04  Data: 0.010 (0.014)
Train: 389 [ 550/1251 ( 44%)]  Loss: 3.309 (3.44)  Time: 0.670s, 1527.37/s  (0.693s, 1478.61/s)  LR: 2.826e-04  Data: 0.010 (0.014)
Train: 389 [ 600/1251 ( 48%)]  Loss: 3.775 (3.47)  Time: 0.678s, 1510.95/s  (0.693s, 1478.65/s)  LR: 2.826e-04  Data: 0.010 (0.013)
Train: 389 [ 650/1251 ( 52%)]  Loss: 3.635 (3.48)  Time: 0.711s, 1440.82/s  (0.693s, 1478.44/s)  LR: 2.826e-04  Data: 0.011 (0.013)
Train: 389 [ 700/1251 ( 56%)]  Loss: 3.395 (3.47)  Time: 0.720s, 1421.47/s  (0.693s, 1477.74/s)  LR: 2.826e-04  Data: 0.010 (0.013)
Train: 389 [ 750/1251 ( 60%)]  Loss: 3.369 (3.47)  Time: 0.670s, 1528.45/s  (0.693s, 1478.41/s)  LR: 2.826e-04  Data: 0.011 (0.013)
Train: 389 [ 800/1251 ( 64%)]  Loss: 3.251 (3.46)  Time: 0.673s, 1521.97/s  (0.692s, 1479.10/s)  LR: 2.826e-04  Data: 0.010 (0.013)
Train: 389 [ 850/1251 ( 68%)]  Loss: 3.535 (3.46)  Time: 0.701s, 1460.63/s  (0.692s, 1478.80/s)  LR: 2.826e-04  Data: 0.010 (0.012)
Train: 389 [ 900/1251 ( 72%)]  Loss: 3.570 (3.47)  Time: 0.749s, 1368.06/s  (0.693s, 1478.58/s)  LR: 2.826e-04  Data: 0.009 (0.012)
Train: 389 [ 950/1251 ( 76%)]  Loss: 3.767 (3.48)  Time: 0.673s, 1521.44/s  (0.693s, 1478.23/s)  LR: 2.826e-04  Data: 0.010 (0.012)
Train: 389 [1000/1251 ( 80%)]  Loss: 3.448 (3.48)  Time: 0.754s, 1358.10/s  (0.692s, 1479.20/s)  LR: 2.826e-04  Data: 0.009 (0.012)
Train: 389 [1050/1251 ( 84%)]  Loss: 3.914 (3.50)  Time: 0.717s, 1428.57/s  (0.692s, 1479.23/s)  LR: 2.826e-04  Data: 0.011 (0.012)
Train: 389 [1100/1251 ( 88%)]  Loss: 3.114 (3.48)  Time: 0.705s, 1452.09/s  (0.692s, 1479.48/s)  LR: 2.826e-04  Data: 0.008 (0.012)
Train: 389 [1150/1251 ( 92%)]  Loss: 3.416 (3.48)  Time: 0.699s, 1464.32/s  (0.692s, 1479.54/s)  LR: 2.826e-04  Data: 0.009 (0.012)
Train: 389 [1200/1251 ( 96%)]  Loss: 3.290 (3.47)  Time: 0.671s, 1525.78/s  (0.692s, 1479.99/s)  LR: 2.826e-04  Data: 0.010 (0.012)
Train: 389 [1250/1251 (100%)]  Loss: 3.488 (3.47)  Time: 0.681s, 1503.30/s  (0.692s, 1479.76/s)  LR: 2.826e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.529 (1.529)  Loss:  0.7529 (0.7529)  Acc@1: 91.0156 (91.0156)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.136 (0.574)  Loss:  0.8096 (1.3028)  Acc@1: 85.8491 (75.5760)  Acc@5: 96.9340 (92.8280)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-389.pth.tar', 75.57600005859375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-386.pth.tar', 75.46400010742188)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-381.pth.tar', 75.44600000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-382.pth.tar', 75.36199998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-380.pth.tar', 75.31200004150391)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-378.pth.tar', 75.27799995361327)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-387.pth.tar', 75.23799991210937)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-377.pth.tar', 75.23200000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-385.pth.tar', 75.11600001464844)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-371.pth.tar', 75.10600014160157)

Train: 390 [   0/1251 (  0%)]  Loss: 3.388 (3.39)  Time: 2.160s,  474.03/s  (2.160s,  474.03/s)  LR: 2.803e-04  Data: 1.519 (1.519)
Train: 390 [  50/1251 (  4%)]  Loss: 3.268 (3.33)  Time: 0.685s, 1495.64/s  (0.720s, 1422.09/s)  LR: 2.803e-04  Data: 0.010 (0.046)
Train: 390 [ 100/1251 (  8%)]  Loss: 3.629 (3.43)  Time: 0.672s, 1523.38/s  (0.702s, 1458.59/s)  LR: 2.803e-04  Data: 0.011 (0.028)
Train: 390 [ 150/1251 ( 12%)]  Loss: 3.623 (3.48)  Time: 0.673s, 1520.48/s  (0.698s, 1466.03/s)  LR: 2.803e-04  Data: 0.009 (0.022)
Train: 390 [ 200/1251 ( 16%)]  Loss: 3.739 (3.53)  Time: 0.704s, 1455.50/s  (0.697s, 1469.24/s)  LR: 2.803e-04  Data: 0.013 (0.019)
Train: 390 [ 250/1251 ( 20%)]  Loss: 3.431 (3.51)  Time: 0.736s, 1391.43/s  (0.697s, 1468.67/s)  LR: 2.803e-04  Data: 0.011 (0.018)
Train: 390 [ 300/1251 ( 24%)]  Loss: 3.475 (3.51)  Time: 0.672s, 1522.86/s  (0.696s, 1470.51/s)  LR: 2.803e-04  Data: 0.010 (0.016)
Train: 390 [ 350/1251 ( 28%)]  Loss: 3.821 (3.55)  Time: 0.671s, 1526.91/s  (0.696s, 1472.00/s)  LR: 2.803e-04  Data: 0.011 (0.016)
Train: 390 [ 400/1251 ( 32%)]  Loss: 3.717 (3.57)  Time: 0.710s, 1441.84/s  (0.695s, 1474.27/s)  LR: 2.803e-04  Data: 0.009 (0.015)
Train: 390 [ 450/1251 ( 36%)]  Loss: 3.462 (3.56)  Time: 0.679s, 1508.10/s  (0.695s, 1473.36/s)  LR: 2.803e-04  Data: 0.011 (0.014)
Train: 390 [ 500/1251 ( 40%)]  Loss: 3.840 (3.58)  Time: 0.676s, 1515.34/s  (0.695s, 1474.32/s)  LR: 2.803e-04  Data: 0.010 (0.014)
Train: 390 [ 550/1251 ( 44%)]  Loss: 3.326 (3.56)  Time: 0.673s, 1521.03/s  (0.694s, 1474.55/s)  LR: 2.803e-04  Data: 0.011 (0.014)
Train: 390 [ 600/1251 ( 48%)]  Loss: 3.572 (3.56)  Time: 0.688s, 1487.92/s  (0.694s, 1476.23/s)  LR: 2.803e-04  Data: 0.009 (0.013)
Train: 390 [ 650/1251 ( 52%)]  Loss: 3.407 (3.55)  Time: 0.673s, 1521.96/s  (0.694s, 1476.57/s)  LR: 2.803e-04  Data: 0.010 (0.013)
Train: 390 [ 700/1251 ( 56%)]  Loss: 3.287 (3.53)  Time: 0.671s, 1526.25/s  (0.693s, 1477.00/s)  LR: 2.803e-04  Data: 0.011 (0.013)
Train: 390 [ 750/1251 ( 60%)]  Loss: 3.320 (3.52)  Time: 0.673s, 1520.69/s  (0.693s, 1476.99/s)  LR: 2.803e-04  Data: 0.010 (0.013)
Train: 390 [ 800/1251 ( 64%)]  Loss: 3.878 (3.54)  Time: 0.668s, 1532.92/s  (0.693s, 1477.44/s)  LR: 2.803e-04  Data: 0.010 (0.013)
Train: 390 [ 850/1251 ( 68%)]  Loss: 3.411 (3.53)  Time: 0.697s, 1469.49/s  (0.693s, 1477.40/s)  LR: 2.803e-04  Data: 0.009 (0.012)
Train: 390 [ 900/1251 ( 72%)]  Loss: 3.838 (3.55)  Time: 0.706s, 1449.65/s  (0.693s, 1477.53/s)  LR: 2.803e-04  Data: 0.010 (0.012)
Train: 390 [ 950/1251 ( 76%)]  Loss: 3.383 (3.54)  Time: 0.692s, 1479.99/s  (0.693s, 1477.99/s)  LR: 2.803e-04  Data: 0.010 (0.012)
Train: 390 [1000/1251 ( 80%)]  Loss: 3.508 (3.54)  Time: 0.728s, 1406.78/s  (0.693s, 1477.95/s)  LR: 2.803e-04  Data: 0.010 (0.012)
Train: 390 [1050/1251 ( 84%)]  Loss: 3.580 (3.54)  Time: 0.718s, 1426.64/s  (0.693s, 1477.36/s)  LR: 2.803e-04  Data: 0.009 (0.012)
Train: 390 [1100/1251 ( 88%)]  Loss: 3.408 (3.54)  Time: 0.710s, 1441.62/s  (0.694s, 1475.67/s)  LR: 2.803e-04  Data: 0.010 (0.012)
Train: 390 [1150/1251 ( 92%)]  Loss: 3.273 (3.52)  Time: 0.697s, 1469.21/s  (0.695s, 1473.77/s)  LR: 2.803e-04  Data: 0.013 (0.012)
Train: 390 [1200/1251 ( 96%)]  Loss: 3.801 (3.54)  Time: 0.674s, 1519.57/s  (0.695s, 1473.32/s)  LR: 2.803e-04  Data: 0.011 (0.012)
Train: 390 [1250/1251 (100%)]  Loss: 3.258 (3.52)  Time: 0.655s, 1562.77/s  (0.695s, 1474.18/s)  LR: 2.803e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.484 (1.484)  Loss:  0.7280 (0.7280)  Acc@1: 91.4062 (91.4062)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.136 (0.545)  Loss:  0.8267 (1.2986)  Acc@1: 85.7311 (75.7140)  Acc@5: 96.2264 (92.7440)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-390.pth.tar', 75.71400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-389.pth.tar', 75.57600005859375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-386.pth.tar', 75.46400010742188)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-381.pth.tar', 75.44600000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-382.pth.tar', 75.36199998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-380.pth.tar', 75.31200004150391)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-378.pth.tar', 75.27799995361327)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-387.pth.tar', 75.23799991210937)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-377.pth.tar', 75.23200000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-385.pth.tar', 75.11600001464844)

Train: 391 [   0/1251 (  0%)]  Loss: 3.644 (3.64)  Time: 2.173s,  471.30/s  (2.173s,  471.30/s)  LR: 2.780e-04  Data: 1.524 (1.524)
Train: 391 [  50/1251 (  4%)]  Loss: 3.450 (3.55)  Time: 0.707s, 1448.05/s  (0.715s, 1432.08/s)  LR: 2.780e-04  Data: 0.009 (0.048)
Train: 391 [ 100/1251 (  8%)]  Loss: 3.546 (3.55)  Time: 0.676s, 1515.37/s  (0.701s, 1461.15/s)  LR: 2.780e-04  Data: 0.010 (0.029)
Train: 391 [ 150/1251 ( 12%)]  Loss: 3.390 (3.51)  Time: 0.794s, 1288.91/s  (0.700s, 1463.11/s)  LR: 2.780e-04  Data: 0.011 (0.023)
Train: 391 [ 200/1251 ( 16%)]  Loss: 3.548 (3.52)  Time: 0.749s, 1367.36/s  (0.697s, 1469.80/s)  LR: 2.780e-04  Data: 0.010 (0.020)
Train: 391 [ 250/1251 ( 20%)]  Loss: 3.264 (3.47)  Time: 0.670s, 1527.73/s  (0.695s, 1473.22/s)  LR: 2.780e-04  Data: 0.010 (0.018)
Train: 391 [ 300/1251 ( 24%)]  Loss: 3.225 (3.44)  Time: 0.673s, 1521.38/s  (0.695s, 1473.98/s)  LR: 2.780e-04  Data: 0.011 (0.017)
Train: 391 [ 350/1251 ( 28%)]  Loss: 2.968 (3.38)  Time: 0.704s, 1455.53/s  (0.696s, 1471.93/s)  LR: 2.780e-04  Data: 0.009 (0.016)
Train: 391 [ 400/1251 ( 32%)]  Loss: 3.318 (3.37)  Time: 0.690s, 1483.33/s  (0.695s, 1473.65/s)  LR: 2.780e-04  Data: 0.013 (0.015)
Train: 391 [ 450/1251 ( 36%)]  Loss: 3.526 (3.39)  Time: 0.766s, 1336.77/s  (0.694s, 1475.33/s)  LR: 2.780e-04  Data: 0.010 (0.015)
Train: 391 [ 500/1251 ( 40%)]  Loss: 3.495 (3.40)  Time: 0.676s, 1514.68/s  (0.693s, 1476.76/s)  LR: 2.780e-04  Data: 0.010 (0.014)
Train: 391 [ 550/1251 ( 44%)]  Loss: 3.228 (3.38)  Time: 0.670s, 1528.62/s  (0.694s, 1476.29/s)  LR: 2.780e-04  Data: 0.011 (0.014)
Train: 391 [ 600/1251 ( 48%)]  Loss: 3.755 (3.41)  Time: 0.677s, 1511.79/s  (0.694s, 1475.50/s)  LR: 2.780e-04  Data: 0.010 (0.014)
Train: 391 [ 650/1251 ( 52%)]  Loss: 3.445 (3.41)  Time: 0.680s, 1506.67/s  (0.694s, 1476.32/s)  LR: 2.780e-04  Data: 0.010 (0.013)
Train: 391 [ 700/1251 ( 56%)]  Loss: 3.239 (3.40)  Time: 0.670s, 1528.14/s  (0.693s, 1477.75/s)  LR: 2.780e-04  Data: 0.009 (0.013)
Train: 391 [ 750/1251 ( 60%)]  Loss: 3.737 (3.42)  Time: 0.679s, 1508.51/s  (0.693s, 1478.57/s)  LR: 2.780e-04  Data: 0.009 (0.013)
Train: 391 [ 800/1251 ( 64%)]  Loss: 3.444 (3.42)  Time: 0.705s, 1452.13/s  (0.693s, 1477.94/s)  LR: 2.780e-04  Data: 0.009 (0.013)
Train: 391 [ 850/1251 ( 68%)]  Loss: 3.661 (3.44)  Time: 0.676s, 1515.87/s  (0.692s, 1478.80/s)  LR: 2.780e-04  Data: 0.013 (0.013)
Train: 391 [ 900/1251 ( 72%)]  Loss: 3.225 (3.43)  Time: 0.712s, 1438.49/s  (0.693s, 1478.55/s)  LR: 2.780e-04  Data: 0.009 (0.013)
Train: 391 [ 950/1251 ( 76%)]  Loss: 3.597 (3.44)  Time: 0.672s, 1524.45/s  (0.692s, 1478.83/s)  LR: 2.780e-04  Data: 0.009 (0.012)
Train: 391 [1000/1251 ( 80%)]  Loss: 3.681 (3.45)  Time: 0.681s, 1504.58/s  (0.692s, 1478.75/s)  LR: 2.780e-04  Data: 0.010 (0.012)
Train: 391 [1050/1251 ( 84%)]  Loss: 3.261 (3.44)  Time: 0.671s, 1526.20/s  (0.692s, 1478.90/s)  LR: 2.780e-04  Data: 0.010 (0.012)
Train: 391 [1100/1251 ( 88%)]  Loss: 3.567 (3.44)  Time: 0.715s, 1431.52/s  (0.692s, 1478.70/s)  LR: 2.780e-04  Data: 0.009 (0.012)
Train: 391 [1150/1251 ( 92%)]  Loss: 3.500 (3.45)  Time: 0.673s, 1521.78/s  (0.692s, 1479.38/s)  LR: 2.780e-04  Data: 0.010 (0.012)
Train: 391 [1200/1251 ( 96%)]  Loss: 3.816 (3.46)  Time: 0.680s, 1506.67/s  (0.692s, 1479.71/s)  LR: 2.780e-04  Data: 0.009 (0.012)
Train: 391 [1250/1251 (100%)]  Loss: 3.650 (3.47)  Time: 0.657s, 1557.85/s  (0.692s, 1479.76/s)  LR: 2.780e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.591 (1.591)  Loss:  0.8184 (0.8184)  Acc@1: 90.7227 (90.7227)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.137 (0.578)  Loss:  0.9370 (1.3517)  Acc@1: 85.7311 (75.4180)  Acc@5: 96.4623 (92.6800)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-390.pth.tar', 75.71400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-389.pth.tar', 75.57600005859375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-386.pth.tar', 75.46400010742188)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-381.pth.tar', 75.44600000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-391.pth.tar', 75.41800000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-382.pth.tar', 75.36199998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-380.pth.tar', 75.31200004150391)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-378.pth.tar', 75.27799995361327)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-387.pth.tar', 75.23799991210937)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-377.pth.tar', 75.23200000732422)

Train: 392 [   0/1251 (  0%)]  Loss: 3.385 (3.39)  Time: 2.295s,  446.18/s  (2.295s,  446.18/s)  LR: 2.757e-04  Data: 1.680 (1.680)
Train: 392 [  50/1251 (  4%)]  Loss: 3.687 (3.54)  Time: 0.705s, 1452.19/s  (0.744s, 1376.52/s)  LR: 2.757e-04  Data: 0.011 (0.050)
Train: 392 [ 100/1251 (  8%)]  Loss: 3.509 (3.53)  Time: 0.667s, 1534.51/s  (0.717s, 1427.88/s)  LR: 2.757e-04  Data: 0.011 (0.030)
Train: 392 [ 150/1251 ( 12%)]  Loss: 3.514 (3.52)  Time: 0.676s, 1514.78/s  (0.707s, 1448.66/s)  LR: 2.757e-04  Data: 0.011 (0.024)
Train: 392 [ 200/1251 ( 16%)]  Loss: 3.371 (3.49)  Time: 0.669s, 1530.87/s  (0.703s, 1456.99/s)  LR: 2.757e-04  Data: 0.011 (0.020)
Train: 392 [ 250/1251 ( 20%)]  Loss: 3.524 (3.50)  Time: 0.673s, 1522.37/s  (0.700s, 1463.34/s)  LR: 2.757e-04  Data: 0.011 (0.018)
Train: 392 [ 300/1251 ( 24%)]  Loss: 3.220 (3.46)  Time: 0.673s, 1522.66/s  (0.698s, 1467.05/s)  LR: 2.757e-04  Data: 0.011 (0.017)
Train: 392 [ 350/1251 ( 28%)]  Loss: 3.467 (3.46)  Time: 0.751s, 1364.25/s  (0.698s, 1467.36/s)  LR: 2.757e-04  Data: 0.010 (0.016)
Train: 392 [ 400/1251 ( 32%)]  Loss: 3.634 (3.48)  Time: 0.665s, 1538.98/s  (0.697s, 1468.23/s)  LR: 2.757e-04  Data: 0.010 (0.015)
Train: 392 [ 450/1251 ( 36%)]  Loss: 3.415 (3.47)  Time: 0.706s, 1450.01/s  (0.697s, 1468.30/s)  LR: 2.757e-04  Data: 0.010 (0.015)
Train: 392 [ 500/1251 ( 40%)]  Loss: 3.431 (3.47)  Time: 0.673s, 1520.54/s  (0.697s, 1468.91/s)  LR: 2.757e-04  Data: 0.013 (0.015)
Train: 392 [ 550/1251 ( 44%)]  Loss: 3.403 (3.46)  Time: 0.672s, 1524.94/s  (0.696s, 1470.50/s)  LR: 2.757e-04  Data: 0.011 (0.014)
Train: 392 [ 600/1251 ( 48%)]  Loss: 3.626 (3.48)  Time: 0.668s, 1532.60/s  (0.696s, 1470.71/s)  LR: 2.757e-04  Data: 0.009 (0.014)
Train: 392 [ 650/1251 ( 52%)]  Loss: 3.607 (3.49)  Time: 0.714s, 1434.12/s  (0.696s, 1471.41/s)  LR: 2.757e-04  Data: 0.009 (0.014)
Train: 392 [ 700/1251 ( 56%)]  Loss: 3.240 (3.47)  Time: 0.701s, 1461.06/s  (0.696s, 1471.65/s)  LR: 2.757e-04  Data: 0.009 (0.013)
Train: 392 [ 750/1251 ( 60%)]  Loss: 3.777 (3.49)  Time: 0.705s, 1451.68/s  (0.695s, 1473.24/s)  LR: 2.757e-04  Data: 0.011 (0.013)
Train: 392 [ 800/1251 ( 64%)]  Loss: 3.387 (3.48)  Time: 0.702s, 1458.79/s  (0.695s, 1473.05/s)  LR: 2.757e-04  Data: 0.010 (0.013)
Train: 392 [ 850/1251 ( 68%)]  Loss: 3.444 (3.48)  Time: 0.677s, 1512.41/s  (0.695s, 1473.66/s)  LR: 2.757e-04  Data: 0.011 (0.013)
Train: 392 [ 900/1251 ( 72%)]  Loss: 3.368 (3.47)  Time: 0.679s, 1508.52/s  (0.695s, 1473.54/s)  LR: 2.757e-04  Data: 0.015 (0.013)
Train: 392 [ 950/1251 ( 76%)]  Loss: 3.092 (3.46)  Time: 0.692s, 1480.38/s  (0.694s, 1474.67/s)  LR: 2.757e-04  Data: 0.035 (0.013)
Train: 392 [1000/1251 ( 80%)]  Loss: 3.451 (3.45)  Time: 0.685s, 1495.07/s  (0.694s, 1474.90/s)  LR: 2.757e-04  Data: 0.010 (0.013)
Train: 392 [1050/1251 ( 84%)]  Loss: 3.646 (3.46)  Time: 0.704s, 1453.52/s  (0.694s, 1475.40/s)  LR: 2.757e-04  Data: 0.010 (0.012)
Train: 392 [1100/1251 ( 88%)]  Loss: 3.469 (3.46)  Time: 0.689s, 1485.95/s  (0.694s, 1475.99/s)  LR: 2.757e-04  Data: 0.011 (0.012)
Train: 392 [1150/1251 ( 92%)]  Loss: 3.420 (3.46)  Time: 0.684s, 1496.80/s  (0.694s, 1476.41/s)  LR: 2.757e-04  Data: 0.011 (0.012)
Train: 392 [1200/1251 ( 96%)]  Loss: 3.812 (3.48)  Time: 0.671s, 1526.77/s  (0.693s, 1476.89/s)  LR: 2.757e-04  Data: 0.011 (0.012)
Train: 392 [1250/1251 (100%)]  Loss: 3.724 (3.49)  Time: 0.698s, 1466.30/s  (0.694s, 1476.35/s)  LR: 2.757e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.647 (1.647)  Loss:  0.8677 (0.8677)  Acc@1: 90.4297 (90.4297)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.136 (0.579)  Loss:  0.8389 (1.4160)  Acc@1: 86.6745 (75.0680)  Acc@5: 96.8160 (92.4820)
Train: 393 [   0/1251 (  0%)]  Loss: 3.643 (3.64)  Time: 2.198s,  465.84/s  (2.198s,  465.84/s)  LR: 2.734e-04  Data: 1.533 (1.533)
Train: 393 [  50/1251 (  4%)]  Loss: 3.583 (3.61)  Time: 0.732s, 1398.40/s  (0.722s, 1419.09/s)  LR: 2.734e-04  Data: 0.010 (0.044)
Train: 393 [ 100/1251 (  8%)]  Loss: 3.096 (3.44)  Time: 0.672s, 1524.33/s  (0.708s, 1446.75/s)  LR: 2.734e-04  Data: 0.010 (0.027)
Train: 393 [ 150/1251 ( 12%)]  Loss: 3.518 (3.46)  Time: 0.701s, 1460.87/s  (0.702s, 1458.65/s)  LR: 2.734e-04  Data: 0.009 (0.022)
Train: 393 [ 200/1251 ( 16%)]  Loss: 3.671 (3.50)  Time: 0.678s, 1510.76/s  (0.701s, 1460.68/s)  LR: 2.734e-04  Data: 0.010 (0.019)
Train: 393 [ 250/1251 ( 20%)]  Loss: 3.512 (3.50)  Time: 0.740s, 1383.20/s  (0.699s, 1465.98/s)  LR: 2.734e-04  Data: 0.010 (0.017)
Train: 393 [ 300/1251 ( 24%)]  Loss: 3.325 (3.48)  Time: 0.830s, 1233.45/s  (0.698s, 1467.41/s)  LR: 2.734e-04  Data: 0.009 (0.016)
Train: 393 [ 350/1251 ( 28%)]  Loss: 3.706 (3.51)  Time: 0.678s, 1511.24/s  (0.697s, 1468.49/s)  LR: 2.734e-04  Data: 0.011 (0.015)
Train: 393 [ 400/1251 ( 32%)]  Loss: 3.236 (3.48)  Time: 0.714s, 1433.59/s  (0.696s, 1470.61/s)  LR: 2.734e-04  Data: 0.009 (0.015)
Train: 393 [ 450/1251 ( 36%)]  Loss: 3.533 (3.48)  Time: 0.701s, 1460.68/s  (0.696s, 1471.78/s)  LR: 2.734e-04  Data: 0.009 (0.014)
Train: 393 [ 500/1251 ( 40%)]  Loss: 3.647 (3.50)  Time: 0.754s, 1357.89/s  (0.695s, 1472.43/s)  LR: 2.734e-04  Data: 0.010 (0.014)
Train: 393 [ 550/1251 ( 44%)]  Loss: 3.374 (3.49)  Time: 0.687s, 1490.22/s  (0.695s, 1473.23/s)  LR: 2.734e-04  Data: 0.011 (0.013)
Train: 393 [ 600/1251 ( 48%)]  Loss: 3.333 (3.48)  Time: 0.706s, 1450.28/s  (0.695s, 1472.66/s)  LR: 2.734e-04  Data: 0.012 (0.013)
Train: 393 [ 650/1251 ( 52%)]  Loss: 3.376 (3.47)  Time: 0.671s, 1525.56/s  (0.695s, 1473.68/s)  LR: 2.734e-04  Data: 0.011 (0.013)
Train: 393 [ 700/1251 ( 56%)]  Loss: 3.578 (3.48)  Time: 0.672s, 1523.36/s  (0.695s, 1473.96/s)  LR: 2.734e-04  Data: 0.010 (0.013)
Train: 393 [ 750/1251 ( 60%)]  Loss: 3.693 (3.49)  Time: 0.667s, 1536.13/s  (0.694s, 1474.54/s)  LR: 2.734e-04  Data: 0.010 (0.013)
Train: 393 [ 800/1251 ( 64%)]  Loss: 3.320 (3.48)  Time: 0.684s, 1497.90/s  (0.694s, 1475.18/s)  LR: 2.734e-04  Data: 0.015 (0.013)
Train: 393 [ 850/1251 ( 68%)]  Loss: 3.637 (3.49)  Time: 0.675s, 1516.24/s  (0.694s, 1475.38/s)  LR: 2.734e-04  Data: 0.009 (0.012)
Train: 393 [ 900/1251 ( 72%)]  Loss: 3.387 (3.48)  Time: 0.678s, 1509.26/s  (0.694s, 1475.75/s)  LR: 2.734e-04  Data: 0.010 (0.012)
Train: 393 [ 950/1251 ( 76%)]  Loss: 3.295 (3.47)  Time: 0.673s, 1521.94/s  (0.694s, 1476.27/s)  LR: 2.734e-04  Data: 0.010 (0.012)
Train: 393 [1000/1251 ( 80%)]  Loss: 3.493 (3.47)  Time: 0.671s, 1527.18/s  (0.694s, 1476.56/s)  LR: 2.734e-04  Data: 0.011 (0.012)
Train: 393 [1050/1251 ( 84%)]  Loss: 3.630 (3.48)  Time: 0.670s, 1528.22/s  (0.693s, 1477.43/s)  LR: 2.734e-04  Data: 0.010 (0.012)
Train: 393 [1100/1251 ( 88%)]  Loss: 3.632 (3.49)  Time: 0.753s, 1360.56/s  (0.693s, 1477.13/s)  LR: 2.734e-04  Data: 0.009 (0.012)
Train: 393 [1150/1251 ( 92%)]  Loss: 3.237 (3.48)  Time: 0.671s, 1524.96/s  (0.693s, 1477.58/s)  LR: 2.734e-04  Data: 0.010 (0.012)
Train: 393 [1200/1251 ( 96%)]  Loss: 3.593 (3.48)  Time: 0.683s, 1499.22/s  (0.693s, 1477.31/s)  LR: 2.734e-04  Data: 0.010 (0.012)
Train: 393 [1250/1251 (100%)]  Loss: 4.014 (3.50)  Time: 0.656s, 1560.41/s  (0.693s, 1477.22/s)  LR: 2.734e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.500 (1.500)  Loss:  0.7480 (0.7480)  Acc@1: 91.2109 (91.2109)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.136 (0.584)  Loss:  0.8896 (1.3509)  Acc@1: 86.6745 (75.7420)  Acc@5: 96.1085 (92.6800)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-393.pth.tar', 75.74200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-390.pth.tar', 75.71400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-389.pth.tar', 75.57600005859375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-386.pth.tar', 75.46400010742188)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-381.pth.tar', 75.44600000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-391.pth.tar', 75.41800000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-382.pth.tar', 75.36199998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-380.pth.tar', 75.31200004150391)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-378.pth.tar', 75.27799995361327)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-387.pth.tar', 75.23799991210937)

Train: 394 [   0/1251 (  0%)]  Loss: 3.638 (3.64)  Time: 2.364s,  433.16/s  (2.364s,  433.16/s)  LR: 2.711e-04  Data: 1.703 (1.703)
Train: 394 [  50/1251 (  4%)]  Loss: 3.547 (3.59)  Time: 0.674s, 1519.02/s  (0.738s, 1388.33/s)  LR: 2.711e-04  Data: 0.011 (0.051)
Train: 394 [ 100/1251 (  8%)]  Loss: 3.410 (3.53)  Time: 0.701s, 1460.02/s  (0.716s, 1430.63/s)  LR: 2.711e-04  Data: 0.010 (0.031)
Train: 394 [ 150/1251 ( 12%)]  Loss: 3.744 (3.58)  Time: 0.671s, 1525.07/s  (0.709s, 1445.03/s)  LR: 2.711e-04  Data: 0.010 (0.024)
Train: 394 [ 200/1251 ( 16%)]  Loss: 3.350 (3.54)  Time: 0.728s, 1407.03/s  (0.706s, 1450.17/s)  LR: 2.711e-04  Data: 0.009 (0.021)
Train: 394 [ 250/1251 ( 20%)]  Loss: 3.373 (3.51)  Time: 0.672s, 1523.66/s  (0.703s, 1456.47/s)  LR: 2.711e-04  Data: 0.009 (0.019)
Train: 394 [ 300/1251 ( 24%)]  Loss: 3.267 (3.48)  Time: 0.671s, 1525.26/s  (0.702s, 1458.22/s)  LR: 2.711e-04  Data: 0.009 (0.017)
Train: 394 [ 350/1251 ( 28%)]  Loss: 3.452 (3.47)  Time: 0.678s, 1511.10/s  (0.701s, 1461.71/s)  LR: 2.711e-04  Data: 0.009 (0.016)
Train: 394 [ 400/1251 ( 32%)]  Loss: 3.424 (3.47)  Time: 0.708s, 1445.46/s  (0.699s, 1464.95/s)  LR: 2.711e-04  Data: 0.010 (0.016)
Train: 394 [ 450/1251 ( 36%)]  Loss: 3.410 (3.46)  Time: 0.672s, 1524.67/s  (0.699s, 1465.80/s)  LR: 2.711e-04  Data: 0.011 (0.015)
Train: 394 [ 500/1251 ( 40%)]  Loss: 3.258 (3.44)  Time: 0.672s, 1523.28/s  (0.698s, 1467.93/s)  LR: 2.711e-04  Data: 0.012 (0.015)
Train: 394 [ 550/1251 ( 44%)]  Loss: 3.565 (3.45)  Time: 0.711s, 1440.09/s  (0.697s, 1469.19/s)  LR: 2.711e-04  Data: 0.009 (0.014)
Train: 394 [ 600/1251 ( 48%)]  Loss: 3.552 (3.46)  Time: 0.714s, 1434.74/s  (0.696s, 1470.46/s)  LR: 2.711e-04  Data: 0.011 (0.014)
Train: 394 [ 650/1251 ( 52%)]  Loss: 3.688 (3.48)  Time: 0.673s, 1521.67/s  (0.696s, 1471.08/s)  LR: 2.711e-04  Data: 0.011 (0.014)
Train: 394 [ 700/1251 ( 56%)]  Loss: 3.697 (3.49)  Time: 0.700s, 1463.03/s  (0.696s, 1472.05/s)  LR: 2.711e-04  Data: 0.009 (0.013)
Train: 394 [ 750/1251 ( 60%)]  Loss: 3.269 (3.48)  Time: 0.723s, 1416.97/s  (0.696s, 1471.11/s)  LR: 2.711e-04  Data: 0.010 (0.013)
Train: 394 [ 800/1251 ( 64%)]  Loss: 3.781 (3.50)  Time: 0.700s, 1462.79/s  (0.696s, 1471.63/s)  LR: 2.711e-04  Data: 0.011 (0.013)
Train: 394 [ 850/1251 ( 68%)]  Loss: 3.420 (3.49)  Time: 0.716s, 1429.56/s  (0.696s, 1471.81/s)  LR: 2.711e-04  Data: 0.008 (0.013)
Train: 394 [ 900/1251 ( 72%)]  Loss: 3.620 (3.50)  Time: 0.672s, 1523.32/s  (0.695s, 1472.37/s)  LR: 2.711e-04  Data: 0.012 (0.013)
Train: 394 [ 950/1251 ( 76%)]  Loss: 3.730 (3.51)  Time: 0.671s, 1525.77/s  (0.696s, 1472.03/s)  LR: 2.711e-04  Data: 0.010 (0.013)
Train: 394 [1000/1251 ( 80%)]  Loss: 3.913 (3.53)  Time: 0.700s, 1463.68/s  (0.696s, 1471.71/s)  LR: 2.711e-04  Data: 0.011 (0.013)
Train: 394 [1050/1251 ( 84%)]  Loss: 3.501 (3.53)  Time: 0.704s, 1454.09/s  (0.695s, 1472.65/s)  LR: 2.711e-04  Data: 0.010 (0.012)
Train: 394 [1100/1251 ( 88%)]  Loss: 3.533 (3.53)  Time: 0.685s, 1494.80/s  (0.695s, 1472.77/s)  LR: 2.711e-04  Data: 0.011 (0.012)
Train: 394 [1150/1251 ( 92%)]  Loss: 3.798 (3.54)  Time: 0.683s, 1498.75/s  (0.695s, 1473.44/s)  LR: 2.711e-04  Data: 0.010 (0.012)
Train: 394 [1200/1251 ( 96%)]  Loss: 3.519 (3.54)  Time: 0.734s, 1395.19/s  (0.695s, 1473.53/s)  LR: 2.711e-04  Data: 0.011 (0.012)
Train: 394 [1250/1251 (100%)]  Loss: 3.521 (3.54)  Time: 0.658s, 1555.56/s  (0.695s, 1473.54/s)  LR: 2.711e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.482 (1.482)  Loss:  0.7358 (0.7358)  Acc@1: 90.8203 (90.8203)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.136 (0.591)  Loss:  0.9429 (1.2871)  Acc@1: 83.6085 (75.2480)  Acc@5: 95.9906 (92.5880)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-393.pth.tar', 75.74200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-390.pth.tar', 75.71400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-389.pth.tar', 75.57600005859375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-386.pth.tar', 75.46400010742188)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-381.pth.tar', 75.44600000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-391.pth.tar', 75.41800000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-382.pth.tar', 75.36199998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-380.pth.tar', 75.31200004150391)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-378.pth.tar', 75.27799995361327)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-394.pth.tar', 75.24800011962891)

Train: 395 [   0/1251 (  0%)]  Loss: 3.772 (3.77)  Time: 2.393s,  427.89/s  (2.393s,  427.89/s)  LR: 2.688e-04  Data: 1.780 (1.780)
Train: 395 [  50/1251 (  4%)]  Loss: 3.724 (3.75)  Time: 0.744s, 1377.14/s  (0.746s, 1372.53/s)  LR: 2.688e-04  Data: 0.009 (0.058)
Train: 395 [ 100/1251 (  8%)]  Loss: 3.361 (3.62)  Time: 0.671s, 1525.93/s  (0.721s, 1419.50/s)  LR: 2.688e-04  Data: 0.009 (0.034)
Train: 395 [ 150/1251 ( 12%)]  Loss: 3.566 (3.61)  Time: 0.666s, 1536.99/s  (0.714s, 1433.51/s)  LR: 2.688e-04  Data: 0.010 (0.026)
Train: 395 [ 200/1251 ( 16%)]  Loss: 3.387 (3.56)  Time: 0.705s, 1452.01/s  (0.709s, 1444.64/s)  LR: 2.688e-04  Data: 0.009 (0.022)
Train: 395 [ 250/1251 ( 20%)]  Loss: 3.849 (3.61)  Time: 0.704s, 1453.92/s  (0.705s, 1451.88/s)  LR: 2.688e-04  Data: 0.010 (0.020)
Train: 395 [ 300/1251 ( 24%)]  Loss: 3.481 (3.59)  Time: 0.711s, 1441.03/s  (0.702s, 1458.45/s)  LR: 2.688e-04  Data: 0.010 (0.018)
Train: 395 [ 350/1251 ( 28%)]  Loss: 3.542 (3.59)  Time: 0.677s, 1512.41/s  (0.700s, 1462.49/s)  LR: 2.688e-04  Data: 0.016 (0.017)
Train: 395 [ 400/1251 ( 32%)]  Loss: 3.498 (3.58)  Time: 0.679s, 1507.94/s  (0.699s, 1464.66/s)  LR: 2.688e-04  Data: 0.010 (0.016)
Train: 395 [ 450/1251 ( 36%)]  Loss: 3.537 (3.57)  Time: 0.676s, 1514.73/s  (0.698s, 1466.96/s)  LR: 2.688e-04  Data: 0.013 (0.016)
Train: 395 [ 500/1251 ( 40%)]  Loss: 3.405 (3.56)  Time: 0.672s, 1522.93/s  (0.697s, 1468.21/s)  LR: 2.688e-04  Data: 0.011 (0.015)
Train: 395 [ 550/1251 ( 44%)]  Loss: 3.571 (3.56)  Time: 0.675s, 1517.76/s  (0.697s, 1468.33/s)  LR: 2.688e-04  Data: 0.010 (0.015)
Train: 395 [ 600/1251 ( 48%)]  Loss: 3.684 (3.57)  Time: 0.670s, 1527.22/s  (0.696s, 1470.37/s)  LR: 2.688e-04  Data: 0.010 (0.014)
Train: 395 [ 650/1251 ( 52%)]  Loss: 3.311 (3.55)  Time: 0.708s, 1446.25/s  (0.697s, 1470.14/s)  LR: 2.688e-04  Data: 0.010 (0.014)
Train: 395 [ 700/1251 ( 56%)]  Loss: 3.531 (3.55)  Time: 0.671s, 1525.07/s  (0.696s, 1470.96/s)  LR: 2.688e-04  Data: 0.012 (0.014)
Train: 395 [ 750/1251 ( 60%)]  Loss: 3.668 (3.56)  Time: 0.678s, 1509.61/s  (0.696s, 1471.07/s)  LR: 2.688e-04  Data: 0.010 (0.014)
Train: 395 [ 800/1251 ( 64%)]  Loss: 3.735 (3.57)  Time: 0.704s, 1453.98/s  (0.696s, 1471.80/s)  LR: 2.688e-04  Data: 0.010 (0.013)
Train: 395 [ 850/1251 ( 68%)]  Loss: 3.576 (3.57)  Time: 0.706s, 1449.76/s  (0.696s, 1471.65/s)  LR: 2.688e-04  Data: 0.011 (0.013)
Train: 395 [ 900/1251 ( 72%)]  Loss: 3.383 (3.56)  Time: 0.671s, 1525.09/s  (0.695s, 1472.36/s)  LR: 2.688e-04  Data: 0.010 (0.013)
Train: 395 [ 950/1251 ( 76%)]  Loss: 3.266 (3.54)  Time: 0.671s, 1526.90/s  (0.695s, 1473.45/s)  LR: 2.688e-04  Data: 0.010 (0.013)
Train: 395 [1000/1251 ( 80%)]  Loss: 3.311 (3.53)  Time: 0.671s, 1525.56/s  (0.695s, 1473.11/s)  LR: 2.688e-04  Data: 0.010 (0.013)
Train: 395 [1050/1251 ( 84%)]  Loss: 3.675 (3.54)  Time: 0.703s, 1456.01/s  (0.695s, 1473.82/s)  LR: 2.688e-04  Data: 0.010 (0.013)
Train: 395 [1100/1251 ( 88%)]  Loss: 3.834 (3.55)  Time: 0.671s, 1526.73/s  (0.695s, 1473.58/s)  LR: 2.688e-04  Data: 0.010 (0.013)
Train: 395 [1150/1251 ( 92%)]  Loss: 3.388 (3.54)  Time: 0.675s, 1517.56/s  (0.695s, 1474.05/s)  LR: 2.688e-04  Data: 0.009 (0.013)
Train: 395 [1200/1251 ( 96%)]  Loss: 3.137 (3.53)  Time: 0.671s, 1525.70/s  (0.694s, 1474.52/s)  LR: 2.688e-04  Data: 0.011 (0.012)
Train: 395 [1250/1251 (100%)]  Loss: 3.507 (3.53)  Time: 0.692s, 1478.79/s  (0.694s, 1474.72/s)  LR: 2.688e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.443 (1.443)  Loss:  0.9717 (0.9717)  Acc@1: 89.9414 (89.9414)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.136 (0.588)  Loss:  1.0303 (1.4325)  Acc@1: 84.4340 (75.1200)  Acc@5: 96.2264 (92.6340)
Train: 396 [   0/1251 (  0%)]  Loss: 3.288 (3.29)  Time: 2.227s,  459.83/s  (2.227s,  459.83/s)  LR: 2.665e-04  Data: 1.611 (1.611)
Train: 396 [  50/1251 (  4%)]  Loss: 3.515 (3.40)  Time: 0.673s, 1521.17/s  (0.739s, 1385.11/s)  LR: 2.665e-04  Data: 0.011 (0.050)
Train: 396 [ 100/1251 (  8%)]  Loss: 3.330 (3.38)  Time: 0.681s, 1503.32/s  (0.715s, 1431.82/s)  LR: 2.665e-04  Data: 0.011 (0.030)
Train: 396 [ 150/1251 ( 12%)]  Loss: 3.324 (3.36)  Time: 0.681s, 1502.73/s  (0.706s, 1450.12/s)  LR: 2.665e-04  Data: 0.011 (0.024)
Train: 396 [ 200/1251 ( 16%)]  Loss: 3.483 (3.39)  Time: 0.712s, 1437.37/s  (0.703s, 1456.64/s)  LR: 2.665e-04  Data: 0.009 (0.021)
Train: 396 [ 250/1251 ( 20%)]  Loss: 3.606 (3.42)  Time: 0.672s, 1523.69/s  (0.701s, 1461.51/s)  LR: 2.665e-04  Data: 0.010 (0.019)
Train: 396 [ 300/1251 ( 24%)]  Loss: 3.132 (3.38)  Time: 0.702s, 1458.90/s  (0.700s, 1463.55/s)  LR: 2.665e-04  Data: 0.009 (0.017)
Train: 396 [ 350/1251 ( 28%)]  Loss: 3.558 (3.40)  Time: 0.703s, 1457.58/s  (0.698s, 1466.70/s)  LR: 2.665e-04  Data: 0.009 (0.016)
Train: 396 [ 400/1251 ( 32%)]  Loss: 3.361 (3.40)  Time: 0.672s, 1522.90/s  (0.698s, 1467.14/s)  LR: 2.665e-04  Data: 0.011 (0.016)
Train: 396 [ 450/1251 ( 36%)]  Loss: 3.352 (3.39)  Time: 0.704s, 1454.59/s  (0.698s, 1467.93/s)  LR: 2.665e-04  Data: 0.009 (0.015)
Train: 396 [ 500/1251 ( 40%)]  Loss: 3.213 (3.38)  Time: 0.742s, 1379.44/s  (0.697s, 1469.69/s)  LR: 2.665e-04  Data: 0.013 (0.014)
Train: 396 [ 550/1251 ( 44%)]  Loss: 3.504 (3.39)  Time: 0.680s, 1506.31/s  (0.697s, 1469.93/s)  LR: 2.665e-04  Data: 0.016 (0.014)
Train: 396 [ 600/1251 ( 48%)]  Loss: 3.861 (3.43)  Time: 0.695s, 1473.38/s  (0.696s, 1471.04/s)  LR: 2.665e-04  Data: 0.011 (0.014)
Train: 396 [ 650/1251 ( 52%)]  Loss: 3.392 (3.42)  Time: 0.671s, 1525.41/s  (0.696s, 1472.21/s)  LR: 2.665e-04  Data: 0.011 (0.014)
Train: 396 [ 700/1251 ( 56%)]  Loss: 3.495 (3.43)  Time: 0.702s, 1459.37/s  (0.695s, 1472.73/s)  LR: 2.665e-04  Data: 0.010 (0.013)
Train: 396 [ 750/1251 ( 60%)]  Loss: 3.179 (3.41)  Time: 0.670s, 1527.69/s  (0.695s, 1473.40/s)  LR: 2.665e-04  Data: 0.009 (0.013)
Train: 396 [ 800/1251 ( 64%)]  Loss: 3.355 (3.41)  Time: 0.670s, 1528.64/s  (0.695s, 1473.81/s)  LR: 2.665e-04  Data: 0.012 (0.013)
Train: 396 [ 850/1251 ( 68%)]  Loss: 3.880 (3.43)  Time: 0.714s, 1435.15/s  (0.695s, 1473.98/s)  LR: 2.665e-04  Data: 0.010 (0.013)
Train: 396 [ 900/1251 ( 72%)]  Loss: 3.351 (3.43)  Time: 0.708s, 1445.96/s  (0.694s, 1474.72/s)  LR: 2.665e-04  Data: 0.009 (0.013)
Train: 396 [ 950/1251 ( 76%)]  Loss: 3.226 (3.42)  Time: 0.673s, 1521.57/s  (0.695s, 1474.05/s)  LR: 2.665e-04  Data: 0.014 (0.013)
Train: 396 [1000/1251 ( 80%)]  Loss: 3.206 (3.41)  Time: 0.685s, 1494.01/s  (0.694s, 1475.11/s)  LR: 2.665e-04  Data: 0.008 (0.012)
Train: 396 [1050/1251 ( 84%)]  Loss: 3.305 (3.41)  Time: 0.701s, 1460.22/s  (0.694s, 1474.82/s)  LR: 2.665e-04  Data: 0.010 (0.012)
Train: 396 [1100/1251 ( 88%)]  Loss: 3.697 (3.42)  Time: 0.709s, 1444.38/s  (0.694s, 1474.82/s)  LR: 2.665e-04  Data: 0.009 (0.012)
Train: 396 [1150/1251 ( 92%)]  Loss: 3.639 (3.43)  Time: 0.704s, 1454.38/s  (0.694s, 1475.03/s)  LR: 2.665e-04  Data: 0.009 (0.012)
Train: 396 [1200/1251 ( 96%)]  Loss: 3.326 (3.42)  Time: 0.673s, 1522.12/s  (0.694s, 1475.71/s)  LR: 2.665e-04  Data: 0.011 (0.012)
Train: 396 [1250/1251 (100%)]  Loss: 3.834 (3.44)  Time: 0.657s, 1557.94/s  (0.694s, 1475.61/s)  LR: 2.665e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.557 (1.557)  Loss:  0.8926 (0.8926)  Acc@1: 90.4297 (90.4297)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.137 (0.579)  Loss:  0.9243 (1.4221)  Acc@1: 84.9057 (75.5580)  Acc@5: 95.7547 (92.7180)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-393.pth.tar', 75.74200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-390.pth.tar', 75.71400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-389.pth.tar', 75.57600005859375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-396.pth.tar', 75.5580000366211)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-386.pth.tar', 75.46400010742188)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-381.pth.tar', 75.44600000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-391.pth.tar', 75.41800000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-382.pth.tar', 75.36199998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-380.pth.tar', 75.31200004150391)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-378.pth.tar', 75.27799995361327)

Train: 397 [   0/1251 (  0%)]  Loss: 3.410 (3.41)  Time: 2.174s,  470.94/s  (2.174s,  470.94/s)  LR: 2.643e-04  Data: 1.520 (1.520)
Train: 397 [  50/1251 (  4%)]  Loss: 3.667 (3.54)  Time: 0.711s, 1439.64/s  (0.727s, 1408.30/s)  LR: 2.643e-04  Data: 0.011 (0.048)
Train: 397 [ 100/1251 (  8%)]  Loss: 3.391 (3.49)  Time: 0.699s, 1464.36/s  (0.710s, 1442.12/s)  LR: 2.643e-04  Data: 0.011 (0.030)
Train: 397 [ 150/1251 ( 12%)]  Loss: 3.682 (3.54)  Time: 0.672s, 1524.38/s  (0.702s, 1458.57/s)  LR: 2.643e-04  Data: 0.011 (0.023)
Train: 397 [ 200/1251 ( 16%)]  Loss: 3.444 (3.52)  Time: 0.671s, 1525.42/s  (0.698s, 1467.52/s)  LR: 2.643e-04  Data: 0.010 (0.020)
Train: 397 [ 250/1251 ( 20%)]  Loss: 2.965 (3.43)  Time: 0.720s, 1422.46/s  (0.695s, 1472.49/s)  LR: 2.643e-04  Data: 0.010 (0.018)
Train: 397 [ 300/1251 ( 24%)]  Loss: 3.280 (3.41)  Time: 0.750s, 1365.63/s  (0.694s, 1474.60/s)  LR: 2.643e-04  Data: 0.011 (0.017)
Train: 397 [ 350/1251 ( 28%)]  Loss: 3.330 (3.40)  Time: 0.706s, 1449.48/s  (0.694s, 1474.65/s)  LR: 2.643e-04  Data: 0.010 (0.016)
Train: 397 [ 400/1251 ( 32%)]  Loss: 3.702 (3.43)  Time: 0.672s, 1524.85/s  (0.694s, 1476.06/s)  LR: 2.643e-04  Data: 0.010 (0.015)
Train: 397 [ 450/1251 ( 36%)]  Loss: 3.828 (3.47)  Time: 0.698s, 1466.83/s  (0.693s, 1477.86/s)  LR: 2.643e-04  Data: 0.009 (0.015)
Train: 397 [ 500/1251 ( 40%)]  Loss: 3.678 (3.49)  Time: 0.690s, 1484.25/s  (0.693s, 1478.32/s)  LR: 2.643e-04  Data: 0.009 (0.014)
Train: 397 [ 550/1251 ( 44%)]  Loss: 3.256 (3.47)  Time: 0.673s, 1522.43/s  (0.693s, 1478.48/s)  LR: 2.643e-04  Data: 0.010 (0.014)
Train: 397 [ 600/1251 ( 48%)]  Loss: 3.740 (3.49)  Time: 0.714s, 1433.29/s  (0.693s, 1478.64/s)  LR: 2.643e-04  Data: 0.009 (0.014)
Train: 397 [ 650/1251 ( 52%)]  Loss: 3.683 (3.50)  Time: 0.669s, 1529.67/s  (0.692s, 1479.54/s)  LR: 2.643e-04  Data: 0.009 (0.013)
Train: 397 [ 700/1251 ( 56%)]  Loss: 3.381 (3.50)  Time: 0.701s, 1460.91/s  (0.692s, 1479.49/s)  LR: 2.643e-04  Data: 0.010 (0.013)
Train: 397 [ 750/1251 ( 60%)]  Loss: 3.385 (3.49)  Time: 0.687s, 1490.77/s  (0.692s, 1479.90/s)  LR: 2.643e-04  Data: 0.009 (0.013)
Train: 397 [ 800/1251 ( 64%)]  Loss: 3.556 (3.49)  Time: 0.730s, 1402.22/s  (0.692s, 1480.35/s)  LR: 2.643e-04  Data: 0.009 (0.013)
Train: 397 [ 850/1251 ( 68%)]  Loss: 3.309 (3.48)  Time: 0.765s, 1338.59/s  (0.692s, 1479.66/s)  LR: 2.643e-04  Data: 0.010 (0.013)
Train: 397 [ 900/1251 ( 72%)]  Loss: 3.571 (3.49)  Time: 0.704s, 1454.63/s  (0.692s, 1479.87/s)  LR: 2.643e-04  Data: 0.010 (0.012)
Train: 397 [ 950/1251 ( 76%)]  Loss: 3.580 (3.49)  Time: 0.694s, 1474.94/s  (0.692s, 1479.48/s)  LR: 2.643e-04  Data: 0.010 (0.012)
Train: 397 [1000/1251 ( 80%)]  Loss: 3.385 (3.49)  Time: 0.698s, 1466.04/s  (0.692s, 1478.76/s)  LR: 2.643e-04  Data: 0.009 (0.012)
Train: 397 [1050/1251 ( 84%)]  Loss: 3.520 (3.49)  Time: 0.669s, 1529.78/s  (0.693s, 1478.58/s)  LR: 2.643e-04  Data: 0.010 (0.012)
Train: 397 [1100/1251 ( 88%)]  Loss: 3.475 (3.49)  Time: 0.720s, 1421.46/s  (0.692s, 1479.30/s)  LR: 2.643e-04  Data: 0.011 (0.012)
Train: 397 [1150/1251 ( 92%)]  Loss: 3.481 (3.49)  Time: 0.715s, 1432.72/s  (0.692s, 1479.47/s)  LR: 2.643e-04  Data: 0.009 (0.012)
Train: 397 [1200/1251 ( 96%)]  Loss: 3.704 (3.50)  Time: 0.796s, 1286.71/s  (0.692s, 1478.85/s)  LR: 2.643e-04  Data: 0.009 (0.012)
Train: 397 [1250/1251 (100%)]  Loss: 3.550 (3.50)  Time: 0.658s, 1555.67/s  (0.692s, 1479.46/s)  LR: 2.643e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.500 (1.500)  Loss:  0.7407 (0.7407)  Acc@1: 90.5273 (90.5273)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.136 (0.582)  Loss:  0.8413 (1.2683)  Acc@1: 85.3774 (75.5760)  Acc@5: 96.6981 (92.7980)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-393.pth.tar', 75.74200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-390.pth.tar', 75.71400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-389.pth.tar', 75.57600005859375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-397.pth.tar', 75.57599998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-396.pth.tar', 75.5580000366211)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-386.pth.tar', 75.46400010742188)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-381.pth.tar', 75.44600000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-391.pth.tar', 75.41800000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-382.pth.tar', 75.36199998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-380.pth.tar', 75.31200004150391)

Train: 398 [   0/1251 (  0%)]  Loss: 3.349 (3.35)  Time: 2.273s,  450.42/s  (2.273s,  450.42/s)  LR: 2.620e-04  Data: 1.615 (1.615)
Train: 398 [  50/1251 (  4%)]  Loss: 3.372 (3.36)  Time: 0.683s, 1498.24/s  (0.731s, 1400.61/s)  LR: 2.620e-04  Data: 0.011 (0.047)
Train: 398 [ 100/1251 (  8%)]  Loss: 3.427 (3.38)  Time: 0.675s, 1517.43/s  (0.711s, 1439.24/s)  LR: 2.620e-04  Data: 0.011 (0.029)
Train: 398 [ 150/1251 ( 12%)]  Loss: 3.407 (3.39)  Time: 0.673s, 1521.01/s  (0.707s, 1449.27/s)  LR: 2.620e-04  Data: 0.009 (0.023)
Train: 398 [ 200/1251 ( 16%)]  Loss: 3.613 (3.43)  Time: 0.709s, 1443.47/s  (0.702s, 1458.86/s)  LR: 2.620e-04  Data: 0.011 (0.020)
Train: 398 [ 250/1251 ( 20%)]  Loss: 3.700 (3.48)  Time: 0.683s, 1498.27/s  (0.698s, 1466.14/s)  LR: 2.620e-04  Data: 0.014 (0.018)
Train: 398 [ 300/1251 ( 24%)]  Loss: 3.554 (3.49)  Time: 0.674s, 1519.58/s  (0.698s, 1467.22/s)  LR: 2.620e-04  Data: 0.012 (0.017)
Train: 398 [ 350/1251 ( 28%)]  Loss: 3.704 (3.52)  Time: 0.676s, 1515.79/s  (0.697s, 1469.35/s)  LR: 2.620e-04  Data: 0.011 (0.016)
Train: 398 [ 400/1251 ( 32%)]  Loss: 3.115 (3.47)  Time: 0.671s, 1526.12/s  (0.696s, 1471.57/s)  LR: 2.620e-04  Data: 0.010 (0.015)
Train: 398 [ 450/1251 ( 36%)]  Loss: 3.610 (3.48)  Time: 0.671s, 1526.41/s  (0.695s, 1473.57/s)  LR: 2.620e-04  Data: 0.011 (0.015)
Train: 398 [ 500/1251 ( 40%)]  Loss: 3.935 (3.53)  Time: 0.671s, 1525.70/s  (0.695s, 1474.41/s)  LR: 2.620e-04  Data: 0.010 (0.014)
Train: 398 [ 550/1251 ( 44%)]  Loss: 3.588 (3.53)  Time: 0.706s, 1450.55/s  (0.694s, 1475.13/s)  LR: 2.620e-04  Data: 0.010 (0.014)
Train: 398 [ 600/1251 ( 48%)]  Loss: 3.787 (3.55)  Time: 0.674s, 1520.40/s  (0.695s, 1474.13/s)  LR: 2.620e-04  Data: 0.012 (0.014)
Train: 398 [ 650/1251 ( 52%)]  Loss: 3.388 (3.54)  Time: 0.703s, 1456.23/s  (0.694s, 1474.75/s)  LR: 2.620e-04  Data: 0.010 (0.013)
Train: 398 [ 700/1251 ( 56%)]  Loss: 3.460 (3.53)  Time: 0.679s, 1507.41/s  (0.695s, 1474.36/s)  LR: 2.620e-04  Data: 0.010 (0.013)
Train: 398 [ 750/1251 ( 60%)]  Loss: 3.581 (3.54)  Time: 0.673s, 1521.65/s  (0.694s, 1475.12/s)  LR: 2.620e-04  Data: 0.009 (0.013)
Train: 398 [ 800/1251 ( 64%)]  Loss: 3.535 (3.54)  Time: 0.706s, 1450.28/s  (0.694s, 1476.02/s)  LR: 2.620e-04  Data: 0.011 (0.013)
Train: 398 [ 850/1251 ( 68%)]  Loss: 3.620 (3.54)  Time: 0.726s, 1411.03/s  (0.694s, 1476.13/s)  LR: 2.620e-04  Data: 0.010 (0.013)
Train: 398 [ 900/1251 ( 72%)]  Loss: 3.640 (3.55)  Time: 0.708s, 1445.99/s  (0.694s, 1476.14/s)  LR: 2.620e-04  Data: 0.010 (0.013)
Train: 398 [ 950/1251 ( 76%)]  Loss: 3.875 (3.56)  Time: 0.709s, 1443.93/s  (0.694s, 1475.89/s)  LR: 2.620e-04  Data: 0.010 (0.012)
Train: 398 [1000/1251 ( 80%)]  Loss: 3.965 (3.58)  Time: 0.670s, 1528.14/s  (0.694s, 1475.89/s)  LR: 2.620e-04  Data: 0.009 (0.012)
Train: 398 [1050/1251 ( 84%)]  Loss: 3.335 (3.57)  Time: 0.717s, 1428.15/s  (0.694s, 1476.45/s)  LR: 2.620e-04  Data: 0.009 (0.012)
Train: 398 [1100/1251 ( 88%)]  Loss: 3.661 (3.57)  Time: 0.699s, 1465.69/s  (0.693s, 1476.69/s)  LR: 2.620e-04  Data: 0.011 (0.012)
Train: 398 [1150/1251 ( 92%)]  Loss: 3.552 (3.57)  Time: 0.672s, 1524.31/s  (0.693s, 1477.68/s)  LR: 2.620e-04  Data: 0.011 (0.012)
Train: 398 [1200/1251 ( 96%)]  Loss: 3.386 (3.57)  Time: 0.671s, 1525.28/s  (0.693s, 1477.92/s)  LR: 2.620e-04  Data: 0.010 (0.012)
Train: 398 [1250/1251 (100%)]  Loss: 3.632 (3.57)  Time: 0.661s, 1549.89/s  (0.693s, 1478.09/s)  LR: 2.620e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.543 (1.543)  Loss:  0.8613 (0.8613)  Acc@1: 90.2344 (90.2344)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.136 (0.585)  Loss:  0.9673 (1.3504)  Acc@1: 83.3726 (75.6320)  Acc@5: 95.9906 (92.6340)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-393.pth.tar', 75.74200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-390.pth.tar', 75.71400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-398.pth.tar', 75.63200001708984)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-389.pth.tar', 75.57600005859375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-397.pth.tar', 75.57599998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-396.pth.tar', 75.5580000366211)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-386.pth.tar', 75.46400010742188)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-381.pth.tar', 75.44600000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-391.pth.tar', 75.41800000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-382.pth.tar', 75.36199998779297)

Train: 399 [   0/1251 (  0%)]  Loss: 3.763 (3.76)  Time: 2.188s,  468.00/s  (2.188s,  468.00/s)  LR: 2.597e-04  Data: 1.539 (1.539)
Train: 399 [  50/1251 (  4%)]  Loss: 3.710 (3.74)  Time: 0.670s, 1527.49/s  (0.725s, 1412.47/s)  LR: 2.597e-04  Data: 0.011 (0.046)
Train: 399 [ 100/1251 (  8%)]  Loss: 3.829 (3.77)  Time: 0.705s, 1451.82/s  (0.705s, 1451.99/s)  LR: 2.597e-04  Data: 0.010 (0.028)
Train: 399 [ 150/1251 ( 12%)]  Loss: 3.421 (3.68)  Time: 0.670s, 1527.53/s  (0.701s, 1460.39/s)  LR: 2.597e-04  Data: 0.012 (0.022)
Train: 399 [ 200/1251 ( 16%)]  Loss: 3.272 (3.60)  Time: 0.704s, 1453.53/s  (0.700s, 1462.15/s)  LR: 2.597e-04  Data: 0.010 (0.019)
Train: 399 [ 250/1251 ( 20%)]  Loss: 3.549 (3.59)  Time: 0.680s, 1505.43/s  (0.699s, 1465.80/s)  LR: 2.597e-04  Data: 0.009 (0.017)
Train: 399 [ 300/1251 ( 24%)]  Loss: 3.726 (3.61)  Time: 0.673s, 1520.86/s  (0.697s, 1469.45/s)  LR: 2.597e-04  Data: 0.010 (0.016)
Train: 399 [ 350/1251 ( 28%)]  Loss: 3.400 (3.58)  Time: 0.671s, 1525.16/s  (0.695s, 1472.48/s)  LR: 2.597e-04  Data: 0.011 (0.015)
Train: 399 [ 400/1251 ( 32%)]  Loss: 3.539 (3.58)  Time: 0.676s, 1514.66/s  (0.695s, 1473.86/s)  LR: 2.597e-04  Data: 0.010 (0.015)
Train: 399 [ 450/1251 ( 36%)]  Loss: 3.542 (3.58)  Time: 0.671s, 1526.92/s  (0.694s, 1475.10/s)  LR: 2.597e-04  Data: 0.009 (0.014)
Train: 399 [ 500/1251 ( 40%)]  Loss: 3.332 (3.55)  Time: 0.694s, 1475.61/s  (0.694s, 1475.03/s)  LR: 2.597e-04  Data: 0.010 (0.014)
Train: 399 [ 550/1251 ( 44%)]  Loss: 3.645 (3.56)  Time: 0.671s, 1526.89/s  (0.694s, 1474.68/s)  LR: 2.597e-04  Data: 0.010 (0.014)
Train: 399 [ 600/1251 ( 48%)]  Loss: 3.267 (3.54)  Time: 0.714s, 1433.62/s  (0.694s, 1475.78/s)  LR: 2.597e-04  Data: 0.009 (0.013)
Train: 399 [ 650/1251 ( 52%)]  Loss: 3.340 (3.52)  Time: 0.672s, 1524.12/s  (0.693s, 1476.79/s)  LR: 2.597e-04  Data: 0.010 (0.013)
Train: 399 [ 700/1251 ( 56%)]  Loss: 3.420 (3.52)  Time: 0.701s, 1461.36/s  (0.693s, 1476.99/s)  LR: 2.597e-04  Data: 0.009 (0.013)
Train: 399 [ 750/1251 ( 60%)]  Loss: 3.326 (3.51)  Time: 0.714s, 1433.75/s  (0.693s, 1476.60/s)  LR: 2.597e-04  Data: 0.012 (0.013)
Train: 399 [ 800/1251 ( 64%)]  Loss: 3.413 (3.50)  Time: 0.705s, 1452.09/s  (0.693s, 1477.28/s)  LR: 2.597e-04  Data: 0.009 (0.013)
Train: 399 [ 850/1251 ( 68%)]  Loss: 3.609 (3.51)  Time: 0.671s, 1526.98/s  (0.693s, 1477.81/s)  LR: 2.597e-04  Data: 0.010 (0.012)
Train: 399 [ 900/1251 ( 72%)]  Loss: 3.548 (3.51)  Time: 0.720s, 1422.12/s  (0.692s, 1479.04/s)  LR: 2.597e-04  Data: 0.010 (0.012)
Train: 399 [ 950/1251 ( 76%)]  Loss: 3.657 (3.52)  Time: 0.678s, 1509.40/s  (0.693s, 1477.93/s)  LR: 2.597e-04  Data: 0.010 (0.012)
Train: 399 [1000/1251 ( 80%)]  Loss: 3.338 (3.51)  Time: 0.702s, 1458.36/s  (0.693s, 1478.01/s)  LR: 2.597e-04  Data: 0.009 (0.012)
Train: 399 [1050/1251 ( 84%)]  Loss: 3.849 (3.52)  Time: 0.673s, 1521.79/s  (0.693s, 1478.15/s)  LR: 2.597e-04  Data: 0.011 (0.012)
Train: 399 [1100/1251 ( 88%)]  Loss: 3.714 (3.53)  Time: 0.604s, 1696.43/s  (0.693s, 1478.50/s)  LR: 2.597e-04  Data: 0.010 (0.012)
Train: 399 [1150/1251 ( 92%)]  Loss: 3.570 (3.53)  Time: 0.680s, 1504.83/s  (0.692s, 1479.24/s)  LR: 2.597e-04  Data: 0.010 (0.012)
Train: 399 [1200/1251 ( 96%)]  Loss: 3.116 (3.52)  Time: 0.670s, 1528.81/s  (0.692s, 1478.85/s)  LR: 2.597e-04  Data: 0.011 (0.012)
Train: 399 [1250/1251 (100%)]  Loss: 3.229 (3.50)  Time: 0.689s, 1487.16/s  (0.692s, 1478.71/s)  LR: 2.597e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.648 (1.648)  Loss:  0.6978 (0.6978)  Acc@1: 90.5273 (90.5273)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.137 (0.592)  Loss:  0.8711 (1.2723)  Acc@1: 85.6132 (75.6600)  Acc@5: 96.2264 (92.8220)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-393.pth.tar', 75.74200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-390.pth.tar', 75.71400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-399.pth.tar', 75.66000008544921)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-398.pth.tar', 75.63200001708984)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-389.pth.tar', 75.57600005859375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-397.pth.tar', 75.57599998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-396.pth.tar', 75.5580000366211)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-386.pth.tar', 75.46400010742188)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-381.pth.tar', 75.44600000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-391.pth.tar', 75.41800000732422)

Train: 400 [   0/1251 (  0%)]  Loss: 3.483 (3.48)  Time: 2.236s,  458.00/s  (2.236s,  458.00/s)  LR: 2.575e-04  Data: 1.587 (1.587)
Train: 400 [  50/1251 (  4%)]  Loss: 3.298 (3.39)  Time: 0.674s, 1519.31/s  (0.728s, 1407.29/s)  LR: 2.575e-04  Data: 0.013 (0.048)
Train: 400 [ 100/1251 (  8%)]  Loss: 3.183 (3.32)  Time: 0.675s, 1518.03/s  (0.709s, 1443.82/s)  LR: 2.575e-04  Data: 0.009 (0.029)
Train: 400 [ 150/1251 ( 12%)]  Loss: 3.283 (3.31)  Time: 0.666s, 1538.69/s  (0.705s, 1453.01/s)  LR: 2.575e-04  Data: 0.010 (0.023)
Train: 400 [ 200/1251 ( 16%)]  Loss: 3.215 (3.29)  Time: 0.707s, 1447.67/s  (0.702s, 1459.62/s)  LR: 2.575e-04  Data: 0.010 (0.020)
Train: 400 [ 250/1251 ( 20%)]  Loss: 3.354 (3.30)  Time: 0.736s, 1390.56/s  (0.702s, 1458.69/s)  LR: 2.575e-04  Data: 0.009 (0.018)
Train: 400 [ 300/1251 ( 24%)]  Loss: 3.466 (3.33)  Time: 0.678s, 1510.09/s  (0.699s, 1464.37/s)  LR: 2.575e-04  Data: 0.015 (0.017)
Train: 400 [ 350/1251 ( 28%)]  Loss: 3.632 (3.36)  Time: 0.672s, 1524.34/s  (0.697s, 1468.47/s)  LR: 2.575e-04  Data: 0.011 (0.016)
Train: 400 [ 400/1251 ( 32%)]  Loss: 3.543 (3.38)  Time: 0.672s, 1523.16/s  (0.697s, 1468.67/s)  LR: 2.575e-04  Data: 0.012 (0.015)
Train: 400 [ 450/1251 ( 36%)]  Loss: 3.327 (3.38)  Time: 0.672s, 1523.26/s  (0.697s, 1470.09/s)  LR: 2.575e-04  Data: 0.011 (0.015)
Train: 400 [ 500/1251 ( 40%)]  Loss: 3.590 (3.40)  Time: 0.666s, 1537.10/s  (0.697s, 1470.12/s)  LR: 2.575e-04  Data: 0.010 (0.014)
Train: 400 [ 550/1251 ( 44%)]  Loss: 3.691 (3.42)  Time: 0.695s, 1474.38/s  (0.696s, 1470.75/s)  LR: 2.575e-04  Data: 0.013 (0.014)
Train: 400 [ 600/1251 ( 48%)]  Loss: 3.418 (3.42)  Time: 0.748s, 1368.50/s  (0.697s, 1468.28/s)  LR: 2.575e-04  Data: 0.011 (0.014)
Train: 400 [ 650/1251 ( 52%)]  Loss: 3.380 (3.42)  Time: 0.683s, 1500.13/s  (0.698s, 1466.89/s)  LR: 2.575e-04  Data: 0.010 (0.014)
Train: 400 [ 700/1251 ( 56%)]  Loss: 3.374 (3.42)  Time: 0.672s, 1524.52/s  (0.699s, 1465.59/s)  LR: 2.575e-04  Data: 0.011 (0.014)
Train: 400 [ 750/1251 ( 60%)]  Loss: 3.177 (3.40)  Time: 0.676s, 1514.44/s  (0.697s, 1468.13/s)  LR: 2.575e-04  Data: 0.010 (0.013)
Train: 400 [ 800/1251 ( 64%)]  Loss: 3.798 (3.42)  Time: 0.672s, 1523.37/s  (0.696s, 1470.44/s)  LR: 2.575e-04  Data: 0.010 (0.013)
Train: 400 [ 850/1251 ( 68%)]  Loss: 3.255 (3.41)  Time: 0.712s, 1438.18/s  (0.696s, 1470.40/s)  LR: 2.575e-04  Data: 0.009 (0.013)
Train: 400 [ 900/1251 ( 72%)]  Loss: 3.666 (3.43)  Time: 0.672s, 1523.59/s  (0.696s, 1471.78/s)  LR: 2.575e-04  Data: 0.010 (0.013)
Train: 400 [ 950/1251 ( 76%)]  Loss: 3.655 (3.44)  Time: 0.691s, 1481.83/s  (0.696s, 1471.50/s)  LR: 2.575e-04  Data: 0.011 (0.013)
Train: 400 [1000/1251 ( 80%)]  Loss: 3.259 (3.43)  Time: 0.719s, 1423.63/s  (0.696s, 1472.21/s)  LR: 2.575e-04  Data: 0.010 (0.013)
Train: 400 [1050/1251 ( 84%)]  Loss: 3.523 (3.43)  Time: 0.709s, 1445.02/s  (0.695s, 1473.03/s)  LR: 2.575e-04  Data: 0.009 (0.012)
Train: 400 [1100/1251 ( 88%)]  Loss: 3.115 (3.42)  Time: 0.671s, 1525.41/s  (0.695s, 1473.86/s)  LR: 2.575e-04  Data: 0.010 (0.012)
Train: 400 [1150/1251 ( 92%)]  Loss: 3.563 (3.43)  Time: 0.672s, 1524.54/s  (0.695s, 1474.19/s)  LR: 2.575e-04  Data: 0.011 (0.012)
Train: 400 [1200/1251 ( 96%)]  Loss: 3.251 (3.42)  Time: 0.703s, 1457.49/s  (0.695s, 1474.01/s)  LR: 2.575e-04  Data: 0.010 (0.012)
Train: 400 [1250/1251 (100%)]  Loss: 3.408 (3.42)  Time: 0.658s, 1555.80/s  (0.695s, 1473.96/s)  LR: 2.575e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.486 (1.486)  Loss:  0.6479 (0.6479)  Acc@1: 91.1133 (91.1133)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.136 (0.587)  Loss:  0.7925 (1.2110)  Acc@1: 85.6132 (75.9640)  Acc@5: 96.1085 (92.9720)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-400.pth.tar', 75.96399995605469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-393.pth.tar', 75.74200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-390.pth.tar', 75.71400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-399.pth.tar', 75.66000008544921)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-398.pth.tar', 75.63200001708984)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-389.pth.tar', 75.57600005859375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-397.pth.tar', 75.57599998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-396.pth.tar', 75.5580000366211)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-386.pth.tar', 75.46400010742188)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-381.pth.tar', 75.44600000732422)

Train: 401 [   0/1251 (  0%)]  Loss: 3.238 (3.24)  Time: 2.324s,  440.61/s  (2.324s,  440.61/s)  LR: 2.553e-04  Data: 1.708 (1.708)
Train: 401 [  50/1251 (  4%)]  Loss: 3.677 (3.46)  Time: 0.671s, 1524.99/s  (0.732s, 1398.02/s)  LR: 2.553e-04  Data: 0.010 (0.051)
Train: 401 [ 100/1251 (  8%)]  Loss: 3.453 (3.46)  Time: 0.673s, 1520.66/s  (0.708s, 1445.36/s)  LR: 2.553e-04  Data: 0.010 (0.031)
Train: 401 [ 150/1251 ( 12%)]  Loss: 3.618 (3.50)  Time: 0.671s, 1526.68/s  (0.703s, 1456.83/s)  LR: 2.553e-04  Data: 0.009 (0.024)
Train: 401 [ 200/1251 ( 16%)]  Loss: 3.531 (3.50)  Time: 0.671s, 1525.33/s  (0.701s, 1460.87/s)  LR: 2.553e-04  Data: 0.010 (0.021)
Train: 401 [ 250/1251 ( 20%)]  Loss: 3.624 (3.52)  Time: 0.688s, 1488.84/s  (0.699s, 1465.33/s)  LR: 2.553e-04  Data: 0.012 (0.019)
Train: 401 [ 300/1251 ( 24%)]  Loss: 3.776 (3.56)  Time: 0.737s, 1390.19/s  (0.697s, 1469.34/s)  LR: 2.553e-04  Data: 0.010 (0.017)
Train: 401 [ 350/1251 ( 28%)]  Loss: 3.580 (3.56)  Time: 0.672s, 1523.20/s  (0.696s, 1472.14/s)  LR: 2.553e-04  Data: 0.011 (0.016)
Train: 401 [ 400/1251 ( 32%)]  Loss: 3.496 (3.55)  Time: 0.693s, 1478.42/s  (0.695s, 1472.44/s)  LR: 2.553e-04  Data: 0.010 (0.016)
Train: 401 [ 450/1251 ( 36%)]  Loss: 3.059 (3.51)  Time: 0.671s, 1526.75/s  (0.695s, 1474.15/s)  LR: 2.553e-04  Data: 0.009 (0.015)
Train: 401 [ 500/1251 ( 40%)]  Loss: 3.226 (3.48)  Time: 0.677s, 1513.08/s  (0.694s, 1475.93/s)  LR: 2.553e-04  Data: 0.016 (0.015)
Train: 401 [ 550/1251 ( 44%)]  Loss: 3.306 (3.47)  Time: 0.707s, 1449.00/s  (0.693s, 1477.06/s)  LR: 2.553e-04  Data: 0.009 (0.014)
Train: 401 [ 600/1251 ( 48%)]  Loss: 3.098 (3.44)  Time: 0.691s, 1482.86/s  (0.693s, 1478.67/s)  LR: 2.553e-04  Data: 0.009 (0.014)
Train: 401 [ 650/1251 ( 52%)]  Loss: 3.499 (3.44)  Time: 0.711s, 1439.80/s  (0.693s, 1477.72/s)  LR: 2.553e-04  Data: 0.014 (0.014)
Train: 401 [ 700/1251 ( 56%)]  Loss: 3.577 (3.45)  Time: 0.719s, 1424.60/s  (0.693s, 1477.88/s)  LR: 2.553e-04  Data: 0.009 (0.013)
Train: 401 [ 750/1251 ( 60%)]  Loss: 3.390 (3.45)  Time: 0.676s, 1514.38/s  (0.693s, 1478.67/s)  LR: 2.553e-04  Data: 0.011 (0.013)
Train: 401 [ 800/1251 ( 64%)]  Loss: 3.411 (3.44)  Time: 0.670s, 1528.02/s  (0.692s, 1479.58/s)  LR: 2.553e-04  Data: 0.010 (0.013)
Train: 401 [ 850/1251 ( 68%)]  Loss: 3.689 (3.46)  Time: 0.672s, 1523.18/s  (0.692s, 1480.49/s)  LR: 2.553e-04  Data: 0.010 (0.013)
Train: 401 [ 900/1251 ( 72%)]  Loss: 3.445 (3.46)  Time: 0.704s, 1453.83/s  (0.692s, 1480.69/s)  LR: 2.553e-04  Data: 0.009 (0.013)
Train: 401 [ 950/1251 ( 76%)]  Loss: 3.216 (3.45)  Time: 0.672s, 1522.97/s  (0.691s, 1480.96/s)  LR: 2.553e-04  Data: 0.010 (0.013)
Train: 401 [1000/1251 ( 80%)]  Loss: 3.650 (3.46)  Time: 0.671s, 1526.11/s  (0.691s, 1481.53/s)  LR: 2.553e-04  Data: 0.010 (0.013)
Train: 401 [1050/1251 ( 84%)]  Loss: 3.610 (3.46)  Time: 0.711s, 1440.46/s  (0.691s, 1482.06/s)  LR: 2.553e-04  Data: 0.011 (0.012)
Train: 401 [1100/1251 ( 88%)]  Loss: 3.265 (3.45)  Time: 0.700s, 1462.97/s  (0.691s, 1482.09/s)  LR: 2.553e-04  Data: 0.010 (0.012)
Train: 401 [1150/1251 ( 92%)]  Loss: 3.136 (3.44)  Time: 0.704s, 1455.52/s  (0.691s, 1482.24/s)  LR: 2.553e-04  Data: 0.010 (0.012)
Train: 401 [1200/1251 ( 96%)]  Loss: 3.654 (3.45)  Time: 0.673s, 1520.70/s  (0.691s, 1481.39/s)  LR: 2.553e-04  Data: 0.010 (0.012)
Train: 401 [1250/1251 (100%)]  Loss: 3.482 (3.45)  Time: 0.656s, 1559.89/s  (0.691s, 1482.06/s)  LR: 2.553e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.603 (1.603)  Loss:  0.8208 (0.8208)  Acc@1: 90.2344 (90.2344)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.136 (0.582)  Loss:  0.8501 (1.2885)  Acc@1: 85.6132 (75.9040)  Acc@5: 96.2264 (92.8420)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-400.pth.tar', 75.96399995605469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-401.pth.tar', 75.90400008544921)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-393.pth.tar', 75.74200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-390.pth.tar', 75.71400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-399.pth.tar', 75.66000008544921)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-398.pth.tar', 75.63200001708984)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-389.pth.tar', 75.57600005859375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-397.pth.tar', 75.57599998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-396.pth.tar', 75.5580000366211)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-386.pth.tar', 75.46400010742188)

Train: 402 [   0/1251 (  0%)]  Loss: 3.708 (3.71)  Time: 2.159s,  474.32/s  (2.159s,  474.32/s)  LR: 2.530e-04  Data: 1.542 (1.542)
Train: 402 [  50/1251 (  4%)]  Loss: 3.523 (3.62)  Time: 0.672s, 1524.52/s  (0.732s, 1399.12/s)  LR: 2.530e-04  Data: 0.011 (0.049)
Train: 402 [ 100/1251 (  8%)]  Loss: 3.379 (3.54)  Time: 0.709s, 1444.14/s  (0.713s, 1436.79/s)  LR: 2.530e-04  Data: 0.011 (0.030)
Train: 402 [ 150/1251 ( 12%)]  Loss: 3.266 (3.47)  Time: 0.702s, 1458.15/s  (0.706s, 1450.93/s)  LR: 2.530e-04  Data: 0.009 (0.023)
Train: 402 [ 200/1251 ( 16%)]  Loss: 3.367 (3.45)  Time: 0.672s, 1524.00/s  (0.703s, 1455.88/s)  LR: 2.530e-04  Data: 0.011 (0.020)
Train: 402 [ 250/1251 ( 20%)]  Loss: 3.244 (3.41)  Time: 0.702s, 1458.33/s  (0.700s, 1461.96/s)  LR: 2.530e-04  Data: 0.010 (0.018)
Train: 402 [ 300/1251 ( 24%)]  Loss: 3.515 (3.43)  Time: 0.673s, 1520.63/s  (0.697s, 1468.25/s)  LR: 2.530e-04  Data: 0.011 (0.017)
Train: 402 [ 350/1251 ( 28%)]  Loss: 3.695 (3.46)  Time: 0.668s, 1532.70/s  (0.696s, 1471.53/s)  LR: 2.530e-04  Data: 0.009 (0.016)
Train: 402 [ 400/1251 ( 32%)]  Loss: 3.531 (3.47)  Time: 0.682s, 1500.48/s  (0.696s, 1472.06/s)  LR: 2.530e-04  Data: 0.011 (0.015)
Train: 402 [ 450/1251 ( 36%)]  Loss: 3.735 (3.50)  Time: 0.667s, 1536.05/s  (0.695s, 1473.07/s)  LR: 2.530e-04  Data: 0.010 (0.015)
Train: 402 [ 500/1251 ( 40%)]  Loss: 3.507 (3.50)  Time: 0.672s, 1524.73/s  (0.695s, 1474.30/s)  LR: 2.530e-04  Data: 0.010 (0.014)
Train: 402 [ 550/1251 ( 44%)]  Loss: 3.961 (3.54)  Time: 0.695s, 1474.12/s  (0.694s, 1474.69/s)  LR: 2.530e-04  Data: 0.009 (0.014)
Train: 402 [ 600/1251 ( 48%)]  Loss: 3.981 (3.57)  Time: 0.686s, 1493.19/s  (0.694s, 1475.84/s)  LR: 2.530e-04  Data: 0.011 (0.014)
Train: 402 [ 650/1251 ( 52%)]  Loss: 3.766 (3.58)  Time: 0.673s, 1522.57/s  (0.693s, 1477.27/s)  LR: 2.530e-04  Data: 0.010 (0.013)
Train: 402 [ 700/1251 ( 56%)]  Loss: 3.467 (3.58)  Time: 0.674s, 1519.13/s  (0.693s, 1477.84/s)  LR: 2.530e-04  Data: 0.011 (0.013)
Train: 402 [ 750/1251 ( 60%)]  Loss: 3.528 (3.57)  Time: 0.705s, 1453.15/s  (0.693s, 1478.34/s)  LR: 2.530e-04  Data: 0.009 (0.013)
Train: 402 [ 800/1251 ( 64%)]  Loss: 3.772 (3.58)  Time: 0.723s, 1416.68/s  (0.693s, 1477.50/s)  LR: 2.530e-04  Data: 0.010 (0.013)
Train: 402 [ 850/1251 ( 68%)]  Loss: 3.461 (3.58)  Time: 0.675s, 1517.30/s  (0.693s, 1477.07/s)  LR: 2.530e-04  Data: 0.011 (0.013)
Train: 402 [ 900/1251 ( 72%)]  Loss: 3.744 (3.59)  Time: 0.725s, 1412.13/s  (0.693s, 1477.15/s)  LR: 2.530e-04  Data: 0.009 (0.012)
Train: 402 [ 950/1251 ( 76%)]  Loss: 3.422 (3.58)  Time: 0.671s, 1525.66/s  (0.693s, 1476.68/s)  LR: 2.530e-04  Data: 0.011 (0.012)
Train: 402 [1000/1251 ( 80%)]  Loss: 3.347 (3.57)  Time: 0.699s, 1465.28/s  (0.693s, 1477.05/s)  LR: 2.530e-04  Data: 0.012 (0.012)
Train: 402 [1050/1251 ( 84%)]  Loss: 3.758 (3.58)  Time: 0.704s, 1454.46/s  (0.693s, 1477.11/s)  LR: 2.530e-04  Data: 0.012 (0.012)
Train: 402 [1100/1251 ( 88%)]  Loss: 3.092 (3.56)  Time: 0.723s, 1416.40/s  (0.693s, 1477.54/s)  LR: 2.530e-04  Data: 0.010 (0.012)
Train: 402 [1150/1251 ( 92%)]  Loss: 3.848 (3.57)  Time: 0.701s, 1460.99/s  (0.693s, 1477.13/s)  LR: 2.530e-04  Data: 0.009 (0.012)
Train: 402 [1200/1251 ( 96%)]  Loss: 3.349 (3.56)  Time: 0.709s, 1445.25/s  (0.693s, 1476.61/s)  LR: 2.530e-04  Data: 0.009 (0.012)
Train: 402 [1250/1251 (100%)]  Loss: 3.361 (3.55)  Time: 0.662s, 1546.95/s  (0.693s, 1476.99/s)  LR: 2.530e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.439 (1.439)  Loss:  0.7983 (0.7983)  Acc@1: 90.1367 (90.1367)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.136 (0.581)  Loss:  0.8818 (1.3389)  Acc@1: 84.7877 (76.0280)  Acc@5: 96.8160 (92.9420)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-402.pth.tar', 76.02800011474609)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-400.pth.tar', 75.96399995605469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-401.pth.tar', 75.90400008544921)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-393.pth.tar', 75.74200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-390.pth.tar', 75.71400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-399.pth.tar', 75.66000008544921)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-398.pth.tar', 75.63200001708984)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-389.pth.tar', 75.57600005859375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-397.pth.tar', 75.57599998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-396.pth.tar', 75.5580000366211)

Train: 403 [   0/1251 (  0%)]  Loss: 3.654 (3.65)  Time: 2.364s,  433.15/s  (2.364s,  433.15/s)  LR: 2.508e-04  Data: 1.748 (1.748)
Train: 403 [  50/1251 (  4%)]  Loss: 3.683 (3.67)  Time: 0.686s, 1493.41/s  (0.733s, 1397.39/s)  LR: 2.508e-04  Data: 0.009 (0.051)
Train: 403 [ 100/1251 (  8%)]  Loss: 3.689 (3.68)  Time: 0.716s, 1429.73/s  (0.712s, 1437.20/s)  LR: 2.508e-04  Data: 0.010 (0.031)
Train: 403 [ 150/1251 ( 12%)]  Loss: 3.452 (3.62)  Time: 0.672s, 1524.10/s  (0.706s, 1451.31/s)  LR: 2.508e-04  Data: 0.011 (0.024)
Train: 403 [ 200/1251 ( 16%)]  Loss: 3.523 (3.60)  Time: 0.733s, 1397.92/s  (0.702s, 1459.28/s)  LR: 2.508e-04  Data: 0.010 (0.021)
Train: 403 [ 250/1251 ( 20%)]  Loss: 3.551 (3.59)  Time: 0.671s, 1525.07/s  (0.701s, 1460.20/s)  LR: 2.508e-04  Data: 0.010 (0.019)
Train: 403 [ 300/1251 ( 24%)]  Loss: 2.829 (3.48)  Time: 0.672s, 1523.98/s  (0.699s, 1465.59/s)  LR: 2.508e-04  Data: 0.010 (0.017)
Train: 403 [ 350/1251 ( 28%)]  Loss: 3.315 (3.46)  Time: 0.670s, 1529.12/s  (0.697s, 1470.07/s)  LR: 2.508e-04  Data: 0.010 (0.016)
Train: 403 [ 400/1251 ( 32%)]  Loss: 3.557 (3.47)  Time: 0.715s, 1431.63/s  (0.696s, 1471.11/s)  LR: 2.508e-04  Data: 0.009 (0.016)
Train: 403 [ 450/1251 ( 36%)]  Loss: 3.769 (3.50)  Time: 0.705s, 1451.55/s  (0.696s, 1470.52/s)  LR: 2.508e-04  Data: 0.012 (0.015)
Train: 403 [ 500/1251 ( 40%)]  Loss: 3.433 (3.50)  Time: 0.667s, 1536.32/s  (0.695s, 1472.61/s)  LR: 2.508e-04  Data: 0.009 (0.015)
Train: 403 [ 550/1251 ( 44%)]  Loss: 3.474 (3.49)  Time: 0.676s, 1514.04/s  (0.695s, 1473.26/s)  LR: 2.508e-04  Data: 0.010 (0.014)
Train: 403 [ 600/1251 ( 48%)]  Loss: 3.415 (3.49)  Time: 0.683s, 1499.17/s  (0.694s, 1474.69/s)  LR: 2.508e-04  Data: 0.010 (0.014)
Train: 403 [ 650/1251 ( 52%)]  Loss: 3.587 (3.50)  Time: 0.737s, 1389.84/s  (0.694s, 1475.59/s)  LR: 2.508e-04  Data: 0.009 (0.014)
Train: 403 [ 700/1251 ( 56%)]  Loss: 3.603 (3.50)  Time: 0.678s, 1510.44/s  (0.693s, 1476.73/s)  LR: 2.508e-04  Data: 0.011 (0.013)
Train: 403 [ 750/1251 ( 60%)]  Loss: 3.666 (3.51)  Time: 0.756s, 1354.80/s  (0.693s, 1476.62/s)  LR: 2.508e-04  Data: 0.010 (0.013)
Train: 403 [ 800/1251 ( 64%)]  Loss: 3.720 (3.52)  Time: 0.757s, 1352.13/s  (0.693s, 1477.67/s)  LR: 2.508e-04  Data: 0.010 (0.013)
Train: 403 [ 850/1251 ( 68%)]  Loss: 3.671 (3.53)  Time: 0.703s, 1456.64/s  (0.693s, 1477.22/s)  LR: 2.508e-04  Data: 0.009 (0.013)
Train: 403 [ 900/1251 ( 72%)]  Loss: 3.631 (3.54)  Time: 0.670s, 1527.98/s  (0.694s, 1476.55/s)  LR: 2.508e-04  Data: 0.011 (0.013)
Train: 403 [ 950/1251 ( 76%)]  Loss: 2.986 (3.51)  Time: 0.697s, 1469.71/s  (0.693s, 1477.25/s)  LR: 2.508e-04  Data: 0.009 (0.013)
Train: 403 [1000/1251 ( 80%)]  Loss: 3.378 (3.50)  Time: 0.668s, 1531.85/s  (0.693s, 1478.12/s)  LR: 2.508e-04  Data: 0.010 (0.012)
Train: 403 [1050/1251 ( 84%)]  Loss: 3.480 (3.50)  Time: 0.671s, 1526.16/s  (0.693s, 1478.54/s)  LR: 2.508e-04  Data: 0.010 (0.012)
Train: 403 [1100/1251 ( 88%)]  Loss: 3.765 (3.51)  Time: 0.673s, 1522.25/s  (0.693s, 1478.35/s)  LR: 2.508e-04  Data: 0.010 (0.012)
Train: 403 [1150/1251 ( 92%)]  Loss: 3.536 (3.52)  Time: 0.724s, 1414.24/s  (0.693s, 1478.58/s)  LR: 2.508e-04  Data: 0.010 (0.012)
Train: 403 [1200/1251 ( 96%)]  Loss: 3.741 (3.52)  Time: 0.672s, 1523.58/s  (0.692s, 1479.36/s)  LR: 2.508e-04  Data: 0.009 (0.012)
Train: 403 [1250/1251 (100%)]  Loss: 3.631 (3.53)  Time: 0.695s, 1472.35/s  (0.692s, 1479.54/s)  LR: 2.508e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.583 (1.583)  Loss:  0.8008 (0.8008)  Acc@1: 90.0391 (90.0391)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.137 (0.590)  Loss:  0.9478 (1.3313)  Acc@1: 85.2594 (75.7260)  Acc@5: 96.8160 (92.6260)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-402.pth.tar', 76.02800011474609)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-400.pth.tar', 75.96399995605469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-401.pth.tar', 75.90400008544921)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-393.pth.tar', 75.74200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-403.pth.tar', 75.72600006103515)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-390.pth.tar', 75.71400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-399.pth.tar', 75.66000008544921)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-398.pth.tar', 75.63200001708984)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-389.pth.tar', 75.57600005859375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-397.pth.tar', 75.57599998291016)

Train: 404 [   0/1251 (  0%)]  Loss: 3.254 (3.25)  Time: 2.278s,  449.52/s  (2.278s,  449.52/s)  LR: 2.486e-04  Data: 1.579 (1.579)
Train: 404 [  50/1251 (  4%)]  Loss: 3.614 (3.43)  Time: 0.703s, 1456.38/s  (0.727s, 1408.92/s)  LR: 2.486e-04  Data: 0.009 (0.048)
Train: 404 [ 100/1251 (  8%)]  Loss: 3.528 (3.47)  Time: 0.725s, 1411.94/s  (0.709s, 1444.51/s)  LR: 2.486e-04  Data: 0.009 (0.029)
Train: 404 [ 150/1251 ( 12%)]  Loss: 3.967 (3.59)  Time: 0.692s, 1479.24/s  (0.704s, 1453.87/s)  LR: 2.486e-04  Data: 0.011 (0.023)
Train: 404 [ 200/1251 ( 16%)]  Loss: 3.368 (3.55)  Time: 0.710s, 1442.08/s  (0.703s, 1456.52/s)  LR: 2.486e-04  Data: 0.011 (0.020)
Train: 404 [ 250/1251 ( 20%)]  Loss: 3.406 (3.52)  Time: 0.690s, 1483.71/s  (0.701s, 1461.57/s)  LR: 2.486e-04  Data: 0.014 (0.018)
Train: 404 [ 300/1251 ( 24%)]  Loss: 3.166 (3.47)  Time: 0.673s, 1521.93/s  (0.699s, 1465.90/s)  LR: 2.486e-04  Data: 0.009 (0.017)
Train: 404 [ 350/1251 ( 28%)]  Loss: 3.523 (3.48)  Time: 0.687s, 1491.50/s  (0.698s, 1467.83/s)  LR: 2.486e-04  Data: 0.010 (0.016)
Train: 404 [ 400/1251 ( 32%)]  Loss: 3.638 (3.50)  Time: 0.673s, 1520.50/s  (0.697s, 1468.90/s)  LR: 2.486e-04  Data: 0.011 (0.015)
Train: 404 [ 450/1251 ( 36%)]  Loss: 3.105 (3.46)  Time: 0.683s, 1499.79/s  (0.697s, 1469.81/s)  LR: 2.486e-04  Data: 0.014 (0.015)
Train: 404 [ 500/1251 ( 40%)]  Loss: 3.807 (3.49)  Time: 0.715s, 1431.20/s  (0.697s, 1468.90/s)  LR: 2.486e-04  Data: 0.013 (0.014)
Train: 404 [ 550/1251 ( 44%)]  Loss: 3.234 (3.47)  Time: 0.707s, 1447.58/s  (0.697s, 1469.74/s)  LR: 2.486e-04  Data: 0.009 (0.014)
Train: 404 [ 600/1251 ( 48%)]  Loss: 3.488 (3.47)  Time: 0.696s, 1470.39/s  (0.696s, 1470.85/s)  LR: 2.486e-04  Data: 0.010 (0.014)
Train: 404 [ 650/1251 ( 52%)]  Loss: 3.507 (3.47)  Time: 0.676s, 1515.57/s  (0.695s, 1472.42/s)  LR: 2.486e-04  Data: 0.010 (0.013)
Train: 404 [ 700/1251 ( 56%)]  Loss: 3.668 (3.48)  Time: 0.666s, 1537.09/s  (0.696s, 1471.49/s)  LR: 2.486e-04  Data: 0.010 (0.013)
Train: 404 [ 750/1251 ( 60%)]  Loss: 3.679 (3.50)  Time: 0.724s, 1413.58/s  (0.695s, 1472.58/s)  LR: 2.486e-04  Data: 0.009 (0.013)
Train: 404 [ 800/1251 ( 64%)]  Loss: 3.333 (3.49)  Time: 0.715s, 1432.45/s  (0.695s, 1473.05/s)  LR: 2.486e-04  Data: 0.013 (0.013)
Train: 404 [ 850/1251 ( 68%)]  Loss: 3.541 (3.49)  Time: 0.673s, 1522.58/s  (0.695s, 1474.06/s)  LR: 2.486e-04  Data: 0.010 (0.013)
Train: 404 [ 900/1251 ( 72%)]  Loss: 3.135 (3.47)  Time: 0.708s, 1445.83/s  (0.695s, 1474.42/s)  LR: 2.486e-04  Data: 0.010 (0.013)
Train: 404 [ 950/1251 ( 76%)]  Loss: 3.606 (3.48)  Time: 0.707s, 1449.10/s  (0.695s, 1473.91/s)  LR: 2.486e-04  Data: 0.009 (0.012)
Train: 404 [1000/1251 ( 80%)]  Loss: 3.412 (3.48)  Time: 0.672s, 1524.66/s  (0.694s, 1475.09/s)  LR: 2.486e-04  Data: 0.011 (0.012)
Train: 404 [1050/1251 ( 84%)]  Loss: 3.245 (3.46)  Time: 0.671s, 1525.34/s  (0.694s, 1476.19/s)  LR: 2.486e-04  Data: 0.011 (0.012)
Train: 404 [1100/1251 ( 88%)]  Loss: 3.551 (3.47)  Time: 0.671s, 1525.42/s  (0.694s, 1476.56/s)  LR: 2.486e-04  Data: 0.010 (0.012)
Train: 404 [1150/1251 ( 92%)]  Loss: 3.329 (3.46)  Time: 0.672s, 1524.38/s  (0.693s, 1477.19/s)  LR: 2.486e-04  Data: 0.010 (0.012)
Train: 404 [1200/1251 ( 96%)]  Loss: 3.584 (3.47)  Time: 0.669s, 1531.16/s  (0.693s, 1477.08/s)  LR: 2.486e-04  Data: 0.009 (0.012)
Train: 404 [1250/1251 (100%)]  Loss: 3.622 (3.47)  Time: 0.661s, 1549.59/s  (0.693s, 1477.17/s)  LR: 2.486e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.534 (1.534)  Loss:  0.8799 (0.8799)  Acc@1: 89.1602 (89.1602)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.136 (0.581)  Loss:  0.9507 (1.3563)  Acc@1: 84.9057 (75.3600)  Acc@5: 95.6368 (92.6700)
Train: 405 [   0/1251 (  0%)]  Loss: 3.427 (3.43)  Time: 2.309s,  443.46/s  (2.309s,  443.46/s)  LR: 2.464e-04  Data: 1.693 (1.693)
Train: 405 [  50/1251 (  4%)]  Loss: 3.568 (3.50)  Time: 0.676s, 1515.21/s  (0.734s, 1395.33/s)  LR: 2.464e-04  Data: 0.010 (0.048)
Train: 405 [ 100/1251 (  8%)]  Loss: 3.603 (3.53)  Time: 0.668s, 1533.65/s  (0.715s, 1432.98/s)  LR: 2.464e-04  Data: 0.010 (0.030)
Train: 405 [ 150/1251 ( 12%)]  Loss: 3.529 (3.53)  Time: 0.673s, 1522.34/s  (0.708s, 1446.66/s)  LR: 2.464e-04  Data: 0.010 (0.023)
Train: 405 [ 200/1251 ( 16%)]  Loss: 3.346 (3.49)  Time: 0.702s, 1458.30/s  (0.705s, 1452.78/s)  LR: 2.464e-04  Data: 0.009 (0.020)
Train: 405 [ 250/1251 ( 20%)]  Loss: 3.324 (3.47)  Time: 0.683s, 1498.44/s  (0.703s, 1457.20/s)  LR: 2.464e-04  Data: 0.009 (0.018)
Train: 405 [ 300/1251 ( 24%)]  Loss: 3.402 (3.46)  Time: 0.703s, 1456.15/s  (0.701s, 1461.63/s)  LR: 2.464e-04  Data: 0.009 (0.017)
Train: 405 [ 350/1251 ( 28%)]  Loss: 3.654 (3.48)  Time: 0.676s, 1514.32/s  (0.700s, 1463.82/s)  LR: 2.464e-04  Data: 0.013 (0.016)
Train: 405 [ 400/1251 ( 32%)]  Loss: 3.777 (3.51)  Time: 0.670s, 1527.46/s  (0.699s, 1464.54/s)  LR: 2.464e-04  Data: 0.012 (0.015)
Train: 405 [ 450/1251 ( 36%)]  Loss: 3.445 (3.51)  Time: 0.717s, 1427.59/s  (0.699s, 1465.55/s)  LR: 2.464e-04  Data: 0.009 (0.015)
Train: 405 [ 500/1251 ( 40%)]  Loss: 3.498 (3.51)  Time: 0.704s, 1454.04/s  (0.698s, 1467.39/s)  LR: 2.464e-04  Data: 0.010 (0.014)
Train: 405 [ 550/1251 ( 44%)]  Loss: 3.747 (3.53)  Time: 0.674s, 1519.21/s  (0.697s, 1469.83/s)  LR: 2.464e-04  Data: 0.011 (0.014)
Train: 405 [ 600/1251 ( 48%)]  Loss: 3.535 (3.53)  Time: 0.667s, 1535.56/s  (0.696s, 1470.90/s)  LR: 2.464e-04  Data: 0.010 (0.014)
Train: 405 [ 650/1251 ( 52%)]  Loss: 3.632 (3.53)  Time: 0.709s, 1444.30/s  (0.696s, 1472.18/s)  LR: 2.464e-04  Data: 0.011 (0.013)
Train: 405 [ 700/1251 ( 56%)]  Loss: 3.808 (3.55)  Time: 0.672s, 1523.28/s  (0.695s, 1473.01/s)  LR: 2.464e-04  Data: 0.010 (0.013)
Train: 405 [ 750/1251 ( 60%)]  Loss: 3.344 (3.54)  Time: 0.695s, 1474.01/s  (0.695s, 1472.44/s)  LR: 2.464e-04  Data: 0.009 (0.013)
Train: 405 [ 800/1251 ( 64%)]  Loss: 3.518 (3.54)  Time: 0.723s, 1416.30/s  (0.695s, 1472.72/s)  LR: 2.464e-04  Data: 0.010 (0.013)
Train: 405 [ 850/1251 ( 68%)]  Loss: 3.841 (3.56)  Time: 0.701s, 1460.74/s  (0.696s, 1471.99/s)  LR: 2.464e-04  Data: 0.014 (0.013)
Train: 405 [ 900/1251 ( 72%)]  Loss: 3.561 (3.56)  Time: 0.674s, 1520.05/s  (0.696s, 1471.75/s)  LR: 2.464e-04  Data: 0.010 (0.013)
Train: 405 [ 950/1251 ( 76%)]  Loss: 3.139 (3.53)  Time: 0.681s, 1504.62/s  (0.696s, 1472.20/s)  LR: 2.464e-04  Data: 0.010 (0.012)
Train: 405 [1000/1251 ( 80%)]  Loss: 3.402 (3.53)  Time: 0.675s, 1517.58/s  (0.695s, 1472.87/s)  LR: 2.464e-04  Data: 0.010 (0.012)
Train: 405 [1050/1251 ( 84%)]  Loss: 3.710 (3.54)  Time: 0.713s, 1436.25/s  (0.695s, 1473.32/s)  LR: 2.464e-04  Data: 0.009 (0.012)
Train: 405 [1100/1251 ( 88%)]  Loss: 3.483 (3.53)  Time: 0.690s, 1484.45/s  (0.695s, 1473.39/s)  LR: 2.464e-04  Data: 0.014 (0.012)
Train: 405 [1150/1251 ( 92%)]  Loss: 3.325 (3.53)  Time: 0.671s, 1525.42/s  (0.695s, 1474.00/s)  LR: 2.464e-04  Data: 0.010 (0.012)
Train: 405 [1200/1251 ( 96%)]  Loss: 3.567 (3.53)  Time: 0.693s, 1477.01/s  (0.695s, 1474.04/s)  LR: 2.464e-04  Data: 0.009 (0.012)
Train: 405 [1250/1251 (100%)]  Loss: 3.213 (3.52)  Time: 0.658s, 1557.28/s  (0.695s, 1474.39/s)  LR: 2.464e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.452 (1.452)  Loss:  0.6328 (0.6328)  Acc@1: 90.1367 (90.1367)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.136 (0.582)  Loss:  0.7461 (1.2400)  Acc@1: 85.6132 (75.8100)  Acc@5: 97.2877 (92.8880)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-402.pth.tar', 76.02800011474609)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-400.pth.tar', 75.96399995605469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-401.pth.tar', 75.90400008544921)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-405.pth.tar', 75.81000008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-393.pth.tar', 75.74200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-403.pth.tar', 75.72600006103515)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-390.pth.tar', 75.71400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-399.pth.tar', 75.66000008544921)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-398.pth.tar', 75.63200001708984)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-389.pth.tar', 75.57600005859375)

Train: 406 [   0/1251 (  0%)]  Loss: 3.775 (3.77)  Time: 2.135s,  479.61/s  (2.135s,  479.61/s)  LR: 2.442e-04  Data: 1.518 (1.518)
Train: 406 [  50/1251 (  4%)]  Loss: 3.594 (3.68)  Time: 0.693s, 1477.73/s  (0.730s, 1402.28/s)  LR: 2.442e-04  Data: 0.011 (0.047)
Train: 406 [ 100/1251 (  8%)]  Loss: 3.680 (3.68)  Time: 0.727s, 1409.48/s  (0.712s, 1438.40/s)  LR: 2.442e-04  Data: 0.009 (0.029)
Train: 406 [ 150/1251 ( 12%)]  Loss: 2.811 (3.46)  Time: 0.671s, 1525.10/s  (0.706s, 1450.28/s)  LR: 2.442e-04  Data: 0.010 (0.023)
Train: 406 [ 200/1251 ( 16%)]  Loss: 3.706 (3.51)  Time: 0.669s, 1530.73/s  (0.701s, 1461.20/s)  LR: 2.442e-04  Data: 0.010 (0.020)
Train: 406 [ 250/1251 ( 20%)]  Loss: 3.483 (3.51)  Time: 0.722s, 1417.96/s  (0.699s, 1465.65/s)  LR: 2.442e-04  Data: 0.009 (0.018)
Train: 406 [ 300/1251 ( 24%)]  Loss: 3.631 (3.53)  Time: 0.668s, 1533.51/s  (0.698s, 1466.99/s)  LR: 2.442e-04  Data: 0.009 (0.017)
Train: 406 [ 350/1251 ( 28%)]  Loss: 3.143 (3.48)  Time: 0.668s, 1533.39/s  (0.698s, 1467.29/s)  LR: 2.442e-04  Data: 0.009 (0.016)
Train: 406 [ 400/1251 ( 32%)]  Loss: 3.617 (3.49)  Time: 0.715s, 1432.78/s  (0.697s, 1468.41/s)  LR: 2.442e-04  Data: 0.014 (0.015)
Train: 406 [ 450/1251 ( 36%)]  Loss: 3.378 (3.48)  Time: 0.670s, 1528.05/s  (0.697s, 1469.44/s)  LR: 2.442e-04  Data: 0.009 (0.015)
Train: 406 [ 500/1251 ( 40%)]  Loss: 3.523 (3.49)  Time: 0.672s, 1522.69/s  (0.696s, 1471.68/s)  LR: 2.442e-04  Data: 0.011 (0.014)
Train: 406 [ 550/1251 ( 44%)]  Loss: 3.312 (3.47)  Time: 0.720s, 1422.55/s  (0.695s, 1472.55/s)  LR: 2.442e-04  Data: 0.013 (0.014)
Train: 406 [ 600/1251 ( 48%)]  Loss: 3.285 (3.46)  Time: 0.668s, 1532.07/s  (0.695s, 1473.32/s)  LR: 2.442e-04  Data: 0.014 (0.014)
Train: 406 [ 650/1251 ( 52%)]  Loss: 3.496 (3.46)  Time: 0.672s, 1522.87/s  (0.695s, 1474.19/s)  LR: 2.442e-04  Data: 0.011 (0.013)
Train: 406 [ 700/1251 ( 56%)]  Loss: 3.369 (3.45)  Time: 0.697s, 1468.36/s  (0.694s, 1475.17/s)  LR: 2.442e-04  Data: 0.012 (0.013)
Train: 406 [ 750/1251 ( 60%)]  Loss: 3.314 (3.44)  Time: 0.714s, 1433.44/s  (0.694s, 1475.25/s)  LR: 2.442e-04  Data: 0.012 (0.013)
Train: 406 [ 800/1251 ( 64%)]  Loss: 3.528 (3.45)  Time: 0.706s, 1449.46/s  (0.694s, 1475.47/s)  LR: 2.442e-04  Data: 0.010 (0.013)
Train: 406 [ 850/1251 ( 68%)]  Loss: 3.482 (3.45)  Time: 0.671s, 1525.22/s  (0.694s, 1475.19/s)  LR: 2.442e-04  Data: 0.013 (0.013)
Train: 406 [ 900/1251 ( 72%)]  Loss: 3.559 (3.46)  Time: 0.671s, 1527.05/s  (0.694s, 1476.37/s)  LR: 2.442e-04  Data: 0.011 (0.013)
Train: 406 [ 950/1251 ( 76%)]  Loss: 3.426 (3.46)  Time: 0.681s, 1504.17/s  (0.694s, 1475.98/s)  LR: 2.442e-04  Data: 0.012 (0.012)
Train: 406 [1000/1251 ( 80%)]  Loss: 3.566 (3.46)  Time: 0.715s, 1431.35/s  (0.694s, 1476.17/s)  LR: 2.442e-04  Data: 0.013 (0.012)
Train: 406 [1050/1251 ( 84%)]  Loss: 3.593 (3.47)  Time: 0.672s, 1523.15/s  (0.693s, 1477.29/s)  LR: 2.442e-04  Data: 0.010 (0.012)
Train: 406 [1100/1251 ( 88%)]  Loss: 3.537 (3.47)  Time: 0.665s, 1539.08/s  (0.693s, 1477.29/s)  LR: 2.442e-04  Data: 0.010 (0.012)
Train: 406 [1150/1251 ( 92%)]  Loss: 3.275 (3.46)  Time: 0.674s, 1519.08/s  (0.693s, 1477.90/s)  LR: 2.442e-04  Data: 0.010 (0.012)
Train: 406 [1200/1251 ( 96%)]  Loss: 3.153 (3.45)  Time: 0.673s, 1522.45/s  (0.693s, 1477.00/s)  LR: 2.442e-04  Data: 0.011 (0.012)
Train: 406 [1250/1251 (100%)]  Loss: 3.261 (3.44)  Time: 0.658s, 1555.88/s  (0.693s, 1477.35/s)  LR: 2.442e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.563 (1.563)  Loss:  0.8125 (0.8125)  Acc@1: 90.2344 (90.2344)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.136 (0.576)  Loss:  0.8730 (1.3452)  Acc@1: 86.4387 (75.7260)  Acc@5: 96.3443 (92.8240)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-402.pth.tar', 76.02800011474609)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-400.pth.tar', 75.96399995605469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-401.pth.tar', 75.90400008544921)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-405.pth.tar', 75.81000008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-393.pth.tar', 75.74200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-403.pth.tar', 75.72600006103515)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-406.pth.tar', 75.72600005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-390.pth.tar', 75.71400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-399.pth.tar', 75.66000008544921)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-398.pth.tar', 75.63200001708984)

Train: 407 [   0/1251 (  0%)]  Loss: 3.702 (3.70)  Time: 2.233s,  458.55/s  (2.233s,  458.55/s)  LR: 2.420e-04  Data: 1.618 (1.618)
Train: 407 [  50/1251 (  4%)]  Loss: 3.573 (3.64)  Time: 0.700s, 1462.03/s  (0.732s, 1399.71/s)  LR: 2.420e-04  Data: 0.009 (0.048)
Train: 407 [ 100/1251 (  8%)]  Loss: 3.042 (3.44)  Time: 0.676s, 1513.88/s  (0.711s, 1441.09/s)  LR: 2.420e-04  Data: 0.015 (0.029)
Train: 407 [ 150/1251 ( 12%)]  Loss: 3.780 (3.52)  Time: 0.729s, 1405.32/s  (0.704s, 1455.38/s)  LR: 2.420e-04  Data: 0.009 (0.023)
Train: 407 [ 200/1251 ( 16%)]  Loss: 3.477 (3.51)  Time: 0.667s, 1535.77/s  (0.702s, 1459.54/s)  LR: 2.420e-04  Data: 0.010 (0.020)
Train: 407 [ 250/1251 ( 20%)]  Loss: 3.277 (3.48)  Time: 0.696s, 1471.24/s  (0.700s, 1463.75/s)  LR: 2.420e-04  Data: 0.009 (0.018)
Train: 407 [ 300/1251 ( 24%)]  Loss: 3.513 (3.48)  Time: 0.673s, 1520.65/s  (0.699s, 1464.95/s)  LR: 2.420e-04  Data: 0.010 (0.017)
Train: 407 [ 350/1251 ( 28%)]  Loss: 3.854 (3.53)  Time: 0.680s, 1506.37/s  (0.698s, 1468.02/s)  LR: 2.420e-04  Data: 0.012 (0.016)
Train: 407 [ 400/1251 ( 32%)]  Loss: 3.150 (3.49)  Time: 0.703s, 1456.77/s  (0.697s, 1468.35/s)  LR: 2.420e-04  Data: 0.009 (0.015)
Train: 407 [ 450/1251 ( 36%)]  Loss: 3.451 (3.48)  Time: 0.705s, 1451.66/s  (0.696s, 1470.49/s)  LR: 2.420e-04  Data: 0.010 (0.015)
Train: 407 [ 500/1251 ( 40%)]  Loss: 3.553 (3.49)  Time: 0.670s, 1528.39/s  (0.696s, 1470.74/s)  LR: 2.420e-04  Data: 0.009 (0.014)
Train: 407 [ 550/1251 ( 44%)]  Loss: 3.485 (3.49)  Time: 0.713s, 1436.06/s  (0.697s, 1470.11/s)  LR: 2.420e-04  Data: 0.010 (0.014)
Train: 407 [ 600/1251 ( 48%)]  Loss: 3.424 (3.48)  Time: 0.715s, 1433.06/s  (0.697s, 1470.17/s)  LR: 2.420e-04  Data: 0.009 (0.014)
Train: 407 [ 650/1251 ( 52%)]  Loss: 3.492 (3.48)  Time: 0.669s, 1531.66/s  (0.696s, 1471.66/s)  LR: 2.420e-04  Data: 0.010 (0.013)
Train: 407 [ 700/1251 ( 56%)]  Loss: 3.307 (3.47)  Time: 0.675s, 1517.23/s  (0.696s, 1472.28/s)  LR: 2.420e-04  Data: 0.011 (0.013)
Train: 407 [ 750/1251 ( 60%)]  Loss: 3.484 (3.47)  Time: 0.800s, 1279.59/s  (0.696s, 1472.17/s)  LR: 2.420e-04  Data: 0.018 (0.013)
Train: 407 [ 800/1251 ( 64%)]  Loss: 3.291 (3.46)  Time: 0.671s, 1524.98/s  (0.695s, 1472.81/s)  LR: 2.420e-04  Data: 0.009 (0.013)
Train: 407 [ 850/1251 ( 68%)]  Loss: 3.508 (3.46)  Time: 0.670s, 1528.03/s  (0.695s, 1472.72/s)  LR: 2.420e-04  Data: 0.010 (0.013)
Train: 407 [ 900/1251 ( 72%)]  Loss: 3.522 (3.47)  Time: 0.700s, 1461.94/s  (0.695s, 1472.75/s)  LR: 2.420e-04  Data: 0.008 (0.013)
Train: 407 [ 950/1251 ( 76%)]  Loss: 3.172 (3.45)  Time: 0.671s, 1526.97/s  (0.695s, 1472.51/s)  LR: 2.420e-04  Data: 0.010 (0.012)
Train: 407 [1000/1251 ( 80%)]  Loss: 3.423 (3.45)  Time: 0.704s, 1455.27/s  (0.696s, 1472.26/s)  LR: 2.420e-04  Data: 0.010 (0.012)
Train: 407 [1050/1251 ( 84%)]  Loss: 3.267 (3.44)  Time: 0.723s, 1416.66/s  (0.695s, 1472.32/s)  LR: 2.420e-04  Data: 0.012 (0.012)
Train: 407 [1100/1251 ( 88%)]  Loss: 3.222 (3.43)  Time: 0.672s, 1523.13/s  (0.695s, 1473.08/s)  LR: 2.420e-04  Data: 0.010 (0.012)
Train: 407 [1150/1251 ( 92%)]  Loss: 3.422 (3.43)  Time: 0.718s, 1426.73/s  (0.695s, 1473.39/s)  LR: 2.420e-04  Data: 0.010 (0.012)
Train: 407 [1200/1251 ( 96%)]  Loss: 3.481 (3.43)  Time: 0.686s, 1492.05/s  (0.695s, 1474.01/s)  LR: 2.420e-04  Data: 0.008 (0.012)
Train: 407 [1250/1251 (100%)]  Loss: 3.434 (3.43)  Time: 0.666s, 1537.45/s  (0.694s, 1474.76/s)  LR: 2.420e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.548 (1.548)  Loss:  0.7871 (0.7871)  Acc@1: 90.3320 (90.3320)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.136 (0.584)  Loss:  0.9121 (1.3045)  Acc@1: 86.3208 (76.2320)  Acc@5: 96.5802 (92.9900)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-407.pth.tar', 76.2320000048828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-402.pth.tar', 76.02800011474609)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-400.pth.tar', 75.96399995605469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-401.pth.tar', 75.90400008544921)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-405.pth.tar', 75.81000008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-393.pth.tar', 75.74200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-403.pth.tar', 75.72600006103515)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-406.pth.tar', 75.72600005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-390.pth.tar', 75.71400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-399.pth.tar', 75.66000008544921)

Train: 408 [   0/1251 (  0%)]  Loss: 3.701 (3.70)  Time: 2.074s,  493.66/s  (2.074s,  493.66/s)  LR: 2.398e-04  Data: 1.461 (1.461)
Train: 408 [  50/1251 (  4%)]  Loss: 3.509 (3.61)  Time: 0.721s, 1420.19/s  (0.732s, 1399.17/s)  LR: 2.398e-04  Data: 0.012 (0.051)
Train: 408 [ 100/1251 (  8%)]  Loss: 3.645 (3.62)  Time: 0.721s, 1420.41/s  (0.713s, 1436.19/s)  LR: 2.398e-04  Data: 0.017 (0.031)
Train: 408 [ 150/1251 ( 12%)]  Loss: 3.371 (3.56)  Time: 0.675s, 1517.62/s  (0.705s, 1453.46/s)  LR: 2.398e-04  Data: 0.011 (0.024)
Train: 408 [ 200/1251 ( 16%)]  Loss: 3.535 (3.55)  Time: 0.674s, 1519.36/s  (0.702s, 1458.75/s)  LR: 2.398e-04  Data: 0.009 (0.021)
Train: 408 [ 250/1251 ( 20%)]  Loss: 3.746 (3.58)  Time: 0.718s, 1426.37/s  (0.699s, 1464.53/s)  LR: 2.398e-04  Data: 0.010 (0.019)
Train: 408 [ 300/1251 ( 24%)]  Loss: 3.925 (3.63)  Time: 0.675s, 1517.01/s  (0.697s, 1469.53/s)  LR: 2.398e-04  Data: 0.011 (0.017)
Train: 408 [ 350/1251 ( 28%)]  Loss: 3.399 (3.60)  Time: 0.705s, 1452.22/s  (0.696s, 1471.69/s)  LR: 2.398e-04  Data: 0.014 (0.016)
Train: 408 [ 400/1251 ( 32%)]  Loss: 3.659 (3.61)  Time: 0.673s, 1520.94/s  (0.696s, 1471.56/s)  LR: 2.398e-04  Data: 0.010 (0.016)
Train: 408 [ 450/1251 ( 36%)]  Loss: 3.398 (3.59)  Time: 0.707s, 1449.14/s  (0.695s, 1472.47/s)  LR: 2.398e-04  Data: 0.011 (0.015)
Train: 408 [ 500/1251 ( 40%)]  Loss: 3.619 (3.59)  Time: 0.698s, 1466.55/s  (0.695s, 1472.59/s)  LR: 2.398e-04  Data: 0.011 (0.015)
Train: 408 [ 550/1251 ( 44%)]  Loss: 3.276 (3.57)  Time: 0.672s, 1524.26/s  (0.695s, 1473.78/s)  LR: 2.398e-04  Data: 0.011 (0.014)
Train: 408 [ 600/1251 ( 48%)]  Loss: 3.160 (3.53)  Time: 0.693s, 1478.50/s  (0.695s, 1474.35/s)  LR: 2.398e-04  Data: 0.009 (0.014)
Train: 408 [ 650/1251 ( 52%)]  Loss: 3.528 (3.53)  Time: 0.678s, 1510.00/s  (0.695s, 1474.25/s)  LR: 2.398e-04  Data: 0.011 (0.014)
Train: 408 [ 700/1251 ( 56%)]  Loss: 3.134 (3.51)  Time: 0.687s, 1490.95/s  (0.694s, 1475.69/s)  LR: 2.398e-04  Data: 0.010 (0.013)
Train: 408 [ 750/1251 ( 60%)]  Loss: 3.709 (3.52)  Time: 0.757s, 1352.04/s  (0.694s, 1474.69/s)  LR: 2.398e-04  Data: 0.011 (0.013)
Train: 408 [ 800/1251 ( 64%)]  Loss: 3.765 (3.53)  Time: 0.675s, 1517.62/s  (0.695s, 1474.16/s)  LR: 2.398e-04  Data: 0.011 (0.013)
Train: 408 [ 850/1251 ( 68%)]  Loss: 3.050 (3.51)  Time: 0.672s, 1522.80/s  (0.694s, 1474.80/s)  LR: 2.398e-04  Data: 0.010 (0.013)
Train: 408 [ 900/1251 ( 72%)]  Loss: 3.720 (3.52)  Time: 0.703s, 1455.63/s  (0.694s, 1475.66/s)  LR: 2.398e-04  Data: 0.009 (0.013)
Train: 408 [ 950/1251 ( 76%)]  Loss: 3.356 (3.51)  Time: 0.706s, 1450.36/s  (0.694s, 1476.22/s)  LR: 2.398e-04  Data: 0.011 (0.013)
Train: 408 [1000/1251 ( 80%)]  Loss: 3.162 (3.49)  Time: 0.671s, 1526.52/s  (0.694s, 1476.14/s)  LR: 2.398e-04  Data: 0.011 (0.013)
Train: 408 [1050/1251 ( 84%)]  Loss: 3.680 (3.50)  Time: 0.709s, 1444.22/s  (0.694s, 1475.71/s)  LR: 2.398e-04  Data: 0.014 (0.012)
Train: 408 [1100/1251 ( 88%)]  Loss: 3.889 (3.52)  Time: 0.674s, 1519.99/s  (0.694s, 1475.83/s)  LR: 2.398e-04  Data: 0.011 (0.012)
Train: 408 [1150/1251 ( 92%)]  Loss: 3.503 (3.52)  Time: 0.705s, 1452.33/s  (0.694s, 1475.74/s)  LR: 2.398e-04  Data: 0.011 (0.012)
Train: 408 [1200/1251 ( 96%)]  Loss: 3.271 (3.51)  Time: 0.673s, 1521.79/s  (0.694s, 1475.83/s)  LR: 2.398e-04  Data: 0.011 (0.012)
Train: 408 [1250/1251 (100%)]  Loss: 3.391 (3.50)  Time: 0.717s, 1428.61/s  (0.694s, 1475.44/s)  LR: 2.398e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.621 (1.621)  Loss:  0.7104 (0.7104)  Acc@1: 91.2109 (91.2109)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.136 (0.579)  Loss:  0.8027 (1.2368)  Acc@1: 85.7311 (76.1440)  Acc@5: 97.1698 (92.8060)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-407.pth.tar', 76.2320000048828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-408.pth.tar', 76.14400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-402.pth.tar', 76.02800011474609)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-400.pth.tar', 75.96399995605469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-401.pth.tar', 75.90400008544921)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-405.pth.tar', 75.81000008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-393.pth.tar', 75.74200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-403.pth.tar', 75.72600006103515)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-406.pth.tar', 75.72600005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-390.pth.tar', 75.71400000732422)

Train: 409 [   0/1251 (  0%)]  Loss: 3.424 (3.42)  Time: 2.204s,  464.70/s  (2.204s,  464.70/s)  LR: 2.376e-04  Data: 1.587 (1.587)
Train: 409 [  50/1251 (  4%)]  Loss: 3.620 (3.52)  Time: 0.672s, 1523.99/s  (0.731s, 1399.94/s)  LR: 2.376e-04  Data: 0.010 (0.049)
Train: 409 [ 100/1251 (  8%)]  Loss: 3.700 (3.58)  Time: 0.672s, 1524.94/s  (0.713s, 1435.81/s)  LR: 2.376e-04  Data: 0.010 (0.030)
Train: 409 [ 150/1251 ( 12%)]  Loss: 3.315 (3.51)  Time: 0.730s, 1402.91/s  (0.707s, 1447.77/s)  LR: 2.376e-04  Data: 0.011 (0.023)
Train: 409 [ 200/1251 ( 16%)]  Loss: 3.574 (3.53)  Time: 0.717s, 1428.81/s  (0.702s, 1458.24/s)  LR: 2.376e-04  Data: 0.009 (0.020)
Train: 409 [ 250/1251 ( 20%)]  Loss: 3.482 (3.52)  Time: 0.677s, 1512.68/s  (0.699s, 1464.29/s)  LR: 2.376e-04  Data: 0.010 (0.018)
Train: 409 [ 300/1251 ( 24%)]  Loss: 3.561 (3.53)  Time: 0.718s, 1425.77/s  (0.697s, 1468.54/s)  LR: 2.376e-04  Data: 0.009 (0.017)
Train: 409 [ 350/1251 ( 28%)]  Loss: 3.550 (3.53)  Time: 0.673s, 1521.96/s  (0.697s, 1469.81/s)  LR: 2.376e-04  Data: 0.009 (0.016)
Train: 409 [ 400/1251 ( 32%)]  Loss: 3.126 (3.48)  Time: 0.720s, 1422.40/s  (0.697s, 1469.80/s)  LR: 2.376e-04  Data: 0.008 (0.015)
Train: 409 [ 450/1251 ( 36%)]  Loss: 3.503 (3.49)  Time: 0.679s, 1508.27/s  (0.697s, 1470.12/s)  LR: 2.376e-04  Data: 0.010 (0.015)
Train: 409 [ 500/1251 ( 40%)]  Loss: 3.609 (3.50)  Time: 0.668s, 1532.19/s  (0.696s, 1471.40/s)  LR: 2.376e-04  Data: 0.009 (0.014)
Train: 409 [ 550/1251 ( 44%)]  Loss: 3.309 (3.48)  Time: 0.688s, 1488.80/s  (0.695s, 1473.17/s)  LR: 2.376e-04  Data: 0.009 (0.014)
Train: 409 [ 600/1251 ( 48%)]  Loss: 3.659 (3.49)  Time: 0.705s, 1452.59/s  (0.695s, 1474.03/s)  LR: 2.376e-04  Data: 0.010 (0.014)
Train: 409 [ 650/1251 ( 52%)]  Loss: 3.442 (3.49)  Time: 0.670s, 1527.52/s  (0.695s, 1474.35/s)  LR: 2.376e-04  Data: 0.009 (0.013)
Train: 409 [ 700/1251 ( 56%)]  Loss: 3.515 (3.49)  Time: 0.673s, 1522.40/s  (0.694s, 1475.91/s)  LR: 2.376e-04  Data: 0.011 (0.013)
Train: 409 [ 750/1251 ( 60%)]  Loss: 3.542 (3.50)  Time: 0.671s, 1526.77/s  (0.694s, 1476.19/s)  LR: 2.376e-04  Data: 0.010 (0.013)
Train: 409 [ 800/1251 ( 64%)]  Loss: 3.262 (3.48)  Time: 0.671s, 1526.62/s  (0.694s, 1475.89/s)  LR: 2.376e-04  Data: 0.010 (0.013)
Train: 409 [ 850/1251 ( 68%)]  Loss: 3.310 (3.47)  Time: 0.709s, 1444.69/s  (0.694s, 1474.74/s)  LR: 2.376e-04  Data: 0.009 (0.013)
Train: 409 [ 900/1251 ( 72%)]  Loss: 3.271 (3.46)  Time: 0.757s, 1352.40/s  (0.694s, 1474.92/s)  LR: 2.376e-04  Data: 0.010 (0.013)
Train: 409 [ 950/1251 ( 76%)]  Loss: 3.129 (3.45)  Time: 0.672s, 1523.37/s  (0.694s, 1475.15/s)  LR: 2.376e-04  Data: 0.010 (0.012)
Train: 409 [1000/1251 ( 80%)]  Loss: 3.310 (3.44)  Time: 0.671s, 1526.40/s  (0.694s, 1475.24/s)  LR: 2.376e-04  Data: 0.010 (0.012)
Train: 409 [1050/1251 ( 84%)]  Loss: 3.888 (3.46)  Time: 0.699s, 1464.48/s  (0.694s, 1474.53/s)  LR: 2.376e-04  Data: 0.010 (0.012)
Train: 409 [1100/1251 ( 88%)]  Loss: 3.335 (3.45)  Time: 0.729s, 1405.61/s  (0.694s, 1474.83/s)  LR: 2.376e-04  Data: 0.009 (0.012)
Train: 409 [1150/1251 ( 92%)]  Loss: 3.340 (3.45)  Time: 0.706s, 1450.91/s  (0.694s, 1475.28/s)  LR: 2.376e-04  Data: 0.009 (0.012)
Train: 409 [1200/1251 ( 96%)]  Loss: 3.382 (3.45)  Time: 0.671s, 1526.84/s  (0.694s, 1475.04/s)  LR: 2.376e-04  Data: 0.009 (0.012)
Train: 409 [1250/1251 (100%)]  Loss: 3.504 (3.45)  Time: 0.692s, 1480.04/s  (0.694s, 1475.85/s)  LR: 2.376e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.520 (1.520)  Loss:  0.7632 (0.7632)  Acc@1: 90.6250 (90.6250)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.136 (0.589)  Loss:  0.8740 (1.3085)  Acc@1: 84.3160 (75.7800)  Acc@5: 96.1085 (92.9220)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-407.pth.tar', 76.2320000048828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-408.pth.tar', 76.14400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-402.pth.tar', 76.02800011474609)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-400.pth.tar', 75.96399995605469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-401.pth.tar', 75.90400008544921)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-405.pth.tar', 75.81000008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-409.pth.tar', 75.78000016845704)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-393.pth.tar', 75.74200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-403.pth.tar', 75.72600006103515)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-406.pth.tar', 75.72600005615234)

Train: 410 [   0/1251 (  0%)]  Loss: 3.272 (3.27)  Time: 2.207s,  463.87/s  (2.207s,  463.87/s)  LR: 2.354e-04  Data: 1.593 (1.593)
Train: 410 [  50/1251 (  4%)]  Loss: 3.458 (3.36)  Time: 0.729s, 1404.35/s  (0.731s, 1400.20/s)  LR: 2.354e-04  Data: 0.011 (0.054)
Train: 410 [ 100/1251 (  8%)]  Loss: 3.645 (3.46)  Time: 0.695s, 1472.87/s  (0.724s, 1414.59/s)  LR: 2.354e-04  Data: 0.010 (0.033)
Train: 410 [ 150/1251 ( 12%)]  Loss: 3.533 (3.48)  Time: 0.710s, 1442.93/s  (0.720s, 1423.20/s)  LR: 2.354e-04  Data: 0.009 (0.026)
Train: 410 [ 200/1251 ( 16%)]  Loss: 3.496 (3.48)  Time: 0.679s, 1508.38/s  (0.717s, 1427.20/s)  LR: 2.354e-04  Data: 0.011 (0.022)
Train: 410 [ 250/1251 ( 20%)]  Loss: 3.489 (3.48)  Time: 0.716s, 1430.83/s  (0.715s, 1431.97/s)  LR: 2.354e-04  Data: 0.010 (0.020)
Train: 410 [ 300/1251 ( 24%)]  Loss: 3.678 (3.51)  Time: 0.704s, 1455.56/s  (0.711s, 1440.99/s)  LR: 2.354e-04  Data: 0.009 (0.018)
Train: 410 [ 350/1251 ( 28%)]  Loss: 3.557 (3.52)  Time: 0.672s, 1524.73/s  (0.706s, 1450.25/s)  LR: 2.354e-04  Data: 0.011 (0.017)
Train: 410 [ 400/1251 ( 32%)]  Loss: 3.812 (3.55)  Time: 0.668s, 1533.42/s  (0.705s, 1452.33/s)  LR: 2.354e-04  Data: 0.010 (0.016)
Train: 410 [ 450/1251 ( 36%)]  Loss: 3.283 (3.52)  Time: 0.676s, 1514.64/s  (0.704s, 1454.50/s)  LR: 2.354e-04  Data: 0.010 (0.016)
Train: 410 [ 500/1251 ( 40%)]  Loss: 3.143 (3.49)  Time: 0.668s, 1531.95/s  (0.703s, 1456.84/s)  LR: 2.354e-04  Data: 0.009 (0.015)
Train: 410 [ 550/1251 ( 44%)]  Loss: 3.351 (3.48)  Time: 0.678s, 1510.62/s  (0.701s, 1459.88/s)  LR: 2.354e-04  Data: 0.011 (0.015)
Train: 410 [ 600/1251 ( 48%)]  Loss: 3.291 (3.46)  Time: 0.732s, 1399.84/s  (0.700s, 1461.92/s)  LR: 2.354e-04  Data: 0.011 (0.014)
Train: 410 [ 650/1251 ( 52%)]  Loss: 3.605 (3.47)  Time: 0.769s, 1331.48/s  (0.700s, 1463.66/s)  LR: 2.354e-04  Data: 0.009 (0.014)
Train: 410 [ 700/1251 ( 56%)]  Loss: 3.472 (3.47)  Time: 0.720s, 1421.94/s  (0.699s, 1465.17/s)  LR: 2.354e-04  Data: 0.009 (0.014)
Train: 410 [ 750/1251 ( 60%)]  Loss: 3.119 (3.45)  Time: 0.688s, 1489.14/s  (0.698s, 1466.78/s)  LR: 2.354e-04  Data: 0.011 (0.014)
Train: 410 [ 800/1251 ( 64%)]  Loss: 3.487 (3.45)  Time: 0.668s, 1532.15/s  (0.698s, 1467.67/s)  LR: 2.354e-04  Data: 0.009 (0.013)
Train: 410 [ 850/1251 ( 68%)]  Loss: 3.783 (3.47)  Time: 0.670s, 1528.64/s  (0.698s, 1467.99/s)  LR: 2.354e-04  Data: 0.010 (0.013)
Train: 410 [ 900/1251 ( 72%)]  Loss: 3.627 (3.48)  Time: 0.679s, 1507.78/s  (0.697s, 1469.40/s)  LR: 2.354e-04  Data: 0.009 (0.013)
Train: 410 [ 950/1251 ( 76%)]  Loss: 3.813 (3.50)  Time: 0.708s, 1446.36/s  (0.697s, 1470.12/s)  LR: 2.354e-04  Data: 0.009 (0.013)
Train: 410 [1000/1251 ( 80%)]  Loss: 3.312 (3.49)  Time: 0.677s, 1513.55/s  (0.696s, 1470.72/s)  LR: 2.354e-04  Data: 0.009 (0.013)
Train: 410 [1050/1251 ( 84%)]  Loss: 3.862 (3.50)  Time: 0.674s, 1519.31/s  (0.696s, 1471.11/s)  LR: 2.354e-04  Data: 0.011 (0.013)
Train: 410 [1100/1251 ( 88%)]  Loss: 3.245 (3.49)  Time: 0.727s, 1408.17/s  (0.696s, 1471.99/s)  LR: 2.354e-04  Data: 0.009 (0.012)
Train: 410 [1150/1251 ( 92%)]  Loss: 3.244 (3.48)  Time: 0.667s, 1535.54/s  (0.696s, 1472.28/s)  LR: 2.354e-04  Data: 0.009 (0.012)
Train: 410 [1200/1251 ( 96%)]  Loss: 3.529 (3.48)  Time: 0.709s, 1443.96/s  (0.695s, 1472.72/s)  LR: 2.354e-04  Data: 0.014 (0.012)
Train: 410 [1250/1251 (100%)]  Loss: 3.306 (3.48)  Time: 0.657s, 1558.39/s  (0.695s, 1473.11/s)  LR: 2.354e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.447 (1.447)  Loss:  0.7607 (0.7607)  Acc@1: 90.4297 (90.4297)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.139 (0.586)  Loss:  0.8320 (1.2920)  Acc@1: 86.6745 (75.8240)  Acc@5: 96.8160 (92.8760)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-407.pth.tar', 76.2320000048828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-408.pth.tar', 76.14400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-402.pth.tar', 76.02800011474609)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-400.pth.tar', 75.96399995605469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-401.pth.tar', 75.90400008544921)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-410.pth.tar', 75.82400002929687)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-405.pth.tar', 75.81000008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-409.pth.tar', 75.78000016845704)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-393.pth.tar', 75.74200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-403.pth.tar', 75.72600006103515)

Train: 411 [   0/1251 (  0%)]  Loss: 3.639 (3.64)  Time: 2.357s,  434.39/s  (2.357s,  434.39/s)  LR: 2.332e-04  Data: 1.738 (1.738)
Train: 411 [  50/1251 (  4%)]  Loss: 2.596 (3.12)  Time: 0.672s, 1522.83/s  (0.728s, 1406.01/s)  LR: 2.332e-04  Data: 0.012 (0.045)
Train: 411 [ 100/1251 (  8%)]  Loss: 3.692 (3.31)  Time: 0.727s, 1407.72/s  (0.712s, 1438.94/s)  LR: 2.332e-04  Data: 0.009 (0.028)
Train: 411 [ 150/1251 ( 12%)]  Loss: 3.595 (3.38)  Time: 0.677s, 1513.53/s  (0.706s, 1451.39/s)  LR: 2.332e-04  Data: 0.011 (0.022)
Train: 411 [ 200/1251 ( 16%)]  Loss: 3.565 (3.42)  Time: 0.671s, 1525.66/s  (0.702s, 1457.89/s)  LR: 2.332e-04  Data: 0.010 (0.019)
Train: 411 [ 250/1251 ( 20%)]  Loss: 3.443 (3.42)  Time: 0.717s, 1429.03/s  (0.700s, 1462.20/s)  LR: 2.332e-04  Data: 0.011 (0.018)
Train: 411 [ 300/1251 ( 24%)]  Loss: 3.629 (3.45)  Time: 0.673s, 1520.86/s  (0.700s, 1462.66/s)  LR: 2.332e-04  Data: 0.010 (0.016)
Train: 411 [ 350/1251 ( 28%)]  Loss: 3.483 (3.46)  Time: 0.697s, 1469.10/s  (0.698s, 1467.86/s)  LR: 2.332e-04  Data: 0.009 (0.016)
Train: 411 [ 400/1251 ( 32%)]  Loss: 3.361 (3.44)  Time: 0.671s, 1525.40/s  (0.696s, 1470.49/s)  LR: 2.332e-04  Data: 0.010 (0.015)
Train: 411 [ 450/1251 ( 36%)]  Loss: 3.190 (3.42)  Time: 0.688s, 1487.75/s  (0.696s, 1470.74/s)  LR: 2.332e-04  Data: 0.013 (0.014)
Train: 411 [ 500/1251 ( 40%)]  Loss: 3.658 (3.44)  Time: 0.672s, 1522.85/s  (0.696s, 1471.39/s)  LR: 2.332e-04  Data: 0.012 (0.014)
Train: 411 [ 550/1251 ( 44%)]  Loss: 3.288 (3.43)  Time: 0.724s, 1414.78/s  (0.696s, 1471.15/s)  LR: 2.332e-04  Data: 0.010 (0.014)
Train: 411 [ 600/1251 ( 48%)]  Loss: 3.269 (3.42)  Time: 0.675s, 1517.57/s  (0.696s, 1471.91/s)  LR: 2.332e-04  Data: 0.011 (0.014)
Train: 411 [ 650/1251 ( 52%)]  Loss: 3.801 (3.44)  Time: 0.705s, 1451.75/s  (0.696s, 1471.88/s)  LR: 2.332e-04  Data: 0.009 (0.013)
Train: 411 [ 700/1251 ( 56%)]  Loss: 3.491 (3.45)  Time: 0.670s, 1527.58/s  (0.696s, 1471.77/s)  LR: 2.332e-04  Data: 0.010 (0.013)
Train: 411 [ 750/1251 ( 60%)]  Loss: 3.472 (3.45)  Time: 0.793s, 1291.56/s  (0.696s, 1472.09/s)  LR: 2.332e-04  Data: 0.009 (0.013)
Train: 411 [ 800/1251 ( 64%)]  Loss: 3.239 (3.44)  Time: 0.694s, 1474.60/s  (0.696s, 1471.88/s)  LR: 2.332e-04  Data: 0.010 (0.013)
Train: 411 [ 850/1251 ( 68%)]  Loss: 3.370 (3.43)  Time: 0.758s, 1350.86/s  (0.696s, 1471.97/s)  LR: 2.332e-04  Data: 0.009 (0.013)
Train: 411 [ 900/1251 ( 72%)]  Loss: 3.681 (3.45)  Time: 0.682s, 1500.48/s  (0.695s, 1472.39/s)  LR: 2.332e-04  Data: 0.009 (0.013)
Train: 411 [ 950/1251 ( 76%)]  Loss: 3.567 (3.45)  Time: 0.700s, 1463.75/s  (0.695s, 1472.65/s)  LR: 2.332e-04  Data: 0.009 (0.012)
Train: 411 [1000/1251 ( 80%)]  Loss: 3.172 (3.44)  Time: 0.714s, 1435.12/s  (0.695s, 1473.43/s)  LR: 2.332e-04  Data: 0.010 (0.012)
Train: 411 [1050/1251 ( 84%)]  Loss: 3.397 (3.44)  Time: 0.680s, 1506.46/s  (0.695s, 1474.31/s)  LR: 2.332e-04  Data: 0.009 (0.012)
Train: 411 [1100/1251 ( 88%)]  Loss: 3.533 (3.44)  Time: 0.676s, 1514.06/s  (0.694s, 1475.22/s)  LR: 2.332e-04  Data: 0.010 (0.012)
Train: 411 [1150/1251 ( 92%)]  Loss: 3.662 (3.45)  Time: 0.689s, 1485.88/s  (0.694s, 1475.32/s)  LR: 2.332e-04  Data: 0.009 (0.012)
Train: 411 [1200/1251 ( 96%)]  Loss: 3.709 (3.46)  Time: 0.676s, 1514.32/s  (0.694s, 1475.53/s)  LR: 2.332e-04  Data: 0.010 (0.012)
Train: 411 [1250/1251 (100%)]  Loss: 3.930 (3.48)  Time: 0.689s, 1485.40/s  (0.694s, 1476.03/s)  LR: 2.332e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.636 (1.636)  Loss:  0.7466 (0.7466)  Acc@1: 89.5508 (89.5508)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.137 (0.591)  Loss:  0.9473 (1.2960)  Acc@1: 85.0236 (75.9540)  Acc@5: 96.3443 (92.7760)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-407.pth.tar', 76.2320000048828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-408.pth.tar', 76.14400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-402.pth.tar', 76.02800011474609)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-400.pth.tar', 75.96399995605469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-411.pth.tar', 75.95400008789062)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-401.pth.tar', 75.90400008544921)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-410.pth.tar', 75.82400002929687)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-405.pth.tar', 75.81000008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-409.pth.tar', 75.78000016845704)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-393.pth.tar', 75.74200002929688)

Train: 412 [   0/1251 (  0%)]  Loss: 3.458 (3.46)  Time: 2.165s,  473.05/s  (2.165s,  473.05/s)  LR: 2.311e-04  Data: 1.549 (1.549)
Train: 412 [  50/1251 (  4%)]  Loss: 3.809 (3.63)  Time: 0.693s, 1478.28/s  (0.729s, 1405.21/s)  LR: 2.311e-04  Data: 0.010 (0.046)
Train: 412 [ 100/1251 (  8%)]  Loss: 3.381 (3.55)  Time: 0.746s, 1372.89/s  (0.713s, 1436.17/s)  LR: 2.311e-04  Data: 0.009 (0.028)
Train: 412 [ 150/1251 ( 12%)]  Loss: 3.855 (3.63)  Time: 0.678s, 1510.99/s  (0.707s, 1447.76/s)  LR: 2.311e-04  Data: 0.015 (0.022)
Train: 412 [ 200/1251 ( 16%)]  Loss: 3.634 (3.63)  Time: 0.670s, 1528.21/s  (0.705s, 1452.55/s)  LR: 2.311e-04  Data: 0.010 (0.019)
Train: 412 [ 250/1251 ( 20%)]  Loss: 3.385 (3.59)  Time: 0.674s, 1520.25/s  (0.703s, 1456.76/s)  LR: 2.311e-04  Data: 0.010 (0.018)
Train: 412 [ 300/1251 ( 24%)]  Loss: 3.394 (3.56)  Time: 0.671s, 1525.34/s  (0.700s, 1462.11/s)  LR: 2.311e-04  Data: 0.010 (0.016)
Train: 412 [ 350/1251 ( 28%)]  Loss: 3.693 (3.58)  Time: 0.675s, 1517.71/s  (0.699s, 1464.68/s)  LR: 2.311e-04  Data: 0.015 (0.016)
Train: 412 [ 400/1251 ( 32%)]  Loss: 3.203 (3.53)  Time: 0.665s, 1539.69/s  (0.698s, 1467.32/s)  LR: 2.311e-04  Data: 0.009 (0.015)
Train: 412 [ 450/1251 ( 36%)]  Loss: 3.248 (3.51)  Time: 0.696s, 1471.99/s  (0.698s, 1467.25/s)  LR: 2.311e-04  Data: 0.009 (0.015)
Train: 412 [ 500/1251 ( 40%)]  Loss: 3.292 (3.49)  Time: 0.671s, 1526.31/s  (0.697s, 1468.88/s)  LR: 2.311e-04  Data: 0.010 (0.014)
Train: 412 [ 550/1251 ( 44%)]  Loss: 3.630 (3.50)  Time: 0.675s, 1516.08/s  (0.697s, 1469.68/s)  LR: 2.311e-04  Data: 0.010 (0.014)
Train: 412 [ 600/1251 ( 48%)]  Loss: 3.551 (3.50)  Time: 0.709s, 1444.59/s  (0.697s, 1469.46/s)  LR: 2.311e-04  Data: 0.009 (0.014)
Train: 412 [ 650/1251 ( 52%)]  Loss: 3.567 (3.51)  Time: 0.672s, 1522.72/s  (0.696s, 1470.85/s)  LR: 2.311e-04  Data: 0.012 (0.013)
Train: 412 [ 700/1251 ( 56%)]  Loss: 3.307 (3.49)  Time: 0.694s, 1474.63/s  (0.696s, 1471.19/s)  LR: 2.311e-04  Data: 0.015 (0.013)
Train: 412 [ 750/1251 ( 60%)]  Loss: 3.610 (3.50)  Time: 0.720s, 1421.52/s  (0.696s, 1472.23/s)  LR: 2.311e-04  Data: 0.011 (0.013)
Train: 412 [ 800/1251 ( 64%)]  Loss: 3.842 (3.52)  Time: 0.679s, 1509.03/s  (0.696s, 1472.15/s)  LR: 2.311e-04  Data: 0.009 (0.013)
Train: 412 [ 850/1251 ( 68%)]  Loss: 3.439 (3.52)  Time: 0.668s, 1533.03/s  (0.695s, 1472.42/s)  LR: 2.311e-04  Data: 0.011 (0.013)
Train: 412 [ 900/1251 ( 72%)]  Loss: 3.486 (3.52)  Time: 0.673s, 1522.29/s  (0.695s, 1472.64/s)  LR: 2.311e-04  Data: 0.010 (0.012)
Train: 412 [ 950/1251 ( 76%)]  Loss: 3.676 (3.52)  Time: 0.703s, 1457.10/s  (0.695s, 1472.47/s)  LR: 2.311e-04  Data: 0.010 (0.012)
Train: 412 [1000/1251 ( 80%)]  Loss: 3.882 (3.54)  Time: 0.698s, 1467.67/s  (0.695s, 1473.14/s)  LR: 2.311e-04  Data: 0.012 (0.012)
Train: 412 [1050/1251 ( 84%)]  Loss: 3.267 (3.53)  Time: 0.673s, 1521.12/s  (0.695s, 1473.27/s)  LR: 2.311e-04  Data: 0.010 (0.012)
Train: 412 [1100/1251 ( 88%)]  Loss: 3.735 (3.54)  Time: 0.672s, 1523.20/s  (0.695s, 1474.33/s)  LR: 2.311e-04  Data: 0.010 (0.012)
Train: 412 [1150/1251 ( 92%)]  Loss: 3.469 (3.53)  Time: 0.710s, 1442.08/s  (0.694s, 1474.61/s)  LR: 2.311e-04  Data: 0.013 (0.012)
Train: 412 [1200/1251 ( 96%)]  Loss: 3.698 (3.54)  Time: 0.680s, 1506.40/s  (0.695s, 1474.38/s)  LR: 2.311e-04  Data: 0.010 (0.012)
Train: 412 [1250/1251 (100%)]  Loss: 3.138 (3.53)  Time: 0.659s, 1553.55/s  (0.694s, 1474.67/s)  LR: 2.311e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.453 (1.453)  Loss:  0.7227 (0.7227)  Acc@1: 90.6250 (90.6250)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.136 (0.579)  Loss:  0.9404 (1.3164)  Acc@1: 84.1981 (75.8500)  Acc@5: 96.3443 (92.8180)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-407.pth.tar', 76.2320000048828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-408.pth.tar', 76.14400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-402.pth.tar', 76.02800011474609)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-400.pth.tar', 75.96399995605469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-411.pth.tar', 75.95400008789062)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-401.pth.tar', 75.90400008544921)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-412.pth.tar', 75.84999998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-410.pth.tar', 75.82400002929687)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-405.pth.tar', 75.81000008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-409.pth.tar', 75.78000016845704)

Train: 413 [   0/1251 (  0%)]  Loss: 3.213 (3.21)  Time: 2.103s,  486.89/s  (2.103s,  486.89/s)  LR: 2.289e-04  Data: 1.488 (1.488)
Train: 413 [  50/1251 (  4%)]  Loss: 3.174 (3.19)  Time: 0.704s, 1454.70/s  (0.728s, 1405.79/s)  LR: 2.289e-04  Data: 0.009 (0.048)
Train: 413 [ 100/1251 (  8%)]  Loss: 3.736 (3.37)  Time: 0.706s, 1451.37/s  (0.707s, 1447.60/s)  LR: 2.289e-04  Data: 0.011 (0.029)
Train: 413 [ 150/1251 ( 12%)]  Loss: 3.747 (3.47)  Time: 0.681s, 1503.51/s  (0.705s, 1453.26/s)  LR: 2.289e-04  Data: 0.009 (0.023)
Train: 413 [ 200/1251 ( 16%)]  Loss: 3.413 (3.46)  Time: 0.677s, 1513.22/s  (0.701s, 1460.20/s)  LR: 2.289e-04  Data: 0.010 (0.020)
Train: 413 [ 250/1251 ( 20%)]  Loss: 3.692 (3.50)  Time: 0.673s, 1521.43/s  (0.700s, 1462.77/s)  LR: 2.289e-04  Data: 0.011 (0.018)
Train: 413 [ 300/1251 ( 24%)]  Loss: 3.187 (3.45)  Time: 0.713s, 1435.37/s  (0.698s, 1466.11/s)  LR: 2.289e-04  Data: 0.009 (0.017)
Train: 413 [ 350/1251 ( 28%)]  Loss: 3.309 (3.43)  Time: 0.713s, 1437.11/s  (0.698s, 1467.33/s)  LR: 2.289e-04  Data: 0.010 (0.016)
Train: 413 [ 400/1251 ( 32%)]  Loss: 3.441 (3.43)  Time: 0.671s, 1525.55/s  (0.697s, 1469.30/s)  LR: 2.289e-04  Data: 0.009 (0.015)
Train: 413 [ 450/1251 ( 36%)]  Loss: 3.397 (3.43)  Time: 0.671s, 1526.17/s  (0.696s, 1471.30/s)  LR: 2.289e-04  Data: 0.010 (0.014)
Train: 413 [ 500/1251 ( 40%)]  Loss: 3.399 (3.43)  Time: 0.699s, 1464.73/s  (0.696s, 1471.87/s)  LR: 2.289e-04  Data: 0.009 (0.014)
Train: 413 [ 550/1251 ( 44%)]  Loss: 3.524 (3.44)  Time: 0.720s, 1421.74/s  (0.696s, 1472.28/s)  LR: 2.289e-04  Data: 0.009 (0.014)
Train: 413 [ 600/1251 ( 48%)]  Loss: 3.369 (3.43)  Time: 0.760s, 1347.78/s  (0.695s, 1472.77/s)  LR: 2.289e-04  Data: 0.010 (0.014)
Train: 413 [ 650/1251 ( 52%)]  Loss: 3.358 (3.43)  Time: 0.671s, 1526.91/s  (0.695s, 1473.77/s)  LR: 2.289e-04  Data: 0.009 (0.013)
Train: 413 [ 700/1251 ( 56%)]  Loss: 3.504 (3.43)  Time: 0.701s, 1460.98/s  (0.695s, 1473.74/s)  LR: 2.289e-04  Data: 0.011 (0.013)
Train: 413 [ 750/1251 ( 60%)]  Loss: 3.656 (3.45)  Time: 0.704s, 1454.18/s  (0.694s, 1475.29/s)  LR: 2.289e-04  Data: 0.010 (0.013)
Train: 413 [ 800/1251 ( 64%)]  Loss: 3.200 (3.43)  Time: 0.679s, 1508.96/s  (0.695s, 1474.42/s)  LR: 2.289e-04  Data: 0.010 (0.013)
Train: 413 [ 850/1251 ( 68%)]  Loss: 3.479 (3.43)  Time: 0.706s, 1449.53/s  (0.695s, 1474.01/s)  LR: 2.289e-04  Data: 0.011 (0.013)
Train: 413 [ 900/1251 ( 72%)]  Loss: 3.181 (3.42)  Time: 0.671s, 1525.12/s  (0.695s, 1473.94/s)  LR: 2.289e-04  Data: 0.010 (0.013)
Train: 413 [ 950/1251 ( 76%)]  Loss: 3.617 (3.43)  Time: 0.671s, 1525.63/s  (0.695s, 1473.54/s)  LR: 2.289e-04  Data: 0.010 (0.012)
Train: 413 [1000/1251 ( 80%)]  Loss: 3.392 (3.43)  Time: 0.670s, 1528.21/s  (0.695s, 1472.94/s)  LR: 2.289e-04  Data: 0.010 (0.012)
Train: 413 [1050/1251 ( 84%)]  Loss: 3.505 (3.43)  Time: 0.670s, 1527.70/s  (0.695s, 1474.16/s)  LR: 2.289e-04  Data: 0.010 (0.012)
Train: 413 [1100/1251 ( 88%)]  Loss: 3.158 (3.42)  Time: 0.725s, 1411.68/s  (0.694s, 1474.50/s)  LR: 2.289e-04  Data: 0.010 (0.012)
Train: 413 [1150/1251 ( 92%)]  Loss: 3.357 (3.42)  Time: 0.699s, 1464.59/s  (0.695s, 1473.82/s)  LR: 2.289e-04  Data: 0.009 (0.012)
Train: 413 [1200/1251 ( 96%)]  Loss: 3.348 (3.41)  Time: 0.674s, 1519.83/s  (0.694s, 1474.62/s)  LR: 2.289e-04  Data: 0.010 (0.012)
Train: 413 [1250/1251 (100%)]  Loss: 3.648 (3.42)  Time: 0.655s, 1562.30/s  (0.694s, 1475.29/s)  LR: 2.289e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.574 (1.574)  Loss:  0.7021 (0.7021)  Acc@1: 90.7227 (90.7227)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.136 (0.582)  Loss:  0.8560 (1.2802)  Acc@1: 86.3207 (75.9020)  Acc@5: 97.0519 (93.0120)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-407.pth.tar', 76.2320000048828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-408.pth.tar', 76.14400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-402.pth.tar', 76.02800011474609)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-400.pth.tar', 75.96399995605469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-411.pth.tar', 75.95400008789062)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-401.pth.tar', 75.90400008544921)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-413.pth.tar', 75.90199987548829)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-412.pth.tar', 75.84999998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-410.pth.tar', 75.82400002929687)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-405.pth.tar', 75.81000008544922)

Train: 414 [   0/1251 (  0%)]  Loss: 3.498 (3.50)  Time: 2.113s,  484.63/s  (2.113s,  484.63/s)  LR: 2.268e-04  Data: 1.498 (1.498)
Train: 414 [  50/1251 (  4%)]  Loss: 3.565 (3.53)  Time: 0.737s, 1388.53/s  (0.735s, 1392.70/s)  LR: 2.268e-04  Data: 0.011 (0.052)
Train: 414 [ 100/1251 (  8%)]  Loss: 3.179 (3.41)  Time: 0.705s, 1451.84/s  (0.714s, 1434.90/s)  LR: 2.268e-04  Data: 0.010 (0.032)
Train: 414 [ 150/1251 ( 12%)]  Loss: 3.387 (3.41)  Time: 0.674s, 1518.32/s  (0.705s, 1452.79/s)  LR: 2.268e-04  Data: 0.010 (0.025)
Train: 414 [ 200/1251 ( 16%)]  Loss: 3.058 (3.34)  Time: 0.671s, 1525.82/s  (0.700s, 1462.18/s)  LR: 2.268e-04  Data: 0.011 (0.021)
Train: 414 [ 250/1251 ( 20%)]  Loss: 3.111 (3.30)  Time: 0.681s, 1503.59/s  (0.699s, 1464.81/s)  LR: 2.268e-04  Data: 0.009 (0.019)
Train: 414 [ 300/1251 ( 24%)]  Loss: 3.356 (3.31)  Time: 0.678s, 1509.50/s  (0.699s, 1465.61/s)  LR: 2.268e-04  Data: 0.013 (0.017)
Train: 414 [ 350/1251 ( 28%)]  Loss: 3.876 (3.38)  Time: 0.666s, 1537.77/s  (0.697s, 1468.81/s)  LR: 2.268e-04  Data: 0.010 (0.016)
Train: 414 [ 400/1251 ( 32%)]  Loss: 3.318 (3.37)  Time: 0.673s, 1520.45/s  (0.696s, 1470.80/s)  LR: 2.268e-04  Data: 0.011 (0.016)
Train: 414 [ 450/1251 ( 36%)]  Loss: 3.283 (3.36)  Time: 0.675s, 1517.96/s  (0.696s, 1470.98/s)  LR: 2.268e-04  Data: 0.010 (0.015)
Train: 414 [ 500/1251 ( 40%)]  Loss: 3.268 (3.35)  Time: 0.682s, 1502.56/s  (0.697s, 1469.89/s)  LR: 2.268e-04  Data: 0.010 (0.015)
Train: 414 [ 550/1251 ( 44%)]  Loss: 3.346 (3.35)  Time: 0.675s, 1517.51/s  (0.696s, 1472.18/s)  LR: 2.268e-04  Data: 0.010 (0.014)
Train: 414 [ 600/1251 ( 48%)]  Loss: 3.388 (3.36)  Time: 0.705s, 1452.75/s  (0.695s, 1472.97/s)  LR: 2.268e-04  Data: 0.011 (0.014)
Train: 414 [ 650/1251 ( 52%)]  Loss: 3.481 (3.37)  Time: 0.702s, 1459.18/s  (0.695s, 1472.82/s)  LR: 2.268e-04  Data: 0.009 (0.014)
Train: 414 [ 700/1251 ( 56%)]  Loss: 3.508 (3.37)  Time: 0.672s, 1523.36/s  (0.695s, 1472.42/s)  LR: 2.268e-04  Data: 0.011 (0.013)
Train: 414 [ 750/1251 ( 60%)]  Loss: 3.397 (3.38)  Time: 0.673s, 1520.53/s  (0.695s, 1472.65/s)  LR: 2.268e-04  Data: 0.011 (0.013)
Train: 414 [ 800/1251 ( 64%)]  Loss: 3.221 (3.37)  Time: 0.670s, 1527.63/s  (0.695s, 1472.57/s)  LR: 2.268e-04  Data: 0.009 (0.013)
Train: 414 [ 850/1251 ( 68%)]  Loss: 3.149 (3.36)  Time: 0.683s, 1499.21/s  (0.695s, 1472.89/s)  LR: 2.268e-04  Data: 0.009 (0.013)
Train: 414 [ 900/1251 ( 72%)]  Loss: 3.315 (3.35)  Time: 0.677s, 1512.83/s  (0.695s, 1473.75/s)  LR: 2.268e-04  Data: 0.011 (0.013)
Train: 414 [ 950/1251 ( 76%)]  Loss: 3.654 (3.37)  Time: 0.703s, 1455.79/s  (0.695s, 1473.75/s)  LR: 2.268e-04  Data: 0.010 (0.013)
Train: 414 [1000/1251 ( 80%)]  Loss: 3.547 (3.38)  Time: 0.672s, 1524.10/s  (0.694s, 1474.79/s)  LR: 2.268e-04  Data: 0.010 (0.013)
Train: 414 [1050/1251 ( 84%)]  Loss: 3.318 (3.37)  Time: 0.703s, 1455.92/s  (0.694s, 1475.05/s)  LR: 2.268e-04  Data: 0.010 (0.012)
Train: 414 [1100/1251 ( 88%)]  Loss: 3.601 (3.38)  Time: 0.702s, 1458.55/s  (0.694s, 1474.83/s)  LR: 2.268e-04  Data: 0.010 (0.012)
Train: 414 [1150/1251 ( 92%)]  Loss: 3.380 (3.38)  Time: 0.673s, 1521.36/s  (0.694s, 1474.94/s)  LR: 2.268e-04  Data: 0.011 (0.012)
Train: 414 [1200/1251 ( 96%)]  Loss: 3.413 (3.38)  Time: 0.672s, 1523.54/s  (0.694s, 1474.69/s)  LR: 2.268e-04  Data: 0.011 (0.012)
Train: 414 [1250/1251 (100%)]  Loss: 3.573 (3.39)  Time: 0.688s, 1489.14/s  (0.694s, 1474.67/s)  LR: 2.268e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.563 (1.563)  Loss:  0.7646 (0.7646)  Acc@1: 91.0156 (91.0156)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.136 (0.572)  Loss:  0.8730 (1.3293)  Acc@1: 85.2594 (75.9500)  Acc@5: 96.6981 (92.9800)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-407.pth.tar', 76.2320000048828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-408.pth.tar', 76.14400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-402.pth.tar', 76.02800011474609)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-400.pth.tar', 75.96399995605469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-411.pth.tar', 75.95400008789062)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-414.pth.tar', 75.94999993164062)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-401.pth.tar', 75.90400008544921)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-413.pth.tar', 75.90199987548829)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-412.pth.tar', 75.84999998779297)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-410.pth.tar', 75.82400002929687)

Train: 415 [   0/1251 (  0%)]  Loss: 3.778 (3.78)  Time: 2.218s,  461.62/s  (2.218s,  461.62/s)  LR: 2.246e-04  Data: 1.603 (1.603)
Train: 415 [  50/1251 (  4%)]  Loss: 3.510 (3.64)  Time: 0.715s, 1431.75/s  (0.723s, 1416.08/s)  LR: 2.246e-04  Data: 0.010 (0.047)
Train: 415 [ 100/1251 (  8%)]  Loss: 3.381 (3.56)  Time: 0.672s, 1524.68/s  (0.709s, 1443.75/s)  LR: 2.246e-04  Data: 0.010 (0.029)
Train: 415 [ 150/1251 ( 12%)]  Loss: 3.312 (3.50)  Time: 0.672s, 1524.70/s  (0.702s, 1457.88/s)  LR: 2.246e-04  Data: 0.012 (0.023)
Train: 415 [ 200/1251 ( 16%)]  Loss: 3.650 (3.53)  Time: 0.751s, 1363.10/s  (0.701s, 1461.55/s)  LR: 2.246e-04  Data: 0.010 (0.020)
Train: 415 [ 250/1251 ( 20%)]  Loss: 3.566 (3.53)  Time: 0.686s, 1493.07/s  (0.698s, 1466.49/s)  LR: 2.246e-04  Data: 0.011 (0.018)
Train: 415 [ 300/1251 ( 24%)]  Loss: 3.239 (3.49)  Time: 0.709s, 1443.39/s  (0.698s, 1467.31/s)  LR: 2.246e-04  Data: 0.010 (0.017)
Train: 415 [ 350/1251 ( 28%)]  Loss: 3.347 (3.47)  Time: 0.674s, 1519.28/s  (0.697s, 1469.07/s)  LR: 2.246e-04  Data: 0.010 (0.016)
Train: 415 [ 400/1251 ( 32%)]  Loss: 3.670 (3.49)  Time: 0.670s, 1527.45/s  (0.696s, 1471.55/s)  LR: 2.246e-04  Data: 0.010 (0.015)
Train: 415 [ 450/1251 ( 36%)]  Loss: 3.518 (3.50)  Time: 0.714s, 1433.18/s  (0.696s, 1471.72/s)  LR: 2.246e-04  Data: 0.010 (0.015)
Train: 415 [ 500/1251 ( 40%)]  Loss: 3.544 (3.50)  Time: 0.667s, 1535.54/s  (0.696s, 1472.10/s)  LR: 2.246e-04  Data: 0.009 (0.014)
Train: 415 [ 550/1251 ( 44%)]  Loss: 3.046 (3.46)  Time: 0.698s, 1466.89/s  (0.695s, 1473.19/s)  LR: 2.246e-04  Data: 0.015 (0.014)
Train: 415 [ 600/1251 ( 48%)]  Loss: 3.259 (3.45)  Time: 0.714s, 1435.05/s  (0.695s, 1473.39/s)  LR: 2.246e-04  Data: 0.010 (0.014)
Train: 415 [ 650/1251 ( 52%)]  Loss: 3.612 (3.46)  Time: 0.699s, 1465.98/s  (0.695s, 1474.01/s)  LR: 2.246e-04  Data: 0.009 (0.013)
Train: 415 [ 700/1251 ( 56%)]  Loss: 3.117 (3.44)  Time: 0.703s, 1456.30/s  (0.695s, 1474.28/s)  LR: 2.246e-04  Data: 0.009 (0.013)
Train: 415 [ 750/1251 ( 60%)]  Loss: 3.614 (3.45)  Time: 0.675s, 1517.73/s  (0.694s, 1474.81/s)  LR: 2.246e-04  Data: 0.009 (0.013)
Train: 415 [ 800/1251 ( 64%)]  Loss: 3.524 (3.45)  Time: 0.691s, 1482.09/s  (0.695s, 1474.33/s)  LR: 2.246e-04  Data: 0.010 (0.013)
Train: 415 [ 850/1251 ( 68%)]  Loss: 3.334 (3.45)  Time: 0.721s, 1420.27/s  (0.694s, 1475.30/s)  LR: 2.246e-04  Data: 0.009 (0.013)
Train: 415 [ 900/1251 ( 72%)]  Loss: 3.440 (3.45)  Time: 0.701s, 1460.50/s  (0.694s, 1476.39/s)  LR: 2.246e-04  Data: 0.009 (0.013)
Train: 415 [ 950/1251 ( 76%)]  Loss: 3.782 (3.46)  Time: 0.673s, 1521.11/s  (0.693s, 1476.62/s)  LR: 2.246e-04  Data: 0.009 (0.012)
Train: 415 [1000/1251 ( 80%)]  Loss: 3.679 (3.47)  Time: 0.699s, 1463.96/s  (0.694s, 1476.54/s)  LR: 2.246e-04  Data: 0.009 (0.012)
Train: 415 [1050/1251 ( 84%)]  Loss: 3.281 (3.46)  Time: 0.672s, 1523.81/s  (0.693s, 1477.35/s)  LR: 2.246e-04  Data: 0.011 (0.012)
Train: 415 [1100/1251 ( 88%)]  Loss: 3.527 (3.47)  Time: 0.700s, 1463.37/s  (0.693s, 1477.59/s)  LR: 2.246e-04  Data: 0.009 (0.012)
Train: 415 [1150/1251 ( 92%)]  Loss: 3.502 (3.47)  Time: 0.675s, 1517.85/s  (0.693s, 1478.07/s)  LR: 2.246e-04  Data: 0.011 (0.012)
Train: 415 [1200/1251 ( 96%)]  Loss: 3.492 (3.47)  Time: 0.706s, 1450.17/s  (0.693s, 1478.48/s)  LR: 2.246e-04  Data: 0.009 (0.012)
Train: 415 [1250/1251 (100%)]  Loss: 3.457 (3.47)  Time: 0.659s, 1553.48/s  (0.692s, 1478.90/s)  LR: 2.246e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.497 (1.497)  Loss:  0.7773 (0.7773)  Acc@1: 91.0156 (91.0156)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.136 (0.580)  Loss:  0.9263 (1.4098)  Acc@1: 86.2028 (76.0040)  Acc@5: 97.0519 (92.9600)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-407.pth.tar', 76.2320000048828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-408.pth.tar', 76.14400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-402.pth.tar', 76.02800011474609)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-415.pth.tar', 76.00399995361329)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-400.pth.tar', 75.96399995605469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-411.pth.tar', 75.95400008789062)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-414.pth.tar', 75.94999993164062)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-401.pth.tar', 75.90400008544921)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-413.pth.tar', 75.90199987548829)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-412.pth.tar', 75.84999998779297)

Train: 416 [   0/1251 (  0%)]  Loss: 3.480 (3.48)  Time: 2.209s,  463.49/s  (2.209s,  463.49/s)  LR: 2.225e-04  Data: 1.595 (1.595)
Train: 416 [  50/1251 (  4%)]  Loss: 3.459 (3.47)  Time: 0.725s, 1412.47/s  (0.723s, 1416.10/s)  LR: 2.225e-04  Data: 0.009 (0.046)
Train: 416 [ 100/1251 (  8%)]  Loss: 3.624 (3.52)  Time: 0.668s, 1531.98/s  (0.707s, 1447.78/s)  LR: 2.225e-04  Data: 0.009 (0.028)
Train: 416 [ 150/1251 ( 12%)]  Loss: 3.749 (3.58)  Time: 0.704s, 1455.33/s  (0.702s, 1458.58/s)  LR: 2.225e-04  Data: 0.010 (0.022)
Train: 416 [ 200/1251 ( 16%)]  Loss: 3.471 (3.56)  Time: 0.711s, 1440.33/s  (0.699s, 1464.46/s)  LR: 2.225e-04  Data: 0.010 (0.020)
Train: 416 [ 250/1251 ( 20%)]  Loss: 3.506 (3.55)  Time: 0.716s, 1429.99/s  (0.698s, 1468.02/s)  LR: 2.225e-04  Data: 0.013 (0.018)
Train: 416 [ 300/1251 ( 24%)]  Loss: 3.473 (3.54)  Time: 0.686s, 1492.84/s  (0.696s, 1470.57/s)  LR: 2.225e-04  Data: 0.010 (0.017)
Train: 416 [ 350/1251 ( 28%)]  Loss: 3.736 (3.56)  Time: 0.668s, 1532.10/s  (0.696s, 1470.95/s)  LR: 2.225e-04  Data: 0.010 (0.016)
Train: 416 [ 400/1251 ( 32%)]  Loss: 3.587 (3.57)  Time: 0.672s, 1523.52/s  (0.695s, 1473.21/s)  LR: 2.225e-04  Data: 0.010 (0.015)
Train: 416 [ 450/1251 ( 36%)]  Loss: 3.446 (3.55)  Time: 0.758s, 1351.72/s  (0.695s, 1473.42/s)  LR: 2.225e-04  Data: 0.010 (0.014)
Train: 416 [ 500/1251 ( 40%)]  Loss: 3.457 (3.54)  Time: 0.672s, 1524.14/s  (0.695s, 1474.18/s)  LR: 2.225e-04  Data: 0.010 (0.014)
Train: 416 [ 550/1251 ( 44%)]  Loss: 3.489 (3.54)  Time: 0.682s, 1502.32/s  (0.694s, 1475.29/s)  LR: 2.225e-04  Data: 0.009 (0.014)
Train: 416 [ 600/1251 ( 48%)]  Loss: 3.120 (3.51)  Time: 0.682s, 1501.23/s  (0.694s, 1475.30/s)  LR: 2.225e-04  Data: 0.011 (0.013)
Train: 416 [ 650/1251 ( 52%)]  Loss: 3.629 (3.52)  Time: 0.702s, 1459.09/s  (0.694s, 1476.34/s)  LR: 2.225e-04  Data: 0.010 (0.013)
Train: 416 [ 700/1251 ( 56%)]  Loss: 3.051 (3.49)  Time: 0.671s, 1526.24/s  (0.693s, 1476.80/s)  LR: 2.225e-04  Data: 0.011 (0.013)
Train: 416 [ 750/1251 ( 60%)]  Loss: 3.267 (3.47)  Time: 0.721s, 1420.34/s  (0.693s, 1477.73/s)  LR: 2.225e-04  Data: 0.011 (0.013)
Train: 416 [ 800/1251 ( 64%)]  Loss: 3.497 (3.47)  Time: 0.695s, 1474.13/s  (0.693s, 1477.53/s)  LR: 2.225e-04  Data: 0.011 (0.013)
Train: 416 [ 850/1251 ( 68%)]  Loss: 3.422 (3.47)  Time: 0.701s, 1461.81/s  (0.693s, 1478.19/s)  LR: 2.225e-04  Data: 0.011 (0.013)
Train: 416 [ 900/1251 ( 72%)]  Loss: 3.484 (3.47)  Time: 0.672s, 1524.07/s  (0.693s, 1478.47/s)  LR: 2.225e-04  Data: 0.010 (0.012)
Train: 416 [ 950/1251 ( 76%)]  Loss: 3.463 (3.47)  Time: 0.672s, 1523.27/s  (0.692s, 1479.20/s)  LR: 2.225e-04  Data: 0.011 (0.012)
Train: 416 [1000/1251 ( 80%)]  Loss: 3.943 (3.49)  Time: 0.672s, 1524.83/s  (0.692s, 1478.85/s)  LR: 2.225e-04  Data: 0.011 (0.012)
Train: 416 [1050/1251 ( 84%)]  Loss: 3.239 (3.48)  Time: 0.672s, 1523.19/s  (0.692s, 1479.25/s)  LR: 2.225e-04  Data: 0.009 (0.012)
Train: 416 [1100/1251 ( 88%)]  Loss: 3.289 (3.47)  Time: 0.672s, 1524.10/s  (0.692s, 1479.63/s)  LR: 2.225e-04  Data: 0.010 (0.012)
Train: 416 [1150/1251 ( 92%)]  Loss: 3.536 (3.48)  Time: 0.674s, 1520.33/s  (0.692s, 1479.59/s)  LR: 2.225e-04  Data: 0.011 (0.012)
Train: 416 [1200/1251 ( 96%)]  Loss: 3.404 (3.47)  Time: 0.736s, 1391.15/s  (0.692s, 1478.95/s)  LR: 2.225e-04  Data: 0.011 (0.012)
Train: 416 [1250/1251 (100%)]  Loss: 3.490 (3.47)  Time: 0.682s, 1501.21/s  (0.692s, 1478.84/s)  LR: 2.225e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.600 (1.600)  Loss:  0.7471 (0.7471)  Acc@1: 90.5273 (90.5273)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.136 (0.575)  Loss:  0.8652 (1.3181)  Acc@1: 84.9057 (76.0560)  Acc@5: 96.4623 (93.0540)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-407.pth.tar', 76.2320000048828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-408.pth.tar', 76.14400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-416.pth.tar', 76.05599990722656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-402.pth.tar', 76.02800011474609)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-415.pth.tar', 76.00399995361329)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-400.pth.tar', 75.96399995605469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-411.pth.tar', 75.95400008789062)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-414.pth.tar', 75.94999993164062)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-401.pth.tar', 75.90400008544921)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-413.pth.tar', 75.90199987548829)

Train: 417 [   0/1251 (  0%)]  Loss: 3.704 (3.70)  Time: 2.079s,  492.62/s  (2.079s,  492.62/s)  LR: 2.204e-04  Data: 1.462 (1.462)
Train: 417 [  50/1251 (  4%)]  Loss: 3.327 (3.52)  Time: 0.666s, 1536.63/s  (0.727s, 1408.61/s)  LR: 2.204e-04  Data: 0.010 (0.050)
Train: 417 [ 100/1251 (  8%)]  Loss: 3.373 (3.47)  Time: 0.740s, 1383.36/s  (0.719s, 1425.06/s)  LR: 2.204e-04  Data: 0.013 (0.031)
Train: 417 [ 150/1251 ( 12%)]  Loss: 3.063 (3.37)  Time: 0.713s, 1435.62/s  (0.709s, 1444.77/s)  LR: 2.204e-04  Data: 0.011 (0.024)
Train: 417 [ 200/1251 ( 16%)]  Loss: 3.359 (3.37)  Time: 0.706s, 1450.95/s  (0.704s, 1455.30/s)  LR: 2.204e-04  Data: 0.010 (0.021)
Train: 417 [ 250/1251 ( 20%)]  Loss: 3.141 (3.33)  Time: 0.670s, 1527.31/s  (0.700s, 1463.24/s)  LR: 2.204e-04  Data: 0.009 (0.019)
Train: 417 [ 300/1251 ( 24%)]  Loss: 3.515 (3.35)  Time: 0.703s, 1455.80/s  (0.699s, 1464.61/s)  LR: 2.204e-04  Data: 0.009 (0.017)
Train: 417 [ 350/1251 ( 28%)]  Loss: 3.629 (3.39)  Time: 0.709s, 1443.77/s  (0.698s, 1467.22/s)  LR: 2.204e-04  Data: 0.009 (0.016)
Train: 417 [ 400/1251 ( 32%)]  Loss: 3.093 (3.36)  Time: 0.670s, 1528.80/s  (0.697s, 1468.71/s)  LR: 2.204e-04  Data: 0.009 (0.016)
Train: 417 [ 450/1251 ( 36%)]  Loss: 3.523 (3.37)  Time: 0.739s, 1384.83/s  (0.697s, 1468.23/s)  LR: 2.204e-04  Data: 0.011 (0.015)
Train: 417 [ 500/1251 ( 40%)]  Loss: 3.537 (3.39)  Time: 0.717s, 1427.45/s  (0.696s, 1470.54/s)  LR: 2.204e-04  Data: 0.010 (0.015)
Train: 417 [ 550/1251 ( 44%)]  Loss: 3.470 (3.39)  Time: 0.680s, 1506.75/s  (0.696s, 1472.24/s)  LR: 2.204e-04  Data: 0.010 (0.014)
Train: 417 [ 600/1251 ( 48%)]  Loss: 3.161 (3.38)  Time: 0.769s, 1332.46/s  (0.695s, 1473.52/s)  LR: 2.204e-04  Data: 0.008 (0.014)
Train: 417 [ 650/1251 ( 52%)]  Loss: 3.584 (3.39)  Time: 0.708s, 1446.78/s  (0.695s, 1472.82/s)  LR: 2.204e-04  Data: 0.009 (0.014)
Train: 417 [ 700/1251 ( 56%)]  Loss: 3.718 (3.41)  Time: 0.685s, 1495.68/s  (0.695s, 1473.34/s)  LR: 2.204e-04  Data: 0.012 (0.013)
Train: 417 [ 750/1251 ( 60%)]  Loss: 3.287 (3.41)  Time: 0.683s, 1498.30/s  (0.695s, 1473.56/s)  LR: 2.204e-04  Data: 0.009 (0.013)
Train: 417 [ 800/1251 ( 64%)]  Loss: 3.244 (3.40)  Time: 0.713s, 1436.44/s  (0.695s, 1473.36/s)  LR: 2.204e-04  Data: 0.010 (0.013)
Train: 417 [ 850/1251 ( 68%)]  Loss: 3.484 (3.40)  Time: 0.678s, 1509.79/s  (0.695s, 1473.09/s)  LR: 2.204e-04  Data: 0.010 (0.013)
Train: 417 [ 900/1251 ( 72%)]  Loss: 3.618 (3.41)  Time: 0.673s, 1521.43/s  (0.695s, 1473.52/s)  LR: 2.204e-04  Data: 0.011 (0.013)
Train: 417 [ 950/1251 ( 76%)]  Loss: 3.538 (3.42)  Time: 0.672s, 1524.63/s  (0.695s, 1473.60/s)  LR: 2.204e-04  Data: 0.010 (0.013)
Train: 417 [1000/1251 ( 80%)]  Loss: 3.159 (3.41)  Time: 0.710s, 1442.99/s  (0.695s, 1473.56/s)  LR: 2.204e-04  Data: 0.010 (0.013)
Train: 417 [1050/1251 ( 84%)]  Loss: 3.269 (3.40)  Time: 0.671s, 1525.05/s  (0.694s, 1474.50/s)  LR: 2.204e-04  Data: 0.012 (0.012)
Train: 417 [1100/1251 ( 88%)]  Loss: 3.477 (3.40)  Time: 0.699s, 1464.06/s  (0.694s, 1475.35/s)  LR: 2.204e-04  Data: 0.015 (0.012)
Train: 417 [1150/1251 ( 92%)]  Loss: 3.682 (3.41)  Time: 0.712s, 1437.66/s  (0.695s, 1473.98/s)  LR: 2.204e-04  Data: 0.013 (0.012)
Train: 417 [1200/1251 ( 96%)]  Loss: 3.424 (3.42)  Time: 0.700s, 1462.13/s  (0.694s, 1474.60/s)  LR: 2.204e-04  Data: 0.009 (0.012)
Train: 417 [1250/1251 (100%)]  Loss: 3.702 (3.43)  Time: 0.654s, 1566.31/s  (0.695s, 1474.17/s)  LR: 2.204e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.538 (1.538)  Loss:  0.7725 (0.7725)  Acc@1: 90.8203 (90.8203)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.136 (0.576)  Loss:  0.8662 (1.3357)  Acc@1: 85.4953 (76.0980)  Acc@5: 96.2264 (93.0920)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-407.pth.tar', 76.2320000048828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-408.pth.tar', 76.14400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-417.pth.tar', 76.09800003417969)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-416.pth.tar', 76.05599990722656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-402.pth.tar', 76.02800011474609)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-415.pth.tar', 76.00399995361329)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-400.pth.tar', 75.96399995605469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-411.pth.tar', 75.95400008789062)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-414.pth.tar', 75.94999993164062)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-401.pth.tar', 75.90400008544921)

Train: 418 [   0/1251 (  0%)]  Loss: 3.317 (3.32)  Time: 2.325s,  440.45/s  (2.325s,  440.45/s)  LR: 2.183e-04  Data: 1.681 (1.681)
Train: 418 [  50/1251 (  4%)]  Loss: 3.528 (3.42)  Time: 0.699s, 1465.52/s  (0.726s, 1410.25/s)  LR: 2.183e-04  Data: 0.009 (0.048)
Train: 418 [ 100/1251 (  8%)]  Loss: 3.431 (3.43)  Time: 0.715s, 1432.39/s  (0.714s, 1434.61/s)  LR: 2.183e-04  Data: 0.009 (0.029)
Train: 418 [ 150/1251 ( 12%)]  Loss: 3.055 (3.33)  Time: 0.668s, 1532.28/s  (0.706s, 1450.73/s)  LR: 2.183e-04  Data: 0.011 (0.023)
Train: 418 [ 200/1251 ( 16%)]  Loss: 3.458 (3.36)  Time: 0.673s, 1520.87/s  (0.701s, 1461.62/s)  LR: 2.183e-04  Data: 0.011 (0.020)
Train: 418 [ 250/1251 ( 20%)]  Loss: 3.115 (3.32)  Time: 0.673s, 1521.96/s  (0.698s, 1467.19/s)  LR: 2.183e-04  Data: 0.010 (0.018)
Train: 418 [ 300/1251 ( 24%)]  Loss: 3.440 (3.33)  Time: 0.673s, 1522.48/s  (0.696s, 1471.46/s)  LR: 2.183e-04  Data: 0.010 (0.017)
Train: 418 [ 350/1251 ( 28%)]  Loss: 3.525 (3.36)  Time: 0.673s, 1521.52/s  (0.695s, 1472.81/s)  LR: 2.183e-04  Data: 0.010 (0.016)
Train: 418 [ 400/1251 ( 32%)]  Loss: 3.644 (3.39)  Time: 0.674s, 1519.57/s  (0.694s, 1474.87/s)  LR: 2.183e-04  Data: 0.011 (0.015)
Train: 418 [ 450/1251 ( 36%)]  Loss: 3.255 (3.38)  Time: 0.708s, 1447.10/s  (0.694s, 1476.22/s)  LR: 2.183e-04  Data: 0.009 (0.015)
Train: 418 [ 500/1251 ( 40%)]  Loss: 3.675 (3.40)  Time: 0.700s, 1462.94/s  (0.693s, 1477.77/s)  LR: 2.183e-04  Data: 0.010 (0.014)
Train: 418 [ 550/1251 ( 44%)]  Loss: 3.337 (3.40)  Time: 0.680s, 1504.82/s  (0.692s, 1479.56/s)  LR: 2.183e-04  Data: 0.009 (0.014)
Train: 418 [ 600/1251 ( 48%)]  Loss: 3.634 (3.42)  Time: 0.677s, 1513.48/s  (0.692s, 1480.58/s)  LR: 2.183e-04  Data: 0.011 (0.013)
Train: 418 [ 650/1251 ( 52%)]  Loss: 3.694 (3.44)  Time: 0.673s, 1521.86/s  (0.692s, 1480.57/s)  LR: 2.183e-04  Data: 0.011 (0.013)
Train: 418 [ 700/1251 ( 56%)]  Loss: 3.507 (3.44)  Time: 0.674s, 1518.85/s  (0.691s, 1481.09/s)  LR: 2.183e-04  Data: 0.012 (0.013)
Train: 418 [ 750/1251 ( 60%)]  Loss: 3.338 (3.43)  Time: 0.604s, 1696.63/s  (0.691s, 1481.64/s)  LR: 2.183e-04  Data: 0.010 (0.013)
Train: 418 [ 800/1251 ( 64%)]  Loss: 3.574 (3.44)  Time: 0.679s, 1508.64/s  (0.691s, 1481.28/s)  LR: 2.183e-04  Data: 0.011 (0.013)
Train: 418 [ 850/1251 ( 68%)]  Loss: 3.602 (3.45)  Time: 0.672s, 1523.36/s  (0.691s, 1481.54/s)  LR: 2.183e-04  Data: 0.012 (0.013)
Train: 418 [ 900/1251 ( 72%)]  Loss: 3.314 (3.44)  Time: 0.724s, 1414.55/s  (0.691s, 1482.20/s)  LR: 2.183e-04  Data: 0.011 (0.012)
Train: 418 [ 950/1251 ( 76%)]  Loss: 3.625 (3.45)  Time: 0.666s, 1538.08/s  (0.691s, 1482.48/s)  LR: 2.183e-04  Data: 0.009 (0.012)
Train: 418 [1000/1251 ( 80%)]  Loss: 3.411 (3.45)  Time: 0.701s, 1461.13/s  (0.691s, 1482.26/s)  LR: 2.183e-04  Data: 0.011 (0.012)
Train: 418 [1050/1251 ( 84%)]  Loss: 3.444 (3.45)  Time: 0.704s, 1454.32/s  (0.691s, 1481.55/s)  LR: 2.183e-04  Data: 0.009 (0.012)
Train: 418 [1100/1251 ( 88%)]  Loss: 3.446 (3.45)  Time: 0.680s, 1506.13/s  (0.692s, 1480.62/s)  LR: 2.183e-04  Data: 0.010 (0.012)
Train: 418 [1150/1251 ( 92%)]  Loss: 3.327 (3.45)  Time: 0.677s, 1513.33/s  (0.692s, 1480.64/s)  LR: 2.183e-04  Data: 0.010 (0.012)
Train: 418 [1200/1251 ( 96%)]  Loss: 3.337 (3.44)  Time: 0.695s, 1473.98/s  (0.691s, 1481.05/s)  LR: 2.183e-04  Data: 0.012 (0.012)
Train: 418 [1250/1251 (100%)]  Loss: 3.398 (3.44)  Time: 0.678s, 1510.24/s  (0.691s, 1480.85/s)  LR: 2.183e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.540 (1.540)  Loss:  0.7422 (0.7422)  Acc@1: 90.4297 (90.4297)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.136 (0.571)  Loss:  0.8613 (1.2600)  Acc@1: 85.1415 (76.3760)  Acc@5: 96.5802 (93.0440)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-418.pth.tar', 76.37600000976562)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-407.pth.tar', 76.2320000048828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-408.pth.tar', 76.14400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-417.pth.tar', 76.09800003417969)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-416.pth.tar', 76.05599990722656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-402.pth.tar', 76.02800011474609)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-415.pth.tar', 76.00399995361329)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-400.pth.tar', 75.96399995605469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-411.pth.tar', 75.95400008789062)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-414.pth.tar', 75.94999993164062)

Train: 419 [   0/1251 (  0%)]  Loss: 3.690 (3.69)  Time: 2.155s,  475.28/s  (2.155s,  475.28/s)  LR: 2.161e-04  Data: 1.541 (1.541)
Train: 419 [  50/1251 (  4%)]  Loss: 3.294 (3.49)  Time: 0.696s, 1470.74/s  (0.728s, 1405.84/s)  LR: 2.161e-04  Data: 0.010 (0.050)
Train: 419 [ 100/1251 (  8%)]  Loss: 3.310 (3.43)  Time: 0.692s, 1479.67/s  (0.712s, 1437.49/s)  LR: 2.161e-04  Data: 0.014 (0.030)
Train: 419 [ 150/1251 ( 12%)]  Loss: 3.265 (3.39)  Time: 0.671s, 1527.10/s  (0.706s, 1451.27/s)  LR: 2.161e-04  Data: 0.009 (0.024)
Train: 419 [ 200/1251 ( 16%)]  Loss: 3.416 (3.39)  Time: 0.706s, 1450.36/s  (0.701s, 1459.82/s)  LR: 2.161e-04  Data: 0.013 (0.020)
Train: 419 [ 250/1251 ( 20%)]  Loss: 3.404 (3.40)  Time: 0.673s, 1521.87/s  (0.699s, 1465.33/s)  LR: 2.161e-04  Data: 0.011 (0.018)
Train: 419 [ 300/1251 ( 24%)]  Loss: 3.539 (3.42)  Time: 0.670s, 1527.80/s  (0.697s, 1469.91/s)  LR: 2.161e-04  Data: 0.011 (0.017)
Train: 419 [ 350/1251 ( 28%)]  Loss: 3.218 (3.39)  Time: 0.694s, 1476.22/s  (0.695s, 1472.48/s)  LR: 2.161e-04  Data: 0.011 (0.016)
Train: 419 [ 400/1251 ( 32%)]  Loss: 3.572 (3.41)  Time: 0.673s, 1521.57/s  (0.694s, 1474.71/s)  LR: 2.161e-04  Data: 0.010 (0.015)
Train: 419 [ 450/1251 ( 36%)]  Loss: 3.653 (3.44)  Time: 0.805s, 1272.31/s  (0.694s, 1475.56/s)  LR: 2.161e-04  Data: 0.010 (0.015)
Train: 419 [ 500/1251 ( 40%)]  Loss: 3.140 (3.41)  Time: 0.726s, 1409.66/s  (0.694s, 1476.32/s)  LR: 2.161e-04  Data: 0.010 (0.014)
Train: 419 [ 550/1251 ( 44%)]  Loss: 3.502 (3.42)  Time: 0.671s, 1526.41/s  (0.693s, 1477.36/s)  LR: 2.161e-04  Data: 0.010 (0.014)
Train: 419 [ 600/1251 ( 48%)]  Loss: 3.484 (3.42)  Time: 0.702s, 1458.02/s  (0.693s, 1476.71/s)  LR: 2.161e-04  Data: 0.009 (0.014)
Train: 419 [ 650/1251 ( 52%)]  Loss: 3.562 (3.43)  Time: 0.740s, 1384.12/s  (0.694s, 1476.15/s)  LR: 2.161e-04  Data: 0.014 (0.014)
Train: 419 [ 700/1251 ( 56%)]  Loss: 3.746 (3.45)  Time: 0.707s, 1448.20/s  (0.694s, 1475.62/s)  LR: 2.161e-04  Data: 0.011 (0.013)
Train: 419 [ 750/1251 ( 60%)]  Loss: 3.456 (3.45)  Time: 0.694s, 1476.19/s  (0.694s, 1476.57/s)  LR: 2.161e-04  Data: 0.011 (0.013)
Train: 419 [ 800/1251 ( 64%)]  Loss: 4.036 (3.49)  Time: 0.681s, 1504.76/s  (0.693s, 1476.83/s)  LR: 2.161e-04  Data: 0.011 (0.013)
Train: 419 [ 850/1251 ( 68%)]  Loss: 3.517 (3.49)  Time: 0.700s, 1463.50/s  (0.693s, 1477.23/s)  LR: 2.161e-04  Data: 0.010 (0.013)
Train: 419 [ 900/1251 ( 72%)]  Loss: 3.296 (3.48)  Time: 0.702s, 1457.86/s  (0.693s, 1478.42/s)  LR: 2.161e-04  Data: 0.010 (0.013)
Train: 419 [ 950/1251 ( 76%)]  Loss: 3.640 (3.49)  Time: 0.698s, 1466.60/s  (0.693s, 1477.31/s)  LR: 2.161e-04  Data: 0.010 (0.013)
Train: 419 [1000/1251 ( 80%)]  Loss: 3.409 (3.48)  Time: 0.691s, 1481.90/s  (0.694s, 1475.72/s)  LR: 2.161e-04  Data: 0.013 (0.013)
Train: 419 [1050/1251 ( 84%)]  Loss: 3.044 (3.46)  Time: 0.721s, 1420.06/s  (0.695s, 1473.63/s)  LR: 2.161e-04  Data: 0.010 (0.013)
Train: 419 [1100/1251 ( 88%)]  Loss: 3.442 (3.46)  Time: 0.677s, 1512.58/s  (0.695s, 1473.52/s)  LR: 2.161e-04  Data: 0.009 (0.012)
Train: 419 [1150/1251 ( 92%)]  Loss: 3.248 (3.45)  Time: 0.672s, 1523.37/s  (0.694s, 1475.00/s)  LR: 2.161e-04  Data: 0.010 (0.012)
Train: 419 [1200/1251 ( 96%)]  Loss: 3.270 (3.45)  Time: 0.671s, 1525.13/s  (0.693s, 1476.73/s)  LR: 2.161e-04  Data: 0.010 (0.012)
Train: 419 [1250/1251 (100%)]  Loss: 3.446 (3.45)  Time: 0.658s, 1556.27/s  (0.693s, 1478.27/s)  LR: 2.161e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.631 (1.631)  Loss:  0.7261 (0.7261)  Acc@1: 91.3086 (91.3086)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.136 (0.580)  Loss:  0.8486 (1.2454)  Acc@1: 85.4953 (76.4080)  Acc@5: 96.1085 (93.2500)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-419.pth.tar', 76.4080000341797)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-418.pth.tar', 76.37600000976562)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-407.pth.tar', 76.2320000048828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-408.pth.tar', 76.14400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-417.pth.tar', 76.09800003417969)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-416.pth.tar', 76.05599990722656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-402.pth.tar', 76.02800011474609)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-415.pth.tar', 76.00399995361329)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-400.pth.tar', 75.96399995605469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-411.pth.tar', 75.95400008789062)

Train: 420 [   0/1251 (  0%)]  Loss: 3.672 (3.67)  Time: 2.234s,  458.43/s  (2.234s,  458.43/s)  LR: 2.140e-04  Data: 1.596 (1.596)
Train: 420 [  50/1251 (  4%)]  Loss: 3.269 (3.47)  Time: 0.674s, 1520.39/s  (0.726s, 1411.18/s)  LR: 2.140e-04  Data: 0.011 (0.047)
Train: 420 [ 100/1251 (  8%)]  Loss: 3.819 (3.59)  Time: 0.673s, 1521.38/s  (0.711s, 1440.85/s)  LR: 2.140e-04  Data: 0.010 (0.029)
Train: 420 [ 150/1251 ( 12%)]  Loss: 3.631 (3.60)  Time: 0.769s, 1331.26/s  (0.704s, 1455.23/s)  LR: 2.140e-04  Data: 0.009 (0.023)
Train: 420 [ 200/1251 ( 16%)]  Loss: 3.638 (3.61)  Time: 0.676s, 1514.37/s  (0.700s, 1463.65/s)  LR: 2.140e-04  Data: 0.011 (0.020)
Train: 420 [ 250/1251 ( 20%)]  Loss: 3.345 (3.56)  Time: 0.668s, 1532.12/s  (0.699s, 1465.63/s)  LR: 2.140e-04  Data: 0.010 (0.018)
Train: 420 [ 300/1251 ( 24%)]  Loss: 3.700 (3.58)  Time: 0.671s, 1525.43/s  (0.698s, 1467.37/s)  LR: 2.140e-04  Data: 0.012 (0.017)
Train: 420 [ 350/1251 ( 28%)]  Loss: 3.237 (3.54)  Time: 0.673s, 1520.84/s  (0.697s, 1468.94/s)  LR: 2.140e-04  Data: 0.010 (0.016)
Train: 420 [ 400/1251 ( 32%)]  Loss: 3.318 (3.51)  Time: 0.719s, 1424.13/s  (0.696s, 1470.70/s)  LR: 2.140e-04  Data: 0.010 (0.015)
Train: 420 [ 450/1251 ( 36%)]  Loss: 3.633 (3.53)  Time: 0.670s, 1527.33/s  (0.696s, 1472.16/s)  LR: 2.140e-04  Data: 0.010 (0.014)
Train: 420 [ 500/1251 ( 40%)]  Loss: 3.468 (3.52)  Time: 0.671s, 1527.07/s  (0.695s, 1473.71/s)  LR: 2.140e-04  Data: 0.011 (0.014)
Train: 420 [ 550/1251 ( 44%)]  Loss: 3.186 (3.49)  Time: 0.671s, 1527.05/s  (0.694s, 1474.77/s)  LR: 2.140e-04  Data: 0.010 (0.014)
Train: 420 [ 600/1251 ( 48%)]  Loss: 3.519 (3.49)  Time: 0.673s, 1522.33/s  (0.694s, 1475.54/s)  LR: 2.140e-04  Data: 0.011 (0.014)
Train: 420 [ 650/1251 ( 52%)]  Loss: 3.459 (3.49)  Time: 0.669s, 1529.81/s  (0.693s, 1476.84/s)  LR: 2.140e-04  Data: 0.011 (0.013)
Train: 420 [ 700/1251 ( 56%)]  Loss: 3.483 (3.49)  Time: 0.673s, 1521.97/s  (0.693s, 1477.06/s)  LR: 2.140e-04  Data: 0.011 (0.013)
Train: 420 [ 750/1251 ( 60%)]  Loss: 3.565 (3.50)  Time: 0.678s, 1510.31/s  (0.693s, 1478.18/s)  LR: 2.140e-04  Data: 0.009 (0.013)
Train: 420 [ 800/1251 ( 64%)]  Loss: 3.587 (3.50)  Time: 0.673s, 1522.08/s  (0.693s, 1478.49/s)  LR: 2.140e-04  Data: 0.011 (0.013)
Train: 420 [ 850/1251 ( 68%)]  Loss: 3.462 (3.50)  Time: 0.714s, 1433.29/s  (0.693s, 1477.40/s)  LR: 2.140e-04  Data: 0.009 (0.013)
Train: 420 [ 900/1251 ( 72%)]  Loss: 3.848 (3.52)  Time: 0.714s, 1433.41/s  (0.693s, 1478.13/s)  LR: 2.140e-04  Data: 0.009 (0.013)
Train: 420 [ 950/1251 ( 76%)]  Loss: 3.467 (3.52)  Time: 0.676s, 1515.16/s  (0.692s, 1479.05/s)  LR: 2.140e-04  Data: 0.014 (0.012)
Train: 420 [1000/1251 ( 80%)]  Loss: 3.244 (3.50)  Time: 0.671s, 1527.16/s  (0.693s, 1478.04/s)  LR: 2.140e-04  Data: 0.010 (0.012)
Train: 420 [1050/1251 ( 84%)]  Loss: 3.360 (3.50)  Time: 0.728s, 1407.53/s  (0.693s, 1478.40/s)  LR: 2.140e-04  Data: 0.011 (0.012)
Train: 420 [1100/1251 ( 88%)]  Loss: 3.316 (3.49)  Time: 0.715s, 1431.78/s  (0.693s, 1478.49/s)  LR: 2.140e-04  Data: 0.013 (0.012)
Train: 420 [1150/1251 ( 92%)]  Loss: 3.134 (3.47)  Time: 0.674s, 1518.85/s  (0.692s, 1478.91/s)  LR: 2.140e-04  Data: 0.010 (0.012)
Train: 420 [1200/1251 ( 96%)]  Loss: 3.487 (3.47)  Time: 0.671s, 1525.21/s  (0.692s, 1479.10/s)  LR: 2.140e-04  Data: 0.014 (0.012)
Train: 420 [1250/1251 (100%)]  Loss: 3.075 (3.46)  Time: 0.658s, 1555.16/s  (0.692s, 1479.48/s)  LR: 2.140e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.514 (1.514)  Loss:  0.7778 (0.7778)  Acc@1: 91.1133 (91.1133)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.136 (0.584)  Loss:  0.8379 (1.3107)  Acc@1: 85.3774 (76.0960)  Acc@5: 96.1085 (92.9540)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-419.pth.tar', 76.4080000341797)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-418.pth.tar', 76.37600000976562)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-407.pth.tar', 76.2320000048828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-408.pth.tar', 76.14400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-417.pth.tar', 76.09800003417969)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-420.pth.tar', 76.09599998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-416.pth.tar', 76.05599990722656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-402.pth.tar', 76.02800011474609)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-415.pth.tar', 76.00399995361329)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-400.pth.tar', 75.96399995605469)

Train: 421 [   0/1251 (  0%)]  Loss: 3.584 (3.58)  Time: 2.133s,  480.14/s  (2.133s,  480.14/s)  LR: 2.120e-04  Data: 1.476 (1.476)
Train: 421 [  50/1251 (  4%)]  Loss: 3.630 (3.61)  Time: 0.671s, 1525.27/s  (0.726s, 1409.72/s)  LR: 2.120e-04  Data: 0.009 (0.049)
Train: 421 [ 100/1251 (  8%)]  Loss: 3.248 (3.49)  Time: 0.672s, 1524.89/s  (0.710s, 1442.08/s)  LR: 2.120e-04  Data: 0.010 (0.030)
Train: 421 [ 150/1251 ( 12%)]  Loss: 3.330 (3.45)  Time: 0.721s, 1420.34/s  (0.704s, 1455.07/s)  LR: 2.120e-04  Data: 0.009 (0.024)
Train: 421 [ 200/1251 ( 16%)]  Loss: 3.304 (3.42)  Time: 0.721s, 1419.37/s  (0.700s, 1462.19/s)  LR: 2.120e-04  Data: 0.011 (0.021)
Train: 421 [ 250/1251 ( 20%)]  Loss: 3.505 (3.43)  Time: 0.675s, 1518.06/s  (0.698s, 1467.51/s)  LR: 2.120e-04  Data: 0.010 (0.018)
Train: 421 [ 300/1251 ( 24%)]  Loss: 3.704 (3.47)  Time: 0.671s, 1526.05/s  (0.697s, 1470.10/s)  LR: 2.120e-04  Data: 0.010 (0.017)
Train: 421 [ 350/1251 ( 28%)]  Loss: 3.353 (3.46)  Time: 0.671s, 1526.19/s  (0.696s, 1470.41/s)  LR: 2.120e-04  Data: 0.010 (0.016)
Train: 421 [ 400/1251 ( 32%)]  Loss: 3.203 (3.43)  Time: 0.711s, 1440.38/s  (0.697s, 1469.95/s)  LR: 2.120e-04  Data: 0.009 (0.015)
Train: 421 [ 450/1251 ( 36%)]  Loss: 3.467 (3.43)  Time: 0.680s, 1505.34/s  (0.697s, 1470.16/s)  LR: 2.120e-04  Data: 0.009 (0.015)
Train: 421 [ 500/1251 ( 40%)]  Loss: 3.686 (3.46)  Time: 0.739s, 1385.38/s  (0.697s, 1469.42/s)  LR: 2.120e-04  Data: 0.009 (0.014)
Train: 421 [ 550/1251 ( 44%)]  Loss: 3.130 (3.43)  Time: 0.714s, 1434.78/s  (0.696s, 1470.49/s)  LR: 2.120e-04  Data: 0.009 (0.014)
Train: 421 [ 600/1251 ( 48%)]  Loss: 3.577 (3.44)  Time: 0.671s, 1525.26/s  (0.696s, 1470.97/s)  LR: 2.120e-04  Data: 0.011 (0.014)
Train: 421 [ 650/1251 ( 52%)]  Loss: 3.347 (3.43)  Time: 0.669s, 1529.98/s  (0.696s, 1471.84/s)  LR: 2.120e-04  Data: 0.009 (0.013)
Train: 421 [ 700/1251 ( 56%)]  Loss: 3.529 (3.44)  Time: 0.671s, 1526.71/s  (0.695s, 1472.55/s)  LR: 2.120e-04  Data: 0.010 (0.013)
Train: 421 [ 750/1251 ( 60%)]  Loss: 3.463 (3.44)  Time: 0.683s, 1500.09/s  (0.696s, 1472.26/s)  LR: 2.120e-04  Data: 0.010 (0.013)
Train: 421 [ 800/1251 ( 64%)]  Loss: 3.464 (3.44)  Time: 0.671s, 1525.72/s  (0.695s, 1472.81/s)  LR: 2.120e-04  Data: 0.010 (0.013)
Train: 421 [ 850/1251 ( 68%)]  Loss: 3.441 (3.44)  Time: 0.704s, 1455.29/s  (0.695s, 1473.50/s)  LR: 2.120e-04  Data: 0.010 (0.013)
Train: 421 [ 900/1251 ( 72%)]  Loss: 3.881 (3.47)  Time: 0.670s, 1529.37/s  (0.695s, 1473.94/s)  LR: 2.120e-04  Data: 0.009 (0.013)
Train: 421 [ 950/1251 ( 76%)]  Loss: 3.266 (3.46)  Time: 0.675s, 1517.50/s  (0.694s, 1474.58/s)  LR: 2.120e-04  Data: 0.009 (0.012)
Train: 421 [1000/1251 ( 80%)]  Loss: 3.554 (3.46)  Time: 0.704s, 1454.89/s  (0.694s, 1474.67/s)  LR: 2.120e-04  Data: 0.009 (0.012)
Train: 421 [1050/1251 ( 84%)]  Loss: 3.618 (3.47)  Time: 0.672s, 1524.48/s  (0.694s, 1475.76/s)  LR: 2.120e-04  Data: 0.010 (0.012)
Train: 421 [1100/1251 ( 88%)]  Loss: 2.723 (3.44)  Time: 0.672s, 1523.00/s  (0.694s, 1475.69/s)  LR: 2.120e-04  Data: 0.010 (0.012)
Train: 421 [1150/1251 ( 92%)]  Loss: 3.562 (3.44)  Time: 0.688s, 1487.63/s  (0.694s, 1476.49/s)  LR: 2.120e-04  Data: 0.010 (0.012)
Train: 421 [1200/1251 ( 96%)]  Loss: 3.460 (3.44)  Time: 0.675s, 1517.29/s  (0.693s, 1476.58/s)  LR: 2.120e-04  Data: 0.010 (0.012)
Train: 421 [1250/1251 (100%)]  Loss: 2.980 (3.42)  Time: 0.658s, 1555.47/s  (0.693s, 1477.04/s)  LR: 2.120e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.634 (1.634)  Loss:  0.7681 (0.7681)  Acc@1: 90.8203 (90.8203)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.136 (0.578)  Loss:  0.8442 (1.2850)  Acc@1: 84.9057 (75.9980)  Acc@5: 97.0519 (92.9320)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-419.pth.tar', 76.4080000341797)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-418.pth.tar', 76.37600000976562)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-407.pth.tar', 76.2320000048828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-408.pth.tar', 76.14400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-417.pth.tar', 76.09800003417969)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-420.pth.tar', 76.09599998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-416.pth.tar', 76.05599990722656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-402.pth.tar', 76.02800011474609)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-415.pth.tar', 76.00399995361329)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-421.pth.tar', 75.9980000366211)

Train: 422 [   0/1251 (  0%)]  Loss: 3.657 (3.66)  Time: 2.221s,  461.07/s  (2.221s,  461.07/s)  LR: 2.099e-04  Data: 1.605 (1.605)
Train: 422 [  50/1251 (  4%)]  Loss: 3.296 (3.48)  Time: 0.673s, 1520.43/s  (0.730s, 1402.86/s)  LR: 2.099e-04  Data: 0.009 (0.049)
Train: 422 [ 100/1251 (  8%)]  Loss: 3.618 (3.52)  Time: 0.704s, 1454.54/s  (0.713s, 1436.28/s)  LR: 2.099e-04  Data: 0.010 (0.030)
Train: 422 [ 150/1251 ( 12%)]  Loss: 3.517 (3.52)  Time: 0.702s, 1458.17/s  (0.706s, 1451.25/s)  LR: 2.099e-04  Data: 0.011 (0.024)
Train: 422 [ 200/1251 ( 16%)]  Loss: 3.895 (3.60)  Time: 0.675s, 1517.04/s  (0.702s, 1457.68/s)  LR: 2.099e-04  Data: 0.010 (0.020)
Train: 422 [ 250/1251 ( 20%)]  Loss: 3.415 (3.57)  Time: 0.692s, 1480.81/s  (0.699s, 1464.97/s)  LR: 2.099e-04  Data: 0.009 (0.018)
Train: 422 [ 300/1251 ( 24%)]  Loss: 3.362 (3.54)  Time: 0.692s, 1479.51/s  (0.699s, 1465.16/s)  LR: 2.099e-04  Data: 0.013 (0.017)
Train: 422 [ 350/1251 ( 28%)]  Loss: 3.433 (3.52)  Time: 0.693s, 1476.79/s  (0.697s, 1468.33/s)  LR: 2.099e-04  Data: 0.011 (0.016)
Train: 422 [ 400/1251 ( 32%)]  Loss: 3.586 (3.53)  Time: 0.686s, 1492.50/s  (0.697s, 1469.45/s)  LR: 2.099e-04  Data: 0.008 (0.015)
Train: 422 [ 450/1251 ( 36%)]  Loss: 3.687 (3.55)  Time: 0.682s, 1501.38/s  (0.697s, 1469.91/s)  LR: 2.099e-04  Data: 0.011 (0.015)
Train: 422 [ 500/1251 ( 40%)]  Loss: 3.165 (3.51)  Time: 0.709s, 1443.67/s  (0.697s, 1469.24/s)  LR: 2.099e-04  Data: 0.010 (0.014)
Train: 422 [ 550/1251 ( 44%)]  Loss: 3.297 (3.49)  Time: 0.717s, 1429.12/s  (0.697s, 1470.11/s)  LR: 2.099e-04  Data: 0.011 (0.014)
Train: 422 [ 600/1251 ( 48%)]  Loss: 3.202 (3.47)  Time: 0.676s, 1515.20/s  (0.696s, 1470.97/s)  LR: 2.099e-04  Data: 0.010 (0.014)
Train: 422 [ 650/1251 ( 52%)]  Loss: 3.646 (3.48)  Time: 0.681s, 1504.52/s  (0.696s, 1471.65/s)  LR: 2.099e-04  Data: 0.011 (0.014)
Train: 422 [ 700/1251 ( 56%)]  Loss: 3.370 (3.48)  Time: 0.667s, 1534.36/s  (0.696s, 1472.24/s)  LR: 2.099e-04  Data: 0.013 (0.013)
Train: 422 [ 750/1251 ( 60%)]  Loss: 3.472 (3.48)  Time: 0.721s, 1419.35/s  (0.695s, 1473.30/s)  LR: 2.099e-04  Data: 0.009 (0.013)
Train: 422 [ 800/1251 ( 64%)]  Loss: 2.990 (3.45)  Time: 0.674s, 1520.02/s  (0.694s, 1474.54/s)  LR: 2.099e-04  Data: 0.011 (0.013)
Train: 422 [ 850/1251 ( 68%)]  Loss: 3.551 (3.45)  Time: 0.715s, 1432.30/s  (0.695s, 1473.94/s)  LR: 2.099e-04  Data: 0.009 (0.013)
Train: 422 [ 900/1251 ( 72%)]  Loss: 3.344 (3.45)  Time: 0.718s, 1426.85/s  (0.694s, 1475.25/s)  LR: 2.099e-04  Data: 0.010 (0.013)
Train: 422 [ 950/1251 ( 76%)]  Loss: 3.726 (3.46)  Time: 0.676s, 1515.33/s  (0.694s, 1475.61/s)  LR: 2.099e-04  Data: 0.010 (0.013)
Train: 422 [1000/1251 ( 80%)]  Loss: 3.690 (3.47)  Time: 0.673s, 1520.86/s  (0.694s, 1475.12/s)  LR: 2.099e-04  Data: 0.010 (0.012)
Train: 422 [1050/1251 ( 84%)]  Loss: 3.358 (3.47)  Time: 0.677s, 1512.06/s  (0.694s, 1475.20/s)  LR: 2.099e-04  Data: 0.010 (0.012)
Train: 422 [1100/1251 ( 88%)]  Loss: 3.218 (3.46)  Time: 0.697s, 1470.06/s  (0.694s, 1475.64/s)  LR: 2.099e-04  Data: 0.011 (0.012)
Train: 422 [1150/1251 ( 92%)]  Loss: 3.794 (3.47)  Time: 0.667s, 1534.52/s  (0.694s, 1475.82/s)  LR: 2.099e-04  Data: 0.011 (0.012)
Train: 422 [1200/1251 ( 96%)]  Loss: 3.467 (3.47)  Time: 0.681s, 1503.43/s  (0.694s, 1475.78/s)  LR: 2.099e-04  Data: 0.009 (0.012)
Train: 422 [1250/1251 (100%)]  Loss: 3.645 (3.48)  Time: 0.660s, 1552.30/s  (0.694s, 1476.51/s)  LR: 2.099e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.461 (1.461)  Loss:  0.8022 (0.8022)  Acc@1: 90.3320 (90.3320)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.139 (0.575)  Loss:  0.9331 (1.3095)  Acc@1: 86.4387 (76.3720)  Acc@5: 96.8160 (93.1620)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-419.pth.tar', 76.4080000341797)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-418.pth.tar', 76.37600000976562)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-422.pth.tar', 76.37200005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-407.pth.tar', 76.2320000048828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-408.pth.tar', 76.14400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-417.pth.tar', 76.09800003417969)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-420.pth.tar', 76.09599998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-416.pth.tar', 76.05599990722656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-402.pth.tar', 76.02800011474609)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-415.pth.tar', 76.00399995361329)

Train: 423 [   0/1251 (  0%)]  Loss: 3.306 (3.31)  Time: 2.286s,  447.96/s  (2.286s,  447.96/s)  LR: 2.078e-04  Data: 1.670 (1.670)
Train: 423 [  50/1251 (  4%)]  Loss: 3.670 (3.49)  Time: 0.671s, 1526.05/s  (0.727s, 1408.63/s)  LR: 2.078e-04  Data: 0.010 (0.046)
Train: 423 [ 100/1251 (  8%)]  Loss: 3.650 (3.54)  Time: 0.708s, 1446.98/s  (0.711s, 1439.63/s)  LR: 2.078e-04  Data: 0.010 (0.029)
Train: 423 [ 150/1251 ( 12%)]  Loss: 3.446 (3.52)  Time: 0.701s, 1459.88/s  (0.708s, 1447.01/s)  LR: 2.078e-04  Data: 0.009 (0.023)
Train: 423 [ 200/1251 ( 16%)]  Loss: 2.894 (3.39)  Time: 0.713s, 1435.67/s  (0.705s, 1453.38/s)  LR: 2.078e-04  Data: 0.011 (0.020)
Train: 423 [ 250/1251 ( 20%)]  Loss: 3.170 (3.36)  Time: 0.673s, 1522.28/s  (0.702s, 1459.38/s)  LR: 2.078e-04  Data: 0.010 (0.018)
Train: 423 [ 300/1251 ( 24%)]  Loss: 3.515 (3.38)  Time: 0.718s, 1426.97/s  (0.700s, 1462.62/s)  LR: 2.078e-04  Data: 0.008 (0.017)
Train: 423 [ 350/1251 ( 28%)]  Loss: 2.941 (3.32)  Time: 0.700s, 1462.20/s  (0.698s, 1466.73/s)  LR: 2.078e-04  Data: 0.009 (0.016)
Train: 423 [ 400/1251 ( 32%)]  Loss: 3.557 (3.35)  Time: 0.673s, 1522.40/s  (0.697s, 1468.78/s)  LR: 2.078e-04  Data: 0.010 (0.015)
Train: 423 [ 450/1251 ( 36%)]  Loss: 3.450 (3.36)  Time: 0.717s, 1427.50/s  (0.696s, 1470.33/s)  LR: 2.078e-04  Data: 0.009 (0.014)
Train: 423 [ 500/1251 ( 40%)]  Loss: 3.313 (3.36)  Time: 0.711s, 1439.27/s  (0.696s, 1471.81/s)  LR: 2.078e-04  Data: 0.009 (0.014)
Train: 423 [ 550/1251 ( 44%)]  Loss: 3.142 (3.34)  Time: 0.671s, 1527.15/s  (0.695s, 1472.78/s)  LR: 2.078e-04  Data: 0.010 (0.014)
Train: 423 [ 600/1251 ( 48%)]  Loss: 3.215 (3.33)  Time: 0.668s, 1533.53/s  (0.695s, 1473.15/s)  LR: 2.078e-04  Data: 0.009 (0.014)
Train: 423 [ 650/1251 ( 52%)]  Loss: 3.392 (3.33)  Time: 0.705s, 1452.25/s  (0.696s, 1472.21/s)  LR: 2.078e-04  Data: 0.011 (0.013)
Train: 423 [ 700/1251 ( 56%)]  Loss: 3.423 (3.34)  Time: 0.671s, 1525.41/s  (0.696s, 1471.67/s)  LR: 2.078e-04  Data: 0.010 (0.013)
Train: 423 [ 750/1251 ( 60%)]  Loss: 3.886 (3.37)  Time: 0.670s, 1527.29/s  (0.696s, 1471.18/s)  LR: 2.078e-04  Data: 0.009 (0.013)
Train: 423 [ 800/1251 ( 64%)]  Loss: 3.360 (3.37)  Time: 0.673s, 1521.09/s  (0.696s, 1472.29/s)  LR: 2.078e-04  Data: 0.011 (0.013)
Train: 423 [ 850/1251 ( 68%)]  Loss: 3.407 (3.37)  Time: 0.697s, 1469.71/s  (0.695s, 1473.34/s)  LR: 2.078e-04  Data: 0.010 (0.013)
Train: 423 [ 900/1251 ( 72%)]  Loss: 3.532 (3.38)  Time: 0.702s, 1459.17/s  (0.695s, 1472.98/s)  LR: 2.078e-04  Data: 0.009 (0.012)
Train: 423 [ 950/1251 ( 76%)]  Loss: 3.418 (3.38)  Time: 0.700s, 1463.33/s  (0.695s, 1473.83/s)  LR: 2.078e-04  Data: 0.009 (0.012)
Train: 423 [1000/1251 ( 80%)]  Loss: 3.547 (3.39)  Time: 0.744s, 1376.33/s  (0.695s, 1474.05/s)  LR: 2.078e-04  Data: 0.011 (0.012)
Train: 423 [1050/1251 ( 84%)]  Loss: 3.599 (3.40)  Time: 0.699s, 1465.45/s  (0.695s, 1474.34/s)  LR: 2.078e-04  Data: 0.009 (0.012)
Train: 423 [1100/1251 ( 88%)]  Loss: 3.722 (3.42)  Time: 0.682s, 1501.43/s  (0.694s, 1474.82/s)  LR: 2.078e-04  Data: 0.010 (0.012)
Train: 423 [1150/1251 ( 92%)]  Loss: 3.427 (3.42)  Time: 0.700s, 1461.82/s  (0.694s, 1474.94/s)  LR: 2.078e-04  Data: 0.010 (0.012)
Train: 423 [1200/1251 ( 96%)]  Loss: 3.161 (3.41)  Time: 0.678s, 1509.73/s  (0.694s, 1475.62/s)  LR: 2.078e-04  Data: 0.010 (0.012)
Train: 423 [1250/1251 (100%)]  Loss: 3.196 (3.40)  Time: 0.704s, 1455.29/s  (0.694s, 1476.35/s)  LR: 2.078e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.583 (1.583)  Loss:  0.7344 (0.7344)  Acc@1: 90.7227 (90.7227)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.136 (0.575)  Loss:  0.8311 (1.2961)  Acc@1: 85.4953 (76.1820)  Acc@5: 97.1698 (93.1720)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-419.pth.tar', 76.4080000341797)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-418.pth.tar', 76.37600000976562)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-422.pth.tar', 76.37200005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-407.pth.tar', 76.2320000048828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-423.pth.tar', 76.18199990478516)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-408.pth.tar', 76.14400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-417.pth.tar', 76.09800003417969)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-420.pth.tar', 76.09599998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-416.pth.tar', 76.05599990722656)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-402.pth.tar', 76.02800011474609)

Train: 424 [   0/1251 (  0%)]  Loss: 3.368 (3.37)  Time: 2.371s,  431.84/s  (2.371s,  431.84/s)  LR: 2.057e-04  Data: 1.756 (1.756)
Train: 424 [  50/1251 (  4%)]  Loss: 3.299 (3.33)  Time: 0.675s, 1516.21/s  (0.721s, 1419.32/s)  LR: 2.057e-04  Data: 0.010 (0.050)
Train: 424 [ 100/1251 (  8%)]  Loss: 3.479 (3.38)  Time: 0.673s, 1521.63/s  (0.707s, 1448.99/s)  LR: 2.057e-04  Data: 0.011 (0.030)
Train: 424 [ 150/1251 ( 12%)]  Loss: 3.699 (3.46)  Time: 0.671s, 1526.76/s  (0.699s, 1465.18/s)  LR: 2.057e-04  Data: 0.010 (0.024)
Train: 424 [ 200/1251 ( 16%)]  Loss: 3.430 (3.45)  Time: 0.698s, 1466.72/s  (0.699s, 1465.01/s)  LR: 2.057e-04  Data: 0.013 (0.020)
Train: 424 [ 250/1251 ( 20%)]  Loss: 3.480 (3.46)  Time: 0.708s, 1445.84/s  (0.698s, 1467.96/s)  LR: 2.057e-04  Data: 0.011 (0.018)
Train: 424 [ 300/1251 ( 24%)]  Loss: 3.329 (3.44)  Time: 0.670s, 1528.78/s  (0.697s, 1468.18/s)  LR: 2.057e-04  Data: 0.013 (0.017)
Train: 424 [ 350/1251 ( 28%)]  Loss: 3.483 (3.45)  Time: 0.710s, 1442.33/s  (0.696s, 1471.88/s)  LR: 2.057e-04  Data: 0.011 (0.016)
Train: 424 [ 400/1251 ( 32%)]  Loss: 3.486 (3.45)  Time: 0.673s, 1521.65/s  (0.695s, 1473.73/s)  LR: 2.057e-04  Data: 0.011 (0.015)
Train: 424 [ 450/1251 ( 36%)]  Loss: 3.445 (3.45)  Time: 0.713s, 1435.57/s  (0.695s, 1473.88/s)  LR: 2.057e-04  Data: 0.012 (0.015)
Train: 424 [ 500/1251 ( 40%)]  Loss: 3.727 (3.48)  Time: 0.679s, 1507.92/s  (0.695s, 1474.14/s)  LR: 2.057e-04  Data: 0.009 (0.014)
Train: 424 [ 550/1251 ( 44%)]  Loss: 3.368 (3.47)  Time: 0.733s, 1396.84/s  (0.695s, 1474.44/s)  LR: 2.057e-04  Data: 0.010 (0.014)
Train: 424 [ 600/1251 ( 48%)]  Loss: 3.293 (3.45)  Time: 0.679s, 1507.78/s  (0.694s, 1474.99/s)  LR: 2.057e-04  Data: 0.012 (0.014)
Train: 424 [ 650/1251 ( 52%)]  Loss: 3.566 (3.46)  Time: 0.691s, 1480.87/s  (0.694s, 1475.44/s)  LR: 2.057e-04  Data: 0.016 (0.013)
Train: 424 [ 700/1251 ( 56%)]  Loss: 3.424 (3.46)  Time: 0.705s, 1452.12/s  (0.694s, 1475.91/s)  LR: 2.057e-04  Data: 0.010 (0.013)
Train: 424 [ 750/1251 ( 60%)]  Loss: 3.649 (3.47)  Time: 0.690s, 1483.06/s  (0.694s, 1476.26/s)  LR: 2.057e-04  Data: 0.014 (0.013)
Train: 424 [ 800/1251 ( 64%)]  Loss: 3.328 (3.46)  Time: 0.680s, 1506.25/s  (0.694s, 1476.45/s)  LR: 2.057e-04  Data: 0.012 (0.013)
Train: 424 [ 850/1251 ( 68%)]  Loss: 3.447 (3.46)  Time: 0.754s, 1358.30/s  (0.693s, 1476.65/s)  LR: 2.057e-04  Data: 0.010 (0.013)
Train: 424 [ 900/1251 ( 72%)]  Loss: 3.754 (3.48)  Time: 0.686s, 1492.95/s  (0.694s, 1476.27/s)  LR: 2.057e-04  Data: 0.013 (0.013)
Train: 424 [ 950/1251 ( 76%)]  Loss: 3.607 (3.48)  Time: 0.672s, 1524.61/s  (0.693s, 1476.79/s)  LR: 2.057e-04  Data: 0.012 (0.013)
Train: 424 [1000/1251 ( 80%)]  Loss: 3.583 (3.49)  Time: 0.682s, 1500.64/s  (0.694s, 1476.53/s)  LR: 2.057e-04  Data: 0.014 (0.012)
Train: 424 [1050/1251 ( 84%)]  Loss: 3.433 (3.49)  Time: 0.708s, 1447.20/s  (0.693s, 1476.99/s)  LR: 2.057e-04  Data: 0.010 (0.012)
Train: 424 [1100/1251 ( 88%)]  Loss: 3.347 (3.48)  Time: 0.686s, 1491.80/s  (0.693s, 1478.03/s)  LR: 2.057e-04  Data: 0.011 (0.012)
Train: 424 [1150/1251 ( 92%)]  Loss: 3.406 (3.48)  Time: 0.674s, 1519.01/s  (0.693s, 1478.68/s)  LR: 2.057e-04  Data: 0.013 (0.012)
Train: 424 [1200/1251 ( 96%)]  Loss: 3.266 (3.47)  Time: 0.670s, 1527.23/s  (0.692s, 1478.81/s)  LR: 2.057e-04  Data: 0.011 (0.012)
Train: 424 [1250/1251 (100%)]  Loss: 3.444 (3.47)  Time: 0.656s, 1560.80/s  (0.692s, 1479.15/s)  LR: 2.057e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.578 (1.578)  Loss:  0.9863 (0.9863)  Acc@1: 90.9180 (90.9180)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.136 (0.572)  Loss:  1.0430 (1.4551)  Acc@1: 84.9057 (76.0660)  Acc@5: 96.5802 (93.0240)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-419.pth.tar', 76.4080000341797)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-418.pth.tar', 76.37600000976562)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-422.pth.tar', 76.37200005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-407.pth.tar', 76.2320000048828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-423.pth.tar', 76.18199990478516)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-408.pth.tar', 76.14400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-417.pth.tar', 76.09800003417969)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-420.pth.tar', 76.09599998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-424.pth.tar', 76.06600003662109)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-416.pth.tar', 76.05599990722656)

Train: 425 [   0/1251 (  0%)]  Loss: 3.367 (3.37)  Time: 2.202s,  465.10/s  (2.202s,  465.10/s)  LR: 2.037e-04  Data: 1.585 (1.585)
Train: 425 [  50/1251 (  4%)]  Loss: 3.365 (3.37)  Time: 0.715s, 1432.99/s  (0.722s, 1417.95/s)  LR: 2.037e-04  Data: 0.010 (0.046)
Train: 425 [ 100/1251 (  8%)]  Loss: 3.302 (3.34)  Time: 0.699s, 1465.04/s  (0.707s, 1449.38/s)  LR: 2.037e-04  Data: 0.009 (0.028)
Train: 425 [ 150/1251 ( 12%)]  Loss: 3.335 (3.34)  Time: 0.705s, 1453.04/s  (0.701s, 1461.36/s)  LR: 2.037e-04  Data: 0.010 (0.022)
Train: 425 [ 200/1251 ( 16%)]  Loss: 3.395 (3.35)  Time: 0.691s, 1481.51/s  (0.699s, 1464.62/s)  LR: 2.037e-04  Data: 0.012 (0.019)
Train: 425 [ 250/1251 ( 20%)]  Loss: 3.311 (3.35)  Time: 0.674s, 1518.92/s  (0.696s, 1471.08/s)  LR: 2.037e-04  Data: 0.010 (0.018)
Train: 425 [ 300/1251 ( 24%)]  Loss: 3.403 (3.35)  Time: 0.789s, 1298.17/s  (0.696s, 1470.93/s)  LR: 2.037e-04  Data: 0.011 (0.016)
Train: 425 [ 350/1251 ( 28%)]  Loss: 3.181 (3.33)  Time: 0.704s, 1454.75/s  (0.695s, 1472.62/s)  LR: 2.037e-04  Data: 0.010 (0.016)
Train: 425 [ 400/1251 ( 32%)]  Loss: 3.073 (3.30)  Time: 0.711s, 1440.03/s  (0.695s, 1473.62/s)  LR: 2.037e-04  Data: 0.010 (0.015)
Train: 425 [ 450/1251 ( 36%)]  Loss: 3.703 (3.34)  Time: 0.674s, 1518.23/s  (0.695s, 1473.53/s)  LR: 2.037e-04  Data: 0.010 (0.014)
Train: 425 [ 500/1251 ( 40%)]  Loss: 3.480 (3.36)  Time: 0.705s, 1453.17/s  (0.695s, 1472.48/s)  LR: 2.037e-04  Data: 0.010 (0.014)
Train: 425 [ 550/1251 ( 44%)]  Loss: 3.961 (3.41)  Time: 0.702s, 1459.08/s  (0.695s, 1472.71/s)  LR: 2.037e-04  Data: 0.010 (0.014)
Train: 425 [ 600/1251 ( 48%)]  Loss: 3.559 (3.42)  Time: 0.672s, 1524.46/s  (0.696s, 1471.98/s)  LR: 2.037e-04  Data: 0.009 (0.013)
Train: 425 [ 650/1251 ( 52%)]  Loss: 3.628 (3.43)  Time: 0.717s, 1428.36/s  (0.696s, 1471.61/s)  LR: 2.037e-04  Data: 0.012 (0.013)
Train: 425 [ 700/1251 ( 56%)]  Loss: 3.330 (3.43)  Time: 0.672s, 1524.19/s  (0.696s, 1472.19/s)  LR: 2.037e-04  Data: 0.010 (0.013)
Train: 425 [ 750/1251 ( 60%)]  Loss: 3.471 (3.43)  Time: 0.674s, 1519.25/s  (0.695s, 1473.34/s)  LR: 2.037e-04  Data: 0.011 (0.013)
Train: 425 [ 800/1251 ( 64%)]  Loss: 3.642 (3.44)  Time: 0.672s, 1522.91/s  (0.695s, 1472.95/s)  LR: 2.037e-04  Data: 0.010 (0.013)
Train: 425 [ 850/1251 ( 68%)]  Loss: 3.639 (3.45)  Time: 0.708s, 1446.90/s  (0.695s, 1473.58/s)  LR: 2.037e-04  Data: 0.009 (0.013)
Train: 425 [ 900/1251 ( 72%)]  Loss: 3.859 (3.47)  Time: 0.702s, 1459.34/s  (0.695s, 1473.98/s)  LR: 2.037e-04  Data: 0.009 (0.012)
Train: 425 [ 950/1251 ( 76%)]  Loss: 3.386 (3.47)  Time: 0.704s, 1454.09/s  (0.695s, 1474.09/s)  LR: 2.037e-04  Data: 0.016 (0.012)
Train: 425 [1000/1251 ( 80%)]  Loss: 3.598 (3.48)  Time: 0.702s, 1459.68/s  (0.695s, 1474.36/s)  LR: 2.037e-04  Data: 0.010 (0.012)
Train: 425 [1050/1251 ( 84%)]  Loss: 3.131 (3.46)  Time: 0.699s, 1464.01/s  (0.695s, 1474.10/s)  LR: 2.037e-04  Data: 0.009 (0.012)
Train: 425 [1100/1251 ( 88%)]  Loss: 3.347 (3.46)  Time: 0.687s, 1491.32/s  (0.694s, 1474.77/s)  LR: 2.037e-04  Data: 0.011 (0.012)
Train: 425 [1150/1251 ( 92%)]  Loss: 3.501 (3.46)  Time: 0.680s, 1506.55/s  (0.694s, 1475.51/s)  LR: 2.037e-04  Data: 0.014 (0.012)
Train: 425 [1200/1251 ( 96%)]  Loss: 2.673 (3.43)  Time: 0.705s, 1452.25/s  (0.694s, 1475.46/s)  LR: 2.037e-04  Data: 0.009 (0.012)
Train: 425 [1250/1251 (100%)]  Loss: 3.263 (3.42)  Time: 0.734s, 1395.23/s  (0.694s, 1475.39/s)  LR: 2.037e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.534 (1.534)  Loss:  0.8071 (0.8071)  Acc@1: 91.2109 (91.2109)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.136 (0.581)  Loss:  0.8599 (1.3186)  Acc@1: 86.4387 (76.5180)  Acc@5: 97.8774 (93.0800)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-425.pth.tar', 76.51800018554688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-419.pth.tar', 76.4080000341797)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-418.pth.tar', 76.37600000976562)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-422.pth.tar', 76.37200005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-407.pth.tar', 76.2320000048828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-423.pth.tar', 76.18199990478516)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-408.pth.tar', 76.14400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-417.pth.tar', 76.09800003417969)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-420.pth.tar', 76.09599998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-424.pth.tar', 76.06600003662109)

Train: 426 [   0/1251 (  0%)]  Loss: 3.549 (3.55)  Time: 2.136s,  479.37/s  (2.136s,  479.37/s)  LR: 2.016e-04  Data: 1.483 (1.483)
Train: 426 [  50/1251 (  4%)]  Loss: 3.167 (3.36)  Time: 0.671s, 1525.55/s  (0.728s, 1407.37/s)  LR: 2.016e-04  Data: 0.013 (0.048)
Train: 426 [ 100/1251 (  8%)]  Loss: 3.271 (3.33)  Time: 0.672s, 1524.15/s  (0.707s, 1447.89/s)  LR: 2.016e-04  Data: 0.011 (0.030)
Train: 426 [ 150/1251 ( 12%)]  Loss: 3.163 (3.29)  Time: 0.680s, 1505.00/s  (0.702s, 1457.84/s)  LR: 2.016e-04  Data: 0.009 (0.023)
Train: 426 [ 200/1251 ( 16%)]  Loss: 3.458 (3.32)  Time: 0.674s, 1519.26/s  (0.699s, 1464.87/s)  LR: 2.016e-04  Data: 0.010 (0.020)
Train: 426 [ 250/1251 ( 20%)]  Loss: 3.615 (3.37)  Time: 0.672s, 1523.60/s  (0.698s, 1467.73/s)  LR: 2.016e-04  Data: 0.011 (0.018)
Train: 426 [ 300/1251 ( 24%)]  Loss: 3.523 (3.39)  Time: 0.671s, 1525.62/s  (0.697s, 1469.73/s)  LR: 2.016e-04  Data: 0.010 (0.017)
Train: 426 [ 350/1251 ( 28%)]  Loss: 3.328 (3.38)  Time: 0.686s, 1492.38/s  (0.696s, 1471.68/s)  LR: 2.016e-04  Data: 0.014 (0.016)
Train: 426 [ 400/1251 ( 32%)]  Loss: 3.625 (3.41)  Time: 0.672s, 1523.85/s  (0.695s, 1472.69/s)  LR: 2.016e-04  Data: 0.011 (0.015)
Train: 426 [ 450/1251 ( 36%)]  Loss: 3.208 (3.39)  Time: 0.674s, 1520.08/s  (0.695s, 1472.43/s)  LR: 2.016e-04  Data: 0.011 (0.015)
Train: 426 [ 500/1251 ( 40%)]  Loss: 3.571 (3.41)  Time: 0.684s, 1498.04/s  (0.695s, 1473.69/s)  LR: 2.016e-04  Data: 0.011 (0.014)
Train: 426 [ 550/1251 ( 44%)]  Loss: 3.262 (3.40)  Time: 0.673s, 1520.71/s  (0.695s, 1473.31/s)  LR: 2.016e-04  Data: 0.011 (0.014)
Train: 426 [ 600/1251 ( 48%)]  Loss: 3.559 (3.41)  Time: 0.671s, 1525.21/s  (0.694s, 1474.60/s)  LR: 2.016e-04  Data: 0.010 (0.014)
Train: 426 [ 650/1251 ( 52%)]  Loss: 3.587 (3.42)  Time: 0.672s, 1524.44/s  (0.694s, 1475.01/s)  LR: 2.016e-04  Data: 0.010 (0.013)
Train: 426 [ 700/1251 ( 56%)]  Loss: 3.488 (3.43)  Time: 0.672s, 1522.82/s  (0.694s, 1475.41/s)  LR: 2.016e-04  Data: 0.010 (0.013)
Train: 426 [ 750/1251 ( 60%)]  Loss: 3.189 (3.41)  Time: 0.720s, 1421.42/s  (0.694s, 1476.15/s)  LR: 2.016e-04  Data: 0.010 (0.013)
Train: 426 [ 800/1251 ( 64%)]  Loss: 3.348 (3.41)  Time: 0.698s, 1466.72/s  (0.694s, 1475.12/s)  LR: 2.016e-04  Data: 0.011 (0.013)
Train: 426 [ 850/1251 ( 68%)]  Loss: 3.491 (3.41)  Time: 0.671s, 1525.17/s  (0.695s, 1474.44/s)  LR: 2.016e-04  Data: 0.014 (0.013)
Train: 426 [ 900/1251 ( 72%)]  Loss: 3.736 (3.43)  Time: 0.723s, 1415.89/s  (0.694s, 1475.29/s)  LR: 2.016e-04  Data: 0.009 (0.013)
Train: 426 [ 950/1251 ( 76%)]  Loss: 3.257 (3.42)  Time: 0.676s, 1514.40/s  (0.694s, 1474.91/s)  LR: 2.016e-04  Data: 0.011 (0.012)
Train: 426 [1000/1251 ( 80%)]  Loss: 3.713 (3.43)  Time: 0.697s, 1468.97/s  (0.694s, 1475.55/s)  LR: 2.016e-04  Data: 0.011 (0.012)
Train: 426 [1050/1251 ( 84%)]  Loss: 3.499 (3.44)  Time: 0.672s, 1524.40/s  (0.694s, 1476.27/s)  LR: 2.016e-04  Data: 0.011 (0.012)
Train: 426 [1100/1251 ( 88%)]  Loss: 3.152 (3.42)  Time: 0.686s, 1491.91/s  (0.693s, 1476.57/s)  LR: 2.016e-04  Data: 0.009 (0.012)
Train: 426 [1150/1251 ( 92%)]  Loss: 3.148 (3.41)  Time: 0.669s, 1531.74/s  (0.694s, 1476.27/s)  LR: 2.016e-04  Data: 0.011 (0.012)
Train: 426 [1200/1251 ( 96%)]  Loss: 3.591 (3.42)  Time: 0.714s, 1434.11/s  (0.694s, 1476.10/s)  LR: 2.016e-04  Data: 0.009 (0.012)
Train: 426 [1250/1251 (100%)]  Loss: 3.456 (3.42)  Time: 0.657s, 1559.06/s  (0.694s, 1476.39/s)  LR: 2.016e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.561 (1.561)  Loss:  0.8701 (0.8701)  Acc@1: 88.9648 (88.9648)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.136 (0.572)  Loss:  0.9253 (1.2832)  Acc@1: 84.3160 (76.3420)  Acc@5: 97.0519 (93.0820)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-425.pth.tar', 76.51800018554688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-419.pth.tar', 76.4080000341797)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-418.pth.tar', 76.37600000976562)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-422.pth.tar', 76.37200005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-426.pth.tar', 76.3420000390625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-407.pth.tar', 76.2320000048828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-423.pth.tar', 76.18199990478516)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-408.pth.tar', 76.14400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-417.pth.tar', 76.09800003417969)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-420.pth.tar', 76.09599998291016)

Train: 427 [   0/1251 (  0%)]  Loss: 3.162 (3.16)  Time: 2.326s,  440.26/s  (2.326s,  440.26/s)  LR: 1.996e-04  Data: 1.695 (1.695)
Train: 427 [  50/1251 (  4%)]  Loss: 3.219 (3.19)  Time: 0.672s, 1524.78/s  (0.734s, 1394.82/s)  LR: 1.996e-04  Data: 0.011 (0.050)
Train: 427 [ 100/1251 (  8%)]  Loss: 3.595 (3.33)  Time: 0.683s, 1500.04/s  (0.710s, 1442.30/s)  LR: 1.996e-04  Data: 0.012 (0.030)
Train: 427 [ 150/1251 ( 12%)]  Loss: 3.825 (3.45)  Time: 0.699s, 1464.24/s  (0.706s, 1450.75/s)  LR: 1.996e-04  Data: 0.009 (0.024)
Train: 427 [ 200/1251 ( 16%)]  Loss: 3.623 (3.48)  Time: 0.671s, 1527.09/s  (0.702s, 1458.93/s)  LR: 1.996e-04  Data: 0.011 (0.021)
Train: 427 [ 250/1251 ( 20%)]  Loss: 3.579 (3.50)  Time: 0.668s, 1532.64/s  (0.702s, 1459.61/s)  LR: 1.996e-04  Data: 0.011 (0.019)
Train: 427 [ 300/1251 ( 24%)]  Loss: 3.762 (3.54)  Time: 0.671s, 1525.07/s  (0.701s, 1461.40/s)  LR: 1.996e-04  Data: 0.010 (0.017)
Train: 427 [ 350/1251 ( 28%)]  Loss: 3.644 (3.55)  Time: 0.672s, 1523.38/s  (0.699s, 1464.00/s)  LR: 1.996e-04  Data: 0.011 (0.016)
Train: 427 [ 400/1251 ( 32%)]  Loss: 3.697 (3.57)  Time: 0.700s, 1462.73/s  (0.698s, 1466.31/s)  LR: 1.996e-04  Data: 0.011 (0.016)
Train: 427 [ 450/1251 ( 36%)]  Loss: 3.865 (3.60)  Time: 0.800s, 1279.95/s  (0.698s, 1466.39/s)  LR: 1.996e-04  Data: 0.009 (0.015)
Train: 427 [ 500/1251 ( 40%)]  Loss: 3.533 (3.59)  Time: 0.693s, 1478.19/s  (0.698s, 1467.68/s)  LR: 1.996e-04  Data: 0.014 (0.015)
Train: 427 [ 550/1251 ( 44%)]  Loss: 3.403 (3.58)  Time: 0.673s, 1520.58/s  (0.697s, 1468.91/s)  LR: 1.996e-04  Data: 0.009 (0.014)
Train: 427 [ 600/1251 ( 48%)]  Loss: 3.221 (3.55)  Time: 0.671s, 1526.71/s  (0.696s, 1470.35/s)  LR: 1.996e-04  Data: 0.010 (0.014)
Train: 427 [ 650/1251 ( 52%)]  Loss: 3.178 (3.52)  Time: 0.711s, 1441.06/s  (0.696s, 1470.40/s)  LR: 1.996e-04  Data: 0.010 (0.014)
Train: 427 [ 700/1251 ( 56%)]  Loss: 3.450 (3.52)  Time: 0.763s, 1341.72/s  (0.696s, 1470.35/s)  LR: 1.996e-04  Data: 0.010 (0.013)
Train: 427 [ 750/1251 ( 60%)]  Loss: 3.219 (3.50)  Time: 0.763s, 1342.11/s  (0.696s, 1471.24/s)  LR: 1.996e-04  Data: 0.009 (0.013)
Train: 427 [ 800/1251 ( 64%)]  Loss: 3.190 (3.48)  Time: 0.690s, 1483.71/s  (0.696s, 1472.01/s)  LR: 1.996e-04  Data: 0.010 (0.013)
Train: 427 [ 850/1251 ( 68%)]  Loss: 3.330 (3.47)  Time: 0.673s, 1521.51/s  (0.695s, 1472.49/s)  LR: 1.996e-04  Data: 0.010 (0.013)
Train: 427 [ 900/1251 ( 72%)]  Loss: 3.551 (3.48)  Time: 0.726s, 1409.60/s  (0.695s, 1473.17/s)  LR: 1.996e-04  Data: 0.009 (0.013)
Train: 427 [ 950/1251 ( 76%)]  Loss: 3.195 (3.46)  Time: 0.670s, 1528.16/s  (0.695s, 1473.90/s)  LR: 1.996e-04  Data: 0.010 (0.013)
Train: 427 [1000/1251 ( 80%)]  Loss: 3.369 (3.46)  Time: 0.674s, 1518.81/s  (0.695s, 1473.78/s)  LR: 1.996e-04  Data: 0.010 (0.013)
Train: 427 [1050/1251 ( 84%)]  Loss: 3.131 (3.44)  Time: 0.672s, 1523.64/s  (0.695s, 1473.71/s)  LR: 1.996e-04  Data: 0.010 (0.013)
Train: 427 [1100/1251 ( 88%)]  Loss: 3.510 (3.45)  Time: 0.666s, 1537.37/s  (0.694s, 1474.50/s)  LR: 1.996e-04  Data: 0.009 (0.012)
Train: 427 [1150/1251 ( 92%)]  Loss: 3.387 (3.44)  Time: 0.671s, 1525.57/s  (0.694s, 1474.92/s)  LR: 1.996e-04  Data: 0.010 (0.012)
Train: 427 [1200/1251 ( 96%)]  Loss: 3.560 (3.45)  Time: 0.671s, 1525.11/s  (0.694s, 1475.51/s)  LR: 1.996e-04  Data: 0.010 (0.012)
Train: 427 [1250/1251 (100%)]  Loss: 3.483 (3.45)  Time: 0.661s, 1550.04/s  (0.694s, 1476.04/s)  LR: 1.996e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.470 (1.470)  Loss:  0.8418 (0.8418)  Acc@1: 90.3320 (90.3320)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.136 (0.588)  Loss:  0.9214 (1.3347)  Acc@1: 85.3774 (75.8360)  Acc@5: 96.5802 (92.8660)
Train: 428 [   0/1251 (  0%)]  Loss: 3.310 (3.31)  Time: 2.159s,  474.35/s  (2.159s,  474.35/s)  LR: 1.975e-04  Data: 1.541 (1.541)
Train: 428 [  50/1251 (  4%)]  Loss: 3.058 (3.18)  Time: 0.713s, 1435.54/s  (0.729s, 1403.78/s)  LR: 1.975e-04  Data: 0.011 (0.048)
Train: 428 [ 100/1251 (  8%)]  Loss: 3.344 (3.24)  Time: 0.674s, 1519.17/s  (0.710s, 1441.91/s)  LR: 1.975e-04  Data: 0.010 (0.029)
Train: 428 [ 150/1251 ( 12%)]  Loss: 3.459 (3.29)  Time: 0.721s, 1420.95/s  (0.705s, 1453.50/s)  LR: 1.975e-04  Data: 0.013 (0.023)
Train: 428 [ 200/1251 ( 16%)]  Loss: 3.211 (3.28)  Time: 0.717s, 1427.48/s  (0.704s, 1454.32/s)  LR: 1.975e-04  Data: 0.016 (0.020)
Train: 428 [ 250/1251 ( 20%)]  Loss: 3.672 (3.34)  Time: 0.673s, 1521.18/s  (0.702s, 1458.68/s)  LR: 1.975e-04  Data: 0.011 (0.018)
Train: 428 [ 300/1251 ( 24%)]  Loss: 3.377 (3.35)  Time: 0.671s, 1525.70/s  (0.700s, 1463.41/s)  LR: 1.975e-04  Data: 0.010 (0.017)
Train: 428 [ 350/1251 ( 28%)]  Loss: 3.220 (3.33)  Time: 0.670s, 1529.26/s  (0.701s, 1460.60/s)  LR: 1.975e-04  Data: 0.010 (0.016)
Train: 428 [ 400/1251 ( 32%)]  Loss: 3.761 (3.38)  Time: 0.705s, 1452.84/s  (0.699s, 1465.35/s)  LR: 1.975e-04  Data: 0.009 (0.015)
Train: 428 [ 450/1251 ( 36%)]  Loss: 3.353 (3.38)  Time: 0.678s, 1509.82/s  (0.698s, 1466.83/s)  LR: 1.975e-04  Data: 0.010 (0.015)
Train: 428 [ 500/1251 ( 40%)]  Loss: 3.364 (3.38)  Time: 0.698s, 1466.76/s  (0.697s, 1468.23/s)  LR: 1.975e-04  Data: 0.010 (0.014)
Train: 428 [ 550/1251 ( 44%)]  Loss: 3.433 (3.38)  Time: 0.676s, 1514.93/s  (0.697s, 1470.13/s)  LR: 1.975e-04  Data: 0.011 (0.014)
Train: 428 [ 600/1251 ( 48%)]  Loss: 3.180 (3.36)  Time: 0.673s, 1522.54/s  (0.697s, 1469.62/s)  LR: 1.975e-04  Data: 0.011 (0.014)
Train: 428 [ 650/1251 ( 52%)]  Loss: 3.356 (3.36)  Time: 0.701s, 1460.21/s  (0.697s, 1469.80/s)  LR: 1.975e-04  Data: 0.009 (0.014)
Train: 428 [ 700/1251 ( 56%)]  Loss: 3.357 (3.36)  Time: 0.670s, 1527.68/s  (0.696s, 1470.78/s)  LR: 1.975e-04  Data: 0.012 (0.013)
Train: 428 [ 750/1251 ( 60%)]  Loss: 3.045 (3.34)  Time: 0.701s, 1460.09/s  (0.696s, 1471.96/s)  LR: 1.975e-04  Data: 0.009 (0.013)
Train: 428 [ 800/1251 ( 64%)]  Loss: 3.562 (3.36)  Time: 0.747s, 1370.23/s  (0.696s, 1471.93/s)  LR: 1.975e-04  Data: 0.011 (0.013)
Train: 428 [ 850/1251 ( 68%)]  Loss: 3.579 (3.37)  Time: 0.684s, 1496.35/s  (0.695s, 1472.44/s)  LR: 1.975e-04  Data: 0.009 (0.013)
Train: 428 [ 900/1251 ( 72%)]  Loss: 3.529 (3.38)  Time: 0.674s, 1520.13/s  (0.695s, 1473.78/s)  LR: 1.975e-04  Data: 0.011 (0.013)
Train: 428 [ 950/1251 ( 76%)]  Loss: 3.452 (3.38)  Time: 0.730s, 1403.55/s  (0.695s, 1474.33/s)  LR: 1.975e-04  Data: 0.011 (0.013)
Train: 428 [1000/1251 ( 80%)]  Loss: 3.286 (3.38)  Time: 0.709s, 1443.93/s  (0.694s, 1474.85/s)  LR: 1.975e-04  Data: 0.009 (0.012)
Train: 428 [1050/1251 ( 84%)]  Loss: 3.637 (3.39)  Time: 0.725s, 1412.82/s  (0.694s, 1475.35/s)  LR: 1.975e-04  Data: 0.011 (0.012)
Train: 428 [1100/1251 ( 88%)]  Loss: 3.498 (3.39)  Time: 0.669s, 1531.68/s  (0.694s, 1476.36/s)  LR: 1.975e-04  Data: 0.010 (0.012)
Train: 428 [1150/1251 ( 92%)]  Loss: 3.414 (3.39)  Time: 0.704s, 1455.47/s  (0.694s, 1476.48/s)  LR: 1.975e-04  Data: 0.009 (0.012)
Train: 428 [1200/1251 ( 96%)]  Loss: 3.409 (3.39)  Time: 0.708s, 1445.88/s  (0.694s, 1476.51/s)  LR: 1.975e-04  Data: 0.009 (0.012)
Train: 428 [1250/1251 (100%)]  Loss: 3.350 (3.39)  Time: 0.659s, 1553.20/s  (0.694s, 1476.23/s)  LR: 1.975e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.601 (1.601)  Loss:  0.7041 (0.7041)  Acc@1: 90.2344 (90.2344)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.136 (0.574)  Loss:  0.7378 (1.2578)  Acc@1: 86.9104 (76.4260)  Acc@5: 97.2877 (93.1880)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-425.pth.tar', 76.51800018554688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-428.pth.tar', 76.42600000244141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-419.pth.tar', 76.4080000341797)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-418.pth.tar', 76.37600000976562)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-422.pth.tar', 76.37200005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-426.pth.tar', 76.3420000390625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-407.pth.tar', 76.2320000048828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-423.pth.tar', 76.18199990478516)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-408.pth.tar', 76.14400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-417.pth.tar', 76.09800003417969)

Train: 429 [   0/1251 (  0%)]  Loss: 2.975 (2.98)  Time: 2.174s,  470.91/s  (2.174s,  470.91/s)  LR: 1.955e-04  Data: 1.520 (1.520)
Train: 429 [  50/1251 (  4%)]  Loss: 3.305 (3.14)  Time: 0.705s, 1452.69/s  (0.732s, 1399.22/s)  LR: 1.955e-04  Data: 0.010 (0.048)
Train: 429 [ 100/1251 (  8%)]  Loss: 3.475 (3.25)  Time: 0.673s, 1521.04/s  (0.714s, 1433.82/s)  LR: 1.955e-04  Data: 0.010 (0.030)
Train: 429 [ 150/1251 ( 12%)]  Loss: 3.635 (3.35)  Time: 0.666s, 1537.60/s  (0.704s, 1453.98/s)  LR: 1.955e-04  Data: 0.010 (0.023)
Train: 429 [ 200/1251 ( 16%)]  Loss: 3.563 (3.39)  Time: 0.727s, 1408.61/s  (0.702s, 1459.34/s)  LR: 1.955e-04  Data: 0.009 (0.020)
Train: 429 [ 250/1251 ( 20%)]  Loss: 3.354 (3.38)  Time: 0.720s, 1421.46/s  (0.701s, 1460.08/s)  LR: 1.955e-04  Data: 0.013 (0.018)
Train: 429 [ 300/1251 ( 24%)]  Loss: 3.149 (3.35)  Time: 0.756s, 1355.01/s  (0.700s, 1462.56/s)  LR: 1.955e-04  Data: 0.010 (0.017)
Train: 429 [ 350/1251 ( 28%)]  Loss: 3.061 (3.31)  Time: 0.674s, 1519.45/s  (0.699s, 1465.99/s)  LR: 1.955e-04  Data: 0.010 (0.016)
Train: 429 [ 400/1251 ( 32%)]  Loss: 3.596 (3.35)  Time: 0.672s, 1523.77/s  (0.697s, 1468.68/s)  LR: 1.955e-04  Data: 0.014 (0.015)
Train: 429 [ 450/1251 ( 36%)]  Loss: 3.581 (3.37)  Time: 0.706s, 1450.25/s  (0.697s, 1469.02/s)  LR: 1.955e-04  Data: 0.010 (0.015)
Train: 429 [ 500/1251 ( 40%)]  Loss: 3.595 (3.39)  Time: 0.700s, 1463.68/s  (0.696s, 1470.52/s)  LR: 1.955e-04  Data: 0.009 (0.014)
Train: 429 [ 550/1251 ( 44%)]  Loss: 3.654 (3.41)  Time: 0.687s, 1491.05/s  (0.697s, 1468.46/s)  LR: 1.955e-04  Data: 0.011 (0.014)
Train: 429 [ 600/1251 ( 48%)]  Loss: 3.409 (3.41)  Time: 0.767s, 1334.70/s  (0.699s, 1465.86/s)  LR: 1.955e-04  Data: 0.009 (0.014)
Train: 429 [ 650/1251 ( 52%)]  Loss: 3.176 (3.39)  Time: 0.727s, 1407.59/s  (0.699s, 1463.97/s)  LR: 1.955e-04  Data: 0.010 (0.014)
Train: 429 [ 700/1251 ( 56%)]  Loss: 3.033 (3.37)  Time: 0.672s, 1524.29/s  (0.700s, 1462.65/s)  LR: 1.955e-04  Data: 0.011 (0.013)
Train: 429 [ 750/1251 ( 60%)]  Loss: 3.505 (3.38)  Time: 0.672s, 1522.68/s  (0.699s, 1464.16/s)  LR: 1.955e-04  Data: 0.010 (0.013)
Train: 429 [ 800/1251 ( 64%)]  Loss: 3.470 (3.38)  Time: 0.672s, 1523.28/s  (0.698s, 1467.27/s)  LR: 1.955e-04  Data: 0.011 (0.013)
Train: 429 [ 850/1251 ( 68%)]  Loss: 3.743 (3.40)  Time: 0.714s, 1434.19/s  (0.697s, 1469.77/s)  LR: 1.955e-04  Data: 0.009 (0.013)
Train: 429 [ 900/1251 ( 72%)]  Loss: 3.206 (3.39)  Time: 0.679s, 1508.20/s  (0.696s, 1470.54/s)  LR: 1.955e-04  Data: 0.010 (0.013)
Train: 429 [ 950/1251 ( 76%)]  Loss: 3.215 (3.38)  Time: 0.708s, 1446.00/s  (0.696s, 1471.16/s)  LR: 1.955e-04  Data: 0.011 (0.013)
Train: 429 [1000/1251 ( 80%)]  Loss: 3.538 (3.39)  Time: 0.705s, 1452.31/s  (0.696s, 1471.44/s)  LR: 1.955e-04  Data: 0.009 (0.012)
Train: 429 [1050/1251 ( 84%)]  Loss: 3.508 (3.40)  Time: 0.709s, 1445.03/s  (0.696s, 1471.65/s)  LR: 1.955e-04  Data: 0.010 (0.012)
Train: 429 [1100/1251 ( 88%)]  Loss: 3.143 (3.39)  Time: 0.668s, 1533.23/s  (0.695s, 1472.86/s)  LR: 1.955e-04  Data: 0.012 (0.012)
Train: 429 [1150/1251 ( 92%)]  Loss: 3.609 (3.40)  Time: 0.680s, 1504.96/s  (0.695s, 1473.56/s)  LR: 1.955e-04  Data: 0.009 (0.012)
Train: 429 [1200/1251 ( 96%)]  Loss: 3.366 (3.39)  Time: 0.672s, 1522.85/s  (0.695s, 1474.32/s)  LR: 1.955e-04  Data: 0.012 (0.012)
Train: 429 [1250/1251 (100%)]  Loss: 3.090 (3.38)  Time: 0.656s, 1561.87/s  (0.694s, 1474.81/s)  LR: 1.955e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.519 (1.519)  Loss:  0.8867 (0.8867)  Acc@1: 90.3320 (90.3320)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.136 (0.575)  Loss:  0.9492 (1.3912)  Acc@1: 85.9670 (76.4200)  Acc@5: 96.6981 (93.2620)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-425.pth.tar', 76.51800018554688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-428.pth.tar', 76.42600000244141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-429.pth.tar', 76.41999998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-419.pth.tar', 76.4080000341797)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-418.pth.tar', 76.37600000976562)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-422.pth.tar', 76.37200005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-426.pth.tar', 76.3420000390625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-407.pth.tar', 76.2320000048828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-423.pth.tar', 76.18199990478516)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-408.pth.tar', 76.14400000732422)

Train: 430 [   0/1251 (  0%)]  Loss: 3.097 (3.10)  Time: 2.030s,  504.35/s  (2.030s,  504.35/s)  LR: 1.935e-04  Data: 1.416 (1.416)
Train: 430 [  50/1251 (  4%)]  Loss: 3.203 (3.15)  Time: 0.673s, 1522.10/s  (0.720s, 1421.39/s)  LR: 1.935e-04  Data: 0.011 (0.047)
Train: 430 [ 100/1251 (  8%)]  Loss: 3.622 (3.31)  Time: 0.672s, 1523.89/s  (0.710s, 1441.79/s)  LR: 1.935e-04  Data: 0.010 (0.029)
Train: 430 [ 150/1251 ( 12%)]  Loss: 3.419 (3.34)  Time: 0.758s, 1351.05/s  (0.707s, 1447.68/s)  LR: 1.935e-04  Data: 0.010 (0.023)
Train: 430 [ 200/1251 ( 16%)]  Loss: 3.406 (3.35)  Time: 0.706s, 1451.40/s  (0.703s, 1456.67/s)  LR: 1.935e-04  Data: 0.009 (0.020)
Train: 430 [ 250/1251 ( 20%)]  Loss: 3.350 (3.35)  Time: 0.704s, 1455.08/s  (0.699s, 1465.17/s)  LR: 1.935e-04  Data: 0.012 (0.018)
Train: 430 [ 300/1251 ( 24%)]  Loss: 3.372 (3.35)  Time: 0.693s, 1478.03/s  (0.698s, 1467.90/s)  LR: 1.935e-04  Data: 0.009 (0.017)
Train: 430 [ 350/1251 ( 28%)]  Loss: 3.242 (3.34)  Time: 0.673s, 1520.51/s  (0.696s, 1470.22/s)  LR: 1.935e-04  Data: 0.010 (0.016)
Train: 430 [ 400/1251 ( 32%)]  Loss: 3.444 (3.35)  Time: 0.711s, 1440.58/s  (0.697s, 1469.92/s)  LR: 1.935e-04  Data: 0.012 (0.015)
Train: 430 [ 450/1251 ( 36%)]  Loss: 3.521 (3.37)  Time: 0.676s, 1514.27/s  (0.696s, 1470.79/s)  LR: 1.935e-04  Data: 0.011 (0.015)
Train: 430 [ 500/1251 ( 40%)]  Loss: 3.587 (3.39)  Time: 0.676s, 1515.22/s  (0.696s, 1471.45/s)  LR: 1.935e-04  Data: 0.010 (0.014)
Train: 430 [ 550/1251 ( 44%)]  Loss: 3.369 (3.39)  Time: 0.677s, 1512.97/s  (0.696s, 1471.19/s)  LR: 1.935e-04  Data: 0.011 (0.014)
Train: 430 [ 600/1251 ( 48%)]  Loss: 3.599 (3.40)  Time: 0.673s, 1521.33/s  (0.696s, 1471.12/s)  LR: 1.935e-04  Data: 0.011 (0.014)
Train: 430 [ 650/1251 ( 52%)]  Loss: 3.339 (3.40)  Time: 0.671s, 1526.04/s  (0.696s, 1471.09/s)  LR: 1.935e-04  Data: 0.011 (0.013)
Train: 430 [ 700/1251 ( 56%)]  Loss: 3.146 (3.38)  Time: 0.673s, 1521.70/s  (0.696s, 1471.74/s)  LR: 1.935e-04  Data: 0.010 (0.013)
Train: 430 [ 750/1251 ( 60%)]  Loss: 3.034 (3.36)  Time: 0.697s, 1468.33/s  (0.695s, 1473.29/s)  LR: 1.935e-04  Data: 0.010 (0.013)
Train: 430 [ 800/1251 ( 64%)]  Loss: 3.272 (3.35)  Time: 0.678s, 1511.44/s  (0.695s, 1473.81/s)  LR: 1.935e-04  Data: 0.010 (0.013)
Train: 430 [ 850/1251 ( 68%)]  Loss: 3.458 (3.36)  Time: 0.665s, 1540.96/s  (0.695s, 1474.21/s)  LR: 1.935e-04  Data: 0.008 (0.013)
Train: 430 [ 900/1251 ( 72%)]  Loss: 3.370 (3.36)  Time: 0.727s, 1408.49/s  (0.694s, 1474.94/s)  LR: 1.935e-04  Data: 0.010 (0.012)
Train: 430 [ 950/1251 ( 76%)]  Loss: 3.391 (3.36)  Time: 0.703s, 1456.22/s  (0.694s, 1475.64/s)  LR: 1.935e-04  Data: 0.010 (0.012)
Train: 430 [1000/1251 ( 80%)]  Loss: 3.666 (3.38)  Time: 0.666s, 1536.89/s  (0.694s, 1476.26/s)  LR: 1.935e-04  Data: 0.010 (0.012)
Train: 430 [1050/1251 ( 84%)]  Loss: 3.389 (3.38)  Time: 0.675s, 1517.54/s  (0.693s, 1476.90/s)  LR: 1.935e-04  Data: 0.011 (0.012)
Train: 430 [1100/1251 ( 88%)]  Loss: 3.449 (3.38)  Time: 0.672s, 1524.21/s  (0.693s, 1477.62/s)  LR: 1.935e-04  Data: 0.009 (0.012)
Train: 430 [1150/1251 ( 92%)]  Loss: 3.482 (3.38)  Time: 0.703s, 1456.72/s  (0.693s, 1477.35/s)  LR: 1.935e-04  Data: 0.010 (0.012)
Train: 430 [1200/1251 ( 96%)]  Loss: 3.323 (3.38)  Time: 0.681s, 1503.42/s  (0.693s, 1477.92/s)  LR: 1.935e-04  Data: 0.011 (0.012)
Train: 430 [1250/1251 (100%)]  Loss: 3.152 (3.37)  Time: 0.657s, 1558.86/s  (0.693s, 1478.26/s)  LR: 1.935e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.485 (1.485)  Loss:  0.7476 (0.7476)  Acc@1: 90.8203 (90.8203)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.136 (0.573)  Loss:  0.8696 (1.2805)  Acc@1: 85.7311 (76.6440)  Acc@5: 96.4623 (93.1660)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-430.pth.tar', 76.64400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-425.pth.tar', 76.51800018554688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-428.pth.tar', 76.42600000244141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-429.pth.tar', 76.41999998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-419.pth.tar', 76.4080000341797)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-418.pth.tar', 76.37600000976562)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-422.pth.tar', 76.37200005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-426.pth.tar', 76.3420000390625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-407.pth.tar', 76.2320000048828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-423.pth.tar', 76.18199990478516)

Train: 431 [   0/1251 (  0%)]  Loss: 3.344 (3.34)  Time: 2.170s,  471.86/s  (2.170s,  471.86/s)  LR: 1.915e-04  Data: 1.530 (1.530)
Train: 431 [  50/1251 (  4%)]  Loss: 3.826 (3.58)  Time: 0.670s, 1528.16/s  (0.726s, 1410.76/s)  LR: 1.915e-04  Data: 0.010 (0.049)
Train: 431 [ 100/1251 (  8%)]  Loss: 3.628 (3.60)  Time: 0.704s, 1453.83/s  (0.711s, 1440.42/s)  LR: 1.915e-04  Data: 0.009 (0.030)
Train: 431 [ 150/1251 ( 12%)]  Loss: 3.345 (3.54)  Time: 0.693s, 1477.48/s  (0.705s, 1452.71/s)  LR: 1.915e-04  Data: 0.009 (0.023)
Train: 431 [ 200/1251 ( 16%)]  Loss: 3.791 (3.59)  Time: 0.686s, 1493.11/s  (0.700s, 1462.40/s)  LR: 1.915e-04  Data: 0.013 (0.020)
Train: 431 [ 250/1251 ( 20%)]  Loss: 3.517 (3.58)  Time: 0.684s, 1497.12/s  (0.699s, 1464.31/s)  LR: 1.915e-04  Data: 0.011 (0.018)
Train: 431 [ 300/1251 ( 24%)]  Loss: 3.270 (3.53)  Time: 0.684s, 1497.08/s  (0.697s, 1469.57/s)  LR: 1.915e-04  Data: 0.013 (0.017)
Train: 431 [ 350/1251 ( 28%)]  Loss: 3.506 (3.53)  Time: 0.735s, 1393.10/s  (0.697s, 1468.40/s)  LR: 1.915e-04  Data: 0.009 (0.016)
Train: 431 [ 400/1251 ( 32%)]  Loss: 3.412 (3.52)  Time: 0.675s, 1516.43/s  (0.696s, 1470.30/s)  LR: 1.915e-04  Data: 0.012 (0.015)
Train: 431 [ 450/1251 ( 36%)]  Loss: 3.342 (3.50)  Time: 0.760s, 1346.90/s  (0.696s, 1471.71/s)  LR: 1.915e-04  Data: 0.009 (0.015)
Train: 431 [ 500/1251 ( 40%)]  Loss: 3.459 (3.49)  Time: 0.675s, 1516.45/s  (0.695s, 1472.66/s)  LR: 1.915e-04  Data: 0.011 (0.014)
Train: 431 [ 550/1251 ( 44%)]  Loss: 3.609 (3.50)  Time: 0.755s, 1355.49/s  (0.695s, 1473.79/s)  LR: 1.915e-04  Data: 0.009 (0.014)
Train: 431 [ 600/1251 ( 48%)]  Loss: 3.326 (3.49)  Time: 0.672s, 1524.56/s  (0.695s, 1474.00/s)  LR: 1.915e-04  Data: 0.011 (0.014)
Train: 431 [ 650/1251 ( 52%)]  Loss: 3.419 (3.49)  Time: 0.671s, 1526.13/s  (0.694s, 1475.53/s)  LR: 1.915e-04  Data: 0.009 (0.014)
Train: 431 [ 700/1251 ( 56%)]  Loss: 3.269 (3.47)  Time: 0.700s, 1462.05/s  (0.694s, 1476.21/s)  LR: 1.915e-04  Data: 0.009 (0.013)
Train: 431 [ 750/1251 ( 60%)]  Loss: 3.420 (3.47)  Time: 0.671s, 1525.09/s  (0.694s, 1476.45/s)  LR: 1.915e-04  Data: 0.009 (0.013)
Train: 431 [ 800/1251 ( 64%)]  Loss: 3.606 (3.48)  Time: 0.697s, 1468.74/s  (0.693s, 1477.19/s)  LR: 1.915e-04  Data: 0.012 (0.013)
Train: 431 [ 850/1251 ( 68%)]  Loss: 3.644 (3.49)  Time: 0.672s, 1522.88/s  (0.693s, 1477.05/s)  LR: 1.915e-04  Data: 0.010 (0.013)
Train: 431 [ 900/1251 ( 72%)]  Loss: 3.574 (3.49)  Time: 0.713s, 1436.10/s  (0.693s, 1477.44/s)  LR: 1.915e-04  Data: 0.009 (0.013)
Train: 431 [ 950/1251 ( 76%)]  Loss: 3.301 (3.48)  Time: 0.702s, 1458.58/s  (0.693s, 1477.68/s)  LR: 1.915e-04  Data: 0.010 (0.013)
Train: 431 [1000/1251 ( 80%)]  Loss: 3.299 (3.47)  Time: 0.690s, 1483.05/s  (0.693s, 1477.39/s)  LR: 1.915e-04  Data: 0.016 (0.012)
Train: 431 [1050/1251 ( 84%)]  Loss: 3.463 (3.47)  Time: 0.703s, 1455.80/s  (0.693s, 1476.90/s)  LR: 1.915e-04  Data: 0.009 (0.012)
Train: 431 [1100/1251 ( 88%)]  Loss: 3.649 (3.48)  Time: 0.687s, 1489.65/s  (0.694s, 1475.92/s)  LR: 1.915e-04  Data: 0.013 (0.012)
Train: 431 [1150/1251 ( 92%)]  Loss: 3.257 (3.47)  Time: 0.705s, 1453.07/s  (0.694s, 1476.14/s)  LR: 1.915e-04  Data: 0.010 (0.012)
Train: 431 [1200/1251 ( 96%)]  Loss: 3.432 (3.47)  Time: 0.673s, 1520.46/s  (0.694s, 1475.97/s)  LR: 1.915e-04  Data: 0.009 (0.012)
Train: 431 [1250/1251 (100%)]  Loss: 3.670 (3.48)  Time: 0.657s, 1558.02/s  (0.694s, 1476.26/s)  LR: 1.915e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.610 (1.610)  Loss:  0.7505 (0.7505)  Acc@1: 90.8203 (90.8203)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.136 (0.584)  Loss:  0.9214 (1.2905)  Acc@1: 84.7877 (76.4160)  Acc@5: 96.3443 (93.0420)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-430.pth.tar', 76.64400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-425.pth.tar', 76.51800018554688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-428.pth.tar', 76.42600000244141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-429.pth.tar', 76.41999998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-431.pth.tar', 76.41599998535156)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-419.pth.tar', 76.4080000341797)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-418.pth.tar', 76.37600000976562)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-422.pth.tar', 76.37200005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-426.pth.tar', 76.3420000390625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-407.pth.tar', 76.2320000048828)

Train: 432 [   0/1251 (  0%)]  Loss: 3.432 (3.43)  Time: 2.151s,  475.99/s  (2.151s,  475.99/s)  LR: 1.895e-04  Data: 1.495 (1.495)
Train: 432 [  50/1251 (  4%)]  Loss: 3.370 (3.40)  Time: 0.679s, 1507.05/s  (0.726s, 1410.23/s)  LR: 1.895e-04  Data: 0.011 (0.047)
Train: 432 [ 100/1251 (  8%)]  Loss: 3.907 (3.57)  Time: 0.673s, 1522.42/s  (0.712s, 1439.16/s)  LR: 1.895e-04  Data: 0.011 (0.029)
Train: 432 [ 150/1251 ( 12%)]  Loss: 3.633 (3.59)  Time: 0.710s, 1443.27/s  (0.702s, 1458.44/s)  LR: 1.895e-04  Data: 0.010 (0.023)
Train: 432 [ 200/1251 ( 16%)]  Loss: 3.574 (3.58)  Time: 0.673s, 1521.48/s  (0.698s, 1466.20/s)  LR: 1.895e-04  Data: 0.011 (0.020)
Train: 432 [ 250/1251 ( 20%)]  Loss: 3.279 (3.53)  Time: 0.680s, 1506.95/s  (0.697s, 1470.11/s)  LR: 1.895e-04  Data: 0.011 (0.018)
Train: 432 [ 300/1251 ( 24%)]  Loss: 3.494 (3.53)  Time: 0.692s, 1480.03/s  (0.696s, 1471.85/s)  LR: 1.895e-04  Data: 0.012 (0.017)
Train: 432 [ 350/1251 ( 28%)]  Loss: 3.493 (3.52)  Time: 0.666s, 1536.65/s  (0.695s, 1474.20/s)  LR: 1.895e-04  Data: 0.010 (0.016)
Train: 432 [ 400/1251 ( 32%)]  Loss: 3.323 (3.50)  Time: 0.710s, 1442.68/s  (0.695s, 1473.93/s)  LR: 1.895e-04  Data: 0.010 (0.015)
Train: 432 [ 450/1251 ( 36%)]  Loss: 3.493 (3.50)  Time: 0.708s, 1446.04/s  (0.695s, 1472.47/s)  LR: 1.895e-04  Data: 0.009 (0.015)
Train: 432 [ 500/1251 ( 40%)]  Loss: 3.614 (3.51)  Time: 0.713s, 1436.26/s  (0.695s, 1472.55/s)  LR: 1.895e-04  Data: 0.013 (0.014)
Train: 432 [ 550/1251 ( 44%)]  Loss: 3.451 (3.51)  Time: 0.703s, 1457.39/s  (0.695s, 1473.25/s)  LR: 1.895e-04  Data: 0.012 (0.014)
Train: 432 [ 600/1251 ( 48%)]  Loss: 3.544 (3.51)  Time: 0.703s, 1455.88/s  (0.695s, 1473.28/s)  LR: 1.895e-04  Data: 0.011 (0.014)
Train: 432 [ 650/1251 ( 52%)]  Loss: 3.571 (3.51)  Time: 0.667s, 1535.19/s  (0.695s, 1473.14/s)  LR: 1.895e-04  Data: 0.011 (0.013)
Train: 432 [ 700/1251 ( 56%)]  Loss: 3.061 (3.48)  Time: 0.702s, 1458.00/s  (0.695s, 1474.02/s)  LR: 1.895e-04  Data: 0.009 (0.013)
Train: 432 [ 750/1251 ( 60%)]  Loss: 3.485 (3.48)  Time: 0.697s, 1468.52/s  (0.694s, 1474.82/s)  LR: 1.895e-04  Data: 0.011 (0.013)
Train: 432 [ 800/1251 ( 64%)]  Loss: 3.330 (3.47)  Time: 0.701s, 1461.78/s  (0.694s, 1475.52/s)  LR: 1.895e-04  Data: 0.011 (0.013)
Train: 432 [ 850/1251 ( 68%)]  Loss: 3.455 (3.47)  Time: 0.681s, 1503.23/s  (0.694s, 1476.29/s)  LR: 1.895e-04  Data: 0.011 (0.013)
Train: 432 [ 900/1251 ( 72%)]  Loss: 3.555 (3.48)  Time: 0.670s, 1527.47/s  (0.693s, 1476.84/s)  LR: 1.895e-04  Data: 0.010 (0.012)
Train: 432 [ 950/1251 ( 76%)]  Loss: 3.490 (3.48)  Time: 0.671s, 1526.65/s  (0.693s, 1476.80/s)  LR: 1.895e-04  Data: 0.010 (0.012)
Train: 432 [1000/1251 ( 80%)]  Loss: 3.271 (3.47)  Time: 0.674s, 1519.11/s  (0.693s, 1477.08/s)  LR: 1.895e-04  Data: 0.010 (0.012)
Train: 432 [1050/1251 ( 84%)]  Loss: 3.687 (3.48)  Time: 0.706s, 1451.21/s  (0.693s, 1476.77/s)  LR: 1.895e-04  Data: 0.010 (0.012)
Train: 432 [1100/1251 ( 88%)]  Loss: 3.279 (3.47)  Time: 0.665s, 1540.07/s  (0.693s, 1477.03/s)  LR: 1.895e-04  Data: 0.010 (0.012)
Train: 432 [1150/1251 ( 92%)]  Loss: 3.198 (3.46)  Time: 0.674s, 1518.99/s  (0.693s, 1476.97/s)  LR: 1.895e-04  Data: 0.011 (0.012)
Train: 432 [1200/1251 ( 96%)]  Loss: 3.757 (3.47)  Time: 0.705s, 1451.86/s  (0.693s, 1477.77/s)  LR: 1.895e-04  Data: 0.011 (0.012)
Train: 432 [1250/1251 (100%)]  Loss: 3.494 (3.47)  Time: 0.657s, 1558.17/s  (0.693s, 1477.61/s)  LR: 1.895e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.598 (1.598)  Loss:  0.7246 (0.7246)  Acc@1: 90.6250 (90.6250)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.136 (0.578)  Loss:  0.8008 (1.2493)  Acc@1: 85.4953 (76.6300)  Acc@5: 96.1085 (93.2660)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-430.pth.tar', 76.64400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-432.pth.tar', 76.63000003417969)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-425.pth.tar', 76.51800018554688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-428.pth.tar', 76.42600000244141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-429.pth.tar', 76.41999998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-431.pth.tar', 76.41599998535156)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-419.pth.tar', 76.4080000341797)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-418.pth.tar', 76.37600000976562)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-422.pth.tar', 76.37200005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-426.pth.tar', 76.3420000390625)

Train: 433 [   0/1251 (  0%)]  Loss: 3.353 (3.35)  Time: 2.123s,  482.34/s  (2.123s,  482.34/s)  LR: 1.875e-04  Data: 1.507 (1.507)
Train: 433 [  50/1251 (  4%)]  Loss: 3.282 (3.32)  Time: 0.672s, 1524.11/s  (0.721s, 1419.99/s)  LR: 1.875e-04  Data: 0.009 (0.044)
Train: 433 [ 100/1251 (  8%)]  Loss: 3.587 (3.41)  Time: 0.697s, 1469.96/s  (0.714s, 1434.43/s)  LR: 1.875e-04  Data: 0.009 (0.028)
Train: 433 [ 150/1251 ( 12%)]  Loss: 3.691 (3.48)  Time: 0.699s, 1464.91/s  (0.707s, 1449.10/s)  LR: 1.875e-04  Data: 0.009 (0.022)
Train: 433 [ 200/1251 ( 16%)]  Loss: 3.290 (3.44)  Time: 0.666s, 1537.37/s  (0.702s, 1458.56/s)  LR: 1.875e-04  Data: 0.010 (0.019)
Train: 433 [ 250/1251 ( 20%)]  Loss: 3.256 (3.41)  Time: 0.688s, 1488.22/s  (0.700s, 1463.69/s)  LR: 1.875e-04  Data: 0.009 (0.017)
Train: 433 [ 300/1251 ( 24%)]  Loss: 3.453 (3.42)  Time: 0.723s, 1416.44/s  (0.698s, 1466.21/s)  LR: 1.875e-04  Data: 0.011 (0.016)
Train: 433 [ 350/1251 ( 28%)]  Loss: 3.487 (3.42)  Time: 0.672s, 1522.91/s  (0.698s, 1466.49/s)  LR: 1.875e-04  Data: 0.010 (0.016)
Train: 433 [ 400/1251 ( 32%)]  Loss: 3.635 (3.45)  Time: 0.718s, 1426.62/s  (0.697s, 1468.79/s)  LR: 1.875e-04  Data: 0.014 (0.015)
Train: 433 [ 450/1251 ( 36%)]  Loss: 3.320 (3.44)  Time: 0.671s, 1524.96/s  (0.697s, 1469.55/s)  LR: 1.875e-04  Data: 0.010 (0.014)
Train: 433 [ 500/1251 ( 40%)]  Loss: 3.635 (3.45)  Time: 0.677s, 1512.58/s  (0.697s, 1469.03/s)  LR: 1.875e-04  Data: 0.011 (0.014)
Train: 433 [ 550/1251 ( 44%)]  Loss: 3.203 (3.43)  Time: 0.683s, 1498.79/s  (0.696s, 1470.89/s)  LR: 1.875e-04  Data: 0.009 (0.014)
Train: 433 [ 600/1251 ( 48%)]  Loss: 3.501 (3.44)  Time: 0.670s, 1527.74/s  (0.696s, 1471.15/s)  LR: 1.875e-04  Data: 0.011 (0.013)
Train: 433 [ 650/1251 ( 52%)]  Loss: 3.698 (3.46)  Time: 0.671s, 1525.69/s  (0.696s, 1472.30/s)  LR: 1.875e-04  Data: 0.012 (0.013)
Train: 433 [ 700/1251 ( 56%)]  Loss: 3.574 (3.46)  Time: 0.671s, 1527.07/s  (0.695s, 1472.36/s)  LR: 1.875e-04  Data: 0.009 (0.013)
Train: 433 [ 750/1251 ( 60%)]  Loss: 3.372 (3.46)  Time: 0.670s, 1528.13/s  (0.695s, 1473.14/s)  LR: 1.875e-04  Data: 0.009 (0.013)
Train: 433 [ 800/1251 ( 64%)]  Loss: 3.426 (3.46)  Time: 0.765s, 1338.49/s  (0.695s, 1473.45/s)  LR: 1.875e-04  Data: 0.010 (0.013)
Train: 433 [ 850/1251 ( 68%)]  Loss: 3.532 (3.46)  Time: 0.672s, 1522.96/s  (0.695s, 1473.44/s)  LR: 1.875e-04  Data: 0.009 (0.013)
Train: 433 [ 900/1251 ( 72%)]  Loss: 3.941 (3.49)  Time: 0.706s, 1450.64/s  (0.695s, 1474.30/s)  LR: 1.875e-04  Data: 0.010 (0.012)
Train: 433 [ 950/1251 ( 76%)]  Loss: 3.486 (3.49)  Time: 0.714s, 1434.68/s  (0.695s, 1474.44/s)  LR: 1.875e-04  Data: 0.009 (0.012)
Train: 433 [1000/1251 ( 80%)]  Loss: 3.384 (3.48)  Time: 0.702s, 1459.16/s  (0.694s, 1475.46/s)  LR: 1.875e-04  Data: 0.010 (0.012)
Train: 433 [1050/1251 ( 84%)]  Loss: 3.246 (3.47)  Time: 0.706s, 1450.29/s  (0.695s, 1474.44/s)  LR: 1.875e-04  Data: 0.011 (0.012)
Train: 433 [1100/1251 ( 88%)]  Loss: 3.531 (3.47)  Time: 0.709s, 1444.17/s  (0.695s, 1473.67/s)  LR: 1.875e-04  Data: 0.010 (0.012)
Train: 433 [1150/1251 ( 92%)]  Loss: 3.386 (3.47)  Time: 0.753s, 1359.36/s  (0.695s, 1473.39/s)  LR: 1.875e-04  Data: 0.010 (0.012)
Train: 433 [1200/1251 ( 96%)]  Loss: 3.730 (3.48)  Time: 0.674s, 1518.76/s  (0.695s, 1472.99/s)  LR: 1.875e-04  Data: 0.010 (0.012)
Train: 433 [1250/1251 (100%)]  Loss: 3.516 (3.48)  Time: 0.665s, 1539.95/s  (0.695s, 1473.58/s)  LR: 1.875e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.572 (1.572)  Loss:  0.7847 (0.7847)  Acc@1: 91.5039 (91.5039)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.136 (0.589)  Loss:  0.8525 (1.2681)  Acc@1: 84.9057 (76.6480)  Acc@5: 96.1085 (93.2440)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-433.pth.tar', 76.6480000366211)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-430.pth.tar', 76.64400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-432.pth.tar', 76.63000003417969)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-425.pth.tar', 76.51800018554688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-428.pth.tar', 76.42600000244141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-429.pth.tar', 76.41999998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-431.pth.tar', 76.41599998535156)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-419.pth.tar', 76.4080000341797)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-418.pth.tar', 76.37600000976562)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-422.pth.tar', 76.37200005615234)

Train: 434 [   0/1251 (  0%)]  Loss: 3.514 (3.51)  Time: 2.297s,  445.75/s  (2.297s,  445.75/s)  LR: 1.855e-04  Data: 1.682 (1.682)
Train: 434 [  50/1251 (  4%)]  Loss: 3.338 (3.43)  Time: 0.718s, 1425.21/s  (0.726s, 1411.15/s)  LR: 1.855e-04  Data: 0.010 (0.049)
Train: 434 [ 100/1251 (  8%)]  Loss: 3.593 (3.48)  Time: 0.672s, 1523.58/s  (0.708s, 1446.43/s)  LR: 1.855e-04  Data: 0.013 (0.030)
Train: 434 [ 150/1251 ( 12%)]  Loss: 3.487 (3.48)  Time: 0.702s, 1458.63/s  (0.702s, 1459.31/s)  LR: 1.855e-04  Data: 0.010 (0.023)
Train: 434 [ 200/1251 ( 16%)]  Loss: 3.786 (3.54)  Time: 0.720s, 1422.06/s  (0.698s, 1466.51/s)  LR: 1.855e-04  Data: 0.011 (0.020)
Train: 434 [ 250/1251 ( 20%)]  Loss: 3.005 (3.45)  Time: 0.672s, 1524.57/s  (0.697s, 1469.90/s)  LR: 1.855e-04  Data: 0.011 (0.018)
Train: 434 [ 300/1251 ( 24%)]  Loss: 3.666 (3.48)  Time: 0.678s, 1510.55/s  (0.697s, 1469.98/s)  LR: 1.855e-04  Data: 0.011 (0.017)
Train: 434 [ 350/1251 ( 28%)]  Loss: 3.972 (3.55)  Time: 0.673s, 1521.40/s  (0.696s, 1472.23/s)  LR: 1.855e-04  Data: 0.011 (0.016)
Train: 434 [ 400/1251 ( 32%)]  Loss: 3.212 (3.51)  Time: 0.677s, 1512.49/s  (0.695s, 1473.41/s)  LR: 1.855e-04  Data: 0.011 (0.015)
Train: 434 [ 450/1251 ( 36%)]  Loss: 3.570 (3.51)  Time: 0.716s, 1429.78/s  (0.695s, 1473.37/s)  LR: 1.855e-04  Data: 0.011 (0.015)
Train: 434 [ 500/1251 ( 40%)]  Loss: 3.225 (3.49)  Time: 0.664s, 1543.08/s  (0.694s, 1474.76/s)  LR: 1.855e-04  Data: 0.009 (0.014)
Train: 434 [ 550/1251 ( 44%)]  Loss: 3.339 (3.48)  Time: 0.721s, 1421.05/s  (0.694s, 1476.28/s)  LR: 1.855e-04  Data: 0.010 (0.014)
Train: 434 [ 600/1251 ( 48%)]  Loss: 3.308 (3.46)  Time: 0.673s, 1521.79/s  (0.694s, 1476.42/s)  LR: 1.855e-04  Data: 0.011 (0.014)
Train: 434 [ 650/1251 ( 52%)]  Loss: 3.532 (3.47)  Time: 0.706s, 1449.55/s  (0.694s, 1474.62/s)  LR: 1.855e-04  Data: 0.009 (0.013)
Train: 434 [ 700/1251 ( 56%)]  Loss: 3.154 (3.45)  Time: 0.690s, 1484.37/s  (0.694s, 1475.58/s)  LR: 1.855e-04  Data: 0.011 (0.013)
Train: 434 [ 750/1251 ( 60%)]  Loss: 3.657 (3.46)  Time: 0.672s, 1523.93/s  (0.694s, 1476.20/s)  LR: 1.855e-04  Data: 0.011 (0.013)
Train: 434 [ 800/1251 ( 64%)]  Loss: 3.535 (3.46)  Time: 0.702s, 1459.50/s  (0.694s, 1476.56/s)  LR: 1.855e-04  Data: 0.009 (0.013)
Train: 434 [ 850/1251 ( 68%)]  Loss: 3.418 (3.46)  Time: 0.674s, 1520.07/s  (0.693s, 1477.22/s)  LR: 1.855e-04  Data: 0.010 (0.013)
Train: 434 [ 900/1251 ( 72%)]  Loss: 3.284 (3.45)  Time: 0.712s, 1437.53/s  (0.693s, 1477.85/s)  LR: 1.855e-04  Data: 0.009 (0.012)
Train: 434 [ 950/1251 ( 76%)]  Loss: 3.487 (3.45)  Time: 0.717s, 1427.64/s  (0.693s, 1478.18/s)  LR: 1.855e-04  Data: 0.009 (0.012)
Train: 434 [1000/1251 ( 80%)]  Loss: 3.487 (3.46)  Time: 0.683s, 1498.17/s  (0.693s, 1478.03/s)  LR: 1.855e-04  Data: 0.011 (0.012)
Train: 434 [1050/1251 ( 84%)]  Loss: 3.354 (3.45)  Time: 0.674s, 1520.06/s  (0.693s, 1477.82/s)  LR: 1.855e-04  Data: 0.010 (0.012)
Train: 434 [1100/1251 ( 88%)]  Loss: 3.387 (3.45)  Time: 0.673s, 1521.82/s  (0.693s, 1478.31/s)  LR: 1.855e-04  Data: 0.010 (0.012)
Train: 434 [1150/1251 ( 92%)]  Loss: 3.713 (3.46)  Time: 0.674s, 1519.65/s  (0.693s, 1478.46/s)  LR: 1.855e-04  Data: 0.011 (0.012)
Train: 434 [1200/1251 ( 96%)]  Loss: 3.363 (3.46)  Time: 0.678s, 1510.24/s  (0.692s, 1478.82/s)  LR: 1.855e-04  Data: 0.012 (0.012)
Train: 434 [1250/1251 (100%)]  Loss: 3.422 (3.45)  Time: 0.712s, 1438.64/s  (0.692s, 1478.80/s)  LR: 1.855e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.571 (1.571)  Loss:  0.8179 (0.8179)  Acc@1: 90.3320 (90.3320)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.137 (0.582)  Loss:  0.9370 (1.3151)  Acc@1: 84.6698 (76.3880)  Acc@5: 96.2264 (93.1860)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-433.pth.tar', 76.6480000366211)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-430.pth.tar', 76.64400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-432.pth.tar', 76.63000003417969)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-425.pth.tar', 76.51800018554688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-428.pth.tar', 76.42600000244141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-429.pth.tar', 76.41999998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-431.pth.tar', 76.41599998535156)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-419.pth.tar', 76.4080000341797)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-434.pth.tar', 76.38799993408203)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-418.pth.tar', 76.37600000976562)

Train: 435 [   0/1251 (  0%)]  Loss: 3.590 (3.59)  Time: 2.171s,  471.66/s  (2.171s,  471.66/s)  LR: 1.835e-04  Data: 1.555 (1.555)
Train: 435 [  50/1251 (  4%)]  Loss: 3.447 (3.52)  Time: 0.667s, 1535.37/s  (0.734s, 1395.10/s)  LR: 1.835e-04  Data: 0.011 (0.053)
Train: 435 [ 100/1251 (  8%)]  Loss: 3.246 (3.43)  Time: 0.701s, 1459.99/s  (0.711s, 1439.69/s)  LR: 1.835e-04  Data: 0.009 (0.032)
Train: 435 [ 150/1251 ( 12%)]  Loss: 3.382 (3.42)  Time: 0.667s, 1535.49/s  (0.705s, 1453.42/s)  LR: 1.835e-04  Data: 0.011 (0.025)
Train: 435 [ 200/1251 ( 16%)]  Loss: 3.358 (3.40)  Time: 0.700s, 1462.80/s  (0.700s, 1462.19/s)  LR: 1.835e-04  Data: 0.010 (0.021)
Train: 435 [ 250/1251 ( 20%)]  Loss: 3.338 (3.39)  Time: 0.671s, 1525.30/s  (0.699s, 1464.13/s)  LR: 1.835e-04  Data: 0.009 (0.019)
Train: 435 [ 300/1251 ( 24%)]  Loss: 3.576 (3.42)  Time: 0.704s, 1454.65/s  (0.698s, 1467.02/s)  LR: 1.835e-04  Data: 0.009 (0.018)
Train: 435 [ 350/1251 ( 28%)]  Loss: 3.030 (3.37)  Time: 0.714s, 1434.11/s  (0.697s, 1468.67/s)  LR: 1.835e-04  Data: 0.008 (0.016)
Train: 435 [ 400/1251 ( 32%)]  Loss: 3.177 (3.35)  Time: 0.699s, 1464.17/s  (0.697s, 1469.68/s)  LR: 1.835e-04  Data: 0.010 (0.016)
Train: 435 [ 450/1251 ( 36%)]  Loss: 3.431 (3.36)  Time: 0.718s, 1425.36/s  (0.697s, 1469.38/s)  LR: 1.835e-04  Data: 0.010 (0.015)
Train: 435 [ 500/1251 ( 40%)]  Loss: 3.768 (3.39)  Time: 0.667s, 1534.39/s  (0.696s, 1471.56/s)  LR: 1.835e-04  Data: 0.012 (0.015)
Train: 435 [ 550/1251 ( 44%)]  Loss: 3.488 (3.40)  Time: 0.706s, 1450.14/s  (0.696s, 1471.21/s)  LR: 1.835e-04  Data: 0.011 (0.014)
Train: 435 [ 600/1251 ( 48%)]  Loss: 3.308 (3.40)  Time: 0.706s, 1450.22/s  (0.696s, 1472.32/s)  LR: 1.835e-04  Data: 0.015 (0.014)
Train: 435 [ 650/1251 ( 52%)]  Loss: 3.705 (3.42)  Time: 0.671s, 1525.84/s  (0.695s, 1472.89/s)  LR: 1.835e-04  Data: 0.010 (0.014)
Train: 435 [ 700/1251 ( 56%)]  Loss: 3.378 (3.41)  Time: 0.725s, 1412.49/s  (0.695s, 1472.41/s)  LR: 1.835e-04  Data: 0.009 (0.013)
Train: 435 [ 750/1251 ( 60%)]  Loss: 3.409 (3.41)  Time: 0.680s, 1506.90/s  (0.695s, 1473.68/s)  LR: 1.835e-04  Data: 0.011 (0.013)
Train: 435 [ 800/1251 ( 64%)]  Loss: 3.653 (3.43)  Time: 0.672s, 1523.44/s  (0.694s, 1474.66/s)  LR: 1.835e-04  Data: 0.010 (0.013)
Train: 435 [ 850/1251 ( 68%)]  Loss: 3.401 (3.43)  Time: 0.704s, 1454.79/s  (0.695s, 1473.01/s)  LR: 1.835e-04  Data: 0.009 (0.013)
Train: 435 [ 900/1251 ( 72%)]  Loss: 3.423 (3.43)  Time: 0.755s, 1357.07/s  (0.695s, 1473.22/s)  LR: 1.835e-04  Data: 0.009 (0.013)
Train: 435 [ 950/1251 ( 76%)]  Loss: 3.370 (3.42)  Time: 0.703s, 1456.26/s  (0.695s, 1472.77/s)  LR: 1.835e-04  Data: 0.009 (0.013)
Train: 435 [1000/1251 ( 80%)]  Loss: 3.646 (3.43)  Time: 0.671s, 1526.21/s  (0.695s, 1473.14/s)  LR: 1.835e-04  Data: 0.010 (0.013)
Train: 435 [1050/1251 ( 84%)]  Loss: 3.162 (3.42)  Time: 0.679s, 1507.84/s  (0.695s, 1472.97/s)  LR: 1.835e-04  Data: 0.009 (0.012)
Train: 435 [1100/1251 ( 88%)]  Loss: 3.776 (3.44)  Time: 0.703s, 1457.61/s  (0.695s, 1473.88/s)  LR: 1.835e-04  Data: 0.009 (0.012)
Train: 435 [1150/1251 ( 92%)]  Loss: 3.230 (3.43)  Time: 0.714s, 1434.30/s  (0.695s, 1473.91/s)  LR: 1.835e-04  Data: 0.010 (0.012)
Train: 435 [1200/1251 ( 96%)]  Loss: 3.551 (3.43)  Time: 0.709s, 1444.90/s  (0.695s, 1473.77/s)  LR: 1.835e-04  Data: 0.009 (0.012)
Train: 435 [1250/1251 (100%)]  Loss: 3.209 (3.43)  Time: 0.694s, 1474.46/s  (0.695s, 1473.79/s)  LR: 1.835e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.492 (1.492)  Loss:  0.7324 (0.7324)  Acc@1: 91.0156 (91.0156)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.136 (0.578)  Loss:  0.8838 (1.3126)  Acc@1: 86.0849 (76.3480)  Acc@5: 96.5802 (93.1940)
Train: 436 [   0/1251 (  0%)]  Loss: 3.499 (3.50)  Time: 2.134s,  479.74/s  (2.134s,  479.74/s)  LR: 1.816e-04  Data: 1.518 (1.518)
Train: 436 [  50/1251 (  4%)]  Loss: 3.755 (3.63)  Time: 0.720s, 1423.05/s  (0.736s, 1391.12/s)  LR: 1.816e-04  Data: 0.009 (0.051)
Train: 436 [ 100/1251 (  8%)]  Loss: 3.223 (3.49)  Time: 0.744s, 1376.21/s  (0.717s, 1428.09/s)  LR: 1.816e-04  Data: 0.015 (0.031)
Train: 436 [ 150/1251 ( 12%)]  Loss: 3.403 (3.47)  Time: 0.715s, 1432.06/s  (0.712s, 1438.27/s)  LR: 1.816e-04  Data: 0.009 (0.024)
Train: 436 [ 200/1251 ( 16%)]  Loss: 3.327 (3.44)  Time: 0.705s, 1451.54/s  (0.708s, 1445.40/s)  LR: 1.816e-04  Data: 0.010 (0.021)
Train: 436 [ 250/1251 ( 20%)]  Loss: 3.473 (3.45)  Time: 0.672s, 1524.37/s  (0.705s, 1452.64/s)  LR: 1.816e-04  Data: 0.010 (0.019)
Train: 436 [ 300/1251 ( 24%)]  Loss: 3.601 (3.47)  Time: 0.672s, 1523.22/s  (0.703s, 1456.45/s)  LR: 1.816e-04  Data: 0.011 (0.017)
Train: 436 [ 350/1251 ( 28%)]  Loss: 3.744 (3.50)  Time: 0.695s, 1472.75/s  (0.701s, 1460.28/s)  LR: 1.816e-04  Data: 0.009 (0.016)
Train: 436 [ 400/1251 ( 32%)]  Loss: 3.244 (3.47)  Time: 0.672s, 1523.72/s  (0.701s, 1461.18/s)  LR: 1.816e-04  Data: 0.011 (0.015)
Train: 436 [ 450/1251 ( 36%)]  Loss: 3.487 (3.48)  Time: 0.702s, 1458.76/s  (0.700s, 1461.93/s)  LR: 1.816e-04  Data: 0.009 (0.015)
Train: 436 [ 500/1251 ( 40%)]  Loss: 3.561 (3.48)  Time: 0.731s, 1400.74/s  (0.699s, 1464.32/s)  LR: 1.816e-04  Data: 0.011 (0.014)
Train: 436 [ 550/1251 ( 44%)]  Loss: 3.525 (3.49)  Time: 0.688s, 1487.56/s  (0.700s, 1463.44/s)  LR: 1.816e-04  Data: 0.010 (0.014)
Train: 436 [ 600/1251 ( 48%)]  Loss: 3.143 (3.46)  Time: 0.674s, 1519.83/s  (0.698s, 1466.16/s)  LR: 1.816e-04  Data: 0.011 (0.014)
Train: 436 [ 650/1251 ( 52%)]  Loss: 3.403 (3.46)  Time: 0.671s, 1526.35/s  (0.698s, 1466.94/s)  LR: 1.816e-04  Data: 0.010 (0.013)
Train: 436 [ 700/1251 ( 56%)]  Loss: 3.491 (3.46)  Time: 0.691s, 1482.94/s  (0.698s, 1466.54/s)  LR: 1.816e-04  Data: 0.010 (0.013)
Train: 436 [ 750/1251 ( 60%)]  Loss: 3.385 (3.45)  Time: 0.700s, 1462.38/s  (0.698s, 1467.30/s)  LR: 1.816e-04  Data: 0.010 (0.013)
Train: 436 [ 800/1251 ( 64%)]  Loss: 3.659 (3.47)  Time: 0.675s, 1517.87/s  (0.698s, 1467.96/s)  LR: 1.816e-04  Data: 0.009 (0.013)
Train: 436 [ 850/1251 ( 68%)]  Loss: 3.373 (3.46)  Time: 0.666s, 1538.23/s  (0.697s, 1468.75/s)  LR: 1.816e-04  Data: 0.010 (0.013)
Train: 436 [ 900/1251 ( 72%)]  Loss: 3.101 (3.44)  Time: 0.708s, 1445.70/s  (0.697s, 1468.95/s)  LR: 1.816e-04  Data: 0.010 (0.013)
Train: 436 [ 950/1251 ( 76%)]  Loss: 3.790 (3.46)  Time: 0.691s, 1482.31/s  (0.697s, 1469.47/s)  LR: 1.816e-04  Data: 0.010 (0.013)
Train: 436 [1000/1251 ( 80%)]  Loss: 3.562 (3.46)  Time: 0.673s, 1520.77/s  (0.697s, 1469.31/s)  LR: 1.816e-04  Data: 0.013 (0.012)
Train: 436 [1050/1251 ( 84%)]  Loss: 3.148 (3.45)  Time: 0.676s, 1514.37/s  (0.697s, 1469.30/s)  LR: 1.816e-04  Data: 0.011 (0.012)
Train: 436 [1100/1251 ( 88%)]  Loss: 3.257 (3.44)  Time: 0.696s, 1470.26/s  (0.696s, 1470.36/s)  LR: 1.816e-04  Data: 0.011 (0.012)
Train: 436 [1150/1251 ( 92%)]  Loss: 3.674 (3.45)  Time: 0.728s, 1406.57/s  (0.696s, 1470.60/s)  LR: 1.816e-04  Data: 0.009 (0.012)
Train: 436 [1200/1251 ( 96%)]  Loss: 2.944 (3.43)  Time: 0.672s, 1523.14/s  (0.696s, 1471.47/s)  LR: 1.816e-04  Data: 0.011 (0.012)
Train: 436 [1250/1251 (100%)]  Loss: 3.549 (3.44)  Time: 0.654s, 1565.08/s  (0.696s, 1471.67/s)  LR: 1.816e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.609 (1.609)  Loss:  0.7021 (0.7021)  Acc@1: 90.5273 (90.5273)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.136 (0.579)  Loss:  0.8970 (1.2829)  Acc@1: 86.2028 (76.4460)  Acc@5: 97.0519 (93.3160)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-433.pth.tar', 76.6480000366211)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-430.pth.tar', 76.64400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-432.pth.tar', 76.63000003417969)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-425.pth.tar', 76.51800018554688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-436.pth.tar', 76.44600008300782)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-428.pth.tar', 76.42600000244141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-429.pth.tar', 76.41999998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-431.pth.tar', 76.41599998535156)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-419.pth.tar', 76.4080000341797)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-434.pth.tar', 76.38799993408203)

Train: 437 [   0/1251 (  0%)]  Loss: 3.547 (3.55)  Time: 2.300s,  445.18/s  (2.300s,  445.18/s)  LR: 1.796e-04  Data: 1.682 (1.682)
Train: 437 [  50/1251 (  4%)]  Loss: 3.489 (3.52)  Time: 0.670s, 1527.48/s  (0.718s, 1425.88/s)  LR: 1.796e-04  Data: 0.010 (0.046)
Train: 437 [ 100/1251 (  8%)]  Loss: 3.183 (3.41)  Time: 0.708s, 1446.82/s  (0.707s, 1449.36/s)  LR: 1.796e-04  Data: 0.009 (0.029)
Train: 437 [ 150/1251 ( 12%)]  Loss: 3.048 (3.32)  Time: 0.708s, 1447.33/s  (0.701s, 1461.19/s)  LR: 1.796e-04  Data: 0.011 (0.023)
Train: 437 [ 200/1251 ( 16%)]  Loss: 3.163 (3.29)  Time: 0.783s, 1307.39/s  (0.700s, 1462.21/s)  LR: 1.796e-04  Data: 0.010 (0.020)
Train: 437 [ 250/1251 ( 20%)]  Loss: 3.472 (3.32)  Time: 0.672s, 1524.71/s  (0.697s, 1470.02/s)  LR: 1.796e-04  Data: 0.012 (0.018)
Train: 437 [ 300/1251 ( 24%)]  Loss: 3.664 (3.37)  Time: 0.691s, 1481.99/s  (0.697s, 1470.18/s)  LR: 1.796e-04  Data: 0.010 (0.017)
Train: 437 [ 350/1251 ( 28%)]  Loss: 3.634 (3.40)  Time: 0.689s, 1486.39/s  (0.696s, 1470.27/s)  LR: 1.796e-04  Data: 0.010 (0.016)
Train: 437 [ 400/1251 ( 32%)]  Loss: 3.528 (3.41)  Time: 0.735s, 1393.18/s  (0.696s, 1471.46/s)  LR: 1.796e-04  Data: 0.009 (0.015)
Train: 437 [ 450/1251 ( 36%)]  Loss: 3.257 (3.40)  Time: 0.691s, 1481.17/s  (0.695s, 1472.70/s)  LR: 1.796e-04  Data: 0.009 (0.015)
Train: 437 [ 500/1251 ( 40%)]  Loss: 3.307 (3.39)  Time: 0.671s, 1526.39/s  (0.695s, 1473.15/s)  LR: 1.796e-04  Data: 0.010 (0.014)
Train: 437 [ 550/1251 ( 44%)]  Loss: 3.564 (3.40)  Time: 0.672s, 1524.57/s  (0.694s, 1474.53/s)  LR: 1.796e-04  Data: 0.011 (0.014)
Train: 437 [ 600/1251 ( 48%)]  Loss: 3.692 (3.43)  Time: 0.671s, 1526.58/s  (0.694s, 1476.10/s)  LR: 1.796e-04  Data: 0.010 (0.014)
Train: 437 [ 650/1251 ( 52%)]  Loss: 3.527 (3.43)  Time: 0.767s, 1334.49/s  (0.693s, 1476.58/s)  LR: 1.796e-04  Data: 0.010 (0.013)
Train: 437 [ 700/1251 ( 56%)]  Loss: 3.577 (3.44)  Time: 0.672s, 1523.22/s  (0.693s, 1477.34/s)  LR: 1.796e-04  Data: 0.010 (0.013)
Train: 437 [ 750/1251 ( 60%)]  Loss: 3.194 (3.43)  Time: 0.670s, 1528.31/s  (0.693s, 1478.60/s)  LR: 1.796e-04  Data: 0.011 (0.013)
Train: 437 [ 800/1251 ( 64%)]  Loss: 3.222 (3.42)  Time: 0.667s, 1535.17/s  (0.692s, 1479.66/s)  LR: 1.796e-04  Data: 0.009 (0.013)
Train: 437 [ 850/1251 ( 68%)]  Loss: 3.279 (3.41)  Time: 0.732s, 1398.73/s  (0.692s, 1480.74/s)  LR: 1.796e-04  Data: 0.010 (0.013)
Train: 437 [ 900/1251 ( 72%)]  Loss: 3.174 (3.40)  Time: 0.667s, 1535.44/s  (0.691s, 1480.86/s)  LR: 1.796e-04  Data: 0.009 (0.013)
Train: 437 [ 950/1251 ( 76%)]  Loss: 3.096 (3.38)  Time: 0.680s, 1506.68/s  (0.692s, 1480.30/s)  LR: 1.796e-04  Data: 0.012 (0.012)
Train: 437 [1000/1251 ( 80%)]  Loss: 3.300 (3.38)  Time: 0.681s, 1503.44/s  (0.692s, 1480.00/s)  LR: 1.796e-04  Data: 0.010 (0.012)
Train: 437 [1050/1251 ( 84%)]  Loss: 3.057 (3.36)  Time: 0.698s, 1466.84/s  (0.691s, 1480.97/s)  LR: 1.796e-04  Data: 0.010 (0.012)
Train: 437 [1100/1251 ( 88%)]  Loss: 3.299 (3.36)  Time: 0.674s, 1519.92/s  (0.691s, 1481.25/s)  LR: 1.796e-04  Data: 0.011 (0.012)
Train: 437 [1150/1251 ( 92%)]  Loss: 3.476 (3.36)  Time: 0.694s, 1476.22/s  (0.691s, 1481.70/s)  LR: 1.796e-04  Data: 0.010 (0.012)
Train: 437 [1200/1251 ( 96%)]  Loss: 3.552 (3.37)  Time: 0.671s, 1526.34/s  (0.691s, 1481.87/s)  LR: 1.796e-04  Data: 0.011 (0.012)
Train: 437 [1250/1251 (100%)]  Loss: 3.378 (3.37)  Time: 0.685s, 1494.91/s  (0.691s, 1482.41/s)  LR: 1.796e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.478 (1.478)  Loss:  0.7212 (0.7212)  Acc@1: 91.3086 (91.3086)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.136 (0.575)  Loss:  0.8843 (1.2809)  Acc@1: 85.6132 (76.3120)  Acc@5: 96.9340 (93.2260)
Train: 438 [   0/1251 (  0%)]  Loss: 3.031 (3.03)  Time: 2.361s,  433.80/s  (2.361s,  433.80/s)  LR: 1.777e-04  Data: 1.745 (1.745)
Train: 438 [  50/1251 (  4%)]  Loss: 3.179 (3.11)  Time: 0.677s, 1512.20/s  (0.733s, 1396.30/s)  LR: 1.777e-04  Data: 0.013 (0.055)
Train: 438 [ 100/1251 (  8%)]  Loss: 3.303 (3.17)  Time: 0.671s, 1524.99/s  (0.717s, 1427.23/s)  LR: 1.777e-04  Data: 0.011 (0.033)
Train: 438 [ 150/1251 ( 12%)]  Loss: 3.142 (3.16)  Time: 0.705s, 1452.70/s  (0.708s, 1445.59/s)  LR: 1.777e-04  Data: 0.010 (0.025)
Train: 438 [ 200/1251 ( 16%)]  Loss: 3.395 (3.21)  Time: 0.678s, 1510.77/s  (0.704s, 1455.46/s)  LR: 1.777e-04  Data: 0.009 (0.022)
Train: 438 [ 250/1251 ( 20%)]  Loss: 3.853 (3.32)  Time: 0.709s, 1444.90/s  (0.701s, 1460.71/s)  LR: 1.777e-04  Data: 0.009 (0.019)
Train: 438 [ 300/1251 ( 24%)]  Loss: 3.363 (3.32)  Time: 0.690s, 1484.25/s  (0.700s, 1463.72/s)  LR: 1.777e-04  Data: 0.010 (0.018)
Train: 438 [ 350/1251 ( 28%)]  Loss: 2.799 (3.26)  Time: 0.695s, 1473.94/s  (0.699s, 1464.86/s)  LR: 1.777e-04  Data: 0.013 (0.017)
Train: 438 [ 400/1251 ( 32%)]  Loss: 3.310 (3.26)  Time: 0.672s, 1523.50/s  (0.698s, 1466.32/s)  LR: 1.777e-04  Data: 0.012 (0.016)
Train: 438 [ 450/1251 ( 36%)]  Loss: 3.456 (3.28)  Time: 0.666s, 1537.86/s  (0.697s, 1468.11/s)  LR: 1.777e-04  Data: 0.010 (0.016)
Train: 438 [ 500/1251 ( 40%)]  Loss: 3.397 (3.29)  Time: 0.675s, 1517.35/s  (0.696s, 1470.34/s)  LR: 1.777e-04  Data: 0.011 (0.015)
Train: 438 [ 550/1251 ( 44%)]  Loss: 3.453 (3.31)  Time: 0.703s, 1456.12/s  (0.696s, 1470.73/s)  LR: 1.777e-04  Data: 0.010 (0.015)
Train: 438 [ 600/1251 ( 48%)]  Loss: 3.692 (3.34)  Time: 0.681s, 1503.52/s  (0.696s, 1472.02/s)  LR: 1.777e-04  Data: 0.014 (0.014)
Train: 438 [ 650/1251 ( 52%)]  Loss: 3.488 (3.35)  Time: 0.676s, 1513.82/s  (0.695s, 1473.29/s)  LR: 1.777e-04  Data: 0.011 (0.014)
Train: 438 [ 700/1251 ( 56%)]  Loss: 3.439 (3.35)  Time: 0.680s, 1505.11/s  (0.694s, 1474.50/s)  LR: 1.777e-04  Data: 0.011 (0.014)
Train: 438 [ 750/1251 ( 60%)]  Loss: 2.710 (3.31)  Time: 0.730s, 1403.26/s  (0.695s, 1473.93/s)  LR: 1.777e-04  Data: 0.011 (0.014)
Train: 438 [ 800/1251 ( 64%)]  Loss: 3.008 (3.30)  Time: 0.718s, 1425.78/s  (0.694s, 1474.99/s)  LR: 1.777e-04  Data: 0.012 (0.013)
Train: 438 [ 850/1251 ( 68%)]  Loss: 3.282 (3.29)  Time: 0.717s, 1427.48/s  (0.694s, 1475.46/s)  LR: 1.777e-04  Data: 0.011 (0.013)
Train: 438 [ 900/1251 ( 72%)]  Loss: 3.660 (3.31)  Time: 0.670s, 1527.67/s  (0.694s, 1475.84/s)  LR: 1.777e-04  Data: 0.010 (0.013)
Train: 438 [ 950/1251 ( 76%)]  Loss: 3.270 (3.31)  Time: 0.678s, 1510.02/s  (0.694s, 1476.40/s)  LR: 1.777e-04  Data: 0.011 (0.013)
Train: 438 [1000/1251 ( 80%)]  Loss: 3.260 (3.31)  Time: 0.679s, 1509.00/s  (0.693s, 1476.75/s)  LR: 1.777e-04  Data: 0.012 (0.013)
Train: 438 [1050/1251 ( 84%)]  Loss: 3.402 (3.31)  Time: 0.710s, 1443.14/s  (0.693s, 1477.74/s)  LR: 1.777e-04  Data: 0.011 (0.013)
Train: 438 [1100/1251 ( 88%)]  Loss: 3.492 (3.32)  Time: 0.680s, 1504.80/s  (0.692s, 1478.76/s)  LR: 1.777e-04  Data: 0.010 (0.012)
Train: 438 [1150/1251 ( 92%)]  Loss: 3.648 (3.33)  Time: 0.710s, 1441.87/s  (0.692s, 1478.90/s)  LR: 1.777e-04  Data: 0.010 (0.012)
Train: 438 [1200/1251 ( 96%)]  Loss: 3.481 (3.34)  Time: 0.680s, 1505.24/s  (0.692s, 1479.26/s)  LR: 1.777e-04  Data: 0.009 (0.012)
Train: 438 [1250/1251 (100%)]  Loss: 3.164 (3.33)  Time: 0.697s, 1470.04/s  (0.693s, 1478.62/s)  LR: 1.777e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.497 (1.497)  Loss:  0.7261 (0.7261)  Acc@1: 91.3086 (91.3086)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.136 (0.587)  Loss:  0.8770 (1.2439)  Acc@1: 85.9670 (77.0260)  Acc@5: 96.8160 (93.4100)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-438.pth.tar', 77.02600010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-433.pth.tar', 76.6480000366211)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-430.pth.tar', 76.64400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-432.pth.tar', 76.63000003417969)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-425.pth.tar', 76.51800018554688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-436.pth.tar', 76.44600008300782)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-428.pth.tar', 76.42600000244141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-429.pth.tar', 76.41999998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-431.pth.tar', 76.41599998535156)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-419.pth.tar', 76.4080000341797)

Train: 439 [   0/1251 (  0%)]  Loss: 3.716 (3.72)  Time: 2.045s,  500.71/s  (2.045s,  500.71/s)  LR: 1.757e-04  Data: 1.431 (1.431)
Train: 439 [  50/1251 (  4%)]  Loss: 3.541 (3.63)  Time: 0.686s, 1493.68/s  (0.721s, 1420.91/s)  LR: 1.757e-04  Data: 0.013 (0.046)
Train: 439 [ 100/1251 (  8%)]  Loss: 3.604 (3.62)  Time: 0.682s, 1502.33/s  (0.710s, 1442.44/s)  LR: 1.757e-04  Data: 0.014 (0.029)
Train: 439 [ 150/1251 ( 12%)]  Loss: 3.587 (3.61)  Time: 0.747s, 1370.89/s  (0.710s, 1442.89/s)  LR: 1.757e-04  Data: 0.013 (0.023)
Train: 439 [ 200/1251 ( 16%)]  Loss: 3.806 (3.65)  Time: 0.676s, 1513.74/s  (0.710s, 1443.11/s)  LR: 1.757e-04  Data: 0.010 (0.021)
Train: 439 [ 250/1251 ( 20%)]  Loss: 3.564 (3.64)  Time: 0.698s, 1466.76/s  (0.709s, 1443.51/s)  LR: 1.757e-04  Data: 0.010 (0.019)
Train: 439 [ 300/1251 ( 24%)]  Loss: 3.566 (3.63)  Time: 0.670s, 1528.75/s  (0.705s, 1453.39/s)  LR: 1.757e-04  Data: 0.010 (0.017)
Train: 439 [ 350/1251 ( 28%)]  Loss: 3.467 (3.61)  Time: 0.671s, 1526.47/s  (0.701s, 1461.56/s)  LR: 1.757e-04  Data: 0.010 (0.016)
Train: 439 [ 400/1251 ( 32%)]  Loss: 3.683 (3.61)  Time: 0.672s, 1523.02/s  (0.697s, 1468.46/s)  LR: 1.757e-04  Data: 0.010 (0.016)
Train: 439 [ 450/1251 ( 36%)]  Loss: 3.444 (3.60)  Time: 0.767s, 1335.37/s  (0.696s, 1470.80/s)  LR: 1.757e-04  Data: 0.011 (0.015)
Train: 439 [ 500/1251 ( 40%)]  Loss: 3.652 (3.60)  Time: 0.682s, 1502.10/s  (0.696s, 1471.53/s)  LR: 1.757e-04  Data: 0.010 (0.015)
Train: 439 [ 550/1251 ( 44%)]  Loss: 3.297 (3.58)  Time: 0.672s, 1523.74/s  (0.695s, 1474.12/s)  LR: 1.757e-04  Data: 0.010 (0.014)
Train: 439 [ 600/1251 ( 48%)]  Loss: 3.654 (3.58)  Time: 0.710s, 1441.77/s  (0.695s, 1474.14/s)  LR: 1.757e-04  Data: 0.009 (0.014)
Train: 439 [ 650/1251 ( 52%)]  Loss: 3.440 (3.57)  Time: 0.703s, 1456.90/s  (0.694s, 1475.25/s)  LR: 1.757e-04  Data: 0.009 (0.014)
Train: 439 [ 700/1251 ( 56%)]  Loss: 3.406 (3.56)  Time: 0.665s, 1538.71/s  (0.694s, 1474.50/s)  LR: 1.757e-04  Data: 0.008 (0.013)
Train: 439 [ 750/1251 ( 60%)]  Loss: 3.571 (3.56)  Time: 0.671s, 1525.59/s  (0.694s, 1475.23/s)  LR: 1.757e-04  Data: 0.009 (0.013)
Train: 439 [ 800/1251 ( 64%)]  Loss: 3.665 (3.57)  Time: 0.722s, 1418.59/s  (0.694s, 1474.70/s)  LR: 1.757e-04  Data: 0.009 (0.013)
Train: 439 [ 850/1251 ( 68%)]  Loss: 3.456 (3.56)  Time: 0.743s, 1378.79/s  (0.695s, 1474.25/s)  LR: 1.757e-04  Data: 0.009 (0.013)
Train: 439 [ 900/1251 ( 72%)]  Loss: 3.397 (3.55)  Time: 0.703s, 1457.04/s  (0.694s, 1475.44/s)  LR: 1.757e-04  Data: 0.009 (0.013)
Train: 439 [ 950/1251 ( 76%)]  Loss: 3.053 (3.53)  Time: 0.683s, 1499.57/s  (0.694s, 1475.17/s)  LR: 1.757e-04  Data: 0.010 (0.013)
Train: 439 [1000/1251 ( 80%)]  Loss: 3.317 (3.52)  Time: 0.667s, 1536.34/s  (0.694s, 1475.40/s)  LR: 1.757e-04  Data: 0.011 (0.013)
Train: 439 [1050/1251 ( 84%)]  Loss: 3.618 (3.52)  Time: 0.796s, 1287.05/s  (0.694s, 1475.36/s)  LR: 1.757e-04  Data: 0.011 (0.012)
Train: 439 [1100/1251 ( 88%)]  Loss: 3.104 (3.50)  Time: 0.703s, 1457.12/s  (0.694s, 1475.21/s)  LR: 1.757e-04  Data: 0.010 (0.012)
Train: 439 [1150/1251 ( 92%)]  Loss: 3.497 (3.50)  Time: 0.673s, 1521.25/s  (0.694s, 1475.58/s)  LR: 1.757e-04  Data: 0.011 (0.012)
Train: 439 [1200/1251 ( 96%)]  Loss: 3.161 (3.49)  Time: 0.680s, 1506.96/s  (0.694s, 1475.29/s)  LR: 1.757e-04  Data: 0.013 (0.012)
Train: 439 [1250/1251 (100%)]  Loss: 3.523 (3.49)  Time: 0.659s, 1553.97/s  (0.694s, 1475.11/s)  LR: 1.757e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.467 (1.467)  Loss:  0.7886 (0.7886)  Acc@1: 91.4062 (91.4062)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.136 (0.587)  Loss:  0.8901 (1.3211)  Acc@1: 85.2594 (76.5240)  Acc@5: 96.3443 (93.1980)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-438.pth.tar', 77.02600010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-433.pth.tar', 76.6480000366211)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-430.pth.tar', 76.64400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-432.pth.tar', 76.63000003417969)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-439.pth.tar', 76.52400006103515)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-425.pth.tar', 76.51800018554688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-436.pth.tar', 76.44600008300782)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-428.pth.tar', 76.42600000244141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-429.pth.tar', 76.41999998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-431.pth.tar', 76.41599998535156)

Train: 440 [   0/1251 (  0%)]  Loss: 3.530 (3.53)  Time: 2.181s,  469.57/s  (2.181s,  469.57/s)  LR: 1.738e-04  Data: 1.564 (1.564)
Train: 440 [  50/1251 (  4%)]  Loss: 3.478 (3.50)  Time: 0.683s, 1499.31/s  (0.727s, 1407.98/s)  LR: 1.738e-04  Data: 0.010 (0.049)
Train: 440 [ 100/1251 (  8%)]  Loss: 3.363 (3.46)  Time: 0.772s, 1325.79/s  (0.710s, 1442.42/s)  LR: 1.738e-04  Data: 0.011 (0.030)
Train: 440 [ 150/1251 ( 12%)]  Loss: 2.857 (3.31)  Time: 0.679s, 1507.21/s  (0.707s, 1447.96/s)  LR: 1.738e-04  Data: 0.015 (0.023)
Train: 440 [ 200/1251 ( 16%)]  Loss: 3.647 (3.38)  Time: 0.679s, 1509.05/s  (0.706s, 1451.32/s)  LR: 1.738e-04  Data: 0.014 (0.020)
Train: 440 [ 250/1251 ( 20%)]  Loss: 3.447 (3.39)  Time: 0.672s, 1523.01/s  (0.703s, 1457.27/s)  LR: 1.738e-04  Data: 0.011 (0.018)
Train: 440 [ 300/1251 ( 24%)]  Loss: 3.209 (3.36)  Time: 0.678s, 1510.16/s  (0.700s, 1462.04/s)  LR: 1.738e-04  Data: 0.011 (0.017)
Train: 440 [ 350/1251 ( 28%)]  Loss: 3.126 (3.33)  Time: 0.675s, 1517.10/s  (0.698s, 1466.95/s)  LR: 1.738e-04  Data: 0.011 (0.016)
Train: 440 [ 400/1251 ( 32%)]  Loss: 3.398 (3.34)  Time: 0.715s, 1432.61/s  (0.697s, 1468.55/s)  LR: 1.738e-04  Data: 0.012 (0.015)
Train: 440 [ 450/1251 ( 36%)]  Loss: 3.277 (3.33)  Time: 0.670s, 1527.58/s  (0.697s, 1469.49/s)  LR: 1.738e-04  Data: 0.011 (0.015)
Train: 440 [ 500/1251 ( 40%)]  Loss: 3.208 (3.32)  Time: 0.688s, 1488.07/s  (0.696s, 1470.50/s)  LR: 1.738e-04  Data: 0.012 (0.014)
Train: 440 [ 550/1251 ( 44%)]  Loss: 3.197 (3.31)  Time: 0.698s, 1466.48/s  (0.696s, 1471.45/s)  LR: 1.738e-04  Data: 0.010 (0.014)
Train: 440 [ 600/1251 ( 48%)]  Loss: 3.817 (3.35)  Time: 0.666s, 1537.50/s  (0.696s, 1471.50/s)  LR: 1.738e-04  Data: 0.009 (0.014)
Train: 440 [ 650/1251 ( 52%)]  Loss: 3.540 (3.36)  Time: 0.707s, 1448.58/s  (0.696s, 1472.27/s)  LR: 1.738e-04  Data: 0.009 (0.013)
Train: 440 [ 700/1251 ( 56%)]  Loss: 3.637 (3.38)  Time: 0.682s, 1502.34/s  (0.695s, 1473.73/s)  LR: 1.738e-04  Data: 0.010 (0.013)
Train: 440 [ 750/1251 ( 60%)]  Loss: 3.538 (3.39)  Time: 0.673s, 1520.82/s  (0.694s, 1475.56/s)  LR: 1.738e-04  Data: 0.010 (0.013)
Train: 440 [ 800/1251 ( 64%)]  Loss: 3.170 (3.38)  Time: 0.704s, 1454.17/s  (0.693s, 1476.64/s)  LR: 1.738e-04  Data: 0.010 (0.013)
Train: 440 [ 850/1251 ( 68%)]  Loss: 3.826 (3.40)  Time: 0.716s, 1430.41/s  (0.693s, 1476.63/s)  LR: 1.738e-04  Data: 0.013 (0.013)
Train: 440 [ 900/1251 ( 72%)]  Loss: 2.985 (3.38)  Time: 0.709s, 1444.04/s  (0.693s, 1476.92/s)  LR: 1.738e-04  Data: 0.009 (0.013)
Train: 440 [ 950/1251 ( 76%)]  Loss: 3.486 (3.39)  Time: 0.702s, 1457.92/s  (0.694s, 1476.51/s)  LR: 1.738e-04  Data: 0.009 (0.012)
Train: 440 [1000/1251 ( 80%)]  Loss: 3.458 (3.39)  Time: 0.740s, 1383.86/s  (0.693s, 1476.67/s)  LR: 1.738e-04  Data: 0.010 (0.012)
Train: 440 [1050/1251 ( 84%)]  Loss: 3.452 (3.39)  Time: 0.712s, 1438.15/s  (0.694s, 1476.55/s)  LR: 1.738e-04  Data: 0.012 (0.012)
Train: 440 [1100/1251 ( 88%)]  Loss: 3.435 (3.39)  Time: 0.667s, 1535.08/s  (0.693s, 1476.95/s)  LR: 1.738e-04  Data: 0.009 (0.012)
Train: 440 [1150/1251 ( 92%)]  Loss: 3.407 (3.40)  Time: 0.679s, 1507.22/s  (0.693s, 1477.50/s)  LR: 1.738e-04  Data: 0.013 (0.012)
Train: 440 [1200/1251 ( 96%)]  Loss: 3.250 (3.39)  Time: 0.713s, 1437.16/s  (0.693s, 1477.60/s)  LR: 1.738e-04  Data: 0.009 (0.012)
Train: 440 [1250/1251 (100%)]  Loss: 3.279 (3.39)  Time: 0.661s, 1548.31/s  (0.693s, 1478.04/s)  LR: 1.738e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.614 (1.614)  Loss:  0.7705 (0.7705)  Acc@1: 91.0156 (91.0156)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.136 (0.577)  Loss:  0.7998 (1.2772)  Acc@1: 86.2028 (76.6540)  Acc@5: 96.6981 (93.2920)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-438.pth.tar', 77.02600010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-440.pth.tar', 76.65399995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-433.pth.tar', 76.6480000366211)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-430.pth.tar', 76.64400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-432.pth.tar', 76.63000003417969)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-439.pth.tar', 76.52400006103515)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-425.pth.tar', 76.51800018554688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-436.pth.tar', 76.44600008300782)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-428.pth.tar', 76.42600000244141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-429.pth.tar', 76.41999998046875)

Train: 441 [   0/1251 (  0%)]  Loss: 3.190 (3.19)  Time: 2.321s,  441.21/s  (2.321s,  441.21/s)  LR: 1.719e-04  Data: 1.704 (1.704)
Train: 441 [  50/1251 (  4%)]  Loss: 3.195 (3.19)  Time: 0.698s, 1466.24/s  (0.725s, 1412.55/s)  LR: 1.719e-04  Data: 0.009 (0.045)
Train: 441 [ 100/1251 (  8%)]  Loss: 3.440 (3.28)  Time: 0.710s, 1441.55/s  (0.711s, 1439.53/s)  LR: 1.719e-04  Data: 0.015 (0.028)
Train: 441 [ 150/1251 ( 12%)]  Loss: 3.666 (3.37)  Time: 0.725s, 1413.00/s  (0.705s, 1453.18/s)  LR: 1.719e-04  Data: 0.009 (0.022)
Train: 441 [ 200/1251 ( 16%)]  Loss: 3.335 (3.37)  Time: 0.723s, 1416.89/s  (0.701s, 1459.81/s)  LR: 1.719e-04  Data: 0.010 (0.019)
Train: 441 [ 250/1251 ( 20%)]  Loss: 3.261 (3.35)  Time: 0.673s, 1521.33/s  (0.700s, 1462.21/s)  LR: 1.719e-04  Data: 0.011 (0.018)
Train: 441 [ 300/1251 ( 24%)]  Loss: 3.277 (3.34)  Time: 0.706s, 1450.81/s  (0.698s, 1466.52/s)  LR: 1.719e-04  Data: 0.009 (0.016)
Train: 441 [ 350/1251 ( 28%)]  Loss: 3.605 (3.37)  Time: 0.676s, 1514.13/s  (0.697s, 1470.03/s)  LR: 1.719e-04  Data: 0.010 (0.016)
Train: 441 [ 400/1251 ( 32%)]  Loss: 3.216 (3.35)  Time: 0.699s, 1464.13/s  (0.696s, 1471.43/s)  LR: 1.719e-04  Data: 0.009 (0.015)
Train: 441 [ 450/1251 ( 36%)]  Loss: 3.639 (3.38)  Time: 0.698s, 1467.24/s  (0.695s, 1472.64/s)  LR: 1.719e-04  Data: 0.012 (0.014)
Train: 441 [ 500/1251 ( 40%)]  Loss: 3.488 (3.39)  Time: 0.729s, 1404.98/s  (0.696s, 1472.26/s)  LR: 1.719e-04  Data: 0.009 (0.014)
Train: 441 [ 550/1251 ( 44%)]  Loss: 3.627 (3.41)  Time: 0.702s, 1458.97/s  (0.695s, 1473.44/s)  LR: 1.719e-04  Data: 0.009 (0.014)
Train: 441 [ 600/1251 ( 48%)]  Loss: 3.462 (3.42)  Time: 0.700s, 1462.90/s  (0.694s, 1474.93/s)  LR: 1.719e-04  Data: 0.009 (0.013)
Train: 441 [ 650/1251 ( 52%)]  Loss: 3.282 (3.41)  Time: 0.666s, 1538.36/s  (0.694s, 1474.49/s)  LR: 1.719e-04  Data: 0.011 (0.013)
Train: 441 [ 700/1251 ( 56%)]  Loss: 3.384 (3.40)  Time: 0.672s, 1524.63/s  (0.694s, 1475.08/s)  LR: 1.719e-04  Data: 0.010 (0.013)
Train: 441 [ 750/1251 ( 60%)]  Loss: 3.298 (3.40)  Time: 0.672s, 1524.45/s  (0.694s, 1476.41/s)  LR: 1.719e-04  Data: 0.011 (0.013)
Train: 441 [ 800/1251 ( 64%)]  Loss: 3.134 (3.38)  Time: 0.671s, 1524.99/s  (0.693s, 1476.74/s)  LR: 1.719e-04  Data: 0.009 (0.013)
Train: 441 [ 850/1251 ( 68%)]  Loss: 3.360 (3.38)  Time: 0.703s, 1457.18/s  (0.693s, 1477.00/s)  LR: 1.719e-04  Data: 0.009 (0.012)
Train: 441 [ 900/1251 ( 72%)]  Loss: 3.353 (3.38)  Time: 0.672s, 1524.25/s  (0.693s, 1477.26/s)  LR: 1.719e-04  Data: 0.011 (0.012)
Train: 441 [ 950/1251 ( 76%)]  Loss: 3.683 (3.39)  Time: 0.711s, 1440.53/s  (0.693s, 1477.69/s)  LR: 1.719e-04  Data: 0.010 (0.012)
Train: 441 [1000/1251 ( 80%)]  Loss: 3.714 (3.41)  Time: 0.667s, 1535.96/s  (0.693s, 1477.53/s)  LR: 1.719e-04  Data: 0.010 (0.012)
Train: 441 [1050/1251 ( 84%)]  Loss: 3.312 (3.41)  Time: 0.704s, 1453.96/s  (0.693s, 1477.79/s)  LR: 1.719e-04  Data: 0.010 (0.012)
Train: 441 [1100/1251 ( 88%)]  Loss: 3.372 (3.40)  Time: 0.664s, 1542.46/s  (0.693s, 1478.04/s)  LR: 1.719e-04  Data: 0.008 (0.012)
Train: 441 [1150/1251 ( 92%)]  Loss: 3.522 (3.41)  Time: 0.709s, 1444.95/s  (0.693s, 1478.24/s)  LR: 1.719e-04  Data: 0.009 (0.012)
Train: 441 [1200/1251 ( 96%)]  Loss: 3.030 (3.39)  Time: 0.665s, 1539.36/s  (0.693s, 1478.51/s)  LR: 1.719e-04  Data: 0.009 (0.012)
Train: 441 [1250/1251 (100%)]  Loss: 3.856 (3.41)  Time: 0.661s, 1549.94/s  (0.692s, 1478.91/s)  LR: 1.719e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.558 (1.558)  Loss:  0.8149 (0.8149)  Acc@1: 91.5039 (91.5039)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.136 (0.578)  Loss:  0.9204 (1.3188)  Acc@1: 85.3774 (76.7800)  Acc@5: 97.4057 (93.3240)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-438.pth.tar', 77.02600010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-441.pth.tar', 76.77999998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-440.pth.tar', 76.65399995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-433.pth.tar', 76.6480000366211)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-430.pth.tar', 76.64400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-432.pth.tar', 76.63000003417969)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-439.pth.tar', 76.52400006103515)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-425.pth.tar', 76.51800018554688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-436.pth.tar', 76.44600008300782)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-428.pth.tar', 76.42600000244141)

Train: 442 [   0/1251 (  0%)]  Loss: 3.551 (3.55)  Time: 2.254s,  454.35/s  (2.254s,  454.35/s)  LR: 1.699e-04  Data: 1.639 (1.639)
Train: 442 [  50/1251 (  4%)]  Loss: 3.500 (3.53)  Time: 0.715s, 1432.68/s  (0.724s, 1413.83/s)  LR: 1.699e-04  Data: 0.010 (0.055)
Train: 442 [ 100/1251 (  8%)]  Loss: 3.246 (3.43)  Time: 0.671s, 1525.83/s  (0.709s, 1443.67/s)  LR: 1.699e-04  Data: 0.011 (0.033)
Train: 442 [ 150/1251 ( 12%)]  Loss: 3.726 (3.51)  Time: 0.667s, 1534.74/s  (0.703s, 1457.40/s)  LR: 1.699e-04  Data: 0.010 (0.025)
Train: 442 [ 200/1251 ( 16%)]  Loss: 3.233 (3.45)  Time: 0.690s, 1485.09/s  (0.700s, 1463.82/s)  LR: 1.699e-04  Data: 0.010 (0.022)
Train: 442 [ 250/1251 ( 20%)]  Loss: 3.522 (3.46)  Time: 0.682s, 1502.34/s  (0.697s, 1470.17/s)  LR: 1.699e-04  Data: 0.011 (0.019)
Train: 442 [ 300/1251 ( 24%)]  Loss: 3.369 (3.45)  Time: 0.708s, 1445.87/s  (0.696s, 1471.59/s)  LR: 1.699e-04  Data: 0.011 (0.018)
Train: 442 [ 350/1251 ( 28%)]  Loss: 3.037 (3.40)  Time: 0.688s, 1488.42/s  (0.694s, 1474.67/s)  LR: 1.699e-04  Data: 0.013 (0.017)
Train: 442 [ 400/1251 ( 32%)]  Loss: 3.471 (3.41)  Time: 0.696s, 1470.25/s  (0.694s, 1475.69/s)  LR: 1.699e-04  Data: 0.010 (0.016)
Train: 442 [ 450/1251 ( 36%)]  Loss: 3.243 (3.39)  Time: 0.672s, 1524.03/s  (0.693s, 1478.01/s)  LR: 1.699e-04  Data: 0.009 (0.015)
Train: 442 [ 500/1251 ( 40%)]  Loss: 3.396 (3.39)  Time: 0.716s, 1430.45/s  (0.692s, 1478.93/s)  LR: 1.699e-04  Data: 0.015 (0.015)
Train: 442 [ 550/1251 ( 44%)]  Loss: 3.337 (3.39)  Time: 0.677s, 1512.62/s  (0.692s, 1479.32/s)  LR: 1.699e-04  Data: 0.012 (0.014)
Train: 442 [ 600/1251 ( 48%)]  Loss: 3.631 (3.40)  Time: 0.707s, 1448.84/s  (0.692s, 1479.00/s)  LR: 1.699e-04  Data: 0.010 (0.014)
Train: 442 [ 650/1251 ( 52%)]  Loss: 3.454 (3.41)  Time: 0.718s, 1425.68/s  (0.693s, 1477.70/s)  LR: 1.699e-04  Data: 0.012 (0.014)
Train: 442 [ 700/1251 ( 56%)]  Loss: 3.606 (3.42)  Time: 0.681s, 1503.67/s  (0.693s, 1477.60/s)  LR: 1.699e-04  Data: 0.013 (0.014)
Train: 442 [ 750/1251 ( 60%)]  Loss: 3.482 (3.43)  Time: 0.668s, 1532.88/s  (0.693s, 1477.37/s)  LR: 1.699e-04  Data: 0.010 (0.013)
Train: 442 [ 800/1251 ( 64%)]  Loss: 3.254 (3.42)  Time: 0.709s, 1444.79/s  (0.693s, 1478.51/s)  LR: 1.699e-04  Data: 0.011 (0.013)
Train: 442 [ 850/1251 ( 68%)]  Loss: 3.337 (3.41)  Time: 0.716s, 1430.11/s  (0.693s, 1477.86/s)  LR: 1.699e-04  Data: 0.013 (0.013)
Train: 442 [ 900/1251 ( 72%)]  Loss: 3.374 (3.41)  Time: 0.679s, 1508.06/s  (0.693s, 1477.51/s)  LR: 1.699e-04  Data: 0.013 (0.013)
Train: 442 [ 950/1251 ( 76%)]  Loss: 3.107 (3.39)  Time: 0.704s, 1454.92/s  (0.693s, 1477.57/s)  LR: 1.699e-04  Data: 0.013 (0.013)
Train: 442 [1000/1251 ( 80%)]  Loss: 3.216 (3.39)  Time: 0.699s, 1464.98/s  (0.693s, 1477.12/s)  LR: 1.699e-04  Data: 0.011 (0.013)
Train: 442 [1050/1251 ( 84%)]  Loss: 3.342 (3.38)  Time: 0.663s, 1543.90/s  (0.693s, 1477.33/s)  LR: 1.699e-04  Data: 0.009 (0.013)
Train: 442 [1100/1251 ( 88%)]  Loss: 3.162 (3.37)  Time: 0.676s, 1515.90/s  (0.693s, 1477.55/s)  LR: 1.699e-04  Data: 0.011 (0.012)
Train: 442 [1150/1251 ( 92%)]  Loss: 3.344 (3.37)  Time: 0.672s, 1523.08/s  (0.693s, 1478.11/s)  LR: 1.699e-04  Data: 0.011 (0.012)
Train: 442 [1200/1251 ( 96%)]  Loss: 3.088 (3.36)  Time: 0.708s, 1445.68/s  (0.693s, 1478.29/s)  LR: 1.699e-04  Data: 0.009 (0.012)
Train: 442 [1250/1251 (100%)]  Loss: 3.718 (3.37)  Time: 0.663s, 1545.42/s  (0.693s, 1478.10/s)  LR: 1.699e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.558 (1.558)  Loss:  0.8496 (0.8496)  Acc@1: 91.0156 (91.0156)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.136 (0.572)  Loss:  0.9443 (1.3511)  Acc@1: 85.6132 (76.8600)  Acc@5: 96.6981 (93.3800)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-438.pth.tar', 77.02600010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-442.pth.tar', 76.86000008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-441.pth.tar', 76.77999998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-440.pth.tar', 76.65399995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-433.pth.tar', 76.6480000366211)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-430.pth.tar', 76.64400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-432.pth.tar', 76.63000003417969)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-439.pth.tar', 76.52400006103515)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-425.pth.tar', 76.51800018554688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-436.pth.tar', 76.44600008300782)

Train: 443 [   0/1251 (  0%)]  Loss: 3.391 (3.39)  Time: 2.375s,  431.19/s  (2.375s,  431.19/s)  LR: 1.680e-04  Data: 1.758 (1.758)
Train: 443 [  50/1251 (  4%)]  Loss: 3.367 (3.38)  Time: 0.731s, 1401.21/s  (0.729s, 1404.14/s)  LR: 1.680e-04  Data: 0.011 (0.049)
Train: 443 [ 100/1251 (  8%)]  Loss: 3.502 (3.42)  Time: 0.681s, 1503.62/s  (0.708s, 1445.98/s)  LR: 1.680e-04  Data: 0.010 (0.030)
Train: 443 [ 150/1251 ( 12%)]  Loss: 3.580 (3.46)  Time: 0.673s, 1521.63/s  (0.703s, 1457.21/s)  LR: 1.680e-04  Data: 0.013 (0.023)
Train: 443 [ 200/1251 ( 16%)]  Loss: 3.381 (3.44)  Time: 0.701s, 1460.20/s  (0.700s, 1463.19/s)  LR: 1.680e-04  Data: 0.009 (0.020)
Train: 443 [ 250/1251 ( 20%)]  Loss: 3.626 (3.47)  Time: 0.706s, 1450.50/s  (0.698s, 1467.74/s)  LR: 1.680e-04  Data: 0.010 (0.018)
Train: 443 [ 300/1251 ( 24%)]  Loss: 3.648 (3.50)  Time: 0.672s, 1523.79/s  (0.696s, 1471.73/s)  LR: 1.680e-04  Data: 0.009 (0.017)
Train: 443 [ 350/1251 ( 28%)]  Loss: 3.601 (3.51)  Time: 0.717s, 1428.60/s  (0.696s, 1471.71/s)  LR: 1.680e-04  Data: 0.009 (0.016)
Train: 443 [ 400/1251 ( 32%)]  Loss: 3.800 (3.54)  Time: 0.695s, 1473.22/s  (0.696s, 1471.82/s)  LR: 1.680e-04  Data: 0.011 (0.015)
Train: 443 [ 450/1251 ( 36%)]  Loss: 3.129 (3.50)  Time: 0.672s, 1523.51/s  (0.696s, 1471.62/s)  LR: 1.680e-04  Data: 0.009 (0.015)
Train: 443 [ 500/1251 ( 40%)]  Loss: 3.574 (3.51)  Time: 0.684s, 1496.62/s  (0.696s, 1470.62/s)  LR: 1.680e-04  Data: 0.010 (0.014)
Train: 443 [ 550/1251 ( 44%)]  Loss: 3.167 (3.48)  Time: 0.675s, 1517.78/s  (0.695s, 1473.00/s)  LR: 1.680e-04  Data: 0.010 (0.014)
Train: 443 [ 600/1251 ( 48%)]  Loss: 3.833 (3.51)  Time: 0.706s, 1449.58/s  (0.694s, 1474.80/s)  LR: 1.680e-04  Data: 0.009 (0.014)
Train: 443 [ 650/1251 ( 52%)]  Loss: 3.227 (3.49)  Time: 0.674s, 1519.27/s  (0.694s, 1475.05/s)  LR: 1.680e-04  Data: 0.014 (0.013)
Train: 443 [ 700/1251 ( 56%)]  Loss: 3.878 (3.51)  Time: 0.704s, 1454.10/s  (0.694s, 1475.97/s)  LR: 1.680e-04  Data: 0.012 (0.013)
Train: 443 [ 750/1251 ( 60%)]  Loss: 2.989 (3.48)  Time: 0.712s, 1437.70/s  (0.695s, 1474.34/s)  LR: 1.680e-04  Data: 0.009 (0.013)
Train: 443 [ 800/1251 ( 64%)]  Loss: 3.780 (3.50)  Time: 0.691s, 1481.76/s  (0.695s, 1474.35/s)  LR: 1.680e-04  Data: 0.009 (0.013)
Train: 443 [ 850/1251 ( 68%)]  Loss: 3.595 (3.50)  Time: 0.689s, 1486.21/s  (0.694s, 1474.46/s)  LR: 1.680e-04  Data: 0.009 (0.013)
Train: 443 [ 900/1251 ( 72%)]  Loss: 3.466 (3.50)  Time: 0.670s, 1527.74/s  (0.695s, 1474.22/s)  LR: 1.680e-04  Data: 0.010 (0.013)
Train: 443 [ 950/1251 ( 76%)]  Loss: 3.246 (3.49)  Time: 0.670s, 1527.25/s  (0.694s, 1474.76/s)  LR: 1.680e-04  Data: 0.009 (0.012)
Train: 443 [1000/1251 ( 80%)]  Loss: 3.452 (3.49)  Time: 0.672s, 1524.18/s  (0.694s, 1474.82/s)  LR: 1.680e-04  Data: 0.010 (0.012)
Train: 443 [1050/1251 ( 84%)]  Loss: 3.213 (3.47)  Time: 0.792s, 1292.64/s  (0.694s, 1474.85/s)  LR: 1.680e-04  Data: 0.008 (0.012)
Train: 443 [1100/1251 ( 88%)]  Loss: 3.334 (3.47)  Time: 0.679s, 1508.00/s  (0.694s, 1475.42/s)  LR: 1.680e-04  Data: 0.009 (0.012)
Train: 443 [1150/1251 ( 92%)]  Loss: 3.938 (3.49)  Time: 0.664s, 1541.68/s  (0.694s, 1474.81/s)  LR: 1.680e-04  Data: 0.009 (0.012)
Train: 443 [1200/1251 ( 96%)]  Loss: 3.537 (3.49)  Time: 0.670s, 1528.77/s  (0.694s, 1475.12/s)  LR: 1.680e-04  Data: 0.010 (0.012)
Train: 443 [1250/1251 (100%)]  Loss: 3.630 (3.50)  Time: 0.693s, 1477.23/s  (0.694s, 1474.57/s)  LR: 1.680e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.600 (1.600)  Loss:  0.7266 (0.7266)  Acc@1: 90.1367 (90.1367)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.136 (0.581)  Loss:  0.8530 (1.2855)  Acc@1: 86.6745 (76.4680)  Acc@5: 96.3444 (93.1480)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-438.pth.tar', 77.02600010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-442.pth.tar', 76.86000008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-441.pth.tar', 76.77999998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-440.pth.tar', 76.65399995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-433.pth.tar', 76.6480000366211)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-430.pth.tar', 76.64400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-432.pth.tar', 76.63000003417969)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-439.pth.tar', 76.52400006103515)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-425.pth.tar', 76.51800018554688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-443.pth.tar', 76.46800002929687)

Train: 444 [   0/1251 (  0%)]  Loss: 3.246 (3.25)  Time: 2.130s,  480.79/s  (2.130s,  480.79/s)  LR: 1.661e-04  Data: 1.514 (1.514)
Train: 444 [  50/1251 (  4%)]  Loss: 3.528 (3.39)  Time: 0.690s, 1483.17/s  (0.726s, 1410.48/s)  LR: 1.661e-04  Data: 0.012 (0.046)
Train: 444 [ 100/1251 (  8%)]  Loss: 3.257 (3.34)  Time: 0.706s, 1450.17/s  (0.709s, 1443.51/s)  LR: 1.661e-04  Data: 0.011 (0.029)
Train: 444 [ 150/1251 ( 12%)]  Loss: 3.427 (3.36)  Time: 0.686s, 1492.86/s  (0.705s, 1453.37/s)  LR: 1.661e-04  Data: 0.009 (0.023)
Train: 444 [ 200/1251 ( 16%)]  Loss: 3.410 (3.37)  Time: 0.672s, 1523.44/s  (0.701s, 1460.91/s)  LR: 1.661e-04  Data: 0.011 (0.019)
Train: 444 [ 250/1251 ( 20%)]  Loss: 3.476 (3.39)  Time: 0.731s, 1400.73/s  (0.699s, 1463.90/s)  LR: 1.661e-04  Data: 0.009 (0.018)
Train: 444 [ 300/1251 ( 24%)]  Loss: 3.430 (3.40)  Time: 0.715s, 1432.22/s  (0.699s, 1465.37/s)  LR: 1.661e-04  Data: 0.010 (0.017)
Train: 444 [ 350/1251 ( 28%)]  Loss: 3.539 (3.41)  Time: 0.673s, 1522.34/s  (0.697s, 1468.81/s)  LR: 1.661e-04  Data: 0.010 (0.016)
Train: 444 [ 400/1251 ( 32%)]  Loss: 3.347 (3.41)  Time: 0.696s, 1471.77/s  (0.696s, 1470.80/s)  LR: 1.661e-04  Data: 0.010 (0.015)
Train: 444 [ 450/1251 ( 36%)]  Loss: 3.436 (3.41)  Time: 0.674s, 1519.78/s  (0.696s, 1471.61/s)  LR: 1.661e-04  Data: 0.011 (0.015)
Train: 444 [ 500/1251 ( 40%)]  Loss: 3.356 (3.40)  Time: 0.704s, 1454.37/s  (0.695s, 1472.58/s)  LR: 1.661e-04  Data: 0.009 (0.014)
Train: 444 [ 550/1251 ( 44%)]  Loss: 3.480 (3.41)  Time: 0.717s, 1427.80/s  (0.695s, 1473.55/s)  LR: 1.661e-04  Data: 0.009 (0.014)
Train: 444 [ 600/1251 ( 48%)]  Loss: 3.532 (3.42)  Time: 0.672s, 1523.45/s  (0.695s, 1472.88/s)  LR: 1.661e-04  Data: 0.010 (0.014)
Train: 444 [ 650/1251 ( 52%)]  Loss: 3.509 (3.43)  Time: 0.675s, 1516.53/s  (0.695s, 1473.34/s)  LR: 1.661e-04  Data: 0.009 (0.013)
Train: 444 [ 700/1251 ( 56%)]  Loss: 3.736 (3.45)  Time: 0.717s, 1427.91/s  (0.695s, 1472.99/s)  LR: 1.661e-04  Data: 0.009 (0.013)
Train: 444 [ 750/1251 ( 60%)]  Loss: 3.607 (3.46)  Time: 0.672s, 1523.70/s  (0.695s, 1473.13/s)  LR: 1.661e-04  Data: 0.010 (0.013)
Train: 444 [ 800/1251 ( 64%)]  Loss: 3.242 (3.44)  Time: 0.712s, 1438.82/s  (0.694s, 1474.46/s)  LR: 1.661e-04  Data: 0.011 (0.013)
Train: 444 [ 850/1251 ( 68%)]  Loss: 3.385 (3.44)  Time: 0.706s, 1449.71/s  (0.695s, 1473.65/s)  LR: 1.661e-04  Data: 0.010 (0.013)
Train: 444 [ 900/1251 ( 72%)]  Loss: 3.594 (3.45)  Time: 0.702s, 1459.19/s  (0.694s, 1474.51/s)  LR: 1.661e-04  Data: 0.011 (0.012)
Train: 444 [ 950/1251 ( 76%)]  Loss: 3.361 (3.44)  Time: 0.673s, 1521.24/s  (0.694s, 1475.43/s)  LR: 1.661e-04  Data: 0.011 (0.012)
Train: 444 [1000/1251 ( 80%)]  Loss: 3.390 (3.44)  Time: 0.672s, 1524.63/s  (0.694s, 1475.36/s)  LR: 1.661e-04  Data: 0.009 (0.012)
Train: 444 [1050/1251 ( 84%)]  Loss: 3.516 (3.45)  Time: 0.674s, 1518.31/s  (0.694s, 1474.88/s)  LR: 1.661e-04  Data: 0.011 (0.012)
Train: 444 [1100/1251 ( 88%)]  Loss: 3.277 (3.44)  Time: 0.673s, 1520.56/s  (0.694s, 1475.95/s)  LR: 1.661e-04  Data: 0.010 (0.012)
Train: 444 [1150/1251 ( 92%)]  Loss: 3.233 (3.43)  Time: 0.701s, 1460.95/s  (0.694s, 1476.00/s)  LR: 1.661e-04  Data: 0.011 (0.012)
Train: 444 [1200/1251 ( 96%)]  Loss: 3.759 (3.44)  Time: 0.718s, 1425.45/s  (0.693s, 1476.75/s)  LR: 1.661e-04  Data: 0.011 (0.012)
Train: 444 [1250/1251 (100%)]  Loss: 3.384 (3.44)  Time: 0.656s, 1560.31/s  (0.694s, 1476.50/s)  LR: 1.661e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.612 (1.612)  Loss:  0.6396 (0.6396)  Acc@1: 90.8203 (90.8203)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.136 (0.576)  Loss:  0.8091 (1.2005)  Acc@1: 85.3774 (76.9560)  Acc@5: 97.4057 (93.4440)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-438.pth.tar', 77.02600010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-444.pth.tar', 76.95599998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-442.pth.tar', 76.86000008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-441.pth.tar', 76.77999998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-440.pth.tar', 76.65399995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-433.pth.tar', 76.6480000366211)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-430.pth.tar', 76.64400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-432.pth.tar', 76.63000003417969)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-439.pth.tar', 76.52400006103515)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-425.pth.tar', 76.51800018554688)

Train: 445 [   0/1251 (  0%)]  Loss: 3.401 (3.40)  Time: 2.289s,  447.35/s  (2.289s,  447.35/s)  LR: 1.643e-04  Data: 1.631 (1.631)
Train: 445 [  50/1251 (  4%)]  Loss: 3.529 (3.46)  Time: 0.688s, 1487.60/s  (0.729s, 1404.56/s)  LR: 1.643e-04  Data: 0.014 (0.051)
Train: 445 [ 100/1251 (  8%)]  Loss: 3.043 (3.32)  Time: 0.671s, 1526.53/s  (0.715s, 1432.99/s)  LR: 1.643e-04  Data: 0.010 (0.031)
Train: 445 [ 150/1251 ( 12%)]  Loss: 3.273 (3.31)  Time: 0.704s, 1455.28/s  (0.707s, 1447.88/s)  LR: 1.643e-04  Data: 0.009 (0.024)
Train: 445 [ 200/1251 ( 16%)]  Loss: 3.456 (3.34)  Time: 0.699s, 1464.26/s  (0.702s, 1458.01/s)  LR: 1.643e-04  Data: 0.009 (0.021)
Train: 445 [ 250/1251 ( 20%)]  Loss: 3.276 (3.33)  Time: 0.671s, 1526.62/s  (0.700s, 1461.95/s)  LR: 1.643e-04  Data: 0.010 (0.019)
Train: 445 [ 300/1251 ( 24%)]  Loss: 2.995 (3.28)  Time: 0.675s, 1517.05/s  (0.699s, 1464.81/s)  LR: 1.643e-04  Data: 0.010 (0.017)
Train: 445 [ 350/1251 ( 28%)]  Loss: 3.841 (3.35)  Time: 0.669s, 1531.49/s  (0.697s, 1468.62/s)  LR: 1.643e-04  Data: 0.009 (0.016)
Train: 445 [ 400/1251 ( 32%)]  Loss: 3.522 (3.37)  Time: 0.679s, 1508.94/s  (0.696s, 1470.82/s)  LR: 1.643e-04  Data: 0.012 (0.016)
Train: 445 [ 450/1251 ( 36%)]  Loss: 3.543 (3.39)  Time: 0.672s, 1523.11/s  (0.695s, 1473.16/s)  LR: 1.643e-04  Data: 0.010 (0.015)
Train: 445 [ 500/1251 ( 40%)]  Loss: 3.272 (3.38)  Time: 0.675s, 1517.01/s  (0.695s, 1473.85/s)  LR: 1.643e-04  Data: 0.009 (0.015)
Train: 445 [ 550/1251 ( 44%)]  Loss: 3.678 (3.40)  Time: 0.678s, 1510.22/s  (0.694s, 1475.00/s)  LR: 1.643e-04  Data: 0.010 (0.014)
Train: 445 [ 600/1251 ( 48%)]  Loss: 3.527 (3.41)  Time: 0.702s, 1458.07/s  (0.694s, 1475.19/s)  LR: 1.643e-04  Data: 0.009 (0.014)
Train: 445 [ 650/1251 ( 52%)]  Loss: 3.364 (3.41)  Time: 0.674s, 1518.92/s  (0.694s, 1475.20/s)  LR: 1.643e-04  Data: 0.010 (0.014)
Train: 445 [ 700/1251 ( 56%)]  Loss: 3.386 (3.41)  Time: 0.674s, 1518.73/s  (0.694s, 1475.06/s)  LR: 1.643e-04  Data: 0.011 (0.013)
Train: 445 [ 750/1251 ( 60%)]  Loss: 3.329 (3.40)  Time: 0.669s, 1530.59/s  (0.694s, 1474.70/s)  LR: 1.643e-04  Data: 0.009 (0.013)
Train: 445 [ 800/1251 ( 64%)]  Loss: 3.491 (3.41)  Time: 0.685s, 1495.24/s  (0.694s, 1475.09/s)  LR: 1.643e-04  Data: 0.011 (0.013)
Train: 445 [ 850/1251 ( 68%)]  Loss: 3.405 (3.41)  Time: 0.733s, 1396.35/s  (0.694s, 1475.41/s)  LR: 1.643e-04  Data: 0.009 (0.013)
Train: 445 [ 900/1251 ( 72%)]  Loss: 3.203 (3.40)  Time: 0.683s, 1498.97/s  (0.694s, 1474.92/s)  LR: 1.643e-04  Data: 0.012 (0.013)
Train: 445 [ 950/1251 ( 76%)]  Loss: 3.498 (3.40)  Time: 0.669s, 1530.67/s  (0.694s, 1475.15/s)  LR: 1.643e-04  Data: 0.012 (0.013)
Train: 445 [1000/1251 ( 80%)]  Loss: 3.610 (3.41)  Time: 0.675s, 1517.12/s  (0.694s, 1475.60/s)  LR: 1.643e-04  Data: 0.010 (0.013)
Train: 445 [1050/1251 ( 84%)]  Loss: 3.496 (3.42)  Time: 0.702s, 1458.43/s  (0.694s, 1475.38/s)  LR: 1.643e-04  Data: 0.009 (0.012)
Train: 445 [1100/1251 ( 88%)]  Loss: 3.409 (3.42)  Time: 0.669s, 1530.16/s  (0.694s, 1476.12/s)  LR: 1.643e-04  Data: 0.009 (0.012)
Train: 445 [1150/1251 ( 92%)]  Loss: 3.520 (3.42)  Time: 0.672s, 1522.71/s  (0.694s, 1476.55/s)  LR: 1.643e-04  Data: 0.010 (0.012)
Train: 445 [1200/1251 ( 96%)]  Loss: 3.380 (3.42)  Time: 0.691s, 1481.11/s  (0.693s, 1476.86/s)  LR: 1.643e-04  Data: 0.013 (0.012)
Train: 445 [1250/1251 (100%)]  Loss: 3.763 (3.43)  Time: 0.660s, 1552.55/s  (0.693s, 1476.92/s)  LR: 1.643e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.501 (1.501)  Loss:  0.8301 (0.8301)  Acc@1: 90.8203 (90.8203)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.136 (0.587)  Loss:  0.8433 (1.3387)  Acc@1: 86.7924 (76.4120)  Acc@5: 97.0519 (93.2480)
Train: 446 [   0/1251 (  0%)]  Loss: 3.935 (3.93)  Time: 2.217s,  461.80/s  (2.217s,  461.80/s)  LR: 1.624e-04  Data: 1.603 (1.603)
Train: 446 [  50/1251 (  4%)]  Loss: 3.448 (3.69)  Time: 0.676s, 1514.88/s  (0.725s, 1412.02/s)  LR: 1.624e-04  Data: 0.010 (0.048)
Train: 446 [ 100/1251 (  8%)]  Loss: 3.277 (3.55)  Time: 0.671s, 1526.75/s  (0.708s, 1446.02/s)  LR: 1.624e-04  Data: 0.011 (0.029)
Train: 446 [ 150/1251 ( 12%)]  Loss: 3.535 (3.55)  Time: 0.671s, 1525.43/s  (0.702s, 1459.40/s)  LR: 1.624e-04  Data: 0.011 (0.023)
Train: 446 [ 200/1251 ( 16%)]  Loss: 3.351 (3.51)  Time: 0.685s, 1495.56/s  (0.698s, 1466.96/s)  LR: 1.624e-04  Data: 0.011 (0.020)
Train: 446 [ 250/1251 ( 20%)]  Loss: 3.317 (3.48)  Time: 0.718s, 1426.14/s  (0.696s, 1470.73/s)  LR: 1.624e-04  Data: 0.011 (0.018)
Train: 446 [ 300/1251 ( 24%)]  Loss: 3.678 (3.51)  Time: 0.702s, 1457.81/s  (0.695s, 1472.35/s)  LR: 1.624e-04  Data: 0.015 (0.017)
Train: 446 [ 350/1251 ( 28%)]  Loss: 3.304 (3.48)  Time: 0.704s, 1453.94/s  (0.696s, 1470.89/s)  LR: 1.624e-04  Data: 0.009 (0.016)
Train: 446 [ 400/1251 ( 32%)]  Loss: 3.118 (3.44)  Time: 0.740s, 1383.12/s  (0.696s, 1471.32/s)  LR: 1.624e-04  Data: 0.012 (0.015)
Train: 446 [ 450/1251 ( 36%)]  Loss: 3.513 (3.45)  Time: 0.712s, 1438.14/s  (0.696s, 1471.75/s)  LR: 1.624e-04  Data: 0.009 (0.015)
Train: 446 [ 500/1251 ( 40%)]  Loss: 3.765 (3.48)  Time: 0.701s, 1459.83/s  (0.695s, 1472.98/s)  LR: 1.624e-04  Data: 0.009 (0.014)
Train: 446 [ 550/1251 ( 44%)]  Loss: 3.377 (3.47)  Time: 0.727s, 1409.12/s  (0.695s, 1472.47/s)  LR: 1.624e-04  Data: 0.011 (0.014)
Train: 446 [ 600/1251 ( 48%)]  Loss: 3.202 (3.45)  Time: 0.674s, 1519.97/s  (0.695s, 1473.17/s)  LR: 1.624e-04  Data: 0.010 (0.014)
Train: 446 [ 650/1251 ( 52%)]  Loss: 3.182 (3.43)  Time: 0.684s, 1497.94/s  (0.695s, 1473.69/s)  LR: 1.624e-04  Data: 0.013 (0.013)
Train: 446 [ 700/1251 ( 56%)]  Loss: 3.700 (3.45)  Time: 0.675s, 1517.84/s  (0.694s, 1474.85/s)  LR: 1.624e-04  Data: 0.010 (0.013)
Train: 446 [ 750/1251 ( 60%)]  Loss: 3.336 (3.44)  Time: 0.672s, 1523.80/s  (0.694s, 1475.79/s)  LR: 1.624e-04  Data: 0.009 (0.013)
Train: 446 [ 800/1251 ( 64%)]  Loss: 3.549 (3.45)  Time: 0.673s, 1521.46/s  (0.693s, 1476.90/s)  LR: 1.624e-04  Data: 0.013 (0.013)
Train: 446 [ 850/1251 ( 68%)]  Loss: 3.543 (3.45)  Time: 0.711s, 1440.69/s  (0.694s, 1476.27/s)  LR: 1.624e-04  Data: 0.010 (0.013)
Train: 446 [ 900/1251 ( 72%)]  Loss: 2.992 (3.43)  Time: 0.721s, 1419.27/s  (0.694s, 1476.36/s)  LR: 1.624e-04  Data: 0.010 (0.013)
Train: 446 [ 950/1251 ( 76%)]  Loss: 3.632 (3.44)  Time: 0.677s, 1511.82/s  (0.694s, 1476.06/s)  LR: 1.624e-04  Data: 0.010 (0.012)
Train: 446 [1000/1251 ( 80%)]  Loss: 3.537 (3.44)  Time: 0.673s, 1521.78/s  (0.694s, 1475.16/s)  LR: 1.624e-04  Data: 0.011 (0.012)
Train: 446 [1050/1251 ( 84%)]  Loss: 3.389 (3.44)  Time: 0.671s, 1526.41/s  (0.694s, 1475.30/s)  LR: 1.624e-04  Data: 0.008 (0.012)
Train: 446 [1100/1251 ( 88%)]  Loss: 3.605 (3.45)  Time: 0.755s, 1355.62/s  (0.694s, 1475.93/s)  LR: 1.624e-04  Data: 0.009 (0.012)
Train: 446 [1150/1251 ( 92%)]  Loss: 3.217 (3.44)  Time: 0.733s, 1397.75/s  (0.693s, 1476.62/s)  LR: 1.624e-04  Data: 0.010 (0.012)
Train: 446 [1200/1251 ( 96%)]  Loss: 3.260 (3.43)  Time: 0.673s, 1521.82/s  (0.693s, 1476.84/s)  LR: 1.624e-04  Data: 0.012 (0.012)
Train: 446 [1250/1251 (100%)]  Loss: 3.780 (3.44)  Time: 0.657s, 1559.09/s  (0.693s, 1476.85/s)  LR: 1.624e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.506 (1.506)  Loss:  0.8335 (0.8335)  Acc@1: 90.4297 (90.4297)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.137 (0.584)  Loss:  0.9932 (1.3772)  Acc@1: 85.6132 (76.5780)  Acc@5: 96.9340 (93.3120)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-438.pth.tar', 77.02600010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-444.pth.tar', 76.95599998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-442.pth.tar', 76.86000008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-441.pth.tar', 76.77999998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-440.pth.tar', 76.65399995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-433.pth.tar', 76.6480000366211)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-430.pth.tar', 76.64400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-432.pth.tar', 76.63000003417969)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-446.pth.tar', 76.57800008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-439.pth.tar', 76.52400006103515)

Train: 447 [   0/1251 (  0%)]  Loss: 3.370 (3.37)  Time: 2.276s,  449.83/s  (2.276s,  449.83/s)  LR: 1.605e-04  Data: 1.621 (1.621)
Train: 447 [  50/1251 (  4%)]  Loss: 3.335 (3.35)  Time: 0.704s, 1453.60/s  (0.731s, 1400.43/s)  LR: 1.605e-04  Data: 0.009 (0.048)
Train: 447 [ 100/1251 (  8%)]  Loss: 3.483 (3.40)  Time: 0.674s, 1519.07/s  (0.710s, 1443.03/s)  LR: 1.605e-04  Data: 0.011 (0.029)
Train: 447 [ 150/1251 ( 12%)]  Loss: 3.590 (3.44)  Time: 0.763s, 1341.98/s  (0.704s, 1453.55/s)  LR: 1.605e-04  Data: 0.010 (0.023)
Train: 447 [ 200/1251 ( 16%)]  Loss: 3.539 (3.46)  Time: 0.672s, 1524.91/s  (0.701s, 1460.40/s)  LR: 1.605e-04  Data: 0.012 (0.020)
Train: 447 [ 250/1251 ( 20%)]  Loss: 3.448 (3.46)  Time: 0.673s, 1522.66/s  (0.699s, 1464.30/s)  LR: 1.605e-04  Data: 0.010 (0.018)
Train: 447 [ 300/1251 ( 24%)]  Loss: 3.725 (3.50)  Time: 0.672s, 1523.37/s  (0.697s, 1469.10/s)  LR: 1.605e-04  Data: 0.010 (0.017)
Train: 447 [ 350/1251 ( 28%)]  Loss: 3.337 (3.48)  Time: 0.710s, 1442.89/s  (0.696s, 1471.15/s)  LR: 1.605e-04  Data: 0.009 (0.016)
Train: 447 [ 400/1251 ( 32%)]  Loss: 3.426 (3.47)  Time: 0.710s, 1442.72/s  (0.695s, 1473.13/s)  LR: 1.605e-04  Data: 0.011 (0.015)
Train: 447 [ 450/1251 ( 36%)]  Loss: 3.105 (3.44)  Time: 0.755s, 1356.20/s  (0.695s, 1473.93/s)  LR: 1.605e-04  Data: 0.010 (0.015)
Train: 447 [ 500/1251 ( 40%)]  Loss: 3.721 (3.46)  Time: 0.670s, 1528.05/s  (0.695s, 1473.62/s)  LR: 1.605e-04  Data: 0.010 (0.014)
Train: 447 [ 550/1251 ( 44%)]  Loss: 3.702 (3.48)  Time: 0.707s, 1447.70/s  (0.695s, 1474.40/s)  LR: 1.605e-04  Data: 0.009 (0.014)
Train: 447 [ 600/1251 ( 48%)]  Loss: 3.192 (3.46)  Time: 0.706s, 1449.48/s  (0.695s, 1473.49/s)  LR: 1.605e-04  Data: 0.010 (0.014)
Train: 447 [ 650/1251 ( 52%)]  Loss: 3.336 (3.45)  Time: 0.694s, 1475.11/s  (0.696s, 1472.10/s)  LR: 1.605e-04  Data: 0.010 (0.013)
Train: 447 [ 700/1251 ( 56%)]  Loss: 3.308 (3.44)  Time: 0.713s, 1436.65/s  (0.695s, 1473.19/s)  LR: 1.605e-04  Data: 0.010 (0.013)
Train: 447 [ 750/1251 ( 60%)]  Loss: 3.377 (3.44)  Time: 0.737s, 1390.19/s  (0.695s, 1472.72/s)  LR: 1.605e-04  Data: 0.010 (0.013)
Train: 447 [ 800/1251 ( 64%)]  Loss: 3.368 (3.43)  Time: 0.670s, 1527.57/s  (0.695s, 1473.18/s)  LR: 1.605e-04  Data: 0.010 (0.013)
Train: 447 [ 850/1251 ( 68%)]  Loss: 3.537 (3.44)  Time: 0.672s, 1524.28/s  (0.695s, 1473.27/s)  LR: 1.605e-04  Data: 0.010 (0.013)
Train: 447 [ 900/1251 ( 72%)]  Loss: 3.294 (3.43)  Time: 0.701s, 1460.28/s  (0.695s, 1474.41/s)  LR: 1.605e-04  Data: 0.009 (0.013)
Train: 447 [ 950/1251 ( 76%)]  Loss: 3.365 (3.43)  Time: 0.687s, 1490.87/s  (0.694s, 1474.86/s)  LR: 1.605e-04  Data: 0.009 (0.012)
Train: 447 [1000/1251 ( 80%)]  Loss: 3.373 (3.43)  Time: 0.711s, 1439.36/s  (0.694s, 1475.35/s)  LR: 1.605e-04  Data: 0.010 (0.012)
Train: 447 [1050/1251 ( 84%)]  Loss: 3.441 (3.43)  Time: 0.682s, 1500.59/s  (0.694s, 1475.98/s)  LR: 1.605e-04  Data: 0.010 (0.012)
Train: 447 [1100/1251 ( 88%)]  Loss: 3.499 (3.43)  Time: 0.671s, 1526.99/s  (0.694s, 1476.44/s)  LR: 1.605e-04  Data: 0.010 (0.012)
Train: 447 [1150/1251 ( 92%)]  Loss: 3.067 (3.41)  Time: 0.676s, 1515.19/s  (0.693s, 1476.92/s)  LR: 1.605e-04  Data: 0.009 (0.012)
Train: 447 [1200/1251 ( 96%)]  Loss: 3.129 (3.40)  Time: 0.678s, 1511.19/s  (0.693s, 1477.07/s)  LR: 1.605e-04  Data: 0.011 (0.012)
Train: 447 [1250/1251 (100%)]  Loss: 3.324 (3.40)  Time: 0.656s, 1561.53/s  (0.693s, 1477.03/s)  LR: 1.605e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.546 (1.546)  Loss:  0.7129 (0.7129)  Acc@1: 90.8203 (90.8203)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.136 (0.585)  Loss:  0.8740 (1.2452)  Acc@1: 86.2028 (76.7600)  Acc@5: 96.3443 (93.4560)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-438.pth.tar', 77.02600010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-444.pth.tar', 76.95599998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-442.pth.tar', 76.86000008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-441.pth.tar', 76.77999998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-447.pth.tar', 76.76000008300781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-440.pth.tar', 76.65399995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-433.pth.tar', 76.6480000366211)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-430.pth.tar', 76.64400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-432.pth.tar', 76.63000003417969)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-446.pth.tar', 76.57800008544922)

Train: 448 [   0/1251 (  0%)]  Loss: 3.540 (3.54)  Time: 2.109s,  485.64/s  (2.109s,  485.64/s)  LR: 1.587e-04  Data: 1.454 (1.454)
Train: 448 [  50/1251 (  4%)]  Loss: 3.747 (3.64)  Time: 0.680s, 1504.81/s  (0.726s, 1411.11/s)  LR: 1.587e-04  Data: 0.010 (0.047)
Train: 448 [ 100/1251 (  8%)]  Loss: 3.510 (3.60)  Time: 0.674s, 1518.43/s  (0.707s, 1448.92/s)  LR: 1.587e-04  Data: 0.010 (0.029)
Train: 448 [ 150/1251 ( 12%)]  Loss: 3.579 (3.59)  Time: 0.690s, 1484.80/s  (0.699s, 1464.55/s)  LR: 1.587e-04  Data: 0.013 (0.023)
Train: 448 [ 200/1251 ( 16%)]  Loss: 3.294 (3.53)  Time: 0.739s, 1385.80/s  (0.698s, 1466.44/s)  LR: 1.587e-04  Data: 0.011 (0.020)
Train: 448 [ 250/1251 ( 20%)]  Loss: 3.149 (3.47)  Time: 0.683s, 1498.88/s  (0.697s, 1469.20/s)  LR: 1.587e-04  Data: 0.011 (0.018)
Train: 448 [ 300/1251 ( 24%)]  Loss: 3.673 (3.50)  Time: 0.672s, 1523.53/s  (0.697s, 1469.62/s)  LR: 1.587e-04  Data: 0.010 (0.017)
Train: 448 [ 350/1251 ( 28%)]  Loss: 3.481 (3.50)  Time: 0.676s, 1515.01/s  (0.696s, 1470.75/s)  LR: 1.587e-04  Data: 0.011 (0.016)
Train: 448 [ 400/1251 ( 32%)]  Loss: 3.662 (3.51)  Time: 0.683s, 1499.35/s  (0.694s, 1474.78/s)  LR: 1.587e-04  Data: 0.011 (0.015)
Train: 448 [ 450/1251 ( 36%)]  Loss: 2.966 (3.46)  Time: 0.702s, 1459.59/s  (0.694s, 1475.50/s)  LR: 1.587e-04  Data: 0.008 (0.014)
Train: 448 [ 500/1251 ( 40%)]  Loss: 3.533 (3.47)  Time: 0.703s, 1457.42/s  (0.695s, 1473.37/s)  LR: 1.587e-04  Data: 0.010 (0.014)
Train: 448 [ 550/1251 ( 44%)]  Loss: 3.586 (3.48)  Time: 0.672s, 1523.08/s  (0.694s, 1474.87/s)  LR: 1.587e-04  Data: 0.011 (0.014)
Train: 448 [ 600/1251 ( 48%)]  Loss: 3.141 (3.45)  Time: 0.673s, 1521.55/s  (0.694s, 1475.75/s)  LR: 1.587e-04  Data: 0.012 (0.013)
Train: 448 [ 650/1251 ( 52%)]  Loss: 3.446 (3.45)  Time: 0.680s, 1506.09/s  (0.694s, 1475.15/s)  LR: 1.587e-04  Data: 0.010 (0.013)
Train: 448 [ 700/1251 ( 56%)]  Loss: 3.499 (3.45)  Time: 0.710s, 1443.23/s  (0.694s, 1475.07/s)  LR: 1.587e-04  Data: 0.011 (0.013)
Train: 448 [ 750/1251 ( 60%)]  Loss: 3.491 (3.46)  Time: 0.709s, 1443.77/s  (0.694s, 1475.80/s)  LR: 1.587e-04  Data: 0.013 (0.013)
Train: 448 [ 800/1251 ( 64%)]  Loss: 3.155 (3.44)  Time: 0.706s, 1449.86/s  (0.694s, 1476.15/s)  LR: 1.587e-04  Data: 0.010 (0.013)
Train: 448 [ 850/1251 ( 68%)]  Loss: 3.291 (3.43)  Time: 0.673s, 1520.98/s  (0.694s, 1476.50/s)  LR: 1.587e-04  Data: 0.011 (0.013)
Train: 448 [ 900/1251 ( 72%)]  Loss: 3.099 (3.41)  Time: 0.671s, 1525.19/s  (0.693s, 1476.82/s)  LR: 1.587e-04  Data: 0.010 (0.012)
Train: 448 [ 950/1251 ( 76%)]  Loss: 3.263 (3.41)  Time: 0.700s, 1463.56/s  (0.694s, 1475.95/s)  LR: 1.587e-04  Data: 0.012 (0.012)
Train: 448 [1000/1251 ( 80%)]  Loss: 3.473 (3.41)  Time: 0.692s, 1479.73/s  (0.695s, 1473.74/s)  LR: 1.587e-04  Data: 0.011 (0.012)
Train: 448 [1050/1251 ( 84%)]  Loss: 3.754 (3.42)  Time: 0.690s, 1483.95/s  (0.696s, 1472.00/s)  LR: 1.587e-04  Data: 0.028 (0.012)
Train: 448 [1100/1251 ( 88%)]  Loss: 3.621 (3.43)  Time: 0.739s, 1386.11/s  (0.696s, 1470.93/s)  LR: 1.587e-04  Data: 0.009 (0.012)
Train: 448 [1150/1251 ( 92%)]  Loss: 3.479 (3.43)  Time: 0.687s, 1491.07/s  (0.696s, 1471.32/s)  LR: 1.587e-04  Data: 0.010 (0.012)
Train: 448 [1200/1251 ( 96%)]  Loss: 3.547 (3.44)  Time: 0.675s, 1518.16/s  (0.695s, 1473.11/s)  LR: 1.587e-04  Data: 0.010 (0.012)
Train: 448 [1250/1251 (100%)]  Loss: 3.539 (3.44)  Time: 0.657s, 1559.76/s  (0.694s, 1475.08/s)  LR: 1.587e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.458 (1.458)  Loss:  0.7104 (0.7104)  Acc@1: 91.0156 (91.0156)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.136 (0.578)  Loss:  0.8809 (1.2250)  Acc@1: 85.9670 (76.9400)  Acc@5: 96.2264 (93.2840)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-438.pth.tar', 77.02600010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-444.pth.tar', 76.95599998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-448.pth.tar', 76.94000010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-442.pth.tar', 76.86000008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-441.pth.tar', 76.77999998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-447.pth.tar', 76.76000008300781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-440.pth.tar', 76.65399995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-433.pth.tar', 76.6480000366211)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-430.pth.tar', 76.64400000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-432.pth.tar', 76.63000003417969)

Train: 449 [   0/1251 (  0%)]  Loss: 3.449 (3.45)  Time: 2.264s,  452.28/s  (2.264s,  452.28/s)  LR: 1.568e-04  Data: 1.609 (1.609)
Train: 449 [  50/1251 (  4%)]  Loss: 3.669 (3.56)  Time: 0.669s, 1531.04/s  (0.730s, 1401.85/s)  LR: 1.568e-04  Data: 0.011 (0.047)
Train: 449 [ 100/1251 (  8%)]  Loss: 3.329 (3.48)  Time: 0.679s, 1508.76/s  (0.712s, 1438.95/s)  LR: 1.568e-04  Data: 0.010 (0.029)
Train: 449 [ 150/1251 ( 12%)]  Loss: 3.570 (3.50)  Time: 0.700s, 1462.17/s  (0.707s, 1448.73/s)  LR: 1.568e-04  Data: 0.009 (0.023)
Train: 449 [ 200/1251 ( 16%)]  Loss: 3.038 (3.41)  Time: 0.705s, 1453.20/s  (0.705s, 1452.46/s)  LR: 1.568e-04  Data: 0.009 (0.020)
Train: 449 [ 250/1251 ( 20%)]  Loss: 3.151 (3.37)  Time: 0.747s, 1369.90/s  (0.702s, 1459.22/s)  LR: 1.568e-04  Data: 0.009 (0.018)
Train: 449 [ 300/1251 ( 24%)]  Loss: 3.631 (3.41)  Time: 0.713s, 1435.95/s  (0.700s, 1463.71/s)  LR: 1.568e-04  Data: 0.010 (0.017)
Train: 449 [ 350/1251 ( 28%)]  Loss: 3.310 (3.39)  Time: 0.675s, 1518.15/s  (0.698s, 1466.72/s)  LR: 1.568e-04  Data: 0.009 (0.016)
Train: 449 [ 400/1251 ( 32%)]  Loss: 3.348 (3.39)  Time: 0.699s, 1464.98/s  (0.698s, 1466.76/s)  LR: 1.568e-04  Data: 0.009 (0.015)
Train: 449 [ 450/1251 ( 36%)]  Loss: 2.954 (3.34)  Time: 0.703s, 1457.50/s  (0.697s, 1470.18/s)  LR: 1.568e-04  Data: 0.009 (0.015)
Train: 449 [ 500/1251 ( 40%)]  Loss: 3.087 (3.32)  Time: 0.689s, 1486.62/s  (0.696s, 1471.17/s)  LR: 1.568e-04  Data: 0.010 (0.014)
Train: 449 [ 550/1251 ( 44%)]  Loss: 3.531 (3.34)  Time: 0.672s, 1522.80/s  (0.696s, 1471.99/s)  LR: 1.568e-04  Data: 0.010 (0.014)
Train: 449 [ 600/1251 ( 48%)]  Loss: 3.547 (3.36)  Time: 0.692s, 1479.85/s  (0.696s, 1470.75/s)  LR: 1.568e-04  Data: 0.009 (0.014)
Train: 449 [ 650/1251 ( 52%)]  Loss: 3.441 (3.36)  Time: 0.676s, 1515.54/s  (0.696s, 1471.24/s)  LR: 1.568e-04  Data: 0.010 (0.013)
Train: 449 [ 700/1251 ( 56%)]  Loss: 3.595 (3.38)  Time: 0.714s, 1434.44/s  (0.696s, 1472.05/s)  LR: 1.568e-04  Data: 0.011 (0.013)
Train: 449 [ 750/1251 ( 60%)]  Loss: 3.483 (3.38)  Time: 0.702s, 1458.59/s  (0.695s, 1472.79/s)  LR: 1.568e-04  Data: 0.012 (0.013)
Train: 449 [ 800/1251 ( 64%)]  Loss: 3.272 (3.38)  Time: 0.683s, 1499.52/s  (0.695s, 1472.96/s)  LR: 1.568e-04  Data: 0.013 (0.013)
Train: 449 [ 850/1251 ( 68%)]  Loss: 3.629 (3.39)  Time: 0.674s, 1518.59/s  (0.695s, 1474.25/s)  LR: 1.568e-04  Data: 0.010 (0.013)
Train: 449 [ 900/1251 ( 72%)]  Loss: 3.247 (3.38)  Time: 0.669s, 1530.35/s  (0.694s, 1474.98/s)  LR: 1.568e-04  Data: 0.011 (0.013)
Train: 449 [ 950/1251 ( 76%)]  Loss: 3.589 (3.39)  Time: 0.722s, 1417.97/s  (0.694s, 1475.37/s)  LR: 1.568e-04  Data: 0.008 (0.012)
Train: 449 [1000/1251 ( 80%)]  Loss: 3.436 (3.40)  Time: 0.677s, 1513.05/s  (0.694s, 1475.42/s)  LR: 1.568e-04  Data: 0.011 (0.012)
Train: 449 [1050/1251 ( 84%)]  Loss: 3.326 (3.39)  Time: 0.692s, 1480.29/s  (0.694s, 1475.66/s)  LR: 1.568e-04  Data: 0.008 (0.012)
Train: 449 [1100/1251 ( 88%)]  Loss: 3.519 (3.40)  Time: 0.675s, 1516.84/s  (0.694s, 1475.80/s)  LR: 1.568e-04  Data: 0.010 (0.012)
Train: 449 [1150/1251 ( 92%)]  Loss: 3.025 (3.38)  Time: 0.712s, 1438.45/s  (0.694s, 1475.66/s)  LR: 1.568e-04  Data: 0.009 (0.012)
Train: 449 [1200/1251 ( 96%)]  Loss: 3.333 (3.38)  Time: 0.719s, 1424.04/s  (0.694s, 1476.00/s)  LR: 1.568e-04  Data: 0.011 (0.012)
Train: 449 [1250/1251 (100%)]  Loss: 3.464 (3.38)  Time: 0.659s, 1553.60/s  (0.693s, 1476.80/s)  LR: 1.568e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.544 (1.544)  Loss:  0.7427 (0.7427)  Acc@1: 91.2109 (91.2109)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.136 (0.587)  Loss:  0.8301 (1.2604)  Acc@1: 85.6132 (76.8580)  Acc@5: 97.1698 (93.3600)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-438.pth.tar', 77.02600010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-444.pth.tar', 76.95599998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-448.pth.tar', 76.94000010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-442.pth.tar', 76.86000008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-449.pth.tar', 76.85800008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-441.pth.tar', 76.77999998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-447.pth.tar', 76.76000008300781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-440.pth.tar', 76.65399995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-433.pth.tar', 76.6480000366211)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-430.pth.tar', 76.64400000732422)

Train: 450 [   0/1251 (  0%)]  Loss: 3.606 (3.61)  Time: 2.231s,  459.05/s  (2.231s,  459.05/s)  LR: 1.550e-04  Data: 1.615 (1.615)
Train: 450 [  50/1251 (  4%)]  Loss: 3.619 (3.61)  Time: 0.705s, 1453.26/s  (0.736s, 1390.65/s)  LR: 1.550e-04  Data: 0.009 (0.050)
Train: 450 [ 100/1251 (  8%)]  Loss: 3.406 (3.54)  Time: 0.686s, 1492.78/s  (0.712s, 1438.01/s)  LR: 1.550e-04  Data: 0.010 (0.031)
Train: 450 [ 150/1251 ( 12%)]  Loss: 3.296 (3.48)  Time: 0.672s, 1523.04/s  (0.704s, 1455.12/s)  LR: 1.550e-04  Data: 0.011 (0.024)
Train: 450 [ 200/1251 ( 16%)]  Loss: 3.042 (3.39)  Time: 0.703s, 1456.91/s  (0.703s, 1456.01/s)  LR: 1.550e-04  Data: 0.010 (0.021)
Train: 450 [ 250/1251 ( 20%)]  Loss: 3.245 (3.37)  Time: 0.717s, 1428.47/s  (0.701s, 1460.69/s)  LR: 1.550e-04  Data: 0.009 (0.019)
Train: 450 [ 300/1251 ( 24%)]  Loss: 3.716 (3.42)  Time: 0.678s, 1511.08/s  (0.699s, 1465.11/s)  LR: 1.550e-04  Data: 0.011 (0.017)
Train: 450 [ 350/1251 ( 28%)]  Loss: 3.292 (3.40)  Time: 0.673s, 1521.35/s  (0.699s, 1465.53/s)  LR: 1.550e-04  Data: 0.011 (0.016)
Train: 450 [ 400/1251 ( 32%)]  Loss: 3.681 (3.43)  Time: 0.672s, 1524.07/s  (0.698s, 1466.32/s)  LR: 1.550e-04  Data: 0.009 (0.016)
Train: 450 [ 450/1251 ( 36%)]  Loss: 3.617 (3.45)  Time: 0.674s, 1518.68/s  (0.697s, 1468.71/s)  LR: 1.550e-04  Data: 0.011 (0.015)
Train: 450 [ 500/1251 ( 40%)]  Loss: 3.505 (3.46)  Time: 0.706s, 1449.66/s  (0.697s, 1469.20/s)  LR: 1.550e-04  Data: 0.009 (0.015)
Train: 450 [ 550/1251 ( 44%)]  Loss: 3.045 (3.42)  Time: 0.672s, 1523.16/s  (0.696s, 1470.37/s)  LR: 1.550e-04  Data: 0.014 (0.014)
Train: 450 [ 600/1251 ( 48%)]  Loss: 3.024 (3.39)  Time: 0.737s, 1388.64/s  (0.696s, 1470.69/s)  LR: 1.550e-04  Data: 0.010 (0.014)
Train: 450 [ 650/1251 ( 52%)]  Loss: 3.284 (3.38)  Time: 0.704s, 1453.95/s  (0.696s, 1472.08/s)  LR: 1.550e-04  Data: 0.009 (0.014)
Train: 450 [ 700/1251 ( 56%)]  Loss: 3.094 (3.36)  Time: 0.778s, 1316.73/s  (0.696s, 1471.35/s)  LR: 1.550e-04  Data: 0.011 (0.013)
Train: 450 [ 750/1251 ( 60%)]  Loss: 3.392 (3.37)  Time: 0.711s, 1441.16/s  (0.696s, 1472.19/s)  LR: 1.550e-04  Data: 0.010 (0.013)
Train: 450 [ 800/1251 ( 64%)]  Loss: 3.242 (3.36)  Time: 0.681s, 1502.71/s  (0.695s, 1473.50/s)  LR: 1.550e-04  Data: 0.011 (0.013)
Train: 450 [ 850/1251 ( 68%)]  Loss: 3.072 (3.34)  Time: 0.689s, 1485.78/s  (0.695s, 1473.33/s)  LR: 1.550e-04  Data: 0.011 (0.013)
Train: 450 [ 900/1251 ( 72%)]  Loss: 3.609 (3.36)  Time: 0.673s, 1521.15/s  (0.694s, 1474.56/s)  LR: 1.550e-04  Data: 0.012 (0.013)
Train: 450 [ 950/1251 ( 76%)]  Loss: 3.749 (3.38)  Time: 0.671s, 1525.23/s  (0.694s, 1475.13/s)  LR: 1.550e-04  Data: 0.010 (0.013)
Train: 450 [1000/1251 ( 80%)]  Loss: 3.159 (3.37)  Time: 0.673s, 1521.06/s  (0.694s, 1475.53/s)  LR: 1.550e-04  Data: 0.011 (0.012)
Train: 450 [1050/1251 ( 84%)]  Loss: 3.321 (3.36)  Time: 0.673s, 1522.04/s  (0.694s, 1474.97/s)  LR: 1.550e-04  Data: 0.011 (0.012)
Train: 450 [1100/1251 ( 88%)]  Loss: 3.425 (3.37)  Time: 0.671s, 1525.76/s  (0.694s, 1475.52/s)  LR: 1.550e-04  Data: 0.012 (0.012)
Train: 450 [1150/1251 ( 92%)]  Loss: 3.285 (3.36)  Time: 0.691s, 1481.51/s  (0.694s, 1475.93/s)  LR: 1.550e-04  Data: 0.012 (0.012)
Train: 450 [1200/1251 ( 96%)]  Loss: 3.162 (3.36)  Time: 0.672s, 1524.46/s  (0.694s, 1475.19/s)  LR: 1.550e-04  Data: 0.010 (0.012)
Train: 450 [1250/1251 (100%)]  Loss: 3.140 (3.35)  Time: 0.657s, 1559.45/s  (0.694s, 1474.93/s)  LR: 1.550e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.531 (1.531)  Loss:  0.7881 (0.7881)  Acc@1: 91.1133 (91.1133)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.135 (0.577)  Loss:  0.9048 (1.2970)  Acc@1: 85.8491 (76.8500)  Acc@5: 96.8160 (93.4260)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-438.pth.tar', 77.02600010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-444.pth.tar', 76.95599998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-448.pth.tar', 76.94000010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-442.pth.tar', 76.86000008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-449.pth.tar', 76.85800008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-450.pth.tar', 76.84999992919921)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-441.pth.tar', 76.77999998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-447.pth.tar', 76.76000008300781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-440.pth.tar', 76.65399995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-433.pth.tar', 76.6480000366211)

Train: 451 [   0/1251 (  0%)]  Loss: 3.354 (3.35)  Time: 2.178s,  470.26/s  (2.178s,  470.26/s)  LR: 1.532e-04  Data: 1.561 (1.561)
Train: 451 [  50/1251 (  4%)]  Loss: 3.261 (3.31)  Time: 0.666s, 1538.54/s  (0.742s, 1380.15/s)  LR: 1.532e-04  Data: 0.010 (0.046)
Train: 451 [ 100/1251 (  8%)]  Loss: 3.432 (3.35)  Time: 0.720s, 1422.52/s  (0.714s, 1433.99/s)  LR: 1.532e-04  Data: 0.010 (0.028)
Train: 451 [ 150/1251 ( 12%)]  Loss: 3.430 (3.37)  Time: 0.720s, 1421.84/s  (0.706s, 1450.47/s)  LR: 1.532e-04  Data: 0.014 (0.022)
Train: 451 [ 200/1251 ( 16%)]  Loss: 3.269 (3.35)  Time: 0.666s, 1537.16/s  (0.702s, 1457.74/s)  LR: 1.532e-04  Data: 0.010 (0.019)
Train: 451 [ 250/1251 ( 20%)]  Loss: 3.611 (3.39)  Time: 0.719s, 1424.82/s  (0.700s, 1463.85/s)  LR: 1.532e-04  Data: 0.010 (0.018)
Train: 451 [ 300/1251 ( 24%)]  Loss: 3.452 (3.40)  Time: 0.679s, 1507.79/s  (0.698s, 1466.08/s)  LR: 1.532e-04  Data: 0.009 (0.016)
Train: 451 [ 350/1251 ( 28%)]  Loss: 3.653 (3.43)  Time: 0.709s, 1444.50/s  (0.698s, 1466.77/s)  LR: 1.532e-04  Data: 0.011 (0.016)
Train: 451 [ 400/1251 ( 32%)]  Loss: 3.230 (3.41)  Time: 0.670s, 1527.89/s  (0.698s, 1466.40/s)  LR: 1.532e-04  Data: 0.010 (0.015)
Train: 451 [ 450/1251 ( 36%)]  Loss: 3.251 (3.39)  Time: 0.771s, 1328.92/s  (0.698s, 1467.50/s)  LR: 1.532e-04  Data: 0.010 (0.014)
Train: 451 [ 500/1251 ( 40%)]  Loss: 3.670 (3.42)  Time: 0.716s, 1430.07/s  (0.697s, 1468.23/s)  LR: 1.532e-04  Data: 0.010 (0.014)
Train: 451 [ 550/1251 ( 44%)]  Loss: 3.043 (3.39)  Time: 0.672s, 1523.08/s  (0.698s, 1467.63/s)  LR: 1.532e-04  Data: 0.010 (0.014)
Train: 451 [ 600/1251 ( 48%)]  Loss: 3.470 (3.39)  Time: 0.670s, 1527.57/s  (0.697s, 1469.66/s)  LR: 1.532e-04  Data: 0.010 (0.013)
Train: 451 [ 650/1251 ( 52%)]  Loss: 3.387 (3.39)  Time: 0.735s, 1392.43/s  (0.696s, 1471.21/s)  LR: 1.532e-04  Data: 0.009 (0.013)
Train: 451 [ 700/1251 ( 56%)]  Loss: 3.647 (3.41)  Time: 0.671s, 1526.72/s  (0.696s, 1471.34/s)  LR: 1.532e-04  Data: 0.009 (0.013)
Train: 451 [ 750/1251 ( 60%)]  Loss: 3.710 (3.43)  Time: 0.671s, 1526.39/s  (0.696s, 1471.09/s)  LR: 1.532e-04  Data: 0.010 (0.013)
Train: 451 [ 800/1251 ( 64%)]  Loss: 3.507 (3.43)  Time: 0.687s, 1489.97/s  (0.696s, 1471.54/s)  LR: 1.532e-04  Data: 0.016 (0.013)
Train: 451 [ 850/1251 ( 68%)]  Loss: 3.396 (3.43)  Time: 0.673s, 1521.98/s  (0.696s, 1472.10/s)  LR: 1.532e-04  Data: 0.011 (0.013)
Train: 451 [ 900/1251 ( 72%)]  Loss: 3.730 (3.45)  Time: 0.672s, 1523.38/s  (0.695s, 1472.82/s)  LR: 1.532e-04  Data: 0.009 (0.012)
Train: 451 [ 950/1251 ( 76%)]  Loss: 3.280 (3.44)  Time: 0.668s, 1532.76/s  (0.695s, 1473.38/s)  LR: 1.532e-04  Data: 0.009 (0.012)
Train: 451 [1000/1251 ( 80%)]  Loss: 3.214 (3.43)  Time: 0.671s, 1525.30/s  (0.695s, 1473.79/s)  LR: 1.532e-04  Data: 0.010 (0.012)
Train: 451 [1050/1251 ( 84%)]  Loss: 3.396 (3.43)  Time: 0.702s, 1458.78/s  (0.695s, 1473.77/s)  LR: 1.532e-04  Data: 0.010 (0.012)
Train: 451 [1100/1251 ( 88%)]  Loss: 3.655 (3.44)  Time: 0.672s, 1523.27/s  (0.695s, 1473.11/s)  LR: 1.532e-04  Data: 0.009 (0.012)
Train: 451 [1150/1251 ( 92%)]  Loss: 3.696 (3.45)  Time: 0.670s, 1527.85/s  (0.695s, 1473.50/s)  LR: 1.532e-04  Data: 0.010 (0.012)
Train: 451 [1200/1251 ( 96%)]  Loss: 3.436 (3.45)  Time: 0.671s, 1525.38/s  (0.695s, 1474.14/s)  LR: 1.532e-04  Data: 0.010 (0.012)
Train: 451 [1250/1251 (100%)]  Loss: 3.531 (3.45)  Time: 0.656s, 1561.61/s  (0.694s, 1475.07/s)  LR: 1.532e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.549 (1.549)  Loss:  0.7622 (0.7622)  Acc@1: 91.5039 (91.5039)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.136 (0.588)  Loss:  0.8530 (1.2752)  Acc@1: 85.6132 (76.8820)  Acc@5: 96.4623 (93.2220)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-438.pth.tar', 77.02600010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-444.pth.tar', 76.95599998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-448.pth.tar', 76.94000010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-451.pth.tar', 76.88199995605468)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-442.pth.tar', 76.86000008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-449.pth.tar', 76.85800008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-450.pth.tar', 76.84999992919921)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-441.pth.tar', 76.77999998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-447.pth.tar', 76.76000008300781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-440.pth.tar', 76.65399995361328)

Train: 452 [   0/1251 (  0%)]  Loss: 3.734 (3.73)  Time: 2.334s,  438.76/s  (2.334s,  438.76/s)  LR: 1.513e-04  Data: 1.679 (1.679)
Train: 452 [  50/1251 (  4%)]  Loss: 3.362 (3.55)  Time: 0.677s, 1511.75/s  (0.737s, 1389.48/s)  LR: 1.513e-04  Data: 0.011 (0.052)
Train: 452 [ 100/1251 (  8%)]  Loss: 3.403 (3.50)  Time: 0.736s, 1391.32/s  (0.713s, 1437.12/s)  LR: 1.513e-04  Data: 0.009 (0.032)
Train: 452 [ 150/1251 ( 12%)]  Loss: 3.488 (3.50)  Time: 0.673s, 1520.65/s  (0.707s, 1448.47/s)  LR: 1.513e-04  Data: 0.011 (0.025)
Train: 452 [ 200/1251 ( 16%)]  Loss: 3.340 (3.47)  Time: 0.705s, 1452.86/s  (0.704s, 1455.13/s)  LR: 1.513e-04  Data: 0.009 (0.021)
Train: 452 [ 250/1251 ( 20%)]  Loss: 3.647 (3.50)  Time: 0.672s, 1523.69/s  (0.703s, 1457.56/s)  LR: 1.513e-04  Data: 0.010 (0.019)
Train: 452 [ 300/1251 ( 24%)]  Loss: 3.510 (3.50)  Time: 0.707s, 1448.14/s  (0.702s, 1459.31/s)  LR: 1.513e-04  Data: 0.009 (0.017)
Train: 452 [ 350/1251 ( 28%)]  Loss: 3.164 (3.46)  Time: 0.673s, 1520.67/s  (0.700s, 1462.94/s)  LR: 1.513e-04  Data: 0.011 (0.016)
Train: 452 [ 400/1251 ( 32%)]  Loss: 3.290 (3.44)  Time: 0.671s, 1525.05/s  (0.698s, 1466.71/s)  LR: 1.513e-04  Data: 0.010 (0.016)
Train: 452 [ 450/1251 ( 36%)]  Loss: 3.334 (3.43)  Time: 0.716s, 1429.55/s  (0.697s, 1468.10/s)  LR: 1.513e-04  Data: 0.010 (0.015)
Train: 452 [ 500/1251 ( 40%)]  Loss: 3.382 (3.42)  Time: 0.684s, 1496.61/s  (0.697s, 1469.48/s)  LR: 1.513e-04  Data: 0.011 (0.015)
Train: 452 [ 550/1251 ( 44%)]  Loss: 3.182 (3.40)  Time: 0.673s, 1522.17/s  (0.696s, 1471.30/s)  LR: 1.513e-04  Data: 0.010 (0.014)
Train: 452 [ 600/1251 ( 48%)]  Loss: 3.532 (3.41)  Time: 0.672s, 1523.51/s  (0.696s, 1471.48/s)  LR: 1.513e-04  Data: 0.012 (0.014)
Train: 452 [ 650/1251 ( 52%)]  Loss: 3.259 (3.40)  Time: 0.711s, 1439.94/s  (0.695s, 1472.43/s)  LR: 1.513e-04  Data: 0.010 (0.014)
Train: 452 [ 700/1251 ( 56%)]  Loss: 3.305 (3.40)  Time: 0.666s, 1537.53/s  (0.695s, 1473.27/s)  LR: 1.513e-04  Data: 0.010 (0.013)
Train: 452 [ 750/1251 ( 60%)]  Loss: 3.566 (3.41)  Time: 0.710s, 1442.98/s  (0.695s, 1473.92/s)  LR: 1.513e-04  Data: 0.009 (0.013)
Train: 452 [ 800/1251 ( 64%)]  Loss: 3.354 (3.40)  Time: 0.704s, 1453.78/s  (0.694s, 1474.87/s)  LR: 1.513e-04  Data: 0.011 (0.013)
Train: 452 [ 850/1251 ( 68%)]  Loss: 3.294 (3.40)  Time: 0.673s, 1522.63/s  (0.695s, 1474.39/s)  LR: 1.513e-04  Data: 0.012 (0.013)
Train: 452 [ 900/1251 ( 72%)]  Loss: 3.275 (3.39)  Time: 0.703s, 1456.46/s  (0.694s, 1475.57/s)  LR: 1.513e-04  Data: 0.009 (0.013)
Train: 452 [ 950/1251 ( 76%)]  Loss: 3.548 (3.40)  Time: 0.705s, 1452.34/s  (0.694s, 1475.70/s)  LR: 1.513e-04  Data: 0.010 (0.013)
Train: 452 [1000/1251 ( 80%)]  Loss: 3.406 (3.40)  Time: 0.681s, 1504.58/s  (0.694s, 1475.89/s)  LR: 1.513e-04  Data: 0.011 (0.012)
Train: 452 [1050/1251 ( 84%)]  Loss: 3.451 (3.40)  Time: 0.695s, 1472.70/s  (0.694s, 1476.26/s)  LR: 1.513e-04  Data: 0.013 (0.012)
Train: 452 [1100/1251 ( 88%)]  Loss: 3.365 (3.40)  Time: 0.673s, 1521.88/s  (0.693s, 1476.64/s)  LR: 1.513e-04  Data: 0.011 (0.012)
Train: 452 [1150/1251 ( 92%)]  Loss: 3.538 (3.41)  Time: 0.678s, 1510.18/s  (0.693s, 1476.93/s)  LR: 1.513e-04  Data: 0.009 (0.012)
Train: 452 [1200/1251 ( 96%)]  Loss: 3.544 (3.41)  Time: 0.680s, 1505.67/s  (0.693s, 1476.67/s)  LR: 1.513e-04  Data: 0.010 (0.012)
Train: 452 [1250/1251 (100%)]  Loss: 3.303 (3.41)  Time: 0.659s, 1554.49/s  (0.694s, 1476.47/s)  LR: 1.513e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.537 (1.537)  Loss:  0.7773 (0.7773)  Acc@1: 91.3086 (91.3086)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.136 (0.584)  Loss:  0.8848 (1.3097)  Acc@1: 86.7925 (76.6860)  Acc@5: 97.0519 (93.2600)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-438.pth.tar', 77.02600010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-444.pth.tar', 76.95599998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-448.pth.tar', 76.94000010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-451.pth.tar', 76.88199995605468)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-442.pth.tar', 76.86000008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-449.pth.tar', 76.85800008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-450.pth.tar', 76.84999992919921)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-441.pth.tar', 76.77999998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-447.pth.tar', 76.76000008300781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-452.pth.tar', 76.6860000805664)

Train: 453 [   0/1251 (  0%)]  Loss: 3.700 (3.70)  Time: 2.304s,  444.38/s  (2.304s,  444.38/s)  LR: 1.495e-04  Data: 1.670 (1.670)
Train: 453 [  50/1251 (  4%)]  Loss: 3.363 (3.53)  Time: 0.709s, 1444.26/s  (0.736s, 1392.15/s)  LR: 1.495e-04  Data: 0.009 (0.049)
Train: 453 [ 100/1251 (  8%)]  Loss: 3.300 (3.45)  Time: 0.698s, 1466.49/s  (0.716s, 1430.85/s)  LR: 1.495e-04  Data: 0.009 (0.030)
Train: 453 [ 150/1251 ( 12%)]  Loss: 3.437 (3.45)  Time: 0.686s, 1492.97/s  (0.708s, 1447.11/s)  LR: 1.495e-04  Data: 0.010 (0.024)
Train: 453 [ 200/1251 ( 16%)]  Loss: 3.639 (3.49)  Time: 0.733s, 1397.02/s  (0.704s, 1455.57/s)  LR: 1.495e-04  Data: 0.017 (0.020)
Train: 453 [ 250/1251 ( 20%)]  Loss: 3.335 (3.46)  Time: 0.707s, 1447.85/s  (0.700s, 1461.87/s)  LR: 1.495e-04  Data: 0.008 (0.018)
Train: 453 [ 300/1251 ( 24%)]  Loss: 3.315 (3.44)  Time: 0.797s, 1285.48/s  (0.699s, 1464.25/s)  LR: 1.495e-04  Data: 0.009 (0.017)
Train: 453 [ 350/1251 ( 28%)]  Loss: 3.493 (3.45)  Time: 0.705s, 1452.20/s  (0.698s, 1466.19/s)  LR: 1.495e-04  Data: 0.011 (0.016)
Train: 453 [ 400/1251 ( 32%)]  Loss: 3.024 (3.40)  Time: 0.710s, 1441.64/s  (0.699s, 1465.66/s)  LR: 1.495e-04  Data: 0.008 (0.015)
Train: 453 [ 450/1251 ( 36%)]  Loss: 3.327 (3.39)  Time: 0.701s, 1460.98/s  (0.699s, 1465.64/s)  LR: 1.495e-04  Data: 0.010 (0.015)
Train: 453 [ 500/1251 ( 40%)]  Loss: 3.243 (3.38)  Time: 0.710s, 1441.28/s  (0.699s, 1464.61/s)  LR: 1.495e-04  Data: 0.011 (0.014)
Train: 453 [ 550/1251 ( 44%)]  Loss: 3.438 (3.38)  Time: 0.687s, 1490.68/s  (0.699s, 1465.80/s)  LR: 1.495e-04  Data: 0.014 (0.014)
Train: 453 [ 600/1251 ( 48%)]  Loss: 3.067 (3.36)  Time: 0.731s, 1399.95/s  (0.699s, 1465.82/s)  LR: 1.495e-04  Data: 0.010 (0.014)
Train: 453 [ 650/1251 ( 52%)]  Loss: 3.404 (3.36)  Time: 0.671s, 1525.34/s  (0.698s, 1467.81/s)  LR: 1.495e-04  Data: 0.009 (0.014)
Train: 453 [ 700/1251 ( 56%)]  Loss: 3.399 (3.37)  Time: 0.724s, 1415.07/s  (0.697s, 1469.07/s)  LR: 1.495e-04  Data: 0.010 (0.013)
Train: 453 [ 750/1251 ( 60%)]  Loss: 3.423 (3.37)  Time: 0.671s, 1525.04/s  (0.697s, 1469.57/s)  LR: 1.495e-04  Data: 0.011 (0.013)
Train: 453 [ 800/1251 ( 64%)]  Loss: 3.483 (3.38)  Time: 0.705s, 1451.46/s  (0.696s, 1470.42/s)  LR: 1.495e-04  Data: 0.010 (0.013)
Train: 453 [ 850/1251 ( 68%)]  Loss: 3.077 (3.36)  Time: 0.671s, 1525.97/s  (0.696s, 1470.39/s)  LR: 1.495e-04  Data: 0.010 (0.013)
Train: 453 [ 900/1251 ( 72%)]  Loss: 3.696 (3.38)  Time: 0.694s, 1475.15/s  (0.696s, 1471.27/s)  LR: 1.495e-04  Data: 0.010 (0.013)
Train: 453 [ 950/1251 ( 76%)]  Loss: 3.148 (3.37)  Time: 0.667s, 1535.30/s  (0.696s, 1471.21/s)  LR: 1.495e-04  Data: 0.011 (0.013)
Train: 453 [1000/1251 ( 80%)]  Loss: 3.624 (3.38)  Time: 0.674s, 1518.66/s  (0.696s, 1472.24/s)  LR: 1.495e-04  Data: 0.010 (0.013)
Train: 453 [1050/1251 ( 84%)]  Loss: 3.494 (3.38)  Time: 0.671s, 1526.29/s  (0.695s, 1472.95/s)  LR: 1.495e-04  Data: 0.011 (0.012)
Train: 453 [1100/1251 ( 88%)]  Loss: 3.346 (3.38)  Time: 0.705s, 1452.12/s  (0.695s, 1473.72/s)  LR: 1.495e-04  Data: 0.009 (0.012)
Train: 453 [1150/1251 ( 92%)]  Loss: 2.988 (3.37)  Time: 0.714s, 1434.19/s  (0.695s, 1472.53/s)  LR: 1.495e-04  Data: 0.009 (0.012)
Train: 453 [1200/1251 ( 96%)]  Loss: 3.608 (3.37)  Time: 0.670s, 1529.40/s  (0.695s, 1472.99/s)  LR: 1.495e-04  Data: 0.011 (0.012)
Train: 453 [1250/1251 (100%)]  Loss: 3.261 (3.37)  Time: 0.659s, 1554.78/s  (0.695s, 1473.57/s)  LR: 1.495e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.459 (1.459)  Loss:  0.7173 (0.7173)  Acc@1: 90.7227 (90.7227)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.136 (0.583)  Loss:  0.8477 (1.2442)  Acc@1: 86.0849 (76.9720)  Acc@5: 96.1085 (93.4560)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-438.pth.tar', 77.02600010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-453.pth.tar', 76.97199990234375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-444.pth.tar', 76.95599998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-448.pth.tar', 76.94000010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-451.pth.tar', 76.88199995605468)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-442.pth.tar', 76.86000008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-449.pth.tar', 76.85800008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-450.pth.tar', 76.84999992919921)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-441.pth.tar', 76.77999998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-447.pth.tar', 76.76000008300781)

Train: 454 [   0/1251 (  0%)]  Loss: 3.474 (3.47)  Time: 2.263s,  452.56/s  (2.263s,  452.56/s)  LR: 1.477e-04  Data: 1.626 (1.626)
Train: 454 [  50/1251 (  4%)]  Loss: 3.361 (3.42)  Time: 0.673s, 1521.48/s  (0.729s, 1404.17/s)  LR: 1.477e-04  Data: 0.010 (0.052)
Train: 454 [ 100/1251 (  8%)]  Loss: 3.377 (3.40)  Time: 0.675s, 1516.16/s  (0.711s, 1439.66/s)  LR: 1.477e-04  Data: 0.009 (0.031)
Train: 454 [ 150/1251 ( 12%)]  Loss: 3.459 (3.42)  Time: 0.683s, 1498.31/s  (0.703s, 1456.65/s)  LR: 1.477e-04  Data: 0.010 (0.024)
Train: 454 [ 200/1251 ( 16%)]  Loss: 3.556 (3.45)  Time: 0.701s, 1460.09/s  (0.700s, 1462.99/s)  LR: 1.477e-04  Data: 0.011 (0.021)
Train: 454 [ 250/1251 ( 20%)]  Loss: 3.135 (3.39)  Time: 0.667s, 1535.99/s  (0.699s, 1465.47/s)  LR: 1.477e-04  Data: 0.010 (0.019)
Train: 454 [ 300/1251 ( 24%)]  Loss: 3.177 (3.36)  Time: 0.677s, 1511.58/s  (0.698s, 1467.95/s)  LR: 1.477e-04  Data: 0.010 (0.017)
Train: 454 [ 350/1251 ( 28%)]  Loss: 3.457 (3.37)  Time: 0.674s, 1518.79/s  (0.698s, 1467.92/s)  LR: 1.477e-04  Data: 0.011 (0.016)
Train: 454 [ 400/1251 ( 32%)]  Loss: 3.425 (3.38)  Time: 0.705s, 1452.73/s  (0.697s, 1470.05/s)  LR: 1.477e-04  Data: 0.010 (0.016)
Train: 454 [ 450/1251 ( 36%)]  Loss: 3.427 (3.38)  Time: 0.722s, 1417.35/s  (0.695s, 1472.49/s)  LR: 1.477e-04  Data: 0.011 (0.015)
Train: 454 [ 500/1251 ( 40%)]  Loss: 3.336 (3.38)  Time: 0.737s, 1390.35/s  (0.696s, 1470.98/s)  LR: 1.477e-04  Data: 0.011 (0.014)
Train: 454 [ 550/1251 ( 44%)]  Loss: 3.635 (3.40)  Time: 0.677s, 1511.97/s  (0.696s, 1471.57/s)  LR: 1.477e-04  Data: 0.011 (0.014)
Train: 454 [ 600/1251 ( 48%)]  Loss: 3.203 (3.39)  Time: 0.692s, 1480.53/s  (0.696s, 1470.57/s)  LR: 1.477e-04  Data: 0.014 (0.014)
Train: 454 [ 650/1251 ( 52%)]  Loss: 3.283 (3.38)  Time: 0.669s, 1530.83/s  (0.696s, 1470.49/s)  LR: 1.477e-04  Data: 0.010 (0.014)
Train: 454 [ 700/1251 ( 56%)]  Loss: 3.654 (3.40)  Time: 0.728s, 1406.29/s  (0.696s, 1471.17/s)  LR: 1.477e-04  Data: 0.010 (0.013)
Train: 454 [ 750/1251 ( 60%)]  Loss: 3.483 (3.40)  Time: 0.674s, 1520.30/s  (0.696s, 1471.84/s)  LR: 1.477e-04  Data: 0.011 (0.013)
Train: 454 [ 800/1251 ( 64%)]  Loss: 3.564 (3.41)  Time: 0.718s, 1426.26/s  (0.695s, 1472.58/s)  LR: 1.477e-04  Data: 0.009 (0.013)
Train: 454 [ 850/1251 ( 68%)]  Loss: 3.323 (3.41)  Time: 0.702s, 1458.72/s  (0.695s, 1473.51/s)  LR: 1.477e-04  Data: 0.010 (0.013)
Train: 454 [ 900/1251 ( 72%)]  Loss: 2.960 (3.38)  Time: 0.674s, 1520.01/s  (0.694s, 1474.46/s)  LR: 1.477e-04  Data: 0.011 (0.013)
Train: 454 [ 950/1251 ( 76%)]  Loss: 3.324 (3.38)  Time: 0.734s, 1394.35/s  (0.695s, 1474.20/s)  LR: 1.477e-04  Data: 0.009 (0.013)
Train: 454 [1000/1251 ( 80%)]  Loss: 3.790 (3.40)  Time: 0.718s, 1426.33/s  (0.695s, 1473.78/s)  LR: 1.477e-04  Data: 0.010 (0.012)
Train: 454 [1050/1251 ( 84%)]  Loss: 3.476 (3.40)  Time: 0.706s, 1449.93/s  (0.695s, 1473.51/s)  LR: 1.477e-04  Data: 0.013 (0.012)
Train: 454 [1100/1251 ( 88%)]  Loss: 3.410 (3.40)  Time: 0.714s, 1434.14/s  (0.695s, 1473.58/s)  LR: 1.477e-04  Data: 0.012 (0.012)
Train: 454 [1150/1251 ( 92%)]  Loss: 2.995 (3.39)  Time: 0.701s, 1461.17/s  (0.695s, 1474.28/s)  LR: 1.477e-04  Data: 0.010 (0.012)
Train: 454 [1200/1251 ( 96%)]  Loss: 3.352 (3.39)  Time: 0.744s, 1375.60/s  (0.695s, 1474.09/s)  LR: 1.477e-04  Data: 0.010 (0.012)
Train: 454 [1250/1251 (100%)]  Loss: 3.286 (3.38)  Time: 0.695s, 1474.07/s  (0.695s, 1474.37/s)  LR: 1.477e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.521 (1.521)  Loss:  0.7529 (0.7529)  Acc@1: 91.1133 (91.1133)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.136 (0.589)  Loss:  0.8579 (1.2463)  Acc@1: 86.2028 (77.3280)  Acc@5: 96.8160 (93.5300)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-454.pth.tar', 77.32800008300781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-438.pth.tar', 77.02600010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-453.pth.tar', 76.97199990234375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-444.pth.tar', 76.95599998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-448.pth.tar', 76.94000010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-451.pth.tar', 76.88199995605468)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-442.pth.tar', 76.86000008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-449.pth.tar', 76.85800008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-450.pth.tar', 76.84999992919921)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-441.pth.tar', 76.77999998291016)

Train: 455 [   0/1251 (  0%)]  Loss: 3.297 (3.30)  Time: 2.201s,  465.30/s  (2.201s,  465.30/s)  LR: 1.459e-04  Data: 1.584 (1.584)
Train: 455 [  50/1251 (  4%)]  Loss: 3.091 (3.19)  Time: 0.712s, 1439.06/s  (0.731s, 1401.50/s)  LR: 1.459e-04  Data: 0.011 (0.050)
Train: 455 [ 100/1251 (  8%)]  Loss: 3.128 (3.17)  Time: 0.685s, 1493.92/s  (0.709s, 1443.42/s)  LR: 1.459e-04  Data: 0.017 (0.031)
Train: 455 [ 150/1251 ( 12%)]  Loss: 3.737 (3.31)  Time: 0.670s, 1528.73/s  (0.703s, 1455.70/s)  LR: 1.459e-04  Data: 0.010 (0.024)
Train: 455 [ 200/1251 ( 16%)]  Loss: 3.169 (3.28)  Time: 0.714s, 1433.80/s  (0.701s, 1461.57/s)  LR: 1.459e-04  Data: 0.009 (0.021)
Train: 455 [ 250/1251 ( 20%)]  Loss: 3.456 (3.31)  Time: 0.705s, 1453.19/s  (0.698s, 1466.42/s)  LR: 1.459e-04  Data: 0.009 (0.019)
Train: 455 [ 300/1251 ( 24%)]  Loss: 3.613 (3.36)  Time: 0.755s, 1355.43/s  (0.697s, 1469.08/s)  LR: 1.459e-04  Data: 0.009 (0.017)
Train: 455 [ 350/1251 ( 28%)]  Loss: 3.169 (3.33)  Time: 0.677s, 1512.77/s  (0.696s, 1471.02/s)  LR: 1.459e-04  Data: 0.008 (0.016)
Train: 455 [ 400/1251 ( 32%)]  Loss: 3.399 (3.34)  Time: 0.716s, 1430.97/s  (0.695s, 1472.39/s)  LR: 1.459e-04  Data: 0.011 (0.015)
Train: 455 [ 450/1251 ( 36%)]  Loss: 3.729 (3.38)  Time: 0.762s, 1343.92/s  (0.695s, 1473.38/s)  LR: 1.459e-04  Data: 0.012 (0.015)
Train: 455 [ 500/1251 ( 40%)]  Loss: 3.375 (3.38)  Time: 0.671s, 1525.97/s  (0.695s, 1474.25/s)  LR: 1.459e-04  Data: 0.011 (0.014)
Train: 455 [ 550/1251 ( 44%)]  Loss: 3.112 (3.36)  Time: 0.672s, 1524.93/s  (0.694s, 1475.49/s)  LR: 1.459e-04  Data: 0.010 (0.014)
Train: 455 [ 600/1251 ( 48%)]  Loss: 3.801 (3.39)  Time: 0.708s, 1446.82/s  (0.694s, 1475.12/s)  LR: 1.459e-04  Data: 0.009 (0.014)
Train: 455 [ 650/1251 ( 52%)]  Loss: 3.641 (3.41)  Time: 0.674s, 1520.31/s  (0.694s, 1476.14/s)  LR: 1.459e-04  Data: 0.015 (0.014)
Train: 455 [ 700/1251 ( 56%)]  Loss: 3.201 (3.39)  Time: 0.667s, 1534.15/s  (0.694s, 1475.25/s)  LR: 1.459e-04  Data: 0.010 (0.013)
Train: 455 [ 750/1251 ( 60%)]  Loss: 3.773 (3.42)  Time: 0.789s, 1297.38/s  (0.694s, 1475.91/s)  LR: 1.459e-04  Data: 0.010 (0.013)
Train: 455 [ 800/1251 ( 64%)]  Loss: 3.156 (3.40)  Time: 0.725s, 1411.90/s  (0.694s, 1476.23/s)  LR: 1.459e-04  Data: 0.009 (0.013)
Train: 455 [ 850/1251 ( 68%)]  Loss: 3.240 (3.39)  Time: 0.727s, 1408.37/s  (0.694s, 1476.54/s)  LR: 1.459e-04  Data: 0.010 (0.013)
Train: 455 [ 900/1251 ( 72%)]  Loss: 3.228 (3.38)  Time: 0.674s, 1519.39/s  (0.694s, 1476.34/s)  LR: 1.459e-04  Data: 0.010 (0.013)
Train: 455 [ 950/1251 ( 76%)]  Loss: 3.206 (3.38)  Time: 0.686s, 1492.03/s  (0.693s, 1477.08/s)  LR: 1.459e-04  Data: 0.009 (0.013)
Train: 455 [1000/1251 ( 80%)]  Loss: 3.521 (3.38)  Time: 0.709s, 1443.61/s  (0.693s, 1476.86/s)  LR: 1.459e-04  Data: 0.012 (0.012)
Train: 455 [1050/1251 ( 84%)]  Loss: 3.738 (3.40)  Time: 0.714s, 1434.03/s  (0.693s, 1477.24/s)  LR: 1.459e-04  Data: 0.011 (0.012)
Train: 455 [1100/1251 ( 88%)]  Loss: 3.437 (3.40)  Time: 0.671s, 1525.06/s  (0.693s, 1478.05/s)  LR: 1.459e-04  Data: 0.010 (0.012)
Train: 455 [1150/1251 ( 92%)]  Loss: 3.386 (3.40)  Time: 0.679s, 1507.87/s  (0.693s, 1477.92/s)  LR: 1.459e-04  Data: 0.010 (0.012)
Train: 455 [1200/1251 ( 96%)]  Loss: 3.382 (3.40)  Time: 0.670s, 1527.71/s  (0.693s, 1478.14/s)  LR: 1.459e-04  Data: 0.009 (0.012)
Train: 455 [1250/1251 (100%)]  Loss: 3.499 (3.40)  Time: 0.660s, 1551.97/s  (0.693s, 1478.09/s)  LR: 1.459e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.515 (1.515)  Loss:  0.7559 (0.7559)  Acc@1: 90.8203 (90.8203)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.136 (0.587)  Loss:  0.8691 (1.2679)  Acc@1: 85.9670 (77.3740)  Acc@5: 97.6415 (93.6000)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-455.pth.tar', 77.37399998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-454.pth.tar', 77.32800008300781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-438.pth.tar', 77.02600010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-453.pth.tar', 76.97199990234375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-444.pth.tar', 76.95599998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-448.pth.tar', 76.94000010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-451.pth.tar', 76.88199995605468)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-442.pth.tar', 76.86000008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-449.pth.tar', 76.85800008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-450.pth.tar', 76.84999992919921)

Train: 456 [   0/1251 (  0%)]  Loss: 3.483 (3.48)  Time: 2.214s,  462.42/s  (2.214s,  462.42/s)  LR: 1.442e-04  Data: 1.551 (1.551)
Train: 456 [  50/1251 (  4%)]  Loss: 3.365 (3.42)  Time: 0.688s, 1488.21/s  (0.733s, 1397.88/s)  LR: 1.442e-04  Data: 0.009 (0.049)
Train: 456 [ 100/1251 (  8%)]  Loss: 3.313 (3.39)  Time: 0.678s, 1511.01/s  (0.710s, 1442.93/s)  LR: 1.442e-04  Data: 0.010 (0.030)
Train: 456 [ 150/1251 ( 12%)]  Loss: 3.609 (3.44)  Time: 0.688s, 1487.87/s  (0.701s, 1459.90/s)  LR: 1.442e-04  Data: 0.011 (0.023)
Train: 456 [ 200/1251 ( 16%)]  Loss: 3.336 (3.42)  Time: 0.701s, 1459.98/s  (0.699s, 1465.28/s)  LR: 1.442e-04  Data: 0.009 (0.020)
Train: 456 [ 250/1251 ( 20%)]  Loss: 3.451 (3.43)  Time: 0.702s, 1457.80/s  (0.697s, 1469.49/s)  LR: 1.442e-04  Data: 0.010 (0.018)
Train: 456 [ 300/1251 ( 24%)]  Loss: 3.110 (3.38)  Time: 0.681s, 1503.35/s  (0.695s, 1472.92/s)  LR: 1.442e-04  Data: 0.011 (0.017)
Train: 456 [ 350/1251 ( 28%)]  Loss: 3.151 (3.35)  Time: 0.672s, 1524.48/s  (0.694s, 1474.55/s)  LR: 1.442e-04  Data: 0.010 (0.016)
Train: 456 [ 400/1251 ( 32%)]  Loss: 3.216 (3.34)  Time: 0.667s, 1535.14/s  (0.694s, 1474.90/s)  LR: 1.442e-04  Data: 0.010 (0.015)
Train: 456 [ 450/1251 ( 36%)]  Loss: 3.361 (3.34)  Time: 0.723s, 1415.68/s  (0.694s, 1474.55/s)  LR: 1.442e-04  Data: 0.011 (0.015)
Train: 456 [ 500/1251 ( 40%)]  Loss: 3.504 (3.35)  Time: 0.748s, 1368.95/s  (0.695s, 1473.62/s)  LR: 1.442e-04  Data: 0.013 (0.014)
Train: 456 [ 550/1251 ( 44%)]  Loss: 3.494 (3.37)  Time: 0.672s, 1522.93/s  (0.694s, 1474.79/s)  LR: 1.442e-04  Data: 0.010 (0.014)
Train: 456 [ 600/1251 ( 48%)]  Loss: 3.172 (3.35)  Time: 0.672s, 1523.56/s  (0.694s, 1475.97/s)  LR: 1.442e-04  Data: 0.009 (0.014)
Train: 456 [ 650/1251 ( 52%)]  Loss: 3.534 (3.36)  Time: 0.675s, 1517.97/s  (0.694s, 1476.37/s)  LR: 1.442e-04  Data: 0.011 (0.013)
Train: 456 [ 700/1251 ( 56%)]  Loss: 3.438 (3.37)  Time: 0.673s, 1520.78/s  (0.693s, 1476.88/s)  LR: 1.442e-04  Data: 0.011 (0.013)
Train: 456 [ 750/1251 ( 60%)]  Loss: 3.176 (3.36)  Time: 0.702s, 1458.76/s  (0.693s, 1476.97/s)  LR: 1.442e-04  Data: 0.009 (0.013)
Train: 456 [ 800/1251 ( 64%)]  Loss: 3.616 (3.37)  Time: 0.700s, 1462.81/s  (0.693s, 1477.22/s)  LR: 1.442e-04  Data: 0.011 (0.013)
Train: 456 [ 850/1251 ( 68%)]  Loss: 3.166 (3.36)  Time: 0.698s, 1467.90/s  (0.693s, 1477.33/s)  LR: 1.442e-04  Data: 0.013 (0.013)
Train: 456 [ 900/1251 ( 72%)]  Loss: 3.187 (3.35)  Time: 0.681s, 1504.12/s  (0.693s, 1478.03/s)  LR: 1.442e-04  Data: 0.011 (0.013)
Train: 456 [ 950/1251 ( 76%)]  Loss: 3.182 (3.34)  Time: 0.673s, 1521.80/s  (0.693s, 1478.24/s)  LR: 1.442e-04  Data: 0.010 (0.013)
Train: 456 [1000/1251 ( 80%)]  Loss: 3.681 (3.36)  Time: 0.671s, 1525.14/s  (0.693s, 1477.48/s)  LR: 1.442e-04  Data: 0.011 (0.012)
Train: 456 [1050/1251 ( 84%)]  Loss: 3.309 (3.36)  Time: 0.669s, 1531.00/s  (0.693s, 1478.17/s)  LR: 1.442e-04  Data: 0.010 (0.012)
Train: 456 [1100/1251 ( 88%)]  Loss: 3.314 (3.36)  Time: 0.677s, 1512.28/s  (0.692s, 1479.14/s)  LR: 1.442e-04  Data: 0.011 (0.012)
Train: 456 [1150/1251 ( 92%)]  Loss: 3.384 (3.36)  Time: 0.672s, 1524.23/s  (0.692s, 1479.20/s)  LR: 1.442e-04  Data: 0.010 (0.012)
Train: 456 [1200/1251 ( 96%)]  Loss: 3.534 (3.36)  Time: 0.715s, 1431.57/s  (0.692s, 1479.34/s)  LR: 1.442e-04  Data: 0.009 (0.012)
Train: 456 [1250/1251 (100%)]  Loss: 3.281 (3.36)  Time: 0.657s, 1558.66/s  (0.692s, 1479.39/s)  LR: 1.442e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.700 (1.700)  Loss:  0.6782 (0.6782)  Acc@1: 92.2852 (92.2852)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.136 (0.585)  Loss:  0.8013 (1.2614)  Acc@1: 86.6745 (77.0480)  Acc@5: 97.1698 (93.4280)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-455.pth.tar', 77.37399998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-454.pth.tar', 77.32800008300781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-456.pth.tar', 77.04800002929687)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-438.pth.tar', 77.02600010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-453.pth.tar', 76.97199990234375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-444.pth.tar', 76.95599998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-448.pth.tar', 76.94000010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-451.pth.tar', 76.88199995605468)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-442.pth.tar', 76.86000008544922)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-449.pth.tar', 76.85800008544922)

Train: 457 [   0/1251 (  0%)]  Loss: 3.562 (3.56)  Time: 2.249s,  455.33/s  (2.249s,  455.33/s)  LR: 1.424e-04  Data: 1.633 (1.633)
Train: 457 [  50/1251 (  4%)]  Loss: 3.215 (3.39)  Time: 0.699s, 1465.38/s  (0.729s, 1404.92/s)  LR: 1.424e-04  Data: 0.010 (0.052)
Train: 457 [ 100/1251 (  8%)]  Loss: 3.387 (3.39)  Time: 0.668s, 1533.95/s  (0.713s, 1436.58/s)  LR: 1.424e-04  Data: 0.010 (0.031)
Train: 457 [ 150/1251 ( 12%)]  Loss: 3.688 (3.46)  Time: 0.708s, 1446.03/s  (0.706s, 1450.77/s)  LR: 1.424e-04  Data: 0.009 (0.024)
Train: 457 [ 200/1251 ( 16%)]  Loss: 3.717 (3.51)  Time: 0.694s, 1474.78/s  (0.705s, 1453.50/s)  LR: 1.424e-04  Data: 0.016 (0.021)
Train: 457 [ 250/1251 ( 20%)]  Loss: 3.579 (3.52)  Time: 0.724s, 1414.40/s  (0.701s, 1460.08/s)  LR: 1.424e-04  Data: 0.009 (0.019)
Train: 457 [ 300/1251 ( 24%)]  Loss: 3.308 (3.49)  Time: 0.670s, 1529.09/s  (0.700s, 1462.24/s)  LR: 1.424e-04  Data: 0.010 (0.017)
Train: 457 [ 350/1251 ( 28%)]  Loss: 3.272 (3.47)  Time: 0.674s, 1518.72/s  (0.699s, 1464.98/s)  LR: 1.424e-04  Data: 0.009 (0.016)
Train: 457 [ 400/1251 ( 32%)]  Loss: 3.307 (3.45)  Time: 0.672s, 1524.06/s  (0.699s, 1465.25/s)  LR: 1.424e-04  Data: 0.011 (0.016)
Train: 457 [ 450/1251 ( 36%)]  Loss: 3.583 (3.46)  Time: 0.671s, 1525.27/s  (0.698s, 1467.80/s)  LR: 1.424e-04  Data: 0.010 (0.015)
Train: 457 [ 500/1251 ( 40%)]  Loss: 3.437 (3.46)  Time: 0.677s, 1512.00/s  (0.697s, 1469.07/s)  LR: 1.424e-04  Data: 0.010 (0.015)
Train: 457 [ 550/1251 ( 44%)]  Loss: 3.232 (3.44)  Time: 0.668s, 1532.72/s  (0.697s, 1469.64/s)  LR: 1.424e-04  Data: 0.010 (0.014)
Train: 457 [ 600/1251 ( 48%)]  Loss: 3.583 (3.45)  Time: 0.671s, 1526.61/s  (0.696s, 1471.31/s)  LR: 1.424e-04  Data: 0.011 (0.014)
Train: 457 [ 650/1251 ( 52%)]  Loss: 3.256 (3.44)  Time: 0.703s, 1457.30/s  (0.696s, 1472.13/s)  LR: 1.424e-04  Data: 0.010 (0.014)
Train: 457 [ 700/1251 ( 56%)]  Loss: 3.002 (3.41)  Time: 0.672s, 1524.45/s  (0.695s, 1473.66/s)  LR: 1.424e-04  Data: 0.010 (0.013)
Train: 457 [ 750/1251 ( 60%)]  Loss: 3.452 (3.41)  Time: 0.735s, 1393.53/s  (0.694s, 1474.46/s)  LR: 1.424e-04  Data: 0.010 (0.013)
Train: 457 [ 800/1251 ( 64%)]  Loss: 3.245 (3.40)  Time: 0.705s, 1452.60/s  (0.695s, 1474.43/s)  LR: 1.424e-04  Data: 0.008 (0.013)
Train: 457 [ 850/1251 ( 68%)]  Loss: 3.409 (3.40)  Time: 0.707s, 1448.30/s  (0.694s, 1475.80/s)  LR: 1.424e-04  Data: 0.010 (0.013)
Train: 457 [ 900/1251 ( 72%)]  Loss: 3.769 (3.42)  Time: 0.673s, 1522.06/s  (0.694s, 1476.05/s)  LR: 1.424e-04  Data: 0.011 (0.013)
Train: 457 [ 950/1251 ( 76%)]  Loss: 3.412 (3.42)  Time: 0.708s, 1445.33/s  (0.694s, 1475.42/s)  LR: 1.424e-04  Data: 0.010 (0.013)
Train: 457 [1000/1251 ( 80%)]  Loss: 3.541 (3.43)  Time: 0.703s, 1457.19/s  (0.694s, 1475.48/s)  LR: 1.424e-04  Data: 0.010 (0.013)
Train: 457 [1050/1251 ( 84%)]  Loss: 3.219 (3.42)  Time: 0.674s, 1518.97/s  (0.694s, 1475.65/s)  LR: 1.424e-04  Data: 0.010 (0.012)
Train: 457 [1100/1251 ( 88%)]  Loss: 3.128 (3.40)  Time: 0.672s, 1524.56/s  (0.694s, 1475.66/s)  LR: 1.424e-04  Data: 0.013 (0.012)
Train: 457 [1150/1251 ( 92%)]  Loss: 3.309 (3.40)  Time: 0.669s, 1529.84/s  (0.694s, 1475.51/s)  LR: 1.424e-04  Data: 0.010 (0.012)
Train: 457 [1200/1251 ( 96%)]  Loss: 3.469 (3.40)  Time: 0.751s, 1364.00/s  (0.694s, 1475.34/s)  LR: 1.424e-04  Data: 0.009 (0.012)
Train: 457 [1250/1251 (100%)]  Loss: 3.339 (3.40)  Time: 0.659s, 1553.95/s  (0.694s, 1475.70/s)  LR: 1.424e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.515 (1.515)  Loss:  0.8311 (0.8311)  Acc@1: 90.8203 (90.8203)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.136 (0.596)  Loss:  0.9946 (1.3343)  Acc@1: 84.9057 (76.4500)  Acc@5: 96.5802 (93.1800)
Train: 458 [   0/1251 (  0%)]  Loss: 3.627 (3.63)  Time: 2.302s,  444.92/s  (2.302s,  444.92/s)  LR: 1.406e-04  Data: 1.650 (1.650)
Train: 458 [  50/1251 (  4%)]  Loss: 3.551 (3.59)  Time: 0.668s, 1533.78/s  (0.733s, 1396.12/s)  LR: 1.406e-04  Data: 0.011 (0.051)
Train: 458 [ 100/1251 (  8%)]  Loss: 3.165 (3.45)  Time: 0.673s, 1520.71/s  (0.714s, 1434.30/s)  LR: 1.406e-04  Data: 0.011 (0.031)
Train: 458 [ 150/1251 ( 12%)]  Loss: 3.515 (3.46)  Time: 0.702s, 1459.71/s  (0.709s, 1444.50/s)  LR: 1.406e-04  Data: 0.010 (0.024)
Train: 458 [ 200/1251 ( 16%)]  Loss: 3.197 (3.41)  Time: 0.674s, 1518.83/s  (0.704s, 1454.54/s)  LR: 1.406e-04  Data: 0.013 (0.021)
Train: 458 [ 250/1251 ( 20%)]  Loss: 3.339 (3.40)  Time: 0.666s, 1537.57/s  (0.703s, 1456.45/s)  LR: 1.406e-04  Data: 0.011 (0.019)
Train: 458 [ 300/1251 ( 24%)]  Loss: 3.515 (3.42)  Time: 0.696s, 1471.07/s  (0.701s, 1460.84/s)  LR: 1.406e-04  Data: 0.009 (0.017)
Train: 458 [ 350/1251 ( 28%)]  Loss: 3.613 (3.44)  Time: 0.677s, 1512.58/s  (0.701s, 1461.45/s)  LR: 1.406e-04  Data: 0.011 (0.016)
Train: 458 [ 400/1251 ( 32%)]  Loss: 3.845 (3.49)  Time: 0.709s, 1444.10/s  (0.700s, 1462.31/s)  LR: 1.406e-04  Data: 0.009 (0.016)
Train: 458 [ 450/1251 ( 36%)]  Loss: 3.417 (3.48)  Time: 0.751s, 1364.32/s  (0.699s, 1464.21/s)  LR: 1.406e-04  Data: 0.009 (0.015)
Train: 458 [ 500/1251 ( 40%)]  Loss: 3.613 (3.49)  Time: 0.674s, 1519.34/s  (0.699s, 1465.52/s)  LR: 1.406e-04  Data: 0.012 (0.015)
Train: 458 [ 550/1251 ( 44%)]  Loss: 3.254 (3.47)  Time: 0.695s, 1473.86/s  (0.699s, 1464.01/s)  LR: 1.406e-04  Data: 0.025 (0.014)
Train: 458 [ 600/1251 ( 48%)]  Loss: 3.656 (3.49)  Time: 0.700s, 1462.31/s  (0.700s, 1462.14/s)  LR: 1.406e-04  Data: 0.014 (0.014)
Train: 458 [ 650/1251 ( 52%)]  Loss: 3.432 (3.48)  Time: 0.682s, 1502.24/s  (0.701s, 1460.75/s)  LR: 1.406e-04  Data: 0.012 (0.014)
Train: 458 [ 700/1251 ( 56%)]  Loss: 3.663 (3.49)  Time: 0.673s, 1522.53/s  (0.702s, 1459.63/s)  LR: 1.406e-04  Data: 0.011 (0.014)
Train: 458 [ 750/1251 ( 60%)]  Loss: 3.399 (3.49)  Time: 0.702s, 1459.58/s  (0.701s, 1461.09/s)  LR: 1.406e-04  Data: 0.009 (0.014)
Train: 458 [ 800/1251 ( 64%)]  Loss: 3.226 (3.47)  Time: 0.671s, 1527.04/s  (0.699s, 1464.38/s)  LR: 1.406e-04  Data: 0.011 (0.013)
Train: 458 [ 850/1251 ( 68%)]  Loss: 3.043 (3.45)  Time: 0.723s, 1417.18/s  (0.699s, 1465.29/s)  LR: 1.406e-04  Data: 0.011 (0.013)
Train: 458 [ 900/1251 ( 72%)]  Loss: 3.324 (3.44)  Time: 0.681s, 1503.66/s  (0.699s, 1465.67/s)  LR: 1.406e-04  Data: 0.013 (0.013)
Train: 458 [ 950/1251 ( 76%)]  Loss: 3.352 (3.44)  Time: 0.671s, 1525.07/s  (0.699s, 1465.99/s)  LR: 1.406e-04  Data: 0.013 (0.013)
Train: 458 [1000/1251 ( 80%)]  Loss: 2.924 (3.41)  Time: 0.689s, 1486.49/s  (0.698s, 1466.46/s)  LR: 1.406e-04  Data: 0.010 (0.013)
Train: 458 [1050/1251 ( 84%)]  Loss: 3.655 (3.42)  Time: 0.693s, 1478.58/s  (0.698s, 1466.48/s)  LR: 1.406e-04  Data: 0.014 (0.013)
Train: 458 [1100/1251 ( 88%)]  Loss: 3.160 (3.41)  Time: 0.705s, 1451.55/s  (0.698s, 1466.87/s)  LR: 1.406e-04  Data: 0.011 (0.013)
Train: 458 [1150/1251 ( 92%)]  Loss: 3.262 (3.41)  Time: 0.716s, 1431.12/s  (0.698s, 1467.20/s)  LR: 1.406e-04  Data: 0.009 (0.013)
Train: 458 [1200/1251 ( 96%)]  Loss: 3.364 (3.40)  Time: 0.736s, 1392.17/s  (0.698s, 1467.21/s)  LR: 1.406e-04  Data: 0.009 (0.012)
Train: 458 [1250/1251 (100%)]  Loss: 3.515 (3.41)  Time: 0.659s, 1554.37/s  (0.698s, 1467.80/s)  LR: 1.406e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.502 (1.502)  Loss:  0.7324 (0.7324)  Acc@1: 91.4062 (91.4062)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.136 (0.585)  Loss:  0.7808 (1.2602)  Acc@1: 86.5566 (76.9960)  Acc@5: 97.6415 (93.4520)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-455.pth.tar', 77.37399998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-454.pth.tar', 77.32800008300781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-456.pth.tar', 77.04800002929687)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-438.pth.tar', 77.02600010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-458.pth.tar', 76.99599997802734)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-453.pth.tar', 76.97199990234375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-444.pth.tar', 76.95599998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-448.pth.tar', 76.94000010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-451.pth.tar', 76.88199995605468)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-442.pth.tar', 76.86000008544922)

Train: 459 [   0/1251 (  0%)]  Loss: 2.806 (2.81)  Time: 2.372s,  431.71/s  (2.372s,  431.71/s)  LR: 1.389e-04  Data: 1.756 (1.756)
Train: 459 [  50/1251 (  4%)]  Loss: 3.439 (3.12)  Time: 0.706s, 1450.52/s  (0.727s, 1408.56/s)  LR: 1.389e-04  Data: 0.009 (0.052)
Train: 459 [ 100/1251 (  8%)]  Loss: 3.138 (3.13)  Time: 0.710s, 1442.66/s  (0.708s, 1446.39/s)  LR: 1.389e-04  Data: 0.009 (0.031)
Train: 459 [ 150/1251 ( 12%)]  Loss: 3.506 (3.22)  Time: 0.707s, 1448.16/s  (0.702s, 1459.27/s)  LR: 1.389e-04  Data: 0.009 (0.025)
Train: 459 [ 200/1251 ( 16%)]  Loss: 3.456 (3.27)  Time: 0.702s, 1457.69/s  (0.701s, 1461.18/s)  LR: 1.389e-04  Data: 0.009 (0.021)
Train: 459 [ 250/1251 ( 20%)]  Loss: 3.275 (3.27)  Time: 0.672s, 1524.63/s  (0.697s, 1468.52/s)  LR: 1.389e-04  Data: 0.010 (0.019)
Train: 459 [ 300/1251 ( 24%)]  Loss: 3.676 (3.33)  Time: 0.672s, 1524.90/s  (0.696s, 1471.40/s)  LR: 1.389e-04  Data: 0.009 (0.018)
Train: 459 [ 350/1251 ( 28%)]  Loss: 3.469 (3.35)  Time: 0.717s, 1427.51/s  (0.695s, 1472.54/s)  LR: 1.389e-04  Data: 0.010 (0.017)
Train: 459 [ 400/1251 ( 32%)]  Loss: 3.254 (3.34)  Time: 0.728s, 1406.83/s  (0.695s, 1473.30/s)  LR: 1.389e-04  Data: 0.010 (0.016)
Train: 459 [ 450/1251 ( 36%)]  Loss: 3.707 (3.37)  Time: 0.796s, 1286.55/s  (0.695s, 1474.07/s)  LR: 1.389e-04  Data: 0.011 (0.015)
Train: 459 [ 500/1251 ( 40%)]  Loss: 3.642 (3.40)  Time: 0.675s, 1516.82/s  (0.694s, 1474.65/s)  LR: 1.389e-04  Data: 0.011 (0.015)
Train: 459 [ 550/1251 ( 44%)]  Loss: 3.370 (3.39)  Time: 0.671s, 1525.86/s  (0.694s, 1475.02/s)  LR: 1.389e-04  Data: 0.009 (0.014)
Train: 459 [ 600/1251 ( 48%)]  Loss: 3.553 (3.41)  Time: 0.756s, 1355.05/s  (0.695s, 1474.27/s)  LR: 1.389e-04  Data: 0.010 (0.014)
Train: 459 [ 650/1251 ( 52%)]  Loss: 3.769 (3.43)  Time: 0.679s, 1507.31/s  (0.694s, 1475.11/s)  LR: 1.389e-04  Data: 0.009 (0.014)
Train: 459 [ 700/1251 ( 56%)]  Loss: 3.225 (3.42)  Time: 0.697s, 1468.35/s  (0.694s, 1476.09/s)  LR: 1.389e-04  Data: 0.009 (0.014)
Train: 459 [ 750/1251 ( 60%)]  Loss: 3.349 (3.41)  Time: 0.702s, 1459.69/s  (0.693s, 1477.08/s)  LR: 1.389e-04  Data: 0.009 (0.013)
Train: 459 [ 800/1251 ( 64%)]  Loss: 3.065 (3.39)  Time: 0.668s, 1532.53/s  (0.693s, 1478.24/s)  LR: 1.389e-04  Data: 0.011 (0.013)
Train: 459 [ 850/1251 ( 68%)]  Loss: 3.504 (3.40)  Time: 0.672s, 1524.84/s  (0.693s, 1477.21/s)  LR: 1.389e-04  Data: 0.010 (0.013)
Train: 459 [ 900/1251 ( 72%)]  Loss: 3.926 (3.43)  Time: 0.699s, 1464.51/s  (0.693s, 1477.84/s)  LR: 1.389e-04  Data: 0.010 (0.013)
Train: 459 [ 950/1251 ( 76%)]  Loss: 3.186 (3.42)  Time: 0.672s, 1524.33/s  (0.693s, 1477.09/s)  LR: 1.389e-04  Data: 0.010 (0.013)
Train: 459 [1000/1251 ( 80%)]  Loss: 3.355 (3.41)  Time: 0.712s, 1438.50/s  (0.693s, 1476.69/s)  LR: 1.389e-04  Data: 0.010 (0.013)
Train: 459 [1050/1251 ( 84%)]  Loss: 3.460 (3.42)  Time: 0.688s, 1488.96/s  (0.694s, 1476.01/s)  LR: 1.389e-04  Data: 0.010 (0.013)
Train: 459 [1100/1251 ( 88%)]  Loss: 3.342 (3.41)  Time: 0.718s, 1426.15/s  (0.694s, 1476.14/s)  LR: 1.389e-04  Data: 0.010 (0.012)
Train: 459 [1150/1251 ( 92%)]  Loss: 3.596 (3.42)  Time: 0.699s, 1464.10/s  (0.694s, 1476.52/s)  LR: 1.389e-04  Data: 0.009 (0.012)
Train: 459 [1200/1251 ( 96%)]  Loss: 3.404 (3.42)  Time: 0.675s, 1517.25/s  (0.693s, 1476.59/s)  LR: 1.389e-04  Data: 0.009 (0.012)
Train: 459 [1250/1251 (100%)]  Loss: 3.483 (3.42)  Time: 0.661s, 1548.89/s  (0.693s, 1476.63/s)  LR: 1.389e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.488 (1.488)  Loss:  0.6641 (0.6641)  Acc@1: 91.7969 (91.7969)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.136 (0.591)  Loss:  0.7734 (1.1746)  Acc@1: 85.4953 (77.2820)  Acc@5: 97.5236 (93.6020)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-455.pth.tar', 77.37399998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-454.pth.tar', 77.32800008300781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-459.pth.tar', 77.28200003417969)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-456.pth.tar', 77.04800002929687)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-438.pth.tar', 77.02600010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-458.pth.tar', 76.99599997802734)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-453.pth.tar', 76.97199990234375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-444.pth.tar', 76.95599998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-448.pth.tar', 76.94000010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-451.pth.tar', 76.88199995605468)

Train: 460 [   0/1251 (  0%)]  Loss: 3.151 (3.15)  Time: 2.379s,  430.46/s  (2.379s,  430.46/s)  LR: 1.371e-04  Data: 1.763 (1.763)
Train: 460 [  50/1251 (  4%)]  Loss: 3.571 (3.36)  Time: 0.674s, 1520.23/s  (0.727s, 1409.39/s)  LR: 1.371e-04  Data: 0.010 (0.047)
Train: 460 [ 100/1251 (  8%)]  Loss: 3.279 (3.33)  Time: 0.743s, 1378.84/s  (0.709s, 1443.35/s)  LR: 1.371e-04  Data: 0.009 (0.029)
Train: 460 [ 150/1251 ( 12%)]  Loss: 3.407 (3.35)  Time: 0.703s, 1456.63/s  (0.704s, 1455.45/s)  LR: 1.371e-04  Data: 0.009 (0.023)
Train: 460 [ 200/1251 ( 16%)]  Loss: 3.416 (3.36)  Time: 0.678s, 1511.15/s  (0.702s, 1458.74/s)  LR: 1.371e-04  Data: 0.009 (0.020)
Train: 460 [ 250/1251 ( 20%)]  Loss: 3.202 (3.34)  Time: 0.771s, 1328.66/s  (0.700s, 1463.22/s)  LR: 1.371e-04  Data: 0.009 (0.018)
Train: 460 [ 300/1251 ( 24%)]  Loss: 3.162 (3.31)  Time: 0.728s, 1407.48/s  (0.699s, 1463.95/s)  LR: 1.371e-04  Data: 0.009 (0.017)
Train: 460 [ 350/1251 ( 28%)]  Loss: 3.535 (3.34)  Time: 0.676s, 1513.99/s  (0.699s, 1465.80/s)  LR: 1.371e-04  Data: 0.010 (0.016)
Train: 460 [ 400/1251 ( 32%)]  Loss: 3.540 (3.36)  Time: 0.664s, 1543.31/s  (0.697s, 1468.20/s)  LR: 1.371e-04  Data: 0.008 (0.015)
Train: 460 [ 450/1251 ( 36%)]  Loss: 3.363 (3.36)  Time: 0.684s, 1497.25/s  (0.696s, 1471.19/s)  LR: 1.371e-04  Data: 0.012 (0.015)
Train: 460 [ 500/1251 ( 40%)]  Loss: 3.638 (3.39)  Time: 0.713s, 1436.95/s  (0.696s, 1471.41/s)  LR: 1.371e-04  Data: 0.009 (0.014)
Train: 460 [ 550/1251 ( 44%)]  Loss: 3.601 (3.41)  Time: 0.674s, 1520.11/s  (0.695s, 1472.49/s)  LR: 1.371e-04  Data: 0.010 (0.014)
Train: 460 [ 600/1251 ( 48%)]  Loss: 3.233 (3.39)  Time: 0.739s, 1384.74/s  (0.696s, 1471.81/s)  LR: 1.371e-04  Data: 0.008 (0.014)
Train: 460 [ 650/1251 ( 52%)]  Loss: 3.251 (3.38)  Time: 0.671s, 1527.12/s  (0.696s, 1472.01/s)  LR: 1.371e-04  Data: 0.010 (0.013)
Train: 460 [ 700/1251 ( 56%)]  Loss: 3.285 (3.38)  Time: 0.701s, 1461.18/s  (0.695s, 1472.48/s)  LR: 1.371e-04  Data: 0.009 (0.013)
Train: 460 [ 750/1251 ( 60%)]  Loss: 3.144 (3.36)  Time: 0.675s, 1517.49/s  (0.695s, 1472.85/s)  LR: 1.371e-04  Data: 0.011 (0.013)
Train: 460 [ 800/1251 ( 64%)]  Loss: 3.449 (3.37)  Time: 0.671s, 1526.70/s  (0.695s, 1473.04/s)  LR: 1.371e-04  Data: 0.010 (0.013)
Train: 460 [ 850/1251 ( 68%)]  Loss: 3.632 (3.38)  Time: 0.672s, 1522.73/s  (0.695s, 1473.77/s)  LR: 1.371e-04  Data: 0.011 (0.013)
Train: 460 [ 900/1251 ( 72%)]  Loss: 3.338 (3.38)  Time: 0.725s, 1411.91/s  (0.695s, 1473.83/s)  LR: 1.371e-04  Data: 0.010 (0.013)
Train: 460 [ 950/1251 ( 76%)]  Loss: 3.190 (3.37)  Time: 0.675s, 1517.42/s  (0.695s, 1473.81/s)  LR: 1.371e-04  Data: 0.011 (0.012)
Train: 460 [1000/1251 ( 80%)]  Loss: 3.500 (3.38)  Time: 0.699s, 1465.55/s  (0.695s, 1474.26/s)  LR: 1.371e-04  Data: 0.009 (0.012)
Train: 460 [1050/1251 ( 84%)]  Loss: 3.313 (3.37)  Time: 0.719s, 1424.44/s  (0.695s, 1474.31/s)  LR: 1.371e-04  Data: 0.009 (0.012)
Train: 460 [1100/1251 ( 88%)]  Loss: 3.195 (3.37)  Time: 0.698s, 1466.36/s  (0.694s, 1474.70/s)  LR: 1.371e-04  Data: 0.014 (0.012)
Train: 460 [1150/1251 ( 92%)]  Loss: 3.156 (3.36)  Time: 0.677s, 1512.18/s  (0.694s, 1474.92/s)  LR: 1.371e-04  Data: 0.011 (0.012)
Train: 460 [1200/1251 ( 96%)]  Loss: 3.363 (3.36)  Time: 0.671s, 1525.66/s  (0.694s, 1475.33/s)  LR: 1.371e-04  Data: 0.010 (0.012)
Train: 460 [1250/1251 (100%)]  Loss: 3.537 (3.36)  Time: 0.692s, 1479.49/s  (0.694s, 1475.69/s)  LR: 1.371e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.550 (1.550)  Loss:  0.7612 (0.7612)  Acc@1: 90.8203 (90.8203)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.136 (0.590)  Loss:  0.8687 (1.2879)  Acc@1: 85.7311 (77.2040)  Acc@5: 95.9906 (93.5740)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-455.pth.tar', 77.37399998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-454.pth.tar', 77.32800008300781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-459.pth.tar', 77.28200003417969)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-460.pth.tar', 77.20400000732423)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-456.pth.tar', 77.04800002929687)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-438.pth.tar', 77.02600010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-458.pth.tar', 76.99599997802734)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-453.pth.tar', 76.97199990234375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-444.pth.tar', 76.95599998291016)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-448.pth.tar', 76.94000010986328)

Train: 461 [   0/1251 (  0%)]  Loss: 3.261 (3.26)  Time: 2.291s,  447.03/s  (2.291s,  447.03/s)  LR: 1.354e-04  Data: 1.677 (1.677)
Train: 461 [  50/1251 (  4%)]  Loss: 3.152 (3.21)  Time: 0.711s, 1440.47/s  (0.741s, 1381.51/s)  LR: 1.354e-04  Data: 0.012 (0.052)
Train: 461 [ 100/1251 (  8%)]  Loss: 3.569 (3.33)  Time: 0.675s, 1517.01/s  (0.720s, 1423.04/s)  LR: 1.354e-04  Data: 0.011 (0.031)
Train: 461 [ 150/1251 ( 12%)]  Loss: 3.207 (3.30)  Time: 0.672s, 1523.78/s  (0.710s, 1442.53/s)  LR: 1.354e-04  Data: 0.010 (0.025)
Train: 461 [ 200/1251 ( 16%)]  Loss: 3.549 (3.35)  Time: 0.677s, 1512.19/s  (0.707s, 1448.15/s)  LR: 1.354e-04  Data: 0.009 (0.021)
Train: 461 [ 250/1251 ( 20%)]  Loss: 3.720 (3.41)  Time: 0.760s, 1347.92/s  (0.706s, 1451.27/s)  LR: 1.354e-04  Data: 0.013 (0.019)
Train: 461 [ 300/1251 ( 24%)]  Loss: 3.275 (3.39)  Time: 0.759s, 1349.99/s  (0.703s, 1455.91/s)  LR: 1.354e-04  Data: 0.010 (0.018)
Train: 461 [ 350/1251 ( 28%)]  Loss: 3.137 (3.36)  Time: 0.711s, 1440.21/s  (0.702s, 1459.31/s)  LR: 1.354e-04  Data: 0.011 (0.016)
Train: 461 [ 400/1251 ( 32%)]  Loss: 3.244 (3.35)  Time: 0.702s, 1458.19/s  (0.700s, 1462.73/s)  LR: 1.354e-04  Data: 0.009 (0.016)
Train: 461 [ 450/1251 ( 36%)]  Loss: 3.232 (3.33)  Time: 0.672s, 1524.34/s  (0.698s, 1466.17/s)  LR: 1.354e-04  Data: 0.010 (0.015)
Train: 461 [ 500/1251 ( 40%)]  Loss: 3.173 (3.32)  Time: 0.670s, 1527.24/s  (0.699s, 1465.48/s)  LR: 1.354e-04  Data: 0.011 (0.015)
Train: 461 [ 550/1251 ( 44%)]  Loss: 3.379 (3.32)  Time: 0.674s, 1520.15/s  (0.698s, 1466.49/s)  LR: 1.354e-04  Data: 0.015 (0.014)
Train: 461 [ 600/1251 ( 48%)]  Loss: 3.379 (3.33)  Time: 0.713s, 1436.38/s  (0.697s, 1468.52/s)  LR: 1.354e-04  Data: 0.010 (0.014)
Train: 461 [ 650/1251 ( 52%)]  Loss: 3.581 (3.35)  Time: 0.670s, 1528.38/s  (0.697s, 1470.14/s)  LR: 1.354e-04  Data: 0.010 (0.014)
Train: 461 [ 700/1251 ( 56%)]  Loss: 3.522 (3.36)  Time: 0.698s, 1467.51/s  (0.696s, 1471.25/s)  LR: 1.354e-04  Data: 0.010 (0.014)
Train: 461 [ 750/1251 ( 60%)]  Loss: 3.446 (3.36)  Time: 0.741s, 1381.01/s  (0.696s, 1471.71/s)  LR: 1.354e-04  Data: 0.011 (0.013)
Train: 461 [ 800/1251 ( 64%)]  Loss: 3.195 (3.35)  Time: 0.673s, 1520.94/s  (0.696s, 1471.87/s)  LR: 1.354e-04  Data: 0.010 (0.013)
Train: 461 [ 850/1251 ( 68%)]  Loss: 3.288 (3.35)  Time: 0.715s, 1431.45/s  (0.696s, 1471.09/s)  LR: 1.354e-04  Data: 0.011 (0.013)
Train: 461 [ 900/1251 ( 72%)]  Loss: 3.041 (3.33)  Time: 0.670s, 1529.04/s  (0.696s, 1471.49/s)  LR: 1.354e-04  Data: 0.010 (0.013)
Train: 461 [ 950/1251 ( 76%)]  Loss: 3.243 (3.33)  Time: 0.747s, 1371.16/s  (0.696s, 1470.91/s)  LR: 1.354e-04  Data: 0.009 (0.013)
Train: 461 [1000/1251 ( 80%)]  Loss: 3.525 (3.34)  Time: 0.702s, 1458.07/s  (0.696s, 1472.14/s)  LR: 1.354e-04  Data: 0.010 (0.013)
Train: 461 [1050/1251 ( 84%)]  Loss: 3.623 (3.35)  Time: 0.668s, 1533.93/s  (0.695s, 1472.82/s)  LR: 1.354e-04  Data: 0.009 (0.013)
Train: 461 [1100/1251 ( 88%)]  Loss: 3.491 (3.36)  Time: 0.671s, 1527.04/s  (0.695s, 1472.94/s)  LR: 1.354e-04  Data: 0.010 (0.012)
Train: 461 [1150/1251 ( 92%)]  Loss: 3.396 (3.36)  Time: 0.676s, 1515.57/s  (0.695s, 1473.06/s)  LR: 1.354e-04  Data: 0.020 (0.012)
Train: 461 [1200/1251 ( 96%)]  Loss: 3.725 (3.37)  Time: 0.846s, 1210.33/s  (0.695s, 1473.04/s)  LR: 1.354e-04  Data: 0.009 (0.012)
Train: 461 [1250/1251 (100%)]  Loss: 3.365 (3.37)  Time: 0.695s, 1473.68/s  (0.695s, 1472.98/s)  LR: 1.354e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.610 (1.610)  Loss:  0.7852 (0.7852)  Acc@1: 90.1367 (90.1367)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.136 (0.585)  Loss:  0.8882 (1.2692)  Acc@1: 85.8491 (77.1680)  Acc@5: 96.8160 (93.5500)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-455.pth.tar', 77.37399998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-454.pth.tar', 77.32800008300781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-459.pth.tar', 77.28200003417969)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-460.pth.tar', 77.20400000732423)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-461.pth.tar', 77.16799992919921)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-456.pth.tar', 77.04800002929687)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-438.pth.tar', 77.02600010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-458.pth.tar', 76.99599997802734)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-453.pth.tar', 76.97199990234375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-444.pth.tar', 76.95599998291016)

Train: 462 [   0/1251 (  0%)]  Loss: 3.765 (3.76)  Time: 2.182s,  469.24/s  (2.182s,  469.24/s)  LR: 1.337e-04  Data: 1.569 (1.569)
Train: 462 [  50/1251 (  4%)]  Loss: 3.516 (3.64)  Time: 0.715s, 1431.36/s  (0.736s, 1390.58/s)  LR: 1.337e-04  Data: 0.010 (0.055)
Train: 462 [ 100/1251 (  8%)]  Loss: 3.629 (3.64)  Time: 0.689s, 1486.10/s  (0.713s, 1436.83/s)  LR: 1.337e-04  Data: 0.014 (0.033)
Train: 462 [ 150/1251 ( 12%)]  Loss: 3.233 (3.54)  Time: 0.673s, 1520.78/s  (0.707s, 1449.29/s)  LR: 1.337e-04  Data: 0.011 (0.025)
Train: 462 [ 200/1251 ( 16%)]  Loss: 3.076 (3.44)  Time: 0.672s, 1524.34/s  (0.702s, 1458.31/s)  LR: 1.337e-04  Data: 0.012 (0.022)
Train: 462 [ 250/1251 ( 20%)]  Loss: 3.439 (3.44)  Time: 0.675s, 1516.46/s  (0.700s, 1461.83/s)  LR: 1.337e-04  Data: 0.011 (0.020)
Train: 462 [ 300/1251 ( 24%)]  Loss: 3.303 (3.42)  Time: 0.668s, 1532.70/s  (0.699s, 1464.56/s)  LR: 1.337e-04  Data: 0.010 (0.018)
Train: 462 [ 350/1251 ( 28%)]  Loss: 3.332 (3.41)  Time: 0.671s, 1525.68/s  (0.697s, 1469.15/s)  LR: 1.337e-04  Data: 0.010 (0.017)
Train: 462 [ 400/1251 ( 32%)]  Loss: 3.104 (3.38)  Time: 0.673s, 1520.84/s  (0.696s, 1471.48/s)  LR: 1.337e-04  Data: 0.011 (0.016)
Train: 462 [ 450/1251 ( 36%)]  Loss: 3.427 (3.38)  Time: 0.672s, 1524.74/s  (0.695s, 1472.36/s)  LR: 1.337e-04  Data: 0.011 (0.015)
Train: 462 [ 500/1251 ( 40%)]  Loss: 3.068 (3.35)  Time: 0.672s, 1523.83/s  (0.695s, 1473.72/s)  LR: 1.337e-04  Data: 0.010 (0.015)
Train: 462 [ 550/1251 ( 44%)]  Loss: 2.979 (3.32)  Time: 0.703s, 1455.94/s  (0.694s, 1475.56/s)  LR: 1.337e-04  Data: 0.011 (0.015)
Train: 462 [ 600/1251 ( 48%)]  Loss: 3.284 (3.32)  Time: 0.675s, 1516.21/s  (0.694s, 1475.58/s)  LR: 1.337e-04  Data: 0.013 (0.014)
Train: 462 [ 650/1251 ( 52%)]  Loss: 3.324 (3.32)  Time: 0.677s, 1512.24/s  (0.694s, 1474.60/s)  LR: 1.337e-04  Data: 0.010 (0.014)
Train: 462 [ 700/1251 ( 56%)]  Loss: 3.242 (3.31)  Time: 0.671s, 1525.92/s  (0.694s, 1475.50/s)  LR: 1.337e-04  Data: 0.010 (0.014)
Train: 462 [ 750/1251 ( 60%)]  Loss: 3.159 (3.31)  Time: 0.686s, 1493.68/s  (0.694s, 1475.98/s)  LR: 1.337e-04  Data: 0.010 (0.014)
Train: 462 [ 800/1251 ( 64%)]  Loss: 3.421 (3.31)  Time: 0.674s, 1518.19/s  (0.694s, 1476.03/s)  LR: 1.337e-04  Data: 0.011 (0.013)
Train: 462 [ 850/1251 ( 68%)]  Loss: 3.260 (3.31)  Time: 0.687s, 1490.92/s  (0.694s, 1476.40/s)  LR: 1.337e-04  Data: 0.009 (0.013)
Train: 462 [ 900/1251 ( 72%)]  Loss: 3.468 (3.32)  Time: 0.711s, 1439.38/s  (0.693s, 1476.75/s)  LR: 1.337e-04  Data: 0.016 (0.013)
Train: 462 [ 950/1251 ( 76%)]  Loss: 3.224 (3.31)  Time: 0.706s, 1449.80/s  (0.693s, 1477.06/s)  LR: 1.337e-04  Data: 0.009 (0.013)
Train: 462 [1000/1251 ( 80%)]  Loss: 3.696 (3.33)  Time: 0.672s, 1523.91/s  (0.693s, 1476.89/s)  LR: 1.337e-04  Data: 0.011 (0.013)
Train: 462 [1050/1251 ( 84%)]  Loss: 3.449 (3.34)  Time: 0.686s, 1493.05/s  (0.693s, 1477.47/s)  LR: 1.337e-04  Data: 0.011 (0.013)
Train: 462 [1100/1251 ( 88%)]  Loss: 3.249 (3.33)  Time: 0.723s, 1416.82/s  (0.693s, 1477.99/s)  LR: 1.337e-04  Data: 0.009 (0.013)
Train: 462 [1150/1251 ( 92%)]  Loss: 3.434 (3.34)  Time: 0.725s, 1412.59/s  (0.693s, 1478.41/s)  LR: 1.337e-04  Data: 0.010 (0.012)
Train: 462 [1200/1251 ( 96%)]  Loss: 3.365 (3.34)  Time: 0.704s, 1455.34/s  (0.693s, 1478.64/s)  LR: 1.337e-04  Data: 0.009 (0.012)
Train: 462 [1250/1251 (100%)]  Loss: 3.494 (3.34)  Time: 0.657s, 1557.94/s  (0.692s, 1478.91/s)  LR: 1.337e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.605 (1.605)  Loss:  0.7207 (0.7207)  Acc@1: 91.3086 (91.3086)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.136 (0.598)  Loss:  0.9023 (1.2565)  Acc@1: 85.7311 (77.2380)  Acc@5: 96.5802 (93.5920)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-455.pth.tar', 77.37399998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-454.pth.tar', 77.32800008300781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-459.pth.tar', 77.28200003417969)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-462.pth.tar', 77.23800013671875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-460.pth.tar', 77.20400000732423)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-461.pth.tar', 77.16799992919921)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-456.pth.tar', 77.04800002929687)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-438.pth.tar', 77.02600010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-458.pth.tar', 76.99599997802734)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-453.pth.tar', 76.97199990234375)

Train: 463 [   0/1251 (  0%)]  Loss: 3.234 (3.23)  Time: 2.443s,  419.17/s  (2.443s,  419.17/s)  LR: 1.320e-04  Data: 1.811 (1.811)
Train: 463 [  50/1251 (  4%)]  Loss: 3.024 (3.13)  Time: 0.727s, 1408.98/s  (0.737s, 1389.39/s)  LR: 1.320e-04  Data: 0.010 (0.049)
Train: 463 [ 100/1251 (  8%)]  Loss: 3.233 (3.16)  Time: 0.741s, 1382.62/s  (0.750s, 1365.10/s)  LR: 1.320e-04  Data: 0.011 (0.034)
Train: 463 [ 150/1251 ( 12%)]  Loss: 3.473 (3.24)  Time: 0.703s, 1455.94/s  (0.767s, 1335.22/s)  LR: 1.320e-04  Data: 0.016 (0.028)
Train: 463 [ 200/1251 ( 16%)]  Loss: 3.509 (3.29)  Time: 0.763s, 1342.11/s  (0.781s, 1311.55/s)  LR: 1.320e-04  Data: 0.035 (0.024)
Train: 463 [ 250/1251 ( 20%)]  Loss: 3.185 (3.28)  Time: 0.752s, 1361.10/s  (0.781s, 1310.45/s)  LR: 1.320e-04  Data: 0.016 (0.030)
Train: 463 [ 300/1251 ( 24%)]  Loss: 3.497 (3.31)  Time: 1.056s,  969.35/s  (0.792s, 1293.60/s)  LR: 1.320e-04  Data: 0.009 (0.027)
Train: 463 [ 350/1251 ( 28%)]  Loss: 3.166 (3.29)  Time: 0.682s, 1500.41/s  (0.781s, 1311.47/s)  LR: 1.320e-04  Data: 0.009 (0.025)
Train: 463 [ 400/1251 ( 32%)]  Loss: 2.972 (3.25)  Time: 0.671s, 1526.59/s  (0.770s, 1329.94/s)  LR: 1.320e-04  Data: 0.011 (0.023)
Train: 463 [ 450/1251 ( 36%)]  Loss: 3.388 (3.27)  Time: 0.777s, 1318.27/s  (0.762s, 1343.96/s)  LR: 1.320e-04  Data: 0.010 (0.022)
Train: 463 [ 500/1251 ( 40%)]  Loss: 3.571 (3.30)  Time: 0.672s, 1524.62/s  (0.755s, 1356.69/s)  LR: 1.320e-04  Data: 0.011 (0.021)
Train: 463 [ 550/1251 ( 44%)]  Loss: 3.359 (3.30)  Time: 0.667s, 1536.07/s  (0.749s, 1366.33/s)  LR: 1.320e-04  Data: 0.010 (0.020)
Train: 463 [ 600/1251 ( 48%)]  Loss: 3.268 (3.30)  Time: 0.735s, 1393.73/s  (0.745s, 1373.71/s)  LR: 1.320e-04  Data: 0.010 (0.019)
Train: 463 [ 650/1251 ( 52%)]  Loss: 3.125 (3.29)  Time: 0.680s, 1505.36/s  (0.742s, 1380.80/s)  LR: 1.320e-04  Data: 0.010 (0.018)
Train: 463 [ 700/1251 ( 56%)]  Loss: 3.151 (3.28)  Time: 0.671s, 1526.56/s  (0.738s, 1386.83/s)  LR: 1.320e-04  Data: 0.010 (0.018)
Train: 463 [ 750/1251 ( 60%)]  Loss: 3.459 (3.29)  Time: 0.671s, 1525.20/s  (0.735s, 1393.25/s)  LR: 1.320e-04  Data: 0.010 (0.017)
Train: 463 [ 800/1251 ( 64%)]  Loss: 3.615 (3.31)  Time: 0.715s, 1432.35/s  (0.732s, 1398.00/s)  LR: 1.320e-04  Data: 0.010 (0.017)
Train: 463 [ 850/1251 ( 68%)]  Loss: 3.211 (3.30)  Time: 0.672s, 1524.14/s  (0.730s, 1402.99/s)  LR: 1.320e-04  Data: 0.009 (0.017)
Train: 463 [ 900/1251 ( 72%)]  Loss: 3.261 (3.30)  Time: 0.664s, 1541.13/s  (0.728s, 1406.98/s)  LR: 1.320e-04  Data: 0.009 (0.016)
Train: 463 [ 950/1251 ( 76%)]  Loss: 3.557 (3.31)  Time: 0.711s, 1439.66/s  (0.726s, 1410.69/s)  LR: 1.320e-04  Data: 0.010 (0.016)
Train: 463 [1000/1251 ( 80%)]  Loss: 3.637 (3.33)  Time: 0.701s, 1461.54/s  (0.724s, 1414.27/s)  LR: 1.320e-04  Data: 0.010 (0.016)
Train: 463 [1050/1251 ( 84%)]  Loss: 3.422 (3.33)  Time: 0.763s, 1341.84/s  (0.723s, 1417.00/s)  LR: 1.320e-04  Data: 0.009 (0.016)
Train: 463 [1100/1251 ( 88%)]  Loss: 3.279 (3.33)  Time: 0.670s, 1527.58/s  (0.721s, 1419.69/s)  LR: 1.320e-04  Data: 0.010 (0.015)
Train: 463 [1150/1251 ( 92%)]  Loss: 3.544 (3.34)  Time: 0.694s, 1476.48/s  (0.720s, 1423.05/s)  LR: 1.320e-04  Data: 0.010 (0.015)
Train: 463 [1200/1251 ( 96%)]  Loss: 3.291 (3.34)  Time: 0.678s, 1510.70/s  (0.718s, 1425.39/s)  LR: 1.320e-04  Data: 0.011 (0.015)
Train: 463 [1250/1251 (100%)]  Loss: 3.063 (3.33)  Time: 0.698s, 1467.22/s  (0.717s, 1427.82/s)  LR: 1.320e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.493 (1.493)  Loss:  0.8066 (0.8066)  Acc@1: 91.5039 (91.5039)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.136 (0.582)  Loss:  0.8706 (1.3008)  Acc@1: 86.4387 (77.1260)  Acc@5: 96.5802 (93.5280)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-455.pth.tar', 77.37399998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-454.pth.tar', 77.32800008300781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-459.pth.tar', 77.28200003417969)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-462.pth.tar', 77.23800013671875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-460.pth.tar', 77.20400000732423)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-461.pth.tar', 77.16799992919921)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-463.pth.tar', 77.12599992675781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-456.pth.tar', 77.04800002929687)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-438.pth.tar', 77.02600010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-458.pth.tar', 76.99599997802734)

Train: 464 [   0/1251 (  0%)]  Loss: 3.639 (3.64)  Time: 2.460s,  416.28/s  (2.460s,  416.28/s)  LR: 1.303e-04  Data: 1.839 (1.839)
Train: 464 [  50/1251 (  4%)]  Loss: 3.385 (3.51)  Time: 0.727s, 1408.51/s  (0.723s, 1415.52/s)  LR: 1.303e-04  Data: 0.011 (0.049)
Train: 464 [ 100/1251 (  8%)]  Loss: 3.715 (3.58)  Time: 0.717s, 1427.19/s  (0.705s, 1452.29/s)  LR: 1.303e-04  Data: 0.011 (0.030)
Train: 464 [ 150/1251 ( 12%)]  Loss: 3.283 (3.51)  Time: 0.731s, 1401.20/s  (0.701s, 1461.55/s)  LR: 1.303e-04  Data: 0.010 (0.024)
Train: 464 [ 200/1251 ( 16%)]  Loss: 3.362 (3.48)  Time: 0.673s, 1521.75/s  (0.701s, 1461.57/s)  LR: 1.303e-04  Data: 0.010 (0.021)
Train: 464 [ 250/1251 ( 20%)]  Loss: 3.119 (3.42)  Time: 0.737s, 1390.28/s  (0.698s, 1466.71/s)  LR: 1.303e-04  Data: 0.010 (0.019)
Train: 464 [ 300/1251 ( 24%)]  Loss: 3.576 (3.44)  Time: 0.671s, 1525.07/s  (0.699s, 1465.38/s)  LR: 1.303e-04  Data: 0.012 (0.017)
Train: 464 [ 350/1251 ( 28%)]  Loss: 3.285 (3.42)  Time: 0.671s, 1525.13/s  (0.699s, 1464.40/s)  LR: 1.303e-04  Data: 0.011 (0.016)
Train: 464 [ 400/1251 ( 32%)]  Loss: 3.206 (3.40)  Time: 0.677s, 1513.05/s  (0.698s, 1466.52/s)  LR: 1.303e-04  Data: 0.017 (0.016)
Train: 464 [ 450/1251 ( 36%)]  Loss: 3.100 (3.37)  Time: 0.725s, 1411.76/s  (0.697s, 1468.19/s)  LR: 1.303e-04  Data: 0.010 (0.015)
Train: 464 [ 500/1251 ( 40%)]  Loss: 3.502 (3.38)  Time: 0.672s, 1523.42/s  (0.697s, 1468.51/s)  LR: 1.303e-04  Data: 0.011 (0.015)
Train: 464 [ 550/1251 ( 44%)]  Loss: 3.525 (3.39)  Time: 0.670s, 1528.11/s  (0.697s, 1469.99/s)  LR: 1.303e-04  Data: 0.010 (0.014)
Train: 464 [ 600/1251 ( 48%)]  Loss: 3.332 (3.39)  Time: 0.701s, 1460.68/s  (0.697s, 1469.56/s)  LR: 1.303e-04  Data: 0.010 (0.014)
Train: 464 [ 650/1251 ( 52%)]  Loss: 3.426 (3.39)  Time: 0.735s, 1393.86/s  (0.697s, 1469.55/s)  LR: 1.303e-04  Data: 0.010 (0.014)
Train: 464 [ 700/1251 ( 56%)]  Loss: 3.271 (3.38)  Time: 0.709s, 1443.55/s  (0.696s, 1470.78/s)  LR: 1.303e-04  Data: 0.010 (0.014)
Train: 464 [ 750/1251 ( 60%)]  Loss: 3.256 (3.37)  Time: 0.672s, 1522.73/s  (0.696s, 1470.84/s)  LR: 1.303e-04  Data: 0.011 (0.013)
Train: 464 [ 800/1251 ( 64%)]  Loss: 3.415 (3.38)  Time: 0.674s, 1520.27/s  (0.696s, 1470.21/s)  LR: 1.303e-04  Data: 0.011 (0.013)
Train: 464 [ 850/1251 ( 68%)]  Loss: 3.129 (3.36)  Time: 0.729s, 1404.70/s  (0.697s, 1469.97/s)  LR: 1.303e-04  Data: 0.011 (0.013)
Train: 464 [ 900/1251 ( 72%)]  Loss: 3.272 (3.36)  Time: 0.666s, 1538.29/s  (0.697s, 1470.19/s)  LR: 1.303e-04  Data: 0.009 (0.013)
Train: 464 [ 950/1251 ( 76%)]  Loss: 3.239 (3.35)  Time: 0.676s, 1514.68/s  (0.697s, 1469.92/s)  LR: 1.303e-04  Data: 0.010 (0.013)
Train: 464 [1000/1251 ( 80%)]  Loss: 3.467 (3.36)  Time: 0.671s, 1525.22/s  (0.697s, 1470.14/s)  LR: 1.303e-04  Data: 0.010 (0.013)
Train: 464 [1050/1251 ( 84%)]  Loss: 3.437 (3.36)  Time: 0.704s, 1454.54/s  (0.697s, 1470.19/s)  LR: 1.303e-04  Data: 0.015 (0.013)
Train: 464 [1100/1251 ( 88%)]  Loss: 3.191 (3.35)  Time: 0.694s, 1475.31/s  (0.697s, 1469.69/s)  LR: 1.303e-04  Data: 0.011 (0.013)
Train: 464 [1150/1251 ( 92%)]  Loss: 3.489 (3.36)  Time: 0.667s, 1535.96/s  (0.697s, 1469.69/s)  LR: 1.303e-04  Data: 0.012 (0.012)
Train: 464 [1200/1251 ( 96%)]  Loss: 3.385 (3.36)  Time: 0.674s, 1519.25/s  (0.696s, 1471.13/s)  LR: 1.303e-04  Data: 0.010 (0.012)
Train: 464 [1250/1251 (100%)]  Loss: 3.369 (3.36)  Time: 0.687s, 1489.65/s  (0.696s, 1471.32/s)  LR: 1.303e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.578 (1.578)  Loss:  0.7627 (0.7627)  Acc@1: 91.5039 (91.5039)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.135 (0.584)  Loss:  0.7705 (1.2906)  Acc@1: 85.6132 (77.0320)  Acc@5: 97.5236 (93.4780)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-455.pth.tar', 77.37399998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-454.pth.tar', 77.32800008300781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-459.pth.tar', 77.28200003417969)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-462.pth.tar', 77.23800013671875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-460.pth.tar', 77.20400000732423)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-461.pth.tar', 77.16799992919921)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-463.pth.tar', 77.12599992675781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-456.pth.tar', 77.04800002929687)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-464.pth.tar', 77.03200008544921)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-438.pth.tar', 77.02600010986328)

Train: 465 [   0/1251 (  0%)]  Loss: 3.312 (3.31)  Time: 2.096s,  488.46/s  (2.096s,  488.46/s)  LR: 1.286e-04  Data: 1.485 (1.485)
Train: 465 [  50/1251 (  4%)]  Loss: 3.624 (3.47)  Time: 0.673s, 1522.31/s  (0.743s, 1377.63/s)  LR: 1.286e-04  Data: 0.012 (0.054)
Train: 465 [ 100/1251 (  8%)]  Loss: 3.044 (3.33)  Time: 0.722s, 1417.92/s  (0.721s, 1420.67/s)  LR: 1.286e-04  Data: 0.013 (0.033)
Train: 465 [ 150/1251 ( 12%)]  Loss: 3.436 (3.35)  Time: 0.677s, 1511.92/s  (0.715s, 1432.85/s)  LR: 1.286e-04  Data: 0.011 (0.025)
Train: 465 [ 200/1251 ( 16%)]  Loss: 3.714 (3.43)  Time: 0.673s, 1521.60/s  (0.707s, 1447.74/s)  LR: 1.286e-04  Data: 0.010 (0.022)
Train: 465 [ 250/1251 ( 20%)]  Loss: 3.543 (3.45)  Time: 0.670s, 1528.66/s  (0.703s, 1455.87/s)  LR: 1.286e-04  Data: 0.010 (0.019)
Train: 465 [ 300/1251 ( 24%)]  Loss: 3.195 (3.41)  Time: 0.713s, 1435.19/s  (0.701s, 1460.42/s)  LR: 1.286e-04  Data: 0.009 (0.018)
Train: 465 [ 350/1251 ( 28%)]  Loss: 3.260 (3.39)  Time: 0.673s, 1520.56/s  (0.701s, 1461.30/s)  LR: 1.286e-04  Data: 0.010 (0.017)
Train: 465 [ 400/1251 ( 32%)]  Loss: 3.385 (3.39)  Time: 0.720s, 1422.87/s  (0.699s, 1464.43/s)  LR: 1.286e-04  Data: 0.009 (0.016)
Train: 465 [ 450/1251 ( 36%)]  Loss: 3.288 (3.38)  Time: 0.689s, 1487.28/s  (0.698s, 1466.09/s)  LR: 1.286e-04  Data: 0.010 (0.016)
Train: 465 [ 500/1251 ( 40%)]  Loss: 3.408 (3.38)  Time: 0.701s, 1461.40/s  (0.698s, 1466.60/s)  LR: 1.286e-04  Data: 0.014 (0.015)
Train: 465 [ 550/1251 ( 44%)]  Loss: 3.639 (3.40)  Time: 0.675s, 1517.58/s  (0.698s, 1467.85/s)  LR: 1.286e-04  Data: 0.010 (0.015)
Train: 465 [ 600/1251 ( 48%)]  Loss: 2.991 (3.37)  Time: 0.670s, 1528.74/s  (0.697s, 1469.64/s)  LR: 1.286e-04  Data: 0.010 (0.014)
Train: 465 [ 650/1251 ( 52%)]  Loss: 3.451 (3.38)  Time: 0.709s, 1444.43/s  (0.697s, 1469.48/s)  LR: 1.286e-04  Data: 0.010 (0.014)
Train: 465 [ 700/1251 ( 56%)]  Loss: 3.359 (3.38)  Time: 0.702s, 1458.72/s  (0.696s, 1470.47/s)  LR: 1.286e-04  Data: 0.010 (0.014)
Train: 465 [ 750/1251 ( 60%)]  Loss: 3.509 (3.38)  Time: 0.667s, 1534.21/s  (0.696s, 1471.67/s)  LR: 1.286e-04  Data: 0.009 (0.014)
Train: 465 [ 800/1251 ( 64%)]  Loss: 3.394 (3.39)  Time: 0.673s, 1522.54/s  (0.695s, 1472.32/s)  LR: 1.286e-04  Data: 0.010 (0.013)
Train: 465 [ 850/1251 ( 68%)]  Loss: 3.270 (3.38)  Time: 0.683s, 1498.61/s  (0.696s, 1471.08/s)  LR: 1.286e-04  Data: 0.012 (0.013)
Train: 465 [ 900/1251 ( 72%)]  Loss: 3.437 (3.38)  Time: 0.683s, 1499.35/s  (0.696s, 1471.93/s)  LR: 1.286e-04  Data: 0.011 (0.013)
Train: 465 [ 950/1251 ( 76%)]  Loss: 3.439 (3.38)  Time: 0.754s, 1357.19/s  (0.696s, 1472.07/s)  LR: 1.286e-04  Data: 0.011 (0.013)
Train: 465 [1000/1251 ( 80%)]  Loss: 3.596 (3.39)  Time: 0.670s, 1529.11/s  (0.695s, 1472.40/s)  LR: 1.286e-04  Data: 0.009 (0.013)
Train: 465 [1050/1251 ( 84%)]  Loss: 3.038 (3.38)  Time: 0.682s, 1502.09/s  (0.696s, 1471.10/s)  LR: 1.286e-04  Data: 0.010 (0.013)
Train: 465 [1100/1251 ( 88%)]  Loss: 3.137 (3.37)  Time: 0.725s, 1411.85/s  (0.696s, 1470.74/s)  LR: 1.286e-04  Data: 0.009 (0.013)
Train: 465 [1150/1251 ( 92%)]  Loss: 3.246 (3.36)  Time: 0.677s, 1512.65/s  (0.696s, 1471.13/s)  LR: 1.286e-04  Data: 0.010 (0.013)
Train: 465 [1200/1251 ( 96%)]  Loss: 3.252 (3.36)  Time: 0.756s, 1354.53/s  (0.696s, 1471.09/s)  LR: 1.286e-04  Data: 0.009 (0.012)
Train: 465 [1250/1251 (100%)]  Loss: 3.256 (3.35)  Time: 0.662s, 1546.12/s  (0.696s, 1471.84/s)  LR: 1.286e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.481 (1.481)  Loss:  0.6626 (0.6626)  Acc@1: 91.4062 (91.4062)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.136 (0.582)  Loss:  0.8022 (1.1896)  Acc@1: 86.5566 (77.3780)  Acc@5: 96.6981 (93.6600)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-465.pth.tar', 77.37799997802735)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-455.pth.tar', 77.37399998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-454.pth.tar', 77.32800008300781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-459.pth.tar', 77.28200003417969)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-462.pth.tar', 77.23800013671875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-460.pth.tar', 77.20400000732423)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-461.pth.tar', 77.16799992919921)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-463.pth.tar', 77.12599992675781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-456.pth.tar', 77.04800002929687)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-464.pth.tar', 77.03200008544921)

Train: 466 [   0/1251 (  0%)]  Loss: 3.126 (3.13)  Time: 2.237s,  457.83/s  (2.237s,  457.83/s)  LR: 1.269e-04  Data: 1.622 (1.622)
Train: 466 [  50/1251 (  4%)]  Loss: 3.141 (3.13)  Time: 0.722s, 1418.70/s  (0.741s, 1381.57/s)  LR: 1.269e-04  Data: 0.013 (0.053)
Train: 466 [ 100/1251 (  8%)]  Loss: 3.316 (3.19)  Time: 0.739s, 1384.81/s  (0.717s, 1428.66/s)  LR: 1.269e-04  Data: 0.009 (0.032)
Train: 466 [ 150/1251 ( 12%)]  Loss: 3.096 (3.17)  Time: 0.702s, 1458.38/s  (0.709s, 1444.77/s)  LR: 1.269e-04  Data: 0.009 (0.025)
Train: 466 [ 200/1251 ( 16%)]  Loss: 3.757 (3.29)  Time: 0.671s, 1526.64/s  (0.704s, 1454.46/s)  LR: 1.269e-04  Data: 0.010 (0.021)
Train: 466 [ 250/1251 ( 20%)]  Loss: 2.667 (3.18)  Time: 0.675s, 1517.42/s  (0.702s, 1459.60/s)  LR: 1.269e-04  Data: 0.011 (0.019)
Train: 466 [ 300/1251 ( 24%)]  Loss: 3.544 (3.24)  Time: 0.682s, 1500.41/s  (0.699s, 1465.43/s)  LR: 1.269e-04  Data: 0.017 (0.018)
Train: 466 [ 350/1251 ( 28%)]  Loss: 3.245 (3.24)  Time: 0.712s, 1438.66/s  (0.699s, 1465.38/s)  LR: 1.269e-04  Data: 0.010 (0.017)
Train: 466 [ 400/1251 ( 32%)]  Loss: 3.390 (3.25)  Time: 0.725s, 1412.83/s  (0.698s, 1466.73/s)  LR: 1.269e-04  Data: 0.010 (0.016)
Train: 466 [ 450/1251 ( 36%)]  Loss: 3.375 (3.27)  Time: 0.703s, 1456.49/s  (0.697s, 1468.22/s)  LR: 1.269e-04  Data: 0.010 (0.015)
Train: 466 [ 500/1251 ( 40%)]  Loss: 3.574 (3.29)  Time: 0.715s, 1431.59/s  (0.696s, 1471.07/s)  LR: 1.269e-04  Data: 0.011 (0.015)
Train: 466 [ 550/1251 ( 44%)]  Loss: 3.284 (3.29)  Time: 0.701s, 1461.24/s  (0.696s, 1470.75/s)  LR: 1.269e-04  Data: 0.009 (0.015)
Train: 466 [ 600/1251 ( 48%)]  Loss: 3.402 (3.30)  Time: 0.706s, 1449.53/s  (0.696s, 1471.00/s)  LR: 1.269e-04  Data: 0.011 (0.014)
Train: 466 [ 650/1251 ( 52%)]  Loss: 3.214 (3.29)  Time: 0.704s, 1454.70/s  (0.696s, 1470.82/s)  LR: 1.269e-04  Data: 0.010 (0.014)
Train: 466 [ 700/1251 ( 56%)]  Loss: 3.386 (3.30)  Time: 0.671s, 1525.29/s  (0.697s, 1470.17/s)  LR: 1.269e-04  Data: 0.011 (0.014)
Train: 466 [ 750/1251 ( 60%)]  Loss: 3.578 (3.32)  Time: 0.674s, 1518.92/s  (0.696s, 1471.35/s)  LR: 1.269e-04  Data: 0.010 (0.013)
Train: 466 [ 800/1251 ( 64%)]  Loss: 3.408 (3.32)  Time: 0.685s, 1494.90/s  (0.695s, 1472.92/s)  LR: 1.269e-04  Data: 0.011 (0.013)
Train: 466 [ 850/1251 ( 68%)]  Loss: 3.349 (3.33)  Time: 0.668s, 1532.55/s  (0.695s, 1473.56/s)  LR: 1.269e-04  Data: 0.009 (0.013)
Train: 466 [ 900/1251 ( 72%)]  Loss: 3.593 (3.34)  Time: 0.746s, 1372.61/s  (0.695s, 1473.80/s)  LR: 1.269e-04  Data: 0.009 (0.013)
Train: 466 [ 950/1251 ( 76%)]  Loss: 3.314 (3.34)  Time: 0.664s, 1541.35/s  (0.695s, 1473.73/s)  LR: 1.269e-04  Data: 0.011 (0.013)
Train: 466 [1000/1251 ( 80%)]  Loss: 3.185 (3.33)  Time: 0.725s, 1412.01/s  (0.695s, 1473.01/s)  LR: 1.269e-04  Data: 0.009 (0.013)
Train: 466 [1050/1251 ( 84%)]  Loss: 3.494 (3.34)  Time: 0.694s, 1476.47/s  (0.695s, 1472.37/s)  LR: 1.269e-04  Data: 0.010 (0.013)
Train: 466 [1100/1251 ( 88%)]  Loss: 3.242 (3.33)  Time: 0.665s, 1540.25/s  (0.695s, 1472.75/s)  LR: 1.269e-04  Data: 0.009 (0.013)
Train: 466 [1150/1251 ( 92%)]  Loss: 3.402 (3.34)  Time: 0.706s, 1450.85/s  (0.695s, 1472.87/s)  LR: 1.269e-04  Data: 0.011 (0.012)
Train: 466 [1200/1251 ( 96%)]  Loss: 3.334 (3.34)  Time: 0.671s, 1525.40/s  (0.695s, 1473.25/s)  LR: 1.269e-04  Data: 0.011 (0.012)
Train: 466 [1250/1251 (100%)]  Loss: 2.956 (3.32)  Time: 0.656s, 1561.07/s  (0.695s, 1473.76/s)  LR: 1.269e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.555 (1.555)  Loss:  0.7251 (0.7251)  Acc@1: 90.9180 (90.9180)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.137 (0.576)  Loss:  0.8701 (1.2442)  Acc@1: 85.3774 (77.1500)  Acc@5: 96.6981 (93.5860)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-465.pth.tar', 77.37799997802735)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-455.pth.tar', 77.37399998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-454.pth.tar', 77.32800008300781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-459.pth.tar', 77.28200003417969)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-462.pth.tar', 77.23800013671875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-460.pth.tar', 77.20400000732423)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-461.pth.tar', 77.16799992919921)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-466.pth.tar', 77.15000011230468)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-463.pth.tar', 77.12599992675781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-456.pth.tar', 77.04800002929687)

Train: 467 [   0/1251 (  0%)]  Loss: 3.401 (3.40)  Time: 2.685s,  381.39/s  (2.685s,  381.39/s)  LR: 1.253e-04  Data: 2.005 (2.005)
Train: 467 [  50/1251 (  4%)]  Loss: 3.376 (3.39)  Time: 0.671s, 1526.35/s  (0.730s, 1402.99/s)  LR: 1.253e-04  Data: 0.010 (0.050)
Train: 467 [ 100/1251 (  8%)]  Loss: 3.329 (3.37)  Time: 0.680s, 1505.59/s  (0.714s, 1434.85/s)  LR: 1.253e-04  Data: 0.010 (0.030)
Train: 467 [ 150/1251 ( 12%)]  Loss: 2.986 (3.27)  Time: 0.797s, 1285.12/s  (0.709s, 1443.56/s)  LR: 1.253e-04  Data: 0.010 (0.024)
Train: 467 [ 200/1251 ( 16%)]  Loss: 3.454 (3.31)  Time: 0.707s, 1447.85/s  (0.705s, 1453.25/s)  LR: 1.253e-04  Data: 0.012 (0.020)
Train: 467 [ 250/1251 ( 20%)]  Loss: 3.234 (3.30)  Time: 0.706s, 1450.42/s  (0.702s, 1459.35/s)  LR: 1.253e-04  Data: 0.010 (0.019)
Train: 467 [ 300/1251 ( 24%)]  Loss: 3.421 (3.31)  Time: 0.701s, 1459.91/s  (0.699s, 1464.15/s)  LR: 1.253e-04  Data: 0.009 (0.017)
Train: 467 [ 350/1251 ( 28%)]  Loss: 3.120 (3.29)  Time: 0.732s, 1398.74/s  (0.698s, 1466.99/s)  LR: 1.253e-04  Data: 0.012 (0.016)
Train: 467 [ 400/1251 ( 32%)]  Loss: 3.405 (3.30)  Time: 0.671s, 1525.74/s  (0.696s, 1470.31/s)  LR: 1.253e-04  Data: 0.012 (0.016)
Train: 467 [ 450/1251 ( 36%)]  Loss: 3.538 (3.33)  Time: 0.712s, 1438.83/s  (0.696s, 1472.20/s)  LR: 1.253e-04  Data: 0.010 (0.015)
Train: 467 [ 500/1251 ( 40%)]  Loss: 3.178 (3.31)  Time: 0.671s, 1526.12/s  (0.695s, 1473.35/s)  LR: 1.253e-04  Data: 0.010 (0.015)
Train: 467 [ 550/1251 ( 44%)]  Loss: 3.392 (3.32)  Time: 0.711s, 1440.49/s  (0.695s, 1474.07/s)  LR: 1.253e-04  Data: 0.010 (0.014)
Train: 467 [ 600/1251 ( 48%)]  Loss: 3.123 (3.30)  Time: 0.705s, 1453.41/s  (0.695s, 1474.42/s)  LR: 1.253e-04  Data: 0.011 (0.014)
Train: 467 [ 650/1251 ( 52%)]  Loss: 3.268 (3.30)  Time: 0.748s, 1368.39/s  (0.694s, 1475.05/s)  LR: 1.253e-04  Data: 0.009 (0.014)
Train: 467 [ 700/1251 ( 56%)]  Loss: 2.886 (3.27)  Time: 0.705s, 1452.69/s  (0.694s, 1475.14/s)  LR: 1.253e-04  Data: 0.010 (0.013)
Train: 467 [ 750/1251 ( 60%)]  Loss: 3.406 (3.28)  Time: 0.669s, 1529.69/s  (0.694s, 1476.25/s)  LR: 1.253e-04  Data: 0.009 (0.013)
Train: 467 [ 800/1251 ( 64%)]  Loss: 3.527 (3.30)  Time: 0.699s, 1464.84/s  (0.694s, 1476.54/s)  LR: 1.253e-04  Data: 0.009 (0.013)
Train: 467 [ 850/1251 ( 68%)]  Loss: 3.586 (3.31)  Time: 0.674s, 1518.93/s  (0.693s, 1476.90/s)  LR: 1.253e-04  Data: 0.010 (0.013)
Train: 467 [ 900/1251 ( 72%)]  Loss: 3.127 (3.30)  Time: 0.723s, 1415.71/s  (0.693s, 1477.14/s)  LR: 1.253e-04  Data: 0.010 (0.013)
Train: 467 [ 950/1251 ( 76%)]  Loss: 3.259 (3.30)  Time: 0.758s, 1350.05/s  (0.694s, 1476.56/s)  LR: 1.253e-04  Data: 0.011 (0.013)
Train: 467 [1000/1251 ( 80%)]  Loss: 3.743 (3.32)  Time: 0.685s, 1495.31/s  (0.693s, 1477.20/s)  LR: 1.253e-04  Data: 0.009 (0.013)
Train: 467 [1050/1251 ( 84%)]  Loss: 3.183 (3.32)  Time: 0.800s, 1279.81/s  (0.694s, 1476.51/s)  LR: 1.253e-04  Data: 0.012 (0.012)
Train: 467 [1100/1251 ( 88%)]  Loss: 3.454 (3.32)  Time: 0.674s, 1518.20/s  (0.693s, 1476.90/s)  LR: 1.253e-04  Data: 0.014 (0.012)
Train: 467 [1150/1251 ( 92%)]  Loss: 3.633 (3.33)  Time: 0.669s, 1529.76/s  (0.693s, 1477.12/s)  LR: 1.253e-04  Data: 0.010 (0.012)
Train: 467 [1200/1251 ( 96%)]  Loss: 3.372 (3.34)  Time: 0.687s, 1490.03/s  (0.693s, 1476.62/s)  LR: 1.253e-04  Data: 0.009 (0.012)
Train: 467 [1250/1251 (100%)]  Loss: 3.098 (3.33)  Time: 0.655s, 1562.94/s  (0.693s, 1477.19/s)  LR: 1.253e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.465 (1.465)  Loss:  0.7446 (0.7446)  Acc@1: 91.5039 (91.5039)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.136 (0.587)  Loss:  0.8965 (1.2744)  Acc@1: 85.8491 (77.3560)  Acc@5: 96.9340 (93.4900)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-465.pth.tar', 77.37799997802735)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-455.pth.tar', 77.37399998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-467.pth.tar', 77.35600005859375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-454.pth.tar', 77.32800008300781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-459.pth.tar', 77.28200003417969)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-462.pth.tar', 77.23800013671875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-460.pth.tar', 77.20400000732423)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-461.pth.tar', 77.16799992919921)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-466.pth.tar', 77.15000011230468)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-463.pth.tar', 77.12599992675781)

Train: 468 [   0/1251 (  0%)]  Loss: 3.318 (3.32)  Time: 2.372s,  431.71/s  (2.372s,  431.71/s)  LR: 1.236e-04  Data: 1.738 (1.738)
Train: 468 [  50/1251 (  4%)]  Loss: 3.709 (3.51)  Time: 0.703s, 1457.21/s  (0.740s, 1384.41/s)  LR: 1.236e-04  Data: 0.010 (0.047)
Train: 468 [ 100/1251 (  8%)]  Loss: 3.415 (3.48)  Time: 0.674s, 1519.47/s  (0.716s, 1430.30/s)  LR: 1.236e-04  Data: 0.011 (0.029)
Train: 468 [ 150/1251 ( 12%)]  Loss: 3.401 (3.46)  Time: 0.737s, 1388.78/s  (0.711s, 1441.07/s)  LR: 1.236e-04  Data: 0.012 (0.023)
Train: 468 [ 200/1251 ( 16%)]  Loss: 3.286 (3.43)  Time: 0.702s, 1458.03/s  (0.708s, 1446.81/s)  LR: 1.236e-04  Data: 0.009 (0.020)
Train: 468 [ 250/1251 ( 20%)]  Loss: 3.220 (3.39)  Time: 0.701s, 1460.46/s  (0.704s, 1454.49/s)  LR: 1.236e-04  Data: 0.009 (0.018)
Train: 468 [ 300/1251 ( 24%)]  Loss: 3.235 (3.37)  Time: 0.731s, 1400.14/s  (0.703s, 1457.39/s)  LR: 1.236e-04  Data: 0.013 (0.017)
Train: 468 [ 350/1251 ( 28%)]  Loss: 3.246 (3.35)  Time: 0.713s, 1436.27/s  (0.704s, 1453.57/s)  LR: 1.236e-04  Data: 0.012 (0.016)
Train: 468 [ 400/1251 ( 32%)]  Loss: 3.306 (3.35)  Time: 0.742s, 1379.19/s  (0.705s, 1452.35/s)  LR: 1.236e-04  Data: 0.010 (0.016)
Train: 468 [ 450/1251 ( 36%)]  Loss: 3.188 (3.33)  Time: 0.732s, 1398.67/s  (0.706s, 1450.28/s)  LR: 1.236e-04  Data: 0.011 (0.015)
Train: 468 [ 500/1251 ( 40%)]  Loss: 3.217 (3.32)  Time: 0.704s, 1453.56/s  (0.704s, 1453.74/s)  LR: 1.236e-04  Data: 0.010 (0.015)
Train: 468 [ 550/1251 ( 44%)]  Loss: 3.542 (3.34)  Time: 0.675s, 1517.99/s  (0.702s, 1459.00/s)  LR: 1.236e-04  Data: 0.011 (0.014)
Train: 468 [ 600/1251 ( 48%)]  Loss: 2.943 (3.31)  Time: 0.703s, 1456.87/s  (0.700s, 1463.13/s)  LR: 1.236e-04  Data: 0.010 (0.014)
Train: 468 [ 650/1251 ( 52%)]  Loss: 3.443 (3.32)  Time: 0.737s, 1388.88/s  (0.699s, 1464.00/s)  LR: 1.236e-04  Data: 0.010 (0.014)
Train: 468 [ 700/1251 ( 56%)]  Loss: 3.372 (3.32)  Time: 0.672s, 1524.41/s  (0.699s, 1464.76/s)  LR: 1.236e-04  Data: 0.011 (0.013)
Train: 468 [ 750/1251 ( 60%)]  Loss: 3.280 (3.32)  Time: 0.674s, 1519.81/s  (0.699s, 1465.51/s)  LR: 1.236e-04  Data: 0.011 (0.013)
Train: 468 [ 800/1251 ( 64%)]  Loss: 3.475 (3.33)  Time: 0.678s, 1509.27/s  (0.698s, 1466.42/s)  LR: 1.236e-04  Data: 0.009 (0.013)
Train: 468 [ 850/1251 ( 68%)]  Loss: 3.341 (3.33)  Time: 0.701s, 1461.08/s  (0.698s, 1467.25/s)  LR: 1.236e-04  Data: 0.012 (0.013)
Train: 468 [ 900/1251 ( 72%)]  Loss: 3.617 (3.34)  Time: 0.713s, 1435.77/s  (0.698s, 1467.97/s)  LR: 1.236e-04  Data: 0.010 (0.013)
Train: 468 [ 950/1251 ( 76%)]  Loss: 3.510 (3.35)  Time: 0.680s, 1505.00/s  (0.697s, 1468.55/s)  LR: 1.236e-04  Data: 0.011 (0.013)
Train: 468 [1000/1251 ( 80%)]  Loss: 3.019 (3.34)  Time: 0.680s, 1506.33/s  (0.697s, 1468.72/s)  LR: 1.236e-04  Data: 0.012 (0.013)
Train: 468 [1050/1251 ( 84%)]  Loss: 3.675 (3.35)  Time: 0.686s, 1492.06/s  (0.697s, 1469.04/s)  LR: 1.236e-04  Data: 0.011 (0.012)
Train: 468 [1100/1251 ( 88%)]  Loss: 3.490 (3.36)  Time: 0.706s, 1451.01/s  (0.697s, 1469.51/s)  LR: 1.236e-04  Data: 0.009 (0.012)
Train: 468 [1150/1251 ( 92%)]  Loss: 3.436 (3.36)  Time: 0.687s, 1489.58/s  (0.697s, 1470.03/s)  LR: 1.236e-04  Data: 0.009 (0.012)
Train: 468 [1200/1251 ( 96%)]  Loss: 3.668 (3.37)  Time: 0.682s, 1502.18/s  (0.696s, 1470.44/s)  LR: 1.236e-04  Data: 0.011 (0.012)
Train: 468 [1250/1251 (100%)]  Loss: 3.460 (3.38)  Time: 0.657s, 1558.39/s  (0.696s, 1471.14/s)  LR: 1.236e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.507 (1.507)  Loss:  0.7788 (0.7788)  Acc@1: 90.7227 (90.7227)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.136 (0.571)  Loss:  0.8955 (1.2765)  Acc@1: 86.5566 (77.3340)  Acc@5: 96.9340 (93.6580)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-465.pth.tar', 77.37799997802735)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-455.pth.tar', 77.37399998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-467.pth.tar', 77.35600005859375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-468.pth.tar', 77.33400010742187)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-454.pth.tar', 77.32800008300781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-459.pth.tar', 77.28200003417969)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-462.pth.tar', 77.23800013671875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-460.pth.tar', 77.20400000732423)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-461.pth.tar', 77.16799992919921)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-466.pth.tar', 77.15000011230468)

Train: 469 [   0/1251 (  0%)]  Loss: 3.091 (3.09)  Time: 2.457s,  416.79/s  (2.457s,  416.79/s)  LR: 1.219e-04  Data: 1.824 (1.824)
Train: 469 [  50/1251 (  4%)]  Loss: 3.831 (3.46)  Time: 0.688s, 1487.96/s  (0.734s, 1395.51/s)  LR: 1.219e-04  Data: 0.009 (0.048)
Train: 469 [ 100/1251 (  8%)]  Loss: 3.240 (3.39)  Time: 0.670s, 1527.79/s  (0.717s, 1428.01/s)  LR: 1.219e-04  Data: 0.010 (0.029)
Train: 469 [ 150/1251 ( 12%)]  Loss: 3.344 (3.38)  Time: 0.675s, 1517.06/s  (0.711s, 1440.36/s)  LR: 1.219e-04  Data: 0.009 (0.023)
Train: 469 [ 200/1251 ( 16%)]  Loss: 3.089 (3.32)  Time: 0.670s, 1527.43/s  (0.707s, 1447.99/s)  LR: 1.219e-04  Data: 0.010 (0.020)
Train: 469 [ 250/1251 ( 20%)]  Loss: 3.311 (3.32)  Time: 0.676s, 1514.04/s  (0.704s, 1454.77/s)  LR: 1.219e-04  Data: 0.009 (0.018)
Train: 469 [ 300/1251 ( 24%)]  Loss: 3.434 (3.33)  Time: 0.704s, 1454.34/s  (0.702s, 1458.39/s)  LR: 1.219e-04  Data: 0.014 (0.017)
Train: 469 [ 350/1251 ( 28%)]  Loss: 3.413 (3.34)  Time: 0.722s, 1417.53/s  (0.701s, 1460.65/s)  LR: 1.219e-04  Data: 0.009 (0.016)
Train: 469 [ 400/1251 ( 32%)]  Loss: 2.968 (3.30)  Time: 0.707s, 1447.79/s  (0.700s, 1461.91/s)  LR: 1.219e-04  Data: 0.012 (0.015)
Train: 469 [ 450/1251 ( 36%)]  Loss: 3.263 (3.30)  Time: 0.707s, 1449.29/s  (0.699s, 1464.31/s)  LR: 1.219e-04  Data: 0.009 (0.015)
Train: 469 [ 500/1251 ( 40%)]  Loss: 3.305 (3.30)  Time: 0.690s, 1484.42/s  (0.698s, 1466.47/s)  LR: 1.219e-04  Data: 0.009 (0.014)
Train: 469 [ 550/1251 ( 44%)]  Loss: 3.222 (3.29)  Time: 0.753s, 1360.58/s  (0.698s, 1466.51/s)  LR: 1.219e-04  Data: 0.009 (0.014)
Train: 469 [ 600/1251 ( 48%)]  Loss: 3.143 (3.28)  Time: 0.671s, 1526.19/s  (0.698s, 1467.56/s)  LR: 1.219e-04  Data: 0.011 (0.014)
Train: 469 [ 650/1251 ( 52%)]  Loss: 3.695 (3.31)  Time: 0.674s, 1519.60/s  (0.697s, 1468.14/s)  LR: 1.219e-04  Data: 0.013 (0.013)
Train: 469 [ 700/1251 ( 56%)]  Loss: 3.512 (3.32)  Time: 0.686s, 1491.70/s  (0.697s, 1469.10/s)  LR: 1.219e-04  Data: 0.010 (0.013)
Train: 469 [ 750/1251 ( 60%)]  Loss: 3.434 (3.33)  Time: 0.729s, 1404.02/s  (0.696s, 1470.30/s)  LR: 1.219e-04  Data: 0.009 (0.013)
Train: 469 [ 800/1251 ( 64%)]  Loss: 3.464 (3.34)  Time: 0.683s, 1498.91/s  (0.696s, 1471.46/s)  LR: 1.219e-04  Data: 0.011 (0.013)
Train: 469 [ 850/1251 ( 68%)]  Loss: 3.474 (3.35)  Time: 0.681s, 1504.65/s  (0.696s, 1471.02/s)  LR: 1.219e-04  Data: 0.013 (0.013)
Train: 469 [ 900/1251 ( 72%)]  Loss: 3.451 (3.35)  Time: 0.671s, 1525.64/s  (0.696s, 1472.27/s)  LR: 1.219e-04  Data: 0.010 (0.013)
Train: 469 [ 950/1251 ( 76%)]  Loss: 3.208 (3.34)  Time: 0.687s, 1490.01/s  (0.695s, 1472.38/s)  LR: 1.219e-04  Data: 0.012 (0.013)
Train: 469 [1000/1251 ( 80%)]  Loss: 3.365 (3.35)  Time: 0.692s, 1478.85/s  (0.695s, 1472.38/s)  LR: 1.219e-04  Data: 0.009 (0.012)
Train: 469 [1050/1251 ( 84%)]  Loss: 3.736 (3.36)  Time: 0.689s, 1486.08/s  (0.695s, 1472.84/s)  LR: 1.219e-04  Data: 0.014 (0.012)
Train: 469 [1100/1251 ( 88%)]  Loss: 3.254 (3.36)  Time: 0.701s, 1460.86/s  (0.695s, 1472.54/s)  LR: 1.219e-04  Data: 0.010 (0.012)
Train: 469 [1150/1251 ( 92%)]  Loss: 3.440 (3.36)  Time: 0.671s, 1524.97/s  (0.695s, 1472.97/s)  LR: 1.219e-04  Data: 0.010 (0.012)
Train: 469 [1200/1251 ( 96%)]  Loss: 3.188 (3.36)  Time: 0.698s, 1466.66/s  (0.695s, 1472.60/s)  LR: 1.219e-04  Data: 0.010 (0.012)
Train: 469 [1250/1251 (100%)]  Loss: 3.307 (3.35)  Time: 0.656s, 1561.27/s  (0.695s, 1472.56/s)  LR: 1.219e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.558 (1.558)  Loss:  0.7295 (0.7295)  Acc@1: 91.4062 (91.4062)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.136 (0.586)  Loss:  0.8740 (1.2205)  Acc@1: 85.9670 (77.3460)  Acc@5: 96.1085 (93.5180)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-465.pth.tar', 77.37799997802735)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-455.pth.tar', 77.37399998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-467.pth.tar', 77.35600005859375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-469.pth.tar', 77.34599998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-468.pth.tar', 77.33400010742187)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-454.pth.tar', 77.32800008300781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-459.pth.tar', 77.28200003417969)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-462.pth.tar', 77.23800013671875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-460.pth.tar', 77.20400000732423)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-461.pth.tar', 77.16799992919921)

Train: 470 [   0/1251 (  0%)]  Loss: 3.529 (3.53)  Time: 2.219s,  461.46/s  (2.219s,  461.46/s)  LR: 1.203e-04  Data: 1.603 (1.603)
Train: 470 [  50/1251 (  4%)]  Loss: 3.219 (3.37)  Time: 0.675s, 1517.58/s  (0.733s, 1396.18/s)  LR: 1.203e-04  Data: 0.010 (0.052)
Train: 470 [ 100/1251 (  8%)]  Loss: 3.499 (3.42)  Time: 0.675s, 1517.36/s  (0.712s, 1437.64/s)  LR: 1.203e-04  Data: 0.014 (0.031)
Train: 470 [ 150/1251 ( 12%)]  Loss: 3.674 (3.48)  Time: 0.671s, 1525.10/s  (0.706s, 1450.48/s)  LR: 1.203e-04  Data: 0.010 (0.024)
Train: 470 [ 200/1251 ( 16%)]  Loss: 3.139 (3.41)  Time: 0.692s, 1480.19/s  (0.701s, 1460.52/s)  LR: 1.203e-04  Data: 0.009 (0.021)
Train: 470 [ 250/1251 ( 20%)]  Loss: 3.239 (3.38)  Time: 0.673s, 1522.18/s  (0.699s, 1464.40/s)  LR: 1.203e-04  Data: 0.011 (0.019)
Train: 470 [ 300/1251 ( 24%)]  Loss: 2.848 (3.31)  Time: 0.675s, 1516.53/s  (0.698s, 1467.64/s)  LR: 1.203e-04  Data: 0.011 (0.017)
Train: 470 [ 350/1251 ( 28%)]  Loss: 3.479 (3.33)  Time: 0.715s, 1432.96/s  (0.697s, 1468.67/s)  LR: 1.203e-04  Data: 0.010 (0.016)
Train: 470 [ 400/1251 ( 32%)]  Loss: 3.517 (3.35)  Time: 0.670s, 1528.07/s  (0.697s, 1470.12/s)  LR: 1.203e-04  Data: 0.011 (0.016)
Train: 470 [ 450/1251 ( 36%)]  Loss: 3.376 (3.35)  Time: 0.674s, 1518.54/s  (0.696s, 1470.57/s)  LR: 1.203e-04  Data: 0.011 (0.015)
Train: 470 [ 500/1251 ( 40%)]  Loss: 3.536 (3.37)  Time: 0.990s, 1033.85/s  (0.702s, 1459.09/s)  LR: 1.203e-04  Data: 0.014 (0.015)
Train: 470 [ 550/1251 ( 44%)]  Loss: 3.360 (3.37)  Time: 0.724s, 1414.64/s  (0.711s, 1440.84/s)  LR: 1.203e-04  Data: 0.020 (0.015)
Train: 470 [ 600/1251 ( 48%)]  Loss: 3.712 (3.39)  Time: 0.741s, 1381.84/s  (0.717s, 1427.24/s)  LR: 1.203e-04  Data: 0.014 (0.015)
Train: 470 [ 650/1251 ( 52%)]  Loss: 3.598 (3.41)  Time: 0.733s, 1397.48/s  (0.725s, 1413.06/s)  LR: 1.203e-04  Data: 0.011 (0.015)
Train: 470 [ 700/1251 ( 56%)]  Loss: 3.542 (3.42)  Time: 0.718s, 1426.14/s  (0.728s, 1406.64/s)  LR: 1.203e-04  Data: 0.016 (0.015)
Train: 470 [ 750/1251 ( 60%)]  Loss: 3.684 (3.43)  Time: 0.728s, 1406.16/s  (0.729s, 1404.59/s)  LR: 1.203e-04  Data: 0.010 (0.015)
Train: 470 [ 800/1251 ( 64%)]  Loss: 3.555 (3.44)  Time: 0.726s, 1411.09/s  (0.727s, 1409.23/s)  LR: 1.203e-04  Data: 0.009 (0.015)
Train: 470 [ 850/1251 ( 68%)]  Loss: 3.537 (3.45)  Time: 0.703s, 1457.06/s  (0.724s, 1413.77/s)  LR: 1.203e-04  Data: 0.012 (0.015)
Train: 470 [ 900/1251 ( 72%)]  Loss: 3.508 (3.45)  Time: 0.672s, 1522.87/s  (0.723s, 1417.06/s)  LR: 1.203e-04  Data: 0.011 (0.015)
Train: 470 [ 950/1251 ( 76%)]  Loss: 3.387 (3.45)  Time: 0.675s, 1517.39/s  (0.721s, 1419.81/s)  LR: 1.203e-04  Data: 0.011 (0.014)
Train: 470 [1000/1251 ( 80%)]  Loss: 3.211 (3.44)  Time: 0.672s, 1524.15/s  (0.720s, 1422.11/s)  LR: 1.203e-04  Data: 0.010 (0.014)
Train: 470 [1050/1251 ( 84%)]  Loss: 3.682 (3.45)  Time: 0.673s, 1521.90/s  (0.719s, 1424.45/s)  LR: 1.203e-04  Data: 0.012 (0.014)
Train: 470 [1100/1251 ( 88%)]  Loss: 3.496 (3.45)  Time: 0.702s, 1458.57/s  (0.718s, 1426.95/s)  LR: 1.203e-04  Data: 0.009 (0.014)
Train: 470 [1150/1251 ( 92%)]  Loss: 3.158 (3.44)  Time: 0.715s, 1432.04/s  (0.717s, 1429.13/s)  LR: 1.203e-04  Data: 0.009 (0.014)
Train: 470 [1200/1251 ( 96%)]  Loss: 3.488 (3.44)  Time: 0.712s, 1438.44/s  (0.715s, 1431.26/s)  LR: 1.203e-04  Data: 0.012 (0.014)
Train: 470 [1250/1251 (100%)]  Loss: 3.405 (3.44)  Time: 0.669s, 1531.24/s  (0.715s, 1432.75/s)  LR: 1.203e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.626 (1.626)  Loss:  0.7578 (0.7578)  Acc@1: 90.5273 (90.5273)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.136 (0.578)  Loss:  0.8286 (1.2243)  Acc@1: 84.9057 (77.7320)  Acc@5: 96.9340 (93.6700)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-470.pth.tar', 77.73200003662109)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-465.pth.tar', 77.37799997802735)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-455.pth.tar', 77.37399998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-467.pth.tar', 77.35600005859375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-469.pth.tar', 77.34599998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-468.pth.tar', 77.33400010742187)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-454.pth.tar', 77.32800008300781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-459.pth.tar', 77.28200003417969)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-462.pth.tar', 77.23800013671875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-460.pth.tar', 77.20400000732423)

Train: 471 [   0/1251 (  0%)]  Loss: 3.553 (3.55)  Time: 2.346s,  436.40/s  (2.346s,  436.40/s)  LR: 1.187e-04  Data: 1.727 (1.727)
Train: 471 [  50/1251 (  4%)]  Loss: 3.295 (3.42)  Time: 0.666s, 1536.68/s  (0.735s, 1392.81/s)  LR: 1.187e-04  Data: 0.010 (0.054)
Train: 471 [ 100/1251 (  8%)]  Loss: 3.521 (3.46)  Time: 0.674s, 1518.66/s  (0.710s, 1442.21/s)  LR: 1.187e-04  Data: 0.014 (0.032)
Train: 471 [ 150/1251 ( 12%)]  Loss: 3.360 (3.43)  Time: 0.711s, 1439.96/s  (0.704s, 1454.16/s)  LR: 1.187e-04  Data: 0.009 (0.025)
Train: 471 [ 200/1251 ( 16%)]  Loss: 3.112 (3.37)  Time: 0.671s, 1524.95/s  (0.703s, 1456.45/s)  LR: 1.187e-04  Data: 0.011 (0.021)
Train: 471 [ 250/1251 ( 20%)]  Loss: 3.612 (3.41)  Time: 0.672s, 1524.32/s  (0.701s, 1461.04/s)  LR: 1.187e-04  Data: 0.010 (0.019)
Train: 471 [ 300/1251 ( 24%)]  Loss: 3.490 (3.42)  Time: 0.670s, 1527.23/s  (0.701s, 1461.19/s)  LR: 1.187e-04  Data: 0.009 (0.018)
Train: 471 [ 350/1251 ( 28%)]  Loss: 3.384 (3.42)  Time: 0.672s, 1523.07/s  (0.700s, 1463.40/s)  LR: 1.187e-04  Data: 0.009 (0.017)
Train: 471 [ 400/1251 ( 32%)]  Loss: 3.413 (3.42)  Time: 0.678s, 1510.05/s  (0.698s, 1466.60/s)  LR: 1.187e-04  Data: 0.011 (0.016)
Train: 471 [ 450/1251 ( 36%)]  Loss: 3.175 (3.39)  Time: 0.702s, 1457.86/s  (0.698s, 1467.55/s)  LR: 1.187e-04  Data: 0.009 (0.015)
Train: 471 [ 500/1251 ( 40%)]  Loss: 3.589 (3.41)  Time: 0.670s, 1527.23/s  (0.697s, 1468.78/s)  LR: 1.187e-04  Data: 0.010 (0.015)
Train: 471 [ 550/1251 ( 44%)]  Loss: 3.415 (3.41)  Time: 0.708s, 1446.51/s  (0.697s, 1469.47/s)  LR: 1.187e-04  Data: 0.011 (0.014)
Train: 471 [ 600/1251 ( 48%)]  Loss: 3.272 (3.40)  Time: 0.728s, 1406.13/s  (0.697s, 1469.92/s)  LR: 1.187e-04  Data: 0.010 (0.014)
Train: 471 [ 650/1251 ( 52%)]  Loss: 2.921 (3.37)  Time: 0.671s, 1526.97/s  (0.696s, 1471.16/s)  LR: 1.187e-04  Data: 0.010 (0.014)
Train: 471 [ 700/1251 ( 56%)]  Loss: 3.647 (3.38)  Time: 0.744s, 1375.82/s  (0.696s, 1470.67/s)  LR: 1.187e-04  Data: 0.010 (0.014)
Train: 471 [ 750/1251 ( 60%)]  Loss: 3.467 (3.39)  Time: 0.680s, 1506.10/s  (0.696s, 1471.19/s)  LR: 1.187e-04  Data: 0.010 (0.013)
Train: 471 [ 800/1251 ( 64%)]  Loss: 3.445 (3.39)  Time: 0.679s, 1508.45/s  (0.696s, 1471.84/s)  LR: 1.187e-04  Data: 0.009 (0.013)
Train: 471 [ 850/1251 ( 68%)]  Loss: 3.599 (3.40)  Time: 0.675s, 1517.44/s  (0.696s, 1472.24/s)  LR: 1.187e-04  Data: 0.011 (0.013)
Train: 471 [ 900/1251 ( 72%)]  Loss: 3.297 (3.40)  Time: 0.708s, 1446.57/s  (0.696s, 1471.47/s)  LR: 1.187e-04  Data: 0.010 (0.013)
Train: 471 [ 950/1251 ( 76%)]  Loss: 3.269 (3.39)  Time: 0.671s, 1525.18/s  (0.696s, 1471.91/s)  LR: 1.187e-04  Data: 0.010 (0.013)
Train: 471 [1000/1251 ( 80%)]  Loss: 3.385 (3.39)  Time: 0.675s, 1516.46/s  (0.695s, 1472.52/s)  LR: 1.187e-04  Data: 0.010 (0.013)
Train: 471 [1050/1251 ( 84%)]  Loss: 3.535 (3.40)  Time: 0.781s, 1311.75/s  (0.696s, 1471.73/s)  LR: 1.187e-04  Data: 0.013 (0.013)
Train: 471 [1100/1251 ( 88%)]  Loss: 3.235 (3.39)  Time: 0.715s, 1432.67/s  (0.696s, 1472.23/s)  LR: 1.187e-04  Data: 0.010 (0.013)
Train: 471 [1150/1251 ( 92%)]  Loss: 3.543 (3.40)  Time: 0.710s, 1442.18/s  (0.695s, 1472.62/s)  LR: 1.187e-04  Data: 0.011 (0.012)
Train: 471 [1200/1251 ( 96%)]  Loss: 3.001 (3.38)  Time: 0.710s, 1441.78/s  (0.695s, 1472.87/s)  LR: 1.187e-04  Data: 0.009 (0.012)
Train: 471 [1250/1251 (100%)]  Loss: 3.544 (3.39)  Time: 0.656s, 1561.57/s  (0.695s, 1472.89/s)  LR: 1.187e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.568 (1.568)  Loss:  0.8228 (0.8228)  Acc@1: 91.1133 (91.1133)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.136 (0.591)  Loss:  0.9409 (1.3078)  Acc@1: 85.4953 (77.4980)  Acc@5: 96.3443 (93.5320)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-470.pth.tar', 77.73200003662109)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-471.pth.tar', 77.49800003417968)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-465.pth.tar', 77.37799997802735)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-455.pth.tar', 77.37399998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-467.pth.tar', 77.35600005859375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-469.pth.tar', 77.34599998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-468.pth.tar', 77.33400010742187)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-454.pth.tar', 77.32800008300781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-459.pth.tar', 77.28200003417969)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-462.pth.tar', 77.23800013671875)

Train: 472 [   0/1251 (  0%)]  Loss: 2.988 (2.99)  Time: 2.283s,  448.63/s  (2.283s,  448.63/s)  LR: 1.171e-04  Data: 1.654 (1.654)
Train: 472 [  50/1251 (  4%)]  Loss: 3.444 (3.22)  Time: 0.712s, 1438.72/s  (0.739s, 1386.25/s)  LR: 1.171e-04  Data: 0.011 (0.051)
Train: 472 [ 100/1251 (  8%)]  Loss: 3.104 (3.18)  Time: 0.703s, 1456.24/s  (0.717s, 1427.97/s)  LR: 1.171e-04  Data: 0.010 (0.031)
Train: 472 [ 150/1251 ( 12%)]  Loss: 2.937 (3.12)  Time: 0.672s, 1523.94/s  (0.709s, 1443.94/s)  LR: 1.171e-04  Data: 0.011 (0.024)
Train: 472 [ 200/1251 ( 16%)]  Loss: 3.190 (3.13)  Time: 0.694s, 1476.33/s  (0.705s, 1451.46/s)  LR: 1.171e-04  Data: 0.009 (0.021)
Train: 472 [ 250/1251 ( 20%)]  Loss: 3.622 (3.21)  Time: 0.769s, 1331.43/s  (0.707s, 1448.28/s)  LR: 1.171e-04  Data: 0.017 (0.019)
Train: 472 [ 300/1251 ( 24%)]  Loss: 3.224 (3.22)  Time: 0.812s, 1260.53/s  (0.726s, 1409.74/s)  LR: 1.171e-04  Data: 0.008 (0.033)
Train: 472 [ 350/1251 ( 28%)]  Loss: 3.456 (3.25)  Time: 0.715s, 1432.67/s  (0.733s, 1397.02/s)  LR: 1.171e-04  Data: 0.009 (0.035)
Train: 472 [ 400/1251 ( 32%)]  Loss: 2.880 (3.20)  Time: 1.155s,  886.78/s  (0.741s, 1381.32/s)  LR: 1.171e-04  Data: 0.015 (0.033)
Train: 472 [ 450/1251 ( 36%)]  Loss: 3.702 (3.25)  Time: 0.804s, 1273.34/s  (0.746s, 1372.80/s)  LR: 1.171e-04  Data: 0.013 (0.031)
Train: 472 [ 500/1251 ( 40%)]  Loss: 3.445 (3.27)  Time: 0.681s, 1502.77/s  (0.751s, 1363.36/s)  LR: 1.171e-04  Data: 0.010 (0.029)
Train: 472 [ 550/1251 ( 44%)]  Loss: 3.670 (3.31)  Time: 0.722s, 1418.46/s  (0.747s, 1371.34/s)  LR: 1.171e-04  Data: 0.010 (0.028)
Train: 472 [ 600/1251 ( 48%)]  Loss: 3.230 (3.30)  Time: 0.666s, 1537.56/s  (0.742s, 1380.02/s)  LR: 1.171e-04  Data: 0.011 (0.026)
Train: 472 [ 650/1251 ( 52%)]  Loss: 3.537 (3.32)  Time: 0.707s, 1447.96/s  (0.738s, 1387.70/s)  LR: 1.171e-04  Data: 0.010 (0.025)
Train: 472 [ 700/1251 ( 56%)]  Loss: 3.637 (3.34)  Time: 0.682s, 1502.51/s  (0.735s, 1393.45/s)  LR: 1.171e-04  Data: 0.009 (0.024)
Train: 472 [ 750/1251 ( 60%)]  Loss: 3.445 (3.34)  Time: 0.673s, 1521.43/s  (0.732s, 1398.22/s)  LR: 1.171e-04  Data: 0.011 (0.023)
Train: 472 [ 800/1251 ( 64%)]  Loss: 3.228 (3.34)  Time: 0.670s, 1527.90/s  (0.730s, 1403.44/s)  LR: 1.171e-04  Data: 0.011 (0.022)
Train: 472 [ 850/1251 ( 68%)]  Loss: 3.681 (3.36)  Time: 0.711s, 1441.09/s  (0.728s, 1406.93/s)  LR: 1.171e-04  Data: 0.011 (0.022)
Train: 472 [ 900/1251 ( 72%)]  Loss: 3.597 (3.37)  Time: 0.675s, 1518.11/s  (0.726s, 1411.29/s)  LR: 1.171e-04  Data: 0.012 (0.021)
Train: 472 [ 950/1251 ( 76%)]  Loss: 3.328 (3.37)  Time: 0.670s, 1527.82/s  (0.723s, 1415.78/s)  LR: 1.171e-04  Data: 0.010 (0.020)
Train: 472 [1000/1251 ( 80%)]  Loss: 3.730 (3.38)  Time: 0.672s, 1524.64/s  (0.722s, 1418.60/s)  LR: 1.171e-04  Data: 0.011 (0.020)
Train: 472 [1050/1251 ( 84%)]  Loss: 3.264 (3.38)  Time: 0.736s, 1391.27/s  (0.721s, 1420.53/s)  LR: 1.171e-04  Data: 0.009 (0.019)
Train: 472 [1100/1251 ( 88%)]  Loss: 3.023 (3.36)  Time: 0.675s, 1517.01/s  (0.720s, 1422.89/s)  LR: 1.171e-04  Data: 0.011 (0.019)
Train: 472 [1150/1251 ( 92%)]  Loss: 3.465 (3.37)  Time: 0.717s, 1427.25/s  (0.718s, 1425.33/s)  LR: 1.171e-04  Data: 0.009 (0.019)
Train: 472 [1200/1251 ( 96%)]  Loss: 2.787 (3.34)  Time: 0.691s, 1482.12/s  (0.717s, 1427.92/s)  LR: 1.171e-04  Data: 0.014 (0.018)
Train: 472 [1250/1251 (100%)]  Loss: 3.082 (3.33)  Time: 0.663s, 1543.53/s  (0.716s, 1429.59/s)  LR: 1.171e-04  Data: 0.000 (0.018)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.528 (1.528)  Loss:  0.7949 (0.7949)  Acc@1: 91.6992 (91.6992)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.136 (0.812)  Loss:  0.8608 (1.2502)  Acc@1: 85.4953 (77.5060)  Acc@5: 96.9340 (93.7320)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-470.pth.tar', 77.73200003662109)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-472.pth.tar', 77.50600016357421)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-471.pth.tar', 77.49800003417968)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-465.pth.tar', 77.37799997802735)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-455.pth.tar', 77.37399998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-467.pth.tar', 77.35600005859375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-469.pth.tar', 77.34599998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-468.pth.tar', 77.33400010742187)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-454.pth.tar', 77.32800008300781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-459.pth.tar', 77.28200003417969)

Train: 473 [   0/1251 (  0%)]  Loss: 3.274 (3.27)  Time: 3.258s,  314.26/s  (3.258s,  314.26/s)  LR: 1.155e-04  Data: 2.568 (2.568)
Train: 473 [  50/1251 (  4%)]  Loss: 2.939 (3.11)  Time: 0.754s, 1358.71/s  (0.851s, 1202.90/s)  LR: 1.155e-04  Data: 0.015 (0.086)
Train: 473 [ 100/1251 (  8%)]  Loss: 3.475 (3.23)  Time: 0.732s, 1398.24/s  (0.821s, 1247.44/s)  LR: 1.155e-04  Data: 0.014 (0.078)
Train: 473 [ 150/1251 ( 12%)]  Loss: 3.592 (3.32)  Time: 0.756s, 1354.94/s  (0.814s, 1257.59/s)  LR: 1.155e-04  Data: 0.014 (0.057)
Train: 473 [ 200/1251 ( 16%)]  Loss: 3.224 (3.30)  Time: 0.700s, 1463.62/s  (0.815s, 1256.09/s)  LR: 1.155e-04  Data: 0.017 (0.047)
Train: 473 [ 250/1251 ( 20%)]  Loss: 3.362 (3.31)  Time: 0.674s, 1518.40/s  (0.796s, 1286.11/s)  LR: 1.155e-04  Data: 0.010 (0.040)
Train: 473 [ 300/1251 ( 24%)]  Loss: 3.329 (3.31)  Time: 0.733s, 1396.72/s  (0.779s, 1315.00/s)  LR: 1.155e-04  Data: 0.009 (0.035)
Train: 473 [ 350/1251 ( 28%)]  Loss: 3.376 (3.32)  Time: 0.696s, 1470.88/s  (0.766s, 1336.11/s)  LR: 1.155e-04  Data: 0.010 (0.031)
Train: 473 [ 400/1251 ( 32%)]  Loss: 3.263 (3.31)  Time: 0.675s, 1516.89/s  (0.757s, 1353.41/s)  LR: 1.155e-04  Data: 0.010 (0.029)
Train: 473 [ 450/1251 ( 36%)]  Loss: 3.404 (3.32)  Time: 0.689s, 1486.35/s  (0.749s, 1366.51/s)  LR: 1.155e-04  Data: 0.010 (0.027)
Train: 473 [ 500/1251 ( 40%)]  Loss: 3.575 (3.35)  Time: 0.676s, 1513.88/s  (0.743s, 1378.06/s)  LR: 1.155e-04  Data: 0.009 (0.025)
Train: 473 [ 550/1251 ( 44%)]  Loss: 3.160 (3.33)  Time: 0.671s, 1526.85/s  (0.738s, 1386.95/s)  LR: 1.155e-04  Data: 0.010 (0.024)
Train: 473 [ 600/1251 ( 48%)]  Loss: 3.424 (3.34)  Time: 0.705s, 1453.41/s  (0.735s, 1393.42/s)  LR: 1.155e-04  Data: 0.009 (0.023)
Train: 473 [ 650/1251 ( 52%)]  Loss: 3.033 (3.32)  Time: 0.690s, 1484.74/s  (0.731s, 1400.02/s)  LR: 1.155e-04  Data: 0.009 (0.022)
Train: 473 [ 700/1251 ( 56%)]  Loss: 3.636 (3.34)  Time: 0.709s, 1445.24/s  (0.729s, 1404.85/s)  LR: 1.155e-04  Data: 0.012 (0.021)
Train: 473 [ 750/1251 ( 60%)]  Loss: 3.347 (3.34)  Time: 0.716s, 1430.66/s  (0.726s, 1409.61/s)  LR: 1.155e-04  Data: 0.010 (0.020)
Train: 473 [ 800/1251 ( 64%)]  Loss: 3.160 (3.33)  Time: 0.721s, 1419.78/s  (0.724s, 1413.93/s)  LR: 1.155e-04  Data: 0.010 (0.020)
Train: 473 [ 850/1251 ( 68%)]  Loss: 3.291 (3.33)  Time: 0.689s, 1485.31/s  (0.723s, 1417.03/s)  LR: 1.155e-04  Data: 0.015 (0.019)
Train: 473 [ 900/1251 ( 72%)]  Loss: 3.221 (3.32)  Time: 0.790s, 1295.83/s  (0.721s, 1420.54/s)  LR: 1.155e-04  Data: 0.010 (0.019)
Train: 473 [ 950/1251 ( 76%)]  Loss: 3.425 (3.33)  Time: 0.715s, 1432.58/s  (0.719s, 1423.70/s)  LR: 1.155e-04  Data: 0.010 (0.018)
Train: 473 [1000/1251 ( 80%)]  Loss: 3.157 (3.32)  Time: 0.674s, 1519.90/s  (0.718s, 1426.41/s)  LR: 1.155e-04  Data: 0.013 (0.018)
Train: 473 [1050/1251 ( 84%)]  Loss: 3.216 (3.31)  Time: 0.716s, 1430.19/s  (0.716s, 1429.35/s)  LR: 1.155e-04  Data: 0.009 (0.018)
Train: 473 [1100/1251 ( 88%)]  Loss: 3.412 (3.32)  Time: 0.701s, 1461.19/s  (0.715s, 1431.42/s)  LR: 1.155e-04  Data: 0.009 (0.017)
Train: 473 [1150/1251 ( 92%)]  Loss: 3.372 (3.32)  Time: 0.709s, 1443.73/s  (0.714s, 1433.53/s)  LR: 1.155e-04  Data: 0.010 (0.017)
Train: 473 [1200/1251 ( 96%)]  Loss: 3.591 (3.33)  Time: 0.683s, 1498.90/s  (0.713s, 1435.57/s)  LR: 1.155e-04  Data: 0.011 (0.017)
Train: 473 [1250/1251 (100%)]  Loss: 3.571 (3.34)  Time: 0.657s, 1558.33/s  (0.713s, 1436.89/s)  LR: 1.155e-04  Data: 0.000 (0.016)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.626 (1.626)  Loss:  0.8501 (0.8501)  Acc@1: 91.6992 (91.6992)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.136 (0.582)  Loss:  0.8770 (1.3105)  Acc@1: 85.2594 (77.4420)  Acc@5: 96.5802 (93.7060)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-470.pth.tar', 77.73200003662109)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-472.pth.tar', 77.50600016357421)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-471.pth.tar', 77.49800003417968)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-473.pth.tar', 77.44200006103516)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-465.pth.tar', 77.37799997802735)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-455.pth.tar', 77.37399998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-467.pth.tar', 77.35600005859375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-469.pth.tar', 77.34599998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-468.pth.tar', 77.33400010742187)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-454.pth.tar', 77.32800008300781)

Train: 474 [   0/1251 (  0%)]  Loss: 3.154 (3.15)  Time: 2.166s,  472.76/s  (2.166s,  472.76/s)  LR: 1.139e-04  Data: 1.550 (1.550)
Train: 474 [  50/1251 (  4%)]  Loss: 3.326 (3.24)  Time: 0.672s, 1524.84/s  (0.732s, 1399.36/s)  LR: 1.139e-04  Data: 0.011 (0.047)
Train: 474 [ 100/1251 (  8%)]  Loss: 3.657 (3.38)  Time: 0.709s, 1443.33/s  (0.709s, 1443.41/s)  LR: 1.139e-04  Data: 0.011 (0.029)
Train: 474 [ 150/1251 ( 12%)]  Loss: 3.682 (3.45)  Time: 0.691s, 1482.48/s  (0.704s, 1455.32/s)  LR: 1.139e-04  Data: 0.010 (0.023)
Train: 474 [ 200/1251 ( 16%)]  Loss: 3.510 (3.47)  Time: 0.672s, 1524.56/s  (0.702s, 1458.67/s)  LR: 1.139e-04  Data: 0.010 (0.020)
Train: 474 [ 250/1251 ( 20%)]  Loss: 3.208 (3.42)  Time: 0.707s, 1447.40/s  (0.701s, 1460.12/s)  LR: 1.139e-04  Data: 0.009 (0.018)
Train: 474 [ 300/1251 ( 24%)]  Loss: 3.314 (3.41)  Time: 0.731s, 1400.98/s  (0.700s, 1463.11/s)  LR: 1.139e-04  Data: 0.010 (0.017)
Train: 474 [ 350/1251 ( 28%)]  Loss: 3.597 (3.43)  Time: 0.704s, 1453.56/s  (0.698s, 1466.14/s)  LR: 1.139e-04  Data: 0.009 (0.016)
Train: 474 [ 400/1251 ( 32%)]  Loss: 3.450 (3.43)  Time: 0.673s, 1520.85/s  (0.698s, 1466.96/s)  LR: 1.139e-04  Data: 0.011 (0.015)
Train: 474 [ 450/1251 ( 36%)]  Loss: 3.283 (3.42)  Time: 0.690s, 1484.40/s  (0.697s, 1469.29/s)  LR: 1.139e-04  Data: 0.011 (0.015)
Train: 474 [ 500/1251 ( 40%)]  Loss: 3.687 (3.44)  Time: 0.703s, 1456.18/s  (0.697s, 1469.97/s)  LR: 1.139e-04  Data: 0.010 (0.014)
Train: 474 [ 550/1251 ( 44%)]  Loss: 3.452 (3.44)  Time: 0.673s, 1522.08/s  (0.696s, 1470.24/s)  LR: 1.139e-04  Data: 0.011 (0.014)
Train: 474 [ 600/1251 ( 48%)]  Loss: 3.518 (3.45)  Time: 0.679s, 1509.00/s  (0.696s, 1471.89/s)  LR: 1.139e-04  Data: 0.010 (0.014)
Train: 474 [ 650/1251 ( 52%)]  Loss: 3.684 (3.47)  Time: 0.704s, 1455.44/s  (0.695s, 1472.32/s)  LR: 1.139e-04  Data: 0.013 (0.013)
Train: 474 [ 700/1251 ( 56%)]  Loss: 3.086 (3.44)  Time: 0.734s, 1395.46/s  (0.696s, 1471.25/s)  LR: 1.139e-04  Data: 0.010 (0.013)
Train: 474 [ 750/1251 ( 60%)]  Loss: 3.052 (3.42)  Time: 0.690s, 1483.72/s  (0.696s, 1471.11/s)  LR: 1.139e-04  Data: 0.010 (0.013)
Train: 474 [ 800/1251 ( 64%)]  Loss: 3.461 (3.42)  Time: 0.704s, 1453.85/s  (0.696s, 1471.54/s)  LR: 1.139e-04  Data: 0.009 (0.013)
Train: 474 [ 850/1251 ( 68%)]  Loss: 3.716 (3.44)  Time: 0.671s, 1525.97/s  (0.695s, 1472.35/s)  LR: 1.139e-04  Data: 0.010 (0.013)
Train: 474 [ 900/1251 ( 72%)]  Loss: 3.206 (3.42)  Time: 0.671s, 1526.54/s  (0.696s, 1472.15/s)  LR: 1.139e-04  Data: 0.008 (0.013)
Train: 474 [ 950/1251 ( 76%)]  Loss: 3.453 (3.42)  Time: 0.682s, 1501.15/s  (0.695s, 1472.37/s)  LR: 1.139e-04  Data: 0.011 (0.012)
Train: 474 [1000/1251 ( 80%)]  Loss: 2.949 (3.40)  Time: 0.671s, 1526.02/s  (0.696s, 1472.13/s)  LR: 1.139e-04  Data: 0.009 (0.012)
Train: 474 [1050/1251 ( 84%)]  Loss: 3.549 (3.41)  Time: 0.683s, 1498.63/s  (0.696s, 1471.59/s)  LR: 1.139e-04  Data: 0.011 (0.012)
Train: 474 [1100/1251 ( 88%)]  Loss: 3.485 (3.41)  Time: 0.695s, 1474.27/s  (0.696s, 1471.41/s)  LR: 1.139e-04  Data: 0.012 (0.012)
Train: 474 [1150/1251 ( 92%)]  Loss: 3.209 (3.40)  Time: 0.711s, 1440.56/s  (0.696s, 1471.39/s)  LR: 1.139e-04  Data: 0.011 (0.012)
Train: 474 [1200/1251 ( 96%)]  Loss: 3.557 (3.41)  Time: 0.675s, 1516.47/s  (0.696s, 1471.82/s)  LR: 1.139e-04  Data: 0.010 (0.012)
Train: 474 [1250/1251 (100%)]  Loss: 3.020 (3.39)  Time: 0.657s, 1559.36/s  (0.696s, 1472.14/s)  LR: 1.139e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.624 (1.624)  Loss:  0.7305 (0.7305)  Acc@1: 91.1133 (91.1133)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.136 (0.580)  Loss:  0.8237 (1.2350)  Acc@1: 85.9670 (77.4340)  Acc@5: 96.9340 (93.7400)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-470.pth.tar', 77.73200003662109)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-472.pth.tar', 77.50600016357421)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-471.pth.tar', 77.49800003417968)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-473.pth.tar', 77.44200006103516)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-474.pth.tar', 77.43400010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-465.pth.tar', 77.37799997802735)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-455.pth.tar', 77.37399998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-467.pth.tar', 77.35600005859375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-469.pth.tar', 77.34599998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-468.pth.tar', 77.33400010742187)

Train: 475 [   0/1251 (  0%)]  Loss: 3.311 (3.31)  Time: 2.347s,  436.23/s  (2.347s,  436.23/s)  LR: 1.123e-04  Data: 1.716 (1.716)
Train: 475 [  50/1251 (  4%)]  Loss: 3.331 (3.32)  Time: 0.682s, 1501.43/s  (0.723s, 1416.91/s)  LR: 1.123e-04  Data: 0.013 (0.044)
Train: 475 [ 100/1251 (  8%)]  Loss: 3.491 (3.38)  Time: 0.671s, 1526.84/s  (0.708s, 1446.32/s)  LR: 1.123e-04  Data: 0.010 (0.027)
Train: 475 [ 150/1251 ( 12%)]  Loss: 3.193 (3.33)  Time: 0.671s, 1526.38/s  (0.705s, 1452.33/s)  LR: 1.123e-04  Data: 0.009 (0.022)
Train: 475 [ 200/1251 ( 16%)]  Loss: 3.640 (3.39)  Time: 0.682s, 1501.32/s  (0.702s, 1458.80/s)  LR: 1.123e-04  Data: 0.010 (0.019)
Train: 475 [ 250/1251 ( 20%)]  Loss: 3.532 (3.42)  Time: 0.672s, 1522.90/s  (0.699s, 1465.55/s)  LR: 1.123e-04  Data: 0.012 (0.017)
Train: 475 [ 300/1251 ( 24%)]  Loss: 3.417 (3.42)  Time: 0.672s, 1522.86/s  (0.696s, 1470.72/s)  LR: 1.123e-04  Data: 0.010 (0.016)
Train: 475 [ 350/1251 ( 28%)]  Loss: 2.878 (3.35)  Time: 0.737s, 1388.93/s  (0.696s, 1472.07/s)  LR: 1.123e-04  Data: 0.011 (0.015)
Train: 475 [ 400/1251 ( 32%)]  Loss: 3.578 (3.37)  Time: 0.672s, 1524.61/s  (0.695s, 1473.08/s)  LR: 1.123e-04  Data: 0.010 (0.014)
Train: 475 [ 450/1251 ( 36%)]  Loss: 3.507 (3.39)  Time: 0.719s, 1423.64/s  (0.695s, 1474.23/s)  LR: 1.123e-04  Data: 0.009 (0.014)
Train: 475 [ 500/1251 ( 40%)]  Loss: 3.874 (3.43)  Time: 0.700s, 1463.55/s  (0.695s, 1473.63/s)  LR: 1.123e-04  Data: 0.008 (0.014)
Train: 475 [ 550/1251 ( 44%)]  Loss: 3.323 (3.42)  Time: 0.699s, 1465.03/s  (0.695s, 1473.11/s)  LR: 1.123e-04  Data: 0.008 (0.013)
Train: 475 [ 600/1251 ( 48%)]  Loss: 3.616 (3.44)  Time: 0.714s, 1433.22/s  (0.695s, 1473.86/s)  LR: 1.123e-04  Data: 0.009 (0.013)
Train: 475 [ 650/1251 ( 52%)]  Loss: 3.045 (3.41)  Time: 0.671s, 1525.44/s  (0.695s, 1474.39/s)  LR: 1.123e-04  Data: 0.009 (0.013)
Train: 475 [ 700/1251 ( 56%)]  Loss: 3.459 (3.41)  Time: 0.711s, 1439.62/s  (0.694s, 1475.25/s)  LR: 1.123e-04  Data: 0.009 (0.013)
Train: 475 [ 750/1251 ( 60%)]  Loss: 3.270 (3.40)  Time: 0.666s, 1536.52/s  (0.695s, 1474.18/s)  LR: 1.123e-04  Data: 0.010 (0.013)
Train: 475 [ 800/1251 ( 64%)]  Loss: 3.401 (3.40)  Time: 0.710s, 1441.79/s  (0.694s, 1475.30/s)  LR: 1.123e-04  Data: 0.010 (0.012)
Train: 475 [ 850/1251 ( 68%)]  Loss: 3.353 (3.40)  Time: 0.669s, 1529.55/s  (0.694s, 1475.51/s)  LR: 1.123e-04  Data: 0.009 (0.012)
Train: 475 [ 900/1251 ( 72%)]  Loss: 3.673 (3.42)  Time: 0.770s, 1329.42/s  (0.694s, 1475.30/s)  LR: 1.123e-04  Data: 0.009 (0.012)
Train: 475 [ 950/1251 ( 76%)]  Loss: 3.261 (3.41)  Time: 0.671s, 1526.47/s  (0.694s, 1475.66/s)  LR: 1.123e-04  Data: 0.010 (0.012)
Train: 475 [1000/1251 ( 80%)]  Loss: 3.438 (3.41)  Time: 0.737s, 1389.22/s  (0.694s, 1475.44/s)  LR: 1.123e-04  Data: 0.009 (0.012)
Train: 475 [1050/1251 ( 84%)]  Loss: 3.471 (3.41)  Time: 0.704s, 1455.46/s  (0.694s, 1476.55/s)  LR: 1.123e-04  Data: 0.009 (0.012)
Train: 475 [1100/1251 ( 88%)]  Loss: 3.425 (3.41)  Time: 0.719s, 1424.58/s  (0.694s, 1475.85/s)  LR: 1.123e-04  Data: 0.011 (0.012)
Train: 475 [1150/1251 ( 92%)]  Loss: 3.539 (3.42)  Time: 0.713s, 1435.75/s  (0.693s, 1476.77/s)  LR: 1.123e-04  Data: 0.010 (0.012)
Train: 475 [1200/1251 ( 96%)]  Loss: 3.342 (3.41)  Time: 0.708s, 1446.90/s  (0.693s, 1476.95/s)  LR: 1.123e-04  Data: 0.010 (0.012)
Train: 475 [1250/1251 (100%)]  Loss: 3.150 (3.40)  Time: 0.687s, 1491.25/s  (0.694s, 1476.22/s)  LR: 1.123e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.551 (1.551)  Loss:  0.7422 (0.7422)  Acc@1: 90.9180 (90.9180)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.136 (0.585)  Loss:  0.8457 (1.2487)  Acc@1: 85.3774 (77.4500)  Acc@5: 96.8160 (93.7240)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-470.pth.tar', 77.73200003662109)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-472.pth.tar', 77.50600016357421)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-471.pth.tar', 77.49800003417968)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-475.pth.tar', 77.45000011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-473.pth.tar', 77.44200006103516)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-474.pth.tar', 77.43400010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-465.pth.tar', 77.37799997802735)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-455.pth.tar', 77.37399998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-467.pth.tar', 77.35600005859375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-469.pth.tar', 77.34599998046875)

Train: 476 [   0/1251 (  0%)]  Loss: 3.155 (3.15)  Time: 2.439s,  419.83/s  (2.439s,  419.83/s)  LR: 1.107e-04  Data: 1.824 (1.824)
Train: 476 [  50/1251 (  4%)]  Loss: 3.286 (3.22)  Time: 0.668s, 1532.00/s  (0.736s, 1391.39/s)  LR: 1.107e-04  Data: 0.009 (0.047)
Train: 476 [ 100/1251 (  8%)]  Loss: 3.332 (3.26)  Time: 0.702s, 1459.42/s  (0.715s, 1432.56/s)  LR: 1.107e-04  Data: 0.008 (0.029)
Train: 476 [ 150/1251 ( 12%)]  Loss: 3.548 (3.33)  Time: 0.671s, 1525.49/s  (0.706s, 1450.55/s)  LR: 1.107e-04  Data: 0.012 (0.023)
Train: 476 [ 200/1251 ( 16%)]  Loss: 3.145 (3.29)  Time: 0.673s, 1522.32/s  (0.703s, 1455.99/s)  LR: 1.107e-04  Data: 0.014 (0.020)
Train: 476 [ 250/1251 ( 20%)]  Loss: 3.383 (3.31)  Time: 0.704s, 1454.79/s  (0.701s, 1461.04/s)  LR: 1.107e-04  Data: 0.011 (0.018)
Train: 476 [ 300/1251 ( 24%)]  Loss: 3.185 (3.29)  Time: 0.679s, 1507.07/s  (0.702s, 1458.97/s)  LR: 1.107e-04  Data: 0.010 (0.017)
Train: 476 [ 350/1251 ( 28%)]  Loss: 3.363 (3.30)  Time: 0.678s, 1510.26/s  (0.700s, 1462.43/s)  LR: 1.107e-04  Data: 0.011 (0.016)
Train: 476 [ 400/1251 ( 32%)]  Loss: 3.112 (3.28)  Time: 0.671s, 1527.06/s  (0.699s, 1465.55/s)  LR: 1.107e-04  Data: 0.010 (0.015)
Train: 476 [ 450/1251 ( 36%)]  Loss: 2.966 (3.25)  Time: 0.673s, 1521.59/s  (0.698s, 1467.55/s)  LR: 1.107e-04  Data: 0.011 (0.015)
Train: 476 [ 500/1251 ( 40%)]  Loss: 3.401 (3.26)  Time: 0.671s, 1526.47/s  (0.697s, 1469.48/s)  LR: 1.107e-04  Data: 0.014 (0.014)
Train: 476 [ 550/1251 ( 44%)]  Loss: 3.299 (3.26)  Time: 0.688s, 1488.55/s  (0.697s, 1469.46/s)  LR: 1.107e-04  Data: 0.013 (0.014)
Train: 476 [ 600/1251 ( 48%)]  Loss: 3.814 (3.31)  Time: 0.672s, 1522.82/s  (0.696s, 1470.23/s)  LR: 1.107e-04  Data: 0.010 (0.014)
Train: 476 [ 650/1251 ( 52%)]  Loss: 3.613 (3.33)  Time: 0.670s, 1527.85/s  (0.696s, 1471.60/s)  LR: 1.107e-04  Data: 0.009 (0.013)
Train: 476 [ 700/1251 ( 56%)]  Loss: 3.373 (3.33)  Time: 0.779s, 1315.27/s  (0.696s, 1471.15/s)  LR: 1.107e-04  Data: 0.009 (0.013)
Train: 476 [ 750/1251 ( 60%)]  Loss: 3.361 (3.33)  Time: 0.708s, 1446.64/s  (0.698s, 1466.48/s)  LR: 1.107e-04  Data: 0.015 (0.013)
Train: 476 [ 800/1251 ( 64%)]  Loss: 3.199 (3.33)  Time: 0.768s, 1332.53/s  (0.704s, 1455.03/s)  LR: 1.107e-04  Data: 0.013 (0.017)
Train: 476 [ 850/1251 ( 68%)]  Loss: 3.119 (3.31)  Time: 0.970s, 1055.24/s  (0.709s, 1443.38/s)  LR: 1.107e-04  Data: 0.011 (0.018)
Train: 476 [ 900/1251 ( 72%)]  Loss: 3.383 (3.32)  Time: 0.731s, 1401.10/s  (0.712s, 1438.75/s)  LR: 1.107e-04  Data: 0.010 (0.018)
Train: 476 [ 950/1251 ( 76%)]  Loss: 3.380 (3.32)  Time: 0.708s, 1445.78/s  (0.715s, 1432.98/s)  LR: 1.107e-04  Data: 0.015 (0.017)
Train: 476 [1000/1251 ( 80%)]  Loss: 3.479 (3.33)  Time: 0.735s, 1393.59/s  (0.717s, 1428.41/s)  LR: 1.107e-04  Data: 0.050 (0.019)
Train: 476 [1050/1251 ( 84%)]  Loss: 3.344 (3.33)  Time: 0.706s, 1450.05/s  (0.718s, 1425.55/s)  LR: 1.107e-04  Data: 0.010 (0.019)
Train: 476 [1100/1251 ( 88%)]  Loss: 3.588 (3.34)  Time: 0.668s, 1532.48/s  (0.717s, 1427.80/s)  LR: 1.107e-04  Data: 0.010 (0.019)
Train: 476 [1150/1251 ( 92%)]  Loss: 3.320 (3.34)  Time: 0.708s, 1446.51/s  (0.716s, 1429.41/s)  LR: 1.107e-04  Data: 0.009 (0.018)
Train: 476 [1200/1251 ( 96%)]  Loss: 3.333 (3.34)  Time: 0.673s, 1520.54/s  (0.715s, 1431.25/s)  LR: 1.107e-04  Data: 0.014 (0.018)
Train: 476 [1250/1251 (100%)]  Loss: 3.333 (3.34)  Time: 0.691s, 1482.21/s  (0.714s, 1433.20/s)  LR: 1.107e-04  Data: 0.000 (0.018)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.626 (1.626)  Loss:  0.7686 (0.7686)  Acc@1: 91.4062 (91.4062)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.135 (0.583)  Loss:  0.9590 (1.3055)  Acc@1: 85.3774 (77.3460)  Acc@5: 97.5236 (93.7320)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-470.pth.tar', 77.73200003662109)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-472.pth.tar', 77.50600016357421)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-471.pth.tar', 77.49800003417968)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-475.pth.tar', 77.45000011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-473.pth.tar', 77.44200006103516)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-474.pth.tar', 77.43400010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-465.pth.tar', 77.37799997802735)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-455.pth.tar', 77.37399998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-467.pth.tar', 77.35600005859375)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-476.pth.tar', 77.34599998291016)

Train: 477 [   0/1251 (  0%)]  Loss: 3.362 (3.36)  Time: 2.191s,  467.27/s  (2.191s,  467.27/s)  LR: 1.092e-04  Data: 1.574 (1.574)
Train: 477 [  50/1251 (  4%)]  Loss: 3.445 (3.40)  Time: 0.697s, 1468.80/s  (0.729s, 1404.88/s)  LR: 1.092e-04  Data: 0.009 (0.049)
Train: 477 [ 100/1251 (  8%)]  Loss: 3.487 (3.43)  Time: 0.667s, 1535.73/s  (0.714s, 1434.30/s)  LR: 1.092e-04  Data: 0.010 (0.030)
Train: 477 [ 150/1251 ( 12%)]  Loss: 3.501 (3.45)  Time: 0.672s, 1524.56/s  (0.706s, 1451.35/s)  LR: 1.092e-04  Data: 0.010 (0.024)
Train: 477 [ 200/1251 ( 16%)]  Loss: 3.684 (3.50)  Time: 0.682s, 1502.20/s  (0.704s, 1454.79/s)  LR: 1.092e-04  Data: 0.012 (0.020)
Train: 477 [ 250/1251 ( 20%)]  Loss: 3.320 (3.47)  Time: 0.673s, 1521.62/s  (0.701s, 1461.15/s)  LR: 1.092e-04  Data: 0.010 (0.018)
Train: 477 [ 300/1251 ( 24%)]  Loss: 3.083 (3.41)  Time: 0.719s, 1423.67/s  (0.699s, 1464.73/s)  LR: 1.092e-04  Data: 0.010 (0.017)
Train: 477 [ 350/1251 ( 28%)]  Loss: 3.304 (3.40)  Time: 0.751s, 1362.67/s  (0.698s, 1467.28/s)  LR: 1.092e-04  Data: 0.010 (0.016)
Train: 477 [ 400/1251 ( 32%)]  Loss: 3.241 (3.38)  Time: 0.708s, 1446.56/s  (0.697s, 1469.04/s)  LR: 1.092e-04  Data: 0.009 (0.015)
Train: 477 [ 450/1251 ( 36%)]  Loss: 3.522 (3.39)  Time: 0.715s, 1431.46/s  (0.697s, 1469.80/s)  LR: 1.092e-04  Data: 0.011 (0.015)
Train: 477 [ 500/1251 ( 40%)]  Loss: 3.251 (3.38)  Time: 0.674s, 1519.38/s  (0.696s, 1470.32/s)  LR: 1.092e-04  Data: 0.011 (0.015)
Train: 477 [ 550/1251 ( 44%)]  Loss: 3.366 (3.38)  Time: 0.695s, 1472.54/s  (0.696s, 1471.00/s)  LR: 1.092e-04  Data: 0.010 (0.014)
Train: 477 [ 600/1251 ( 48%)]  Loss: 3.448 (3.39)  Time: 0.679s, 1507.40/s  (0.695s, 1472.77/s)  LR: 1.092e-04  Data: 0.011 (0.014)
Train: 477 [ 650/1251 ( 52%)]  Loss: 3.273 (3.38)  Time: 0.707s, 1448.55/s  (0.695s, 1473.40/s)  LR: 1.092e-04  Data: 0.011 (0.014)
Train: 477 [ 700/1251 ( 56%)]  Loss: 3.485 (3.38)  Time: 0.711s, 1440.42/s  (0.694s, 1474.55/s)  LR: 1.092e-04  Data: 0.010 (0.013)
Train: 477 [ 750/1251 ( 60%)]  Loss: 3.490 (3.39)  Time: 0.669s, 1531.45/s  (0.694s, 1475.18/s)  LR: 1.092e-04  Data: 0.009 (0.013)
Train: 477 [ 800/1251 ( 64%)]  Loss: 3.425 (3.39)  Time: 0.724s, 1415.34/s  (0.694s, 1475.28/s)  LR: 1.092e-04  Data: 0.010 (0.013)
Train: 477 [ 850/1251 ( 68%)]  Loss: 3.191 (3.38)  Time: 0.669s, 1531.44/s  (0.694s, 1475.53/s)  LR: 1.092e-04  Data: 0.009 (0.013)
Train: 477 [ 900/1251 ( 72%)]  Loss: 3.264 (3.38)  Time: 0.673s, 1522.24/s  (0.694s, 1475.64/s)  LR: 1.092e-04  Data: 0.010 (0.013)
Train: 477 [ 950/1251 ( 76%)]  Loss: 3.715 (3.39)  Time: 0.714s, 1435.00/s  (0.694s, 1474.89/s)  LR: 1.092e-04  Data: 0.010 (0.013)
Train: 477 [1000/1251 ( 80%)]  Loss: 3.336 (3.39)  Time: 0.671s, 1526.02/s  (0.694s, 1474.45/s)  LR: 1.092e-04  Data: 0.010 (0.012)
Train: 477 [1050/1251 ( 84%)]  Loss: 3.230 (3.38)  Time: 0.708s, 1446.25/s  (0.695s, 1474.41/s)  LR: 1.092e-04  Data: 0.011 (0.012)
Train: 477 [1100/1251 ( 88%)]  Loss: 3.445 (3.39)  Time: 0.670s, 1527.61/s  (0.694s, 1474.75/s)  LR: 1.092e-04  Data: 0.010 (0.012)
Train: 477 [1150/1251 ( 92%)]  Loss: 3.284 (3.38)  Time: 0.697s, 1468.84/s  (0.695s, 1474.19/s)  LR: 1.092e-04  Data: 0.008 (0.012)
Train: 477 [1200/1251 ( 96%)]  Loss: 3.334 (3.38)  Time: 0.679s, 1509.13/s  (0.694s, 1474.66/s)  LR: 1.092e-04  Data: 0.010 (0.012)
Train: 477 [1250/1251 (100%)]  Loss: 3.163 (3.37)  Time: 0.656s, 1559.86/s  (0.694s, 1475.38/s)  LR: 1.092e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.666 (1.666)  Loss:  0.6421 (0.6421)  Acc@1: 90.8203 (90.8203)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.137 (0.584)  Loss:  0.7803 (1.1584)  Acc@1: 85.7311 (77.5220)  Acc@5: 97.7594 (93.8260)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-470.pth.tar', 77.73200003662109)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-477.pth.tar', 77.52200000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-472.pth.tar', 77.50600016357421)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-471.pth.tar', 77.49800003417968)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-475.pth.tar', 77.45000011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-473.pth.tar', 77.44200006103516)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-474.pth.tar', 77.43400010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-465.pth.tar', 77.37799997802735)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-455.pth.tar', 77.37399998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-467.pth.tar', 77.35600005859375)

Train: 478 [   0/1251 (  0%)]  Loss: 3.471 (3.47)  Time: 2.238s,  457.61/s  (2.238s,  457.61/s)  LR: 1.076e-04  Data: 1.579 (1.579)
Train: 478 [  50/1251 (  4%)]  Loss: 3.295 (3.38)  Time: 0.672s, 1524.75/s  (0.721s, 1420.24/s)  LR: 1.076e-04  Data: 0.010 (0.046)
Train: 478 [ 100/1251 (  8%)]  Loss: 3.362 (3.38)  Time: 0.706s, 1450.53/s  (0.708s, 1445.64/s)  LR: 1.076e-04  Data: 0.009 (0.029)
Train: 478 [ 150/1251 ( 12%)]  Loss: 3.394 (3.38)  Time: 0.715s, 1431.88/s  (0.703s, 1455.97/s)  LR: 1.076e-04  Data: 0.009 (0.023)
Train: 478 [ 200/1251 ( 16%)]  Loss: 3.330 (3.37)  Time: 0.671s, 1526.04/s  (0.700s, 1463.01/s)  LR: 1.076e-04  Data: 0.011 (0.020)
Train: 478 [ 250/1251 ( 20%)]  Loss: 3.233 (3.35)  Time: 0.760s, 1347.65/s  (0.697s, 1468.20/s)  LR: 1.076e-04  Data: 0.010 (0.018)
Train: 478 [ 300/1251 ( 24%)]  Loss: 3.469 (3.36)  Time: 0.667s, 1536.15/s  (0.697s, 1470.15/s)  LR: 1.076e-04  Data: 0.010 (0.017)
Train: 478 [ 350/1251 ( 28%)]  Loss: 3.704 (3.41)  Time: 0.672s, 1524.72/s  (0.696s, 1471.03/s)  LR: 1.076e-04  Data: 0.010 (0.016)
Train: 478 [ 400/1251 ( 32%)]  Loss: 3.412 (3.41)  Time: 0.680s, 1505.16/s  (0.696s, 1471.38/s)  LR: 1.076e-04  Data: 0.010 (0.015)
Train: 478 [ 450/1251 ( 36%)]  Loss: 3.039 (3.37)  Time: 0.669s, 1531.29/s  (0.696s, 1471.61/s)  LR: 1.076e-04  Data: 0.009 (0.015)
Train: 478 [ 500/1251 ( 40%)]  Loss: 3.360 (3.37)  Time: 0.693s, 1477.82/s  (0.696s, 1471.79/s)  LR: 1.076e-04  Data: 0.011 (0.014)
Train: 478 [ 550/1251 ( 44%)]  Loss: 3.086 (3.35)  Time: 0.670s, 1527.80/s  (0.696s, 1472.07/s)  LR: 1.076e-04  Data: 0.010 (0.014)
Train: 478 [ 600/1251 ( 48%)]  Loss: 3.343 (3.35)  Time: 0.676s, 1515.36/s  (0.696s, 1472.10/s)  LR: 1.076e-04  Data: 0.011 (0.014)
Train: 478 [ 650/1251 ( 52%)]  Loss: 3.443 (3.35)  Time: 0.756s, 1354.06/s  (0.696s, 1470.91/s)  LR: 1.076e-04  Data: 0.011 (0.013)
Train: 478 [ 700/1251 ( 56%)]  Loss: 3.542 (3.37)  Time: 0.714s, 1434.52/s  (0.698s, 1467.79/s)  LR: 1.076e-04  Data: 0.013 (0.013)
Train: 478 [ 750/1251 ( 60%)]  Loss: 3.456 (3.37)  Time: 0.686s, 1491.63/s  (0.699s, 1465.60/s)  LR: 1.076e-04  Data: 0.014 (0.013)
Train: 478 [ 800/1251 ( 64%)]  Loss: 3.701 (3.39)  Time: 0.673s, 1522.03/s  (0.699s, 1464.78/s)  LR: 1.076e-04  Data: 0.011 (0.013)
Train: 478 [ 850/1251 ( 68%)]  Loss: 3.105 (3.37)  Time: 0.671s, 1525.83/s  (0.698s, 1466.65/s)  LR: 1.076e-04  Data: 0.011 (0.013)
Train: 478 [ 900/1251 ( 72%)]  Loss: 3.292 (3.37)  Time: 0.673s, 1522.61/s  (0.697s, 1469.68/s)  LR: 1.076e-04  Data: 0.011 (0.013)
Train: 478 [ 950/1251 ( 76%)]  Loss: 3.337 (3.37)  Time: 0.674s, 1519.29/s  (0.696s, 1471.78/s)  LR: 1.076e-04  Data: 0.014 (0.013)
Train: 478 [1000/1251 ( 80%)]  Loss: 3.070 (3.35)  Time: 0.671s, 1525.80/s  (0.695s, 1472.57/s)  LR: 1.076e-04  Data: 0.009 (0.013)
Train: 478 [1050/1251 ( 84%)]  Loss: 3.200 (3.35)  Time: 0.677s, 1511.67/s  (0.695s, 1472.98/s)  LR: 1.076e-04  Data: 0.011 (0.012)
Train: 478 [1100/1251 ( 88%)]  Loss: 3.090 (3.34)  Time: 0.707s, 1447.93/s  (0.695s, 1473.07/s)  LR: 1.076e-04  Data: 0.011 (0.012)
Train: 478 [1150/1251 ( 92%)]  Loss: 3.160 (3.33)  Time: 0.682s, 1501.89/s  (0.695s, 1473.32/s)  LR: 1.076e-04  Data: 0.011 (0.012)
Train: 478 [1200/1251 ( 96%)]  Loss: 3.293 (3.33)  Time: 0.672s, 1524.66/s  (0.695s, 1473.72/s)  LR: 1.076e-04  Data: 0.011 (0.012)
Train: 478 [1250/1251 (100%)]  Loss: 3.323 (3.33)  Time: 0.701s, 1460.10/s  (0.695s, 1473.67/s)  LR: 1.076e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.575 (1.575)  Loss:  0.8389 (0.8389)  Acc@1: 91.9922 (91.9922)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.136 (0.579)  Loss:  0.8857 (1.2586)  Acc@1: 85.7311 (77.4660)  Acc@5: 97.2877 (93.7780)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-470.pth.tar', 77.73200003662109)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-477.pth.tar', 77.52200000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-472.pth.tar', 77.50600016357421)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-471.pth.tar', 77.49800003417968)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-478.pth.tar', 77.46600000732421)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-475.pth.tar', 77.45000011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-473.pth.tar', 77.44200006103516)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-474.pth.tar', 77.43400010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-465.pth.tar', 77.37799997802735)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-455.pth.tar', 77.37399998046875)

Train: 479 [   0/1251 (  0%)]  Loss: 3.101 (3.10)  Time: 2.215s,  462.30/s  (2.215s,  462.30/s)  LR: 1.061e-04  Data: 1.560 (1.560)
Train: 479 [  50/1251 (  4%)]  Loss: 3.289 (3.19)  Time: 0.673s, 1522.65/s  (0.725s, 1412.14/s)  LR: 1.061e-04  Data: 0.010 (0.048)
Train: 479 [ 100/1251 (  8%)]  Loss: 3.593 (3.33)  Time: 0.673s, 1522.16/s  (0.707s, 1448.90/s)  LR: 1.061e-04  Data: 0.010 (0.030)
Train: 479 [ 150/1251 ( 12%)]  Loss: 3.234 (3.30)  Time: 0.740s, 1383.10/s  (0.703s, 1456.25/s)  LR: 1.061e-04  Data: 0.009 (0.023)
Train: 479 [ 200/1251 ( 16%)]  Loss: 3.053 (3.25)  Time: 0.671s, 1525.36/s  (0.701s, 1460.03/s)  LR: 1.061e-04  Data: 0.010 (0.020)
Train: 479 [ 250/1251 ( 20%)]  Loss: 3.211 (3.25)  Time: 0.720s, 1422.86/s  (0.699s, 1464.69/s)  LR: 1.061e-04  Data: 0.010 (0.018)
Train: 479 [ 300/1251 ( 24%)]  Loss: 3.184 (3.24)  Time: 0.710s, 1441.30/s  (0.697s, 1468.97/s)  LR: 1.061e-04  Data: 0.009 (0.017)
Train: 479 [ 350/1251 ( 28%)]  Loss: 3.623 (3.29)  Time: 0.670s, 1527.76/s  (0.696s, 1472.32/s)  LR: 1.061e-04  Data: 0.011 (0.016)
Train: 479 [ 400/1251 ( 32%)]  Loss: 3.103 (3.27)  Time: 0.670s, 1527.67/s  (0.694s, 1474.88/s)  LR: 1.061e-04  Data: 0.010 (0.015)
Train: 479 [ 450/1251 ( 36%)]  Loss: 3.315 (3.27)  Time: 0.684s, 1498.00/s  (0.694s, 1475.11/s)  LR: 1.061e-04  Data: 0.010 (0.015)
Train: 479 [ 500/1251 ( 40%)]  Loss: 3.364 (3.28)  Time: 0.673s, 1522.57/s  (0.694s, 1474.59/s)  LR: 1.061e-04  Data: 0.010 (0.014)
Train: 479 [ 550/1251 ( 44%)]  Loss: 3.196 (3.27)  Time: 0.700s, 1462.97/s  (0.694s, 1474.59/s)  LR: 1.061e-04  Data: 0.009 (0.014)
Train: 479 [ 600/1251 ( 48%)]  Loss: 3.559 (3.29)  Time: 0.698s, 1467.86/s  (0.694s, 1475.06/s)  LR: 1.061e-04  Data: 0.009 (0.014)
Train: 479 [ 650/1251 ( 52%)]  Loss: 3.146 (3.28)  Time: 0.673s, 1521.33/s  (0.694s, 1476.50/s)  LR: 1.061e-04  Data: 0.010 (0.013)
Train: 479 [ 700/1251 ( 56%)]  Loss: 3.310 (3.29)  Time: 0.672s, 1524.91/s  (0.693s, 1477.82/s)  LR: 1.061e-04  Data: 0.011 (0.013)
Train: 479 [ 750/1251 ( 60%)]  Loss: 2.819 (3.26)  Time: 0.744s, 1376.68/s  (0.693s, 1478.28/s)  LR: 1.061e-04  Data: 0.017 (0.013)
Train: 479 [ 800/1251 ( 64%)]  Loss: 3.176 (3.25)  Time: 0.671s, 1526.27/s  (0.693s, 1477.43/s)  LR: 1.061e-04  Data: 0.011 (0.013)
Train: 479 [ 850/1251 ( 68%)]  Loss: 3.477 (3.26)  Time: 0.710s, 1441.75/s  (0.693s, 1477.02/s)  LR: 1.061e-04  Data: 0.009 (0.013)
Train: 479 [ 900/1251 ( 72%)]  Loss: 3.417 (3.27)  Time: 0.672s, 1524.35/s  (0.693s, 1477.58/s)  LR: 1.061e-04  Data: 0.010 (0.013)
Train: 479 [ 950/1251 ( 76%)]  Loss: 3.370 (3.28)  Time: 0.696s, 1471.12/s  (0.693s, 1478.07/s)  LR: 1.061e-04  Data: 0.009 (0.012)
Train: 479 [1000/1251 ( 80%)]  Loss: 3.430 (3.28)  Time: 0.673s, 1520.44/s  (0.692s, 1479.31/s)  LR: 1.061e-04  Data: 0.009 (0.012)
Train: 479 [1050/1251 ( 84%)]  Loss: 3.176 (3.28)  Time: 0.671s, 1524.95/s  (0.692s, 1480.27/s)  LR: 1.061e-04  Data: 0.011 (0.012)
Train: 479 [1100/1251 ( 88%)]  Loss: 3.161 (3.27)  Time: 0.700s, 1463.87/s  (0.692s, 1480.17/s)  LR: 1.061e-04  Data: 0.009 (0.012)
Train: 479 [1150/1251 ( 92%)]  Loss: 3.493 (3.28)  Time: 0.700s, 1463.50/s  (0.692s, 1480.31/s)  LR: 1.061e-04  Data: 0.009 (0.012)
Train: 479 [1200/1251 ( 96%)]  Loss: 3.027 (3.27)  Time: 0.671s, 1526.70/s  (0.692s, 1479.84/s)  LR: 1.061e-04  Data: 0.010 (0.012)
Train: 479 [1250/1251 (100%)]  Loss: 3.098 (3.27)  Time: 0.662s, 1546.77/s  (0.692s, 1480.16/s)  LR: 1.061e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.652 (1.652)  Loss:  0.7119 (0.7119)  Acc@1: 90.2344 (90.2344)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.137 (0.588)  Loss:  0.8315 (1.2171)  Acc@1: 85.9670 (77.6080)  Acc@5: 97.0519 (93.8780)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-470.pth.tar', 77.73200003662109)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-479.pth.tar', 77.60799998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-477.pth.tar', 77.52200000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-472.pth.tar', 77.50600016357421)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-471.pth.tar', 77.49800003417968)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-478.pth.tar', 77.46600000732421)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-475.pth.tar', 77.45000011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-473.pth.tar', 77.44200006103516)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-474.pth.tar', 77.43400010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-465.pth.tar', 77.37799997802735)

Train: 480 [   0/1251 (  0%)]  Loss: 3.135 (3.13)  Time: 2.221s,  461.11/s  (2.221s,  461.11/s)  LR: 1.045e-04  Data: 1.604 (1.604)
Train: 480 [  50/1251 (  4%)]  Loss: 2.894 (3.01)  Time: 0.684s, 1497.92/s  (0.735s, 1392.61/s)  LR: 1.045e-04  Data: 0.010 (0.048)
Train: 480 [ 100/1251 (  8%)]  Loss: 3.299 (3.11)  Time: 0.671s, 1525.31/s  (0.711s, 1439.42/s)  LR: 1.045e-04  Data: 0.009 (0.029)
Train: 480 [ 150/1251 ( 12%)]  Loss: 3.578 (3.23)  Time: 0.712s, 1439.10/s  (0.706s, 1450.42/s)  LR: 1.045e-04  Data: 0.011 (0.023)
Train: 480 [ 200/1251 ( 16%)]  Loss: 3.537 (3.29)  Time: 0.672s, 1524.71/s  (0.702s, 1458.26/s)  LR: 1.045e-04  Data: 0.011 (0.020)
Train: 480 [ 250/1251 ( 20%)]  Loss: 3.175 (3.27)  Time: 0.705s, 1452.92/s  (0.699s, 1464.25/s)  LR: 1.045e-04  Data: 0.009 (0.018)
Train: 480 [ 300/1251 ( 24%)]  Loss: 3.360 (3.28)  Time: 0.673s, 1522.60/s  (0.698s, 1466.17/s)  LR: 1.045e-04  Data: 0.011 (0.017)
Train: 480 [ 350/1251 ( 28%)]  Loss: 3.438 (3.30)  Time: 0.674s, 1518.25/s  (0.697s, 1468.25/s)  LR: 1.045e-04  Data: 0.011 (0.016)
Train: 480 [ 400/1251 ( 32%)]  Loss: 3.554 (3.33)  Time: 0.745s, 1374.39/s  (0.698s, 1467.85/s)  LR: 1.045e-04  Data: 0.011 (0.015)
Train: 480 [ 450/1251 ( 36%)]  Loss: 3.105 (3.31)  Time: 0.672s, 1524.56/s  (0.697s, 1469.29/s)  LR: 1.045e-04  Data: 0.011 (0.015)
Train: 480 [ 500/1251 ( 40%)]  Loss: 3.183 (3.30)  Time: 0.675s, 1517.36/s  (0.697s, 1469.36/s)  LR: 1.045e-04  Data: 0.009 (0.014)
Train: 480 [ 550/1251 ( 44%)]  Loss: 3.386 (3.30)  Time: 0.689s, 1486.17/s  (0.697s, 1469.71/s)  LR: 1.045e-04  Data: 0.015 (0.014)
Train: 480 [ 600/1251 ( 48%)]  Loss: 3.385 (3.31)  Time: 0.706s, 1451.06/s  (0.699s, 1465.12/s)  LR: 1.045e-04  Data: 0.012 (0.014)
Train: 480 [ 650/1251 ( 52%)]  Loss: 3.217 (3.30)  Time: 0.739s, 1386.27/s  (0.704s, 1454.93/s)  LR: 1.045e-04  Data: 0.012 (0.014)
Train: 480 [ 700/1251 ( 56%)]  Loss: 3.217 (3.30)  Time: 0.736s, 1390.82/s  (0.706s, 1450.21/s)  LR: 1.045e-04  Data: 0.014 (0.014)
Train: 480 [ 750/1251 ( 60%)]  Loss: 3.134 (3.29)  Time: 0.730s, 1401.87/s  (0.709s, 1444.99/s)  LR: 1.045e-04  Data: 0.014 (0.015)
Train: 480 [ 800/1251 ( 64%)]  Loss: 3.342 (3.29)  Time: 1.013s, 1011.28/s  (0.711s, 1440.53/s)  LR: 1.045e-04  Data: 0.009 (0.015)
Train: 480 [ 850/1251 ( 68%)]  Loss: 3.661 (3.31)  Time: 0.741s, 1381.94/s  (0.713s, 1436.34/s)  LR: 1.045e-04  Data: 0.017 (0.015)
Train: 480 [ 900/1251 ( 72%)]  Loss: 3.172 (3.30)  Time: 0.734s, 1394.47/s  (0.715s, 1432.06/s)  LR: 1.045e-04  Data: 0.020 (0.015)
Train: 480 [ 950/1251 ( 76%)]  Loss: 3.230 (3.30)  Time: 0.684s, 1498.14/s  (0.718s, 1427.05/s)  LR: 1.045e-04  Data: 0.013 (0.014)
Train: 480 [1000/1251 ( 80%)]  Loss: 3.569 (3.31)  Time: 0.683s, 1499.63/s  (0.719s, 1424.51/s)  LR: 1.045e-04  Data: 0.011 (0.014)
Train: 480 [1050/1251 ( 84%)]  Loss: 3.058 (3.30)  Time: 0.678s, 1509.69/s  (0.718s, 1426.93/s)  LR: 1.045e-04  Data: 0.019 (0.014)
Train: 480 [1100/1251 ( 88%)]  Loss: 3.628 (3.32)  Time: 0.674s, 1519.18/s  (0.717s, 1429.10/s)  LR: 1.045e-04  Data: 0.010 (0.014)
Train: 480 [1150/1251 ( 92%)]  Loss: 3.077 (3.31)  Time: 0.677s, 1511.95/s  (0.715s, 1431.42/s)  LR: 1.045e-04  Data: 0.011 (0.014)
Train: 480 [1200/1251 ( 96%)]  Loss: 3.625 (3.32)  Time: 0.670s, 1527.99/s  (0.714s, 1433.79/s)  LR: 1.045e-04  Data: 0.010 (0.014)
Train: 480 [1250/1251 (100%)]  Loss: 3.129 (3.31)  Time: 0.689s, 1486.71/s  (0.713s, 1435.92/s)  LR: 1.045e-04  Data: 0.000 (0.014)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.670 (1.670)  Loss:  0.7676 (0.7676)  Acc@1: 91.3086 (91.3086)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.136 (0.580)  Loss:  0.8423 (1.2569)  Acc@1: 85.4953 (77.5500)  Acc@5: 97.0519 (93.7420)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-470.pth.tar', 77.73200003662109)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-479.pth.tar', 77.60799998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-480.pth.tar', 77.55000003417969)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-477.pth.tar', 77.52200000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-472.pth.tar', 77.50600016357421)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-471.pth.tar', 77.49800003417968)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-478.pth.tar', 77.46600000732421)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-475.pth.tar', 77.45000011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-473.pth.tar', 77.44200006103516)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-474.pth.tar', 77.43400010986328)

Train: 481 [   0/1251 (  0%)]  Loss: 3.353 (3.35)  Time: 2.660s,  384.90/s  (2.660s,  384.90/s)  LR: 1.030e-04  Data: 2.017 (2.017)
Train: 481 [  50/1251 (  4%)]  Loss: 3.222 (3.29)  Time: 0.692s, 1479.05/s  (0.741s, 1382.13/s)  LR: 1.030e-04  Data: 0.010 (0.058)
Train: 481 [ 100/1251 (  8%)]  Loss: 3.024 (3.20)  Time: 0.693s, 1476.75/s  (0.719s, 1424.05/s)  LR: 1.030e-04  Data: 0.012 (0.034)
Train: 481 [ 150/1251 ( 12%)]  Loss: 3.387 (3.25)  Time: 0.685s, 1495.48/s  (0.712s, 1437.82/s)  LR: 1.030e-04  Data: 0.009 (0.027)
Train: 481 [ 200/1251 ( 16%)]  Loss: 3.067 (3.21)  Time: 0.730s, 1403.46/s  (0.710s, 1442.79/s)  LR: 1.030e-04  Data: 0.010 (0.023)
Train: 481 [ 250/1251 ( 20%)]  Loss: 3.095 (3.19)  Time: 0.681s, 1503.54/s  (0.706s, 1450.18/s)  LR: 1.030e-04  Data: 0.009 (0.020)
Train: 481 [ 300/1251 ( 24%)]  Loss: 3.125 (3.18)  Time: 0.729s, 1405.14/s  (0.704s, 1454.08/s)  LR: 1.030e-04  Data: 0.010 (0.019)
Train: 481 [ 350/1251 ( 28%)]  Loss: 3.433 (3.21)  Time: 0.732s, 1398.10/s  (0.702s, 1457.69/s)  LR: 1.030e-04  Data: 0.009 (0.018)
Train: 481 [ 400/1251 ( 32%)]  Loss: 3.713 (3.27)  Time: 0.691s, 1482.79/s  (0.702s, 1458.77/s)  LR: 1.030e-04  Data: 0.009 (0.017)
Train: 481 [ 450/1251 ( 36%)]  Loss: 3.654 (3.31)  Time: 0.706s, 1451.24/s  (0.701s, 1460.42/s)  LR: 1.030e-04  Data: 0.011 (0.016)
Train: 481 [ 500/1251 ( 40%)]  Loss: 3.312 (3.31)  Time: 0.671s, 1525.60/s  (0.700s, 1462.64/s)  LR: 1.030e-04  Data: 0.010 (0.015)
Train: 481 [ 550/1251 ( 44%)]  Loss: 3.273 (3.30)  Time: 0.689s, 1487.24/s  (0.700s, 1463.74/s)  LR: 1.030e-04  Data: 0.009 (0.015)
Train: 481 [ 600/1251 ( 48%)]  Loss: 3.649 (3.33)  Time: 0.709s, 1443.46/s  (0.699s, 1464.00/s)  LR: 1.030e-04  Data: 0.010 (0.015)
Train: 481 [ 650/1251 ( 52%)]  Loss: 3.119 (3.32)  Time: 0.698s, 1467.80/s  (0.699s, 1464.65/s)  LR: 1.030e-04  Data: 0.009 (0.014)
Train: 481 [ 700/1251 ( 56%)]  Loss: 2.984 (3.29)  Time: 0.673s, 1521.32/s  (0.698s, 1467.07/s)  LR: 1.030e-04  Data: 0.010 (0.014)
Train: 481 [ 750/1251 ( 60%)]  Loss: 3.213 (3.29)  Time: 0.715s, 1432.15/s  (0.698s, 1467.75/s)  LR: 1.030e-04  Data: 0.010 (0.014)
Train: 481 [ 800/1251 ( 64%)]  Loss: 3.115 (3.28)  Time: 0.709s, 1444.83/s  (0.697s, 1468.28/s)  LR: 1.030e-04  Data: 0.011 (0.014)
Train: 481 [ 850/1251 ( 68%)]  Loss: 3.650 (3.30)  Time: 0.701s, 1461.11/s  (0.697s, 1469.03/s)  LR: 1.030e-04  Data: 0.008 (0.013)
Train: 481 [ 900/1251 ( 72%)]  Loss: 3.411 (3.31)  Time: 0.671s, 1527.15/s  (0.696s, 1470.38/s)  LR: 1.030e-04  Data: 0.009 (0.013)
Train: 481 [ 950/1251 ( 76%)]  Loss: 3.383 (3.31)  Time: 0.667s, 1536.11/s  (0.696s, 1471.10/s)  LR: 1.030e-04  Data: 0.009 (0.013)
Train: 481 [1000/1251 ( 80%)]  Loss: 3.421 (3.31)  Time: 0.671s, 1525.60/s  (0.696s, 1471.80/s)  LR: 1.030e-04  Data: 0.010 (0.013)
Train: 481 [1050/1251 ( 84%)]  Loss: 3.468 (3.32)  Time: 0.670s, 1528.08/s  (0.696s, 1471.54/s)  LR: 1.030e-04  Data: 0.010 (0.013)
Train: 481 [1100/1251 ( 88%)]  Loss: 3.096 (3.31)  Time: 0.687s, 1490.26/s  (0.696s, 1472.28/s)  LR: 1.030e-04  Data: 0.015 (0.013)
Train: 481 [1150/1251 ( 92%)]  Loss: 3.210 (3.31)  Time: 0.670s, 1529.50/s  (0.695s, 1472.98/s)  LR: 1.030e-04  Data: 0.010 (0.013)
Train: 481 [1200/1251 ( 96%)]  Loss: 3.435 (3.31)  Time: 0.671s, 1526.94/s  (0.695s, 1473.56/s)  LR: 1.030e-04  Data: 0.010 (0.013)
Train: 481 [1250/1251 (100%)]  Loss: 3.343 (3.31)  Time: 0.694s, 1475.82/s  (0.695s, 1474.29/s)  LR: 1.030e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.447 (1.447)  Loss:  0.7739 (0.7739)  Acc@1: 90.6250 (90.6250)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.136 (0.585)  Loss:  0.9287 (1.2690)  Acc@1: 84.9057 (77.6860)  Acc@5: 96.6981 (93.8720)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-470.pth.tar', 77.73200003662109)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-481.pth.tar', 77.6860000366211)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-479.pth.tar', 77.60799998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-480.pth.tar', 77.55000003417969)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-477.pth.tar', 77.52200000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-472.pth.tar', 77.50600016357421)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-471.pth.tar', 77.49800003417968)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-478.pth.tar', 77.46600000732421)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-475.pth.tar', 77.45000011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-473.pth.tar', 77.44200006103516)

Train: 482 [   0/1251 (  0%)]  Loss: 3.198 (3.20)  Time: 2.420s,  423.19/s  (2.420s,  423.19/s)  LR: 1.015e-04  Data: 1.789 (1.789)
Train: 482 [  50/1251 (  4%)]  Loss: 3.107 (3.15)  Time: 0.670s, 1527.71/s  (0.742s, 1379.52/s)  LR: 1.015e-04  Data: 0.010 (0.052)
Train: 482 [ 100/1251 (  8%)]  Loss: 3.197 (3.17)  Time: 0.700s, 1463.20/s  (0.715s, 1432.49/s)  LR: 1.015e-04  Data: 0.011 (0.031)
Train: 482 [ 150/1251 ( 12%)]  Loss: 3.830 (3.33)  Time: 0.673s, 1521.77/s  (0.708s, 1445.86/s)  LR: 1.015e-04  Data: 0.014 (0.025)
Train: 482 [ 200/1251 ( 16%)]  Loss: 3.480 (3.36)  Time: 0.708s, 1445.46/s  (0.704s, 1455.49/s)  LR: 1.015e-04  Data: 0.010 (0.021)
Train: 482 [ 250/1251 ( 20%)]  Loss: 3.375 (3.36)  Time: 0.672s, 1523.80/s  (0.700s, 1463.10/s)  LR: 1.015e-04  Data: 0.009 (0.019)
Train: 482 [ 300/1251 ( 24%)]  Loss: 3.541 (3.39)  Time: 0.677s, 1511.66/s  (0.698s, 1466.65/s)  LR: 1.015e-04  Data: 0.012 (0.017)
Train: 482 [ 350/1251 ( 28%)]  Loss: 3.500 (3.40)  Time: 0.673s, 1522.43/s  (0.698s, 1467.02/s)  LR: 1.015e-04  Data: 0.012 (0.016)
Train: 482 [ 400/1251 ( 32%)]  Loss: 3.464 (3.41)  Time: 0.673s, 1521.71/s  (0.697s, 1469.52/s)  LR: 1.015e-04  Data: 0.011 (0.016)
Train: 482 [ 450/1251 ( 36%)]  Loss: 3.347 (3.40)  Time: 0.672s, 1524.14/s  (0.696s, 1471.28/s)  LR: 1.015e-04  Data: 0.010 (0.015)
Train: 482 [ 500/1251 ( 40%)]  Loss: 3.210 (3.39)  Time: 0.740s, 1383.40/s  (0.695s, 1474.01/s)  LR: 1.015e-04  Data: 0.009 (0.015)
Train: 482 [ 550/1251 ( 44%)]  Loss: 3.373 (3.39)  Time: 0.722s, 1419.17/s  (0.694s, 1474.49/s)  LR: 1.015e-04  Data: 0.009 (0.014)
Train: 482 [ 600/1251 ( 48%)]  Loss: 3.154 (3.37)  Time: 0.697s, 1469.48/s  (0.694s, 1475.15/s)  LR: 1.015e-04  Data: 0.018 (0.014)
Train: 482 [ 650/1251 ( 52%)]  Loss: 3.157 (3.35)  Time: 0.675s, 1517.54/s  (0.694s, 1475.81/s)  LR: 1.015e-04  Data: 0.011 (0.014)
Train: 482 [ 700/1251 ( 56%)]  Loss: 3.569 (3.37)  Time: 0.673s, 1522.49/s  (0.694s, 1475.97/s)  LR: 1.015e-04  Data: 0.011 (0.013)
Train: 482 [ 750/1251 ( 60%)]  Loss: 3.423 (3.37)  Time: 0.678s, 1510.36/s  (0.693s, 1476.88/s)  LR: 1.015e-04  Data: 0.010 (0.013)
Train: 482 [ 800/1251 ( 64%)]  Loss: 3.416 (3.37)  Time: 0.694s, 1474.81/s  (0.693s, 1477.54/s)  LR: 1.015e-04  Data: 0.009 (0.013)
Train: 482 [ 850/1251 ( 68%)]  Loss: 3.587 (3.38)  Time: 0.673s, 1522.07/s  (0.693s, 1478.42/s)  LR: 1.015e-04  Data: 0.011 (0.013)
Train: 482 [ 900/1251 ( 72%)]  Loss: 3.553 (3.39)  Time: 0.706s, 1450.23/s  (0.693s, 1478.35/s)  LR: 1.015e-04  Data: 0.011 (0.013)
Train: 482 [ 950/1251 ( 76%)]  Loss: 3.725 (3.41)  Time: 0.672s, 1523.02/s  (0.693s, 1477.67/s)  LR: 1.015e-04  Data: 0.011 (0.013)
Train: 482 [1000/1251 ( 80%)]  Loss: 3.574 (3.42)  Time: 0.687s, 1490.67/s  (0.693s, 1477.48/s)  LR: 1.015e-04  Data: 0.011 (0.013)
Train: 482 [1050/1251 ( 84%)]  Loss: 3.509 (3.42)  Time: 0.703s, 1456.96/s  (0.693s, 1477.57/s)  LR: 1.015e-04  Data: 0.010 (0.012)
Train: 482 [1100/1251 ( 88%)]  Loss: 3.524 (3.43)  Time: 0.693s, 1478.07/s  (0.693s, 1478.34/s)  LR: 1.015e-04  Data: 0.016 (0.012)
Train: 482 [1150/1251 ( 92%)]  Loss: 3.084 (3.41)  Time: 0.678s, 1511.44/s  (0.693s, 1478.07/s)  LR: 1.015e-04  Data: 0.017 (0.012)
Train: 482 [1200/1251 ( 96%)]  Loss: 3.310 (3.41)  Time: 0.698s, 1466.99/s  (0.693s, 1478.26/s)  LR: 1.015e-04  Data: 0.014 (0.012)
Train: 482 [1250/1251 (100%)]  Loss: 3.595 (3.42)  Time: 0.693s, 1477.02/s  (0.693s, 1478.15/s)  LR: 1.015e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.626 (1.626)  Loss:  0.6846 (0.6846)  Acc@1: 91.1133 (91.1133)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.136 (0.582)  Loss:  0.8472 (1.2155)  Acc@1: 85.7311 (77.5400)  Acc@5: 97.2877 (93.7540)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-470.pth.tar', 77.73200003662109)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-481.pth.tar', 77.6860000366211)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-479.pth.tar', 77.60799998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-480.pth.tar', 77.55000003417969)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-482.pth.tar', 77.54000000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-477.pth.tar', 77.52200000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-472.pth.tar', 77.50600016357421)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-471.pth.tar', 77.49800003417968)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-478.pth.tar', 77.46600000732421)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-475.pth.tar', 77.45000011230469)

Train: 483 [   0/1251 (  0%)]  Loss: 3.227 (3.23)  Time: 2.182s,  469.36/s  (2.182s,  469.36/s)  LR: 1.000e-04  Data: 1.543 (1.543)
Train: 483 [  50/1251 (  4%)]  Loss: 3.065 (3.15)  Time: 0.692s, 1479.62/s  (0.732s, 1398.27/s)  LR: 1.000e-04  Data: 0.009 (0.048)
Train: 483 [ 100/1251 (  8%)]  Loss: 3.655 (3.32)  Time: 0.704s, 1453.93/s  (0.712s, 1438.08/s)  LR: 1.000e-04  Data: 0.009 (0.029)
Train: 483 [ 150/1251 ( 12%)]  Loss: 3.166 (3.28)  Time: 0.702s, 1458.55/s  (0.706s, 1450.76/s)  LR: 1.000e-04  Data: 0.009 (0.023)
Train: 483 [ 200/1251 ( 16%)]  Loss: 3.707 (3.36)  Time: 0.697s, 1469.83/s  (0.702s, 1459.36/s)  LR: 1.000e-04  Data: 0.011 (0.020)
Train: 483 [ 250/1251 ( 20%)]  Loss: 3.463 (3.38)  Time: 0.687s, 1489.69/s  (0.699s, 1464.12/s)  LR: 1.000e-04  Data: 0.008 (0.018)
Train: 483 [ 300/1251 ( 24%)]  Loss: 3.187 (3.35)  Time: 0.674s, 1519.02/s  (0.699s, 1464.39/s)  LR: 1.000e-04  Data: 0.010 (0.017)
Train: 483 [ 350/1251 ( 28%)]  Loss: 3.398 (3.36)  Time: 0.665s, 1539.75/s  (0.699s, 1465.31/s)  LR: 1.000e-04  Data: 0.010 (0.016)
Train: 483 [ 400/1251 ( 32%)]  Loss: 3.264 (3.35)  Time: 0.688s, 1488.95/s  (0.698s, 1467.02/s)  LR: 1.000e-04  Data: 0.017 (0.015)
Train: 483 [ 450/1251 ( 36%)]  Loss: 3.419 (3.36)  Time: 0.671s, 1524.98/s  (0.698s, 1467.91/s)  LR: 1.000e-04  Data: 0.009 (0.015)
Train: 483 [ 500/1251 ( 40%)]  Loss: 3.391 (3.36)  Time: 0.688s, 1487.38/s  (0.697s, 1469.53/s)  LR: 1.000e-04  Data: 0.010 (0.014)
Train: 483 [ 550/1251 ( 44%)]  Loss: 3.424 (3.36)  Time: 0.713s, 1436.19/s  (0.696s, 1470.59/s)  LR: 1.000e-04  Data: 0.009 (0.014)
Train: 483 [ 600/1251 ( 48%)]  Loss: 3.030 (3.34)  Time: 0.715s, 1431.51/s  (0.696s, 1472.32/s)  LR: 1.000e-04  Data: 0.010 (0.014)
Train: 483 [ 650/1251 ( 52%)]  Loss: 3.253 (3.33)  Time: 0.706s, 1450.70/s  (0.695s, 1473.59/s)  LR: 1.000e-04  Data: 0.010 (0.013)
Train: 483 [ 700/1251 ( 56%)]  Loss: 3.180 (3.32)  Time: 0.722s, 1418.20/s  (0.695s, 1473.37/s)  LR: 1.000e-04  Data: 0.011 (0.013)
Train: 483 [ 750/1251 ( 60%)]  Loss: 3.439 (3.33)  Time: 0.671s, 1526.71/s  (0.695s, 1473.11/s)  LR: 1.000e-04  Data: 0.010 (0.013)
Train: 483 [ 800/1251 ( 64%)]  Loss: 3.334 (3.33)  Time: 0.672s, 1524.72/s  (0.695s, 1473.18/s)  LR: 1.000e-04  Data: 0.011 (0.013)
Train: 483 [ 850/1251 ( 68%)]  Loss: 3.392 (3.33)  Time: 0.672s, 1523.57/s  (0.695s, 1473.88/s)  LR: 1.000e-04  Data: 0.011 (0.013)
Train: 483 [ 900/1251 ( 72%)]  Loss: 3.650 (3.35)  Time: 0.669s, 1531.43/s  (0.694s, 1474.57/s)  LR: 1.000e-04  Data: 0.011 (0.013)
Train: 483 [ 950/1251 ( 76%)]  Loss: 3.419 (3.35)  Time: 0.677s, 1512.81/s  (0.694s, 1475.52/s)  LR: 1.000e-04  Data: 0.012 (0.012)
Train: 483 [1000/1251 ( 80%)]  Loss: 3.175 (3.34)  Time: 0.672s, 1524.51/s  (0.694s, 1476.36/s)  LR: 1.000e-04  Data: 0.010 (0.012)
Train: 483 [1050/1251 ( 84%)]  Loss: 3.347 (3.34)  Time: 0.680s, 1505.58/s  (0.693s, 1476.75/s)  LR: 1.000e-04  Data: 0.010 (0.012)
Train: 483 [1100/1251 ( 88%)]  Loss: 3.156 (3.34)  Time: 0.671s, 1525.03/s  (0.693s, 1476.74/s)  LR: 1.000e-04  Data: 0.009 (0.012)
Train: 483 [1150/1251 ( 92%)]  Loss: 3.693 (3.35)  Time: 0.671s, 1525.37/s  (0.693s, 1477.36/s)  LR: 1.000e-04  Data: 0.010 (0.012)
Train: 483 [1200/1251 ( 96%)]  Loss: 3.297 (3.35)  Time: 0.695s, 1472.91/s  (0.693s, 1477.31/s)  LR: 1.000e-04  Data: 0.011 (0.012)
Train: 483 [1250/1251 (100%)]  Loss: 3.405 (3.35)  Time: 0.659s, 1553.11/s  (0.693s, 1477.80/s)  LR: 1.000e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.479 (1.479)  Loss:  0.6772 (0.6772)  Acc@1: 91.6016 (91.6016)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.136 (0.584)  Loss:  0.8125 (1.1780)  Acc@1: 85.7311 (77.7860)  Acc@5: 96.6981 (93.7560)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-483.pth.tar', 77.78600000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-470.pth.tar', 77.73200003662109)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-481.pth.tar', 77.6860000366211)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-479.pth.tar', 77.60799998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-480.pth.tar', 77.55000003417969)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-482.pth.tar', 77.54000000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-477.pth.tar', 77.52200000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-472.pth.tar', 77.50600016357421)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-471.pth.tar', 77.49800003417968)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-478.pth.tar', 77.46600000732421)

Train: 484 [   0/1251 (  0%)]  Loss: 3.380 (3.38)  Time: 2.153s,  475.61/s  (2.153s,  475.61/s)  LR: 9.853e-05  Data: 1.538 (1.538)
Train: 484 [  50/1251 (  4%)]  Loss: 3.016 (3.20)  Time: 0.705s, 1453.24/s  (0.730s, 1402.97/s)  LR: 9.853e-05  Data: 0.010 (0.044)
Train: 484 [ 100/1251 (  8%)]  Loss: 3.408 (3.27)  Time: 0.701s, 1460.20/s  (0.710s, 1442.75/s)  LR: 9.853e-05  Data: 0.009 (0.027)
Train: 484 [ 150/1251 ( 12%)]  Loss: 3.110 (3.23)  Time: 0.672s, 1523.33/s  (0.703s, 1456.19/s)  LR: 9.853e-05  Data: 0.011 (0.022)
Train: 484 [ 200/1251 ( 16%)]  Loss: 3.653 (3.31)  Time: 0.715s, 1432.51/s  (0.701s, 1459.92/s)  LR: 9.853e-05  Data: 0.013 (0.019)
Train: 484 [ 250/1251 ( 20%)]  Loss: 3.307 (3.31)  Time: 0.676s, 1514.40/s  (0.702s, 1459.28/s)  LR: 9.853e-05  Data: 0.011 (0.017)
Train: 484 [ 300/1251 ( 24%)]  Loss: 3.472 (3.34)  Time: 0.675s, 1515.99/s  (0.700s, 1463.37/s)  LR: 9.853e-05  Data: 0.009 (0.016)
Train: 484 [ 350/1251 ( 28%)]  Loss: 3.546 (3.36)  Time: 0.679s, 1507.79/s  (0.699s, 1465.62/s)  LR: 9.853e-05  Data: 0.010 (0.015)
Train: 484 [ 400/1251 ( 32%)]  Loss: 3.331 (3.36)  Time: 0.666s, 1537.67/s  (0.698s, 1467.21/s)  LR: 9.853e-05  Data: 0.010 (0.015)
Train: 484 [ 450/1251 ( 36%)]  Loss: 3.286 (3.35)  Time: 0.673s, 1521.69/s  (0.697s, 1469.58/s)  LR: 9.853e-05  Data: 0.010 (0.014)
Train: 484 [ 500/1251 ( 40%)]  Loss: 3.002 (3.32)  Time: 0.725s, 1412.56/s  (0.696s, 1471.85/s)  LR: 9.853e-05  Data: 0.009 (0.014)
Train: 484 [ 550/1251 ( 44%)]  Loss: 3.417 (3.33)  Time: 0.669s, 1531.44/s  (0.695s, 1473.07/s)  LR: 9.853e-05  Data: 0.009 (0.014)
Train: 484 [ 600/1251 ( 48%)]  Loss: 3.271 (3.32)  Time: 0.671s, 1527.19/s  (0.694s, 1474.73/s)  LR: 9.853e-05  Data: 0.010 (0.013)
Train: 484 [ 650/1251 ( 52%)]  Loss: 3.297 (3.32)  Time: 0.672s, 1523.62/s  (0.694s, 1474.92/s)  LR: 9.853e-05  Data: 0.010 (0.013)
Train: 484 [ 700/1251 ( 56%)]  Loss: 3.316 (3.32)  Time: 0.677s, 1513.55/s  (0.694s, 1474.64/s)  LR: 9.853e-05  Data: 0.011 (0.013)
Train: 484 [ 750/1251 ( 60%)]  Loss: 3.316 (3.32)  Time: 0.703s, 1456.77/s  (0.694s, 1474.86/s)  LR: 9.853e-05  Data: 0.010 (0.013)
Train: 484 [ 800/1251 ( 64%)]  Loss: 3.465 (3.33)  Time: 0.759s, 1349.86/s  (0.694s, 1474.99/s)  LR: 9.853e-05  Data: 0.010 (0.013)
Train: 484 [ 850/1251 ( 68%)]  Loss: 3.220 (3.32)  Time: 0.671s, 1525.10/s  (0.695s, 1473.99/s)  LR: 9.853e-05  Data: 0.009 (0.013)
Train: 484 [ 900/1251 ( 72%)]  Loss: 3.366 (3.33)  Time: 0.713s, 1436.96/s  (0.695s, 1473.23/s)  LR: 9.853e-05  Data: 0.009 (0.012)
Train: 484 [ 950/1251 ( 76%)]  Loss: 3.578 (3.34)  Time: 0.668s, 1533.98/s  (0.695s, 1473.49/s)  LR: 9.853e-05  Data: 0.011 (0.012)
Train: 484 [1000/1251 ( 80%)]  Loss: 3.248 (3.33)  Time: 0.673s, 1521.29/s  (0.695s, 1474.20/s)  LR: 9.853e-05  Data: 0.011 (0.012)
Train: 484 [1050/1251 ( 84%)]  Loss: 3.838 (3.36)  Time: 0.683s, 1499.93/s  (0.695s, 1474.06/s)  LR: 9.853e-05  Data: 0.012 (0.012)
Train: 484 [1100/1251 ( 88%)]  Loss: 3.330 (3.36)  Time: 0.672s, 1523.66/s  (0.694s, 1474.54/s)  LR: 9.853e-05  Data: 0.011 (0.012)
Train: 484 [1150/1251 ( 92%)]  Loss: 3.537 (3.36)  Time: 0.685s, 1495.52/s  (0.695s, 1473.75/s)  LR: 9.853e-05  Data: 0.014 (0.012)
Train: 484 [1200/1251 ( 96%)]  Loss: 3.380 (3.36)  Time: 0.680s, 1506.62/s  (0.695s, 1474.32/s)  LR: 9.853e-05  Data: 0.011 (0.012)
Train: 484 [1250/1251 (100%)]  Loss: 2.969 (3.35)  Time: 0.685s, 1494.19/s  (0.694s, 1474.63/s)  LR: 9.853e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.583 (1.583)  Loss:  0.8271 (0.8271)  Acc@1: 91.2109 (91.2109)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.135 (0.576)  Loss:  0.8652 (1.2952)  Acc@1: 86.5566 (77.7420)  Acc@5: 97.1698 (93.8700)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-483.pth.tar', 77.78600000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-484.pth.tar', 77.74200010742187)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-470.pth.tar', 77.73200003662109)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-481.pth.tar', 77.6860000366211)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-479.pth.tar', 77.60799998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-480.pth.tar', 77.55000003417969)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-482.pth.tar', 77.54000000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-477.pth.tar', 77.52200000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-472.pth.tar', 77.50600016357421)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-471.pth.tar', 77.49800003417968)

Train: 485 [   0/1251 (  0%)]  Loss: 3.470 (3.47)  Time: 2.201s,  465.20/s  (2.201s,  465.20/s)  LR: 9.706e-05  Data: 1.532 (1.532)
Train: 485 [  50/1251 (  4%)]  Loss: 3.487 (3.48)  Time: 0.698s, 1467.51/s  (0.730s, 1402.75/s)  LR: 9.706e-05  Data: 0.010 (0.047)
Train: 485 [ 100/1251 (  8%)]  Loss: 3.236 (3.40)  Time: 0.679s, 1508.94/s  (0.710s, 1441.95/s)  LR: 9.706e-05  Data: 0.010 (0.029)
Train: 485 [ 150/1251 ( 12%)]  Loss: 3.565 (3.44)  Time: 0.672s, 1524.17/s  (0.705s, 1451.90/s)  LR: 9.706e-05  Data: 0.010 (0.023)
Train: 485 [ 200/1251 ( 16%)]  Loss: 3.295 (3.41)  Time: 0.713s, 1435.73/s  (0.705s, 1451.46/s)  LR: 9.706e-05  Data: 0.011 (0.020)
Train: 485 [ 250/1251 ( 20%)]  Loss: 3.216 (3.38)  Time: 0.670s, 1527.76/s  (0.703s, 1456.89/s)  LR: 9.706e-05  Data: 0.010 (0.018)
Train: 485 [ 300/1251 ( 24%)]  Loss: 3.333 (3.37)  Time: 0.689s, 1487.16/s  (0.702s, 1459.41/s)  LR: 9.706e-05  Data: 0.009 (0.017)
Train: 485 [ 350/1251 ( 28%)]  Loss: 3.330 (3.37)  Time: 0.666s, 1536.76/s  (0.700s, 1463.88/s)  LR: 9.706e-05  Data: 0.010 (0.016)
Train: 485 [ 400/1251 ( 32%)]  Loss: 2.978 (3.32)  Time: 0.678s, 1510.23/s  (0.698s, 1466.85/s)  LR: 9.706e-05  Data: 0.009 (0.015)
Train: 485 [ 450/1251 ( 36%)]  Loss: 3.046 (3.30)  Time: 0.684s, 1496.78/s  (0.697s, 1469.32/s)  LR: 9.706e-05  Data: 0.010 (0.015)
Train: 485 [ 500/1251 ( 40%)]  Loss: 3.287 (3.29)  Time: 0.672s, 1524.36/s  (0.696s, 1470.55/s)  LR: 9.706e-05  Data: 0.010 (0.014)
Train: 485 [ 550/1251 ( 44%)]  Loss: 3.576 (3.32)  Time: 0.673s, 1521.26/s  (0.697s, 1469.43/s)  LR: 9.706e-05  Data: 0.010 (0.014)
Train: 485 [ 600/1251 ( 48%)]  Loss: 3.231 (3.31)  Time: 0.753s, 1359.22/s  (0.696s, 1470.59/s)  LR: 9.706e-05  Data: 0.009 (0.014)
Train: 485 [ 650/1251 ( 52%)]  Loss: 3.726 (3.34)  Time: 0.671s, 1526.67/s  (0.695s, 1472.74/s)  LR: 9.706e-05  Data: 0.010 (0.013)
Train: 485 [ 700/1251 ( 56%)]  Loss: 3.139 (3.33)  Time: 0.712s, 1437.88/s  (0.695s, 1473.70/s)  LR: 9.706e-05  Data: 0.011 (0.013)
Train: 485 [ 750/1251 ( 60%)]  Loss: 3.307 (3.33)  Time: 0.668s, 1531.84/s  (0.695s, 1473.60/s)  LR: 9.706e-05  Data: 0.011 (0.013)
Train: 485 [ 800/1251 ( 64%)]  Loss: 3.517 (3.34)  Time: 0.671s, 1527.19/s  (0.695s, 1474.32/s)  LR: 9.706e-05  Data: 0.011 (0.013)
Train: 485 [ 850/1251 ( 68%)]  Loss: 3.549 (3.35)  Time: 0.703s, 1457.14/s  (0.695s, 1473.54/s)  LR: 9.706e-05  Data: 0.009 (0.013)
Train: 485 [ 900/1251 ( 72%)]  Loss: 3.365 (3.35)  Time: 0.740s, 1383.33/s  (0.695s, 1474.08/s)  LR: 9.706e-05  Data: 0.010 (0.013)
Train: 485 [ 950/1251 ( 76%)]  Loss: 3.253 (3.35)  Time: 0.673s, 1522.11/s  (0.695s, 1473.80/s)  LR: 9.706e-05  Data: 0.010 (0.012)
Train: 485 [1000/1251 ( 80%)]  Loss: 3.809 (3.37)  Time: 0.707s, 1449.32/s  (0.695s, 1474.00/s)  LR: 9.706e-05  Data: 0.009 (0.012)
Train: 485 [1050/1251 ( 84%)]  Loss: 3.335 (3.37)  Time: 0.672s, 1524.13/s  (0.694s, 1474.93/s)  LR: 9.706e-05  Data: 0.010 (0.012)
Train: 485 [1100/1251 ( 88%)]  Loss: 3.257 (3.36)  Time: 0.674s, 1518.47/s  (0.694s, 1475.25/s)  LR: 9.706e-05  Data: 0.010 (0.012)
Train: 485 [1150/1251 ( 92%)]  Loss: 3.431 (3.36)  Time: 0.732s, 1398.33/s  (0.694s, 1476.08/s)  LR: 9.706e-05  Data: 0.009 (0.012)
Train: 485 [1200/1251 ( 96%)]  Loss: 3.587 (3.37)  Time: 0.693s, 1478.59/s  (0.694s, 1476.56/s)  LR: 9.706e-05  Data: 0.009 (0.012)
Train: 485 [1250/1251 (100%)]  Loss: 3.388 (3.37)  Time: 0.690s, 1484.83/s  (0.693s, 1477.19/s)  LR: 9.706e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.661 (1.661)  Loss:  0.7515 (0.7515)  Acc@1: 91.1133 (91.1133)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.136 (0.580)  Loss:  0.8242 (1.2288)  Acc@1: 86.5566 (77.7160)  Acc@5: 97.0519 (93.9460)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-483.pth.tar', 77.78600000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-484.pth.tar', 77.74200010742187)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-470.pth.tar', 77.73200003662109)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-485.pth.tar', 77.71599997802734)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-481.pth.tar', 77.6860000366211)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-479.pth.tar', 77.60799998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-480.pth.tar', 77.55000003417969)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-482.pth.tar', 77.54000000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-477.pth.tar', 77.52200000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-472.pth.tar', 77.50600016357421)

Train: 486 [   0/1251 (  0%)]  Loss: 3.245 (3.25)  Time: 2.320s,  441.46/s  (2.320s,  441.46/s)  LR: 9.560e-05  Data: 1.704 (1.704)
Train: 486 [  50/1251 (  4%)]  Loss: 3.249 (3.25)  Time: 0.682s, 1501.05/s  (0.741s, 1381.56/s)  LR: 9.560e-05  Data: 0.012 (0.050)
Train: 486 [ 100/1251 (  8%)]  Loss: 3.346 (3.28)  Time: 0.673s, 1522.64/s  (0.718s, 1426.55/s)  LR: 9.560e-05  Data: 0.011 (0.031)
Train: 486 [ 150/1251 ( 12%)]  Loss: 2.937 (3.19)  Time: 0.670s, 1527.22/s  (0.708s, 1446.56/s)  LR: 9.560e-05  Data: 0.011 (0.024)
Train: 486 [ 200/1251 ( 16%)]  Loss: 3.326 (3.22)  Time: 0.721s, 1420.85/s  (0.704s, 1454.72/s)  LR: 9.560e-05  Data: 0.009 (0.020)
Train: 486 [ 250/1251 ( 20%)]  Loss: 3.261 (3.23)  Time: 0.666s, 1536.75/s  (0.702s, 1457.73/s)  LR: 9.560e-05  Data: 0.009 (0.018)
Train: 486 [ 300/1251 ( 24%)]  Loss: 3.125 (3.21)  Time: 0.690s, 1485.12/s  (0.702s, 1458.75/s)  LR: 9.560e-05  Data: 0.013 (0.017)
Train: 486 [ 350/1251 ( 28%)]  Loss: 3.115 (3.20)  Time: 0.792s, 1292.31/s  (0.702s, 1459.37/s)  LR: 9.560e-05  Data: 0.010 (0.016)
Train: 486 [ 400/1251 ( 32%)]  Loss: 3.215 (3.20)  Time: 0.673s, 1522.56/s  (0.702s, 1459.43/s)  LR: 9.560e-05  Data: 0.009 (0.016)
Train: 486 [ 450/1251 ( 36%)]  Loss: 3.062 (3.19)  Time: 0.675s, 1516.12/s  (0.701s, 1461.44/s)  LR: 9.560e-05  Data: 0.011 (0.015)
Train: 486 [ 500/1251 ( 40%)]  Loss: 3.330 (3.20)  Time: 0.699s, 1465.72/s  (0.701s, 1461.41/s)  LR: 9.560e-05  Data: 0.010 (0.015)
Train: 486 [ 550/1251 ( 44%)]  Loss: 3.034 (3.19)  Time: 0.693s, 1477.70/s  (0.699s, 1464.25/s)  LR: 9.560e-05  Data: 0.011 (0.014)
Train: 486 [ 600/1251 ( 48%)]  Loss: 3.103 (3.18)  Time: 0.669s, 1529.97/s  (0.699s, 1465.97/s)  LR: 9.560e-05  Data: 0.008 (0.014)
Train: 486 [ 650/1251 ( 52%)]  Loss: 3.236 (3.18)  Time: 0.706s, 1450.14/s  (0.698s, 1467.69/s)  LR: 9.560e-05  Data: 0.011 (0.014)
Train: 486 [ 700/1251 ( 56%)]  Loss: 3.330 (3.19)  Time: 0.709s, 1444.56/s  (0.698s, 1467.85/s)  LR: 9.560e-05  Data: 0.009 (0.013)
Train: 486 [ 750/1251 ( 60%)]  Loss: 3.390 (3.21)  Time: 0.674s, 1519.07/s  (0.698s, 1467.56/s)  LR: 9.560e-05  Data: 0.011 (0.013)
Train: 486 [ 800/1251 ( 64%)]  Loss: 3.729 (3.24)  Time: 0.674s, 1518.73/s  (0.698s, 1468.01/s)  LR: 9.560e-05  Data: 0.014 (0.013)
Train: 486 [ 850/1251 ( 68%)]  Loss: 3.286 (3.24)  Time: 0.684s, 1497.45/s  (0.698s, 1467.99/s)  LR: 9.560e-05  Data: 0.011 (0.013)
Train: 486 [ 900/1251 ( 72%)]  Loss: 3.220 (3.24)  Time: 0.705s, 1452.00/s  (0.697s, 1468.85/s)  LR: 9.560e-05  Data: 0.011 (0.013)
Train: 486 [ 950/1251 ( 76%)]  Loss: 3.179 (3.24)  Time: 0.673s, 1521.82/s  (0.697s, 1469.66/s)  LR: 9.560e-05  Data: 0.011 (0.013)
Train: 486 [1000/1251 ( 80%)]  Loss: 3.504 (3.25)  Time: 0.674s, 1520.05/s  (0.696s, 1470.47/s)  LR: 9.560e-05  Data: 0.011 (0.013)
Train: 486 [1050/1251 ( 84%)]  Loss: 3.338 (3.25)  Time: 0.728s, 1406.97/s  (0.697s, 1470.13/s)  LR: 9.560e-05  Data: 0.011 (0.012)
Train: 486 [1100/1251 ( 88%)]  Loss: 2.913 (3.24)  Time: 0.684s, 1497.86/s  (0.697s, 1470.10/s)  LR: 9.560e-05  Data: 0.011 (0.012)
Train: 486 [1150/1251 ( 92%)]  Loss: 3.186 (3.24)  Time: 0.675s, 1516.22/s  (0.696s, 1470.39/s)  LR: 9.560e-05  Data: 0.009 (0.012)
Train: 486 [1200/1251 ( 96%)]  Loss: 3.264 (3.24)  Time: 0.686s, 1492.84/s  (0.696s, 1470.97/s)  LR: 9.560e-05  Data: 0.011 (0.012)
Train: 486 [1250/1251 (100%)]  Loss: 3.621 (3.25)  Time: 0.725s, 1411.88/s  (0.696s, 1470.99/s)  LR: 9.560e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.471 (1.471)  Loss:  0.7295 (0.7295)  Acc@1: 90.8203 (90.8203)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.136 (0.574)  Loss:  0.8335 (1.2495)  Acc@1: 86.2028 (77.8960)  Acc@5: 97.0519 (93.8280)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-486.pth.tar', 77.89599995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-483.pth.tar', 77.78600000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-484.pth.tar', 77.74200010742187)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-470.pth.tar', 77.73200003662109)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-485.pth.tar', 77.71599997802734)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-481.pth.tar', 77.6860000366211)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-479.pth.tar', 77.60799998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-480.pth.tar', 77.55000003417969)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-482.pth.tar', 77.54000000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-477.pth.tar', 77.52200000732422)

Train: 487 [   0/1251 (  0%)]  Loss: 3.732 (3.73)  Time: 2.226s,  459.99/s  (2.226s,  459.99/s)  LR: 9.414e-05  Data: 1.560 (1.560)
Train: 487 [  50/1251 (  4%)]  Loss: 3.314 (3.52)  Time: 0.705s, 1453.13/s  (0.735s, 1392.36/s)  LR: 9.414e-05  Data: 0.010 (0.051)
Train: 487 [ 100/1251 (  8%)]  Loss: 3.333 (3.46)  Time: 0.706s, 1449.96/s  (0.720s, 1423.20/s)  LR: 9.414e-05  Data: 0.010 (0.031)
Train: 487 [ 150/1251 ( 12%)]  Loss: 3.325 (3.43)  Time: 0.672s, 1524.71/s  (0.709s, 1443.48/s)  LR: 9.414e-05  Data: 0.010 (0.024)
Train: 487 [ 200/1251 ( 16%)]  Loss: 3.250 (3.39)  Time: 0.695s, 1474.23/s  (0.706s, 1449.42/s)  LR: 9.414e-05  Data: 0.010 (0.021)
Train: 487 [ 250/1251 ( 20%)]  Loss: 3.150 (3.35)  Time: 0.705s, 1452.83/s  (0.704s, 1455.10/s)  LR: 9.414e-05  Data: 0.010 (0.019)
Train: 487 [ 300/1251 ( 24%)]  Loss: 3.500 (3.37)  Time: 0.731s, 1400.44/s  (0.701s, 1459.74/s)  LR: 9.414e-05  Data: 0.010 (0.017)
Train: 487 [ 350/1251 ( 28%)]  Loss: 3.611 (3.40)  Time: 0.667s, 1534.66/s  (0.700s, 1463.07/s)  LR: 9.414e-05  Data: 0.009 (0.016)
Train: 487 [ 400/1251 ( 32%)]  Loss: 3.311 (3.39)  Time: 0.710s, 1442.77/s  (0.699s, 1465.14/s)  LR: 9.414e-05  Data: 0.010 (0.016)
Train: 487 [ 450/1251 ( 36%)]  Loss: 3.448 (3.40)  Time: 0.714s, 1434.96/s  (0.699s, 1465.54/s)  LR: 9.414e-05  Data: 0.010 (0.015)
Train: 487 [ 500/1251 ( 40%)]  Loss: 3.293 (3.39)  Time: 0.695s, 1473.02/s  (0.698s, 1466.07/s)  LR: 9.414e-05  Data: 0.012 (0.015)
Train: 487 [ 550/1251 ( 44%)]  Loss: 3.247 (3.38)  Time: 0.672s, 1524.93/s  (0.698s, 1467.70/s)  LR: 9.414e-05  Data: 0.011 (0.014)
Train: 487 [ 600/1251 ( 48%)]  Loss: 3.137 (3.36)  Time: 0.668s, 1533.74/s  (0.697s, 1468.82/s)  LR: 9.414e-05  Data: 0.010 (0.014)
Train: 487 [ 650/1251 ( 52%)]  Loss: 3.121 (3.34)  Time: 0.691s, 1482.67/s  (0.697s, 1469.88/s)  LR: 9.414e-05  Data: 0.015 (0.014)
Train: 487 [ 700/1251 ( 56%)]  Loss: 3.316 (3.34)  Time: 0.702s, 1458.89/s  (0.696s, 1470.28/s)  LR: 9.414e-05  Data: 0.009 (0.013)
Train: 487 [ 750/1251 ( 60%)]  Loss: 3.279 (3.34)  Time: 0.681s, 1503.14/s  (0.696s, 1471.18/s)  LR: 9.414e-05  Data: 0.009 (0.013)
Train: 487 [ 800/1251 ( 64%)]  Loss: 3.391 (3.34)  Time: 0.670s, 1528.94/s  (0.696s, 1471.38/s)  LR: 9.414e-05  Data: 0.010 (0.013)
Train: 487 [ 850/1251 ( 68%)]  Loss: 3.644 (3.36)  Time: 0.720s, 1422.25/s  (0.696s, 1471.50/s)  LR: 9.414e-05  Data: 0.010 (0.013)
Train: 487 [ 900/1251 ( 72%)]  Loss: 3.101 (3.34)  Time: 0.674s, 1519.77/s  (0.696s, 1471.59/s)  LR: 9.414e-05  Data: 0.013 (0.013)
Train: 487 [ 950/1251 ( 76%)]  Loss: 3.105 (3.33)  Time: 0.722s, 1418.34/s  (0.696s, 1471.89/s)  LR: 9.414e-05  Data: 0.009 (0.013)
Train: 487 [1000/1251 ( 80%)]  Loss: 3.609 (3.34)  Time: 0.716s, 1430.53/s  (0.695s, 1472.36/s)  LR: 9.414e-05  Data: 0.019 (0.012)
Train: 487 [1050/1251 ( 84%)]  Loss: 3.501 (3.35)  Time: 0.724s, 1414.35/s  (0.696s, 1471.93/s)  LR: 9.414e-05  Data: 0.010 (0.012)
Train: 487 [1100/1251 ( 88%)]  Loss: 3.442 (3.35)  Time: 0.684s, 1496.51/s  (0.696s, 1471.48/s)  LR: 9.414e-05  Data: 0.011 (0.012)
Train: 487 [1150/1251 ( 92%)]  Loss: 3.577 (3.36)  Time: 0.673s, 1521.88/s  (0.696s, 1471.41/s)  LR: 9.414e-05  Data: 0.009 (0.012)
Train: 487 [1200/1251 ( 96%)]  Loss: 3.436 (3.37)  Time: 0.685s, 1495.96/s  (0.696s, 1472.24/s)  LR: 9.414e-05  Data: 0.011 (0.012)
Train: 487 [1250/1251 (100%)]  Loss: 3.542 (3.37)  Time: 0.672s, 1524.32/s  (0.695s, 1472.45/s)  LR: 9.414e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.538 (1.538)  Loss:  0.7427 (0.7427)  Acc@1: 91.0156 (91.0156)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.136 (0.581)  Loss:  0.8721 (1.2541)  Acc@1: 86.7925 (77.8500)  Acc@5: 97.1698 (93.8320)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-486.pth.tar', 77.89599995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-487.pth.tar', 77.8500000805664)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-483.pth.tar', 77.78600000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-484.pth.tar', 77.74200010742187)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-470.pth.tar', 77.73200003662109)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-485.pth.tar', 77.71599997802734)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-481.pth.tar', 77.6860000366211)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-479.pth.tar', 77.60799998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-480.pth.tar', 77.55000003417969)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-482.pth.tar', 77.54000000732422)

Train: 488 [   0/1251 (  0%)]  Loss: 3.612 (3.61)  Time: 2.212s,  462.87/s  (2.212s,  462.87/s)  LR: 9.270e-05  Data: 1.596 (1.596)
Train: 488 [  50/1251 (  4%)]  Loss: 2.980 (3.30)  Time: 0.673s, 1521.71/s  (0.731s, 1401.27/s)  LR: 9.270e-05  Data: 0.010 (0.051)
Train: 488 [ 100/1251 (  8%)]  Loss: 3.026 (3.21)  Time: 0.709s, 1444.42/s  (0.713s, 1437.17/s)  LR: 9.270e-05  Data: 0.010 (0.031)
Train: 488 [ 150/1251 ( 12%)]  Loss: 3.641 (3.31)  Time: 0.667s, 1536.23/s  (0.703s, 1455.65/s)  LR: 9.270e-05  Data: 0.010 (0.024)
Train: 488 [ 200/1251 ( 16%)]  Loss: 2.935 (3.24)  Time: 0.671s, 1525.67/s  (0.701s, 1460.67/s)  LR: 9.270e-05  Data: 0.012 (0.021)
Train: 488 [ 250/1251 ( 20%)]  Loss: 3.037 (3.21)  Time: 0.739s, 1386.36/s  (0.700s, 1463.69/s)  LR: 9.270e-05  Data: 0.018 (0.019)
Train: 488 [ 300/1251 ( 24%)]  Loss: 3.122 (3.19)  Time: 0.670s, 1527.59/s  (0.699s, 1464.07/s)  LR: 9.270e-05  Data: 0.013 (0.017)
Train: 488 [ 350/1251 ( 28%)]  Loss: 3.305 (3.21)  Time: 0.672s, 1523.21/s  (0.699s, 1464.71/s)  LR: 9.270e-05  Data: 0.010 (0.016)
Train: 488 [ 400/1251 ( 32%)]  Loss: 3.500 (3.24)  Time: 0.707s, 1448.19/s  (0.699s, 1465.32/s)  LR: 9.270e-05  Data: 0.010 (0.016)
Train: 488 [ 450/1251 ( 36%)]  Loss: 3.454 (3.26)  Time: 0.712s, 1437.81/s  (0.699s, 1464.91/s)  LR: 9.270e-05  Data: 0.011 (0.015)
Train: 488 [ 500/1251 ( 40%)]  Loss: 3.487 (3.28)  Time: 0.691s, 1482.44/s  (0.699s, 1465.81/s)  LR: 9.270e-05  Data: 0.012 (0.015)
Train: 488 [ 550/1251 ( 44%)]  Loss: 3.036 (3.26)  Time: 0.738s, 1387.45/s  (0.698s, 1467.02/s)  LR: 9.270e-05  Data: 0.013 (0.014)
Train: 488 [ 600/1251 ( 48%)]  Loss: 3.373 (3.27)  Time: 0.735s, 1392.78/s  (0.699s, 1464.49/s)  LR: 9.270e-05  Data: 0.017 (0.014)
Train: 488 [ 650/1251 ( 52%)]  Loss: 2.949 (3.25)  Time: 0.686s, 1492.95/s  (0.700s, 1462.47/s)  LR: 9.270e-05  Data: 0.012 (0.014)
Train: 488 [ 700/1251 ( 56%)]  Loss: 3.660 (3.27)  Time: 0.736s, 1390.55/s  (0.701s, 1460.92/s)  LR: 9.270e-05  Data: 0.011 (0.014)
Train: 488 [ 750/1251 ( 60%)]  Loss: 3.380 (3.28)  Time: 0.671s, 1526.71/s  (0.700s, 1461.95/s)  LR: 9.270e-05  Data: 0.010 (0.014)
Train: 488 [ 800/1251 ( 64%)]  Loss: 3.420 (3.29)  Time: 0.671s, 1526.77/s  (0.699s, 1465.26/s)  LR: 9.270e-05  Data: 0.012 (0.013)
Train: 488 [ 850/1251 ( 68%)]  Loss: 3.214 (3.29)  Time: 0.673s, 1522.50/s  (0.697s, 1468.55/s)  LR: 9.270e-05  Data: 0.011 (0.013)
Train: 488 [ 900/1251 ( 72%)]  Loss: 3.491 (3.30)  Time: 0.685s, 1495.74/s  (0.697s, 1469.03/s)  LR: 9.270e-05  Data: 0.011 (0.013)
Train: 488 [ 950/1251 ( 76%)]  Loss: 3.473 (3.30)  Time: 0.702s, 1458.74/s  (0.697s, 1468.49/s)  LR: 9.270e-05  Data: 0.010 (0.013)
Train: 488 [1000/1251 ( 80%)]  Loss: 3.415 (3.31)  Time: 0.672s, 1523.28/s  (0.697s, 1468.84/s)  LR: 9.270e-05  Data: 0.012 (0.013)
Train: 488 [1050/1251 ( 84%)]  Loss: 2.920 (3.29)  Time: 0.676s, 1515.27/s  (0.697s, 1469.38/s)  LR: 9.270e-05  Data: 0.014 (0.013)
Train: 488 [1100/1251 ( 88%)]  Loss: 2.958 (3.28)  Time: 0.714s, 1433.90/s  (0.697s, 1469.37/s)  LR: 9.270e-05  Data: 0.013 (0.013)
Train: 488 [1150/1251 ( 92%)]  Loss: 3.360 (3.28)  Time: 0.704s, 1453.53/s  (0.696s, 1470.29/s)  LR: 9.270e-05  Data: 0.010 (0.013)
Train: 488 [1200/1251 ( 96%)]  Loss: 3.462 (3.29)  Time: 0.679s, 1508.54/s  (0.696s, 1471.22/s)  LR: 9.270e-05  Data: 0.011 (0.013)
Train: 488 [1250/1251 (100%)]  Loss: 3.194 (3.28)  Time: 0.657s, 1557.62/s  (0.696s, 1471.55/s)  LR: 9.270e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.528 (1.528)  Loss:  0.7471 (0.7471)  Acc@1: 90.8203 (90.8203)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.136 (0.576)  Loss:  0.8701 (1.2961)  Acc@1: 86.2028 (77.7920)  Acc@5: 96.9340 (93.8100)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-486.pth.tar', 77.89599995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-487.pth.tar', 77.8500000805664)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-488.pth.tar', 77.7920000830078)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-483.pth.tar', 77.78600000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-484.pth.tar', 77.74200010742187)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-470.pth.tar', 77.73200003662109)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-485.pth.tar', 77.71599997802734)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-481.pth.tar', 77.6860000366211)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-479.pth.tar', 77.60799998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-480.pth.tar', 77.55000003417969)

Train: 489 [   0/1251 (  0%)]  Loss: 3.548 (3.55)  Time: 2.144s,  477.70/s  (2.144s,  477.70/s)  LR: 9.128e-05  Data: 1.529 (1.529)
Train: 489 [  50/1251 (  4%)]  Loss: 3.263 (3.41)  Time: 0.686s, 1491.78/s  (0.731s, 1401.24/s)  LR: 9.128e-05  Data: 0.013 (0.047)
Train: 489 [ 100/1251 (  8%)]  Loss: 3.288 (3.37)  Time: 0.670s, 1527.41/s  (0.715s, 1431.31/s)  LR: 9.128e-05  Data: 0.009 (0.029)
Train: 489 [ 150/1251 ( 12%)]  Loss: 3.296 (3.35)  Time: 0.671s, 1525.88/s  (0.708s, 1447.09/s)  LR: 9.128e-05  Data: 0.011 (0.023)
Train: 489 [ 200/1251 ( 16%)]  Loss: 3.180 (3.31)  Time: 0.718s, 1425.37/s  (0.703s, 1456.85/s)  LR: 9.128e-05  Data: 0.010 (0.020)
Train: 489 [ 250/1251 ( 20%)]  Loss: 3.529 (3.35)  Time: 0.702s, 1459.67/s  (0.703s, 1456.61/s)  LR: 9.128e-05  Data: 0.009 (0.018)
Train: 489 [ 300/1251 ( 24%)]  Loss: 3.202 (3.33)  Time: 0.701s, 1460.97/s  (0.701s, 1461.39/s)  LR: 9.128e-05  Data: 0.011 (0.017)
Train: 489 [ 350/1251 ( 28%)]  Loss: 3.574 (3.36)  Time: 0.675s, 1516.84/s  (0.699s, 1464.95/s)  LR: 9.128e-05  Data: 0.010 (0.016)
Train: 489 [ 400/1251 ( 32%)]  Loss: 3.278 (3.35)  Time: 0.673s, 1520.82/s  (0.698s, 1467.56/s)  LR: 9.128e-05  Data: 0.010 (0.015)
Train: 489 [ 450/1251 ( 36%)]  Loss: 3.570 (3.37)  Time: 0.674s, 1519.94/s  (0.698s, 1467.30/s)  LR: 9.128e-05  Data: 0.011 (0.015)
Train: 489 [ 500/1251 ( 40%)]  Loss: 3.335 (3.37)  Time: 0.666s, 1537.91/s  (0.697s, 1468.52/s)  LR: 9.128e-05  Data: 0.011 (0.014)
Train: 489 [ 550/1251 ( 44%)]  Loss: 3.177 (3.35)  Time: 0.685s, 1494.55/s  (0.697s, 1469.11/s)  LR: 9.128e-05  Data: 0.010 (0.014)
Train: 489 [ 600/1251 ( 48%)]  Loss: 3.163 (3.34)  Time: 0.716s, 1429.53/s  (0.697s, 1469.89/s)  LR: 9.128e-05  Data: 0.010 (0.014)
Train: 489 [ 650/1251 ( 52%)]  Loss: 3.316 (3.34)  Time: 0.707s, 1449.36/s  (0.696s, 1471.27/s)  LR: 9.128e-05  Data: 0.010 (0.013)
Train: 489 [ 700/1251 ( 56%)]  Loss: 3.411 (3.34)  Time: 0.671s, 1526.25/s  (0.696s, 1471.83/s)  LR: 9.128e-05  Data: 0.010 (0.013)
Train: 489 [ 750/1251 ( 60%)]  Loss: 3.044 (3.32)  Time: 0.695s, 1473.18/s  (0.696s, 1471.06/s)  LR: 9.128e-05  Data: 0.009 (0.013)
Train: 489 [ 800/1251 ( 64%)]  Loss: 3.555 (3.34)  Time: 0.723s, 1416.24/s  (0.697s, 1470.11/s)  LR: 9.128e-05  Data: 0.011 (0.013)
Train: 489 [ 850/1251 ( 68%)]  Loss: 3.287 (3.33)  Time: 0.706s, 1450.67/s  (0.696s, 1470.70/s)  LR: 9.128e-05  Data: 0.010 (0.013)
Train: 489 [ 900/1251 ( 72%)]  Loss: 3.343 (3.33)  Time: 0.714s, 1433.31/s  (0.696s, 1471.27/s)  LR: 9.128e-05  Data: 0.010 (0.013)
Train: 489 [ 950/1251 ( 76%)]  Loss: 3.459 (3.34)  Time: 0.725s, 1412.39/s  (0.696s, 1471.39/s)  LR: 9.128e-05  Data: 0.009 (0.012)
Train: 489 [1000/1251 ( 80%)]  Loss: 3.453 (3.35)  Time: 0.710s, 1442.72/s  (0.696s, 1472.05/s)  LR: 9.128e-05  Data: 0.009 (0.012)
Train: 489 [1050/1251 ( 84%)]  Loss: 3.104 (3.34)  Time: 0.701s, 1461.04/s  (0.695s, 1472.93/s)  LR: 9.128e-05  Data: 0.009 (0.012)
Train: 489 [1100/1251 ( 88%)]  Loss: 3.219 (3.33)  Time: 0.702s, 1458.80/s  (0.695s, 1473.70/s)  LR: 9.128e-05  Data: 0.009 (0.012)
Train: 489 [1150/1251 ( 92%)]  Loss: 3.522 (3.34)  Time: 0.683s, 1498.21/s  (0.694s, 1474.51/s)  LR: 9.128e-05  Data: 0.013 (0.012)
Train: 489 [1200/1251 ( 96%)]  Loss: 3.375 (3.34)  Time: 0.791s, 1294.34/s  (0.695s, 1473.31/s)  LR: 9.128e-05  Data: 0.009 (0.012)
Train: 489 [1250/1251 (100%)]  Loss: 3.179 (3.33)  Time: 0.658s, 1557.32/s  (0.695s, 1473.99/s)  LR: 9.128e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.491 (1.491)  Loss:  0.6895 (0.6895)  Acc@1: 91.1133 (91.1133)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.136 (0.578)  Loss:  0.8237 (1.2000)  Acc@1: 85.6132 (77.5840)  Acc@5: 96.4623 (93.7460)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-486.pth.tar', 77.89599995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-487.pth.tar', 77.8500000805664)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-488.pth.tar', 77.7920000830078)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-483.pth.tar', 77.78600000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-484.pth.tar', 77.74200010742187)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-470.pth.tar', 77.73200003662109)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-485.pth.tar', 77.71599997802734)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-481.pth.tar', 77.6860000366211)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-479.pth.tar', 77.60799998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-489.pth.tar', 77.58399995605468)

Train: 490 [   0/1251 (  0%)]  Loss: 3.187 (3.19)  Time: 2.100s,  487.66/s  (2.100s,  487.66/s)  LR: 8.986e-05  Data: 1.484 (1.484)
Train: 490 [  50/1251 (  4%)]  Loss: 3.069 (3.13)  Time: 0.701s, 1459.87/s  (0.720s, 1421.29/s)  LR: 8.986e-05  Data: 0.012 (0.045)
Train: 490 [ 100/1251 (  8%)]  Loss: 3.187 (3.15)  Time: 0.665s, 1538.91/s  (0.704s, 1454.95/s)  LR: 8.986e-05  Data: 0.010 (0.028)
Train: 490 [ 150/1251 ( 12%)]  Loss: 3.731 (3.29)  Time: 0.703s, 1456.93/s  (0.699s, 1465.71/s)  LR: 8.986e-05  Data: 0.009 (0.022)
Train: 490 [ 200/1251 ( 16%)]  Loss: 3.357 (3.31)  Time: 0.703s, 1456.78/s  (0.697s, 1470.20/s)  LR: 8.986e-05  Data: 0.010 (0.019)
Train: 490 [ 250/1251 ( 20%)]  Loss: 3.217 (3.29)  Time: 0.684s, 1497.76/s  (0.698s, 1466.60/s)  LR: 8.986e-05  Data: 0.011 (0.017)
Train: 490 [ 300/1251 ( 24%)]  Loss: 3.250 (3.29)  Time: 0.677s, 1512.48/s  (0.697s, 1468.76/s)  LR: 8.986e-05  Data: 0.010 (0.016)
Train: 490 [ 350/1251 ( 28%)]  Loss: 3.363 (3.30)  Time: 0.680s, 1506.56/s  (0.696s, 1470.77/s)  LR: 8.986e-05  Data: 0.013 (0.015)
Train: 490 [ 400/1251 ( 32%)]  Loss: 3.457 (3.31)  Time: 0.702s, 1457.67/s  (0.696s, 1471.15/s)  LR: 8.986e-05  Data: 0.009 (0.015)
Train: 490 [ 450/1251 ( 36%)]  Loss: 3.595 (3.34)  Time: 0.679s, 1507.21/s  (0.696s, 1471.02/s)  LR: 8.986e-05  Data: 0.015 (0.014)
Train: 490 [ 500/1251 ( 40%)]  Loss: 3.282 (3.34)  Time: 0.672s, 1523.68/s  (0.695s, 1473.32/s)  LR: 8.986e-05  Data: 0.011 (0.014)
Train: 490 [ 550/1251 ( 44%)]  Loss: 3.339 (3.34)  Time: 0.672s, 1523.06/s  (0.695s, 1473.24/s)  LR: 8.986e-05  Data: 0.010 (0.014)
Train: 490 [ 600/1251 ( 48%)]  Loss: 3.390 (3.34)  Time: 0.705s, 1452.94/s  (0.695s, 1473.95/s)  LR: 8.986e-05  Data: 0.009 (0.013)
Train: 490 [ 650/1251 ( 52%)]  Loss: 3.727 (3.37)  Time: 0.682s, 1502.54/s  (0.695s, 1473.80/s)  LR: 8.986e-05  Data: 0.011 (0.013)
Train: 490 [ 700/1251 ( 56%)]  Loss: 3.296 (3.36)  Time: 0.680s, 1505.23/s  (0.694s, 1474.57/s)  LR: 8.986e-05  Data: 0.010 (0.013)
Train: 490 [ 750/1251 ( 60%)]  Loss: 3.157 (3.35)  Time: 0.672s, 1524.61/s  (0.694s, 1474.73/s)  LR: 8.986e-05  Data: 0.011 (0.013)
Train: 490 [ 800/1251 ( 64%)]  Loss: 3.617 (3.37)  Time: 0.758s, 1351.60/s  (0.694s, 1475.49/s)  LR: 8.986e-05  Data: 0.009 (0.013)
Train: 490 [ 850/1251 ( 68%)]  Loss: 3.440 (3.37)  Time: 0.679s, 1508.31/s  (0.694s, 1475.71/s)  LR: 8.986e-05  Data: 0.011 (0.013)
Train: 490 [ 900/1251 ( 72%)]  Loss: 2.919 (3.35)  Time: 0.709s, 1444.73/s  (0.694s, 1476.32/s)  LR: 8.986e-05  Data: 0.010 (0.012)
Train: 490 [ 950/1251 ( 76%)]  Loss: 3.373 (3.35)  Time: 0.672s, 1524.39/s  (0.694s, 1476.17/s)  LR: 8.986e-05  Data: 0.012 (0.012)
Train: 490 [1000/1251 ( 80%)]  Loss: 3.367 (3.35)  Time: 0.673s, 1521.15/s  (0.693s, 1476.80/s)  LR: 8.986e-05  Data: 0.010 (0.012)
Train: 490 [1050/1251 ( 84%)]  Loss: 3.511 (3.36)  Time: 0.673s, 1521.09/s  (0.693s, 1476.57/s)  LR: 8.986e-05  Data: 0.010 (0.012)
Train: 490 [1100/1251 ( 88%)]  Loss: 3.362 (3.36)  Time: 0.697s, 1469.20/s  (0.694s, 1476.41/s)  LR: 8.986e-05  Data: 0.010 (0.012)
Train: 490 [1150/1251 ( 92%)]  Loss: 2.977 (3.34)  Time: 0.691s, 1481.22/s  (0.694s, 1476.30/s)  LR: 8.986e-05  Data: 0.009 (0.012)
Train: 490 [1200/1251 ( 96%)]  Loss: 3.161 (3.33)  Time: 0.685s, 1495.38/s  (0.694s, 1476.42/s)  LR: 8.986e-05  Data: 0.009 (0.012)
Train: 490 [1250/1251 (100%)]  Loss: 3.272 (3.33)  Time: 0.710s, 1441.48/s  (0.693s, 1476.72/s)  LR: 8.986e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.609 (1.609)  Loss:  0.7256 (0.7256)  Acc@1: 91.3086 (91.3086)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.136 (0.574)  Loss:  0.9087 (1.2623)  Acc@1: 85.7311 (78.0900)  Acc@5: 97.4057 (93.9160)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-490.pth.tar', 78.09000000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-486.pth.tar', 77.89599995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-487.pth.tar', 77.8500000805664)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-488.pth.tar', 77.7920000830078)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-483.pth.tar', 77.78600000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-484.pth.tar', 77.74200010742187)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-470.pth.tar', 77.73200003662109)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-485.pth.tar', 77.71599997802734)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-481.pth.tar', 77.6860000366211)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-479.pth.tar', 77.60799998046875)

Train: 491 [   0/1251 (  0%)]  Loss: 3.289 (3.29)  Time: 2.317s,  441.88/s  (2.317s,  441.88/s)  LR: 8.845e-05  Data: 1.670 (1.670)
Train: 491 [  50/1251 (  4%)]  Loss: 3.163 (3.23)  Time: 0.673s, 1522.05/s  (0.727s, 1408.68/s)  LR: 8.845e-05  Data: 0.011 (0.049)
Train: 491 [ 100/1251 (  8%)]  Loss: 3.208 (3.22)  Time: 0.719s, 1423.74/s  (0.711s, 1440.94/s)  LR: 8.845e-05  Data: 0.010 (0.030)
Train: 491 [ 150/1251 ( 12%)]  Loss: 3.292 (3.24)  Time: 0.666s, 1536.87/s  (0.706s, 1450.37/s)  LR: 8.845e-05  Data: 0.009 (0.024)
Train: 491 [ 200/1251 ( 16%)]  Loss: 3.685 (3.33)  Time: 0.673s, 1520.66/s  (0.703s, 1455.88/s)  LR: 8.845e-05  Data: 0.010 (0.021)
Train: 491 [ 250/1251 ( 20%)]  Loss: 3.585 (3.37)  Time: 0.672s, 1524.05/s  (0.702s, 1459.11/s)  LR: 8.845e-05  Data: 0.011 (0.018)
Train: 491 [ 300/1251 ( 24%)]  Loss: 3.356 (3.37)  Time: 0.699s, 1465.61/s  (0.700s, 1462.39/s)  LR: 8.845e-05  Data: 0.009 (0.017)
Train: 491 [ 350/1251 ( 28%)]  Loss: 3.581 (3.39)  Time: 0.690s, 1484.43/s  (0.698s, 1467.35/s)  LR: 8.845e-05  Data: 0.009 (0.016)
Train: 491 [ 400/1251 ( 32%)]  Loss: 3.280 (3.38)  Time: 0.716s, 1430.08/s  (0.697s, 1468.58/s)  LR: 8.845e-05  Data: 0.010 (0.015)
Train: 491 [ 450/1251 ( 36%)]  Loss: 3.206 (3.36)  Time: 0.672s, 1524.41/s  (0.696s, 1470.76/s)  LR: 8.845e-05  Data: 0.010 (0.015)
Train: 491 [ 500/1251 ( 40%)]  Loss: 3.628 (3.39)  Time: 0.704s, 1454.59/s  (0.696s, 1472.25/s)  LR: 8.845e-05  Data: 0.011 (0.014)
Train: 491 [ 550/1251 ( 44%)]  Loss: 2.999 (3.36)  Time: 0.668s, 1533.53/s  (0.695s, 1474.00/s)  LR: 8.845e-05  Data: 0.010 (0.014)
Train: 491 [ 600/1251 ( 48%)]  Loss: 3.333 (3.35)  Time: 0.699s, 1464.11/s  (0.695s, 1473.42/s)  LR: 8.845e-05  Data: 0.009 (0.014)
Train: 491 [ 650/1251 ( 52%)]  Loss: 3.139 (3.34)  Time: 0.692s, 1479.24/s  (0.695s, 1474.12/s)  LR: 8.845e-05  Data: 0.010 (0.013)
Train: 491 [ 700/1251 ( 56%)]  Loss: 3.418 (3.34)  Time: 0.673s, 1520.61/s  (0.695s, 1473.73/s)  LR: 8.845e-05  Data: 0.010 (0.013)
Train: 491 [ 750/1251 ( 60%)]  Loss: 3.545 (3.36)  Time: 0.705s, 1452.56/s  (0.695s, 1473.67/s)  LR: 8.845e-05  Data: 0.011 (0.013)
Train: 491 [ 800/1251 ( 64%)]  Loss: 3.592 (3.37)  Time: 0.675s, 1516.70/s  (0.695s, 1472.97/s)  LR: 8.845e-05  Data: 0.010 (0.013)
Train: 491 [ 850/1251 ( 68%)]  Loss: 3.280 (3.37)  Time: 0.673s, 1520.48/s  (0.695s, 1472.78/s)  LR: 8.845e-05  Data: 0.011 (0.013)
Train: 491 [ 900/1251 ( 72%)]  Loss: 3.483 (3.37)  Time: 0.678s, 1509.26/s  (0.695s, 1472.56/s)  LR: 8.845e-05  Data: 0.010 (0.013)
Train: 491 [ 950/1251 ( 76%)]  Loss: 3.210 (3.36)  Time: 0.670s, 1528.44/s  (0.696s, 1472.25/s)  LR: 8.845e-05  Data: 0.010 (0.013)
Train: 491 [1000/1251 ( 80%)]  Loss: 3.338 (3.36)  Time: 0.706s, 1450.24/s  (0.695s, 1473.03/s)  LR: 8.845e-05  Data: 0.009 (0.012)
Train: 491 [1050/1251 ( 84%)]  Loss: 3.569 (3.37)  Time: 0.682s, 1501.38/s  (0.695s, 1472.78/s)  LR: 8.845e-05  Data: 0.010 (0.012)
Train: 491 [1100/1251 ( 88%)]  Loss: 3.147 (3.36)  Time: 0.706s, 1450.80/s  (0.695s, 1473.28/s)  LR: 8.845e-05  Data: 0.011 (0.012)
Train: 491 [1150/1251 ( 92%)]  Loss: 3.156 (3.35)  Time: 0.694s, 1476.40/s  (0.695s, 1474.21/s)  LR: 8.845e-05  Data: 0.010 (0.012)
Train: 491 [1200/1251 ( 96%)]  Loss: 3.370 (3.35)  Time: 0.672s, 1523.86/s  (0.694s, 1474.76/s)  LR: 8.845e-05  Data: 0.010 (0.012)
Train: 491 [1250/1251 (100%)]  Loss: 3.145 (3.35)  Time: 0.676s, 1514.70/s  (0.694s, 1475.14/s)  LR: 8.845e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.465 (1.465)  Loss:  0.7397 (0.7397)  Acc@1: 91.3086 (91.3086)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.136 (0.590)  Loss:  0.8281 (1.2557)  Acc@1: 85.9670 (77.9180)  Acc@5: 97.5236 (93.7980)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-490.pth.tar', 78.09000000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-491.pth.tar', 77.91799998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-486.pth.tar', 77.89599995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-487.pth.tar', 77.8500000805664)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-488.pth.tar', 77.7920000830078)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-483.pth.tar', 77.78600000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-484.pth.tar', 77.74200010742187)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-470.pth.tar', 77.73200003662109)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-485.pth.tar', 77.71599997802734)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-481.pth.tar', 77.6860000366211)

Train: 492 [   0/1251 (  0%)]  Loss: 3.228 (3.23)  Time: 2.157s,  474.82/s  (2.157s,  474.82/s)  LR: 8.706e-05  Data: 1.521 (1.521)
Train: 492 [  50/1251 (  4%)]  Loss: 3.152 (3.19)  Time: 0.744s, 1377.03/s  (0.736s, 1391.74/s)  LR: 8.706e-05  Data: 0.009 (0.046)
Train: 492 [ 100/1251 (  8%)]  Loss: 3.235 (3.21)  Time: 0.701s, 1460.92/s  (0.716s, 1431.11/s)  LR: 8.706e-05  Data: 0.011 (0.029)
Train: 492 [ 150/1251 ( 12%)]  Loss: 3.198 (3.20)  Time: 0.692s, 1478.93/s  (0.708s, 1447.29/s)  LR: 8.706e-05  Data: 0.011 (0.023)
Train: 492 [ 200/1251 ( 16%)]  Loss: 3.550 (3.27)  Time: 0.715s, 1433.01/s  (0.703s, 1455.61/s)  LR: 8.706e-05  Data: 0.011 (0.020)
Train: 492 [ 250/1251 ( 20%)]  Loss: 3.429 (3.30)  Time: 0.667s, 1534.21/s  (0.700s, 1462.03/s)  LR: 8.706e-05  Data: 0.009 (0.018)
Train: 492 [ 300/1251 ( 24%)]  Loss: 3.335 (3.30)  Time: 0.675s, 1517.28/s  (0.699s, 1464.46/s)  LR: 8.706e-05  Data: 0.015 (0.017)
Train: 492 [ 350/1251 ( 28%)]  Loss: 3.183 (3.29)  Time: 0.674s, 1520.04/s  (0.699s, 1465.28/s)  LR: 8.706e-05  Data: 0.010 (0.016)
Train: 492 [ 400/1251 ( 32%)]  Loss: 3.389 (3.30)  Time: 0.694s, 1475.12/s  (0.699s, 1465.27/s)  LR: 8.706e-05  Data: 0.009 (0.015)
Train: 492 [ 450/1251 ( 36%)]  Loss: 3.109 (3.28)  Time: 0.674s, 1519.33/s  (0.697s, 1468.73/s)  LR: 8.706e-05  Data: 0.011 (0.015)
Train: 492 [ 500/1251 ( 40%)]  Loss: 3.277 (3.28)  Time: 0.719s, 1424.27/s  (0.697s, 1470.17/s)  LR: 8.706e-05  Data: 0.009 (0.014)
Train: 492 [ 550/1251 ( 44%)]  Loss: 3.613 (3.31)  Time: 0.690s, 1483.64/s  (0.696s, 1471.19/s)  LR: 8.706e-05  Data: 0.014 (0.014)
Train: 492 [ 600/1251 ( 48%)]  Loss: 3.380 (3.31)  Time: 0.680s, 1506.31/s  (0.696s, 1470.84/s)  LR: 8.706e-05  Data: 0.009 (0.014)
Train: 492 [ 650/1251 ( 52%)]  Loss: 3.622 (3.34)  Time: 0.672s, 1523.11/s  (0.696s, 1471.70/s)  LR: 8.706e-05  Data: 0.016 (0.013)
Train: 492 [ 700/1251 ( 56%)]  Loss: 3.419 (3.34)  Time: 0.721s, 1420.61/s  (0.696s, 1471.34/s)  LR: 8.706e-05  Data: 0.017 (0.013)
Train: 492 [ 750/1251 ( 60%)]  Loss: 3.149 (3.33)  Time: 0.721s, 1421.21/s  (0.697s, 1469.66/s)  LR: 8.706e-05  Data: 0.011 (0.013)
Train: 492 [ 800/1251 ( 64%)]  Loss: 3.273 (3.33)  Time: 0.743s, 1377.28/s  (0.697s, 1468.88/s)  LR: 8.706e-05  Data: 0.009 (0.013)
Train: 492 [ 850/1251 ( 68%)]  Loss: 3.538 (3.34)  Time: 0.687s, 1491.47/s  (0.696s, 1470.55/s)  LR: 8.706e-05  Data: 0.009 (0.013)
Train: 492 [ 900/1251 ( 72%)]  Loss: 3.532 (3.35)  Time: 0.671s, 1525.27/s  (0.696s, 1471.74/s)  LR: 8.706e-05  Data: 0.011 (0.013)
Train: 492 [ 950/1251 ( 76%)]  Loss: 3.396 (3.35)  Time: 0.690s, 1483.18/s  (0.696s, 1472.22/s)  LR: 8.706e-05  Data: 0.011 (0.012)
Train: 492 [1000/1251 ( 80%)]  Loss: 3.377 (3.35)  Time: 0.672s, 1524.88/s  (0.695s, 1472.45/s)  LR: 8.706e-05  Data: 0.011 (0.012)
Train: 492 [1050/1251 ( 84%)]  Loss: 3.043 (3.34)  Time: 0.703s, 1455.80/s  (0.695s, 1472.70/s)  LR: 8.706e-05  Data: 0.009 (0.012)
Train: 492 [1100/1251 ( 88%)]  Loss: 3.663 (3.35)  Time: 0.673s, 1522.06/s  (0.695s, 1473.64/s)  LR: 8.706e-05  Data: 0.010 (0.012)
Train: 492 [1150/1251 ( 92%)]  Loss: 3.055 (3.34)  Time: 0.668s, 1533.30/s  (0.695s, 1473.92/s)  LR: 8.706e-05  Data: 0.009 (0.012)
Train: 492 [1200/1251 ( 96%)]  Loss: 3.295 (3.34)  Time: 0.714s, 1433.61/s  (0.695s, 1474.38/s)  LR: 8.706e-05  Data: 0.010 (0.012)
Train: 492 [1250/1251 (100%)]  Loss: 3.251 (3.33)  Time: 0.657s, 1558.66/s  (0.694s, 1475.04/s)  LR: 8.706e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.578 (1.578)  Loss:  0.7280 (0.7280)  Acc@1: 91.4062 (91.4062)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.136 (0.575)  Loss:  0.7632 (1.2111)  Acc@1: 86.6745 (77.9820)  Acc@5: 96.8160 (93.9100)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-490.pth.tar', 78.09000000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-492.pth.tar', 77.9820001586914)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-491.pth.tar', 77.91799998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-486.pth.tar', 77.89599995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-487.pth.tar', 77.8500000805664)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-488.pth.tar', 77.7920000830078)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-483.pth.tar', 77.78600000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-484.pth.tar', 77.74200010742187)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-470.pth.tar', 77.73200003662109)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-485.pth.tar', 77.71599997802734)

Train: 493 [   0/1251 (  0%)]  Loss: 3.491 (3.49)  Time: 2.213s,  462.67/s  (2.213s,  462.67/s)  LR: 8.567e-05  Data: 1.597 (1.597)
Train: 493 [  50/1251 (  4%)]  Loss: 3.257 (3.37)  Time: 0.677s, 1512.82/s  (0.730s, 1403.16/s)  LR: 8.567e-05  Data: 0.009 (0.049)
Train: 493 [ 100/1251 (  8%)]  Loss: 3.267 (3.34)  Time: 0.665s, 1538.78/s  (0.707s, 1449.14/s)  LR: 8.567e-05  Data: 0.010 (0.030)
Train: 493 [ 150/1251 ( 12%)]  Loss: 3.138 (3.29)  Time: 0.725s, 1412.02/s  (0.700s, 1463.56/s)  LR: 8.567e-05  Data: 0.009 (0.023)
Train: 493 [ 200/1251 ( 16%)]  Loss: 3.585 (3.35)  Time: 0.716s, 1430.67/s  (0.697s, 1468.29/s)  LR: 8.567e-05  Data: 0.009 (0.020)
Train: 493 [ 250/1251 ( 20%)]  Loss: 2.964 (3.28)  Time: 0.673s, 1522.65/s  (0.695s, 1472.91/s)  LR: 8.567e-05  Data: 0.009 (0.018)
Train: 493 [ 300/1251 ( 24%)]  Loss: 3.179 (3.27)  Time: 0.688s, 1489.16/s  (0.696s, 1471.62/s)  LR: 8.567e-05  Data: 0.013 (0.017)
Train: 493 [ 350/1251 ( 28%)]  Loss: 3.359 (3.28)  Time: 0.704s, 1454.11/s  (0.697s, 1469.91/s)  LR: 8.567e-05  Data: 0.009 (0.016)
Train: 493 [ 400/1251 ( 32%)]  Loss: 3.276 (3.28)  Time: 0.675s, 1516.31/s  (0.697s, 1469.61/s)  LR: 8.567e-05  Data: 0.010 (0.015)
Train: 493 [ 450/1251 ( 36%)]  Loss: 3.076 (3.26)  Time: 0.750s, 1364.57/s  (0.696s, 1470.49/s)  LR: 8.567e-05  Data: 0.011 (0.015)
Train: 493 [ 500/1251 ( 40%)]  Loss: 3.212 (3.25)  Time: 0.672s, 1523.81/s  (0.696s, 1471.37/s)  LR: 8.567e-05  Data: 0.010 (0.014)
Train: 493 [ 550/1251 ( 44%)]  Loss: 3.548 (3.28)  Time: 0.671s, 1525.43/s  (0.695s, 1472.74/s)  LR: 8.567e-05  Data: 0.010 (0.014)
Train: 493 [ 600/1251 ( 48%)]  Loss: 3.371 (3.29)  Time: 0.696s, 1470.22/s  (0.695s, 1472.99/s)  LR: 8.567e-05  Data: 0.009 (0.014)
Train: 493 [ 650/1251 ( 52%)]  Loss: 3.338 (3.29)  Time: 0.720s, 1421.44/s  (0.695s, 1473.25/s)  LR: 8.567e-05  Data: 0.011 (0.013)
Train: 493 [ 700/1251 ( 56%)]  Loss: 3.161 (3.28)  Time: 0.716s, 1431.10/s  (0.695s, 1473.34/s)  LR: 8.567e-05  Data: 0.010 (0.013)
Train: 493 [ 750/1251 ( 60%)]  Loss: 3.269 (3.28)  Time: 0.672s, 1524.50/s  (0.695s, 1473.89/s)  LR: 8.567e-05  Data: 0.010 (0.013)
Train: 493 [ 800/1251 ( 64%)]  Loss: 2.867 (3.26)  Time: 0.673s, 1522.55/s  (0.694s, 1474.58/s)  LR: 8.567e-05  Data: 0.010 (0.013)
Train: 493 [ 850/1251 ( 68%)]  Loss: 3.413 (3.27)  Time: 0.671s, 1525.42/s  (0.694s, 1475.13/s)  LR: 8.567e-05  Data: 0.010 (0.013)
Train: 493 [ 900/1251 ( 72%)]  Loss: 3.486 (3.28)  Time: 0.727s, 1409.01/s  (0.694s, 1475.82/s)  LR: 8.567e-05  Data: 0.009 (0.013)
Train: 493 [ 950/1251 ( 76%)]  Loss: 3.385 (3.28)  Time: 0.668s, 1533.76/s  (0.694s, 1476.48/s)  LR: 8.567e-05  Data: 0.010 (0.012)
Train: 493 [1000/1251 ( 80%)]  Loss: 3.041 (3.27)  Time: 0.694s, 1475.77/s  (0.694s, 1476.28/s)  LR: 8.567e-05  Data: 0.011 (0.012)
Train: 493 [1050/1251 ( 84%)]  Loss: 3.406 (3.28)  Time: 0.671s, 1526.70/s  (0.694s, 1476.14/s)  LR: 8.567e-05  Data: 0.010 (0.012)
Train: 493 [1100/1251 ( 88%)]  Loss: 3.114 (3.27)  Time: 0.670s, 1527.86/s  (0.693s, 1476.64/s)  LR: 8.567e-05  Data: 0.014 (0.012)
Train: 493 [1150/1251 ( 92%)]  Loss: 3.156 (3.26)  Time: 0.666s, 1537.74/s  (0.693s, 1476.64/s)  LR: 8.567e-05  Data: 0.010 (0.012)
Train: 493 [1200/1251 ( 96%)]  Loss: 3.537 (3.28)  Time: 0.758s, 1351.20/s  (0.694s, 1475.98/s)  LR: 8.567e-05  Data: 0.011 (0.012)
Train: 493 [1250/1251 (100%)]  Loss: 3.608 (3.29)  Time: 0.657s, 1559.37/s  (0.694s, 1476.32/s)  LR: 8.567e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.620 (1.620)  Loss:  0.7188 (0.7188)  Acc@1: 91.4062 (91.4062)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.137 (0.577)  Loss:  0.8677 (1.2343)  Acc@1: 86.2028 (78.0260)  Acc@5: 96.5802 (93.9160)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-490.pth.tar', 78.09000000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-493.pth.tar', 78.02600008300782)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-492.pth.tar', 77.9820001586914)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-491.pth.tar', 77.91799998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-486.pth.tar', 77.89599995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-487.pth.tar', 77.8500000805664)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-488.pth.tar', 77.7920000830078)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-483.pth.tar', 77.78600000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-484.pth.tar', 77.74200010742187)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-470.pth.tar', 77.73200003662109)

Train: 494 [   0/1251 (  0%)]  Loss: 3.319 (3.32)  Time: 2.235s,  458.24/s  (2.235s,  458.24/s)  LR: 8.430e-05  Data: 1.620 (1.620)
Train: 494 [  50/1251 (  4%)]  Loss: 3.079 (3.20)  Time: 0.708s, 1446.04/s  (0.729s, 1404.91/s)  LR: 8.430e-05  Data: 0.011 (0.049)
Train: 494 [ 100/1251 (  8%)]  Loss: 3.459 (3.29)  Time: 0.671s, 1525.68/s  (0.713s, 1435.57/s)  LR: 8.430e-05  Data: 0.009 (0.030)
Train: 494 [ 150/1251 ( 12%)]  Loss: 3.140 (3.25)  Time: 0.702s, 1459.60/s  (0.707s, 1448.24/s)  LR: 8.430e-05  Data: 0.011 (0.024)
Train: 494 [ 200/1251 ( 16%)]  Loss: 3.572 (3.31)  Time: 0.696s, 1470.54/s  (0.704s, 1455.06/s)  LR: 8.430e-05  Data: 0.009 (0.020)
Train: 494 [ 250/1251 ( 20%)]  Loss: 3.189 (3.29)  Time: 0.718s, 1426.39/s  (0.703s, 1457.00/s)  LR: 8.430e-05  Data: 0.011 (0.019)
Train: 494 [ 300/1251 ( 24%)]  Loss: 3.488 (3.32)  Time: 0.671s, 1525.49/s  (0.702s, 1459.56/s)  LR: 8.430e-05  Data: 0.011 (0.017)
Train: 494 [ 350/1251 ( 28%)]  Loss: 3.462 (3.34)  Time: 0.673s, 1522.29/s  (0.701s, 1461.52/s)  LR: 8.430e-05  Data: 0.010 (0.016)
Train: 494 [ 400/1251 ( 32%)]  Loss: 3.570 (3.36)  Time: 0.679s, 1509.14/s  (0.699s, 1465.20/s)  LR: 8.430e-05  Data: 0.010 (0.016)
Train: 494 [ 450/1251 ( 36%)]  Loss: 3.208 (3.35)  Time: 0.673s, 1521.10/s  (0.697s, 1468.64/s)  LR: 8.430e-05  Data: 0.011 (0.015)
Train: 494 [ 500/1251 ( 40%)]  Loss: 3.613 (3.37)  Time: 0.712s, 1438.88/s  (0.697s, 1469.62/s)  LR: 8.430e-05  Data: 0.018 (0.015)
Train: 494 [ 550/1251 ( 44%)]  Loss: 3.239 (3.36)  Time: 0.703s, 1455.81/s  (0.697s, 1470.11/s)  LR: 8.430e-05  Data: 0.010 (0.014)
Train: 494 [ 600/1251 ( 48%)]  Loss: 3.558 (3.38)  Time: 0.672s, 1523.74/s  (0.696s, 1471.69/s)  LR: 8.430e-05  Data: 0.011 (0.014)
Train: 494 [ 650/1251 ( 52%)]  Loss: 3.103 (3.36)  Time: 0.704s, 1454.91/s  (0.696s, 1472.12/s)  LR: 8.430e-05  Data: 0.011 (0.014)
Train: 494 [ 700/1251 ( 56%)]  Loss: 3.413 (3.36)  Time: 0.667s, 1534.24/s  (0.696s, 1472.03/s)  LR: 8.430e-05  Data: 0.009 (0.013)
Train: 494 [ 750/1251 ( 60%)]  Loss: 3.160 (3.35)  Time: 0.672s, 1524.14/s  (0.696s, 1471.81/s)  LR: 8.430e-05  Data: 0.010 (0.013)
Train: 494 [ 800/1251 ( 64%)]  Loss: 3.709 (3.37)  Time: 0.678s, 1510.18/s  (0.696s, 1472.17/s)  LR: 8.430e-05  Data: 0.009 (0.013)
Train: 494 [ 850/1251 ( 68%)]  Loss: 3.471 (3.38)  Time: 0.672s, 1524.52/s  (0.695s, 1473.52/s)  LR: 8.430e-05  Data: 0.010 (0.013)
Train: 494 [ 900/1251 ( 72%)]  Loss: 3.624 (3.39)  Time: 0.671s, 1525.37/s  (0.695s, 1473.34/s)  LR: 8.430e-05  Data: 0.010 (0.013)
Train: 494 [ 950/1251 ( 76%)]  Loss: 3.099 (3.37)  Time: 0.674s, 1519.96/s  (0.695s, 1473.70/s)  LR: 8.430e-05  Data: 0.011 (0.013)
Train: 494 [1000/1251 ( 80%)]  Loss: 3.306 (3.37)  Time: 0.671s, 1525.07/s  (0.694s, 1474.48/s)  LR: 8.430e-05  Data: 0.010 (0.013)
Train: 494 [1050/1251 ( 84%)]  Loss: 3.461 (3.37)  Time: 0.673s, 1521.84/s  (0.695s, 1474.44/s)  LR: 8.430e-05  Data: 0.011 (0.012)
Train: 494 [1100/1251 ( 88%)]  Loss: 3.582 (3.38)  Time: 0.695s, 1474.05/s  (0.694s, 1475.03/s)  LR: 8.430e-05  Data: 0.011 (0.012)
Train: 494 [1150/1251 ( 92%)]  Loss: 3.519 (3.39)  Time: 0.672s, 1522.81/s  (0.694s, 1475.82/s)  LR: 8.430e-05  Data: 0.011 (0.012)
Train: 494 [1200/1251 ( 96%)]  Loss: 3.372 (3.39)  Time: 0.717s, 1427.90/s  (0.694s, 1476.53/s)  LR: 8.430e-05  Data: 0.009 (0.012)
Train: 494 [1250/1251 (100%)]  Loss: 3.157 (3.38)  Time: 0.683s, 1498.50/s  (0.694s, 1476.24/s)  LR: 8.430e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.583 (1.583)  Loss:  0.7559 (0.7559)  Acc@1: 91.7969 (91.7969)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.136 (0.575)  Loss:  0.8823 (1.2476)  Acc@1: 86.6745 (78.0420)  Acc@5: 96.5802 (93.8280)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-490.pth.tar', 78.09000000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-494.pth.tar', 78.04200015869141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-493.pth.tar', 78.02600008300782)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-492.pth.tar', 77.9820001586914)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-491.pth.tar', 77.91799998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-486.pth.tar', 77.89599995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-487.pth.tar', 77.8500000805664)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-488.pth.tar', 77.7920000830078)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-483.pth.tar', 77.78600000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-484.pth.tar', 77.74200010742187)

Train: 495 [   0/1251 (  0%)]  Loss: 3.097 (3.10)  Time: 2.385s,  429.38/s  (2.385s,  429.38/s)  LR: 8.294e-05  Data: 1.769 (1.769)
Train: 495 [  50/1251 (  4%)]  Loss: 3.711 (3.40)  Time: 0.667s, 1535.95/s  (0.725s, 1412.26/s)  LR: 8.294e-05  Data: 0.011 (0.052)
Train: 495 [ 100/1251 (  8%)]  Loss: 3.024 (3.28)  Time: 0.701s, 1460.79/s  (0.712s, 1438.04/s)  LR: 8.294e-05  Data: 0.009 (0.032)
Train: 495 [ 150/1251 ( 12%)]  Loss: 3.532 (3.34)  Time: 0.693s, 1478.02/s  (0.705s, 1452.70/s)  LR: 8.294e-05  Data: 0.010 (0.025)
Train: 495 [ 200/1251 ( 16%)]  Loss: 3.567 (3.39)  Time: 0.727s, 1407.92/s  (0.703s, 1456.03/s)  LR: 8.294e-05  Data: 0.013 (0.021)
Train: 495 [ 250/1251 ( 20%)]  Loss: 3.281 (3.37)  Time: 0.670s, 1528.74/s  (0.702s, 1459.43/s)  LR: 8.294e-05  Data: 0.010 (0.019)
Train: 495 [ 300/1251 ( 24%)]  Loss: 3.078 (3.33)  Time: 0.669s, 1530.04/s  (0.699s, 1464.39/s)  LR: 8.294e-05  Data: 0.014 (0.017)
Train: 495 [ 350/1251 ( 28%)]  Loss: 3.298 (3.32)  Time: 0.670s, 1527.49/s  (0.698s, 1466.86/s)  LR: 8.294e-05  Data: 0.010 (0.016)
Train: 495 [ 400/1251 ( 32%)]  Loss: 3.405 (3.33)  Time: 0.706s, 1451.37/s  (0.698s, 1466.75/s)  LR: 8.294e-05  Data: 0.010 (0.016)
Train: 495 [ 450/1251 ( 36%)]  Loss: 3.248 (3.32)  Time: 0.761s, 1345.46/s  (0.698s, 1467.12/s)  LR: 8.294e-05  Data: 0.009 (0.015)
Train: 495 [ 500/1251 ( 40%)]  Loss: 3.127 (3.31)  Time: 0.710s, 1443.24/s  (0.699s, 1465.86/s)  LR: 8.294e-05  Data: 0.012 (0.015)
Train: 495 [ 550/1251 ( 44%)]  Loss: 3.043 (3.28)  Time: 0.683s, 1499.99/s  (0.699s, 1465.62/s)  LR: 8.294e-05  Data: 0.015 (0.014)
Train: 495 [ 600/1251 ( 48%)]  Loss: 3.170 (3.28)  Time: 0.672s, 1522.84/s  (0.698s, 1466.80/s)  LR: 8.294e-05  Data: 0.009 (0.014)
Train: 495 [ 650/1251 ( 52%)]  Loss: 3.431 (3.29)  Time: 0.691s, 1481.31/s  (0.698s, 1468.09/s)  LR: 8.294e-05  Data: 0.019 (0.014)
Train: 495 [ 700/1251 ( 56%)]  Loss: 3.024 (3.27)  Time: 0.677s, 1513.57/s  (0.697s, 1469.68/s)  LR: 8.294e-05  Data: 0.010 (0.013)
Train: 495 [ 750/1251 ( 60%)]  Loss: 3.711 (3.30)  Time: 0.670s, 1527.59/s  (0.696s, 1470.88/s)  LR: 8.294e-05  Data: 0.010 (0.013)
Train: 495 [ 800/1251 ( 64%)]  Loss: 2.986 (3.28)  Time: 0.702s, 1459.72/s  (0.696s, 1470.42/s)  LR: 8.294e-05  Data: 0.011 (0.013)
Train: 495 [ 850/1251 ( 68%)]  Loss: 3.536 (3.29)  Time: 0.694s, 1474.89/s  (0.697s, 1470.17/s)  LR: 8.294e-05  Data: 0.009 (0.013)
Train: 495 [ 900/1251 ( 72%)]  Loss: 3.671 (3.31)  Time: 0.679s, 1508.53/s  (0.696s, 1470.85/s)  LR: 8.294e-05  Data: 0.009 (0.013)
Train: 495 [ 950/1251 ( 76%)]  Loss: 3.215 (3.31)  Time: 0.700s, 1461.87/s  (0.696s, 1471.57/s)  LR: 8.294e-05  Data: 0.013 (0.013)
Train: 495 [1000/1251 ( 80%)]  Loss: 3.348 (3.31)  Time: 0.729s, 1403.97/s  (0.696s, 1471.41/s)  LR: 8.294e-05  Data: 0.010 (0.013)
Train: 495 [1050/1251 ( 84%)]  Loss: 3.597 (3.32)  Time: 0.673s, 1521.76/s  (0.696s, 1471.86/s)  LR: 8.294e-05  Data: 0.011 (0.013)
Train: 495 [1100/1251 ( 88%)]  Loss: 3.026 (3.31)  Time: 0.712s, 1439.19/s  (0.695s, 1472.57/s)  LR: 8.294e-05  Data: 0.011 (0.012)
Train: 495 [1150/1251 ( 92%)]  Loss: 3.228 (3.31)  Time: 0.672s, 1522.83/s  (0.695s, 1472.72/s)  LR: 8.294e-05  Data: 0.010 (0.012)
Train: 495 [1200/1251 ( 96%)]  Loss: 3.450 (3.31)  Time: 0.673s, 1522.31/s  (0.695s, 1473.05/s)  LR: 8.294e-05  Data: 0.009 (0.012)
Train: 495 [1250/1251 (100%)]  Loss: 3.037 (3.30)  Time: 0.658s, 1555.70/s  (0.695s, 1473.24/s)  LR: 8.294e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.479 (1.479)  Loss:  0.6348 (0.6348)  Acc@1: 90.5273 (90.5273)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.136 (0.579)  Loss:  0.7905 (1.1660)  Acc@1: 85.2594 (78.1120)  Acc@5: 96.9340 (93.8860)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-495.pth.tar', 78.11200006103516)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-490.pth.tar', 78.09000000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-494.pth.tar', 78.04200015869141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-493.pth.tar', 78.02600008300782)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-492.pth.tar', 77.9820001586914)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-491.pth.tar', 77.91799998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-486.pth.tar', 77.89599995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-487.pth.tar', 77.8500000805664)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-488.pth.tar', 77.7920000830078)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-483.pth.tar', 77.78600000732422)

Train: 496 [   0/1251 (  0%)]  Loss: 3.340 (3.34)  Time: 2.358s,  434.20/s  (2.358s,  434.20/s)  LR: 8.159e-05  Data: 1.743 (1.743)
Train: 496 [  50/1251 (  4%)]  Loss: 2.999 (3.17)  Time: 0.673s, 1521.04/s  (0.729s, 1404.23/s)  LR: 8.159e-05  Data: 0.011 (0.051)
Train: 496 [ 100/1251 (  8%)]  Loss: 3.537 (3.29)  Time: 0.704s, 1455.07/s  (0.717s, 1429.06/s)  LR: 8.159e-05  Data: 0.009 (0.031)
Train: 496 [ 150/1251 ( 12%)]  Loss: 3.545 (3.36)  Time: 0.676s, 1513.85/s  (0.708s, 1446.94/s)  LR: 8.159e-05  Data: 0.011 (0.024)
Train: 496 [ 200/1251 ( 16%)]  Loss: 3.499 (3.38)  Time: 0.675s, 1516.65/s  (0.703s, 1457.10/s)  LR: 8.159e-05  Data: 0.011 (0.021)
Train: 496 [ 250/1251 ( 20%)]  Loss: 3.211 (3.36)  Time: 0.672s, 1524.90/s  (0.700s, 1462.12/s)  LR: 8.159e-05  Data: 0.011 (0.019)
Train: 496 [ 300/1251 ( 24%)]  Loss: 3.302 (3.35)  Time: 0.673s, 1521.13/s  (0.698s, 1467.40/s)  LR: 8.159e-05  Data: 0.014 (0.017)
Train: 496 [ 350/1251 ( 28%)]  Loss: 3.666 (3.39)  Time: 0.687s, 1490.22/s  (0.697s, 1468.67/s)  LR: 8.159e-05  Data: 0.013 (0.016)
Train: 496 [ 400/1251 ( 32%)]  Loss: 3.292 (3.38)  Time: 0.687s, 1490.34/s  (0.698s, 1467.66/s)  LR: 8.159e-05  Data: 0.014 (0.016)
Train: 496 [ 450/1251 ( 36%)]  Loss: 3.326 (3.37)  Time: 0.668s, 1532.97/s  (0.697s, 1468.32/s)  LR: 8.159e-05  Data: 0.010 (0.015)
Train: 496 [ 500/1251 ( 40%)]  Loss: 3.577 (3.39)  Time: 0.706s, 1450.73/s  (0.696s, 1470.26/s)  LR: 8.159e-05  Data: 0.010 (0.015)
Train: 496 [ 550/1251 ( 44%)]  Loss: 3.487 (3.40)  Time: 0.706s, 1451.21/s  (0.696s, 1471.28/s)  LR: 8.159e-05  Data: 0.011 (0.014)
Train: 496 [ 600/1251 ( 48%)]  Loss: 3.182 (3.38)  Time: 0.703s, 1457.63/s  (0.695s, 1473.64/s)  LR: 8.159e-05  Data: 0.010 (0.014)
Train: 496 [ 650/1251 ( 52%)]  Loss: 3.316 (3.38)  Time: 0.717s, 1428.78/s  (0.695s, 1473.95/s)  LR: 8.159e-05  Data: 0.009 (0.014)
Train: 496 [ 700/1251 ( 56%)]  Loss: 3.487 (3.38)  Time: 0.671s, 1526.76/s  (0.695s, 1473.90/s)  LR: 8.159e-05  Data: 0.011 (0.013)
Train: 496 [ 750/1251 ( 60%)]  Loss: 3.521 (3.39)  Time: 0.684s, 1497.72/s  (0.694s, 1475.19/s)  LR: 8.159e-05  Data: 0.009 (0.013)
Train: 496 [ 800/1251 ( 64%)]  Loss: 3.617 (3.41)  Time: 0.696s, 1470.55/s  (0.694s, 1475.05/s)  LR: 8.159e-05  Data: 0.010 (0.013)
Train: 496 [ 850/1251 ( 68%)]  Loss: 3.796 (3.43)  Time: 0.698s, 1468.03/s  (0.694s, 1475.89/s)  LR: 8.159e-05  Data: 0.011 (0.013)
Train: 496 [ 900/1251 ( 72%)]  Loss: 3.675 (3.44)  Time: 0.674s, 1518.91/s  (0.694s, 1475.79/s)  LR: 8.159e-05  Data: 0.012 (0.013)
Train: 496 [ 950/1251 ( 76%)]  Loss: 3.347 (3.44)  Time: 0.718s, 1426.74/s  (0.694s, 1475.98/s)  LR: 8.159e-05  Data: 0.011 (0.013)
Train: 496 [1000/1251 ( 80%)]  Loss: 3.167 (3.42)  Time: 0.674s, 1518.75/s  (0.694s, 1476.05/s)  LR: 8.159e-05  Data: 0.009 (0.012)
Train: 496 [1050/1251 ( 84%)]  Loss: 3.128 (3.41)  Time: 0.672s, 1523.42/s  (0.694s, 1476.23/s)  LR: 8.159e-05  Data: 0.010 (0.012)
Train: 496 [1100/1251 ( 88%)]  Loss: 3.102 (3.40)  Time: 0.692s, 1480.46/s  (0.694s, 1476.39/s)  LR: 8.159e-05  Data: 0.010 (0.012)
Train: 496 [1150/1251 ( 92%)]  Loss: 2.993 (3.38)  Time: 0.684s, 1497.10/s  (0.694s, 1476.55/s)  LR: 8.159e-05  Data: 0.010 (0.012)
Train: 496 [1200/1251 ( 96%)]  Loss: 3.328 (3.38)  Time: 0.672s, 1523.54/s  (0.693s, 1476.94/s)  LR: 8.159e-05  Data: 0.011 (0.012)
Train: 496 [1250/1251 (100%)]  Loss: 3.146 (3.37)  Time: 0.662s, 1547.00/s  (0.693s, 1477.00/s)  LR: 8.159e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.570 (1.570)  Loss:  0.7056 (0.7056)  Acc@1: 91.1133 (91.1133)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.136 (0.575)  Loss:  0.8262 (1.2199)  Acc@1: 86.6745 (78.0060)  Acc@5: 96.8160 (93.7980)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-495.pth.tar', 78.11200006103516)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-490.pth.tar', 78.09000000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-494.pth.tar', 78.04200015869141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-493.pth.tar', 78.02600008300782)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-496.pth.tar', 78.00599989990235)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-492.pth.tar', 77.9820001586914)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-491.pth.tar', 77.91799998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-486.pth.tar', 77.89599995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-487.pth.tar', 77.8500000805664)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-488.pth.tar', 77.7920000830078)

Train: 497 [   0/1251 (  0%)]  Loss: 3.556 (3.56)  Time: 2.228s,  459.67/s  (2.228s,  459.67/s)  LR: 8.026e-05  Data: 1.613 (1.613)
Train: 497 [  50/1251 (  4%)]  Loss: 3.189 (3.37)  Time: 0.671s, 1525.21/s  (0.725s, 1412.49/s)  LR: 8.026e-05  Data: 0.010 (0.050)
Train: 497 [ 100/1251 (  8%)]  Loss: 3.462 (3.40)  Time: 0.671s, 1526.45/s  (0.708s, 1446.61/s)  LR: 8.026e-05  Data: 0.011 (0.030)
Train: 497 [ 150/1251 ( 12%)]  Loss: 3.574 (3.45)  Time: 0.705s, 1452.07/s  (0.701s, 1460.36/s)  LR: 8.026e-05  Data: 0.009 (0.024)
Train: 497 [ 200/1251 ( 16%)]  Loss: 3.393 (3.43)  Time: 0.678s, 1511.09/s  (0.698s, 1467.42/s)  LR: 8.026e-05  Data: 0.011 (0.020)
Train: 497 [ 250/1251 ( 20%)]  Loss: 3.389 (3.43)  Time: 0.706s, 1450.70/s  (0.697s, 1468.62/s)  LR: 8.026e-05  Data: 0.010 (0.018)
Train: 497 [ 300/1251 ( 24%)]  Loss: 3.178 (3.39)  Time: 0.681s, 1503.79/s  (0.697s, 1469.36/s)  LR: 8.026e-05  Data: 0.012 (0.017)
Train: 497 [ 350/1251 ( 28%)]  Loss: 3.618 (3.42)  Time: 0.671s, 1526.11/s  (0.696s, 1471.72/s)  LR: 8.026e-05  Data: 0.011 (0.016)
Train: 497 [ 400/1251 ( 32%)]  Loss: 3.565 (3.44)  Time: 0.705s, 1452.04/s  (0.696s, 1472.10/s)  LR: 8.026e-05  Data: 0.011 (0.016)
Train: 497 [ 450/1251 ( 36%)]  Loss: 3.369 (3.43)  Time: 0.671s, 1527.00/s  (0.695s, 1474.22/s)  LR: 8.026e-05  Data: 0.011 (0.015)
Train: 497 [ 500/1251 ( 40%)]  Loss: 3.428 (3.43)  Time: 0.677s, 1511.46/s  (0.694s, 1474.62/s)  LR: 8.026e-05  Data: 0.009 (0.015)
Train: 497 [ 550/1251 ( 44%)]  Loss: 3.092 (3.40)  Time: 0.711s, 1441.18/s  (0.694s, 1475.82/s)  LR: 8.026e-05  Data: 0.014 (0.014)
Train: 497 [ 600/1251 ( 48%)]  Loss: 3.444 (3.40)  Time: 0.751s, 1363.67/s  (0.694s, 1475.75/s)  LR: 8.026e-05  Data: 0.010 (0.014)
Train: 497 [ 650/1251 ( 52%)]  Loss: 3.495 (3.41)  Time: 0.675s, 1517.96/s  (0.694s, 1476.35/s)  LR: 8.026e-05  Data: 0.009 (0.014)
Train: 497 [ 700/1251 ( 56%)]  Loss: 3.374 (3.41)  Time: 0.667s, 1534.39/s  (0.694s, 1476.40/s)  LR: 8.026e-05  Data: 0.011 (0.013)
Train: 497 [ 750/1251 ( 60%)]  Loss: 3.593 (3.42)  Time: 0.704s, 1455.00/s  (0.693s, 1476.92/s)  LR: 8.026e-05  Data: 0.009 (0.013)
Train: 497 [ 800/1251 ( 64%)]  Loss: 3.362 (3.42)  Time: 0.697s, 1470.19/s  (0.693s, 1477.66/s)  LR: 8.026e-05  Data: 0.009 (0.013)
Train: 497 [ 850/1251 ( 68%)]  Loss: 3.408 (3.42)  Time: 0.707s, 1448.97/s  (0.693s, 1478.38/s)  LR: 8.026e-05  Data: 0.011 (0.013)
Train: 497 [ 900/1251 ( 72%)]  Loss: 3.270 (3.41)  Time: 0.702s, 1459.72/s  (0.693s, 1477.56/s)  LR: 8.026e-05  Data: 0.010 (0.013)
Train: 497 [ 950/1251 ( 76%)]  Loss: 3.332 (3.40)  Time: 0.712s, 1437.24/s  (0.693s, 1476.73/s)  LR: 8.026e-05  Data: 0.011 (0.013)
Train: 497 [1000/1251 ( 80%)]  Loss: 3.243 (3.40)  Time: 0.706s, 1450.56/s  (0.694s, 1475.62/s)  LR: 8.026e-05  Data: 0.010 (0.013)
Train: 497 [1050/1251 ( 84%)]  Loss: 3.161 (3.39)  Time: 0.667s, 1535.59/s  (0.694s, 1475.83/s)  LR: 8.026e-05  Data: 0.010 (0.012)
Train: 497 [1100/1251 ( 88%)]  Loss: 3.545 (3.39)  Time: 0.747s, 1371.50/s  (0.694s, 1476.41/s)  LR: 8.026e-05  Data: 0.009 (0.012)
Train: 497 [1150/1251 ( 92%)]  Loss: 3.366 (3.39)  Time: 0.674s, 1520.02/s  (0.694s, 1476.27/s)  LR: 8.026e-05  Data: 0.010 (0.012)
Train: 497 [1200/1251 ( 96%)]  Loss: 3.070 (3.38)  Time: 0.768s, 1333.77/s  (0.693s, 1476.80/s)  LR: 8.026e-05  Data: 0.010 (0.012)
Train: 497 [1250/1251 (100%)]  Loss: 3.316 (3.38)  Time: 0.656s, 1560.13/s  (0.693s, 1477.20/s)  LR: 8.026e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.638 (1.638)  Loss:  0.7520 (0.7520)  Acc@1: 91.5039 (91.5039)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.136 (0.583)  Loss:  0.8813 (1.2576)  Acc@1: 86.0849 (77.8920)  Acc@5: 96.4623 (93.9040)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-495.pth.tar', 78.11200006103516)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-490.pth.tar', 78.09000000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-494.pth.tar', 78.04200015869141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-493.pth.tar', 78.02600008300782)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-496.pth.tar', 78.00599989990235)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-492.pth.tar', 77.9820001586914)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-491.pth.tar', 77.91799998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-486.pth.tar', 77.89599995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-497.pth.tar', 77.89199990234376)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-487.pth.tar', 77.8500000805664)

Train: 498 [   0/1251 (  0%)]  Loss: 3.380 (3.38)  Time: 2.086s,  490.78/s  (2.086s,  490.78/s)  LR: 7.893e-05  Data: 1.467 (1.467)
Train: 498 [  50/1251 (  4%)]  Loss: 3.305 (3.34)  Time: 0.675s, 1517.84/s  (0.720s, 1421.43/s)  LR: 7.893e-05  Data: 0.011 (0.047)
Train: 498 [ 100/1251 (  8%)]  Loss: 3.120 (3.27)  Time: 0.733s, 1397.53/s  (0.708s, 1445.58/s)  LR: 7.893e-05  Data: 0.012 (0.029)
Train: 498 [ 150/1251 ( 12%)]  Loss: 3.379 (3.30)  Time: 0.696s, 1471.81/s  (0.709s, 1443.68/s)  LR: 7.893e-05  Data: 0.010 (0.024)
Train: 498 [ 200/1251 ( 16%)]  Loss: 3.553 (3.35)  Time: 0.687s, 1489.51/s  (0.710s, 1441.37/s)  LR: 7.893e-05  Data: 0.012 (0.021)
Train: 498 [ 250/1251 ( 20%)]  Loss: 3.178 (3.32)  Time: 0.677s, 1512.83/s  (0.708s, 1446.23/s)  LR: 7.893e-05  Data: 0.011 (0.019)
Train: 498 [ 300/1251 ( 24%)]  Loss: 3.139 (3.29)  Time: 0.672s, 1523.88/s  (0.704s, 1454.45/s)  LR: 7.893e-05  Data: 0.011 (0.018)
Train: 498 [ 350/1251 ( 28%)]  Loss: 3.363 (3.30)  Time: 0.672s, 1523.28/s  (0.702s, 1458.52/s)  LR: 7.893e-05  Data: 0.009 (0.016)
Train: 498 [ 400/1251 ( 32%)]  Loss: 3.177 (3.29)  Time: 0.702s, 1457.94/s  (0.700s, 1462.13/s)  LR: 7.893e-05  Data: 0.011 (0.016)
Train: 498 [ 450/1251 ( 36%)]  Loss: 3.432 (3.30)  Time: 0.694s, 1475.73/s  (0.699s, 1465.56/s)  LR: 7.893e-05  Data: 0.010 (0.015)
Train: 498 [ 500/1251 ( 40%)]  Loss: 3.347 (3.31)  Time: 0.709s, 1443.95/s  (0.698s, 1466.43/s)  LR: 7.893e-05  Data: 0.011 (0.015)
Train: 498 [ 550/1251 ( 44%)]  Loss: 3.102 (3.29)  Time: 0.712s, 1437.69/s  (0.698s, 1467.63/s)  LR: 7.893e-05  Data: 0.010 (0.014)
Train: 498 [ 600/1251 ( 48%)]  Loss: 3.418 (3.30)  Time: 0.706s, 1450.94/s  (0.698s, 1467.91/s)  LR: 7.893e-05  Data: 0.011 (0.014)
Train: 498 [ 650/1251 ( 52%)]  Loss: 2.893 (3.27)  Time: 0.675s, 1517.40/s  (0.697s, 1468.88/s)  LR: 7.893e-05  Data: 0.010 (0.014)
Train: 498 [ 700/1251 ( 56%)]  Loss: 3.233 (3.27)  Time: 0.711s, 1440.81/s  (0.697s, 1469.23/s)  LR: 7.893e-05  Data: 0.010 (0.013)
Train: 498 [ 750/1251 ( 60%)]  Loss: 3.334 (3.27)  Time: 0.704s, 1453.63/s  (0.697s, 1468.74/s)  LR: 7.893e-05  Data: 0.011 (0.013)
Train: 498 [ 800/1251 ( 64%)]  Loss: 2.994 (3.26)  Time: 0.666s, 1536.89/s  (0.697s, 1468.96/s)  LR: 7.893e-05  Data: 0.010 (0.013)
Train: 498 [ 850/1251 ( 68%)]  Loss: 3.221 (3.25)  Time: 0.671s, 1526.67/s  (0.697s, 1470.16/s)  LR: 7.893e-05  Data: 0.011 (0.013)
Train: 498 [ 900/1251 ( 72%)]  Loss: 3.380 (3.26)  Time: 0.666s, 1537.43/s  (0.696s, 1470.70/s)  LR: 7.893e-05  Data: 0.011 (0.013)
Train: 498 [ 950/1251 ( 76%)]  Loss: 3.266 (3.26)  Time: 0.689s, 1486.17/s  (0.696s, 1470.89/s)  LR: 7.893e-05  Data: 0.013 (0.013)
Train: 498 [1000/1251 ( 80%)]  Loss: 2.715 (3.23)  Time: 0.682s, 1500.86/s  (0.696s, 1471.56/s)  LR: 7.893e-05  Data: 0.014 (0.013)
Train: 498 [1050/1251 ( 84%)]  Loss: 3.551 (3.25)  Time: 0.673s, 1520.94/s  (0.696s, 1471.88/s)  LR: 7.893e-05  Data: 0.010 (0.012)
Train: 498 [1100/1251 ( 88%)]  Loss: 3.177 (3.25)  Time: 0.679s, 1507.99/s  (0.695s, 1472.67/s)  LR: 7.893e-05  Data: 0.011 (0.012)
Train: 498 [1150/1251 ( 92%)]  Loss: 3.419 (3.25)  Time: 0.679s, 1509.05/s  (0.695s, 1473.10/s)  LR: 7.893e-05  Data: 0.013 (0.012)
Train: 498 [1200/1251 ( 96%)]  Loss: 3.725 (3.27)  Time: 0.714s, 1434.17/s  (0.695s, 1473.19/s)  LR: 7.893e-05  Data: 0.010 (0.012)
Train: 498 [1250/1251 (100%)]  Loss: 3.282 (3.27)  Time: 0.661s, 1549.90/s  (0.695s, 1474.11/s)  LR: 7.893e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.503 (1.503)  Loss:  0.7007 (0.7007)  Acc@1: 91.5039 (91.5039)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.136 (0.575)  Loss:  0.8145 (1.2111)  Acc@1: 86.4387 (78.1160)  Acc@5: 97.1698 (93.9660)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-498.pth.tar', 78.11600005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-495.pth.tar', 78.11200006103516)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-490.pth.tar', 78.09000000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-494.pth.tar', 78.04200015869141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-493.pth.tar', 78.02600008300782)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-496.pth.tar', 78.00599989990235)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-492.pth.tar', 77.9820001586914)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-491.pth.tar', 77.91799998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-486.pth.tar', 77.89599995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-497.pth.tar', 77.89199990234376)

Train: 499 [   0/1251 (  0%)]  Loss: 3.468 (3.47)  Time: 2.168s,  472.31/s  (2.168s,  472.31/s)  LR: 7.762e-05  Data: 1.520 (1.520)
Train: 499 [  50/1251 (  4%)]  Loss: 3.146 (3.31)  Time: 0.691s, 1482.05/s  (0.726s, 1410.09/s)  LR: 7.762e-05  Data: 0.012 (0.047)
Train: 499 [ 100/1251 (  8%)]  Loss: 3.475 (3.36)  Time: 0.673s, 1521.86/s  (0.709s, 1443.35/s)  LR: 7.762e-05  Data: 0.012 (0.029)
Train: 499 [ 150/1251 ( 12%)]  Loss: 3.410 (3.37)  Time: 0.678s, 1510.30/s  (0.704s, 1455.38/s)  LR: 7.762e-05  Data: 0.010 (0.023)
Train: 499 [ 200/1251 ( 16%)]  Loss: 3.093 (3.32)  Time: 0.700s, 1462.76/s  (0.703s, 1456.88/s)  LR: 7.762e-05  Data: 0.009 (0.020)
Train: 499 [ 250/1251 ( 20%)]  Loss: 3.343 (3.32)  Time: 0.676s, 1514.08/s  (0.703s, 1457.55/s)  LR: 7.762e-05  Data: 0.010 (0.018)
Train: 499 [ 300/1251 ( 24%)]  Loss: 3.002 (3.28)  Time: 0.709s, 1444.11/s  (0.700s, 1463.34/s)  LR: 7.762e-05  Data: 0.012 (0.017)
Train: 499 [ 350/1251 ( 28%)]  Loss: 3.038 (3.25)  Time: 0.695s, 1473.93/s  (0.699s, 1464.77/s)  LR: 7.762e-05  Data: 0.011 (0.016)
Train: 499 [ 400/1251 ( 32%)]  Loss: 3.162 (3.24)  Time: 0.672s, 1524.18/s  (0.698s, 1467.47/s)  LR: 7.762e-05  Data: 0.011 (0.015)
Train: 499 [ 450/1251 ( 36%)]  Loss: 3.209 (3.23)  Time: 0.763s, 1341.78/s  (0.697s, 1468.86/s)  LR: 7.762e-05  Data: 0.009 (0.015)
Train: 499 [ 500/1251 ( 40%)]  Loss: 3.281 (3.24)  Time: 0.701s, 1461.58/s  (0.697s, 1469.87/s)  LR: 7.762e-05  Data: 0.010 (0.014)
Train: 499 [ 550/1251 ( 44%)]  Loss: 3.199 (3.24)  Time: 0.701s, 1460.31/s  (0.696s, 1470.69/s)  LR: 7.762e-05  Data: 0.009 (0.014)
Train: 499 [ 600/1251 ( 48%)]  Loss: 3.229 (3.23)  Time: 0.672s, 1523.10/s  (0.696s, 1471.65/s)  LR: 7.762e-05  Data: 0.010 (0.014)
Train: 499 [ 650/1251 ( 52%)]  Loss: 3.130 (3.23)  Time: 0.672s, 1524.56/s  (0.695s, 1473.35/s)  LR: 7.762e-05  Data: 0.011 (0.013)
Train: 499 [ 700/1251 ( 56%)]  Loss: 3.120 (3.22)  Time: 0.671s, 1526.88/s  (0.695s, 1473.45/s)  LR: 7.762e-05  Data: 0.010 (0.013)
Train: 499 [ 750/1251 ( 60%)]  Loss: 3.473 (3.24)  Time: 0.762s, 1342.96/s  (0.695s, 1473.52/s)  LR: 7.762e-05  Data: 0.009 (0.013)
Train: 499 [ 800/1251 ( 64%)]  Loss: 3.304 (3.24)  Time: 0.697s, 1469.40/s  (0.694s, 1474.90/s)  LR: 7.762e-05  Data: 0.009 (0.013)
Train: 499 [ 850/1251 ( 68%)]  Loss: 3.124 (3.23)  Time: 0.691s, 1481.27/s  (0.694s, 1474.51/s)  LR: 7.762e-05  Data: 0.013 (0.013)
Train: 499 [ 900/1251 ( 72%)]  Loss: 3.294 (3.24)  Time: 0.702s, 1458.67/s  (0.694s, 1474.82/s)  LR: 7.762e-05  Data: 0.009 (0.013)
Train: 499 [ 950/1251 ( 76%)]  Loss: 3.217 (3.24)  Time: 0.705s, 1451.53/s  (0.694s, 1475.44/s)  LR: 7.762e-05  Data: 0.009 (0.012)
Train: 499 [1000/1251 ( 80%)]  Loss: 3.685 (3.26)  Time: 0.704s, 1454.64/s  (0.694s, 1476.02/s)  LR: 7.762e-05  Data: 0.009 (0.012)
Train: 499 [1050/1251 ( 84%)]  Loss: 2.965 (3.24)  Time: 0.673s, 1520.85/s  (0.694s, 1475.45/s)  LR: 7.762e-05  Data: 0.010 (0.012)
Train: 499 [1100/1251 ( 88%)]  Loss: 3.719 (3.26)  Time: 0.724s, 1414.93/s  (0.694s, 1476.08/s)  LR: 7.762e-05  Data: 0.009 (0.012)
Train: 499 [1150/1251 ( 92%)]  Loss: 3.143 (3.26)  Time: 0.722s, 1418.86/s  (0.694s, 1476.50/s)  LR: 7.762e-05  Data: 0.009 (0.012)
Train: 499 [1200/1251 ( 96%)]  Loss: 3.199 (3.26)  Time: 0.710s, 1441.83/s  (0.693s, 1476.70/s)  LR: 7.762e-05  Data: 0.010 (0.012)
Train: 499 [1250/1251 (100%)]  Loss: 3.291 (3.26)  Time: 0.691s, 1480.88/s  (0.693s, 1476.61/s)  LR: 7.762e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.571 (1.571)  Loss:  0.7202 (0.7202)  Acc@1: 91.0156 (91.0156)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.136 (0.584)  Loss:  0.8228 (1.2045)  Acc@1: 86.6745 (78.0560)  Acc@5: 96.9340 (93.8940)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-498.pth.tar', 78.11600005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-495.pth.tar', 78.11200006103516)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-490.pth.tar', 78.09000000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-499.pth.tar', 78.05600002929687)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-494.pth.tar', 78.04200015869141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-493.pth.tar', 78.02600008300782)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-496.pth.tar', 78.00599989990235)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-492.pth.tar', 77.9820001586914)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-491.pth.tar', 77.91799998046875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-486.pth.tar', 77.89599995361328)

Train: 500 [   0/1251 (  0%)]  Loss: 3.390 (3.39)  Time: 2.158s,  474.52/s  (2.158s,  474.52/s)  LR: 7.632e-05  Data: 1.542 (1.542)
Train: 500 [  50/1251 (  4%)]  Loss: 2.866 (3.13)  Time: 0.743s, 1379.02/s  (0.739s, 1386.58/s)  LR: 7.632e-05  Data: 0.013 (0.046)
Train: 500 [ 100/1251 (  8%)]  Loss: 3.465 (3.24)  Time: 0.671s, 1525.88/s  (0.715s, 1431.85/s)  LR: 7.632e-05  Data: 0.010 (0.028)
Train: 500 [ 150/1251 ( 12%)]  Loss: 3.303 (3.26)  Time: 0.689s, 1485.58/s  (0.705s, 1451.57/s)  LR: 7.632e-05  Data: 0.010 (0.022)
Train: 500 [ 200/1251 ( 16%)]  Loss: 3.444 (3.29)  Time: 0.702s, 1458.35/s  (0.703s, 1456.73/s)  LR: 7.632e-05  Data: 0.011 (0.019)
Train: 500 [ 250/1251 ( 20%)]  Loss: 3.494 (3.33)  Time: 0.688s, 1487.85/s  (0.702s, 1458.90/s)  LR: 7.632e-05  Data: 0.011 (0.018)
Train: 500 [ 300/1251 ( 24%)]  Loss: 3.069 (3.29)  Time: 0.721s, 1420.90/s  (0.701s, 1461.49/s)  LR: 7.632e-05  Data: 0.009 (0.017)
Train: 500 [ 350/1251 ( 28%)]  Loss: 3.072 (3.26)  Time: 0.673s, 1522.15/s  (0.699s, 1464.76/s)  LR: 7.632e-05  Data: 0.011 (0.016)
Train: 500 [ 400/1251 ( 32%)]  Loss: 3.333 (3.27)  Time: 0.667s, 1534.33/s  (0.698s, 1467.09/s)  LR: 7.632e-05  Data: 0.011 (0.015)
Train: 500 [ 450/1251 ( 36%)]  Loss: 3.114 (3.26)  Time: 0.693s, 1478.57/s  (0.697s, 1468.58/s)  LR: 7.632e-05  Data: 0.009 (0.015)
Train: 500 [ 500/1251 ( 40%)]  Loss: 3.308 (3.26)  Time: 0.670s, 1528.45/s  (0.696s, 1471.12/s)  LR: 7.632e-05  Data: 0.010 (0.014)
Train: 500 [ 550/1251 ( 44%)]  Loss: 3.268 (3.26)  Time: 0.674s, 1519.69/s  (0.696s, 1471.76/s)  LR: 7.632e-05  Data: 0.010 (0.014)
Train: 500 [ 600/1251 ( 48%)]  Loss: 3.307 (3.26)  Time: 0.713s, 1436.32/s  (0.695s, 1472.73/s)  LR: 7.632e-05  Data: 0.011 (0.014)
Train: 500 [ 650/1251 ( 52%)]  Loss: 3.481 (3.28)  Time: 0.679s, 1508.53/s  (0.695s, 1472.67/s)  LR: 7.632e-05  Data: 0.011 (0.013)
Train: 500 [ 700/1251 ( 56%)]  Loss: 2.978 (3.26)  Time: 0.664s, 1541.90/s  (0.695s, 1473.37/s)  LR: 7.632e-05  Data: 0.008 (0.013)
Train: 500 [ 750/1251 ( 60%)]  Loss: 3.408 (3.27)  Time: 0.676s, 1514.58/s  (0.695s, 1474.19/s)  LR: 7.632e-05  Data: 0.014 (0.013)
Train: 500 [ 800/1251 ( 64%)]  Loss: 3.132 (3.26)  Time: 0.674s, 1518.54/s  (0.695s, 1474.24/s)  LR: 7.632e-05  Data: 0.011 (0.013)
Train: 500 [ 850/1251 ( 68%)]  Loss: 3.349 (3.27)  Time: 0.722s, 1418.61/s  (0.694s, 1475.04/s)  LR: 7.632e-05  Data: 0.009 (0.013)
Train: 500 [ 900/1251 ( 72%)]  Loss: 3.101 (3.26)  Time: 0.689s, 1485.50/s  (0.694s, 1475.34/s)  LR: 7.632e-05  Data: 0.011 (0.012)
Train: 500 [ 950/1251 ( 76%)]  Loss: 3.568 (3.27)  Time: 0.706s, 1451.20/s  (0.694s, 1475.47/s)  LR: 7.632e-05  Data: 0.010 (0.012)
Train: 500 [1000/1251 ( 80%)]  Loss: 3.366 (3.28)  Time: 0.724s, 1415.00/s  (0.694s, 1474.99/s)  LR: 7.632e-05  Data: 0.011 (0.012)
Train: 500 [1050/1251 ( 84%)]  Loss: 3.443 (3.28)  Time: 0.672s, 1523.94/s  (0.694s, 1475.60/s)  LR: 7.632e-05  Data: 0.010 (0.012)
Train: 500 [1100/1251 ( 88%)]  Loss: 3.654 (3.30)  Time: 0.688s, 1488.86/s  (0.694s, 1476.03/s)  LR: 7.632e-05  Data: 0.009 (0.012)
Train: 500 [1150/1251 ( 92%)]  Loss: 3.474 (3.31)  Time: 0.672s, 1523.18/s  (0.694s, 1476.53/s)  LR: 7.632e-05  Data: 0.011 (0.012)
Train: 500 [1200/1251 ( 96%)]  Loss: 3.373 (3.31)  Time: 0.705s, 1451.94/s  (0.694s, 1476.47/s)  LR: 7.632e-05  Data: 0.011 (0.012)
Train: 500 [1250/1251 (100%)]  Loss: 3.455 (3.32)  Time: 0.660s, 1551.92/s  (0.693s, 1477.15/s)  LR: 7.632e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.517 (1.517)  Loss:  0.7495 (0.7495)  Acc@1: 91.2109 (91.2109)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.136 (0.572)  Loss:  0.8989 (1.2714)  Acc@1: 86.3208 (77.8000)  Acc@5: 96.6981 (93.8200)
Train: 501 [   0/1251 (  0%)]  Loss: 3.240 (3.24)  Time: 2.133s,  480.07/s  (2.133s,  480.07/s)  LR: 7.503e-05  Data: 1.466 (1.466)
Train: 501 [  50/1251 (  4%)]  Loss: 3.163 (3.20)  Time: 0.689s, 1486.64/s  (0.726s, 1410.80/s)  LR: 7.503e-05  Data: 0.010 (0.050)
Train: 501 [ 100/1251 (  8%)]  Loss: 3.194 (3.20)  Time: 0.701s, 1460.30/s  (0.708s, 1446.83/s)  LR: 7.503e-05  Data: 0.009 (0.030)
Train: 501 [ 150/1251 ( 12%)]  Loss: 3.334 (3.23)  Time: 0.668s, 1533.04/s  (0.703s, 1457.21/s)  LR: 7.503e-05  Data: 0.010 (0.024)
Train: 501 [ 200/1251 ( 16%)]  Loss: 3.541 (3.29)  Time: 0.686s, 1493.35/s  (0.699s, 1464.65/s)  LR: 7.503e-05  Data: 0.009 (0.020)
Train: 501 [ 250/1251 ( 20%)]  Loss: 3.206 (3.28)  Time: 0.680s, 1505.04/s  (0.699s, 1465.30/s)  LR: 7.503e-05  Data: 0.010 (0.019)
Train: 501 [ 300/1251 ( 24%)]  Loss: 3.311 (3.28)  Time: 0.672s, 1523.12/s  (0.699s, 1464.85/s)  LR: 7.503e-05  Data: 0.011 (0.017)
Train: 501 [ 350/1251 ( 28%)]  Loss: 3.159 (3.27)  Time: 0.674s, 1518.68/s  (0.698s, 1466.98/s)  LR: 7.503e-05  Data: 0.009 (0.016)
Train: 501 [ 400/1251 ( 32%)]  Loss: 3.291 (3.27)  Time: 0.671s, 1527.01/s  (0.697s, 1469.04/s)  LR: 7.503e-05  Data: 0.010 (0.016)
Train: 501 [ 450/1251 ( 36%)]  Loss: 3.081 (3.25)  Time: 0.673s, 1520.85/s  (0.696s, 1470.85/s)  LR: 7.503e-05  Data: 0.010 (0.015)
Train: 501 [ 500/1251 ( 40%)]  Loss: 3.398 (3.27)  Time: 0.707s, 1448.95/s  (0.697s, 1469.88/s)  LR: 7.503e-05  Data: 0.011 (0.014)
Train: 501 [ 550/1251 ( 44%)]  Loss: 3.372 (3.27)  Time: 0.699s, 1465.54/s  (0.697s, 1470.16/s)  LR: 7.503e-05  Data: 0.009 (0.014)
Train: 501 [ 600/1251 ( 48%)]  Loss: 3.486 (3.29)  Time: 0.677s, 1512.82/s  (0.696s, 1471.41/s)  LR: 7.503e-05  Data: 0.009 (0.014)
Train: 501 [ 650/1251 ( 52%)]  Loss: 3.380 (3.30)  Time: 0.672s, 1524.47/s  (0.695s, 1472.44/s)  LR: 7.503e-05  Data: 0.009 (0.013)
Train: 501 [ 700/1251 ( 56%)]  Loss: 3.372 (3.30)  Time: 0.674s, 1519.48/s  (0.695s, 1474.31/s)  LR: 7.503e-05  Data: 0.011 (0.013)
Train: 501 [ 750/1251 ( 60%)]  Loss: 3.407 (3.31)  Time: 0.705s, 1451.96/s  (0.694s, 1474.76/s)  LR: 7.503e-05  Data: 0.009 (0.013)
Train: 501 [ 800/1251 ( 64%)]  Loss: 2.979 (3.29)  Time: 0.699s, 1464.40/s  (0.694s, 1475.71/s)  LR: 7.503e-05  Data: 0.009 (0.013)
Train: 501 [ 850/1251 ( 68%)]  Loss: 3.609 (3.31)  Time: 0.703s, 1457.22/s  (0.694s, 1475.85/s)  LR: 7.503e-05  Data: 0.010 (0.013)
Train: 501 [ 900/1251 ( 72%)]  Loss: 3.199 (3.30)  Time: 0.704s, 1455.23/s  (0.694s, 1476.23/s)  LR: 7.503e-05  Data: 0.009 (0.013)
Train: 501 [ 950/1251 ( 76%)]  Loss: 3.249 (3.30)  Time: 0.671s, 1526.58/s  (0.694s, 1475.52/s)  LR: 7.503e-05  Data: 0.010 (0.013)
Train: 501 [1000/1251 ( 80%)]  Loss: 3.081 (3.29)  Time: 0.671s, 1527.21/s  (0.694s, 1475.46/s)  LR: 7.503e-05  Data: 0.010 (0.012)
Train: 501 [1050/1251 ( 84%)]  Loss: 3.251 (3.29)  Time: 0.677s, 1513.57/s  (0.694s, 1475.50/s)  LR: 7.503e-05  Data: 0.016 (0.012)
Train: 501 [1100/1251 ( 88%)]  Loss: 3.211 (3.28)  Time: 0.722s, 1418.04/s  (0.694s, 1476.05/s)  LR: 7.503e-05  Data: 0.010 (0.012)
Train: 501 [1150/1251 ( 92%)]  Loss: 3.301 (3.28)  Time: 0.692s, 1479.80/s  (0.693s, 1476.67/s)  LR: 7.503e-05  Data: 0.013 (0.012)
Train: 501 [1200/1251 ( 96%)]  Loss: 3.431 (3.29)  Time: 0.727s, 1408.56/s  (0.694s, 1476.17/s)  LR: 7.503e-05  Data: 0.009 (0.012)
Train: 501 [1250/1251 (100%)]  Loss: 3.377 (3.29)  Time: 0.657s, 1559.42/s  (0.694s, 1476.02/s)  LR: 7.503e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.504 (1.504)  Loss:  0.7490 (0.7490)  Acc@1: 91.6016 (91.6016)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.136 (0.577)  Loss:  0.9072 (1.2530)  Acc@1: 86.0849 (77.9960)  Acc@5: 97.0519 (93.8060)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-498.pth.tar', 78.11600005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-495.pth.tar', 78.11200006103516)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-490.pth.tar', 78.09000000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-499.pth.tar', 78.05600002929687)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-494.pth.tar', 78.04200015869141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-493.pth.tar', 78.02600008300782)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-496.pth.tar', 78.00599989990235)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-501.pth.tar', 77.99600003173828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-492.pth.tar', 77.9820001586914)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-491.pth.tar', 77.91799998046875)

Train: 502 [   0/1251 (  0%)]  Loss: 3.655 (3.66)  Time: 2.122s,  482.64/s  (2.122s,  482.64/s)  LR: 7.375e-05  Data: 1.505 (1.505)
Train: 502 [  50/1251 (  4%)]  Loss: 3.357 (3.51)  Time: 0.674s, 1519.50/s  (0.732s, 1399.62/s)  LR: 7.375e-05  Data: 0.012 (0.050)
Train: 502 [ 100/1251 (  8%)]  Loss: 3.313 (3.44)  Time: 0.729s, 1404.56/s  (0.712s, 1439.09/s)  LR: 7.375e-05  Data: 0.009 (0.030)
Train: 502 [ 150/1251 ( 12%)]  Loss: 3.418 (3.44)  Time: 0.712s, 1437.39/s  (0.702s, 1458.14/s)  LR: 7.375e-05  Data: 0.010 (0.024)
Train: 502 [ 200/1251 ( 16%)]  Loss: 3.474 (3.44)  Time: 0.714s, 1434.10/s  (0.699s, 1464.36/s)  LR: 7.375e-05  Data: 0.009 (0.020)
Train: 502 [ 250/1251 ( 20%)]  Loss: 2.959 (3.36)  Time: 0.674s, 1518.19/s  (0.699s, 1465.70/s)  LR: 7.375e-05  Data: 0.010 (0.018)
Train: 502 [ 300/1251 ( 24%)]  Loss: 3.422 (3.37)  Time: 0.722s, 1417.56/s  (0.698s, 1466.78/s)  LR: 7.375e-05  Data: 0.009 (0.017)
Train: 502 [ 350/1251 ( 28%)]  Loss: 3.370 (3.37)  Time: 0.678s, 1509.71/s  (0.698s, 1467.40/s)  LR: 7.375e-05  Data: 0.015 (0.016)
Train: 502 [ 400/1251 ( 32%)]  Loss: 3.036 (3.33)  Time: 0.668s, 1532.20/s  (0.697s, 1469.91/s)  LR: 7.375e-05  Data: 0.010 (0.015)
Train: 502 [ 450/1251 ( 36%)]  Loss: 3.410 (3.34)  Time: 0.679s, 1507.67/s  (0.697s, 1469.78/s)  LR: 7.375e-05  Data: 0.011 (0.015)
Train: 502 [ 500/1251 ( 40%)]  Loss: 2.808 (3.29)  Time: 0.672s, 1524.11/s  (0.695s, 1473.14/s)  LR: 7.375e-05  Data: 0.010 (0.014)
Train: 502 [ 550/1251 ( 44%)]  Loss: 3.442 (3.31)  Time: 0.672s, 1524.65/s  (0.695s, 1472.97/s)  LR: 7.375e-05  Data: 0.010 (0.014)
Train: 502 [ 600/1251 ( 48%)]  Loss: 3.098 (3.29)  Time: 0.672s, 1522.81/s  (0.694s, 1474.66/s)  LR: 7.375e-05  Data: 0.011 (0.014)
Train: 502 [ 650/1251 ( 52%)]  Loss: 3.497 (3.30)  Time: 0.693s, 1478.62/s  (0.694s, 1475.22/s)  LR: 7.375e-05  Data: 0.011 (0.014)
Train: 502 [ 700/1251 ( 56%)]  Loss: 3.315 (3.30)  Time: 0.671s, 1525.37/s  (0.694s, 1475.20/s)  LR: 7.375e-05  Data: 0.011 (0.013)
Train: 502 [ 750/1251 ( 60%)]  Loss: 3.124 (3.29)  Time: 0.676s, 1514.21/s  (0.694s, 1475.25/s)  LR: 7.375e-05  Data: 0.011 (0.013)
Train: 502 [ 800/1251 ( 64%)]  Loss: 3.514 (3.31)  Time: 0.672s, 1524.75/s  (0.694s, 1476.14/s)  LR: 7.375e-05  Data: 0.010 (0.013)
Train: 502 [ 850/1251 ( 68%)]  Loss: 3.026 (3.29)  Time: 0.731s, 1401.48/s  (0.693s, 1476.80/s)  LR: 7.375e-05  Data: 0.011 (0.013)
Train: 502 [ 900/1251 ( 72%)]  Loss: 3.606 (3.31)  Time: 0.671s, 1525.94/s  (0.693s, 1477.21/s)  LR: 7.375e-05  Data: 0.011 (0.013)
Train: 502 [ 950/1251 ( 76%)]  Loss: 3.104 (3.30)  Time: 0.775s, 1321.51/s  (0.693s, 1476.69/s)  LR: 7.375e-05  Data: 0.010 (0.013)
Train: 502 [1000/1251 ( 80%)]  Loss: 3.425 (3.30)  Time: 0.702s, 1458.86/s  (0.694s, 1476.04/s)  LR: 7.375e-05  Data: 0.010 (0.012)
Train: 502 [1050/1251 ( 84%)]  Loss: 3.534 (3.31)  Time: 0.701s, 1459.90/s  (0.694s, 1475.42/s)  LR: 7.375e-05  Data: 0.009 (0.012)
Train: 502 [1100/1251 ( 88%)]  Loss: 3.318 (3.31)  Time: 0.704s, 1454.02/s  (0.694s, 1475.34/s)  LR: 7.375e-05  Data: 0.010 (0.012)
Train: 502 [1150/1251 ( 92%)]  Loss: 3.158 (3.31)  Time: 0.679s, 1507.74/s  (0.694s, 1475.64/s)  LR: 7.375e-05  Data: 0.013 (0.012)
Train: 502 [1200/1251 ( 96%)]  Loss: 3.112 (3.30)  Time: 0.672s, 1523.68/s  (0.694s, 1475.51/s)  LR: 7.375e-05  Data: 0.010 (0.012)
Train: 502 [1250/1251 (100%)]  Loss: 3.263 (3.30)  Time: 0.690s, 1483.14/s  (0.694s, 1475.85/s)  LR: 7.375e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.521 (1.521)  Loss:  0.6348 (0.6348)  Acc@1: 91.7969 (91.7969)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.136 (0.573)  Loss:  0.7441 (1.1651)  Acc@1: 86.5566 (78.1700)  Acc@5: 97.1698 (93.9540)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-502.pth.tar', 78.16999997802735)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-498.pth.tar', 78.11600005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-495.pth.tar', 78.11200006103516)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-490.pth.tar', 78.09000000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-499.pth.tar', 78.05600002929687)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-494.pth.tar', 78.04200015869141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-493.pth.tar', 78.02600008300782)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-496.pth.tar', 78.00599989990235)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-501.pth.tar', 77.99600003173828)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-492.pth.tar', 77.9820001586914)

Train: 503 [   0/1251 (  0%)]  Loss: 2.836 (2.84)  Time: 2.244s,  456.37/s  (2.244s,  456.37/s)  LR: 7.248e-05  Data: 1.589 (1.589)
Train: 503 [  50/1251 (  4%)]  Loss: 3.296 (3.07)  Time: 0.686s, 1493.70/s  (0.727s, 1408.55/s)  LR: 7.248e-05  Data: 0.013 (0.046)
Train: 503 [ 100/1251 (  8%)]  Loss: 3.165 (3.10)  Time: 0.702s, 1459.41/s  (0.709s, 1444.16/s)  LR: 7.248e-05  Data: 0.010 (0.028)
Train: 503 [ 150/1251 ( 12%)]  Loss: 3.302 (3.15)  Time: 0.685s, 1495.18/s  (0.706s, 1451.26/s)  LR: 7.248e-05  Data: 0.009 (0.022)
Train: 503 [ 200/1251 ( 16%)]  Loss: 3.158 (3.15)  Time: 0.715s, 1431.66/s  (0.705s, 1452.96/s)  LR: 7.248e-05  Data: 0.010 (0.019)
Train: 503 [ 250/1251 ( 20%)]  Loss: 3.083 (3.14)  Time: 0.700s, 1462.39/s  (0.702s, 1458.24/s)  LR: 7.248e-05  Data: 0.010 (0.018)
Train: 503 [ 300/1251 ( 24%)]  Loss: 3.476 (3.19)  Time: 0.671s, 1526.98/s  (0.701s, 1461.73/s)  LR: 7.248e-05  Data: 0.011 (0.017)
Train: 503 [ 350/1251 ( 28%)]  Loss: 3.633 (3.24)  Time: 0.704s, 1454.69/s  (0.698s, 1466.46/s)  LR: 7.248e-05  Data: 0.009 (0.016)
Train: 503 [ 400/1251 ( 32%)]  Loss: 3.345 (3.25)  Time: 0.740s, 1383.63/s  (0.698s, 1468.03/s)  LR: 7.248e-05  Data: 0.010 (0.015)
Train: 503 [ 450/1251 ( 36%)]  Loss: 3.254 (3.25)  Time: 0.697s, 1469.71/s  (0.696s, 1470.93/s)  LR: 7.248e-05  Data: 0.010 (0.014)
Train: 503 [ 500/1251 ( 40%)]  Loss: 3.366 (3.26)  Time: 0.701s, 1460.64/s  (0.696s, 1471.15/s)  LR: 7.248e-05  Data: 0.010 (0.014)
Train: 503 [ 550/1251 ( 44%)]  Loss: 3.298 (3.27)  Time: 0.673s, 1522.03/s  (0.696s, 1470.73/s)  LR: 7.248e-05  Data: 0.010 (0.014)
Train: 503 [ 600/1251 ( 48%)]  Loss: 3.282 (3.27)  Time: 0.672s, 1524.11/s  (0.696s, 1472.01/s)  LR: 7.248e-05  Data: 0.010 (0.014)
Train: 503 [ 650/1251 ( 52%)]  Loss: 3.249 (3.27)  Time: 0.698s, 1467.44/s  (0.695s, 1472.45/s)  LR: 7.248e-05  Data: 0.010 (0.013)
Train: 503 [ 700/1251 ( 56%)]  Loss: 3.123 (3.26)  Time: 0.674s, 1520.24/s  (0.695s, 1474.02/s)  LR: 7.248e-05  Data: 0.010 (0.013)
Train: 503 [ 750/1251 ( 60%)]  Loss: 3.129 (3.25)  Time: 0.664s, 1541.81/s  (0.694s, 1474.81/s)  LR: 7.248e-05  Data: 0.009 (0.013)
Train: 503 [ 800/1251 ( 64%)]  Loss: 3.388 (3.26)  Time: 0.690s, 1484.70/s  (0.694s, 1475.03/s)  LR: 7.248e-05  Data: 0.011 (0.013)
Train: 503 [ 850/1251 ( 68%)]  Loss: 3.154 (3.25)  Time: 0.670s, 1527.91/s  (0.694s, 1474.71/s)  LR: 7.248e-05  Data: 0.010 (0.013)
Train: 503 [ 900/1251 ( 72%)]  Loss: 3.354 (3.26)  Time: 0.684s, 1496.71/s  (0.694s, 1474.96/s)  LR: 7.248e-05  Data: 0.011 (0.013)
Train: 503 [ 950/1251 ( 76%)]  Loss: 3.362 (3.26)  Time: 0.702s, 1459.29/s  (0.695s, 1474.15/s)  LR: 7.248e-05  Data: 0.009 (0.012)
Train: 503 [1000/1251 ( 80%)]  Loss: 3.414 (3.27)  Time: 0.681s, 1503.68/s  (0.694s, 1475.08/s)  LR: 7.248e-05  Data: 0.010 (0.012)
Train: 503 [1050/1251 ( 84%)]  Loss: 3.294 (3.27)  Time: 0.675s, 1517.81/s  (0.694s, 1475.45/s)  LR: 7.248e-05  Data: 0.010 (0.012)
Train: 503 [1100/1251 ( 88%)]  Loss: 3.422 (3.28)  Time: 0.714s, 1434.77/s  (0.694s, 1475.60/s)  LR: 7.248e-05  Data: 0.010 (0.012)
Train: 503 [1150/1251 ( 92%)]  Loss: 2.870 (3.26)  Time: 0.671s, 1525.58/s  (0.694s, 1475.35/s)  LR: 7.248e-05  Data: 0.012 (0.012)
Train: 503 [1200/1251 ( 96%)]  Loss: 3.040 (3.25)  Time: 0.672s, 1524.05/s  (0.694s, 1475.12/s)  LR: 7.248e-05  Data: 0.013 (0.012)
Train: 503 [1250/1251 (100%)]  Loss: 3.379 (3.26)  Time: 0.659s, 1554.50/s  (0.694s, 1475.49/s)  LR: 7.248e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.479 (1.479)  Loss:  0.8159 (0.8159)  Acc@1: 91.6016 (91.6016)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.136 (0.584)  Loss:  0.9233 (1.3187)  Acc@1: 86.4387 (77.7780)  Acc@5: 97.0519 (93.8140)
Train: 504 [   0/1251 (  0%)]  Loss: 3.526 (3.53)  Time: 2.241s,  457.03/s  (2.241s,  457.03/s)  LR: 7.123e-05  Data: 1.622 (1.622)
Train: 504 [  50/1251 (  4%)]  Loss: 3.391 (3.46)  Time: 0.674s, 1518.50/s  (0.728s, 1406.96/s)  LR: 7.123e-05  Data: 0.010 (0.049)
Train: 504 [ 100/1251 (  8%)]  Loss: 3.374 (3.43)  Time: 0.704s, 1455.02/s  (0.711s, 1439.56/s)  LR: 7.123e-05  Data: 0.009 (0.030)
Train: 504 [ 150/1251 ( 12%)]  Loss: 3.101 (3.35)  Time: 0.670s, 1528.62/s  (0.702s, 1459.28/s)  LR: 7.123e-05  Data: 0.010 (0.023)
Train: 504 [ 200/1251 ( 16%)]  Loss: 3.037 (3.29)  Time: 0.702s, 1458.30/s  (0.700s, 1463.47/s)  LR: 7.123e-05  Data: 0.015 (0.020)
Train: 504 [ 250/1251 ( 20%)]  Loss: 3.008 (3.24)  Time: 0.668s, 1533.49/s  (0.698s, 1466.30/s)  LR: 7.123e-05  Data: 0.010 (0.018)
Train: 504 [ 300/1251 ( 24%)]  Loss: 3.443 (3.27)  Time: 0.672s, 1523.65/s  (0.697s, 1468.67/s)  LR: 7.123e-05  Data: 0.011 (0.017)
Train: 504 [ 350/1251 ( 28%)]  Loss: 3.334 (3.28)  Time: 0.709s, 1444.58/s  (0.697s, 1469.72/s)  LR: 7.123e-05  Data: 0.011 (0.016)
Train: 504 [ 400/1251 ( 32%)]  Loss: 3.433 (3.29)  Time: 0.706s, 1449.81/s  (0.695s, 1472.44/s)  LR: 7.123e-05  Data: 0.010 (0.015)
Train: 504 [ 450/1251 ( 36%)]  Loss: 3.468 (3.31)  Time: 0.700s, 1463.47/s  (0.696s, 1471.78/s)  LR: 7.123e-05  Data: 0.010 (0.015)
Train: 504 [ 500/1251 ( 40%)]  Loss: 3.315 (3.31)  Time: 0.682s, 1501.71/s  (0.695s, 1473.73/s)  LR: 7.123e-05  Data: 0.010 (0.014)
Train: 504 [ 550/1251 ( 44%)]  Loss: 3.407 (3.32)  Time: 0.724s, 1413.57/s  (0.695s, 1474.27/s)  LR: 7.123e-05  Data: 0.008 (0.014)
Train: 504 [ 600/1251 ( 48%)]  Loss: 3.150 (3.31)  Time: 0.677s, 1511.73/s  (0.694s, 1475.08/s)  LR: 7.123e-05  Data: 0.011 (0.014)
Train: 504 [ 650/1251 ( 52%)]  Loss: 3.115 (3.29)  Time: 0.672s, 1522.71/s  (0.694s, 1475.70/s)  LR: 7.123e-05  Data: 0.009 (0.013)
Train: 504 [ 700/1251 ( 56%)]  Loss: 3.445 (3.30)  Time: 0.703s, 1456.49/s  (0.694s, 1475.80/s)  LR: 7.123e-05  Data: 0.009 (0.013)
Train: 504 [ 750/1251 ( 60%)]  Loss: 3.292 (3.30)  Time: 0.719s, 1424.78/s  (0.693s, 1476.71/s)  LR: 7.123e-05  Data: 0.012 (0.013)
Train: 504 [ 800/1251 ( 64%)]  Loss: 2.890 (3.28)  Time: 0.671s, 1527.02/s  (0.693s, 1477.34/s)  LR: 7.123e-05  Data: 0.011 (0.013)
Train: 504 [ 850/1251 ( 68%)]  Loss: 3.396 (3.28)  Time: 0.708s, 1446.61/s  (0.693s, 1477.83/s)  LR: 7.123e-05  Data: 0.011 (0.013)
Train: 504 [ 900/1251 ( 72%)]  Loss: 2.881 (3.26)  Time: 0.755s, 1357.03/s  (0.693s, 1476.74/s)  LR: 7.123e-05  Data: 0.009 (0.013)
Train: 504 [ 950/1251 ( 76%)]  Loss: 3.116 (3.26)  Time: 0.681s, 1503.35/s  (0.693s, 1477.50/s)  LR: 7.123e-05  Data: 0.012 (0.012)
Train: 504 [1000/1251 ( 80%)]  Loss: 3.148 (3.25)  Time: 0.671s, 1526.72/s  (0.693s, 1478.31/s)  LR: 7.123e-05  Data: 0.010 (0.012)
Train: 504 [1050/1251 ( 84%)]  Loss: 3.527 (3.26)  Time: 0.720s, 1421.27/s  (0.692s, 1478.76/s)  LR: 7.123e-05  Data: 0.010 (0.012)
Train: 504 [1100/1251 ( 88%)]  Loss: 3.142 (3.26)  Time: 0.673s, 1521.61/s  (0.692s, 1478.93/s)  LR: 7.123e-05  Data: 0.010 (0.012)
Train: 504 [1150/1251 ( 92%)]  Loss: 3.060 (3.25)  Time: 0.698s, 1467.37/s  (0.692s, 1478.84/s)  LR: 7.123e-05  Data: 0.010 (0.012)
Train: 504 [1200/1251 ( 96%)]  Loss: 3.368 (3.25)  Time: 0.671s, 1526.16/s  (0.692s, 1478.71/s)  LR: 7.123e-05  Data: 0.011 (0.012)
Train: 504 [1250/1251 (100%)]  Loss: 3.119 (3.25)  Time: 0.725s, 1411.51/s  (0.693s, 1478.43/s)  LR: 7.123e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.526 (1.526)  Loss:  0.7119 (0.7119)  Acc@1: 91.6992 (91.6992)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.136 (0.574)  Loss:  0.8379 (1.2018)  Acc@1: 86.2028 (78.2080)  Acc@5: 97.2877 (93.9480)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-504.pth.tar', 78.20800008300782)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-502.pth.tar', 78.16999997802735)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-498.pth.tar', 78.11600005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-495.pth.tar', 78.11200006103516)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-490.pth.tar', 78.09000000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-499.pth.tar', 78.05600002929687)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-494.pth.tar', 78.04200015869141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-493.pth.tar', 78.02600008300782)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-496.pth.tar', 78.00599989990235)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-501.pth.tar', 77.99600003173828)

Train: 505 [   0/1251 (  0%)]  Loss: 3.057 (3.06)  Time: 2.205s,  464.45/s  (2.205s,  464.45/s)  LR: 6.999e-05  Data: 1.584 (1.584)
Train: 505 [  50/1251 (  4%)]  Loss: 3.270 (3.16)  Time: 0.680s, 1505.18/s  (0.731s, 1400.91/s)  LR: 6.999e-05  Data: 0.009 (0.052)
Train: 505 [ 100/1251 (  8%)]  Loss: 3.646 (3.32)  Time: 0.692s, 1478.98/s  (0.712s, 1438.00/s)  LR: 6.999e-05  Data: 0.009 (0.031)
Train: 505 [ 150/1251 ( 12%)]  Loss: 3.380 (3.34)  Time: 0.676s, 1514.56/s  (0.708s, 1446.51/s)  LR: 6.999e-05  Data: 0.010 (0.025)
Train: 505 [ 200/1251 ( 16%)]  Loss: 3.379 (3.35)  Time: 0.671s, 1526.61/s  (0.703s, 1456.56/s)  LR: 6.999e-05  Data: 0.010 (0.021)
Train: 505 [ 250/1251 ( 20%)]  Loss: 2.845 (3.26)  Time: 0.671s, 1526.11/s  (0.702s, 1458.75/s)  LR: 6.999e-05  Data: 0.013 (0.019)
Train: 505 [ 300/1251 ( 24%)]  Loss: 2.873 (3.21)  Time: 0.669s, 1530.01/s  (0.700s, 1462.76/s)  LR: 6.999e-05  Data: 0.013 (0.018)
Train: 505 [ 350/1251 ( 28%)]  Loss: 3.795 (3.28)  Time: 0.725s, 1411.74/s  (0.699s, 1464.61/s)  LR: 6.999e-05  Data: 0.008 (0.017)
Train: 505 [ 400/1251 ( 32%)]  Loss: 3.077 (3.26)  Time: 0.700s, 1462.60/s  (0.698s, 1468.03/s)  LR: 6.999e-05  Data: 0.013 (0.016)
Train: 505 [ 450/1251 ( 36%)]  Loss: 3.039 (3.24)  Time: 0.672s, 1523.91/s  (0.697s, 1468.96/s)  LR: 6.999e-05  Data: 0.010 (0.015)
Train: 505 [ 500/1251 ( 40%)]  Loss: 3.338 (3.25)  Time: 0.671s, 1525.24/s  (0.696s, 1471.24/s)  LR: 6.999e-05  Data: 0.010 (0.015)
Train: 505 [ 550/1251 ( 44%)]  Loss: 3.476 (3.26)  Time: 0.671s, 1527.22/s  (0.695s, 1472.90/s)  LR: 6.999e-05  Data: 0.010 (0.014)
Train: 505 [ 600/1251 ( 48%)]  Loss: 3.234 (3.26)  Time: 0.770s, 1329.57/s  (0.695s, 1473.66/s)  LR: 6.999e-05  Data: 0.009 (0.014)
Train: 505 [ 650/1251 ( 52%)]  Loss: 3.380 (3.27)  Time: 0.672s, 1524.05/s  (0.694s, 1474.48/s)  LR: 6.999e-05  Data: 0.009 (0.014)
Train: 505 [ 700/1251 ( 56%)]  Loss: 3.340 (3.28)  Time: 0.714s, 1433.84/s  (0.694s, 1475.66/s)  LR: 6.999e-05  Data: 0.010 (0.013)
Train: 505 [ 750/1251 ( 60%)]  Loss: 3.268 (3.27)  Time: 0.673s, 1521.33/s  (0.694s, 1476.38/s)  LR: 6.999e-05  Data: 0.010 (0.013)
Train: 505 [ 800/1251 ( 64%)]  Loss: 3.194 (3.27)  Time: 0.741s, 1382.11/s  (0.694s, 1476.38/s)  LR: 6.999e-05  Data: 0.009 (0.013)
Train: 505 [ 850/1251 ( 68%)]  Loss: 3.547 (3.29)  Time: 0.671s, 1526.21/s  (0.694s, 1476.51/s)  LR: 6.999e-05  Data: 0.010 (0.013)
Train: 505 [ 900/1251 ( 72%)]  Loss: 3.093 (3.28)  Time: 0.672s, 1523.69/s  (0.693s, 1477.32/s)  LR: 6.999e-05  Data: 0.010 (0.013)
Train: 505 [ 950/1251 ( 76%)]  Loss: 3.146 (3.27)  Time: 0.672s, 1524.30/s  (0.693s, 1477.68/s)  LR: 6.999e-05  Data: 0.010 (0.013)
Train: 505 [1000/1251 ( 80%)]  Loss: 3.051 (3.26)  Time: 0.673s, 1521.45/s  (0.693s, 1478.03/s)  LR: 6.999e-05  Data: 0.011 (0.013)
Train: 505 [1050/1251 ( 84%)]  Loss: 3.143 (3.25)  Time: 0.668s, 1532.99/s  (0.692s, 1478.75/s)  LR: 6.999e-05  Data: 0.010 (0.012)
Train: 505 [1100/1251 ( 88%)]  Loss: 3.383 (3.26)  Time: 0.702s, 1458.39/s  (0.692s, 1479.18/s)  LR: 6.999e-05  Data: 0.009 (0.012)
Train: 505 [1150/1251 ( 92%)]  Loss: 2.953 (3.25)  Time: 0.675s, 1515.99/s  (0.692s, 1479.51/s)  LR: 6.999e-05  Data: 0.013 (0.012)
Train: 505 [1200/1251 ( 96%)]  Loss: 3.419 (3.25)  Time: 0.676s, 1514.48/s  (0.692s, 1479.08/s)  LR: 6.999e-05  Data: 0.010 (0.012)
Train: 505 [1250/1251 (100%)]  Loss: 3.284 (3.25)  Time: 0.687s, 1491.35/s  (0.692s, 1479.30/s)  LR: 6.999e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.587 (1.587)  Loss:  0.6953 (0.6953)  Acc@1: 91.0156 (91.0156)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.137 (0.581)  Loss:  0.7979 (1.1591)  Acc@1: 85.3774 (78.4160)  Acc@5: 97.2877 (93.9920)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-505.pth.tar', 78.41600011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-504.pth.tar', 78.20800008300782)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-502.pth.tar', 78.16999997802735)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-498.pth.tar', 78.11600005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-495.pth.tar', 78.11200006103516)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-490.pth.tar', 78.09000000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-499.pth.tar', 78.05600002929687)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-494.pth.tar', 78.04200015869141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-493.pth.tar', 78.02600008300782)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-496.pth.tar', 78.00599989990235)

Train: 506 [   0/1251 (  0%)]  Loss: 3.179 (3.18)  Time: 2.304s,  444.39/s  (2.304s,  444.39/s)  LR: 6.875e-05  Data: 1.638 (1.638)
Train: 506 [  50/1251 (  4%)]  Loss: 3.346 (3.26)  Time: 0.719s, 1424.49/s  (0.738s, 1387.08/s)  LR: 6.875e-05  Data: 0.010 (0.051)
Train: 506 [ 100/1251 (  8%)]  Loss: 3.339 (3.29)  Time: 0.691s, 1481.86/s  (0.718s, 1425.65/s)  LR: 6.875e-05  Data: 0.011 (0.031)
Train: 506 [ 150/1251 ( 12%)]  Loss: 2.987 (3.21)  Time: 0.675s, 1518.08/s  (0.711s, 1439.55/s)  LR: 6.875e-05  Data: 0.009 (0.024)
Train: 506 [ 200/1251 ( 16%)]  Loss: 3.269 (3.22)  Time: 0.701s, 1460.44/s  (0.707s, 1448.42/s)  LR: 6.875e-05  Data: 0.011 (0.021)
Train: 506 [ 250/1251 ( 20%)]  Loss: 3.263 (3.23)  Time: 0.667s, 1536.27/s  (0.703s, 1456.06/s)  LR: 6.875e-05  Data: 0.009 (0.019)
Train: 506 [ 300/1251 ( 24%)]  Loss: 3.057 (3.21)  Time: 0.671s, 1525.13/s  (0.701s, 1460.78/s)  LR: 6.875e-05  Data: 0.011 (0.017)
Train: 506 [ 350/1251 ( 28%)]  Loss: 3.411 (3.23)  Time: 0.704s, 1454.34/s  (0.700s, 1462.14/s)  LR: 6.875e-05  Data: 0.009 (0.016)
Train: 506 [ 400/1251 ( 32%)]  Loss: 3.572 (3.27)  Time: 0.685s, 1495.36/s  (0.698s, 1466.29/s)  LR: 6.875e-05  Data: 0.010 (0.016)
Train: 506 [ 450/1251 ( 36%)]  Loss: 3.502 (3.29)  Time: 0.672s, 1523.82/s  (0.697s, 1468.31/s)  LR: 6.875e-05  Data: 0.013 (0.015)
Train: 506 [ 500/1251 ( 40%)]  Loss: 3.268 (3.29)  Time: 0.720s, 1421.60/s  (0.697s, 1468.73/s)  LR: 6.875e-05  Data: 0.010 (0.015)
Train: 506 [ 550/1251 ( 44%)]  Loss: 3.541 (3.31)  Time: 0.674s, 1519.23/s  (0.697s, 1468.96/s)  LR: 6.875e-05  Data: 0.011 (0.014)
Train: 506 [ 600/1251 ( 48%)]  Loss: 3.244 (3.31)  Time: 0.723s, 1416.43/s  (0.697s, 1470.19/s)  LR: 6.875e-05  Data: 0.011 (0.014)
Train: 506 [ 650/1251 ( 52%)]  Loss: 3.061 (3.29)  Time: 0.726s, 1410.67/s  (0.697s, 1469.77/s)  LR: 6.875e-05  Data: 0.009 (0.014)
Train: 506 [ 700/1251 ( 56%)]  Loss: 3.267 (3.29)  Time: 0.694s, 1475.16/s  (0.696s, 1470.76/s)  LR: 6.875e-05  Data: 0.010 (0.013)
Train: 506 [ 750/1251 ( 60%)]  Loss: 3.541 (3.30)  Time: 0.681s, 1503.93/s  (0.696s, 1471.24/s)  LR: 6.875e-05  Data: 0.011 (0.013)
Train: 506 [ 800/1251 ( 64%)]  Loss: 3.311 (3.30)  Time: 0.675s, 1517.63/s  (0.696s, 1472.14/s)  LR: 6.875e-05  Data: 0.010 (0.013)
Train: 506 [ 850/1251 ( 68%)]  Loss: 3.311 (3.30)  Time: 0.680s, 1506.21/s  (0.695s, 1473.68/s)  LR: 6.875e-05  Data: 0.011 (0.013)
Train: 506 [ 900/1251 ( 72%)]  Loss: 3.367 (3.31)  Time: 0.670s, 1527.93/s  (0.694s, 1474.56/s)  LR: 6.875e-05  Data: 0.009 (0.013)
Train: 506 [ 950/1251 ( 76%)]  Loss: 3.208 (3.30)  Time: 0.718s, 1426.19/s  (0.694s, 1474.70/s)  LR: 6.875e-05  Data: 0.009 (0.013)
Train: 506 [1000/1251 ( 80%)]  Loss: 2.870 (3.28)  Time: 0.703s, 1455.90/s  (0.694s, 1474.50/s)  LR: 6.875e-05  Data: 0.009 (0.013)
Train: 506 [1050/1251 ( 84%)]  Loss: 3.371 (3.29)  Time: 0.674s, 1518.94/s  (0.695s, 1474.30/s)  LR: 6.875e-05  Data: 0.011 (0.012)
Train: 506 [1100/1251 ( 88%)]  Loss: 3.169 (3.28)  Time: 0.673s, 1521.88/s  (0.695s, 1473.84/s)  LR: 6.875e-05  Data: 0.011 (0.012)
Train: 506 [1150/1251 ( 92%)]  Loss: 3.329 (3.28)  Time: 0.673s, 1521.67/s  (0.694s, 1474.48/s)  LR: 6.875e-05  Data: 0.011 (0.012)
Train: 506 [1200/1251 ( 96%)]  Loss: 3.223 (3.28)  Time: 0.680s, 1506.83/s  (0.694s, 1475.27/s)  LR: 6.875e-05  Data: 0.010 (0.012)
Train: 506 [1250/1251 (100%)]  Loss: 3.307 (3.28)  Time: 0.665s, 1539.12/s  (0.694s, 1475.38/s)  LR: 6.875e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.597 (1.597)  Loss:  0.7310 (0.7310)  Acc@1: 91.9922 (91.9922)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.136 (0.574)  Loss:  0.8711 (1.2159)  Acc@1: 87.0283 (78.1920)  Acc@5: 96.9340 (94.0680)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-505.pth.tar', 78.41600011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-504.pth.tar', 78.20800008300782)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-506.pth.tar', 78.19200005371094)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-502.pth.tar', 78.16999997802735)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-498.pth.tar', 78.11600005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-495.pth.tar', 78.11200006103516)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-490.pth.tar', 78.09000000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-499.pth.tar', 78.05600002929687)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-494.pth.tar', 78.04200015869141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-493.pth.tar', 78.02600008300782)

Train: 507 [   0/1251 (  0%)]  Loss: 3.152 (3.15)  Time: 2.165s,  472.93/s  (2.165s,  472.93/s)  LR: 6.754e-05  Data: 1.550 (1.550)
Train: 507 [  50/1251 (  4%)]  Loss: 3.402 (3.28)  Time: 0.699s, 1464.21/s  (0.720s, 1421.73/s)  LR: 6.754e-05  Data: 0.019 (0.047)
Train: 507 [ 100/1251 (  8%)]  Loss: 3.667 (3.41)  Time: 0.665s, 1539.00/s  (0.711s, 1440.66/s)  LR: 6.754e-05  Data: 0.009 (0.029)
Train: 507 [ 150/1251 ( 12%)]  Loss: 3.392 (3.40)  Time: 0.680s, 1506.10/s  (0.704s, 1453.62/s)  LR: 6.754e-05  Data: 0.010 (0.023)
Train: 507 [ 200/1251 ( 16%)]  Loss: 3.405 (3.40)  Time: 0.673s, 1522.35/s  (0.702s, 1459.13/s)  LR: 6.754e-05  Data: 0.010 (0.020)
Train: 507 [ 250/1251 ( 20%)]  Loss: 3.568 (3.43)  Time: 0.671s, 1526.05/s  (0.699s, 1464.77/s)  LR: 6.754e-05  Data: 0.010 (0.018)
Train: 507 [ 300/1251 ( 24%)]  Loss: 3.313 (3.41)  Time: 0.672s, 1524.16/s  (0.698s, 1466.65/s)  LR: 6.754e-05  Data: 0.011 (0.017)
Train: 507 [ 350/1251 ( 28%)]  Loss: 3.312 (3.40)  Time: 0.674s, 1518.48/s  (0.697s, 1468.48/s)  LR: 6.754e-05  Data: 0.010 (0.016)
Train: 507 [ 400/1251 ( 32%)]  Loss: 3.516 (3.41)  Time: 0.701s, 1461.18/s  (0.696s, 1471.18/s)  LR: 6.754e-05  Data: 0.009 (0.015)
Train: 507 [ 450/1251 ( 36%)]  Loss: 3.150 (3.39)  Time: 0.758s, 1350.41/s  (0.697s, 1469.50/s)  LR: 6.754e-05  Data: 0.010 (0.014)
Train: 507 [ 500/1251 ( 40%)]  Loss: 3.235 (3.37)  Time: 0.671s, 1525.44/s  (0.696s, 1471.21/s)  LR: 6.754e-05  Data: 0.010 (0.014)
Train: 507 [ 550/1251 ( 44%)]  Loss: 3.478 (3.38)  Time: 0.673s, 1521.37/s  (0.696s, 1471.60/s)  LR: 6.754e-05  Data: 0.011 (0.014)
Train: 507 [ 600/1251 ( 48%)]  Loss: 3.566 (3.40)  Time: 0.667s, 1534.62/s  (0.695s, 1472.58/s)  LR: 6.754e-05  Data: 0.009 (0.013)
Train: 507 [ 650/1251 ( 52%)]  Loss: 3.381 (3.40)  Time: 0.667s, 1536.33/s  (0.696s, 1471.11/s)  LR: 6.754e-05  Data: 0.009 (0.013)
Train: 507 [ 700/1251 ( 56%)]  Loss: 3.288 (3.39)  Time: 0.670s, 1529.12/s  (0.695s, 1472.54/s)  LR: 6.754e-05  Data: 0.010 (0.013)
Train: 507 [ 750/1251 ( 60%)]  Loss: 3.094 (3.37)  Time: 0.688s, 1488.00/s  (0.694s, 1474.52/s)  LR: 6.754e-05  Data: 0.010 (0.013)
Train: 507 [ 800/1251 ( 64%)]  Loss: 3.263 (3.36)  Time: 0.671s, 1527.17/s  (0.694s, 1475.38/s)  LR: 6.754e-05  Data: 0.010 (0.013)
Train: 507 [ 850/1251 ( 68%)]  Loss: 3.374 (3.36)  Time: 0.715s, 1431.63/s  (0.695s, 1474.39/s)  LR: 6.754e-05  Data: 0.010 (0.013)
Train: 507 [ 900/1251 ( 72%)]  Loss: 3.358 (3.36)  Time: 0.684s, 1497.84/s  (0.694s, 1475.18/s)  LR: 6.754e-05  Data: 0.017 (0.012)
Train: 507 [ 950/1251 ( 76%)]  Loss: 3.430 (3.37)  Time: 0.757s, 1352.30/s  (0.694s, 1475.27/s)  LR: 6.754e-05  Data: 0.015 (0.012)
Train: 507 [1000/1251 ( 80%)]  Loss: 3.543 (3.38)  Time: 0.741s, 1381.08/s  (0.695s, 1473.03/s)  LR: 6.754e-05  Data: 0.009 (0.012)
Train: 507 [1050/1251 ( 84%)]  Loss: 3.469 (3.38)  Time: 0.743s, 1377.84/s  (0.696s, 1471.11/s)  LR: 6.754e-05  Data: 0.011 (0.012)
Train: 507 [1100/1251 ( 88%)]  Loss: 3.397 (3.38)  Time: 0.767s, 1335.62/s  (0.697s, 1470.17/s)  LR: 6.754e-05  Data: 0.017 (0.012)
Train: 507 [1150/1251 ( 92%)]  Loss: 3.509 (3.39)  Time: 0.708s, 1445.98/s  (0.696s, 1470.92/s)  LR: 6.754e-05  Data: 0.011 (0.012)
Train: 507 [1200/1251 ( 96%)]  Loss: 3.496 (3.39)  Time: 0.699s, 1465.94/s  (0.696s, 1471.64/s)  LR: 6.754e-05  Data: 0.009 (0.012)
Train: 507 [1250/1251 (100%)]  Loss: 3.067 (3.38)  Time: 0.655s, 1562.21/s  (0.696s, 1471.89/s)  LR: 6.754e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.511 (1.511)  Loss:  0.7119 (0.7119)  Acc@1: 90.7227 (90.7227)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.136 (0.573)  Loss:  0.8525 (1.2469)  Acc@1: 87.1462 (78.2520)  Acc@5: 96.6981 (93.9240)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-505.pth.tar', 78.41600011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-507.pth.tar', 78.25199997558593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-504.pth.tar', 78.20800008300782)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-506.pth.tar', 78.19200005371094)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-502.pth.tar', 78.16999997802735)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-498.pth.tar', 78.11600005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-495.pth.tar', 78.11200006103516)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-490.pth.tar', 78.09000000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-499.pth.tar', 78.05600002929687)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-494.pth.tar', 78.04200015869141)

Train: 508 [   0/1251 (  0%)]  Loss: 3.366 (3.37)  Time: 2.145s,  477.39/s  (2.145s,  477.39/s)  LR: 6.633e-05  Data: 1.532 (1.532)
Train: 508 [  50/1251 (  4%)]  Loss: 3.191 (3.28)  Time: 0.748s, 1368.28/s  (0.733s, 1397.10/s)  LR: 6.633e-05  Data: 0.009 (0.048)
Train: 508 [ 100/1251 (  8%)]  Loss: 3.129 (3.23)  Time: 0.706s, 1450.11/s  (0.718s, 1425.78/s)  LR: 6.633e-05  Data: 0.011 (0.029)
Train: 508 [ 150/1251 ( 12%)]  Loss: 3.699 (3.35)  Time: 0.722s, 1418.12/s  (0.711s, 1440.90/s)  LR: 6.633e-05  Data: 0.009 (0.023)
Train: 508 [ 200/1251 ( 16%)]  Loss: 3.368 (3.35)  Time: 0.706s, 1450.91/s  (0.709s, 1444.92/s)  LR: 6.633e-05  Data: 0.012 (0.020)
Train: 508 [ 250/1251 ( 20%)]  Loss: 3.532 (3.38)  Time: 0.725s, 1413.15/s  (0.705s, 1453.14/s)  LR: 6.633e-05  Data: 0.009 (0.018)
Train: 508 [ 300/1251 ( 24%)]  Loss: 3.145 (3.35)  Time: 0.687s, 1490.32/s  (0.704s, 1455.51/s)  LR: 6.633e-05  Data: 0.015 (0.017)
Train: 508 [ 350/1251 ( 28%)]  Loss: 3.073 (3.31)  Time: 0.669s, 1530.67/s  (0.702s, 1458.49/s)  LR: 6.633e-05  Data: 0.008 (0.016)
Train: 508 [ 400/1251 ( 32%)]  Loss: 3.250 (3.31)  Time: 0.671s, 1526.73/s  (0.700s, 1462.39/s)  LR: 6.633e-05  Data: 0.009 (0.015)
Train: 508 [ 450/1251 ( 36%)]  Loss: 3.126 (3.29)  Time: 0.683s, 1498.21/s  (0.699s, 1465.03/s)  LR: 6.633e-05  Data: 0.011 (0.015)
Train: 508 [ 500/1251 ( 40%)]  Loss: 3.220 (3.28)  Time: 0.682s, 1501.05/s  (0.698s, 1466.65/s)  LR: 6.633e-05  Data: 0.009 (0.014)
Train: 508 [ 550/1251 ( 44%)]  Loss: 2.951 (3.25)  Time: 0.689s, 1485.90/s  (0.697s, 1468.50/s)  LR: 6.633e-05  Data: 0.009 (0.014)
Train: 508 [ 600/1251 ( 48%)]  Loss: 3.207 (3.25)  Time: 0.683s, 1500.18/s  (0.697s, 1468.69/s)  LR: 6.633e-05  Data: 0.010 (0.013)
Train: 508 [ 650/1251 ( 52%)]  Loss: 3.026 (3.23)  Time: 0.674s, 1519.25/s  (0.697s, 1469.51/s)  LR: 6.633e-05  Data: 0.010 (0.013)
Train: 508 [ 700/1251 ( 56%)]  Loss: 3.315 (3.24)  Time: 0.712s, 1437.77/s  (0.697s, 1470.21/s)  LR: 6.633e-05  Data: 0.011 (0.013)
Train: 508 [ 750/1251 ( 60%)]  Loss: 3.206 (3.24)  Time: 0.719s, 1423.45/s  (0.697s, 1469.56/s)  LR: 6.633e-05  Data: 0.010 (0.013)
Train: 508 [ 800/1251 ( 64%)]  Loss: 2.822 (3.21)  Time: 0.683s, 1499.48/s  (0.696s, 1470.39/s)  LR: 6.633e-05  Data: 0.011 (0.013)
Train: 508 [ 850/1251 ( 68%)]  Loss: 3.500 (3.23)  Time: 0.672s, 1524.19/s  (0.696s, 1471.40/s)  LR: 6.633e-05  Data: 0.010 (0.013)
Train: 508 [ 900/1251 ( 72%)]  Loss: 3.309 (3.23)  Time: 0.726s, 1410.19/s  (0.696s, 1471.53/s)  LR: 6.633e-05  Data: 0.011 (0.013)
Train: 508 [ 950/1251 ( 76%)]  Loss: 3.523 (3.25)  Time: 0.723s, 1416.52/s  (0.696s, 1472.00/s)  LR: 6.633e-05  Data: 0.012 (0.012)
Train: 508 [1000/1251 ( 80%)]  Loss: 3.287 (3.25)  Time: 0.691s, 1481.07/s  (0.696s, 1472.23/s)  LR: 6.633e-05  Data: 0.009 (0.012)
Train: 508 [1050/1251 ( 84%)]  Loss: 3.639 (3.27)  Time: 0.686s, 1493.13/s  (0.695s, 1473.24/s)  LR: 6.633e-05  Data: 0.010 (0.012)
Train: 508 [1100/1251 ( 88%)]  Loss: 3.401 (3.27)  Time: 0.671s, 1525.58/s  (0.695s, 1473.89/s)  LR: 6.633e-05  Data: 0.011 (0.012)
Train: 508 [1150/1251 ( 92%)]  Loss: 2.962 (3.26)  Time: 0.717s, 1427.34/s  (0.695s, 1472.84/s)  LR: 6.633e-05  Data: 0.013 (0.012)
Train: 508 [1200/1251 ( 96%)]  Loss: 3.385 (3.27)  Time: 0.706s, 1451.24/s  (0.696s, 1472.19/s)  LR: 6.633e-05  Data: 0.010 (0.012)
Train: 508 [1250/1251 (100%)]  Loss: 3.295 (3.27)  Time: 0.659s, 1554.47/s  (0.695s, 1472.64/s)  LR: 6.633e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.478 (1.478)  Loss:  0.6929 (0.6929)  Acc@1: 91.6016 (91.6016)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.136 (0.582)  Loss:  0.7866 (1.1880)  Acc@1: 86.6745 (78.2820)  Acc@5: 97.2877 (94.0260)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-505.pth.tar', 78.41600011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-508.pth.tar', 78.28200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-507.pth.tar', 78.25199997558593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-504.pth.tar', 78.20800008300782)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-506.pth.tar', 78.19200005371094)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-502.pth.tar', 78.16999997802735)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-498.pth.tar', 78.11600005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-495.pth.tar', 78.11200006103516)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-490.pth.tar', 78.09000000732422)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-499.pth.tar', 78.05600002929687)

Train: 509 [   0/1251 (  0%)]  Loss: 2.892 (2.89)  Time: 2.413s,  424.44/s  (2.413s,  424.44/s)  LR: 6.513e-05  Data: 1.796 (1.796)
Train: 509 [  50/1251 (  4%)]  Loss: 3.087 (2.99)  Time: 0.673s, 1521.77/s  (0.733s, 1396.30/s)  LR: 6.513e-05  Data: 0.017 (0.052)
Train: 509 [ 100/1251 (  8%)]  Loss: 3.027 (3.00)  Time: 0.700s, 1463.79/s  (0.714s, 1433.36/s)  LR: 6.513e-05  Data: 0.009 (0.032)
Train: 509 [ 150/1251 ( 12%)]  Loss: 3.338 (3.09)  Time: 0.672s, 1524.67/s  (0.709s, 1444.93/s)  LR: 6.513e-05  Data: 0.010 (0.025)
Train: 509 [ 200/1251 ( 16%)]  Loss: 3.073 (3.08)  Time: 0.701s, 1460.75/s  (0.706s, 1450.76/s)  LR: 6.513e-05  Data: 0.010 (0.021)
Train: 509 [ 250/1251 ( 20%)]  Loss: 2.767 (3.03)  Time: 0.702s, 1458.35/s  (0.704s, 1454.79/s)  LR: 6.513e-05  Data: 0.013 (0.019)
Train: 509 [ 300/1251 ( 24%)]  Loss: 3.265 (3.06)  Time: 0.736s, 1391.58/s  (0.703s, 1456.50/s)  LR: 6.513e-05  Data: 0.009 (0.018)
Train: 509 [ 350/1251 ( 28%)]  Loss: 3.079 (3.07)  Time: 0.686s, 1492.28/s  (0.702s, 1459.58/s)  LR: 6.513e-05  Data: 0.009 (0.017)
Train: 509 [ 400/1251 ( 32%)]  Loss: 3.231 (3.08)  Time: 0.701s, 1461.05/s  (0.699s, 1464.20/s)  LR: 6.513e-05  Data: 0.010 (0.016)
Train: 509 [ 450/1251 ( 36%)]  Loss: 3.417 (3.12)  Time: 0.671s, 1525.87/s  (0.698s, 1466.44/s)  LR: 6.513e-05  Data: 0.009 (0.015)
Train: 509 [ 500/1251 ( 40%)]  Loss: 3.064 (3.11)  Time: 0.671s, 1525.42/s  (0.697s, 1468.41/s)  LR: 6.513e-05  Data: 0.010 (0.015)
Train: 509 [ 550/1251 ( 44%)]  Loss: 3.095 (3.11)  Time: 0.704s, 1453.57/s  (0.697s, 1469.65/s)  LR: 6.513e-05  Data: 0.009 (0.014)
Train: 509 [ 600/1251 ( 48%)]  Loss: 3.357 (3.13)  Time: 0.673s, 1522.61/s  (0.696s, 1470.32/s)  LR: 6.513e-05  Data: 0.010 (0.014)
Train: 509 [ 650/1251 ( 52%)]  Loss: 3.089 (3.13)  Time: 0.718s, 1426.62/s  (0.696s, 1470.93/s)  LR: 6.513e-05  Data: 0.010 (0.014)
Train: 509 [ 700/1251 ( 56%)]  Loss: 3.159 (3.13)  Time: 0.683s, 1500.29/s  (0.695s, 1472.71/s)  LR: 6.513e-05  Data: 0.015 (0.014)
Train: 509 [ 750/1251 ( 60%)]  Loss: 3.231 (3.14)  Time: 0.675s, 1517.29/s  (0.695s, 1473.87/s)  LR: 6.513e-05  Data: 0.010 (0.013)
Train: 509 [ 800/1251 ( 64%)]  Loss: 2.823 (3.12)  Time: 0.670s, 1529.28/s  (0.695s, 1473.16/s)  LR: 6.513e-05  Data: 0.010 (0.013)
Train: 509 [ 850/1251 ( 68%)]  Loss: 3.467 (3.14)  Time: 0.700s, 1463.15/s  (0.695s, 1473.76/s)  LR: 6.513e-05  Data: 0.010 (0.013)
Train: 509 [ 900/1251 ( 72%)]  Loss: 3.286 (3.14)  Time: 0.669s, 1530.90/s  (0.695s, 1473.84/s)  LR: 6.513e-05  Data: 0.011 (0.013)
Train: 509 [ 950/1251 ( 76%)]  Loss: 3.422 (3.16)  Time: 0.726s, 1409.58/s  (0.695s, 1473.90/s)  LR: 6.513e-05  Data: 0.010 (0.013)
Train: 509 [1000/1251 ( 80%)]  Loss: 3.528 (3.18)  Time: 0.733s, 1397.19/s  (0.695s, 1473.27/s)  LR: 6.513e-05  Data: 0.010 (0.013)
Train: 509 [1050/1251 ( 84%)]  Loss: 3.612 (3.20)  Time: 0.668s, 1533.71/s  (0.695s, 1473.84/s)  LR: 6.513e-05  Data: 0.009 (0.013)
Train: 509 [1100/1251 ( 88%)]  Loss: 3.214 (3.20)  Time: 0.680s, 1505.94/s  (0.695s, 1473.46/s)  LR: 6.513e-05  Data: 0.015 (0.013)
Train: 509 [1150/1251 ( 92%)]  Loss: 3.482 (3.21)  Time: 0.671s, 1526.70/s  (0.695s, 1473.91/s)  LR: 6.513e-05  Data: 0.009 (0.012)
Train: 509 [1200/1251 ( 96%)]  Loss: 3.131 (3.21)  Time: 0.670s, 1527.34/s  (0.695s, 1474.17/s)  LR: 6.513e-05  Data: 0.010 (0.012)
Train: 509 [1250/1251 (100%)]  Loss: 3.125 (3.20)  Time: 0.658s, 1557.08/s  (0.695s, 1474.30/s)  LR: 6.513e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.560 (1.560)  Loss:  0.7017 (0.7017)  Acc@1: 91.3086 (91.3086)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.136 (0.589)  Loss:  0.8276 (1.2183)  Acc@1: 87.1462 (78.2680)  Acc@5: 97.1698 (94.0600)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-505.pth.tar', 78.41600011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-508.pth.tar', 78.28200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-509.pth.tar', 78.26799997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-507.pth.tar', 78.25199997558593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-504.pth.tar', 78.20800008300782)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-506.pth.tar', 78.19200005371094)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-502.pth.tar', 78.16999997802735)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-498.pth.tar', 78.11600005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-495.pth.tar', 78.11200006103516)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-490.pth.tar', 78.09000000732422)

Train: 510 [   0/1251 (  0%)]  Loss: 3.120 (3.12)  Time: 2.108s,  485.84/s  (2.108s,  485.84/s)  LR: 6.395e-05  Data: 1.494 (1.494)
Train: 510 [  50/1251 (  4%)]  Loss: 3.361 (3.24)  Time: 0.696s, 1471.26/s  (0.729s, 1404.97/s)  LR: 6.395e-05  Data: 0.010 (0.047)
Train: 510 [ 100/1251 (  8%)]  Loss: 2.924 (3.14)  Time: 0.721s, 1419.51/s  (0.711s, 1440.66/s)  LR: 6.395e-05  Data: 0.010 (0.029)
Train: 510 [ 150/1251 ( 12%)]  Loss: 3.486 (3.22)  Time: 0.670s, 1527.67/s  (0.704s, 1455.44/s)  LR: 6.395e-05  Data: 0.010 (0.023)
Train: 510 [ 200/1251 ( 16%)]  Loss: 3.129 (3.20)  Time: 0.730s, 1402.90/s  (0.702s, 1459.26/s)  LR: 6.395e-05  Data: 0.011 (0.020)
Train: 510 [ 250/1251 ( 20%)]  Loss: 3.453 (3.25)  Time: 0.702s, 1458.38/s  (0.700s, 1462.95/s)  LR: 6.395e-05  Data: 0.009 (0.018)
Train: 510 [ 300/1251 ( 24%)]  Loss: 3.508 (3.28)  Time: 0.685s, 1493.85/s  (0.699s, 1464.84/s)  LR: 6.395e-05  Data: 0.010 (0.017)
Train: 510 [ 350/1251 ( 28%)]  Loss: 3.158 (3.27)  Time: 0.725s, 1412.66/s  (0.699s, 1464.91/s)  LR: 6.395e-05  Data: 0.011 (0.016)
Train: 510 [ 400/1251 ( 32%)]  Loss: 3.205 (3.26)  Time: 0.756s, 1354.82/s  (0.698s, 1466.51/s)  LR: 6.395e-05  Data: 0.011 (0.015)
Train: 510 [ 450/1251 ( 36%)]  Loss: 3.525 (3.29)  Time: 0.671s, 1526.82/s  (0.698s, 1466.43/s)  LR: 6.395e-05  Data: 0.010 (0.015)
Train: 510 [ 500/1251 ( 40%)]  Loss: 3.283 (3.29)  Time: 0.704s, 1455.27/s  (0.698s, 1468.07/s)  LR: 6.395e-05  Data: 0.009 (0.014)
Train: 510 [ 550/1251 ( 44%)]  Loss: 2.909 (3.25)  Time: 0.670s, 1527.40/s  (0.697s, 1469.08/s)  LR: 6.395e-05  Data: 0.011 (0.014)
Train: 510 [ 600/1251 ( 48%)]  Loss: 3.068 (3.24)  Time: 0.672s, 1523.43/s  (0.698s, 1468.07/s)  LR: 6.395e-05  Data: 0.010 (0.014)
Train: 510 [ 650/1251 ( 52%)]  Loss: 3.364 (3.25)  Time: 0.746s, 1372.87/s  (0.698s, 1468.04/s)  LR: 6.395e-05  Data: 0.010 (0.013)
Train: 510 [ 700/1251 ( 56%)]  Loss: 3.449 (3.26)  Time: 0.710s, 1442.93/s  (0.697s, 1469.32/s)  LR: 6.395e-05  Data: 0.009 (0.013)
Train: 510 [ 750/1251 ( 60%)]  Loss: 3.603 (3.28)  Time: 0.672s, 1522.77/s  (0.697s, 1469.49/s)  LR: 6.395e-05  Data: 0.010 (0.013)
Train: 510 [ 800/1251 ( 64%)]  Loss: 3.021 (3.27)  Time: 0.711s, 1440.28/s  (0.697s, 1470.09/s)  LR: 6.395e-05  Data: 0.010 (0.013)
Train: 510 [ 850/1251 ( 68%)]  Loss: 3.198 (3.26)  Time: 0.687s, 1490.56/s  (0.697s, 1469.83/s)  LR: 6.395e-05  Data: 0.010 (0.013)
Train: 510 [ 900/1251 ( 72%)]  Loss: 3.351 (3.27)  Time: 0.699s, 1464.05/s  (0.696s, 1470.82/s)  LR: 6.395e-05  Data: 0.010 (0.013)
Train: 510 [ 950/1251 ( 76%)]  Loss: 3.209 (3.27)  Time: 0.681s, 1503.49/s  (0.696s, 1471.83/s)  LR: 6.395e-05  Data: 0.011 (0.012)
Train: 510 [1000/1251 ( 80%)]  Loss: 3.205 (3.26)  Time: 0.713s, 1437.06/s  (0.696s, 1472.23/s)  LR: 6.395e-05  Data: 0.010 (0.012)
Train: 510 [1050/1251 ( 84%)]  Loss: 3.137 (3.26)  Time: 0.673s, 1520.52/s  (0.695s, 1472.35/s)  LR: 6.395e-05  Data: 0.010 (0.012)
Train: 510 [1100/1251 ( 88%)]  Loss: 3.090 (3.25)  Time: 0.706s, 1450.89/s  (0.695s, 1473.20/s)  LR: 6.395e-05  Data: 0.010 (0.012)
Train: 510 [1150/1251 ( 92%)]  Loss: 3.351 (3.25)  Time: 0.675s, 1516.40/s  (0.695s, 1473.36/s)  LR: 6.395e-05  Data: 0.011 (0.012)
Train: 510 [1200/1251 ( 96%)]  Loss: 3.308 (3.26)  Time: 0.711s, 1440.11/s  (0.695s, 1473.95/s)  LR: 6.395e-05  Data: 0.010 (0.012)
Train: 510 [1250/1251 (100%)]  Loss: 3.142 (3.25)  Time: 0.663s, 1545.37/s  (0.694s, 1474.64/s)  LR: 6.395e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.543 (1.543)  Loss:  0.6694 (0.6694)  Acc@1: 91.3086 (91.3086)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.137 (0.583)  Loss:  0.7896 (1.1718)  Acc@1: 87.1462 (78.4020)  Acc@5: 97.4057 (93.9820)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-505.pth.tar', 78.41600011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-510.pth.tar', 78.40199997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-508.pth.tar', 78.28200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-509.pth.tar', 78.26799997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-507.pth.tar', 78.25199997558593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-504.pth.tar', 78.20800008300782)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-506.pth.tar', 78.19200005371094)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-502.pth.tar', 78.16999997802735)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-498.pth.tar', 78.11600005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-495.pth.tar', 78.11200006103516)

Train: 511 [   0/1251 (  0%)]  Loss: 3.103 (3.10)  Time: 2.233s,  458.49/s  (2.233s,  458.49/s)  LR: 6.278e-05  Data: 1.618 (1.618)
Train: 511 [  50/1251 (  4%)]  Loss: 3.246 (3.17)  Time: 0.675s, 1517.03/s  (0.727s, 1408.07/s)  LR: 6.278e-05  Data: 0.012 (0.050)
Train: 511 [ 100/1251 (  8%)]  Loss: 2.979 (3.11)  Time: 0.673s, 1521.26/s  (0.711s, 1439.73/s)  LR: 6.278e-05  Data: 0.010 (0.031)
Train: 511 [ 150/1251 ( 12%)]  Loss: 3.163 (3.12)  Time: 0.813s, 1259.51/s  (0.708s, 1447.04/s)  LR: 6.278e-05  Data: 0.016 (0.024)
Train: 511 [ 200/1251 ( 16%)]  Loss: 3.132 (3.12)  Time: 0.666s, 1538.34/s  (0.706s, 1451.34/s)  LR: 6.278e-05  Data: 0.008 (0.021)
Train: 511 [ 250/1251 ( 20%)]  Loss: 3.357 (3.16)  Time: 0.688s, 1488.34/s  (0.702s, 1457.91/s)  LR: 6.278e-05  Data: 0.014 (0.019)
Train: 511 [ 300/1251 ( 24%)]  Loss: 3.089 (3.15)  Time: 0.706s, 1450.29/s  (0.702s, 1458.73/s)  LR: 6.278e-05  Data: 0.010 (0.017)
Train: 511 [ 350/1251 ( 28%)]  Loss: 3.089 (3.14)  Time: 0.671s, 1525.67/s  (0.701s, 1461.26/s)  LR: 6.278e-05  Data: 0.010 (0.016)
Train: 511 [ 400/1251 ( 32%)]  Loss: 3.208 (3.15)  Time: 0.675s, 1515.94/s  (0.700s, 1461.92/s)  LR: 6.278e-05  Data: 0.010 (0.016)
Train: 511 [ 450/1251 ( 36%)]  Loss: 3.290 (3.17)  Time: 0.757s, 1351.93/s  (0.699s, 1465.38/s)  LR: 6.278e-05  Data: 0.014 (0.015)
Train: 511 [ 500/1251 ( 40%)]  Loss: 3.128 (3.16)  Time: 0.671s, 1526.72/s  (0.698s, 1466.59/s)  LR: 6.278e-05  Data: 0.010 (0.015)
Train: 511 [ 550/1251 ( 44%)]  Loss: 3.294 (3.17)  Time: 0.729s, 1403.80/s  (0.698s, 1467.33/s)  LR: 6.278e-05  Data: 0.010 (0.014)
Train: 511 [ 600/1251 ( 48%)]  Loss: 3.458 (3.20)  Time: 0.705s, 1453.15/s  (0.697s, 1468.12/s)  LR: 6.278e-05  Data: 0.012 (0.014)
Train: 511 [ 650/1251 ( 52%)]  Loss: 3.374 (3.21)  Time: 0.676s, 1514.94/s  (0.697s, 1468.55/s)  LR: 6.278e-05  Data: 0.012 (0.014)
Train: 511 [ 700/1251 ( 56%)]  Loss: 3.192 (3.21)  Time: 0.688s, 1488.97/s  (0.697s, 1469.32/s)  LR: 6.278e-05  Data: 0.009 (0.013)
Train: 511 [ 750/1251 ( 60%)]  Loss: 3.379 (3.22)  Time: 0.676s, 1514.61/s  (0.697s, 1469.11/s)  LR: 6.278e-05  Data: 0.013 (0.013)
Train: 511 [ 800/1251 ( 64%)]  Loss: 3.580 (3.24)  Time: 0.671s, 1526.89/s  (0.697s, 1469.57/s)  LR: 6.278e-05  Data: 0.010 (0.013)
Train: 511 [ 850/1251 ( 68%)]  Loss: 3.248 (3.24)  Time: 0.722s, 1418.53/s  (0.697s, 1469.35/s)  LR: 6.278e-05  Data: 0.009 (0.013)
Train: 511 [ 900/1251 ( 72%)]  Loss: 3.152 (3.23)  Time: 0.689s, 1487.22/s  (0.697s, 1469.80/s)  LR: 6.278e-05  Data: 0.009 (0.013)
Train: 511 [ 950/1251 ( 76%)]  Loss: 3.532 (3.25)  Time: 0.663s, 1543.52/s  (0.697s, 1470.19/s)  LR: 6.278e-05  Data: 0.009 (0.013)
Train: 511 [1000/1251 ( 80%)]  Loss: 3.374 (3.26)  Time: 0.710s, 1443.04/s  (0.696s, 1470.73/s)  LR: 6.278e-05  Data: 0.010 (0.012)
Train: 511 [1050/1251 ( 84%)]  Loss: 3.374 (3.26)  Time: 0.675s, 1517.29/s  (0.696s, 1470.59/s)  LR: 6.278e-05  Data: 0.010 (0.012)
Train: 511 [1100/1251 ( 88%)]  Loss: 3.602 (3.28)  Time: 0.691s, 1482.16/s  (0.696s, 1470.46/s)  LR: 6.278e-05  Data: 0.008 (0.012)
Train: 511 [1150/1251 ( 92%)]  Loss: 3.497 (3.28)  Time: 0.672s, 1524.90/s  (0.696s, 1471.30/s)  LR: 6.278e-05  Data: 0.010 (0.012)
Train: 511 [1200/1251 ( 96%)]  Loss: 3.109 (3.28)  Time: 0.702s, 1458.43/s  (0.696s, 1471.87/s)  LR: 6.278e-05  Data: 0.009 (0.012)
Train: 511 [1250/1251 (100%)]  Loss: 3.638 (3.29)  Time: 0.655s, 1562.31/s  (0.696s, 1472.13/s)  LR: 6.278e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.542 (1.542)  Loss:  0.7881 (0.7881)  Acc@1: 91.4062 (91.4062)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.136 (0.587)  Loss:  0.8892 (1.3022)  Acc@1: 86.3208 (78.2180)  Acc@5: 97.0519 (93.9980)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-505.pth.tar', 78.41600011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-510.pth.tar', 78.40199997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-508.pth.tar', 78.28200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-509.pth.tar', 78.26799997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-507.pth.tar', 78.25199997558593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-511.pth.tar', 78.21800013427735)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-504.pth.tar', 78.20800008300782)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-506.pth.tar', 78.19200005371094)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-502.pth.tar', 78.16999997802735)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-498.pth.tar', 78.11600005615234)

Train: 512 [   0/1251 (  0%)]  Loss: 3.280 (3.28)  Time: 2.230s,  459.16/s  (2.230s,  459.16/s)  LR: 6.162e-05  Data: 1.614 (1.614)
Train: 512 [  50/1251 (  4%)]  Loss: 3.215 (3.25)  Time: 0.687s, 1490.06/s  (0.739s, 1386.19/s)  LR: 6.162e-05  Data: 0.009 (0.052)
Train: 512 [ 100/1251 (  8%)]  Loss: 3.183 (3.23)  Time: 0.700s, 1462.10/s  (0.717s, 1428.67/s)  LR: 6.162e-05  Data: 0.010 (0.031)
Train: 512 [ 150/1251 ( 12%)]  Loss: 3.654 (3.33)  Time: 0.672s, 1522.95/s  (0.706s, 1449.46/s)  LR: 6.162e-05  Data: 0.011 (0.025)
Train: 512 [ 200/1251 ( 16%)]  Loss: 3.297 (3.33)  Time: 0.693s, 1478.34/s  (0.704s, 1455.22/s)  LR: 6.162e-05  Data: 0.009 (0.021)
Train: 512 [ 250/1251 ( 20%)]  Loss: 3.353 (3.33)  Time: 0.702s, 1459.17/s  (0.701s, 1460.20/s)  LR: 6.162e-05  Data: 0.009 (0.019)
Train: 512 [ 300/1251 ( 24%)]  Loss: 3.445 (3.35)  Time: 0.678s, 1511.42/s  (0.701s, 1461.75/s)  LR: 6.162e-05  Data: 0.011 (0.018)
Train: 512 [ 350/1251 ( 28%)]  Loss: 3.511 (3.37)  Time: 0.722s, 1418.57/s  (0.700s, 1463.22/s)  LR: 6.162e-05  Data: 0.009 (0.017)
Train: 512 [ 400/1251 ( 32%)]  Loss: 3.213 (3.35)  Time: 0.683s, 1498.48/s  (0.699s, 1464.74/s)  LR: 6.162e-05  Data: 0.013 (0.016)
Train: 512 [ 450/1251 ( 36%)]  Loss: 3.407 (3.36)  Time: 0.674s, 1519.55/s  (0.698s, 1466.69/s)  LR: 6.162e-05  Data: 0.011 (0.015)
Train: 512 [ 500/1251 ( 40%)]  Loss: 3.743 (3.39)  Time: 0.674s, 1518.64/s  (0.698s, 1468.06/s)  LR: 6.162e-05  Data: 0.010 (0.015)
Train: 512 [ 550/1251 ( 44%)]  Loss: 2.822 (3.34)  Time: 0.733s, 1397.59/s  (0.697s, 1469.72/s)  LR: 6.162e-05  Data: 0.012 (0.014)
Train: 512 [ 600/1251 ( 48%)]  Loss: 3.519 (3.36)  Time: 0.711s, 1439.53/s  (0.697s, 1470.21/s)  LR: 6.162e-05  Data: 0.009 (0.014)
Train: 512 [ 650/1251 ( 52%)]  Loss: 3.281 (3.35)  Time: 0.674s, 1519.63/s  (0.696s, 1470.70/s)  LR: 6.162e-05  Data: 0.011 (0.014)
Train: 512 [ 700/1251 ( 56%)]  Loss: 3.205 (3.34)  Time: 0.672s, 1524.65/s  (0.696s, 1471.71/s)  LR: 6.162e-05  Data: 0.010 (0.014)
Train: 512 [ 750/1251 ( 60%)]  Loss: 3.421 (3.35)  Time: 0.672s, 1524.77/s  (0.695s, 1473.58/s)  LR: 6.162e-05  Data: 0.011 (0.013)
Train: 512 [ 800/1251 ( 64%)]  Loss: 3.626 (3.36)  Time: 0.679s, 1508.52/s  (0.694s, 1474.48/s)  LR: 6.162e-05  Data: 0.010 (0.013)
Train: 512 [ 850/1251 ( 68%)]  Loss: 3.203 (3.35)  Time: 0.673s, 1522.16/s  (0.694s, 1474.53/s)  LR: 6.162e-05  Data: 0.011 (0.013)
Train: 512 [ 900/1251 ( 72%)]  Loss: 3.025 (3.34)  Time: 0.672s, 1523.89/s  (0.694s, 1475.09/s)  LR: 6.162e-05  Data: 0.011 (0.013)
Train: 512 [ 950/1251 ( 76%)]  Loss: 3.024 (3.32)  Time: 0.712s, 1438.37/s  (0.694s, 1474.72/s)  LR: 6.162e-05  Data: 0.009 (0.013)
Train: 512 [1000/1251 ( 80%)]  Loss: 3.394 (3.32)  Time: 0.668s, 1532.27/s  (0.694s, 1475.27/s)  LR: 6.162e-05  Data: 0.009 (0.013)
Train: 512 [1050/1251 ( 84%)]  Loss: 3.191 (3.32)  Time: 0.677s, 1511.96/s  (0.694s, 1474.86/s)  LR: 6.162e-05  Data: 0.014 (0.013)
Train: 512 [1100/1251 ( 88%)]  Loss: 3.314 (3.32)  Time: 0.676s, 1514.05/s  (0.694s, 1474.60/s)  LR: 6.162e-05  Data: 0.013 (0.012)
Train: 512 [1150/1251 ( 92%)]  Loss: 3.550 (3.33)  Time: 0.677s, 1513.43/s  (0.694s, 1475.41/s)  LR: 6.162e-05  Data: 0.011 (0.012)
Train: 512 [1200/1251 ( 96%)]  Loss: 3.181 (3.32)  Time: 0.699s, 1465.71/s  (0.694s, 1475.30/s)  LR: 6.162e-05  Data: 0.011 (0.012)
Train: 512 [1250/1251 (100%)]  Loss: 3.583 (3.33)  Time: 0.707s, 1447.67/s  (0.694s, 1475.73/s)  LR: 6.162e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.500 (1.500)  Loss:  0.7935 (0.7935)  Acc@1: 91.1133 (91.1133)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.136 (0.601)  Loss:  0.8711 (1.2729)  Acc@1: 86.2028 (78.4480)  Acc@5: 96.8160 (94.0620)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-512.pth.tar', 78.44800021240235)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-505.pth.tar', 78.41600011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-510.pth.tar', 78.40199997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-508.pth.tar', 78.28200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-509.pth.tar', 78.26799997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-507.pth.tar', 78.25199997558593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-511.pth.tar', 78.21800013427735)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-504.pth.tar', 78.20800008300782)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-506.pth.tar', 78.19200005371094)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-502.pth.tar', 78.16999997802735)

Train: 513 [   0/1251 (  0%)]  Loss: 2.983 (2.98)  Time: 2.172s,  471.39/s  (2.172s,  471.39/s)  LR: 6.048e-05  Data: 1.556 (1.556)
Train: 513 [  50/1251 (  4%)]  Loss: 3.201 (3.09)  Time: 0.670s, 1529.44/s  (0.732s, 1398.75/s)  LR: 6.048e-05  Data: 0.010 (0.050)
Train: 513 [ 100/1251 (  8%)]  Loss: 2.968 (3.05)  Time: 0.700s, 1462.59/s  (0.711s, 1440.68/s)  LR: 6.048e-05  Data: 0.010 (0.030)
Train: 513 [ 150/1251 ( 12%)]  Loss: 3.692 (3.21)  Time: 0.674s, 1518.83/s  (0.706s, 1450.66/s)  LR: 6.048e-05  Data: 0.010 (0.024)
Train: 513 [ 200/1251 ( 16%)]  Loss: 3.350 (3.24)  Time: 0.697s, 1469.31/s  (0.703s, 1457.53/s)  LR: 6.048e-05  Data: 0.010 (0.020)
Train: 513 [ 250/1251 ( 20%)]  Loss: 3.525 (3.29)  Time: 0.709s, 1443.28/s  (0.701s, 1461.01/s)  LR: 6.048e-05  Data: 0.011 (0.018)
Train: 513 [ 300/1251 ( 24%)]  Loss: 3.178 (3.27)  Time: 0.704s, 1454.67/s  (0.699s, 1464.46/s)  LR: 6.048e-05  Data: 0.011 (0.017)
Train: 513 [ 350/1251 ( 28%)]  Loss: 3.238 (3.27)  Time: 0.715s, 1431.93/s  (0.699s, 1465.30/s)  LR: 6.048e-05  Data: 0.011 (0.016)
Train: 513 [ 400/1251 ( 32%)]  Loss: 3.087 (3.25)  Time: 0.720s, 1422.10/s  (0.698s, 1467.81/s)  LR: 6.048e-05  Data: 0.010 (0.015)
Train: 513 [ 450/1251 ( 36%)]  Loss: 3.308 (3.25)  Time: 0.674s, 1518.84/s  (0.697s, 1469.15/s)  LR: 6.048e-05  Data: 0.013 (0.015)
Train: 513 [ 500/1251 ( 40%)]  Loss: 3.256 (3.25)  Time: 0.716s, 1430.54/s  (0.697s, 1469.56/s)  LR: 6.048e-05  Data: 0.009 (0.014)
Train: 513 [ 550/1251 ( 44%)]  Loss: 3.508 (3.27)  Time: 0.669s, 1529.63/s  (0.696s, 1470.50/s)  LR: 6.048e-05  Data: 0.010 (0.014)
Train: 513 [ 600/1251 ( 48%)]  Loss: 3.312 (3.28)  Time: 0.670s, 1529.22/s  (0.695s, 1472.53/s)  LR: 6.048e-05  Data: 0.010 (0.014)
Train: 513 [ 650/1251 ( 52%)]  Loss: 3.618 (3.30)  Time: 0.685s, 1494.88/s  (0.695s, 1472.78/s)  LR: 6.048e-05  Data: 0.011 (0.013)
Train: 513 [ 700/1251 ( 56%)]  Loss: 3.004 (3.28)  Time: 0.707s, 1449.23/s  (0.695s, 1474.14/s)  LR: 6.048e-05  Data: 0.010 (0.013)
Train: 513 [ 750/1251 ( 60%)]  Loss: 3.274 (3.28)  Time: 0.715s, 1433.06/s  (0.694s, 1475.25/s)  LR: 6.048e-05  Data: 0.016 (0.013)
Train: 513 [ 800/1251 ( 64%)]  Loss: 3.416 (3.29)  Time: 0.673s, 1521.65/s  (0.694s, 1476.22/s)  LR: 6.048e-05  Data: 0.011 (0.013)
Train: 513 [ 850/1251 ( 68%)]  Loss: 2.727 (3.26)  Time: 0.712s, 1437.61/s  (0.694s, 1476.01/s)  LR: 6.048e-05  Data: 0.011 (0.013)
Train: 513 [ 900/1251 ( 72%)]  Loss: 3.223 (3.26)  Time: 0.706s, 1450.74/s  (0.694s, 1474.88/s)  LR: 6.048e-05  Data: 0.010 (0.013)
Train: 513 [ 950/1251 ( 76%)]  Loss: 3.155 (3.25)  Time: 0.669s, 1530.03/s  (0.694s, 1475.28/s)  LR: 6.048e-05  Data: 0.013 (0.012)
Train: 513 [1000/1251 ( 80%)]  Loss: 3.638 (3.27)  Time: 0.746s, 1373.31/s  (0.694s, 1475.46/s)  LR: 6.048e-05  Data: 0.010 (0.012)
Train: 513 [1050/1251 ( 84%)]  Loss: 3.155 (3.26)  Time: 0.672s, 1524.19/s  (0.694s, 1475.81/s)  LR: 6.048e-05  Data: 0.010 (0.012)
Train: 513 [1100/1251 ( 88%)]  Loss: 3.212 (3.26)  Time: 0.673s, 1522.65/s  (0.694s, 1475.60/s)  LR: 6.048e-05  Data: 0.010 (0.012)
Train: 513 [1150/1251 ( 92%)]  Loss: 3.331 (3.26)  Time: 0.679s, 1508.93/s  (0.693s, 1476.70/s)  LR: 6.048e-05  Data: 0.011 (0.012)
Train: 513 [1200/1251 ( 96%)]  Loss: 2.866 (3.25)  Time: 0.764s, 1340.31/s  (0.693s, 1477.26/s)  LR: 6.048e-05  Data: 0.010 (0.012)
Train: 513 [1250/1251 (100%)]  Loss: 3.352 (3.25)  Time: 0.659s, 1554.01/s  (0.693s, 1476.99/s)  LR: 6.048e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.472 (1.472)  Loss:  0.6694 (0.6694)  Acc@1: 91.1133 (91.1133)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.137 (0.586)  Loss:  0.8018 (1.2094)  Acc@1: 86.5566 (78.3960)  Acc@5: 97.1698 (94.0320)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-512.pth.tar', 78.44800021240235)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-505.pth.tar', 78.41600011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-510.pth.tar', 78.40199997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-513.pth.tar', 78.39599997802735)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-508.pth.tar', 78.28200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-509.pth.tar', 78.26799997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-507.pth.tar', 78.25199997558593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-511.pth.tar', 78.21800013427735)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-504.pth.tar', 78.20800008300782)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-506.pth.tar', 78.19200005371094)

Train: 514 [   0/1251 (  0%)]  Loss: 3.541 (3.54)  Time: 2.252s,  454.66/s  (2.252s,  454.66/s)  LR: 5.934e-05  Data: 1.583 (1.583)
Train: 514 [  50/1251 (  4%)]  Loss: 3.264 (3.40)  Time: 0.711s, 1439.46/s  (0.724s, 1413.93/s)  LR: 5.934e-05  Data: 0.012 (0.050)
Train: 514 [ 100/1251 (  8%)]  Loss: 3.423 (3.41)  Time: 0.690s, 1484.55/s  (0.712s, 1437.51/s)  LR: 5.934e-05  Data: 0.009 (0.030)
Train: 514 [ 150/1251 ( 12%)]  Loss: 3.523 (3.44)  Time: 0.693s, 1476.87/s  (0.704s, 1454.28/s)  LR: 5.934e-05  Data: 0.009 (0.024)
Train: 514 [ 200/1251 ( 16%)]  Loss: 3.461 (3.44)  Time: 0.672s, 1522.89/s  (0.699s, 1463.98/s)  LR: 5.934e-05  Data: 0.010 (0.021)
Train: 514 [ 250/1251 ( 20%)]  Loss: 3.024 (3.37)  Time: 0.672s, 1524.10/s  (0.698s, 1467.92/s)  LR: 5.934e-05  Data: 0.011 (0.019)
Train: 514 [ 300/1251 ( 24%)]  Loss: 3.515 (3.39)  Time: 0.705s, 1451.63/s  (0.698s, 1467.52/s)  LR: 5.934e-05  Data: 0.011 (0.017)
Train: 514 [ 350/1251 ( 28%)]  Loss: 3.550 (3.41)  Time: 0.708s, 1446.59/s  (0.698s, 1467.21/s)  LR: 5.934e-05  Data: 0.011 (0.016)
Train: 514 [ 400/1251 ( 32%)]  Loss: 3.241 (3.39)  Time: 0.667s, 1534.38/s  (0.697s, 1469.50/s)  LR: 5.934e-05  Data: 0.010 (0.016)
Train: 514 [ 450/1251 ( 36%)]  Loss: 3.443 (3.40)  Time: 0.683s, 1499.28/s  (0.696s, 1471.12/s)  LR: 5.934e-05  Data: 0.012 (0.015)
Train: 514 [ 500/1251 ( 40%)]  Loss: 3.501 (3.41)  Time: 0.756s, 1353.72/s  (0.696s, 1471.35/s)  LR: 5.934e-05  Data: 0.011 (0.015)
Train: 514 [ 550/1251 ( 44%)]  Loss: 3.097 (3.38)  Time: 0.702s, 1457.81/s  (0.696s, 1471.95/s)  LR: 5.934e-05  Data: 0.010 (0.014)
Train: 514 [ 600/1251 ( 48%)]  Loss: 3.486 (3.39)  Time: 0.715s, 1431.87/s  (0.695s, 1472.51/s)  LR: 5.934e-05  Data: 0.013 (0.014)
Train: 514 [ 650/1251 ( 52%)]  Loss: 3.401 (3.39)  Time: 0.723s, 1415.75/s  (0.695s, 1473.38/s)  LR: 5.934e-05  Data: 0.010 (0.014)
Train: 514 [ 700/1251 ( 56%)]  Loss: 3.192 (3.38)  Time: 0.731s, 1400.36/s  (0.695s, 1473.55/s)  LR: 5.934e-05  Data: 0.009 (0.013)
Train: 514 [ 750/1251 ( 60%)]  Loss: 3.331 (3.37)  Time: 0.672s, 1523.65/s  (0.695s, 1474.40/s)  LR: 5.934e-05  Data: 0.012 (0.013)
Train: 514 [ 800/1251 ( 64%)]  Loss: 3.158 (3.36)  Time: 0.675s, 1517.39/s  (0.694s, 1474.45/s)  LR: 5.934e-05  Data: 0.014 (0.013)
Train: 514 [ 850/1251 ( 68%)]  Loss: 2.963 (3.34)  Time: 0.705s, 1452.26/s  (0.695s, 1474.02/s)  LR: 5.934e-05  Data: 0.010 (0.013)
Train: 514 [ 900/1251 ( 72%)]  Loss: 2.881 (3.32)  Time: 0.719s, 1425.09/s  (0.695s, 1472.67/s)  LR: 5.934e-05  Data: 0.009 (0.013)
Train: 514 [ 950/1251 ( 76%)]  Loss: 3.060 (3.30)  Time: 0.671s, 1525.50/s  (0.695s, 1472.72/s)  LR: 5.934e-05  Data: 0.011 (0.013)
Train: 514 [1000/1251 ( 80%)]  Loss: 3.139 (3.30)  Time: 0.670s, 1527.72/s  (0.695s, 1473.20/s)  LR: 5.934e-05  Data: 0.009 (0.013)
Train: 514 [1050/1251 ( 84%)]  Loss: 3.113 (3.29)  Time: 0.673s, 1521.03/s  (0.695s, 1473.52/s)  LR: 5.934e-05  Data: 0.011 (0.012)
Train: 514 [1100/1251 ( 88%)]  Loss: 3.533 (3.30)  Time: 0.726s, 1411.03/s  (0.695s, 1473.85/s)  LR: 5.934e-05  Data: 0.011 (0.012)
Train: 514 [1150/1251 ( 92%)]  Loss: 2.973 (3.28)  Time: 0.734s, 1396.03/s  (0.695s, 1473.81/s)  LR: 5.934e-05  Data: 0.009 (0.012)
Train: 514 [1200/1251 ( 96%)]  Loss: 3.457 (3.29)  Time: 0.723s, 1415.89/s  (0.695s, 1473.95/s)  LR: 5.934e-05  Data: 0.011 (0.012)
Train: 514 [1250/1251 (100%)]  Loss: 3.080 (3.28)  Time: 0.659s, 1554.66/s  (0.695s, 1474.18/s)  LR: 5.934e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.449 (1.449)  Loss:  0.6958 (0.6958)  Acc@1: 91.6016 (91.6016)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.138 (0.585)  Loss:  0.8267 (1.1631)  Acc@1: 86.6745 (78.5020)  Acc@5: 97.1698 (94.0820)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-514.pth.tar', 78.50200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-512.pth.tar', 78.44800021240235)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-505.pth.tar', 78.41600011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-510.pth.tar', 78.40199997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-513.pth.tar', 78.39599997802735)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-508.pth.tar', 78.28200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-509.pth.tar', 78.26799997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-507.pth.tar', 78.25199997558593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-511.pth.tar', 78.21800013427735)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-504.pth.tar', 78.20800008300782)

Train: 515 [   0/1251 (  0%)]  Loss: 3.509 (3.51)  Time: 2.184s,  468.77/s  (2.184s,  468.77/s)  LR: 5.822e-05  Data: 1.568 (1.568)
Train: 515 [  50/1251 (  4%)]  Loss: 3.517 (3.51)  Time: 0.709s, 1444.96/s  (0.725s, 1412.33/s)  LR: 5.822e-05  Data: 0.009 (0.048)
Train: 515 [ 100/1251 (  8%)]  Loss: 3.232 (3.42)  Time: 0.666s, 1537.60/s  (0.708s, 1446.42/s)  LR: 5.822e-05  Data: 0.010 (0.030)
Train: 515 [ 150/1251 ( 12%)]  Loss: 3.386 (3.41)  Time: 0.788s, 1298.90/s  (0.704s, 1453.57/s)  LR: 5.822e-05  Data: 0.010 (0.023)
Train: 515 [ 200/1251 ( 16%)]  Loss: 3.046 (3.34)  Time: 0.695s, 1473.44/s  (0.700s, 1462.17/s)  LR: 5.822e-05  Data: 0.011 (0.020)
Train: 515 [ 250/1251 ( 20%)]  Loss: 3.144 (3.31)  Time: 0.670s, 1528.93/s  (0.699s, 1464.75/s)  LR: 5.822e-05  Data: 0.009 (0.018)
Train: 515 [ 300/1251 ( 24%)]  Loss: 3.468 (3.33)  Time: 0.727s, 1408.33/s  (0.697s, 1468.73/s)  LR: 5.822e-05  Data: 0.009 (0.017)
Train: 515 [ 350/1251 ( 28%)]  Loss: 3.118 (3.30)  Time: 0.685s, 1495.08/s  (0.696s, 1471.55/s)  LR: 5.822e-05  Data: 0.010 (0.016)
Train: 515 [ 400/1251 ( 32%)]  Loss: 3.146 (3.28)  Time: 0.717s, 1428.20/s  (0.695s, 1474.09/s)  LR: 5.822e-05  Data: 0.010 (0.015)
Train: 515 [ 450/1251 ( 36%)]  Loss: 3.265 (3.28)  Time: 0.723s, 1416.06/s  (0.694s, 1474.79/s)  LR: 5.822e-05  Data: 0.009 (0.015)
Train: 515 [ 500/1251 ( 40%)]  Loss: 3.254 (3.28)  Time: 0.671s, 1526.26/s  (0.695s, 1474.36/s)  LR: 5.822e-05  Data: 0.009 (0.014)
Train: 515 [ 550/1251 ( 44%)]  Loss: 2.821 (3.24)  Time: 0.711s, 1439.50/s  (0.694s, 1475.01/s)  LR: 5.822e-05  Data: 0.011 (0.014)
Train: 515 [ 600/1251 ( 48%)]  Loss: 3.083 (3.23)  Time: 0.707s, 1448.26/s  (0.694s, 1474.68/s)  LR: 5.822e-05  Data: 0.011 (0.014)
Train: 515 [ 650/1251 ( 52%)]  Loss: 3.208 (3.23)  Time: 0.666s, 1537.11/s  (0.694s, 1475.10/s)  LR: 5.822e-05  Data: 0.010 (0.013)
Train: 515 [ 700/1251 ( 56%)]  Loss: 3.307 (3.23)  Time: 0.671s, 1526.96/s  (0.694s, 1475.56/s)  LR: 5.822e-05  Data: 0.010 (0.013)
Train: 515 [ 750/1251 ( 60%)]  Loss: 3.156 (3.23)  Time: 0.672s, 1524.75/s  (0.694s, 1476.17/s)  LR: 5.822e-05  Data: 0.010 (0.013)
Train: 515 [ 800/1251 ( 64%)]  Loss: 3.654 (3.25)  Time: 0.672s, 1523.81/s  (0.693s, 1476.64/s)  LR: 5.822e-05  Data: 0.010 (0.013)
Train: 515 [ 850/1251 ( 68%)]  Loss: 3.436 (3.26)  Time: 0.712s, 1437.25/s  (0.693s, 1477.20/s)  LR: 5.822e-05  Data: 0.010 (0.013)
Train: 515 [ 900/1251 ( 72%)]  Loss: 3.265 (3.26)  Time: 0.672s, 1524.69/s  (0.693s, 1477.38/s)  LR: 5.822e-05  Data: 0.009 (0.013)
Train: 515 [ 950/1251 ( 76%)]  Loss: 3.379 (3.27)  Time: 0.734s, 1394.41/s  (0.693s, 1477.14/s)  LR: 5.822e-05  Data: 0.011 (0.012)
Train: 515 [1000/1251 ( 80%)]  Loss: 3.471 (3.28)  Time: 0.671s, 1525.17/s  (0.693s, 1477.75/s)  LR: 5.822e-05  Data: 0.011 (0.012)
Train: 515 [1050/1251 ( 84%)]  Loss: 3.152 (3.27)  Time: 0.669s, 1530.06/s  (0.693s, 1476.71/s)  LR: 5.822e-05  Data: 0.010 (0.012)
Train: 515 [1100/1251 ( 88%)]  Loss: 3.444 (3.28)  Time: 0.672s, 1524.62/s  (0.693s, 1476.61/s)  LR: 5.822e-05  Data: 0.010 (0.012)
Train: 515 [1150/1251 ( 92%)]  Loss: 3.555 (3.29)  Time: 0.697s, 1469.16/s  (0.694s, 1475.91/s)  LR: 5.822e-05  Data: 0.011 (0.012)
Train: 515 [1200/1251 ( 96%)]  Loss: 3.278 (3.29)  Time: 0.705s, 1452.94/s  (0.694s, 1475.43/s)  LR: 5.822e-05  Data: 0.010 (0.012)
Train: 515 [1250/1251 (100%)]  Loss: 3.118 (3.29)  Time: 0.679s, 1508.65/s  (0.694s, 1475.19/s)  LR: 5.822e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.486 (1.486)  Loss:  0.7656 (0.7656)  Acc@1: 91.6992 (91.6992)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.137 (0.590)  Loss:  0.8408 (1.2254)  Acc@1: 86.2028 (78.3820)  Acc@5: 97.0519 (94.0980)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-514.pth.tar', 78.50200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-512.pth.tar', 78.44800021240235)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-505.pth.tar', 78.41600011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-510.pth.tar', 78.40199997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-513.pth.tar', 78.39599997802735)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-515.pth.tar', 78.38200008300781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-508.pth.tar', 78.28200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-509.pth.tar', 78.26799997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-507.pth.tar', 78.25199997558593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-511.pth.tar', 78.21800013427735)

Train: 516 [   0/1251 (  0%)]  Loss: 2.961 (2.96)  Time: 2.251s,  454.91/s  (2.251s,  454.91/s)  LR: 5.711e-05  Data: 1.635 (1.635)
Train: 516 [  50/1251 (  4%)]  Loss: 3.176 (3.07)  Time: 0.675s, 1516.69/s  (0.732s, 1398.18/s)  LR: 5.711e-05  Data: 0.010 (0.050)
Train: 516 [ 100/1251 (  8%)]  Loss: 3.319 (3.15)  Time: 0.699s, 1463.98/s  (0.716s, 1429.73/s)  LR: 5.711e-05  Data: 0.011 (0.031)
Train: 516 [ 150/1251 ( 12%)]  Loss: 3.277 (3.18)  Time: 0.704s, 1454.19/s  (0.709s, 1444.08/s)  LR: 5.711e-05  Data: 0.010 (0.024)
Train: 516 [ 200/1251 ( 16%)]  Loss: 2.996 (3.15)  Time: 0.697s, 1468.34/s  (0.704s, 1453.58/s)  LR: 5.711e-05  Data: 0.011 (0.021)
Train: 516 [ 250/1251 ( 20%)]  Loss: 2.895 (3.10)  Time: 0.721s, 1419.68/s  (0.701s, 1460.03/s)  LR: 5.711e-05  Data: 0.009 (0.019)
Train: 516 [ 300/1251 ( 24%)]  Loss: 3.108 (3.10)  Time: 0.675s, 1517.44/s  (0.699s, 1465.89/s)  LR: 5.711e-05  Data: 0.011 (0.017)
Train: 516 [ 350/1251 ( 28%)]  Loss: 3.381 (3.14)  Time: 0.670s, 1527.50/s  (0.698s, 1467.49/s)  LR: 5.711e-05  Data: 0.009 (0.016)
Train: 516 [ 400/1251 ( 32%)]  Loss: 3.378 (3.17)  Time: 0.675s, 1516.44/s  (0.697s, 1469.33/s)  LR: 5.711e-05  Data: 0.012 (0.016)
Train: 516 [ 450/1251 ( 36%)]  Loss: 3.246 (3.17)  Time: 0.707s, 1449.05/s  (0.696s, 1470.54/s)  LR: 5.711e-05  Data: 0.011 (0.015)
Train: 516 [ 500/1251 ( 40%)]  Loss: 3.229 (3.18)  Time: 0.675s, 1516.09/s  (0.696s, 1471.70/s)  LR: 5.711e-05  Data: 0.013 (0.015)
Train: 516 [ 550/1251 ( 44%)]  Loss: 3.267 (3.19)  Time: 0.726s, 1410.55/s  (0.695s, 1472.46/s)  LR: 5.711e-05  Data: 0.009 (0.014)
Train: 516 [ 600/1251 ( 48%)]  Loss: 3.326 (3.20)  Time: 0.692s, 1478.85/s  (0.695s, 1472.77/s)  LR: 5.711e-05  Data: 0.011 (0.014)
Train: 516 [ 650/1251 ( 52%)]  Loss: 2.870 (3.17)  Time: 0.680s, 1505.01/s  (0.695s, 1473.85/s)  LR: 5.711e-05  Data: 0.011 (0.014)
Train: 516 [ 700/1251 ( 56%)]  Loss: 3.551 (3.20)  Time: 0.671s, 1526.15/s  (0.695s, 1474.28/s)  LR: 5.711e-05  Data: 0.011 (0.013)
Train: 516 [ 750/1251 ( 60%)]  Loss: 3.220 (3.20)  Time: 0.705s, 1452.31/s  (0.694s, 1474.86/s)  LR: 5.711e-05  Data: 0.010 (0.013)
Train: 516 [ 800/1251 ( 64%)]  Loss: 2.850 (3.18)  Time: 0.673s, 1521.87/s  (0.694s, 1475.81/s)  LR: 5.711e-05  Data: 0.011 (0.013)
Train: 516 [ 850/1251 ( 68%)]  Loss: 3.279 (3.19)  Time: 0.683s, 1500.17/s  (0.694s, 1476.02/s)  LR: 5.711e-05  Data: 0.011 (0.013)
Train: 516 [ 900/1251 ( 72%)]  Loss: 3.733 (3.21)  Time: 0.691s, 1481.79/s  (0.694s, 1476.21/s)  LR: 5.711e-05  Data: 0.009 (0.013)
Train: 516 [ 950/1251 ( 76%)]  Loss: 3.251 (3.22)  Time: 0.736s, 1391.16/s  (0.693s, 1477.17/s)  LR: 5.711e-05  Data: 0.011 (0.013)
Train: 516 [1000/1251 ( 80%)]  Loss: 3.198 (3.21)  Time: 0.679s, 1507.73/s  (0.693s, 1477.40/s)  LR: 5.711e-05  Data: 0.009 (0.012)
Train: 516 [1050/1251 ( 84%)]  Loss: 3.392 (3.22)  Time: 0.683s, 1498.59/s  (0.693s, 1477.46/s)  LR: 5.711e-05  Data: 0.012 (0.012)
Train: 516 [1100/1251 ( 88%)]  Loss: 3.495 (3.23)  Time: 0.724s, 1413.47/s  (0.693s, 1477.17/s)  LR: 5.711e-05  Data: 0.009 (0.012)
Train: 516 [1150/1251 ( 92%)]  Loss: 3.034 (3.23)  Time: 0.669s, 1530.51/s  (0.693s, 1477.16/s)  LR: 5.711e-05  Data: 0.012 (0.012)
Train: 516 [1200/1251 ( 96%)]  Loss: 3.325 (3.23)  Time: 0.729s, 1405.59/s  (0.693s, 1476.75/s)  LR: 5.711e-05  Data: 0.011 (0.012)
Train: 516 [1250/1251 (100%)]  Loss: 3.166 (3.23)  Time: 0.688s, 1487.54/s  (0.693s, 1477.03/s)  LR: 5.711e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.559 (1.559)  Loss:  0.7949 (0.7949)  Acc@1: 91.9922 (91.9922)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.136 (0.582)  Loss:  0.9019 (1.2777)  Acc@1: 87.0283 (78.3040)  Acc@5: 96.8160 (94.0700)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-514.pth.tar', 78.50200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-512.pth.tar', 78.44800021240235)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-505.pth.tar', 78.41600011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-510.pth.tar', 78.40199997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-513.pth.tar', 78.39599997802735)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-515.pth.tar', 78.38200008300781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-516.pth.tar', 78.30400005371094)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-508.pth.tar', 78.28200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-509.pth.tar', 78.26799997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-507.pth.tar', 78.25199997558593)

Train: 517 [   0/1251 (  0%)]  Loss: 3.171 (3.17)  Time: 2.254s,  454.31/s  (2.254s,  454.31/s)  LR: 5.601e-05  Data: 1.585 (1.585)
Train: 517 [  50/1251 (  4%)]  Loss: 3.031 (3.10)  Time: 0.707s, 1448.62/s  (0.735s, 1392.32/s)  LR: 5.601e-05  Data: 0.011 (0.052)
Train: 517 [ 100/1251 (  8%)]  Loss: 3.192 (3.13)  Time: 0.673s, 1520.75/s  (0.714s, 1434.89/s)  LR: 5.601e-05  Data: 0.010 (0.031)
Train: 517 [ 150/1251 ( 12%)]  Loss: 3.249 (3.16)  Time: 0.673s, 1521.57/s  (0.707s, 1447.67/s)  LR: 5.601e-05  Data: 0.010 (0.025)
Train: 517 [ 200/1251 ( 16%)]  Loss: 3.387 (3.21)  Time: 0.700s, 1463.58/s  (0.704s, 1453.53/s)  LR: 5.601e-05  Data: 0.010 (0.021)
Train: 517 [ 250/1251 ( 20%)]  Loss: 3.318 (3.22)  Time: 0.672s, 1523.64/s  (0.702s, 1459.43/s)  LR: 5.601e-05  Data: 0.011 (0.019)
Train: 517 [ 300/1251 ( 24%)]  Loss: 3.441 (3.26)  Time: 0.841s, 1217.55/s  (0.700s, 1462.23/s)  LR: 5.601e-05  Data: 0.009 (0.018)
Train: 517 [ 350/1251 ( 28%)]  Loss: 3.375 (3.27)  Time: 0.672s, 1524.70/s  (0.699s, 1465.09/s)  LR: 5.601e-05  Data: 0.011 (0.017)
Train: 517 [ 400/1251 ( 32%)]  Loss: 3.140 (3.26)  Time: 0.671s, 1527.19/s  (0.699s, 1465.50/s)  LR: 5.601e-05  Data: 0.010 (0.016)
Train: 517 [ 450/1251 ( 36%)]  Loss: 3.141 (3.24)  Time: 0.690s, 1483.30/s  (0.698s, 1467.10/s)  LR: 5.601e-05  Data: 0.010 (0.015)
Train: 517 [ 500/1251 ( 40%)]  Loss: 3.155 (3.24)  Time: 0.691s, 1482.72/s  (0.697s, 1469.30/s)  LR: 5.601e-05  Data: 0.011 (0.015)
Train: 517 [ 550/1251 ( 44%)]  Loss: 3.094 (3.22)  Time: 0.681s, 1503.60/s  (0.699s, 1465.33/s)  LR: 5.601e-05  Data: 0.010 (0.015)
Train: 517 [ 600/1251 ( 48%)]  Loss: 3.193 (3.22)  Time: 0.696s, 1471.76/s  (0.700s, 1462.52/s)  LR: 5.601e-05  Data: 0.010 (0.014)
Train: 517 [ 650/1251 ( 52%)]  Loss: 3.061 (3.21)  Time: 0.673s, 1522.63/s  (0.702s, 1459.59/s)  LR: 5.601e-05  Data: 0.011 (0.014)
Train: 517 [ 700/1251 ( 56%)]  Loss: 3.530 (3.23)  Time: 0.732s, 1398.60/s  (0.700s, 1462.33/s)  LR: 5.601e-05  Data: 0.010 (0.014)
Train: 517 [ 750/1251 ( 60%)]  Loss: 3.348 (3.24)  Time: 0.671s, 1525.67/s  (0.698s, 1466.20/s)  LR: 5.601e-05  Data: 0.010 (0.014)
Train: 517 [ 800/1251 ( 64%)]  Loss: 3.255 (3.24)  Time: 0.671s, 1526.34/s  (0.697s, 1469.68/s)  LR: 5.601e-05  Data: 0.009 (0.014)
Train: 517 [ 850/1251 ( 68%)]  Loss: 3.213 (3.24)  Time: 0.681s, 1503.67/s  (0.696s, 1470.72/s)  LR: 5.601e-05  Data: 0.010 (0.013)
Train: 517 [ 900/1251 ( 72%)]  Loss: 3.323 (3.24)  Time: 0.672s, 1524.72/s  (0.696s, 1471.30/s)  LR: 5.601e-05  Data: 0.016 (0.013)
Train: 517 [ 950/1251 ( 76%)]  Loss: 3.340 (3.25)  Time: 0.705s, 1452.08/s  (0.696s, 1471.36/s)  LR: 5.601e-05  Data: 0.010 (0.013)
Train: 517 [1000/1251 ( 80%)]  Loss: 3.016 (3.24)  Time: 0.671s, 1526.47/s  (0.695s, 1472.74/s)  LR: 5.601e-05  Data: 0.012 (0.013)
Train: 517 [1050/1251 ( 84%)]  Loss: 3.103 (3.23)  Time: 0.710s, 1442.25/s  (0.695s, 1473.57/s)  LR: 5.601e-05  Data: 0.009 (0.013)
Train: 517 [1100/1251 ( 88%)]  Loss: 3.370 (3.24)  Time: 0.693s, 1477.18/s  (0.695s, 1473.70/s)  LR: 5.601e-05  Data: 0.013 (0.013)
Train: 517 [1150/1251 ( 92%)]  Loss: 3.220 (3.24)  Time: 0.670s, 1528.06/s  (0.695s, 1474.08/s)  LR: 5.601e-05  Data: 0.009 (0.013)
Train: 517 [1200/1251 ( 96%)]  Loss: 3.247 (3.24)  Time: 0.761s, 1345.73/s  (0.695s, 1474.37/s)  LR: 5.601e-05  Data: 0.009 (0.013)
Train: 517 [1250/1251 (100%)]  Loss: 3.446 (3.24)  Time: 0.657s, 1558.65/s  (0.694s, 1475.35/s)  LR: 5.601e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.606 (1.606)  Loss:  0.7100 (0.7100)  Acc@1: 91.1133 (91.1133)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.136 (0.587)  Loss:  0.7861 (1.1925)  Acc@1: 87.0283 (78.2160)  Acc@5: 97.1698 (94.0540)
Train: 518 [   0/1251 (  0%)]  Loss: 3.206 (3.21)  Time: 2.358s,  434.28/s  (2.358s,  434.28/s)  LR: 5.493e-05  Data: 1.742 (1.742)
Train: 518 [  50/1251 (  4%)]  Loss: 3.270 (3.24)  Time: 0.700s, 1462.26/s  (0.735s, 1392.83/s)  LR: 5.493e-05  Data: 0.010 (0.051)
Train: 518 [ 100/1251 (  8%)]  Loss: 3.179 (3.22)  Time: 0.716s, 1429.55/s  (0.718s, 1426.72/s)  LR: 5.493e-05  Data: 0.010 (0.031)
Train: 518 [ 150/1251 ( 12%)]  Loss: 2.945 (3.15)  Time: 0.686s, 1493.42/s  (0.708s, 1446.52/s)  LR: 5.493e-05  Data: 0.010 (0.024)
Train: 518 [ 200/1251 ( 16%)]  Loss: 3.484 (3.22)  Time: 0.670s, 1527.64/s  (0.703s, 1457.63/s)  LR: 5.493e-05  Data: 0.010 (0.021)
Train: 518 [ 250/1251 ( 20%)]  Loss: 3.281 (3.23)  Time: 0.670s, 1528.38/s  (0.700s, 1462.69/s)  LR: 5.493e-05  Data: 0.011 (0.019)
Train: 518 [ 300/1251 ( 24%)]  Loss: 3.121 (3.21)  Time: 0.675s, 1517.57/s  (0.698s, 1467.88/s)  LR: 5.493e-05  Data: 0.010 (0.017)
Train: 518 [ 350/1251 ( 28%)]  Loss: 3.195 (3.21)  Time: 0.684s, 1496.74/s  (0.698s, 1467.06/s)  LR: 5.493e-05  Data: 0.010 (0.016)
Train: 518 [ 400/1251 ( 32%)]  Loss: 2.940 (3.18)  Time: 0.689s, 1485.73/s  (0.697s, 1468.38/s)  LR: 5.493e-05  Data: 0.016 (0.016)
Train: 518 [ 450/1251 ( 36%)]  Loss: 3.363 (3.20)  Time: 0.676s, 1515.20/s  (0.697s, 1469.08/s)  LR: 5.493e-05  Data: 0.010 (0.015)
Train: 518 [ 500/1251 ( 40%)]  Loss: 3.630 (3.24)  Time: 0.723s, 1416.20/s  (0.697s, 1468.85/s)  LR: 5.493e-05  Data: 0.010 (0.015)
Train: 518 [ 550/1251 ( 44%)]  Loss: 3.422 (3.25)  Time: 0.671s, 1526.95/s  (0.697s, 1469.05/s)  LR: 5.493e-05  Data: 0.011 (0.014)
Train: 518 [ 600/1251 ( 48%)]  Loss: 3.395 (3.26)  Time: 0.672s, 1523.18/s  (0.696s, 1471.61/s)  LR: 5.493e-05  Data: 0.011 (0.014)
Train: 518 [ 650/1251 ( 52%)]  Loss: 3.361 (3.27)  Time: 0.671s, 1526.95/s  (0.696s, 1471.35/s)  LR: 5.493e-05  Data: 0.010 (0.014)
Train: 518 [ 700/1251 ( 56%)]  Loss: 3.222 (3.27)  Time: 0.706s, 1450.07/s  (0.696s, 1471.76/s)  LR: 5.493e-05  Data: 0.013 (0.014)
Train: 518 [ 750/1251 ( 60%)]  Loss: 3.239 (3.27)  Time: 0.704s, 1454.38/s  (0.695s, 1473.19/s)  LR: 5.493e-05  Data: 0.011 (0.013)
Train: 518 [ 800/1251 ( 64%)]  Loss: 3.371 (3.27)  Time: 0.668s, 1532.75/s  (0.695s, 1472.92/s)  LR: 5.493e-05  Data: 0.011 (0.013)
Train: 518 [ 850/1251 ( 68%)]  Loss: 3.278 (3.27)  Time: 0.688s, 1488.43/s  (0.695s, 1473.15/s)  LR: 5.493e-05  Data: 0.015 (0.013)
Train: 518 [ 900/1251 ( 72%)]  Loss: 3.676 (3.29)  Time: 0.674s, 1519.15/s  (0.695s, 1474.03/s)  LR: 5.493e-05  Data: 0.010 (0.013)
Train: 518 [ 950/1251 ( 76%)]  Loss: 3.024 (3.28)  Time: 0.722s, 1419.13/s  (0.695s, 1473.77/s)  LR: 5.493e-05  Data: 0.011 (0.013)
Train: 518 [1000/1251 ( 80%)]  Loss: 2.875 (3.26)  Time: 0.672s, 1524.01/s  (0.694s, 1474.80/s)  LR: 5.493e-05  Data: 0.011 (0.013)
Train: 518 [1050/1251 ( 84%)]  Loss: 2.979 (3.25)  Time: 0.695s, 1472.34/s  (0.694s, 1475.09/s)  LR: 5.493e-05  Data: 0.009 (0.013)
Train: 518 [1100/1251 ( 88%)]  Loss: 3.189 (3.25)  Time: 0.678s, 1511.16/s  (0.694s, 1474.88/s)  LR: 5.493e-05  Data: 0.013 (0.012)
Train: 518 [1150/1251 ( 92%)]  Loss: 3.211 (3.24)  Time: 0.671s, 1527.04/s  (0.694s, 1475.08/s)  LR: 5.493e-05  Data: 0.010 (0.012)
Train: 518 [1200/1251 ( 96%)]  Loss: 3.321 (3.25)  Time: 0.672s, 1523.58/s  (0.694s, 1475.66/s)  LR: 5.493e-05  Data: 0.012 (0.012)
Train: 518 [1250/1251 (100%)]  Loss: 3.092 (3.24)  Time: 0.656s, 1561.08/s  (0.694s, 1476.01/s)  LR: 5.493e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.509 (1.509)  Loss:  0.6763 (0.6763)  Acc@1: 91.4062 (91.4062)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.136 (0.585)  Loss:  0.7759 (1.1543)  Acc@1: 86.9104 (78.4880)  Acc@5: 97.0519 (94.1780)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-514.pth.tar', 78.50200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-518.pth.tar', 78.48800000244141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-512.pth.tar', 78.44800021240235)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-505.pth.tar', 78.41600011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-510.pth.tar', 78.40199997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-513.pth.tar', 78.39599997802735)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-515.pth.tar', 78.38200008300781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-516.pth.tar', 78.30400005371094)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-508.pth.tar', 78.28200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-509.pth.tar', 78.26799997558594)

Train: 519 [   0/1251 (  0%)]  Loss: 2.991 (2.99)  Time: 2.144s,  477.52/s  (2.144s,  477.52/s)  LR: 5.386e-05  Data: 1.528 (1.528)
Train: 519 [  50/1251 (  4%)]  Loss: 3.252 (3.12)  Time: 0.673s, 1521.97/s  (0.726s, 1411.42/s)  LR: 5.386e-05  Data: 0.011 (0.049)
Train: 519 [ 100/1251 (  8%)]  Loss: 3.111 (3.12)  Time: 0.708s, 1446.88/s  (0.709s, 1444.68/s)  LR: 5.386e-05  Data: 0.010 (0.030)
Train: 519 [ 150/1251 ( 12%)]  Loss: 3.148 (3.13)  Time: 0.670s, 1527.59/s  (0.704s, 1454.12/s)  LR: 5.386e-05  Data: 0.011 (0.023)
Train: 519 [ 200/1251 ( 16%)]  Loss: 2.969 (3.09)  Time: 0.674s, 1520.08/s  (0.701s, 1460.06/s)  LR: 5.386e-05  Data: 0.011 (0.020)
Train: 519 [ 250/1251 ( 20%)]  Loss: 2.767 (3.04)  Time: 0.671s, 1525.68/s  (0.699s, 1464.89/s)  LR: 5.386e-05  Data: 0.010 (0.018)
Train: 519 [ 300/1251 ( 24%)]  Loss: 3.290 (3.08)  Time: 0.714s, 1433.37/s  (0.697s, 1469.24/s)  LR: 5.386e-05  Data: 0.009 (0.017)
Train: 519 [ 350/1251 ( 28%)]  Loss: 3.345 (3.11)  Time: 0.670s, 1529.02/s  (0.696s, 1472.12/s)  LR: 5.386e-05  Data: 0.010 (0.016)
Train: 519 [ 400/1251 ( 32%)]  Loss: 3.712 (3.18)  Time: 0.697s, 1468.11/s  (0.695s, 1473.82/s)  LR: 5.386e-05  Data: 0.010 (0.015)
Train: 519 [ 450/1251 ( 36%)]  Loss: 2.975 (3.16)  Time: 0.699s, 1464.82/s  (0.694s, 1475.08/s)  LR: 5.386e-05  Data: 0.009 (0.015)
Train: 519 [ 500/1251 ( 40%)]  Loss: 3.206 (3.16)  Time: 0.674s, 1518.37/s  (0.694s, 1475.66/s)  LR: 5.386e-05  Data: 0.010 (0.014)
Train: 519 [ 550/1251 ( 44%)]  Loss: 3.622 (3.20)  Time: 0.685s, 1495.78/s  (0.693s, 1477.12/s)  LR: 5.386e-05  Data: 0.013 (0.014)
Train: 519 [ 600/1251 ( 48%)]  Loss: 3.466 (3.22)  Time: 0.704s, 1454.96/s  (0.694s, 1476.46/s)  LR: 5.386e-05  Data: 0.011 (0.014)
Train: 519 [ 650/1251 ( 52%)]  Loss: 3.387 (3.23)  Time: 0.691s, 1481.65/s  (0.693s, 1476.61/s)  LR: 5.386e-05  Data: 0.010 (0.013)
Train: 519 [ 700/1251 ( 56%)]  Loss: 3.388 (3.24)  Time: 0.670s, 1527.72/s  (0.693s, 1477.65/s)  LR: 5.386e-05  Data: 0.010 (0.013)
Train: 519 [ 750/1251 ( 60%)]  Loss: 3.285 (3.24)  Time: 0.718s, 1427.13/s  (0.693s, 1478.35/s)  LR: 5.386e-05  Data: 0.009 (0.013)
Train: 519 [ 800/1251 ( 64%)]  Loss: 3.142 (3.24)  Time: 0.700s, 1463.52/s  (0.693s, 1478.55/s)  LR: 5.386e-05  Data: 0.009 (0.013)
Train: 519 [ 850/1251 ( 68%)]  Loss: 3.205 (3.24)  Time: 0.672s, 1524.26/s  (0.692s, 1479.11/s)  LR: 5.386e-05  Data: 0.012 (0.013)
Train: 519 [ 900/1251 ( 72%)]  Loss: 3.088 (3.23)  Time: 0.670s, 1528.65/s  (0.692s, 1479.09/s)  LR: 5.386e-05  Data: 0.012 (0.013)
Train: 519 [ 950/1251 ( 76%)]  Loss: 3.703 (3.25)  Time: 0.666s, 1536.92/s  (0.692s, 1478.86/s)  LR: 5.386e-05  Data: 0.011 (0.012)
Train: 519 [1000/1251 ( 80%)]  Loss: 3.270 (3.25)  Time: 0.670s, 1528.31/s  (0.692s, 1479.24/s)  LR: 5.386e-05  Data: 0.010 (0.012)
Train: 519 [1050/1251 ( 84%)]  Loss: 3.470 (3.26)  Time: 0.750s, 1365.50/s  (0.692s, 1479.08/s)  LR: 5.386e-05  Data: 0.009 (0.012)
Train: 519 [1100/1251 ( 88%)]  Loss: 2.966 (3.25)  Time: 0.673s, 1520.78/s  (0.692s, 1479.60/s)  LR: 5.386e-05  Data: 0.009 (0.012)
Train: 519 [1150/1251 ( 92%)]  Loss: 3.664 (3.27)  Time: 0.671s, 1525.60/s  (0.692s, 1480.27/s)  LR: 5.386e-05  Data: 0.009 (0.012)
Train: 519 [1200/1251 ( 96%)]  Loss: 3.271 (3.27)  Time: 0.739s, 1385.65/s  (0.692s, 1480.25/s)  LR: 5.386e-05  Data: 0.015 (0.012)
Train: 519 [1250/1251 (100%)]  Loss: 3.181 (3.26)  Time: 0.659s, 1552.76/s  (0.692s, 1480.22/s)  LR: 5.386e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.469 (1.469)  Loss:  0.7241 (0.7241)  Acc@1: 91.1133 (91.1133)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.137 (0.584)  Loss:  0.8613 (1.2056)  Acc@1: 85.9670 (78.3580)  Acc@5: 96.5802 (94.1700)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-514.pth.tar', 78.50200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-518.pth.tar', 78.48800000244141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-512.pth.tar', 78.44800021240235)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-505.pth.tar', 78.41600011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-510.pth.tar', 78.40199997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-513.pth.tar', 78.39599997802735)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-515.pth.tar', 78.38200008300781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-519.pth.tar', 78.35800010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-516.pth.tar', 78.30400005371094)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-508.pth.tar', 78.28200002929688)

Train: 520 [   0/1251 (  0%)]  Loss: 3.203 (3.20)  Time: 2.164s,  473.25/s  (2.164s,  473.25/s)  LR: 5.279e-05  Data: 1.550 (1.550)
Train: 520 [  50/1251 (  4%)]  Loss: 3.338 (3.27)  Time: 0.670s, 1529.32/s  (0.726s, 1411.28/s)  LR: 5.279e-05  Data: 0.012 (0.045)
Train: 520 [ 100/1251 (  8%)]  Loss: 3.365 (3.30)  Time: 0.679s, 1507.30/s  (0.715s, 1432.15/s)  LR: 5.279e-05  Data: 0.010 (0.028)
Train: 520 [ 150/1251 ( 12%)]  Loss: 2.564 (3.12)  Time: 0.677s, 1511.66/s  (0.707s, 1448.38/s)  LR: 5.279e-05  Data: 0.011 (0.022)
Train: 520 [ 200/1251 ( 16%)]  Loss: 3.237 (3.14)  Time: 0.677s, 1512.76/s  (0.703s, 1456.90/s)  LR: 5.279e-05  Data: 0.011 (0.019)
Train: 520 [ 250/1251 ( 20%)]  Loss: 3.552 (3.21)  Time: 0.711s, 1440.07/s  (0.701s, 1460.37/s)  LR: 5.279e-05  Data: 0.010 (0.018)
Train: 520 [ 300/1251 ( 24%)]  Loss: 3.494 (3.25)  Time: 0.700s, 1462.54/s  (0.699s, 1464.12/s)  LR: 5.279e-05  Data: 0.009 (0.016)
Train: 520 [ 350/1251 ( 28%)]  Loss: 3.332 (3.26)  Time: 0.675s, 1516.02/s  (0.697s, 1468.12/s)  LR: 5.279e-05  Data: 0.011 (0.015)
Train: 520 [ 400/1251 ( 32%)]  Loss: 3.078 (3.24)  Time: 0.671s, 1526.98/s  (0.697s, 1469.69/s)  LR: 5.279e-05  Data: 0.010 (0.015)
Train: 520 [ 450/1251 ( 36%)]  Loss: 2.780 (3.19)  Time: 0.696s, 1470.97/s  (0.697s, 1468.66/s)  LR: 5.279e-05  Data: 0.009 (0.014)
Train: 520 [ 500/1251 ( 40%)]  Loss: 3.308 (3.20)  Time: 0.709s, 1443.76/s  (0.697s, 1468.69/s)  LR: 5.279e-05  Data: 0.010 (0.014)
Train: 520 [ 550/1251 ( 44%)]  Loss: 3.016 (3.19)  Time: 0.671s, 1525.76/s  (0.697s, 1468.90/s)  LR: 5.279e-05  Data: 0.012 (0.014)
Train: 520 [ 600/1251 ( 48%)]  Loss: 3.659 (3.23)  Time: 0.674s, 1519.64/s  (0.696s, 1470.43/s)  LR: 5.279e-05  Data: 0.011 (0.013)
Train: 520 [ 650/1251 ( 52%)]  Loss: 3.263 (3.23)  Time: 0.729s, 1404.16/s  (0.696s, 1470.53/s)  LR: 5.279e-05  Data: 0.009 (0.013)
Train: 520 [ 700/1251 ( 56%)]  Loss: 3.312 (3.23)  Time: 0.724s, 1414.76/s  (0.697s, 1469.96/s)  LR: 5.279e-05  Data: 0.010 (0.013)
Train: 520 [ 750/1251 ( 60%)]  Loss: 3.275 (3.24)  Time: 0.673s, 1522.27/s  (0.696s, 1470.28/s)  LR: 5.279e-05  Data: 0.009 (0.013)
Train: 520 [ 800/1251 ( 64%)]  Loss: 3.155 (3.23)  Time: 0.674s, 1518.76/s  (0.696s, 1470.33/s)  LR: 5.279e-05  Data: 0.010 (0.013)
Train: 520 [ 850/1251 ( 68%)]  Loss: 3.476 (3.24)  Time: 0.684s, 1496.80/s  (0.696s, 1470.60/s)  LR: 5.279e-05  Data: 0.015 (0.013)
Train: 520 [ 900/1251 ( 72%)]  Loss: 3.150 (3.24)  Time: 0.673s, 1521.70/s  (0.696s, 1471.17/s)  LR: 5.279e-05  Data: 0.014 (0.012)
Train: 520 [ 950/1251 ( 76%)]  Loss: 3.325 (3.24)  Time: 0.670s, 1527.69/s  (0.696s, 1471.85/s)  LR: 5.279e-05  Data: 0.011 (0.012)
Train: 520 [1000/1251 ( 80%)]  Loss: 3.437 (3.25)  Time: 0.716s, 1429.54/s  (0.695s, 1472.86/s)  LR: 5.279e-05  Data: 0.011 (0.012)
Train: 520 [1050/1251 ( 84%)]  Loss: 3.412 (3.26)  Time: 0.672s, 1524.68/s  (0.695s, 1472.46/s)  LR: 5.279e-05  Data: 0.009 (0.012)
Train: 520 [1100/1251 ( 88%)]  Loss: 3.010 (3.25)  Time: 0.682s, 1502.47/s  (0.695s, 1473.19/s)  LR: 5.279e-05  Data: 0.011 (0.012)
Train: 520 [1150/1251 ( 92%)]  Loss: 3.110 (3.24)  Time: 0.676s, 1515.43/s  (0.695s, 1473.32/s)  LR: 5.279e-05  Data: 0.010 (0.012)
Train: 520 [1200/1251 ( 96%)]  Loss: 2.755 (3.22)  Time: 0.694s, 1475.99/s  (0.695s, 1473.63/s)  LR: 5.279e-05  Data: 0.011 (0.012)
Train: 520 [1250/1251 (100%)]  Loss: 3.143 (3.22)  Time: 0.656s, 1561.41/s  (0.695s, 1474.41/s)  LR: 5.279e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.613 (1.613)  Loss:  0.7339 (0.7339)  Acc@1: 90.6250 (90.6250)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.136 (0.584)  Loss:  0.8813 (1.2368)  Acc@1: 85.8491 (78.1900)  Acc@5: 97.1698 (94.0840)
Train: 521 [   0/1251 (  0%)]  Loss: 3.574 (3.57)  Time: 2.198s,  465.97/s  (2.198s,  465.97/s)  LR: 5.175e-05  Data: 1.560 (1.560)
Train: 521 [  50/1251 (  4%)]  Loss: 2.849 (3.21)  Time: 0.713s, 1436.95/s  (0.730s, 1403.45/s)  LR: 5.175e-05  Data: 0.010 (0.050)
Train: 521 [ 100/1251 (  8%)]  Loss: 3.166 (3.20)  Time: 0.700s, 1463.07/s  (0.710s, 1441.98/s)  LR: 5.175e-05  Data: 0.010 (0.030)
Train: 521 [ 150/1251 ( 12%)]  Loss: 3.113 (3.18)  Time: 0.671s, 1527.03/s  (0.706s, 1450.30/s)  LR: 5.175e-05  Data: 0.009 (0.024)
Train: 521 [ 200/1251 ( 16%)]  Loss: 3.101 (3.16)  Time: 0.700s, 1463.33/s  (0.704s, 1455.43/s)  LR: 5.175e-05  Data: 0.009 (0.021)
Train: 521 [ 250/1251 ( 20%)]  Loss: 3.362 (3.19)  Time: 0.706s, 1450.12/s  (0.702s, 1458.50/s)  LR: 5.175e-05  Data: 0.010 (0.019)
Train: 521 [ 300/1251 ( 24%)]  Loss: 3.192 (3.19)  Time: 0.668s, 1533.10/s  (0.700s, 1462.13/s)  LR: 5.175e-05  Data: 0.009 (0.017)
Train: 521 [ 350/1251 ( 28%)]  Loss: 3.165 (3.19)  Time: 0.697s, 1470.13/s  (0.699s, 1465.13/s)  LR: 5.175e-05  Data: 0.010 (0.016)
Train: 521 [ 400/1251 ( 32%)]  Loss: 3.023 (3.17)  Time: 0.701s, 1460.30/s  (0.698s, 1466.26/s)  LR: 5.175e-05  Data: 0.010 (0.016)
Train: 521 [ 450/1251 ( 36%)]  Loss: 3.411 (3.20)  Time: 0.666s, 1536.98/s  (0.697s, 1468.26/s)  LR: 5.175e-05  Data: 0.009 (0.015)
Train: 521 [ 500/1251 ( 40%)]  Loss: 3.340 (3.21)  Time: 0.670s, 1528.38/s  (0.697s, 1468.92/s)  LR: 5.175e-05  Data: 0.010 (0.015)
Train: 521 [ 550/1251 ( 44%)]  Loss: 3.253 (3.21)  Time: 0.703s, 1457.32/s  (0.697s, 1468.63/s)  LR: 5.175e-05  Data: 0.009 (0.014)
Train: 521 [ 600/1251 ( 48%)]  Loss: 2.997 (3.20)  Time: 0.818s, 1252.14/s  (0.697s, 1469.39/s)  LR: 5.175e-05  Data: 0.010 (0.014)
Train: 521 [ 650/1251 ( 52%)]  Loss: 3.140 (3.19)  Time: 0.699s, 1464.95/s  (0.697s, 1469.41/s)  LR: 5.175e-05  Data: 0.009 (0.014)
Train: 521 [ 700/1251 ( 56%)]  Loss: 3.567 (3.22)  Time: 0.686s, 1493.25/s  (0.697s, 1469.35/s)  LR: 5.175e-05  Data: 0.016 (0.013)
Train: 521 [ 750/1251 ( 60%)]  Loss: 3.252 (3.22)  Time: 0.667s, 1534.24/s  (0.696s, 1470.29/s)  LR: 5.175e-05  Data: 0.009 (0.013)
Train: 521 [ 800/1251 ( 64%)]  Loss: 3.291 (3.22)  Time: 0.673s, 1520.72/s  (0.696s, 1470.82/s)  LR: 5.175e-05  Data: 0.009 (0.013)
Train: 521 [ 850/1251 ( 68%)]  Loss: 3.226 (3.22)  Time: 0.671s, 1526.84/s  (0.696s, 1470.98/s)  LR: 5.175e-05  Data: 0.010 (0.013)
Train: 521 [ 900/1251 ( 72%)]  Loss: 3.479 (3.24)  Time: 0.695s, 1473.81/s  (0.696s, 1470.31/s)  LR: 5.175e-05  Data: 0.009 (0.013)
Train: 521 [ 950/1251 ( 76%)]  Loss: 3.154 (3.23)  Time: 0.674s, 1520.13/s  (0.697s, 1469.91/s)  LR: 5.175e-05  Data: 0.010 (0.013)
Train: 521 [1000/1251 ( 80%)]  Loss: 3.603 (3.25)  Time: 0.677s, 1513.05/s  (0.696s, 1470.81/s)  LR: 5.175e-05  Data: 0.010 (0.013)
Train: 521 [1050/1251 ( 84%)]  Loss: 3.184 (3.25)  Time: 0.674s, 1520.06/s  (0.696s, 1471.55/s)  LR: 5.175e-05  Data: 0.010 (0.012)
Train: 521 [1100/1251 ( 88%)]  Loss: 3.466 (3.26)  Time: 0.705s, 1451.82/s  (0.696s, 1471.82/s)  LR: 5.175e-05  Data: 0.010 (0.012)
Train: 521 [1150/1251 ( 92%)]  Loss: 3.402 (3.26)  Time: 0.673s, 1522.40/s  (0.695s, 1472.46/s)  LR: 5.175e-05  Data: 0.010 (0.012)
Train: 521 [1200/1251 ( 96%)]  Loss: 3.669 (3.28)  Time: 0.695s, 1473.10/s  (0.695s, 1472.74/s)  LR: 5.175e-05  Data: 0.010 (0.012)
Train: 521 [1250/1251 (100%)]  Loss: 3.489 (3.29)  Time: 0.701s, 1461.17/s  (0.695s, 1472.64/s)  LR: 5.175e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.525 (1.525)  Loss:  0.7549 (0.7549)  Acc@1: 91.5039 (91.5039)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.136 (0.590)  Loss:  0.8408 (1.2030)  Acc@1: 86.5566 (78.4740)  Acc@5: 96.8160 (94.2260)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-514.pth.tar', 78.50200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-518.pth.tar', 78.48800000244141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-521.pth.tar', 78.47399997802735)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-512.pth.tar', 78.44800021240235)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-505.pth.tar', 78.41600011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-510.pth.tar', 78.40199997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-513.pth.tar', 78.39599997802735)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-515.pth.tar', 78.38200008300781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-519.pth.tar', 78.35800010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-516.pth.tar', 78.30400005371094)

Train: 522 [   0/1251 (  0%)]  Loss: 3.005 (3.01)  Time: 2.310s,  443.25/s  (2.310s,  443.25/s)  LR: 5.071e-05  Data: 1.695 (1.695)
Train: 522 [  50/1251 (  4%)]  Loss: 3.055 (3.03)  Time: 0.672s, 1522.98/s  (0.727s, 1409.00/s)  LR: 5.071e-05  Data: 0.011 (0.049)
Train: 522 [ 100/1251 (  8%)]  Loss: 2.776 (2.95)  Time: 0.728s, 1406.21/s  (0.709s, 1444.23/s)  LR: 5.071e-05  Data: 0.011 (0.030)
Train: 522 [ 150/1251 ( 12%)]  Loss: 3.593 (3.11)  Time: 0.673s, 1522.04/s  (0.706s, 1451.11/s)  LR: 5.071e-05  Data: 0.011 (0.023)
Train: 522 [ 200/1251 ( 16%)]  Loss: 2.821 (3.05)  Time: 0.673s, 1521.91/s  (0.702s, 1458.05/s)  LR: 5.071e-05  Data: 0.010 (0.020)
Train: 522 [ 250/1251 ( 20%)]  Loss: 3.401 (3.11)  Time: 0.677s, 1513.54/s  (0.700s, 1461.91/s)  LR: 5.071e-05  Data: 0.010 (0.018)
Train: 522 [ 300/1251 ( 24%)]  Loss: 3.544 (3.17)  Time: 0.704s, 1454.54/s  (0.699s, 1464.20/s)  LR: 5.071e-05  Data: 0.009 (0.017)
Train: 522 [ 350/1251 ( 28%)]  Loss: 3.007 (3.15)  Time: 0.685s, 1495.54/s  (0.699s, 1464.37/s)  LR: 5.071e-05  Data: 0.010 (0.016)
Train: 522 [ 400/1251 ( 32%)]  Loss: 3.172 (3.15)  Time: 0.672s, 1523.58/s  (0.698s, 1466.43/s)  LR: 5.071e-05  Data: 0.011 (0.015)
Train: 522 [ 450/1251 ( 36%)]  Loss: 3.395 (3.18)  Time: 0.696s, 1470.46/s  (0.699s, 1465.87/s)  LR: 5.071e-05  Data: 0.009 (0.015)
Train: 522 [ 500/1251 ( 40%)]  Loss: 3.298 (3.19)  Time: 0.671s, 1526.92/s  (0.698s, 1466.91/s)  LR: 5.071e-05  Data: 0.011 (0.014)
Train: 522 [ 550/1251 ( 44%)]  Loss: 3.354 (3.20)  Time: 0.714s, 1434.04/s  (0.698s, 1467.58/s)  LR: 5.071e-05  Data: 0.011 (0.014)
Train: 522 [ 600/1251 ( 48%)]  Loss: 3.066 (3.19)  Time: 0.680s, 1505.33/s  (0.697s, 1469.30/s)  LR: 5.071e-05  Data: 0.009 (0.014)
Train: 522 [ 650/1251 ( 52%)]  Loss: 3.332 (3.20)  Time: 0.673s, 1521.05/s  (0.696s, 1470.72/s)  LR: 5.071e-05  Data: 0.011 (0.013)
Train: 522 [ 700/1251 ( 56%)]  Loss: 3.065 (3.19)  Time: 0.672s, 1524.64/s  (0.696s, 1471.29/s)  LR: 5.071e-05  Data: 0.011 (0.013)
Train: 522 [ 750/1251 ( 60%)]  Loss: 3.167 (3.19)  Time: 0.670s, 1528.30/s  (0.695s, 1472.34/s)  LR: 5.071e-05  Data: 0.011 (0.013)
Train: 522 [ 800/1251 ( 64%)]  Loss: 3.327 (3.20)  Time: 0.676s, 1515.34/s  (0.695s, 1472.79/s)  LR: 5.071e-05  Data: 0.011 (0.013)
Train: 522 [ 850/1251 ( 68%)]  Loss: 3.339 (3.21)  Time: 0.727s, 1407.69/s  (0.695s, 1472.45/s)  LR: 5.071e-05  Data: 0.009 (0.013)
Train: 522 [ 900/1251 ( 72%)]  Loss: 3.153 (3.20)  Time: 0.703s, 1456.37/s  (0.695s, 1473.20/s)  LR: 5.071e-05  Data: 0.009 (0.013)
Train: 522 [ 950/1251 ( 76%)]  Loss: 2.741 (3.18)  Time: 0.702s, 1457.78/s  (0.695s, 1474.01/s)  LR: 5.071e-05  Data: 0.010 (0.012)
Train: 522 [1000/1251 ( 80%)]  Loss: 3.029 (3.17)  Time: 0.677s, 1512.30/s  (0.694s, 1474.61/s)  LR: 5.071e-05  Data: 0.011 (0.012)
Train: 522 [1050/1251 ( 84%)]  Loss: 3.538 (3.19)  Time: 0.676s, 1514.88/s  (0.694s, 1475.59/s)  LR: 5.071e-05  Data: 0.015 (0.012)
Train: 522 [1100/1251 ( 88%)]  Loss: 3.373 (3.20)  Time: 0.670s, 1528.14/s  (0.694s, 1475.23/s)  LR: 5.071e-05  Data: 0.010 (0.012)
Train: 522 [1150/1251 ( 92%)]  Loss: 3.514 (3.21)  Time: 0.702s, 1457.93/s  (0.694s, 1475.38/s)  LR: 5.071e-05  Data: 0.010 (0.012)
Train: 522 [1200/1251 ( 96%)]  Loss: 2.849 (3.20)  Time: 0.729s, 1404.37/s  (0.694s, 1475.49/s)  LR: 5.071e-05  Data: 0.010 (0.012)
Train: 522 [1250/1251 (100%)]  Loss: 3.056 (3.19)  Time: 0.688s, 1489.14/s  (0.694s, 1475.50/s)  LR: 5.071e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.581 (1.581)  Loss:  0.7324 (0.7324)  Acc@1: 91.7969 (91.7969)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.136 (0.589)  Loss:  0.8555 (1.2238)  Acc@1: 86.2028 (78.2820)  Acc@5: 96.8160 (94.1060)
Train: 523 [   0/1251 (  0%)]  Loss: 3.320 (3.32)  Time: 2.028s,  504.96/s  (2.028s,  504.96/s)  LR: 4.969e-05  Data: 1.402 (1.402)
Train: 523 [  50/1251 (  4%)]  Loss: 2.960 (3.14)  Time: 0.667s, 1535.56/s  (0.723s, 1415.60/s)  LR: 4.969e-05  Data: 0.010 (0.045)
Train: 523 [ 100/1251 (  8%)]  Loss: 3.335 (3.21)  Time: 0.674s, 1520.10/s  (0.707s, 1448.31/s)  LR: 4.969e-05  Data: 0.009 (0.028)
Train: 523 [ 150/1251 ( 12%)]  Loss: 3.420 (3.26)  Time: 0.779s, 1314.71/s  (0.703s, 1457.54/s)  LR: 4.969e-05  Data: 0.012 (0.022)
Train: 523 [ 200/1251 ( 16%)]  Loss: 3.263 (3.26)  Time: 0.688s, 1487.79/s  (0.700s, 1462.99/s)  LR: 4.969e-05  Data: 0.010 (0.019)
Train: 523 [ 250/1251 ( 20%)]  Loss: 3.553 (3.31)  Time: 0.673s, 1522.32/s  (0.698s, 1466.66/s)  LR: 4.969e-05  Data: 0.011 (0.017)
Train: 523 [ 300/1251 ( 24%)]  Loss: 3.062 (3.27)  Time: 0.672s, 1523.71/s  (0.696s, 1471.24/s)  LR: 4.969e-05  Data: 0.010 (0.016)
Train: 523 [ 350/1251 ( 28%)]  Loss: 2.826 (3.22)  Time: 0.665s, 1539.55/s  (0.695s, 1474.23/s)  LR: 4.969e-05  Data: 0.010 (0.015)
Train: 523 [ 400/1251 ( 32%)]  Loss: 3.133 (3.21)  Time: 0.672s, 1523.50/s  (0.695s, 1473.92/s)  LR: 4.969e-05  Data: 0.010 (0.015)
Train: 523 [ 450/1251 ( 36%)]  Loss: 3.218 (3.21)  Time: 0.707s, 1448.87/s  (0.694s, 1475.53/s)  LR: 4.969e-05  Data: 0.010 (0.014)
Train: 523 [ 500/1251 ( 40%)]  Loss: 3.214 (3.21)  Time: 0.741s, 1382.85/s  (0.694s, 1475.76/s)  LR: 4.969e-05  Data: 0.009 (0.014)
Train: 523 [ 550/1251 ( 44%)]  Loss: 3.424 (3.23)  Time: 0.700s, 1463.63/s  (0.694s, 1475.65/s)  LR: 4.969e-05  Data: 0.009 (0.013)
Train: 523 [ 600/1251 ( 48%)]  Loss: 3.209 (3.23)  Time: 0.672s, 1524.09/s  (0.694s, 1476.38/s)  LR: 4.969e-05  Data: 0.009 (0.013)
Train: 523 [ 650/1251 ( 52%)]  Loss: 3.456 (3.24)  Time: 0.704s, 1454.55/s  (0.693s, 1477.55/s)  LR: 4.969e-05  Data: 0.010 (0.013)
Train: 523 [ 700/1251 ( 56%)]  Loss: 3.212 (3.24)  Time: 0.670s, 1528.55/s  (0.693s, 1478.21/s)  LR: 4.969e-05  Data: 0.010 (0.013)
Train: 523 [ 750/1251 ( 60%)]  Loss: 3.281 (3.24)  Time: 0.696s, 1471.05/s  (0.693s, 1477.64/s)  LR: 4.969e-05  Data: 0.010 (0.013)
Train: 523 [ 800/1251 ( 64%)]  Loss: 2.654 (3.21)  Time: 0.687s, 1490.86/s  (0.693s, 1477.86/s)  LR: 4.969e-05  Data: 0.010 (0.012)
Train: 523 [ 850/1251 ( 68%)]  Loss: 3.579 (3.23)  Time: 0.671s, 1525.93/s  (0.693s, 1478.46/s)  LR: 4.969e-05  Data: 0.012 (0.012)
Train: 523 [ 900/1251 ( 72%)]  Loss: 3.525 (3.24)  Time: 0.718s, 1425.21/s  (0.693s, 1477.18/s)  LR: 4.969e-05  Data: 0.009 (0.012)
Train: 523 [ 950/1251 ( 76%)]  Loss: 3.225 (3.24)  Time: 0.673s, 1520.81/s  (0.693s, 1477.11/s)  LR: 4.969e-05  Data: 0.009 (0.012)
Train: 523 [1000/1251 ( 80%)]  Loss: 3.084 (3.24)  Time: 0.688s, 1489.00/s  (0.694s, 1476.42/s)  LR: 4.969e-05  Data: 0.019 (0.012)
Train: 523 [1050/1251 ( 84%)]  Loss: 3.059 (3.23)  Time: 0.800s, 1280.73/s  (0.694s, 1476.36/s)  LR: 4.969e-05  Data: 0.011 (0.012)
Train: 523 [1100/1251 ( 88%)]  Loss: 3.496 (3.24)  Time: 0.707s, 1449.03/s  (0.694s, 1475.49/s)  LR: 4.969e-05  Data: 0.011 (0.012)
Train: 523 [1150/1251 ( 92%)]  Loss: 3.237 (3.24)  Time: 0.699s, 1465.97/s  (0.694s, 1476.18/s)  LR: 4.969e-05  Data: 0.010 (0.012)
Train: 523 [1200/1251 ( 96%)]  Loss: 3.256 (3.24)  Time: 0.673s, 1520.49/s  (0.693s, 1476.86/s)  LR: 4.969e-05  Data: 0.010 (0.012)
Train: 523 [1250/1251 (100%)]  Loss: 2.959 (3.23)  Time: 0.714s, 1434.37/s  (0.693s, 1476.97/s)  LR: 4.969e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.472 (1.472)  Loss:  0.7222 (0.7222)  Acc@1: 91.5039 (91.5039)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.136 (0.588)  Loss:  0.8213 (1.1885)  Acc@1: 87.1462 (78.6940)  Acc@5: 97.0519 (94.2040)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-523.pth.tar', 78.69399997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-514.pth.tar', 78.50200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-518.pth.tar', 78.48800000244141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-521.pth.tar', 78.47399997802735)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-512.pth.tar', 78.44800021240235)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-505.pth.tar', 78.41600011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-510.pth.tar', 78.40199997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-513.pth.tar', 78.39599997802735)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-515.pth.tar', 78.38200008300781)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-519.pth.tar', 78.35800010986328)

Train: 524 [   0/1251 (  0%)]  Loss: 3.248 (3.25)  Time: 2.264s,  452.33/s  (2.264s,  452.33/s)  LR: 4.868e-05  Data: 1.648 (1.648)
Train: 524 [  50/1251 (  4%)]  Loss: 3.106 (3.18)  Time: 0.672s, 1523.76/s  (0.730s, 1402.36/s)  LR: 4.868e-05  Data: 0.010 (0.051)
Train: 524 [ 100/1251 (  8%)]  Loss: 3.534 (3.30)  Time: 0.672s, 1524.34/s  (0.712s, 1438.51/s)  LR: 4.868e-05  Data: 0.012 (0.031)
Train: 524 [ 150/1251 ( 12%)]  Loss: 3.179 (3.27)  Time: 0.725s, 1412.14/s  (0.705s, 1453.10/s)  LR: 4.868e-05  Data: 0.023 (0.024)
Train: 524 [ 200/1251 ( 16%)]  Loss: 3.209 (3.26)  Time: 0.674s, 1520.37/s  (0.702s, 1457.84/s)  LR: 4.868e-05  Data: 0.011 (0.021)
Train: 524 [ 250/1251 ( 20%)]  Loss: 3.247 (3.25)  Time: 0.712s, 1438.85/s  (0.701s, 1459.89/s)  LR: 4.868e-05  Data: 0.016 (0.019)
Train: 524 [ 300/1251 ( 24%)]  Loss: 3.424 (3.28)  Time: 0.699s, 1464.13/s  (0.699s, 1465.17/s)  LR: 4.868e-05  Data: 0.011 (0.017)
Train: 524 [ 350/1251 ( 28%)]  Loss: 3.310 (3.28)  Time: 0.691s, 1482.84/s  (0.698s, 1467.02/s)  LR: 4.868e-05  Data: 0.008 (0.016)
Train: 524 [ 400/1251 ( 32%)]  Loss: 3.166 (3.27)  Time: 0.672s, 1522.88/s  (0.697s, 1468.89/s)  LR: 4.868e-05  Data: 0.012 (0.015)
Train: 524 [ 450/1251 ( 36%)]  Loss: 3.335 (3.28)  Time: 0.673s, 1521.85/s  (0.696s, 1471.08/s)  LR: 4.868e-05  Data: 0.011 (0.015)
Train: 524 [ 500/1251 ( 40%)]  Loss: 3.654 (3.31)  Time: 0.703s, 1456.16/s  (0.695s, 1473.77/s)  LR: 4.868e-05  Data: 0.009 (0.014)
Train: 524 [ 550/1251 ( 44%)]  Loss: 3.828 (3.35)  Time: 0.718s, 1425.83/s  (0.695s, 1473.48/s)  LR: 4.868e-05  Data: 0.010 (0.014)
Train: 524 [ 600/1251 ( 48%)]  Loss: 3.562 (3.37)  Time: 0.697s, 1468.82/s  (0.694s, 1475.63/s)  LR: 4.868e-05  Data: 0.010 (0.014)
Train: 524 [ 650/1251 ( 52%)]  Loss: 3.293 (3.36)  Time: 0.711s, 1441.05/s  (0.694s, 1475.15/s)  LR: 4.868e-05  Data: 0.010 (0.013)
Train: 524 [ 700/1251 ( 56%)]  Loss: 3.205 (3.35)  Time: 0.668s, 1531.87/s  (0.694s, 1475.01/s)  LR: 4.868e-05  Data: 0.010 (0.013)
Train: 524 [ 750/1251 ( 60%)]  Loss: 3.204 (3.34)  Time: 0.759s, 1348.85/s  (0.694s, 1474.47/s)  LR: 4.868e-05  Data: 0.009 (0.013)
Train: 524 [ 800/1251 ( 64%)]  Loss: 3.243 (3.34)  Time: 0.690s, 1484.90/s  (0.695s, 1474.06/s)  LR: 4.868e-05  Data: 0.009 (0.013)
Train: 524 [ 850/1251 ( 68%)]  Loss: 3.350 (3.34)  Time: 0.673s, 1521.22/s  (0.695s, 1474.16/s)  LR: 4.868e-05  Data: 0.010 (0.013)
Train: 524 [ 900/1251 ( 72%)]  Loss: 3.486 (3.35)  Time: 0.675s, 1517.74/s  (0.694s, 1475.09/s)  LR: 4.868e-05  Data: 0.011 (0.013)
Train: 524 [ 950/1251 ( 76%)]  Loss: 3.396 (3.35)  Time: 0.689s, 1486.95/s  (0.694s, 1476.03/s)  LR: 4.868e-05  Data: 0.010 (0.012)
Train: 524 [1000/1251 ( 80%)]  Loss: 3.379 (3.35)  Time: 0.672s, 1523.78/s  (0.693s, 1476.64/s)  LR: 4.868e-05  Data: 0.010 (0.012)
Train: 524 [1050/1251 ( 84%)]  Loss: 3.368 (3.35)  Time: 0.694s, 1476.12/s  (0.693s, 1476.93/s)  LR: 4.868e-05  Data: 0.014 (0.012)
Train: 524 [1100/1251 ( 88%)]  Loss: 2.921 (3.33)  Time: 0.677s, 1511.45/s  (0.694s, 1476.33/s)  LR: 4.868e-05  Data: 0.010 (0.012)
Train: 524 [1150/1251 ( 92%)]  Loss: 3.077 (3.32)  Time: 0.682s, 1501.68/s  (0.694s, 1475.81/s)  LR: 4.868e-05  Data: 0.011 (0.012)
Train: 524 [1200/1251 ( 96%)]  Loss: 3.143 (3.31)  Time: 0.671s, 1526.37/s  (0.694s, 1475.49/s)  LR: 4.868e-05  Data: 0.011 (0.012)
Train: 524 [1250/1251 (100%)]  Loss: 3.202 (3.31)  Time: 0.656s, 1561.17/s  (0.694s, 1475.70/s)  LR: 4.868e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.616 (1.616)  Loss:  0.6733 (0.6733)  Acc@1: 91.5039 (91.5039)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.136 (0.582)  Loss:  0.7944 (1.1842)  Acc@1: 86.4387 (78.6660)  Acc@5: 96.9340 (94.2060)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-523.pth.tar', 78.69399997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-524.pth.tar', 78.66600005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-514.pth.tar', 78.50200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-518.pth.tar', 78.48800000244141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-521.pth.tar', 78.47399997802735)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-512.pth.tar', 78.44800021240235)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-505.pth.tar', 78.41600011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-510.pth.tar', 78.40199997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-513.pth.tar', 78.39599997802735)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-515.pth.tar', 78.38200008300781)

Train: 525 [   0/1251 (  0%)]  Loss: 3.161 (3.16)  Time: 2.281s,  448.92/s  (2.281s,  448.92/s)  LR: 4.768e-05  Data: 1.665 (1.665)
Train: 525 [  50/1251 (  4%)]  Loss: 3.066 (3.11)  Time: 0.702s, 1459.41/s  (0.724s, 1415.31/s)  LR: 4.768e-05  Data: 0.009 (0.050)
Train: 525 [ 100/1251 (  8%)]  Loss: 3.314 (3.18)  Time: 0.669s, 1529.58/s  (0.704s, 1453.77/s)  LR: 4.768e-05  Data: 0.009 (0.030)
Train: 525 [ 150/1251 ( 12%)]  Loss: 3.222 (3.19)  Time: 0.672s, 1523.64/s  (0.700s, 1463.82/s)  LR: 4.768e-05  Data: 0.010 (0.024)
Train: 525 [ 200/1251 ( 16%)]  Loss: 3.161 (3.18)  Time: 0.676s, 1514.02/s  (0.700s, 1462.33/s)  LR: 4.768e-05  Data: 0.014 (0.020)
Train: 525 [ 250/1251 ( 20%)]  Loss: 3.094 (3.17)  Time: 0.699s, 1465.37/s  (0.699s, 1465.00/s)  LR: 4.768e-05  Data: 0.011 (0.018)
Train: 525 [ 300/1251 ( 24%)]  Loss: 3.482 (3.21)  Time: 0.713s, 1436.57/s  (0.699s, 1465.70/s)  LR: 4.768e-05  Data: 0.009 (0.017)
Train: 525 [ 350/1251 ( 28%)]  Loss: 3.018 (3.19)  Time: 0.673s, 1520.54/s  (0.697s, 1468.65/s)  LR: 4.768e-05  Data: 0.009 (0.016)
Train: 525 [ 400/1251 ( 32%)]  Loss: 3.524 (3.23)  Time: 0.674s, 1518.60/s  (0.696s, 1471.01/s)  LR: 4.768e-05  Data: 0.010 (0.015)
Train: 525 [ 450/1251 ( 36%)]  Loss: 3.184 (3.22)  Time: 0.701s, 1459.73/s  (0.695s, 1473.23/s)  LR: 4.768e-05  Data: 0.009 (0.015)
Train: 525 [ 500/1251 ( 40%)]  Loss: 3.360 (3.24)  Time: 0.719s, 1423.94/s  (0.695s, 1472.77/s)  LR: 4.768e-05  Data: 0.009 (0.014)
Train: 525 [ 550/1251 ( 44%)]  Loss: 3.419 (3.25)  Time: 0.700s, 1462.58/s  (0.696s, 1471.55/s)  LR: 4.768e-05  Data: 0.009 (0.014)
Train: 525 [ 600/1251 ( 48%)]  Loss: 3.071 (3.24)  Time: 0.671s, 1525.89/s  (0.696s, 1471.23/s)  LR: 4.768e-05  Data: 0.011 (0.014)
Train: 525 [ 650/1251 ( 52%)]  Loss: 3.045 (3.22)  Time: 0.671s, 1527.09/s  (0.696s, 1471.26/s)  LR: 4.768e-05  Data: 0.010 (0.013)
Train: 525 [ 700/1251 ( 56%)]  Loss: 3.136 (3.22)  Time: 0.679s, 1507.33/s  (0.695s, 1472.36/s)  LR: 4.768e-05  Data: 0.016 (0.013)
Train: 525 [ 750/1251 ( 60%)]  Loss: 3.578 (3.24)  Time: 0.671s, 1526.85/s  (0.695s, 1473.41/s)  LR: 4.768e-05  Data: 0.010 (0.013)
Train: 525 [ 800/1251 ( 64%)]  Loss: 3.247 (3.24)  Time: 0.714s, 1433.90/s  (0.695s, 1474.44/s)  LR: 4.768e-05  Data: 0.010 (0.013)
Train: 525 [ 850/1251 ( 68%)]  Loss: 2.804 (3.22)  Time: 0.691s, 1480.94/s  (0.694s, 1474.62/s)  LR: 4.768e-05  Data: 0.011 (0.013)
Train: 525 [ 900/1251 ( 72%)]  Loss: 3.457 (3.23)  Time: 0.671s, 1526.84/s  (0.695s, 1474.37/s)  LR: 4.768e-05  Data: 0.014 (0.013)
Train: 525 [ 950/1251 ( 76%)]  Loss: 3.325 (3.23)  Time: 0.705s, 1453.37/s  (0.694s, 1474.55/s)  LR: 4.768e-05  Data: 0.009 (0.013)
Train: 525 [1000/1251 ( 80%)]  Loss: 2.798 (3.21)  Time: 0.704s, 1455.52/s  (0.694s, 1475.20/s)  LR: 4.768e-05  Data: 0.009 (0.012)
Train: 525 [1050/1251 ( 84%)]  Loss: 3.439 (3.22)  Time: 0.674s, 1519.05/s  (0.694s, 1475.93/s)  LR: 4.768e-05  Data: 0.013 (0.012)
Train: 525 [1100/1251 ( 88%)]  Loss: 3.385 (3.23)  Time: 0.713s, 1436.11/s  (0.694s, 1476.11/s)  LR: 4.768e-05  Data: 0.010 (0.012)
Train: 525 [1150/1251 ( 92%)]  Loss: 3.135 (3.23)  Time: 0.672s, 1522.68/s  (0.693s, 1477.01/s)  LR: 4.768e-05  Data: 0.009 (0.012)
Train: 525 [1200/1251 ( 96%)]  Loss: 2.899 (3.21)  Time: 0.713s, 1435.74/s  (0.693s, 1476.93/s)  LR: 4.768e-05  Data: 0.010 (0.012)
Train: 525 [1250/1251 (100%)]  Loss: 3.371 (3.22)  Time: 0.656s, 1559.88/s  (0.693s, 1476.78/s)  LR: 4.768e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.592 (1.592)  Loss:  0.6841 (0.6841)  Acc@1: 91.0156 (91.0156)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.136 (0.583)  Loss:  0.7324 (1.1634)  Acc@1: 87.2642 (78.5860)  Acc@5: 97.5236 (94.1920)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-523.pth.tar', 78.69399997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-524.pth.tar', 78.66600005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-525.pth.tar', 78.58600002685547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-514.pth.tar', 78.50200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-518.pth.tar', 78.48800000244141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-521.pth.tar', 78.47399997802735)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-512.pth.tar', 78.44800021240235)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-505.pth.tar', 78.41600011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-510.pth.tar', 78.40199997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-513.pth.tar', 78.39599997802735)

Train: 526 [   0/1251 (  0%)]  Loss: 3.187 (3.19)  Time: 2.160s,  474.02/s  (2.160s,  474.02/s)  LR: 4.669e-05  Data: 1.510 (1.510)
Train: 526 [  50/1251 (  4%)]  Loss: 3.165 (3.18)  Time: 0.673s, 1522.09/s  (0.728s, 1407.20/s)  LR: 4.669e-05  Data: 0.010 (0.049)
Train: 526 [ 100/1251 (  8%)]  Loss: 3.026 (3.13)  Time: 0.670s, 1527.66/s  (0.706s, 1449.47/s)  LR: 4.669e-05  Data: 0.009 (0.030)
Train: 526 [ 150/1251 ( 12%)]  Loss: 2.865 (3.06)  Time: 0.674s, 1519.09/s  (0.701s, 1459.92/s)  LR: 4.669e-05  Data: 0.013 (0.023)
Train: 526 [ 200/1251 ( 16%)]  Loss: 3.151 (3.08)  Time: 0.738s, 1388.18/s  (0.701s, 1461.23/s)  LR: 4.669e-05  Data: 0.009 (0.020)
Train: 526 [ 250/1251 ( 20%)]  Loss: 3.122 (3.09)  Time: 0.709s, 1443.78/s  (0.698s, 1466.35/s)  LR: 4.669e-05  Data: 0.011 (0.018)
Train: 526 [ 300/1251 ( 24%)]  Loss: 3.416 (3.13)  Time: 0.690s, 1483.20/s  (0.697s, 1469.14/s)  LR: 4.669e-05  Data: 0.011 (0.017)
Train: 526 [ 350/1251 ( 28%)]  Loss: 3.659 (3.20)  Time: 0.671s, 1524.96/s  (0.696s, 1470.51/s)  LR: 4.669e-05  Data: 0.010 (0.016)
Train: 526 [ 400/1251 ( 32%)]  Loss: 3.392 (3.22)  Time: 0.715s, 1432.20/s  (0.697s, 1469.81/s)  LR: 4.669e-05  Data: 0.010 (0.015)
Train: 526 [ 450/1251 ( 36%)]  Loss: 3.014 (3.20)  Time: 0.676s, 1514.17/s  (0.696s, 1471.28/s)  LR: 4.669e-05  Data: 0.011 (0.015)
Train: 526 [ 500/1251 ( 40%)]  Loss: 3.040 (3.19)  Time: 0.706s, 1450.84/s  (0.695s, 1472.67/s)  LR: 4.669e-05  Data: 0.010 (0.014)
Train: 526 [ 550/1251 ( 44%)]  Loss: 2.973 (3.17)  Time: 0.686s, 1492.74/s  (0.695s, 1472.61/s)  LR: 4.669e-05  Data: 0.011 (0.014)
Train: 526 [ 600/1251 ( 48%)]  Loss: 3.481 (3.19)  Time: 0.759s, 1348.51/s  (0.699s, 1464.06/s)  LR: 4.669e-05  Data: 0.011 (0.014)
Train: 526 [ 650/1251 ( 52%)]  Loss: 3.349 (3.20)  Time: 0.722s, 1418.12/s  (0.704s, 1453.92/s)  LR: 4.669e-05  Data: 0.016 (0.014)
Train: 526 [ 700/1251 ( 56%)]  Loss: 3.082 (3.19)  Time: 0.731s, 1401.18/s  (0.711s, 1441.17/s)  LR: 4.669e-05  Data: 0.013 (0.014)
Train: 526 [ 750/1251 ( 60%)]  Loss: 3.258 (3.20)  Time: 0.707s, 1449.28/s  (0.715s, 1433.05/s)  LR: 4.669e-05  Data: 0.012 (0.013)
Train: 526 [ 800/1251 ( 64%)]  Loss: 3.063 (3.19)  Time: 0.724s, 1414.74/s  (0.718s, 1426.36/s)  LR: 4.669e-05  Data: 0.018 (0.013)
Train: 526 [ 850/1251 ( 68%)]  Loss: 2.795 (3.17)  Time: 0.724s, 1414.03/s  (0.720s, 1422.00/s)  LR: 4.669e-05  Data: 0.012 (0.013)
Train: 526 [ 900/1251 ( 72%)]  Loss: 3.098 (3.17)  Time: 0.734s, 1394.92/s  (0.725s, 1413.11/s)  LR: 4.669e-05  Data: 0.013 (0.013)
Train: 526 [ 950/1251 ( 76%)]  Loss: 3.494 (3.18)  Time: 0.736s, 1390.92/s  (0.727s, 1408.86/s)  LR: 4.669e-05  Data: 0.010 (0.013)
Train: 526 [1000/1251 ( 80%)]  Loss: 2.889 (3.17)  Time: 0.709s, 1444.25/s  (0.727s, 1408.25/s)  LR: 4.669e-05  Data: 0.009 (0.013)
Train: 526 [1050/1251 ( 84%)]  Loss: 3.076 (3.16)  Time: 0.678s, 1510.11/s  (0.725s, 1411.62/s)  LR: 4.669e-05  Data: 0.011 (0.013)
Train: 526 [1100/1251 ( 88%)]  Loss: 2.934 (3.15)  Time: 0.719s, 1423.79/s  (0.724s, 1414.94/s)  LR: 4.669e-05  Data: 0.009 (0.013)
Train: 526 [1150/1251 ( 92%)]  Loss: 3.315 (3.16)  Time: 0.668s, 1532.48/s  (0.723s, 1416.29/s)  LR: 4.669e-05  Data: 0.011 (0.013)
Train: 526 [1200/1251 ( 96%)]  Loss: 3.413 (3.17)  Time: 0.671s, 1525.81/s  (0.722s, 1419.00/s)  LR: 4.669e-05  Data: 0.011 (0.013)
Train: 526 [1250/1251 (100%)]  Loss: 3.102 (3.17)  Time: 0.650s, 1574.56/s  (0.721s, 1420.46/s)  LR: 4.669e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.572 (1.572)  Loss:  0.7173 (0.7173)  Acc@1: 92.0898 (92.0898)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.136 (0.582)  Loss:  0.8555 (1.2145)  Acc@1: 85.9670 (78.6380)  Acc@5: 96.9340 (94.1520)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-523.pth.tar', 78.69399997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-524.pth.tar', 78.66600005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-526.pth.tar', 78.63800010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-525.pth.tar', 78.58600002685547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-514.pth.tar', 78.50200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-518.pth.tar', 78.48800000244141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-521.pth.tar', 78.47399997802735)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-512.pth.tar', 78.44800021240235)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-505.pth.tar', 78.41600011230469)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-510.pth.tar', 78.40199997558594)

Train: 527 [   0/1251 (  0%)]  Loss: 3.481 (3.48)  Time: 2.169s,  472.04/s  (2.169s,  472.04/s)  LR: 4.572e-05  Data: 1.554 (1.554)
Train: 527 [  50/1251 (  4%)]  Loss: 2.865 (3.17)  Time: 0.670s, 1527.46/s  (0.734s, 1394.52/s)  LR: 4.572e-05  Data: 0.010 (0.050)
Train: 527 [ 100/1251 (  8%)]  Loss: 3.219 (3.19)  Time: 0.672s, 1523.80/s  (0.715s, 1432.88/s)  LR: 4.572e-05  Data: 0.010 (0.031)
Train: 527 [ 150/1251 ( 12%)]  Loss: 3.694 (3.31)  Time: 0.670s, 1527.76/s  (0.708s, 1446.92/s)  LR: 4.572e-05  Data: 0.010 (0.024)
Train: 527 [ 200/1251 ( 16%)]  Loss: 3.234 (3.30)  Time: 0.707s, 1448.23/s  (0.705s, 1453.08/s)  LR: 4.572e-05  Data: 0.010 (0.021)
Train: 527 [ 250/1251 ( 20%)]  Loss: 3.315 (3.30)  Time: 0.720s, 1423.02/s  (0.704s, 1455.45/s)  LR: 4.572e-05  Data: 0.010 (0.019)
Train: 527 [ 300/1251 ( 24%)]  Loss: 3.592 (3.34)  Time: 0.671s, 1525.38/s  (0.701s, 1460.20/s)  LR: 4.572e-05  Data: 0.009 (0.017)
Train: 527 [ 350/1251 ( 28%)]  Loss: 3.358 (3.34)  Time: 0.780s, 1313.57/s  (0.700s, 1461.97/s)  LR: 4.572e-05  Data: 0.010 (0.016)
Train: 527 [ 400/1251 ( 32%)]  Loss: 3.118 (3.32)  Time: 0.736s, 1390.75/s  (0.702s, 1458.34/s)  LR: 4.572e-05  Data: 0.011 (0.016)
Train: 527 [ 450/1251 ( 36%)]  Loss: 3.144 (3.30)  Time: 0.755s, 1356.46/s  (0.703s, 1456.36/s)  LR: 4.572e-05  Data: 0.015 (0.015)
Train: 527 [ 500/1251 ( 40%)]  Loss: 3.055 (3.28)  Time: 0.705s, 1451.52/s  (0.703s, 1456.11/s)  LR: 4.572e-05  Data: 0.011 (0.015)
Train: 527 [ 550/1251 ( 44%)]  Loss: 3.734 (3.32)  Time: 0.726s, 1411.41/s  (0.703s, 1456.85/s)  LR: 4.572e-05  Data: 0.010 (0.015)
Train: 527 [ 600/1251 ( 48%)]  Loss: 3.520 (3.33)  Time: 0.706s, 1450.55/s  (0.702s, 1458.38/s)  LR: 4.572e-05  Data: 0.009 (0.014)
Train: 527 [ 650/1251 ( 52%)]  Loss: 3.615 (3.35)  Time: 0.703s, 1457.02/s  (0.700s, 1462.17/s)  LR: 4.572e-05  Data: 0.010 (0.014)
Train: 527 [ 700/1251 ( 56%)]  Loss: 2.864 (3.32)  Time: 0.681s, 1503.23/s  (0.700s, 1463.75/s)  LR: 4.572e-05  Data: 0.011 (0.014)
Train: 527 [ 750/1251 ( 60%)]  Loss: 3.229 (3.31)  Time: 0.671s, 1526.68/s  (0.699s, 1465.04/s)  LR: 4.572e-05  Data: 0.009 (0.014)
Train: 527 [ 800/1251 ( 64%)]  Loss: 3.170 (3.31)  Time: 0.719s, 1423.59/s  (0.698s, 1466.06/s)  LR: 4.572e-05  Data: 0.009 (0.013)
Train: 527 [ 850/1251 ( 68%)]  Loss: 2.949 (3.29)  Time: 0.670s, 1528.06/s  (0.698s, 1467.16/s)  LR: 4.572e-05  Data: 0.011 (0.013)
Train: 527 [ 900/1251 ( 72%)]  Loss: 3.302 (3.29)  Time: 0.722s, 1418.03/s  (0.698s, 1467.58/s)  LR: 4.572e-05  Data: 0.010 (0.013)
Train: 527 [ 950/1251 ( 76%)]  Loss: 3.427 (3.29)  Time: 0.698s, 1466.90/s  (0.698s, 1467.66/s)  LR: 4.572e-05  Data: 0.010 (0.013)
Train: 527 [1000/1251 ( 80%)]  Loss: 2.929 (3.28)  Time: 0.704s, 1453.90/s  (0.697s, 1468.21/s)  LR: 4.572e-05  Data: 0.010 (0.013)
Train: 527 [1050/1251 ( 84%)]  Loss: 3.446 (3.28)  Time: 0.749s, 1367.18/s  (0.698s, 1468.08/s)  LR: 4.572e-05  Data: 0.009 (0.013)
Train: 527 [1100/1251 ( 88%)]  Loss: 3.112 (3.28)  Time: 0.697s, 1469.39/s  (0.697s, 1468.45/s)  LR: 4.572e-05  Data: 0.010 (0.013)
Train: 527 [1150/1251 ( 92%)]  Loss: 3.523 (3.29)  Time: 0.671s, 1526.15/s  (0.697s, 1468.52/s)  LR: 4.572e-05  Data: 0.010 (0.013)
Train: 527 [1200/1251 ( 96%)]  Loss: 3.399 (3.29)  Time: 0.694s, 1474.61/s  (0.697s, 1469.01/s)  LR: 4.572e-05  Data: 0.010 (0.012)
Train: 527 [1250/1251 (100%)]  Loss: 3.013 (3.28)  Time: 0.657s, 1559.53/s  (0.697s, 1468.75/s)  LR: 4.572e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.588 (1.588)  Loss:  0.6357 (0.6357)  Acc@1: 91.0156 (91.0156)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.137 (0.591)  Loss:  0.7666 (1.1287)  Acc@1: 86.4387 (78.6600)  Acc@5: 97.1698 (94.1500)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-523.pth.tar', 78.69399997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-524.pth.tar', 78.66600005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-527.pth.tar', 78.66000005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-526.pth.tar', 78.63800010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-525.pth.tar', 78.58600002685547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-514.pth.tar', 78.50200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-518.pth.tar', 78.48800000244141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-521.pth.tar', 78.47399997802735)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-512.pth.tar', 78.44800021240235)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-505.pth.tar', 78.41600011230469)

Train: 528 [   0/1251 (  0%)]  Loss: 3.197 (3.20)  Time: 2.245s,  456.21/s  (2.245s,  456.21/s)  LR: 4.476e-05  Data: 1.629 (1.629)
Train: 528 [  50/1251 (  4%)]  Loss: 3.167 (3.18)  Time: 0.702s, 1458.83/s  (0.733s, 1397.65/s)  LR: 4.476e-05  Data: 0.008 (0.050)
Train: 528 [ 100/1251 (  8%)]  Loss: 3.337 (3.23)  Time: 0.733s, 1396.87/s  (0.716s, 1429.72/s)  LR: 4.476e-05  Data: 0.010 (0.030)
Train: 528 [ 150/1251 ( 12%)]  Loss: 3.253 (3.24)  Time: 0.721s, 1420.14/s  (0.706s, 1449.82/s)  LR: 4.476e-05  Data: 0.009 (0.024)
Train: 528 [ 200/1251 ( 16%)]  Loss: 2.667 (3.12)  Time: 0.704s, 1455.24/s  (0.701s, 1459.79/s)  LR: 4.476e-05  Data: 0.010 (0.020)
Train: 528 [ 250/1251 ( 20%)]  Loss: 3.385 (3.17)  Time: 0.706s, 1450.44/s  (0.699s, 1465.33/s)  LR: 4.476e-05  Data: 0.010 (0.018)
Train: 528 [ 300/1251 ( 24%)]  Loss: 3.404 (3.20)  Time: 0.672s, 1523.85/s  (0.697s, 1469.87/s)  LR: 4.476e-05  Data: 0.011 (0.017)
Train: 528 [ 350/1251 ( 28%)]  Loss: 3.323 (3.22)  Time: 0.703s, 1457.18/s  (0.696s, 1470.84/s)  LR: 4.476e-05  Data: 0.010 (0.016)
Train: 528 [ 400/1251 ( 32%)]  Loss: 3.366 (3.23)  Time: 0.705s, 1452.30/s  (0.696s, 1470.75/s)  LR: 4.476e-05  Data: 0.009 (0.015)
Train: 528 [ 450/1251 ( 36%)]  Loss: 3.157 (3.23)  Time: 0.708s, 1446.43/s  (0.696s, 1472.09/s)  LR: 4.476e-05  Data: 0.010 (0.015)
Train: 528 [ 500/1251 ( 40%)]  Loss: 3.097 (3.21)  Time: 0.695s, 1472.84/s  (0.694s, 1475.25/s)  LR: 4.476e-05  Data: 0.013 (0.014)
Train: 528 [ 550/1251 ( 44%)]  Loss: 3.357 (3.23)  Time: 0.673s, 1521.69/s  (0.694s, 1475.25/s)  LR: 4.476e-05  Data: 0.011 (0.014)
Train: 528 [ 600/1251 ( 48%)]  Loss: 2.962 (3.21)  Time: 0.678s, 1509.65/s  (0.693s, 1476.67/s)  LR: 4.476e-05  Data: 0.009 (0.014)
Train: 528 [ 650/1251 ( 52%)]  Loss: 3.020 (3.19)  Time: 0.707s, 1448.38/s  (0.693s, 1477.62/s)  LR: 4.476e-05  Data: 0.010 (0.013)
Train: 528 [ 700/1251 ( 56%)]  Loss: 3.291 (3.20)  Time: 0.689s, 1487.11/s  (0.693s, 1478.05/s)  LR: 4.476e-05  Data: 0.010 (0.013)
Train: 528 [ 750/1251 ( 60%)]  Loss: 3.567 (3.22)  Time: 0.717s, 1428.05/s  (0.693s, 1477.89/s)  LR: 4.476e-05  Data: 0.011 (0.013)
Train: 528 [ 800/1251 ( 64%)]  Loss: 3.390 (3.23)  Time: 0.680s, 1506.24/s  (0.693s, 1477.91/s)  LR: 4.476e-05  Data: 0.011 (0.013)
Train: 528 [ 850/1251 ( 68%)]  Loss: 3.050 (3.22)  Time: 0.717s, 1427.92/s  (0.693s, 1478.66/s)  LR: 4.476e-05  Data: 0.010 (0.013)
Train: 528 [ 900/1251 ( 72%)]  Loss: 3.054 (3.21)  Time: 0.670s, 1527.70/s  (0.693s, 1477.96/s)  LR: 4.476e-05  Data: 0.012 (0.013)
Train: 528 [ 950/1251 ( 76%)]  Loss: 2.997 (3.20)  Time: 0.707s, 1448.18/s  (0.693s, 1478.42/s)  LR: 4.476e-05  Data: 0.010 (0.013)
Train: 528 [1000/1251 ( 80%)]  Loss: 3.150 (3.20)  Time: 0.715s, 1432.59/s  (0.692s, 1479.00/s)  LR: 4.476e-05  Data: 0.011 (0.012)
Train: 528 [1050/1251 ( 84%)]  Loss: 3.241 (3.20)  Time: 0.716s, 1429.76/s  (0.692s, 1478.98/s)  LR: 4.476e-05  Data: 0.014 (0.012)
Train: 528 [1100/1251 ( 88%)]  Loss: 2.878 (3.19)  Time: 0.672s, 1523.75/s  (0.693s, 1477.76/s)  LR: 4.476e-05  Data: 0.010 (0.012)
Train: 528 [1150/1251 ( 92%)]  Loss: 3.039 (3.18)  Time: 0.700s, 1462.82/s  (0.693s, 1476.85/s)  LR: 4.476e-05  Data: 0.012 (0.012)
Train: 528 [1200/1251 ( 96%)]  Loss: 3.293 (3.19)  Time: 0.679s, 1508.02/s  (0.693s, 1477.00/s)  LR: 4.476e-05  Data: 0.011 (0.012)
Train: 528 [1250/1251 (100%)]  Loss: 3.507 (3.20)  Time: 0.694s, 1476.54/s  (0.693s, 1476.80/s)  LR: 4.476e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.685 (1.685)  Loss:  0.7085 (0.7085)  Acc@1: 91.3086 (91.3086)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.136 (0.589)  Loss:  0.8555 (1.1927)  Acc@1: 87.0283 (78.3400)  Acc@5: 97.1698 (94.0720)
Train: 529 [   0/1251 (  0%)]  Loss: 2.943 (2.94)  Time: 2.200s,  465.50/s  (2.200s,  465.50/s)  LR: 4.381e-05  Data: 1.577 (1.577)
Train: 529 [  50/1251 (  4%)]  Loss: 3.179 (3.06)  Time: 0.703s, 1456.35/s  (0.722s, 1418.08/s)  LR: 4.381e-05  Data: 0.009 (0.047)
Train: 529 [ 100/1251 (  8%)]  Loss: 3.135 (3.09)  Time: 0.674s, 1518.81/s  (0.706s, 1450.78/s)  LR: 4.381e-05  Data: 0.010 (0.029)
Train: 529 [ 150/1251 ( 12%)]  Loss: 2.988 (3.06)  Time: 0.670s, 1527.70/s  (0.701s, 1459.96/s)  LR: 4.381e-05  Data: 0.010 (0.023)
Train: 529 [ 200/1251 ( 16%)]  Loss: 3.567 (3.16)  Time: 0.674s, 1519.57/s  (0.700s, 1463.77/s)  LR: 4.381e-05  Data: 0.010 (0.020)
Train: 529 [ 250/1251 ( 20%)]  Loss: 2.971 (3.13)  Time: 0.674s, 1519.45/s  (0.700s, 1463.31/s)  LR: 4.381e-05  Data: 0.009 (0.018)
Train: 529 [ 300/1251 ( 24%)]  Loss: 3.185 (3.14)  Time: 0.706s, 1450.83/s  (0.699s, 1465.19/s)  LR: 4.381e-05  Data: 0.010 (0.016)
Train: 529 [ 350/1251 ( 28%)]  Loss: 3.045 (3.13)  Time: 0.722s, 1418.19/s  (0.697s, 1469.23/s)  LR: 4.381e-05  Data: 0.010 (0.016)
Train: 529 [ 400/1251 ( 32%)]  Loss: 2.765 (3.09)  Time: 0.683s, 1498.31/s  (0.696s, 1471.60/s)  LR: 4.381e-05  Data: 0.009 (0.015)
Train: 529 [ 450/1251 ( 36%)]  Loss: 3.765 (3.15)  Time: 0.689s, 1486.11/s  (0.696s, 1470.43/s)  LR: 4.381e-05  Data: 0.009 (0.014)
Train: 529 [ 500/1251 ( 40%)]  Loss: 3.375 (3.17)  Time: 0.671s, 1526.31/s  (0.696s, 1471.80/s)  LR: 4.381e-05  Data: 0.010 (0.014)
Train: 529 [ 550/1251 ( 44%)]  Loss: 3.461 (3.20)  Time: 0.704s, 1455.07/s  (0.696s, 1471.77/s)  LR: 4.381e-05  Data: 0.010 (0.014)
Train: 529 [ 600/1251 ( 48%)]  Loss: 3.239 (3.20)  Time: 0.686s, 1492.98/s  (0.695s, 1472.87/s)  LR: 4.381e-05  Data: 0.009 (0.013)
Train: 529 [ 650/1251 ( 52%)]  Loss: 3.352 (3.21)  Time: 0.669s, 1529.54/s  (0.695s, 1473.93/s)  LR: 4.381e-05  Data: 0.011 (0.013)
Train: 529 [ 700/1251 ( 56%)]  Loss: 3.151 (3.21)  Time: 0.715s, 1432.83/s  (0.695s, 1474.21/s)  LR: 4.381e-05  Data: 0.015 (0.013)
Train: 529 [ 750/1251 ( 60%)]  Loss: 3.290 (3.21)  Time: 0.719s, 1424.68/s  (0.694s, 1475.44/s)  LR: 4.381e-05  Data: 0.010 (0.013)
Train: 529 [ 800/1251 ( 64%)]  Loss: 3.435 (3.23)  Time: 0.673s, 1521.95/s  (0.694s, 1476.22/s)  LR: 4.381e-05  Data: 0.009 (0.013)
Train: 529 [ 850/1251 ( 68%)]  Loss: 3.105 (3.22)  Time: 0.692s, 1480.51/s  (0.694s, 1474.96/s)  LR: 4.381e-05  Data: 0.014 (0.013)
Train: 529 [ 900/1251 ( 72%)]  Loss: 3.000 (3.21)  Time: 0.693s, 1477.34/s  (0.694s, 1475.72/s)  LR: 4.381e-05  Data: 0.010 (0.012)
Train: 529 [ 950/1251 ( 76%)]  Loss: 3.336 (3.21)  Time: 0.677s, 1511.98/s  (0.694s, 1475.28/s)  LR: 4.381e-05  Data: 0.011 (0.012)
Train: 529 [1000/1251 ( 80%)]  Loss: 3.117 (3.21)  Time: 0.743s, 1378.66/s  (0.694s, 1475.36/s)  LR: 4.381e-05  Data: 0.010 (0.012)
Train: 529 [1050/1251 ( 84%)]  Loss: 2.928 (3.20)  Time: 0.705s, 1452.09/s  (0.694s, 1475.81/s)  LR: 4.381e-05  Data: 0.009 (0.012)
Train: 529 [1100/1251 ( 88%)]  Loss: 3.311 (3.20)  Time: 0.674s, 1519.89/s  (0.694s, 1476.44/s)  LR: 4.381e-05  Data: 0.010 (0.012)
Train: 529 [1150/1251 ( 92%)]  Loss: 3.333 (3.21)  Time: 0.671s, 1525.84/s  (0.694s, 1476.37/s)  LR: 4.381e-05  Data: 0.010 (0.012)
Train: 529 [1200/1251 ( 96%)]  Loss: 3.450 (3.22)  Time: 0.706s, 1450.56/s  (0.693s, 1477.22/s)  LR: 4.381e-05  Data: 0.010 (0.012)
Train: 529 [1250/1251 (100%)]  Loss: 3.274 (3.22)  Time: 0.687s, 1491.49/s  (0.693s, 1477.13/s)  LR: 4.381e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.458 (1.458)  Loss:  0.7007 (0.7007)  Acc@1: 91.3086 (91.3086)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.136 (0.589)  Loss:  0.8276 (1.1906)  Acc@1: 86.6745 (78.6920)  Acc@5: 97.1698 (94.2360)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-523.pth.tar', 78.69399997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-529.pth.tar', 78.69200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-524.pth.tar', 78.66600005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-527.pth.tar', 78.66000005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-526.pth.tar', 78.63800010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-525.pth.tar', 78.58600002685547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-514.pth.tar', 78.50200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-518.pth.tar', 78.48800000244141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-521.pth.tar', 78.47399997802735)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-512.pth.tar', 78.44800021240235)

Train: 530 [   0/1251 (  0%)]  Loss: 3.102 (3.10)  Time: 2.248s,  455.50/s  (2.248s,  455.50/s)  LR: 4.288e-05  Data: 1.631 (1.631)
Train: 530 [  50/1251 (  4%)]  Loss: 3.483 (3.29)  Time: 0.713s, 1437.02/s  (0.734s, 1395.43/s)  LR: 4.288e-05  Data: 0.012 (0.049)
Train: 530 [ 100/1251 (  8%)]  Loss: 3.094 (3.23)  Time: 0.711s, 1439.33/s  (0.712s, 1438.64/s)  LR: 4.288e-05  Data: 0.011 (0.030)
Train: 530 [ 150/1251 ( 12%)]  Loss: 3.308 (3.25)  Time: 0.674s, 1519.37/s  (0.705s, 1452.30/s)  LR: 4.288e-05  Data: 0.011 (0.023)
Train: 530 [ 200/1251 ( 16%)]  Loss: 3.063 (3.21)  Time: 0.675s, 1517.80/s  (0.701s, 1461.10/s)  LR: 4.288e-05  Data: 0.010 (0.020)
Train: 530 [ 250/1251 ( 20%)]  Loss: 3.524 (3.26)  Time: 0.673s, 1521.06/s  (0.698s, 1467.49/s)  LR: 4.288e-05  Data: 0.011 (0.018)
Train: 530 [ 300/1251 ( 24%)]  Loss: 3.483 (3.29)  Time: 0.672s, 1522.72/s  (0.696s, 1471.31/s)  LR: 4.288e-05  Data: 0.011 (0.017)
Train: 530 [ 350/1251 ( 28%)]  Loss: 3.207 (3.28)  Time: 0.675s, 1516.88/s  (0.696s, 1472.20/s)  LR: 4.288e-05  Data: 0.010 (0.016)
Train: 530 [ 400/1251 ( 32%)]  Loss: 3.176 (3.27)  Time: 0.705s, 1452.69/s  (0.696s, 1471.91/s)  LR: 4.288e-05  Data: 0.011 (0.015)
Train: 530 [ 450/1251 ( 36%)]  Loss: 3.081 (3.25)  Time: 0.673s, 1522.28/s  (0.695s, 1472.35/s)  LR: 4.288e-05  Data: 0.010 (0.015)
Train: 530 [ 500/1251 ( 40%)]  Loss: 3.116 (3.24)  Time: 0.672s, 1524.70/s  (0.695s, 1472.44/s)  LR: 4.288e-05  Data: 0.011 (0.014)
Train: 530 [ 550/1251 ( 44%)]  Loss: 3.326 (3.25)  Time: 0.712s, 1437.55/s  (0.695s, 1473.56/s)  LR: 4.288e-05  Data: 0.010 (0.014)
Train: 530 [ 600/1251 ( 48%)]  Loss: 3.133 (3.24)  Time: 0.672s, 1523.38/s  (0.694s, 1475.29/s)  LR: 4.288e-05  Data: 0.010 (0.014)
Train: 530 [ 650/1251 ( 52%)]  Loss: 3.353 (3.25)  Time: 0.684s, 1496.82/s  (0.694s, 1476.26/s)  LR: 4.288e-05  Data: 0.011 (0.013)
Train: 530 [ 700/1251 ( 56%)]  Loss: 3.256 (3.25)  Time: 0.672s, 1523.97/s  (0.693s, 1476.92/s)  LR: 4.288e-05  Data: 0.011 (0.013)
Train: 530 [ 750/1251 ( 60%)]  Loss: 3.145 (3.24)  Time: 0.757s, 1353.13/s  (0.693s, 1476.90/s)  LR: 4.288e-05  Data: 0.008 (0.013)
Train: 530 [ 800/1251 ( 64%)]  Loss: 3.345 (3.25)  Time: 0.703s, 1456.40/s  (0.693s, 1477.87/s)  LR: 4.288e-05  Data: 0.009 (0.013)
Train: 530 [ 850/1251 ( 68%)]  Loss: 3.635 (3.27)  Time: 0.720s, 1421.26/s  (0.693s, 1477.78/s)  LR: 4.288e-05  Data: 0.009 (0.013)
Train: 530 [ 900/1251 ( 72%)]  Loss: 2.955 (3.25)  Time: 0.702s, 1459.51/s  (0.693s, 1478.25/s)  LR: 4.288e-05  Data: 0.010 (0.013)
Train: 530 [ 950/1251 ( 76%)]  Loss: 3.363 (3.26)  Time: 0.673s, 1520.42/s  (0.693s, 1478.18/s)  LR: 4.288e-05  Data: 0.009 (0.012)
Train: 530 [1000/1251 ( 80%)]  Loss: 3.241 (3.26)  Time: 0.704s, 1454.08/s  (0.693s, 1477.57/s)  LR: 4.288e-05  Data: 0.010 (0.012)
Train: 530 [1050/1251 ( 84%)]  Loss: 3.155 (3.25)  Time: 0.674s, 1519.23/s  (0.693s, 1478.20/s)  LR: 4.288e-05  Data: 0.011 (0.012)
Train: 530 [1100/1251 ( 88%)]  Loss: 3.434 (3.26)  Time: 0.687s, 1490.65/s  (0.693s, 1478.36/s)  LR: 4.288e-05  Data: 0.012 (0.012)
Train: 530 [1150/1251 ( 92%)]  Loss: 3.150 (3.26)  Time: 0.727s, 1409.11/s  (0.693s, 1478.46/s)  LR: 4.288e-05  Data: 0.010 (0.012)
Train: 530 [1200/1251 ( 96%)]  Loss: 3.416 (3.26)  Time: 0.698s, 1467.79/s  (0.693s, 1478.30/s)  LR: 4.288e-05  Data: 0.009 (0.012)
Train: 530 [1250/1251 (100%)]  Loss: 3.524 (3.27)  Time: 0.658s, 1556.82/s  (0.693s, 1478.18/s)  LR: 4.288e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.549 (1.549)  Loss:  0.6904 (0.6904)  Acc@1: 92.1875 (92.1875)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.136 (0.584)  Loss:  0.7783 (1.1682)  Acc@1: 86.4387 (78.7460)  Acc@5: 96.9340 (94.2320)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-530.pth.tar', 78.74600005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-523.pth.tar', 78.69399997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-529.pth.tar', 78.69200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-524.pth.tar', 78.66600005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-527.pth.tar', 78.66000005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-526.pth.tar', 78.63800010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-525.pth.tar', 78.58600002685547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-514.pth.tar', 78.50200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-518.pth.tar', 78.48800000244141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-521.pth.tar', 78.47399997802735)

Train: 531 [   0/1251 (  0%)]  Loss: 3.098 (3.10)  Time: 2.083s,  491.66/s  (2.083s,  491.66/s)  LR: 4.196e-05  Data: 1.468 (1.468)
Train: 531 [  50/1251 (  4%)]  Loss: 3.464 (3.28)  Time: 0.686s, 1492.11/s  (0.720s, 1421.79/s)  LR: 4.196e-05  Data: 0.010 (0.047)
Train: 531 [ 100/1251 (  8%)]  Loss: 2.914 (3.16)  Time: 0.720s, 1422.21/s  (0.705s, 1451.99/s)  LR: 4.196e-05  Data: 0.010 (0.029)
Train: 531 [ 150/1251 ( 12%)]  Loss: 3.367 (3.21)  Time: 0.793s, 1291.50/s  (0.702s, 1459.57/s)  LR: 4.196e-05  Data: 0.009 (0.023)
Train: 531 [ 200/1251 ( 16%)]  Loss: 3.253 (3.22)  Time: 0.671s, 1525.65/s  (0.699s, 1464.67/s)  LR: 4.196e-05  Data: 0.011 (0.020)
Train: 531 [ 250/1251 ( 20%)]  Loss: 3.512 (3.27)  Time: 0.665s, 1539.72/s  (0.698s, 1467.90/s)  LR: 4.196e-05  Data: 0.011 (0.018)
Train: 531 [ 300/1251 ( 24%)]  Loss: 3.380 (3.28)  Time: 0.673s, 1522.00/s  (0.695s, 1472.60/s)  LR: 4.196e-05  Data: 0.010 (0.017)
Train: 531 [ 350/1251 ( 28%)]  Loss: 3.454 (3.31)  Time: 0.703s, 1456.97/s  (0.695s, 1473.54/s)  LR: 4.196e-05  Data: 0.009 (0.016)
Train: 531 [ 400/1251 ( 32%)]  Loss: 3.353 (3.31)  Time: 0.677s, 1512.06/s  (0.694s, 1475.43/s)  LR: 4.196e-05  Data: 0.010 (0.015)
Train: 531 [ 450/1251 ( 36%)]  Loss: 3.319 (3.31)  Time: 0.704s, 1453.62/s  (0.694s, 1476.27/s)  LR: 4.196e-05  Data: 0.009 (0.015)
Train: 531 [ 500/1251 ( 40%)]  Loss: 3.006 (3.28)  Time: 0.670s, 1528.12/s  (0.693s, 1477.87/s)  LR: 4.196e-05  Data: 0.010 (0.014)
Train: 531 [ 550/1251 ( 44%)]  Loss: 3.463 (3.30)  Time: 0.694s, 1474.45/s  (0.692s, 1478.72/s)  LR: 4.196e-05  Data: 0.009 (0.014)
Train: 531 [ 600/1251 ( 48%)]  Loss: 2.838 (3.26)  Time: 0.703s, 1457.22/s  (0.693s, 1477.06/s)  LR: 4.196e-05  Data: 0.009 (0.014)
Train: 531 [ 650/1251 ( 52%)]  Loss: 3.294 (3.27)  Time: 0.675s, 1516.53/s  (0.693s, 1477.03/s)  LR: 4.196e-05  Data: 0.009 (0.013)
Train: 531 [ 700/1251 ( 56%)]  Loss: 3.052 (3.25)  Time: 0.676s, 1514.59/s  (0.693s, 1477.96/s)  LR: 4.196e-05  Data: 0.011 (0.013)
Train: 531 [ 750/1251 ( 60%)]  Loss: 3.126 (3.24)  Time: 0.680s, 1505.17/s  (0.693s, 1477.89/s)  LR: 4.196e-05  Data: 0.010 (0.013)
Train: 531 [ 800/1251 ( 64%)]  Loss: 3.389 (3.25)  Time: 0.680s, 1505.16/s  (0.692s, 1478.74/s)  LR: 4.196e-05  Data: 0.009 (0.013)
Train: 531 [ 850/1251 ( 68%)]  Loss: 2.925 (3.23)  Time: 0.700s, 1463.26/s  (0.692s, 1478.84/s)  LR: 4.196e-05  Data: 0.012 (0.013)
Train: 531 [ 900/1251 ( 72%)]  Loss: 2.774 (3.21)  Time: 0.697s, 1468.16/s  (0.692s, 1479.26/s)  LR: 4.196e-05  Data: 0.009 (0.012)
Train: 531 [ 950/1251 ( 76%)]  Loss: 3.435 (3.22)  Time: 0.699s, 1464.96/s  (0.692s, 1479.47/s)  LR: 4.196e-05  Data: 0.010 (0.012)
Train: 531 [1000/1251 ( 80%)]  Loss: 3.553 (3.24)  Time: 0.701s, 1459.89/s  (0.692s, 1478.95/s)  LR: 4.196e-05  Data: 0.009 (0.012)
Train: 531 [1050/1251 ( 84%)]  Loss: 3.058 (3.23)  Time: 0.756s, 1355.13/s  (0.692s, 1479.78/s)  LR: 4.196e-05  Data: 0.010 (0.012)
Train: 531 [1100/1251 ( 88%)]  Loss: 3.105 (3.22)  Time: 0.671s, 1526.63/s  (0.692s, 1480.13/s)  LR: 4.196e-05  Data: 0.010 (0.012)
Train: 531 [1150/1251 ( 92%)]  Loss: 3.331 (3.23)  Time: 0.663s, 1543.94/s  (0.692s, 1479.15/s)  LR: 4.196e-05  Data: 0.009 (0.012)
Train: 531 [1200/1251 ( 96%)]  Loss: 3.271 (3.23)  Time: 0.674s, 1520.26/s  (0.692s, 1479.09/s)  LR: 4.196e-05  Data: 0.009 (0.012)
Train: 531 [1250/1251 (100%)]  Loss: 3.291 (3.23)  Time: 0.657s, 1557.48/s  (0.692s, 1478.83/s)  LR: 4.196e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.461 (1.461)  Loss:  0.7485 (0.7485)  Acc@1: 91.7969 (91.7969)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.136 (0.579)  Loss:  0.8335 (1.2218)  Acc@1: 86.7925 (78.6720)  Acc@5: 97.0519 (94.1680)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-530.pth.tar', 78.74600005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-523.pth.tar', 78.69399997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-529.pth.tar', 78.69200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-531.pth.tar', 78.6720000805664)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-524.pth.tar', 78.66600005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-527.pth.tar', 78.66000005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-526.pth.tar', 78.63800010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-525.pth.tar', 78.58600002685547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-514.pth.tar', 78.50200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-518.pth.tar', 78.48800000244141)

Train: 532 [   0/1251 (  0%)]  Loss: 3.109 (3.11)  Time: 2.263s,  452.48/s  (2.263s,  452.48/s)  LR: 4.105e-05  Data: 1.647 (1.647)
Train: 532 [  50/1251 (  4%)]  Loss: 3.117 (3.11)  Time: 0.683s, 1499.23/s  (0.729s, 1405.47/s)  LR: 4.105e-05  Data: 0.011 (0.050)
Train: 532 [ 100/1251 (  8%)]  Loss: 2.955 (3.06)  Time: 0.672s, 1524.30/s  (0.710s, 1441.75/s)  LR: 4.105e-05  Data: 0.011 (0.031)
Train: 532 [ 150/1251 ( 12%)]  Loss: 3.314 (3.12)  Time: 0.673s, 1520.50/s  (0.706s, 1450.40/s)  LR: 4.105e-05  Data: 0.011 (0.024)
Train: 532 [ 200/1251 ( 16%)]  Loss: 3.531 (3.21)  Time: 0.675s, 1518.11/s  (0.701s, 1461.55/s)  LR: 4.105e-05  Data: 0.010 (0.021)
Train: 532 [ 250/1251 ( 20%)]  Loss: 3.539 (3.26)  Time: 0.691s, 1482.85/s  (0.697s, 1469.37/s)  LR: 4.105e-05  Data: 0.011 (0.019)
Train: 532 [ 300/1251 ( 24%)]  Loss: 3.333 (3.27)  Time: 0.706s, 1451.39/s  (0.695s, 1473.28/s)  LR: 4.105e-05  Data: 0.011 (0.017)
Train: 532 [ 350/1251 ( 28%)]  Loss: 3.160 (3.26)  Time: 0.681s, 1503.74/s  (0.695s, 1474.24/s)  LR: 4.105e-05  Data: 0.010 (0.016)
Train: 532 [ 400/1251 ( 32%)]  Loss: 2.926 (3.22)  Time: 0.670s, 1529.10/s  (0.694s, 1475.96/s)  LR: 4.105e-05  Data: 0.012 (0.015)
Train: 532 [ 450/1251 ( 36%)]  Loss: 3.001 (3.20)  Time: 0.676s, 1514.32/s  (0.694s, 1475.13/s)  LR: 4.105e-05  Data: 0.016 (0.015)
Train: 532 [ 500/1251 ( 40%)]  Loss: 3.204 (3.20)  Time: 0.699s, 1464.08/s  (0.694s, 1476.24/s)  LR: 4.105e-05  Data: 0.009 (0.014)
Train: 532 [ 550/1251 ( 44%)]  Loss: 3.552 (3.23)  Time: 0.706s, 1450.67/s  (0.693s, 1477.03/s)  LR: 4.105e-05  Data: 0.011 (0.014)
Train: 532 [ 600/1251 ( 48%)]  Loss: 3.213 (3.23)  Time: 0.668s, 1532.56/s  (0.693s, 1477.05/s)  LR: 4.105e-05  Data: 0.010 (0.014)
Train: 532 [ 650/1251 ( 52%)]  Loss: 3.316 (3.23)  Time: 0.688s, 1487.67/s  (0.693s, 1478.13/s)  LR: 4.105e-05  Data: 0.012 (0.013)
Train: 532 [ 700/1251 ( 56%)]  Loss: 2.901 (3.21)  Time: 0.745s, 1373.78/s  (0.693s, 1476.63/s)  LR: 4.105e-05  Data: 0.009 (0.013)
Train: 532 [ 750/1251 ( 60%)]  Loss: 3.077 (3.20)  Time: 0.683s, 1498.26/s  (0.693s, 1477.23/s)  LR: 4.105e-05  Data: 0.010 (0.013)
Train: 532 [ 800/1251 ( 64%)]  Loss: 2.907 (3.19)  Time: 0.719s, 1424.00/s  (0.694s, 1476.55/s)  LR: 4.105e-05  Data: 0.010 (0.013)
Train: 532 [ 850/1251 ( 68%)]  Loss: 2.999 (3.18)  Time: 0.710s, 1441.24/s  (0.694s, 1476.39/s)  LR: 4.105e-05  Data: 0.009 (0.013)
Train: 532 [ 900/1251 ( 72%)]  Loss: 3.504 (3.19)  Time: 0.678s, 1509.59/s  (0.693s, 1477.21/s)  LR: 4.105e-05  Data: 0.011 (0.013)
Train: 532 [ 950/1251 ( 76%)]  Loss: 3.353 (3.20)  Time: 0.672s, 1524.33/s  (0.693s, 1477.44/s)  LR: 4.105e-05  Data: 0.011 (0.013)
Train: 532 [1000/1251 ( 80%)]  Loss: 2.812 (3.18)  Time: 0.673s, 1520.77/s  (0.693s, 1477.47/s)  LR: 4.105e-05  Data: 0.011 (0.012)
Train: 532 [1050/1251 ( 84%)]  Loss: 3.289 (3.19)  Time: 0.673s, 1521.80/s  (0.693s, 1477.93/s)  LR: 4.105e-05  Data: 0.011 (0.012)
Train: 532 [1100/1251 ( 88%)]  Loss: 3.278 (3.19)  Time: 0.719s, 1424.88/s  (0.693s, 1477.34/s)  LR: 4.105e-05  Data: 0.009 (0.012)
Train: 532 [1150/1251 ( 92%)]  Loss: 3.395 (3.20)  Time: 0.672s, 1523.29/s  (0.694s, 1476.54/s)  LR: 4.105e-05  Data: 0.011 (0.012)
Train: 532 [1200/1251 ( 96%)]  Loss: 3.084 (3.19)  Time: 0.670s, 1527.63/s  (0.694s, 1476.01/s)  LR: 4.105e-05  Data: 0.011 (0.012)
Train: 532 [1250/1251 (100%)]  Loss: 3.036 (3.19)  Time: 0.702s, 1458.68/s  (0.693s, 1476.59/s)  LR: 4.105e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.541 (1.541)  Loss:  0.6899 (0.6899)  Acc@1: 91.6992 (91.6992)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.136 (0.574)  Loss:  0.8208 (1.1965)  Acc@1: 87.0283 (78.7140)  Acc@5: 96.4623 (94.2800)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-530.pth.tar', 78.74600005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-532.pth.tar', 78.71400018310547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-523.pth.tar', 78.69399997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-529.pth.tar', 78.69200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-531.pth.tar', 78.6720000805664)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-524.pth.tar', 78.66600005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-527.pth.tar', 78.66000005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-526.pth.tar', 78.63800010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-525.pth.tar', 78.58600002685547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-514.pth.tar', 78.50200002929688)

Train: 533 [   0/1251 (  0%)]  Loss: 3.312 (3.31)  Time: 2.293s,  446.54/s  (2.293s,  446.54/s)  LR: 4.015e-05  Data: 1.616 (1.616)
Train: 533 [  50/1251 (  4%)]  Loss: 2.710 (3.01)  Time: 0.684s, 1497.77/s  (0.743s, 1378.42/s)  LR: 4.015e-05  Data: 0.010 (0.052)
Train: 533 [ 100/1251 (  8%)]  Loss: 3.622 (3.21)  Time: 0.671s, 1525.20/s  (0.718s, 1426.68/s)  LR: 4.015e-05  Data: 0.010 (0.031)
Train: 533 [ 150/1251 ( 12%)]  Loss: 3.026 (3.17)  Time: 0.689s, 1486.70/s  (0.709s, 1444.58/s)  LR: 4.015e-05  Data: 0.009 (0.024)
Train: 533 [ 200/1251 ( 16%)]  Loss: 2.989 (3.13)  Time: 0.679s, 1509.13/s  (0.704s, 1454.60/s)  LR: 4.015e-05  Data: 0.010 (0.021)
Train: 533 [ 250/1251 ( 20%)]  Loss: 3.367 (3.17)  Time: 0.702s, 1459.03/s  (0.702s, 1458.23/s)  LR: 4.015e-05  Data: 0.011 (0.019)
Train: 533 [ 300/1251 ( 24%)]  Loss: 3.479 (3.21)  Time: 0.690s, 1483.52/s  (0.701s, 1460.89/s)  LR: 4.015e-05  Data: 0.013 (0.017)
Train: 533 [ 350/1251 ( 28%)]  Loss: 3.375 (3.24)  Time: 0.700s, 1463.25/s  (0.701s, 1460.43/s)  LR: 4.015e-05  Data: 0.010 (0.016)
Train: 533 [ 400/1251 ( 32%)]  Loss: 3.364 (3.25)  Time: 0.673s, 1521.26/s  (0.700s, 1463.28/s)  LR: 4.015e-05  Data: 0.010 (0.016)
Train: 533 [ 450/1251 ( 36%)]  Loss: 3.005 (3.22)  Time: 0.692s, 1478.92/s  (0.698s, 1467.30/s)  LR: 4.015e-05  Data: 0.010 (0.015)
Train: 533 [ 500/1251 ( 40%)]  Loss: 3.186 (3.22)  Time: 0.691s, 1481.49/s  (0.698s, 1467.97/s)  LR: 4.015e-05  Data: 0.011 (0.015)
Train: 533 [ 550/1251 ( 44%)]  Loss: 3.077 (3.21)  Time: 0.715s, 1431.30/s  (0.698s, 1467.57/s)  LR: 4.015e-05  Data: 0.010 (0.014)
Train: 533 [ 600/1251 ( 48%)]  Loss: 3.474 (3.23)  Time: 0.773s, 1325.32/s  (0.697s, 1468.94/s)  LR: 4.015e-05  Data: 0.011 (0.014)
Train: 533 [ 650/1251 ( 52%)]  Loss: 3.130 (3.22)  Time: 0.669s, 1531.45/s  (0.696s, 1471.27/s)  LR: 4.015e-05  Data: 0.009 (0.014)
Train: 533 [ 700/1251 ( 56%)]  Loss: 3.401 (3.23)  Time: 0.714s, 1434.89/s  (0.696s, 1471.49/s)  LR: 4.015e-05  Data: 0.010 (0.013)
Train: 533 [ 750/1251 ( 60%)]  Loss: 3.325 (3.24)  Time: 0.691s, 1481.47/s  (0.695s, 1472.44/s)  LR: 4.015e-05  Data: 0.009 (0.013)
Train: 533 [ 800/1251 ( 64%)]  Loss: 3.362 (3.25)  Time: 0.672s, 1523.28/s  (0.695s, 1473.00/s)  LR: 4.015e-05  Data: 0.010 (0.013)
Train: 533 [ 850/1251 ( 68%)]  Loss: 2.960 (3.23)  Time: 0.716s, 1429.44/s  (0.695s, 1474.01/s)  LR: 4.015e-05  Data: 0.012 (0.013)
Train: 533 [ 900/1251 ( 72%)]  Loss: 3.515 (3.25)  Time: 0.700s, 1463.85/s  (0.695s, 1474.36/s)  LR: 4.015e-05  Data: 0.009 (0.013)
Train: 533 [ 950/1251 ( 76%)]  Loss: 3.069 (3.24)  Time: 0.672s, 1523.29/s  (0.695s, 1473.76/s)  LR: 4.015e-05  Data: 0.009 (0.013)
Train: 533 [1000/1251 ( 80%)]  Loss: 2.836 (3.22)  Time: 0.706s, 1450.64/s  (0.695s, 1473.90/s)  LR: 4.015e-05  Data: 0.011 (0.012)
Train: 533 [1050/1251 ( 84%)]  Loss: 3.180 (3.22)  Time: 0.671s, 1526.17/s  (0.695s, 1473.99/s)  LR: 4.015e-05  Data: 0.010 (0.012)
Train: 533 [1100/1251 ( 88%)]  Loss: 2.987 (3.21)  Time: 0.725s, 1411.77/s  (0.695s, 1474.34/s)  LR: 4.015e-05  Data: 0.011 (0.012)
Train: 533 [1150/1251 ( 92%)]  Loss: 3.321 (3.21)  Time: 0.730s, 1402.25/s  (0.695s, 1474.06/s)  LR: 4.015e-05  Data: 0.011 (0.012)
Train: 533 [1200/1251 ( 96%)]  Loss: 2.933 (3.20)  Time: 0.719s, 1424.39/s  (0.694s, 1474.52/s)  LR: 4.015e-05  Data: 0.011 (0.012)
Train: 533 [1250/1251 (100%)]  Loss: 3.459 (3.21)  Time: 0.655s, 1562.72/s  (0.694s, 1474.72/s)  LR: 4.015e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.641 (1.641)  Loss:  0.6851 (0.6851)  Acc@1: 91.7969 (91.7969)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.136 (0.579)  Loss:  0.8369 (1.1919)  Acc@1: 86.0849 (78.4900)  Acc@5: 96.8160 (94.2180)
Train: 534 [   0/1251 (  0%)]  Loss: 3.213 (3.21)  Time: 2.423s,  422.56/s  (2.423s,  422.56/s)  LR: 3.926e-05  Data: 1.793 (1.793)
Train: 534 [  50/1251 (  4%)]  Loss: 3.357 (3.29)  Time: 0.676s, 1514.30/s  (0.726s, 1410.63/s)  LR: 3.926e-05  Data: 0.009 (0.045)
Train: 534 [ 100/1251 (  8%)]  Loss: 3.210 (3.26)  Time: 0.694s, 1475.53/s  (0.715s, 1431.78/s)  LR: 3.926e-05  Data: 0.013 (0.028)
Train: 534 [ 150/1251 ( 12%)]  Loss: 3.513 (3.32)  Time: 0.670s, 1528.29/s  (0.707s, 1447.45/s)  LR: 3.926e-05  Data: 0.011 (0.022)
Train: 534 [ 200/1251 ( 16%)]  Loss: 3.210 (3.30)  Time: 0.667s, 1534.82/s  (0.704s, 1454.52/s)  LR: 3.926e-05  Data: 0.010 (0.019)
Train: 534 [ 250/1251 ( 20%)]  Loss: 3.400 (3.32)  Time: 0.715s, 1431.44/s  (0.702s, 1459.59/s)  LR: 3.926e-05  Data: 0.010 (0.017)
Train: 534 [ 300/1251 ( 24%)]  Loss: 3.119 (3.29)  Time: 0.734s, 1395.65/s  (0.699s, 1464.39/s)  LR: 3.926e-05  Data: 0.011 (0.016)
Train: 534 [ 350/1251 ( 28%)]  Loss: 3.493 (3.31)  Time: 0.692s, 1478.84/s  (0.699s, 1465.65/s)  LR: 3.926e-05  Data: 0.009 (0.015)
Train: 534 [ 400/1251 ( 32%)]  Loss: 3.260 (3.31)  Time: 0.716s, 1431.13/s  (0.698s, 1466.23/s)  LR: 3.926e-05  Data: 0.010 (0.015)
Train: 534 [ 450/1251 ( 36%)]  Loss: 3.212 (3.30)  Time: 0.671s, 1525.58/s  (0.698s, 1466.04/s)  LR: 3.926e-05  Data: 0.011 (0.014)
Train: 534 [ 500/1251 ( 40%)]  Loss: 3.173 (3.29)  Time: 0.670s, 1529.29/s  (0.698s, 1467.39/s)  LR: 3.926e-05  Data: 0.011 (0.014)
Train: 534 [ 550/1251 ( 44%)]  Loss: 3.273 (3.29)  Time: 0.675s, 1516.82/s  (0.697s, 1469.15/s)  LR: 3.926e-05  Data: 0.011 (0.014)
Train: 534 [ 600/1251 ( 48%)]  Loss: 3.224 (3.28)  Time: 0.680s, 1506.14/s  (0.696s, 1470.70/s)  LR: 3.926e-05  Data: 0.011 (0.013)
Train: 534 [ 650/1251 ( 52%)]  Loss: 3.434 (3.29)  Time: 0.718s, 1425.44/s  (0.696s, 1470.61/s)  LR: 3.926e-05  Data: 0.011 (0.013)
Train: 534 [ 700/1251 ( 56%)]  Loss: 3.188 (3.29)  Time: 0.697s, 1468.26/s  (0.696s, 1471.37/s)  LR: 3.926e-05  Data: 0.010 (0.013)
Train: 534 [ 750/1251 ( 60%)]  Loss: 2.889 (3.26)  Time: 0.715s, 1431.57/s  (0.696s, 1471.68/s)  LR: 3.926e-05  Data: 0.014 (0.013)
Train: 534 [ 800/1251 ( 64%)]  Loss: 3.209 (3.26)  Time: 0.673s, 1521.45/s  (0.696s, 1471.80/s)  LR: 3.926e-05  Data: 0.011 (0.013)
Train: 534 [ 850/1251 ( 68%)]  Loss: 3.042 (3.25)  Time: 0.683s, 1499.90/s  (0.695s, 1473.05/s)  LR: 3.926e-05  Data: 0.011 (0.013)
Train: 534 [ 900/1251 ( 72%)]  Loss: 3.214 (3.24)  Time: 0.727s, 1407.89/s  (0.695s, 1474.03/s)  LR: 3.926e-05  Data: 0.009 (0.012)
Train: 534 [ 950/1251 ( 76%)]  Loss: 3.232 (3.24)  Time: 0.746s, 1373.03/s  (0.694s, 1474.71/s)  LR: 3.926e-05  Data: 0.011 (0.012)
Train: 534 [1000/1251 ( 80%)]  Loss: 3.049 (3.23)  Time: 0.730s, 1403.68/s  (0.694s, 1474.98/s)  LR: 3.926e-05  Data: 0.010 (0.012)
Train: 534 [1050/1251 ( 84%)]  Loss: 3.348 (3.24)  Time: 0.707s, 1448.47/s  (0.694s, 1475.04/s)  LR: 3.926e-05  Data: 0.010 (0.012)
Train: 534 [1100/1251 ( 88%)]  Loss: 3.322 (3.24)  Time: 0.718s, 1426.30/s  (0.694s, 1475.33/s)  LR: 3.926e-05  Data: 0.010 (0.012)
Train: 534 [1150/1251 ( 92%)]  Loss: 3.592 (3.26)  Time: 0.690s, 1484.31/s  (0.694s, 1475.33/s)  LR: 3.926e-05  Data: 0.010 (0.012)
Train: 534 [1200/1251 ( 96%)]  Loss: 2.922 (3.24)  Time: 0.670s, 1527.50/s  (0.694s, 1475.56/s)  LR: 3.926e-05  Data: 0.009 (0.012)
Train: 534 [1250/1251 (100%)]  Loss: 3.393 (3.25)  Time: 0.663s, 1545.29/s  (0.694s, 1476.12/s)  LR: 3.926e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.493 (1.493)  Loss:  0.6558 (0.6558)  Acc@1: 91.7969 (91.7969)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.137 (0.579)  Loss:  0.7773 (1.1413)  Acc@1: 87.5000 (78.8020)  Acc@5: 96.9340 (94.2640)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-534.pth.tar', 78.802)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-530.pth.tar', 78.74600005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-532.pth.tar', 78.71400018310547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-523.pth.tar', 78.69399997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-529.pth.tar', 78.69200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-531.pth.tar', 78.6720000805664)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-524.pth.tar', 78.66600005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-527.pth.tar', 78.66000005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-526.pth.tar', 78.63800010986328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-525.pth.tar', 78.58600002685547)

Train: 535 [   0/1251 (  0%)]  Loss: 3.475 (3.48)  Time: 2.200s,  465.37/s  (2.200s,  465.37/s)  LR: 3.839e-05  Data: 1.544 (1.544)
Train: 535 [  50/1251 (  4%)]  Loss: 3.138 (3.31)  Time: 0.673s, 1521.66/s  (0.722s, 1418.24/s)  LR: 3.839e-05  Data: 0.009 (0.047)
Train: 535 [ 100/1251 (  8%)]  Loss: 2.919 (3.18)  Time: 0.703s, 1456.13/s  (0.707s, 1449.10/s)  LR: 3.839e-05  Data: 0.009 (0.029)
Train: 535 [ 150/1251 ( 12%)]  Loss: 3.449 (3.25)  Time: 0.672s, 1523.69/s  (0.700s, 1462.17/s)  LR: 3.839e-05  Data: 0.010 (0.023)
Train: 535 [ 200/1251 ( 16%)]  Loss: 2.977 (3.19)  Time: 0.692s, 1480.58/s  (0.698s, 1467.44/s)  LR: 3.839e-05  Data: 0.010 (0.020)
Train: 535 [ 250/1251 ( 20%)]  Loss: 2.921 (3.15)  Time: 0.672s, 1522.93/s  (0.697s, 1469.89/s)  LR: 3.839e-05  Data: 0.010 (0.018)
Train: 535 [ 300/1251 ( 24%)]  Loss: 3.241 (3.16)  Time: 0.670s, 1528.00/s  (0.695s, 1474.25/s)  LR: 3.839e-05  Data: 0.010 (0.017)
Train: 535 [ 350/1251 ( 28%)]  Loss: 3.293 (3.18)  Time: 0.672s, 1523.66/s  (0.694s, 1474.78/s)  LR: 3.839e-05  Data: 0.011 (0.016)
Train: 535 [ 400/1251 ( 32%)]  Loss: 3.323 (3.19)  Time: 0.725s, 1413.34/s  (0.695s, 1473.50/s)  LR: 3.839e-05  Data: 0.010 (0.015)
Train: 535 [ 450/1251 ( 36%)]  Loss: 3.318 (3.21)  Time: 0.710s, 1442.65/s  (0.695s, 1473.83/s)  LR: 3.839e-05  Data: 0.010 (0.015)
Train: 535 [ 500/1251 ( 40%)]  Loss: 2.988 (3.19)  Time: 0.714s, 1434.19/s  (0.694s, 1475.09/s)  LR: 3.839e-05  Data: 0.009 (0.014)
Train: 535 [ 550/1251 ( 44%)]  Loss: 3.394 (3.20)  Time: 0.678s, 1509.74/s  (0.694s, 1476.41/s)  LR: 3.839e-05  Data: 0.015 (0.014)
Train: 535 [ 600/1251 ( 48%)]  Loss: 3.098 (3.20)  Time: 0.673s, 1521.68/s  (0.694s, 1475.83/s)  LR: 3.839e-05  Data: 0.010 (0.014)
Train: 535 [ 650/1251 ( 52%)]  Loss: 3.079 (3.19)  Time: 0.702s, 1458.62/s  (0.694s, 1475.61/s)  LR: 3.839e-05  Data: 0.009 (0.013)
Train: 535 [ 700/1251 ( 56%)]  Loss: 3.396 (3.20)  Time: 0.671s, 1525.62/s  (0.693s, 1476.98/s)  LR: 3.839e-05  Data: 0.009 (0.013)
Train: 535 [ 750/1251 ( 60%)]  Loss: 2.926 (3.18)  Time: 0.726s, 1410.09/s  (0.693s, 1476.94/s)  LR: 3.839e-05  Data: 0.011 (0.013)
Train: 535 [ 800/1251 ( 64%)]  Loss: 3.140 (3.18)  Time: 0.677s, 1513.12/s  (0.693s, 1477.16/s)  LR: 3.839e-05  Data: 0.011 (0.013)
Train: 535 [ 850/1251 ( 68%)]  Loss: 3.304 (3.19)  Time: 0.699s, 1463.93/s  (0.693s, 1477.73/s)  LR: 3.839e-05  Data: 0.009 (0.013)
Train: 535 [ 900/1251 ( 72%)]  Loss: 3.399 (3.20)  Time: 0.683s, 1499.49/s  (0.692s, 1478.79/s)  LR: 3.839e-05  Data: 0.012 (0.013)
Train: 535 [ 950/1251 ( 76%)]  Loss: 3.312 (3.20)  Time: 0.674s, 1518.74/s  (0.692s, 1479.06/s)  LR: 3.839e-05  Data: 0.013 (0.012)
Train: 535 [1000/1251 ( 80%)]  Loss: 2.889 (3.19)  Time: 0.684s, 1497.98/s  (0.693s, 1478.36/s)  LR: 3.839e-05  Data: 0.010 (0.012)
Train: 535 [1050/1251 ( 84%)]  Loss: 3.308 (3.19)  Time: 0.780s, 1313.08/s  (0.693s, 1478.67/s)  LR: 3.839e-05  Data: 0.009 (0.012)
Train: 535 [1100/1251 ( 88%)]  Loss: 3.183 (3.19)  Time: 0.671s, 1526.36/s  (0.693s, 1478.49/s)  LR: 3.839e-05  Data: 0.010 (0.012)
Train: 535 [1150/1251 ( 92%)]  Loss: 2.940 (3.18)  Time: 0.675s, 1517.51/s  (0.693s, 1478.70/s)  LR: 3.839e-05  Data: 0.010 (0.012)
Train: 535 [1200/1251 ( 96%)]  Loss: 3.140 (3.18)  Time: 0.704s, 1454.27/s  (0.692s, 1478.72/s)  LR: 3.839e-05  Data: 0.009 (0.012)
Train: 535 [1250/1251 (100%)]  Loss: 3.292 (3.19)  Time: 0.657s, 1559.17/s  (0.693s, 1478.28/s)  LR: 3.839e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.544 (1.544)  Loss:  0.6240 (0.6240)  Acc@1: 91.3086 (91.3086)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.137 (0.584)  Loss:  0.7285 (1.1116)  Acc@1: 86.9104 (78.8220)  Acc@5: 97.2877 (94.3500)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-535.pth.tar', 78.8220000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-534.pth.tar', 78.802)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-530.pth.tar', 78.74600005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-532.pth.tar', 78.71400018310547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-523.pth.tar', 78.69399997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-529.pth.tar', 78.69200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-531.pth.tar', 78.6720000805664)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-524.pth.tar', 78.66600005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-527.pth.tar', 78.66000005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-526.pth.tar', 78.63800010986328)

Train: 536 [   0/1251 (  0%)]  Loss: 3.407 (3.41)  Time: 2.162s,  473.60/s  (2.162s,  473.60/s)  LR: 3.753e-05  Data: 1.489 (1.489)
Train: 536 [  50/1251 (  4%)]  Loss: 3.428 (3.42)  Time: 0.711s, 1440.00/s  (0.724s, 1414.11/s)  LR: 3.753e-05  Data: 0.011 (0.045)
Train: 536 [ 100/1251 (  8%)]  Loss: 2.845 (3.23)  Time: 0.716s, 1430.70/s  (0.712s, 1438.86/s)  LR: 3.753e-05  Data: 0.011 (0.028)
Train: 536 [ 150/1251 ( 12%)]  Loss: 3.496 (3.29)  Time: 0.688s, 1487.61/s  (0.705s, 1453.29/s)  LR: 3.753e-05  Data: 0.011 (0.022)
Train: 536 [ 200/1251 ( 16%)]  Loss: 3.347 (3.30)  Time: 0.669s, 1530.92/s  (0.702s, 1459.68/s)  LR: 3.753e-05  Data: 0.010 (0.019)
Train: 536 [ 250/1251 ( 20%)]  Loss: 2.853 (3.23)  Time: 0.675s, 1517.36/s  (0.700s, 1462.14/s)  LR: 3.753e-05  Data: 0.010 (0.017)
Train: 536 [ 300/1251 ( 24%)]  Loss: 3.616 (3.28)  Time: 0.697s, 1468.60/s  (0.698s, 1466.95/s)  LR: 3.753e-05  Data: 0.010 (0.016)
Train: 536 [ 350/1251 ( 28%)]  Loss: 3.170 (3.27)  Time: 0.685s, 1494.30/s  (0.698s, 1467.24/s)  LR: 3.753e-05  Data: 0.009 (0.015)
Train: 536 [ 400/1251 ( 32%)]  Loss: 2.973 (3.24)  Time: 0.702s, 1459.65/s  (0.698s, 1467.94/s)  LR: 3.753e-05  Data: 0.010 (0.015)
Train: 536 [ 450/1251 ( 36%)]  Loss: 3.646 (3.28)  Time: 0.714s, 1434.16/s  (0.697s, 1469.99/s)  LR: 3.753e-05  Data: 0.010 (0.014)
Train: 536 [ 500/1251 ( 40%)]  Loss: 3.367 (3.29)  Time: 0.665s, 1539.82/s  (0.696s, 1471.46/s)  LR: 3.753e-05  Data: 0.010 (0.014)
Train: 536 [ 550/1251 ( 44%)]  Loss: 3.143 (3.27)  Time: 0.667s, 1534.09/s  (0.695s, 1472.60/s)  LR: 3.753e-05  Data: 0.008 (0.014)
Train: 536 [ 600/1251 ( 48%)]  Loss: 3.292 (3.28)  Time: 0.703s, 1455.98/s  (0.695s, 1473.82/s)  LR: 3.753e-05  Data: 0.010 (0.013)
Train: 536 [ 650/1251 ( 52%)]  Loss: 3.108 (3.26)  Time: 0.709s, 1443.89/s  (0.695s, 1474.03/s)  LR: 3.753e-05  Data: 0.009 (0.013)
Train: 536 [ 700/1251 ( 56%)]  Loss: 3.277 (3.26)  Time: 0.675s, 1517.13/s  (0.694s, 1474.48/s)  LR: 3.753e-05  Data: 0.011 (0.013)
Train: 536 [ 750/1251 ( 60%)]  Loss: 3.178 (3.26)  Time: 0.677s, 1512.69/s  (0.695s, 1474.41/s)  LR: 3.753e-05  Data: 0.012 (0.013)
Train: 536 [ 800/1251 ( 64%)]  Loss: 3.482 (3.27)  Time: 0.739s, 1385.63/s  (0.694s, 1475.14/s)  LR: 3.753e-05  Data: 0.010 (0.013)
Train: 536 [ 850/1251 ( 68%)]  Loss: 3.289 (3.27)  Time: 0.721s, 1420.65/s  (0.694s, 1475.15/s)  LR: 3.753e-05  Data: 0.009 (0.013)
Train: 536 [ 900/1251 ( 72%)]  Loss: 3.215 (3.27)  Time: 0.673s, 1522.04/s  (0.694s, 1475.93/s)  LR: 3.753e-05  Data: 0.013 (0.012)
Train: 536 [ 950/1251 ( 76%)]  Loss: 3.136 (3.26)  Time: 0.712s, 1437.94/s  (0.694s, 1475.91/s)  LR: 3.753e-05  Data: 0.011 (0.012)
Train: 536 [1000/1251 ( 80%)]  Loss: 3.229 (3.26)  Time: 0.714s, 1435.15/s  (0.694s, 1476.23/s)  LR: 3.753e-05  Data: 0.010 (0.012)
Train: 536 [1050/1251 ( 84%)]  Loss: 3.571 (3.28)  Time: 0.672s, 1523.91/s  (0.693s, 1476.57/s)  LR: 3.753e-05  Data: 0.010 (0.012)
Train: 536 [1100/1251 ( 88%)]  Loss: 3.156 (3.27)  Time: 0.673s, 1520.99/s  (0.693s, 1477.32/s)  LR: 3.753e-05  Data: 0.011 (0.012)
Train: 536 [1150/1251 ( 92%)]  Loss: 3.623 (3.29)  Time: 0.688s, 1488.89/s  (0.694s, 1476.49/s)  LR: 3.753e-05  Data: 0.017 (0.012)
Train: 536 [1200/1251 ( 96%)]  Loss: 2.788 (3.27)  Time: 0.684s, 1497.48/s  (0.694s, 1476.23/s)  LR: 3.753e-05  Data: 0.011 (0.012)
Train: 536 [1250/1251 (100%)]  Loss: 3.529 (3.28)  Time: 0.668s, 1532.43/s  (0.694s, 1474.45/s)  LR: 3.753e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.736 (1.736)  Loss:  0.7720 (0.7720)  Acc@1: 91.7969 (91.7969)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.136 (0.665)  Loss:  0.8594 (1.2634)  Acc@1: 86.9104 (78.6660)  Acc@5: 96.9340 (94.2120)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-535.pth.tar', 78.8220000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-534.pth.tar', 78.802)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-530.pth.tar', 78.74600005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-532.pth.tar', 78.71400018310547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-523.pth.tar', 78.69399997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-529.pth.tar', 78.69200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-531.pth.tar', 78.6720000805664)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-524.pth.tar', 78.66600005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-536.pth.tar', 78.66600000244141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-527.pth.tar', 78.66000005615234)

Train: 537 [   0/1251 (  0%)]  Loss: 2.817 (2.82)  Time: 2.316s,  442.06/s  (2.316s,  442.06/s)  LR: 3.669e-05  Data: 1.675 (1.675)
Train: 537 [  50/1251 (  4%)]  Loss: 3.211 (3.01)  Time: 0.667s, 1534.38/s  (0.755s, 1357.06/s)  LR: 3.669e-05  Data: 0.010 (0.064)
Train: 537 [ 100/1251 (  8%)]  Loss: 3.467 (3.17)  Time: 0.667s, 1536.35/s  (0.716s, 1430.38/s)  LR: 3.669e-05  Data: 0.009 (0.038)
Train: 537 [ 150/1251 ( 12%)]  Loss: 3.499 (3.25)  Time: 0.672s, 1524.74/s  (0.702s, 1458.57/s)  LR: 3.669e-05  Data: 0.011 (0.029)
Train: 537 [ 200/1251 ( 16%)]  Loss: 3.266 (3.25)  Time: 0.708s, 1446.45/s  (0.697s, 1469.62/s)  LR: 3.669e-05  Data: 0.013 (0.024)
Train: 537 [ 250/1251 ( 20%)]  Loss: 3.294 (3.26)  Time: 0.666s, 1537.20/s  (0.698s, 1467.76/s)  LR: 3.669e-05  Data: 0.009 (0.021)
Train: 537 [ 300/1251 ( 24%)]  Loss: 3.657 (3.32)  Time: 0.719s, 1424.94/s  (0.697s, 1469.83/s)  LR: 3.669e-05  Data: 0.009 (0.019)
Train: 537 [ 350/1251 ( 28%)]  Loss: 3.437 (3.33)  Time: 0.714s, 1434.48/s  (0.697s, 1469.56/s)  LR: 3.669e-05  Data: 0.011 (0.018)
Train: 537 [ 400/1251 ( 32%)]  Loss: 3.230 (3.32)  Time: 0.677s, 1512.47/s  (0.696s, 1472.12/s)  LR: 3.669e-05  Data: 0.009 (0.017)
Train: 537 [ 450/1251 ( 36%)]  Loss: 2.687 (3.26)  Time: 0.723s, 1415.90/s  (0.695s, 1472.89/s)  LR: 3.669e-05  Data: 0.011 (0.016)
Train: 537 [ 500/1251 ( 40%)]  Loss: 3.483 (3.28)  Time: 0.674s, 1519.27/s  (0.694s, 1474.58/s)  LR: 3.669e-05  Data: 0.010 (0.016)
Train: 537 [ 550/1251 ( 44%)]  Loss: 3.186 (3.27)  Time: 0.710s, 1442.48/s  (0.694s, 1474.56/s)  LR: 3.669e-05  Data: 0.012 (0.015)
Train: 537 [ 600/1251 ( 48%)]  Loss: 3.379 (3.28)  Time: 0.696s, 1472.27/s  (0.695s, 1473.83/s)  LR: 3.669e-05  Data: 0.012 (0.015)
Train: 537 [ 650/1251 ( 52%)]  Loss: 3.240 (3.28)  Time: 0.673s, 1522.45/s  (0.695s, 1473.00/s)  LR: 3.669e-05  Data: 0.011 (0.015)
Train: 537 [ 700/1251 ( 56%)]  Loss: 3.180 (3.27)  Time: 0.670s, 1527.82/s  (0.695s, 1474.27/s)  LR: 3.669e-05  Data: 0.011 (0.014)
Train: 537 [ 750/1251 ( 60%)]  Loss: 3.247 (3.27)  Time: 0.669s, 1531.45/s  (0.694s, 1475.21/s)  LR: 3.669e-05  Data: 0.009 (0.014)
Train: 537 [ 800/1251 ( 64%)]  Loss: 3.016 (3.25)  Time: 0.708s, 1445.71/s  (0.694s, 1475.49/s)  LR: 3.669e-05  Data: 0.011 (0.014)
Train: 537 [ 850/1251 ( 68%)]  Loss: 3.227 (3.25)  Time: 0.682s, 1502.44/s  (0.694s, 1476.11/s)  LR: 3.669e-05  Data: 0.010 (0.014)
Train: 537 [ 900/1251 ( 72%)]  Loss: 3.413 (3.26)  Time: 0.681s, 1503.37/s  (0.694s, 1476.50/s)  LR: 3.669e-05  Data: 0.010 (0.013)
Train: 537 [ 950/1251 ( 76%)]  Loss: 3.012 (3.25)  Time: 0.696s, 1470.92/s  (0.694s, 1476.07/s)  LR: 3.669e-05  Data: 0.010 (0.013)
Train: 537 [1000/1251 ( 80%)]  Loss: 3.291 (3.25)  Time: 0.694s, 1475.92/s  (0.693s, 1476.59/s)  LR: 3.669e-05  Data: 0.009 (0.013)
Train: 537 [1050/1251 ( 84%)]  Loss: 3.285 (3.25)  Time: 0.720s, 1422.54/s  (0.694s, 1475.88/s)  LR: 3.669e-05  Data: 0.009 (0.013)
Train: 537 [1100/1251 ( 88%)]  Loss: 2.783 (3.23)  Time: 0.713s, 1435.34/s  (0.693s, 1476.80/s)  LR: 3.669e-05  Data: 0.010 (0.013)
Train: 537 [1150/1251 ( 92%)]  Loss: 3.135 (3.23)  Time: 0.674s, 1519.59/s  (0.693s, 1477.36/s)  LR: 3.669e-05  Data: 0.010 (0.013)
Train: 537 [1200/1251 ( 96%)]  Loss: 3.251 (3.23)  Time: 0.670s, 1528.04/s  (0.693s, 1477.42/s)  LR: 3.669e-05  Data: 0.010 (0.013)
Train: 537 [1250/1251 (100%)]  Loss: 2.967 (3.22)  Time: 0.654s, 1565.29/s  (0.693s, 1478.06/s)  LR: 3.669e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.615 (1.615)  Loss:  0.5996 (0.5996)  Acc@1: 91.8945 (91.8945)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.136 (0.578)  Loss:  0.7207 (1.0847)  Acc@1: 86.4387 (78.7560)  Acc@5: 97.2877 (94.3320)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-535.pth.tar', 78.8220000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-534.pth.tar', 78.802)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-537.pth.tar', 78.75600005615235)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-530.pth.tar', 78.74600005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-532.pth.tar', 78.71400018310547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-523.pth.tar', 78.69399997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-529.pth.tar', 78.69200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-531.pth.tar', 78.6720000805664)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-524.pth.tar', 78.66600005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-536.pth.tar', 78.66600000244141)

Train: 538 [   0/1251 (  0%)]  Loss: 3.525 (3.53)  Time: 2.192s,  467.26/s  (2.192s,  467.26/s)  LR: 3.585e-05  Data: 1.575 (1.575)
Train: 538 [  50/1251 (  4%)]  Loss: 3.033 (3.28)  Time: 0.674s, 1519.24/s  (0.736s, 1391.08/s)  LR: 3.585e-05  Data: 0.011 (0.049)
Train: 538 [ 100/1251 (  8%)]  Loss: 3.314 (3.29)  Time: 0.705s, 1453.14/s  (0.718s, 1425.59/s)  LR: 3.585e-05  Data: 0.010 (0.030)
Train: 538 [ 150/1251 ( 12%)]  Loss: 3.320 (3.30)  Time: 0.666s, 1536.50/s  (0.709s, 1445.00/s)  LR: 3.585e-05  Data: 0.010 (0.023)
Train: 538 [ 200/1251 ( 16%)]  Loss: 3.120 (3.26)  Time: 0.672s, 1524.00/s  (0.704s, 1454.53/s)  LR: 3.585e-05  Data: 0.011 (0.020)
Train: 538 [ 250/1251 ( 20%)]  Loss: 3.129 (3.24)  Time: 0.686s, 1492.41/s  (0.701s, 1460.22/s)  LR: 3.585e-05  Data: 0.015 (0.018)
Train: 538 [ 300/1251 ( 24%)]  Loss: 3.116 (3.22)  Time: 0.602s, 1699.79/s  (0.701s, 1461.25/s)  LR: 3.585e-05  Data: 0.011 (0.017)
Train: 538 [ 350/1251 ( 28%)]  Loss: 3.237 (3.22)  Time: 0.706s, 1449.58/s  (0.701s, 1461.43/s)  LR: 3.585e-05  Data: 0.011 (0.016)
Train: 538 [ 400/1251 ( 32%)]  Loss: 3.180 (3.22)  Time: 0.710s, 1442.43/s  (0.699s, 1464.86/s)  LR: 3.585e-05  Data: 0.009 (0.015)
Train: 538 [ 450/1251 ( 36%)]  Loss: 2.840 (3.18)  Time: 0.675s, 1517.49/s  (0.699s, 1465.81/s)  LR: 3.585e-05  Data: 0.009 (0.015)
Train: 538 [ 500/1251 ( 40%)]  Loss: 2.974 (3.16)  Time: 0.723s, 1416.54/s  (0.698s, 1467.66/s)  LR: 3.585e-05  Data: 0.010 (0.014)
Train: 538 [ 550/1251 ( 44%)]  Loss: 3.087 (3.16)  Time: 0.672s, 1523.79/s  (0.697s, 1468.26/s)  LR: 3.585e-05  Data: 0.011 (0.014)
Train: 538 [ 600/1251 ( 48%)]  Loss: 3.156 (3.16)  Time: 0.695s, 1472.58/s  (0.696s, 1470.27/s)  LR: 3.585e-05  Data: 0.011 (0.014)
Train: 538 [ 650/1251 ( 52%)]  Loss: 2.921 (3.14)  Time: 0.677s, 1513.61/s  (0.696s, 1470.67/s)  LR: 3.585e-05  Data: 0.010 (0.014)
Train: 538 [ 700/1251 ( 56%)]  Loss: 3.581 (3.17)  Time: 0.701s, 1460.75/s  (0.696s, 1470.80/s)  LR: 3.585e-05  Data: 0.010 (0.013)
Train: 538 [ 750/1251 ( 60%)]  Loss: 3.153 (3.17)  Time: 0.729s, 1405.12/s  (0.696s, 1470.69/s)  LR: 3.585e-05  Data: 0.009 (0.013)
Train: 538 [ 800/1251 ( 64%)]  Loss: 3.061 (3.16)  Time: 0.672s, 1523.62/s  (0.696s, 1471.17/s)  LR: 3.585e-05  Data: 0.011 (0.013)
Train: 538 [ 850/1251 ( 68%)]  Loss: 3.028 (3.15)  Time: 0.683s, 1499.66/s  (0.696s, 1472.14/s)  LR: 3.585e-05  Data: 0.009 (0.013)
Train: 538 [ 900/1251 ( 72%)]  Loss: 2.839 (3.14)  Time: 0.671s, 1525.15/s  (0.695s, 1472.94/s)  LR: 3.585e-05  Data: 0.011 (0.013)
Train: 538 [ 950/1251 ( 76%)]  Loss: 3.196 (3.14)  Time: 0.673s, 1522.34/s  (0.695s, 1473.87/s)  LR: 3.585e-05  Data: 0.011 (0.013)
Train: 538 [1000/1251 ( 80%)]  Loss: 3.259 (3.15)  Time: 0.672s, 1524.46/s  (0.694s, 1474.78/s)  LR: 3.585e-05  Data: 0.010 (0.012)
Train: 538 [1050/1251 ( 84%)]  Loss: 3.535 (3.16)  Time: 0.726s, 1409.78/s  (0.694s, 1475.19/s)  LR: 3.585e-05  Data: 0.010 (0.012)
Train: 538 [1100/1251 ( 88%)]  Loss: 3.267 (3.17)  Time: 0.665s, 1539.57/s  (0.694s, 1475.13/s)  LR: 3.585e-05  Data: 0.010 (0.012)
Train: 538 [1150/1251 ( 92%)]  Loss: 3.246 (3.17)  Time: 0.693s, 1477.30/s  (0.694s, 1474.67/s)  LR: 3.585e-05  Data: 0.009 (0.012)
Train: 538 [1200/1251 ( 96%)]  Loss: 3.278 (3.18)  Time: 0.706s, 1451.44/s  (0.694s, 1475.39/s)  LR: 3.585e-05  Data: 0.010 (0.012)
Train: 538 [1250/1251 (100%)]  Loss: 3.390 (3.18)  Time: 0.658s, 1555.80/s  (0.694s, 1475.05/s)  LR: 3.585e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.594 (1.594)  Loss:  0.6875 (0.6875)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.136 (0.573)  Loss:  0.8071 (1.1865)  Acc@1: 87.2642 (78.8540)  Acc@5: 97.5236 (94.4020)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-538.pth.tar', 78.85400015625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-535.pth.tar', 78.8220000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-534.pth.tar', 78.802)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-537.pth.tar', 78.75600005615235)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-530.pth.tar', 78.74600005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-532.pth.tar', 78.71400018310547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-523.pth.tar', 78.69399997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-529.pth.tar', 78.69200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-531.pth.tar', 78.6720000805664)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-524.pth.tar', 78.66600005615234)

Train: 539 [   0/1251 (  0%)]  Loss: 3.104 (3.10)  Time: 2.203s,  464.81/s  (2.203s,  464.81/s)  LR: 3.503e-05  Data: 1.587 (1.587)
Train: 539 [  50/1251 (  4%)]  Loss: 3.211 (3.16)  Time: 0.671s, 1525.70/s  (0.725s, 1411.76/s)  LR: 3.503e-05  Data: 0.010 (0.051)
Train: 539 [ 100/1251 (  8%)]  Loss: 3.240 (3.19)  Time: 0.673s, 1521.76/s  (0.709s, 1443.44/s)  LR: 3.503e-05  Data: 0.010 (0.031)
Train: 539 [ 150/1251 ( 12%)]  Loss: 3.263 (3.20)  Time: 0.672s, 1523.26/s  (0.703s, 1456.73/s)  LR: 3.503e-05  Data: 0.010 (0.024)
Train: 539 [ 200/1251 ( 16%)]  Loss: 3.094 (3.18)  Time: 0.675s, 1517.51/s  (0.702s, 1459.54/s)  LR: 3.503e-05  Data: 0.010 (0.021)
Train: 539 [ 250/1251 ( 20%)]  Loss: 3.354 (3.21)  Time: 0.688s, 1487.88/s  (0.700s, 1461.84/s)  LR: 3.503e-05  Data: 0.010 (0.019)
Train: 539 [ 300/1251 ( 24%)]  Loss: 3.346 (3.23)  Time: 0.671s, 1526.78/s  (0.699s, 1464.11/s)  LR: 3.503e-05  Data: 0.014 (0.017)
Train: 539 [ 350/1251 ( 28%)]  Loss: 3.083 (3.21)  Time: 0.673s, 1521.96/s  (0.698s, 1466.18/s)  LR: 3.503e-05  Data: 0.010 (0.016)
Train: 539 [ 400/1251 ( 32%)]  Loss: 3.337 (3.23)  Time: 0.670s, 1527.74/s  (0.698s, 1467.26/s)  LR: 3.503e-05  Data: 0.010 (0.016)
Train: 539 [ 450/1251 ( 36%)]  Loss: 3.251 (3.23)  Time: 0.728s, 1407.31/s  (0.697s, 1468.31/s)  LR: 3.503e-05  Data: 0.009 (0.015)
Train: 539 [ 500/1251 ( 40%)]  Loss: 3.185 (3.22)  Time: 0.722s, 1418.45/s  (0.697s, 1470.10/s)  LR: 3.503e-05  Data: 0.010 (0.015)
Train: 539 [ 550/1251 ( 44%)]  Loss: 3.203 (3.22)  Time: 0.676s, 1514.98/s  (0.696s, 1471.27/s)  LR: 3.503e-05  Data: 0.010 (0.014)
Train: 539 [ 600/1251 ( 48%)]  Loss: 3.198 (3.22)  Time: 0.686s, 1492.46/s  (0.696s, 1471.95/s)  LR: 3.503e-05  Data: 0.012 (0.014)
Train: 539 [ 650/1251 ( 52%)]  Loss: 3.288 (3.23)  Time: 0.698s, 1466.82/s  (0.696s, 1472.07/s)  LR: 3.503e-05  Data: 0.012 (0.014)
Train: 539 [ 700/1251 ( 56%)]  Loss: 3.024 (3.21)  Time: 0.670s, 1529.05/s  (0.695s, 1472.53/s)  LR: 3.503e-05  Data: 0.010 (0.013)
Train: 539 [ 750/1251 ( 60%)]  Loss: 3.121 (3.21)  Time: 0.671s, 1526.23/s  (0.695s, 1473.27/s)  LR: 3.503e-05  Data: 0.009 (0.013)
Train: 539 [ 800/1251 ( 64%)]  Loss: 3.326 (3.21)  Time: 0.671s, 1526.33/s  (0.695s, 1473.68/s)  LR: 3.503e-05  Data: 0.010 (0.013)
Train: 539 [ 850/1251 ( 68%)]  Loss: 2.998 (3.20)  Time: 0.709s, 1445.31/s  (0.695s, 1473.93/s)  LR: 3.503e-05  Data: 0.012 (0.013)
Train: 539 [ 900/1251 ( 72%)]  Loss: 2.999 (3.19)  Time: 0.667s, 1535.60/s  (0.694s, 1474.59/s)  LR: 3.503e-05  Data: 0.009 (0.013)
Train: 539 [ 950/1251 ( 76%)]  Loss: 3.067 (3.18)  Time: 0.674s, 1520.01/s  (0.695s, 1474.19/s)  LR: 3.503e-05  Data: 0.011 (0.013)
Train: 539 [1000/1251 ( 80%)]  Loss: 3.731 (3.21)  Time: 0.672s, 1524.13/s  (0.694s, 1474.83/s)  LR: 3.503e-05  Data: 0.009 (0.013)
Train: 539 [1050/1251 ( 84%)]  Loss: 3.147 (3.21)  Time: 0.670s, 1527.32/s  (0.694s, 1475.26/s)  LR: 3.503e-05  Data: 0.011 (0.012)
Train: 539 [1100/1251 ( 88%)]  Loss: 3.186 (3.21)  Time: 0.705s, 1452.16/s  (0.694s, 1474.82/s)  LR: 3.503e-05  Data: 0.011 (0.012)
Train: 539 [1150/1251 ( 92%)]  Loss: 3.133 (3.20)  Time: 0.683s, 1498.30/s  (0.694s, 1475.19/s)  LR: 3.503e-05  Data: 0.010 (0.012)
Train: 539 [1200/1251 ( 96%)]  Loss: 3.051 (3.20)  Time: 0.739s, 1385.69/s  (0.694s, 1474.82/s)  LR: 3.503e-05  Data: 0.009 (0.012)
Train: 539 [1250/1251 (100%)]  Loss: 2.980 (3.19)  Time: 0.655s, 1563.42/s  (0.694s, 1474.45/s)  LR: 3.503e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.446 (1.446)  Loss:  0.6860 (0.6860)  Acc@1: 91.6992 (91.6992)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.136 (0.583)  Loss:  0.8223 (1.1854)  Acc@1: 86.3208 (78.7340)  Acc@5: 96.4623 (94.2540)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-538.pth.tar', 78.85400015625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-535.pth.tar', 78.8220000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-534.pth.tar', 78.802)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-537.pth.tar', 78.75600005615235)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-530.pth.tar', 78.74600005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-539.pth.tar', 78.73400013427734)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-532.pth.tar', 78.71400018310547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-523.pth.tar', 78.69399997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-529.pth.tar', 78.69200002929688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-531.pth.tar', 78.6720000805664)

Train: 540 [   0/1251 (  0%)]  Loss: 3.399 (3.40)  Time: 2.287s,  447.69/s  (2.287s,  447.69/s)  LR: 3.423e-05  Data: 1.622 (1.622)
Train: 540 [  50/1251 (  4%)]  Loss: 3.239 (3.32)  Time: 0.672s, 1524.92/s  (0.731s, 1400.57/s)  LR: 3.423e-05  Data: 0.009 (0.048)
Train: 540 [ 100/1251 (  8%)]  Loss: 3.293 (3.31)  Time: 0.727s, 1407.94/s  (0.716s, 1430.94/s)  LR: 3.423e-05  Data: 0.009 (0.030)
Train: 540 [ 150/1251 ( 12%)]  Loss: 3.084 (3.25)  Time: 0.674s, 1519.02/s  (0.708s, 1447.25/s)  LR: 3.423e-05  Data: 0.011 (0.023)
Train: 540 [ 200/1251 ( 16%)]  Loss: 3.212 (3.25)  Time: 0.672s, 1523.35/s  (0.704s, 1454.46/s)  LR: 3.423e-05  Data: 0.011 (0.020)
Train: 540 [ 250/1251 ( 20%)]  Loss: 3.383 (3.27)  Time: 0.673s, 1521.11/s  (0.700s, 1462.60/s)  LR: 3.423e-05  Data: 0.010 (0.018)
Train: 540 [ 300/1251 ( 24%)]  Loss: 3.163 (3.25)  Time: 0.670s, 1528.39/s  (0.699s, 1465.39/s)  LR: 3.423e-05  Data: 0.010 (0.017)
Train: 540 [ 350/1251 ( 28%)]  Loss: 3.078 (3.23)  Time: 0.672s, 1522.98/s  (0.698s, 1467.87/s)  LR: 3.423e-05  Data: 0.012 (0.016)
Train: 540 [ 400/1251 ( 32%)]  Loss: 3.198 (3.23)  Time: 0.706s, 1450.79/s  (0.698s, 1467.77/s)  LR: 3.423e-05  Data: 0.011 (0.015)
Train: 540 [ 450/1251 ( 36%)]  Loss: 3.342 (3.24)  Time: 0.673s, 1522.02/s  (0.697s, 1468.56/s)  LR: 3.423e-05  Data: 0.013 (0.015)
Train: 540 [ 500/1251 ( 40%)]  Loss: 3.326 (3.25)  Time: 0.716s, 1429.62/s  (0.697s, 1469.15/s)  LR: 3.423e-05  Data: 0.011 (0.014)
Train: 540 [ 550/1251 ( 44%)]  Loss: 2.886 (3.22)  Time: 0.716s, 1429.63/s  (0.697s, 1468.99/s)  LR: 3.423e-05  Data: 0.014 (0.014)
Train: 540 [ 600/1251 ( 48%)]  Loss: 3.351 (3.23)  Time: 0.670s, 1527.37/s  (0.696s, 1470.72/s)  LR: 3.423e-05  Data: 0.011 (0.014)
Train: 540 [ 650/1251 ( 52%)]  Loss: 2.911 (3.20)  Time: 0.702s, 1458.84/s  (0.695s, 1472.58/s)  LR: 3.423e-05  Data: 0.010 (0.013)
Train: 540 [ 700/1251 ( 56%)]  Loss: 3.122 (3.20)  Time: 0.703s, 1456.45/s  (0.695s, 1473.16/s)  LR: 3.423e-05  Data: 0.010 (0.013)
Train: 540 [ 750/1251 ( 60%)]  Loss: 2.797 (3.17)  Time: 0.707s, 1448.09/s  (0.695s, 1472.56/s)  LR: 3.423e-05  Data: 0.009 (0.013)
Train: 540 [ 800/1251 ( 64%)]  Loss: 3.293 (3.18)  Time: 0.672s, 1523.33/s  (0.695s, 1473.11/s)  LR: 3.423e-05  Data: 0.011 (0.013)
Train: 540 [ 850/1251 ( 68%)]  Loss: 3.276 (3.19)  Time: 0.673s, 1522.45/s  (0.695s, 1473.88/s)  LR: 3.423e-05  Data: 0.011 (0.013)
Train: 540 [ 900/1251 ( 72%)]  Loss: 2.994 (3.18)  Time: 0.719s, 1423.78/s  (0.695s, 1474.15/s)  LR: 3.423e-05  Data: 0.009 (0.013)
Train: 540 [ 950/1251 ( 76%)]  Loss: 3.130 (3.17)  Time: 0.672s, 1523.75/s  (0.694s, 1474.44/s)  LR: 3.423e-05  Data: 0.010 (0.012)
Train: 540 [1000/1251 ( 80%)]  Loss: 2.999 (3.17)  Time: 0.669s, 1529.76/s  (0.694s, 1474.81/s)  LR: 3.423e-05  Data: 0.010 (0.012)
Train: 540 [1050/1251 ( 84%)]  Loss: 2.814 (3.15)  Time: 0.694s, 1475.44/s  (0.694s, 1475.09/s)  LR: 3.423e-05  Data: 0.011 (0.012)
Train: 540 [1100/1251 ( 88%)]  Loss: 2.910 (3.14)  Time: 0.672s, 1523.05/s  (0.694s, 1475.08/s)  LR: 3.423e-05  Data: 0.009 (0.012)
Train: 540 [1150/1251 ( 92%)]  Loss: 2.972 (3.13)  Time: 0.674s, 1520.25/s  (0.694s, 1475.17/s)  LR: 3.423e-05  Data: 0.011 (0.012)
Train: 540 [1200/1251 ( 96%)]  Loss: 3.329 (3.14)  Time: 0.673s, 1522.42/s  (0.694s, 1475.78/s)  LR: 3.423e-05  Data: 0.011 (0.012)
Train: 540 [1250/1251 (100%)]  Loss: 3.426 (3.15)  Time: 0.689s, 1486.83/s  (0.694s, 1476.02/s)  LR: 3.423e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.553 (1.553)  Loss:  0.7246 (0.7246)  Acc@1: 91.7969 (91.7969)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.137 (0.579)  Loss:  0.8369 (1.2074)  Acc@1: 86.5566 (78.7000)  Acc@5: 96.8160 (94.3340)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-538.pth.tar', 78.85400015625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-535.pth.tar', 78.8220000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-534.pth.tar', 78.802)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-537.pth.tar', 78.75600005615235)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-530.pth.tar', 78.74600005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-539.pth.tar', 78.73400013427734)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-532.pth.tar', 78.71400018310547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-540.pth.tar', 78.70000023681641)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-523.pth.tar', 78.69399997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-529.pth.tar', 78.69200002929688)

Train: 541 [   0/1251 (  0%)]  Loss: 3.028 (3.03)  Time: 2.740s,  373.70/s  (2.740s,  373.70/s)  LR: 3.343e-05  Data: 2.105 (2.105)
Train: 541 [  50/1251 (  4%)]  Loss: 3.128 (3.08)  Time: 0.701s, 1461.73/s  (0.731s, 1400.60/s)  LR: 3.343e-05  Data: 0.009 (0.052)
Train: 541 [ 100/1251 (  8%)]  Loss: 3.266 (3.14)  Time: 0.683s, 1500.18/s  (0.711s, 1440.56/s)  LR: 3.343e-05  Data: 0.011 (0.031)
Train: 541 [ 150/1251 ( 12%)]  Loss: 3.368 (3.20)  Time: 0.705s, 1452.26/s  (0.704s, 1454.90/s)  LR: 3.343e-05  Data: 0.009 (0.024)
Train: 541 [ 200/1251 ( 16%)]  Loss: 3.162 (3.19)  Time: 0.670s, 1527.75/s  (0.701s, 1461.14/s)  LR: 3.343e-05  Data: 0.010 (0.021)
Train: 541 [ 250/1251 ( 20%)]  Loss: 3.395 (3.22)  Time: 0.689s, 1486.54/s  (0.699s, 1465.91/s)  LR: 3.343e-05  Data: 0.011 (0.019)
Train: 541 [ 300/1251 ( 24%)]  Loss: 3.635 (3.28)  Time: 0.786s, 1302.53/s  (0.698s, 1466.85/s)  LR: 3.343e-05  Data: 0.010 (0.017)
Train: 541 [ 350/1251 ( 28%)]  Loss: 3.124 (3.26)  Time: 0.672s, 1524.61/s  (0.697s, 1470.16/s)  LR: 3.343e-05  Data: 0.009 (0.016)
Train: 541 [ 400/1251 ( 32%)]  Loss: 3.139 (3.25)  Time: 0.676s, 1514.15/s  (0.696s, 1471.51/s)  LR: 3.343e-05  Data: 0.010 (0.016)
Train: 541 [ 450/1251 ( 36%)]  Loss: 3.575 (3.28)  Time: 0.671s, 1526.24/s  (0.695s, 1473.84/s)  LR: 3.343e-05  Data: 0.010 (0.015)
Train: 541 [ 500/1251 ( 40%)]  Loss: 3.368 (3.29)  Time: 0.671s, 1525.47/s  (0.694s, 1474.88/s)  LR: 3.343e-05  Data: 0.010 (0.014)
Train: 541 [ 550/1251 ( 44%)]  Loss: 3.265 (3.29)  Time: 0.713s, 1435.24/s  (0.694s, 1474.94/s)  LR: 3.343e-05  Data: 0.010 (0.014)
Train: 541 [ 600/1251 ( 48%)]  Loss: 3.478 (3.30)  Time: 0.674s, 1518.96/s  (0.694s, 1475.79/s)  LR: 3.343e-05  Data: 0.010 (0.014)
Train: 541 [ 650/1251 ( 52%)]  Loss: 3.021 (3.28)  Time: 0.673s, 1521.78/s  (0.694s, 1475.58/s)  LR: 3.343e-05  Data: 0.010 (0.014)
Train: 541 [ 700/1251 ( 56%)]  Loss: 3.172 (3.28)  Time: 0.674s, 1518.17/s  (0.694s, 1476.00/s)  LR: 3.343e-05  Data: 0.009 (0.013)
Train: 541 [ 750/1251 ( 60%)]  Loss: 3.435 (3.29)  Time: 0.708s, 1446.39/s  (0.693s, 1476.89/s)  LR: 3.343e-05  Data: 0.010 (0.013)
Train: 541 [ 800/1251 ( 64%)]  Loss: 3.407 (3.29)  Time: 0.671s, 1526.50/s  (0.693s, 1477.95/s)  LR: 3.343e-05  Data: 0.010 (0.013)
Train: 541 [ 850/1251 ( 68%)]  Loss: 3.362 (3.30)  Time: 0.707s, 1448.42/s  (0.693s, 1477.45/s)  LR: 3.343e-05  Data: 0.011 (0.013)
Train: 541 [ 900/1251 ( 72%)]  Loss: 2.972 (3.28)  Time: 0.671s, 1525.64/s  (0.693s, 1476.84/s)  LR: 3.343e-05  Data: 0.010 (0.013)
Train: 541 [ 950/1251 ( 76%)]  Loss: 3.171 (3.27)  Time: 0.689s, 1485.49/s  (0.693s, 1477.27/s)  LR: 3.343e-05  Data: 0.009 (0.013)
Train: 541 [1000/1251 ( 80%)]  Loss: 2.799 (3.25)  Time: 0.684s, 1497.17/s  (0.693s, 1477.42/s)  LR: 3.343e-05  Data: 0.010 (0.012)
Train: 541 [1050/1251 ( 84%)]  Loss: 3.561 (3.27)  Time: 0.708s, 1446.07/s  (0.693s, 1478.09/s)  LR: 3.343e-05  Data: 0.016 (0.012)
Train: 541 [1100/1251 ( 88%)]  Loss: 3.185 (3.26)  Time: 0.676s, 1514.11/s  (0.693s, 1478.43/s)  LR: 3.343e-05  Data: 0.011 (0.012)
Train: 541 [1150/1251 ( 92%)]  Loss: 3.589 (3.28)  Time: 0.705s, 1452.47/s  (0.692s, 1478.81/s)  LR: 3.343e-05  Data: 0.009 (0.012)
Train: 541 [1200/1251 ( 96%)]  Loss: 3.163 (3.27)  Time: 0.686s, 1492.17/s  (0.693s, 1478.25/s)  LR: 3.343e-05  Data: 0.009 (0.012)
Train: 541 [1250/1251 (100%)]  Loss: 2.955 (3.26)  Time: 0.656s, 1561.65/s  (0.692s, 1478.70/s)  LR: 3.343e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.505 (1.505)  Loss:  0.7349 (0.7349)  Acc@1: 91.8945 (91.8945)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.136 (0.578)  Loss:  0.8569 (1.2195)  Acc@1: 86.5566 (78.8300)  Acc@5: 97.2877 (94.3600)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-538.pth.tar', 78.85400015625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-541.pth.tar', 78.83000010742188)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-535.pth.tar', 78.8220000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-534.pth.tar', 78.802)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-537.pth.tar', 78.75600005615235)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-530.pth.tar', 78.74600005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-539.pth.tar', 78.73400013427734)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-532.pth.tar', 78.71400018310547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-540.pth.tar', 78.70000023681641)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-523.pth.tar', 78.69399997558594)

Train: 542 [   0/1251 (  0%)]  Loss: 3.129 (3.13)  Time: 2.209s,  463.61/s  (2.209s,  463.61/s)  LR: 3.265e-05  Data: 1.591 (1.591)
Train: 542 [  50/1251 (  4%)]  Loss: 3.182 (3.16)  Time: 0.672s, 1523.79/s  (0.729s, 1403.89/s)  LR: 3.265e-05  Data: 0.011 (0.054)
Train: 542 [ 100/1251 (  8%)]  Loss: 3.400 (3.24)  Time: 0.677s, 1512.72/s  (0.713s, 1436.51/s)  LR: 3.265e-05  Data: 0.010 (0.032)
Train: 542 [ 150/1251 ( 12%)]  Loss: 3.129 (3.21)  Time: 0.688s, 1488.99/s  (0.704s, 1455.21/s)  LR: 3.265e-05  Data: 0.017 (0.025)
Train: 542 [ 200/1251 ( 16%)]  Loss: 3.195 (3.21)  Time: 0.672s, 1524.01/s  (0.702s, 1459.20/s)  LR: 3.265e-05  Data: 0.010 (0.021)
Train: 542 [ 250/1251 ( 20%)]  Loss: 2.898 (3.16)  Time: 0.706s, 1451.29/s  (0.700s, 1463.60/s)  LR: 3.265e-05  Data: 0.011 (0.019)
Train: 542 [ 300/1251 ( 24%)]  Loss: 2.790 (3.10)  Time: 0.707s, 1448.52/s  (0.699s, 1465.57/s)  LR: 3.265e-05  Data: 0.011 (0.018)
Train: 542 [ 350/1251 ( 28%)]  Loss: 3.275 (3.12)  Time: 0.673s, 1521.42/s  (0.698s, 1466.23/s)  LR: 3.265e-05  Data: 0.011 (0.017)
Train: 542 [ 400/1251 ( 32%)]  Loss: 3.499 (3.17)  Time: 0.672s, 1523.80/s  (0.697s, 1469.52/s)  LR: 3.265e-05  Data: 0.010 (0.016)
Train: 542 [ 450/1251 ( 36%)]  Loss: 3.453 (3.20)  Time: 0.687s, 1491.49/s  (0.696s, 1471.26/s)  LR: 3.265e-05  Data: 0.009 (0.015)
Train: 542 [ 500/1251 ( 40%)]  Loss: 3.248 (3.20)  Time: 0.697s, 1469.42/s  (0.697s, 1470.09/s)  LR: 3.265e-05  Data: 0.011 (0.015)
Train: 542 [ 550/1251 ( 44%)]  Loss: 3.174 (3.20)  Time: 0.670s, 1527.35/s  (0.696s, 1470.72/s)  LR: 3.265e-05  Data: 0.010 (0.014)
Train: 542 [ 600/1251 ( 48%)]  Loss: 3.316 (3.21)  Time: 0.674s, 1519.96/s  (0.697s, 1469.91/s)  LR: 3.265e-05  Data: 0.010 (0.014)
Train: 542 [ 650/1251 ( 52%)]  Loss: 3.339 (3.22)  Time: 0.710s, 1441.91/s  (0.696s, 1472.14/s)  LR: 3.265e-05  Data: 0.010 (0.014)
Train: 542 [ 700/1251 ( 56%)]  Loss: 2.850 (3.19)  Time: 0.710s, 1443.02/s  (0.695s, 1473.08/s)  LR: 3.265e-05  Data: 0.009 (0.014)
Train: 542 [ 750/1251 ( 60%)]  Loss: 3.089 (3.19)  Time: 0.756s, 1355.33/s  (0.696s, 1471.99/s)  LR: 3.265e-05  Data: 0.010 (0.013)
Train: 542 [ 800/1251 ( 64%)]  Loss: 2.987 (3.17)  Time: 0.742s, 1380.36/s  (0.695s, 1472.85/s)  LR: 3.265e-05  Data: 0.015 (0.013)
Train: 542 [ 850/1251 ( 68%)]  Loss: 3.052 (3.17)  Time: 0.685s, 1495.47/s  (0.696s, 1472.03/s)  LR: 3.265e-05  Data: 0.010 (0.013)
Train: 542 [ 900/1251 ( 72%)]  Loss: 3.058 (3.16)  Time: 0.703s, 1457.13/s  (0.695s, 1472.99/s)  LR: 3.265e-05  Data: 0.010 (0.013)
Train: 542 [ 950/1251 ( 76%)]  Loss: 3.163 (3.16)  Time: 0.671s, 1525.44/s  (0.695s, 1473.26/s)  LR: 3.265e-05  Data: 0.011 (0.013)
Train: 542 [1000/1251 ( 80%)]  Loss: 3.221 (3.16)  Time: 0.694s, 1474.93/s  (0.695s, 1473.77/s)  LR: 3.265e-05  Data: 0.010 (0.013)
Train: 542 [1050/1251 ( 84%)]  Loss: 3.424 (3.18)  Time: 0.707s, 1447.74/s  (0.694s, 1474.48/s)  LR: 3.265e-05  Data: 0.009 (0.013)
Train: 542 [1100/1251 ( 88%)]  Loss: 3.276 (3.18)  Time: 0.672s, 1524.02/s  (0.694s, 1474.52/s)  LR: 3.265e-05  Data: 0.009 (0.013)
Train: 542 [1150/1251 ( 92%)]  Loss: 3.124 (3.18)  Time: 0.716s, 1430.57/s  (0.695s, 1474.25/s)  LR: 3.265e-05  Data: 0.011 (0.012)
Train: 542 [1200/1251 ( 96%)]  Loss: 3.323 (3.18)  Time: 0.674s, 1519.33/s  (0.694s, 1474.46/s)  LR: 3.265e-05  Data: 0.010 (0.012)
Train: 542 [1250/1251 (100%)]  Loss: 3.307 (3.19)  Time: 0.656s, 1560.12/s  (0.694s, 1474.54/s)  LR: 3.265e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.539 (1.539)  Loss:  0.6064 (0.6064)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.136 (0.582)  Loss:  0.7080 (1.0918)  Acc@1: 87.0283 (78.9240)  Acc@5: 97.1698 (94.3820)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-542.pth.tar', 78.9239999243164)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-538.pth.tar', 78.85400015625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-541.pth.tar', 78.83000010742188)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-535.pth.tar', 78.8220000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-534.pth.tar', 78.802)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-537.pth.tar', 78.75600005615235)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-530.pth.tar', 78.74600005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-539.pth.tar', 78.73400013427734)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-532.pth.tar', 78.71400018310547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-540.pth.tar', 78.70000023681641)

Train: 543 [   0/1251 (  0%)]  Loss: 3.281 (3.28)  Time: 2.162s,  473.62/s  (2.162s,  473.62/s)  LR: 3.188e-05  Data: 1.547 (1.547)
Train: 543 [  50/1251 (  4%)]  Loss: 3.162 (3.22)  Time: 0.601s, 1702.47/s  (0.728s, 1406.06/s)  LR: 3.188e-05  Data: 0.011 (0.049)
Train: 543 [ 100/1251 (  8%)]  Loss: 3.172 (3.21)  Time: 0.691s, 1480.96/s  (0.710s, 1441.54/s)  LR: 3.188e-05  Data: 0.010 (0.030)
Train: 543 [ 150/1251 ( 12%)]  Loss: 3.237 (3.21)  Time: 0.700s, 1462.94/s  (0.706s, 1451.36/s)  LR: 3.188e-05  Data: 0.010 (0.024)
Train: 543 [ 200/1251 ( 16%)]  Loss: 3.152 (3.20)  Time: 0.696s, 1471.76/s  (0.705s, 1452.32/s)  LR: 3.188e-05  Data: 0.010 (0.020)
Train: 543 [ 250/1251 ( 20%)]  Loss: 3.152 (3.19)  Time: 0.671s, 1525.05/s  (0.703s, 1456.78/s)  LR: 3.188e-05  Data: 0.009 (0.018)
Train: 543 [ 300/1251 ( 24%)]  Loss: 3.189 (3.19)  Time: 0.701s, 1460.99/s  (0.702s, 1458.96/s)  LR: 3.188e-05  Data: 0.009 (0.017)
Train: 543 [ 350/1251 ( 28%)]  Loss: 3.425 (3.22)  Time: 0.727s, 1409.45/s  (0.700s, 1463.83/s)  LR: 3.188e-05  Data: 0.011 (0.016)
Train: 543 [ 400/1251 ( 32%)]  Loss: 3.556 (3.26)  Time: 0.678s, 1511.41/s  (0.699s, 1464.04/s)  LR: 3.188e-05  Data: 0.015 (0.015)
Train: 543 [ 450/1251 ( 36%)]  Loss: 3.304 (3.26)  Time: 0.736s, 1391.76/s  (0.699s, 1464.83/s)  LR: 3.188e-05  Data: 0.010 (0.015)
Train: 543 [ 500/1251 ( 40%)]  Loss: 3.066 (3.25)  Time: 0.671s, 1526.42/s  (0.698s, 1467.04/s)  LR: 3.188e-05  Data: 0.009 (0.014)
Train: 543 [ 550/1251 ( 44%)]  Loss: 3.392 (3.26)  Time: 0.673s, 1522.36/s  (0.698s, 1467.28/s)  LR: 3.188e-05  Data: 0.010 (0.014)
Train: 543 [ 600/1251 ( 48%)]  Loss: 3.173 (3.25)  Time: 0.669s, 1529.90/s  (0.697s, 1468.10/s)  LR: 3.188e-05  Data: 0.010 (0.014)
Train: 543 [ 650/1251 ( 52%)]  Loss: 3.148 (3.24)  Time: 0.670s, 1528.29/s  (0.697s, 1469.99/s)  LR: 3.188e-05  Data: 0.009 (0.013)
Train: 543 [ 700/1251 ( 56%)]  Loss: 3.395 (3.25)  Time: 0.676s, 1515.77/s  (0.696s, 1471.19/s)  LR: 3.188e-05  Data: 0.010 (0.013)
Train: 543 [ 750/1251 ( 60%)]  Loss: 3.460 (3.27)  Time: 0.704s, 1454.10/s  (0.696s, 1471.20/s)  LR: 3.188e-05  Data: 0.013 (0.013)
Train: 543 [ 800/1251 ( 64%)]  Loss: 3.321 (3.27)  Time: 0.719s, 1424.81/s  (0.696s, 1471.86/s)  LR: 3.188e-05  Data: 0.010 (0.013)
Train: 543 [ 850/1251 ( 68%)]  Loss: 3.311 (3.27)  Time: 0.706s, 1450.34/s  (0.696s, 1472.12/s)  LR: 3.188e-05  Data: 0.009 (0.013)
Train: 543 [ 900/1251 ( 72%)]  Loss: 2.992 (3.26)  Time: 0.679s, 1508.03/s  (0.695s, 1472.73/s)  LR: 3.188e-05  Data: 0.011 (0.013)
Train: 543 [ 950/1251 ( 76%)]  Loss: 3.367 (3.26)  Time: 0.702s, 1458.27/s  (0.695s, 1472.70/s)  LR: 3.188e-05  Data: 0.009 (0.013)
Train: 543 [1000/1251 ( 80%)]  Loss: 2.893 (3.25)  Time: 0.714s, 1433.68/s  (0.695s, 1473.16/s)  LR: 3.188e-05  Data: 0.011 (0.012)
Train: 543 [1050/1251 ( 84%)]  Loss: 3.192 (3.24)  Time: 0.702s, 1459.63/s  (0.695s, 1472.40/s)  LR: 3.188e-05  Data: 0.009 (0.012)
Train: 543 [1100/1251 ( 88%)]  Loss: 3.051 (3.23)  Time: 0.671s, 1525.98/s  (0.695s, 1473.18/s)  LR: 3.188e-05  Data: 0.011 (0.012)
Train: 543 [1150/1251 ( 92%)]  Loss: 3.275 (3.24)  Time: 0.706s, 1451.45/s  (0.695s, 1473.69/s)  LR: 3.188e-05  Data: 0.009 (0.012)
Train: 543 [1200/1251 ( 96%)]  Loss: 3.109 (3.23)  Time: 0.669s, 1530.19/s  (0.694s, 1474.68/s)  LR: 3.188e-05  Data: 0.011 (0.012)
Train: 543 [1250/1251 (100%)]  Loss: 3.434 (3.24)  Time: 0.690s, 1484.38/s  (0.694s, 1474.73/s)  LR: 3.188e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.524 (1.524)  Loss:  0.6802 (0.6802)  Acc@1: 91.9922 (91.9922)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.136 (0.587)  Loss:  0.7925 (1.1640)  Acc@1: 86.9104 (78.9680)  Acc@5: 97.5236 (94.3420)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-543.pth.tar', 78.9680000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-542.pth.tar', 78.9239999243164)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-538.pth.tar', 78.85400015625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-541.pth.tar', 78.83000010742188)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-535.pth.tar', 78.8220000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-534.pth.tar', 78.802)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-537.pth.tar', 78.75600005615235)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-530.pth.tar', 78.74600005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-539.pth.tar', 78.73400013427734)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-532.pth.tar', 78.71400018310547)

Train: 544 [   0/1251 (  0%)]  Loss: 3.097 (3.10)  Time: 2.067s,  495.46/s  (2.067s,  495.46/s)  LR: 3.113e-05  Data: 1.454 (1.454)
Train: 544 [  50/1251 (  4%)]  Loss: 3.489 (3.29)  Time: 0.675s, 1517.93/s  (0.728s, 1407.41/s)  LR: 3.113e-05  Data: 0.010 (0.048)
Train: 544 [ 100/1251 (  8%)]  Loss: 2.753 (3.11)  Time: 0.673s, 1521.57/s  (0.714s, 1433.97/s)  LR: 3.113e-05  Data: 0.011 (0.029)
Train: 544 [ 150/1251 ( 12%)]  Loss: 3.370 (3.18)  Time: 0.674s, 1518.38/s  (0.706s, 1450.54/s)  LR: 3.113e-05  Data: 0.010 (0.023)
Train: 544 [ 200/1251 ( 16%)]  Loss: 2.887 (3.12)  Time: 0.716s, 1430.72/s  (0.704s, 1454.95/s)  LR: 3.113e-05  Data: 0.011 (0.020)
Train: 544 [ 250/1251 ( 20%)]  Loss: 3.154 (3.12)  Time: 0.686s, 1492.32/s  (0.702s, 1459.15/s)  LR: 3.113e-05  Data: 0.011 (0.018)
Train: 544 [ 300/1251 ( 24%)]  Loss: 3.088 (3.12)  Time: 0.692s, 1479.61/s  (0.700s, 1463.21/s)  LR: 3.113e-05  Data: 0.009 (0.017)
Train: 544 [ 350/1251 ( 28%)]  Loss: 3.010 (3.11)  Time: 0.679s, 1507.79/s  (0.699s, 1464.94/s)  LR: 3.113e-05  Data: 0.015 (0.016)
Train: 544 [ 400/1251 ( 32%)]  Loss: 2.958 (3.09)  Time: 0.672s, 1523.72/s  (0.698s, 1466.03/s)  LR: 3.113e-05  Data: 0.011 (0.015)
Train: 544 [ 450/1251 ( 36%)]  Loss: 3.193 (3.10)  Time: 0.703s, 1455.77/s  (0.697s, 1468.57/s)  LR: 3.113e-05  Data: 0.010 (0.015)
Train: 544 [ 500/1251 ( 40%)]  Loss: 3.255 (3.11)  Time: 0.695s, 1474.16/s  (0.697s, 1469.84/s)  LR: 3.113e-05  Data: 0.010 (0.014)
Train: 544 [ 550/1251 ( 44%)]  Loss: 2.541 (3.07)  Time: 0.673s, 1520.76/s  (0.697s, 1469.54/s)  LR: 3.113e-05  Data: 0.011 (0.014)
Train: 544 [ 600/1251 ( 48%)]  Loss: 3.126 (3.07)  Time: 0.671s, 1525.00/s  (0.697s, 1469.59/s)  LR: 3.113e-05  Data: 0.012 (0.014)
Train: 544 [ 650/1251 ( 52%)]  Loss: 3.479 (3.10)  Time: 0.676s, 1515.52/s  (0.696s, 1470.69/s)  LR: 3.113e-05  Data: 0.011 (0.013)
Train: 544 [ 700/1251 ( 56%)]  Loss: 3.176 (3.10)  Time: 0.677s, 1513.16/s  (0.695s, 1472.66/s)  LR: 3.113e-05  Data: 0.010 (0.013)
Train: 544 [ 750/1251 ( 60%)]  Loss: 3.409 (3.12)  Time: 0.672s, 1524.93/s  (0.695s, 1473.01/s)  LR: 3.113e-05  Data: 0.011 (0.013)
Train: 544 [ 800/1251 ( 64%)]  Loss: 3.162 (3.13)  Time: 0.708s, 1446.37/s  (0.695s, 1473.25/s)  LR: 3.113e-05  Data: 0.010 (0.013)
Train: 544 [ 850/1251 ( 68%)]  Loss: 3.049 (3.12)  Time: 0.708s, 1446.65/s  (0.694s, 1474.48/s)  LR: 3.113e-05  Data: 0.009 (0.013)
Train: 544 [ 900/1251 ( 72%)]  Loss: 3.281 (3.13)  Time: 0.693s, 1477.28/s  (0.694s, 1474.69/s)  LR: 3.113e-05  Data: 0.014 (0.013)
Train: 544 [ 950/1251 ( 76%)]  Loss: 3.349 (3.14)  Time: 0.678s, 1511.15/s  (0.694s, 1475.04/s)  LR: 3.113e-05  Data: 0.011 (0.012)
Train: 544 [1000/1251 ( 80%)]  Loss: 3.179 (3.14)  Time: 0.672s, 1523.25/s  (0.694s, 1475.94/s)  LR: 3.113e-05  Data: 0.011 (0.012)
Train: 544 [1050/1251 ( 84%)]  Loss: 3.407 (3.16)  Time: 0.688s, 1487.52/s  (0.694s, 1476.14/s)  LR: 3.113e-05  Data: 0.011 (0.012)
Train: 544 [1100/1251 ( 88%)]  Loss: 3.614 (3.18)  Time: 0.671s, 1524.97/s  (0.693s, 1476.99/s)  LR: 3.113e-05  Data: 0.010 (0.012)
Train: 544 [1150/1251 ( 92%)]  Loss: 3.419 (3.19)  Time: 0.706s, 1450.93/s  (0.693s, 1476.85/s)  LR: 3.113e-05  Data: 0.011 (0.012)
Train: 544 [1200/1251 ( 96%)]  Loss: 3.059 (3.18)  Time: 0.672s, 1522.70/s  (0.693s, 1477.59/s)  LR: 3.113e-05  Data: 0.011 (0.012)
Train: 544 [1250/1251 (100%)]  Loss: 3.272 (3.18)  Time: 0.654s, 1566.85/s  (0.693s, 1477.55/s)  LR: 3.113e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.600 (1.600)  Loss:  0.6235 (0.6235)  Acc@1: 92.3828 (92.3828)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.136 (0.586)  Loss:  0.7358 (1.1379)  Acc@1: 87.9717 (78.9000)  Acc@5: 97.1698 (94.3800)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-543.pth.tar', 78.9680000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-542.pth.tar', 78.9239999243164)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-544.pth.tar', 78.9000000756836)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-538.pth.tar', 78.85400015625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-541.pth.tar', 78.83000010742188)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-535.pth.tar', 78.8220000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-534.pth.tar', 78.802)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-537.pth.tar', 78.75600005615235)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-530.pth.tar', 78.74600005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-539.pth.tar', 78.73400013427734)

Train: 545 [   0/1251 (  0%)]  Loss: 2.958 (2.96)  Time: 2.211s,  463.24/s  (2.211s,  463.24/s)  LR: 3.038e-05  Data: 1.595 (1.595)
Train: 545 [  50/1251 (  4%)]  Loss: 3.373 (3.17)  Time: 0.674s, 1518.34/s  (0.724s, 1414.87/s)  LR: 3.038e-05  Data: 0.010 (0.046)
Train: 545 [ 100/1251 (  8%)]  Loss: 3.051 (3.13)  Time: 0.670s, 1527.74/s  (0.704s, 1453.83/s)  LR: 3.038e-05  Data: 0.009 (0.028)
Train: 545 [ 150/1251 ( 12%)]  Loss: 3.139 (3.13)  Time: 0.716s, 1429.37/s  (0.698s, 1467.07/s)  LR: 3.038e-05  Data: 0.010 (0.022)
Train: 545 [ 200/1251 ( 16%)]  Loss: 3.533 (3.21)  Time: 0.701s, 1459.83/s  (0.698s, 1466.20/s)  LR: 3.038e-05  Data: 0.009 (0.019)
Train: 545 [ 250/1251 ( 20%)]  Loss: 3.126 (3.20)  Time: 0.750s, 1365.70/s  (0.698s, 1467.47/s)  LR: 3.038e-05  Data: 0.009 (0.017)
Train: 545 [ 300/1251 ( 24%)]  Loss: 3.343 (3.22)  Time: 0.671s, 1525.98/s  (0.696s, 1471.28/s)  LR: 3.038e-05  Data: 0.010 (0.016)
Train: 545 [ 350/1251 ( 28%)]  Loss: 3.263 (3.22)  Time: 0.708s, 1446.85/s  (0.697s, 1469.59/s)  LR: 3.038e-05  Data: 0.009 (0.015)
Train: 545 [ 400/1251 ( 32%)]  Loss: 2.990 (3.20)  Time: 0.675s, 1517.53/s  (0.696s, 1471.40/s)  LR: 3.038e-05  Data: 0.009 (0.015)
Train: 545 [ 450/1251 ( 36%)]  Loss: 2.553 (3.13)  Time: 0.677s, 1512.02/s  (0.695s, 1473.86/s)  LR: 3.038e-05  Data: 0.010 (0.014)
Train: 545 [ 500/1251 ( 40%)]  Loss: 3.330 (3.15)  Time: 0.693s, 1477.11/s  (0.694s, 1474.65/s)  LR: 3.038e-05  Data: 0.009 (0.014)
Train: 545 [ 550/1251 ( 44%)]  Loss: 3.178 (3.15)  Time: 0.671s, 1524.95/s  (0.694s, 1476.00/s)  LR: 3.038e-05  Data: 0.010 (0.014)
Train: 545 [ 600/1251 ( 48%)]  Loss: 2.667 (3.12)  Time: 0.778s, 1316.18/s  (0.694s, 1476.15/s)  LR: 3.038e-05  Data: 0.011 (0.013)
Train: 545 [ 650/1251 ( 52%)]  Loss: 3.101 (3.11)  Time: 0.676s, 1513.80/s  (0.694s, 1476.41/s)  LR: 3.038e-05  Data: 0.010 (0.013)
Train: 545 [ 700/1251 ( 56%)]  Loss: 3.184 (3.12)  Time: 0.671s, 1526.77/s  (0.693s, 1477.29/s)  LR: 3.038e-05  Data: 0.009 (0.013)
Train: 545 [ 750/1251 ( 60%)]  Loss: 3.237 (3.13)  Time: 0.673s, 1522.19/s  (0.693s, 1476.79/s)  LR: 3.038e-05  Data: 0.010 (0.013)
Train: 545 [ 800/1251 ( 64%)]  Loss: 3.559 (3.15)  Time: 0.700s, 1463.35/s  (0.693s, 1477.06/s)  LR: 3.038e-05  Data: 0.010 (0.013)
Train: 545 [ 850/1251 ( 68%)]  Loss: 3.184 (3.15)  Time: 0.677s, 1512.57/s  (0.693s, 1477.68/s)  LR: 3.038e-05  Data: 0.010 (0.013)
Train: 545 [ 900/1251 ( 72%)]  Loss: 3.256 (3.16)  Time: 0.776s, 1319.37/s  (0.693s, 1477.71/s)  LR: 3.038e-05  Data: 0.009 (0.012)
Train: 545 [ 950/1251 ( 76%)]  Loss: 3.327 (3.17)  Time: 0.672s, 1523.05/s  (0.693s, 1477.41/s)  LR: 3.038e-05  Data: 0.009 (0.012)
Train: 545 [1000/1251 ( 80%)]  Loss: 3.290 (3.17)  Time: 0.693s, 1477.03/s  (0.693s, 1478.32/s)  LR: 3.038e-05  Data: 0.010 (0.012)
Train: 545 [1050/1251 ( 84%)]  Loss: 2.920 (3.16)  Time: 0.676s, 1514.01/s  (0.693s, 1478.37/s)  LR: 3.038e-05  Data: 0.010 (0.012)
Train: 545 [1100/1251 ( 88%)]  Loss: 2.813 (3.15)  Time: 0.703s, 1457.65/s  (0.692s, 1479.38/s)  LR: 3.038e-05  Data: 0.008 (0.012)
Train: 545 [1150/1251 ( 92%)]  Loss: 3.213 (3.15)  Time: 0.700s, 1462.81/s  (0.692s, 1479.88/s)  LR: 3.038e-05  Data: 0.009 (0.012)
Train: 545 [1200/1251 ( 96%)]  Loss: 3.377 (3.16)  Time: 0.671s, 1526.47/s  (0.692s, 1480.39/s)  LR: 3.038e-05  Data: 0.012 (0.012)
Train: 545 [1250/1251 (100%)]  Loss: 3.298 (3.16)  Time: 0.660s, 1552.31/s  (0.692s, 1480.15/s)  LR: 3.038e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.560 (1.560)  Loss:  0.6816 (0.6816)  Acc@1: 91.6016 (91.6016)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.137 (0.586)  Loss:  0.7920 (1.1538)  Acc@1: 86.5566 (78.7600)  Acc@5: 97.2877 (94.4040)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-543.pth.tar', 78.9680000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-542.pth.tar', 78.9239999243164)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-544.pth.tar', 78.9000000756836)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-538.pth.tar', 78.85400015625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-541.pth.tar', 78.83000010742188)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-535.pth.tar', 78.8220000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-534.pth.tar', 78.802)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-545.pth.tar', 78.75999997802734)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-537.pth.tar', 78.75600005615235)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-530.pth.tar', 78.74600005615234)

Train: 546 [   0/1251 (  0%)]  Loss: 3.094 (3.09)  Time: 2.161s,  473.89/s  (2.161s,  473.89/s)  LR: 2.965e-05  Data: 1.497 (1.497)
Train: 546 [  50/1251 (  4%)]  Loss: 3.069 (3.08)  Time: 0.680s, 1506.74/s  (0.736s, 1392.12/s)  LR: 2.965e-05  Data: 0.012 (0.045)
Train: 546 [ 100/1251 (  8%)]  Loss: 3.073 (3.08)  Time: 0.672s, 1524.68/s  (0.713s, 1435.49/s)  LR: 2.965e-05  Data: 0.010 (0.028)
Train: 546 [ 150/1251 ( 12%)]  Loss: 3.080 (3.08)  Time: 0.673s, 1520.71/s  (0.705s, 1453.41/s)  LR: 2.965e-05  Data: 0.010 (0.022)
Train: 546 [ 200/1251 ( 16%)]  Loss: 3.077 (3.08)  Time: 0.714s, 1434.35/s  (0.703s, 1457.54/s)  LR: 2.965e-05  Data: 0.011 (0.019)
Train: 546 [ 250/1251 ( 20%)]  Loss: 3.201 (3.10)  Time: 0.672s, 1524.52/s  (0.701s, 1461.21/s)  LR: 2.965e-05  Data: 0.011 (0.017)
Train: 546 [ 300/1251 ( 24%)]  Loss: 3.115 (3.10)  Time: 0.674s, 1519.81/s  (0.698s, 1467.77/s)  LR: 2.965e-05  Data: 0.014 (0.016)
Train: 546 [ 350/1251 ( 28%)]  Loss: 3.048 (3.09)  Time: 0.703s, 1456.07/s  (0.698s, 1468.05/s)  LR: 2.965e-05  Data: 0.010 (0.015)
Train: 546 [ 400/1251 ( 32%)]  Loss: 3.324 (3.12)  Time: 0.672s, 1523.75/s  (0.697s, 1469.16/s)  LR: 2.965e-05  Data: 0.011 (0.015)
Train: 546 [ 450/1251 ( 36%)]  Loss: 3.037 (3.11)  Time: 0.672s, 1523.01/s  (0.696s, 1470.23/s)  LR: 2.965e-05  Data: 0.010 (0.014)
Train: 546 [ 500/1251 ( 40%)]  Loss: 3.241 (3.12)  Time: 0.717s, 1428.51/s  (0.697s, 1470.02/s)  LR: 2.965e-05  Data: 0.011 (0.014)
Train: 546 [ 550/1251 ( 44%)]  Loss: 3.300 (3.14)  Time: 0.671s, 1524.96/s  (0.696s, 1470.98/s)  LR: 2.965e-05  Data: 0.011 (0.014)
Train: 546 [ 600/1251 ( 48%)]  Loss: 3.430 (3.16)  Time: 0.675s, 1515.99/s  (0.696s, 1472.21/s)  LR: 2.965e-05  Data: 0.012 (0.013)
Train: 546 [ 650/1251 ( 52%)]  Loss: 3.235 (3.17)  Time: 0.672s, 1523.02/s  (0.695s, 1473.21/s)  LR: 2.965e-05  Data: 0.010 (0.013)
Train: 546 [ 700/1251 ( 56%)]  Loss: 3.226 (3.17)  Time: 0.672s, 1524.78/s  (0.695s, 1473.70/s)  LR: 2.965e-05  Data: 0.011 (0.013)
Train: 546 [ 750/1251 ( 60%)]  Loss: 2.976 (3.16)  Time: 0.672s, 1524.35/s  (0.694s, 1474.48/s)  LR: 2.965e-05  Data: 0.011 (0.013)
Train: 546 [ 800/1251 ( 64%)]  Loss: 3.161 (3.16)  Time: 0.709s, 1444.69/s  (0.696s, 1471.77/s)  LR: 2.965e-05  Data: 0.011 (0.013)
Train: 546 [ 850/1251 ( 68%)]  Loss: 3.407 (3.17)  Time: 0.722s, 1417.98/s  (0.697s, 1469.53/s)  LR: 2.965e-05  Data: 0.012 (0.013)
Train: 546 [ 900/1251 ( 72%)]  Loss: 2.863 (3.16)  Time: 0.687s, 1490.37/s  (0.697s, 1468.12/s)  LR: 2.965e-05  Data: 0.009 (0.013)
Train: 546 [ 950/1251 ( 76%)]  Loss: 3.336 (3.16)  Time: 0.674s, 1518.62/s  (0.697s, 1468.14/s)  LR: 2.965e-05  Data: 0.011 (0.013)
Train: 546 [1000/1251 ( 80%)]  Loss: 3.491 (3.18)  Time: 0.674s, 1519.51/s  (0.697s, 1469.87/s)  LR: 2.965e-05  Data: 0.010 (0.012)
Train: 546 [1050/1251 ( 84%)]  Loss: 3.258 (3.18)  Time: 0.708s, 1446.48/s  (0.696s, 1471.36/s)  LR: 2.965e-05  Data: 0.009 (0.012)
Train: 546 [1100/1251 ( 88%)]  Loss: 3.406 (3.19)  Time: 0.717s, 1427.68/s  (0.696s, 1471.87/s)  LR: 2.965e-05  Data: 0.011 (0.012)
Train: 546 [1150/1251 ( 92%)]  Loss: 3.277 (3.20)  Time: 0.711s, 1440.25/s  (0.696s, 1471.84/s)  LR: 2.965e-05  Data: 0.010 (0.012)
Train: 546 [1200/1251 ( 96%)]  Loss: 3.456 (3.21)  Time: 0.672s, 1523.89/s  (0.695s, 1472.70/s)  LR: 2.965e-05  Data: 0.011 (0.012)
Train: 546 [1250/1251 (100%)]  Loss: 3.053 (3.20)  Time: 0.654s, 1566.75/s  (0.695s, 1473.42/s)  LR: 2.965e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.605 (1.605)  Loss:  0.6582 (0.6582)  Acc@1: 91.6992 (91.6992)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.136 (0.584)  Loss:  0.7759 (1.1635)  Acc@1: 87.1462 (78.8840)  Acc@5: 97.1698 (94.3680)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-543.pth.tar', 78.9680000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-542.pth.tar', 78.9239999243164)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-544.pth.tar', 78.9000000756836)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-546.pth.tar', 78.88399997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-538.pth.tar', 78.85400015625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-541.pth.tar', 78.83000010742188)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-535.pth.tar', 78.8220000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-534.pth.tar', 78.802)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-545.pth.tar', 78.75999997802734)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-537.pth.tar', 78.75600005615235)

Train: 547 [   0/1251 (  0%)]  Loss: 3.092 (3.09)  Time: 2.130s,  480.79/s  (2.130s,  480.79/s)  LR: 2.894e-05  Data: 1.513 (1.513)
Train: 547 [  50/1251 (  4%)]  Loss: 3.397 (3.24)  Time: 0.701s, 1461.31/s  (0.728s, 1406.38/s)  LR: 2.894e-05  Data: 0.010 (0.049)
Train: 547 [ 100/1251 (  8%)]  Loss: 3.163 (3.22)  Time: 0.694s, 1476.55/s  (0.708s, 1447.34/s)  LR: 2.894e-05  Data: 0.010 (0.030)
Train: 547 [ 150/1251 ( 12%)]  Loss: 3.391 (3.26)  Time: 0.671s, 1526.44/s  (0.700s, 1462.03/s)  LR: 2.894e-05  Data: 0.009 (0.023)
Train: 547 [ 200/1251 ( 16%)]  Loss: 3.435 (3.30)  Time: 0.734s, 1395.62/s  (0.699s, 1465.06/s)  LR: 2.894e-05  Data: 0.009 (0.020)
Train: 547 [ 250/1251 ( 20%)]  Loss: 3.140 (3.27)  Time: 0.709s, 1444.47/s  (0.698s, 1467.20/s)  LR: 2.894e-05  Data: 0.010 (0.018)
Train: 547 [ 300/1251 ( 24%)]  Loss: 3.279 (3.27)  Time: 0.674s, 1520.41/s  (0.697s, 1469.91/s)  LR: 2.894e-05  Data: 0.010 (0.017)
Train: 547 [ 350/1251 ( 28%)]  Loss: 3.413 (3.29)  Time: 0.671s, 1526.69/s  (0.695s, 1473.45/s)  LR: 2.894e-05  Data: 0.010 (0.016)
Train: 547 [ 400/1251 ( 32%)]  Loss: 3.359 (3.30)  Time: 0.700s, 1462.72/s  (0.695s, 1472.86/s)  LR: 2.894e-05  Data: 0.009 (0.015)
Train: 547 [ 450/1251 ( 36%)]  Loss: 3.188 (3.29)  Time: 0.676s, 1513.83/s  (0.695s, 1474.15/s)  LR: 2.894e-05  Data: 0.009 (0.015)
Train: 547 [ 500/1251 ( 40%)]  Loss: 3.115 (3.27)  Time: 0.682s, 1502.21/s  (0.694s, 1476.16/s)  LR: 2.894e-05  Data: 0.011 (0.014)
Train: 547 [ 550/1251 ( 44%)]  Loss: 3.095 (3.26)  Time: 0.699s, 1464.14/s  (0.694s, 1476.04/s)  LR: 2.894e-05  Data: 0.012 (0.014)
Train: 547 [ 600/1251 ( 48%)]  Loss: 3.160 (3.25)  Time: 0.669s, 1529.82/s  (0.694s, 1476.46/s)  LR: 2.894e-05  Data: 0.010 (0.014)
Train: 547 [ 650/1251 ( 52%)]  Loss: 3.096 (3.24)  Time: 0.672s, 1523.90/s  (0.694s, 1476.50/s)  LR: 2.894e-05  Data: 0.010 (0.013)
Train: 547 [ 700/1251 ( 56%)]  Loss: 2.822 (3.21)  Time: 0.708s, 1446.82/s  (0.694s, 1475.76/s)  LR: 2.894e-05  Data: 0.010 (0.013)
Train: 547 [ 750/1251 ( 60%)]  Loss: 3.051 (3.20)  Time: 0.698s, 1467.07/s  (0.694s, 1475.41/s)  LR: 2.894e-05  Data: 0.009 (0.013)
Train: 547 [ 800/1251 ( 64%)]  Loss: 3.200 (3.20)  Time: 0.704s, 1454.28/s  (0.694s, 1476.29/s)  LR: 2.894e-05  Data: 0.010 (0.013)
Train: 547 [ 850/1251 ( 68%)]  Loss: 3.231 (3.20)  Time: 0.671s, 1526.89/s  (0.693s, 1476.74/s)  LR: 2.894e-05  Data: 0.010 (0.013)
Train: 547 [ 900/1251 ( 72%)]  Loss: 3.390 (3.21)  Time: 0.699s, 1464.43/s  (0.694s, 1476.28/s)  LR: 2.894e-05  Data: 0.009 (0.013)
Train: 547 [ 950/1251 ( 76%)]  Loss: 3.260 (3.21)  Time: 0.724s, 1414.32/s  (0.693s, 1476.69/s)  LR: 2.894e-05  Data: 0.010 (0.013)
Train: 547 [1000/1251 ( 80%)]  Loss: 3.442 (3.22)  Time: 0.671s, 1526.13/s  (0.693s, 1477.30/s)  LR: 2.894e-05  Data: 0.010 (0.012)
Train: 547 [1050/1251 ( 84%)]  Loss: 3.202 (3.22)  Time: 0.707s, 1449.26/s  (0.693s, 1477.44/s)  LR: 2.894e-05  Data: 0.009 (0.012)
Train: 547 [1100/1251 ( 88%)]  Loss: 3.152 (3.22)  Time: 0.673s, 1521.77/s  (0.693s, 1477.89/s)  LR: 2.894e-05  Data: 0.010 (0.012)
Train: 547 [1150/1251 ( 92%)]  Loss: 3.372 (3.23)  Time: 0.671s, 1526.69/s  (0.693s, 1478.46/s)  LR: 2.894e-05  Data: 0.009 (0.012)
Train: 547 [1200/1251 ( 96%)]  Loss: 3.305 (3.23)  Time: 0.672s, 1523.19/s  (0.693s, 1478.06/s)  LR: 2.894e-05  Data: 0.013 (0.012)
Train: 547 [1250/1251 (100%)]  Loss: 3.048 (3.22)  Time: 0.656s, 1561.57/s  (0.693s, 1478.65/s)  LR: 2.894e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.532 (1.532)  Loss:  0.6855 (0.6855)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.138 (0.582)  Loss:  0.7686 (1.1642)  Acc@1: 87.0283 (79.0080)  Acc@5: 97.1698 (94.4040)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-547.pth.tar', 79.00799992431641)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-543.pth.tar', 78.9680000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-542.pth.tar', 78.9239999243164)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-544.pth.tar', 78.9000000756836)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-546.pth.tar', 78.88399997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-538.pth.tar', 78.85400015625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-541.pth.tar', 78.83000010742188)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-535.pth.tar', 78.8220000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-534.pth.tar', 78.802)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-545.pth.tar', 78.75999997802734)

Train: 548 [   0/1251 (  0%)]  Loss: 3.567 (3.57)  Time: 2.165s,  472.97/s  (2.165s,  472.97/s)  LR: 2.823e-05  Data: 1.549 (1.549)
Train: 548 [  50/1251 (  4%)]  Loss: 3.093 (3.33)  Time: 0.668s, 1531.87/s  (0.725s, 1412.26/s)  LR: 2.823e-05  Data: 0.011 (0.049)
Train: 548 [ 100/1251 (  8%)]  Loss: 3.009 (3.22)  Time: 0.670s, 1528.43/s  (0.711s, 1439.89/s)  LR: 2.823e-05  Data: 0.010 (0.030)
Train: 548 [ 150/1251 ( 12%)]  Loss: 3.459 (3.28)  Time: 0.710s, 1441.67/s  (0.706s, 1450.74/s)  LR: 2.823e-05  Data: 0.010 (0.024)
Train: 548 [ 200/1251 ( 16%)]  Loss: 3.175 (3.26)  Time: 0.701s, 1459.76/s  (0.702s, 1459.06/s)  LR: 2.823e-05  Data: 0.011 (0.020)
Train: 548 [ 250/1251 ( 20%)]  Loss: 3.281 (3.26)  Time: 0.671s, 1525.31/s  (0.699s, 1464.21/s)  LR: 2.823e-05  Data: 0.011 (0.018)
Train: 548 [ 300/1251 ( 24%)]  Loss: 3.459 (3.29)  Time: 0.736s, 1391.85/s  (0.697s, 1468.36/s)  LR: 2.823e-05  Data: 0.011 (0.017)
Train: 548 [ 350/1251 ( 28%)]  Loss: 2.970 (3.25)  Time: 0.672s, 1523.19/s  (0.696s, 1470.39/s)  LR: 2.823e-05  Data: 0.010 (0.016)
Train: 548 [ 400/1251 ( 32%)]  Loss: 3.004 (3.22)  Time: 0.674s, 1518.57/s  (0.696s, 1471.01/s)  LR: 2.823e-05  Data: 0.010 (0.015)
Train: 548 [ 450/1251 ( 36%)]  Loss: 3.327 (3.23)  Time: 0.665s, 1539.09/s  (0.697s, 1470.15/s)  LR: 2.823e-05  Data: 0.010 (0.015)
Train: 548 [ 500/1251 ( 40%)]  Loss: 3.091 (3.22)  Time: 0.712s, 1439.19/s  (0.697s, 1469.41/s)  LR: 2.823e-05  Data: 0.009 (0.014)
Train: 548 [ 550/1251 ( 44%)]  Loss: 3.241 (3.22)  Time: 0.731s, 1401.28/s  (0.696s, 1470.25/s)  LR: 2.823e-05  Data: 0.010 (0.014)
Train: 548 [ 600/1251 ( 48%)]  Loss: 2.687 (3.18)  Time: 0.669s, 1530.60/s  (0.696s, 1471.31/s)  LR: 2.823e-05  Data: 0.009 (0.014)
Train: 548 [ 650/1251 ( 52%)]  Loss: 3.289 (3.19)  Time: 0.683s, 1498.68/s  (0.695s, 1472.63/s)  LR: 2.823e-05  Data: 0.013 (0.013)
Train: 548 [ 700/1251 ( 56%)]  Loss: 3.325 (3.20)  Time: 0.678s, 1510.92/s  (0.695s, 1474.04/s)  LR: 2.823e-05  Data: 0.011 (0.013)
Train: 548 [ 750/1251 ( 60%)]  Loss: 3.081 (3.19)  Time: 0.712s, 1437.45/s  (0.695s, 1473.72/s)  LR: 2.823e-05  Data: 0.010 (0.013)
Train: 548 [ 800/1251 ( 64%)]  Loss: 3.406 (3.20)  Time: 0.717s, 1427.24/s  (0.695s, 1472.40/s)  LR: 2.823e-05  Data: 0.011 (0.013)
Train: 548 [ 850/1251 ( 68%)]  Loss: 3.062 (3.20)  Time: 0.725s, 1413.21/s  (0.695s, 1472.96/s)  LR: 2.823e-05  Data: 0.009 (0.013)
Train: 548 [ 900/1251 ( 72%)]  Loss: 3.179 (3.20)  Time: 0.678s, 1510.52/s  (0.695s, 1473.68/s)  LR: 2.823e-05  Data: 0.011 (0.013)
Train: 548 [ 950/1251 ( 76%)]  Loss: 3.466 (3.21)  Time: 0.692s, 1479.98/s  (0.695s, 1473.96/s)  LR: 2.823e-05  Data: 0.011 (0.012)
Train: 548 [1000/1251 ( 80%)]  Loss: 3.362 (3.22)  Time: 0.687s, 1490.40/s  (0.695s, 1473.86/s)  LR: 2.823e-05  Data: 0.010 (0.012)
Train: 548 [1050/1251 ( 84%)]  Loss: 3.355 (3.22)  Time: 0.718s, 1425.61/s  (0.694s, 1474.50/s)  LR: 2.823e-05  Data: 0.009 (0.012)
Train: 548 [1100/1251 ( 88%)]  Loss: 3.443 (3.23)  Time: 0.675s, 1517.04/s  (0.694s, 1475.24/s)  LR: 2.823e-05  Data: 0.010 (0.012)
Train: 548 [1150/1251 ( 92%)]  Loss: 2.961 (3.22)  Time: 0.673s, 1520.81/s  (0.694s, 1474.68/s)  LR: 2.823e-05  Data: 0.012 (0.012)
Train: 548 [1200/1251 ( 96%)]  Loss: 2.977 (3.21)  Time: 0.700s, 1461.92/s  (0.694s, 1474.46/s)  LR: 2.823e-05  Data: 0.009 (0.012)
Train: 548 [1250/1251 (100%)]  Loss: 3.124 (3.21)  Time: 0.681s, 1504.02/s  (0.695s, 1474.14/s)  LR: 2.823e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.660 (1.660)  Loss:  0.7451 (0.7451)  Acc@1: 91.7969 (91.7969)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.136 (0.576)  Loss:  0.8394 (1.2185)  Acc@1: 86.0849 (78.9240)  Acc@5: 96.9340 (94.4060)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-547.pth.tar', 79.00799992431641)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-543.pth.tar', 78.9680000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-548.pth.tar', 78.92400003173829)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-542.pth.tar', 78.9239999243164)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-544.pth.tar', 78.9000000756836)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-546.pth.tar', 78.88399997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-538.pth.tar', 78.85400015625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-541.pth.tar', 78.83000010742188)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-535.pth.tar', 78.8220000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-534.pth.tar', 78.802)

Train: 549 [   0/1251 (  0%)]  Loss: 3.341 (3.34)  Time: 2.463s,  415.78/s  (2.463s,  415.78/s)  LR: 2.754e-05  Data: 1.841 (1.841)
Train: 549 [  50/1251 (  4%)]  Loss: 3.576 (3.46)  Time: 0.671s, 1525.97/s  (0.724s, 1414.03/s)  LR: 2.754e-05  Data: 0.011 (0.048)
Train: 549 [ 100/1251 (  8%)]  Loss: 2.969 (3.30)  Time: 0.672s, 1524.77/s  (0.706s, 1451.01/s)  LR: 2.754e-05  Data: 0.010 (0.029)
Train: 549 [ 150/1251 ( 12%)]  Loss: 3.303 (3.30)  Time: 0.693s, 1477.78/s  (0.702s, 1458.72/s)  LR: 2.754e-05  Data: 0.010 (0.023)
Train: 549 [ 200/1251 ( 16%)]  Loss: 3.245 (3.29)  Time: 0.674s, 1519.06/s  (0.701s, 1461.19/s)  LR: 2.754e-05  Data: 0.014 (0.020)
Train: 549 [ 250/1251 ( 20%)]  Loss: 3.389 (3.30)  Time: 0.710s, 1442.91/s  (0.700s, 1462.40/s)  LR: 2.754e-05  Data: 0.010 (0.018)
Train: 549 [ 300/1251 ( 24%)]  Loss: 3.276 (3.30)  Time: 0.683s, 1498.47/s  (0.699s, 1465.28/s)  LR: 2.754e-05  Data: 0.009 (0.017)
Train: 549 [ 350/1251 ( 28%)]  Loss: 2.999 (3.26)  Time: 0.706s, 1451.39/s  (0.697s, 1468.61/s)  LR: 2.754e-05  Data: 0.009 (0.016)
Train: 549 [ 400/1251 ( 32%)]  Loss: 3.137 (3.25)  Time: 0.737s, 1389.45/s  (0.696s, 1471.14/s)  LR: 2.754e-05  Data: 0.012 (0.015)
Train: 549 [ 450/1251 ( 36%)]  Loss: 3.554 (3.28)  Time: 0.677s, 1512.31/s  (0.695s, 1472.46/s)  LR: 2.754e-05  Data: 0.010 (0.015)
Train: 549 [ 500/1251 ( 40%)]  Loss: 3.024 (3.26)  Time: 0.716s, 1429.59/s  (0.694s, 1474.54/s)  LR: 2.754e-05  Data: 0.008 (0.014)
Train: 549 [ 550/1251 ( 44%)]  Loss: 3.078 (3.24)  Time: 0.671s, 1526.20/s  (0.694s, 1474.50/s)  LR: 2.754e-05  Data: 0.010 (0.014)
Train: 549 [ 600/1251 ( 48%)]  Loss: 3.166 (3.24)  Time: 0.770s, 1329.44/s  (0.694s, 1475.34/s)  LR: 2.754e-05  Data: 0.010 (0.014)
Train: 549 [ 650/1251 ( 52%)]  Loss: 2.885 (3.21)  Time: 0.673s, 1522.26/s  (0.694s, 1474.89/s)  LR: 2.754e-05  Data: 0.010 (0.013)
Train: 549 [ 700/1251 ( 56%)]  Loss: 3.600 (3.24)  Time: 0.670s, 1527.37/s  (0.695s, 1474.15/s)  LR: 2.754e-05  Data: 0.010 (0.013)
Train: 549 [ 750/1251 ( 60%)]  Loss: 3.155 (3.23)  Time: 0.670s, 1528.18/s  (0.694s, 1474.78/s)  LR: 2.754e-05  Data: 0.013 (0.013)
Train: 549 [ 800/1251 ( 64%)]  Loss: 3.604 (3.25)  Time: 0.706s, 1450.54/s  (0.694s, 1475.52/s)  LR: 2.754e-05  Data: 0.010 (0.013)
Train: 549 [ 850/1251 ( 68%)]  Loss: 3.182 (3.25)  Time: 0.670s, 1527.46/s  (0.694s, 1476.03/s)  LR: 2.754e-05  Data: 0.010 (0.013)
Train: 549 [ 900/1251 ( 72%)]  Loss: 3.404 (3.26)  Time: 0.717s, 1427.80/s  (0.694s, 1476.20/s)  LR: 2.754e-05  Data: 0.010 (0.013)
Train: 549 [ 950/1251 ( 76%)]  Loss: 3.177 (3.25)  Time: 0.726s, 1411.09/s  (0.694s, 1475.80/s)  LR: 2.754e-05  Data: 0.009 (0.012)
Train: 549 [1000/1251 ( 80%)]  Loss: 3.305 (3.26)  Time: 0.734s, 1394.17/s  (0.694s, 1475.81/s)  LR: 2.754e-05  Data: 0.009 (0.012)
Train: 549 [1050/1251 ( 84%)]  Loss: 3.255 (3.26)  Time: 0.671s, 1526.45/s  (0.694s, 1475.96/s)  LR: 2.754e-05  Data: 0.009 (0.012)
Train: 549 [1100/1251 ( 88%)]  Loss: 3.035 (3.25)  Time: 0.703s, 1455.63/s  (0.694s, 1476.51/s)  LR: 2.754e-05  Data: 0.009 (0.012)
Train: 549 [1150/1251 ( 92%)]  Loss: 3.283 (3.25)  Time: 0.702s, 1459.27/s  (0.693s, 1476.68/s)  LR: 2.754e-05  Data: 0.010 (0.012)
Train: 549 [1200/1251 ( 96%)]  Loss: 3.060 (3.24)  Time: 0.674s, 1518.19/s  (0.693s, 1476.94/s)  LR: 2.754e-05  Data: 0.009 (0.012)
Train: 549 [1250/1251 (100%)]  Loss: 3.172 (3.24)  Time: 0.661s, 1548.34/s  (0.693s, 1477.02/s)  LR: 2.754e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.488 (1.488)  Loss:  0.6846 (0.6846)  Acc@1: 91.4062 (91.4062)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.136 (0.572)  Loss:  0.7725 (1.1563)  Acc@1: 86.9104 (78.8900)  Acc@5: 96.9340 (94.3860)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-547.pth.tar', 79.00799992431641)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-543.pth.tar', 78.9680000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-548.pth.tar', 78.92400003173829)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-542.pth.tar', 78.9239999243164)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-544.pth.tar', 78.9000000756836)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-549.pth.tar', 78.89000000244141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-546.pth.tar', 78.88399997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-538.pth.tar', 78.85400015625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-541.pth.tar', 78.83000010742188)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-535.pth.tar', 78.8220000024414)

Train: 550 [   0/1251 (  0%)]  Loss: 3.369 (3.37)  Time: 2.401s,  426.54/s  (2.401s,  426.54/s)  LR: 2.687e-05  Data: 1.774 (1.774)
Train: 550 [  50/1251 (  4%)]  Loss: 3.447 (3.41)  Time: 0.675s, 1516.71/s  (0.733s, 1396.57/s)  LR: 2.687e-05  Data: 0.011 (0.049)
Train: 550 [ 100/1251 (  8%)]  Loss: 3.135 (3.32)  Time: 0.673s, 1522.51/s  (0.716s, 1429.75/s)  LR: 2.687e-05  Data: 0.011 (0.030)
Train: 550 [ 150/1251 ( 12%)]  Loss: 2.990 (3.24)  Time: 0.671s, 1525.47/s  (0.707s, 1449.38/s)  LR: 2.687e-05  Data: 0.010 (0.024)
Train: 550 [ 200/1251 ( 16%)]  Loss: 2.873 (3.16)  Time: 0.672s, 1524.05/s  (0.702s, 1458.53/s)  LR: 2.687e-05  Data: 0.011 (0.020)
Train: 550 [ 250/1251 ( 20%)]  Loss: 3.490 (3.22)  Time: 0.670s, 1527.28/s  (0.699s, 1465.00/s)  LR: 2.687e-05  Data: 0.010 (0.018)
Train: 550 [ 300/1251 ( 24%)]  Loss: 3.257 (3.22)  Time: 0.720s, 1421.41/s  (0.699s, 1465.32/s)  LR: 2.687e-05  Data: 0.013 (0.017)
Train: 550 [ 350/1251 ( 28%)]  Loss: 3.064 (3.20)  Time: 0.720s, 1422.34/s  (0.698s, 1466.80/s)  LR: 2.687e-05  Data: 0.009 (0.016)
Train: 550 [ 400/1251 ( 32%)]  Loss: 3.362 (3.22)  Time: 0.690s, 1483.31/s  (0.698s, 1466.82/s)  LR: 2.687e-05  Data: 0.017 (0.015)
Train: 550 [ 450/1251 ( 36%)]  Loss: 3.376 (3.24)  Time: 0.672s, 1524.77/s  (0.699s, 1466.00/s)  LR: 2.687e-05  Data: 0.013 (0.015)
Train: 550 [ 500/1251 ( 40%)]  Loss: 3.619 (3.27)  Time: 0.711s, 1440.75/s  (0.697s, 1468.25/s)  LR: 2.687e-05  Data: 0.009 (0.014)
Train: 550 [ 550/1251 ( 44%)]  Loss: 3.193 (3.26)  Time: 0.707s, 1448.95/s  (0.698s, 1467.40/s)  LR: 2.687e-05  Data: 0.011 (0.014)
Train: 550 [ 600/1251 ( 48%)]  Loss: 3.236 (3.26)  Time: 0.695s, 1472.85/s  (0.697s, 1468.60/s)  LR: 2.687e-05  Data: 0.009 (0.014)
Train: 550 [ 650/1251 ( 52%)]  Loss: 3.103 (3.25)  Time: 0.680s, 1506.79/s  (0.697s, 1470.03/s)  LR: 2.687e-05  Data: 0.010 (0.014)
Train: 550 [ 700/1251 ( 56%)]  Loss: 3.346 (3.26)  Time: 0.707s, 1449.38/s  (0.696s, 1471.06/s)  LR: 2.687e-05  Data: 0.012 (0.013)
Train: 550 [ 750/1251 ( 60%)]  Loss: 3.075 (3.25)  Time: 0.680s, 1505.79/s  (0.695s, 1472.50/s)  LR: 2.687e-05  Data: 0.011 (0.013)
Train: 550 [ 800/1251 ( 64%)]  Loss: 3.314 (3.25)  Time: 0.672s, 1524.92/s  (0.696s, 1472.19/s)  LR: 2.687e-05  Data: 0.011 (0.013)
Train: 550 [ 850/1251 ( 68%)]  Loss: 3.157 (3.24)  Time: 0.707s, 1447.59/s  (0.695s, 1472.70/s)  LR: 2.687e-05  Data: 0.010 (0.013)
Train: 550 [ 900/1251 ( 72%)]  Loss: 3.356 (3.25)  Time: 0.713s, 1435.76/s  (0.695s, 1473.30/s)  LR: 2.687e-05  Data: 0.011 (0.013)
Train: 550 [ 950/1251 ( 76%)]  Loss: 3.431 (3.26)  Time: 0.673s, 1521.68/s  (0.695s, 1473.86/s)  LR: 2.687e-05  Data: 0.013 (0.013)
Train: 550 [1000/1251 ( 80%)]  Loss: 3.435 (3.27)  Time: 0.678s, 1511.27/s  (0.695s, 1473.88/s)  LR: 2.687e-05  Data: 0.011 (0.012)
Train: 550 [1050/1251 ( 84%)]  Loss: 3.122 (3.26)  Time: 0.674s, 1520.21/s  (0.694s, 1474.95/s)  LR: 2.687e-05  Data: 0.011 (0.012)
Train: 550 [1100/1251 ( 88%)]  Loss: 2.822 (3.24)  Time: 0.675s, 1517.49/s  (0.694s, 1475.24/s)  LR: 2.687e-05  Data: 0.011 (0.012)
Train: 550 [1150/1251 ( 92%)]  Loss: 2.917 (3.23)  Time: 0.671s, 1525.13/s  (0.694s, 1474.51/s)  LR: 2.687e-05  Data: 0.013 (0.012)
Train: 550 [1200/1251 ( 96%)]  Loss: 3.036 (3.22)  Time: 0.671s, 1525.73/s  (0.694s, 1475.05/s)  LR: 2.687e-05  Data: 0.013 (0.012)
Train: 550 [1250/1251 (100%)]  Loss: 3.234 (3.22)  Time: 0.655s, 1562.70/s  (0.694s, 1475.75/s)  LR: 2.687e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.592 (1.592)  Loss:  0.6387 (0.6387)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.136 (0.568)  Loss:  0.7798 (1.1252)  Acc@1: 86.9104 (78.8560)  Acc@5: 96.3443 (94.3980)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-547.pth.tar', 79.00799992431641)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-543.pth.tar', 78.9680000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-548.pth.tar', 78.92400003173829)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-542.pth.tar', 78.9239999243164)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-544.pth.tar', 78.9000000756836)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-549.pth.tar', 78.89000000244141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-546.pth.tar', 78.88399997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-550.pth.tar', 78.85600013183594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-538.pth.tar', 78.85400015625)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-541.pth.tar', 78.83000010742188)

Train: 551 [   0/1251 (  0%)]  Loss: 3.046 (3.05)  Time: 2.289s,  447.38/s  (2.289s,  447.38/s)  LR: 2.620e-05  Data: 1.656 (1.656)
Train: 551 [  50/1251 (  4%)]  Loss: 3.235 (3.14)  Time: 0.670s, 1529.17/s  (0.730s, 1402.99/s)  LR: 2.620e-05  Data: 0.010 (0.048)
Train: 551 [ 100/1251 (  8%)]  Loss: 3.045 (3.11)  Time: 0.702s, 1459.67/s  (0.712s, 1439.14/s)  LR: 2.620e-05  Data: 0.010 (0.029)
Train: 551 [ 150/1251 ( 12%)]  Loss: 3.538 (3.22)  Time: 0.680s, 1506.19/s  (0.702s, 1457.84/s)  LR: 2.620e-05  Data: 0.010 (0.023)
Train: 551 [ 200/1251 ( 16%)]  Loss: 3.212 (3.22)  Time: 0.712s, 1438.50/s  (0.700s, 1463.19/s)  LR: 2.620e-05  Data: 0.010 (0.020)
Train: 551 [ 250/1251 ( 20%)]  Loss: 3.285 (3.23)  Time: 0.721s, 1420.41/s  (0.698s, 1466.92/s)  LR: 2.620e-05  Data: 0.010 (0.018)
Train: 551 [ 300/1251 ( 24%)]  Loss: 3.195 (3.22)  Time: 0.671s, 1526.05/s  (0.697s, 1468.69/s)  LR: 2.620e-05  Data: 0.010 (0.017)
Train: 551 [ 350/1251 ( 28%)]  Loss: 3.318 (3.23)  Time: 0.671s, 1526.44/s  (0.697s, 1469.08/s)  LR: 2.620e-05  Data: 0.010 (0.016)
Train: 551 [ 400/1251 ( 32%)]  Loss: 3.024 (3.21)  Time: 0.708s, 1446.24/s  (0.696s, 1470.68/s)  LR: 2.620e-05  Data: 0.011 (0.015)
Train: 551 [ 450/1251 ( 36%)]  Loss: 3.200 (3.21)  Time: 0.700s, 1463.35/s  (0.695s, 1472.61/s)  LR: 2.620e-05  Data: 0.009 (0.015)
Train: 551 [ 500/1251 ( 40%)]  Loss: 3.221 (3.21)  Time: 0.705s, 1452.80/s  (0.695s, 1474.37/s)  LR: 2.620e-05  Data: 0.009 (0.014)
Train: 551 [ 550/1251 ( 44%)]  Loss: 3.263 (3.22)  Time: 0.669s, 1531.57/s  (0.694s, 1474.73/s)  LR: 2.620e-05  Data: 0.009 (0.014)
Train: 551 [ 600/1251 ( 48%)]  Loss: 3.367 (3.23)  Time: 0.675s, 1518.14/s  (0.694s, 1475.85/s)  LR: 2.620e-05  Data: 0.010 (0.014)
Train: 551 [ 650/1251 ( 52%)]  Loss: 3.491 (3.25)  Time: 0.698s, 1467.00/s  (0.694s, 1475.62/s)  LR: 2.620e-05  Data: 0.009 (0.013)
Train: 551 [ 700/1251 ( 56%)]  Loss: 3.126 (3.24)  Time: 0.673s, 1522.64/s  (0.694s, 1476.48/s)  LR: 2.620e-05  Data: 0.011 (0.013)
Train: 551 [ 750/1251 ( 60%)]  Loss: 3.290 (3.24)  Time: 0.700s, 1462.97/s  (0.693s, 1477.26/s)  LR: 2.620e-05  Data: 0.010 (0.013)
Train: 551 [ 800/1251 ( 64%)]  Loss: 3.093 (3.23)  Time: 0.706s, 1451.41/s  (0.693s, 1477.14/s)  LR: 2.620e-05  Data: 0.012 (0.013)
Train: 551 [ 850/1251 ( 68%)]  Loss: 3.398 (3.24)  Time: 0.671s, 1527.19/s  (0.693s, 1477.77/s)  LR: 2.620e-05  Data: 0.010 (0.013)
Train: 551 [ 900/1251 ( 72%)]  Loss: 3.158 (3.24)  Time: 0.698s, 1467.29/s  (0.693s, 1477.44/s)  LR: 2.620e-05  Data: 0.010 (0.013)
Train: 551 [ 950/1251 ( 76%)]  Loss: 3.048 (3.23)  Time: 0.671s, 1526.82/s  (0.693s, 1477.79/s)  LR: 2.620e-05  Data: 0.010 (0.012)
Train: 551 [1000/1251 ( 80%)]  Loss: 3.142 (3.22)  Time: 0.669s, 1530.26/s  (0.693s, 1478.52/s)  LR: 2.620e-05  Data: 0.009 (0.012)
Train: 551 [1050/1251 ( 84%)]  Loss: 2.942 (3.21)  Time: 0.711s, 1439.93/s  (0.693s, 1478.48/s)  LR: 2.620e-05  Data: 0.010 (0.012)
Train: 551 [1100/1251 ( 88%)]  Loss: 3.429 (3.22)  Time: 0.708s, 1446.89/s  (0.693s, 1477.95/s)  LR: 2.620e-05  Data: 0.010 (0.012)
Train: 551 [1150/1251 ( 92%)]  Loss: 2.736 (3.20)  Time: 0.677s, 1512.30/s  (0.693s, 1477.87/s)  LR: 2.620e-05  Data: 0.009 (0.012)
Train: 551 [1200/1251 ( 96%)]  Loss: 3.443 (3.21)  Time: 0.675s, 1518.04/s  (0.693s, 1477.33/s)  LR: 2.620e-05  Data: 0.010 (0.012)
Train: 551 [1250/1251 (100%)]  Loss: 3.093 (3.21)  Time: 0.657s, 1557.75/s  (0.693s, 1477.56/s)  LR: 2.620e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.593 (1.593)  Loss:  0.6763 (0.6763)  Acc@1: 92.2852 (92.2852)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.136 (0.574)  Loss:  0.7886 (1.1722)  Acc@1: 86.6745 (78.8580)  Acc@5: 96.5802 (94.4260)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-547.pth.tar', 79.00799992431641)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-543.pth.tar', 78.9680000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-548.pth.tar', 78.92400003173829)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-542.pth.tar', 78.9239999243164)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-544.pth.tar', 78.9000000756836)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-549.pth.tar', 78.89000000244141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-546.pth.tar', 78.88399997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-551.pth.tar', 78.85800002929687)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-550.pth.tar', 78.85600013183594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-538.pth.tar', 78.85400015625)

Train: 552 [   0/1251 (  0%)]  Loss: 2.985 (2.99)  Time: 2.445s,  418.86/s  (2.445s,  418.86/s)  LR: 2.555e-05  Data: 1.812 (1.812)
Train: 552 [  50/1251 (  4%)]  Loss: 3.193 (3.09)  Time: 0.696s, 1470.46/s  (0.732s, 1398.86/s)  LR: 2.555e-05  Data: 0.011 (0.049)
Train: 552 [ 100/1251 (  8%)]  Loss: 2.944 (3.04)  Time: 0.735s, 1392.97/s  (0.717s, 1428.31/s)  LR: 2.555e-05  Data: 0.010 (0.030)
Train: 552 [ 150/1251 ( 12%)]  Loss: 3.493 (3.15)  Time: 0.674s, 1520.01/s  (0.709s, 1444.71/s)  LR: 2.555e-05  Data: 0.010 (0.024)
Train: 552 [ 200/1251 ( 16%)]  Loss: 3.265 (3.18)  Time: 0.669s, 1529.96/s  (0.705s, 1452.12/s)  LR: 2.555e-05  Data: 0.011 (0.021)
Train: 552 [ 250/1251 ( 20%)]  Loss: 3.339 (3.20)  Time: 0.671s, 1525.97/s  (0.703s, 1457.07/s)  LR: 2.555e-05  Data: 0.011 (0.019)
Train: 552 [ 300/1251 ( 24%)]  Loss: 3.413 (3.23)  Time: 0.731s, 1401.68/s  (0.701s, 1459.85/s)  LR: 2.555e-05  Data: 0.010 (0.017)
Train: 552 [ 350/1251 ( 28%)]  Loss: 3.356 (3.25)  Time: 0.684s, 1496.19/s  (0.701s, 1459.75/s)  LR: 2.555e-05  Data: 0.011 (0.016)
Train: 552 [ 400/1251 ( 32%)]  Loss: 2.842 (3.20)  Time: 0.674s, 1519.15/s  (0.700s, 1462.96/s)  LR: 2.555e-05  Data: 0.011 (0.016)
Train: 552 [ 450/1251 ( 36%)]  Loss: 2.754 (3.16)  Time: 0.719s, 1424.33/s  (0.700s, 1463.82/s)  LR: 2.555e-05  Data: 0.011 (0.015)
Train: 552 [ 500/1251 ( 40%)]  Loss: 3.038 (3.15)  Time: 0.716s, 1430.55/s  (0.699s, 1464.92/s)  LR: 2.555e-05  Data: 0.012 (0.015)
Train: 552 [ 550/1251 ( 44%)]  Loss: 3.077 (3.14)  Time: 0.705s, 1452.53/s  (0.698s, 1467.13/s)  LR: 2.555e-05  Data: 0.011 (0.014)
Train: 552 [ 600/1251 ( 48%)]  Loss: 3.033 (3.13)  Time: 0.672s, 1523.35/s  (0.697s, 1468.72/s)  LR: 2.555e-05  Data: 0.010 (0.014)
Train: 552 [ 650/1251 ( 52%)]  Loss: 3.261 (3.14)  Time: 0.671s, 1525.35/s  (0.697s, 1468.65/s)  LR: 2.555e-05  Data: 0.015 (0.014)
Train: 552 [ 700/1251 ( 56%)]  Loss: 2.802 (3.12)  Time: 0.677s, 1513.45/s  (0.697s, 1469.05/s)  LR: 2.555e-05  Data: 0.014 (0.014)
Train: 552 [ 750/1251 ( 60%)]  Loss: 3.229 (3.13)  Time: 0.713s, 1436.01/s  (0.697s, 1469.47/s)  LR: 2.555e-05  Data: 0.010 (0.013)
Train: 552 [ 800/1251 ( 64%)]  Loss: 3.140 (3.13)  Time: 0.700s, 1462.40/s  (0.697s, 1470.11/s)  LR: 2.555e-05  Data: 0.011 (0.013)
Train: 552 [ 850/1251 ( 68%)]  Loss: 3.353 (3.14)  Time: 0.673s, 1520.67/s  (0.696s, 1471.18/s)  LR: 2.555e-05  Data: 0.010 (0.013)
Train: 552 [ 900/1251 ( 72%)]  Loss: 3.170 (3.14)  Time: 0.694s, 1476.16/s  (0.696s, 1471.57/s)  LR: 2.555e-05  Data: 0.014 (0.013)
Train: 552 [ 950/1251 ( 76%)]  Loss: 3.086 (3.14)  Time: 0.671s, 1525.62/s  (0.696s, 1472.06/s)  LR: 2.555e-05  Data: 0.011 (0.013)
Train: 552 [1000/1251 ( 80%)]  Loss: 3.706 (3.17)  Time: 0.672s, 1524.02/s  (0.696s, 1471.91/s)  LR: 2.555e-05  Data: 0.010 (0.013)
Train: 552 [1050/1251 ( 84%)]  Loss: 3.116 (3.16)  Time: 0.682s, 1502.26/s  (0.695s, 1472.87/s)  LR: 2.555e-05  Data: 0.010 (0.013)
Train: 552 [1100/1251 ( 88%)]  Loss: 3.656 (3.18)  Time: 0.674s, 1520.40/s  (0.695s, 1473.70/s)  LR: 2.555e-05  Data: 0.011 (0.012)
Train: 552 [1150/1251 ( 92%)]  Loss: 3.129 (3.18)  Time: 0.675s, 1516.70/s  (0.695s, 1474.39/s)  LR: 2.555e-05  Data: 0.010 (0.012)
Train: 552 [1200/1251 ( 96%)]  Loss: 2.910 (3.17)  Time: 0.686s, 1493.09/s  (0.694s, 1474.55/s)  LR: 2.555e-05  Data: 0.013 (0.012)
Train: 552 [1250/1251 (100%)]  Loss: 3.420 (3.18)  Time: 0.697s, 1468.31/s  (0.694s, 1474.76/s)  LR: 2.555e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.608 (1.608)  Loss:  0.6704 (0.6704)  Acc@1: 91.7969 (91.7969)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.136 (0.573)  Loss:  0.7935 (1.1705)  Acc@1: 87.0283 (79.0120)  Acc@5: 96.6981 (94.3920)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-552.pth.tar', 79.01200005371093)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-547.pth.tar', 79.00799992431641)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-543.pth.tar', 78.9680000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-548.pth.tar', 78.92400003173829)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-542.pth.tar', 78.9239999243164)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-544.pth.tar', 78.9000000756836)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-549.pth.tar', 78.89000000244141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-546.pth.tar', 78.88399997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-551.pth.tar', 78.85800002929687)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-550.pth.tar', 78.85600013183594)

Train: 553 [   0/1251 (  0%)]  Loss: 3.102 (3.10)  Time: 2.234s,  458.37/s  (2.234s,  458.37/s)  LR: 2.491e-05  Data: 1.597 (1.597)
Train: 553 [  50/1251 (  4%)]  Loss: 3.004 (3.05)  Time: 0.671s, 1526.31/s  (0.724s, 1413.63/s)  LR: 2.491e-05  Data: 0.010 (0.047)
Train: 553 [ 100/1251 (  8%)]  Loss: 2.932 (3.01)  Time: 0.673s, 1521.81/s  (0.707s, 1449.35/s)  LR: 2.491e-05  Data: 0.010 (0.029)
Train: 553 [ 150/1251 ( 12%)]  Loss: 3.193 (3.06)  Time: 0.672s, 1524.92/s  (0.701s, 1460.98/s)  LR: 2.491e-05  Data: 0.011 (0.023)
Train: 553 [ 200/1251 ( 16%)]  Loss: 3.228 (3.09)  Time: 0.719s, 1423.41/s  (0.700s, 1463.76/s)  LR: 2.491e-05  Data: 0.010 (0.020)
Train: 553 [ 250/1251 ( 20%)]  Loss: 2.771 (3.04)  Time: 0.706s, 1450.68/s  (0.699s, 1465.51/s)  LR: 2.491e-05  Data: 0.010 (0.018)
Train: 553 [ 300/1251 ( 24%)]  Loss: 3.234 (3.07)  Time: 0.676s, 1514.16/s  (0.697s, 1468.71/s)  LR: 2.491e-05  Data: 0.013 (0.017)
Train: 553 [ 350/1251 ( 28%)]  Loss: 3.044 (3.06)  Time: 0.673s, 1521.46/s  (0.696s, 1470.66/s)  LR: 2.491e-05  Data: 0.009 (0.016)
Train: 553 [ 400/1251 ( 32%)]  Loss: 3.384 (3.10)  Time: 0.670s, 1527.95/s  (0.696s, 1472.03/s)  LR: 2.491e-05  Data: 0.011 (0.015)
Train: 553 [ 450/1251 ( 36%)]  Loss: 3.077 (3.10)  Time: 0.676s, 1515.19/s  (0.694s, 1474.58/s)  LR: 2.491e-05  Data: 0.009 (0.015)
Train: 553 [ 500/1251 ( 40%)]  Loss: 3.453 (3.13)  Time: 0.670s, 1527.68/s  (0.695s, 1474.06/s)  LR: 2.491e-05  Data: 0.010 (0.014)
Train: 553 [ 550/1251 ( 44%)]  Loss: 3.390 (3.15)  Time: 0.702s, 1458.63/s  (0.694s, 1474.45/s)  LR: 2.491e-05  Data: 0.010 (0.014)
Train: 553 [ 600/1251 ( 48%)]  Loss: 3.553 (3.18)  Time: 0.802s, 1276.32/s  (0.694s, 1475.45/s)  LR: 2.491e-05  Data: 0.017 (0.014)
Train: 553 [ 650/1251 ( 52%)]  Loss: 3.157 (3.18)  Time: 0.671s, 1525.72/s  (0.694s, 1475.18/s)  LR: 2.491e-05  Data: 0.009 (0.013)
Train: 553 [ 700/1251 ( 56%)]  Loss: 3.342 (3.19)  Time: 0.704s, 1455.18/s  (0.694s, 1475.91/s)  LR: 2.491e-05  Data: 0.009 (0.013)
Train: 553 [ 750/1251 ( 60%)]  Loss: 3.593 (3.22)  Time: 0.674s, 1518.78/s  (0.694s, 1476.37/s)  LR: 2.491e-05  Data: 0.014 (0.013)
Train: 553 [ 800/1251 ( 64%)]  Loss: 3.331 (3.22)  Time: 0.672s, 1524.10/s  (0.694s, 1476.29/s)  LR: 2.491e-05  Data: 0.010 (0.013)
Train: 553 [ 850/1251 ( 68%)]  Loss: 3.172 (3.22)  Time: 0.734s, 1394.22/s  (0.694s, 1476.39/s)  LR: 2.491e-05  Data: 0.009 (0.013)
Train: 553 [ 900/1251 ( 72%)]  Loss: 3.238 (3.22)  Time: 0.711s, 1439.87/s  (0.694s, 1476.44/s)  LR: 2.491e-05  Data: 0.011 (0.013)
Train: 553 [ 950/1251 ( 76%)]  Loss: 2.989 (3.21)  Time: 0.715s, 1432.21/s  (0.693s, 1476.73/s)  LR: 2.491e-05  Data: 0.010 (0.013)
Train: 553 [1000/1251 ( 80%)]  Loss: 3.171 (3.21)  Time: 0.685s, 1495.14/s  (0.693s, 1476.68/s)  LR: 2.491e-05  Data: 0.010 (0.013)
Train: 553 [1050/1251 ( 84%)]  Loss: 3.344 (3.21)  Time: 0.680s, 1504.83/s  (0.693s, 1476.78/s)  LR: 2.491e-05  Data: 0.010 (0.012)
Train: 553 [1100/1251 ( 88%)]  Loss: 3.446 (3.22)  Time: 0.671s, 1526.28/s  (0.693s, 1477.06/s)  LR: 2.491e-05  Data: 0.011 (0.012)
Train: 553 [1150/1251 ( 92%)]  Loss: 3.320 (3.23)  Time: 0.673s, 1522.15/s  (0.693s, 1477.39/s)  LR: 2.491e-05  Data: 0.011 (0.012)
Train: 553 [1200/1251 ( 96%)]  Loss: 2.944 (3.22)  Time: 0.698s, 1466.41/s  (0.693s, 1477.40/s)  LR: 2.491e-05  Data: 0.011 (0.012)
Train: 553 [1250/1251 (100%)]  Loss: 3.450 (3.23)  Time: 0.693s, 1477.00/s  (0.693s, 1477.63/s)  LR: 2.491e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.569 (1.569)  Loss:  0.7104 (0.7104)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.136 (0.577)  Loss:  0.8345 (1.1926)  Acc@1: 86.3208 (79.0840)  Acc@5: 96.9340 (94.3960)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-553.pth.tar', 79.08400000488281)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-552.pth.tar', 79.01200005371093)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-547.pth.tar', 79.00799992431641)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-543.pth.tar', 78.9680000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-548.pth.tar', 78.92400003173829)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-542.pth.tar', 78.9239999243164)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-544.pth.tar', 78.9000000756836)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-549.pth.tar', 78.89000000244141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-546.pth.tar', 78.88399997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-551.pth.tar', 78.85800002929687)

Train: 554 [   0/1251 (  0%)]  Loss: 2.928 (2.93)  Time: 2.115s,  484.24/s  (2.115s,  484.24/s)  LR: 2.429e-05  Data: 1.500 (1.500)
Train: 554 [  50/1251 (  4%)]  Loss: 3.124 (3.03)  Time: 0.714s, 1434.36/s  (0.730s, 1402.10/s)  LR: 2.429e-05  Data: 0.011 (0.050)
Train: 554 [ 100/1251 (  8%)]  Loss: 3.123 (3.06)  Time: 0.757s, 1352.29/s  (0.712s, 1437.97/s)  LR: 2.429e-05  Data: 0.009 (0.030)
Train: 554 [ 150/1251 ( 12%)]  Loss: 3.437 (3.15)  Time: 0.746s, 1372.93/s  (0.705s, 1452.02/s)  LR: 2.429e-05  Data: 0.011 (0.024)
Train: 554 [ 200/1251 ( 16%)]  Loss: 3.385 (3.20)  Time: 0.673s, 1520.90/s  (0.702s, 1458.06/s)  LR: 2.429e-05  Data: 0.011 (0.021)
Train: 554 [ 250/1251 ( 20%)]  Loss: 3.266 (3.21)  Time: 0.682s, 1500.69/s  (0.700s, 1462.93/s)  LR: 2.429e-05  Data: 0.015 (0.019)
Train: 554 [ 300/1251 ( 24%)]  Loss: 3.332 (3.23)  Time: 0.668s, 1532.57/s  (0.698s, 1467.83/s)  LR: 2.429e-05  Data: 0.010 (0.017)
Train: 554 [ 350/1251 ( 28%)]  Loss: 3.317 (3.24)  Time: 0.676s, 1514.15/s  (0.697s, 1469.59/s)  LR: 2.429e-05  Data: 0.011 (0.016)
Train: 554 [ 400/1251 ( 32%)]  Loss: 3.033 (3.22)  Time: 0.703s, 1456.07/s  (0.696s, 1471.62/s)  LR: 2.429e-05  Data: 0.010 (0.016)
Train: 554 [ 450/1251 ( 36%)]  Loss: 2.800 (3.17)  Time: 0.705s, 1451.98/s  (0.696s, 1471.96/s)  LR: 2.429e-05  Data: 0.009 (0.015)
Train: 554 [ 500/1251 ( 40%)]  Loss: 3.247 (3.18)  Time: 0.727s, 1407.59/s  (0.695s, 1473.29/s)  LR: 2.429e-05  Data: 0.011 (0.015)
Train: 554 [ 550/1251 ( 44%)]  Loss: 3.062 (3.17)  Time: 0.673s, 1520.81/s  (0.695s, 1472.67/s)  LR: 2.429e-05  Data: 0.009 (0.014)
Train: 554 [ 600/1251 ( 48%)]  Loss: 2.950 (3.15)  Time: 0.693s, 1477.55/s  (0.695s, 1473.62/s)  LR: 2.429e-05  Data: 0.010 (0.014)
Train: 554 [ 650/1251 ( 52%)]  Loss: 3.222 (3.16)  Time: 0.710s, 1442.73/s  (0.695s, 1473.77/s)  LR: 2.429e-05  Data: 0.009 (0.014)
Train: 554 [ 700/1251 ( 56%)]  Loss: 3.525 (3.18)  Time: 0.675s, 1516.11/s  (0.694s, 1474.69/s)  LR: 2.429e-05  Data: 0.010 (0.013)
Train: 554 [ 750/1251 ( 60%)]  Loss: 3.198 (3.18)  Time: 0.674s, 1518.68/s  (0.695s, 1474.18/s)  LR: 2.429e-05  Data: 0.011 (0.013)
Train: 554 [ 800/1251 ( 64%)]  Loss: 3.298 (3.19)  Time: 0.739s, 1385.76/s  (0.695s, 1473.65/s)  LR: 2.429e-05  Data: 0.010 (0.013)
Train: 554 [ 850/1251 ( 68%)]  Loss: 3.038 (3.18)  Time: 0.713s, 1435.21/s  (0.695s, 1474.26/s)  LR: 2.429e-05  Data: 0.009 (0.013)
Train: 554 [ 900/1251 ( 72%)]  Loss: 3.298 (3.19)  Time: 0.689s, 1486.68/s  (0.694s, 1474.45/s)  LR: 2.429e-05  Data: 0.013 (0.013)
Train: 554 [ 950/1251 ( 76%)]  Loss: 2.903 (3.17)  Time: 0.718s, 1427.05/s  (0.695s, 1474.27/s)  LR: 2.429e-05  Data: 0.010 (0.013)
Train: 554 [1000/1251 ( 80%)]  Loss: 3.410 (3.19)  Time: 0.706s, 1450.05/s  (0.695s, 1474.38/s)  LR: 2.429e-05  Data: 0.009 (0.013)
Train: 554 [1050/1251 ( 84%)]  Loss: 3.114 (3.18)  Time: 0.672s, 1523.46/s  (0.694s, 1475.20/s)  LR: 2.429e-05  Data: 0.011 (0.012)
Train: 554 [1100/1251 ( 88%)]  Loss: 3.264 (3.19)  Time: 0.672s, 1522.68/s  (0.694s, 1475.69/s)  LR: 2.429e-05  Data: 0.011 (0.012)
Train: 554 [1150/1251 ( 92%)]  Loss: 3.153 (3.18)  Time: 0.672s, 1524.66/s  (0.694s, 1474.69/s)  LR: 2.429e-05  Data: 0.010 (0.012)
Train: 554 [1200/1251 ( 96%)]  Loss: 3.471 (3.20)  Time: 0.674s, 1519.63/s  (0.694s, 1475.02/s)  LR: 2.429e-05  Data: 0.014 (0.012)
Train: 554 [1250/1251 (100%)]  Loss: 2.999 (3.19)  Time: 0.657s, 1557.74/s  (0.694s, 1475.78/s)  LR: 2.429e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.641 (1.641)  Loss:  0.7749 (0.7749)  Acc@1: 91.4062 (91.4062)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.136 (0.575)  Loss:  0.8721 (1.2610)  Acc@1: 86.9104 (78.8740)  Acc@5: 96.5802 (94.3520)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-553.pth.tar', 79.08400000488281)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-552.pth.tar', 79.01200005371093)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-547.pth.tar', 79.00799992431641)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-543.pth.tar', 78.9680000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-548.pth.tar', 78.92400003173829)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-542.pth.tar', 78.9239999243164)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-544.pth.tar', 78.9000000756836)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-549.pth.tar', 78.89000000244141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-546.pth.tar', 78.88399997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-554.pth.tar', 78.8740000024414)

Train: 555 [   0/1251 (  0%)]  Loss: 3.374 (3.37)  Time: 2.311s,  443.08/s  (2.311s,  443.08/s)  LR: 2.368e-05  Data: 1.696 (1.696)
Train: 555 [  50/1251 (  4%)]  Loss: 3.394 (3.38)  Time: 0.671s, 1526.31/s  (0.730s, 1402.74/s)  LR: 2.368e-05  Data: 0.011 (0.050)
Train: 555 [ 100/1251 (  8%)]  Loss: 3.614 (3.46)  Time: 0.716s, 1430.61/s  (0.708s, 1447.21/s)  LR: 2.368e-05  Data: 0.013 (0.030)
Train: 555 [ 150/1251 ( 12%)]  Loss: 3.489 (3.47)  Time: 0.677s, 1511.61/s  (0.702s, 1458.33/s)  LR: 2.368e-05  Data: 0.011 (0.024)
Train: 555 [ 200/1251 ( 16%)]  Loss: 2.893 (3.35)  Time: 0.690s, 1483.91/s  (0.700s, 1463.60/s)  LR: 2.368e-05  Data: 0.017 (0.021)
Train: 555 [ 250/1251 ( 20%)]  Loss: 3.215 (3.33)  Time: 0.704s, 1454.21/s  (0.699s, 1465.79/s)  LR: 2.368e-05  Data: 0.010 (0.019)
Train: 555 [ 300/1251 ( 24%)]  Loss: 3.233 (3.32)  Time: 0.673s, 1522.47/s  (0.698s, 1466.73/s)  LR: 2.368e-05  Data: 0.012 (0.017)
Train: 555 [ 350/1251 ( 28%)]  Loss: 3.314 (3.32)  Time: 0.712s, 1438.70/s  (0.697s, 1469.48/s)  LR: 2.368e-05  Data: 0.010 (0.016)
Train: 555 [ 400/1251 ( 32%)]  Loss: 2.831 (3.26)  Time: 0.676s, 1515.31/s  (0.696s, 1470.69/s)  LR: 2.368e-05  Data: 0.009 (0.015)
Train: 555 [ 450/1251 ( 36%)]  Loss: 3.604 (3.30)  Time: 0.671s, 1525.80/s  (0.696s, 1470.43/s)  LR: 2.368e-05  Data: 0.010 (0.015)
Train: 555 [ 500/1251 ( 40%)]  Loss: 3.423 (3.31)  Time: 0.673s, 1522.06/s  (0.696s, 1471.17/s)  LR: 2.368e-05  Data: 0.010 (0.014)
Train: 555 [ 550/1251 ( 44%)]  Loss: 3.007 (3.28)  Time: 0.670s, 1527.67/s  (0.696s, 1471.76/s)  LR: 2.368e-05  Data: 0.010 (0.014)
Train: 555 [ 600/1251 ( 48%)]  Loss: 2.868 (3.25)  Time: 0.668s, 1532.78/s  (0.696s, 1472.16/s)  LR: 2.368e-05  Data: 0.009 (0.014)
Train: 555 [ 650/1251 ( 52%)]  Loss: 3.211 (3.25)  Time: 0.703s, 1456.41/s  (0.696s, 1471.65/s)  LR: 2.368e-05  Data: 0.009 (0.014)
Train: 555 [ 700/1251 ( 56%)]  Loss: 2.954 (3.23)  Time: 0.670s, 1529.23/s  (0.696s, 1472.31/s)  LR: 2.368e-05  Data: 0.010 (0.013)
Train: 555 [ 750/1251 ( 60%)]  Loss: 3.270 (3.23)  Time: 0.673s, 1522.30/s  (0.695s, 1473.14/s)  LR: 2.368e-05  Data: 0.009 (0.013)
Train: 555 [ 800/1251 ( 64%)]  Loss: 2.975 (3.22)  Time: 0.703s, 1456.47/s  (0.695s, 1473.59/s)  LR: 2.368e-05  Data: 0.011 (0.013)
Train: 555 [ 850/1251 ( 68%)]  Loss: 3.174 (3.21)  Time: 0.751s, 1363.68/s  (0.697s, 1468.45/s)  LR: 2.368e-05  Data: 0.012 (0.014)
Train: 555 [ 900/1251 ( 72%)]  Loss: 2.827 (3.19)  Time: 0.737s, 1389.27/s  (0.701s, 1459.81/s)  LR: 2.368e-05  Data: 0.013 (0.015)
Train: 555 [ 950/1251 ( 76%)]  Loss: 2.663 (3.17)  Time: 0.681s, 1502.57/s  (0.702s, 1457.90/s)  LR: 2.368e-05  Data: 0.014 (0.015)
Train: 555 [1000/1251 ( 80%)]  Loss: 3.154 (3.17)  Time: 0.671s, 1526.35/s  (0.702s, 1458.47/s)  LR: 2.368e-05  Data: 0.010 (0.015)
Train: 555 [1050/1251 ( 84%)]  Loss: 3.073 (3.16)  Time: 0.671s, 1525.08/s  (0.702s, 1459.46/s)  LR: 2.368e-05  Data: 0.011 (0.015)
Train: 555 [1100/1251 ( 88%)]  Loss: 2.974 (3.15)  Time: 0.674s, 1519.49/s  (0.701s, 1460.81/s)  LR: 2.368e-05  Data: 0.010 (0.014)
Train: 555 [1150/1251 ( 92%)]  Loss: 3.127 (3.15)  Time: 0.673s, 1521.68/s  (0.700s, 1462.34/s)  LR: 2.368e-05  Data: 0.011 (0.014)
Train: 555 [1200/1251 ( 96%)]  Loss: 3.110 (3.15)  Time: 0.676s, 1513.99/s  (0.700s, 1463.36/s)  LR: 2.368e-05  Data: 0.011 (0.014)
Train: 555 [1250/1251 (100%)]  Loss: 3.163 (3.15)  Time: 0.659s, 1554.59/s  (0.699s, 1464.75/s)  LR: 2.368e-05  Data: 0.000 (0.014)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.480 (1.480)  Loss:  0.7080 (0.7080)  Acc@1: 91.8945 (91.8945)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.137 (0.585)  Loss:  0.8247 (1.1949)  Acc@1: 86.7924 (78.9100)  Acc@5: 96.8160 (94.4360)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-553.pth.tar', 79.08400000488281)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-552.pth.tar', 79.01200005371093)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-547.pth.tar', 79.00799992431641)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-543.pth.tar', 78.9680000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-548.pth.tar', 78.92400003173829)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-542.pth.tar', 78.9239999243164)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-555.pth.tar', 78.90999995117187)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-544.pth.tar', 78.9000000756836)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-549.pth.tar', 78.89000000244141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-546.pth.tar', 78.88399997558594)

Train: 556 [   0/1251 (  0%)]  Loss: 3.288 (3.29)  Time: 2.225s,  460.19/s  (2.225s,  460.19/s)  LR: 2.308e-05  Data: 1.611 (1.611)
Train: 556 [  50/1251 (  4%)]  Loss: 3.260 (3.27)  Time: 0.676s, 1514.55/s  (0.733s, 1396.83/s)  LR: 2.308e-05  Data: 0.013 (0.051)
Train: 556 [ 100/1251 (  8%)]  Loss: 3.271 (3.27)  Time: 0.681s, 1504.22/s  (0.713s, 1435.72/s)  LR: 2.308e-05  Data: 0.011 (0.031)
Train: 556 [ 150/1251 ( 12%)]  Loss: 3.083 (3.23)  Time: 0.711s, 1440.31/s  (0.706s, 1450.02/s)  LR: 2.308e-05  Data: 0.011 (0.024)
Train: 556 [ 200/1251 ( 16%)]  Loss: 3.454 (3.27)  Time: 0.702s, 1458.86/s  (0.706s, 1450.92/s)  LR: 2.308e-05  Data: 0.010 (0.021)
Train: 556 [ 250/1251 ( 20%)]  Loss: 3.404 (3.29)  Time: 0.694s, 1475.97/s  (0.704s, 1455.35/s)  LR: 2.308e-05  Data: 0.010 (0.019)
Train: 556 [ 300/1251 ( 24%)]  Loss: 3.381 (3.31)  Time: 0.680s, 1504.89/s  (0.701s, 1460.38/s)  LR: 2.308e-05  Data: 0.013 (0.018)
Train: 556 [ 350/1251 ( 28%)]  Loss: 3.096 (3.28)  Time: 0.672s, 1524.30/s  (0.699s, 1464.50/s)  LR: 2.308e-05  Data: 0.011 (0.017)
Train: 556 [ 400/1251 ( 32%)]  Loss: 3.412 (3.29)  Time: 0.735s, 1393.19/s  (0.699s, 1465.90/s)  LR: 2.308e-05  Data: 0.013 (0.016)
Train: 556 [ 450/1251 ( 36%)]  Loss: 3.251 (3.29)  Time: 0.736s, 1390.88/s  (0.701s, 1460.83/s)  LR: 2.308e-05  Data: 0.017 (0.016)
Train: 556 [ 500/1251 ( 40%)]  Loss: 3.039 (3.27)  Time: 0.713s, 1436.61/s  (0.702s, 1459.02/s)  LR: 2.308e-05  Data: 0.020 (0.015)
Train: 556 [ 550/1251 ( 44%)]  Loss: 2.995 (3.24)  Time: 0.702s, 1458.75/s  (0.703s, 1457.07/s)  LR: 2.308e-05  Data: 0.010 (0.015)
Train: 556 [ 600/1251 ( 48%)]  Loss: 3.250 (3.24)  Time: 0.705s, 1451.97/s  (0.702s, 1459.43/s)  LR: 2.308e-05  Data: 0.010 (0.015)
Train: 556 [ 650/1251 ( 52%)]  Loss: 3.265 (3.25)  Time: 0.702s, 1457.67/s  (0.700s, 1462.77/s)  LR: 2.308e-05  Data: 0.009 (0.014)
Train: 556 [ 700/1251 ( 56%)]  Loss: 3.451 (3.26)  Time: 0.681s, 1502.78/s  (0.700s, 1463.52/s)  LR: 2.308e-05  Data: 0.011 (0.014)
Train: 556 [ 750/1251 ( 60%)]  Loss: 2.912 (3.24)  Time: 0.673s, 1521.88/s  (0.699s, 1464.49/s)  LR: 2.308e-05  Data: 0.011 (0.014)
Train: 556 [ 800/1251 ( 64%)]  Loss: 2.744 (3.21)  Time: 0.707s, 1447.71/s  (0.699s, 1465.42/s)  LR: 2.308e-05  Data: 0.009 (0.013)
Train: 556 [ 850/1251 ( 68%)]  Loss: 3.216 (3.21)  Time: 0.741s, 1382.66/s  (0.698s, 1466.60/s)  LR: 2.308e-05  Data: 0.015 (0.013)
Train: 556 [ 900/1251 ( 72%)]  Loss: 3.463 (3.22)  Time: 0.678s, 1511.21/s  (0.698s, 1467.14/s)  LR: 2.308e-05  Data: 0.011 (0.013)
Train: 556 [ 950/1251 ( 76%)]  Loss: 2.950 (3.21)  Time: 0.748s, 1369.74/s  (0.698s, 1467.77/s)  LR: 2.308e-05  Data: 0.012 (0.013)
Train: 556 [1000/1251 ( 80%)]  Loss: 3.107 (3.20)  Time: 0.671s, 1525.91/s  (0.697s, 1468.26/s)  LR: 2.308e-05  Data: 0.011 (0.013)
Train: 556 [1050/1251 ( 84%)]  Loss: 2.872 (3.19)  Time: 0.674s, 1519.92/s  (0.697s, 1468.93/s)  LR: 2.308e-05  Data: 0.011 (0.013)
Train: 556 [1100/1251 ( 88%)]  Loss: 3.258 (3.19)  Time: 0.722s, 1417.69/s  (0.697s, 1469.62/s)  LR: 2.308e-05  Data: 0.011 (0.013)
Train: 556 [1150/1251 ( 92%)]  Loss: 3.222 (3.19)  Time: 0.697s, 1470.20/s  (0.697s, 1469.99/s)  LR: 2.308e-05  Data: 0.009 (0.013)
Train: 556 [1200/1251 ( 96%)]  Loss: 3.174 (3.19)  Time: 0.676s, 1514.57/s  (0.697s, 1469.93/s)  LR: 2.308e-05  Data: 0.013 (0.012)
Train: 556 [1250/1251 (100%)]  Loss: 3.157 (3.19)  Time: 0.689s, 1486.48/s  (0.696s, 1470.30/s)  LR: 2.308e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.543 (1.543)  Loss:  0.7471 (0.7471)  Acc@1: 91.3086 (91.3086)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.136 (0.575)  Loss:  0.8364 (1.2098)  Acc@1: 87.1462 (79.0400)  Acc@5: 96.6981 (94.3680)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-553.pth.tar', 79.08400000488281)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-556.pth.tar', 79.04000010498046)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-552.pth.tar', 79.01200005371093)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-547.pth.tar', 79.00799992431641)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-543.pth.tar', 78.9680000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-548.pth.tar', 78.92400003173829)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-542.pth.tar', 78.9239999243164)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-555.pth.tar', 78.90999995117187)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-544.pth.tar', 78.9000000756836)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-549.pth.tar', 78.89000000244141)

Train: 557 [   0/1251 (  0%)]  Loss: 3.341 (3.34)  Time: 2.315s,  442.24/s  (2.315s,  442.24/s)  LR: 2.249e-05  Data: 1.700 (1.700)
Train: 557 [  50/1251 (  4%)]  Loss: 3.084 (3.21)  Time: 0.679s, 1507.33/s  (0.722s, 1418.59/s)  LR: 2.249e-05  Data: 0.010 (0.051)
Train: 557 [ 100/1251 (  8%)]  Loss: 3.015 (3.15)  Time: 0.672s, 1522.71/s  (0.702s, 1458.08/s)  LR: 2.249e-05  Data: 0.010 (0.031)
Train: 557 [ 150/1251 ( 12%)]  Loss: 2.941 (3.10)  Time: 0.673s, 1521.53/s  (0.700s, 1463.50/s)  LR: 2.249e-05  Data: 0.010 (0.024)
Train: 557 [ 200/1251 ( 16%)]  Loss: 2.862 (3.05)  Time: 0.726s, 1410.85/s  (0.698s, 1466.52/s)  LR: 2.249e-05  Data: 0.009 (0.021)
Train: 557 [ 250/1251 ( 20%)]  Loss: 3.235 (3.08)  Time: 0.701s, 1460.41/s  (0.698s, 1467.16/s)  LR: 2.249e-05  Data: 0.010 (0.019)
Train: 557 [ 300/1251 ( 24%)]  Loss: 2.984 (3.07)  Time: 0.700s, 1461.94/s  (0.697s, 1469.62/s)  LR: 2.249e-05  Data: 0.010 (0.017)
Train: 557 [ 350/1251 ( 28%)]  Loss: 2.878 (3.04)  Time: 0.672s, 1522.88/s  (0.696s, 1471.47/s)  LR: 2.249e-05  Data: 0.010 (0.016)
Train: 557 [ 400/1251 ( 32%)]  Loss: 3.006 (3.04)  Time: 0.711s, 1440.44/s  (0.696s, 1472.13/s)  LR: 2.249e-05  Data: 0.011 (0.016)
Train: 557 [ 450/1251 ( 36%)]  Loss: 3.163 (3.05)  Time: 0.708s, 1445.42/s  (0.694s, 1474.58/s)  LR: 2.249e-05  Data: 0.010 (0.015)
Train: 557 [ 500/1251 ( 40%)]  Loss: 3.019 (3.05)  Time: 0.695s, 1473.46/s  (0.695s, 1474.43/s)  LR: 2.249e-05  Data: 0.009 (0.015)
Train: 557 [ 550/1251 ( 44%)]  Loss: 3.032 (3.05)  Time: 0.702s, 1459.47/s  (0.695s, 1473.25/s)  LR: 2.249e-05  Data: 0.011 (0.014)
Train: 557 [ 600/1251 ( 48%)]  Loss: 3.227 (3.06)  Time: 0.693s, 1478.08/s  (0.698s, 1466.20/s)  LR: 2.249e-05  Data: 0.015 (0.014)
Train: 557 [ 650/1251 ( 52%)]  Loss: 3.081 (3.06)  Time: 0.730s, 1401.94/s  (0.702s, 1458.40/s)  LR: 2.249e-05  Data: 0.010 (0.014)
Train: 557 [ 700/1251 ( 56%)]  Loss: 3.262 (3.08)  Time: 0.772s, 1325.76/s  (0.705s, 1451.50/s)  LR: 2.249e-05  Data: 0.011 (0.014)
Train: 557 [ 750/1251 ( 60%)]  Loss: 3.650 (3.11)  Time: 0.680s, 1506.32/s  (0.709s, 1444.67/s)  LR: 2.249e-05  Data: 0.011 (0.014)
Train: 557 [ 800/1251 ( 64%)]  Loss: 2.979 (3.10)  Time: 0.736s, 1391.61/s  (0.712s, 1438.54/s)  LR: 2.249e-05  Data: 0.013 (0.014)
Train: 557 [ 850/1251 ( 68%)]  Loss: 3.248 (3.11)  Time: 0.762s, 1343.13/s  (0.714s, 1434.09/s)  LR: 2.249e-05  Data: 0.009 (0.015)
Train: 557 [ 900/1251 ( 72%)]  Loss: 3.431 (3.13)  Time: 0.758s, 1351.10/s  (0.717s, 1427.20/s)  LR: 2.249e-05  Data: 0.016 (0.016)
Train: 557 [ 950/1251 ( 76%)]  Loss: 3.177 (3.13)  Time: 0.730s, 1402.36/s  (0.719s, 1424.13/s)  LR: 2.249e-05  Data: 0.009 (0.015)
Train: 557 [1000/1251 ( 80%)]  Loss: 3.291 (3.14)  Time: 0.692s, 1480.02/s  (0.718s, 1426.25/s)  LR: 2.249e-05  Data: 0.009 (0.015)
Train: 557 [1050/1251 ( 84%)]  Loss: 3.106 (3.14)  Time: 0.676s, 1514.77/s  (0.717s, 1428.22/s)  LR: 2.249e-05  Data: 0.015 (0.015)
Train: 557 [1100/1251 ( 88%)]  Loss: 3.412 (3.15)  Time: 0.671s, 1525.50/s  (0.716s, 1429.58/s)  LR: 2.249e-05  Data: 0.010 (0.015)
Train: 557 [1150/1251 ( 92%)]  Loss: 3.061 (3.15)  Time: 0.674s, 1518.73/s  (0.715s, 1432.41/s)  LR: 2.249e-05  Data: 0.009 (0.015)
Train: 557 [1200/1251 ( 96%)]  Loss: 2.928 (3.14)  Time: 0.671s, 1525.99/s  (0.714s, 1434.87/s)  LR: 2.249e-05  Data: 0.010 (0.014)
Train: 557 [1250/1251 (100%)]  Loss: 3.055 (3.13)  Time: 0.660s, 1551.68/s  (0.713s, 1436.85/s)  LR: 2.249e-05  Data: 0.000 (0.014)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.499 (1.499)  Loss:  0.6909 (0.6909)  Acc@1: 91.7969 (91.7969)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.136 (0.578)  Loss:  0.7949 (1.1508)  Acc@1: 86.9104 (79.1480)  Acc@5: 97.0519 (94.4200)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-557.pth.tar', 79.1480000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-553.pth.tar', 79.08400000488281)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-556.pth.tar', 79.04000010498046)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-552.pth.tar', 79.01200005371093)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-547.pth.tar', 79.00799992431641)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-543.pth.tar', 78.9680000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-548.pth.tar', 78.92400003173829)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-542.pth.tar', 78.9239999243164)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-555.pth.tar', 78.90999995117187)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-544.pth.tar', 78.9000000756836)

Train: 558 [   0/1251 (  0%)]  Loss: 3.098 (3.10)  Time: 2.218s,  461.63/s  (2.218s,  461.63/s)  LR: 2.192e-05  Data: 1.602 (1.602)
Train: 558 [  50/1251 (  4%)]  Loss: 2.815 (2.96)  Time: 0.714s, 1433.96/s  (0.722s, 1417.91/s)  LR: 2.192e-05  Data: 0.010 (0.047)
Train: 558 [ 100/1251 (  8%)]  Loss: 3.265 (3.06)  Time: 0.672s, 1523.09/s  (0.707s, 1448.60/s)  LR: 2.192e-05  Data: 0.011 (0.029)
Train: 558 [ 150/1251 ( 12%)]  Loss: 3.084 (3.07)  Time: 0.697s, 1468.39/s  (0.701s, 1461.43/s)  LR: 2.192e-05  Data: 0.010 (0.023)
Train: 558 [ 200/1251 ( 16%)]  Loss: 3.111 (3.07)  Time: 0.674s, 1519.74/s  (0.698s, 1466.91/s)  LR: 2.192e-05  Data: 0.014 (0.020)
Train: 558 [ 250/1251 ( 20%)]  Loss: 3.451 (3.14)  Time: 0.710s, 1442.40/s  (0.696s, 1470.25/s)  LR: 2.192e-05  Data: 0.010 (0.018)
Train: 558 [ 300/1251 ( 24%)]  Loss: 3.025 (3.12)  Time: 0.675s, 1516.52/s  (0.694s, 1474.86/s)  LR: 2.192e-05  Data: 0.011 (0.017)
Train: 558 [ 350/1251 ( 28%)]  Loss: 2.846 (3.09)  Time: 0.712s, 1437.97/s  (0.694s, 1475.17/s)  LR: 2.192e-05  Data: 0.011 (0.016)
Train: 558 [ 400/1251 ( 32%)]  Loss: 3.056 (3.08)  Time: 0.703s, 1457.61/s  (0.693s, 1476.83/s)  LR: 2.192e-05  Data: 0.012 (0.015)
Train: 558 [ 450/1251 ( 36%)]  Loss: 2.909 (3.07)  Time: 0.673s, 1522.00/s  (0.693s, 1476.85/s)  LR: 2.192e-05  Data: 0.011 (0.015)
Train: 558 [ 500/1251 ( 40%)]  Loss: 3.222 (3.08)  Time: 0.684s, 1497.22/s  (0.693s, 1476.94/s)  LR: 2.192e-05  Data: 0.011 (0.014)
Train: 558 [ 550/1251 ( 44%)]  Loss: 3.375 (3.10)  Time: 0.699s, 1465.69/s  (0.694s, 1476.34/s)  LR: 2.192e-05  Data: 0.010 (0.014)
Train: 558 [ 600/1251 ( 48%)]  Loss: 3.446 (3.13)  Time: 0.702s, 1459.23/s  (0.693s, 1477.97/s)  LR: 2.192e-05  Data: 0.011 (0.014)
Train: 558 [ 650/1251 ( 52%)]  Loss: 3.554 (3.16)  Time: 0.672s, 1523.71/s  (0.693s, 1478.19/s)  LR: 2.192e-05  Data: 0.010 (0.013)
Train: 558 [ 700/1251 ( 56%)]  Loss: 3.167 (3.16)  Time: 0.697s, 1468.88/s  (0.693s, 1477.36/s)  LR: 2.192e-05  Data: 0.010 (0.013)
Train: 558 [ 750/1251 ( 60%)]  Loss: 3.105 (3.16)  Time: 0.702s, 1459.33/s  (0.693s, 1477.92/s)  LR: 2.192e-05  Data: 0.010 (0.013)
Train: 558 [ 800/1251 ( 64%)]  Loss: 3.042 (3.15)  Time: 0.729s, 1404.96/s  (0.693s, 1477.42/s)  LR: 2.192e-05  Data: 0.010 (0.013)
Train: 558 [ 850/1251 ( 68%)]  Loss: 3.230 (3.16)  Time: 0.707s, 1449.08/s  (0.694s, 1476.27/s)  LR: 2.192e-05  Data: 0.012 (0.013)
Train: 558 [ 900/1251 ( 72%)]  Loss: 3.131 (3.15)  Time: 0.700s, 1462.50/s  (0.693s, 1476.83/s)  LR: 2.192e-05  Data: 0.009 (0.013)
Train: 558 [ 950/1251 ( 76%)]  Loss: 3.057 (3.15)  Time: 0.675s, 1517.32/s  (0.694s, 1475.93/s)  LR: 2.192e-05  Data: 0.011 (0.012)
Train: 558 [1000/1251 ( 80%)]  Loss: 3.277 (3.16)  Time: 0.691s, 1481.45/s  (0.694s, 1474.94/s)  LR: 2.192e-05  Data: 0.015 (0.012)
Train: 558 [1050/1251 ( 84%)]  Loss: 2.661 (3.13)  Time: 0.730s, 1402.47/s  (0.696s, 1470.29/s)  LR: 2.192e-05  Data: 0.015 (0.013)
Train: 558 [1100/1251 ( 88%)]  Loss: 3.314 (3.14)  Time: 0.803s, 1275.37/s  (0.701s, 1460.83/s)  LR: 2.192e-05  Data: 0.012 (0.016)
Train: 558 [1150/1251 ( 92%)]  Loss: 3.010 (3.14)  Time: 0.702s, 1459.10/s  (0.703s, 1456.28/s)  LR: 2.192e-05  Data: 0.016 (0.016)
Train: 558 [1200/1251 ( 96%)]  Loss: 3.110 (3.13)  Time: 0.685s, 1494.42/s  (0.706s, 1449.75/s)  LR: 2.192e-05  Data: 0.014 (0.016)
Train: 558 [1250/1251 (100%)]  Loss: 3.253 (3.14)  Time: 0.719s, 1424.39/s  (0.709s, 1444.14/s)  LR: 2.192e-05  Data: 0.000 (0.016)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.893 (1.893)  Loss:  0.6787 (0.6787)  Acc@1: 91.8945 (91.8945)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.136 (0.781)  Loss:  0.7842 (1.1615)  Acc@1: 86.2028 (79.2020)  Acc@5: 97.0519 (94.3640)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-558.pth.tar', 79.20199995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-557.pth.tar', 79.1480000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-553.pth.tar', 79.08400000488281)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-556.pth.tar', 79.04000010498046)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-552.pth.tar', 79.01200005371093)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-547.pth.tar', 79.00799992431641)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-543.pth.tar', 78.9680000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-548.pth.tar', 78.92400003173829)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-542.pth.tar', 78.9239999243164)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-555.pth.tar', 78.90999995117187)

Train: 559 [   0/1251 (  0%)]  Loss: 3.129 (3.13)  Time: 2.910s,  351.84/s  (2.910s,  351.84/s)  LR: 2.136e-05  Data: 2.270 (2.270)
Train: 559 [  50/1251 (  4%)]  Loss: 3.340 (3.23)  Time: 0.704s, 1454.00/s  (0.837s, 1223.27/s)  LR: 2.136e-05  Data: 0.013 (0.103)
Train: 559 [ 100/1251 (  8%)]  Loss: 3.333 (3.27)  Time: 0.683s, 1499.56/s  (0.796s, 1286.09/s)  LR: 2.136e-05  Data: 0.014 (0.058)
Train: 559 [ 150/1251 ( 12%)]  Loss: 3.181 (3.25)  Time: 0.725s, 1411.60/s  (0.768s, 1332.58/s)  LR: 2.136e-05  Data: 0.009 (0.044)
Train: 559 [ 200/1251 ( 16%)]  Loss: 3.318 (3.26)  Time: 0.671s, 1525.39/s  (0.750s, 1365.63/s)  LR: 2.136e-05  Data: 0.010 (0.035)
Train: 559 [ 250/1251 ( 20%)]  Loss: 3.136 (3.24)  Time: 0.672s, 1524.72/s  (0.740s, 1383.68/s)  LR: 2.136e-05  Data: 0.010 (0.030)
Train: 559 [ 300/1251 ( 24%)]  Loss: 3.183 (3.23)  Time: 0.678s, 1509.74/s  (0.731s, 1400.04/s)  LR: 2.136e-05  Data: 0.010 (0.027)
Train: 559 [ 350/1251 ( 28%)]  Loss: 2.911 (3.19)  Time: 0.703s, 1457.44/s  (0.725s, 1412.60/s)  LR: 2.136e-05  Data: 0.009 (0.025)
Train: 559 [ 400/1251 ( 32%)]  Loss: 2.978 (3.17)  Time: 0.672s, 1524.40/s  (0.721s, 1419.71/s)  LR: 2.136e-05  Data: 0.010 (0.023)
Train: 559 [ 450/1251 ( 36%)]  Loss: 3.269 (3.18)  Time: 0.696s, 1471.26/s  (0.718s, 1425.62/s)  LR: 2.136e-05  Data: 0.014 (0.022)
Train: 559 [ 500/1251 ( 40%)]  Loss: 2.979 (3.16)  Time: 0.713s, 1435.27/s  (0.715s, 1431.65/s)  LR: 2.136e-05  Data: 0.010 (0.021)
Train: 559 [ 550/1251 ( 44%)]  Loss: 3.260 (3.17)  Time: 0.668s, 1532.95/s  (0.713s, 1435.43/s)  LR: 2.136e-05  Data: 0.009 (0.020)
Train: 559 [ 600/1251 ( 48%)]  Loss: 2.864 (3.14)  Time: 0.754s, 1358.64/s  (0.713s, 1435.93/s)  LR: 2.136e-05  Data: 0.013 (0.019)
Train: 559 [ 650/1251 ( 52%)]  Loss: 3.424 (3.16)  Time: 0.785s, 1303.67/s  (0.716s, 1430.26/s)  LR: 2.136e-05  Data: 0.013 (0.019)
Train: 559 [ 700/1251 ( 56%)]  Loss: 3.029 (3.16)  Time: 0.677s, 1512.95/s  (0.717s, 1428.52/s)  LR: 2.136e-05  Data: 0.011 (0.019)
Train: 559 [ 750/1251 ( 60%)]  Loss: 3.220 (3.16)  Time: 0.700s, 1463.86/s  (0.716s, 1431.03/s)  LR: 2.136e-05  Data: 0.010 (0.018)
Train: 559 [ 800/1251 ( 64%)]  Loss: 3.147 (3.16)  Time: 0.741s, 1382.77/s  (0.714s, 1434.01/s)  LR: 2.136e-05  Data: 0.010 (0.018)
Train: 559 [ 850/1251 ( 68%)]  Loss: 3.394 (3.17)  Time: 0.714s, 1433.84/s  (0.713s, 1437.14/s)  LR: 2.136e-05  Data: 0.009 (0.017)
Train: 559 [ 900/1251 ( 72%)]  Loss: 3.105 (3.17)  Time: 0.676s, 1515.43/s  (0.712s, 1439.14/s)  LR: 2.136e-05  Data: 0.010 (0.017)
Train: 559 [ 950/1251 ( 76%)]  Loss: 3.352 (3.18)  Time: 0.700s, 1463.87/s  (0.710s, 1441.29/s)  LR: 2.136e-05  Data: 0.009 (0.017)
Train: 559 [1000/1251 ( 80%)]  Loss: 3.242 (3.18)  Time: 0.676s, 1515.56/s  (0.710s, 1443.03/s)  LR: 2.136e-05  Data: 0.009 (0.016)
Train: 559 [1050/1251 ( 84%)]  Loss: 3.039 (3.17)  Time: 0.668s, 1532.49/s  (0.709s, 1445.12/s)  LR: 2.136e-05  Data: 0.009 (0.016)
Train: 559 [1100/1251 ( 88%)]  Loss: 3.499 (3.19)  Time: 0.698s, 1466.73/s  (0.708s, 1446.15/s)  LR: 2.136e-05  Data: 0.015 (0.016)
Train: 559 [1150/1251 ( 92%)]  Loss: 3.132 (3.19)  Time: 0.718s, 1427.18/s  (0.708s, 1446.67/s)  LR: 2.136e-05  Data: 0.008 (0.016)
Train: 559 [1200/1251 ( 96%)]  Loss: 2.976 (3.18)  Time: 0.686s, 1492.43/s  (0.709s, 1444.30/s)  LR: 2.136e-05  Data: 0.012 (0.016)
Train: 559 [1250/1251 (100%)]  Loss: 3.190 (3.18)  Time: 0.729s, 1405.50/s  (0.710s, 1442.49/s)  LR: 2.136e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.954 (1.954)  Loss:  0.7378 (0.7378)  Acc@1: 91.5039 (91.5039)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.136 (0.620)  Loss:  0.8364 (1.2156)  Acc@1: 86.9104 (78.9580)  Acc@5: 96.9340 (94.3900)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-558.pth.tar', 79.20199995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-557.pth.tar', 79.1480000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-553.pth.tar', 79.08400000488281)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-556.pth.tar', 79.04000010498046)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-552.pth.tar', 79.01200005371093)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-547.pth.tar', 79.00799992431641)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-543.pth.tar', 78.9680000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-559.pth.tar', 78.95800000244141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-548.pth.tar', 78.92400003173829)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-542.pth.tar', 78.9239999243164)

Train: 560 [   0/1251 (  0%)]  Loss: 2.951 (2.95)  Time: 2.175s,  470.84/s  (2.175s,  470.84/s)  LR: 2.082e-05  Data: 1.532 (1.532)
Train: 560 [  50/1251 (  4%)]  Loss: 3.209 (3.08)  Time: 0.697s, 1469.08/s  (0.732s, 1399.09/s)  LR: 2.082e-05  Data: 0.015 (0.049)
Train: 560 [ 100/1251 (  8%)]  Loss: 3.110 (3.09)  Time: 0.736s, 1391.33/s  (0.716s, 1429.19/s)  LR: 2.082e-05  Data: 0.010 (0.030)
Train: 560 [ 150/1251 ( 12%)]  Loss: 3.330 (3.15)  Time: 0.671s, 1525.41/s  (0.707s, 1448.59/s)  LR: 2.082e-05  Data: 0.011 (0.023)
Train: 560 [ 200/1251 ( 16%)]  Loss: 2.889 (3.10)  Time: 0.717s, 1428.92/s  (0.703s, 1455.69/s)  LR: 2.082e-05  Data: 0.009 (0.020)
Train: 560 [ 250/1251 ( 20%)]  Loss: 3.605 (3.18)  Time: 0.674s, 1520.30/s  (0.700s, 1462.62/s)  LR: 2.082e-05  Data: 0.011 (0.018)
Train: 560 [ 300/1251 ( 24%)]  Loss: 3.316 (3.20)  Time: 0.671s, 1525.01/s  (0.699s, 1465.94/s)  LR: 2.082e-05  Data: 0.011 (0.017)
Train: 560 [ 350/1251 ( 28%)]  Loss: 3.243 (3.21)  Time: 0.671s, 1525.42/s  (0.697s, 1469.98/s)  LR: 2.082e-05  Data: 0.011 (0.016)
Train: 560 [ 400/1251 ( 32%)]  Loss: 3.246 (3.21)  Time: 0.673s, 1520.53/s  (0.696s, 1470.89/s)  LR: 2.082e-05  Data: 0.011 (0.015)
Train: 560 [ 450/1251 ( 36%)]  Loss: 3.018 (3.19)  Time: 0.672s, 1522.96/s  (0.695s, 1472.44/s)  LR: 2.082e-05  Data: 0.011 (0.015)
Train: 560 [ 500/1251 ( 40%)]  Loss: 3.141 (3.19)  Time: 0.668s, 1533.03/s  (0.695s, 1473.67/s)  LR: 2.082e-05  Data: 0.011 (0.015)
Train: 560 [ 550/1251 ( 44%)]  Loss: 3.282 (3.20)  Time: 0.704s, 1454.07/s  (0.695s, 1473.86/s)  LR: 2.082e-05  Data: 0.011 (0.014)
Train: 560 [ 600/1251 ( 48%)]  Loss: 3.400 (3.21)  Time: 0.678s, 1511.24/s  (0.694s, 1474.99/s)  LR: 2.082e-05  Data: 0.014 (0.014)
Train: 560 [ 650/1251 ( 52%)]  Loss: 3.513 (3.23)  Time: 0.717s, 1427.76/s  (0.694s, 1474.88/s)  LR: 2.082e-05  Data: 0.010 (0.014)
Train: 560 [ 700/1251 ( 56%)]  Loss: 2.959 (3.21)  Time: 0.723s, 1416.53/s  (0.694s, 1475.71/s)  LR: 2.082e-05  Data: 0.010 (0.013)
Train: 560 [ 750/1251 ( 60%)]  Loss: 3.127 (3.21)  Time: 0.705s, 1451.91/s  (0.694s, 1475.81/s)  LR: 2.082e-05  Data: 0.014 (0.013)
Train: 560 [ 800/1251 ( 64%)]  Loss: 3.284 (3.21)  Time: 0.729s, 1405.60/s  (0.694s, 1476.42/s)  LR: 2.082e-05  Data: 0.011 (0.013)
Train: 560 [ 850/1251 ( 68%)]  Loss: 3.158 (3.21)  Time: 0.674s, 1520.33/s  (0.693s, 1476.78/s)  LR: 2.082e-05  Data: 0.011 (0.013)
Train: 560 [ 900/1251 ( 72%)]  Loss: 3.524 (3.23)  Time: 0.678s, 1511.37/s  (0.693s, 1477.08/s)  LR: 2.082e-05  Data: 0.012 (0.013)
Train: 560 [ 950/1251 ( 76%)]  Loss: 3.167 (3.22)  Time: 0.708s, 1446.18/s  (0.694s, 1475.91/s)  LR: 2.082e-05  Data: 0.018 (0.013)
Train: 560 [1000/1251 ( 80%)]  Loss: 3.312 (3.23)  Time: 0.673s, 1521.85/s  (0.694s, 1475.90/s)  LR: 2.082e-05  Data: 0.010 (0.013)
Train: 560 [1050/1251 ( 84%)]  Loss: 3.500 (3.24)  Time: 0.671s, 1526.04/s  (0.694s, 1476.38/s)  LR: 2.082e-05  Data: 0.011 (0.012)
Train: 560 [1100/1251 ( 88%)]  Loss: 2.949 (3.23)  Time: 0.673s, 1520.96/s  (0.694s, 1476.52/s)  LR: 2.082e-05  Data: 0.010 (0.012)
Train: 560 [1150/1251 ( 92%)]  Loss: 3.371 (3.23)  Time: 0.672s, 1524.22/s  (0.693s, 1476.86/s)  LR: 2.082e-05  Data: 0.011 (0.012)
Train: 560 [1200/1251 ( 96%)]  Loss: 2.817 (3.22)  Time: 0.694s, 1476.34/s  (0.693s, 1476.95/s)  LR: 2.082e-05  Data: 0.011 (0.012)
Train: 560 [1250/1251 (100%)]  Loss: 2.937 (3.21)  Time: 0.655s, 1563.12/s  (0.693s, 1477.29/s)  LR: 2.082e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.544 (1.544)  Loss:  0.7207 (0.7207)  Acc@1: 91.6016 (91.6016)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.136 (0.573)  Loss:  0.8218 (1.1764)  Acc@1: 87.6179 (79.0620)  Acc@5: 97.4057 (94.4160)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-558.pth.tar', 79.20199995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-557.pth.tar', 79.1480000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-553.pth.tar', 79.08400000488281)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-560.pth.tar', 79.06200005126954)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-556.pth.tar', 79.04000010498046)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-552.pth.tar', 79.01200005371093)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-547.pth.tar', 79.00799992431641)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-543.pth.tar', 78.9680000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-559.pth.tar', 78.95800000244141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-548.pth.tar', 78.92400003173829)

Train: 561 [   0/1251 (  0%)]  Loss: 2.814 (2.81)  Time: 2.197s,  466.14/s  (2.197s,  466.14/s)  LR: 2.028e-05  Data: 1.571 (1.571)
Train: 561 [  50/1251 (  4%)]  Loss: 3.394 (3.10)  Time: 0.671s, 1525.65/s  (0.730s, 1402.47/s)  LR: 2.028e-05  Data: 0.012 (0.051)
Train: 561 [ 100/1251 (  8%)]  Loss: 3.106 (3.11)  Time: 0.706s, 1451.14/s  (0.711s, 1441.23/s)  LR: 2.028e-05  Data: 0.010 (0.031)
Train: 561 [ 150/1251 ( 12%)]  Loss: 2.989 (3.08)  Time: 0.704s, 1455.45/s  (0.704s, 1455.48/s)  LR: 2.028e-05  Data: 0.009 (0.024)
Train: 561 [ 200/1251 ( 16%)]  Loss: 3.287 (3.12)  Time: 0.704s, 1455.53/s  (0.700s, 1463.28/s)  LR: 2.028e-05  Data: 0.009 (0.021)
Train: 561 [ 250/1251 ( 20%)]  Loss: 3.197 (3.13)  Time: 0.724s, 1413.58/s  (0.698s, 1466.95/s)  LR: 2.028e-05  Data: 0.010 (0.019)
Train: 561 [ 300/1251 ( 24%)]  Loss: 3.307 (3.16)  Time: 0.697s, 1468.80/s  (0.696s, 1470.40/s)  LR: 2.028e-05  Data: 0.013 (0.017)
Train: 561 [ 350/1251 ( 28%)]  Loss: 3.200 (3.16)  Time: 0.672s, 1524.89/s  (0.694s, 1474.95/s)  LR: 2.028e-05  Data: 0.010 (0.016)
Train: 561 [ 400/1251 ( 32%)]  Loss: 3.105 (3.16)  Time: 0.699s, 1464.10/s  (0.694s, 1476.42/s)  LR: 2.028e-05  Data: 0.010 (0.016)
Train: 561 [ 450/1251 ( 36%)]  Loss: 3.316 (3.17)  Time: 0.666s, 1536.49/s  (0.692s, 1479.12/s)  LR: 2.028e-05  Data: 0.010 (0.015)
Train: 561 [ 500/1251 ( 40%)]  Loss: 3.469 (3.20)  Time: 0.699s, 1464.91/s  (0.693s, 1477.14/s)  LR: 2.028e-05  Data: 0.009 (0.015)
Train: 561 [ 550/1251 ( 44%)]  Loss: 2.832 (3.17)  Time: 0.701s, 1460.16/s  (0.693s, 1477.32/s)  LR: 2.028e-05  Data: 0.010 (0.014)
Train: 561 [ 600/1251 ( 48%)]  Loss: 2.803 (3.14)  Time: 0.767s, 1334.34/s  (0.693s, 1477.08/s)  LR: 2.028e-05  Data: 0.012 (0.014)
Train: 561 [ 650/1251 ( 52%)]  Loss: 3.140 (3.14)  Time: 0.670s, 1527.70/s  (0.693s, 1477.04/s)  LR: 2.028e-05  Data: 0.010 (0.014)
Train: 561 [ 700/1251 ( 56%)]  Loss: 3.083 (3.14)  Time: 0.702s, 1457.93/s  (0.693s, 1477.93/s)  LR: 2.028e-05  Data: 0.010 (0.014)
Train: 561 [ 750/1251 ( 60%)]  Loss: 3.116 (3.14)  Time: 0.703s, 1457.46/s  (0.693s, 1477.84/s)  LR: 2.028e-05  Data: 0.009 (0.013)
Train: 561 [ 800/1251 ( 64%)]  Loss: 3.436 (3.15)  Time: 0.674s, 1518.39/s  (0.692s, 1479.03/s)  LR: 2.028e-05  Data: 0.011 (0.013)
Train: 561 [ 850/1251 ( 68%)]  Loss: 3.148 (3.15)  Time: 0.704s, 1454.88/s  (0.692s, 1479.00/s)  LR: 2.028e-05  Data: 0.009 (0.013)
Train: 561 [ 900/1251 ( 72%)]  Loss: 3.219 (3.16)  Time: 0.666s, 1537.16/s  (0.692s, 1479.91/s)  LR: 2.028e-05  Data: 0.010 (0.013)
Train: 561 [ 950/1251 ( 76%)]  Loss: 3.261 (3.16)  Time: 0.705s, 1452.68/s  (0.692s, 1480.67/s)  LR: 2.028e-05  Data: 0.009 (0.013)
Train: 561 [1000/1251 ( 80%)]  Loss: 3.407 (3.17)  Time: 0.672s, 1522.79/s  (0.691s, 1481.06/s)  LR: 2.028e-05  Data: 0.011 (0.013)
Train: 561 [1050/1251 ( 84%)]  Loss: 3.375 (3.18)  Time: 0.672s, 1524.36/s  (0.691s, 1481.25/s)  LR: 2.028e-05  Data: 0.010 (0.013)
Train: 561 [1100/1251 ( 88%)]  Loss: 3.265 (3.19)  Time: 0.674s, 1519.42/s  (0.691s, 1481.11/s)  LR: 2.028e-05  Data: 0.010 (0.012)
Train: 561 [1150/1251 ( 92%)]  Loss: 3.153 (3.18)  Time: 0.694s, 1475.46/s  (0.692s, 1480.79/s)  LR: 2.028e-05  Data: 0.010 (0.012)
Train: 561 [1200/1251 ( 96%)]  Loss: 2.982 (3.18)  Time: 0.675s, 1516.10/s  (0.692s, 1480.63/s)  LR: 2.028e-05  Data: 0.009 (0.012)
Train: 561 [1250/1251 (100%)]  Loss: 3.262 (3.18)  Time: 0.657s, 1558.58/s  (0.691s, 1481.20/s)  LR: 2.028e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.573 (1.573)  Loss:  0.7471 (0.7471)  Acc@1: 91.6016 (91.6016)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.137 (0.579)  Loss:  0.8340 (1.2238)  Acc@1: 87.6179 (79.0760)  Acc@5: 97.1698 (94.4200)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-558.pth.tar', 79.20199995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-557.pth.tar', 79.1480000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-553.pth.tar', 79.08400000488281)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-561.pth.tar', 79.075999921875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-560.pth.tar', 79.06200005126954)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-556.pth.tar', 79.04000010498046)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-552.pth.tar', 79.01200005371093)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-547.pth.tar', 79.00799992431641)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-543.pth.tar', 78.9680000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-559.pth.tar', 78.95800000244141)

Train: 562 [   0/1251 (  0%)]  Loss: 3.023 (3.02)  Time: 2.331s,  439.31/s  (2.331s,  439.31/s)  LR: 1.977e-05  Data: 1.715 (1.715)
Train: 562 [  50/1251 (  4%)]  Loss: 3.144 (3.08)  Time: 0.711s, 1440.88/s  (0.728s, 1405.83/s)  LR: 1.977e-05  Data: 0.011 (0.050)
Train: 562 [ 100/1251 (  8%)]  Loss: 3.197 (3.12)  Time: 0.745s, 1373.85/s  (0.713s, 1435.29/s)  LR: 1.977e-05  Data: 0.009 (0.030)
Train: 562 [ 150/1251 ( 12%)]  Loss: 3.192 (3.14)  Time: 0.742s, 1380.70/s  (0.707s, 1448.85/s)  LR: 1.977e-05  Data: 0.009 (0.024)
Train: 562 [ 200/1251 ( 16%)]  Loss: 3.379 (3.19)  Time: 0.701s, 1460.39/s  (0.705s, 1451.69/s)  LR: 1.977e-05  Data: 0.010 (0.021)
Train: 562 [ 250/1251 ( 20%)]  Loss: 3.487 (3.24)  Time: 0.705s, 1452.81/s  (0.702s, 1459.42/s)  LR: 1.977e-05  Data: 0.012 (0.019)
Train: 562 [ 300/1251 ( 24%)]  Loss: 3.488 (3.27)  Time: 0.716s, 1430.41/s  (0.700s, 1462.82/s)  LR: 1.977e-05  Data: 0.010 (0.017)
Train: 562 [ 350/1251 ( 28%)]  Loss: 3.034 (3.24)  Time: 0.673s, 1521.92/s  (0.699s, 1465.75/s)  LR: 1.977e-05  Data: 0.010 (0.016)
Train: 562 [ 400/1251 ( 32%)]  Loss: 3.471 (3.27)  Time: 0.673s, 1522.33/s  (0.698s, 1466.74/s)  LR: 1.977e-05  Data: 0.011 (0.016)
Train: 562 [ 450/1251 ( 36%)]  Loss: 3.061 (3.25)  Time: 0.740s, 1384.50/s  (0.697s, 1468.48/s)  LR: 1.977e-05  Data: 0.009 (0.015)
Train: 562 [ 500/1251 ( 40%)]  Loss: 3.194 (3.24)  Time: 0.693s, 1477.25/s  (0.697s, 1468.51/s)  LR: 1.977e-05  Data: 0.011 (0.015)
Train: 562 [ 550/1251 ( 44%)]  Loss: 3.304 (3.25)  Time: 0.674s, 1518.76/s  (0.696s, 1470.32/s)  LR: 1.977e-05  Data: 0.011 (0.014)
Train: 562 [ 600/1251 ( 48%)]  Loss: 3.528 (3.27)  Time: 0.700s, 1462.16/s  (0.696s, 1471.00/s)  LR: 1.977e-05  Data: 0.010 (0.014)
Train: 562 [ 650/1251 ( 52%)]  Loss: 3.142 (3.26)  Time: 0.698s, 1467.64/s  (0.696s, 1472.08/s)  LR: 1.977e-05  Data: 0.010 (0.014)
Train: 562 [ 700/1251 ( 56%)]  Loss: 3.661 (3.29)  Time: 0.683s, 1498.87/s  (0.695s, 1472.79/s)  LR: 1.977e-05  Data: 0.014 (0.013)
Train: 562 [ 750/1251 ( 60%)]  Loss: 3.077 (3.27)  Time: 0.703s, 1456.76/s  (0.695s, 1472.35/s)  LR: 1.977e-05  Data: 0.010 (0.013)
Train: 562 [ 800/1251 ( 64%)]  Loss: 2.840 (3.25)  Time: 0.679s, 1507.93/s  (0.696s, 1471.85/s)  LR: 1.977e-05  Data: 0.010 (0.013)
Train: 562 [ 850/1251 ( 68%)]  Loss: 3.077 (3.24)  Time: 0.703s, 1457.23/s  (0.695s, 1472.89/s)  LR: 1.977e-05  Data: 0.011 (0.013)
Train: 562 [ 900/1251 ( 72%)]  Loss: 3.319 (3.24)  Time: 0.676s, 1514.37/s  (0.695s, 1473.75/s)  LR: 1.977e-05  Data: 0.011 (0.013)
Train: 562 [ 950/1251 ( 76%)]  Loss: 3.378 (3.25)  Time: 0.681s, 1504.68/s  (0.695s, 1474.08/s)  LR: 1.977e-05  Data: 0.009 (0.013)
Train: 562 [1000/1251 ( 80%)]  Loss: 2.952 (3.24)  Time: 0.742s, 1379.17/s  (0.694s, 1474.56/s)  LR: 1.977e-05  Data: 0.010 (0.013)
Train: 562 [1050/1251 ( 84%)]  Loss: 2.742 (3.21)  Time: 0.671s, 1525.98/s  (0.694s, 1475.10/s)  LR: 1.977e-05  Data: 0.009 (0.012)
Train: 562 [1100/1251 ( 88%)]  Loss: 3.541 (3.23)  Time: 0.669s, 1530.61/s  (0.694s, 1475.27/s)  LR: 1.977e-05  Data: 0.010 (0.012)
Train: 562 [1150/1251 ( 92%)]  Loss: 3.095 (3.22)  Time: 0.724s, 1414.04/s  (0.694s, 1475.54/s)  LR: 1.977e-05  Data: 0.010 (0.012)
Train: 562 [1200/1251 ( 96%)]  Loss: 3.129 (3.22)  Time: 0.696s, 1471.70/s  (0.694s, 1475.97/s)  LR: 1.977e-05  Data: 0.012 (0.012)
Train: 562 [1250/1251 (100%)]  Loss: 3.359 (3.22)  Time: 0.692s, 1479.88/s  (0.694s, 1475.86/s)  LR: 1.977e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.495 (1.495)  Loss:  0.7617 (0.7617)  Acc@1: 91.5039 (91.5039)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.137 (0.575)  Loss:  0.8564 (1.2583)  Acc@1: 86.6745 (78.8920)  Acc@5: 97.2877 (94.3540)
Train: 563 [   0/1251 (  0%)]  Loss: 3.282 (3.28)  Time: 2.150s,  476.28/s  (2.150s,  476.28/s)  LR: 1.926e-05  Data: 1.532 (1.532)
Train: 563 [  50/1251 (  4%)]  Loss: 2.995 (3.14)  Time: 0.700s, 1463.33/s  (0.732s, 1398.61/s)  LR: 1.926e-05  Data: 0.009 (0.048)
Train: 563 [ 100/1251 (  8%)]  Loss: 3.278 (3.18)  Time: 0.670s, 1528.43/s  (0.713s, 1436.15/s)  LR: 1.926e-05  Data: 0.010 (0.029)
Train: 563 [ 150/1251 ( 12%)]  Loss: 3.033 (3.15)  Time: 0.724s, 1414.75/s  (0.706s, 1451.17/s)  LR: 1.926e-05  Data: 0.022 (0.023)
Train: 563 [ 200/1251 ( 16%)]  Loss: 3.148 (3.15)  Time: 0.673s, 1521.59/s  (0.702s, 1459.08/s)  LR: 1.926e-05  Data: 0.010 (0.020)
Train: 563 [ 250/1251 ( 20%)]  Loss: 3.271 (3.17)  Time: 0.671s, 1526.01/s  (0.699s, 1465.65/s)  LR: 1.926e-05  Data: 0.011 (0.018)
Train: 563 [ 300/1251 ( 24%)]  Loss: 3.041 (3.15)  Time: 0.670s, 1527.27/s  (0.697s, 1468.97/s)  LR: 1.926e-05  Data: 0.010 (0.017)
Train: 563 [ 350/1251 ( 28%)]  Loss: 3.086 (3.14)  Time: 0.733s, 1397.79/s  (0.696s, 1471.09/s)  LR: 1.926e-05  Data: 0.010 (0.016)
Train: 563 [ 400/1251 ( 32%)]  Loss: 3.019 (3.13)  Time: 0.683s, 1499.05/s  (0.696s, 1470.47/s)  LR: 1.926e-05  Data: 0.016 (0.015)
Train: 563 [ 450/1251 ( 36%)]  Loss: 3.342 (3.15)  Time: 0.715s, 1432.40/s  (0.697s, 1470.07/s)  LR: 1.926e-05  Data: 0.010 (0.015)
Train: 563 [ 500/1251 ( 40%)]  Loss: 3.577 (3.19)  Time: 0.670s, 1527.52/s  (0.696s, 1472.10/s)  LR: 1.926e-05  Data: 0.012 (0.014)
Train: 563 [ 550/1251 ( 44%)]  Loss: 3.020 (3.17)  Time: 0.705s, 1451.58/s  (0.695s, 1473.51/s)  LR: 1.926e-05  Data: 0.010 (0.014)
Train: 563 [ 600/1251 ( 48%)]  Loss: 3.129 (3.17)  Time: 0.671s, 1526.19/s  (0.694s, 1474.61/s)  LR: 1.926e-05  Data: 0.010 (0.014)
Train: 563 [ 650/1251 ( 52%)]  Loss: 3.730 (3.21)  Time: 0.688s, 1487.93/s  (0.694s, 1474.48/s)  LR: 1.926e-05  Data: 0.011 (0.013)
Train: 563 [ 700/1251 ( 56%)]  Loss: 2.782 (3.18)  Time: 0.695s, 1474.18/s  (0.694s, 1474.95/s)  LR: 1.926e-05  Data: 0.009 (0.013)
Train: 563 [ 750/1251 ( 60%)]  Loss: 3.223 (3.18)  Time: 0.776s, 1320.12/s  (0.694s, 1476.21/s)  LR: 1.926e-05  Data: 0.010 (0.013)
Train: 563 [ 800/1251 ( 64%)]  Loss: 3.081 (3.18)  Time: 0.671s, 1526.30/s  (0.693s, 1477.17/s)  LR: 1.926e-05  Data: 0.011 (0.013)
Train: 563 [ 850/1251 ( 68%)]  Loss: 2.780 (3.16)  Time: 0.669s, 1531.25/s  (0.693s, 1477.75/s)  LR: 1.926e-05  Data: 0.009 (0.013)
Train: 563 [ 900/1251 ( 72%)]  Loss: 3.387 (3.17)  Time: 0.670s, 1527.98/s  (0.692s, 1479.15/s)  LR: 1.926e-05  Data: 0.010 (0.013)
Train: 563 [ 950/1251 ( 76%)]  Loss: 3.220 (3.17)  Time: 0.671s, 1525.15/s  (0.692s, 1479.23/s)  LR: 1.926e-05  Data: 0.013 (0.012)
Train: 563 [1000/1251 ( 80%)]  Loss: 3.033 (3.16)  Time: 0.674s, 1518.62/s  (0.692s, 1479.00/s)  LR: 1.926e-05  Data: 0.012 (0.012)
Train: 563 [1050/1251 ( 84%)]  Loss: 3.712 (3.19)  Time: 0.709s, 1445.25/s  (0.692s, 1479.23/s)  LR: 1.926e-05  Data: 0.010 (0.012)
Train: 563 [1100/1251 ( 88%)]  Loss: 3.006 (3.18)  Time: 0.706s, 1450.01/s  (0.692s, 1479.13/s)  LR: 1.926e-05  Data: 0.011 (0.012)
Train: 563 [1150/1251 ( 92%)]  Loss: 3.277 (3.19)  Time: 0.701s, 1461.22/s  (0.692s, 1479.65/s)  LR: 1.926e-05  Data: 0.010 (0.012)
Train: 563 [1200/1251 ( 96%)]  Loss: 2.911 (3.17)  Time: 0.670s, 1528.75/s  (0.692s, 1479.91/s)  LR: 1.926e-05  Data: 0.010 (0.012)
Train: 563 [1250/1251 (100%)]  Loss: 3.249 (3.18)  Time: 0.658s, 1557.41/s  (0.692s, 1479.99/s)  LR: 1.926e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.619 (1.619)  Loss:  0.6982 (0.6982)  Acc@1: 91.7969 (91.7969)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.136 (0.578)  Loss:  0.8091 (1.1903)  Acc@1: 87.1462 (79.0260)  Acc@5: 97.0519 (94.4380)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-558.pth.tar', 79.20199995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-557.pth.tar', 79.1480000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-553.pth.tar', 79.08400000488281)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-561.pth.tar', 79.075999921875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-560.pth.tar', 79.06200005126954)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-556.pth.tar', 79.04000010498046)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-563.pth.tar', 79.02599997558593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-552.pth.tar', 79.01200005371093)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-547.pth.tar', 79.00799992431641)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-543.pth.tar', 78.9680000024414)

Train: 564 [   0/1251 (  0%)]  Loss: 3.033 (3.03)  Time: 2.122s,  482.64/s  (2.122s,  482.64/s)  LR: 1.877e-05  Data: 1.483 (1.483)
Train: 564 [  50/1251 (  4%)]  Loss: 3.320 (3.18)  Time: 0.674s, 1519.52/s  (0.718s, 1425.79/s)  LR: 1.877e-05  Data: 0.009 (0.050)
Train: 564 [ 100/1251 (  8%)]  Loss: 3.342 (3.23)  Time: 0.670s, 1529.18/s  (0.705s, 1452.64/s)  LR: 1.877e-05  Data: 0.012 (0.030)
Train: 564 [ 150/1251 ( 12%)]  Loss: 2.842 (3.13)  Time: 0.698s, 1466.63/s  (0.699s, 1464.73/s)  LR: 1.877e-05  Data: 0.011 (0.024)
Train: 564 [ 200/1251 ( 16%)]  Loss: 2.933 (3.09)  Time: 0.675s, 1516.11/s  (0.697s, 1469.16/s)  LR: 1.877e-05  Data: 0.010 (0.020)
Train: 564 [ 250/1251 ( 20%)]  Loss: 3.345 (3.14)  Time: 0.672s, 1524.85/s  (0.695s, 1473.16/s)  LR: 1.877e-05  Data: 0.012 (0.018)
Train: 564 [ 300/1251 ( 24%)]  Loss: 2.978 (3.11)  Time: 0.672s, 1523.18/s  (0.695s, 1473.81/s)  LR: 1.877e-05  Data: 0.011 (0.017)
Train: 564 [ 350/1251 ( 28%)]  Loss: 2.738 (3.07)  Time: 0.676s, 1515.06/s  (0.694s, 1475.42/s)  LR: 1.877e-05  Data: 0.014 (0.016)
Train: 564 [ 400/1251 ( 32%)]  Loss: 3.170 (3.08)  Time: 0.668s, 1532.68/s  (0.694s, 1475.64/s)  LR: 1.877e-05  Data: 0.012 (0.015)
Train: 564 [ 450/1251 ( 36%)]  Loss: 3.229 (3.09)  Time: 0.725s, 1413.13/s  (0.694s, 1476.52/s)  LR: 1.877e-05  Data: 0.009 (0.015)
Train: 564 [ 500/1251 ( 40%)]  Loss: 3.008 (3.09)  Time: 0.680s, 1506.29/s  (0.693s, 1477.81/s)  LR: 1.877e-05  Data: 0.011 (0.014)
Train: 564 [ 550/1251 ( 44%)]  Loss: 3.160 (3.09)  Time: 0.666s, 1537.89/s  (0.693s, 1478.35/s)  LR: 1.877e-05  Data: 0.010 (0.014)
Train: 564 [ 600/1251 ( 48%)]  Loss: 3.272 (3.11)  Time: 0.671s, 1526.62/s  (0.693s, 1478.42/s)  LR: 1.877e-05  Data: 0.011 (0.014)
Train: 564 [ 650/1251 ( 52%)]  Loss: 3.322 (3.12)  Time: 0.710s, 1441.76/s  (0.693s, 1477.67/s)  LR: 1.877e-05  Data: 0.010 (0.014)
Train: 564 [ 700/1251 ( 56%)]  Loss: 3.326 (3.13)  Time: 0.756s, 1354.24/s  (0.693s, 1477.89/s)  LR: 1.877e-05  Data: 0.011 (0.013)
Train: 564 [ 750/1251 ( 60%)]  Loss: 3.013 (3.13)  Time: 0.691s, 1481.14/s  (0.693s, 1477.16/s)  LR: 1.877e-05  Data: 0.011 (0.013)
Train: 564 [ 800/1251 ( 64%)]  Loss: 3.282 (3.14)  Time: 0.671s, 1524.96/s  (0.693s, 1477.38/s)  LR: 1.877e-05  Data: 0.011 (0.013)
Train: 564 [ 850/1251 ( 68%)]  Loss: 3.265 (3.14)  Time: 0.688s, 1488.93/s  (0.693s, 1478.16/s)  LR: 1.877e-05  Data: 0.010 (0.013)
Train: 564 [ 900/1251 ( 72%)]  Loss: 3.064 (3.14)  Time: 0.672s, 1522.70/s  (0.693s, 1478.52/s)  LR: 1.877e-05  Data: 0.011 (0.013)
Train: 564 [ 950/1251 ( 76%)]  Loss: 2.921 (3.13)  Time: 0.674s, 1519.97/s  (0.692s, 1478.83/s)  LR: 1.877e-05  Data: 0.010 (0.013)
Train: 564 [1000/1251 ( 80%)]  Loss: 3.075 (3.13)  Time: 0.697s, 1470.19/s  (0.693s, 1478.63/s)  LR: 1.877e-05  Data: 0.010 (0.013)
Train: 564 [1050/1251 ( 84%)]  Loss: 2.948 (3.12)  Time: 0.683s, 1498.90/s  (0.693s, 1478.11/s)  LR: 1.877e-05  Data: 0.014 (0.012)
Train: 564 [1100/1251 ( 88%)]  Loss: 3.219 (3.12)  Time: 0.673s, 1522.64/s  (0.693s, 1478.13/s)  LR: 1.877e-05  Data: 0.011 (0.012)
Train: 564 [1150/1251 ( 92%)]  Loss: 3.141 (3.12)  Time: 0.710s, 1441.45/s  (0.693s, 1478.29/s)  LR: 1.877e-05  Data: 0.011 (0.012)
Train: 564 [1200/1251 ( 96%)]  Loss: 2.983 (3.12)  Time: 0.672s, 1524.51/s  (0.693s, 1478.20/s)  LR: 1.877e-05  Data: 0.010 (0.012)
Train: 564 [1250/1251 (100%)]  Loss: 2.945 (3.11)  Time: 0.667s, 1534.97/s  (0.693s, 1478.26/s)  LR: 1.877e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.593 (1.593)  Loss:  0.7495 (0.7495)  Acc@1: 91.1133 (91.1133)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.136 (0.578)  Loss:  0.8374 (1.2247)  Acc@1: 86.6745 (78.9700)  Acc@5: 96.9340 (94.4780)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-558.pth.tar', 79.20199995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-557.pth.tar', 79.1480000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-553.pth.tar', 79.08400000488281)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-561.pth.tar', 79.075999921875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-560.pth.tar', 79.06200005126954)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-556.pth.tar', 79.04000010498046)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-563.pth.tar', 79.02599997558593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-552.pth.tar', 79.01200005371093)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-547.pth.tar', 79.00799992431641)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-564.pth.tar', 78.97000002929687)

Train: 565 [   0/1251 (  0%)]  Loss: 2.995 (2.99)  Time: 2.182s,  469.30/s  (2.182s,  469.30/s)  LR: 1.829e-05  Data: 1.563 (1.563)
Train: 565 [  50/1251 (  4%)]  Loss: 3.105 (3.05)  Time: 0.711s, 1441.00/s  (0.730s, 1401.81/s)  LR: 1.829e-05  Data: 0.016 (0.048)
Train: 565 [ 100/1251 (  8%)]  Loss: 3.496 (3.20)  Time: 0.681s, 1502.70/s  (0.714s, 1434.00/s)  LR: 1.829e-05  Data: 0.014 (0.029)
Train: 565 [ 150/1251 ( 12%)]  Loss: 3.354 (3.24)  Time: 0.673s, 1522.66/s  (0.706s, 1450.08/s)  LR: 1.829e-05  Data: 0.010 (0.023)
Train: 565 [ 200/1251 ( 16%)]  Loss: 3.793 (3.35)  Time: 0.671s, 1525.60/s  (0.703s, 1457.25/s)  LR: 1.829e-05  Data: 0.011 (0.020)
Train: 565 [ 250/1251 ( 20%)]  Loss: 3.134 (3.31)  Time: 0.704s, 1453.52/s  (0.701s, 1460.27/s)  LR: 1.829e-05  Data: 0.009 (0.018)
Train: 565 [ 300/1251 ( 24%)]  Loss: 3.078 (3.28)  Time: 0.672s, 1523.94/s  (0.700s, 1462.07/s)  LR: 1.829e-05  Data: 0.012 (0.017)
Train: 565 [ 350/1251 ( 28%)]  Loss: 3.379 (3.29)  Time: 0.711s, 1439.99/s  (0.700s, 1462.24/s)  LR: 1.829e-05  Data: 0.013 (0.016)
Train: 565 [ 400/1251 ( 32%)]  Loss: 3.306 (3.29)  Time: 0.667s, 1534.40/s  (0.700s, 1463.28/s)  LR: 1.829e-05  Data: 0.011 (0.015)
Train: 565 [ 450/1251 ( 36%)]  Loss: 2.989 (3.26)  Time: 0.670s, 1528.41/s  (0.699s, 1465.36/s)  LR: 1.829e-05  Data: 0.010 (0.015)
Train: 565 [ 500/1251 ( 40%)]  Loss: 2.897 (3.23)  Time: 0.666s, 1537.85/s  (0.697s, 1468.24/s)  LR: 1.829e-05  Data: 0.010 (0.014)
Train: 565 [ 550/1251 ( 44%)]  Loss: 3.293 (3.23)  Time: 0.689s, 1486.72/s  (0.698s, 1467.92/s)  LR: 1.829e-05  Data: 0.013 (0.014)
Train: 565 [ 600/1251 ( 48%)]  Loss: 3.102 (3.22)  Time: 0.791s, 1294.74/s  (0.697s, 1468.34/s)  LR: 1.829e-05  Data: 0.010 (0.014)
Train: 565 [ 650/1251 ( 52%)]  Loss: 3.180 (3.22)  Time: 0.666s, 1538.32/s  (0.697s, 1469.48/s)  LR: 1.829e-05  Data: 0.010 (0.014)
Train: 565 [ 700/1251 ( 56%)]  Loss: 3.373 (3.23)  Time: 0.677s, 1513.32/s  (0.697s, 1469.89/s)  LR: 1.829e-05  Data: 0.013 (0.013)
Train: 565 [ 750/1251 ( 60%)]  Loss: 3.034 (3.22)  Time: 0.675s, 1516.82/s  (0.696s, 1471.24/s)  LR: 1.829e-05  Data: 0.009 (0.013)
Train: 565 [ 800/1251 ( 64%)]  Loss: 3.053 (3.21)  Time: 0.705s, 1452.88/s  (0.695s, 1472.32/s)  LR: 1.829e-05  Data: 0.009 (0.013)
Train: 565 [ 850/1251 ( 68%)]  Loss: 3.256 (3.21)  Time: 0.666s, 1537.90/s  (0.695s, 1473.33/s)  LR: 1.829e-05  Data: 0.010 (0.013)
Train: 565 [ 900/1251 ( 72%)]  Loss: 3.352 (3.22)  Time: 0.679s, 1508.88/s  (0.695s, 1474.01/s)  LR: 1.829e-05  Data: 0.011 (0.013)
Train: 565 [ 950/1251 ( 76%)]  Loss: 2.701 (3.19)  Time: 0.706s, 1450.31/s  (0.695s, 1473.94/s)  LR: 1.829e-05  Data: 0.011 (0.013)
Train: 565 [1000/1251 ( 80%)]  Loss: 3.277 (3.20)  Time: 0.744s, 1377.15/s  (0.695s, 1473.16/s)  LR: 1.829e-05  Data: 0.012 (0.012)
Train: 565 [1050/1251 ( 84%)]  Loss: 2.937 (3.19)  Time: 0.712s, 1438.89/s  (0.695s, 1472.89/s)  LR: 1.829e-05  Data: 0.012 (0.012)
Train: 565 [1100/1251 ( 88%)]  Loss: 3.175 (3.19)  Time: 0.693s, 1478.20/s  (0.695s, 1473.61/s)  LR: 1.829e-05  Data: 0.011 (0.012)
Train: 565 [1150/1251 ( 92%)]  Loss: 3.339 (3.19)  Time: 0.681s, 1503.49/s  (0.695s, 1474.36/s)  LR: 1.829e-05  Data: 0.011 (0.012)
Train: 565 [1200/1251 ( 96%)]  Loss: 3.278 (3.20)  Time: 0.725s, 1412.17/s  (0.695s, 1474.41/s)  LR: 1.829e-05  Data: 0.009 (0.012)
Train: 565 [1250/1251 (100%)]  Loss: 3.124 (3.19)  Time: 0.655s, 1564.46/s  (0.694s, 1474.82/s)  LR: 1.829e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.581 (1.581)  Loss:  0.7383 (0.7383)  Acc@1: 91.8945 (91.8945)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.136 (0.578)  Loss:  0.8291 (1.2034)  Acc@1: 87.3821 (78.9880)  Acc@5: 97.4057 (94.3960)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-558.pth.tar', 79.20199995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-557.pth.tar', 79.1480000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-553.pth.tar', 79.08400000488281)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-561.pth.tar', 79.075999921875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-560.pth.tar', 79.06200005126954)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-556.pth.tar', 79.04000010498046)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-563.pth.tar', 79.02599997558593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-552.pth.tar', 79.01200005371093)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-547.pth.tar', 79.00799992431641)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-565.pth.tar', 78.988000078125)

Train: 566 [   0/1251 (  0%)]  Loss: 3.244 (3.24)  Time: 2.113s,  484.73/s  (2.113s,  484.73/s)  LR: 1.782e-05  Data: 1.497 (1.497)
Train: 566 [  50/1251 (  4%)]  Loss: 3.548 (3.40)  Time: 0.671s, 1525.58/s  (0.726s, 1409.51/s)  LR: 1.782e-05  Data: 0.010 (0.048)
Train: 566 [ 100/1251 (  8%)]  Loss: 3.165 (3.32)  Time: 0.671s, 1526.26/s  (0.709s, 1444.68/s)  LR: 1.782e-05  Data: 0.010 (0.029)
Train: 566 [ 150/1251 ( 12%)]  Loss: 3.377 (3.33)  Time: 0.707s, 1448.64/s  (0.705s, 1451.97/s)  LR: 1.782e-05  Data: 0.010 (0.023)
Train: 566 [ 200/1251 ( 16%)]  Loss: 3.476 (3.36)  Time: 0.675s, 1517.51/s  (0.700s, 1462.90/s)  LR: 1.782e-05  Data: 0.011 (0.020)
Train: 566 [ 250/1251 ( 20%)]  Loss: 3.227 (3.34)  Time: 0.673s, 1521.57/s  (0.700s, 1463.62/s)  LR: 1.782e-05  Data: 0.010 (0.018)
Train: 566 [ 300/1251 ( 24%)]  Loss: 3.125 (3.31)  Time: 0.695s, 1473.82/s  (0.698s, 1466.46/s)  LR: 1.782e-05  Data: 0.010 (0.017)
Train: 566 [ 350/1251 ( 28%)]  Loss: 3.432 (3.32)  Time: 0.716s, 1430.53/s  (0.697s, 1469.74/s)  LR: 1.782e-05  Data: 0.010 (0.016)
Train: 566 [ 400/1251 ( 32%)]  Loss: 3.063 (3.30)  Time: 0.671s, 1525.24/s  (0.696s, 1471.60/s)  LR: 1.782e-05  Data: 0.011 (0.015)
Train: 566 [ 450/1251 ( 36%)]  Loss: 2.847 (3.25)  Time: 0.716s, 1430.79/s  (0.696s, 1471.01/s)  LR: 1.782e-05  Data: 0.009 (0.015)
Train: 566 [ 500/1251 ( 40%)]  Loss: 3.249 (3.25)  Time: 0.706s, 1449.96/s  (0.696s, 1471.68/s)  LR: 1.782e-05  Data: 0.010 (0.014)
Train: 566 [ 550/1251 ( 44%)]  Loss: 2.993 (3.23)  Time: 0.703s, 1455.75/s  (0.695s, 1472.82/s)  LR: 1.782e-05  Data: 0.010 (0.014)
Train: 566 [ 600/1251 ( 48%)]  Loss: 3.125 (3.22)  Time: 0.668s, 1533.55/s  (0.695s, 1473.53/s)  LR: 1.782e-05  Data: 0.009 (0.014)
Train: 566 [ 650/1251 ( 52%)]  Loss: 3.120 (3.21)  Time: 0.673s, 1520.85/s  (0.694s, 1474.61/s)  LR: 1.782e-05  Data: 0.011 (0.013)
Train: 566 [ 700/1251 ( 56%)]  Loss: 3.582 (3.24)  Time: 0.755s, 1355.75/s  (0.696s, 1472.14/s)  LR: 1.782e-05  Data: 0.015 (0.013)
Train: 566 [ 750/1251 ( 60%)]  Loss: 3.423 (3.25)  Time: 0.724s, 1413.97/s  (0.697s, 1469.73/s)  LR: 1.782e-05  Data: 0.013 (0.013)
Train: 566 [ 800/1251 ( 64%)]  Loss: 3.272 (3.25)  Time: 0.726s, 1409.95/s  (0.698s, 1466.51/s)  LR: 1.782e-05  Data: 0.009 (0.013)
Train: 566 [ 850/1251 ( 68%)]  Loss: 3.180 (3.25)  Time: 0.674s, 1519.52/s  (0.698s, 1467.17/s)  LR: 1.782e-05  Data: 0.011 (0.013)
Train: 566 [ 900/1251 ( 72%)]  Loss: 3.423 (3.26)  Time: 0.680s, 1506.09/s  (0.698s, 1467.85/s)  LR: 1.782e-05  Data: 0.010 (0.013)
Train: 566 [ 950/1251 ( 76%)]  Loss: 3.458 (3.27)  Time: 0.670s, 1528.25/s  (0.697s, 1469.51/s)  LR: 1.782e-05  Data: 0.011 (0.013)
Train: 566 [1000/1251 ( 80%)]  Loss: 2.884 (3.25)  Time: 0.682s, 1501.10/s  (0.697s, 1469.43/s)  LR: 1.782e-05  Data: 0.011 (0.013)
Train: 566 [1050/1251 ( 84%)]  Loss: 3.131 (3.24)  Time: 0.681s, 1502.58/s  (0.697s, 1469.68/s)  LR: 1.782e-05  Data: 0.011 (0.013)
Train: 566 [1100/1251 ( 88%)]  Loss: 3.051 (3.23)  Time: 0.670s, 1528.17/s  (0.697s, 1469.69/s)  LR: 1.782e-05  Data: 0.010 (0.012)
Train: 566 [1150/1251 ( 92%)]  Loss: 3.089 (3.23)  Time: 0.690s, 1484.96/s  (0.696s, 1470.37/s)  LR: 1.782e-05  Data: 0.009 (0.012)
Train: 566 [1200/1251 ( 96%)]  Loss: 2.849 (3.21)  Time: 0.672s, 1523.46/s  (0.696s, 1470.54/s)  LR: 1.782e-05  Data: 0.011 (0.012)
Train: 566 [1250/1251 (100%)]  Loss: 3.319 (3.22)  Time: 0.656s, 1560.67/s  (0.696s, 1470.64/s)  LR: 1.782e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.587 (1.587)  Loss:  0.7383 (0.7383)  Acc@1: 91.7969 (91.7969)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.135 (0.575)  Loss:  0.8394 (1.2137)  Acc@1: 87.1462 (79.0580)  Acc@5: 97.4057 (94.4280)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-558.pth.tar', 79.20199995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-557.pth.tar', 79.1480000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-553.pth.tar', 79.08400000488281)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-561.pth.tar', 79.075999921875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-560.pth.tar', 79.06200005126954)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-566.pth.tar', 79.05799997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-556.pth.tar', 79.04000010498046)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-563.pth.tar', 79.02599997558593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-552.pth.tar', 79.01200005371093)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-547.pth.tar', 79.00799992431641)

Train: 567 [   0/1251 (  0%)]  Loss: 3.458 (3.46)  Time: 2.204s,  464.66/s  (2.204s,  464.66/s)  LR: 1.737e-05  Data: 1.589 (1.589)
Train: 567 [  50/1251 (  4%)]  Loss: 3.345 (3.40)  Time: 0.731s, 1400.81/s  (0.737s, 1389.30/s)  LR: 1.737e-05  Data: 0.009 (0.048)
Train: 567 [ 100/1251 (  8%)]  Loss: 3.125 (3.31)  Time: 0.667s, 1534.51/s  (0.713s, 1436.95/s)  LR: 1.737e-05  Data: 0.010 (0.029)
Train: 567 [ 150/1251 ( 12%)]  Loss: 3.085 (3.25)  Time: 0.666s, 1538.30/s  (0.705s, 1451.47/s)  LR: 1.737e-05  Data: 0.010 (0.023)
Train: 567 [ 200/1251 ( 16%)]  Loss: 3.444 (3.29)  Time: 0.710s, 1442.06/s  (0.703s, 1457.03/s)  LR: 1.737e-05  Data: 0.011 (0.020)
Train: 567 [ 250/1251 ( 20%)]  Loss: 2.971 (3.24)  Time: 0.696s, 1471.88/s  (0.701s, 1461.14/s)  LR: 1.737e-05  Data: 0.011 (0.018)
Train: 567 [ 300/1251 ( 24%)]  Loss: 3.272 (3.24)  Time: 0.719s, 1424.08/s  (0.699s, 1464.67/s)  LR: 1.737e-05  Data: 0.010 (0.017)
Train: 567 [ 350/1251 ( 28%)]  Loss: 3.325 (3.25)  Time: 0.716s, 1430.42/s  (0.698s, 1466.52/s)  LR: 1.737e-05  Data: 0.009 (0.016)
Train: 567 [ 400/1251 ( 32%)]  Loss: 3.432 (3.27)  Time: 0.672s, 1524.93/s  (0.697s, 1468.48/s)  LR: 1.737e-05  Data: 0.010 (0.015)
Train: 567 [ 450/1251 ( 36%)]  Loss: 3.433 (3.29)  Time: 0.669s, 1529.85/s  (0.697s, 1470.17/s)  LR: 1.737e-05  Data: 0.010 (0.015)
Train: 567 [ 500/1251 ( 40%)]  Loss: 3.154 (3.28)  Time: 0.679s, 1508.47/s  (0.696s, 1472.04/s)  LR: 1.737e-05  Data: 0.010 (0.014)
Train: 567 [ 550/1251 ( 44%)]  Loss: 3.190 (3.27)  Time: 0.708s, 1446.56/s  (0.695s, 1472.82/s)  LR: 1.737e-05  Data: 0.009 (0.014)
Train: 567 [ 600/1251 ( 48%)]  Loss: 3.349 (3.28)  Time: 0.672s, 1523.37/s  (0.695s, 1473.62/s)  LR: 1.737e-05  Data: 0.011 (0.014)
Train: 567 [ 650/1251 ( 52%)]  Loss: 2.845 (3.24)  Time: 0.689s, 1487.28/s  (0.695s, 1473.33/s)  LR: 1.737e-05  Data: 0.010 (0.013)
Train: 567 [ 700/1251 ( 56%)]  Loss: 3.232 (3.24)  Time: 0.742s, 1380.24/s  (0.695s, 1473.19/s)  LR: 1.737e-05  Data: 0.010 (0.013)
Train: 567 [ 750/1251 ( 60%)]  Loss: 3.228 (3.24)  Time: 0.754s, 1358.76/s  (0.695s, 1474.13/s)  LR: 1.737e-05  Data: 0.010 (0.013)
Train: 567 [ 800/1251 ( 64%)]  Loss: 2.941 (3.23)  Time: 0.686s, 1492.69/s  (0.694s, 1474.66/s)  LR: 1.737e-05  Data: 0.010 (0.013)
Train: 567 [ 850/1251 ( 68%)]  Loss: 3.608 (3.25)  Time: 0.681s, 1504.62/s  (0.694s, 1474.51/s)  LR: 1.737e-05  Data: 0.011 (0.013)
Train: 567 [ 900/1251 ( 72%)]  Loss: 2.806 (3.22)  Time: 0.677s, 1513.10/s  (0.694s, 1475.33/s)  LR: 1.737e-05  Data: 0.010 (0.012)
Train: 567 [ 950/1251 ( 76%)]  Loss: 3.445 (3.23)  Time: 0.671s, 1526.10/s  (0.694s, 1475.92/s)  LR: 1.737e-05  Data: 0.010 (0.012)
Train: 567 [1000/1251 ( 80%)]  Loss: 3.365 (3.24)  Time: 0.698s, 1466.47/s  (0.694s, 1475.63/s)  LR: 1.737e-05  Data: 0.009 (0.012)
Train: 567 [1050/1251 ( 84%)]  Loss: 3.416 (3.25)  Time: 0.771s, 1328.91/s  (0.694s, 1475.81/s)  LR: 1.737e-05  Data: 0.010 (0.012)
Train: 567 [1100/1251 ( 88%)]  Loss: 3.308 (3.25)  Time: 0.727s, 1407.69/s  (0.694s, 1475.41/s)  LR: 1.737e-05  Data: 0.010 (0.012)
Train: 567 [1150/1251 ( 92%)]  Loss: 3.393 (3.26)  Time: 0.672s, 1524.72/s  (0.694s, 1475.54/s)  LR: 1.737e-05  Data: 0.011 (0.012)
Train: 567 [1200/1251 ( 96%)]  Loss: 3.347 (3.26)  Time: 0.700s, 1463.71/s  (0.694s, 1475.91/s)  LR: 1.737e-05  Data: 0.009 (0.012)
Train: 567 [1250/1251 (100%)]  Loss: 3.284 (3.26)  Time: 0.661s, 1548.80/s  (0.694s, 1476.02/s)  LR: 1.737e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.506 (1.506)  Loss:  0.7280 (0.7280)  Acc@1: 91.5039 (91.5039)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.136 (0.577)  Loss:  0.8340 (1.1849)  Acc@1: 87.0283 (78.9760)  Acc@5: 97.2877 (94.4140)
Train: 568 [   0/1251 (  0%)]  Loss: 3.102 (3.10)  Time: 2.178s,  470.13/s  (2.178s,  470.13/s)  LR: 1.693e-05  Data: 1.562 (1.562)
Train: 568 [  50/1251 (  4%)]  Loss: 3.323 (3.21)  Time: 0.712s, 1437.58/s  (0.724s, 1414.21/s)  LR: 1.693e-05  Data: 0.012 (0.049)
Train: 568 [ 100/1251 (  8%)]  Loss: 3.179 (3.20)  Time: 0.673s, 1521.27/s  (0.710s, 1441.54/s)  LR: 1.693e-05  Data: 0.011 (0.030)
Train: 568 [ 150/1251 ( 12%)]  Loss: 3.012 (3.15)  Time: 0.673s, 1521.75/s  (0.705s, 1452.97/s)  LR: 1.693e-05  Data: 0.010 (0.024)
Train: 568 [ 200/1251 ( 16%)]  Loss: 3.370 (3.20)  Time: 0.707s, 1447.55/s  (0.702s, 1458.11/s)  LR: 1.693e-05  Data: 0.009 (0.020)
Train: 568 [ 250/1251 ( 20%)]  Loss: 3.007 (3.17)  Time: 0.674s, 1518.23/s  (0.700s, 1462.91/s)  LR: 1.693e-05  Data: 0.012 (0.018)
Train: 568 [ 300/1251 ( 24%)]  Loss: 3.067 (3.15)  Time: 0.700s, 1462.34/s  (0.698s, 1466.63/s)  LR: 1.693e-05  Data: 0.009 (0.017)
Train: 568 [ 350/1251 ( 28%)]  Loss: 3.056 (3.14)  Time: 0.746s, 1373.06/s  (0.699s, 1465.73/s)  LR: 1.693e-05  Data: 0.011 (0.016)
Train: 568 [ 400/1251 ( 32%)]  Loss: 3.271 (3.15)  Time: 0.675s, 1515.96/s  (0.698s, 1468.09/s)  LR: 1.693e-05  Data: 0.011 (0.015)
Train: 568 [ 450/1251 ( 36%)]  Loss: 2.914 (3.13)  Time: 0.666s, 1538.46/s  (0.697s, 1469.03/s)  LR: 1.693e-05  Data: 0.011 (0.015)
Train: 568 [ 500/1251 ( 40%)]  Loss: 2.832 (3.10)  Time: 0.680s, 1505.57/s  (0.696s, 1471.81/s)  LR: 1.693e-05  Data: 0.011 (0.014)
Train: 568 [ 550/1251 ( 44%)]  Loss: 3.298 (3.12)  Time: 0.716s, 1431.10/s  (0.696s, 1472.23/s)  LR: 1.693e-05  Data: 0.010 (0.014)
Train: 568 [ 600/1251 ( 48%)]  Loss: 3.009 (3.11)  Time: 0.687s, 1490.52/s  (0.696s, 1472.08/s)  LR: 1.693e-05  Data: 0.016 (0.014)
Train: 568 [ 650/1251 ( 52%)]  Loss: 3.350 (3.13)  Time: 0.683s, 1498.30/s  (0.695s, 1472.59/s)  LR: 1.693e-05  Data: 0.010 (0.014)
Train: 568 [ 700/1251 ( 56%)]  Loss: 3.402 (3.15)  Time: 0.731s, 1401.40/s  (0.695s, 1473.27/s)  LR: 1.693e-05  Data: 0.013 (0.013)
Train: 568 [ 750/1251 ( 60%)]  Loss: 3.521 (3.17)  Time: 0.674s, 1519.70/s  (0.695s, 1474.07/s)  LR: 1.693e-05  Data: 0.011 (0.013)
Train: 568 [ 800/1251 ( 64%)]  Loss: 3.224 (3.17)  Time: 0.672s, 1524.27/s  (0.694s, 1475.60/s)  LR: 1.693e-05  Data: 0.010 (0.013)
Train: 568 [ 850/1251 ( 68%)]  Loss: 3.342 (3.18)  Time: 0.680s, 1505.99/s  (0.694s, 1476.10/s)  LR: 1.693e-05  Data: 0.013 (0.013)
Train: 568 [ 900/1251 ( 72%)]  Loss: 2.943 (3.17)  Time: 0.666s, 1536.49/s  (0.693s, 1476.91/s)  LR: 1.693e-05  Data: 0.010 (0.013)
Train: 568 [ 950/1251 ( 76%)]  Loss: 3.471 (3.18)  Time: 0.715s, 1432.05/s  (0.693s, 1476.88/s)  LR: 1.693e-05  Data: 0.013 (0.013)
Train: 568 [1000/1251 ( 80%)]  Loss: 3.189 (3.18)  Time: 0.713s, 1436.52/s  (0.693s, 1476.60/s)  LR: 1.693e-05  Data: 0.011 (0.012)
Train: 568 [1050/1251 ( 84%)]  Loss: 2.862 (3.17)  Time: 0.679s, 1507.64/s  (0.693s, 1476.81/s)  LR: 1.693e-05  Data: 0.010 (0.012)
Train: 568 [1100/1251 ( 88%)]  Loss: 2.999 (3.16)  Time: 0.712s, 1438.31/s  (0.693s, 1476.93/s)  LR: 1.693e-05  Data: 0.011 (0.012)
Train: 568 [1150/1251 ( 92%)]  Loss: 2.961 (3.15)  Time: 0.670s, 1527.51/s  (0.693s, 1477.50/s)  LR: 1.693e-05  Data: 0.010 (0.012)
Train: 568 [1200/1251 ( 96%)]  Loss: 3.260 (3.16)  Time: 0.699s, 1465.15/s  (0.693s, 1477.63/s)  LR: 1.693e-05  Data: 0.010 (0.012)
Train: 568 [1250/1251 (100%)]  Loss: 2.809 (3.15)  Time: 0.693s, 1477.78/s  (0.693s, 1477.13/s)  LR: 1.693e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.496 (1.496)  Loss:  0.6743 (0.6743)  Acc@1: 91.6992 (91.6992)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.136 (0.573)  Loss:  0.7852 (1.1406)  Acc@1: 87.1462 (79.0360)  Acc@5: 97.5236 (94.4400)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-558.pth.tar', 79.20199995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-557.pth.tar', 79.1480000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-553.pth.tar', 79.08400000488281)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-561.pth.tar', 79.075999921875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-560.pth.tar', 79.06200005126954)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-566.pth.tar', 79.05799997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-556.pth.tar', 79.04000010498046)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-568.pth.tar', 79.03600010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-563.pth.tar', 79.02599997558593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-552.pth.tar', 79.01200005371093)

Train: 569 [   0/1251 (  0%)]  Loss: 3.303 (3.30)  Time: 2.486s,  411.85/s  (2.486s,  411.85/s)  LR: 1.651e-05  Data: 1.858 (1.858)
Train: 569 [  50/1251 (  4%)]  Loss: 3.360 (3.33)  Time: 0.695s, 1474.10/s  (0.737s, 1389.71/s)  LR: 1.651e-05  Data: 0.010 (0.054)
Train: 569 [ 100/1251 (  8%)]  Loss: 3.359 (3.34)  Time: 0.706s, 1450.93/s  (0.714s, 1433.69/s)  LR: 1.651e-05  Data: 0.011 (0.032)
Train: 569 [ 150/1251 ( 12%)]  Loss: 3.494 (3.38)  Time: 0.706s, 1450.40/s  (0.711s, 1440.19/s)  LR: 1.651e-05  Data: 0.009 (0.025)
Train: 569 [ 200/1251 ( 16%)]  Loss: 3.162 (3.34)  Time: 0.729s, 1403.76/s  (0.709s, 1444.99/s)  LR: 1.651e-05  Data: 0.009 (0.022)
Train: 569 [ 250/1251 ( 20%)]  Loss: 2.851 (3.25)  Time: 0.723s, 1415.89/s  (0.706s, 1450.73/s)  LR: 1.651e-05  Data: 0.010 (0.019)
Train: 569 [ 300/1251 ( 24%)]  Loss: 3.359 (3.27)  Time: 0.701s, 1459.79/s  (0.704s, 1455.46/s)  LR: 1.651e-05  Data: 0.009 (0.018)
Train: 569 [ 350/1251 ( 28%)]  Loss: 3.175 (3.26)  Time: 0.670s, 1527.53/s  (0.702s, 1458.74/s)  LR: 1.651e-05  Data: 0.010 (0.017)
Train: 569 [ 400/1251 ( 32%)]  Loss: 2.721 (3.20)  Time: 0.667s, 1536.29/s  (0.700s, 1462.97/s)  LR: 1.651e-05  Data: 0.009 (0.016)
Train: 569 [ 450/1251 ( 36%)]  Loss: 3.433 (3.22)  Time: 0.672s, 1523.05/s  (0.699s, 1465.88/s)  LR: 1.651e-05  Data: 0.010 (0.015)
Train: 569 [ 500/1251 ( 40%)]  Loss: 2.626 (3.17)  Time: 0.684s, 1496.52/s  (0.698s, 1468.08/s)  LR: 1.651e-05  Data: 0.010 (0.015)
Train: 569 [ 550/1251 ( 44%)]  Loss: 3.192 (3.17)  Time: 0.670s, 1527.82/s  (0.697s, 1469.90/s)  LR: 1.651e-05  Data: 0.010 (0.014)
Train: 569 [ 600/1251 ( 48%)]  Loss: 3.534 (3.20)  Time: 0.692s, 1479.92/s  (0.696s, 1470.41/s)  LR: 1.651e-05  Data: 0.009 (0.014)
Train: 569 [ 650/1251 ( 52%)]  Loss: 2.926 (3.18)  Time: 0.702s, 1458.63/s  (0.696s, 1471.69/s)  LR: 1.651e-05  Data: 0.009 (0.014)
Train: 569 [ 700/1251 ( 56%)]  Loss: 3.683 (3.21)  Time: 0.690s, 1484.66/s  (0.695s, 1472.67/s)  LR: 1.651e-05  Data: 0.010 (0.014)
Train: 569 [ 750/1251 ( 60%)]  Loss: 3.174 (3.21)  Time: 0.673s, 1521.84/s  (0.696s, 1472.18/s)  LR: 1.651e-05  Data: 0.013 (0.013)
Train: 569 [ 800/1251 ( 64%)]  Loss: 3.186 (3.21)  Time: 0.678s, 1511.31/s  (0.695s, 1473.36/s)  LR: 1.651e-05  Data: 0.011 (0.013)
Train: 569 [ 850/1251 ( 68%)]  Loss: 3.238 (3.21)  Time: 0.668s, 1532.91/s  (0.695s, 1473.63/s)  LR: 1.651e-05  Data: 0.009 (0.013)
Train: 569 [ 900/1251 ( 72%)]  Loss: 3.259 (3.21)  Time: 0.666s, 1536.62/s  (0.695s, 1473.35/s)  LR: 1.651e-05  Data: 0.010 (0.013)
Train: 569 [ 950/1251 ( 76%)]  Loss: 3.225 (3.21)  Time: 0.673s, 1522.02/s  (0.695s, 1473.93/s)  LR: 1.651e-05  Data: 0.010 (0.013)
Train: 569 [1000/1251 ( 80%)]  Loss: 3.295 (3.22)  Time: 0.716s, 1429.84/s  (0.695s, 1473.57/s)  LR: 1.651e-05  Data: 0.009 (0.013)
Train: 569 [1050/1251 ( 84%)]  Loss: 3.192 (3.22)  Time: 0.699s, 1465.56/s  (0.695s, 1474.21/s)  LR: 1.651e-05  Data: 0.009 (0.013)
Train: 569 [1100/1251 ( 88%)]  Loss: 3.248 (3.22)  Time: 0.665s, 1540.81/s  (0.695s, 1473.92/s)  LR: 1.651e-05  Data: 0.010 (0.012)
Train: 569 [1150/1251 ( 92%)]  Loss: 3.312 (3.22)  Time: 0.707s, 1448.48/s  (0.695s, 1474.25/s)  LR: 1.651e-05  Data: 0.011 (0.012)
Train: 569 [1200/1251 ( 96%)]  Loss: 3.619 (3.24)  Time: 0.680s, 1506.60/s  (0.694s, 1474.53/s)  LR: 1.651e-05  Data: 0.010 (0.012)
Train: 569 [1250/1251 (100%)]  Loss: 3.262 (3.24)  Time: 0.658s, 1555.66/s  (0.694s, 1475.11/s)  LR: 1.651e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.580 (1.580)  Loss:  0.6763 (0.6763)  Acc@1: 91.9922 (91.9922)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.136 (0.581)  Loss:  0.7866 (1.1674)  Acc@1: 87.1462 (79.0700)  Acc@5: 97.2877 (94.5140)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-558.pth.tar', 79.20199995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-557.pth.tar', 79.1480000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-553.pth.tar', 79.08400000488281)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-561.pth.tar', 79.075999921875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-569.pth.tar', 79.07000010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-560.pth.tar', 79.06200005126954)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-566.pth.tar', 79.05799997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-556.pth.tar', 79.04000010498046)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-568.pth.tar', 79.03600010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-563.pth.tar', 79.02599997558593)

Train: 570 [   0/1251 (  0%)]  Loss: 3.073 (3.07)  Time: 2.099s,  487.90/s  (2.099s,  487.90/s)  LR: 1.609e-05  Data: 1.483 (1.483)
Train: 570 [  50/1251 (  4%)]  Loss: 3.003 (3.04)  Time: 0.696s, 1471.82/s  (0.725s, 1413.38/s)  LR: 1.609e-05  Data: 0.009 (0.045)
Train: 570 [ 100/1251 (  8%)]  Loss: 2.918 (3.00)  Time: 0.693s, 1478.62/s  (0.705s, 1453.03/s)  LR: 1.609e-05  Data: 0.010 (0.028)
Train: 570 [ 150/1251 ( 12%)]  Loss: 3.071 (3.02)  Time: 0.721s, 1419.91/s  (0.699s, 1464.51/s)  LR: 1.609e-05  Data: 0.010 (0.022)
Train: 570 [ 200/1251 ( 16%)]  Loss: 3.242 (3.06)  Time: 0.674s, 1519.46/s  (0.700s, 1462.99/s)  LR: 1.609e-05  Data: 0.010 (0.019)
Train: 570 [ 250/1251 ( 20%)]  Loss: 2.969 (3.05)  Time: 0.703s, 1457.03/s  (0.700s, 1463.61/s)  LR: 1.609e-05  Data: 0.010 (0.017)
Train: 570 [ 300/1251 ( 24%)]  Loss: 3.475 (3.11)  Time: 0.672s, 1524.67/s  (0.698s, 1467.96/s)  LR: 1.609e-05  Data: 0.009 (0.016)
Train: 570 [ 350/1251 ( 28%)]  Loss: 2.888 (3.08)  Time: 0.673s, 1520.99/s  (0.696s, 1471.49/s)  LR: 1.609e-05  Data: 0.011 (0.015)
Train: 570 [ 400/1251 ( 32%)]  Loss: 2.934 (3.06)  Time: 0.705s, 1452.04/s  (0.695s, 1473.34/s)  LR: 1.609e-05  Data: 0.011 (0.015)
Train: 570 [ 450/1251 ( 36%)]  Loss: 3.464 (3.10)  Time: 0.672s, 1524.81/s  (0.694s, 1474.97/s)  LR: 1.609e-05  Data: 0.011 (0.014)
Train: 570 [ 500/1251 ( 40%)]  Loss: 3.175 (3.11)  Time: 0.674s, 1519.54/s  (0.694s, 1475.86/s)  LR: 1.609e-05  Data: 0.011 (0.014)
Train: 570 [ 550/1251 ( 44%)]  Loss: 3.510 (3.14)  Time: 0.692s, 1479.33/s  (0.694s, 1476.35/s)  LR: 1.609e-05  Data: 0.011 (0.014)
Train: 570 [ 600/1251 ( 48%)]  Loss: 3.120 (3.14)  Time: 0.674s, 1519.44/s  (0.693s, 1476.63/s)  LR: 1.609e-05  Data: 0.010 (0.013)
Train: 570 [ 650/1251 ( 52%)]  Loss: 3.372 (3.16)  Time: 0.769s, 1331.96/s  (0.693s, 1477.19/s)  LR: 1.609e-05  Data: 0.011 (0.013)
Train: 570 [ 700/1251 ( 56%)]  Loss: 3.448 (3.18)  Time: 0.674s, 1519.30/s  (0.694s, 1475.90/s)  LR: 1.609e-05  Data: 0.013 (0.013)
Train: 570 [ 750/1251 ( 60%)]  Loss: 3.348 (3.19)  Time: 0.667s, 1536.27/s  (0.693s, 1476.89/s)  LR: 1.609e-05  Data: 0.010 (0.013)
Train: 570 [ 800/1251 ( 64%)]  Loss: 3.340 (3.20)  Time: 0.702s, 1458.21/s  (0.693s, 1477.75/s)  LR: 1.609e-05  Data: 0.010 (0.013)
Train: 570 [ 850/1251 ( 68%)]  Loss: 3.277 (3.20)  Time: 0.704s, 1454.86/s  (0.693s, 1478.44/s)  LR: 1.609e-05  Data: 0.011 (0.013)
Train: 570 [ 900/1251 ( 72%)]  Loss: 3.390 (3.21)  Time: 0.702s, 1458.99/s  (0.692s, 1478.77/s)  LR: 1.609e-05  Data: 0.010 (0.012)
Train: 570 [ 950/1251 ( 76%)]  Loss: 3.090 (3.21)  Time: 0.683s, 1498.47/s  (0.692s, 1479.26/s)  LR: 1.609e-05  Data: 0.010 (0.012)
Train: 570 [1000/1251 ( 80%)]  Loss: 3.161 (3.20)  Time: 0.699s, 1464.31/s  (0.692s, 1479.13/s)  LR: 1.609e-05  Data: 0.010 (0.012)
Train: 570 [1050/1251 ( 84%)]  Loss: 3.592 (3.22)  Time: 0.671s, 1525.71/s  (0.692s, 1479.05/s)  LR: 1.609e-05  Data: 0.010 (0.012)
Train: 570 [1100/1251 ( 88%)]  Loss: 3.398 (3.23)  Time: 0.684s, 1498.16/s  (0.692s, 1479.55/s)  LR: 1.609e-05  Data: 0.010 (0.012)
Train: 570 [1150/1251 ( 92%)]  Loss: 2.993 (3.22)  Time: 0.679s, 1509.20/s  (0.692s, 1479.57/s)  LR: 1.609e-05  Data: 0.010 (0.012)
Train: 570 [1200/1251 ( 96%)]  Loss: 3.201 (3.22)  Time: 0.703s, 1456.54/s  (0.692s, 1479.00/s)  LR: 1.609e-05  Data: 0.011 (0.012)
Train: 570 [1250/1251 (100%)]  Loss: 3.265 (3.22)  Time: 0.694s, 1474.73/s  (0.692s, 1478.78/s)  LR: 1.609e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.462 (1.462)  Loss:  0.7544 (0.7544)  Acc@1: 91.3086 (91.3086)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.136 (0.583)  Loss:  0.8750 (1.2273)  Acc@1: 87.0283 (78.9220)  Acc@5: 97.4057 (94.3980)
Train: 571 [   0/1251 (  0%)]  Loss: 3.464 (3.46)  Time: 2.466s,  415.27/s  (2.466s,  415.27/s)  LR: 1.570e-05  Data: 1.844 (1.844)
Train: 571 [  50/1251 (  4%)]  Loss: 3.384 (3.42)  Time: 0.703s, 1456.84/s  (0.737s, 1388.54/s)  LR: 1.570e-05  Data: 0.010 (0.055)
Train: 571 [ 100/1251 (  8%)]  Loss: 3.313 (3.39)  Time: 0.672s, 1524.27/s  (0.711s, 1440.76/s)  LR: 1.570e-05  Data: 0.010 (0.033)
Train: 571 [ 150/1251 ( 12%)]  Loss: 2.998 (3.29)  Time: 0.669s, 1530.36/s  (0.703s, 1455.72/s)  LR: 1.570e-05  Data: 0.010 (0.026)
Train: 571 [ 200/1251 ( 16%)]  Loss: 3.113 (3.25)  Time: 0.671s, 1525.79/s  (0.701s, 1461.48/s)  LR: 1.570e-05  Data: 0.010 (0.022)
Train: 571 [ 250/1251 ( 20%)]  Loss: 3.459 (3.29)  Time: 0.672s, 1524.67/s  (0.698s, 1467.29/s)  LR: 1.570e-05  Data: 0.010 (0.020)
Train: 571 [ 300/1251 ( 24%)]  Loss: 3.071 (3.26)  Time: 0.676s, 1513.79/s  (0.697s, 1469.10/s)  LR: 1.570e-05  Data: 0.010 (0.018)
Train: 571 [ 350/1251 ( 28%)]  Loss: 2.892 (3.21)  Time: 0.671s, 1525.65/s  (0.695s, 1473.02/s)  LR: 1.570e-05  Data: 0.010 (0.017)
Train: 571 [ 400/1251 ( 32%)]  Loss: 3.434 (3.24)  Time: 0.672s, 1524.52/s  (0.695s, 1473.26/s)  LR: 1.570e-05  Data: 0.014 (0.016)
Train: 571 [ 450/1251 ( 36%)]  Loss: 3.209 (3.23)  Time: 0.702s, 1458.54/s  (0.694s, 1475.24/s)  LR: 1.570e-05  Data: 0.011 (0.016)
Train: 571 [ 500/1251 ( 40%)]  Loss: 2.866 (3.20)  Time: 0.704s, 1455.05/s  (0.695s, 1473.55/s)  LR: 1.570e-05  Data: 0.009 (0.015)
Train: 571 [ 550/1251 ( 44%)]  Loss: 3.329 (3.21)  Time: 0.666s, 1536.43/s  (0.695s, 1474.18/s)  LR: 1.570e-05  Data: 0.009 (0.015)
Train: 571 [ 600/1251 ( 48%)]  Loss: 3.248 (3.21)  Time: 0.666s, 1538.21/s  (0.694s, 1476.11/s)  LR: 1.570e-05  Data: 0.011 (0.014)
Train: 571 [ 650/1251 ( 52%)]  Loss: 3.238 (3.22)  Time: 0.671s, 1525.98/s  (0.694s, 1475.65/s)  LR: 1.570e-05  Data: 0.009 (0.014)
Train: 571 [ 700/1251 ( 56%)]  Loss: 3.143 (3.21)  Time: 0.720s, 1421.33/s  (0.693s, 1476.82/s)  LR: 1.570e-05  Data: 0.010 (0.014)
Train: 571 [ 750/1251 ( 60%)]  Loss: 3.222 (3.21)  Time: 0.702s, 1458.99/s  (0.693s, 1477.46/s)  LR: 1.570e-05  Data: 0.009 (0.014)
Train: 571 [ 800/1251 ( 64%)]  Loss: 3.256 (3.21)  Time: 0.673s, 1522.62/s  (0.692s, 1478.73/s)  LR: 1.570e-05  Data: 0.011 (0.013)
Train: 571 [ 850/1251 ( 68%)]  Loss: 2.990 (3.20)  Time: 0.718s, 1426.12/s  (0.692s, 1479.13/s)  LR: 1.570e-05  Data: 0.009 (0.013)
Train: 571 [ 900/1251 ( 72%)]  Loss: 2.993 (3.19)  Time: 0.706s, 1449.96/s  (0.692s, 1479.02/s)  LR: 1.570e-05  Data: 0.010 (0.013)
Train: 571 [ 950/1251 ( 76%)]  Loss: 3.396 (3.20)  Time: 0.672s, 1523.77/s  (0.692s, 1478.82/s)  LR: 1.570e-05  Data: 0.010 (0.013)
Train: 571 [1000/1251 ( 80%)]  Loss: 3.184 (3.20)  Time: 0.668s, 1532.18/s  (0.692s, 1479.90/s)  LR: 1.570e-05  Data: 0.010 (0.013)
Train: 571 [1050/1251 ( 84%)]  Loss: 3.039 (3.19)  Time: 0.675s, 1517.84/s  (0.692s, 1480.46/s)  LR: 1.570e-05  Data: 0.012 (0.013)
Train: 571 [1100/1251 ( 88%)]  Loss: 3.584 (3.21)  Time: 0.673s, 1522.58/s  (0.692s, 1480.22/s)  LR: 1.570e-05  Data: 0.010 (0.012)
Train: 571 [1150/1251 ( 92%)]  Loss: 3.366 (3.22)  Time: 0.717s, 1428.16/s  (0.692s, 1480.07/s)  LR: 1.570e-05  Data: 0.011 (0.012)
Train: 571 [1200/1251 ( 96%)]  Loss: 3.297 (3.22)  Time: 0.677s, 1511.86/s  (0.692s, 1479.53/s)  LR: 1.570e-05  Data: 0.011 (0.012)
Train: 571 [1250/1251 (100%)]  Loss: 3.408 (3.23)  Time: 0.656s, 1560.50/s  (0.692s, 1479.97/s)  LR: 1.570e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.459 (1.459)  Loss:  0.6255 (0.6255)  Acc@1: 91.4062 (91.4062)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.136 (0.584)  Loss:  0.7080 (1.0823)  Acc@1: 87.0283 (79.1720)  Acc@5: 97.1698 (94.5280)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-558.pth.tar', 79.20199995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-571.pth.tar', 79.17199992431641)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-557.pth.tar', 79.1480000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-553.pth.tar', 79.08400000488281)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-561.pth.tar', 79.075999921875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-569.pth.tar', 79.07000010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-560.pth.tar', 79.06200005126954)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-566.pth.tar', 79.05799997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-556.pth.tar', 79.04000010498046)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-568.pth.tar', 79.03600010498047)

Train: 572 [   0/1251 (  0%)]  Loss: 2.913 (2.91)  Time: 2.290s,  447.21/s  (2.290s,  447.21/s)  LR: 1.531e-05  Data: 1.624 (1.624)
Train: 572 [  50/1251 (  4%)]  Loss: 2.924 (2.92)  Time: 0.673s, 1520.69/s  (0.726s, 1409.88/s)  LR: 1.531e-05  Data: 0.011 (0.048)
Train: 572 [ 100/1251 (  8%)]  Loss: 3.192 (3.01)  Time: 0.680s, 1506.69/s  (0.709s, 1443.75/s)  LR: 1.531e-05  Data: 0.011 (0.029)
Train: 572 [ 150/1251 ( 12%)]  Loss: 2.713 (2.94)  Time: 0.671s, 1525.29/s  (0.701s, 1459.77/s)  LR: 1.531e-05  Data: 0.012 (0.023)
Train: 572 [ 200/1251 ( 16%)]  Loss: 3.243 (3.00)  Time: 0.687s, 1490.40/s  (0.699s, 1464.50/s)  LR: 1.531e-05  Data: 0.009 (0.020)
Train: 572 [ 250/1251 ( 20%)]  Loss: 3.316 (3.05)  Time: 0.703s, 1456.61/s  (0.698s, 1467.86/s)  LR: 1.531e-05  Data: 0.009 (0.018)
Train: 572 [ 300/1251 ( 24%)]  Loss: 3.080 (3.05)  Time: 0.673s, 1521.20/s  (0.696s, 1470.52/s)  LR: 1.531e-05  Data: 0.010 (0.017)
Train: 572 [ 350/1251 ( 28%)]  Loss: 3.265 (3.08)  Time: 0.701s, 1461.32/s  (0.695s, 1473.85/s)  LR: 1.531e-05  Data: 0.014 (0.016)
Train: 572 [ 400/1251 ( 32%)]  Loss: 3.440 (3.12)  Time: 0.677s, 1512.39/s  (0.695s, 1473.21/s)  LR: 1.531e-05  Data: 0.009 (0.015)
Train: 572 [ 450/1251 ( 36%)]  Loss: 3.135 (3.12)  Time: 0.671s, 1525.13/s  (0.695s, 1474.44/s)  LR: 1.531e-05  Data: 0.011 (0.015)
Train: 572 [ 500/1251 ( 40%)]  Loss: 3.343 (3.14)  Time: 0.673s, 1521.75/s  (0.694s, 1475.93/s)  LR: 1.531e-05  Data: 0.009 (0.014)
Train: 572 [ 550/1251 ( 44%)]  Loss: 3.020 (3.13)  Time: 0.672s, 1523.18/s  (0.694s, 1475.66/s)  LR: 1.531e-05  Data: 0.010 (0.014)
Train: 572 [ 600/1251 ( 48%)]  Loss: 3.165 (3.13)  Time: 0.674s, 1519.58/s  (0.694s, 1476.55/s)  LR: 1.531e-05  Data: 0.011 (0.014)
Train: 572 [ 650/1251 ( 52%)]  Loss: 2.723 (3.11)  Time: 0.673s, 1522.50/s  (0.694s, 1475.66/s)  LR: 1.531e-05  Data: 0.009 (0.013)
Train: 572 [ 700/1251 ( 56%)]  Loss: 3.283 (3.12)  Time: 0.677s, 1513.20/s  (0.693s, 1476.60/s)  LR: 1.531e-05  Data: 0.011 (0.013)
Train: 572 [ 750/1251 ( 60%)]  Loss: 3.227 (3.12)  Time: 0.674s, 1518.52/s  (0.694s, 1476.07/s)  LR: 1.531e-05  Data: 0.011 (0.013)
Train: 572 [ 800/1251 ( 64%)]  Loss: 3.160 (3.13)  Time: 0.667s, 1535.86/s  (0.693s, 1476.87/s)  LR: 1.531e-05  Data: 0.010 (0.013)
Train: 572 [ 850/1251 ( 68%)]  Loss: 3.283 (3.13)  Time: 0.672s, 1524.72/s  (0.694s, 1476.10/s)  LR: 1.531e-05  Data: 0.009 (0.013)
Train: 572 [ 900/1251 ( 72%)]  Loss: 2.803 (3.12)  Time: 0.672s, 1523.61/s  (0.693s, 1477.47/s)  LR: 1.531e-05  Data: 0.011 (0.013)
Train: 572 [ 950/1251 ( 76%)]  Loss: 3.395 (3.13)  Time: 0.670s, 1527.49/s  (0.693s, 1477.45/s)  LR: 1.531e-05  Data: 0.011 (0.012)
Train: 572 [1000/1251 ( 80%)]  Loss: 2.962 (3.12)  Time: 0.685s, 1495.00/s  (0.693s, 1476.62/s)  LR: 1.531e-05  Data: 0.010 (0.012)
Train: 572 [1050/1251 ( 84%)]  Loss: 3.335 (3.13)  Time: 0.700s, 1463.69/s  (0.694s, 1476.35/s)  LR: 1.531e-05  Data: 0.013 (0.012)
Train: 572 [1100/1251 ( 88%)]  Loss: 3.279 (3.14)  Time: 0.670s, 1528.46/s  (0.693s, 1476.74/s)  LR: 1.531e-05  Data: 0.010 (0.012)
Train: 572 [1150/1251 ( 92%)]  Loss: 3.158 (3.14)  Time: 0.706s, 1451.44/s  (0.693s, 1476.82/s)  LR: 1.531e-05  Data: 0.011 (0.012)
Train: 572 [1200/1251 ( 96%)]  Loss: 3.243 (3.14)  Time: 0.667s, 1534.15/s  (0.694s, 1476.43/s)  LR: 1.531e-05  Data: 0.010 (0.012)
Train: 572 [1250/1251 (100%)]  Loss: 3.325 (3.15)  Time: 0.688s, 1487.29/s  (0.694s, 1476.28/s)  LR: 1.531e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.573 (1.573)  Loss:  0.7100 (0.7100)  Acc@1: 91.7969 (91.7969)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.136 (0.575)  Loss:  0.8115 (1.1738)  Acc@1: 86.9104 (79.2120)  Acc@5: 97.2877 (94.5280)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-572.pth.tar', 79.2120000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-558.pth.tar', 79.20199995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-571.pth.tar', 79.17199992431641)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-557.pth.tar', 79.1480000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-553.pth.tar', 79.08400000488281)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-561.pth.tar', 79.075999921875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-569.pth.tar', 79.07000010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-560.pth.tar', 79.06200005126954)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-566.pth.tar', 79.05799997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-556.pth.tar', 79.04000010498046)

Train: 573 [   0/1251 (  0%)]  Loss: 3.230 (3.23)  Time: 2.061s,  496.88/s  (2.061s,  496.88/s)  LR: 1.494e-05  Data: 1.447 (1.447)
Train: 573 [  50/1251 (  4%)]  Loss: 2.820 (3.03)  Time: 0.671s, 1526.02/s  (0.725s, 1411.50/s)  LR: 1.494e-05  Data: 0.010 (0.047)
Train: 573 [ 100/1251 (  8%)]  Loss: 3.511 (3.19)  Time: 0.704s, 1455.29/s  (0.710s, 1442.47/s)  LR: 1.494e-05  Data: 0.009 (0.029)
Train: 573 [ 150/1251 ( 12%)]  Loss: 3.263 (3.21)  Time: 0.680s, 1505.92/s  (0.701s, 1461.62/s)  LR: 1.494e-05  Data: 0.009 (0.023)
Train: 573 [ 200/1251 ( 16%)]  Loss: 2.967 (3.16)  Time: 0.667s, 1535.10/s  (0.697s, 1469.50/s)  LR: 1.494e-05  Data: 0.009 (0.020)
Train: 573 [ 250/1251 ( 20%)]  Loss: 3.113 (3.15)  Time: 0.671s, 1525.35/s  (0.696s, 1471.60/s)  LR: 1.494e-05  Data: 0.011 (0.018)
Train: 573 [ 300/1251 ( 24%)]  Loss: 2.847 (3.11)  Time: 0.756s, 1354.81/s  (0.694s, 1474.94/s)  LR: 1.494e-05  Data: 0.011 (0.016)
Train: 573 [ 350/1251 ( 28%)]  Loss: 3.101 (3.11)  Time: 0.706s, 1450.39/s  (0.694s, 1475.42/s)  LR: 1.494e-05  Data: 0.009 (0.016)
Train: 573 [ 400/1251 ( 32%)]  Loss: 3.033 (3.10)  Time: 0.702s, 1458.13/s  (0.693s, 1477.43/s)  LR: 1.494e-05  Data: 0.009 (0.015)
Train: 573 [ 450/1251 ( 36%)]  Loss: 3.296 (3.12)  Time: 0.669s, 1530.31/s  (0.692s, 1478.71/s)  LR: 1.494e-05  Data: 0.010 (0.014)
Train: 573 [ 500/1251 ( 40%)]  Loss: 3.322 (3.14)  Time: 0.716s, 1430.37/s  (0.692s, 1478.87/s)  LR: 1.494e-05  Data: 0.009 (0.014)
Train: 573 [ 550/1251 ( 44%)]  Loss: 3.550 (3.17)  Time: 0.761s, 1345.33/s  (0.692s, 1479.30/s)  LR: 1.494e-05  Data: 0.010 (0.014)
Train: 573 [ 600/1251 ( 48%)]  Loss: 2.562 (3.12)  Time: 0.702s, 1458.09/s  (0.692s, 1479.16/s)  LR: 1.494e-05  Data: 0.009 (0.014)
Train: 573 [ 650/1251 ( 52%)]  Loss: 3.048 (3.12)  Time: 0.724s, 1413.49/s  (0.692s, 1479.07/s)  LR: 1.494e-05  Data: 0.010 (0.013)
Train: 573 [ 700/1251 ( 56%)]  Loss: 3.235 (3.13)  Time: 0.674s, 1519.23/s  (0.692s, 1479.47/s)  LR: 1.494e-05  Data: 0.010 (0.013)
Train: 573 [ 750/1251 ( 60%)]  Loss: 3.323 (3.14)  Time: 0.671s, 1526.28/s  (0.692s, 1479.60/s)  LR: 1.494e-05  Data: 0.010 (0.013)
Train: 573 [ 800/1251 ( 64%)]  Loss: 3.391 (3.15)  Time: 0.669s, 1530.67/s  (0.692s, 1479.06/s)  LR: 1.494e-05  Data: 0.010 (0.013)
Train: 573 [ 850/1251 ( 68%)]  Loss: 2.956 (3.14)  Time: 0.672s, 1524.52/s  (0.692s, 1479.20/s)  LR: 1.494e-05  Data: 0.009 (0.013)
Train: 573 [ 900/1251 ( 72%)]  Loss: 3.215 (3.15)  Time: 0.671s, 1525.76/s  (0.692s, 1480.29/s)  LR: 1.494e-05  Data: 0.010 (0.012)
Train: 573 [ 950/1251 ( 76%)]  Loss: 3.342 (3.16)  Time: 0.672s, 1524.65/s  (0.692s, 1480.36/s)  LR: 1.494e-05  Data: 0.011 (0.012)
Train: 573 [1000/1251 ( 80%)]  Loss: 3.515 (3.17)  Time: 0.672s, 1522.73/s  (0.692s, 1480.69/s)  LR: 1.494e-05  Data: 0.010 (0.012)
Train: 573 [1050/1251 ( 84%)]  Loss: 3.436 (3.19)  Time: 0.673s, 1522.57/s  (0.691s, 1481.07/s)  LR: 1.494e-05  Data: 0.010 (0.012)
Train: 573 [1100/1251 ( 88%)]  Loss: 2.813 (3.17)  Time: 0.666s, 1536.81/s  (0.691s, 1481.35/s)  LR: 1.494e-05  Data: 0.010 (0.012)
Train: 573 [1150/1251 ( 92%)]  Loss: 3.175 (3.17)  Time: 0.672s, 1523.54/s  (0.691s, 1480.91/s)  LR: 1.494e-05  Data: 0.011 (0.012)
Train: 573 [1200/1251 ( 96%)]  Loss: 3.161 (3.17)  Time: 0.702s, 1457.88/s  (0.691s, 1481.11/s)  LR: 1.494e-05  Data: 0.009 (0.012)
Train: 573 [1250/1251 (100%)]  Loss: 3.286 (3.17)  Time: 0.707s, 1448.75/s  (0.691s, 1481.11/s)  LR: 1.494e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.475 (1.475)  Loss:  0.7144 (0.7144)  Acc@1: 92.2852 (92.2852)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.136 (0.587)  Loss:  0.8374 (1.2046)  Acc@1: 86.9104 (79.2080)  Acc@5: 96.9340 (94.4800)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-572.pth.tar', 79.2120000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-573.pth.tar', 79.20800013183593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-558.pth.tar', 79.20199995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-571.pth.tar', 79.17199992431641)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-557.pth.tar', 79.1480000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-553.pth.tar', 79.08400000488281)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-561.pth.tar', 79.075999921875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-569.pth.tar', 79.07000010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-560.pth.tar', 79.06200005126954)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-566.pth.tar', 79.05799997558594)

Train: 574 [   0/1251 (  0%)]  Loss: 3.315 (3.32)  Time: 2.050s,  499.57/s  (2.050s,  499.57/s)  LR: 1.458e-05  Data: 1.434 (1.434)
Train: 574 [  50/1251 (  4%)]  Loss: 3.356 (3.34)  Time: 0.687s, 1490.84/s  (0.725s, 1412.23/s)  LR: 1.458e-05  Data: 0.011 (0.047)
Train: 574 [ 100/1251 (  8%)]  Loss: 3.327 (3.33)  Time: 0.673s, 1521.96/s  (0.708s, 1445.41/s)  LR: 1.458e-05  Data: 0.010 (0.029)
Train: 574 [ 150/1251 ( 12%)]  Loss: 3.623 (3.41)  Time: 0.699s, 1464.72/s  (0.702s, 1459.56/s)  LR: 1.458e-05  Data: 0.010 (0.023)
Train: 574 [ 200/1251 ( 16%)]  Loss: 3.225 (3.37)  Time: 0.672s, 1523.03/s  (0.698s, 1466.75/s)  LR: 1.458e-05  Data: 0.012 (0.019)
Train: 574 [ 250/1251 ( 20%)]  Loss: 2.799 (3.27)  Time: 0.668s, 1532.35/s  (0.698s, 1467.36/s)  LR: 1.458e-05  Data: 0.009 (0.018)
Train: 574 [ 300/1251 ( 24%)]  Loss: 3.150 (3.26)  Time: 0.673s, 1522.60/s  (0.697s, 1468.51/s)  LR: 1.458e-05  Data: 0.009 (0.017)
Train: 574 [ 350/1251 ( 28%)]  Loss: 3.110 (3.24)  Time: 0.707s, 1448.44/s  (0.696s, 1470.38/s)  LR: 1.458e-05  Data: 0.010 (0.016)
Train: 574 [ 400/1251 ( 32%)]  Loss: 3.170 (3.23)  Time: 0.674s, 1519.40/s  (0.695s, 1472.97/s)  LR: 1.458e-05  Data: 0.011 (0.015)
Train: 574 [ 450/1251 ( 36%)]  Loss: 2.938 (3.20)  Time: 0.665s, 1540.48/s  (0.695s, 1474.08/s)  LR: 1.458e-05  Data: 0.009 (0.014)
Train: 574 [ 500/1251 ( 40%)]  Loss: 3.333 (3.21)  Time: 0.665s, 1539.17/s  (0.694s, 1475.23/s)  LR: 1.458e-05  Data: 0.010 (0.014)
Train: 574 [ 550/1251 ( 44%)]  Loss: 3.147 (3.21)  Time: 0.674s, 1520.22/s  (0.694s, 1475.08/s)  LR: 1.458e-05  Data: 0.013 (0.014)
Train: 574 [ 600/1251 ( 48%)]  Loss: 2.861 (3.18)  Time: 0.705s, 1452.91/s  (0.694s, 1475.06/s)  LR: 1.458e-05  Data: 0.011 (0.013)
Train: 574 [ 650/1251 ( 52%)]  Loss: 3.212 (3.18)  Time: 0.672s, 1524.56/s  (0.694s, 1475.27/s)  LR: 1.458e-05  Data: 0.011 (0.013)
Train: 574 [ 700/1251 ( 56%)]  Loss: 3.427 (3.20)  Time: 0.675s, 1516.23/s  (0.693s, 1476.92/s)  LR: 1.458e-05  Data: 0.011 (0.013)
Train: 574 [ 750/1251 ( 60%)]  Loss: 3.106 (3.19)  Time: 0.667s, 1534.21/s  (0.693s, 1477.46/s)  LR: 1.458e-05  Data: 0.010 (0.013)
Train: 574 [ 800/1251 ( 64%)]  Loss: 3.365 (3.20)  Time: 0.700s, 1462.29/s  (0.693s, 1476.79/s)  LR: 1.458e-05  Data: 0.009 (0.013)
Train: 574 [ 850/1251 ( 68%)]  Loss: 2.981 (3.19)  Time: 0.680s, 1505.09/s  (0.693s, 1476.97/s)  LR: 1.458e-05  Data: 0.011 (0.013)
Train: 574 [ 900/1251 ( 72%)]  Loss: 3.078 (3.19)  Time: 0.727s, 1409.18/s  (0.693s, 1477.50/s)  LR: 1.458e-05  Data: 0.010 (0.013)
Train: 574 [ 950/1251 ( 76%)]  Loss: 3.178 (3.19)  Time: 0.674s, 1518.63/s  (0.693s, 1477.45/s)  LR: 1.458e-05  Data: 0.011 (0.012)
Train: 574 [1000/1251 ( 80%)]  Loss: 3.166 (3.18)  Time: 0.681s, 1502.79/s  (0.693s, 1477.29/s)  LR: 1.458e-05  Data: 0.009 (0.012)
Train: 574 [1050/1251 ( 84%)]  Loss: 3.235 (3.19)  Time: 0.671s, 1526.71/s  (0.693s, 1477.78/s)  LR: 1.458e-05  Data: 0.010 (0.012)
Train: 574 [1100/1251 ( 88%)]  Loss: 3.423 (3.20)  Time: 0.680s, 1505.68/s  (0.693s, 1478.65/s)  LR: 1.458e-05  Data: 0.010 (0.012)
Train: 574 [1150/1251 ( 92%)]  Loss: 3.578 (3.21)  Time: 0.709s, 1445.15/s  (0.692s, 1479.55/s)  LR: 1.458e-05  Data: 0.009 (0.012)
Train: 574 [1200/1251 ( 96%)]  Loss: 3.691 (3.23)  Time: 0.676s, 1515.75/s  (0.692s, 1479.45/s)  LR: 1.458e-05  Data: 0.011 (0.012)
Train: 574 [1250/1251 (100%)]  Loss: 3.492 (3.24)  Time: 0.657s, 1559.19/s  (0.692s, 1479.42/s)  LR: 1.458e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.565 (1.565)  Loss:  0.6748 (0.6748)  Acc@1: 91.8945 (91.8945)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.136 (0.572)  Loss:  0.7886 (1.1593)  Acc@1: 87.3821 (79.2040)  Acc@5: 97.0519 (94.4820)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-572.pth.tar', 79.2120000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-573.pth.tar', 79.20800013183593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-574.pth.tar', 79.20399994873047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-558.pth.tar', 79.20199995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-571.pth.tar', 79.17199992431641)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-557.pth.tar', 79.1480000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-553.pth.tar', 79.08400000488281)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-561.pth.tar', 79.075999921875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-569.pth.tar', 79.07000010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-560.pth.tar', 79.06200005126954)

Train: 575 [   0/1251 (  0%)]  Loss: 3.041 (3.04)  Time: 2.170s,  471.94/s  (2.170s,  471.94/s)  LR: 1.423e-05  Data: 1.554 (1.554)
Train: 575 [  50/1251 (  4%)]  Loss: 2.818 (2.93)  Time: 0.671s, 1526.02/s  (0.735s, 1392.32/s)  LR: 1.423e-05  Data: 0.010 (0.050)
Train: 575 [ 100/1251 (  8%)]  Loss: 2.929 (2.93)  Time: 0.703s, 1456.72/s  (0.712s, 1437.98/s)  LR: 1.423e-05  Data: 0.009 (0.030)
Train: 575 [ 150/1251 ( 12%)]  Loss: 3.072 (2.96)  Time: 0.724s, 1413.99/s  (0.705s, 1452.49/s)  LR: 1.423e-05  Data: 0.009 (0.024)
Train: 575 [ 200/1251 ( 16%)]  Loss: 3.461 (3.06)  Time: 0.715s, 1431.80/s  (0.701s, 1460.96/s)  LR: 1.423e-05  Data: 0.010 (0.020)
Train: 575 [ 250/1251 ( 20%)]  Loss: 3.207 (3.09)  Time: 0.721s, 1419.85/s  (0.699s, 1464.87/s)  LR: 1.423e-05  Data: 0.011 (0.018)
Train: 575 [ 300/1251 ( 24%)]  Loss: 3.383 (3.13)  Time: 0.685s, 1495.78/s  (0.698s, 1467.68/s)  LR: 1.423e-05  Data: 0.011 (0.017)
Train: 575 [ 350/1251 ( 28%)]  Loss: 3.085 (3.12)  Time: 0.702s, 1458.03/s  (0.697s, 1469.45/s)  LR: 1.423e-05  Data: 0.009 (0.016)
Train: 575 [ 400/1251 ( 32%)]  Loss: 3.309 (3.14)  Time: 0.672s, 1523.47/s  (0.696s, 1470.70/s)  LR: 1.423e-05  Data: 0.010 (0.016)
Train: 575 [ 450/1251 ( 36%)]  Loss: 2.984 (3.13)  Time: 0.693s, 1478.67/s  (0.695s, 1472.78/s)  LR: 1.423e-05  Data: 0.009 (0.015)
Train: 575 [ 500/1251 ( 40%)]  Loss: 3.368 (3.15)  Time: 0.691s, 1481.08/s  (0.694s, 1474.49/s)  LR: 1.423e-05  Data: 0.010 (0.014)
Train: 575 [ 550/1251 ( 44%)]  Loss: 3.020 (3.14)  Time: 0.677s, 1513.14/s  (0.694s, 1474.69/s)  LR: 1.423e-05  Data: 0.009 (0.014)
Train: 575 [ 600/1251 ( 48%)]  Loss: 3.118 (3.14)  Time: 0.703s, 1456.26/s  (0.695s, 1474.02/s)  LR: 1.423e-05  Data: 0.013 (0.014)
Train: 575 [ 650/1251 ( 52%)]  Loss: 3.142 (3.14)  Time: 0.671s, 1525.33/s  (0.695s, 1474.38/s)  LR: 1.423e-05  Data: 0.010 (0.014)
Train: 575 [ 700/1251 ( 56%)]  Loss: 2.935 (3.12)  Time: 0.680s, 1506.86/s  (0.694s, 1475.12/s)  LR: 1.423e-05  Data: 0.009 (0.013)
Train: 575 [ 750/1251 ( 60%)]  Loss: 3.155 (3.13)  Time: 0.684s, 1497.87/s  (0.694s, 1476.43/s)  LR: 1.423e-05  Data: 0.011 (0.013)
Train: 575 [ 800/1251 ( 64%)]  Loss: 3.167 (3.13)  Time: 0.670s, 1528.94/s  (0.693s, 1477.28/s)  LR: 1.423e-05  Data: 0.010 (0.013)
Train: 575 [ 850/1251 ( 68%)]  Loss: 3.582 (3.15)  Time: 0.671s, 1525.65/s  (0.693s, 1478.16/s)  LR: 1.423e-05  Data: 0.010 (0.013)
Train: 575 [ 900/1251 ( 72%)]  Loss: 3.429 (3.17)  Time: 0.668s, 1533.20/s  (0.693s, 1478.42/s)  LR: 1.423e-05  Data: 0.011 (0.013)
Train: 575 [ 950/1251 ( 76%)]  Loss: 3.184 (3.17)  Time: 0.708s, 1447.22/s  (0.693s, 1478.30/s)  LR: 1.423e-05  Data: 0.011 (0.013)
Train: 575 [1000/1251 ( 80%)]  Loss: 3.556 (3.19)  Time: 0.704s, 1454.93/s  (0.693s, 1477.52/s)  LR: 1.423e-05  Data: 0.009 (0.013)
Train: 575 [1050/1251 ( 84%)]  Loss: 3.082 (3.18)  Time: 0.751s, 1364.29/s  (0.693s, 1477.96/s)  LR: 1.423e-05  Data: 0.010 (0.012)
Train: 575 [1100/1251 ( 88%)]  Loss: 3.166 (3.18)  Time: 0.680s, 1504.87/s  (0.693s, 1478.26/s)  LR: 1.423e-05  Data: 0.010 (0.012)
Train: 575 [1150/1251 ( 92%)]  Loss: 3.347 (3.19)  Time: 0.670s, 1528.27/s  (0.693s, 1478.25/s)  LR: 1.423e-05  Data: 0.010 (0.012)
Train: 575 [1200/1251 ( 96%)]  Loss: 3.433 (3.20)  Time: 0.672s, 1524.94/s  (0.693s, 1478.69/s)  LR: 1.423e-05  Data: 0.011 (0.012)
Train: 575 [1250/1251 (100%)]  Loss: 3.530 (3.21)  Time: 0.693s, 1476.85/s  (0.693s, 1478.22/s)  LR: 1.423e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.485 (1.485)  Loss:  0.7231 (0.7231)  Acc@1: 91.6016 (91.6016)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.137 (0.575)  Loss:  0.8130 (1.1911)  Acc@1: 87.5000 (79.1700)  Acc@5: 97.1698 (94.4280)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-572.pth.tar', 79.2120000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-573.pth.tar', 79.20800013183593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-574.pth.tar', 79.20399994873047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-558.pth.tar', 79.20199995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-571.pth.tar', 79.17199992431641)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-575.pth.tar', 79.17000012939454)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-557.pth.tar', 79.1480000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-553.pth.tar', 79.08400000488281)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-561.pth.tar', 79.075999921875)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-569.pth.tar', 79.07000010498047)

Train: 576 [   0/1251 (  0%)]  Loss: 3.515 (3.51)  Time: 2.190s,  467.48/s  (2.190s,  467.48/s)  LR: 1.390e-05  Data: 1.575 (1.575)
Train: 576 [  50/1251 (  4%)]  Loss: 2.723 (3.12)  Time: 0.681s, 1504.03/s  (0.725s, 1411.61/s)  LR: 1.390e-05  Data: 0.011 (0.049)
Train: 576 [ 100/1251 (  8%)]  Loss: 3.202 (3.15)  Time: 0.748s, 1369.16/s  (0.711s, 1440.64/s)  LR: 1.390e-05  Data: 0.009 (0.030)
Train: 576 [ 150/1251 ( 12%)]  Loss: 3.172 (3.15)  Time: 0.691s, 1481.43/s  (0.707s, 1448.51/s)  LR: 1.390e-05  Data: 0.011 (0.023)
Train: 576 [ 200/1251 ( 16%)]  Loss: 3.220 (3.17)  Time: 0.674s, 1519.22/s  (0.704s, 1454.75/s)  LR: 1.390e-05  Data: 0.011 (0.020)
Train: 576 [ 250/1251 ( 20%)]  Loss: 3.236 (3.18)  Time: 0.689s, 1486.10/s  (0.703s, 1457.14/s)  LR: 1.390e-05  Data: 0.013 (0.018)
Train: 576 [ 300/1251 ( 24%)]  Loss: 3.108 (3.17)  Time: 0.707s, 1449.32/s  (0.704s, 1454.56/s)  LR: 1.390e-05  Data: 0.014 (0.017)
Train: 576 [ 350/1251 ( 28%)]  Loss: 3.155 (3.17)  Time: 0.678s, 1510.91/s  (0.706s, 1451.32/s)  LR: 1.390e-05  Data: 0.013 (0.017)
Train: 576 [ 400/1251 ( 32%)]  Loss: 3.045 (3.15)  Time: 0.675s, 1516.26/s  (0.705s, 1452.65/s)  LR: 1.390e-05  Data: 0.009 (0.016)
Train: 576 [ 450/1251 ( 36%)]  Loss: 3.171 (3.15)  Time: 0.680s, 1506.11/s  (0.703s, 1457.64/s)  LR: 1.390e-05  Data: 0.010 (0.015)
Train: 576 [ 500/1251 ( 40%)]  Loss: 3.223 (3.16)  Time: 0.672s, 1523.05/s  (0.700s, 1463.43/s)  LR: 1.390e-05  Data: 0.011 (0.015)
Train: 576 [ 550/1251 ( 44%)]  Loss: 3.235 (3.17)  Time: 0.672s, 1523.12/s  (0.698s, 1466.49/s)  LR: 1.390e-05  Data: 0.011 (0.015)
Train: 576 [ 600/1251 ( 48%)]  Loss: 3.411 (3.19)  Time: 0.720s, 1422.71/s  (0.698s, 1467.40/s)  LR: 1.390e-05  Data: 0.013 (0.014)
Train: 576 [ 650/1251 ( 52%)]  Loss: 3.073 (3.18)  Time: 0.673s, 1521.68/s  (0.697s, 1468.40/s)  LR: 1.390e-05  Data: 0.011 (0.014)
Train: 576 [ 700/1251 ( 56%)]  Loss: 3.447 (3.20)  Time: 0.724s, 1414.78/s  (0.697s, 1468.77/s)  LR: 1.390e-05  Data: 0.014 (0.014)
Train: 576 [ 750/1251 ( 60%)]  Loss: 2.930 (3.18)  Time: 0.696s, 1471.17/s  (0.697s, 1469.70/s)  LR: 1.390e-05  Data: 0.011 (0.013)
Train: 576 [ 800/1251 ( 64%)]  Loss: 3.240 (3.18)  Time: 0.667s, 1535.96/s  (0.696s, 1470.31/s)  LR: 1.390e-05  Data: 0.011 (0.013)
Train: 576 [ 850/1251 ( 68%)]  Loss: 3.170 (3.18)  Time: 0.672s, 1522.96/s  (0.696s, 1471.30/s)  LR: 1.390e-05  Data: 0.011 (0.013)
Train: 576 [ 900/1251 ( 72%)]  Loss: 3.528 (3.20)  Time: 0.674s, 1519.71/s  (0.696s, 1471.61/s)  LR: 1.390e-05  Data: 0.010 (0.013)
Train: 576 [ 950/1251 ( 76%)]  Loss: 3.186 (3.20)  Time: 0.706s, 1450.12/s  (0.696s, 1471.02/s)  LR: 1.390e-05  Data: 0.011 (0.013)
Train: 576 [1000/1251 ( 80%)]  Loss: 3.375 (3.21)  Time: 0.735s, 1392.58/s  (0.696s, 1470.94/s)  LR: 1.390e-05  Data: 0.015 (0.013)
Train: 576 [1050/1251 ( 84%)]  Loss: 3.263 (3.21)  Time: 0.721s, 1420.40/s  (0.696s, 1471.29/s)  LR: 1.390e-05  Data: 0.011 (0.013)
Train: 576 [1100/1251 ( 88%)]  Loss: 3.296 (3.21)  Time: 0.665s, 1539.47/s  (0.696s, 1471.87/s)  LR: 1.390e-05  Data: 0.009 (0.012)
Train: 576 [1150/1251 ( 92%)]  Loss: 3.349 (3.22)  Time: 0.694s, 1475.07/s  (0.696s, 1472.02/s)  LR: 1.390e-05  Data: 0.010 (0.012)
Train: 576 [1200/1251 ( 96%)]  Loss: 3.148 (3.22)  Time: 0.672s, 1524.10/s  (0.696s, 1472.06/s)  LR: 1.390e-05  Data: 0.011 (0.012)
Train: 576 [1250/1251 (100%)]  Loss: 3.370 (3.22)  Time: 0.693s, 1478.46/s  (0.696s, 1472.17/s)  LR: 1.390e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.620 (1.620)  Loss:  0.7129 (0.7129)  Acc@1: 91.5039 (91.5039)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.136 (0.566)  Loss:  0.8306 (1.1979)  Acc@1: 86.9104 (79.1200)  Acc@5: 97.1698 (94.4280)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-572.pth.tar', 79.2120000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-573.pth.tar', 79.20800013183593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-574.pth.tar', 79.20399994873047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-558.pth.tar', 79.20199995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-571.pth.tar', 79.17199992431641)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-575.pth.tar', 79.17000012939454)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-557.pth.tar', 79.1480000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-576.pth.tar', 79.1200000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-553.pth.tar', 79.08400000488281)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-561.pth.tar', 79.075999921875)

Train: 577 [   0/1251 (  0%)]  Loss: 3.235 (3.24)  Time: 2.213s,  462.78/s  (2.213s,  462.78/s)  LR: 1.359e-05  Data: 1.597 (1.597)
Train: 577 [  50/1251 (  4%)]  Loss: 3.038 (3.14)  Time: 0.724s, 1413.44/s  (0.730s, 1403.55/s)  LR: 1.359e-05  Data: 0.011 (0.049)
Train: 577 [ 100/1251 (  8%)]  Loss: 2.993 (3.09)  Time: 0.706s, 1449.70/s  (0.718s, 1427.10/s)  LR: 1.359e-05  Data: 0.010 (0.030)
Train: 577 [ 150/1251 ( 12%)]  Loss: 2.867 (3.03)  Time: 0.671s, 1526.80/s  (0.709s, 1443.88/s)  LR: 1.359e-05  Data: 0.009 (0.024)
Train: 577 [ 200/1251 ( 16%)]  Loss: 3.259 (3.08)  Time: 0.672s, 1523.01/s  (0.705s, 1453.31/s)  LR: 1.359e-05  Data: 0.010 (0.020)
Train: 577 [ 250/1251 ( 20%)]  Loss: 3.195 (3.10)  Time: 0.706s, 1449.97/s  (0.703s, 1457.44/s)  LR: 1.359e-05  Data: 0.009 (0.018)
Train: 577 [ 300/1251 ( 24%)]  Loss: 2.927 (3.07)  Time: 0.730s, 1403.31/s  (0.701s, 1461.20/s)  LR: 1.359e-05  Data: 0.009 (0.017)
Train: 577 [ 350/1251 ( 28%)]  Loss: 3.278 (3.10)  Time: 0.699s, 1463.90/s  (0.700s, 1463.65/s)  LR: 1.359e-05  Data: 0.009 (0.016)
Train: 577 [ 400/1251 ( 32%)]  Loss: 3.208 (3.11)  Time: 0.672s, 1523.45/s  (0.700s, 1463.89/s)  LR: 1.359e-05  Data: 0.010 (0.015)
Train: 577 [ 450/1251 ( 36%)]  Loss: 3.327 (3.13)  Time: 0.701s, 1460.84/s  (0.698s, 1466.65/s)  LR: 1.359e-05  Data: 0.009 (0.015)
Train: 577 [ 500/1251 ( 40%)]  Loss: 3.147 (3.13)  Time: 0.671s, 1525.76/s  (0.698s, 1467.00/s)  LR: 1.359e-05  Data: 0.010 (0.014)
Train: 577 [ 550/1251 ( 44%)]  Loss: 3.300 (3.15)  Time: 0.671s, 1526.95/s  (0.697s, 1468.87/s)  LR: 1.359e-05  Data: 0.011 (0.014)
Train: 577 [ 600/1251 ( 48%)]  Loss: 3.128 (3.15)  Time: 0.754s, 1358.31/s  (0.696s, 1470.23/s)  LR: 1.359e-05  Data: 0.010 (0.014)
Train: 577 [ 650/1251 ( 52%)]  Loss: 3.255 (3.15)  Time: 0.671s, 1525.28/s  (0.696s, 1471.41/s)  LR: 1.359e-05  Data: 0.010 (0.013)
Train: 577 [ 700/1251 ( 56%)]  Loss: 3.177 (3.16)  Time: 0.672s, 1523.82/s  (0.695s, 1472.34/s)  LR: 1.359e-05  Data: 0.010 (0.013)
Train: 577 [ 750/1251 ( 60%)]  Loss: 2.902 (3.14)  Time: 0.716s, 1429.83/s  (0.695s, 1473.83/s)  LR: 1.359e-05  Data: 0.009 (0.013)
Train: 577 [ 800/1251 ( 64%)]  Loss: 3.502 (3.16)  Time: 0.669s, 1530.15/s  (0.695s, 1474.39/s)  LR: 1.359e-05  Data: 0.010 (0.013)
Train: 577 [ 850/1251 ( 68%)]  Loss: 3.177 (3.16)  Time: 0.736s, 1391.54/s  (0.694s, 1474.47/s)  LR: 1.359e-05  Data: 0.009 (0.013)
Train: 577 [ 900/1251 ( 72%)]  Loss: 2.983 (3.15)  Time: 0.674s, 1518.23/s  (0.694s, 1474.95/s)  LR: 1.359e-05  Data: 0.012 (0.013)
Train: 577 [ 950/1251 ( 76%)]  Loss: 3.229 (3.16)  Time: 0.672s, 1523.57/s  (0.694s, 1475.61/s)  LR: 1.359e-05  Data: 0.010 (0.013)
Train: 577 [1000/1251 ( 80%)]  Loss: 3.339 (3.16)  Time: 0.672s, 1523.91/s  (0.694s, 1475.77/s)  LR: 1.359e-05  Data: 0.010 (0.012)
Train: 577 [1050/1251 ( 84%)]  Loss: 2.932 (3.15)  Time: 0.705s, 1451.85/s  (0.694s, 1476.27/s)  LR: 1.359e-05  Data: 0.009 (0.012)
Train: 577 [1100/1251 ( 88%)]  Loss: 3.393 (3.16)  Time: 0.703s, 1457.03/s  (0.694s, 1476.30/s)  LR: 1.359e-05  Data: 0.009 (0.012)
Train: 577 [1150/1251 ( 92%)]  Loss: 3.195 (3.17)  Time: 0.672s, 1523.73/s  (0.693s, 1476.76/s)  LR: 1.359e-05  Data: 0.010 (0.012)
Train: 577 [1200/1251 ( 96%)]  Loss: 2.821 (3.15)  Time: 0.672s, 1524.17/s  (0.693s, 1477.47/s)  LR: 1.359e-05  Data: 0.010 (0.012)
Train: 577 [1250/1251 (100%)]  Loss: 3.279 (3.16)  Time: 0.661s, 1549.18/s  (0.693s, 1477.84/s)  LR: 1.359e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.504 (1.504)  Loss:  0.6865 (0.6865)  Acc@1: 91.9922 (91.9922)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.136 (0.570)  Loss:  0.7988 (1.1752)  Acc@1: 87.1462 (79.2860)  Acc@5: 96.9340 (94.4980)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-577.pth.tar', 79.28600010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-572.pth.tar', 79.2120000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-573.pth.tar', 79.20800013183593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-574.pth.tar', 79.20399994873047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-558.pth.tar', 79.20199995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-571.pth.tar', 79.17199992431641)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-575.pth.tar', 79.17000012939454)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-557.pth.tar', 79.1480000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-576.pth.tar', 79.1200000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-553.pth.tar', 79.08400000488281)

Train: 578 [   0/1251 (  0%)]  Loss: 3.009 (3.01)  Time: 2.297s,  445.86/s  (2.297s,  445.86/s)  LR: 1.328e-05  Data: 1.620 (1.620)
Train: 578 [  50/1251 (  4%)]  Loss: 3.384 (3.20)  Time: 0.672s, 1524.57/s  (0.726s, 1410.81/s)  LR: 1.328e-05  Data: 0.009 (0.051)
Train: 578 [ 100/1251 (  8%)]  Loss: 3.351 (3.25)  Time: 0.673s, 1520.56/s  (0.706s, 1449.83/s)  LR: 1.328e-05  Data: 0.010 (0.031)
Train: 578 [ 150/1251 ( 12%)]  Loss: 2.998 (3.19)  Time: 0.696s, 1471.47/s  (0.704s, 1454.94/s)  LR: 1.328e-05  Data: 0.011 (0.024)
Train: 578 [ 200/1251 ( 16%)]  Loss: 2.986 (3.15)  Time: 0.679s, 1507.50/s  (0.701s, 1461.02/s)  LR: 1.328e-05  Data: 0.010 (0.021)
Train: 578 [ 250/1251 ( 20%)]  Loss: 3.203 (3.15)  Time: 0.680s, 1506.36/s  (0.699s, 1465.62/s)  LR: 1.328e-05  Data: 0.011 (0.019)
Train: 578 [ 300/1251 ( 24%)]  Loss: 2.934 (3.12)  Time: 0.675s, 1516.48/s  (0.697s, 1468.49/s)  LR: 1.328e-05  Data: 0.010 (0.017)
Train: 578 [ 350/1251 ( 28%)]  Loss: 2.740 (3.08)  Time: 0.755s, 1355.82/s  (0.696s, 1470.51/s)  LR: 1.328e-05  Data: 0.011 (0.016)
Train: 578 [ 400/1251 ( 32%)]  Loss: 3.351 (3.11)  Time: 0.668s, 1532.42/s  (0.695s, 1472.93/s)  LR: 1.328e-05  Data: 0.010 (0.016)
Train: 578 [ 450/1251 ( 36%)]  Loss: 3.611 (3.16)  Time: 0.684s, 1497.76/s  (0.695s, 1473.71/s)  LR: 1.328e-05  Data: 0.016 (0.015)
Train: 578 [ 500/1251 ( 40%)]  Loss: 3.135 (3.15)  Time: 0.733s, 1397.86/s  (0.694s, 1474.83/s)  LR: 1.328e-05  Data: 0.009 (0.015)
Train: 578 [ 550/1251 ( 44%)]  Loss: 3.292 (3.17)  Time: 0.673s, 1522.13/s  (0.694s, 1475.31/s)  LR: 1.328e-05  Data: 0.011 (0.014)
Train: 578 [ 600/1251 ( 48%)]  Loss: 2.886 (3.14)  Time: 0.674s, 1520.19/s  (0.694s, 1476.07/s)  LR: 1.328e-05  Data: 0.011 (0.014)
Train: 578 [ 650/1251 ( 52%)]  Loss: 2.847 (3.12)  Time: 0.672s, 1524.57/s  (0.693s, 1477.49/s)  LR: 1.328e-05  Data: 0.010 (0.014)
Train: 578 [ 700/1251 ( 56%)]  Loss: 3.072 (3.12)  Time: 0.671s, 1527.03/s  (0.693s, 1478.37/s)  LR: 1.328e-05  Data: 0.012 (0.013)
Train: 578 [ 750/1251 ( 60%)]  Loss: 3.102 (3.12)  Time: 0.671s, 1526.45/s  (0.692s, 1478.98/s)  LR: 1.328e-05  Data: 0.010 (0.013)
Train: 578 [ 800/1251 ( 64%)]  Loss: 3.287 (3.13)  Time: 0.671s, 1525.18/s  (0.692s, 1478.82/s)  LR: 1.328e-05  Data: 0.013 (0.013)
Train: 578 [ 850/1251 ( 68%)]  Loss: 2.968 (3.12)  Time: 0.709s, 1443.94/s  (0.693s, 1478.21/s)  LR: 1.328e-05  Data: 0.009 (0.013)
Train: 578 [ 900/1251 ( 72%)]  Loss: 3.054 (3.12)  Time: 0.672s, 1523.46/s  (0.693s, 1478.09/s)  LR: 1.328e-05  Data: 0.013 (0.013)
Train: 578 [ 950/1251 ( 76%)]  Loss: 3.337 (3.13)  Time: 0.673s, 1520.59/s  (0.693s, 1477.43/s)  LR: 1.328e-05  Data: 0.009 (0.013)
Train: 578 [1000/1251 ( 80%)]  Loss: 3.496 (3.14)  Time: 0.680s, 1505.14/s  (0.693s, 1477.89/s)  LR: 1.328e-05  Data: 0.011 (0.013)
Train: 578 [1050/1251 ( 84%)]  Loss: 3.240 (3.15)  Time: 0.673s, 1521.93/s  (0.693s, 1478.60/s)  LR: 1.328e-05  Data: 0.011 (0.012)
Train: 578 [1100/1251 ( 88%)]  Loss: 3.100 (3.15)  Time: 0.668s, 1532.21/s  (0.693s, 1478.61/s)  LR: 1.328e-05  Data: 0.012 (0.012)
Train: 578 [1150/1251 ( 92%)]  Loss: 3.105 (3.15)  Time: 0.720s, 1422.81/s  (0.692s, 1478.82/s)  LR: 1.328e-05  Data: 0.016 (0.012)
Train: 578 [1200/1251 ( 96%)]  Loss: 2.940 (3.14)  Time: 0.679s, 1507.26/s  (0.693s, 1478.38/s)  LR: 1.328e-05  Data: 0.012 (0.012)
Train: 578 [1250/1251 (100%)]  Loss: 3.082 (3.13)  Time: 0.658s, 1557.28/s  (0.693s, 1478.47/s)  LR: 1.328e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.576 (1.576)  Loss:  0.7354 (0.7354)  Acc@1: 92.0898 (92.0898)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.136 (0.571)  Loss:  0.8506 (1.2011)  Acc@1: 87.3821 (79.2580)  Acc@5: 97.2877 (94.5220)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-577.pth.tar', 79.28600010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-578.pth.tar', 79.258000078125)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-572.pth.tar', 79.2120000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-573.pth.tar', 79.20800013183593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-574.pth.tar', 79.20399994873047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-558.pth.tar', 79.20199995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-571.pth.tar', 79.17199992431641)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-575.pth.tar', 79.17000012939454)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-557.pth.tar', 79.1480000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-576.pth.tar', 79.1200000024414)

Train: 579 [   0/1251 (  0%)]  Loss: 3.035 (3.04)  Time: 2.282s,  448.79/s  (2.282s,  448.79/s)  LR: 1.299e-05  Data: 1.666 (1.666)
Train: 579 [  50/1251 (  4%)]  Loss: 3.120 (3.08)  Time: 0.671s, 1526.45/s  (0.738s, 1387.13/s)  LR: 1.299e-05  Data: 0.010 (0.050)
Train: 579 [ 100/1251 (  8%)]  Loss: 3.360 (3.17)  Time: 0.672s, 1522.95/s  (0.714s, 1434.66/s)  LR: 1.299e-05  Data: 0.010 (0.030)
Train: 579 [ 150/1251 ( 12%)]  Loss: 3.228 (3.19)  Time: 0.671s, 1526.60/s  (0.705s, 1453.32/s)  LR: 1.299e-05  Data: 0.010 (0.024)
Train: 579 [ 200/1251 ( 16%)]  Loss: 3.419 (3.23)  Time: 0.685s, 1494.35/s  (0.702s, 1457.98/s)  LR: 1.299e-05  Data: 0.010 (0.020)
Train: 579 [ 250/1251 ( 20%)]  Loss: 3.264 (3.24)  Time: 0.733s, 1396.58/s  (0.701s, 1460.84/s)  LR: 1.299e-05  Data: 0.010 (0.018)
Train: 579 [ 300/1251 ( 24%)]  Loss: 3.026 (3.21)  Time: 0.667s, 1536.09/s  (0.700s, 1462.97/s)  LR: 1.299e-05  Data: 0.011 (0.017)
Train: 579 [ 350/1251 ( 28%)]  Loss: 3.141 (3.20)  Time: 0.672s, 1524.73/s  (0.699s, 1465.63/s)  LR: 1.299e-05  Data: 0.011 (0.016)
Train: 579 [ 400/1251 ( 32%)]  Loss: 3.156 (3.19)  Time: 0.673s, 1521.38/s  (0.697s, 1468.17/s)  LR: 1.299e-05  Data: 0.009 (0.015)
Train: 579 [ 450/1251 ( 36%)]  Loss: 3.087 (3.18)  Time: 0.703s, 1456.70/s  (0.697s, 1468.80/s)  LR: 1.299e-05  Data: 0.011 (0.015)
Train: 579 [ 500/1251 ( 40%)]  Loss: 3.194 (3.18)  Time: 0.706s, 1449.90/s  (0.697s, 1469.25/s)  LR: 1.299e-05  Data: 0.011 (0.014)
Train: 579 [ 550/1251 ( 44%)]  Loss: 3.404 (3.20)  Time: 0.700s, 1461.86/s  (0.697s, 1469.38/s)  LR: 1.299e-05  Data: 0.009 (0.014)
Train: 579 [ 600/1251 ( 48%)]  Loss: 3.464 (3.22)  Time: 0.676s, 1514.48/s  (0.696s, 1471.67/s)  LR: 1.299e-05  Data: 0.011 (0.014)
Train: 579 [ 650/1251 ( 52%)]  Loss: 3.063 (3.21)  Time: 0.675s, 1517.32/s  (0.696s, 1471.48/s)  LR: 1.299e-05  Data: 0.011 (0.013)
Train: 579 [ 700/1251 ( 56%)]  Loss: 3.060 (3.20)  Time: 0.680s, 1506.26/s  (0.695s, 1472.67/s)  LR: 1.299e-05  Data: 0.010 (0.013)
Train: 579 [ 750/1251 ( 60%)]  Loss: 3.386 (3.21)  Time: 0.751s, 1363.65/s  (0.695s, 1473.92/s)  LR: 1.299e-05  Data: 0.010 (0.013)
Train: 579 [ 800/1251 ( 64%)]  Loss: 3.087 (3.21)  Time: 0.719s, 1424.99/s  (0.694s, 1475.39/s)  LR: 1.299e-05  Data: 0.011 (0.013)
Train: 579 [ 850/1251 ( 68%)]  Loss: 3.323 (3.21)  Time: 0.702s, 1457.81/s  (0.694s, 1475.85/s)  LR: 1.299e-05  Data: 0.009 (0.013)
Train: 579 [ 900/1251 ( 72%)]  Loss: 3.148 (3.21)  Time: 0.715s, 1431.21/s  (0.694s, 1476.31/s)  LR: 1.299e-05  Data: 0.010 (0.013)
Train: 579 [ 950/1251 ( 76%)]  Loss: 3.165 (3.21)  Time: 0.672s, 1523.71/s  (0.693s, 1477.44/s)  LR: 1.299e-05  Data: 0.010 (0.012)
Train: 579 [1000/1251 ( 80%)]  Loss: 2.947 (3.19)  Time: 0.671s, 1525.67/s  (0.693s, 1477.03/s)  LR: 1.299e-05  Data: 0.013 (0.012)
Train: 579 [1050/1251 ( 84%)]  Loss: 3.029 (3.19)  Time: 0.702s, 1459.50/s  (0.693s, 1476.69/s)  LR: 1.299e-05  Data: 0.009 (0.012)
Train: 579 [1100/1251 ( 88%)]  Loss: 3.297 (3.19)  Time: 0.670s, 1529.34/s  (0.694s, 1476.40/s)  LR: 1.299e-05  Data: 0.010 (0.012)
Train: 579 [1150/1251 ( 92%)]  Loss: 3.245 (3.19)  Time: 0.705s, 1453.33/s  (0.693s, 1476.85/s)  LR: 1.299e-05  Data: 0.009 (0.012)
Train: 579 [1200/1251 ( 96%)]  Loss: 3.185 (3.19)  Time: 0.704s, 1453.66/s  (0.694s, 1476.13/s)  LR: 1.299e-05  Data: 0.010 (0.012)
Train: 579 [1250/1251 (100%)]  Loss: 3.355 (3.20)  Time: 0.653s, 1567.96/s  (0.694s, 1475.69/s)  LR: 1.299e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.502 (1.502)  Loss:  0.7148 (0.7148)  Acc@1: 91.9922 (91.9922)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.136 (0.577)  Loss:  0.8125 (1.1878)  Acc@1: 87.2642 (79.1440)  Acc@5: 97.2877 (94.5000)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-577.pth.tar', 79.28600010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-578.pth.tar', 79.258000078125)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-572.pth.tar', 79.2120000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-573.pth.tar', 79.20800013183593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-574.pth.tar', 79.20399994873047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-558.pth.tar', 79.20199995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-571.pth.tar', 79.17199992431641)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-575.pth.tar', 79.17000012939454)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-557.pth.tar', 79.1480000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-579.pth.tar', 79.14400002685547)

Train: 580 [   0/1251 (  0%)]  Loss: 3.058 (3.06)  Time: 2.347s,  436.27/s  (2.347s,  436.27/s)  LR: 1.271e-05  Data: 1.714 (1.714)
Train: 580 [  50/1251 (  4%)]  Loss: 2.993 (3.03)  Time: 0.679s, 1508.85/s  (0.730s, 1403.17/s)  LR: 1.271e-05  Data: 0.012 (0.050)
Train: 580 [ 100/1251 (  8%)]  Loss: 3.243 (3.10)  Time: 0.717s, 1429.08/s  (0.709s, 1443.98/s)  LR: 1.271e-05  Data: 0.010 (0.030)
Train: 580 [ 150/1251 ( 12%)]  Loss: 2.999 (3.07)  Time: 0.706s, 1449.43/s  (0.705s, 1453.46/s)  LR: 1.271e-05  Data: 0.010 (0.023)
Train: 580 [ 200/1251 ( 16%)]  Loss: 2.994 (3.06)  Time: 0.674s, 1519.00/s  (0.703s, 1456.59/s)  LR: 1.271e-05  Data: 0.013 (0.020)
Train: 580 [ 250/1251 ( 20%)]  Loss: 3.065 (3.06)  Time: 0.664s, 1541.87/s  (0.702s, 1458.82/s)  LR: 1.271e-05  Data: 0.009 (0.018)
Train: 580 [ 300/1251 ( 24%)]  Loss: 3.224 (3.08)  Time: 0.676s, 1515.16/s  (0.701s, 1461.46/s)  LR: 1.271e-05  Data: 0.011 (0.017)
Train: 580 [ 350/1251 ( 28%)]  Loss: 3.120 (3.09)  Time: 0.707s, 1448.71/s  (0.699s, 1464.23/s)  LR: 1.271e-05  Data: 0.011 (0.016)
Train: 580 [ 400/1251 ( 32%)]  Loss: 3.324 (3.11)  Time: 0.693s, 1478.34/s  (0.698s, 1467.28/s)  LR: 1.271e-05  Data: 0.012 (0.016)
Train: 580 [ 450/1251 ( 36%)]  Loss: 3.256 (3.13)  Time: 0.673s, 1521.10/s  (0.698s, 1468.09/s)  LR: 1.271e-05  Data: 0.011 (0.015)
Train: 580 [ 500/1251 ( 40%)]  Loss: 3.219 (3.14)  Time: 0.710s, 1441.81/s  (0.697s, 1468.44/s)  LR: 1.271e-05  Data: 0.009 (0.014)
Train: 580 [ 550/1251 ( 44%)]  Loss: 3.410 (3.16)  Time: 0.704s, 1454.31/s  (0.697s, 1469.21/s)  LR: 1.271e-05  Data: 0.010 (0.014)
Train: 580 [ 600/1251 ( 48%)]  Loss: 3.104 (3.15)  Time: 0.673s, 1520.70/s  (0.696s, 1470.65/s)  LR: 1.271e-05  Data: 0.009 (0.014)
Train: 580 [ 650/1251 ( 52%)]  Loss: 3.065 (3.15)  Time: 0.672s, 1523.39/s  (0.697s, 1469.11/s)  LR: 1.271e-05  Data: 0.011 (0.014)
Train: 580 [ 700/1251 ( 56%)]  Loss: 3.237 (3.15)  Time: 0.671s, 1525.05/s  (0.697s, 1470.07/s)  LR: 1.271e-05  Data: 0.009 (0.013)
Train: 580 [ 750/1251 ( 60%)]  Loss: 3.249 (3.16)  Time: 0.683s, 1499.97/s  (0.696s, 1471.36/s)  LR: 1.271e-05  Data: 0.011 (0.013)
Train: 580 [ 800/1251 ( 64%)]  Loss: 3.087 (3.16)  Time: 0.727s, 1408.16/s  (0.695s, 1472.49/s)  LR: 1.271e-05  Data: 0.010 (0.013)
Train: 580 [ 850/1251 ( 68%)]  Loss: 3.477 (3.17)  Time: 0.670s, 1527.30/s  (0.695s, 1472.49/s)  LR: 1.271e-05  Data: 0.010 (0.013)
Train: 580 [ 900/1251 ( 72%)]  Loss: 3.325 (3.18)  Time: 0.727s, 1408.45/s  (0.695s, 1472.71/s)  LR: 1.271e-05  Data: 0.011 (0.013)
Train: 580 [ 950/1251 ( 76%)]  Loss: 3.190 (3.18)  Time: 0.678s, 1509.49/s  (0.695s, 1473.59/s)  LR: 1.271e-05  Data: 0.010 (0.013)
Train: 580 [1000/1251 ( 80%)]  Loss: 3.186 (3.18)  Time: 0.692s, 1479.01/s  (0.695s, 1473.66/s)  LR: 1.271e-05  Data: 0.012 (0.012)
Train: 580 [1050/1251 ( 84%)]  Loss: 2.911 (3.17)  Time: 0.706s, 1451.21/s  (0.695s, 1473.96/s)  LR: 1.271e-05  Data: 0.010 (0.012)
Train: 580 [1100/1251 ( 88%)]  Loss: 3.263 (3.17)  Time: 0.715s, 1432.63/s  (0.695s, 1474.37/s)  LR: 1.271e-05  Data: 0.010 (0.012)
Train: 580 [1150/1251 ( 92%)]  Loss: 3.379 (3.18)  Time: 0.672s, 1523.88/s  (0.694s, 1474.76/s)  LR: 1.271e-05  Data: 0.011 (0.012)
Train: 580 [1200/1251 ( 96%)]  Loss: 3.232 (3.18)  Time: 0.691s, 1482.13/s  (0.694s, 1474.46/s)  LR: 1.271e-05  Data: 0.011 (0.012)
Train: 580 [1250/1251 (100%)]  Loss: 3.416 (3.19)  Time: 0.693s, 1477.80/s  (0.694s, 1474.59/s)  LR: 1.271e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.517 (1.517)  Loss:  0.7393 (0.7393)  Acc@1: 91.9922 (91.9922)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.136 (0.571)  Loss:  0.8682 (1.2201)  Acc@1: 87.3821 (79.1520)  Acc@5: 97.0519 (94.4280)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-577.pth.tar', 79.28600010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-578.pth.tar', 79.258000078125)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-572.pth.tar', 79.2120000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-573.pth.tar', 79.20800013183593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-574.pth.tar', 79.20399994873047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-558.pth.tar', 79.20199995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-571.pth.tar', 79.17199992431641)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-575.pth.tar', 79.17000012939454)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-580.pth.tar', 79.152000078125)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-557.pth.tar', 79.1480000024414)

Train: 581 [   0/1251 (  0%)]  Loss: 3.295 (3.30)  Time: 2.161s,  473.92/s  (2.161s,  473.92/s)  LR: 1.245e-05  Data: 1.544 (1.544)
Train: 581 [  50/1251 (  4%)]  Loss: 3.336 (3.32)  Time: 0.679s, 1508.37/s  (0.722s, 1418.35/s)  LR: 1.245e-05  Data: 0.009 (0.044)
Train: 581 [ 100/1251 (  8%)]  Loss: 3.144 (3.26)  Time: 0.701s, 1460.36/s  (0.708s, 1446.31/s)  LR: 1.245e-05  Data: 0.009 (0.027)
Train: 581 [ 150/1251 ( 12%)]  Loss: 3.071 (3.21)  Time: 0.668s, 1534.03/s  (0.701s, 1461.63/s)  LR: 1.245e-05  Data: 0.010 (0.022)
Train: 581 [ 200/1251 ( 16%)]  Loss: 2.839 (3.14)  Time: 0.676s, 1514.69/s  (0.699s, 1465.96/s)  LR: 1.245e-05  Data: 0.011 (0.019)
Train: 581 [ 250/1251 ( 20%)]  Loss: 3.561 (3.21)  Time: 0.713s, 1435.71/s  (0.698s, 1467.96/s)  LR: 1.245e-05  Data: 0.010 (0.017)
Train: 581 [ 300/1251 ( 24%)]  Loss: 2.922 (3.17)  Time: 0.805s, 1271.57/s  (0.698s, 1467.79/s)  LR: 1.245e-05  Data: 0.009 (0.016)
Train: 581 [ 350/1251 ( 28%)]  Loss: 3.124 (3.16)  Time: 0.674s, 1518.63/s  (0.698s, 1467.32/s)  LR: 1.245e-05  Data: 0.009 (0.015)
Train: 581 [ 400/1251 ( 32%)]  Loss: 2.991 (3.14)  Time: 0.672s, 1524.28/s  (0.698s, 1467.65/s)  LR: 1.245e-05  Data: 0.010 (0.015)
Train: 581 [ 450/1251 ( 36%)]  Loss: 3.294 (3.16)  Time: 0.679s, 1507.60/s  (0.697s, 1469.64/s)  LR: 1.245e-05  Data: 0.009 (0.014)
Train: 581 [ 500/1251 ( 40%)]  Loss: 3.225 (3.16)  Time: 0.674s, 1518.88/s  (0.696s, 1470.49/s)  LR: 1.245e-05  Data: 0.010 (0.014)
Train: 581 [ 550/1251 ( 44%)]  Loss: 3.121 (3.16)  Time: 0.673s, 1521.34/s  (0.696s, 1470.44/s)  LR: 1.245e-05  Data: 0.009 (0.014)
Train: 581 [ 600/1251 ( 48%)]  Loss: 3.339 (3.17)  Time: 0.759s, 1349.50/s  (0.696s, 1471.94/s)  LR: 1.245e-05  Data: 0.010 (0.013)
Train: 581 [ 650/1251 ( 52%)]  Loss: 3.256 (3.18)  Time: 0.670s, 1528.10/s  (0.695s, 1472.45/s)  LR: 1.245e-05  Data: 0.010 (0.013)
Train: 581 [ 700/1251 ( 56%)]  Loss: 3.276 (3.19)  Time: 0.741s, 1381.67/s  (0.695s, 1474.02/s)  LR: 1.245e-05  Data: 0.009 (0.013)
Train: 581 [ 750/1251 ( 60%)]  Loss: 3.585 (3.21)  Time: 0.686s, 1493.29/s  (0.695s, 1474.33/s)  LR: 1.245e-05  Data: 0.010 (0.013)
Train: 581 [ 800/1251 ( 64%)]  Loss: 2.856 (3.19)  Time: 0.672s, 1524.03/s  (0.694s, 1474.70/s)  LR: 1.245e-05  Data: 0.012 (0.013)
Train: 581 [ 850/1251 ( 68%)]  Loss: 3.297 (3.20)  Time: 0.724s, 1415.24/s  (0.694s, 1474.88/s)  LR: 1.245e-05  Data: 0.015 (0.013)
Train: 581 [ 900/1251 ( 72%)]  Loss: 3.087 (3.19)  Time: 0.680s, 1505.42/s  (0.694s, 1474.90/s)  LR: 1.245e-05  Data: 0.009 (0.012)
Train: 581 [ 950/1251 ( 76%)]  Loss: 3.182 (3.19)  Time: 0.709s, 1444.90/s  (0.694s, 1475.25/s)  LR: 1.245e-05  Data: 0.012 (0.012)
Train: 581 [1000/1251 ( 80%)]  Loss: 3.051 (3.18)  Time: 0.703s, 1455.92/s  (0.694s, 1475.28/s)  LR: 1.245e-05  Data: 0.009 (0.012)
Train: 581 [1050/1251 ( 84%)]  Loss: 3.491 (3.20)  Time: 0.673s, 1522.03/s  (0.694s, 1475.04/s)  LR: 1.245e-05  Data: 0.010 (0.012)
Train: 581 [1100/1251 ( 88%)]  Loss: 3.030 (3.19)  Time: 0.673s, 1521.57/s  (0.694s, 1474.52/s)  LR: 1.245e-05  Data: 0.011 (0.012)
Train: 581 [1150/1251 ( 92%)]  Loss: 3.469 (3.20)  Time: 0.766s, 1337.21/s  (0.694s, 1475.08/s)  LR: 1.245e-05  Data: 0.009 (0.012)
Train: 581 [1200/1251 ( 96%)]  Loss: 3.222 (3.20)  Time: 0.796s, 1286.01/s  (0.694s, 1475.00/s)  LR: 1.245e-05  Data: 0.013 (0.012)
Train: 581 [1250/1251 (100%)]  Loss: 3.120 (3.20)  Time: 0.660s, 1551.06/s  (0.694s, 1475.50/s)  LR: 1.245e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.519 (1.519)  Loss:  0.6245 (0.6245)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.136 (0.579)  Loss:  0.7544 (1.0993)  Acc@1: 86.6745 (79.3300)  Acc@5: 97.6415 (94.5460)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-581.pth.tar', 79.3300001586914)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-577.pth.tar', 79.28600010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-578.pth.tar', 79.258000078125)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-572.pth.tar', 79.2120000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-573.pth.tar', 79.20800013183593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-574.pth.tar', 79.20399994873047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-558.pth.tar', 79.20199995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-571.pth.tar', 79.17199992431641)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-575.pth.tar', 79.17000012939454)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-580.pth.tar', 79.152000078125)

Train: 582 [   0/1251 (  0%)]  Loss: 3.552 (3.55)  Time: 2.180s,  469.68/s  (2.180s,  469.68/s)  LR: 1.220e-05  Data: 1.560 (1.560)
Train: 582 [  50/1251 (  4%)]  Loss: 3.320 (3.44)  Time: 0.668s, 1533.35/s  (0.726s, 1410.84/s)  LR: 1.220e-05  Data: 0.011 (0.046)
Train: 582 [ 100/1251 (  8%)]  Loss: 3.032 (3.30)  Time: 0.673s, 1521.58/s  (0.707s, 1448.35/s)  LR: 1.220e-05  Data: 0.011 (0.028)
Train: 582 [ 150/1251 ( 12%)]  Loss: 3.140 (3.26)  Time: 0.709s, 1445.23/s  (0.704s, 1454.25/s)  LR: 1.220e-05  Data: 0.011 (0.022)
Train: 582 [ 200/1251 ( 16%)]  Loss: 3.177 (3.24)  Time: 0.675s, 1517.36/s  (0.702s, 1459.42/s)  LR: 1.220e-05  Data: 0.013 (0.019)
Train: 582 [ 250/1251 ( 20%)]  Loss: 2.910 (3.19)  Time: 0.705s, 1453.24/s  (0.699s, 1465.02/s)  LR: 1.220e-05  Data: 0.011 (0.018)
Train: 582 [ 300/1251 ( 24%)]  Loss: 3.333 (3.21)  Time: 0.703s, 1456.30/s  (0.697s, 1468.45/s)  LR: 1.220e-05  Data: 0.010 (0.017)
Train: 582 [ 350/1251 ( 28%)]  Loss: 2.998 (3.18)  Time: 0.721s, 1419.53/s  (0.698s, 1467.92/s)  LR: 1.220e-05  Data: 0.009 (0.016)
Train: 582 [ 400/1251 ( 32%)]  Loss: 3.357 (3.20)  Time: 0.673s, 1521.35/s  (0.696s, 1470.91/s)  LR: 1.220e-05  Data: 0.011 (0.015)
Train: 582 [ 450/1251 ( 36%)]  Loss: 2.972 (3.18)  Time: 0.720s, 1421.78/s  (0.696s, 1470.96/s)  LR: 1.220e-05  Data: 0.010 (0.015)
Train: 582 [ 500/1251 ( 40%)]  Loss: 3.181 (3.18)  Time: 0.717s, 1427.65/s  (0.696s, 1471.45/s)  LR: 1.220e-05  Data: 0.010 (0.014)
Train: 582 [ 550/1251 ( 44%)]  Loss: 2.876 (3.15)  Time: 0.680s, 1505.65/s  (0.695s, 1472.48/s)  LR: 1.220e-05  Data: 0.011 (0.014)
Train: 582 [ 600/1251 ( 48%)]  Loss: 3.405 (3.17)  Time: 0.701s, 1461.23/s  (0.695s, 1473.08/s)  LR: 1.220e-05  Data: 0.010 (0.014)
Train: 582 [ 650/1251 ( 52%)]  Loss: 3.292 (3.18)  Time: 0.757s, 1352.99/s  (0.694s, 1474.62/s)  LR: 1.220e-05  Data: 0.010 (0.013)
Train: 582 [ 700/1251 ( 56%)]  Loss: 3.455 (3.20)  Time: 0.719s, 1423.89/s  (0.694s, 1475.21/s)  LR: 1.220e-05  Data: 0.012 (0.013)
Train: 582 [ 750/1251 ( 60%)]  Loss: 3.221 (3.20)  Time: 0.682s, 1501.42/s  (0.694s, 1475.60/s)  LR: 1.220e-05  Data: 0.011 (0.013)
Train: 582 [ 800/1251 ( 64%)]  Loss: 3.123 (3.20)  Time: 0.670s, 1529.19/s  (0.694s, 1476.00/s)  LR: 1.220e-05  Data: 0.011 (0.013)
Train: 582 [ 850/1251 ( 68%)]  Loss: 3.294 (3.20)  Time: 0.673s, 1522.18/s  (0.694s, 1476.22/s)  LR: 1.220e-05  Data: 0.011 (0.013)
Train: 582 [ 900/1251 ( 72%)]  Loss: 3.337 (3.21)  Time: 0.713s, 1435.69/s  (0.694s, 1476.14/s)  LR: 1.220e-05  Data: 0.011 (0.013)
Train: 582 [ 950/1251 ( 76%)]  Loss: 2.990 (3.20)  Time: 0.686s, 1493.00/s  (0.694s, 1475.92/s)  LR: 1.220e-05  Data: 0.010 (0.012)
Train: 582 [1000/1251 ( 80%)]  Loss: 3.334 (3.20)  Time: 0.667s, 1534.80/s  (0.694s, 1476.24/s)  LR: 1.220e-05  Data: 0.009 (0.012)
Train: 582 [1050/1251 ( 84%)]  Loss: 3.222 (3.21)  Time: 0.706s, 1451.29/s  (0.693s, 1477.11/s)  LR: 1.220e-05  Data: 0.010 (0.012)
Train: 582 [1100/1251 ( 88%)]  Loss: 3.157 (3.20)  Time: 0.674s, 1518.66/s  (0.693s, 1477.76/s)  LR: 1.220e-05  Data: 0.012 (0.012)
Train: 582 [1150/1251 ( 92%)]  Loss: 3.166 (3.20)  Time: 0.723s, 1417.12/s  (0.693s, 1477.94/s)  LR: 1.220e-05  Data: 0.011 (0.012)
Train: 582 [1200/1251 ( 96%)]  Loss: 3.043 (3.20)  Time: 0.674s, 1519.63/s  (0.693s, 1477.62/s)  LR: 1.220e-05  Data: 0.011 (0.012)
Train: 582 [1250/1251 (100%)]  Loss: 3.109 (3.19)  Time: 0.779s, 1313.83/s  (0.693s, 1477.28/s)  LR: 1.220e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.475 (1.475)  Loss:  0.6768 (0.6768)  Acc@1: 92.0898 (92.0898)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.136 (0.580)  Loss:  0.8105 (1.1622)  Acc@1: 86.4387 (79.1840)  Acc@5: 97.0519 (94.5360)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-581.pth.tar', 79.3300001586914)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-577.pth.tar', 79.28600010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-578.pth.tar', 79.258000078125)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-572.pth.tar', 79.2120000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-573.pth.tar', 79.20800013183593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-574.pth.tar', 79.20399994873047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-558.pth.tar', 79.20199995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-582.pth.tar', 79.18400005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-571.pth.tar', 79.17199992431641)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-575.pth.tar', 79.17000012939454)

Train: 583 [   0/1251 (  0%)]  Loss: 3.332 (3.33)  Time: 2.239s,  457.32/s  (2.239s,  457.32/s)  LR: 1.196e-05  Data: 1.622 (1.622)
Train: 583 [  50/1251 (  4%)]  Loss: 3.239 (3.29)  Time: 0.730s, 1403.30/s  (0.734s, 1394.39/s)  LR: 1.196e-05  Data: 0.010 (0.049)
Train: 583 [ 100/1251 (  8%)]  Loss: 3.428 (3.33)  Time: 0.701s, 1459.97/s  (0.711s, 1439.95/s)  LR: 1.196e-05  Data: 0.010 (0.030)
Train: 583 [ 150/1251 ( 12%)]  Loss: 3.161 (3.29)  Time: 0.721s, 1420.49/s  (0.707s, 1448.22/s)  LR: 1.196e-05  Data: 0.009 (0.024)
Train: 583 [ 200/1251 ( 16%)]  Loss: 3.398 (3.31)  Time: 0.671s, 1525.78/s  (0.702s, 1459.62/s)  LR: 1.196e-05  Data: 0.010 (0.020)
Train: 583 [ 250/1251 ( 20%)]  Loss: 3.225 (3.30)  Time: 0.674s, 1518.60/s  (0.701s, 1460.00/s)  LR: 1.196e-05  Data: 0.010 (0.019)
Train: 583 [ 300/1251 ( 24%)]  Loss: 3.137 (3.27)  Time: 0.666s, 1538.02/s  (0.700s, 1463.71/s)  LR: 1.196e-05  Data: 0.011 (0.017)
Train: 583 [ 350/1251 ( 28%)]  Loss: 3.150 (3.26)  Time: 0.669s, 1530.35/s  (0.699s, 1464.32/s)  LR: 1.196e-05  Data: 0.013 (0.016)
Train: 583 [ 400/1251 ( 32%)]  Loss: 3.373 (3.27)  Time: 0.666s, 1537.17/s  (0.698s, 1467.27/s)  LR: 1.196e-05  Data: 0.010 (0.016)
Train: 583 [ 450/1251 ( 36%)]  Loss: 3.086 (3.25)  Time: 0.675s, 1516.92/s  (0.697s, 1468.69/s)  LR: 1.196e-05  Data: 0.010 (0.015)
Train: 583 [ 500/1251 ( 40%)]  Loss: 3.432 (3.27)  Time: 0.714s, 1433.95/s  (0.696s, 1470.37/s)  LR: 1.196e-05  Data: 0.010 (0.014)
Train: 583 [ 550/1251 ( 44%)]  Loss: 3.305 (3.27)  Time: 0.670s, 1528.50/s  (0.696s, 1471.09/s)  LR: 1.196e-05  Data: 0.011 (0.014)
Train: 583 [ 600/1251 ( 48%)]  Loss: 3.548 (3.29)  Time: 0.714s, 1433.82/s  (0.695s, 1472.74/s)  LR: 1.196e-05  Data: 0.010 (0.014)
Train: 583 [ 650/1251 ( 52%)]  Loss: 3.051 (3.28)  Time: 0.672s, 1524.77/s  (0.695s, 1473.18/s)  LR: 1.196e-05  Data: 0.010 (0.013)
Train: 583 [ 700/1251 ( 56%)]  Loss: 3.342 (3.28)  Time: 0.670s, 1528.20/s  (0.695s, 1473.26/s)  LR: 1.196e-05  Data: 0.010 (0.013)
Train: 583 [ 750/1251 ( 60%)]  Loss: 3.081 (3.27)  Time: 0.671s, 1526.11/s  (0.695s, 1473.25/s)  LR: 1.196e-05  Data: 0.012 (0.013)
Train: 583 [ 800/1251 ( 64%)]  Loss: 3.112 (3.26)  Time: 0.673s, 1521.40/s  (0.695s, 1473.20/s)  LR: 1.196e-05  Data: 0.010 (0.013)
Train: 583 [ 850/1251 ( 68%)]  Loss: 3.393 (3.27)  Time: 0.706s, 1449.95/s  (0.695s, 1473.27/s)  LR: 1.196e-05  Data: 0.011 (0.013)
Train: 583 [ 900/1251 ( 72%)]  Loss: 3.405 (3.27)  Time: 0.673s, 1521.15/s  (0.695s, 1473.74/s)  LR: 1.196e-05  Data: 0.011 (0.013)
Train: 583 [ 950/1251 ( 76%)]  Loss: 3.386 (3.28)  Time: 0.670s, 1528.68/s  (0.695s, 1473.86/s)  LR: 1.196e-05  Data: 0.009 (0.013)
Train: 583 [1000/1251 ( 80%)]  Loss: 3.477 (3.29)  Time: 0.703s, 1456.99/s  (0.695s, 1473.40/s)  LR: 1.196e-05  Data: 0.009 (0.013)
Train: 583 [1050/1251 ( 84%)]  Loss: 3.242 (3.29)  Time: 0.675s, 1517.91/s  (0.695s, 1474.02/s)  LR: 1.196e-05  Data: 0.014 (0.012)
Train: 583 [1100/1251 ( 88%)]  Loss: 3.056 (3.28)  Time: 0.673s, 1520.98/s  (0.694s, 1474.62/s)  LR: 1.196e-05  Data: 0.010 (0.012)
Train: 583 [1150/1251 ( 92%)]  Loss: 3.081 (3.27)  Time: 0.689s, 1485.86/s  (0.694s, 1475.12/s)  LR: 1.196e-05  Data: 0.014 (0.012)
Train: 583 [1200/1251 ( 96%)]  Loss: 3.096 (3.26)  Time: 0.676s, 1515.81/s  (0.694s, 1475.21/s)  LR: 1.196e-05  Data: 0.010 (0.012)
Train: 583 [1250/1251 (100%)]  Loss: 3.399 (3.27)  Time: 0.716s, 1429.55/s  (0.694s, 1475.89/s)  LR: 1.196e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.536 (1.536)  Loss:  0.6821 (0.6821)  Acc@1: 91.9922 (91.9922)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.136 (0.585)  Loss:  0.7866 (1.1563)  Acc@1: 86.9104 (79.2140)  Acc@5: 97.2877 (94.5320)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-581.pth.tar', 79.3300001586914)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-577.pth.tar', 79.28600010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-578.pth.tar', 79.258000078125)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-583.pth.tar', 79.21400000244141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-572.pth.tar', 79.2120000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-573.pth.tar', 79.20800013183593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-574.pth.tar', 79.20399994873047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-558.pth.tar', 79.20199995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-582.pth.tar', 79.18400005615234)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-571.pth.tar', 79.17199992431641)

Train: 584 [   0/1251 (  0%)]  Loss: 3.365 (3.37)  Time: 2.390s,  428.37/s  (2.390s,  428.37/s)  LR: 1.174e-05  Data: 1.715 (1.715)
Train: 584 [  50/1251 (  4%)]  Loss: 3.312 (3.34)  Time: 0.726s, 1410.02/s  (0.727s, 1409.11/s)  LR: 1.174e-05  Data: 0.009 (0.050)
Train: 584 [ 100/1251 (  8%)]  Loss: 2.924 (3.20)  Time: 0.667s, 1535.84/s  (0.713s, 1437.05/s)  LR: 1.174e-05  Data: 0.009 (0.030)
Train: 584 [ 150/1251 ( 12%)]  Loss: 3.057 (3.16)  Time: 0.667s, 1534.13/s  (0.706s, 1451.06/s)  LR: 1.174e-05  Data: 0.009 (0.024)
Train: 584 [ 200/1251 ( 16%)]  Loss: 3.309 (3.19)  Time: 0.702s, 1458.06/s  (0.704s, 1454.82/s)  LR: 1.174e-05  Data: 0.008 (0.020)
Train: 584 [ 250/1251 ( 20%)]  Loss: 3.270 (3.21)  Time: 0.737s, 1389.12/s  (0.702s, 1458.88/s)  LR: 1.174e-05  Data: 0.010 (0.018)
Train: 584 [ 300/1251 ( 24%)]  Loss: 2.968 (3.17)  Time: 0.692s, 1478.89/s  (0.698s, 1466.03/s)  LR: 1.174e-05  Data: 0.009 (0.017)
Train: 584 [ 350/1251 ( 28%)]  Loss: 3.040 (3.16)  Time: 0.671s, 1525.01/s  (0.697s, 1468.84/s)  LR: 1.174e-05  Data: 0.010 (0.016)
Train: 584 [ 400/1251 ( 32%)]  Loss: 3.258 (3.17)  Time: 0.727s, 1408.62/s  (0.697s, 1470.10/s)  LR: 1.174e-05  Data: 0.010 (0.015)
Train: 584 [ 450/1251 ( 36%)]  Loss: 3.022 (3.15)  Time: 0.716s, 1430.67/s  (0.696s, 1470.76/s)  LR: 1.174e-05  Data: 0.013 (0.015)
Train: 584 [ 500/1251 ( 40%)]  Loss: 3.147 (3.15)  Time: 0.674s, 1519.40/s  (0.696s, 1470.85/s)  LR: 1.174e-05  Data: 0.011 (0.014)
Train: 584 [ 550/1251 ( 44%)]  Loss: 3.162 (3.15)  Time: 0.673s, 1522.19/s  (0.696s, 1472.09/s)  LR: 1.174e-05  Data: 0.011 (0.014)
Train: 584 [ 600/1251 ( 48%)]  Loss: 3.060 (3.15)  Time: 0.716s, 1429.78/s  (0.695s, 1472.85/s)  LR: 1.174e-05  Data: 0.011 (0.014)
Train: 584 [ 650/1251 ( 52%)]  Loss: 3.010 (3.14)  Time: 0.732s, 1399.55/s  (0.695s, 1472.65/s)  LR: 1.174e-05  Data: 0.010 (0.013)
Train: 584 [ 700/1251 ( 56%)]  Loss: 3.036 (3.13)  Time: 0.707s, 1447.91/s  (0.695s, 1472.61/s)  LR: 1.174e-05  Data: 0.010 (0.013)
Train: 584 [ 750/1251 ( 60%)]  Loss: 3.301 (3.14)  Time: 0.670s, 1527.93/s  (0.695s, 1473.03/s)  LR: 1.174e-05  Data: 0.010 (0.013)
Train: 584 [ 800/1251 ( 64%)]  Loss: 3.268 (3.15)  Time: 0.684s, 1496.33/s  (0.695s, 1473.82/s)  LR: 1.174e-05  Data: 0.010 (0.013)
Train: 584 [ 850/1251 ( 68%)]  Loss: 3.370 (3.16)  Time: 0.694s, 1475.64/s  (0.695s, 1473.69/s)  LR: 1.174e-05  Data: 0.008 (0.013)
Train: 584 [ 900/1251 ( 72%)]  Loss: 3.154 (3.16)  Time: 0.710s, 1442.83/s  (0.695s, 1473.79/s)  LR: 1.174e-05  Data: 0.009 (0.013)
Train: 584 [ 950/1251 ( 76%)]  Loss: 3.062 (3.15)  Time: 0.708s, 1446.30/s  (0.695s, 1473.55/s)  LR: 1.174e-05  Data: 0.010 (0.013)
Train: 584 [1000/1251 ( 80%)]  Loss: 3.527 (3.17)  Time: 0.669s, 1531.24/s  (0.695s, 1474.01/s)  LR: 1.174e-05  Data: 0.010 (0.012)
Train: 584 [1050/1251 ( 84%)]  Loss: 2.949 (3.16)  Time: 0.712s, 1438.66/s  (0.694s, 1474.73/s)  LR: 1.174e-05  Data: 0.010 (0.012)
Train: 584 [1100/1251 ( 88%)]  Loss: 3.013 (3.16)  Time: 0.717s, 1428.19/s  (0.694s, 1475.09/s)  LR: 1.174e-05  Data: 0.009 (0.012)
Train: 584 [1150/1251 ( 92%)]  Loss: 3.344 (3.16)  Time: 0.720s, 1422.70/s  (0.694s, 1475.49/s)  LR: 1.174e-05  Data: 0.014 (0.012)
Train: 584 [1200/1251 ( 96%)]  Loss: 3.282 (3.17)  Time: 0.674s, 1520.02/s  (0.694s, 1476.11/s)  LR: 1.174e-05  Data: 0.014 (0.012)
Train: 584 [1250/1251 (100%)]  Loss: 3.096 (3.17)  Time: 0.692s, 1480.46/s  (0.694s, 1476.46/s)  LR: 1.174e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.504 (1.504)  Loss:  0.6558 (0.6558)  Acc@1: 91.8945 (91.8945)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.137 (0.580)  Loss:  0.7695 (1.1163)  Acc@1: 87.0283 (79.3060)  Acc@5: 97.2877 (94.4940)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-581.pth.tar', 79.3300001586914)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-584.pth.tar', 79.30600005371093)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-577.pth.tar', 79.28600010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-578.pth.tar', 79.258000078125)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-583.pth.tar', 79.21400000244141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-572.pth.tar', 79.2120000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-573.pth.tar', 79.20800013183593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-574.pth.tar', 79.20399994873047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-558.pth.tar', 79.20199995361328)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-582.pth.tar', 79.18400005615234)

Train: 585 [   0/1251 (  0%)]  Loss: 2.856 (2.86)  Time: 2.237s,  457.80/s  (2.237s,  457.80/s)  LR: 1.153e-05  Data: 1.621 (1.621)
Train: 585 [  50/1251 (  4%)]  Loss: 3.451 (3.15)  Time: 0.704s, 1454.70/s  (0.729s, 1404.65/s)  LR: 1.153e-05  Data: 0.010 (0.051)
Train: 585 [ 100/1251 (  8%)]  Loss: 3.194 (3.17)  Time: 0.697s, 1468.57/s  (0.710s, 1442.69/s)  LR: 1.153e-05  Data: 0.012 (0.031)
Train: 585 [ 150/1251 ( 12%)]  Loss: 3.067 (3.14)  Time: 0.671s, 1526.02/s  (0.703s, 1455.97/s)  LR: 1.153e-05  Data: 0.009 (0.024)
Train: 585 [ 200/1251 ( 16%)]  Loss: 3.176 (3.15)  Time: 0.695s, 1473.45/s  (0.700s, 1461.97/s)  LR: 1.153e-05  Data: 0.015 (0.021)
Train: 585 [ 250/1251 ( 20%)]  Loss: 3.569 (3.22)  Time: 0.672s, 1522.97/s  (0.699s, 1465.06/s)  LR: 1.153e-05  Data: 0.011 (0.019)
Train: 585 [ 300/1251 ( 24%)]  Loss: 3.377 (3.24)  Time: 0.710s, 1441.51/s  (0.698s, 1467.09/s)  LR: 1.153e-05  Data: 0.010 (0.018)
Train: 585 [ 350/1251 ( 28%)]  Loss: 3.452 (3.27)  Time: 0.687s, 1491.54/s  (0.697s, 1468.81/s)  LR: 1.153e-05  Data: 0.012 (0.016)
Train: 585 [ 400/1251 ( 32%)]  Loss: 3.319 (3.27)  Time: 0.730s, 1402.65/s  (0.696s, 1470.66/s)  LR: 1.153e-05  Data: 0.011 (0.016)
Train: 585 [ 450/1251 ( 36%)]  Loss: 3.190 (3.26)  Time: 0.725s, 1412.36/s  (0.696s, 1472.26/s)  LR: 1.153e-05  Data: 0.009 (0.015)
Train: 585 [ 500/1251 ( 40%)]  Loss: 3.059 (3.25)  Time: 0.671s, 1526.08/s  (0.695s, 1473.45/s)  LR: 1.153e-05  Data: 0.009 (0.015)
Train: 585 [ 550/1251 ( 44%)]  Loss: 3.343 (3.25)  Time: 0.673s, 1522.59/s  (0.695s, 1474.13/s)  LR: 1.153e-05  Data: 0.010 (0.014)
Train: 585 [ 600/1251 ( 48%)]  Loss: 3.287 (3.26)  Time: 0.772s, 1327.24/s  (0.694s, 1475.38/s)  LR: 1.153e-05  Data: 0.009 (0.014)
Train: 585 [ 650/1251 ( 52%)]  Loss: 3.354 (3.26)  Time: 0.672s, 1524.26/s  (0.694s, 1475.67/s)  LR: 1.153e-05  Data: 0.010 (0.014)
Train: 585 [ 700/1251 ( 56%)]  Loss: 3.158 (3.26)  Time: 0.696s, 1471.02/s  (0.694s, 1476.53/s)  LR: 1.153e-05  Data: 0.010 (0.013)
Train: 585 [ 750/1251 ( 60%)]  Loss: 3.468 (3.27)  Time: 0.700s, 1463.56/s  (0.694s, 1476.02/s)  LR: 1.153e-05  Data: 0.009 (0.013)
Train: 585 [ 800/1251 ( 64%)]  Loss: 3.064 (3.26)  Time: 0.670s, 1528.04/s  (0.694s, 1475.05/s)  LR: 1.153e-05  Data: 0.011 (0.013)
Train: 585 [ 850/1251 ( 68%)]  Loss: 3.218 (3.26)  Time: 0.704s, 1454.54/s  (0.695s, 1474.33/s)  LR: 1.153e-05  Data: 0.013 (0.013)
Train: 585 [ 900/1251 ( 72%)]  Loss: 3.406 (3.26)  Time: 0.702s, 1458.16/s  (0.694s, 1474.63/s)  LR: 1.153e-05  Data: 0.011 (0.013)
Train: 585 [ 950/1251 ( 76%)]  Loss: 3.414 (3.27)  Time: 0.711s, 1440.95/s  (0.694s, 1474.95/s)  LR: 1.153e-05  Data: 0.012 (0.013)
Train: 585 [1000/1251 ( 80%)]  Loss: 3.546 (3.28)  Time: 0.682s, 1500.77/s  (0.694s, 1475.30/s)  LR: 1.153e-05  Data: 0.010 (0.013)
Train: 585 [1050/1251 ( 84%)]  Loss: 3.274 (3.28)  Time: 0.714s, 1434.42/s  (0.694s, 1475.01/s)  LR: 1.153e-05  Data: 0.014 (0.012)
Train: 585 [1100/1251 ( 88%)]  Loss: 3.354 (3.29)  Time: 0.697s, 1468.89/s  (0.694s, 1475.10/s)  LR: 1.153e-05  Data: 0.016 (0.012)
Train: 585 [1150/1251 ( 92%)]  Loss: 3.268 (3.29)  Time: 0.694s, 1476.44/s  (0.695s, 1472.94/s)  LR: 1.153e-05  Data: 0.012 (0.012)
Train: 585 [1200/1251 ( 96%)]  Loss: 3.341 (3.29)  Time: 0.720s, 1422.36/s  (0.696s, 1471.46/s)  LR: 1.153e-05  Data: 0.009 (0.012)
Train: 585 [1250/1251 (100%)]  Loss: 3.477 (3.30)  Time: 0.677s, 1512.36/s  (0.696s, 1470.49/s)  LR: 1.153e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.566 (1.566)  Loss:  0.6206 (0.6206)  Acc@1: 92.1875 (92.1875)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.136 (0.561)  Loss:  0.7446 (1.0897)  Acc@1: 87.1462 (79.2900)  Acc@5: 97.1698 (94.5740)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-581.pth.tar', 79.3300001586914)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-584.pth.tar', 79.30600005371093)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-585.pth.tar', 79.29000010498046)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-577.pth.tar', 79.28600010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-578.pth.tar', 79.258000078125)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-583.pth.tar', 79.21400000244141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-572.pth.tar', 79.2120000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-573.pth.tar', 79.20800013183593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-574.pth.tar', 79.20399994873047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-558.pth.tar', 79.20199995361328)

Train: 586 [   0/1251 (  0%)]  Loss: 3.142 (3.14)  Time: 2.192s,  467.09/s  (2.192s,  467.09/s)  LR: 1.133e-05  Data: 1.569 (1.569)
Train: 586 [  50/1251 (  4%)]  Loss: 3.185 (3.16)  Time: 0.672s, 1524.77/s  (0.709s, 1444.14/s)  LR: 1.133e-05  Data: 0.010 (0.049)
Train: 586 [ 100/1251 (  8%)]  Loss: 3.293 (3.21)  Time: 0.672s, 1524.41/s  (0.692s, 1480.34/s)  LR: 1.133e-05  Data: 0.011 (0.030)
Train: 586 [ 150/1251 ( 12%)]  Loss: 2.916 (3.13)  Time: 0.671s, 1525.22/s  (0.691s, 1481.13/s)  LR: 1.133e-05  Data: 0.011 (0.024)
Train: 586 [ 200/1251 ( 16%)]  Loss: 3.122 (3.13)  Time: 0.728s, 1407.24/s  (0.692s, 1480.78/s)  LR: 1.133e-05  Data: 0.010 (0.020)
Train: 586 [ 250/1251 ( 20%)]  Loss: 3.242 (3.15)  Time: 0.729s, 1405.54/s  (0.694s, 1475.84/s)  LR: 1.133e-05  Data: 0.010 (0.018)
Train: 586 [ 300/1251 ( 24%)]  Loss: 3.172 (3.15)  Time: 0.671s, 1525.67/s  (0.693s, 1477.45/s)  LR: 1.133e-05  Data: 0.014 (0.017)
Train: 586 [ 350/1251 ( 28%)]  Loss: 3.119 (3.15)  Time: 0.666s, 1537.57/s  (0.693s, 1477.72/s)  LR: 1.133e-05  Data: 0.010 (0.016)
Train: 586 [ 400/1251 ( 32%)]  Loss: 3.086 (3.14)  Time: 0.735s, 1393.59/s  (0.692s, 1478.70/s)  LR: 1.133e-05  Data: 0.011 (0.015)
Train: 586 [ 450/1251 ( 36%)]  Loss: 3.281 (3.16)  Time: 0.692s, 1480.68/s  (0.692s, 1478.77/s)  LR: 1.133e-05  Data: 0.011 (0.015)
Train: 586 [ 500/1251 ( 40%)]  Loss: 3.494 (3.19)  Time: 0.678s, 1510.74/s  (0.693s, 1478.30/s)  LR: 1.133e-05  Data: 0.011 (0.014)
Train: 586 [ 550/1251 ( 44%)]  Loss: 3.017 (3.17)  Time: 0.668s, 1532.19/s  (0.694s, 1474.94/s)  LR: 1.133e-05  Data: 0.012 (0.014)
Train: 586 [ 600/1251 ( 48%)]  Loss: 3.270 (3.18)  Time: 0.666s, 1537.04/s  (0.695s, 1474.00/s)  LR: 1.133e-05  Data: 0.010 (0.014)
Train: 586 [ 650/1251 ( 52%)]  Loss: 3.464 (3.20)  Time: 0.711s, 1439.63/s  (0.694s, 1474.66/s)  LR: 1.133e-05  Data: 0.009 (0.014)
Train: 586 [ 700/1251 ( 56%)]  Loss: 3.291 (3.21)  Time: 0.674s, 1519.63/s  (0.694s, 1475.20/s)  LR: 1.133e-05  Data: 0.011 (0.013)
Train: 586 [ 750/1251 ( 60%)]  Loss: 3.060 (3.20)  Time: 0.689s, 1485.49/s  (0.694s, 1476.08/s)  LR: 1.133e-05  Data: 0.010 (0.013)
Train: 586 [ 800/1251 ( 64%)]  Loss: 3.184 (3.20)  Time: 0.671s, 1525.68/s  (0.694s, 1476.36/s)  LR: 1.133e-05  Data: 0.011 (0.013)
Train: 586 [ 850/1251 ( 68%)]  Loss: 3.319 (3.20)  Time: 0.672s, 1522.71/s  (0.694s, 1476.23/s)  LR: 1.133e-05  Data: 0.010 (0.013)
Train: 586 [ 900/1251 ( 72%)]  Loss: 2.957 (3.19)  Time: 0.706s, 1449.69/s  (0.694s, 1475.72/s)  LR: 1.133e-05  Data: 0.010 (0.013)
Train: 586 [ 950/1251 ( 76%)]  Loss: 3.357 (3.20)  Time: 0.728s, 1406.26/s  (0.694s, 1475.07/s)  LR: 1.133e-05  Data: 0.011 (0.012)
Train: 586 [1000/1251 ( 80%)]  Loss: 3.223 (3.20)  Time: 0.677s, 1513.65/s  (0.694s, 1476.22/s)  LR: 1.133e-05  Data: 0.011 (0.012)
Train: 586 [1050/1251 ( 84%)]  Loss: 2.800 (3.18)  Time: 0.700s, 1463.09/s  (0.693s, 1476.85/s)  LR: 1.133e-05  Data: 0.009 (0.012)
Train: 586 [1100/1251 ( 88%)]  Loss: 3.396 (3.19)  Time: 0.709s, 1443.66/s  (0.693s, 1476.98/s)  LR: 1.133e-05  Data: 0.011 (0.012)
Train: 586 [1150/1251 ( 92%)]  Loss: 3.432 (3.20)  Time: 0.690s, 1483.21/s  (0.693s, 1477.40/s)  LR: 1.133e-05  Data: 0.010 (0.012)
Train: 586 [1200/1251 ( 96%)]  Loss: 2.924 (3.19)  Time: 0.681s, 1504.55/s  (0.693s, 1477.03/s)  LR: 1.133e-05  Data: 0.014 (0.012)
Train: 586 [1250/1251 (100%)]  Loss: 3.645 (3.21)  Time: 0.657s, 1559.09/s  (0.693s, 1477.49/s)  LR: 1.133e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.593 (1.593)  Loss:  0.6748 (0.6748)  Acc@1: 92.0898 (92.0898)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.136 (0.577)  Loss:  0.7910 (1.1517)  Acc@1: 87.5000 (79.1980)  Acc@5: 97.4057 (94.5480)
Train: 587 [   0/1251 (  0%)]  Loss: 3.365 (3.37)  Time: 2.316s,  442.20/s  (2.316s,  442.20/s)  LR: 1.115e-05  Data: 1.699 (1.699)
Train: 587 [  50/1251 (  4%)]  Loss: 3.096 (3.23)  Time: 0.675s, 1516.28/s  (0.730s, 1402.55/s)  LR: 1.115e-05  Data: 0.011 (0.051)
Train: 587 [ 100/1251 (  8%)]  Loss: 3.107 (3.19)  Time: 0.720s, 1421.74/s  (0.713s, 1435.49/s)  LR: 1.115e-05  Data: 0.009 (0.031)
Train: 587 [ 150/1251 ( 12%)]  Loss: 3.008 (3.14)  Time: 0.809s, 1265.94/s  (0.707s, 1448.22/s)  LR: 1.115e-05  Data: 0.009 (0.024)
Train: 587 [ 200/1251 ( 16%)]  Loss: 3.477 (3.21)  Time: 0.672s, 1523.83/s  (0.703s, 1456.93/s)  LR: 1.115e-05  Data: 0.011 (0.021)
Train: 587 [ 250/1251 ( 20%)]  Loss: 2.936 (3.16)  Time: 0.673s, 1522.42/s  (0.699s, 1464.72/s)  LR: 1.115e-05  Data: 0.010 (0.019)
Train: 587 [ 300/1251 ( 24%)]  Loss: 3.366 (3.19)  Time: 0.671s, 1527.10/s  (0.696s, 1470.67/s)  LR: 1.115e-05  Data: 0.010 (0.017)
Train: 587 [ 350/1251 ( 28%)]  Loss: 3.328 (3.21)  Time: 0.705s, 1451.72/s  (0.696s, 1471.65/s)  LR: 1.115e-05  Data: 0.009 (0.016)
Train: 587 [ 400/1251 ( 32%)]  Loss: 3.293 (3.22)  Time: 0.695s, 1473.10/s  (0.697s, 1468.60/s)  LR: 1.115e-05  Data: 0.009 (0.016)
Train: 587 [ 450/1251 ( 36%)]  Loss: 3.167 (3.21)  Time: 0.708s, 1447.20/s  (0.697s, 1469.32/s)  LR: 1.115e-05  Data: 0.010 (0.015)
Train: 587 [ 500/1251 ( 40%)]  Loss: 2.822 (3.18)  Time: 0.680s, 1505.94/s  (0.696s, 1470.58/s)  LR: 1.115e-05  Data: 0.012 (0.015)
Train: 587 [ 550/1251 ( 44%)]  Loss: 3.420 (3.20)  Time: 0.674s, 1518.89/s  (0.696s, 1471.43/s)  LR: 1.115e-05  Data: 0.010 (0.014)
Train: 587 [ 600/1251 ( 48%)]  Loss: 3.520 (3.22)  Time: 0.707s, 1448.10/s  (0.695s, 1473.63/s)  LR: 1.115e-05  Data: 0.010 (0.014)
Train: 587 [ 650/1251 ( 52%)]  Loss: 3.433 (3.24)  Time: 0.710s, 1441.42/s  (0.695s, 1473.80/s)  LR: 1.115e-05  Data: 0.011 (0.014)
Train: 587 [ 700/1251 ( 56%)]  Loss: 3.222 (3.24)  Time: 0.674s, 1519.99/s  (0.695s, 1473.36/s)  LR: 1.115e-05  Data: 0.012 (0.014)
Train: 587 [ 750/1251 ( 60%)]  Loss: 3.491 (3.25)  Time: 0.771s, 1327.75/s  (0.696s, 1471.98/s)  LR: 1.115e-05  Data: 0.009 (0.013)
Train: 587 [ 800/1251 ( 64%)]  Loss: 3.521 (3.27)  Time: 0.729s, 1405.22/s  (0.695s, 1472.90/s)  LR: 1.115e-05  Data: 0.009 (0.013)
Train: 587 [ 850/1251 ( 68%)]  Loss: 3.187 (3.26)  Time: 0.671s, 1526.08/s  (0.695s, 1473.95/s)  LR: 1.115e-05  Data: 0.010 (0.013)
Train: 587 [ 900/1251 ( 72%)]  Loss: 3.210 (3.26)  Time: 0.702s, 1459.35/s  (0.695s, 1473.82/s)  LR: 1.115e-05  Data: 0.009 (0.013)
Train: 587 [ 950/1251 ( 76%)]  Loss: 2.885 (3.24)  Time: 0.673s, 1521.30/s  (0.695s, 1473.98/s)  LR: 1.115e-05  Data: 0.011 (0.013)
Train: 587 [1000/1251 ( 80%)]  Loss: 3.338 (3.25)  Time: 0.700s, 1462.83/s  (0.695s, 1474.43/s)  LR: 1.115e-05  Data: 0.009 (0.013)
Train: 587 [1050/1251 ( 84%)]  Loss: 3.054 (3.24)  Time: 0.760s, 1347.17/s  (0.695s, 1474.07/s)  LR: 1.115e-05  Data: 0.010 (0.013)
Train: 587 [1100/1251 ( 88%)]  Loss: 3.454 (3.25)  Time: 0.718s, 1425.55/s  (0.695s, 1474.04/s)  LR: 1.115e-05  Data: 0.010 (0.012)
Train: 587 [1150/1251 ( 92%)]  Loss: 2.986 (3.24)  Time: 0.710s, 1442.55/s  (0.695s, 1473.95/s)  LR: 1.115e-05  Data: 0.009 (0.012)
Train: 587 [1200/1251 ( 96%)]  Loss: 3.457 (3.25)  Time: 0.716s, 1430.63/s  (0.694s, 1474.47/s)  LR: 1.115e-05  Data: 0.011 (0.012)
Train: 587 [1250/1251 (100%)]  Loss: 3.046 (3.24)  Time: 0.697s, 1468.46/s  (0.695s, 1473.92/s)  LR: 1.115e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.558 (1.558)  Loss:  0.6187 (0.6187)  Acc@1: 92.0898 (92.0898)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.136 (0.581)  Loss:  0.7310 (1.0951)  Acc@1: 87.1462 (79.3360)  Acc@5: 97.4057 (94.5200)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-587.pth.tar', 79.33600010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-581.pth.tar', 79.3300001586914)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-584.pth.tar', 79.30600005371093)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-585.pth.tar', 79.29000010498046)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-577.pth.tar', 79.28600010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-578.pth.tar', 79.258000078125)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-583.pth.tar', 79.21400000244141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-572.pth.tar', 79.2120000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-573.pth.tar', 79.20800013183593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-574.pth.tar', 79.20399994873047)

Train: 588 [   0/1251 (  0%)]  Loss: 3.043 (3.04)  Time: 2.257s,  453.71/s  (2.257s,  453.71/s)  LR: 1.098e-05  Data: 1.583 (1.583)
Train: 588 [  50/1251 (  4%)]  Loss: 3.438 (3.24)  Time: 0.674s, 1520.36/s  (0.730s, 1402.01/s)  LR: 1.098e-05  Data: 0.009 (0.048)
Train: 588 [ 100/1251 (  8%)]  Loss: 2.922 (3.13)  Time: 0.692s, 1480.11/s  (0.711s, 1441.03/s)  LR: 1.098e-05  Data: 0.009 (0.030)
Train: 588 [ 150/1251 ( 12%)]  Loss: 2.931 (3.08)  Time: 0.682s, 1501.51/s  (0.706s, 1451.21/s)  LR: 1.098e-05  Data: 0.009 (0.023)
Train: 588 [ 200/1251 ( 16%)]  Loss: 3.079 (3.08)  Time: 0.736s, 1391.23/s  (0.703s, 1455.60/s)  LR: 1.098e-05  Data: 0.009 (0.020)
Train: 588 [ 250/1251 ( 20%)]  Loss: 3.056 (3.08)  Time: 0.671s, 1525.41/s  (0.701s, 1461.27/s)  LR: 1.098e-05  Data: 0.010 (0.018)
Train: 588 [ 300/1251 ( 24%)]  Loss: 3.129 (3.09)  Time: 0.717s, 1427.79/s  (0.701s, 1461.44/s)  LR: 1.098e-05  Data: 0.017 (0.017)
Train: 588 [ 350/1251 ( 28%)]  Loss: 3.174 (3.10)  Time: 0.673s, 1520.83/s  (0.700s, 1463.14/s)  LR: 1.098e-05  Data: 0.011 (0.016)
Train: 588 [ 400/1251 ( 32%)]  Loss: 3.319 (3.12)  Time: 0.693s, 1478.06/s  (0.698s, 1466.40/s)  LR: 1.098e-05  Data: 0.010 (0.015)
Train: 588 [ 450/1251 ( 36%)]  Loss: 2.537 (3.06)  Time: 0.700s, 1463.51/s  (0.698s, 1467.32/s)  LR: 1.098e-05  Data: 0.009 (0.015)
Train: 588 [ 500/1251 ( 40%)]  Loss: 3.140 (3.07)  Time: 0.697s, 1469.01/s  (0.697s, 1468.89/s)  LR: 1.098e-05  Data: 0.009 (0.015)
Train: 588 [ 550/1251 ( 44%)]  Loss: 3.087 (3.07)  Time: 0.715s, 1432.68/s  (0.697s, 1468.19/s)  LR: 1.098e-05  Data: 0.011 (0.014)
Train: 588 [ 600/1251 ( 48%)]  Loss: 3.278 (3.09)  Time: 0.729s, 1404.05/s  (0.698s, 1467.97/s)  LR: 1.098e-05  Data: 0.011 (0.014)
Train: 588 [ 650/1251 ( 52%)]  Loss: 3.213 (3.10)  Time: 0.737s, 1389.65/s  (0.697s, 1468.64/s)  LR: 1.098e-05  Data: 0.011 (0.014)
Train: 588 [ 700/1251 ( 56%)]  Loss: 3.432 (3.12)  Time: 0.726s, 1410.57/s  (0.697s, 1469.70/s)  LR: 1.098e-05  Data: 0.016 (0.013)
Train: 588 [ 750/1251 ( 60%)]  Loss: 2.999 (3.11)  Time: 0.722s, 1419.02/s  (0.696s, 1470.27/s)  LR: 1.098e-05  Data: 0.011 (0.013)
Train: 588 [ 800/1251 ( 64%)]  Loss: 3.270 (3.12)  Time: 0.728s, 1406.20/s  (0.696s, 1471.52/s)  LR: 1.098e-05  Data: 0.011 (0.013)
Train: 588 [ 850/1251 ( 68%)]  Loss: 3.331 (3.13)  Time: 0.706s, 1449.71/s  (0.696s, 1472.17/s)  LR: 1.098e-05  Data: 0.010 (0.013)
Train: 588 [ 900/1251 ( 72%)]  Loss: 3.273 (3.14)  Time: 0.673s, 1521.25/s  (0.695s, 1472.57/s)  LR: 1.098e-05  Data: 0.010 (0.013)
Train: 588 [ 950/1251 ( 76%)]  Loss: 3.161 (3.14)  Time: 0.674s, 1520.30/s  (0.695s, 1473.02/s)  LR: 1.098e-05  Data: 0.011 (0.013)
Train: 588 [1000/1251 ( 80%)]  Loss: 3.328 (3.15)  Time: 0.671s, 1525.03/s  (0.695s, 1473.58/s)  LR: 1.098e-05  Data: 0.011 (0.013)
Train: 588 [1050/1251 ( 84%)]  Loss: 3.322 (3.16)  Time: 0.685s, 1494.14/s  (0.695s, 1474.00/s)  LR: 1.098e-05  Data: 0.011 (0.012)
Train: 588 [1100/1251 ( 88%)]  Loss: 2.930 (3.15)  Time: 0.671s, 1526.18/s  (0.694s, 1474.66/s)  LR: 1.098e-05  Data: 0.010 (0.012)
Train: 588 [1150/1251 ( 92%)]  Loss: 3.254 (3.15)  Time: 0.701s, 1460.59/s  (0.695s, 1474.30/s)  LR: 1.098e-05  Data: 0.010 (0.012)
Train: 588 [1200/1251 ( 96%)]  Loss: 3.542 (3.17)  Time: 0.705s, 1451.48/s  (0.694s, 1474.89/s)  LR: 1.098e-05  Data: 0.009 (0.012)
Train: 588 [1250/1251 (100%)]  Loss: 2.973 (3.16)  Time: 0.655s, 1563.94/s  (0.694s, 1475.13/s)  LR: 1.098e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.548 (1.548)  Loss:  0.6929 (0.6929)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.136 (0.581)  Loss:  0.7915 (1.1734)  Acc@1: 87.0283 (79.1800)  Acc@5: 97.1698 (94.5140)
Train: 589 [   0/1251 (  0%)]  Loss: 2.857 (2.86)  Time: 2.162s,  473.74/s  (2.162s,  473.74/s)  LR: 1.082e-05  Data: 1.492 (1.492)
Train: 589 [  50/1251 (  4%)]  Loss: 3.006 (2.93)  Time: 0.693s, 1478.43/s  (0.724s, 1413.46/s)  LR: 1.082e-05  Data: 0.011 (0.048)
Train: 589 [ 100/1251 (  8%)]  Loss: 3.243 (3.04)  Time: 0.671s, 1526.33/s  (0.707s, 1448.09/s)  LR: 1.082e-05  Data: 0.009 (0.030)
Train: 589 [ 150/1251 ( 12%)]  Loss: 3.171 (3.07)  Time: 0.670s, 1527.88/s  (0.702s, 1459.36/s)  LR: 1.082e-05  Data: 0.010 (0.023)
Train: 589 [ 200/1251 ( 16%)]  Loss: 3.422 (3.14)  Time: 0.672s, 1523.47/s  (0.699s, 1464.02/s)  LR: 1.082e-05  Data: 0.011 (0.020)
Train: 589 [ 250/1251 ( 20%)]  Loss: 3.176 (3.15)  Time: 0.675s, 1516.74/s  (0.698s, 1466.54/s)  LR: 1.082e-05  Data: 0.010 (0.018)
Train: 589 [ 300/1251 ( 24%)]  Loss: 3.295 (3.17)  Time: 0.703s, 1457.63/s  (0.697s, 1470.02/s)  LR: 1.082e-05  Data: 0.009 (0.017)
Train: 589 [ 350/1251 ( 28%)]  Loss: 2.615 (3.10)  Time: 0.671s, 1525.17/s  (0.696s, 1471.69/s)  LR: 1.082e-05  Data: 0.010 (0.016)
Train: 589 [ 400/1251 ( 32%)]  Loss: 2.872 (3.07)  Time: 0.683s, 1500.27/s  (0.696s, 1470.51/s)  LR: 1.082e-05  Data: 0.009 (0.015)
Train: 589 [ 450/1251 ( 36%)]  Loss: 3.121 (3.08)  Time: 0.701s, 1460.98/s  (0.697s, 1469.18/s)  LR: 1.082e-05  Data: 0.012 (0.015)
Train: 589 [ 500/1251 ( 40%)]  Loss: 3.136 (3.08)  Time: 0.671s, 1525.08/s  (0.696s, 1471.63/s)  LR: 1.082e-05  Data: 0.011 (0.014)
Train: 589 [ 550/1251 ( 44%)]  Loss: 3.026 (3.08)  Time: 0.710s, 1442.08/s  (0.696s, 1470.35/s)  LR: 1.082e-05  Data: 0.014 (0.014)
Train: 589 [ 600/1251 ( 48%)]  Loss: 3.006 (3.07)  Time: 0.708s, 1446.54/s  (0.696s, 1471.06/s)  LR: 1.082e-05  Data: 0.009 (0.014)
Train: 589 [ 650/1251 ( 52%)]  Loss: 3.518 (3.10)  Time: 0.715s, 1432.12/s  (0.696s, 1470.89/s)  LR: 1.082e-05  Data: 0.009 (0.014)
Train: 589 [ 700/1251 ( 56%)]  Loss: 3.348 (3.12)  Time: 0.673s, 1522.01/s  (0.696s, 1471.10/s)  LR: 1.082e-05  Data: 0.010 (0.013)
Train: 589 [ 750/1251 ( 60%)]  Loss: 2.894 (3.11)  Time: 0.673s, 1522.29/s  (0.695s, 1472.47/s)  LR: 1.082e-05  Data: 0.011 (0.013)
Train: 589 [ 800/1251 ( 64%)]  Loss: 3.216 (3.11)  Time: 0.711s, 1439.75/s  (0.695s, 1472.33/s)  LR: 1.082e-05  Data: 0.012 (0.013)
Train: 589 [ 850/1251 ( 68%)]  Loss: 3.326 (3.12)  Time: 0.673s, 1522.51/s  (0.695s, 1472.78/s)  LR: 1.082e-05  Data: 0.012 (0.013)
Train: 589 [ 900/1251 ( 72%)]  Loss: 3.201 (3.13)  Time: 0.679s, 1507.41/s  (0.695s, 1473.93/s)  LR: 1.082e-05  Data: 0.009 (0.013)
Train: 589 [ 950/1251 ( 76%)]  Loss: 3.286 (3.14)  Time: 0.706s, 1449.57/s  (0.695s, 1473.80/s)  LR: 1.082e-05  Data: 0.010 (0.013)
Train: 589 [1000/1251 ( 80%)]  Loss: 3.346 (3.15)  Time: 0.672s, 1524.84/s  (0.695s, 1473.73/s)  LR: 1.082e-05  Data: 0.010 (0.012)
Train: 589 [1050/1251 ( 84%)]  Loss: 3.004 (3.14)  Time: 0.671s, 1526.82/s  (0.695s, 1473.78/s)  LR: 1.082e-05  Data: 0.011 (0.012)
Train: 589 [1100/1251 ( 88%)]  Loss: 3.376 (3.15)  Time: 0.672s, 1523.82/s  (0.695s, 1473.67/s)  LR: 1.082e-05  Data: 0.010 (0.012)
Train: 589 [1150/1251 ( 92%)]  Loss: 3.055 (3.15)  Time: 0.715s, 1432.65/s  (0.695s, 1473.77/s)  LR: 1.082e-05  Data: 0.009 (0.012)
Train: 589 [1200/1251 ( 96%)]  Loss: 3.274 (3.15)  Time: 0.674s, 1519.55/s  (0.694s, 1474.44/s)  LR: 1.082e-05  Data: 0.012 (0.012)
Train: 589 [1250/1251 (100%)]  Loss: 3.091 (3.15)  Time: 0.654s, 1564.86/s  (0.694s, 1474.63/s)  LR: 1.082e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.566 (1.566)  Loss:  0.6338 (0.6338)  Acc@1: 91.7969 (91.7969)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.137 (0.580)  Loss:  0.7432 (1.1054)  Acc@1: 86.9104 (79.3780)  Acc@5: 97.2877 (94.6040)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-589.pth.tar', 79.37800013183593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-587.pth.tar', 79.33600010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-581.pth.tar', 79.3300001586914)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-584.pth.tar', 79.30600005371093)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-585.pth.tar', 79.29000010498046)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-577.pth.tar', 79.28600010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-578.pth.tar', 79.258000078125)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-583.pth.tar', 79.21400000244141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-572.pth.tar', 79.2120000024414)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-573.pth.tar', 79.20800013183593)

Train: 590 [   0/1251 (  0%)]  Loss: 3.069 (3.07)  Time: 2.414s,  424.21/s  (2.414s,  424.21/s)  LR: 1.068e-05  Data: 1.781 (1.781)
Train: 590 [  50/1251 (  4%)]  Loss: 3.375 (3.22)  Time: 0.679s, 1507.25/s  (0.726s, 1410.36/s)  LR: 1.068e-05  Data: 0.010 (0.045)
Train: 590 [ 100/1251 (  8%)]  Loss: 3.176 (3.21)  Time: 0.704s, 1453.55/s  (0.710s, 1442.22/s)  LR: 1.068e-05  Data: 0.010 (0.028)
Train: 590 [ 150/1251 ( 12%)]  Loss: 3.145 (3.19)  Time: 0.674s, 1519.95/s  (0.704s, 1455.13/s)  LR: 1.068e-05  Data: 0.011 (0.022)
Train: 590 [ 200/1251 ( 16%)]  Loss: 3.237 (3.20)  Time: 0.685s, 1493.83/s  (0.701s, 1460.81/s)  LR: 1.068e-05  Data: 0.011 (0.019)
Train: 590 [ 250/1251 ( 20%)]  Loss: 3.146 (3.19)  Time: 0.737s, 1388.53/s  (0.700s, 1463.12/s)  LR: 1.068e-05  Data: 0.009 (0.017)
Train: 590 [ 300/1251 ( 24%)]  Loss: 3.123 (3.18)  Time: 0.683s, 1499.57/s  (0.700s, 1462.36/s)  LR: 1.068e-05  Data: 0.009 (0.016)
Train: 590 [ 350/1251 ( 28%)]  Loss: 3.450 (3.22)  Time: 0.680s, 1505.44/s  (0.699s, 1465.15/s)  LR: 1.068e-05  Data: 0.010 (0.016)
Train: 590 [ 400/1251 ( 32%)]  Loss: 3.151 (3.21)  Time: 0.671s, 1525.78/s  (0.697s, 1468.39/s)  LR: 1.068e-05  Data: 0.013 (0.015)
Train: 590 [ 450/1251 ( 36%)]  Loss: 3.040 (3.19)  Time: 0.671s, 1525.49/s  (0.696s, 1470.39/s)  LR: 1.068e-05  Data: 0.011 (0.014)
Train: 590 [ 500/1251 ( 40%)]  Loss: 2.947 (3.17)  Time: 0.696s, 1470.68/s  (0.696s, 1471.17/s)  LR: 1.068e-05  Data: 0.010 (0.014)
Train: 590 [ 550/1251 ( 44%)]  Loss: 3.173 (3.17)  Time: 0.697s, 1468.63/s  (0.696s, 1471.48/s)  LR: 1.068e-05  Data: 0.011 (0.014)
Train: 590 [ 600/1251 ( 48%)]  Loss: 2.885 (3.15)  Time: 0.686s, 1491.89/s  (0.696s, 1471.98/s)  LR: 1.068e-05  Data: 0.011 (0.013)
Train: 590 [ 650/1251 ( 52%)]  Loss: 3.048 (3.14)  Time: 0.709s, 1443.88/s  (0.696s, 1471.81/s)  LR: 1.068e-05  Data: 0.009 (0.013)
Train: 590 [ 700/1251 ( 56%)]  Loss: 3.245 (3.15)  Time: 0.693s, 1477.94/s  (0.696s, 1471.93/s)  LR: 1.068e-05  Data: 0.009 (0.013)
Train: 590 [ 750/1251 ( 60%)]  Loss: 2.803 (3.13)  Time: 0.693s, 1478.07/s  (0.695s, 1473.29/s)  LR: 1.068e-05  Data: 0.011 (0.013)
Train: 590 [ 800/1251 ( 64%)]  Loss: 3.323 (3.14)  Time: 0.701s, 1460.74/s  (0.695s, 1472.55/s)  LR: 1.068e-05  Data: 0.009 (0.013)
Train: 590 [ 850/1251 ( 68%)]  Loss: 3.071 (3.13)  Time: 0.669s, 1531.50/s  (0.696s, 1472.28/s)  LR: 1.068e-05  Data: 0.010 (0.013)
Train: 590 [ 900/1251 ( 72%)]  Loss: 3.229 (3.14)  Time: 0.667s, 1535.56/s  (0.696s, 1471.51/s)  LR: 1.068e-05  Data: 0.010 (0.012)
Train: 590 [ 950/1251 ( 76%)]  Loss: 3.209 (3.14)  Time: 0.671s, 1526.96/s  (0.696s, 1472.29/s)  LR: 1.068e-05  Data: 0.011 (0.012)
Train: 590 [1000/1251 ( 80%)]  Loss: 3.165 (3.14)  Time: 0.672s, 1523.35/s  (0.695s, 1472.98/s)  LR: 1.068e-05  Data: 0.011 (0.012)
Train: 590 [1050/1251 ( 84%)]  Loss: 3.424 (3.16)  Time: 0.704s, 1455.16/s  (0.695s, 1473.73/s)  LR: 1.068e-05  Data: 0.009 (0.012)
Train: 590 [1100/1251 ( 88%)]  Loss: 3.191 (3.16)  Time: 0.721s, 1419.44/s  (0.695s, 1474.10/s)  LR: 1.068e-05  Data: 0.010 (0.012)
Train: 590 [1150/1251 ( 92%)]  Loss: 3.243 (3.16)  Time: 0.682s, 1502.11/s  (0.694s, 1474.54/s)  LR: 1.068e-05  Data: 0.011 (0.012)
Train: 590 [1200/1251 ( 96%)]  Loss: 3.618 (3.18)  Time: 0.671s, 1526.08/s  (0.694s, 1474.64/s)  LR: 1.068e-05  Data: 0.010 (0.012)
Train: 590 [1250/1251 (100%)]  Loss: 3.176 (3.18)  Time: 0.654s, 1564.72/s  (0.694s, 1475.35/s)  LR: 1.068e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.489 (1.489)  Loss:  0.6685 (0.6685)  Acc@1: 91.7969 (91.7969)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.136 (0.581)  Loss:  0.7920 (1.1492)  Acc@1: 87.2642 (79.3420)  Acc@5: 97.1698 (94.5220)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-589.pth.tar', 79.37800013183593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-590.pth.tar', 79.34200002685547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-587.pth.tar', 79.33600010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-581.pth.tar', 79.3300001586914)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-584.pth.tar', 79.30600005371093)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-585.pth.tar', 79.29000010498046)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-577.pth.tar', 79.28600010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-578.pth.tar', 79.258000078125)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-583.pth.tar', 79.21400000244141)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-572.pth.tar', 79.2120000024414)

Train: 591 [   0/1251 (  0%)]  Loss: 2.984 (2.98)  Time: 2.340s,  437.58/s  (2.340s,  437.58/s)  LR: 1.055e-05  Data: 1.725 (1.725)
Train: 591 [  50/1251 (  4%)]  Loss: 2.758 (2.87)  Time: 0.703s, 1456.22/s  (0.734s, 1394.52/s)  LR: 1.055e-05  Data: 0.010 (0.049)
Train: 591 [ 100/1251 (  8%)]  Loss: 3.109 (2.95)  Time: 0.712s, 1438.07/s  (0.714s, 1433.69/s)  LR: 1.055e-05  Data: 0.009 (0.030)
Train: 591 [ 150/1251 ( 12%)]  Loss: 3.078 (2.98)  Time: 0.674s, 1519.84/s  (0.705s, 1452.71/s)  LR: 1.055e-05  Data: 0.012 (0.024)
Train: 591 [ 200/1251 ( 16%)]  Loss: 3.257 (3.04)  Time: 0.722s, 1418.94/s  (0.703s, 1456.22/s)  LR: 1.055e-05  Data: 0.010 (0.020)
Train: 591 [ 250/1251 ( 20%)]  Loss: 2.990 (3.03)  Time: 0.674s, 1519.62/s  (0.702s, 1459.17/s)  LR: 1.055e-05  Data: 0.009 (0.018)
Train: 591 [ 300/1251 ( 24%)]  Loss: 3.192 (3.05)  Time: 0.703s, 1457.49/s  (0.701s, 1460.98/s)  LR: 1.055e-05  Data: 0.009 (0.017)
Train: 591 [ 350/1251 ( 28%)]  Loss: 3.125 (3.06)  Time: 0.672s, 1524.55/s  (0.701s, 1461.75/s)  LR: 1.055e-05  Data: 0.010 (0.016)
Train: 591 [ 400/1251 ( 32%)]  Loss: 3.134 (3.07)  Time: 0.676s, 1513.93/s  (0.701s, 1461.65/s)  LR: 1.055e-05  Data: 0.009 (0.015)
Train: 591 [ 450/1251 ( 36%)]  Loss: 3.194 (3.08)  Time: 0.671s, 1526.63/s  (0.699s, 1465.08/s)  LR: 1.055e-05  Data: 0.009 (0.015)
Train: 591 [ 500/1251 ( 40%)]  Loss: 3.362 (3.11)  Time: 0.690s, 1483.11/s  (0.699s, 1465.58/s)  LR: 1.055e-05  Data: 0.012 (0.014)
Train: 591 [ 550/1251 ( 44%)]  Loss: 3.120 (3.11)  Time: 0.704s, 1455.26/s  (0.698s, 1467.45/s)  LR: 1.055e-05  Data: 0.009 (0.014)
Train: 591 [ 600/1251 ( 48%)]  Loss: 3.322 (3.12)  Time: 0.705s, 1451.61/s  (0.697s, 1469.02/s)  LR: 1.055e-05  Data: 0.009 (0.014)
Train: 591 [ 650/1251 ( 52%)]  Loss: 3.330 (3.14)  Time: 0.681s, 1503.63/s  (0.696s, 1470.44/s)  LR: 1.055e-05  Data: 0.010 (0.013)
Train: 591 [ 700/1251 ( 56%)]  Loss: 3.319 (3.15)  Time: 0.677s, 1511.64/s  (0.696s, 1471.84/s)  LR: 1.055e-05  Data: 0.010 (0.013)
Train: 591 [ 750/1251 ( 60%)]  Loss: 3.305 (3.16)  Time: 0.834s, 1227.23/s  (0.696s, 1472.09/s)  LR: 1.055e-05  Data: 0.009 (0.013)
Train: 591 [ 800/1251 ( 64%)]  Loss: 3.268 (3.17)  Time: 0.722s, 1418.35/s  (0.696s, 1471.90/s)  LR: 1.055e-05  Data: 0.010 (0.013)
Train: 591 [ 850/1251 ( 68%)]  Loss: 3.119 (3.16)  Time: 0.712s, 1439.09/s  (0.696s, 1471.68/s)  LR: 1.055e-05  Data: 0.011 (0.013)
Train: 591 [ 900/1251 ( 72%)]  Loss: 3.151 (3.16)  Time: 0.705s, 1452.15/s  (0.696s, 1472.01/s)  LR: 1.055e-05  Data: 0.011 (0.013)
Train: 591 [ 950/1251 ( 76%)]  Loss: 3.363 (3.17)  Time: 0.673s, 1521.58/s  (0.696s, 1471.44/s)  LR: 1.055e-05  Data: 0.010 (0.013)
Train: 591 [1000/1251 ( 80%)]  Loss: 3.692 (3.20)  Time: 0.699s, 1464.53/s  (0.695s, 1472.51/s)  LR: 1.055e-05  Data: 0.010 (0.012)
Train: 591 [1050/1251 ( 84%)]  Loss: 3.560 (3.22)  Time: 0.744s, 1377.14/s  (0.695s, 1472.49/s)  LR: 1.055e-05  Data: 0.011 (0.012)
Train: 591 [1100/1251 ( 88%)]  Loss: 3.190 (3.21)  Time: 0.699s, 1464.35/s  (0.695s, 1472.39/s)  LR: 1.055e-05  Data: 0.009 (0.012)
Train: 591 [1150/1251 ( 92%)]  Loss: 3.454 (3.22)  Time: 0.676s, 1515.30/s  (0.696s, 1472.23/s)  LR: 1.055e-05  Data: 0.012 (0.012)
Train: 591 [1200/1251 ( 96%)]  Loss: 3.099 (3.22)  Time: 0.670s, 1527.49/s  (0.695s, 1472.91/s)  LR: 1.055e-05  Data: 0.010 (0.012)
Train: 591 [1250/1251 (100%)]  Loss: 3.337 (3.22)  Time: 0.683s, 1498.24/s  (0.695s, 1473.64/s)  LR: 1.055e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.662 (1.662)  Loss:  0.6880 (0.6880)  Acc@1: 92.0898 (92.0898)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.136 (0.588)  Loss:  0.8052 (1.1757)  Acc@1: 87.1462 (79.2780)  Acc@5: 97.0519 (94.4760)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-589.pth.tar', 79.37800013183593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-590.pth.tar', 79.34200002685547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-587.pth.tar', 79.33600010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-581.pth.tar', 79.3300001586914)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-584.pth.tar', 79.30600005371093)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-585.pth.tar', 79.29000010498046)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-577.pth.tar', 79.28600010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-591.pth.tar', 79.27799997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-578.pth.tar', 79.258000078125)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-583.pth.tar', 79.21400000244141)

Train: 592 [   0/1251 (  0%)]  Loss: 2.956 (2.96)  Time: 2.306s,  444.12/s  (2.306s,  444.12/s)  LR: 1.043e-05  Data: 1.676 (1.676)
Train: 592 [  50/1251 (  4%)]  Loss: 3.347 (3.15)  Time: 0.697s, 1468.78/s  (0.731s, 1401.41/s)  LR: 1.043e-05  Data: 0.011 (0.052)
Train: 592 [ 100/1251 (  8%)]  Loss: 3.180 (3.16)  Time: 0.705s, 1453.13/s  (0.708s, 1445.71/s)  LR: 1.043e-05  Data: 0.009 (0.032)
Train: 592 [ 150/1251 ( 12%)]  Loss: 3.407 (3.22)  Time: 0.671s, 1527.21/s  (0.702s, 1458.85/s)  LR: 1.043e-05  Data: 0.011 (0.025)
Train: 592 [ 200/1251 ( 16%)]  Loss: 3.704 (3.32)  Time: 0.674s, 1520.31/s  (0.701s, 1461.03/s)  LR: 1.043e-05  Data: 0.009 (0.021)
Train: 592 [ 250/1251 ( 20%)]  Loss: 3.230 (3.30)  Time: 0.693s, 1477.31/s  (0.698s, 1467.06/s)  LR: 1.043e-05  Data: 0.011 (0.019)
Train: 592 [ 300/1251 ( 24%)]  Loss: 3.497 (3.33)  Time: 0.697s, 1469.01/s  (0.697s, 1468.15/s)  LR: 1.043e-05  Data: 0.010 (0.018)
Train: 592 [ 350/1251 ( 28%)]  Loss: 3.231 (3.32)  Time: 0.746s, 1371.74/s  (0.698s, 1467.98/s)  LR: 1.043e-05  Data: 0.017 (0.017)
Train: 592 [ 400/1251 ( 32%)]  Loss: 3.126 (3.30)  Time: 0.691s, 1482.52/s  (0.697s, 1468.31/s)  LR: 1.043e-05  Data: 0.011 (0.016)
Train: 592 [ 450/1251 ( 36%)]  Loss: 3.154 (3.28)  Time: 0.673s, 1522.35/s  (0.697s, 1469.70/s)  LR: 1.043e-05  Data: 0.011 (0.015)
Train: 592 [ 500/1251 ( 40%)]  Loss: 3.229 (3.28)  Time: 0.706s, 1451.44/s  (0.696s, 1470.94/s)  LR: 1.043e-05  Data: 0.009 (0.015)
Train: 592 [ 550/1251 ( 44%)]  Loss: 2.963 (3.25)  Time: 0.704s, 1454.06/s  (0.696s, 1472.13/s)  LR: 1.043e-05  Data: 0.009 (0.014)
Train: 592 [ 600/1251 ( 48%)]  Loss: 3.028 (3.23)  Time: 0.671s, 1526.96/s  (0.695s, 1472.50/s)  LR: 1.043e-05  Data: 0.010 (0.014)
Train: 592 [ 650/1251 ( 52%)]  Loss: 3.396 (3.25)  Time: 0.715s, 1432.86/s  (0.695s, 1473.21/s)  LR: 1.043e-05  Data: 0.009 (0.014)
Train: 592 [ 700/1251 ( 56%)]  Loss: 3.294 (3.25)  Time: 0.678s, 1510.68/s  (0.694s, 1474.47/s)  LR: 1.043e-05  Data: 0.009 (0.014)
Train: 592 [ 750/1251 ( 60%)]  Loss: 2.908 (3.23)  Time: 0.674s, 1519.06/s  (0.694s, 1475.28/s)  LR: 1.043e-05  Data: 0.010 (0.013)
Train: 592 [ 800/1251 ( 64%)]  Loss: 3.016 (3.22)  Time: 0.682s, 1502.23/s  (0.694s, 1475.87/s)  LR: 1.043e-05  Data: 0.011 (0.013)
Train: 592 [ 850/1251 ( 68%)]  Loss: 3.271 (3.22)  Time: 0.706s, 1451.19/s  (0.694s, 1475.83/s)  LR: 1.043e-05  Data: 0.010 (0.013)
Train: 592 [ 900/1251 ( 72%)]  Loss: 2.679 (3.19)  Time: 0.680s, 1506.25/s  (0.694s, 1476.46/s)  LR: 1.043e-05  Data: 0.012 (0.013)
Train: 592 [ 950/1251 ( 76%)]  Loss: 3.514 (3.21)  Time: 0.672s, 1523.28/s  (0.694s, 1475.87/s)  LR: 1.043e-05  Data: 0.011 (0.013)
Train: 592 [1000/1251 ( 80%)]  Loss: 3.060 (3.20)  Time: 0.717s, 1427.90/s  (0.694s, 1475.75/s)  LR: 1.043e-05  Data: 0.009 (0.013)
Train: 592 [1050/1251 ( 84%)]  Loss: 3.555 (3.22)  Time: 0.672s, 1523.87/s  (0.694s, 1475.33/s)  LR: 1.043e-05  Data: 0.012 (0.013)
Train: 592 [1100/1251 ( 88%)]  Loss: 2.825 (3.20)  Time: 0.684s, 1497.95/s  (0.694s, 1475.55/s)  LR: 1.043e-05  Data: 0.015 (0.012)
Train: 592 [1150/1251 ( 92%)]  Loss: 3.190 (3.20)  Time: 0.672s, 1524.12/s  (0.694s, 1475.55/s)  LR: 1.043e-05  Data: 0.011 (0.012)
Train: 592 [1200/1251 ( 96%)]  Loss: 2.986 (3.19)  Time: 0.677s, 1512.43/s  (0.694s, 1476.03/s)  LR: 1.043e-05  Data: 0.010 (0.012)
Train: 592 [1250/1251 (100%)]  Loss: 2.958 (3.18)  Time: 0.659s, 1553.98/s  (0.694s, 1475.92/s)  LR: 1.043e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.504 (1.504)  Loss:  0.6870 (0.6870)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.136 (0.592)  Loss:  0.7749 (1.1469)  Acc@1: 87.3821 (79.3200)  Acc@5: 97.2877 (94.5460)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-589.pth.tar', 79.37800013183593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-590.pth.tar', 79.34200002685547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-587.pth.tar', 79.33600010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-581.pth.tar', 79.3300001586914)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-592.pth.tar', 79.320000078125)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-584.pth.tar', 79.30600005371093)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-585.pth.tar', 79.29000010498046)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-577.pth.tar', 79.28600010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-591.pth.tar', 79.27799997558594)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-578.pth.tar', 79.258000078125)

Train: 593 [   0/1251 (  0%)]  Loss: 3.166 (3.17)  Time: 2.234s,  458.27/s  (2.234s,  458.27/s)  LR: 1.033e-05  Data: 1.563 (1.563)
Train: 593 [  50/1251 (  4%)]  Loss: 3.479 (3.32)  Time: 0.675s, 1516.68/s  (0.731s, 1400.41/s)  LR: 1.033e-05  Data: 0.010 (0.045)
Train: 593 [ 100/1251 (  8%)]  Loss: 3.294 (3.31)  Time: 0.717s, 1427.71/s  (0.710s, 1442.58/s)  LR: 1.033e-05  Data: 0.010 (0.028)
Train: 593 [ 150/1251 ( 12%)]  Loss: 3.285 (3.31)  Time: 0.674s, 1519.29/s  (0.703s, 1457.15/s)  LR: 1.033e-05  Data: 0.013 (0.022)
Train: 593 [ 200/1251 ( 16%)]  Loss: 3.173 (3.28)  Time: 0.673s, 1521.07/s  (0.701s, 1460.87/s)  LR: 1.033e-05  Data: 0.010 (0.019)
Train: 593 [ 250/1251 ( 20%)]  Loss: 3.313 (3.28)  Time: 0.670s, 1528.24/s  (0.698s, 1467.91/s)  LR: 1.033e-05  Data: 0.010 (0.018)
Train: 593 [ 300/1251 ( 24%)]  Loss: 3.130 (3.26)  Time: 0.674s, 1518.82/s  (0.697s, 1470.07/s)  LR: 1.033e-05  Data: 0.010 (0.016)
Train: 593 [ 350/1251 ( 28%)]  Loss: 2.921 (3.22)  Time: 0.729s, 1404.19/s  (0.696s, 1471.39/s)  LR: 1.033e-05  Data: 0.009 (0.016)
Train: 593 [ 400/1251 ( 32%)]  Loss: 3.378 (3.24)  Time: 0.704s, 1454.82/s  (0.695s, 1473.80/s)  LR: 1.033e-05  Data: 0.009 (0.015)
Train: 593 [ 450/1251 ( 36%)]  Loss: 3.391 (3.25)  Time: 0.708s, 1447.24/s  (0.695s, 1474.36/s)  LR: 1.033e-05  Data: 0.010 (0.014)
Train: 593 [ 500/1251 ( 40%)]  Loss: 3.391 (3.27)  Time: 0.672s, 1524.43/s  (0.694s, 1476.06/s)  LR: 1.033e-05  Data: 0.010 (0.014)
Train: 593 [ 550/1251 ( 44%)]  Loss: 3.499 (3.28)  Time: 0.674s, 1520.15/s  (0.693s, 1477.34/s)  LR: 1.033e-05  Data: 0.011 (0.014)
Train: 593 [ 600/1251 ( 48%)]  Loss: 3.035 (3.27)  Time: 0.682s, 1502.28/s  (0.692s, 1478.77/s)  LR: 1.033e-05  Data: 0.009 (0.013)
Train: 593 [ 650/1251 ( 52%)]  Loss: 3.043 (3.25)  Time: 0.690s, 1484.60/s  (0.692s, 1478.95/s)  LR: 1.033e-05  Data: 0.013 (0.013)
Train: 593 [ 700/1251 ( 56%)]  Loss: 3.222 (3.25)  Time: 0.672s, 1523.81/s  (0.692s, 1479.04/s)  LR: 1.033e-05  Data: 0.011 (0.013)
Train: 593 [ 750/1251 ( 60%)]  Loss: 3.175 (3.24)  Time: 0.670s, 1528.63/s  (0.692s, 1479.25/s)  LR: 1.033e-05  Data: 0.010 (0.013)
Train: 593 [ 800/1251 ( 64%)]  Loss: 3.412 (3.25)  Time: 0.679s, 1507.04/s  (0.692s, 1480.36/s)  LR: 1.033e-05  Data: 0.011 (0.013)
Train: 593 [ 850/1251 ( 68%)]  Loss: 3.280 (3.25)  Time: 0.686s, 1493.79/s  (0.692s, 1480.50/s)  LR: 1.033e-05  Data: 0.009 (0.013)
Train: 593 [ 900/1251 ( 72%)]  Loss: 3.028 (3.24)  Time: 0.669s, 1531.36/s  (0.691s, 1481.04/s)  LR: 1.033e-05  Data: 0.013 (0.012)
Train: 593 [ 950/1251 ( 76%)]  Loss: 3.308 (3.25)  Time: 0.715s, 1431.73/s  (0.691s, 1481.26/s)  LR: 1.033e-05  Data: 0.009 (0.012)
Train: 593 [1000/1251 ( 80%)]  Loss: 3.129 (3.24)  Time: 0.689s, 1486.54/s  (0.691s, 1481.64/s)  LR: 1.033e-05  Data: 0.009 (0.012)
Train: 593 [1050/1251 ( 84%)]  Loss: 3.382 (3.25)  Time: 0.666s, 1537.77/s  (0.691s, 1481.79/s)  LR: 1.033e-05  Data: 0.011 (0.012)
Train: 593 [1100/1251 ( 88%)]  Loss: 3.218 (3.25)  Time: 0.731s, 1401.06/s  (0.691s, 1481.37/s)  LR: 1.033e-05  Data: 0.010 (0.012)
Train: 593 [1150/1251 ( 92%)]  Loss: 3.180 (3.24)  Time: 0.672s, 1523.66/s  (0.691s, 1481.26/s)  LR: 1.033e-05  Data: 0.011 (0.012)
Train: 593 [1200/1251 ( 96%)]  Loss: 3.185 (3.24)  Time: 0.672s, 1523.97/s  (0.691s, 1481.10/s)  LR: 1.033e-05  Data: 0.009 (0.012)
Train: 593 [1250/1251 (100%)]  Loss: 3.017 (3.23)  Time: 0.659s, 1554.92/s  (0.691s, 1481.20/s)  LR: 1.033e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.507 (1.507)  Loss:  0.6909 (0.6909)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.136 (0.589)  Loss:  0.8037 (1.1709)  Acc@1: 87.1462 (79.2000)  Acc@5: 97.4057 (94.5200)
Train: 594 [   0/1251 (  0%)]  Loss: 3.342 (3.34)  Time: 2.330s,  439.48/s  (2.330s,  439.48/s)  LR: 1.024e-05  Data: 1.677 (1.677)
Train: 594 [  50/1251 (  4%)]  Loss: 3.382 (3.36)  Time: 0.708s, 1446.16/s  (0.731s, 1400.04/s)  LR: 1.024e-05  Data: 0.010 (0.052)
Train: 594 [ 100/1251 (  8%)]  Loss: 3.419 (3.38)  Time: 0.678s, 1510.09/s  (0.715s, 1433.04/s)  LR: 1.024e-05  Data: 0.010 (0.031)
Train: 594 [ 150/1251 ( 12%)]  Loss: 3.228 (3.34)  Time: 0.673s, 1520.50/s  (0.707s, 1448.69/s)  LR: 1.024e-05  Data: 0.011 (0.025)
Train: 594 [ 200/1251 ( 16%)]  Loss: 2.833 (3.24)  Time: 0.666s, 1538.17/s  (0.702s, 1458.42/s)  LR: 1.024e-05  Data: 0.011 (0.021)
Train: 594 [ 250/1251 ( 20%)]  Loss: 3.495 (3.28)  Time: 0.673s, 1522.09/s  (0.700s, 1463.21/s)  LR: 1.024e-05  Data: 0.011 (0.019)
Train: 594 [ 300/1251 ( 24%)]  Loss: 3.102 (3.26)  Time: 0.703s, 1456.71/s  (0.699s, 1464.11/s)  LR: 1.024e-05  Data: 0.009 (0.018)
Train: 594 [ 350/1251 ( 28%)]  Loss: 3.206 (3.25)  Time: 0.706s, 1450.02/s  (0.697s, 1468.60/s)  LR: 1.024e-05  Data: 0.009 (0.017)
Train: 594 [ 400/1251 ( 32%)]  Loss: 3.263 (3.25)  Time: 0.675s, 1516.02/s  (0.696s, 1471.50/s)  LR: 1.024e-05  Data: 0.012 (0.016)
Train: 594 [ 450/1251 ( 36%)]  Loss: 3.385 (3.27)  Time: 0.672s, 1523.36/s  (0.695s, 1472.56/s)  LR: 1.024e-05  Data: 0.011 (0.015)
Train: 594 [ 500/1251 ( 40%)]  Loss: 2.865 (3.23)  Time: 0.672s, 1522.76/s  (0.695s, 1472.67/s)  LR: 1.024e-05  Data: 0.011 (0.015)
Train: 594 [ 550/1251 ( 44%)]  Loss: 3.282 (3.23)  Time: 0.688s, 1488.89/s  (0.696s, 1471.29/s)  LR: 1.024e-05  Data: 0.011 (0.014)
Train: 594 [ 600/1251 ( 48%)]  Loss: 3.453 (3.25)  Time: 0.672s, 1523.03/s  (0.696s, 1471.53/s)  LR: 1.024e-05  Data: 0.011 (0.014)
Train: 594 [ 650/1251 ( 52%)]  Loss: 3.034 (3.23)  Time: 0.667s, 1535.55/s  (0.696s, 1472.23/s)  LR: 1.024e-05  Data: 0.009 (0.014)
Train: 594 [ 700/1251 ( 56%)]  Loss: 3.418 (3.25)  Time: 0.675s, 1516.11/s  (0.695s, 1473.56/s)  LR: 1.024e-05  Data: 0.011 (0.014)
Train: 594 [ 750/1251 ( 60%)]  Loss: 3.410 (3.26)  Time: 0.683s, 1499.62/s  (0.694s, 1474.58/s)  LR: 1.024e-05  Data: 0.009 (0.013)
Train: 594 [ 800/1251 ( 64%)]  Loss: 3.322 (3.26)  Time: 0.675s, 1517.09/s  (0.694s, 1475.58/s)  LR: 1.024e-05  Data: 0.010 (0.013)
Train: 594 [ 850/1251 ( 68%)]  Loss: 3.035 (3.25)  Time: 0.702s, 1459.07/s  (0.694s, 1476.16/s)  LR: 1.024e-05  Data: 0.010 (0.013)
Train: 594 [ 900/1251 ( 72%)]  Loss: 3.075 (3.24)  Time: 0.684s, 1497.24/s  (0.694s, 1475.80/s)  LR: 1.024e-05  Data: 0.011 (0.013)
Train: 594 [ 950/1251 ( 76%)]  Loss: 3.165 (3.24)  Time: 0.692s, 1480.48/s  (0.694s, 1476.35/s)  LR: 1.024e-05  Data: 0.010 (0.013)
Train: 594 [1000/1251 ( 80%)]  Loss: 3.137 (3.23)  Time: 0.676s, 1515.33/s  (0.693s, 1476.98/s)  LR: 1.024e-05  Data: 0.014 (0.013)
Train: 594 [1050/1251 ( 84%)]  Loss: 3.134 (3.23)  Time: 0.674s, 1519.76/s  (0.693s, 1476.94/s)  LR: 1.024e-05  Data: 0.011 (0.012)
Train: 594 [1100/1251 ( 88%)]  Loss: 3.302 (3.23)  Time: 0.704s, 1455.11/s  (0.693s, 1477.02/s)  LR: 1.024e-05  Data: 0.010 (0.012)
Train: 594 [1150/1251 ( 92%)]  Loss: 3.643 (3.25)  Time: 0.725s, 1413.30/s  (0.693s, 1477.57/s)  LR: 1.024e-05  Data: 0.009 (0.012)
Train: 594 [1200/1251 ( 96%)]  Loss: 2.815 (3.23)  Time: 0.703s, 1456.75/s  (0.693s, 1477.84/s)  LR: 1.024e-05  Data: 0.010 (0.012)
Train: 594 [1250/1251 (100%)]  Loss: 3.000 (3.22)  Time: 0.662s, 1546.08/s  (0.693s, 1477.93/s)  LR: 1.024e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.473 (1.473)  Loss:  0.6699 (0.6699)  Acc@1: 92.0898 (92.0898)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.136 (0.583)  Loss:  0.7695 (1.1351)  Acc@1: 87.2642 (79.2880)  Acc@5: 97.2877 (94.5560)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-589.pth.tar', 79.37800013183593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-590.pth.tar', 79.34200002685547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-587.pth.tar', 79.33600010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-581.pth.tar', 79.3300001586914)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-592.pth.tar', 79.320000078125)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-584.pth.tar', 79.30600005371093)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-585.pth.tar', 79.29000010498046)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-594.pth.tar', 79.28800002685547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-577.pth.tar', 79.28600010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-591.pth.tar', 79.27799997558594)

Train: 595 [   0/1251 (  0%)]  Loss: 3.130 (3.13)  Time: 2.166s,  472.68/s  (2.166s,  472.68/s)  LR: 1.017e-05  Data: 1.545 (1.545)
Train: 595 [  50/1251 (  4%)]  Loss: 3.329 (3.23)  Time: 0.666s, 1538.27/s  (0.735s, 1392.52/s)  LR: 1.017e-05  Data: 0.010 (0.048)
Train: 595 [ 100/1251 (  8%)]  Loss: 3.126 (3.20)  Time: 0.673s, 1522.66/s  (0.715s, 1431.70/s)  LR: 1.017e-05  Data: 0.010 (0.030)
Train: 595 [ 150/1251 ( 12%)]  Loss: 2.818 (3.10)  Time: 0.669s, 1530.32/s  (0.707s, 1448.06/s)  LR: 1.017e-05  Data: 0.009 (0.023)
Train: 595 [ 200/1251 ( 16%)]  Loss: 3.373 (3.16)  Time: 0.702s, 1459.70/s  (0.706s, 1451.27/s)  LR: 1.017e-05  Data: 0.009 (0.020)
Train: 595 [ 250/1251 ( 20%)]  Loss: 3.200 (3.16)  Time: 0.672s, 1523.81/s  (0.704s, 1455.29/s)  LR: 1.017e-05  Data: 0.010 (0.018)
Train: 595 [ 300/1251 ( 24%)]  Loss: 3.177 (3.16)  Time: 0.673s, 1520.45/s  (0.702s, 1459.60/s)  LR: 1.017e-05  Data: 0.011 (0.017)
Train: 595 [ 350/1251 ( 28%)]  Loss: 3.301 (3.18)  Time: 0.671s, 1526.17/s  (0.700s, 1463.32/s)  LR: 1.017e-05  Data: 0.012 (0.016)
Train: 595 [ 400/1251 ( 32%)]  Loss: 3.409 (3.21)  Time: 0.707s, 1449.27/s  (0.699s, 1463.99/s)  LR: 1.017e-05  Data: 0.009 (0.015)
Train: 595 [ 450/1251 ( 36%)]  Loss: 3.266 (3.21)  Time: 0.703s, 1457.28/s  (0.699s, 1465.42/s)  LR: 1.017e-05  Data: 0.009 (0.015)
Train: 595 [ 500/1251 ( 40%)]  Loss: 3.313 (3.22)  Time: 0.671s, 1525.64/s  (0.697s, 1468.25/s)  LR: 1.017e-05  Data: 0.009 (0.014)
Train: 595 [ 550/1251 ( 44%)]  Loss: 3.091 (3.21)  Time: 0.672s, 1524.94/s  (0.697s, 1468.22/s)  LR: 1.017e-05  Data: 0.011 (0.014)
Train: 595 [ 600/1251 ( 48%)]  Loss: 2.830 (3.18)  Time: 0.677s, 1513.19/s  (0.697s, 1469.94/s)  LR: 1.017e-05  Data: 0.010 (0.014)
Train: 595 [ 650/1251 ( 52%)]  Loss: 3.119 (3.18)  Time: 0.673s, 1522.28/s  (0.696s, 1471.51/s)  LR: 1.017e-05  Data: 0.010 (0.013)
Train: 595 [ 700/1251 ( 56%)]  Loss: 2.970 (3.16)  Time: 0.728s, 1406.17/s  (0.697s, 1469.87/s)  LR: 1.017e-05  Data: 0.011 (0.013)
Train: 595 [ 750/1251 ( 60%)]  Loss: 2.942 (3.15)  Time: 0.796s, 1286.56/s  (0.698s, 1467.28/s)  LR: 1.017e-05  Data: 0.010 (0.013)
Train: 595 [ 800/1251 ( 64%)]  Loss: 2.959 (3.14)  Time: 0.715s, 1431.75/s  (0.699s, 1465.39/s)  LR: 1.017e-05  Data: 0.009 (0.013)
Train: 595 [ 850/1251 ( 68%)]  Loss: 3.029 (3.13)  Time: 0.674s, 1518.93/s  (0.698s, 1466.05/s)  LR: 1.017e-05  Data: 0.011 (0.013)
Train: 595 [ 900/1251 ( 72%)]  Loss: 3.463 (3.15)  Time: 0.699s, 1465.14/s  (0.698s, 1467.33/s)  LR: 1.017e-05  Data: 0.009 (0.013)
Train: 595 [ 950/1251 ( 76%)]  Loss: 2.944 (3.14)  Time: 0.700s, 1463.85/s  (0.697s, 1468.58/s)  LR: 1.017e-05  Data: 0.009 (0.013)
Train: 595 [1000/1251 ( 80%)]  Loss: 3.261 (3.15)  Time: 0.679s, 1508.88/s  (0.697s, 1469.13/s)  LR: 1.017e-05  Data: 0.009 (0.013)
Train: 595 [1050/1251 ( 84%)]  Loss: 2.797 (3.13)  Time: 0.699s, 1464.79/s  (0.697s, 1469.77/s)  LR: 1.017e-05  Data: 0.009 (0.012)
Train: 595 [1100/1251 ( 88%)]  Loss: 3.336 (3.14)  Time: 0.721s, 1420.19/s  (0.697s, 1470.10/s)  LR: 1.017e-05  Data: 0.009 (0.012)
Train: 595 [1150/1251 ( 92%)]  Loss: 3.138 (3.14)  Time: 0.695s, 1472.81/s  (0.696s, 1470.50/s)  LR: 1.017e-05  Data: 0.008 (0.012)
Train: 595 [1200/1251 ( 96%)]  Loss: 2.982 (3.13)  Time: 0.674s, 1519.88/s  (0.696s, 1471.07/s)  LR: 1.017e-05  Data: 0.010 (0.012)
Train: 595 [1250/1251 (100%)]  Loss: 3.128 (3.13)  Time: 0.695s, 1473.87/s  (0.696s, 1471.03/s)  LR: 1.017e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.594 (1.594)  Loss:  0.6562 (0.6562)  Acc@1: 91.8945 (91.8945)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.137 (0.584)  Loss:  0.7661 (1.1328)  Acc@1: 86.7925 (79.2720)  Acc@5: 96.6981 (94.5280)
Train: 596 [   0/1251 (  0%)]  Loss: 3.402 (3.40)  Time: 2.093s,  489.18/s  (2.093s,  489.18/s)  LR: 1.011e-05  Data: 1.477 (1.477)
Train: 596 [  50/1251 (  4%)]  Loss: 3.410 (3.41)  Time: 0.704s, 1455.42/s  (0.730s, 1402.86/s)  LR: 1.011e-05  Data: 0.011 (0.047)
Train: 596 [ 100/1251 (  8%)]  Loss: 2.929 (3.25)  Time: 0.703s, 1455.88/s  (0.712s, 1438.92/s)  LR: 1.011e-05  Data: 0.010 (0.029)
Train: 596 [ 150/1251 ( 12%)]  Loss: 3.322 (3.27)  Time: 0.715s, 1432.33/s  (0.706s, 1450.16/s)  LR: 1.011e-05  Data: 0.009 (0.023)
Train: 596 [ 200/1251 ( 16%)]  Loss: 3.324 (3.28)  Time: 0.686s, 1493.72/s  (0.704s, 1455.02/s)  LR: 1.011e-05  Data: 0.016 (0.020)
Train: 596 [ 250/1251 ( 20%)]  Loss: 3.181 (3.26)  Time: 0.667s, 1535.32/s  (0.703s, 1457.34/s)  LR: 1.011e-05  Data: 0.010 (0.018)
Train: 596 [ 300/1251 ( 24%)]  Loss: 3.278 (3.26)  Time: 0.678s, 1509.30/s  (0.700s, 1462.70/s)  LR: 1.011e-05  Data: 0.010 (0.017)
Train: 596 [ 350/1251 ( 28%)]  Loss: 3.248 (3.26)  Time: 0.706s, 1449.78/s  (0.699s, 1465.80/s)  LR: 1.011e-05  Data: 0.011 (0.016)
Train: 596 [ 400/1251 ( 32%)]  Loss: 2.922 (3.22)  Time: 0.677s, 1511.44/s  (0.697s, 1469.56/s)  LR: 1.011e-05  Data: 0.011 (0.015)
Train: 596 [ 450/1251 ( 36%)]  Loss: 2.904 (3.19)  Time: 0.708s, 1446.77/s  (0.696s, 1470.63/s)  LR: 1.011e-05  Data: 0.009 (0.015)
Train: 596 [ 500/1251 ( 40%)]  Loss: 3.423 (3.21)  Time: 0.694s, 1475.53/s  (0.695s, 1472.40/s)  LR: 1.011e-05  Data: 0.009 (0.014)
Train: 596 [ 550/1251 ( 44%)]  Loss: 3.435 (3.23)  Time: 0.708s, 1445.79/s  (0.695s, 1473.26/s)  LR: 1.011e-05  Data: 0.009 (0.014)
Train: 596 [ 600/1251 ( 48%)]  Loss: 3.343 (3.24)  Time: 0.675s, 1517.94/s  (0.695s, 1474.08/s)  LR: 1.011e-05  Data: 0.010 (0.014)
Train: 596 [ 650/1251 ( 52%)]  Loss: 3.043 (3.23)  Time: 0.688s, 1488.53/s  (0.695s, 1473.27/s)  LR: 1.011e-05  Data: 0.013 (0.013)
Train: 596 [ 700/1251 ( 56%)]  Loss: 3.268 (3.23)  Time: 0.692s, 1479.82/s  (0.694s, 1474.55/s)  LR: 1.011e-05  Data: 0.012 (0.013)
Train: 596 [ 750/1251 ( 60%)]  Loss: 3.152 (3.22)  Time: 0.671s, 1526.36/s  (0.694s, 1474.64/s)  LR: 1.011e-05  Data: 0.011 (0.013)
Train: 596 [ 800/1251 ( 64%)]  Loss: 3.332 (3.23)  Time: 0.710s, 1441.80/s  (0.694s, 1475.37/s)  LR: 1.011e-05  Data: 0.009 (0.013)
Train: 596 [ 850/1251 ( 68%)]  Loss: 3.120 (3.22)  Time: 0.686s, 1493.13/s  (0.694s, 1476.07/s)  LR: 1.011e-05  Data: 0.017 (0.013)
Train: 596 [ 900/1251 ( 72%)]  Loss: 3.433 (3.24)  Time: 0.680s, 1505.51/s  (0.694s, 1476.11/s)  LR: 1.011e-05  Data: 0.014 (0.013)
Train: 596 [ 950/1251 ( 76%)]  Loss: 3.105 (3.23)  Time: 0.701s, 1461.03/s  (0.694s, 1475.92/s)  LR: 1.011e-05  Data: 0.011 (0.012)
Train: 596 [1000/1251 ( 80%)]  Loss: 3.084 (3.22)  Time: 0.686s, 1492.61/s  (0.694s, 1476.17/s)  LR: 1.011e-05  Data: 0.011 (0.012)
Train: 596 [1050/1251 ( 84%)]  Loss: 2.983 (3.21)  Time: 0.672s, 1524.82/s  (0.693s, 1476.90/s)  LR: 1.011e-05  Data: 0.010 (0.012)
Train: 596 [1100/1251 ( 88%)]  Loss: 3.299 (3.21)  Time: 0.682s, 1501.28/s  (0.694s, 1476.12/s)  LR: 1.011e-05  Data: 0.016 (0.012)
Train: 596 [1150/1251 ( 92%)]  Loss: 3.068 (3.21)  Time: 0.674s, 1520.25/s  (0.694s, 1476.48/s)  LR: 1.011e-05  Data: 0.011 (0.012)
Train: 596 [1200/1251 ( 96%)]  Loss: 2.742 (3.19)  Time: 0.671s, 1526.21/s  (0.694s, 1476.09/s)  LR: 1.011e-05  Data: 0.011 (0.012)
Train: 596 [1250/1251 (100%)]  Loss: 2.790 (3.17)  Time: 0.661s, 1549.49/s  (0.694s, 1475.62/s)  LR: 1.011e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.559 (1.559)  Loss:  0.7388 (0.7388)  Acc@1: 92.0898 (92.0898)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.136 (0.581)  Loss:  0.8218 (1.1955)  Acc@1: 87.2642 (79.3100)  Acc@5: 97.1698 (94.4720)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-589.pth.tar', 79.37800013183593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-590.pth.tar', 79.34200002685547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-587.pth.tar', 79.33600010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-581.pth.tar', 79.3300001586914)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-592.pth.tar', 79.320000078125)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-596.pth.tar', 79.31000002685546)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-584.pth.tar', 79.30600005371093)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-585.pth.tar', 79.29000010498046)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-594.pth.tar', 79.28800002685547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-577.pth.tar', 79.28600010498047)

Train: 597 [   0/1251 (  0%)]  Loss: 3.530 (3.53)  Time: 2.317s,  441.98/s  (2.317s,  441.98/s)  LR: 1.006e-05  Data: 1.684 (1.684)
Train: 597 [  50/1251 (  4%)]  Loss: 3.539 (3.53)  Time: 0.689s, 1485.98/s  (0.721s, 1420.96/s)  LR: 1.006e-05  Data: 0.009 (0.045)
Train: 597 [ 100/1251 (  8%)]  Loss: 3.346 (3.47)  Time: 0.671s, 1525.49/s  (0.706s, 1450.75/s)  LR: 1.006e-05  Data: 0.009 (0.028)
Train: 597 [ 150/1251 ( 12%)]  Loss: 3.211 (3.41)  Time: 0.685s, 1495.49/s  (0.702s, 1459.70/s)  LR: 1.006e-05  Data: 0.010 (0.022)
Train: 597 [ 200/1251 ( 16%)]  Loss: 2.910 (3.31)  Time: 0.682s, 1502.24/s  (0.700s, 1463.11/s)  LR: 1.006e-05  Data: 0.009 (0.019)
Train: 597 [ 250/1251 ( 20%)]  Loss: 3.350 (3.31)  Time: 0.673s, 1521.07/s  (0.696s, 1470.31/s)  LR: 1.006e-05  Data: 0.009 (0.017)
Train: 597 [ 300/1251 ( 24%)]  Loss: 3.257 (3.31)  Time: 0.709s, 1443.94/s  (0.697s, 1469.13/s)  LR: 1.006e-05  Data: 0.010 (0.016)
Train: 597 [ 350/1251 ( 28%)]  Loss: 3.169 (3.29)  Time: 0.673s, 1521.88/s  (0.696s, 1470.28/s)  LR: 1.006e-05  Data: 0.012 (0.015)
Train: 597 [ 400/1251 ( 32%)]  Loss: 3.396 (3.30)  Time: 0.728s, 1405.69/s  (0.696s, 1471.95/s)  LR: 1.006e-05  Data: 0.010 (0.015)
Train: 597 [ 450/1251 ( 36%)]  Loss: 3.250 (3.30)  Time: 0.706s, 1451.32/s  (0.696s, 1472.28/s)  LR: 1.006e-05  Data: 0.014 (0.014)
Train: 597 [ 500/1251 ( 40%)]  Loss: 2.896 (3.26)  Time: 0.681s, 1503.86/s  (0.695s, 1473.61/s)  LR: 1.006e-05  Data: 0.010 (0.014)
Train: 597 [ 550/1251 ( 44%)]  Loss: 2.703 (3.21)  Time: 0.667s, 1534.36/s  (0.694s, 1474.46/s)  LR: 1.006e-05  Data: 0.011 (0.014)
Train: 597 [ 600/1251 ( 48%)]  Loss: 3.158 (3.21)  Time: 0.701s, 1460.04/s  (0.694s, 1475.43/s)  LR: 1.006e-05  Data: 0.009 (0.013)
Train: 597 [ 650/1251 ( 52%)]  Loss: 3.198 (3.21)  Time: 0.667s, 1534.87/s  (0.694s, 1476.08/s)  LR: 1.006e-05  Data: 0.011 (0.013)
Train: 597 [ 700/1251 ( 56%)]  Loss: 3.118 (3.20)  Time: 0.731s, 1399.91/s  (0.694s, 1476.37/s)  LR: 1.006e-05  Data: 0.009 (0.013)
Train: 597 [ 750/1251 ( 60%)]  Loss: 3.328 (3.21)  Time: 0.751s, 1362.71/s  (0.694s, 1476.35/s)  LR: 1.006e-05  Data: 0.010 (0.013)
Train: 597 [ 800/1251 ( 64%)]  Loss: 3.155 (3.21)  Time: 0.671s, 1527.14/s  (0.693s, 1477.30/s)  LR: 1.006e-05  Data: 0.009 (0.013)
Train: 597 [ 850/1251 ( 68%)]  Loss: 2.762 (3.18)  Time: 0.678s, 1510.16/s  (0.693s, 1477.44/s)  LR: 1.006e-05  Data: 0.014 (0.012)
Train: 597 [ 900/1251 ( 72%)]  Loss: 3.452 (3.20)  Time: 0.792s, 1293.15/s  (0.693s, 1477.56/s)  LR: 1.006e-05  Data: 0.010 (0.012)
Train: 597 [ 950/1251 ( 76%)]  Loss: 2.959 (3.18)  Time: 0.706s, 1450.85/s  (0.693s, 1476.97/s)  LR: 1.006e-05  Data: 0.009 (0.012)
Train: 597 [1000/1251 ( 80%)]  Loss: 3.156 (3.18)  Time: 0.671s, 1526.34/s  (0.693s, 1476.85/s)  LR: 1.006e-05  Data: 0.010 (0.012)
Train: 597 [1050/1251 ( 84%)]  Loss: 3.096 (3.18)  Time: 0.671s, 1525.60/s  (0.693s, 1477.02/s)  LR: 1.006e-05  Data: 0.010 (0.012)
Train: 597 [1100/1251 ( 88%)]  Loss: 2.930 (3.17)  Time: 0.692s, 1479.84/s  (0.693s, 1477.04/s)  LR: 1.006e-05  Data: 0.010 (0.012)
Train: 597 [1150/1251 ( 92%)]  Loss: 3.263 (3.17)  Time: 0.709s, 1444.48/s  (0.693s, 1477.35/s)  LR: 1.006e-05  Data: 0.009 (0.012)
Train: 597 [1200/1251 ( 96%)]  Loss: 3.390 (3.18)  Time: 0.781s, 1311.85/s  (0.693s, 1477.28/s)  LR: 1.006e-05  Data: 0.009 (0.012)
Train: 597 [1250/1251 (100%)]  Loss: 3.267 (3.18)  Time: 0.656s, 1561.32/s  (0.693s, 1477.33/s)  LR: 1.006e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.594 (1.594)  Loss:  0.6597 (0.6597)  Acc@1: 91.9922 (91.9922)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.136 (0.586)  Loss:  0.7852 (1.1321)  Acc@1: 86.9104 (79.2540)  Acc@5: 97.1698 (94.5200)
Train: 598 [   0/1251 (  0%)]  Loss: 3.251 (3.25)  Time: 2.235s,  458.16/s  (2.235s,  458.16/s)  LR: 1.003e-05  Data: 1.620 (1.620)
Train: 598 [  50/1251 (  4%)]  Loss: 3.178 (3.21)  Time: 0.703s, 1456.21/s  (0.733s, 1397.73/s)  LR: 1.003e-05  Data: 0.009 (0.049)
Train: 598 [ 100/1251 (  8%)]  Loss: 3.077 (3.17)  Time: 0.672s, 1523.71/s  (0.711s, 1440.19/s)  LR: 1.003e-05  Data: 0.011 (0.030)
Train: 598 [ 150/1251 ( 12%)]  Loss: 3.002 (3.13)  Time: 0.672s, 1523.14/s  (0.707s, 1449.29/s)  LR: 1.003e-05  Data: 0.010 (0.023)
Train: 598 [ 200/1251 ( 16%)]  Loss: 3.096 (3.12)  Time: 0.674s, 1520.38/s  (0.703s, 1455.78/s)  LR: 1.003e-05  Data: 0.010 (0.020)
Train: 598 [ 250/1251 ( 20%)]  Loss: 2.931 (3.09)  Time: 0.740s, 1383.32/s  (0.703s, 1457.07/s)  LR: 1.003e-05  Data: 0.009 (0.018)
Train: 598 [ 300/1251 ( 24%)]  Loss: 3.099 (3.09)  Time: 0.667s, 1536.11/s  (0.702s, 1458.76/s)  LR: 1.003e-05  Data: 0.010 (0.017)
Train: 598 [ 350/1251 ( 28%)]  Loss: 3.083 (3.09)  Time: 0.674s, 1519.22/s  (0.700s, 1463.03/s)  LR: 1.003e-05  Data: 0.011 (0.016)
Train: 598 [ 400/1251 ( 32%)]  Loss: 3.180 (3.10)  Time: 0.673s, 1522.65/s  (0.700s, 1462.72/s)  LR: 1.003e-05  Data: 0.010 (0.015)
Train: 598 [ 450/1251 ( 36%)]  Loss: 3.150 (3.10)  Time: 0.703s, 1456.76/s  (0.699s, 1464.91/s)  LR: 1.003e-05  Data: 0.009 (0.015)
Train: 598 [ 500/1251 ( 40%)]  Loss: 2.986 (3.09)  Time: 0.729s, 1404.85/s  (0.698s, 1466.03/s)  LR: 1.003e-05  Data: 0.013 (0.014)
Train: 598 [ 550/1251 ( 44%)]  Loss: 3.210 (3.10)  Time: 0.691s, 1481.09/s  (0.698s, 1466.12/s)  LR: 1.003e-05  Data: 0.011 (0.014)
Train: 598 [ 600/1251 ( 48%)]  Loss: 2.998 (3.10)  Time: 0.745s, 1374.36/s  (0.698s, 1466.25/s)  LR: 1.003e-05  Data: 0.010 (0.014)
Train: 598 [ 650/1251 ( 52%)]  Loss: 3.017 (3.09)  Time: 0.723s, 1416.35/s  (0.697s, 1468.39/s)  LR: 1.003e-05  Data: 0.009 (0.014)
Train: 598 [ 700/1251 ( 56%)]  Loss: 3.513 (3.12)  Time: 0.675s, 1517.07/s  (0.697s, 1469.42/s)  LR: 1.003e-05  Data: 0.011 (0.013)
Train: 598 [ 750/1251 ( 60%)]  Loss: 3.174 (3.12)  Time: 0.666s, 1537.55/s  (0.697s, 1469.94/s)  LR: 1.003e-05  Data: 0.011 (0.013)
Train: 598 [ 800/1251 ( 64%)]  Loss: 3.268 (3.13)  Time: 0.682s, 1502.20/s  (0.696s, 1471.20/s)  LR: 1.003e-05  Data: 0.010 (0.013)
Train: 598 [ 850/1251 ( 68%)]  Loss: 2.998 (3.12)  Time: 0.670s, 1527.74/s  (0.696s, 1471.89/s)  LR: 1.003e-05  Data: 0.010 (0.013)
Train: 598 [ 900/1251 ( 72%)]  Loss: 2.887 (3.11)  Time: 0.702s, 1457.72/s  (0.696s, 1471.09/s)  LR: 1.003e-05  Data: 0.010 (0.013)
Train: 598 [ 950/1251 ( 76%)]  Loss: 3.539 (3.13)  Time: 0.713s, 1435.62/s  (0.696s, 1471.70/s)  LR: 1.003e-05  Data: 0.011 (0.013)
Train: 598 [1000/1251 ( 80%)]  Loss: 2.833 (3.12)  Time: 0.706s, 1450.84/s  (0.696s, 1472.01/s)  LR: 1.003e-05  Data: 0.009 (0.012)
Train: 598 [1050/1251 ( 84%)]  Loss: 3.411 (3.13)  Time: 0.669s, 1530.21/s  (0.695s, 1472.88/s)  LR: 1.003e-05  Data: 0.009 (0.012)
Train: 598 [1100/1251 ( 88%)]  Loss: 2.845 (3.12)  Time: 0.673s, 1522.32/s  (0.695s, 1472.82/s)  LR: 1.003e-05  Data: 0.010 (0.012)
Train: 598 [1150/1251 ( 92%)]  Loss: 3.196 (3.12)  Time: 0.703s, 1456.51/s  (0.695s, 1473.76/s)  LR: 1.003e-05  Data: 0.011 (0.012)
Train: 598 [1200/1251 ( 96%)]  Loss: 3.011 (3.12)  Time: 0.681s, 1503.50/s  (0.695s, 1474.32/s)  LR: 1.003e-05  Data: 0.011 (0.012)
Train: 598 [1250/1251 (100%)]  Loss: 3.149 (3.12)  Time: 0.655s, 1563.24/s  (0.694s, 1474.91/s)  LR: 1.003e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.565 (1.565)  Loss:  0.7363 (0.7363)  Acc@1: 91.8945 (91.8945)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.137 (0.577)  Loss:  0.8291 (1.2113)  Acc@1: 86.7925 (79.2060)  Acc@5: 97.4057 (94.5020)
Train: 599 [   0/1251 (  0%)]  Loss: 3.327 (3.33)  Time: 2.338s,  438.07/s  (2.338s,  438.07/s)  LR: 1.001e-05  Data: 1.671 (1.671)
Train: 599 [  50/1251 (  4%)]  Loss: 3.021 (3.17)  Time: 0.745s, 1374.67/s  (0.734s, 1394.20/s)  LR: 1.001e-05  Data: 0.010 (0.046)
Train: 599 [ 100/1251 (  8%)]  Loss: 3.423 (3.26)  Time: 0.686s, 1492.61/s  (0.713s, 1435.25/s)  LR: 1.001e-05  Data: 0.014 (0.028)
Train: 599 [ 150/1251 ( 12%)]  Loss: 3.334 (3.28)  Time: 0.671s, 1526.08/s  (0.705s, 1452.28/s)  LR: 1.001e-05  Data: 0.010 (0.022)
Train: 599 [ 200/1251 ( 16%)]  Loss: 3.096 (3.24)  Time: 0.701s, 1461.69/s  (0.704s, 1454.01/s)  LR: 1.001e-05  Data: 0.009 (0.019)
Train: 599 [ 250/1251 ( 20%)]  Loss: 3.192 (3.23)  Time: 0.674s, 1520.21/s  (0.702s, 1458.80/s)  LR: 1.001e-05  Data: 0.010 (0.018)
Train: 599 [ 300/1251 ( 24%)]  Loss: 3.213 (3.23)  Time: 0.700s, 1462.59/s  (0.699s, 1464.28/s)  LR: 1.001e-05  Data: 0.009 (0.017)
Train: 599 [ 350/1251 ( 28%)]  Loss: 3.078 (3.21)  Time: 0.762s, 1343.20/s  (0.699s, 1464.75/s)  LR: 1.001e-05  Data: 0.010 (0.016)
Train: 599 [ 400/1251 ( 32%)]  Loss: 3.028 (3.19)  Time: 0.670s, 1529.40/s  (0.698s, 1466.49/s)  LR: 1.001e-05  Data: 0.010 (0.015)
Train: 599 [ 450/1251 ( 36%)]  Loss: 2.879 (3.16)  Time: 0.687s, 1491.10/s  (0.698s, 1468.03/s)  LR: 1.001e-05  Data: 0.010 (0.014)
Train: 599 [ 500/1251 ( 40%)]  Loss: 3.307 (3.17)  Time: 0.670s, 1528.89/s  (0.697s, 1469.34/s)  LR: 1.001e-05  Data: 0.010 (0.014)
Train: 599 [ 550/1251 ( 44%)]  Loss: 3.449 (3.20)  Time: 0.726s, 1410.07/s  (0.696s, 1470.67/s)  LR: 1.001e-05  Data: 0.010 (0.014)
Train: 599 [ 600/1251 ( 48%)]  Loss: 3.327 (3.21)  Time: 0.674s, 1519.97/s  (0.696s, 1472.30/s)  LR: 1.001e-05  Data: 0.015 (0.013)
Train: 599 [ 650/1251 ( 52%)]  Loss: 3.014 (3.19)  Time: 0.694s, 1474.93/s  (0.695s, 1472.76/s)  LR: 1.001e-05  Data: 0.010 (0.013)
Train: 599 [ 700/1251 ( 56%)]  Loss: 3.469 (3.21)  Time: 0.695s, 1472.65/s  (0.695s, 1474.09/s)  LR: 1.001e-05  Data: 0.009 (0.013)
Train: 599 [ 750/1251 ( 60%)]  Loss: 2.990 (3.20)  Time: 0.669s, 1531.28/s  (0.694s, 1474.55/s)  LR: 1.001e-05  Data: 0.010 (0.013)
Train: 599 [ 800/1251 ( 64%)]  Loss: 3.381 (3.21)  Time: 0.671s, 1525.01/s  (0.694s, 1474.90/s)  LR: 1.001e-05  Data: 0.011 (0.013)
Train: 599 [ 850/1251 ( 68%)]  Loss: 3.401 (3.22)  Time: 0.708s, 1447.06/s  (0.694s, 1474.78/s)  LR: 1.001e-05  Data: 0.010 (0.013)
Train: 599 [ 900/1251 ( 72%)]  Loss: 2.935 (3.20)  Time: 0.718s, 1426.31/s  (0.694s, 1475.33/s)  LR: 1.001e-05  Data: 0.010 (0.012)
Train: 599 [ 950/1251 ( 76%)]  Loss: 3.300 (3.21)  Time: 0.672s, 1524.07/s  (0.694s, 1475.92/s)  LR: 1.001e-05  Data: 0.009 (0.012)
Train: 599 [1000/1251 ( 80%)]  Loss: 3.254 (3.21)  Time: 0.709s, 1445.23/s  (0.694s, 1475.60/s)  LR: 1.001e-05  Data: 0.010 (0.012)
Train: 599 [1050/1251 ( 84%)]  Loss: 3.214 (3.21)  Time: 0.672s, 1524.22/s  (0.694s, 1476.18/s)  LR: 1.001e-05  Data: 0.010 (0.012)
Train: 599 [1100/1251 ( 88%)]  Loss: 3.330 (3.22)  Time: 0.674s, 1519.31/s  (0.694s, 1475.32/s)  LR: 1.001e-05  Data: 0.009 (0.012)
Train: 599 [1150/1251 ( 92%)]  Loss: 2.864 (3.20)  Time: 0.707s, 1448.69/s  (0.694s, 1475.58/s)  LR: 1.001e-05  Data: 0.011 (0.012)
Train: 599 [1200/1251 ( 96%)]  Loss: 2.861 (3.19)  Time: 0.701s, 1460.65/s  (0.694s, 1476.13/s)  LR: 1.001e-05  Data: 0.010 (0.012)
Train: 599 [1250/1251 (100%)]  Loss: 3.019 (3.18)  Time: 0.659s, 1553.76/s  (0.693s, 1476.96/s)  LR: 1.001e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.449 (1.449)  Loss:  0.7427 (0.7427)  Acc@1: 91.5039 (91.5039)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.136 (0.582)  Loss:  0.8398 (1.2115)  Acc@1: 87.5000 (79.2380)  Acc@5: 97.2877 (94.4940)
Train: 600 [   0/1251 (  0%)]  Loss: 3.190 (3.19)  Time: 2.356s,  434.60/s  (2.356s,  434.60/s)  LR: 1.000e-05  Data: 1.702 (1.702)
Train: 600 [  50/1251 (  4%)]  Loss: 3.438 (3.31)  Time: 0.716s, 1429.64/s  (0.748s, 1369.51/s)  LR: 1.000e-05  Data: 0.010 (0.051)
Train: 600 [ 100/1251 (  8%)]  Loss: 3.060 (3.23)  Time: 0.706s, 1451.20/s  (0.722s, 1418.94/s)  LR: 1.000e-05  Data: 0.009 (0.031)
Train: 600 [ 150/1251 ( 12%)]  Loss: 2.894 (3.15)  Time: 0.682s, 1501.81/s  (0.711s, 1441.14/s)  LR: 1.000e-05  Data: 0.011 (0.024)
Train: 600 [ 200/1251 ( 16%)]  Loss: 3.068 (3.13)  Time: 0.719s, 1424.40/s  (0.706s, 1449.78/s)  LR: 1.000e-05  Data: 0.009 (0.021)
Train: 600 [ 250/1251 ( 20%)]  Loss: 2.934 (3.10)  Time: 0.723s, 1416.06/s  (0.705s, 1452.27/s)  LR: 1.000e-05  Data: 0.009 (0.019)
Train: 600 [ 300/1251 ( 24%)]  Loss: 3.046 (3.09)  Time: 0.682s, 1501.40/s  (0.702s, 1458.20/s)  LR: 1.000e-05  Data: 0.009 (0.017)
Train: 600 [ 350/1251 ( 28%)]  Loss: 3.077 (3.09)  Time: 0.715s, 1432.85/s  (0.701s, 1461.47/s)  LR: 1.000e-05  Data: 0.011 (0.016)
Train: 600 [ 400/1251 ( 32%)]  Loss: 3.487 (3.13)  Time: 0.672s, 1524.29/s  (0.699s, 1464.12/s)  LR: 1.000e-05  Data: 0.010 (0.016)
Train: 600 [ 450/1251 ( 36%)]  Loss: 3.388 (3.16)  Time: 0.720s, 1422.52/s  (0.699s, 1465.00/s)  LR: 1.000e-05  Data: 0.010 (0.015)
Train: 600 [ 500/1251 ( 40%)]  Loss: 3.165 (3.16)  Time: 0.685s, 1494.66/s  (0.698s, 1466.03/s)  LR: 1.000e-05  Data: 0.009 (0.015)
Train: 600 [ 550/1251 ( 44%)]  Loss: 3.160 (3.16)  Time: 0.673s, 1521.22/s  (0.699s, 1465.58/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 600 [ 600/1251 ( 48%)]  Loss: 3.311 (3.17)  Time: 0.751s, 1363.79/s  (0.699s, 1465.31/s)  LR: 1.000e-05  Data: 0.009 (0.014)
Train: 600 [ 650/1251 ( 52%)]  Loss: 3.330 (3.18)  Time: 0.717s, 1428.62/s  (0.698s, 1466.31/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 600 [ 700/1251 ( 56%)]  Loss: 3.074 (3.17)  Time: 0.725s, 1412.83/s  (0.697s, 1468.17/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 600 [ 750/1251 ( 60%)]  Loss: 3.115 (3.17)  Time: 0.683s, 1500.09/s  (0.697s, 1468.88/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 600 [ 800/1251 ( 64%)]  Loss: 3.289 (3.18)  Time: 0.675s, 1515.98/s  (0.697s, 1469.14/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 600 [ 850/1251 ( 68%)]  Loss: 3.371 (3.19)  Time: 0.672s, 1524.03/s  (0.696s, 1470.61/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 600 [ 900/1251 ( 72%)]  Loss: 3.045 (3.18)  Time: 0.688s, 1487.75/s  (0.696s, 1470.52/s)  LR: 1.000e-05  Data: 0.015 (0.013)
Train: 600 [ 950/1251 ( 76%)]  Loss: 3.208 (3.18)  Time: 0.670s, 1528.81/s  (0.696s, 1470.72/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 600 [1000/1251 ( 80%)]  Loss: 3.117 (3.18)  Time: 0.691s, 1480.98/s  (0.696s, 1470.81/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 600 [1050/1251 ( 84%)]  Loss: 2.959 (3.17)  Time: 0.703s, 1457.30/s  (0.696s, 1470.57/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 600 [1100/1251 ( 88%)]  Loss: 3.054 (3.16)  Time: 0.682s, 1501.18/s  (0.696s, 1470.88/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 600 [1150/1251 ( 92%)]  Loss: 3.135 (3.16)  Time: 0.703s, 1456.35/s  (0.696s, 1471.57/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 600 [1200/1251 ( 96%)]  Loss: 3.218 (3.17)  Time: 0.672s, 1523.59/s  (0.696s, 1471.74/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 600 [1250/1251 (100%)]  Loss: 2.792 (3.15)  Time: 0.658s, 1556.33/s  (0.695s, 1472.51/s)  LR: 1.000e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.514 (1.514)  Loss:  0.7407 (0.7407)  Acc@1: 91.7969 (91.7969)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.136 (0.599)  Loss:  0.8491 (1.2084)  Acc@1: 87.2642 (79.2380)  Acc@5: 97.1698 (94.4920)
Train: 601 [   0/1251 (  0%)]  Loss: 3.014 (3.01)  Time: 2.396s,  427.29/s  (2.396s,  427.29/s)  LR: 1.000e-05  Data: 1.763 (1.763)
Train: 601 [  50/1251 (  4%)]  Loss: 2.948 (2.98)  Time: 0.678s, 1511.18/s  (0.726s, 1410.21/s)  LR: 1.000e-05  Data: 0.009 (0.046)
Train: 601 [ 100/1251 (  8%)]  Loss: 3.387 (3.12)  Time: 0.700s, 1462.75/s  (0.710s, 1442.90/s)  LR: 1.000e-05  Data: 0.009 (0.029)
Train: 601 [ 150/1251 ( 12%)]  Loss: 2.965 (3.08)  Time: 0.737s, 1389.64/s  (0.704s, 1453.63/s)  LR: 1.000e-05  Data: 0.009 (0.023)
Train: 601 [ 200/1251 ( 16%)]  Loss: 2.659 (2.99)  Time: 0.704s, 1454.48/s  (0.702s, 1459.36/s)  LR: 1.000e-05  Data: 0.010 (0.020)
Train: 601 [ 250/1251 ( 20%)]  Loss: 3.043 (3.00)  Time: 0.675s, 1516.14/s  (0.699s, 1464.28/s)  LR: 1.000e-05  Data: 0.010 (0.018)
Train: 601 [ 300/1251 ( 24%)]  Loss: 3.045 (3.01)  Time: 0.737s, 1388.99/s  (0.697s, 1468.42/s)  LR: 1.000e-05  Data: 0.009 (0.017)
Train: 601 [ 350/1251 ( 28%)]  Loss: 3.209 (3.03)  Time: 0.676s, 1515.36/s  (0.696s, 1471.03/s)  LR: 1.000e-05  Data: 0.009 (0.016)
Train: 601 [ 400/1251 ( 32%)]  Loss: 3.252 (3.06)  Time: 0.674s, 1519.59/s  (0.695s, 1472.48/s)  LR: 1.000e-05  Data: 0.009 (0.015)
Train: 601 [ 450/1251 ( 36%)]  Loss: 3.137 (3.07)  Time: 0.677s, 1511.94/s  (0.696s, 1470.84/s)  LR: 1.000e-05  Data: 0.012 (0.015)
Train: 601 [ 500/1251 ( 40%)]  Loss: 3.503 (3.11)  Time: 0.742s, 1380.45/s  (0.696s, 1471.27/s)  LR: 1.000e-05  Data: 0.012 (0.014)
Train: 601 [ 550/1251 ( 44%)]  Loss: 2.870 (3.09)  Time: 0.701s, 1461.71/s  (0.695s, 1472.68/s)  LR: 1.000e-05  Data: 0.009 (0.014)
Train: 601 [ 600/1251 ( 48%)]  Loss: 3.368 (3.11)  Time: 0.704s, 1453.82/s  (0.695s, 1473.79/s)  LR: 1.000e-05  Data: 0.009 (0.014)
Train: 601 [ 650/1251 ( 52%)]  Loss: 3.029 (3.10)  Time: 0.720s, 1422.62/s  (0.694s, 1474.98/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 601 [ 700/1251 ( 56%)]  Loss: 3.199 (3.11)  Time: 0.682s, 1501.96/s  (0.694s, 1474.92/s)  LR: 1.000e-05  Data: 0.015 (0.013)
Train: 601 [ 750/1251 ( 60%)]  Loss: 3.103 (3.11)  Time: 0.674s, 1518.17/s  (0.694s, 1474.89/s)  LR: 1.000e-05  Data: 0.008 (0.013)
Train: 601 [ 800/1251 ( 64%)]  Loss: 3.162 (3.11)  Time: 0.704s, 1454.36/s  (0.695s, 1474.44/s)  LR: 1.000e-05  Data: 0.009 (0.013)
Train: 601 [ 850/1251 ( 68%)]  Loss: 2.933 (3.10)  Time: 0.724s, 1415.04/s  (0.695s, 1474.41/s)  LR: 1.000e-05  Data: 0.013 (0.013)
Train: 601 [ 900/1251 ( 72%)]  Loss: 2.882 (3.09)  Time: 0.728s, 1407.16/s  (0.694s, 1475.21/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 601 [ 950/1251 ( 76%)]  Loss: 3.268 (3.10)  Time: 0.705s, 1452.72/s  (0.694s, 1474.62/s)  LR: 1.000e-05  Data: 0.009 (0.012)
Train: 601 [1000/1251 ( 80%)]  Loss: 3.453 (3.12)  Time: 0.672s, 1522.95/s  (0.694s, 1475.80/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 601 [1050/1251 ( 84%)]  Loss: 3.420 (3.13)  Time: 0.669s, 1530.43/s  (0.693s, 1476.83/s)  LR: 1.000e-05  Data: 0.009 (0.012)
Train: 601 [1100/1251 ( 88%)]  Loss: 2.983 (3.12)  Time: 0.726s, 1410.13/s  (0.693s, 1476.89/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 601 [1150/1251 ( 92%)]  Loss: 3.175 (3.13)  Time: 0.671s, 1526.98/s  (0.694s, 1475.88/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 601 [1200/1251 ( 96%)]  Loss: 3.163 (3.13)  Time: 0.721s, 1420.15/s  (0.694s, 1476.01/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 601 [1250/1251 (100%)]  Loss: 3.537 (3.14)  Time: 0.661s, 1549.70/s  (0.693s, 1476.57/s)  LR: 1.000e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.558 (1.558)  Loss:  0.7832 (0.7832)  Acc@1: 91.7969 (91.7969)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.137 (0.581)  Loss:  0.8750 (1.2465)  Acc@1: 87.0283 (79.1120)  Acc@5: 97.2877 (94.4620)
Train: 602 [   0/1251 (  0%)]  Loss: 2.775 (2.77)  Time: 2.222s,  460.89/s  (2.222s,  460.89/s)  LR: 1.000e-05  Data: 1.552 (1.552)
Train: 602 [  50/1251 (  4%)]  Loss: 3.379 (3.08)  Time: 0.669s, 1531.32/s  (0.733s, 1396.91/s)  LR: 1.000e-05  Data: 0.009 (0.046)
Train: 602 [ 100/1251 (  8%)]  Loss: 3.272 (3.14)  Time: 0.702s, 1459.06/s  (0.714s, 1434.62/s)  LR: 1.000e-05  Data: 0.009 (0.028)
Train: 602 [ 150/1251 ( 12%)]  Loss: 3.151 (3.14)  Time: 0.715s, 1431.47/s  (0.709s, 1444.79/s)  LR: 1.000e-05  Data: 0.013 (0.022)
Train: 602 [ 200/1251 ( 16%)]  Loss: 3.490 (3.21)  Time: 0.676s, 1515.33/s  (0.706s, 1449.50/s)  LR: 1.000e-05  Data: 0.010 (0.020)
Train: 602 [ 250/1251 ( 20%)]  Loss: 3.207 (3.21)  Time: 0.694s, 1474.79/s  (0.705s, 1452.35/s)  LR: 1.000e-05  Data: 0.009 (0.018)
Train: 602 [ 300/1251 ( 24%)]  Loss: 3.037 (3.19)  Time: 0.730s, 1403.64/s  (0.703s, 1456.31/s)  LR: 1.000e-05  Data: 0.030 (0.017)
Train: 602 [ 350/1251 ( 28%)]  Loss: 3.262 (3.20)  Time: 0.673s, 1521.31/s  (0.701s, 1460.09/s)  LR: 1.000e-05  Data: 0.009 (0.016)
Train: 602 [ 400/1251 ( 32%)]  Loss: 3.099 (3.19)  Time: 0.672s, 1524.38/s  (0.700s, 1462.73/s)  LR: 1.000e-05  Data: 0.011 (0.015)
Train: 602 [ 450/1251 ( 36%)]  Loss: 3.248 (3.19)  Time: 0.712s, 1439.08/s  (0.700s, 1462.40/s)  LR: 1.000e-05  Data: 0.009 (0.015)
Train: 602 [ 500/1251 ( 40%)]  Loss: 3.263 (3.20)  Time: 0.751s, 1363.31/s  (0.700s, 1463.83/s)  LR: 1.000e-05  Data: 0.009 (0.014)
Train: 602 [ 550/1251 ( 44%)]  Loss: 3.117 (3.19)  Time: 0.723s, 1416.22/s  (0.699s, 1464.73/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 602 [ 600/1251 ( 48%)]  Loss: 2.993 (3.18)  Time: 0.677s, 1513.59/s  (0.699s, 1465.74/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 602 [ 650/1251 ( 52%)]  Loss: 2.867 (3.15)  Time: 0.686s, 1491.78/s  (0.698s, 1468.07/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 602 [ 700/1251 ( 56%)]  Loss: 3.195 (3.16)  Time: 0.676s, 1515.16/s  (0.697s, 1468.67/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 602 [ 750/1251 ( 60%)]  Loss: 3.384 (3.17)  Time: 0.672s, 1524.57/s  (0.697s, 1469.81/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 602 [ 800/1251 ( 64%)]  Loss: 3.023 (3.16)  Time: 0.718s, 1426.04/s  (0.697s, 1469.83/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 602 [ 850/1251 ( 68%)]  Loss: 3.457 (3.18)  Time: 0.667s, 1535.71/s  (0.696s, 1470.71/s)  LR: 1.000e-05  Data: 0.009 (0.013)
Train: 602 [ 900/1251 ( 72%)]  Loss: 3.253 (3.18)  Time: 0.672s, 1524.15/s  (0.696s, 1471.07/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 602 [ 950/1251 ( 76%)]  Loss: 3.215 (3.18)  Time: 0.692s, 1479.38/s  (0.696s, 1471.99/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 602 [1000/1251 ( 80%)]  Loss: 3.178 (3.18)  Time: 0.677s, 1512.44/s  (0.696s, 1471.99/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 602 [1050/1251 ( 84%)]  Loss: 3.274 (3.19)  Time: 0.668s, 1532.73/s  (0.695s, 1472.81/s)  LR: 1.000e-05  Data: 0.009 (0.012)
Train: 602 [1100/1251 ( 88%)]  Loss: 2.925 (3.18)  Time: 0.677s, 1513.53/s  (0.695s, 1473.90/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 602 [1150/1251 ( 92%)]  Loss: 3.143 (3.18)  Time: 0.675s, 1517.50/s  (0.695s, 1473.34/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 602 [1200/1251 ( 96%)]  Loss: 3.106 (3.17)  Time: 0.759s, 1348.86/s  (0.695s, 1472.89/s)  LR: 1.000e-05  Data: 0.015 (0.012)
Train: 602 [1250/1251 (100%)]  Loss: 3.115 (3.17)  Time: 0.659s, 1555.01/s  (0.695s, 1472.68/s)  LR: 1.000e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.451 (1.451)  Loss:  0.7046 (0.7046)  Acc@1: 92.0898 (92.0898)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.136 (0.586)  Loss:  0.8125 (1.1652)  Acc@1: 87.0283 (79.2420)  Acc@5: 97.1698 (94.5020)
Train: 603 [   0/1251 (  0%)]  Loss: 3.329 (3.33)  Time: 2.298s,  445.65/s  (2.298s,  445.65/s)  LR: 1.000e-05  Data: 1.650 (1.650)
Train: 603 [  50/1251 (  4%)]  Loss: 3.592 (3.46)  Time: 0.670s, 1527.53/s  (0.721s, 1420.35/s)  LR: 1.000e-05  Data: 0.011 (0.048)
Train: 603 [ 100/1251 (  8%)]  Loss: 3.260 (3.39)  Time: 0.700s, 1463.76/s  (0.708s, 1447.06/s)  LR: 1.000e-05  Data: 0.009 (0.029)
Train: 603 [ 150/1251 ( 12%)]  Loss: 3.348 (3.38)  Time: 0.672s, 1523.71/s  (0.702s, 1458.93/s)  LR: 1.000e-05  Data: 0.009 (0.023)
Train: 603 [ 200/1251 ( 16%)]  Loss: 3.269 (3.36)  Time: 0.679s, 1507.94/s  (0.700s, 1462.88/s)  LR: 1.000e-05  Data: 0.014 (0.020)
Train: 603 [ 250/1251 ( 20%)]  Loss: 3.314 (3.35)  Time: 0.674s, 1518.91/s  (0.698s, 1466.75/s)  LR: 1.000e-05  Data: 0.010 (0.018)
Train: 603 [ 300/1251 ( 24%)]  Loss: 3.029 (3.31)  Time: 0.700s, 1462.65/s  (0.697s, 1469.10/s)  LR: 1.000e-05  Data: 0.009 (0.017)
Train: 603 [ 350/1251 ( 28%)]  Loss: 3.322 (3.31)  Time: 0.699s, 1465.75/s  (0.695s, 1472.49/s)  LR: 1.000e-05  Data: 0.010 (0.016)
Train: 603 [ 400/1251 ( 32%)]  Loss: 3.135 (3.29)  Time: 0.709s, 1443.71/s  (0.695s, 1473.77/s)  LR: 1.000e-05  Data: 0.010 (0.015)
Train: 603 [ 450/1251 ( 36%)]  Loss: 3.310 (3.29)  Time: 0.671s, 1526.38/s  (0.694s, 1475.12/s)  LR: 1.000e-05  Data: 0.010 (0.015)
Train: 603 [ 500/1251 ( 40%)]  Loss: 3.307 (3.29)  Time: 0.715s, 1431.73/s  (0.695s, 1474.19/s)  LR: 1.000e-05  Data: 0.009 (0.014)
Train: 603 [ 550/1251 ( 44%)]  Loss: 3.590 (3.32)  Time: 0.711s, 1441.17/s  (0.695s, 1473.76/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 603 [ 600/1251 ( 48%)]  Loss: 3.010 (3.29)  Time: 0.678s, 1511.06/s  (0.695s, 1474.35/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 603 [ 650/1251 ( 52%)]  Loss: 3.478 (3.31)  Time: 0.674s, 1519.40/s  (0.694s, 1476.25/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 603 [ 700/1251 ( 56%)]  Loss: 2.834 (3.28)  Time: 0.671s, 1525.55/s  (0.693s, 1477.27/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 603 [ 750/1251 ( 60%)]  Loss: 3.180 (3.27)  Time: 0.792s, 1292.65/s  (0.694s, 1476.56/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 603 [ 800/1251 ( 64%)]  Loss: 3.286 (3.27)  Time: 0.711s, 1440.13/s  (0.694s, 1475.69/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 603 [ 850/1251 ( 68%)]  Loss: 2.939 (3.25)  Time: 0.774s, 1322.45/s  (0.694s, 1476.53/s)  LR: 1.000e-05  Data: 0.009 (0.013)
Train: 603 [ 900/1251 ( 72%)]  Loss: 2.837 (3.23)  Time: 0.675s, 1516.13/s  (0.693s, 1477.52/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 603 [ 950/1251 ( 76%)]  Loss: 3.165 (3.23)  Time: 0.701s, 1461.50/s  (0.693s, 1477.83/s)  LR: 1.000e-05  Data: 0.009 (0.012)
Train: 603 [1000/1251 ( 80%)]  Loss: 3.356 (3.23)  Time: 0.673s, 1521.18/s  (0.693s, 1478.06/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 603 [1050/1251 ( 84%)]  Loss: 3.285 (3.24)  Time: 0.707s, 1448.72/s  (0.693s, 1478.04/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 603 [1100/1251 ( 88%)]  Loss: 3.280 (3.24)  Time: 0.681s, 1503.61/s  (0.693s, 1478.28/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 603 [1150/1251 ( 92%)]  Loss: 3.216 (3.24)  Time: 0.672s, 1523.61/s  (0.693s, 1478.56/s)  LR: 1.000e-05  Data: 0.009 (0.012)
Train: 603 [1200/1251 ( 96%)]  Loss: 3.101 (3.23)  Time: 0.705s, 1452.35/s  (0.692s, 1479.00/s)  LR: 1.000e-05  Data: 0.009 (0.012)
Train: 603 [1250/1251 (100%)]  Loss: 3.422 (3.24)  Time: 0.713s, 1436.41/s  (0.693s, 1477.84/s)  LR: 1.000e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.451 (1.451)  Loss:  0.6763 (0.6763)  Acc@1: 91.6016 (91.6016)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.136 (0.585)  Loss:  0.8066 (1.1579)  Acc@1: 87.0283 (79.3120)  Acc@5: 97.1698 (94.6120)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-589.pth.tar', 79.37800013183593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-590.pth.tar', 79.34200002685547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-587.pth.tar', 79.33600010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-581.pth.tar', 79.3300001586914)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-592.pth.tar', 79.320000078125)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-603.pth.tar', 79.31200005371093)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-596.pth.tar', 79.31000002685546)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-584.pth.tar', 79.30600005371093)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-585.pth.tar', 79.29000010498046)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-594.pth.tar', 79.28800002685547)

Train: 604 [   0/1251 (  0%)]  Loss: 3.447 (3.45)  Time: 2.272s,  450.66/s  (2.272s,  450.66/s)  LR: 1.000e-05  Data: 1.627 (1.627)
Train: 604 [  50/1251 (  4%)]  Loss: 3.039 (3.24)  Time: 0.704s, 1453.54/s  (0.739s, 1385.55/s)  LR: 1.000e-05  Data: 0.009 (0.056)
Train: 604 [ 100/1251 (  8%)]  Loss: 2.728 (3.07)  Time: 0.711s, 1441.10/s  (0.714s, 1433.82/s)  LR: 1.000e-05  Data: 0.010 (0.034)
Train: 604 [ 150/1251 ( 12%)]  Loss: 3.146 (3.09)  Time: 0.717s, 1428.82/s  (0.710s, 1442.27/s)  LR: 1.000e-05  Data: 0.011 (0.026)
Train: 604 [ 200/1251 ( 16%)]  Loss: 3.149 (3.10)  Time: 0.680s, 1505.24/s  (0.705s, 1453.29/s)  LR: 1.000e-05  Data: 0.009 (0.022)
Train: 604 [ 250/1251 ( 20%)]  Loss: 2.935 (3.07)  Time: 0.674s, 1519.47/s  (0.703s, 1455.89/s)  LR: 1.000e-05  Data: 0.009 (0.020)
Train: 604 [ 300/1251 ( 24%)]  Loss: 3.433 (3.13)  Time: 0.671s, 1526.38/s  (0.701s, 1461.16/s)  LR: 1.000e-05  Data: 0.011 (0.018)
Train: 604 [ 350/1251 ( 28%)]  Loss: 2.973 (3.11)  Time: 0.675s, 1517.26/s  (0.699s, 1465.81/s)  LR: 1.000e-05  Data: 0.010 (0.017)
Train: 604 [ 400/1251 ( 32%)]  Loss: 3.428 (3.14)  Time: 0.677s, 1512.42/s  (0.697s, 1468.38/s)  LR: 1.000e-05  Data: 0.012 (0.016)
Train: 604 [ 450/1251 ( 36%)]  Loss: 3.309 (3.16)  Time: 0.672s, 1524.79/s  (0.697s, 1468.56/s)  LR: 1.000e-05  Data: 0.011 (0.016)
Train: 604 [ 500/1251 ( 40%)]  Loss: 3.352 (3.18)  Time: 0.706s, 1449.63/s  (0.696s, 1470.31/s)  LR: 1.000e-05  Data: 0.011 (0.015)
Train: 604 [ 550/1251 ( 44%)]  Loss: 3.276 (3.18)  Time: 0.672s, 1523.30/s  (0.697s, 1469.88/s)  LR: 1.000e-05  Data: 0.012 (0.015)
Train: 604 [ 600/1251 ( 48%)]  Loss: 2.958 (3.17)  Time: 0.671s, 1526.07/s  (0.696s, 1470.23/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 604 [ 650/1251 ( 52%)]  Loss: 3.049 (3.16)  Time: 0.687s, 1490.44/s  (0.696s, 1471.24/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 604 [ 700/1251 ( 56%)]  Loss: 3.022 (3.15)  Time: 0.672s, 1524.47/s  (0.696s, 1472.29/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 604 [ 750/1251 ( 60%)]  Loss: 3.031 (3.14)  Time: 0.671s, 1525.89/s  (0.696s, 1471.95/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 604 [ 800/1251 ( 64%)]  Loss: 3.463 (3.16)  Time: 0.725s, 1413.21/s  (0.696s, 1472.23/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 604 [ 850/1251 ( 68%)]  Loss: 3.261 (3.17)  Time: 0.703s, 1457.05/s  (0.695s, 1472.82/s)  LR: 1.000e-05  Data: 0.009 (0.013)
Train: 604 [ 900/1251 ( 72%)]  Loss: 3.280 (3.17)  Time: 0.720s, 1422.00/s  (0.695s, 1473.41/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 604 [ 950/1251 ( 76%)]  Loss: 3.264 (3.18)  Time: 0.673s, 1521.34/s  (0.695s, 1473.02/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 604 [1000/1251 ( 80%)]  Loss: 3.051 (3.17)  Time: 0.673s, 1522.28/s  (0.695s, 1473.96/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 604 [1050/1251 ( 84%)]  Loss: 3.274 (3.18)  Time: 0.673s, 1521.66/s  (0.695s, 1473.96/s)  LR: 1.000e-05  Data: 0.014 (0.013)
Train: 604 [1100/1251 ( 88%)]  Loss: 3.271 (3.18)  Time: 0.725s, 1413.21/s  (0.695s, 1474.43/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 604 [1150/1251 ( 92%)]  Loss: 3.112 (3.18)  Time: 0.686s, 1493.47/s  (0.694s, 1475.25/s)  LR: 1.000e-05  Data: 0.009 (0.013)
Train: 604 [1200/1251 ( 96%)]  Loss: 3.394 (3.19)  Time: 0.672s, 1523.75/s  (0.694s, 1475.31/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 604 [1250/1251 (100%)]  Loss: 3.084 (3.18)  Time: 0.656s, 1561.32/s  (0.694s, 1475.84/s)  LR: 1.000e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.470 (1.470)  Loss:  0.7124 (0.7124)  Acc@1: 91.2109 (91.2109)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.137 (0.590)  Loss:  0.8257 (1.1877)  Acc@1: 86.6745 (79.2360)  Acc@5: 97.2877 (94.5120)
Train: 605 [   0/1251 (  0%)]  Loss: 2.789 (2.79)  Time: 2.198s,  465.96/s  (2.198s,  465.96/s)  LR: 1.000e-05  Data: 1.545 (1.545)
Train: 605 [  50/1251 (  4%)]  Loss: 3.180 (2.98)  Time: 0.676s, 1515.66/s  (0.740s, 1383.33/s)  LR: 1.000e-05  Data: 0.009 (0.052)
Train: 605 [ 100/1251 (  8%)]  Loss: 3.194 (3.05)  Time: 0.698s, 1468.06/s  (0.716s, 1429.59/s)  LR: 1.000e-05  Data: 0.009 (0.032)
Train: 605 [ 150/1251 ( 12%)]  Loss: 3.324 (3.12)  Time: 0.672s, 1524.08/s  (0.710s, 1442.66/s)  LR: 1.000e-05  Data: 0.009 (0.025)
Train: 605 [ 200/1251 ( 16%)]  Loss: 3.221 (3.14)  Time: 0.675s, 1518.00/s  (0.706s, 1450.58/s)  LR: 1.000e-05  Data: 0.010 (0.021)
Train: 605 [ 250/1251 ( 20%)]  Loss: 3.091 (3.13)  Time: 0.732s, 1399.19/s  (0.704s, 1455.28/s)  LR: 1.000e-05  Data: 0.011 (0.019)
Train: 605 [ 300/1251 ( 24%)]  Loss: 3.120 (3.13)  Time: 0.744s, 1377.23/s  (0.705s, 1453.33/s)  LR: 1.000e-05  Data: 0.010 (0.018)
Train: 605 [ 350/1251 ( 28%)]  Loss: 3.147 (3.13)  Time: 0.699s, 1464.96/s  (0.705s, 1451.46/s)  LR: 1.000e-05  Data: 0.010 (0.017)
Train: 605 [ 400/1251 ( 32%)]  Loss: 2.978 (3.12)  Time: 0.671s, 1525.33/s  (0.705s, 1451.87/s)  LR: 1.000e-05  Data: 0.011 (0.016)
Train: 605 [ 450/1251 ( 36%)]  Loss: 3.012 (3.11)  Time: 0.671s, 1526.24/s  (0.702s, 1457.86/s)  LR: 1.000e-05  Data: 0.010 (0.016)
Train: 605 [ 500/1251 ( 40%)]  Loss: 3.338 (3.13)  Time: 0.670s, 1528.08/s  (0.700s, 1463.87/s)  LR: 1.000e-05  Data: 0.010 (0.015)
Train: 605 [ 550/1251 ( 44%)]  Loss: 3.094 (3.12)  Time: 0.718s, 1425.89/s  (0.698s, 1467.64/s)  LR: 1.000e-05  Data: 0.011 (0.015)
Train: 605 [ 600/1251 ( 48%)]  Loss: 3.301 (3.14)  Time: 0.709s, 1443.43/s  (0.697s, 1469.28/s)  LR: 1.000e-05  Data: 0.009 (0.014)
Train: 605 [ 650/1251 ( 52%)]  Loss: 2.958 (3.12)  Time: 0.703s, 1456.17/s  (0.697s, 1470.20/s)  LR: 1.000e-05  Data: 0.009 (0.014)
Train: 605 [ 700/1251 ( 56%)]  Loss: 3.393 (3.14)  Time: 0.704s, 1453.81/s  (0.696s, 1470.56/s)  LR: 1.000e-05  Data: 0.009 (0.014)
Train: 605 [ 750/1251 ( 60%)]  Loss: 3.091 (3.14)  Time: 0.689s, 1487.10/s  (0.696s, 1471.14/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 605 [ 800/1251 ( 64%)]  Loss: 3.031 (3.13)  Time: 0.672s, 1524.94/s  (0.697s, 1470.08/s)  LR: 1.000e-05  Data: 0.009 (0.013)
Train: 605 [ 850/1251 ( 68%)]  Loss: 3.083 (3.13)  Time: 0.705s, 1451.90/s  (0.696s, 1470.82/s)  LR: 1.000e-05  Data: 0.009 (0.013)
Train: 605 [ 900/1251 ( 72%)]  Loss: 3.387 (3.14)  Time: 0.699s, 1464.66/s  (0.696s, 1470.69/s)  LR: 1.000e-05  Data: 0.009 (0.013)
Train: 605 [ 950/1251 ( 76%)]  Loss: 3.214 (3.15)  Time: 0.703s, 1457.03/s  (0.696s, 1471.53/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 605 [1000/1251 ( 80%)]  Loss: 3.126 (3.15)  Time: 0.672s, 1523.63/s  (0.695s, 1472.56/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 605 [1050/1251 ( 84%)]  Loss: 3.262 (3.15)  Time: 0.669s, 1529.89/s  (0.695s, 1473.25/s)  LR: 1.000e-05  Data: 0.009 (0.013)
Train: 605 [1100/1251 ( 88%)]  Loss: 3.109 (3.15)  Time: 0.689s, 1486.90/s  (0.695s, 1473.39/s)  LR: 1.000e-05  Data: 0.009 (0.013)
Train: 605 [1150/1251 ( 92%)]  Loss: 2.725 (3.13)  Time: 0.676s, 1513.75/s  (0.695s, 1473.40/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 605 [1200/1251 ( 96%)]  Loss: 3.411 (3.14)  Time: 0.709s, 1443.65/s  (0.695s, 1473.58/s)  LR: 1.000e-05  Data: 0.009 (0.012)
Train: 605 [1250/1251 (100%)]  Loss: 3.153 (3.14)  Time: 0.655s, 1564.41/s  (0.695s, 1473.45/s)  LR: 1.000e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.633 (1.633)  Loss:  0.7349 (0.7349)  Acc@1: 91.8945 (91.8945)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.136 (0.588)  Loss:  0.8433 (1.2166)  Acc@1: 87.0283 (79.2860)  Acc@5: 96.8160 (94.5160)
Train: 606 [   0/1251 (  0%)]  Loss: 3.177 (3.18)  Time: 2.200s,  465.44/s  (2.200s,  465.44/s)  LR: 1.000e-05  Data: 1.585 (1.585)
Train: 606 [  50/1251 (  4%)]  Loss: 2.999 (3.09)  Time: 0.683s, 1498.87/s  (0.721s, 1419.89/s)  LR: 1.000e-05  Data: 0.010 (0.050)
Train: 606 [ 100/1251 (  8%)]  Loss: 3.010 (3.06)  Time: 0.711s, 1440.29/s  (0.708s, 1445.63/s)  LR: 1.000e-05  Data: 0.011 (0.030)
Train: 606 [ 150/1251 ( 12%)]  Loss: 3.391 (3.14)  Time: 0.729s, 1405.01/s  (0.702s, 1457.70/s)  LR: 1.000e-05  Data: 0.010 (0.024)
Train: 606 [ 200/1251 ( 16%)]  Loss: 3.184 (3.15)  Time: 0.671s, 1527.12/s  (0.699s, 1464.39/s)  LR: 1.000e-05  Data: 0.011 (0.020)
Train: 606 [ 250/1251 ( 20%)]  Loss: 3.425 (3.20)  Time: 0.683s, 1499.26/s  (0.697s, 1468.85/s)  LR: 1.000e-05  Data: 0.012 (0.018)
Train: 606 [ 300/1251 ( 24%)]  Loss: 3.397 (3.23)  Time: 0.688s, 1488.75/s  (0.698s, 1467.68/s)  LR: 1.000e-05  Data: 0.010 (0.017)
Train: 606 [ 350/1251 ( 28%)]  Loss: 3.249 (3.23)  Time: 0.692s, 1479.95/s  (0.696s, 1470.83/s)  LR: 1.000e-05  Data: 0.009 (0.016)
Train: 606 [ 400/1251 ( 32%)]  Loss: 2.767 (3.18)  Time: 0.673s, 1522.50/s  (0.696s, 1471.67/s)  LR: 1.000e-05  Data: 0.010 (0.015)
Train: 606 [ 450/1251 ( 36%)]  Loss: 3.177 (3.18)  Time: 0.677s, 1513.23/s  (0.695s, 1472.72/s)  LR: 1.000e-05  Data: 0.011 (0.015)
Train: 606 [ 500/1251 ( 40%)]  Loss: 3.230 (3.18)  Time: 0.675s, 1517.83/s  (0.696s, 1471.12/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 606 [ 550/1251 ( 44%)]  Loss: 3.183 (3.18)  Time: 0.714s, 1433.58/s  (0.696s, 1471.60/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 606 [ 600/1251 ( 48%)]  Loss: 3.448 (3.20)  Time: 0.674s, 1519.99/s  (0.695s, 1472.83/s)  LR: 1.000e-05  Data: 0.009 (0.014)
Train: 606 [ 650/1251 ( 52%)]  Loss: 3.301 (3.21)  Time: 0.687s, 1490.00/s  (0.695s, 1472.89/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 606 [ 700/1251 ( 56%)]  Loss: 3.080 (3.20)  Time: 0.723s, 1415.78/s  (0.695s, 1474.02/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 606 [ 750/1251 ( 60%)]  Loss: 2.862 (3.18)  Time: 0.702s, 1458.43/s  (0.694s, 1474.95/s)  LR: 1.000e-05  Data: 0.009 (0.013)
Train: 606 [ 800/1251 ( 64%)]  Loss: 3.179 (3.18)  Time: 0.671s, 1526.26/s  (0.694s, 1475.80/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 606 [ 850/1251 ( 68%)]  Loss: 3.235 (3.18)  Time: 0.707s, 1447.93/s  (0.694s, 1476.21/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 606 [ 900/1251 ( 72%)]  Loss: 3.002 (3.17)  Time: 0.664s, 1541.47/s  (0.694s, 1475.71/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 606 [ 950/1251 ( 76%)]  Loss: 2.832 (3.16)  Time: 0.671s, 1525.83/s  (0.694s, 1476.22/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 606 [1000/1251 ( 80%)]  Loss: 3.228 (3.16)  Time: 0.671s, 1525.84/s  (0.693s, 1476.98/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 606 [1050/1251 ( 84%)]  Loss: 3.283 (3.17)  Time: 0.672s, 1523.35/s  (0.693s, 1477.41/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 606 [1100/1251 ( 88%)]  Loss: 2.923 (3.15)  Time: 0.706s, 1450.65/s  (0.693s, 1477.34/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 606 [1150/1251 ( 92%)]  Loss: 3.451 (3.17)  Time: 0.797s, 1284.73/s  (0.693s, 1477.55/s)  LR: 1.000e-05  Data: 0.009 (0.012)
Train: 606 [1200/1251 ( 96%)]  Loss: 3.068 (3.16)  Time: 0.725s, 1411.94/s  (0.693s, 1477.15/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 606 [1250/1251 (100%)]  Loss: 3.154 (3.16)  Time: 0.654s, 1566.85/s  (0.693s, 1477.32/s)  LR: 1.000e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.547 (1.547)  Loss:  0.6836 (0.6836)  Acc@1: 91.8945 (91.8945)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.137 (0.587)  Loss:  0.7764 (1.1494)  Acc@1: 87.0283 (79.2840)  Acc@5: 97.0519 (94.5180)
Train: 607 [   0/1251 (  0%)]  Loss: 2.922 (2.92)  Time: 2.443s,  419.14/s  (2.443s,  419.14/s)  LR: 1.000e-05  Data: 1.759 (1.759)
Train: 607 [  50/1251 (  4%)]  Loss: 3.267 (3.09)  Time: 0.670s, 1527.85/s  (0.730s, 1402.30/s)  LR: 1.000e-05  Data: 0.010 (0.052)
Train: 607 [ 100/1251 (  8%)]  Loss: 3.360 (3.18)  Time: 0.719s, 1423.34/s  (0.713s, 1436.85/s)  LR: 1.000e-05  Data: 0.009 (0.031)
Train: 607 [ 150/1251 ( 12%)]  Loss: 3.073 (3.16)  Time: 0.781s, 1311.43/s  (0.708s, 1446.01/s)  LR: 1.000e-05  Data: 0.009 (0.024)
Train: 607 [ 200/1251 ( 16%)]  Loss: 2.799 (3.08)  Time: 0.672s, 1524.50/s  (0.704s, 1454.33/s)  LR: 1.000e-05  Data: 0.011 (0.021)
Train: 607 [ 250/1251 ( 20%)]  Loss: 3.330 (3.13)  Time: 0.674s, 1519.87/s  (0.700s, 1461.89/s)  LR: 1.000e-05  Data: 0.010 (0.019)
Train: 607 [ 300/1251 ( 24%)]  Loss: 3.284 (3.15)  Time: 0.709s, 1444.56/s  (0.698s, 1466.78/s)  LR: 1.000e-05  Data: 0.011 (0.017)
Train: 607 [ 350/1251 ( 28%)]  Loss: 2.875 (3.11)  Time: 0.675s, 1517.60/s  (0.697s, 1468.11/s)  LR: 1.000e-05  Data: 0.009 (0.016)
Train: 607 [ 400/1251 ( 32%)]  Loss: 2.828 (3.08)  Time: 0.671s, 1525.47/s  (0.696s, 1470.62/s)  LR: 1.000e-05  Data: 0.013 (0.015)
Train: 607 [ 450/1251 ( 36%)]  Loss: 3.093 (3.08)  Time: 0.790s, 1295.99/s  (0.696s, 1471.27/s)  LR: 1.000e-05  Data: 0.011 (0.015)
Train: 607 [ 500/1251 ( 40%)]  Loss: 3.389 (3.11)  Time: 0.673s, 1521.10/s  (0.696s, 1472.01/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 607 [ 550/1251 ( 44%)]  Loss: 3.282 (3.13)  Time: 0.761s, 1346.35/s  (0.696s, 1471.47/s)  LR: 1.000e-05  Data: 0.009 (0.014)
Train: 607 [ 600/1251 ( 48%)]  Loss: 3.023 (3.12)  Time: 0.689s, 1485.69/s  (0.696s, 1471.80/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 607 [ 650/1251 ( 52%)]  Loss: 3.007 (3.11)  Time: 0.672s, 1523.21/s  (0.695s, 1473.18/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 607 [ 700/1251 ( 56%)]  Loss: 3.224 (3.12)  Time: 0.695s, 1472.72/s  (0.694s, 1474.54/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 607 [ 750/1251 ( 60%)]  Loss: 3.042 (3.11)  Time: 0.705s, 1452.11/s  (0.694s, 1475.01/s)  LR: 1.000e-05  Data: 0.009 (0.013)
Train: 607 [ 800/1251 ( 64%)]  Loss: 3.292 (3.12)  Time: 0.719s, 1423.98/s  (0.695s, 1474.18/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 607 [ 850/1251 ( 68%)]  Loss: 3.245 (3.13)  Time: 0.673s, 1521.65/s  (0.694s, 1474.61/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 607 [ 900/1251 ( 72%)]  Loss: 3.137 (3.13)  Time: 0.703s, 1456.27/s  (0.694s, 1475.52/s)  LR: 1.000e-05  Data: 0.009 (0.013)
Train: 607 [ 950/1251 ( 76%)]  Loss: 3.008 (3.12)  Time: 0.671s, 1525.85/s  (0.694s, 1476.09/s)  LR: 1.000e-05  Data: 0.009 (0.013)
Train: 607 [1000/1251 ( 80%)]  Loss: 2.877 (3.11)  Time: 0.670s, 1527.55/s  (0.693s, 1476.60/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 607 [1050/1251 ( 84%)]  Loss: 3.440 (3.13)  Time: 0.801s, 1278.59/s  (0.693s, 1477.03/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 607 [1100/1251 ( 88%)]  Loss: 2.979 (3.12)  Time: 0.676s, 1515.64/s  (0.693s, 1477.76/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 607 [1150/1251 ( 92%)]  Loss: 3.447 (3.13)  Time: 0.685s, 1495.63/s  (0.693s, 1477.45/s)  LR: 1.000e-05  Data: 0.015 (0.012)
Train: 607 [1200/1251 ( 96%)]  Loss: 3.134 (3.13)  Time: 0.675s, 1518.11/s  (0.693s, 1477.34/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 607 [1250/1251 (100%)]  Loss: 3.319 (3.14)  Time: 0.688s, 1488.53/s  (0.693s, 1476.67/s)  LR: 1.000e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.459 (1.459)  Loss:  0.7881 (0.7881)  Acc@1: 91.9922 (91.9922)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.136 (0.586)  Loss:  0.8867 (1.2536)  Acc@1: 86.7924 (79.2640)  Acc@5: 97.1698 (94.5120)
Train: 608 [   0/1251 (  0%)]  Loss: 3.127 (3.13)  Time: 2.285s,  448.05/s  (2.285s,  448.05/s)  LR: 1.000e-05  Data: 1.672 (1.672)
Train: 608 [  50/1251 (  4%)]  Loss: 3.290 (3.21)  Time: 0.692s, 1479.19/s  (0.738s, 1386.91/s)  LR: 1.000e-05  Data: 0.010 (0.053)
Train: 608 [ 100/1251 (  8%)]  Loss: 2.774 (3.06)  Time: 0.692s, 1479.94/s  (0.718s, 1426.55/s)  LR: 1.000e-05  Data: 0.011 (0.032)
Train: 608 [ 150/1251 ( 12%)]  Loss: 3.098 (3.07)  Time: 0.672s, 1524.21/s  (0.710s, 1442.01/s)  LR: 1.000e-05  Data: 0.011 (0.025)
Train: 608 [ 200/1251 ( 16%)]  Loss: 3.192 (3.10)  Time: 0.714s, 1435.02/s  (0.707s, 1448.52/s)  LR: 1.000e-05  Data: 0.011 (0.021)
Train: 608 [ 250/1251 ( 20%)]  Loss: 3.112 (3.10)  Time: 0.701s, 1460.55/s  (0.704s, 1454.53/s)  LR: 1.000e-05  Data: 0.009 (0.019)
Train: 608 [ 300/1251 ( 24%)]  Loss: 3.045 (3.09)  Time: 0.675s, 1516.11/s  (0.701s, 1459.74/s)  LR: 1.000e-05  Data: 0.011 (0.018)
Train: 608 [ 350/1251 ( 28%)]  Loss: 2.922 (3.07)  Time: 0.700s, 1462.41/s  (0.701s, 1460.55/s)  LR: 1.000e-05  Data: 0.014 (0.017)
Train: 608 [ 400/1251 ( 32%)]  Loss: 2.638 (3.02)  Time: 0.702s, 1459.10/s  (0.700s, 1462.69/s)  LR: 1.000e-05  Data: 0.010 (0.016)
Train: 608 [ 450/1251 ( 36%)]  Loss: 2.979 (3.02)  Time: 0.705s, 1451.85/s  (0.699s, 1465.12/s)  LR: 1.000e-05  Data: 0.010 (0.015)
Train: 608 [ 500/1251 ( 40%)]  Loss: 3.417 (3.05)  Time: 0.671s, 1524.99/s  (0.698s, 1467.70/s)  LR: 1.000e-05  Data: 0.011 (0.015)
Train: 608 [ 550/1251 ( 44%)]  Loss: 3.314 (3.08)  Time: 0.691s, 1480.93/s  (0.697s, 1469.45/s)  LR: 1.000e-05  Data: 0.009 (0.014)
Train: 608 [ 600/1251 ( 48%)]  Loss: 3.300 (3.09)  Time: 0.703s, 1456.53/s  (0.697s, 1469.19/s)  LR: 1.000e-05  Data: 0.009 (0.014)
Train: 608 [ 650/1251 ( 52%)]  Loss: 3.399 (3.11)  Time: 0.673s, 1520.67/s  (0.697s, 1470.10/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 608 [ 700/1251 ( 56%)]  Loss: 3.096 (3.11)  Time: 0.735s, 1393.64/s  (0.696s, 1470.33/s)  LR: 1.000e-05  Data: 0.009 (0.014)
Train: 608 [ 750/1251 ( 60%)]  Loss: 3.053 (3.11)  Time: 0.738s, 1388.01/s  (0.696s, 1471.46/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 608 [ 800/1251 ( 64%)]  Loss: 2.955 (3.10)  Time: 0.675s, 1516.90/s  (0.696s, 1472.28/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 608 [ 850/1251 ( 68%)]  Loss: 3.338 (3.11)  Time: 0.716s, 1429.81/s  (0.695s, 1472.43/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 608 [ 900/1251 ( 72%)]  Loss: 2.856 (3.10)  Time: 0.713s, 1435.64/s  (0.695s, 1472.40/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 608 [ 950/1251 ( 76%)]  Loss: 3.411 (3.12)  Time: 0.702s, 1459.61/s  (0.695s, 1472.38/s)  LR: 1.000e-05  Data: 0.009 (0.013)
Train: 608 [1000/1251 ( 80%)]  Loss: 3.260 (3.12)  Time: 0.672s, 1524.63/s  (0.695s, 1473.15/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 608 [1050/1251 ( 84%)]  Loss: 3.119 (3.12)  Time: 0.691s, 1482.17/s  (0.695s, 1473.66/s)  LR: 1.000e-05  Data: 0.014 (0.013)
Train: 608 [1100/1251 ( 88%)]  Loss: 2.846 (3.11)  Time: 0.674s, 1520.23/s  (0.695s, 1474.14/s)  LR: 1.000e-05  Data: 0.013 (0.012)
Train: 608 [1150/1251 ( 92%)]  Loss: 3.006 (3.11)  Time: 0.671s, 1526.23/s  (0.694s, 1475.17/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 608 [1200/1251 ( 96%)]  Loss: 3.197 (3.11)  Time: 0.672s, 1523.18/s  (0.694s, 1475.33/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 608 [1250/1251 (100%)]  Loss: 2.957 (3.10)  Time: 0.653s, 1567.69/s  (0.694s, 1475.65/s)  LR: 1.000e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.638 (1.638)  Loss:  0.6890 (0.6890)  Acc@1: 91.8945 (91.8945)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.136 (0.584)  Loss:  0.7974 (1.1625)  Acc@1: 86.9104 (79.3760)  Acc@5: 97.1698 (94.5300)
Current checkpoints:
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-589.pth.tar', 79.37800013183593)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-608.pth.tar', 79.37599987304688)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-590.pth.tar', 79.34200002685547)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-587.pth.tar', 79.33600010498047)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-581.pth.tar', 79.3300001586914)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-592.pth.tar', 79.320000078125)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-603.pth.tar', 79.31200005371093)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-596.pth.tar', 79.31000002685546)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-584.pth.tar', 79.30600005371093)
 ('./output/train/20220318-221818-hrnet18-224/checkpoint-585.pth.tar', 79.29000010498046)

Train: 609 [   0/1251 (  0%)]  Loss: 3.288 (3.29)  Time: 2.222s,  460.76/s  (2.222s,  460.76/s)  LR: 1.000e-05  Data: 1.606 (1.606)
Train: 609 [  50/1251 (  4%)]  Loss: 2.870 (3.08)  Time: 0.706s, 1450.74/s  (0.726s, 1410.91/s)  LR: 1.000e-05  Data: 0.011 (0.049)
Train: 609 [ 100/1251 (  8%)]  Loss: 3.032 (3.06)  Time: 0.708s, 1446.89/s  (0.709s, 1445.23/s)  LR: 1.000e-05  Data: 0.010 (0.030)
Train: 609 [ 150/1251 ( 12%)]  Loss: 3.360 (3.14)  Time: 0.671s, 1527.22/s  (0.705s, 1453.28/s)  LR: 1.000e-05  Data: 0.010 (0.023)
Train: 609 [ 200/1251 ( 16%)]  Loss: 3.344 (3.18)  Time: 0.706s, 1450.39/s  (0.703s, 1456.87/s)  LR: 1.000e-05  Data: 0.010 (0.020)
Train: 609 [ 250/1251 ( 20%)]  Loss: 3.268 (3.19)  Time: 0.728s, 1407.29/s  (0.700s, 1461.88/s)  LR: 1.000e-05  Data: 0.009 (0.018)
Train: 609 [ 300/1251 ( 24%)]  Loss: 2.793 (3.14)  Time: 0.709s, 1443.59/s  (0.701s, 1460.69/s)  LR: 1.000e-05  Data: 0.010 (0.017)
Train: 609 [ 350/1251 ( 28%)]  Loss: 3.380 (3.17)  Time: 0.690s, 1483.77/s  (0.699s, 1464.74/s)  LR: 1.000e-05  Data: 0.009 (0.016)
Train: 609 [ 400/1251 ( 32%)]  Loss: 3.421 (3.20)  Time: 0.673s, 1522.31/s  (0.699s, 1465.63/s)  LR: 1.000e-05  Data: 0.009 (0.015)
Train: 609 [ 450/1251 ( 36%)]  Loss: 3.254 (3.20)  Time: 0.703s, 1455.89/s  (0.697s, 1468.21/s)  LR: 1.000e-05  Data: 0.009 (0.015)
Train: 609 [ 500/1251 ( 40%)]  Loss: 3.346 (3.21)  Time: 0.697s, 1470.14/s  (0.697s, 1469.22/s)  LR: 1.000e-05  Data: 0.009 (0.014)
Train: 609 [ 550/1251 ( 44%)]  Loss: 3.402 (3.23)  Time: 0.671s, 1526.98/s  (0.696s, 1470.95/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 609 [ 600/1251 ( 48%)]  Loss: 3.346 (3.24)  Time: 0.751s, 1362.66/s  (0.696s, 1472.18/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 609 [ 650/1251 ( 52%)]  Loss: 3.298 (3.24)  Time: 0.672s, 1522.72/s  (0.696s, 1471.79/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 609 [ 700/1251 ( 56%)]  Loss: 3.181 (3.24)  Time: 0.682s, 1501.19/s  (0.696s, 1472.21/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 609 [ 750/1251 ( 60%)]  Loss: 3.395 (3.25)  Time: 0.711s, 1440.33/s  (0.695s, 1473.54/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 609 [ 800/1251 ( 64%)]  Loss: 3.427 (3.26)  Time: 0.671s, 1525.66/s  (0.695s, 1474.42/s)  LR: 1.000e-05  Data: 0.009 (0.013)
Train: 609 [ 850/1251 ( 68%)]  Loss: 3.360 (3.26)  Time: 0.724s, 1414.31/s  (0.695s, 1473.37/s)  LR: 1.000e-05  Data: 0.009 (0.013)
Train: 609 [ 900/1251 ( 72%)]  Loss: 3.201 (3.26)  Time: 0.715s, 1432.88/s  (0.695s, 1473.47/s)  LR: 1.000e-05  Data: 0.009 (0.013)
Train: 609 [ 950/1251 ( 76%)]  Loss: 3.351 (3.27)  Time: 0.672s, 1524.85/s  (0.695s, 1473.43/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 609 [1000/1251 ( 80%)]  Loss: 3.267 (3.27)  Time: 0.671s, 1525.07/s  (0.694s, 1474.57/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 609 [1050/1251 ( 84%)]  Loss: 3.415 (3.27)  Time: 0.672s, 1524.82/s  (0.694s, 1474.62/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 609 [1100/1251 ( 88%)]  Loss: 3.033 (3.26)  Time: 0.709s, 1444.25/s  (0.694s, 1474.97/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 609 [1150/1251 ( 92%)]  Loss: 3.245 (3.26)  Time: 0.702s, 1458.20/s  (0.694s, 1475.26/s)  LR: 1.000e-05  Data: 0.009 (0.012)
Train: 609 [1200/1251 ( 96%)]  Loss: 3.205 (3.26)  Time: 0.752s, 1361.29/s  (0.694s, 1475.81/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 609 [1250/1251 (100%)]  Loss: 3.082 (3.25)  Time: 0.656s, 1560.59/s  (0.694s, 1476.28/s)  LR: 1.000e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.593 (1.593)  Loss:  0.6924 (0.6924)  Acc@1: 91.8945 (91.8945)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.136 (0.589)  Loss:  0.8032 (1.1803)  Acc@1: 87.1462 (79.2760)  Acc@5: 97.1698 (94.5140)
*** Best metric: 79.37800013183593 (epoch 589)
