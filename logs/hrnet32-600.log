/home/baoshengyu/anaconda3/lib/python3.7/site-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Added key: store_based_barrier_key:1 to store for rank: 3
Added key: store_based_barrier_key:1 to store for rank: 5
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 7
Added key: store_based_barrier_key:1 to store for rank: 4
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 6
Added key: store_based_barrier_key:1 to store for rank: 0
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 8.
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 7, total 8.
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 4, total 8.
Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 6, total 8.
Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 5, total 8.
Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 8.
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 8.
HRNet(
  (conv0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
  )
  (conv1): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): Bottleneck(
              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (downsample): Sequential(
                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
          )
        )
      )
    )
  )
  (pool1): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv2): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
          )
        )
      )
    )
  )
  (pool2): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Identity()
      (1): Identity()
      (2): Sequential(
        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv3): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (1): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (2): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (3): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
  )
  (pool3): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Identity()
      (1): Identity()
      (2): Identity()
      (3): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv4): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(32, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
    (1): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(32, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
    (2): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(32, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
  )
  (classifier): CLSHead(
    (smooth_layers): ModuleList(
      (0): Bottleneck(
        (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): Bottleneck(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): Bottleneck(
        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (downsample_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (final_layer): Sequential(
      (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): AdaptiveAvgPool2d(output_size=1)
      (4): Conv2d(2048, 1000, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
HRNet(
  (conv0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
  )
  (conv1): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): Bottleneck(
              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (downsample): Sequential(
                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
          )
        )
      )
    )
  )
  (pool1): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv2): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
          )
        )
      )
    )
  )
  (pool2): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Identity()
      (1): Identity()
      (2): Sequential(
        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv3): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (1): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (2): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (3): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
  )
  (pool3): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Identity()
      (1): Identity()
      (2): Identity()
      (3): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv4): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(32, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
    (1): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(32, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
    (2): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(32, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
  )
  (classifier): CLSHead(
    (smooth_layers): ModuleList(
      (0): Bottleneck(
        (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): Bottleneck(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): Bottleneck(
        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (downsample_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (final_layer): Sequential(
      (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): AdaptiveAvgPool2d(output_size=1)
      (4): Conv2d(2048, 1000, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
HRNet(
  (conv0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
  )
  (conv1): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): Bottleneck(
              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (downsample): Sequential(
                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
          )
        )
      )
    )
  )
  (pool1): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv2): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
          )
        )
      )
    )
  )
  (pool2): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Identity()
      (1): Identity()
      (2): Sequential(
        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv3): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (1): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (2): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (3): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
  )
  (pool3): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Identity()
      (1): Identity()
      (2): Identity()
      (3): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv4): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(32, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
    (1): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(32, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
    (2): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(32, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
  )
  (classifier): CLSHead(
    (smooth_layers): ModuleList(
      (0): Bottleneck(
        (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): Bottleneck(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): Bottleneck(
        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (downsample_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (final_layer): Sequential(
      (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): AdaptiveAvgPool2d(output_size=1)
      (4): Conv2d(2048, 1000, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
HRNet(
  (conv0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
  )
  (conv1): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): Bottleneck(
              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (downsample): Sequential(
                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
          )
        )
      )
    )
  )
  (pool1): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv2): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
          )
        )
      )
    )
  )
  (pool2): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Identity()
      (1): Identity()
      (2): Sequential(
        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv3): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (1): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (2): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (3): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
  )
  (pool3): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Identity()
      (1): Identity()
      (2): Identity()
      (3): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv4): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(32, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
    (1): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(32, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
    (2): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(32, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
  )
  (classifier): CLSHead(
    (smooth_layers): ModuleList(
      (0): Bottleneck(
        (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): Bottleneck(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): Bottleneck(
        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (downsample_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (final_layer): Sequential(
      (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): AdaptiveAvgPool2d(output_size=1)
      (4): Conv2d(2048, 1000, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
HRNet(
  (conv0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
  )
  (conv1): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): Bottleneck(
              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (downsample): Sequential(
                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
          )
        )
      )
    )
  )
  (pool1): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv2): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
          )
        )
      )
    )
  )
  (pool2): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Identity()
      (1): Identity()
      (2): Sequential(
        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv3): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (1): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (2): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (3): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
  )
  (pool3): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Identity()
      (1): Identity()
      (2): Identity()
      (3): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv4): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(32, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
    (1): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(32, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
    (2): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(32, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
  )
  (classifier): CLSHead(
    (smooth_layers): ModuleList(
      (0): Bottleneck(
        (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): Bottleneck(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): Bottleneck(
        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (downsample_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (final_layer): Sequential(
      (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): AdaptiveAvgPool2d(output_size=1)
      (4): Conv2d(2048, 1000, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)HRNet(
  (conv0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
  )
  (conv1): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): Bottleneck(
              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (downsample): Sequential(
                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
          )
        )
      )
    )
  )
  (pool1): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv2): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
          )
        )
      )
    )
  )
  (pool2): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Identity()
      (1): Identity()
      (2): Sequential(
        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv3): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (1): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (2): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (3): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
  )
  (pool3): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Identity()
      (1): Identity()
      (2): Identity()
      (3): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv4): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(32, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
    (1): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(32, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
    (2): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(32, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
  )
  (classifier): CLSHead(
    (smooth_layers): ModuleList(
      (0): Bottleneck(
        (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): Bottleneck(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): Bottleneck(
        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (downsample_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (final_layer): Sequential(
      (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): AdaptiveAvgPool2d(output_size=1)
      (4): Conv2d(2048, 1000, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)HRNet(
  (conv0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
  )
  (conv1): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): Bottleneck(
              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (downsample): Sequential(
                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
          )
        )
      )
    )
  )
  (pool1): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv2): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
          )
        )
      )
    )
  )
  (pool2): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Identity()
      (1): Identity()
      (2): Sequential(
        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv3): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (1): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (2): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (3): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
  )
  (pool3): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Identity()
      (1): Identity()
      (2): Identity()
      (3): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv4): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(32, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
    (1): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(32, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
    (2): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(32, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
  )
  (classifier): CLSHead(
    (smooth_layers): ModuleList(
      (0): Bottleneck(
        (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): Bottleneck(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): Bottleneck(
        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (downsample_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (final_layer): Sequential(
      (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): AdaptiveAvgPool2d(output_size=1)
      (4): Conv2d(2048, 1000, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)


Model hrnet32 created, param count:41237832
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.875
HRNet(
  (conv0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
  )
  (conv1): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): Bottleneck(
              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (downsample): Sequential(
                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
          )
        )
      )
    )
  )
  (pool1): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv2): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
          )
        )
      )
    )
  )
  (pool2): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Identity()
      (1): Identity()
      (2): Sequential(
        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv3): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (1): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (2): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
    (3): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
          )
        )
      )
    )
  )
  (pool3): MRPoolModule(
    (transform_layers): ModuleList(
      (0): Identity()
      (1): Identity()
      (2): Identity()
      (3): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (conv4): Sequential(
    (0): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(32, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
    (1): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(32, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
    (2): MRConvModule(
      (stream_layer): MRStreamBlock(
        (stream_list): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (2): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (3): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (fusion_layer): MRFusionBlock(
        (fusion_list): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (2): Sequential(
              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=bilinear)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
            (3): Sequential(
              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=bilinear)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=bilinear)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(32, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Identity()
              )
            )
            (3): Identity()
          )
        )
      )
    )
  )
  (classifier): CLSHead(
    (smooth_layers): ModuleList(
      (0): Bottleneck(
        (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): Bottleneck(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): Bottleneck(
        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (downsample_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (final_layer): Sequential(
      (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): AdaptiveAvgPool2d(output_size=1)
      (4): Conv2d(2048, 1000, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
Using native Torch AMP. Training in mixed precision.
Using native Torch DistributedDataParallel.
Scheduled epochs: 610
Train: 0 [   0/1251 (  0%)]  Loss: 6.934 (6.93)  Time: 11.060s,   92.59/s  (11.060s,   92.59/s)  LR: 1.000e-06  Data: 2.117 (2.117)
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Train: 0 [  50/1251 (  4%)]  Loss: 6.939 (6.94)  Time: 0.786s, 1303.42/s  (1.047s,  978.48/s)  LR: 1.000e-06  Data: 0.018 (0.054)
Train: 0 [ 100/1251 (  8%)]  Loss: 6.948 (6.94)  Time: 0.802s, 1276.46/s  (0.925s, 1107.18/s)  LR: 1.000e-06  Data: 0.013 (0.033)
Train: 0 [ 150/1251 ( 12%)]  Loss: 6.930 (6.94)  Time: 0.810s, 1264.35/s  (0.883s, 1159.15/s)  LR: 1.000e-06  Data: 0.013 (0.026)
Train: 0 [ 200/1251 ( 16%)]  Loss: 6.931 (6.94)  Time: 0.808s, 1266.96/s  (0.863s, 1186.42/s)  LR: 1.000e-06  Data: 0.016 (0.023)
Train: 0 [ 250/1251 ( 20%)]  Loss: 6.912 (6.93)  Time: 0.825s, 1241.19/s  (0.850s, 1204.18/s)  LR: 1.000e-06  Data: 0.010 (0.020)
Train: 0 [ 300/1251 ( 24%)]  Loss: 6.939 (6.93)  Time: 0.812s, 1261.27/s  (0.842s, 1215.59/s)  LR: 1.000e-06  Data: 0.010 (0.019)
Train: 0 [ 350/1251 ( 28%)]  Loss: 6.927 (6.93)  Time: 0.775s, 1321.27/s  (0.836s, 1224.52/s)  LR: 1.000e-06  Data: 0.010 (0.018)
Train: 0 [ 400/1251 ( 32%)]  Loss: 6.935 (6.93)  Time: 0.806s, 1270.29/s  (0.832s, 1230.15/s)  LR: 1.000e-06  Data: 0.018 (0.017)
Train: 0 [ 450/1251 ( 36%)]  Loss: 6.919 (6.93)  Time: 0.817s, 1254.00/s  (0.829s, 1234.68/s)  LR: 1.000e-06  Data: 0.009 (0.017)
Train: 0 [ 500/1251 ( 40%)]  Loss: 6.916 (6.93)  Time: 0.810s, 1263.61/s  (0.827s, 1238.66/s)  LR: 1.000e-06  Data: 0.014 (0.016)
Train: 0 [ 550/1251 ( 44%)]  Loss: 6.907 (6.93)  Time: 0.782s, 1309.30/s  (0.824s, 1242.27/s)  LR: 1.000e-06  Data: 0.009 (0.016)
Train: 0 [ 600/1251 ( 48%)]  Loss: 6.912 (6.93)  Time: 0.806s, 1270.26/s  (0.822s, 1245.57/s)  LR: 1.000e-06  Data: 0.010 (0.015)
Train: 0 [ 650/1251 ( 52%)]  Loss: 6.912 (6.93)  Time: 0.778s, 1315.51/s  (0.820s, 1248.03/s)  LR: 1.000e-06  Data: 0.010 (0.015)
Train: 0 [ 700/1251 ( 56%)]  Loss: 6.915 (6.93)  Time: 0.785s, 1304.70/s  (0.819s, 1249.90/s)  LR: 1.000e-06  Data: 0.012 (0.015)
Train: 0 [ 750/1251 ( 60%)]  Loss: 6.911 (6.92)  Time: 0.785s, 1304.56/s  (0.818s, 1251.88/s)  LR: 1.000e-06  Data: 0.010 (0.014)
Train: 0 [ 800/1251 ( 64%)]  Loss: 6.917 (6.92)  Time: 0.798s, 1283.66/s  (0.816s, 1254.27/s)  LR: 1.000e-06  Data: 0.010 (0.014)
Train: 0 [ 850/1251 ( 68%)]  Loss: 6.905 (6.92)  Time: 0.778s, 1315.81/s  (0.815s, 1255.94/s)  LR: 1.000e-06  Data: 0.010 (0.014)
Train: 0 [ 900/1251 ( 72%)]  Loss: 6.911 (6.92)  Time: 0.795s, 1288.33/s  (0.815s, 1257.02/s)  LR: 1.000e-06  Data: 0.009 (0.014)
Train: 0 [ 950/1251 ( 76%)]  Loss: 6.922 (6.92)  Time: 0.813s, 1259.01/s  (0.814s, 1257.93/s)  LR: 1.000e-06  Data: 0.010 (0.014)
Train: 0 [1000/1251 ( 80%)]  Loss: 6.919 (6.92)  Time: 0.787s, 1301.94/s  (0.813s, 1259.07/s)  LR: 1.000e-06  Data: 0.010 (0.014)
Train: 0 [1050/1251 ( 84%)]  Loss: 6.918 (6.92)  Time: 0.781s, 1311.31/s  (0.813s, 1259.65/s)  LR: 1.000e-06  Data: 0.011 (0.014)
Train: 0 [1100/1251 ( 88%)]  Loss: 6.924 (6.92)  Time: 0.815s, 1255.72/s  (0.812s, 1260.38/s)  LR: 1.000e-06  Data: 0.010 (0.013)
Train: 0 [1150/1251 ( 92%)]  Loss: 6.916 (6.92)  Time: 0.820s, 1249.17/s  (0.812s, 1261.03/s)  LR: 1.000e-06  Data: 0.010 (0.013)
Train: 0 [1200/1251 ( 96%)]  Loss: 6.909 (6.92)  Time: 0.786s, 1302.96/s  (0.812s, 1261.80/s)  LR: 1.000e-06  Data: 0.014 (0.013)
Train: 0 [1250/1251 (100%)]  Loss: 6.912 (6.92)  Time: 0.802s, 1277.10/s  (0.811s, 1262.26/s)  LR: 1.000e-06  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.089 (2.089)  Loss:  6.8086 (6.8086)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  3.3203 ( 3.3203)
Test: [  48/48]  Time: 3.076 (0.683)  Loss:  6.8398 (6.9010)  Acc@1:  0.9434 ( 0.1740)  Acc@5:  3.3019 ( 0.7520)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-0.pth.tar', 0.17399999872207642)

Train: 1 [   0/1251 (  0%)]  Loss: 6.918 (6.92)  Time: 2.532s,  404.41/s  (2.532s,  404.41/s)  LR: 2.008e-04  Data: 1.689 (1.689)
Train: 1 [  50/1251 (  4%)]  Loss: 6.912 (6.91)  Time: 0.790s, 1296.70/s  (0.851s, 1203.56/s)  LR: 2.008e-04  Data: 0.011 (0.050)
Train: 1 [ 100/1251 (  8%)]  Loss: 6.917 (6.92)  Time: 0.846s, 1210.63/s  (0.829s, 1234.96/s)  LR: 2.008e-04  Data: 0.010 (0.031)
Train: 1 [ 150/1251 ( 12%)]  Loss: 6.882 (6.91)  Time: 0.815s, 1255.86/s  (0.823s, 1244.17/s)  LR: 2.008e-04  Data: 0.017 (0.025)
Train: 1 [ 200/1251 ( 16%)]  Loss: 6.875 (6.90)  Time: 0.807s, 1269.67/s  (0.819s, 1250.06/s)  LR: 2.008e-04  Data: 0.011 (0.021)
Train: 1 [ 250/1251 ( 20%)]  Loss: 6.856 (6.89)  Time: 0.825s, 1241.56/s  (0.818s, 1252.55/s)  LR: 2.008e-04  Data: 0.009 (0.019)
Train: 1 [ 300/1251 ( 24%)]  Loss: 6.834 (6.88)  Time: 0.796s, 1287.11/s  (0.818s, 1252.42/s)  LR: 2.008e-04  Data: 0.012 (0.018)
Train: 1 [ 350/1251 ( 28%)]  Loss: 6.756 (6.87)  Time: 0.779s, 1314.46/s  (0.817s, 1253.67/s)  LR: 2.008e-04  Data: 0.011 (0.017)
Train: 1 [ 400/1251 ( 32%)]  Loss: 6.765 (6.86)  Time: 0.814s, 1258.48/s  (0.816s, 1255.04/s)  LR: 2.008e-04  Data: 0.012 (0.016)
Train: 1 [ 450/1251 ( 36%)]  Loss: 6.736 (6.85)  Time: 0.779s, 1314.85/s  (0.816s, 1254.80/s)  LR: 2.008e-04  Data: 0.014 (0.016)
Train: 1 [ 500/1251 ( 40%)]  Loss: 6.698 (6.83)  Time: 0.845s, 1211.49/s  (0.816s, 1255.66/s)  LR: 2.008e-04  Data: 0.011 (0.015)
Train: 1 [ 550/1251 ( 44%)]  Loss: 6.761 (6.83)  Time: 0.782s, 1308.91/s  (0.815s, 1256.03/s)  LR: 2.008e-04  Data: 0.013 (0.015)
Train: 1 [ 600/1251 ( 48%)]  Loss: 6.664 (6.81)  Time: 0.825s, 1241.78/s  (0.814s, 1257.95/s)  LR: 2.008e-04  Data: 0.011 (0.015)
Train: 1 [ 650/1251 ( 52%)]  Loss: 6.722 (6.81)  Time: 0.811s, 1263.35/s  (0.813s, 1258.81/s)  LR: 2.008e-04  Data: 0.013 (0.015)
Train: 1 [ 700/1251 ( 56%)]  Loss: 6.665 (6.80)  Time: 0.832s, 1230.91/s  (0.814s, 1258.45/s)  LR: 2.008e-04  Data: 0.013 (0.014)
Train: 1 [ 750/1251 ( 60%)]  Loss: 6.654 (6.79)  Time: 0.819s, 1250.39/s  (0.813s, 1259.79/s)  LR: 2.008e-04  Data: 0.015 (0.014)
Train: 1 [ 800/1251 ( 64%)]  Loss: 6.663 (6.78)  Time: 0.813s, 1259.76/s  (0.812s, 1260.60/s)  LR: 2.008e-04  Data: 0.012 (0.014)
Train: 1 [ 850/1251 ( 68%)]  Loss: 6.600 (6.77)  Time: 0.803s, 1275.59/s  (0.812s, 1260.87/s)  LR: 2.008e-04  Data: 0.013 (0.014)
Train: 1 [ 900/1251 ( 72%)]  Loss: 6.563 (6.76)  Time: 0.797s, 1284.58/s  (0.812s, 1261.56/s)  LR: 2.008e-04  Data: 0.013 (0.014)
Train: 1 [ 950/1251 ( 76%)]  Loss: 6.626 (6.75)  Time: 0.786s, 1303.40/s  (0.811s, 1261.93/s)  LR: 2.008e-04  Data: 0.016 (0.014)
Train: 1 [1000/1251 ( 80%)]  Loss: 6.573 (6.74)  Time: 0.787s, 1301.04/s  (0.811s, 1262.64/s)  LR: 2.008e-04  Data: 0.013 (0.014)
Train: 1 [1050/1251 ( 84%)]  Loss: 6.641 (6.74)  Time: 0.861s, 1189.77/s  (0.811s, 1263.16/s)  LR: 2.008e-04  Data: 0.012 (0.013)
Train: 1 [1100/1251 ( 88%)]  Loss: 6.458 (6.73)  Time: 0.803s, 1274.43/s  (0.811s, 1263.38/s)  LR: 2.008e-04  Data: 0.011 (0.013)
Train: 1 [1150/1251 ( 92%)]  Loss: 6.576 (6.72)  Time: 0.872s, 1174.02/s  (0.811s, 1263.37/s)  LR: 2.008e-04  Data: 0.011 (0.013)
Train: 1 [1200/1251 ( 96%)]  Loss: 6.549 (6.71)  Time: 0.783s, 1308.62/s  (0.811s, 1263.40/s)  LR: 2.008e-04  Data: 0.014 (0.013)
Train: 1 [1250/1251 (100%)]  Loss: 6.469 (6.71)  Time: 0.779s, 1315.18/s  (0.810s, 1263.49/s)  LR: 2.008e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.687 (1.687)  Loss:  5.6172 (5.6172)  Acc@1:  0.8789 ( 0.8789)  Acc@5: 12.5977 (12.5977)
Test: [  48/48]  Time: 0.194 (0.609)  Loss:  5.2109 (5.8557)  Acc@1: 14.6226 ( 2.9400)  Acc@5: 26.6509 ( 9.8980)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-1.pth.tar', 2.940000000915527)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-0.pth.tar', 0.17399999872207642)

Train: 2 [   0/1251 (  0%)]  Loss: 6.479 (6.48)  Time: 2.356s,  434.73/s  (2.356s,  434.73/s)  LR: 4.006e-04  Data: 1.627 (1.627)
Train: 2 [  50/1251 (  4%)]  Loss: 6.645 (6.56)  Time: 0.803s, 1275.19/s  (0.844s, 1213.69/s)  LR: 4.006e-04  Data: 0.010 (0.055)
Train: 2 [ 100/1251 (  8%)]  Loss: 6.403 (6.51)  Time: 0.779s, 1314.28/s  (0.830s, 1233.23/s)  LR: 4.006e-04  Data: 0.010 (0.034)
Train: 2 [ 150/1251 ( 12%)]  Loss: 6.491 (6.50)  Time: 0.826s, 1239.04/s  (0.824s, 1242.55/s)  LR: 4.006e-04  Data: 0.010 (0.026)
Train: 2 [ 200/1251 ( 16%)]  Loss: 6.495 (6.50)  Time: 0.775s, 1321.06/s  (0.820s, 1249.41/s)  LR: 4.006e-04  Data: 0.009 (0.023)
Train: 2 [ 250/1251 ( 20%)]  Loss: 6.516 (6.50)  Time: 0.784s, 1305.49/s  (0.817s, 1253.27/s)  LR: 4.006e-04  Data: 0.011 (0.021)
Train: 2 [ 300/1251 ( 24%)]  Loss: 6.430 (6.49)  Time: 0.829s, 1235.00/s  (0.815s, 1256.54/s)  LR: 4.006e-04  Data: 0.009 (0.019)
Train: 2 [ 350/1251 ( 28%)]  Loss: 6.396 (6.48)  Time: 0.814s, 1257.92/s  (0.814s, 1257.82/s)  LR: 4.006e-04  Data: 0.009 (0.018)
Train: 2 [ 400/1251 ( 32%)]  Loss: 6.468 (6.48)  Time: 0.776s, 1318.98/s  (0.812s, 1260.61/s)  LR: 4.006e-04  Data: 0.010 (0.017)
Train: 2 [ 450/1251 ( 36%)]  Loss: 6.363 (6.47)  Time: 0.810s, 1263.43/s  (0.811s, 1261.90/s)  LR: 4.006e-04  Data: 0.010 (0.017)
Train: 2 [ 500/1251 ( 40%)]  Loss: 6.463 (6.47)  Time: 0.819s, 1250.66/s  (0.811s, 1262.20/s)  LR: 4.006e-04  Data: 0.009 (0.016)
Train: 2 [ 550/1251 ( 44%)]  Loss: 6.298 (6.45)  Time: 0.819s, 1250.91/s  (0.811s, 1262.64/s)  LR: 4.006e-04  Data: 0.009 (0.016)
Train: 2 [ 600/1251 ( 48%)]  Loss: 6.264 (6.44)  Time: 0.820s, 1248.39/s  (0.811s, 1263.11/s)  LR: 4.006e-04  Data: 0.011 (0.015)
Train: 2 [ 650/1251 ( 52%)]  Loss: 6.306 (6.43)  Time: 0.835s, 1225.87/s  (0.810s, 1263.55/s)  LR: 4.006e-04  Data: 0.009 (0.015)
Train: 2 [ 700/1251 ( 56%)]  Loss: 6.369 (6.43)  Time: 0.811s, 1262.86/s  (0.810s, 1263.65/s)  LR: 4.006e-04  Data: 0.011 (0.015)
Train: 2 [ 750/1251 ( 60%)]  Loss: 6.323 (6.42)  Time: 0.791s, 1294.97/s  (0.810s, 1264.06/s)  LR: 4.006e-04  Data: 0.014 (0.015)
Train: 2 [ 800/1251 ( 64%)]  Loss: 6.251 (6.41)  Time: 0.796s, 1285.90/s  (0.810s, 1264.00/s)  LR: 4.006e-04  Data: 0.010 (0.015)
Train: 2 [ 850/1251 ( 68%)]  Loss: 6.129 (6.39)  Time: 0.823s, 1244.96/s  (0.810s, 1263.77/s)  LR: 4.006e-04  Data: 0.010 (0.014)
Train: 2 [ 900/1251 ( 72%)]  Loss: 6.266 (6.39)  Time: 0.837s, 1223.43/s  (0.810s, 1264.23/s)  LR: 4.006e-04  Data: 0.010 (0.014)
Train: 2 [ 950/1251 ( 76%)]  Loss: 6.258 (6.38)  Time: 0.776s, 1319.03/s  (0.809s, 1265.33/s)  LR: 4.006e-04  Data: 0.010 (0.014)
Train: 2 [1000/1251 ( 80%)]  Loss: 6.108 (6.37)  Time: 0.778s, 1316.21/s  (0.809s, 1265.76/s)  LR: 4.006e-04  Data: 0.009 (0.014)
Train: 2 [1050/1251 ( 84%)]  Loss: 6.286 (6.36)  Time: 0.773s, 1325.37/s  (0.808s, 1266.64/s)  LR: 4.006e-04  Data: 0.010 (0.014)
Train: 2 [1100/1251 ( 88%)]  Loss: 6.241 (6.36)  Time: 0.777s, 1317.24/s  (0.808s, 1266.93/s)  LR: 4.006e-04  Data: 0.010 (0.014)
Train: 2 [1150/1251 ( 92%)]  Loss: 6.170 (6.35)  Time: 0.845s, 1211.59/s  (0.808s, 1267.39/s)  LR: 4.006e-04  Data: 0.010 (0.014)
Train: 2 [1200/1251 ( 96%)]  Loss: 6.073 (6.34)  Time: 0.802s, 1276.90/s  (0.808s, 1267.75/s)  LR: 4.006e-04  Data: 0.009 (0.014)
Train: 2 [1250/1251 (100%)]  Loss: 6.128 (6.33)  Time: 0.769s, 1331.64/s  (0.808s, 1267.97/s)  LR: 4.006e-04  Data: 0.000 (0.014)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.661 (1.661)  Loss:  5.3633 (5.3633)  Acc@1:  5.8594 ( 5.8594)  Acc@5: 17.7734 (17.7734)
Test: [  48/48]  Time: 0.194 (0.609)  Loss:  5.0352 (5.4792)  Acc@1: 13.3255 ( 5.8060)  Acc@5: 24.1745 (16.9420)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-2.pth.tar', 5.806000003051758)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-1.pth.tar', 2.940000000915527)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-0.pth.tar', 0.17399999872207642)

Train: 3 [   0/1251 (  0%)]  Loss: 6.206 (6.21)  Time: 2.566s,  399.01/s  (2.566s,  399.01/s)  LR: 6.004e-04  Data: 1.796 (1.796)
Train: 3 [  50/1251 (  4%)]  Loss: 6.204 (6.21)  Time: 0.778s, 1316.23/s  (0.844s, 1212.56/s)  LR: 6.004e-04  Data: 0.012 (0.052)
Train: 3 [ 100/1251 (  8%)]  Loss: 6.185 (6.20)  Time: 0.773s, 1325.49/s  (0.826s, 1239.26/s)  LR: 6.004e-04  Data: 0.011 (0.032)
Train: 3 [ 150/1251 ( 12%)]  Loss: 6.128 (6.18)  Time: 0.788s, 1299.03/s  (0.819s, 1249.82/s)  LR: 6.004e-04  Data: 0.014 (0.025)
Train: 3 [ 200/1251 ( 16%)]  Loss: 6.286 (6.20)  Time: 0.834s, 1228.35/s  (0.813s, 1259.43/s)  LR: 6.004e-04  Data: 0.013 (0.022)
Train: 3 [ 250/1251 ( 20%)]  Loss: 6.025 (6.17)  Time: 0.816s, 1254.50/s  (0.817s, 1253.49/s)  LR: 6.004e-04  Data: 0.017 (0.020)
Train: 3 [ 300/1251 ( 24%)]  Loss: 6.034 (6.15)  Time: 0.816s, 1254.17/s  (0.817s, 1253.17/s)  LR: 6.004e-04  Data: 0.011 (0.019)
Train: 3 [ 350/1251 ( 28%)]  Loss: 6.188 (6.16)  Time: 0.778s, 1316.26/s  (0.814s, 1257.83/s)  LR: 6.004e-04  Data: 0.012 (0.018)
Train: 3 [ 400/1251 ( 32%)]  Loss: 6.218 (6.16)  Time: 0.782s, 1310.25/s  (0.813s, 1259.75/s)  LR: 6.004e-04  Data: 0.011 (0.017)
Train: 3 [ 450/1251 ( 36%)]  Loss: 5.867 (6.13)  Time: 0.885s, 1157.39/s  (0.812s, 1260.44/s)  LR: 6.004e-04  Data: 0.009 (0.017)
Train: 3 [ 500/1251 ( 40%)]  Loss: 6.056 (6.13)  Time: 0.801s, 1278.09/s  (0.811s, 1262.29/s)  LR: 6.004e-04  Data: 0.012 (0.016)
Train: 3 [ 550/1251 ( 44%)]  Loss: 6.058 (6.12)  Time: 0.824s, 1242.39/s  (0.811s, 1263.33/s)  LR: 6.004e-04  Data: 0.012 (0.016)
Train: 3 [ 600/1251 ( 48%)]  Loss: 5.889 (6.10)  Time: 0.787s, 1300.66/s  (0.810s, 1263.76/s)  LR: 6.004e-04  Data: 0.017 (0.015)
Train: 3 [ 650/1251 ( 52%)]  Loss: 6.167 (6.11)  Time: 0.776s, 1319.11/s  (0.810s, 1264.54/s)  LR: 6.004e-04  Data: 0.012 (0.015)
Train: 3 [ 700/1251 ( 56%)]  Loss: 5.868 (6.09)  Time: 0.790s, 1295.65/s  (0.810s, 1264.97/s)  LR: 6.004e-04  Data: 0.012 (0.015)
Train: 3 [ 750/1251 ( 60%)]  Loss: 6.023 (6.09)  Time: 0.831s, 1232.35/s  (0.809s, 1265.16/s)  LR: 6.004e-04  Data: 0.015 (0.015)
Train: 3 [ 800/1251 ( 64%)]  Loss: 5.925 (6.08)  Time: 0.827s, 1237.53/s  (0.809s, 1265.77/s)  LR: 6.004e-04  Data: 0.025 (0.015)
Train: 3 [ 850/1251 ( 68%)]  Loss: 5.850 (6.07)  Time: 0.815s, 1255.75/s  (0.809s, 1265.86/s)  LR: 6.004e-04  Data: 0.010 (0.014)
Train: 3 [ 900/1251 ( 72%)]  Loss: 5.776 (6.05)  Time: 0.838s, 1222.01/s  (0.809s, 1266.15/s)  LR: 6.004e-04  Data: 0.013 (0.014)
Train: 3 [ 950/1251 ( 76%)]  Loss: 5.839 (6.04)  Time: 0.780s, 1312.66/s  (0.809s, 1265.79/s)  LR: 6.004e-04  Data: 0.012 (0.014)
Train: 3 [1000/1251 ( 80%)]  Loss: 6.015 (6.04)  Time: 0.792s, 1292.21/s  (0.809s, 1266.27/s)  LR: 6.004e-04  Data: 0.015 (0.014)
Train: 3 [1050/1251 ( 84%)]  Loss: 6.130 (6.04)  Time: 0.803s, 1274.52/s  (0.809s, 1266.48/s)  LR: 6.004e-04  Data: 0.015 (0.014)
Train: 3 [1100/1251 ( 88%)]  Loss: 5.873 (6.04)  Time: 0.813s, 1260.11/s  (0.808s, 1266.61/s)  LR: 6.004e-04  Data: 0.015 (0.014)
Train: 3 [1150/1251 ( 92%)]  Loss: 5.644 (6.02)  Time: 0.782s, 1308.73/s  (0.808s, 1267.13/s)  LR: 6.004e-04  Data: 0.015 (0.014)
Train: 3 [1200/1251 ( 96%)]  Loss: 5.865 (6.01)  Time: 0.777s, 1317.15/s  (0.808s, 1267.57/s)  LR: 6.004e-04  Data: 0.012 (0.014)
Train: 3 [1250/1251 (100%)]  Loss: 5.799 (6.00)  Time: 0.765s, 1338.63/s  (0.808s, 1268.00/s)  LR: 6.004e-04  Data: 0.000 (0.014)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.680 (1.680)  Loss:  4.6641 (4.6641)  Acc@1: 13.7695 (13.7695)  Acc@5: 32.2266 (32.2266)
Test: [  48/48]  Time: 0.194 (0.610)  Loss:  3.7891 (4.5997)  Acc@1: 35.0236 (14.5120)  Acc@5: 54.9528 (34.0560)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-3.pth.tar', 14.512000023193359)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-2.pth.tar', 5.806000003051758)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-1.pth.tar', 2.940000000915527)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-0.pth.tar', 0.17399999872207642)

Train: 4 [   0/1251 (  0%)]  Loss: 6.035 (6.04)  Time: 2.344s,  436.86/s  (2.344s,  436.86/s)  LR: 8.002e-04  Data: 1.611 (1.611)
Train: 4 [  50/1251 (  4%)]  Loss: 5.738 (5.89)  Time: 0.831s, 1232.56/s  (0.838s, 1221.64/s)  LR: 8.002e-04  Data: 0.010 (0.050)
Train: 4 [ 100/1251 (  8%)]  Loss: 5.664 (5.81)  Time: 0.814s, 1257.89/s  (0.822s, 1245.29/s)  LR: 8.002e-04  Data: 0.010 (0.031)
Train: 4 [ 150/1251 ( 12%)]  Loss: 5.803 (5.81)  Time: 0.785s, 1304.45/s  (0.818s, 1251.42/s)  LR: 8.002e-04  Data: 0.010 (0.025)
Train: 4 [ 200/1251 ( 16%)]  Loss: 5.616 (5.77)  Time: 0.782s, 1309.93/s  (0.816s, 1254.36/s)  LR: 8.002e-04  Data: 0.010 (0.022)
Train: 4 [ 250/1251 ( 20%)]  Loss: 5.481 (5.72)  Time: 0.814s, 1257.70/s  (0.814s, 1258.00/s)  LR: 8.002e-04  Data: 0.009 (0.020)
Train: 4 [ 300/1251 ( 24%)]  Loss: 5.960 (5.76)  Time: 0.786s, 1303.12/s  (0.813s, 1258.90/s)  LR: 8.002e-04  Data: 0.013 (0.019)
Train: 4 [ 350/1251 ( 28%)]  Loss: 5.731 (5.75)  Time: 0.815s, 1256.55/s  (0.813s, 1259.86/s)  LR: 8.002e-04  Data: 0.010 (0.018)
Train: 4 [ 400/1251 ( 32%)]  Loss: 5.804 (5.76)  Time: 0.826s, 1239.90/s  (0.813s, 1260.30/s)  LR: 8.002e-04  Data: 0.014 (0.017)
Train: 4 [ 450/1251 ( 36%)]  Loss: 5.779 (5.76)  Time: 0.844s, 1213.21/s  (0.812s, 1261.37/s)  LR: 8.002e-04  Data: 0.011 (0.016)
Train: 4 [ 500/1251 ( 40%)]  Loss: 5.857 (5.77)  Time: 0.804s, 1273.84/s  (0.812s, 1261.65/s)  LR: 8.002e-04  Data: 0.014 (0.016)
Train: 4 [ 550/1251 ( 44%)]  Loss: 5.550 (5.75)  Time: 0.804s, 1274.39/s  (0.811s, 1262.82/s)  LR: 8.002e-04  Data: 0.009 (0.016)
Train: 4 [ 600/1251 ( 48%)]  Loss: 5.771 (5.75)  Time: 0.838s, 1222.53/s  (0.810s, 1263.45/s)  LR: 8.002e-04  Data: 0.012 (0.015)
Train: 4 [ 650/1251 ( 52%)]  Loss: 6.009 (5.77)  Time: 0.832s, 1231.36/s  (0.810s, 1264.16/s)  LR: 8.002e-04  Data: 0.010 (0.015)
Train: 4 [ 700/1251 ( 56%)]  Loss: 5.395 (5.75)  Time: 0.800s, 1279.89/s  (0.810s, 1264.83/s)  LR: 8.002e-04  Data: 0.010 (0.015)
Train: 4 [ 750/1251 ( 60%)]  Loss: 5.744 (5.75)  Time: 0.810s, 1263.70/s  (0.809s, 1265.42/s)  LR: 8.002e-04  Data: 0.010 (0.015)
Train: 4 [ 800/1251 ( 64%)]  Loss: 5.641 (5.74)  Time: 0.807s, 1269.55/s  (0.809s, 1266.01/s)  LR: 8.002e-04  Data: 0.012 (0.014)
Train: 4 [ 850/1251 ( 68%)]  Loss: 5.816 (5.74)  Time: 0.804s, 1273.79/s  (0.809s, 1265.75/s)  LR: 8.002e-04  Data: 0.010 (0.014)
Train: 4 [ 900/1251 ( 72%)]  Loss: 5.775 (5.75)  Time: 0.807s, 1268.76/s  (0.809s, 1265.69/s)  LR: 8.002e-04  Data: 0.009 (0.014)
Train: 4 [ 950/1251 ( 76%)]  Loss: 5.703 (5.74)  Time: 0.813s, 1260.03/s  (0.808s, 1266.67/s)  LR: 8.002e-04  Data: 0.010 (0.014)
Train: 4 [1000/1251 ( 80%)]  Loss: 5.185 (5.72)  Time: 0.797s, 1284.60/s  (0.808s, 1266.78/s)  LR: 8.002e-04  Data: 0.010 (0.014)
Train: 4 [1050/1251 ( 84%)]  Loss: 5.795 (5.72)  Time: 0.807s, 1269.12/s  (0.808s, 1266.80/s)  LR: 8.002e-04  Data: 0.011 (0.014)
Train: 4 [1100/1251 ( 88%)]  Loss: 5.654 (5.72)  Time: 0.863s, 1186.55/s  (0.808s, 1266.78/s)  LR: 8.002e-04  Data: 0.009 (0.014)
Train: 4 [1150/1251 ( 92%)]  Loss: 5.408 (5.70)  Time: 0.803s, 1275.81/s  (0.808s, 1266.96/s)  LR: 8.002e-04  Data: 0.012 (0.014)
Train: 4 [1200/1251 ( 96%)]  Loss: 5.645 (5.70)  Time: 0.820s, 1248.91/s  (0.808s, 1266.91/s)  LR: 8.002e-04  Data: 0.010 (0.014)
Train: 4 [1250/1251 (100%)]  Loss: 5.616 (5.70)  Time: 0.830s, 1233.55/s  (0.808s, 1267.15/s)  LR: 8.002e-04  Data: 0.000 (0.014)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.745 (1.745)  Loss:  3.3867 (3.3867)  Acc@1: 31.7383 (31.7383)  Acc@5: 59.2773 (59.2773)
Test: [  48/48]  Time: 0.194 (0.618)  Loss:  2.4727 (3.8110)  Acc@1: 54.9528 (25.3780)  Acc@5: 75.5896 (51.0160)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-4.pth.tar', 25.378000018310548)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-3.pth.tar', 14.512000023193359)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-2.pth.tar', 5.806000003051758)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-1.pth.tar', 2.940000000915527)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-0.pth.tar', 0.17399999872207642)

Train: 5 [   0/1251 (  0%)]  Loss: 5.489 (5.49)  Time: 2.431s,  421.25/s  (2.431s,  421.25/s)  LR: 9.998e-04  Data: 1.701 (1.701)
Train: 5 [  50/1251 (  4%)]  Loss: 5.573 (5.53)  Time: 0.822s, 1245.04/s  (0.845s, 1211.40/s)  LR: 9.998e-04  Data: 0.017 (0.049)
Train: 5 [ 100/1251 (  8%)]  Loss: 5.497 (5.52)  Time: 0.827s, 1237.89/s  (0.828s, 1237.04/s)  LR: 9.998e-04  Data: 0.017 (0.031)
Train: 5 [ 150/1251 ( 12%)]  Loss: 5.106 (5.42)  Time: 0.776s, 1318.76/s  (0.820s, 1249.24/s)  LR: 9.998e-04  Data: 0.012 (0.025)
Train: 5 [ 200/1251 ( 16%)]  Loss: 5.645 (5.46)  Time: 0.796s, 1286.48/s  (0.816s, 1255.51/s)  LR: 9.998e-04  Data: 0.024 (0.022)
Train: 5 [ 250/1251 ( 20%)]  Loss: 5.654 (5.49)  Time: 0.796s, 1287.11/s  (0.813s, 1258.94/s)  LR: 9.998e-04  Data: 0.017 (0.020)
Train: 5 [ 300/1251 ( 24%)]  Loss: 5.603 (5.51)  Time: 0.779s, 1315.21/s  (0.813s, 1259.86/s)  LR: 9.998e-04  Data: 0.011 (0.018)
Train: 5 [ 350/1251 ( 28%)]  Loss: 5.488 (5.51)  Time: 0.816s, 1254.70/s  (0.811s, 1262.19/s)  LR: 9.998e-04  Data: 0.012 (0.017)
Train: 5 [ 400/1251 ( 32%)]  Loss: 5.472 (5.50)  Time: 0.840s, 1219.67/s  (0.811s, 1262.08/s)  LR: 9.998e-04  Data: 0.021 (0.017)
Train: 5 [ 450/1251 ( 36%)]  Loss: 5.700 (5.52)  Time: 0.825s, 1241.43/s  (0.811s, 1262.85/s)  LR: 9.998e-04  Data: 0.011 (0.016)
Train: 5 [ 500/1251 ( 40%)]  Loss: 5.653 (5.53)  Time: 0.854s, 1199.20/s  (0.810s, 1263.68/s)  LR: 9.998e-04  Data: 0.011 (0.016)
Train: 5 [ 550/1251 ( 44%)]  Loss: 5.522 (5.53)  Time: 0.809s, 1266.37/s  (0.810s, 1264.01/s)  LR: 9.998e-04  Data: 0.012 (0.015)
Train: 5 [ 600/1251 ( 48%)]  Loss: 5.424 (5.53)  Time: 0.788s, 1299.99/s  (0.810s, 1264.61/s)  LR: 9.998e-04  Data: 0.012 (0.015)
Train: 5 [ 650/1251 ( 52%)]  Loss: 5.675 (5.54)  Time: 0.781s, 1311.08/s  (0.810s, 1263.98/s)  LR: 9.998e-04  Data: 0.012 (0.015)
Train: 5 [ 700/1251 ( 56%)]  Loss: 5.402 (5.53)  Time: 0.781s, 1310.93/s  (0.810s, 1264.57/s)  LR: 9.998e-04  Data: 0.014 (0.015)
Train: 5 [ 750/1251 ( 60%)]  Loss: 5.543 (5.53)  Time: 0.819s, 1250.45/s  (0.809s, 1265.14/s)  LR: 9.998e-04  Data: 0.016 (0.014)
Train: 5 [ 800/1251 ( 64%)]  Loss: 5.277 (5.51)  Time: 0.800s, 1280.50/s  (0.809s, 1265.26/s)  LR: 9.998e-04  Data: 0.013 (0.014)
Train: 5 [ 850/1251 ( 68%)]  Loss: 5.534 (5.51)  Time: 0.866s, 1182.69/s  (0.810s, 1264.82/s)  LR: 9.998e-04  Data: 0.011 (0.014)
Train: 5 [ 900/1251 ( 72%)]  Loss: 5.605 (5.52)  Time: 0.774s, 1323.79/s  (0.810s, 1264.94/s)  LR: 9.998e-04  Data: 0.011 (0.014)
Train: 5 [ 950/1251 ( 76%)]  Loss: 5.399 (5.51)  Time: 0.809s, 1265.14/s  (0.809s, 1265.08/s)  LR: 9.998e-04  Data: 0.012 (0.014)
Train: 5 [1000/1251 ( 80%)]  Loss: 5.474 (5.51)  Time: 0.875s, 1170.78/s  (0.809s, 1265.09/s)  LR: 9.998e-04  Data: 0.011 (0.014)
Train: 5 [1050/1251 ( 84%)]  Loss: 5.570 (5.51)  Time: 0.777s, 1317.39/s  (0.809s, 1265.64/s)  LR: 9.998e-04  Data: 0.012 (0.014)
Train: 5 [1100/1251 ( 88%)]  Loss: 5.232 (5.50)  Time: 0.792s, 1293.68/s  (0.809s, 1265.84/s)  LR: 9.998e-04  Data: 0.018 (0.014)
Train: 5 [1150/1251 ( 92%)]  Loss: 5.144 (5.49)  Time: 0.796s, 1286.49/s  (0.809s, 1265.77/s)  LR: 9.998e-04  Data: 0.019 (0.014)
Train: 5 [1200/1251 ( 96%)]  Loss: 5.314 (5.48)  Time: 0.772s, 1325.96/s  (0.809s, 1265.96/s)  LR: 9.998e-04  Data: 0.010 (0.014)
Train: 5 [1250/1251 (100%)]  Loss: 5.322 (5.47)  Time: 0.764s, 1340.96/s  (0.809s, 1266.47/s)  LR: 9.998e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.751 (1.751)  Loss:  2.7285 (2.7285)  Acc@1: 45.8984 (45.8984)  Acc@5: 72.7539 (72.7539)
Test: [  48/48]  Time: 0.195 (0.604)  Loss:  2.4297 (3.3897)  Acc@1: 51.7689 (30.5600)  Acc@5: 73.3491 (57.3940)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-5.pth.tar', 30.560000057373045)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-4.pth.tar', 25.378000018310548)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-3.pth.tar', 14.512000023193359)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-2.pth.tar', 5.806000003051758)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-1.pth.tar', 2.940000000915527)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-0.pth.tar', 0.17399999872207642)

Train: 6 [   0/1251 (  0%)]  Loss: 5.504 (5.50)  Time: 2.451s,  417.83/s  (2.451s,  417.83/s)  LR: 9.998e-04  Data: 1.683 (1.683)
Train: 6 [  50/1251 (  4%)]  Loss: 5.271 (5.39)  Time: 0.789s, 1298.39/s  (0.846s, 1210.27/s)  LR: 9.998e-04  Data: 0.011 (0.051)
Train: 6 [ 100/1251 (  8%)]  Loss: 5.088 (5.29)  Time: 0.802s, 1277.35/s  (0.830s, 1233.98/s)  LR: 9.998e-04  Data: 0.010 (0.032)
Train: 6 [ 150/1251 ( 12%)]  Loss: 5.619 (5.37)  Time: 0.801s, 1278.07/s  (0.822s, 1245.17/s)  LR: 9.998e-04  Data: 0.009 (0.025)
Train: 6 [ 200/1251 ( 16%)]  Loss: 5.220 (5.34)  Time: 0.781s, 1310.69/s  (0.819s, 1249.83/s)  LR: 9.998e-04  Data: 0.010 (0.022)
Train: 6 [ 250/1251 ( 20%)]  Loss: 5.312 (5.34)  Time: 0.812s, 1260.36/s  (0.818s, 1251.89/s)  LR: 9.998e-04  Data: 0.010 (0.020)
Train: 6 [ 300/1251 ( 24%)]  Loss: 5.390 (5.34)  Time: 0.801s, 1278.12/s  (0.815s, 1255.72/s)  LR: 9.998e-04  Data: 0.017 (0.019)
Train: 6 [ 350/1251 ( 28%)]  Loss: 4.961 (5.30)  Time: 0.843s, 1214.47/s  (0.815s, 1256.68/s)  LR: 9.998e-04  Data: 0.009 (0.018)
Train: 6 [ 400/1251 ( 32%)]  Loss: 5.097 (5.27)  Time: 0.811s, 1262.12/s  (0.814s, 1257.48/s)  LR: 9.998e-04  Data: 0.010 (0.017)
Train: 6 [ 450/1251 ( 36%)]  Loss: 5.358 (5.28)  Time: 0.779s, 1314.95/s  (0.814s, 1258.13/s)  LR: 9.998e-04  Data: 0.011 (0.017)
Train: 6 [ 500/1251 ( 40%)]  Loss: 4.775 (5.24)  Time: 0.808s, 1267.56/s  (0.813s, 1259.86/s)  LR: 9.998e-04  Data: 0.010 (0.016)
Train: 6 [ 550/1251 ( 44%)]  Loss: 5.416 (5.25)  Time: 0.866s, 1182.16/s  (0.812s, 1261.02/s)  LR: 9.998e-04  Data: 0.010 (0.016)
Train: 6 [ 600/1251 ( 48%)]  Loss: 5.095 (5.24)  Time: 0.804s, 1273.35/s  (0.811s, 1262.14/s)  LR: 9.998e-04  Data: 0.010 (0.015)
Train: 6 [ 650/1251 ( 52%)]  Loss: 5.310 (5.24)  Time: 0.816s, 1254.73/s  (0.811s, 1262.39/s)  LR: 9.998e-04  Data: 0.011 (0.015)
Train: 6 [ 700/1251 ( 56%)]  Loss: 5.379 (5.25)  Time: 0.786s, 1302.11/s  (0.811s, 1263.01/s)  LR: 9.998e-04  Data: 0.011 (0.015)
Train: 6 [ 750/1251 ( 60%)]  Loss: 5.161 (5.25)  Time: 0.873s, 1172.80/s  (0.811s, 1262.67/s)  LR: 9.998e-04  Data: 0.010 (0.015)
Train: 6 [ 800/1251 ( 64%)]  Loss: 5.144 (5.24)  Time: 0.780s, 1312.81/s  (0.811s, 1263.22/s)  LR: 9.998e-04  Data: 0.009 (0.015)
Train: 6 [ 850/1251 ( 68%)]  Loss: 5.451 (5.25)  Time: 0.789s, 1297.35/s  (0.811s, 1263.15/s)  LR: 9.998e-04  Data: 0.016 (0.014)
Train: 6 [ 900/1251 ( 72%)]  Loss: 4.884 (5.23)  Time: 0.783s, 1307.65/s  (0.810s, 1264.22/s)  LR: 9.998e-04  Data: 0.010 (0.014)
Train: 6 [ 950/1251 ( 76%)]  Loss: 5.111 (5.23)  Time: 0.854s, 1198.99/s  (0.810s, 1264.67/s)  LR: 9.998e-04  Data: 0.012 (0.014)
Train: 6 [1000/1251 ( 80%)]  Loss: 5.182 (5.23)  Time: 0.777s, 1318.34/s  (0.810s, 1264.90/s)  LR: 9.998e-04  Data: 0.011 (0.014)
Train: 6 [1050/1251 ( 84%)]  Loss: 5.328 (5.23)  Time: 0.807s, 1268.92/s  (0.809s, 1265.44/s)  LR: 9.998e-04  Data: 0.016 (0.014)
Train: 6 [1100/1251 ( 88%)]  Loss: 4.990 (5.22)  Time: 0.775s, 1320.70/s  (0.809s, 1265.87/s)  LR: 9.998e-04  Data: 0.009 (0.014)
Train: 6 [1150/1251 ( 92%)]  Loss: 5.253 (5.22)  Time: 0.797s, 1284.90/s  (0.809s, 1266.22/s)  LR: 9.998e-04  Data: 0.010 (0.014)
Train: 6 [1200/1251 ( 96%)]  Loss: 5.034 (5.21)  Time: 0.791s, 1294.80/s  (0.809s, 1265.77/s)  LR: 9.998e-04  Data: 0.011 (0.014)
Train: 6 [1250/1251 (100%)]  Loss: 5.117 (5.21)  Time: 0.768s, 1334.05/s  (0.809s, 1266.23/s)  LR: 9.998e-04  Data: 0.000 (0.014)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.667 (1.667)  Loss:  2.3613 (2.3613)  Acc@1: 56.2500 (56.2500)  Acc@5: 81.9336 (81.9336)
Test: [  48/48]  Time: 0.194 (0.605)  Loss:  2.8105 (3.1504)  Acc@1: 48.7028 (36.4440)  Acc@5: 69.3396 (64.0920)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-6.pth.tar', 36.44400001831055)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-5.pth.tar', 30.560000057373045)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-4.pth.tar', 25.378000018310548)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-3.pth.tar', 14.512000023193359)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-2.pth.tar', 5.806000003051758)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-1.pth.tar', 2.940000000915527)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-0.pth.tar', 0.17399999872207642)

Train: 7 [   0/1251 (  0%)]  Loss: 5.287 (5.29)  Time: 2.408s,  425.22/s  (2.408s,  425.22/s)  LR: 9.997e-04  Data: 1.674 (1.674)
Train: 7 [  50/1251 (  4%)]  Loss: 5.062 (5.17)  Time: 0.817s, 1253.20/s  (0.845s, 1212.19/s)  LR: 9.997e-04  Data: 0.013 (0.049)
Train: 7 [ 100/1251 (  8%)]  Loss: 5.046 (5.13)  Time: 0.849s, 1205.64/s  (0.824s, 1241.98/s)  LR: 9.997e-04  Data: 0.014 (0.031)
Train: 7 [ 150/1251 ( 12%)]  Loss: 5.352 (5.19)  Time: 0.842s, 1215.62/s  (0.821s, 1247.82/s)  LR: 9.997e-04  Data: 0.012 (0.024)
Train: 7 [ 200/1251 ( 16%)]  Loss: 5.434 (5.24)  Time: 0.780s, 1312.15/s  (0.816s, 1255.34/s)  LR: 9.997e-04  Data: 0.012 (0.021)
Train: 7 [ 250/1251 ( 20%)]  Loss: 5.367 (5.26)  Time: 0.777s, 1317.54/s  (0.815s, 1257.15/s)  LR: 9.997e-04  Data: 0.013 (0.019)
Train: 7 [ 300/1251 ( 24%)]  Loss: 5.027 (5.23)  Time: 0.795s, 1288.78/s  (0.814s, 1257.28/s)  LR: 9.997e-04  Data: 0.011 (0.018)
Train: 7 [ 350/1251 ( 28%)]  Loss: 5.169 (5.22)  Time: 0.780s, 1313.32/s  (0.814s, 1258.62/s)  LR: 9.997e-04  Data: 0.015 (0.017)
Train: 7 [ 400/1251 ( 32%)]  Loss: 5.218 (5.22)  Time: 0.796s, 1285.87/s  (0.813s, 1260.09/s)  LR: 9.997e-04  Data: 0.013 (0.017)
Train: 7 [ 450/1251 ( 36%)]  Loss: 4.831 (5.18)  Time: 0.806s, 1269.74/s  (0.812s, 1261.62/s)  LR: 9.997e-04  Data: 0.014 (0.016)
Train: 7 [ 500/1251 ( 40%)]  Loss: 5.037 (5.17)  Time: 0.808s, 1267.17/s  (0.810s, 1263.82/s)  LR: 9.997e-04  Data: 0.013 (0.016)
Train: 7 [ 550/1251 ( 44%)]  Loss: 4.735 (5.13)  Time: 0.823s, 1244.19/s  (0.810s, 1264.41/s)  LR: 9.997e-04  Data: 0.010 (0.015)
Train: 7 [ 600/1251 ( 48%)]  Loss: 5.120 (5.13)  Time: 0.813s, 1259.70/s  (0.810s, 1264.46/s)  LR: 9.997e-04  Data: 0.012 (0.015)
Train: 7 [ 650/1251 ( 52%)]  Loss: 4.834 (5.11)  Time: 0.831s, 1231.56/s  (0.810s, 1264.60/s)  LR: 9.997e-04  Data: 0.011 (0.015)
Train: 7 [ 700/1251 ( 56%)]  Loss: 4.967 (5.10)  Time: 0.817s, 1254.12/s  (0.810s, 1264.42/s)  LR: 9.997e-04  Data: 0.015 (0.015)
Train: 7 [ 750/1251 ( 60%)]  Loss: 4.824 (5.08)  Time: 0.797s, 1284.38/s  (0.809s, 1265.13/s)  LR: 9.997e-04  Data: 0.020 (0.015)
Train: 7 [ 800/1251 ( 64%)]  Loss: 5.087 (5.08)  Time: 0.778s, 1316.84/s  (0.809s, 1265.66/s)  LR: 9.997e-04  Data: 0.012 (0.014)
Train: 7 [ 850/1251 ( 68%)]  Loss: 5.007 (5.08)  Time: 0.789s, 1298.11/s  (0.809s, 1266.37/s)  LR: 9.997e-04  Data: 0.012 (0.014)
Train: 7 [ 900/1251 ( 72%)]  Loss: 5.097 (5.08)  Time: 0.841s, 1217.34/s  (0.809s, 1266.01/s)  LR: 9.997e-04  Data: 0.012 (0.014)
Train: 7 [ 950/1251 ( 76%)]  Loss: 4.891 (5.07)  Time: 0.851s, 1203.73/s  (0.809s, 1266.16/s)  LR: 9.997e-04  Data: 0.011 (0.014)
Train: 7 [1000/1251 ( 80%)]  Loss: 5.058 (5.07)  Time: 0.789s, 1297.66/s  (0.809s, 1265.38/s)  LR: 9.997e-04  Data: 0.013 (0.014)
Train: 7 [1050/1251 ( 84%)]  Loss: 5.036 (5.07)  Time: 0.778s, 1315.51/s  (0.809s, 1266.09/s)  LR: 9.997e-04  Data: 0.013 (0.014)
Train: 7 [1100/1251 ( 88%)]  Loss: 4.861 (5.06)  Time: 0.816s, 1255.32/s  (0.809s, 1266.01/s)  LR: 9.997e-04  Data: 0.014 (0.014)
Train: 7 [1150/1251 ( 92%)]  Loss: 4.893 (5.05)  Time: 0.846s, 1210.43/s  (0.810s, 1264.39/s)  LR: 9.997e-04  Data: 0.014 (0.014)
Train: 7 [1200/1251 ( 96%)]  Loss: 4.916 (5.05)  Time: 0.780s, 1313.25/s  (0.810s, 1264.91/s)  LR: 9.997e-04  Data: 0.012 (0.014)
Train: 7 [1250/1251 (100%)]  Loss: 4.944 (5.04)  Time: 0.802s, 1277.29/s  (0.809s, 1265.62/s)  LR: 9.997e-04  Data: 0.000 (0.014)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.788 (1.788)  Loss:  1.8672 (1.8672)  Acc@1: 62.7930 (62.7930)  Acc@5: 84.6680 (84.6680)
Test: [  48/48]  Time: 0.194 (0.615)  Loss:  1.9512 (2.7857)  Acc@1: 64.2689 (41.8640)  Acc@5: 82.9009 (69.1600)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-7.pth.tar', 41.86399999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-6.pth.tar', 36.44400001831055)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-5.pth.tar', 30.560000057373045)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-4.pth.tar', 25.378000018310548)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-3.pth.tar', 14.512000023193359)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-2.pth.tar', 5.806000003051758)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-1.pth.tar', 2.940000000915527)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-0.pth.tar', 0.17399999872207642)

Train: 8 [   0/1251 (  0%)]  Loss: 5.137 (5.14)  Time: 2.531s,  404.63/s  (2.531s,  404.63/s)  LR: 9.996e-04  Data: 1.761 (1.761)
Train: 8 [  50/1251 (  4%)]  Loss: 4.947 (5.04)  Time: 0.784s, 1305.33/s  (0.846s, 1210.02/s)  LR: 9.996e-04  Data: 0.009 (0.053)
Train: 8 [ 100/1251 (  8%)]  Loss: 4.649 (4.91)  Time: 0.850s, 1205.20/s  (0.826s, 1239.06/s)  LR: 9.996e-04  Data: 0.009 (0.033)
Train: 8 [ 150/1251 ( 12%)]  Loss: 4.840 (4.89)  Time: 0.863s, 1187.15/s  (0.822s, 1245.74/s)  LR: 9.996e-04  Data: 0.013 (0.026)
Train: 8 [ 200/1251 ( 16%)]  Loss: 4.797 (4.87)  Time: 0.810s, 1263.91/s  (0.819s, 1250.02/s)  LR: 9.996e-04  Data: 0.010 (0.022)
Train: 8 [ 250/1251 ( 20%)]  Loss: 4.628 (4.83)  Time: 0.802s, 1276.71/s  (0.816s, 1255.18/s)  LR: 9.996e-04  Data: 0.013 (0.020)
Train: 8 [ 300/1251 ( 24%)]  Loss: 4.981 (4.85)  Time: 0.816s, 1254.96/s  (0.814s, 1258.72/s)  LR: 9.996e-04  Data: 0.009 (0.019)
Train: 8 [ 350/1251 ( 28%)]  Loss: 4.753 (4.84)  Time: 0.813s, 1259.76/s  (0.813s, 1259.99/s)  LR: 9.996e-04  Data: 0.009 (0.018)
Train: 8 [ 400/1251 ( 32%)]  Loss: 4.832 (4.84)  Time: 0.827s, 1237.85/s  (0.812s, 1261.00/s)  LR: 9.996e-04  Data: 0.010 (0.017)
Train: 8 [ 450/1251 ( 36%)]  Loss: 4.781 (4.83)  Time: 0.813s, 1259.59/s  (0.814s, 1258.56/s)  LR: 9.996e-04  Data: 0.011 (0.017)
Train: 8 [ 500/1251 ( 40%)]  Loss: 5.010 (4.85)  Time: 0.833s, 1229.45/s  (0.813s, 1259.59/s)  LR: 9.996e-04  Data: 0.009 (0.016)
Train: 8 [ 550/1251 ( 44%)]  Loss: 4.797 (4.85)  Time: 0.800s, 1280.75/s  (0.812s, 1260.36/s)  LR: 9.996e-04  Data: 0.014 (0.016)
Train: 8 [ 600/1251 ( 48%)]  Loss: 4.761 (4.84)  Time: 0.789s, 1297.79/s  (0.812s, 1260.47/s)  LR: 9.996e-04  Data: 0.012 (0.015)
Train: 8 [ 650/1251 ( 52%)]  Loss: 5.199 (4.87)  Time: 0.825s, 1241.59/s  (0.812s, 1260.83/s)  LR: 9.996e-04  Data: 0.010 (0.015)
Train: 8 [ 700/1251 ( 56%)]  Loss: 4.619 (4.85)  Time: 0.832s, 1230.74/s  (0.812s, 1261.60/s)  LR: 9.996e-04  Data: 0.009 (0.015)
Train: 8 [ 750/1251 ( 60%)]  Loss: 4.865 (4.85)  Time: 0.818s, 1252.06/s  (0.811s, 1262.18/s)  LR: 9.996e-04  Data: 0.012 (0.015)
Train: 8 [ 800/1251 ( 64%)]  Loss: 4.617 (4.84)  Time: 0.787s, 1301.59/s  (0.811s, 1262.59/s)  LR: 9.996e-04  Data: 0.009 (0.015)
Train: 8 [ 850/1251 ( 68%)]  Loss: 4.933 (4.84)  Time: 0.777s, 1317.23/s  (0.811s, 1262.90/s)  LR: 9.996e-04  Data: 0.010 (0.014)
Train: 8 [ 900/1251 ( 72%)]  Loss: 4.545 (4.83)  Time: 0.780s, 1313.48/s  (0.811s, 1263.05/s)  LR: 9.996e-04  Data: 0.010 (0.014)
Train: 8 [ 950/1251 ( 76%)]  Loss: 4.144 (4.79)  Time: 0.826s, 1240.25/s  (0.811s, 1262.61/s)  LR: 9.996e-04  Data: 0.010 (0.014)
Train: 8 [1000/1251 ( 80%)]  Loss: 5.152 (4.81)  Time: 0.786s, 1302.83/s  (0.811s, 1263.04/s)  LR: 9.996e-04  Data: 0.010 (0.014)
Train: 8 [1050/1251 ( 84%)]  Loss: 4.592 (4.80)  Time: 0.797s, 1284.39/s  (0.810s, 1263.55/s)  LR: 9.996e-04  Data: 0.010 (0.014)
Train: 8 [1100/1251 ( 88%)]  Loss: 5.099 (4.81)  Time: 0.776s, 1319.07/s  (0.811s, 1263.37/s)  LR: 9.996e-04  Data: 0.010 (0.014)
Train: 8 [1150/1251 ( 92%)]  Loss: 4.999 (4.82)  Time: 0.817s, 1253.06/s  (0.811s, 1263.34/s)  LR: 9.996e-04  Data: 0.011 (0.014)
Train: 8 [1200/1251 ( 96%)]  Loss: 5.061 (4.83)  Time: 0.815s, 1255.80/s  (0.810s, 1263.83/s)  LR: 9.996e-04  Data: 0.011 (0.014)
Train: 8 [1250/1251 (100%)]  Loss: 4.738 (4.83)  Time: 0.826s, 1239.51/s  (0.810s, 1263.89/s)  LR: 9.996e-04  Data: 0.000 (0.014)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.729 (1.729)  Loss:  1.8506 (1.8506)  Acc@1: 62.6953 (62.6953)  Acc@5: 87.7930 (87.7930)
Test: [  48/48]  Time: 0.194 (0.611)  Loss:  1.8633 (2.5427)  Acc@1: 67.8066 (45.7780)  Acc@5: 83.6085 (72.9540)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-8.pth.tar', 45.777999978027346)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-7.pth.tar', 41.86399999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-6.pth.tar', 36.44400001831055)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-5.pth.tar', 30.560000057373045)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-4.pth.tar', 25.378000018310548)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-3.pth.tar', 14.512000023193359)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-2.pth.tar', 5.806000003051758)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-1.pth.tar', 2.940000000915527)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-0.pth.tar', 0.17399999872207642)

Train: 9 [   0/1251 (  0%)]  Loss: 4.848 (4.85)  Time: 2.560s,  399.96/s  (2.560s,  399.96/s)  LR: 9.995e-04  Data: 1.834 (1.834)
Train: 9 [  50/1251 (  4%)]  Loss: 4.619 (4.73)  Time: 0.817s, 1253.89/s  (0.849s, 1205.67/s)  LR: 9.995e-04  Data: 0.015 (0.055)
Train: 9 [ 100/1251 (  8%)]  Loss: 4.797 (4.75)  Time: 0.817s, 1253.92/s  (0.827s, 1238.54/s)  LR: 9.995e-04  Data: 0.012 (0.033)
Train: 9 [ 150/1251 ( 12%)]  Loss: 4.760 (4.76)  Time: 0.826s, 1240.27/s  (0.820s, 1248.52/s)  LR: 9.995e-04  Data: 0.011 (0.026)
Train: 9 [ 200/1251 ( 16%)]  Loss: 4.548 (4.71)  Time: 0.789s, 1297.54/s  (0.818s, 1251.86/s)  LR: 9.995e-04  Data: 0.010 (0.023)
Train: 9 [ 250/1251 ( 20%)]  Loss: 4.658 (4.71)  Time: 0.776s, 1319.66/s  (0.816s, 1254.51/s)  LR: 9.995e-04  Data: 0.011 (0.021)
Train: 9 [ 300/1251 ( 24%)]  Loss: 4.606 (4.69)  Time: 0.800s, 1280.06/s  (0.815s, 1256.94/s)  LR: 9.995e-04  Data: 0.018 (0.019)
Train: 9 [ 350/1251 ( 28%)]  Loss: 4.979 (4.73)  Time: 0.792s, 1293.30/s  (0.813s, 1258.97/s)  LR: 9.995e-04  Data: 0.017 (0.018)
Train: 9 [ 400/1251 ( 32%)]  Loss: 5.024 (4.76)  Time: 0.777s, 1318.03/s  (0.813s, 1259.82/s)  LR: 9.995e-04  Data: 0.013 (0.017)
Train: 9 [ 450/1251 ( 36%)]  Loss: 4.883 (4.77)  Time: 0.777s, 1318.51/s  (0.812s, 1261.15/s)  LR: 9.995e-04  Data: 0.012 (0.017)
Train: 9 [ 500/1251 ( 40%)]  Loss: 4.412 (4.74)  Time: 0.819s, 1250.18/s  (0.812s, 1260.88/s)  LR: 9.995e-04  Data: 0.012 (0.016)
Train: 9 [ 550/1251 ( 44%)]  Loss: 4.809 (4.75)  Time: 0.783s, 1306.99/s  (0.812s, 1261.74/s)  LR: 9.995e-04  Data: 0.012 (0.016)
Train: 9 [ 600/1251 ( 48%)]  Loss: 4.800 (4.75)  Time: 0.860s, 1190.30/s  (0.811s, 1262.23/s)  LR: 9.995e-04  Data: 0.014 (0.016)
Train: 9 [ 650/1251 ( 52%)]  Loss: 4.359 (4.72)  Time: 0.777s, 1317.27/s  (0.811s, 1262.61/s)  LR: 9.995e-04  Data: 0.010 (0.015)
Train: 9 [ 700/1251 ( 56%)]  Loss: 4.365 (4.70)  Time: 0.778s, 1315.35/s  (0.811s, 1263.36/s)  LR: 9.995e-04  Data: 0.011 (0.015)
Train: 9 [ 750/1251 ( 60%)]  Loss: 5.365 (4.74)  Time: 0.801s, 1279.16/s  (0.810s, 1264.27/s)  LR: 9.995e-04  Data: 0.012 (0.015)
Train: 9 [ 800/1251 ( 64%)]  Loss: 4.579 (4.73)  Time: 0.821s, 1247.03/s  (0.810s, 1264.78/s)  LR: 9.995e-04  Data: 0.012 (0.015)
Train: 9 [ 850/1251 ( 68%)]  Loss: 4.990 (4.74)  Time: 0.875s, 1170.44/s  (0.809s, 1265.14/s)  LR: 9.995e-04  Data: 0.012 (0.014)
Train: 9 [ 900/1251 ( 72%)]  Loss: 4.474 (4.73)  Time: 0.777s, 1318.06/s  (0.809s, 1265.40/s)  LR: 9.995e-04  Data: 0.012 (0.014)
Train: 9 [ 950/1251 ( 76%)]  Loss: 4.826 (4.74)  Time: 0.905s, 1131.69/s  (0.809s, 1265.46/s)  LR: 9.995e-04  Data: 0.013 (0.014)
Train: 9 [1000/1251 ( 80%)]  Loss: 4.755 (4.74)  Time: 0.843s, 1214.53/s  (0.810s, 1264.94/s)  LR: 9.995e-04  Data: 0.018 (0.014)
Train: 9 [1050/1251 ( 84%)]  Loss: 4.735 (4.74)  Time: 0.781s, 1311.72/s  (0.809s, 1265.04/s)  LR: 9.995e-04  Data: 0.012 (0.014)
Train: 9 [1100/1251 ( 88%)]  Loss: 4.523 (4.73)  Time: 0.795s, 1287.52/s  (0.809s, 1265.32/s)  LR: 9.995e-04  Data: 0.014 (0.014)
Train: 9 [1150/1251 ( 92%)]  Loss: 4.843 (4.73)  Time: 0.777s, 1318.38/s  (0.809s, 1265.69/s)  LR: 9.995e-04  Data: 0.013 (0.014)
Train: 9 [1200/1251 ( 96%)]  Loss: 4.837 (4.74)  Time: 0.811s, 1263.20/s  (0.809s, 1265.71/s)  LR: 9.995e-04  Data: 0.013 (0.014)
Train: 9 [1250/1251 (100%)]  Loss: 4.656 (4.73)  Time: 0.810s, 1263.52/s  (0.809s, 1266.05/s)  LR: 9.995e-04  Data: 0.000 (0.014)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.831 (1.831)  Loss:  1.8096 (1.8096)  Acc@1: 68.3594 (68.3594)  Acc@5: 88.5742 (88.5742)
Test: [  48/48]  Time: 0.194 (0.618)  Loss:  1.9551 (2.5620)  Acc@1: 69.3396 (48.2780)  Acc@5: 84.4340 (75.1800)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-9.pth.tar', 48.278000126953124)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-8.pth.tar', 45.777999978027346)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-7.pth.tar', 41.86399999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-6.pth.tar', 36.44400001831055)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-5.pth.tar', 30.560000057373045)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-4.pth.tar', 25.378000018310548)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-3.pth.tar', 14.512000023193359)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-2.pth.tar', 5.806000003051758)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-1.pth.tar', 2.940000000915527)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-0.pth.tar', 0.17399999872207642)

Train: 10 [   0/1251 (  0%)]  Loss: 4.352 (4.35)  Time: 2.367s,  432.65/s  (2.367s,  432.65/s)  LR: 9.993e-04  Data: 1.630 (1.630)
Train: 10 [  50/1251 (  4%)]  Loss: 4.950 (4.65)  Time: 0.795s, 1288.73/s  (0.841s, 1218.03/s)  LR: 9.993e-04  Data: 0.010 (0.046)
Train: 10 [ 100/1251 (  8%)]  Loss: 4.485 (4.60)  Time: 0.866s, 1182.99/s  (0.830s, 1233.93/s)  LR: 9.993e-04  Data: 0.009 (0.029)
Train: 10 [ 150/1251 ( 12%)]  Loss: 4.203 (4.50)  Time: 0.839s, 1220.86/s  (0.822s, 1245.49/s)  LR: 9.993e-04  Data: 0.013 (0.023)
Train: 10 [ 200/1251 ( 16%)]  Loss: 4.460 (4.49)  Time: 0.792s, 1293.38/s  (0.820s, 1249.29/s)  LR: 9.993e-04  Data: 0.010 (0.021)
Train: 10 [ 250/1251 ( 20%)]  Loss: 4.455 (4.48)  Time: 0.839s, 1220.42/s  (0.818s, 1251.74/s)  LR: 9.993e-04  Data: 0.011 (0.019)
Train: 10 [ 300/1251 ( 24%)]  Loss: 4.625 (4.50)  Time: 0.812s, 1261.48/s  (0.816s, 1254.29/s)  LR: 9.993e-04  Data: 0.011 (0.018)
Train: 10 [ 350/1251 ( 28%)]  Loss: 4.647 (4.52)  Time: 0.780s, 1313.26/s  (0.816s, 1255.07/s)  LR: 9.993e-04  Data: 0.014 (0.017)
Train: 10 [ 400/1251 ( 32%)]  Loss: 4.778 (4.55)  Time: 0.811s, 1262.85/s  (0.815s, 1256.85/s)  LR: 9.993e-04  Data: 0.011 (0.016)
Train: 10 [ 450/1251 ( 36%)]  Loss: 4.538 (4.55)  Time: 0.778s, 1315.68/s  (0.815s, 1256.77/s)  LR: 9.993e-04  Data: 0.010 (0.016)
Train: 10 [ 500/1251 ( 40%)]  Loss: 4.797 (4.57)  Time: 0.822s, 1245.35/s  (0.814s, 1258.06/s)  LR: 9.993e-04  Data: 0.014 (0.015)
Train: 10 [ 550/1251 ( 44%)]  Loss: 4.794 (4.59)  Time: 0.780s, 1312.37/s  (0.814s, 1257.94/s)  LR: 9.993e-04  Data: 0.010 (0.015)
Train: 10 [ 600/1251 ( 48%)]  Loss: 4.724 (4.60)  Time: 0.809s, 1265.52/s  (0.814s, 1257.54/s)  LR: 9.993e-04  Data: 0.011 (0.015)
Train: 10 [ 650/1251 ( 52%)]  Loss: 4.864 (4.62)  Time: 0.786s, 1303.59/s  (0.814s, 1257.80/s)  LR: 9.993e-04  Data: 0.016 (0.015)
Train: 10 [ 700/1251 ( 56%)]  Loss: 4.507 (4.61)  Time: 0.802s, 1276.61/s  (0.814s, 1257.94/s)  LR: 9.993e-04  Data: 0.009 (0.014)
Train: 10 [ 750/1251 ( 60%)]  Loss: 4.497 (4.60)  Time: 0.791s, 1295.24/s  (0.813s, 1258.83/s)  LR: 9.993e-04  Data: 0.010 (0.014)
Train: 10 [ 800/1251 ( 64%)]  Loss: 4.815 (4.62)  Time: 0.821s, 1247.45/s  (0.813s, 1259.12/s)  LR: 9.993e-04  Data: 0.010 (0.014)
Train: 10 [ 850/1251 ( 68%)]  Loss: 4.629 (4.62)  Time: 0.822s, 1245.07/s  (0.813s, 1259.41/s)  LR: 9.993e-04  Data: 0.012 (0.014)
Train: 10 [ 900/1251 ( 72%)]  Loss: 4.513 (4.61)  Time: 0.834s, 1227.28/s  (0.812s, 1260.31/s)  LR: 9.993e-04  Data: 0.009 (0.014)
Train: 10 [ 950/1251 ( 76%)]  Loss: 4.457 (4.60)  Time: 0.798s, 1283.88/s  (0.812s, 1260.81/s)  LR: 9.993e-04  Data: 0.014 (0.014)
Train: 10 [1000/1251 ( 80%)]  Loss: 4.590 (4.60)  Time: 0.845s, 1211.28/s  (0.812s, 1260.91/s)  LR: 9.993e-04  Data: 0.011 (0.014)
Train: 10 [1050/1251 ( 84%)]  Loss: 4.953 (4.62)  Time: 0.784s, 1305.64/s  (0.812s, 1261.72/s)  LR: 9.993e-04  Data: 0.009 (0.014)
Train: 10 [1100/1251 ( 88%)]  Loss: 4.787 (4.63)  Time: 0.791s, 1294.19/s  (0.812s, 1261.65/s)  LR: 9.993e-04  Data: 0.010 (0.014)
Train: 10 [1150/1251 ( 92%)]  Loss: 4.942 (4.64)  Time: 0.832s, 1230.45/s  (0.812s, 1261.39/s)  LR: 9.993e-04  Data: 0.009 (0.013)
Train: 10 [1200/1251 ( 96%)]  Loss: 4.771 (4.65)  Time: 0.904s, 1132.84/s  (0.812s, 1261.52/s)  LR: 9.993e-04  Data: 0.010 (0.013)
Train: 10 [1250/1251 (100%)]  Loss: 4.683 (4.65)  Time: 0.793s, 1291.94/s  (0.812s, 1261.67/s)  LR: 9.993e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.758 (1.758)  Loss:  1.6299 (1.6299)  Acc@1: 71.5820 (71.5820)  Acc@5: 88.9648 (88.9648)
Test: [  48/48]  Time: 0.194 (0.603)  Loss:  1.5156 (2.2878)  Acc@1: 72.2877 (52.4980)  Acc@5: 87.9717 (78.1520)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-10.pth.tar', 52.49799998535156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-9.pth.tar', 48.278000126953124)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-8.pth.tar', 45.777999978027346)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-7.pth.tar', 41.86399999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-6.pth.tar', 36.44400001831055)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-5.pth.tar', 30.560000057373045)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-4.pth.tar', 25.378000018310548)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-3.pth.tar', 14.512000023193359)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-2.pth.tar', 5.806000003051758)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-1.pth.tar', 2.940000000915527)

Train: 11 [   0/1251 (  0%)]  Loss: 4.648 (4.65)  Time: 2.561s,  399.84/s  (2.561s,  399.84/s)  LR: 9.992e-04  Data: 1.833 (1.833)
Train: 11 [  50/1251 (  4%)]  Loss: 4.738 (4.69)  Time: 0.784s, 1305.77/s  (0.839s, 1220.05/s)  LR: 9.992e-04  Data: 0.013 (0.051)
Train: 11 [ 100/1251 (  8%)]  Loss: 4.745 (4.71)  Time: 0.807s, 1269.15/s  (0.825s, 1241.38/s)  LR: 9.992e-04  Data: 0.012 (0.032)
Train: 11 [ 150/1251 ( 12%)]  Loss: 4.465 (4.65)  Time: 0.777s, 1317.61/s  (0.818s, 1251.10/s)  LR: 9.992e-04  Data: 0.012 (0.025)
Train: 11 [ 200/1251 ( 16%)]  Loss: 4.295 (4.58)  Time: 0.777s, 1317.69/s  (0.814s, 1258.11/s)  LR: 9.992e-04  Data: 0.012 (0.022)
Train: 11 [ 250/1251 ( 20%)]  Loss: 4.529 (4.57)  Time: 0.788s, 1299.88/s  (0.815s, 1256.91/s)  LR: 9.992e-04  Data: 0.015 (0.020)
Train: 11 [ 300/1251 ( 24%)]  Loss: 4.761 (4.60)  Time: 0.797s, 1284.40/s  (0.814s, 1257.40/s)  LR: 9.992e-04  Data: 0.012 (0.019)
Train: 11 [ 350/1251 ( 28%)]  Loss: 4.714 (4.61)  Time: 0.779s, 1315.10/s  (0.814s, 1258.60/s)  LR: 9.992e-04  Data: 0.012 (0.018)
Train: 11 [ 400/1251 ( 32%)]  Loss: 4.429 (4.59)  Time: 0.828s, 1236.68/s  (0.813s, 1259.51/s)  LR: 9.992e-04  Data: 0.011 (0.017)
Train: 11 [ 450/1251 ( 36%)]  Loss: 4.974 (4.63)  Time: 0.819s, 1250.52/s  (0.812s, 1260.36/s)  LR: 9.992e-04  Data: 0.013 (0.017)
Train: 11 [ 500/1251 ( 40%)]  Loss: 4.257 (4.60)  Time: 0.797s, 1285.40/s  (0.812s, 1260.84/s)  LR: 9.992e-04  Data: 0.012 (0.016)
Train: 11 [ 550/1251 ( 44%)]  Loss: 4.515 (4.59)  Time: 0.815s, 1256.51/s  (0.812s, 1261.07/s)  LR: 9.992e-04  Data: 0.011 (0.016)
Train: 11 [ 600/1251 ( 48%)]  Loss: 4.469 (4.58)  Time: 0.795s, 1288.59/s  (0.812s, 1261.44/s)  LR: 9.992e-04  Data: 0.011 (0.015)
Train: 11 [ 650/1251 ( 52%)]  Loss: 4.905 (4.60)  Time: 0.807s, 1269.22/s  (0.812s, 1261.85/s)  LR: 9.992e-04  Data: 0.016 (0.015)
Train: 11 [ 700/1251 ( 56%)]  Loss: 4.222 (4.58)  Time: 0.820s, 1249.45/s  (0.811s, 1262.25/s)  LR: 9.992e-04  Data: 0.013 (0.015)
Train: 11 [ 750/1251 ( 60%)]  Loss: 4.974 (4.60)  Time: 0.873s, 1173.10/s  (0.811s, 1263.00/s)  LR: 9.992e-04  Data: 0.011 (0.015)
Train: 11 [ 800/1251 ( 64%)]  Loss: 4.452 (4.59)  Time: 0.814s, 1257.31/s  (0.811s, 1262.89/s)  LR: 9.992e-04  Data: 0.011 (0.015)
Train: 11 [ 850/1251 ( 68%)]  Loss: 4.428 (4.58)  Time: 0.844s, 1213.23/s  (0.811s, 1262.71/s)  LR: 9.992e-04  Data: 0.020 (0.014)
Train: 11 [ 900/1251 ( 72%)]  Loss: 4.490 (4.58)  Time: 0.779s, 1315.33/s  (0.811s, 1263.03/s)  LR: 9.992e-04  Data: 0.012 (0.014)
Train: 11 [ 950/1251 ( 76%)]  Loss: 4.540 (4.58)  Time: 0.874s, 1172.03/s  (0.811s, 1262.66/s)  LR: 9.992e-04  Data: 0.012 (0.014)
Train: 11 [1000/1251 ( 80%)]  Loss: 4.560 (4.58)  Time: 0.775s, 1320.77/s  (0.811s, 1262.67/s)  LR: 9.992e-04  Data: 0.012 (0.014)
Train: 11 [1050/1251 ( 84%)]  Loss: 4.425 (4.57)  Time: 0.797s, 1285.44/s  (0.811s, 1262.80/s)  LR: 9.992e-04  Data: 0.013 (0.014)
Train: 11 [1100/1251 ( 88%)]  Loss: 4.228 (4.55)  Time: 0.819s, 1249.83/s  (0.810s, 1263.45/s)  LR: 9.992e-04  Data: 0.013 (0.014)
Train: 11 [1150/1251 ( 92%)]  Loss: 4.651 (4.56)  Time: 0.783s, 1307.76/s  (0.810s, 1263.54/s)  LR: 9.992e-04  Data: 0.015 (0.014)
Train: 11 [1200/1251 ( 96%)]  Loss: 4.516 (4.56)  Time: 0.788s, 1299.85/s  (0.810s, 1263.90/s)  LR: 9.992e-04  Data: 0.013 (0.014)
Train: 11 [1250/1251 (100%)]  Loss: 4.222 (4.54)  Time: 0.769s, 1332.04/s  (0.810s, 1263.94/s)  LR: 9.992e-04  Data: 0.000 (0.014)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.661 (1.661)  Loss:  1.3457 (1.3457)  Acc@1: 76.0742 (76.0742)  Acc@5: 92.5781 (92.5781)
Test: [  48/48]  Time: 0.193 (0.609)  Loss:  1.6338 (2.2262)  Acc@1: 71.1085 (55.0040)  Acc@5: 86.9104 (80.2880)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-11.pth.tar', 55.00399986083984)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-10.pth.tar', 52.49799998535156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-9.pth.tar', 48.278000126953124)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-8.pth.tar', 45.777999978027346)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-7.pth.tar', 41.86399999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-6.pth.tar', 36.44400001831055)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-5.pth.tar', 30.560000057373045)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-4.pth.tar', 25.378000018310548)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-3.pth.tar', 14.512000023193359)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-2.pth.tar', 5.806000003051758)

Train: 12 [   0/1251 (  0%)]  Loss: 4.557 (4.56)  Time: 2.516s,  406.95/s  (2.516s,  406.95/s)  LR: 9.990e-04  Data: 1.786 (1.786)
Train: 12 [  50/1251 (  4%)]  Loss: 4.262 (4.41)  Time: 0.800s, 1280.06/s  (0.854s, 1198.54/s)  LR: 9.990e-04  Data: 0.015 (0.053)
Train: 12 [ 100/1251 (  8%)]  Loss: 4.610 (4.48)  Time: 0.794s, 1288.88/s  (0.832s, 1231.34/s)  LR: 9.990e-04  Data: 0.010 (0.033)
Train: 12 [ 150/1251 ( 12%)]  Loss: 4.430 (4.46)  Time: 0.827s, 1238.60/s  (0.823s, 1243.93/s)  LR: 9.990e-04  Data: 0.009 (0.026)
Train: 12 [ 200/1251 ( 16%)]  Loss: 4.743 (4.52)  Time: 0.783s, 1307.01/s  (0.819s, 1250.13/s)  LR: 9.990e-04  Data: 0.012 (0.022)
Train: 12 [ 250/1251 ( 20%)]  Loss: 4.652 (4.54)  Time: 0.815s, 1256.46/s  (0.817s, 1253.50/s)  LR: 9.990e-04  Data: 0.011 (0.020)
Train: 12 [ 300/1251 ( 24%)]  Loss: 4.316 (4.51)  Time: 0.804s, 1273.56/s  (0.816s, 1254.64/s)  LR: 9.990e-04  Data: 0.010 (0.019)
Train: 12 [ 350/1251 ( 28%)]  Loss: 4.606 (4.52)  Time: 0.799s, 1281.15/s  (0.815s, 1255.96/s)  LR: 9.990e-04  Data: 0.009 (0.018)
Train: 12 [ 400/1251 ( 32%)]  Loss: 4.484 (4.52)  Time: 0.788s, 1299.57/s  (0.815s, 1256.73/s)  LR: 9.990e-04  Data: 0.010 (0.017)
Train: 12 [ 450/1251 ( 36%)]  Loss: 4.582 (4.52)  Time: 0.820s, 1248.48/s  (0.815s, 1257.10/s)  LR: 9.990e-04  Data: 0.010 (0.017)
Train: 12 [ 500/1251 ( 40%)]  Loss: 4.327 (4.51)  Time: 0.781s, 1311.93/s  (0.814s, 1258.37/s)  LR: 9.990e-04  Data: 0.010 (0.016)
Train: 12 [ 550/1251 ( 44%)]  Loss: 4.544 (4.51)  Time: 0.780s, 1312.21/s  (0.814s, 1257.86/s)  LR: 9.990e-04  Data: 0.010 (0.016)
Train: 12 [ 600/1251 ( 48%)]  Loss: 4.397 (4.50)  Time: 0.831s, 1232.88/s  (0.814s, 1258.69/s)  LR: 9.990e-04  Data: 0.010 (0.015)
Train: 12 [ 650/1251 ( 52%)]  Loss: 4.304 (4.49)  Time: 0.792s, 1293.32/s  (0.813s, 1258.76/s)  LR: 9.990e-04  Data: 0.011 (0.015)
Train: 12 [ 700/1251 ( 56%)]  Loss: 4.067 (4.46)  Time: 0.802s, 1276.92/s  (0.813s, 1258.98/s)  LR: 9.990e-04  Data: 0.014 (0.015)
Train: 12 [ 750/1251 ( 60%)]  Loss: 4.479 (4.46)  Time: 0.819s, 1249.82/s  (0.815s, 1256.91/s)  LR: 9.990e-04  Data: 0.011 (0.015)
Train: 12 [ 800/1251 ( 64%)]  Loss: 4.482 (4.46)  Time: 0.817s, 1253.76/s  (0.814s, 1257.28/s)  LR: 9.990e-04  Data: 0.009 (0.015)
Train: 12 [ 850/1251 ( 68%)]  Loss: 4.744 (4.48)  Time: 0.859s, 1191.41/s  (0.813s, 1258.97/s)  LR: 9.990e-04  Data: 0.013 (0.015)
Train: 12 [ 900/1251 ( 72%)]  Loss: 4.428 (4.47)  Time: 0.793s, 1290.79/s  (0.813s, 1259.49/s)  LR: 9.990e-04  Data: 0.013 (0.014)
Train: 12 [ 950/1251 ( 76%)]  Loss: 4.453 (4.47)  Time: 0.777s, 1318.63/s  (0.813s, 1259.78/s)  LR: 9.990e-04  Data: 0.009 (0.014)
Train: 12 [1000/1251 ( 80%)]  Loss: 4.535 (4.48)  Time: 0.784s, 1305.61/s  (0.812s, 1260.44/s)  LR: 9.990e-04  Data: 0.010 (0.014)
Train: 12 [1050/1251 ( 84%)]  Loss: 4.522 (4.48)  Time: 0.798s, 1283.28/s  (0.812s, 1260.47/s)  LR: 9.990e-04  Data: 0.014 (0.014)
Train: 12 [1100/1251 ( 88%)]  Loss: 4.357 (4.47)  Time: 0.804s, 1273.40/s  (0.812s, 1260.69/s)  LR: 9.990e-04  Data: 0.010 (0.014)
Train: 12 [1150/1251 ( 92%)]  Loss: 4.277 (4.46)  Time: 0.790s, 1295.93/s  (0.812s, 1261.33/s)  LR: 9.990e-04  Data: 0.009 (0.013)
Train: 12 [1200/1251 ( 96%)]  Loss: 4.525 (4.47)  Time: 0.799s, 1281.19/s  (0.811s, 1262.08/s)  LR: 9.990e-04  Data: 0.013 (0.013)
Train: 12 [1250/1251 (100%)]  Loss: 4.587 (4.47)  Time: 0.792s, 1292.36/s  (0.811s, 1262.58/s)  LR: 9.990e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.699 (1.699)  Loss:  1.3369 (1.3369)  Acc@1: 77.5391 (77.5391)  Acc@5: 93.2617 (93.2617)
Test: [  48/48]  Time: 0.194 (0.610)  Loss:  1.4482 (2.1852)  Acc@1: 75.5896 (55.9840)  Acc@5: 90.2123 (80.7660)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-12.pth.tar', 55.98399999755859)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-11.pth.tar', 55.00399986083984)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-10.pth.tar', 52.49799998535156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-9.pth.tar', 48.278000126953124)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-8.pth.tar', 45.777999978027346)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-7.pth.tar', 41.86399999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-6.pth.tar', 36.44400001831055)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-5.pth.tar', 30.560000057373045)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-4.pth.tar', 25.378000018310548)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-3.pth.tar', 14.512000023193359)

Train: 13 [   0/1251 (  0%)]  Loss: 4.539 (4.54)  Time: 2.435s,  420.49/s  (2.435s,  420.49/s)  LR: 9.989e-04  Data: 1.687 (1.687)
Train: 13 [  50/1251 (  4%)]  Loss: 4.191 (4.36)  Time: 0.810s, 1264.11/s  (0.845s, 1212.46/s)  LR: 9.989e-04  Data: 0.013 (0.049)
Train: 13 [ 100/1251 (  8%)]  Loss: 4.284 (4.34)  Time: 0.838s, 1221.70/s  (0.828s, 1237.38/s)  LR: 9.989e-04  Data: 0.009 (0.030)
Train: 13 [ 150/1251 ( 12%)]  Loss: 4.368 (4.35)  Time: 0.796s, 1286.49/s  (0.822s, 1245.95/s)  LR: 9.989e-04  Data: 0.013 (0.024)
Train: 13 [ 200/1251 ( 16%)]  Loss: 4.147 (4.31)  Time: 0.798s, 1283.73/s  (0.819s, 1250.31/s)  LR: 9.989e-04  Data: 0.019 (0.020)
Train: 13 [ 250/1251 ( 20%)]  Loss: 4.709 (4.37)  Time: 0.815s, 1256.70/s  (0.818s, 1251.83/s)  LR: 9.989e-04  Data: 0.010 (0.018)
Train: 13 [ 300/1251 ( 24%)]  Loss: 4.500 (4.39)  Time: 0.814s, 1258.60/s  (0.816s, 1255.16/s)  LR: 9.989e-04  Data: 0.009 (0.017)
Train: 13 [ 350/1251 ( 28%)]  Loss: 4.703 (4.43)  Time: 0.846s, 1210.14/s  (0.814s, 1257.63/s)  LR: 9.989e-04  Data: 0.010 (0.016)
Train: 13 [ 400/1251 ( 32%)]  Loss: 4.126 (4.40)  Time: 0.778s, 1316.76/s  (0.814s, 1258.26/s)  LR: 9.989e-04  Data: 0.009 (0.015)
Train: 13 [ 450/1251 ( 36%)]  Loss: 4.544 (4.41)  Time: 0.780s, 1312.61/s  (0.813s, 1259.45/s)  LR: 9.989e-04  Data: 0.010 (0.015)
Train: 13 [ 500/1251 ( 40%)]  Loss: 4.621 (4.43)  Time: 0.783s, 1307.08/s  (0.813s, 1259.91/s)  LR: 9.989e-04  Data: 0.010 (0.015)
Train: 13 [ 550/1251 ( 44%)]  Loss: 4.165 (4.41)  Time: 0.792s, 1292.47/s  (0.812s, 1260.40/s)  LR: 9.989e-04  Data: 0.011 (0.014)
Train: 13 [ 600/1251 ( 48%)]  Loss: 4.238 (4.40)  Time: 0.905s, 1131.67/s  (0.813s, 1260.13/s)  LR: 9.989e-04  Data: 0.011 (0.014)
Train: 13 [ 650/1251 ( 52%)]  Loss: 4.421 (4.40)  Time: 0.782s, 1309.58/s  (0.813s, 1259.68/s)  LR: 9.989e-04  Data: 0.010 (0.014)
Train: 13 [ 700/1251 ( 56%)]  Loss: 4.570 (4.41)  Time: 0.841s, 1217.09/s  (0.812s, 1260.43/s)  LR: 9.989e-04  Data: 0.011 (0.013)
Train: 13 [ 750/1251 ( 60%)]  Loss: 4.338 (4.40)  Time: 0.838s, 1221.66/s  (0.812s, 1260.63/s)  LR: 9.989e-04  Data: 0.009 (0.013)
Train: 13 [ 800/1251 ( 64%)]  Loss: 4.374 (4.40)  Time: 0.845s, 1211.76/s  (0.812s, 1261.34/s)  LR: 9.989e-04  Data: 0.009 (0.013)
Train: 13 [ 850/1251 ( 68%)]  Loss: 3.652 (4.36)  Time: 0.785s, 1303.65/s  (0.812s, 1261.70/s)  LR: 9.989e-04  Data: 0.010 (0.013)
Train: 13 [ 900/1251 ( 72%)]  Loss: 4.293 (4.36)  Time: 0.824s, 1243.26/s  (0.812s, 1261.73/s)  LR: 9.989e-04  Data: 0.010 (0.013)
Train: 13 [ 950/1251 ( 76%)]  Loss: 4.271 (4.35)  Time: 0.833s, 1229.19/s  (0.812s, 1261.63/s)  LR: 9.989e-04  Data: 0.009 (0.013)
Train: 13 [1000/1251 ( 80%)]  Loss: 4.350 (4.35)  Time: 0.806s, 1271.03/s  (0.811s, 1262.29/s)  LR: 9.989e-04  Data: 0.014 (0.013)
Train: 13 [1050/1251 ( 84%)]  Loss: 4.426 (4.36)  Time: 0.783s, 1307.76/s  (0.811s, 1262.56/s)  LR: 9.989e-04  Data: 0.010 (0.012)
Train: 13 [1100/1251 ( 88%)]  Loss: 4.371 (4.36)  Time: 0.852s, 1202.04/s  (0.811s, 1262.83/s)  LR: 9.989e-04  Data: 0.009 (0.012)
Train: 13 [1150/1251 ( 92%)]  Loss: 4.287 (4.35)  Time: 0.782s, 1309.13/s  (0.811s, 1263.02/s)  LR: 9.989e-04  Data: 0.015 (0.012)
Train: 13 [1200/1251 ( 96%)]  Loss: 4.448 (4.36)  Time: 0.795s, 1288.21/s  (0.811s, 1263.32/s)  LR: 9.989e-04  Data: 0.013 (0.012)
Train: 13 [1250/1251 (100%)]  Loss: 4.126 (4.35)  Time: 0.781s, 1310.87/s  (0.810s, 1263.73/s)  LR: 9.989e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.665 (1.665)  Loss:  1.3623 (1.3623)  Acc@1: 77.3438 (77.3438)  Acc@5: 92.2852 (92.2852)
Test: [  48/48]  Time: 0.194 (0.612)  Loss:  1.5615 (2.1282)  Acc@1: 72.8774 (57.2000)  Acc@5: 90.3302 (81.8360)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-13.pth.tar', 57.200000112304686)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-12.pth.tar', 55.98399999755859)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-11.pth.tar', 55.00399986083984)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-10.pth.tar', 52.49799998535156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-9.pth.tar', 48.278000126953124)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-8.pth.tar', 45.777999978027346)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-7.pth.tar', 41.86399999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-6.pth.tar', 36.44400001831055)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-5.pth.tar', 30.560000057373045)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-4.pth.tar', 25.378000018310548)

Train: 14 [   0/1251 (  0%)]  Loss: 4.722 (4.72)  Time: 2.513s,  407.52/s  (2.513s,  407.52/s)  LR: 9.987e-04  Data: 1.748 (1.748)
Train: 14 [  50/1251 (  4%)]  Loss: 4.498 (4.61)  Time: 0.782s, 1310.02/s  (0.847s, 1208.32/s)  LR: 9.987e-04  Data: 0.012 (0.056)
Train: 14 [ 100/1251 (  8%)]  Loss: 3.863 (4.36)  Time: 0.867s, 1180.55/s  (0.828s, 1236.05/s)  LR: 9.987e-04  Data: 0.014 (0.033)
Train: 14 [ 150/1251 ( 12%)]  Loss: 4.448 (4.38)  Time: 0.813s, 1259.18/s  (0.821s, 1246.89/s)  LR: 9.987e-04  Data: 0.010 (0.026)
Train: 14 [ 200/1251 ( 16%)]  Loss: 4.548 (4.42)  Time: 0.872s, 1174.16/s  (0.820s, 1248.92/s)  LR: 9.987e-04  Data: 0.010 (0.022)
Train: 14 [ 250/1251 ( 20%)]  Loss: 4.680 (4.46)  Time: 0.850s, 1204.28/s  (0.818s, 1252.43/s)  LR: 9.987e-04  Data: 0.010 (0.020)
Train: 14 [ 300/1251 ( 24%)]  Loss: 4.389 (4.45)  Time: 0.834s, 1227.68/s  (0.816s, 1255.18/s)  LR: 9.987e-04  Data: 0.009 (0.018)
Train: 14 [ 350/1251 ( 28%)]  Loss: 4.357 (4.44)  Time: 0.832s, 1230.39/s  (0.815s, 1256.40/s)  LR: 9.987e-04  Data: 0.014 (0.017)
Train: 14 [ 400/1251 ( 32%)]  Loss: 4.476 (4.44)  Time: 0.839s, 1220.72/s  (0.814s, 1257.23/s)  LR: 9.987e-04  Data: 0.009 (0.016)
Train: 14 [ 450/1251 ( 36%)]  Loss: 4.548 (4.45)  Time: 0.850s, 1204.59/s  (0.814s, 1257.77/s)  LR: 9.987e-04  Data: 0.009 (0.016)
Train: 14 [ 500/1251 ( 40%)]  Loss: 4.158 (4.43)  Time: 0.787s, 1300.69/s  (0.814s, 1258.15/s)  LR: 9.987e-04  Data: 0.015 (0.015)
Train: 14 [ 550/1251 ( 44%)]  Loss: 4.330 (4.42)  Time: 0.822s, 1245.46/s  (0.813s, 1259.40/s)  LR: 9.987e-04  Data: 0.010 (0.015)
Train: 14 [ 600/1251 ( 48%)]  Loss: 4.530 (4.43)  Time: 0.794s, 1290.10/s  (0.813s, 1259.71/s)  LR: 9.987e-04  Data: 0.009 (0.015)
Train: 14 [ 650/1251 ( 52%)]  Loss: 4.499 (4.43)  Time: 0.795s, 1287.62/s  (0.812s, 1260.46/s)  LR: 9.987e-04  Data: 0.010 (0.014)
Train: 14 [ 700/1251 ( 56%)]  Loss: 4.444 (4.43)  Time: 0.783s, 1307.78/s  (0.812s, 1260.98/s)  LR: 9.987e-04  Data: 0.011 (0.014)
Train: 14 [ 750/1251 ( 60%)]  Loss: 4.513 (4.44)  Time: 0.774s, 1322.61/s  (0.812s, 1261.63/s)  LR: 9.987e-04  Data: 0.011 (0.014)
Train: 14 [ 800/1251 ( 64%)]  Loss: 4.102 (4.42)  Time: 0.801s, 1277.96/s  (0.812s, 1260.95/s)  LR: 9.987e-04  Data: 0.009 (0.013)
Train: 14 [ 850/1251 ( 68%)]  Loss: 3.914 (4.39)  Time: 0.827s, 1238.13/s  (0.812s, 1260.97/s)  LR: 9.987e-04  Data: 0.009 (0.013)
Train: 14 [ 900/1251 ( 72%)]  Loss: 4.096 (4.37)  Time: 0.779s, 1314.57/s  (0.812s, 1261.39/s)  LR: 9.987e-04  Data: 0.010 (0.013)
Train: 14 [ 950/1251 ( 76%)]  Loss: 4.459 (4.38)  Time: 0.802s, 1276.10/s  (0.812s, 1261.29/s)  LR: 9.987e-04  Data: 0.010 (0.013)
Train: 14 [1000/1251 ( 80%)]  Loss: 4.080 (4.36)  Time: 0.810s, 1264.43/s  (0.812s, 1261.52/s)  LR: 9.987e-04  Data: 0.013 (0.013)
Train: 14 [1050/1251 ( 84%)]  Loss: 4.101 (4.35)  Time: 0.782s, 1309.71/s  (0.812s, 1261.69/s)  LR: 9.987e-04  Data: 0.010 (0.013)
Train: 14 [1100/1251 ( 88%)]  Loss: 4.088 (4.34)  Time: 0.792s, 1293.27/s  (0.811s, 1262.16/s)  LR: 9.987e-04  Data: 0.009 (0.013)
Train: 14 [1150/1251 ( 92%)]  Loss: 4.154 (4.33)  Time: 0.786s, 1303.56/s  (0.811s, 1262.26/s)  LR: 9.987e-04  Data: 0.012 (0.013)
Train: 14 [1200/1251 ( 96%)]  Loss: 4.524 (4.34)  Time: 0.784s, 1306.18/s  (0.811s, 1262.72/s)  LR: 9.987e-04  Data: 0.012 (0.013)
Train: 14 [1250/1251 (100%)]  Loss: 4.713 (4.36)  Time: 0.791s, 1295.15/s  (0.811s, 1263.26/s)  LR: 9.987e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.687 (1.687)  Loss:  1.3291 (1.3291)  Acc@1: 75.5859 (75.5859)  Acc@5: 92.5781 (92.5781)
Test: [  48/48]  Time: 0.194 (0.618)  Loss:  1.4199 (2.0656)  Acc@1: 76.0613 (58.5860)  Acc@5: 89.7406 (82.6580)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-14.pth.tar', 58.58599994384765)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-13.pth.tar', 57.200000112304686)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-12.pth.tar', 55.98399999755859)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-11.pth.tar', 55.00399986083984)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-10.pth.tar', 52.49799998535156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-9.pth.tar', 48.278000126953124)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-8.pth.tar', 45.777999978027346)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-7.pth.tar', 41.86399999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-6.pth.tar', 36.44400001831055)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-5.pth.tar', 30.560000057373045)

Train: 15 [   0/1251 (  0%)]  Loss: 4.592 (4.59)  Time: 2.507s,  408.51/s  (2.507s,  408.51/s)  LR: 9.985e-04  Data: 1.773 (1.773)
Train: 15 [  50/1251 (  4%)]  Loss: 4.212 (4.40)  Time: 0.779s, 1314.39/s  (0.837s, 1222.91/s)  LR: 9.985e-04  Data: 0.010 (0.047)
Train: 15 [ 100/1251 (  8%)]  Loss: 4.287 (4.36)  Time: 0.796s, 1285.80/s  (0.824s, 1242.14/s)  LR: 9.985e-04  Data: 0.015 (0.030)
Train: 15 [ 150/1251 ( 12%)]  Loss: 4.061 (4.29)  Time: 0.847s, 1208.63/s  (0.820s, 1248.45/s)  LR: 9.985e-04  Data: 0.010 (0.023)
Train: 15 [ 200/1251 ( 16%)]  Loss: 4.428 (4.32)  Time: 0.778s, 1315.50/s  (0.816s, 1255.43/s)  LR: 9.985e-04  Data: 0.009 (0.020)
Train: 15 [ 250/1251 ( 20%)]  Loss: 3.982 (4.26)  Time: 0.803s, 1275.64/s  (0.815s, 1255.78/s)  LR: 9.985e-04  Data: 0.010 (0.018)
Train: 15 [ 300/1251 ( 24%)]  Loss: 4.495 (4.29)  Time: 0.794s, 1289.44/s  (0.815s, 1256.22/s)  LR: 9.985e-04  Data: 0.010 (0.017)
Train: 15 [ 350/1251 ( 28%)]  Loss: 4.414 (4.31)  Time: 0.823s, 1244.05/s  (0.814s, 1258.18/s)  LR: 9.985e-04  Data: 0.011 (0.016)
Train: 15 [ 400/1251 ( 32%)]  Loss: 4.737 (4.36)  Time: 0.848s, 1207.29/s  (0.813s, 1259.31/s)  LR: 9.985e-04  Data: 0.009 (0.015)
Train: 15 [ 450/1251 ( 36%)]  Loss: 4.727 (4.39)  Time: 0.815s, 1257.19/s  (0.813s, 1260.21/s)  LR: 9.985e-04  Data: 0.013 (0.015)
Train: 15 [ 500/1251 ( 40%)]  Loss: 4.482 (4.40)  Time: 0.801s, 1277.64/s  (0.812s, 1260.79/s)  LR: 9.985e-04  Data: 0.009 (0.014)
Train: 15 [ 550/1251 ( 44%)]  Loss: 4.657 (4.42)  Time: 0.791s, 1294.76/s  (0.812s, 1261.22/s)  LR: 9.985e-04  Data: 0.011 (0.014)
Train: 15 [ 600/1251 ( 48%)]  Loss: 4.388 (4.42)  Time: 0.852s, 1202.35/s  (0.812s, 1260.85/s)  LR: 9.985e-04  Data: 0.009 (0.014)
Train: 15 [ 650/1251 ( 52%)]  Loss: 4.603 (4.43)  Time: 0.780s, 1312.84/s  (0.812s, 1261.26/s)  LR: 9.985e-04  Data: 0.009 (0.014)
Train: 15 [ 700/1251 ( 56%)]  Loss: 4.337 (4.43)  Time: 0.809s, 1266.01/s  (0.812s, 1261.24/s)  LR: 9.985e-04  Data: 0.012 (0.013)
Train: 15 [ 750/1251 ( 60%)]  Loss: 4.106 (4.41)  Time: 0.820s, 1249.08/s  (0.812s, 1261.56/s)  LR: 9.985e-04  Data: 0.018 (0.013)
Train: 15 [ 800/1251 ( 64%)]  Loss: 4.345 (4.40)  Time: 0.827s, 1238.68/s  (0.811s, 1262.11/s)  LR: 9.985e-04  Data: 0.010 (0.013)
Train: 15 [ 850/1251 ( 68%)]  Loss: 4.309 (4.40)  Time: 0.791s, 1295.09/s  (0.811s, 1262.25/s)  LR: 9.985e-04  Data: 0.010 (0.013)
Train: 15 [ 900/1251 ( 72%)]  Loss: 4.681 (4.41)  Time: 0.814s, 1257.60/s  (0.811s, 1262.05/s)  LR: 9.985e-04  Data: 0.010 (0.013)
Train: 15 [ 950/1251 ( 76%)]  Loss: 4.339 (4.41)  Time: 0.805s, 1272.78/s  (0.811s, 1262.12/s)  LR: 9.985e-04  Data: 0.013 (0.013)
Train: 15 [1000/1251 ( 80%)]  Loss: 3.965 (4.39)  Time: 0.828s, 1237.37/s  (0.811s, 1262.41/s)  LR: 9.985e-04  Data: 0.010 (0.013)
Train: 15 [1050/1251 ( 84%)]  Loss: 4.375 (4.39)  Time: 0.812s, 1261.60/s  (0.811s, 1262.92/s)  LR: 9.985e-04  Data: 0.010 (0.012)
Train: 15 [1100/1251 ( 88%)]  Loss: 4.063 (4.37)  Time: 0.777s, 1317.48/s  (0.811s, 1262.91/s)  LR: 9.985e-04  Data: 0.010 (0.012)
Train: 15 [1150/1251 ( 92%)]  Loss: 4.126 (4.36)  Time: 0.806s, 1269.97/s  (0.811s, 1263.28/s)  LR: 9.985e-04  Data: 0.009 (0.012)
Train: 15 [1200/1251 ( 96%)]  Loss: 4.384 (4.36)  Time: 0.808s, 1267.05/s  (0.810s, 1263.72/s)  LR: 9.985e-04  Data: 0.012 (0.012)
Train: 15 [1250/1251 (100%)]  Loss: 4.573 (4.37)  Time: 0.824s, 1243.22/s  (0.810s, 1264.07/s)  LR: 9.985e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.660 (1.660)  Loss:  1.4111 (1.4111)  Acc@1: 77.7344 (77.7344)  Acc@5: 92.4805 (92.4805)
Test: [  48/48]  Time: 0.194 (0.604)  Loss:  1.3281 (2.1012)  Acc@1: 76.2972 (59.0340)  Acc@5: 92.3349 (82.9880)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-15.pth.tar', 59.034000046386716)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-14.pth.tar', 58.58599994384765)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-13.pth.tar', 57.200000112304686)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-12.pth.tar', 55.98399999755859)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-11.pth.tar', 55.00399986083984)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-10.pth.tar', 52.49799998535156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-9.pth.tar', 48.278000126953124)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-8.pth.tar', 45.777999978027346)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-7.pth.tar', 41.86399999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-6.pth.tar', 36.44400001831055)

Train: 16 [   0/1251 (  0%)]  Loss: 4.084 (4.08)  Time: 2.405s,  425.83/s  (2.405s,  425.83/s)  LR: 9.983e-04  Data: 1.661 (1.661)
Train: 16 [  50/1251 (  4%)]  Loss: 4.550 (4.32)  Time: 0.827s, 1238.71/s  (0.850s, 1205.00/s)  LR: 9.983e-04  Data: 0.010 (0.050)
Train: 16 [ 100/1251 (  8%)]  Loss: 3.818 (4.15)  Time: 0.807s, 1268.42/s  (0.829s, 1234.70/s)  LR: 9.983e-04  Data: 0.011 (0.030)
Train: 16 [ 150/1251 ( 12%)]  Loss: 4.004 (4.11)  Time: 0.791s, 1294.09/s  (0.821s, 1247.92/s)  LR: 9.983e-04  Data: 0.009 (0.024)
Train: 16 [ 200/1251 ( 16%)]  Loss: 3.944 (4.08)  Time: 0.846s, 1210.17/s  (0.818s, 1252.57/s)  LR: 9.983e-04  Data: 0.010 (0.020)
Train: 16 [ 250/1251 ( 20%)]  Loss: 4.293 (4.12)  Time: 0.798s, 1282.50/s  (0.815s, 1255.78/s)  LR: 9.983e-04  Data: 0.009 (0.018)
Train: 16 [ 300/1251 ( 24%)]  Loss: 4.443 (4.16)  Time: 0.778s, 1316.19/s  (0.813s, 1258.89/s)  LR: 9.983e-04  Data: 0.010 (0.017)
Train: 16 [ 350/1251 ( 28%)]  Loss: 4.484 (4.20)  Time: 0.809s, 1266.03/s  (0.813s, 1259.15/s)  LR: 9.983e-04  Data: 0.012 (0.016)
Train: 16 [ 400/1251 ( 32%)]  Loss: 4.176 (4.20)  Time: 0.831s, 1232.33/s  (0.813s, 1258.97/s)  LR: 9.983e-04  Data: 0.009 (0.015)
Train: 16 [ 450/1251 ( 36%)]  Loss: 4.865 (4.27)  Time: 0.782s, 1310.03/s  (0.813s, 1259.64/s)  LR: 9.983e-04  Data: 0.010 (0.015)
Train: 16 [ 500/1251 ( 40%)]  Loss: 4.195 (4.26)  Time: 0.792s, 1292.66/s  (0.812s, 1260.56/s)  LR: 9.983e-04  Data: 0.011 (0.014)
Train: 16 [ 550/1251 ( 44%)]  Loss: 4.044 (4.24)  Time: 0.798s, 1282.69/s  (0.812s, 1260.82/s)  LR: 9.983e-04  Data: 0.015 (0.014)
Train: 16 [ 600/1251 ( 48%)]  Loss: 4.671 (4.27)  Time: 0.825s, 1240.48/s  (0.812s, 1261.05/s)  LR: 9.983e-04  Data: 0.010 (0.014)
Train: 16 [ 650/1251 ( 52%)]  Loss: 4.535 (4.29)  Time: 0.800s, 1279.25/s  (0.812s, 1260.92/s)  LR: 9.983e-04  Data: 0.009 (0.014)
Train: 16 [ 700/1251 ( 56%)]  Loss: 4.030 (4.28)  Time: 0.778s, 1315.41/s  (0.812s, 1261.29/s)  LR: 9.983e-04  Data: 0.013 (0.013)
Train: 16 [ 750/1251 ( 60%)]  Loss: 4.087 (4.26)  Time: 0.784s, 1306.41/s  (0.812s, 1261.22/s)  LR: 9.983e-04  Data: 0.010 (0.013)
Train: 16 [ 800/1251 ( 64%)]  Loss: 4.506 (4.28)  Time: 0.774s, 1323.61/s  (0.811s, 1262.07/s)  LR: 9.983e-04  Data: 0.011 (0.013)
Train: 16 [ 850/1251 ( 68%)]  Loss: 4.287 (4.28)  Time: 0.780s, 1313.13/s  (0.811s, 1262.82/s)  LR: 9.983e-04  Data: 0.010 (0.013)
Train: 16 [ 900/1251 ( 72%)]  Loss: 3.936 (4.26)  Time: 0.812s, 1260.40/s  (0.811s, 1263.41/s)  LR: 9.983e-04  Data: 0.014 (0.013)
Train: 16 [ 950/1251 ( 76%)]  Loss: 4.478 (4.27)  Time: 0.776s, 1319.32/s  (0.811s, 1262.83/s)  LR: 9.983e-04  Data: 0.009 (0.013)
Train: 16 [1000/1251 ( 80%)]  Loss: 4.431 (4.28)  Time: 0.775s, 1321.90/s  (0.811s, 1262.82/s)  LR: 9.983e-04  Data: 0.011 (0.013)
Train: 16 [1050/1251 ( 84%)]  Loss: 4.278 (4.28)  Time: 0.794s, 1290.31/s  (0.811s, 1263.21/s)  LR: 9.983e-04  Data: 0.009 (0.013)
Train: 16 [1100/1251 ( 88%)]  Loss: 4.556 (4.29)  Time: 0.826s, 1239.96/s  (0.811s, 1263.19/s)  LR: 9.983e-04  Data: 0.010 (0.012)
Train: 16 [1150/1251 ( 92%)]  Loss: 4.509 (4.30)  Time: 0.812s, 1261.63/s  (0.811s, 1263.01/s)  LR: 9.983e-04  Data: 0.009 (0.012)
Train: 16 [1200/1251 ( 96%)]  Loss: 4.081 (4.29)  Time: 0.823s, 1244.31/s  (0.811s, 1262.76/s)  LR: 9.983e-04  Data: 0.010 (0.012)
Train: 16 [1250/1251 (100%)]  Loss: 4.475 (4.30)  Time: 0.763s, 1341.74/s  (0.811s, 1263.33/s)  LR: 9.983e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.647 (1.647)  Loss:  1.2607 (1.2607)  Acc@1: 78.5156 (78.5156)  Acc@5: 92.6758 (92.6758)
Test: [  48/48]  Time: 0.194 (0.614)  Loss:  1.0996 (1.9651)  Acc@1: 79.7170 (59.9480)  Acc@5: 93.6321 (83.8200)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-16.pth.tar', 59.94799998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-15.pth.tar', 59.034000046386716)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-14.pth.tar', 58.58599994384765)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-13.pth.tar', 57.200000112304686)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-12.pth.tar', 55.98399999755859)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-11.pth.tar', 55.00399986083984)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-10.pth.tar', 52.49799998535156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-9.pth.tar', 48.278000126953124)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-8.pth.tar', 45.777999978027346)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-7.pth.tar', 41.86399999267578)

Train: 17 [   0/1251 (  0%)]  Loss: 4.360 (4.36)  Time: 2.354s,  435.01/s  (2.354s,  435.01/s)  LR: 9.980e-04  Data: 1.603 (1.603)
Train: 17 [  50/1251 (  4%)]  Loss: 4.337 (4.35)  Time: 0.818s, 1252.14/s  (0.842s, 1215.80/s)  LR: 9.980e-04  Data: 0.010 (0.048)
Train: 17 [ 100/1251 (  8%)]  Loss: 3.915 (4.20)  Time: 0.793s, 1291.10/s  (0.824s, 1242.22/s)  LR: 9.980e-04  Data: 0.010 (0.029)
Train: 17 [ 150/1251 ( 12%)]  Loss: 4.480 (4.27)  Time: 0.846s, 1210.81/s  (0.817s, 1253.38/s)  LR: 9.980e-04  Data: 0.009 (0.023)
Train: 17 [ 200/1251 ( 16%)]  Loss: 4.456 (4.31)  Time: 0.794s, 1289.54/s  (0.814s, 1257.52/s)  LR: 9.980e-04  Data: 0.010 (0.020)
Train: 17 [ 250/1251 ( 20%)]  Loss: 4.424 (4.33)  Time: 0.813s, 1259.48/s  (0.814s, 1257.71/s)  LR: 9.980e-04  Data: 0.010 (0.018)
Train: 17 [ 300/1251 ( 24%)]  Loss: 4.382 (4.34)  Time: 0.831s, 1232.75/s  (0.816s, 1254.45/s)  LR: 9.980e-04  Data: 0.009 (0.017)
Train: 17 [ 350/1251 ( 28%)]  Loss: 4.161 (4.31)  Time: 0.784s, 1305.95/s  (0.816s, 1254.98/s)  LR: 9.980e-04  Data: 0.010 (0.016)
Train: 17 [ 400/1251 ( 32%)]  Loss: 3.943 (4.27)  Time: 0.848s, 1206.91/s  (0.815s, 1255.69/s)  LR: 9.980e-04  Data: 0.009 (0.015)
Train: 17 [ 450/1251 ( 36%)]  Loss: 4.160 (4.26)  Time: 0.808s, 1267.69/s  (0.815s, 1256.49/s)  LR: 9.980e-04  Data: 0.013 (0.015)
Train: 17 [ 500/1251 ( 40%)]  Loss: 4.083 (4.25)  Time: 0.782s, 1308.67/s  (0.814s, 1258.30/s)  LR: 9.980e-04  Data: 0.012 (0.014)
Train: 17 [ 550/1251 ( 44%)]  Loss: 4.415 (4.26)  Time: 0.839s, 1220.78/s  (0.813s, 1258.79/s)  LR: 9.980e-04  Data: 0.010 (0.014)
Train: 17 [ 600/1251 ( 48%)]  Loss: 4.505 (4.28)  Time: 0.794s, 1289.70/s  (0.814s, 1258.55/s)  LR: 9.980e-04  Data: 0.010 (0.014)
Train: 17 [ 650/1251 ( 52%)]  Loss: 4.136 (4.27)  Time: 0.779s, 1315.34/s  (0.814s, 1258.66/s)  LR: 9.980e-04  Data: 0.010 (0.013)
Train: 17 [ 700/1251 ( 56%)]  Loss: 4.230 (4.27)  Time: 0.815s, 1256.82/s  (0.813s, 1259.19/s)  LR: 9.980e-04  Data: 0.009 (0.013)
Train: 17 [ 750/1251 ( 60%)]  Loss: 5.049 (4.31)  Time: 0.787s, 1300.67/s  (0.813s, 1259.63/s)  LR: 9.980e-04  Data: 0.010 (0.013)
Train: 17 [ 800/1251 ( 64%)]  Loss: 4.420 (4.32)  Time: 0.792s, 1292.35/s  (0.812s, 1260.36/s)  LR: 9.980e-04  Data: 0.010 (0.013)
Train: 17 [ 850/1251 ( 68%)]  Loss: 3.979 (4.30)  Time: 0.812s, 1260.56/s  (0.812s, 1260.84/s)  LR: 9.980e-04  Data: 0.010 (0.013)
Train: 17 [ 900/1251 ( 72%)]  Loss: 4.168 (4.29)  Time: 0.818s, 1251.86/s  (0.812s, 1261.24/s)  LR: 9.980e-04  Data: 0.009 (0.013)
Train: 17 [ 950/1251 ( 76%)]  Loss: 4.024 (4.28)  Time: 0.815s, 1256.65/s  (0.812s, 1261.31/s)  LR: 9.980e-04  Data: 0.010 (0.012)
Train: 17 [1000/1251 ( 80%)]  Loss: 4.026 (4.27)  Time: 0.817s, 1252.84/s  (0.812s, 1261.76/s)  LR: 9.980e-04  Data: 0.009 (0.012)
Train: 17 [1050/1251 ( 84%)]  Loss: 4.076 (4.26)  Time: 0.863s, 1187.10/s  (0.811s, 1261.87/s)  LR: 9.980e-04  Data: 0.010 (0.012)
Train: 17 [1100/1251 ( 88%)]  Loss: 4.273 (4.26)  Time: 0.824s, 1242.38/s  (0.811s, 1262.30/s)  LR: 9.980e-04  Data: 0.010 (0.012)
Train: 17 [1150/1251 ( 92%)]  Loss: 4.521 (4.27)  Time: 0.803s, 1275.11/s  (0.811s, 1262.96/s)  LR: 9.980e-04  Data: 0.013 (0.012)
Train: 17 [1200/1251 ( 96%)]  Loss: 4.235 (4.27)  Time: 0.834s, 1227.51/s  (0.811s, 1262.86/s)  LR: 9.980e-04  Data: 0.010 (0.012)
Train: 17 [1250/1251 (100%)]  Loss: 4.095 (4.26)  Time: 0.778s, 1316.85/s  (0.811s, 1262.97/s)  LR: 9.980e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.762 (1.762)  Loss:  1.2109 (1.2109)  Acc@1: 78.5156 (78.5156)  Acc@5: 93.7500 (93.7500)
Test: [  48/48]  Time: 0.194 (0.614)  Loss:  1.2539 (1.9085)  Acc@1: 75.5896 (61.3100)  Acc@5: 92.2170 (84.8480)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-17.pth.tar', 61.30999999755859)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-16.pth.tar', 59.94799998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-15.pth.tar', 59.034000046386716)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-14.pth.tar', 58.58599994384765)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-13.pth.tar', 57.200000112304686)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-12.pth.tar', 55.98399999755859)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-11.pth.tar', 55.00399986083984)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-10.pth.tar', 52.49799998535156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-9.pth.tar', 48.278000126953124)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-8.pth.tar', 45.777999978027346)

Train: 18 [   0/1251 (  0%)]  Loss: 4.443 (4.44)  Time: 2.397s,  427.18/s  (2.397s,  427.18/s)  LR: 9.978e-04  Data: 1.666 (1.666)
Train: 18 [  50/1251 (  4%)]  Loss: 4.493 (4.47)  Time: 0.792s, 1292.88/s  (0.846s, 1210.01/s)  LR: 9.978e-04  Data: 0.010 (0.048)
Train: 18 [ 100/1251 (  8%)]  Loss: 4.501 (4.48)  Time: 0.780s, 1313.20/s  (0.826s, 1239.62/s)  LR: 9.978e-04  Data: 0.010 (0.029)
Train: 18 [ 150/1251 ( 12%)]  Loss: 4.477 (4.48)  Time: 0.821s, 1247.39/s  (0.818s, 1251.67/s)  LR: 9.978e-04  Data: 0.014 (0.023)
Train: 18 [ 200/1251 ( 16%)]  Loss: 4.452 (4.47)  Time: 0.787s, 1301.09/s  (0.815s, 1255.93/s)  LR: 9.978e-04  Data: 0.010 (0.020)
Train: 18 [ 250/1251 ( 20%)]  Loss: 3.878 (4.37)  Time: 0.826s, 1240.28/s  (0.813s, 1259.36/s)  LR: 9.978e-04  Data: 0.010 (0.018)
Train: 18 [ 300/1251 ( 24%)]  Loss: 4.253 (4.36)  Time: 0.794s, 1289.29/s  (0.812s, 1260.37/s)  LR: 9.978e-04  Data: 0.016 (0.017)
Train: 18 [ 350/1251 ( 28%)]  Loss: 4.126 (4.33)  Time: 0.798s, 1282.73/s  (0.812s, 1261.65/s)  LR: 9.978e-04  Data: 0.009 (0.016)
Train: 18 [ 400/1251 ( 32%)]  Loss: 4.262 (4.32)  Time: 0.810s, 1263.70/s  (0.812s, 1261.00/s)  LR: 9.978e-04  Data: 0.009 (0.015)
Train: 18 [ 450/1251 ( 36%)]  Loss: 3.811 (4.27)  Time: 0.855s, 1197.49/s  (0.812s, 1261.59/s)  LR: 9.978e-04  Data: 0.010 (0.015)
Train: 18 [ 500/1251 ( 40%)]  Loss: 4.339 (4.28)  Time: 0.811s, 1262.73/s  (0.811s, 1262.09/s)  LR: 9.978e-04  Data: 0.012 (0.014)
Train: 18 [ 550/1251 ( 44%)]  Loss: 3.491 (4.21)  Time: 0.834s, 1227.99/s  (0.811s, 1262.65/s)  LR: 9.978e-04  Data: 0.009 (0.014)
Train: 18 [ 600/1251 ( 48%)]  Loss: 4.209 (4.21)  Time: 0.837s, 1223.16/s  (0.811s, 1263.30/s)  LR: 9.978e-04  Data: 0.011 (0.014)
Train: 18 [ 650/1251 ( 52%)]  Loss: 4.057 (4.20)  Time: 0.822s, 1246.31/s  (0.810s, 1263.58/s)  LR: 9.978e-04  Data: 0.010 (0.014)
Train: 18 [ 700/1251 ( 56%)]  Loss: 4.069 (4.19)  Time: 0.834s, 1227.31/s  (0.810s, 1264.46/s)  LR: 9.978e-04  Data: 0.010 (0.013)
Train: 18 [ 750/1251 ( 60%)]  Loss: 4.016 (4.18)  Time: 0.866s, 1182.88/s  (0.810s, 1264.61/s)  LR: 9.978e-04  Data: 0.012 (0.013)
Train: 18 [ 800/1251 ( 64%)]  Loss: 4.190 (4.18)  Time: 0.818s, 1252.41/s  (0.810s, 1263.95/s)  LR: 9.978e-04  Data: 0.010 (0.013)
Train: 18 [ 850/1251 ( 68%)]  Loss: 4.566 (4.20)  Time: 0.818s, 1251.55/s  (0.810s, 1264.43/s)  LR: 9.978e-04  Data: 0.010 (0.013)
Train: 18 [ 900/1251 ( 72%)]  Loss: 4.174 (4.20)  Time: 0.778s, 1316.29/s  (0.810s, 1264.64/s)  LR: 9.978e-04  Data: 0.009 (0.013)
Train: 18 [ 950/1251 ( 76%)]  Loss: 4.237 (4.20)  Time: 0.781s, 1311.68/s  (0.810s, 1264.81/s)  LR: 9.978e-04  Data: 0.010 (0.013)
Train: 18 [1000/1251 ( 80%)]  Loss: 4.548 (4.22)  Time: 0.847s, 1208.32/s  (0.809s, 1264.99/s)  LR: 9.978e-04  Data: 0.011 (0.013)
Train: 18 [1050/1251 ( 84%)]  Loss: 4.312 (4.22)  Time: 0.858s, 1193.35/s  (0.809s, 1265.02/s)  LR: 9.978e-04  Data: 0.009 (0.012)
Train: 18 [1100/1251 ( 88%)]  Loss: 4.115 (4.22)  Time: 0.846s, 1210.87/s  (0.810s, 1264.44/s)  LR: 9.978e-04  Data: 0.010 (0.012)
Train: 18 [1150/1251 ( 92%)]  Loss: 4.097 (4.21)  Time: 0.830s, 1234.19/s  (0.810s, 1264.15/s)  LR: 9.978e-04  Data: 0.009 (0.012)
Train: 18 [1200/1251 ( 96%)]  Loss: 4.458 (4.22)  Time: 0.778s, 1316.05/s  (0.810s, 1264.64/s)  LR: 9.978e-04  Data: 0.010 (0.012)
Train: 18 [1250/1251 (100%)]  Loss: 4.136 (4.22)  Time: 0.836s, 1225.36/s  (0.809s, 1265.02/s)  LR: 9.978e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.624 (1.624)  Loss:  1.3037 (1.3037)  Acc@1: 79.9805 (79.9805)  Acc@5: 94.3359 (94.3359)
Test: [  48/48]  Time: 0.194 (0.609)  Loss:  1.2500 (1.9727)  Acc@1: 79.0094 (61.4180)  Acc@5: 93.9859 (84.7780)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-18.pth.tar', 61.41799993164062)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-17.pth.tar', 61.30999999755859)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-16.pth.tar', 59.94799998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-15.pth.tar', 59.034000046386716)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-14.pth.tar', 58.58599994384765)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-13.pth.tar', 57.200000112304686)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-12.pth.tar', 55.98399999755859)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-11.pth.tar', 55.00399986083984)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-10.pth.tar', 52.49799998535156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-9.pth.tar', 48.278000126953124)

Train: 19 [   0/1251 (  0%)]  Loss: 4.061 (4.06)  Time: 2.370s,  432.00/s  (2.370s,  432.00/s)  LR: 9.976e-04  Data: 1.643 (1.643)
Train: 19 [  50/1251 (  4%)]  Loss: 4.377 (4.22)  Time: 0.819s, 1250.36/s  (0.844s, 1213.11/s)  LR: 9.976e-04  Data: 0.010 (0.049)
Train: 19 [ 100/1251 (  8%)]  Loss: 4.385 (4.27)  Time: 0.816s, 1255.35/s  (0.827s, 1238.30/s)  LR: 9.976e-04  Data: 0.010 (0.031)
Train: 19 [ 150/1251 ( 12%)]  Loss: 3.955 (4.19)  Time: 0.840s, 1218.79/s  (0.821s, 1248.00/s)  LR: 9.976e-04  Data: 0.017 (0.024)
Train: 19 [ 200/1251 ( 16%)]  Loss: 4.219 (4.20)  Time: 0.782s, 1309.94/s  (0.818s, 1251.14/s)  LR: 9.976e-04  Data: 0.009 (0.021)
Train: 19 [ 250/1251 ( 20%)]  Loss: 4.660 (4.28)  Time: 0.800s, 1280.25/s  (0.817s, 1253.80/s)  LR: 9.976e-04  Data: 0.016 (0.019)
Train: 19 [ 300/1251 ( 24%)]  Loss: 4.391 (4.29)  Time: 0.822s, 1245.06/s  (0.815s, 1255.97/s)  LR: 9.976e-04  Data: 0.009 (0.017)
Train: 19 [ 350/1251 ( 28%)]  Loss: 4.342 (4.30)  Time: 0.812s, 1260.74/s  (0.814s, 1258.46/s)  LR: 9.976e-04  Data: 0.010 (0.016)
Train: 19 [ 400/1251 ( 32%)]  Loss: 4.263 (4.29)  Time: 0.845s, 1212.34/s  (0.814s, 1257.93/s)  LR: 9.976e-04  Data: 0.015 (0.016)
Train: 19 [ 450/1251 ( 36%)]  Loss: 4.259 (4.29)  Time: 0.828s, 1236.69/s  (0.813s, 1258.93/s)  LR: 9.976e-04  Data: 0.009 (0.015)
Train: 19 [ 500/1251 ( 40%)]  Loss: 4.473 (4.31)  Time: 0.783s, 1308.01/s  (0.813s, 1259.84/s)  LR: 9.976e-04  Data: 0.009 (0.015)
Train: 19 [ 550/1251 ( 44%)]  Loss: 4.007 (4.28)  Time: 0.845s, 1212.14/s  (0.813s, 1259.60/s)  LR: 9.976e-04  Data: 0.010 (0.014)
Train: 19 [ 600/1251 ( 48%)]  Loss: 3.791 (4.25)  Time: 0.870s, 1177.30/s  (0.813s, 1259.67/s)  LR: 9.976e-04  Data: 0.009 (0.014)
Train: 19 [ 650/1251 ( 52%)]  Loss: 4.352 (4.25)  Time: 0.804s, 1273.30/s  (0.813s, 1259.72/s)  LR: 9.976e-04  Data: 0.009 (0.014)
Train: 19 [ 700/1251 ( 56%)]  Loss: 4.015 (4.24)  Time: 0.863s, 1186.20/s  (0.813s, 1260.30/s)  LR: 9.976e-04  Data: 0.013 (0.013)
Train: 19 [ 750/1251 ( 60%)]  Loss: 3.975 (4.22)  Time: 0.817s, 1252.86/s  (0.812s, 1261.10/s)  LR: 9.976e-04  Data: 0.013 (0.013)
Train: 19 [ 800/1251 ( 64%)]  Loss: 4.224 (4.22)  Time: 0.792s, 1292.18/s  (0.811s, 1262.01/s)  LR: 9.976e-04  Data: 0.010 (0.013)
Train: 19 [ 850/1251 ( 68%)]  Loss: 4.220 (4.22)  Time: 0.779s, 1315.24/s  (0.811s, 1262.01/s)  LR: 9.976e-04  Data: 0.010 (0.013)
Train: 19 [ 900/1251 ( 72%)]  Loss: 4.204 (4.22)  Time: 0.780s, 1313.09/s  (0.812s, 1261.75/s)  LR: 9.976e-04  Data: 0.010 (0.013)
Train: 19 [ 950/1251 ( 76%)]  Loss: 4.403 (4.23)  Time: 0.783s, 1307.31/s  (0.811s, 1262.08/s)  LR: 9.976e-04  Data: 0.013 (0.013)
Train: 19 [1000/1251 ( 80%)]  Loss: 4.237 (4.23)  Time: 0.831s, 1232.76/s  (0.811s, 1262.62/s)  LR: 9.976e-04  Data: 0.009 (0.013)
Train: 19 [1050/1251 ( 84%)]  Loss: 4.359 (4.24)  Time: 0.785s, 1303.95/s  (0.811s, 1263.16/s)  LR: 9.976e-04  Data: 0.010 (0.012)
Train: 19 [1100/1251 ( 88%)]  Loss: 4.421 (4.24)  Time: 0.868s, 1180.38/s  (0.811s, 1263.27/s)  LR: 9.976e-04  Data: 0.009 (0.012)
Train: 19 [1150/1251 ( 92%)]  Loss: 4.204 (4.24)  Time: 0.832s, 1230.50/s  (0.811s, 1263.41/s)  LR: 9.976e-04  Data: 0.009 (0.012)
Train: 19 [1200/1251 ( 96%)]  Loss: 4.108 (4.24)  Time: 0.788s, 1298.90/s  (0.810s, 1263.54/s)  LR: 9.976e-04  Data: 0.017 (0.012)
Train: 19 [1250/1251 (100%)]  Loss: 4.228 (4.24)  Time: 0.796s, 1286.66/s  (0.810s, 1263.62/s)  LR: 9.976e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.661 (1.661)  Loss:  1.1592 (1.1592)  Acc@1: 81.9336 (81.9336)  Acc@5: 95.2148 (95.2148)
Test: [  48/48]  Time: 0.194 (0.603)  Loss:  1.4932 (1.9196)  Acc@1: 76.4151 (62.4920)  Acc@5: 90.8019 (85.2240)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-19.pth.tar', 62.491999968261716)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-18.pth.tar', 61.41799993164062)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-17.pth.tar', 61.30999999755859)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-16.pth.tar', 59.94799998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-15.pth.tar', 59.034000046386716)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-14.pth.tar', 58.58599994384765)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-13.pth.tar', 57.200000112304686)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-12.pth.tar', 55.98399999755859)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-11.pth.tar', 55.00399986083984)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-10.pth.tar', 52.49799998535156)

Train: 20 [   0/1251 (  0%)]  Loss: 4.098 (4.10)  Time: 2.605s,  393.08/s  (2.605s,  393.08/s)  LR: 9.973e-04  Data: 1.880 (1.880)
Train: 20 [  50/1251 (  4%)]  Loss: 4.207 (4.15)  Time: 0.780s, 1313.14/s  (0.853s, 1200.64/s)  LR: 9.973e-04  Data: 0.010 (0.047)
Train: 20 [ 100/1251 (  8%)]  Loss: 4.189 (4.16)  Time: 0.778s, 1315.85/s  (0.831s, 1232.21/s)  LR: 9.973e-04  Data: 0.010 (0.029)
Train: 20 [ 150/1251 ( 12%)]  Loss: 4.080 (4.14)  Time: 0.777s, 1318.14/s  (0.824s, 1243.25/s)  LR: 9.973e-04  Data: 0.009 (0.023)
Train: 20 [ 200/1251 ( 16%)]  Loss: 4.542 (4.22)  Time: 0.812s, 1261.27/s  (0.820s, 1248.21/s)  LR: 9.973e-04  Data: 0.009 (0.020)
Train: 20 [ 250/1251 ( 20%)]  Loss: 4.654 (4.29)  Time: 0.788s, 1299.66/s  (0.818s, 1251.75/s)  LR: 9.973e-04  Data: 0.010 (0.018)
Train: 20 [ 300/1251 ( 24%)]  Loss: 3.971 (4.25)  Time: 0.796s, 1286.09/s  (0.817s, 1253.95/s)  LR: 9.973e-04  Data: 0.010 (0.017)
Train: 20 [ 350/1251 ( 28%)]  Loss: 3.943 (4.21)  Time: 0.803s, 1275.51/s  (0.816s, 1254.63/s)  LR: 9.973e-04  Data: 0.010 (0.016)
Train: 20 [ 400/1251 ( 32%)]  Loss: 4.360 (4.23)  Time: 0.827s, 1238.39/s  (0.815s, 1256.47/s)  LR: 9.973e-04  Data: 0.013 (0.015)
Train: 20 [ 450/1251 ( 36%)]  Loss: 3.941 (4.20)  Time: 0.853s, 1200.00/s  (0.815s, 1257.10/s)  LR: 9.973e-04  Data: 0.010 (0.015)
Train: 20 [ 500/1251 ( 40%)]  Loss: 4.268 (4.20)  Time: 0.838s, 1222.36/s  (0.814s, 1257.80/s)  LR: 9.973e-04  Data: 0.010 (0.014)
Train: 20 [ 550/1251 ( 44%)]  Loss: 4.002 (4.19)  Time: 0.792s, 1293.28/s  (0.814s, 1257.98/s)  LR: 9.973e-04  Data: 0.010 (0.014)
Train: 20 [ 600/1251 ( 48%)]  Loss: 4.108 (4.18)  Time: 0.782s, 1309.30/s  (0.814s, 1258.45/s)  LR: 9.973e-04  Data: 0.010 (0.014)
Train: 20 [ 650/1251 ( 52%)]  Loss: 4.260 (4.19)  Time: 0.879s, 1164.71/s  (0.814s, 1258.55/s)  LR: 9.973e-04  Data: 0.010 (0.013)
Train: 20 [ 700/1251 ( 56%)]  Loss: 3.973 (4.17)  Time: 0.834s, 1228.02/s  (0.814s, 1258.69/s)  LR: 9.973e-04  Data: 0.010 (0.013)
Train: 20 [ 750/1251 ( 60%)]  Loss: 3.756 (4.15)  Time: 0.808s, 1267.56/s  (0.814s, 1258.67/s)  LR: 9.973e-04  Data: 0.009 (0.013)
Train: 20 [ 800/1251 ( 64%)]  Loss: 3.884 (4.13)  Time: 0.813s, 1259.68/s  (0.813s, 1259.34/s)  LR: 9.973e-04  Data: 0.009 (0.013)
Train: 20 [ 850/1251 ( 68%)]  Loss: 4.086 (4.13)  Time: 0.781s, 1310.33/s  (0.813s, 1259.98/s)  LR: 9.973e-04  Data: 0.010 (0.013)
Train: 20 [ 900/1251 ( 72%)]  Loss: 4.140 (4.13)  Time: 0.779s, 1314.95/s  (0.812s, 1260.82/s)  LR: 9.973e-04  Data: 0.010 (0.013)
Train: 20 [ 950/1251 ( 76%)]  Loss: 4.423 (4.14)  Time: 0.800s, 1280.06/s  (0.812s, 1260.79/s)  LR: 9.973e-04  Data: 0.013 (0.013)
Train: 20 [1000/1251 ( 80%)]  Loss: 4.074 (4.14)  Time: 0.824s, 1242.07/s  (0.812s, 1260.91/s)  LR: 9.973e-04  Data: 0.010 (0.012)
Train: 20 [1050/1251 ( 84%)]  Loss: 4.595 (4.16)  Time: 0.821s, 1246.81/s  (0.812s, 1260.80/s)  LR: 9.973e-04  Data: 0.010 (0.012)
Train: 20 [1100/1251 ( 88%)]  Loss: 4.517 (4.18)  Time: 0.778s, 1316.33/s  (0.812s, 1260.55/s)  LR: 9.973e-04  Data: 0.010 (0.012)
Train: 20 [1150/1251 ( 92%)]  Loss: 4.720 (4.20)  Time: 0.821s, 1247.37/s  (0.812s, 1260.72/s)  LR: 9.973e-04  Data: 0.009 (0.012)
Train: 20 [1200/1251 ( 96%)]  Loss: 4.331 (4.20)  Time: 0.809s, 1266.36/s  (0.812s, 1261.09/s)  LR: 9.973e-04  Data: 0.010 (0.012)
Train: 20 [1250/1251 (100%)]  Loss: 4.097 (4.20)  Time: 0.781s, 1311.58/s  (0.812s, 1261.35/s)  LR: 9.973e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.682 (1.682)  Loss:  1.1855 (1.1855)  Acc@1: 81.9336 (81.9336)  Acc@5: 94.6289 (94.6289)
Test: [  48/48]  Time: 0.194 (0.607)  Loss:  1.2939 (1.9260)  Acc@1: 77.8302 (62.2920)  Acc@5: 93.0425 (85.5760)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-19.pth.tar', 62.491999968261716)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-20.pth.tar', 62.29199993652344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-18.pth.tar', 61.41799993164062)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-17.pth.tar', 61.30999999755859)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-16.pth.tar', 59.94799998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-15.pth.tar', 59.034000046386716)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-14.pth.tar', 58.58599994384765)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-13.pth.tar', 57.200000112304686)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-12.pth.tar', 55.98399999755859)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-11.pth.tar', 55.00399986083984)

Train: 21 [   0/1251 (  0%)]  Loss: 3.624 (3.62)  Time: 2.378s,  430.67/s  (2.378s,  430.67/s)  LR: 9.970e-04  Data: 1.585 (1.585)
Train: 21 [  50/1251 (  4%)]  Loss: 4.298 (3.96)  Time: 0.805s, 1272.25/s  (0.842s, 1216.35/s)  LR: 9.970e-04  Data: 0.009 (0.048)
Train: 21 [ 100/1251 (  8%)]  Loss: 4.294 (4.07)  Time: 0.809s, 1265.20/s  (0.824s, 1242.91/s)  LR: 9.970e-04  Data: 0.009 (0.029)
Train: 21 [ 150/1251 ( 12%)]  Loss: 4.363 (4.14)  Time: 0.834s, 1227.77/s  (0.820s, 1249.21/s)  LR: 9.970e-04  Data: 0.010 (0.023)
Train: 21 [ 200/1251 ( 16%)]  Loss: 3.764 (4.07)  Time: 0.862s, 1188.53/s  (0.817s, 1252.62/s)  LR: 9.970e-04  Data: 0.010 (0.020)
Train: 21 [ 250/1251 ( 20%)]  Loss: 4.213 (4.09)  Time: 0.785s, 1304.84/s  (0.816s, 1254.48/s)  LR: 9.970e-04  Data: 0.009 (0.018)
Train: 21 [ 300/1251 ( 24%)]  Loss: 3.896 (4.06)  Time: 0.786s, 1303.15/s  (0.815s, 1256.32/s)  LR: 9.970e-04  Data: 0.009 (0.017)
Train: 21 [ 350/1251 ( 28%)]  Loss: 4.157 (4.08)  Time: 0.782s, 1310.02/s  (0.814s, 1258.00/s)  LR: 9.970e-04  Data: 0.012 (0.016)
Train: 21 [ 400/1251 ( 32%)]  Loss: 4.042 (4.07)  Time: 0.795s, 1288.12/s  (0.813s, 1259.99/s)  LR: 9.970e-04  Data: 0.014 (0.016)
Train: 21 [ 450/1251 ( 36%)]  Loss: 4.218 (4.09)  Time: 0.872s, 1173.71/s  (0.812s, 1260.87/s)  LR: 9.970e-04  Data: 0.009 (0.015)
Train: 21 [ 500/1251 ( 40%)]  Loss: 4.083 (4.09)  Time: 0.849s, 1205.46/s  (0.812s, 1261.17/s)  LR: 9.970e-04  Data: 0.010 (0.015)
Train: 21 [ 550/1251 ( 44%)]  Loss: 4.030 (4.08)  Time: 0.789s, 1297.82/s  (0.812s, 1260.81/s)  LR: 9.970e-04  Data: 0.009 (0.014)
Train: 21 [ 600/1251 ( 48%)]  Loss: 4.136 (4.09)  Time: 0.855s, 1197.99/s  (0.812s, 1261.04/s)  LR: 9.970e-04  Data: 0.010 (0.014)
Train: 21 [ 650/1251 ( 52%)]  Loss: 4.082 (4.09)  Time: 0.785s, 1304.64/s  (0.812s, 1261.16/s)  LR: 9.970e-04  Data: 0.013 (0.014)
Train: 21 [ 700/1251 ( 56%)]  Loss: 4.254 (4.10)  Time: 0.792s, 1292.91/s  (0.811s, 1261.97/s)  LR: 9.970e-04  Data: 0.015 (0.013)
Train: 21 [ 750/1251 ( 60%)]  Loss: 4.662 (4.13)  Time: 0.805s, 1271.79/s  (0.811s, 1262.61/s)  LR: 9.970e-04  Data: 0.010 (0.013)
Train: 21 [ 800/1251 ( 64%)]  Loss: 4.402 (4.15)  Time: 0.807s, 1269.15/s  (0.811s, 1262.97/s)  LR: 9.970e-04  Data: 0.009 (0.013)
Train: 21 [ 850/1251 ( 68%)]  Loss: 4.353 (4.16)  Time: 0.814s, 1258.28/s  (0.811s, 1263.24/s)  LR: 9.970e-04  Data: 0.009 (0.013)
Train: 21 [ 900/1251 ( 72%)]  Loss: 4.407 (4.17)  Time: 0.881s, 1162.57/s  (0.811s, 1262.83/s)  LR: 9.970e-04  Data: 0.010 (0.013)
Train: 21 [ 950/1251 ( 76%)]  Loss: 4.478 (4.19)  Time: 0.802s, 1277.32/s  (0.811s, 1262.54/s)  LR: 9.970e-04  Data: 0.009 (0.013)
Train: 21 [1000/1251 ( 80%)]  Loss: 3.919 (4.17)  Time: 0.849s, 1205.57/s  (0.811s, 1262.55/s)  LR: 9.970e-04  Data: 0.009 (0.013)
Train: 21 [1050/1251 ( 84%)]  Loss: 4.194 (4.18)  Time: 0.785s, 1304.87/s  (0.811s, 1263.05/s)  LR: 9.970e-04  Data: 0.010 (0.013)
Train: 21 [1100/1251 ( 88%)]  Loss: 4.178 (4.18)  Time: 0.839s, 1220.54/s  (0.811s, 1263.24/s)  LR: 9.970e-04  Data: 0.009 (0.012)
Train: 21 [1150/1251 ( 92%)]  Loss: 4.518 (4.19)  Time: 0.780s, 1312.61/s  (0.810s, 1263.94/s)  LR: 9.970e-04  Data: 0.010 (0.012)
Train: 21 [1200/1251 ( 96%)]  Loss: 4.281 (4.19)  Time: 0.821s, 1247.08/s  (0.811s, 1263.21/s)  LR: 9.970e-04  Data: 0.012 (0.012)
Train: 21 [1250/1251 (100%)]  Loss: 4.210 (4.19)  Time: 0.771s, 1327.58/s  (0.811s, 1263.07/s)  LR: 9.970e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.625 (1.625)  Loss:  1.2061 (1.2061)  Acc@1: 80.9570 (80.9570)  Acc@5: 95.4102 (95.4102)
Test: [  48/48]  Time: 0.193 (0.566)  Loss:  1.3555 (1.8571)  Acc@1: 77.8302 (62.8560)  Acc@5: 91.6274 (85.5440)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-21.pth.tar', 62.85600006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-19.pth.tar', 62.491999968261716)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-20.pth.tar', 62.29199993652344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-18.pth.tar', 61.41799993164062)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-17.pth.tar', 61.30999999755859)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-16.pth.tar', 59.94799998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-15.pth.tar', 59.034000046386716)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-14.pth.tar', 58.58599994384765)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-13.pth.tar', 57.200000112304686)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-12.pth.tar', 55.98399999755859)

Train: 22 [   0/1251 (  0%)]  Loss: 4.395 (4.39)  Time: 2.518s,  406.72/s  (2.518s,  406.72/s)  LR: 9.967e-04  Data: 1.795 (1.795)
Train: 22 [  50/1251 (  4%)]  Loss: 4.114 (4.25)  Time: 0.778s, 1315.49/s  (0.842s, 1216.78/s)  LR: 9.967e-04  Data: 0.010 (0.049)
Train: 22 [ 100/1251 (  8%)]  Loss: 4.116 (4.21)  Time: 0.847s, 1209.15/s  (0.827s, 1238.17/s)  LR: 9.967e-04  Data: 0.009 (0.030)
Train: 22 [ 150/1251 ( 12%)]  Loss: 4.031 (4.16)  Time: 0.820s, 1249.27/s  (0.821s, 1247.66/s)  LR: 9.967e-04  Data: 0.010 (0.024)
Train: 22 [ 200/1251 ( 16%)]  Loss: 4.176 (4.17)  Time: 0.771s, 1328.73/s  (0.817s, 1253.30/s)  LR: 9.967e-04  Data: 0.009 (0.020)
Train: 22 [ 250/1251 ( 20%)]  Loss: 4.372 (4.20)  Time: 0.778s, 1316.17/s  (0.816s, 1254.18/s)  LR: 9.967e-04  Data: 0.010 (0.018)
Train: 22 [ 300/1251 ( 24%)]  Loss: 4.283 (4.21)  Time: 0.809s, 1265.82/s  (0.815s, 1257.12/s)  LR: 9.967e-04  Data: 0.010 (0.017)
Train: 22 [ 350/1251 ( 28%)]  Loss: 4.265 (4.22)  Time: 0.779s, 1314.24/s  (0.814s, 1257.54/s)  LR: 9.967e-04  Data: 0.010 (0.016)
Train: 22 [ 400/1251 ( 32%)]  Loss: 4.048 (4.20)  Time: 0.843s, 1214.87/s  (0.814s, 1258.38/s)  LR: 9.967e-04  Data: 0.010 (0.016)
Train: 22 [ 450/1251 ( 36%)]  Loss: 4.262 (4.21)  Time: 0.775s, 1321.69/s  (0.813s, 1259.74/s)  LR: 9.967e-04  Data: 0.010 (0.015)
Train: 22 [ 500/1251 ( 40%)]  Loss: 3.915 (4.18)  Time: 0.793s, 1292.01/s  (0.813s, 1259.97/s)  LR: 9.967e-04  Data: 0.010 (0.015)
Train: 22 [ 550/1251 ( 44%)]  Loss: 4.505 (4.21)  Time: 0.792s, 1292.23/s  (0.812s, 1260.80/s)  LR: 9.967e-04  Data: 0.014 (0.014)
Train: 22 [ 600/1251 ( 48%)]  Loss: 4.390 (4.22)  Time: 0.810s, 1264.02/s  (0.812s, 1261.55/s)  LR: 9.967e-04  Data: 0.010 (0.014)
Train: 22 [ 650/1251 ( 52%)]  Loss: 4.141 (4.22)  Time: 0.776s, 1319.66/s  (0.812s, 1261.36/s)  LR: 9.967e-04  Data: 0.009 (0.014)
Train: 22 [ 700/1251 ( 56%)]  Loss: 4.206 (4.21)  Time: 0.826s, 1239.39/s  (0.812s, 1261.42/s)  LR: 9.967e-04  Data: 0.010 (0.013)
Train: 22 [ 750/1251 ( 60%)]  Loss: 4.464 (4.23)  Time: 0.790s, 1297.00/s  (0.812s, 1261.61/s)  LR: 9.967e-04  Data: 0.016 (0.013)
Train: 22 [ 800/1251 ( 64%)]  Loss: 4.095 (4.22)  Time: 0.834s, 1228.38/s  (0.811s, 1262.07/s)  LR: 9.967e-04  Data: 0.010 (0.013)
Train: 22 [ 850/1251 ( 68%)]  Loss: 3.952 (4.21)  Time: 0.817s, 1252.87/s  (0.811s, 1263.17/s)  LR: 9.967e-04  Data: 0.011 (0.013)
Train: 22 [ 900/1251 ( 72%)]  Loss: 4.204 (4.21)  Time: 0.790s, 1296.95/s  (0.811s, 1262.85/s)  LR: 9.967e-04  Data: 0.012 (0.013)
Train: 22 [ 950/1251 ( 76%)]  Loss: 4.381 (4.22)  Time: 0.781s, 1311.90/s  (0.810s, 1263.86/s)  LR: 9.967e-04  Data: 0.009 (0.013)
Train: 22 [1000/1251 ( 80%)]  Loss: 3.978 (4.20)  Time: 0.808s, 1267.64/s  (0.810s, 1264.56/s)  LR: 9.967e-04  Data: 0.009 (0.013)
Train: 22 [1050/1251 ( 84%)]  Loss: 4.589 (4.22)  Time: 0.833s, 1228.72/s  (0.810s, 1264.22/s)  LR: 9.967e-04  Data: 0.010 (0.012)
Train: 22 [1100/1251 ( 88%)]  Loss: 4.435 (4.23)  Time: 0.792s, 1293.05/s  (0.810s, 1264.27/s)  LR: 9.967e-04  Data: 0.010 (0.012)
Train: 22 [1150/1251 ( 92%)]  Loss: 4.131 (4.23)  Time: 0.776s, 1319.87/s  (0.810s, 1264.63/s)  LR: 9.967e-04  Data: 0.011 (0.012)
Train: 22 [1200/1251 ( 96%)]  Loss: 4.400 (4.23)  Time: 0.791s, 1294.25/s  (0.810s, 1264.68/s)  LR: 9.967e-04  Data: 0.011 (0.012)
Train: 22 [1250/1251 (100%)]  Loss: 3.788 (4.22)  Time: 0.789s, 1297.17/s  (0.810s, 1264.67/s)  LR: 9.967e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.721 (1.721)  Loss:  1.2549 (1.2549)  Acc@1: 80.2734 (80.2734)  Acc@5: 94.2383 (94.2383)
Test: [  48/48]  Time: 0.194 (0.616)  Loss:  1.1445 (1.7818)  Acc@1: 79.5991 (63.9560)  Acc@5: 93.9859 (86.2120)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-22.pth.tar', 63.95600005859375)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-21.pth.tar', 62.85600006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-19.pth.tar', 62.491999968261716)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-20.pth.tar', 62.29199993652344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-18.pth.tar', 61.41799993164062)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-17.pth.tar', 61.30999999755859)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-16.pth.tar', 59.94799998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-15.pth.tar', 59.034000046386716)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-14.pth.tar', 58.58599994384765)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-13.pth.tar', 57.200000112304686)

Train: 23 [   0/1251 (  0%)]  Loss: 4.289 (4.29)  Time: 2.330s,  439.53/s  (2.330s,  439.53/s)  LR: 9.964e-04  Data: 1.598 (1.598)
Train: 23 [  50/1251 (  4%)]  Loss: 4.149 (4.22)  Time: 0.788s, 1299.03/s  (0.844s, 1213.36/s)  LR: 9.964e-04  Data: 0.010 (0.047)
Train: 23 [ 100/1251 (  8%)]  Loss: 4.302 (4.25)  Time: 0.830s, 1233.70/s  (0.828s, 1236.42/s)  LR: 9.964e-04  Data: 0.016 (0.029)
Train: 23 [ 150/1251 ( 12%)]  Loss: 3.959 (4.17)  Time: 0.846s, 1210.76/s  (0.823s, 1244.83/s)  LR: 9.964e-04  Data: 0.010 (0.023)
Train: 23 [ 200/1251 ( 16%)]  Loss: 4.520 (4.24)  Time: 0.804s, 1273.98/s  (0.818s, 1251.28/s)  LR: 9.964e-04  Data: 0.012 (0.020)
Train: 23 [ 250/1251 ( 20%)]  Loss: 4.257 (4.25)  Time: 0.797s, 1284.51/s  (0.817s, 1252.86/s)  LR: 9.964e-04  Data: 0.010 (0.018)
Train: 23 [ 300/1251 ( 24%)]  Loss: 4.210 (4.24)  Time: 0.796s, 1286.59/s  (0.816s, 1255.23/s)  LR: 9.964e-04  Data: 0.010 (0.017)
Train: 23 [ 350/1251 ( 28%)]  Loss: 4.125 (4.23)  Time: 0.838s, 1221.76/s  (0.816s, 1254.68/s)  LR: 9.964e-04  Data: 0.009 (0.016)
Train: 23 [ 400/1251 ( 32%)]  Loss: 3.951 (4.20)  Time: 0.802s, 1276.62/s  (0.815s, 1256.44/s)  LR: 9.964e-04  Data: 0.010 (0.015)
Train: 23 [ 450/1251 ( 36%)]  Loss: 4.186 (4.19)  Time: 0.792s, 1293.02/s  (0.814s, 1257.86/s)  LR: 9.964e-04  Data: 0.019 (0.015)
Train: 23 [ 500/1251 ( 40%)]  Loss: 4.095 (4.19)  Time: 0.806s, 1270.77/s  (0.814s, 1258.26/s)  LR: 9.964e-04  Data: 0.010 (0.014)
Train: 23 [ 550/1251 ( 44%)]  Loss: 4.243 (4.19)  Time: 0.796s, 1286.85/s  (0.813s, 1259.11/s)  LR: 9.964e-04  Data: 0.009 (0.014)
Train: 23 [ 600/1251 ( 48%)]  Loss: 4.394 (4.21)  Time: 0.782s, 1310.23/s  (0.813s, 1259.46/s)  LR: 9.964e-04  Data: 0.010 (0.014)
Train: 23 [ 650/1251 ( 52%)]  Loss: 3.989 (4.19)  Time: 0.808s, 1266.76/s  (0.813s, 1259.88/s)  LR: 9.964e-04  Data: 0.010 (0.014)
Train: 23 [ 700/1251 ( 56%)]  Loss: 3.839 (4.17)  Time: 0.792s, 1293.66/s  (0.812s, 1260.89/s)  LR: 9.964e-04  Data: 0.009 (0.013)
Train: 23 [ 750/1251 ( 60%)]  Loss: 3.888 (4.15)  Time: 0.772s, 1326.45/s  (0.812s, 1261.44/s)  LR: 9.964e-04  Data: 0.010 (0.013)
Train: 23 [ 800/1251 ( 64%)]  Loss: 4.276 (4.16)  Time: 0.780s, 1313.42/s  (0.812s, 1261.63/s)  LR: 9.964e-04  Data: 0.010 (0.013)
Train: 23 [ 850/1251 ( 68%)]  Loss: 4.319 (4.17)  Time: 0.786s, 1302.43/s  (0.812s, 1261.82/s)  LR: 9.964e-04  Data: 0.015 (0.013)
Train: 23 [ 900/1251 ( 72%)]  Loss: 3.697 (4.14)  Time: 0.801s, 1278.88/s  (0.811s, 1262.44/s)  LR: 9.964e-04  Data: 0.010 (0.013)
Train: 23 [ 950/1251 ( 76%)]  Loss: 4.163 (4.14)  Time: 0.799s, 1281.59/s  (0.811s, 1262.32/s)  LR: 9.964e-04  Data: 0.009 (0.013)
Train: 23 [1000/1251 ( 80%)]  Loss: 3.960 (4.13)  Time: 0.814s, 1257.93/s  (0.811s, 1262.80/s)  LR: 9.964e-04  Data: 0.009 (0.013)
Train: 23 [1050/1251 ( 84%)]  Loss: 4.071 (4.13)  Time: 0.803s, 1274.71/s  (0.811s, 1263.20/s)  LR: 9.964e-04  Data: 0.010 (0.012)
Train: 23 [1100/1251 ( 88%)]  Loss: 4.179 (4.13)  Time: 0.779s, 1314.48/s  (0.811s, 1263.15/s)  LR: 9.964e-04  Data: 0.010 (0.012)
Train: 23 [1150/1251 ( 92%)]  Loss: 4.040 (4.13)  Time: 0.794s, 1289.05/s  (0.811s, 1263.15/s)  LR: 9.964e-04  Data: 0.014 (0.012)
Train: 23 [1200/1251 ( 96%)]  Loss: 4.095 (4.13)  Time: 0.778s, 1316.99/s  (0.810s, 1263.54/s)  LR: 9.964e-04  Data: 0.009 (0.012)
Train: 23 [1250/1251 (100%)]  Loss: 4.164 (4.13)  Time: 0.838s, 1221.40/s  (0.811s, 1263.24/s)  LR: 9.964e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.654 (1.654)  Loss:  1.0547 (1.0547)  Acc@1: 82.8125 (82.8125)  Acc@5: 96.0938 (96.0938)
Test: [  48/48]  Time: 0.194 (0.609)  Loss:  1.0801 (1.7861)  Acc@1: 79.7170 (63.5920)  Acc@5: 94.5755 (86.3400)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-22.pth.tar', 63.95600005859375)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-23.pth.tar', 63.59199998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-21.pth.tar', 62.85600006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-19.pth.tar', 62.491999968261716)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-20.pth.tar', 62.29199993652344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-18.pth.tar', 61.41799993164062)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-17.pth.tar', 61.30999999755859)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-16.pth.tar', 59.94799998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-15.pth.tar', 59.034000046386716)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-14.pth.tar', 58.58599994384765)

Train: 24 [   0/1251 (  0%)]  Loss: 4.284 (4.28)  Time: 2.269s,  451.33/s  (2.269s,  451.33/s)  LR: 9.961e-04  Data: 1.536 (1.536)
Train: 24 [  50/1251 (  4%)]  Loss: 3.907 (4.10)  Time: 0.778s, 1316.42/s  (0.846s, 1209.91/s)  LR: 9.961e-04  Data: 0.010 (0.046)
Train: 24 [ 100/1251 (  8%)]  Loss: 4.451 (4.21)  Time: 0.820s, 1248.79/s  (0.828s, 1236.30/s)  LR: 9.961e-04  Data: 0.010 (0.029)
Train: 24 [ 150/1251 ( 12%)]  Loss: 3.732 (4.09)  Time: 0.819s, 1250.78/s  (0.821s, 1247.78/s)  LR: 9.961e-04  Data: 0.009 (0.023)
Train: 24 [ 200/1251 ( 16%)]  Loss: 4.302 (4.14)  Time: 0.778s, 1316.28/s  (0.816s, 1254.85/s)  LR: 9.961e-04  Data: 0.011 (0.020)
Train: 24 [ 250/1251 ( 20%)]  Loss: 3.840 (4.09)  Time: 0.820s, 1248.93/s  (0.815s, 1257.09/s)  LR: 9.961e-04  Data: 0.009 (0.018)
Train: 24 [ 300/1251 ( 24%)]  Loss: 3.806 (4.05)  Time: 0.806s, 1269.72/s  (0.815s, 1256.79/s)  LR: 9.961e-04  Data: 0.013 (0.017)
Train: 24 [ 350/1251 ( 28%)]  Loss: 4.368 (4.09)  Time: 0.785s, 1303.91/s  (0.814s, 1257.59/s)  LR: 9.961e-04  Data: 0.010 (0.016)
Train: 24 [ 400/1251 ( 32%)]  Loss: 3.723 (4.05)  Time: 0.817s, 1253.71/s  (0.814s, 1257.75/s)  LR: 9.961e-04  Data: 0.010 (0.015)
Train: 24 [ 450/1251 ( 36%)]  Loss: 3.915 (4.03)  Time: 0.777s, 1318.55/s  (0.813s, 1258.92/s)  LR: 9.961e-04  Data: 0.010 (0.015)
Train: 24 [ 500/1251 ( 40%)]  Loss: 3.622 (4.00)  Time: 0.859s, 1192.55/s  (0.813s, 1259.39/s)  LR: 9.961e-04  Data: 0.011 (0.014)
Train: 24 [ 550/1251 ( 44%)]  Loss: 3.638 (3.97)  Time: 0.793s, 1291.71/s  (0.813s, 1260.28/s)  LR: 9.961e-04  Data: 0.014 (0.014)
Train: 24 [ 600/1251 ( 48%)]  Loss: 4.246 (3.99)  Time: 0.797s, 1285.21/s  (0.812s, 1261.66/s)  LR: 9.961e-04  Data: 0.009 (0.013)
Train: 24 [ 650/1251 ( 52%)]  Loss: 3.964 (3.99)  Time: 0.781s, 1310.93/s  (0.811s, 1262.51/s)  LR: 9.961e-04  Data: 0.010 (0.013)
Train: 24 [ 700/1251 ( 56%)]  Loss: 4.235 (4.00)  Time: 0.798s, 1282.51/s  (0.811s, 1262.56/s)  LR: 9.961e-04  Data: 0.010 (0.013)
Train: 24 [ 750/1251 ( 60%)]  Loss: 4.199 (4.01)  Time: 0.787s, 1300.44/s  (0.811s, 1262.52/s)  LR: 9.961e-04  Data: 0.010 (0.013)
Train: 24 [ 800/1251 ( 64%)]  Loss: 4.046 (4.02)  Time: 0.848s, 1207.44/s  (0.811s, 1263.09/s)  LR: 9.961e-04  Data: 0.009 (0.013)
Train: 24 [ 850/1251 ( 68%)]  Loss: 3.912 (4.01)  Time: 0.835s, 1226.64/s  (0.810s, 1263.66/s)  LR: 9.961e-04  Data: 0.010 (0.013)
Train: 24 [ 900/1251 ( 72%)]  Loss: 4.278 (4.02)  Time: 0.830s, 1233.02/s  (0.810s, 1263.55/s)  LR: 9.961e-04  Data: 0.010 (0.012)
Train: 24 [ 950/1251 ( 76%)]  Loss: 4.575 (4.05)  Time: 0.856s, 1196.91/s  (0.810s, 1263.85/s)  LR: 9.961e-04  Data: 0.013 (0.012)
Train: 24 [1000/1251 ( 80%)]  Loss: 4.026 (4.05)  Time: 0.772s, 1326.69/s  (0.810s, 1263.95/s)  LR: 9.961e-04  Data: 0.013 (0.012)
Train: 24 [1050/1251 ( 84%)]  Loss: 4.017 (4.05)  Time: 0.812s, 1260.80/s  (0.810s, 1264.06/s)  LR: 9.961e-04  Data: 0.011 (0.012)
Train: 24 [1100/1251 ( 88%)]  Loss: 3.712 (4.03)  Time: 0.794s, 1289.24/s  (0.810s, 1264.23/s)  LR: 9.961e-04  Data: 0.010 (0.012)
Train: 24 [1150/1251 ( 92%)]  Loss: 3.848 (4.03)  Time: 0.825s, 1240.97/s  (0.810s, 1264.23/s)  LR: 9.961e-04  Data: 0.010 (0.012)
Train: 24 [1200/1251 ( 96%)]  Loss: 4.163 (4.03)  Time: 0.780s, 1312.95/s  (0.810s, 1264.51/s)  LR: 9.961e-04  Data: 0.010 (0.012)
Train: 24 [1250/1251 (100%)]  Loss: 4.110 (4.04)  Time: 0.770s, 1329.53/s  (0.810s, 1264.26/s)  LR: 9.961e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.705 (1.705)  Loss:  0.9507 (0.9507)  Acc@1: 82.7148 (82.7148)  Acc@5: 95.1172 (95.1172)
Test: [  48/48]  Time: 0.194 (0.616)  Loss:  1.1396 (1.7672)  Acc@1: 80.5424 (64.2220)  Acc@5: 93.0425 (86.4580)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-24.pth.tar', 64.22199995117188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-22.pth.tar', 63.95600005859375)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-23.pth.tar', 63.59199998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-21.pth.tar', 62.85600006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-19.pth.tar', 62.491999968261716)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-20.pth.tar', 62.29199993652344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-18.pth.tar', 61.41799993164062)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-17.pth.tar', 61.30999999755859)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-16.pth.tar', 59.94799998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-15.pth.tar', 59.034000046386716)

Train: 25 [   0/1251 (  0%)]  Loss: 3.963 (3.96)  Time: 2.261s,  452.97/s  (2.261s,  452.97/s)  LR: 9.958e-04  Data: 1.535 (1.535)
Train: 25 [  50/1251 (  4%)]  Loss: 4.176 (4.07)  Time: 0.810s, 1263.83/s  (0.840s, 1219.06/s)  LR: 9.958e-04  Data: 0.015 (0.046)
Train: 25 [ 100/1251 (  8%)]  Loss: 4.107 (4.08)  Time: 0.868s, 1180.32/s  (0.824s, 1242.48/s)  LR: 9.958e-04  Data: 0.009 (0.028)
Train: 25 [ 150/1251 ( 12%)]  Loss: 4.119 (4.09)  Time: 0.780s, 1313.37/s  (0.819s, 1250.39/s)  LR: 9.958e-04  Data: 0.009 (0.023)
Train: 25 [ 200/1251 ( 16%)]  Loss: 4.076 (4.09)  Time: 0.814s, 1258.44/s  (0.818s, 1251.60/s)  LR: 9.958e-04  Data: 0.014 (0.020)
Train: 25 [ 250/1251 ( 20%)]  Loss: 3.725 (4.03)  Time: 0.837s, 1222.69/s  (0.817s, 1253.94/s)  LR: 9.958e-04  Data: 0.010 (0.018)
Train: 25 [ 300/1251 ( 24%)]  Loss: 3.787 (3.99)  Time: 0.779s, 1314.42/s  (0.815s, 1255.99/s)  LR: 9.958e-04  Data: 0.012 (0.017)
Train: 25 [ 350/1251 ( 28%)]  Loss: 3.968 (3.99)  Time: 0.779s, 1314.92/s  (0.815s, 1257.21/s)  LR: 9.958e-04  Data: 0.009 (0.016)
Train: 25 [ 400/1251 ( 32%)]  Loss: 4.181 (4.01)  Time: 0.791s, 1293.95/s  (0.814s, 1258.33/s)  LR: 9.958e-04  Data: 0.010 (0.015)
Train: 25 [ 450/1251 ( 36%)]  Loss: 4.193 (4.03)  Time: 0.851s, 1203.00/s  (0.813s, 1259.81/s)  LR: 9.958e-04  Data: 0.015 (0.015)
Train: 25 [ 500/1251 ( 40%)]  Loss: 4.220 (4.05)  Time: 0.897s, 1141.78/s  (0.812s, 1260.32/s)  LR: 9.958e-04  Data: 0.009 (0.014)
Train: 25 [ 550/1251 ( 44%)]  Loss: 4.200 (4.06)  Time: 0.851s, 1203.96/s  (0.813s, 1259.76/s)  LR: 9.958e-04  Data: 0.010 (0.014)
Train: 25 [ 600/1251 ( 48%)]  Loss: 4.026 (4.06)  Time: 0.782s, 1309.37/s  (0.813s, 1259.46/s)  LR: 9.958e-04  Data: 0.009 (0.014)
Train: 25 [ 650/1251 ( 52%)]  Loss: 3.833 (4.04)  Time: 0.781s, 1310.45/s  (0.813s, 1259.77/s)  LR: 9.958e-04  Data: 0.010 (0.013)
Train: 25 [ 700/1251 ( 56%)]  Loss: 3.983 (4.04)  Time: 0.778s, 1316.19/s  (0.813s, 1259.77/s)  LR: 9.958e-04  Data: 0.010 (0.013)
Train: 25 [ 750/1251 ( 60%)]  Loss: 4.165 (4.05)  Time: 0.814s, 1257.44/s  (0.813s, 1260.00/s)  LR: 9.958e-04  Data: 0.010 (0.013)
Train: 25 [ 800/1251 ( 64%)]  Loss: 4.185 (4.05)  Time: 0.813s, 1260.22/s  (0.812s, 1260.43/s)  LR: 9.958e-04  Data: 0.009 (0.013)
Train: 25 [ 850/1251 ( 68%)]  Loss: 3.819 (4.04)  Time: 0.836s, 1224.32/s  (0.812s, 1260.39/s)  LR: 9.958e-04  Data: 0.009 (0.013)
Train: 25 [ 900/1251 ( 72%)]  Loss: 4.162 (4.05)  Time: 0.986s, 1038.88/s  (0.813s, 1259.94/s)  LR: 9.958e-04  Data: 0.009 (0.013)
Train: 25 [ 950/1251 ( 76%)]  Loss: 3.881 (4.04)  Time: 0.780s, 1313.62/s  (0.813s, 1260.27/s)  LR: 9.958e-04  Data: 0.010 (0.013)
Train: 25 [1000/1251 ( 80%)]  Loss: 4.209 (4.05)  Time: 0.774s, 1323.84/s  (0.812s, 1260.55/s)  LR: 9.958e-04  Data: 0.010 (0.012)
Train: 25 [1050/1251 ( 84%)]  Loss: 4.383 (4.06)  Time: 0.791s, 1293.99/s  (0.812s, 1261.36/s)  LR: 9.958e-04  Data: 0.015 (0.012)
Train: 25 [1100/1251 ( 88%)]  Loss: 4.257 (4.07)  Time: 0.793s, 1291.96/s  (0.812s, 1261.41/s)  LR: 9.958e-04  Data: 0.009 (0.012)
Train: 25 [1150/1251 ( 92%)]  Loss: 4.511 (4.09)  Time: 0.858s, 1193.04/s  (0.812s, 1261.52/s)  LR: 9.958e-04  Data: 0.010 (0.012)
Train: 25 [1200/1251 ( 96%)]  Loss: 4.125 (4.09)  Time: 0.818s, 1252.52/s  (0.812s, 1261.21/s)  LR: 9.958e-04  Data: 0.011 (0.012)
Train: 25 [1250/1251 (100%)]  Loss: 4.089 (4.09)  Time: 0.810s, 1264.87/s  (0.812s, 1261.37/s)  LR: 9.958e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.696 (1.696)  Loss:  1.1621 (1.1621)  Acc@1: 82.3242 (82.3242)  Acc@5: 95.6055 (95.6055)
Test: [  48/48]  Time: 0.194 (0.610)  Loss:  1.1680 (1.7933)  Acc@1: 81.9575 (65.2480)  Acc@5: 94.8113 (87.1480)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-25.pth.tar', 65.2479999194336)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-24.pth.tar', 64.22199995117188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-22.pth.tar', 63.95600005859375)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-23.pth.tar', 63.59199998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-21.pth.tar', 62.85600006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-19.pth.tar', 62.491999968261716)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-20.pth.tar', 62.29199993652344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-18.pth.tar', 61.41799993164062)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-17.pth.tar', 61.30999999755859)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-16.pth.tar', 59.94799998046875)

Train: 26 [   0/1251 (  0%)]  Loss: 4.221 (4.22)  Time: 2.381s,  429.99/s  (2.381s,  429.99/s)  LR: 9.954e-04  Data: 1.646 (1.646)
Train: 26 [  50/1251 (  4%)]  Loss: 3.558 (3.89)  Time: 0.857s, 1194.51/s  (0.846s, 1209.76/s)  LR: 9.954e-04  Data: 0.010 (0.046)
Train: 26 [ 100/1251 (  8%)]  Loss: 4.360 (4.05)  Time: 0.808s, 1267.10/s  (0.829s, 1235.29/s)  LR: 9.954e-04  Data: 0.009 (0.029)
Train: 26 [ 150/1251 ( 12%)]  Loss: 4.328 (4.12)  Time: 0.846s, 1210.12/s  (0.821s, 1246.80/s)  LR: 9.954e-04  Data: 0.016 (0.023)
Train: 26 [ 200/1251 ( 16%)]  Loss: 3.833 (4.06)  Time: 0.784s, 1306.29/s  (0.817s, 1253.23/s)  LR: 9.954e-04  Data: 0.009 (0.020)
Train: 26 [ 250/1251 ( 20%)]  Loss: 3.835 (4.02)  Time: 0.876s, 1169.05/s  (0.816s, 1255.66/s)  LR: 9.954e-04  Data: 0.010 (0.018)
Train: 26 [ 300/1251 ( 24%)]  Loss: 4.259 (4.06)  Time: 0.783s, 1307.68/s  (0.816s, 1254.54/s)  LR: 9.954e-04  Data: 0.013 (0.017)
Train: 26 [ 350/1251 ( 28%)]  Loss: 4.262 (4.08)  Time: 0.793s, 1292.05/s  (0.816s, 1255.23/s)  LR: 9.954e-04  Data: 0.010 (0.016)
Train: 26 [ 400/1251 ( 32%)]  Loss: 4.460 (4.12)  Time: 0.813s, 1259.34/s  (0.816s, 1255.65/s)  LR: 9.954e-04  Data: 0.010 (0.015)
Train: 26 [ 450/1251 ( 36%)]  Loss: 4.326 (4.14)  Time: 0.853s, 1200.87/s  (0.814s, 1257.46/s)  LR: 9.954e-04  Data: 0.010 (0.015)
Train: 26 [ 500/1251 ( 40%)]  Loss: 4.037 (4.13)  Time: 0.824s, 1243.34/s  (0.813s, 1259.22/s)  LR: 9.954e-04  Data: 0.010 (0.014)
Train: 26 [ 550/1251 ( 44%)]  Loss: 3.903 (4.12)  Time: 0.794s, 1289.43/s  (0.812s, 1260.71/s)  LR: 9.954e-04  Data: 0.009 (0.014)
Train: 26 [ 600/1251 ( 48%)]  Loss: 4.229 (4.12)  Time: 0.818s, 1252.44/s  (0.813s, 1260.20/s)  LR: 9.954e-04  Data: 0.009 (0.014)
Train: 26 [ 650/1251 ( 52%)]  Loss: 4.196 (4.13)  Time: 0.832s, 1230.68/s  (0.813s, 1259.96/s)  LR: 9.954e-04  Data: 0.012 (0.013)
Train: 26 [ 700/1251 ( 56%)]  Loss: 4.470 (4.15)  Time: 0.810s, 1263.92/s  (0.812s, 1260.35/s)  LR: 9.954e-04  Data: 0.010 (0.013)
Train: 26 [ 750/1251 ( 60%)]  Loss: 3.973 (4.14)  Time: 0.830s, 1233.03/s  (0.812s, 1260.46/s)  LR: 9.954e-04  Data: 0.010 (0.013)
Train: 26 [ 800/1251 ( 64%)]  Loss: 4.438 (4.16)  Time: 0.830s, 1233.65/s  (0.813s, 1260.11/s)  LR: 9.954e-04  Data: 0.014 (0.013)
Train: 26 [ 850/1251 ( 68%)]  Loss: 4.474 (4.18)  Time: 0.775s, 1321.32/s  (0.813s, 1259.20/s)  LR: 9.954e-04  Data: 0.010 (0.013)
Train: 26 [ 900/1251 ( 72%)]  Loss: 4.005 (4.17)  Time: 0.778s, 1316.21/s  (0.812s, 1260.62/s)  LR: 9.954e-04  Data: 0.010 (0.013)
Train: 26 [ 950/1251 ( 76%)]  Loss: 3.453 (4.13)  Time: 0.823s, 1243.86/s  (0.812s, 1261.03/s)  LR: 9.954e-04  Data: 0.010 (0.013)
Train: 26 [1000/1251 ( 80%)]  Loss: 4.243 (4.14)  Time: 0.788s, 1298.91/s  (0.812s, 1261.34/s)  LR: 9.954e-04  Data: 0.009 (0.012)
Train: 26 [1050/1251 ( 84%)]  Loss: 4.110 (4.14)  Time: 0.888s, 1153.46/s  (0.812s, 1261.40/s)  LR: 9.954e-04  Data: 0.009 (0.012)
Train: 26 [1100/1251 ( 88%)]  Loss: 4.003 (4.13)  Time: 0.827s, 1238.05/s  (0.812s, 1261.56/s)  LR: 9.954e-04  Data: 0.015 (0.012)
Train: 26 [1150/1251 ( 92%)]  Loss: 4.097 (4.13)  Time: 0.821s, 1247.60/s  (0.812s, 1261.04/s)  LR: 9.954e-04  Data: 0.011 (0.012)
Train: 26 [1200/1251 ( 96%)]  Loss: 4.109 (4.13)  Time: 0.829s, 1235.34/s  (0.811s, 1261.95/s)  LR: 9.954e-04  Data: 0.014 (0.012)
Train: 26 [1250/1251 (100%)]  Loss: 3.446 (4.10)  Time: 0.817s, 1253.47/s  (0.811s, 1262.29/s)  LR: 9.954e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.659 (1.659)  Loss:  1.0947 (1.0947)  Acc@1: 80.9570 (80.9570)  Acc@5: 95.0195 (95.0195)
Test: [  48/48]  Time: 0.194 (0.607)  Loss:  0.9912 (1.6424)  Acc@1: 81.2500 (65.5580)  Acc@5: 94.3396 (87.2920)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-26.pth.tar', 65.558)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-25.pth.tar', 65.2479999194336)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-24.pth.tar', 64.22199995117188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-22.pth.tar', 63.95600005859375)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-23.pth.tar', 63.59199998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-21.pth.tar', 62.85600006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-19.pth.tar', 62.491999968261716)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-20.pth.tar', 62.29199993652344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-18.pth.tar', 61.41799993164062)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-17.pth.tar', 61.30999999755859)

Train: 27 [   0/1251 (  0%)]  Loss: 4.431 (4.43)  Time: 2.348s,  436.03/s  (2.348s,  436.03/s)  LR: 9.951e-04  Data: 1.623 (1.623)
Train: 27 [  50/1251 (  4%)]  Loss: 4.487 (4.46)  Time: 0.826s, 1239.56/s  (0.841s, 1217.61/s)  LR: 9.951e-04  Data: 0.009 (0.047)
Train: 27 [ 100/1251 (  8%)]  Loss: 4.364 (4.43)  Time: 0.807s, 1268.97/s  (0.824s, 1243.16/s)  LR: 9.951e-04  Data: 0.010 (0.029)
Train: 27 [ 150/1251 ( 12%)]  Loss: 3.821 (4.28)  Time: 0.932s, 1098.54/s  (0.822s, 1246.31/s)  LR: 9.951e-04  Data: 0.010 (0.023)
Train: 27 [ 200/1251 ( 16%)]  Loss: 4.155 (4.25)  Time: 0.805s, 1272.40/s  (0.819s, 1250.57/s)  LR: 9.951e-04  Data: 0.009 (0.020)
Train: 27 [ 250/1251 ( 20%)]  Loss: 4.089 (4.22)  Time: 0.819s, 1250.32/s  (0.816s, 1254.66/s)  LR: 9.951e-04  Data: 0.009 (0.018)
Train: 27 [ 300/1251 ( 24%)]  Loss: 4.349 (4.24)  Time: 0.814s, 1258.03/s  (0.814s, 1257.27/s)  LR: 9.951e-04  Data: 0.010 (0.017)
Train: 27 [ 350/1251 ( 28%)]  Loss: 4.566 (4.28)  Time: 0.799s, 1281.19/s  (0.814s, 1258.38/s)  LR: 9.951e-04  Data: 0.012 (0.016)
Train: 27 [ 400/1251 ( 32%)]  Loss: 3.382 (4.18)  Time: 0.784s, 1305.56/s  (0.813s, 1259.53/s)  LR: 9.951e-04  Data: 0.010 (0.015)
Train: 27 [ 450/1251 ( 36%)]  Loss: 3.620 (4.13)  Time: 0.895s, 1143.92/s  (0.813s, 1259.74/s)  LR: 9.951e-04  Data: 0.010 (0.015)
Train: 27 [ 500/1251 ( 40%)]  Loss: 3.643 (4.08)  Time: 0.784s, 1305.53/s  (0.813s, 1259.23/s)  LR: 9.951e-04  Data: 0.010 (0.014)
Train: 27 [ 550/1251 ( 44%)]  Loss: 4.182 (4.09)  Time: 0.780s, 1312.62/s  (0.813s, 1259.90/s)  LR: 9.951e-04  Data: 0.010 (0.014)
Train: 27 [ 600/1251 ( 48%)]  Loss: 4.149 (4.10)  Time: 0.813s, 1260.26/s  (0.813s, 1260.13/s)  LR: 9.951e-04  Data: 0.009 (0.014)
Train: 27 [ 650/1251 ( 52%)]  Loss: 4.165 (4.10)  Time: 0.828s, 1237.24/s  (0.811s, 1262.10/s)  LR: 9.951e-04  Data: 0.010 (0.013)
Train: 27 [ 700/1251 ( 56%)]  Loss: 4.379 (4.12)  Time: 0.789s, 1297.80/s  (0.811s, 1262.25/s)  LR: 9.951e-04  Data: 0.009 (0.013)
Train: 27 [ 750/1251 ( 60%)]  Loss: 4.113 (4.12)  Time: 0.809s, 1265.11/s  (0.812s, 1261.73/s)  LR: 9.951e-04  Data: 0.010 (0.013)
Train: 27 [ 800/1251 ( 64%)]  Loss: 4.036 (4.11)  Time: 0.773s, 1325.34/s  (0.811s, 1262.08/s)  LR: 9.951e-04  Data: 0.010 (0.013)
Train: 27 [ 850/1251 ( 68%)]  Loss: 4.563 (4.14)  Time: 0.804s, 1273.16/s  (0.811s, 1261.98/s)  LR: 9.951e-04  Data: 0.010 (0.013)
Train: 27 [ 900/1251 ( 72%)]  Loss: 4.203 (4.14)  Time: 0.775s, 1321.76/s  (0.812s, 1261.47/s)  LR: 9.951e-04  Data: 0.009 (0.013)
Train: 27 [ 950/1251 ( 76%)]  Loss: 4.378 (4.15)  Time: 0.786s, 1302.35/s  (0.812s, 1261.42/s)  LR: 9.951e-04  Data: 0.010 (0.013)
Train: 27 [1000/1251 ( 80%)]  Loss: 4.206 (4.16)  Time: 0.808s, 1267.67/s  (0.812s, 1261.55/s)  LR: 9.951e-04  Data: 0.010 (0.012)
Train: 27 [1050/1251 ( 84%)]  Loss: 4.236 (4.16)  Time: 0.805s, 1271.71/s  (0.811s, 1261.92/s)  LR: 9.951e-04  Data: 0.010 (0.012)
Train: 27 [1100/1251 ( 88%)]  Loss: 4.122 (4.16)  Time: 0.815s, 1256.33/s  (0.811s, 1262.27/s)  LR: 9.951e-04  Data: 0.009 (0.012)
Train: 27 [1150/1251 ( 92%)]  Loss: 4.357 (4.17)  Time: 0.799s, 1281.44/s  (0.811s, 1262.22/s)  LR: 9.951e-04  Data: 0.010 (0.012)
Train: 27 [1200/1251 ( 96%)]  Loss: 4.024 (4.16)  Time: 0.828s, 1237.07/s  (0.811s, 1262.59/s)  LR: 9.951e-04  Data: 0.009 (0.012)
Train: 27 [1250/1251 (100%)]  Loss: 4.042 (4.16)  Time: 0.859s, 1192.21/s  (0.811s, 1262.77/s)  LR: 9.951e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.648 (1.648)  Loss:  1.1748 (1.1748)  Acc@1: 82.0312 (82.0312)  Acc@5: 94.9219 (94.9219)
Test: [  48/48]  Time: 0.193 (0.611)  Loss:  1.1230 (1.7930)  Acc@1: 79.7170 (65.6060)  Acc@5: 93.2783 (87.5640)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-27.pth.tar', 65.60599998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-26.pth.tar', 65.558)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-25.pth.tar', 65.2479999194336)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-24.pth.tar', 64.22199995117188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-22.pth.tar', 63.95600005859375)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-23.pth.tar', 63.59199998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-21.pth.tar', 62.85600006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-19.pth.tar', 62.491999968261716)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-20.pth.tar', 62.29199993652344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-18.pth.tar', 61.41799993164062)

Train: 28 [   0/1251 (  0%)]  Loss: 4.063 (4.06)  Time: 2.375s,  431.11/s  (2.375s,  431.11/s)  LR: 9.947e-04  Data: 1.646 (1.646)
Train: 28 [  50/1251 (  4%)]  Loss: 4.174 (4.12)  Time: 0.805s, 1271.60/s  (0.850s, 1204.44/s)  LR: 9.947e-04  Data: 0.014 (0.048)
Train: 28 [ 100/1251 (  8%)]  Loss: 4.028 (4.09)  Time: 0.829s, 1235.53/s  (0.832s, 1230.44/s)  LR: 9.947e-04  Data: 0.010 (0.029)
Train: 28 [ 150/1251 ( 12%)]  Loss: 4.082 (4.09)  Time: 0.840s, 1218.56/s  (0.825s, 1241.47/s)  LR: 9.947e-04  Data: 0.009 (0.023)
Train: 28 [ 200/1251 ( 16%)]  Loss: 4.028 (4.07)  Time: 0.806s, 1269.98/s  (0.820s, 1248.25/s)  LR: 9.947e-04  Data: 0.009 (0.020)
Train: 28 [ 250/1251 ( 20%)]  Loss: 3.832 (4.03)  Time: 0.782s, 1308.96/s  (0.818s, 1251.56/s)  LR: 9.947e-04  Data: 0.009 (0.018)
Train: 28 [ 300/1251 ( 24%)]  Loss: 3.933 (4.02)  Time: 0.825s, 1240.73/s  (0.817s, 1253.43/s)  LR: 9.947e-04  Data: 0.009 (0.017)
Train: 28 [ 350/1251 ( 28%)]  Loss: 3.837 (4.00)  Time: 0.793s, 1290.58/s  (0.815s, 1255.82/s)  LR: 9.947e-04  Data: 0.016 (0.016)
Train: 28 [ 400/1251 ( 32%)]  Loss: 4.221 (4.02)  Time: 0.780s, 1312.49/s  (0.815s, 1256.05/s)  LR: 9.947e-04  Data: 0.013 (0.015)
Train: 28 [ 450/1251 ( 36%)]  Loss: 3.991 (4.02)  Time: 0.856s, 1195.71/s  (0.814s, 1257.43/s)  LR: 9.947e-04  Data: 0.009 (0.015)
Train: 28 [ 500/1251 ( 40%)]  Loss: 3.767 (4.00)  Time: 0.789s, 1298.29/s  (0.814s, 1258.14/s)  LR: 9.947e-04  Data: 0.010 (0.014)
Train: 28 [ 550/1251 ( 44%)]  Loss: 3.919 (3.99)  Time: 0.819s, 1249.99/s  (0.813s, 1258.93/s)  LR: 9.947e-04  Data: 0.009 (0.014)
Train: 28 [ 600/1251 ( 48%)]  Loss: 4.068 (4.00)  Time: 0.782s, 1310.16/s  (0.813s, 1259.52/s)  LR: 9.947e-04  Data: 0.011 (0.014)
Train: 28 [ 650/1251 ( 52%)]  Loss: 3.803 (3.98)  Time: 0.840s, 1218.35/s  (0.813s, 1259.56/s)  LR: 9.947e-04  Data: 0.016 (0.014)
Train: 28 [ 700/1251 ( 56%)]  Loss: 4.148 (3.99)  Time: 0.799s, 1281.88/s  (0.812s, 1260.65/s)  LR: 9.947e-04  Data: 0.010 (0.013)
Train: 28 [ 750/1251 ( 60%)]  Loss: 3.588 (3.97)  Time: 0.898s, 1140.34/s  (0.812s, 1260.37/s)  LR: 9.947e-04  Data: 0.010 (0.013)
Train: 28 [ 800/1251 ( 64%)]  Loss: 3.896 (3.96)  Time: 0.794s, 1290.03/s  (0.812s, 1261.29/s)  LR: 9.947e-04  Data: 0.009 (0.013)
Train: 28 [ 850/1251 ( 68%)]  Loss: 3.842 (3.96)  Time: 0.798s, 1282.48/s  (0.811s, 1262.51/s)  LR: 9.947e-04  Data: 0.010 (0.013)
Train: 28 [ 900/1251 ( 72%)]  Loss: 4.100 (3.96)  Time: 0.802s, 1277.08/s  (0.811s, 1262.29/s)  LR: 9.947e-04  Data: 0.010 (0.013)
Train: 28 [ 950/1251 ( 76%)]  Loss: 4.257 (3.98)  Time: 0.814s, 1257.38/s  (0.811s, 1262.80/s)  LR: 9.947e-04  Data: 0.011 (0.013)
Train: 28 [1000/1251 ( 80%)]  Loss: 3.979 (3.98)  Time: 0.825s, 1241.30/s  (0.811s, 1262.79/s)  LR: 9.947e-04  Data: 0.009 (0.013)
Train: 28 [1050/1251 ( 84%)]  Loss: 3.744 (3.97)  Time: 0.778s, 1315.82/s  (0.811s, 1263.16/s)  LR: 9.947e-04  Data: 0.010 (0.012)
Train: 28 [1100/1251 ( 88%)]  Loss: 3.855 (3.96)  Time: 0.811s, 1263.38/s  (0.811s, 1263.38/s)  LR: 9.947e-04  Data: 0.012 (0.012)
Train: 28 [1150/1251 ( 92%)]  Loss: 4.013 (3.97)  Time: 0.814s, 1258.05/s  (0.811s, 1263.16/s)  LR: 9.947e-04  Data: 0.014 (0.012)
Train: 28 [1200/1251 ( 96%)]  Loss: 4.112 (3.97)  Time: 0.784s, 1306.94/s  (0.811s, 1263.30/s)  LR: 9.947e-04  Data: 0.013 (0.012)
Train: 28 [1250/1251 (100%)]  Loss: 4.086 (3.98)  Time: 0.779s, 1314.52/s  (0.810s, 1263.64/s)  LR: 9.947e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.773 (1.773)  Loss:  1.0684 (1.0684)  Acc@1: 83.1055 (83.1055)  Acc@5: 94.6289 (94.6289)
Test: [  48/48]  Time: 0.193 (0.614)  Loss:  1.1113 (1.6503)  Acc@1: 78.7736 (65.7080)  Acc@5: 93.0424 (87.5520)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-28.pth.tar', 65.70800008789062)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-27.pth.tar', 65.60599998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-26.pth.tar', 65.558)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-25.pth.tar', 65.2479999194336)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-24.pth.tar', 64.22199995117188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-22.pth.tar', 63.95600005859375)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-23.pth.tar', 63.59199998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-21.pth.tar', 62.85600006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-19.pth.tar', 62.491999968261716)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-20.pth.tar', 62.29199993652344)

Train: 29 [   0/1251 (  0%)]  Loss: 4.143 (4.14)  Time: 2.398s,  426.99/s  (2.398s,  426.99/s)  LR: 9.943e-04  Data: 1.663 (1.663)
Train: 29 [  50/1251 (  4%)]  Loss: 4.233 (4.19)  Time: 0.810s, 1264.83/s  (0.853s, 1201.08/s)  LR: 9.943e-04  Data: 0.010 (0.050)
Train: 29 [ 100/1251 (  8%)]  Loss: 4.312 (4.23)  Time: 0.914s, 1120.42/s  (0.831s, 1232.12/s)  LR: 9.943e-04  Data: 0.009 (0.031)
Train: 29 [ 150/1251 ( 12%)]  Loss: 4.074 (4.19)  Time: 0.779s, 1314.25/s  (0.822s, 1246.30/s)  LR: 9.943e-04  Data: 0.013 (0.024)
Train: 29 [ 200/1251 ( 16%)]  Loss: 4.450 (4.24)  Time: 0.778s, 1316.12/s  (0.819s, 1250.10/s)  LR: 9.943e-04  Data: 0.010 (0.021)
Train: 29 [ 250/1251 ( 20%)]  Loss: 3.709 (4.15)  Time: 0.821s, 1246.80/s  (0.817s, 1253.51/s)  LR: 9.943e-04  Data: 0.010 (0.019)
Train: 29 [ 300/1251 ( 24%)]  Loss: 4.031 (4.14)  Time: 0.782s, 1308.69/s  (0.815s, 1256.33/s)  LR: 9.943e-04  Data: 0.010 (0.017)
Train: 29 [ 350/1251 ( 28%)]  Loss: 3.935 (4.11)  Time: 0.772s, 1327.14/s  (0.815s, 1257.04/s)  LR: 9.943e-04  Data: 0.009 (0.016)
Train: 29 [ 400/1251 ( 32%)]  Loss: 4.173 (4.12)  Time: 0.833s, 1229.24/s  (0.813s, 1259.79/s)  LR: 9.943e-04  Data: 0.009 (0.016)
Train: 29 [ 450/1251 ( 36%)]  Loss: 4.329 (4.14)  Time: 0.833s, 1229.36/s  (0.813s, 1260.06/s)  LR: 9.943e-04  Data: 0.009 (0.015)
Train: 29 [ 500/1251 ( 40%)]  Loss: 4.164 (4.14)  Time: 0.825s, 1241.44/s  (0.811s, 1262.03/s)  LR: 9.943e-04  Data: 0.010 (0.014)
Train: 29 [ 550/1251 ( 44%)]  Loss: 4.056 (4.13)  Time: 0.784s, 1306.71/s  (0.812s, 1261.39/s)  LR: 9.943e-04  Data: 0.014 (0.014)
Train: 29 [ 600/1251 ( 48%)]  Loss: 4.177 (4.14)  Time: 0.818s, 1252.31/s  (0.812s, 1260.93/s)  LR: 9.943e-04  Data: 0.009 (0.014)
Train: 29 [ 650/1251 ( 52%)]  Loss: 4.076 (4.13)  Time: 0.835s, 1227.04/s  (0.812s, 1261.82/s)  LR: 9.943e-04  Data: 0.009 (0.014)
Train: 29 [ 700/1251 ( 56%)]  Loss: 4.037 (4.13)  Time: 0.808s, 1267.25/s  (0.812s, 1261.78/s)  LR: 9.943e-04  Data: 0.009 (0.013)
Train: 29 [ 750/1251 ( 60%)]  Loss: 3.494 (4.09)  Time: 0.774s, 1323.40/s  (0.811s, 1262.48/s)  LR: 9.943e-04  Data: 0.011 (0.013)
Train: 29 [ 800/1251 ( 64%)]  Loss: 3.783 (4.07)  Time: 0.825s, 1241.06/s  (0.811s, 1262.59/s)  LR: 9.943e-04  Data: 0.010 (0.013)
Train: 29 [ 850/1251 ( 68%)]  Loss: 4.092 (4.07)  Time: 0.816s, 1255.27/s  (0.811s, 1262.99/s)  LR: 9.943e-04  Data: 0.012 (0.013)
Train: 29 [ 900/1251 ( 72%)]  Loss: 3.774 (4.05)  Time: 0.905s, 1131.03/s  (0.811s, 1263.19/s)  LR: 9.943e-04  Data: 0.009 (0.013)
Train: 29 [ 950/1251 ( 76%)]  Loss: 3.875 (4.05)  Time: 0.833s, 1229.90/s  (0.811s, 1262.82/s)  LR: 9.943e-04  Data: 0.017 (0.013)
Train: 29 [1000/1251 ( 80%)]  Loss: 4.027 (4.04)  Time: 0.781s, 1310.46/s  (0.811s, 1262.60/s)  LR: 9.943e-04  Data: 0.010 (0.013)
Train: 29 [1050/1251 ( 84%)]  Loss: 4.339 (4.06)  Time: 0.825s, 1241.00/s  (0.811s, 1262.63/s)  LR: 9.943e-04  Data: 0.009 (0.012)
Train: 29 [1100/1251 ( 88%)]  Loss: 3.773 (4.05)  Time: 0.791s, 1294.08/s  (0.811s, 1263.01/s)  LR: 9.943e-04  Data: 0.009 (0.012)
Train: 29 [1150/1251 ( 92%)]  Loss: 4.104 (4.05)  Time: 0.804s, 1273.01/s  (0.811s, 1263.24/s)  LR: 9.943e-04  Data: 0.012 (0.012)
Train: 29 [1200/1251 ( 96%)]  Loss: 3.984 (4.05)  Time: 0.780s, 1312.50/s  (0.811s, 1263.40/s)  LR: 9.943e-04  Data: 0.009 (0.012)
Train: 29 [1250/1251 (100%)]  Loss: 4.338 (4.06)  Time: 0.857s, 1195.50/s  (0.810s, 1263.70/s)  LR: 9.943e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.649 (1.649)  Loss:  0.9824 (0.9824)  Acc@1: 84.4727 (84.4727)  Acc@5: 96.1914 (96.1914)
Test: [  48/48]  Time: 0.194 (0.603)  Loss:  1.0391 (1.7006)  Acc@1: 83.0189 (66.3360)  Acc@5: 95.2830 (87.9160)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-29.pth.tar', 66.33599999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-28.pth.tar', 65.70800008789062)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-27.pth.tar', 65.60599998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-26.pth.tar', 65.558)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-25.pth.tar', 65.2479999194336)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-24.pth.tar', 64.22199995117188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-22.pth.tar', 63.95600005859375)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-23.pth.tar', 63.59199998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-21.pth.tar', 62.85600006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-19.pth.tar', 62.491999968261716)

Train: 30 [   0/1251 (  0%)]  Loss: 4.272 (4.27)  Time: 2.422s,  422.72/s  (2.422s,  422.72/s)  LR: 9.939e-04  Data: 1.673 (1.673)
Train: 30 [  50/1251 (  4%)]  Loss: 4.131 (4.20)  Time: 0.814s, 1258.47/s  (0.851s, 1203.17/s)  LR: 9.939e-04  Data: 0.011 (0.052)
Train: 30 [ 100/1251 (  8%)]  Loss: 3.807 (4.07)  Time: 0.809s, 1266.06/s  (0.829s, 1235.03/s)  LR: 9.939e-04  Data: 0.009 (0.031)
Train: 30 [ 150/1251 ( 12%)]  Loss: 3.761 (3.99)  Time: 0.835s, 1226.82/s  (0.823s, 1244.60/s)  LR: 9.939e-04  Data: 0.010 (0.024)
Train: 30 [ 200/1251 ( 16%)]  Loss: 4.034 (4.00)  Time: 0.779s, 1314.37/s  (0.819s, 1249.82/s)  LR: 9.939e-04  Data: 0.010 (0.021)
Train: 30 [ 250/1251 ( 20%)]  Loss: 4.284 (4.05)  Time: 0.782s, 1308.96/s  (0.817s, 1253.94/s)  LR: 9.939e-04  Data: 0.009 (0.019)
Train: 30 [ 300/1251 ( 24%)]  Loss: 4.050 (4.05)  Time: 0.778s, 1317.04/s  (0.815s, 1255.95/s)  LR: 9.939e-04  Data: 0.009 (0.017)
Train: 30 [ 350/1251 ( 28%)]  Loss: 3.741 (4.01)  Time: 0.797s, 1285.41/s  (0.814s, 1257.41/s)  LR: 9.939e-04  Data: 0.009 (0.016)
Train: 30 [ 400/1251 ( 32%)]  Loss: 4.334 (4.05)  Time: 0.802s, 1277.48/s  (0.814s, 1258.22/s)  LR: 9.939e-04  Data: 0.010 (0.016)
Train: 30 [ 450/1251 ( 36%)]  Loss: 4.053 (4.05)  Time: 0.803s, 1275.97/s  (0.813s, 1259.72/s)  LR: 9.939e-04  Data: 0.009 (0.015)
Train: 30 [ 500/1251 ( 40%)]  Loss: 4.283 (4.07)  Time: 0.801s, 1277.88/s  (0.812s, 1260.89/s)  LR: 9.939e-04  Data: 0.011 (0.015)
Train: 30 [ 550/1251 ( 44%)]  Loss: 3.914 (4.06)  Time: 0.814s, 1258.54/s  (0.812s, 1261.14/s)  LR: 9.939e-04  Data: 0.010 (0.014)
Train: 30 [ 600/1251 ( 48%)]  Loss: 3.987 (4.05)  Time: 0.831s, 1231.73/s  (0.811s, 1262.10/s)  LR: 9.939e-04  Data: 0.009 (0.014)
Train: 30 [ 650/1251 ( 52%)]  Loss: 4.221 (4.06)  Time: 0.798s, 1283.14/s  (0.811s, 1262.76/s)  LR: 9.939e-04  Data: 0.010 (0.014)
Train: 30 [ 700/1251 ( 56%)]  Loss: 3.986 (4.06)  Time: 0.769s, 1330.89/s  (0.811s, 1262.87/s)  LR: 9.939e-04  Data: 0.010 (0.014)
Train: 30 [ 750/1251 ( 60%)]  Loss: 4.157 (4.06)  Time: 0.804s, 1273.73/s  (0.811s, 1262.80/s)  LR: 9.939e-04  Data: 0.013 (0.013)
Train: 30 [ 800/1251 ( 64%)]  Loss: 4.327 (4.08)  Time: 0.795s, 1287.49/s  (0.810s, 1263.56/s)  LR: 9.939e-04  Data: 0.010 (0.013)
Train: 30 [ 850/1251 ( 68%)]  Loss: 4.089 (4.08)  Time: 0.814s, 1257.35/s  (0.811s, 1263.34/s)  LR: 9.939e-04  Data: 0.014 (0.013)
Train: 30 [ 900/1251 ( 72%)]  Loss: 4.242 (4.09)  Time: 0.783s, 1307.28/s  (0.810s, 1263.66/s)  LR: 9.939e-04  Data: 0.009 (0.013)
Train: 30 [ 950/1251 ( 76%)]  Loss: 4.464 (4.11)  Time: 0.813s, 1259.25/s  (0.810s, 1263.44/s)  LR: 9.939e-04  Data: 0.010 (0.013)
Train: 30 [1000/1251 ( 80%)]  Loss: 4.068 (4.11)  Time: 0.831s, 1232.90/s  (0.811s, 1263.22/s)  LR: 9.939e-04  Data: 0.009 (0.013)
Train: 30 [1050/1251 ( 84%)]  Loss: 3.626 (4.08)  Time: 0.835s, 1226.30/s  (0.811s, 1263.38/s)  LR: 9.939e-04  Data: 0.011 (0.013)
Train: 30 [1100/1251 ( 88%)]  Loss: 4.488 (4.10)  Time: 0.809s, 1265.78/s  (0.811s, 1262.89/s)  LR: 9.939e-04  Data: 0.010 (0.012)
Train: 30 [1150/1251 ( 92%)]  Loss: 4.131 (4.10)  Time: 0.712s, 1438.11/s  (0.810s, 1263.43/s)  LR: 9.939e-04  Data: 0.010 (0.012)
Train: 30 [1200/1251 ( 96%)]  Loss: 3.688 (4.09)  Time: 0.806s, 1270.12/s  (0.810s, 1263.65/s)  LR: 9.939e-04  Data: 0.012 (0.012)
Train: 30 [1250/1251 (100%)]  Loss: 3.839 (4.08)  Time: 0.766s, 1337.47/s  (0.810s, 1263.94/s)  LR: 9.939e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.742 (1.742)  Loss:  0.9746 (0.9746)  Acc@1: 82.4219 (82.4219)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.195 (0.610)  Loss:  1.0088 (1.6632)  Acc@1: 81.1321 (65.5680)  Acc@5: 94.5755 (87.6240)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-29.pth.tar', 66.33599999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-28.pth.tar', 65.70800008789062)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-27.pth.tar', 65.60599998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-30.pth.tar', 65.568000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-26.pth.tar', 65.558)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-25.pth.tar', 65.2479999194336)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-24.pth.tar', 64.22199995117188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-22.pth.tar', 63.95600005859375)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-23.pth.tar', 63.59199998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-21.pth.tar', 62.85600006591797)

Train: 31 [   0/1251 (  0%)]  Loss: 4.324 (4.32)  Time: 2.331s,  439.35/s  (2.331s,  439.35/s)  LR: 9.935e-04  Data: 1.601 (1.601)
Train: 31 [  50/1251 (  4%)]  Loss: 3.969 (4.15)  Time: 0.789s, 1297.61/s  (0.844s, 1213.78/s)  LR: 9.935e-04  Data: 0.012 (0.052)
Train: 31 [ 100/1251 (  8%)]  Loss: 4.146 (4.15)  Time: 0.801s, 1277.65/s  (0.827s, 1238.03/s)  LR: 9.935e-04  Data: 0.014 (0.032)
Train: 31 [ 150/1251 ( 12%)]  Loss: 3.999 (4.11)  Time: 0.852s, 1201.22/s  (0.819s, 1250.03/s)  LR: 9.935e-04  Data: 0.010 (0.025)
Train: 31 [ 200/1251 ( 16%)]  Loss: 4.086 (4.10)  Time: 0.776s, 1320.25/s  (0.817s, 1253.48/s)  LR: 9.935e-04  Data: 0.011 (0.021)
Train: 31 [ 250/1251 ( 20%)]  Loss: 4.270 (4.13)  Time: 0.802s, 1276.84/s  (0.816s, 1255.08/s)  LR: 9.935e-04  Data: 0.015 (0.019)
Train: 31 [ 300/1251 ( 24%)]  Loss: 3.886 (4.10)  Time: 0.816s, 1255.16/s  (0.814s, 1257.92/s)  LR: 9.935e-04  Data: 0.011 (0.018)
Train: 31 [ 350/1251 ( 28%)]  Loss: 3.562 (4.03)  Time: 0.830s, 1234.04/s  (0.813s, 1259.75/s)  LR: 9.935e-04  Data: 0.010 (0.017)
Train: 31 [ 400/1251 ( 32%)]  Loss: 4.135 (4.04)  Time: 0.795s, 1288.60/s  (0.812s, 1261.26/s)  LR: 9.935e-04  Data: 0.009 (0.016)
Train: 31 [ 450/1251 ( 36%)]  Loss: 4.110 (4.05)  Time: 0.786s, 1302.53/s  (0.812s, 1260.42/s)  LR: 9.935e-04  Data: 0.015 (0.015)
Train: 31 [ 500/1251 ( 40%)]  Loss: 3.988 (4.04)  Time: 0.815s, 1256.14/s  (0.814s, 1258.69/s)  LR: 9.935e-04  Data: 0.012 (0.015)
Train: 31 [ 550/1251 ( 44%)]  Loss: 3.913 (4.03)  Time: 0.779s, 1315.24/s  (0.814s, 1257.47/s)  LR: 9.935e-04  Data: 0.009 (0.015)
Train: 31 [ 600/1251 ( 48%)]  Loss: 3.513 (3.99)  Time: 0.784s, 1306.35/s  (0.812s, 1260.56/s)  LR: 9.935e-04  Data: 0.010 (0.014)
Train: 31 [ 650/1251 ( 52%)]  Loss: 4.004 (3.99)  Time: 0.792s, 1293.14/s  (0.811s, 1262.18/s)  LR: 9.935e-04  Data: 0.009 (0.014)
Train: 31 [ 700/1251 ( 56%)]  Loss: 3.820 (3.98)  Time: 0.815s, 1256.75/s  (0.811s, 1261.92/s)  LR: 9.935e-04  Data: 0.010 (0.014)
Train: 31 [ 750/1251 ( 60%)]  Loss: 3.954 (3.98)  Time: 0.778s, 1316.11/s  (0.811s, 1262.89/s)  LR: 9.935e-04  Data: 0.010 (0.014)
Train: 31 [ 800/1251 ( 64%)]  Loss: 3.981 (3.98)  Time: 0.783s, 1307.26/s  (0.810s, 1263.56/s)  LR: 9.935e-04  Data: 0.009 (0.013)
Train: 31 [ 850/1251 ( 68%)]  Loss: 3.589 (3.96)  Time: 0.850s, 1204.99/s  (0.810s, 1264.02/s)  LR: 9.935e-04  Data: 0.010 (0.013)
Train: 31 [ 900/1251 ( 72%)]  Loss: 4.044 (3.96)  Time: 0.787s, 1300.70/s  (0.810s, 1263.85/s)  LR: 9.935e-04  Data: 0.012 (0.013)
Train: 31 [ 950/1251 ( 76%)]  Loss: 3.931 (3.96)  Time: 0.802s, 1277.14/s  (0.810s, 1263.78/s)  LR: 9.935e-04  Data: 0.009 (0.013)
Train: 31 [1000/1251 ( 80%)]  Loss: 4.245 (3.97)  Time: 0.837s, 1223.66/s  (0.810s, 1264.09/s)  LR: 9.935e-04  Data: 0.009 (0.013)
Train: 31 [1050/1251 ( 84%)]  Loss: 3.802 (3.97)  Time: 0.830s, 1233.72/s  (0.810s, 1264.42/s)  LR: 9.935e-04  Data: 0.009 (0.013)
Train: 31 [1100/1251 ( 88%)]  Loss: 4.025 (3.97)  Time: 0.815s, 1256.10/s  (0.810s, 1264.34/s)  LR: 9.935e-04  Data: 0.011 (0.013)
Train: 31 [1150/1251 ( 92%)]  Loss: 3.924 (3.97)  Time: 0.828s, 1237.41/s  (0.810s, 1264.50/s)  LR: 9.935e-04  Data: 0.010 (0.013)
Train: 31 [1200/1251 ( 96%)]  Loss: 4.329 (3.98)  Time: 0.835s, 1226.29/s  (0.810s, 1264.27/s)  LR: 9.935e-04  Data: 0.009 (0.012)
Train: 31 [1250/1251 (100%)]  Loss: 4.311 (3.99)  Time: 0.810s, 1264.74/s  (0.810s, 1263.91/s)  LR: 9.935e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.740 (1.740)  Loss:  1.0381 (1.0381)  Acc@1: 84.8633 (84.8633)  Acc@5: 96.0938 (96.0938)
Test: [  48/48]  Time: 0.194 (0.606)  Loss:  1.1582 (1.6830)  Acc@1: 80.1887 (66.2940)  Acc@5: 93.8679 (87.5620)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-29.pth.tar', 66.33599999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-31.pth.tar', 66.2939999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-28.pth.tar', 65.70800008789062)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-27.pth.tar', 65.60599998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-30.pth.tar', 65.568000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-26.pth.tar', 65.558)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-25.pth.tar', 65.2479999194336)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-24.pth.tar', 64.22199995117188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-22.pth.tar', 63.95600005859375)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-23.pth.tar', 63.59199998046875)

Train: 32 [   0/1251 (  0%)]  Loss: 4.299 (4.30)  Time: 2.425s,  422.28/s  (2.425s,  422.28/s)  LR: 9.931e-04  Data: 1.682 (1.682)
Train: 32 [  50/1251 (  4%)]  Loss: 3.973 (4.14)  Time: 0.821s, 1247.50/s  (0.850s, 1205.36/s)  LR: 9.931e-04  Data: 0.009 (0.052)
Train: 32 [ 100/1251 (  8%)]  Loss: 3.737 (4.00)  Time: 0.823s, 1244.27/s  (0.831s, 1231.91/s)  LR: 9.931e-04  Data: 0.013 (0.031)
Train: 32 [ 150/1251 ( 12%)]  Loss: 4.082 (4.02)  Time: 0.806s, 1271.23/s  (0.822s, 1245.63/s)  LR: 9.931e-04  Data: 0.012 (0.025)
Train: 32 [ 200/1251 ( 16%)]  Loss: 3.708 (3.96)  Time: 0.834s, 1227.22/s  (0.820s, 1249.44/s)  LR: 9.931e-04  Data: 0.011 (0.021)
Train: 32 [ 250/1251 ( 20%)]  Loss: 3.955 (3.96)  Time: 0.811s, 1262.62/s  (0.819s, 1251.05/s)  LR: 9.931e-04  Data: 0.010 (0.019)
Train: 32 [ 300/1251 ( 24%)]  Loss: 3.770 (3.93)  Time: 0.828s, 1236.30/s  (0.817s, 1252.97/s)  LR: 9.931e-04  Data: 0.013 (0.018)
Train: 32 [ 350/1251 ( 28%)]  Loss: 4.244 (3.97)  Time: 0.827s, 1238.57/s  (0.817s, 1253.91/s)  LR: 9.931e-04  Data: 0.010 (0.016)
Train: 32 [ 400/1251 ( 32%)]  Loss: 3.935 (3.97)  Time: 0.808s, 1267.57/s  (0.816s, 1254.96/s)  LR: 9.931e-04  Data: 0.009 (0.016)
Train: 32 [ 450/1251 ( 36%)]  Loss: 3.340 (3.90)  Time: 0.845s, 1212.27/s  (0.815s, 1257.07/s)  LR: 9.931e-04  Data: 0.009 (0.015)
Train: 32 [ 500/1251 ( 40%)]  Loss: 4.375 (3.95)  Time: 0.831s, 1232.86/s  (0.813s, 1258.80/s)  LR: 9.931e-04  Data: 0.010 (0.015)
Train: 32 [ 550/1251 ( 44%)]  Loss: 4.125 (3.96)  Time: 0.801s, 1278.68/s  (0.813s, 1259.52/s)  LR: 9.931e-04  Data: 0.010 (0.014)
Train: 32 [ 600/1251 ( 48%)]  Loss: 3.855 (3.95)  Time: 0.809s, 1266.35/s  (0.813s, 1259.17/s)  LR: 9.931e-04  Data: 0.009 (0.014)
Train: 32 [ 650/1251 ( 52%)]  Loss: 3.986 (3.96)  Time: 0.784s, 1305.76/s  (0.813s, 1260.09/s)  LR: 9.931e-04  Data: 0.013 (0.014)
Train: 32 [ 700/1251 ( 56%)]  Loss: 3.940 (3.95)  Time: 0.831s, 1231.77/s  (0.812s, 1260.49/s)  LR: 9.931e-04  Data: 0.010 (0.014)
Train: 32 [ 750/1251 ( 60%)]  Loss: 3.765 (3.94)  Time: 0.818s, 1251.96/s  (0.812s, 1260.65/s)  LR: 9.931e-04  Data: 0.010 (0.013)
Train: 32 [ 800/1251 ( 64%)]  Loss: 4.065 (3.95)  Time: 0.814s, 1257.94/s  (0.812s, 1261.58/s)  LR: 9.931e-04  Data: 0.010 (0.013)
Train: 32 [ 850/1251 ( 68%)]  Loss: 3.998 (3.95)  Time: 0.798s, 1282.55/s  (0.812s, 1261.68/s)  LR: 9.931e-04  Data: 0.014 (0.013)
Train: 32 [ 900/1251 ( 72%)]  Loss: 3.988 (3.95)  Time: 0.838s, 1221.35/s  (0.812s, 1261.58/s)  LR: 9.931e-04  Data: 0.010 (0.013)
Train: 32 [ 950/1251 ( 76%)]  Loss: 3.955 (3.95)  Time: 0.807s, 1269.05/s  (0.812s, 1261.59/s)  LR: 9.931e-04  Data: 0.010 (0.013)
Train: 32 [1000/1251 ( 80%)]  Loss: 4.032 (3.96)  Time: 0.793s, 1292.00/s  (0.811s, 1261.90/s)  LR: 9.931e-04  Data: 0.009 (0.013)
Train: 32 [1050/1251 ( 84%)]  Loss: 4.049 (3.96)  Time: 0.796s, 1286.31/s  (0.811s, 1262.47/s)  LR: 9.931e-04  Data: 0.014 (0.013)
Train: 32 [1100/1251 ( 88%)]  Loss: 3.599 (3.95)  Time: 0.818s, 1251.92/s  (0.811s, 1262.80/s)  LR: 9.931e-04  Data: 0.010 (0.013)
Train: 32 [1150/1251 ( 92%)]  Loss: 3.970 (3.95)  Time: 0.795s, 1287.58/s  (0.811s, 1263.30/s)  LR: 9.931e-04  Data: 0.010 (0.012)
Train: 32 [1200/1251 ( 96%)]  Loss: 4.175 (3.96)  Time: 0.816s, 1254.30/s  (0.811s, 1263.13/s)  LR: 9.931e-04  Data: 0.009 (0.012)
Train: 32 [1250/1251 (100%)]  Loss: 4.179 (3.97)  Time: 0.803s, 1275.36/s  (0.811s, 1262.94/s)  LR: 9.931e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.697 (1.697)  Loss:  1.1543 (1.1543)  Acc@1: 84.2773 (84.2773)  Acc@5: 95.9961 (95.9961)
Test: [  48/48]  Time: 0.194 (0.611)  Loss:  1.1475 (1.8021)  Acc@1: 83.6085 (66.4220)  Acc@5: 95.8726 (87.8840)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-32.pth.tar', 66.42199999023437)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-29.pth.tar', 66.33599999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-31.pth.tar', 66.2939999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-28.pth.tar', 65.70800008789062)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-27.pth.tar', 65.60599998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-30.pth.tar', 65.568000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-26.pth.tar', 65.558)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-25.pth.tar', 65.2479999194336)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-24.pth.tar', 64.22199995117188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-22.pth.tar', 63.95600005859375)

Train: 33 [   0/1251 (  0%)]  Loss: 4.041 (4.04)  Time: 2.321s,  441.11/s  (2.321s,  441.11/s)  LR: 9.926e-04  Data: 1.602 (1.602)
Train: 33 [  50/1251 (  4%)]  Loss: 3.430 (3.74)  Time: 0.863s, 1186.36/s  (0.840s, 1219.14/s)  LR: 9.926e-04  Data: 0.009 (0.045)
Train: 33 [ 100/1251 (  8%)]  Loss: 4.086 (3.85)  Time: 0.849s, 1206.38/s  (0.826s, 1239.68/s)  LR: 9.926e-04  Data: 0.010 (0.028)
Train: 33 [ 150/1251 ( 12%)]  Loss: 3.690 (3.81)  Time: 0.796s, 1286.31/s  (0.820s, 1249.34/s)  LR: 9.926e-04  Data: 0.010 (0.022)
Train: 33 [ 200/1251 ( 16%)]  Loss: 3.710 (3.79)  Time: 0.842s, 1216.29/s  (0.819s, 1250.47/s)  LR: 9.926e-04  Data: 0.010 (0.019)
Train: 33 [ 250/1251 ( 20%)]  Loss: 4.014 (3.83)  Time: 0.789s, 1298.45/s  (0.817s, 1253.46/s)  LR: 9.926e-04  Data: 0.010 (0.017)
Train: 33 [ 300/1251 ( 24%)]  Loss: 4.024 (3.86)  Time: 0.800s, 1280.48/s  (0.815s, 1255.88/s)  LR: 9.926e-04  Data: 0.010 (0.016)
Train: 33 [ 350/1251 ( 28%)]  Loss: 3.901 (3.86)  Time: 0.857s, 1194.80/s  (0.814s, 1258.15/s)  LR: 9.926e-04  Data: 0.010 (0.016)
Train: 33 [ 400/1251 ( 32%)]  Loss: 4.009 (3.88)  Time: 0.779s, 1314.22/s  (0.812s, 1260.59/s)  LR: 9.926e-04  Data: 0.010 (0.015)
Train: 33 [ 450/1251 ( 36%)]  Loss: 4.013 (3.89)  Time: 0.787s, 1301.01/s  (0.813s, 1260.15/s)  LR: 9.926e-04  Data: 0.010 (0.014)
Train: 33 [ 500/1251 ( 40%)]  Loss: 3.909 (3.89)  Time: 0.813s, 1259.03/s  (0.812s, 1261.27/s)  LR: 9.926e-04  Data: 0.011 (0.014)
Train: 33 [ 550/1251 ( 44%)]  Loss: 4.020 (3.90)  Time: 0.776s, 1320.36/s  (0.812s, 1260.92/s)  LR: 9.926e-04  Data: 0.010 (0.014)
Train: 33 [ 600/1251 ( 48%)]  Loss: 3.957 (3.91)  Time: 0.835s, 1226.57/s  (0.812s, 1261.02/s)  LR: 9.926e-04  Data: 0.010 (0.014)
Train: 33 [ 650/1251 ( 52%)]  Loss: 3.880 (3.91)  Time: 0.780s, 1313.50/s  (0.812s, 1260.74/s)  LR: 9.926e-04  Data: 0.010 (0.013)
Train: 33 [ 700/1251 ( 56%)]  Loss: 3.898 (3.91)  Time: 0.778s, 1316.67/s  (0.812s, 1260.44/s)  LR: 9.926e-04  Data: 0.010 (0.013)
Train: 33 [ 750/1251 ( 60%)]  Loss: 4.393 (3.94)  Time: 0.796s, 1286.04/s  (0.812s, 1261.27/s)  LR: 9.926e-04  Data: 0.015 (0.013)
Train: 33 [ 800/1251 ( 64%)]  Loss: 4.141 (3.95)  Time: 0.792s, 1292.21/s  (0.812s, 1261.45/s)  LR: 9.926e-04  Data: 0.010 (0.013)
Train: 33 [ 850/1251 ( 68%)]  Loss: 4.009 (3.95)  Time: 0.831s, 1232.74/s  (0.812s, 1261.51/s)  LR: 9.926e-04  Data: 0.009 (0.013)
Train: 33 [ 900/1251 ( 72%)]  Loss: 3.763 (3.94)  Time: 0.891s, 1149.64/s  (0.812s, 1261.78/s)  LR: 9.926e-04  Data: 0.010 (0.013)
Train: 33 [ 950/1251 ( 76%)]  Loss: 3.971 (3.94)  Time: 0.809s, 1266.50/s  (0.812s, 1261.47/s)  LR: 9.926e-04  Data: 0.013 (0.012)
Train: 33 [1000/1251 ( 80%)]  Loss: 4.043 (3.95)  Time: 0.776s, 1319.94/s  (0.812s, 1261.58/s)  LR: 9.926e-04  Data: 0.009 (0.012)
Train: 33 [1050/1251 ( 84%)]  Loss: 3.911 (3.95)  Time: 0.789s, 1297.59/s  (0.812s, 1261.23/s)  LR: 9.926e-04  Data: 0.014 (0.012)
Train: 33 [1100/1251 ( 88%)]  Loss: 4.128 (3.95)  Time: 0.778s, 1316.34/s  (0.811s, 1261.94/s)  LR: 9.926e-04  Data: 0.010 (0.012)
Train: 33 [1150/1251 ( 92%)]  Loss: 3.675 (3.94)  Time: 0.857s, 1194.74/s  (0.812s, 1261.77/s)  LR: 9.926e-04  Data: 0.009 (0.012)
Train: 33 [1200/1251 ( 96%)]  Loss: 4.017 (3.95)  Time: 0.787s, 1301.59/s  (0.811s, 1262.22/s)  LR: 9.926e-04  Data: 0.010 (0.012)
Train: 33 [1250/1251 (100%)]  Loss: 3.973 (3.95)  Time: 0.786s, 1303.63/s  (0.811s, 1262.47/s)  LR: 9.926e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.714 (1.714)  Loss:  1.0078 (1.0078)  Acc@1: 84.5703 (84.5703)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.193 (0.604)  Loss:  1.1123 (1.7562)  Acc@1: 82.0755 (67.0600)  Acc@5: 95.1651 (88.2720)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-33.pth.tar', 67.05999997070313)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-32.pth.tar', 66.42199999023437)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-29.pth.tar', 66.33599999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-31.pth.tar', 66.2939999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-28.pth.tar', 65.70800008789062)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-27.pth.tar', 65.60599998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-30.pth.tar', 65.568000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-26.pth.tar', 65.558)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-25.pth.tar', 65.2479999194336)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-24.pth.tar', 64.22199995117188)

Train: 34 [   0/1251 (  0%)]  Loss: 4.047 (4.05)  Time: 2.413s,  424.29/s  (2.413s,  424.29/s)  LR: 9.922e-04  Data: 1.670 (1.670)
Train: 34 [  50/1251 (  4%)]  Loss: 4.193 (4.12)  Time: 0.785s, 1304.08/s  (0.847s, 1209.04/s)  LR: 9.922e-04  Data: 0.009 (0.048)
Train: 34 [ 100/1251 (  8%)]  Loss: 4.093 (4.11)  Time: 0.826s, 1239.96/s  (0.827s, 1238.07/s)  LR: 9.922e-04  Data: 0.010 (0.029)
Train: 34 [ 150/1251 ( 12%)]  Loss: 3.849 (4.05)  Time: 0.799s, 1281.63/s  (0.821s, 1246.99/s)  LR: 9.922e-04  Data: 0.015 (0.023)
Train: 34 [ 200/1251 ( 16%)]  Loss: 4.257 (4.09)  Time: 0.823s, 1244.28/s  (0.818s, 1251.67/s)  LR: 9.922e-04  Data: 0.010 (0.020)
Train: 34 [ 250/1251 ( 20%)]  Loss: 4.320 (4.13)  Time: 0.787s, 1301.24/s  (0.816s, 1254.59/s)  LR: 9.922e-04  Data: 0.010 (0.018)
Train: 34 [ 300/1251 ( 24%)]  Loss: 3.970 (4.10)  Time: 0.809s, 1265.96/s  (0.815s, 1256.46/s)  LR: 9.922e-04  Data: 0.010 (0.017)
Train: 34 [ 350/1251 ( 28%)]  Loss: 3.842 (4.07)  Time: 0.800s, 1279.42/s  (0.815s, 1256.62/s)  LR: 9.922e-04  Data: 0.009 (0.016)
Train: 34 [ 400/1251 ( 32%)]  Loss: 4.568 (4.13)  Time: 0.784s, 1305.39/s  (0.815s, 1256.59/s)  LR: 9.922e-04  Data: 0.010 (0.015)
Train: 34 [ 450/1251 ( 36%)]  Loss: 4.208 (4.13)  Time: 0.824s, 1242.60/s  (0.814s, 1258.42/s)  LR: 9.922e-04  Data: 0.014 (0.015)
Train: 34 [ 500/1251 ( 40%)]  Loss: 4.159 (4.14)  Time: 0.824s, 1242.81/s  (0.814s, 1258.00/s)  LR: 9.922e-04  Data: 0.010 (0.014)
Train: 34 [ 550/1251 ( 44%)]  Loss: 4.002 (4.13)  Time: 0.795s, 1288.44/s  (0.813s, 1259.57/s)  LR: 9.922e-04  Data: 0.009 (0.014)
Train: 34 [ 600/1251 ( 48%)]  Loss: 3.911 (4.11)  Time: 0.794s, 1289.83/s  (0.813s, 1260.08/s)  LR: 9.922e-04  Data: 0.009 (0.014)
Train: 34 [ 650/1251 ( 52%)]  Loss: 4.273 (4.12)  Time: 0.821s, 1247.46/s  (0.812s, 1261.30/s)  LR: 9.922e-04  Data: 0.009 (0.014)
Train: 34 [ 700/1251 ( 56%)]  Loss: 3.854 (4.10)  Time: 0.818s, 1252.41/s  (0.812s, 1261.27/s)  LR: 9.922e-04  Data: 0.011 (0.013)
Train: 34 [ 750/1251 ( 60%)]  Loss: 3.693 (4.08)  Time: 0.834s, 1227.72/s  (0.812s, 1261.37/s)  LR: 9.922e-04  Data: 0.010 (0.013)
Train: 34 [ 800/1251 ( 64%)]  Loss: 3.763 (4.06)  Time: 0.850s, 1204.36/s  (0.811s, 1262.07/s)  LR: 9.922e-04  Data: 0.011 (0.013)
Train: 34 [ 850/1251 ( 68%)]  Loss: 4.258 (4.07)  Time: 0.774s, 1323.69/s  (0.811s, 1262.82/s)  LR: 9.922e-04  Data: 0.010 (0.013)
Train: 34 [ 900/1251 ( 72%)]  Loss: 3.838 (4.06)  Time: 0.819s, 1249.77/s  (0.811s, 1263.15/s)  LR: 9.922e-04  Data: 0.015 (0.013)
Train: 34 [ 950/1251 ( 76%)]  Loss: 3.991 (4.05)  Time: 0.834s, 1228.38/s  (0.811s, 1263.12/s)  LR: 9.922e-04  Data: 0.010 (0.013)
Train: 34 [1000/1251 ( 80%)]  Loss: 3.839 (4.04)  Time: 0.796s, 1286.82/s  (0.811s, 1263.20/s)  LR: 9.922e-04  Data: 0.015 (0.013)
Train: 34 [1050/1251 ( 84%)]  Loss: 3.915 (4.04)  Time: 0.838s, 1222.51/s  (0.811s, 1262.66/s)  LR: 9.922e-04  Data: 0.015 (0.012)
Train: 34 [1100/1251 ( 88%)]  Loss: 3.999 (4.04)  Time: 0.783s, 1307.03/s  (0.811s, 1262.34/s)  LR: 9.922e-04  Data: 0.012 (0.012)
Train: 34 [1150/1251 ( 92%)]  Loss: 4.122 (4.04)  Time: 0.781s, 1311.44/s  (0.811s, 1262.88/s)  LR: 9.922e-04  Data: 0.010 (0.012)
Train: 34 [1200/1251 ( 96%)]  Loss: 3.417 (4.02)  Time: 0.816s, 1255.27/s  (0.811s, 1262.79/s)  LR: 9.922e-04  Data: 0.011 (0.012)
Train: 34 [1250/1251 (100%)]  Loss: 3.958 (4.01)  Time: 0.765s, 1337.89/s  (0.811s, 1262.87/s)  LR: 9.922e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.676 (1.676)  Loss:  1.0273 (1.0273)  Acc@1: 83.3984 (83.3984)  Acc@5: 95.8008 (95.8008)
Test: [  48/48]  Time: 0.194 (0.601)  Loss:  1.2568 (1.6533)  Acc@1: 79.0094 (66.6740)  Acc@5: 93.2783 (88.1160)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-33.pth.tar', 67.05999997070313)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-34.pth.tar', 66.67400006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-32.pth.tar', 66.42199999023437)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-29.pth.tar', 66.33599999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-31.pth.tar', 66.2939999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-28.pth.tar', 65.70800008789062)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-27.pth.tar', 65.60599998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-30.pth.tar', 65.568000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-26.pth.tar', 65.558)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-25.pth.tar', 65.2479999194336)

Train: 35 [   0/1251 (  0%)]  Loss: 4.145 (4.15)  Time: 2.364s,  433.21/s  (2.364s,  433.21/s)  LR: 9.917e-04  Data: 1.630 (1.630)
Train: 35 [  50/1251 (  4%)]  Loss: 3.824 (3.98)  Time: 0.834s, 1227.63/s  (0.846s, 1209.79/s)  LR: 9.917e-04  Data: 0.010 (0.047)
Train: 35 [ 100/1251 (  8%)]  Loss: 4.026 (4.00)  Time: 0.778s, 1315.76/s  (0.826s, 1240.13/s)  LR: 9.917e-04  Data: 0.010 (0.029)
Train: 35 [ 150/1251 ( 12%)]  Loss: 4.138 (4.03)  Time: 0.872s, 1174.73/s  (0.823s, 1244.12/s)  LR: 9.917e-04  Data: 0.009 (0.023)
Train: 35 [ 200/1251 ( 16%)]  Loss: 3.469 (3.92)  Time: 0.841s, 1218.07/s  (0.822s, 1245.01/s)  LR: 9.917e-04  Data: 0.009 (0.020)
Train: 35 [ 250/1251 ( 20%)]  Loss: 3.987 (3.93)  Time: 0.774s, 1323.01/s  (0.820s, 1248.12/s)  LR: 9.917e-04  Data: 0.011 (0.018)
Train: 35 [ 300/1251 ( 24%)]  Loss: 4.000 (3.94)  Time: 0.805s, 1272.44/s  (0.819s, 1249.58/s)  LR: 9.917e-04  Data: 0.014 (0.017)
Train: 35 [ 350/1251 ( 28%)]  Loss: 3.832 (3.93)  Time: 0.869s, 1178.54/s  (0.818s, 1251.96/s)  LR: 9.917e-04  Data: 0.009 (0.016)
Train: 35 [ 400/1251 ( 32%)]  Loss: 3.952 (3.93)  Time: 0.778s, 1315.79/s  (0.817s, 1253.32/s)  LR: 9.917e-04  Data: 0.010 (0.015)
Train: 35 [ 450/1251 ( 36%)]  Loss: 4.075 (3.94)  Time: 0.792s, 1292.65/s  (0.816s, 1254.33/s)  LR: 9.917e-04  Data: 0.009 (0.015)
Train: 35 [ 500/1251 ( 40%)]  Loss: 3.978 (3.95)  Time: 0.813s, 1259.24/s  (0.815s, 1255.99/s)  LR: 9.917e-04  Data: 0.010 (0.014)
Train: 35 [ 550/1251 ( 44%)]  Loss: 4.127 (3.96)  Time: 0.799s, 1282.39/s  (0.815s, 1256.29/s)  LR: 9.917e-04  Data: 0.009 (0.014)
Train: 35 [ 600/1251 ( 48%)]  Loss: 3.920 (3.96)  Time: 0.860s, 1190.74/s  (0.814s, 1257.49/s)  LR: 9.917e-04  Data: 0.009 (0.014)
Train: 35 [ 650/1251 ( 52%)]  Loss: 4.132 (3.97)  Time: 0.861s, 1189.81/s  (0.814s, 1258.63/s)  LR: 9.917e-04  Data: 0.009 (0.013)
Train: 35 [ 700/1251 ( 56%)]  Loss: 3.957 (3.97)  Time: 0.790s, 1296.67/s  (0.813s, 1258.96/s)  LR: 9.917e-04  Data: 0.010 (0.013)
Train: 35 [ 750/1251 ( 60%)]  Loss: 3.938 (3.97)  Time: 0.860s, 1190.08/s  (0.813s, 1259.74/s)  LR: 9.917e-04  Data: 0.009 (0.013)
Train: 35 [ 800/1251 ( 64%)]  Loss: 4.163 (3.98)  Time: 0.809s, 1265.63/s  (0.813s, 1259.93/s)  LR: 9.917e-04  Data: 0.010 (0.013)
Train: 35 [ 850/1251 ( 68%)]  Loss: 3.949 (3.98)  Time: 0.830s, 1233.16/s  (0.812s, 1260.75/s)  LR: 9.917e-04  Data: 0.009 (0.013)
Train: 35 [ 900/1251 ( 72%)]  Loss: 4.073 (3.98)  Time: 0.794s, 1289.87/s  (0.812s, 1261.11/s)  LR: 9.917e-04  Data: 0.010 (0.013)
Train: 35 [ 950/1251 ( 76%)]  Loss: 3.972 (3.98)  Time: 0.786s, 1303.04/s  (0.812s, 1261.54/s)  LR: 9.917e-04  Data: 0.009 (0.013)
Train: 35 [1000/1251 ( 80%)]  Loss: 4.205 (3.99)  Time: 0.835s, 1226.68/s  (0.812s, 1261.62/s)  LR: 9.917e-04  Data: 0.009 (0.012)
Train: 35 [1050/1251 ( 84%)]  Loss: 3.922 (3.99)  Time: 0.804s, 1273.66/s  (0.811s, 1262.14/s)  LR: 9.917e-04  Data: 0.010 (0.012)
Train: 35 [1100/1251 ( 88%)]  Loss: 4.014 (3.99)  Time: 0.787s, 1300.90/s  (0.811s, 1262.04/s)  LR: 9.917e-04  Data: 0.009 (0.012)
Train: 35 [1150/1251 ( 92%)]  Loss: 4.149 (4.00)  Time: 0.848s, 1207.80/s  (0.811s, 1262.34/s)  LR: 9.917e-04  Data: 0.009 (0.012)
Train: 35 [1200/1251 ( 96%)]  Loss: 4.241 (4.01)  Time: 0.856s, 1196.62/s  (0.811s, 1262.31/s)  LR: 9.917e-04  Data: 0.009 (0.012)
Train: 35 [1250/1251 (100%)]  Loss: 3.811 (4.00)  Time: 0.765s, 1338.55/s  (0.811s, 1262.34/s)  LR: 9.917e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.716 (1.716)  Loss:  1.0566 (1.0566)  Acc@1: 85.2539 (85.2539)  Acc@5: 95.8008 (95.8008)
Test: [  48/48]  Time: 0.194 (0.613)  Loss:  1.0527 (1.6308)  Acc@1: 80.0708 (67.4360)  Acc@5: 93.6321 (88.4520)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-35.pth.tar', 67.43600013427735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-33.pth.tar', 67.05999997070313)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-34.pth.tar', 66.67400006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-32.pth.tar', 66.42199999023437)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-29.pth.tar', 66.33599999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-31.pth.tar', 66.2939999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-28.pth.tar', 65.70800008789062)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-27.pth.tar', 65.60599998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-30.pth.tar', 65.568000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-26.pth.tar', 65.558)

Train: 36 [   0/1251 (  0%)]  Loss: 3.912 (3.91)  Time: 2.317s,  441.90/s  (2.317s,  441.90/s)  LR: 9.912e-04  Data: 1.592 (1.592)
Train: 36 [  50/1251 (  4%)]  Loss: 4.181 (4.05)  Time: 0.788s, 1299.70/s  (0.838s, 1222.61/s)  LR: 9.912e-04  Data: 0.011 (0.043)
Train: 36 [ 100/1251 (  8%)]  Loss: 4.106 (4.07)  Time: 0.879s, 1164.59/s  (0.821s, 1247.30/s)  LR: 9.912e-04  Data: 0.013 (0.027)
Train: 36 [ 150/1251 ( 12%)]  Loss: 4.350 (4.14)  Time: 0.826s, 1239.95/s  (0.816s, 1255.28/s)  LR: 9.912e-04  Data: 0.018 (0.022)
Train: 36 [ 200/1251 ( 16%)]  Loss: 3.772 (4.06)  Time: 0.828s, 1236.42/s  (0.821s, 1247.05/s)  LR: 9.912e-04  Data: 0.014 (0.020)
Train: 36 [ 250/1251 ( 20%)]  Loss: 4.142 (4.08)  Time: 0.778s, 1315.78/s  (0.816s, 1255.02/s)  LR: 9.912e-04  Data: 0.010 (0.018)
Train: 36 [ 300/1251 ( 24%)]  Loss: 3.863 (4.05)  Time: 0.825s, 1241.90/s  (0.810s, 1263.66/s)  LR: 9.912e-04  Data: 0.010 (0.017)
Train: 36 [ 350/1251 ( 28%)]  Loss: 3.959 (4.04)  Time: 0.813s, 1259.47/s  (0.810s, 1263.96/s)  LR: 9.912e-04  Data: 0.010 (0.016)
Train: 36 [ 400/1251 ( 32%)]  Loss: 3.923 (4.02)  Time: 0.779s, 1313.77/s  (0.811s, 1263.41/s)  LR: 9.912e-04  Data: 0.009 (0.015)
Train: 36 [ 450/1251 ( 36%)]  Loss: 3.842 (4.00)  Time: 0.804s, 1273.23/s  (0.810s, 1263.91/s)  LR: 9.912e-04  Data: 0.010 (0.014)
Train: 36 [ 500/1251 ( 40%)]  Loss: 3.708 (3.98)  Time: 0.807s, 1268.45/s  (0.810s, 1264.39/s)  LR: 9.912e-04  Data: 0.010 (0.014)
Train: 36 [ 550/1251 ( 44%)]  Loss: 3.709 (3.96)  Time: 0.807s, 1269.15/s  (0.810s, 1263.90/s)  LR: 9.912e-04  Data: 0.009 (0.014)
Train: 36 [ 600/1251 ( 48%)]  Loss: 3.772 (3.94)  Time: 0.781s, 1311.35/s  (0.810s, 1263.75/s)  LR: 9.912e-04  Data: 0.010 (0.013)
Train: 36 [ 650/1251 ( 52%)]  Loss: 4.213 (3.96)  Time: 0.810s, 1264.83/s  (0.810s, 1263.82/s)  LR: 9.912e-04  Data: 0.010 (0.013)
Train: 36 [ 700/1251 ( 56%)]  Loss: 3.960 (3.96)  Time: 0.814s, 1258.76/s  (0.810s, 1263.43/s)  LR: 9.912e-04  Data: 0.010 (0.013)
Train: 36 [ 750/1251 ( 60%)]  Loss: 4.131 (3.97)  Time: 0.843s, 1214.12/s  (0.811s, 1263.02/s)  LR: 9.912e-04  Data: 0.009 (0.013)
Train: 36 [ 800/1251 ( 64%)]  Loss: 3.800 (3.96)  Time: 0.796s, 1286.35/s  (0.811s, 1263.33/s)  LR: 9.912e-04  Data: 0.012 (0.013)
Train: 36 [ 850/1251 ( 68%)]  Loss: 4.187 (3.97)  Time: 0.827s, 1238.42/s  (0.811s, 1263.39/s)  LR: 9.912e-04  Data: 0.010 (0.013)
Train: 36 [ 900/1251 ( 72%)]  Loss: 4.284 (3.99)  Time: 0.864s, 1185.61/s  (0.811s, 1263.27/s)  LR: 9.912e-04  Data: 0.010 (0.012)
Train: 36 [ 950/1251 ( 76%)]  Loss: 4.003 (3.99)  Time: 0.838s, 1222.50/s  (0.811s, 1263.07/s)  LR: 9.912e-04  Data: 0.010 (0.012)
Train: 36 [1000/1251 ( 80%)]  Loss: 3.780 (3.98)  Time: 0.849s, 1205.95/s  (0.811s, 1263.13/s)  LR: 9.912e-04  Data: 0.009 (0.012)
Train: 36 [1050/1251 ( 84%)]  Loss: 3.650 (3.97)  Time: 0.782s, 1309.30/s  (0.810s, 1263.64/s)  LR: 9.912e-04  Data: 0.011 (0.012)
Train: 36 [1100/1251 ( 88%)]  Loss: 3.820 (3.96)  Time: 0.803s, 1274.78/s  (0.810s, 1263.51/s)  LR: 9.912e-04  Data: 0.009 (0.012)
Train: 36 [1150/1251 ( 92%)]  Loss: 3.751 (3.95)  Time: 0.783s, 1307.87/s  (0.810s, 1263.90/s)  LR: 9.912e-04  Data: 0.010 (0.012)
Train: 36 [1200/1251 ( 96%)]  Loss: 3.671 (3.94)  Time: 0.808s, 1267.25/s  (0.810s, 1264.20/s)  LR: 9.912e-04  Data: 0.009 (0.012)
Train: 36 [1250/1251 (100%)]  Loss: 4.003 (3.94)  Time: 0.764s, 1340.71/s  (0.810s, 1264.77/s)  LR: 9.912e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.744 (1.744)  Loss:  1.0762 (1.0762)  Acc@1: 82.9102 (82.9102)  Acc@5: 95.2148 (95.2148)
Test: [  48/48]  Time: 0.194 (0.617)  Loss:  0.9482 (1.6527)  Acc@1: 81.0142 (66.9920)  Acc@5: 95.5189 (88.2600)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-35.pth.tar', 67.43600013427735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-33.pth.tar', 67.05999997070313)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-36.pth.tar', 66.99200002685546)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-34.pth.tar', 66.67400006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-32.pth.tar', 66.42199999023437)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-29.pth.tar', 66.33599999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-31.pth.tar', 66.2939999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-28.pth.tar', 65.70800008789062)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-27.pth.tar', 65.60599998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-30.pth.tar', 65.568000078125)

Train: 37 [   0/1251 (  0%)]  Loss: 3.872 (3.87)  Time: 2.400s,  426.68/s  (2.400s,  426.68/s)  LR: 9.907e-04  Data: 1.648 (1.648)
Train: 37 [  50/1251 (  4%)]  Loss: 4.043 (3.96)  Time: 0.809s, 1266.49/s  (0.853s, 1200.44/s)  LR: 9.907e-04  Data: 0.009 (0.049)
Train: 37 [ 100/1251 (  8%)]  Loss: 4.071 (4.00)  Time: 0.837s, 1222.93/s  (0.829s, 1235.43/s)  LR: 9.907e-04  Data: 0.009 (0.030)
Train: 37 [ 150/1251 ( 12%)]  Loss: 3.545 (3.88)  Time: 0.801s, 1278.48/s  (0.823s, 1244.38/s)  LR: 9.907e-04  Data: 0.012 (0.023)
Train: 37 [ 200/1251 ( 16%)]  Loss: 3.776 (3.86)  Time: 0.782s, 1309.77/s  (0.819s, 1250.64/s)  LR: 9.907e-04  Data: 0.010 (0.020)
Train: 37 [ 250/1251 ( 20%)]  Loss: 3.982 (3.88)  Time: 0.888s, 1152.96/s  (0.818s, 1252.20/s)  LR: 9.907e-04  Data: 0.009 (0.018)
Train: 37 [ 300/1251 ( 24%)]  Loss: 3.838 (3.88)  Time: 0.822s, 1245.99/s  (0.816s, 1254.41/s)  LR: 9.907e-04  Data: 0.011 (0.017)
Train: 37 [ 350/1251 ( 28%)]  Loss: 4.146 (3.91)  Time: 0.858s, 1193.27/s  (0.816s, 1255.20/s)  LR: 9.907e-04  Data: 0.009 (0.016)
Train: 37 [ 400/1251 ( 32%)]  Loss: 4.119 (3.93)  Time: 0.826s, 1240.22/s  (0.815s, 1256.72/s)  LR: 9.907e-04  Data: 0.010 (0.015)
Train: 37 [ 450/1251 ( 36%)]  Loss: 3.474 (3.89)  Time: 0.817s, 1253.54/s  (0.814s, 1258.73/s)  LR: 9.907e-04  Data: 0.020 (0.015)
Train: 37 [ 500/1251 ( 40%)]  Loss: 4.171 (3.91)  Time: 0.786s, 1303.27/s  (0.813s, 1259.79/s)  LR: 9.907e-04  Data: 0.009 (0.014)
Train: 37 [ 550/1251 ( 44%)]  Loss: 3.919 (3.91)  Time: 0.779s, 1314.46/s  (0.813s, 1259.30/s)  LR: 9.907e-04  Data: 0.010 (0.014)
Train: 37 [ 600/1251 ( 48%)]  Loss: 4.360 (3.95)  Time: 0.789s, 1297.73/s  (0.813s, 1259.02/s)  LR: 9.907e-04  Data: 0.009 (0.014)
Train: 37 [ 650/1251 ( 52%)]  Loss: 4.266 (3.97)  Time: 0.790s, 1295.68/s  (0.813s, 1258.83/s)  LR: 9.907e-04  Data: 0.010 (0.014)
Train: 37 [ 700/1251 ( 56%)]  Loss: 3.985 (3.97)  Time: 0.789s, 1298.51/s  (0.813s, 1259.69/s)  LR: 9.907e-04  Data: 0.012 (0.013)
Train: 37 [ 750/1251 ( 60%)]  Loss: 3.535 (3.94)  Time: 0.780s, 1312.22/s  (0.812s, 1260.36/s)  LR: 9.907e-04  Data: 0.014 (0.013)
Train: 37 [ 800/1251 ( 64%)]  Loss: 4.049 (3.95)  Time: 0.797s, 1284.77/s  (0.812s, 1260.46/s)  LR: 9.907e-04  Data: 0.010 (0.013)
Train: 37 [ 850/1251 ( 68%)]  Loss: 3.703 (3.94)  Time: 0.844s, 1213.03/s  (0.812s, 1260.87/s)  LR: 9.907e-04  Data: 0.009 (0.013)
Train: 37 [ 900/1251 ( 72%)]  Loss: 3.833 (3.93)  Time: 0.907s, 1128.94/s  (0.813s, 1259.61/s)  LR: 9.907e-04  Data: 0.015 (0.013)
Train: 37 [ 950/1251 ( 76%)]  Loss: 4.306 (3.95)  Time: 0.800s, 1279.82/s  (0.813s, 1260.07/s)  LR: 9.907e-04  Data: 0.010 (0.013)
Train: 37 [1000/1251 ( 80%)]  Loss: 4.109 (3.96)  Time: 0.824s, 1242.52/s  (0.813s, 1260.31/s)  LR: 9.907e-04  Data: 0.013 (0.012)
Train: 37 [1050/1251 ( 84%)]  Loss: 4.174 (3.97)  Time: 0.858s, 1194.10/s  (0.813s, 1260.23/s)  LR: 9.907e-04  Data: 0.013 (0.012)
Train: 37 [1100/1251 ( 88%)]  Loss: 3.966 (3.97)  Time: 0.779s, 1314.27/s  (0.812s, 1260.61/s)  LR: 9.907e-04  Data: 0.009 (0.012)
Train: 37 [1150/1251 ( 92%)]  Loss: 4.005 (3.97)  Time: 0.793s, 1292.00/s  (0.812s, 1260.54/s)  LR: 9.907e-04  Data: 0.018 (0.012)
Train: 37 [1200/1251 ( 96%)]  Loss: 3.594 (3.95)  Time: 0.796s, 1285.97/s  (0.812s, 1260.85/s)  LR: 9.907e-04  Data: 0.012 (0.012)
Train: 37 [1250/1251 (100%)]  Loss: 4.324 (3.97)  Time: 0.764s, 1339.96/s  (0.812s, 1260.71/s)  LR: 9.907e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.724 (1.724)  Loss:  1.1055 (1.1055)  Acc@1: 83.3984 (83.3984)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.194 (0.603)  Loss:  1.1992 (1.6672)  Acc@1: 81.1321 (67.4800)  Acc@5: 94.3396 (88.5520)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-37.pth.tar', 67.480000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-35.pth.tar', 67.43600013427735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-33.pth.tar', 67.05999997070313)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-36.pth.tar', 66.99200002685546)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-34.pth.tar', 66.67400006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-32.pth.tar', 66.42199999023437)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-29.pth.tar', 66.33599999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-31.pth.tar', 66.2939999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-28.pth.tar', 65.70800008789062)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-27.pth.tar', 65.60599998046875)

Train: 38 [   0/1251 (  0%)]  Loss: 3.927 (3.93)  Time: 2.452s,  417.68/s  (2.452s,  417.68/s)  LR: 9.902e-04  Data: 1.661 (1.661)
Train: 38 [  50/1251 (  4%)]  Loss: 3.883 (3.91)  Time: 0.777s, 1317.23/s  (0.852s, 1202.57/s)  LR: 9.902e-04  Data: 0.009 (0.047)
Train: 38 [ 100/1251 (  8%)]  Loss: 3.892 (3.90)  Time: 0.811s, 1262.35/s  (0.830s, 1233.46/s)  LR: 9.902e-04  Data: 0.013 (0.029)
Train: 38 [ 150/1251 ( 12%)]  Loss: 3.975 (3.92)  Time: 0.777s, 1317.19/s  (0.822s, 1246.21/s)  LR: 9.902e-04  Data: 0.010 (0.023)
Train: 38 [ 200/1251 ( 16%)]  Loss: 3.809 (3.90)  Time: 0.799s, 1281.12/s  (0.820s, 1248.13/s)  LR: 9.902e-04  Data: 0.013 (0.020)
Train: 38 [ 250/1251 ( 20%)]  Loss: 3.787 (3.88)  Time: 0.777s, 1318.28/s  (0.818s, 1251.97/s)  LR: 9.902e-04  Data: 0.010 (0.018)
Train: 38 [ 300/1251 ( 24%)]  Loss: 4.122 (3.91)  Time: 0.779s, 1314.68/s  (0.817s, 1253.73/s)  LR: 9.902e-04  Data: 0.010 (0.017)
Train: 38 [ 350/1251 ( 28%)]  Loss: 4.302 (3.96)  Time: 0.782s, 1309.55/s  (0.815s, 1256.53/s)  LR: 9.902e-04  Data: 0.009 (0.016)
Train: 38 [ 400/1251 ( 32%)]  Loss: 4.166 (3.98)  Time: 0.790s, 1295.39/s  (0.815s, 1256.43/s)  LR: 9.902e-04  Data: 0.012 (0.015)
Train: 38 [ 450/1251 ( 36%)]  Loss: 3.866 (3.97)  Time: 0.776s, 1320.02/s  (0.814s, 1258.29/s)  LR: 9.902e-04  Data: 0.011 (0.015)
Train: 38 [ 500/1251 ( 40%)]  Loss: 3.925 (3.97)  Time: 0.808s, 1267.54/s  (0.813s, 1259.59/s)  LR: 9.902e-04  Data: 0.009 (0.014)
Train: 38 [ 550/1251 ( 44%)]  Loss: 4.007 (3.97)  Time: 0.839s, 1220.06/s  (0.813s, 1259.26/s)  LR: 9.902e-04  Data: 0.009 (0.014)
Train: 38 [ 600/1251 ( 48%)]  Loss: 4.126 (3.98)  Time: 0.781s, 1311.79/s  (0.813s, 1259.50/s)  LR: 9.902e-04  Data: 0.009 (0.014)
Train: 38 [ 650/1251 ( 52%)]  Loss: 3.903 (3.98)  Time: 0.778s, 1316.10/s  (0.812s, 1260.60/s)  LR: 9.902e-04  Data: 0.010 (0.014)
Train: 38 [ 700/1251 ( 56%)]  Loss: 4.326 (4.00)  Time: 0.799s, 1281.44/s  (0.812s, 1261.66/s)  LR: 9.902e-04  Data: 0.010 (0.013)
Train: 38 [ 750/1251 ( 60%)]  Loss: 3.932 (4.00)  Time: 0.831s, 1231.61/s  (0.811s, 1261.94/s)  LR: 9.902e-04  Data: 0.009 (0.013)
Train: 38 [ 800/1251 ( 64%)]  Loss: 4.254 (4.01)  Time: 0.839s, 1220.33/s  (0.811s, 1262.36/s)  LR: 9.902e-04  Data: 0.011 (0.013)
Train: 38 [ 850/1251 ( 68%)]  Loss: 3.965 (4.01)  Time: 0.810s, 1264.61/s  (0.811s, 1262.39/s)  LR: 9.902e-04  Data: 0.010 (0.013)
Train: 38 [ 900/1251 ( 72%)]  Loss: 3.655 (3.99)  Time: 0.864s, 1185.70/s  (0.811s, 1262.41/s)  LR: 9.902e-04  Data: 0.010 (0.013)
Train: 38 [ 950/1251 ( 76%)]  Loss: 4.279 (4.01)  Time: 0.780s, 1313.57/s  (0.811s, 1262.44/s)  LR: 9.902e-04  Data: 0.010 (0.013)
Train: 38 [1000/1251 ( 80%)]  Loss: 3.904 (4.00)  Time: 0.824s, 1243.37/s  (0.811s, 1262.89/s)  LR: 9.902e-04  Data: 0.010 (0.013)
Train: 38 [1050/1251 ( 84%)]  Loss: 4.264 (4.01)  Time: 0.788s, 1299.81/s  (0.811s, 1262.80/s)  LR: 9.902e-04  Data: 0.009 (0.013)
Train: 38 [1100/1251 ( 88%)]  Loss: 3.908 (4.01)  Time: 0.779s, 1315.27/s  (0.811s, 1263.10/s)  LR: 9.902e-04  Data: 0.010 (0.012)
Train: 38 [1150/1251 ( 92%)]  Loss: 4.285 (4.02)  Time: 0.814s, 1258.64/s  (0.811s, 1263.25/s)  LR: 9.902e-04  Data: 0.011 (0.012)
Train: 38 [1200/1251 ( 96%)]  Loss: 4.189 (4.03)  Time: 0.790s, 1296.86/s  (0.811s, 1263.41/s)  LR: 9.902e-04  Data: 0.010 (0.012)
Train: 38 [1250/1251 (100%)]  Loss: 4.076 (4.03)  Time: 0.803s, 1275.90/s  (0.811s, 1263.27/s)  LR: 9.902e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.676 (1.676)  Loss:  1.1006 (1.1006)  Acc@1: 85.1562 (85.1562)  Acc@5: 95.4102 (95.4102)
Test: [  48/48]  Time: 0.194 (0.609)  Loss:  1.1836 (1.6822)  Acc@1: 82.1934 (67.8380)  Acc@5: 94.1038 (88.6600)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-38.pth.tar', 67.83800002197266)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-37.pth.tar', 67.480000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-35.pth.tar', 67.43600013427735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-33.pth.tar', 67.05999997070313)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-36.pth.tar', 66.99200002685546)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-34.pth.tar', 66.67400006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-32.pth.tar', 66.42199999023437)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-29.pth.tar', 66.33599999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-31.pth.tar', 66.2939999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-28.pth.tar', 65.70800008789062)

Train: 39 [   0/1251 (  0%)]  Loss: 3.991 (3.99)  Time: 2.353s,  435.18/s  (2.353s,  435.18/s)  LR: 9.897e-04  Data: 1.584 (1.584)
Train: 39 [  50/1251 (  4%)]  Loss: 3.991 (3.99)  Time: 0.836s, 1225.36/s  (0.841s, 1217.97/s)  LR: 9.897e-04  Data: 0.013 (0.045)
Train: 39 [ 100/1251 (  8%)]  Loss: 4.125 (4.04)  Time: 0.839s, 1219.97/s  (0.827s, 1238.19/s)  LR: 9.897e-04  Data: 0.010 (0.028)
Train: 39 [ 150/1251 ( 12%)]  Loss: 3.646 (3.94)  Time: 0.938s, 1091.75/s  (0.822s, 1246.09/s)  LR: 9.897e-04  Data: 0.009 (0.022)
Train: 39 [ 200/1251 ( 16%)]  Loss: 3.817 (3.91)  Time: 0.874s, 1171.61/s  (0.819s, 1250.30/s)  LR: 9.897e-04  Data: 0.010 (0.019)
Train: 39 [ 250/1251 ( 20%)]  Loss: 3.954 (3.92)  Time: 0.837s, 1222.82/s  (0.817s, 1253.88/s)  LR: 9.897e-04  Data: 0.010 (0.018)
Train: 39 [ 300/1251 ( 24%)]  Loss: 4.050 (3.94)  Time: 0.827s, 1237.61/s  (0.815s, 1256.77/s)  LR: 9.897e-04  Data: 0.010 (0.016)
Train: 39 [ 350/1251 ( 28%)]  Loss: 3.808 (3.92)  Time: 0.839s, 1220.47/s  (0.813s, 1258.79/s)  LR: 9.897e-04  Data: 0.009 (0.016)
Train: 39 [ 400/1251 ( 32%)]  Loss: 3.898 (3.92)  Time: 0.777s, 1318.25/s  (0.812s, 1261.03/s)  LR: 9.897e-04  Data: 0.010 (0.015)
Train: 39 [ 450/1251 ( 36%)]  Loss: 3.900 (3.92)  Time: 0.784s, 1305.84/s  (0.811s, 1262.25/s)  LR: 9.897e-04  Data: 0.010 (0.015)
Train: 39 [ 500/1251 ( 40%)]  Loss: 3.871 (3.91)  Time: 0.824s, 1242.43/s  (0.811s, 1262.32/s)  LR: 9.897e-04  Data: 0.010 (0.014)
Train: 39 [ 550/1251 ( 44%)]  Loss: 4.107 (3.93)  Time: 0.821s, 1247.65/s  (0.811s, 1262.25/s)  LR: 9.897e-04  Data: 0.012 (0.014)
Train: 39 [ 600/1251 ( 48%)]  Loss: 4.119 (3.94)  Time: 0.800s, 1279.67/s  (0.811s, 1262.73/s)  LR: 9.897e-04  Data: 0.010 (0.014)
Train: 39 [ 650/1251 ( 52%)]  Loss: 4.413 (3.98)  Time: 0.792s, 1293.40/s  (0.811s, 1262.59/s)  LR: 9.897e-04  Data: 0.010 (0.013)
Train: 39 [ 700/1251 ( 56%)]  Loss: 4.054 (3.98)  Time: 0.806s, 1269.88/s  (0.811s, 1263.03/s)  LR: 9.897e-04  Data: 0.010 (0.013)
Train: 39 [ 750/1251 ( 60%)]  Loss: 4.136 (3.99)  Time: 0.867s, 1181.32/s  (0.811s, 1263.02/s)  LR: 9.897e-04  Data: 0.009 (0.013)
Train: 39 [ 800/1251 ( 64%)]  Loss: 4.049 (4.00)  Time: 0.818s, 1251.66/s  (0.811s, 1263.41/s)  LR: 9.897e-04  Data: 0.010 (0.013)
Train: 39 [ 850/1251 ( 68%)]  Loss: 4.261 (4.01)  Time: 0.821s, 1247.14/s  (0.810s, 1263.54/s)  LR: 9.897e-04  Data: 0.012 (0.013)
Train: 39 [ 900/1251 ( 72%)]  Loss: 4.202 (4.02)  Time: 0.780s, 1312.44/s  (0.811s, 1263.29/s)  LR: 9.897e-04  Data: 0.010 (0.013)
Train: 39 [ 950/1251 ( 76%)]  Loss: 3.687 (4.00)  Time: 0.783s, 1308.03/s  (0.811s, 1263.34/s)  LR: 9.897e-04  Data: 0.014 (0.012)
Train: 39 [1000/1251 ( 80%)]  Loss: 3.726 (3.99)  Time: 0.891s, 1148.97/s  (0.810s, 1263.99/s)  LR: 9.897e-04  Data: 0.010 (0.012)
Train: 39 [1050/1251 ( 84%)]  Loss: 4.024 (3.99)  Time: 0.778s, 1317.03/s  (0.810s, 1263.77/s)  LR: 9.897e-04  Data: 0.009 (0.012)
Train: 39 [1100/1251 ( 88%)]  Loss: 4.508 (4.01)  Time: 0.878s, 1166.90/s  (0.810s, 1263.74/s)  LR: 9.897e-04  Data: 0.010 (0.012)
Train: 39 [1150/1251 ( 92%)]  Loss: 4.144 (4.02)  Time: 0.847s, 1208.63/s  (0.810s, 1263.63/s)  LR: 9.897e-04  Data: 0.010 (0.012)
Train: 39 [1200/1251 ( 96%)]  Loss: 4.216 (4.03)  Time: 0.776s, 1319.58/s  (0.810s, 1263.71/s)  LR: 9.897e-04  Data: 0.009 (0.012)
Train: 39 [1250/1251 (100%)]  Loss: 4.095 (4.03)  Time: 0.824s, 1242.01/s  (0.810s, 1263.44/s)  LR: 9.897e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.693 (1.693)  Loss:  1.0811 (1.0811)  Acc@1: 83.7891 (83.7891)  Acc@5: 95.2148 (95.2148)
Test: [  48/48]  Time: 0.194 (0.601)  Loss:  1.1494 (1.7078)  Acc@1: 80.0708 (67.5380)  Acc@5: 93.8679 (88.3660)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-38.pth.tar', 67.83800002197266)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-39.pth.tar', 67.53800000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-37.pth.tar', 67.480000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-35.pth.tar', 67.43600013427735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-33.pth.tar', 67.05999997070313)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-36.pth.tar', 66.99200002685546)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-34.pth.tar', 66.67400006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-32.pth.tar', 66.42199999023437)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-29.pth.tar', 66.33599999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-31.pth.tar', 66.2939999267578)

Train: 40 [   0/1251 (  0%)]  Loss: 3.860 (3.86)  Time: 2.281s,  448.86/s  (2.281s,  448.86/s)  LR: 9.892e-04  Data: 1.535 (1.535)
Train: 40 [  50/1251 (  4%)]  Loss: 4.333 (4.10)  Time: 0.857s, 1194.89/s  (0.844s, 1213.51/s)  LR: 9.892e-04  Data: 0.010 (0.046)
Train: 40 [ 100/1251 (  8%)]  Loss: 4.261 (4.15)  Time: 0.780s, 1313.00/s  (0.823s, 1244.91/s)  LR: 9.892e-04  Data: 0.010 (0.028)
Train: 40 [ 150/1251 ( 12%)]  Loss: 4.222 (4.17)  Time: 0.773s, 1324.33/s  (0.816s, 1254.45/s)  LR: 9.892e-04  Data: 0.010 (0.023)
Train: 40 [ 200/1251 ( 16%)]  Loss: 3.918 (4.12)  Time: 0.832s, 1230.47/s  (0.815s, 1256.12/s)  LR: 9.892e-04  Data: 0.012 (0.020)
Train: 40 [ 250/1251 ( 20%)]  Loss: 3.662 (4.04)  Time: 0.776s, 1319.66/s  (0.814s, 1257.55/s)  LR: 9.892e-04  Data: 0.009 (0.018)
Train: 40 [ 300/1251 ( 24%)]  Loss: 4.212 (4.07)  Time: 0.836s, 1224.54/s  (0.812s, 1260.63/s)  LR: 9.892e-04  Data: 0.009 (0.017)
Train: 40 [ 350/1251 ( 28%)]  Loss: 3.905 (4.05)  Time: 0.799s, 1282.10/s  (0.811s, 1261.86/s)  LR: 9.892e-04  Data: 0.009 (0.016)
Train: 40 [ 400/1251 ( 32%)]  Loss: 4.061 (4.05)  Time: 0.781s, 1310.74/s  (0.811s, 1262.79/s)  LR: 9.892e-04  Data: 0.010 (0.015)
Train: 40 [ 450/1251 ( 36%)]  Loss: 3.888 (4.03)  Time: 0.871s, 1175.18/s  (0.811s, 1262.72/s)  LR: 9.892e-04  Data: 0.011 (0.015)
Train: 40 [ 500/1251 ( 40%)]  Loss: 4.090 (4.04)  Time: 0.785s, 1304.05/s  (0.811s, 1262.91/s)  LR: 9.892e-04  Data: 0.010 (0.014)
Train: 40 [ 550/1251 ( 44%)]  Loss: 3.692 (4.01)  Time: 0.792s, 1292.91/s  (0.811s, 1263.01/s)  LR: 9.892e-04  Data: 0.011 (0.014)
Train: 40 [ 600/1251 ( 48%)]  Loss: 3.788 (3.99)  Time: 0.805s, 1271.99/s  (0.811s, 1263.32/s)  LR: 9.892e-04  Data: 0.010 (0.014)
Train: 40 [ 650/1251 ( 52%)]  Loss: 4.093 (4.00)  Time: 0.827s, 1238.94/s  (0.810s, 1263.95/s)  LR: 9.892e-04  Data: 0.010 (0.013)
Train: 40 [ 700/1251 ( 56%)]  Loss: 3.962 (4.00)  Time: 0.779s, 1315.25/s  (0.810s, 1264.00/s)  LR: 9.892e-04  Data: 0.010 (0.013)
Train: 40 [ 750/1251 ( 60%)]  Loss: 4.036 (4.00)  Time: 0.778s, 1316.62/s  (0.810s, 1264.64/s)  LR: 9.892e-04  Data: 0.009 (0.013)
Train: 40 [ 800/1251 ( 64%)]  Loss: 3.757 (3.98)  Time: 0.830s, 1234.36/s  (0.809s, 1265.60/s)  LR: 9.892e-04  Data: 0.009 (0.013)
Train: 40 [ 850/1251 ( 68%)]  Loss: 4.080 (3.99)  Time: 0.844s, 1213.88/s  (0.809s, 1266.11/s)  LR: 9.892e-04  Data: 0.019 (0.013)
Train: 40 [ 900/1251 ( 72%)]  Loss: 3.722 (3.98)  Time: 0.798s, 1283.26/s  (0.809s, 1265.95/s)  LR: 9.892e-04  Data: 0.009 (0.013)
Train: 40 [ 950/1251 ( 76%)]  Loss: 3.935 (3.97)  Time: 0.789s, 1298.24/s  (0.809s, 1265.93/s)  LR: 9.892e-04  Data: 0.016 (0.012)
Train: 40 [1000/1251 ( 80%)]  Loss: 3.664 (3.96)  Time: 0.846s, 1210.42/s  (0.809s, 1266.32/s)  LR: 9.892e-04  Data: 0.009 (0.012)
Train: 40 [1050/1251 ( 84%)]  Loss: 3.718 (3.95)  Time: 0.837s, 1222.96/s  (0.809s, 1266.04/s)  LR: 9.892e-04  Data: 0.010 (0.012)
Train: 40 [1100/1251 ( 88%)]  Loss: 3.859 (3.94)  Time: 0.785s, 1304.89/s  (0.809s, 1265.92/s)  LR: 9.892e-04  Data: 0.014 (0.012)
Train: 40 [1150/1251 ( 92%)]  Loss: 3.953 (3.94)  Time: 0.847s, 1209.60/s  (0.809s, 1265.67/s)  LR: 9.892e-04  Data: 0.010 (0.012)
Train: 40 [1200/1251 ( 96%)]  Loss: 3.857 (3.94)  Time: 0.785s, 1304.16/s  (0.809s, 1265.03/s)  LR: 9.892e-04  Data: 0.010 (0.012)
Train: 40 [1250/1251 (100%)]  Loss: 3.914 (3.94)  Time: 0.762s, 1343.19/s  (0.809s, 1265.32/s)  LR: 9.892e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.617 (1.617)  Loss:  0.9448 (0.9448)  Acc@1: 83.4961 (83.4961)  Acc@5: 94.7266 (94.7266)
Test: [  48/48]  Time: 0.194 (0.571)  Loss:  1.1533 (1.5754)  Acc@1: 80.0708 (67.2200)  Acc@5: 93.2783 (88.6620)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-38.pth.tar', 67.83800002197266)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-39.pth.tar', 67.53800000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-37.pth.tar', 67.480000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-35.pth.tar', 67.43600013427735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-40.pth.tar', 67.2200000048828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-33.pth.tar', 67.05999997070313)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-36.pth.tar', 66.99200002685546)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-34.pth.tar', 66.67400006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-32.pth.tar', 66.42199999023437)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-29.pth.tar', 66.33599999267578)

Train: 41 [   0/1251 (  0%)]  Loss: 3.758 (3.76)  Time: 2.394s,  427.69/s  (2.394s,  427.69/s)  LR: 9.886e-04  Data: 1.669 (1.669)
Train: 41 [  50/1251 (  4%)]  Loss: 4.029 (3.89)  Time: 0.778s, 1316.14/s  (0.849s, 1206.06/s)  LR: 9.886e-04  Data: 0.009 (0.055)
Train: 41 [ 100/1251 (  8%)]  Loss: 3.765 (3.85)  Time: 0.778s, 1315.94/s  (0.830s, 1233.60/s)  LR: 9.886e-04  Data: 0.009 (0.033)
Train: 41 [ 150/1251 ( 12%)]  Loss: 4.335 (3.97)  Time: 0.824s, 1242.80/s  (0.825s, 1241.76/s)  LR: 9.886e-04  Data: 0.009 (0.026)
Train: 41 [ 200/1251 ( 16%)]  Loss: 4.184 (4.01)  Time: 0.838s, 1221.40/s  (0.821s, 1247.08/s)  LR: 9.886e-04  Data: 0.009 (0.022)
Train: 41 [ 250/1251 ( 20%)]  Loss: 3.971 (4.01)  Time: 0.781s, 1311.64/s  (0.820s, 1249.00/s)  LR: 9.886e-04  Data: 0.010 (0.020)
Train: 41 [ 300/1251 ( 24%)]  Loss: 4.016 (4.01)  Time: 0.783s, 1307.45/s  (0.818s, 1251.09/s)  LR: 9.886e-04  Data: 0.010 (0.018)
Train: 41 [ 350/1251 ( 28%)]  Loss: 4.132 (4.02)  Time: 0.778s, 1315.36/s  (0.817s, 1253.51/s)  LR: 9.886e-04  Data: 0.010 (0.017)
Train: 41 [ 400/1251 ( 32%)]  Loss: 3.453 (3.96)  Time: 0.825s, 1241.00/s  (0.815s, 1255.81/s)  LR: 9.886e-04  Data: 0.010 (0.016)
Train: 41 [ 450/1251 ( 36%)]  Loss: 4.153 (3.98)  Time: 0.795s, 1287.38/s  (0.814s, 1257.64/s)  LR: 9.886e-04  Data: 0.014 (0.016)
Train: 41 [ 500/1251 ( 40%)]  Loss: 3.891 (3.97)  Time: 0.782s, 1309.95/s  (0.813s, 1259.18/s)  LR: 9.886e-04  Data: 0.009 (0.015)
Train: 41 [ 550/1251 ( 44%)]  Loss: 3.825 (3.96)  Time: 0.792s, 1292.78/s  (0.814s, 1258.10/s)  LR: 9.886e-04  Data: 0.010 (0.015)
Train: 41 [ 600/1251 ( 48%)]  Loss: 3.839 (3.95)  Time: 0.798s, 1283.62/s  (0.814s, 1258.09/s)  LR: 9.886e-04  Data: 0.009 (0.014)
Train: 41 [ 650/1251 ( 52%)]  Loss: 3.856 (3.94)  Time: 0.782s, 1309.94/s  (0.814s, 1258.27/s)  LR: 9.886e-04  Data: 0.010 (0.014)
Train: 41 [ 700/1251 ( 56%)]  Loss: 4.109 (3.95)  Time: 0.780s, 1313.21/s  (0.814s, 1257.98/s)  LR: 9.886e-04  Data: 0.014 (0.014)
Train: 41 [ 750/1251 ( 60%)]  Loss: 3.912 (3.95)  Time: 0.855s, 1197.26/s  (0.814s, 1258.50/s)  LR: 9.886e-04  Data: 0.009 (0.014)
Train: 41 [ 800/1251 ( 64%)]  Loss: 4.017 (3.96)  Time: 0.819s, 1250.79/s  (0.813s, 1259.08/s)  LR: 9.886e-04  Data: 0.009 (0.013)
Train: 41 [ 850/1251 ( 68%)]  Loss: 3.660 (3.94)  Time: 0.800s, 1279.62/s  (0.813s, 1259.12/s)  LR: 9.886e-04  Data: 0.010 (0.013)
Train: 41 [ 900/1251 ( 72%)]  Loss: 3.926 (3.94)  Time: 0.855s, 1197.54/s  (0.813s, 1259.38/s)  LR: 9.886e-04  Data: 0.009 (0.013)
Train: 41 [ 950/1251 ( 76%)]  Loss: 3.664 (3.92)  Time: 0.808s, 1267.38/s  (0.813s, 1259.55/s)  LR: 9.886e-04  Data: 0.009 (0.013)
Train: 41 [1000/1251 ( 80%)]  Loss: 3.841 (3.92)  Time: 0.802s, 1277.36/s  (0.813s, 1259.88/s)  LR: 9.886e-04  Data: 0.010 (0.013)
Train: 41 [1050/1251 ( 84%)]  Loss: 4.112 (3.93)  Time: 0.788s, 1299.11/s  (0.812s, 1260.63/s)  LR: 9.886e-04  Data: 0.016 (0.013)
Train: 41 [1100/1251 ( 88%)]  Loss: 3.894 (3.93)  Time: 0.783s, 1308.43/s  (0.812s, 1261.14/s)  LR: 9.886e-04  Data: 0.009 (0.013)
Train: 41 [1150/1251 ( 92%)]  Loss: 3.746 (3.92)  Time: 0.797s, 1285.01/s  (0.812s, 1261.51/s)  LR: 9.886e-04  Data: 0.010 (0.013)
Train: 41 [1200/1251 ( 96%)]  Loss: 4.105 (3.93)  Time: 0.872s, 1174.01/s  (0.812s, 1261.34/s)  LR: 9.886e-04  Data: 0.010 (0.013)
Train: 41 [1250/1251 (100%)]  Loss: 3.977 (3.93)  Time: 0.849s, 1206.72/s  (0.812s, 1261.30/s)  LR: 9.886e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.557 (1.557)  Loss:  0.9531 (0.9531)  Acc@1: 82.9102 (82.9102)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.194 (0.601)  Loss:  0.9902 (1.5266)  Acc@1: 80.8962 (67.6640)  Acc@5: 94.2217 (88.6080)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-38.pth.tar', 67.83800002197266)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-41.pth.tar', 67.66399997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-39.pth.tar', 67.53800000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-37.pth.tar', 67.480000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-35.pth.tar', 67.43600013427735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-40.pth.tar', 67.2200000048828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-33.pth.tar', 67.05999997070313)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-36.pth.tar', 66.99200002685546)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-34.pth.tar', 66.67400006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-32.pth.tar', 66.42199999023437)

Train: 42 [   0/1251 (  0%)]  Loss: 3.743 (3.74)  Time: 2.455s,  417.04/s  (2.455s,  417.04/s)  LR: 9.881e-04  Data: 1.718 (1.718)
Train: 42 [  50/1251 (  4%)]  Loss: 3.621 (3.68)  Time: 0.785s, 1304.65/s  (0.854s, 1198.99/s)  LR: 9.881e-04  Data: 0.010 (0.055)
Train: 42 [ 100/1251 (  8%)]  Loss: 3.930 (3.76)  Time: 0.798s, 1282.51/s  (0.832s, 1231.38/s)  LR: 9.881e-04  Data: 0.013 (0.033)
Train: 42 [ 150/1251 ( 12%)]  Loss: 4.121 (3.85)  Time: 0.803s, 1274.82/s  (0.824s, 1242.83/s)  LR: 9.881e-04  Data: 0.009 (0.025)
Train: 42 [ 200/1251 ( 16%)]  Loss: 3.802 (3.84)  Time: 0.793s, 1290.80/s  (0.820s, 1248.30/s)  LR: 9.881e-04  Data: 0.009 (0.022)
Train: 42 [ 250/1251 ( 20%)]  Loss: 4.205 (3.90)  Time: 0.800s, 1279.92/s  (0.818s, 1252.38/s)  LR: 9.881e-04  Data: 0.009 (0.019)
Train: 42 [ 300/1251 ( 24%)]  Loss: 4.158 (3.94)  Time: 0.854s, 1198.50/s  (0.816s, 1254.58/s)  LR: 9.881e-04  Data: 0.015 (0.018)
Train: 42 [ 350/1251 ( 28%)]  Loss: 3.771 (3.92)  Time: 0.775s, 1321.81/s  (0.817s, 1253.98/s)  LR: 9.881e-04  Data: 0.009 (0.017)
Train: 42 [ 400/1251 ( 32%)]  Loss: 3.912 (3.92)  Time: 0.778s, 1316.01/s  (0.816s, 1255.13/s)  LR: 9.881e-04  Data: 0.010 (0.016)
Train: 42 [ 450/1251 ( 36%)]  Loss: 3.952 (3.92)  Time: 0.817s, 1252.81/s  (0.815s, 1256.11/s)  LR: 9.881e-04  Data: 0.010 (0.015)
Train: 42 [ 500/1251 ( 40%)]  Loss: 4.078 (3.94)  Time: 0.777s, 1317.88/s  (0.815s, 1257.17/s)  LR: 9.881e-04  Data: 0.010 (0.015)
Train: 42 [ 550/1251 ( 44%)]  Loss: 3.576 (3.91)  Time: 0.822s, 1245.05/s  (0.814s, 1258.29/s)  LR: 9.881e-04  Data: 0.011 (0.015)
Train: 42 [ 600/1251 ( 48%)]  Loss: 3.902 (3.91)  Time: 0.823s, 1244.63/s  (0.813s, 1259.91/s)  LR: 9.881e-04  Data: 0.010 (0.014)
Train: 42 [ 650/1251 ( 52%)]  Loss: 3.938 (3.91)  Time: 0.820s, 1248.53/s  (0.813s, 1260.04/s)  LR: 9.881e-04  Data: 0.016 (0.014)
Train: 42 [ 700/1251 ( 56%)]  Loss: 4.023 (3.92)  Time: 0.832s, 1230.61/s  (0.813s, 1260.05/s)  LR: 9.881e-04  Data: 0.009 (0.014)
Train: 42 [ 750/1251 ( 60%)]  Loss: 3.796 (3.91)  Time: 0.779s, 1315.34/s  (0.813s, 1259.88/s)  LR: 9.881e-04  Data: 0.012 (0.014)
Train: 42 [ 800/1251 ( 64%)]  Loss: 4.071 (3.92)  Time: 0.785s, 1303.75/s  (0.812s, 1260.59/s)  LR: 9.881e-04  Data: 0.011 (0.013)
Train: 42 [ 850/1251 ( 68%)]  Loss: 4.111 (3.93)  Time: 0.849s, 1206.37/s  (0.812s, 1260.90/s)  LR: 9.881e-04  Data: 0.009 (0.013)
Train: 42 [ 900/1251 ( 72%)]  Loss: 4.216 (3.94)  Time: 0.870s, 1177.30/s  (0.812s, 1260.83/s)  LR: 9.881e-04  Data: 0.009 (0.013)
Train: 42 [ 950/1251 ( 76%)]  Loss: 4.172 (3.95)  Time: 0.847s, 1209.52/s  (0.812s, 1260.92/s)  LR: 9.881e-04  Data: 0.010 (0.013)
Train: 42 [1000/1251 ( 80%)]  Loss: 4.172 (3.97)  Time: 0.865s, 1183.29/s  (0.812s, 1261.33/s)  LR: 9.881e-04  Data: 0.010 (0.013)
Train: 42 [1050/1251 ( 84%)]  Loss: 3.826 (3.96)  Time: 0.808s, 1267.75/s  (0.812s, 1261.33/s)  LR: 9.881e-04  Data: 0.009 (0.013)
Train: 42 [1100/1251 ( 88%)]  Loss: 4.329 (3.97)  Time: 0.808s, 1267.07/s  (0.812s, 1261.42/s)  LR: 9.881e-04  Data: 0.010 (0.013)
Train: 42 [1150/1251 ( 92%)]  Loss: 4.024 (3.98)  Time: 0.811s, 1263.37/s  (0.812s, 1261.79/s)  LR: 9.881e-04  Data: 0.009 (0.012)
Train: 42 [1200/1251 ( 96%)]  Loss: 4.032 (3.98)  Time: 0.817s, 1253.58/s  (0.811s, 1261.89/s)  LR: 9.881e-04  Data: 0.010 (0.012)
Train: 42 [1250/1251 (100%)]  Loss: 4.024 (3.98)  Time: 0.761s, 1345.50/s  (0.811s, 1262.53/s)  LR: 9.881e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.846 (1.846)  Loss:  1.0283 (1.0283)  Acc@1: 83.5938 (83.5938)  Acc@5: 95.3125 (95.3125)
Test: [  48/48]  Time: 0.194 (0.611)  Loss:  1.0381 (1.5855)  Acc@1: 80.8962 (68.3980)  Acc@5: 93.8679 (89.0460)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-42.pth.tar', 68.39800010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-38.pth.tar', 67.83800002197266)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-41.pth.tar', 67.66399997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-39.pth.tar', 67.53800000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-37.pth.tar', 67.480000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-35.pth.tar', 67.43600013427735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-40.pth.tar', 67.2200000048828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-33.pth.tar', 67.05999997070313)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-36.pth.tar', 66.99200002685546)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-34.pth.tar', 66.67400006103516)

Train: 43 [   0/1251 (  0%)]  Loss: 4.258 (4.26)  Time: 2.571s,  398.32/s  (2.571s,  398.32/s)  LR: 9.875e-04  Data: 1.841 (1.841)
Train: 43 [  50/1251 (  4%)]  Loss: 4.143 (4.20)  Time: 0.770s, 1330.68/s  (0.855s, 1197.18/s)  LR: 9.875e-04  Data: 0.009 (0.055)
Train: 43 [ 100/1251 (  8%)]  Loss: 3.518 (3.97)  Time: 0.787s, 1300.40/s  (0.831s, 1232.14/s)  LR: 9.875e-04  Data: 0.010 (0.033)
Train: 43 [ 150/1251 ( 12%)]  Loss: 4.051 (3.99)  Time: 0.796s, 1285.63/s  (0.825s, 1241.79/s)  LR: 9.875e-04  Data: 0.012 (0.026)
Train: 43 [ 200/1251 ( 16%)]  Loss: 3.641 (3.92)  Time: 0.832s, 1231.05/s  (0.822s, 1245.98/s)  LR: 9.875e-04  Data: 0.015 (0.022)
Train: 43 [ 250/1251 ( 20%)]  Loss: 3.983 (3.93)  Time: 0.805s, 1272.18/s  (0.819s, 1250.42/s)  LR: 9.875e-04  Data: 0.014 (0.020)
Train: 43 [ 300/1251 ( 24%)]  Loss: 3.986 (3.94)  Time: 0.776s, 1319.34/s  (0.817s, 1252.81/s)  LR: 9.875e-04  Data: 0.009 (0.018)
Train: 43 [ 350/1251 ( 28%)]  Loss: 4.102 (3.96)  Time: 0.803s, 1275.91/s  (0.817s, 1253.18/s)  LR: 9.875e-04  Data: 0.012 (0.017)
Train: 43 [ 400/1251 ( 32%)]  Loss: 3.955 (3.96)  Time: 0.802s, 1276.30/s  (0.816s, 1254.44/s)  LR: 9.875e-04  Data: 0.013 (0.016)
Train: 43 [ 450/1251 ( 36%)]  Loss: 3.461 (3.91)  Time: 0.790s, 1296.33/s  (0.816s, 1254.97/s)  LR: 9.875e-04  Data: 0.010 (0.016)
Train: 43 [ 500/1251 ( 40%)]  Loss: 3.889 (3.91)  Time: 0.824s, 1242.81/s  (0.815s, 1256.13/s)  LR: 9.875e-04  Data: 0.017 (0.015)
Train: 43 [ 550/1251 ( 44%)]  Loss: 3.578 (3.88)  Time: 0.837s, 1223.28/s  (0.815s, 1257.15/s)  LR: 9.875e-04  Data: 0.009 (0.015)
Train: 43 [ 600/1251 ( 48%)]  Loss: 3.928 (3.88)  Time: 0.806s, 1270.36/s  (0.814s, 1257.66/s)  LR: 9.875e-04  Data: 0.010 (0.014)
Train: 43 [ 650/1251 ( 52%)]  Loss: 3.904 (3.89)  Time: 0.784s, 1305.60/s  (0.814s, 1258.04/s)  LR: 9.875e-04  Data: 0.010 (0.014)
Train: 43 [ 700/1251 ( 56%)]  Loss: 3.835 (3.88)  Time: 0.782s, 1309.31/s  (0.814s, 1258.31/s)  LR: 9.875e-04  Data: 0.009 (0.014)
Train: 43 [ 750/1251 ( 60%)]  Loss: 3.945 (3.89)  Time: 0.785s, 1304.97/s  (0.813s, 1259.00/s)  LR: 9.875e-04  Data: 0.010 (0.014)
Train: 43 [ 800/1251 ( 64%)]  Loss: 3.927 (3.89)  Time: 0.781s, 1311.64/s  (0.813s, 1259.51/s)  LR: 9.875e-04  Data: 0.010 (0.013)
Train: 43 [ 850/1251 ( 68%)]  Loss: 3.876 (3.89)  Time: 0.777s, 1317.96/s  (0.813s, 1259.28/s)  LR: 9.875e-04  Data: 0.010 (0.013)
Train: 43 [ 900/1251 ( 72%)]  Loss: 3.819 (3.88)  Time: 0.780s, 1312.47/s  (0.813s, 1259.40/s)  LR: 9.875e-04  Data: 0.010 (0.013)
Train: 43 [ 950/1251 ( 76%)]  Loss: 3.717 (3.88)  Time: 0.779s, 1314.11/s  (0.813s, 1259.11/s)  LR: 9.875e-04  Data: 0.013 (0.013)
Train: 43 [1000/1251 ( 80%)]  Loss: 4.095 (3.89)  Time: 0.883s, 1159.93/s  (0.813s, 1259.24/s)  LR: 9.875e-04  Data: 0.009 (0.013)
Train: 43 [1050/1251 ( 84%)]  Loss: 4.140 (3.90)  Time: 0.819s, 1250.36/s  (0.813s, 1259.43/s)  LR: 9.875e-04  Data: 0.009 (0.013)
Train: 43 [1100/1251 ( 88%)]  Loss: 3.820 (3.89)  Time: 0.794s, 1290.11/s  (0.813s, 1259.80/s)  LR: 9.875e-04  Data: 0.011 (0.013)
Train: 43 [1150/1251 ( 92%)]  Loss: 4.106 (3.90)  Time: 0.780s, 1313.47/s  (0.813s, 1260.11/s)  LR: 9.875e-04  Data: 0.010 (0.013)
Train: 43 [1200/1251 ( 96%)]  Loss: 4.395 (3.92)  Time: 0.772s, 1326.27/s  (0.812s, 1260.48/s)  LR: 9.875e-04  Data: 0.010 (0.012)
Train: 43 [1250/1251 (100%)]  Loss: 3.490 (3.91)  Time: 0.836s, 1225.02/s  (0.812s, 1260.45/s)  LR: 9.875e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.669 (1.669)  Loss:  1.0967 (1.0967)  Acc@1: 83.3984 (83.3984)  Acc@5: 95.3125 (95.3125)
Test: [  48/48]  Time: 0.194 (0.606)  Loss:  1.0840 (1.6939)  Acc@1: 82.5472 (67.8780)  Acc@5: 93.9858 (88.5040)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-42.pth.tar', 68.39800010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-43.pth.tar', 67.87800017578125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-38.pth.tar', 67.83800002197266)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-41.pth.tar', 67.66399997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-39.pth.tar', 67.53800000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-37.pth.tar', 67.480000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-35.pth.tar', 67.43600013427735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-40.pth.tar', 67.2200000048828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-33.pth.tar', 67.05999997070313)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-36.pth.tar', 66.99200002685546)

Train: 44 [   0/1251 (  0%)]  Loss: 3.734 (3.73)  Time: 2.336s,  438.44/s  (2.336s,  438.44/s)  LR: 9.869e-04  Data: 1.601 (1.601)
Train: 44 [  50/1251 (  4%)]  Loss: 4.190 (3.96)  Time: 0.817s, 1252.63/s  (0.847s, 1208.50/s)  LR: 9.869e-04  Data: 0.018 (0.049)
Train: 44 [ 100/1251 (  8%)]  Loss: 3.513 (3.81)  Time: 0.799s, 1281.84/s  (0.827s, 1237.95/s)  LR: 9.869e-04  Data: 0.008 (0.030)
Train: 44 [ 150/1251 ( 12%)]  Loss: 3.906 (3.84)  Time: 0.850s, 1205.23/s  (0.823s, 1244.42/s)  LR: 9.869e-04  Data: 0.011 (0.024)
Train: 44 [ 200/1251 ( 16%)]  Loss: 3.711 (3.81)  Time: 0.781s, 1311.33/s  (0.819s, 1250.15/s)  LR: 9.869e-04  Data: 0.010 (0.020)
Train: 44 [ 250/1251 ( 20%)]  Loss: 4.028 (3.85)  Time: 0.779s, 1315.06/s  (0.816s, 1254.64/s)  LR: 9.869e-04  Data: 0.009 (0.018)
Train: 44 [ 300/1251 ( 24%)]  Loss: 4.723 (3.97)  Time: 0.806s, 1269.80/s  (0.816s, 1255.33/s)  LR: 9.869e-04  Data: 0.010 (0.017)
Train: 44 [ 350/1251 ( 28%)]  Loss: 3.343 (3.89)  Time: 0.850s, 1205.11/s  (0.815s, 1255.82/s)  LR: 9.869e-04  Data: 0.010 (0.016)
Train: 44 [ 400/1251 ( 32%)]  Loss: 4.378 (3.95)  Time: 0.785s, 1304.54/s  (0.816s, 1254.33/s)  LR: 9.869e-04  Data: 0.015 (0.015)
Train: 44 [ 450/1251 ( 36%)]  Loss: 3.793 (3.93)  Time: 0.784s, 1306.74/s  (0.815s, 1256.00/s)  LR: 9.869e-04  Data: 0.010 (0.015)
Train: 44 [ 500/1251 ( 40%)]  Loss: 3.529 (3.90)  Time: 0.832s, 1230.76/s  (0.815s, 1256.14/s)  LR: 9.869e-04  Data: 0.009 (0.014)
Train: 44 [ 550/1251 ( 44%)]  Loss: 3.859 (3.89)  Time: 0.796s, 1285.64/s  (0.815s, 1256.01/s)  LR: 9.869e-04  Data: 0.010 (0.014)
Train: 44 [ 600/1251 ( 48%)]  Loss: 3.799 (3.89)  Time: 0.837s, 1223.51/s  (0.814s, 1257.61/s)  LR: 9.869e-04  Data: 0.013 (0.014)
Train: 44 [ 650/1251 ( 52%)]  Loss: 3.797 (3.88)  Time: 0.819s, 1249.93/s  (0.814s, 1257.40/s)  LR: 9.869e-04  Data: 0.011 (0.014)
Train: 44 [ 700/1251 ( 56%)]  Loss: 3.809 (3.87)  Time: 0.792s, 1292.31/s  (0.814s, 1257.58/s)  LR: 9.869e-04  Data: 0.009 (0.013)
Train: 44 [ 750/1251 ( 60%)]  Loss: 4.138 (3.89)  Time: 0.780s, 1313.00/s  (0.814s, 1257.98/s)  LR: 9.869e-04  Data: 0.011 (0.013)
Train: 44 [ 800/1251 ( 64%)]  Loss: 4.086 (3.90)  Time: 0.839s, 1220.98/s  (0.813s, 1258.95/s)  LR: 9.869e-04  Data: 0.012 (0.013)
Train: 44 [ 850/1251 ( 68%)]  Loss: 3.912 (3.90)  Time: 0.815s, 1255.79/s  (0.813s, 1259.90/s)  LR: 9.869e-04  Data: 0.018 (0.013)
Train: 44 [ 900/1251 ( 72%)]  Loss: 3.976 (3.91)  Time: 0.793s, 1291.76/s  (0.813s, 1260.05/s)  LR: 9.869e-04  Data: 0.014 (0.013)
Train: 44 [ 950/1251 ( 76%)]  Loss: 4.125 (3.92)  Time: 0.824s, 1242.57/s  (0.813s, 1259.43/s)  LR: 9.869e-04  Data: 0.009 (0.013)
Train: 44 [1000/1251 ( 80%)]  Loss: 4.022 (3.92)  Time: 0.812s, 1261.20/s  (0.813s, 1259.67/s)  LR: 9.869e-04  Data: 0.012 (0.013)
Train: 44 [1050/1251 ( 84%)]  Loss: 3.899 (3.92)  Time: 0.848s, 1208.01/s  (0.813s, 1259.41/s)  LR: 9.869e-04  Data: 0.010 (0.012)
Train: 44 [1100/1251 ( 88%)]  Loss: 3.950 (3.92)  Time: 0.835s, 1225.84/s  (0.813s, 1259.71/s)  LR: 9.869e-04  Data: 0.010 (0.012)
Train: 44 [1150/1251 ( 92%)]  Loss: 3.686 (3.91)  Time: 0.846s, 1210.99/s  (0.813s, 1260.17/s)  LR: 9.869e-04  Data: 0.010 (0.012)
Train: 44 [1200/1251 ( 96%)]  Loss: 3.782 (3.91)  Time: 0.811s, 1263.10/s  (0.812s, 1260.48/s)  LR: 9.869e-04  Data: 0.009 (0.012)
Train: 44 [1250/1251 (100%)]  Loss: 4.397 (3.93)  Time: 0.771s, 1328.50/s  (0.812s, 1260.96/s)  LR: 9.869e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.769 (1.769)  Loss:  0.8613 (0.8613)  Acc@1: 85.1562 (85.1562)  Acc@5: 95.8984 (95.8984)
Test: [  48/48]  Time: 0.194 (0.615)  Loss:  0.9131 (1.5083)  Acc@1: 82.0755 (68.9060)  Acc@5: 95.2830 (89.2960)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-44.pth.tar', 68.90599997070312)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-42.pth.tar', 68.39800010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-43.pth.tar', 67.87800017578125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-38.pth.tar', 67.83800002197266)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-41.pth.tar', 67.66399997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-39.pth.tar', 67.53800000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-37.pth.tar', 67.480000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-35.pth.tar', 67.43600013427735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-40.pth.tar', 67.2200000048828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-33.pth.tar', 67.05999997070313)

Train: 45 [   0/1251 (  0%)]  Loss: 3.903 (3.90)  Time: 2.390s,  428.45/s  (2.390s,  428.45/s)  LR: 9.863e-04  Data: 1.655 (1.655)
Train: 45 [  50/1251 (  4%)]  Loss: 3.783 (3.84)  Time: 0.783s, 1307.06/s  (0.846s, 1210.97/s)  LR: 9.863e-04  Data: 0.013 (0.048)
Train: 45 [ 100/1251 (  8%)]  Loss: 4.108 (3.93)  Time: 0.798s, 1283.57/s  (0.829s, 1234.63/s)  LR: 9.863e-04  Data: 0.012 (0.029)
Train: 45 [ 150/1251 ( 12%)]  Loss: 4.266 (4.02)  Time: 0.773s, 1324.84/s  (0.823s, 1244.09/s)  LR: 9.863e-04  Data: 0.011 (0.023)
Train: 45 [ 200/1251 ( 16%)]  Loss: 3.995 (4.01)  Time: 0.809s, 1265.26/s  (0.820s, 1248.97/s)  LR: 9.863e-04  Data: 0.009 (0.020)
Train: 45 [ 250/1251 ( 20%)]  Loss: 4.142 (4.03)  Time: 0.800s, 1279.48/s  (0.817s, 1252.63/s)  LR: 9.863e-04  Data: 0.011 (0.018)
Train: 45 [ 300/1251 ( 24%)]  Loss: 3.691 (3.98)  Time: 0.779s, 1314.97/s  (0.816s, 1255.24/s)  LR: 9.863e-04  Data: 0.009 (0.017)
Train: 45 [ 350/1251 ( 28%)]  Loss: 3.773 (3.96)  Time: 0.817s, 1253.55/s  (0.814s, 1257.34/s)  LR: 9.863e-04  Data: 0.010 (0.016)
Train: 45 [ 400/1251 ( 32%)]  Loss: 4.163 (3.98)  Time: 0.819s, 1249.74/s  (0.814s, 1258.42/s)  LR: 9.863e-04  Data: 0.010 (0.015)
Train: 45 [ 450/1251 ( 36%)]  Loss: 3.788 (3.96)  Time: 0.823s, 1244.09/s  (0.813s, 1259.85/s)  LR: 9.863e-04  Data: 0.014 (0.015)
Train: 45 [ 500/1251 ( 40%)]  Loss: 3.497 (3.92)  Time: 0.811s, 1262.59/s  (0.813s, 1259.80/s)  LR: 9.863e-04  Data: 0.010 (0.014)
Train: 45 [ 550/1251 ( 44%)]  Loss: 3.879 (3.92)  Time: 0.777s, 1318.68/s  (0.812s, 1260.60/s)  LR: 9.863e-04  Data: 0.010 (0.014)
Train: 45 [ 600/1251 ( 48%)]  Loss: 3.827 (3.91)  Time: 0.780s, 1313.47/s  (0.812s, 1261.59/s)  LR: 9.863e-04  Data: 0.009 (0.014)
Train: 45 [ 650/1251 ( 52%)]  Loss: 3.584 (3.89)  Time: 0.825s, 1241.57/s  (0.812s, 1261.29/s)  LR: 9.863e-04  Data: 0.009 (0.013)
Train: 45 [ 700/1251 ( 56%)]  Loss: 4.031 (3.90)  Time: 0.812s, 1260.83/s  (0.812s, 1261.37/s)  LR: 9.863e-04  Data: 0.011 (0.013)
Train: 45 [ 750/1251 ( 60%)]  Loss: 3.837 (3.89)  Time: 0.893s, 1146.58/s  (0.812s, 1261.42/s)  LR: 9.863e-04  Data: 0.013 (0.013)
Train: 45 [ 800/1251 ( 64%)]  Loss: 4.118 (3.91)  Time: 0.807s, 1269.64/s  (0.811s, 1262.25/s)  LR: 9.863e-04  Data: 0.009 (0.013)
Train: 45 [ 850/1251 ( 68%)]  Loss: 4.137 (3.92)  Time: 0.841s, 1217.91/s  (0.812s, 1260.79/s)  LR: 9.863e-04  Data: 0.019 (0.013)
Train: 45 [ 900/1251 ( 72%)]  Loss: 3.666 (3.90)  Time: 0.897s, 1141.11/s  (0.813s, 1260.08/s)  LR: 9.863e-04  Data: 0.009 (0.013)
Train: 45 [ 950/1251 ( 76%)]  Loss: 4.254 (3.92)  Time: 0.778s, 1315.63/s  (0.812s, 1260.42/s)  LR: 9.863e-04  Data: 0.010 (0.013)
Train: 45 [1000/1251 ( 80%)]  Loss: 3.474 (3.90)  Time: 0.821s, 1246.71/s  (0.812s, 1260.68/s)  LR: 9.863e-04  Data: 0.010 (0.013)
Train: 45 [1050/1251 ( 84%)]  Loss: 3.880 (3.90)  Time: 0.788s, 1299.65/s  (0.812s, 1260.65/s)  LR: 9.863e-04  Data: 0.009 (0.012)
Train: 45 [1100/1251 ( 88%)]  Loss: 4.011 (3.90)  Time: 0.796s, 1285.74/s  (0.812s, 1261.03/s)  LR: 9.863e-04  Data: 0.014 (0.012)
Train: 45 [1150/1251 ( 92%)]  Loss: 4.238 (3.92)  Time: 0.840s, 1219.00/s  (0.812s, 1261.30/s)  LR: 9.863e-04  Data: 0.010 (0.012)
Train: 45 [1200/1251 ( 96%)]  Loss: 4.151 (3.93)  Time: 0.811s, 1262.87/s  (0.812s, 1261.18/s)  LR: 9.863e-04  Data: 0.010 (0.012)
Train: 45 [1250/1251 (100%)]  Loss: 4.163 (3.94)  Time: 0.766s, 1336.65/s  (0.812s, 1261.27/s)  LR: 9.863e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.672 (1.672)  Loss:  1.0107 (1.0107)  Acc@1: 86.4258 (86.4258)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.194 (0.613)  Loss:  0.9980 (1.5915)  Acc@1: 82.3113 (68.5500)  Acc@5: 94.1038 (88.9540)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-44.pth.tar', 68.90599997070312)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-45.pth.tar', 68.55000007324219)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-42.pth.tar', 68.39800010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-43.pth.tar', 67.87800017578125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-38.pth.tar', 67.83800002197266)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-41.pth.tar', 67.66399997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-39.pth.tar', 67.53800000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-37.pth.tar', 67.480000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-35.pth.tar', 67.43600013427735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-40.pth.tar', 67.2200000048828)

Train: 46 [   0/1251 (  0%)]  Loss: 4.078 (4.08)  Time: 2.271s,  450.95/s  (2.271s,  450.95/s)  LR: 9.857e-04  Data: 1.536 (1.536)
Train: 46 [  50/1251 (  4%)]  Loss: 3.923 (4.00)  Time: 0.777s, 1318.11/s  (0.842s, 1215.96/s)  LR: 9.857e-04  Data: 0.010 (0.047)
Train: 46 [ 100/1251 (  8%)]  Loss: 3.935 (3.98)  Time: 0.855s, 1197.61/s  (0.824s, 1243.47/s)  LR: 9.857e-04  Data: 0.009 (0.029)
Train: 46 [ 150/1251 ( 12%)]  Loss: 3.993 (3.98)  Time: 0.779s, 1315.09/s  (0.819s, 1250.57/s)  LR: 9.857e-04  Data: 0.010 (0.023)
Train: 46 [ 200/1251 ( 16%)]  Loss: 3.518 (3.89)  Time: 0.777s, 1318.14/s  (0.817s, 1254.04/s)  LR: 9.857e-04  Data: 0.009 (0.020)
Train: 46 [ 250/1251 ( 20%)]  Loss: 3.951 (3.90)  Time: 0.804s, 1274.28/s  (0.814s, 1258.72/s)  LR: 9.857e-04  Data: 0.012 (0.018)
Train: 46 [ 300/1251 ( 24%)]  Loss: 3.595 (3.86)  Time: 0.816s, 1255.18/s  (0.814s, 1258.57/s)  LR: 9.857e-04  Data: 0.015 (0.017)
Train: 46 [ 350/1251 ( 28%)]  Loss: 3.879 (3.86)  Time: 0.790s, 1295.54/s  (0.814s, 1258.35/s)  LR: 9.857e-04  Data: 0.010 (0.016)
Train: 46 [ 400/1251 ( 32%)]  Loss: 4.128 (3.89)  Time: 0.820s, 1248.72/s  (0.814s, 1258.17/s)  LR: 9.857e-04  Data: 0.010 (0.015)
Train: 46 [ 450/1251 ( 36%)]  Loss: 3.655 (3.87)  Time: 0.826s, 1239.72/s  (0.813s, 1260.06/s)  LR: 9.857e-04  Data: 0.010 (0.015)
Train: 46 [ 500/1251 ( 40%)]  Loss: 4.048 (3.88)  Time: 0.779s, 1314.11/s  (0.812s, 1260.80/s)  LR: 9.857e-04  Data: 0.010 (0.014)
Train: 46 [ 550/1251 ( 44%)]  Loss: 3.793 (3.87)  Time: 0.867s, 1180.58/s  (0.813s, 1259.89/s)  LR: 9.857e-04  Data: 0.011 (0.014)
Train: 46 [ 600/1251 ( 48%)]  Loss: 4.162 (3.90)  Time: 0.800s, 1279.69/s  (0.812s, 1260.37/s)  LR: 9.857e-04  Data: 0.014 (0.014)
Train: 46 [ 650/1251 ( 52%)]  Loss: 3.861 (3.89)  Time: 0.843s, 1214.66/s  (0.812s, 1260.96/s)  LR: 9.857e-04  Data: 0.009 (0.013)
Train: 46 [ 700/1251 ( 56%)]  Loss: 3.848 (3.89)  Time: 0.826s, 1239.41/s  (0.812s, 1261.35/s)  LR: 9.857e-04  Data: 0.009 (0.013)
Train: 46 [ 750/1251 ( 60%)]  Loss: 3.787 (3.88)  Time: 0.827s, 1237.73/s  (0.812s, 1261.45/s)  LR: 9.857e-04  Data: 0.010 (0.013)
Train: 46 [ 800/1251 ( 64%)]  Loss: 4.403 (3.92)  Time: 0.794s, 1289.71/s  (0.812s, 1261.71/s)  LR: 9.857e-04  Data: 0.010 (0.013)
Train: 46 [ 850/1251 ( 68%)]  Loss: 3.769 (3.91)  Time: 0.778s, 1316.06/s  (0.812s, 1261.46/s)  LR: 9.857e-04  Data: 0.010 (0.013)
Train: 46 [ 900/1251 ( 72%)]  Loss: 4.261 (3.93)  Time: 0.780s, 1313.11/s  (0.812s, 1261.52/s)  LR: 9.857e-04  Data: 0.009 (0.013)
Train: 46 [ 950/1251 ( 76%)]  Loss: 3.613 (3.91)  Time: 0.823s, 1244.46/s  (0.812s, 1261.51/s)  LR: 9.857e-04  Data: 0.010 (0.013)
Train: 46 [1000/1251 ( 80%)]  Loss: 3.775 (3.90)  Time: 0.880s, 1164.24/s  (0.812s, 1261.64/s)  LR: 9.857e-04  Data: 0.010 (0.012)
Train: 46 [1050/1251 ( 84%)]  Loss: 3.959 (3.91)  Time: 0.785s, 1304.62/s  (0.812s, 1261.34/s)  LR: 9.857e-04  Data: 0.010 (0.012)
Train: 46 [1100/1251 ( 88%)]  Loss: 3.994 (3.91)  Time: 0.833s, 1228.70/s  (0.812s, 1261.44/s)  LR: 9.857e-04  Data: 0.009 (0.012)
Train: 46 [1150/1251 ( 92%)]  Loss: 3.824 (3.91)  Time: 0.873s, 1172.51/s  (0.812s, 1261.22/s)  LR: 9.857e-04  Data: 0.013 (0.012)
Train: 46 [1200/1251 ( 96%)]  Loss: 3.718 (3.90)  Time: 0.777s, 1317.44/s  (0.812s, 1261.20/s)  LR: 9.857e-04  Data: 0.010 (0.012)
Train: 46 [1250/1251 (100%)]  Loss: 3.628 (3.89)  Time: 0.766s, 1336.18/s  (0.812s, 1261.50/s)  LR: 9.857e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.720 (1.720)  Loss:  1.0605 (1.0605)  Acc@1: 84.5703 (84.5703)  Acc@5: 95.4102 (95.4102)
Test: [  48/48]  Time: 0.195 (0.610)  Loss:  1.2080 (1.6471)  Acc@1: 81.6038 (68.1240)  Acc@5: 94.5755 (88.6720)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-44.pth.tar', 68.90599997070312)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-45.pth.tar', 68.55000007324219)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-42.pth.tar', 68.39800010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-46.pth.tar', 68.12400002441406)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-43.pth.tar', 67.87800017578125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-38.pth.tar', 67.83800002197266)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-41.pth.tar', 67.66399997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-39.pth.tar', 67.53800000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-37.pth.tar', 67.480000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-35.pth.tar', 67.43600013427735)

Train: 47 [   0/1251 (  0%)]  Loss: 4.109 (4.11)  Time: 2.309s,  443.50/s  (2.309s,  443.50/s)  LR: 9.851e-04  Data: 1.584 (1.584)
Train: 47 [  50/1251 (  4%)]  Loss: 4.176 (4.14)  Time: 0.799s, 1282.34/s  (0.845s, 1211.20/s)  LR: 9.851e-04  Data: 0.014 (0.048)
Train: 47 [ 100/1251 (  8%)]  Loss: 3.623 (3.97)  Time: 0.778s, 1315.60/s  (0.827s, 1238.44/s)  LR: 9.851e-04  Data: 0.010 (0.029)
Train: 47 [ 150/1251 ( 12%)]  Loss: 3.950 (3.96)  Time: 0.874s, 1171.28/s  (0.822s, 1245.79/s)  LR: 9.851e-04  Data: 0.009 (0.023)
Train: 47 [ 200/1251 ( 16%)]  Loss: 3.872 (3.95)  Time: 0.852s, 1202.26/s  (0.821s, 1247.89/s)  LR: 9.851e-04  Data: 0.011 (0.020)
Train: 47 [ 250/1251 ( 20%)]  Loss: 3.596 (3.89)  Time: 0.817s, 1252.65/s  (0.819s, 1250.36/s)  LR: 9.851e-04  Data: 0.010 (0.018)
Train: 47 [ 300/1251 ( 24%)]  Loss: 3.837 (3.88)  Time: 0.779s, 1314.15/s  (0.816s, 1254.75/s)  LR: 9.851e-04  Data: 0.010 (0.017)
Train: 47 [ 350/1251 ( 28%)]  Loss: 3.993 (3.89)  Time: 0.781s, 1311.09/s  (0.814s, 1257.46/s)  LR: 9.851e-04  Data: 0.009 (0.016)
Train: 47 [ 400/1251 ( 32%)]  Loss: 3.887 (3.89)  Time: 0.779s, 1314.20/s  (0.813s, 1258.98/s)  LR: 9.851e-04  Data: 0.009 (0.015)
Train: 47 [ 450/1251 ( 36%)]  Loss: 4.210 (3.93)  Time: 0.828s, 1236.88/s  (0.813s, 1259.68/s)  LR: 9.851e-04  Data: 0.012 (0.015)
Train: 47 [ 500/1251 ( 40%)]  Loss: 4.330 (3.96)  Time: 0.791s, 1294.21/s  (0.812s, 1260.40/s)  LR: 9.851e-04  Data: 0.018 (0.015)
Train: 47 [ 550/1251 ( 44%)]  Loss: 3.777 (3.95)  Time: 0.808s, 1266.92/s  (0.812s, 1260.34/s)  LR: 9.851e-04  Data: 0.014 (0.014)
Train: 47 [ 600/1251 ( 48%)]  Loss: 4.358 (3.98)  Time: 0.817s, 1253.27/s  (0.812s, 1260.38/s)  LR: 9.851e-04  Data: 0.009 (0.014)
Train: 47 [ 650/1251 ( 52%)]  Loss: 3.457 (3.94)  Time: 0.829s, 1235.71/s  (0.812s, 1260.39/s)  LR: 9.851e-04  Data: 0.009 (0.014)
Train: 47 [ 700/1251 ( 56%)]  Loss: 3.846 (3.93)  Time: 0.821s, 1247.13/s  (0.812s, 1260.32/s)  LR: 9.851e-04  Data: 0.009 (0.013)
Train: 47 [ 750/1251 ( 60%)]  Loss: 3.634 (3.92)  Time: 0.804s, 1273.31/s  (0.812s, 1260.58/s)  LR: 9.851e-04  Data: 0.015 (0.013)
Train: 47 [ 800/1251 ( 64%)]  Loss: 3.993 (3.92)  Time: 0.781s, 1310.97/s  (0.812s, 1261.26/s)  LR: 9.851e-04  Data: 0.009 (0.013)
Train: 47 [ 850/1251 ( 68%)]  Loss: 4.023 (3.93)  Time: 0.782s, 1309.06/s  (0.811s, 1262.12/s)  LR: 9.851e-04  Data: 0.009 (0.013)
Train: 47 [ 900/1251 ( 72%)]  Loss: 4.271 (3.94)  Time: 0.827s, 1237.97/s  (0.811s, 1261.94/s)  LR: 9.851e-04  Data: 0.010 (0.013)
Train: 47 [ 950/1251 ( 76%)]  Loss: 3.833 (3.94)  Time: 0.778s, 1316.80/s  (0.811s, 1262.27/s)  LR: 9.851e-04  Data: 0.010 (0.013)
Train: 47 [1000/1251 ( 80%)]  Loss: 3.862 (3.93)  Time: 0.784s, 1306.61/s  (0.811s, 1262.43/s)  LR: 9.851e-04  Data: 0.009 (0.012)
Train: 47 [1050/1251 ( 84%)]  Loss: 4.069 (3.94)  Time: 0.794s, 1289.49/s  (0.811s, 1262.91/s)  LR: 9.851e-04  Data: 0.014 (0.012)
Train: 47 [1100/1251 ( 88%)]  Loss: 3.782 (3.93)  Time: 0.781s, 1311.64/s  (0.811s, 1263.04/s)  LR: 9.851e-04  Data: 0.011 (0.012)
Train: 47 [1150/1251 ( 92%)]  Loss: 3.731 (3.93)  Time: 0.776s, 1319.77/s  (0.811s, 1263.33/s)  LR: 9.851e-04  Data: 0.010 (0.012)
Train: 47 [1200/1251 ( 96%)]  Loss: 3.918 (3.93)  Time: 0.804s, 1274.08/s  (0.810s, 1263.48/s)  LR: 9.851e-04  Data: 0.010 (0.012)
Train: 47 [1250/1251 (100%)]  Loss: 3.785 (3.92)  Time: 0.843s, 1215.26/s  (0.810s, 1263.51/s)  LR: 9.851e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.635 (1.635)  Loss:  0.8765 (0.8765)  Acc@1: 84.4727 (84.4727)  Acc@5: 95.6055 (95.6055)
Test: [  48/48]  Time: 0.194 (0.610)  Loss:  1.0332 (1.5064)  Acc@1: 81.9575 (68.9380)  Acc@5: 93.9859 (89.3660)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-47.pth.tar', 68.9379999194336)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-44.pth.tar', 68.90599997070312)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-45.pth.tar', 68.55000007324219)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-42.pth.tar', 68.39800010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-46.pth.tar', 68.12400002441406)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-43.pth.tar', 67.87800017578125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-38.pth.tar', 67.83800002197266)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-41.pth.tar', 67.66399997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-39.pth.tar', 67.53800000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-37.pth.tar', 67.480000078125)

Train: 48 [   0/1251 (  0%)]  Loss: 4.130 (4.13)  Time: 2.499s,  409.84/s  (2.499s,  409.84/s)  LR: 9.844e-04  Data: 1.764 (1.764)
Train: 48 [  50/1251 (  4%)]  Loss: 3.615 (3.87)  Time: 0.778s, 1316.68/s  (0.850s, 1204.86/s)  LR: 9.844e-04  Data: 0.010 (0.049)
Train: 48 [ 100/1251 (  8%)]  Loss: 4.003 (3.92)  Time: 0.780s, 1312.34/s  (0.827s, 1238.61/s)  LR: 9.844e-04  Data: 0.010 (0.030)
Train: 48 [ 150/1251 ( 12%)]  Loss: 4.255 (4.00)  Time: 0.775s, 1320.51/s  (0.821s, 1247.87/s)  LR: 9.844e-04  Data: 0.010 (0.024)
Train: 48 [ 200/1251 ( 16%)]  Loss: 3.817 (3.96)  Time: 0.781s, 1311.26/s  (0.819s, 1250.88/s)  LR: 9.844e-04  Data: 0.010 (0.020)
Train: 48 [ 250/1251 ( 20%)]  Loss: 4.159 (4.00)  Time: 0.844s, 1213.31/s  (0.816s, 1254.57/s)  LR: 9.844e-04  Data: 0.009 (0.018)
Train: 48 [ 300/1251 ( 24%)]  Loss: 4.061 (4.01)  Time: 0.823s, 1243.82/s  (0.815s, 1255.78/s)  LR: 9.844e-04  Data: 0.009 (0.017)
Train: 48 [ 350/1251 ( 28%)]  Loss: 3.884 (3.99)  Time: 0.822s, 1245.64/s  (0.815s, 1256.05/s)  LR: 9.844e-04  Data: 0.009 (0.016)
Train: 48 [ 400/1251 ( 32%)]  Loss: 4.337 (4.03)  Time: 0.777s, 1318.09/s  (0.814s, 1257.73/s)  LR: 9.844e-04  Data: 0.010 (0.016)
Train: 48 [ 450/1251 ( 36%)]  Loss: 3.892 (4.02)  Time: 0.846s, 1210.85/s  (0.813s, 1258.97/s)  LR: 9.844e-04  Data: 0.010 (0.015)
Train: 48 [ 500/1251 ( 40%)]  Loss: 4.186 (4.03)  Time: 0.802s, 1277.54/s  (0.812s, 1260.43/s)  LR: 9.844e-04  Data: 0.011 (0.015)
Train: 48 [ 550/1251 ( 44%)]  Loss: 4.371 (4.06)  Time: 0.812s, 1261.30/s  (0.812s, 1260.64/s)  LR: 9.844e-04  Data: 0.010 (0.014)
Train: 48 [ 600/1251 ( 48%)]  Loss: 3.669 (4.03)  Time: 0.777s, 1318.42/s  (0.812s, 1261.44/s)  LR: 9.844e-04  Data: 0.010 (0.014)
Train: 48 [ 650/1251 ( 52%)]  Loss: 3.774 (4.01)  Time: 0.796s, 1286.27/s  (0.812s, 1261.32/s)  LR: 9.844e-04  Data: 0.009 (0.014)
Train: 48 [ 700/1251 ( 56%)]  Loss: 3.695 (3.99)  Time: 0.831s, 1231.52/s  (0.812s, 1260.94/s)  LR: 9.844e-04  Data: 0.016 (0.013)
Train: 48 [ 750/1251 ( 60%)]  Loss: 4.002 (3.99)  Time: 0.835s, 1225.82/s  (0.812s, 1260.47/s)  LR: 9.844e-04  Data: 0.010 (0.013)
Train: 48 [ 800/1251 ( 64%)]  Loss: 3.901 (3.99)  Time: 0.781s, 1311.25/s  (0.812s, 1261.10/s)  LR: 9.844e-04  Data: 0.010 (0.013)
Train: 48 [ 850/1251 ( 68%)]  Loss: 4.257 (4.00)  Time: 0.823s, 1244.56/s  (0.812s, 1261.21/s)  LR: 9.844e-04  Data: 0.010 (0.013)
Train: 48 [ 900/1251 ( 72%)]  Loss: 4.081 (4.00)  Time: 0.825s, 1240.83/s  (0.812s, 1261.09/s)  LR: 9.844e-04  Data: 0.017 (0.013)
Train: 48 [ 950/1251 ( 76%)]  Loss: 4.076 (4.01)  Time: 0.779s, 1314.88/s  (0.812s, 1260.97/s)  LR: 9.844e-04  Data: 0.010 (0.013)
Train: 48 [1000/1251 ( 80%)]  Loss: 3.839 (4.00)  Time: 0.820s, 1248.53/s  (0.812s, 1261.22/s)  LR: 9.844e-04  Data: 0.009 (0.013)
Train: 48 [1050/1251 ( 84%)]  Loss: 3.958 (4.00)  Time: 0.781s, 1311.95/s  (0.812s, 1260.75/s)  LR: 9.844e-04  Data: 0.010 (0.013)
Train: 48 [1100/1251 ( 88%)]  Loss: 4.151 (4.00)  Time: 0.893s, 1146.46/s  (0.812s, 1260.88/s)  LR: 9.844e-04  Data: 0.009 (0.012)
Train: 48 [1150/1251 ( 92%)]  Loss: 3.711 (3.99)  Time: 0.775s, 1320.63/s  (0.812s, 1261.43/s)  LR: 9.844e-04  Data: 0.010 (0.012)
Train: 48 [1200/1251 ( 96%)]  Loss: 3.842 (3.99)  Time: 0.808s, 1266.87/s  (0.812s, 1261.51/s)  LR: 9.844e-04  Data: 0.009 (0.012)
Train: 48 [1250/1251 (100%)]  Loss: 4.163 (3.99)  Time: 0.792s, 1293.17/s  (0.812s, 1261.74/s)  LR: 9.844e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.716 (1.716)  Loss:  1.1104 (1.1104)  Acc@1: 84.8633 (84.8633)  Acc@5: 96.0938 (96.0938)
Test: [  48/48]  Time: 0.194 (0.601)  Loss:  1.1230 (1.6788)  Acc@1: 82.0755 (67.9700)  Acc@5: 95.0472 (88.8340)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-47.pth.tar', 68.9379999194336)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-44.pth.tar', 68.90599997070312)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-45.pth.tar', 68.55000007324219)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-42.pth.tar', 68.39800010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-46.pth.tar', 68.12400002441406)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-48.pth.tar', 67.96999997070313)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-43.pth.tar', 67.87800017578125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-38.pth.tar', 67.83800002197266)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-41.pth.tar', 67.66399997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-39.pth.tar', 67.53800000488282)

Train: 49 [   0/1251 (  0%)]  Loss: 3.597 (3.60)  Time: 2.345s,  436.76/s  (2.345s,  436.76/s)  LR: 9.838e-04  Data: 1.623 (1.623)
Train: 49 [  50/1251 (  4%)]  Loss: 3.724 (3.66)  Time: 0.782s, 1309.37/s  (0.849s, 1206.02/s)  LR: 9.838e-04  Data: 0.011 (0.048)
Train: 49 [ 100/1251 (  8%)]  Loss: 4.221 (3.85)  Time: 0.823s, 1243.63/s  (0.830s, 1233.51/s)  LR: 9.838e-04  Data: 0.009 (0.029)
Train: 49 [ 150/1251 ( 12%)]  Loss: 3.599 (3.79)  Time: 0.818s, 1251.24/s  (0.825s, 1241.12/s)  LR: 9.838e-04  Data: 0.009 (0.023)
Train: 49 [ 200/1251 ( 16%)]  Loss: 4.036 (3.84)  Time: 0.813s, 1259.81/s  (0.822s, 1245.64/s)  LR: 9.838e-04  Data: 0.009 (0.020)
Train: 49 [ 250/1251 ( 20%)]  Loss: 4.291 (3.91)  Time: 0.803s, 1274.83/s  (0.820s, 1248.55/s)  LR: 9.838e-04  Data: 0.009 (0.018)
Train: 49 [ 300/1251 ( 24%)]  Loss: 4.161 (3.95)  Time: 0.778s, 1315.98/s  (0.817s, 1253.59/s)  LR: 9.838e-04  Data: 0.010 (0.017)
Train: 49 [ 350/1251 ( 28%)]  Loss: 4.255 (3.99)  Time: 0.778s, 1316.76/s  (0.815s, 1255.71/s)  LR: 9.838e-04  Data: 0.010 (0.016)
Train: 49 [ 400/1251 ( 32%)]  Loss: 3.879 (3.97)  Time: 0.786s, 1303.31/s  (0.815s, 1256.51/s)  LR: 9.838e-04  Data: 0.010 (0.015)
Train: 49 [ 450/1251 ( 36%)]  Loss: 3.833 (3.96)  Time: 0.774s, 1322.93/s  (0.815s, 1256.73/s)  LR: 9.838e-04  Data: 0.010 (0.015)
Train: 49 [ 500/1251 ( 40%)]  Loss: 3.800 (3.95)  Time: 0.782s, 1309.63/s  (0.814s, 1257.59/s)  LR: 9.838e-04  Data: 0.010 (0.014)
Train: 49 [ 550/1251 ( 44%)]  Loss: 4.365 (3.98)  Time: 0.779s, 1314.95/s  (0.814s, 1257.89/s)  LR: 9.838e-04  Data: 0.010 (0.014)
Train: 49 [ 600/1251 ( 48%)]  Loss: 3.585 (3.95)  Time: 0.799s, 1281.42/s  (0.814s, 1257.91/s)  LR: 9.838e-04  Data: 0.010 (0.014)
Train: 49 [ 650/1251 ( 52%)]  Loss: 4.002 (3.95)  Time: 0.783s, 1308.11/s  (0.813s, 1259.09/s)  LR: 9.838e-04  Data: 0.009 (0.014)
Train: 49 [ 700/1251 ( 56%)]  Loss: 4.015 (3.96)  Time: 0.853s, 1199.80/s  (0.813s, 1258.98/s)  LR: 9.838e-04  Data: 0.009 (0.013)
Train: 49 [ 750/1251 ( 60%)]  Loss: 4.111 (3.97)  Time: 0.808s, 1267.64/s  (0.813s, 1259.24/s)  LR: 9.838e-04  Data: 0.010 (0.013)
Train: 49 [ 800/1251 ( 64%)]  Loss: 3.914 (3.96)  Time: 0.845s, 1211.42/s  (0.813s, 1259.61/s)  LR: 9.838e-04  Data: 0.016 (0.013)
Train: 49 [ 850/1251 ( 68%)]  Loss: 4.106 (3.97)  Time: 0.897s, 1141.25/s  (0.813s, 1260.10/s)  LR: 9.838e-04  Data: 0.009 (0.013)
Train: 49 [ 900/1251 ( 72%)]  Loss: 3.975 (3.97)  Time: 0.858s, 1193.10/s  (0.813s, 1260.09/s)  LR: 9.838e-04  Data: 0.012 (0.013)
Train: 49 [ 950/1251 ( 76%)]  Loss: 3.707 (3.96)  Time: 0.817s, 1253.37/s  (0.813s, 1260.11/s)  LR: 9.838e-04  Data: 0.011 (0.013)
Train: 49 [1000/1251 ( 80%)]  Loss: 3.981 (3.96)  Time: 0.793s, 1290.87/s  (0.812s, 1260.45/s)  LR: 9.838e-04  Data: 0.010 (0.013)
Train: 49 [1050/1251 ( 84%)]  Loss: 4.152 (3.97)  Time: 0.811s, 1263.34/s  (0.812s, 1260.98/s)  LR: 9.838e-04  Data: 0.009 (0.012)
Train: 49 [1100/1251 ( 88%)]  Loss: 3.912 (3.97)  Time: 0.840s, 1219.37/s  (0.812s, 1261.11/s)  LR: 9.838e-04  Data: 0.014 (0.012)
Train: 49 [1150/1251 ( 92%)]  Loss: 3.727 (3.96)  Time: 0.785s, 1304.71/s  (0.812s, 1261.40/s)  LR: 9.838e-04  Data: 0.009 (0.012)
Train: 49 [1200/1251 ( 96%)]  Loss: 4.019 (3.96)  Time: 0.939s, 1090.84/s  (0.812s, 1261.64/s)  LR: 9.838e-04  Data: 0.009 (0.012)
Train: 49 [1250/1251 (100%)]  Loss: 3.777 (3.95)  Time: 0.809s, 1265.66/s  (0.812s, 1261.69/s)  LR: 9.838e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.681 (1.681)  Loss:  0.8989 (0.8989)  Acc@1: 86.4258 (86.4258)  Acc@5: 95.8008 (95.8008)
Test: [  48/48]  Time: 0.193 (0.605)  Loss:  0.8599 (1.5457)  Acc@1: 83.9623 (69.2740)  Acc@5: 96.5802 (89.3400)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-49.pth.tar', 69.27400001464844)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-47.pth.tar', 68.9379999194336)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-44.pth.tar', 68.90599997070312)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-45.pth.tar', 68.55000007324219)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-42.pth.tar', 68.39800010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-46.pth.tar', 68.12400002441406)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-48.pth.tar', 67.96999997070313)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-43.pth.tar', 67.87800017578125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-38.pth.tar', 67.83800002197266)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-41.pth.tar', 67.66399997558594)

Train: 50 [   0/1251 (  0%)]  Loss: 3.905 (3.90)  Time: 2.523s,  405.86/s  (2.523s,  405.86/s)  LR: 9.831e-04  Data: 1.750 (1.750)
Train: 50 [  50/1251 (  4%)]  Loss: 3.853 (3.88)  Time: 0.806s, 1270.15/s  (0.845s, 1211.31/s)  LR: 9.831e-04  Data: 0.010 (0.046)
Train: 50 [ 100/1251 (  8%)]  Loss: 4.285 (4.01)  Time: 0.804s, 1274.08/s  (0.826s, 1240.03/s)  LR: 9.831e-04  Data: 0.009 (0.028)
Train: 50 [ 150/1251 ( 12%)]  Loss: 4.130 (4.04)  Time: 0.779s, 1314.19/s  (0.819s, 1250.46/s)  LR: 9.831e-04  Data: 0.010 (0.023)
Train: 50 [ 200/1251 ( 16%)]  Loss: 3.939 (4.02)  Time: 0.831s, 1232.11/s  (0.817s, 1252.76/s)  LR: 9.831e-04  Data: 0.010 (0.020)
Train: 50 [ 250/1251 ( 20%)]  Loss: 3.542 (3.94)  Time: 0.851s, 1202.91/s  (0.816s, 1254.65/s)  LR: 9.831e-04  Data: 0.009 (0.018)
Train: 50 [ 300/1251 ( 24%)]  Loss: 4.227 (3.98)  Time: 0.815s, 1255.92/s  (0.815s, 1255.88/s)  LR: 9.831e-04  Data: 0.009 (0.017)
Train: 50 [ 350/1251 ( 28%)]  Loss: 3.790 (3.96)  Time: 0.799s, 1282.16/s  (0.814s, 1257.35/s)  LR: 9.831e-04  Data: 0.014 (0.016)
Train: 50 [ 400/1251 ( 32%)]  Loss: 3.843 (3.95)  Time: 0.819s, 1250.53/s  (0.814s, 1258.20/s)  LR: 9.831e-04  Data: 0.009 (0.015)
Train: 50 [ 450/1251 ( 36%)]  Loss: 3.749 (3.93)  Time: 0.786s, 1301.98/s  (0.813s, 1260.16/s)  LR: 9.831e-04  Data: 0.011 (0.015)
Train: 50 [ 500/1251 ( 40%)]  Loss: 4.167 (3.95)  Time: 0.801s, 1278.84/s  (0.812s, 1260.52/s)  LR: 9.831e-04  Data: 0.014 (0.014)
Train: 50 [ 550/1251 ( 44%)]  Loss: 3.692 (3.93)  Time: 0.809s, 1265.79/s  (0.814s, 1257.56/s)  LR: 9.831e-04  Data: 0.010 (0.014)
Train: 50 [ 600/1251 ( 48%)]  Loss: 3.767 (3.91)  Time: 0.821s, 1246.71/s  (0.814s, 1257.27/s)  LR: 9.831e-04  Data: 0.010 (0.014)
Train: 50 [ 650/1251 ( 52%)]  Loss: 3.678 (3.90)  Time: 0.822s, 1245.18/s  (0.814s, 1258.43/s)  LR: 9.831e-04  Data: 0.009 (0.014)
Train: 50 [ 700/1251 ( 56%)]  Loss: 3.598 (3.88)  Time: 0.811s, 1261.89/s  (0.814s, 1258.44/s)  LR: 9.831e-04  Data: 0.010 (0.013)
Train: 50 [ 750/1251 ( 60%)]  Loss: 4.154 (3.89)  Time: 0.777s, 1318.03/s  (0.814s, 1258.60/s)  LR: 9.831e-04  Data: 0.010 (0.013)
Train: 50 [ 800/1251 ( 64%)]  Loss: 3.705 (3.88)  Time: 0.829s, 1235.67/s  (0.813s, 1259.30/s)  LR: 9.831e-04  Data: 0.010 (0.013)
Train: 50 [ 850/1251 ( 68%)]  Loss: 3.597 (3.87)  Time: 0.857s, 1194.99/s  (0.813s, 1260.29/s)  LR: 9.831e-04  Data: 0.009 (0.013)
Train: 50 [ 900/1251 ( 72%)]  Loss: 4.267 (3.89)  Time: 0.847s, 1208.26/s  (0.812s, 1260.49/s)  LR: 9.831e-04  Data: 0.009 (0.013)
Train: 50 [ 950/1251 ( 76%)]  Loss: 3.935 (3.89)  Time: 0.873s, 1172.71/s  (0.812s, 1260.38/s)  LR: 9.831e-04  Data: 0.014 (0.013)
Train: 50 [1000/1251 ( 80%)]  Loss: 3.678 (3.88)  Time: 0.828s, 1237.01/s  (0.812s, 1260.45/s)  LR: 9.831e-04  Data: 0.010 (0.012)
Train: 50 [1050/1251 ( 84%)]  Loss: 3.807 (3.88)  Time: 0.804s, 1274.38/s  (0.812s, 1260.92/s)  LR: 9.831e-04  Data: 0.015 (0.012)
Train: 50 [1100/1251 ( 88%)]  Loss: 4.032 (3.88)  Time: 0.779s, 1314.84/s  (0.812s, 1261.19/s)  LR: 9.831e-04  Data: 0.009 (0.012)
Train: 50 [1150/1251 ( 92%)]  Loss: 3.757 (3.88)  Time: 0.856s, 1196.64/s  (0.812s, 1261.19/s)  LR: 9.831e-04  Data: 0.009 (0.012)
Train: 50 [1200/1251 ( 96%)]  Loss: 3.824 (3.88)  Time: 0.778s, 1316.57/s  (0.812s, 1261.62/s)  LR: 9.831e-04  Data: 0.009 (0.012)
Train: 50 [1250/1251 (100%)]  Loss: 3.874 (3.88)  Time: 0.788s, 1299.83/s  (0.811s, 1261.88/s)  LR: 9.831e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.752 (1.752)  Loss:  1.0303 (1.0303)  Acc@1: 86.3281 (86.3281)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.194 (0.614)  Loss:  1.1455 (1.6571)  Acc@1: 82.1934 (68.9020)  Acc@5: 94.9293 (89.3480)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-49.pth.tar', 69.27400001464844)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-47.pth.tar', 68.9379999194336)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-44.pth.tar', 68.90599997070312)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-50.pth.tar', 68.90199989257812)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-45.pth.tar', 68.55000007324219)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-42.pth.tar', 68.39800010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-46.pth.tar', 68.12400002441406)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-48.pth.tar', 67.96999997070313)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-43.pth.tar', 67.87800017578125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-38.pth.tar', 67.83800002197266)

Train: 51 [   0/1251 (  0%)]  Loss: 3.422 (3.42)  Time: 2.641s,  387.67/s  (2.641s,  387.67/s)  LR: 9.825e-04  Data: 1.861 (1.861)
Train: 51 [  50/1251 (  4%)]  Loss: 3.949 (3.69)  Time: 0.793s, 1291.43/s  (0.853s, 1200.76/s)  LR: 9.825e-04  Data: 0.009 (0.056)
Train: 51 [ 100/1251 (  8%)]  Loss: 3.862 (3.74)  Time: 0.830s, 1233.41/s  (0.832s, 1231.46/s)  LR: 9.825e-04  Data: 0.010 (0.034)
Train: 51 [ 150/1251 ( 12%)]  Loss: 3.945 (3.79)  Time: 0.801s, 1277.87/s  (0.825s, 1241.02/s)  LR: 9.825e-04  Data: 0.013 (0.026)
Train: 51 [ 200/1251 ( 16%)]  Loss: 3.926 (3.82)  Time: 0.823s, 1244.57/s  (0.822s, 1245.72/s)  LR: 9.825e-04  Data: 0.011 (0.022)
Train: 51 [ 250/1251 ( 20%)]  Loss: 4.248 (3.89)  Time: 0.809s, 1266.18/s  (0.819s, 1250.04/s)  LR: 9.825e-04  Data: 0.010 (0.020)
Train: 51 [ 300/1251 ( 24%)]  Loss: 3.817 (3.88)  Time: 0.827s, 1238.06/s  (0.818s, 1252.37/s)  LR: 9.825e-04  Data: 0.009 (0.018)
Train: 51 [ 350/1251 ( 28%)]  Loss: 4.089 (3.91)  Time: 0.783s, 1307.23/s  (0.815s, 1256.85/s)  LR: 9.825e-04  Data: 0.009 (0.017)
Train: 51 [ 400/1251 ( 32%)]  Loss: 4.117 (3.93)  Time: 0.810s, 1263.64/s  (0.814s, 1258.03/s)  LR: 9.825e-04  Data: 0.010 (0.016)
Train: 51 [ 450/1251 ( 36%)]  Loss: 3.952 (3.93)  Time: 0.829s, 1235.64/s  (0.814s, 1257.91/s)  LR: 9.825e-04  Data: 0.009 (0.016)
Train: 51 [ 500/1251 ( 40%)]  Loss: 3.873 (3.93)  Time: 0.813s, 1258.77/s  (0.814s, 1257.82/s)  LR: 9.825e-04  Data: 0.011 (0.015)
Train: 51 [ 550/1251 ( 44%)]  Loss: 4.143 (3.95)  Time: 0.797s, 1285.27/s  (0.814s, 1258.35/s)  LR: 9.825e-04  Data: 0.010 (0.015)
Train: 51 [ 600/1251 ( 48%)]  Loss: 3.776 (3.93)  Time: 0.780s, 1312.11/s  (0.814s, 1258.74/s)  LR: 9.825e-04  Data: 0.010 (0.014)
Train: 51 [ 650/1251 ( 52%)]  Loss: 4.042 (3.94)  Time: 0.775s, 1320.83/s  (0.813s, 1259.47/s)  LR: 9.825e-04  Data: 0.012 (0.014)
Train: 51 [ 700/1251 ( 56%)]  Loss: 4.232 (3.96)  Time: 0.785s, 1303.88/s  (0.813s, 1259.88/s)  LR: 9.825e-04  Data: 0.012 (0.014)
Train: 51 [ 750/1251 ( 60%)]  Loss: 3.925 (3.96)  Time: 0.787s, 1301.60/s  (0.813s, 1259.61/s)  LR: 9.825e-04  Data: 0.010 (0.014)
Train: 51 [ 800/1251 ( 64%)]  Loss: 3.826 (3.95)  Time: 0.781s, 1310.89/s  (0.812s, 1260.48/s)  LR: 9.825e-04  Data: 0.014 (0.013)
Train: 51 [ 850/1251 ( 68%)]  Loss: 4.026 (3.95)  Time: 0.816s, 1255.43/s  (0.812s, 1260.53/s)  LR: 9.825e-04  Data: 0.009 (0.013)
Train: 51 [ 900/1251 ( 72%)]  Loss: 3.696 (3.94)  Time: 0.857s, 1195.47/s  (0.812s, 1260.69/s)  LR: 9.825e-04  Data: 0.010 (0.013)
Train: 51 [ 950/1251 ( 76%)]  Loss: 3.583 (3.92)  Time: 0.782s, 1308.99/s  (0.812s, 1260.74/s)  LR: 9.825e-04  Data: 0.010 (0.013)
Train: 51 [1000/1251 ( 80%)]  Loss: 4.124 (3.93)  Time: 0.782s, 1310.08/s  (0.812s, 1261.12/s)  LR: 9.825e-04  Data: 0.010 (0.013)
Train: 51 [1050/1251 ( 84%)]  Loss: 3.687 (3.92)  Time: 0.831s, 1232.49/s  (0.812s, 1261.42/s)  LR: 9.825e-04  Data: 0.010 (0.013)
Train: 51 [1100/1251 ( 88%)]  Loss: 3.913 (3.92)  Time: 0.817s, 1253.92/s  (0.812s, 1261.53/s)  LR: 9.825e-04  Data: 0.010 (0.013)
Train: 51 [1150/1251 ( 92%)]  Loss: 3.837 (3.92)  Time: 0.790s, 1296.28/s  (0.812s, 1261.57/s)  LR: 9.825e-04  Data: 0.012 (0.013)
Train: 51 [1200/1251 ( 96%)]  Loss: 3.575 (3.90)  Time: 0.781s, 1310.31/s  (0.812s, 1261.36/s)  LR: 9.825e-04  Data: 0.010 (0.012)
Train: 51 [1250/1251 (100%)]  Loss: 3.709 (3.90)  Time: 0.926s, 1106.06/s  (0.812s, 1261.47/s)  LR: 9.825e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.655 (1.655)  Loss:  1.0205 (1.0205)  Acc@1: 84.9609 (84.9609)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.195 (0.602)  Loss:  1.1367 (1.5835)  Acc@1: 81.7217 (69.2580)  Acc@5: 94.3396 (89.6580)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-49.pth.tar', 69.27400001464844)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-51.pth.tar', 69.25799994628906)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-47.pth.tar', 68.9379999194336)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-44.pth.tar', 68.90599997070312)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-50.pth.tar', 68.90199989257812)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-45.pth.tar', 68.55000007324219)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-42.pth.tar', 68.39800010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-46.pth.tar', 68.12400002441406)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-48.pth.tar', 67.96999997070313)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-43.pth.tar', 67.87800017578125)

Train: 52 [   0/1251 (  0%)]  Loss: 3.749 (3.75)  Time: 2.399s,  426.87/s  (2.399s,  426.87/s)  LR: 9.818e-04  Data: 1.673 (1.673)
Train: 52 [  50/1251 (  4%)]  Loss: 3.742 (3.75)  Time: 0.891s, 1149.90/s  (0.847s, 1208.34/s)  LR: 9.818e-04  Data: 0.010 (0.048)
Train: 52 [ 100/1251 (  8%)]  Loss: 3.916 (3.80)  Time: 0.823s, 1244.56/s  (0.829s, 1235.22/s)  LR: 9.818e-04  Data: 0.010 (0.030)
Train: 52 [ 150/1251 ( 12%)]  Loss: 4.212 (3.90)  Time: 0.801s, 1278.63/s  (0.824s, 1242.51/s)  LR: 9.818e-04  Data: 0.009 (0.023)
Train: 52 [ 200/1251 ( 16%)]  Loss: 3.715 (3.87)  Time: 0.818s, 1251.45/s  (0.821s, 1247.47/s)  LR: 9.818e-04  Data: 0.010 (0.020)
Train: 52 [ 250/1251 ( 20%)]  Loss: 3.838 (3.86)  Time: 0.828s, 1236.16/s  (0.819s, 1250.08/s)  LR: 9.818e-04  Data: 0.009 (0.018)
Train: 52 [ 300/1251 ( 24%)]  Loss: 3.553 (3.82)  Time: 0.804s, 1273.48/s  (0.816s, 1254.95/s)  LR: 9.818e-04  Data: 0.013 (0.017)
Train: 52 [ 350/1251 ( 28%)]  Loss: 3.537 (3.78)  Time: 0.799s, 1282.28/s  (0.815s, 1256.60/s)  LR: 9.818e-04  Data: 0.009 (0.016)
Train: 52 [ 400/1251 ( 32%)]  Loss: 4.019 (3.81)  Time: 0.805s, 1271.61/s  (0.815s, 1257.17/s)  LR: 9.818e-04  Data: 0.010 (0.015)
Train: 52 [ 450/1251 ( 36%)]  Loss: 3.651 (3.79)  Time: 0.904s, 1133.07/s  (0.814s, 1257.59/s)  LR: 9.818e-04  Data: 0.013 (0.015)
Train: 52 [ 500/1251 ( 40%)]  Loss: 4.158 (3.83)  Time: 0.817s, 1253.39/s  (0.813s, 1259.20/s)  LR: 9.818e-04  Data: 0.009 (0.014)
Train: 52 [ 550/1251 ( 44%)]  Loss: 3.826 (3.83)  Time: 0.813s, 1259.19/s  (0.812s, 1260.73/s)  LR: 9.818e-04  Data: 0.014 (0.014)
Train: 52 [ 600/1251 ( 48%)]  Loss: 3.588 (3.81)  Time: 0.779s, 1314.79/s  (0.811s, 1261.86/s)  LR: 9.818e-04  Data: 0.010 (0.014)
Train: 52 [ 650/1251 ( 52%)]  Loss: 3.897 (3.81)  Time: 0.780s, 1313.59/s  (0.811s, 1262.44/s)  LR: 9.818e-04  Data: 0.010 (0.013)
Train: 52 [ 700/1251 ( 56%)]  Loss: 3.897 (3.82)  Time: 0.874s, 1171.36/s  (0.811s, 1261.91/s)  LR: 9.818e-04  Data: 0.009 (0.013)
Train: 52 [ 750/1251 ( 60%)]  Loss: 3.977 (3.83)  Time: 0.808s, 1267.63/s  (0.811s, 1262.24/s)  LR: 9.818e-04  Data: 0.010 (0.013)
Train: 52 [ 800/1251 ( 64%)]  Loss: 3.893 (3.83)  Time: 0.832s, 1230.76/s  (0.811s, 1262.62/s)  LR: 9.818e-04  Data: 0.010 (0.013)
Train: 52 [ 850/1251 ( 68%)]  Loss: 3.771 (3.83)  Time: 0.791s, 1293.97/s  (0.811s, 1262.79/s)  LR: 9.818e-04  Data: 0.014 (0.013)
Train: 52 [ 900/1251 ( 72%)]  Loss: 4.077 (3.84)  Time: 0.849s, 1205.66/s  (0.811s, 1262.92/s)  LR: 9.818e-04  Data: 0.015 (0.013)
Train: 52 [ 950/1251 ( 76%)]  Loss: 3.810 (3.84)  Time: 0.808s, 1267.00/s  (0.811s, 1262.65/s)  LR: 9.818e-04  Data: 0.009 (0.013)
Train: 52 [1000/1251 ( 80%)]  Loss: 3.789 (3.84)  Time: 0.811s, 1262.36/s  (0.811s, 1262.38/s)  LR: 9.818e-04  Data: 0.010 (0.013)
Train: 52 [1050/1251 ( 84%)]  Loss: 3.952 (3.84)  Time: 0.807s, 1268.42/s  (0.811s, 1262.59/s)  LR: 9.818e-04  Data: 0.010 (0.012)
Train: 52 [1100/1251 ( 88%)]  Loss: 3.952 (3.85)  Time: 0.785s, 1304.62/s  (0.811s, 1262.38/s)  LR: 9.818e-04  Data: 0.012 (0.012)
Train: 52 [1150/1251 ( 92%)]  Loss: 3.792 (3.85)  Time: 0.837s, 1223.68/s  (0.811s, 1261.93/s)  LR: 9.818e-04  Data: 0.010 (0.012)
Train: 52 [1200/1251 ( 96%)]  Loss: 3.928 (3.85)  Time: 0.777s, 1317.34/s  (0.811s, 1261.92/s)  LR: 9.818e-04  Data: 0.009 (0.012)
Train: 52 [1250/1251 (100%)]  Loss: 3.369 (3.83)  Time: 0.760s, 1347.25/s  (0.811s, 1262.05/s)  LR: 9.818e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.701 (1.701)  Loss:  0.8999 (0.8999)  Acc@1: 86.3281 (86.3281)  Acc@5: 95.8984 (95.8984)
Test: [  48/48]  Time: 0.194 (0.604)  Loss:  0.9478 (1.5490)  Acc@1: 84.0802 (69.2880)  Acc@5: 95.9906 (89.6280)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-52.pth.tar', 69.28799993652343)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-49.pth.tar', 69.27400001464844)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-51.pth.tar', 69.25799994628906)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-47.pth.tar', 68.9379999194336)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-44.pth.tar', 68.90599997070312)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-50.pth.tar', 68.90199989257812)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-45.pth.tar', 68.55000007324219)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-42.pth.tar', 68.39800010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-46.pth.tar', 68.12400002441406)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-48.pth.tar', 67.96999997070313)

Train: 53 [   0/1251 (  0%)]  Loss: 3.802 (3.80)  Time: 2.587s,  395.87/s  (2.587s,  395.87/s)  LR: 9.811e-04  Data: 1.860 (1.860)
Train: 53 [  50/1251 (  4%)]  Loss: 3.715 (3.76)  Time: 0.816s, 1255.24/s  (0.853s, 1200.98/s)  LR: 9.811e-04  Data: 0.010 (0.053)
Train: 53 [ 100/1251 (  8%)]  Loss: 3.730 (3.75)  Time: 0.849s, 1205.74/s  (0.835s, 1226.72/s)  LR: 9.811e-04  Data: 0.009 (0.032)
Train: 53 [ 150/1251 ( 12%)]  Loss: 3.981 (3.81)  Time: 0.813s, 1258.86/s  (0.828s, 1237.29/s)  LR: 9.811e-04  Data: 0.010 (0.025)
Train: 53 [ 200/1251 ( 16%)]  Loss: 3.928 (3.83)  Time: 0.778s, 1316.67/s  (0.824s, 1242.77/s)  LR: 9.811e-04  Data: 0.010 (0.021)
Train: 53 [ 250/1251 ( 20%)]  Loss: 3.447 (3.77)  Time: 0.829s, 1234.99/s  (0.821s, 1247.45/s)  LR: 9.811e-04  Data: 0.009 (0.019)
Train: 53 [ 300/1251 ( 24%)]  Loss: 3.720 (3.76)  Time: 0.836s, 1225.44/s  (0.819s, 1250.66/s)  LR: 9.811e-04  Data: 0.010 (0.018)
Train: 53 [ 350/1251 ( 28%)]  Loss: 3.775 (3.76)  Time: 0.855s, 1197.33/s  (0.817s, 1253.75/s)  LR: 9.811e-04  Data: 0.010 (0.017)
Train: 53 [ 400/1251 ( 32%)]  Loss: 4.011 (3.79)  Time: 0.846s, 1210.64/s  (0.816s, 1255.02/s)  LR: 9.811e-04  Data: 0.018 (0.016)
Train: 53 [ 450/1251 ( 36%)]  Loss: 3.936 (3.80)  Time: 0.810s, 1263.69/s  (0.816s, 1255.47/s)  LR: 9.811e-04  Data: 0.010 (0.015)
Train: 53 [ 500/1251 ( 40%)]  Loss: 4.020 (3.82)  Time: 0.883s, 1160.08/s  (0.815s, 1255.78/s)  LR: 9.811e-04  Data: 0.012 (0.015)
Train: 53 [ 550/1251 ( 44%)]  Loss: 4.082 (3.85)  Time: 0.836s, 1224.24/s  (0.815s, 1256.41/s)  LR: 9.811e-04  Data: 0.009 (0.014)
Train: 53 [ 600/1251 ( 48%)]  Loss: 3.620 (3.83)  Time: 0.828s, 1237.29/s  (0.814s, 1257.47/s)  LR: 9.811e-04  Data: 0.015 (0.014)
Train: 53 [ 650/1251 ( 52%)]  Loss: 4.000 (3.84)  Time: 0.803s, 1275.63/s  (0.815s, 1257.16/s)  LR: 9.811e-04  Data: 0.010 (0.014)
Train: 53 [ 700/1251 ( 56%)]  Loss: 3.896 (3.84)  Time: 0.814s, 1258.37/s  (0.814s, 1258.27/s)  LR: 9.811e-04  Data: 0.010 (0.014)
Train: 53 [ 750/1251 ( 60%)]  Loss: 3.945 (3.85)  Time: 0.789s, 1298.43/s  (0.813s, 1258.87/s)  LR: 9.811e-04  Data: 0.014 (0.013)
Train: 53 [ 800/1251 ( 64%)]  Loss: 3.782 (3.85)  Time: 0.823s, 1244.78/s  (0.813s, 1259.63/s)  LR: 9.811e-04  Data: 0.010 (0.013)
Train: 53 [ 850/1251 ( 68%)]  Loss: 3.975 (3.85)  Time: 0.793s, 1291.50/s  (0.813s, 1259.93/s)  LR: 9.811e-04  Data: 0.009 (0.013)
Train: 53 [ 900/1251 ( 72%)]  Loss: 3.656 (3.84)  Time: 0.780s, 1312.57/s  (0.812s, 1260.53/s)  LR: 9.811e-04  Data: 0.010 (0.013)
Train: 53 [ 950/1251 ( 76%)]  Loss: 3.657 (3.83)  Time: 0.794s, 1289.15/s  (0.812s, 1260.79/s)  LR: 9.811e-04  Data: 0.009 (0.013)
Train: 53 [1000/1251 ( 80%)]  Loss: 3.956 (3.84)  Time: 0.780s, 1311.99/s  (0.812s, 1261.34/s)  LR: 9.811e-04  Data: 0.010 (0.013)
Train: 53 [1050/1251 ( 84%)]  Loss: 3.818 (3.84)  Time: 0.804s, 1273.93/s  (0.812s, 1261.76/s)  LR: 9.811e-04  Data: 0.014 (0.013)
Train: 53 [1100/1251 ( 88%)]  Loss: 3.529 (3.83)  Time: 0.782s, 1309.41/s  (0.811s, 1261.96/s)  LR: 9.811e-04  Data: 0.010 (0.013)
Train: 53 [1150/1251 ( 92%)]  Loss: 3.662 (3.82)  Time: 0.770s, 1329.82/s  (0.811s, 1262.33/s)  LR: 9.811e-04  Data: 0.009 (0.012)
Train: 53 [1200/1251 ( 96%)]  Loss: 3.785 (3.82)  Time: 0.961s, 1065.40/s  (0.811s, 1262.37/s)  LR: 9.811e-04  Data: 0.010 (0.012)
Train: 53 [1250/1251 (100%)]  Loss: 3.968 (3.82)  Time: 0.769s, 1332.16/s  (0.811s, 1262.60/s)  LR: 9.811e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.757 (1.757)  Loss:  1.0195 (1.0195)  Acc@1: 85.6445 (85.6445)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.196 (0.615)  Loss:  1.0283 (1.5994)  Acc@1: 82.7830 (69.4980)  Acc@5: 96.5802 (89.5940)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-53.pth.tar', 69.49800014892578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-52.pth.tar', 69.28799993652343)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-49.pth.tar', 69.27400001464844)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-51.pth.tar', 69.25799994628906)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-47.pth.tar', 68.9379999194336)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-44.pth.tar', 68.90599997070312)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-50.pth.tar', 68.90199989257812)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-45.pth.tar', 68.55000007324219)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-42.pth.tar', 68.39800010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-46.pth.tar', 68.12400002441406)

Train: 54 [   0/1251 (  0%)]  Loss: 4.028 (4.03)  Time: 2.380s,  430.30/s  (2.380s,  430.30/s)  LR: 9.803e-04  Data: 1.632 (1.632)
Train: 54 [  50/1251 (  4%)]  Loss: 3.850 (3.94)  Time: 0.781s, 1311.59/s  (0.847s, 1208.82/s)  LR: 9.803e-04  Data: 0.009 (0.048)
Train: 54 [ 100/1251 (  8%)]  Loss: 3.802 (3.89)  Time: 0.886s, 1156.05/s  (0.830s, 1233.15/s)  LR: 9.803e-04  Data: 0.010 (0.030)
Train: 54 [ 150/1251 ( 12%)]  Loss: 4.205 (3.97)  Time: 0.821s, 1247.35/s  (0.822s, 1245.22/s)  LR: 9.803e-04  Data: 0.010 (0.023)
Train: 54 [ 200/1251 ( 16%)]  Loss: 3.718 (3.92)  Time: 0.788s, 1300.13/s  (0.820s, 1249.42/s)  LR: 9.803e-04  Data: 0.010 (0.020)
Train: 54 [ 250/1251 ( 20%)]  Loss: 3.954 (3.93)  Time: 0.835s, 1226.96/s  (0.817s, 1253.81/s)  LR: 9.803e-04  Data: 0.012 (0.018)
Train: 54 [ 300/1251 ( 24%)]  Loss: 4.189 (3.96)  Time: 0.838s, 1222.46/s  (0.815s, 1256.13/s)  LR: 9.803e-04  Data: 0.013 (0.017)
Train: 54 [ 350/1251 ( 28%)]  Loss: 4.113 (3.98)  Time: 0.779s, 1314.59/s  (0.815s, 1256.73/s)  LR: 9.803e-04  Data: 0.010 (0.016)
Train: 54 [ 400/1251 ( 32%)]  Loss: 4.159 (4.00)  Time: 0.846s, 1210.07/s  (0.815s, 1256.85/s)  LR: 9.803e-04  Data: 0.009 (0.015)
Train: 54 [ 450/1251 ( 36%)]  Loss: 3.993 (4.00)  Time: 0.800s, 1279.21/s  (0.814s, 1257.56/s)  LR: 9.803e-04  Data: 0.010 (0.015)
Train: 54 [ 500/1251 ( 40%)]  Loss: 4.044 (4.01)  Time: 0.836s, 1225.50/s  (0.814s, 1258.54/s)  LR: 9.803e-04  Data: 0.010 (0.014)
Train: 54 [ 550/1251 ( 44%)]  Loss: 3.712 (3.98)  Time: 0.779s, 1314.75/s  (0.813s, 1259.28/s)  LR: 9.803e-04  Data: 0.010 (0.014)
Train: 54 [ 600/1251 ( 48%)]  Loss: 3.857 (3.97)  Time: 0.813s, 1259.87/s  (0.813s, 1259.47/s)  LR: 9.803e-04  Data: 0.010 (0.014)
Train: 54 [ 650/1251 ( 52%)]  Loss: 4.021 (3.97)  Time: 0.832s, 1230.53/s  (0.813s, 1259.97/s)  LR: 9.803e-04  Data: 0.009 (0.013)
Train: 54 [ 700/1251 ( 56%)]  Loss: 3.684 (3.96)  Time: 0.821s, 1247.91/s  (0.813s, 1259.85/s)  LR: 9.803e-04  Data: 0.013 (0.013)
Train: 54 [ 750/1251 ( 60%)]  Loss: 3.740 (3.94)  Time: 0.860s, 1191.01/s  (0.813s, 1260.27/s)  LR: 9.803e-04  Data: 0.010 (0.013)
Train: 54 [ 800/1251 ( 64%)]  Loss: 4.160 (3.95)  Time: 0.797s, 1284.92/s  (0.812s, 1260.75/s)  LR: 9.803e-04  Data: 0.018 (0.013)
Train: 54 [ 850/1251 ( 68%)]  Loss: 3.838 (3.95)  Time: 0.835s, 1226.09/s  (0.812s, 1261.19/s)  LR: 9.803e-04  Data: 0.014 (0.013)
Train: 54 [ 900/1251 ( 72%)]  Loss: 3.999 (3.95)  Time: 0.804s, 1273.33/s  (0.812s, 1260.99/s)  LR: 9.803e-04  Data: 0.009 (0.013)
Train: 54 [ 950/1251 ( 76%)]  Loss: 3.517 (3.93)  Time: 0.785s, 1304.53/s  (0.812s, 1261.58/s)  LR: 9.803e-04  Data: 0.010 (0.013)
Train: 54 [1000/1251 ( 80%)]  Loss: 3.708 (3.92)  Time: 0.777s, 1317.17/s  (0.811s, 1262.12/s)  LR: 9.803e-04  Data: 0.009 (0.013)
Train: 54 [1050/1251 ( 84%)]  Loss: 4.150 (3.93)  Time: 0.782s, 1308.66/s  (0.811s, 1262.17/s)  LR: 9.803e-04  Data: 0.009 (0.012)
Train: 54 [1100/1251 ( 88%)]  Loss: 3.790 (3.92)  Time: 0.775s, 1321.12/s  (0.811s, 1262.69/s)  LR: 9.803e-04  Data: 0.009 (0.012)
Train: 54 [1150/1251 ( 92%)]  Loss: 4.006 (3.93)  Time: 0.816s, 1254.95/s  (0.811s, 1262.70/s)  LR: 9.803e-04  Data: 0.010 (0.012)
Train: 54 [1200/1251 ( 96%)]  Loss: 4.019 (3.93)  Time: 0.779s, 1314.95/s  (0.811s, 1262.89/s)  LR: 9.803e-04  Data: 0.010 (0.012)
Train: 54 [1250/1251 (100%)]  Loss: 3.671 (3.92)  Time: 0.802s, 1276.04/s  (0.811s, 1262.48/s)  LR: 9.803e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.696 (1.696)  Loss:  0.9326 (0.9326)  Acc@1: 85.1562 (85.1562)  Acc@5: 95.5078 (95.5078)
Test: [  48/48]  Time: 0.194 (0.604)  Loss:  1.0713 (1.5789)  Acc@1: 81.9575 (69.1540)  Acc@5: 94.5755 (89.5520)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-53.pth.tar', 69.49800014892578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-52.pth.tar', 69.28799993652343)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-49.pth.tar', 69.27400001464844)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-51.pth.tar', 69.25799994628906)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-54.pth.tar', 69.15399991943359)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-47.pth.tar', 68.9379999194336)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-44.pth.tar', 68.90599997070312)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-50.pth.tar', 68.90199989257812)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-45.pth.tar', 68.55000007324219)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-42.pth.tar', 68.39800010498047)

Train: 55 [   0/1251 (  0%)]  Loss: 4.088 (4.09)  Time: 2.395s,  427.48/s  (2.395s,  427.48/s)  LR: 9.796e-04  Data: 1.663 (1.663)
Train: 55 [  50/1251 (  4%)]  Loss: 3.798 (3.94)  Time: 0.807s, 1269.12/s  (0.846s, 1210.20/s)  LR: 9.796e-04  Data: 0.009 (0.049)
Train: 55 [ 100/1251 (  8%)]  Loss: 3.679 (3.85)  Time: 0.788s, 1299.86/s  (0.830s, 1233.71/s)  LR: 9.796e-04  Data: 0.010 (0.030)
Train: 55 [ 150/1251 ( 12%)]  Loss: 3.838 (3.85)  Time: 0.946s, 1082.60/s  (0.824s, 1242.68/s)  LR: 9.796e-04  Data: 0.011 (0.023)
Train: 55 [ 200/1251 ( 16%)]  Loss: 3.637 (3.81)  Time: 0.826s, 1239.42/s  (0.827s, 1237.56/s)  LR: 9.796e-04  Data: 0.011 (0.021)
Train: 55 [ 250/1251 ( 20%)]  Loss: 3.502 (3.76)  Time: 0.820s, 1248.32/s  (0.827s, 1237.67/s)  LR: 9.796e-04  Data: 0.012 (0.019)
Train: 55 [ 300/1251 ( 24%)]  Loss: 4.067 (3.80)  Time: 0.812s, 1260.77/s  (0.826s, 1239.67/s)  LR: 9.796e-04  Data: 0.009 (0.018)
Train: 55 [ 350/1251 ( 28%)]  Loss: 3.809 (3.80)  Time: 0.843s, 1214.94/s  (0.824s, 1243.41/s)  LR: 9.796e-04  Data: 0.009 (0.017)
Train: 55 [ 400/1251 ( 32%)]  Loss: 3.898 (3.81)  Time: 0.828s, 1236.04/s  (0.823s, 1243.59/s)  LR: 9.796e-04  Data: 0.009 (0.016)
Train: 55 [ 450/1251 ( 36%)]  Loss: 4.171 (3.85)  Time: 0.776s, 1319.94/s  (0.822s, 1246.44/s)  LR: 9.796e-04  Data: 0.010 (0.015)
Train: 55 [ 500/1251 ( 40%)]  Loss: 3.830 (3.85)  Time: 0.800s, 1279.83/s  (0.821s, 1247.63/s)  LR: 9.796e-04  Data: 0.014 (0.015)
Train: 55 [ 550/1251 ( 44%)]  Loss: 3.579 (3.82)  Time: 0.806s, 1270.29/s  (0.820s, 1249.41/s)  LR: 9.796e-04  Data: 0.010 (0.014)
Train: 55 [ 600/1251 ( 48%)]  Loss: 3.672 (3.81)  Time: 0.783s, 1307.36/s  (0.819s, 1250.16/s)  LR: 9.796e-04  Data: 0.010 (0.014)
Train: 55 [ 650/1251 ( 52%)]  Loss: 3.572 (3.80)  Time: 0.876s, 1169.41/s  (0.819s, 1250.92/s)  LR: 9.796e-04  Data: 0.010 (0.014)
Train: 55 [ 700/1251 ( 56%)]  Loss: 3.998 (3.81)  Time: 0.809s, 1265.76/s  (0.818s, 1252.21/s)  LR: 9.796e-04  Data: 0.009 (0.014)
Train: 55 [ 750/1251 ( 60%)]  Loss: 3.763 (3.81)  Time: 0.824s, 1242.59/s  (0.817s, 1252.75/s)  LR: 9.796e-04  Data: 0.010 (0.013)
Train: 55 [ 800/1251 ( 64%)]  Loss: 3.830 (3.81)  Time: 0.775s, 1321.88/s  (0.817s, 1253.22/s)  LR: 9.796e-04  Data: 0.010 (0.013)
Train: 55 [ 850/1251 ( 68%)]  Loss: 3.547 (3.79)  Time: 0.782s, 1310.04/s  (0.817s, 1254.13/s)  LR: 9.796e-04  Data: 0.013 (0.013)
Train: 55 [ 900/1251 ( 72%)]  Loss: 3.441 (3.77)  Time: 0.827s, 1238.39/s  (0.816s, 1254.17/s)  LR: 9.796e-04  Data: 0.009 (0.013)
Train: 55 [ 950/1251 ( 76%)]  Loss: 3.410 (3.76)  Time: 0.816s, 1254.50/s  (0.816s, 1254.35/s)  LR: 9.796e-04  Data: 0.010 (0.013)
Train: 55 [1000/1251 ( 80%)]  Loss: 3.566 (3.75)  Time: 0.807s, 1268.49/s  (0.816s, 1255.16/s)  LR: 9.796e-04  Data: 0.010 (0.013)
Train: 55 [1050/1251 ( 84%)]  Loss: 3.727 (3.75)  Time: 0.794s, 1289.76/s  (0.815s, 1255.96/s)  LR: 9.796e-04  Data: 0.010 (0.013)
Train: 55 [1100/1251 ( 88%)]  Loss: 4.027 (3.76)  Time: 0.792s, 1293.10/s  (0.815s, 1256.19/s)  LR: 9.796e-04  Data: 0.014 (0.012)
Train: 55 [1150/1251 ( 92%)]  Loss: 3.770 (3.76)  Time: 0.851s, 1203.63/s  (0.815s, 1256.90/s)  LR: 9.796e-04  Data: 0.012 (0.012)
Train: 55 [1200/1251 ( 96%)]  Loss: 3.878 (3.76)  Time: 0.812s, 1260.95/s  (0.815s, 1257.10/s)  LR: 9.796e-04  Data: 0.009 (0.012)
Train: 55 [1250/1251 (100%)]  Loss: 3.388 (3.75)  Time: 0.803s, 1274.55/s  (0.814s, 1257.37/s)  LR: 9.796e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.660 (1.660)  Loss:  0.8672 (0.8672)  Acc@1: 85.3516 (85.3516)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.194 (0.611)  Loss:  1.1143 (1.5758)  Acc@1: 81.2500 (69.5240)  Acc@5: 94.4576 (89.8860)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-55.pth.tar', 69.524)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-53.pth.tar', 69.49800014892578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-52.pth.tar', 69.28799993652343)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-49.pth.tar', 69.27400001464844)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-51.pth.tar', 69.25799994628906)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-54.pth.tar', 69.15399991943359)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-47.pth.tar', 68.9379999194336)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-44.pth.tar', 68.90599997070312)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-50.pth.tar', 68.90199989257812)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-45.pth.tar', 68.55000007324219)

Train: 56 [   0/1251 (  0%)]  Loss: 3.639 (3.64)  Time: 2.303s,  444.65/s  (2.303s,  444.65/s)  LR: 9.789e-04  Data: 1.555 (1.555)
Train: 56 [  50/1251 (  4%)]  Loss: 3.952 (3.80)  Time: 0.778s, 1316.81/s  (0.843s, 1214.43/s)  LR: 9.789e-04  Data: 0.010 (0.047)
Train: 56 [ 100/1251 (  8%)]  Loss: 3.537 (3.71)  Time: 0.780s, 1313.48/s  (0.822s, 1245.01/s)  LR: 9.789e-04  Data: 0.010 (0.029)
Train: 56 [ 150/1251 ( 12%)]  Loss: 3.356 (3.62)  Time: 0.778s, 1316.66/s  (0.817s, 1253.44/s)  LR: 9.789e-04  Data: 0.010 (0.023)
Train: 56 [ 200/1251 ( 16%)]  Loss: 3.550 (3.61)  Time: 0.779s, 1314.33/s  (0.817s, 1252.84/s)  LR: 9.789e-04  Data: 0.009 (0.020)
Train: 56 [ 250/1251 ( 20%)]  Loss: 3.785 (3.64)  Time: 0.851s, 1203.46/s  (0.815s, 1255.86/s)  LR: 9.789e-04  Data: 0.011 (0.018)
Train: 56 [ 300/1251 ( 24%)]  Loss: 3.963 (3.68)  Time: 0.806s, 1270.42/s  (0.814s, 1258.29/s)  LR: 9.789e-04  Data: 0.010 (0.017)
Train: 56 [ 350/1251 ( 28%)]  Loss: 4.085 (3.73)  Time: 0.781s, 1311.32/s  (0.813s, 1259.65/s)  LR: 9.789e-04  Data: 0.012 (0.016)
Train: 56 [ 400/1251 ( 32%)]  Loss: 3.520 (3.71)  Time: 0.814s, 1258.76/s  (0.812s, 1260.81/s)  LR: 9.789e-04  Data: 0.010 (0.015)
Train: 56 [ 450/1251 ( 36%)]  Loss: 3.413 (3.68)  Time: 0.788s, 1299.99/s  (0.812s, 1260.66/s)  LR: 9.789e-04  Data: 0.010 (0.015)
Train: 56 [ 500/1251 ( 40%)]  Loss: 4.136 (3.72)  Time: 0.791s, 1294.08/s  (0.812s, 1260.75/s)  LR: 9.789e-04  Data: 0.012 (0.014)
Train: 56 [ 550/1251 ( 44%)]  Loss: 3.867 (3.73)  Time: 0.847s, 1208.88/s  (0.812s, 1260.87/s)  LR: 9.789e-04  Data: 0.010 (0.014)
Train: 56 [ 600/1251 ( 48%)]  Loss: 3.535 (3.72)  Time: 0.797s, 1284.16/s  (0.812s, 1261.76/s)  LR: 9.789e-04  Data: 0.010 (0.014)
Train: 56 [ 650/1251 ( 52%)]  Loss: 4.111 (3.75)  Time: 0.778s, 1316.61/s  (0.811s, 1262.15/s)  LR: 9.789e-04  Data: 0.009 (0.013)
Train: 56 [ 700/1251 ( 56%)]  Loss: 3.901 (3.76)  Time: 0.791s, 1295.17/s  (0.812s, 1261.41/s)  LR: 9.789e-04  Data: 0.010 (0.013)
Train: 56 [ 750/1251 ( 60%)]  Loss: 3.664 (3.75)  Time: 0.843s, 1214.30/s  (0.812s, 1261.34/s)  LR: 9.789e-04  Data: 0.010 (0.013)
Train: 56 [ 800/1251 ( 64%)]  Loss: 3.738 (3.75)  Time: 0.782s, 1309.07/s  (0.812s, 1261.05/s)  LR: 9.789e-04  Data: 0.013 (0.013)
Train: 56 [ 850/1251 ( 68%)]  Loss: 3.507 (3.74)  Time: 0.812s, 1260.75/s  (0.812s, 1261.23/s)  LR: 9.789e-04  Data: 0.013 (0.013)
Train: 56 [ 900/1251 ( 72%)]  Loss: 3.907 (3.75)  Time: 0.815s, 1256.45/s  (0.812s, 1261.19/s)  LR: 9.789e-04  Data: 0.010 (0.013)
Train: 56 [ 950/1251 ( 76%)]  Loss: 3.937 (3.76)  Time: 0.845s, 1212.48/s  (0.812s, 1261.28/s)  LR: 9.789e-04  Data: 0.010 (0.012)
Train: 56 [1000/1251 ( 80%)]  Loss: 3.964 (3.77)  Time: 0.811s, 1262.93/s  (0.812s, 1261.67/s)  LR: 9.789e-04  Data: 0.010 (0.012)
Train: 56 [1050/1251 ( 84%)]  Loss: 3.545 (3.76)  Time: 0.783s, 1307.96/s  (0.811s, 1261.87/s)  LR: 9.789e-04  Data: 0.010 (0.012)
Train: 56 [1100/1251 ( 88%)]  Loss: 3.583 (3.75)  Time: 0.780s, 1312.95/s  (0.811s, 1262.09/s)  LR: 9.789e-04  Data: 0.010 (0.012)
Train: 56 [1150/1251 ( 92%)]  Loss: 4.083 (3.76)  Time: 0.796s, 1285.72/s  (0.811s, 1262.43/s)  LR: 9.789e-04  Data: 0.009 (0.012)
Train: 56 [1200/1251 ( 96%)]  Loss: 3.592 (3.75)  Time: 0.844s, 1212.98/s  (0.811s, 1262.13/s)  LR: 9.789e-04  Data: 0.021 (0.012)
Train: 56 [1250/1251 (100%)]  Loss: 3.860 (3.76)  Time: 0.773s, 1325.46/s  (0.811s, 1262.39/s)  LR: 9.789e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.741 (1.741)  Loss:  0.8711 (0.8711)  Acc@1: 86.0352 (86.0352)  Acc@5: 96.0938 (96.0938)
Test: [  48/48]  Time: 0.194 (0.613)  Loss:  1.0010 (1.5200)  Acc@1: 82.1934 (69.5600)  Acc@5: 95.0472 (89.8640)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-56.pth.tar', 69.56000015136719)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-55.pth.tar', 69.524)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-53.pth.tar', 69.49800014892578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-52.pth.tar', 69.28799993652343)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-49.pth.tar', 69.27400001464844)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-51.pth.tar', 69.25799994628906)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-54.pth.tar', 69.15399991943359)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-47.pth.tar', 68.9379999194336)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-44.pth.tar', 68.90599997070312)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-50.pth.tar', 68.90199989257812)

Train: 57 [   0/1251 (  0%)]  Loss: 3.853 (3.85)  Time: 2.408s,  425.25/s  (2.408s,  425.25/s)  LR: 9.781e-04  Data: 1.630 (1.630)
Train: 57 [  50/1251 (  4%)]  Loss: 3.674 (3.76)  Time: 0.778s, 1316.75/s  (0.838s, 1222.05/s)  LR: 9.781e-04  Data: 0.009 (0.046)
Train: 57 [ 100/1251 (  8%)]  Loss: 4.223 (3.92)  Time: 0.785s, 1305.02/s  (0.825s, 1241.61/s)  LR: 9.781e-04  Data: 0.009 (0.029)
Train: 57 [ 150/1251 ( 12%)]  Loss: 4.226 (3.99)  Time: 0.815s, 1256.72/s  (0.825s, 1241.29/s)  LR: 9.781e-04  Data: 0.012 (0.023)
Train: 57 [ 200/1251 ( 16%)]  Loss: 3.828 (3.96)  Time: 0.780s, 1312.03/s  (0.821s, 1247.36/s)  LR: 9.781e-04  Data: 0.010 (0.020)
Train: 57 [ 250/1251 ( 20%)]  Loss: 3.827 (3.94)  Time: 0.799s, 1282.29/s  (0.818s, 1251.36/s)  LR: 9.781e-04  Data: 0.010 (0.018)
Train: 57 [ 300/1251 ( 24%)]  Loss: 3.984 (3.95)  Time: 0.834s, 1227.35/s  (0.816s, 1255.37/s)  LR: 9.781e-04  Data: 0.009 (0.017)
Train: 57 [ 350/1251 ( 28%)]  Loss: 4.122 (3.97)  Time: 0.800s, 1280.28/s  (0.815s, 1257.01/s)  LR: 9.781e-04  Data: 0.009 (0.016)
Train: 57 [ 400/1251 ( 32%)]  Loss: 3.693 (3.94)  Time: 0.830s, 1233.67/s  (0.815s, 1256.87/s)  LR: 9.781e-04  Data: 0.010 (0.015)
Train: 57 [ 450/1251 ( 36%)]  Loss: 4.039 (3.95)  Time: 0.824s, 1242.51/s  (0.814s, 1258.55/s)  LR: 9.781e-04  Data: 0.010 (0.015)
Train: 57 [ 500/1251 ( 40%)]  Loss: 4.114 (3.96)  Time: 0.844s, 1212.96/s  (0.813s, 1258.80/s)  LR: 9.781e-04  Data: 0.014 (0.014)
Train: 57 [ 550/1251 ( 44%)]  Loss: 4.215 (3.98)  Time: 0.812s, 1260.36/s  (0.814s, 1258.54/s)  LR: 9.781e-04  Data: 0.012 (0.014)
Train: 57 [ 600/1251 ( 48%)]  Loss: 3.797 (3.97)  Time: 0.807s, 1269.56/s  (0.813s, 1258.86/s)  LR: 9.781e-04  Data: 0.010 (0.014)
Train: 57 [ 650/1251 ( 52%)]  Loss: 3.942 (3.97)  Time: 0.802s, 1276.43/s  (0.813s, 1259.45/s)  LR: 9.781e-04  Data: 0.009 (0.014)
Train: 57 [ 700/1251 ( 56%)]  Loss: 3.774 (3.95)  Time: 0.790s, 1295.75/s  (0.813s, 1259.58/s)  LR: 9.781e-04  Data: 0.017 (0.013)
Train: 57 [ 750/1251 ( 60%)]  Loss: 3.663 (3.94)  Time: 0.812s, 1261.21/s  (0.813s, 1259.91/s)  LR: 9.781e-04  Data: 0.013 (0.013)
Train: 57 [ 800/1251 ( 64%)]  Loss: 4.147 (3.95)  Time: 0.807s, 1268.32/s  (0.812s, 1260.53/s)  LR: 9.781e-04  Data: 0.010 (0.013)
Train: 57 [ 850/1251 ( 68%)]  Loss: 3.736 (3.94)  Time: 0.776s, 1319.81/s  (0.812s, 1260.38/s)  LR: 9.781e-04  Data: 0.009 (0.013)
Train: 57 [ 900/1251 ( 72%)]  Loss: 4.098 (3.94)  Time: 0.902s, 1135.23/s  (0.813s, 1260.26/s)  LR: 9.781e-04  Data: 0.010 (0.013)
Train: 57 [ 950/1251 ( 76%)]  Loss: 4.080 (3.95)  Time: 0.847s, 1208.75/s  (0.812s, 1260.46/s)  LR: 9.781e-04  Data: 0.013 (0.013)
Train: 57 [1000/1251 ( 80%)]  Loss: 3.848 (3.95)  Time: 0.813s, 1259.20/s  (0.812s, 1260.41/s)  LR: 9.781e-04  Data: 0.009 (0.013)
Train: 57 [1050/1251 ( 84%)]  Loss: 3.678 (3.93)  Time: 0.821s, 1247.36/s  (0.812s, 1261.13/s)  LR: 9.781e-04  Data: 0.010 (0.012)
Train: 57 [1100/1251 ( 88%)]  Loss: 3.630 (3.92)  Time: 0.772s, 1327.15/s  (0.812s, 1261.34/s)  LR: 9.781e-04  Data: 0.011 (0.012)
Train: 57 [1150/1251 ( 92%)]  Loss: 3.898 (3.92)  Time: 0.779s, 1314.06/s  (0.812s, 1261.81/s)  LR: 9.781e-04  Data: 0.010 (0.012)
Train: 57 [1200/1251 ( 96%)]  Loss: 3.849 (3.92)  Time: 0.856s, 1196.04/s  (0.812s, 1261.66/s)  LR: 9.781e-04  Data: 0.010 (0.012)
Train: 57 [1250/1251 (100%)]  Loss: 3.331 (3.89)  Time: 0.806s, 1270.93/s  (0.812s, 1261.43/s)  LR: 9.781e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.744 (1.744)  Loss:  0.9458 (0.9458)  Acc@1: 84.8633 (84.8633)  Acc@5: 95.9961 (95.9961)
Test: [  48/48]  Time: 0.193 (0.611)  Loss:  0.9790 (1.5527)  Acc@1: 82.6651 (69.2120)  Acc@5: 94.5755 (89.4380)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-56.pth.tar', 69.56000015136719)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-55.pth.tar', 69.524)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-53.pth.tar', 69.49800014892578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-52.pth.tar', 69.28799993652343)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-49.pth.tar', 69.27400001464844)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-51.pth.tar', 69.25799994628906)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-57.pth.tar', 69.21200009765624)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-54.pth.tar', 69.15399991943359)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-47.pth.tar', 68.9379999194336)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-44.pth.tar', 68.90599997070312)

Train: 58 [   0/1251 (  0%)]  Loss: 3.770 (3.77)  Time: 2.360s,  433.92/s  (2.360s,  433.92/s)  LR: 9.773e-04  Data: 1.630 (1.630)
Train: 58 [  50/1251 (  4%)]  Loss: 3.549 (3.66)  Time: 0.782s, 1309.88/s  (0.849s, 1206.71/s)  LR: 9.773e-04  Data: 0.009 (0.047)
Train: 58 [ 100/1251 (  8%)]  Loss: 3.915 (3.74)  Time: 0.836s, 1225.10/s  (0.826s, 1240.41/s)  LR: 9.773e-04  Data: 0.012 (0.029)
Train: 58 [ 150/1251 ( 12%)]  Loss: 4.191 (3.86)  Time: 0.797s, 1285.20/s  (0.820s, 1248.97/s)  LR: 9.773e-04  Data: 0.010 (0.023)
Train: 58 [ 200/1251 ( 16%)]  Loss: 3.897 (3.86)  Time: 0.804s, 1273.15/s  (0.818s, 1252.09/s)  LR: 9.773e-04  Data: 0.010 (0.020)
Train: 58 [ 250/1251 ( 20%)]  Loss: 3.676 (3.83)  Time: 0.815s, 1256.59/s  (0.816s, 1254.77/s)  LR: 9.773e-04  Data: 0.011 (0.018)
Train: 58 [ 300/1251 ( 24%)]  Loss: 4.148 (3.88)  Time: 0.821s, 1247.31/s  (0.817s, 1253.16/s)  LR: 9.773e-04  Data: 0.010 (0.017)
Train: 58 [ 350/1251 ( 28%)]  Loss: 4.079 (3.90)  Time: 0.788s, 1299.36/s  (0.816s, 1254.86/s)  LR: 9.773e-04  Data: 0.009 (0.016)
Train: 58 [ 400/1251 ( 32%)]  Loss: 3.670 (3.88)  Time: 0.826s, 1240.41/s  (0.815s, 1255.81/s)  LR: 9.773e-04  Data: 0.010 (0.015)
Train: 58 [ 450/1251 ( 36%)]  Loss: 3.766 (3.87)  Time: 0.799s, 1281.04/s  (0.815s, 1257.10/s)  LR: 9.773e-04  Data: 0.010 (0.015)
Train: 58 [ 500/1251 ( 40%)]  Loss: 3.843 (3.86)  Time: 0.817s, 1252.82/s  (0.814s, 1258.34/s)  LR: 9.773e-04  Data: 0.011 (0.014)
Train: 58 [ 550/1251 ( 44%)]  Loss: 3.778 (3.86)  Time: 0.818s, 1251.19/s  (0.814s, 1257.95/s)  LR: 9.773e-04  Data: 0.012 (0.014)
Train: 58 [ 600/1251 ( 48%)]  Loss: 3.712 (3.85)  Time: 0.802s, 1277.51/s  (0.814s, 1258.03/s)  LR: 9.773e-04  Data: 0.011 (0.014)
Train: 58 [ 650/1251 ( 52%)]  Loss: 3.863 (3.85)  Time: 0.803s, 1275.49/s  (0.814s, 1257.67/s)  LR: 9.773e-04  Data: 0.012 (0.014)
Train: 58 [ 700/1251 ( 56%)]  Loss: 3.655 (3.83)  Time: 0.779s, 1313.72/s  (0.815s, 1257.19/s)  LR: 9.773e-04  Data: 0.010 (0.013)
Train: 58 [ 750/1251 ( 60%)]  Loss: 3.954 (3.84)  Time: 0.815s, 1256.77/s  (0.814s, 1258.20/s)  LR: 9.773e-04  Data: 0.009 (0.013)
Train: 58 [ 800/1251 ( 64%)]  Loss: 3.633 (3.83)  Time: 0.820s, 1249.54/s  (0.813s, 1259.06/s)  LR: 9.773e-04  Data: 0.010 (0.013)
Train: 58 [ 850/1251 ( 68%)]  Loss: 3.898 (3.83)  Time: 0.778s, 1316.48/s  (0.813s, 1258.90/s)  LR: 9.773e-04  Data: 0.009 (0.013)
Train: 58 [ 900/1251 ( 72%)]  Loss: 4.172 (3.85)  Time: 0.825s, 1240.85/s  (0.813s, 1259.05/s)  LR: 9.773e-04  Data: 0.009 (0.013)
Train: 58 [ 950/1251 ( 76%)]  Loss: 3.485 (3.83)  Time: 0.808s, 1268.03/s  (0.813s, 1259.29/s)  LR: 9.773e-04  Data: 0.010 (0.013)
Train: 58 [1000/1251 ( 80%)]  Loss: 4.129 (3.85)  Time: 0.813s, 1260.02/s  (0.813s, 1259.79/s)  LR: 9.773e-04  Data: 0.010 (0.012)
Train: 58 [1050/1251 ( 84%)]  Loss: 4.129 (3.86)  Time: 0.787s, 1301.86/s  (0.813s, 1259.75/s)  LR: 9.773e-04  Data: 0.010 (0.012)
Train: 58 [1100/1251 ( 88%)]  Loss: 3.933 (3.86)  Time: 0.816s, 1255.05/s  (0.813s, 1259.67/s)  LR: 9.773e-04  Data: 0.009 (0.012)
Train: 58 [1150/1251 ( 92%)]  Loss: 3.730 (3.86)  Time: 0.807s, 1268.84/s  (0.813s, 1259.62/s)  LR: 9.773e-04  Data: 0.010 (0.012)
Train: 58 [1200/1251 ( 96%)]  Loss: 3.626 (3.85)  Time: 0.794s, 1290.20/s  (0.812s, 1260.37/s)  LR: 9.773e-04  Data: 0.011 (0.012)
Train: 58 [1250/1251 (100%)]  Loss: 4.013 (3.85)  Time: 0.773s, 1324.17/s  (0.812s, 1260.34/s)  LR: 9.773e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.692 (1.692)  Loss:  0.9116 (0.9116)  Acc@1: 85.4492 (85.4492)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.194 (0.608)  Loss:  1.0420 (1.5076)  Acc@1: 83.2547 (69.8440)  Acc@5: 95.0472 (89.9480)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-58.pth.tar', 69.84399996582032)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-56.pth.tar', 69.56000015136719)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-55.pth.tar', 69.524)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-53.pth.tar', 69.49800014892578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-52.pth.tar', 69.28799993652343)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-49.pth.tar', 69.27400001464844)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-51.pth.tar', 69.25799994628906)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-57.pth.tar', 69.21200009765624)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-54.pth.tar', 69.15399991943359)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-47.pth.tar', 68.9379999194336)

Train: 59 [   0/1251 (  0%)]  Loss: 3.614 (3.61)  Time: 2.450s,  417.94/s  (2.450s,  417.94/s)  LR: 9.766e-04  Data: 1.694 (1.694)
Train: 59 [  50/1251 (  4%)]  Loss: 3.878 (3.75)  Time: 0.802s, 1277.42/s  (0.851s, 1202.90/s)  LR: 9.766e-04  Data: 0.014 (0.049)
Train: 59 [ 100/1251 (  8%)]  Loss: 3.930 (3.81)  Time: 0.777s, 1317.38/s  (0.828s, 1236.26/s)  LR: 9.766e-04  Data: 0.009 (0.030)
Train: 59 [ 150/1251 ( 12%)]  Loss: 3.920 (3.84)  Time: 0.814s, 1258.49/s  (0.822s, 1246.00/s)  LR: 9.766e-04  Data: 0.010 (0.023)
Train: 59 [ 200/1251 ( 16%)]  Loss: 3.795 (3.83)  Time: 0.812s, 1261.71/s  (0.821s, 1247.98/s)  LR: 9.766e-04  Data: 0.010 (0.020)
Train: 59 [ 250/1251 ( 20%)]  Loss: 3.902 (3.84)  Time: 0.777s, 1317.04/s  (0.818s, 1252.24/s)  LR: 9.766e-04  Data: 0.010 (0.018)
Train: 59 [ 300/1251 ( 24%)]  Loss: 3.869 (3.84)  Time: 0.793s, 1291.74/s  (0.816s, 1255.18/s)  LR: 9.766e-04  Data: 0.010 (0.017)
Train: 59 [ 350/1251 ( 28%)]  Loss: 3.908 (3.85)  Time: 0.772s, 1326.11/s  (0.816s, 1254.39/s)  LR: 9.766e-04  Data: 0.009 (0.016)
Train: 59 [ 400/1251 ( 32%)]  Loss: 3.691 (3.83)  Time: 0.774s, 1322.56/s  (0.816s, 1255.25/s)  LR: 9.766e-04  Data: 0.010 (0.015)
Train: 59 [ 450/1251 ( 36%)]  Loss: 3.916 (3.84)  Time: 0.801s, 1277.75/s  (0.815s, 1256.64/s)  LR: 9.766e-04  Data: 0.010 (0.015)
Train: 59 [ 500/1251 ( 40%)]  Loss: 3.904 (3.85)  Time: 0.805s, 1272.71/s  (0.815s, 1256.72/s)  LR: 9.766e-04  Data: 0.011 (0.015)
Train: 59 [ 550/1251 ( 44%)]  Loss: 3.939 (3.86)  Time: 0.778s, 1315.62/s  (0.815s, 1256.94/s)  LR: 9.766e-04  Data: 0.009 (0.014)
Train: 59 [ 600/1251 ( 48%)]  Loss: 3.695 (3.84)  Time: 0.791s, 1294.43/s  (0.814s, 1258.59/s)  LR: 9.766e-04  Data: 0.009 (0.014)
Train: 59 [ 650/1251 ( 52%)]  Loss: 3.718 (3.83)  Time: 0.807s, 1268.20/s  (0.814s, 1258.62/s)  LR: 9.766e-04  Data: 0.010 (0.014)
Train: 59 [ 700/1251 ( 56%)]  Loss: 4.036 (3.85)  Time: 0.857s, 1195.19/s  (0.813s, 1259.25/s)  LR: 9.766e-04  Data: 0.012 (0.013)
Train: 59 [ 750/1251 ( 60%)]  Loss: 3.912 (3.85)  Time: 0.795s, 1287.50/s  (0.813s, 1259.06/s)  LR: 9.766e-04  Data: 0.013 (0.013)
Train: 59 [ 800/1251 ( 64%)]  Loss: 4.052 (3.86)  Time: 0.788s, 1299.88/s  (0.812s, 1260.64/s)  LR: 9.766e-04  Data: 0.009 (0.013)
Train: 59 [ 850/1251 ( 68%)]  Loss: 3.879 (3.86)  Time: 0.790s, 1295.45/s  (0.812s, 1261.00/s)  LR: 9.766e-04  Data: 0.009 (0.013)
Train: 59 [ 900/1251 ( 72%)]  Loss: 3.885 (3.87)  Time: 0.785s, 1303.64/s  (0.812s, 1261.39/s)  LR: 9.766e-04  Data: 0.010 (0.013)
Train: 59 [ 950/1251 ( 76%)]  Loss: 4.031 (3.87)  Time: 0.800s, 1279.70/s  (0.811s, 1261.86/s)  LR: 9.766e-04  Data: 0.010 (0.013)
Train: 59 [1000/1251 ( 80%)]  Loss: 4.003 (3.88)  Time: 0.779s, 1315.28/s  (0.812s, 1261.73/s)  LR: 9.766e-04  Data: 0.012 (0.013)
Train: 59 [1050/1251 ( 84%)]  Loss: 4.108 (3.89)  Time: 0.784s, 1305.84/s  (0.811s, 1262.22/s)  LR: 9.766e-04  Data: 0.010 (0.012)
Train: 59 [1100/1251 ( 88%)]  Loss: 4.084 (3.90)  Time: 0.781s, 1311.63/s  (0.811s, 1262.99/s)  LR: 9.766e-04  Data: 0.010 (0.012)
Train: 59 [1150/1251 ( 92%)]  Loss: 3.335 (3.88)  Time: 0.815s, 1256.68/s  (0.812s, 1261.78/s)  LR: 9.766e-04  Data: 0.011 (0.012)
Train: 59 [1200/1251 ( 96%)]  Loss: 4.036 (3.88)  Time: 0.778s, 1316.26/s  (0.812s, 1261.61/s)  LR: 9.766e-04  Data: 0.009 (0.012)
Train: 59 [1250/1251 (100%)]  Loss: 3.867 (3.88)  Time: 0.810s, 1264.23/s  (0.811s, 1262.84/s)  LR: 9.766e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.742 (1.742)  Loss:  1.0381 (1.0381)  Acc@1: 86.7188 (86.7188)  Acc@5: 95.9961 (95.9961)
Test: [  48/48]  Time: 0.194 (0.591)  Loss:  1.1973 (1.6269)  Acc@1: 81.1321 (69.7160)  Acc@5: 93.0425 (89.4480)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-58.pth.tar', 69.84399996582032)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-59.pth.tar', 69.716000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-56.pth.tar', 69.56000015136719)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-55.pth.tar', 69.524)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-53.pth.tar', 69.49800014892578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-52.pth.tar', 69.28799993652343)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-49.pth.tar', 69.27400001464844)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-51.pth.tar', 69.25799994628906)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-57.pth.tar', 69.21200009765624)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-54.pth.tar', 69.15399991943359)

Train: 60 [   0/1251 (  0%)]  Loss: 3.807 (3.81)  Time: 2.266s,  451.94/s  (2.266s,  451.94/s)  LR: 9.758e-04  Data: 1.538 (1.538)
Train: 60 [  50/1251 (  4%)]  Loss: 3.754 (3.78)  Time: 0.844s, 1212.87/s  (0.839s, 1220.79/s)  LR: 9.758e-04  Data: 0.013 (0.046)
Train: 60 [ 100/1251 (  8%)]  Loss: 4.219 (3.93)  Time: 0.808s, 1267.12/s  (0.821s, 1246.55/s)  LR: 9.758e-04  Data: 0.009 (0.029)
Train: 60 [ 150/1251 ( 12%)]  Loss: 3.544 (3.83)  Time: 0.820s, 1249.06/s  (0.817s, 1253.37/s)  LR: 9.758e-04  Data: 0.012 (0.023)
Train: 60 [ 200/1251 ( 16%)]  Loss: 4.199 (3.90)  Time: 0.780s, 1313.59/s  (0.816s, 1254.56/s)  LR: 9.758e-04  Data: 0.010 (0.020)
Train: 60 [ 250/1251 ( 20%)]  Loss: 3.676 (3.87)  Time: 0.826s, 1240.30/s  (0.814s, 1257.80/s)  LR: 9.758e-04  Data: 0.010 (0.018)
Train: 60 [ 300/1251 ( 24%)]  Loss: 3.582 (3.83)  Time: 0.832s, 1231.36/s  (0.814s, 1257.98/s)  LR: 9.758e-04  Data: 0.010 (0.017)
Train: 60 [ 350/1251 ( 28%)]  Loss: 4.334 (3.89)  Time: 0.777s, 1317.99/s  (0.814s, 1258.64/s)  LR: 9.758e-04  Data: 0.009 (0.016)
Train: 60 [ 400/1251 ( 32%)]  Loss: 3.818 (3.88)  Time: 0.773s, 1324.87/s  (0.813s, 1259.39/s)  LR: 9.758e-04  Data: 0.011 (0.015)
Train: 60 [ 450/1251 ( 36%)]  Loss: 3.952 (3.89)  Time: 0.778s, 1316.99/s  (0.813s, 1260.25/s)  LR: 9.758e-04  Data: 0.010 (0.015)
Train: 60 [ 500/1251 ( 40%)]  Loss: 4.071 (3.91)  Time: 0.848s, 1207.32/s  (0.813s, 1260.10/s)  LR: 9.758e-04  Data: 0.010 (0.014)
Train: 60 [ 550/1251 ( 44%)]  Loss: 3.677 (3.89)  Time: 0.778s, 1316.07/s  (0.813s, 1260.00/s)  LR: 9.758e-04  Data: 0.010 (0.014)
Train: 60 [ 600/1251 ( 48%)]  Loss: 3.776 (3.88)  Time: 0.788s, 1299.80/s  (0.812s, 1260.93/s)  LR: 9.758e-04  Data: 0.010 (0.014)
Train: 60 [ 650/1251 ( 52%)]  Loss: 3.916 (3.88)  Time: 0.818s, 1251.48/s  (0.812s, 1261.63/s)  LR: 9.758e-04  Data: 0.010 (0.013)
Train: 60 [ 700/1251 ( 56%)]  Loss: 4.095 (3.89)  Time: 0.840s, 1219.21/s  (0.812s, 1261.50/s)  LR: 9.758e-04  Data: 0.016 (0.013)
Train: 60 [ 750/1251 ( 60%)]  Loss: 4.054 (3.90)  Time: 0.801s, 1277.90/s  (0.812s, 1261.73/s)  LR: 9.758e-04  Data: 0.010 (0.013)
Train: 60 [ 800/1251 ( 64%)]  Loss: 3.826 (3.90)  Time: 0.828s, 1236.28/s  (0.812s, 1261.51/s)  LR: 9.758e-04  Data: 0.010 (0.013)
Train: 60 [ 850/1251 ( 68%)]  Loss: 3.951 (3.90)  Time: 0.825s, 1241.76/s  (0.812s, 1261.06/s)  LR: 9.758e-04  Data: 0.010 (0.013)
Train: 60 [ 900/1251 ( 72%)]  Loss: 3.835 (3.90)  Time: 0.820s, 1248.62/s  (0.811s, 1262.02/s)  LR: 9.758e-04  Data: 0.014 (0.013)
Train: 60 [ 950/1251 ( 76%)]  Loss: 3.575 (3.88)  Time: 0.805s, 1272.27/s  (0.812s, 1261.77/s)  LR: 9.758e-04  Data: 0.009 (0.013)
Train: 60 [1000/1251 ( 80%)]  Loss: 3.548 (3.87)  Time: 0.807s, 1269.35/s  (0.811s, 1261.94/s)  LR: 9.758e-04  Data: 0.017 (0.012)
Train: 60 [1050/1251 ( 84%)]  Loss: 3.870 (3.87)  Time: 0.778s, 1316.76/s  (0.811s, 1262.54/s)  LR: 9.758e-04  Data: 0.009 (0.012)
Train: 60 [1100/1251 ( 88%)]  Loss: 3.717 (3.86)  Time: 0.777s, 1317.73/s  (0.811s, 1262.54/s)  LR: 9.758e-04  Data: 0.010 (0.012)
Train: 60 [1150/1251 ( 92%)]  Loss: 3.929 (3.86)  Time: 0.774s, 1323.15/s  (0.811s, 1263.00/s)  LR: 9.758e-04  Data: 0.010 (0.012)
Train: 60 [1200/1251 ( 96%)]  Loss: 3.729 (3.86)  Time: 0.829s, 1235.41/s  (0.811s, 1263.40/s)  LR: 9.758e-04  Data: 0.010 (0.012)
Train: 60 [1250/1251 (100%)]  Loss: 3.530 (3.85)  Time: 0.805s, 1272.63/s  (0.810s, 1263.45/s)  LR: 9.758e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.696 (1.696)  Loss:  0.8462 (0.8462)  Acc@1: 87.5000 (87.5000)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.194 (0.597)  Loss:  0.9766 (1.6156)  Acc@1: 82.6651 (68.9540)  Acc@5: 95.1651 (89.1160)
Train: 61 [   0/1251 (  0%)]  Loss: 4.264 (4.26)  Time: 2.316s,  442.13/s  (2.316s,  442.13/s)  LR: 9.750e-04  Data: 1.585 (1.585)
Train: 61 [  50/1251 (  4%)]  Loss: 3.898 (4.08)  Time: 0.779s, 1314.46/s  (0.841s, 1217.47/s)  LR: 9.750e-04  Data: 0.010 (0.047)
Train: 61 [ 100/1251 (  8%)]  Loss: 3.938 (4.03)  Time: 0.828s, 1236.81/s  (0.824s, 1242.21/s)  LR: 9.750e-04  Data: 0.015 (0.029)
Train: 61 [ 150/1251 ( 12%)]  Loss: 4.083 (4.05)  Time: 0.788s, 1298.95/s  (0.822s, 1246.22/s)  LR: 9.750e-04  Data: 0.011 (0.023)
Train: 61 [ 200/1251 ( 16%)]  Loss: 3.954 (4.03)  Time: 0.857s, 1195.10/s  (0.821s, 1247.02/s)  LR: 9.750e-04  Data: 0.010 (0.020)
Train: 61 [ 250/1251 ( 20%)]  Loss: 3.680 (3.97)  Time: 0.814s, 1257.42/s  (0.819s, 1250.25/s)  LR: 9.750e-04  Data: 0.010 (0.018)
Train: 61 [ 300/1251 ( 24%)]  Loss: 3.696 (3.93)  Time: 0.811s, 1262.85/s  (0.817s, 1254.12/s)  LR: 9.750e-04  Data: 0.009 (0.017)
Train: 61 [ 350/1251 ( 28%)]  Loss: 3.823 (3.92)  Time: 0.826s, 1239.78/s  (0.814s, 1257.50/s)  LR: 9.750e-04  Data: 0.009 (0.016)
Train: 61 [ 400/1251 ( 32%)]  Loss: 3.397 (3.86)  Time: 0.811s, 1262.89/s  (0.814s, 1258.17/s)  LR: 9.750e-04  Data: 0.009 (0.015)
Train: 61 [ 450/1251 ( 36%)]  Loss: 3.644 (3.84)  Time: 0.823s, 1244.40/s  (0.814s, 1258.43/s)  LR: 9.750e-04  Data: 0.012 (0.015)
Train: 61 [ 500/1251 ( 40%)]  Loss: 3.700 (3.83)  Time: 0.819s, 1249.72/s  (0.814s, 1258.48/s)  LR: 9.750e-04  Data: 0.009 (0.014)
Train: 61 [ 550/1251 ( 44%)]  Loss: 4.038 (3.84)  Time: 0.778s, 1316.72/s  (0.813s, 1260.07/s)  LR: 9.750e-04  Data: 0.010 (0.014)
Train: 61 [ 600/1251 ( 48%)]  Loss: 4.263 (3.88)  Time: 0.778s, 1315.42/s  (0.812s, 1260.53/s)  LR: 9.750e-04  Data: 0.010 (0.014)
Train: 61 [ 650/1251 ( 52%)]  Loss: 4.009 (3.88)  Time: 0.796s, 1285.68/s  (0.812s, 1260.46/s)  LR: 9.750e-04  Data: 0.016 (0.013)
Train: 61 [ 700/1251 ( 56%)]  Loss: 3.441 (3.86)  Time: 0.803s, 1274.93/s  (0.812s, 1260.85/s)  LR: 9.750e-04  Data: 0.016 (0.013)
Train: 61 [ 750/1251 ( 60%)]  Loss: 4.130 (3.87)  Time: 0.799s, 1282.02/s  (0.812s, 1261.59/s)  LR: 9.750e-04  Data: 0.014 (0.013)
Train: 61 [ 800/1251 ( 64%)]  Loss: 3.491 (3.85)  Time: 0.781s, 1310.34/s  (0.812s, 1261.79/s)  LR: 9.750e-04  Data: 0.009 (0.013)
Train: 61 [ 850/1251 ( 68%)]  Loss: 3.659 (3.84)  Time: 0.823s, 1243.62/s  (0.812s, 1261.44/s)  LR: 9.750e-04  Data: 0.013 (0.013)
Train: 61 [ 900/1251 ( 72%)]  Loss: 4.099 (3.85)  Time: 0.830s, 1233.33/s  (0.812s, 1261.29/s)  LR: 9.750e-04  Data: 0.009 (0.013)
Train: 61 [ 950/1251 ( 76%)]  Loss: 3.933 (3.86)  Time: 0.840s, 1219.20/s  (0.812s, 1261.63/s)  LR: 9.750e-04  Data: 0.010 (0.013)
Train: 61 [1000/1251 ( 80%)]  Loss: 4.099 (3.87)  Time: 0.797s, 1285.25/s  (0.811s, 1262.11/s)  LR: 9.750e-04  Data: 0.010 (0.012)
Train: 61 [1050/1251 ( 84%)]  Loss: 4.061 (3.88)  Time: 0.777s, 1317.17/s  (0.811s, 1262.67/s)  LR: 9.750e-04  Data: 0.010 (0.012)
Train: 61 [1100/1251 ( 88%)]  Loss: 3.837 (3.88)  Time: 0.780s, 1313.27/s  (0.811s, 1262.95/s)  LR: 9.750e-04  Data: 0.010 (0.012)
Train: 61 [1150/1251 ( 92%)]  Loss: 3.576 (3.86)  Time: 0.789s, 1297.52/s  (0.810s, 1263.77/s)  LR: 9.750e-04  Data: 0.010 (0.012)
Train: 61 [1200/1251 ( 96%)]  Loss: 4.249 (3.88)  Time: 0.846s, 1210.11/s  (0.810s, 1263.72/s)  LR: 9.750e-04  Data: 0.010 (0.012)
Train: 61 [1250/1251 (100%)]  Loss: 3.868 (3.88)  Time: 0.767s, 1334.69/s  (0.810s, 1263.71/s)  LR: 9.750e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.674 (1.674)  Loss:  0.8813 (0.8813)  Acc@1: 86.7188 (86.7188)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.194 (0.610)  Loss:  0.8433 (1.4785)  Acc@1: 82.5472 (69.5220)  Acc@5: 95.7547 (89.7440)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-58.pth.tar', 69.84399996582032)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-59.pth.tar', 69.716000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-56.pth.tar', 69.56000015136719)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-55.pth.tar', 69.524)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-61.pth.tar', 69.52200004638672)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-53.pth.tar', 69.49800014892578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-52.pth.tar', 69.28799993652343)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-49.pth.tar', 69.27400001464844)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-51.pth.tar', 69.25799994628906)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-57.pth.tar', 69.21200009765624)

Train: 62 [   0/1251 (  0%)]  Loss: 4.192 (4.19)  Time: 2.427s,  421.89/s  (2.427s,  421.89/s)  LR: 9.741e-04  Data: 1.690 (1.690)
Train: 62 [  50/1251 (  4%)]  Loss: 4.014 (4.10)  Time: 0.775s, 1321.76/s  (0.845s, 1211.44/s)  LR: 9.741e-04  Data: 0.011 (0.047)
Train: 62 [ 100/1251 (  8%)]  Loss: 3.812 (4.01)  Time: 0.904s, 1133.07/s  (0.826s, 1239.17/s)  LR: 9.741e-04  Data: 0.009 (0.029)
Train: 62 [ 150/1251 ( 12%)]  Loss: 3.804 (3.96)  Time: 0.817s, 1253.38/s  (0.820s, 1249.49/s)  LR: 9.741e-04  Data: 0.010 (0.023)
Train: 62 [ 200/1251 ( 16%)]  Loss: 3.449 (3.85)  Time: 0.796s, 1286.17/s  (0.818s, 1252.16/s)  LR: 9.741e-04  Data: 0.016 (0.020)
Train: 62 [ 250/1251 ( 20%)]  Loss: 3.900 (3.86)  Time: 0.875s, 1170.92/s  (0.815s, 1255.83/s)  LR: 9.741e-04  Data: 0.009 (0.018)
Train: 62 [ 300/1251 ( 24%)]  Loss: 3.794 (3.85)  Time: 0.844s, 1212.83/s  (0.814s, 1257.91/s)  LR: 9.741e-04  Data: 0.011 (0.017)
Train: 62 [ 350/1251 ( 28%)]  Loss: 3.968 (3.87)  Time: 0.826s, 1239.60/s  (0.814s, 1258.39/s)  LR: 9.741e-04  Data: 0.010 (0.016)
Train: 62 [ 400/1251 ( 32%)]  Loss: 4.109 (3.89)  Time: 0.783s, 1308.41/s  (0.813s, 1259.75/s)  LR: 9.741e-04  Data: 0.010 (0.015)
Train: 62 [ 450/1251 ( 36%)]  Loss: 3.891 (3.89)  Time: 0.822s, 1246.19/s  (0.812s, 1260.37/s)  LR: 9.741e-04  Data: 0.010 (0.015)
Train: 62 [ 500/1251 ( 40%)]  Loss: 3.933 (3.90)  Time: 0.830s, 1233.67/s  (0.812s, 1261.77/s)  LR: 9.741e-04  Data: 0.009 (0.014)
Train: 62 [ 550/1251 ( 44%)]  Loss: 3.866 (3.89)  Time: 0.780s, 1312.10/s  (0.811s, 1262.50/s)  LR: 9.741e-04  Data: 0.010 (0.014)
Train: 62 [ 600/1251 ( 48%)]  Loss: 3.939 (3.90)  Time: 0.800s, 1280.55/s  (0.811s, 1262.75/s)  LR: 9.741e-04  Data: 0.010 (0.014)
Train: 62 [ 650/1251 ( 52%)]  Loss: 3.833 (3.89)  Time: 0.799s, 1282.40/s  (0.811s, 1263.10/s)  LR: 9.741e-04  Data: 0.010 (0.013)
Train: 62 [ 700/1251 ( 56%)]  Loss: 3.696 (3.88)  Time: 0.818s, 1251.19/s  (0.810s, 1263.48/s)  LR: 9.741e-04  Data: 0.010 (0.013)
Train: 62 [ 750/1251 ( 60%)]  Loss: 3.945 (3.88)  Time: 0.807s, 1268.60/s  (0.810s, 1263.92/s)  LR: 9.741e-04  Data: 0.010 (0.013)
Train: 62 [ 800/1251 ( 64%)]  Loss: 3.799 (3.88)  Time: 0.789s, 1297.97/s  (0.810s, 1264.21/s)  LR: 9.741e-04  Data: 0.010 (0.013)
Train: 62 [ 850/1251 ( 68%)]  Loss: 4.211 (3.90)  Time: 0.789s, 1297.59/s  (0.810s, 1264.89/s)  LR: 9.741e-04  Data: 0.010 (0.013)
Train: 62 [ 900/1251 ( 72%)]  Loss: 3.750 (3.89)  Time: 0.845s, 1212.33/s  (0.810s, 1264.50/s)  LR: 9.741e-04  Data: 0.010 (0.013)
Train: 62 [ 950/1251 ( 76%)]  Loss: 4.017 (3.90)  Time: 0.783s, 1308.59/s  (0.810s, 1264.59/s)  LR: 9.741e-04  Data: 0.012 (0.013)
Train: 62 [1000/1251 ( 80%)]  Loss: 3.597 (3.88)  Time: 0.776s, 1320.22/s  (0.810s, 1264.79/s)  LR: 9.741e-04  Data: 0.010 (0.012)
Train: 62 [1050/1251 ( 84%)]  Loss: 4.183 (3.90)  Time: 0.819s, 1250.12/s  (0.810s, 1264.54/s)  LR: 9.741e-04  Data: 0.009 (0.012)
Train: 62 [1100/1251 ( 88%)]  Loss: 4.020 (3.90)  Time: 0.821s, 1246.77/s  (0.810s, 1264.87/s)  LR: 9.741e-04  Data: 0.010 (0.012)
Train: 62 [1150/1251 ( 92%)]  Loss: 3.474 (3.88)  Time: 0.824s, 1242.06/s  (0.810s, 1264.90/s)  LR: 9.741e-04  Data: 0.009 (0.012)
Train: 62 [1200/1251 ( 96%)]  Loss: 3.516 (3.87)  Time: 0.782s, 1309.15/s  (0.809s, 1265.01/s)  LR: 9.741e-04  Data: 0.011 (0.012)
Train: 62 [1250/1251 (100%)]  Loss: 4.115 (3.88)  Time: 0.809s, 1266.26/s  (0.809s, 1265.12/s)  LR: 9.741e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.652 (1.652)  Loss:  0.9790 (0.9790)  Acc@1: 84.4727 (84.4727)  Acc@5: 95.6055 (95.6055)
Test: [  48/48]  Time: 0.194 (0.602)  Loss:  1.0996 (1.5870)  Acc@1: 83.6085 (69.2040)  Acc@5: 94.9292 (89.6760)
Train: 63 [   0/1251 (  0%)]  Loss: 3.611 (3.61)  Time: 2.262s,  452.75/s  (2.262s,  452.75/s)  LR: 9.733e-04  Data: 1.536 (1.536)
Train: 63 [  50/1251 (  4%)]  Loss: 4.084 (3.85)  Time: 0.784s, 1305.81/s  (0.844s, 1213.17/s)  LR: 9.733e-04  Data: 0.010 (0.047)
Train: 63 [ 100/1251 (  8%)]  Loss: 4.222 (3.97)  Time: 0.801s, 1278.45/s  (0.827s, 1237.86/s)  LR: 9.733e-04  Data: 0.010 (0.029)
Train: 63 [ 150/1251 ( 12%)]  Loss: 3.976 (3.97)  Time: 0.809s, 1265.77/s  (0.823s, 1244.21/s)  LR: 9.733e-04  Data: 0.013 (0.023)
Train: 63 [ 200/1251 ( 16%)]  Loss: 3.638 (3.91)  Time: 0.784s, 1306.35/s  (0.821s, 1247.81/s)  LR: 9.733e-04  Data: 0.010 (0.020)
Train: 63 [ 250/1251 ( 20%)]  Loss: 3.733 (3.88)  Time: 0.840s, 1219.62/s  (0.819s, 1250.16/s)  LR: 9.733e-04  Data: 0.009 (0.018)
Train: 63 [ 300/1251 ( 24%)]  Loss: 3.771 (3.86)  Time: 0.800s, 1279.43/s  (0.817s, 1254.05/s)  LR: 9.733e-04  Data: 0.013 (0.017)
Train: 63 [ 350/1251 ( 28%)]  Loss: 3.610 (3.83)  Time: 0.825s, 1241.74/s  (0.815s, 1256.74/s)  LR: 9.733e-04  Data: 0.009 (0.016)
Train: 63 [ 400/1251 ( 32%)]  Loss: 3.984 (3.85)  Time: 0.835s, 1226.69/s  (0.815s, 1256.18/s)  LR: 9.733e-04  Data: 0.010 (0.015)
Train: 63 [ 450/1251 ( 36%)]  Loss: 3.918 (3.85)  Time: 0.781s, 1310.88/s  (0.814s, 1257.86/s)  LR: 9.733e-04  Data: 0.009 (0.015)
Train: 63 [ 500/1251 ( 40%)]  Loss: 4.007 (3.87)  Time: 0.825s, 1241.20/s  (0.815s, 1256.98/s)  LR: 9.733e-04  Data: 0.010 (0.014)
Train: 63 [ 550/1251 ( 44%)]  Loss: 3.965 (3.88)  Time: 0.800s, 1280.14/s  (0.815s, 1256.78/s)  LR: 9.733e-04  Data: 0.009 (0.014)
Train: 63 [ 600/1251 ( 48%)]  Loss: 3.501 (3.85)  Time: 0.794s, 1290.43/s  (0.814s, 1258.08/s)  LR: 9.733e-04  Data: 0.009 (0.014)
Train: 63 [ 650/1251 ( 52%)]  Loss: 3.789 (3.84)  Time: 0.779s, 1314.91/s  (0.813s, 1259.14/s)  LR: 9.733e-04  Data: 0.009 (0.013)
Train: 63 [ 700/1251 ( 56%)]  Loss: 3.950 (3.85)  Time: 0.779s, 1313.96/s  (0.813s, 1259.66/s)  LR: 9.733e-04  Data: 0.009 (0.013)
Train: 63 [ 750/1251 ( 60%)]  Loss: 3.924 (3.86)  Time: 0.817s, 1253.21/s  (0.813s, 1259.37/s)  LR: 9.733e-04  Data: 0.009 (0.013)
Train: 63 [ 800/1251 ( 64%)]  Loss: 3.741 (3.85)  Time: 0.813s, 1259.11/s  (0.813s, 1259.91/s)  LR: 9.733e-04  Data: 0.009 (0.013)
Train: 63 [ 850/1251 ( 68%)]  Loss: 4.028 (3.86)  Time: 0.800s, 1280.11/s  (0.813s, 1259.70/s)  LR: 9.733e-04  Data: 0.009 (0.013)
Train: 63 [ 900/1251 ( 72%)]  Loss: 3.784 (3.85)  Time: 0.794s, 1290.32/s  (0.812s, 1260.39/s)  LR: 9.733e-04  Data: 0.009 (0.013)
Train: 63 [ 950/1251 ( 76%)]  Loss: 3.929 (3.86)  Time: 0.787s, 1301.44/s  (0.812s, 1260.99/s)  LR: 9.733e-04  Data: 0.008 (0.013)
Train: 63 [1000/1251 ( 80%)]  Loss: 3.839 (3.86)  Time: 0.800s, 1280.39/s  (0.812s, 1261.68/s)  LR: 9.733e-04  Data: 0.010 (0.012)
Train: 63 [1050/1251 ( 84%)]  Loss: 4.122 (3.87)  Time: 0.776s, 1318.87/s  (0.812s, 1261.68/s)  LR: 9.733e-04  Data: 0.010 (0.012)
Train: 63 [1100/1251 ( 88%)]  Loss: 3.958 (3.87)  Time: 0.808s, 1266.69/s  (0.811s, 1261.94/s)  LR: 9.733e-04  Data: 0.015 (0.012)
Train: 63 [1150/1251 ( 92%)]  Loss: 3.968 (3.88)  Time: 0.786s, 1303.59/s  (0.812s, 1261.59/s)  LR: 9.733e-04  Data: 0.014 (0.012)
Train: 63 [1200/1251 ( 96%)]  Loss: 4.068 (3.88)  Time: 0.791s, 1294.09/s  (0.812s, 1261.51/s)  LR: 9.733e-04  Data: 0.015 (0.012)
Train: 63 [1250/1251 (100%)]  Loss: 3.980 (3.89)  Time: 0.821s, 1247.57/s  (0.812s, 1261.86/s)  LR: 9.733e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.664 (1.664)  Loss:  1.0537 (1.0537)  Acc@1: 86.4258 (86.4258)  Acc@5: 96.0938 (96.0938)
Test: [  48/48]  Time: 0.193 (0.616)  Loss:  0.9922 (1.6166)  Acc@1: 83.4906 (69.9620)  Acc@5: 96.3443 (89.9620)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-63.pth.tar', 69.96200006835937)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-58.pth.tar', 69.84399996582032)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-59.pth.tar', 69.716000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-56.pth.tar', 69.56000015136719)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-55.pth.tar', 69.524)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-61.pth.tar', 69.52200004638672)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-53.pth.tar', 69.49800014892578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-52.pth.tar', 69.28799993652343)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-49.pth.tar', 69.27400001464844)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-51.pth.tar', 69.25799994628906)

Train: 64 [   0/1251 (  0%)]  Loss: 3.843 (3.84)  Time: 2.313s,  442.68/s  (2.313s,  442.68/s)  LR: 9.725e-04  Data: 1.587 (1.587)
Train: 64 [  50/1251 (  4%)]  Loss: 3.681 (3.76)  Time: 0.828s, 1236.21/s  (0.838s, 1222.55/s)  LR: 9.725e-04  Data: 0.021 (0.047)
Train: 64 [ 100/1251 (  8%)]  Loss: 3.815 (3.78)  Time: 0.793s, 1291.18/s  (0.821s, 1246.62/s)  LR: 9.725e-04  Data: 0.009 (0.029)
Train: 64 [ 150/1251 ( 12%)]  Loss: 3.927 (3.82)  Time: 0.833s, 1229.31/s  (0.817s, 1253.59/s)  LR: 9.725e-04  Data: 0.009 (0.023)
Train: 64 [ 200/1251 ( 16%)]  Loss: 3.674 (3.79)  Time: 0.778s, 1316.34/s  (0.816s, 1254.61/s)  LR: 9.725e-04  Data: 0.011 (0.020)
Train: 64 [ 250/1251 ( 20%)]  Loss: 4.268 (3.87)  Time: 0.854s, 1199.72/s  (0.815s, 1256.48/s)  LR: 9.725e-04  Data: 0.017 (0.018)
Train: 64 [ 300/1251 ( 24%)]  Loss: 3.633 (3.83)  Time: 0.884s, 1158.57/s  (0.814s, 1258.55/s)  LR: 9.725e-04  Data: 0.009 (0.017)
Train: 64 [ 350/1251 ( 28%)]  Loss: 3.593 (3.80)  Time: 0.793s, 1291.40/s  (0.814s, 1258.23/s)  LR: 9.725e-04  Data: 0.010 (0.016)
Train: 64 [ 400/1251 ( 32%)]  Loss: 3.781 (3.80)  Time: 0.820s, 1248.55/s  (0.814s, 1258.12/s)  LR: 9.725e-04  Data: 0.010 (0.015)
Train: 64 [ 450/1251 ( 36%)]  Loss: 3.611 (3.78)  Time: 0.807s, 1269.15/s  (0.813s, 1259.34/s)  LR: 9.725e-04  Data: 0.010 (0.015)
Train: 64 [ 500/1251 ( 40%)]  Loss: 3.786 (3.78)  Time: 0.810s, 1264.59/s  (0.812s, 1260.48/s)  LR: 9.725e-04  Data: 0.010 (0.014)
Train: 64 [ 550/1251 ( 44%)]  Loss: 3.981 (3.80)  Time: 0.778s, 1316.01/s  (0.812s, 1260.45/s)  LR: 9.725e-04  Data: 0.011 (0.014)
Train: 64 [ 600/1251 ( 48%)]  Loss: 3.784 (3.80)  Time: 0.823s, 1244.28/s  (0.813s, 1259.88/s)  LR: 9.725e-04  Data: 0.010 (0.014)
Train: 64 [ 650/1251 ( 52%)]  Loss: 4.120 (3.82)  Time: 0.787s, 1300.57/s  (0.812s, 1260.55/s)  LR: 9.725e-04  Data: 0.009 (0.013)
Train: 64 [ 700/1251 ( 56%)]  Loss: 3.962 (3.83)  Time: 0.805s, 1272.20/s  (0.812s, 1260.44/s)  LR: 9.725e-04  Data: 0.010 (0.013)
Train: 64 [ 750/1251 ( 60%)]  Loss: 4.142 (3.85)  Time: 0.810s, 1264.57/s  (0.812s, 1260.65/s)  LR: 9.725e-04  Data: 0.010 (0.013)
Train: 64 [ 800/1251 ( 64%)]  Loss: 3.983 (3.86)  Time: 0.822s, 1246.14/s  (0.812s, 1260.77/s)  LR: 9.725e-04  Data: 0.011 (0.013)
Train: 64 [ 850/1251 ( 68%)]  Loss: 4.152 (3.87)  Time: 0.812s, 1261.26/s  (0.813s, 1258.95/s)  LR: 9.725e-04  Data: 0.013 (0.013)
Train: 64 [ 900/1251 ( 72%)]  Loss: 3.784 (3.87)  Time: 0.779s, 1314.98/s  (0.813s, 1259.66/s)  LR: 9.725e-04  Data: 0.010 (0.013)
Train: 64 [ 950/1251 ( 76%)]  Loss: 3.471 (3.85)  Time: 0.807s, 1269.03/s  (0.812s, 1261.15/s)  LR: 9.725e-04  Data: 0.010 (0.013)
Train: 64 [1000/1251 ( 80%)]  Loss: 3.787 (3.85)  Time: 0.811s, 1263.34/s  (0.811s, 1261.94/s)  LR: 9.725e-04  Data: 0.009 (0.013)
Train: 64 [1050/1251 ( 84%)]  Loss: 3.683 (3.84)  Time: 0.823s, 1243.82/s  (0.811s, 1262.57/s)  LR: 9.725e-04  Data: 0.009 (0.012)
Train: 64 [1100/1251 ( 88%)]  Loss: 3.758 (3.84)  Time: 0.785s, 1304.84/s  (0.811s, 1263.09/s)  LR: 9.725e-04  Data: 0.010 (0.012)
Train: 64 [1150/1251 ( 92%)]  Loss: 4.032 (3.84)  Time: 0.790s, 1296.08/s  (0.811s, 1262.80/s)  LR: 9.725e-04  Data: 0.009 (0.012)
Train: 64 [1200/1251 ( 96%)]  Loss: 3.541 (3.83)  Time: 0.844s, 1213.22/s  (0.811s, 1263.02/s)  LR: 9.725e-04  Data: 0.010 (0.012)
Train: 64 [1250/1251 (100%)]  Loss: 4.102 (3.84)  Time: 0.824s, 1242.51/s  (0.811s, 1263.42/s)  LR: 9.725e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.719 (1.719)  Loss:  0.9951 (0.9951)  Acc@1: 86.8164 (86.8164)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.194 (0.595)  Loss:  1.0596 (1.6290)  Acc@1: 81.4859 (68.8860)  Acc@5: 95.2830 (89.4820)
Train: 65 [   0/1251 (  0%)]  Loss: 4.105 (4.10)  Time: 2.223s,  460.59/s  (2.223s,  460.59/s)  LR: 9.716e-04  Data: 1.488 (1.488)
Train: 65 [  50/1251 (  4%)]  Loss: 3.785 (3.94)  Time: 0.851s, 1203.26/s  (0.840s, 1219.11/s)  LR: 9.716e-04  Data: 0.009 (0.045)
Train: 65 [ 100/1251 (  8%)]  Loss: 3.715 (3.87)  Time: 0.871s, 1175.30/s  (0.827s, 1237.86/s)  LR: 9.716e-04  Data: 0.010 (0.028)
Train: 65 [ 150/1251 ( 12%)]  Loss: 3.891 (3.87)  Time: 0.779s, 1314.14/s  (0.821s, 1247.25/s)  LR: 9.716e-04  Data: 0.009 (0.022)
Train: 65 [ 200/1251 ( 16%)]  Loss: 4.096 (3.92)  Time: 0.797s, 1284.34/s  (0.818s, 1251.72/s)  LR: 9.716e-04  Data: 0.009 (0.019)
Train: 65 [ 250/1251 ( 20%)]  Loss: 3.958 (3.92)  Time: 0.830s, 1234.10/s  (0.815s, 1256.68/s)  LR: 9.716e-04  Data: 0.009 (0.017)
Train: 65 [ 300/1251 ( 24%)]  Loss: 3.724 (3.90)  Time: 0.800s, 1280.12/s  (0.814s, 1257.74/s)  LR: 9.716e-04  Data: 0.009 (0.016)
Train: 65 [ 350/1251 ( 28%)]  Loss: 4.194 (3.93)  Time: 0.782s, 1309.82/s  (0.813s, 1259.14/s)  LR: 9.716e-04  Data: 0.009 (0.015)
Train: 65 [ 400/1251 ( 32%)]  Loss: 4.081 (3.95)  Time: 0.824s, 1242.29/s  (0.813s, 1259.35/s)  LR: 9.716e-04  Data: 0.010 (0.015)
Train: 65 [ 450/1251 ( 36%)]  Loss: 3.983 (3.95)  Time: 0.889s, 1152.19/s  (0.813s, 1259.82/s)  LR: 9.716e-04  Data: 0.010 (0.014)
Train: 65 [ 500/1251 ( 40%)]  Loss: 3.899 (3.95)  Time: 0.738s, 1387.43/s  (0.813s, 1260.29/s)  LR: 9.716e-04  Data: 0.010 (0.014)
Train: 65 [ 550/1251 ( 44%)]  Loss: 3.671 (3.93)  Time: 0.778s, 1316.45/s  (0.812s, 1261.28/s)  LR: 9.716e-04  Data: 0.009 (0.014)
Train: 65 [ 600/1251 ( 48%)]  Loss: 3.954 (3.93)  Time: 0.865s, 1183.71/s  (0.812s, 1261.64/s)  LR: 9.716e-04  Data: 0.010 (0.013)
Train: 65 [ 650/1251 ( 52%)]  Loss: 3.563 (3.90)  Time: 0.791s, 1294.55/s  (0.811s, 1262.79/s)  LR: 9.716e-04  Data: 0.011 (0.013)
Train: 65 [ 700/1251 ( 56%)]  Loss: 3.794 (3.89)  Time: 0.782s, 1310.08/s  (0.811s, 1263.09/s)  LR: 9.716e-04  Data: 0.009 (0.013)
Train: 65 [ 750/1251 ( 60%)]  Loss: 3.695 (3.88)  Time: 0.793s, 1291.83/s  (0.811s, 1262.91/s)  LR: 9.716e-04  Data: 0.010 (0.013)
Train: 65 [ 800/1251 ( 64%)]  Loss: 3.583 (3.86)  Time: 0.845s, 1212.26/s  (0.811s, 1262.69/s)  LR: 9.716e-04  Data: 0.009 (0.013)
Train: 65 [ 850/1251 ( 68%)]  Loss: 4.024 (3.87)  Time: 0.794s, 1290.13/s  (0.811s, 1262.67/s)  LR: 9.716e-04  Data: 0.010 (0.013)
Train: 65 [ 900/1251 ( 72%)]  Loss: 3.821 (3.87)  Time: 0.889s, 1152.12/s  (0.811s, 1262.68/s)  LR: 9.716e-04  Data: 0.009 (0.012)
Train: 65 [ 950/1251 ( 76%)]  Loss: 4.001 (3.88)  Time: 0.835s, 1226.28/s  (0.811s, 1263.04/s)  LR: 9.716e-04  Data: 0.009 (0.012)
Train: 65 [1000/1251 ( 80%)]  Loss: 4.028 (3.88)  Time: 0.820s, 1248.33/s  (0.811s, 1263.30/s)  LR: 9.716e-04  Data: 0.011 (0.012)
Train: 65 [1050/1251 ( 84%)]  Loss: 3.764 (3.88)  Time: 0.863s, 1186.82/s  (0.811s, 1263.14/s)  LR: 9.716e-04  Data: 0.009 (0.012)
Train: 65 [1100/1251 ( 88%)]  Loss: 3.865 (3.88)  Time: 0.782s, 1310.13/s  (0.811s, 1263.12/s)  LR: 9.716e-04  Data: 0.009 (0.012)
Train: 65 [1150/1251 ( 92%)]  Loss: 3.965 (3.88)  Time: 0.815s, 1256.90/s  (0.810s, 1263.51/s)  LR: 9.716e-04  Data: 0.015 (0.012)
Train: 65 [1200/1251 ( 96%)]  Loss: 3.622 (3.87)  Time: 0.897s, 1141.48/s  (0.811s, 1262.61/s)  LR: 9.716e-04  Data: 0.010 (0.012)
Train: 65 [1250/1251 (100%)]  Loss: 4.052 (3.88)  Time: 0.777s, 1318.34/s  (0.811s, 1262.50/s)  LR: 9.716e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.724 (1.724)  Loss:  0.8350 (0.8350)  Acc@1: 86.5234 (86.5234)  Acc@5: 96.2891 (96.2891)
Test: [  48/48]  Time: 0.194 (0.598)  Loss:  1.0029 (1.5058)  Acc@1: 81.7217 (69.8740)  Acc@5: 94.6934 (89.9780)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-63.pth.tar', 69.96200006835937)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-65.pth.tar', 69.8740000756836)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-58.pth.tar', 69.84399996582032)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-59.pth.tar', 69.716000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-56.pth.tar', 69.56000015136719)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-55.pth.tar', 69.524)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-61.pth.tar', 69.52200004638672)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-53.pth.tar', 69.49800014892578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-52.pth.tar', 69.28799993652343)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-49.pth.tar', 69.27400001464844)

Train: 66 [   0/1251 (  0%)]  Loss: 3.573 (3.57)  Time: 2.367s,  432.62/s  (2.367s,  432.62/s)  LR: 9.707e-04  Data: 1.615 (1.615)
Train: 66 [  50/1251 (  4%)]  Loss: 4.182 (3.88)  Time: 0.793s, 1290.57/s  (0.839s, 1219.98/s)  LR: 9.707e-04  Data: 0.009 (0.047)
Train: 66 [ 100/1251 (  8%)]  Loss: 4.218 (3.99)  Time: 0.806s, 1270.36/s  (0.824s, 1242.92/s)  LR: 9.707e-04  Data: 0.009 (0.029)
Train: 66 [ 150/1251 ( 12%)]  Loss: 3.717 (3.92)  Time: 0.810s, 1264.32/s  (0.819s, 1250.83/s)  LR: 9.707e-04  Data: 0.010 (0.023)
Train: 66 [ 200/1251 ( 16%)]  Loss: 3.633 (3.86)  Time: 0.790s, 1296.52/s  (0.815s, 1255.83/s)  LR: 9.707e-04  Data: 0.010 (0.020)
Train: 66 [ 250/1251 ( 20%)]  Loss: 3.951 (3.88)  Time: 0.778s, 1316.59/s  (0.815s, 1256.38/s)  LR: 9.707e-04  Data: 0.010 (0.018)
Train: 66 [ 300/1251 ( 24%)]  Loss: 3.935 (3.89)  Time: 0.808s, 1267.58/s  (0.814s, 1258.45/s)  LR: 9.707e-04  Data: 0.009 (0.017)
Train: 66 [ 350/1251 ( 28%)]  Loss: 3.648 (3.86)  Time: 0.800s, 1279.57/s  (0.813s, 1259.28/s)  LR: 9.707e-04  Data: 0.009 (0.016)
Train: 66 [ 400/1251 ( 32%)]  Loss: 3.588 (3.83)  Time: 0.821s, 1247.13/s  (0.812s, 1260.73/s)  LR: 9.707e-04  Data: 0.010 (0.015)
Train: 66 [ 450/1251 ( 36%)]  Loss: 3.949 (3.84)  Time: 0.781s, 1310.86/s  (0.812s, 1260.52/s)  LR: 9.707e-04  Data: 0.010 (0.015)
Train: 66 [ 500/1251 ( 40%)]  Loss: 3.714 (3.83)  Time: 0.810s, 1264.91/s  (0.812s, 1261.47/s)  LR: 9.707e-04  Data: 0.010 (0.014)
Train: 66 [ 550/1251 ( 44%)]  Loss: 3.199 (3.78)  Time: 0.778s, 1316.07/s  (0.812s, 1261.39/s)  LR: 9.707e-04  Data: 0.010 (0.014)
Train: 66 [ 600/1251 ( 48%)]  Loss: 3.731 (3.77)  Time: 0.830s, 1234.13/s  (0.811s, 1262.06/s)  LR: 9.707e-04  Data: 0.014 (0.014)
Train: 66 [ 650/1251 ( 52%)]  Loss: 3.554 (3.76)  Time: 0.777s, 1318.55/s  (0.811s, 1262.46/s)  LR: 9.707e-04  Data: 0.010 (0.013)
Train: 66 [ 700/1251 ( 56%)]  Loss: 4.013 (3.77)  Time: 0.803s, 1274.54/s  (0.811s, 1262.13/s)  LR: 9.707e-04  Data: 0.010 (0.013)
Train: 66 [ 750/1251 ( 60%)]  Loss: 3.891 (3.78)  Time: 0.832s, 1231.23/s  (0.811s, 1262.32/s)  LR: 9.707e-04  Data: 0.010 (0.013)
Train: 66 [ 800/1251 ( 64%)]  Loss: 4.088 (3.80)  Time: 0.797s, 1285.02/s  (0.811s, 1263.18/s)  LR: 9.707e-04  Data: 0.014 (0.013)
Train: 66 [ 850/1251 ( 68%)]  Loss: 3.877 (3.80)  Time: 0.831s, 1231.88/s  (0.811s, 1263.06/s)  LR: 9.707e-04  Data: 0.009 (0.013)
Train: 66 [ 900/1251 ( 72%)]  Loss: 3.539 (3.79)  Time: 0.822s, 1245.10/s  (0.810s, 1263.85/s)  LR: 9.707e-04  Data: 0.010 (0.013)
Train: 66 [ 950/1251 ( 76%)]  Loss: 3.598 (3.78)  Time: 0.797s, 1285.38/s  (0.811s, 1263.16/s)  LR: 9.707e-04  Data: 0.009 (0.013)
Train: 66 [1000/1251 ( 80%)]  Loss: 3.902 (3.79)  Time: 0.791s, 1295.00/s  (0.811s, 1263.20/s)  LR: 9.707e-04  Data: 0.010 (0.012)
Train: 66 [1050/1251 ( 84%)]  Loss: 4.007 (3.80)  Time: 0.792s, 1293.64/s  (0.811s, 1262.76/s)  LR: 9.707e-04  Data: 0.011 (0.012)
Train: 66 [1100/1251 ( 88%)]  Loss: 3.662 (3.79)  Time: 0.783s, 1307.44/s  (0.811s, 1263.33/s)  LR: 9.707e-04  Data: 0.009 (0.012)
Train: 66 [1150/1251 ( 92%)]  Loss: 3.817 (3.79)  Time: 0.790s, 1295.81/s  (0.811s, 1263.42/s)  LR: 9.707e-04  Data: 0.009 (0.012)
Train: 66 [1200/1251 ( 96%)]  Loss: 4.082 (3.80)  Time: 0.795s, 1288.71/s  (0.810s, 1263.48/s)  LR: 9.707e-04  Data: 0.009 (0.012)
Train: 66 [1250/1251 (100%)]  Loss: 4.060 (3.81)  Time: 0.798s, 1283.44/s  (0.810s, 1263.71/s)  LR: 9.707e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.583 (1.583)  Loss:  0.8809 (0.8809)  Acc@1: 87.7930 (87.7930)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.194 (0.605)  Loss:  0.9922 (1.5307)  Acc@1: 83.6085 (69.8520)  Acc@5: 95.8726 (90.0560)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-63.pth.tar', 69.96200006835937)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-65.pth.tar', 69.8740000756836)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-66.pth.tar', 69.85199999023438)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-58.pth.tar', 69.84399996582032)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-59.pth.tar', 69.716000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-56.pth.tar', 69.56000015136719)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-55.pth.tar', 69.524)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-61.pth.tar', 69.52200004638672)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-53.pth.tar', 69.49800014892578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-52.pth.tar', 69.28799993652343)

Train: 67 [   0/1251 (  0%)]  Loss: 4.026 (4.03)  Time: 2.200s,  465.50/s  (2.200s,  465.50/s)  LR: 9.699e-04  Data: 1.465 (1.465)
Train: 67 [  50/1251 (  4%)]  Loss: 4.020 (4.02)  Time: 0.805s, 1271.44/s  (0.839s, 1220.22/s)  LR: 9.699e-04  Data: 0.011 (0.047)
Train: 67 [ 100/1251 (  8%)]  Loss: 3.850 (3.97)  Time: 0.819s, 1249.89/s  (0.825s, 1241.06/s)  LR: 9.699e-04  Data: 0.010 (0.029)
Train: 67 [ 150/1251 ( 12%)]  Loss: 3.863 (3.94)  Time: 0.806s, 1270.14/s  (0.822s, 1245.96/s)  LR: 9.699e-04  Data: 0.010 (0.023)
Train: 67 [ 200/1251 ( 16%)]  Loss: 3.539 (3.86)  Time: 0.783s, 1307.40/s  (0.818s, 1251.29/s)  LR: 9.699e-04  Data: 0.013 (0.020)
Train: 67 [ 250/1251 ( 20%)]  Loss: 4.260 (3.93)  Time: 0.823s, 1244.10/s  (0.817s, 1252.87/s)  LR: 9.699e-04  Data: 0.010 (0.018)
Train: 67 [ 300/1251 ( 24%)]  Loss: 3.819 (3.91)  Time: 0.830s, 1234.19/s  (0.815s, 1256.26/s)  LR: 9.699e-04  Data: 0.011 (0.017)
Train: 67 [ 350/1251 ( 28%)]  Loss: 3.453 (3.85)  Time: 0.779s, 1314.08/s  (0.813s, 1258.95/s)  LR: 9.699e-04  Data: 0.009 (0.016)
Train: 67 [ 400/1251 ( 32%)]  Loss: 3.832 (3.85)  Time: 0.826s, 1239.18/s  (0.813s, 1259.42/s)  LR: 9.699e-04  Data: 0.014 (0.015)
Train: 67 [ 450/1251 ( 36%)]  Loss: 4.113 (3.88)  Time: 0.914s, 1120.17/s  (0.813s, 1260.02/s)  LR: 9.699e-04  Data: 0.010 (0.015)
Train: 67 [ 500/1251 ( 40%)]  Loss: 3.946 (3.88)  Time: 0.813s, 1259.36/s  (0.813s, 1259.80/s)  LR: 9.699e-04  Data: 0.009 (0.014)
Train: 67 [ 550/1251 ( 44%)]  Loss: 3.988 (3.89)  Time: 0.811s, 1262.94/s  (0.813s, 1259.54/s)  LR: 9.699e-04  Data: 0.009 (0.014)
Train: 67 [ 600/1251 ( 48%)]  Loss: 3.711 (3.88)  Time: 0.783s, 1307.15/s  (0.813s, 1259.90/s)  LR: 9.699e-04  Data: 0.014 (0.014)
Train: 67 [ 650/1251 ( 52%)]  Loss: 3.498 (3.85)  Time: 0.800s, 1280.22/s  (0.812s, 1260.99/s)  LR: 9.699e-04  Data: 0.014 (0.014)
Train: 67 [ 700/1251 ( 56%)]  Loss: 3.793 (3.85)  Time: 0.812s, 1261.57/s  (0.812s, 1261.11/s)  LR: 9.699e-04  Data: 0.009 (0.013)
Train: 67 [ 750/1251 ( 60%)]  Loss: 3.337 (3.82)  Time: 0.778s, 1316.47/s  (0.812s, 1261.35/s)  LR: 9.699e-04  Data: 0.010 (0.013)
Train: 67 [ 800/1251 ( 64%)]  Loss: 4.018 (3.83)  Time: 0.798s, 1283.56/s  (0.812s, 1261.71/s)  LR: 9.699e-04  Data: 0.009 (0.013)
Train: 67 [ 850/1251 ( 68%)]  Loss: 4.134 (3.84)  Time: 0.798s, 1283.22/s  (0.812s, 1261.37/s)  LR: 9.699e-04  Data: 0.010 (0.013)
Train: 67 [ 900/1251 ( 72%)]  Loss: 4.002 (3.85)  Time: 0.779s, 1314.96/s  (0.811s, 1261.91/s)  LR: 9.699e-04  Data: 0.010 (0.013)
Train: 67 [ 950/1251 ( 76%)]  Loss: 3.976 (3.86)  Time: 0.806s, 1271.18/s  (0.811s, 1262.33/s)  LR: 9.699e-04  Data: 0.011 (0.013)
Train: 67 [1000/1251 ( 80%)]  Loss: 3.680 (3.85)  Time: 0.785s, 1303.70/s  (0.811s, 1262.55/s)  LR: 9.699e-04  Data: 0.011 (0.012)
Train: 67 [1050/1251 ( 84%)]  Loss: 4.126 (3.86)  Time: 0.794s, 1288.87/s  (0.811s, 1262.56/s)  LR: 9.699e-04  Data: 0.010 (0.012)
Train: 67 [1100/1251 ( 88%)]  Loss: 3.613 (3.85)  Time: 0.785s, 1305.06/s  (0.811s, 1262.32/s)  LR: 9.699e-04  Data: 0.010 (0.012)
Train: 67 [1150/1251 ( 92%)]  Loss: 3.910 (3.85)  Time: 0.780s, 1312.06/s  (0.811s, 1262.76/s)  LR: 9.699e-04  Data: 0.009 (0.012)
Train: 67 [1200/1251 ( 96%)]  Loss: 3.912 (3.86)  Time: 0.780s, 1313.31/s  (0.811s, 1262.71/s)  LR: 9.699e-04  Data: 0.013 (0.012)
Train: 67 [1250/1251 (100%)]  Loss: 3.686 (3.85)  Time: 0.765s, 1338.09/s  (0.811s, 1263.03/s)  LR: 9.699e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.667 (1.667)  Loss:  0.9097 (0.9097)  Acc@1: 87.4023 (87.4023)  Acc@5: 95.8008 (95.8008)
Test: [  48/48]  Time: 0.193 (0.597)  Loss:  1.0029 (1.5153)  Acc@1: 83.8443 (70.2220)  Acc@5: 95.6368 (90.0980)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-67.pth.tar', 70.22200009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-63.pth.tar', 69.96200006835937)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-65.pth.tar', 69.8740000756836)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-66.pth.tar', 69.85199999023438)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-58.pth.tar', 69.84399996582032)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-59.pth.tar', 69.716000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-56.pth.tar', 69.56000015136719)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-55.pth.tar', 69.524)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-61.pth.tar', 69.52200004638672)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-53.pth.tar', 69.49800014892578)

Train: 68 [   0/1251 (  0%)]  Loss: 3.619 (3.62)  Time: 2.524s,  405.73/s  (2.524s,  405.73/s)  LR: 9.690e-04  Data: 1.796 (1.796)
Train: 68 [  50/1251 (  4%)]  Loss: 3.924 (3.77)  Time: 0.803s, 1275.05/s  (0.842s, 1216.23/s)  LR: 9.690e-04  Data: 0.009 (0.050)
Train: 68 [ 100/1251 (  8%)]  Loss: 3.670 (3.74)  Time: 0.784s, 1305.70/s  (0.822s, 1245.28/s)  LR: 9.690e-04  Data: 0.009 (0.030)
Train: 68 [ 150/1251 ( 12%)]  Loss: 3.796 (3.75)  Time: 0.787s, 1301.21/s  (0.816s, 1254.19/s)  LR: 9.690e-04  Data: 0.009 (0.024)
Train: 68 [ 200/1251 ( 16%)]  Loss: 2.980 (3.60)  Time: 0.798s, 1283.94/s  (0.816s, 1255.55/s)  LR: 9.690e-04  Data: 0.009 (0.021)
Train: 68 [ 250/1251 ( 20%)]  Loss: 3.361 (3.56)  Time: 0.842s, 1216.42/s  (0.814s, 1258.22/s)  LR: 9.690e-04  Data: 0.009 (0.019)
Train: 68 [ 300/1251 ( 24%)]  Loss: 3.639 (3.57)  Time: 0.800s, 1279.40/s  (0.813s, 1258.95/s)  LR: 9.690e-04  Data: 0.014 (0.017)
Train: 68 [ 350/1251 ( 28%)]  Loss: 3.835 (3.60)  Time: 0.834s, 1227.18/s  (0.814s, 1258.23/s)  LR: 9.690e-04  Data: 0.009 (0.016)
Train: 68 [ 400/1251 ( 32%)]  Loss: 3.984 (3.65)  Time: 0.824s, 1243.23/s  (0.814s, 1258.34/s)  LR: 9.690e-04  Data: 0.010 (0.016)
Train: 68 [ 450/1251 ( 36%)]  Loss: 3.926 (3.67)  Time: 0.788s, 1300.20/s  (0.813s, 1259.18/s)  LR: 9.690e-04  Data: 0.009 (0.015)
Train: 68 [ 500/1251 ( 40%)]  Loss: 3.508 (3.66)  Time: 0.877s, 1167.71/s  (0.813s, 1259.47/s)  LR: 9.690e-04  Data: 0.010 (0.015)
Train: 68 [ 550/1251 ( 44%)]  Loss: 3.647 (3.66)  Time: 0.807s, 1269.63/s  (0.812s, 1260.47/s)  LR: 9.690e-04  Data: 0.009 (0.014)
Train: 68 [ 600/1251 ( 48%)]  Loss: 3.917 (3.68)  Time: 0.790s, 1295.47/s  (0.812s, 1261.78/s)  LR: 9.690e-04  Data: 0.010 (0.014)
Train: 68 [ 650/1251 ( 52%)]  Loss: 3.889 (3.69)  Time: 0.824s, 1243.16/s  (0.811s, 1261.91/s)  LR: 9.690e-04  Data: 0.009 (0.014)
Train: 68 [ 700/1251 ( 56%)]  Loss: 4.082 (3.72)  Time: 0.828s, 1235.99/s  (0.812s, 1261.66/s)  LR: 9.690e-04  Data: 0.014 (0.013)
Train: 68 [ 750/1251 ( 60%)]  Loss: 3.917 (3.73)  Time: 0.796s, 1286.37/s  (0.812s, 1261.65/s)  LR: 9.690e-04  Data: 0.009 (0.013)
Train: 68 [ 800/1251 ( 64%)]  Loss: 3.822 (3.74)  Time: 0.782s, 1309.42/s  (0.812s, 1261.13/s)  LR: 9.690e-04  Data: 0.010 (0.013)
Train: 68 [ 850/1251 ( 68%)]  Loss: 3.399 (3.72)  Time: 0.812s, 1260.94/s  (0.812s, 1261.23/s)  LR: 9.690e-04  Data: 0.014 (0.013)
Train: 68 [ 900/1251 ( 72%)]  Loss: 3.509 (3.71)  Time: 0.815s, 1255.79/s  (0.812s, 1261.38/s)  LR: 9.690e-04  Data: 0.011 (0.013)
Train: 68 [ 950/1251 ( 76%)]  Loss: 3.649 (3.70)  Time: 0.870s, 1177.42/s  (0.812s, 1261.61/s)  LR: 9.690e-04  Data: 0.009 (0.013)
Train: 68 [1000/1251 ( 80%)]  Loss: 3.886 (3.71)  Time: 0.778s, 1316.09/s  (0.812s, 1261.41/s)  LR: 9.690e-04  Data: 0.010 (0.013)
Train: 68 [1050/1251 ( 84%)]  Loss: 3.570 (3.71)  Time: 0.828s, 1236.38/s  (0.812s, 1261.22/s)  LR: 9.690e-04  Data: 0.010 (0.013)
Train: 68 [1100/1251 ( 88%)]  Loss: 3.812 (3.71)  Time: 0.789s, 1298.41/s  (0.812s, 1261.45/s)  LR: 9.690e-04  Data: 0.010 (0.012)
Train: 68 [1150/1251 ( 92%)]  Loss: 3.701 (3.71)  Time: 0.803s, 1274.54/s  (0.811s, 1262.06/s)  LR: 9.690e-04  Data: 0.011 (0.012)
Train: 68 [1200/1251 ( 96%)]  Loss: 3.928 (3.72)  Time: 0.784s, 1306.00/s  (0.811s, 1262.62/s)  LR: 9.690e-04  Data: 0.009 (0.012)
Train: 68 [1250/1251 (100%)]  Loss: 3.849 (3.72)  Time: 0.769s, 1332.30/s  (0.811s, 1262.86/s)  LR: 9.690e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.641 (1.641)  Loss:  0.9482 (0.9482)  Acc@1: 84.9609 (84.9609)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.194 (0.607)  Loss:  1.1484 (1.5582)  Acc@1: 81.9575 (69.1180)  Acc@5: 93.1604 (89.9120)
Train: 69 [   0/1251 (  0%)]  Loss: 3.657 (3.66)  Time: 2.356s,  434.71/s  (2.356s,  434.71/s)  LR: 9.680e-04  Data: 1.621 (1.621)
Train: 69 [  50/1251 (  4%)]  Loss: 3.854 (3.76)  Time: 0.782s, 1310.29/s  (0.844s, 1212.87/s)  LR: 9.680e-04  Data: 0.009 (0.053)
Train: 69 [ 100/1251 (  8%)]  Loss: 3.880 (3.80)  Time: 0.855s, 1197.78/s  (0.831s, 1231.72/s)  LR: 9.680e-04  Data: 0.010 (0.032)
Train: 69 [ 150/1251 ( 12%)]  Loss: 3.673 (3.77)  Time: 0.808s, 1267.26/s  (0.825s, 1241.53/s)  LR: 9.680e-04  Data: 0.011 (0.025)
Train: 69 [ 200/1251 ( 16%)]  Loss: 4.045 (3.82)  Time: 0.814s, 1258.53/s  (0.821s, 1247.43/s)  LR: 9.680e-04  Data: 0.011 (0.021)
Train: 69 [ 250/1251 ( 20%)]  Loss: 3.628 (3.79)  Time: 0.865s, 1184.44/s  (0.820s, 1249.38/s)  LR: 9.680e-04  Data: 0.009 (0.019)
Train: 69 [ 300/1251 ( 24%)]  Loss: 3.571 (3.76)  Time: 0.799s, 1281.48/s  (0.818s, 1251.37/s)  LR: 9.680e-04  Data: 0.010 (0.018)
Train: 69 [ 350/1251 ( 28%)]  Loss: 3.231 (3.69)  Time: 0.776s, 1318.85/s  (0.817s, 1253.79/s)  LR: 9.680e-04  Data: 0.011 (0.017)
Train: 69 [ 400/1251 ( 32%)]  Loss: 3.722 (3.70)  Time: 0.819s, 1250.50/s  (0.815s, 1256.40/s)  LR: 9.680e-04  Data: 0.016 (0.016)
Train: 69 [ 450/1251 ( 36%)]  Loss: 3.930 (3.72)  Time: 0.825s, 1241.42/s  (0.814s, 1257.24/s)  LR: 9.680e-04  Data: 0.010 (0.015)
Train: 69 [ 500/1251 ( 40%)]  Loss: 3.986 (3.74)  Time: 0.821s, 1246.58/s  (0.813s, 1259.33/s)  LR: 9.680e-04  Data: 0.010 (0.015)
Train: 69 [ 550/1251 ( 44%)]  Loss: 4.084 (3.77)  Time: 0.814s, 1258.43/s  (0.815s, 1256.30/s)  LR: 9.680e-04  Data: 0.012 (0.015)
Train: 69 [ 600/1251 ( 48%)]  Loss: 3.660 (3.76)  Time: 0.778s, 1316.38/s  (0.814s, 1258.07/s)  LR: 9.680e-04  Data: 0.010 (0.014)
Train: 69 [ 650/1251 ( 52%)]  Loss: 3.931 (3.78)  Time: 0.808s, 1267.14/s  (0.812s, 1261.65/s)  LR: 9.680e-04  Data: 0.010 (0.014)
Train: 69 [ 700/1251 ( 56%)]  Loss: 3.839 (3.78)  Time: 0.874s, 1171.03/s  (0.811s, 1262.45/s)  LR: 9.680e-04  Data: 0.009 (0.014)
Train: 69 [ 750/1251 ( 60%)]  Loss: 3.869 (3.79)  Time: 0.812s, 1261.34/s  (0.811s, 1262.70/s)  LR: 9.680e-04  Data: 0.009 (0.014)
Train: 69 [ 800/1251 ( 64%)]  Loss: 3.741 (3.78)  Time: 0.806s, 1270.16/s  (0.811s, 1263.40/s)  LR: 9.680e-04  Data: 0.012 (0.013)
Train: 69 [ 850/1251 ( 68%)]  Loss: 3.999 (3.79)  Time: 0.798s, 1283.99/s  (0.811s, 1262.70/s)  LR: 9.680e-04  Data: 0.009 (0.013)
Train: 69 [ 900/1251 ( 72%)]  Loss: 3.204 (3.76)  Time: 0.774s, 1323.30/s  (0.811s, 1263.17/s)  LR: 9.680e-04  Data: 0.010 (0.013)
Train: 69 [ 950/1251 ( 76%)]  Loss: 3.653 (3.76)  Time: 0.814s, 1258.12/s  (0.811s, 1263.25/s)  LR: 9.680e-04  Data: 0.009 (0.013)
Train: 69 [1000/1251 ( 80%)]  Loss: 4.375 (3.79)  Time: 0.804s, 1273.09/s  (0.810s, 1263.42/s)  LR: 9.680e-04  Data: 0.014 (0.013)
Train: 69 [1050/1251 ( 84%)]  Loss: 3.869 (3.79)  Time: 0.798s, 1282.77/s  (0.810s, 1263.72/s)  LR: 9.680e-04  Data: 0.016 (0.013)
Train: 69 [1100/1251 ( 88%)]  Loss: 3.912 (3.80)  Time: 0.845s, 1211.87/s  (0.811s, 1263.03/s)  LR: 9.680e-04  Data: 0.014 (0.013)
Train: 69 [1150/1251 ( 92%)]  Loss: 3.753 (3.79)  Time: 0.831s, 1232.49/s  (0.811s, 1263.22/s)  LR: 9.680e-04  Data: 0.009 (0.012)
Train: 69 [1200/1251 ( 96%)]  Loss: 3.584 (3.79)  Time: 0.903s, 1133.62/s  (0.811s, 1263.08/s)  LR: 9.680e-04  Data: 0.010 (0.012)
Train: 69 [1250/1251 (100%)]  Loss: 3.878 (3.79)  Time: 0.766s, 1337.63/s  (0.810s, 1263.56/s)  LR: 9.680e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.712 (1.712)  Loss:  0.8730 (0.8730)  Acc@1: 87.1094 (87.1094)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.194 (0.604)  Loss:  0.9214 (1.4804)  Acc@1: 81.4858 (70.2400)  Acc@5: 95.5189 (90.2860)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-69.pth.tar', 70.23999997314453)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-67.pth.tar', 70.22200009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-63.pth.tar', 69.96200006835937)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-65.pth.tar', 69.8740000756836)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-66.pth.tar', 69.85199999023438)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-58.pth.tar', 69.84399996582032)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-59.pth.tar', 69.716000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-56.pth.tar', 69.56000015136719)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-55.pth.tar', 69.524)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-61.pth.tar', 69.52200004638672)

Train: 70 [   0/1251 (  0%)]  Loss: 3.732 (3.73)  Time: 2.311s,  443.18/s  (2.311s,  443.18/s)  LR: 9.671e-04  Data: 1.573 (1.573)
Train: 70 [  50/1251 (  4%)]  Loss: 3.717 (3.72)  Time: 0.790s, 1296.10/s  (0.844s, 1212.98/s)  LR: 9.671e-04  Data: 0.010 (0.050)
Train: 70 [ 100/1251 (  8%)]  Loss: 3.693 (3.71)  Time: 0.793s, 1290.94/s  (0.828s, 1236.01/s)  LR: 9.671e-04  Data: 0.010 (0.030)
Train: 70 [ 150/1251 ( 12%)]  Loss: 3.923 (3.77)  Time: 0.823s, 1244.24/s  (0.821s, 1246.77/s)  LR: 9.671e-04  Data: 0.010 (0.024)
Train: 70 [ 200/1251 ( 16%)]  Loss: 3.687 (3.75)  Time: 0.842s, 1215.86/s  (0.820s, 1248.40/s)  LR: 9.671e-04  Data: 0.011 (0.021)
Train: 70 [ 250/1251 ( 20%)]  Loss: 3.735 (3.75)  Time: 0.808s, 1267.83/s  (0.819s, 1250.99/s)  LR: 9.671e-04  Data: 0.010 (0.019)
Train: 70 [ 300/1251 ( 24%)]  Loss: 3.947 (3.78)  Time: 0.778s, 1316.91/s  (0.818s, 1251.92/s)  LR: 9.671e-04  Data: 0.011 (0.017)
Train: 70 [ 350/1251 ( 28%)]  Loss: 3.815 (3.78)  Time: 0.814s, 1258.23/s  (0.816s, 1254.43/s)  LR: 9.671e-04  Data: 0.014 (0.016)
Train: 70 [ 400/1251 ( 32%)]  Loss: 4.086 (3.81)  Time: 0.778s, 1316.45/s  (0.815s, 1256.77/s)  LR: 9.671e-04  Data: 0.010 (0.016)
Train: 70 [ 450/1251 ( 36%)]  Loss: 4.055 (3.84)  Time: 0.825s, 1240.71/s  (0.814s, 1258.23/s)  LR: 9.671e-04  Data: 0.010 (0.015)
Train: 70 [ 500/1251 ( 40%)]  Loss: 3.701 (3.83)  Time: 0.775s, 1320.95/s  (0.813s, 1259.26/s)  LR: 9.671e-04  Data: 0.011 (0.015)
Train: 70 [ 550/1251 ( 44%)]  Loss: 3.441 (3.79)  Time: 0.834s, 1228.23/s  (0.814s, 1258.44/s)  LR: 9.671e-04  Data: 0.015 (0.014)
Train: 70 [ 600/1251 ( 48%)]  Loss: 3.872 (3.80)  Time: 0.786s, 1302.81/s  (0.813s, 1259.47/s)  LR: 9.671e-04  Data: 0.010 (0.014)
Train: 70 [ 650/1251 ( 52%)]  Loss: 4.142 (3.82)  Time: 0.819s, 1249.65/s  (0.813s, 1259.48/s)  LR: 9.671e-04  Data: 0.012 (0.014)
Train: 70 [ 700/1251 ( 56%)]  Loss: 3.755 (3.82)  Time: 0.817s, 1253.10/s  (0.813s, 1259.47/s)  LR: 9.671e-04  Data: 0.010 (0.014)
Train: 70 [ 750/1251 ( 60%)]  Loss: 4.062 (3.84)  Time: 0.859s, 1192.60/s  (0.813s, 1259.93/s)  LR: 9.671e-04  Data: 0.011 (0.013)
Train: 70 [ 800/1251 ( 64%)]  Loss: 3.629 (3.82)  Time: 0.823s, 1244.72/s  (0.813s, 1259.63/s)  LR: 9.671e-04  Data: 0.013 (0.013)
Train: 70 [ 850/1251 ( 68%)]  Loss: 3.619 (3.81)  Time: 0.802s, 1277.54/s  (0.813s, 1259.65/s)  LR: 9.671e-04  Data: 0.010 (0.013)
Train: 70 [ 900/1251 ( 72%)]  Loss: 3.903 (3.82)  Time: 0.851s, 1203.34/s  (0.813s, 1260.16/s)  LR: 9.671e-04  Data: 0.010 (0.013)
Train: 70 [ 950/1251 ( 76%)]  Loss: 3.708 (3.81)  Time: 0.825s, 1241.47/s  (0.813s, 1259.92/s)  LR: 9.671e-04  Data: 0.010 (0.013)
Train: 70 [1000/1251 ( 80%)]  Loss: 3.752 (3.81)  Time: 0.815s, 1256.08/s  (0.813s, 1259.99/s)  LR: 9.671e-04  Data: 0.010 (0.013)
Train: 70 [1050/1251 ( 84%)]  Loss: 3.700 (3.80)  Time: 0.790s, 1296.03/s  (0.813s, 1259.29/s)  LR: 9.671e-04  Data: 0.013 (0.013)
Train: 70 [1100/1251 ( 88%)]  Loss: 3.634 (3.80)  Time: 0.830s, 1233.20/s  (0.813s, 1259.92/s)  LR: 9.671e-04  Data: 0.010 (0.013)
Train: 70 [1150/1251 ( 92%)]  Loss: 3.865 (3.80)  Time: 0.819s, 1250.12/s  (0.813s, 1260.08/s)  LR: 9.671e-04  Data: 0.019 (0.012)
Train: 70 [1200/1251 ( 96%)]  Loss: 3.673 (3.79)  Time: 0.840s, 1219.25/s  (0.812s, 1260.43/s)  LR: 9.671e-04  Data: 0.010 (0.012)
Train: 70 [1250/1251 (100%)]  Loss: 3.336 (3.78)  Time: 0.810s, 1264.61/s  (0.812s, 1260.74/s)  LR: 9.671e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.692 (1.692)  Loss:  0.8301 (0.8301)  Acc@1: 86.9141 (86.9141)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.194 (0.611)  Loss:  0.9043 (1.4880)  Acc@1: 83.0189 (70.2340)  Acc@5: 95.2830 (90.2460)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-69.pth.tar', 70.23999997314453)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-70.pth.tar', 70.23399999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-67.pth.tar', 70.22200009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-63.pth.tar', 69.96200006835937)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-65.pth.tar', 69.8740000756836)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-66.pth.tar', 69.85199999023438)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-58.pth.tar', 69.84399996582032)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-59.pth.tar', 69.716000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-56.pth.tar', 69.56000015136719)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-55.pth.tar', 69.524)

Train: 71 [   0/1251 (  0%)]  Loss: 3.689 (3.69)  Time: 2.474s,  413.96/s  (2.474s,  413.96/s)  LR: 9.662e-04  Data: 1.699 (1.699)
Train: 71 [  50/1251 (  4%)]  Loss: 3.983 (3.84)  Time: 0.801s, 1278.95/s  (0.851s, 1203.80/s)  LR: 9.662e-04  Data: 0.009 (0.052)
Train: 71 [ 100/1251 (  8%)]  Loss: 3.635 (3.77)  Time: 0.785s, 1304.01/s  (0.831s, 1232.85/s)  LR: 9.662e-04  Data: 0.010 (0.032)
Train: 71 [ 150/1251 ( 12%)]  Loss: 3.394 (3.68)  Time: 0.782s, 1308.87/s  (0.824s, 1243.02/s)  LR: 9.662e-04  Data: 0.009 (0.025)
Train: 71 [ 200/1251 ( 16%)]  Loss: 3.720 (3.68)  Time: 0.778s, 1315.73/s  (0.820s, 1248.63/s)  LR: 9.662e-04  Data: 0.010 (0.021)
Train: 71 [ 250/1251 ( 20%)]  Loss: 3.423 (3.64)  Time: 0.855s, 1197.37/s  (0.819s, 1250.50/s)  LR: 9.662e-04  Data: 0.010 (0.019)
Train: 71 [ 300/1251 ( 24%)]  Loss: 4.142 (3.71)  Time: 0.786s, 1303.15/s  (0.816s, 1254.29/s)  LR: 9.662e-04  Data: 0.013 (0.018)
Train: 71 [ 350/1251 ( 28%)]  Loss: 3.552 (3.69)  Time: 0.835s, 1226.80/s  (0.815s, 1255.70/s)  LR: 9.662e-04  Data: 0.010 (0.017)
Train: 71 [ 400/1251 ( 32%)]  Loss: 4.000 (3.73)  Time: 0.826s, 1239.06/s  (0.814s, 1257.29/s)  LR: 9.662e-04  Data: 0.009 (0.016)
Train: 71 [ 450/1251 ( 36%)]  Loss: 3.792 (3.73)  Time: 0.901s, 1136.88/s  (0.814s, 1257.27/s)  LR: 9.662e-04  Data: 0.009 (0.015)
Train: 71 [ 500/1251 ( 40%)]  Loss: 3.778 (3.74)  Time: 0.791s, 1295.32/s  (0.814s, 1258.00/s)  LR: 9.662e-04  Data: 0.011 (0.015)
Train: 71 [ 550/1251 ( 44%)]  Loss: 3.443 (3.71)  Time: 0.817s, 1253.37/s  (0.813s, 1259.29/s)  LR: 9.662e-04  Data: 0.009 (0.015)
Train: 71 [ 600/1251 ( 48%)]  Loss: 3.640 (3.71)  Time: 0.838s, 1221.41/s  (0.812s, 1260.78/s)  LR: 9.662e-04  Data: 0.009 (0.014)
Train: 71 [ 650/1251 ( 52%)]  Loss: 3.775 (3.71)  Time: 0.822s, 1245.64/s  (0.811s, 1261.93/s)  LR: 9.662e-04  Data: 0.010 (0.014)
Train: 71 [ 700/1251 ( 56%)]  Loss: 3.778 (3.72)  Time: 0.785s, 1304.50/s  (0.811s, 1261.86/s)  LR: 9.662e-04  Data: 0.010 (0.014)
Train: 71 [ 750/1251 ( 60%)]  Loss: 3.848 (3.72)  Time: 0.800s, 1279.97/s  (0.811s, 1262.34/s)  LR: 9.662e-04  Data: 0.009 (0.014)
Train: 71 [ 800/1251 ( 64%)]  Loss: 3.815 (3.73)  Time: 0.843s, 1215.40/s  (0.811s, 1261.95/s)  LR: 9.662e-04  Data: 0.009 (0.013)
Train: 71 [ 850/1251 ( 68%)]  Loss: 4.034 (3.75)  Time: 0.783s, 1307.32/s  (0.812s, 1261.36/s)  LR: 9.662e-04  Data: 0.010 (0.013)
Train: 71 [ 900/1251 ( 72%)]  Loss: 3.975 (3.76)  Time: 0.813s, 1259.46/s  (0.812s, 1261.78/s)  LR: 9.662e-04  Data: 0.010 (0.013)
Train: 71 [ 950/1251 ( 76%)]  Loss: 3.589 (3.75)  Time: 0.867s, 1181.01/s  (0.811s, 1262.24/s)  LR: 9.662e-04  Data: 0.010 (0.013)
Train: 71 [1000/1251 ( 80%)]  Loss: 3.765 (3.75)  Time: 0.824s, 1242.57/s  (0.811s, 1262.24/s)  LR: 9.662e-04  Data: 0.011 (0.013)
Train: 71 [1050/1251 ( 84%)]  Loss: 4.033 (3.76)  Time: 0.814s, 1257.68/s  (0.811s, 1261.87/s)  LR: 9.662e-04  Data: 0.010 (0.013)
Train: 71 [1100/1251 ( 88%)]  Loss: 4.111 (3.78)  Time: 0.793s, 1291.25/s  (0.811s, 1261.90/s)  LR: 9.662e-04  Data: 0.010 (0.013)
Train: 71 [1150/1251 ( 92%)]  Loss: 3.760 (3.78)  Time: 0.837s, 1222.76/s  (0.812s, 1261.75/s)  LR: 9.662e-04  Data: 0.013 (0.013)
Train: 71 [1200/1251 ( 96%)]  Loss: 3.976 (3.79)  Time: 0.782s, 1309.56/s  (0.812s, 1261.63/s)  LR: 9.662e-04  Data: 0.010 (0.012)
Train: 71 [1250/1251 (100%)]  Loss: 4.195 (3.80)  Time: 0.785s, 1303.77/s  (0.811s, 1262.17/s)  LR: 9.662e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.641 (1.641)  Loss:  0.8789 (0.8789)  Acc@1: 85.4492 (85.4492)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.194 (0.611)  Loss:  1.0312 (1.4929)  Acc@1: 81.0142 (70.1460)  Acc@5: 93.7500 (90.1080)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-69.pth.tar', 70.23999997314453)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-70.pth.tar', 70.23399999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-67.pth.tar', 70.22200009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-71.pth.tar', 70.14600002685548)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-63.pth.tar', 69.96200006835937)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-65.pth.tar', 69.8740000756836)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-66.pth.tar', 69.85199999023438)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-58.pth.tar', 69.84399996582032)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-59.pth.tar', 69.716000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-56.pth.tar', 69.56000015136719)

Train: 72 [   0/1251 (  0%)]  Loss: 3.861 (3.86)  Time: 2.463s,  415.77/s  (2.463s,  415.77/s)  LR: 9.652e-04  Data: 1.732 (1.732)
Train: 72 [  50/1251 (  4%)]  Loss: 3.769 (3.81)  Time: 0.779s, 1314.14/s  (0.844s, 1213.44/s)  LR: 9.652e-04  Data: 0.010 (0.049)
Train: 72 [ 100/1251 (  8%)]  Loss: 3.707 (3.78)  Time: 0.804s, 1274.30/s  (0.824s, 1243.46/s)  LR: 9.652e-04  Data: 0.015 (0.030)
Train: 72 [ 150/1251 ( 12%)]  Loss: 3.722 (3.76)  Time: 0.806s, 1269.88/s  (0.822s, 1245.58/s)  LR: 9.652e-04  Data: 0.010 (0.024)
Train: 72 [ 200/1251 ( 16%)]  Loss: 3.962 (3.80)  Time: 0.780s, 1313.16/s  (0.820s, 1249.07/s)  LR: 9.652e-04  Data: 0.010 (0.020)
Train: 72 [ 250/1251 ( 20%)]  Loss: 3.760 (3.80)  Time: 0.849s, 1206.02/s  (0.819s, 1250.47/s)  LR: 9.652e-04  Data: 0.009 (0.018)
Train: 72 [ 300/1251 ( 24%)]  Loss: 3.676 (3.78)  Time: 0.788s, 1298.97/s  (0.818s, 1252.21/s)  LR: 9.652e-04  Data: 0.013 (0.017)
Train: 72 [ 350/1251 ( 28%)]  Loss: 4.156 (3.83)  Time: 0.783s, 1307.95/s  (0.816s, 1254.74/s)  LR: 9.652e-04  Data: 0.009 (0.016)
Train: 72 [ 400/1251 ( 32%)]  Loss: 3.935 (3.84)  Time: 0.872s, 1174.90/s  (0.815s, 1256.81/s)  LR: 9.652e-04  Data: 0.019 (0.015)
Train: 72 [ 450/1251 ( 36%)]  Loss: 3.859 (3.84)  Time: 0.789s, 1297.25/s  (0.814s, 1257.36/s)  LR: 9.652e-04  Data: 0.009 (0.015)
Train: 72 [ 500/1251 ( 40%)]  Loss: 3.993 (3.85)  Time: 0.783s, 1308.08/s  (0.814s, 1258.50/s)  LR: 9.652e-04  Data: 0.014 (0.014)
Train: 72 [ 550/1251 ( 44%)]  Loss: 3.995 (3.87)  Time: 0.845s, 1211.41/s  (0.813s, 1259.58/s)  LR: 9.652e-04  Data: 0.009 (0.014)
Train: 72 [ 600/1251 ( 48%)]  Loss: 3.780 (3.86)  Time: 0.777s, 1318.01/s  (0.812s, 1260.49/s)  LR: 9.652e-04  Data: 0.010 (0.014)
Train: 72 [ 650/1251 ( 52%)]  Loss: 3.590 (3.84)  Time: 0.811s, 1262.49/s  (0.812s, 1260.35/s)  LR: 9.652e-04  Data: 0.011 (0.014)
Train: 72 [ 700/1251 ( 56%)]  Loss: 3.765 (3.84)  Time: 0.831s, 1232.07/s  (0.813s, 1259.84/s)  LR: 9.652e-04  Data: 0.011 (0.013)
Train: 72 [ 750/1251 ( 60%)]  Loss: 3.758 (3.83)  Time: 0.804s, 1273.12/s  (0.812s, 1260.50/s)  LR: 9.652e-04  Data: 0.010 (0.013)
Train: 72 [ 800/1251 ( 64%)]  Loss: 3.780 (3.83)  Time: 0.795s, 1287.70/s  (0.812s, 1260.62/s)  LR: 9.652e-04  Data: 0.010 (0.013)
Train: 72 [ 850/1251 ( 68%)]  Loss: 3.923 (3.83)  Time: 0.779s, 1313.67/s  (0.812s, 1261.13/s)  LR: 9.652e-04  Data: 0.010 (0.013)
Train: 72 [ 900/1251 ( 72%)]  Loss: 3.634 (3.82)  Time: 0.776s, 1319.67/s  (0.812s, 1261.24/s)  LR: 9.652e-04  Data: 0.010 (0.013)
Train: 72 [ 950/1251 ( 76%)]  Loss: 3.585 (3.81)  Time: 0.782s, 1309.23/s  (0.812s, 1261.03/s)  LR: 9.652e-04  Data: 0.010 (0.013)
Train: 72 [1000/1251 ( 80%)]  Loss: 3.717 (3.81)  Time: 0.782s, 1310.04/s  (0.812s, 1260.83/s)  LR: 9.652e-04  Data: 0.010 (0.013)
Train: 72 [1050/1251 ( 84%)]  Loss: 3.851 (3.81)  Time: 0.786s, 1302.18/s  (0.812s, 1261.13/s)  LR: 9.652e-04  Data: 0.010 (0.012)
Train: 72 [1100/1251 ( 88%)]  Loss: 3.507 (3.79)  Time: 0.780s, 1312.10/s  (0.812s, 1261.49/s)  LR: 9.652e-04  Data: 0.009 (0.012)
Train: 72 [1150/1251 ( 92%)]  Loss: 3.423 (3.78)  Time: 0.825s, 1241.42/s  (0.811s, 1261.99/s)  LR: 9.652e-04  Data: 0.009 (0.012)
Train: 72 [1200/1251 ( 96%)]  Loss: 3.786 (3.78)  Time: 0.804s, 1274.18/s  (0.811s, 1262.43/s)  LR: 9.652e-04  Data: 0.010 (0.012)
Train: 72 [1250/1251 (100%)]  Loss: 3.556 (3.77)  Time: 0.811s, 1263.35/s  (0.811s, 1262.65/s)  LR: 9.652e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.647 (1.647)  Loss:  0.8257 (0.8257)  Acc@1: 87.4023 (87.4023)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.194 (0.606)  Loss:  1.0127 (1.4584)  Acc@1: 83.2547 (70.7140)  Acc@5: 95.6368 (90.3820)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-72.pth.tar', 70.7139999658203)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-69.pth.tar', 70.23999997314453)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-70.pth.tar', 70.23399999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-67.pth.tar', 70.22200009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-71.pth.tar', 70.14600002685548)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-63.pth.tar', 69.96200006835937)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-65.pth.tar', 69.8740000756836)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-66.pth.tar', 69.85199999023438)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-58.pth.tar', 69.84399996582032)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-59.pth.tar', 69.716000078125)

Train: 73 [   0/1251 (  0%)]  Loss: 3.977 (3.98)  Time: 2.430s,  421.39/s  (2.430s,  421.39/s)  LR: 9.643e-04  Data: 1.696 (1.696)
Train: 73 [  50/1251 (  4%)]  Loss: 3.737 (3.86)  Time: 0.816s, 1254.29/s  (0.843s, 1215.13/s)  LR: 9.643e-04  Data: 0.009 (0.049)
Train: 73 [ 100/1251 (  8%)]  Loss: 3.183 (3.63)  Time: 0.828s, 1237.25/s  (0.825s, 1240.69/s)  LR: 9.643e-04  Data: 0.010 (0.030)
Train: 73 [ 150/1251 ( 12%)]  Loss: 3.786 (3.67)  Time: 0.777s, 1317.05/s  (0.821s, 1247.56/s)  LR: 9.643e-04  Data: 0.010 (0.024)
Train: 73 [ 200/1251 ( 16%)]  Loss: 3.229 (3.58)  Time: 0.802s, 1276.21/s  (0.819s, 1250.49/s)  LR: 9.643e-04  Data: 0.010 (0.020)
Train: 73 [ 250/1251 ( 20%)]  Loss: 3.927 (3.64)  Time: 0.843s, 1214.83/s  (0.817s, 1253.91/s)  LR: 9.643e-04  Data: 0.010 (0.019)
Train: 73 [ 300/1251 ( 24%)]  Loss: 3.670 (3.64)  Time: 0.827s, 1237.56/s  (0.815s, 1256.27/s)  LR: 9.643e-04  Data: 0.009 (0.017)
Train: 73 [ 350/1251 ( 28%)]  Loss: 3.624 (3.64)  Time: 0.823s, 1243.64/s  (0.814s, 1257.76/s)  LR: 9.643e-04  Data: 0.010 (0.016)
Train: 73 [ 400/1251 ( 32%)]  Loss: 3.660 (3.64)  Time: 0.839s, 1221.03/s  (0.813s, 1259.10/s)  LR: 9.643e-04  Data: 0.010 (0.016)
Train: 73 [ 450/1251 ( 36%)]  Loss: 3.981 (3.68)  Time: 0.823s, 1243.73/s  (0.813s, 1259.81/s)  LR: 9.643e-04  Data: 0.010 (0.015)
Train: 73 [ 500/1251 ( 40%)]  Loss: 3.693 (3.68)  Time: 0.866s, 1181.96/s  (0.813s, 1259.48/s)  LR: 9.643e-04  Data: 0.010 (0.015)
Train: 73 [ 550/1251 ( 44%)]  Loss: 3.973 (3.70)  Time: 0.820s, 1249.37/s  (0.813s, 1259.29/s)  LR: 9.643e-04  Data: 0.010 (0.014)
Train: 73 [ 600/1251 ( 48%)]  Loss: 3.471 (3.69)  Time: 0.783s, 1308.41/s  (0.813s, 1260.13/s)  LR: 9.643e-04  Data: 0.009 (0.014)
Train: 73 [ 650/1251 ( 52%)]  Loss: 4.178 (3.72)  Time: 0.798s, 1282.89/s  (0.813s, 1260.20/s)  LR: 9.643e-04  Data: 0.016 (0.014)
Train: 73 [ 700/1251 ( 56%)]  Loss: 4.279 (3.76)  Time: 0.842s, 1215.45/s  (0.812s, 1261.05/s)  LR: 9.643e-04  Data: 0.009 (0.013)
Train: 73 [ 750/1251 ( 60%)]  Loss: 3.880 (3.77)  Time: 0.783s, 1307.43/s  (0.812s, 1261.53/s)  LR: 9.643e-04  Data: 0.010 (0.013)
Train: 73 [ 800/1251 ( 64%)]  Loss: 3.842 (3.77)  Time: 0.839s, 1219.81/s  (0.812s, 1261.08/s)  LR: 9.643e-04  Data: 0.010 (0.013)
Train: 73 [ 850/1251 ( 68%)]  Loss: 3.372 (3.75)  Time: 0.858s, 1194.07/s  (0.812s, 1261.36/s)  LR: 9.643e-04  Data: 0.009 (0.013)
Train: 73 [ 900/1251 ( 72%)]  Loss: 3.789 (3.75)  Time: 0.799s, 1281.69/s  (0.811s, 1262.25/s)  LR: 9.643e-04  Data: 0.010 (0.013)
Train: 73 [ 950/1251 ( 76%)]  Loss: 3.821 (3.75)  Time: 0.779s, 1313.98/s  (0.811s, 1262.94/s)  LR: 9.643e-04  Data: 0.010 (0.013)
Train: 73 [1000/1251 ( 80%)]  Loss: 4.022 (3.77)  Time: 0.783s, 1307.37/s  (0.810s, 1263.42/s)  LR: 9.643e-04  Data: 0.009 (0.013)
Train: 73 [1050/1251 ( 84%)]  Loss: 3.763 (3.77)  Time: 0.781s, 1311.67/s  (0.810s, 1263.76/s)  LR: 9.643e-04  Data: 0.011 (0.012)
Train: 73 [1100/1251 ( 88%)]  Loss: 3.632 (3.76)  Time: 0.804s, 1274.03/s  (0.810s, 1263.43/s)  LR: 9.643e-04  Data: 0.009 (0.012)
Train: 73 [1150/1251 ( 92%)]  Loss: 3.890 (3.77)  Time: 0.813s, 1260.09/s  (0.810s, 1263.48/s)  LR: 9.643e-04  Data: 0.010 (0.012)
Train: 73 [1200/1251 ( 96%)]  Loss: 3.537 (3.76)  Time: 0.938s, 1091.59/s  (0.811s, 1263.03/s)  LR: 9.643e-04  Data: 0.010 (0.012)
Train: 73 [1250/1251 (100%)]  Loss: 4.127 (3.77)  Time: 0.767s, 1335.86/s  (0.810s, 1263.57/s)  LR: 9.643e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.735 (1.735)  Loss:  0.9365 (0.9365)  Acc@1: 85.0586 (85.0586)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.194 (0.598)  Loss:  0.9922 (1.4883)  Acc@1: 82.0755 (70.7180)  Acc@5: 94.9293 (90.2600)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-73.pth.tar', 70.71800010009765)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-72.pth.tar', 70.7139999658203)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-69.pth.tar', 70.23999997314453)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-70.pth.tar', 70.23399999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-67.pth.tar', 70.22200009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-71.pth.tar', 70.14600002685548)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-63.pth.tar', 69.96200006835937)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-65.pth.tar', 69.8740000756836)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-66.pth.tar', 69.85199999023438)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-58.pth.tar', 69.84399996582032)

Train: 74 [   0/1251 (  0%)]  Loss: 4.177 (4.18)  Time: 2.416s,  423.87/s  (2.416s,  423.87/s)  LR: 9.633e-04  Data: 1.680 (1.680)
Train: 74 [  50/1251 (  4%)]  Loss: 3.660 (3.92)  Time: 0.812s, 1260.95/s  (0.840s, 1219.52/s)  LR: 9.633e-04  Data: 0.010 (0.046)
Train: 74 [ 100/1251 (  8%)]  Loss: 3.950 (3.93)  Time: 0.815s, 1256.12/s  (0.822s, 1245.86/s)  LR: 9.633e-04  Data: 0.009 (0.028)
Train: 74 [ 150/1251 ( 12%)]  Loss: 4.019 (3.95)  Time: 0.803s, 1275.53/s  (0.819s, 1250.47/s)  LR: 9.633e-04  Data: 0.010 (0.022)
Train: 74 [ 200/1251 ( 16%)]  Loss: 3.683 (3.90)  Time: 0.823s, 1244.75/s  (0.816s, 1255.32/s)  LR: 9.633e-04  Data: 0.009 (0.019)
Train: 74 [ 250/1251 ( 20%)]  Loss: 3.704 (3.87)  Time: 0.778s, 1316.81/s  (0.809s, 1265.59/s)  LR: 9.633e-04  Data: 0.010 (0.018)
Train: 74 [ 300/1251 ( 24%)]  Loss: 3.607 (3.83)  Time: 0.778s, 1316.43/s  (0.808s, 1267.92/s)  LR: 9.633e-04  Data: 0.010 (0.016)
Train: 74 [ 350/1251 ( 28%)]  Loss: 3.954 (3.84)  Time: 0.781s, 1311.78/s  (0.804s, 1274.33/s)  LR: 9.633e-04  Data: 0.010 (0.015)
Train: 74 [ 400/1251 ( 32%)]  Loss: 3.881 (3.85)  Time: 0.778s, 1315.57/s  (0.802s, 1276.07/s)  LR: 9.633e-04  Data: 0.010 (0.015)
Train: 74 [ 450/1251 ( 36%)]  Loss: 3.487 (3.81)  Time: 0.780s, 1312.20/s  (0.801s, 1278.53/s)  LR: 9.633e-04  Data: 0.009 (0.014)
Train: 74 [ 500/1251 ( 40%)]  Loss: 3.764 (3.81)  Time: 0.826s, 1240.10/s  (0.799s, 1281.01/s)  LR: 9.633e-04  Data: 0.010 (0.014)
Train: 74 [ 550/1251 ( 44%)]  Loss: 3.434 (3.78)  Time: 0.796s, 1287.22/s  (0.798s, 1283.20/s)  LR: 9.633e-04  Data: 0.009 (0.013)
Train: 74 [ 600/1251 ( 48%)]  Loss: 3.639 (3.77)  Time: 0.778s, 1316.03/s  (0.797s, 1285.37/s)  LR: 9.633e-04  Data: 0.009 (0.013)
Train: 74 [ 650/1251 ( 52%)]  Loss: 3.874 (3.77)  Time: 0.778s, 1316.49/s  (0.796s, 1286.38/s)  LR: 9.633e-04  Data: 0.010 (0.013)
Train: 74 [ 700/1251 ( 56%)]  Loss: 3.393 (3.75)  Time: 0.777s, 1318.58/s  (0.795s, 1288.11/s)  LR: 9.633e-04  Data: 0.010 (0.012)
Train: 74 [ 750/1251 ( 60%)]  Loss: 3.716 (3.75)  Time: 0.778s, 1316.26/s  (0.794s, 1289.95/s)  LR: 9.633e-04  Data: 0.010 (0.012)
Train: 74 [ 800/1251 ( 64%)]  Loss: 4.054 (3.76)  Time: 0.778s, 1316.19/s  (0.793s, 1291.49/s)  LR: 9.633e-04  Data: 0.009 (0.012)
Train: 74 [ 850/1251 ( 68%)]  Loss: 3.831 (3.77)  Time: 0.777s, 1318.01/s  (0.793s, 1291.47/s)  LR: 9.633e-04  Data: 0.009 (0.012)
Train: 74 [ 900/1251 ( 72%)]  Loss: 4.028 (3.78)  Time: 0.778s, 1316.94/s  (0.792s, 1292.52/s)  LR: 9.633e-04  Data: 0.010 (0.012)
Train: 74 [ 950/1251 ( 76%)]  Loss: 4.160 (3.80)  Time: 0.784s, 1306.80/s  (0.792s, 1292.31/s)  LR: 9.633e-04  Data: 0.009 (0.012)
Train: 74 [1000/1251 ( 80%)]  Loss: 3.996 (3.81)  Time: 0.777s, 1317.43/s  (0.792s, 1293.26/s)  LR: 9.633e-04  Data: 0.010 (0.012)
Train: 74 [1050/1251 ( 84%)]  Loss: 4.114 (3.82)  Time: 0.778s, 1316.84/s  (0.791s, 1293.75/s)  LR: 9.633e-04  Data: 0.009 (0.012)
Train: 74 [1100/1251 ( 88%)]  Loss: 3.755 (3.82)  Time: 0.778s, 1316.91/s  (0.791s, 1294.74/s)  LR: 9.633e-04  Data: 0.009 (0.011)
Train: 74 [1150/1251 ( 92%)]  Loss: 3.936 (3.83)  Time: 0.778s, 1316.89/s  (0.790s, 1295.63/s)  LR: 9.633e-04  Data: 0.010 (0.011)
Train: 74 [1200/1251 ( 96%)]  Loss: 3.607 (3.82)  Time: 0.824s, 1242.72/s  (0.792s, 1293.37/s)  LR: 9.633e-04  Data: 0.009 (0.011)
Train: 74 [1250/1251 (100%)]  Loss: 3.525 (3.81)  Time: 0.764s, 1340.42/s  (0.793s, 1291.93/s)  LR: 9.633e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.589 (1.589)  Loss:  0.9180 (0.9180)  Acc@1: 86.8164 (86.8164)  Acc@5: 95.9961 (95.9961)
Test: [  48/48]  Time: 0.194 (0.561)  Loss:  0.9771 (1.4738)  Acc@1: 82.3113 (70.5540)  Acc@5: 95.2830 (90.2560)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-73.pth.tar', 70.71800010009765)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-72.pth.tar', 70.7139999658203)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-74.pth.tar', 70.5540000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-69.pth.tar', 70.23999997314453)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-70.pth.tar', 70.23399999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-67.pth.tar', 70.22200009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-71.pth.tar', 70.14600002685548)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-63.pth.tar', 69.96200006835937)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-65.pth.tar', 69.8740000756836)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-66.pth.tar', 69.85199999023438)

Train: 75 [   0/1251 (  0%)]  Loss: 3.939 (3.94)  Time: 2.245s,  456.09/s  (2.245s,  456.09/s)  LR: 9.623e-04  Data: 1.521 (1.521)
Train: 75 [  50/1251 (  4%)]  Loss: 4.147 (4.04)  Time: 0.782s, 1309.88/s  (0.820s, 1248.66/s)  LR: 9.623e-04  Data: 0.010 (0.041)
Train: 75 [ 100/1251 (  8%)]  Loss: 4.170 (4.09)  Time: 0.813s, 1259.44/s  (0.813s, 1260.17/s)  LR: 9.623e-04  Data: 0.011 (0.026)
Train: 75 [ 150/1251 ( 12%)]  Loss: 3.968 (4.06)  Time: 0.812s, 1260.59/s  (0.812s, 1260.58/s)  LR: 9.623e-04  Data: 0.011 (0.021)
Train: 75 [ 200/1251 ( 16%)]  Loss: 3.805 (4.01)  Time: 0.874s, 1171.69/s  (0.819s, 1250.49/s)  LR: 9.623e-04  Data: 0.015 (0.019)
Train: 75 [ 250/1251 ( 20%)]  Loss: 3.517 (3.92)  Time: 0.823s, 1244.25/s  (0.817s, 1253.78/s)  LR: 9.623e-04  Data: 0.010 (0.018)
Train: 75 [ 300/1251 ( 24%)]  Loss: 4.026 (3.94)  Time: 0.824s, 1242.94/s  (0.813s, 1259.78/s)  LR: 9.623e-04  Data: 0.009 (0.016)
Train: 75 [ 350/1251 ( 28%)]  Loss: 3.637 (3.90)  Time: 0.823s, 1244.19/s  (0.812s, 1261.13/s)  LR: 9.623e-04  Data: 0.010 (0.015)
Train: 75 [ 400/1251 ( 32%)]  Loss: 3.792 (3.89)  Time: 0.837s, 1223.71/s  (0.812s, 1261.54/s)  LR: 9.623e-04  Data: 0.009 (0.015)
Train: 75 [ 450/1251 ( 36%)]  Loss: 3.903 (3.89)  Time: 0.833s, 1229.68/s  (0.812s, 1260.75/s)  LR: 9.623e-04  Data: 0.009 (0.014)
Train: 75 [ 500/1251 ( 40%)]  Loss: 3.462 (3.85)  Time: 0.904s, 1132.30/s  (0.811s, 1261.92/s)  LR: 9.623e-04  Data: 0.009 (0.014)
Train: 75 [ 550/1251 ( 44%)]  Loss: 3.472 (3.82)  Time: 0.877s, 1167.13/s  (0.812s, 1261.57/s)  LR: 9.623e-04  Data: 0.014 (0.014)
Train: 75 [ 600/1251 ( 48%)]  Loss: 4.066 (3.84)  Time: 0.782s, 1309.63/s  (0.811s, 1262.44/s)  LR: 9.623e-04  Data: 0.009 (0.013)
Train: 75 [ 650/1251 ( 52%)]  Loss: 3.518 (3.82)  Time: 0.914s, 1119.95/s  (0.811s, 1263.14/s)  LR: 9.623e-04  Data: 0.013 (0.013)
Train: 75 [ 700/1251 ( 56%)]  Loss: 3.780 (3.81)  Time: 0.892s, 1148.30/s  (0.810s, 1263.60/s)  LR: 9.623e-04  Data: 0.009 (0.013)
Train: 75 [ 750/1251 ( 60%)]  Loss: 3.586 (3.80)  Time: 0.788s, 1299.90/s  (0.810s, 1263.88/s)  LR: 9.623e-04  Data: 0.010 (0.013)
Train: 75 [ 800/1251 ( 64%)]  Loss: 3.878 (3.80)  Time: 0.812s, 1260.85/s  (0.810s, 1263.93/s)  LR: 9.623e-04  Data: 0.009 (0.013)
Train: 75 [ 850/1251 ( 68%)]  Loss: 3.549 (3.79)  Time: 0.841s, 1217.80/s  (0.810s, 1264.13/s)  LR: 9.623e-04  Data: 0.010 (0.013)
Train: 75 [ 900/1251 ( 72%)]  Loss: 3.809 (3.79)  Time: 0.799s, 1281.94/s  (0.810s, 1264.06/s)  LR: 9.623e-04  Data: 0.009 (0.012)
Train: 75 [ 950/1251 ( 76%)]  Loss: 3.554 (3.78)  Time: 0.830s, 1233.75/s  (0.810s, 1264.58/s)  LR: 9.623e-04  Data: 0.009 (0.012)
Train: 75 [1000/1251 ( 80%)]  Loss: 4.045 (3.79)  Time: 0.830s, 1233.22/s  (0.810s, 1264.39/s)  LR: 9.623e-04  Data: 0.010 (0.012)
Train: 75 [1050/1251 ( 84%)]  Loss: 3.634 (3.78)  Time: 0.823s, 1244.82/s  (0.810s, 1264.39/s)  LR: 9.623e-04  Data: 0.010 (0.012)
Train: 75 [1100/1251 ( 88%)]  Loss: 3.953 (3.79)  Time: 0.797s, 1284.51/s  (0.810s, 1264.70/s)  LR: 9.623e-04  Data: 0.010 (0.012)
Train: 75 [1150/1251 ( 92%)]  Loss: 3.653 (3.79)  Time: 0.785s, 1304.57/s  (0.810s, 1264.35/s)  LR: 9.623e-04  Data: 0.010 (0.012)
Train: 75 [1200/1251 ( 96%)]  Loss: 3.627 (3.78)  Time: 0.777s, 1317.73/s  (0.810s, 1264.88/s)  LR: 9.623e-04  Data: 0.011 (0.012)
Train: 75 [1250/1251 (100%)]  Loss: 3.669 (3.78)  Time: 0.818s, 1251.12/s  (0.810s, 1264.59/s)  LR: 9.623e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.709 (1.709)  Loss:  0.8315 (0.8315)  Acc@1: 87.2070 (87.2070)  Acc@5: 96.1914 (96.1914)
Test: [  48/48]  Time: 0.194 (0.614)  Loss:  0.9771 (1.4823)  Acc@1: 82.7830 (70.3100)  Acc@5: 95.8726 (90.2640)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-73.pth.tar', 70.71800010009765)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-72.pth.tar', 70.7139999658203)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-74.pth.tar', 70.5540000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-75.pth.tar', 70.31000001953124)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-69.pth.tar', 70.23999997314453)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-70.pth.tar', 70.23399999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-67.pth.tar', 70.22200009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-71.pth.tar', 70.14600002685548)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-63.pth.tar', 69.96200006835937)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-65.pth.tar', 69.8740000756836)

Train: 76 [   0/1251 (  0%)]  Loss: 3.964 (3.96)  Time: 2.544s,  402.59/s  (2.544s,  402.59/s)  LR: 9.613e-04  Data: 1.814 (1.814)
Train: 76 [  50/1251 (  4%)]  Loss: 3.607 (3.79)  Time: 0.774s, 1322.86/s  (0.845s, 1212.45/s)  LR: 9.613e-04  Data: 0.011 (0.047)
Train: 76 [ 100/1251 (  8%)]  Loss: 3.626 (3.73)  Time: 0.840s, 1218.99/s  (0.828s, 1236.23/s)  LR: 9.613e-04  Data: 0.010 (0.029)
Train: 76 [ 150/1251 ( 12%)]  Loss: 3.386 (3.65)  Time: 0.782s, 1308.67/s  (0.820s, 1248.02/s)  LR: 9.613e-04  Data: 0.010 (0.023)
Train: 76 [ 200/1251 ( 16%)]  Loss: 3.814 (3.68)  Time: 0.789s, 1297.42/s  (0.821s, 1247.93/s)  LR: 9.613e-04  Data: 0.013 (0.020)
Train: 76 [ 250/1251 ( 20%)]  Loss: 3.691 (3.68)  Time: 0.827s, 1238.46/s  (0.819s, 1250.43/s)  LR: 9.613e-04  Data: 0.009 (0.018)
Train: 76 [ 300/1251 ( 24%)]  Loss: 3.844 (3.70)  Time: 0.824s, 1242.13/s  (0.817s, 1252.70/s)  LR: 9.613e-04  Data: 0.017 (0.017)
Train: 76 [ 350/1251 ( 28%)]  Loss: 3.884 (3.73)  Time: 0.778s, 1315.80/s  (0.816s, 1254.71/s)  LR: 9.613e-04  Data: 0.010 (0.016)
Train: 76 [ 400/1251 ( 32%)]  Loss: 3.723 (3.73)  Time: 0.777s, 1318.66/s  (0.816s, 1255.07/s)  LR: 9.613e-04  Data: 0.009 (0.015)
Train: 76 [ 450/1251 ( 36%)]  Loss: 3.814 (3.74)  Time: 0.788s, 1299.43/s  (0.815s, 1256.26/s)  LR: 9.613e-04  Data: 0.010 (0.015)
Train: 76 [ 500/1251 ( 40%)]  Loss: 3.605 (3.72)  Time: 0.800s, 1280.36/s  (0.814s, 1257.42/s)  LR: 9.613e-04  Data: 0.011 (0.014)
Train: 76 [ 550/1251 ( 44%)]  Loss: 4.128 (3.76)  Time: 0.812s, 1261.11/s  (0.814s, 1258.23/s)  LR: 9.613e-04  Data: 0.010 (0.014)
Train: 76 [ 600/1251 ( 48%)]  Loss: 4.075 (3.78)  Time: 0.797s, 1284.61/s  (0.813s, 1259.25/s)  LR: 9.613e-04  Data: 0.010 (0.014)
Train: 76 [ 650/1251 ( 52%)]  Loss: 3.401 (3.75)  Time: 0.796s, 1286.25/s  (0.813s, 1259.58/s)  LR: 9.613e-04  Data: 0.011 (0.014)
Train: 76 [ 700/1251 ( 56%)]  Loss: 3.749 (3.75)  Time: 0.826s, 1240.42/s  (0.813s, 1260.29/s)  LR: 9.613e-04  Data: 0.009 (0.013)
Train: 76 [ 750/1251 ( 60%)]  Loss: 3.651 (3.75)  Time: 0.789s, 1297.66/s  (0.812s, 1260.46/s)  LR: 9.613e-04  Data: 0.011 (0.013)
Train: 76 [ 800/1251 ( 64%)]  Loss: 3.919 (3.76)  Time: 0.818s, 1251.80/s  (0.812s, 1260.41/s)  LR: 9.613e-04  Data: 0.014 (0.013)
Train: 76 [ 850/1251 ( 68%)]  Loss: 3.583 (3.75)  Time: 0.786s, 1303.09/s  (0.813s, 1260.15/s)  LR: 9.613e-04  Data: 0.010 (0.013)
Train: 76 [ 900/1251 ( 72%)]  Loss: 3.676 (3.74)  Time: 0.778s, 1316.28/s  (0.812s, 1260.39/s)  LR: 9.613e-04  Data: 0.010 (0.013)
Train: 76 [ 950/1251 ( 76%)]  Loss: 3.412 (3.73)  Time: 0.841s, 1217.54/s  (0.812s, 1260.32/s)  LR: 9.613e-04  Data: 0.009 (0.013)
Train: 76 [1000/1251 ( 80%)]  Loss: 4.132 (3.75)  Time: 0.870s, 1177.22/s  (0.813s, 1260.23/s)  LR: 9.613e-04  Data: 0.010 (0.013)
Train: 76 [1050/1251 ( 84%)]  Loss: 3.502 (3.74)  Time: 0.780s, 1312.33/s  (0.813s, 1259.95/s)  LR: 9.613e-04  Data: 0.010 (0.012)
Train: 76 [1100/1251 ( 88%)]  Loss: 4.137 (3.75)  Time: 0.792s, 1292.70/s  (0.813s, 1260.28/s)  LR: 9.613e-04  Data: 0.016 (0.012)
Train: 76 [1150/1251 ( 92%)]  Loss: 3.618 (3.75)  Time: 0.816s, 1254.19/s  (0.812s, 1260.90/s)  LR: 9.613e-04  Data: 0.010 (0.012)
Train: 76 [1200/1251 ( 96%)]  Loss: 3.623 (3.74)  Time: 0.827s, 1238.76/s  (0.812s, 1261.19/s)  LR: 9.613e-04  Data: 0.010 (0.012)
Train: 76 [1250/1251 (100%)]  Loss: 3.524 (3.73)  Time: 0.818s, 1251.31/s  (0.812s, 1261.33/s)  LR: 9.613e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.667 (1.667)  Loss:  1.0342 (1.0342)  Acc@1: 83.9844 (83.9844)  Acc@5: 94.6289 (94.6289)
Test: [  48/48]  Time: 0.194 (0.605)  Loss:  1.0098 (1.5150)  Acc@1: 81.2500 (70.2260)  Acc@5: 94.2217 (90.2540)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-73.pth.tar', 70.71800010009765)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-72.pth.tar', 70.7139999658203)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-74.pth.tar', 70.5540000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-75.pth.tar', 70.31000001953124)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-69.pth.tar', 70.23999997314453)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-70.pth.tar', 70.23399999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-76.pth.tar', 70.226)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-67.pth.tar', 70.22200009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-71.pth.tar', 70.14600002685548)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-63.pth.tar', 69.96200006835937)

Train: 77 [   0/1251 (  0%)]  Loss: 3.848 (3.85)  Time: 2.503s,  409.16/s  (2.503s,  409.16/s)  LR: 9.603e-04  Data: 1.769 (1.769)
Train: 77 [  50/1251 (  4%)]  Loss: 3.825 (3.84)  Time: 0.805s, 1271.58/s  (0.847s, 1208.42/s)  LR: 9.603e-04  Data: 0.009 (0.049)
Train: 77 [ 100/1251 (  8%)]  Loss: 3.787 (3.82)  Time: 0.776s, 1320.01/s  (0.829s, 1235.15/s)  LR: 9.603e-04  Data: 0.010 (0.030)
Train: 77 [ 150/1251 ( 12%)]  Loss: 4.214 (3.92)  Time: 0.783s, 1307.65/s  (0.823s, 1244.93/s)  LR: 9.603e-04  Data: 0.013 (0.024)
Train: 77 [ 200/1251 ( 16%)]  Loss: 3.669 (3.87)  Time: 0.800s, 1279.45/s  (0.820s, 1249.13/s)  LR: 9.603e-04  Data: 0.009 (0.020)
Train: 77 [ 250/1251 ( 20%)]  Loss: 4.003 (3.89)  Time: 0.773s, 1324.30/s  (0.817s, 1252.77/s)  LR: 9.603e-04  Data: 0.013 (0.018)
Train: 77 [ 300/1251 ( 24%)]  Loss: 3.616 (3.85)  Time: 0.809s, 1265.94/s  (0.816s, 1255.64/s)  LR: 9.603e-04  Data: 0.011 (0.017)
Train: 77 [ 350/1251 ( 28%)]  Loss: 3.970 (3.87)  Time: 0.782s, 1310.08/s  (0.814s, 1257.37/s)  LR: 9.603e-04  Data: 0.014 (0.016)
Train: 77 [ 400/1251 ( 32%)]  Loss: 3.812 (3.86)  Time: 0.868s, 1179.28/s  (0.814s, 1257.98/s)  LR: 9.603e-04  Data: 0.009 (0.016)
Train: 77 [ 450/1251 ( 36%)]  Loss: 4.321 (3.91)  Time: 0.794s, 1289.24/s  (0.814s, 1258.58/s)  LR: 9.603e-04  Data: 0.010 (0.015)
Train: 77 [ 500/1251 ( 40%)]  Loss: 3.678 (3.89)  Time: 0.796s, 1286.80/s  (0.813s, 1259.19/s)  LR: 9.603e-04  Data: 0.010 (0.015)
Train: 77 [ 550/1251 ( 44%)]  Loss: 3.631 (3.86)  Time: 0.850s, 1204.59/s  (0.813s, 1259.86/s)  LR: 9.603e-04  Data: 0.010 (0.014)
Train: 77 [ 600/1251 ( 48%)]  Loss: 3.620 (3.85)  Time: 0.800s, 1280.60/s  (0.812s, 1260.88/s)  LR: 9.603e-04  Data: 0.009 (0.014)
Train: 77 [ 650/1251 ( 52%)]  Loss: 3.724 (3.84)  Time: 0.860s, 1190.79/s  (0.812s, 1261.31/s)  LR: 9.603e-04  Data: 0.015 (0.014)
Train: 77 [ 700/1251 ( 56%)]  Loss: 3.752 (3.83)  Time: 0.791s, 1294.47/s  (0.811s, 1261.88/s)  LR: 9.603e-04  Data: 0.014 (0.013)
Train: 77 [ 750/1251 ( 60%)]  Loss: 3.597 (3.82)  Time: 0.854s, 1198.37/s  (0.811s, 1261.93/s)  LR: 9.603e-04  Data: 0.017 (0.013)
Train: 77 [ 800/1251 ( 64%)]  Loss: 3.411 (3.79)  Time: 0.811s, 1263.36/s  (0.811s, 1262.21/s)  LR: 9.603e-04  Data: 0.010 (0.013)
Train: 77 [ 850/1251 ( 68%)]  Loss: 3.576 (3.78)  Time: 0.786s, 1302.84/s  (0.811s, 1262.40/s)  LR: 9.603e-04  Data: 0.010 (0.013)
Train: 77 [ 900/1251 ( 72%)]  Loss: 4.082 (3.80)  Time: 0.782s, 1309.40/s  (0.811s, 1262.85/s)  LR: 9.603e-04  Data: 0.010 (0.013)
Train: 77 [ 950/1251 ( 76%)]  Loss: 3.946 (3.80)  Time: 0.801s, 1278.62/s  (0.811s, 1263.30/s)  LR: 9.603e-04  Data: 0.009 (0.013)
Train: 77 [1000/1251 ( 80%)]  Loss: 3.693 (3.80)  Time: 0.795s, 1287.99/s  (0.810s, 1263.43/s)  LR: 9.603e-04  Data: 0.010 (0.013)
Train: 77 [1050/1251 ( 84%)]  Loss: 3.713 (3.79)  Time: 0.814s, 1257.59/s  (0.810s, 1263.44/s)  LR: 9.603e-04  Data: 0.009 (0.012)
Train: 77 [1100/1251 ( 88%)]  Loss: 3.485 (3.78)  Time: 0.857s, 1194.88/s  (0.810s, 1263.50/s)  LR: 9.603e-04  Data: 0.009 (0.012)
Train: 77 [1150/1251 ( 92%)]  Loss: 3.859 (3.78)  Time: 0.832s, 1230.74/s  (0.811s, 1263.22/s)  LR: 9.603e-04  Data: 0.010 (0.012)
Train: 77 [1200/1251 ( 96%)]  Loss: 3.878 (3.79)  Time: 0.882s, 1161.63/s  (0.810s, 1263.47/s)  LR: 9.603e-04  Data: 0.010 (0.012)
Train: 77 [1250/1251 (100%)]  Loss: 3.877 (3.79)  Time: 0.796s, 1287.10/s  (0.810s, 1263.70/s)  LR: 9.603e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.652 (1.652)  Loss:  0.8291 (0.8291)  Acc@1: 87.6953 (87.6953)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.194 (0.605)  Loss:  0.8013 (1.4170)  Acc@1: 83.4906 (70.8360)  Acc@5: 95.5189 (90.2520)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-77.pth.tar', 70.83599993896485)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-73.pth.tar', 70.71800010009765)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-72.pth.tar', 70.7139999658203)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-74.pth.tar', 70.5540000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-75.pth.tar', 70.31000001953124)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-69.pth.tar', 70.23999997314453)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-70.pth.tar', 70.23399999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-76.pth.tar', 70.226)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-67.pth.tar', 70.22200009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-71.pth.tar', 70.14600002685548)

Train: 78 [   0/1251 (  0%)]  Loss: 3.508 (3.51)  Time: 2.429s,  421.59/s  (2.429s,  421.59/s)  LR: 9.593e-04  Data: 1.656 (1.656)
Train: 78 [  50/1251 (  4%)]  Loss: 4.070 (3.79)  Time: 0.777s, 1317.88/s  (0.856s, 1195.65/s)  LR: 9.593e-04  Data: 0.010 (0.050)
Train: 78 [ 100/1251 (  8%)]  Loss: 4.140 (3.91)  Time: 0.807s, 1269.32/s  (0.832s, 1230.15/s)  LR: 9.593e-04  Data: 0.009 (0.030)
Train: 78 [ 150/1251 ( 12%)]  Loss: 3.723 (3.86)  Time: 0.815s, 1256.74/s  (0.823s, 1244.59/s)  LR: 9.593e-04  Data: 0.011 (0.024)
Train: 78 [ 200/1251 ( 16%)]  Loss: 3.410 (3.77)  Time: 0.776s, 1319.76/s  (0.820s, 1248.72/s)  LR: 9.593e-04  Data: 0.010 (0.021)
Train: 78 [ 250/1251 ( 20%)]  Loss: 3.741 (3.77)  Time: 0.791s, 1294.42/s  (0.817s, 1252.81/s)  LR: 9.593e-04  Data: 0.010 (0.019)
Train: 78 [ 300/1251 ( 24%)]  Loss: 4.035 (3.80)  Time: 0.814s, 1258.02/s  (0.816s, 1255.08/s)  LR: 9.593e-04  Data: 0.011 (0.017)
Train: 78 [ 350/1251 ( 28%)]  Loss: 3.606 (3.78)  Time: 0.793s, 1291.86/s  (0.815s, 1256.95/s)  LR: 9.593e-04  Data: 0.012 (0.016)
Train: 78 [ 400/1251 ( 32%)]  Loss: 3.661 (3.77)  Time: 0.830s, 1233.51/s  (0.814s, 1258.68/s)  LR: 9.593e-04  Data: 0.010 (0.016)
Train: 78 [ 450/1251 ( 36%)]  Loss: 3.796 (3.77)  Time: 0.815s, 1256.30/s  (0.813s, 1259.03/s)  LR: 9.593e-04  Data: 0.011 (0.015)
Train: 78 [ 500/1251 ( 40%)]  Loss: 3.969 (3.79)  Time: 0.852s, 1201.86/s  (0.813s, 1260.08/s)  LR: 9.593e-04  Data: 0.014 (0.015)
Train: 78 [ 550/1251 ( 44%)]  Loss: 3.712 (3.78)  Time: 0.776s, 1320.08/s  (0.812s, 1260.58/s)  LR: 9.593e-04  Data: 0.010 (0.014)
Train: 78 [ 600/1251 ( 48%)]  Loss: 3.468 (3.76)  Time: 0.847s, 1209.26/s  (0.813s, 1259.40/s)  LR: 9.593e-04  Data: 0.009 (0.014)
Train: 78 [ 650/1251 ( 52%)]  Loss: 3.774 (3.76)  Time: 0.826s, 1240.25/s  (0.813s, 1259.68/s)  LR: 9.593e-04  Data: 0.010 (0.014)
Train: 78 [ 700/1251 ( 56%)]  Loss: 3.683 (3.75)  Time: 0.788s, 1298.85/s  (0.813s, 1260.05/s)  LR: 9.593e-04  Data: 0.016 (0.014)
Train: 78 [ 750/1251 ( 60%)]  Loss: 3.744 (3.75)  Time: 0.785s, 1305.16/s  (0.812s, 1261.00/s)  LR: 9.593e-04  Data: 0.010 (0.013)
Train: 78 [ 800/1251 ( 64%)]  Loss: 3.950 (3.76)  Time: 0.826s, 1239.79/s  (0.812s, 1261.17/s)  LR: 9.593e-04  Data: 0.010 (0.013)
Train: 78 [ 850/1251 ( 68%)]  Loss: 3.417 (3.74)  Time: 0.800s, 1280.21/s  (0.812s, 1261.76/s)  LR: 9.593e-04  Data: 0.014 (0.013)
Train: 78 [ 900/1251 ( 72%)]  Loss: 4.075 (3.76)  Time: 0.783s, 1308.09/s  (0.812s, 1261.14/s)  LR: 9.593e-04  Data: 0.017 (0.013)
Train: 78 [ 950/1251 ( 76%)]  Loss: 3.877 (3.77)  Time: 0.783s, 1307.69/s  (0.812s, 1261.05/s)  LR: 9.593e-04  Data: 0.010 (0.013)
Train: 78 [1000/1251 ( 80%)]  Loss: 3.737 (3.77)  Time: 0.810s, 1263.68/s  (0.812s, 1261.02/s)  LR: 9.593e-04  Data: 0.010 (0.013)
Train: 78 [1050/1251 ( 84%)]  Loss: 3.762 (3.77)  Time: 0.810s, 1264.63/s  (0.812s, 1260.79/s)  LR: 9.593e-04  Data: 0.010 (0.013)
Train: 78 [1100/1251 ( 88%)]  Loss: 3.834 (3.77)  Time: 0.819s, 1249.85/s  (0.812s, 1261.24/s)  LR: 9.593e-04  Data: 0.009 (0.013)
Train: 78 [1150/1251 ( 92%)]  Loss: 3.673 (3.77)  Time: 0.832s, 1230.06/s  (0.812s, 1261.43/s)  LR: 9.593e-04  Data: 0.014 (0.012)
Train: 78 [1200/1251 ( 96%)]  Loss: 3.862 (3.77)  Time: 0.780s, 1312.37/s  (0.811s, 1261.93/s)  LR: 9.593e-04  Data: 0.010 (0.012)
Train: 78 [1250/1251 (100%)]  Loss: 3.591 (3.76)  Time: 0.799s, 1281.50/s  (0.811s, 1262.53/s)  LR: 9.593e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.717 (1.717)  Loss:  0.8696 (0.8696)  Acc@1: 88.0859 (88.0859)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.194 (0.604)  Loss:  0.9233 (1.5664)  Acc@1: 82.6651 (70.2360)  Acc@5: 94.2217 (90.1980)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-77.pth.tar', 70.83599993896485)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-73.pth.tar', 70.71800010009765)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-72.pth.tar', 70.7139999658203)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-74.pth.tar', 70.5540000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-75.pth.tar', 70.31000001953124)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-69.pth.tar', 70.23999997314453)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-78.pth.tar', 70.23600009765624)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-70.pth.tar', 70.23399999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-76.pth.tar', 70.226)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-67.pth.tar', 70.22200009277344)

Train: 79 [   0/1251 (  0%)]  Loss: 3.518 (3.52)  Time: 2.383s,  429.72/s  (2.383s,  429.72/s)  LR: 9.583e-04  Data: 1.632 (1.632)
Train: 79 [  50/1251 (  4%)]  Loss: 3.635 (3.58)  Time: 0.791s, 1295.24/s  (0.845s, 1211.57/s)  LR: 9.583e-04  Data: 0.009 (0.047)
Train: 79 [ 100/1251 (  8%)]  Loss: 3.508 (3.55)  Time: 0.822s, 1245.52/s  (0.829s, 1234.91/s)  LR: 9.583e-04  Data: 0.009 (0.029)
Train: 79 [ 150/1251 ( 12%)]  Loss: 3.740 (3.60)  Time: 0.807s, 1269.60/s  (0.824s, 1243.37/s)  LR: 9.583e-04  Data: 0.012 (0.023)
Train: 79 [ 200/1251 ( 16%)]  Loss: 4.239 (3.73)  Time: 0.846s, 1210.81/s  (0.820s, 1249.31/s)  LR: 9.583e-04  Data: 0.009 (0.020)
Train: 79 [ 250/1251 ( 20%)]  Loss: 3.836 (3.75)  Time: 0.822s, 1246.07/s  (0.817s, 1253.76/s)  LR: 9.583e-04  Data: 0.009 (0.018)
Train: 79 [ 300/1251 ( 24%)]  Loss: 3.478 (3.71)  Time: 0.787s, 1300.51/s  (0.815s, 1256.33/s)  LR: 9.583e-04  Data: 0.013 (0.017)
Train: 79 [ 350/1251 ( 28%)]  Loss: 4.281 (3.78)  Time: 0.834s, 1228.08/s  (0.814s, 1258.03/s)  LR: 9.583e-04  Data: 0.010 (0.016)
Train: 79 [ 400/1251 ( 32%)]  Loss: 3.749 (3.78)  Time: 0.788s, 1299.57/s  (0.813s, 1259.49/s)  LR: 9.583e-04  Data: 0.009 (0.015)
Train: 79 [ 450/1251 ( 36%)]  Loss: 3.903 (3.79)  Time: 0.853s, 1200.78/s  (0.812s, 1261.10/s)  LR: 9.583e-04  Data: 0.011 (0.015)
Train: 79 [ 500/1251 ( 40%)]  Loss: 3.814 (3.79)  Time: 0.866s, 1181.91/s  (0.812s, 1261.21/s)  LR: 9.583e-04  Data: 0.015 (0.014)
Train: 79 [ 550/1251 ( 44%)]  Loss: 4.018 (3.81)  Time: 0.825s, 1241.36/s  (0.812s, 1260.56/s)  LR: 9.583e-04  Data: 0.009 (0.014)
Train: 79 [ 600/1251 ( 48%)]  Loss: 3.854 (3.81)  Time: 0.808s, 1266.94/s  (0.812s, 1261.26/s)  LR: 9.583e-04  Data: 0.010 (0.013)
Train: 79 [ 650/1251 ( 52%)]  Loss: 3.288 (3.78)  Time: 0.773s, 1324.72/s  (0.812s, 1261.70/s)  LR: 9.583e-04  Data: 0.010 (0.013)
Train: 79 [ 700/1251 ( 56%)]  Loss: 3.926 (3.79)  Time: 0.847s, 1209.54/s  (0.812s, 1261.84/s)  LR: 9.583e-04  Data: 0.010 (0.013)
Train: 79 [ 750/1251 ( 60%)]  Loss: 3.829 (3.79)  Time: 0.908s, 1127.83/s  (0.812s, 1261.70/s)  LR: 9.583e-04  Data: 0.013 (0.013)
Train: 79 [ 800/1251 ( 64%)]  Loss: 3.955 (3.80)  Time: 0.771s, 1328.02/s  (0.812s, 1261.81/s)  LR: 9.583e-04  Data: 0.009 (0.013)
Train: 79 [ 850/1251 ( 68%)]  Loss: 3.816 (3.80)  Time: 0.807s, 1268.35/s  (0.811s, 1262.41/s)  LR: 9.583e-04  Data: 0.015 (0.013)
Train: 79 [ 900/1251 ( 72%)]  Loss: 3.947 (3.81)  Time: 0.846s, 1210.62/s  (0.811s, 1262.64/s)  LR: 9.583e-04  Data: 0.009 (0.013)
Train: 79 [ 950/1251 ( 76%)]  Loss: 3.660 (3.80)  Time: 0.813s, 1259.55/s  (0.811s, 1262.85/s)  LR: 9.583e-04  Data: 0.010 (0.012)
Train: 79 [1000/1251 ( 80%)]  Loss: 4.210 (3.82)  Time: 0.770s, 1329.91/s  (0.811s, 1263.06/s)  LR: 9.583e-04  Data: 0.010 (0.012)
Train: 79 [1050/1251 ( 84%)]  Loss: 3.931 (3.82)  Time: 0.798s, 1283.48/s  (0.811s, 1263.02/s)  LR: 9.583e-04  Data: 0.010 (0.012)
Train: 79 [1100/1251 ( 88%)]  Loss: 3.821 (3.82)  Time: 0.816s, 1255.29/s  (0.811s, 1263.26/s)  LR: 9.583e-04  Data: 0.011 (0.012)
Train: 79 [1150/1251 ( 92%)]  Loss: 3.925 (3.83)  Time: 0.908s, 1127.81/s  (0.812s, 1261.77/s)  LR: 9.583e-04  Data: 0.011 (0.012)
Train: 79 [1200/1251 ( 96%)]  Loss: 3.730 (3.82)  Time: 0.779s, 1314.77/s  (0.812s, 1261.06/s)  LR: 9.583e-04  Data: 0.010 (0.012)
Train: 79 [1250/1251 (100%)]  Loss: 4.296 (3.84)  Time: 0.811s, 1262.26/s  (0.812s, 1261.40/s)  LR: 9.583e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.635 (1.635)  Loss:  0.7319 (0.7319)  Acc@1: 87.6953 (87.6953)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.194 (0.595)  Loss:  0.8711 (1.4199)  Acc@1: 83.4906 (70.5280)  Acc@5: 95.2830 (90.3560)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-77.pth.tar', 70.83599993896485)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-73.pth.tar', 70.71800010009765)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-72.pth.tar', 70.7139999658203)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-74.pth.tar', 70.5540000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-79.pth.tar', 70.52800006835938)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-75.pth.tar', 70.31000001953124)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-69.pth.tar', 70.23999997314453)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-78.pth.tar', 70.23600009765624)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-70.pth.tar', 70.23399999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-76.pth.tar', 70.226)

Train: 80 [   0/1251 (  0%)]  Loss: 3.787 (3.79)  Time: 2.428s,  421.81/s  (2.428s,  421.81/s)  LR: 9.572e-04  Data: 1.708 (1.708)
Train: 80 [  50/1251 (  4%)]  Loss: 3.736 (3.76)  Time: 0.779s, 1314.64/s  (0.841s, 1217.00/s)  LR: 9.572e-04  Data: 0.010 (0.049)
Train: 80 [ 100/1251 (  8%)]  Loss: 3.887 (3.80)  Time: 0.809s, 1265.96/s  (0.826s, 1239.55/s)  LR: 9.572e-04  Data: 0.017 (0.030)
Train: 80 [ 150/1251 ( 12%)]  Loss: 3.823 (3.81)  Time: 0.840s, 1219.53/s  (0.820s, 1249.13/s)  LR: 9.572e-04  Data: 0.010 (0.024)
Train: 80 [ 200/1251 ( 16%)]  Loss: 3.912 (3.83)  Time: 0.808s, 1267.79/s  (0.818s, 1251.36/s)  LR: 9.572e-04  Data: 0.010 (0.020)
Train: 80 [ 250/1251 ( 20%)]  Loss: 3.965 (3.85)  Time: 0.854s, 1199.71/s  (0.817s, 1253.00/s)  LR: 9.572e-04  Data: 0.009 (0.018)
Train: 80 [ 300/1251 ( 24%)]  Loss: 3.663 (3.82)  Time: 0.801s, 1279.05/s  (0.816s, 1254.60/s)  LR: 9.572e-04  Data: 0.015 (0.017)
Train: 80 [ 350/1251 ( 28%)]  Loss: 3.964 (3.84)  Time: 0.784s, 1306.17/s  (0.815s, 1257.17/s)  LR: 9.572e-04  Data: 0.014 (0.016)
Train: 80 [ 400/1251 ( 32%)]  Loss: 4.049 (3.87)  Time: 0.781s, 1311.28/s  (0.813s, 1259.33/s)  LR: 9.572e-04  Data: 0.010 (0.015)
Train: 80 [ 450/1251 ( 36%)]  Loss: 3.832 (3.86)  Time: 0.846s, 1209.89/s  (0.813s, 1259.42/s)  LR: 9.572e-04  Data: 0.010 (0.015)
Train: 80 [ 500/1251 ( 40%)]  Loss: 3.735 (3.85)  Time: 0.826s, 1238.96/s  (0.813s, 1259.63/s)  LR: 9.572e-04  Data: 0.009 (0.015)
Train: 80 [ 550/1251 ( 44%)]  Loss: 3.698 (3.84)  Time: 0.816s, 1254.96/s  (0.813s, 1259.56/s)  LR: 9.572e-04  Data: 0.009 (0.014)
Train: 80 [ 600/1251 ( 48%)]  Loss: 3.947 (3.85)  Time: 0.834s, 1228.31/s  (0.813s, 1259.91/s)  LR: 9.572e-04  Data: 0.010 (0.014)
Train: 80 [ 650/1251 ( 52%)]  Loss: 3.685 (3.83)  Time: 0.795s, 1287.46/s  (0.813s, 1259.96/s)  LR: 9.572e-04  Data: 0.009 (0.014)
Train: 80 [ 700/1251 ( 56%)]  Loss: 3.365 (3.80)  Time: 0.797s, 1285.08/s  (0.813s, 1260.00/s)  LR: 9.572e-04  Data: 0.009 (0.013)
Train: 80 [ 750/1251 ( 60%)]  Loss: 3.956 (3.81)  Time: 0.800s, 1280.34/s  (0.812s, 1260.72/s)  LR: 9.572e-04  Data: 0.009 (0.013)
Train: 80 [ 800/1251 ( 64%)]  Loss: 3.489 (3.79)  Time: 0.815s, 1257.17/s  (0.812s, 1261.45/s)  LR: 9.572e-04  Data: 0.010 (0.013)
Train: 80 [ 850/1251 ( 68%)]  Loss: 3.807 (3.79)  Time: 0.806s, 1269.80/s  (0.812s, 1261.19/s)  LR: 9.572e-04  Data: 0.010 (0.013)
Train: 80 [ 900/1251 ( 72%)]  Loss: 4.054 (3.81)  Time: 0.772s, 1326.73/s  (0.812s, 1261.25/s)  LR: 9.572e-04  Data: 0.009 (0.013)
Train: 80 [ 950/1251 ( 76%)]  Loss: 3.815 (3.81)  Time: 0.858s, 1193.95/s  (0.812s, 1261.21/s)  LR: 9.572e-04  Data: 0.010 (0.013)
Train: 80 [1000/1251 ( 80%)]  Loss: 3.554 (3.80)  Time: 0.821s, 1246.93/s  (0.812s, 1261.17/s)  LR: 9.572e-04  Data: 0.009 (0.013)
Train: 80 [1050/1251 ( 84%)]  Loss: 3.897 (3.80)  Time: 0.784s, 1305.39/s  (0.812s, 1261.60/s)  LR: 9.572e-04  Data: 0.009 (0.013)
Train: 80 [1100/1251 ( 88%)]  Loss: 3.796 (3.80)  Time: 0.827s, 1238.08/s  (0.812s, 1261.78/s)  LR: 9.572e-04  Data: 0.009 (0.012)
Train: 80 [1150/1251 ( 92%)]  Loss: 3.385 (3.78)  Time: 0.827s, 1237.89/s  (0.812s, 1261.55/s)  LR: 9.572e-04  Data: 0.010 (0.012)
Train: 80 [1200/1251 ( 96%)]  Loss: 3.836 (3.79)  Time: 0.823s, 1243.50/s  (0.811s, 1262.06/s)  LR: 9.572e-04  Data: 0.009 (0.012)
Train: 80 [1250/1251 (100%)]  Loss: 3.629 (3.78)  Time: 0.777s, 1318.21/s  (0.811s, 1262.19/s)  LR: 9.572e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.625 (1.625)  Loss:  0.8452 (0.8452)  Acc@1: 86.5234 (86.5234)  Acc@5: 95.6055 (95.6055)
Test: [  48/48]  Time: 0.194 (0.595)  Loss:  0.9507 (1.4661)  Acc@1: 83.4906 (70.5740)  Acc@5: 95.7547 (90.4200)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-77.pth.tar', 70.83599993896485)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-73.pth.tar', 70.71800010009765)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-72.pth.tar', 70.7139999658203)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-80.pth.tar', 70.57399993896485)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-74.pth.tar', 70.5540000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-79.pth.tar', 70.52800006835938)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-75.pth.tar', 70.31000001953124)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-69.pth.tar', 70.23999997314453)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-78.pth.tar', 70.23600009765624)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-70.pth.tar', 70.23399999267578)

Train: 81 [   0/1251 (  0%)]  Loss: 3.715 (3.71)  Time: 2.362s,  433.61/s  (2.362s,  433.61/s)  LR: 9.561e-04  Data: 1.598 (1.598)
Train: 81 [  50/1251 (  4%)]  Loss: 3.762 (3.74)  Time: 0.787s, 1301.14/s  (0.849s, 1205.85/s)  LR: 9.561e-04  Data: 0.010 (0.047)
Train: 81 [ 100/1251 (  8%)]  Loss: 3.296 (3.59)  Time: 0.860s, 1190.93/s  (0.831s, 1231.74/s)  LR: 9.561e-04  Data: 0.009 (0.029)
Train: 81 [ 150/1251 ( 12%)]  Loss: 3.632 (3.60)  Time: 0.843s, 1214.97/s  (0.823s, 1244.40/s)  LR: 9.561e-04  Data: 0.010 (0.023)
Train: 81 [ 200/1251 ( 16%)]  Loss: 3.504 (3.58)  Time: 0.779s, 1314.58/s  (0.819s, 1249.89/s)  LR: 9.561e-04  Data: 0.010 (0.020)
Train: 81 [ 250/1251 ( 20%)]  Loss: 3.686 (3.60)  Time: 0.807s, 1268.38/s  (0.817s, 1253.51/s)  LR: 9.561e-04  Data: 0.009 (0.018)
Train: 81 [ 300/1251 ( 24%)]  Loss: 3.597 (3.60)  Time: 0.835s, 1226.08/s  (0.815s, 1256.51/s)  LR: 9.561e-04  Data: 0.010 (0.017)
Train: 81 [ 350/1251 ( 28%)]  Loss: 3.994 (3.65)  Time: 0.778s, 1315.90/s  (0.814s, 1257.89/s)  LR: 9.561e-04  Data: 0.009 (0.016)
Train: 81 [ 400/1251 ( 32%)]  Loss: 4.249 (3.71)  Time: 0.794s, 1290.28/s  (0.813s, 1258.87/s)  LR: 9.561e-04  Data: 0.010 (0.015)
Train: 81 [ 450/1251 ( 36%)]  Loss: 3.543 (3.70)  Time: 0.821s, 1247.14/s  (0.812s, 1260.66/s)  LR: 9.561e-04  Data: 0.010 (0.015)
Train: 81 [ 500/1251 ( 40%)]  Loss: 3.953 (3.72)  Time: 0.778s, 1316.88/s  (0.812s, 1260.79/s)  LR: 9.561e-04  Data: 0.010 (0.014)
Train: 81 [ 550/1251 ( 44%)]  Loss: 3.850 (3.73)  Time: 0.798s, 1282.71/s  (0.812s, 1261.26/s)  LR: 9.561e-04  Data: 0.010 (0.014)
Train: 81 [ 600/1251 ( 48%)]  Loss: 3.612 (3.72)  Time: 0.828s, 1237.24/s  (0.812s, 1261.27/s)  LR: 9.561e-04  Data: 0.020 (0.014)
Train: 81 [ 650/1251 ( 52%)]  Loss: 3.345 (3.70)  Time: 0.806s, 1270.61/s  (0.811s, 1261.89/s)  LR: 9.561e-04  Data: 0.010 (0.014)
Train: 81 [ 700/1251 ( 56%)]  Loss: 3.799 (3.70)  Time: 0.878s, 1166.75/s  (0.812s, 1261.69/s)  LR: 9.561e-04  Data: 0.009 (0.013)
Train: 81 [ 750/1251 ( 60%)]  Loss: 4.146 (3.73)  Time: 0.814s, 1257.90/s  (0.812s, 1261.11/s)  LR: 9.561e-04  Data: 0.011 (0.013)
Train: 81 [ 800/1251 ( 64%)]  Loss: 3.581 (3.72)  Time: 0.787s, 1300.49/s  (0.812s, 1260.99/s)  LR: 9.561e-04  Data: 0.012 (0.013)
Train: 81 [ 850/1251 ( 68%)]  Loss: 3.945 (3.73)  Time: 0.782s, 1309.28/s  (0.812s, 1260.93/s)  LR: 9.561e-04  Data: 0.010 (0.013)
Train: 81 [ 900/1251 ( 72%)]  Loss: 3.941 (3.74)  Time: 0.837s, 1222.91/s  (0.812s, 1261.15/s)  LR: 9.561e-04  Data: 0.014 (0.013)
Train: 81 [ 950/1251 ( 76%)]  Loss: 3.683 (3.74)  Time: 0.823s, 1244.84/s  (0.812s, 1261.34/s)  LR: 9.561e-04  Data: 0.009 (0.013)
Train: 81 [1000/1251 ( 80%)]  Loss: 3.610 (3.74)  Time: 0.783s, 1308.29/s  (0.812s, 1261.76/s)  LR: 9.561e-04  Data: 0.010 (0.012)
Train: 81 [1050/1251 ( 84%)]  Loss: 3.817 (3.74)  Time: 0.845s, 1212.50/s  (0.811s, 1261.91/s)  LR: 9.561e-04  Data: 0.016 (0.012)
Train: 81 [1100/1251 ( 88%)]  Loss: 3.892 (3.75)  Time: 0.859s, 1191.51/s  (0.811s, 1262.06/s)  LR: 9.561e-04  Data: 0.016 (0.012)
Train: 81 [1150/1251 ( 92%)]  Loss: 3.591 (3.74)  Time: 0.832s, 1230.15/s  (0.811s, 1262.48/s)  LR: 9.561e-04  Data: 0.011 (0.012)
Train: 81 [1200/1251 ( 96%)]  Loss: 3.779 (3.74)  Time: 0.929s, 1102.75/s  (0.811s, 1262.35/s)  LR: 9.561e-04  Data: 0.013 (0.012)
Train: 81 [1250/1251 (100%)]  Loss: 3.935 (3.75)  Time: 0.764s, 1341.12/s  (0.811s, 1262.65/s)  LR: 9.561e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.641 (1.641)  Loss:  0.8545 (0.8545)  Acc@1: 88.1836 (88.1836)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.194 (0.602)  Loss:  1.0244 (1.4890)  Acc@1: 82.6651 (70.9100)  Acc@5: 95.2830 (90.4780)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-81.pth.tar', 70.90999996826172)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-77.pth.tar', 70.83599993896485)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-73.pth.tar', 70.71800010009765)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-72.pth.tar', 70.7139999658203)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-80.pth.tar', 70.57399993896485)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-74.pth.tar', 70.5540000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-79.pth.tar', 70.52800006835938)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-75.pth.tar', 70.31000001953124)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-69.pth.tar', 70.23999997314453)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-78.pth.tar', 70.23600009765624)

Train: 82 [   0/1251 (  0%)]  Loss: 3.977 (3.98)  Time: 2.312s,  442.92/s  (2.312s,  442.92/s)  LR: 9.551e-04  Data: 1.580 (1.580)
Train: 82 [  50/1251 (  4%)]  Loss: 3.584 (3.78)  Time: 0.812s, 1260.42/s  (0.834s, 1228.29/s)  LR: 9.551e-04  Data: 0.009 (0.047)
Train: 82 [ 100/1251 (  8%)]  Loss: 3.828 (3.80)  Time: 0.785s, 1304.48/s  (0.821s, 1247.66/s)  LR: 9.551e-04  Data: 0.010 (0.029)
Train: 82 [ 150/1251 ( 12%)]  Loss: 3.998 (3.85)  Time: 0.838s, 1221.87/s  (0.817s, 1253.51/s)  LR: 9.551e-04  Data: 0.009 (0.023)
Train: 82 [ 200/1251 ( 16%)]  Loss: 3.901 (3.86)  Time: 0.781s, 1310.55/s  (0.815s, 1256.12/s)  LR: 9.551e-04  Data: 0.009 (0.020)
Train: 82 [ 250/1251 ( 20%)]  Loss: 4.339 (3.94)  Time: 0.822s, 1245.20/s  (0.815s, 1256.52/s)  LR: 9.551e-04  Data: 0.009 (0.018)
Train: 82 [ 300/1251 ( 24%)]  Loss: 3.736 (3.91)  Time: 0.824s, 1242.91/s  (0.814s, 1257.61/s)  LR: 9.551e-04  Data: 0.013 (0.017)
Train: 82 [ 350/1251 ( 28%)]  Loss: 3.321 (3.84)  Time: 0.838s, 1221.53/s  (0.814s, 1258.02/s)  LR: 9.551e-04  Data: 0.009 (0.016)
Train: 82 [ 400/1251 ( 32%)]  Loss: 3.468 (3.79)  Time: 0.793s, 1291.17/s  (0.814s, 1257.88/s)  LR: 9.551e-04  Data: 0.014 (0.015)
Train: 82 [ 450/1251 ( 36%)]  Loss: 3.661 (3.78)  Time: 0.798s, 1283.15/s  (0.814s, 1258.42/s)  LR: 9.551e-04  Data: 0.009 (0.015)
Train: 82 [ 500/1251 ( 40%)]  Loss: 3.868 (3.79)  Time: 0.852s, 1202.09/s  (0.814s, 1258.41/s)  LR: 9.551e-04  Data: 0.013 (0.014)
Train: 82 [ 550/1251 ( 44%)]  Loss: 4.144 (3.82)  Time: 0.826s, 1239.88/s  (0.814s, 1258.48/s)  LR: 9.551e-04  Data: 0.010 (0.014)
Train: 82 [ 600/1251 ( 48%)]  Loss: 3.950 (3.83)  Time: 0.806s, 1271.26/s  (0.814s, 1258.00/s)  LR: 9.551e-04  Data: 0.010 (0.014)
Train: 82 [ 650/1251 ( 52%)]  Loss: 4.009 (3.84)  Time: 0.804s, 1274.04/s  (0.814s, 1258.55/s)  LR: 9.551e-04  Data: 0.009 (0.013)
Train: 82 [ 700/1251 ( 56%)]  Loss: 3.802 (3.84)  Time: 0.821s, 1247.81/s  (0.814s, 1258.48/s)  LR: 9.551e-04  Data: 0.014 (0.013)
Train: 82 [ 750/1251 ( 60%)]  Loss: 3.692 (3.83)  Time: 0.781s, 1310.72/s  (0.813s, 1259.54/s)  LR: 9.551e-04  Data: 0.013 (0.013)
Train: 82 [ 800/1251 ( 64%)]  Loss: 3.597 (3.82)  Time: 0.782s, 1309.31/s  (0.813s, 1260.03/s)  LR: 9.551e-04  Data: 0.016 (0.013)
Train: 82 [ 850/1251 ( 68%)]  Loss: 3.704 (3.81)  Time: 0.807s, 1268.71/s  (0.813s, 1260.29/s)  LR: 9.551e-04  Data: 0.010 (0.013)
Train: 82 [ 900/1251 ( 72%)]  Loss: 3.430 (3.79)  Time: 0.784s, 1306.67/s  (0.812s, 1260.60/s)  LR: 9.551e-04  Data: 0.013 (0.013)
Train: 82 [ 950/1251 ( 76%)]  Loss: 3.770 (3.79)  Time: 0.834s, 1228.04/s  (0.812s, 1261.06/s)  LR: 9.551e-04  Data: 0.009 (0.013)
Train: 82 [1000/1251 ( 80%)]  Loss: 3.577 (3.78)  Time: 0.784s, 1306.73/s  (0.812s, 1261.50/s)  LR: 9.551e-04  Data: 0.010 (0.012)
Train: 82 [1050/1251 ( 84%)]  Loss: 3.904 (3.78)  Time: 0.813s, 1259.92/s  (0.812s, 1261.53/s)  LR: 9.551e-04  Data: 0.009 (0.012)
Train: 82 [1100/1251 ( 88%)]  Loss: 3.753 (3.78)  Time: 0.797s, 1285.38/s  (0.811s, 1262.29/s)  LR: 9.551e-04  Data: 0.016 (0.012)
Train: 82 [1150/1251 ( 92%)]  Loss: 3.473 (3.77)  Time: 0.867s, 1181.00/s  (0.811s, 1262.75/s)  LR: 9.551e-04  Data: 0.009 (0.012)
Train: 82 [1200/1251 ( 96%)]  Loss: 3.935 (3.78)  Time: 0.843s, 1214.18/s  (0.811s, 1262.89/s)  LR: 9.551e-04  Data: 0.010 (0.012)
Train: 82 [1250/1251 (100%)]  Loss: 4.101 (3.79)  Time: 0.810s, 1264.12/s  (0.811s, 1262.96/s)  LR: 9.551e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.540 (1.540)  Loss:  0.8579 (0.8579)  Acc@1: 86.1328 (86.1328)  Acc@5: 96.0938 (96.0938)
Test: [  48/48]  Time: 0.194 (0.607)  Loss:  0.8667 (1.4323)  Acc@1: 83.7264 (70.8740)  Acc@5: 95.8726 (90.5920)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-81.pth.tar', 70.90999996826172)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-82.pth.tar', 70.8740000415039)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-77.pth.tar', 70.83599993896485)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-73.pth.tar', 70.71800010009765)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-72.pth.tar', 70.7139999658203)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-80.pth.tar', 70.57399993896485)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-74.pth.tar', 70.5540000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-79.pth.tar', 70.52800006835938)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-75.pth.tar', 70.31000001953124)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-69.pth.tar', 70.23999997314453)

Train: 83 [   0/1251 (  0%)]  Loss: 3.863 (3.86)  Time: 2.469s,  414.75/s  (2.469s,  414.75/s)  LR: 9.540e-04  Data: 1.734 (1.734)
Train: 83 [  50/1251 (  4%)]  Loss: 3.556 (3.71)  Time: 0.815s, 1256.90/s  (0.858s, 1193.22/s)  LR: 9.540e-04  Data: 0.010 (0.051)
Train: 83 [ 100/1251 (  8%)]  Loss: 3.831 (3.75)  Time: 0.797s, 1285.51/s  (0.834s, 1227.15/s)  LR: 9.540e-04  Data: 0.010 (0.031)
Train: 83 [ 150/1251 ( 12%)]  Loss: 3.788 (3.76)  Time: 0.784s, 1305.35/s  (0.825s, 1241.02/s)  LR: 9.540e-04  Data: 0.009 (0.024)
Train: 83 [ 200/1251 ( 16%)]  Loss: 3.854 (3.78)  Time: 0.780s, 1313.10/s  (0.820s, 1248.27/s)  LR: 9.540e-04  Data: 0.012 (0.021)
Train: 83 [ 250/1251 ( 20%)]  Loss: 3.762 (3.78)  Time: 0.812s, 1260.64/s  (0.817s, 1253.09/s)  LR: 9.540e-04  Data: 0.011 (0.019)
Train: 83 [ 300/1251 ( 24%)]  Loss: 3.431 (3.73)  Time: 0.781s, 1311.33/s  (0.816s, 1255.54/s)  LR: 9.540e-04  Data: 0.009 (0.017)
Train: 83 [ 350/1251 ( 28%)]  Loss: 3.677 (3.72)  Time: 0.791s, 1295.17/s  (0.815s, 1257.11/s)  LR: 9.540e-04  Data: 0.009 (0.016)
Train: 83 [ 400/1251 ( 32%)]  Loss: 3.725 (3.72)  Time: 0.775s, 1320.45/s  (0.814s, 1258.39/s)  LR: 9.540e-04  Data: 0.009 (0.016)
Train: 83 [ 450/1251 ( 36%)]  Loss: 3.719 (3.72)  Time: 0.804s, 1273.37/s  (0.814s, 1258.45/s)  LR: 9.540e-04  Data: 0.009 (0.015)
Train: 83 [ 500/1251 ( 40%)]  Loss: 3.581 (3.71)  Time: 0.788s, 1298.71/s  (0.813s, 1259.01/s)  LR: 9.540e-04  Data: 0.011 (0.015)
Train: 83 [ 550/1251 ( 44%)]  Loss: 4.049 (3.74)  Time: 0.811s, 1262.70/s  (0.813s, 1259.80/s)  LR: 9.540e-04  Data: 0.010 (0.014)
Train: 83 [ 600/1251 ( 48%)]  Loss: 3.927 (3.75)  Time: 0.845s, 1212.36/s  (0.812s, 1260.73/s)  LR: 9.540e-04  Data: 0.012 (0.014)
Train: 83 [ 650/1251 ( 52%)]  Loss: 3.818 (3.76)  Time: 0.812s, 1261.05/s  (0.812s, 1261.05/s)  LR: 9.540e-04  Data: 0.010 (0.014)
Train: 83 [ 700/1251 ( 56%)]  Loss: 4.073 (3.78)  Time: 0.805s, 1271.54/s  (0.812s, 1261.84/s)  LR: 9.540e-04  Data: 0.009 (0.013)
Train: 83 [ 750/1251 ( 60%)]  Loss: 3.965 (3.79)  Time: 0.869s, 1178.61/s  (0.812s, 1261.33/s)  LR: 9.540e-04  Data: 0.009 (0.013)
Train: 83 [ 800/1251 ( 64%)]  Loss: 3.807 (3.79)  Time: 0.815s, 1256.63/s  (0.812s, 1261.80/s)  LR: 9.540e-04  Data: 0.009 (0.013)
Train: 83 [ 850/1251 ( 68%)]  Loss: 3.874 (3.79)  Time: 0.783s, 1307.13/s  (0.812s, 1261.41/s)  LR: 9.540e-04  Data: 0.012 (0.013)
Train: 83 [ 900/1251 ( 72%)]  Loss: 3.639 (3.79)  Time: 0.829s, 1235.21/s  (0.811s, 1262.44/s)  LR: 9.540e-04  Data: 0.009 (0.013)
Train: 83 [ 950/1251 ( 76%)]  Loss: 3.891 (3.79)  Time: 0.792s, 1293.28/s  (0.811s, 1262.44/s)  LR: 9.540e-04  Data: 0.013 (0.013)
Train: 83 [1000/1251 ( 80%)]  Loss: 3.788 (3.79)  Time: 0.796s, 1286.65/s  (0.811s, 1262.93/s)  LR: 9.540e-04  Data: 0.010 (0.013)
Train: 83 [1050/1251 ( 84%)]  Loss: 3.782 (3.79)  Time: 0.850s, 1204.01/s  (0.811s, 1263.03/s)  LR: 9.540e-04  Data: 0.010 (0.012)
Train: 83 [1100/1251 ( 88%)]  Loss: 3.608 (3.78)  Time: 0.792s, 1292.36/s  (0.811s, 1263.04/s)  LR: 9.540e-04  Data: 0.010 (0.012)
Train: 83 [1150/1251 ( 92%)]  Loss: 3.770 (3.78)  Time: 0.795s, 1287.78/s  (0.811s, 1263.38/s)  LR: 9.540e-04  Data: 0.009 (0.012)
Train: 83 [1200/1251 ( 96%)]  Loss: 4.023 (3.79)  Time: 0.850s, 1204.81/s  (0.811s, 1263.21/s)  LR: 9.540e-04  Data: 0.010 (0.012)
Train: 83 [1250/1251 (100%)]  Loss: 3.611 (3.79)  Time: 0.863s, 1187.03/s  (0.811s, 1263.12/s)  LR: 9.540e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.619 (1.619)  Loss:  0.9551 (0.9551)  Acc@1: 85.9375 (85.9375)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.194 (0.609)  Loss:  1.0156 (1.5586)  Acc@1: 83.4906 (70.7720)  Acc@5: 94.6934 (90.5220)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-81.pth.tar', 70.90999996826172)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-82.pth.tar', 70.8740000415039)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-77.pth.tar', 70.83599993896485)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-83.pth.tar', 70.77200006835938)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-73.pth.tar', 70.71800010009765)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-72.pth.tar', 70.7139999658203)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-80.pth.tar', 70.57399993896485)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-74.pth.tar', 70.5540000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-79.pth.tar', 70.52800006835938)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-75.pth.tar', 70.31000001953124)

Train: 84 [   0/1251 (  0%)]  Loss: 4.129 (4.13)  Time: 2.382s,  429.83/s  (2.382s,  429.83/s)  LR: 9.529e-04  Data: 1.593 (1.593)
Train: 84 [  50/1251 (  4%)]  Loss: 3.928 (4.03)  Time: 0.795s, 1288.34/s  (0.842s, 1215.85/s)  LR: 9.529e-04  Data: 0.011 (0.046)
Train: 84 [ 100/1251 (  8%)]  Loss: 3.875 (3.98)  Time: 0.806s, 1269.77/s  (0.823s, 1244.67/s)  LR: 9.529e-04  Data: 0.010 (0.028)
Train: 84 [ 150/1251 ( 12%)]  Loss: 3.717 (3.91)  Time: 0.797s, 1284.15/s  (0.818s, 1252.57/s)  LR: 9.529e-04  Data: 0.013 (0.023)
Train: 84 [ 200/1251 ( 16%)]  Loss: 3.681 (3.87)  Time: 0.817s, 1253.52/s  (0.817s, 1253.06/s)  LR: 9.529e-04  Data: 0.016 (0.020)
Train: 84 [ 250/1251 ( 20%)]  Loss: 3.784 (3.85)  Time: 0.793s, 1292.10/s  (0.816s, 1254.91/s)  LR: 9.529e-04  Data: 0.009 (0.018)
Train: 84 [ 300/1251 ( 24%)]  Loss: 3.735 (3.84)  Time: 0.787s, 1300.95/s  (0.815s, 1256.70/s)  LR: 9.529e-04  Data: 0.010 (0.017)
Train: 84 [ 350/1251 ( 28%)]  Loss: 3.700 (3.82)  Time: 0.809s, 1265.56/s  (0.814s, 1257.88/s)  LR: 9.529e-04  Data: 0.010 (0.016)
Train: 84 [ 400/1251 ( 32%)]  Loss: 3.707 (3.81)  Time: 0.824s, 1243.24/s  (0.814s, 1257.72/s)  LR: 9.529e-04  Data: 0.009 (0.015)
Train: 84 [ 450/1251 ( 36%)]  Loss: 3.724 (3.80)  Time: 0.807s, 1269.27/s  (0.813s, 1259.78/s)  LR: 9.529e-04  Data: 0.011 (0.015)
Train: 84 [ 500/1251 ( 40%)]  Loss: 3.980 (3.81)  Time: 0.822s, 1245.58/s  (0.812s, 1261.05/s)  LR: 9.529e-04  Data: 0.010 (0.014)
Train: 84 [ 550/1251 ( 44%)]  Loss: 3.724 (3.81)  Time: 0.794s, 1289.95/s  (0.812s, 1260.56/s)  LR: 9.529e-04  Data: 0.016 (0.014)
Train: 84 [ 600/1251 ( 48%)]  Loss: 3.804 (3.81)  Time: 0.780s, 1312.43/s  (0.812s, 1261.75/s)  LR: 9.529e-04  Data: 0.009 (0.014)
Train: 84 [ 650/1251 ( 52%)]  Loss: 3.694 (3.80)  Time: 0.800s, 1280.20/s  (0.811s, 1262.06/s)  LR: 9.529e-04  Data: 0.010 (0.013)
Train: 84 [ 700/1251 ( 56%)]  Loss: 3.706 (3.79)  Time: 0.832s, 1230.61/s  (0.812s, 1261.65/s)  LR: 9.529e-04  Data: 0.015 (0.013)
Train: 84 [ 750/1251 ( 60%)]  Loss: 3.883 (3.80)  Time: 0.785s, 1304.91/s  (0.811s, 1262.00/s)  LR: 9.529e-04  Data: 0.010 (0.013)
Train: 84 [ 800/1251 ( 64%)]  Loss: 3.804 (3.80)  Time: 0.823s, 1244.70/s  (0.811s, 1262.31/s)  LR: 9.529e-04  Data: 0.010 (0.013)
Train: 84 [ 850/1251 ( 68%)]  Loss: 3.792 (3.80)  Time: 0.830s, 1233.67/s  (0.813s, 1259.92/s)  LR: 9.529e-04  Data: 0.014 (0.013)
Train: 84 [ 900/1251 ( 72%)]  Loss: 3.845 (3.80)  Time: 0.780s, 1312.72/s  (0.812s, 1260.57/s)  LR: 9.529e-04  Data: 0.010 (0.013)
Train: 84 [ 950/1251 ( 76%)]  Loss: 4.029 (3.81)  Time: 0.781s, 1311.97/s  (0.811s, 1262.69/s)  LR: 9.529e-04  Data: 0.010 (0.013)
Train: 84 [1000/1251 ( 80%)]  Loss: 3.825 (3.81)  Time: 0.792s, 1293.42/s  (0.811s, 1263.05/s)  LR: 9.529e-04  Data: 0.010 (0.012)
Train: 84 [1050/1251 ( 84%)]  Loss: 3.724 (3.81)  Time: 0.797s, 1284.52/s  (0.811s, 1263.00/s)  LR: 9.529e-04  Data: 0.013 (0.012)
Train: 84 [1100/1251 ( 88%)]  Loss: 3.829 (3.81)  Time: 0.827s, 1237.96/s  (0.811s, 1262.59/s)  LR: 9.529e-04  Data: 0.010 (0.012)
Train: 84 [1150/1251 ( 92%)]  Loss: 3.838 (3.81)  Time: 0.782s, 1309.94/s  (0.811s, 1262.88/s)  LR: 9.529e-04  Data: 0.010 (0.012)
Train: 84 [1200/1251 ( 96%)]  Loss: 4.061 (3.82)  Time: 0.813s, 1259.48/s  (0.811s, 1263.21/s)  LR: 9.529e-04  Data: 0.010 (0.012)
Train: 84 [1250/1251 (100%)]  Loss: 3.882 (3.82)  Time: 0.786s, 1303.20/s  (0.810s, 1263.74/s)  LR: 9.529e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.623 (1.623)  Loss:  0.8877 (0.8877)  Acc@1: 87.6953 (87.6953)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.194 (0.598)  Loss:  1.0098 (1.5072)  Acc@1: 83.0189 (71.2900)  Acc@5: 95.2830 (90.6420)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-84.pth.tar', 71.28999999267577)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-81.pth.tar', 70.90999996826172)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-82.pth.tar', 70.8740000415039)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-77.pth.tar', 70.83599993896485)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-83.pth.tar', 70.77200006835938)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-73.pth.tar', 70.71800010009765)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-72.pth.tar', 70.7139999658203)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-80.pth.tar', 70.57399993896485)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-74.pth.tar', 70.5540000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-79.pth.tar', 70.52800006835938)

Train: 85 [   0/1251 (  0%)]  Loss: 4.063 (4.06)  Time: 2.282s,  448.74/s  (2.282s,  448.74/s)  LR: 9.518e-04  Data: 1.547 (1.547)
Train: 85 [  50/1251 (  4%)]  Loss: 3.987 (4.02)  Time: 0.826s, 1239.15/s  (0.855s, 1197.12/s)  LR: 9.518e-04  Data: 0.010 (0.050)
Train: 85 [ 100/1251 (  8%)]  Loss: 3.960 (4.00)  Time: 0.825s, 1240.62/s  (0.836s, 1224.93/s)  LR: 9.518e-04  Data: 0.010 (0.030)
Train: 85 [ 150/1251 ( 12%)]  Loss: 3.544 (3.89)  Time: 0.779s, 1314.80/s  (0.826s, 1239.02/s)  LR: 9.518e-04  Data: 0.010 (0.024)
Train: 85 [ 200/1251 ( 16%)]  Loss: 3.791 (3.87)  Time: 0.814s, 1258.58/s  (0.823s, 1244.83/s)  LR: 9.518e-04  Data: 0.011 (0.021)
Train: 85 [ 250/1251 ( 20%)]  Loss: 3.537 (3.81)  Time: 0.807s, 1268.93/s  (0.819s, 1250.67/s)  LR: 9.518e-04  Data: 0.009 (0.019)
Train: 85 [ 300/1251 ( 24%)]  Loss: 3.755 (3.81)  Time: 0.797s, 1284.61/s  (0.817s, 1253.68/s)  LR: 9.518e-04  Data: 0.014 (0.017)
Train: 85 [ 350/1251 ( 28%)]  Loss: 3.986 (3.83)  Time: 0.896s, 1142.45/s  (0.817s, 1253.55/s)  LR: 9.518e-04  Data: 0.010 (0.016)
Train: 85 [ 400/1251 ( 32%)]  Loss: 3.372 (3.78)  Time: 0.785s, 1304.94/s  (0.816s, 1255.06/s)  LR: 9.518e-04  Data: 0.010 (0.016)
Train: 85 [ 450/1251 ( 36%)]  Loss: 3.368 (3.74)  Time: 0.830s, 1233.59/s  (0.815s, 1256.06/s)  LR: 9.518e-04  Data: 0.010 (0.015)
Train: 85 [ 500/1251 ( 40%)]  Loss: 3.876 (3.75)  Time: 0.776s, 1319.22/s  (0.814s, 1257.58/s)  LR: 9.518e-04  Data: 0.010 (0.015)
Train: 85 [ 550/1251 ( 44%)]  Loss: 3.889 (3.76)  Time: 0.794s, 1289.76/s  (0.814s, 1258.49/s)  LR: 9.518e-04  Data: 0.010 (0.014)
Train: 85 [ 600/1251 ( 48%)]  Loss: 3.911 (3.77)  Time: 0.781s, 1311.35/s  (0.814s, 1258.66/s)  LR: 9.518e-04  Data: 0.012 (0.014)
Train: 85 [ 650/1251 ( 52%)]  Loss: 3.832 (3.78)  Time: 0.826s, 1240.24/s  (0.813s, 1259.42/s)  LR: 9.518e-04  Data: 0.011 (0.014)
Train: 85 [ 700/1251 ( 56%)]  Loss: 3.788 (3.78)  Time: 0.802s, 1276.79/s  (0.813s, 1259.68/s)  LR: 9.518e-04  Data: 0.015 (0.014)
Train: 85 [ 750/1251 ( 60%)]  Loss: 3.851 (3.78)  Time: 0.821s, 1247.47/s  (0.813s, 1259.60/s)  LR: 9.518e-04  Data: 0.010 (0.013)
Train: 85 [ 800/1251 ( 64%)]  Loss: 3.665 (3.77)  Time: 0.805s, 1271.83/s  (0.813s, 1260.04/s)  LR: 9.518e-04  Data: 0.009 (0.013)
Train: 85 [ 850/1251 ( 68%)]  Loss: 3.555 (3.76)  Time: 0.783s, 1308.43/s  (0.812s, 1260.33/s)  LR: 9.518e-04  Data: 0.010 (0.013)
Train: 85 [ 900/1251 ( 72%)]  Loss: 3.873 (3.77)  Time: 0.806s, 1269.90/s  (0.812s, 1261.12/s)  LR: 9.518e-04  Data: 0.010 (0.013)
Train: 85 [ 950/1251 ( 76%)]  Loss: 3.860 (3.77)  Time: 0.781s, 1311.30/s  (0.812s, 1261.06/s)  LR: 9.518e-04  Data: 0.010 (0.013)
Train: 85 [1000/1251 ( 80%)]  Loss: 3.748 (3.77)  Time: 0.789s, 1298.42/s  (0.812s, 1261.03/s)  LR: 9.518e-04  Data: 0.013 (0.013)
Train: 85 [1050/1251 ( 84%)]  Loss: 3.437 (3.76)  Time: 0.885s, 1156.58/s  (0.812s, 1261.47/s)  LR: 9.518e-04  Data: 0.010 (0.013)
Train: 85 [1100/1251 ( 88%)]  Loss: 3.947 (3.77)  Time: 0.834s, 1227.21/s  (0.812s, 1261.35/s)  LR: 9.518e-04  Data: 0.010 (0.012)
Train: 85 [1150/1251 ( 92%)]  Loss: 3.655 (3.76)  Time: 0.824s, 1242.69/s  (0.812s, 1261.42/s)  LR: 9.518e-04  Data: 0.010 (0.012)
Train: 85 [1200/1251 ( 96%)]  Loss: 4.024 (3.77)  Time: 0.924s, 1108.05/s  (0.812s, 1261.26/s)  LR: 9.518e-04  Data: 0.010 (0.012)
Train: 85 [1250/1251 (100%)]  Loss: 3.846 (3.77)  Time: 0.820s, 1248.22/s  (0.812s, 1261.50/s)  LR: 9.518e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.802 (1.802)  Loss:  0.8691 (0.8691)  Acc@1: 86.8164 (86.8164)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.194 (0.600)  Loss:  0.8711 (1.4476)  Acc@1: 83.1368 (70.7080)  Acc@5: 95.2830 (90.4380)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-84.pth.tar', 71.28999999267577)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-81.pth.tar', 70.90999996826172)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-82.pth.tar', 70.8740000415039)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-77.pth.tar', 70.83599993896485)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-83.pth.tar', 70.77200006835938)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-73.pth.tar', 70.71800010009765)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-72.pth.tar', 70.7139999658203)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-85.pth.tar', 70.70800004394532)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-80.pth.tar', 70.57399993896485)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-74.pth.tar', 70.5540000732422)

Train: 86 [   0/1251 (  0%)]  Loss: 3.847 (3.85)  Time: 2.401s,  426.41/s  (2.401s,  426.41/s)  LR: 9.507e-04  Data: 1.655 (1.655)
Train: 86 [  50/1251 (  4%)]  Loss: 3.787 (3.82)  Time: 0.781s, 1310.95/s  (0.850s, 1204.22/s)  LR: 9.507e-04  Data: 0.009 (0.050)
Train: 86 [ 100/1251 (  8%)]  Loss: 3.671 (3.77)  Time: 0.873s, 1173.51/s  (0.831s, 1231.82/s)  LR: 9.507e-04  Data: 0.009 (0.031)
Train: 86 [ 150/1251 ( 12%)]  Loss: 4.097 (3.85)  Time: 0.789s, 1298.27/s  (0.823s, 1244.22/s)  LR: 9.507e-04  Data: 0.010 (0.024)
Train: 86 [ 200/1251 ( 16%)]  Loss: 3.964 (3.87)  Time: 0.821s, 1246.78/s  (0.820s, 1248.76/s)  LR: 9.507e-04  Data: 0.014 (0.021)
Train: 86 [ 250/1251 ( 20%)]  Loss: 3.665 (3.84)  Time: 0.795s, 1287.33/s  (0.818s, 1251.52/s)  LR: 9.507e-04  Data: 0.010 (0.019)
Train: 86 [ 300/1251 ( 24%)]  Loss: 3.569 (3.80)  Time: 0.827s, 1238.16/s  (0.818s, 1251.89/s)  LR: 9.507e-04  Data: 0.010 (0.017)
Train: 86 [ 350/1251 ( 28%)]  Loss: 3.509 (3.76)  Time: 0.827s, 1237.56/s  (0.816s, 1255.35/s)  LR: 9.507e-04  Data: 0.010 (0.016)
Train: 86 [ 400/1251 ( 32%)]  Loss: 3.631 (3.75)  Time: 0.798s, 1283.57/s  (0.815s, 1256.87/s)  LR: 9.507e-04  Data: 0.010 (0.016)
Train: 86 [ 450/1251 ( 36%)]  Loss: 3.882 (3.76)  Time: 0.779s, 1315.03/s  (0.814s, 1257.40/s)  LR: 9.507e-04  Data: 0.009 (0.015)
Train: 86 [ 500/1251 ( 40%)]  Loss: 4.006 (3.78)  Time: 0.782s, 1310.29/s  (0.813s, 1259.07/s)  LR: 9.507e-04  Data: 0.010 (0.015)
Train: 86 [ 550/1251 ( 44%)]  Loss: 3.895 (3.79)  Time: 0.821s, 1247.47/s  (0.813s, 1258.83/s)  LR: 9.507e-04  Data: 0.009 (0.014)
Train: 86 [ 600/1251 ( 48%)]  Loss: 3.737 (3.79)  Time: 0.809s, 1265.15/s  (0.814s, 1258.65/s)  LR: 9.507e-04  Data: 0.009 (0.014)
Train: 86 [ 650/1251 ( 52%)]  Loss: 3.791 (3.79)  Time: 0.841s, 1217.16/s  (0.813s, 1259.46/s)  LR: 9.507e-04  Data: 0.010 (0.014)
Train: 86 [ 700/1251 ( 56%)]  Loss: 3.816 (3.79)  Time: 0.834s, 1227.92/s  (0.813s, 1259.61/s)  LR: 9.507e-04  Data: 0.010 (0.014)
Train: 86 [ 750/1251 ( 60%)]  Loss: 3.849 (3.79)  Time: 0.785s, 1304.95/s  (0.812s, 1260.33/s)  LR: 9.507e-04  Data: 0.013 (0.013)
Train: 86 [ 800/1251 ( 64%)]  Loss: 3.470 (3.78)  Time: 0.826s, 1239.57/s  (0.812s, 1260.66/s)  LR: 9.507e-04  Data: 0.010 (0.013)
Train: 86 [ 850/1251 ( 68%)]  Loss: 3.555 (3.76)  Time: 0.826s, 1239.73/s  (0.812s, 1260.82/s)  LR: 9.507e-04  Data: 0.010 (0.013)
Train: 86 [ 900/1251 ( 72%)]  Loss: 4.181 (3.79)  Time: 0.823s, 1243.74/s  (0.812s, 1260.64/s)  LR: 9.507e-04  Data: 0.009 (0.013)
Train: 86 [ 950/1251 ( 76%)]  Loss: 3.744 (3.78)  Time: 0.805s, 1271.96/s  (0.812s, 1260.53/s)  LR: 9.507e-04  Data: 0.011 (0.013)
Train: 86 [1000/1251 ( 80%)]  Loss: 3.994 (3.79)  Time: 0.792s, 1293.10/s  (0.813s, 1260.14/s)  LR: 9.507e-04  Data: 0.010 (0.013)
Train: 86 [1050/1251 ( 84%)]  Loss: 3.624 (3.79)  Time: 0.805s, 1272.64/s  (0.813s, 1259.88/s)  LR: 9.507e-04  Data: 0.011 (0.013)
Train: 86 [1100/1251 ( 88%)]  Loss: 3.845 (3.79)  Time: 0.817s, 1253.73/s  (0.812s, 1260.39/s)  LR: 9.507e-04  Data: 0.009 (0.013)
Train: 86 [1150/1251 ( 92%)]  Loss: 3.635 (3.78)  Time: 0.833s, 1229.41/s  (0.812s, 1260.71/s)  LR: 9.507e-04  Data: 0.009 (0.012)
Train: 86 [1200/1251 ( 96%)]  Loss: 3.641 (3.78)  Time: 0.841s, 1217.32/s  (0.812s, 1260.98/s)  LR: 9.507e-04  Data: 0.012 (0.012)
Train: 86 [1250/1251 (100%)]  Loss: 4.119 (3.79)  Time: 0.817s, 1253.45/s  (0.812s, 1260.97/s)  LR: 9.507e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.604 (1.604)  Loss:  1.0000 (1.0000)  Acc@1: 85.5469 (85.5469)  Acc@5: 95.5078 (95.5078)
Test: [  48/48]  Time: 0.194 (0.603)  Loss:  0.9326 (1.5156)  Acc@1: 82.1934 (70.6820)  Acc@5: 95.2830 (90.5040)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-84.pth.tar', 71.28999999267577)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-81.pth.tar', 70.90999996826172)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-82.pth.tar', 70.8740000415039)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-77.pth.tar', 70.83599993896485)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-83.pth.tar', 70.77200006835938)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-73.pth.tar', 70.71800010009765)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-72.pth.tar', 70.7139999658203)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-85.pth.tar', 70.70800004394532)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-86.pth.tar', 70.68200002197266)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-80.pth.tar', 70.57399993896485)

Train: 87 [   0/1251 (  0%)]  Loss: 3.614 (3.61)  Time: 2.276s,  449.86/s  (2.276s,  449.86/s)  LR: 9.495e-04  Data: 1.546 (1.546)
Train: 87 [  50/1251 (  4%)]  Loss: 3.432 (3.52)  Time: 0.780s, 1313.20/s  (0.852s, 1202.53/s)  LR: 9.495e-04  Data: 0.010 (0.046)
Train: 87 [ 100/1251 (  8%)]  Loss: 3.667 (3.57)  Time: 0.779s, 1314.53/s  (0.826s, 1239.40/s)  LR: 9.495e-04  Data: 0.011 (0.028)
Train: 87 [ 150/1251 ( 12%)]  Loss: 3.658 (3.59)  Time: 0.787s, 1301.52/s  (0.820s, 1248.13/s)  LR: 9.495e-04  Data: 0.010 (0.022)
Train: 87 [ 200/1251 ( 16%)]  Loss: 3.246 (3.52)  Time: 0.796s, 1286.63/s  (0.818s, 1251.11/s)  LR: 9.495e-04  Data: 0.010 (0.019)
Train: 87 [ 250/1251 ( 20%)]  Loss: 3.728 (3.56)  Time: 0.842s, 1216.64/s  (0.816s, 1255.06/s)  LR: 9.495e-04  Data: 0.015 (0.018)
Train: 87 [ 300/1251 ( 24%)]  Loss: 3.428 (3.54)  Time: 0.794s, 1290.32/s  (0.815s, 1257.08/s)  LR: 9.495e-04  Data: 0.012 (0.017)
Train: 87 [ 350/1251 ( 28%)]  Loss: 3.994 (3.60)  Time: 0.834s, 1227.12/s  (0.815s, 1256.91/s)  LR: 9.495e-04  Data: 0.011 (0.016)
Train: 87 [ 400/1251 ( 32%)]  Loss: 3.543 (3.59)  Time: 0.779s, 1314.44/s  (0.814s, 1258.09/s)  LR: 9.495e-04  Data: 0.010 (0.015)
Train: 87 [ 450/1251 ( 36%)]  Loss: 3.507 (3.58)  Time: 0.780s, 1313.41/s  (0.813s, 1259.99/s)  LR: 9.495e-04  Data: 0.009 (0.015)
Train: 87 [ 500/1251 ( 40%)]  Loss: 3.921 (3.61)  Time: 0.820s, 1248.71/s  (0.813s, 1259.87/s)  LR: 9.495e-04  Data: 0.009 (0.014)
Train: 87 [ 550/1251 ( 44%)]  Loss: 3.727 (3.62)  Time: 0.777s, 1317.73/s  (0.812s, 1260.89/s)  LR: 9.495e-04  Data: 0.009 (0.014)
Train: 87 [ 600/1251 ( 48%)]  Loss: 3.835 (3.64)  Time: 0.805s, 1272.64/s  (0.812s, 1261.27/s)  LR: 9.495e-04  Data: 0.011 (0.014)
Train: 87 [ 650/1251 ( 52%)]  Loss: 3.607 (3.64)  Time: 0.821s, 1247.35/s  (0.811s, 1262.70/s)  LR: 9.495e-04  Data: 0.009 (0.013)
Train: 87 [ 700/1251 ( 56%)]  Loss: 3.711 (3.64)  Time: 0.829s, 1235.71/s  (0.811s, 1262.79/s)  LR: 9.495e-04  Data: 0.009 (0.013)
Train: 87 [ 750/1251 ( 60%)]  Loss: 4.003 (3.66)  Time: 0.787s, 1300.80/s  (0.811s, 1262.87/s)  LR: 9.495e-04  Data: 0.010 (0.013)
Train: 87 [ 800/1251 ( 64%)]  Loss: 3.994 (3.68)  Time: 0.795s, 1288.76/s  (0.810s, 1263.78/s)  LR: 9.495e-04  Data: 0.012 (0.013)
Train: 87 [ 850/1251 ( 68%)]  Loss: 3.699 (3.68)  Time: 0.800s, 1280.26/s  (0.810s, 1263.81/s)  LR: 9.495e-04  Data: 0.015 (0.013)
Train: 87 [ 900/1251 ( 72%)]  Loss: 3.866 (3.69)  Time: 0.779s, 1314.42/s  (0.810s, 1264.83/s)  LR: 9.495e-04  Data: 0.010 (0.013)
Train: 87 [ 950/1251 ( 76%)]  Loss: 3.591 (3.69)  Time: 0.812s, 1260.86/s  (0.810s, 1264.96/s)  LR: 9.495e-04  Data: 0.010 (0.013)
Train: 87 [1000/1251 ( 80%)]  Loss: 3.815 (3.69)  Time: 0.871s, 1175.65/s  (0.810s, 1264.91/s)  LR: 9.495e-04  Data: 0.009 (0.012)
Train: 87 [1050/1251 ( 84%)]  Loss: 3.821 (3.70)  Time: 0.782s, 1310.08/s  (0.810s, 1264.85/s)  LR: 9.495e-04  Data: 0.010 (0.012)
Train: 87 [1100/1251 ( 88%)]  Loss: 4.097 (3.72)  Time: 0.823s, 1244.40/s  (0.810s, 1264.65/s)  LR: 9.495e-04  Data: 0.009 (0.012)
Train: 87 [1150/1251 ( 92%)]  Loss: 3.781 (3.72)  Time: 0.841s, 1217.27/s  (0.810s, 1264.90/s)  LR: 9.495e-04  Data: 0.009 (0.012)
Train: 87 [1200/1251 ( 96%)]  Loss: 4.013 (3.73)  Time: 0.869s, 1178.74/s  (0.810s, 1264.80/s)  LR: 9.495e-04  Data: 0.009 (0.012)
Train: 87 [1250/1251 (100%)]  Loss: 3.915 (3.74)  Time: 0.793s, 1291.78/s  (0.810s, 1264.90/s)  LR: 9.495e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.608 (1.608)  Loss:  0.9487 (0.9487)  Acc@1: 86.6211 (86.6211)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.194 (0.601)  Loss:  1.1787 (1.5840)  Acc@1: 82.3113 (70.6100)  Acc@5: 94.8113 (90.4060)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-84.pth.tar', 71.28999999267577)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-81.pth.tar', 70.90999996826172)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-82.pth.tar', 70.8740000415039)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-77.pth.tar', 70.83599993896485)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-83.pth.tar', 70.77200006835938)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-73.pth.tar', 70.71800010009765)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-72.pth.tar', 70.7139999658203)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-85.pth.tar', 70.70800004394532)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-86.pth.tar', 70.68200002197266)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-87.pth.tar', 70.61000007324219)

Train: 88 [   0/1251 (  0%)]  Loss: 3.791 (3.79)  Time: 2.389s,  428.60/s  (2.389s,  428.60/s)  LR: 9.484e-04  Data: 1.622 (1.622)
Train: 88 [  50/1251 (  4%)]  Loss: 3.539 (3.66)  Time: 0.819s, 1250.99/s  (0.849s, 1206.09/s)  LR: 9.484e-04  Data: 0.010 (0.046)
Train: 88 [ 100/1251 (  8%)]  Loss: 3.935 (3.75)  Time: 0.801s, 1277.66/s  (0.828s, 1237.36/s)  LR: 9.484e-04  Data: 0.013 (0.028)
Train: 88 [ 150/1251 ( 12%)]  Loss: 3.661 (3.73)  Time: 0.805s, 1271.42/s  (0.820s, 1248.76/s)  LR: 9.484e-04  Data: 0.009 (0.023)
Train: 88 [ 200/1251 ( 16%)]  Loss: 3.881 (3.76)  Time: 0.795s, 1288.51/s  (0.818s, 1252.52/s)  LR: 9.484e-04  Data: 0.011 (0.020)
Train: 88 [ 250/1251 ( 20%)]  Loss: 3.961 (3.79)  Time: 0.815s, 1256.44/s  (0.816s, 1254.34/s)  LR: 9.484e-04  Data: 0.011 (0.018)
Train: 88 [ 300/1251 ( 24%)]  Loss: 3.750 (3.79)  Time: 0.829s, 1235.69/s  (0.816s, 1255.63/s)  LR: 9.484e-04  Data: 0.009 (0.017)
Train: 88 [ 350/1251 ( 28%)]  Loss: 3.873 (3.80)  Time: 0.842s, 1216.44/s  (0.814s, 1257.85/s)  LR: 9.484e-04  Data: 0.010 (0.016)
Train: 88 [ 400/1251 ( 32%)]  Loss: 3.968 (3.82)  Time: 0.776s, 1319.91/s  (0.812s, 1261.50/s)  LR: 9.484e-04  Data: 0.010 (0.015)
Train: 88 [ 450/1251 ( 36%)]  Loss: 3.917 (3.83)  Time: 0.833s, 1229.03/s  (0.812s, 1261.56/s)  LR: 9.484e-04  Data: 0.010 (0.015)
Train: 88 [ 500/1251 ( 40%)]  Loss: 3.724 (3.82)  Time: 0.781s, 1311.37/s  (0.813s, 1260.25/s)  LR: 9.484e-04  Data: 0.009 (0.014)
Train: 88 [ 550/1251 ( 44%)]  Loss: 3.765 (3.81)  Time: 0.809s, 1265.78/s  (0.813s, 1259.67/s)  LR: 9.484e-04  Data: 0.009 (0.014)
Train: 88 [ 600/1251 ( 48%)]  Loss: 4.167 (3.84)  Time: 0.778s, 1315.43/s  (0.812s, 1260.35/s)  LR: 9.484e-04  Data: 0.010 (0.014)
Train: 88 [ 650/1251 ( 52%)]  Loss: 3.504 (3.82)  Time: 0.778s, 1316.59/s  (0.812s, 1261.54/s)  LR: 9.484e-04  Data: 0.009 (0.013)
Train: 88 [ 700/1251 ( 56%)]  Loss: 3.998 (3.83)  Time: 0.792s, 1293.11/s  (0.811s, 1262.02/s)  LR: 9.484e-04  Data: 0.010 (0.013)
Train: 88 [ 750/1251 ( 60%)]  Loss: 4.184 (3.85)  Time: 0.874s, 1171.10/s  (0.811s, 1261.97/s)  LR: 9.484e-04  Data: 0.010 (0.013)
Train: 88 [ 800/1251 ( 64%)]  Loss: 3.857 (3.85)  Time: 0.825s, 1241.41/s  (0.811s, 1262.18/s)  LR: 9.484e-04  Data: 0.010 (0.013)
Train: 88 [ 850/1251 ( 68%)]  Loss: 3.792 (3.85)  Time: 0.823s, 1243.49/s  (0.811s, 1262.24/s)  LR: 9.484e-04  Data: 0.010 (0.013)
Train: 88 [ 900/1251 ( 72%)]  Loss: 3.472 (3.83)  Time: 0.798s, 1282.91/s  (0.811s, 1262.91/s)  LR: 9.484e-04  Data: 0.009 (0.013)
Train: 88 [ 950/1251 ( 76%)]  Loss: 3.411 (3.81)  Time: 0.824s, 1242.03/s  (0.811s, 1263.18/s)  LR: 9.484e-04  Data: 0.009 (0.013)
Train: 88 [1000/1251 ( 80%)]  Loss: 3.842 (3.81)  Time: 0.778s, 1315.71/s  (0.811s, 1263.32/s)  LR: 9.484e-04  Data: 0.010 (0.012)
Train: 88 [1050/1251 ( 84%)]  Loss: 3.975 (3.82)  Time: 0.779s, 1314.28/s  (0.811s, 1263.36/s)  LR: 9.484e-04  Data: 0.009 (0.012)
Train: 88 [1100/1251 ( 88%)]  Loss: 3.679 (3.81)  Time: 0.808s, 1267.82/s  (0.810s, 1263.79/s)  LR: 9.484e-04  Data: 0.014 (0.012)
Train: 88 [1150/1251 ( 92%)]  Loss: 3.915 (3.82)  Time: 0.777s, 1317.45/s  (0.810s, 1263.89/s)  LR: 9.484e-04  Data: 0.009 (0.012)
Train: 88 [1200/1251 ( 96%)]  Loss: 3.787 (3.81)  Time: 0.774s, 1322.15/s  (0.810s, 1263.93/s)  LR: 9.484e-04  Data: 0.010 (0.012)
Train: 88 [1250/1251 (100%)]  Loss: 3.776 (3.81)  Time: 0.794s, 1290.38/s  (0.810s, 1263.85/s)  LR: 9.484e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.650 (1.650)  Loss:  0.9482 (0.9482)  Acc@1: 86.5234 (86.5234)  Acc@5: 95.8984 (95.8984)
Test: [  48/48]  Time: 0.193 (0.613)  Loss:  0.8877 (1.4222)  Acc@1: 82.9009 (71.4360)  Acc@5: 95.2830 (90.7920)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-88.pth.tar', 71.43600007080079)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-84.pth.tar', 71.28999999267577)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-81.pth.tar', 70.90999996826172)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-82.pth.tar', 70.8740000415039)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-77.pth.tar', 70.83599993896485)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-83.pth.tar', 70.77200006835938)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-73.pth.tar', 70.71800010009765)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-72.pth.tar', 70.7139999658203)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-85.pth.tar', 70.70800004394532)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-86.pth.tar', 70.68200002197266)

Train: 89 [   0/1251 (  0%)]  Loss: 3.637 (3.64)  Time: 2.604s,  393.20/s  (2.604s,  393.20/s)  LR: 9.472e-04  Data: 1.876 (1.876)
Train: 89 [  50/1251 (  4%)]  Loss: 3.772 (3.70)  Time: 0.814s, 1258.08/s  (0.848s, 1207.66/s)  LR: 9.472e-04  Data: 0.009 (0.053)
Train: 89 [ 100/1251 (  8%)]  Loss: 3.587 (3.67)  Time: 0.783s, 1307.72/s  (0.832s, 1231.25/s)  LR: 9.472e-04  Data: 0.010 (0.032)
Train: 89 [ 150/1251 ( 12%)]  Loss: 3.612 (3.65)  Time: 0.817s, 1254.13/s  (0.825s, 1240.71/s)  LR: 9.472e-04  Data: 0.009 (0.025)
Train: 89 [ 200/1251 ( 16%)]  Loss: 3.801 (3.68)  Time: 0.777s, 1317.36/s  (0.822s, 1245.50/s)  LR: 9.472e-04  Data: 0.010 (0.021)
Train: 89 [ 250/1251 ( 20%)]  Loss: 3.543 (3.66)  Time: 0.783s, 1308.28/s  (0.819s, 1251.01/s)  LR: 9.472e-04  Data: 0.013 (0.019)
Train: 89 [ 300/1251 ( 24%)]  Loss: 3.671 (3.66)  Time: 0.882s, 1161.24/s  (0.817s, 1253.70/s)  LR: 9.472e-04  Data: 0.010 (0.018)
Train: 89 [ 350/1251 ( 28%)]  Loss: 3.787 (3.68)  Time: 0.772s, 1325.61/s  (0.815s, 1256.09/s)  LR: 9.472e-04  Data: 0.009 (0.017)
Train: 89 [ 400/1251 ( 32%)]  Loss: 3.628 (3.67)  Time: 0.800s, 1280.47/s  (0.815s, 1256.61/s)  LR: 9.472e-04  Data: 0.015 (0.016)
Train: 89 [ 450/1251 ( 36%)]  Loss: 3.796 (3.68)  Time: 0.796s, 1286.29/s  (0.815s, 1256.97/s)  LR: 9.472e-04  Data: 0.016 (0.015)
Train: 89 [ 500/1251 ( 40%)]  Loss: 3.918 (3.70)  Time: 0.817s, 1253.32/s  (0.814s, 1258.20/s)  LR: 9.472e-04  Data: 0.010 (0.015)
Train: 89 [ 550/1251 ( 44%)]  Loss: 3.610 (3.70)  Time: 0.806s, 1269.89/s  (0.815s, 1255.75/s)  LR: 9.472e-04  Data: 0.011 (0.015)
Train: 89 [ 600/1251 ( 48%)]  Loss: 3.832 (3.71)  Time: 0.781s, 1311.83/s  (0.815s, 1256.59/s)  LR: 9.472e-04  Data: 0.010 (0.014)
Train: 89 [ 650/1251 ( 52%)]  Loss: 3.735 (3.71)  Time: 0.779s, 1315.01/s  (0.812s, 1260.52/s)  LR: 9.472e-04  Data: 0.009 (0.014)
Train: 89 [ 700/1251 ( 56%)]  Loss: 3.599 (3.70)  Time: 0.787s, 1300.99/s  (0.812s, 1261.02/s)  LR: 9.472e-04  Data: 0.011 (0.014)
Train: 89 [ 750/1251 ( 60%)]  Loss: 3.715 (3.70)  Time: 0.784s, 1306.13/s  (0.811s, 1261.92/s)  LR: 9.472e-04  Data: 0.010 (0.014)
Train: 89 [ 800/1251 ( 64%)]  Loss: 3.806 (3.71)  Time: 0.793s, 1291.63/s  (0.812s, 1261.78/s)  LR: 9.472e-04  Data: 0.012 (0.013)
Train: 89 [ 850/1251 ( 68%)]  Loss: 3.747 (3.71)  Time: 0.808s, 1267.87/s  (0.812s, 1261.51/s)  LR: 9.472e-04  Data: 0.012 (0.013)
Train: 89 [ 900/1251 ( 72%)]  Loss: 3.934 (3.72)  Time: 0.809s, 1265.90/s  (0.812s, 1260.90/s)  LR: 9.472e-04  Data: 0.013 (0.013)
Train: 89 [ 950/1251 ( 76%)]  Loss: 3.751 (3.72)  Time: 0.809s, 1266.27/s  (0.812s, 1260.80/s)  LR: 9.472e-04  Data: 0.010 (0.013)
Train: 89 [1000/1251 ( 80%)]  Loss: 4.058 (3.74)  Time: 0.819s, 1250.28/s  (0.812s, 1260.43/s)  LR: 9.472e-04  Data: 0.009 (0.013)
Train: 89 [1050/1251 ( 84%)]  Loss: 3.818 (3.74)  Time: 0.810s, 1263.69/s  (0.812s, 1260.51/s)  LR: 9.472e-04  Data: 0.009 (0.013)
Train: 89 [1100/1251 ( 88%)]  Loss: 3.639 (3.74)  Time: 0.837s, 1223.42/s  (0.813s, 1260.10/s)  LR: 9.472e-04  Data: 0.009 (0.013)
Train: 89 [1150/1251 ( 92%)]  Loss: 3.898 (3.75)  Time: 0.814s, 1257.22/s  (0.813s, 1260.26/s)  LR: 9.472e-04  Data: 0.010 (0.013)
Train: 89 [1200/1251 ( 96%)]  Loss: 3.364 (3.73)  Time: 0.825s, 1241.06/s  (0.813s, 1260.29/s)  LR: 9.472e-04  Data: 0.009 (0.012)
Train: 89 [1250/1251 (100%)]  Loss: 3.713 (3.73)  Time: 0.824s, 1242.90/s  (0.812s, 1260.57/s)  LR: 9.472e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.748 (1.748)  Loss:  0.9805 (0.9805)  Acc@1: 86.5234 (86.5234)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.194 (0.608)  Loss:  0.9980 (1.5075)  Acc@1: 83.6085 (71.4560)  Acc@5: 95.8727 (90.8300)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-89.pth.tar', 71.45599999023437)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-88.pth.tar', 71.43600007080079)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-84.pth.tar', 71.28999999267577)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-81.pth.tar', 70.90999996826172)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-82.pth.tar', 70.8740000415039)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-77.pth.tar', 70.83599993896485)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-83.pth.tar', 70.77200006835938)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-73.pth.tar', 70.71800010009765)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-72.pth.tar', 70.7139999658203)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-85.pth.tar', 70.70800004394532)

Train: 90 [   0/1251 (  0%)]  Loss: 3.729 (3.73)  Time: 2.353s,  435.26/s  (2.353s,  435.26/s)  LR: 9.460e-04  Data: 1.634 (1.634)
Train: 90 [  50/1251 (  4%)]  Loss: 3.683 (3.71)  Time: 0.801s, 1278.77/s  (0.846s, 1210.34/s)  LR: 9.460e-04  Data: 0.010 (0.049)
Train: 90 [ 100/1251 (  8%)]  Loss: 3.985 (3.80)  Time: 0.779s, 1314.32/s  (0.828s, 1236.16/s)  LR: 9.460e-04  Data: 0.010 (0.030)
Train: 90 [ 150/1251 ( 12%)]  Loss: 3.360 (3.69)  Time: 0.770s, 1329.09/s  (0.823s, 1244.41/s)  LR: 9.460e-04  Data: 0.011 (0.023)
Train: 90 [ 200/1251 ( 16%)]  Loss: 3.623 (3.68)  Time: 0.779s, 1314.18/s  (0.819s, 1250.34/s)  LR: 9.460e-04  Data: 0.010 (0.020)
Train: 90 [ 250/1251 ( 20%)]  Loss: 3.844 (3.70)  Time: 0.824s, 1242.02/s  (0.818s, 1251.27/s)  LR: 9.460e-04  Data: 0.010 (0.018)
Train: 90 [ 300/1251 ( 24%)]  Loss: 3.527 (3.68)  Time: 0.780s, 1312.92/s  (0.816s, 1255.04/s)  LR: 9.460e-04  Data: 0.011 (0.017)
Train: 90 [ 350/1251 ( 28%)]  Loss: 3.443 (3.65)  Time: 0.853s, 1200.92/s  (0.815s, 1256.32/s)  LR: 9.460e-04  Data: 0.013 (0.016)
Train: 90 [ 400/1251 ( 32%)]  Loss: 3.815 (3.67)  Time: 0.803s, 1275.38/s  (0.815s, 1256.45/s)  LR: 9.460e-04  Data: 0.013 (0.015)
Train: 90 [ 450/1251 ( 36%)]  Loss: 3.714 (3.67)  Time: 0.850s, 1205.04/s  (0.815s, 1256.97/s)  LR: 9.460e-04  Data: 0.010 (0.015)
Train: 90 [ 500/1251 ( 40%)]  Loss: 3.747 (3.68)  Time: 0.780s, 1312.20/s  (0.814s, 1258.44/s)  LR: 9.460e-04  Data: 0.010 (0.015)
Train: 90 [ 550/1251 ( 44%)]  Loss: 3.684 (3.68)  Time: 0.794s, 1289.74/s  (0.813s, 1259.37/s)  LR: 9.460e-04  Data: 0.009 (0.014)
Train: 90 [ 600/1251 ( 48%)]  Loss: 3.899 (3.70)  Time: 0.779s, 1315.13/s  (0.812s, 1260.41/s)  LR: 9.460e-04  Data: 0.010 (0.014)
Train: 90 [ 650/1251 ( 52%)]  Loss: 3.901 (3.71)  Time: 0.810s, 1264.88/s  (0.812s, 1260.68/s)  LR: 9.460e-04  Data: 0.009 (0.014)
Train: 90 [ 700/1251 ( 56%)]  Loss: 3.939 (3.73)  Time: 0.777s, 1317.63/s  (0.812s, 1260.45/s)  LR: 9.460e-04  Data: 0.010 (0.013)
Train: 90 [ 750/1251 ( 60%)]  Loss: 3.717 (3.73)  Time: 0.791s, 1294.55/s  (0.812s, 1261.36/s)  LR: 9.460e-04  Data: 0.010 (0.013)
Train: 90 [ 800/1251 ( 64%)]  Loss: 4.041 (3.74)  Time: 0.776s, 1320.13/s  (0.811s, 1261.91/s)  LR: 9.460e-04  Data: 0.010 (0.013)
Train: 90 [ 850/1251 ( 68%)]  Loss: 3.331 (3.72)  Time: 0.791s, 1294.37/s  (0.811s, 1262.36/s)  LR: 9.460e-04  Data: 0.015 (0.013)
Train: 90 [ 900/1251 ( 72%)]  Loss: 3.562 (3.71)  Time: 0.804s, 1272.86/s  (0.811s, 1263.20/s)  LR: 9.460e-04  Data: 0.009 (0.013)
Train: 90 [ 950/1251 ( 76%)]  Loss: 3.217 (3.69)  Time: 0.804s, 1273.95/s  (0.811s, 1262.75/s)  LR: 9.460e-04  Data: 0.009 (0.013)
Train: 90 [1000/1251 ( 80%)]  Loss: 3.647 (3.69)  Time: 0.860s, 1190.82/s  (0.811s, 1262.44/s)  LR: 9.460e-04  Data: 0.010 (0.013)
Train: 90 [1050/1251 ( 84%)]  Loss: 3.777 (3.69)  Time: 0.777s, 1317.06/s  (0.811s, 1262.06/s)  LR: 9.460e-04  Data: 0.011 (0.012)
Train: 90 [1100/1251 ( 88%)]  Loss: 4.039 (3.71)  Time: 0.812s, 1261.37/s  (0.811s, 1262.08/s)  LR: 9.460e-04  Data: 0.009 (0.012)
Train: 90 [1150/1251 ( 92%)]  Loss: 3.658 (3.70)  Time: 0.776s, 1318.95/s  (0.811s, 1262.63/s)  LR: 9.460e-04  Data: 0.010 (0.012)
Train: 90 [1200/1251 ( 96%)]  Loss: 3.864 (3.71)  Time: 0.812s, 1261.86/s  (0.811s, 1262.44/s)  LR: 9.460e-04  Data: 0.009 (0.012)
Train: 90 [1250/1251 (100%)]  Loss: 3.755 (3.71)  Time: 0.801s, 1278.21/s  (0.811s, 1262.51/s)  LR: 9.460e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.644 (1.644)  Loss:  0.9521 (0.9521)  Acc@1: 87.6953 (87.6953)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.194 (0.605)  Loss:  0.9331 (1.5008)  Acc@1: 85.0236 (71.7160)  Acc@5: 95.5189 (90.9600)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-90.pth.tar', 71.7159999584961)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-89.pth.tar', 71.45599999023437)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-88.pth.tar', 71.43600007080079)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-84.pth.tar', 71.28999999267577)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-81.pth.tar', 70.90999996826172)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-82.pth.tar', 70.8740000415039)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-77.pth.tar', 70.83599993896485)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-83.pth.tar', 70.77200006835938)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-73.pth.tar', 70.71800010009765)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-72.pth.tar', 70.7139999658203)

Train: 91 [   0/1251 (  0%)]  Loss: 3.436 (3.44)  Time: 2.337s,  438.25/s  (2.337s,  438.25/s)  LR: 9.449e-04  Data: 1.608 (1.608)
Train: 91 [  50/1251 (  4%)]  Loss: 4.015 (3.73)  Time: 0.823s, 1244.38/s  (0.844s, 1213.09/s)  LR: 9.449e-04  Data: 0.010 (0.047)
Train: 91 [ 100/1251 (  8%)]  Loss: 3.799 (3.75)  Time: 0.802s, 1276.03/s  (0.826s, 1240.43/s)  LR: 9.449e-04  Data: 0.009 (0.029)
Train: 91 [ 150/1251 ( 12%)]  Loss: 3.725 (3.74)  Time: 0.779s, 1314.40/s  (0.821s, 1247.81/s)  LR: 9.449e-04  Data: 0.009 (0.023)
Train: 91 [ 200/1251 ( 16%)]  Loss: 3.757 (3.75)  Time: 0.865s, 1184.33/s  (0.817s, 1253.16/s)  LR: 9.449e-04  Data: 0.011 (0.020)
Train: 91 [ 250/1251 ( 20%)]  Loss: 3.735 (3.74)  Time: 0.797s, 1284.69/s  (0.815s, 1256.47/s)  LR: 9.449e-04  Data: 0.009 (0.018)
Train: 91 [ 300/1251 ( 24%)]  Loss: 3.764 (3.75)  Time: 0.831s, 1232.38/s  (0.814s, 1258.56/s)  LR: 9.449e-04  Data: 0.010 (0.017)
Train: 91 [ 350/1251 ( 28%)]  Loss: 3.858 (3.76)  Time: 0.777s, 1318.36/s  (0.813s, 1260.04/s)  LR: 9.449e-04  Data: 0.009 (0.016)
Train: 91 [ 400/1251 ( 32%)]  Loss: 3.810 (3.77)  Time: 0.780s, 1313.15/s  (0.812s, 1260.93/s)  LR: 9.449e-04  Data: 0.009 (0.015)
Train: 91 [ 450/1251 ( 36%)]  Loss: 3.659 (3.76)  Time: 0.882s, 1161.09/s  (0.812s, 1261.09/s)  LR: 9.449e-04  Data: 0.009 (0.015)
Train: 91 [ 500/1251 ( 40%)]  Loss: 3.235 (3.71)  Time: 0.795s, 1288.00/s  (0.812s, 1260.50/s)  LR: 9.449e-04  Data: 0.012 (0.014)
Train: 91 [ 550/1251 ( 44%)]  Loss: 3.698 (3.71)  Time: 0.820s, 1248.22/s  (0.812s, 1260.43/s)  LR: 9.449e-04  Data: 0.012 (0.014)
Train: 91 [ 600/1251 ( 48%)]  Loss: 3.970 (3.73)  Time: 0.838s, 1221.88/s  (0.812s, 1261.23/s)  LR: 9.449e-04  Data: 0.015 (0.014)
Train: 91 [ 650/1251 ( 52%)]  Loss: 3.524 (3.71)  Time: 0.812s, 1260.85/s  (0.811s, 1262.20/s)  LR: 9.449e-04  Data: 0.009 (0.013)
Train: 91 [ 700/1251 ( 56%)]  Loss: 3.946 (3.73)  Time: 0.777s, 1317.71/s  (0.811s, 1262.17/s)  LR: 9.449e-04  Data: 0.011 (0.013)
Train: 91 [ 750/1251 ( 60%)]  Loss: 3.407 (3.71)  Time: 0.774s, 1322.53/s  (0.811s, 1262.21/s)  LR: 9.449e-04  Data: 0.012 (0.013)
Train: 91 [ 800/1251 ( 64%)]  Loss: 3.760 (3.71)  Time: 0.842s, 1216.79/s  (0.812s, 1261.84/s)  LR: 9.449e-04  Data: 0.010 (0.013)
Train: 91 [ 850/1251 ( 68%)]  Loss: 3.565 (3.70)  Time: 0.808s, 1266.55/s  (0.812s, 1261.25/s)  LR: 9.449e-04  Data: 0.010 (0.013)
Train: 91 [ 900/1251 ( 72%)]  Loss: 3.950 (3.72)  Time: 0.785s, 1304.65/s  (0.812s, 1261.66/s)  LR: 9.449e-04  Data: 0.015 (0.013)
Train: 91 [ 950/1251 ( 76%)]  Loss: 3.878 (3.72)  Time: 0.804s, 1274.18/s  (0.812s, 1261.79/s)  LR: 9.449e-04  Data: 0.012 (0.013)
Train: 91 [1000/1251 ( 80%)]  Loss: 4.058 (3.74)  Time: 0.790s, 1296.47/s  (0.811s, 1262.41/s)  LR: 9.449e-04  Data: 0.009 (0.012)
Train: 91 [1050/1251 ( 84%)]  Loss: 3.959 (3.75)  Time: 0.823s, 1244.23/s  (0.811s, 1263.13/s)  LR: 9.449e-04  Data: 0.010 (0.012)
Train: 91 [1100/1251 ( 88%)]  Loss: 3.894 (3.76)  Time: 0.833s, 1228.58/s  (0.811s, 1263.34/s)  LR: 9.449e-04  Data: 0.009 (0.012)
Train: 91 [1150/1251 ( 92%)]  Loss: 3.502 (3.75)  Time: 0.791s, 1293.77/s  (0.811s, 1262.95/s)  LR: 9.449e-04  Data: 0.009 (0.012)
Train: 91 [1200/1251 ( 96%)]  Loss: 3.633 (3.74)  Time: 0.779s, 1313.95/s  (0.811s, 1263.42/s)  LR: 9.449e-04  Data: 0.010 (0.012)
Train: 91 [1250/1251 (100%)]  Loss: 3.408 (3.73)  Time: 0.765s, 1338.75/s  (0.811s, 1263.02/s)  LR: 9.449e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.740 (1.740)  Loss:  1.0859 (1.0859)  Acc@1: 85.9375 (85.9375)  Acc@5: 96.1914 (96.1914)
Test: [  48/48]  Time: 0.194 (0.597)  Loss:  0.9897 (1.5220)  Acc@1: 84.1981 (70.8280)  Acc@5: 94.3396 (90.4460)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-90.pth.tar', 71.7159999584961)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-89.pth.tar', 71.45599999023437)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-88.pth.tar', 71.43600007080079)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-84.pth.tar', 71.28999999267577)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-81.pth.tar', 70.90999996826172)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-82.pth.tar', 70.8740000415039)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-77.pth.tar', 70.83599993896485)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-91.pth.tar', 70.8280001171875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-83.pth.tar', 70.77200006835938)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-73.pth.tar', 70.71800010009765)

Train: 92 [   0/1251 (  0%)]  Loss: 3.966 (3.97)  Time: 2.555s,  400.79/s  (2.555s,  400.79/s)  LR: 9.437e-04  Data: 1.792 (1.792)
Train: 92 [  50/1251 (  4%)]  Loss: 3.932 (3.95)  Time: 0.780s, 1312.53/s  (0.837s, 1223.77/s)  LR: 9.437e-04  Data: 0.010 (0.045)
Train: 92 [ 100/1251 (  8%)]  Loss: 3.559 (3.82)  Time: 0.782s, 1309.37/s  (0.820s, 1249.26/s)  LR: 9.437e-04  Data: 0.012 (0.028)
Train: 92 [ 150/1251 ( 12%)]  Loss: 3.829 (3.82)  Time: 0.779s, 1313.88/s  (0.814s, 1257.82/s)  LR: 9.437e-04  Data: 0.010 (0.022)
Train: 92 [ 200/1251 ( 16%)]  Loss: 3.660 (3.79)  Time: 0.778s, 1316.15/s  (0.813s, 1259.34/s)  LR: 9.437e-04  Data: 0.009 (0.020)
Train: 92 [ 250/1251 ( 20%)]  Loss: 3.710 (3.78)  Time: 0.811s, 1263.12/s  (0.813s, 1259.86/s)  LR: 9.437e-04  Data: 0.011 (0.018)
Train: 92 [ 300/1251 ( 24%)]  Loss: 3.876 (3.79)  Time: 0.795s, 1288.00/s  (0.812s, 1261.82/s)  LR: 9.437e-04  Data: 0.010 (0.017)
Train: 92 [ 350/1251 ( 28%)]  Loss: 3.665 (3.77)  Time: 0.850s, 1205.08/s  (0.812s, 1261.30/s)  LR: 9.437e-04  Data: 0.009 (0.016)
Train: 92 [ 400/1251 ( 32%)]  Loss: 3.720 (3.77)  Time: 0.803s, 1275.83/s  (0.811s, 1261.91/s)  LR: 9.437e-04  Data: 0.011 (0.015)
Train: 92 [ 450/1251 ( 36%)]  Loss: 3.698 (3.76)  Time: 0.785s, 1304.35/s  (0.811s, 1263.20/s)  LR: 9.437e-04  Data: 0.013 (0.015)
Train: 92 [ 500/1251 ( 40%)]  Loss: 4.071 (3.79)  Time: 0.784s, 1306.85/s  (0.810s, 1263.69/s)  LR: 9.437e-04  Data: 0.010 (0.014)
Train: 92 [ 550/1251 ( 44%)]  Loss: 4.133 (3.82)  Time: 0.787s, 1301.86/s  (0.811s, 1263.28/s)  LR: 9.437e-04  Data: 0.009 (0.014)
Train: 92 [ 600/1251 ( 48%)]  Loss: 3.881 (3.82)  Time: 0.786s, 1302.77/s  (0.811s, 1263.13/s)  LR: 9.437e-04  Data: 0.014 (0.014)
Train: 92 [ 650/1251 ( 52%)]  Loss: 3.660 (3.81)  Time: 0.857s, 1195.29/s  (0.811s, 1263.30/s)  LR: 9.437e-04  Data: 0.011 (0.013)
Train: 92 [ 700/1251 ( 56%)]  Loss: 3.790 (3.81)  Time: 0.820s, 1248.80/s  (0.810s, 1263.53/s)  LR: 9.437e-04  Data: 0.013 (0.013)
Train: 92 [ 750/1251 ( 60%)]  Loss: 3.882 (3.81)  Time: 0.779s, 1313.85/s  (0.811s, 1263.08/s)  LR: 9.437e-04  Data: 0.010 (0.013)
Train: 92 [ 800/1251 ( 64%)]  Loss: 3.549 (3.80)  Time: 0.827s, 1237.56/s  (0.810s, 1263.45/s)  LR: 9.437e-04  Data: 0.010 (0.013)
Train: 92 [ 850/1251 ( 68%)]  Loss: 3.816 (3.80)  Time: 0.801s, 1277.98/s  (0.811s, 1263.17/s)  LR: 9.437e-04  Data: 0.010 (0.013)
Train: 92 [ 900/1251 ( 72%)]  Loss: 3.808 (3.80)  Time: 0.779s, 1313.97/s  (0.811s, 1263.10/s)  LR: 9.437e-04  Data: 0.010 (0.013)
Train: 92 [ 950/1251 ( 76%)]  Loss: 3.880 (3.80)  Time: 0.777s, 1318.27/s  (0.810s, 1263.44/s)  LR: 9.437e-04  Data: 0.010 (0.013)
Train: 92 [1000/1251 ( 80%)]  Loss: 3.652 (3.80)  Time: 0.809s, 1265.37/s  (0.811s, 1262.99/s)  LR: 9.437e-04  Data: 0.010 (0.012)
Train: 92 [1050/1251 ( 84%)]  Loss: 3.930 (3.80)  Time: 0.785s, 1304.41/s  (0.811s, 1263.41/s)  LR: 9.437e-04  Data: 0.010 (0.012)
Train: 92 [1100/1251 ( 88%)]  Loss: 4.029 (3.81)  Time: 0.782s, 1309.05/s  (0.810s, 1263.89/s)  LR: 9.437e-04  Data: 0.010 (0.012)
Train: 92 [1150/1251 ( 92%)]  Loss: 3.770 (3.81)  Time: 0.847s, 1209.05/s  (0.810s, 1264.49/s)  LR: 9.437e-04  Data: 0.009 (0.012)
Train: 92 [1200/1251 ( 96%)]  Loss: 3.555 (3.80)  Time: 0.804s, 1274.14/s  (0.810s, 1264.88/s)  LR: 9.437e-04  Data: 0.009 (0.012)
Train: 92 [1250/1251 (100%)]  Loss: 3.669 (3.80)  Time: 0.812s, 1261.34/s  (0.809s, 1265.36/s)  LR: 9.437e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.639 (1.639)  Loss:  0.7568 (0.7568)  Acc@1: 86.0352 (86.0352)  Acc@5: 96.1914 (96.1914)
Test: [  48/48]  Time: 0.193 (0.594)  Loss:  0.9033 (1.3628)  Acc@1: 82.7830 (71.5960)  Acc@5: 95.1651 (90.8460)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-90.pth.tar', 71.7159999584961)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-92.pth.tar', 71.59600001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-89.pth.tar', 71.45599999023437)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-88.pth.tar', 71.43600007080079)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-84.pth.tar', 71.28999999267577)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-81.pth.tar', 70.90999996826172)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-82.pth.tar', 70.8740000415039)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-77.pth.tar', 70.83599993896485)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-91.pth.tar', 70.8280001171875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-83.pth.tar', 70.77200006835938)

Train: 93 [   0/1251 (  0%)]  Loss: 4.269 (4.27)  Time: 2.401s,  426.52/s  (2.401s,  426.52/s)  LR: 9.425e-04  Data: 1.668 (1.668)
Train: 93 [  50/1251 (  4%)]  Loss: 3.532 (3.90)  Time: 0.795s, 1288.41/s  (0.851s, 1202.85/s)  LR: 9.425e-04  Data: 0.017 (0.052)
Train: 93 [ 100/1251 (  8%)]  Loss: 3.963 (3.92)  Time: 0.779s, 1314.89/s  (0.828s, 1236.19/s)  LR: 9.425e-04  Data: 0.010 (0.032)
Train: 93 [ 150/1251 ( 12%)]  Loss: 3.770 (3.88)  Time: 0.782s, 1309.59/s  (0.824s, 1242.17/s)  LR: 9.425e-04  Data: 0.010 (0.025)
Train: 93 [ 200/1251 ( 16%)]  Loss: 3.516 (3.81)  Time: 0.935s, 1095.61/s  (0.820s, 1248.59/s)  LR: 9.425e-04  Data: 0.009 (0.021)
Train: 93 [ 250/1251 ( 20%)]  Loss: 3.767 (3.80)  Time: 0.811s, 1261.95/s  (0.817s, 1253.15/s)  LR: 9.425e-04  Data: 0.009 (0.019)
Train: 93 [ 300/1251 ( 24%)]  Loss: 3.748 (3.79)  Time: 0.794s, 1289.39/s  (0.816s, 1255.02/s)  LR: 9.425e-04  Data: 0.013 (0.018)
Train: 93 [ 350/1251 ( 28%)]  Loss: 3.487 (3.76)  Time: 0.773s, 1325.54/s  (0.815s, 1256.86/s)  LR: 9.425e-04  Data: 0.011 (0.017)
Train: 93 [ 400/1251 ( 32%)]  Loss: 3.604 (3.74)  Time: 0.806s, 1269.99/s  (0.814s, 1257.88/s)  LR: 9.425e-04  Data: 0.009 (0.016)
Train: 93 [ 450/1251 ( 36%)]  Loss: 3.913 (3.76)  Time: 0.801s, 1279.00/s  (0.814s, 1258.26/s)  LR: 9.425e-04  Data: 0.009 (0.015)
Train: 93 [ 500/1251 ( 40%)]  Loss: 3.638 (3.75)  Time: 0.783s, 1307.87/s  (0.813s, 1258.97/s)  LR: 9.425e-04  Data: 0.013 (0.015)
Train: 93 [ 550/1251 ( 44%)]  Loss: 3.922 (3.76)  Time: 0.786s, 1303.31/s  (0.813s, 1259.54/s)  LR: 9.425e-04  Data: 0.009 (0.014)
Train: 93 [ 600/1251 ( 48%)]  Loss: 3.833 (3.77)  Time: 0.862s, 1188.44/s  (0.813s, 1259.39/s)  LR: 9.425e-04  Data: 0.009 (0.014)
Train: 93 [ 650/1251 ( 52%)]  Loss: 3.755 (3.77)  Time: 0.802s, 1276.61/s  (0.813s, 1259.43/s)  LR: 9.425e-04  Data: 0.013 (0.014)
Train: 93 [ 700/1251 ( 56%)]  Loss: 3.884 (3.77)  Time: 0.808s, 1267.62/s  (0.812s, 1260.92/s)  LR: 9.425e-04  Data: 0.010 (0.014)
Train: 93 [ 750/1251 ( 60%)]  Loss: 3.499 (3.76)  Time: 0.812s, 1261.50/s  (0.812s, 1261.34/s)  LR: 9.425e-04  Data: 0.010 (0.013)
Train: 93 [ 800/1251 ( 64%)]  Loss: 3.954 (3.77)  Time: 0.779s, 1315.09/s  (0.812s, 1260.96/s)  LR: 9.425e-04  Data: 0.010 (0.013)
Train: 93 [ 850/1251 ( 68%)]  Loss: 3.743 (3.77)  Time: 0.822s, 1246.38/s  (0.812s, 1261.14/s)  LR: 9.425e-04  Data: 0.010 (0.013)
Train: 93 [ 900/1251 ( 72%)]  Loss: 3.845 (3.77)  Time: 0.779s, 1315.06/s  (0.812s, 1261.84/s)  LR: 9.425e-04  Data: 0.010 (0.013)
Train: 93 [ 950/1251 ( 76%)]  Loss: 4.017 (3.78)  Time: 0.870s, 1177.22/s  (0.811s, 1261.91/s)  LR: 9.425e-04  Data: 0.009 (0.013)
Train: 93 [1000/1251 ( 80%)]  Loss: 3.491 (3.77)  Time: 0.781s, 1311.30/s  (0.811s, 1261.99/s)  LR: 9.425e-04  Data: 0.013 (0.013)
Train: 93 [1050/1251 ( 84%)]  Loss: 3.641 (3.76)  Time: 0.803s, 1275.51/s  (0.811s, 1262.33/s)  LR: 9.425e-04  Data: 0.010 (0.013)
Train: 93 [1100/1251 ( 88%)]  Loss: 3.789 (3.76)  Time: 0.807s, 1269.57/s  (0.811s, 1262.51/s)  LR: 9.425e-04  Data: 0.010 (0.012)
Train: 93 [1150/1251 ( 92%)]  Loss: 3.452 (3.75)  Time: 0.803s, 1275.52/s  (0.811s, 1262.57/s)  LR: 9.425e-04  Data: 0.010 (0.012)
Train: 93 [1200/1251 ( 96%)]  Loss: 3.527 (3.74)  Time: 0.899s, 1139.61/s  (0.811s, 1262.52/s)  LR: 9.425e-04  Data: 0.010 (0.012)
Train: 93 [1250/1251 (100%)]  Loss: 4.170 (3.76)  Time: 0.767s, 1335.89/s  (0.811s, 1262.91/s)  LR: 9.425e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.778 (1.778)  Loss:  0.8936 (0.8936)  Acc@1: 87.0117 (87.0117)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.194 (0.599)  Loss:  0.9624 (1.5082)  Acc@1: 82.9009 (71.2300)  Acc@5: 95.7547 (90.6420)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-90.pth.tar', 71.7159999584961)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-92.pth.tar', 71.59600001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-89.pth.tar', 71.45599999023437)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-88.pth.tar', 71.43600007080079)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-84.pth.tar', 71.28999999267577)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-93.pth.tar', 71.22999994140625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-81.pth.tar', 70.90999996826172)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-82.pth.tar', 70.8740000415039)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-77.pth.tar', 70.83599993896485)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-91.pth.tar', 70.8280001171875)

Train: 94 [   0/1251 (  0%)]  Loss: 3.372 (3.37)  Time: 2.351s,  435.58/s  (2.351s,  435.58/s)  LR: 9.412e-04  Data: 1.615 (1.615)
Train: 94 [  50/1251 (  4%)]  Loss: 3.734 (3.55)  Time: 0.780s, 1312.70/s  (0.845s, 1211.91/s)  LR: 9.412e-04  Data: 0.009 (0.050)
Train: 94 [ 100/1251 (  8%)]  Loss: 3.969 (3.69)  Time: 0.784s, 1306.84/s  (0.827s, 1237.76/s)  LR: 9.412e-04  Data: 0.009 (0.030)
Train: 94 [ 150/1251 ( 12%)]  Loss: 3.608 (3.67)  Time: 0.776s, 1318.79/s  (0.819s, 1249.86/s)  LR: 9.412e-04  Data: 0.010 (0.024)
Train: 94 [ 200/1251 ( 16%)]  Loss: 3.685 (3.67)  Time: 0.778s, 1316.29/s  (0.814s, 1258.24/s)  LR: 9.412e-04  Data: 0.009 (0.020)
Train: 94 [ 250/1251 ( 20%)]  Loss: 3.645 (3.67)  Time: 0.782s, 1310.09/s  (0.807s, 1268.94/s)  LR: 9.412e-04  Data: 0.010 (0.018)
Train: 94 [ 300/1251 ( 24%)]  Loss: 3.597 (3.66)  Time: 0.778s, 1316.43/s  (0.803s, 1275.73/s)  LR: 9.412e-04  Data: 0.010 (0.017)
Train: 94 [ 350/1251 ( 28%)]  Loss: 3.924 (3.69)  Time: 0.778s, 1315.55/s  (0.799s, 1281.16/s)  LR: 9.412e-04  Data: 0.010 (0.016)
Train: 94 [ 400/1251 ( 32%)]  Loss: 3.777 (3.70)  Time: 0.777s, 1317.23/s  (0.797s, 1285.32/s)  LR: 9.412e-04  Data: 0.010 (0.015)
Train: 94 [ 450/1251 ( 36%)]  Loss: 3.795 (3.71)  Time: 0.778s, 1316.20/s  (0.795s, 1288.65/s)  LR: 9.412e-04  Data: 0.010 (0.014)
Train: 94 [ 500/1251 ( 40%)]  Loss: 4.434 (3.78)  Time: 0.778s, 1315.94/s  (0.793s, 1291.09/s)  LR: 9.412e-04  Data: 0.010 (0.014)
Train: 94 [ 550/1251 ( 44%)]  Loss: 3.473 (3.75)  Time: 0.778s, 1316.90/s  (0.792s, 1292.56/s)  LR: 9.412e-04  Data: 0.010 (0.014)
Train: 94 [ 600/1251 ( 48%)]  Loss: 3.779 (3.75)  Time: 0.778s, 1316.04/s  (0.791s, 1293.97/s)  LR: 9.412e-04  Data: 0.010 (0.013)
Train: 94 [ 650/1251 ( 52%)]  Loss: 3.704 (3.75)  Time: 0.780s, 1312.70/s  (0.790s, 1295.40/s)  LR: 9.412e-04  Data: 0.010 (0.013)
Train: 94 [ 700/1251 ( 56%)]  Loss: 3.828 (3.75)  Time: 0.778s, 1315.63/s  (0.790s, 1296.65/s)  LR: 9.412e-04  Data: 0.010 (0.013)
Train: 94 [ 750/1251 ( 60%)]  Loss: 3.530 (3.74)  Time: 0.778s, 1316.05/s  (0.789s, 1297.94/s)  LR: 9.412e-04  Data: 0.010 (0.013)
Train: 94 [ 800/1251 ( 64%)]  Loss: 3.666 (3.74)  Time: 0.778s, 1315.81/s  (0.789s, 1298.04/s)  LR: 9.412e-04  Data: 0.010 (0.012)
Train: 94 [ 850/1251 ( 68%)]  Loss: 3.569 (3.73)  Time: 0.777s, 1318.14/s  (0.788s, 1298.90/s)  LR: 9.412e-04  Data: 0.010 (0.012)
Train: 94 [ 900/1251 ( 72%)]  Loss: 3.433 (3.71)  Time: 0.779s, 1315.01/s  (0.788s, 1299.66/s)  LR: 9.412e-04  Data: 0.010 (0.012)
Train: 94 [ 950/1251 ( 76%)]  Loss: 4.278 (3.74)  Time: 0.779s, 1314.76/s  (0.788s, 1300.03/s)  LR: 9.412e-04  Data: 0.010 (0.012)
Train: 94 [1000/1251 ( 80%)]  Loss: 4.006 (3.75)  Time: 0.777s, 1317.43/s  (0.787s, 1300.59/s)  LR: 9.412e-04  Data: 0.010 (0.012)
Train: 94 [1050/1251 ( 84%)]  Loss: 4.042 (3.77)  Time: 0.777s, 1317.28/s  (0.787s, 1301.12/s)  LR: 9.412e-04  Data: 0.010 (0.012)
Train: 94 [1100/1251 ( 88%)]  Loss: 3.665 (3.76)  Time: 0.778s, 1316.01/s  (0.787s, 1301.79/s)  LR: 9.412e-04  Data: 0.009 (0.012)
Train: 94 [1150/1251 ( 92%)]  Loss: 3.586 (3.75)  Time: 0.779s, 1315.03/s  (0.786s, 1302.41/s)  LR: 9.412e-04  Data: 0.009 (0.012)
Train: 94 [1200/1251 ( 96%)]  Loss: 4.078 (3.77)  Time: 0.778s, 1316.89/s  (0.786s, 1302.88/s)  LR: 9.412e-04  Data: 0.010 (0.011)
Train: 94 [1250/1251 (100%)]  Loss: 3.631 (3.76)  Time: 0.764s, 1339.95/s  (0.786s, 1303.28/s)  LR: 9.412e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.551 (1.551)  Loss:  0.8530 (0.8530)  Acc@1: 87.1094 (87.1094)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.194 (0.558)  Loss:  0.8306 (1.4051)  Acc@1: 83.7264 (71.5760)  Acc@5: 96.1085 (90.8140)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-90.pth.tar', 71.7159999584961)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-92.pth.tar', 71.59600001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-94.pth.tar', 71.57599991210938)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-89.pth.tar', 71.45599999023437)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-88.pth.tar', 71.43600007080079)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-84.pth.tar', 71.28999999267577)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-93.pth.tar', 71.22999994140625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-81.pth.tar', 70.90999996826172)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-82.pth.tar', 70.8740000415039)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-77.pth.tar', 70.83599993896485)

Train: 95 [   0/1251 (  0%)]  Loss: 3.643 (3.64)  Time: 2.528s,  405.00/s  (2.528s,  405.00/s)  LR: 9.400e-04  Data: 1.801 (1.801)
Train: 95 [  50/1251 (  4%)]  Loss: 3.867 (3.75)  Time: 0.778s, 1316.93/s  (0.820s, 1248.38/s)  LR: 9.400e-04  Data: 0.009 (0.051)
Train: 95 [ 100/1251 (  8%)]  Loss: 3.237 (3.58)  Time: 0.780s, 1312.28/s  (0.802s, 1276.73/s)  LR: 9.400e-04  Data: 0.010 (0.030)
Train: 95 [ 150/1251 ( 12%)]  Loss: 3.768 (3.63)  Time: 0.821s, 1246.51/s  (0.808s, 1267.30/s)  LR: 9.400e-04  Data: 0.018 (0.024)
Train: 95 [ 200/1251 ( 16%)]  Loss: 4.273 (3.76)  Time: 0.814s, 1257.66/s  (0.813s, 1259.81/s)  LR: 9.400e-04  Data: 0.010 (0.021)
Train: 95 [ 250/1251 ( 20%)]  Loss: 4.155 (3.82)  Time: 0.814s, 1257.47/s  (0.815s, 1256.49/s)  LR: 9.400e-04  Data: 0.010 (0.019)
Train: 95 [ 300/1251 ( 24%)]  Loss: 3.693 (3.81)  Time: 0.814s, 1257.57/s  (0.815s, 1255.92/s)  LR: 9.400e-04  Data: 0.010 (0.017)
Train: 95 [ 350/1251 ( 28%)]  Loss: 3.837 (3.81)  Time: 0.777s, 1317.09/s  (0.812s, 1261.16/s)  LR: 9.400e-04  Data: 0.010 (0.016)
Train: 95 [ 400/1251 ( 32%)]  Loss: 3.592 (3.79)  Time: 0.778s, 1316.63/s  (0.808s, 1267.37/s)  LR: 9.400e-04  Data: 0.010 (0.016)
Train: 95 [ 450/1251 ( 36%)]  Loss: 3.929 (3.80)  Time: 0.775s, 1322.10/s  (0.808s, 1267.30/s)  LR: 9.400e-04  Data: 0.010 (0.015)
Train: 95 [ 500/1251 ( 40%)]  Loss: 3.834 (3.80)  Time: 0.814s, 1258.39/s  (0.809s, 1266.03/s)  LR: 9.400e-04  Data: 0.011 (0.015)
Train: 95 [ 550/1251 ( 44%)]  Loss: 3.637 (3.79)  Time: 0.814s, 1257.67/s  (0.809s, 1265.02/s)  LR: 9.400e-04  Data: 0.011 (0.014)
Train: 95 [ 600/1251 ( 48%)]  Loss: 3.828 (3.79)  Time: 0.814s, 1257.78/s  (0.810s, 1264.38/s)  LR: 9.400e-04  Data: 0.011 (0.014)
Train: 95 [ 650/1251 ( 52%)]  Loss: 3.167 (3.75)  Time: 0.778s, 1316.60/s  (0.810s, 1264.90/s)  LR: 9.400e-04  Data: 0.010 (0.014)
Train: 95 [ 700/1251 ( 56%)]  Loss: 3.900 (3.76)  Time: 0.779s, 1314.87/s  (0.807s, 1268.36/s)  LR: 9.400e-04  Data: 0.010 (0.013)
Train: 95 [ 750/1251 ( 60%)]  Loss: 4.032 (3.77)  Time: 0.777s, 1317.84/s  (0.806s, 1271.04/s)  LR: 9.400e-04  Data: 0.010 (0.013)
Train: 95 [ 800/1251 ( 64%)]  Loss: 3.907 (3.78)  Time: 0.779s, 1314.44/s  (0.804s, 1273.43/s)  LR: 9.400e-04  Data: 0.010 (0.013)
Train: 95 [ 850/1251 ( 68%)]  Loss: 3.912 (3.79)  Time: 0.778s, 1316.40/s  (0.803s, 1275.64/s)  LR: 9.400e-04  Data: 0.010 (0.013)
Train: 95 [ 900/1251 ( 72%)]  Loss: 3.886 (3.79)  Time: 0.778s, 1316.93/s  (0.801s, 1277.76/s)  LR: 9.400e-04  Data: 0.010 (0.013)
Train: 95 [ 950/1251 ( 76%)]  Loss: 3.735 (3.79)  Time: 0.777s, 1317.08/s  (0.800s, 1279.61/s)  LR: 9.400e-04  Data: 0.010 (0.013)
Train: 95 [1000/1251 ( 80%)]  Loss: 3.847 (3.79)  Time: 0.778s, 1317.00/s  (0.799s, 1281.40/s)  LR: 9.400e-04  Data: 0.010 (0.012)
Train: 95 [1050/1251 ( 84%)]  Loss: 3.603 (3.79)  Time: 0.777s, 1318.21/s  (0.798s, 1282.93/s)  LR: 9.400e-04  Data: 0.010 (0.012)
Train: 95 [1100/1251 ( 88%)]  Loss: 3.572 (3.78)  Time: 0.778s, 1316.69/s  (0.797s, 1284.12/s)  LR: 9.400e-04  Data: 0.010 (0.012)
Train: 95 [1150/1251 ( 92%)]  Loss: 3.798 (3.78)  Time: 0.778s, 1317.01/s  (0.797s, 1285.26/s)  LR: 9.400e-04  Data: 0.010 (0.012)
Train: 95 [1200/1251 ( 96%)]  Loss: 3.176 (3.75)  Time: 0.779s, 1314.20/s  (0.796s, 1286.50/s)  LR: 9.400e-04  Data: 0.010 (0.012)
Train: 95 [1250/1251 (100%)]  Loss: 3.758 (3.75)  Time: 0.763s, 1341.53/s  (0.795s, 1287.50/s)  LR: 9.400e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.664 (1.664)  Loss:  0.7998 (0.7998)  Acc@1: 88.6719 (88.6719)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.193 (0.556)  Loss:  0.8145 (1.4304)  Acc@1: 84.7877 (71.3780)  Acc@5: 95.7547 (90.7560)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-90.pth.tar', 71.7159999584961)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-92.pth.tar', 71.59600001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-94.pth.tar', 71.57599991210938)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-89.pth.tar', 71.45599999023437)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-88.pth.tar', 71.43600007080079)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-95.pth.tar', 71.37799998535156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-84.pth.tar', 71.28999999267577)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-93.pth.tar', 71.22999994140625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-81.pth.tar', 70.90999996826172)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-82.pth.tar', 70.8740000415039)

Train: 96 [   0/1251 (  0%)]  Loss: 3.755 (3.76)  Time: 2.197s,  466.11/s  (2.197s,  466.11/s)  LR: 9.388e-04  Data: 1.468 (1.468)
Train: 96 [  50/1251 (  4%)]  Loss: 4.097 (3.93)  Time: 0.778s, 1316.78/s  (0.813s, 1259.10/s)  LR: 9.388e-04  Data: 0.010 (0.045)
Train: 96 [ 100/1251 (  8%)]  Loss: 3.768 (3.87)  Time: 0.778s, 1316.73/s  (0.797s, 1285.44/s)  LR: 9.388e-04  Data: 0.010 (0.028)
Train: 96 [ 150/1251 ( 12%)]  Loss: 4.189 (3.95)  Time: 0.777s, 1317.40/s  (0.792s, 1293.27/s)  LR: 9.388e-04  Data: 0.010 (0.022)
Train: 96 [ 200/1251 ( 16%)]  Loss: 3.561 (3.87)  Time: 0.779s, 1313.99/s  (0.789s, 1297.05/s)  LR: 9.388e-04  Data: 0.010 (0.019)
Train: 96 [ 250/1251 ( 20%)]  Loss: 3.449 (3.80)  Time: 0.778s, 1316.56/s  (0.788s, 1299.31/s)  LR: 9.388e-04  Data: 0.010 (0.017)
Train: 96 [ 300/1251 ( 24%)]  Loss: 3.854 (3.81)  Time: 0.778s, 1315.90/s  (0.788s, 1299.33/s)  LR: 9.388e-04  Data: 0.010 (0.016)
Train: 96 [ 350/1251 ( 28%)]  Loss: 3.694 (3.80)  Time: 0.779s, 1315.16/s  (0.787s, 1300.91/s)  LR: 9.388e-04  Data: 0.010 (0.015)
Train: 96 [ 400/1251 ( 32%)]  Loss: 3.653 (3.78)  Time: 0.845s, 1211.94/s  (0.788s, 1299.15/s)  LR: 9.388e-04  Data: 0.010 (0.014)
Train: 96 [ 450/1251 ( 36%)]  Loss: 4.020 (3.80)  Time: 0.824s, 1242.05/s  (0.788s, 1298.74/s)  LR: 9.388e-04  Data: 0.010 (0.014)
Train: 96 [ 500/1251 ( 40%)]  Loss: 3.679 (3.79)  Time: 0.778s, 1316.46/s  (0.788s, 1299.88/s)  LR: 9.388e-04  Data: 0.010 (0.013)
Train: 96 [ 550/1251 ( 44%)]  Loss: 3.497 (3.77)  Time: 0.779s, 1315.23/s  (0.787s, 1300.83/s)  LR: 9.388e-04  Data: 0.010 (0.013)
Train: 96 [ 600/1251 ( 48%)]  Loss: 3.763 (3.77)  Time: 0.782s, 1310.20/s  (0.787s, 1301.77/s)  LR: 9.388e-04  Data: 0.010 (0.013)
Train: 96 [ 650/1251 ( 52%)]  Loss: 3.451 (3.74)  Time: 0.778s, 1315.71/s  (0.786s, 1302.37/s)  LR: 9.388e-04  Data: 0.010 (0.013)
Train: 96 [ 700/1251 ( 56%)]  Loss: 3.817 (3.75)  Time: 0.779s, 1314.51/s  (0.786s, 1303.05/s)  LR: 9.388e-04  Data: 0.010 (0.012)
Train: 96 [ 750/1251 ( 60%)]  Loss: 3.461 (3.73)  Time: 0.778s, 1316.07/s  (0.785s, 1303.84/s)  LR: 9.388e-04  Data: 0.010 (0.012)
Train: 96 [ 800/1251 ( 64%)]  Loss: 3.795 (3.74)  Time: 0.778s, 1315.72/s  (0.786s, 1302.46/s)  LR: 9.388e-04  Data: 0.010 (0.012)
Train: 96 [ 850/1251 ( 68%)]  Loss: 3.573 (3.73)  Time: 0.778s, 1315.63/s  (0.786s, 1302.87/s)  LR: 9.388e-04  Data: 0.010 (0.012)
Train: 96 [ 900/1251 ( 72%)]  Loss: 3.544 (3.72)  Time: 0.778s, 1315.56/s  (0.786s, 1303.43/s)  LR: 9.388e-04  Data: 0.010 (0.012)
Train: 96 [ 950/1251 ( 76%)]  Loss: 3.540 (3.71)  Time: 0.779s, 1315.22/s  (0.785s, 1303.81/s)  LR: 9.388e-04  Data: 0.010 (0.012)
Train: 96 [1000/1251 ( 80%)]  Loss: 3.917 (3.72)  Time: 0.777s, 1317.21/s  (0.785s, 1304.27/s)  LR: 9.388e-04  Data: 0.010 (0.012)
Train: 96 [1050/1251 ( 84%)]  Loss: 3.575 (3.71)  Time: 0.778s, 1316.85/s  (0.787s, 1301.61/s)  LR: 9.388e-04  Data: 0.010 (0.012)
Train: 96 [1100/1251 ( 88%)]  Loss: 3.567 (3.71)  Time: 0.822s, 1245.57/s  (0.788s, 1299.88/s)  LR: 9.388e-04  Data: 0.010 (0.011)
Train: 96 [1150/1251 ( 92%)]  Loss: 3.685 (3.70)  Time: 0.792s, 1292.76/s  (0.789s, 1298.30/s)  LR: 9.388e-04  Data: 0.010 (0.011)
Train: 96 [1200/1251 ( 96%)]  Loss: 3.981 (3.72)  Time: 0.823s, 1244.93/s  (0.789s, 1297.85/s)  LR: 9.388e-04  Data: 0.010 (0.011)
Train: 96 [1250/1251 (100%)]  Loss: 3.368 (3.70)  Time: 0.763s, 1341.72/s  (0.789s, 1298.55/s)  LR: 9.388e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.498 (1.498)  Loss:  0.8291 (0.8291)  Acc@1: 88.0859 (88.0859)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.194 (0.563)  Loss:  1.0107 (1.4910)  Acc@1: 83.4906 (71.3720)  Acc@5: 94.9293 (90.7240)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-90.pth.tar', 71.7159999584961)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-92.pth.tar', 71.59600001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-94.pth.tar', 71.57599991210938)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-89.pth.tar', 71.45599999023437)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-88.pth.tar', 71.43600007080079)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-95.pth.tar', 71.37799998535156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-96.pth.tar', 71.37199993896485)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-84.pth.tar', 71.28999999267577)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-93.pth.tar', 71.22999994140625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-81.pth.tar', 70.90999996826172)

Train: 97 [   0/1251 (  0%)]  Loss: 3.937 (3.94)  Time: 2.287s,  447.82/s  (2.287s,  447.82/s)  LR: 9.375e-04  Data: 1.513 (1.513)
Train: 97 [  50/1251 (  4%)]  Loss: 3.902 (3.92)  Time: 0.778s, 1315.95/s  (0.814s, 1258.47/s)  LR: 9.375e-04  Data: 0.010 (0.040)
Train: 97 [ 100/1251 (  8%)]  Loss: 3.442 (3.76)  Time: 0.775s, 1321.64/s  (0.798s, 1283.81/s)  LR: 9.375e-04  Data: 0.010 (0.025)
Train: 97 [ 150/1251 ( 12%)]  Loss: 3.927 (3.80)  Time: 0.779s, 1315.20/s  (0.793s, 1291.81/s)  LR: 9.375e-04  Data: 0.010 (0.020)
Train: 97 [ 200/1251 ( 16%)]  Loss: 3.851 (3.81)  Time: 0.779s, 1315.27/s  (0.792s, 1292.97/s)  LR: 9.375e-04  Data: 0.010 (0.018)
Train: 97 [ 250/1251 ( 20%)]  Loss: 3.675 (3.79)  Time: 0.778s, 1315.58/s  (0.790s, 1296.98/s)  LR: 9.375e-04  Data: 0.010 (0.016)
Train: 97 [ 300/1251 ( 24%)]  Loss: 4.116 (3.84)  Time: 0.848s, 1208.19/s  (0.788s, 1299.69/s)  LR: 9.375e-04  Data: 0.009 (0.015)
Train: 97 [ 350/1251 ( 28%)]  Loss: 3.646 (3.81)  Time: 0.798s, 1282.93/s  (0.789s, 1298.66/s)  LR: 9.375e-04  Data: 0.010 (0.014)
Train: 97 [ 400/1251 ( 32%)]  Loss: 3.660 (3.80)  Time: 0.823s, 1244.06/s  (0.792s, 1292.23/s)  LR: 9.375e-04  Data: 0.010 (0.014)
Train: 97 [ 450/1251 ( 36%)]  Loss: 3.492 (3.76)  Time: 0.778s, 1316.52/s  (0.792s, 1293.67/s)  LR: 9.375e-04  Data: 0.010 (0.013)
Train: 97 [ 500/1251 ( 40%)]  Loss: 3.617 (3.75)  Time: 0.778s, 1316.85/s  (0.792s, 1292.78/s)  LR: 9.375e-04  Data: 0.010 (0.013)
Train: 97 [ 550/1251 ( 44%)]  Loss: 3.576 (3.74)  Time: 0.777s, 1318.13/s  (0.791s, 1294.38/s)  LR: 9.375e-04  Data: 0.010 (0.013)
Train: 97 [ 600/1251 ( 48%)]  Loss: 3.760 (3.74)  Time: 0.777s, 1317.76/s  (0.790s, 1295.93/s)  LR: 9.375e-04  Data: 0.010 (0.012)
Train: 97 [ 650/1251 ( 52%)]  Loss: 3.786 (3.74)  Time: 0.778s, 1316.13/s  (0.790s, 1295.95/s)  LR: 9.375e-04  Data: 0.010 (0.012)
Train: 97 [ 700/1251 ( 56%)]  Loss: 3.995 (3.76)  Time: 0.781s, 1311.83/s  (0.791s, 1294.70/s)  LR: 9.375e-04  Data: 0.010 (0.012)
Train: 97 [ 750/1251 ( 60%)]  Loss: 3.946 (3.77)  Time: 0.814s, 1258.63/s  (0.790s, 1295.68/s)  LR: 9.375e-04  Data: 0.009 (0.012)
Train: 97 [ 800/1251 ( 64%)]  Loss: 3.528 (3.76)  Time: 0.778s, 1316.43/s  (0.792s, 1293.74/s)  LR: 9.375e-04  Data: 0.010 (0.012)
Train: 97 [ 850/1251 ( 68%)]  Loss: 3.825 (3.76)  Time: 0.777s, 1317.55/s  (0.791s, 1294.77/s)  LR: 9.375e-04  Data: 0.010 (0.012)
Train: 97 [ 900/1251 ( 72%)]  Loss: 3.276 (3.73)  Time: 0.778s, 1315.72/s  (0.791s, 1294.09/s)  LR: 9.375e-04  Data: 0.010 (0.012)
Train: 97 [ 950/1251 ( 76%)]  Loss: 3.792 (3.74)  Time: 0.778s, 1315.95/s  (0.791s, 1295.06/s)  LR: 9.375e-04  Data: 0.010 (0.011)
Train: 97 [1000/1251 ( 80%)]  Loss: 4.060 (3.75)  Time: 0.822s, 1245.30/s  (0.790s, 1295.71/s)  LR: 9.375e-04  Data: 0.010 (0.011)
Train: 97 [1050/1251 ( 84%)]  Loss: 3.894 (3.76)  Time: 0.779s, 1314.82/s  (0.790s, 1296.22/s)  LR: 9.375e-04  Data: 0.010 (0.011)
Train: 97 [1100/1251 ( 88%)]  Loss: 3.513 (3.75)  Time: 0.815s, 1256.67/s  (0.790s, 1296.30/s)  LR: 9.375e-04  Data: 0.010 (0.011)
Train: 97 [1150/1251 ( 92%)]  Loss: 3.784 (3.75)  Time: 0.815s, 1256.24/s  (0.791s, 1294.38/s)  LR: 9.375e-04  Data: 0.010 (0.011)
Train: 97 [1200/1251 ( 96%)]  Loss: 3.915 (3.76)  Time: 0.815s, 1256.18/s  (0.792s, 1292.57/s)  LR: 9.375e-04  Data: 0.010 (0.011)
Train: 97 [1250/1251 (100%)]  Loss: 4.096 (3.77)  Time: 0.799s, 1282.09/s  (0.793s, 1290.98/s)  LR: 9.375e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.564 (1.564)  Loss:  0.9204 (0.9204)  Acc@1: 87.1094 (87.1094)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.193 (0.555)  Loss:  1.1221 (1.5182)  Acc@1: 82.6651 (71.3380)  Acc@5: 94.3396 (90.7640)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-90.pth.tar', 71.7159999584961)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-92.pth.tar', 71.59600001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-94.pth.tar', 71.57599991210938)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-89.pth.tar', 71.45599999023437)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-88.pth.tar', 71.43600007080079)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-95.pth.tar', 71.37799998535156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-96.pth.tar', 71.37199993896485)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-97.pth.tar', 71.33799996826171)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-84.pth.tar', 71.28999999267577)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-93.pth.tar', 71.22999994140625)

Train: 98 [   0/1251 (  0%)]  Loss: 3.783 (3.78)  Time: 2.112s,  484.93/s  (2.112s,  484.93/s)  LR: 9.363e-04  Data: 1.381 (1.381)
Train: 98 [  50/1251 (  4%)]  Loss: 3.834 (3.81)  Time: 0.823s, 1243.87/s  (0.821s, 1247.01/s)  LR: 9.363e-04  Data: 0.009 (0.040)
Train: 98 [ 100/1251 (  8%)]  Loss: 3.856 (3.82)  Time: 0.778s, 1315.35/s  (0.806s, 1270.96/s)  LR: 9.363e-04  Data: 0.010 (0.025)
Train: 98 [ 150/1251 ( 12%)]  Loss: 3.469 (3.74)  Time: 0.778s, 1316.24/s  (0.798s, 1283.74/s)  LR: 9.363e-04  Data: 0.010 (0.020)
Train: 98 [ 200/1251 ( 16%)]  Loss: 4.321 (3.85)  Time: 0.782s, 1310.15/s  (0.794s, 1289.62/s)  LR: 9.363e-04  Data: 0.010 (0.018)
Train: 98 [ 250/1251 ( 20%)]  Loss: 3.519 (3.80)  Time: 0.781s, 1311.81/s  (0.793s, 1290.75/s)  LR: 9.363e-04  Data: 0.010 (0.016)
Train: 98 [ 300/1251 ( 24%)]  Loss: 3.792 (3.80)  Time: 0.778s, 1316.86/s  (0.792s, 1293.70/s)  LR: 9.363e-04  Data: 0.010 (0.015)
Train: 98 [ 350/1251 ( 28%)]  Loss: 3.511 (3.76)  Time: 0.827s, 1238.04/s  (0.794s, 1289.79/s)  LR: 9.363e-04  Data: 0.010 (0.014)
Train: 98 [ 400/1251 ( 32%)]  Loss: 3.731 (3.76)  Time: 0.778s, 1316.96/s  (0.792s, 1292.78/s)  LR: 9.363e-04  Data: 0.010 (0.014)
Train: 98 [ 450/1251 ( 36%)]  Loss: 3.967 (3.78)  Time: 0.778s, 1315.63/s  (0.791s, 1294.23/s)  LR: 9.363e-04  Data: 0.010 (0.013)
Train: 98 [ 500/1251 ( 40%)]  Loss: 3.524 (3.76)  Time: 0.781s, 1311.05/s  (0.791s, 1295.30/s)  LR: 9.363e-04  Data: 0.010 (0.013)
Train: 98 [ 550/1251 ( 44%)]  Loss: 3.889 (3.77)  Time: 0.777s, 1317.88/s  (0.790s, 1296.58/s)  LR: 9.363e-04  Data: 0.010 (0.013)
Train: 98 [ 600/1251 ( 48%)]  Loss: 4.190 (3.80)  Time: 0.781s, 1310.79/s  (0.789s, 1297.73/s)  LR: 9.363e-04  Data: 0.010 (0.012)
Train: 98 [ 650/1251 ( 52%)]  Loss: 3.825 (3.80)  Time: 0.778s, 1315.75/s  (0.789s, 1298.06/s)  LR: 9.363e-04  Data: 0.009 (0.012)
Train: 98 [ 700/1251 ( 56%)]  Loss: 3.798 (3.80)  Time: 0.783s, 1307.32/s  (0.788s, 1298.70/s)  LR: 9.363e-04  Data: 0.010 (0.012)
Train: 98 [ 750/1251 ( 60%)]  Loss: 3.854 (3.80)  Time: 0.779s, 1314.53/s  (0.788s, 1299.23/s)  LR: 9.363e-04  Data: 0.010 (0.012)
Train: 98 [ 800/1251 ( 64%)]  Loss: 3.811 (3.80)  Time: 0.778s, 1316.26/s  (0.788s, 1300.12/s)  LR: 9.363e-04  Data: 0.010 (0.012)
Train: 98 [ 850/1251 ( 68%)]  Loss: 3.421 (3.78)  Time: 0.782s, 1310.13/s  (0.788s, 1299.98/s)  LR: 9.363e-04  Data: 0.010 (0.012)
Train: 98 [ 900/1251 ( 72%)]  Loss: 3.636 (3.78)  Time: 0.778s, 1316.35/s  (0.787s, 1300.77/s)  LR: 9.363e-04  Data: 0.010 (0.012)
Train: 98 [ 950/1251 ( 76%)]  Loss: 3.668 (3.77)  Time: 0.779s, 1314.23/s  (0.787s, 1301.09/s)  LR: 9.363e-04  Data: 0.010 (0.011)
Train: 98 [1000/1251 ( 80%)]  Loss: 3.536 (3.76)  Time: 0.779s, 1314.79/s  (0.787s, 1301.33/s)  LR: 9.363e-04  Data: 0.010 (0.011)
Train: 98 [1050/1251 ( 84%)]  Loss: 3.795 (3.76)  Time: 0.823s, 1244.42/s  (0.788s, 1299.84/s)  LR: 9.363e-04  Data: 0.010 (0.011)
Train: 98 [1100/1251 ( 88%)]  Loss: 3.686 (3.76)  Time: 0.782s, 1309.40/s  (0.787s, 1300.43/s)  LR: 9.363e-04  Data: 0.010 (0.011)
Train: 98 [1150/1251 ( 92%)]  Loss: 4.142 (3.77)  Time: 0.780s, 1313.29/s  (0.787s, 1300.94/s)  LR: 9.363e-04  Data: 0.010 (0.011)
Train: 98 [1200/1251 ( 96%)]  Loss: 3.863 (3.78)  Time: 0.779s, 1314.51/s  (0.787s, 1301.44/s)  LR: 9.363e-04  Data: 0.009 (0.011)
Train: 98 [1250/1251 (100%)]  Loss: 3.947 (3.78)  Time: 0.768s, 1333.31/s  (0.787s, 1301.62/s)  LR: 9.363e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.527 (1.527)  Loss:  0.9858 (0.9858)  Acc@1: 87.8906 (87.8906)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.194 (0.569)  Loss:  1.0342 (1.5378)  Acc@1: 83.1368 (70.9740)  Acc@5: 95.7547 (90.5300)
Train: 99 [   0/1251 (  0%)]  Loss: 3.979 (3.98)  Time: 2.302s,  444.87/s  (2.302s,  444.87/s)  LR: 9.350e-04  Data: 1.573 (1.573)
Train: 99 [  50/1251 (  4%)]  Loss: 3.750 (3.86)  Time: 0.842s, 1216.56/s  (0.839s, 1220.14/s)  LR: 9.350e-04  Data: 0.015 (0.046)
Train: 99 [ 100/1251 (  8%)]  Loss: 3.985 (3.90)  Time: 0.781s, 1311.97/s  (0.825s, 1241.39/s)  LR: 9.350e-04  Data: 0.010 (0.029)
Train: 99 [ 150/1251 ( 12%)]  Loss: 3.931 (3.91)  Time: 0.927s, 1105.10/s  (0.820s, 1248.90/s)  LR: 9.350e-04  Data: 0.010 (0.023)
Train: 99 [ 200/1251 ( 16%)]  Loss: 3.600 (3.85)  Time: 0.826s, 1240.32/s  (0.817s, 1252.69/s)  LR: 9.350e-04  Data: 0.009 (0.020)
Train: 99 [ 250/1251 ( 20%)]  Loss: 3.436 (3.78)  Time: 0.822s, 1245.98/s  (0.815s, 1256.13/s)  LR: 9.350e-04  Data: 0.009 (0.018)
Train: 99 [ 300/1251 ( 24%)]  Loss: 3.770 (3.78)  Time: 0.824s, 1243.32/s  (0.813s, 1259.10/s)  LR: 9.350e-04  Data: 0.009 (0.017)
Train: 99 [ 350/1251 ( 28%)]  Loss: 3.901 (3.79)  Time: 0.780s, 1313.45/s  (0.813s, 1260.24/s)  LR: 9.350e-04  Data: 0.010 (0.016)
Train: 99 [ 400/1251 ( 32%)]  Loss: 3.379 (3.75)  Time: 0.798s, 1283.06/s  (0.813s, 1260.23/s)  LR: 9.350e-04  Data: 0.010 (0.015)
Train: 99 [ 450/1251 ( 36%)]  Loss: 4.141 (3.79)  Time: 0.849s, 1206.59/s  (0.813s, 1259.29/s)  LR: 9.350e-04  Data: 0.010 (0.015)
Train: 99 [ 500/1251 ( 40%)]  Loss: 3.878 (3.80)  Time: 0.803s, 1275.71/s  (0.813s, 1260.24/s)  LR: 9.350e-04  Data: 0.009 (0.014)
Train: 99 [ 550/1251 ( 44%)]  Loss: 3.856 (3.80)  Time: 0.831s, 1231.52/s  (0.812s, 1260.86/s)  LR: 9.350e-04  Data: 0.012 (0.014)
Train: 99 [ 600/1251 ( 48%)]  Loss: 3.736 (3.80)  Time: 0.782s, 1310.27/s  (0.812s, 1261.60/s)  LR: 9.350e-04  Data: 0.010 (0.014)
Train: 99 [ 650/1251 ( 52%)]  Loss: 3.771 (3.79)  Time: 0.784s, 1306.66/s  (0.811s, 1262.21/s)  LR: 9.350e-04  Data: 0.014 (0.013)
Train: 99 [ 700/1251 ( 56%)]  Loss: 3.837 (3.80)  Time: 0.809s, 1265.82/s  (0.811s, 1262.90/s)  LR: 9.350e-04  Data: 0.014 (0.013)
Train: 99 [ 750/1251 ( 60%)]  Loss: 4.097 (3.82)  Time: 0.847s, 1209.58/s  (0.811s, 1262.47/s)  LR: 9.350e-04  Data: 0.009 (0.013)
Train: 99 [ 800/1251 ( 64%)]  Loss: 3.704 (3.81)  Time: 0.815s, 1256.26/s  (0.811s, 1262.38/s)  LR: 9.350e-04  Data: 0.010 (0.013)
Train: 99 [ 850/1251 ( 68%)]  Loss: 3.982 (3.82)  Time: 0.778s, 1316.28/s  (0.811s, 1262.34/s)  LR: 9.350e-04  Data: 0.010 (0.013)
Train: 99 [ 900/1251 ( 72%)]  Loss: 3.544 (3.80)  Time: 0.829s, 1235.35/s  (0.811s, 1262.53/s)  LR: 9.350e-04  Data: 0.010 (0.013)
Train: 99 [ 950/1251 ( 76%)]  Loss: 3.432 (3.79)  Time: 0.779s, 1314.02/s  (0.811s, 1263.04/s)  LR: 9.350e-04  Data: 0.013 (0.013)
Train: 99 [1000/1251 ( 80%)]  Loss: 3.636 (3.78)  Time: 0.815s, 1256.86/s  (0.810s, 1263.56/s)  LR: 9.350e-04  Data: 0.010 (0.012)
Train: 99 [1050/1251 ( 84%)]  Loss: 3.778 (3.78)  Time: 0.783s, 1307.96/s  (0.810s, 1264.76/s)  LR: 9.350e-04  Data: 0.014 (0.012)
Train: 99 [1100/1251 ( 88%)]  Loss: 4.132 (3.79)  Time: 0.798s, 1283.32/s  (0.809s, 1265.94/s)  LR: 9.350e-04  Data: 0.010 (0.012)
Train: 99 [1150/1251 ( 92%)]  Loss: 3.750 (3.79)  Time: 0.791s, 1293.79/s  (0.809s, 1266.22/s)  LR: 9.350e-04  Data: 0.012 (0.012)
Train: 99 [1200/1251 ( 96%)]  Loss: 4.015 (3.80)  Time: 0.778s, 1316.48/s  (0.809s, 1266.08/s)  LR: 9.350e-04  Data: 0.010 (0.012)
Train: 99 [1250/1251 (100%)]  Loss: 3.954 (3.81)  Time: 0.781s, 1311.72/s  (0.809s, 1266.24/s)  LR: 9.350e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.707 (1.707)  Loss:  0.9121 (0.9121)  Acc@1: 87.6953 (87.6953)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.194 (0.596)  Loss:  0.9829 (1.4878)  Acc@1: 83.3726 (71.5960)  Acc@5: 94.6934 (90.9220)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-90.pth.tar', 71.7159999584961)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-92.pth.tar', 71.59600001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-99.pth.tar', 71.59600001708985)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-94.pth.tar', 71.57599991210938)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-89.pth.tar', 71.45599999023437)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-88.pth.tar', 71.43600007080079)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-95.pth.tar', 71.37799998535156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-96.pth.tar', 71.37199993896485)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-97.pth.tar', 71.33799996826171)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-84.pth.tar', 71.28999999267577)

Train: 100 [   0/1251 (  0%)]  Loss: 3.777 (3.78)  Time: 2.252s,  454.72/s  (2.252s,  454.72/s)  LR: 9.337e-04  Data: 1.527 (1.527)
Train: 100 [  50/1251 (  4%)]  Loss: 3.658 (3.72)  Time: 0.780s, 1312.39/s  (0.818s, 1252.33/s)  LR: 9.337e-04  Data: 0.010 (0.046)
Train: 100 [ 100/1251 (  8%)]  Loss: 4.168 (3.87)  Time: 0.783s, 1307.52/s  (0.801s, 1278.64/s)  LR: 9.337e-04  Data: 0.010 (0.028)
Train: 100 [ 150/1251 ( 12%)]  Loss: 3.315 (3.73)  Time: 0.845s, 1212.01/s  (0.802s, 1277.23/s)  LR: 9.337e-04  Data: 0.009 (0.022)
Train: 100 [ 200/1251 ( 16%)]  Loss: 3.597 (3.70)  Time: 0.816s, 1255.53/s  (0.803s, 1274.94/s)  LR: 9.337e-04  Data: 0.014 (0.019)
Train: 100 [ 250/1251 ( 20%)]  Loss: 3.856 (3.73)  Time: 0.804s, 1273.77/s  (0.805s, 1272.54/s)  LR: 9.337e-04  Data: 0.015 (0.018)
Train: 100 [ 300/1251 ( 24%)]  Loss: 3.843 (3.74)  Time: 0.776s, 1318.76/s  (0.805s, 1271.60/s)  LR: 9.337e-04  Data: 0.009 (0.016)
Train: 100 [ 350/1251 ( 28%)]  Loss: 3.914 (3.77)  Time: 0.793s, 1290.54/s  (0.806s, 1270.82/s)  LR: 9.337e-04  Data: 0.010 (0.016)
Train: 100 [ 400/1251 ( 32%)]  Loss: 3.551 (3.74)  Time: 0.858s, 1194.11/s  (0.805s, 1271.68/s)  LR: 9.337e-04  Data: 0.010 (0.015)
Train: 100 [ 450/1251 ( 36%)]  Loss: 3.808 (3.75)  Time: 0.782s, 1309.69/s  (0.805s, 1271.97/s)  LR: 9.337e-04  Data: 0.013 (0.015)
Train: 100 [ 500/1251 ( 40%)]  Loss: 3.508 (3.73)  Time: 0.785s, 1304.25/s  (0.805s, 1271.94/s)  LR: 9.337e-04  Data: 0.009 (0.014)
Train: 100 [ 550/1251 ( 44%)]  Loss: 3.865 (3.74)  Time: 0.802s, 1277.08/s  (0.805s, 1272.09/s)  LR: 9.337e-04  Data: 0.009 (0.014)
Train: 100 [ 600/1251 ( 48%)]  Loss: 3.836 (3.75)  Time: 0.805s, 1272.40/s  (0.805s, 1271.65/s)  LR: 9.337e-04  Data: 0.010 (0.014)
Train: 100 [ 650/1251 ( 52%)]  Loss: 3.961 (3.76)  Time: 0.779s, 1314.19/s  (0.805s, 1271.57/s)  LR: 9.337e-04  Data: 0.011 (0.013)
Train: 100 [ 700/1251 ( 56%)]  Loss: 3.489 (3.74)  Time: 0.783s, 1307.42/s  (0.806s, 1271.08/s)  LR: 9.337e-04  Data: 0.010 (0.013)
Train: 100 [ 750/1251 ( 60%)]  Loss: 3.766 (3.74)  Time: 0.775s, 1321.11/s  (0.805s, 1271.28/s)  LR: 9.337e-04  Data: 0.010 (0.013)
Train: 100 [ 800/1251 ( 64%)]  Loss: 3.655 (3.74)  Time: 0.828s, 1237.27/s  (0.805s, 1271.32/s)  LR: 9.337e-04  Data: 0.010 (0.013)
Train: 100 [ 850/1251 ( 68%)]  Loss: 3.546 (3.73)  Time: 0.825s, 1241.27/s  (0.805s, 1271.30/s)  LR: 9.337e-04  Data: 0.010 (0.013)
Train: 100 [ 900/1251 ( 72%)]  Loss: 3.531 (3.72)  Time: 0.806s, 1269.71/s  (0.806s, 1271.10/s)  LR: 9.337e-04  Data: 0.010 (0.013)
Train: 100 [ 950/1251 ( 76%)]  Loss: 4.209 (3.74)  Time: 0.801s, 1278.48/s  (0.806s, 1270.52/s)  LR: 9.337e-04  Data: 0.010 (0.013)
Train: 100 [1000/1251 ( 80%)]  Loss: 3.694 (3.74)  Time: 0.807s, 1269.47/s  (0.806s, 1270.56/s)  LR: 9.337e-04  Data: 0.012 (0.013)
Train: 100 [1050/1251 ( 84%)]  Loss: 3.737 (3.74)  Time: 0.782s, 1309.98/s  (0.806s, 1270.28/s)  LR: 9.337e-04  Data: 0.010 (0.012)
Train: 100 [1100/1251 ( 88%)]  Loss: 3.566 (3.73)  Time: 0.782s, 1308.91/s  (0.806s, 1270.15/s)  LR: 9.337e-04  Data: 0.013 (0.012)
Train: 100 [1150/1251 ( 92%)]  Loss: 3.843 (3.74)  Time: 0.777s, 1317.83/s  (0.806s, 1270.13/s)  LR: 9.337e-04  Data: 0.010 (0.012)
Train: 100 [1200/1251 ( 96%)]  Loss: 3.775 (3.74)  Time: 0.784s, 1305.63/s  (0.806s, 1270.29/s)  LR: 9.337e-04  Data: 0.010 (0.012)
Train: 100 [1250/1251 (100%)]  Loss: 3.658 (3.74)  Time: 0.782s, 1309.74/s  (0.806s, 1270.17/s)  LR: 9.337e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.578 (1.578)  Loss:  0.9683 (0.9683)  Acc@1: 87.3047 (87.3047)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.194 (0.613)  Loss:  1.0889 (1.4932)  Acc@1: 82.1934 (71.0400)  Acc@5: 94.8113 (90.5600)
Train: 101 [   0/1251 (  0%)]  Loss: 3.582 (3.58)  Time: 2.480s,  412.85/s  (2.480s,  412.85/s)  LR: 9.324e-04  Data: 1.749 (1.749)
Train: 101 [  50/1251 (  4%)]  Loss: 3.964 (3.77)  Time: 0.789s, 1297.26/s  (0.844s, 1213.62/s)  LR: 9.324e-04  Data: 0.017 (0.048)
Train: 101 [ 100/1251 (  8%)]  Loss: 3.587 (3.71)  Time: 0.784s, 1305.74/s  (0.825s, 1241.08/s)  LR: 9.324e-04  Data: 0.009 (0.030)
Train: 101 [ 150/1251 ( 12%)]  Loss: 3.803 (3.73)  Time: 0.867s, 1181.34/s  (0.819s, 1249.90/s)  LR: 9.324e-04  Data: 0.014 (0.023)
Train: 101 [ 200/1251 ( 16%)]  Loss: 3.448 (3.68)  Time: 0.780s, 1312.30/s  (0.817s, 1253.55/s)  LR: 9.324e-04  Data: 0.013 (0.020)
Train: 101 [ 250/1251 ( 20%)]  Loss: 3.719 (3.68)  Time: 0.786s, 1302.86/s  (0.815s, 1256.70/s)  LR: 9.324e-04  Data: 0.009 (0.018)
Train: 101 [ 300/1251 ( 24%)]  Loss: 3.636 (3.68)  Time: 0.825s, 1241.35/s  (0.813s, 1260.10/s)  LR: 9.324e-04  Data: 0.009 (0.017)
Train: 101 [ 350/1251 ( 28%)]  Loss: 3.874 (3.70)  Time: 0.805s, 1271.64/s  (0.811s, 1262.25/s)  LR: 9.324e-04  Data: 0.010 (0.016)
Train: 101 [ 400/1251 ( 32%)]  Loss: 3.644 (3.70)  Time: 0.784s, 1306.85/s  (0.811s, 1262.14/s)  LR: 9.324e-04  Data: 0.013 (0.015)
Train: 101 [ 450/1251 ( 36%)]  Loss: 3.642 (3.69)  Time: 0.842s, 1216.58/s  (0.812s, 1261.43/s)  LR: 9.324e-04  Data: 0.011 (0.015)
Train: 101 [ 500/1251 ( 40%)]  Loss: 4.011 (3.72)  Time: 0.780s, 1312.31/s  (0.811s, 1263.35/s)  LR: 9.324e-04  Data: 0.010 (0.015)
Train: 101 [ 550/1251 ( 44%)]  Loss: 3.697 (3.72)  Time: 0.792s, 1293.58/s  (0.811s, 1262.83/s)  LR: 9.324e-04  Data: 0.012 (0.014)
Train: 101 [ 600/1251 ( 48%)]  Loss: 3.688 (3.71)  Time: 0.862s, 1187.37/s  (0.810s, 1263.63/s)  LR: 9.324e-04  Data: 0.010 (0.014)
Train: 101 [ 650/1251 ( 52%)]  Loss: 4.035 (3.74)  Time: 0.826s, 1239.08/s  (0.810s, 1264.28/s)  LR: 9.324e-04  Data: 0.009 (0.014)
Train: 101 [ 700/1251 ( 56%)]  Loss: 4.063 (3.76)  Time: 0.779s, 1314.06/s  (0.810s, 1264.60/s)  LR: 9.324e-04  Data: 0.010 (0.013)
Train: 101 [ 750/1251 ( 60%)]  Loss: 3.728 (3.76)  Time: 0.786s, 1302.48/s  (0.810s, 1264.45/s)  LR: 9.324e-04  Data: 0.009 (0.013)
Train: 101 [ 800/1251 ( 64%)]  Loss: 3.637 (3.75)  Time: 0.796s, 1287.19/s  (0.810s, 1264.84/s)  LR: 9.324e-04  Data: 0.010 (0.013)
Train: 101 [ 850/1251 ( 68%)]  Loss: 3.926 (3.76)  Time: 0.808s, 1267.99/s  (0.810s, 1264.78/s)  LR: 9.324e-04  Data: 0.011 (0.013)
Train: 101 [ 900/1251 ( 72%)]  Loss: 3.623 (3.75)  Time: 0.782s, 1309.15/s  (0.810s, 1264.08/s)  LR: 9.324e-04  Data: 0.012 (0.013)
Train: 101 [ 950/1251 ( 76%)]  Loss: 3.908 (3.76)  Time: 0.802s, 1276.10/s  (0.810s, 1264.72/s)  LR: 9.324e-04  Data: 0.012 (0.013)
Train: 101 [1000/1251 ( 80%)]  Loss: 4.016 (3.77)  Time: 0.809s, 1265.60/s  (0.810s, 1264.82/s)  LR: 9.324e-04  Data: 0.010 (0.013)
Train: 101 [1050/1251 ( 84%)]  Loss: 4.146 (3.79)  Time: 0.837s, 1224.11/s  (0.809s, 1265.33/s)  LR: 9.324e-04  Data: 0.009 (0.013)
Train: 101 [1100/1251 ( 88%)]  Loss: 3.454 (3.78)  Time: 0.787s, 1301.38/s  (0.809s, 1265.51/s)  LR: 9.324e-04  Data: 0.014 (0.013)
Train: 101 [1150/1251 ( 92%)]  Loss: 3.709 (3.77)  Time: 0.782s, 1309.54/s  (0.809s, 1265.83/s)  LR: 9.324e-04  Data: 0.010 (0.012)
Train: 101 [1200/1251 ( 96%)]  Loss: 3.184 (3.75)  Time: 0.835s, 1226.54/s  (0.809s, 1265.49/s)  LR: 9.324e-04  Data: 0.009 (0.012)
Train: 101 [1250/1251 (100%)]  Loss: 3.902 (3.75)  Time: 0.796s, 1286.48/s  (0.809s, 1265.36/s)  LR: 9.324e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.795 (1.795)  Loss:  0.8145 (0.8145)  Acc@1: 87.1094 (87.1094)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.194 (0.610)  Loss:  0.9688 (1.4281)  Acc@1: 83.0189 (71.6100)  Acc@5: 94.2217 (90.6040)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-90.pth.tar', 71.7159999584961)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-101.pth.tar', 71.61000012207032)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-92.pth.tar', 71.59600001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-99.pth.tar', 71.59600001708985)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-94.pth.tar', 71.57599991210938)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-89.pth.tar', 71.45599999023437)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-88.pth.tar', 71.43600007080079)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-95.pth.tar', 71.37799998535156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-96.pth.tar', 71.37199993896485)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-97.pth.tar', 71.33799996826171)

Train: 102 [   0/1251 (  0%)]  Loss: 3.907 (3.91)  Time: 2.376s,  431.05/s  (2.376s,  431.05/s)  LR: 9.311e-04  Data: 1.588 (1.588)
Train: 102 [  50/1251 (  4%)]  Loss: 3.849 (3.88)  Time: 0.801s, 1279.02/s  (0.844s, 1213.89/s)  LR: 9.311e-04  Data: 0.015 (0.047)
Train: 102 [ 100/1251 (  8%)]  Loss: 4.168 (3.97)  Time: 0.808s, 1267.74/s  (0.824s, 1242.46/s)  LR: 9.311e-04  Data: 0.010 (0.029)
Train: 102 [ 150/1251 ( 12%)]  Loss: 3.770 (3.92)  Time: 0.789s, 1297.80/s  (0.814s, 1258.54/s)  LR: 9.311e-04  Data: 0.011 (0.023)
Train: 102 [ 200/1251 ( 16%)]  Loss: 3.595 (3.86)  Time: 0.793s, 1291.90/s  (0.809s, 1265.14/s)  LR: 9.311e-04  Data: 0.010 (0.020)
Train: 102 [ 250/1251 ( 20%)]  Loss: 3.779 (3.84)  Time: 0.784s, 1306.84/s  (0.809s, 1266.40/s)  LR: 9.311e-04  Data: 0.011 (0.018)
Train: 102 [ 300/1251 ( 24%)]  Loss: 3.574 (3.81)  Time: 0.801s, 1278.32/s  (0.808s, 1267.55/s)  LR: 9.311e-04  Data: 0.009 (0.017)
Train: 102 [ 350/1251 ( 28%)]  Loss: 3.958 (3.82)  Time: 0.779s, 1315.11/s  (0.808s, 1267.89/s)  LR: 9.311e-04  Data: 0.011 (0.016)
Train: 102 [ 400/1251 ( 32%)]  Loss: 3.634 (3.80)  Time: 0.808s, 1267.15/s  (0.807s, 1268.81/s)  LR: 9.311e-04  Data: 0.010 (0.015)
Train: 102 [ 450/1251 ( 36%)]  Loss: 3.918 (3.82)  Time: 0.780s, 1312.98/s  (0.807s, 1268.98/s)  LR: 9.311e-04  Data: 0.009 (0.015)
Train: 102 [ 500/1251 ( 40%)]  Loss: 4.157 (3.85)  Time: 0.817s, 1252.69/s  (0.807s, 1268.12/s)  LR: 9.311e-04  Data: 0.009 (0.014)
Train: 102 [ 550/1251 ( 44%)]  Loss: 3.612 (3.83)  Time: 0.809s, 1265.56/s  (0.808s, 1267.31/s)  LR: 9.311e-04  Data: 0.010 (0.014)
Train: 102 [ 600/1251 ( 48%)]  Loss: 3.834 (3.83)  Time: 0.824s, 1243.10/s  (0.808s, 1267.23/s)  LR: 9.311e-04  Data: 0.010 (0.014)
Train: 102 [ 650/1251 ( 52%)]  Loss: 3.514 (3.80)  Time: 0.823s, 1244.00/s  (0.809s, 1266.45/s)  LR: 9.311e-04  Data: 0.009 (0.013)
Train: 102 [ 700/1251 ( 56%)]  Loss: 3.678 (3.80)  Time: 0.804s, 1272.95/s  (0.809s, 1266.23/s)  LR: 9.311e-04  Data: 0.014 (0.013)
Train: 102 [ 750/1251 ( 60%)]  Loss: 3.727 (3.79)  Time: 0.782s, 1308.65/s  (0.809s, 1265.79/s)  LR: 9.311e-04  Data: 0.013 (0.013)
Train: 102 [ 800/1251 ( 64%)]  Loss: 3.723 (3.79)  Time: 0.788s, 1299.74/s  (0.809s, 1266.12/s)  LR: 9.311e-04  Data: 0.010 (0.013)
Train: 102 [ 850/1251 ( 68%)]  Loss: 3.695 (3.78)  Time: 0.801s, 1277.66/s  (0.809s, 1266.17/s)  LR: 9.311e-04  Data: 0.014 (0.013)
Train: 102 [ 900/1251 ( 72%)]  Loss: 3.719 (3.78)  Time: 0.809s, 1266.26/s  (0.809s, 1266.35/s)  LR: 9.311e-04  Data: 0.009 (0.013)
Train: 102 [ 950/1251 ( 76%)]  Loss: 3.590 (3.77)  Time: 0.793s, 1291.55/s  (0.809s, 1265.97/s)  LR: 9.311e-04  Data: 0.010 (0.013)
Train: 102 [1000/1251 ( 80%)]  Loss: 3.540 (3.76)  Time: 0.787s, 1301.39/s  (0.809s, 1266.30/s)  LR: 9.311e-04  Data: 0.010 (0.013)
Train: 102 [1050/1251 ( 84%)]  Loss: 4.138 (3.78)  Time: 0.799s, 1281.58/s  (0.809s, 1266.30/s)  LR: 9.311e-04  Data: 0.014 (0.012)
Train: 102 [1100/1251 ( 88%)]  Loss: 3.636 (3.77)  Time: 0.850s, 1204.10/s  (0.809s, 1266.24/s)  LR: 9.311e-04  Data: 0.011 (0.012)
Train: 102 [1150/1251 ( 92%)]  Loss: 3.726 (3.77)  Time: 0.810s, 1264.14/s  (0.808s, 1266.63/s)  LR: 9.311e-04  Data: 0.011 (0.012)
Train: 102 [1200/1251 ( 96%)]  Loss: 3.541 (3.76)  Time: 0.808s, 1267.85/s  (0.808s, 1266.70/s)  LR: 9.311e-04  Data: 0.010 (0.012)
Train: 102 [1250/1251 (100%)]  Loss: 3.885 (3.76)  Time: 0.793s, 1292.03/s  (0.808s, 1266.79/s)  LR: 9.311e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.703 (1.703)  Loss:  0.8237 (0.8237)  Acc@1: 87.0117 (87.0117)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.194 (0.599)  Loss:  0.9893 (1.5049)  Acc@1: 83.0189 (71.6000)  Acc@5: 94.9292 (90.6640)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-90.pth.tar', 71.7159999584961)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-101.pth.tar', 71.61000012207032)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-102.pth.tar', 71.59999999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-92.pth.tar', 71.59600001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-99.pth.tar', 71.59600001708985)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-94.pth.tar', 71.57599991210938)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-89.pth.tar', 71.45599999023437)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-88.pth.tar', 71.43600007080079)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-95.pth.tar', 71.37799998535156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-96.pth.tar', 71.37199993896485)

Train: 103 [   0/1251 (  0%)]  Loss: 3.718 (3.72)  Time: 2.623s,  390.45/s  (2.623s,  390.45/s)  LR: 9.297e-04  Data: 1.891 (1.891)
Train: 103 [  50/1251 (  4%)]  Loss: 3.684 (3.70)  Time: 0.781s, 1311.01/s  (0.852s, 1202.43/s)  LR: 9.297e-04  Data: 0.013 (0.054)
Train: 103 [ 100/1251 (  8%)]  Loss: 3.504 (3.64)  Time: 0.823s, 1244.00/s  (0.828s, 1237.45/s)  LR: 9.297e-04  Data: 0.010 (0.033)
Train: 103 [ 150/1251 ( 12%)]  Loss: 3.742 (3.66)  Time: 0.781s, 1311.27/s  (0.822s, 1245.65/s)  LR: 9.297e-04  Data: 0.010 (0.026)
Train: 103 [ 200/1251 ( 16%)]  Loss: 3.774 (3.68)  Time: 0.777s, 1318.00/s  (0.818s, 1252.54/s)  LR: 9.297e-04  Data: 0.009 (0.022)
Train: 103 [ 250/1251 ( 20%)]  Loss: 3.856 (3.71)  Time: 0.787s, 1301.66/s  (0.815s, 1257.06/s)  LR: 9.297e-04  Data: 0.010 (0.020)
Train: 103 [ 300/1251 ( 24%)]  Loss: 3.907 (3.74)  Time: 0.782s, 1309.41/s  (0.812s, 1261.03/s)  LR: 9.297e-04  Data: 0.010 (0.018)
Train: 103 [ 350/1251 ( 28%)]  Loss: 3.666 (3.73)  Time: 0.808s, 1267.37/s  (0.811s, 1262.59/s)  LR: 9.297e-04  Data: 0.010 (0.017)
Train: 103 [ 400/1251 ( 32%)]  Loss: 3.534 (3.71)  Time: 0.776s, 1319.47/s  (0.810s, 1264.08/s)  LR: 9.297e-04  Data: 0.011 (0.017)
Train: 103 [ 450/1251 ( 36%)]  Loss: 3.373 (3.68)  Time: 0.778s, 1315.78/s  (0.809s, 1265.97/s)  LR: 9.297e-04  Data: 0.010 (0.016)
Train: 103 [ 500/1251 ( 40%)]  Loss: 3.692 (3.68)  Time: 0.819s, 1249.75/s  (0.809s, 1266.53/s)  LR: 9.297e-04  Data: 0.009 (0.015)
Train: 103 [ 550/1251 ( 44%)]  Loss: 3.775 (3.69)  Time: 0.781s, 1310.47/s  (0.808s, 1267.20/s)  LR: 9.297e-04  Data: 0.015 (0.015)
Train: 103 [ 600/1251 ( 48%)]  Loss: 3.596 (3.68)  Time: 0.784s, 1306.58/s  (0.807s, 1268.89/s)  LR: 9.297e-04  Data: 0.013 (0.015)
Train: 103 [ 650/1251 ( 52%)]  Loss: 3.551 (3.67)  Time: 0.781s, 1311.49/s  (0.807s, 1269.54/s)  LR: 9.297e-04  Data: 0.010 (0.014)
Train: 103 [ 700/1251 ( 56%)]  Loss: 3.622 (3.67)  Time: 0.795s, 1288.51/s  (0.806s, 1270.27/s)  LR: 9.297e-04  Data: 0.013 (0.014)
Train: 103 [ 750/1251 ( 60%)]  Loss: 3.976 (3.69)  Time: 0.853s, 1200.64/s  (0.806s, 1270.66/s)  LR: 9.297e-04  Data: 0.014 (0.014)
Train: 103 [ 800/1251 ( 64%)]  Loss: 3.662 (3.68)  Time: 0.814s, 1257.51/s  (0.805s, 1271.37/s)  LR: 9.297e-04  Data: 0.010 (0.014)
Train: 103 [ 850/1251 ( 68%)]  Loss: 3.635 (3.68)  Time: 0.774s, 1323.03/s  (0.805s, 1271.96/s)  LR: 9.297e-04  Data: 0.009 (0.014)
Train: 103 [ 900/1251 ( 72%)]  Loss: 3.522 (3.67)  Time: 0.773s, 1325.04/s  (0.805s, 1272.34/s)  LR: 9.297e-04  Data: 0.010 (0.014)
Train: 103 [ 950/1251 ( 76%)]  Loss: 4.128 (3.70)  Time: 0.800s, 1279.65/s  (0.805s, 1272.47/s)  LR: 9.297e-04  Data: 0.010 (0.013)
Train: 103 [1000/1251 ( 80%)]  Loss: 3.585 (3.69)  Time: 0.807s, 1268.23/s  (0.805s, 1272.59/s)  LR: 9.297e-04  Data: 0.016 (0.013)
Train: 103 [1050/1251 ( 84%)]  Loss: 3.581 (3.69)  Time: 0.833s, 1229.63/s  (0.805s, 1272.60/s)  LR: 9.297e-04  Data: 0.010 (0.013)
Train: 103 [1100/1251 ( 88%)]  Loss: 3.781 (3.69)  Time: 0.803s, 1274.86/s  (0.805s, 1272.63/s)  LR: 9.297e-04  Data: 0.011 (0.013)
Train: 103 [1150/1251 ( 92%)]  Loss: 3.629 (3.69)  Time: 0.803s, 1275.46/s  (0.805s, 1272.29/s)  LR: 9.297e-04  Data: 0.009 (0.013)
Train: 103 [1200/1251 ( 96%)]  Loss: 3.762 (3.69)  Time: 0.814s, 1257.65/s  (0.805s, 1272.09/s)  LR: 9.297e-04  Data: 0.011 (0.013)
Train: 103 [1250/1251 (100%)]  Loss: 3.990 (3.70)  Time: 0.760s, 1347.23/s  (0.805s, 1272.54/s)  LR: 9.297e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.721 (1.721)  Loss:  0.9146 (0.9146)  Acc@1: 86.6211 (86.6211)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.194 (0.599)  Loss:  0.9761 (1.4384)  Acc@1: 82.9009 (72.0500)  Acc@5: 95.2830 (91.0460)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-103.pth.tar', 72.05000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-90.pth.tar', 71.7159999584961)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-101.pth.tar', 71.61000012207032)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-102.pth.tar', 71.59999999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-92.pth.tar', 71.59600001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-99.pth.tar', 71.59600001708985)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-94.pth.tar', 71.57599991210938)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-89.pth.tar', 71.45599999023437)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-88.pth.tar', 71.43600007080079)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-95.pth.tar', 71.37799998535156)

Train: 104 [   0/1251 (  0%)]  Loss: 3.775 (3.78)  Time: 2.400s,  426.61/s  (2.400s,  426.61/s)  LR: 9.284e-04  Data: 1.648 (1.648)
Train: 104 [  50/1251 (  4%)]  Loss: 3.749 (3.76)  Time: 0.790s, 1296.48/s  (0.843s, 1214.31/s)  LR: 9.284e-04  Data: 0.012 (0.049)
Train: 104 [ 100/1251 (  8%)]  Loss: 3.771 (3.76)  Time: 0.773s, 1323.99/s  (0.822s, 1245.89/s)  LR: 9.284e-04  Data: 0.010 (0.030)
Train: 104 [ 150/1251 ( 12%)]  Loss: 3.461 (3.69)  Time: 0.812s, 1261.59/s  (0.814s, 1257.65/s)  LR: 9.284e-04  Data: 0.011 (0.024)
Train: 104 [ 200/1251 ( 16%)]  Loss: 3.561 (3.66)  Time: 0.814s, 1258.00/s  (0.812s, 1261.32/s)  LR: 9.284e-04  Data: 0.010 (0.021)
Train: 104 [ 250/1251 ( 20%)]  Loss: 3.449 (3.63)  Time: 0.820s, 1248.19/s  (0.811s, 1263.01/s)  LR: 9.284e-04  Data: 0.010 (0.019)
Train: 104 [ 300/1251 ( 24%)]  Loss: 3.569 (3.62)  Time: 0.775s, 1321.72/s  (0.809s, 1265.15/s)  LR: 9.284e-04  Data: 0.011 (0.018)
Train: 104 [ 350/1251 ( 28%)]  Loss: 3.819 (3.64)  Time: 0.775s, 1321.76/s  (0.809s, 1266.30/s)  LR: 9.284e-04  Data: 0.010 (0.017)
Train: 104 [ 400/1251 ( 32%)]  Loss: 3.735 (3.65)  Time: 0.850s, 1204.41/s  (0.808s, 1267.98/s)  LR: 9.284e-04  Data: 0.010 (0.016)
Train: 104 [ 450/1251 ( 36%)]  Loss: 4.050 (3.69)  Time: 0.783s, 1307.53/s  (0.806s, 1269.94/s)  LR: 9.284e-04  Data: 0.016 (0.016)
Train: 104 [ 500/1251 ( 40%)]  Loss: 3.368 (3.66)  Time: 0.803s, 1275.97/s  (0.806s, 1269.77/s)  LR: 9.284e-04  Data: 0.010 (0.015)
Train: 104 [ 550/1251 ( 44%)]  Loss: 3.369 (3.64)  Time: 0.788s, 1299.24/s  (0.807s, 1269.42/s)  LR: 9.284e-04  Data: 0.010 (0.015)
Train: 104 [ 600/1251 ( 48%)]  Loss: 3.801 (3.65)  Time: 0.796s, 1285.88/s  (0.806s, 1270.35/s)  LR: 9.284e-04  Data: 0.010 (0.015)
Train: 104 [ 650/1251 ( 52%)]  Loss: 3.707 (3.66)  Time: 0.779s, 1314.85/s  (0.806s, 1270.34/s)  LR: 9.284e-04  Data: 0.011 (0.014)
Train: 104 [ 700/1251 ( 56%)]  Loss: 3.806 (3.67)  Time: 0.810s, 1264.67/s  (0.806s, 1270.17/s)  LR: 9.284e-04  Data: 0.014 (0.014)
Train: 104 [ 750/1251 ( 60%)]  Loss: 3.751 (3.67)  Time: 0.824s, 1242.37/s  (0.805s, 1271.36/s)  LR: 9.284e-04  Data: 0.010 (0.014)
Train: 104 [ 800/1251 ( 64%)]  Loss: 3.911 (3.69)  Time: 0.772s, 1325.68/s  (0.805s, 1271.99/s)  LR: 9.284e-04  Data: 0.010 (0.014)
Train: 104 [ 850/1251 ( 68%)]  Loss: 3.487 (3.67)  Time: 0.773s, 1325.10/s  (0.804s, 1272.98/s)  LR: 9.284e-04  Data: 0.010 (0.014)
Train: 104 [ 900/1251 ( 72%)]  Loss: 3.557 (3.67)  Time: 0.797s, 1284.86/s  (0.804s, 1273.63/s)  LR: 9.284e-04  Data: 0.017 (0.014)
Train: 104 [ 950/1251 ( 76%)]  Loss: 3.612 (3.67)  Time: 0.769s, 1330.94/s  (0.804s, 1273.88/s)  LR: 9.284e-04  Data: 0.011 (0.013)
Train: 104 [1000/1251 ( 80%)]  Loss: 3.950 (3.68)  Time: 0.774s, 1322.94/s  (0.804s, 1274.33/s)  LR: 9.284e-04  Data: 0.011 (0.013)
Train: 104 [1050/1251 ( 84%)]  Loss: 3.891 (3.69)  Time: 0.819s, 1250.59/s  (0.804s, 1274.36/s)  LR: 9.284e-04  Data: 0.010 (0.013)
Train: 104 [1100/1251 ( 88%)]  Loss: 3.729 (3.69)  Time: 0.803s, 1274.61/s  (0.803s, 1274.43/s)  LR: 9.284e-04  Data: 0.010 (0.013)
Train: 104 [1150/1251 ( 92%)]  Loss: 3.751 (3.69)  Time: 0.820s, 1248.96/s  (0.803s, 1274.49/s)  LR: 9.284e-04  Data: 0.010 (0.013)
Train: 104 [1200/1251 ( 96%)]  Loss: 3.627 (3.69)  Time: 0.832s, 1231.33/s  (0.803s, 1274.45/s)  LR: 9.284e-04  Data: 0.010 (0.013)
Train: 104 [1250/1251 (100%)]  Loss: 3.803 (3.69)  Time: 0.788s, 1299.14/s  (0.803s, 1274.49/s)  LR: 9.284e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.683 (1.683)  Loss:  0.8926 (0.8926)  Acc@1: 86.8164 (86.8164)  Acc@5: 96.2891 (96.2891)
Test: [  48/48]  Time: 0.194 (0.598)  Loss:  0.8740 (1.4700)  Acc@1: 84.7877 (71.4400)  Acc@5: 95.6368 (90.8280)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-103.pth.tar', 72.05000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-90.pth.tar', 71.7159999584961)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-101.pth.tar', 71.61000012207032)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-102.pth.tar', 71.59999999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-92.pth.tar', 71.59600001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-99.pth.tar', 71.59600001708985)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-94.pth.tar', 71.57599991210938)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-89.pth.tar', 71.45599999023437)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-104.pth.tar', 71.43999998535156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-88.pth.tar', 71.43600007080079)

Train: 105 [   0/1251 (  0%)]  Loss: 3.883 (3.88)  Time: 2.336s,  438.27/s  (2.336s,  438.27/s)  LR: 9.271e-04  Data: 1.592 (1.592)
Train: 105 [  50/1251 (  4%)]  Loss: 3.504 (3.69)  Time: 0.822s, 1245.48/s  (0.841s, 1217.95/s)  LR: 9.271e-04  Data: 0.010 (0.051)
Train: 105 [ 100/1251 (  8%)]  Loss: 3.887 (3.76)  Time: 0.838s, 1222.39/s  (0.824s, 1242.77/s)  LR: 9.271e-04  Data: 0.010 (0.031)
Train: 105 [ 150/1251 ( 12%)]  Loss: 3.435 (3.68)  Time: 0.815s, 1256.32/s  (0.816s, 1255.63/s)  LR: 9.271e-04  Data: 0.010 (0.025)
Train: 105 [ 200/1251 ( 16%)]  Loss: 3.862 (3.71)  Time: 0.775s, 1320.46/s  (0.812s, 1261.32/s)  LR: 9.271e-04  Data: 0.010 (0.021)
Train: 105 [ 250/1251 ( 20%)]  Loss: 3.520 (3.68)  Time: 0.860s, 1190.29/s  (0.809s, 1265.20/s)  LR: 9.271e-04  Data: 0.011 (0.019)
Train: 105 [ 300/1251 ( 24%)]  Loss: 3.668 (3.68)  Time: 0.853s, 1200.65/s  (0.808s, 1267.39/s)  LR: 9.271e-04  Data: 0.010 (0.018)
Train: 105 [ 350/1251 ( 28%)]  Loss: 3.653 (3.68)  Time: 0.785s, 1305.13/s  (0.807s, 1268.67/s)  LR: 9.271e-04  Data: 0.010 (0.017)
Train: 105 [ 400/1251 ( 32%)]  Loss: 3.385 (3.64)  Time: 0.774s, 1323.13/s  (0.806s, 1269.87/s)  LR: 9.271e-04  Data: 0.011 (0.016)
Train: 105 [ 450/1251 ( 36%)]  Loss: 3.984 (3.68)  Time: 0.775s, 1321.69/s  (0.805s, 1271.39/s)  LR: 9.271e-04  Data: 0.010 (0.016)
Train: 105 [ 500/1251 ( 40%)]  Loss: 3.766 (3.69)  Time: 0.795s, 1288.30/s  (0.806s, 1271.05/s)  LR: 9.271e-04  Data: 0.010 (0.015)
Train: 105 [ 550/1251 ( 44%)]  Loss: 3.864 (3.70)  Time: 0.781s, 1310.37/s  (0.805s, 1271.80/s)  LR: 9.271e-04  Data: 0.010 (0.015)
Train: 105 [ 600/1251 ( 48%)]  Loss: 4.028 (3.73)  Time: 0.784s, 1306.29/s  (0.805s, 1272.54/s)  LR: 9.271e-04  Data: 0.010 (0.015)
Train: 105 [ 650/1251 ( 52%)]  Loss: 4.026 (3.75)  Time: 0.878s, 1166.34/s  (0.805s, 1272.64/s)  LR: 9.271e-04  Data: 0.010 (0.014)
Train: 105 [ 700/1251 ( 56%)]  Loss: 3.797 (3.75)  Time: 0.841s, 1217.02/s  (0.805s, 1272.81/s)  LR: 9.271e-04  Data: 0.009 (0.014)
Train: 105 [ 750/1251 ( 60%)]  Loss: 3.636 (3.74)  Time: 0.781s, 1311.36/s  (0.804s, 1273.27/s)  LR: 9.271e-04  Data: 0.016 (0.014)
Train: 105 [ 800/1251 ( 64%)]  Loss: 3.954 (3.76)  Time: 0.776s, 1319.16/s  (0.804s, 1274.06/s)  LR: 9.271e-04  Data: 0.010 (0.014)
Train: 105 [ 850/1251 ( 68%)]  Loss: 3.460 (3.74)  Time: 0.814s, 1258.56/s  (0.804s, 1274.11/s)  LR: 9.271e-04  Data: 0.010 (0.014)
Train: 105 [ 900/1251 ( 72%)]  Loss: 3.808 (3.74)  Time: 0.866s, 1182.48/s  (0.804s, 1274.23/s)  LR: 9.271e-04  Data: 0.010 (0.014)
Train: 105 [ 950/1251 ( 76%)]  Loss: 3.800 (3.75)  Time: 0.792s, 1292.87/s  (0.804s, 1274.39/s)  LR: 9.271e-04  Data: 0.010 (0.013)
Train: 105 [1000/1251 ( 80%)]  Loss: 3.595 (3.74)  Time: 0.773s, 1324.87/s  (0.803s, 1274.98/s)  LR: 9.271e-04  Data: 0.010 (0.013)
Train: 105 [1050/1251 ( 84%)]  Loss: 3.513 (3.73)  Time: 0.801s, 1278.36/s  (0.803s, 1274.47/s)  LR: 9.271e-04  Data: 0.010 (0.013)
Train: 105 [1100/1251 ( 88%)]  Loss: 3.350 (3.71)  Time: 0.806s, 1270.85/s  (0.803s, 1274.59/s)  LR: 9.271e-04  Data: 0.010 (0.013)
Train: 105 [1150/1251 ( 92%)]  Loss: 3.932 (3.72)  Time: 0.776s, 1319.61/s  (0.803s, 1274.94/s)  LR: 9.271e-04  Data: 0.010 (0.013)
Train: 105 [1200/1251 ( 96%)]  Loss: 4.046 (3.73)  Time: 0.912s, 1123.30/s  (0.803s, 1274.93/s)  LR: 9.271e-04  Data: 0.014 (0.013)
Train: 105 [1250/1251 (100%)]  Loss: 4.095 (3.75)  Time: 0.764s, 1339.58/s  (0.803s, 1275.01/s)  LR: 9.271e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.737 (1.737)  Loss:  0.8071 (0.8071)  Acc@1: 86.3281 (86.3281)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.194 (0.606)  Loss:  1.0039 (1.4223)  Acc@1: 82.7830 (72.0060)  Acc@5: 94.8113 (91.1540)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-103.pth.tar', 72.05000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-105.pth.tar', 72.00599989013672)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-90.pth.tar', 71.7159999584961)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-101.pth.tar', 71.61000012207032)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-102.pth.tar', 71.59999999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-92.pth.tar', 71.59600001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-99.pth.tar', 71.59600001708985)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-94.pth.tar', 71.57599991210938)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-89.pth.tar', 71.45599999023437)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-104.pth.tar', 71.43999998535156)

Train: 106 [   0/1251 (  0%)]  Loss: 3.700 (3.70)  Time: 2.609s,  392.50/s  (2.609s,  392.50/s)  LR: 9.257e-04  Data: 1.883 (1.883)
Train: 106 [  50/1251 (  4%)]  Loss: 3.617 (3.66)  Time: 0.798s, 1283.91/s  (0.846s, 1210.45/s)  LR: 9.257e-04  Data: 0.013 (0.061)
Train: 106 [ 100/1251 (  8%)]  Loss: 3.817 (3.71)  Time: 0.781s, 1310.49/s  (0.823s, 1244.58/s)  LR: 9.257e-04  Data: 0.015 (0.036)
Train: 106 [ 150/1251 ( 12%)]  Loss: 3.617 (3.69)  Time: 0.836s, 1225.22/s  (0.815s, 1255.93/s)  LR: 9.257e-04  Data: 0.010 (0.027)
Train: 106 [ 200/1251 ( 16%)]  Loss: 3.875 (3.73)  Time: 0.799s, 1281.91/s  (0.814s, 1258.67/s)  LR: 9.257e-04  Data: 0.010 (0.023)
Train: 106 [ 250/1251 ( 20%)]  Loss: 3.996 (3.77)  Time: 0.773s, 1324.59/s  (0.812s, 1261.52/s)  LR: 9.257e-04  Data: 0.011 (0.021)
Train: 106 [ 300/1251 ( 24%)]  Loss: 3.645 (3.75)  Time: 0.784s, 1306.93/s  (0.810s, 1264.15/s)  LR: 9.257e-04  Data: 0.015 (0.019)
Train: 106 [ 350/1251 ( 28%)]  Loss: 3.908 (3.77)  Time: 0.840s, 1219.77/s  (0.809s, 1265.32/s)  LR: 9.257e-04  Data: 0.010 (0.018)
Train: 106 [ 400/1251 ( 32%)]  Loss: 3.575 (3.75)  Time: 0.773s, 1324.29/s  (0.808s, 1267.02/s)  LR: 9.257e-04  Data: 0.010 (0.017)
Train: 106 [ 450/1251 ( 36%)]  Loss: 4.023 (3.78)  Time: 0.794s, 1290.39/s  (0.807s, 1268.94/s)  LR: 9.257e-04  Data: 0.017 (0.017)
Train: 106 [ 500/1251 ( 40%)]  Loss: 4.028 (3.80)  Time: 0.771s, 1327.76/s  (0.807s, 1269.24/s)  LR: 9.257e-04  Data: 0.010 (0.016)
Train: 106 [ 550/1251 ( 44%)]  Loss: 3.821 (3.80)  Time: 0.779s, 1314.58/s  (0.807s, 1269.65/s)  LR: 9.257e-04  Data: 0.010 (0.016)
Train: 106 [ 600/1251 ( 48%)]  Loss: 3.377 (3.77)  Time: 0.794s, 1289.38/s  (0.806s, 1270.47/s)  LR: 9.257e-04  Data: 0.013 (0.015)
Train: 106 [ 650/1251 ( 52%)]  Loss: 3.846 (3.77)  Time: 0.791s, 1294.65/s  (0.806s, 1270.68/s)  LR: 9.257e-04  Data: 0.016 (0.015)
Train: 106 [ 700/1251 ( 56%)]  Loss: 3.630 (3.77)  Time: 0.781s, 1310.99/s  (0.806s, 1271.08/s)  LR: 9.257e-04  Data: 0.012 (0.015)
Train: 106 [ 750/1251 ( 60%)]  Loss: 3.686 (3.76)  Time: 0.807s, 1268.12/s  (0.805s, 1272.37/s)  LR: 9.257e-04  Data: 0.010 (0.014)
Train: 106 [ 800/1251 ( 64%)]  Loss: 3.855 (3.77)  Time: 0.772s, 1326.24/s  (0.804s, 1272.99/s)  LR: 9.257e-04  Data: 0.011 (0.014)
Train: 106 [ 850/1251 ( 68%)]  Loss: 3.973 (3.78)  Time: 0.813s, 1259.15/s  (0.804s, 1273.02/s)  LR: 9.257e-04  Data: 0.014 (0.014)
Train: 106 [ 900/1251 ( 72%)]  Loss: 3.971 (3.79)  Time: 0.829s, 1235.96/s  (0.806s, 1270.55/s)  LR: 9.257e-04  Data: 0.012 (0.014)
Train: 106 [ 950/1251 ( 76%)]  Loss: 4.031 (3.80)  Time: 0.771s, 1327.82/s  (0.805s, 1272.08/s)  LR: 9.257e-04  Data: 0.010 (0.014)
Train: 106 [1000/1251 ( 80%)]  Loss: 3.422 (3.78)  Time: 0.773s, 1325.37/s  (0.803s, 1274.43/s)  LR: 9.257e-04  Data: 0.010 (0.014)
Train: 106 [1050/1251 ( 84%)]  Loss: 3.609 (3.77)  Time: 0.784s, 1305.58/s  (0.803s, 1275.11/s)  LR: 9.257e-04  Data: 0.010 (0.014)
Train: 106 [1100/1251 ( 88%)]  Loss: 3.879 (3.78)  Time: 0.801s, 1277.92/s  (0.803s, 1274.84/s)  LR: 9.257e-04  Data: 0.015 (0.013)
Train: 106 [1150/1251 ( 92%)]  Loss: 3.895 (3.78)  Time: 0.836s, 1224.47/s  (0.803s, 1274.90/s)  LR: 9.257e-04  Data: 0.013 (0.013)
Train: 106 [1200/1251 ( 96%)]  Loss: 3.936 (3.79)  Time: 0.840s, 1219.10/s  (0.803s, 1274.66/s)  LR: 9.257e-04  Data: 0.010 (0.013)
Train: 106 [1250/1251 (100%)]  Loss: 3.811 (3.79)  Time: 0.763s, 1341.66/s  (0.803s, 1274.52/s)  LR: 9.257e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.631 (1.631)  Loss:  1.0420 (1.0420)  Acc@1: 88.1836 (88.1836)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.194 (0.610)  Loss:  1.0264 (1.5051)  Acc@1: 81.8396 (71.8420)  Acc@5: 96.2264 (90.9900)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-103.pth.tar', 72.05000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-105.pth.tar', 72.00599989013672)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-106.pth.tar', 71.84200012695312)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-90.pth.tar', 71.7159999584961)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-101.pth.tar', 71.61000012207032)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-102.pth.tar', 71.59999999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-92.pth.tar', 71.59600001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-99.pth.tar', 71.59600001708985)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-94.pth.tar', 71.57599991210938)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-89.pth.tar', 71.45599999023437)

Train: 107 [   0/1251 (  0%)]  Loss: 3.407 (3.41)  Time: 2.449s,  418.06/s  (2.449s,  418.06/s)  LR: 9.243e-04  Data: 1.700 (1.700)
Train: 107 [  50/1251 (  4%)]  Loss: 3.721 (3.56)  Time: 0.854s, 1198.67/s  (0.842s, 1215.66/s)  LR: 9.243e-04  Data: 0.009 (0.049)
Train: 107 [ 100/1251 (  8%)]  Loss: 3.664 (3.60)  Time: 0.822s, 1245.41/s  (0.822s, 1246.45/s)  LR: 9.243e-04  Data: 0.010 (0.030)
Train: 107 [ 150/1251 ( 12%)]  Loss: 4.227 (3.75)  Time: 0.948s, 1080.29/s  (0.815s, 1256.81/s)  LR: 9.243e-04  Data: 0.010 (0.024)
Train: 107 [ 200/1251 ( 16%)]  Loss: 3.826 (3.77)  Time: 0.782s, 1309.34/s  (0.812s, 1260.41/s)  LR: 9.243e-04  Data: 0.010 (0.021)
Train: 107 [ 250/1251 ( 20%)]  Loss: 3.813 (3.78)  Time: 0.788s, 1300.12/s  (0.811s, 1262.04/s)  LR: 9.243e-04  Data: 0.010 (0.019)
Train: 107 [ 300/1251 ( 24%)]  Loss: 3.566 (3.75)  Time: 0.868s, 1179.81/s  (0.810s, 1264.31/s)  LR: 9.243e-04  Data: 0.010 (0.018)
Train: 107 [ 350/1251 ( 28%)]  Loss: 3.743 (3.75)  Time: 0.819s, 1249.87/s  (0.809s, 1265.22/s)  LR: 9.243e-04  Data: 0.013 (0.017)
Train: 107 [ 400/1251 ( 32%)]  Loss: 3.616 (3.73)  Time: 0.854s, 1198.41/s  (0.809s, 1265.76/s)  LR: 9.243e-04  Data: 0.010 (0.016)
Train: 107 [ 450/1251 ( 36%)]  Loss: 3.849 (3.74)  Time: 0.828s, 1237.12/s  (0.808s, 1266.91/s)  LR: 9.243e-04  Data: 0.014 (0.016)
Train: 107 [ 500/1251 ( 40%)]  Loss: 3.861 (3.75)  Time: 0.830s, 1233.56/s  (0.808s, 1267.34/s)  LR: 9.243e-04  Data: 0.009 (0.015)
Train: 107 [ 550/1251 ( 44%)]  Loss: 3.627 (3.74)  Time: 0.785s, 1303.93/s  (0.808s, 1268.10/s)  LR: 9.243e-04  Data: 0.014 (0.015)
Train: 107 [ 600/1251 ( 48%)]  Loss: 3.983 (3.76)  Time: 0.772s, 1326.30/s  (0.807s, 1269.39/s)  LR: 9.243e-04  Data: 0.010 (0.014)
Train: 107 [ 650/1251 ( 52%)]  Loss: 3.383 (3.73)  Time: 0.804s, 1273.79/s  (0.806s, 1270.13/s)  LR: 9.243e-04  Data: 0.014 (0.014)
Train: 107 [ 700/1251 ( 56%)]  Loss: 3.682 (3.73)  Time: 0.798s, 1282.44/s  (0.806s, 1270.13/s)  LR: 9.243e-04  Data: 0.020 (0.014)
Train: 107 [ 750/1251 ( 60%)]  Loss: 3.594 (3.72)  Time: 0.814s, 1257.43/s  (0.806s, 1270.62/s)  LR: 9.243e-04  Data: 0.010 (0.014)
Train: 107 [ 800/1251 ( 64%)]  Loss: 3.661 (3.72)  Time: 0.790s, 1296.56/s  (0.806s, 1271.09/s)  LR: 9.243e-04  Data: 0.010 (0.014)
Train: 107 [ 850/1251 ( 68%)]  Loss: 3.660 (3.72)  Time: 0.797s, 1284.31/s  (0.806s, 1271.07/s)  LR: 9.243e-04  Data: 0.010 (0.014)
Train: 107 [ 900/1251 ( 72%)]  Loss: 3.729 (3.72)  Time: 0.838s, 1221.57/s  (0.805s, 1271.55/s)  LR: 9.243e-04  Data: 0.010 (0.013)
Train: 107 [ 950/1251 ( 76%)]  Loss: 3.582 (3.71)  Time: 0.788s, 1299.48/s  (0.805s, 1271.84/s)  LR: 9.243e-04  Data: 0.015 (0.013)
Train: 107 [1000/1251 ( 80%)]  Loss: 3.722 (3.71)  Time: 0.796s, 1287.14/s  (0.805s, 1272.28/s)  LR: 9.243e-04  Data: 0.015 (0.013)
Train: 107 [1050/1251 ( 84%)]  Loss: 3.423 (3.70)  Time: 0.785s, 1304.49/s  (0.805s, 1272.27/s)  LR: 9.243e-04  Data: 0.014 (0.013)
Train: 107 [1100/1251 ( 88%)]  Loss: 3.500 (3.69)  Time: 0.798s, 1282.48/s  (0.805s, 1272.61/s)  LR: 9.243e-04  Data: 0.010 (0.013)
Train: 107 [1150/1251 ( 92%)]  Loss: 3.813 (3.69)  Time: 0.803s, 1274.72/s  (0.804s, 1273.04/s)  LR: 9.243e-04  Data: 0.010 (0.013)
Train: 107 [1200/1251 ( 96%)]  Loss: 3.706 (3.69)  Time: 0.812s, 1260.84/s  (0.804s, 1273.25/s)  LR: 9.243e-04  Data: 0.010 (0.013)
Train: 107 [1250/1251 (100%)]  Loss: 3.659 (3.69)  Time: 0.764s, 1339.58/s  (0.804s, 1273.31/s)  LR: 9.243e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.660 (1.660)  Loss:  0.8677 (0.8677)  Acc@1: 87.5977 (87.5977)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.194 (0.613)  Loss:  0.9229 (1.5005)  Acc@1: 82.7830 (71.7640)  Acc@5: 96.1085 (90.9740)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-103.pth.tar', 72.05000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-105.pth.tar', 72.00599989013672)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-106.pth.tar', 71.84200012695312)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-107.pth.tar', 71.76400001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-90.pth.tar', 71.7159999584961)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-101.pth.tar', 71.61000012207032)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-102.pth.tar', 71.59999999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-92.pth.tar', 71.59600001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-99.pth.tar', 71.59600001708985)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-94.pth.tar', 71.57599991210938)

Train: 108 [   0/1251 (  0%)]  Loss: 3.963 (3.96)  Time: 2.362s,  433.62/s  (2.362s,  433.62/s)  LR: 9.229e-04  Data: 1.614 (1.614)
Train: 108 [  50/1251 (  4%)]  Loss: 3.513 (3.74)  Time: 0.781s, 1310.44/s  (0.837s, 1223.77/s)  LR: 9.229e-04  Data: 0.010 (0.048)
Train: 108 [ 100/1251 (  8%)]  Loss: 3.272 (3.58)  Time: 0.822s, 1245.24/s  (0.823s, 1243.51/s)  LR: 9.229e-04  Data: 0.014 (0.030)
Train: 108 [ 150/1251 ( 12%)]  Loss: 3.775 (3.63)  Time: 0.804s, 1273.55/s  (0.817s, 1253.21/s)  LR: 9.229e-04  Data: 0.010 (0.024)
Train: 108 [ 200/1251 ( 16%)]  Loss: 3.749 (3.65)  Time: 0.772s, 1326.96/s  (0.814s, 1258.29/s)  LR: 9.229e-04  Data: 0.010 (0.020)
Train: 108 [ 250/1251 ( 20%)]  Loss: 3.764 (3.67)  Time: 0.796s, 1286.13/s  (0.812s, 1261.84/s)  LR: 9.229e-04  Data: 0.010 (0.019)
Train: 108 [ 300/1251 ( 24%)]  Loss: 3.963 (3.71)  Time: 0.772s, 1325.60/s  (0.809s, 1265.17/s)  LR: 9.229e-04  Data: 0.010 (0.017)
Train: 108 [ 350/1251 ( 28%)]  Loss: 3.616 (3.70)  Time: 0.784s, 1306.60/s  (0.808s, 1267.89/s)  LR: 9.229e-04  Data: 0.014 (0.016)
Train: 108 [ 400/1251 ( 32%)]  Loss: 3.822 (3.72)  Time: 0.829s, 1235.80/s  (0.807s, 1269.27/s)  LR: 9.229e-04  Data: 0.010 (0.016)
Train: 108 [ 450/1251 ( 36%)]  Loss: 4.017 (3.75)  Time: 0.815s, 1256.27/s  (0.807s, 1269.49/s)  LR: 9.229e-04  Data: 0.015 (0.015)
Train: 108 [ 500/1251 ( 40%)]  Loss: 3.466 (3.72)  Time: 0.777s, 1318.19/s  (0.807s, 1269.25/s)  LR: 9.229e-04  Data: 0.010 (0.015)
Train: 108 [ 550/1251 ( 44%)]  Loss: 3.752 (3.72)  Time: 0.779s, 1314.41/s  (0.807s, 1269.33/s)  LR: 9.229e-04  Data: 0.011 (0.015)
Train: 108 [ 600/1251 ( 48%)]  Loss: 3.783 (3.73)  Time: 0.821s, 1247.95/s  (0.806s, 1269.95/s)  LR: 9.229e-04  Data: 0.010 (0.014)
Train: 108 [ 650/1251 ( 52%)]  Loss: 3.778 (3.73)  Time: 0.786s, 1302.53/s  (0.806s, 1270.09/s)  LR: 9.229e-04  Data: 0.010 (0.014)
Train: 108 [ 700/1251 ( 56%)]  Loss: 3.896 (3.74)  Time: 0.797s, 1285.24/s  (0.806s, 1270.47/s)  LR: 9.229e-04  Data: 0.013 (0.014)
Train: 108 [ 750/1251 ( 60%)]  Loss: 3.902 (3.75)  Time: 0.816s, 1254.23/s  (0.806s, 1270.94/s)  LR: 9.229e-04  Data: 0.010 (0.014)
Train: 108 [ 800/1251 ( 64%)]  Loss: 3.924 (3.76)  Time: 0.777s, 1318.07/s  (0.805s, 1271.94/s)  LR: 9.229e-04  Data: 0.009 (0.013)
Train: 108 [ 850/1251 ( 68%)]  Loss: 3.603 (3.75)  Time: 0.784s, 1305.42/s  (0.805s, 1272.19/s)  LR: 9.229e-04  Data: 0.016 (0.013)
Train: 108 [ 900/1251 ( 72%)]  Loss: 3.701 (3.75)  Time: 0.786s, 1303.29/s  (0.805s, 1272.31/s)  LR: 9.229e-04  Data: 0.015 (0.013)
Train: 108 [ 950/1251 ( 76%)]  Loss: 3.531 (3.74)  Time: 0.782s, 1308.92/s  (0.805s, 1272.18/s)  LR: 9.229e-04  Data: 0.010 (0.013)
Train: 108 [1000/1251 ( 80%)]  Loss: 3.514 (3.73)  Time: 0.825s, 1240.71/s  (0.805s, 1272.75/s)  LR: 9.229e-04  Data: 0.012 (0.013)
Train: 108 [1050/1251 ( 84%)]  Loss: 3.619 (3.72)  Time: 0.776s, 1319.05/s  (0.805s, 1272.78/s)  LR: 9.229e-04  Data: 0.012 (0.013)
Train: 108 [1100/1251 ( 88%)]  Loss: 3.680 (3.72)  Time: 0.833s, 1228.88/s  (0.804s, 1273.04/s)  LR: 9.229e-04  Data: 0.010 (0.013)
Train: 108 [1150/1251 ( 92%)]  Loss: 4.054 (3.74)  Time: 0.789s, 1298.01/s  (0.804s, 1273.58/s)  LR: 9.229e-04  Data: 0.010 (0.013)
Train: 108 [1200/1251 ( 96%)]  Loss: 3.817 (3.74)  Time: 0.815s, 1255.70/s  (0.804s, 1273.73/s)  LR: 9.229e-04  Data: 0.011 (0.013)
Train: 108 [1250/1251 (100%)]  Loss: 3.917 (3.75)  Time: 0.776s, 1319.66/s  (0.804s, 1274.24/s)  LR: 9.229e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.650 (1.650)  Loss:  0.9102 (0.9102)  Acc@1: 88.1836 (88.1836)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.194 (0.611)  Loss:  0.9043 (1.4048)  Acc@1: 83.9623 (72.0100)  Acc@5: 95.6368 (91.0420)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-103.pth.tar', 72.05000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-108.pth.tar', 72.01000001464844)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-105.pth.tar', 72.00599989013672)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-106.pth.tar', 71.84200012695312)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-107.pth.tar', 71.76400001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-90.pth.tar', 71.7159999584961)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-101.pth.tar', 71.61000012207032)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-102.pth.tar', 71.59999999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-92.pth.tar', 71.59600001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-99.pth.tar', 71.59600001708985)

Train: 109 [   0/1251 (  0%)]  Loss: 3.737 (3.74)  Time: 2.447s,  418.41/s  (2.447s,  418.41/s)  LR: 9.215e-04  Data: 1.717 (1.717)
Train: 109 [  50/1251 (  4%)]  Loss: 3.763 (3.75)  Time: 0.821s, 1246.96/s  (0.843s, 1214.39/s)  LR: 9.215e-04  Data: 0.010 (0.048)
Train: 109 [ 100/1251 (  8%)]  Loss: 3.981 (3.83)  Time: 0.839s, 1220.30/s  (0.818s, 1252.54/s)  LR: 9.215e-04  Data: 0.010 (0.030)
Train: 109 [ 150/1251 ( 12%)]  Loss: 3.252 (3.68)  Time: 0.779s, 1313.97/s  (0.813s, 1259.36/s)  LR: 9.215e-04  Data: 0.010 (0.024)
Train: 109 [ 200/1251 ( 16%)]  Loss: 3.693 (3.69)  Time: 0.801s, 1278.22/s  (0.811s, 1263.30/s)  LR: 9.215e-04  Data: 0.010 (0.021)
Train: 109 [ 250/1251 ( 20%)]  Loss: 4.008 (3.74)  Time: 0.828s, 1237.25/s  (0.808s, 1266.91/s)  LR: 9.215e-04  Data: 0.009 (0.019)
Train: 109 [ 300/1251 ( 24%)]  Loss: 3.903 (3.76)  Time: 0.912s, 1123.10/s  (0.807s, 1268.57/s)  LR: 9.215e-04  Data: 0.010 (0.017)
Train: 109 [ 350/1251 ( 28%)]  Loss: 3.729 (3.76)  Time: 0.773s, 1324.40/s  (0.806s, 1270.77/s)  LR: 9.215e-04  Data: 0.010 (0.016)
Train: 109 [ 400/1251 ( 32%)]  Loss: 3.930 (3.78)  Time: 0.818s, 1252.30/s  (0.806s, 1269.78/s)  LR: 9.215e-04  Data: 0.010 (0.016)
Train: 109 [ 450/1251 ( 36%)]  Loss: 3.935 (3.79)  Time: 0.779s, 1313.76/s  (0.806s, 1271.09/s)  LR: 9.215e-04  Data: 0.009 (0.015)
Train: 109 [ 500/1251 ( 40%)]  Loss: 3.999 (3.81)  Time: 0.770s, 1329.87/s  (0.805s, 1271.30/s)  LR: 9.215e-04  Data: 0.010 (0.015)
Train: 109 [ 550/1251 ( 44%)]  Loss: 3.898 (3.82)  Time: 0.797s, 1284.05/s  (0.805s, 1272.25/s)  LR: 9.215e-04  Data: 0.011 (0.014)
Train: 109 [ 600/1251 ( 48%)]  Loss: 3.734 (3.81)  Time: 0.819s, 1250.33/s  (0.804s, 1273.34/s)  LR: 9.215e-04  Data: 0.010 (0.014)
Train: 109 [ 650/1251 ( 52%)]  Loss: 3.661 (3.80)  Time: 0.801s, 1278.67/s  (0.804s, 1273.39/s)  LR: 9.215e-04  Data: 0.010 (0.014)
Train: 109 [ 700/1251 ( 56%)]  Loss: 3.468 (3.78)  Time: 0.789s, 1297.99/s  (0.804s, 1274.15/s)  LR: 9.215e-04  Data: 0.011 (0.014)
Train: 109 [ 750/1251 ( 60%)]  Loss: 3.776 (3.78)  Time: 0.769s, 1331.10/s  (0.804s, 1274.36/s)  LR: 9.215e-04  Data: 0.010 (0.014)
Train: 109 [ 800/1251 ( 64%)]  Loss: 4.332 (3.81)  Time: 0.817s, 1253.77/s  (0.803s, 1275.15/s)  LR: 9.215e-04  Data: 0.013 (0.013)
Train: 109 [ 850/1251 ( 68%)]  Loss: 3.682 (3.80)  Time: 0.791s, 1294.41/s  (0.803s, 1275.12/s)  LR: 9.215e-04  Data: 0.010 (0.013)
Train: 109 [ 900/1251 ( 72%)]  Loss: 3.989 (3.81)  Time: 0.774s, 1322.45/s  (0.803s, 1275.65/s)  LR: 9.215e-04  Data: 0.012 (0.013)
Train: 109 [ 950/1251 ( 76%)]  Loss: 3.533 (3.80)  Time: 0.792s, 1293.47/s  (0.803s, 1275.84/s)  LR: 9.215e-04  Data: 0.012 (0.013)
Train: 109 [1000/1251 ( 80%)]  Loss: 3.370 (3.78)  Time: 0.791s, 1295.15/s  (0.802s, 1276.14/s)  LR: 9.215e-04  Data: 0.009 (0.013)
Train: 109 [1050/1251 ( 84%)]  Loss: 4.084 (3.79)  Time: 0.809s, 1266.16/s  (0.802s, 1276.11/s)  LR: 9.215e-04  Data: 0.010 (0.013)
Train: 109 [1100/1251 ( 88%)]  Loss: 3.758 (3.79)  Time: 0.852s, 1201.73/s  (0.802s, 1276.14/s)  LR: 9.215e-04  Data: 0.011 (0.013)
Train: 109 [1150/1251 ( 92%)]  Loss: 3.841 (3.79)  Time: 0.774s, 1323.45/s  (0.802s, 1276.30/s)  LR: 9.215e-04  Data: 0.010 (0.013)
Train: 109 [1200/1251 ( 96%)]  Loss: 3.413 (3.78)  Time: 0.890s, 1150.80/s  (0.802s, 1276.53/s)  LR: 9.215e-04  Data: 0.011 (0.013)
Train: 109 [1250/1251 (100%)]  Loss: 3.943 (3.79)  Time: 0.762s, 1344.21/s  (0.802s, 1276.51/s)  LR: 9.215e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.686 (1.686)  Loss:  0.9941 (0.9941)  Acc@1: 88.3789 (88.3789)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.193 (0.599)  Loss:  0.9219 (1.5326)  Acc@1: 84.3160 (71.7900)  Acc@5: 96.4623 (90.9080)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-103.pth.tar', 72.05000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-108.pth.tar', 72.01000001464844)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-105.pth.tar', 72.00599989013672)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-106.pth.tar', 71.84200012695312)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-109.pth.tar', 71.7900000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-107.pth.tar', 71.76400001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-90.pth.tar', 71.7159999584961)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-101.pth.tar', 71.61000012207032)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-102.pth.tar', 71.59999999267578)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-92.pth.tar', 71.59600001953125)

Train: 110 [   0/1251 (  0%)]  Loss: 3.740 (3.74)  Time: 2.452s,  417.58/s  (2.452s,  417.58/s)  LR: 9.201e-04  Data: 1.717 (1.717)
Train: 110 [  50/1251 (  4%)]  Loss: 3.687 (3.71)  Time: 0.802s, 1277.47/s  (0.835s, 1226.27/s)  LR: 9.201e-04  Data: 0.014 (0.049)
Train: 110 [ 100/1251 (  8%)]  Loss: 3.840 (3.76)  Time: 0.830s, 1233.85/s  (0.817s, 1253.23/s)  LR: 9.201e-04  Data: 0.010 (0.030)
Train: 110 [ 150/1251 ( 12%)]  Loss: 3.767 (3.76)  Time: 0.805s, 1272.09/s  (0.813s, 1260.14/s)  LR: 9.201e-04  Data: 0.013 (0.024)
Train: 110 [ 200/1251 ( 16%)]  Loss: 3.800 (3.77)  Time: 0.888s, 1153.01/s  (0.810s, 1263.68/s)  LR: 9.201e-04  Data: 0.010 (0.021)
Train: 110 [ 250/1251 ( 20%)]  Loss: 3.783 (3.77)  Time: 0.815s, 1256.16/s  (0.809s, 1265.22/s)  LR: 9.201e-04  Data: 0.022 (0.019)
Train: 110 [ 300/1251 ( 24%)]  Loss: 3.777 (3.77)  Time: 0.821s, 1247.12/s  (0.808s, 1267.25/s)  LR: 9.201e-04  Data: 0.010 (0.018)
Train: 110 [ 350/1251 ( 28%)]  Loss: 3.659 (3.76)  Time: 0.776s, 1320.27/s  (0.807s, 1268.65/s)  LR: 9.201e-04  Data: 0.010 (0.017)
Train: 110 [ 400/1251 ( 32%)]  Loss: 3.864 (3.77)  Time: 0.774s, 1322.77/s  (0.806s, 1269.86/s)  LR: 9.201e-04  Data: 0.011 (0.016)
Train: 110 [ 450/1251 ( 36%)]  Loss: 3.676 (3.76)  Time: 0.776s, 1320.04/s  (0.806s, 1270.44/s)  LR: 9.201e-04  Data: 0.010 (0.015)
Train: 110 [ 500/1251 ( 40%)]  Loss: 3.560 (3.74)  Time: 0.793s, 1290.55/s  (0.805s, 1271.33/s)  LR: 9.201e-04  Data: 0.015 (0.015)
Train: 110 [ 550/1251 ( 44%)]  Loss: 3.901 (3.75)  Time: 0.798s, 1282.92/s  (0.805s, 1271.65/s)  LR: 9.201e-04  Data: 0.010 (0.015)
Train: 110 [ 600/1251 ( 48%)]  Loss: 3.627 (3.74)  Time: 0.782s, 1308.95/s  (0.805s, 1272.04/s)  LR: 9.201e-04  Data: 0.010 (0.014)
Train: 110 [ 650/1251 ( 52%)]  Loss: 3.891 (3.76)  Time: 0.832s, 1230.36/s  (0.805s, 1272.59/s)  LR: 9.201e-04  Data: 0.011 (0.014)
Train: 110 [ 700/1251 ( 56%)]  Loss: 3.597 (3.74)  Time: 0.787s, 1301.61/s  (0.805s, 1271.81/s)  LR: 9.201e-04  Data: 0.010 (0.014)
Train: 110 [ 750/1251 ( 60%)]  Loss: 3.715 (3.74)  Time: 0.795s, 1288.78/s  (0.805s, 1272.61/s)  LR: 9.201e-04  Data: 0.009 (0.014)
Train: 110 [ 800/1251 ( 64%)]  Loss: 3.963 (3.76)  Time: 0.789s, 1297.43/s  (0.805s, 1272.49/s)  LR: 9.201e-04  Data: 0.016 (0.014)
Train: 110 [ 850/1251 ( 68%)]  Loss: 3.901 (3.76)  Time: 0.791s, 1294.44/s  (0.804s, 1273.23/s)  LR: 9.201e-04  Data: 0.015 (0.013)
Train: 110 [ 900/1251 ( 72%)]  Loss: 3.918 (3.77)  Time: 0.776s, 1318.87/s  (0.804s, 1273.57/s)  LR: 9.201e-04  Data: 0.010 (0.013)
Train: 110 [ 950/1251 ( 76%)]  Loss: 3.936 (3.78)  Time: 0.834s, 1228.38/s  (0.804s, 1273.59/s)  LR: 9.201e-04  Data: 0.011 (0.013)
Train: 110 [1000/1251 ( 80%)]  Loss: 3.463 (3.77)  Time: 0.773s, 1323.95/s  (0.804s, 1274.30/s)  LR: 9.201e-04  Data: 0.011 (0.013)
Train: 110 [1050/1251 ( 84%)]  Loss: 3.751 (3.76)  Time: 0.854s, 1199.13/s  (0.804s, 1274.08/s)  LR: 9.201e-04  Data: 0.010 (0.013)
Train: 110 [1100/1251 ( 88%)]  Loss: 3.181 (3.74)  Time: 0.773s, 1325.51/s  (0.803s, 1274.72/s)  LR: 9.201e-04  Data: 0.010 (0.013)
Train: 110 [1150/1251 ( 92%)]  Loss: 3.952 (3.75)  Time: 0.782s, 1309.62/s  (0.803s, 1274.76/s)  LR: 9.201e-04  Data: 0.010 (0.013)
Train: 110 [1200/1251 ( 96%)]  Loss: 3.812 (3.75)  Time: 0.799s, 1281.58/s  (0.804s, 1274.26/s)  LR: 9.201e-04  Data: 0.011 (0.013)
Train: 110 [1250/1251 (100%)]  Loss: 3.567 (3.74)  Time: 0.789s, 1298.60/s  (0.803s, 1274.57/s)  LR: 9.201e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.558 (1.558)  Loss:  0.8936 (0.8936)  Acc@1: 87.2070 (87.2070)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.194 (0.608)  Loss:  0.9277 (1.4471)  Acc@1: 84.1981 (71.8960)  Acc@5: 95.2830 (91.1860)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-103.pth.tar', 72.05000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-108.pth.tar', 72.01000001464844)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-105.pth.tar', 72.00599989013672)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-110.pth.tar', 71.8960001171875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-106.pth.tar', 71.84200012695312)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-109.pth.tar', 71.7900000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-107.pth.tar', 71.76400001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-90.pth.tar', 71.7159999584961)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-101.pth.tar', 71.61000012207032)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-102.pth.tar', 71.59999999267578)

Train: 111 [   0/1251 (  0%)]  Loss: 3.455 (3.45)  Time: 2.686s,  381.24/s  (2.686s,  381.24/s)  LR: 9.187e-04  Data: 1.945 (1.945)
Train: 111 [  50/1251 (  4%)]  Loss: 3.659 (3.56)  Time: 0.786s, 1303.23/s  (0.839s, 1219.83/s)  LR: 9.187e-04  Data: 0.010 (0.052)
Train: 111 [ 100/1251 (  8%)]  Loss: 3.645 (3.59)  Time: 0.845s, 1212.24/s  (0.821s, 1247.54/s)  LR: 9.187e-04  Data: 0.010 (0.032)
Train: 111 [ 150/1251 ( 12%)]  Loss: 4.075 (3.71)  Time: 0.781s, 1310.47/s  (0.814s, 1257.79/s)  LR: 9.187e-04  Data: 0.009 (0.025)
Train: 111 [ 200/1251 ( 16%)]  Loss: 3.232 (3.61)  Time: 0.793s, 1291.99/s  (0.811s, 1262.62/s)  LR: 9.187e-04  Data: 0.010 (0.021)
Train: 111 [ 250/1251 ( 20%)]  Loss: 3.951 (3.67)  Time: 0.804s, 1274.42/s  (0.809s, 1266.19/s)  LR: 9.187e-04  Data: 0.010 (0.019)
Train: 111 [ 300/1251 ( 24%)]  Loss: 3.856 (3.70)  Time: 0.782s, 1309.70/s  (0.807s, 1269.42/s)  LR: 9.187e-04  Data: 0.010 (0.018)
Train: 111 [ 350/1251 ( 28%)]  Loss: 4.166 (3.75)  Time: 0.773s, 1324.06/s  (0.805s, 1271.45/s)  LR: 9.187e-04  Data: 0.010 (0.017)
Train: 111 [ 400/1251 ( 32%)]  Loss: 3.316 (3.71)  Time: 0.802s, 1276.26/s  (0.805s, 1271.71/s)  LR: 9.187e-04  Data: 0.012 (0.016)
Train: 111 [ 450/1251 ( 36%)]  Loss: 3.947 (3.73)  Time: 0.813s, 1259.97/s  (0.807s, 1268.57/s)  LR: 9.187e-04  Data: 0.010 (0.016)
Train: 111 [ 500/1251 ( 40%)]  Loss: 3.218 (3.68)  Time: 0.773s, 1324.86/s  (0.805s, 1271.88/s)  LR: 9.187e-04  Data: 0.010 (0.015)
Train: 111 [ 550/1251 ( 44%)]  Loss: 3.657 (3.68)  Time: 0.774s, 1323.24/s  (0.803s, 1275.98/s)  LR: 9.187e-04  Data: 0.010 (0.015)
Train: 111 [ 600/1251 ( 48%)]  Loss: 3.567 (3.67)  Time: 0.795s, 1287.53/s  (0.802s, 1277.05/s)  LR: 9.187e-04  Data: 0.012 (0.015)
Train: 111 [ 650/1251 ( 52%)]  Loss: 3.916 (3.69)  Time: 0.798s, 1283.68/s  (0.802s, 1276.70/s)  LR: 9.187e-04  Data: 0.009 (0.014)
Train: 111 [ 700/1251 ( 56%)]  Loss: 3.956 (3.71)  Time: 0.773s, 1324.76/s  (0.802s, 1277.00/s)  LR: 9.187e-04  Data: 0.010 (0.014)
Train: 111 [ 750/1251 ( 60%)]  Loss: 3.830 (3.72)  Time: 0.772s, 1326.34/s  (0.802s, 1276.57/s)  LR: 9.187e-04  Data: 0.010 (0.014)
Train: 111 [ 800/1251 ( 64%)]  Loss: 3.509 (3.70)  Time: 0.784s, 1306.31/s  (0.802s, 1276.98/s)  LR: 9.187e-04  Data: 0.010 (0.014)
Train: 111 [ 850/1251 ( 68%)]  Loss: 3.862 (3.71)  Time: 0.786s, 1302.81/s  (0.802s, 1276.97/s)  LR: 9.187e-04  Data: 0.010 (0.014)
Train: 111 [ 900/1251 ( 72%)]  Loss: 3.830 (3.72)  Time: 0.820s, 1248.85/s  (0.802s, 1277.05/s)  LR: 9.187e-04  Data: 0.010 (0.013)
Train: 111 [ 950/1251 ( 76%)]  Loss: 3.816 (3.72)  Time: 0.779s, 1314.93/s  (0.802s, 1277.16/s)  LR: 9.187e-04  Data: 0.011 (0.013)
Train: 111 [1000/1251 ( 80%)]  Loss: 3.791 (3.73)  Time: 0.778s, 1315.54/s  (0.802s, 1276.98/s)  LR: 9.187e-04  Data: 0.010 (0.013)
Train: 111 [1050/1251 ( 84%)]  Loss: 3.534 (3.72)  Time: 0.809s, 1266.51/s  (0.802s, 1277.01/s)  LR: 9.187e-04  Data: 0.010 (0.013)
Train: 111 [1100/1251 ( 88%)]  Loss: 4.004 (3.73)  Time: 0.833s, 1229.71/s  (0.802s, 1276.10/s)  LR: 9.187e-04  Data: 0.014 (0.013)
Train: 111 [1150/1251 ( 92%)]  Loss: 3.894 (3.74)  Time: 0.791s, 1295.29/s  (0.802s, 1276.50/s)  LR: 9.187e-04  Data: 0.010 (0.013)
Train: 111 [1200/1251 ( 96%)]  Loss: 4.097 (3.75)  Time: 0.771s, 1328.02/s  (0.802s, 1276.74/s)  LR: 9.187e-04  Data: 0.010 (0.013)
Train: 111 [1250/1251 (100%)]  Loss: 4.000 (3.76)  Time: 0.772s, 1327.07/s  (0.802s, 1276.50/s)  LR: 9.187e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.633 (1.633)  Loss:  0.8931 (0.8931)  Acc@1: 86.4258 (86.4258)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.194 (0.600)  Loss:  0.8418 (1.4237)  Acc@1: 82.7830 (71.7340)  Acc@5: 95.9906 (90.9360)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-103.pth.tar', 72.05000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-108.pth.tar', 72.01000001464844)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-105.pth.tar', 72.00599989013672)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-110.pth.tar', 71.8960001171875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-106.pth.tar', 71.84200012695312)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-109.pth.tar', 71.7900000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-107.pth.tar', 71.76400001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-111.pth.tar', 71.73400001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-90.pth.tar', 71.7159999584961)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-101.pth.tar', 71.61000012207032)

Train: 112 [   0/1251 (  0%)]  Loss: 3.871 (3.87)  Time: 2.318s,  441.69/s  (2.318s,  441.69/s)  LR: 9.173e-04  Data: 1.587 (1.587)
Train: 112 [  50/1251 (  4%)]  Loss: 3.481 (3.68)  Time: 0.833s, 1229.79/s  (0.842s, 1216.80/s)  LR: 9.173e-04  Data: 0.015 (0.051)
Train: 112 [ 100/1251 (  8%)]  Loss: 3.830 (3.73)  Time: 0.864s, 1185.84/s  (0.823s, 1243.49/s)  LR: 9.173e-04  Data: 0.010 (0.031)
Train: 112 [ 150/1251 ( 12%)]  Loss: 3.582 (3.69)  Time: 0.820s, 1248.65/s  (0.819s, 1250.88/s)  LR: 9.173e-04  Data: 0.010 (0.025)
Train: 112 [ 200/1251 ( 16%)]  Loss: 3.913 (3.74)  Time: 0.784s, 1305.67/s  (0.815s, 1256.50/s)  LR: 9.173e-04  Data: 0.013 (0.021)
Train: 112 [ 250/1251 ( 20%)]  Loss: 3.756 (3.74)  Time: 0.771s, 1327.72/s  (0.813s, 1258.97/s)  LR: 9.173e-04  Data: 0.010 (0.019)
Train: 112 [ 300/1251 ( 24%)]  Loss: 3.463 (3.70)  Time: 0.863s, 1186.30/s  (0.811s, 1262.01/s)  LR: 9.173e-04  Data: 0.014 (0.018)
Train: 112 [ 350/1251 ( 28%)]  Loss: 3.866 (3.72)  Time: 0.842s, 1216.21/s  (0.810s, 1263.79/s)  LR: 9.173e-04  Data: 0.010 (0.017)
Train: 112 [ 400/1251 ( 32%)]  Loss: 3.723 (3.72)  Time: 0.804s, 1273.75/s  (0.809s, 1265.46/s)  LR: 9.173e-04  Data: 0.011 (0.016)
Train: 112 [ 450/1251 ( 36%)]  Loss: 3.584 (3.71)  Time: 0.772s, 1326.66/s  (0.808s, 1266.85/s)  LR: 9.173e-04  Data: 0.010 (0.016)
Train: 112 [ 500/1251 ( 40%)]  Loss: 3.730 (3.71)  Time: 0.809s, 1265.11/s  (0.807s, 1268.23/s)  LR: 9.173e-04  Data: 0.010 (0.015)
Train: 112 [ 550/1251 ( 44%)]  Loss: 3.690 (3.71)  Time: 0.818s, 1251.78/s  (0.807s, 1268.61/s)  LR: 9.173e-04  Data: 0.014 (0.015)
Train: 112 [ 600/1251 ( 48%)]  Loss: 3.864 (3.72)  Time: 0.819s, 1250.97/s  (0.807s, 1268.37/s)  LR: 9.173e-04  Data: 0.010 (0.015)
Train: 112 [ 650/1251 ( 52%)]  Loss: 3.689 (3.72)  Time: 0.783s, 1308.50/s  (0.806s, 1269.97/s)  LR: 9.173e-04  Data: 0.010 (0.014)
Train: 112 [ 700/1251 ( 56%)]  Loss: 3.632 (3.71)  Time: 0.800s, 1280.10/s  (0.806s, 1270.37/s)  LR: 9.173e-04  Data: 0.010 (0.014)
Train: 112 [ 750/1251 ( 60%)]  Loss: 3.806 (3.72)  Time: 0.775s, 1321.12/s  (0.805s, 1271.47/s)  LR: 9.173e-04  Data: 0.010 (0.014)
Train: 112 [ 800/1251 ( 64%)]  Loss: 3.872 (3.73)  Time: 0.780s, 1312.29/s  (0.805s, 1272.40/s)  LR: 9.173e-04  Data: 0.010 (0.014)
Train: 112 [ 850/1251 ( 68%)]  Loss: 3.685 (3.72)  Time: 0.804s, 1273.97/s  (0.805s, 1272.53/s)  LR: 9.173e-04  Data: 0.010 (0.014)
Train: 112 [ 900/1251 ( 72%)]  Loss: 3.813 (3.73)  Time: 0.843s, 1215.08/s  (0.804s, 1272.87/s)  LR: 9.173e-04  Data: 0.015 (0.013)
Train: 112 [ 950/1251 ( 76%)]  Loss: 3.909 (3.74)  Time: 0.839s, 1220.43/s  (0.805s, 1272.40/s)  LR: 9.173e-04  Data: 0.014 (0.013)
Train: 112 [1000/1251 ( 80%)]  Loss: 3.838 (3.74)  Time: 0.835s, 1225.70/s  (0.804s, 1272.92/s)  LR: 9.173e-04  Data: 0.015 (0.013)
Train: 112 [1050/1251 ( 84%)]  Loss: 3.664 (3.74)  Time: 0.839s, 1220.62/s  (0.805s, 1272.66/s)  LR: 9.173e-04  Data: 0.010 (0.013)
Train: 112 [1100/1251 ( 88%)]  Loss: 3.878 (3.75)  Time: 0.814s, 1258.02/s  (0.804s, 1272.97/s)  LR: 9.173e-04  Data: 0.011 (0.013)
Train: 112 [1150/1251 ( 92%)]  Loss: 3.905 (3.75)  Time: 0.785s, 1304.48/s  (0.804s, 1273.06/s)  LR: 9.173e-04  Data: 0.014 (0.013)
Train: 112 [1200/1251 ( 96%)]  Loss: 3.457 (3.74)  Time: 0.777s, 1317.15/s  (0.804s, 1273.26/s)  LR: 9.173e-04  Data: 0.011 (0.013)
Train: 112 [1250/1251 (100%)]  Loss: 3.552 (3.73)  Time: 0.811s, 1263.19/s  (0.804s, 1273.64/s)  LR: 9.173e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.664 (1.664)  Loss:  0.8613 (0.8613)  Acc@1: 87.5000 (87.5000)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.194 (0.599)  Loss:  1.0000 (1.4894)  Acc@1: 83.8443 (71.6040)  Acc@5: 95.2830 (90.9320)
Train: 113 [   0/1251 (  0%)]  Loss: 3.995 (3.99)  Time: 2.491s,  411.03/s  (2.491s,  411.03/s)  LR: 9.159e-04  Data: 1.763 (1.763)
Train: 113 [  50/1251 (  4%)]  Loss: 3.750 (3.87)  Time: 0.797s, 1284.46/s  (0.836s, 1225.26/s)  LR: 9.159e-04  Data: 0.009 (0.048)
Train: 113 [ 100/1251 (  8%)]  Loss: 3.963 (3.90)  Time: 0.836s, 1224.32/s  (0.820s, 1249.09/s)  LR: 9.159e-04  Data: 0.012 (0.030)
Train: 113 [ 150/1251 ( 12%)]  Loss: 4.006 (3.93)  Time: 0.780s, 1313.48/s  (0.813s, 1259.64/s)  LR: 9.159e-04  Data: 0.012 (0.024)
Train: 113 [ 200/1251 ( 16%)]  Loss: 3.581 (3.86)  Time: 0.825s, 1240.88/s  (0.811s, 1262.16/s)  LR: 9.159e-04  Data: 0.012 (0.020)
Train: 113 [ 250/1251 ( 20%)]  Loss: 3.803 (3.85)  Time: 0.802s, 1276.04/s  (0.811s, 1262.38/s)  LR: 9.159e-04  Data: 0.010 (0.019)
Train: 113 [ 300/1251 ( 24%)]  Loss: 3.523 (3.80)  Time: 0.775s, 1321.99/s  (0.810s, 1264.48/s)  LR: 9.159e-04  Data: 0.011 (0.017)
Train: 113 [ 350/1251 ( 28%)]  Loss: 3.494 (3.76)  Time: 0.803s, 1274.89/s  (0.808s, 1267.96/s)  LR: 9.159e-04  Data: 0.011 (0.016)
Train: 113 [ 400/1251 ( 32%)]  Loss: 3.862 (3.78)  Time: 0.817s, 1254.00/s  (0.807s, 1268.56/s)  LR: 9.159e-04  Data: 0.010 (0.016)
Train: 113 [ 450/1251 ( 36%)]  Loss: 3.678 (3.77)  Time: 0.853s, 1200.46/s  (0.806s, 1270.51/s)  LR: 9.159e-04  Data: 0.009 (0.015)
Train: 113 [ 500/1251 ( 40%)]  Loss: 3.553 (3.75)  Time: 0.789s, 1297.39/s  (0.805s, 1271.87/s)  LR: 9.159e-04  Data: 0.010 (0.015)
Train: 113 [ 550/1251 ( 44%)]  Loss: 3.980 (3.77)  Time: 0.773s, 1325.07/s  (0.805s, 1272.78/s)  LR: 9.159e-04  Data: 0.010 (0.014)
Train: 113 [ 600/1251 ( 48%)]  Loss: 3.601 (3.75)  Time: 0.822s, 1245.78/s  (0.805s, 1272.21/s)  LR: 9.159e-04  Data: 0.009 (0.014)
Train: 113 [ 650/1251 ( 52%)]  Loss: 3.801 (3.76)  Time: 0.810s, 1264.95/s  (0.805s, 1272.50/s)  LR: 9.159e-04  Data: 0.010 (0.014)
Train: 113 [ 700/1251 ( 56%)]  Loss: 3.649 (3.75)  Time: 0.790s, 1295.64/s  (0.804s, 1273.06/s)  LR: 9.159e-04  Data: 0.016 (0.014)
Train: 113 [ 750/1251 ( 60%)]  Loss: 3.750 (3.75)  Time: 0.780s, 1313.02/s  (0.804s, 1273.52/s)  LR: 9.159e-04  Data: 0.010 (0.014)
Train: 113 [ 800/1251 ( 64%)]  Loss: 3.552 (3.74)  Time: 0.782s, 1309.10/s  (0.804s, 1273.71/s)  LR: 9.159e-04  Data: 0.010 (0.013)
Train: 113 [ 850/1251 ( 68%)]  Loss: 3.718 (3.74)  Time: 0.810s, 1264.92/s  (0.804s, 1274.00/s)  LR: 9.159e-04  Data: 0.014 (0.013)
Train: 113 [ 900/1251 ( 72%)]  Loss: 3.797 (3.74)  Time: 0.834s, 1227.45/s  (0.803s, 1274.42/s)  LR: 9.159e-04  Data: 0.010 (0.013)
Train: 113 [ 950/1251 ( 76%)]  Loss: 3.293 (3.72)  Time: 0.793s, 1290.86/s  (0.804s, 1274.40/s)  LR: 9.159e-04  Data: 0.014 (0.013)
Train: 113 [1000/1251 ( 80%)]  Loss: 3.906 (3.73)  Time: 0.801s, 1278.87/s  (0.803s, 1274.85/s)  LR: 9.159e-04  Data: 0.010 (0.013)
Train: 113 [1050/1251 ( 84%)]  Loss: 3.492 (3.72)  Time: 0.776s, 1320.00/s  (0.803s, 1275.39/s)  LR: 9.159e-04  Data: 0.009 (0.013)
Train: 113 [1100/1251 ( 88%)]  Loss: 3.556 (3.71)  Time: 0.820s, 1249.09/s  (0.803s, 1275.19/s)  LR: 9.159e-04  Data: 0.010 (0.013)
Train: 113 [1150/1251 ( 92%)]  Loss: 3.816 (3.71)  Time: 0.816s, 1254.54/s  (0.803s, 1275.19/s)  LR: 9.159e-04  Data: 0.010 (0.013)
Train: 113 [1200/1251 ( 96%)]  Loss: 3.618 (3.71)  Time: 0.919s, 1113.97/s  (0.803s, 1275.55/s)  LR: 9.159e-04  Data: 0.012 (0.013)
Train: 113 [1250/1251 (100%)]  Loss: 3.443 (3.70)  Time: 0.759s, 1349.85/s  (0.803s, 1275.38/s)  LR: 9.159e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.695 (1.695)  Loss:  0.8867 (0.8867)  Acc@1: 87.1094 (87.1094)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.193 (0.604)  Loss:  1.0352 (1.5069)  Acc@1: 82.3113 (71.6140)  Acc@5: 95.1651 (90.6880)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-103.pth.tar', 72.05000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-108.pth.tar', 72.01000001464844)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-105.pth.tar', 72.00599989013672)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-110.pth.tar', 71.8960001171875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-106.pth.tar', 71.84200012695312)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-109.pth.tar', 71.7900000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-107.pth.tar', 71.76400001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-111.pth.tar', 71.73400001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-90.pth.tar', 71.7159999584961)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-113.pth.tar', 71.61400007324218)

Train: 114 [   0/1251 (  0%)]  Loss: 3.576 (3.58)  Time: 2.462s,  415.96/s  (2.462s,  415.96/s)  LR: 9.144e-04  Data: 1.689 (1.689)
Train: 114 [  50/1251 (  4%)]  Loss: 3.638 (3.61)  Time: 0.798s, 1283.72/s  (0.842s, 1216.46/s)  LR: 9.144e-04  Data: 0.010 (0.051)
Train: 114 [ 100/1251 (  8%)]  Loss: 3.468 (3.56)  Time: 0.841s, 1217.72/s  (0.821s, 1246.99/s)  LR: 9.144e-04  Data: 0.010 (0.031)
Train: 114 [ 150/1251 ( 12%)]  Loss: 3.800 (3.62)  Time: 0.806s, 1269.85/s  (0.814s, 1258.42/s)  LR: 9.144e-04  Data: 0.010 (0.025)
Train: 114 [ 200/1251 ( 16%)]  Loss: 3.720 (3.64)  Time: 0.809s, 1265.25/s  (0.812s, 1261.64/s)  LR: 9.144e-04  Data: 0.011 (0.021)
Train: 114 [ 250/1251 ( 20%)]  Loss: 3.620 (3.64)  Time: 0.825s, 1241.90/s  (0.809s, 1266.12/s)  LR: 9.144e-04  Data: 0.010 (0.019)
Train: 114 [ 300/1251 ( 24%)]  Loss: 3.710 (3.65)  Time: 0.791s, 1295.30/s  (0.807s, 1268.64/s)  LR: 9.144e-04  Data: 0.010 (0.018)
Train: 114 [ 350/1251 ( 28%)]  Loss: 3.456 (3.62)  Time: 0.818s, 1251.49/s  (0.807s, 1268.90/s)  LR: 9.144e-04  Data: 0.010 (0.017)
Train: 114 [ 400/1251 ( 32%)]  Loss: 3.678 (3.63)  Time: 0.804s, 1273.44/s  (0.807s, 1269.44/s)  LR: 9.144e-04  Data: 0.010 (0.016)
Train: 114 [ 450/1251 ( 36%)]  Loss: 3.839 (3.65)  Time: 0.773s, 1324.72/s  (0.807s, 1269.47/s)  LR: 9.144e-04  Data: 0.010 (0.016)
Train: 114 [ 500/1251 ( 40%)]  Loss: 3.568 (3.64)  Time: 0.773s, 1325.09/s  (0.806s, 1270.78/s)  LR: 9.144e-04  Data: 0.011 (0.015)
Train: 114 [ 550/1251 ( 44%)]  Loss: 3.707 (3.65)  Time: 0.800s, 1279.61/s  (0.805s, 1271.48/s)  LR: 9.144e-04  Data: 0.014 (0.015)
Train: 114 [ 600/1251 ( 48%)]  Loss: 3.331 (3.62)  Time: 0.829s, 1234.74/s  (0.805s, 1272.55/s)  LR: 9.144e-04  Data: 0.010 (0.015)
Train: 114 [ 650/1251 ( 52%)]  Loss: 4.052 (3.65)  Time: 0.802s, 1276.45/s  (0.804s, 1272.85/s)  LR: 9.144e-04  Data: 0.011 (0.014)
Train: 114 [ 700/1251 ( 56%)]  Loss: 3.828 (3.67)  Time: 0.810s, 1263.52/s  (0.805s, 1272.83/s)  LR: 9.144e-04  Data: 0.012 (0.014)
Train: 114 [ 750/1251 ( 60%)]  Loss: 3.987 (3.69)  Time: 0.844s, 1213.86/s  (0.804s, 1272.95/s)  LR: 9.144e-04  Data: 0.010 (0.014)
Train: 114 [ 800/1251 ( 64%)]  Loss: 4.084 (3.71)  Time: 0.831s, 1232.18/s  (0.804s, 1273.91/s)  LR: 9.144e-04  Data: 0.011 (0.014)
Train: 114 [ 850/1251 ( 68%)]  Loss: 3.428 (3.69)  Time: 0.816s, 1255.33/s  (0.804s, 1273.80/s)  LR: 9.144e-04  Data: 0.010 (0.014)
Train: 114 [ 900/1251 ( 72%)]  Loss: 4.065 (3.71)  Time: 0.793s, 1291.80/s  (0.804s, 1273.51/s)  LR: 9.144e-04  Data: 0.014 (0.013)
Train: 114 [ 950/1251 ( 76%)]  Loss: 3.770 (3.72)  Time: 0.798s, 1283.44/s  (0.804s, 1273.21/s)  LR: 9.144e-04  Data: 0.019 (0.013)
Train: 114 [1000/1251 ( 80%)]  Loss: 3.770 (3.72)  Time: 0.775s, 1321.08/s  (0.804s, 1274.05/s)  LR: 9.144e-04  Data: 0.010 (0.013)
Train: 114 [1050/1251 ( 84%)]  Loss: 3.991 (3.73)  Time: 0.774s, 1322.28/s  (0.804s, 1274.10/s)  LR: 9.144e-04  Data: 0.011 (0.013)
Train: 114 [1100/1251 ( 88%)]  Loss: 3.773 (3.73)  Time: 0.771s, 1327.93/s  (0.804s, 1274.23/s)  LR: 9.144e-04  Data: 0.010 (0.013)
Train: 114 [1150/1251 ( 92%)]  Loss: 3.663 (3.73)  Time: 0.812s, 1260.58/s  (0.803s, 1274.50/s)  LR: 9.144e-04  Data: 0.010 (0.013)
Train: 114 [1200/1251 ( 96%)]  Loss: 3.884 (3.74)  Time: 0.785s, 1303.64/s  (0.803s, 1274.52/s)  LR: 9.144e-04  Data: 0.013 (0.013)
Train: 114 [1250/1251 (100%)]  Loss: 3.492 (3.73)  Time: 0.771s, 1328.22/s  (0.804s, 1274.11/s)  LR: 9.144e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.678 (1.678)  Loss:  0.8594 (0.8594)  Acc@1: 89.4531 (89.4531)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.194 (0.605)  Loss:  0.9165 (1.4404)  Acc@1: 83.6085 (72.4020)  Acc@5: 94.8113 (91.1560)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-114.pth.tar', 72.40200011962891)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-103.pth.tar', 72.05000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-108.pth.tar', 72.01000001464844)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-105.pth.tar', 72.00599989013672)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-110.pth.tar', 71.8960001171875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-106.pth.tar', 71.84200012695312)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-109.pth.tar', 71.7900000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-107.pth.tar', 71.76400001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-111.pth.tar', 71.73400001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-90.pth.tar', 71.7159999584961)

Train: 115 [   0/1251 (  0%)]  Loss: 3.497 (3.50)  Time: 2.416s,  423.85/s  (2.416s,  423.85/s)  LR: 9.129e-04  Data: 1.652 (1.652)
Train: 115 [  50/1251 (  4%)]  Loss: 4.032 (3.76)  Time: 0.830s, 1233.51/s  (0.835s, 1225.82/s)  LR: 9.129e-04  Data: 0.010 (0.050)
Train: 115 [ 100/1251 (  8%)]  Loss: 3.583 (3.70)  Time: 0.776s, 1319.66/s  (0.816s, 1255.35/s)  LR: 9.129e-04  Data: 0.011 (0.030)
Train: 115 [ 150/1251 ( 12%)]  Loss: 3.575 (3.67)  Time: 0.812s, 1260.93/s  (0.813s, 1260.08/s)  LR: 9.129e-04  Data: 0.010 (0.024)
Train: 115 [ 200/1251 ( 16%)]  Loss: 3.718 (3.68)  Time: 0.811s, 1262.41/s  (0.809s, 1265.72/s)  LR: 9.129e-04  Data: 0.010 (0.021)
Train: 115 [ 250/1251 ( 20%)]  Loss: 3.444 (3.64)  Time: 0.789s, 1298.15/s  (0.807s, 1268.86/s)  LR: 9.129e-04  Data: 0.010 (0.019)
Train: 115 [ 300/1251 ( 24%)]  Loss: 3.662 (3.64)  Time: 0.785s, 1304.35/s  (0.806s, 1270.55/s)  LR: 9.129e-04  Data: 0.014 (0.018)
Train: 115 [ 350/1251 ( 28%)]  Loss: 3.824 (3.67)  Time: 0.771s, 1327.74/s  (0.806s, 1270.51/s)  LR: 9.129e-04  Data: 0.010 (0.017)
Train: 115 [ 400/1251 ( 32%)]  Loss: 3.272 (3.62)  Time: 0.778s, 1316.13/s  (0.805s, 1271.85/s)  LR: 9.129e-04  Data: 0.011 (0.016)
Train: 115 [ 450/1251 ( 36%)]  Loss: 3.841 (3.64)  Time: 0.803s, 1275.09/s  (0.805s, 1272.79/s)  LR: 9.129e-04  Data: 0.010 (0.015)
Train: 115 [ 500/1251 ( 40%)]  Loss: 3.685 (3.65)  Time: 0.776s, 1320.16/s  (0.804s, 1273.25/s)  LR: 9.129e-04  Data: 0.010 (0.015)
Train: 115 [ 550/1251 ( 44%)]  Loss: 3.643 (3.65)  Time: 0.773s, 1324.30/s  (0.804s, 1273.97/s)  LR: 9.129e-04  Data: 0.012 (0.015)
Train: 115 [ 600/1251 ( 48%)]  Loss: 3.862 (3.66)  Time: 0.781s, 1311.67/s  (0.804s, 1273.88/s)  LR: 9.129e-04  Data: 0.010 (0.014)
Train: 115 [ 650/1251 ( 52%)]  Loss: 3.775 (3.67)  Time: 0.778s, 1315.35/s  (0.804s, 1273.58/s)  LR: 9.129e-04  Data: 0.012 (0.014)
Train: 115 [ 700/1251 ( 56%)]  Loss: 3.712 (3.68)  Time: 0.818s, 1252.00/s  (0.804s, 1274.18/s)  LR: 9.129e-04  Data: 0.010 (0.014)
Train: 115 [ 750/1251 ( 60%)]  Loss: 3.796 (3.68)  Time: 0.772s, 1326.22/s  (0.803s, 1274.80/s)  LR: 9.129e-04  Data: 0.011 (0.014)
Train: 115 [ 800/1251 ( 64%)]  Loss: 3.770 (3.69)  Time: 0.815s, 1256.51/s  (0.803s, 1275.50/s)  LR: 9.129e-04  Data: 0.010 (0.013)
Train: 115 [ 850/1251 ( 68%)]  Loss: 3.690 (3.69)  Time: 0.819s, 1250.54/s  (0.803s, 1274.92/s)  LR: 9.129e-04  Data: 0.009 (0.013)
Train: 115 [ 900/1251 ( 72%)]  Loss: 3.464 (3.68)  Time: 0.818s, 1251.11/s  (0.803s, 1274.70/s)  LR: 9.129e-04  Data: 0.010 (0.013)
Train: 115 [ 950/1251 ( 76%)]  Loss: 3.776 (3.68)  Time: 0.804s, 1274.05/s  (0.803s, 1274.82/s)  LR: 9.129e-04  Data: 0.015 (0.013)
Train: 115 [1000/1251 ( 80%)]  Loss: 3.829 (3.69)  Time: 0.781s, 1311.49/s  (0.803s, 1274.93/s)  LR: 9.129e-04  Data: 0.010 (0.013)
Train: 115 [1050/1251 ( 84%)]  Loss: 3.375 (3.67)  Time: 0.844s, 1213.91/s  (0.803s, 1275.11/s)  LR: 9.129e-04  Data: 0.010 (0.013)
Train: 115 [1100/1251 ( 88%)]  Loss: 3.663 (3.67)  Time: 0.786s, 1302.96/s  (0.803s, 1275.14/s)  LR: 9.129e-04  Data: 0.014 (0.013)
Train: 115 [1150/1251 ( 92%)]  Loss: 3.487 (3.67)  Time: 0.784s, 1305.65/s  (0.803s, 1275.36/s)  LR: 9.129e-04  Data: 0.013 (0.013)
Train: 115 [1200/1251 ( 96%)]  Loss: 3.977 (3.68)  Time: 0.777s, 1317.50/s  (0.803s, 1275.57/s)  LR: 9.129e-04  Data: 0.010 (0.013)
Train: 115 [1250/1251 (100%)]  Loss: 3.776 (3.68)  Time: 0.760s, 1347.91/s  (0.803s, 1275.98/s)  LR: 9.129e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.612 (1.612)  Loss:  0.8501 (0.8501)  Acc@1: 87.0117 (87.0117)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.195 (0.751)  Loss:  0.9634 (1.4756)  Acc@1: 85.1415 (71.7540)  Acc@5: 96.2264 (90.9320)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-114.pth.tar', 72.40200011962891)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-103.pth.tar', 72.05000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-108.pth.tar', 72.01000001464844)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-105.pth.tar', 72.00599989013672)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-110.pth.tar', 71.8960001171875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-106.pth.tar', 71.84200012695312)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-109.pth.tar', 71.7900000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-107.pth.tar', 71.76400001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-115.pth.tar', 71.7539998803711)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-111.pth.tar', 71.73400001953125)

Train: 116 [   0/1251 (  0%)]  Loss: 3.816 (3.82)  Time: 2.757s,  371.47/s  (2.757s,  371.47/s)  LR: 9.115e-04  Data: 2.011 (2.011)
Train: 116 [  50/1251 (  4%)]  Loss: 4.046 (3.93)  Time: 0.785s, 1303.86/s  (0.860s, 1190.92/s)  LR: 9.115e-04  Data: 0.010 (0.070)
Train: 116 [ 100/1251 (  8%)]  Loss: 3.703 (3.85)  Time: 0.786s, 1303.11/s  (0.818s, 1251.19/s)  LR: 9.115e-04  Data: 0.017 (0.041)
Train: 116 [ 150/1251 ( 12%)]  Loss: 3.677 (3.81)  Time: 0.773s, 1324.61/s  (0.812s, 1261.22/s)  LR: 9.115e-04  Data: 0.010 (0.031)
Train: 116 [ 200/1251 ( 16%)]  Loss: 3.911 (3.83)  Time: 0.832s, 1230.93/s  (0.811s, 1262.77/s)  LR: 9.115e-04  Data: 0.010 (0.026)
Train: 116 [ 250/1251 ( 20%)]  Loss: 3.935 (3.85)  Time: 0.812s, 1260.38/s  (0.808s, 1267.10/s)  LR: 9.115e-04  Data: 0.010 (0.023)
Train: 116 [ 300/1251 ( 24%)]  Loss: 3.880 (3.85)  Time: 0.810s, 1264.45/s  (0.807s, 1269.25/s)  LR: 9.115e-04  Data: 0.010 (0.021)
Train: 116 [ 350/1251 ( 28%)]  Loss: 4.141 (3.89)  Time: 0.824s, 1243.20/s  (0.806s, 1270.36/s)  LR: 9.115e-04  Data: 0.009 (0.019)
Train: 116 [ 400/1251 ( 32%)]  Loss: 3.903 (3.89)  Time: 0.812s, 1260.85/s  (0.806s, 1270.39/s)  LR: 9.115e-04  Data: 0.010 (0.018)
Train: 116 [ 450/1251 ( 36%)]  Loss: 3.612 (3.86)  Time: 0.815s, 1256.20/s  (0.805s, 1272.17/s)  LR: 9.115e-04  Data: 0.010 (0.018)
Train: 116 [ 500/1251 ( 40%)]  Loss: 3.898 (3.87)  Time: 0.805s, 1271.68/s  (0.804s, 1273.09/s)  LR: 9.115e-04  Data: 0.009 (0.017)
Train: 116 [ 550/1251 ( 44%)]  Loss: 3.748 (3.86)  Time: 0.799s, 1280.81/s  (0.804s, 1272.98/s)  LR: 9.115e-04  Data: 0.011 (0.016)
Train: 116 [ 600/1251 ( 48%)]  Loss: 3.837 (3.85)  Time: 0.817s, 1253.29/s  (0.804s, 1273.78/s)  LR: 9.115e-04  Data: 0.009 (0.016)
Train: 116 [ 650/1251 ( 52%)]  Loss: 3.781 (3.85)  Time: 0.805s, 1272.08/s  (0.803s, 1274.50/s)  LR: 9.115e-04  Data: 0.010 (0.016)
Train: 116 [ 700/1251 ( 56%)]  Loss: 3.557 (3.83)  Time: 0.815s, 1256.02/s  (0.803s, 1274.71/s)  LR: 9.115e-04  Data: 0.010 (0.015)
Train: 116 [ 750/1251 ( 60%)]  Loss: 3.916 (3.84)  Time: 0.816s, 1254.52/s  (0.803s, 1275.24/s)  LR: 9.115e-04  Data: 0.010 (0.015)
Train: 116 [ 800/1251 ( 64%)]  Loss: 4.036 (3.85)  Time: 0.815s, 1256.85/s  (0.803s, 1275.87/s)  LR: 9.115e-04  Data: 0.011 (0.015)
Train: 116 [ 850/1251 ( 68%)]  Loss: 4.264 (3.87)  Time: 0.775s, 1321.54/s  (0.803s, 1275.83/s)  LR: 9.115e-04  Data: 0.011 (0.014)
Train: 116 [ 900/1251 ( 72%)]  Loss: 3.820 (3.87)  Time: 0.790s, 1296.64/s  (0.802s, 1276.02/s)  LR: 9.115e-04  Data: 0.010 (0.014)
Train: 116 [ 950/1251 ( 76%)]  Loss: 3.904 (3.87)  Time: 0.802s, 1276.56/s  (0.803s, 1275.84/s)  LR: 9.115e-04  Data: 0.010 (0.014)
Train: 116 [1000/1251 ( 80%)]  Loss: 3.461 (3.85)  Time: 0.820s, 1249.22/s  (0.802s, 1276.25/s)  LR: 9.115e-04  Data: 0.010 (0.014)
Train: 116 [1050/1251 ( 84%)]  Loss: 3.715 (3.84)  Time: 0.802s, 1276.51/s  (0.802s, 1276.14/s)  LR: 9.115e-04  Data: 0.013 (0.014)
Train: 116 [1100/1251 ( 88%)]  Loss: 4.045 (3.85)  Time: 0.817s, 1252.71/s  (0.802s, 1276.13/s)  LR: 9.115e-04  Data: 0.010 (0.014)
Train: 116 [1150/1251 ( 92%)]  Loss: 3.765 (3.85)  Time: 0.772s, 1326.59/s  (0.802s, 1276.12/s)  LR: 9.115e-04  Data: 0.010 (0.014)
Train: 116 [1200/1251 ( 96%)]  Loss: 4.026 (3.86)  Time: 0.771s, 1327.80/s  (0.803s, 1276.01/s)  LR: 9.115e-04  Data: 0.009 (0.013)
Train: 116 [1250/1251 (100%)]  Loss: 3.877 (3.86)  Time: 0.769s, 1331.11/s  (0.802s, 1276.05/s)  LR: 9.115e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.662 (1.662)  Loss:  0.9014 (0.9014)  Acc@1: 88.4766 (88.4766)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.194 (0.605)  Loss:  1.0352 (1.5024)  Acc@1: 84.0802 (71.7820)  Acc@5: 95.1651 (90.9100)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-114.pth.tar', 72.40200011962891)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-103.pth.tar', 72.05000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-108.pth.tar', 72.01000001464844)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-105.pth.tar', 72.00599989013672)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-110.pth.tar', 71.8960001171875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-106.pth.tar', 71.84200012695312)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-109.pth.tar', 71.7900000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-116.pth.tar', 71.78200006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-107.pth.tar', 71.76400001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-115.pth.tar', 71.7539998803711)

Train: 117 [   0/1251 (  0%)]  Loss: 3.640 (3.64)  Time: 2.484s,  412.16/s  (2.484s,  412.16/s)  LR: 9.100e-04  Data: 1.756 (1.756)
Train: 117 [  50/1251 (  4%)]  Loss: 3.838 (3.74)  Time: 0.806s, 1269.92/s  (0.839s, 1220.72/s)  LR: 9.100e-04  Data: 0.010 (0.048)
Train: 117 [ 100/1251 (  8%)]  Loss: 3.471 (3.65)  Time: 0.816s, 1255.27/s  (0.820s, 1248.98/s)  LR: 9.100e-04  Data: 0.011 (0.030)
Train: 117 [ 150/1251 ( 12%)]  Loss: 3.653 (3.65)  Time: 0.834s, 1227.69/s  (0.812s, 1260.53/s)  LR: 9.100e-04  Data: 0.010 (0.024)
Train: 117 [ 200/1251 ( 16%)]  Loss: 3.414 (3.60)  Time: 0.775s, 1320.93/s  (0.809s, 1265.79/s)  LR: 9.100e-04  Data: 0.014 (0.021)
Train: 117 [ 250/1251 ( 20%)]  Loss: 3.941 (3.66)  Time: 0.807s, 1268.99/s  (0.808s, 1266.76/s)  LR: 9.100e-04  Data: 0.010 (0.019)
Train: 117 [ 300/1251 ( 24%)]  Loss: 3.222 (3.60)  Time: 0.890s, 1151.06/s  (0.808s, 1267.12/s)  LR: 9.100e-04  Data: 0.010 (0.017)
Train: 117 [ 350/1251 ( 28%)]  Loss: 3.589 (3.60)  Time: 0.773s, 1324.72/s  (0.808s, 1268.08/s)  LR: 9.100e-04  Data: 0.011 (0.017)
Train: 117 [ 400/1251 ( 32%)]  Loss: 3.843 (3.62)  Time: 0.820s, 1248.37/s  (0.807s, 1269.34/s)  LR: 9.100e-04  Data: 0.015 (0.016)
Train: 117 [ 450/1251 ( 36%)]  Loss: 3.983 (3.66)  Time: 0.845s, 1212.40/s  (0.806s, 1270.79/s)  LR: 9.100e-04  Data: 0.014 (0.015)
Train: 117 [ 500/1251 ( 40%)]  Loss: 3.851 (3.68)  Time: 0.829s, 1235.88/s  (0.806s, 1270.89/s)  LR: 9.100e-04  Data: 0.011 (0.015)
Train: 117 [ 550/1251 ( 44%)]  Loss: 3.542 (3.67)  Time: 0.818s, 1251.57/s  (0.806s, 1270.91/s)  LR: 9.100e-04  Data: 0.012 (0.015)
Train: 117 [ 600/1251 ( 48%)]  Loss: 3.876 (3.68)  Time: 0.773s, 1323.87/s  (0.805s, 1271.85/s)  LR: 9.100e-04  Data: 0.010 (0.014)
Train: 117 [ 650/1251 ( 52%)]  Loss: 3.674 (3.68)  Time: 0.771s, 1327.32/s  (0.805s, 1271.99/s)  LR: 9.100e-04  Data: 0.010 (0.014)
Train: 117 [ 700/1251 ( 56%)]  Loss: 3.872 (3.69)  Time: 0.861s, 1188.96/s  (0.805s, 1271.58/s)  LR: 9.100e-04  Data: 0.010 (0.014)
Train: 117 [ 750/1251 ( 60%)]  Loss: 3.536 (3.68)  Time: 0.771s, 1328.58/s  (0.805s, 1271.74/s)  LR: 9.100e-04  Data: 0.010 (0.014)
Train: 117 [ 800/1251 ( 64%)]  Loss: 3.601 (3.68)  Time: 0.775s, 1321.98/s  (0.805s, 1272.63/s)  LR: 9.100e-04  Data: 0.010 (0.013)
Train: 117 [ 850/1251 ( 68%)]  Loss: 3.612 (3.68)  Time: 0.792s, 1293.59/s  (0.804s, 1273.14/s)  LR: 9.100e-04  Data: 0.018 (0.013)
Train: 117 [ 900/1251 ( 72%)]  Loss: 3.746 (3.68)  Time: 0.816s, 1254.69/s  (0.805s, 1272.60/s)  LR: 9.100e-04  Data: 0.009 (0.013)
Train: 117 [ 950/1251 ( 76%)]  Loss: 3.947 (3.69)  Time: 0.794s, 1290.36/s  (0.804s, 1272.89/s)  LR: 9.100e-04  Data: 0.015 (0.013)
Train: 117 [1000/1251 ( 80%)]  Loss: 3.604 (3.69)  Time: 0.776s, 1320.23/s  (0.804s, 1273.60/s)  LR: 9.100e-04  Data: 0.010 (0.013)
Train: 117 [1050/1251 ( 84%)]  Loss: 3.752 (3.69)  Time: 0.774s, 1322.33/s  (0.804s, 1273.30/s)  LR: 9.100e-04  Data: 0.010 (0.013)
Train: 117 [1100/1251 ( 88%)]  Loss: 4.109 (3.71)  Time: 0.831s, 1232.76/s  (0.804s, 1272.94/s)  LR: 9.100e-04  Data: 0.010 (0.013)
Train: 117 [1150/1251 ( 92%)]  Loss: 3.496 (3.70)  Time: 0.819s, 1249.61/s  (0.804s, 1273.48/s)  LR: 9.100e-04  Data: 0.010 (0.013)
Train: 117 [1200/1251 ( 96%)]  Loss: 3.857 (3.71)  Time: 0.815s, 1257.13/s  (0.804s, 1273.25/s)  LR: 9.100e-04  Data: 0.011 (0.013)
Train: 117 [1250/1251 (100%)]  Loss: 3.372 (3.69)  Time: 0.780s, 1312.21/s  (0.804s, 1273.69/s)  LR: 9.100e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.640 (1.640)  Loss:  0.7393 (0.7393)  Acc@1: 88.2812 (88.2812)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.194 (0.597)  Loss:  0.9692 (1.4383)  Acc@1: 81.8396 (71.8540)  Acc@5: 94.8113 (91.1320)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-114.pth.tar', 72.40200011962891)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-103.pth.tar', 72.05000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-108.pth.tar', 72.01000001464844)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-105.pth.tar', 72.00599989013672)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-110.pth.tar', 71.8960001171875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-117.pth.tar', 71.85399999755859)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-106.pth.tar', 71.84200012695312)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-109.pth.tar', 71.7900000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-116.pth.tar', 71.78200006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-107.pth.tar', 71.76400001953125)

Train: 118 [   0/1251 (  0%)]  Loss: 3.120 (3.12)  Time: 2.522s,  405.99/s  (2.522s,  405.99/s)  LR: 9.085e-04  Data: 1.757 (1.757)
Train: 118 [  50/1251 (  4%)]  Loss: 3.924 (3.52)  Time: 0.800s, 1280.32/s  (0.843s, 1214.10/s)  LR: 9.085e-04  Data: 0.011 (0.056)
Train: 118 [ 100/1251 (  8%)]  Loss: 3.841 (3.63)  Time: 0.825s, 1240.62/s  (0.824s, 1243.17/s)  LR: 9.085e-04  Data: 0.009 (0.034)
Train: 118 [ 150/1251 ( 12%)]  Loss: 3.601 (3.62)  Time: 0.821s, 1246.96/s  (0.818s, 1252.02/s)  LR: 9.085e-04  Data: 0.010 (0.026)
Train: 118 [ 200/1251 ( 16%)]  Loss: 4.018 (3.70)  Time: 0.811s, 1262.29/s  (0.814s, 1257.91/s)  LR: 9.085e-04  Data: 0.011 (0.023)
Train: 118 [ 250/1251 ( 20%)]  Loss: 3.838 (3.72)  Time: 0.774s, 1323.23/s  (0.811s, 1262.95/s)  LR: 9.085e-04  Data: 0.010 (0.020)
Train: 118 [ 300/1251 ( 24%)]  Loss: 3.562 (3.70)  Time: 0.799s, 1281.58/s  (0.808s, 1267.54/s)  LR: 9.085e-04  Data: 0.010 (0.019)
Train: 118 [ 350/1251 ( 28%)]  Loss: 3.604 (3.69)  Time: 0.773s, 1325.02/s  (0.808s, 1267.12/s)  LR: 9.085e-04  Data: 0.010 (0.018)
Train: 118 [ 400/1251 ( 32%)]  Loss: 3.681 (3.69)  Time: 0.773s, 1325.33/s  (0.807s, 1268.50/s)  LR: 9.085e-04  Data: 0.011 (0.017)
Train: 118 [ 450/1251 ( 36%)]  Loss: 3.600 (3.68)  Time: 0.858s, 1193.71/s  (0.807s, 1268.97/s)  LR: 9.085e-04  Data: 0.009 (0.016)
Train: 118 [ 500/1251 ( 40%)]  Loss: 3.580 (3.67)  Time: 0.791s, 1294.85/s  (0.807s, 1269.66/s)  LR: 9.085e-04  Data: 0.010 (0.016)
Train: 118 [ 550/1251 ( 44%)]  Loss: 3.564 (3.66)  Time: 0.817s, 1253.29/s  (0.807s, 1269.00/s)  LR: 9.085e-04  Data: 0.011 (0.015)
Train: 118 [ 600/1251 ( 48%)]  Loss: 3.859 (3.68)  Time: 0.825s, 1241.88/s  (0.807s, 1269.65/s)  LR: 9.085e-04  Data: 0.013 (0.015)
Train: 118 [ 650/1251 ( 52%)]  Loss: 3.787 (3.68)  Time: 0.786s, 1303.51/s  (0.806s, 1270.81/s)  LR: 9.085e-04  Data: 0.010 (0.015)
Train: 118 [ 700/1251 ( 56%)]  Loss: 3.614 (3.68)  Time: 0.773s, 1324.21/s  (0.805s, 1271.55/s)  LR: 9.085e-04  Data: 0.010 (0.014)
Train: 118 [ 750/1251 ( 60%)]  Loss: 3.170 (3.65)  Time: 0.805s, 1272.40/s  (0.805s, 1272.40/s)  LR: 9.085e-04  Data: 0.011 (0.014)
Train: 118 [ 800/1251 ( 64%)]  Loss: 3.688 (3.65)  Time: 0.783s, 1308.04/s  (0.805s, 1272.81/s)  LR: 9.085e-04  Data: 0.010 (0.014)
Train: 118 [ 850/1251 ( 68%)]  Loss: 3.424 (3.64)  Time: 0.816s, 1255.45/s  (0.804s, 1272.97/s)  LR: 9.085e-04  Data: 0.010 (0.014)
Train: 118 [ 900/1251 ( 72%)]  Loss: 3.758 (3.64)  Time: 0.809s, 1265.27/s  (0.804s, 1272.93/s)  LR: 9.085e-04  Data: 0.012 (0.014)
Train: 118 [ 950/1251 ( 76%)]  Loss: 3.623 (3.64)  Time: 0.774s, 1322.25/s  (0.804s, 1273.27/s)  LR: 9.085e-04  Data: 0.010 (0.013)
Train: 118 [1000/1251 ( 80%)]  Loss: 3.491 (3.64)  Time: 0.789s, 1298.52/s  (0.804s, 1273.77/s)  LR: 9.085e-04  Data: 0.010 (0.013)
Train: 118 [1050/1251 ( 84%)]  Loss: 3.717 (3.64)  Time: 0.807s, 1269.39/s  (0.804s, 1274.12/s)  LR: 9.085e-04  Data: 0.011 (0.013)
Train: 118 [1100/1251 ( 88%)]  Loss: 3.600 (3.64)  Time: 0.804s, 1273.20/s  (0.804s, 1274.40/s)  LR: 9.085e-04  Data: 0.010 (0.013)
Train: 118 [1150/1251 ( 92%)]  Loss: 4.018 (3.65)  Time: 0.793s, 1291.73/s  (0.803s, 1274.65/s)  LR: 9.085e-04  Data: 0.014 (0.013)
Train: 118 [1200/1251 ( 96%)]  Loss: 4.001 (3.67)  Time: 0.793s, 1291.81/s  (0.803s, 1274.47/s)  LR: 9.085e-04  Data: 0.013 (0.013)
Train: 118 [1250/1251 (100%)]  Loss: 3.852 (3.67)  Time: 0.759s, 1349.01/s  (0.803s, 1274.44/s)  LR: 9.085e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.649 (1.649)  Loss:  0.9800 (0.9800)  Acc@1: 88.4766 (88.4766)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.194 (0.609)  Loss:  0.9302 (1.5135)  Acc@1: 85.7311 (72.0060)  Acc@5: 95.6368 (91.0100)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-114.pth.tar', 72.40200011962891)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-103.pth.tar', 72.05000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-108.pth.tar', 72.01000001464844)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-118.pth.tar', 72.00600000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-105.pth.tar', 72.00599989013672)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-110.pth.tar', 71.8960001171875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-117.pth.tar', 71.85399999755859)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-106.pth.tar', 71.84200012695312)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-109.pth.tar', 71.7900000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-116.pth.tar', 71.78200006591797)

Train: 119 [   0/1251 (  0%)]  Loss: 3.955 (3.95)  Time: 2.311s,  443.17/s  (2.311s,  443.17/s)  LR: 9.070e-04  Data: 1.561 (1.561)
Train: 119 [  50/1251 (  4%)]  Loss: 3.525 (3.74)  Time: 0.797s, 1285.54/s  (0.836s, 1224.61/s)  LR: 9.070e-04  Data: 0.013 (0.048)
Train: 119 [ 100/1251 (  8%)]  Loss: 3.526 (3.67)  Time: 0.836s, 1224.44/s  (0.823s, 1244.71/s)  LR: 9.070e-04  Data: 0.012 (0.030)
Train: 119 [ 150/1251 ( 12%)]  Loss: 3.557 (3.64)  Time: 0.778s, 1315.96/s  (0.817s, 1254.07/s)  LR: 9.070e-04  Data: 0.011 (0.024)
Train: 119 [ 200/1251 ( 16%)]  Loss: 3.546 (3.62)  Time: 0.826s, 1239.63/s  (0.812s, 1260.54/s)  LR: 9.070e-04  Data: 0.010 (0.021)
Train: 119 [ 250/1251 ( 20%)]  Loss: 3.528 (3.61)  Time: 0.771s, 1328.83/s  (0.810s, 1263.55/s)  LR: 9.070e-04  Data: 0.009 (0.019)
Train: 119 [ 300/1251 ( 24%)]  Loss: 3.453 (3.58)  Time: 0.803s, 1274.66/s  (0.810s, 1264.82/s)  LR: 9.070e-04  Data: 0.010 (0.017)
Train: 119 [ 350/1251 ( 28%)]  Loss: 3.868 (3.62)  Time: 0.771s, 1328.35/s  (0.810s, 1264.59/s)  LR: 9.070e-04  Data: 0.010 (0.017)
Train: 119 [ 400/1251 ( 32%)]  Loss: 3.883 (3.65)  Time: 0.828s, 1237.32/s  (0.809s, 1265.55/s)  LR: 9.070e-04  Data: 0.010 (0.016)
Train: 119 [ 450/1251 ( 36%)]  Loss: 3.626 (3.65)  Time: 0.783s, 1308.58/s  (0.808s, 1267.15/s)  LR: 9.070e-04  Data: 0.014 (0.015)
Train: 119 [ 500/1251 ( 40%)]  Loss: 3.232 (3.61)  Time: 0.797s, 1285.39/s  (0.808s, 1267.60/s)  LR: 9.070e-04  Data: 0.010 (0.015)
Train: 119 [ 550/1251 ( 44%)]  Loss: 3.669 (3.61)  Time: 0.789s, 1298.37/s  (0.807s, 1268.23/s)  LR: 9.070e-04  Data: 0.010 (0.015)
Train: 119 [ 600/1251 ( 48%)]  Loss: 3.559 (3.61)  Time: 0.775s, 1322.02/s  (0.807s, 1269.38/s)  LR: 9.070e-04  Data: 0.010 (0.014)
Train: 119 [ 650/1251 ( 52%)]  Loss: 3.667 (3.61)  Time: 0.797s, 1284.58/s  (0.807s, 1268.64/s)  LR: 9.070e-04  Data: 0.009 (0.014)
Train: 119 [ 700/1251 ( 56%)]  Loss: 3.455 (3.60)  Time: 0.784s, 1305.87/s  (0.807s, 1269.13/s)  LR: 9.070e-04  Data: 0.010 (0.014)
Train: 119 [ 750/1251 ( 60%)]  Loss: 3.701 (3.61)  Time: 0.828s, 1236.85/s  (0.806s, 1270.31/s)  LR: 9.070e-04  Data: 0.010 (0.014)
Train: 119 [ 800/1251 ( 64%)]  Loss: 3.923 (3.63)  Time: 0.820s, 1248.93/s  (0.806s, 1270.77/s)  LR: 9.070e-04  Data: 0.010 (0.013)
Train: 119 [ 850/1251 ( 68%)]  Loss: 3.905 (3.64)  Time: 0.789s, 1297.21/s  (0.806s, 1271.08/s)  LR: 9.070e-04  Data: 0.010 (0.013)
Train: 119 [ 900/1251 ( 72%)]  Loss: 3.638 (3.64)  Time: 0.811s, 1262.22/s  (0.806s, 1271.23/s)  LR: 9.070e-04  Data: 0.014 (0.013)
Train: 119 [ 950/1251 ( 76%)]  Loss: 3.746 (3.65)  Time: 0.772s, 1325.76/s  (0.805s, 1272.09/s)  LR: 9.070e-04  Data: 0.010 (0.013)
Train: 119 [1000/1251 ( 80%)]  Loss: 3.542 (3.64)  Time: 0.825s, 1240.92/s  (0.805s, 1272.11/s)  LR: 9.070e-04  Data: 0.011 (0.013)
Train: 119 [1050/1251 ( 84%)]  Loss: 3.604 (3.64)  Time: 0.886s, 1155.18/s  (0.805s, 1271.91/s)  LR: 9.070e-04  Data: 0.010 (0.013)
Train: 119 [1100/1251 ( 88%)]  Loss: 4.170 (3.66)  Time: 0.788s, 1299.10/s  (0.805s, 1272.33/s)  LR: 9.070e-04  Data: 0.009 (0.013)
Train: 119 [1150/1251 ( 92%)]  Loss: 3.672 (3.66)  Time: 0.784s, 1306.66/s  (0.804s, 1272.88/s)  LR: 9.070e-04  Data: 0.010 (0.013)
Train: 119 [1200/1251 ( 96%)]  Loss: 3.525 (3.66)  Time: 0.775s, 1320.91/s  (0.804s, 1273.18/s)  LR: 9.070e-04  Data: 0.010 (0.013)
Train: 119 [1250/1251 (100%)]  Loss: 3.914 (3.67)  Time: 0.780s, 1313.13/s  (0.804s, 1272.88/s)  LR: 9.070e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.690 (1.690)  Loss:  0.8267 (0.8267)  Acc@1: 88.3789 (88.3789)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.194 (0.608)  Loss:  0.9692 (1.4274)  Acc@1: 82.6651 (71.9680)  Acc@5: 95.0472 (91.0900)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-114.pth.tar', 72.40200011962891)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-103.pth.tar', 72.05000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-108.pth.tar', 72.01000001464844)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-118.pth.tar', 72.00600000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-105.pth.tar', 72.00599989013672)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-119.pth.tar', 71.96799996826172)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-110.pth.tar', 71.8960001171875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-117.pth.tar', 71.85399999755859)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-106.pth.tar', 71.84200012695312)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-109.pth.tar', 71.7900000390625)

Train: 120 [   0/1251 (  0%)]  Loss: 4.040 (4.04)  Time: 2.617s,  391.31/s  (2.617s,  391.31/s)  LR: 9.055e-04  Data: 1.891 (1.891)
Train: 120 [  50/1251 (  4%)]  Loss: 3.908 (3.97)  Time: 0.773s, 1324.06/s  (0.835s, 1226.32/s)  LR: 9.055e-04  Data: 0.010 (0.049)
Train: 120 [ 100/1251 (  8%)]  Loss: 3.702 (3.88)  Time: 0.786s, 1302.88/s  (0.817s, 1253.36/s)  LR: 9.055e-04  Data: 0.009 (0.030)
Train: 120 [ 150/1251 ( 12%)]  Loss: 3.708 (3.84)  Time: 0.774s, 1323.22/s  (0.813s, 1260.30/s)  LR: 9.055e-04  Data: 0.011 (0.024)
Train: 120 [ 200/1251 ( 16%)]  Loss: 3.579 (3.79)  Time: 0.838s, 1221.43/s  (0.811s, 1263.14/s)  LR: 9.055e-04  Data: 0.010 (0.021)
Train: 120 [ 250/1251 ( 20%)]  Loss: 3.836 (3.80)  Time: 0.791s, 1295.04/s  (0.809s, 1265.81/s)  LR: 9.055e-04  Data: 0.013 (0.019)
Train: 120 [ 300/1251 ( 24%)]  Loss: 3.763 (3.79)  Time: 0.820s, 1248.68/s  (0.808s, 1267.40/s)  LR: 9.055e-04  Data: 0.014 (0.017)
Train: 120 [ 350/1251 ( 28%)]  Loss: 3.602 (3.77)  Time: 0.781s, 1311.54/s  (0.807s, 1269.36/s)  LR: 9.055e-04  Data: 0.013 (0.017)
Train: 120 [ 400/1251 ( 32%)]  Loss: 3.947 (3.79)  Time: 0.825s, 1240.79/s  (0.805s, 1271.28/s)  LR: 9.055e-04  Data: 0.009 (0.016)
Train: 120 [ 450/1251 ( 36%)]  Loss: 3.288 (3.74)  Time: 0.782s, 1309.20/s  (0.805s, 1272.29/s)  LR: 9.055e-04  Data: 0.010 (0.015)
Train: 120 [ 500/1251 ( 40%)]  Loss: 3.952 (3.76)  Time: 0.798s, 1282.79/s  (0.804s, 1273.48/s)  LR: 9.055e-04  Data: 0.013 (0.015)
Train: 120 [ 550/1251 ( 44%)]  Loss: 3.421 (3.73)  Time: 0.808s, 1266.69/s  (0.804s, 1273.20/s)  LR: 9.055e-04  Data: 0.013 (0.014)
Train: 120 [ 600/1251 ( 48%)]  Loss: 3.693 (3.73)  Time: 0.799s, 1282.38/s  (0.804s, 1273.32/s)  LR: 9.055e-04  Data: 0.010 (0.014)
Train: 120 [ 650/1251 ( 52%)]  Loss: 3.928 (3.74)  Time: 0.814s, 1257.38/s  (0.804s, 1273.61/s)  LR: 9.055e-04  Data: 0.013 (0.014)
Train: 120 [ 700/1251 ( 56%)]  Loss: 4.082 (3.76)  Time: 0.814s, 1257.53/s  (0.804s, 1273.50/s)  LR: 9.055e-04  Data: 0.010 (0.014)
Train: 120 [ 750/1251 ( 60%)]  Loss: 3.621 (3.75)  Time: 0.794s, 1289.00/s  (0.804s, 1273.66/s)  LR: 9.055e-04  Data: 0.012 (0.014)
Train: 120 [ 800/1251 ( 64%)]  Loss: 3.632 (3.75)  Time: 0.797s, 1284.51/s  (0.804s, 1274.18/s)  LR: 9.055e-04  Data: 0.011 (0.013)
Train: 120 [ 850/1251 ( 68%)]  Loss: 3.745 (3.75)  Time: 0.773s, 1324.73/s  (0.803s, 1274.89/s)  LR: 9.055e-04  Data: 0.010 (0.013)
Train: 120 [ 900/1251 ( 72%)]  Loss: 3.666 (3.74)  Time: 0.854s, 1198.83/s  (0.804s, 1273.36/s)  LR: 9.055e-04  Data: 0.013 (0.013)
Train: 120 [ 950/1251 ( 76%)]  Loss: 3.338 (3.72)  Time: 0.774s, 1323.15/s  (0.804s, 1273.38/s)  LR: 9.055e-04  Data: 0.009 (0.013)
Train: 120 [1000/1251 ( 80%)]  Loss: 3.850 (3.73)  Time: 0.773s, 1325.27/s  (0.803s, 1275.36/s)  LR: 9.055e-04  Data: 0.010 (0.013)
Train: 120 [1050/1251 ( 84%)]  Loss: 4.132 (3.75)  Time: 0.820s, 1249.03/s  (0.802s, 1276.42/s)  LR: 9.055e-04  Data: 0.014 (0.013)
Train: 120 [1100/1251 ( 88%)]  Loss: 3.612 (3.74)  Time: 0.829s, 1234.92/s  (0.802s, 1276.84/s)  LR: 9.055e-04  Data: 0.015 (0.013)
Train: 120 [1150/1251 ( 92%)]  Loss: 3.774 (3.74)  Time: 0.863s, 1186.80/s  (0.802s, 1276.74/s)  LR: 9.055e-04  Data: 0.011 (0.013)
Train: 120 [1200/1251 ( 96%)]  Loss: 3.866 (3.75)  Time: 0.788s, 1299.41/s  (0.802s, 1276.60/s)  LR: 9.055e-04  Data: 0.013 (0.013)
Train: 120 [1250/1251 (100%)]  Loss: 3.886 (3.75)  Time: 0.763s, 1342.02/s  (0.802s, 1276.68/s)  LR: 9.055e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.717 (1.717)  Loss:  0.7969 (0.7969)  Acc@1: 88.6719 (88.6719)  Acc@5: 96.2891 (96.2891)
Test: [  48/48]  Time: 0.194 (0.598)  Loss:  0.8716 (1.3634)  Acc@1: 84.1981 (72.4560)  Acc@5: 95.9906 (91.2740)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-120.pth.tar', 72.4560001171875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-114.pth.tar', 72.40200011962891)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-103.pth.tar', 72.05000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-108.pth.tar', 72.01000001464844)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-118.pth.tar', 72.00600000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-105.pth.tar', 72.00599989013672)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-119.pth.tar', 71.96799996826172)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-110.pth.tar', 71.8960001171875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-117.pth.tar', 71.85399999755859)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-106.pth.tar', 71.84200012695312)

Train: 121 [   0/1251 (  0%)]  Loss: 3.672 (3.67)  Time: 2.597s,  394.36/s  (2.597s,  394.36/s)  LR: 9.039e-04  Data: 1.868 (1.868)
Train: 121 [  50/1251 (  4%)]  Loss: 3.587 (3.63)  Time: 0.780s, 1312.21/s  (0.849s, 1206.29/s)  LR: 9.039e-04  Data: 0.010 (0.059)
Train: 121 [ 100/1251 (  8%)]  Loss: 3.515 (3.59)  Time: 0.795s, 1288.68/s  (0.823s, 1244.04/s)  LR: 9.039e-04  Data: 0.012 (0.035)
Train: 121 [ 150/1251 ( 12%)]  Loss: 3.588 (3.59)  Time: 0.791s, 1295.04/s  (0.814s, 1257.54/s)  LR: 9.039e-04  Data: 0.015 (0.027)
Train: 121 [ 200/1251 ( 16%)]  Loss: 3.734 (3.62)  Time: 0.810s, 1263.91/s  (0.811s, 1262.66/s)  LR: 9.039e-04  Data: 0.010 (0.023)
Train: 121 [ 250/1251 ( 20%)]  Loss: 3.572 (3.61)  Time: 0.833s, 1229.77/s  (0.809s, 1266.05/s)  LR: 9.039e-04  Data: 0.014 (0.021)
Train: 121 [ 300/1251 ( 24%)]  Loss: 3.729 (3.63)  Time: 0.891s, 1149.08/s  (0.808s, 1267.45/s)  LR: 9.039e-04  Data: 0.010 (0.019)
Train: 121 [ 350/1251 ( 28%)]  Loss: 3.765 (3.65)  Time: 0.833s, 1229.01/s  (0.807s, 1268.67/s)  LR: 9.039e-04  Data: 0.010 (0.018)
Train: 121 [ 400/1251 ( 32%)]  Loss: 3.666 (3.65)  Time: 0.825s, 1241.24/s  (0.806s, 1270.46/s)  LR: 9.039e-04  Data: 0.009 (0.017)
Train: 121 [ 450/1251 ( 36%)]  Loss: 3.775 (3.66)  Time: 0.772s, 1325.93/s  (0.805s, 1271.33/s)  LR: 9.039e-04  Data: 0.010 (0.016)
Train: 121 [ 500/1251 ( 40%)]  Loss: 4.150 (3.70)  Time: 0.774s, 1322.85/s  (0.805s, 1271.89/s)  LR: 9.039e-04  Data: 0.011 (0.016)
Train: 121 [ 550/1251 ( 44%)]  Loss: 3.473 (3.69)  Time: 0.777s, 1318.68/s  (0.805s, 1272.08/s)  LR: 9.039e-04  Data: 0.010 (0.016)
Train: 121 [ 600/1251 ( 48%)]  Loss: 3.615 (3.68)  Time: 0.888s, 1153.16/s  (0.805s, 1272.28/s)  LR: 9.039e-04  Data: 0.011 (0.015)
Train: 121 [ 650/1251 ( 52%)]  Loss: 4.081 (3.71)  Time: 0.815s, 1255.88/s  (0.805s, 1272.44/s)  LR: 9.039e-04  Data: 0.009 (0.015)
Train: 121 [ 700/1251 ( 56%)]  Loss: 3.633 (3.70)  Time: 0.862s, 1187.92/s  (0.805s, 1271.92/s)  LR: 9.039e-04  Data: 0.010 (0.015)
Train: 121 [ 750/1251 ( 60%)]  Loss: 3.303 (3.68)  Time: 0.777s, 1317.38/s  (0.805s, 1272.72/s)  LR: 9.039e-04  Data: 0.015 (0.014)
Train: 121 [ 800/1251 ( 64%)]  Loss: 3.975 (3.70)  Time: 0.796s, 1286.91/s  (0.804s, 1273.12/s)  LR: 9.039e-04  Data: 0.010 (0.014)
Train: 121 [ 850/1251 ( 68%)]  Loss: 3.855 (3.70)  Time: 0.785s, 1305.00/s  (0.804s, 1274.01/s)  LR: 9.039e-04  Data: 0.010 (0.014)
Train: 121 [ 900/1251 ( 72%)]  Loss: 3.423 (3.69)  Time: 0.785s, 1304.03/s  (0.804s, 1274.01/s)  LR: 9.039e-04  Data: 0.016 (0.014)
Train: 121 [ 950/1251 ( 76%)]  Loss: 3.695 (3.69)  Time: 0.861s, 1188.99/s  (0.804s, 1274.00/s)  LR: 9.039e-04  Data: 0.009 (0.014)
Train: 121 [1000/1251 ( 80%)]  Loss: 3.557 (3.68)  Time: 0.807s, 1268.84/s  (0.804s, 1274.33/s)  LR: 9.039e-04  Data: 0.009 (0.014)
Train: 121 [1050/1251 ( 84%)]  Loss: 3.578 (3.68)  Time: 0.843s, 1214.18/s  (0.804s, 1274.24/s)  LR: 9.039e-04  Data: 0.013 (0.013)
Train: 121 [1100/1251 ( 88%)]  Loss: 3.570 (3.67)  Time: 0.772s, 1326.71/s  (0.804s, 1273.99/s)  LR: 9.039e-04  Data: 0.011 (0.013)
Train: 121 [1150/1251 ( 92%)]  Loss: 3.791 (3.68)  Time: 0.780s, 1312.40/s  (0.804s, 1274.18/s)  LR: 9.039e-04  Data: 0.011 (0.013)
Train: 121 [1200/1251 ( 96%)]  Loss: 3.791 (3.68)  Time: 0.814s, 1257.63/s  (0.804s, 1274.24/s)  LR: 9.039e-04  Data: 0.009 (0.013)
Train: 121 [1250/1251 (100%)]  Loss: 3.945 (3.69)  Time: 0.811s, 1263.00/s  (0.804s, 1273.94/s)  LR: 9.039e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.663 (1.663)  Loss:  0.7485 (0.7485)  Acc@1: 87.7930 (87.7930)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.194 (0.599)  Loss:  0.8511 (1.3502)  Acc@1: 83.8443 (71.9960)  Acc@5: 95.0472 (91.1760)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-120.pth.tar', 72.4560001171875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-114.pth.tar', 72.40200011962891)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-103.pth.tar', 72.05000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-108.pth.tar', 72.01000001464844)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-118.pth.tar', 72.00600000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-105.pth.tar', 72.00599989013672)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-121.pth.tar', 71.99600009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-119.pth.tar', 71.96799996826172)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-110.pth.tar', 71.8960001171875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-117.pth.tar', 71.85399999755859)

Train: 122 [   0/1251 (  0%)]  Loss: 3.446 (3.45)  Time: 2.487s,  411.79/s  (2.487s,  411.79/s)  LR: 9.024e-04  Data: 1.755 (1.755)
Train: 122 [  50/1251 (  4%)]  Loss: 3.540 (3.49)  Time: 0.828s, 1236.34/s  (0.842s, 1215.85/s)  LR: 9.024e-04  Data: 0.010 (0.054)
Train: 122 [ 100/1251 (  8%)]  Loss: 3.794 (3.59)  Time: 0.796s, 1286.86/s  (0.820s, 1248.23/s)  LR: 9.024e-04  Data: 0.009 (0.032)
Train: 122 [ 150/1251 ( 12%)]  Loss: 3.905 (3.67)  Time: 0.786s, 1303.01/s  (0.815s, 1256.37/s)  LR: 9.024e-04  Data: 0.015 (0.025)
Train: 122 [ 200/1251 ( 16%)]  Loss: 3.440 (3.62)  Time: 0.828s, 1236.64/s  (0.813s, 1259.91/s)  LR: 9.024e-04  Data: 0.014 (0.022)
Train: 122 [ 250/1251 ( 20%)]  Loss: 3.839 (3.66)  Time: 0.800s, 1280.70/s  (0.810s, 1264.50/s)  LR: 9.024e-04  Data: 0.009 (0.020)
Train: 122 [ 300/1251 ( 24%)]  Loss: 3.695 (3.67)  Time: 0.772s, 1326.66/s  (0.807s, 1268.57/s)  LR: 9.024e-04  Data: 0.010 (0.018)
Train: 122 [ 350/1251 ( 28%)]  Loss: 3.604 (3.66)  Time: 0.834s, 1228.55/s  (0.807s, 1268.88/s)  LR: 9.024e-04  Data: 0.015 (0.017)
Train: 122 [ 400/1251 ( 32%)]  Loss: 4.084 (3.71)  Time: 0.772s, 1325.61/s  (0.805s, 1271.29/s)  LR: 9.024e-04  Data: 0.010 (0.016)
Train: 122 [ 450/1251 ( 36%)]  Loss: 3.271 (3.66)  Time: 0.776s, 1318.89/s  (0.805s, 1272.07/s)  LR: 9.024e-04  Data: 0.010 (0.016)
Train: 122 [ 500/1251 ( 40%)]  Loss: 3.719 (3.67)  Time: 0.783s, 1308.04/s  (0.805s, 1272.45/s)  LR: 9.024e-04  Data: 0.010 (0.015)
Train: 122 [ 550/1251 ( 44%)]  Loss: 3.537 (3.66)  Time: 0.775s, 1321.87/s  (0.805s, 1272.65/s)  LR: 9.024e-04  Data: 0.010 (0.015)
Train: 122 [ 600/1251 ( 48%)]  Loss: 3.737 (3.66)  Time: 0.798s, 1283.20/s  (0.804s, 1273.10/s)  LR: 9.024e-04  Data: 0.011 (0.015)
Train: 122 [ 650/1251 ( 52%)]  Loss: 3.594 (3.66)  Time: 0.812s, 1260.87/s  (0.804s, 1273.14/s)  LR: 9.024e-04  Data: 0.010 (0.014)
Train: 122 [ 700/1251 ( 56%)]  Loss: 3.826 (3.67)  Time: 0.821s, 1247.47/s  (0.804s, 1274.14/s)  LR: 9.024e-04  Data: 0.010 (0.014)
Train: 122 [ 750/1251 ( 60%)]  Loss: 3.457 (3.66)  Time: 0.774s, 1323.53/s  (0.804s, 1274.22/s)  LR: 9.024e-04  Data: 0.011 (0.014)
Train: 122 [ 800/1251 ( 64%)]  Loss: 3.393 (3.64)  Time: 0.798s, 1283.77/s  (0.803s, 1274.74/s)  LR: 9.024e-04  Data: 0.010 (0.014)
Train: 122 [ 850/1251 ( 68%)]  Loss: 3.564 (3.64)  Time: 0.781s, 1311.25/s  (0.804s, 1274.04/s)  LR: 9.024e-04  Data: 0.010 (0.014)
Train: 122 [ 900/1251 ( 72%)]  Loss: 3.258 (3.62)  Time: 0.793s, 1291.57/s  (0.804s, 1274.07/s)  LR: 9.024e-04  Data: 0.009 (0.014)
Train: 122 [ 950/1251 ( 76%)]  Loss: 3.930 (3.63)  Time: 0.786s, 1302.01/s  (0.804s, 1273.85/s)  LR: 9.024e-04  Data: 0.013 (0.013)
Train: 122 [1000/1251 ( 80%)]  Loss: 3.593 (3.63)  Time: 0.788s, 1299.86/s  (0.804s, 1273.88/s)  LR: 9.024e-04  Data: 0.010 (0.013)
Train: 122 [1050/1251 ( 84%)]  Loss: 3.646 (3.63)  Time: 0.772s, 1326.08/s  (0.803s, 1274.43/s)  LR: 9.024e-04  Data: 0.011 (0.013)
Train: 122 [1100/1251 ( 88%)]  Loss: 3.672 (3.63)  Time: 0.783s, 1308.61/s  (0.803s, 1274.44/s)  LR: 9.024e-04  Data: 0.009 (0.013)
Train: 122 [1150/1251 ( 92%)]  Loss: 3.742 (3.64)  Time: 0.796s, 1286.39/s  (0.803s, 1274.89/s)  LR: 9.024e-04  Data: 0.011 (0.013)
Train: 122 [1200/1251 ( 96%)]  Loss: 3.572 (3.63)  Time: 0.774s, 1322.84/s  (0.803s, 1275.25/s)  LR: 9.024e-04  Data: 0.010 (0.013)
Train: 122 [1250/1251 (100%)]  Loss: 3.858 (3.64)  Time: 0.783s, 1307.81/s  (0.803s, 1275.24/s)  LR: 9.024e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.616 (1.616)  Loss:  0.8442 (0.8442)  Acc@1: 88.1836 (88.1836)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.194 (0.615)  Loss:  0.9312 (1.4310)  Acc@1: 83.7264 (71.7280)  Acc@5: 95.4009 (91.0680)
Train: 123 [   0/1251 (  0%)]  Loss: 3.370 (3.37)  Time: 2.392s,  428.11/s  (2.392s,  428.11/s)  LR: 9.008e-04  Data: 1.656 (1.656)
Train: 123 [  50/1251 (  4%)]  Loss: 3.767 (3.57)  Time: 0.817s, 1253.25/s  (0.836s, 1224.24/s)  LR: 9.008e-04  Data: 0.011 (0.047)
Train: 123 [ 100/1251 (  8%)]  Loss: 3.523 (3.55)  Time: 0.815s, 1256.72/s  (0.823s, 1243.80/s)  LR: 9.008e-04  Data: 0.014 (0.029)
Train: 123 [ 150/1251 ( 12%)]  Loss: 4.088 (3.69)  Time: 0.771s, 1328.13/s  (0.816s, 1255.46/s)  LR: 9.008e-04  Data: 0.010 (0.023)
Train: 123 [ 200/1251 ( 16%)]  Loss: 3.551 (3.66)  Time: 0.778s, 1315.39/s  (0.814s, 1258.42/s)  LR: 9.008e-04  Data: 0.010 (0.020)
Train: 123 [ 250/1251 ( 20%)]  Loss: 3.972 (3.71)  Time: 0.852s, 1201.21/s  (0.812s, 1261.27/s)  LR: 9.008e-04  Data: 0.010 (0.018)
Train: 123 [ 300/1251 ( 24%)]  Loss: 3.856 (3.73)  Time: 0.839s, 1220.86/s  (0.810s, 1263.91/s)  LR: 9.008e-04  Data: 0.010 (0.017)
Train: 123 [ 350/1251 ( 28%)]  Loss: 3.347 (3.68)  Time: 0.792s, 1292.95/s  (0.809s, 1266.02/s)  LR: 9.008e-04  Data: 0.010 (0.016)
Train: 123 [ 400/1251 ( 32%)]  Loss: 3.541 (3.67)  Time: 0.779s, 1315.10/s  (0.808s, 1267.57/s)  LR: 9.008e-04  Data: 0.010 (0.016)
Train: 123 [ 450/1251 ( 36%)]  Loss: 3.680 (3.67)  Time: 0.811s, 1262.17/s  (0.808s, 1267.71/s)  LR: 9.008e-04  Data: 0.010 (0.015)
Train: 123 [ 500/1251 ( 40%)]  Loss: 3.567 (3.66)  Time: 0.808s, 1267.40/s  (0.807s, 1268.72/s)  LR: 9.008e-04  Data: 0.010 (0.015)
Train: 123 [ 550/1251 ( 44%)]  Loss: 3.805 (3.67)  Time: 0.804s, 1274.40/s  (0.807s, 1269.13/s)  LR: 9.008e-04  Data: 0.014 (0.014)
Train: 123 [ 600/1251 ( 48%)]  Loss: 3.725 (3.68)  Time: 0.815s, 1256.90/s  (0.807s, 1269.26/s)  LR: 9.008e-04  Data: 0.010 (0.014)
Train: 123 [ 650/1251 ( 52%)]  Loss: 3.791 (3.68)  Time: 0.805s, 1271.29/s  (0.806s, 1269.86/s)  LR: 9.008e-04  Data: 0.011 (0.014)
Train: 123 [ 700/1251 ( 56%)]  Loss: 3.830 (3.69)  Time: 0.824s, 1242.23/s  (0.806s, 1270.12/s)  LR: 9.008e-04  Data: 0.010 (0.014)
Train: 123 [ 750/1251 ( 60%)]  Loss: 3.410 (3.68)  Time: 0.773s, 1324.41/s  (0.806s, 1270.22/s)  LR: 9.008e-04  Data: 0.010 (0.014)
Train: 123 [ 800/1251 ( 64%)]  Loss: 3.838 (3.69)  Time: 0.819s, 1250.59/s  (0.805s, 1271.38/s)  LR: 9.008e-04  Data: 0.010 (0.013)
Train: 123 [ 850/1251 ( 68%)]  Loss: 3.994 (3.70)  Time: 0.778s, 1316.88/s  (0.805s, 1272.35/s)  LR: 9.008e-04  Data: 0.009 (0.013)
Train: 123 [ 900/1251 ( 72%)]  Loss: 3.780 (3.71)  Time: 0.799s, 1281.18/s  (0.805s, 1272.06/s)  LR: 9.008e-04  Data: 0.011 (0.013)
Train: 123 [ 950/1251 ( 76%)]  Loss: 3.481 (3.70)  Time: 0.780s, 1312.78/s  (0.804s, 1272.86/s)  LR: 9.008e-04  Data: 0.010 (0.013)
Train: 123 [1000/1251 ( 80%)]  Loss: 3.480 (3.69)  Time: 0.772s, 1325.59/s  (0.804s, 1272.93/s)  LR: 9.008e-04  Data: 0.010 (0.013)
Train: 123 [1050/1251 ( 84%)]  Loss: 3.713 (3.69)  Time: 0.901s, 1136.83/s  (0.804s, 1273.05/s)  LR: 9.008e-04  Data: 0.013 (0.013)
Train: 123 [1100/1251 ( 88%)]  Loss: 3.424 (3.68)  Time: 0.801s, 1278.59/s  (0.804s, 1273.61/s)  LR: 9.008e-04  Data: 0.009 (0.013)
Train: 123 [1150/1251 ( 92%)]  Loss: 3.684 (3.68)  Time: 0.799s, 1281.62/s  (0.804s, 1273.46/s)  LR: 9.008e-04  Data: 0.010 (0.013)
Train: 123 [1200/1251 ( 96%)]  Loss: 3.430 (3.67)  Time: 0.779s, 1315.19/s  (0.804s, 1273.62/s)  LR: 9.008e-04  Data: 0.011 (0.013)
Train: 123 [1250/1251 (100%)]  Loss: 4.044 (3.68)  Time: 0.760s, 1347.31/s  (0.804s, 1273.90/s)  LR: 9.008e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.720 (1.720)  Loss:  0.9009 (0.9009)  Acc@1: 88.0859 (88.0859)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.194 (0.608)  Loss:  1.0400 (1.4899)  Acc@1: 82.1934 (72.0100)  Acc@5: 93.7500 (90.8680)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-120.pth.tar', 72.4560001171875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-114.pth.tar', 72.40200011962891)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-103.pth.tar', 72.05000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-123.pth.tar', 72.01000002197266)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-108.pth.tar', 72.01000001464844)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-118.pth.tar', 72.00600000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-105.pth.tar', 72.00599989013672)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-121.pth.tar', 71.99600009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-119.pth.tar', 71.96799996826172)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-110.pth.tar', 71.8960001171875)

Train: 124 [   0/1251 (  0%)]  Loss: 3.615 (3.61)  Time: 2.411s,  424.76/s  (2.411s,  424.76/s)  LR: 8.993e-04  Data: 1.681 (1.681)
Train: 124 [  50/1251 (  4%)]  Loss: 3.525 (3.57)  Time: 0.774s, 1322.49/s  (0.844s, 1212.99/s)  LR: 8.993e-04  Data: 0.010 (0.057)
Train: 124 [ 100/1251 (  8%)]  Loss: 3.619 (3.59)  Time: 0.779s, 1314.26/s  (0.824s, 1242.78/s)  LR: 8.993e-04  Data: 0.010 (0.034)
Train: 124 [ 150/1251 ( 12%)]  Loss: 3.845 (3.65)  Time: 0.860s, 1190.25/s  (0.819s, 1250.38/s)  LR: 8.993e-04  Data: 0.010 (0.027)
Train: 124 [ 200/1251 ( 16%)]  Loss: 3.422 (3.61)  Time: 0.850s, 1205.12/s  (0.816s, 1255.03/s)  LR: 8.993e-04  Data: 0.014 (0.023)
Train: 124 [ 250/1251 ( 20%)]  Loss: 3.713 (3.62)  Time: 0.775s, 1320.74/s  (0.813s, 1260.24/s)  LR: 8.993e-04  Data: 0.010 (0.021)
Train: 124 [ 300/1251 ( 24%)]  Loss: 3.821 (3.65)  Time: 0.829s, 1235.01/s  (0.810s, 1264.03/s)  LR: 8.993e-04  Data: 0.010 (0.019)
Train: 124 [ 350/1251 ( 28%)]  Loss: 4.028 (3.70)  Time: 0.772s, 1325.98/s  (0.809s, 1265.43/s)  LR: 8.993e-04  Data: 0.010 (0.018)
Train: 124 [ 400/1251 ( 32%)]  Loss: 4.037 (3.74)  Time: 0.838s, 1221.45/s  (0.807s, 1268.54/s)  LR: 8.993e-04  Data: 0.011 (0.017)
Train: 124 [ 450/1251 ( 36%)]  Loss: 3.848 (3.75)  Time: 0.773s, 1324.41/s  (0.807s, 1269.31/s)  LR: 8.993e-04  Data: 0.010 (0.016)
Train: 124 [ 500/1251 ( 40%)]  Loss: 3.522 (3.73)  Time: 0.783s, 1308.48/s  (0.806s, 1270.27/s)  LR: 8.993e-04  Data: 0.010 (0.016)
Train: 124 [ 550/1251 ( 44%)]  Loss: 3.599 (3.72)  Time: 0.790s, 1296.48/s  (0.806s, 1270.89/s)  LR: 8.993e-04  Data: 0.009 (0.015)
Train: 124 [ 600/1251 ( 48%)]  Loss: 3.383 (3.69)  Time: 0.800s, 1279.24/s  (0.806s, 1270.95/s)  LR: 8.993e-04  Data: 0.011 (0.015)
Train: 124 [ 650/1251 ( 52%)]  Loss: 3.668 (3.69)  Time: 0.831s, 1232.87/s  (0.806s, 1271.20/s)  LR: 8.993e-04  Data: 0.010 (0.015)
Train: 124 [ 700/1251 ( 56%)]  Loss: 3.812 (3.70)  Time: 0.799s, 1282.14/s  (0.805s, 1272.34/s)  LR: 8.993e-04  Data: 0.011 (0.014)
Train: 124 [ 750/1251 ( 60%)]  Loss: 3.844 (3.71)  Time: 0.772s, 1327.11/s  (0.804s, 1273.35/s)  LR: 8.993e-04  Data: 0.010 (0.014)
Train: 124 [ 800/1251 ( 64%)]  Loss: 3.724 (3.71)  Time: 0.776s, 1320.07/s  (0.804s, 1273.78/s)  LR: 8.993e-04  Data: 0.010 (0.014)
Train: 124 [ 850/1251 ( 68%)]  Loss: 3.817 (3.71)  Time: 0.774s, 1323.35/s  (0.804s, 1274.10/s)  LR: 8.993e-04  Data: 0.010 (0.014)
Train: 124 [ 900/1251 ( 72%)]  Loss: 3.584 (3.71)  Time: 0.856s, 1196.43/s  (0.804s, 1274.31/s)  LR: 8.993e-04  Data: 0.014 (0.014)
Train: 124 [ 950/1251 ( 76%)]  Loss: 3.752 (3.71)  Time: 0.790s, 1296.61/s  (0.803s, 1274.54/s)  LR: 8.993e-04  Data: 0.010 (0.014)
Train: 124 [1000/1251 ( 80%)]  Loss: 3.475 (3.70)  Time: 0.772s, 1325.74/s  (0.803s, 1274.43/s)  LR: 8.993e-04  Data: 0.011 (0.013)
Train: 124 [1050/1251 ( 84%)]  Loss: 4.047 (3.71)  Time: 0.805s, 1272.44/s  (0.804s, 1274.11/s)  LR: 8.993e-04  Data: 0.013 (0.013)
Train: 124 [1100/1251 ( 88%)]  Loss: 4.029 (3.73)  Time: 0.778s, 1315.37/s  (0.803s, 1274.56/s)  LR: 8.993e-04  Data: 0.011 (0.013)
Train: 124 [1150/1251 ( 92%)]  Loss: 3.951 (3.74)  Time: 0.824s, 1243.10/s  (0.803s, 1274.78/s)  LR: 8.993e-04  Data: 0.010 (0.013)
Train: 124 [1200/1251 ( 96%)]  Loss: 4.188 (3.75)  Time: 0.813s, 1260.27/s  (0.803s, 1275.04/s)  LR: 8.993e-04  Data: 0.010 (0.013)
Train: 124 [1250/1251 (100%)]  Loss: 3.698 (3.75)  Time: 0.771s, 1328.84/s  (0.803s, 1275.52/s)  LR: 8.993e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.688 (1.688)  Loss:  0.8813 (0.8813)  Acc@1: 86.7188 (86.7188)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.193 (0.604)  Loss:  0.9409 (1.4284)  Acc@1: 84.3160 (71.6220)  Acc@5: 95.8726 (91.0140)
Train: 125 [   0/1251 (  0%)]  Loss: 3.927 (3.93)  Time: 2.445s,  418.74/s  (2.445s,  418.74/s)  LR: 8.977e-04  Data: 1.725 (1.725)
Train: 125 [  50/1251 (  4%)]  Loss: 3.812 (3.87)  Time: 0.779s, 1315.31/s  (0.840s, 1219.59/s)  LR: 8.977e-04  Data: 0.009 (0.051)
Train: 125 [ 100/1251 (  8%)]  Loss: 3.688 (3.81)  Time: 0.771s, 1328.84/s  (0.820s, 1248.80/s)  LR: 8.977e-04  Data: 0.010 (0.031)
Train: 125 [ 150/1251 ( 12%)]  Loss: 3.824 (3.81)  Time: 0.773s, 1325.36/s  (0.815s, 1255.94/s)  LR: 8.977e-04  Data: 0.010 (0.025)
Train: 125 [ 200/1251 ( 16%)]  Loss: 3.755 (3.80)  Time: 0.793s, 1291.94/s  (0.811s, 1262.41/s)  LR: 8.977e-04  Data: 0.010 (0.021)
Train: 125 [ 250/1251 ( 20%)]  Loss: 3.638 (3.77)  Time: 0.782s, 1310.23/s  (0.809s, 1265.44/s)  LR: 8.977e-04  Data: 0.010 (0.019)
Train: 125 [ 300/1251 ( 24%)]  Loss: 3.759 (3.77)  Time: 0.780s, 1312.85/s  (0.809s, 1266.49/s)  LR: 8.977e-04  Data: 0.014 (0.018)
Train: 125 [ 350/1251 ( 28%)]  Loss: 3.616 (3.75)  Time: 0.806s, 1270.81/s  (0.807s, 1269.36/s)  LR: 8.977e-04  Data: 0.010 (0.017)
Train: 125 [ 400/1251 ( 32%)]  Loss: 3.563 (3.73)  Time: 0.776s, 1320.07/s  (0.805s, 1271.76/s)  LR: 8.977e-04  Data: 0.010 (0.016)
Train: 125 [ 450/1251 ( 36%)]  Loss: 3.692 (3.73)  Time: 0.798s, 1283.42/s  (0.804s, 1272.93/s)  LR: 8.977e-04  Data: 0.009 (0.016)
Train: 125 [ 500/1251 ( 40%)]  Loss: 3.754 (3.73)  Time: 0.775s, 1321.25/s  (0.805s, 1272.69/s)  LR: 8.977e-04  Data: 0.010 (0.015)
Train: 125 [ 550/1251 ( 44%)]  Loss: 3.615 (3.72)  Time: 0.828s, 1237.21/s  (0.805s, 1271.68/s)  LR: 8.977e-04  Data: 0.038 (0.015)
Train: 125 [ 600/1251 ( 48%)]  Loss: 3.445 (3.70)  Time: 0.776s, 1319.47/s  (0.806s, 1270.24/s)  LR: 8.977e-04  Data: 0.011 (0.015)
Train: 125 [ 650/1251 ( 52%)]  Loss: 3.626 (3.69)  Time: 0.772s, 1325.65/s  (0.805s, 1272.78/s)  LR: 8.977e-04  Data: 0.010 (0.014)
Train: 125 [ 700/1251 ( 56%)]  Loss: 3.767 (3.70)  Time: 0.811s, 1262.79/s  (0.803s, 1275.21/s)  LR: 8.977e-04  Data: 0.010 (0.014)
Train: 125 [ 750/1251 ( 60%)]  Loss: 3.787 (3.70)  Time: 0.777s, 1318.40/s  (0.803s, 1275.24/s)  LR: 8.977e-04  Data: 0.009 (0.014)
Train: 125 [ 800/1251 ( 64%)]  Loss: 3.659 (3.70)  Time: 0.791s, 1294.79/s  (0.803s, 1275.27/s)  LR: 8.977e-04  Data: 0.009 (0.014)
Train: 125 [ 850/1251 ( 68%)]  Loss: 3.614 (3.70)  Time: 0.779s, 1315.24/s  (0.803s, 1275.56/s)  LR: 8.977e-04  Data: 0.013 (0.014)
Train: 125 [ 900/1251 ( 72%)]  Loss: 3.649 (3.69)  Time: 0.848s, 1207.11/s  (0.803s, 1275.80/s)  LR: 8.977e-04  Data: 0.009 (0.013)
Train: 125 [ 950/1251 ( 76%)]  Loss: 3.726 (3.70)  Time: 0.810s, 1264.97/s  (0.802s, 1276.18/s)  LR: 8.977e-04  Data: 0.010 (0.013)
Train: 125 [1000/1251 ( 80%)]  Loss: 3.444 (3.68)  Time: 0.803s, 1274.90/s  (0.803s, 1275.55/s)  LR: 8.977e-04  Data: 0.010 (0.013)
Train: 125 [1050/1251 ( 84%)]  Loss: 3.545 (3.68)  Time: 0.790s, 1296.18/s  (0.803s, 1275.62/s)  LR: 8.977e-04  Data: 0.010 (0.013)
Train: 125 [1100/1251 ( 88%)]  Loss: 3.895 (3.69)  Time: 0.806s, 1269.81/s  (0.802s, 1276.40/s)  LR: 8.977e-04  Data: 0.014 (0.013)
Train: 125 [1150/1251 ( 92%)]  Loss: 3.323 (3.67)  Time: 0.828s, 1237.17/s  (0.803s, 1275.99/s)  LR: 8.977e-04  Data: 0.015 (0.013)
Train: 125 [1200/1251 ( 96%)]  Loss: 3.583 (3.67)  Time: 0.849s, 1205.75/s  (0.802s, 1276.34/s)  LR: 8.977e-04  Data: 0.017 (0.013)
Train: 125 [1250/1251 (100%)]  Loss: 3.470 (3.66)  Time: 0.787s, 1301.47/s  (0.802s, 1276.72/s)  LR: 8.977e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.701 (1.701)  Loss:  0.8340 (0.8340)  Acc@1: 87.4023 (87.4023)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.193 (0.592)  Loss:  0.9385 (1.3979)  Acc@1: 83.2547 (71.3780)  Acc@5: 94.5755 (90.8460)
Train: 126 [   0/1251 (  0%)]  Loss: 3.727 (3.73)  Time: 2.386s,  429.15/s  (2.386s,  429.15/s)  LR: 8.961e-04  Data: 1.613 (1.613)
Train: 126 [  50/1251 (  4%)]  Loss: 3.785 (3.76)  Time: 0.797s, 1285.20/s  (0.838s, 1222.15/s)  LR: 8.961e-04  Data: 0.010 (0.046)
Train: 126 [ 100/1251 (  8%)]  Loss: 3.469 (3.66)  Time: 0.792s, 1292.82/s  (0.820s, 1248.30/s)  LR: 8.961e-04  Data: 0.010 (0.028)
Train: 126 [ 150/1251 ( 12%)]  Loss: 3.511 (3.62)  Time: 0.819s, 1250.27/s  (0.814s, 1258.21/s)  LR: 8.961e-04  Data: 0.009 (0.023)
Train: 126 [ 200/1251 ( 16%)]  Loss: 3.705 (3.64)  Time: 0.845s, 1211.95/s  (0.812s, 1261.12/s)  LR: 8.961e-04  Data: 0.010 (0.020)
Train: 126 [ 250/1251 ( 20%)]  Loss: 3.947 (3.69)  Time: 0.815s, 1256.20/s  (0.811s, 1262.73/s)  LR: 8.961e-04  Data: 0.015 (0.018)
Train: 126 [ 300/1251 ( 24%)]  Loss: 3.472 (3.66)  Time: 0.816s, 1254.72/s  (0.810s, 1264.78/s)  LR: 8.961e-04  Data: 0.010 (0.017)
Train: 126 [ 350/1251 ( 28%)]  Loss: 3.433 (3.63)  Time: 0.773s, 1325.02/s  (0.809s, 1266.33/s)  LR: 8.961e-04  Data: 0.012 (0.016)
Train: 126 [ 400/1251 ( 32%)]  Loss: 4.035 (3.68)  Time: 0.802s, 1277.22/s  (0.808s, 1267.87/s)  LR: 8.961e-04  Data: 0.010 (0.015)
Train: 126 [ 450/1251 ( 36%)]  Loss: 3.664 (3.67)  Time: 0.793s, 1290.63/s  (0.807s, 1268.78/s)  LR: 8.961e-04  Data: 0.015 (0.015)
Train: 126 [ 500/1251 ( 40%)]  Loss: 3.563 (3.66)  Time: 0.801s, 1277.67/s  (0.806s, 1269.92/s)  LR: 8.961e-04  Data: 0.010 (0.015)
Train: 126 [ 550/1251 ( 44%)]  Loss: 3.794 (3.68)  Time: 0.785s, 1305.16/s  (0.806s, 1270.21/s)  LR: 8.961e-04  Data: 0.013 (0.014)
Train: 126 [ 600/1251 ( 48%)]  Loss: 3.983 (3.70)  Time: 0.801s, 1278.92/s  (0.806s, 1271.21/s)  LR: 8.961e-04  Data: 0.016 (0.014)
Train: 126 [ 650/1251 ( 52%)]  Loss: 3.726 (3.70)  Time: 0.809s, 1266.39/s  (0.805s, 1271.30/s)  LR: 8.961e-04  Data: 0.010 (0.014)
Train: 126 [ 700/1251 ( 56%)]  Loss: 3.638 (3.70)  Time: 0.825s, 1241.25/s  (0.806s, 1271.20/s)  LR: 8.961e-04  Data: 0.010 (0.014)
Train: 126 [ 750/1251 ( 60%)]  Loss: 3.781 (3.70)  Time: 0.788s, 1299.83/s  (0.805s, 1271.69/s)  LR: 8.961e-04  Data: 0.016 (0.013)
Train: 126 [ 800/1251 ( 64%)]  Loss: 3.619 (3.70)  Time: 0.801s, 1278.76/s  (0.805s, 1272.26/s)  LR: 8.961e-04  Data: 0.014 (0.013)
Train: 126 [ 850/1251 ( 68%)]  Loss: 3.998 (3.71)  Time: 0.798s, 1282.46/s  (0.805s, 1272.31/s)  LR: 8.961e-04  Data: 0.015 (0.013)
Train: 126 [ 900/1251 ( 72%)]  Loss: 3.652 (3.71)  Time: 0.780s, 1313.09/s  (0.805s, 1272.71/s)  LR: 8.961e-04  Data: 0.013 (0.013)
Train: 126 [ 950/1251 ( 76%)]  Loss: 3.800 (3.72)  Time: 0.811s, 1263.01/s  (0.804s, 1272.96/s)  LR: 8.961e-04  Data: 0.010 (0.013)
Train: 126 [1000/1251 ( 80%)]  Loss: 3.481 (3.70)  Time: 0.776s, 1320.18/s  (0.804s, 1273.27/s)  LR: 8.961e-04  Data: 0.010 (0.013)
Train: 126 [1050/1251 ( 84%)]  Loss: 3.658 (3.70)  Time: 0.807s, 1269.55/s  (0.804s, 1273.37/s)  LR: 8.961e-04  Data: 0.010 (0.013)
Train: 126 [1100/1251 ( 88%)]  Loss: 3.368 (3.69)  Time: 0.814s, 1258.55/s  (0.804s, 1273.72/s)  LR: 8.961e-04  Data: 0.010 (0.013)
Train: 126 [1150/1251 ( 92%)]  Loss: 3.550 (3.68)  Time: 0.847s, 1209.44/s  (0.804s, 1273.93/s)  LR: 8.961e-04  Data: 0.012 (0.013)
Train: 126 [1200/1251 ( 96%)]  Loss: 3.513 (3.67)  Time: 0.809s, 1265.95/s  (0.804s, 1273.88/s)  LR: 8.961e-04  Data: 0.010 (0.013)
Train: 126 [1250/1251 (100%)]  Loss: 3.298 (3.66)  Time: 0.759s, 1349.00/s  (0.804s, 1274.16/s)  LR: 8.961e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.599 (1.599)  Loss:  0.8276 (0.8276)  Acc@1: 88.7695 (88.7695)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.194 (0.603)  Loss:  0.8774 (1.3958)  Acc@1: 82.7830 (72.2280)  Acc@5: 95.5189 (91.2040)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-120.pth.tar', 72.4560001171875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-114.pth.tar', 72.40200011962891)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-126.pth.tar', 72.22800001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-103.pth.tar', 72.05000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-123.pth.tar', 72.01000002197266)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-108.pth.tar', 72.01000001464844)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-118.pth.tar', 72.00600000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-105.pth.tar', 72.00599989013672)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-121.pth.tar', 71.99600009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-119.pth.tar', 71.96799996826172)

Train: 127 [   0/1251 (  0%)]  Loss: 3.684 (3.68)  Time: 2.470s,  414.50/s  (2.470s,  414.50/s)  LR: 8.945e-04  Data: 1.744 (1.744)
Train: 127 [  50/1251 (  4%)]  Loss: 3.918 (3.80)  Time: 0.773s, 1324.38/s  (0.841s, 1217.01/s)  LR: 8.945e-04  Data: 0.010 (0.055)
Train: 127 [ 100/1251 (  8%)]  Loss: 3.761 (3.79)  Time: 0.797s, 1284.72/s  (0.820s, 1249.01/s)  LR: 8.945e-04  Data: 0.010 (0.033)
Train: 127 [ 150/1251 ( 12%)]  Loss: 3.841 (3.80)  Time: 0.772s, 1326.08/s  (0.814s, 1258.08/s)  LR: 8.945e-04  Data: 0.010 (0.026)
Train: 127 [ 200/1251 ( 16%)]  Loss: 3.570 (3.75)  Time: 0.819s, 1249.95/s  (0.811s, 1262.81/s)  LR: 8.945e-04  Data: 0.012 (0.022)
Train: 127 [ 250/1251 ( 20%)]  Loss: 3.824 (3.77)  Time: 0.784s, 1305.67/s  (0.810s, 1263.82/s)  LR: 8.945e-04  Data: 0.014 (0.020)
Train: 127 [ 300/1251 ( 24%)]  Loss: 3.560 (3.74)  Time: 0.832s, 1231.42/s  (0.808s, 1266.68/s)  LR: 8.945e-04  Data: 0.011 (0.019)
Train: 127 [ 350/1251 ( 28%)]  Loss: 3.296 (3.68)  Time: 0.774s, 1323.51/s  (0.807s, 1268.87/s)  LR: 8.945e-04  Data: 0.011 (0.018)
Train: 127 [ 400/1251 ( 32%)]  Loss: 3.995 (3.72)  Time: 0.776s, 1319.31/s  (0.806s, 1270.47/s)  LR: 8.945e-04  Data: 0.010 (0.017)
Train: 127 [ 450/1251 ( 36%)]  Loss: 3.777 (3.72)  Time: 0.792s, 1293.59/s  (0.807s, 1269.58/s)  LR: 8.945e-04  Data: 0.010 (0.016)
Train: 127 [ 500/1251 ( 40%)]  Loss: 4.026 (3.75)  Time: 0.822s, 1245.45/s  (0.806s, 1270.27/s)  LR: 8.945e-04  Data: 0.011 (0.016)
Train: 127 [ 550/1251 ( 44%)]  Loss: 3.902 (3.76)  Time: 0.806s, 1270.77/s  (0.806s, 1271.05/s)  LR: 8.945e-04  Data: 0.010 (0.015)
Train: 127 [ 600/1251 ( 48%)]  Loss: 3.799 (3.77)  Time: 0.798s, 1283.12/s  (0.805s, 1271.41/s)  LR: 8.945e-04  Data: 0.015 (0.015)
Train: 127 [ 650/1251 ( 52%)]  Loss: 3.980 (3.78)  Time: 0.773s, 1325.21/s  (0.805s, 1271.32/s)  LR: 8.945e-04  Data: 0.010 (0.015)
Train: 127 [ 700/1251 ( 56%)]  Loss: 3.759 (3.78)  Time: 0.780s, 1312.58/s  (0.805s, 1271.92/s)  LR: 8.945e-04  Data: 0.012 (0.014)
Train: 127 [ 750/1251 ( 60%)]  Loss: 3.479 (3.76)  Time: 0.817s, 1253.58/s  (0.805s, 1272.80/s)  LR: 8.945e-04  Data: 0.014 (0.014)
Train: 127 [ 800/1251 ( 64%)]  Loss: 3.706 (3.76)  Time: 0.773s, 1325.08/s  (0.804s, 1273.01/s)  LR: 8.945e-04  Data: 0.010 (0.014)
Train: 127 [ 850/1251 ( 68%)]  Loss: 3.650 (3.75)  Time: 0.784s, 1306.10/s  (0.804s, 1273.36/s)  LR: 8.945e-04  Data: 0.013 (0.014)
Train: 127 [ 900/1251 ( 72%)]  Loss: 3.650 (3.75)  Time: 0.849s, 1205.94/s  (0.804s, 1274.27/s)  LR: 8.945e-04  Data: 0.010 (0.014)
Train: 127 [ 950/1251 ( 76%)]  Loss: 3.517 (3.73)  Time: 0.786s, 1301.99/s  (0.803s, 1274.93/s)  LR: 8.945e-04  Data: 0.013 (0.013)
Train: 127 [1000/1251 ( 80%)]  Loss: 3.763 (3.74)  Time: 0.788s, 1300.31/s  (0.803s, 1274.44/s)  LR: 8.945e-04  Data: 0.017 (0.013)
Train: 127 [1050/1251 ( 84%)]  Loss: 3.833 (3.74)  Time: 0.830s, 1233.81/s  (0.803s, 1274.47/s)  LR: 8.945e-04  Data: 0.010 (0.013)
Train: 127 [1100/1251 ( 88%)]  Loss: 3.331 (3.72)  Time: 0.817s, 1253.74/s  (0.803s, 1274.82/s)  LR: 8.945e-04  Data: 0.010 (0.013)
Train: 127 [1150/1251 ( 92%)]  Loss: 3.697 (3.72)  Time: 0.777s, 1318.04/s  (0.803s, 1274.87/s)  LR: 8.945e-04  Data: 0.013 (0.013)
Train: 127 [1200/1251 ( 96%)]  Loss: 3.738 (3.72)  Time: 0.811s, 1263.01/s  (0.803s, 1274.90/s)  LR: 8.945e-04  Data: 0.010 (0.013)
Train: 127 [1250/1251 (100%)]  Loss: 3.651 (3.72)  Time: 0.798s, 1283.35/s  (0.803s, 1275.09/s)  LR: 8.945e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.643 (1.643)  Loss:  0.8804 (0.8804)  Acc@1: 86.9141 (86.9141)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.194 (0.608)  Loss:  0.9668 (1.4883)  Acc@1: 82.9009 (71.9440)  Acc@5: 95.0472 (91.1400)
Train: 128 [   0/1251 (  0%)]  Loss: 3.993 (3.99)  Time: 2.339s,  437.76/s  (2.339s,  437.76/s)  LR: 8.929e-04  Data: 1.607 (1.607)
Train: 128 [  50/1251 (  4%)]  Loss: 3.560 (3.78)  Time: 0.800s, 1279.22/s  (0.832s, 1231.49/s)  LR: 8.929e-04  Data: 0.014 (0.046)
Train: 128 [ 100/1251 (  8%)]  Loss: 3.640 (3.73)  Time: 0.773s, 1324.83/s  (0.823s, 1244.01/s)  LR: 8.929e-04  Data: 0.010 (0.029)
Train: 128 [ 150/1251 ( 12%)]  Loss: 3.929 (3.78)  Time: 0.814s, 1257.53/s  (0.816s, 1254.19/s)  LR: 8.929e-04  Data: 0.011 (0.023)
Train: 128 [ 200/1251 ( 16%)]  Loss: 3.624 (3.75)  Time: 0.771s, 1327.81/s  (0.814s, 1257.42/s)  LR: 8.929e-04  Data: 0.010 (0.020)
Train: 128 [ 250/1251 ( 20%)]  Loss: 3.990 (3.79)  Time: 0.798s, 1282.90/s  (0.811s, 1262.58/s)  LR: 8.929e-04  Data: 0.010 (0.018)
Train: 128 [ 300/1251 ( 24%)]  Loss: 3.706 (3.78)  Time: 0.780s, 1312.72/s  (0.809s, 1266.30/s)  LR: 8.929e-04  Data: 0.010 (0.017)
Train: 128 [ 350/1251 ( 28%)]  Loss: 3.304 (3.72)  Time: 0.800s, 1279.25/s  (0.808s, 1267.88/s)  LR: 8.929e-04  Data: 0.010 (0.016)
Train: 128 [ 400/1251 ( 32%)]  Loss: 3.836 (3.73)  Time: 0.777s, 1317.43/s  (0.807s, 1269.26/s)  LR: 8.929e-04  Data: 0.014 (0.016)
Train: 128 [ 450/1251 ( 36%)]  Loss: 3.592 (3.72)  Time: 0.772s, 1326.31/s  (0.806s, 1270.14/s)  LR: 8.929e-04  Data: 0.010 (0.015)
Train: 128 [ 500/1251 ( 40%)]  Loss: 3.499 (3.70)  Time: 0.845s, 1211.13/s  (0.806s, 1269.87/s)  LR: 8.929e-04  Data: 0.009 (0.015)
Train: 128 [ 550/1251 ( 44%)]  Loss: 4.173 (3.74)  Time: 0.779s, 1315.15/s  (0.806s, 1271.08/s)  LR: 8.929e-04  Data: 0.010 (0.014)
Train: 128 [ 600/1251 ( 48%)]  Loss: 3.862 (3.75)  Time: 0.823s, 1244.03/s  (0.806s, 1271.00/s)  LR: 8.929e-04  Data: 0.010 (0.014)
Train: 128 [ 650/1251 ( 52%)]  Loss: 3.569 (3.73)  Time: 0.849s, 1206.70/s  (0.805s, 1271.95/s)  LR: 8.929e-04  Data: 0.012 (0.014)
Train: 128 [ 700/1251 ( 56%)]  Loss: 3.385 (3.71)  Time: 0.772s, 1326.12/s  (0.805s, 1272.33/s)  LR: 8.929e-04  Data: 0.010 (0.014)
Train: 128 [ 750/1251 ( 60%)]  Loss: 3.419 (3.69)  Time: 0.796s, 1286.25/s  (0.804s, 1273.15/s)  LR: 8.929e-04  Data: 0.017 (0.014)
Train: 128 [ 800/1251 ( 64%)]  Loss: 3.663 (3.69)  Time: 0.773s, 1324.98/s  (0.804s, 1273.15/s)  LR: 8.929e-04  Data: 0.010 (0.013)
Train: 128 [ 850/1251 ( 68%)]  Loss: 3.782 (3.70)  Time: 0.814s, 1257.26/s  (0.804s, 1273.34/s)  LR: 8.929e-04  Data: 0.010 (0.013)
Train: 128 [ 900/1251 ( 72%)]  Loss: 3.591 (3.69)  Time: 0.793s, 1291.92/s  (0.804s, 1273.97/s)  LR: 8.929e-04  Data: 0.015 (0.013)
Train: 128 [ 950/1251 ( 76%)]  Loss: 3.800 (3.70)  Time: 0.773s, 1325.02/s  (0.804s, 1274.04/s)  LR: 8.929e-04  Data: 0.010 (0.013)
Train: 128 [1000/1251 ( 80%)]  Loss: 3.826 (3.70)  Time: 0.780s, 1312.28/s  (0.804s, 1273.40/s)  LR: 8.929e-04  Data: 0.013 (0.013)
Train: 128 [1050/1251 ( 84%)]  Loss: 3.818 (3.71)  Time: 0.789s, 1297.30/s  (0.804s, 1273.86/s)  LR: 8.929e-04  Data: 0.010 (0.013)
Train: 128 [1100/1251 ( 88%)]  Loss: 4.018 (3.72)  Time: 0.776s, 1320.05/s  (0.804s, 1274.20/s)  LR: 8.929e-04  Data: 0.010 (0.013)
Train: 128 [1150/1251 ( 92%)]  Loss: 3.748 (3.72)  Time: 0.772s, 1326.62/s  (0.803s, 1274.45/s)  LR: 8.929e-04  Data: 0.010 (0.013)
Train: 128 [1200/1251 ( 96%)]  Loss: 3.547 (3.72)  Time: 0.824s, 1242.14/s  (0.804s, 1273.93/s)  LR: 8.929e-04  Data: 0.010 (0.013)
Train: 128 [1250/1251 (100%)]  Loss: 3.589 (3.71)  Time: 0.861s, 1189.21/s  (0.804s, 1274.34/s)  LR: 8.929e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.658 (1.658)  Loss:  0.9595 (0.9595)  Acc@1: 86.4258 (86.4258)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.194 (0.586)  Loss:  0.8799 (1.4517)  Acc@1: 85.0236 (71.9740)  Acc@5: 96.2264 (91.1980)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-120.pth.tar', 72.4560001171875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-114.pth.tar', 72.40200011962891)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-126.pth.tar', 72.22800001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-103.pth.tar', 72.05000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-123.pth.tar', 72.01000002197266)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-108.pth.tar', 72.01000001464844)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-118.pth.tar', 72.00600000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-105.pth.tar', 72.00599989013672)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-121.pth.tar', 71.99600009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-128.pth.tar', 71.9739999584961)

Train: 129 [   0/1251 (  0%)]  Loss: 3.925 (3.93)  Time: 2.550s,  401.60/s  (2.550s,  401.60/s)  LR: 8.913e-04  Data: 1.767 (1.767)
Train: 129 [  50/1251 (  4%)]  Loss: 3.674 (3.80)  Time: 0.800s, 1279.96/s  (0.837s, 1223.03/s)  LR: 8.913e-04  Data: 0.010 (0.051)
Train: 129 [ 100/1251 (  8%)]  Loss: 3.683 (3.76)  Time: 0.802s, 1276.98/s  (0.820s, 1248.37/s)  LR: 8.913e-04  Data: 0.010 (0.032)
Train: 129 [ 150/1251 ( 12%)]  Loss: 3.658 (3.73)  Time: 0.819s, 1250.71/s  (0.815s, 1257.12/s)  LR: 8.913e-04  Data: 0.010 (0.025)
Train: 129 [ 200/1251 ( 16%)]  Loss: 3.577 (3.70)  Time: 0.775s, 1321.75/s  (0.812s, 1260.65/s)  LR: 8.913e-04  Data: 0.010 (0.022)
Train: 129 [ 250/1251 ( 20%)]  Loss: 3.739 (3.71)  Time: 0.797s, 1284.04/s  (0.811s, 1262.96/s)  LR: 8.913e-04  Data: 0.011 (0.019)
Train: 129 [ 300/1251 ( 24%)]  Loss: 4.166 (3.77)  Time: 0.879s, 1165.58/s  (0.809s, 1265.36/s)  LR: 8.913e-04  Data: 0.009 (0.018)
Train: 129 [ 350/1251 ( 28%)]  Loss: 3.526 (3.74)  Time: 0.818s, 1251.43/s  (0.808s, 1267.98/s)  LR: 8.913e-04  Data: 0.010 (0.017)
Train: 129 [ 400/1251 ( 32%)]  Loss: 3.677 (3.74)  Time: 0.814s, 1258.51/s  (0.808s, 1267.78/s)  LR: 8.913e-04  Data: 0.011 (0.016)
Train: 129 [ 450/1251 ( 36%)]  Loss: 4.028 (3.77)  Time: 0.849s, 1205.50/s  (0.807s, 1269.01/s)  LR: 8.913e-04  Data: 0.011 (0.016)
Train: 129 [ 500/1251 ( 40%)]  Loss: 3.747 (3.76)  Time: 0.798s, 1283.02/s  (0.807s, 1269.27/s)  LR: 8.913e-04  Data: 0.010 (0.015)
Train: 129 [ 550/1251 ( 44%)]  Loss: 4.054 (3.79)  Time: 0.801s, 1278.53/s  (0.806s, 1270.97/s)  LR: 8.913e-04  Data: 0.011 (0.015)
Train: 129 [ 600/1251 ( 48%)]  Loss: 3.484 (3.76)  Time: 0.773s, 1324.49/s  (0.806s, 1270.86/s)  LR: 8.913e-04  Data: 0.010 (0.015)
Train: 129 [ 650/1251 ( 52%)]  Loss: 3.666 (3.76)  Time: 0.774s, 1322.32/s  (0.805s, 1271.38/s)  LR: 8.913e-04  Data: 0.010 (0.014)
Train: 129 [ 700/1251 ( 56%)]  Loss: 3.732 (3.76)  Time: 0.771s, 1328.50/s  (0.805s, 1272.04/s)  LR: 8.913e-04  Data: 0.013 (0.014)
Train: 129 [ 750/1251 ( 60%)]  Loss: 3.848 (3.76)  Time: 0.807s, 1268.70/s  (0.805s, 1272.73/s)  LR: 8.913e-04  Data: 0.016 (0.014)
Train: 129 [ 800/1251 ( 64%)]  Loss: 3.687 (3.76)  Time: 0.773s, 1324.09/s  (0.804s, 1273.20/s)  LR: 8.913e-04  Data: 0.010 (0.014)
Train: 129 [ 850/1251 ( 68%)]  Loss: 3.635 (3.75)  Time: 0.786s, 1303.26/s  (0.804s, 1273.84/s)  LR: 8.913e-04  Data: 0.014 (0.014)
Train: 129 [ 900/1251 ( 72%)]  Loss: 3.297 (3.73)  Time: 0.795s, 1287.89/s  (0.804s, 1274.12/s)  LR: 8.913e-04  Data: 0.011 (0.013)
Train: 129 [ 950/1251 ( 76%)]  Loss: 3.787 (3.73)  Time: 0.777s, 1317.37/s  (0.804s, 1273.73/s)  LR: 8.913e-04  Data: 0.010 (0.013)
Train: 129 [1000/1251 ( 80%)]  Loss: 3.914 (3.74)  Time: 0.800s, 1279.78/s  (0.804s, 1273.96/s)  LR: 8.913e-04  Data: 0.010 (0.013)
Train: 129 [1050/1251 ( 84%)]  Loss: 3.714 (3.74)  Time: 0.772s, 1326.78/s  (0.804s, 1274.19/s)  LR: 8.913e-04  Data: 0.010 (0.013)
Train: 129 [1100/1251 ( 88%)]  Loss: 3.551 (3.73)  Time: 0.792s, 1293.07/s  (0.803s, 1274.51/s)  LR: 8.913e-04  Data: 0.010 (0.013)
Train: 129 [1150/1251 ( 92%)]  Loss: 3.541 (3.72)  Time: 0.827s, 1238.88/s  (0.803s, 1275.11/s)  LR: 8.913e-04  Data: 0.010 (0.013)
Train: 129 [1200/1251 ( 96%)]  Loss: 3.923 (3.73)  Time: 0.793s, 1291.93/s  (0.803s, 1275.04/s)  LR: 8.913e-04  Data: 0.014 (0.013)
Train: 129 [1250/1251 (100%)]  Loss: 3.764 (3.73)  Time: 0.768s, 1333.58/s  (0.803s, 1275.27/s)  LR: 8.913e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.729 (1.729)  Loss:  1.0439 (1.0439)  Acc@1: 87.4023 (87.4023)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.194 (0.600)  Loss:  1.0107 (1.5254)  Acc@1: 84.7877 (71.9560)  Acc@5: 95.8726 (91.0540)
Train: 130 [   0/1251 (  0%)]  Loss: 3.744 (3.74)  Time: 2.409s,  425.08/s  (2.409s,  425.08/s)  LR: 8.897e-04  Data: 1.671 (1.671)
Train: 130 [  50/1251 (  4%)]  Loss: 3.828 (3.79)  Time: 0.780s, 1313.04/s  (0.840s, 1218.36/s)  LR: 8.897e-04  Data: 0.010 (0.051)
Train: 130 [ 100/1251 (  8%)]  Loss: 3.661 (3.74)  Time: 0.794s, 1289.51/s  (0.825s, 1241.56/s)  LR: 8.897e-04  Data: 0.023 (0.031)
Train: 130 [ 150/1251 ( 12%)]  Loss: 3.688 (3.73)  Time: 0.778s, 1316.86/s  (0.818s, 1252.44/s)  LR: 8.897e-04  Data: 0.010 (0.025)
Train: 130 [ 200/1251 ( 16%)]  Loss: 3.546 (3.69)  Time: 0.779s, 1314.37/s  (0.814s, 1258.51/s)  LR: 8.897e-04  Data: 0.010 (0.021)
Train: 130 [ 250/1251 ( 20%)]  Loss: 3.538 (3.67)  Time: 0.847s, 1208.56/s  (0.813s, 1259.88/s)  LR: 8.897e-04  Data: 0.014 (0.019)
Train: 130 [ 300/1251 ( 24%)]  Loss: 3.987 (3.71)  Time: 0.774s, 1323.74/s  (0.814s, 1257.26/s)  LR: 8.897e-04  Data: 0.012 (0.018)
Train: 130 [ 350/1251 ( 28%)]  Loss: 3.718 (3.71)  Time: 0.772s, 1326.03/s  (0.810s, 1264.72/s)  LR: 8.897e-04  Data: 0.010 (0.017)
Train: 130 [ 400/1251 ( 32%)]  Loss: 3.596 (3.70)  Time: 0.780s, 1312.94/s  (0.806s, 1270.44/s)  LR: 8.897e-04  Data: 0.012 (0.016)
Train: 130 [ 450/1251 ( 36%)]  Loss: 3.152 (3.65)  Time: 0.811s, 1262.60/s  (0.806s, 1271.12/s)  LR: 8.897e-04  Data: 0.010 (0.016)
Train: 130 [ 500/1251 ( 40%)]  Loss: 3.833 (3.66)  Time: 0.835s, 1226.65/s  (0.804s, 1272.95/s)  LR: 8.897e-04  Data: 0.015 (0.015)
Train: 130 [ 550/1251 ( 44%)]  Loss: 3.708 (3.67)  Time: 0.828s, 1236.92/s  (0.804s, 1272.87/s)  LR: 8.897e-04  Data: 0.010 (0.015)
Train: 130 [ 600/1251 ( 48%)]  Loss: 4.084 (3.70)  Time: 0.778s, 1315.77/s  (0.805s, 1272.06/s)  LR: 8.897e-04  Data: 0.010 (0.015)
Train: 130 [ 650/1251 ( 52%)]  Loss: 3.556 (3.69)  Time: 0.802s, 1277.32/s  (0.805s, 1271.61/s)  LR: 8.897e-04  Data: 0.010 (0.014)
Train: 130 [ 700/1251 ( 56%)]  Loss: 3.913 (3.70)  Time: 0.780s, 1313.26/s  (0.805s, 1271.95/s)  LR: 8.897e-04  Data: 0.010 (0.014)
Train: 130 [ 750/1251 ( 60%)]  Loss: 3.539 (3.69)  Time: 0.843s, 1214.62/s  (0.805s, 1272.03/s)  LR: 8.897e-04  Data: 0.010 (0.014)
Train: 130 [ 800/1251 ( 64%)]  Loss: 3.668 (3.69)  Time: 0.785s, 1304.13/s  (0.805s, 1271.62/s)  LR: 8.897e-04  Data: 0.014 (0.014)
Train: 130 [ 850/1251 ( 68%)]  Loss: 3.781 (3.70)  Time: 0.833s, 1229.47/s  (0.805s, 1271.95/s)  LR: 8.897e-04  Data: 0.009 (0.013)
Train: 130 [ 900/1251 ( 72%)]  Loss: 3.575 (3.69)  Time: 0.775s, 1321.04/s  (0.805s, 1272.58/s)  LR: 8.897e-04  Data: 0.010 (0.013)
Train: 130 [ 950/1251 ( 76%)]  Loss: 3.722 (3.69)  Time: 0.774s, 1323.47/s  (0.804s, 1273.31/s)  LR: 8.897e-04  Data: 0.010 (0.013)
Train: 130 [1000/1251 ( 80%)]  Loss: 3.316 (3.67)  Time: 0.775s, 1320.95/s  (0.804s, 1273.87/s)  LR: 8.897e-04  Data: 0.011 (0.013)
Train: 130 [1050/1251 ( 84%)]  Loss: 3.640 (3.67)  Time: 0.872s, 1173.83/s  (0.804s, 1273.70/s)  LR: 8.897e-04  Data: 0.010 (0.013)
Train: 130 [1100/1251 ( 88%)]  Loss: 3.168 (3.65)  Time: 0.800s, 1280.29/s  (0.804s, 1273.80/s)  LR: 8.897e-04  Data: 0.013 (0.013)
Train: 130 [1150/1251 ( 92%)]  Loss: 3.806 (3.66)  Time: 0.810s, 1264.44/s  (0.804s, 1273.58/s)  LR: 8.897e-04  Data: 0.010 (0.013)
Train: 130 [1200/1251 ( 96%)]  Loss: 3.545 (3.65)  Time: 0.771s, 1328.74/s  (0.804s, 1273.61/s)  LR: 8.897e-04  Data: 0.010 (0.013)
Train: 130 [1250/1251 (100%)]  Loss: 3.641 (3.65)  Time: 0.759s, 1349.04/s  (0.804s, 1273.48/s)  LR: 8.897e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.713 (1.713)  Loss:  0.8184 (0.8184)  Acc@1: 89.1602 (89.1602)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.194 (0.609)  Loss:  0.9756 (1.3592)  Acc@1: 82.5472 (72.4680)  Acc@5: 95.6368 (91.4580)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-130.pth.tar', 72.46799991699218)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-120.pth.tar', 72.4560001171875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-114.pth.tar', 72.40200011962891)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-126.pth.tar', 72.22800001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-103.pth.tar', 72.05000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-123.pth.tar', 72.01000002197266)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-108.pth.tar', 72.01000001464844)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-118.pth.tar', 72.00600000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-105.pth.tar', 72.00599989013672)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-121.pth.tar', 71.99600009277344)

Train: 131 [   0/1251 (  0%)]  Loss: 3.721 (3.72)  Time: 2.434s,  420.76/s  (2.434s,  420.76/s)  LR: 8.881e-04  Data: 1.687 (1.687)
Train: 131 [  50/1251 (  4%)]  Loss: 4.125 (3.92)  Time: 0.799s, 1282.37/s  (0.840s, 1219.09/s)  LR: 8.881e-04  Data: 0.011 (0.049)
Train: 131 [ 100/1251 (  8%)]  Loss: 3.696 (3.85)  Time: 0.823s, 1244.55/s  (0.820s, 1248.03/s)  LR: 8.881e-04  Data: 0.010 (0.031)
Train: 131 [ 150/1251 ( 12%)]  Loss: 3.786 (3.83)  Time: 0.782s, 1309.18/s  (0.815s, 1257.07/s)  LR: 8.881e-04  Data: 0.012 (0.024)
Train: 131 [ 200/1251 ( 16%)]  Loss: 3.535 (3.77)  Time: 0.771s, 1327.68/s  (0.812s, 1261.80/s)  LR: 8.881e-04  Data: 0.010 (0.021)
Train: 131 [ 250/1251 ( 20%)]  Loss: 3.778 (3.77)  Time: 0.797s, 1284.08/s  (0.810s, 1264.10/s)  LR: 8.881e-04  Data: 0.015 (0.019)
Train: 131 [ 300/1251 ( 24%)]  Loss: 3.924 (3.79)  Time: 0.780s, 1312.12/s  (0.809s, 1265.81/s)  LR: 8.881e-04  Data: 0.011 (0.018)
Train: 131 [ 350/1251 ( 28%)]  Loss: 3.533 (3.76)  Time: 0.842s, 1216.02/s  (0.808s, 1268.09/s)  LR: 8.881e-04  Data: 0.010 (0.017)
Train: 131 [ 400/1251 ( 32%)]  Loss: 4.068 (3.80)  Time: 0.770s, 1329.65/s  (0.806s, 1269.94/s)  LR: 8.881e-04  Data: 0.010 (0.016)
Train: 131 [ 450/1251 ( 36%)]  Loss: 3.713 (3.79)  Time: 0.853s, 1200.91/s  (0.805s, 1271.48/s)  LR: 8.881e-04  Data: 0.010 (0.015)
Train: 131 [ 500/1251 ( 40%)]  Loss: 3.457 (3.76)  Time: 0.788s, 1300.12/s  (0.805s, 1272.41/s)  LR: 8.881e-04  Data: 0.010 (0.015)
Train: 131 [ 550/1251 ( 44%)]  Loss: 3.833 (3.76)  Time: 0.775s, 1321.72/s  (0.804s, 1272.92/s)  LR: 8.881e-04  Data: 0.010 (0.015)
Train: 131 [ 600/1251 ( 48%)]  Loss: 3.334 (3.73)  Time: 0.801s, 1278.72/s  (0.805s, 1272.21/s)  LR: 8.881e-04  Data: 0.011 (0.014)
Train: 131 [ 650/1251 ( 52%)]  Loss: 3.913 (3.74)  Time: 0.809s, 1265.17/s  (0.805s, 1272.34/s)  LR: 8.881e-04  Data: 0.010 (0.014)
Train: 131 [ 700/1251 ( 56%)]  Loss: 3.722 (3.74)  Time: 0.799s, 1280.84/s  (0.805s, 1272.77/s)  LR: 8.881e-04  Data: 0.010 (0.014)
Train: 131 [ 750/1251 ( 60%)]  Loss: 2.976 (3.69)  Time: 0.893s, 1146.45/s  (0.804s, 1273.30/s)  LR: 8.881e-04  Data: 0.010 (0.014)
Train: 131 [ 800/1251 ( 64%)]  Loss: 3.896 (3.71)  Time: 0.773s, 1325.18/s  (0.804s, 1273.92/s)  LR: 8.881e-04  Data: 0.010 (0.014)
Train: 131 [ 850/1251 ( 68%)]  Loss: 3.681 (3.71)  Time: 0.772s, 1327.15/s  (0.803s, 1274.75/s)  LR: 8.881e-04  Data: 0.010 (0.013)
Train: 131 [ 900/1251 ( 72%)]  Loss: 4.034 (3.72)  Time: 0.810s, 1264.04/s  (0.803s, 1275.20/s)  LR: 8.881e-04  Data: 0.011 (0.013)
Train: 131 [ 950/1251 ( 76%)]  Loss: 4.150 (3.74)  Time: 0.789s, 1298.52/s  (0.803s, 1274.67/s)  LR: 8.881e-04  Data: 0.010 (0.013)
Train: 131 [1000/1251 ( 80%)]  Loss: 3.452 (3.73)  Time: 0.774s, 1323.15/s  (0.803s, 1275.26/s)  LR: 8.881e-04  Data: 0.011 (0.013)
Train: 131 [1050/1251 ( 84%)]  Loss: 3.840 (3.73)  Time: 0.799s, 1282.00/s  (0.803s, 1275.48/s)  LR: 8.881e-04  Data: 0.011 (0.013)
Train: 131 [1100/1251 ( 88%)]  Loss: 3.587 (3.73)  Time: 0.795s, 1287.55/s  (0.803s, 1275.69/s)  LR: 8.881e-04  Data: 0.010 (0.013)
Train: 131 [1150/1251 ( 92%)]  Loss: 3.929 (3.74)  Time: 0.797s, 1285.21/s  (0.803s, 1275.84/s)  LR: 8.881e-04  Data: 0.010 (0.013)
Train: 131 [1200/1251 ( 96%)]  Loss: 3.738 (3.74)  Time: 0.794s, 1290.45/s  (0.802s, 1276.11/s)  LR: 8.881e-04  Data: 0.014 (0.013)
Train: 131 [1250/1251 (100%)]  Loss: 3.957 (3.75)  Time: 0.760s, 1347.73/s  (0.802s, 1276.04/s)  LR: 8.881e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.699 (1.699)  Loss:  0.7104 (0.7104)  Acc@1: 88.2812 (88.2812)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.194 (0.603)  Loss:  0.8608 (1.3704)  Acc@1: 83.2547 (71.8780)  Acc@5: 95.6368 (91.0640)
Train: 132 [   0/1251 (  0%)]  Loss: 3.705 (3.70)  Time: 2.501s,  409.37/s  (2.501s,  409.37/s)  LR: 8.864e-04  Data: 1.771 (1.771)
Train: 132 [  50/1251 (  4%)]  Loss: 3.655 (3.68)  Time: 0.783s, 1307.75/s  (0.848s, 1208.06/s)  LR: 8.864e-04  Data: 0.010 (0.053)
Train: 132 [ 100/1251 (  8%)]  Loss: 3.732 (3.70)  Time: 0.772s, 1327.21/s  (0.830s, 1233.43/s)  LR: 8.864e-04  Data: 0.010 (0.032)
Train: 132 [ 150/1251 ( 12%)]  Loss: 3.217 (3.58)  Time: 0.777s, 1317.57/s  (0.818s, 1251.68/s)  LR: 8.864e-04  Data: 0.010 (0.025)
Train: 132 [ 200/1251 ( 16%)]  Loss: 3.876 (3.64)  Time: 0.778s, 1316.75/s  (0.813s, 1259.47/s)  LR: 8.864e-04  Data: 0.009 (0.022)
Train: 132 [ 250/1251 ( 20%)]  Loss: 3.894 (3.68)  Time: 0.842s, 1215.59/s  (0.811s, 1262.53/s)  LR: 8.864e-04  Data: 0.010 (0.019)
Train: 132 [ 300/1251 ( 24%)]  Loss: 3.866 (3.71)  Time: 0.801s, 1279.08/s  (0.809s, 1266.49/s)  LR: 8.864e-04  Data: 0.010 (0.018)
Train: 132 [ 350/1251 ( 28%)]  Loss: 3.805 (3.72)  Time: 0.792s, 1293.57/s  (0.808s, 1266.86/s)  LR: 8.864e-04  Data: 0.010 (0.017)
Train: 132 [ 400/1251 ( 32%)]  Loss: 3.998 (3.75)  Time: 0.803s, 1275.67/s  (0.808s, 1267.74/s)  LR: 8.864e-04  Data: 0.010 (0.016)
Train: 132 [ 450/1251 ( 36%)]  Loss: 3.762 (3.75)  Time: 0.815s, 1257.12/s  (0.807s, 1268.39/s)  LR: 8.864e-04  Data: 0.010 (0.016)
Train: 132 [ 500/1251 ( 40%)]  Loss: 3.504 (3.73)  Time: 0.791s, 1294.85/s  (0.806s, 1270.25/s)  LR: 8.864e-04  Data: 0.010 (0.015)
Train: 132 [ 550/1251 ( 44%)]  Loss: 3.771 (3.73)  Time: 0.823s, 1244.34/s  (0.806s, 1270.25/s)  LR: 8.864e-04  Data: 0.010 (0.015)
Train: 132 [ 600/1251 ( 48%)]  Loss: 3.594 (3.72)  Time: 0.794s, 1289.88/s  (0.806s, 1270.73/s)  LR: 8.864e-04  Data: 0.016 (0.015)
Train: 132 [ 650/1251 ( 52%)]  Loss: 3.479 (3.70)  Time: 0.808s, 1266.66/s  (0.805s, 1271.73/s)  LR: 8.864e-04  Data: 0.009 (0.014)
Train: 132 [ 700/1251 ( 56%)]  Loss: 3.523 (3.69)  Time: 0.826s, 1239.85/s  (0.805s, 1272.64/s)  LR: 8.864e-04  Data: 0.014 (0.014)
Train: 132 [ 750/1251 ( 60%)]  Loss: 4.002 (3.71)  Time: 0.777s, 1318.73/s  (0.805s, 1272.42/s)  LR: 8.864e-04  Data: 0.010 (0.014)
Train: 132 [ 800/1251 ( 64%)]  Loss: 3.387 (3.69)  Time: 0.775s, 1320.59/s  (0.804s, 1273.24/s)  LR: 8.864e-04  Data: 0.010 (0.014)
Train: 132 [ 850/1251 ( 68%)]  Loss: 3.792 (3.70)  Time: 0.799s, 1281.55/s  (0.804s, 1273.21/s)  LR: 8.864e-04  Data: 0.014 (0.014)
Train: 132 [ 900/1251 ( 72%)]  Loss: 3.652 (3.70)  Time: 0.861s, 1189.24/s  (0.804s, 1273.40/s)  LR: 8.864e-04  Data: 0.010 (0.013)
Train: 132 [ 950/1251 ( 76%)]  Loss: 3.818 (3.70)  Time: 0.792s, 1292.80/s  (0.804s, 1273.49/s)  LR: 8.864e-04  Data: 0.009 (0.013)
Train: 132 [1000/1251 ( 80%)]  Loss: 3.637 (3.70)  Time: 0.775s, 1321.62/s  (0.804s, 1273.34/s)  LR: 8.864e-04  Data: 0.010 (0.013)
Train: 132 [1050/1251 ( 84%)]  Loss: 3.584 (3.69)  Time: 0.794s, 1289.79/s  (0.804s, 1273.25/s)  LR: 8.864e-04  Data: 0.009 (0.013)
Train: 132 [1100/1251 ( 88%)]  Loss: 3.700 (3.69)  Time: 0.880s, 1163.85/s  (0.804s, 1273.20/s)  LR: 8.864e-04  Data: 0.013 (0.013)
Train: 132 [1150/1251 ( 92%)]  Loss: 3.827 (3.70)  Time: 0.796s, 1286.93/s  (0.804s, 1273.41/s)  LR: 8.864e-04  Data: 0.010 (0.013)
Train: 132 [1200/1251 ( 96%)]  Loss: 3.577 (3.69)  Time: 0.806s, 1270.67/s  (0.804s, 1273.50/s)  LR: 8.864e-04  Data: 0.010 (0.013)
Train: 132 [1250/1251 (100%)]  Loss: 3.429 (3.68)  Time: 0.821s, 1247.13/s  (0.804s, 1274.35/s)  LR: 8.864e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.598 (1.598)  Loss:  0.7964 (0.7964)  Acc@1: 87.6953 (87.6953)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.194 (0.608)  Loss:  1.0059 (1.4142)  Acc@1: 84.5519 (72.5200)  Acc@5: 95.1651 (91.2800)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-132.pth.tar', 72.52000001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-130.pth.tar', 72.46799991699218)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-120.pth.tar', 72.4560001171875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-114.pth.tar', 72.40200011962891)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-126.pth.tar', 72.22800001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-103.pth.tar', 72.05000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-123.pth.tar', 72.01000002197266)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-108.pth.tar', 72.01000001464844)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-118.pth.tar', 72.00600000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-105.pth.tar', 72.00599989013672)

Train: 133 [   0/1251 (  0%)]  Loss: 3.610 (3.61)  Time: 2.448s,  418.24/s  (2.448s,  418.24/s)  LR: 8.847e-04  Data: 1.720 (1.720)
Train: 133 [  50/1251 (  4%)]  Loss: 3.978 (3.79)  Time: 0.810s, 1264.15/s  (0.845s, 1211.90/s)  LR: 8.847e-04  Data: 0.009 (0.050)
Train: 133 [ 100/1251 (  8%)]  Loss: 3.635 (3.74)  Time: 0.777s, 1317.83/s  (0.821s, 1246.64/s)  LR: 8.847e-04  Data: 0.010 (0.031)
Train: 133 [ 150/1251 ( 12%)]  Loss: 3.293 (3.63)  Time: 0.772s, 1326.44/s  (0.814s, 1257.23/s)  LR: 8.847e-04  Data: 0.010 (0.024)
Train: 133 [ 200/1251 ( 16%)]  Loss: 3.811 (3.67)  Time: 0.812s, 1261.42/s  (0.812s, 1260.63/s)  LR: 8.847e-04  Data: 0.010 (0.021)
Train: 133 [ 250/1251 ( 20%)]  Loss: 3.571 (3.65)  Time: 0.805s, 1271.49/s  (0.812s, 1261.05/s)  LR: 8.847e-04  Data: 0.013 (0.019)
Train: 133 [ 300/1251 ( 24%)]  Loss: 3.630 (3.65)  Time: 0.773s, 1324.00/s  (0.811s, 1262.95/s)  LR: 8.847e-04  Data: 0.010 (0.018)
Train: 133 [ 350/1251 ( 28%)]  Loss: 3.685 (3.65)  Time: 0.842s, 1216.09/s  (0.810s, 1264.45/s)  LR: 8.847e-04  Data: 0.014 (0.017)
Train: 133 [ 400/1251 ( 32%)]  Loss: 3.541 (3.64)  Time: 0.810s, 1264.94/s  (0.810s, 1264.51/s)  LR: 8.847e-04  Data: 0.010 (0.016)
Train: 133 [ 450/1251 ( 36%)]  Loss: 3.976 (3.67)  Time: 0.781s, 1311.29/s  (0.809s, 1265.80/s)  LR: 8.847e-04  Data: 0.011 (0.016)
Train: 133 [ 500/1251 ( 40%)]  Loss: 3.851 (3.69)  Time: 0.794s, 1289.35/s  (0.808s, 1267.37/s)  LR: 8.847e-04  Data: 0.009 (0.015)
Train: 133 [ 550/1251 ( 44%)]  Loss: 3.529 (3.68)  Time: 0.863s, 1185.99/s  (0.808s, 1267.60/s)  LR: 8.847e-04  Data: 0.011 (0.015)
Train: 133 [ 600/1251 ( 48%)]  Loss: 3.723 (3.68)  Time: 0.904s, 1133.03/s  (0.808s, 1267.85/s)  LR: 8.847e-04  Data: 0.011 (0.014)
Train: 133 [ 650/1251 ( 52%)]  Loss: 3.612 (3.67)  Time: 0.775s, 1321.79/s  (0.807s, 1268.87/s)  LR: 8.847e-04  Data: 0.011 (0.014)
Train: 133 [ 700/1251 ( 56%)]  Loss: 3.766 (3.68)  Time: 0.817s, 1253.13/s  (0.807s, 1269.63/s)  LR: 8.847e-04  Data: 0.012 (0.014)
Train: 133 [ 750/1251 ( 60%)]  Loss: 3.340 (3.66)  Time: 0.816s, 1255.18/s  (0.806s, 1270.23/s)  LR: 8.847e-04  Data: 0.010 (0.014)
Train: 133 [ 800/1251 ( 64%)]  Loss: 3.530 (3.65)  Time: 0.820s, 1249.20/s  (0.806s, 1270.05/s)  LR: 8.847e-04  Data: 0.011 (0.014)
Train: 133 [ 850/1251 ( 68%)]  Loss: 3.705 (3.65)  Time: 0.800s, 1280.39/s  (0.806s, 1270.61/s)  LR: 8.847e-04  Data: 0.010 (0.013)
Train: 133 [ 900/1251 ( 72%)]  Loss: 3.482 (3.65)  Time: 0.817s, 1253.34/s  (0.806s, 1271.08/s)  LR: 8.847e-04  Data: 0.016 (0.013)
Train: 133 [ 950/1251 ( 76%)]  Loss: 4.045 (3.67)  Time: 0.784s, 1306.36/s  (0.805s, 1271.27/s)  LR: 8.847e-04  Data: 0.009 (0.013)
Train: 133 [1000/1251 ( 80%)]  Loss: 3.959 (3.68)  Time: 0.780s, 1312.69/s  (0.805s, 1271.87/s)  LR: 8.847e-04  Data: 0.010 (0.013)
Train: 133 [1050/1251 ( 84%)]  Loss: 3.294 (3.66)  Time: 0.793s, 1291.70/s  (0.805s, 1272.75/s)  LR: 8.847e-04  Data: 0.010 (0.013)
Train: 133 [1100/1251 ( 88%)]  Loss: 3.583 (3.66)  Time: 0.787s, 1300.92/s  (0.804s, 1273.39/s)  LR: 8.847e-04  Data: 0.016 (0.013)
Train: 133 [1150/1251 ( 92%)]  Loss: 3.652 (3.66)  Time: 0.795s, 1288.60/s  (0.804s, 1273.48/s)  LR: 8.847e-04  Data: 0.010 (0.013)
Train: 133 [1200/1251 ( 96%)]  Loss: 4.067 (3.67)  Time: 0.774s, 1323.11/s  (0.804s, 1273.54/s)  LR: 8.847e-04  Data: 0.011 (0.013)
Train: 133 [1250/1251 (100%)]  Loss: 3.891 (3.68)  Time: 0.769s, 1331.50/s  (0.804s, 1273.64/s)  LR: 8.847e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.640 (1.640)  Loss:  0.8467 (0.8467)  Acc@1: 87.7930 (87.7930)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.194 (0.610)  Loss:  0.8774 (1.3571)  Acc@1: 83.3727 (72.5140)  Acc@5: 95.6368 (91.5180)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-132.pth.tar', 72.52000001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-133.pth.tar', 72.51400014648438)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-130.pth.tar', 72.46799991699218)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-120.pth.tar', 72.4560001171875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-114.pth.tar', 72.40200011962891)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-126.pth.tar', 72.22800001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-103.pth.tar', 72.05000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-123.pth.tar', 72.01000002197266)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-108.pth.tar', 72.01000001464844)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-118.pth.tar', 72.00600000732422)

Train: 134 [   0/1251 (  0%)]  Loss: 3.806 (3.81)  Time: 2.480s,  412.87/s  (2.480s,  412.87/s)  LR: 8.831e-04  Data: 1.751 (1.751)
Train: 134 [  50/1251 (  4%)]  Loss: 3.847 (3.83)  Time: 0.776s, 1318.94/s  (0.843s, 1214.78/s)  LR: 8.831e-04  Data: 0.010 (0.052)
Train: 134 [ 100/1251 (  8%)]  Loss: 3.288 (3.65)  Time: 0.805s, 1272.35/s  (0.824s, 1242.07/s)  LR: 8.831e-04  Data: 0.011 (0.032)
Train: 134 [ 150/1251 ( 12%)]  Loss: 3.871 (3.70)  Time: 0.805s, 1271.69/s  (0.816s, 1254.81/s)  LR: 8.831e-04  Data: 0.010 (0.025)
Train: 134 [ 200/1251 ( 16%)]  Loss: 3.781 (3.72)  Time: 0.929s, 1101.84/s  (0.812s, 1261.38/s)  LR: 8.831e-04  Data: 0.012 (0.022)
Train: 134 [ 250/1251 ( 20%)]  Loss: 3.607 (3.70)  Time: 0.800s, 1280.43/s  (0.811s, 1262.87/s)  LR: 8.831e-04  Data: 0.010 (0.019)
Train: 134 [ 300/1251 ( 24%)]  Loss: 3.409 (3.66)  Time: 0.791s, 1294.90/s  (0.810s, 1264.81/s)  LR: 8.831e-04  Data: 0.010 (0.018)
Train: 134 [ 350/1251 ( 28%)]  Loss: 3.432 (3.63)  Time: 0.801s, 1278.62/s  (0.809s, 1266.43/s)  LR: 8.831e-04  Data: 0.010 (0.017)
Train: 134 [ 400/1251 ( 32%)]  Loss: 3.742 (3.64)  Time: 0.774s, 1323.03/s  (0.808s, 1267.96/s)  LR: 8.831e-04  Data: 0.010 (0.016)
Train: 134 [ 450/1251 ( 36%)]  Loss: 3.933 (3.67)  Time: 0.833s, 1229.02/s  (0.807s, 1268.91/s)  LR: 8.831e-04  Data: 0.010 (0.016)
Train: 134 [ 500/1251 ( 40%)]  Loss: 3.610 (3.67)  Time: 0.823s, 1244.78/s  (0.807s, 1268.99/s)  LR: 8.831e-04  Data: 0.014 (0.015)
Train: 134 [ 550/1251 ( 44%)]  Loss: 3.647 (3.66)  Time: 0.813s, 1259.17/s  (0.806s, 1270.62/s)  LR: 8.831e-04  Data: 0.013 (0.015)
Train: 134 [ 600/1251 ( 48%)]  Loss: 3.448 (3.65)  Time: 0.780s, 1312.50/s  (0.806s, 1270.82/s)  LR: 8.831e-04  Data: 0.010 (0.015)
Train: 134 [ 650/1251 ( 52%)]  Loss: 3.709 (3.65)  Time: 0.778s, 1315.54/s  (0.805s, 1271.71/s)  LR: 8.831e-04  Data: 0.010 (0.014)
Train: 134 [ 700/1251 ( 56%)]  Loss: 3.331 (3.63)  Time: 0.825s, 1240.53/s  (0.805s, 1272.23/s)  LR: 8.831e-04  Data: 0.010 (0.014)
Train: 134 [ 750/1251 ( 60%)]  Loss: 3.449 (3.62)  Time: 0.801s, 1278.92/s  (0.805s, 1272.24/s)  LR: 8.831e-04  Data: 0.010 (0.014)
Train: 134 [ 800/1251 ( 64%)]  Loss: 3.742 (3.63)  Time: 0.805s, 1272.19/s  (0.805s, 1272.66/s)  LR: 8.831e-04  Data: 0.013 (0.014)
Train: 134 [ 850/1251 ( 68%)]  Loss: 3.712 (3.63)  Time: 0.803s, 1275.49/s  (0.805s, 1272.20/s)  LR: 8.831e-04  Data: 0.010 (0.014)
Train: 134 [ 900/1251 ( 72%)]  Loss: 3.638 (3.63)  Time: 0.794s, 1289.18/s  (0.804s, 1273.18/s)  LR: 8.831e-04  Data: 0.011 (0.014)
Train: 134 [ 950/1251 ( 76%)]  Loss: 3.724 (3.64)  Time: 0.793s, 1291.14/s  (0.804s, 1273.05/s)  LR: 8.831e-04  Data: 0.016 (0.013)
Train: 134 [1000/1251 ( 80%)]  Loss: 3.542 (3.63)  Time: 0.778s, 1316.68/s  (0.804s, 1273.00/s)  LR: 8.831e-04  Data: 0.010 (0.013)
Train: 134 [1050/1251 ( 84%)]  Loss: 3.771 (3.64)  Time: 0.800s, 1279.97/s  (0.804s, 1273.29/s)  LR: 8.831e-04  Data: 0.010 (0.013)
Train: 134 [1100/1251 ( 88%)]  Loss: 3.242 (3.62)  Time: 0.835s, 1226.29/s  (0.804s, 1273.65/s)  LR: 8.831e-04  Data: 0.010 (0.013)
Train: 134 [1150/1251 ( 92%)]  Loss: 3.419 (3.61)  Time: 0.802s, 1277.06/s  (0.804s, 1273.50/s)  LR: 8.831e-04  Data: 0.013 (0.013)
Train: 134 [1200/1251 ( 96%)]  Loss: 3.722 (3.62)  Time: 0.774s, 1323.78/s  (0.804s, 1273.92/s)  LR: 8.831e-04  Data: 0.009 (0.013)
Train: 134 [1250/1251 (100%)]  Loss: 3.807 (3.62)  Time: 0.860s, 1190.40/s  (0.804s, 1273.49/s)  LR: 8.831e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.133 (2.133)  Loss:  0.7622 (0.7622)  Acc@1: 88.5742 (88.5742)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.193 (0.783)  Loss:  0.9424 (1.3352)  Acc@1: 83.1368 (72.3740)  Acc@5: 95.5189 (91.1240)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-132.pth.tar', 72.52000001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-133.pth.tar', 72.51400014648438)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-130.pth.tar', 72.46799991699218)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-120.pth.tar', 72.4560001171875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-114.pth.tar', 72.40200011962891)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-134.pth.tar', 72.37400004394532)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-126.pth.tar', 72.22800001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-103.pth.tar', 72.05000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-123.pth.tar', 72.01000002197266)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-108.pth.tar', 72.01000001464844)

Train: 135 [   0/1251 (  0%)]  Loss: 3.556 (3.56)  Time: 2.164s,  473.26/s  (2.164s,  473.26/s)  LR: 8.814e-04  Data: 1.432 (1.432)
Train: 135 [  50/1251 (  4%)]  Loss: 3.783 (3.67)  Time: 0.773s, 1325.18/s  (0.820s, 1249.44/s)  LR: 8.814e-04  Data: 0.010 (0.050)
Train: 135 [ 100/1251 (  8%)]  Loss: 3.861 (3.73)  Time: 0.819s, 1250.93/s  (0.802s, 1277.55/s)  LR: 8.814e-04  Data: 0.010 (0.030)
Train: 135 [ 150/1251 ( 12%)]  Loss: 3.233 (3.61)  Time: 0.836s, 1225.56/s  (0.803s, 1275.69/s)  LR: 8.814e-04  Data: 0.009 (0.024)
Train: 135 [ 200/1251 ( 16%)]  Loss: 3.837 (3.65)  Time: 0.836s, 1224.25/s  (0.803s, 1275.74/s)  LR: 8.814e-04  Data: 0.013 (0.020)
Train: 135 [ 250/1251 ( 20%)]  Loss: 3.918 (3.70)  Time: 0.807s, 1268.98/s  (0.803s, 1275.97/s)  LR: 8.814e-04  Data: 0.010 (0.018)
Train: 135 [ 300/1251 ( 24%)]  Loss: 3.568 (3.68)  Time: 0.785s, 1304.20/s  (0.801s, 1277.88/s)  LR: 8.814e-04  Data: 0.015 (0.017)
Train: 135 [ 350/1251 ( 28%)]  Loss: 3.663 (3.68)  Time: 0.822s, 1245.48/s  (0.802s, 1277.07/s)  LR: 8.814e-04  Data: 0.009 (0.016)
Train: 135 [ 400/1251 ( 32%)]  Loss: 3.639 (3.67)  Time: 0.787s, 1301.21/s  (0.801s, 1278.38/s)  LR: 8.814e-04  Data: 0.013 (0.016)
Train: 135 [ 450/1251 ( 36%)]  Loss: 3.665 (3.67)  Time: 0.772s, 1326.00/s  (0.800s, 1279.25/s)  LR: 8.814e-04  Data: 0.010 (0.015)
Train: 135 [ 500/1251 ( 40%)]  Loss: 3.645 (3.67)  Time: 0.844s, 1213.61/s  (0.800s, 1279.28/s)  LR: 8.814e-04  Data: 0.012 (0.015)
Train: 135 [ 550/1251 ( 44%)]  Loss: 3.741 (3.68)  Time: 0.821s, 1247.72/s  (0.801s, 1277.81/s)  LR: 8.814e-04  Data: 0.011 (0.014)
Train: 135 [ 600/1251 ( 48%)]  Loss: 3.667 (3.68)  Time: 0.819s, 1250.14/s  (0.802s, 1277.05/s)  LR: 8.814e-04  Data: 0.010 (0.014)
Train: 135 [ 650/1251 ( 52%)]  Loss: 3.818 (3.69)  Time: 0.796s, 1286.79/s  (0.802s, 1277.24/s)  LR: 8.814e-04  Data: 0.010 (0.014)
Train: 135 [ 700/1251 ( 56%)]  Loss: 3.490 (3.67)  Time: 0.785s, 1304.62/s  (0.802s, 1276.96/s)  LR: 8.814e-04  Data: 0.010 (0.014)
Train: 135 [ 750/1251 ( 60%)]  Loss: 3.499 (3.66)  Time: 0.775s, 1320.58/s  (0.802s, 1277.26/s)  LR: 8.814e-04  Data: 0.010 (0.013)
Train: 135 [ 800/1251 ( 64%)]  Loss: 3.455 (3.65)  Time: 0.832s, 1230.57/s  (0.801s, 1277.74/s)  LR: 8.814e-04  Data: 0.010 (0.013)
Train: 135 [ 850/1251 ( 68%)]  Loss: 3.995 (3.67)  Time: 0.792s, 1293.02/s  (0.801s, 1278.07/s)  LR: 8.814e-04  Data: 0.010 (0.013)
Train: 135 [ 900/1251 ( 72%)]  Loss: 3.479 (3.66)  Time: 0.769s, 1331.34/s  (0.801s, 1277.96/s)  LR: 8.814e-04  Data: 0.010 (0.013)
Train: 135 [ 950/1251 ( 76%)]  Loss: 3.773 (3.66)  Time: 0.820s, 1248.90/s  (0.802s, 1277.47/s)  LR: 8.814e-04  Data: 0.012 (0.013)
Train: 135 [1000/1251 ( 80%)]  Loss: 3.720 (3.67)  Time: 0.805s, 1272.45/s  (0.802s, 1277.47/s)  LR: 8.814e-04  Data: 0.010 (0.013)
Train: 135 [1050/1251 ( 84%)]  Loss: 3.569 (3.66)  Time: 0.779s, 1314.14/s  (0.801s, 1277.62/s)  LR: 8.814e-04  Data: 0.013 (0.013)
Train: 135 [1100/1251 ( 88%)]  Loss: 3.786 (3.67)  Time: 0.783s, 1308.41/s  (0.802s, 1277.43/s)  LR: 8.814e-04  Data: 0.010 (0.013)
Train: 135 [1150/1251 ( 92%)]  Loss: 3.789 (3.67)  Time: 0.775s, 1321.22/s  (0.802s, 1277.18/s)  LR: 8.814e-04  Data: 0.010 (0.013)
Train: 135 [1200/1251 ( 96%)]  Loss: 3.822 (3.68)  Time: 0.773s, 1324.03/s  (0.802s, 1277.41/s)  LR: 8.814e-04  Data: 0.010 (0.013)
Train: 135 [1250/1251 (100%)]  Loss: 3.689 (3.68)  Time: 0.766s, 1337.56/s  (0.801s, 1277.66/s)  LR: 8.814e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.624 (1.624)  Loss:  0.8535 (0.8535)  Acc@1: 85.5469 (85.5469)  Acc@5: 96.1914 (96.1914)
Test: [  48/48]  Time: 0.194 (0.603)  Loss:  1.0146 (1.4022)  Acc@1: 81.8396 (71.3520)  Acc@5: 93.9858 (90.6500)
Train: 136 [   0/1251 (  0%)]  Loss: 3.732 (3.73)  Time: 2.232s,  458.80/s  (2.232s,  458.80/s)  LR: 8.797e-04  Data: 1.498 (1.498)
Train: 136 [  50/1251 (  4%)]  Loss: 3.642 (3.69)  Time: 0.791s, 1294.55/s  (0.832s, 1230.15/s)  LR: 8.797e-04  Data: 0.016 (0.045)
Train: 136 [ 100/1251 (  8%)]  Loss: 3.366 (3.58)  Time: 0.815s, 1256.98/s  (0.814s, 1257.50/s)  LR: 8.797e-04  Data: 0.010 (0.028)
Train: 136 [ 150/1251 ( 12%)]  Loss: 3.701 (3.61)  Time: 0.808s, 1266.88/s  (0.809s, 1266.04/s)  LR: 8.797e-04  Data: 0.010 (0.023)
Train: 136 [ 200/1251 ( 16%)]  Loss: 3.925 (3.67)  Time: 0.840s, 1219.25/s  (0.807s, 1269.10/s)  LR: 8.797e-04  Data: 0.014 (0.020)
Train: 136 [ 250/1251 ( 20%)]  Loss: 4.049 (3.74)  Time: 0.832s, 1231.17/s  (0.806s, 1270.58/s)  LR: 8.797e-04  Data: 0.009 (0.018)
Train: 136 [ 300/1251 ( 24%)]  Loss: 3.931 (3.76)  Time: 0.779s, 1314.78/s  (0.804s, 1272.94/s)  LR: 8.797e-04  Data: 0.009 (0.017)
Train: 136 [ 350/1251 ( 28%)]  Loss: 3.833 (3.77)  Time: 0.773s, 1324.68/s  (0.804s, 1274.25/s)  LR: 8.797e-04  Data: 0.010 (0.016)
Train: 136 [ 400/1251 ( 32%)]  Loss: 3.744 (3.77)  Time: 0.849s, 1205.75/s  (0.804s, 1272.98/s)  LR: 8.797e-04  Data: 0.010 (0.015)
Train: 136 [ 450/1251 ( 36%)]  Loss: 3.790 (3.77)  Time: 0.843s, 1214.67/s  (0.804s, 1272.97/s)  LR: 8.797e-04  Data: 0.010 (0.015)
Train: 136 [ 500/1251 ( 40%)]  Loss: 3.470 (3.74)  Time: 0.777s, 1318.23/s  (0.804s, 1273.68/s)  LR: 8.797e-04  Data: 0.011 (0.014)
Train: 136 [ 550/1251 ( 44%)]  Loss: 3.909 (3.76)  Time: 0.789s, 1297.82/s  (0.803s, 1274.56/s)  LR: 8.797e-04  Data: 0.014 (0.014)
Train: 136 [ 600/1251 ( 48%)]  Loss: 3.129 (3.71)  Time: 0.802s, 1277.37/s  (0.803s, 1275.05/s)  LR: 8.797e-04  Data: 0.013 (0.014)
Train: 136 [ 650/1251 ( 52%)]  Loss: 3.732 (3.71)  Time: 0.777s, 1317.21/s  (0.802s, 1276.21/s)  LR: 8.797e-04  Data: 0.011 (0.014)
Train: 136 [ 700/1251 ( 56%)]  Loss: 3.814 (3.72)  Time: 0.837s, 1223.72/s  (0.802s, 1276.87/s)  LR: 8.797e-04  Data: 0.009 (0.013)
Train: 136 [ 750/1251 ( 60%)]  Loss: 3.696 (3.72)  Time: 0.847s, 1208.38/s  (0.802s, 1276.65/s)  LR: 8.797e-04  Data: 0.010 (0.013)
Train: 136 [ 800/1251 ( 64%)]  Loss: 3.894 (3.73)  Time: 0.870s, 1176.54/s  (0.802s, 1276.55/s)  LR: 8.797e-04  Data: 0.010 (0.013)
Train: 136 [ 850/1251 ( 68%)]  Loss: 3.808 (3.73)  Time: 0.844s, 1212.99/s  (0.802s, 1277.07/s)  LR: 8.797e-04  Data: 0.009 (0.013)
Train: 136 [ 900/1251 ( 72%)]  Loss: 3.291 (3.71)  Time: 0.840s, 1219.48/s  (0.802s, 1276.98/s)  LR: 8.797e-04  Data: 0.010 (0.013)
Train: 136 [ 950/1251 ( 76%)]  Loss: 3.278 (3.69)  Time: 0.776s, 1319.88/s  (0.802s, 1277.18/s)  LR: 8.797e-04  Data: 0.010 (0.013)
Train: 136 [1000/1251 ( 80%)]  Loss: 3.757 (3.69)  Time: 0.849s, 1206.53/s  (0.802s, 1277.48/s)  LR: 8.797e-04  Data: 0.010 (0.013)
Train: 136 [1050/1251 ( 84%)]  Loss: 3.516 (3.68)  Time: 0.803s, 1275.08/s  (0.802s, 1277.15/s)  LR: 8.797e-04  Data: 0.010 (0.013)
Train: 136 [1100/1251 ( 88%)]  Loss: 3.948 (3.69)  Time: 0.847s, 1209.28/s  (0.802s, 1277.16/s)  LR: 8.797e-04  Data: 0.010 (0.013)
Train: 136 [1150/1251 ( 92%)]  Loss: 3.933 (3.70)  Time: 0.853s, 1200.76/s  (0.802s, 1276.85/s)  LR: 8.797e-04  Data: 0.009 (0.012)
Train: 136 [1200/1251 ( 96%)]  Loss: 3.642 (3.70)  Time: 0.820s, 1248.47/s  (0.802s, 1276.98/s)  LR: 8.797e-04  Data: 0.010 (0.012)
Train: 136 [1250/1251 (100%)]  Loss: 3.555 (3.70)  Time: 0.772s, 1326.44/s  (0.802s, 1277.32/s)  LR: 8.797e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.590 (1.590)  Loss:  0.8574 (0.8574)  Acc@1: 86.4258 (86.4258)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.194 (0.596)  Loss:  0.9478 (1.4646)  Acc@1: 83.0189 (71.5940)  Acc@5: 94.9292 (90.8380)
Train: 137 [   0/1251 (  0%)]  Loss: 3.731 (3.73)  Time: 2.432s,  420.97/s  (2.432s,  420.97/s)  LR: 8.780e-04  Data: 1.649 (1.649)
Train: 137 [  50/1251 (  4%)]  Loss: 3.942 (3.84)  Time: 0.788s, 1299.38/s  (0.829s, 1234.75/s)  LR: 8.780e-04  Data: 0.010 (0.046)
Train: 137 [ 100/1251 (  8%)]  Loss: 3.746 (3.81)  Time: 0.830s, 1233.41/s  (0.815s, 1256.17/s)  LR: 8.780e-04  Data: 0.010 (0.029)
Train: 137 [ 150/1251 ( 12%)]  Loss: 3.145 (3.64)  Time: 0.803s, 1275.78/s  (0.810s, 1264.01/s)  LR: 8.780e-04  Data: 0.009 (0.023)
Train: 137 [ 200/1251 ( 16%)]  Loss: 3.770 (3.67)  Time: 0.797s, 1284.90/s  (0.807s, 1268.83/s)  LR: 8.780e-04  Data: 0.017 (0.020)
Train: 137 [ 250/1251 ( 20%)]  Loss: 4.027 (3.73)  Time: 0.791s, 1294.11/s  (0.807s, 1269.49/s)  LR: 8.780e-04  Data: 0.013 (0.018)
Train: 137 [ 300/1251 ( 24%)]  Loss: 3.924 (3.75)  Time: 0.789s, 1297.37/s  (0.804s, 1273.03/s)  LR: 8.780e-04  Data: 0.010 (0.017)
Train: 137 [ 350/1251 ( 28%)]  Loss: 3.785 (3.76)  Time: 0.772s, 1326.36/s  (0.803s, 1274.90/s)  LR: 8.780e-04  Data: 0.010 (0.016)
Train: 137 [ 400/1251 ( 32%)]  Loss: 3.703 (3.75)  Time: 0.811s, 1262.36/s  (0.803s, 1275.55/s)  LR: 8.780e-04  Data: 0.010 (0.016)
Train: 137 [ 450/1251 ( 36%)]  Loss: 3.621 (3.74)  Time: 0.772s, 1325.79/s  (0.803s, 1275.01/s)  LR: 8.780e-04  Data: 0.010 (0.015)
Train: 137 [ 500/1251 ( 40%)]  Loss: 3.619 (3.73)  Time: 0.817s, 1253.27/s  (0.803s, 1275.40/s)  LR: 8.780e-04  Data: 0.009 (0.015)
Train: 137 [ 550/1251 ( 44%)]  Loss: 3.580 (3.72)  Time: 0.835s, 1226.85/s  (0.803s, 1275.22/s)  LR: 8.780e-04  Data: 0.010 (0.014)
Train: 137 [ 600/1251 ( 48%)]  Loss: 3.671 (3.71)  Time: 0.851s, 1203.80/s  (0.802s, 1276.09/s)  LR: 8.780e-04  Data: 0.010 (0.014)
Train: 137 [ 650/1251 ( 52%)]  Loss: 3.668 (3.71)  Time: 0.806s, 1270.51/s  (0.802s, 1276.34/s)  LR: 8.780e-04  Data: 0.010 (0.014)
Train: 137 [ 700/1251 ( 56%)]  Loss: 3.819 (3.72)  Time: 0.841s, 1218.18/s  (0.802s, 1276.53/s)  LR: 8.780e-04  Data: 0.010 (0.014)
Train: 137 [ 750/1251 ( 60%)]  Loss: 3.653 (3.71)  Time: 0.802s, 1276.86/s  (0.802s, 1276.82/s)  LR: 8.780e-04  Data: 0.010 (0.013)
Train: 137 [ 800/1251 ( 64%)]  Loss: 3.638 (3.71)  Time: 0.775s, 1321.57/s  (0.802s, 1277.14/s)  LR: 8.780e-04  Data: 0.010 (0.013)
Train: 137 [ 850/1251 ( 68%)]  Loss: 3.790 (3.71)  Time: 0.834s, 1227.13/s  (0.802s, 1276.83/s)  LR: 8.780e-04  Data: 0.017 (0.013)
Train: 137 [ 900/1251 ( 72%)]  Loss: 3.595 (3.71)  Time: 0.920s, 1113.45/s  (0.802s, 1276.52/s)  LR: 8.780e-04  Data: 0.010 (0.013)
Train: 137 [ 950/1251 ( 76%)]  Loss: 3.503 (3.70)  Time: 0.805s, 1271.81/s  (0.802s, 1276.76/s)  LR: 8.780e-04  Data: 0.017 (0.013)
Train: 137 [1000/1251 ( 80%)]  Loss: 3.598 (3.69)  Time: 0.842s, 1216.42/s  (0.802s, 1276.83/s)  LR: 8.780e-04  Data: 0.010 (0.013)
Train: 137 [1050/1251 ( 84%)]  Loss: 3.647 (3.69)  Time: 0.771s, 1328.15/s  (0.802s, 1277.22/s)  LR: 8.780e-04  Data: 0.010 (0.013)
Train: 137 [1100/1251 ( 88%)]  Loss: 3.319 (3.67)  Time: 0.779s, 1314.81/s  (0.802s, 1277.26/s)  LR: 8.780e-04  Data: 0.010 (0.013)
Train: 137 [1150/1251 ( 92%)]  Loss: 3.602 (3.67)  Time: 0.779s, 1315.15/s  (0.801s, 1277.63/s)  LR: 8.780e-04  Data: 0.010 (0.013)
Train: 137 [1200/1251 ( 96%)]  Loss: 3.533 (3.67)  Time: 0.773s, 1324.06/s  (0.801s, 1277.80/s)  LR: 8.780e-04  Data: 0.010 (0.013)
Train: 137 [1250/1251 (100%)]  Loss: 3.858 (3.67)  Time: 0.770s, 1330.22/s  (0.802s, 1277.57/s)  LR: 8.780e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.645 (1.645)  Loss:  0.9316 (0.9316)  Acc@1: 87.9883 (87.9883)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.194 (0.611)  Loss:  0.9468 (1.5074)  Acc@1: 83.0189 (71.7740)  Acc@5: 96.1085 (91.0560)
Train: 138 [   0/1251 (  0%)]  Loss: 3.790 (3.79)  Time: 2.509s,  408.21/s  (2.509s,  408.21/s)  LR: 8.763e-04  Data: 1.773 (1.773)
Train: 138 [  50/1251 (  4%)]  Loss: 3.426 (3.61)  Time: 0.808s, 1266.69/s  (0.847s, 1208.93/s)  LR: 8.763e-04  Data: 0.015 (0.054)
Train: 138 [ 100/1251 (  8%)]  Loss: 4.141 (3.79)  Time: 0.829s, 1234.76/s  (0.825s, 1241.44/s)  LR: 8.763e-04  Data: 0.010 (0.033)
Train: 138 [ 150/1251 ( 12%)]  Loss: 3.966 (3.83)  Time: 0.828s, 1236.47/s  (0.816s, 1255.18/s)  LR: 8.763e-04  Data: 0.009 (0.025)
Train: 138 [ 200/1251 ( 16%)]  Loss: 3.669 (3.80)  Time: 0.789s, 1298.25/s  (0.812s, 1260.70/s)  LR: 8.763e-04  Data: 0.019 (0.022)
Train: 138 [ 250/1251 ( 20%)]  Loss: 3.470 (3.74)  Time: 0.773s, 1324.40/s  (0.810s, 1263.62/s)  LR: 8.763e-04  Data: 0.010 (0.020)
Train: 138 [ 300/1251 ( 24%)]  Loss: 3.866 (3.76)  Time: 0.841s, 1217.59/s  (0.808s, 1267.24/s)  LR: 8.763e-04  Data: 0.015 (0.018)
Train: 138 [ 350/1251 ( 28%)]  Loss: 3.961 (3.79)  Time: 0.805s, 1272.36/s  (0.808s, 1267.69/s)  LR: 8.763e-04  Data: 0.018 (0.017)
Train: 138 [ 400/1251 ( 32%)]  Loss: 3.873 (3.80)  Time: 0.816s, 1255.61/s  (0.807s, 1268.52/s)  LR: 8.763e-04  Data: 0.010 (0.017)
Train: 138 [ 450/1251 ( 36%)]  Loss: 3.859 (3.80)  Time: 0.788s, 1299.24/s  (0.806s, 1269.99/s)  LR: 8.763e-04  Data: 0.014 (0.016)
Train: 138 [ 500/1251 ( 40%)]  Loss: 3.729 (3.80)  Time: 0.839s, 1220.96/s  (0.806s, 1270.61/s)  LR: 8.763e-04  Data: 0.011 (0.015)
Train: 138 [ 550/1251 ( 44%)]  Loss: 3.773 (3.79)  Time: 0.815s, 1256.98/s  (0.805s, 1271.67/s)  LR: 8.763e-04  Data: 0.010 (0.015)
Train: 138 [ 600/1251 ( 48%)]  Loss: 3.801 (3.79)  Time: 0.815s, 1257.12/s  (0.805s, 1271.84/s)  LR: 8.763e-04  Data: 0.010 (0.015)
Train: 138 [ 650/1251 ( 52%)]  Loss: 3.875 (3.80)  Time: 0.801s, 1277.92/s  (0.804s, 1272.85/s)  LR: 8.763e-04  Data: 0.012 (0.014)
Train: 138 [ 700/1251 ( 56%)]  Loss: 3.748 (3.80)  Time: 0.773s, 1323.95/s  (0.804s, 1273.87/s)  LR: 8.763e-04  Data: 0.010 (0.014)
Train: 138 [ 750/1251 ( 60%)]  Loss: 3.700 (3.79)  Time: 0.773s, 1324.61/s  (0.804s, 1273.70/s)  LR: 8.763e-04  Data: 0.010 (0.014)
Train: 138 [ 800/1251 ( 64%)]  Loss: 4.015 (3.80)  Time: 0.817s, 1252.69/s  (0.804s, 1273.50/s)  LR: 8.763e-04  Data: 0.013 (0.014)
Train: 138 [ 850/1251 ( 68%)]  Loss: 3.730 (3.80)  Time: 0.774s, 1323.27/s  (0.804s, 1274.03/s)  LR: 8.763e-04  Data: 0.010 (0.014)
Train: 138 [ 900/1251 ( 72%)]  Loss: 3.534 (3.79)  Time: 0.814s, 1257.27/s  (0.804s, 1274.00/s)  LR: 8.763e-04  Data: 0.010 (0.013)
Train: 138 [ 950/1251 ( 76%)]  Loss: 3.490 (3.77)  Time: 0.826s, 1239.97/s  (0.804s, 1273.71/s)  LR: 8.763e-04  Data: 0.010 (0.013)
Train: 138 [1000/1251 ( 80%)]  Loss: 3.965 (3.78)  Time: 0.813s, 1259.98/s  (0.804s, 1274.20/s)  LR: 8.763e-04  Data: 0.010 (0.013)
Train: 138 [1050/1251 ( 84%)]  Loss: 3.996 (3.79)  Time: 0.817s, 1254.03/s  (0.803s, 1274.71/s)  LR: 8.763e-04  Data: 0.011 (0.013)
Train: 138 [1100/1251 ( 88%)]  Loss: 3.704 (3.79)  Time: 0.815s, 1257.14/s  (0.804s, 1274.28/s)  LR: 8.763e-04  Data: 0.017 (0.013)
Train: 138 [1150/1251 ( 92%)]  Loss: 3.738 (3.78)  Time: 0.822s, 1245.36/s  (0.803s, 1274.53/s)  LR: 8.763e-04  Data: 0.014 (0.013)
Train: 138 [1200/1251 ( 96%)]  Loss: 3.837 (3.79)  Time: 0.776s, 1319.93/s  (0.803s, 1274.64/s)  LR: 8.763e-04  Data: 0.009 (0.013)
Train: 138 [1250/1251 (100%)]  Loss: 3.620 (3.78)  Time: 0.767s, 1335.76/s  (0.803s, 1275.11/s)  LR: 8.763e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.623 (1.623)  Loss:  0.7139 (0.7139)  Acc@1: 88.3789 (88.3789)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.194 (0.596)  Loss:  0.8179 (1.3700)  Acc@1: 85.3774 (72.7280)  Acc@5: 96.9340 (91.4320)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-138.pth.tar', 72.72799998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-132.pth.tar', 72.52000001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-133.pth.tar', 72.51400014648438)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-130.pth.tar', 72.46799991699218)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-120.pth.tar', 72.4560001171875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-114.pth.tar', 72.40200011962891)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-134.pth.tar', 72.37400004394532)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-126.pth.tar', 72.22800001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-103.pth.tar', 72.05000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-123.pth.tar', 72.01000002197266)

Train: 139 [   0/1251 (  0%)]  Loss: 3.288 (3.29)  Time: 2.402s,  426.38/s  (2.402s,  426.38/s)  LR: 8.746e-04  Data: 1.668 (1.668)
Train: 139 [  50/1251 (  4%)]  Loss: 3.713 (3.50)  Time: 0.833s, 1229.46/s  (0.835s, 1226.79/s)  LR: 8.746e-04  Data: 0.009 (0.048)
Train: 139 [ 100/1251 (  8%)]  Loss: 3.571 (3.52)  Time: 0.813s, 1259.07/s  (0.820s, 1248.20/s)  LR: 8.746e-04  Data: 0.015 (0.030)
Train: 139 [ 150/1251 ( 12%)]  Loss: 3.542 (3.53)  Time: 0.773s, 1324.59/s  (0.814s, 1258.11/s)  LR: 8.746e-04  Data: 0.011 (0.024)
Train: 139 [ 200/1251 ( 16%)]  Loss: 3.708 (3.56)  Time: 0.800s, 1280.43/s  (0.813s, 1259.60/s)  LR: 8.746e-04  Data: 0.010 (0.021)
Train: 139 [ 250/1251 ( 20%)]  Loss: 3.879 (3.62)  Time: 0.772s, 1325.93/s  (0.810s, 1264.41/s)  LR: 8.746e-04  Data: 0.011 (0.019)
Train: 139 [ 300/1251 ( 24%)]  Loss: 3.672 (3.62)  Time: 0.789s, 1298.11/s  (0.809s, 1265.83/s)  LR: 8.746e-04  Data: 0.011 (0.018)
Train: 139 [ 350/1251 ( 28%)]  Loss: 3.827 (3.65)  Time: 0.823s, 1243.66/s  (0.808s, 1266.97/s)  LR: 8.746e-04  Data: 0.010 (0.017)
Train: 139 [ 400/1251 ( 32%)]  Loss: 3.408 (3.62)  Time: 0.778s, 1316.30/s  (0.807s, 1269.05/s)  LR: 8.746e-04  Data: 0.013 (0.016)
Train: 139 [ 450/1251 ( 36%)]  Loss: 3.571 (3.62)  Time: 0.775s, 1320.57/s  (0.806s, 1270.29/s)  LR: 8.746e-04  Data: 0.010 (0.015)
Train: 139 [ 500/1251 ( 40%)]  Loss: 3.980 (3.65)  Time: 0.777s, 1317.10/s  (0.805s, 1271.30/s)  LR: 8.746e-04  Data: 0.010 (0.015)
Train: 139 [ 550/1251 ( 44%)]  Loss: 3.448 (3.63)  Time: 0.773s, 1325.09/s  (0.805s, 1272.56/s)  LR: 8.746e-04  Data: 0.010 (0.015)
Train: 139 [ 600/1251 ( 48%)]  Loss: 3.690 (3.64)  Time: 0.774s, 1322.93/s  (0.805s, 1272.66/s)  LR: 8.746e-04  Data: 0.011 (0.014)
Train: 139 [ 650/1251 ( 52%)]  Loss: 3.551 (3.63)  Time: 0.797s, 1285.48/s  (0.804s, 1273.59/s)  LR: 8.746e-04  Data: 0.013 (0.014)
Train: 139 [ 700/1251 ( 56%)]  Loss: 3.566 (3.63)  Time: 0.845s, 1212.09/s  (0.804s, 1274.37/s)  LR: 8.746e-04  Data: 0.015 (0.014)
Train: 139 [ 750/1251 ( 60%)]  Loss: 3.791 (3.64)  Time: 0.786s, 1302.64/s  (0.803s, 1274.58/s)  LR: 8.746e-04  Data: 0.013 (0.014)
Train: 139 [ 800/1251 ( 64%)]  Loss: 3.406 (3.62)  Time: 0.772s, 1326.36/s  (0.803s, 1275.26/s)  LR: 8.746e-04  Data: 0.010 (0.014)
Train: 139 [ 850/1251 ( 68%)]  Loss: 3.663 (3.63)  Time: 0.773s, 1325.50/s  (0.803s, 1275.50/s)  LR: 8.746e-04  Data: 0.010 (0.013)
Train: 139 [ 900/1251 ( 72%)]  Loss: 4.134 (3.65)  Time: 0.797s, 1284.53/s  (0.803s, 1275.07/s)  LR: 8.746e-04  Data: 0.013 (0.013)
Train: 139 [ 950/1251 ( 76%)]  Loss: 3.657 (3.65)  Time: 0.775s, 1321.59/s  (0.803s, 1274.45/s)  LR: 8.746e-04  Data: 0.011 (0.013)
Train: 139 [1000/1251 ( 80%)]  Loss: 3.684 (3.65)  Time: 0.802s, 1276.43/s  (0.804s, 1274.13/s)  LR: 8.746e-04  Data: 0.012 (0.013)
Train: 139 [1050/1251 ( 84%)]  Loss: 3.192 (3.63)  Time: 0.774s, 1323.29/s  (0.804s, 1273.09/s)  LR: 8.746e-04  Data: 0.011 (0.013)
Train: 139 [1100/1251 ( 88%)]  Loss: 3.690 (3.64)  Time: 0.779s, 1313.93/s  (0.803s, 1274.66/s)  LR: 8.746e-04  Data: 0.010 (0.013)
Train: 139 [1150/1251 ( 92%)]  Loss: 3.858 (3.65)  Time: 0.811s, 1262.19/s  (0.803s, 1275.42/s)  LR: 8.746e-04  Data: 0.013 (0.013)
Train: 139 [1200/1251 ( 96%)]  Loss: 3.891 (3.66)  Time: 0.773s, 1324.79/s  (0.803s, 1275.88/s)  LR: 8.746e-04  Data: 0.010 (0.013)
Train: 139 [1250/1251 (100%)]  Loss: 3.973 (3.67)  Time: 0.771s, 1328.99/s  (0.803s, 1275.87/s)  LR: 8.746e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.697 (1.697)  Loss:  0.8413 (0.8413)  Acc@1: 87.4023 (87.4023)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.193 (0.605)  Loss:  0.9902 (1.4076)  Acc@1: 84.0802 (72.5220)  Acc@5: 95.1651 (91.4560)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-138.pth.tar', 72.72799998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-139.pth.tar', 72.52199993652344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-132.pth.tar', 72.52000001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-133.pth.tar', 72.51400014648438)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-130.pth.tar', 72.46799991699218)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-120.pth.tar', 72.4560001171875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-114.pth.tar', 72.40200011962891)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-134.pth.tar', 72.37400004394532)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-126.pth.tar', 72.22800001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-103.pth.tar', 72.05000007080078)

Train: 140 [   0/1251 (  0%)]  Loss: 3.800 (3.80)  Time: 2.481s,  412.68/s  (2.481s,  412.68/s)  LR: 8.729e-04  Data: 1.728 (1.728)
Train: 140 [  50/1251 (  4%)]  Loss: 3.709 (3.75)  Time: 0.773s, 1325.45/s  (0.845s, 1212.28/s)  LR: 8.729e-04  Data: 0.010 (0.052)
Train: 140 [ 100/1251 (  8%)]  Loss: 3.206 (3.57)  Time: 0.788s, 1299.57/s  (0.823s, 1244.84/s)  LR: 8.729e-04  Data: 0.014 (0.032)
Train: 140 [ 150/1251 ( 12%)]  Loss: 3.470 (3.55)  Time: 0.777s, 1318.13/s  (0.816s, 1255.09/s)  LR: 8.729e-04  Data: 0.010 (0.025)
Train: 140 [ 200/1251 ( 16%)]  Loss: 3.335 (3.50)  Time: 0.773s, 1324.09/s  (0.811s, 1261.93/s)  LR: 8.729e-04  Data: 0.010 (0.021)
Train: 140 [ 250/1251 ( 20%)]  Loss: 3.495 (3.50)  Time: 0.799s, 1281.24/s  (0.810s, 1263.82/s)  LR: 8.729e-04  Data: 0.011 (0.019)
Train: 140 [ 300/1251 ( 24%)]  Loss: 3.698 (3.53)  Time: 0.774s, 1322.82/s  (0.808s, 1267.48/s)  LR: 8.729e-04  Data: 0.010 (0.018)
Train: 140 [ 350/1251 ( 28%)]  Loss: 3.538 (3.53)  Time: 0.856s, 1196.32/s  (0.808s, 1266.68/s)  LR: 8.729e-04  Data: 0.010 (0.017)
Train: 140 [ 400/1251 ( 32%)]  Loss: 4.042 (3.59)  Time: 0.792s, 1292.80/s  (0.808s, 1267.38/s)  LR: 8.729e-04  Data: 0.011 (0.016)
Train: 140 [ 450/1251 ( 36%)]  Loss: 3.946 (3.62)  Time: 0.824s, 1242.68/s  (0.807s, 1269.17/s)  LR: 8.729e-04  Data: 0.009 (0.016)
Train: 140 [ 500/1251 ( 40%)]  Loss: 3.489 (3.61)  Time: 0.789s, 1297.40/s  (0.806s, 1269.94/s)  LR: 8.729e-04  Data: 0.009 (0.015)
Train: 140 [ 550/1251 ( 44%)]  Loss: 3.471 (3.60)  Time: 0.828s, 1236.29/s  (0.806s, 1271.00/s)  LR: 8.729e-04  Data: 0.014 (0.015)
Train: 140 [ 600/1251 ( 48%)]  Loss: 3.655 (3.60)  Time: 0.784s, 1306.29/s  (0.805s, 1271.84/s)  LR: 8.729e-04  Data: 0.012 (0.014)
Train: 140 [ 650/1251 ( 52%)]  Loss: 3.474 (3.59)  Time: 0.774s, 1322.47/s  (0.805s, 1272.53/s)  LR: 8.729e-04  Data: 0.010 (0.014)
Train: 140 [ 700/1251 ( 56%)]  Loss: 3.645 (3.60)  Time: 0.782s, 1308.86/s  (0.804s, 1272.88/s)  LR: 8.729e-04  Data: 0.013 (0.014)
Train: 140 [ 750/1251 ( 60%)]  Loss: 3.692 (3.60)  Time: 0.809s, 1266.26/s  (0.804s, 1273.26/s)  LR: 8.729e-04  Data: 0.014 (0.014)
Train: 140 [ 800/1251 ( 64%)]  Loss: 3.832 (3.62)  Time: 0.787s, 1301.62/s  (0.804s, 1273.66/s)  LR: 8.729e-04  Data: 0.016 (0.014)
Train: 140 [ 850/1251 ( 68%)]  Loss: 3.418 (3.61)  Time: 0.775s, 1321.70/s  (0.804s, 1274.22/s)  LR: 8.729e-04  Data: 0.010 (0.013)
Train: 140 [ 900/1251 ( 72%)]  Loss: 3.357 (3.59)  Time: 0.783s, 1308.00/s  (0.803s, 1274.66/s)  LR: 8.729e-04  Data: 0.010 (0.013)
Train: 140 [ 950/1251 ( 76%)]  Loss: 3.295 (3.58)  Time: 0.856s, 1195.77/s  (0.804s, 1274.37/s)  LR: 8.729e-04  Data: 0.012 (0.013)
Train: 140 [1000/1251 ( 80%)]  Loss: 3.669 (3.58)  Time: 0.845s, 1211.33/s  (0.803s, 1274.84/s)  LR: 8.729e-04  Data: 0.010 (0.013)
Train: 140 [1050/1251 ( 84%)]  Loss: 3.699 (3.59)  Time: 0.827s, 1237.73/s  (0.803s, 1275.01/s)  LR: 8.729e-04  Data: 0.011 (0.013)
Train: 140 [1100/1251 ( 88%)]  Loss: 4.016 (3.61)  Time: 0.785s, 1304.19/s  (0.803s, 1275.09/s)  LR: 8.729e-04  Data: 0.015 (0.013)
Train: 140 [1150/1251 ( 92%)]  Loss: 3.528 (3.60)  Time: 0.775s, 1320.68/s  (0.803s, 1275.16/s)  LR: 8.729e-04  Data: 0.011 (0.013)
Train: 140 [1200/1251 ( 96%)]  Loss: 3.651 (3.61)  Time: 0.784s, 1305.68/s  (0.803s, 1275.23/s)  LR: 8.729e-04  Data: 0.014 (0.013)
Train: 140 [1250/1251 (100%)]  Loss: 3.388 (3.60)  Time: 0.760s, 1347.75/s  (0.803s, 1275.38/s)  LR: 8.729e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.710 (1.710)  Loss:  0.9019 (0.9019)  Acc@1: 88.5742 (88.5742)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.194 (0.600)  Loss:  1.0273 (1.4302)  Acc@1: 83.4906 (72.3300)  Acc@5: 94.4576 (91.3780)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-138.pth.tar', 72.72799998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-139.pth.tar', 72.52199993652344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-132.pth.tar', 72.52000001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-133.pth.tar', 72.51400014648438)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-130.pth.tar', 72.46799991699218)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-120.pth.tar', 72.4560001171875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-114.pth.tar', 72.40200011962891)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-134.pth.tar', 72.37400004394532)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-140.pth.tar', 72.32999993896485)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-126.pth.tar', 72.22800001953125)

Train: 141 [   0/1251 (  0%)]  Loss: 3.826 (3.83)  Time: 2.374s,  431.26/s  (2.374s,  431.26/s)  LR: 8.711e-04  Data: 1.639 (1.639)
Train: 141 [  50/1251 (  4%)]  Loss: 3.706 (3.77)  Time: 0.798s, 1282.71/s  (0.836s, 1224.79/s)  LR: 8.711e-04  Data: 0.012 (0.053)
Train: 141 [ 100/1251 (  8%)]  Loss: 3.854 (3.80)  Time: 0.813s, 1260.27/s  (0.816s, 1255.13/s)  LR: 8.711e-04  Data: 0.010 (0.032)
Train: 141 [ 150/1251 ( 12%)]  Loss: 3.757 (3.79)  Time: 0.821s, 1247.77/s  (0.813s, 1259.36/s)  LR: 8.711e-04  Data: 0.010 (0.025)
Train: 141 [ 200/1251 ( 16%)]  Loss: 3.513 (3.73)  Time: 0.778s, 1315.83/s  (0.810s, 1263.95/s)  LR: 8.711e-04  Data: 0.010 (0.022)
Train: 141 [ 250/1251 ( 20%)]  Loss: 3.730 (3.73)  Time: 0.787s, 1300.45/s  (0.808s, 1267.31/s)  LR: 8.711e-04  Data: 0.013 (0.020)
Train: 141 [ 300/1251 ( 24%)]  Loss: 3.788 (3.74)  Time: 0.773s, 1324.35/s  (0.806s, 1270.09/s)  LR: 8.711e-04  Data: 0.010 (0.018)
Train: 141 [ 350/1251 ( 28%)]  Loss: 3.577 (3.72)  Time: 0.865s, 1183.72/s  (0.806s, 1270.38/s)  LR: 8.711e-04  Data: 0.012 (0.017)
Train: 141 [ 400/1251 ( 32%)]  Loss: 3.387 (3.68)  Time: 0.775s, 1321.42/s  (0.806s, 1270.86/s)  LR: 8.711e-04  Data: 0.010 (0.016)
Train: 141 [ 450/1251 ( 36%)]  Loss: 3.793 (3.69)  Time: 0.795s, 1287.52/s  (0.805s, 1272.65/s)  LR: 8.711e-04  Data: 0.013 (0.016)
Train: 141 [ 500/1251 ( 40%)]  Loss: 3.437 (3.67)  Time: 0.799s, 1282.00/s  (0.805s, 1272.40/s)  LR: 8.711e-04  Data: 0.010 (0.015)
Train: 141 [ 550/1251 ( 44%)]  Loss: 3.713 (3.67)  Time: 0.820s, 1249.47/s  (0.804s, 1272.88/s)  LR: 8.711e-04  Data: 0.010 (0.015)
Train: 141 [ 600/1251 ( 48%)]  Loss: 3.701 (3.68)  Time: 0.795s, 1288.46/s  (0.804s, 1273.24/s)  LR: 8.711e-04  Data: 0.015 (0.015)
Train: 141 [ 650/1251 ( 52%)]  Loss: 3.762 (3.68)  Time: 0.798s, 1283.85/s  (0.804s, 1273.36/s)  LR: 8.711e-04  Data: 0.009 (0.014)
Train: 141 [ 700/1251 ( 56%)]  Loss: 3.703 (3.68)  Time: 0.820s, 1248.67/s  (0.804s, 1273.61/s)  LR: 8.711e-04  Data: 0.011 (0.014)
Train: 141 [ 750/1251 ( 60%)]  Loss: 3.832 (3.69)  Time: 0.804s, 1274.08/s  (0.804s, 1273.51/s)  LR: 8.711e-04  Data: 0.010 (0.014)
Train: 141 [ 800/1251 ( 64%)]  Loss: 3.762 (3.70)  Time: 0.795s, 1288.07/s  (0.804s, 1273.21/s)  LR: 8.711e-04  Data: 0.009 (0.014)
Train: 141 [ 850/1251 ( 68%)]  Loss: 3.428 (3.68)  Time: 0.785s, 1303.86/s  (0.804s, 1272.94/s)  LR: 8.711e-04  Data: 0.010 (0.014)
Train: 141 [ 900/1251 ( 72%)]  Loss: 3.711 (3.68)  Time: 0.912s, 1123.01/s  (0.804s, 1273.00/s)  LR: 8.711e-04  Data: 0.015 (0.013)
Train: 141 [ 950/1251 ( 76%)]  Loss: 3.871 (3.69)  Time: 0.773s, 1324.12/s  (0.804s, 1273.15/s)  LR: 8.711e-04  Data: 0.010 (0.013)
Train: 141 [1000/1251 ( 80%)]  Loss: 3.986 (3.71)  Time: 0.854s, 1199.05/s  (0.804s, 1273.71/s)  LR: 8.711e-04  Data: 0.010 (0.013)
Train: 141 [1050/1251 ( 84%)]  Loss: 3.734 (3.71)  Time: 0.773s, 1324.13/s  (0.804s, 1274.27/s)  LR: 8.711e-04  Data: 0.010 (0.013)
Train: 141 [1100/1251 ( 88%)]  Loss: 3.851 (3.71)  Time: 0.771s, 1328.99/s  (0.803s, 1275.22/s)  LR: 8.711e-04  Data: 0.010 (0.013)
Train: 141 [1150/1251 ( 92%)]  Loss: 3.579 (3.71)  Time: 0.778s, 1315.53/s  (0.803s, 1275.50/s)  LR: 8.711e-04  Data: 0.010 (0.013)
Train: 141 [1200/1251 ( 96%)]  Loss: 3.259 (3.69)  Time: 0.785s, 1304.28/s  (0.803s, 1275.61/s)  LR: 8.711e-04  Data: 0.010 (0.013)
Train: 141 [1250/1251 (100%)]  Loss: 3.658 (3.69)  Time: 0.763s, 1342.19/s  (0.803s, 1275.61/s)  LR: 8.711e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.705 (1.705)  Loss:  0.8271 (0.8271)  Acc@1: 88.2812 (88.2812)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.194 (0.602)  Loss:  0.8345 (1.3817)  Acc@1: 84.7877 (72.1840)  Acc@5: 96.5802 (91.1280)
Train: 142 [   0/1251 (  0%)]  Loss: 3.447 (3.45)  Time: 2.480s,  412.91/s  (2.480s,  412.91/s)  LR: 8.694e-04  Data: 1.688 (1.688)
Train: 142 [  50/1251 (  4%)]  Loss: 3.688 (3.57)  Time: 0.784s, 1306.27/s  (0.842s, 1216.40/s)  LR: 8.694e-04  Data: 0.013 (0.049)
Train: 142 [ 100/1251 (  8%)]  Loss: 3.946 (3.69)  Time: 0.860s, 1190.15/s  (0.822s, 1245.37/s)  LR: 8.694e-04  Data: 0.010 (0.030)
Train: 142 [ 150/1251 ( 12%)]  Loss: 3.651 (3.68)  Time: 0.774s, 1323.80/s  (0.816s, 1254.88/s)  LR: 8.694e-04  Data: 0.010 (0.024)
Train: 142 [ 200/1251 ( 16%)]  Loss: 3.873 (3.72)  Time: 0.807s, 1268.67/s  (0.813s, 1259.83/s)  LR: 8.694e-04  Data: 0.010 (0.021)
Train: 142 [ 250/1251 ( 20%)]  Loss: 3.531 (3.69)  Time: 0.819s, 1250.11/s  (0.809s, 1265.74/s)  LR: 8.694e-04  Data: 0.010 (0.019)
Train: 142 [ 300/1251 ( 24%)]  Loss: 3.680 (3.69)  Time: 0.774s, 1322.92/s  (0.807s, 1269.68/s)  LR: 8.694e-04  Data: 0.009 (0.017)
Train: 142 [ 350/1251 ( 28%)]  Loss: 3.945 (3.72)  Time: 0.814s, 1258.32/s  (0.807s, 1269.03/s)  LR: 8.694e-04  Data: 0.014 (0.017)
Train: 142 [ 400/1251 ( 32%)]  Loss: 3.882 (3.74)  Time: 0.784s, 1306.82/s  (0.806s, 1269.96/s)  LR: 8.694e-04  Data: 0.013 (0.016)
Train: 142 [ 450/1251 ( 36%)]  Loss: 3.489 (3.71)  Time: 0.780s, 1312.39/s  (0.806s, 1271.15/s)  LR: 8.694e-04  Data: 0.010 (0.015)
Train: 142 [ 500/1251 ( 40%)]  Loss: 3.688 (3.71)  Time: 0.801s, 1278.33/s  (0.805s, 1271.85/s)  LR: 8.694e-04  Data: 0.011 (0.015)
Train: 142 [ 550/1251 ( 44%)]  Loss: 3.603 (3.70)  Time: 0.780s, 1313.56/s  (0.805s, 1272.67/s)  LR: 8.694e-04  Data: 0.014 (0.015)
Train: 142 [ 600/1251 ( 48%)]  Loss: 3.227 (3.67)  Time: 0.798s, 1282.92/s  (0.805s, 1272.72/s)  LR: 8.694e-04  Data: 0.010 (0.014)
Train: 142 [ 650/1251 ( 52%)]  Loss: 3.719 (3.67)  Time: 0.787s, 1300.66/s  (0.804s, 1273.20/s)  LR: 8.694e-04  Data: 0.010 (0.014)
Train: 142 [ 700/1251 ( 56%)]  Loss: 3.637 (3.67)  Time: 0.779s, 1314.67/s  (0.804s, 1273.53/s)  LR: 8.694e-04  Data: 0.011 (0.014)
Train: 142 [ 750/1251 ( 60%)]  Loss: 3.327 (3.65)  Time: 0.785s, 1304.38/s  (0.804s, 1273.55/s)  LR: 8.694e-04  Data: 0.010 (0.014)
Train: 142 [ 800/1251 ( 64%)]  Loss: 3.609 (3.64)  Time: 0.807s, 1268.20/s  (0.804s, 1273.84/s)  LR: 8.694e-04  Data: 0.010 (0.013)
Train: 142 [ 850/1251 ( 68%)]  Loss: 3.353 (3.63)  Time: 0.805s, 1271.59/s  (0.804s, 1273.91/s)  LR: 8.694e-04  Data: 0.010 (0.013)
Train: 142 [ 900/1251 ( 72%)]  Loss: 3.526 (3.62)  Time: 0.805s, 1271.45/s  (0.803s, 1274.49/s)  LR: 8.694e-04  Data: 0.009 (0.013)
Train: 142 [ 950/1251 ( 76%)]  Loss: 3.706 (3.63)  Time: 0.813s, 1258.90/s  (0.803s, 1274.96/s)  LR: 8.694e-04  Data: 0.013 (0.013)
Train: 142 [1000/1251 ( 80%)]  Loss: 3.693 (3.63)  Time: 0.773s, 1325.34/s  (0.803s, 1274.78/s)  LR: 8.694e-04  Data: 0.011 (0.013)
Train: 142 [1050/1251 ( 84%)]  Loss: 3.469 (3.62)  Time: 0.772s, 1326.00/s  (0.803s, 1274.74/s)  LR: 8.694e-04  Data: 0.010 (0.013)
Train: 142 [1100/1251 ( 88%)]  Loss: 3.675 (3.62)  Time: 0.826s, 1239.71/s  (0.804s, 1274.16/s)  LR: 8.694e-04  Data: 0.014 (0.013)
Train: 142 [1150/1251 ( 92%)]  Loss: 3.495 (3.62)  Time: 0.793s, 1291.23/s  (0.803s, 1274.51/s)  LR: 8.694e-04  Data: 0.009 (0.013)
Train: 142 [1200/1251 ( 96%)]  Loss: 3.262 (3.60)  Time: 0.778s, 1316.50/s  (0.803s, 1274.53/s)  LR: 8.694e-04  Data: 0.010 (0.013)
Train: 142 [1250/1251 (100%)]  Loss: 3.783 (3.61)  Time: 0.799s, 1280.88/s  (0.803s, 1274.80/s)  LR: 8.694e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.676 (1.676)  Loss:  0.7524 (0.7524)  Acc@1: 88.2812 (88.2812)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.194 (0.602)  Loss:  0.8428 (1.3815)  Acc@1: 83.6085 (72.1280)  Acc@5: 95.5189 (91.2600)
Train: 143 [   0/1251 (  0%)]  Loss: 3.589 (3.59)  Time: 2.371s,  431.83/s  (2.371s,  431.83/s)  LR: 8.676e-04  Data: 1.641 (1.641)
Train: 143 [  50/1251 (  4%)]  Loss: 3.654 (3.62)  Time: 0.775s, 1321.76/s  (0.836s, 1224.91/s)  LR: 8.676e-04  Data: 0.009 (0.046)
Train: 143 [ 100/1251 (  8%)]  Loss: 3.353 (3.53)  Time: 0.857s, 1194.76/s  (0.815s, 1256.18/s)  LR: 8.676e-04  Data: 0.009 (0.028)
Train: 143 [ 150/1251 ( 12%)]  Loss: 3.874 (3.62)  Time: 0.776s, 1319.30/s  (0.810s, 1263.50/s)  LR: 8.676e-04  Data: 0.010 (0.023)
Train: 143 [ 200/1251 ( 16%)]  Loss: 3.651 (3.62)  Time: 0.783s, 1307.12/s  (0.811s, 1263.27/s)  LR: 8.676e-04  Data: 0.010 (0.020)
Train: 143 [ 250/1251 ( 20%)]  Loss: 3.790 (3.65)  Time: 0.791s, 1294.82/s  (0.809s, 1265.79/s)  LR: 8.676e-04  Data: 0.014 (0.018)
Train: 143 [ 300/1251 ( 24%)]  Loss: 3.738 (3.66)  Time: 0.801s, 1278.65/s  (0.808s, 1267.66/s)  LR: 8.676e-04  Data: 0.009 (0.017)
Train: 143 [ 350/1251 ( 28%)]  Loss: 4.136 (3.72)  Time: 0.791s, 1295.14/s  (0.807s, 1269.60/s)  LR: 8.676e-04  Data: 0.009 (0.016)
Train: 143 [ 400/1251 ( 32%)]  Loss: 3.676 (3.72)  Time: 0.806s, 1270.62/s  (0.806s, 1271.04/s)  LR: 8.676e-04  Data: 0.010 (0.015)
Train: 143 [ 450/1251 ( 36%)]  Loss: 3.622 (3.71)  Time: 0.773s, 1324.24/s  (0.805s, 1272.10/s)  LR: 8.676e-04  Data: 0.010 (0.015)
Train: 143 [ 500/1251 ( 40%)]  Loss: 3.450 (3.68)  Time: 0.773s, 1325.44/s  (0.805s, 1272.06/s)  LR: 8.676e-04  Data: 0.011 (0.015)
Train: 143 [ 550/1251 ( 44%)]  Loss: 4.114 (3.72)  Time: 0.838s, 1221.53/s  (0.805s, 1271.72/s)  LR: 8.676e-04  Data: 0.010 (0.014)
Train: 143 [ 600/1251 ( 48%)]  Loss: 3.434 (3.70)  Time: 0.787s, 1300.32/s  (0.805s, 1272.10/s)  LR: 8.676e-04  Data: 0.014 (0.014)
Train: 143 [ 650/1251 ( 52%)]  Loss: 3.803 (3.71)  Time: 0.772s, 1325.76/s  (0.804s, 1272.94/s)  LR: 8.676e-04  Data: 0.011 (0.014)
Train: 143 [ 700/1251 ( 56%)]  Loss: 3.860 (3.72)  Time: 0.773s, 1325.37/s  (0.804s, 1274.24/s)  LR: 8.676e-04  Data: 0.010 (0.014)
Train: 143 [ 750/1251 ( 60%)]  Loss: 3.851 (3.72)  Time: 0.771s, 1328.72/s  (0.803s, 1274.48/s)  LR: 8.676e-04  Data: 0.009 (0.013)
Train: 143 [ 800/1251 ( 64%)]  Loss: 3.958 (3.74)  Time: 0.801s, 1278.98/s  (0.803s, 1274.85/s)  LR: 8.676e-04  Data: 0.013 (0.013)
Train: 143 [ 850/1251 ( 68%)]  Loss: 3.648 (3.73)  Time: 0.799s, 1281.54/s  (0.803s, 1275.82/s)  LR: 8.676e-04  Data: 0.010 (0.013)
Train: 143 [ 900/1251 ( 72%)]  Loss: 4.172 (3.76)  Time: 0.814s, 1258.22/s  (0.803s, 1275.76/s)  LR: 8.676e-04  Data: 0.010 (0.013)
Train: 143 [ 950/1251 ( 76%)]  Loss: 3.919 (3.76)  Time: 0.779s, 1314.91/s  (0.803s, 1275.74/s)  LR: 8.676e-04  Data: 0.011 (0.013)
Train: 143 [1000/1251 ( 80%)]  Loss: 3.593 (3.76)  Time: 0.788s, 1299.86/s  (0.802s, 1276.02/s)  LR: 8.676e-04  Data: 0.014 (0.013)
Train: 143 [1050/1251 ( 84%)]  Loss: 3.610 (3.75)  Time: 0.771s, 1327.98/s  (0.802s, 1276.46/s)  LR: 8.676e-04  Data: 0.010 (0.013)
Train: 143 [1100/1251 ( 88%)]  Loss: 3.645 (3.75)  Time: 0.811s, 1263.34/s  (0.802s, 1276.23/s)  LR: 8.676e-04  Data: 0.010 (0.013)
Train: 143 [1150/1251 ( 92%)]  Loss: 3.678 (3.74)  Time: 0.784s, 1305.34/s  (0.802s, 1276.29/s)  LR: 8.676e-04  Data: 0.010 (0.013)
Train: 143 [1200/1251 ( 96%)]  Loss: 3.570 (3.74)  Time: 0.850s, 1205.35/s  (0.802s, 1276.96/s)  LR: 8.676e-04  Data: 0.009 (0.012)
Train: 143 [1250/1251 (100%)]  Loss: 3.736 (3.74)  Time: 0.776s, 1319.58/s  (0.802s, 1276.65/s)  LR: 8.676e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.593 (1.593)  Loss:  0.8701 (0.8701)  Acc@1: 87.5000 (87.5000)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.194 (0.604)  Loss:  1.0146 (1.4250)  Acc@1: 83.4906 (72.0620)  Acc@5: 95.1651 (91.3460)
Train: 144 [   0/1251 (  0%)]  Loss: 3.650 (3.65)  Time: 2.700s,  379.24/s  (2.700s,  379.24/s)  LR: 8.658e-04  Data: 1.968 (1.968)
Train: 144 [  50/1251 (  4%)]  Loss: 4.019 (3.83)  Time: 0.796s, 1286.39/s  (0.842s, 1215.51/s)  LR: 8.658e-04  Data: 0.010 (0.049)
Train: 144 [ 100/1251 (  8%)]  Loss: 3.567 (3.75)  Time: 0.841s, 1216.95/s  (0.824s, 1242.94/s)  LR: 8.658e-04  Data: 0.013 (0.030)
Train: 144 [ 150/1251 ( 12%)]  Loss: 3.627 (3.72)  Time: 0.832s, 1230.11/s  (0.817s, 1253.41/s)  LR: 8.658e-04  Data: 0.010 (0.024)
Train: 144 [ 200/1251 ( 16%)]  Loss: 4.011 (3.77)  Time: 0.773s, 1324.00/s  (0.817s, 1253.94/s)  LR: 8.658e-04  Data: 0.010 (0.021)
Train: 144 [ 250/1251 ( 20%)]  Loss: 3.398 (3.71)  Time: 0.774s, 1323.71/s  (0.814s, 1257.86/s)  LR: 8.658e-04  Data: 0.010 (0.019)
Train: 144 [ 300/1251 ( 24%)]  Loss: 3.611 (3.70)  Time: 0.814s, 1258.07/s  (0.813s, 1260.08/s)  LR: 8.658e-04  Data: 0.010 (0.018)
Train: 144 [ 350/1251 ( 28%)]  Loss: 3.621 (3.69)  Time: 0.781s, 1311.11/s  (0.811s, 1262.43/s)  LR: 8.658e-04  Data: 0.010 (0.017)
Train: 144 [ 400/1251 ( 32%)]  Loss: 3.220 (3.64)  Time: 0.783s, 1308.36/s  (0.811s, 1262.70/s)  LR: 8.658e-04  Data: 0.013 (0.016)
Train: 144 [ 450/1251 ( 36%)]  Loss: 3.601 (3.63)  Time: 0.844s, 1213.92/s  (0.810s, 1264.62/s)  LR: 8.658e-04  Data: 0.009 (0.015)
Train: 144 [ 500/1251 ( 40%)]  Loss: 3.877 (3.65)  Time: 0.774s, 1322.53/s  (0.809s, 1265.83/s)  LR: 8.658e-04  Data: 0.010 (0.015)
Train: 144 [ 550/1251 ( 44%)]  Loss: 3.647 (3.65)  Time: 0.779s, 1314.29/s  (0.808s, 1266.95/s)  LR: 8.658e-04  Data: 0.013 (0.015)
Train: 144 [ 600/1251 ( 48%)]  Loss: 3.434 (3.64)  Time: 0.843s, 1215.40/s  (0.808s, 1267.93/s)  LR: 8.658e-04  Data: 0.010 (0.014)
Train: 144 [ 650/1251 ( 52%)]  Loss: 3.839 (3.65)  Time: 0.821s, 1246.72/s  (0.807s, 1268.82/s)  LR: 8.658e-04  Data: 0.021 (0.014)
Train: 144 [ 700/1251 ( 56%)]  Loss: 3.577 (3.65)  Time: 0.825s, 1240.77/s  (0.807s, 1269.08/s)  LR: 8.658e-04  Data: 0.010 (0.014)
Train: 144 [ 750/1251 ( 60%)]  Loss: 3.452 (3.63)  Time: 0.787s, 1301.94/s  (0.806s, 1270.11/s)  LR: 8.658e-04  Data: 0.010 (0.014)
Train: 144 [ 800/1251 ( 64%)]  Loss: 3.658 (3.64)  Time: 0.803s, 1275.13/s  (0.807s, 1269.19/s)  LR: 8.658e-04  Data: 0.015 (0.014)
Train: 144 [ 850/1251 ( 68%)]  Loss: 3.820 (3.65)  Time: 0.774s, 1323.56/s  (0.807s, 1269.00/s)  LR: 8.658e-04  Data: 0.011 (0.014)
Train: 144 [ 900/1251 ( 72%)]  Loss: 3.700 (3.65)  Time: 0.772s, 1326.57/s  (0.806s, 1271.24/s)  LR: 8.658e-04  Data: 0.010 (0.013)
Train: 144 [ 950/1251 ( 76%)]  Loss: 3.510 (3.64)  Time: 0.773s, 1324.75/s  (0.805s, 1272.70/s)  LR: 8.658e-04  Data: 0.010 (0.013)
Train: 144 [1000/1251 ( 80%)]  Loss: 3.455 (3.63)  Time: 0.796s, 1286.35/s  (0.805s, 1272.80/s)  LR: 8.658e-04  Data: 0.010 (0.013)
Train: 144 [1050/1251 ( 84%)]  Loss: 3.997 (3.65)  Time: 0.776s, 1319.11/s  (0.804s, 1273.01/s)  LR: 8.658e-04  Data: 0.010 (0.013)
Train: 144 [1100/1251 ( 88%)]  Loss: 3.548 (3.65)  Time: 0.772s, 1325.68/s  (0.804s, 1273.09/s)  LR: 8.658e-04  Data: 0.010 (0.013)
Train: 144 [1150/1251 ( 92%)]  Loss: 3.350 (3.63)  Time: 0.820s, 1249.32/s  (0.804s, 1273.23/s)  LR: 8.658e-04  Data: 0.010 (0.013)
Train: 144 [1200/1251 ( 96%)]  Loss: 3.659 (3.63)  Time: 0.773s, 1324.98/s  (0.804s, 1273.63/s)  LR: 8.658e-04  Data: 0.010 (0.013)
Train: 144 [1250/1251 (100%)]  Loss: 3.721 (3.64)  Time: 0.797s, 1284.69/s  (0.804s, 1273.50/s)  LR: 8.658e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.656 (1.656)  Loss:  0.8389 (0.8389)  Acc@1: 88.1836 (88.1836)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.194 (0.591)  Loss:  0.9648 (1.4109)  Acc@1: 82.7830 (72.3680)  Acc@5: 94.5755 (91.3760)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-138.pth.tar', 72.72799998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-139.pth.tar', 72.52199993652344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-132.pth.tar', 72.52000001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-133.pth.tar', 72.51400014648438)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-130.pth.tar', 72.46799991699218)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-120.pth.tar', 72.4560001171875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-114.pth.tar', 72.40200011962891)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-134.pth.tar', 72.37400004394532)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-144.pth.tar', 72.36800014892579)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-140.pth.tar', 72.32999993896485)

Train: 145 [   0/1251 (  0%)]  Loss: 3.856 (3.86)  Time: 2.607s,  392.76/s  (2.607s,  392.76/s)  LR: 8.641e-04  Data: 1.881 (1.881)
Train: 145 [  50/1251 (  4%)]  Loss: 3.809 (3.83)  Time: 0.842s, 1216.54/s  (0.849s, 1206.82/s)  LR: 8.641e-04  Data: 0.010 (0.055)
Train: 145 [ 100/1251 (  8%)]  Loss: 3.514 (3.73)  Time: 0.784s, 1306.89/s  (0.824s, 1242.55/s)  LR: 8.641e-04  Data: 0.012 (0.033)
Train: 145 [ 150/1251 ( 12%)]  Loss: 3.565 (3.69)  Time: 0.780s, 1312.53/s  (0.816s, 1255.38/s)  LR: 8.641e-04  Data: 0.010 (0.026)
Train: 145 [ 200/1251 ( 16%)]  Loss: 3.515 (3.65)  Time: 0.774s, 1322.73/s  (0.813s, 1259.49/s)  LR: 8.641e-04  Data: 0.010 (0.022)
Train: 145 [ 250/1251 ( 20%)]  Loss: 3.107 (3.56)  Time: 0.778s, 1315.89/s  (0.811s, 1262.55/s)  LR: 8.641e-04  Data: 0.010 (0.020)
Train: 145 [ 300/1251 ( 24%)]  Loss: 3.693 (3.58)  Time: 0.774s, 1322.28/s  (0.809s, 1265.23/s)  LR: 8.641e-04  Data: 0.010 (0.019)
Train: 145 [ 350/1251 ( 28%)]  Loss: 3.773 (3.60)  Time: 0.779s, 1314.81/s  (0.808s, 1267.79/s)  LR: 8.641e-04  Data: 0.011 (0.018)
Train: 145 [ 400/1251 ( 32%)]  Loss: 3.471 (3.59)  Time: 0.780s, 1312.76/s  (0.807s, 1269.15/s)  LR: 8.641e-04  Data: 0.010 (0.017)
Train: 145 [ 450/1251 ( 36%)]  Loss: 3.548 (3.59)  Time: 0.777s, 1318.42/s  (0.806s, 1269.79/s)  LR: 8.641e-04  Data: 0.010 (0.016)
Train: 145 [ 500/1251 ( 40%)]  Loss: 3.737 (3.60)  Time: 0.813s, 1259.15/s  (0.806s, 1270.97/s)  LR: 8.641e-04  Data: 0.010 (0.016)
Train: 145 [ 550/1251 ( 44%)]  Loss: 3.564 (3.60)  Time: 0.785s, 1303.64/s  (0.806s, 1271.03/s)  LR: 8.641e-04  Data: 0.010 (0.015)
Train: 145 [ 600/1251 ( 48%)]  Loss: 3.293 (3.57)  Time: 0.800s, 1279.29/s  (0.805s, 1271.54/s)  LR: 8.641e-04  Data: 0.010 (0.015)
Train: 145 [ 650/1251 ( 52%)]  Loss: 3.628 (3.58)  Time: 0.808s, 1267.96/s  (0.805s, 1272.51/s)  LR: 8.641e-04  Data: 0.010 (0.015)
Train: 145 [ 700/1251 ( 56%)]  Loss: 3.751 (3.59)  Time: 0.834s, 1227.31/s  (0.804s, 1273.03/s)  LR: 8.641e-04  Data: 0.010 (0.014)
Train: 145 [ 750/1251 ( 60%)]  Loss: 4.030 (3.62)  Time: 0.823s, 1244.36/s  (0.804s, 1273.39/s)  LR: 8.641e-04  Data: 0.010 (0.014)
Train: 145 [ 800/1251 ( 64%)]  Loss: 3.744 (3.62)  Time: 0.783s, 1307.28/s  (0.804s, 1274.41/s)  LR: 8.641e-04  Data: 0.010 (0.014)
Train: 145 [ 850/1251 ( 68%)]  Loss: 3.906 (3.64)  Time: 0.835s, 1225.89/s  (0.803s, 1274.51/s)  LR: 8.641e-04  Data: 0.014 (0.014)
Train: 145 [ 900/1251 ( 72%)]  Loss: 3.634 (3.64)  Time: 0.915s, 1118.60/s  (0.804s, 1274.00/s)  LR: 8.641e-04  Data: 0.010 (0.014)
Train: 145 [ 950/1251 ( 76%)]  Loss: 3.371 (3.63)  Time: 0.777s, 1318.39/s  (0.804s, 1274.16/s)  LR: 8.641e-04  Data: 0.010 (0.014)
Train: 145 [1000/1251 ( 80%)]  Loss: 3.462 (3.62)  Time: 0.807s, 1269.11/s  (0.803s, 1274.47/s)  LR: 8.641e-04  Data: 0.010 (0.013)
Train: 145 [1050/1251 ( 84%)]  Loss: 3.556 (3.61)  Time: 0.771s, 1327.70/s  (0.803s, 1274.43/s)  LR: 8.641e-04  Data: 0.011 (0.013)
Train: 145 [1100/1251 ( 88%)]  Loss: 3.500 (3.61)  Time: 0.815s, 1256.98/s  (0.803s, 1274.88/s)  LR: 8.641e-04  Data: 0.010 (0.013)
Train: 145 [1150/1251 ( 92%)]  Loss: 3.294 (3.60)  Time: 0.792s, 1293.42/s  (0.803s, 1275.29/s)  LR: 8.641e-04  Data: 0.010 (0.013)
Train: 145 [1200/1251 ( 96%)]  Loss: 3.867 (3.61)  Time: 0.791s, 1294.76/s  (0.803s, 1275.39/s)  LR: 8.641e-04  Data: 0.010 (0.013)
Train: 145 [1250/1251 (100%)]  Loss: 3.866 (3.62)  Time: 0.767s, 1334.70/s  (0.803s, 1275.16/s)  LR: 8.641e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.631 (1.631)  Loss:  0.8218 (0.8218)  Acc@1: 87.9883 (87.9883)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.194 (0.598)  Loss:  0.9331 (1.4271)  Acc@1: 84.3160 (72.3780)  Acc@5: 95.6368 (91.2980)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-138.pth.tar', 72.72799998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-139.pth.tar', 72.52199993652344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-132.pth.tar', 72.52000001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-133.pth.tar', 72.51400014648438)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-130.pth.tar', 72.46799991699218)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-120.pth.tar', 72.4560001171875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-114.pth.tar', 72.40200011962891)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-145.pth.tar', 72.37799990966796)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-134.pth.tar', 72.37400004394532)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-144.pth.tar', 72.36800014892579)

Train: 146 [   0/1251 (  0%)]  Loss: 3.665 (3.67)  Time: 2.378s,  430.60/s  (2.378s,  430.60/s)  LR: 8.623e-04  Data: 1.645 (1.645)
Train: 146 [  50/1251 (  4%)]  Loss: 3.454 (3.56)  Time: 0.770s, 1329.03/s  (0.839s, 1221.01/s)  LR: 8.623e-04  Data: 0.010 (0.049)
Train: 146 [ 100/1251 (  8%)]  Loss: 3.776 (3.63)  Time: 0.833s, 1229.27/s  (0.820s, 1248.99/s)  LR: 8.623e-04  Data: 0.013 (0.030)
Train: 146 [ 150/1251 ( 12%)]  Loss: 3.792 (3.67)  Time: 0.780s, 1312.83/s  (0.813s, 1259.89/s)  LR: 8.623e-04  Data: 0.009 (0.024)
Train: 146 [ 200/1251 ( 16%)]  Loss: 3.533 (3.64)  Time: 0.836s, 1225.34/s  (0.811s, 1262.44/s)  LR: 8.623e-04  Data: 0.018 (0.021)
Train: 146 [ 250/1251 ( 20%)]  Loss: 3.438 (3.61)  Time: 0.771s, 1327.68/s  (0.809s, 1266.47/s)  LR: 8.623e-04  Data: 0.011 (0.019)
Train: 146 [ 300/1251 ( 24%)]  Loss: 3.451 (3.59)  Time: 0.819s, 1249.70/s  (0.806s, 1270.24/s)  LR: 8.623e-04  Data: 0.010 (0.018)
Train: 146 [ 350/1251 ( 28%)]  Loss: 3.643 (3.59)  Time: 0.778s, 1316.22/s  (0.806s, 1271.03/s)  LR: 8.623e-04  Data: 0.010 (0.017)
Train: 146 [ 400/1251 ( 32%)]  Loss: 3.567 (3.59)  Time: 0.790s, 1295.82/s  (0.805s, 1272.36/s)  LR: 8.623e-04  Data: 0.010 (0.016)
Train: 146 [ 450/1251 ( 36%)]  Loss: 3.724 (3.60)  Time: 0.771s, 1327.36/s  (0.805s, 1272.11/s)  LR: 8.623e-04  Data: 0.010 (0.015)
Train: 146 [ 500/1251 ( 40%)]  Loss: 3.702 (3.61)  Time: 0.847s, 1209.13/s  (0.804s, 1273.18/s)  LR: 8.623e-04  Data: 0.014 (0.015)
Train: 146 [ 550/1251 ( 44%)]  Loss: 3.959 (3.64)  Time: 0.807s, 1268.29/s  (0.805s, 1272.44/s)  LR: 8.623e-04  Data: 0.010 (0.015)
Train: 146 [ 600/1251 ( 48%)]  Loss: 3.668 (3.64)  Time: 0.808s, 1267.69/s  (0.804s, 1273.98/s)  LR: 8.623e-04  Data: 0.011 (0.014)
Train: 146 [ 650/1251 ( 52%)]  Loss: 3.530 (3.64)  Time: 0.777s, 1318.01/s  (0.803s, 1274.49/s)  LR: 8.623e-04  Data: 0.010 (0.014)
Train: 146 [ 700/1251 ( 56%)]  Loss: 4.110 (3.67)  Time: 0.856s, 1196.61/s  (0.803s, 1274.46/s)  LR: 8.623e-04  Data: 0.010 (0.014)
Train: 146 [ 750/1251 ( 60%)]  Loss: 3.681 (3.67)  Time: 0.780s, 1312.48/s  (0.804s, 1274.08/s)  LR: 8.623e-04  Data: 0.011 (0.014)
Train: 146 [ 800/1251 ( 64%)]  Loss: 3.845 (3.68)  Time: 0.773s, 1325.06/s  (0.803s, 1274.96/s)  LR: 8.623e-04  Data: 0.011 (0.013)
Train: 146 [ 850/1251 ( 68%)]  Loss: 3.809 (3.69)  Time: 0.810s, 1263.51/s  (0.803s, 1274.85/s)  LR: 8.623e-04  Data: 0.010 (0.013)
Train: 146 [ 900/1251 ( 72%)]  Loss: 3.596 (3.68)  Time: 0.787s, 1300.36/s  (0.803s, 1274.99/s)  LR: 8.623e-04  Data: 0.015 (0.013)
Train: 146 [ 950/1251 ( 76%)]  Loss: 4.198 (3.71)  Time: 0.780s, 1312.86/s  (0.803s, 1275.34/s)  LR: 8.623e-04  Data: 0.012 (0.013)
Train: 146 [1000/1251 ( 80%)]  Loss: 3.472 (3.70)  Time: 0.811s, 1261.91/s  (0.803s, 1275.46/s)  LR: 8.623e-04  Data: 0.010 (0.013)
Train: 146 [1050/1251 ( 84%)]  Loss: 3.578 (3.69)  Time: 0.773s, 1325.56/s  (0.803s, 1275.71/s)  LR: 8.623e-04  Data: 0.010 (0.013)
Train: 146 [1100/1251 ( 88%)]  Loss: 3.991 (3.70)  Time: 0.783s, 1308.39/s  (0.803s, 1274.58/s)  LR: 8.623e-04  Data: 0.013 (0.013)
Train: 146 [1150/1251 ( 92%)]  Loss: 3.494 (3.69)  Time: 0.789s, 1297.83/s  (0.803s, 1274.54/s)  LR: 8.623e-04  Data: 0.010 (0.013)
Train: 146 [1200/1251 ( 96%)]  Loss: 3.358 (3.68)  Time: 0.818s, 1251.43/s  (0.803s, 1275.13/s)  LR: 8.623e-04  Data: 0.010 (0.013)
Train: 146 [1250/1251 (100%)]  Loss: 3.847 (3.69)  Time: 0.763s, 1341.55/s  (0.803s, 1275.79/s)  LR: 8.623e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.648 (1.648)  Loss:  0.8994 (0.8994)  Acc@1: 85.5469 (85.5469)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.194 (0.602)  Loss:  1.0137 (1.4804)  Acc@1: 82.7830 (72.1840)  Acc@5: 95.6368 (91.2120)
Train: 147 [   0/1251 (  0%)]  Loss: 3.613 (3.61)  Time: 2.407s,  425.45/s  (2.407s,  425.45/s)  LR: 8.605e-04  Data: 1.677 (1.677)
Train: 147 [  50/1251 (  4%)]  Loss: 4.005 (3.81)  Time: 0.856s, 1196.28/s  (0.831s, 1231.60/s)  LR: 8.605e-04  Data: 0.010 (0.047)
Train: 147 [ 100/1251 (  8%)]  Loss: 3.427 (3.68)  Time: 0.870s, 1177.04/s  (0.815s, 1256.18/s)  LR: 8.605e-04  Data: 0.010 (0.029)
Train: 147 [ 150/1251 ( 12%)]  Loss: 3.728 (3.69)  Time: 0.881s, 1161.95/s  (0.810s, 1264.83/s)  LR: 8.605e-04  Data: 0.010 (0.023)
Train: 147 [ 200/1251 ( 16%)]  Loss: 3.571 (3.67)  Time: 0.784s, 1306.41/s  (0.807s, 1268.27/s)  LR: 8.605e-04  Data: 0.010 (0.020)
Train: 147 [ 250/1251 ( 20%)]  Loss: 3.472 (3.64)  Time: 0.791s, 1294.79/s  (0.806s, 1270.51/s)  LR: 8.605e-04  Data: 0.010 (0.018)
Train: 147 [ 300/1251 ( 24%)]  Loss: 3.501 (3.62)  Time: 0.793s, 1291.53/s  (0.804s, 1273.02/s)  LR: 8.605e-04  Data: 0.010 (0.017)
Train: 147 [ 350/1251 ( 28%)]  Loss: 3.412 (3.59)  Time: 0.814s, 1258.31/s  (0.804s, 1273.35/s)  LR: 8.605e-04  Data: 0.016 (0.016)
Train: 147 [ 400/1251 ( 32%)]  Loss: 3.470 (3.58)  Time: 0.778s, 1315.70/s  (0.803s, 1274.78/s)  LR: 8.605e-04  Data: 0.013 (0.016)
Train: 147 [ 450/1251 ( 36%)]  Loss: 3.481 (3.57)  Time: 0.826s, 1240.16/s  (0.803s, 1275.37/s)  LR: 8.605e-04  Data: 0.014 (0.015)
Train: 147 [ 500/1251 ( 40%)]  Loss: 3.528 (3.56)  Time: 0.828s, 1236.28/s  (0.803s, 1276.00/s)  LR: 8.605e-04  Data: 0.010 (0.015)
Train: 147 [ 550/1251 ( 44%)]  Loss: 3.691 (3.57)  Time: 0.774s, 1323.22/s  (0.803s, 1275.10/s)  LR: 8.605e-04  Data: 0.010 (0.014)
Train: 147 [ 600/1251 ( 48%)]  Loss: 3.439 (3.56)  Time: 0.819s, 1249.60/s  (0.803s, 1275.18/s)  LR: 8.605e-04  Data: 0.010 (0.014)
Train: 147 [ 650/1251 ( 52%)]  Loss: 3.819 (3.58)  Time: 0.813s, 1259.22/s  (0.802s, 1276.44/s)  LR: 8.605e-04  Data: 0.010 (0.014)
Train: 147 [ 700/1251 ( 56%)]  Loss: 3.547 (3.58)  Time: 0.791s, 1294.42/s  (0.802s, 1276.53/s)  LR: 8.605e-04  Data: 0.010 (0.014)
Train: 147 [ 750/1251 ( 60%)]  Loss: 3.620 (3.58)  Time: 0.774s, 1322.67/s  (0.802s, 1276.43/s)  LR: 8.605e-04  Data: 0.010 (0.013)
Train: 147 [ 800/1251 ( 64%)]  Loss: 3.370 (3.57)  Time: 0.772s, 1325.58/s  (0.802s, 1276.67/s)  LR: 8.605e-04  Data: 0.010 (0.013)
Train: 147 [ 850/1251 ( 68%)]  Loss: 3.381 (3.56)  Time: 0.850s, 1204.54/s  (0.802s, 1276.26/s)  LR: 8.605e-04  Data: 0.015 (0.013)
Train: 147 [ 900/1251 ( 72%)]  Loss: 4.225 (3.59)  Time: 0.804s, 1273.24/s  (0.802s, 1276.03/s)  LR: 8.605e-04  Data: 0.013 (0.013)
Train: 147 [ 950/1251 ( 76%)]  Loss: 3.999 (3.62)  Time: 0.797s, 1284.85/s  (0.802s, 1276.01/s)  LR: 8.605e-04  Data: 0.014 (0.013)
Train: 147 [1000/1251 ( 80%)]  Loss: 3.795 (3.62)  Time: 0.775s, 1320.93/s  (0.802s, 1276.33/s)  LR: 8.605e-04  Data: 0.012 (0.013)
Train: 147 [1050/1251 ( 84%)]  Loss: 3.452 (3.62)  Time: 0.801s, 1278.52/s  (0.802s, 1276.14/s)  LR: 8.605e-04  Data: 0.009 (0.013)
Train: 147 [1100/1251 ( 88%)]  Loss: 3.678 (3.62)  Time: 0.794s, 1290.38/s  (0.802s, 1276.11/s)  LR: 8.605e-04  Data: 0.011 (0.013)
Train: 147 [1150/1251 ( 92%)]  Loss: 3.580 (3.62)  Time: 0.804s, 1274.07/s  (0.802s, 1276.15/s)  LR: 8.605e-04  Data: 0.012 (0.013)
Train: 147 [1200/1251 ( 96%)]  Loss: 3.771 (3.62)  Time: 0.772s, 1325.66/s  (0.802s, 1276.58/s)  LR: 8.605e-04  Data: 0.010 (0.013)
Train: 147 [1250/1251 (100%)]  Loss: 3.793 (3.63)  Time: 0.772s, 1326.51/s  (0.802s, 1277.02/s)  LR: 8.605e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.697 (1.697)  Loss:  0.8008 (0.8008)  Acc@1: 88.2812 (88.2812)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.194 (0.609)  Loss:  0.9102 (1.3803)  Acc@1: 82.6651 (72.6900)  Acc@5: 95.4009 (91.4540)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-138.pth.tar', 72.72799998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-147.pth.tar', 72.68999996826172)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-139.pth.tar', 72.52199993652344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-132.pth.tar', 72.52000001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-133.pth.tar', 72.51400014648438)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-130.pth.tar', 72.46799991699218)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-120.pth.tar', 72.4560001171875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-114.pth.tar', 72.40200011962891)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-145.pth.tar', 72.37799990966796)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-134.pth.tar', 72.37400004394532)

Train: 148 [   0/1251 (  0%)]  Loss: 3.654 (3.65)  Time: 2.499s,  409.81/s  (2.499s,  409.81/s)  LR: 8.587e-04  Data: 1.773 (1.773)
Train: 148 [  50/1251 (  4%)]  Loss: 3.837 (3.75)  Time: 0.822s, 1245.20/s  (0.839s, 1219.86/s)  LR: 8.587e-04  Data: 0.010 (0.052)
Train: 148 [ 100/1251 (  8%)]  Loss: 3.703 (3.73)  Time: 0.776s, 1320.22/s  (0.820s, 1248.60/s)  LR: 8.587e-04  Data: 0.010 (0.032)
Train: 148 [ 150/1251 ( 12%)]  Loss: 3.377 (3.64)  Time: 0.775s, 1321.62/s  (0.813s, 1259.73/s)  LR: 8.587e-04  Data: 0.011 (0.025)
Train: 148 [ 200/1251 ( 16%)]  Loss: 3.881 (3.69)  Time: 0.808s, 1267.16/s  (0.810s, 1264.18/s)  LR: 8.587e-04  Data: 0.009 (0.022)
Train: 148 [ 250/1251 ( 20%)]  Loss: 3.393 (3.64)  Time: 0.775s, 1322.00/s  (0.808s, 1267.98/s)  LR: 8.587e-04  Data: 0.012 (0.020)
Train: 148 [ 300/1251 ( 24%)]  Loss: 3.482 (3.62)  Time: 0.777s, 1318.57/s  (0.807s, 1268.57/s)  LR: 8.587e-04  Data: 0.011 (0.018)
Train: 148 [ 350/1251 ( 28%)]  Loss: 3.734 (3.63)  Time: 0.772s, 1327.19/s  (0.807s, 1269.61/s)  LR: 8.587e-04  Data: 0.010 (0.017)
Train: 148 [ 400/1251 ( 32%)]  Loss: 3.683 (3.64)  Time: 0.843s, 1214.51/s  (0.806s, 1269.74/s)  LR: 8.587e-04  Data: 0.010 (0.017)
Train: 148 [ 450/1251 ( 36%)]  Loss: 3.618 (3.64)  Time: 0.816s, 1255.03/s  (0.805s, 1271.91/s)  LR: 8.587e-04  Data: 0.010 (0.016)
Train: 148 [ 500/1251 ( 40%)]  Loss: 3.668 (3.64)  Time: 0.843s, 1214.58/s  (0.805s, 1272.55/s)  LR: 8.587e-04  Data: 0.013 (0.015)
Train: 148 [ 550/1251 ( 44%)]  Loss: 3.693 (3.64)  Time: 0.782s, 1309.72/s  (0.805s, 1272.18/s)  LR: 8.587e-04  Data: 0.010 (0.015)
Train: 148 [ 600/1251 ( 48%)]  Loss: 3.897 (3.66)  Time: 0.817s, 1252.66/s  (0.805s, 1272.67/s)  LR: 8.587e-04  Data: 0.010 (0.015)
Train: 148 [ 650/1251 ( 52%)]  Loss: 4.011 (3.69)  Time: 0.773s, 1324.77/s  (0.804s, 1273.26/s)  LR: 8.587e-04  Data: 0.010 (0.014)
Train: 148 [ 700/1251 ( 56%)]  Loss: 3.701 (3.69)  Time: 0.775s, 1321.33/s  (0.804s, 1273.70/s)  LR: 8.587e-04  Data: 0.010 (0.014)
Train: 148 [ 750/1251 ( 60%)]  Loss: 3.932 (3.70)  Time: 0.822s, 1245.57/s  (0.804s, 1273.64/s)  LR: 8.587e-04  Data: 0.010 (0.014)
Train: 148 [ 800/1251 ( 64%)]  Loss: 3.757 (3.71)  Time: 0.795s, 1288.66/s  (0.804s, 1274.33/s)  LR: 8.587e-04  Data: 0.021 (0.014)
Train: 148 [ 850/1251 ( 68%)]  Loss: 3.639 (3.70)  Time: 0.787s, 1301.63/s  (0.803s, 1275.07/s)  LR: 8.587e-04  Data: 0.011 (0.014)
Train: 148 [ 900/1251 ( 72%)]  Loss: 3.634 (3.70)  Time: 0.801s, 1277.92/s  (0.803s, 1275.57/s)  LR: 8.587e-04  Data: 0.011 (0.014)
Train: 148 [ 950/1251 ( 76%)]  Loss: 3.904 (3.71)  Time: 0.795s, 1287.53/s  (0.803s, 1275.96/s)  LR: 8.587e-04  Data: 0.010 (0.013)
Train: 148 [1000/1251 ( 80%)]  Loss: 3.630 (3.71)  Time: 0.769s, 1332.32/s  (0.802s, 1276.24/s)  LR: 8.587e-04  Data: 0.010 (0.013)
Train: 148 [1050/1251 ( 84%)]  Loss: 3.796 (3.71)  Time: 0.775s, 1321.86/s  (0.803s, 1275.78/s)  LR: 8.587e-04  Data: 0.011 (0.013)
Train: 148 [1100/1251 ( 88%)]  Loss: 3.315 (3.69)  Time: 0.807s, 1269.37/s  (0.803s, 1275.79/s)  LR: 8.587e-04  Data: 0.011 (0.013)
Train: 148 [1150/1251 ( 92%)]  Loss: 3.739 (3.69)  Time: 0.802s, 1276.92/s  (0.802s, 1276.18/s)  LR: 8.587e-04  Data: 0.010 (0.013)
Train: 148 [1200/1251 ( 96%)]  Loss: 3.857 (3.70)  Time: 0.773s, 1323.91/s  (0.802s, 1276.60/s)  LR: 8.587e-04  Data: 0.010 (0.013)
Train: 148 [1250/1251 (100%)]  Loss: 3.350 (3.69)  Time: 0.793s, 1291.60/s  (0.802s, 1276.86/s)  LR: 8.587e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.626 (1.626)  Loss:  0.9775 (0.9775)  Acc@1: 89.1602 (89.1602)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.194 (0.603)  Loss:  1.0117 (1.5332)  Acc@1: 84.1981 (72.6780)  Acc@5: 96.1085 (91.2800)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-138.pth.tar', 72.72799998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-147.pth.tar', 72.68999996826172)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-148.pth.tar', 72.67799998779297)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-139.pth.tar', 72.52199993652344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-132.pth.tar', 72.52000001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-133.pth.tar', 72.51400014648438)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-130.pth.tar', 72.46799991699218)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-120.pth.tar', 72.4560001171875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-114.pth.tar', 72.40200011962891)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-145.pth.tar', 72.37799990966796)

Train: 149 [   0/1251 (  0%)]  Loss: 3.792 (3.79)  Time: 2.449s,  418.15/s  (2.449s,  418.15/s)  LR: 8.568e-04  Data: 1.657 (1.657)
Train: 149 [  50/1251 (  4%)]  Loss: 3.455 (3.62)  Time: 0.782s, 1310.03/s  (0.845s, 1211.87/s)  LR: 8.568e-04  Data: 0.012 (0.049)
Train: 149 [ 100/1251 (  8%)]  Loss: 3.716 (3.65)  Time: 0.773s, 1324.04/s  (0.819s, 1250.40/s)  LR: 8.568e-04  Data: 0.010 (0.030)
Train: 149 [ 150/1251 ( 12%)]  Loss: 3.439 (3.60)  Time: 0.850s, 1205.28/s  (0.811s, 1262.73/s)  LR: 8.568e-04  Data: 0.009 (0.024)
Train: 149 [ 200/1251 ( 16%)]  Loss: 3.165 (3.51)  Time: 0.793s, 1291.52/s  (0.809s, 1265.49/s)  LR: 8.568e-04  Data: 0.015 (0.021)
Train: 149 [ 250/1251 ( 20%)]  Loss: 3.481 (3.51)  Time: 0.829s, 1235.48/s  (0.806s, 1270.09/s)  LR: 8.568e-04  Data: 0.010 (0.019)
Train: 149 [ 300/1251 ( 24%)]  Loss: 3.748 (3.54)  Time: 0.770s, 1329.85/s  (0.804s, 1274.22/s)  LR: 8.568e-04  Data: 0.010 (0.017)
Train: 149 [ 350/1251 ( 28%)]  Loss: 3.468 (3.53)  Time: 0.782s, 1309.96/s  (0.803s, 1274.91/s)  LR: 8.568e-04  Data: 0.010 (0.016)
Train: 149 [ 400/1251 ( 32%)]  Loss: 3.742 (3.56)  Time: 0.801s, 1279.17/s  (0.803s, 1275.73/s)  LR: 8.568e-04  Data: 0.015 (0.016)
Train: 149 [ 450/1251 ( 36%)]  Loss: 3.983 (3.60)  Time: 0.774s, 1323.43/s  (0.801s, 1277.69/s)  LR: 8.568e-04  Data: 0.010 (0.015)
Train: 149 [ 500/1251 ( 40%)]  Loss: 3.825 (3.62)  Time: 0.791s, 1294.63/s  (0.801s, 1278.18/s)  LR: 8.568e-04  Data: 0.014 (0.015)
Train: 149 [ 550/1251 ( 44%)]  Loss: 3.545 (3.61)  Time: 0.808s, 1267.96/s  (0.802s, 1277.53/s)  LR: 8.568e-04  Data: 0.010 (0.014)
Train: 149 [ 600/1251 ( 48%)]  Loss: 3.392 (3.60)  Time: 0.803s, 1275.43/s  (0.803s, 1275.02/s)  LR: 8.568e-04  Data: 0.013 (0.014)
Train: 149 [ 650/1251 ( 52%)]  Loss: 3.820 (3.61)  Time: 0.774s, 1322.26/s  (0.803s, 1275.19/s)  LR: 8.568e-04  Data: 0.009 (0.014)
Train: 149 [ 700/1251 ( 56%)]  Loss: 3.819 (3.63)  Time: 0.771s, 1328.04/s  (0.801s, 1278.21/s)  LR: 8.568e-04  Data: 0.010 (0.014)
Train: 149 [ 750/1251 ( 60%)]  Loss: 3.423 (3.61)  Time: 0.773s, 1325.45/s  (0.801s, 1278.69/s)  LR: 8.568e-04  Data: 0.010 (0.014)
Train: 149 [ 800/1251 ( 64%)]  Loss: 3.676 (3.62)  Time: 0.821s, 1247.02/s  (0.801s, 1278.65/s)  LR: 8.568e-04  Data: 0.009 (0.013)
Train: 149 [ 850/1251 ( 68%)]  Loss: 3.006 (3.58)  Time: 0.782s, 1309.00/s  (0.801s, 1278.91/s)  LR: 8.568e-04  Data: 0.013 (0.013)
Train: 149 [ 900/1251 ( 72%)]  Loss: 3.492 (3.58)  Time: 0.891s, 1149.38/s  (0.801s, 1278.15/s)  LR: 8.568e-04  Data: 0.010 (0.013)
Train: 149 [ 950/1251 ( 76%)]  Loss: 3.883 (3.59)  Time: 0.772s, 1325.90/s  (0.801s, 1277.98/s)  LR: 8.568e-04  Data: 0.009 (0.013)
Train: 149 [1000/1251 ( 80%)]  Loss: 3.222 (3.58)  Time: 0.815s, 1255.94/s  (0.801s, 1278.39/s)  LR: 8.568e-04  Data: 0.010 (0.013)
Train: 149 [1050/1251 ( 84%)]  Loss: 3.979 (3.59)  Time: 0.816s, 1255.64/s  (0.801s, 1278.06/s)  LR: 8.568e-04  Data: 0.010 (0.013)
Train: 149 [1100/1251 ( 88%)]  Loss: 3.580 (3.59)  Time: 0.800s, 1279.65/s  (0.801s, 1277.92/s)  LR: 8.568e-04  Data: 0.010 (0.013)
Train: 149 [1150/1251 ( 92%)]  Loss: 3.801 (3.60)  Time: 0.786s, 1303.12/s  (0.801s, 1278.04/s)  LR: 8.568e-04  Data: 0.010 (0.013)
Train: 149 [1200/1251 ( 96%)]  Loss: 3.528 (3.60)  Time: 0.808s, 1266.56/s  (0.801s, 1277.96/s)  LR: 8.568e-04  Data: 0.010 (0.013)
Train: 149 [1250/1251 (100%)]  Loss: 3.348 (3.59)  Time: 0.778s, 1315.85/s  (0.801s, 1277.98/s)  LR: 8.568e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.705 (1.705)  Loss:  0.8433 (0.8433)  Acc@1: 89.3555 (89.3555)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.194 (0.597)  Loss:  0.9561 (1.4398)  Acc@1: 83.4906 (72.2900)  Acc@5: 96.4623 (91.3380)
Train: 150 [   0/1251 (  0%)]  Loss: 3.779 (3.78)  Time: 2.409s,  425.01/s  (2.409s,  425.01/s)  LR: 8.550e-04  Data: 1.666 (1.666)
Train: 150 [  50/1251 (  4%)]  Loss: 3.718 (3.75)  Time: 0.773s, 1325.16/s  (0.843s, 1214.81/s)  LR: 8.550e-04  Data: 0.010 (0.052)
Train: 150 [ 100/1251 (  8%)]  Loss: 3.837 (3.78)  Time: 0.789s, 1297.72/s  (0.820s, 1249.03/s)  LR: 8.550e-04  Data: 0.014 (0.032)
Train: 150 [ 150/1251 ( 12%)]  Loss: 3.650 (3.75)  Time: 0.830s, 1233.41/s  (0.814s, 1258.47/s)  LR: 8.550e-04  Data: 0.010 (0.025)
Train: 150 [ 200/1251 ( 16%)]  Loss: 3.669 (3.73)  Time: 0.875s, 1170.47/s  (0.811s, 1262.61/s)  LR: 8.550e-04  Data: 0.010 (0.021)
Train: 150 [ 250/1251 ( 20%)]  Loss: 3.931 (3.76)  Time: 0.809s, 1264.98/s  (0.808s, 1267.31/s)  LR: 8.550e-04  Data: 0.012 (0.019)
Train: 150 [ 300/1251 ( 24%)]  Loss: 3.941 (3.79)  Time: 0.781s, 1310.84/s  (0.805s, 1271.31/s)  LR: 8.550e-04  Data: 0.010 (0.018)
Train: 150 [ 350/1251 ( 28%)]  Loss: 3.465 (3.75)  Time: 0.773s, 1324.02/s  (0.805s, 1272.08/s)  LR: 8.550e-04  Data: 0.010 (0.017)
Train: 150 [ 400/1251 ( 32%)]  Loss: 3.736 (3.75)  Time: 0.779s, 1314.01/s  (0.804s, 1272.91/s)  LR: 8.550e-04  Data: 0.010 (0.016)
Train: 150 [ 450/1251 ( 36%)]  Loss: 3.680 (3.74)  Time: 0.828s, 1236.08/s  (0.804s, 1274.13/s)  LR: 8.550e-04  Data: 0.012 (0.016)
Train: 150 [ 500/1251 ( 40%)]  Loss: 4.021 (3.77)  Time: 0.777s, 1317.15/s  (0.803s, 1274.43/s)  LR: 8.550e-04  Data: 0.010 (0.015)
Train: 150 [ 550/1251 ( 44%)]  Loss: 3.520 (3.75)  Time: 0.781s, 1310.99/s  (0.803s, 1275.22/s)  LR: 8.550e-04  Data: 0.011 (0.015)
Train: 150 [ 600/1251 ( 48%)]  Loss: 3.979 (3.76)  Time: 0.805s, 1271.54/s  (0.803s, 1276.01/s)  LR: 8.550e-04  Data: 0.010 (0.014)
Train: 150 [ 650/1251 ( 52%)]  Loss: 3.815 (3.77)  Time: 0.803s, 1275.25/s  (0.802s, 1277.14/s)  LR: 8.550e-04  Data: 0.010 (0.014)
Train: 150 [ 700/1251 ( 56%)]  Loss: 3.645 (3.76)  Time: 0.811s, 1261.90/s  (0.802s, 1276.93/s)  LR: 8.550e-04  Data: 0.009 (0.014)
Train: 150 [ 750/1251 ( 60%)]  Loss: 3.302 (3.73)  Time: 0.770s, 1329.26/s  (0.802s, 1277.33/s)  LR: 8.550e-04  Data: 0.010 (0.014)
Train: 150 [ 800/1251 ( 64%)]  Loss: 3.547 (3.72)  Time: 0.790s, 1296.16/s  (0.802s, 1277.22/s)  LR: 8.550e-04  Data: 0.014 (0.014)
Train: 150 [ 850/1251 ( 68%)]  Loss: 3.383 (3.70)  Time: 0.793s, 1291.21/s  (0.801s, 1277.68/s)  LR: 8.550e-04  Data: 0.013 (0.013)
Train: 150 [ 900/1251 ( 72%)]  Loss: 3.822 (3.71)  Time: 0.837s, 1223.68/s  (0.801s, 1277.62/s)  LR: 8.550e-04  Data: 0.011 (0.013)
Train: 150 [ 950/1251 ( 76%)]  Loss: 3.608 (3.70)  Time: 0.777s, 1318.15/s  (0.802s, 1277.60/s)  LR: 8.550e-04  Data: 0.013 (0.013)
Train: 150 [1000/1251 ( 80%)]  Loss: 3.816 (3.71)  Time: 0.781s, 1311.17/s  (0.801s, 1277.99/s)  LR: 8.550e-04  Data: 0.010 (0.013)
Train: 150 [1050/1251 ( 84%)]  Loss: 3.839 (3.71)  Time: 0.801s, 1279.05/s  (0.801s, 1278.20/s)  LR: 8.550e-04  Data: 0.015 (0.013)
Train: 150 [1100/1251 ( 88%)]  Loss: 3.758 (3.72)  Time: 0.789s, 1297.45/s  (0.801s, 1278.45/s)  LR: 8.550e-04  Data: 0.010 (0.013)
Train: 150 [1150/1251 ( 92%)]  Loss: 3.586 (3.71)  Time: 0.862s, 1188.37/s  (0.801s, 1278.59/s)  LR: 8.550e-04  Data: 0.010 (0.013)
Train: 150 [1200/1251 ( 96%)]  Loss: 3.683 (3.71)  Time: 0.785s, 1305.08/s  (0.801s, 1278.45/s)  LR: 8.550e-04  Data: 0.010 (0.013)
Train: 150 [1250/1251 (100%)]  Loss: 3.710 (3.71)  Time: 0.784s, 1305.67/s  (0.801s, 1278.33/s)  LR: 8.550e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.602 (1.602)  Loss:  0.7324 (0.7324)  Acc@1: 89.3555 (89.3555)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.194 (0.594)  Loss:  0.8765 (1.3939)  Acc@1: 84.0802 (72.7820)  Acc@5: 95.5189 (91.6400)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-150.pth.tar', 72.78200006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-138.pth.tar', 72.72799998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-147.pth.tar', 72.68999996826172)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-148.pth.tar', 72.67799998779297)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-139.pth.tar', 72.52199993652344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-132.pth.tar', 72.52000001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-133.pth.tar', 72.51400014648438)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-130.pth.tar', 72.46799991699218)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-120.pth.tar', 72.4560001171875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-114.pth.tar', 72.40200011962891)

Train: 151 [   0/1251 (  0%)]  Loss: 3.993 (3.99)  Time: 2.497s,  410.14/s  (2.497s,  410.14/s)  LR: 8.532e-04  Data: 1.771 (1.771)
Train: 151 [  50/1251 (  4%)]  Loss: 3.442 (3.72)  Time: 0.773s, 1324.12/s  (0.840s, 1218.35/s)  LR: 8.532e-04  Data: 0.010 (0.055)
Train: 151 [ 100/1251 (  8%)]  Loss: 3.558 (3.66)  Time: 0.781s, 1310.90/s  (0.822s, 1245.00/s)  LR: 8.532e-04  Data: 0.010 (0.033)
Train: 151 [ 150/1251 ( 12%)]  Loss: 3.285 (3.57)  Time: 0.991s, 1033.67/s  (0.815s, 1255.83/s)  LR: 8.532e-04  Data: 0.010 (0.026)
Train: 151 [ 200/1251 ( 16%)]  Loss: 3.891 (3.63)  Time: 0.774s, 1322.16/s  (0.812s, 1260.57/s)  LR: 8.532e-04  Data: 0.010 (0.022)
Train: 151 [ 250/1251 ( 20%)]  Loss: 3.801 (3.66)  Time: 0.772s, 1326.66/s  (0.809s, 1265.82/s)  LR: 8.532e-04  Data: 0.010 (0.020)
Train: 151 [ 300/1251 ( 24%)]  Loss: 3.819 (3.68)  Time: 0.815s, 1256.80/s  (0.810s, 1264.44/s)  LR: 8.532e-04  Data: 0.011 (0.019)
Train: 151 [ 350/1251 ( 28%)]  Loss: 3.906 (3.71)  Time: 0.847s, 1209.45/s  (0.810s, 1264.96/s)  LR: 8.532e-04  Data: 0.010 (0.018)
Train: 151 [ 400/1251 ( 32%)]  Loss: 3.562 (3.69)  Time: 0.780s, 1313.54/s  (0.808s, 1267.74/s)  LR: 8.532e-04  Data: 0.010 (0.017)
Train: 151 [ 450/1251 ( 36%)]  Loss: 3.893 (3.71)  Time: 0.808s, 1266.94/s  (0.807s, 1268.66/s)  LR: 8.532e-04  Data: 0.010 (0.016)
Train: 151 [ 500/1251 ( 40%)]  Loss: 3.395 (3.69)  Time: 0.777s, 1318.09/s  (0.806s, 1269.83/s)  LR: 8.532e-04  Data: 0.010 (0.016)
Train: 151 [ 550/1251 ( 44%)]  Loss: 3.752 (3.69)  Time: 0.844s, 1212.98/s  (0.806s, 1269.97/s)  LR: 8.532e-04  Data: 0.010 (0.015)
Train: 151 [ 600/1251 ( 48%)]  Loss: 3.743 (3.70)  Time: 0.830s, 1233.19/s  (0.806s, 1271.19/s)  LR: 8.532e-04  Data: 0.011 (0.015)
Train: 151 [ 650/1251 ( 52%)]  Loss: 3.795 (3.70)  Time: 0.783s, 1307.60/s  (0.805s, 1272.30/s)  LR: 8.532e-04  Data: 0.013 (0.015)
Train: 151 [ 700/1251 ( 56%)]  Loss: 3.882 (3.71)  Time: 0.801s, 1278.22/s  (0.804s, 1273.20/s)  LR: 8.532e-04  Data: 0.011 (0.014)
Train: 151 [ 750/1251 ( 60%)]  Loss: 3.551 (3.70)  Time: 0.788s, 1299.05/s  (0.804s, 1273.67/s)  LR: 8.532e-04  Data: 0.013 (0.014)
Train: 151 [ 800/1251 ( 64%)]  Loss: 3.216 (3.68)  Time: 0.852s, 1201.56/s  (0.804s, 1274.04/s)  LR: 8.532e-04  Data: 0.015 (0.014)
Train: 151 [ 850/1251 ( 68%)]  Loss: 3.483 (3.66)  Time: 0.832s, 1230.68/s  (0.804s, 1274.38/s)  LR: 8.532e-04  Data: 0.010 (0.014)
Train: 151 [ 900/1251 ( 72%)]  Loss: 3.492 (3.66)  Time: 0.817s, 1252.80/s  (0.804s, 1273.83/s)  LR: 8.532e-04  Data: 0.011 (0.014)
Train: 151 [ 950/1251 ( 76%)]  Loss: 3.615 (3.65)  Time: 0.775s, 1321.99/s  (0.804s, 1274.42/s)  LR: 8.532e-04  Data: 0.010 (0.014)
Train: 151 [1000/1251 ( 80%)]  Loss: 3.766 (3.66)  Time: 0.829s, 1234.82/s  (0.803s, 1275.15/s)  LR: 8.532e-04  Data: 0.013 (0.013)
Train: 151 [1050/1251 ( 84%)]  Loss: 3.783 (3.66)  Time: 0.796s, 1287.03/s  (0.803s, 1275.93/s)  LR: 8.532e-04  Data: 0.011 (0.013)
Train: 151 [1100/1251 ( 88%)]  Loss: 3.296 (3.65)  Time: 0.802s, 1276.29/s  (0.803s, 1275.93/s)  LR: 8.532e-04  Data: 0.010 (0.013)
Train: 151 [1150/1251 ( 92%)]  Loss: 4.117 (3.67)  Time: 0.799s, 1281.72/s  (0.802s, 1276.06/s)  LR: 8.532e-04  Data: 0.010 (0.013)
Train: 151 [1200/1251 ( 96%)]  Loss: 3.424 (3.66)  Time: 0.816s, 1254.46/s  (0.802s, 1276.38/s)  LR: 8.532e-04  Data: 0.010 (0.013)
Train: 151 [1250/1251 (100%)]  Loss: 3.873 (3.67)  Time: 0.828s, 1237.26/s  (0.802s, 1276.34/s)  LR: 8.532e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.672 (1.672)  Loss:  0.7119 (0.7119)  Acc@1: 86.6211 (86.6211)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.194 (0.606)  Loss:  0.8594 (1.3034)  Acc@1: 84.3160 (72.6840)  Acc@5: 95.8727 (91.4760)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-150.pth.tar', 72.78200006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-138.pth.tar', 72.72799998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-147.pth.tar', 72.68999996826172)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-151.pth.tar', 72.6840000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-148.pth.tar', 72.67799998779297)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-139.pth.tar', 72.52199993652344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-132.pth.tar', 72.52000001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-133.pth.tar', 72.51400014648438)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-130.pth.tar', 72.46799991699218)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-120.pth.tar', 72.4560001171875)

Train: 152 [   0/1251 (  0%)]  Loss: 3.696 (3.70)  Time: 2.548s,  401.87/s  (2.548s,  401.87/s)  LR: 8.513e-04  Data: 1.816 (1.816)
Train: 152 [  50/1251 (  4%)]  Loss: 3.670 (3.68)  Time: 0.809s, 1265.28/s  (0.839s, 1220.95/s)  LR: 8.513e-04  Data: 0.015 (0.050)
Train: 152 [ 100/1251 (  8%)]  Loss: 3.372 (3.58)  Time: 0.774s, 1323.41/s  (0.818s, 1251.32/s)  LR: 8.513e-04  Data: 0.010 (0.031)
Train: 152 [ 150/1251 ( 12%)]  Loss: 3.688 (3.61)  Time: 0.816s, 1254.83/s  (0.812s, 1260.84/s)  LR: 8.513e-04  Data: 0.010 (0.024)
Train: 152 [ 200/1251 ( 16%)]  Loss: 3.703 (3.63)  Time: 0.781s, 1311.68/s  (0.810s, 1264.53/s)  LR: 8.513e-04  Data: 0.009 (0.021)
Train: 152 [ 250/1251 ( 20%)]  Loss: 3.487 (3.60)  Time: 0.795s, 1287.35/s  (0.808s, 1267.78/s)  LR: 8.513e-04  Data: 0.009 (0.019)
Train: 152 [ 300/1251 ( 24%)]  Loss: 3.606 (3.60)  Time: 0.777s, 1318.68/s  (0.806s, 1270.25/s)  LR: 8.513e-04  Data: 0.011 (0.018)
Train: 152 [ 350/1251 ( 28%)]  Loss: 4.028 (3.66)  Time: 0.819s, 1250.97/s  (0.806s, 1270.67/s)  LR: 8.513e-04  Data: 0.010 (0.017)
Train: 152 [ 400/1251 ( 32%)]  Loss: 3.777 (3.67)  Time: 0.796s, 1285.90/s  (0.805s, 1272.10/s)  LR: 8.513e-04  Data: 0.010 (0.016)
Train: 152 [ 450/1251 ( 36%)]  Loss: 3.298 (3.63)  Time: 0.771s, 1327.77/s  (0.804s, 1273.00/s)  LR: 8.513e-04  Data: 0.010 (0.016)
Train: 152 [ 500/1251 ( 40%)]  Loss: 3.696 (3.64)  Time: 0.774s, 1323.57/s  (0.804s, 1274.04/s)  LR: 8.513e-04  Data: 0.010 (0.015)
Train: 152 [ 550/1251 ( 44%)]  Loss: 4.015 (3.67)  Time: 0.810s, 1264.49/s  (0.804s, 1274.31/s)  LR: 8.513e-04  Data: 0.011 (0.015)
Train: 152 [ 600/1251 ( 48%)]  Loss: 3.843 (3.68)  Time: 0.838s, 1221.44/s  (0.803s, 1274.64/s)  LR: 8.513e-04  Data: 0.010 (0.014)
Train: 152 [ 650/1251 ( 52%)]  Loss: 3.909 (3.70)  Time: 0.787s, 1301.92/s  (0.803s, 1275.00/s)  LR: 8.513e-04  Data: 0.015 (0.014)
Train: 152 [ 700/1251 ( 56%)]  Loss: 3.839 (3.71)  Time: 0.784s, 1305.56/s  (0.803s, 1274.97/s)  LR: 8.513e-04  Data: 0.015 (0.014)
Train: 152 [ 750/1251 ( 60%)]  Loss: 3.692 (3.71)  Time: 0.779s, 1314.04/s  (0.803s, 1275.71/s)  LR: 8.513e-04  Data: 0.010 (0.014)
Train: 152 [ 800/1251 ( 64%)]  Loss: 3.466 (3.69)  Time: 0.807s, 1268.22/s  (0.803s, 1275.42/s)  LR: 8.513e-04  Data: 0.014 (0.014)
Train: 152 [ 850/1251 ( 68%)]  Loss: 3.703 (3.69)  Time: 0.810s, 1264.87/s  (0.803s, 1275.66/s)  LR: 8.513e-04  Data: 0.010 (0.014)
Train: 152 [ 900/1251 ( 72%)]  Loss: 3.727 (3.70)  Time: 0.779s, 1314.33/s  (0.803s, 1275.55/s)  LR: 8.513e-04  Data: 0.009 (0.013)
Train: 152 [ 950/1251 ( 76%)]  Loss: 3.709 (3.70)  Time: 0.788s, 1299.72/s  (0.803s, 1275.00/s)  LR: 8.513e-04  Data: 0.010 (0.013)
Train: 152 [1000/1251 ( 80%)]  Loss: 3.460 (3.68)  Time: 0.795s, 1288.23/s  (0.803s, 1275.23/s)  LR: 8.513e-04  Data: 0.010 (0.013)
Train: 152 [1050/1251 ( 84%)]  Loss: 3.426 (3.67)  Time: 0.842s, 1215.73/s  (0.803s, 1274.96/s)  LR: 8.513e-04  Data: 0.010 (0.013)
Train: 152 [1100/1251 ( 88%)]  Loss: 3.559 (3.67)  Time: 0.828s, 1236.38/s  (0.803s, 1274.64/s)  LR: 8.513e-04  Data: 0.010 (0.013)
Train: 152 [1150/1251 ( 92%)]  Loss: 3.808 (3.67)  Time: 0.819s, 1249.78/s  (0.803s, 1274.68/s)  LR: 8.513e-04  Data: 0.010 (0.013)
Train: 152 [1200/1251 ( 96%)]  Loss: 3.730 (3.68)  Time: 0.774s, 1323.74/s  (0.803s, 1275.00/s)  LR: 8.513e-04  Data: 0.010 (0.013)
Train: 152 [1250/1251 (100%)]  Loss: 3.642 (3.67)  Time: 0.796s, 1286.17/s  (0.803s, 1275.18/s)  LR: 8.513e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.711 (1.711)  Loss:  0.7544 (0.7544)  Acc@1: 88.4766 (88.4766)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.194 (0.598)  Loss:  0.8892 (1.3860)  Acc@1: 83.3726 (72.5880)  Acc@5: 96.9340 (91.4900)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-150.pth.tar', 72.78200006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-138.pth.tar', 72.72799998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-147.pth.tar', 72.68999996826172)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-151.pth.tar', 72.6840000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-148.pth.tar', 72.67799998779297)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-152.pth.tar', 72.58800001708984)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-139.pth.tar', 72.52199993652344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-132.pth.tar', 72.52000001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-133.pth.tar', 72.51400014648438)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-130.pth.tar', 72.46799991699218)

Train: 153 [   0/1251 (  0%)]  Loss: 3.527 (3.53)  Time: 2.506s,  408.65/s  (2.506s,  408.65/s)  LR: 8.495e-04  Data: 1.720 (1.720)
Train: 153 [  50/1251 (  4%)]  Loss: 4.170 (3.85)  Time: 0.777s, 1318.35/s  (0.837s, 1223.62/s)  LR: 8.495e-04  Data: 0.010 (0.050)
Train: 153 [ 100/1251 (  8%)]  Loss: 3.802 (3.83)  Time: 0.835s, 1226.33/s  (0.820s, 1248.88/s)  LR: 8.495e-04  Data: 0.014 (0.031)
Train: 153 [ 150/1251 ( 12%)]  Loss: 4.212 (3.93)  Time: 0.813s, 1259.64/s  (0.815s, 1256.65/s)  LR: 8.495e-04  Data: 0.010 (0.024)
Train: 153 [ 200/1251 ( 16%)]  Loss: 3.524 (3.85)  Time: 0.785s, 1304.07/s  (0.812s, 1261.40/s)  LR: 8.495e-04  Data: 0.016 (0.021)
Train: 153 [ 250/1251 ( 20%)]  Loss: 3.466 (3.78)  Time: 0.793s, 1290.57/s  (0.810s, 1264.24/s)  LR: 8.495e-04  Data: 0.012 (0.019)
Train: 153 [ 300/1251 ( 24%)]  Loss: 3.558 (3.75)  Time: 0.785s, 1305.07/s  (0.808s, 1266.71/s)  LR: 8.495e-04  Data: 0.011 (0.018)
Train: 153 [ 350/1251 ( 28%)]  Loss: 3.418 (3.71)  Time: 0.794s, 1289.12/s  (0.807s, 1268.60/s)  LR: 8.495e-04  Data: 0.010 (0.017)
Train: 153 [ 400/1251 ( 32%)]  Loss: 3.455 (3.68)  Time: 0.773s, 1324.39/s  (0.807s, 1268.98/s)  LR: 8.495e-04  Data: 0.010 (0.016)
Train: 153 [ 450/1251 ( 36%)]  Loss: 3.727 (3.69)  Time: 0.774s, 1322.61/s  (0.806s, 1270.64/s)  LR: 8.495e-04  Data: 0.010 (0.016)
Train: 153 [ 500/1251 ( 40%)]  Loss: 3.678 (3.69)  Time: 0.774s, 1323.15/s  (0.805s, 1271.60/s)  LR: 8.495e-04  Data: 0.010 (0.015)
Train: 153 [ 550/1251 ( 44%)]  Loss: 3.381 (3.66)  Time: 0.772s, 1326.02/s  (0.805s, 1271.50/s)  LR: 8.495e-04  Data: 0.010 (0.015)
Train: 153 [ 600/1251 ( 48%)]  Loss: 3.532 (3.65)  Time: 0.786s, 1303.23/s  (0.805s, 1272.52/s)  LR: 8.495e-04  Data: 0.010 (0.014)
Train: 153 [ 650/1251 ( 52%)]  Loss: 3.460 (3.64)  Time: 0.780s, 1312.12/s  (0.805s, 1272.53/s)  LR: 8.495e-04  Data: 0.015 (0.014)
Train: 153 [ 700/1251 ( 56%)]  Loss: 4.097 (3.67)  Time: 0.775s, 1321.78/s  (0.804s, 1273.58/s)  LR: 8.495e-04  Data: 0.010 (0.014)
Train: 153 [ 750/1251 ( 60%)]  Loss: 3.954 (3.69)  Time: 0.812s, 1261.29/s  (0.804s, 1274.40/s)  LR: 8.495e-04  Data: 0.010 (0.014)
Train: 153 [ 800/1251 ( 64%)]  Loss: 3.510 (3.67)  Time: 0.836s, 1224.93/s  (0.803s, 1275.25/s)  LR: 8.495e-04  Data: 0.014 (0.014)
Train: 153 [ 850/1251 ( 68%)]  Loss: 3.747 (3.68)  Time: 0.835s, 1225.74/s  (0.803s, 1275.46/s)  LR: 8.495e-04  Data: 0.009 (0.013)
Train: 153 [ 900/1251 ( 72%)]  Loss: 3.698 (3.68)  Time: 0.868s, 1179.20/s  (0.803s, 1274.95/s)  LR: 8.495e-04  Data: 0.010 (0.013)
Train: 153 [ 950/1251 ( 76%)]  Loss: 3.712 (3.68)  Time: 0.791s, 1294.10/s  (0.803s, 1275.43/s)  LR: 8.495e-04  Data: 0.011 (0.013)
Train: 153 [1000/1251 ( 80%)]  Loss: 3.607 (3.68)  Time: 0.797s, 1285.05/s  (0.803s, 1275.70/s)  LR: 8.495e-04  Data: 0.010 (0.013)
Train: 153 [1050/1251 ( 84%)]  Loss: 3.559 (3.67)  Time: 0.782s, 1310.02/s  (0.803s, 1275.98/s)  LR: 8.495e-04  Data: 0.015 (0.013)
Train: 153 [1100/1251 ( 88%)]  Loss: 3.502 (3.67)  Time: 0.799s, 1281.97/s  (0.803s, 1275.77/s)  LR: 8.495e-04  Data: 0.010 (0.013)
Train: 153 [1150/1251 ( 92%)]  Loss: 3.939 (3.68)  Time: 0.811s, 1263.14/s  (0.803s, 1275.82/s)  LR: 8.495e-04  Data: 0.009 (0.013)
Train: 153 [1200/1251 ( 96%)]  Loss: 3.945 (3.69)  Time: 0.827s, 1238.42/s  (0.803s, 1275.69/s)  LR: 8.495e-04  Data: 0.010 (0.013)
Train: 153 [1250/1251 (100%)]  Loss: 3.751 (3.69)  Time: 0.787s, 1301.17/s  (0.803s, 1275.73/s)  LR: 8.495e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.587 (1.587)  Loss:  1.0000 (1.0000)  Acc@1: 87.7930 (87.7930)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.194 (0.601)  Loss:  1.0322 (1.4895)  Acc@1: 83.0189 (72.6400)  Acc@5: 96.3443 (91.5660)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-150.pth.tar', 72.78200006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-138.pth.tar', 72.72799998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-147.pth.tar', 72.68999996826172)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-151.pth.tar', 72.6840000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-148.pth.tar', 72.67799998779297)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-153.pth.tar', 72.64000012207032)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-152.pth.tar', 72.58800001708984)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-139.pth.tar', 72.52199993652344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-132.pth.tar', 72.52000001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-133.pth.tar', 72.51400014648438)

Train: 154 [   0/1251 (  0%)]  Loss: 3.210 (3.21)  Time: 2.577s,  397.34/s  (2.577s,  397.34/s)  LR: 8.476e-04  Data: 1.832 (1.832)
Train: 154 [  50/1251 (  4%)]  Loss: 4.045 (3.63)  Time: 0.796s, 1287.08/s  (0.838s, 1222.50/s)  LR: 8.476e-04  Data: 0.014 (0.047)
Train: 154 [ 100/1251 (  8%)]  Loss: 3.762 (3.67)  Time: 0.786s, 1303.32/s  (0.819s, 1249.63/s)  LR: 8.476e-04  Data: 0.010 (0.029)
Train: 154 [ 150/1251 ( 12%)]  Loss: 3.627 (3.66)  Time: 0.793s, 1291.40/s  (0.813s, 1260.18/s)  LR: 8.476e-04  Data: 0.014 (0.023)
Train: 154 [ 200/1251 ( 16%)]  Loss: 3.981 (3.73)  Time: 0.807s, 1268.14/s  (0.811s, 1263.20/s)  LR: 8.476e-04  Data: 0.017 (0.020)
Train: 154 [ 250/1251 ( 20%)]  Loss: 3.935 (3.76)  Time: 0.802s, 1276.65/s  (0.810s, 1264.65/s)  LR: 8.476e-04  Data: 0.011 (0.019)
Train: 154 [ 300/1251 ( 24%)]  Loss: 3.259 (3.69)  Time: 0.782s, 1310.07/s  (0.808s, 1266.74/s)  LR: 8.476e-04  Data: 0.010 (0.017)
Train: 154 [ 350/1251 ( 28%)]  Loss: 3.587 (3.68)  Time: 0.901s, 1136.79/s  (0.808s, 1266.77/s)  LR: 8.476e-04  Data: 0.011 (0.017)
Train: 154 [ 400/1251 ( 32%)]  Loss: 3.841 (3.69)  Time: 0.813s, 1259.07/s  (0.811s, 1263.06/s)  LR: 8.476e-04  Data: 0.015 (0.016)
Train: 154 [ 450/1251 ( 36%)]  Loss: 2.990 (3.62)  Time: 0.777s, 1317.08/s  (0.808s, 1267.81/s)  LR: 8.476e-04  Data: 0.010 (0.015)
Train: 154 [ 500/1251 ( 40%)]  Loss: 4.189 (3.68)  Time: 0.775s, 1321.04/s  (0.805s, 1272.34/s)  LR: 8.476e-04  Data: 0.011 (0.015)
Train: 154 [ 550/1251 ( 44%)]  Loss: 3.271 (3.64)  Time: 0.772s, 1326.97/s  (0.804s, 1272.93/s)  LR: 8.476e-04  Data: 0.010 (0.015)
Train: 154 [ 600/1251 ( 48%)]  Loss: 3.961 (3.67)  Time: 0.771s, 1328.26/s  (0.804s, 1274.29/s)  LR: 8.476e-04  Data: 0.010 (0.014)
Train: 154 [ 650/1251 ( 52%)]  Loss: 3.784 (3.67)  Time: 0.782s, 1309.09/s  (0.803s, 1274.64/s)  LR: 8.476e-04  Data: 0.009 (0.014)
Train: 154 [ 700/1251 ( 56%)]  Loss: 3.768 (3.68)  Time: 0.813s, 1259.73/s  (0.803s, 1274.93/s)  LR: 8.476e-04  Data: 0.009 (0.014)
Train: 154 [ 750/1251 ( 60%)]  Loss: 3.795 (3.69)  Time: 0.775s, 1321.62/s  (0.803s, 1274.93/s)  LR: 8.476e-04  Data: 0.010 (0.014)
Train: 154 [ 800/1251 ( 64%)]  Loss: 3.342 (3.67)  Time: 0.829s, 1235.25/s  (0.803s, 1275.06/s)  LR: 8.476e-04  Data: 0.010 (0.013)
Train: 154 [ 850/1251 ( 68%)]  Loss: 3.906 (3.68)  Time: 0.784s, 1306.82/s  (0.803s, 1275.60/s)  LR: 8.476e-04  Data: 0.010 (0.013)
Train: 154 [ 900/1251 ( 72%)]  Loss: 4.097 (3.70)  Time: 0.774s, 1322.21/s  (0.803s, 1275.49/s)  LR: 8.476e-04  Data: 0.010 (0.013)
Train: 154 [ 950/1251 ( 76%)]  Loss: 3.672 (3.70)  Time: 0.769s, 1331.06/s  (0.803s, 1275.35/s)  LR: 8.476e-04  Data: 0.010 (0.013)
Train: 154 [1000/1251 ( 80%)]  Loss: 3.751 (3.70)  Time: 0.771s, 1327.52/s  (0.803s, 1275.98/s)  LR: 8.476e-04  Data: 0.010 (0.013)
Train: 154 [1050/1251 ( 84%)]  Loss: 3.486 (3.69)  Time: 0.822s, 1246.49/s  (0.803s, 1275.45/s)  LR: 8.476e-04  Data: 0.010 (0.013)
Train: 154 [1100/1251 ( 88%)]  Loss: 3.813 (3.70)  Time: 0.872s, 1174.35/s  (0.803s, 1275.00/s)  LR: 8.476e-04  Data: 0.010 (0.013)
Train: 154 [1150/1251 ( 92%)]  Loss: 3.814 (3.70)  Time: 0.812s, 1261.24/s  (0.803s, 1275.34/s)  LR: 8.476e-04  Data: 0.009 (0.013)
Train: 154 [1200/1251 ( 96%)]  Loss: 3.961 (3.71)  Time: 0.812s, 1261.67/s  (0.803s, 1275.91/s)  LR: 8.476e-04  Data: 0.015 (0.013)
Train: 154 [1250/1251 (100%)]  Loss: 3.688 (3.71)  Time: 0.789s, 1298.06/s  (0.802s, 1276.17/s)  LR: 8.476e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.534 (1.534)  Loss:  0.7061 (0.7061)  Acc@1: 88.4766 (88.4766)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.194 (0.589)  Loss:  0.9023 (1.3753)  Acc@1: 84.0802 (72.7020)  Acc@5: 95.7547 (91.4640)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-150.pth.tar', 72.78200006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-138.pth.tar', 72.72799998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-154.pth.tar', 72.70200006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-147.pth.tar', 72.68999996826172)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-151.pth.tar', 72.6840000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-148.pth.tar', 72.67799998779297)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-153.pth.tar', 72.64000012207032)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-152.pth.tar', 72.58800001708984)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-139.pth.tar', 72.52199993652344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-132.pth.tar', 72.52000001220703)

Train: 155 [   0/1251 (  0%)]  Loss: 3.721 (3.72)  Time: 2.426s,  422.16/s  (2.426s,  422.16/s)  LR: 8.457e-04  Data: 1.701 (1.701)
Train: 155 [  50/1251 (  4%)]  Loss: 3.615 (3.67)  Time: 0.801s, 1278.09/s  (0.840s, 1218.99/s)  LR: 8.457e-04  Data: 0.015 (0.052)
Train: 155 [ 100/1251 (  8%)]  Loss: 3.822 (3.72)  Time: 0.785s, 1304.10/s  (0.819s, 1250.96/s)  LR: 8.457e-04  Data: 0.010 (0.032)
Train: 155 [ 150/1251 ( 12%)]  Loss: 3.559 (3.68)  Time: 0.924s, 1108.64/s  (0.815s, 1256.81/s)  LR: 8.457e-04  Data: 0.014 (0.025)
Train: 155 [ 200/1251 ( 16%)]  Loss: 3.741 (3.69)  Time: 0.787s, 1300.36/s  (0.811s, 1262.70/s)  LR: 8.457e-04  Data: 0.015 (0.021)
Train: 155 [ 250/1251 ( 20%)]  Loss: 3.807 (3.71)  Time: 0.820s, 1248.29/s  (0.810s, 1264.91/s)  LR: 8.457e-04  Data: 0.010 (0.019)
Train: 155 [ 300/1251 ( 24%)]  Loss: 3.350 (3.66)  Time: 0.832s, 1230.12/s  (0.808s, 1267.12/s)  LR: 8.457e-04  Data: 0.012 (0.018)
Train: 155 [ 350/1251 ( 28%)]  Loss: 4.058 (3.71)  Time: 0.808s, 1267.42/s  (0.807s, 1269.23/s)  LR: 8.457e-04  Data: 0.010 (0.017)
Train: 155 [ 400/1251 ( 32%)]  Loss: 3.737 (3.71)  Time: 0.785s, 1303.77/s  (0.806s, 1269.89/s)  LR: 8.457e-04  Data: 0.010 (0.016)
Train: 155 [ 450/1251 ( 36%)]  Loss: 3.927 (3.73)  Time: 0.773s, 1325.39/s  (0.806s, 1271.21/s)  LR: 8.457e-04  Data: 0.010 (0.016)
Train: 155 [ 500/1251 ( 40%)]  Loss: 3.145 (3.68)  Time: 0.785s, 1304.74/s  (0.805s, 1271.69/s)  LR: 8.457e-04  Data: 0.014 (0.015)
Train: 155 [ 550/1251 ( 44%)]  Loss: 3.745 (3.69)  Time: 0.815s, 1256.65/s  (0.805s, 1271.43/s)  LR: 8.457e-04  Data: 0.011 (0.015)
Train: 155 [ 600/1251 ( 48%)]  Loss: 3.696 (3.69)  Time: 0.804s, 1273.62/s  (0.805s, 1271.59/s)  LR: 8.457e-04  Data: 0.010 (0.014)
Train: 155 [ 650/1251 ( 52%)]  Loss: 3.536 (3.68)  Time: 0.855s, 1197.96/s  (0.805s, 1272.07/s)  LR: 8.457e-04  Data: 0.010 (0.014)
Train: 155 [ 700/1251 ( 56%)]  Loss: 3.611 (3.67)  Time: 0.787s, 1300.80/s  (0.805s, 1272.55/s)  LR: 8.457e-04  Data: 0.009 (0.014)
Train: 155 [ 750/1251 ( 60%)]  Loss: 3.698 (3.67)  Time: 0.786s, 1302.82/s  (0.804s, 1273.31/s)  LR: 8.457e-04  Data: 0.014 (0.014)
Train: 155 [ 800/1251 ( 64%)]  Loss: 3.213 (3.65)  Time: 0.818s, 1251.91/s  (0.804s, 1273.87/s)  LR: 8.457e-04  Data: 0.010 (0.014)
Train: 155 [ 850/1251 ( 68%)]  Loss: 3.527 (3.64)  Time: 0.811s, 1263.13/s  (0.803s, 1274.75/s)  LR: 8.457e-04  Data: 0.010 (0.013)
Train: 155 [ 900/1251 ( 72%)]  Loss: 3.265 (3.62)  Time: 0.876s, 1169.42/s  (0.803s, 1274.57/s)  LR: 8.457e-04  Data: 0.010 (0.013)
Train: 155 [ 950/1251 ( 76%)]  Loss: 3.674 (3.62)  Time: 0.817s, 1253.58/s  (0.803s, 1274.55/s)  LR: 8.457e-04  Data: 0.013 (0.013)
Train: 155 [1000/1251 ( 80%)]  Loss: 3.695 (3.63)  Time: 0.849s, 1205.71/s  (0.803s, 1274.59/s)  LR: 8.457e-04  Data: 0.010 (0.013)
Train: 155 [1050/1251 ( 84%)]  Loss: 3.678 (3.63)  Time: 0.817s, 1253.20/s  (0.803s, 1274.80/s)  LR: 8.457e-04  Data: 0.011 (0.013)
Train: 155 [1100/1251 ( 88%)]  Loss: 3.916 (3.64)  Time: 0.814s, 1257.44/s  (0.803s, 1274.64/s)  LR: 8.457e-04  Data: 0.011 (0.013)
Train: 155 [1150/1251 ( 92%)]  Loss: 3.722 (3.64)  Time: 0.789s, 1297.73/s  (0.803s, 1275.28/s)  LR: 8.457e-04  Data: 0.018 (0.013)
Train: 155 [1200/1251 ( 96%)]  Loss: 3.660 (3.64)  Time: 0.814s, 1258.51/s  (0.803s, 1275.17/s)  LR: 8.457e-04  Data: 0.013 (0.013)
Train: 155 [1250/1251 (100%)]  Loss: 3.825 (3.65)  Time: 0.768s, 1333.71/s  (0.803s, 1275.18/s)  LR: 8.457e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.736 (1.736)  Loss:  0.7490 (0.7490)  Acc@1: 88.7695 (88.7695)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.194 (0.592)  Loss:  0.8984 (1.3857)  Acc@1: 82.9009 (73.0800)  Acc@5: 95.7547 (91.6120)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-155.pth.tar', 73.08000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-150.pth.tar', 72.78200006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-138.pth.tar', 72.72799998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-154.pth.tar', 72.70200006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-147.pth.tar', 72.68999996826172)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-151.pth.tar', 72.6840000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-148.pth.tar', 72.67799998779297)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-153.pth.tar', 72.64000012207032)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-152.pth.tar', 72.58800001708984)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-139.pth.tar', 72.52199993652344)

Train: 156 [   0/1251 (  0%)]  Loss: 3.653 (3.65)  Time: 2.346s,  436.45/s  (2.346s,  436.45/s)  LR: 8.439e-04  Data: 1.617 (1.617)
Train: 156 [  50/1251 (  4%)]  Loss: 3.608 (3.63)  Time: 0.812s, 1261.75/s  (0.843s, 1215.25/s)  LR: 8.439e-04  Data: 0.013 (0.048)
Train: 156 [ 100/1251 (  8%)]  Loss: 3.947 (3.74)  Time: 0.782s, 1309.08/s  (0.824s, 1242.22/s)  LR: 8.439e-04  Data: 0.010 (0.030)
Train: 156 [ 150/1251 ( 12%)]  Loss: 3.478 (3.67)  Time: 0.848s, 1206.86/s  (0.815s, 1256.45/s)  LR: 8.439e-04  Data: 0.011 (0.024)
Train: 156 [ 200/1251 ( 16%)]  Loss: 3.755 (3.69)  Time: 0.770s, 1329.07/s  (0.811s, 1261.96/s)  LR: 8.439e-04  Data: 0.010 (0.020)
Train: 156 [ 250/1251 ( 20%)]  Loss: 3.468 (3.65)  Time: 0.804s, 1274.14/s  (0.809s, 1265.06/s)  LR: 8.439e-04  Data: 0.010 (0.019)
Train: 156 [ 300/1251 ( 24%)]  Loss: 3.238 (3.59)  Time: 0.771s, 1328.37/s  (0.808s, 1268.04/s)  LR: 8.439e-04  Data: 0.010 (0.017)
Train: 156 [ 350/1251 ( 28%)]  Loss: 3.820 (3.62)  Time: 0.788s, 1298.70/s  (0.807s, 1268.95/s)  LR: 8.439e-04  Data: 0.014 (0.016)
Train: 156 [ 400/1251 ( 32%)]  Loss: 3.757 (3.64)  Time: 0.796s, 1286.98/s  (0.806s, 1270.03/s)  LR: 8.439e-04  Data: 0.011 (0.016)
Train: 156 [ 450/1251 ( 36%)]  Loss: 3.599 (3.63)  Time: 0.800s, 1280.74/s  (0.806s, 1270.98/s)  LR: 8.439e-04  Data: 0.010 (0.015)
Train: 156 [ 500/1251 ( 40%)]  Loss: 3.647 (3.63)  Time: 0.824s, 1243.09/s  (0.805s, 1272.23/s)  LR: 8.439e-04  Data: 0.013 (0.015)
Train: 156 [ 550/1251 ( 44%)]  Loss: 3.452 (3.62)  Time: 0.841s, 1218.15/s  (0.805s, 1272.05/s)  LR: 8.439e-04  Data: 0.016 (0.014)
Train: 156 [ 600/1251 ( 48%)]  Loss: 3.848 (3.64)  Time: 0.792s, 1292.50/s  (0.805s, 1272.43/s)  LR: 8.439e-04  Data: 0.012 (0.014)
Train: 156 [ 650/1251 ( 52%)]  Loss: 3.866 (3.65)  Time: 0.776s, 1319.91/s  (0.804s, 1273.34/s)  LR: 8.439e-04  Data: 0.010 (0.014)
Train: 156 [ 700/1251 ( 56%)]  Loss: 3.428 (3.64)  Time: 0.792s, 1292.62/s  (0.804s, 1273.74/s)  LR: 8.439e-04  Data: 0.010 (0.014)
Train: 156 [ 750/1251 ( 60%)]  Loss: 3.609 (3.64)  Time: 0.772s, 1325.62/s  (0.804s, 1273.89/s)  LR: 8.439e-04  Data: 0.010 (0.014)
Train: 156 [ 800/1251 ( 64%)]  Loss: 3.390 (3.62)  Time: 0.773s, 1325.17/s  (0.803s, 1274.50/s)  LR: 8.439e-04  Data: 0.010 (0.013)
Train: 156 [ 850/1251 ( 68%)]  Loss: 3.749 (3.63)  Time: 0.777s, 1317.39/s  (0.803s, 1275.48/s)  LR: 8.439e-04  Data: 0.010 (0.013)
Train: 156 [ 900/1251 ( 72%)]  Loss: 3.528 (3.62)  Time: 0.793s, 1290.87/s  (0.803s, 1275.79/s)  LR: 8.439e-04  Data: 0.015 (0.013)
Train: 156 [ 950/1251 ( 76%)]  Loss: 3.528 (3.62)  Time: 0.806s, 1270.93/s  (0.802s, 1276.13/s)  LR: 8.439e-04  Data: 0.010 (0.013)
Train: 156 [1000/1251 ( 80%)]  Loss: 3.615 (3.62)  Time: 0.834s, 1227.59/s  (0.803s, 1275.99/s)  LR: 8.439e-04  Data: 0.010 (0.013)
Train: 156 [1050/1251 ( 84%)]  Loss: 3.629 (3.62)  Time: 0.772s, 1325.68/s  (0.803s, 1275.89/s)  LR: 8.439e-04  Data: 0.011 (0.013)
Train: 156 [1100/1251 ( 88%)]  Loss: 3.831 (3.63)  Time: 0.838s, 1222.01/s  (0.803s, 1275.64/s)  LR: 8.439e-04  Data: 0.011 (0.013)
Train: 156 [1150/1251 ( 92%)]  Loss: 3.535 (3.62)  Time: 0.803s, 1275.08/s  (0.803s, 1275.98/s)  LR: 8.439e-04  Data: 0.011 (0.013)
Train: 156 [1200/1251 ( 96%)]  Loss: 3.797 (3.63)  Time: 0.811s, 1263.17/s  (0.802s, 1276.07/s)  LR: 8.439e-04  Data: 0.010 (0.013)
Train: 156 [1250/1251 (100%)]  Loss: 3.382 (3.62)  Time: 0.788s, 1299.27/s  (0.803s, 1275.98/s)  LR: 8.439e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.610 (1.610)  Loss:  0.7539 (0.7539)  Acc@1: 88.2812 (88.2812)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.194 (0.600)  Loss:  0.7998 (1.3345)  Acc@1: 84.5519 (72.8980)  Acc@5: 95.8727 (91.6180)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-155.pth.tar', 73.08000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-156.pth.tar', 72.89800001220704)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-150.pth.tar', 72.78200006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-138.pth.tar', 72.72799998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-154.pth.tar', 72.70200006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-147.pth.tar', 72.68999996826172)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-151.pth.tar', 72.6840000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-148.pth.tar', 72.67799998779297)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-153.pth.tar', 72.64000012207032)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-152.pth.tar', 72.58800001708984)

Train: 157 [   0/1251 (  0%)]  Loss: 3.873 (3.87)  Time: 2.475s,  413.78/s  (2.475s,  413.78/s)  LR: 8.420e-04  Data: 1.701 (1.701)
Train: 157 [  50/1251 (  4%)]  Loss: 3.640 (3.76)  Time: 0.773s, 1324.89/s  (0.842s, 1216.05/s)  LR: 8.420e-04  Data: 0.011 (0.052)
Train: 157 [ 100/1251 (  8%)]  Loss: 3.831 (3.78)  Time: 0.794s, 1290.08/s  (0.821s, 1247.82/s)  LR: 8.420e-04  Data: 0.011 (0.032)
Train: 157 [ 150/1251 ( 12%)]  Loss: 4.038 (3.85)  Time: 0.802s, 1277.37/s  (0.815s, 1256.85/s)  LR: 8.420e-04  Data: 0.010 (0.025)
Train: 157 [ 200/1251 ( 16%)]  Loss: 3.275 (3.73)  Time: 0.817s, 1252.78/s  (0.812s, 1261.71/s)  LR: 8.420e-04  Data: 0.011 (0.021)
Train: 157 [ 250/1251 ( 20%)]  Loss: 3.376 (3.67)  Time: 0.809s, 1265.79/s  (0.810s, 1264.65/s)  LR: 8.420e-04  Data: 0.010 (0.019)
Train: 157 [ 300/1251 ( 24%)]  Loss: 3.180 (3.60)  Time: 0.775s, 1321.44/s  (0.809s, 1265.28/s)  LR: 8.420e-04  Data: 0.010 (0.018)
Train: 157 [ 350/1251 ( 28%)]  Loss: 3.577 (3.60)  Time: 0.787s, 1300.71/s  (0.807s, 1268.13/s)  LR: 8.420e-04  Data: 0.010 (0.017)
Train: 157 [ 400/1251 ( 32%)]  Loss: 3.673 (3.61)  Time: 0.855s, 1197.54/s  (0.806s, 1270.08/s)  LR: 8.420e-04  Data: 0.010 (0.016)
Train: 157 [ 450/1251 ( 36%)]  Loss: 3.732 (3.62)  Time: 0.802s, 1277.34/s  (0.806s, 1270.81/s)  LR: 8.420e-04  Data: 0.010 (0.015)
Train: 157 [ 500/1251 ( 40%)]  Loss: 3.745 (3.63)  Time: 0.781s, 1311.07/s  (0.805s, 1271.52/s)  LR: 8.420e-04  Data: 0.012 (0.015)
Train: 157 [ 550/1251 ( 44%)]  Loss: 3.983 (3.66)  Time: 0.795s, 1288.54/s  (0.805s, 1271.57/s)  LR: 8.420e-04  Data: 0.010 (0.015)
Train: 157 [ 600/1251 ( 48%)]  Loss: 3.391 (3.64)  Time: 0.779s, 1314.19/s  (0.805s, 1272.55/s)  LR: 8.420e-04  Data: 0.009 (0.014)
Train: 157 [ 650/1251 ( 52%)]  Loss: 3.948 (3.66)  Time: 0.847s, 1209.53/s  (0.804s, 1273.09/s)  LR: 8.420e-04  Data: 0.010 (0.014)
Train: 157 [ 700/1251 ( 56%)]  Loss: 3.550 (3.65)  Time: 0.779s, 1313.75/s  (0.804s, 1273.85/s)  LR: 8.420e-04  Data: 0.010 (0.014)
Train: 157 [ 750/1251 ( 60%)]  Loss: 3.868 (3.67)  Time: 0.795s, 1287.56/s  (0.803s, 1274.53/s)  LR: 8.420e-04  Data: 0.010 (0.014)
Train: 157 [ 800/1251 ( 64%)]  Loss: 3.775 (3.67)  Time: 0.820s, 1248.11/s  (0.803s, 1275.41/s)  LR: 8.420e-04  Data: 0.010 (0.013)
Train: 157 [ 850/1251 ( 68%)]  Loss: 3.361 (3.66)  Time: 0.806s, 1270.93/s  (0.803s, 1275.63/s)  LR: 8.420e-04  Data: 0.011 (0.013)
Train: 157 [ 900/1251 ( 72%)]  Loss: 3.749 (3.66)  Time: 0.775s, 1320.78/s  (0.803s, 1275.59/s)  LR: 8.420e-04  Data: 0.010 (0.013)
Train: 157 [ 950/1251 ( 76%)]  Loss: 3.403 (3.65)  Time: 0.832s, 1230.58/s  (0.802s, 1276.40/s)  LR: 8.420e-04  Data: 0.010 (0.013)
Train: 157 [1000/1251 ( 80%)]  Loss: 3.751 (3.65)  Time: 0.832s, 1230.59/s  (0.802s, 1276.66/s)  LR: 8.420e-04  Data: 0.014 (0.013)
Train: 157 [1050/1251 ( 84%)]  Loss: 3.780 (3.66)  Time: 0.772s, 1326.94/s  (0.802s, 1277.07/s)  LR: 8.420e-04  Data: 0.012 (0.013)
Train: 157 [1100/1251 ( 88%)]  Loss: 3.577 (3.66)  Time: 0.775s, 1321.69/s  (0.802s, 1277.56/s)  LR: 8.420e-04  Data: 0.010 (0.013)
Train: 157 [1150/1251 ( 92%)]  Loss: 3.527 (3.65)  Time: 0.796s, 1286.54/s  (0.801s, 1277.97/s)  LR: 8.420e-04  Data: 0.010 (0.013)
Train: 157 [1200/1251 ( 96%)]  Loss: 3.608 (3.65)  Time: 0.941s, 1088.32/s  (0.801s, 1278.18/s)  LR: 8.420e-04  Data: 0.010 (0.013)
Train: 157 [1250/1251 (100%)]  Loss: 3.668 (3.65)  Time: 0.774s, 1323.45/s  (0.801s, 1278.09/s)  LR: 8.420e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.680 (1.680)  Loss:  0.9229 (0.9229)  Acc@1: 87.4023 (87.4023)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.194 (0.595)  Loss:  0.9072 (1.4386)  Acc@1: 83.2547 (72.7980)  Acc@5: 95.4009 (91.5320)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-155.pth.tar', 73.08000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-156.pth.tar', 72.89800001220704)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-157.pth.tar', 72.79800009521485)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-150.pth.tar', 72.78200006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-138.pth.tar', 72.72799998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-154.pth.tar', 72.70200006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-147.pth.tar', 72.68999996826172)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-151.pth.tar', 72.6840000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-148.pth.tar', 72.67799998779297)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-153.pth.tar', 72.64000012207032)

Train: 158 [   0/1251 (  0%)]  Loss: 3.781 (3.78)  Time: 2.597s,  394.36/s  (2.597s,  394.36/s)  LR: 8.401e-04  Data: 1.851 (1.851)
Train: 158 [  50/1251 (  4%)]  Loss: 4.045 (3.91)  Time: 0.773s, 1324.22/s  (0.837s, 1222.92/s)  LR: 8.401e-04  Data: 0.010 (0.047)
Train: 158 [ 100/1251 (  8%)]  Loss: 3.467 (3.76)  Time: 0.845s, 1211.81/s  (0.821s, 1247.41/s)  LR: 8.401e-04  Data: 0.010 (0.029)
Train: 158 [ 150/1251 ( 12%)]  Loss: 3.666 (3.74)  Time: 0.820s, 1248.77/s  (0.813s, 1259.85/s)  LR: 8.401e-04  Data: 0.010 (0.023)
Train: 158 [ 200/1251 ( 16%)]  Loss: 3.722 (3.74)  Time: 0.852s, 1202.51/s  (0.811s, 1263.03/s)  LR: 8.401e-04  Data: 0.010 (0.020)
Train: 158 [ 250/1251 ( 20%)]  Loss: 3.530 (3.70)  Time: 0.809s, 1265.71/s  (0.809s, 1266.24/s)  LR: 8.401e-04  Data: 0.010 (0.019)
Train: 158 [ 300/1251 ( 24%)]  Loss: 3.798 (3.72)  Time: 0.793s, 1292.09/s  (0.807s, 1268.38/s)  LR: 8.401e-04  Data: 0.012 (0.017)
Train: 158 [ 350/1251 ( 28%)]  Loss: 3.690 (3.71)  Time: 0.774s, 1322.52/s  (0.808s, 1267.88/s)  LR: 8.401e-04  Data: 0.010 (0.016)
Train: 158 [ 400/1251 ( 32%)]  Loss: 3.805 (3.72)  Time: 0.799s, 1281.32/s  (0.808s, 1267.09/s)  LR: 8.401e-04  Data: 0.010 (0.016)
Train: 158 [ 450/1251 ( 36%)]  Loss: 3.440 (3.69)  Time: 0.771s, 1328.36/s  (0.807s, 1268.73/s)  LR: 8.401e-04  Data: 0.010 (0.015)
Train: 158 [ 500/1251 ( 40%)]  Loss: 3.880 (3.71)  Time: 0.794s, 1289.53/s  (0.806s, 1271.23/s)  LR: 8.401e-04  Data: 0.012 (0.015)
Train: 158 [ 550/1251 ( 44%)]  Loss: 3.714 (3.71)  Time: 0.771s, 1327.70/s  (0.805s, 1271.75/s)  LR: 8.401e-04  Data: 0.011 (0.014)
Train: 158 [ 600/1251 ( 48%)]  Loss: 3.756 (3.71)  Time: 0.792s, 1293.35/s  (0.805s, 1272.76/s)  LR: 8.401e-04  Data: 0.010 (0.014)
Train: 158 [ 650/1251 ( 52%)]  Loss: 3.554 (3.70)  Time: 0.777s, 1317.37/s  (0.804s, 1273.45/s)  LR: 8.401e-04  Data: 0.010 (0.014)
Train: 158 [ 700/1251 ( 56%)]  Loss: 3.822 (3.71)  Time: 0.775s, 1320.93/s  (0.805s, 1272.34/s)  LR: 8.401e-04  Data: 0.010 (0.014)
Train: 158 [ 750/1251 ( 60%)]  Loss: 3.355 (3.69)  Time: 0.808s, 1266.79/s  (0.805s, 1272.78/s)  LR: 8.401e-04  Data: 0.030 (0.014)
Train: 158 [ 800/1251 ( 64%)]  Loss: 3.224 (3.66)  Time: 0.770s, 1329.94/s  (0.805s, 1272.66/s)  LR: 8.401e-04  Data: 0.010 (0.013)
Train: 158 [ 850/1251 ( 68%)]  Loss: 3.569 (3.66)  Time: 0.789s, 1298.05/s  (0.804s, 1272.91/s)  LR: 8.401e-04  Data: 0.010 (0.013)
Train: 158 [ 900/1251 ( 72%)]  Loss: 3.822 (3.67)  Time: 0.779s, 1314.84/s  (0.804s, 1273.39/s)  LR: 8.401e-04  Data: 0.010 (0.013)
Train: 158 [ 950/1251 ( 76%)]  Loss: 3.577 (3.66)  Time: 0.800s, 1279.33/s  (0.804s, 1273.28/s)  LR: 8.401e-04  Data: 0.011 (0.013)
Train: 158 [1000/1251 ( 80%)]  Loss: 3.454 (3.65)  Time: 0.789s, 1297.31/s  (0.804s, 1274.01/s)  LR: 8.401e-04  Data: 0.010 (0.013)
Train: 158 [1050/1251 ( 84%)]  Loss: 3.718 (3.65)  Time: 0.780s, 1312.64/s  (0.804s, 1273.62/s)  LR: 8.401e-04  Data: 0.016 (0.013)
Train: 158 [1100/1251 ( 88%)]  Loss: 3.545 (3.65)  Time: 0.778s, 1315.75/s  (0.804s, 1274.17/s)  LR: 8.401e-04  Data: 0.012 (0.013)
Train: 158 [1150/1251 ( 92%)]  Loss: 3.811 (3.66)  Time: 0.803s, 1275.07/s  (0.804s, 1274.34/s)  LR: 8.401e-04  Data: 0.009 (0.013)
Train: 158 [1200/1251 ( 96%)]  Loss: 3.586 (3.65)  Time: 0.784s, 1305.69/s  (0.804s, 1274.14/s)  LR: 8.401e-04  Data: 0.010 (0.013)
Train: 158 [1250/1251 (100%)]  Loss: 3.838 (3.66)  Time: 0.759s, 1348.84/s  (0.803s, 1274.45/s)  LR: 8.401e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.616 (1.616)  Loss:  0.7808 (0.7808)  Acc@1: 89.2578 (89.2578)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.194 (0.596)  Loss:  0.9419 (1.4196)  Acc@1: 82.6651 (72.9740)  Acc@5: 96.1085 (91.6800)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-155.pth.tar', 73.08000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-158.pth.tar', 72.97400009765624)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-156.pth.tar', 72.89800001220704)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-157.pth.tar', 72.79800009521485)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-150.pth.tar', 72.78200006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-138.pth.tar', 72.72799998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-154.pth.tar', 72.70200006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-147.pth.tar', 72.68999996826172)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-151.pth.tar', 72.6840000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-148.pth.tar', 72.67799998779297)

Train: 159 [   0/1251 (  0%)]  Loss: 3.493 (3.49)  Time: 2.261s,  452.91/s  (2.261s,  452.91/s)  LR: 8.381e-04  Data: 1.493 (1.493)
Train: 159 [  50/1251 (  4%)]  Loss: 3.790 (3.64)  Time: 0.795s, 1287.89/s  (0.833s, 1228.69/s)  LR: 8.381e-04  Data: 0.010 (0.044)
Train: 159 [ 100/1251 (  8%)]  Loss: 3.764 (3.68)  Time: 0.788s, 1299.08/s  (0.814s, 1257.69/s)  LR: 8.381e-04  Data: 0.010 (0.027)
Train: 159 [ 150/1251 ( 12%)]  Loss: 3.494 (3.64)  Time: 0.846s, 1210.57/s  (0.819s, 1250.85/s)  LR: 8.381e-04  Data: 0.013 (0.023)
Train: 159 [ 200/1251 ( 16%)]  Loss: 3.364 (3.58)  Time: 0.774s, 1323.50/s  (0.815s, 1256.40/s)  LR: 8.381e-04  Data: 0.010 (0.020)
Train: 159 [ 250/1251 ( 20%)]  Loss: 3.517 (3.57)  Time: 0.773s, 1324.27/s  (0.808s, 1267.30/s)  LR: 8.381e-04  Data: 0.010 (0.018)
Train: 159 [ 300/1251 ( 24%)]  Loss: 3.440 (3.55)  Time: 0.774s, 1322.46/s  (0.803s, 1275.63/s)  LR: 8.381e-04  Data: 0.010 (0.017)
Train: 159 [ 350/1251 ( 28%)]  Loss: 3.902 (3.60)  Time: 0.835s, 1226.11/s  (0.799s, 1280.84/s)  LR: 8.381e-04  Data: 0.010 (0.016)
Train: 159 [ 400/1251 ( 32%)]  Loss: 3.457 (3.58)  Time: 0.774s, 1323.69/s  (0.796s, 1285.64/s)  LR: 8.381e-04  Data: 0.010 (0.015)
Train: 159 [ 450/1251 ( 36%)]  Loss: 3.491 (3.57)  Time: 0.773s, 1324.54/s  (0.794s, 1289.53/s)  LR: 8.381e-04  Data: 0.010 (0.014)
Train: 159 [ 500/1251 ( 40%)]  Loss: 3.495 (3.56)  Time: 0.775s, 1321.26/s  (0.793s, 1291.32/s)  LR: 8.381e-04  Data: 0.010 (0.014)
Train: 159 [ 550/1251 ( 44%)]  Loss: 3.702 (3.58)  Time: 0.774s, 1323.45/s  (0.792s, 1293.28/s)  LR: 8.381e-04  Data: 0.010 (0.013)
Train: 159 [ 600/1251 ( 48%)]  Loss: 3.559 (3.57)  Time: 0.810s, 1263.92/s  (0.791s, 1294.69/s)  LR: 8.381e-04  Data: 0.010 (0.013)
Train: 159 [ 650/1251 ( 52%)]  Loss: 3.644 (3.58)  Time: 0.776s, 1319.27/s  (0.790s, 1296.14/s)  LR: 8.381e-04  Data: 0.010 (0.013)
Train: 159 [ 700/1251 ( 56%)]  Loss: 3.620 (3.58)  Time: 0.772s, 1325.79/s  (0.789s, 1297.95/s)  LR: 8.381e-04  Data: 0.010 (0.013)
Train: 159 [ 750/1251 ( 60%)]  Loss: 3.825 (3.60)  Time: 0.816s, 1254.33/s  (0.789s, 1298.49/s)  LR: 8.381e-04  Data: 0.010 (0.012)
Train: 159 [ 800/1251 ( 64%)]  Loss: 3.912 (3.62)  Time: 0.775s, 1320.64/s  (0.788s, 1299.35/s)  LR: 8.381e-04  Data: 0.010 (0.012)
Train: 159 [ 850/1251 ( 68%)]  Loss: 3.437 (3.61)  Time: 0.774s, 1322.87/s  (0.788s, 1299.58/s)  LR: 8.381e-04  Data: 0.010 (0.012)
Train: 159 [ 900/1251 ( 72%)]  Loss: 3.512 (3.60)  Time: 0.777s, 1317.70/s  (0.787s, 1300.44/s)  LR: 8.381e-04  Data: 0.010 (0.012)
Train: 159 [ 950/1251 ( 76%)]  Loss: 3.701 (3.61)  Time: 0.777s, 1318.17/s  (0.787s, 1301.06/s)  LR: 8.381e-04  Data: 0.010 (0.012)
Train: 159 [1000/1251 ( 80%)]  Loss: 3.352 (3.59)  Time: 0.773s, 1324.94/s  (0.787s, 1301.63/s)  LR: 8.381e-04  Data: 0.010 (0.012)
Train: 159 [1050/1251 ( 84%)]  Loss: 3.626 (3.60)  Time: 0.773s, 1324.89/s  (0.786s, 1302.24/s)  LR: 8.381e-04  Data: 0.009 (0.012)
Train: 159 [1100/1251 ( 88%)]  Loss: 3.551 (3.59)  Time: 0.773s, 1325.53/s  (0.786s, 1302.71/s)  LR: 8.381e-04  Data: 0.010 (0.012)
Train: 159 [1150/1251 ( 92%)]  Loss: 3.658 (3.60)  Time: 0.774s, 1323.24/s  (0.786s, 1303.45/s)  LR: 8.381e-04  Data: 0.010 (0.012)
Train: 159 [1200/1251 ( 96%)]  Loss: 3.518 (3.59)  Time: 0.773s, 1324.35/s  (0.785s, 1304.13/s)  LR: 8.381e-04  Data: 0.010 (0.011)
Train: 159 [1250/1251 (100%)]  Loss: 3.481 (3.59)  Time: 0.756s, 1354.95/s  (0.785s, 1304.54/s)  LR: 8.381e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.527 (1.527)  Loss:  0.8008 (0.8008)  Acc@1: 87.9883 (87.9883)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.194 (0.557)  Loss:  0.9775 (1.3635)  Acc@1: 82.3113 (72.8400)  Acc@5: 95.6368 (91.6100)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-155.pth.tar', 73.08000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-158.pth.tar', 72.97400009765624)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-156.pth.tar', 72.89800001220704)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-159.pth.tar', 72.8400000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-157.pth.tar', 72.79800009521485)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-150.pth.tar', 72.78200006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-138.pth.tar', 72.72799998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-154.pth.tar', 72.70200006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-147.pth.tar', 72.68999996826172)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-151.pth.tar', 72.6840000390625)

Train: 160 [   0/1251 (  0%)]  Loss: 4.008 (4.01)  Time: 2.248s,  455.43/s  (2.248s,  455.43/s)  LR: 8.362e-04  Data: 1.517 (1.517)
Train: 160 [  50/1251 (  4%)]  Loss: 4.027 (4.02)  Time: 0.797s, 1284.39/s  (0.809s, 1266.40/s)  LR: 8.362e-04  Data: 0.010 (0.043)
Train: 160 [ 100/1251 (  8%)]  Loss: 3.501 (3.85)  Time: 0.775s, 1320.74/s  (0.793s, 1291.45/s)  LR: 8.362e-04  Data: 0.010 (0.026)
Train: 160 [ 150/1251 ( 12%)]  Loss: 3.690 (3.81)  Time: 0.813s, 1260.11/s  (0.788s, 1299.91/s)  LR: 8.362e-04  Data: 0.010 (0.021)
Train: 160 [ 200/1251 ( 16%)]  Loss: 3.845 (3.81)  Time: 0.774s, 1323.72/s  (0.787s, 1301.95/s)  LR: 8.362e-04  Data: 0.010 (0.018)
Train: 160 [ 250/1251 ( 20%)]  Loss: 3.605 (3.78)  Time: 0.771s, 1328.90/s  (0.784s, 1305.36/s)  LR: 8.362e-04  Data: 0.010 (0.017)
Train: 160 [ 300/1251 ( 24%)]  Loss: 3.638 (3.76)  Time: 0.774s, 1323.31/s  (0.783s, 1307.58/s)  LR: 8.362e-04  Data: 0.010 (0.015)
Train: 160 [ 350/1251 ( 28%)]  Loss: 3.926 (3.78)  Time: 0.773s, 1325.25/s  (0.783s, 1307.85/s)  LR: 8.362e-04  Data: 0.010 (0.015)
Train: 160 [ 400/1251 ( 32%)]  Loss: 3.289 (3.73)  Time: 0.772s, 1326.68/s  (0.783s, 1308.10/s)  LR: 8.362e-04  Data: 0.010 (0.014)
Train: 160 [ 450/1251 ( 36%)]  Loss: 4.014 (3.75)  Time: 0.774s, 1322.57/s  (0.782s, 1308.95/s)  LR: 8.362e-04  Data: 0.010 (0.014)
Train: 160 [ 500/1251 ( 40%)]  Loss: 3.562 (3.74)  Time: 0.772s, 1326.58/s  (0.782s, 1309.32/s)  LR: 8.362e-04  Data: 0.010 (0.013)
Train: 160 [ 550/1251 ( 44%)]  Loss: 3.754 (3.74)  Time: 0.774s, 1323.20/s  (0.782s, 1309.43/s)  LR: 8.362e-04  Data: 0.010 (0.013)
Train: 160 [ 600/1251 ( 48%)]  Loss: 3.601 (3.73)  Time: 0.774s, 1323.80/s  (0.782s, 1310.10/s)  LR: 8.362e-04  Data: 0.010 (0.013)
Train: 160 [ 650/1251 ( 52%)]  Loss: 3.421 (3.71)  Time: 0.775s, 1321.61/s  (0.782s, 1310.26/s)  LR: 8.362e-04  Data: 0.010 (0.012)
Train: 160 [ 700/1251 ( 56%)]  Loss: 3.765 (3.71)  Time: 0.772s, 1326.74/s  (0.781s, 1310.46/s)  LR: 8.362e-04  Data: 0.010 (0.012)
Train: 160 [ 750/1251 ( 60%)]  Loss: 4.003 (3.73)  Time: 0.810s, 1264.50/s  (0.781s, 1311.05/s)  LR: 8.362e-04  Data: 0.010 (0.012)
Train: 160 [ 800/1251 ( 64%)]  Loss: 3.563 (3.72)  Time: 0.772s, 1326.61/s  (0.781s, 1310.95/s)  LR: 8.362e-04  Data: 0.010 (0.012)
Train: 160 [ 850/1251 ( 68%)]  Loss: 3.531 (3.71)  Time: 0.775s, 1320.61/s  (0.781s, 1311.33/s)  LR: 8.362e-04  Data: 0.010 (0.012)
Train: 160 [ 900/1251 ( 72%)]  Loss: 3.673 (3.71)  Time: 0.772s, 1325.82/s  (0.781s, 1311.94/s)  LR: 8.362e-04  Data: 0.010 (0.012)
Train: 160 [ 950/1251 ( 76%)]  Loss: 3.753 (3.71)  Time: 0.771s, 1327.59/s  (0.780s, 1312.17/s)  LR: 8.362e-04  Data: 0.010 (0.012)
Train: 160 [1000/1251 ( 80%)]  Loss: 3.630 (3.70)  Time: 0.772s, 1325.87/s  (0.780s, 1312.70/s)  LR: 8.362e-04  Data: 0.010 (0.012)
Train: 160 [1050/1251 ( 84%)]  Loss: 3.733 (3.71)  Time: 0.773s, 1325.28/s  (0.780s, 1312.74/s)  LR: 8.362e-04  Data: 0.010 (0.012)
Train: 160 [1100/1251 ( 88%)]  Loss: 3.542 (3.70)  Time: 0.774s, 1323.44/s  (0.780s, 1313.00/s)  LR: 8.362e-04  Data: 0.010 (0.011)
Train: 160 [1150/1251 ( 92%)]  Loss: 3.909 (3.71)  Time: 0.774s, 1323.04/s  (0.780s, 1313.27/s)  LR: 8.362e-04  Data: 0.010 (0.011)
Train: 160 [1200/1251 ( 96%)]  Loss: 3.218 (3.69)  Time: 0.776s, 1319.73/s  (0.780s, 1313.37/s)  LR: 8.362e-04  Data: 0.010 (0.011)
Train: 160 [1250/1251 (100%)]  Loss: 3.177 (3.67)  Time: 0.762s, 1344.38/s  (0.780s, 1313.56/s)  LR: 8.362e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.520 (1.520)  Loss:  0.7275 (0.7275)  Acc@1: 88.5742 (88.5742)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.194 (0.564)  Loss:  0.8579 (1.2957)  Acc@1: 82.7830 (73.0820)  Acc@5: 95.5189 (91.5380)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-160.pth.tar', 73.08200001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-155.pth.tar', 73.08000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-158.pth.tar', 72.97400009765624)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-156.pth.tar', 72.89800001220704)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-159.pth.tar', 72.8400000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-157.pth.tar', 72.79800009521485)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-150.pth.tar', 72.78200006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-138.pth.tar', 72.72799998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-154.pth.tar', 72.70200006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-147.pth.tar', 72.68999996826172)

Train: 161 [   0/1251 (  0%)]  Loss: 3.862 (3.86)  Time: 2.211s,  463.08/s  (2.211s,  463.08/s)  LR: 8.343e-04  Data: 1.477 (1.477)
Train: 161 [  50/1251 (  4%)]  Loss: 3.992 (3.93)  Time: 0.812s, 1261.08/s  (0.818s, 1251.74/s)  LR: 8.343e-04  Data: 0.010 (0.045)
Train: 161 [ 100/1251 (  8%)]  Loss: 3.838 (3.90)  Time: 0.773s, 1324.64/s  (0.800s, 1280.57/s)  LR: 8.343e-04  Data: 0.010 (0.028)
Train: 161 [ 150/1251 ( 12%)]  Loss: 3.576 (3.82)  Time: 0.812s, 1261.48/s  (0.793s, 1290.78/s)  LR: 8.343e-04  Data: 0.010 (0.022)
Train: 161 [ 200/1251 ( 16%)]  Loss: 3.390 (3.73)  Time: 0.773s, 1325.18/s  (0.789s, 1297.19/s)  LR: 8.343e-04  Data: 0.010 (0.019)
Train: 161 [ 250/1251 ( 20%)]  Loss: 3.501 (3.69)  Time: 0.775s, 1321.76/s  (0.787s, 1300.53/s)  LR: 8.343e-04  Data: 0.010 (0.017)
Train: 161 [ 300/1251 ( 24%)]  Loss: 3.746 (3.70)  Time: 0.776s, 1320.26/s  (0.786s, 1302.63/s)  LR: 8.343e-04  Data: 0.010 (0.016)
Train: 161 [ 350/1251 ( 28%)]  Loss: 3.562 (3.68)  Time: 0.776s, 1320.04/s  (0.785s, 1305.07/s)  LR: 8.343e-04  Data: 0.010 (0.015)
Train: 161 [ 400/1251 ( 32%)]  Loss: 3.856 (3.70)  Time: 0.772s, 1325.70/s  (0.784s, 1306.86/s)  LR: 8.343e-04  Data: 0.010 (0.014)
Train: 161 [ 450/1251 ( 36%)]  Loss: 3.696 (3.70)  Time: 0.775s, 1322.08/s  (0.783s, 1308.29/s)  LR: 8.343e-04  Data: 0.010 (0.014)
Train: 161 [ 500/1251 ( 40%)]  Loss: 3.887 (3.72)  Time: 0.773s, 1324.44/s  (0.782s, 1308.74/s)  LR: 8.343e-04  Data: 0.010 (0.014)
Train: 161 [ 550/1251 ( 44%)]  Loss: 4.087 (3.75)  Time: 0.774s, 1323.79/s  (0.783s, 1308.60/s)  LR: 8.343e-04  Data: 0.010 (0.013)
Train: 161 [ 600/1251 ( 48%)]  Loss: 3.508 (3.73)  Time: 0.774s, 1323.65/s  (0.782s, 1309.69/s)  LR: 8.343e-04  Data: 0.010 (0.013)
Train: 161 [ 650/1251 ( 52%)]  Loss: 3.891 (3.74)  Time: 0.773s, 1324.19/s  (0.782s, 1310.03/s)  LR: 8.343e-04  Data: 0.010 (0.013)
Train: 161 [ 700/1251 ( 56%)]  Loss: 3.721 (3.74)  Time: 0.773s, 1324.32/s  (0.781s, 1310.52/s)  LR: 8.343e-04  Data: 0.010 (0.012)
Train: 161 [ 750/1251 ( 60%)]  Loss: 3.337 (3.72)  Time: 0.775s, 1320.69/s  (0.781s, 1310.69/s)  LR: 8.343e-04  Data: 0.010 (0.012)
Train: 161 [ 800/1251 ( 64%)]  Loss: 3.635 (3.71)  Time: 0.811s, 1262.23/s  (0.781s, 1310.81/s)  LR: 8.343e-04  Data: 0.010 (0.012)
Train: 161 [ 850/1251 ( 68%)]  Loss: 3.510 (3.70)  Time: 0.814s, 1258.00/s  (0.781s, 1311.21/s)  LR: 8.343e-04  Data: 0.010 (0.012)
Train: 161 [ 900/1251 ( 72%)]  Loss: 3.420 (3.68)  Time: 0.774s, 1323.02/s  (0.781s, 1310.52/s)  LR: 8.343e-04  Data: 0.010 (0.012)
Train: 161 [ 950/1251 ( 76%)]  Loss: 3.366 (3.67)  Time: 0.772s, 1325.80/s  (0.781s, 1310.91/s)  LR: 8.343e-04  Data: 0.010 (0.012)
Train: 161 [1000/1251 ( 80%)]  Loss: 3.706 (3.67)  Time: 0.777s, 1318.62/s  (0.781s, 1310.37/s)  LR: 8.343e-04  Data: 0.010 (0.012)
Train: 161 [1050/1251 ( 84%)]  Loss: 3.773 (3.68)  Time: 0.773s, 1324.75/s  (0.781s, 1310.95/s)  LR: 8.343e-04  Data: 0.010 (0.012)
Train: 161 [1100/1251 ( 88%)]  Loss: 3.694 (3.68)  Time: 0.773s, 1324.23/s  (0.781s, 1311.16/s)  LR: 8.343e-04  Data: 0.010 (0.012)
Train: 161 [1150/1251 ( 92%)]  Loss: 4.023 (3.69)  Time: 0.773s, 1324.26/s  (0.781s, 1311.54/s)  LR: 8.343e-04  Data: 0.010 (0.011)
Train: 161 [1200/1251 ( 96%)]  Loss: 4.013 (3.70)  Time: 0.813s, 1259.75/s  (0.781s, 1311.75/s)  LR: 8.343e-04  Data: 0.010 (0.011)
Train: 161 [1250/1251 (100%)]  Loss: 4.130 (3.72)  Time: 0.789s, 1297.43/s  (0.781s, 1311.25/s)  LR: 8.343e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.581 (1.581)  Loss:  0.7656 (0.7656)  Acc@1: 87.9883 (87.9883)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.194 (0.558)  Loss:  0.9004 (1.3608)  Acc@1: 81.8396 (72.9600)  Acc@5: 96.3443 (91.5300)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-160.pth.tar', 73.08200001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-155.pth.tar', 73.08000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-158.pth.tar', 72.97400009765624)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-161.pth.tar', 72.9599999975586)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-156.pth.tar', 72.89800001220704)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-159.pth.tar', 72.8400000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-157.pth.tar', 72.79800009521485)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-150.pth.tar', 72.78200006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-138.pth.tar', 72.72799998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-154.pth.tar', 72.70200006591797)

Train: 162 [   0/1251 (  0%)]  Loss: 4.035 (4.03)  Time: 2.337s,  438.17/s  (2.337s,  438.17/s)  LR: 8.323e-04  Data: 1.607 (1.607)
Train: 162 [  50/1251 (  4%)]  Loss: 3.749 (3.89)  Time: 0.771s, 1327.81/s  (0.815s, 1257.04/s)  LR: 8.323e-04  Data: 0.010 (0.047)
Train: 162 [ 100/1251 (  8%)]  Loss: 3.125 (3.64)  Time: 0.772s, 1326.50/s  (0.795s, 1287.41/s)  LR: 8.323e-04  Data: 0.010 (0.029)
Train: 162 [ 150/1251 ( 12%)]  Loss: 3.209 (3.53)  Time: 0.775s, 1321.27/s  (0.789s, 1297.17/s)  LR: 8.323e-04  Data: 0.010 (0.023)
Train: 162 [ 200/1251 ( 16%)]  Loss: 3.406 (3.50)  Time: 0.773s, 1325.32/s  (0.788s, 1300.00/s)  LR: 8.323e-04  Data: 0.010 (0.019)
Train: 162 [ 250/1251 ( 20%)]  Loss: 3.562 (3.51)  Time: 0.771s, 1327.39/s  (0.786s, 1302.36/s)  LR: 8.323e-04  Data: 0.010 (0.017)
Train: 162 [ 300/1251 ( 24%)]  Loss: 3.741 (3.55)  Time: 0.776s, 1319.12/s  (0.785s, 1304.73/s)  LR: 8.323e-04  Data: 0.010 (0.016)
Train: 162 [ 350/1251 ( 28%)]  Loss: 3.509 (3.54)  Time: 0.812s, 1261.30/s  (0.784s, 1305.35/s)  LR: 8.323e-04  Data: 0.010 (0.015)
Train: 162 [ 400/1251 ( 32%)]  Loss: 3.631 (3.55)  Time: 0.772s, 1325.63/s  (0.785s, 1304.89/s)  LR: 8.323e-04  Data: 0.010 (0.015)
Train: 162 [ 450/1251 ( 36%)]  Loss: 3.903 (3.59)  Time: 0.775s, 1320.71/s  (0.784s, 1305.58/s)  LR: 8.323e-04  Data: 0.010 (0.014)
Train: 162 [ 500/1251 ( 40%)]  Loss: 3.845 (3.61)  Time: 0.776s, 1318.88/s  (0.783s, 1306.97/s)  LR: 8.323e-04  Data: 0.010 (0.014)
Train: 162 [ 550/1251 ( 44%)]  Loss: 4.042 (3.65)  Time: 0.773s, 1324.35/s  (0.783s, 1307.28/s)  LR: 8.323e-04  Data: 0.010 (0.013)
Train: 162 [ 600/1251 ( 48%)]  Loss: 3.911 (3.67)  Time: 0.775s, 1321.73/s  (0.783s, 1307.78/s)  LR: 8.323e-04  Data: 0.010 (0.013)
Train: 162 [ 650/1251 ( 52%)]  Loss: 3.837 (3.68)  Time: 0.775s, 1320.76/s  (0.783s, 1308.50/s)  LR: 8.323e-04  Data: 0.009 (0.013)
Train: 162 [ 700/1251 ( 56%)]  Loss: 3.699 (3.68)  Time: 0.774s, 1323.59/s  (0.782s, 1308.93/s)  LR: 8.323e-04  Data: 0.010 (0.013)
Train: 162 [ 750/1251 ( 60%)]  Loss: 3.682 (3.68)  Time: 0.810s, 1263.69/s  (0.782s, 1309.28/s)  LR: 8.323e-04  Data: 0.009 (0.012)
Train: 162 [ 800/1251 ( 64%)]  Loss: 3.953 (3.70)  Time: 0.774s, 1322.36/s  (0.782s, 1309.23/s)  LR: 8.323e-04  Data: 0.009 (0.012)
Train: 162 [ 850/1251 ( 68%)]  Loss: 3.670 (3.69)  Time: 0.771s, 1327.88/s  (0.782s, 1308.68/s)  LR: 8.323e-04  Data: 0.010 (0.012)
Train: 162 [ 900/1251 ( 72%)]  Loss: 3.909 (3.71)  Time: 0.772s, 1326.26/s  (0.782s, 1309.10/s)  LR: 8.323e-04  Data: 0.010 (0.012)
Train: 162 [ 950/1251 ( 76%)]  Loss: 3.404 (3.69)  Time: 0.773s, 1324.27/s  (0.782s, 1309.56/s)  LR: 8.323e-04  Data: 0.010 (0.012)
Train: 162 [1000/1251 ( 80%)]  Loss: 3.382 (3.68)  Time: 0.773s, 1325.13/s  (0.782s, 1309.76/s)  LR: 8.323e-04  Data: 0.010 (0.012)
Train: 162 [1050/1251 ( 84%)]  Loss: 3.628 (3.67)  Time: 0.772s, 1325.72/s  (0.782s, 1310.25/s)  LR: 8.323e-04  Data: 0.010 (0.012)
Train: 162 [1100/1251 ( 88%)]  Loss: 3.530 (3.67)  Time: 0.773s, 1325.06/s  (0.782s, 1310.26/s)  LR: 8.323e-04  Data: 0.010 (0.012)
Train: 162 [1150/1251 ( 92%)]  Loss: 3.786 (3.67)  Time: 0.772s, 1326.18/s  (0.781s, 1310.59/s)  LR: 8.323e-04  Data: 0.010 (0.012)
Train: 162 [1200/1251 ( 96%)]  Loss: 3.446 (3.66)  Time: 0.772s, 1325.77/s  (0.781s, 1310.66/s)  LR: 8.323e-04  Data: 0.010 (0.011)
Train: 162 [1250/1251 (100%)]  Loss: 4.004 (3.68)  Time: 0.760s, 1347.21/s  (0.781s, 1310.98/s)  LR: 8.323e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.526 (1.526)  Loss:  0.8267 (0.8267)  Acc@1: 88.5742 (88.5742)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.194 (0.564)  Loss:  0.8687 (1.3694)  Acc@1: 84.7877 (72.6760)  Acc@5: 95.6368 (91.4600)
Train: 163 [   0/1251 (  0%)]  Loss: 3.482 (3.48)  Time: 2.147s,  476.90/s  (2.147s,  476.90/s)  LR: 8.304e-04  Data: 1.416 (1.416)
Train: 163 [  50/1251 (  4%)]  Loss: 3.535 (3.51)  Time: 0.777s, 1318.39/s  (0.807s, 1268.84/s)  LR: 8.304e-04  Data: 0.010 (0.041)
Train: 163 [ 100/1251 (  8%)]  Loss: 3.146 (3.39)  Time: 0.773s, 1325.12/s  (0.796s, 1287.05/s)  LR: 8.304e-04  Data: 0.010 (0.026)
Train: 163 [ 150/1251 ( 12%)]  Loss: 3.411 (3.39)  Time: 0.861s, 1189.06/s  (0.792s, 1293.49/s)  LR: 8.304e-04  Data: 0.010 (0.020)
Train: 163 [ 200/1251 ( 16%)]  Loss: 3.666 (3.45)  Time: 0.775s, 1322.01/s  (0.790s, 1296.68/s)  LR: 8.304e-04  Data: 0.010 (0.018)
Train: 163 [ 250/1251 ( 20%)]  Loss: 3.195 (3.41)  Time: 0.773s, 1324.51/s  (0.787s, 1300.88/s)  LR: 8.304e-04  Data: 0.010 (0.016)
Train: 163 [ 300/1251 ( 24%)]  Loss: 3.507 (3.42)  Time: 0.773s, 1323.91/s  (0.786s, 1303.61/s)  LR: 8.304e-04  Data: 0.010 (0.015)
Train: 163 [ 350/1251 ( 28%)]  Loss: 3.813 (3.47)  Time: 0.782s, 1309.11/s  (0.784s, 1306.41/s)  LR: 8.304e-04  Data: 0.010 (0.014)
Train: 163 [ 400/1251 ( 32%)]  Loss: 3.611 (3.48)  Time: 0.773s, 1324.99/s  (0.783s, 1308.53/s)  LR: 8.304e-04  Data: 0.010 (0.014)
Train: 163 [ 450/1251 ( 36%)]  Loss: 3.515 (3.49)  Time: 0.772s, 1326.47/s  (0.782s, 1309.77/s)  LR: 8.304e-04  Data: 0.010 (0.013)
Train: 163 [ 500/1251 ( 40%)]  Loss: 3.312 (3.47)  Time: 0.773s, 1325.02/s  (0.781s, 1310.73/s)  LR: 8.304e-04  Data: 0.010 (0.013)
Train: 163 [ 550/1251 ( 44%)]  Loss: 3.897 (3.51)  Time: 0.772s, 1326.56/s  (0.781s, 1311.06/s)  LR: 8.304e-04  Data: 0.010 (0.013)
Train: 163 [ 600/1251 ( 48%)]  Loss: 3.652 (3.52)  Time: 0.775s, 1321.20/s  (0.781s, 1311.76/s)  LR: 8.304e-04  Data: 0.010 (0.013)
Train: 163 [ 650/1251 ( 52%)]  Loss: 3.381 (3.51)  Time: 0.773s, 1324.32/s  (0.780s, 1312.27/s)  LR: 8.304e-04  Data: 0.010 (0.012)
Train: 163 [ 700/1251 ( 56%)]  Loss: 3.511 (3.51)  Time: 0.779s, 1315.25/s  (0.780s, 1312.84/s)  LR: 8.304e-04  Data: 0.010 (0.012)
Train: 163 [ 750/1251 ( 60%)]  Loss: 3.540 (3.51)  Time: 0.773s, 1325.44/s  (0.780s, 1313.44/s)  LR: 8.304e-04  Data: 0.010 (0.012)
Train: 163 [ 800/1251 ( 64%)]  Loss: 3.752 (3.53)  Time: 0.773s, 1324.70/s  (0.779s, 1313.78/s)  LR: 8.304e-04  Data: 0.010 (0.012)
Train: 163 [ 850/1251 ( 68%)]  Loss: 3.567 (3.53)  Time: 0.773s, 1324.12/s  (0.779s, 1314.21/s)  LR: 8.304e-04  Data: 0.010 (0.012)
Train: 163 [ 900/1251 ( 72%)]  Loss: 3.617 (3.53)  Time: 0.773s, 1324.46/s  (0.779s, 1314.13/s)  LR: 8.304e-04  Data: 0.010 (0.012)
Train: 163 [ 950/1251 ( 76%)]  Loss: 4.129 (3.56)  Time: 0.774s, 1323.83/s  (0.779s, 1314.26/s)  LR: 8.304e-04  Data: 0.010 (0.012)
Train: 163 [1000/1251 ( 80%)]  Loss: 3.811 (3.57)  Time: 0.773s, 1325.31/s  (0.779s, 1314.51/s)  LR: 8.304e-04  Data: 0.010 (0.011)
Train: 163 [1050/1251 ( 84%)]  Loss: 3.708 (3.58)  Time: 0.773s, 1323.87/s  (0.779s, 1314.81/s)  LR: 8.304e-04  Data: 0.010 (0.011)
Train: 163 [1100/1251 ( 88%)]  Loss: 3.663 (3.58)  Time: 0.773s, 1324.54/s  (0.779s, 1315.11/s)  LR: 8.304e-04  Data: 0.010 (0.011)
Train: 163 [1150/1251 ( 92%)]  Loss: 3.712 (3.59)  Time: 0.772s, 1326.35/s  (0.779s, 1315.02/s)  LR: 8.304e-04  Data: 0.010 (0.011)
Train: 163 [1200/1251 ( 96%)]  Loss: 3.833 (3.60)  Time: 0.776s, 1320.23/s  (0.779s, 1315.28/s)  LR: 8.304e-04  Data: 0.010 (0.011)
Train: 163 [1250/1251 (100%)]  Loss: 3.554 (3.60)  Time: 0.799s, 1281.72/s  (0.779s, 1315.12/s)  LR: 8.304e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.540 (1.540)  Loss:  0.8594 (0.8594)  Acc@1: 88.3789 (88.3789)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.194 (0.562)  Loss:  0.9248 (1.3957)  Acc@1: 83.9623 (73.1480)  Acc@5: 95.6368 (91.7400)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-163.pth.tar', 73.14800001464843)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-160.pth.tar', 73.08200001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-155.pth.tar', 73.08000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-158.pth.tar', 72.97400009765624)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-161.pth.tar', 72.9599999975586)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-156.pth.tar', 72.89800001220704)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-159.pth.tar', 72.8400000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-157.pth.tar', 72.79800009521485)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-150.pth.tar', 72.78200006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-138.pth.tar', 72.72799998291016)

Train: 164 [   0/1251 (  0%)]  Loss: 3.559 (3.56)  Time: 2.368s,  432.50/s  (2.368s,  432.50/s)  LR: 8.284e-04  Data: 1.633 (1.633)
Train: 164 [  50/1251 (  4%)]  Loss: 4.082 (3.82)  Time: 0.777s, 1318.56/s  (0.817s, 1254.12/s)  LR: 8.284e-04  Data: 0.010 (0.050)
Train: 164 [ 100/1251 (  8%)]  Loss: 3.412 (3.68)  Time: 0.776s, 1318.96/s  (0.797s, 1284.15/s)  LR: 8.284e-04  Data: 0.013 (0.030)
Train: 164 [ 150/1251 ( 12%)]  Loss: 3.891 (3.74)  Time: 0.807s, 1268.24/s  (0.790s, 1296.47/s)  LR: 8.284e-04  Data: 0.010 (0.024)
Train: 164 [ 200/1251 ( 16%)]  Loss: 3.945 (3.78)  Time: 0.772s, 1326.34/s  (0.786s, 1302.27/s)  LR: 8.284e-04  Data: 0.010 (0.020)
Train: 164 [ 250/1251 ( 20%)]  Loss: 3.314 (3.70)  Time: 0.774s, 1323.12/s  (0.784s, 1306.13/s)  LR: 8.284e-04  Data: 0.010 (0.018)
Train: 164 [ 300/1251 ( 24%)]  Loss: 3.337 (3.65)  Time: 0.774s, 1322.94/s  (0.783s, 1308.35/s)  LR: 8.284e-04  Data: 0.010 (0.017)
Train: 164 [ 350/1251 ( 28%)]  Loss: 3.757 (3.66)  Time: 0.804s, 1272.86/s  (0.782s, 1308.96/s)  LR: 8.284e-04  Data: 0.010 (0.016)
Train: 164 [ 400/1251 ( 32%)]  Loss: 3.494 (3.64)  Time: 0.787s, 1300.61/s  (0.786s, 1303.58/s)  LR: 8.284e-04  Data: 0.013 (0.015)
Train: 164 [ 450/1251 ( 36%)]  Loss: 3.495 (3.63)  Time: 0.807s, 1268.98/s  (0.788s, 1300.27/s)  LR: 8.284e-04  Data: 0.012 (0.015)
Train: 164 [ 500/1251 ( 40%)]  Loss: 3.610 (3.63)  Time: 0.772s, 1326.42/s  (0.789s, 1297.96/s)  LR: 8.284e-04  Data: 0.010 (0.014)
Train: 164 [ 550/1251 ( 44%)]  Loss: 4.121 (3.67)  Time: 0.783s, 1308.26/s  (0.789s, 1297.31/s)  LR: 8.284e-04  Data: 0.010 (0.014)
Train: 164 [ 600/1251 ( 48%)]  Loss: 3.950 (3.69)  Time: 0.809s, 1266.52/s  (0.790s, 1295.52/s)  LR: 8.284e-04  Data: 0.014 (0.014)
Train: 164 [ 650/1251 ( 52%)]  Loss: 3.765 (3.70)  Time: 0.816s, 1255.26/s  (0.792s, 1293.48/s)  LR: 8.284e-04  Data: 0.012 (0.014)
Train: 164 [ 700/1251 ( 56%)]  Loss: 3.795 (3.70)  Time: 0.810s, 1263.51/s  (0.792s, 1292.61/s)  LR: 8.284e-04  Data: 0.010 (0.013)
Train: 164 [ 750/1251 ( 60%)]  Loss: 3.510 (3.69)  Time: 0.797s, 1284.35/s  (0.793s, 1291.32/s)  LR: 8.284e-04  Data: 0.018 (0.013)
Train: 164 [ 800/1251 ( 64%)]  Loss: 3.632 (3.69)  Time: 0.783s, 1308.01/s  (0.793s, 1290.80/s)  LR: 8.284e-04  Data: 0.012 (0.013)
Train: 164 [ 850/1251 ( 68%)]  Loss: 3.562 (3.68)  Time: 0.780s, 1311.99/s  (0.794s, 1290.12/s)  LR: 8.284e-04  Data: 0.010 (0.013)
Train: 164 [ 900/1251 ( 72%)]  Loss: 3.559 (3.67)  Time: 0.799s, 1280.88/s  (0.794s, 1289.11/s)  LR: 8.284e-04  Data: 0.016 (0.013)
Train: 164 [ 950/1251 ( 76%)]  Loss: 3.363 (3.66)  Time: 0.787s, 1301.80/s  (0.795s, 1288.51/s)  LR: 8.284e-04  Data: 0.010 (0.013)
Train: 164 [1000/1251 ( 80%)]  Loss: 3.863 (3.67)  Time: 0.773s, 1323.87/s  (0.795s, 1287.82/s)  LR: 8.284e-04  Data: 0.009 (0.013)
Train: 164 [1050/1251 ( 84%)]  Loss: 3.756 (3.67)  Time: 0.781s, 1311.42/s  (0.795s, 1287.75/s)  LR: 8.284e-04  Data: 0.012 (0.012)
Train: 164 [1100/1251 ( 88%)]  Loss: 3.495 (3.66)  Time: 0.780s, 1313.20/s  (0.796s, 1286.85/s)  LR: 8.284e-04  Data: 0.011 (0.012)
Train: 164 [1150/1251 ( 92%)]  Loss: 3.743 (3.67)  Time: 0.772s, 1326.56/s  (0.796s, 1286.50/s)  LR: 8.284e-04  Data: 0.010 (0.012)
Train: 164 [1200/1251 ( 96%)]  Loss: 3.559 (3.66)  Time: 0.782s, 1309.38/s  (0.796s, 1286.17/s)  LR: 8.284e-04  Data: 0.014 (0.012)
Train: 164 [1250/1251 (100%)]  Loss: 3.605 (3.66)  Time: 0.756s, 1354.47/s  (0.796s, 1286.24/s)  LR: 8.284e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.624 (1.624)  Loss:  0.8052 (0.8052)  Acc@1: 87.8906 (87.8906)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.193 (0.604)  Loss:  0.9111 (1.4129)  Acc@1: 84.5519 (72.6840)  Acc@5: 96.4623 (91.4660)
Train: 165 [   0/1251 (  0%)]  Loss: 4.132 (4.13)  Time: 2.464s,  415.53/s  (2.464s,  415.53/s)  LR: 8.265e-04  Data: 1.738 (1.738)
Train: 165 [  50/1251 (  4%)]  Loss: 3.751 (3.94)  Time: 0.787s, 1300.42/s  (0.840s, 1219.45/s)  LR: 8.265e-04  Data: 0.009 (0.047)
Train: 165 [ 100/1251 (  8%)]  Loss: 3.787 (3.89)  Time: 0.772s, 1325.80/s  (0.819s, 1249.87/s)  LR: 8.265e-04  Data: 0.010 (0.029)
Train: 165 [ 150/1251 ( 12%)]  Loss: 3.687 (3.84)  Time: 0.817s, 1252.84/s  (0.812s, 1261.27/s)  LR: 8.265e-04  Data: 0.010 (0.023)
Train: 165 [ 200/1251 ( 16%)]  Loss: 3.580 (3.79)  Time: 0.817s, 1254.07/s  (0.811s, 1262.95/s)  LR: 8.265e-04  Data: 0.009 (0.020)
Train: 165 [ 250/1251 ( 20%)]  Loss: 3.549 (3.75)  Time: 0.782s, 1308.95/s  (0.809s, 1266.18/s)  LR: 8.265e-04  Data: 0.010 (0.018)
Train: 165 [ 300/1251 ( 24%)]  Loss: 3.562 (3.72)  Time: 0.814s, 1258.55/s  (0.808s, 1267.80/s)  LR: 8.265e-04  Data: 0.015 (0.017)
Train: 165 [ 350/1251 ( 28%)]  Loss: 3.732 (3.72)  Time: 0.854s, 1199.27/s  (0.807s, 1268.59/s)  LR: 8.265e-04  Data: 0.009 (0.016)
Train: 165 [ 400/1251 ( 32%)]  Loss: 3.793 (3.73)  Time: 0.787s, 1301.51/s  (0.806s, 1269.72/s)  LR: 8.265e-04  Data: 0.010 (0.015)
Train: 165 [ 450/1251 ( 36%)]  Loss: 3.788 (3.74)  Time: 0.801s, 1278.99/s  (0.806s, 1269.80/s)  LR: 8.265e-04  Data: 0.010 (0.015)
Train: 165 [ 500/1251 ( 40%)]  Loss: 3.630 (3.73)  Time: 0.784s, 1306.95/s  (0.806s, 1269.86/s)  LR: 8.265e-04  Data: 0.013 (0.015)
Train: 165 [ 550/1251 ( 44%)]  Loss: 3.606 (3.72)  Time: 0.783s, 1308.22/s  (0.806s, 1269.79/s)  LR: 8.265e-04  Data: 0.013 (0.014)
Train: 165 [ 600/1251 ( 48%)]  Loss: 3.650 (3.71)  Time: 0.803s, 1275.05/s  (0.806s, 1271.08/s)  LR: 8.265e-04  Data: 0.010 (0.014)
Train: 165 [ 650/1251 ( 52%)]  Loss: 3.572 (3.70)  Time: 0.784s, 1306.63/s  (0.805s, 1271.85/s)  LR: 8.265e-04  Data: 0.010 (0.014)
Train: 165 [ 700/1251 ( 56%)]  Loss: 3.449 (3.68)  Time: 0.846s, 1210.09/s  (0.805s, 1272.13/s)  LR: 8.265e-04  Data: 0.009 (0.013)
Train: 165 [ 750/1251 ( 60%)]  Loss: 3.889 (3.70)  Time: 0.803s, 1274.79/s  (0.805s, 1272.76/s)  LR: 8.265e-04  Data: 0.010 (0.013)
Train: 165 [ 800/1251 ( 64%)]  Loss: 3.716 (3.70)  Time: 0.773s, 1325.27/s  (0.805s, 1272.69/s)  LR: 8.265e-04  Data: 0.010 (0.013)
Train: 165 [ 850/1251 ( 68%)]  Loss: 3.867 (3.71)  Time: 0.810s, 1264.29/s  (0.804s, 1273.53/s)  LR: 8.265e-04  Data: 0.010 (0.013)
Train: 165 [ 900/1251 ( 72%)]  Loss: 3.740 (3.71)  Time: 0.819s, 1250.40/s  (0.804s, 1273.28/s)  LR: 8.265e-04  Data: 0.011 (0.013)
Train: 165 [ 950/1251 ( 76%)]  Loss: 3.775 (3.71)  Time: 0.771s, 1328.54/s  (0.804s, 1273.12/s)  LR: 8.265e-04  Data: 0.010 (0.013)
Train: 165 [1000/1251 ( 80%)]  Loss: 3.473 (3.70)  Time: 0.796s, 1287.05/s  (0.804s, 1273.20/s)  LR: 8.265e-04  Data: 0.009 (0.013)
Train: 165 [1050/1251 ( 84%)]  Loss: 2.987 (3.67)  Time: 0.783s, 1308.13/s  (0.804s, 1273.14/s)  LR: 8.265e-04  Data: 0.009 (0.012)
Train: 165 [1100/1251 ( 88%)]  Loss: 3.752 (3.67)  Time: 0.789s, 1298.39/s  (0.804s, 1273.38/s)  LR: 8.265e-04  Data: 0.009 (0.012)
Train: 165 [1150/1251 ( 92%)]  Loss: 3.221 (3.65)  Time: 0.781s, 1311.22/s  (0.804s, 1273.04/s)  LR: 8.265e-04  Data: 0.013 (0.012)
Train: 165 [1200/1251 ( 96%)]  Loss: 3.905 (3.66)  Time: 0.846s, 1210.50/s  (0.804s, 1273.22/s)  LR: 8.265e-04  Data: 0.010 (0.012)
Train: 165 [1250/1251 (100%)]  Loss: 3.853 (3.67)  Time: 0.772s, 1326.23/s  (0.804s, 1272.95/s)  LR: 8.265e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.688 (1.688)  Loss:  0.8022 (0.8022)  Acc@1: 88.1836 (88.1836)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.194 (0.601)  Loss:  1.0322 (1.3853)  Acc@1: 83.7264 (73.0440)  Acc@5: 95.7547 (91.5620)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-163.pth.tar', 73.14800001464843)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-160.pth.tar', 73.08200001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-155.pth.tar', 73.08000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-165.pth.tar', 73.0440000415039)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-158.pth.tar', 72.97400009765624)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-161.pth.tar', 72.9599999975586)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-156.pth.tar', 72.89800001220704)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-159.pth.tar', 72.8400000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-157.pth.tar', 72.79800009521485)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-150.pth.tar', 72.78200006591797)

Train: 166 [   0/1251 (  0%)]  Loss: 3.564 (3.56)  Time: 2.327s,  439.98/s  (2.327s,  439.98/s)  LR: 8.245e-04  Data: 1.567 (1.567)
Train: 166 [  50/1251 (  4%)]  Loss: 3.478 (3.52)  Time: 0.773s, 1324.86/s  (0.838s, 1222.59/s)  LR: 8.245e-04  Data: 0.010 (0.046)
Train: 166 [ 100/1251 (  8%)]  Loss: 3.730 (3.59)  Time: 0.792s, 1293.34/s  (0.817s, 1253.51/s)  LR: 8.245e-04  Data: 0.013 (0.029)
Train: 166 [ 150/1251 ( 12%)]  Loss: 3.826 (3.65)  Time: 0.796s, 1286.67/s  (0.811s, 1263.04/s)  LR: 8.245e-04  Data: 0.010 (0.023)
Train: 166 [ 200/1251 ( 16%)]  Loss: 3.834 (3.69)  Time: 0.771s, 1327.82/s  (0.809s, 1266.15/s)  LR: 8.245e-04  Data: 0.009 (0.020)
Train: 166 [ 250/1251 ( 20%)]  Loss: 3.746 (3.70)  Time: 0.834s, 1227.71/s  (0.808s, 1267.95/s)  LR: 8.245e-04  Data: 0.010 (0.018)
Train: 166 [ 300/1251 ( 24%)]  Loss: 3.502 (3.67)  Time: 0.774s, 1322.90/s  (0.807s, 1268.77/s)  LR: 8.245e-04  Data: 0.010 (0.017)
Train: 166 [ 350/1251 ( 28%)]  Loss: 3.467 (3.64)  Time: 0.800s, 1279.21/s  (0.807s, 1268.62/s)  LR: 8.245e-04  Data: 0.011 (0.016)
Train: 166 [ 400/1251 ( 32%)]  Loss: 3.280 (3.60)  Time: 0.782s, 1309.14/s  (0.807s, 1269.51/s)  LR: 8.245e-04  Data: 0.010 (0.016)
Train: 166 [ 450/1251 ( 36%)]  Loss: 3.172 (3.56)  Time: 0.791s, 1294.27/s  (0.806s, 1270.58/s)  LR: 8.245e-04  Data: 0.014 (0.015)
Train: 166 [ 500/1251 ( 40%)]  Loss: 3.769 (3.58)  Time: 0.773s, 1325.38/s  (0.805s, 1272.31/s)  LR: 8.245e-04  Data: 0.009 (0.015)
Train: 166 [ 550/1251 ( 44%)]  Loss: 3.556 (3.58)  Time: 0.774s, 1322.28/s  (0.804s, 1273.08/s)  LR: 8.245e-04  Data: 0.010 (0.014)
Train: 166 [ 600/1251 ( 48%)]  Loss: 3.499 (3.57)  Time: 0.781s, 1310.74/s  (0.805s, 1272.34/s)  LR: 8.245e-04  Data: 0.010 (0.014)
Train: 166 [ 650/1251 ( 52%)]  Loss: 3.676 (3.58)  Time: 0.818s, 1251.47/s  (0.804s, 1273.28/s)  LR: 8.245e-04  Data: 0.010 (0.014)
Train: 166 [ 700/1251 ( 56%)]  Loss: 3.839 (3.60)  Time: 0.799s, 1281.24/s  (0.804s, 1273.00/s)  LR: 8.245e-04  Data: 0.011 (0.014)
Train: 166 [ 750/1251 ( 60%)]  Loss: 3.399 (3.58)  Time: 0.824s, 1242.99/s  (0.804s, 1273.15/s)  LR: 8.245e-04  Data: 0.013 (0.013)
Train: 166 [ 800/1251 ( 64%)]  Loss: 3.735 (3.59)  Time: 0.818s, 1252.53/s  (0.804s, 1273.16/s)  LR: 8.245e-04  Data: 0.010 (0.013)
Train: 166 [ 850/1251 ( 68%)]  Loss: 3.643 (3.60)  Time: 0.826s, 1239.85/s  (0.804s, 1272.85/s)  LR: 8.245e-04  Data: 0.013 (0.013)
Train: 166 [ 900/1251 ( 72%)]  Loss: 3.436 (3.59)  Time: 0.784s, 1306.86/s  (0.805s, 1272.36/s)  LR: 8.245e-04  Data: 0.016 (0.013)
Train: 166 [ 950/1251 ( 76%)]  Loss: 3.784 (3.60)  Time: 0.849s, 1206.48/s  (0.805s, 1272.30/s)  LR: 8.245e-04  Data: 0.009 (0.013)
Train: 166 [1000/1251 ( 80%)]  Loss: 3.665 (3.60)  Time: 0.803s, 1275.35/s  (0.805s, 1272.56/s)  LR: 8.245e-04  Data: 0.010 (0.013)
Train: 166 [1050/1251 ( 84%)]  Loss: 3.525 (3.60)  Time: 0.837s, 1223.43/s  (0.805s, 1272.55/s)  LR: 8.245e-04  Data: 0.012 (0.013)
Train: 166 [1100/1251 ( 88%)]  Loss: 3.826 (3.61)  Time: 0.772s, 1326.54/s  (0.805s, 1272.67/s)  LR: 8.245e-04  Data: 0.010 (0.013)
Train: 166 [1150/1251 ( 92%)]  Loss: 3.724 (3.61)  Time: 0.816s, 1255.47/s  (0.805s, 1272.19/s)  LR: 8.245e-04  Data: 0.009 (0.013)
Train: 166 [1200/1251 ( 96%)]  Loss: 3.632 (3.61)  Time: 0.774s, 1322.70/s  (0.805s, 1272.36/s)  LR: 8.245e-04  Data: 0.010 (0.013)
Train: 166 [1250/1251 (100%)]  Loss: 3.691 (3.62)  Time: 0.760s, 1346.60/s  (0.805s, 1272.82/s)  LR: 8.245e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.637 (1.637)  Loss:  0.8711 (0.8711)  Acc@1: 87.7930 (87.7930)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.194 (0.609)  Loss:  0.9717 (1.4158)  Acc@1: 84.7877 (72.9500)  Acc@5: 95.1651 (91.4480)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-163.pth.tar', 73.14800001464843)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-160.pth.tar', 73.08200001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-155.pth.tar', 73.08000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-165.pth.tar', 73.0440000415039)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-158.pth.tar', 72.97400009765624)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-161.pth.tar', 72.9599999975586)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-166.pth.tar', 72.94999998535157)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-156.pth.tar', 72.89800001220704)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-159.pth.tar', 72.8400000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-157.pth.tar', 72.79800009521485)

Train: 167 [   0/1251 (  0%)]  Loss: 3.747 (3.75)  Time: 2.771s,  369.55/s  (2.771s,  369.55/s)  LR: 8.225e-04  Data: 2.040 (2.040)
Train: 167 [  50/1251 (  4%)]  Loss: 3.516 (3.63)  Time: 0.774s, 1322.73/s  (0.851s, 1202.93/s)  LR: 8.225e-04  Data: 0.010 (0.058)
Train: 167 [ 100/1251 (  8%)]  Loss: 3.591 (3.62)  Time: 0.804s, 1273.34/s  (0.826s, 1239.64/s)  LR: 8.225e-04  Data: 0.011 (0.035)
Train: 167 [ 150/1251 ( 12%)]  Loss: 3.860 (3.68)  Time: 0.863s, 1186.71/s  (0.819s, 1251.01/s)  LR: 8.225e-04  Data: 0.010 (0.027)
Train: 167 [ 200/1251 ( 16%)]  Loss: 3.489 (3.64)  Time: 0.774s, 1322.87/s  (0.814s, 1258.33/s)  LR: 8.225e-04  Data: 0.010 (0.023)
Train: 167 [ 250/1251 ( 20%)]  Loss: 3.531 (3.62)  Time: 0.793s, 1290.51/s  (0.810s, 1263.63/s)  LR: 8.225e-04  Data: 0.016 (0.021)
Train: 167 [ 300/1251 ( 24%)]  Loss: 3.625 (3.62)  Time: 0.860s, 1190.25/s  (0.809s, 1265.61/s)  LR: 8.225e-04  Data: 0.010 (0.019)
Train: 167 [ 350/1251 ( 28%)]  Loss: 3.393 (3.59)  Time: 0.836s, 1224.54/s  (0.807s, 1268.71/s)  LR: 8.225e-04  Data: 0.010 (0.018)
Train: 167 [ 400/1251 ( 32%)]  Loss: 3.468 (3.58)  Time: 0.824s, 1243.06/s  (0.807s, 1269.12/s)  LR: 8.225e-04  Data: 0.010 (0.017)
Train: 167 [ 450/1251 ( 36%)]  Loss: 3.493 (3.57)  Time: 0.773s, 1325.15/s  (0.806s, 1271.13/s)  LR: 8.225e-04  Data: 0.010 (0.016)
Train: 167 [ 500/1251 ( 40%)]  Loss: 3.617 (3.58)  Time: 0.806s, 1270.57/s  (0.805s, 1271.34/s)  LR: 8.225e-04  Data: 0.010 (0.016)
Train: 167 [ 550/1251 ( 44%)]  Loss: 3.485 (3.57)  Time: 0.824s, 1242.83/s  (0.806s, 1270.64/s)  LR: 8.225e-04  Data: 0.009 (0.015)
Train: 167 [ 600/1251 ( 48%)]  Loss: 3.639 (3.57)  Time: 0.788s, 1299.89/s  (0.806s, 1270.95/s)  LR: 8.225e-04  Data: 0.010 (0.015)
Train: 167 [ 650/1251 ( 52%)]  Loss: 3.896 (3.60)  Time: 0.808s, 1267.52/s  (0.805s, 1271.36/s)  LR: 8.225e-04  Data: 0.010 (0.015)
Train: 167 [ 700/1251 ( 56%)]  Loss: 3.760 (3.61)  Time: 0.801s, 1277.79/s  (0.805s, 1271.91/s)  LR: 8.225e-04  Data: 0.011 (0.015)
Train: 167 [ 750/1251 ( 60%)]  Loss: 3.864 (3.62)  Time: 0.944s, 1085.12/s  (0.805s, 1271.91/s)  LR: 8.225e-04  Data: 0.012 (0.014)
Train: 167 [ 800/1251 ( 64%)]  Loss: 3.649 (3.62)  Time: 0.802s, 1276.19/s  (0.805s, 1272.66/s)  LR: 8.225e-04  Data: 0.013 (0.014)
Train: 167 [ 850/1251 ( 68%)]  Loss: 3.875 (3.64)  Time: 0.795s, 1287.38/s  (0.805s, 1272.65/s)  LR: 8.225e-04  Data: 0.014 (0.014)
Train: 167 [ 900/1251 ( 72%)]  Loss: 3.592 (3.64)  Time: 0.834s, 1228.50/s  (0.805s, 1272.68/s)  LR: 8.225e-04  Data: 0.010 (0.014)
Train: 167 [ 950/1251 ( 76%)]  Loss: 3.658 (3.64)  Time: 0.771s, 1327.31/s  (0.805s, 1272.68/s)  LR: 8.225e-04  Data: 0.010 (0.014)
Train: 167 [1000/1251 ( 80%)]  Loss: 3.457 (3.63)  Time: 0.860s, 1190.73/s  (0.805s, 1272.80/s)  LR: 8.225e-04  Data: 0.009 (0.013)
Train: 167 [1050/1251 ( 84%)]  Loss: 3.759 (3.63)  Time: 0.776s, 1319.38/s  (0.804s, 1273.41/s)  LR: 8.225e-04  Data: 0.010 (0.013)
Train: 167 [1100/1251 ( 88%)]  Loss: 3.641 (3.64)  Time: 0.787s, 1301.27/s  (0.804s, 1273.63/s)  LR: 8.225e-04  Data: 0.011 (0.013)
Train: 167 [1150/1251 ( 92%)]  Loss: 3.316 (3.62)  Time: 0.790s, 1296.40/s  (0.804s, 1273.58/s)  LR: 8.225e-04  Data: 0.012 (0.013)
Train: 167 [1200/1251 ( 96%)]  Loss: 3.358 (3.61)  Time: 0.822s, 1245.48/s  (0.804s, 1273.45/s)  LR: 8.225e-04  Data: 0.010 (0.013)
Train: 167 [1250/1251 (100%)]  Loss: 3.633 (3.61)  Time: 0.806s, 1269.94/s  (0.804s, 1273.62/s)  LR: 8.225e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.710 (1.710)  Loss:  0.7759 (0.7759)  Acc@1: 87.6953 (87.6953)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.194 (0.600)  Loss:  0.8501 (1.3950)  Acc@1: 82.4292 (72.3880)  Acc@5: 95.8726 (91.2720)
Train: 168 [   0/1251 (  0%)]  Loss: 3.639 (3.64)  Time: 2.387s,  429.01/s  (2.387s,  429.01/s)  LR: 8.205e-04  Data: 1.580 (1.580)
Train: 168 [  50/1251 (  4%)]  Loss: 3.163 (3.40)  Time: 0.776s, 1320.17/s  (0.843s, 1215.27/s)  LR: 8.205e-04  Data: 0.010 (0.047)
Train: 168 [ 100/1251 (  8%)]  Loss: 3.406 (3.40)  Time: 0.773s, 1324.99/s  (0.817s, 1252.67/s)  LR: 8.205e-04  Data: 0.009 (0.029)
Train: 168 [ 150/1251 ( 12%)]  Loss: 4.186 (3.60)  Time: 0.781s, 1311.44/s  (0.811s, 1263.33/s)  LR: 8.205e-04  Data: 0.013 (0.023)
Train: 168 [ 200/1251 ( 16%)]  Loss: 3.691 (3.62)  Time: 0.807s, 1269.02/s  (0.808s, 1267.04/s)  LR: 8.205e-04  Data: 0.013 (0.020)
Train: 168 [ 250/1251 ( 20%)]  Loss: 3.455 (3.59)  Time: 0.794s, 1289.51/s  (0.807s, 1269.09/s)  LR: 8.205e-04  Data: 0.010 (0.018)
Train: 168 [ 300/1251 ( 24%)]  Loss: 3.523 (3.58)  Time: 0.815s, 1256.78/s  (0.806s, 1269.88/s)  LR: 8.205e-04  Data: 0.011 (0.017)
Train: 168 [ 350/1251 ( 28%)]  Loss: 3.874 (3.62)  Time: 0.775s, 1321.24/s  (0.807s, 1269.07/s)  LR: 8.205e-04  Data: 0.011 (0.016)
Train: 168 [ 400/1251 ( 32%)]  Loss: 3.200 (3.57)  Time: 0.778s, 1316.03/s  (0.806s, 1269.71/s)  LR: 8.205e-04  Data: 0.013 (0.015)
Train: 168 [ 450/1251 ( 36%)]  Loss: 3.788 (3.59)  Time: 0.776s, 1319.14/s  (0.806s, 1270.75/s)  LR: 8.205e-04  Data: 0.011 (0.015)
Train: 168 [ 500/1251 ( 40%)]  Loss: 3.883 (3.62)  Time: 0.772s, 1327.09/s  (0.805s, 1271.33/s)  LR: 8.205e-04  Data: 0.009 (0.014)
Train: 168 [ 550/1251 ( 44%)]  Loss: 3.518 (3.61)  Time: 0.803s, 1274.68/s  (0.805s, 1272.36/s)  LR: 8.205e-04  Data: 0.010 (0.014)
Train: 168 [ 600/1251 ( 48%)]  Loss: 3.736 (3.62)  Time: 0.786s, 1303.29/s  (0.805s, 1272.36/s)  LR: 8.205e-04  Data: 0.010 (0.014)
Train: 168 [ 650/1251 ( 52%)]  Loss: 3.501 (3.61)  Time: 0.807s, 1269.06/s  (0.805s, 1272.01/s)  LR: 8.205e-04  Data: 0.012 (0.014)
Train: 168 [ 700/1251 ( 56%)]  Loss: 3.634 (3.61)  Time: 0.780s, 1312.41/s  (0.805s, 1272.04/s)  LR: 8.205e-04  Data: 0.010 (0.013)
Train: 168 [ 750/1251 ( 60%)]  Loss: 3.541 (3.61)  Time: 0.810s, 1263.54/s  (0.805s, 1272.14/s)  LR: 8.205e-04  Data: 0.010 (0.013)
Train: 168 [ 800/1251 ( 64%)]  Loss: 3.837 (3.62)  Time: 0.811s, 1261.91/s  (0.805s, 1271.45/s)  LR: 8.205e-04  Data: 0.012 (0.013)
Train: 168 [ 850/1251 ( 68%)]  Loss: 3.977 (3.64)  Time: 0.852s, 1202.17/s  (0.805s, 1271.81/s)  LR: 8.205e-04  Data: 0.011 (0.013)
Train: 168 [ 900/1251 ( 72%)]  Loss: 3.550 (3.64)  Time: 0.789s, 1297.21/s  (0.805s, 1272.36/s)  LR: 8.205e-04  Data: 0.011 (0.013)
Train: 168 [ 950/1251 ( 76%)]  Loss: 3.283 (3.62)  Time: 0.793s, 1291.50/s  (0.805s, 1272.59/s)  LR: 8.205e-04  Data: 0.009 (0.013)
Train: 168 [1000/1251 ( 80%)]  Loss: 3.849 (3.63)  Time: 0.816s, 1254.46/s  (0.805s, 1272.70/s)  LR: 8.205e-04  Data: 0.011 (0.013)
Train: 168 [1050/1251 ( 84%)]  Loss: 3.385 (3.62)  Time: 0.864s, 1184.96/s  (0.806s, 1271.04/s)  LR: 8.205e-04  Data: 0.013 (0.013)
Train: 168 [1100/1251 ( 88%)]  Loss: 3.666 (3.62)  Time: 0.775s, 1321.39/s  (0.806s, 1270.90/s)  LR: 8.205e-04  Data: 0.010 (0.013)
Train: 168 [1150/1251 ( 92%)]  Loss: 4.310 (3.65)  Time: 0.805s, 1272.19/s  (0.805s, 1272.72/s)  LR: 8.205e-04  Data: 0.010 (0.013)
Train: 168 [1200/1251 ( 96%)]  Loss: 3.331 (3.64)  Time: 0.772s, 1326.31/s  (0.804s, 1273.76/s)  LR: 8.205e-04  Data: 0.010 (0.012)
Train: 168 [1250/1251 (100%)]  Loss: 3.930 (3.65)  Time: 0.800s, 1279.60/s  (0.804s, 1273.88/s)  LR: 8.205e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.626 (1.626)  Loss:  0.9531 (0.9531)  Acc@1: 88.2812 (88.2812)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.194 (0.594)  Loss:  0.8936 (1.4049)  Acc@1: 83.6085 (73.0560)  Acc@5: 95.6368 (91.8000)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-163.pth.tar', 73.14800001464843)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-160.pth.tar', 73.08200001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-155.pth.tar', 73.08000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-168.pth.tar', 73.05599999023437)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-165.pth.tar', 73.0440000415039)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-158.pth.tar', 72.97400009765624)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-161.pth.tar', 72.9599999975586)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-166.pth.tar', 72.94999998535157)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-156.pth.tar', 72.89800001220704)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-159.pth.tar', 72.8400000732422)

Train: 169 [   0/1251 (  0%)]  Loss: 3.418 (3.42)  Time: 2.384s,  429.52/s  (2.384s,  429.52/s)  LR: 8.185e-04  Data: 1.656 (1.656)
Train: 169 [  50/1251 (  4%)]  Loss: 3.534 (3.48)  Time: 0.808s, 1266.56/s  (0.849s, 1206.02/s)  LR: 8.185e-04  Data: 0.011 (0.049)
Train: 169 [ 100/1251 (  8%)]  Loss: 3.273 (3.41)  Time: 0.773s, 1325.05/s  (0.825s, 1241.45/s)  LR: 8.185e-04  Data: 0.009 (0.030)
Train: 169 [ 150/1251 ( 12%)]  Loss: 3.648 (3.47)  Time: 0.796s, 1286.18/s  (0.819s, 1250.13/s)  LR: 8.185e-04  Data: 0.010 (0.024)
Train: 169 [ 200/1251 ( 16%)]  Loss: 3.598 (3.49)  Time: 0.798s, 1282.98/s  (0.816s, 1254.53/s)  LR: 8.185e-04  Data: 0.010 (0.020)
Train: 169 [ 250/1251 ( 20%)]  Loss: 3.222 (3.45)  Time: 0.846s, 1210.66/s  (0.814s, 1257.56/s)  LR: 8.185e-04  Data: 0.010 (0.019)
Train: 169 [ 300/1251 ( 24%)]  Loss: 3.851 (3.51)  Time: 0.833s, 1228.57/s  (0.811s, 1262.71/s)  LR: 8.185e-04  Data: 0.009 (0.017)
Train: 169 [ 350/1251 ( 28%)]  Loss: 3.411 (3.49)  Time: 0.774s, 1322.59/s  (0.810s, 1264.49/s)  LR: 8.185e-04  Data: 0.011 (0.016)
Train: 169 [ 400/1251 ( 32%)]  Loss: 3.678 (3.51)  Time: 0.783s, 1307.32/s  (0.810s, 1264.30/s)  LR: 8.185e-04  Data: 0.010 (0.016)
Train: 169 [ 450/1251 ( 36%)]  Loss: 3.273 (3.49)  Time: 0.803s, 1274.98/s  (0.808s, 1266.62/s)  LR: 8.185e-04  Data: 0.010 (0.015)
Train: 169 [ 500/1251 ( 40%)]  Loss: 3.440 (3.49)  Time: 0.837s, 1223.13/s  (0.808s, 1267.43/s)  LR: 8.185e-04  Data: 0.010 (0.015)
Train: 169 [ 550/1251 ( 44%)]  Loss: 3.897 (3.52)  Time: 0.774s, 1322.68/s  (0.808s, 1267.96/s)  LR: 8.185e-04  Data: 0.010 (0.014)
Train: 169 [ 600/1251 ( 48%)]  Loss: 3.857 (3.55)  Time: 0.786s, 1302.70/s  (0.807s, 1269.56/s)  LR: 8.185e-04  Data: 0.010 (0.014)
Train: 169 [ 650/1251 ( 52%)]  Loss: 3.328 (3.53)  Time: 0.817s, 1252.71/s  (0.806s, 1269.82/s)  LR: 8.185e-04  Data: 0.012 (0.014)
Train: 169 [ 700/1251 ( 56%)]  Loss: 3.693 (3.54)  Time: 0.850s, 1205.28/s  (0.806s, 1269.98/s)  LR: 8.185e-04  Data: 0.010 (0.014)
Train: 169 [ 750/1251 ( 60%)]  Loss: 3.509 (3.54)  Time: 0.837s, 1222.70/s  (0.806s, 1269.90/s)  LR: 8.185e-04  Data: 0.010 (0.013)
Train: 169 [ 800/1251 ( 64%)]  Loss: 3.753 (3.55)  Time: 0.771s, 1327.34/s  (0.806s, 1270.14/s)  LR: 8.185e-04  Data: 0.010 (0.013)
Train: 169 [ 850/1251 ( 68%)]  Loss: 3.827 (3.57)  Time: 0.833s, 1229.09/s  (0.806s, 1270.24/s)  LR: 8.185e-04  Data: 0.015 (0.013)
Train: 169 [ 900/1251 ( 72%)]  Loss: 3.985 (3.59)  Time: 0.795s, 1287.55/s  (0.806s, 1270.47/s)  LR: 8.185e-04  Data: 0.018 (0.013)
Train: 169 [ 950/1251 ( 76%)]  Loss: 3.667 (3.59)  Time: 0.812s, 1260.63/s  (0.806s, 1270.68/s)  LR: 8.185e-04  Data: 0.011 (0.013)
Train: 169 [1000/1251 ( 80%)]  Loss: 3.728 (3.60)  Time: 0.783s, 1307.53/s  (0.806s, 1270.55/s)  LR: 8.185e-04  Data: 0.010 (0.013)
Train: 169 [1050/1251 ( 84%)]  Loss: 3.711 (3.60)  Time: 0.860s, 1190.81/s  (0.806s, 1270.71/s)  LR: 8.185e-04  Data: 0.010 (0.013)
Train: 169 [1100/1251 ( 88%)]  Loss: 3.799 (3.61)  Time: 0.787s, 1300.88/s  (0.806s, 1270.99/s)  LR: 8.185e-04  Data: 0.010 (0.013)
Train: 169 [1150/1251 ( 92%)]  Loss: 3.505 (3.61)  Time: 0.776s, 1318.89/s  (0.806s, 1271.21/s)  LR: 8.185e-04  Data: 0.010 (0.013)
Train: 169 [1200/1251 ( 96%)]  Loss: 3.643 (3.61)  Time: 0.894s, 1144.82/s  (0.806s, 1271.14/s)  LR: 8.185e-04  Data: 0.010 (0.013)
Train: 169 [1250/1251 (100%)]  Loss: 3.677 (3.61)  Time: 0.759s, 1348.92/s  (0.805s, 1271.41/s)  LR: 8.185e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.614 (1.614)  Loss:  0.8652 (0.8652)  Acc@1: 89.2578 (89.2578)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.194 (0.605)  Loss:  1.1240 (1.5025)  Acc@1: 83.3726 (72.9120)  Acc@5: 94.8113 (91.6240)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-163.pth.tar', 73.14800001464843)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-160.pth.tar', 73.08200001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-155.pth.tar', 73.08000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-168.pth.tar', 73.05599999023437)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-165.pth.tar', 73.0440000415039)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-158.pth.tar', 72.97400009765624)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-161.pth.tar', 72.9599999975586)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-166.pth.tar', 72.94999998535157)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-169.pth.tar', 72.91200001708984)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-156.pth.tar', 72.89800001220704)

Train: 170 [   0/1251 (  0%)]  Loss: 3.634 (3.63)  Time: 2.274s,  450.23/s  (2.274s,  450.23/s)  LR: 8.165e-04  Data: 1.540 (1.540)
Train: 170 [  50/1251 (  4%)]  Loss: 3.841 (3.74)  Time: 0.867s, 1181.25/s  (0.834s, 1228.29/s)  LR: 8.165e-04  Data: 0.010 (0.046)
Train: 170 [ 100/1251 (  8%)]  Loss: 3.473 (3.65)  Time: 0.770s, 1329.07/s  (0.819s, 1249.61/s)  LR: 8.165e-04  Data: 0.010 (0.029)
Train: 170 [ 150/1251 ( 12%)]  Loss: 3.614 (3.64)  Time: 0.805s, 1272.34/s  (0.814s, 1258.62/s)  LR: 8.165e-04  Data: 0.013 (0.023)
Train: 170 [ 200/1251 ( 16%)]  Loss: 3.611 (3.63)  Time: 0.770s, 1330.01/s  (0.810s, 1264.37/s)  LR: 8.165e-04  Data: 0.010 (0.020)
Train: 170 [ 250/1251 ( 20%)]  Loss: 3.413 (3.60)  Time: 0.824s, 1242.65/s  (0.808s, 1267.51/s)  LR: 8.165e-04  Data: 0.010 (0.018)
Train: 170 [ 300/1251 ( 24%)]  Loss: 3.646 (3.60)  Time: 0.785s, 1305.27/s  (0.807s, 1268.87/s)  LR: 8.165e-04  Data: 0.010 (0.017)
Train: 170 [ 350/1251 ( 28%)]  Loss: 3.867 (3.64)  Time: 0.777s, 1317.93/s  (0.806s, 1271.00/s)  LR: 8.165e-04  Data: 0.010 (0.016)
Train: 170 [ 400/1251 ( 32%)]  Loss: 3.644 (3.64)  Time: 0.784s, 1306.12/s  (0.805s, 1272.16/s)  LR: 8.165e-04  Data: 0.010 (0.015)
Train: 170 [ 450/1251 ( 36%)]  Loss: 3.942 (3.67)  Time: 0.811s, 1262.70/s  (0.805s, 1272.60/s)  LR: 8.165e-04  Data: 0.009 (0.015)
Train: 170 [ 500/1251 ( 40%)]  Loss: 3.500 (3.65)  Time: 0.818s, 1252.07/s  (0.805s, 1272.59/s)  LR: 8.165e-04  Data: 0.015 (0.015)
Train: 170 [ 550/1251 ( 44%)]  Loss: 3.289 (3.62)  Time: 0.791s, 1294.52/s  (0.804s, 1273.07/s)  LR: 8.165e-04  Data: 0.016 (0.014)
Train: 170 [ 600/1251 ( 48%)]  Loss: 3.938 (3.65)  Time: 0.776s, 1319.14/s  (0.804s, 1273.22/s)  LR: 8.165e-04  Data: 0.010 (0.014)
Train: 170 [ 650/1251 ( 52%)]  Loss: 3.421 (3.63)  Time: 0.842s, 1216.31/s  (0.804s, 1273.05/s)  LR: 8.165e-04  Data: 0.010 (0.014)
Train: 170 [ 700/1251 ( 56%)]  Loss: 3.567 (3.63)  Time: 0.772s, 1326.32/s  (0.804s, 1273.29/s)  LR: 8.165e-04  Data: 0.010 (0.014)
Train: 170 [ 750/1251 ( 60%)]  Loss: 3.402 (3.61)  Time: 0.807s, 1269.26/s  (0.804s, 1273.27/s)  LR: 8.165e-04  Data: 0.018 (0.014)
Train: 170 [ 800/1251 ( 64%)]  Loss: 3.582 (3.61)  Time: 0.810s, 1264.86/s  (0.804s, 1273.40/s)  LR: 8.165e-04  Data: 0.016 (0.013)
Train: 170 [ 850/1251 ( 68%)]  Loss: 3.676 (3.61)  Time: 0.829s, 1235.68/s  (0.804s, 1274.06/s)  LR: 8.165e-04  Data: 0.010 (0.013)
Train: 170 [ 900/1251 ( 72%)]  Loss: 3.942 (3.63)  Time: 0.824s, 1243.12/s  (0.803s, 1274.67/s)  LR: 8.165e-04  Data: 0.010 (0.013)
Train: 170 [ 950/1251 ( 76%)]  Loss: 3.361 (3.62)  Time: 0.800s, 1280.47/s  (0.803s, 1274.83/s)  LR: 8.165e-04  Data: 0.015 (0.013)
Train: 170 [1000/1251 ( 80%)]  Loss: 3.444 (3.61)  Time: 0.791s, 1295.24/s  (0.803s, 1274.76/s)  LR: 8.165e-04  Data: 0.011 (0.013)
Train: 170 [1050/1251 ( 84%)]  Loss: 3.516 (3.61)  Time: 0.800s, 1279.81/s  (0.803s, 1274.63/s)  LR: 8.165e-04  Data: 0.012 (0.013)
Train: 170 [1100/1251 ( 88%)]  Loss: 3.561 (3.60)  Time: 0.773s, 1325.32/s  (0.803s, 1275.09/s)  LR: 8.165e-04  Data: 0.010 (0.013)
Train: 170 [1150/1251 ( 92%)]  Loss: 3.439 (3.60)  Time: 0.790s, 1296.28/s  (0.803s, 1274.99/s)  LR: 8.165e-04  Data: 0.013 (0.013)
Train: 170 [1200/1251 ( 96%)]  Loss: 3.564 (3.60)  Time: 0.825s, 1241.63/s  (0.803s, 1274.97/s)  LR: 8.165e-04  Data: 0.012 (0.013)
Train: 170 [1250/1251 (100%)]  Loss: 3.601 (3.60)  Time: 0.789s, 1298.31/s  (0.803s, 1275.00/s)  LR: 8.165e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.658 (1.658)  Loss:  0.7876 (0.7876)  Acc@1: 89.1602 (89.1602)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.194 (0.589)  Loss:  0.9238 (1.4601)  Acc@1: 83.7264 (72.4440)  Acc@5: 96.3443 (91.5940)
Train: 171 [   0/1251 (  0%)]  Loss: 3.402 (3.40)  Time: 2.505s,  408.74/s  (2.505s,  408.74/s)  LR: 8.145e-04  Data: 1.777 (1.777)
Train: 171 [  50/1251 (  4%)]  Loss: 3.665 (3.53)  Time: 0.835s, 1226.10/s  (0.845s, 1211.52/s)  LR: 8.145e-04  Data: 0.009 (0.049)
Train: 171 [ 100/1251 (  8%)]  Loss: 3.613 (3.56)  Time: 0.785s, 1304.83/s  (0.822s, 1245.09/s)  LR: 8.145e-04  Data: 0.015 (0.031)
Train: 171 [ 150/1251 ( 12%)]  Loss: 3.206 (3.47)  Time: 0.783s, 1307.06/s  (0.818s, 1252.03/s)  LR: 8.145e-04  Data: 0.014 (0.024)
Train: 171 [ 200/1251 ( 16%)]  Loss: 3.765 (3.53)  Time: 0.774s, 1322.30/s  (0.813s, 1259.19/s)  LR: 8.145e-04  Data: 0.010 (0.021)
Train: 171 [ 250/1251 ( 20%)]  Loss: 3.609 (3.54)  Time: 0.824s, 1243.34/s  (0.810s, 1264.68/s)  LR: 8.145e-04  Data: 0.010 (0.019)
Train: 171 [ 300/1251 ( 24%)]  Loss: 3.600 (3.55)  Time: 0.777s, 1317.59/s  (0.809s, 1266.48/s)  LR: 8.145e-04  Data: 0.009 (0.018)
Train: 171 [ 350/1251 ( 28%)]  Loss: 3.552 (3.55)  Time: 0.772s, 1326.59/s  (0.808s, 1268.02/s)  LR: 8.145e-04  Data: 0.010 (0.017)
Train: 171 [ 400/1251 ( 32%)]  Loss: 3.570 (3.55)  Time: 0.841s, 1217.51/s  (0.806s, 1269.86/s)  LR: 8.145e-04  Data: 0.013 (0.016)
Train: 171 [ 450/1251 ( 36%)]  Loss: 3.809 (3.58)  Time: 0.871s, 1176.32/s  (0.806s, 1270.15/s)  LR: 8.145e-04  Data: 0.011 (0.015)
Train: 171 [ 500/1251 ( 40%)]  Loss: 3.912 (3.61)  Time: 0.830s, 1233.49/s  (0.806s, 1269.88/s)  LR: 8.145e-04  Data: 0.009 (0.015)
Train: 171 [ 550/1251 ( 44%)]  Loss: 3.929 (3.64)  Time: 0.813s, 1259.79/s  (0.806s, 1269.77/s)  LR: 8.145e-04  Data: 0.011 (0.015)
Train: 171 [ 600/1251 ( 48%)]  Loss: 3.629 (3.64)  Time: 0.804s, 1273.58/s  (0.807s, 1269.55/s)  LR: 8.145e-04  Data: 0.012 (0.014)
Train: 171 [ 650/1251 ( 52%)]  Loss: 3.881 (3.65)  Time: 0.796s, 1286.99/s  (0.807s, 1269.45/s)  LR: 8.145e-04  Data: 0.010 (0.014)
Train: 171 [ 700/1251 ( 56%)]  Loss: 3.728 (3.66)  Time: 0.800s, 1279.45/s  (0.806s, 1270.37/s)  LR: 8.145e-04  Data: 0.011 (0.014)
Train: 171 [ 750/1251 ( 60%)]  Loss: 3.656 (3.66)  Time: 0.846s, 1209.85/s  (0.806s, 1270.67/s)  LR: 8.145e-04  Data: 0.010 (0.014)
Train: 171 [ 800/1251 ( 64%)]  Loss: 3.367 (3.64)  Time: 0.799s, 1281.36/s  (0.806s, 1271.01/s)  LR: 8.145e-04  Data: 0.010 (0.013)
Train: 171 [ 850/1251 ( 68%)]  Loss: 3.507 (3.63)  Time: 0.869s, 1178.22/s  (0.805s, 1271.30/s)  LR: 8.145e-04  Data: 0.010 (0.013)
Train: 171 [ 900/1251 ( 72%)]  Loss: 3.848 (3.64)  Time: 0.768s, 1332.55/s  (0.805s, 1271.82/s)  LR: 8.145e-04  Data: 0.010 (0.013)
Train: 171 [ 950/1251 ( 76%)]  Loss: 3.142 (3.62)  Time: 0.799s, 1280.86/s  (0.805s, 1272.37/s)  LR: 8.145e-04  Data: 0.010 (0.013)
Train: 171 [1000/1251 ( 80%)]  Loss: 3.674 (3.62)  Time: 0.773s, 1324.81/s  (0.804s, 1272.91/s)  LR: 8.145e-04  Data: 0.011 (0.013)
Train: 171 [1050/1251 ( 84%)]  Loss: 3.684 (3.62)  Time: 0.782s, 1309.72/s  (0.804s, 1273.26/s)  LR: 8.145e-04  Data: 0.009 (0.013)
Train: 171 [1100/1251 ( 88%)]  Loss: 3.799 (3.63)  Time: 0.829s, 1234.71/s  (0.804s, 1273.57/s)  LR: 8.145e-04  Data: 0.010 (0.013)
Train: 171 [1150/1251 ( 92%)]  Loss: 3.509 (3.63)  Time: 0.839s, 1220.27/s  (0.804s, 1273.69/s)  LR: 8.145e-04  Data: 0.009 (0.013)
Train: 171 [1200/1251 ( 96%)]  Loss: 3.750 (3.63)  Time: 0.773s, 1324.32/s  (0.804s, 1273.81/s)  LR: 8.145e-04  Data: 0.010 (0.013)
Train: 171 [1250/1251 (100%)]  Loss: 3.450 (3.63)  Time: 0.758s, 1350.15/s  (0.804s, 1274.34/s)  LR: 8.145e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.722 (1.722)  Loss:  0.8071 (0.8071)  Acc@1: 88.7695 (88.7695)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.194 (0.612)  Loss:  0.9824 (1.3712)  Acc@1: 85.2594 (73.3680)  Acc@5: 95.7547 (91.9460)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-171.pth.tar', 73.36800006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-163.pth.tar', 73.14800001464843)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-160.pth.tar', 73.08200001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-155.pth.tar', 73.08000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-168.pth.tar', 73.05599999023437)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-165.pth.tar', 73.0440000415039)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-158.pth.tar', 72.97400009765624)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-161.pth.tar', 72.9599999975586)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-166.pth.tar', 72.94999998535157)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-169.pth.tar', 72.91200001708984)

Train: 172 [   0/1251 (  0%)]  Loss: 3.254 (3.25)  Time: 2.354s,  435.04/s  (2.354s,  435.04/s)  LR: 8.125e-04  Data: 1.619 (1.619)
Train: 172 [  50/1251 (  4%)]  Loss: 3.788 (3.52)  Time: 0.806s, 1270.67/s  (0.835s, 1226.73/s)  LR: 8.125e-04  Data: 0.013 (0.045)
Train: 172 [ 100/1251 (  8%)]  Loss: 3.770 (3.60)  Time: 0.811s, 1262.12/s  (0.818s, 1251.79/s)  LR: 8.125e-04  Data: 0.010 (0.028)
Train: 172 [ 150/1251 ( 12%)]  Loss: 3.515 (3.58)  Time: 0.807s, 1268.94/s  (0.813s, 1260.06/s)  LR: 8.125e-04  Data: 0.010 (0.023)
Train: 172 [ 200/1251 ( 16%)]  Loss: 3.366 (3.54)  Time: 0.833s, 1229.84/s  (0.811s, 1262.62/s)  LR: 8.125e-04  Data: 0.010 (0.020)
Train: 172 [ 250/1251 ( 20%)]  Loss: 3.901 (3.60)  Time: 0.772s, 1325.82/s  (0.809s, 1265.52/s)  LR: 8.125e-04  Data: 0.012 (0.018)
Train: 172 [ 300/1251 ( 24%)]  Loss: 3.331 (3.56)  Time: 0.881s, 1162.75/s  (0.808s, 1267.12/s)  LR: 8.125e-04  Data: 0.010 (0.017)
Train: 172 [ 350/1251 ( 28%)]  Loss: 3.648 (3.57)  Time: 0.872s, 1174.64/s  (0.807s, 1268.39/s)  LR: 8.125e-04  Data: 0.010 (0.016)
Train: 172 [ 400/1251 ( 32%)]  Loss: 3.733 (3.59)  Time: 0.866s, 1183.13/s  (0.807s, 1269.33/s)  LR: 8.125e-04  Data: 0.010 (0.015)
Train: 172 [ 450/1251 ( 36%)]  Loss: 3.777 (3.61)  Time: 0.802s, 1277.20/s  (0.806s, 1270.97/s)  LR: 8.125e-04  Data: 0.010 (0.015)
Train: 172 [ 500/1251 ( 40%)]  Loss: 3.487 (3.60)  Time: 0.860s, 1190.07/s  (0.806s, 1271.12/s)  LR: 8.125e-04  Data: 0.010 (0.015)
Train: 172 [ 550/1251 ( 44%)]  Loss: 3.817 (3.62)  Time: 0.772s, 1326.71/s  (0.805s, 1271.71/s)  LR: 8.125e-04  Data: 0.009 (0.014)
Train: 172 [ 600/1251 ( 48%)]  Loss: 3.635 (3.62)  Time: 0.782s, 1309.99/s  (0.805s, 1272.25/s)  LR: 8.125e-04  Data: 0.012 (0.014)
Train: 172 [ 650/1251 ( 52%)]  Loss: 3.682 (3.62)  Time: 0.813s, 1259.18/s  (0.805s, 1272.71/s)  LR: 8.125e-04  Data: 0.010 (0.014)
Train: 172 [ 700/1251 ( 56%)]  Loss: 3.798 (3.63)  Time: 0.791s, 1295.22/s  (0.804s, 1273.56/s)  LR: 8.125e-04  Data: 0.010 (0.014)
Train: 172 [ 750/1251 ( 60%)]  Loss: 3.737 (3.64)  Time: 0.778s, 1316.07/s  (0.804s, 1274.08/s)  LR: 8.125e-04  Data: 0.010 (0.013)
Train: 172 [ 800/1251 ( 64%)]  Loss: 3.771 (3.65)  Time: 0.820s, 1249.37/s  (0.803s, 1274.48/s)  LR: 8.125e-04  Data: 0.010 (0.013)
Train: 172 [ 850/1251 ( 68%)]  Loss: 3.772 (3.65)  Time: 0.776s, 1320.20/s  (0.803s, 1274.90/s)  LR: 8.125e-04  Data: 0.010 (0.013)
Train: 172 [ 900/1251 ( 72%)]  Loss: 3.497 (3.65)  Time: 0.798s, 1283.14/s  (0.803s, 1274.88/s)  LR: 8.125e-04  Data: 0.010 (0.013)
Train: 172 [ 950/1251 ( 76%)]  Loss: 3.651 (3.65)  Time: 0.793s, 1290.83/s  (0.803s, 1275.45/s)  LR: 8.125e-04  Data: 0.010 (0.013)
Train: 172 [1000/1251 ( 80%)]  Loss: 3.745 (3.65)  Time: 0.773s, 1324.13/s  (0.803s, 1275.58/s)  LR: 8.125e-04  Data: 0.010 (0.013)
Train: 172 [1050/1251 ( 84%)]  Loss: 3.734 (3.65)  Time: 0.811s, 1262.30/s  (0.803s, 1275.55/s)  LR: 8.125e-04  Data: 0.010 (0.013)
Train: 172 [1100/1251 ( 88%)]  Loss: 3.552 (3.65)  Time: 0.816s, 1255.31/s  (0.803s, 1275.46/s)  LR: 8.125e-04  Data: 0.010 (0.013)
Train: 172 [1150/1251 ( 92%)]  Loss: 3.496 (3.64)  Time: 0.794s, 1289.96/s  (0.803s, 1275.35/s)  LR: 8.125e-04  Data: 0.015 (0.013)
Train: 172 [1200/1251 ( 96%)]  Loss: 3.907 (3.65)  Time: 0.779s, 1313.80/s  (0.803s, 1275.38/s)  LR: 8.125e-04  Data: 0.011 (0.013)
Train: 172 [1250/1251 (100%)]  Loss: 3.562 (3.65)  Time: 0.791s, 1294.22/s  (0.803s, 1275.59/s)  LR: 8.125e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.765 (1.765)  Loss:  0.8589 (0.8589)  Acc@1: 88.3789 (88.3789)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.194 (0.600)  Loss:  0.8813 (1.3987)  Acc@1: 83.8443 (73.2160)  Acc@5: 95.8727 (91.7260)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-171.pth.tar', 73.36800006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-172.pth.tar', 73.2159999633789)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-163.pth.tar', 73.14800001464843)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-160.pth.tar', 73.08200001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-155.pth.tar', 73.08000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-168.pth.tar', 73.05599999023437)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-165.pth.tar', 73.0440000415039)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-158.pth.tar', 72.97400009765624)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-161.pth.tar', 72.9599999975586)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-166.pth.tar', 72.94999998535157)

Train: 173 [   0/1251 (  0%)]  Loss: 3.888 (3.89)  Time: 2.405s,  425.80/s  (2.405s,  425.80/s)  LR: 8.104e-04  Data: 1.646 (1.646)
Train: 173 [  50/1251 (  4%)]  Loss: 3.955 (3.92)  Time: 0.773s, 1324.91/s  (0.839s, 1220.26/s)  LR: 8.104e-04  Data: 0.010 (0.051)
Train: 173 [ 100/1251 (  8%)]  Loss: 3.244 (3.70)  Time: 0.829s, 1235.30/s  (0.819s, 1249.74/s)  LR: 8.104e-04  Data: 0.009 (0.031)
Train: 173 [ 150/1251 ( 12%)]  Loss: 3.706 (3.70)  Time: 0.772s, 1325.74/s  (0.812s, 1261.17/s)  LR: 8.104e-04  Data: 0.011 (0.024)
Train: 173 [ 200/1251 ( 16%)]  Loss: 3.746 (3.71)  Time: 0.831s, 1232.02/s  (0.811s, 1262.84/s)  LR: 8.104e-04  Data: 0.010 (0.021)
Train: 173 [ 250/1251 ( 20%)]  Loss: 3.647 (3.70)  Time: 0.835s, 1226.53/s  (0.809s, 1266.47/s)  LR: 8.104e-04  Data: 0.016 (0.019)
Train: 173 [ 300/1251 ( 24%)]  Loss: 3.491 (3.67)  Time: 0.774s, 1322.27/s  (0.806s, 1270.73/s)  LR: 8.104e-04  Data: 0.010 (0.018)
Train: 173 [ 350/1251 ( 28%)]  Loss: 3.686 (3.67)  Time: 0.812s, 1261.03/s  (0.805s, 1272.46/s)  LR: 8.104e-04  Data: 0.017 (0.017)
Train: 173 [ 400/1251 ( 32%)]  Loss: 3.439 (3.64)  Time: 0.774s, 1323.60/s  (0.805s, 1272.49/s)  LR: 8.104e-04  Data: 0.011 (0.016)
Train: 173 [ 450/1251 ( 36%)]  Loss: 3.909 (3.67)  Time: 0.813s, 1259.94/s  (0.805s, 1272.48/s)  LR: 8.104e-04  Data: 0.015 (0.016)
Train: 173 [ 500/1251 ( 40%)]  Loss: 3.637 (3.67)  Time: 0.800s, 1279.43/s  (0.805s, 1272.01/s)  LR: 8.104e-04  Data: 0.009 (0.015)
Train: 173 [ 550/1251 ( 44%)]  Loss: 3.677 (3.67)  Time: 0.817s, 1253.11/s  (0.805s, 1271.59/s)  LR: 8.104e-04  Data: 0.014 (0.015)
Train: 173 [ 600/1251 ( 48%)]  Loss: 3.630 (3.67)  Time: 0.806s, 1270.31/s  (0.805s, 1271.48/s)  LR: 8.104e-04  Data: 0.010 (0.014)
Train: 173 [ 650/1251 ( 52%)]  Loss: 3.750 (3.67)  Time: 0.774s, 1322.49/s  (0.805s, 1272.79/s)  LR: 8.104e-04  Data: 0.010 (0.014)
Train: 173 [ 700/1251 ( 56%)]  Loss: 3.725 (3.68)  Time: 0.838s, 1221.38/s  (0.806s, 1270.40/s)  LR: 8.104e-04  Data: 0.013 (0.014)
Train: 173 [ 750/1251 ( 60%)]  Loss: 4.027 (3.70)  Time: 0.774s, 1323.19/s  (0.806s, 1270.96/s)  LR: 8.104e-04  Data: 0.009 (0.014)
Train: 173 [ 800/1251 ( 64%)]  Loss: 3.603 (3.69)  Time: 0.773s, 1324.43/s  (0.804s, 1273.88/s)  LR: 8.104e-04  Data: 0.010 (0.014)
Train: 173 [ 850/1251 ( 68%)]  Loss: 3.625 (3.69)  Time: 0.773s, 1324.99/s  (0.804s, 1274.22/s)  LR: 8.104e-04  Data: 0.010 (0.014)
Train: 173 [ 900/1251 ( 72%)]  Loss: 3.519 (3.68)  Time: 0.851s, 1203.10/s  (0.804s, 1274.27/s)  LR: 8.104e-04  Data: 0.010 (0.013)
Train: 173 [ 950/1251 ( 76%)]  Loss: 3.335 (3.66)  Time: 0.798s, 1282.48/s  (0.803s, 1274.83/s)  LR: 8.104e-04  Data: 0.009 (0.013)
Train: 173 [1000/1251 ( 80%)]  Loss: 3.632 (3.66)  Time: 0.839s, 1220.85/s  (0.803s, 1275.03/s)  LR: 8.104e-04  Data: 0.010 (0.013)
Train: 173 [1050/1251 ( 84%)]  Loss: 3.987 (3.68)  Time: 0.773s, 1324.27/s  (0.803s, 1274.96/s)  LR: 8.104e-04  Data: 0.011 (0.013)
Train: 173 [1100/1251 ( 88%)]  Loss: 3.487 (3.67)  Time: 0.772s, 1325.61/s  (0.803s, 1275.40/s)  LR: 8.104e-04  Data: 0.010 (0.013)
Train: 173 [1150/1251 ( 92%)]  Loss: 3.738 (3.67)  Time: 0.797s, 1285.11/s  (0.803s, 1275.47/s)  LR: 8.104e-04  Data: 0.011 (0.013)
Train: 173 [1200/1251 ( 96%)]  Loss: 3.215 (3.65)  Time: 0.786s, 1303.40/s  (0.803s, 1275.40/s)  LR: 8.104e-04  Data: 0.015 (0.013)
Train: 173 [1250/1251 (100%)]  Loss: 3.934 (3.66)  Time: 0.789s, 1298.18/s  (0.803s, 1275.57/s)  LR: 8.104e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.753 (1.753)  Loss:  0.8081 (0.8081)  Acc@1: 87.7930 (87.7930)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.194 (0.607)  Loss:  0.8418 (1.3863)  Acc@1: 84.7877 (73.0480)  Acc@5: 95.9906 (91.6820)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-171.pth.tar', 73.36800006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-172.pth.tar', 73.2159999633789)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-163.pth.tar', 73.14800001464843)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-160.pth.tar', 73.08200001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-155.pth.tar', 73.08000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-168.pth.tar', 73.05599999023437)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-173.pth.tar', 73.04800011474609)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-165.pth.tar', 73.0440000415039)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-158.pth.tar', 72.97400009765624)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-161.pth.tar', 72.9599999975586)

Train: 174 [   0/1251 (  0%)]  Loss: 3.674 (3.67)  Time: 2.361s,  433.74/s  (2.361s,  433.74/s)  LR: 8.084e-04  Data: 1.623 (1.623)
Train: 174 [  50/1251 (  4%)]  Loss: 4.019 (3.85)  Time: 0.777s, 1318.13/s  (0.843s, 1215.09/s)  LR: 8.084e-04  Data: 0.010 (0.052)
Train: 174 [ 100/1251 (  8%)]  Loss: 3.602 (3.77)  Time: 0.773s, 1325.28/s  (0.822s, 1246.22/s)  LR: 8.084e-04  Data: 0.010 (0.032)
Train: 174 [ 150/1251 ( 12%)]  Loss: 4.040 (3.83)  Time: 0.776s, 1319.01/s  (0.814s, 1258.75/s)  LR: 8.084e-04  Data: 0.010 (0.025)
Train: 174 [ 200/1251 ( 16%)]  Loss: 3.515 (3.77)  Time: 0.803s, 1275.36/s  (0.810s, 1264.81/s)  LR: 8.084e-04  Data: 0.010 (0.021)
Train: 174 [ 250/1251 ( 20%)]  Loss: 3.680 (3.76)  Time: 0.813s, 1258.95/s  (0.808s, 1267.20/s)  LR: 8.084e-04  Data: 0.011 (0.019)
Train: 174 [ 300/1251 ( 24%)]  Loss: 3.874 (3.77)  Time: 0.801s, 1278.39/s  (0.809s, 1265.69/s)  LR: 8.084e-04  Data: 0.009 (0.018)
Train: 174 [ 350/1251 ( 28%)]  Loss: 3.722 (3.77)  Time: 0.879s, 1164.57/s  (0.807s, 1268.83/s)  LR: 8.084e-04  Data: 0.010 (0.017)
Train: 174 [ 400/1251 ( 32%)]  Loss: 3.494 (3.74)  Time: 0.806s, 1269.78/s  (0.807s, 1269.25/s)  LR: 8.084e-04  Data: 0.010 (0.016)
Train: 174 [ 450/1251 ( 36%)]  Loss: 3.532 (3.72)  Time: 0.785s, 1305.06/s  (0.807s, 1269.44/s)  LR: 8.084e-04  Data: 0.014 (0.016)
Train: 174 [ 500/1251 ( 40%)]  Loss: 3.806 (3.72)  Time: 0.791s, 1294.02/s  (0.806s, 1269.81/s)  LR: 8.084e-04  Data: 0.016 (0.015)
Train: 174 [ 550/1251 ( 44%)]  Loss: 3.551 (3.71)  Time: 0.795s, 1288.68/s  (0.807s, 1269.67/s)  LR: 8.084e-04  Data: 0.010 (0.015)
Train: 174 [ 600/1251 ( 48%)]  Loss: 3.637 (3.70)  Time: 0.785s, 1303.65/s  (0.806s, 1271.17/s)  LR: 8.084e-04  Data: 0.010 (0.014)
Train: 174 [ 650/1251 ( 52%)]  Loss: 3.600 (3.70)  Time: 0.781s, 1310.98/s  (0.805s, 1271.92/s)  LR: 8.084e-04  Data: 0.010 (0.014)
Train: 174 [ 700/1251 ( 56%)]  Loss: 3.352 (3.67)  Time: 0.799s, 1280.89/s  (0.805s, 1272.44/s)  LR: 8.084e-04  Data: 0.011 (0.014)
Train: 174 [ 750/1251 ( 60%)]  Loss: 3.792 (3.68)  Time: 0.838s, 1222.62/s  (0.805s, 1272.78/s)  LR: 8.084e-04  Data: 0.010 (0.014)
Train: 174 [ 800/1251 ( 64%)]  Loss: 3.281 (3.66)  Time: 0.783s, 1307.16/s  (0.804s, 1273.20/s)  LR: 8.084e-04  Data: 0.014 (0.014)
Train: 174 [ 850/1251 ( 68%)]  Loss: 3.461 (3.65)  Time: 0.855s, 1198.22/s  (0.804s, 1273.26/s)  LR: 8.084e-04  Data: 0.010 (0.013)
Train: 174 [ 900/1251 ( 72%)]  Loss: 3.075 (3.62)  Time: 0.804s, 1273.46/s  (0.804s, 1273.17/s)  LR: 8.084e-04  Data: 0.010 (0.013)
Train: 174 [ 950/1251 ( 76%)]  Loss: 3.906 (3.63)  Time: 0.835s, 1226.46/s  (0.804s, 1273.15/s)  LR: 8.084e-04  Data: 0.010 (0.013)
Train: 174 [1000/1251 ( 80%)]  Loss: 3.594 (3.63)  Time: 0.813s, 1259.77/s  (0.805s, 1272.52/s)  LR: 8.084e-04  Data: 0.010 (0.013)
Train: 174 [1050/1251 ( 84%)]  Loss: 3.605 (3.63)  Time: 0.800s, 1279.20/s  (0.805s, 1272.55/s)  LR: 8.084e-04  Data: 0.010 (0.013)
Train: 174 [1100/1251 ( 88%)]  Loss: 3.662 (3.63)  Time: 0.812s, 1260.94/s  (0.804s, 1273.02/s)  LR: 8.084e-04  Data: 0.013 (0.013)
Train: 174 [1150/1251 ( 92%)]  Loss: 3.588 (3.63)  Time: 0.862s, 1187.53/s  (0.804s, 1272.90/s)  LR: 8.084e-04  Data: 0.010 (0.013)
Train: 174 [1200/1251 ( 96%)]  Loss: 3.907 (3.64)  Time: 0.832s, 1230.10/s  (0.804s, 1273.04/s)  LR: 8.084e-04  Data: 0.010 (0.013)
Train: 174 [1250/1251 (100%)]  Loss: 3.681 (3.64)  Time: 0.799s, 1281.51/s  (0.804s, 1273.52/s)  LR: 8.084e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.550 (1.550)  Loss:  0.9248 (0.9248)  Acc@1: 88.6719 (88.6719)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.194 (0.593)  Loss:  0.9277 (1.4427)  Acc@1: 84.5519 (73.0360)  Acc@5: 95.5189 (91.7540)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-171.pth.tar', 73.36800006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-172.pth.tar', 73.2159999633789)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-163.pth.tar', 73.14800001464843)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-160.pth.tar', 73.08200001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-155.pth.tar', 73.08000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-168.pth.tar', 73.05599999023437)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-173.pth.tar', 73.04800011474609)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-165.pth.tar', 73.0440000415039)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-174.pth.tar', 73.03600001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-158.pth.tar', 72.97400009765624)

Train: 175 [   0/1251 (  0%)]  Loss: 3.579 (3.58)  Time: 2.414s,  424.13/s  (2.414s,  424.13/s)  LR: 8.063e-04  Data: 1.679 (1.679)
Train: 175 [  50/1251 (  4%)]  Loss: 3.502 (3.54)  Time: 0.813s, 1259.31/s  (0.833s, 1230.03/s)  LR: 8.063e-04  Data: 0.010 (0.049)
Train: 175 [ 100/1251 (  8%)]  Loss: 3.385 (3.49)  Time: 0.827s, 1238.46/s  (0.821s, 1246.86/s)  LR: 8.063e-04  Data: 0.010 (0.030)
Train: 175 [ 150/1251 ( 12%)]  Loss: 3.581 (3.51)  Time: 0.798s, 1283.19/s  (0.815s, 1255.74/s)  LR: 8.063e-04  Data: 0.014 (0.024)
Train: 175 [ 200/1251 ( 16%)]  Loss: 3.646 (3.54)  Time: 0.808s, 1267.37/s  (0.813s, 1259.98/s)  LR: 8.063e-04  Data: 0.012 (0.021)
Train: 175 [ 250/1251 ( 20%)]  Loss: 3.731 (3.57)  Time: 0.808s, 1267.90/s  (0.810s, 1263.66/s)  LR: 8.063e-04  Data: 0.010 (0.019)
Train: 175 [ 300/1251 ( 24%)]  Loss: 3.284 (3.53)  Time: 0.807s, 1268.60/s  (0.809s, 1265.04/s)  LR: 8.063e-04  Data: 0.016 (0.017)
Train: 175 [ 350/1251 ( 28%)]  Loss: 4.031 (3.59)  Time: 0.785s, 1304.94/s  (0.810s, 1264.89/s)  LR: 8.063e-04  Data: 0.014 (0.017)
Train: 175 [ 400/1251 ( 32%)]  Loss: 3.481 (3.58)  Time: 0.773s, 1325.10/s  (0.808s, 1267.31/s)  LR: 8.063e-04  Data: 0.010 (0.016)
Train: 175 [ 450/1251 ( 36%)]  Loss: 3.656 (3.59)  Time: 0.901s, 1136.84/s  (0.808s, 1267.95/s)  LR: 8.063e-04  Data: 0.013 (0.015)
Train: 175 [ 500/1251 ( 40%)]  Loss: 3.151 (3.55)  Time: 0.802s, 1277.26/s  (0.806s, 1269.74/s)  LR: 8.063e-04  Data: 0.010 (0.015)
Train: 175 [ 550/1251 ( 44%)]  Loss: 3.541 (3.55)  Time: 0.771s, 1328.11/s  (0.806s, 1271.01/s)  LR: 8.063e-04  Data: 0.011 (0.015)
Train: 175 [ 600/1251 ( 48%)]  Loss: 3.300 (3.53)  Time: 0.807s, 1269.59/s  (0.805s, 1271.64/s)  LR: 8.063e-04  Data: 0.010 (0.014)
Train: 175 [ 650/1251 ( 52%)]  Loss: 3.736 (3.54)  Time: 0.808s, 1267.44/s  (0.805s, 1272.38/s)  LR: 8.063e-04  Data: 0.011 (0.014)
Train: 175 [ 700/1251 ( 56%)]  Loss: 3.717 (3.55)  Time: 0.792s, 1293.08/s  (0.805s, 1272.82/s)  LR: 8.063e-04  Data: 0.015 (0.014)
Train: 175 [ 750/1251 ( 60%)]  Loss: 3.684 (3.56)  Time: 0.778s, 1315.94/s  (0.804s, 1272.91/s)  LR: 8.063e-04  Data: 0.014 (0.014)
Train: 175 [ 800/1251 ( 64%)]  Loss: 3.852 (3.58)  Time: 0.773s, 1324.40/s  (0.805s, 1272.78/s)  LR: 8.063e-04  Data: 0.010 (0.014)
Train: 175 [ 850/1251 ( 68%)]  Loss: 3.499 (3.58)  Time: 0.811s, 1262.93/s  (0.805s, 1272.75/s)  LR: 8.063e-04  Data: 0.010 (0.013)
Train: 175 [ 900/1251 ( 72%)]  Loss: 3.437 (3.57)  Time: 0.810s, 1264.73/s  (0.805s, 1272.59/s)  LR: 8.063e-04  Data: 0.010 (0.013)
Train: 175 [ 950/1251 ( 76%)]  Loss: 3.699 (3.57)  Time: 0.787s, 1301.13/s  (0.804s, 1272.96/s)  LR: 8.063e-04  Data: 0.014 (0.013)
Train: 175 [1000/1251 ( 80%)]  Loss: 3.497 (3.57)  Time: 0.778s, 1316.39/s  (0.804s, 1273.80/s)  LR: 8.063e-04  Data: 0.010 (0.013)
Train: 175 [1050/1251 ( 84%)]  Loss: 3.865 (3.58)  Time: 0.774s, 1323.62/s  (0.803s, 1274.52/s)  LR: 8.063e-04  Data: 0.010 (0.013)
Train: 175 [1100/1251 ( 88%)]  Loss: 3.501 (3.58)  Time: 0.819s, 1250.43/s  (0.803s, 1274.43/s)  LR: 8.063e-04  Data: 0.011 (0.013)
Train: 175 [1150/1251 ( 92%)]  Loss: 3.533 (3.58)  Time: 0.812s, 1261.59/s  (0.803s, 1274.87/s)  LR: 8.063e-04  Data: 0.010 (0.013)
Train: 175 [1200/1251 ( 96%)]  Loss: 3.411 (3.57)  Time: 0.784s, 1306.48/s  (0.803s, 1274.75/s)  LR: 8.063e-04  Data: 0.010 (0.013)
Train: 175 [1250/1251 (100%)]  Loss: 3.273 (3.56)  Time: 0.769s, 1331.02/s  (0.803s, 1275.12/s)  LR: 8.063e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.667 (1.667)  Loss:  0.7915 (0.7915)  Acc@1: 87.3047 (87.3047)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.194 (0.603)  Loss:  0.8228 (1.3411)  Acc@1: 84.3160 (72.8820)  Acc@5: 96.6981 (91.7400)
Train: 176 [   0/1251 (  0%)]  Loss: 3.521 (3.52)  Time: 2.468s,  414.98/s  (2.468s,  414.98/s)  LR: 8.043e-04  Data: 1.741 (1.741)
Train: 176 [  50/1251 (  4%)]  Loss: 3.549 (3.53)  Time: 0.779s, 1314.26/s  (0.845s, 1211.38/s)  LR: 8.043e-04  Data: 0.010 (0.053)
Train: 176 [ 100/1251 (  8%)]  Loss: 3.701 (3.59)  Time: 0.778s, 1315.86/s  (0.824s, 1242.97/s)  LR: 8.043e-04  Data: 0.010 (0.032)
Train: 176 [ 150/1251 ( 12%)]  Loss: 3.506 (3.57)  Time: 0.792s, 1293.53/s  (0.816s, 1255.40/s)  LR: 8.043e-04  Data: 0.016 (0.025)
Train: 176 [ 200/1251 ( 16%)]  Loss: 3.889 (3.63)  Time: 0.778s, 1316.64/s  (0.811s, 1263.21/s)  LR: 8.043e-04  Data: 0.010 (0.022)
Train: 176 [ 250/1251 ( 20%)]  Loss: 3.849 (3.67)  Time: 0.796s, 1287.01/s  (0.809s, 1266.17/s)  LR: 8.043e-04  Data: 0.015 (0.020)
Train: 176 [ 300/1251 ( 24%)]  Loss: 3.277 (3.61)  Time: 0.821s, 1247.21/s  (0.808s, 1267.43/s)  LR: 8.043e-04  Data: 0.016 (0.018)
Train: 176 [ 350/1251 ( 28%)]  Loss: 3.749 (3.63)  Time: 0.788s, 1299.71/s  (0.807s, 1268.50/s)  LR: 8.043e-04  Data: 0.013 (0.017)
Train: 176 [ 400/1251 ( 32%)]  Loss: 3.685 (3.64)  Time: 0.774s, 1323.26/s  (0.807s, 1268.15/s)  LR: 8.043e-04  Data: 0.011 (0.016)
Train: 176 [ 450/1251 ( 36%)]  Loss: 3.431 (3.62)  Time: 0.772s, 1326.07/s  (0.807s, 1269.02/s)  LR: 8.043e-04  Data: 0.010 (0.016)
Train: 176 [ 500/1251 ( 40%)]  Loss: 3.341 (3.59)  Time: 0.832s, 1231.11/s  (0.807s, 1269.30/s)  LR: 8.043e-04  Data: 0.016 (0.015)
Train: 176 [ 550/1251 ( 44%)]  Loss: 3.632 (3.59)  Time: 0.815s, 1256.81/s  (0.806s, 1269.73/s)  LR: 8.043e-04  Data: 0.009 (0.015)
Train: 176 [ 600/1251 ( 48%)]  Loss: 3.577 (3.59)  Time: 0.782s, 1309.70/s  (0.806s, 1270.43/s)  LR: 8.043e-04  Data: 0.010 (0.015)
Train: 176 [ 650/1251 ( 52%)]  Loss: 3.258 (3.57)  Time: 0.854s, 1199.27/s  (0.806s, 1270.47/s)  LR: 8.043e-04  Data: 0.010 (0.014)
Train: 176 [ 700/1251 ( 56%)]  Loss: 3.696 (3.58)  Time: 0.774s, 1323.70/s  (0.806s, 1270.41/s)  LR: 8.043e-04  Data: 0.010 (0.014)
Train: 176 [ 750/1251 ( 60%)]  Loss: 3.357 (3.56)  Time: 0.781s, 1311.85/s  (0.806s, 1270.22/s)  LR: 8.043e-04  Data: 0.014 (0.014)
Train: 176 [ 800/1251 ( 64%)]  Loss: 3.564 (3.56)  Time: 0.784s, 1305.94/s  (0.806s, 1270.76/s)  LR: 8.043e-04  Data: 0.013 (0.014)
Train: 176 [ 850/1251 ( 68%)]  Loss: 3.763 (3.57)  Time: 0.775s, 1321.40/s  (0.806s, 1271.14/s)  LR: 8.043e-04  Data: 0.010 (0.014)
Train: 176 [ 900/1251 ( 72%)]  Loss: 3.807 (3.59)  Time: 0.800s, 1280.10/s  (0.805s, 1272.39/s)  LR: 8.043e-04  Data: 0.009 (0.013)
Train: 176 [ 950/1251 ( 76%)]  Loss: 3.756 (3.60)  Time: 0.812s, 1261.43/s  (0.804s, 1273.06/s)  LR: 8.043e-04  Data: 0.010 (0.013)
Train: 176 [1000/1251 ( 80%)]  Loss: 3.546 (3.59)  Time: 0.773s, 1325.28/s  (0.804s, 1273.25/s)  LR: 8.043e-04  Data: 0.010 (0.013)
Train: 176 [1050/1251 ( 84%)]  Loss: 3.737 (3.60)  Time: 0.794s, 1288.97/s  (0.804s, 1273.41/s)  LR: 8.043e-04  Data: 0.010 (0.013)
Train: 176 [1100/1251 ( 88%)]  Loss: 3.807 (3.61)  Time: 0.792s, 1293.71/s  (0.804s, 1273.45/s)  LR: 8.043e-04  Data: 0.013 (0.013)
Train: 176 [1150/1251 ( 92%)]  Loss: 3.483 (3.60)  Time: 0.782s, 1308.65/s  (0.804s, 1273.77/s)  LR: 8.043e-04  Data: 0.010 (0.013)
Train: 176 [1200/1251 ( 96%)]  Loss: 3.694 (3.61)  Time: 0.771s, 1328.74/s  (0.804s, 1274.02/s)  LR: 8.043e-04  Data: 0.010 (0.013)
Train: 176 [1250/1251 (100%)]  Loss: 3.498 (3.60)  Time: 0.770s, 1329.93/s  (0.804s, 1273.56/s)  LR: 8.043e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.643 (1.643)  Loss:  0.7705 (0.7705)  Acc@1: 88.3789 (88.3789)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.193 (0.600)  Loss:  0.8843 (1.3719)  Acc@1: 83.8443 (73.4040)  Acc@5: 95.7547 (91.9100)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-176.pth.tar', 73.40400009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-171.pth.tar', 73.36800006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-172.pth.tar', 73.2159999633789)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-163.pth.tar', 73.14800001464843)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-160.pth.tar', 73.08200001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-155.pth.tar', 73.08000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-168.pth.tar', 73.05599999023437)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-173.pth.tar', 73.04800011474609)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-165.pth.tar', 73.0440000415039)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-174.pth.tar', 73.03600001220703)

Train: 177 [   0/1251 (  0%)]  Loss: 3.743 (3.74)  Time: 2.520s,  406.38/s  (2.520s,  406.38/s)  LR: 8.022e-04  Data: 1.750 (1.750)
Train: 177 [  50/1251 (  4%)]  Loss: 3.858 (3.80)  Time: 0.849s, 1206.36/s  (0.851s, 1202.86/s)  LR: 8.022e-04  Data: 0.010 (0.056)
Train: 177 [ 100/1251 (  8%)]  Loss: 3.931 (3.84)  Time: 0.783s, 1307.78/s  (0.827s, 1237.53/s)  LR: 8.022e-04  Data: 0.010 (0.034)
Train: 177 [ 150/1251 ( 12%)]  Loss: 3.514 (3.76)  Time: 0.823s, 1243.59/s  (0.819s, 1249.75/s)  LR: 8.022e-04  Data: 0.009 (0.026)
Train: 177 [ 200/1251 ( 16%)]  Loss: 4.014 (3.81)  Time: 0.772s, 1327.21/s  (0.817s, 1252.90/s)  LR: 8.022e-04  Data: 0.011 (0.023)
Train: 177 [ 250/1251 ( 20%)]  Loss: 3.341 (3.73)  Time: 0.812s, 1261.73/s  (0.815s, 1256.99/s)  LR: 8.022e-04  Data: 0.010 (0.020)
Train: 177 [ 300/1251 ( 24%)]  Loss: 3.681 (3.73)  Time: 0.807s, 1268.67/s  (0.811s, 1261.92/s)  LR: 8.022e-04  Data: 0.010 (0.019)
Train: 177 [ 350/1251 ( 28%)]  Loss: 3.808 (3.74)  Time: 0.785s, 1303.97/s  (0.811s, 1263.05/s)  LR: 8.022e-04  Data: 0.017 (0.018)
Train: 177 [ 400/1251 ( 32%)]  Loss: 3.756 (3.74)  Time: 0.826s, 1239.90/s  (0.810s, 1264.36/s)  LR: 8.022e-04  Data: 0.010 (0.017)
Train: 177 [ 450/1251 ( 36%)]  Loss: 3.885 (3.75)  Time: 0.809s, 1265.96/s  (0.809s, 1265.28/s)  LR: 8.022e-04  Data: 0.010 (0.016)
Train: 177 [ 500/1251 ( 40%)]  Loss: 3.827 (3.76)  Time: 0.771s, 1327.86/s  (0.808s, 1267.42/s)  LR: 8.022e-04  Data: 0.011 (0.016)
Train: 177 [ 550/1251 ( 44%)]  Loss: 3.918 (3.77)  Time: 0.785s, 1305.20/s  (0.807s, 1268.46/s)  LR: 8.022e-04  Data: 0.014 (0.015)
Train: 177 [ 600/1251 ( 48%)]  Loss: 3.654 (3.76)  Time: 0.816s, 1254.24/s  (0.806s, 1269.79/s)  LR: 8.022e-04  Data: 0.010 (0.015)
Train: 177 [ 650/1251 ( 52%)]  Loss: 3.481 (3.74)  Time: 0.772s, 1325.77/s  (0.806s, 1270.56/s)  LR: 8.022e-04  Data: 0.010 (0.015)
Train: 177 [ 700/1251 ( 56%)]  Loss: 3.923 (3.76)  Time: 0.814s, 1258.59/s  (0.806s, 1271.15/s)  LR: 8.022e-04  Data: 0.010 (0.014)
Train: 177 [ 750/1251 ( 60%)]  Loss: 3.391 (3.73)  Time: 0.783s, 1307.32/s  (0.805s, 1272.05/s)  LR: 8.022e-04  Data: 0.012 (0.014)
Train: 177 [ 800/1251 ( 64%)]  Loss: 3.806 (3.74)  Time: 0.773s, 1323.85/s  (0.805s, 1272.33/s)  LR: 8.022e-04  Data: 0.010 (0.014)
Train: 177 [ 850/1251 ( 68%)]  Loss: 3.419 (3.72)  Time: 0.807s, 1268.55/s  (0.805s, 1272.29/s)  LR: 8.022e-04  Data: 0.010 (0.014)
Train: 177 [ 900/1251 ( 72%)]  Loss: 3.651 (3.72)  Time: 0.812s, 1260.47/s  (0.805s, 1272.48/s)  LR: 8.022e-04  Data: 0.015 (0.014)
Train: 177 [ 950/1251 ( 76%)]  Loss: 3.190 (3.69)  Time: 0.775s, 1321.03/s  (0.805s, 1272.09/s)  LR: 8.022e-04  Data: 0.010 (0.013)
Train: 177 [1000/1251 ( 80%)]  Loss: 3.787 (3.69)  Time: 0.841s, 1218.25/s  (0.805s, 1272.27/s)  LR: 8.022e-04  Data: 0.010 (0.013)
Train: 177 [1050/1251 ( 84%)]  Loss: 3.644 (3.69)  Time: 0.823s, 1244.15/s  (0.805s, 1272.25/s)  LR: 8.022e-04  Data: 0.010 (0.013)
Train: 177 [1100/1251 ( 88%)]  Loss: 3.387 (3.68)  Time: 0.836s, 1225.01/s  (0.805s, 1272.60/s)  LR: 8.022e-04  Data: 0.013 (0.013)
Train: 177 [1150/1251 ( 92%)]  Loss: 4.027 (3.69)  Time: 0.771s, 1327.72/s  (0.804s, 1272.99/s)  LR: 8.022e-04  Data: 0.010 (0.013)
Train: 177 [1200/1251 ( 96%)]  Loss: 3.579 (3.69)  Time: 0.801s, 1277.84/s  (0.804s, 1273.06/s)  LR: 8.022e-04  Data: 0.010 (0.013)
Train: 177 [1250/1251 (100%)]  Loss: 3.633 (3.69)  Time: 0.794s, 1289.46/s  (0.804s, 1273.47/s)  LR: 8.022e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.628 (1.628)  Loss:  0.8115 (0.8115)  Acc@1: 88.6719 (88.6719)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.194 (0.603)  Loss:  0.9136 (1.4006)  Acc@1: 83.9623 (72.9140)  Acc@5: 96.4623 (91.7760)
Train: 178 [   0/1251 (  0%)]  Loss: 3.840 (3.84)  Time: 2.492s,  410.91/s  (2.492s,  410.91/s)  LR: 8.001e-04  Data: 1.764 (1.764)
Train: 178 [  50/1251 (  4%)]  Loss: 3.687 (3.76)  Time: 0.792s, 1293.20/s  (0.834s, 1228.39/s)  LR: 8.001e-04  Data: 0.015 (0.045)
Train: 178 [ 100/1251 (  8%)]  Loss: 3.739 (3.76)  Time: 0.844s, 1213.46/s  (0.814s, 1257.38/s)  LR: 8.001e-04  Data: 0.010 (0.028)
Train: 178 [ 150/1251 ( 12%)]  Loss: 3.499 (3.69)  Time: 0.799s, 1281.75/s  (0.810s, 1264.79/s)  LR: 8.001e-04  Data: 0.010 (0.023)
Train: 178 [ 200/1251 ( 16%)]  Loss: 3.811 (3.72)  Time: 0.827s, 1237.85/s  (0.809s, 1266.24/s)  LR: 8.001e-04  Data: 0.010 (0.020)
Train: 178 [ 250/1251 ( 20%)]  Loss: 3.543 (3.69)  Time: 0.775s, 1320.88/s  (0.807s, 1268.61/s)  LR: 8.001e-04  Data: 0.010 (0.018)
Train: 178 [ 300/1251 ( 24%)]  Loss: 3.820 (3.71)  Time: 0.798s, 1283.62/s  (0.806s, 1270.37/s)  LR: 8.001e-04  Data: 0.010 (0.017)
Train: 178 [ 350/1251 ( 28%)]  Loss: 3.844 (3.72)  Time: 0.816s, 1254.49/s  (0.805s, 1271.81/s)  LR: 8.001e-04  Data: 0.010 (0.016)
Train: 178 [ 400/1251 ( 32%)]  Loss: 3.192 (3.66)  Time: 0.828s, 1237.08/s  (0.807s, 1269.64/s)  LR: 8.001e-04  Data: 0.010 (0.015)
Train: 178 [ 450/1251 ( 36%)]  Loss: 3.674 (3.66)  Time: 0.779s, 1314.88/s  (0.807s, 1268.22/s)  LR: 8.001e-04  Data: 0.011 (0.015)
Train: 178 [ 500/1251 ( 40%)]  Loss: 3.730 (3.67)  Time: 0.774s, 1323.26/s  (0.804s, 1272.88/s)  LR: 8.001e-04  Data: 0.010 (0.014)
Train: 178 [ 550/1251 ( 44%)]  Loss: 3.883 (3.69)  Time: 0.814s, 1258.31/s  (0.803s, 1275.80/s)  LR: 8.001e-04  Data: 0.010 (0.014)
Train: 178 [ 600/1251 ( 48%)]  Loss: 3.186 (3.65)  Time: 0.813s, 1259.01/s  (0.802s, 1277.13/s)  LR: 8.001e-04  Data: 0.010 (0.014)
Train: 178 [ 650/1251 ( 52%)]  Loss: 3.426 (3.63)  Time: 0.859s, 1192.34/s  (0.803s, 1275.62/s)  LR: 8.001e-04  Data: 0.009 (0.014)
Train: 178 [ 700/1251 ( 56%)]  Loss: 3.658 (3.64)  Time: 0.807s, 1269.50/s  (0.803s, 1275.87/s)  LR: 8.001e-04  Data: 0.009 (0.013)
Train: 178 [ 750/1251 ( 60%)]  Loss: 3.504 (3.63)  Time: 0.810s, 1263.67/s  (0.803s, 1275.03/s)  LR: 8.001e-04  Data: 0.015 (0.013)
Train: 178 [ 800/1251 ( 64%)]  Loss: 3.739 (3.63)  Time: 0.828s, 1236.32/s  (0.803s, 1275.59/s)  LR: 8.001e-04  Data: 0.009 (0.013)
Train: 178 [ 850/1251 ( 68%)]  Loss: 3.538 (3.63)  Time: 0.875s, 1170.77/s  (0.803s, 1275.09/s)  LR: 8.001e-04  Data: 0.010 (0.013)
Train: 178 [ 900/1251 ( 72%)]  Loss: 3.389 (3.62)  Time: 0.796s, 1286.74/s  (0.803s, 1275.07/s)  LR: 8.001e-04  Data: 0.009 (0.013)
Train: 178 [ 950/1251 ( 76%)]  Loss: 3.188 (3.59)  Time: 0.828s, 1236.21/s  (0.803s, 1275.07/s)  LR: 8.001e-04  Data: 0.013 (0.013)
Train: 178 [1000/1251 ( 80%)]  Loss: 3.768 (3.60)  Time: 0.786s, 1302.55/s  (0.803s, 1274.74/s)  LR: 8.001e-04  Data: 0.009 (0.013)
Train: 178 [1050/1251 ( 84%)]  Loss: 3.532 (3.60)  Time: 0.773s, 1324.47/s  (0.803s, 1275.37/s)  LR: 8.001e-04  Data: 0.011 (0.013)
Train: 178 [1100/1251 ( 88%)]  Loss: 3.913 (3.61)  Time: 0.815s, 1256.97/s  (0.803s, 1275.16/s)  LR: 8.001e-04  Data: 0.010 (0.012)
Train: 178 [1150/1251 ( 92%)]  Loss: 3.632 (3.61)  Time: 0.796s, 1286.68/s  (0.803s, 1275.33/s)  LR: 8.001e-04  Data: 0.010 (0.012)
Train: 178 [1200/1251 ( 96%)]  Loss: 3.560 (3.61)  Time: 0.772s, 1325.73/s  (0.803s, 1275.53/s)  LR: 8.001e-04  Data: 0.010 (0.012)
Train: 178 [1250/1251 (100%)]  Loss: 3.858 (3.62)  Time: 0.764s, 1340.89/s  (0.803s, 1275.57/s)  LR: 8.001e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.588 (1.588)  Loss:  0.9004 (0.9004)  Acc@1: 88.0859 (88.0859)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.194 (0.590)  Loss:  0.8740 (1.4216)  Acc@1: 84.6698 (73.2400)  Acc@5: 96.3443 (91.6600)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-176.pth.tar', 73.40400009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-171.pth.tar', 73.36800006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-178.pth.tar', 73.24000006347656)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-172.pth.tar', 73.2159999633789)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-163.pth.tar', 73.14800001464843)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-160.pth.tar', 73.08200001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-155.pth.tar', 73.08000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-168.pth.tar', 73.05599999023437)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-173.pth.tar', 73.04800011474609)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-165.pth.tar', 73.0440000415039)

Train: 179 [   0/1251 (  0%)]  Loss: 3.601 (3.60)  Time: 2.270s,  451.20/s  (2.270s,  451.20/s)  LR: 7.980e-04  Data: 1.534 (1.534)
Train: 179 [  50/1251 (  4%)]  Loss: 3.521 (3.56)  Time: 0.802s, 1276.43/s  (0.838s, 1221.46/s)  LR: 7.980e-04  Data: 0.010 (0.046)
Train: 179 [ 100/1251 (  8%)]  Loss: 3.612 (3.58)  Time: 0.801s, 1277.99/s  (0.820s, 1248.76/s)  LR: 7.980e-04  Data: 0.014 (0.029)
Train: 179 [ 150/1251 ( 12%)]  Loss: 3.278 (3.50)  Time: 0.773s, 1324.54/s  (0.815s, 1256.19/s)  LR: 7.980e-04  Data: 0.011 (0.023)
Train: 179 [ 200/1251 ( 16%)]  Loss: 3.664 (3.54)  Time: 0.825s, 1240.81/s  (0.812s, 1260.38/s)  LR: 7.980e-04  Data: 0.010 (0.020)
Train: 179 [ 250/1251 ( 20%)]  Loss: 3.448 (3.52)  Time: 0.805s, 1271.35/s  (0.810s, 1263.50/s)  LR: 7.980e-04  Data: 0.009 (0.018)
Train: 179 [ 300/1251 ( 24%)]  Loss: 3.949 (3.58)  Time: 0.858s, 1193.06/s  (0.809s, 1266.04/s)  LR: 7.980e-04  Data: 0.010 (0.017)
Train: 179 [ 350/1251 ( 28%)]  Loss: 3.670 (3.59)  Time: 0.802s, 1277.30/s  (0.808s, 1266.72/s)  LR: 7.980e-04  Data: 0.010 (0.016)
Train: 179 [ 400/1251 ( 32%)]  Loss: 3.605 (3.59)  Time: 0.865s, 1184.24/s  (0.807s, 1268.41/s)  LR: 7.980e-04  Data: 0.010 (0.016)
Train: 179 [ 450/1251 ( 36%)]  Loss: 3.633 (3.60)  Time: 0.816s, 1255.00/s  (0.806s, 1270.10/s)  LR: 7.980e-04  Data: 0.010 (0.015)
Train: 179 [ 500/1251 ( 40%)]  Loss: 3.574 (3.60)  Time: 0.774s, 1323.10/s  (0.806s, 1270.90/s)  LR: 7.980e-04  Data: 0.010 (0.015)
Train: 179 [ 550/1251 ( 44%)]  Loss: 3.710 (3.61)  Time: 0.845s, 1212.40/s  (0.805s, 1271.78/s)  LR: 7.980e-04  Data: 0.014 (0.014)
Train: 179 [ 600/1251 ( 48%)]  Loss: 3.556 (3.60)  Time: 0.772s, 1326.09/s  (0.806s, 1270.77/s)  LR: 7.980e-04  Data: 0.010 (0.014)
Train: 179 [ 650/1251 ( 52%)]  Loss: 3.425 (3.59)  Time: 0.850s, 1204.07/s  (0.805s, 1271.45/s)  LR: 7.980e-04  Data: 0.014 (0.014)
Train: 179 [ 700/1251 ( 56%)]  Loss: 3.750 (3.60)  Time: 0.795s, 1287.70/s  (0.805s, 1271.46/s)  LR: 7.980e-04  Data: 0.010 (0.014)
Train: 179 [ 750/1251 ( 60%)]  Loss: 3.678 (3.60)  Time: 0.785s, 1305.27/s  (0.805s, 1271.66/s)  LR: 7.980e-04  Data: 0.013 (0.013)
Train: 179 [ 800/1251 ( 64%)]  Loss: 3.635 (3.61)  Time: 0.827s, 1238.82/s  (0.805s, 1271.76/s)  LR: 7.980e-04  Data: 0.014 (0.013)
Train: 179 [ 850/1251 ( 68%)]  Loss: 3.708 (3.61)  Time: 0.782s, 1308.69/s  (0.805s, 1271.87/s)  LR: 7.980e-04  Data: 0.010 (0.013)
Train: 179 [ 900/1251 ( 72%)]  Loss: 3.876 (3.63)  Time: 0.826s, 1240.40/s  (0.805s, 1272.10/s)  LR: 7.980e-04  Data: 0.010 (0.013)
Train: 179 [ 950/1251 ( 76%)]  Loss: 3.860 (3.64)  Time: 0.819s, 1250.34/s  (0.805s, 1272.28/s)  LR: 7.980e-04  Data: 0.010 (0.013)
Train: 179 [1000/1251 ( 80%)]  Loss: 3.557 (3.63)  Time: 0.775s, 1320.62/s  (0.804s, 1272.92/s)  LR: 7.980e-04  Data: 0.010 (0.013)
Train: 179 [1050/1251 ( 84%)]  Loss: 3.436 (3.62)  Time: 0.820s, 1248.35/s  (0.804s, 1273.39/s)  LR: 7.980e-04  Data: 0.011 (0.013)
Train: 179 [1100/1251 ( 88%)]  Loss: 3.658 (3.63)  Time: 0.840s, 1219.53/s  (0.804s, 1273.94/s)  LR: 7.980e-04  Data: 0.010 (0.013)
Train: 179 [1150/1251 ( 92%)]  Loss: 3.750 (3.63)  Time: 0.782s, 1309.14/s  (0.804s, 1274.15/s)  LR: 7.980e-04  Data: 0.010 (0.013)
Train: 179 [1200/1251 ( 96%)]  Loss: 3.356 (3.62)  Time: 0.849s, 1206.56/s  (0.804s, 1274.36/s)  LR: 7.980e-04  Data: 0.011 (0.013)
Train: 179 [1250/1251 (100%)]  Loss: 3.790 (3.63)  Time: 0.773s, 1325.14/s  (0.804s, 1274.40/s)  LR: 7.980e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.582 (1.582)  Loss:  0.8540 (0.8540)  Acc@1: 88.0859 (88.0859)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.194 (0.599)  Loss:  0.8701 (1.3653)  Acc@1: 84.6698 (73.1900)  Acc@5: 95.8726 (91.8140)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-176.pth.tar', 73.40400009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-171.pth.tar', 73.36800006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-178.pth.tar', 73.24000006347656)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-172.pth.tar', 73.2159999633789)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-179.pth.tar', 73.18999993408202)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-163.pth.tar', 73.14800001464843)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-160.pth.tar', 73.08200001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-155.pth.tar', 73.08000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-168.pth.tar', 73.05599999023437)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-173.pth.tar', 73.04800011474609)

Train: 180 [   0/1251 (  0%)]  Loss: 3.494 (3.49)  Time: 2.390s,  428.42/s  (2.390s,  428.42/s)  LR: 7.960e-04  Data: 1.600 (1.600)
Train: 180 [  50/1251 (  4%)]  Loss: 3.681 (3.59)  Time: 0.772s, 1325.95/s  (0.844s, 1213.21/s)  LR: 7.960e-04  Data: 0.009 (0.048)
Train: 180 [ 100/1251 (  8%)]  Loss: 3.870 (3.68)  Time: 0.800s, 1279.43/s  (0.825s, 1241.62/s)  LR: 7.960e-04  Data: 0.011 (0.030)
Train: 180 [ 150/1251 ( 12%)]  Loss: 3.494 (3.63)  Time: 0.804s, 1273.59/s  (0.817s, 1253.71/s)  LR: 7.960e-04  Data: 0.013 (0.023)
Train: 180 [ 200/1251 ( 16%)]  Loss: 3.880 (3.68)  Time: 0.772s, 1325.59/s  (0.811s, 1262.49/s)  LR: 7.960e-04  Data: 0.010 (0.020)
Train: 180 [ 250/1251 ( 20%)]  Loss: 3.779 (3.70)  Time: 0.845s, 1211.63/s  (0.807s, 1268.94/s)  LR: 7.960e-04  Data: 0.010 (0.018)
Train: 180 [ 300/1251 ( 24%)]  Loss: 3.713 (3.70)  Time: 0.793s, 1291.94/s  (0.806s, 1270.44/s)  LR: 7.960e-04  Data: 0.010 (0.017)
Train: 180 [ 350/1251 ( 28%)]  Loss: 3.363 (3.66)  Time: 0.784s, 1305.58/s  (0.806s, 1270.75/s)  LR: 7.960e-04  Data: 0.013 (0.016)
Train: 180 [ 400/1251 ( 32%)]  Loss: 3.806 (3.68)  Time: 0.773s, 1324.74/s  (0.805s, 1272.07/s)  LR: 7.960e-04  Data: 0.010 (0.015)
Train: 180 [ 450/1251 ( 36%)]  Loss: 3.859 (3.69)  Time: 0.793s, 1291.08/s  (0.804s, 1273.17/s)  LR: 7.960e-04  Data: 0.010 (0.015)
Train: 180 [ 500/1251 ( 40%)]  Loss: 3.533 (3.68)  Time: 0.785s, 1303.65/s  (0.804s, 1273.84/s)  LR: 7.960e-04  Data: 0.010 (0.015)
Train: 180 [ 550/1251 ( 44%)]  Loss: 3.537 (3.67)  Time: 0.829s, 1234.91/s  (0.804s, 1273.61/s)  LR: 7.960e-04  Data: 0.018 (0.014)
Train: 180 [ 600/1251 ( 48%)]  Loss: 3.755 (3.67)  Time: 0.797s, 1284.05/s  (0.804s, 1273.50/s)  LR: 7.960e-04  Data: 0.010 (0.014)
Train: 180 [ 650/1251 ( 52%)]  Loss: 3.805 (3.68)  Time: 0.783s, 1308.02/s  (0.804s, 1273.79/s)  LR: 7.960e-04  Data: 0.010 (0.014)
Train: 180 [ 700/1251 ( 56%)]  Loss: 3.581 (3.68)  Time: 0.810s, 1263.65/s  (0.803s, 1274.75/s)  LR: 7.960e-04  Data: 0.012 (0.014)
Train: 180 [ 750/1251 ( 60%)]  Loss: 3.163 (3.64)  Time: 0.845s, 1211.65/s  (0.803s, 1275.04/s)  LR: 7.960e-04  Data: 0.010 (0.014)
Train: 180 [ 800/1251 ( 64%)]  Loss: 3.584 (3.64)  Time: 0.773s, 1325.43/s  (0.803s, 1275.32/s)  LR: 7.960e-04  Data: 0.010 (0.013)
Train: 180 [ 850/1251 ( 68%)]  Loss: 3.301 (3.62)  Time: 0.800s, 1279.54/s  (0.803s, 1275.40/s)  LR: 7.960e-04  Data: 0.011 (0.013)
Train: 180 [ 900/1251 ( 72%)]  Loss: 3.774 (3.63)  Time: 0.779s, 1314.09/s  (0.802s, 1276.18/s)  LR: 7.960e-04  Data: 0.010 (0.013)
Train: 180 [ 950/1251 ( 76%)]  Loss: 3.740 (3.64)  Time: 0.774s, 1323.72/s  (0.802s, 1276.27/s)  LR: 7.960e-04  Data: 0.011 (0.013)
Train: 180 [1000/1251 ( 80%)]  Loss: 3.426 (3.63)  Time: 0.774s, 1323.34/s  (0.803s, 1275.94/s)  LR: 7.960e-04  Data: 0.010 (0.013)
Train: 180 [1050/1251 ( 84%)]  Loss: 3.922 (3.64)  Time: 0.802s, 1277.32/s  (0.802s, 1276.08/s)  LR: 7.960e-04  Data: 0.010 (0.013)
Train: 180 [1100/1251 ( 88%)]  Loss: 3.821 (3.65)  Time: 0.801s, 1278.86/s  (0.802s, 1276.26/s)  LR: 7.960e-04  Data: 0.011 (0.013)
Train: 180 [1150/1251 ( 92%)]  Loss: 3.842 (3.66)  Time: 0.780s, 1312.52/s  (0.802s, 1276.64/s)  LR: 7.960e-04  Data: 0.010 (0.013)
Train: 180 [1200/1251 ( 96%)]  Loss: 3.789 (3.66)  Time: 0.778s, 1315.74/s  (0.802s, 1276.66/s)  LR: 7.960e-04  Data: 0.011 (0.013)
Train: 180 [1250/1251 (100%)]  Loss: 3.414 (3.65)  Time: 0.759s, 1349.44/s  (0.802s, 1276.83/s)  LR: 7.960e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.649 (1.649)  Loss:  0.8809 (0.8809)  Acc@1: 88.9648 (88.9648)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.194 (0.600)  Loss:  0.9585 (1.3893)  Acc@1: 84.5519 (73.5580)  Acc@5: 95.6368 (91.8720)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-180.pth.tar', 73.55800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-176.pth.tar', 73.40400009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-171.pth.tar', 73.36800006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-178.pth.tar', 73.24000006347656)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-172.pth.tar', 73.2159999633789)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-179.pth.tar', 73.18999993408202)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-163.pth.tar', 73.14800001464843)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-160.pth.tar', 73.08200001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-155.pth.tar', 73.08000007080078)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-168.pth.tar', 73.05599999023437)

Train: 181 [   0/1251 (  0%)]  Loss: 3.608 (3.61)  Time: 2.425s,  422.28/s  (2.425s,  422.28/s)  LR: 7.939e-04  Data: 1.699 (1.699)
Train: 181 [  50/1251 (  4%)]  Loss: 3.715 (3.66)  Time: 0.781s, 1311.81/s  (0.830s, 1234.21/s)  LR: 7.939e-04  Data: 0.010 (0.047)
Train: 181 [ 100/1251 (  8%)]  Loss: 3.549 (3.62)  Time: 0.776s, 1319.88/s  (0.817s, 1252.96/s)  LR: 7.939e-04  Data: 0.010 (0.029)
Train: 181 [ 150/1251 ( 12%)]  Loss: 3.889 (3.69)  Time: 0.830s, 1234.38/s  (0.811s, 1262.12/s)  LR: 7.939e-04  Data: 0.010 (0.023)
Train: 181 [ 200/1251 ( 16%)]  Loss: 3.592 (3.67)  Time: 0.785s, 1304.81/s  (0.811s, 1263.20/s)  LR: 7.939e-04  Data: 0.018 (0.020)
Train: 181 [ 250/1251 ( 20%)]  Loss: 3.617 (3.66)  Time: 0.792s, 1293.29/s  (0.809s, 1265.54/s)  LR: 7.939e-04  Data: 0.009 (0.019)
Train: 181 [ 300/1251 ( 24%)]  Loss: 3.632 (3.66)  Time: 0.786s, 1302.20/s  (0.808s, 1266.81/s)  LR: 7.939e-04  Data: 0.010 (0.017)
Train: 181 [ 350/1251 ( 28%)]  Loss: 3.713 (3.66)  Time: 0.782s, 1310.29/s  (0.807s, 1268.57/s)  LR: 7.939e-04  Data: 0.011 (0.016)
Train: 181 [ 400/1251 ( 32%)]  Loss: 3.741 (3.67)  Time: 0.822s, 1246.47/s  (0.807s, 1269.01/s)  LR: 7.939e-04  Data: 0.010 (0.016)
Train: 181 [ 450/1251 ( 36%)]  Loss: 3.506 (3.66)  Time: 0.801s, 1277.96/s  (0.806s, 1269.82/s)  LR: 7.939e-04  Data: 0.017 (0.015)
Train: 181 [ 500/1251 ( 40%)]  Loss: 3.888 (3.68)  Time: 0.804s, 1272.86/s  (0.807s, 1269.61/s)  LR: 7.939e-04  Data: 0.010 (0.015)
Train: 181 [ 550/1251 ( 44%)]  Loss: 3.725 (3.68)  Time: 0.807s, 1268.74/s  (0.806s, 1270.20/s)  LR: 7.939e-04  Data: 0.010 (0.014)
Train: 181 [ 600/1251 ( 48%)]  Loss: 3.411 (3.66)  Time: 0.785s, 1305.23/s  (0.805s, 1271.66/s)  LR: 7.939e-04  Data: 0.009 (0.014)
Train: 181 [ 650/1251 ( 52%)]  Loss: 3.834 (3.67)  Time: 0.811s, 1263.04/s  (0.805s, 1272.08/s)  LR: 7.939e-04  Data: 0.010 (0.014)
Train: 181 [ 700/1251 ( 56%)]  Loss: 3.490 (3.66)  Time: 0.783s, 1308.60/s  (0.804s, 1272.96/s)  LR: 7.939e-04  Data: 0.013 (0.014)
Train: 181 [ 750/1251 ( 60%)]  Loss: 3.461 (3.65)  Time: 0.773s, 1325.15/s  (0.804s, 1273.87/s)  LR: 7.939e-04  Data: 0.011 (0.014)
Train: 181 [ 800/1251 ( 64%)]  Loss: 4.010 (3.67)  Time: 0.845s, 1211.44/s  (0.804s, 1272.87/s)  LR: 7.939e-04  Data: 0.010 (0.013)
Train: 181 [ 850/1251 ( 68%)]  Loss: 3.691 (3.67)  Time: 0.777s, 1318.23/s  (0.804s, 1273.16/s)  LR: 7.939e-04  Data: 0.011 (0.013)
Train: 181 [ 900/1251 ( 72%)]  Loss: 3.576 (3.67)  Time: 0.841s, 1216.99/s  (0.804s, 1273.70/s)  LR: 7.939e-04  Data: 0.016 (0.013)
Train: 181 [ 950/1251 ( 76%)]  Loss: 3.570 (3.66)  Time: 0.789s, 1297.98/s  (0.804s, 1274.03/s)  LR: 7.939e-04  Data: 0.014 (0.013)
Train: 181 [1000/1251 ( 80%)]  Loss: 3.588 (3.66)  Time: 0.869s, 1178.23/s  (0.804s, 1274.34/s)  LR: 7.939e-04  Data: 0.010 (0.013)
Train: 181 [1050/1251 ( 84%)]  Loss: 3.510 (3.65)  Time: 0.783s, 1308.56/s  (0.803s, 1274.48/s)  LR: 7.939e-04  Data: 0.010 (0.013)
Train: 181 [1100/1251 ( 88%)]  Loss: 3.635 (3.65)  Time: 0.809s, 1265.27/s  (0.803s, 1274.83/s)  LR: 7.939e-04  Data: 0.010 (0.013)
Train: 181 [1150/1251 ( 92%)]  Loss: 3.954 (3.66)  Time: 0.772s, 1326.56/s  (0.803s, 1274.57/s)  LR: 7.939e-04  Data: 0.010 (0.013)
Train: 181 [1200/1251 ( 96%)]  Loss: 3.360 (3.65)  Time: 0.772s, 1326.29/s  (0.803s, 1274.73/s)  LR: 7.939e-04  Data: 0.010 (0.013)
Train: 181 [1250/1251 (100%)]  Loss: 3.575 (3.65)  Time: 0.759s, 1350.00/s  (0.803s, 1274.56/s)  LR: 7.939e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.696 (1.696)  Loss:  0.8584 (0.8584)  Acc@1: 87.9883 (87.9883)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.194 (0.601)  Loss:  0.8711 (1.3617)  Acc@1: 83.8443 (73.4300)  Acc@5: 96.8160 (91.9120)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-180.pth.tar', 73.55800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-181.pth.tar', 73.43000009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-176.pth.tar', 73.40400009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-171.pth.tar', 73.36800006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-178.pth.tar', 73.24000006347656)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-172.pth.tar', 73.2159999633789)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-179.pth.tar', 73.18999993408202)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-163.pth.tar', 73.14800001464843)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-160.pth.tar', 73.08200001953125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-155.pth.tar', 73.08000007080078)

Train: 182 [   0/1251 (  0%)]  Loss: 3.218 (3.22)  Time: 2.392s,  428.12/s  (2.392s,  428.12/s)  LR: 7.917e-04  Data: 1.663 (1.663)
Train: 182 [  50/1251 (  4%)]  Loss: 3.911 (3.56)  Time: 0.811s, 1262.72/s  (0.837s, 1223.63/s)  LR: 7.917e-04  Data: 0.009 (0.049)
Train: 182 [ 100/1251 (  8%)]  Loss: 3.383 (3.50)  Time: 0.779s, 1315.20/s  (0.820s, 1248.37/s)  LR: 7.917e-04  Data: 0.010 (0.030)
Train: 182 [ 150/1251 ( 12%)]  Loss: 3.594 (3.53)  Time: 0.774s, 1323.33/s  (0.812s, 1260.46/s)  LR: 7.917e-04  Data: 0.010 (0.024)
Train: 182 [ 200/1251 ( 16%)]  Loss: 3.520 (3.53)  Time: 0.811s, 1262.37/s  (0.809s, 1265.61/s)  LR: 7.917e-04  Data: 0.014 (0.021)
Train: 182 [ 250/1251 ( 20%)]  Loss: 4.008 (3.61)  Time: 0.815s, 1256.97/s  (0.809s, 1265.44/s)  LR: 7.917e-04  Data: 0.013 (0.019)
Train: 182 [ 300/1251 ( 24%)]  Loss: 3.756 (3.63)  Time: 0.799s, 1282.33/s  (0.809s, 1265.77/s)  LR: 7.917e-04  Data: 0.009 (0.017)
Train: 182 [ 350/1251 ( 28%)]  Loss: 3.899 (3.66)  Time: 0.824s, 1243.11/s  (0.807s, 1268.29/s)  LR: 7.917e-04  Data: 0.010 (0.016)
Train: 182 [ 400/1251 ( 32%)]  Loss: 3.518 (3.65)  Time: 0.837s, 1223.88/s  (0.807s, 1269.19/s)  LR: 7.917e-04  Data: 0.010 (0.016)
Train: 182 [ 450/1251 ( 36%)]  Loss: 3.097 (3.59)  Time: 0.846s, 1209.76/s  (0.807s, 1269.59/s)  LR: 7.917e-04  Data: 0.011 (0.015)
Train: 182 [ 500/1251 ( 40%)]  Loss: 3.463 (3.58)  Time: 0.773s, 1324.77/s  (0.806s, 1271.02/s)  LR: 7.917e-04  Data: 0.010 (0.015)
Train: 182 [ 550/1251 ( 44%)]  Loss: 4.050 (3.62)  Time: 0.784s, 1306.29/s  (0.805s, 1271.49/s)  LR: 7.917e-04  Data: 0.010 (0.014)
Train: 182 [ 600/1251 ( 48%)]  Loss: 3.647 (3.62)  Time: 0.809s, 1266.50/s  (0.805s, 1271.93/s)  LR: 7.917e-04  Data: 0.011 (0.014)
Train: 182 [ 650/1251 ( 52%)]  Loss: 3.478 (3.61)  Time: 0.782s, 1309.59/s  (0.805s, 1272.12/s)  LR: 7.917e-04  Data: 0.010 (0.014)
Train: 182 [ 700/1251 ( 56%)]  Loss: 3.470 (3.60)  Time: 0.831s, 1232.67/s  (0.805s, 1271.59/s)  LR: 7.917e-04  Data: 0.015 (0.014)
Train: 182 [ 750/1251 ( 60%)]  Loss: 3.904 (3.62)  Time: 0.786s, 1303.48/s  (0.805s, 1272.04/s)  LR: 7.917e-04  Data: 0.010 (0.014)
Train: 182 [ 800/1251 ( 64%)]  Loss: 3.472 (3.61)  Time: 0.799s, 1282.26/s  (0.805s, 1272.57/s)  LR: 7.917e-04  Data: 0.013 (0.013)
Train: 182 [ 850/1251 ( 68%)]  Loss: 3.663 (3.61)  Time: 0.853s, 1200.36/s  (0.805s, 1272.57/s)  LR: 7.917e-04  Data: 0.010 (0.013)
Train: 182 [ 900/1251 ( 72%)]  Loss: 3.608 (3.61)  Time: 0.780s, 1313.57/s  (0.805s, 1272.82/s)  LR: 7.917e-04  Data: 0.010 (0.013)
Train: 182 [ 950/1251 ( 76%)]  Loss: 3.817 (3.62)  Time: 0.774s, 1323.62/s  (0.804s, 1273.09/s)  LR: 7.917e-04  Data: 0.010 (0.013)
Train: 182 [1000/1251 ( 80%)]  Loss: 3.859 (3.64)  Time: 0.873s, 1173.50/s  (0.804s, 1273.22/s)  LR: 7.917e-04  Data: 0.010 (0.013)
Train: 182 [1050/1251 ( 84%)]  Loss: 3.506 (3.63)  Time: 0.815s, 1255.81/s  (0.804s, 1273.99/s)  LR: 7.917e-04  Data: 0.010 (0.013)
Train: 182 [1100/1251 ( 88%)]  Loss: 3.511 (3.62)  Time: 0.786s, 1302.32/s  (0.804s, 1274.33/s)  LR: 7.917e-04  Data: 0.009 (0.013)
Train: 182 [1150/1251 ( 92%)]  Loss: 3.303 (3.61)  Time: 0.791s, 1295.01/s  (0.804s, 1274.28/s)  LR: 7.917e-04  Data: 0.010 (0.013)
Train: 182 [1200/1251 ( 96%)]  Loss: 3.504 (3.61)  Time: 0.798s, 1283.77/s  (0.803s, 1274.50/s)  LR: 7.917e-04  Data: 0.016 (0.013)
Train: 182 [1250/1251 (100%)]  Loss: 3.460 (3.60)  Time: 0.760s, 1347.91/s  (0.803s, 1274.95/s)  LR: 7.917e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.670 (1.670)  Loss:  0.8799 (0.8799)  Acc@1: 87.5000 (87.5000)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.194 (0.601)  Loss:  0.9229 (1.4193)  Acc@1: 83.4906 (72.6140)  Acc@5: 95.4009 (91.6360)
Train: 183 [   0/1251 (  0%)]  Loss: 3.560 (3.56)  Time: 3.125s,  327.72/s  (3.125s,  327.72/s)  LR: 7.896e-04  Data: 2.398 (2.398)
Train: 183 [  50/1251 (  4%)]  Loss: 3.444 (3.50)  Time: 0.823s, 1244.91/s  (0.846s, 1210.71/s)  LR: 7.896e-04  Data: 0.010 (0.062)
Train: 183 [ 100/1251 (  8%)]  Loss: 3.776 (3.59)  Time: 0.782s, 1310.25/s  (0.825s, 1241.03/s)  LR: 7.896e-04  Data: 0.010 (0.037)
Train: 183 [ 150/1251 ( 12%)]  Loss: 3.837 (3.65)  Time: 0.772s, 1326.18/s  (0.815s, 1256.45/s)  LR: 7.896e-04  Data: 0.009 (0.028)
Train: 183 [ 200/1251 ( 16%)]  Loss: 3.625 (3.65)  Time: 0.827s, 1238.68/s  (0.814s, 1258.67/s)  LR: 7.896e-04  Data: 0.014 (0.024)
Train: 183 [ 250/1251 ( 20%)]  Loss: 3.681 (3.65)  Time: 0.839s, 1219.81/s  (0.815s, 1256.02/s)  LR: 7.896e-04  Data: 0.010 (0.022)
Train: 183 [ 300/1251 ( 24%)]  Loss: 3.565 (3.64)  Time: 0.771s, 1327.53/s  (0.809s, 1265.65/s)  LR: 7.896e-04  Data: 0.010 (0.020)
Train: 183 [ 350/1251 ( 28%)]  Loss: 3.882 (3.67)  Time: 0.877s, 1167.72/s  (0.805s, 1271.87/s)  LR: 7.896e-04  Data: 0.019 (0.018)
Train: 183 [ 400/1251 ( 32%)]  Loss: 3.303 (3.63)  Time: 0.774s, 1322.57/s  (0.804s, 1273.52/s)  LR: 7.896e-04  Data: 0.009 (0.017)
Train: 183 [ 450/1251 ( 36%)]  Loss: 3.724 (3.64)  Time: 0.891s, 1149.52/s  (0.803s, 1274.87/s)  LR: 7.896e-04  Data: 0.011 (0.017)
Train: 183 [ 500/1251 ( 40%)]  Loss: 3.586 (3.63)  Time: 0.791s, 1294.15/s  (0.803s, 1275.78/s)  LR: 7.896e-04  Data: 0.011 (0.016)
Train: 183 [ 550/1251 ( 44%)]  Loss: 3.759 (3.65)  Time: 0.773s, 1324.53/s  (0.803s, 1274.86/s)  LR: 7.896e-04  Data: 0.011 (0.016)
Train: 183 [ 600/1251 ( 48%)]  Loss: 3.151 (3.61)  Time: 0.832s, 1230.18/s  (0.803s, 1275.25/s)  LR: 7.896e-04  Data: 0.010 (0.015)
Train: 183 [ 650/1251 ( 52%)]  Loss: 3.628 (3.61)  Time: 0.772s, 1326.60/s  (0.803s, 1275.83/s)  LR: 7.896e-04  Data: 0.009 (0.015)
Train: 183 [ 700/1251 ( 56%)]  Loss: 3.852 (3.62)  Time: 0.772s, 1326.66/s  (0.802s, 1276.55/s)  LR: 7.896e-04  Data: 0.010 (0.015)
Train: 183 [ 750/1251 ( 60%)]  Loss: 3.683 (3.63)  Time: 0.870s, 1176.34/s  (0.802s, 1276.11/s)  LR: 7.896e-04  Data: 0.009 (0.014)
Train: 183 [ 800/1251 ( 64%)]  Loss: 3.129 (3.60)  Time: 0.800s, 1280.55/s  (0.803s, 1275.38/s)  LR: 7.896e-04  Data: 0.011 (0.014)
Train: 183 [ 850/1251 ( 68%)]  Loss: 3.963 (3.62)  Time: 0.811s, 1262.57/s  (0.803s, 1275.58/s)  LR: 7.896e-04  Data: 0.009 (0.014)
Train: 183 [ 900/1251 ( 72%)]  Loss: 3.746 (3.63)  Time: 0.784s, 1305.36/s  (0.803s, 1275.52/s)  LR: 7.896e-04  Data: 0.010 (0.014)
Train: 183 [ 950/1251 ( 76%)]  Loss: 3.808 (3.63)  Time: 0.780s, 1312.15/s  (0.803s, 1275.24/s)  LR: 7.896e-04  Data: 0.010 (0.014)
Train: 183 [1000/1251 ( 80%)]  Loss: 3.097 (3.61)  Time: 0.806s, 1269.76/s  (0.803s, 1275.70/s)  LR: 7.896e-04  Data: 0.012 (0.013)
Train: 183 [1050/1251 ( 84%)]  Loss: 3.569 (3.61)  Time: 0.799s, 1282.30/s  (0.802s, 1276.09/s)  LR: 7.896e-04  Data: 0.010 (0.013)
Train: 183 [1100/1251 ( 88%)]  Loss: 3.845 (3.62)  Time: 0.849s, 1205.96/s  (0.802s, 1276.32/s)  LR: 7.896e-04  Data: 0.010 (0.013)
Train: 183 [1150/1251 ( 92%)]  Loss: 3.518 (3.61)  Time: 0.826s, 1239.94/s  (0.802s, 1276.20/s)  LR: 7.896e-04  Data: 0.014 (0.013)
Train: 183 [1200/1251 ( 96%)]  Loss: 3.587 (3.61)  Time: 0.796s, 1286.22/s  (0.802s, 1276.35/s)  LR: 7.896e-04  Data: 0.010 (0.013)
Train: 183 [1250/1251 (100%)]  Loss: 3.324 (3.60)  Time: 0.805s, 1271.59/s  (0.802s, 1276.39/s)  LR: 7.896e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.719 (1.719)  Loss:  0.7622 (0.7622)  Acc@1: 86.4258 (86.4258)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.194 (0.603)  Loss:  0.9043 (1.3412)  Acc@1: 83.1368 (72.7980)  Acc@5: 95.0472 (91.5920)
Train: 184 [   0/1251 (  0%)]  Loss: 3.599 (3.60)  Time: 2.275s,  450.01/s  (2.275s,  450.01/s)  LR: 7.875e-04  Data: 1.551 (1.551)
Train: 184 [  50/1251 (  4%)]  Loss: 3.637 (3.62)  Time: 0.793s, 1291.13/s  (0.834s, 1228.12/s)  LR: 7.875e-04  Data: 0.016 (0.047)
Train: 184 [ 100/1251 (  8%)]  Loss: 3.481 (3.57)  Time: 0.802s, 1277.50/s  (0.819s, 1250.19/s)  LR: 7.875e-04  Data: 0.014 (0.029)
Train: 184 [ 150/1251 ( 12%)]  Loss: 3.632 (3.59)  Time: 0.810s, 1264.88/s  (0.814s, 1258.37/s)  LR: 7.875e-04  Data: 0.014 (0.023)
Train: 184 [ 200/1251 ( 16%)]  Loss: 3.598 (3.59)  Time: 0.785s, 1304.86/s  (0.811s, 1262.34/s)  LR: 7.875e-04  Data: 0.010 (0.020)
Train: 184 [ 250/1251 ( 20%)]  Loss: 3.460 (3.57)  Time: 0.835s, 1225.66/s  (0.809s, 1265.53/s)  LR: 7.875e-04  Data: 0.009 (0.018)
Train: 184 [ 300/1251 ( 24%)]  Loss: 3.217 (3.52)  Time: 0.845s, 1211.14/s  (0.809s, 1265.87/s)  LR: 7.875e-04  Data: 0.010 (0.017)
Train: 184 [ 350/1251 ( 28%)]  Loss: 3.346 (3.50)  Time: 0.816s, 1255.43/s  (0.808s, 1267.33/s)  LR: 7.875e-04  Data: 0.010 (0.016)
Train: 184 [ 400/1251 ( 32%)]  Loss: 3.774 (3.53)  Time: 0.774s, 1322.69/s  (0.807s, 1269.41/s)  LR: 7.875e-04  Data: 0.010 (0.015)
Train: 184 [ 450/1251 ( 36%)]  Loss: 3.441 (3.52)  Time: 0.791s, 1294.74/s  (0.807s, 1269.51/s)  LR: 7.875e-04  Data: 0.010 (0.015)
Train: 184 [ 500/1251 ( 40%)]  Loss: 3.438 (3.51)  Time: 0.841s, 1217.97/s  (0.806s, 1271.02/s)  LR: 7.875e-04  Data: 0.011 (0.015)
Train: 184 [ 550/1251 ( 44%)]  Loss: 3.428 (3.50)  Time: 0.784s, 1306.41/s  (0.806s, 1270.88/s)  LR: 7.875e-04  Data: 0.010 (0.014)
Train: 184 [ 600/1251 ( 48%)]  Loss: 3.786 (3.53)  Time: 0.832s, 1231.06/s  (0.806s, 1271.10/s)  LR: 7.875e-04  Data: 0.010 (0.014)
Train: 184 [ 650/1251 ( 52%)]  Loss: 3.367 (3.51)  Time: 0.821s, 1247.46/s  (0.805s, 1271.81/s)  LR: 7.875e-04  Data: 0.016 (0.014)
Train: 184 [ 700/1251 ( 56%)]  Loss: 3.397 (3.51)  Time: 0.776s, 1319.15/s  (0.805s, 1271.71/s)  LR: 7.875e-04  Data: 0.011 (0.014)
Train: 184 [ 750/1251 ( 60%)]  Loss: 3.967 (3.54)  Time: 0.773s, 1325.46/s  (0.805s, 1272.23/s)  LR: 7.875e-04  Data: 0.010 (0.013)
Train: 184 [ 800/1251 ( 64%)]  Loss: 3.462 (3.53)  Time: 0.796s, 1286.75/s  (0.805s, 1272.76/s)  LR: 7.875e-04  Data: 0.012 (0.013)
Train: 184 [ 850/1251 ( 68%)]  Loss: 3.787 (3.55)  Time: 0.874s, 1171.57/s  (0.804s, 1272.98/s)  LR: 7.875e-04  Data: 0.009 (0.013)
Train: 184 [ 900/1251 ( 72%)]  Loss: 3.740 (3.56)  Time: 0.776s, 1319.02/s  (0.804s, 1273.46/s)  LR: 7.875e-04  Data: 0.010 (0.013)
Train: 184 [ 950/1251 ( 76%)]  Loss: 3.756 (3.57)  Time: 0.827s, 1238.41/s  (0.804s, 1273.58/s)  LR: 7.875e-04  Data: 0.010 (0.013)
Train: 184 [1000/1251 ( 80%)]  Loss: 3.659 (3.57)  Time: 0.813s, 1259.12/s  (0.804s, 1273.34/s)  LR: 7.875e-04  Data: 0.010 (0.013)
Train: 184 [1050/1251 ( 84%)]  Loss: 3.510 (3.57)  Time: 0.814s, 1258.33/s  (0.804s, 1273.50/s)  LR: 7.875e-04  Data: 0.013 (0.013)
Train: 184 [1100/1251 ( 88%)]  Loss: 3.467 (3.56)  Time: 0.791s, 1293.91/s  (0.804s, 1273.56/s)  LR: 7.875e-04  Data: 0.010 (0.013)
Train: 184 [1150/1251 ( 92%)]  Loss: 3.682 (3.57)  Time: 0.803s, 1275.60/s  (0.804s, 1273.66/s)  LR: 7.875e-04  Data: 0.010 (0.013)
Train: 184 [1200/1251 ( 96%)]  Loss: 3.370 (3.56)  Time: 0.789s, 1297.98/s  (0.804s, 1273.75/s)  LR: 7.875e-04  Data: 0.011 (0.012)
Train: 184 [1250/1251 (100%)]  Loss: 3.804 (3.57)  Time: 0.760s, 1348.01/s  (0.804s, 1274.02/s)  LR: 7.875e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.632 (1.632)  Loss:  0.8130 (0.8130)  Acc@1: 89.2578 (89.2578)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.194 (0.596)  Loss:  0.8232 (1.4210)  Acc@1: 85.8491 (72.9540)  Acc@5: 96.3443 (91.7460)
Train: 185 [   0/1251 (  0%)]  Loss: 3.467 (3.47)  Time: 2.208s,  463.79/s  (2.208s,  463.79/s)  LR: 7.854e-04  Data: 1.479 (1.479)
Train: 185 [  50/1251 (  4%)]  Loss: 3.749 (3.61)  Time: 0.800s, 1280.33/s  (0.827s, 1238.54/s)  LR: 7.854e-04  Data: 0.010 (0.042)
Train: 185 [ 100/1251 (  8%)]  Loss: 3.030 (3.42)  Time: 0.775s, 1321.63/s  (0.810s, 1263.49/s)  LR: 7.854e-04  Data: 0.010 (0.026)
Train: 185 [ 150/1251 ( 12%)]  Loss: 4.012 (3.56)  Time: 0.826s, 1239.18/s  (0.809s, 1265.87/s)  LR: 7.854e-04  Data: 0.014 (0.021)
Train: 185 [ 200/1251 ( 16%)]  Loss: 3.301 (3.51)  Time: 0.782s, 1308.82/s  (0.808s, 1267.15/s)  LR: 7.854e-04  Data: 0.012 (0.019)
Train: 185 [ 250/1251 ( 20%)]  Loss: 3.743 (3.55)  Time: 0.862s, 1187.97/s  (0.807s, 1269.06/s)  LR: 7.854e-04  Data: 0.010 (0.017)
Train: 185 [ 300/1251 ( 24%)]  Loss: 3.791 (3.58)  Time: 0.777s, 1317.38/s  (0.805s, 1271.53/s)  LR: 7.854e-04  Data: 0.009 (0.016)
Train: 185 [ 350/1251 ( 28%)]  Loss: 3.440 (3.57)  Time: 0.774s, 1322.85/s  (0.804s, 1273.49/s)  LR: 7.854e-04  Data: 0.010 (0.015)
Train: 185 [ 400/1251 ( 32%)]  Loss: 3.347 (3.54)  Time: 0.829s, 1235.47/s  (0.804s, 1273.21/s)  LR: 7.854e-04  Data: 0.015 (0.015)
Train: 185 [ 450/1251 ( 36%)]  Loss: 3.405 (3.53)  Time: 0.846s, 1210.30/s  (0.805s, 1272.22/s)  LR: 7.854e-04  Data: 0.010 (0.014)
Train: 185 [ 500/1251 ( 40%)]  Loss: 3.376 (3.51)  Time: 0.773s, 1325.20/s  (0.805s, 1272.53/s)  LR: 7.854e-04  Data: 0.010 (0.014)
Train: 185 [ 550/1251 ( 44%)]  Loss: 3.564 (3.52)  Time: 0.814s, 1258.76/s  (0.805s, 1272.76/s)  LR: 7.854e-04  Data: 0.010 (0.014)
Train: 185 [ 600/1251 ( 48%)]  Loss: 3.687 (3.53)  Time: 0.853s, 1200.51/s  (0.804s, 1273.64/s)  LR: 7.854e-04  Data: 0.010 (0.014)
Train: 185 [ 650/1251 ( 52%)]  Loss: 3.540 (3.53)  Time: 0.808s, 1267.93/s  (0.803s, 1274.53/s)  LR: 7.854e-04  Data: 0.010 (0.013)
Train: 185 [ 700/1251 ( 56%)]  Loss: 3.728 (3.55)  Time: 0.774s, 1323.25/s  (0.803s, 1274.91/s)  LR: 7.854e-04  Data: 0.011 (0.013)
Train: 185 [ 750/1251 ( 60%)]  Loss: 3.413 (3.54)  Time: 0.815s, 1256.09/s  (0.803s, 1275.76/s)  LR: 7.854e-04  Data: 0.010 (0.013)
Train: 185 [ 800/1251 ( 64%)]  Loss: 3.219 (3.52)  Time: 0.828s, 1236.21/s  (0.803s, 1275.75/s)  LR: 7.854e-04  Data: 0.022 (0.013)
Train: 185 [ 850/1251 ( 68%)]  Loss: 3.688 (3.53)  Time: 0.771s, 1328.32/s  (0.803s, 1275.21/s)  LR: 7.854e-04  Data: 0.010 (0.013)
Train: 185 [ 900/1251 ( 72%)]  Loss: 3.502 (3.53)  Time: 0.948s, 1079.72/s  (0.803s, 1275.42/s)  LR: 7.854e-04  Data: 0.009 (0.013)
Train: 185 [ 950/1251 ( 76%)]  Loss: 3.253 (3.51)  Time: 0.796s, 1286.93/s  (0.803s, 1275.74/s)  LR: 7.854e-04  Data: 0.010 (0.013)
Train: 185 [1000/1251 ( 80%)]  Loss: 3.494 (3.51)  Time: 0.773s, 1324.36/s  (0.803s, 1275.77/s)  LR: 7.854e-04  Data: 0.010 (0.013)
Train: 185 [1050/1251 ( 84%)]  Loss: 3.904 (3.53)  Time: 0.780s, 1312.30/s  (0.803s, 1275.88/s)  LR: 7.854e-04  Data: 0.010 (0.013)
Train: 185 [1100/1251 ( 88%)]  Loss: 3.930 (3.55)  Time: 0.773s, 1325.41/s  (0.802s, 1276.27/s)  LR: 7.854e-04  Data: 0.010 (0.012)
Train: 185 [1150/1251 ( 92%)]  Loss: 3.617 (3.55)  Time: 0.792s, 1293.24/s  (0.802s, 1276.09/s)  LR: 7.854e-04  Data: 0.010 (0.012)
Train: 185 [1200/1251 ( 96%)]  Loss: 3.896 (3.56)  Time: 0.773s, 1324.67/s  (0.802s, 1276.38/s)  LR: 7.854e-04  Data: 0.010 (0.012)
Train: 185 [1250/1251 (100%)]  Loss: 3.634 (3.57)  Time: 0.760s, 1347.86/s  (0.802s, 1276.66/s)  LR: 7.854e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.680 (1.680)  Loss:  0.7983 (0.7983)  Acc@1: 87.5000 (87.5000)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.193 (0.603)  Loss:  0.9600 (1.4311)  Acc@1: 83.6085 (73.2040)  Acc@5: 94.8113 (92.0140)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-180.pth.tar', 73.55800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-181.pth.tar', 73.43000009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-176.pth.tar', 73.40400009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-171.pth.tar', 73.36800006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-178.pth.tar', 73.24000006347656)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-172.pth.tar', 73.2159999633789)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-185.pth.tar', 73.2040001196289)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-179.pth.tar', 73.18999993408202)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-163.pth.tar', 73.14800001464843)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-160.pth.tar', 73.08200001953125)

Train: 186 [   0/1251 (  0%)]  Loss: 3.570 (3.57)  Time: 2.585s,  396.14/s  (2.585s,  396.14/s)  LR: 7.832e-04  Data: 1.850 (1.850)
Train: 186 [  50/1251 (  4%)]  Loss: 3.244 (3.41)  Time: 0.799s, 1281.53/s  (0.842s, 1215.81/s)  LR: 7.832e-04  Data: 0.013 (0.052)
Train: 186 [ 100/1251 (  8%)]  Loss: 3.866 (3.56)  Time: 0.796s, 1285.96/s  (0.820s, 1248.17/s)  LR: 7.832e-04  Data: 0.011 (0.032)
Train: 186 [ 150/1251 ( 12%)]  Loss: 3.835 (3.63)  Time: 0.806s, 1269.94/s  (0.813s, 1259.14/s)  LR: 7.832e-04  Data: 0.016 (0.025)
Train: 186 [ 200/1251 ( 16%)]  Loss: 3.730 (3.65)  Time: 0.804s, 1273.39/s  (0.811s, 1263.18/s)  LR: 7.832e-04  Data: 0.010 (0.022)
Train: 186 [ 250/1251 ( 20%)]  Loss: 3.584 (3.64)  Time: 0.801s, 1278.46/s  (0.810s, 1264.08/s)  LR: 7.832e-04  Data: 0.010 (0.020)
Train: 186 [ 300/1251 ( 24%)]  Loss: 3.656 (3.64)  Time: 0.810s, 1264.55/s  (0.809s, 1265.80/s)  LR: 7.832e-04  Data: 0.010 (0.018)
Train: 186 [ 350/1251 ( 28%)]  Loss: 3.580 (3.63)  Time: 0.814s, 1257.58/s  (0.808s, 1267.58/s)  LR: 7.832e-04  Data: 0.010 (0.017)
Train: 186 [ 400/1251 ( 32%)]  Loss: 3.719 (3.64)  Time: 0.803s, 1274.49/s  (0.806s, 1270.18/s)  LR: 7.832e-04  Data: 0.013 (0.016)
Train: 186 [ 450/1251 ( 36%)]  Loss: 3.156 (3.59)  Time: 0.776s, 1319.97/s  (0.805s, 1271.37/s)  LR: 7.832e-04  Data: 0.010 (0.016)
Train: 186 [ 500/1251 ( 40%)]  Loss: 3.484 (3.58)  Time: 0.773s, 1324.70/s  (0.805s, 1271.59/s)  LR: 7.832e-04  Data: 0.010 (0.015)
Train: 186 [ 550/1251 ( 44%)]  Loss: 3.690 (3.59)  Time: 0.798s, 1282.59/s  (0.805s, 1272.66/s)  LR: 7.832e-04  Data: 0.010 (0.015)
Train: 186 [ 600/1251 ( 48%)]  Loss: 3.378 (3.58)  Time: 0.796s, 1286.81/s  (0.805s, 1272.59/s)  LR: 7.832e-04  Data: 0.010 (0.015)
Train: 186 [ 650/1251 ( 52%)]  Loss: 3.567 (3.58)  Time: 0.809s, 1265.80/s  (0.805s, 1272.09/s)  LR: 7.832e-04  Data: 0.014 (0.014)
Train: 186 [ 700/1251 ( 56%)]  Loss: 3.878 (3.60)  Time: 0.773s, 1325.06/s  (0.804s, 1272.87/s)  LR: 7.832e-04  Data: 0.010 (0.014)
Train: 186 [ 750/1251 ( 60%)]  Loss: 3.682 (3.60)  Time: 0.810s, 1263.68/s  (0.804s, 1272.97/s)  LR: 7.832e-04  Data: 0.011 (0.014)
Train: 186 [ 800/1251 ( 64%)]  Loss: 3.553 (3.60)  Time: 0.788s, 1299.83/s  (0.805s, 1272.52/s)  LR: 7.832e-04  Data: 0.015 (0.014)
Train: 186 [ 850/1251 ( 68%)]  Loss: 3.495 (3.59)  Time: 0.804s, 1273.22/s  (0.804s, 1273.06/s)  LR: 7.832e-04  Data: 0.010 (0.014)
Train: 186 [ 900/1251 ( 72%)]  Loss: 3.616 (3.59)  Time: 0.807s, 1268.51/s  (0.804s, 1273.48/s)  LR: 7.832e-04  Data: 0.011 (0.014)
Train: 186 [ 950/1251 ( 76%)]  Loss: 3.518 (3.59)  Time: 0.774s, 1323.52/s  (0.804s, 1274.17/s)  LR: 7.832e-04  Data: 0.010 (0.013)
Train: 186 [1000/1251 ( 80%)]  Loss: 3.780 (3.60)  Time: 0.792s, 1293.01/s  (0.804s, 1274.37/s)  LR: 7.832e-04  Data: 0.009 (0.013)
Train: 186 [1050/1251 ( 84%)]  Loss: 3.201 (3.58)  Time: 0.774s, 1323.20/s  (0.803s, 1274.89/s)  LR: 7.832e-04  Data: 0.010 (0.013)
Train: 186 [1100/1251 ( 88%)]  Loss: 3.593 (3.58)  Time: 0.787s, 1301.59/s  (0.803s, 1275.15/s)  LR: 7.832e-04  Data: 0.014 (0.013)
Train: 186 [1150/1251 ( 92%)]  Loss: 3.670 (3.59)  Time: 0.838s, 1221.48/s  (0.803s, 1275.37/s)  LR: 7.832e-04  Data: 0.017 (0.013)
Train: 186 [1200/1251 ( 96%)]  Loss: 3.753 (3.59)  Time: 0.798s, 1282.50/s  (0.803s, 1275.67/s)  LR: 7.832e-04  Data: 0.011 (0.013)
Train: 186 [1250/1251 (100%)]  Loss: 3.298 (3.58)  Time: 0.767s, 1334.60/s  (0.803s, 1275.58/s)  LR: 7.832e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.612 (1.612)  Loss:  0.7944 (0.7944)  Acc@1: 89.6484 (89.6484)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.194 (0.604)  Loss:  0.8672 (1.3238)  Acc@1: 83.8443 (73.6200)  Acc@5: 95.7547 (92.0420)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-186.pth.tar', 73.62000009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-180.pth.tar', 73.55800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-181.pth.tar', 73.43000009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-176.pth.tar', 73.40400009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-171.pth.tar', 73.36800006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-178.pth.tar', 73.24000006347656)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-172.pth.tar', 73.2159999633789)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-185.pth.tar', 73.2040001196289)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-179.pth.tar', 73.18999993408202)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-163.pth.tar', 73.14800001464843)

Train: 187 [   0/1251 (  0%)]  Loss: 3.647 (3.65)  Time: 2.478s,  413.27/s  (2.478s,  413.27/s)  LR: 7.811e-04  Data: 1.748 (1.748)
Train: 187 [  50/1251 (  4%)]  Loss: 3.485 (3.57)  Time: 0.784s, 1305.39/s  (0.845s, 1212.25/s)  LR: 7.811e-04  Data: 0.010 (0.051)
Train: 187 [ 100/1251 (  8%)]  Loss: 3.551 (3.56)  Time: 0.810s, 1264.86/s  (0.825s, 1241.27/s)  LR: 7.811e-04  Data: 0.011 (0.032)
Train: 187 [ 150/1251 ( 12%)]  Loss: 4.007 (3.67)  Time: 0.861s, 1188.86/s  (0.817s, 1254.07/s)  LR: 7.811e-04  Data: 0.014 (0.025)
Train: 187 [ 200/1251 ( 16%)]  Loss: 3.829 (3.70)  Time: 0.773s, 1324.35/s  (0.813s, 1259.43/s)  LR: 7.811e-04  Data: 0.010 (0.021)
Train: 187 [ 250/1251 ( 20%)]  Loss: 3.831 (3.73)  Time: 0.775s, 1321.48/s  (0.809s, 1265.08/s)  LR: 7.811e-04  Data: 0.010 (0.019)
Train: 187 [ 300/1251 ( 24%)]  Loss: 4.009 (3.77)  Time: 0.789s, 1297.42/s  (0.807s, 1268.22/s)  LR: 7.811e-04  Data: 0.011 (0.018)
Train: 187 [ 350/1251 ( 28%)]  Loss: 3.218 (3.70)  Time: 0.861s, 1189.71/s  (0.806s, 1270.92/s)  LR: 7.811e-04  Data: 0.010 (0.017)
Train: 187 [ 400/1251 ( 32%)]  Loss: 3.795 (3.71)  Time: 0.780s, 1312.39/s  (0.805s, 1272.32/s)  LR: 7.811e-04  Data: 0.011 (0.016)
Train: 187 [ 450/1251 ( 36%)]  Loss: 3.619 (3.70)  Time: 0.793s, 1291.19/s  (0.805s, 1271.66/s)  LR: 7.811e-04  Data: 0.010 (0.016)
Train: 187 [ 500/1251 ( 40%)]  Loss: 3.580 (3.69)  Time: 0.773s, 1324.79/s  (0.804s, 1273.28/s)  LR: 7.811e-04  Data: 0.010 (0.015)
Train: 187 [ 550/1251 ( 44%)]  Loss: 3.533 (3.68)  Time: 0.838s, 1222.01/s  (0.804s, 1273.15/s)  LR: 7.811e-04  Data: 0.018 (0.015)
Train: 187 [ 600/1251 ( 48%)]  Loss: 3.723 (3.68)  Time: 0.795s, 1288.06/s  (0.804s, 1273.77/s)  LR: 7.811e-04  Data: 0.014 (0.014)
Train: 187 [ 650/1251 ( 52%)]  Loss: 3.534 (3.67)  Time: 0.778s, 1315.91/s  (0.803s, 1274.50/s)  LR: 7.811e-04  Data: 0.011 (0.014)
Train: 187 [ 700/1251 ( 56%)]  Loss: 3.479 (3.66)  Time: 0.779s, 1314.76/s  (0.803s, 1274.90/s)  LR: 7.811e-04  Data: 0.010 (0.014)
Train: 187 [ 750/1251 ( 60%)]  Loss: 3.581 (3.65)  Time: 0.878s, 1166.25/s  (0.804s, 1274.41/s)  LR: 7.811e-04  Data: 0.015 (0.014)
Train: 187 [ 800/1251 ( 64%)]  Loss: 3.753 (3.66)  Time: 0.783s, 1307.26/s  (0.804s, 1274.17/s)  LR: 7.811e-04  Data: 0.013 (0.014)
Train: 187 [ 850/1251 ( 68%)]  Loss: 3.449 (3.65)  Time: 0.835s, 1225.68/s  (0.803s, 1275.27/s)  LR: 7.811e-04  Data: 0.010 (0.014)
Train: 187 [ 900/1251 ( 72%)]  Loss: 3.687 (3.65)  Time: 0.775s, 1321.92/s  (0.803s, 1275.54/s)  LR: 7.811e-04  Data: 0.010 (0.013)
Train: 187 [ 950/1251 ( 76%)]  Loss: 3.789 (3.66)  Time: 0.799s, 1282.24/s  (0.803s, 1275.03/s)  LR: 7.811e-04  Data: 0.013 (0.013)
Train: 187 [1000/1251 ( 80%)]  Loss: 3.589 (3.65)  Time: 0.771s, 1327.57/s  (0.803s, 1275.34/s)  LR: 7.811e-04  Data: 0.010 (0.013)
Train: 187 [1050/1251 ( 84%)]  Loss: 3.607 (3.65)  Time: 0.774s, 1322.23/s  (0.803s, 1274.85/s)  LR: 7.811e-04  Data: 0.010 (0.013)
Train: 187 [1100/1251 ( 88%)]  Loss: 3.144 (3.63)  Time: 0.817s, 1253.70/s  (0.803s, 1274.95/s)  LR: 7.811e-04  Data: 0.010 (0.013)
Train: 187 [1150/1251 ( 92%)]  Loss: 3.503 (3.62)  Time: 0.787s, 1301.43/s  (0.804s, 1274.21/s)  LR: 7.811e-04  Data: 0.015 (0.013)
Train: 187 [1200/1251 ( 96%)]  Loss: 3.449 (3.62)  Time: 0.807s, 1269.44/s  (0.804s, 1274.34/s)  LR: 7.811e-04  Data: 0.014 (0.013)
Train: 187 [1250/1251 (100%)]  Loss: 4.012 (3.63)  Time: 0.798s, 1283.14/s  (0.804s, 1274.18/s)  LR: 7.811e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.672 (1.672)  Loss:  0.8936 (0.8936)  Acc@1: 86.4258 (86.4258)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.194 (0.576)  Loss:  0.9722 (1.3844)  Acc@1: 83.3726 (73.2400)  Acc@5: 95.2830 (91.9000)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-186.pth.tar', 73.62000009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-180.pth.tar', 73.55800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-181.pth.tar', 73.43000009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-176.pth.tar', 73.40400009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-171.pth.tar', 73.36800006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-178.pth.tar', 73.24000006347656)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-187.pth.tar', 73.24000001708984)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-172.pth.tar', 73.2159999633789)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-185.pth.tar', 73.2040001196289)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-179.pth.tar', 73.18999993408202)

Train: 188 [   0/1251 (  0%)]  Loss: 4.000 (4.00)  Time: 2.715s,  377.23/s  (2.715s,  377.23/s)  LR: 7.789e-04  Data: 1.959 (1.959)
Train: 188 [  50/1251 (  4%)]  Loss: 3.547 (3.77)  Time: 0.838s, 1221.33/s  (0.883s, 1160.12/s)  LR: 7.789e-04  Data: 0.010 (0.067)
Train: 188 [ 100/1251 (  8%)]  Loss: 3.533 (3.69)  Time: 0.813s, 1260.01/s  (0.838s, 1222.57/s)  LR: 7.789e-04  Data: 0.010 (0.039)
Train: 188 [ 150/1251 ( 12%)]  Loss: 3.835 (3.73)  Time: 0.772s, 1325.65/s  (0.818s, 1252.06/s)  LR: 7.789e-04  Data: 0.009 (0.030)
Train: 188 [ 200/1251 ( 16%)]  Loss: 3.873 (3.76)  Time: 0.809s, 1266.02/s  (0.813s, 1258.96/s)  LR: 7.789e-04  Data: 0.014 (0.025)
Train: 188 [ 250/1251 ( 20%)]  Loss: 3.391 (3.70)  Time: 0.858s, 1193.72/s  (0.811s, 1262.10/s)  LR: 7.789e-04  Data: 0.013 (0.022)
Train: 188 [ 300/1251 ( 24%)]  Loss: 3.719 (3.70)  Time: 0.816s, 1255.14/s  (0.810s, 1263.45/s)  LR: 7.789e-04  Data: 0.010 (0.020)
Train: 188 [ 350/1251 ( 28%)]  Loss: 3.737 (3.70)  Time: 0.796s, 1286.44/s  (0.810s, 1263.96/s)  LR: 7.789e-04  Data: 0.014 (0.019)
Train: 188 [ 400/1251 ( 32%)]  Loss: 3.627 (3.70)  Time: 0.779s, 1314.05/s  (0.810s, 1264.51/s)  LR: 7.789e-04  Data: 0.013 (0.018)
Train: 188 [ 450/1251 ( 36%)]  Loss: 3.457 (3.67)  Time: 0.773s, 1325.23/s  (0.809s, 1266.49/s)  LR: 7.789e-04  Data: 0.009 (0.017)
Train: 188 [ 500/1251 ( 40%)]  Loss: 3.681 (3.67)  Time: 0.784s, 1305.77/s  (0.808s, 1267.54/s)  LR: 7.789e-04  Data: 0.015 (0.016)
Train: 188 [ 550/1251 ( 44%)]  Loss: 3.287 (3.64)  Time: 0.809s, 1265.04/s  (0.807s, 1268.58/s)  LR: 7.789e-04  Data: 0.009 (0.016)
Train: 188 [ 600/1251 ( 48%)]  Loss: 3.744 (3.65)  Time: 0.782s, 1308.99/s  (0.807s, 1269.28/s)  LR: 7.789e-04  Data: 0.014 (0.015)
Train: 188 [ 650/1251 ( 52%)]  Loss: 3.891 (3.67)  Time: 0.858s, 1194.00/s  (0.807s, 1269.28/s)  LR: 7.789e-04  Data: 0.015 (0.015)
Train: 188 [ 700/1251 ( 56%)]  Loss: 3.567 (3.66)  Time: 0.810s, 1263.59/s  (0.806s, 1269.70/s)  LR: 7.789e-04  Data: 0.011 (0.015)
Train: 188 [ 750/1251 ( 60%)]  Loss: 3.818 (3.67)  Time: 0.792s, 1293.43/s  (0.806s, 1270.02/s)  LR: 7.789e-04  Data: 0.014 (0.015)
Train: 188 [ 800/1251 ( 64%)]  Loss: 3.469 (3.66)  Time: 0.802s, 1277.05/s  (0.806s, 1270.19/s)  LR: 7.789e-04  Data: 0.010 (0.014)
Train: 188 [ 850/1251 ( 68%)]  Loss: 3.872 (3.67)  Time: 0.786s, 1302.59/s  (0.806s, 1270.34/s)  LR: 7.789e-04  Data: 0.010 (0.014)
Train: 188 [ 900/1251 ( 72%)]  Loss: 3.467 (3.66)  Time: 0.780s, 1312.55/s  (0.806s, 1270.97/s)  LR: 7.789e-04  Data: 0.010 (0.014)
Train: 188 [ 950/1251 ( 76%)]  Loss: 3.582 (3.65)  Time: 0.809s, 1265.94/s  (0.806s, 1271.04/s)  LR: 7.789e-04  Data: 0.010 (0.014)
Train: 188 [1000/1251 ( 80%)]  Loss: 3.486 (3.65)  Time: 0.778s, 1316.93/s  (0.805s, 1271.72/s)  LR: 7.789e-04  Data: 0.010 (0.014)
Train: 188 [1050/1251 ( 84%)]  Loss: 3.263 (3.63)  Time: 0.819s, 1249.83/s  (0.805s, 1271.97/s)  LR: 7.789e-04  Data: 0.013 (0.014)
Train: 188 [1100/1251 ( 88%)]  Loss: 3.853 (3.64)  Time: 0.781s, 1310.81/s  (0.805s, 1271.36/s)  LR: 7.789e-04  Data: 0.010 (0.014)
Train: 188 [1150/1251 ( 92%)]  Loss: 3.613 (3.64)  Time: 0.803s, 1275.11/s  (0.805s, 1271.37/s)  LR: 7.789e-04  Data: 0.016 (0.013)
Train: 188 [1200/1251 ( 96%)]  Loss: 3.618 (3.64)  Time: 0.773s, 1324.00/s  (0.806s, 1271.12/s)  LR: 7.789e-04  Data: 0.010 (0.013)
Train: 188 [1250/1251 (100%)]  Loss: 3.213 (3.62)  Time: 0.818s, 1252.19/s  (0.805s, 1271.65/s)  LR: 7.789e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.545 (1.545)  Loss:  0.7148 (0.7148)  Acc@1: 89.8438 (89.8438)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.194 (0.592)  Loss:  0.8062 (1.3072)  Acc@1: 86.2028 (73.3800)  Acc@5: 96.2264 (91.9480)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-186.pth.tar', 73.62000009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-180.pth.tar', 73.55800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-181.pth.tar', 73.43000009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-176.pth.tar', 73.40400009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-188.pth.tar', 73.37999995361328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-171.pth.tar', 73.36800006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-178.pth.tar', 73.24000006347656)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-187.pth.tar', 73.24000001708984)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-172.pth.tar', 73.2159999633789)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-185.pth.tar', 73.2040001196289)

Train: 189 [   0/1251 (  0%)]  Loss: 3.358 (3.36)  Time: 2.574s,  397.88/s  (2.574s,  397.88/s)  LR: 7.768e-04  Data: 1.840 (1.840)
Train: 189 [  50/1251 (  4%)]  Loss: 3.750 (3.55)  Time: 0.783s, 1307.23/s  (0.840s, 1218.64/s)  LR: 7.768e-04  Data: 0.011 (0.052)
Train: 189 [ 100/1251 (  8%)]  Loss: 3.751 (3.62)  Time: 0.803s, 1275.11/s  (0.820s, 1248.17/s)  LR: 7.768e-04  Data: 0.011 (0.032)
Train: 189 [ 150/1251 ( 12%)]  Loss: 3.904 (3.69)  Time: 0.873s, 1172.38/s  (0.814s, 1257.76/s)  LR: 7.768e-04  Data: 0.010 (0.025)
Train: 189 [ 200/1251 ( 16%)]  Loss: 3.910 (3.73)  Time: 0.793s, 1292.10/s  (0.813s, 1260.06/s)  LR: 7.768e-04  Data: 0.010 (0.022)
Train: 189 [ 250/1251 ( 20%)]  Loss: 3.157 (3.64)  Time: 0.786s, 1302.36/s  (0.810s, 1263.54/s)  LR: 7.768e-04  Data: 0.010 (0.019)
Train: 189 [ 300/1251 ( 24%)]  Loss: 3.400 (3.60)  Time: 0.772s, 1326.14/s  (0.808s, 1266.99/s)  LR: 7.768e-04  Data: 0.011 (0.018)
Train: 189 [ 350/1251 ( 28%)]  Loss: 3.664 (3.61)  Time: 0.778s, 1315.79/s  (0.807s, 1269.60/s)  LR: 7.768e-04  Data: 0.010 (0.017)
Train: 189 [ 400/1251 ( 32%)]  Loss: 3.941 (3.65)  Time: 0.774s, 1323.80/s  (0.807s, 1269.66/s)  LR: 7.768e-04  Data: 0.010 (0.016)
Train: 189 [ 450/1251 ( 36%)]  Loss: 3.565 (3.64)  Time: 0.778s, 1316.96/s  (0.806s, 1270.96/s)  LR: 7.768e-04  Data: 0.010 (0.016)
Train: 189 [ 500/1251 ( 40%)]  Loss: 3.503 (3.63)  Time: 0.844s, 1212.91/s  (0.805s, 1271.32/s)  LR: 7.768e-04  Data: 0.010 (0.015)
Train: 189 [ 550/1251 ( 44%)]  Loss: 3.778 (3.64)  Time: 0.845s, 1211.42/s  (0.805s, 1271.72/s)  LR: 7.768e-04  Data: 0.009 (0.015)
Train: 189 [ 600/1251 ( 48%)]  Loss: 3.487 (3.63)  Time: 0.772s, 1326.35/s  (0.805s, 1271.87/s)  LR: 7.768e-04  Data: 0.010 (0.015)
Train: 189 [ 650/1251 ( 52%)]  Loss: 3.371 (3.61)  Time: 0.810s, 1263.68/s  (0.805s, 1272.27/s)  LR: 7.768e-04  Data: 0.011 (0.014)
Train: 189 [ 700/1251 ( 56%)]  Loss: 3.333 (3.59)  Time: 0.773s, 1324.47/s  (0.804s, 1273.42/s)  LR: 7.768e-04  Data: 0.011 (0.014)
Train: 189 [ 750/1251 ( 60%)]  Loss: 3.479 (3.58)  Time: 0.776s, 1319.90/s  (0.804s, 1273.82/s)  LR: 7.768e-04  Data: 0.010 (0.014)
Train: 189 [ 800/1251 ( 64%)]  Loss: 3.761 (3.59)  Time: 0.838s, 1222.52/s  (0.804s, 1274.26/s)  LR: 7.768e-04  Data: 0.016 (0.014)
Train: 189 [ 850/1251 ( 68%)]  Loss: 3.651 (3.60)  Time: 0.816s, 1254.40/s  (0.803s, 1274.48/s)  LR: 7.768e-04  Data: 0.010 (0.014)
Train: 189 [ 900/1251 ( 72%)]  Loss: 3.895 (3.61)  Time: 0.988s, 1035.92/s  (0.804s, 1273.99/s)  LR: 7.768e-04  Data: 0.010 (0.013)
Train: 189 [ 950/1251 ( 76%)]  Loss: 3.698 (3.62)  Time: 0.776s, 1319.55/s  (0.804s, 1274.35/s)  LR: 7.768e-04  Data: 0.010 (0.013)
Train: 189 [1000/1251 ( 80%)]  Loss: 3.569 (3.62)  Time: 0.847s, 1208.52/s  (0.804s, 1274.14/s)  LR: 7.768e-04  Data: 0.010 (0.013)
Train: 189 [1050/1251 ( 84%)]  Loss: 3.467 (3.61)  Time: 0.801s, 1277.90/s  (0.803s, 1274.65/s)  LR: 7.768e-04  Data: 0.010 (0.013)
Train: 189 [1100/1251 ( 88%)]  Loss: 3.238 (3.59)  Time: 0.832s, 1231.36/s  (0.803s, 1274.48/s)  LR: 7.768e-04  Data: 0.010 (0.013)
Train: 189 [1150/1251 ( 92%)]  Loss: 3.885 (3.60)  Time: 0.773s, 1324.36/s  (0.803s, 1274.43/s)  LR: 7.768e-04  Data: 0.010 (0.013)
Train: 189 [1200/1251 ( 96%)]  Loss: 3.976 (3.62)  Time: 0.838s, 1222.60/s  (0.803s, 1274.75/s)  LR: 7.768e-04  Data: 0.015 (0.013)
Train: 189 [1250/1251 (100%)]  Loss: 3.455 (3.61)  Time: 0.759s, 1348.97/s  (0.803s, 1274.89/s)  LR: 7.768e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.674 (1.674)  Loss:  0.8760 (0.8760)  Acc@1: 87.6953 (87.6953)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.194 (0.609)  Loss:  0.9409 (1.4224)  Acc@1: 82.7830 (73.3600)  Acc@5: 95.7547 (91.7100)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-186.pth.tar', 73.62000009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-180.pth.tar', 73.55800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-181.pth.tar', 73.43000009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-176.pth.tar', 73.40400009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-188.pth.tar', 73.37999995361328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-171.pth.tar', 73.36800006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-189.pth.tar', 73.36000001953126)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-178.pth.tar', 73.24000006347656)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-187.pth.tar', 73.24000001708984)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-172.pth.tar', 73.2159999633789)

Train: 190 [   0/1251 (  0%)]  Loss: 3.966 (3.97)  Time: 2.420s,  423.15/s  (2.420s,  423.15/s)  LR: 7.746e-04  Data: 1.664 (1.664)
Train: 190 [  50/1251 (  4%)]  Loss: 3.473 (3.72)  Time: 0.820s, 1248.27/s  (0.839s, 1219.87/s)  LR: 7.746e-04  Data: 0.010 (0.047)
Train: 190 [ 100/1251 (  8%)]  Loss: 3.810 (3.75)  Time: 0.775s, 1320.55/s  (0.821s, 1246.86/s)  LR: 7.746e-04  Data: 0.010 (0.029)
Train: 190 [ 150/1251 ( 12%)]  Loss: 3.584 (3.71)  Time: 0.781s, 1310.65/s  (0.814s, 1257.90/s)  LR: 7.746e-04  Data: 0.013 (0.023)
Train: 190 [ 200/1251 ( 16%)]  Loss: 3.855 (3.74)  Time: 0.796s, 1287.04/s  (0.811s, 1262.01/s)  LR: 7.746e-04  Data: 0.015 (0.020)
Train: 190 [ 250/1251 ( 20%)]  Loss: 3.856 (3.76)  Time: 0.773s, 1324.29/s  (0.809s, 1265.71/s)  LR: 7.746e-04  Data: 0.010 (0.018)
Train: 190 [ 300/1251 ( 24%)]  Loss: 3.492 (3.72)  Time: 0.785s, 1304.40/s  (0.807s, 1268.48/s)  LR: 7.746e-04  Data: 0.012 (0.017)
Train: 190 [ 350/1251 ( 28%)]  Loss: 3.272 (3.66)  Time: 0.788s, 1298.90/s  (0.807s, 1269.61/s)  LR: 7.746e-04  Data: 0.016 (0.016)
Train: 190 [ 400/1251 ( 32%)]  Loss: 3.702 (3.67)  Time: 0.802s, 1276.45/s  (0.806s, 1270.81/s)  LR: 7.746e-04  Data: 0.010 (0.015)
Train: 190 [ 450/1251 ( 36%)]  Loss: 3.426 (3.64)  Time: 0.805s, 1272.03/s  (0.805s, 1271.31/s)  LR: 7.746e-04  Data: 0.014 (0.015)
Train: 190 [ 500/1251 ( 40%)]  Loss: 3.658 (3.64)  Time: 0.772s, 1327.19/s  (0.805s, 1272.40/s)  LR: 7.746e-04  Data: 0.010 (0.015)
Train: 190 [ 550/1251 ( 44%)]  Loss: 3.632 (3.64)  Time: 0.782s, 1308.75/s  (0.805s, 1271.60/s)  LR: 7.746e-04  Data: 0.014 (0.014)
Train: 190 [ 600/1251 ( 48%)]  Loss: 3.552 (3.64)  Time: 0.818s, 1251.65/s  (0.805s, 1272.37/s)  LR: 7.746e-04  Data: 0.010 (0.014)
Train: 190 [ 650/1251 ( 52%)]  Loss: 3.447 (3.62)  Time: 0.812s, 1261.67/s  (0.804s, 1273.35/s)  LR: 7.746e-04  Data: 0.013 (0.014)
Train: 190 [ 700/1251 ( 56%)]  Loss: 3.649 (3.62)  Time: 0.780s, 1312.84/s  (0.804s, 1273.65/s)  LR: 7.746e-04  Data: 0.010 (0.014)
Train: 190 [ 750/1251 ( 60%)]  Loss: 3.657 (3.63)  Time: 0.791s, 1295.36/s  (0.804s, 1273.41/s)  LR: 7.746e-04  Data: 0.010 (0.013)
Train: 190 [ 800/1251 ( 64%)]  Loss: 3.848 (3.64)  Time: 0.795s, 1287.69/s  (0.804s, 1273.72/s)  LR: 7.746e-04  Data: 0.010 (0.013)
Train: 190 [ 850/1251 ( 68%)]  Loss: 3.486 (3.63)  Time: 0.774s, 1323.35/s  (0.804s, 1274.35/s)  LR: 7.746e-04  Data: 0.010 (0.013)
Train: 190 [ 900/1251 ( 72%)]  Loss: 3.172 (3.61)  Time: 0.791s, 1293.92/s  (0.804s, 1274.30/s)  LR: 7.746e-04  Data: 0.015 (0.013)
Train: 190 [ 950/1251 ( 76%)]  Loss: 3.464 (3.60)  Time: 0.777s, 1318.41/s  (0.804s, 1274.33/s)  LR: 7.746e-04  Data: 0.010 (0.013)
Train: 190 [1000/1251 ( 80%)]  Loss: 3.884 (3.61)  Time: 0.812s, 1261.70/s  (0.803s, 1274.68/s)  LR: 7.746e-04  Data: 0.010 (0.013)
Train: 190 [1050/1251 ( 84%)]  Loss: 3.187 (3.59)  Time: 0.781s, 1310.45/s  (0.803s, 1274.83/s)  LR: 7.746e-04  Data: 0.013 (0.013)
Train: 190 [1100/1251 ( 88%)]  Loss: 3.475 (3.59)  Time: 0.804s, 1273.56/s  (0.803s, 1274.60/s)  LR: 7.746e-04  Data: 0.010 (0.013)
Train: 190 [1150/1251 ( 92%)]  Loss: 3.511 (3.59)  Time: 0.807s, 1269.59/s  (0.803s, 1274.77/s)  LR: 7.746e-04  Data: 0.010 (0.013)
Train: 190 [1200/1251 ( 96%)]  Loss: 3.796 (3.59)  Time: 0.854s, 1199.32/s  (0.803s, 1274.89/s)  LR: 7.746e-04  Data: 0.010 (0.013)
Train: 190 [1250/1251 (100%)]  Loss: 4.057 (3.61)  Time: 0.760s, 1348.00/s  (0.803s, 1275.20/s)  LR: 7.746e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.644 (1.644)  Loss:  0.8521 (0.8521)  Acc@1: 88.4766 (88.4766)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.194 (0.604)  Loss:  0.8896 (1.3593)  Acc@1: 83.6085 (73.7040)  Acc@5: 95.0472 (91.9760)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-190.pth.tar', 73.70399999023438)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-186.pth.tar', 73.62000009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-180.pth.tar', 73.55800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-181.pth.tar', 73.43000009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-176.pth.tar', 73.40400009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-188.pth.tar', 73.37999995361328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-171.pth.tar', 73.36800006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-189.pth.tar', 73.36000001953126)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-178.pth.tar', 73.24000006347656)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-187.pth.tar', 73.24000001708984)

Train: 191 [   0/1251 (  0%)]  Loss: 3.596 (3.60)  Time: 2.544s,  402.56/s  (2.544s,  402.56/s)  LR: 7.724e-04  Data: 1.812 (1.812)
Train: 191 [  50/1251 (  4%)]  Loss: 3.653 (3.62)  Time: 0.788s, 1299.34/s  (0.838s, 1221.69/s)  LR: 7.724e-04  Data: 0.014 (0.051)
Train: 191 [ 100/1251 (  8%)]  Loss: 3.405 (3.55)  Time: 0.802s, 1276.25/s  (0.816s, 1254.28/s)  LR: 7.724e-04  Data: 0.010 (0.031)
Train: 191 [ 150/1251 ( 12%)]  Loss: 3.635 (3.57)  Time: 0.801s, 1279.19/s  (0.810s, 1263.47/s)  LR: 7.724e-04  Data: 0.010 (0.024)
Train: 191 [ 200/1251 ( 16%)]  Loss: 3.426 (3.54)  Time: 0.787s, 1301.89/s  (0.808s, 1267.67/s)  LR: 7.724e-04  Data: 0.014 (0.021)
Train: 191 [ 250/1251 ( 20%)]  Loss: 3.527 (3.54)  Time: 0.862s, 1188.54/s  (0.806s, 1270.89/s)  LR: 7.724e-04  Data: 0.010 (0.019)
Train: 191 [ 300/1251 ( 24%)]  Loss: 3.467 (3.53)  Time: 0.817s, 1253.21/s  (0.804s, 1272.86/s)  LR: 7.724e-04  Data: 0.010 (0.018)
Train: 191 [ 350/1251 ( 28%)]  Loss: 3.525 (3.53)  Time: 0.797s, 1284.43/s  (0.803s, 1274.82/s)  LR: 7.724e-04  Data: 0.011 (0.017)
Train: 191 [ 400/1251 ( 32%)]  Loss: 3.631 (3.54)  Time: 0.829s, 1234.84/s  (0.803s, 1274.84/s)  LR: 7.724e-04  Data: 0.010 (0.016)
Train: 191 [ 450/1251 ( 36%)]  Loss: 3.483 (3.53)  Time: 0.782s, 1310.17/s  (0.803s, 1274.47/s)  LR: 7.724e-04  Data: 0.010 (0.016)
Train: 191 [ 500/1251 ( 40%)]  Loss: 4.025 (3.58)  Time: 0.834s, 1228.20/s  (0.804s, 1273.86/s)  LR: 7.724e-04  Data: 0.010 (0.015)
Train: 191 [ 550/1251 ( 44%)]  Loss: 3.176 (3.55)  Time: 0.787s, 1301.78/s  (0.804s, 1273.11/s)  LR: 7.724e-04  Data: 0.010 (0.015)
Train: 191 [ 600/1251 ( 48%)]  Loss: 3.335 (3.53)  Time: 0.859s, 1191.91/s  (0.804s, 1274.22/s)  LR: 7.724e-04  Data: 0.010 (0.015)
Train: 191 [ 650/1251 ( 52%)]  Loss: 3.630 (3.54)  Time: 0.845s, 1211.45/s  (0.803s, 1274.83/s)  LR: 7.724e-04  Data: 0.010 (0.014)
Train: 191 [ 700/1251 ( 56%)]  Loss: 3.605 (3.54)  Time: 0.815s, 1256.88/s  (0.803s, 1274.98/s)  LR: 7.724e-04  Data: 0.013 (0.014)
Train: 191 [ 750/1251 ( 60%)]  Loss: 3.843 (3.56)  Time: 0.826s, 1239.33/s  (0.804s, 1274.27/s)  LR: 7.724e-04  Data: 0.010 (0.014)
Train: 191 [ 800/1251 ( 64%)]  Loss: 3.359 (3.55)  Time: 0.805s, 1271.82/s  (0.804s, 1274.24/s)  LR: 7.724e-04  Data: 0.015 (0.014)
Train: 191 [ 850/1251 ( 68%)]  Loss: 3.780 (3.56)  Time: 0.816s, 1255.50/s  (0.803s, 1274.92/s)  LR: 7.724e-04  Data: 0.010 (0.014)
Train: 191 [ 900/1251 ( 72%)]  Loss: 3.966 (3.58)  Time: 0.782s, 1308.99/s  (0.803s, 1275.44/s)  LR: 7.724e-04  Data: 0.013 (0.013)
Train: 191 [ 950/1251 ( 76%)]  Loss: 3.536 (3.58)  Time: 0.819s, 1250.54/s  (0.803s, 1275.03/s)  LR: 7.724e-04  Data: 0.010 (0.013)
Train: 191 [1000/1251 ( 80%)]  Loss: 3.564 (3.58)  Time: 0.812s, 1261.80/s  (0.803s, 1275.43/s)  LR: 7.724e-04  Data: 0.010 (0.013)
Train: 191 [1050/1251 ( 84%)]  Loss: 3.869 (3.59)  Time: 0.773s, 1324.77/s  (0.803s, 1275.53/s)  LR: 7.724e-04  Data: 0.010 (0.013)
Train: 191 [1100/1251 ( 88%)]  Loss: 3.266 (3.58)  Time: 0.810s, 1264.13/s  (0.803s, 1275.22/s)  LR: 7.724e-04  Data: 0.015 (0.013)
Train: 191 [1150/1251 ( 92%)]  Loss: 3.798 (3.59)  Time: 0.819s, 1250.88/s  (0.803s, 1274.88/s)  LR: 7.724e-04  Data: 0.011 (0.013)
Train: 191 [1200/1251 ( 96%)]  Loss: 3.242 (3.57)  Time: 0.827s, 1238.52/s  (0.803s, 1274.94/s)  LR: 7.724e-04  Data: 0.010 (0.013)
Train: 191 [1250/1251 (100%)]  Loss: 3.930 (3.59)  Time: 0.758s, 1350.50/s  (0.803s, 1275.12/s)  LR: 7.724e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.712 (1.712)  Loss:  0.8101 (0.8101)  Acc@1: 87.3047 (87.3047)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.194 (0.604)  Loss:  0.8789 (1.3723)  Acc@1: 84.4340 (73.2640)  Acc@5: 96.1085 (91.7420)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-190.pth.tar', 73.70399999023438)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-186.pth.tar', 73.62000009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-180.pth.tar', 73.55800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-181.pth.tar', 73.43000009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-176.pth.tar', 73.40400009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-188.pth.tar', 73.37999995361328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-171.pth.tar', 73.36800006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-189.pth.tar', 73.36000001953126)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-191.pth.tar', 73.26400009033203)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-178.pth.tar', 73.24000006347656)

Train: 192 [   0/1251 (  0%)]  Loss: 3.480 (3.48)  Time: 2.910s,  351.89/s  (2.910s,  351.89/s)  LR: 7.702e-04  Data: 2.131 (2.131)
Train: 192 [  50/1251 (  4%)]  Loss: 3.391 (3.44)  Time: 0.818s, 1251.89/s  (0.854s, 1199.13/s)  LR: 7.702e-04  Data: 0.010 (0.065)
Train: 192 [ 100/1251 (  8%)]  Loss: 3.840 (3.57)  Time: 0.790s, 1296.08/s  (0.829s, 1235.04/s)  LR: 7.702e-04  Data: 0.015 (0.038)
Train: 192 [ 150/1251 ( 12%)]  Loss: 3.629 (3.59)  Time: 0.775s, 1320.87/s  (0.817s, 1252.81/s)  LR: 7.702e-04  Data: 0.012 (0.029)
Train: 192 [ 200/1251 ( 16%)]  Loss: 3.958 (3.66)  Time: 0.857s, 1195.38/s  (0.815s, 1256.96/s)  LR: 7.702e-04  Data: 0.010 (0.025)
Train: 192 [ 250/1251 ( 20%)]  Loss: 3.564 (3.64)  Time: 0.816s, 1255.29/s  (0.814s, 1258.03/s)  LR: 7.702e-04  Data: 0.011 (0.022)
Train: 192 [ 300/1251 ( 24%)]  Loss: 3.190 (3.58)  Time: 0.816s, 1255.65/s  (0.812s, 1260.95/s)  LR: 7.702e-04  Data: 0.010 (0.020)
Train: 192 [ 350/1251 ( 28%)]  Loss: 3.466 (3.56)  Time: 0.780s, 1313.24/s  (0.811s, 1263.25/s)  LR: 7.702e-04  Data: 0.010 (0.019)
Train: 192 [ 400/1251 ( 32%)]  Loss: 3.641 (3.57)  Time: 0.787s, 1301.66/s  (0.809s, 1265.58/s)  LR: 7.702e-04  Data: 0.010 (0.018)
Train: 192 [ 450/1251 ( 36%)]  Loss: 3.256 (3.54)  Time: 0.782s, 1308.81/s  (0.808s, 1267.05/s)  LR: 7.702e-04  Data: 0.010 (0.017)
Train: 192 [ 500/1251 ( 40%)]  Loss: 3.955 (3.58)  Time: 0.786s, 1303.24/s  (0.808s, 1267.50/s)  LR: 7.702e-04  Data: 0.014 (0.017)
Train: 192 [ 550/1251 ( 44%)]  Loss: 3.825 (3.60)  Time: 0.801s, 1278.75/s  (0.807s, 1268.52/s)  LR: 7.702e-04  Data: 0.010 (0.016)
Train: 192 [ 600/1251 ( 48%)]  Loss: 3.723 (3.61)  Time: 0.792s, 1293.28/s  (0.807s, 1269.46/s)  LR: 7.702e-04  Data: 0.014 (0.016)
Train: 192 [ 650/1251 ( 52%)]  Loss: 3.630 (3.61)  Time: 0.775s, 1321.36/s  (0.806s, 1270.55/s)  LR: 7.702e-04  Data: 0.010 (0.015)
Train: 192 [ 700/1251 ( 56%)]  Loss: 3.725 (3.62)  Time: 0.782s, 1310.26/s  (0.806s, 1270.59/s)  LR: 7.702e-04  Data: 0.010 (0.015)
Train: 192 [ 750/1251 ( 60%)]  Loss: 3.444 (3.61)  Time: 0.815s, 1256.53/s  (0.806s, 1270.62/s)  LR: 7.702e-04  Data: 0.014 (0.015)
Train: 192 [ 800/1251 ( 64%)]  Loss: 3.252 (3.59)  Time: 0.772s, 1326.34/s  (0.805s, 1271.60/s)  LR: 7.702e-04  Data: 0.011 (0.015)
Train: 192 [ 850/1251 ( 68%)]  Loss: 3.768 (3.60)  Time: 0.788s, 1299.06/s  (0.805s, 1271.99/s)  LR: 7.702e-04  Data: 0.010 (0.014)
Train: 192 [ 900/1251 ( 72%)]  Loss: 3.567 (3.59)  Time: 0.774s, 1322.20/s  (0.805s, 1272.65/s)  LR: 7.702e-04  Data: 0.010 (0.014)
Train: 192 [ 950/1251 ( 76%)]  Loss: 3.481 (3.59)  Time: 0.784s, 1306.10/s  (0.804s, 1272.95/s)  LR: 7.702e-04  Data: 0.010 (0.014)
Train: 192 [1000/1251 ( 80%)]  Loss: 3.810 (3.60)  Time: 0.814s, 1258.63/s  (0.804s, 1273.24/s)  LR: 7.702e-04  Data: 0.010 (0.014)
Train: 192 [1050/1251 ( 84%)]  Loss: 3.802 (3.61)  Time: 0.775s, 1321.87/s  (0.804s, 1273.01/s)  LR: 7.702e-04  Data: 0.010 (0.014)
Train: 192 [1100/1251 ( 88%)]  Loss: 3.666 (3.61)  Time: 0.775s, 1321.22/s  (0.804s, 1273.71/s)  LR: 7.702e-04  Data: 0.010 (0.014)
Train: 192 [1150/1251 ( 92%)]  Loss: 3.536 (3.61)  Time: 0.823s, 1244.25/s  (0.805s, 1271.95/s)  LR: 7.702e-04  Data: 0.013 (0.014)
Train: 192 [1200/1251 ( 96%)]  Loss: 3.318 (3.60)  Time: 0.775s, 1320.92/s  (0.805s, 1272.71/s)  LR: 7.702e-04  Data: 0.010 (0.014)
Train: 192 [1250/1251 (100%)]  Loss: 3.612 (3.60)  Time: 0.758s, 1351.31/s  (0.804s, 1274.42/s)  LR: 7.702e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.568 (1.568)  Loss:  0.9595 (0.9595)  Acc@1: 87.8906 (87.8906)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.194 (0.593)  Loss:  0.9697 (1.4256)  Acc@1: 83.3727 (73.3040)  Acc@5: 96.8160 (91.9380)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-190.pth.tar', 73.70399999023438)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-186.pth.tar', 73.62000009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-180.pth.tar', 73.55800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-181.pth.tar', 73.43000009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-176.pth.tar', 73.40400009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-188.pth.tar', 73.37999995361328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-171.pth.tar', 73.36800006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-189.pth.tar', 73.36000001953126)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-192.pth.tar', 73.30400014648437)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-191.pth.tar', 73.26400009033203)

Train: 193 [   0/1251 (  0%)]  Loss: 3.532 (3.53)  Time: 2.402s,  426.38/s  (2.402s,  426.38/s)  LR: 7.680e-04  Data: 1.613 (1.613)
Train: 193 [  50/1251 (  4%)]  Loss: 3.645 (3.59)  Time: 0.829s, 1234.84/s  (0.844s, 1213.18/s)  LR: 7.680e-04  Data: 0.010 (0.047)
Train: 193 [ 100/1251 (  8%)]  Loss: 3.437 (3.54)  Time: 0.810s, 1264.78/s  (0.821s, 1247.21/s)  LR: 7.680e-04  Data: 0.010 (0.029)
Train: 193 [ 150/1251 ( 12%)]  Loss: 3.515 (3.53)  Time: 0.798s, 1283.21/s  (0.813s, 1258.83/s)  LR: 7.680e-04  Data: 0.009 (0.023)
Train: 193 [ 200/1251 ( 16%)]  Loss: 3.624 (3.55)  Time: 0.783s, 1307.06/s  (0.811s, 1262.57/s)  LR: 7.680e-04  Data: 0.012 (0.020)
Train: 193 [ 250/1251 ( 20%)]  Loss: 3.458 (3.54)  Time: 0.800s, 1280.54/s  (0.809s, 1266.29/s)  LR: 7.680e-04  Data: 0.009 (0.018)
Train: 193 [ 300/1251 ( 24%)]  Loss: 3.632 (3.55)  Time: 0.834s, 1227.34/s  (0.807s, 1268.77/s)  LR: 7.680e-04  Data: 0.010 (0.017)
Train: 193 [ 350/1251 ( 28%)]  Loss: 3.812 (3.58)  Time: 0.783s, 1308.42/s  (0.806s, 1270.42/s)  LR: 7.680e-04  Data: 0.013 (0.016)
Train: 193 [ 400/1251 ( 32%)]  Loss: 4.100 (3.64)  Time: 0.847s, 1208.37/s  (0.806s, 1271.19/s)  LR: 7.680e-04  Data: 0.009 (0.015)
Train: 193 [ 450/1251 ( 36%)]  Loss: 3.488 (3.62)  Time: 0.815s, 1256.90/s  (0.804s, 1273.32/s)  LR: 7.680e-04  Data: 0.010 (0.015)
Train: 193 [ 500/1251 ( 40%)]  Loss: 4.057 (3.66)  Time: 0.773s, 1325.42/s  (0.804s, 1273.77/s)  LR: 7.680e-04  Data: 0.010 (0.014)
Train: 193 [ 550/1251 ( 44%)]  Loss: 3.741 (3.67)  Time: 0.809s, 1265.70/s  (0.804s, 1273.66/s)  LR: 7.680e-04  Data: 0.010 (0.014)
Train: 193 [ 600/1251 ( 48%)]  Loss: 3.657 (3.67)  Time: 0.792s, 1292.84/s  (0.804s, 1273.11/s)  LR: 7.680e-04  Data: 0.014 (0.014)
Train: 193 [ 650/1251 ( 52%)]  Loss: 3.471 (3.65)  Time: 0.845s, 1211.72/s  (0.804s, 1273.57/s)  LR: 7.680e-04  Data: 0.010 (0.014)
Train: 193 [ 700/1251 ( 56%)]  Loss: 3.628 (3.65)  Time: 0.793s, 1291.36/s  (0.804s, 1273.56/s)  LR: 7.680e-04  Data: 0.014 (0.014)
Train: 193 [ 750/1251 ( 60%)]  Loss: 4.020 (3.68)  Time: 0.853s, 1200.31/s  (0.804s, 1274.17/s)  LR: 7.680e-04  Data: 0.010 (0.013)
Train: 193 [ 800/1251 ( 64%)]  Loss: 3.462 (3.66)  Time: 0.774s, 1322.80/s  (0.803s, 1274.77/s)  LR: 7.680e-04  Data: 0.010 (0.013)
Train: 193 [ 850/1251 ( 68%)]  Loss: 3.562 (3.66)  Time: 0.831s, 1231.81/s  (0.804s, 1274.39/s)  LR: 7.680e-04  Data: 0.010 (0.013)
Train: 193 [ 900/1251 ( 72%)]  Loss: 4.093 (3.68)  Time: 0.772s, 1325.86/s  (0.803s, 1274.69/s)  LR: 7.680e-04  Data: 0.009 (0.013)
Train: 193 [ 950/1251 ( 76%)]  Loss: 3.828 (3.69)  Time: 0.783s, 1307.19/s  (0.803s, 1274.95/s)  LR: 7.680e-04  Data: 0.014 (0.013)
Train: 193 [1000/1251 ( 80%)]  Loss: 3.801 (3.69)  Time: 0.778s, 1316.16/s  (0.803s, 1275.31/s)  LR: 7.680e-04  Data: 0.010 (0.013)
Train: 193 [1050/1251 ( 84%)]  Loss: 3.822 (3.70)  Time: 0.773s, 1324.16/s  (0.803s, 1275.40/s)  LR: 7.680e-04  Data: 0.010 (0.013)
Train: 193 [1100/1251 ( 88%)]  Loss: 3.760 (3.70)  Time: 0.783s, 1307.17/s  (0.803s, 1275.28/s)  LR: 7.680e-04  Data: 0.011 (0.013)
Train: 193 [1150/1251 ( 92%)]  Loss: 3.897 (3.71)  Time: 0.774s, 1323.43/s  (0.803s, 1275.41/s)  LR: 7.680e-04  Data: 0.010 (0.013)
Train: 193 [1200/1251 ( 96%)]  Loss: 3.696 (3.71)  Time: 0.862s, 1187.99/s  (0.803s, 1275.59/s)  LR: 7.680e-04  Data: 0.010 (0.012)
Train: 193 [1250/1251 (100%)]  Loss: 3.547 (3.70)  Time: 0.784s, 1305.79/s  (0.803s, 1275.64/s)  LR: 7.680e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.545 (1.545)  Loss:  0.8604 (0.8604)  Acc@1: 88.5742 (88.5742)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.194 (0.557)  Loss:  1.0449 (1.4062)  Acc@1: 82.6651 (72.7860)  Acc@5: 95.4009 (91.4780)
Train: 194 [   0/1251 (  0%)]  Loss: 3.384 (3.38)  Time: 2.260s,  453.08/s  (2.260s,  453.08/s)  LR: 7.658e-04  Data: 1.530 (1.530)
Train: 194 [  50/1251 (  4%)]  Loss: 3.554 (3.47)  Time: 0.809s, 1265.67/s  (0.814s, 1258.30/s)  LR: 7.658e-04  Data: 0.010 (0.045)
Train: 194 [ 100/1251 (  8%)]  Loss: 3.205 (3.38)  Time: 0.775s, 1321.01/s  (0.795s, 1288.70/s)  LR: 7.658e-04  Data: 0.010 (0.028)
Train: 194 [ 150/1251 ( 12%)]  Loss: 3.401 (3.39)  Time: 0.772s, 1326.36/s  (0.789s, 1298.16/s)  LR: 7.658e-04  Data: 0.010 (0.022)
Train: 194 [ 200/1251 ( 16%)]  Loss: 3.842 (3.48)  Time: 0.773s, 1324.73/s  (0.788s, 1299.96/s)  LR: 7.658e-04  Data: 0.010 (0.019)
Train: 194 [ 250/1251 ( 20%)]  Loss: 3.578 (3.49)  Time: 0.773s, 1324.24/s  (0.787s, 1301.76/s)  LR: 7.658e-04  Data: 0.010 (0.017)
Train: 194 [ 300/1251 ( 24%)]  Loss: 3.271 (3.46)  Time: 0.774s, 1322.77/s  (0.796s, 1286.69/s)  LR: 7.658e-04  Data: 0.010 (0.016)
Train: 194 [ 350/1251 ( 28%)]  Loss: 3.745 (3.50)  Time: 0.772s, 1326.65/s  (0.797s, 1285.48/s)  LR: 7.658e-04  Data: 0.010 (0.015)
Train: 194 [ 400/1251 ( 32%)]  Loss: 3.624 (3.51)  Time: 0.775s, 1321.77/s  (0.795s, 1288.35/s)  LR: 7.658e-04  Data: 0.010 (0.015)
Train: 194 [ 450/1251 ( 36%)]  Loss: 3.551 (3.52)  Time: 0.776s, 1319.90/s  (0.796s, 1286.84/s)  LR: 7.658e-04  Data: 0.011 (0.014)
Train: 194 [ 500/1251 ( 40%)]  Loss: 3.573 (3.52)  Time: 0.775s, 1320.51/s  (0.794s, 1289.72/s)  LR: 7.658e-04  Data: 0.010 (0.014)
Train: 194 [ 550/1251 ( 44%)]  Loss: 3.592 (3.53)  Time: 0.816s, 1254.51/s  (0.793s, 1291.42/s)  LR: 7.658e-04  Data: 0.010 (0.014)
Train: 194 [ 600/1251 ( 48%)]  Loss: 3.281 (3.51)  Time: 0.809s, 1265.75/s  (0.792s, 1292.28/s)  LR: 7.658e-04  Data: 0.012 (0.013)
Train: 194 [ 650/1251 ( 52%)]  Loss: 3.058 (3.48)  Time: 0.774s, 1322.93/s  (0.791s, 1294.02/s)  LR: 7.658e-04  Data: 0.010 (0.013)
Train: 194 [ 700/1251 ( 56%)]  Loss: 3.818 (3.50)  Time: 0.834s, 1228.42/s  (0.791s, 1294.81/s)  LR: 7.658e-04  Data: 0.009 (0.013)
Train: 194 [ 750/1251 ( 60%)]  Loss: 3.747 (3.51)  Time: 0.771s, 1327.75/s  (0.791s, 1295.28/s)  LR: 7.658e-04  Data: 0.010 (0.013)
Train: 194 [ 800/1251 ( 64%)]  Loss: 3.765 (3.53)  Time: 0.778s, 1316.19/s  (0.790s, 1296.16/s)  LR: 7.658e-04  Data: 0.014 (0.012)
Train: 194 [ 850/1251 ( 68%)]  Loss: 3.935 (3.55)  Time: 0.774s, 1322.41/s  (0.790s, 1296.99/s)  LR: 7.658e-04  Data: 0.010 (0.012)
Train: 194 [ 900/1251 ( 72%)]  Loss: 3.738 (3.56)  Time: 0.771s, 1327.69/s  (0.789s, 1297.63/s)  LR: 7.658e-04  Data: 0.010 (0.012)
Train: 194 [ 950/1251 ( 76%)]  Loss: 3.642 (3.57)  Time: 0.773s, 1324.36/s  (0.788s, 1298.67/s)  LR: 7.658e-04  Data: 0.010 (0.012)
Train: 194 [1000/1251 ( 80%)]  Loss: 3.741 (3.57)  Time: 0.772s, 1326.50/s  (0.788s, 1299.50/s)  LR: 7.658e-04  Data: 0.010 (0.012)
Train: 194 [1050/1251 ( 84%)]  Loss: 3.759 (3.58)  Time: 0.775s, 1320.61/s  (0.788s, 1300.23/s)  LR: 7.658e-04  Data: 0.009 (0.012)
Train: 194 [1100/1251 ( 88%)]  Loss: 3.914 (3.60)  Time: 0.772s, 1326.23/s  (0.787s, 1300.82/s)  LR: 7.658e-04  Data: 0.010 (0.012)
Train: 194 [1150/1251 ( 92%)]  Loss: 3.557 (3.59)  Time: 0.772s, 1327.11/s  (0.787s, 1301.55/s)  LR: 7.658e-04  Data: 0.010 (0.012)
Train: 194 [1200/1251 ( 96%)]  Loss: 3.421 (3.59)  Time: 0.776s, 1320.03/s  (0.787s, 1301.90/s)  LR: 7.658e-04  Data: 0.011 (0.012)
Train: 194 [1250/1251 (100%)]  Loss: 3.594 (3.59)  Time: 0.759s, 1348.46/s  (0.786s, 1302.33/s)  LR: 7.658e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.534 (1.534)  Loss:  0.9248 (0.9248)  Acc@1: 88.9648 (88.9648)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.194 (0.562)  Loss:  0.9536 (1.4110)  Acc@1: 83.7264 (73.7260)  Acc@5: 95.5189 (92.0320)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-194.pth.tar', 73.72600004150391)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-190.pth.tar', 73.70399999023438)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-186.pth.tar', 73.62000009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-180.pth.tar', 73.55800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-181.pth.tar', 73.43000009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-176.pth.tar', 73.40400009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-188.pth.tar', 73.37999995361328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-171.pth.tar', 73.36800006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-189.pth.tar', 73.36000001953126)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-192.pth.tar', 73.30400014648437)

Train: 195 [   0/1251 (  0%)]  Loss: 3.638 (3.64)  Time: 2.245s,  456.05/s  (2.245s,  456.05/s)  LR: 7.636e-04  Data: 1.516 (1.516)
Train: 195 [  50/1251 (  4%)]  Loss: 3.622 (3.63)  Time: 0.785s, 1303.78/s  (0.824s, 1242.49/s)  LR: 7.636e-04  Data: 0.014 (0.045)
Train: 195 [ 100/1251 (  8%)]  Loss: 3.719 (3.66)  Time: 0.787s, 1301.55/s  (0.803s, 1275.88/s)  LR: 7.636e-04  Data: 0.010 (0.028)
Train: 195 [ 150/1251 ( 12%)]  Loss: 3.504 (3.62)  Time: 0.782s, 1309.85/s  (0.799s, 1281.73/s)  LR: 7.636e-04  Data: 0.010 (0.022)
Train: 195 [ 200/1251 ( 16%)]  Loss: 3.226 (3.54)  Time: 0.772s, 1325.99/s  (0.795s, 1288.27/s)  LR: 7.636e-04  Data: 0.010 (0.019)
Train: 195 [ 250/1251 ( 20%)]  Loss: 3.419 (3.52)  Time: 0.784s, 1306.45/s  (0.793s, 1291.97/s)  LR: 7.636e-04  Data: 0.010 (0.018)
Train: 195 [ 300/1251 ( 24%)]  Loss: 3.161 (3.47)  Time: 0.784s, 1306.15/s  (0.790s, 1295.54/s)  LR: 7.636e-04  Data: 0.010 (0.016)
Train: 195 [ 350/1251 ( 28%)]  Loss: 3.249 (3.44)  Time: 0.773s, 1325.19/s  (0.789s, 1297.77/s)  LR: 7.636e-04  Data: 0.010 (0.016)
Train: 195 [ 400/1251 ( 32%)]  Loss: 3.560 (3.46)  Time: 0.777s, 1318.70/s  (0.792s, 1293.43/s)  LR: 7.636e-04  Data: 0.010 (0.015)
Train: 195 [ 450/1251 ( 36%)]  Loss: 3.457 (3.46)  Time: 0.774s, 1322.60/s  (0.790s, 1295.70/s)  LR: 7.636e-04  Data: 0.010 (0.015)
Train: 195 [ 500/1251 ( 40%)]  Loss: 3.692 (3.48)  Time: 0.773s, 1324.81/s  (0.792s, 1293.16/s)  LR: 7.636e-04  Data: 0.010 (0.014)
Train: 195 [ 550/1251 ( 44%)]  Loss: 3.409 (3.47)  Time: 0.773s, 1325.50/s  (0.791s, 1294.14/s)  LR: 7.636e-04  Data: 0.010 (0.014)
Train: 195 [ 600/1251 ( 48%)]  Loss: 3.478 (3.47)  Time: 0.775s, 1321.13/s  (0.791s, 1294.47/s)  LR: 7.636e-04  Data: 0.010 (0.013)
Train: 195 [ 650/1251 ( 52%)]  Loss: 3.500 (3.47)  Time: 0.782s, 1309.36/s  (0.790s, 1295.70/s)  LR: 7.636e-04  Data: 0.010 (0.013)
Train: 195 [ 700/1251 ( 56%)]  Loss: 3.654 (3.49)  Time: 0.776s, 1320.01/s  (0.789s, 1297.39/s)  LR: 7.636e-04  Data: 0.011 (0.013)
Train: 195 [ 750/1251 ( 60%)]  Loss: 3.460 (3.48)  Time: 0.775s, 1321.10/s  (0.789s, 1297.84/s)  LR: 7.636e-04  Data: 0.010 (0.013)
Train: 195 [ 800/1251 ( 64%)]  Loss: 3.569 (3.49)  Time: 0.798s, 1282.78/s  (0.789s, 1298.56/s)  LR: 7.636e-04  Data: 0.010 (0.013)
Train: 195 [ 850/1251 ( 68%)]  Loss: 3.533 (3.49)  Time: 0.772s, 1326.00/s  (0.789s, 1298.55/s)  LR: 7.636e-04  Data: 0.010 (0.013)
Train: 195 [ 900/1251 ( 72%)]  Loss: 3.270 (3.48)  Time: 0.779s, 1313.68/s  (0.788s, 1299.06/s)  LR: 7.636e-04  Data: 0.010 (0.012)
Train: 195 [ 950/1251 ( 76%)]  Loss: 3.347 (3.47)  Time: 0.776s, 1319.65/s  (0.788s, 1299.56/s)  LR: 7.636e-04  Data: 0.010 (0.012)
Train: 195 [1000/1251 ( 80%)]  Loss: 3.623 (3.48)  Time: 0.769s, 1331.18/s  (0.787s, 1300.44/s)  LR: 7.636e-04  Data: 0.010 (0.012)
Train: 195 [1050/1251 ( 84%)]  Loss: 3.843 (3.50)  Time: 0.784s, 1306.55/s  (0.787s, 1301.05/s)  LR: 7.636e-04  Data: 0.010 (0.012)
Train: 195 [1100/1251 ( 88%)]  Loss: 3.814 (3.51)  Time: 0.778s, 1316.23/s  (0.788s, 1299.67/s)  LR: 7.636e-04  Data: 0.010 (0.012)
Train: 195 [1150/1251 ( 92%)]  Loss: 3.596 (3.51)  Time: 0.790s, 1295.90/s  (0.788s, 1299.85/s)  LR: 7.636e-04  Data: 0.010 (0.012)
Train: 195 [1200/1251 ( 96%)]  Loss: 3.812 (3.53)  Time: 0.794s, 1289.36/s  (0.788s, 1300.24/s)  LR: 7.636e-04  Data: 0.009 (0.012)
Train: 195 [1250/1251 (100%)]  Loss: 3.562 (3.53)  Time: 0.760s, 1348.06/s  (0.787s, 1300.52/s)  LR: 7.636e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.543 (1.543)  Loss:  0.8726 (0.8726)  Acc@1: 89.5508 (89.5508)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.194 (0.566)  Loss:  0.8608 (1.3878)  Acc@1: 84.7877 (73.2700)  Acc@5: 96.3443 (91.8180)
Train: 196 [   0/1251 (  0%)]  Loss: 3.933 (3.93)  Time: 2.323s,  440.75/s  (2.323s,  440.75/s)  LR: 7.614e-04  Data: 1.588 (1.588)
Train: 196 [  50/1251 (  4%)]  Loss: 3.543 (3.74)  Time: 0.801s, 1278.92/s  (0.815s, 1256.82/s)  LR: 7.614e-04  Data: 0.009 (0.043)
Train: 196 [ 100/1251 (  8%)]  Loss: 3.618 (3.70)  Time: 0.815s, 1256.82/s  (0.800s, 1279.32/s)  LR: 7.614e-04  Data: 0.011 (0.027)
Train: 196 [ 150/1251 ( 12%)]  Loss: 3.701 (3.70)  Time: 0.772s, 1326.36/s  (0.795s, 1288.32/s)  LR: 7.614e-04  Data: 0.010 (0.021)
Train: 196 [ 200/1251 ( 16%)]  Loss: 3.770 (3.71)  Time: 0.814s, 1258.25/s  (0.795s, 1288.68/s)  LR: 7.614e-04  Data: 0.010 (0.018)
Train: 196 [ 250/1251 ( 20%)]  Loss: 3.390 (3.66)  Time: 0.773s, 1325.06/s  (0.796s, 1287.03/s)  LR: 7.614e-04  Data: 0.009 (0.017)
Train: 196 [ 300/1251 ( 24%)]  Loss: 3.641 (3.66)  Time: 0.773s, 1324.80/s  (0.794s, 1290.36/s)  LR: 7.614e-04  Data: 0.011 (0.016)
Train: 196 [ 350/1251 ( 28%)]  Loss: 3.383 (3.62)  Time: 0.776s, 1320.34/s  (0.793s, 1291.14/s)  LR: 7.614e-04  Data: 0.010 (0.015)
Train: 196 [ 400/1251 ( 32%)]  Loss: 3.630 (3.62)  Time: 0.773s, 1324.00/s  (0.791s, 1294.22/s)  LR: 7.614e-04  Data: 0.010 (0.014)
Train: 196 [ 450/1251 ( 36%)]  Loss: 3.477 (3.61)  Time: 0.773s, 1324.82/s  (0.790s, 1296.63/s)  LR: 7.614e-04  Data: 0.011 (0.014)
Train: 196 [ 500/1251 ( 40%)]  Loss: 3.506 (3.60)  Time: 0.774s, 1323.55/s  (0.790s, 1296.67/s)  LR: 7.614e-04  Data: 0.009 (0.014)
Train: 196 [ 550/1251 ( 44%)]  Loss: 3.826 (3.62)  Time: 0.771s, 1327.44/s  (0.789s, 1297.52/s)  LR: 7.614e-04  Data: 0.009 (0.013)
Train: 196 [ 600/1251 ( 48%)]  Loss: 3.661 (3.62)  Time: 0.772s, 1326.00/s  (0.788s, 1298.76/s)  LR: 7.614e-04  Data: 0.010 (0.013)
Train: 196 [ 650/1251 ( 52%)]  Loss: 3.624 (3.62)  Time: 0.775s, 1321.90/s  (0.788s, 1299.95/s)  LR: 7.614e-04  Data: 0.009 (0.013)
Train: 196 [ 700/1251 ( 56%)]  Loss: 3.393 (3.61)  Time: 0.772s, 1326.74/s  (0.787s, 1300.51/s)  LR: 7.614e-04  Data: 0.010 (0.013)
Train: 196 [ 750/1251 ( 60%)]  Loss: 3.639 (3.61)  Time: 0.776s, 1319.46/s  (0.787s, 1300.36/s)  LR: 7.614e-04  Data: 0.011 (0.012)
Train: 196 [ 800/1251 ( 64%)]  Loss: 3.734 (3.62)  Time: 0.775s, 1321.23/s  (0.787s, 1301.50/s)  LR: 7.614e-04  Data: 0.010 (0.012)
Train: 196 [ 850/1251 ( 68%)]  Loss: 4.011 (3.64)  Time: 0.780s, 1312.86/s  (0.786s, 1302.25/s)  LR: 7.614e-04  Data: 0.011 (0.012)
Train: 196 [ 900/1251 ( 72%)]  Loss: 3.484 (3.63)  Time: 0.784s, 1305.54/s  (0.786s, 1302.62/s)  LR: 7.614e-04  Data: 0.010 (0.012)
Train: 196 [ 950/1251 ( 76%)]  Loss: 3.764 (3.64)  Time: 0.773s, 1325.05/s  (0.787s, 1301.29/s)  LR: 7.614e-04  Data: 0.010 (0.012)
Train: 196 [1000/1251 ( 80%)]  Loss: 3.382 (3.62)  Time: 0.775s, 1321.02/s  (0.787s, 1301.89/s)  LR: 7.614e-04  Data: 0.009 (0.012)
Train: 196 [1050/1251 ( 84%)]  Loss: 3.536 (3.62)  Time: 0.774s, 1322.19/s  (0.786s, 1302.33/s)  LR: 7.614e-04  Data: 0.009 (0.012)
Train: 196 [1100/1251 ( 88%)]  Loss: 3.363 (3.61)  Time: 0.775s, 1321.53/s  (0.786s, 1302.69/s)  LR: 7.614e-04  Data: 0.010 (0.012)
Train: 196 [1150/1251 ( 92%)]  Loss: 3.637 (3.61)  Time: 0.815s, 1256.80/s  (0.787s, 1301.17/s)  LR: 7.614e-04  Data: 0.010 (0.012)
Train: 196 [1200/1251 ( 96%)]  Loss: 3.804 (3.62)  Time: 0.781s, 1311.27/s  (0.787s, 1300.73/s)  LR: 7.614e-04  Data: 0.009 (0.011)
Train: 196 [1250/1251 (100%)]  Loss: 3.677 (3.62)  Time: 0.757s, 1352.04/s  (0.787s, 1301.29/s)  LR: 7.614e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.539 (1.539)  Loss:  0.8057 (0.8057)  Acc@1: 89.7461 (89.7461)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.194 (0.568)  Loss:  0.9780 (1.4436)  Acc@1: 85.0236 (73.5380)  Acc@5: 95.8726 (91.7680)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-194.pth.tar', 73.72600004150391)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-190.pth.tar', 73.70399999023438)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-186.pth.tar', 73.62000009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-180.pth.tar', 73.55800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-196.pth.tar', 73.53799995849609)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-181.pth.tar', 73.43000009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-176.pth.tar', 73.40400009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-188.pth.tar', 73.37999995361328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-171.pth.tar', 73.36800006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-189.pth.tar', 73.36000001953126)

Train: 197 [   0/1251 (  0%)]  Loss: 3.556 (3.56)  Time: 2.164s,  473.10/s  (2.164s,  473.10/s)  LR: 7.592e-04  Data: 1.436 (1.436)
Train: 197 [  50/1251 (  4%)]  Loss: 3.741 (3.65)  Time: 0.774s, 1322.55/s  (0.813s, 1259.31/s)  LR: 7.592e-04  Data: 0.009 (0.043)
Train: 197 [ 100/1251 (  8%)]  Loss: 3.689 (3.66)  Time: 0.773s, 1325.43/s  (0.803s, 1274.56/s)  LR: 7.592e-04  Data: 0.010 (0.027)
Train: 197 [ 150/1251 ( 12%)]  Loss: 3.484 (3.62)  Time: 0.778s, 1315.73/s  (0.797s, 1285.55/s)  LR: 7.592e-04  Data: 0.010 (0.021)
Train: 197 [ 200/1251 ( 16%)]  Loss: 3.335 (3.56)  Time: 0.771s, 1327.61/s  (0.793s, 1292.07/s)  LR: 7.592e-04  Data: 0.010 (0.018)
Train: 197 [ 250/1251 ( 20%)]  Loss: 3.624 (3.57)  Time: 0.778s, 1315.63/s  (0.791s, 1294.89/s)  LR: 7.592e-04  Data: 0.010 (0.017)
Train: 197 [ 300/1251 ( 24%)]  Loss: 3.390 (3.55)  Time: 0.777s, 1318.54/s  (0.789s, 1298.24/s)  LR: 7.592e-04  Data: 0.010 (0.016)
Train: 197 [ 350/1251 ( 28%)]  Loss: 3.345 (3.52)  Time: 0.790s, 1296.39/s  (0.787s, 1301.01/s)  LR: 7.592e-04  Data: 0.009 (0.015)
Train: 197 [ 400/1251 ( 32%)]  Loss: 3.600 (3.53)  Time: 0.776s, 1319.58/s  (0.786s, 1302.83/s)  LR: 7.592e-04  Data: 0.010 (0.014)
Train: 197 [ 450/1251 ( 36%)]  Loss: 3.522 (3.53)  Time: 0.821s, 1247.04/s  (0.786s, 1302.29/s)  LR: 7.592e-04  Data: 0.010 (0.014)
Train: 197 [ 500/1251 ( 40%)]  Loss: 3.476 (3.52)  Time: 0.844s, 1212.58/s  (0.786s, 1302.99/s)  LR: 7.592e-04  Data: 0.009 (0.013)
Train: 197 [ 550/1251 ( 44%)]  Loss: 3.531 (3.52)  Time: 0.773s, 1325.04/s  (0.786s, 1302.56/s)  LR: 7.592e-04  Data: 0.011 (0.013)
Train: 197 [ 600/1251 ( 48%)]  Loss: 3.531 (3.52)  Time: 0.777s, 1317.54/s  (0.786s, 1302.91/s)  LR: 7.592e-04  Data: 0.011 (0.013)
Train: 197 [ 650/1251 ( 52%)]  Loss: 3.618 (3.53)  Time: 0.831s, 1231.56/s  (0.788s, 1299.47/s)  LR: 7.592e-04  Data: 0.009 (0.013)
Train: 197 [ 700/1251 ( 56%)]  Loss: 3.889 (3.56)  Time: 0.784s, 1306.87/s  (0.788s, 1299.25/s)  LR: 7.592e-04  Data: 0.010 (0.012)
Train: 197 [ 750/1251 ( 60%)]  Loss: 3.577 (3.56)  Time: 0.796s, 1286.10/s  (0.789s, 1298.65/s)  LR: 7.592e-04  Data: 0.013 (0.012)
Train: 197 [ 800/1251 ( 64%)]  Loss: 3.757 (3.57)  Time: 0.778s, 1315.74/s  (0.789s, 1297.92/s)  LR: 7.592e-04  Data: 0.009 (0.012)
Train: 197 [ 850/1251 ( 68%)]  Loss: 3.861 (3.58)  Time: 0.783s, 1307.40/s  (0.789s, 1298.12/s)  LR: 7.592e-04  Data: 0.010 (0.012)
Train: 197 [ 900/1251 ( 72%)]  Loss: 3.497 (3.58)  Time: 0.779s, 1314.39/s  (0.789s, 1298.50/s)  LR: 7.592e-04  Data: 0.011 (0.012)
Train: 197 [ 950/1251 ( 76%)]  Loss: 3.576 (3.58)  Time: 0.774s, 1322.90/s  (0.788s, 1299.18/s)  LR: 7.592e-04  Data: 0.012 (0.012)
Train: 197 [1000/1251 ( 80%)]  Loss: 3.596 (3.58)  Time: 0.782s, 1310.05/s  (0.788s, 1299.91/s)  LR: 7.592e-04  Data: 0.009 (0.012)
Train: 197 [1050/1251 ( 84%)]  Loss: 3.682 (3.59)  Time: 0.783s, 1307.09/s  (0.788s, 1300.29/s)  LR: 7.592e-04  Data: 0.013 (0.012)
Train: 197 [1100/1251 ( 88%)]  Loss: 3.316 (3.57)  Time: 0.772s, 1326.20/s  (0.787s, 1300.81/s)  LR: 7.592e-04  Data: 0.010 (0.012)
Train: 197 [1150/1251 ( 92%)]  Loss: 3.716 (3.58)  Time: 0.802s, 1276.24/s  (0.787s, 1300.81/s)  LR: 7.592e-04  Data: 0.010 (0.012)
Train: 197 [1200/1251 ( 96%)]  Loss: 3.940 (3.59)  Time: 0.863s, 1186.59/s  (0.787s, 1300.77/s)  LR: 7.592e-04  Data: 0.010 (0.011)
Train: 197 [1250/1251 (100%)]  Loss: 3.769 (3.60)  Time: 0.761s, 1345.42/s  (0.787s, 1301.34/s)  LR: 7.592e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.545 (1.545)  Loss:  0.7202 (0.7202)  Acc@1: 88.4766 (88.4766)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.194 (0.583)  Loss:  0.8555 (1.3325)  Acc@1: 85.1415 (73.6980)  Acc@5: 96.4623 (92.0780)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-194.pth.tar', 73.72600004150391)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-190.pth.tar', 73.70399999023438)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-197.pth.tar', 73.69800000976562)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-186.pth.tar', 73.62000009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-180.pth.tar', 73.55800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-196.pth.tar', 73.53799995849609)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-181.pth.tar', 73.43000009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-176.pth.tar', 73.40400009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-188.pth.tar', 73.37999995361328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-171.pth.tar', 73.36800006103516)

Train: 198 [   0/1251 (  0%)]  Loss: 3.414 (3.41)  Time: 2.372s,  431.79/s  (2.372s,  431.79/s)  LR: 7.570e-04  Data: 1.589 (1.589)
Train: 198 [  50/1251 (  4%)]  Loss: 3.732 (3.57)  Time: 0.830s, 1234.07/s  (0.841s, 1217.87/s)  LR: 7.570e-04  Data: 0.010 (0.042)
Train: 198 [ 100/1251 (  8%)]  Loss: 3.615 (3.59)  Time: 0.798s, 1283.84/s  (0.817s, 1253.00/s)  LR: 7.570e-04  Data: 0.010 (0.026)
Train: 198 [ 150/1251 ( 12%)]  Loss: 3.697 (3.61)  Time: 0.775s, 1321.13/s  (0.806s, 1270.14/s)  LR: 7.570e-04  Data: 0.010 (0.021)
Train: 198 [ 200/1251 ( 16%)]  Loss: 3.696 (3.63)  Time: 0.776s, 1320.12/s  (0.800s, 1279.65/s)  LR: 7.570e-04  Data: 0.010 (0.018)
Train: 198 [ 250/1251 ( 20%)]  Loss: 3.746 (3.65)  Time: 0.833s, 1229.40/s  (0.800s, 1280.33/s)  LR: 7.570e-04  Data: 0.010 (0.017)
Train: 198 [ 300/1251 ( 24%)]  Loss: 3.900 (3.69)  Time: 0.782s, 1309.13/s  (0.798s, 1283.92/s)  LR: 7.570e-04  Data: 0.011 (0.016)
Train: 198 [ 350/1251 ( 28%)]  Loss: 3.724 (3.69)  Time: 0.780s, 1312.44/s  (0.796s, 1287.24/s)  LR: 7.570e-04  Data: 0.010 (0.015)
Train: 198 [ 400/1251 ( 32%)]  Loss: 3.516 (3.67)  Time: 0.771s, 1328.42/s  (0.794s, 1289.60/s)  LR: 7.570e-04  Data: 0.010 (0.014)
Train: 198 [ 450/1251 ( 36%)]  Loss: 3.552 (3.66)  Time: 0.806s, 1269.87/s  (0.795s, 1288.60/s)  LR: 7.570e-04  Data: 0.009 (0.014)
Train: 198 [ 500/1251 ( 40%)]  Loss: 3.694 (3.66)  Time: 0.777s, 1317.85/s  (0.794s, 1289.45/s)  LR: 7.570e-04  Data: 0.011 (0.014)
Train: 198 [ 550/1251 ( 44%)]  Loss: 3.494 (3.65)  Time: 0.783s, 1308.62/s  (0.793s, 1290.58/s)  LR: 7.570e-04  Data: 0.009 (0.013)
Train: 198 [ 600/1251 ( 48%)]  Loss: 3.912 (3.67)  Time: 0.772s, 1327.27/s  (0.792s, 1292.56/s)  LR: 7.570e-04  Data: 0.010 (0.013)
Train: 198 [ 650/1251 ( 52%)]  Loss: 3.715 (3.67)  Time: 0.771s, 1327.87/s  (0.793s, 1291.95/s)  LR: 7.570e-04  Data: 0.010 (0.013)
Train: 198 [ 700/1251 ( 56%)]  Loss: 3.512 (3.66)  Time: 0.772s, 1326.35/s  (0.792s, 1292.70/s)  LR: 7.570e-04  Data: 0.009 (0.013)
Train: 198 [ 750/1251 ( 60%)]  Loss: 3.798 (3.67)  Time: 0.776s, 1319.93/s  (0.792s, 1293.41/s)  LR: 7.570e-04  Data: 0.010 (0.012)
Train: 198 [ 800/1251 ( 64%)]  Loss: 3.494 (3.66)  Time: 0.773s, 1324.96/s  (0.793s, 1291.14/s)  LR: 7.570e-04  Data: 0.010 (0.012)
Train: 198 [ 850/1251 ( 68%)]  Loss: 3.805 (3.67)  Time: 0.781s, 1311.55/s  (0.792s, 1292.76/s)  LR: 7.570e-04  Data: 0.009 (0.012)
Train: 198 [ 900/1251 ( 72%)]  Loss: 3.478 (3.66)  Time: 0.776s, 1319.64/s  (0.792s, 1292.66/s)  LR: 7.570e-04  Data: 0.010 (0.012)
Train: 198 [ 950/1251 ( 76%)]  Loss: 3.412 (3.65)  Time: 0.775s, 1322.08/s  (0.791s, 1293.79/s)  LR: 7.570e-04  Data: 0.010 (0.012)
Train: 198 [1000/1251 ( 80%)]  Loss: 3.446 (3.64)  Time: 0.774s, 1322.55/s  (0.792s, 1293.01/s)  LR: 7.570e-04  Data: 0.010 (0.012)
Train: 198 [1050/1251 ( 84%)]  Loss: 3.214 (3.62)  Time: 0.774s, 1323.03/s  (0.792s, 1293.65/s)  LR: 7.570e-04  Data: 0.009 (0.012)
Train: 198 [1100/1251 ( 88%)]  Loss: 3.601 (3.62)  Time: 0.773s, 1324.72/s  (0.792s, 1293.17/s)  LR: 7.570e-04  Data: 0.010 (0.012)
Train: 198 [1150/1251 ( 92%)]  Loss: 3.686 (3.62)  Time: 0.773s, 1325.04/s  (0.792s, 1293.69/s)  LR: 7.570e-04  Data: 0.010 (0.012)
Train: 198 [1200/1251 ( 96%)]  Loss: 3.623 (3.62)  Time: 0.772s, 1326.00/s  (0.791s, 1294.57/s)  LR: 7.570e-04  Data: 0.010 (0.012)
Train: 198 [1250/1251 (100%)]  Loss: 3.817 (3.63)  Time: 0.759s, 1348.63/s  (0.791s, 1295.23/s)  LR: 7.570e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.590 (1.590)  Loss:  0.8755 (0.8755)  Acc@1: 88.1836 (88.1836)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.194 (0.567)  Loss:  0.9365 (1.3884)  Acc@1: 84.0802 (73.4260)  Acc@5: 95.5189 (91.7560)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-194.pth.tar', 73.72600004150391)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-190.pth.tar', 73.70399999023438)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-197.pth.tar', 73.69800000976562)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-186.pth.tar', 73.62000009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-180.pth.tar', 73.55800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-196.pth.tar', 73.53799995849609)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-181.pth.tar', 73.43000009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-198.pth.tar', 73.42600006591798)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-176.pth.tar', 73.40400009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-188.pth.tar', 73.37999995361328)

Train: 199 [   0/1251 (  0%)]  Loss: 3.685 (3.68)  Time: 2.245s,  456.20/s  (2.245s,  456.20/s)  LR: 7.547e-04  Data: 1.508 (1.508)
Train: 199 [  50/1251 (  4%)]  Loss: 3.615 (3.65)  Time: 0.776s, 1319.67/s  (0.810s, 1263.59/s)  LR: 7.547e-04  Data: 0.009 (0.042)
Train: 199 [ 100/1251 (  8%)]  Loss: 3.744 (3.68)  Time: 0.777s, 1317.25/s  (0.796s, 1286.83/s)  LR: 7.547e-04  Data: 0.010 (0.026)
Train: 199 [ 150/1251 ( 12%)]  Loss: 3.644 (3.67)  Time: 0.904s, 1132.20/s  (0.792s, 1293.43/s)  LR: 7.547e-04  Data: 0.010 (0.021)
Train: 199 [ 200/1251 ( 16%)]  Loss: 3.798 (3.70)  Time: 0.776s, 1318.98/s  (0.790s, 1296.16/s)  LR: 7.547e-04  Data: 0.010 (0.018)
Train: 199 [ 250/1251 ( 20%)]  Loss: 3.764 (3.71)  Time: 0.772s, 1325.70/s  (0.789s, 1297.35/s)  LR: 7.547e-04  Data: 0.010 (0.016)
Train: 199 [ 300/1251 ( 24%)]  Loss: 3.625 (3.70)  Time: 0.815s, 1256.46/s  (0.790s, 1295.81/s)  LR: 7.547e-04  Data: 0.011 (0.016)
Train: 199 [ 350/1251 ( 28%)]  Loss: 3.689 (3.70)  Time: 0.772s, 1326.76/s  (0.791s, 1293.83/s)  LR: 7.547e-04  Data: 0.010 (0.015)
Train: 199 [ 400/1251 ( 32%)]  Loss: 3.586 (3.68)  Time: 0.781s, 1310.91/s  (0.790s, 1295.94/s)  LR: 7.547e-04  Data: 0.011 (0.014)
Train: 199 [ 450/1251 ( 36%)]  Loss: 3.923 (3.71)  Time: 0.776s, 1318.98/s  (0.789s, 1297.46/s)  LR: 7.547e-04  Data: 0.011 (0.014)
Train: 199 [ 500/1251 ( 40%)]  Loss: 3.714 (3.71)  Time: 0.776s, 1320.31/s  (0.789s, 1298.02/s)  LR: 7.547e-04  Data: 0.010 (0.014)
Train: 199 [ 550/1251 ( 44%)]  Loss: 3.313 (3.68)  Time: 0.774s, 1323.75/s  (0.788s, 1299.19/s)  LR: 7.547e-04  Data: 0.010 (0.013)
Train: 199 [ 600/1251 ( 48%)]  Loss: 3.661 (3.67)  Time: 0.774s, 1322.67/s  (0.787s, 1300.39/s)  LR: 7.547e-04  Data: 0.009 (0.013)
Train: 199 [ 650/1251 ( 52%)]  Loss: 3.348 (3.65)  Time: 0.786s, 1303.25/s  (0.787s, 1300.88/s)  LR: 7.547e-04  Data: 0.010 (0.013)
Train: 199 [ 700/1251 ( 56%)]  Loss: 3.423 (3.64)  Time: 0.774s, 1323.48/s  (0.787s, 1301.75/s)  LR: 7.547e-04  Data: 0.010 (0.013)
Train: 199 [ 750/1251 ( 60%)]  Loss: 3.860 (3.65)  Time: 0.789s, 1297.12/s  (0.787s, 1301.82/s)  LR: 7.547e-04  Data: 0.009 (0.012)
Train: 199 [ 800/1251 ( 64%)]  Loss: 3.747 (3.66)  Time: 0.775s, 1321.34/s  (0.786s, 1302.82/s)  LR: 7.547e-04  Data: 0.009 (0.012)
Train: 199 [ 850/1251 ( 68%)]  Loss: 3.905 (3.67)  Time: 0.779s, 1314.30/s  (0.786s, 1303.48/s)  LR: 7.547e-04  Data: 0.009 (0.012)
Train: 199 [ 900/1251 ( 72%)]  Loss: 3.797 (3.68)  Time: 0.773s, 1324.16/s  (0.786s, 1303.45/s)  LR: 7.547e-04  Data: 0.010 (0.012)
Train: 199 [ 950/1251 ( 76%)]  Loss: 3.370 (3.66)  Time: 0.826s, 1238.96/s  (0.786s, 1303.37/s)  LR: 7.547e-04  Data: 0.013 (0.012)
Train: 199 [1000/1251 ( 80%)]  Loss: 3.709 (3.66)  Time: 0.778s, 1316.59/s  (0.786s, 1303.01/s)  LR: 7.547e-04  Data: 0.009 (0.012)
Train: 199 [1050/1251 ( 84%)]  Loss: 3.868 (3.67)  Time: 0.774s, 1323.02/s  (0.786s, 1303.54/s)  LR: 7.547e-04  Data: 0.010 (0.012)
Train: 199 [1100/1251 ( 88%)]  Loss: 3.773 (3.68)  Time: 0.830s, 1233.56/s  (0.786s, 1302.07/s)  LR: 7.547e-04  Data: 0.013 (0.012)
Train: 199 [1150/1251 ( 92%)]  Loss: 3.303 (3.66)  Time: 0.773s, 1325.47/s  (0.786s, 1302.37/s)  LR: 7.547e-04  Data: 0.009 (0.012)
Train: 199 [1200/1251 ( 96%)]  Loss: 3.401 (3.65)  Time: 0.807s, 1268.12/s  (0.786s, 1302.53/s)  LR: 7.547e-04  Data: 0.009 (0.011)
Train: 199 [1250/1251 (100%)]  Loss: 3.501 (3.64)  Time: 0.760s, 1347.76/s  (0.786s, 1302.94/s)  LR: 7.547e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.543 (1.543)  Loss:  0.7256 (0.7256)  Acc@1: 88.9648 (88.9648)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.194 (0.566)  Loss:  0.7490 (1.2759)  Acc@1: 85.4953 (73.5840)  Acc@5: 96.6981 (91.9080)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-194.pth.tar', 73.72600004150391)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-190.pth.tar', 73.70399999023438)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-197.pth.tar', 73.69800000976562)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-186.pth.tar', 73.62000009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-199.pth.tar', 73.58400016357422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-180.pth.tar', 73.55800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-196.pth.tar', 73.53799995849609)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-181.pth.tar', 73.43000009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-198.pth.tar', 73.42600006591798)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-176.pth.tar', 73.40400009277344)

Train: 200 [   0/1251 (  0%)]  Loss: 3.712 (3.71)  Time: 2.528s,  405.13/s  (2.528s,  405.13/s)  LR: 7.525e-04  Data: 1.798 (1.798)
Train: 200 [  50/1251 (  4%)]  Loss: 3.276 (3.49)  Time: 0.847s, 1208.42/s  (0.829s, 1235.25/s)  LR: 7.525e-04  Data: 0.010 (0.046)
Train: 200 [ 100/1251 (  8%)]  Loss: 3.765 (3.58)  Time: 0.772s, 1326.64/s  (0.811s, 1263.25/s)  LR: 7.525e-04  Data: 0.010 (0.028)
Train: 200 [ 150/1251 ( 12%)]  Loss: 3.854 (3.65)  Time: 0.775s, 1321.85/s  (0.802s, 1276.37/s)  LR: 7.525e-04  Data: 0.009 (0.022)
Train: 200 [ 200/1251 ( 16%)]  Loss: 3.483 (3.62)  Time: 0.777s, 1317.63/s  (0.797s, 1284.25/s)  LR: 7.525e-04  Data: 0.010 (0.019)
Train: 200 [ 250/1251 ( 20%)]  Loss: 3.618 (3.62)  Time: 0.773s, 1324.43/s  (0.795s, 1288.50/s)  LR: 7.525e-04  Data: 0.010 (0.017)
Train: 200 [ 300/1251 ( 24%)]  Loss: 3.653 (3.62)  Time: 0.772s, 1326.57/s  (0.793s, 1291.72/s)  LR: 7.525e-04  Data: 0.010 (0.016)
Train: 200 [ 350/1251 ( 28%)]  Loss: 3.459 (3.60)  Time: 0.774s, 1322.87/s  (0.792s, 1293.24/s)  LR: 7.525e-04  Data: 0.011 (0.015)
Train: 200 [ 400/1251 ( 32%)]  Loss: 3.795 (3.62)  Time: 0.777s, 1318.59/s  (0.791s, 1294.29/s)  LR: 7.525e-04  Data: 0.009 (0.015)
Train: 200 [ 450/1251 ( 36%)]  Loss: 3.685 (3.63)  Time: 0.832s, 1230.21/s  (0.791s, 1294.08/s)  LR: 7.525e-04  Data: 0.009 (0.014)
Train: 200 [ 500/1251 ( 40%)]  Loss: 3.586 (3.63)  Time: 0.774s, 1323.33/s  (0.790s, 1296.13/s)  LR: 7.525e-04  Data: 0.011 (0.014)
Train: 200 [ 550/1251 ( 44%)]  Loss: 3.213 (3.59)  Time: 0.772s, 1326.33/s  (0.789s, 1297.28/s)  LR: 7.525e-04  Data: 0.010 (0.013)
Train: 200 [ 600/1251 ( 48%)]  Loss: 3.837 (3.61)  Time: 0.909s, 1126.57/s  (0.789s, 1298.40/s)  LR: 7.525e-04  Data: 0.009 (0.013)
Train: 200 [ 650/1251 ( 52%)]  Loss: 3.660 (3.61)  Time: 0.774s, 1323.75/s  (0.788s, 1298.99/s)  LR: 7.525e-04  Data: 0.010 (0.013)
Train: 200 [ 700/1251 ( 56%)]  Loss: 3.621 (3.61)  Time: 0.808s, 1266.89/s  (0.789s, 1298.25/s)  LR: 7.525e-04  Data: 0.009 (0.013)
Train: 200 [ 750/1251 ( 60%)]  Loss: 3.638 (3.62)  Time: 0.814s, 1258.22/s  (0.788s, 1298.98/s)  LR: 7.525e-04  Data: 0.010 (0.012)
Train: 200 [ 800/1251 ( 64%)]  Loss: 3.816 (3.63)  Time: 0.775s, 1321.79/s  (0.788s, 1299.29/s)  LR: 7.525e-04  Data: 0.009 (0.012)
Train: 200 [ 850/1251 ( 68%)]  Loss: 3.635 (3.63)  Time: 0.772s, 1325.78/s  (0.788s, 1300.02/s)  LR: 7.525e-04  Data: 0.009 (0.012)
Train: 200 [ 900/1251 ( 72%)]  Loss: 3.663 (3.63)  Time: 0.775s, 1321.14/s  (0.788s, 1300.18/s)  LR: 7.525e-04  Data: 0.010 (0.012)
Train: 200 [ 950/1251 ( 76%)]  Loss: 3.390 (3.62)  Time: 0.838s, 1222.10/s  (0.787s, 1300.82/s)  LR: 7.525e-04  Data: 0.010 (0.012)
Train: 200 [1000/1251 ( 80%)]  Loss: 3.875 (3.63)  Time: 0.780s, 1313.64/s  (0.787s, 1301.14/s)  LR: 7.525e-04  Data: 0.010 (0.012)
Train: 200 [1050/1251 ( 84%)]  Loss: 3.436 (3.62)  Time: 0.785s, 1304.82/s  (0.787s, 1301.82/s)  LR: 7.525e-04  Data: 0.010 (0.012)
Train: 200 [1100/1251 ( 88%)]  Loss: 3.325 (3.61)  Time: 0.772s, 1325.64/s  (0.786s, 1302.20/s)  LR: 7.525e-04  Data: 0.010 (0.012)
Train: 200 [1150/1251 ( 92%)]  Loss: 3.627 (3.61)  Time: 0.773s, 1325.38/s  (0.786s, 1302.07/s)  LR: 7.525e-04  Data: 0.010 (0.012)
Train: 200 [1200/1251 ( 96%)]  Loss: 3.583 (3.61)  Time: 0.776s, 1319.68/s  (0.786s, 1302.45/s)  LR: 7.525e-04  Data: 0.010 (0.012)
Train: 200 [1250/1251 (100%)]  Loss: 3.346 (3.60)  Time: 0.759s, 1349.52/s  (0.786s, 1302.74/s)  LR: 7.525e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.506 (1.506)  Loss:  0.7876 (0.7876)  Acc@1: 88.6719 (88.6719)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.194 (0.559)  Loss:  0.9155 (1.3410)  Acc@1: 83.4906 (73.9520)  Acc@5: 95.6368 (91.8200)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-200.pth.tar', 73.95200006835937)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-194.pth.tar', 73.72600004150391)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-190.pth.tar', 73.70399999023438)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-197.pth.tar', 73.69800000976562)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-186.pth.tar', 73.62000009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-199.pth.tar', 73.58400016357422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-180.pth.tar', 73.55800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-196.pth.tar', 73.53799995849609)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-181.pth.tar', 73.43000009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-198.pth.tar', 73.42600006591798)

Train: 201 [   0/1251 (  0%)]  Loss: 3.663 (3.66)  Time: 2.317s,  441.94/s  (2.317s,  441.94/s)  LR: 7.503e-04  Data: 1.585 (1.585)
Train: 201 [  50/1251 (  4%)]  Loss: 3.596 (3.63)  Time: 0.779s, 1313.77/s  (0.819s, 1250.42/s)  LR: 7.503e-04  Data: 0.010 (0.050)
Train: 201 [ 100/1251 (  8%)]  Loss: 3.501 (3.59)  Time: 0.775s, 1321.36/s  (0.802s, 1276.12/s)  LR: 7.503e-04  Data: 0.011 (0.030)
Train: 201 [ 150/1251 ( 12%)]  Loss: 3.770 (3.63)  Time: 0.781s, 1310.72/s  (0.796s, 1285.81/s)  LR: 7.503e-04  Data: 0.010 (0.023)
Train: 201 [ 200/1251 ( 16%)]  Loss: 3.298 (3.57)  Time: 0.780s, 1313.28/s  (0.794s, 1289.35/s)  LR: 7.503e-04  Data: 0.009 (0.020)
Train: 201 [ 250/1251 ( 20%)]  Loss: 3.892 (3.62)  Time: 0.784s, 1305.65/s  (0.792s, 1292.27/s)  LR: 7.503e-04  Data: 0.010 (0.018)
Train: 201 [ 300/1251 ( 24%)]  Loss: 3.384 (3.59)  Time: 0.861s, 1189.62/s  (0.790s, 1296.08/s)  LR: 7.503e-04  Data: 0.009 (0.017)
Train: 201 [ 350/1251 ( 28%)]  Loss: 3.531 (3.58)  Time: 0.814s, 1257.96/s  (0.790s, 1296.43/s)  LR: 7.503e-04  Data: 0.009 (0.016)
Train: 201 [ 400/1251 ( 32%)]  Loss: 3.817 (3.61)  Time: 0.827s, 1237.90/s  (0.790s, 1296.30/s)  LR: 7.503e-04  Data: 0.014 (0.015)
Train: 201 [ 450/1251 ( 36%)]  Loss: 3.596 (3.60)  Time: 0.771s, 1327.93/s  (0.790s, 1295.51/s)  LR: 7.503e-04  Data: 0.009 (0.015)
Train: 201 [ 500/1251 ( 40%)]  Loss: 3.636 (3.61)  Time: 0.782s, 1308.91/s  (0.790s, 1296.95/s)  LR: 7.503e-04  Data: 0.009 (0.014)
Train: 201 [ 550/1251 ( 44%)]  Loss: 3.455 (3.59)  Time: 0.824s, 1242.19/s  (0.789s, 1297.92/s)  LR: 7.503e-04  Data: 0.010 (0.014)
Train: 201 [ 600/1251 ( 48%)]  Loss: 3.590 (3.59)  Time: 0.778s, 1315.97/s  (0.791s, 1294.51/s)  LR: 7.503e-04  Data: 0.010 (0.014)
Train: 201 [ 650/1251 ( 52%)]  Loss: 3.411 (3.58)  Time: 0.777s, 1317.34/s  (0.791s, 1294.97/s)  LR: 7.503e-04  Data: 0.010 (0.013)
Train: 201 [ 700/1251 ( 56%)]  Loss: 3.497 (3.58)  Time: 0.781s, 1310.93/s  (0.790s, 1296.22/s)  LR: 7.503e-04  Data: 0.009 (0.013)
Train: 201 [ 750/1251 ( 60%)]  Loss: 3.730 (3.59)  Time: 0.774s, 1323.04/s  (0.790s, 1297.01/s)  LR: 7.503e-04  Data: 0.010 (0.013)
Train: 201 [ 800/1251 ( 64%)]  Loss: 3.587 (3.59)  Time: 0.772s, 1326.31/s  (0.789s, 1297.95/s)  LR: 7.503e-04  Data: 0.010 (0.013)
Train: 201 [ 850/1251 ( 68%)]  Loss: 3.165 (3.56)  Time: 0.774s, 1323.84/s  (0.789s, 1297.78/s)  LR: 7.503e-04  Data: 0.010 (0.013)
Train: 201 [ 900/1251 ( 72%)]  Loss: 3.462 (3.56)  Time: 0.772s, 1326.22/s  (0.789s, 1298.55/s)  LR: 7.503e-04  Data: 0.010 (0.012)
Train: 201 [ 950/1251 ( 76%)]  Loss: 3.548 (3.56)  Time: 0.818s, 1251.49/s  (0.789s, 1298.58/s)  LR: 7.503e-04  Data: 0.011 (0.012)
Train: 201 [1000/1251 ( 80%)]  Loss: 3.588 (3.56)  Time: 0.829s, 1235.17/s  (0.788s, 1298.71/s)  LR: 7.503e-04  Data: 0.014 (0.012)
Train: 201 [1050/1251 ( 84%)]  Loss: 3.743 (3.57)  Time: 0.773s, 1325.25/s  (0.788s, 1298.98/s)  LR: 7.503e-04  Data: 0.010 (0.012)
Train: 201 [1100/1251 ( 88%)]  Loss: 3.673 (3.57)  Time: 0.777s, 1318.31/s  (0.788s, 1299.64/s)  LR: 7.503e-04  Data: 0.010 (0.012)
Train: 201 [1150/1251 ( 92%)]  Loss: 3.566 (3.57)  Time: 0.779s, 1315.13/s  (0.788s, 1300.14/s)  LR: 7.503e-04  Data: 0.010 (0.012)
Train: 201 [1200/1251 ( 96%)]  Loss: 3.576 (3.57)  Time: 0.772s, 1327.21/s  (0.788s, 1299.91/s)  LR: 7.503e-04  Data: 0.010 (0.012)
Train: 201 [1250/1251 (100%)]  Loss: 3.533 (3.57)  Time: 0.799s, 1280.85/s  (0.788s, 1298.84/s)  LR: 7.503e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.510 (1.510)  Loss:  0.6953 (0.6953)  Acc@1: 89.6484 (89.6484)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.194 (0.567)  Loss:  0.8408 (1.3443)  Acc@1: 85.1415 (73.6980)  Acc@5: 95.9906 (91.9860)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-200.pth.tar', 73.95200006835937)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-194.pth.tar', 73.72600004150391)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-190.pth.tar', 73.70399999023438)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-201.pth.tar', 73.69800013916016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-197.pth.tar', 73.69800000976562)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-186.pth.tar', 73.62000009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-199.pth.tar', 73.58400016357422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-180.pth.tar', 73.55800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-196.pth.tar', 73.53799995849609)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-181.pth.tar', 73.43000009277344)

Train: 202 [   0/1251 (  0%)]  Loss: 3.650 (3.65)  Time: 2.286s,  447.95/s  (2.286s,  447.95/s)  LR: 7.480e-04  Data: 1.558 (1.558)
Train: 202 [  50/1251 (  4%)]  Loss: 3.104 (3.38)  Time: 0.773s, 1324.48/s  (0.820s, 1249.19/s)  LR: 7.480e-04  Data: 0.010 (0.046)
Train: 202 [ 100/1251 (  8%)]  Loss: 3.449 (3.40)  Time: 0.826s, 1239.70/s  (0.806s, 1270.91/s)  LR: 7.480e-04  Data: 0.013 (0.029)
Train: 202 [ 150/1251 ( 12%)]  Loss: 3.317 (3.38)  Time: 0.774s, 1322.47/s  (0.802s, 1277.52/s)  LR: 7.480e-04  Data: 0.010 (0.023)
Train: 202 [ 200/1251 ( 16%)]  Loss: 3.550 (3.41)  Time: 0.778s, 1315.64/s  (0.797s, 1284.47/s)  LR: 7.480e-04  Data: 0.010 (0.019)
Train: 202 [ 250/1251 ( 20%)]  Loss: 3.557 (3.44)  Time: 0.774s, 1323.56/s  (0.794s, 1289.16/s)  LR: 7.480e-04  Data: 0.010 (0.018)
Train: 202 [ 300/1251 ( 24%)]  Loss: 3.907 (3.50)  Time: 0.843s, 1214.58/s  (0.793s, 1291.82/s)  LR: 7.480e-04  Data: 0.009 (0.016)
Train: 202 [ 350/1251 ( 28%)]  Loss: 3.705 (3.53)  Time: 0.776s, 1320.30/s  (0.791s, 1294.82/s)  LR: 7.480e-04  Data: 0.010 (0.015)
Train: 202 [ 400/1251 ( 32%)]  Loss: 3.939 (3.58)  Time: 0.770s, 1329.87/s  (0.790s, 1296.68/s)  LR: 7.480e-04  Data: 0.010 (0.015)
Train: 202 [ 450/1251 ( 36%)]  Loss: 3.435 (3.56)  Time: 0.773s, 1324.40/s  (0.789s, 1297.72/s)  LR: 7.480e-04  Data: 0.009 (0.014)
Train: 202 [ 500/1251 ( 40%)]  Loss: 3.737 (3.58)  Time: 0.772s, 1326.34/s  (0.788s, 1299.06/s)  LR: 7.480e-04  Data: 0.010 (0.014)
Train: 202 [ 550/1251 ( 44%)]  Loss: 3.786 (3.59)  Time: 0.777s, 1317.37/s  (0.788s, 1299.61/s)  LR: 7.480e-04  Data: 0.012 (0.013)
Train: 202 [ 600/1251 ( 48%)]  Loss: 2.943 (3.54)  Time: 0.774s, 1322.89/s  (0.787s, 1300.81/s)  LR: 7.480e-04  Data: 0.010 (0.013)
Train: 202 [ 650/1251 ( 52%)]  Loss: 3.549 (3.54)  Time: 0.786s, 1302.35/s  (0.787s, 1301.46/s)  LR: 7.480e-04  Data: 0.009 (0.013)
Train: 202 [ 700/1251 ( 56%)]  Loss: 3.687 (3.55)  Time: 0.772s, 1326.82/s  (0.787s, 1301.54/s)  LR: 7.480e-04  Data: 0.010 (0.013)
Train: 202 [ 750/1251 ( 60%)]  Loss: 3.029 (3.52)  Time: 0.775s, 1321.28/s  (0.787s, 1301.48/s)  LR: 7.480e-04  Data: 0.009 (0.012)
Train: 202 [ 800/1251 ( 64%)]  Loss: 3.510 (3.52)  Time: 0.771s, 1328.39/s  (0.786s, 1302.04/s)  LR: 7.480e-04  Data: 0.009 (0.012)
Train: 202 [ 850/1251 ( 68%)]  Loss: 3.657 (3.53)  Time: 0.773s, 1325.10/s  (0.786s, 1302.60/s)  LR: 7.480e-04  Data: 0.010 (0.012)
Train: 202 [ 900/1251 ( 72%)]  Loss: 3.881 (3.55)  Time: 0.772s, 1327.00/s  (0.786s, 1302.81/s)  LR: 7.480e-04  Data: 0.010 (0.012)
Train: 202 [ 950/1251 ( 76%)]  Loss: 3.643 (3.55)  Time: 0.785s, 1304.96/s  (0.786s, 1303.07/s)  LR: 7.480e-04  Data: 0.009 (0.012)
Train: 202 [1000/1251 ( 80%)]  Loss: 3.712 (3.56)  Time: 0.773s, 1324.49/s  (0.786s, 1302.80/s)  LR: 7.480e-04  Data: 0.010 (0.012)
Train: 202 [1050/1251 ( 84%)]  Loss: 3.460 (3.55)  Time: 0.772s, 1325.59/s  (0.786s, 1302.91/s)  LR: 7.480e-04  Data: 0.009 (0.012)
Train: 202 [1100/1251 ( 88%)]  Loss: 3.495 (3.55)  Time: 0.827s, 1238.51/s  (0.786s, 1302.46/s)  LR: 7.480e-04  Data: 0.014 (0.012)
Train: 202 [1150/1251 ( 92%)]  Loss: 3.712 (3.56)  Time: 0.773s, 1325.05/s  (0.786s, 1302.13/s)  LR: 7.480e-04  Data: 0.010 (0.012)
Train: 202 [1200/1251 ( 96%)]  Loss: 3.527 (3.56)  Time: 0.776s, 1319.62/s  (0.787s, 1301.68/s)  LR: 7.480e-04  Data: 0.011 (0.012)
Train: 202 [1250/1251 (100%)]  Loss: 3.658 (3.56)  Time: 0.762s, 1344.40/s  (0.787s, 1301.51/s)  LR: 7.480e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.607 (1.607)  Loss:  0.8232 (0.8232)  Acc@1: 90.0391 (90.0391)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.194 (0.560)  Loss:  0.9331 (1.3945)  Acc@1: 84.1981 (73.8080)  Acc@5: 96.1085 (92.0320)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-200.pth.tar', 73.95200006835937)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-202.pth.tar', 73.8080001171875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-194.pth.tar', 73.72600004150391)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-190.pth.tar', 73.70399999023438)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-201.pth.tar', 73.69800013916016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-197.pth.tar', 73.69800000976562)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-186.pth.tar', 73.62000009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-199.pth.tar', 73.58400016357422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-180.pth.tar', 73.55800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-196.pth.tar', 73.53799995849609)

Train: 203 [   0/1251 (  0%)]  Loss: 3.469 (3.47)  Time: 2.408s,  425.31/s  (2.408s,  425.31/s)  LR: 7.457e-04  Data: 1.669 (1.669)
Train: 203 [  50/1251 (  4%)]  Loss: 3.594 (3.53)  Time: 0.783s, 1308.18/s  (0.820s, 1249.20/s)  LR: 7.457e-04  Data: 0.011 (0.049)
Train: 203 [ 100/1251 (  8%)]  Loss: 3.324 (3.46)  Time: 0.785s, 1304.46/s  (0.808s, 1266.88/s)  LR: 7.457e-04  Data: 0.010 (0.030)
Train: 203 [ 150/1251 ( 12%)]  Loss: 3.304 (3.42)  Time: 0.785s, 1303.84/s  (0.800s, 1280.06/s)  LR: 7.457e-04  Data: 0.011 (0.024)
Train: 203 [ 200/1251 ( 16%)]  Loss: 3.730 (3.48)  Time: 0.774s, 1322.20/s  (0.796s, 1286.94/s)  LR: 7.457e-04  Data: 0.010 (0.020)
Train: 203 [ 250/1251 ( 20%)]  Loss: 3.437 (3.48)  Time: 0.773s, 1323.86/s  (0.793s, 1291.21/s)  LR: 7.457e-04  Data: 0.011 (0.018)
Train: 203 [ 300/1251 ( 24%)]  Loss: 3.894 (3.54)  Time: 0.781s, 1310.33/s  (0.791s, 1294.22/s)  LR: 7.457e-04  Data: 0.010 (0.017)
Train: 203 [ 350/1251 ( 28%)]  Loss: 3.738 (3.56)  Time: 0.779s, 1314.72/s  (0.790s, 1295.99/s)  LR: 7.457e-04  Data: 0.010 (0.016)
Train: 203 [ 400/1251 ( 32%)]  Loss: 3.594 (3.56)  Time: 0.774s, 1323.77/s  (0.789s, 1297.82/s)  LR: 7.457e-04  Data: 0.010 (0.015)
Train: 203 [ 450/1251 ( 36%)]  Loss: 3.526 (3.56)  Time: 0.864s, 1184.81/s  (0.789s, 1298.66/s)  LR: 7.457e-04  Data: 0.010 (0.015)
Train: 203 [ 500/1251 ( 40%)]  Loss: 3.713 (3.57)  Time: 0.776s, 1319.31/s  (0.788s, 1299.35/s)  LR: 7.457e-04  Data: 0.010 (0.014)
Train: 203 [ 550/1251 ( 44%)]  Loss: 3.701 (3.59)  Time: 0.772s, 1327.21/s  (0.788s, 1299.59/s)  LR: 7.457e-04  Data: 0.010 (0.014)
Train: 203 [ 600/1251 ( 48%)]  Loss: 3.483 (3.58)  Time: 0.773s, 1325.05/s  (0.787s, 1300.86/s)  LR: 7.457e-04  Data: 0.010 (0.014)
Train: 203 [ 650/1251 ( 52%)]  Loss: 3.835 (3.60)  Time: 0.773s, 1324.06/s  (0.787s, 1300.74/s)  LR: 7.457e-04  Data: 0.011 (0.013)
Train: 203 [ 700/1251 ( 56%)]  Loss: 3.134 (3.57)  Time: 0.830s, 1233.93/s  (0.787s, 1301.74/s)  LR: 7.457e-04  Data: 0.010 (0.013)
Train: 203 [ 750/1251 ( 60%)]  Loss: 3.575 (3.57)  Time: 0.834s, 1227.63/s  (0.786s, 1302.60/s)  LR: 7.457e-04  Data: 0.009 (0.013)
Train: 203 [ 800/1251 ( 64%)]  Loss: 3.743 (3.58)  Time: 0.785s, 1305.26/s  (0.786s, 1302.97/s)  LR: 7.457e-04  Data: 0.010 (0.013)
Train: 203 [ 850/1251 ( 68%)]  Loss: 3.713 (3.58)  Time: 0.773s, 1325.40/s  (0.786s, 1303.25/s)  LR: 7.457e-04  Data: 0.009 (0.012)
Train: 203 [ 900/1251 ( 72%)]  Loss: 3.748 (3.59)  Time: 0.787s, 1301.23/s  (0.786s, 1303.17/s)  LR: 7.457e-04  Data: 0.010 (0.012)
Train: 203 [ 950/1251 ( 76%)]  Loss: 3.967 (3.61)  Time: 0.863s, 1185.95/s  (0.786s, 1302.84/s)  LR: 7.457e-04  Data: 0.010 (0.012)
Train: 203 [1000/1251 ( 80%)]  Loss: 3.185 (3.59)  Time: 0.779s, 1314.62/s  (0.786s, 1302.60/s)  LR: 7.457e-04  Data: 0.009 (0.012)
Train: 203 [1050/1251 ( 84%)]  Loss: 3.969 (3.61)  Time: 0.865s, 1183.52/s  (0.786s, 1303.02/s)  LR: 7.457e-04  Data: 0.010 (0.012)
Train: 203 [1100/1251 ( 88%)]  Loss: 3.827 (3.62)  Time: 0.817s, 1252.74/s  (0.786s, 1303.24/s)  LR: 7.457e-04  Data: 0.011 (0.012)
Train: 203 [1150/1251 ( 92%)]  Loss: 3.735 (3.62)  Time: 0.827s, 1238.24/s  (0.786s, 1302.34/s)  LR: 7.457e-04  Data: 0.009 (0.012)
Train: 203 [1200/1251 ( 96%)]  Loss: 3.476 (3.62)  Time: 0.787s, 1300.98/s  (0.786s, 1302.40/s)  LR: 7.457e-04  Data: 0.009 (0.012)
Train: 203 [1250/1251 (100%)]  Loss: 3.308 (3.60)  Time: 0.760s, 1347.95/s  (0.786s, 1302.50/s)  LR: 7.457e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.521 (1.521)  Loss:  0.7153 (0.7153)  Acc@1: 89.4531 (89.4531)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.194 (0.570)  Loss:  0.9375 (1.3103)  Acc@1: 83.7264 (73.8960)  Acc@5: 95.8726 (92.1780)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-200.pth.tar', 73.95200006835937)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-203.pth.tar', 73.89600004150391)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-202.pth.tar', 73.8080001171875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-194.pth.tar', 73.72600004150391)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-190.pth.tar', 73.70399999023438)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-201.pth.tar', 73.69800013916016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-197.pth.tar', 73.69800000976562)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-186.pth.tar', 73.62000009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-199.pth.tar', 73.58400016357422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-180.pth.tar', 73.55800001220703)

Train: 204 [   0/1251 (  0%)]  Loss: 3.330 (3.33)  Time: 2.342s,  437.15/s  (2.342s,  437.15/s)  LR: 7.435e-04  Data: 1.615 (1.615)
Train: 204 [  50/1251 (  4%)]  Loss: 3.579 (3.45)  Time: 0.782s, 1309.47/s  (0.824s, 1242.73/s)  LR: 7.435e-04  Data: 0.010 (0.050)
Train: 204 [ 100/1251 (  8%)]  Loss: 3.350 (3.42)  Time: 0.853s, 1200.46/s  (0.812s, 1261.27/s)  LR: 7.435e-04  Data: 0.010 (0.030)
Train: 204 [ 150/1251 ( 12%)]  Loss: 3.797 (3.51)  Time: 0.784s, 1306.22/s  (0.804s, 1273.53/s)  LR: 7.435e-04  Data: 0.012 (0.023)
Train: 204 [ 200/1251 ( 16%)]  Loss: 3.525 (3.52)  Time: 0.782s, 1308.70/s  (0.801s, 1278.01/s)  LR: 7.435e-04  Data: 0.010 (0.020)
Train: 204 [ 250/1251 ( 20%)]  Loss: 3.391 (3.50)  Time: 0.775s, 1320.51/s  (0.798s, 1282.89/s)  LR: 7.435e-04  Data: 0.009 (0.018)
Train: 204 [ 300/1251 ( 24%)]  Loss: 3.512 (3.50)  Time: 0.772s, 1327.19/s  (0.795s, 1288.34/s)  LR: 7.435e-04  Data: 0.010 (0.017)
Train: 204 [ 350/1251 ( 28%)]  Loss: 3.440 (3.49)  Time: 0.776s, 1319.78/s  (0.794s, 1290.17/s)  LR: 7.435e-04  Data: 0.010 (0.016)
Train: 204 [ 400/1251 ( 32%)]  Loss: 3.836 (3.53)  Time: 0.774s, 1322.50/s  (0.792s, 1293.15/s)  LR: 7.435e-04  Data: 0.011 (0.015)
Train: 204 [ 450/1251 ( 36%)]  Loss: 3.655 (3.54)  Time: 0.781s, 1310.85/s  (0.792s, 1292.16/s)  LR: 7.435e-04  Data: 0.010 (0.015)
Train: 204 [ 500/1251 ( 40%)]  Loss: 3.478 (3.54)  Time: 0.815s, 1255.98/s  (0.793s, 1291.89/s)  LR: 7.435e-04  Data: 0.011 (0.014)
Train: 204 [ 550/1251 ( 44%)]  Loss: 3.666 (3.55)  Time: 0.772s, 1325.89/s  (0.793s, 1291.33/s)  LR: 7.435e-04  Data: 0.009 (0.014)
Train: 204 [ 600/1251 ( 48%)]  Loss: 3.527 (3.55)  Time: 0.774s, 1323.25/s  (0.792s, 1293.13/s)  LR: 7.435e-04  Data: 0.009 (0.014)
Train: 204 [ 650/1251 ( 52%)]  Loss: 3.600 (3.55)  Time: 0.774s, 1322.83/s  (0.792s, 1293.01/s)  LR: 7.435e-04  Data: 0.010 (0.013)
Train: 204 [ 700/1251 ( 56%)]  Loss: 3.922 (3.57)  Time: 0.809s, 1265.95/s  (0.791s, 1293.90/s)  LR: 7.435e-04  Data: 0.011 (0.013)
Train: 204 [ 750/1251 ( 60%)]  Loss: 3.616 (3.58)  Time: 0.774s, 1322.19/s  (0.791s, 1294.47/s)  LR: 7.435e-04  Data: 0.010 (0.013)
Train: 204 [ 800/1251 ( 64%)]  Loss: 3.566 (3.58)  Time: 0.803s, 1275.92/s  (0.790s, 1295.42/s)  LR: 7.435e-04  Data: 0.013 (0.013)
Train: 204 [ 850/1251 ( 68%)]  Loss: 3.305 (3.56)  Time: 0.786s, 1303.27/s  (0.790s, 1295.39/s)  LR: 7.435e-04  Data: 0.009 (0.013)
Train: 204 [ 900/1251 ( 72%)]  Loss: 3.828 (3.57)  Time: 0.813s, 1258.98/s  (0.791s, 1294.21/s)  LR: 7.435e-04  Data: 0.010 (0.012)
Train: 204 [ 950/1251 ( 76%)]  Loss: 3.162 (3.55)  Time: 0.779s, 1313.77/s  (0.791s, 1294.57/s)  LR: 7.435e-04  Data: 0.009 (0.012)
Train: 204 [1000/1251 ( 80%)]  Loss: 3.755 (3.56)  Time: 0.783s, 1307.52/s  (0.791s, 1294.67/s)  LR: 7.435e-04  Data: 0.012 (0.012)
Train: 204 [1050/1251 ( 84%)]  Loss: 3.484 (3.56)  Time: 0.771s, 1327.34/s  (0.791s, 1295.34/s)  LR: 7.435e-04  Data: 0.011 (0.012)
Train: 204 [1100/1251 ( 88%)]  Loss: 3.304 (3.55)  Time: 0.770s, 1330.40/s  (0.790s, 1295.50/s)  LR: 7.435e-04  Data: 0.010 (0.012)
Train: 204 [1150/1251 ( 92%)]  Loss: 3.509 (3.55)  Time: 0.776s, 1319.54/s  (0.790s, 1295.70/s)  LR: 7.435e-04  Data: 0.010 (0.012)
Train: 204 [1200/1251 ( 96%)]  Loss: 3.242 (3.54)  Time: 0.773s, 1324.74/s  (0.790s, 1295.52/s)  LR: 7.435e-04  Data: 0.010 (0.012)
Train: 204 [1250/1251 (100%)]  Loss: 3.690 (3.54)  Time: 0.764s, 1340.75/s  (0.790s, 1295.84/s)  LR: 7.435e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.528 (1.528)  Loss:  0.8032 (0.8032)  Acc@1: 89.0625 (89.0625)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.193 (0.571)  Loss:  1.0293 (1.3693)  Acc@1: 81.6038 (73.4440)  Acc@5: 96.4623 (91.7460)
Train: 205 [   0/1251 (  0%)]  Loss: 3.036 (3.04)  Time: 2.263s,  452.56/s  (2.263s,  452.56/s)  LR: 7.412e-04  Data: 1.520 (1.520)
Train: 205 [  50/1251 (  4%)]  Loss: 3.518 (3.28)  Time: 0.839s, 1220.67/s  (0.814s, 1257.63/s)  LR: 7.412e-04  Data: 0.012 (0.041)
Train: 205 [ 100/1251 (  8%)]  Loss: 3.680 (3.41)  Time: 0.775s, 1320.73/s  (0.799s, 1281.84/s)  LR: 7.412e-04  Data: 0.009 (0.026)
Train: 205 [ 150/1251 ( 12%)]  Loss: 3.261 (3.37)  Time: 0.833s, 1229.77/s  (0.796s, 1286.14/s)  LR: 7.412e-04  Data: 0.009 (0.020)
Train: 205 [ 200/1251 ( 16%)]  Loss: 3.516 (3.40)  Time: 0.781s, 1311.80/s  (0.793s, 1291.73/s)  LR: 7.412e-04  Data: 0.009 (0.018)
Train: 205 [ 250/1251 ( 20%)]  Loss: 3.715 (3.45)  Time: 0.772s, 1325.57/s  (0.790s, 1295.65/s)  LR: 7.412e-04  Data: 0.010 (0.016)
Train: 205 [ 300/1251 ( 24%)]  Loss: 3.431 (3.45)  Time: 0.849s, 1206.10/s  (0.790s, 1296.61/s)  LR: 7.412e-04  Data: 0.010 (0.015)
Train: 205 [ 350/1251 ( 28%)]  Loss: 3.734 (3.49)  Time: 0.775s, 1321.77/s  (0.789s, 1297.65/s)  LR: 7.412e-04  Data: 0.009 (0.014)
Train: 205 [ 400/1251 ( 32%)]  Loss: 3.797 (3.52)  Time: 0.776s, 1320.35/s  (0.789s, 1297.34/s)  LR: 7.412e-04  Data: 0.010 (0.014)
Train: 205 [ 450/1251 ( 36%)]  Loss: 3.919 (3.56)  Time: 0.771s, 1327.29/s  (0.789s, 1298.27/s)  LR: 7.412e-04  Data: 0.010 (0.013)
Train: 205 [ 500/1251 ( 40%)]  Loss: 3.839 (3.59)  Time: 0.774s, 1323.78/s  (0.788s, 1300.10/s)  LR: 7.412e-04  Data: 0.010 (0.013)
Train: 205 [ 550/1251 ( 44%)]  Loss: 3.672 (3.59)  Time: 0.772s, 1326.04/s  (0.788s, 1299.38/s)  LR: 7.412e-04  Data: 0.010 (0.013)
Train: 205 [ 600/1251 ( 48%)]  Loss: 3.225 (3.56)  Time: 0.772s, 1326.19/s  (0.787s, 1300.72/s)  LR: 7.412e-04  Data: 0.009 (0.013)
Train: 205 [ 650/1251 ( 52%)]  Loss: 3.695 (3.57)  Time: 0.781s, 1311.60/s  (0.787s, 1301.57/s)  LR: 7.412e-04  Data: 0.011 (0.012)
Train: 205 [ 700/1251 ( 56%)]  Loss: 3.549 (3.57)  Time: 0.775s, 1321.74/s  (0.786s, 1302.36/s)  LR: 7.412e-04  Data: 0.010 (0.012)
Train: 205 [ 750/1251 ( 60%)]  Loss: 3.946 (3.60)  Time: 0.827s, 1238.79/s  (0.787s, 1301.73/s)  LR: 7.412e-04  Data: 0.009 (0.012)
Train: 205 [ 800/1251 ( 64%)]  Loss: 3.948 (3.62)  Time: 0.787s, 1301.87/s  (0.786s, 1302.20/s)  LR: 7.412e-04  Data: 0.010 (0.012)
Train: 205 [ 850/1251 ( 68%)]  Loss: 3.585 (3.61)  Time: 0.780s, 1313.37/s  (0.786s, 1303.13/s)  LR: 7.412e-04  Data: 0.010 (0.012)
Train: 205 [ 900/1251 ( 72%)]  Loss: 3.786 (3.62)  Time: 0.779s, 1314.07/s  (0.787s, 1301.19/s)  LR: 7.412e-04  Data: 0.016 (0.012)
Train: 205 [ 950/1251 ( 76%)]  Loss: 3.627 (3.62)  Time: 0.773s, 1325.24/s  (0.786s, 1302.05/s)  LR: 7.412e-04  Data: 0.009 (0.012)
Train: 205 [1000/1251 ( 80%)]  Loss: 3.616 (3.62)  Time: 0.771s, 1327.53/s  (0.787s, 1301.41/s)  LR: 7.412e-04  Data: 0.010 (0.012)
Train: 205 [1050/1251 ( 84%)]  Loss: 3.538 (3.62)  Time: 0.772s, 1326.29/s  (0.787s, 1301.90/s)  LR: 7.412e-04  Data: 0.010 (0.011)
Train: 205 [1100/1251 ( 88%)]  Loss: 3.690 (3.62)  Time: 0.812s, 1261.42/s  (0.786s, 1302.51/s)  LR: 7.412e-04  Data: 0.010 (0.011)
Train: 205 [1150/1251 ( 92%)]  Loss: 3.877 (3.63)  Time: 0.772s, 1326.52/s  (0.786s, 1303.07/s)  LR: 7.412e-04  Data: 0.010 (0.011)
Train: 205 [1200/1251 ( 96%)]  Loss: 3.497 (3.63)  Time: 0.781s, 1311.10/s  (0.786s, 1303.03/s)  LR: 7.412e-04  Data: 0.010 (0.011)
Train: 205 [1250/1251 (100%)]  Loss: 3.517 (3.62)  Time: 0.760s, 1348.21/s  (0.786s, 1303.38/s)  LR: 7.412e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.588 (1.588)  Loss:  0.7559 (0.7559)  Acc@1: 88.2812 (88.2812)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.194 (0.565)  Loss:  0.8975 (1.2859)  Acc@1: 83.1368 (73.6900)  Acc@5: 95.4009 (91.9080)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-200.pth.tar', 73.95200006835937)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-203.pth.tar', 73.89600004150391)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-202.pth.tar', 73.8080001171875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-194.pth.tar', 73.72600004150391)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-190.pth.tar', 73.70399999023438)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-201.pth.tar', 73.69800013916016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-197.pth.tar', 73.69800000976562)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-205.pth.tar', 73.69000004394532)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-186.pth.tar', 73.62000009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-199.pth.tar', 73.58400016357422)

Train: 206 [   0/1251 (  0%)]  Loss: 3.484 (3.48)  Time: 2.322s,  441.01/s  (2.322s,  441.01/s)  LR: 7.389e-04  Data: 1.592 (1.592)
Train: 206 [  50/1251 (  4%)]  Loss: 3.572 (3.53)  Time: 0.772s, 1326.77/s  (0.816s, 1255.39/s)  LR: 7.389e-04  Data: 0.010 (0.047)
Train: 206 [ 100/1251 (  8%)]  Loss: 3.998 (3.68)  Time: 0.775s, 1321.22/s  (0.801s, 1278.06/s)  LR: 7.389e-04  Data: 0.011 (0.029)
Train: 206 [ 150/1251 ( 12%)]  Loss: 3.214 (3.57)  Time: 0.772s, 1326.92/s  (0.798s, 1282.75/s)  LR: 7.389e-04  Data: 0.010 (0.022)
Train: 206 [ 200/1251 ( 16%)]  Loss: 3.377 (3.53)  Time: 0.774s, 1323.28/s  (0.795s, 1288.56/s)  LR: 7.389e-04  Data: 0.010 (0.019)
Train: 206 [ 250/1251 ( 20%)]  Loss: 3.063 (3.45)  Time: 0.772s, 1326.64/s  (0.795s, 1287.80/s)  LR: 7.389e-04  Data: 0.010 (0.018)
Train: 206 [ 300/1251 ( 24%)]  Loss: 3.370 (3.44)  Time: 0.782s, 1309.64/s  (0.792s, 1292.75/s)  LR: 7.389e-04  Data: 0.010 (0.016)
Train: 206 [ 350/1251 ( 28%)]  Loss: 3.433 (3.44)  Time: 0.781s, 1311.63/s  (0.792s, 1292.22/s)  LR: 7.389e-04  Data: 0.010 (0.015)
Train: 206 [ 400/1251 ( 32%)]  Loss: 3.395 (3.43)  Time: 0.782s, 1309.71/s  (0.792s, 1293.45/s)  LR: 7.389e-04  Data: 0.010 (0.015)
Train: 206 [ 450/1251 ( 36%)]  Loss: 3.656 (3.46)  Time: 0.773s, 1325.30/s  (0.791s, 1294.14/s)  LR: 7.389e-04  Data: 0.009 (0.014)
Train: 206 [ 500/1251 ( 40%)]  Loss: 3.561 (3.47)  Time: 0.773s, 1325.24/s  (0.791s, 1295.33/s)  LR: 7.389e-04  Data: 0.011 (0.014)
Train: 206 [ 550/1251 ( 44%)]  Loss: 3.641 (3.48)  Time: 0.775s, 1321.38/s  (0.791s, 1294.44/s)  LR: 7.389e-04  Data: 0.010 (0.013)
Train: 206 [ 600/1251 ( 48%)]  Loss: 3.418 (3.48)  Time: 0.777s, 1317.55/s  (0.790s, 1295.95/s)  LR: 7.389e-04  Data: 0.010 (0.013)
Train: 206 [ 650/1251 ( 52%)]  Loss: 3.590 (3.48)  Time: 0.845s, 1211.27/s  (0.790s, 1296.83/s)  LR: 7.389e-04  Data: 0.010 (0.013)
Train: 206 [ 700/1251 ( 56%)]  Loss: 3.676 (3.50)  Time: 0.772s, 1326.80/s  (0.789s, 1297.66/s)  LR: 7.389e-04  Data: 0.009 (0.013)
Train: 206 [ 750/1251 ( 60%)]  Loss: 3.725 (3.51)  Time: 0.773s, 1324.85/s  (0.790s, 1297.02/s)  LR: 7.389e-04  Data: 0.010 (0.012)
Train: 206 [ 800/1251 ( 64%)]  Loss: 3.410 (3.51)  Time: 0.773s, 1324.92/s  (0.789s, 1298.13/s)  LR: 7.389e-04  Data: 0.010 (0.012)
Train: 206 [ 850/1251 ( 68%)]  Loss: 3.552 (3.51)  Time: 0.773s, 1325.49/s  (0.789s, 1298.01/s)  LR: 7.389e-04  Data: 0.010 (0.012)
Train: 206 [ 900/1251 ( 72%)]  Loss: 3.954 (3.53)  Time: 0.817s, 1253.13/s  (0.790s, 1296.99/s)  LR: 7.389e-04  Data: 0.009 (0.012)
Train: 206 [ 950/1251 ( 76%)]  Loss: 3.610 (3.54)  Time: 0.776s, 1319.67/s  (0.789s, 1297.62/s)  LR: 7.389e-04  Data: 0.009 (0.012)
Train: 206 [1000/1251 ( 80%)]  Loss: 3.668 (3.54)  Time: 0.773s, 1324.61/s  (0.789s, 1298.01/s)  LR: 7.389e-04  Data: 0.010 (0.012)
Train: 206 [1050/1251 ( 84%)]  Loss: 3.077 (3.52)  Time: 0.772s, 1326.71/s  (0.789s, 1297.59/s)  LR: 7.389e-04  Data: 0.009 (0.012)
Train: 206 [1100/1251 ( 88%)]  Loss: 3.514 (3.52)  Time: 0.814s, 1258.45/s  (0.789s, 1298.12/s)  LR: 7.389e-04  Data: 0.009 (0.012)
Train: 206 [1150/1251 ( 92%)]  Loss: 3.385 (3.51)  Time: 0.772s, 1326.98/s  (0.788s, 1298.91/s)  LR: 7.389e-04  Data: 0.009 (0.012)
Train: 206 [1200/1251 ( 96%)]  Loss: 3.656 (3.52)  Time: 0.778s, 1315.37/s  (0.788s, 1298.78/s)  LR: 7.389e-04  Data: 0.014 (0.012)
Train: 206 [1250/1251 (100%)]  Loss: 3.652 (3.53)  Time: 0.760s, 1347.51/s  (0.788s, 1299.65/s)  LR: 7.389e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.537 (1.537)  Loss:  0.7427 (0.7427)  Acc@1: 88.3789 (88.3789)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.194 (0.562)  Loss:  0.8804 (1.3437)  Acc@1: 84.3160 (73.1180)  Acc@5: 96.8160 (91.7040)
Train: 207 [   0/1251 (  0%)]  Loss: 3.576 (3.58)  Time: 2.210s,  463.31/s  (2.210s,  463.31/s)  LR: 7.366e-04  Data: 1.478 (1.478)
Train: 207 [  50/1251 (  4%)]  Loss: 3.642 (3.61)  Time: 0.774s, 1322.35/s  (0.815s, 1256.50/s)  LR: 7.366e-04  Data: 0.009 (0.044)
Train: 207 [ 100/1251 (  8%)]  Loss: 3.563 (3.59)  Time: 0.778s, 1315.83/s  (0.799s, 1281.90/s)  LR: 7.366e-04  Data: 0.009 (0.027)
Train: 207 [ 150/1251 ( 12%)]  Loss: 3.494 (3.57)  Time: 0.773s, 1324.98/s  (0.795s, 1288.55/s)  LR: 7.366e-04  Data: 0.009 (0.021)
Train: 207 [ 200/1251 ( 16%)]  Loss: 3.517 (3.56)  Time: 0.785s, 1303.63/s  (0.791s, 1294.39/s)  LR: 7.366e-04  Data: 0.009 (0.018)
Train: 207 [ 250/1251 ( 20%)]  Loss: 3.670 (3.58)  Time: 0.822s, 1245.60/s  (0.789s, 1298.13/s)  LR: 7.366e-04  Data: 0.009 (0.017)
Train: 207 [ 300/1251 ( 24%)]  Loss: 3.465 (3.56)  Time: 0.774s, 1323.66/s  (0.787s, 1300.35/s)  LR: 7.366e-04  Data: 0.010 (0.016)
Train: 207 [ 350/1251 ( 28%)]  Loss: 3.557 (3.56)  Time: 0.772s, 1326.11/s  (0.788s, 1299.85/s)  LR: 7.366e-04  Data: 0.009 (0.015)
Train: 207 [ 400/1251 ( 32%)]  Loss: 3.029 (3.50)  Time: 0.771s, 1328.71/s  (0.787s, 1301.04/s)  LR: 7.366e-04  Data: 0.010 (0.014)
Train: 207 [ 450/1251 ( 36%)]  Loss: 3.207 (3.47)  Time: 0.780s, 1313.60/s  (0.787s, 1300.88/s)  LR: 7.366e-04  Data: 0.010 (0.014)
Train: 207 [ 500/1251 ( 40%)]  Loss: 3.572 (3.48)  Time: 0.772s, 1327.15/s  (0.787s, 1301.93/s)  LR: 7.366e-04  Data: 0.009 (0.013)
Train: 207 [ 550/1251 ( 44%)]  Loss: 3.258 (3.46)  Time: 0.772s, 1326.96/s  (0.786s, 1302.84/s)  LR: 7.366e-04  Data: 0.009 (0.013)
Train: 207 [ 600/1251 ( 48%)]  Loss: 3.689 (3.48)  Time: 0.773s, 1324.48/s  (0.785s, 1304.41/s)  LR: 7.366e-04  Data: 0.010 (0.013)
Train: 207 [ 650/1251 ( 52%)]  Loss: 3.999 (3.52)  Time: 0.773s, 1325.52/s  (0.785s, 1304.72/s)  LR: 7.366e-04  Data: 0.010 (0.013)
Train: 207 [ 700/1251 ( 56%)]  Loss: 3.579 (3.52)  Time: 0.774s, 1322.66/s  (0.785s, 1304.70/s)  LR: 7.366e-04  Data: 0.010 (0.012)
Train: 207 [ 750/1251 ( 60%)]  Loss: 3.645 (3.53)  Time: 0.773s, 1324.18/s  (0.785s, 1304.55/s)  LR: 7.366e-04  Data: 0.009 (0.012)
Train: 207 [ 800/1251 ( 64%)]  Loss: 3.596 (3.53)  Time: 0.790s, 1296.95/s  (0.785s, 1304.43/s)  LR: 7.366e-04  Data: 0.009 (0.012)
Train: 207 [ 850/1251 ( 68%)]  Loss: 3.624 (3.54)  Time: 0.779s, 1314.38/s  (0.785s, 1304.52/s)  LR: 7.366e-04  Data: 0.010 (0.012)
Train: 207 [ 900/1251 ( 72%)]  Loss: 4.025 (3.56)  Time: 0.779s, 1315.08/s  (0.785s, 1304.39/s)  LR: 7.366e-04  Data: 0.009 (0.012)
Train: 207 [ 950/1251 ( 76%)]  Loss: 3.610 (3.57)  Time: 0.789s, 1297.21/s  (0.785s, 1304.49/s)  LR: 7.366e-04  Data: 0.009 (0.012)
Train: 207 [1000/1251 ( 80%)]  Loss: 3.561 (3.57)  Time: 0.780s, 1312.19/s  (0.785s, 1304.48/s)  LR: 7.366e-04  Data: 0.011 (0.012)
Train: 207 [1050/1251 ( 84%)]  Loss: 3.640 (3.57)  Time: 0.781s, 1310.78/s  (0.785s, 1304.65/s)  LR: 7.366e-04  Data: 0.009 (0.012)
Train: 207 [1100/1251 ( 88%)]  Loss: 3.679 (3.57)  Time: 0.790s, 1296.55/s  (0.785s, 1305.04/s)  LR: 7.366e-04  Data: 0.010 (0.011)
Train: 207 [1150/1251 ( 92%)]  Loss: 4.077 (3.59)  Time: 0.778s, 1316.21/s  (0.785s, 1304.66/s)  LR: 7.366e-04  Data: 0.010 (0.011)
Train: 207 [1200/1251 ( 96%)]  Loss: 3.244 (3.58)  Time: 0.771s, 1327.84/s  (0.785s, 1304.99/s)  LR: 7.366e-04  Data: 0.009 (0.011)
Train: 207 [1250/1251 (100%)]  Loss: 3.685 (3.58)  Time: 0.770s, 1330.52/s  (0.785s, 1304.52/s)  LR: 7.366e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.549 (1.549)  Loss:  0.7632 (0.7632)  Acc@1: 89.0625 (89.0625)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.194 (0.565)  Loss:  0.9902 (1.3362)  Acc@1: 82.6651 (73.7040)  Acc@5: 95.6368 (92.2100)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-200.pth.tar', 73.95200006835937)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-203.pth.tar', 73.89600004150391)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-202.pth.tar', 73.8080001171875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-194.pth.tar', 73.72600004150391)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-207.pth.tar', 73.70400009765625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-190.pth.tar', 73.70399999023438)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-201.pth.tar', 73.69800013916016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-197.pth.tar', 73.69800000976562)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-205.pth.tar', 73.69000004394532)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-186.pth.tar', 73.62000009277344)

Train: 208 [   0/1251 (  0%)]  Loss: 3.777 (3.78)  Time: 2.300s,  445.14/s  (2.300s,  445.14/s)  LR: 7.343e-04  Data: 1.565 (1.565)
Train: 208 [  50/1251 (  4%)]  Loss: 3.904 (3.84)  Time: 0.771s, 1327.78/s  (0.813s, 1260.09/s)  LR: 7.343e-04  Data: 0.009 (0.045)
Train: 208 [ 100/1251 (  8%)]  Loss: 3.815 (3.83)  Time: 0.773s, 1325.05/s  (0.796s, 1286.02/s)  LR: 7.343e-04  Data: 0.010 (0.028)
Train: 208 [ 150/1251 ( 12%)]  Loss: 3.653 (3.79)  Time: 0.773s, 1324.12/s  (0.789s, 1297.16/s)  LR: 7.343e-04  Data: 0.010 (0.022)
Train: 208 [ 200/1251 ( 16%)]  Loss: 3.541 (3.74)  Time: 0.773s, 1324.14/s  (0.787s, 1301.60/s)  LR: 7.343e-04  Data: 0.010 (0.019)
Train: 208 [ 250/1251 ( 20%)]  Loss: 3.711 (3.73)  Time: 0.773s, 1324.69/s  (0.786s, 1303.15/s)  LR: 7.343e-04  Data: 0.009 (0.017)
Train: 208 [ 300/1251 ( 24%)]  Loss: 3.459 (3.69)  Time: 0.770s, 1329.98/s  (0.785s, 1304.14/s)  LR: 7.343e-04  Data: 0.009 (0.016)
Train: 208 [ 350/1251 ( 28%)]  Loss: 3.427 (3.66)  Time: 0.813s, 1259.84/s  (0.785s, 1303.99/s)  LR: 7.343e-04  Data: 0.009 (0.015)
Train: 208 [ 400/1251 ( 32%)]  Loss: 3.831 (3.68)  Time: 0.780s, 1313.15/s  (0.785s, 1305.24/s)  LR: 7.343e-04  Data: 0.011 (0.014)
Train: 208 [ 450/1251 ( 36%)]  Loss: 3.564 (3.67)  Time: 0.778s, 1316.98/s  (0.784s, 1306.32/s)  LR: 7.343e-04  Data: 0.009 (0.014)
Train: 208 [ 500/1251 ( 40%)]  Loss: 3.432 (3.65)  Time: 0.787s, 1301.45/s  (0.784s, 1305.71/s)  LR: 7.343e-04  Data: 0.011 (0.013)
Train: 208 [ 550/1251 ( 44%)]  Loss: 3.851 (3.66)  Time: 0.806s, 1269.93/s  (0.784s, 1306.32/s)  LR: 7.343e-04  Data: 0.009 (0.013)
Train: 208 [ 600/1251 ( 48%)]  Loss: 3.619 (3.66)  Time: 0.775s, 1321.09/s  (0.784s, 1306.35/s)  LR: 7.343e-04  Data: 0.010 (0.013)
Train: 208 [ 650/1251 ( 52%)]  Loss: 3.844 (3.67)  Time: 0.772s, 1326.62/s  (0.784s, 1306.23/s)  LR: 7.343e-04  Data: 0.009 (0.013)
Train: 208 [ 700/1251 ( 56%)]  Loss: 3.480 (3.66)  Time: 0.774s, 1323.32/s  (0.784s, 1306.34/s)  LR: 7.343e-04  Data: 0.010 (0.012)
Train: 208 [ 750/1251 ( 60%)]  Loss: 3.480 (3.65)  Time: 0.775s, 1320.75/s  (0.784s, 1306.29/s)  LR: 7.343e-04  Data: 0.009 (0.012)
Train: 208 [ 800/1251 ( 64%)]  Loss: 3.354 (3.63)  Time: 0.774s, 1322.89/s  (0.784s, 1306.38/s)  LR: 7.343e-04  Data: 0.010 (0.012)
Train: 208 [ 850/1251 ( 68%)]  Loss: 3.426 (3.62)  Time: 0.771s, 1327.77/s  (0.784s, 1306.77/s)  LR: 7.343e-04  Data: 0.009 (0.012)
Train: 208 [ 900/1251 ( 72%)]  Loss: 3.584 (3.62)  Time: 0.773s, 1324.19/s  (0.784s, 1306.36/s)  LR: 7.343e-04  Data: 0.010 (0.012)
Train: 208 [ 950/1251 ( 76%)]  Loss: 3.477 (3.61)  Time: 0.776s, 1319.93/s  (0.784s, 1306.56/s)  LR: 7.343e-04  Data: 0.010 (0.012)
Train: 208 [1000/1251 ( 80%)]  Loss: 3.773 (3.62)  Time: 0.772s, 1326.72/s  (0.784s, 1305.61/s)  LR: 7.343e-04  Data: 0.010 (0.012)
Train: 208 [1050/1251 ( 84%)]  Loss: 3.669 (3.62)  Time: 0.777s, 1317.16/s  (0.785s, 1304.65/s)  LR: 7.343e-04  Data: 0.010 (0.012)
Train: 208 [1100/1251 ( 88%)]  Loss: 3.530 (3.62)  Time: 0.782s, 1309.40/s  (0.785s, 1304.06/s)  LR: 7.343e-04  Data: 0.010 (0.012)
Train: 208 [1150/1251 ( 92%)]  Loss: 3.554 (3.61)  Time: 0.771s, 1327.97/s  (0.785s, 1304.36/s)  LR: 7.343e-04  Data: 0.010 (0.012)
Train: 208 [1200/1251 ( 96%)]  Loss: 3.836 (3.62)  Time: 0.773s, 1324.24/s  (0.785s, 1304.42/s)  LR: 7.343e-04  Data: 0.010 (0.011)
Train: 208 [1250/1251 (100%)]  Loss: 3.753 (3.63)  Time: 0.758s, 1350.27/s  (0.785s, 1304.89/s)  LR: 7.343e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.512 (1.512)  Loss:  0.7803 (0.7803)  Acc@1: 88.9648 (88.9648)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.194 (0.563)  Loss:  0.9326 (1.3469)  Acc@1: 83.9623 (73.1940)  Acc@5: 95.5189 (91.7340)
Train: 209 [   0/1251 (  0%)]  Loss: 3.375 (3.38)  Time: 2.348s,  436.03/s  (2.348s,  436.03/s)  LR: 7.320e-04  Data: 1.618 (1.618)
Train: 209 [  50/1251 (  4%)]  Loss: 3.638 (3.51)  Time: 0.820s, 1249.52/s  (0.814s, 1257.52/s)  LR: 7.320e-04  Data: 0.011 (0.045)
Train: 209 [ 100/1251 (  8%)]  Loss: 3.374 (3.46)  Time: 0.777s, 1318.07/s  (0.805s, 1272.81/s)  LR: 7.320e-04  Data: 0.009 (0.028)
Train: 209 [ 150/1251 ( 12%)]  Loss: 3.529 (3.48)  Time: 0.775s, 1321.46/s  (0.798s, 1283.78/s)  LR: 7.320e-04  Data: 0.009 (0.022)
Train: 209 [ 200/1251 ( 16%)]  Loss: 3.460 (3.48)  Time: 0.775s, 1321.85/s  (0.794s, 1289.83/s)  LR: 7.320e-04  Data: 0.010 (0.019)
Train: 209 [ 250/1251 ( 20%)]  Loss: 3.773 (3.52)  Time: 0.774s, 1323.66/s  (0.792s, 1292.48/s)  LR: 7.320e-04  Data: 0.011 (0.017)
Train: 209 [ 300/1251 ( 24%)]  Loss: 3.703 (3.55)  Time: 0.777s, 1318.22/s  (0.791s, 1294.73/s)  LR: 7.320e-04  Data: 0.010 (0.016)
Train: 209 [ 350/1251 ( 28%)]  Loss: 3.379 (3.53)  Time: 0.801s, 1279.13/s  (0.790s, 1295.79/s)  LR: 7.320e-04  Data: 0.009 (0.015)
Train: 209 [ 400/1251 ( 32%)]  Loss: 3.207 (3.49)  Time: 0.778s, 1316.51/s  (0.789s, 1297.05/s)  LR: 7.320e-04  Data: 0.009 (0.014)
Train: 209 [ 450/1251 ( 36%)]  Loss: 3.852 (3.53)  Time: 0.783s, 1308.47/s  (0.789s, 1298.24/s)  LR: 7.320e-04  Data: 0.012 (0.014)
Train: 209 [ 500/1251 ( 40%)]  Loss: 3.849 (3.56)  Time: 0.782s, 1310.12/s  (0.788s, 1299.39/s)  LR: 7.320e-04  Data: 0.009 (0.013)
Train: 209 [ 550/1251 ( 44%)]  Loss: 3.624 (3.56)  Time: 0.817s, 1252.76/s  (0.788s, 1300.18/s)  LR: 7.320e-04  Data: 0.010 (0.013)
Train: 209 [ 600/1251 ( 48%)]  Loss: 3.691 (3.57)  Time: 0.773s, 1325.19/s  (0.787s, 1301.84/s)  LR: 7.320e-04  Data: 0.010 (0.013)
Train: 209 [ 650/1251 ( 52%)]  Loss: 3.600 (3.58)  Time: 0.773s, 1324.70/s  (0.789s, 1298.43/s)  LR: 7.320e-04  Data: 0.009 (0.013)
Train: 209 [ 700/1251 ( 56%)]  Loss: 3.750 (3.59)  Time: 0.779s, 1314.44/s  (0.788s, 1299.51/s)  LR: 7.320e-04  Data: 0.011 (0.013)
Train: 209 [ 750/1251 ( 60%)]  Loss: 3.523 (3.58)  Time: 0.777s, 1317.19/s  (0.788s, 1299.71/s)  LR: 7.320e-04  Data: 0.010 (0.012)
Train: 209 [ 800/1251 ( 64%)]  Loss: 3.820 (3.60)  Time: 0.777s, 1317.74/s  (0.787s, 1300.67/s)  LR: 7.320e-04  Data: 0.010 (0.012)
Train: 209 [ 850/1251 ( 68%)]  Loss: 3.420 (3.59)  Time: 0.782s, 1310.20/s  (0.787s, 1301.14/s)  LR: 7.320e-04  Data: 0.010 (0.012)
Train: 209 [ 900/1251 ( 72%)]  Loss: 3.532 (3.58)  Time: 0.788s, 1300.14/s  (0.787s, 1300.53/s)  LR: 7.320e-04  Data: 0.012 (0.012)
Train: 209 [ 950/1251 ( 76%)]  Loss: 3.926 (3.60)  Time: 0.775s, 1320.56/s  (0.787s, 1301.37/s)  LR: 7.320e-04  Data: 0.013 (0.012)
Train: 209 [1000/1251 ( 80%)]  Loss: 4.102 (3.63)  Time: 0.772s, 1326.11/s  (0.787s, 1301.83/s)  LR: 7.320e-04  Data: 0.010 (0.012)
Train: 209 [1050/1251 ( 84%)]  Loss: 3.567 (3.62)  Time: 0.796s, 1287.12/s  (0.786s, 1302.22/s)  LR: 7.320e-04  Data: 0.016 (0.012)
Train: 209 [1100/1251 ( 88%)]  Loss: 3.588 (3.62)  Time: 0.782s, 1310.16/s  (0.786s, 1302.55/s)  LR: 7.320e-04  Data: 0.010 (0.012)
Train: 209 [1150/1251 ( 92%)]  Loss: 3.490 (3.62)  Time: 0.799s, 1282.17/s  (0.786s, 1302.61/s)  LR: 7.320e-04  Data: 0.011 (0.012)
Train: 209 [1200/1251 ( 96%)]  Loss: 3.927 (3.63)  Time: 0.893s, 1146.31/s  (0.786s, 1302.59/s)  LR: 7.320e-04  Data: 0.011 (0.012)
Train: 209 [1250/1251 (100%)]  Loss: 3.506 (3.62)  Time: 0.795s, 1287.68/s  (0.787s, 1301.63/s)  LR: 7.320e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.558 (1.558)  Loss:  0.7529 (0.7529)  Acc@1: 88.6719 (88.6719)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.193 (0.565)  Loss:  0.8936 (1.3599)  Acc@1: 83.9623 (73.0960)  Acc@5: 96.5802 (91.7500)
Train: 210 [   0/1251 (  0%)]  Loss: 3.594 (3.59)  Time: 2.331s,  439.31/s  (2.331s,  439.31/s)  LR: 7.297e-04  Data: 1.598 (1.598)
Train: 210 [  50/1251 (  4%)]  Loss: 3.401 (3.50)  Time: 0.790s, 1296.95/s  (0.818s, 1252.32/s)  LR: 7.297e-04  Data: 0.010 (0.047)
Train: 210 [ 100/1251 (  8%)]  Loss: 3.522 (3.51)  Time: 0.786s, 1302.22/s  (0.803s, 1275.68/s)  LR: 7.297e-04  Data: 0.010 (0.029)
Train: 210 [ 150/1251 ( 12%)]  Loss: 3.971 (3.62)  Time: 0.772s, 1326.12/s  (0.794s, 1289.02/s)  LR: 7.297e-04  Data: 0.010 (0.023)
Train: 210 [ 200/1251 ( 16%)]  Loss: 3.542 (3.61)  Time: 0.772s, 1327.11/s  (0.792s, 1292.26/s)  LR: 7.297e-04  Data: 0.010 (0.020)
Train: 210 [ 250/1251 ( 20%)]  Loss: 3.458 (3.58)  Time: 0.784s, 1305.47/s  (0.791s, 1295.19/s)  LR: 7.297e-04  Data: 0.010 (0.018)
Train: 210 [ 300/1251 ( 24%)]  Loss: 3.450 (3.56)  Time: 0.834s, 1228.05/s  (0.790s, 1296.36/s)  LR: 7.297e-04  Data: 0.010 (0.016)
Train: 210 [ 350/1251 ( 28%)]  Loss: 3.899 (3.60)  Time: 0.777s, 1317.30/s  (0.788s, 1299.14/s)  LR: 7.297e-04  Data: 0.010 (0.015)
Train: 210 [ 400/1251 ( 32%)]  Loss: 3.589 (3.60)  Time: 0.772s, 1326.15/s  (0.789s, 1298.59/s)  LR: 7.297e-04  Data: 0.010 (0.015)
Train: 210 [ 450/1251 ( 36%)]  Loss: 3.556 (3.60)  Time: 0.778s, 1316.04/s  (0.788s, 1299.92/s)  LR: 7.297e-04  Data: 0.011 (0.014)
Train: 210 [ 500/1251 ( 40%)]  Loss: 3.838 (3.62)  Time: 0.779s, 1314.15/s  (0.788s, 1300.06/s)  LR: 7.297e-04  Data: 0.011 (0.014)
Train: 210 [ 550/1251 ( 44%)]  Loss: 3.696 (3.63)  Time: 0.815s, 1255.69/s  (0.787s, 1300.49/s)  LR: 7.297e-04  Data: 0.009 (0.013)
Train: 210 [ 600/1251 ( 48%)]  Loss: 3.448 (3.61)  Time: 0.773s, 1324.76/s  (0.789s, 1298.65/s)  LR: 7.297e-04  Data: 0.010 (0.013)
Train: 210 [ 650/1251 ( 52%)]  Loss: 3.963 (3.64)  Time: 0.775s, 1321.36/s  (0.788s, 1298.91/s)  LR: 7.297e-04  Data: 0.010 (0.013)
Train: 210 [ 700/1251 ( 56%)]  Loss: 3.403 (3.62)  Time: 0.772s, 1327.10/s  (0.789s, 1297.11/s)  LR: 7.297e-04  Data: 0.009 (0.013)
Train: 210 [ 750/1251 ( 60%)]  Loss: 3.502 (3.61)  Time: 0.772s, 1325.76/s  (0.789s, 1297.86/s)  LR: 7.297e-04  Data: 0.010 (0.013)
Train: 210 [ 800/1251 ( 64%)]  Loss: 3.574 (3.61)  Time: 0.772s, 1326.92/s  (0.788s, 1298.79/s)  LR: 7.297e-04  Data: 0.009 (0.012)
Train: 210 [ 850/1251 ( 68%)]  Loss: 3.422 (3.60)  Time: 0.782s, 1310.19/s  (0.788s, 1299.36/s)  LR: 7.297e-04  Data: 0.010 (0.012)
Train: 210 [ 900/1251 ( 72%)]  Loss: 3.256 (3.58)  Time: 0.783s, 1307.85/s  (0.788s, 1299.65/s)  LR: 7.297e-04  Data: 0.010 (0.012)
Train: 210 [ 950/1251 ( 76%)]  Loss: 3.402 (3.57)  Time: 0.775s, 1321.44/s  (0.788s, 1300.23/s)  LR: 7.297e-04  Data: 0.010 (0.012)
Train: 210 [1000/1251 ( 80%)]  Loss: 3.690 (3.58)  Time: 0.783s, 1306.98/s  (0.787s, 1300.63/s)  LR: 7.297e-04  Data: 0.009 (0.012)
Train: 210 [1050/1251 ( 84%)]  Loss: 3.360 (3.57)  Time: 0.832s, 1230.74/s  (0.787s, 1301.15/s)  LR: 7.297e-04  Data: 0.009 (0.012)
Train: 210 [1100/1251 ( 88%)]  Loss: 3.636 (3.57)  Time: 0.813s, 1258.90/s  (0.787s, 1301.41/s)  LR: 7.297e-04  Data: 0.009 (0.012)
Train: 210 [1150/1251 ( 92%)]  Loss: 3.544 (3.57)  Time: 0.773s, 1325.35/s  (0.787s, 1301.61/s)  LR: 7.297e-04  Data: 0.010 (0.012)
Train: 210 [1200/1251 ( 96%)]  Loss: 3.448 (3.57)  Time: 0.771s, 1328.04/s  (0.787s, 1301.60/s)  LR: 7.297e-04  Data: 0.009 (0.012)
Train: 210 [1250/1251 (100%)]  Loss: 3.227 (3.55)  Time: 0.758s, 1350.59/s  (0.787s, 1301.74/s)  LR: 7.297e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.520 (1.520)  Loss:  0.8398 (0.8398)  Acc@1: 88.9648 (88.9648)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.194 (0.559)  Loss:  0.9160 (1.4125)  Acc@1: 84.7877 (73.4800)  Acc@5: 96.1085 (91.8500)
Train: 211 [   0/1251 (  0%)]  Loss: 3.467 (3.47)  Time: 2.472s,  414.24/s  (2.472s,  414.24/s)  LR: 7.274e-04  Data: 1.743 (1.743)
Train: 211 [  50/1251 (  4%)]  Loss: 3.466 (3.47)  Time: 0.830s, 1233.71/s  (0.840s, 1218.63/s)  LR: 7.274e-04  Data: 0.011 (0.054)
Train: 211 [ 100/1251 (  8%)]  Loss: 3.460 (3.46)  Time: 0.834s, 1227.26/s  (0.812s, 1260.64/s)  LR: 7.274e-04  Data: 0.010 (0.032)
Train: 211 [ 150/1251 ( 12%)]  Loss: 3.611 (3.50)  Time: 0.847s, 1209.60/s  (0.802s, 1276.37/s)  LR: 7.274e-04  Data: 0.010 (0.025)
Train: 211 [ 200/1251 ( 16%)]  Loss: 3.529 (3.51)  Time: 0.776s, 1320.01/s  (0.797s, 1284.51/s)  LR: 7.274e-04  Data: 0.010 (0.021)
Train: 211 [ 250/1251 ( 20%)]  Loss: 3.788 (3.55)  Time: 0.781s, 1310.85/s  (0.794s, 1290.23/s)  LR: 7.274e-04  Data: 0.010 (0.019)
Train: 211 [ 300/1251 ( 24%)]  Loss: 3.434 (3.54)  Time: 0.774s, 1323.72/s  (0.792s, 1292.12/s)  LR: 7.274e-04  Data: 0.010 (0.017)
Train: 211 [ 350/1251 ( 28%)]  Loss: 3.476 (3.53)  Time: 0.777s, 1317.42/s  (0.791s, 1294.62/s)  LR: 7.274e-04  Data: 0.010 (0.016)
Train: 211 [ 400/1251 ( 32%)]  Loss: 3.550 (3.53)  Time: 0.772s, 1326.50/s  (0.791s, 1295.21/s)  LR: 7.274e-04  Data: 0.010 (0.016)
Train: 211 [ 450/1251 ( 36%)]  Loss: 3.434 (3.52)  Time: 0.783s, 1307.21/s  (0.790s, 1295.53/s)  LR: 7.274e-04  Data: 0.010 (0.015)
Train: 211 [ 500/1251 ( 40%)]  Loss: 3.903 (3.56)  Time: 0.773s, 1323.91/s  (0.791s, 1294.98/s)  LR: 7.274e-04  Data: 0.011 (0.014)
Train: 211 [ 550/1251 ( 44%)]  Loss: 4.028 (3.60)  Time: 0.772s, 1326.66/s  (0.790s, 1296.62/s)  LR: 7.274e-04  Data: 0.010 (0.014)
Train: 211 [ 600/1251 ( 48%)]  Loss: 3.930 (3.62)  Time: 0.776s, 1319.16/s  (0.791s, 1295.01/s)  LR: 7.274e-04  Data: 0.010 (0.014)
Train: 211 [ 650/1251 ( 52%)]  Loss: 3.466 (3.61)  Time: 0.773s, 1325.45/s  (0.790s, 1295.89/s)  LR: 7.274e-04  Data: 0.010 (0.013)
Train: 211 [ 700/1251 ( 56%)]  Loss: 3.765 (3.62)  Time: 0.772s, 1327.17/s  (0.790s, 1296.57/s)  LR: 7.274e-04  Data: 0.010 (0.013)
Train: 211 [ 750/1251 ( 60%)]  Loss: 3.590 (3.62)  Time: 0.857s, 1194.30/s  (0.790s, 1296.84/s)  LR: 7.274e-04  Data: 0.010 (0.013)
Train: 211 [ 800/1251 ( 64%)]  Loss: 3.453 (3.61)  Time: 0.814s, 1257.96/s  (0.789s, 1297.19/s)  LR: 7.274e-04  Data: 0.009 (0.013)
Train: 211 [ 850/1251 ( 68%)]  Loss: 3.580 (3.61)  Time: 0.772s, 1326.38/s  (0.789s, 1297.85/s)  LR: 7.274e-04  Data: 0.010 (0.013)
Train: 211 [ 900/1251 ( 72%)]  Loss: 3.750 (3.61)  Time: 0.782s, 1310.18/s  (0.789s, 1298.54/s)  LR: 7.274e-04  Data: 0.010 (0.013)
Train: 211 [ 950/1251 ( 76%)]  Loss: 3.583 (3.61)  Time: 0.774s, 1323.40/s  (0.788s, 1299.65/s)  LR: 7.274e-04  Data: 0.010 (0.012)
Train: 211 [1000/1251 ( 80%)]  Loss: 3.441 (3.60)  Time: 0.782s, 1309.67/s  (0.788s, 1299.99/s)  LR: 7.274e-04  Data: 0.013 (0.012)
Train: 211 [1050/1251 ( 84%)]  Loss: 3.774 (3.61)  Time: 0.785s, 1305.11/s  (0.788s, 1298.94/s)  LR: 7.274e-04  Data: 0.009 (0.012)
Train: 211 [1100/1251 ( 88%)]  Loss: 3.673 (3.62)  Time: 0.827s, 1237.65/s  (0.788s, 1298.97/s)  LR: 7.274e-04  Data: 0.013 (0.012)
Train: 211 [1150/1251 ( 92%)]  Loss: 3.573 (3.61)  Time: 0.773s, 1325.38/s  (0.788s, 1299.08/s)  LR: 7.274e-04  Data: 0.010 (0.012)
Train: 211 [1200/1251 ( 96%)]  Loss: 3.488 (3.61)  Time: 0.776s, 1320.30/s  (0.788s, 1298.94/s)  LR: 7.274e-04  Data: 0.009 (0.012)
Train: 211 [1250/1251 (100%)]  Loss: 3.437 (3.60)  Time: 0.804s, 1274.21/s  (0.789s, 1298.41/s)  LR: 7.274e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.566 (1.566)  Loss:  0.8564 (0.8564)  Acc@1: 88.5742 (88.5742)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.194 (0.568)  Loss:  0.8774 (1.3724)  Acc@1: 85.7311 (73.9200)  Acc@5: 96.3443 (92.3200)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-200.pth.tar', 73.95200006835937)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-211.pth.tar', 73.92000000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-203.pth.tar', 73.89600004150391)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-202.pth.tar', 73.8080001171875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-194.pth.tar', 73.72600004150391)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-207.pth.tar', 73.70400009765625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-190.pth.tar', 73.70399999023438)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-201.pth.tar', 73.69800013916016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-197.pth.tar', 73.69800000976562)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-205.pth.tar', 73.69000004394532)

Train: 212 [   0/1251 (  0%)]  Loss: 3.795 (3.80)  Time: 2.265s,  452.09/s  (2.265s,  452.09/s)  LR: 7.251e-04  Data: 1.518 (1.518)
Train: 212 [  50/1251 (  4%)]  Loss: 3.686 (3.74)  Time: 0.773s, 1324.81/s  (0.825s, 1240.78/s)  LR: 7.251e-04  Data: 0.010 (0.048)
Train: 212 [ 100/1251 (  8%)]  Loss: 3.580 (3.69)  Time: 0.772s, 1327.06/s  (0.804s, 1273.01/s)  LR: 7.251e-04  Data: 0.009 (0.029)
Train: 212 [ 150/1251 ( 12%)]  Loss: 3.700 (3.69)  Time: 0.775s, 1320.54/s  (0.798s, 1283.74/s)  LR: 7.251e-04  Data: 0.010 (0.023)
Train: 212 [ 200/1251 ( 16%)]  Loss: 3.564 (3.67)  Time: 0.773s, 1324.80/s  (0.795s, 1288.06/s)  LR: 7.251e-04  Data: 0.010 (0.020)
Train: 212 [ 250/1251 ( 20%)]  Loss: 3.588 (3.65)  Time: 0.790s, 1296.52/s  (0.791s, 1294.33/s)  LR: 7.251e-04  Data: 0.010 (0.018)
Train: 212 [ 300/1251 ( 24%)]  Loss: 3.681 (3.66)  Time: 0.779s, 1314.53/s  (0.791s, 1294.42/s)  LR: 7.251e-04  Data: 0.010 (0.016)
Train: 212 [ 350/1251 ( 28%)]  Loss: 3.850 (3.68)  Time: 0.772s, 1326.16/s  (0.790s, 1296.30/s)  LR: 7.251e-04  Data: 0.010 (0.015)
Train: 212 [ 400/1251 ( 32%)]  Loss: 3.704 (3.68)  Time: 0.783s, 1307.35/s  (0.790s, 1296.26/s)  LR: 7.251e-04  Data: 0.013 (0.015)
Train: 212 [ 450/1251 ( 36%)]  Loss: 3.513 (3.67)  Time: 0.836s, 1225.09/s  (0.791s, 1293.79/s)  LR: 7.251e-04  Data: 0.009 (0.014)
Train: 212 [ 500/1251 ( 40%)]  Loss: 3.779 (3.68)  Time: 0.780s, 1313.05/s  (0.790s, 1295.50/s)  LR: 7.251e-04  Data: 0.009 (0.014)
Train: 212 [ 550/1251 ( 44%)]  Loss: 3.447 (3.66)  Time: 0.811s, 1262.17/s  (0.790s, 1296.75/s)  LR: 7.251e-04  Data: 0.011 (0.014)
Train: 212 [ 600/1251 ( 48%)]  Loss: 3.338 (3.63)  Time: 0.773s, 1324.26/s  (0.789s, 1297.48/s)  LR: 7.251e-04  Data: 0.010 (0.013)
Train: 212 [ 650/1251 ( 52%)]  Loss: 3.684 (3.64)  Time: 0.824s, 1242.41/s  (0.789s, 1298.06/s)  LR: 7.251e-04  Data: 0.013 (0.013)
Train: 212 [ 700/1251 ( 56%)]  Loss: 3.386 (3.62)  Time: 0.776s, 1319.40/s  (0.790s, 1296.27/s)  LR: 7.251e-04  Data: 0.010 (0.013)
Train: 212 [ 750/1251 ( 60%)]  Loss: 3.397 (3.61)  Time: 0.772s, 1326.37/s  (0.789s, 1297.27/s)  LR: 7.251e-04  Data: 0.010 (0.013)
Train: 212 [ 800/1251 ( 64%)]  Loss: 3.446 (3.60)  Time: 0.772s, 1325.71/s  (0.789s, 1298.08/s)  LR: 7.251e-04  Data: 0.009 (0.012)
Train: 212 [ 850/1251 ( 68%)]  Loss: 3.367 (3.58)  Time: 0.774s, 1322.67/s  (0.789s, 1298.55/s)  LR: 7.251e-04  Data: 0.010 (0.012)
Train: 212 [ 900/1251 ( 72%)]  Loss: 3.183 (3.56)  Time: 0.776s, 1320.20/s  (0.788s, 1299.39/s)  LR: 7.251e-04  Data: 0.010 (0.012)
Train: 212 [ 950/1251 ( 76%)]  Loss: 3.655 (3.57)  Time: 0.806s, 1271.09/s  (0.788s, 1299.57/s)  LR: 7.251e-04  Data: 0.010 (0.012)
Train: 212 [1000/1251 ( 80%)]  Loss: 3.471 (3.56)  Time: 0.772s, 1326.06/s  (0.788s, 1300.28/s)  LR: 7.251e-04  Data: 0.010 (0.012)
Train: 212 [1050/1251 ( 84%)]  Loss: 3.624 (3.57)  Time: 0.776s, 1320.14/s  (0.788s, 1299.58/s)  LR: 7.251e-04  Data: 0.010 (0.012)
Train: 212 [1100/1251 ( 88%)]  Loss: 3.580 (3.57)  Time: 0.775s, 1321.90/s  (0.787s, 1300.37/s)  LR: 7.251e-04  Data: 0.010 (0.012)
Train: 212 [1150/1251 ( 92%)]  Loss: 3.367 (3.56)  Time: 0.774s, 1323.45/s  (0.787s, 1300.53/s)  LR: 7.251e-04  Data: 0.010 (0.012)
Train: 212 [1200/1251 ( 96%)]  Loss: 3.814 (3.57)  Time: 0.773s, 1325.55/s  (0.787s, 1301.11/s)  LR: 7.251e-04  Data: 0.010 (0.012)
Train: 212 [1250/1251 (100%)]  Loss: 3.865 (3.58)  Time: 0.763s, 1342.73/s  (0.787s, 1301.33/s)  LR: 7.251e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.561 (1.561)  Loss:  0.8276 (0.8276)  Acc@1: 88.9648 (88.9648)  Acc@5: 96.2891 (96.2891)
Test: [  48/48]  Time: 0.194 (0.569)  Loss:  0.8452 (1.3710)  Acc@1: 83.7264 (73.4240)  Acc@5: 96.6981 (91.9360)
Train: 213 [   0/1251 (  0%)]  Loss: 3.832 (3.83)  Time: 2.260s,  453.16/s  (2.260s,  453.16/s)  LR: 7.228e-04  Data: 1.530 (1.530)
Train: 213 [  50/1251 (  4%)]  Loss: 3.832 (3.83)  Time: 0.778s, 1315.73/s  (0.816s, 1254.47/s)  LR: 7.228e-04  Data: 0.010 (0.046)
Train: 213 [ 100/1251 (  8%)]  Loss: 3.562 (3.74)  Time: 0.777s, 1317.91/s  (0.798s, 1282.45/s)  LR: 7.228e-04  Data: 0.009 (0.028)
Train: 213 [ 150/1251 ( 12%)]  Loss: 3.377 (3.65)  Time: 0.782s, 1309.76/s  (0.795s, 1287.91/s)  LR: 7.228e-04  Data: 0.010 (0.022)
Train: 213 [ 200/1251 ( 16%)]  Loss: 3.907 (3.70)  Time: 0.845s, 1212.27/s  (0.794s, 1289.92/s)  LR: 7.228e-04  Data: 0.010 (0.019)
Train: 213 [ 250/1251 ( 20%)]  Loss: 3.527 (3.67)  Time: 0.773s, 1325.36/s  (0.792s, 1292.41/s)  LR: 7.228e-04  Data: 0.010 (0.017)
Train: 213 [ 300/1251 ( 24%)]  Loss: 3.522 (3.65)  Time: 0.774s, 1323.17/s  (0.790s, 1295.62/s)  LR: 7.228e-04  Data: 0.009 (0.016)
Train: 213 [ 350/1251 ( 28%)]  Loss: 3.372 (3.62)  Time: 0.773s, 1325.12/s  (0.789s, 1297.62/s)  LR: 7.228e-04  Data: 0.010 (0.015)
Train: 213 [ 400/1251 ( 32%)]  Loss: 3.237 (3.57)  Time: 0.785s, 1304.66/s  (0.788s, 1299.78/s)  LR: 7.228e-04  Data: 0.009 (0.014)
Train: 213 [ 450/1251 ( 36%)]  Loss: 3.519 (3.57)  Time: 0.782s, 1309.72/s  (0.787s, 1301.58/s)  LR: 7.228e-04  Data: 0.010 (0.014)
Train: 213 [ 500/1251 ( 40%)]  Loss: 3.675 (3.58)  Time: 0.771s, 1327.41/s  (0.786s, 1302.65/s)  LR: 7.228e-04  Data: 0.010 (0.014)
Train: 213 [ 550/1251 ( 44%)]  Loss: 3.563 (3.58)  Time: 0.772s, 1326.74/s  (0.786s, 1302.90/s)  LR: 7.228e-04  Data: 0.009 (0.013)
Train: 213 [ 600/1251 ( 48%)]  Loss: 3.238 (3.55)  Time: 0.774s, 1323.07/s  (0.786s, 1303.34/s)  LR: 7.228e-04  Data: 0.010 (0.013)
Train: 213 [ 650/1251 ( 52%)]  Loss: 3.350 (3.54)  Time: 0.774s, 1322.57/s  (0.785s, 1304.20/s)  LR: 7.228e-04  Data: 0.011 (0.013)
Train: 213 [ 700/1251 ( 56%)]  Loss: 3.882 (3.56)  Time: 0.772s, 1326.82/s  (0.785s, 1304.82/s)  LR: 7.228e-04  Data: 0.010 (0.013)
Train: 213 [ 750/1251 ( 60%)]  Loss: 3.988 (3.59)  Time: 0.774s, 1322.56/s  (0.784s, 1305.66/s)  LR: 7.228e-04  Data: 0.010 (0.012)
Train: 213 [ 800/1251 ( 64%)]  Loss: 3.764 (3.60)  Time: 0.774s, 1323.57/s  (0.784s, 1306.20/s)  LR: 7.228e-04  Data: 0.011 (0.012)
Train: 213 [ 850/1251 ( 68%)]  Loss: 3.580 (3.60)  Time: 0.778s, 1316.06/s  (0.784s, 1306.39/s)  LR: 7.228e-04  Data: 0.010 (0.012)
Train: 213 [ 900/1251 ( 72%)]  Loss: 3.698 (3.60)  Time: 0.784s, 1306.31/s  (0.784s, 1306.29/s)  LR: 7.228e-04  Data: 0.010 (0.012)
Train: 213 [ 950/1251 ( 76%)]  Loss: 3.661 (3.60)  Time: 0.838s, 1222.61/s  (0.784s, 1305.66/s)  LR: 7.228e-04  Data: 0.010 (0.012)
Train: 213 [1000/1251 ( 80%)]  Loss: 3.286 (3.59)  Time: 0.774s, 1322.84/s  (0.784s, 1305.93/s)  LR: 7.228e-04  Data: 0.010 (0.012)
Train: 213 [1050/1251 ( 84%)]  Loss: 3.722 (3.60)  Time: 0.772s, 1325.77/s  (0.784s, 1306.38/s)  LR: 7.228e-04  Data: 0.010 (0.012)
Train: 213 [1100/1251 ( 88%)]  Loss: 3.811 (3.60)  Time: 0.779s, 1314.88/s  (0.784s, 1306.19/s)  LR: 7.228e-04  Data: 0.009 (0.012)
Train: 213 [1150/1251 ( 92%)]  Loss: 3.408 (3.60)  Time: 0.835s, 1225.80/s  (0.784s, 1306.48/s)  LR: 7.228e-04  Data: 0.009 (0.012)
Train: 213 [1200/1251 ( 96%)]  Loss: 3.893 (3.61)  Time: 0.773s, 1324.30/s  (0.784s, 1306.58/s)  LR: 7.228e-04  Data: 0.010 (0.011)
Train: 213 [1250/1251 (100%)]  Loss: 3.545 (3.61)  Time: 0.759s, 1348.87/s  (0.784s, 1306.57/s)  LR: 7.228e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.519 (1.519)  Loss:  0.7495 (0.7495)  Acc@1: 88.8672 (88.8672)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.194 (0.572)  Loss:  0.8325 (1.3044)  Acc@1: 85.9670 (74.1360)  Acc@5: 96.8160 (92.2500)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-213.pth.tar', 74.13599998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-200.pth.tar', 73.95200006835937)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-211.pth.tar', 73.92000000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-203.pth.tar', 73.89600004150391)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-202.pth.tar', 73.8080001171875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-194.pth.tar', 73.72600004150391)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-207.pth.tar', 73.70400009765625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-190.pth.tar', 73.70399999023438)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-201.pth.tar', 73.69800013916016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-197.pth.tar', 73.69800000976562)

Train: 214 [   0/1251 (  0%)]  Loss: 3.609 (3.61)  Time: 2.322s,  441.05/s  (2.322s,  441.05/s)  LR: 7.204e-04  Data: 1.553 (1.553)
Train: 214 [  50/1251 (  4%)]  Loss: 3.694 (3.65)  Time: 0.781s, 1311.67/s  (0.821s, 1247.47/s)  LR: 7.204e-04  Data: 0.009 (0.044)
Train: 214 [ 100/1251 (  8%)]  Loss: 3.663 (3.66)  Time: 0.773s, 1325.37/s  (0.800s, 1279.85/s)  LR: 7.204e-04  Data: 0.009 (0.027)
Train: 214 [ 150/1251 ( 12%)]  Loss: 3.785 (3.69)  Time: 0.775s, 1322.10/s  (0.793s, 1291.86/s)  LR: 7.204e-04  Data: 0.010 (0.021)
Train: 214 [ 200/1251 ( 16%)]  Loss: 3.844 (3.72)  Time: 0.827s, 1237.93/s  (0.792s, 1292.71/s)  LR: 7.204e-04  Data: 0.009 (0.019)
Train: 214 [ 250/1251 ( 20%)]  Loss: 3.723 (3.72)  Time: 0.776s, 1320.25/s  (0.790s, 1296.91/s)  LR: 7.204e-04  Data: 0.010 (0.017)
Train: 214 [ 300/1251 ( 24%)]  Loss: 3.247 (3.65)  Time: 0.820s, 1248.24/s  (0.788s, 1299.61/s)  LR: 7.204e-04  Data: 0.010 (0.016)
Train: 214 [ 350/1251 ( 28%)]  Loss: 3.370 (3.62)  Time: 0.785s, 1304.63/s  (0.787s, 1300.86/s)  LR: 7.204e-04  Data: 0.010 (0.015)
Train: 214 [ 400/1251 ( 32%)]  Loss: 3.567 (3.61)  Time: 0.785s, 1305.04/s  (0.786s, 1302.68/s)  LR: 7.204e-04  Data: 0.011 (0.014)
Train: 214 [ 450/1251 ( 36%)]  Loss: 3.498 (3.60)  Time: 0.780s, 1312.65/s  (0.786s, 1303.56/s)  LR: 7.204e-04  Data: 0.010 (0.014)
Train: 214 [ 500/1251 ( 40%)]  Loss: 3.639 (3.60)  Time: 0.774s, 1322.87/s  (0.785s, 1303.82/s)  LR: 7.204e-04  Data: 0.009 (0.013)
Train: 214 [ 550/1251 ( 44%)]  Loss: 3.704 (3.61)  Time: 0.771s, 1328.07/s  (0.786s, 1302.68/s)  LR: 7.204e-04  Data: 0.010 (0.013)
Train: 214 [ 600/1251 ( 48%)]  Loss: 3.573 (3.61)  Time: 0.780s, 1312.08/s  (0.786s, 1302.11/s)  LR: 7.204e-04  Data: 0.009 (0.013)
Train: 214 [ 650/1251 ( 52%)]  Loss: 3.527 (3.60)  Time: 0.772s, 1326.10/s  (0.786s, 1302.83/s)  LR: 7.204e-04  Data: 0.009 (0.013)
Train: 214 [ 700/1251 ( 56%)]  Loss: 3.588 (3.60)  Time: 0.773s, 1324.41/s  (0.786s, 1303.11/s)  LR: 7.204e-04  Data: 0.010 (0.012)
Train: 214 [ 750/1251 ( 60%)]  Loss: 3.972 (3.63)  Time: 0.774s, 1323.83/s  (0.785s, 1304.02/s)  LR: 7.204e-04  Data: 0.012 (0.012)
Train: 214 [ 800/1251 ( 64%)]  Loss: 3.390 (3.61)  Time: 0.816s, 1254.69/s  (0.785s, 1304.02/s)  LR: 7.204e-04  Data: 0.011 (0.012)
Train: 214 [ 850/1251 ( 68%)]  Loss: 3.982 (3.63)  Time: 0.782s, 1310.17/s  (0.785s, 1304.01/s)  LR: 7.204e-04  Data: 0.010 (0.012)
Train: 214 [ 900/1251 ( 72%)]  Loss: 3.264 (3.61)  Time: 0.817s, 1253.01/s  (0.785s, 1304.56/s)  LR: 7.204e-04  Data: 0.010 (0.012)
Train: 214 [ 950/1251 ( 76%)]  Loss: 3.470 (3.61)  Time: 0.777s, 1318.59/s  (0.785s, 1304.55/s)  LR: 7.204e-04  Data: 0.009 (0.012)
Train: 214 [1000/1251 ( 80%)]  Loss: 3.636 (3.61)  Time: 0.772s, 1326.25/s  (0.785s, 1305.23/s)  LR: 7.204e-04  Data: 0.009 (0.012)
Train: 214 [1050/1251 ( 84%)]  Loss: 3.670 (3.61)  Time: 0.824s, 1242.81/s  (0.785s, 1305.20/s)  LR: 7.204e-04  Data: 0.012 (0.012)
Train: 214 [1100/1251 ( 88%)]  Loss: 3.711 (3.61)  Time: 0.770s, 1329.20/s  (0.784s, 1305.54/s)  LR: 7.204e-04  Data: 0.010 (0.012)
Train: 214 [1150/1251 ( 92%)]  Loss: 3.682 (3.62)  Time: 0.774s, 1323.32/s  (0.785s, 1304.93/s)  LR: 7.204e-04  Data: 0.010 (0.012)
Train: 214 [1200/1251 ( 96%)]  Loss: 3.480 (3.61)  Time: 0.775s, 1321.98/s  (0.785s, 1304.74/s)  LR: 7.204e-04  Data: 0.009 (0.011)
Train: 214 [1250/1251 (100%)]  Loss: 3.700 (3.61)  Time: 0.770s, 1329.25/s  (0.785s, 1304.37/s)  LR: 7.204e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.554 (1.554)  Loss:  0.7310 (0.7310)  Acc@1: 88.9648 (88.9648)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.194 (0.564)  Loss:  0.8687 (1.3260)  Acc@1: 84.5519 (73.7940)  Acc@5: 96.2264 (92.2000)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-213.pth.tar', 74.13599998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-200.pth.tar', 73.95200006835937)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-211.pth.tar', 73.92000000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-203.pth.tar', 73.89600004150391)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-202.pth.tar', 73.8080001171875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-214.pth.tar', 73.79400001220704)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-194.pth.tar', 73.72600004150391)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-207.pth.tar', 73.70400009765625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-190.pth.tar', 73.70399999023438)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-201.pth.tar', 73.69800013916016)

Train: 215 [   0/1251 (  0%)]  Loss: 3.790 (3.79)  Time: 2.278s,  449.61/s  (2.278s,  449.61/s)  LR: 7.181e-04  Data: 1.533 (1.533)
Train: 215 [  50/1251 (  4%)]  Loss: 3.697 (3.74)  Time: 0.778s, 1316.03/s  (0.829s, 1235.62/s)  LR: 7.181e-04  Data: 0.010 (0.043)
Train: 215 [ 100/1251 (  8%)]  Loss: 3.331 (3.61)  Time: 0.777s, 1317.09/s  (0.809s, 1265.75/s)  LR: 7.181e-04  Data: 0.011 (0.027)
Train: 215 [ 150/1251 ( 12%)]  Loss: 3.353 (3.54)  Time: 0.771s, 1328.22/s  (0.803s, 1275.90/s)  LR: 7.181e-04  Data: 0.010 (0.021)
Train: 215 [ 200/1251 ( 16%)]  Loss: 3.679 (3.57)  Time: 0.777s, 1318.11/s  (0.802s, 1277.37/s)  LR: 7.181e-04  Data: 0.010 (0.019)
Train: 215 [ 250/1251 ( 20%)]  Loss: 3.366 (3.54)  Time: 0.779s, 1314.53/s  (0.798s, 1282.57/s)  LR: 7.181e-04  Data: 0.010 (0.017)
Train: 215 [ 300/1251 ( 24%)]  Loss: 3.480 (3.53)  Time: 0.772s, 1326.14/s  (0.797s, 1285.01/s)  LR: 7.181e-04  Data: 0.011 (0.016)
Train: 215 [ 350/1251 ( 28%)]  Loss: 3.822 (3.56)  Time: 0.787s, 1301.10/s  (0.794s, 1288.96/s)  LR: 7.181e-04  Data: 0.011 (0.015)
Train: 215 [ 400/1251 ( 32%)]  Loss: 3.814 (3.59)  Time: 0.780s, 1312.15/s  (0.793s, 1291.05/s)  LR: 7.181e-04  Data: 0.010 (0.015)
Train: 215 [ 450/1251 ( 36%)]  Loss: 3.389 (3.57)  Time: 0.773s, 1324.61/s  (0.794s, 1290.40/s)  LR: 7.181e-04  Data: 0.010 (0.014)
Train: 215 [ 500/1251 ( 40%)]  Loss: 3.413 (3.56)  Time: 0.823s, 1244.00/s  (0.792s, 1292.61/s)  LR: 7.181e-04  Data: 0.010 (0.014)
Train: 215 [ 550/1251 ( 44%)]  Loss: 4.037 (3.60)  Time: 0.817s, 1253.13/s  (0.791s, 1294.25/s)  LR: 7.181e-04  Data: 0.012 (0.013)
Train: 215 [ 600/1251 ( 48%)]  Loss: 3.861 (3.62)  Time: 0.774s, 1323.43/s  (0.790s, 1295.45/s)  LR: 7.181e-04  Data: 0.010 (0.013)
Train: 215 [ 650/1251 ( 52%)]  Loss: 3.444 (3.61)  Time: 0.776s, 1319.91/s  (0.791s, 1295.02/s)  LR: 7.181e-04  Data: 0.010 (0.013)
Train: 215 [ 700/1251 ( 56%)]  Loss: 3.703 (3.61)  Time: 0.772s, 1327.07/s  (0.791s, 1293.89/s)  LR: 7.181e-04  Data: 0.010 (0.013)
Train: 215 [ 750/1251 ( 60%)]  Loss: 3.726 (3.62)  Time: 0.786s, 1303.06/s  (0.791s, 1294.73/s)  LR: 7.181e-04  Data: 0.010 (0.013)
Train: 215 [ 800/1251 ( 64%)]  Loss: 3.527 (3.61)  Time: 0.774s, 1322.15/s  (0.792s, 1293.58/s)  LR: 7.181e-04  Data: 0.010 (0.013)
Train: 215 [ 850/1251 ( 68%)]  Loss: 3.220 (3.59)  Time: 0.784s, 1305.79/s  (0.791s, 1294.97/s)  LR: 7.181e-04  Data: 0.010 (0.012)
Train: 215 [ 900/1251 ( 72%)]  Loss: 3.297 (3.58)  Time: 0.773s, 1325.33/s  (0.790s, 1296.11/s)  LR: 7.181e-04  Data: 0.010 (0.012)
Train: 215 [ 950/1251 ( 76%)]  Loss: 3.733 (3.58)  Time: 0.781s, 1310.60/s  (0.790s, 1296.62/s)  LR: 7.181e-04  Data: 0.009 (0.012)
Train: 215 [1000/1251 ( 80%)]  Loss: 3.736 (3.59)  Time: 0.780s, 1313.56/s  (0.789s, 1297.34/s)  LR: 7.181e-04  Data: 0.010 (0.012)
Train: 215 [1050/1251 ( 84%)]  Loss: 3.493 (3.59)  Time: 0.772s, 1326.25/s  (0.789s, 1297.46/s)  LR: 7.181e-04  Data: 0.009 (0.012)
Train: 215 [1100/1251 ( 88%)]  Loss: 3.836 (3.60)  Time: 0.774s, 1322.81/s  (0.789s, 1297.84/s)  LR: 7.181e-04  Data: 0.010 (0.012)
Train: 215 [1150/1251 ( 92%)]  Loss: 3.420 (3.59)  Time: 0.786s, 1303.27/s  (0.789s, 1298.40/s)  LR: 7.181e-04  Data: 0.010 (0.012)
Train: 215 [1200/1251 ( 96%)]  Loss: 3.424 (3.58)  Time: 0.786s, 1303.05/s  (0.788s, 1299.35/s)  LR: 7.181e-04  Data: 0.013 (0.012)
Train: 215 [1250/1251 (100%)]  Loss: 3.728 (3.59)  Time: 0.764s, 1340.51/s  (0.788s, 1299.22/s)  LR: 7.181e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.566 (1.566)  Loss:  0.7803 (0.7803)  Acc@1: 88.7695 (88.7695)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.194 (0.565)  Loss:  0.8247 (1.3408)  Acc@1: 84.9057 (73.9700)  Acc@5: 95.7547 (92.1700)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-213.pth.tar', 74.13599998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-215.pth.tar', 73.97000003662109)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-200.pth.tar', 73.95200006835937)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-211.pth.tar', 73.92000000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-203.pth.tar', 73.89600004150391)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-202.pth.tar', 73.8080001171875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-214.pth.tar', 73.79400001220704)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-194.pth.tar', 73.72600004150391)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-207.pth.tar', 73.70400009765625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-190.pth.tar', 73.70399999023438)

Train: 216 [   0/1251 (  0%)]  Loss: 3.823 (3.82)  Time: 2.267s,  451.70/s  (2.267s,  451.70/s)  LR: 7.158e-04  Data: 1.530 (1.530)
Train: 216 [  50/1251 (  4%)]  Loss: 3.751 (3.79)  Time: 0.772s, 1326.68/s  (0.837s, 1223.69/s)  LR: 7.158e-04  Data: 0.011 (0.050)
Train: 216 [ 100/1251 (  8%)]  Loss: 3.632 (3.74)  Time: 0.773s, 1323.97/s  (0.811s, 1262.20/s)  LR: 7.158e-04  Data: 0.009 (0.030)
Train: 216 [ 150/1251 ( 12%)]  Loss: 3.630 (3.71)  Time: 0.773s, 1324.95/s  (0.801s, 1278.86/s)  LR: 7.158e-04  Data: 0.009 (0.024)
Train: 216 [ 200/1251 ( 16%)]  Loss: 3.327 (3.63)  Time: 0.781s, 1310.90/s  (0.797s, 1284.07/s)  LR: 7.158e-04  Data: 0.010 (0.020)
Train: 216 [ 250/1251 ( 20%)]  Loss: 3.722 (3.65)  Time: 0.811s, 1262.47/s  (0.794s, 1289.36/s)  LR: 7.158e-04  Data: 0.010 (0.018)
Train: 216 [ 300/1251 ( 24%)]  Loss: 3.691 (3.65)  Time: 0.773s, 1324.45/s  (0.792s, 1292.62/s)  LR: 7.158e-04  Data: 0.010 (0.017)
Train: 216 [ 350/1251 ( 28%)]  Loss: 3.458 (3.63)  Time: 0.772s, 1326.99/s  (0.793s, 1291.74/s)  LR: 7.158e-04  Data: 0.010 (0.016)
Train: 216 [ 400/1251 ( 32%)]  Loss: 3.620 (3.63)  Time: 0.772s, 1327.04/s  (0.792s, 1292.91/s)  LR: 7.158e-04  Data: 0.009 (0.015)
Train: 216 [ 450/1251 ( 36%)]  Loss: 3.513 (3.62)  Time: 0.772s, 1325.90/s  (0.790s, 1295.62/s)  LR: 7.158e-04  Data: 0.010 (0.015)
Train: 216 [ 500/1251 ( 40%)]  Loss: 3.564 (3.61)  Time: 0.779s, 1315.14/s  (0.790s, 1296.35/s)  LR: 7.158e-04  Data: 0.010 (0.014)
Train: 216 [ 550/1251 ( 44%)]  Loss: 3.603 (3.61)  Time: 0.778s, 1315.86/s  (0.789s, 1297.14/s)  LR: 7.158e-04  Data: 0.010 (0.014)
Train: 216 [ 600/1251 ( 48%)]  Loss: 3.285 (3.59)  Time: 0.804s, 1272.93/s  (0.789s, 1297.13/s)  LR: 7.158e-04  Data: 0.013 (0.014)
Train: 216 [ 650/1251 ( 52%)]  Loss: 3.393 (3.57)  Time: 0.779s, 1314.91/s  (0.790s, 1296.98/s)  LR: 7.158e-04  Data: 0.010 (0.013)
Train: 216 [ 700/1251 ( 56%)]  Loss: 3.774 (3.59)  Time: 0.703s, 1456.68/s  (0.789s, 1297.52/s)  LR: 7.158e-04  Data: 0.010 (0.013)
Train: 216 [ 750/1251 ( 60%)]  Loss: 3.641 (3.59)  Time: 0.773s, 1324.31/s  (0.788s, 1299.04/s)  LR: 7.158e-04  Data: 0.009 (0.013)
Train: 216 [ 800/1251 ( 64%)]  Loss: 3.509 (3.58)  Time: 0.789s, 1298.61/s  (0.788s, 1299.46/s)  LR: 7.158e-04  Data: 0.010 (0.013)
Train: 216 [ 850/1251 ( 68%)]  Loss: 3.770 (3.59)  Time: 0.773s, 1324.38/s  (0.787s, 1300.44/s)  LR: 7.158e-04  Data: 0.009 (0.012)
Train: 216 [ 900/1251 ( 72%)]  Loss: 3.702 (3.60)  Time: 0.773s, 1325.22/s  (0.787s, 1300.55/s)  LR: 7.158e-04  Data: 0.010 (0.012)
Train: 216 [ 950/1251 ( 76%)]  Loss: 3.455 (3.59)  Time: 0.816s, 1254.40/s  (0.787s, 1301.19/s)  LR: 7.158e-04  Data: 0.009 (0.012)
Train: 216 [1000/1251 ( 80%)]  Loss: 3.503 (3.59)  Time: 0.782s, 1308.80/s  (0.787s, 1301.28/s)  LR: 7.158e-04  Data: 0.010 (0.012)
Train: 216 [1050/1251 ( 84%)]  Loss: 3.435 (3.58)  Time: 0.773s, 1325.47/s  (0.787s, 1301.91/s)  LR: 7.158e-04  Data: 0.010 (0.012)
Train: 216 [1100/1251 ( 88%)]  Loss: 3.524 (3.58)  Time: 0.776s, 1319.95/s  (0.786s, 1302.30/s)  LR: 7.158e-04  Data: 0.009 (0.012)
Train: 216 [1150/1251 ( 92%)]  Loss: 3.620 (3.58)  Time: 0.773s, 1325.15/s  (0.786s, 1302.10/s)  LR: 7.158e-04  Data: 0.010 (0.012)
Train: 216 [1200/1251 ( 96%)]  Loss: 3.285 (3.57)  Time: 0.773s, 1324.51/s  (0.786s, 1302.74/s)  LR: 7.158e-04  Data: 0.009 (0.012)
Train: 216 [1250/1251 (100%)]  Loss: 3.032 (3.55)  Time: 0.760s, 1347.60/s  (0.786s, 1303.27/s)  LR: 7.158e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.535 (1.535)  Loss:  0.8340 (0.8340)  Acc@1: 90.5273 (90.5273)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.194 (0.568)  Loss:  0.9595 (1.3912)  Acc@1: 84.3160 (73.3480)  Acc@5: 95.7547 (91.8800)
Train: 217 [   0/1251 (  0%)]  Loss: 3.694 (3.69)  Time: 2.231s,  458.93/s  (2.231s,  458.93/s)  LR: 7.134e-04  Data: 1.458 (1.458)
Train: 217 [  50/1251 (  4%)]  Loss: 3.766 (3.73)  Time: 0.772s, 1326.01/s  (0.814s, 1258.60/s)  LR: 7.134e-04  Data: 0.009 (0.044)
Train: 217 [ 100/1251 (  8%)]  Loss: 3.757 (3.74)  Time: 0.771s, 1327.81/s  (0.802s, 1276.20/s)  LR: 7.134e-04  Data: 0.010 (0.027)
Train: 217 [ 150/1251 ( 12%)]  Loss: 3.602 (3.70)  Time: 0.773s, 1325.35/s  (0.797s, 1285.06/s)  LR: 7.134e-04  Data: 0.010 (0.021)
Train: 217 [ 200/1251 ( 16%)]  Loss: 3.882 (3.74)  Time: 0.772s, 1326.71/s  (0.794s, 1289.87/s)  LR: 7.134e-04  Data: 0.010 (0.018)
Train: 217 [ 250/1251 ( 20%)]  Loss: 3.562 (3.71)  Time: 0.878s, 1166.28/s  (0.793s, 1291.99/s)  LR: 7.134e-04  Data: 0.009 (0.017)
Train: 217 [ 300/1251 ( 24%)]  Loss: 3.572 (3.69)  Time: 0.863s, 1186.89/s  (0.792s, 1292.58/s)  LR: 7.134e-04  Data: 0.010 (0.016)
Train: 217 [ 350/1251 ( 28%)]  Loss: 3.522 (3.67)  Time: 0.778s, 1315.68/s  (0.791s, 1294.81/s)  LR: 7.134e-04  Data: 0.010 (0.015)
Train: 217 [ 400/1251 ( 32%)]  Loss: 3.358 (3.63)  Time: 0.769s, 1331.43/s  (0.789s, 1297.35/s)  LR: 7.134e-04  Data: 0.010 (0.014)
Train: 217 [ 450/1251 ( 36%)]  Loss: 3.677 (3.64)  Time: 0.827s, 1238.60/s  (0.789s, 1298.21/s)  LR: 7.134e-04  Data: 0.014 (0.014)
Train: 217 [ 500/1251 ( 40%)]  Loss: 3.635 (3.64)  Time: 0.773s, 1325.39/s  (0.788s, 1299.11/s)  LR: 7.134e-04  Data: 0.010 (0.013)
Train: 217 [ 550/1251 ( 44%)]  Loss: 3.540 (3.63)  Time: 0.774s, 1322.92/s  (0.788s, 1298.74/s)  LR: 7.134e-04  Data: 0.010 (0.013)
Train: 217 [ 600/1251 ( 48%)]  Loss: 3.823 (3.65)  Time: 0.867s, 1181.62/s  (0.788s, 1299.50/s)  LR: 7.134e-04  Data: 0.010 (0.013)
Train: 217 [ 650/1251 ( 52%)]  Loss: 3.341 (3.62)  Time: 0.771s, 1327.97/s  (0.788s, 1299.30/s)  LR: 7.134e-04  Data: 0.009 (0.013)
Train: 217 [ 700/1251 ( 56%)]  Loss: 3.538 (3.62)  Time: 0.774s, 1322.20/s  (0.788s, 1300.03/s)  LR: 7.134e-04  Data: 0.010 (0.012)
Train: 217 [ 750/1251 ( 60%)]  Loss: 3.450 (3.61)  Time: 0.780s, 1313.53/s  (0.787s, 1301.05/s)  LR: 7.134e-04  Data: 0.009 (0.012)
Train: 217 [ 800/1251 ( 64%)]  Loss: 3.582 (3.61)  Time: 0.774s, 1323.10/s  (0.787s, 1301.62/s)  LR: 7.134e-04  Data: 0.010 (0.012)
Train: 217 [ 850/1251 ( 68%)]  Loss: 3.653 (3.61)  Time: 0.807s, 1269.61/s  (0.787s, 1301.67/s)  LR: 7.134e-04  Data: 0.010 (0.012)
Train: 217 [ 900/1251 ( 72%)]  Loss: 3.431 (3.60)  Time: 0.780s, 1313.39/s  (0.787s, 1301.58/s)  LR: 7.134e-04  Data: 0.011 (0.012)
Train: 217 [ 950/1251 ( 76%)]  Loss: 3.772 (3.61)  Time: 0.771s, 1327.31/s  (0.787s, 1301.84/s)  LR: 7.134e-04  Data: 0.010 (0.012)
Train: 217 [1000/1251 ( 80%)]  Loss: 4.040 (3.63)  Time: 0.781s, 1311.10/s  (0.787s, 1301.82/s)  LR: 7.134e-04  Data: 0.010 (0.012)
Train: 217 [1050/1251 ( 84%)]  Loss: 3.368 (3.62)  Time: 0.779s, 1314.11/s  (0.786s, 1302.32/s)  LR: 7.134e-04  Data: 0.009 (0.012)
Train: 217 [1100/1251 ( 88%)]  Loss: 3.563 (3.61)  Time: 0.773s, 1325.53/s  (0.787s, 1301.64/s)  LR: 7.134e-04  Data: 0.009 (0.012)
Train: 217 [1150/1251 ( 92%)]  Loss: 3.402 (3.61)  Time: 0.784s, 1306.46/s  (0.786s, 1302.18/s)  LR: 7.134e-04  Data: 0.013 (0.012)
Train: 217 [1200/1251 ( 96%)]  Loss: 3.499 (3.60)  Time: 0.774s, 1323.23/s  (0.786s, 1302.80/s)  LR: 7.134e-04  Data: 0.012 (0.011)
Train: 217 [1250/1251 (100%)]  Loss: 3.505 (3.60)  Time: 0.810s, 1264.95/s  (0.786s, 1302.85/s)  LR: 7.134e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.550 (1.550)  Loss:  0.7754 (0.7754)  Acc@1: 88.5742 (88.5742)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.193 (0.563)  Loss:  0.9048 (1.3550)  Acc@1: 84.0802 (73.9580)  Acc@5: 95.5189 (92.2080)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-213.pth.tar', 74.13599998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-215.pth.tar', 73.97000003662109)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-217.pth.tar', 73.95800006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-200.pth.tar', 73.95200006835937)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-211.pth.tar', 73.92000000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-203.pth.tar', 73.89600004150391)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-202.pth.tar', 73.8080001171875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-214.pth.tar', 73.79400001220704)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-194.pth.tar', 73.72600004150391)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-207.pth.tar', 73.70400009765625)

Train: 218 [   0/1251 (  0%)]  Loss: 3.825 (3.82)  Time: 2.168s,  472.39/s  (2.168s,  472.39/s)  LR: 7.111e-04  Data: 1.439 (1.439)
Train: 218 [  50/1251 (  4%)]  Loss: 3.812 (3.82)  Time: 0.773s, 1324.73/s  (0.814s, 1257.96/s)  LR: 7.111e-04  Data: 0.010 (0.043)
Train: 218 [ 100/1251 (  8%)]  Loss: 3.394 (3.68)  Time: 0.776s, 1318.84/s  (0.798s, 1283.87/s)  LR: 7.111e-04  Data: 0.009 (0.027)
Train: 218 [ 150/1251 ( 12%)]  Loss: 3.313 (3.59)  Time: 0.776s, 1319.02/s  (0.794s, 1289.68/s)  LR: 7.111e-04  Data: 0.010 (0.021)
Train: 218 [ 200/1251 ( 16%)]  Loss: 3.322 (3.53)  Time: 0.774s, 1322.57/s  (0.793s, 1291.13/s)  LR: 7.111e-04  Data: 0.011 (0.019)
Train: 218 [ 250/1251 ( 20%)]  Loss: 3.484 (3.53)  Time: 0.777s, 1317.14/s  (0.792s, 1292.78/s)  LR: 7.111e-04  Data: 0.014 (0.017)
Train: 218 [ 300/1251 ( 24%)]  Loss: 4.019 (3.60)  Time: 0.774s, 1322.73/s  (0.791s, 1295.32/s)  LR: 7.111e-04  Data: 0.010 (0.016)
Train: 218 [ 350/1251 ( 28%)]  Loss: 3.553 (3.59)  Time: 0.778s, 1316.65/s  (0.789s, 1297.35/s)  LR: 7.111e-04  Data: 0.010 (0.015)
Train: 218 [ 400/1251 ( 32%)]  Loss: 3.617 (3.59)  Time: 0.775s, 1322.14/s  (0.789s, 1297.39/s)  LR: 7.111e-04  Data: 0.010 (0.014)
Train: 218 [ 450/1251 ( 36%)]  Loss: 3.843 (3.62)  Time: 0.795s, 1287.31/s  (0.789s, 1298.35/s)  LR: 7.111e-04  Data: 0.010 (0.014)
Train: 218 [ 500/1251 ( 40%)]  Loss: 3.496 (3.61)  Time: 0.784s, 1306.60/s  (0.788s, 1299.82/s)  LR: 7.111e-04  Data: 0.010 (0.014)
Train: 218 [ 550/1251 ( 44%)]  Loss: 3.865 (3.63)  Time: 0.776s, 1319.63/s  (0.787s, 1301.12/s)  LR: 7.111e-04  Data: 0.010 (0.013)
Train: 218 [ 600/1251 ( 48%)]  Loss: 3.153 (3.59)  Time: 0.820s, 1248.70/s  (0.788s, 1300.26/s)  LR: 7.111e-04  Data: 0.010 (0.013)
Train: 218 [ 650/1251 ( 52%)]  Loss: 3.773 (3.61)  Time: 0.771s, 1327.36/s  (0.787s, 1300.99/s)  LR: 7.111e-04  Data: 0.009 (0.013)
Train: 218 [ 700/1251 ( 56%)]  Loss: 3.597 (3.60)  Time: 0.774s, 1322.19/s  (0.787s, 1301.58/s)  LR: 7.111e-04  Data: 0.009 (0.013)
Train: 218 [ 750/1251 ( 60%)]  Loss: 3.737 (3.61)  Time: 0.774s, 1322.39/s  (0.787s, 1301.54/s)  LR: 7.111e-04  Data: 0.010 (0.012)
Train: 218 [ 800/1251 ( 64%)]  Loss: 3.696 (3.62)  Time: 0.801s, 1278.42/s  (0.786s, 1302.14/s)  LR: 7.111e-04  Data: 0.009 (0.012)
Train: 218 [ 850/1251 ( 68%)]  Loss: 3.866 (3.63)  Time: 0.773s, 1324.90/s  (0.786s, 1302.14/s)  LR: 7.111e-04  Data: 0.010 (0.012)
Train: 218 [ 900/1251 ( 72%)]  Loss: 3.857 (3.64)  Time: 0.773s, 1324.10/s  (0.786s, 1302.38/s)  LR: 7.111e-04  Data: 0.010 (0.012)
Train: 218 [ 950/1251 ( 76%)]  Loss: 3.364 (3.63)  Time: 0.796s, 1286.56/s  (0.786s, 1302.62/s)  LR: 7.111e-04  Data: 0.009 (0.012)
Train: 218 [1000/1251 ( 80%)]  Loss: 3.504 (3.62)  Time: 0.774s, 1323.48/s  (0.786s, 1303.30/s)  LR: 7.111e-04  Data: 0.010 (0.012)
Train: 218 [1050/1251 ( 84%)]  Loss: 3.457 (3.62)  Time: 0.787s, 1300.32/s  (0.785s, 1303.65/s)  LR: 7.111e-04  Data: 0.009 (0.012)
Train: 218 [1100/1251 ( 88%)]  Loss: 3.594 (3.61)  Time: 0.772s, 1327.07/s  (0.785s, 1304.06/s)  LR: 7.111e-04  Data: 0.010 (0.012)
Train: 218 [1150/1251 ( 92%)]  Loss: 3.775 (3.62)  Time: 0.772s, 1325.81/s  (0.785s, 1304.10/s)  LR: 7.111e-04  Data: 0.009 (0.011)
Train: 218 [1200/1251 ( 96%)]  Loss: 3.482 (3.62)  Time: 0.772s, 1326.30/s  (0.785s, 1304.59/s)  LR: 7.111e-04  Data: 0.010 (0.011)
Train: 218 [1250/1251 (100%)]  Loss: 3.394 (3.61)  Time: 0.760s, 1348.22/s  (0.785s, 1304.81/s)  LR: 7.111e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.516 (1.516)  Loss:  0.8506 (0.8506)  Acc@1: 87.6953 (87.6953)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.194 (0.563)  Loss:  0.8857 (1.3871)  Acc@1: 84.9057 (73.4200)  Acc@5: 96.3443 (92.0060)
Train: 219 [   0/1251 (  0%)]  Loss: 3.757 (3.76)  Time: 2.360s,  433.93/s  (2.360s,  433.93/s)  LR: 7.087e-04  Data: 1.629 (1.629)
Train: 219 [  50/1251 (  4%)]  Loss: 2.911 (3.33)  Time: 0.773s, 1325.01/s  (0.818s, 1252.36/s)  LR: 7.087e-04  Data: 0.010 (0.043)
Train: 219 [ 100/1251 (  8%)]  Loss: 3.828 (3.50)  Time: 0.773s, 1324.03/s  (0.809s, 1265.97/s)  LR: 7.087e-04  Data: 0.012 (0.027)
Train: 219 [ 150/1251 ( 12%)]  Loss: 3.505 (3.50)  Time: 0.786s, 1302.39/s  (0.800s, 1279.90/s)  LR: 7.087e-04  Data: 0.009 (0.022)
Train: 219 [ 200/1251 ( 16%)]  Loss: 3.627 (3.53)  Time: 0.772s, 1325.61/s  (0.797s, 1285.17/s)  LR: 7.087e-04  Data: 0.010 (0.019)
Train: 219 [ 250/1251 ( 20%)]  Loss: 3.567 (3.53)  Time: 0.772s, 1325.82/s  (0.795s, 1287.36/s)  LR: 7.087e-04  Data: 0.010 (0.017)
Train: 219 [ 300/1251 ( 24%)]  Loss: 3.410 (3.52)  Time: 0.773s, 1324.05/s  (0.794s, 1290.07/s)  LR: 7.087e-04  Data: 0.010 (0.016)
Train: 219 [ 350/1251 ( 28%)]  Loss: 3.899 (3.56)  Time: 0.772s, 1326.03/s  (0.793s, 1291.96/s)  LR: 7.087e-04  Data: 0.010 (0.015)
Train: 219 [ 400/1251 ( 32%)]  Loss: 3.569 (3.56)  Time: 0.773s, 1325.49/s  (0.791s, 1294.15/s)  LR: 7.087e-04  Data: 0.010 (0.014)
Train: 219 [ 450/1251 ( 36%)]  Loss: 3.348 (3.54)  Time: 0.784s, 1305.64/s  (0.790s, 1296.72/s)  LR: 7.087e-04  Data: 0.011 (0.014)
Train: 219 [ 500/1251 ( 40%)]  Loss: 3.587 (3.55)  Time: 0.773s, 1325.09/s  (0.789s, 1297.70/s)  LR: 7.087e-04  Data: 0.009 (0.014)
Train: 219 [ 550/1251 ( 44%)]  Loss: 3.484 (3.54)  Time: 0.773s, 1324.34/s  (0.788s, 1298.87/s)  LR: 7.087e-04  Data: 0.009 (0.013)
Train: 219 [ 600/1251 ( 48%)]  Loss: 3.514 (3.54)  Time: 0.773s, 1325.08/s  (0.788s, 1299.53/s)  LR: 7.087e-04  Data: 0.010 (0.013)
Train: 219 [ 650/1251 ( 52%)]  Loss: 3.240 (3.52)  Time: 0.775s, 1320.44/s  (0.788s, 1300.30/s)  LR: 7.087e-04  Data: 0.009 (0.013)
Train: 219 [ 700/1251 ( 56%)]  Loss: 3.528 (3.52)  Time: 0.773s, 1325.19/s  (0.787s, 1301.42/s)  LR: 7.087e-04  Data: 0.010 (0.013)
Train: 219 [ 750/1251 ( 60%)]  Loss: 3.822 (3.54)  Time: 0.774s, 1322.81/s  (0.786s, 1302.39/s)  LR: 7.087e-04  Data: 0.009 (0.012)
Train: 219 [ 800/1251 ( 64%)]  Loss: 3.620 (3.54)  Time: 0.773s, 1324.30/s  (0.786s, 1303.47/s)  LR: 7.087e-04  Data: 0.010 (0.012)
Train: 219 [ 850/1251 ( 68%)]  Loss: 3.797 (3.56)  Time: 0.771s, 1327.96/s  (0.786s, 1303.21/s)  LR: 7.087e-04  Data: 0.010 (0.012)
Train: 219 [ 900/1251 ( 72%)]  Loss: 3.619 (3.56)  Time: 0.773s, 1324.76/s  (0.786s, 1303.57/s)  LR: 7.087e-04  Data: 0.010 (0.012)
Train: 219 [ 950/1251 ( 76%)]  Loss: 3.673 (3.57)  Time: 0.774s, 1322.60/s  (0.786s, 1303.39/s)  LR: 7.087e-04  Data: 0.012 (0.012)
Train: 219 [1000/1251 ( 80%)]  Loss: 3.492 (3.56)  Time: 0.773s, 1325.38/s  (0.786s, 1302.28/s)  LR: 7.087e-04  Data: 0.010 (0.012)
Train: 219 [1050/1251 ( 84%)]  Loss: 3.428 (3.56)  Time: 0.775s, 1321.87/s  (0.786s, 1302.82/s)  LR: 7.087e-04  Data: 0.011 (0.012)
Train: 219 [1100/1251 ( 88%)]  Loss: 3.579 (3.56)  Time: 0.787s, 1301.07/s  (0.786s, 1302.94/s)  LR: 7.087e-04  Data: 0.010 (0.012)
Train: 219 [1150/1251 ( 92%)]  Loss: 3.741 (3.56)  Time: 0.825s, 1240.49/s  (0.786s, 1302.60/s)  LR: 7.087e-04  Data: 0.013 (0.012)
Train: 219 [1200/1251 ( 96%)]  Loss: 3.807 (3.57)  Time: 0.774s, 1323.05/s  (0.786s, 1302.22/s)  LR: 7.087e-04  Data: 0.010 (0.012)
Train: 219 [1250/1251 (100%)]  Loss: 3.747 (3.58)  Time: 0.759s, 1348.44/s  (0.786s, 1302.85/s)  LR: 7.087e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.602 (1.602)  Loss:  0.8086 (0.8086)  Acc@1: 89.0625 (89.0625)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.194 (0.563)  Loss:  0.9497 (1.3767)  Acc@1: 84.5519 (74.3400)  Acc@5: 96.6981 (92.4860)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-219.pth.tar', 74.34000014160156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-213.pth.tar', 74.13599998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-215.pth.tar', 73.97000003662109)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-217.pth.tar', 73.95800006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-200.pth.tar', 73.95200006835937)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-211.pth.tar', 73.92000000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-203.pth.tar', 73.89600004150391)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-202.pth.tar', 73.8080001171875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-214.pth.tar', 73.79400001220704)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-194.pth.tar', 73.72600004150391)

Train: 220 [   0/1251 (  0%)]  Loss: 3.430 (3.43)  Time: 2.296s,  445.98/s  (2.296s,  445.98/s)  LR: 7.063e-04  Data: 1.539 (1.539)
Train: 220 [  50/1251 (  4%)]  Loss: 3.815 (3.62)  Time: 0.772s, 1326.63/s  (0.840s, 1219.24/s)  LR: 7.063e-04  Data: 0.010 (0.047)
Train: 220 [ 100/1251 (  8%)]  Loss: 3.416 (3.55)  Time: 0.777s, 1317.95/s  (0.816s, 1254.78/s)  LR: 7.063e-04  Data: 0.013 (0.029)
Train: 220 [ 150/1251 ( 12%)]  Loss: 3.267 (3.48)  Time: 0.772s, 1326.37/s  (0.802s, 1276.15/s)  LR: 7.063e-04  Data: 0.010 (0.023)
Train: 220 [ 200/1251 ( 16%)]  Loss: 3.503 (3.49)  Time: 0.772s, 1325.99/s  (0.797s, 1285.13/s)  LR: 7.063e-04  Data: 0.010 (0.020)
Train: 220 [ 250/1251 ( 20%)]  Loss: 3.794 (3.54)  Time: 0.832s, 1230.27/s  (0.794s, 1289.86/s)  LR: 7.063e-04  Data: 0.009 (0.018)
Train: 220 [ 300/1251 ( 24%)]  Loss: 3.664 (3.56)  Time: 0.772s, 1325.65/s  (0.793s, 1291.80/s)  LR: 7.063e-04  Data: 0.010 (0.016)
Train: 220 [ 350/1251 ( 28%)]  Loss: 3.395 (3.54)  Time: 0.773s, 1324.42/s  (0.791s, 1294.29/s)  LR: 7.063e-04  Data: 0.010 (0.015)
Train: 220 [ 400/1251 ( 32%)]  Loss: 3.871 (3.57)  Time: 0.782s, 1310.29/s  (0.790s, 1296.04/s)  LR: 7.063e-04  Data: 0.011 (0.015)
Train: 220 [ 450/1251 ( 36%)]  Loss: 3.186 (3.53)  Time: 0.774s, 1322.96/s  (0.789s, 1297.52/s)  LR: 7.063e-04  Data: 0.011 (0.014)
Train: 220 [ 500/1251 ( 40%)]  Loss: 3.691 (3.55)  Time: 0.775s, 1320.60/s  (0.789s, 1298.10/s)  LR: 7.063e-04  Data: 0.010 (0.014)
Train: 220 [ 550/1251 ( 44%)]  Loss: 3.679 (3.56)  Time: 0.771s, 1327.80/s  (0.788s, 1299.18/s)  LR: 7.063e-04  Data: 0.010 (0.013)
Train: 220 [ 600/1251 ( 48%)]  Loss: 3.937 (3.59)  Time: 0.772s, 1325.58/s  (0.789s, 1298.39/s)  LR: 7.063e-04  Data: 0.010 (0.013)
Train: 220 [ 650/1251 ( 52%)]  Loss: 3.636 (3.59)  Time: 0.778s, 1315.94/s  (0.789s, 1298.60/s)  LR: 7.063e-04  Data: 0.010 (0.013)
Train: 220 [ 700/1251 ( 56%)]  Loss: 3.426 (3.58)  Time: 0.771s, 1327.66/s  (0.788s, 1299.38/s)  LR: 7.063e-04  Data: 0.011 (0.013)
Train: 220 [ 750/1251 ( 60%)]  Loss: 3.348 (3.57)  Time: 0.803s, 1274.57/s  (0.788s, 1300.01/s)  LR: 7.063e-04  Data: 0.010 (0.013)
Train: 220 [ 800/1251 ( 64%)]  Loss: 3.182 (3.54)  Time: 0.778s, 1315.72/s  (0.788s, 1300.21/s)  LR: 7.063e-04  Data: 0.010 (0.013)
Train: 220 [ 850/1251 ( 68%)]  Loss: 3.516 (3.54)  Time: 0.776s, 1319.53/s  (0.788s, 1299.37/s)  LR: 7.063e-04  Data: 0.010 (0.012)
Train: 220 [ 900/1251 ( 72%)]  Loss: 3.329 (3.53)  Time: 0.777s, 1318.58/s  (0.788s, 1299.52/s)  LR: 7.063e-04  Data: 0.009 (0.012)
Train: 220 [ 950/1251 ( 76%)]  Loss: 3.834 (3.55)  Time: 0.783s, 1308.54/s  (0.788s, 1300.09/s)  LR: 7.063e-04  Data: 0.012 (0.012)
Train: 220 [1000/1251 ( 80%)]  Loss: 3.697 (3.55)  Time: 0.775s, 1322.04/s  (0.787s, 1300.83/s)  LR: 7.063e-04  Data: 0.009 (0.012)
Train: 220 [1050/1251 ( 84%)]  Loss: 3.340 (3.54)  Time: 0.774s, 1322.19/s  (0.787s, 1301.37/s)  LR: 7.063e-04  Data: 0.010 (0.012)
Train: 220 [1100/1251 ( 88%)]  Loss: 3.575 (3.54)  Time: 0.773s, 1323.89/s  (0.787s, 1301.27/s)  LR: 7.063e-04  Data: 0.011 (0.012)
Train: 220 [1150/1251 ( 92%)]  Loss: 3.329 (3.54)  Time: 0.777s, 1318.05/s  (0.788s, 1300.18/s)  LR: 7.063e-04  Data: 0.010 (0.012)
Train: 220 [1200/1251 ( 96%)]  Loss: 3.901 (3.55)  Time: 0.782s, 1309.12/s  (0.787s, 1300.46/s)  LR: 7.063e-04  Data: 0.013 (0.012)
Train: 220 [1250/1251 (100%)]  Loss: 3.373 (3.54)  Time: 0.805s, 1272.19/s  (0.787s, 1300.80/s)  LR: 7.063e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.518 (1.518)  Loss:  0.7617 (0.7617)  Acc@1: 88.0859 (88.0859)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.193 (0.584)  Loss:  0.8452 (1.2968)  Acc@1: 83.7264 (73.5700)  Acc@5: 95.7547 (92.0900)
Train: 221 [   0/1251 (  0%)]  Loss: 3.587 (3.59)  Time: 2.407s,  425.40/s  (2.407s,  425.40/s)  LR: 7.040e-04  Data: 1.678 (1.678)
Train: 221 [  50/1251 (  4%)]  Loss: 3.506 (3.55)  Time: 0.783s, 1308.03/s  (0.810s, 1264.45/s)  LR: 7.040e-04  Data: 0.010 (0.043)
Train: 221 [ 100/1251 (  8%)]  Loss: 3.542 (3.54)  Time: 0.774s, 1322.68/s  (0.793s, 1291.28/s)  LR: 7.040e-04  Data: 0.010 (0.027)
Train: 221 [ 150/1251 ( 12%)]  Loss: 3.424 (3.51)  Time: 0.779s, 1313.70/s  (0.792s, 1292.91/s)  LR: 7.040e-04  Data: 0.009 (0.021)
Train: 221 [ 200/1251 ( 16%)]  Loss: 3.841 (3.58)  Time: 0.773s, 1324.96/s  (0.790s, 1296.73/s)  LR: 7.040e-04  Data: 0.010 (0.018)
Train: 221 [ 250/1251 ( 20%)]  Loss: 3.264 (3.53)  Time: 0.772s, 1325.93/s  (0.788s, 1298.84/s)  LR: 7.040e-04  Data: 0.010 (0.017)
Train: 221 [ 300/1251 ( 24%)]  Loss: 3.310 (3.50)  Time: 0.772s, 1327.20/s  (0.788s, 1299.24/s)  LR: 7.040e-04  Data: 0.010 (0.016)
Train: 221 [ 350/1251 ( 28%)]  Loss: 3.563 (3.50)  Time: 0.771s, 1328.12/s  (0.788s, 1298.90/s)  LR: 7.040e-04  Data: 0.010 (0.015)
Train: 221 [ 400/1251 ( 32%)]  Loss: 3.386 (3.49)  Time: 0.782s, 1308.87/s  (0.788s, 1299.77/s)  LR: 7.040e-04  Data: 0.012 (0.014)
Train: 221 [ 450/1251 ( 36%)]  Loss: 3.653 (3.51)  Time: 0.819s, 1249.92/s  (0.788s, 1299.40/s)  LR: 7.040e-04  Data: 0.012 (0.014)
Train: 221 [ 500/1251 ( 40%)]  Loss: 3.560 (3.51)  Time: 0.778s, 1315.36/s  (0.788s, 1299.83/s)  LR: 7.040e-04  Data: 0.011 (0.014)
Train: 221 [ 550/1251 ( 44%)]  Loss: 3.694 (3.53)  Time: 0.773s, 1324.99/s  (0.787s, 1301.14/s)  LR: 7.040e-04  Data: 0.010 (0.013)
Train: 221 [ 600/1251 ( 48%)]  Loss: 3.809 (3.55)  Time: 0.772s, 1326.58/s  (0.787s, 1300.74/s)  LR: 7.040e-04  Data: 0.011 (0.013)
Train: 221 [ 650/1251 ( 52%)]  Loss: 3.263 (3.53)  Time: 0.775s, 1321.21/s  (0.787s, 1301.17/s)  LR: 7.040e-04  Data: 0.011 (0.013)
Train: 221 [ 700/1251 ( 56%)]  Loss: 3.371 (3.52)  Time: 0.773s, 1325.23/s  (0.788s, 1299.84/s)  LR: 7.040e-04  Data: 0.010 (0.013)
Train: 221 [ 750/1251 ( 60%)]  Loss: 3.636 (3.53)  Time: 0.844s, 1212.59/s  (0.788s, 1298.83/s)  LR: 7.040e-04  Data: 0.010 (0.012)
Train: 221 [ 800/1251 ( 64%)]  Loss: 3.946 (3.55)  Time: 0.774s, 1323.02/s  (0.789s, 1297.98/s)  LR: 7.040e-04  Data: 0.009 (0.012)
Train: 221 [ 850/1251 ( 68%)]  Loss: 3.459 (3.55)  Time: 0.772s, 1326.98/s  (0.788s, 1299.16/s)  LR: 7.040e-04  Data: 0.010 (0.012)
Train: 221 [ 900/1251 ( 72%)]  Loss: 3.668 (3.55)  Time: 0.865s, 1183.71/s  (0.788s, 1299.53/s)  LR: 7.040e-04  Data: 0.009 (0.012)
Train: 221 [ 950/1251 ( 76%)]  Loss: 3.751 (3.56)  Time: 0.786s, 1302.22/s  (0.788s, 1300.06/s)  LR: 7.040e-04  Data: 0.010 (0.012)
Train: 221 [1000/1251 ( 80%)]  Loss: 3.664 (3.57)  Time: 0.777s, 1318.11/s  (0.788s, 1300.08/s)  LR: 7.040e-04  Data: 0.009 (0.012)
Train: 221 [1050/1251 ( 84%)]  Loss: 3.510 (3.56)  Time: 0.780s, 1313.62/s  (0.788s, 1299.21/s)  LR: 7.040e-04  Data: 0.012 (0.012)
Train: 221 [1100/1251 ( 88%)]  Loss: 3.778 (3.57)  Time: 0.773s, 1324.87/s  (0.788s, 1299.82/s)  LR: 7.040e-04  Data: 0.010 (0.012)
Train: 221 [1150/1251 ( 92%)]  Loss: 3.756 (3.58)  Time: 0.772s, 1325.59/s  (0.788s, 1300.08/s)  LR: 7.040e-04  Data: 0.010 (0.012)
Train: 221 [1200/1251 ( 96%)]  Loss: 3.506 (3.58)  Time: 0.849s, 1205.94/s  (0.788s, 1300.29/s)  LR: 7.040e-04  Data: 0.010 (0.012)
Train: 221 [1250/1251 (100%)]  Loss: 3.535 (3.58)  Time: 0.762s, 1343.51/s  (0.788s, 1300.25/s)  LR: 7.040e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.545 (1.545)  Loss:  0.8438 (0.8438)  Acc@1: 88.4766 (88.4766)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.194 (0.567)  Loss:  0.9111 (1.3533)  Acc@1: 84.6698 (73.8480)  Acc@5: 96.2264 (92.1860)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-219.pth.tar', 74.34000014160156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-213.pth.tar', 74.13599998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-215.pth.tar', 73.97000003662109)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-217.pth.tar', 73.95800006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-200.pth.tar', 73.95200006835937)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-211.pth.tar', 73.92000000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-203.pth.tar', 73.89600004150391)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-221.pth.tar', 73.84800006347656)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-202.pth.tar', 73.8080001171875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-214.pth.tar', 73.79400001220704)

Train: 222 [   0/1251 (  0%)]  Loss: 3.497 (3.50)  Time: 2.180s,  469.65/s  (2.180s,  469.65/s)  LR: 7.016e-04  Data: 1.459 (1.459)
Train: 222 [  50/1251 (  4%)]  Loss: 3.595 (3.55)  Time: 0.774s, 1323.48/s  (0.826s, 1240.29/s)  LR: 7.016e-04  Data: 0.010 (0.045)
Train: 222 [ 100/1251 (  8%)]  Loss: 3.379 (3.49)  Time: 0.778s, 1316.15/s  (0.807s, 1268.46/s)  LR: 7.016e-04  Data: 0.010 (0.028)
Train: 222 [ 150/1251 ( 12%)]  Loss: 3.744 (3.55)  Time: 0.772s, 1326.81/s  (0.804s, 1273.65/s)  LR: 7.016e-04  Data: 0.010 (0.022)
Train: 222 [ 200/1251 ( 16%)]  Loss: 3.765 (3.60)  Time: 0.770s, 1329.20/s  (0.802s, 1277.45/s)  LR: 7.016e-04  Data: 0.009 (0.019)
Train: 222 [ 250/1251 ( 20%)]  Loss: 3.306 (3.55)  Time: 0.784s, 1305.69/s  (0.798s, 1282.54/s)  LR: 7.016e-04  Data: 0.011 (0.017)
Train: 222 [ 300/1251 ( 24%)]  Loss: 3.715 (3.57)  Time: 0.775s, 1321.49/s  (0.796s, 1286.23/s)  LR: 7.016e-04  Data: 0.010 (0.016)
Train: 222 [ 350/1251 ( 28%)]  Loss: 3.327 (3.54)  Time: 0.774s, 1323.74/s  (0.797s, 1284.04/s)  LR: 7.016e-04  Data: 0.009 (0.016)
Train: 222 [ 400/1251 ( 32%)]  Loss: 3.668 (3.56)  Time: 0.777s, 1318.23/s  (0.796s, 1286.19/s)  LR: 7.016e-04  Data: 0.010 (0.015)
Train: 222 [ 450/1251 ( 36%)]  Loss: 3.379 (3.54)  Time: 0.772s, 1325.69/s  (0.795s, 1287.69/s)  LR: 7.016e-04  Data: 0.010 (0.014)
Train: 222 [ 500/1251 ( 40%)]  Loss: 3.740 (3.56)  Time: 0.780s, 1312.64/s  (0.793s, 1290.79/s)  LR: 7.016e-04  Data: 0.010 (0.014)
Train: 222 [ 550/1251 ( 44%)]  Loss: 3.769 (3.57)  Time: 0.773s, 1325.47/s  (0.792s, 1292.16/s)  LR: 7.016e-04  Data: 0.010 (0.014)
Train: 222 [ 600/1251 ( 48%)]  Loss: 3.567 (3.57)  Time: 0.772s, 1326.17/s  (0.791s, 1293.98/s)  LR: 7.016e-04  Data: 0.010 (0.013)
Train: 222 [ 650/1251 ( 52%)]  Loss: 3.932 (3.60)  Time: 0.783s, 1307.60/s  (0.792s, 1292.50/s)  LR: 7.016e-04  Data: 0.010 (0.013)
Train: 222 [ 700/1251 ( 56%)]  Loss: 3.906 (3.62)  Time: 0.777s, 1318.19/s  (0.792s, 1293.38/s)  LR: 7.016e-04  Data: 0.010 (0.013)
Train: 222 [ 750/1251 ( 60%)]  Loss: 3.588 (3.62)  Time: 0.773s, 1325.55/s  (0.791s, 1294.09/s)  LR: 7.016e-04  Data: 0.010 (0.013)
Train: 222 [ 800/1251 ( 64%)]  Loss: 3.659 (3.62)  Time: 0.779s, 1313.95/s  (0.791s, 1295.04/s)  LR: 7.016e-04  Data: 0.010 (0.013)
Train: 222 [ 850/1251 ( 68%)]  Loss: 3.402 (3.61)  Time: 0.773s, 1324.08/s  (0.790s, 1295.81/s)  LR: 7.016e-04  Data: 0.009 (0.012)
Train: 222 [ 900/1251 ( 72%)]  Loss: 3.420 (3.60)  Time: 0.776s, 1320.08/s  (0.790s, 1295.83/s)  LR: 7.016e-04  Data: 0.010 (0.012)
Train: 222 [ 950/1251 ( 76%)]  Loss: 3.424 (3.59)  Time: 0.771s, 1327.86/s  (0.790s, 1296.81/s)  LR: 7.016e-04  Data: 0.010 (0.012)
Train: 222 [1000/1251 ( 80%)]  Loss: 3.664 (3.59)  Time: 0.773s, 1324.65/s  (0.789s, 1297.64/s)  LR: 7.016e-04  Data: 0.010 (0.012)
Train: 222 [1050/1251 ( 84%)]  Loss: 3.482 (3.59)  Time: 0.772s, 1326.94/s  (0.789s, 1298.13/s)  LR: 7.016e-04  Data: 0.010 (0.012)
Train: 222 [1100/1251 ( 88%)]  Loss: 3.903 (3.60)  Time: 0.774s, 1323.47/s  (0.789s, 1298.48/s)  LR: 7.016e-04  Data: 0.011 (0.012)
Train: 222 [1150/1251 ( 92%)]  Loss: 3.681 (3.60)  Time: 0.773s, 1324.54/s  (0.789s, 1298.53/s)  LR: 7.016e-04  Data: 0.010 (0.012)
Train: 222 [1200/1251 ( 96%)]  Loss: 3.315 (3.59)  Time: 0.775s, 1322.08/s  (0.789s, 1298.19/s)  LR: 7.016e-04  Data: 0.010 (0.012)
Train: 222 [1250/1251 (100%)]  Loss: 3.843 (3.60)  Time: 0.761s, 1346.15/s  (0.789s, 1297.57/s)  LR: 7.016e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.556 (1.556)  Loss:  0.7246 (0.7246)  Acc@1: 87.5977 (87.5977)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.194 (0.564)  Loss:  0.8086 (1.2727)  Acc@1: 84.5519 (73.9220)  Acc@5: 96.2264 (92.3520)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-219.pth.tar', 74.34000014160156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-213.pth.tar', 74.13599998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-215.pth.tar', 73.97000003662109)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-217.pth.tar', 73.95800006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-200.pth.tar', 73.95200006835937)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-222.pth.tar', 73.9219998828125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-211.pth.tar', 73.92000000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-203.pth.tar', 73.89600004150391)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-221.pth.tar', 73.84800006347656)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-202.pth.tar', 73.8080001171875)

Train: 223 [   0/1251 (  0%)]  Loss: 3.508 (3.51)  Time: 2.336s,  438.30/s  (2.336s,  438.30/s)  LR: 6.992e-04  Data: 1.566 (1.566)
Train: 223 [  50/1251 (  4%)]  Loss: 3.318 (3.41)  Time: 0.774s, 1322.77/s  (0.819s, 1250.39/s)  LR: 6.992e-04  Data: 0.010 (0.045)
Train: 223 [ 100/1251 (  8%)]  Loss: 3.765 (3.53)  Time: 0.773s, 1325.04/s  (0.800s, 1279.70/s)  LR: 6.992e-04  Data: 0.010 (0.027)
Train: 223 [ 150/1251 ( 12%)]  Loss: 3.793 (3.60)  Time: 0.894s, 1145.87/s  (0.795s, 1287.46/s)  LR: 6.992e-04  Data: 0.010 (0.022)
Train: 223 [ 200/1251 ( 16%)]  Loss: 3.657 (3.61)  Time: 0.826s, 1239.17/s  (0.796s, 1286.94/s)  LR: 6.992e-04  Data: 0.010 (0.019)
Train: 223 [ 250/1251 ( 20%)]  Loss: 3.515 (3.59)  Time: 0.783s, 1308.57/s  (0.794s, 1289.12/s)  LR: 6.992e-04  Data: 0.010 (0.017)
Train: 223 [ 300/1251 ( 24%)]  Loss: 3.928 (3.64)  Time: 0.774s, 1323.42/s  (0.792s, 1293.56/s)  LR: 6.992e-04  Data: 0.009 (0.016)
Train: 223 [ 350/1251 ( 28%)]  Loss: 3.877 (3.67)  Time: 0.783s, 1307.13/s  (0.790s, 1296.14/s)  LR: 6.992e-04  Data: 0.010 (0.015)
Train: 223 [ 400/1251 ( 32%)]  Loss: 3.558 (3.66)  Time: 0.829s, 1234.59/s  (0.789s, 1297.38/s)  LR: 6.992e-04  Data: 0.009 (0.014)
Train: 223 [ 450/1251 ( 36%)]  Loss: 3.754 (3.67)  Time: 0.769s, 1331.77/s  (0.789s, 1298.63/s)  LR: 6.992e-04  Data: 0.010 (0.014)
Train: 223 [ 500/1251 ( 40%)]  Loss: 3.410 (3.64)  Time: 0.774s, 1322.38/s  (0.788s, 1298.73/s)  LR: 6.992e-04  Data: 0.010 (0.013)
Train: 223 [ 550/1251 ( 44%)]  Loss: 3.608 (3.64)  Time: 0.788s, 1300.08/s  (0.788s, 1300.06/s)  LR: 6.992e-04  Data: 0.011 (0.013)
Train: 223 [ 600/1251 ( 48%)]  Loss: 3.514 (3.63)  Time: 0.770s, 1329.11/s  (0.788s, 1300.00/s)  LR: 6.992e-04  Data: 0.010 (0.013)
Train: 223 [ 650/1251 ( 52%)]  Loss: 3.361 (3.61)  Time: 0.816s, 1254.20/s  (0.790s, 1296.96/s)  LR: 6.992e-04  Data: 0.013 (0.013)
Train: 223 [ 700/1251 ( 56%)]  Loss: 3.553 (3.61)  Time: 0.783s, 1307.08/s  (0.790s, 1296.64/s)  LR: 6.992e-04  Data: 0.010 (0.012)
Train: 223 [ 750/1251 ( 60%)]  Loss: 3.822 (3.62)  Time: 0.773s, 1324.80/s  (0.789s, 1298.04/s)  LR: 6.992e-04  Data: 0.009 (0.012)
Train: 223 [ 800/1251 ( 64%)]  Loss: 3.851 (3.63)  Time: 0.780s, 1313.53/s  (0.789s, 1298.15/s)  LR: 6.992e-04  Data: 0.010 (0.012)
Train: 223 [ 850/1251 ( 68%)]  Loss: 3.514 (3.63)  Time: 0.772s, 1325.91/s  (0.788s, 1299.19/s)  LR: 6.992e-04  Data: 0.010 (0.012)
Train: 223 [ 900/1251 ( 72%)]  Loss: 3.704 (3.63)  Time: 0.776s, 1319.54/s  (0.788s, 1299.90/s)  LR: 6.992e-04  Data: 0.010 (0.012)
Train: 223 [ 950/1251 ( 76%)]  Loss: 3.568 (3.63)  Time: 0.772s, 1326.37/s  (0.787s, 1300.46/s)  LR: 6.992e-04  Data: 0.010 (0.012)
Train: 223 [1000/1251 ( 80%)]  Loss: 3.754 (3.63)  Time: 0.814s, 1258.32/s  (0.787s, 1301.14/s)  LR: 6.992e-04  Data: 0.010 (0.012)
Train: 223 [1050/1251 ( 84%)]  Loss: 3.805 (3.64)  Time: 0.847s, 1208.97/s  (0.787s, 1301.63/s)  LR: 6.992e-04  Data: 0.010 (0.012)
Train: 223 [1100/1251 ( 88%)]  Loss: 3.668 (3.64)  Time: 0.772s, 1325.80/s  (0.786s, 1302.05/s)  LR: 6.992e-04  Data: 0.010 (0.012)
Train: 223 [1150/1251 ( 92%)]  Loss: 3.526 (3.64)  Time: 0.773s, 1325.54/s  (0.787s, 1301.60/s)  LR: 6.992e-04  Data: 0.010 (0.012)
Train: 223 [1200/1251 ( 96%)]  Loss: 3.556 (3.64)  Time: 0.771s, 1327.90/s  (0.786s, 1302.09/s)  LR: 6.992e-04  Data: 0.010 (0.011)
Train: 223 [1250/1251 (100%)]  Loss: 3.395 (3.63)  Time: 0.803s, 1274.75/s  (0.788s, 1300.27/s)  LR: 6.992e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.536 (1.536)  Loss:  0.6904 (0.6904)  Acc@1: 90.5273 (90.5273)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.194 (0.570)  Loss:  0.8115 (1.2870)  Acc@1: 85.4953 (73.8140)  Acc@5: 96.1085 (92.0060)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-219.pth.tar', 74.34000014160156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-213.pth.tar', 74.13599998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-215.pth.tar', 73.97000003662109)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-217.pth.tar', 73.95800006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-200.pth.tar', 73.95200006835937)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-222.pth.tar', 73.9219998828125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-211.pth.tar', 73.92000000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-203.pth.tar', 73.89600004150391)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-221.pth.tar', 73.84800006347656)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-223.pth.tar', 73.81400003417968)

Train: 224 [   0/1251 (  0%)]  Loss: 3.278 (3.28)  Time: 2.359s,  434.04/s  (2.359s,  434.04/s)  LR: 6.968e-04  Data: 1.629 (1.629)
Train: 224 [  50/1251 (  4%)]  Loss: 3.311 (3.29)  Time: 0.810s, 1264.64/s  (0.835s, 1226.88/s)  LR: 6.968e-04  Data: 0.009 (0.048)
Train: 224 [ 100/1251 (  8%)]  Loss: 3.515 (3.37)  Time: 0.774s, 1323.56/s  (0.810s, 1264.10/s)  LR: 6.968e-04  Data: 0.010 (0.029)
Train: 224 [ 150/1251 ( 12%)]  Loss: 3.511 (3.40)  Time: 0.784s, 1306.63/s  (0.803s, 1275.07/s)  LR: 6.968e-04  Data: 0.009 (0.023)
Train: 224 [ 200/1251 ( 16%)]  Loss: 3.549 (3.43)  Time: 0.779s, 1314.32/s  (0.804s, 1274.07/s)  LR: 6.968e-04  Data: 0.010 (0.020)
Train: 224 [ 250/1251 ( 20%)]  Loss: 3.638 (3.47)  Time: 0.776s, 1320.05/s  (0.799s, 1281.51/s)  LR: 6.968e-04  Data: 0.010 (0.018)
Train: 224 [ 300/1251 ( 24%)]  Loss: 3.373 (3.45)  Time: 0.782s, 1309.82/s  (0.797s, 1285.40/s)  LR: 6.968e-04  Data: 0.010 (0.016)
Train: 224 [ 350/1251 ( 28%)]  Loss: 4.067 (3.53)  Time: 0.772s, 1326.89/s  (0.794s, 1289.02/s)  LR: 6.968e-04  Data: 0.009 (0.016)
Train: 224 [ 400/1251 ( 32%)]  Loss: 3.733 (3.55)  Time: 0.772s, 1326.03/s  (0.793s, 1291.07/s)  LR: 6.968e-04  Data: 0.010 (0.015)
Train: 224 [ 450/1251 ( 36%)]  Loss: 3.663 (3.56)  Time: 0.774s, 1323.51/s  (0.792s, 1293.11/s)  LR: 6.968e-04  Data: 0.010 (0.014)
Train: 224 [ 500/1251 ( 40%)]  Loss: 3.677 (3.57)  Time: 0.771s, 1327.81/s  (0.791s, 1295.19/s)  LR: 6.968e-04  Data: 0.009 (0.014)
Train: 224 [ 550/1251 ( 44%)]  Loss: 3.305 (3.55)  Time: 0.801s, 1278.27/s  (0.790s, 1295.97/s)  LR: 6.968e-04  Data: 0.009 (0.014)
Train: 224 [ 600/1251 ( 48%)]  Loss: 3.629 (3.56)  Time: 0.774s, 1323.20/s  (0.790s, 1296.87/s)  LR: 6.968e-04  Data: 0.010 (0.013)
Train: 224 [ 650/1251 ( 52%)]  Loss: 3.443 (3.55)  Time: 0.787s, 1301.80/s  (0.790s, 1296.07/s)  LR: 6.968e-04  Data: 0.010 (0.013)
Train: 224 [ 700/1251 ( 56%)]  Loss: 3.693 (3.56)  Time: 0.776s, 1320.14/s  (0.789s, 1297.14/s)  LR: 6.968e-04  Data: 0.010 (0.013)
Train: 224 [ 750/1251 ( 60%)]  Loss: 3.458 (3.55)  Time: 0.779s, 1314.81/s  (0.789s, 1297.28/s)  LR: 6.968e-04  Data: 0.010 (0.013)
Train: 224 [ 800/1251 ( 64%)]  Loss: 3.449 (3.55)  Time: 0.778s, 1315.60/s  (0.790s, 1296.88/s)  LR: 6.968e-04  Data: 0.010 (0.013)
Train: 224 [ 850/1251 ( 68%)]  Loss: 3.556 (3.55)  Time: 0.772s, 1326.17/s  (0.789s, 1297.26/s)  LR: 6.968e-04  Data: 0.010 (0.012)
Train: 224 [ 900/1251 ( 72%)]  Loss: 3.569 (3.55)  Time: 0.780s, 1312.74/s  (0.789s, 1297.85/s)  LR: 6.968e-04  Data: 0.010 (0.012)
Train: 224 [ 950/1251 ( 76%)]  Loss: 3.505 (3.55)  Time: 0.825s, 1241.38/s  (0.789s, 1298.05/s)  LR: 6.968e-04  Data: 0.010 (0.012)
Train: 224 [1000/1251 ( 80%)]  Loss: 3.436 (3.54)  Time: 0.781s, 1311.46/s  (0.789s, 1298.64/s)  LR: 6.968e-04  Data: 0.009 (0.012)
Train: 224 [1050/1251 ( 84%)]  Loss: 3.571 (3.54)  Time: 0.773s, 1325.55/s  (0.788s, 1298.93/s)  LR: 6.968e-04  Data: 0.009 (0.012)
Train: 224 [1100/1251 ( 88%)]  Loss: 3.869 (3.56)  Time: 0.799s, 1281.16/s  (0.789s, 1298.62/s)  LR: 6.968e-04  Data: 0.012 (0.012)
Train: 224 [1150/1251 ( 92%)]  Loss: 3.369 (3.55)  Time: 0.773s, 1325.40/s  (0.788s, 1299.20/s)  LR: 6.968e-04  Data: 0.009 (0.012)
Train: 224 [1200/1251 ( 96%)]  Loss: 3.775 (3.56)  Time: 0.773s, 1325.03/s  (0.788s, 1299.53/s)  LR: 6.968e-04  Data: 0.010 (0.012)
Train: 224 [1250/1251 (100%)]  Loss: 3.570 (3.56)  Time: 0.759s, 1349.62/s  (0.788s, 1300.10/s)  LR: 6.968e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.530 (1.530)  Loss:  0.8262 (0.8262)  Acc@1: 89.5508 (89.5508)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.194 (0.575)  Loss:  0.8262 (1.3396)  Acc@1: 85.0236 (74.1620)  Acc@5: 97.2877 (92.3160)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-219.pth.tar', 74.34000014160156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-224.pth.tar', 74.1619999584961)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-213.pth.tar', 74.13599998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-215.pth.tar', 73.97000003662109)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-217.pth.tar', 73.95800006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-200.pth.tar', 73.95200006835937)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-222.pth.tar', 73.9219998828125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-211.pth.tar', 73.92000000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-203.pth.tar', 73.89600004150391)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-221.pth.tar', 73.84800006347656)

Train: 225 [   0/1251 (  0%)]  Loss: 3.490 (3.49)  Time: 2.521s,  406.22/s  (2.521s,  406.22/s)  LR: 6.944e-04  Data: 1.786 (1.786)
Train: 225 [  50/1251 (  4%)]  Loss: 3.224 (3.36)  Time: 0.822s, 1245.51/s  (0.818s, 1251.83/s)  LR: 6.944e-04  Data: 0.014 (0.045)
Train: 225 [ 100/1251 (  8%)]  Loss: 3.418 (3.38)  Time: 0.771s, 1327.65/s  (0.799s, 1281.28/s)  LR: 6.944e-04  Data: 0.010 (0.028)
Train: 225 [ 150/1251 ( 12%)]  Loss: 3.302 (3.36)  Time: 0.775s, 1320.78/s  (0.792s, 1292.55/s)  LR: 6.944e-04  Data: 0.010 (0.022)
Train: 225 [ 200/1251 ( 16%)]  Loss: 3.235 (3.33)  Time: 0.774s, 1322.36/s  (0.792s, 1293.74/s)  LR: 6.944e-04  Data: 0.010 (0.019)
Train: 225 [ 250/1251 ( 20%)]  Loss: 3.765 (3.41)  Time: 0.775s, 1321.03/s  (0.790s, 1296.36/s)  LR: 6.944e-04  Data: 0.009 (0.017)
Train: 225 [ 300/1251 ( 24%)]  Loss: 3.322 (3.39)  Time: 0.771s, 1327.85/s  (0.789s, 1298.32/s)  LR: 6.944e-04  Data: 0.010 (0.016)
Train: 225 [ 350/1251 ( 28%)]  Loss: 3.608 (3.42)  Time: 0.772s, 1325.62/s  (0.788s, 1299.68/s)  LR: 6.944e-04  Data: 0.010 (0.015)
Train: 225 [ 400/1251 ( 32%)]  Loss: 3.551 (3.44)  Time: 0.773s, 1324.65/s  (0.787s, 1300.73/s)  LR: 6.944e-04  Data: 0.009 (0.014)
Train: 225 [ 450/1251 ( 36%)]  Loss: 3.384 (3.43)  Time: 0.778s, 1316.27/s  (0.789s, 1298.17/s)  LR: 6.944e-04  Data: 0.010 (0.014)
Train: 225 [ 500/1251 ( 40%)]  Loss: 3.201 (3.41)  Time: 0.774s, 1322.19/s  (0.788s, 1299.32/s)  LR: 6.944e-04  Data: 0.010 (0.013)
Train: 225 [ 550/1251 ( 44%)]  Loss: 3.695 (3.43)  Time: 0.787s, 1301.54/s  (0.788s, 1300.11/s)  LR: 6.944e-04  Data: 0.009 (0.013)
Train: 225 [ 600/1251 ( 48%)]  Loss: 3.421 (3.43)  Time: 0.771s, 1328.04/s  (0.787s, 1300.46/s)  LR: 6.944e-04  Data: 0.009 (0.013)
Train: 225 [ 650/1251 ( 52%)]  Loss: 3.810 (3.46)  Time: 0.825s, 1241.47/s  (0.788s, 1299.83/s)  LR: 6.944e-04  Data: 0.011 (0.013)
Train: 225 [ 700/1251 ( 56%)]  Loss: 3.630 (3.47)  Time: 0.770s, 1329.06/s  (0.788s, 1299.04/s)  LR: 6.944e-04  Data: 0.009 (0.012)
Train: 225 [ 750/1251 ( 60%)]  Loss: 3.379 (3.46)  Time: 0.788s, 1298.78/s  (0.788s, 1299.39/s)  LR: 6.944e-04  Data: 0.013 (0.012)
Train: 225 [ 800/1251 ( 64%)]  Loss: 3.405 (3.46)  Time: 0.777s, 1317.07/s  (0.787s, 1300.56/s)  LR: 6.944e-04  Data: 0.010 (0.012)
Train: 225 [ 850/1251 ( 68%)]  Loss: 3.666 (3.47)  Time: 0.774s, 1323.45/s  (0.787s, 1301.20/s)  LR: 6.944e-04  Data: 0.010 (0.012)
Train: 225 [ 900/1251 ( 72%)]  Loss: 2.918 (3.44)  Time: 0.866s, 1182.31/s  (0.787s, 1301.45/s)  LR: 6.944e-04  Data: 0.010 (0.012)
Train: 225 [ 950/1251 ( 76%)]  Loss: 3.573 (3.45)  Time: 0.777s, 1317.23/s  (0.787s, 1300.56/s)  LR: 6.944e-04  Data: 0.010 (0.012)
Train: 225 [1000/1251 ( 80%)]  Loss: 3.551 (3.45)  Time: 0.775s, 1320.45/s  (0.787s, 1300.86/s)  LR: 6.944e-04  Data: 0.010 (0.012)
Train: 225 [1050/1251 ( 84%)]  Loss: 3.410 (3.45)  Time: 0.790s, 1296.58/s  (0.787s, 1300.69/s)  LR: 6.944e-04  Data: 0.011 (0.012)
Train: 225 [1100/1251 ( 88%)]  Loss: 3.606 (3.46)  Time: 0.784s, 1306.38/s  (0.787s, 1301.28/s)  LR: 6.944e-04  Data: 0.010 (0.012)
Train: 225 [1150/1251 ( 92%)]  Loss: 3.222 (3.45)  Time: 0.774s, 1322.97/s  (0.787s, 1301.82/s)  LR: 6.944e-04  Data: 0.010 (0.012)
Train: 225 [1200/1251 ( 96%)]  Loss: 3.649 (3.46)  Time: 0.785s, 1304.92/s  (0.786s, 1302.00/s)  LR: 6.944e-04  Data: 0.011 (0.012)
Train: 225 [1250/1251 (100%)]  Loss: 3.277 (3.45)  Time: 0.759s, 1348.52/s  (0.787s, 1301.09/s)  LR: 6.944e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.598 (1.598)  Loss:  0.7700 (0.7700)  Acc@1: 88.2812 (88.2812)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.193 (0.571)  Loss:  0.9180 (1.3433)  Acc@1: 84.3160 (74.1540)  Acc@5: 96.2264 (92.2780)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-219.pth.tar', 74.34000014160156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-224.pth.tar', 74.1619999584961)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-225.pth.tar', 74.1540000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-213.pth.tar', 74.13599998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-215.pth.tar', 73.97000003662109)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-217.pth.tar', 73.95800006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-200.pth.tar', 73.95200006835937)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-222.pth.tar', 73.9219998828125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-211.pth.tar', 73.92000000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-203.pth.tar', 73.89600004150391)

Train: 226 [   0/1251 (  0%)]  Loss: 3.908 (3.91)  Time: 2.379s,  430.44/s  (2.379s,  430.44/s)  LR: 6.920e-04  Data: 1.650 (1.650)
Train: 226 [  50/1251 (  4%)]  Loss: 3.660 (3.78)  Time: 0.775s, 1322.10/s  (0.824s, 1243.34/s)  LR: 6.920e-04  Data: 0.010 (0.048)
Train: 226 [ 100/1251 (  8%)]  Loss: 3.534 (3.70)  Time: 0.774s, 1323.85/s  (0.804s, 1273.90/s)  LR: 6.920e-04  Data: 0.010 (0.030)
Train: 226 [ 150/1251 ( 12%)]  Loss: 3.559 (3.67)  Time: 0.771s, 1328.81/s  (0.796s, 1286.25/s)  LR: 6.920e-04  Data: 0.009 (0.023)
Train: 226 [ 200/1251 ( 16%)]  Loss: 3.362 (3.60)  Time: 0.774s, 1323.07/s  (0.793s, 1291.72/s)  LR: 6.920e-04  Data: 0.010 (0.020)
Train: 226 [ 250/1251 ( 20%)]  Loss: 3.578 (3.60)  Time: 0.771s, 1327.72/s  (0.792s, 1292.20/s)  LR: 6.920e-04  Data: 0.010 (0.018)
Train: 226 [ 300/1251 ( 24%)]  Loss: 3.495 (3.59)  Time: 0.811s, 1263.01/s  (0.791s, 1294.44/s)  LR: 6.920e-04  Data: 0.010 (0.017)
Train: 226 [ 350/1251 ( 28%)]  Loss: 3.350 (3.56)  Time: 0.775s, 1321.09/s  (0.791s, 1295.07/s)  LR: 6.920e-04  Data: 0.010 (0.016)
Train: 226 [ 400/1251 ( 32%)]  Loss: 3.528 (3.55)  Time: 0.838s, 1221.84/s  (0.794s, 1290.42/s)  LR: 6.920e-04  Data: 0.009 (0.015)
Train: 226 [ 450/1251 ( 36%)]  Loss: 3.484 (3.55)  Time: 0.805s, 1271.46/s  (0.793s, 1291.66/s)  LR: 6.920e-04  Data: 0.010 (0.014)
Train: 226 [ 500/1251 ( 40%)]  Loss: 3.348 (3.53)  Time: 0.774s, 1322.67/s  (0.794s, 1290.46/s)  LR: 6.920e-04  Data: 0.010 (0.014)
Train: 226 [ 550/1251 ( 44%)]  Loss: 3.747 (3.55)  Time: 0.774s, 1322.33/s  (0.792s, 1292.42/s)  LR: 6.920e-04  Data: 0.011 (0.014)
Train: 226 [ 600/1251 ( 48%)]  Loss: 3.450 (3.54)  Time: 0.775s, 1320.47/s  (0.791s, 1294.25/s)  LR: 6.920e-04  Data: 0.010 (0.013)
Train: 226 [ 650/1251 ( 52%)]  Loss: 3.977 (3.57)  Time: 0.773s, 1324.09/s  (0.790s, 1295.82/s)  LR: 6.920e-04  Data: 0.010 (0.013)
Train: 226 [ 700/1251 ( 56%)]  Loss: 3.091 (3.54)  Time: 0.782s, 1309.64/s  (0.790s, 1296.94/s)  LR: 6.920e-04  Data: 0.012 (0.013)
Train: 226 [ 750/1251 ( 60%)]  Loss: 3.647 (3.54)  Time: 0.783s, 1307.92/s  (0.790s, 1296.98/s)  LR: 6.920e-04  Data: 0.010 (0.013)
Train: 226 [ 800/1251 ( 64%)]  Loss: 3.857 (3.56)  Time: 0.774s, 1323.12/s  (0.789s, 1298.15/s)  LR: 6.920e-04  Data: 0.009 (0.013)
Train: 226 [ 850/1251 ( 68%)]  Loss: 3.341 (3.55)  Time: 0.822s, 1245.54/s  (0.789s, 1297.90/s)  LR: 6.920e-04  Data: 0.010 (0.012)
Train: 226 [ 900/1251 ( 72%)]  Loss: 3.397 (3.54)  Time: 0.777s, 1318.31/s  (0.788s, 1298.75/s)  LR: 6.920e-04  Data: 0.009 (0.012)
Train: 226 [ 950/1251 ( 76%)]  Loss: 3.625 (3.55)  Time: 0.773s, 1324.29/s  (0.788s, 1298.87/s)  LR: 6.920e-04  Data: 0.009 (0.012)
Train: 226 [1000/1251 ( 80%)]  Loss: 3.205 (3.53)  Time: 0.836s, 1225.05/s  (0.788s, 1299.30/s)  LR: 6.920e-04  Data: 0.009 (0.012)
Train: 226 [1050/1251 ( 84%)]  Loss: 3.702 (3.54)  Time: 0.773s, 1324.93/s  (0.788s, 1299.75/s)  LR: 6.920e-04  Data: 0.010 (0.012)
Train: 226 [1100/1251 ( 88%)]  Loss: 3.708 (3.55)  Time: 0.781s, 1311.22/s  (0.787s, 1300.44/s)  LR: 6.920e-04  Data: 0.011 (0.012)
Train: 226 [1150/1251 ( 92%)]  Loss: 3.243 (3.53)  Time: 0.777s, 1318.05/s  (0.788s, 1298.88/s)  LR: 6.920e-04  Data: 0.010 (0.012)
Train: 226 [1200/1251 ( 96%)]  Loss: 3.472 (3.53)  Time: 0.781s, 1310.35/s  (0.788s, 1299.74/s)  LR: 6.920e-04  Data: 0.010 (0.012)
Train: 226 [1250/1251 (100%)]  Loss: 3.841 (3.54)  Time: 0.762s, 1344.64/s  (0.788s, 1300.12/s)  LR: 6.920e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.568 (1.568)  Loss:  0.8564 (0.8564)  Acc@1: 90.3320 (90.3320)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.193 (0.563)  Loss:  0.9155 (1.4136)  Acc@1: 85.8491 (74.4080)  Acc@5: 97.1698 (92.4240)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-226.pth.tar', 74.40799992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-219.pth.tar', 74.34000014160156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-224.pth.tar', 74.1619999584961)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-225.pth.tar', 74.1540000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-213.pth.tar', 74.13599998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-215.pth.tar', 73.97000003662109)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-217.pth.tar', 73.95800006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-200.pth.tar', 73.95200006835937)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-222.pth.tar', 73.9219998828125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-211.pth.tar', 73.92000000732422)

Train: 227 [   0/1251 (  0%)]  Loss: 3.109 (3.11)  Time: 2.246s,  455.87/s  (2.246s,  455.87/s)  LR: 6.896e-04  Data: 1.519 (1.519)
Train: 227 [  50/1251 (  4%)]  Loss: 3.964 (3.54)  Time: 0.779s, 1314.94/s  (0.835s, 1225.74/s)  LR: 6.896e-04  Data: 0.009 (0.044)
Train: 227 [ 100/1251 (  8%)]  Loss: 3.684 (3.59)  Time: 0.809s, 1265.88/s  (0.809s, 1265.26/s)  LR: 6.896e-04  Data: 0.009 (0.027)
Train: 227 [ 150/1251 ( 12%)]  Loss: 3.517 (3.57)  Time: 0.771s, 1327.60/s  (0.800s, 1280.58/s)  LR: 6.896e-04  Data: 0.009 (0.021)
Train: 227 [ 200/1251 ( 16%)]  Loss: 3.752 (3.61)  Time: 0.782s, 1309.92/s  (0.799s, 1281.19/s)  LR: 6.896e-04  Data: 0.011 (0.019)
Train: 227 [ 250/1251 ( 20%)]  Loss: 3.383 (3.57)  Time: 0.837s, 1223.22/s  (0.796s, 1286.33/s)  LR: 6.896e-04  Data: 0.010 (0.017)
Train: 227 [ 300/1251 ( 24%)]  Loss: 3.637 (3.58)  Time: 0.772s, 1327.09/s  (0.794s, 1289.72/s)  LR: 6.896e-04  Data: 0.009 (0.016)
Train: 227 [ 350/1251 ( 28%)]  Loss: 3.548 (3.57)  Time: 0.775s, 1321.63/s  (0.792s, 1292.94/s)  LR: 6.896e-04  Data: 0.011 (0.015)
Train: 227 [ 400/1251 ( 32%)]  Loss: 3.788 (3.60)  Time: 0.773s, 1325.52/s  (0.792s, 1292.91/s)  LR: 6.896e-04  Data: 0.010 (0.014)
Train: 227 [ 450/1251 ( 36%)]  Loss: 3.496 (3.59)  Time: 0.854s, 1199.16/s  (0.791s, 1294.97/s)  LR: 6.896e-04  Data: 0.010 (0.014)
Train: 227 [ 500/1251 ( 40%)]  Loss: 3.818 (3.61)  Time: 0.772s, 1326.08/s  (0.790s, 1296.02/s)  LR: 6.896e-04  Data: 0.010 (0.013)
Train: 227 [ 550/1251 ( 44%)]  Loss: 3.719 (3.62)  Time: 0.775s, 1321.05/s  (0.792s, 1293.73/s)  LR: 6.896e-04  Data: 0.010 (0.013)
Train: 227 [ 600/1251 ( 48%)]  Loss: 3.572 (3.61)  Time: 0.772s, 1325.78/s  (0.791s, 1293.85/s)  LR: 6.896e-04  Data: 0.009 (0.013)
Train: 227 [ 650/1251 ( 52%)]  Loss: 3.709 (3.62)  Time: 0.776s, 1319.58/s  (0.792s, 1292.63/s)  LR: 6.896e-04  Data: 0.010 (0.013)
Train: 227 [ 700/1251 ( 56%)]  Loss: 3.561 (3.62)  Time: 0.782s, 1310.10/s  (0.792s, 1293.51/s)  LR: 6.896e-04  Data: 0.012 (0.012)
Train: 227 [ 750/1251 ( 60%)]  Loss: 3.582 (3.62)  Time: 0.860s, 1190.17/s  (0.791s, 1294.29/s)  LR: 6.896e-04  Data: 0.010 (0.012)
Train: 227 [ 800/1251 ( 64%)]  Loss: 3.552 (3.61)  Time: 0.818s, 1251.54/s  (0.791s, 1294.91/s)  LR: 6.896e-04  Data: 0.009 (0.012)
Train: 227 [ 850/1251 ( 68%)]  Loss: 3.628 (3.61)  Time: 0.784s, 1306.73/s  (0.790s, 1295.80/s)  LR: 6.896e-04  Data: 0.010 (0.012)
Train: 227 [ 900/1251 ( 72%)]  Loss: 3.167 (3.59)  Time: 0.777s, 1317.08/s  (0.790s, 1296.69/s)  LR: 6.896e-04  Data: 0.010 (0.012)
Train: 227 [ 950/1251 ( 76%)]  Loss: 3.653 (3.59)  Time: 0.788s, 1299.99/s  (0.790s, 1296.14/s)  LR: 6.896e-04  Data: 0.010 (0.012)
Train: 227 [1000/1251 ( 80%)]  Loss: 3.604 (3.59)  Time: 0.773s, 1325.04/s  (0.790s, 1296.75/s)  LR: 6.896e-04  Data: 0.010 (0.012)
Train: 227 [1050/1251 ( 84%)]  Loss: 3.504 (3.59)  Time: 0.771s, 1327.62/s  (0.789s, 1297.40/s)  LR: 6.896e-04  Data: 0.009 (0.012)
Train: 227 [1100/1251 ( 88%)]  Loss: 3.624 (3.59)  Time: 0.776s, 1318.75/s  (0.789s, 1297.99/s)  LR: 6.896e-04  Data: 0.011 (0.012)
Train: 227 [1150/1251 ( 92%)]  Loss: 3.411 (3.58)  Time: 0.773s, 1325.00/s  (0.789s, 1298.64/s)  LR: 6.896e-04  Data: 0.010 (0.011)
Train: 227 [1200/1251 ( 96%)]  Loss: 3.263 (3.57)  Time: 0.791s, 1294.31/s  (0.789s, 1298.24/s)  LR: 6.896e-04  Data: 0.009 (0.011)
Train: 227 [1250/1251 (100%)]  Loss: 3.595 (3.57)  Time: 0.762s, 1344.21/s  (0.788s, 1298.84/s)  LR: 6.896e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.608 (1.608)  Loss:  0.8857 (0.8857)  Acc@1: 89.4531 (89.4531)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.194 (0.561)  Loss:  0.9653 (1.3781)  Acc@1: 84.7877 (74.1280)  Acc@5: 96.1085 (92.2100)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-226.pth.tar', 74.40799992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-219.pth.tar', 74.34000014160156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-224.pth.tar', 74.1619999584961)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-225.pth.tar', 74.1540000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-213.pth.tar', 74.13599998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-227.pth.tar', 74.12799998535156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-215.pth.tar', 73.97000003662109)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-217.pth.tar', 73.95800006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-200.pth.tar', 73.95200006835937)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-222.pth.tar', 73.9219998828125)

Train: 228 [   0/1251 (  0%)]  Loss: 3.451 (3.45)  Time: 2.284s,  448.39/s  (2.284s,  448.39/s)  LR: 6.872e-04  Data: 1.539 (1.539)
Train: 228 [  50/1251 (  4%)]  Loss: 3.857 (3.65)  Time: 0.789s, 1297.28/s  (0.827s, 1237.70/s)  LR: 6.872e-04  Data: 0.014 (0.044)
Train: 228 [ 100/1251 (  8%)]  Loss: 3.531 (3.61)  Time: 0.774s, 1323.82/s  (0.806s, 1271.02/s)  LR: 6.872e-04  Data: 0.010 (0.027)
Train: 228 [ 150/1251 ( 12%)]  Loss: 3.690 (3.63)  Time: 0.826s, 1239.99/s  (0.804s, 1274.10/s)  LR: 6.872e-04  Data: 0.013 (0.022)
Train: 228 [ 200/1251 ( 16%)]  Loss: 4.001 (3.71)  Time: 0.784s, 1305.55/s  (0.802s, 1277.20/s)  LR: 6.872e-04  Data: 0.009 (0.019)
Train: 228 [ 250/1251 ( 20%)]  Loss: 3.607 (3.69)  Time: 0.772s, 1326.10/s  (0.799s, 1281.63/s)  LR: 6.872e-04  Data: 0.010 (0.017)
Train: 228 [ 300/1251 ( 24%)]  Loss: 3.287 (3.63)  Time: 0.771s, 1327.71/s  (0.796s, 1286.79/s)  LR: 6.872e-04  Data: 0.010 (0.016)
Train: 228 [ 350/1251 ( 28%)]  Loss: 3.312 (3.59)  Time: 0.772s, 1327.11/s  (0.794s, 1290.33/s)  LR: 6.872e-04  Data: 0.010 (0.015)
Train: 228 [ 400/1251 ( 32%)]  Loss: 3.406 (3.57)  Time: 0.772s, 1327.25/s  (0.792s, 1292.46/s)  LR: 6.872e-04  Data: 0.009 (0.014)
Train: 228 [ 450/1251 ( 36%)]  Loss: 3.599 (3.57)  Time: 0.774s, 1322.56/s  (0.792s, 1293.63/s)  LR: 6.872e-04  Data: 0.010 (0.014)
Train: 228 [ 500/1251 ( 40%)]  Loss: 3.369 (3.56)  Time: 0.774s, 1323.12/s  (0.791s, 1295.30/s)  LR: 6.872e-04  Data: 0.010 (0.014)
Train: 228 [ 550/1251 ( 44%)]  Loss: 3.886 (3.58)  Time: 0.774s, 1322.68/s  (0.790s, 1296.59/s)  LR: 6.872e-04  Data: 0.010 (0.013)
Train: 228 [ 600/1251 ( 48%)]  Loss: 3.373 (3.57)  Time: 0.772s, 1326.59/s  (0.789s, 1297.75/s)  LR: 6.872e-04  Data: 0.011 (0.013)
Train: 228 [ 650/1251 ( 52%)]  Loss: 3.734 (3.58)  Time: 0.780s, 1312.89/s  (0.789s, 1297.62/s)  LR: 6.872e-04  Data: 0.010 (0.013)
Train: 228 [ 700/1251 ( 56%)]  Loss: 3.280 (3.56)  Time: 0.819s, 1250.95/s  (0.790s, 1296.34/s)  LR: 6.872e-04  Data: 0.012 (0.013)
Train: 228 [ 750/1251 ( 60%)]  Loss: 3.246 (3.54)  Time: 0.775s, 1321.82/s  (0.790s, 1296.39/s)  LR: 6.872e-04  Data: 0.010 (0.012)
Train: 228 [ 800/1251 ( 64%)]  Loss: 3.723 (3.55)  Time: 0.772s, 1327.17/s  (0.789s, 1297.06/s)  LR: 6.872e-04  Data: 0.009 (0.012)
Train: 228 [ 850/1251 ( 68%)]  Loss: 3.655 (3.56)  Time: 0.774s, 1323.15/s  (0.789s, 1297.63/s)  LR: 6.872e-04  Data: 0.009 (0.012)
Train: 228 [ 900/1251 ( 72%)]  Loss: 3.758 (3.57)  Time: 0.785s, 1304.32/s  (0.789s, 1298.30/s)  LR: 6.872e-04  Data: 0.009 (0.012)
Train: 228 [ 950/1251 ( 76%)]  Loss: 3.414 (3.56)  Time: 0.770s, 1329.03/s  (0.788s, 1298.95/s)  LR: 6.872e-04  Data: 0.009 (0.012)
Train: 228 [1000/1251 ( 80%)]  Loss: 3.318 (3.55)  Time: 0.772s, 1326.59/s  (0.788s, 1299.65/s)  LR: 6.872e-04  Data: 0.010 (0.012)
Train: 228 [1050/1251 ( 84%)]  Loss: 3.429 (3.54)  Time: 0.784s, 1306.60/s  (0.788s, 1300.30/s)  LR: 6.872e-04  Data: 0.010 (0.012)
Train: 228 [1100/1251 ( 88%)]  Loss: 3.573 (3.54)  Time: 0.817s, 1252.91/s  (0.787s, 1300.54/s)  LR: 6.872e-04  Data: 0.011 (0.012)
Train: 228 [1150/1251 ( 92%)]  Loss: 3.507 (3.54)  Time: 0.771s, 1328.14/s  (0.787s, 1300.88/s)  LR: 6.872e-04  Data: 0.010 (0.012)
Train: 228 [1200/1251 ( 96%)]  Loss: 3.409 (3.54)  Time: 0.774s, 1323.05/s  (0.787s, 1301.43/s)  LR: 6.872e-04  Data: 0.010 (0.012)
Train: 228 [1250/1251 (100%)]  Loss: 3.361 (3.53)  Time: 0.772s, 1326.81/s  (0.787s, 1301.95/s)  LR: 6.872e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.523 (1.523)  Loss:  0.7812 (0.7812)  Acc@1: 87.9883 (87.9883)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.193 (0.561)  Loss:  0.8442 (1.3253)  Acc@1: 84.9057 (74.3380)  Acc@5: 96.4623 (92.1780)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-226.pth.tar', 74.40799992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-219.pth.tar', 74.34000014160156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-228.pth.tar', 74.33799990722656)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-224.pth.tar', 74.1619999584961)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-225.pth.tar', 74.1540000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-213.pth.tar', 74.13599998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-227.pth.tar', 74.12799998535156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-215.pth.tar', 73.97000003662109)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-217.pth.tar', 73.95800006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-200.pth.tar', 73.95200006835937)

Train: 229 [   0/1251 (  0%)]  Loss: 3.817 (3.82)  Time: 2.180s,  469.73/s  (2.180s,  469.73/s)  LR: 6.848e-04  Data: 1.465 (1.465)
Train: 229 [  50/1251 (  4%)]  Loss: 3.836 (3.83)  Time: 0.774s, 1323.71/s  (0.822s, 1245.47/s)  LR: 6.848e-04  Data: 0.010 (0.043)
Train: 229 [ 100/1251 (  8%)]  Loss: 3.472 (3.71)  Time: 0.781s, 1310.71/s  (0.803s, 1274.94/s)  LR: 6.848e-04  Data: 0.010 (0.027)
Train: 229 [ 150/1251 ( 12%)]  Loss: 3.672 (3.70)  Time: 0.779s, 1313.84/s  (0.797s, 1285.24/s)  LR: 6.848e-04  Data: 0.010 (0.021)
Train: 229 [ 200/1251 ( 16%)]  Loss: 3.855 (3.73)  Time: 0.773s, 1323.90/s  (0.795s, 1288.80/s)  LR: 6.848e-04  Data: 0.010 (0.018)
Train: 229 [ 250/1251 ( 20%)]  Loss: 3.416 (3.68)  Time: 0.831s, 1232.46/s  (0.794s, 1289.41/s)  LR: 6.848e-04  Data: 0.014 (0.017)
Train: 229 [ 300/1251 ( 24%)]  Loss: 3.341 (3.63)  Time: 0.774s, 1322.36/s  (0.794s, 1289.13/s)  LR: 6.848e-04  Data: 0.010 (0.016)
Train: 229 [ 350/1251 ( 28%)]  Loss: 3.662 (3.63)  Time: 0.828s, 1237.40/s  (0.792s, 1292.41/s)  LR: 6.848e-04  Data: 0.011 (0.015)
Train: 229 [ 400/1251 ( 32%)]  Loss: 3.520 (3.62)  Time: 0.827s, 1238.71/s  (0.792s, 1292.72/s)  LR: 6.848e-04  Data: 0.009 (0.014)
Train: 229 [ 450/1251 ( 36%)]  Loss: 3.896 (3.65)  Time: 0.771s, 1327.62/s  (0.790s, 1295.41/s)  LR: 6.848e-04  Data: 0.010 (0.014)
Train: 229 [ 500/1251 ( 40%)]  Loss: 3.411 (3.63)  Time: 0.777s, 1318.62/s  (0.789s, 1297.21/s)  LR: 6.848e-04  Data: 0.010 (0.013)
Train: 229 [ 550/1251 ( 44%)]  Loss: 3.534 (3.62)  Time: 0.773s, 1324.98/s  (0.789s, 1298.34/s)  LR: 6.848e-04  Data: 0.011 (0.013)
Train: 229 [ 600/1251 ( 48%)]  Loss: 3.556 (3.61)  Time: 0.826s, 1239.81/s  (0.789s, 1298.30/s)  LR: 6.848e-04  Data: 0.013 (0.013)
Train: 229 [ 650/1251 ( 52%)]  Loss: 3.761 (3.63)  Time: 0.780s, 1312.03/s  (0.789s, 1298.14/s)  LR: 6.848e-04  Data: 0.010 (0.013)
Train: 229 [ 700/1251 ( 56%)]  Loss: 3.809 (3.64)  Time: 0.772s, 1326.91/s  (0.789s, 1298.59/s)  LR: 6.848e-04  Data: 0.010 (0.012)
Train: 229 [ 750/1251 ( 60%)]  Loss: 3.587 (3.63)  Time: 0.777s, 1317.85/s  (0.789s, 1298.45/s)  LR: 6.848e-04  Data: 0.009 (0.012)
Train: 229 [ 800/1251 ( 64%)]  Loss: 3.560 (3.63)  Time: 0.773s, 1324.56/s  (0.789s, 1298.58/s)  LR: 6.848e-04  Data: 0.010 (0.012)
Train: 229 [ 850/1251 ( 68%)]  Loss: 3.501 (3.62)  Time: 0.777s, 1317.07/s  (0.788s, 1299.51/s)  LR: 6.848e-04  Data: 0.009 (0.012)
Train: 229 [ 900/1251 ( 72%)]  Loss: 3.675 (3.63)  Time: 0.870s, 1177.39/s  (0.788s, 1299.44/s)  LR: 6.848e-04  Data: 0.010 (0.012)
Train: 229 [ 950/1251 ( 76%)]  Loss: 3.243 (3.61)  Time: 0.777s, 1317.62/s  (0.788s, 1300.22/s)  LR: 6.848e-04  Data: 0.014 (0.012)
Train: 229 [1000/1251 ( 80%)]  Loss: 3.759 (3.61)  Time: 0.787s, 1300.80/s  (0.788s, 1299.49/s)  LR: 6.848e-04  Data: 0.009 (0.012)
Train: 229 [1050/1251 ( 84%)]  Loss: 3.516 (3.61)  Time: 0.772s, 1326.71/s  (0.788s, 1299.37/s)  LR: 6.848e-04  Data: 0.010 (0.012)
Train: 229 [1100/1251 ( 88%)]  Loss: 3.656 (3.61)  Time: 0.774s, 1323.74/s  (0.788s, 1300.12/s)  LR: 6.848e-04  Data: 0.009 (0.012)
Train: 229 [1150/1251 ( 92%)]  Loss: 3.761 (3.62)  Time: 0.772s, 1326.63/s  (0.787s, 1300.69/s)  LR: 6.848e-04  Data: 0.009 (0.012)
Train: 229 [1200/1251 ( 96%)]  Loss: 3.335 (3.61)  Time: 0.772s, 1326.00/s  (0.787s, 1300.57/s)  LR: 6.848e-04  Data: 0.010 (0.012)
Train: 229 [1250/1251 (100%)]  Loss: 3.540 (3.60)  Time: 0.765s, 1339.00/s  (0.787s, 1300.78/s)  LR: 6.848e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.558 (1.558)  Loss:  0.8242 (0.8242)  Acc@1: 89.2578 (89.2578)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.193 (0.570)  Loss:  0.8398 (1.3871)  Acc@1: 85.9670 (73.5760)  Acc@5: 95.9906 (91.8200)
Train: 230 [   0/1251 (  0%)]  Loss: 3.639 (3.64)  Time: 2.224s,  460.48/s  (2.224s,  460.48/s)  LR: 6.824e-04  Data: 1.499 (1.499)
Train: 230 [  50/1251 (  4%)]  Loss: 3.456 (3.55)  Time: 0.775s, 1321.41/s  (0.829s, 1234.88/s)  LR: 6.824e-04  Data: 0.010 (0.044)
Train: 230 [ 100/1251 (  8%)]  Loss: 3.578 (3.56)  Time: 0.784s, 1306.87/s  (0.806s, 1270.00/s)  LR: 6.824e-04  Data: 0.009 (0.027)
Train: 230 [ 150/1251 ( 12%)]  Loss: 3.658 (3.58)  Time: 0.779s, 1313.68/s  (0.799s, 1281.35/s)  LR: 6.824e-04  Data: 0.010 (0.022)
Train: 230 [ 200/1251 ( 16%)]  Loss: 3.517 (3.57)  Time: 0.774s, 1322.38/s  (0.795s, 1288.04/s)  LR: 6.824e-04  Data: 0.010 (0.019)
Train: 230 [ 250/1251 ( 20%)]  Loss: 3.383 (3.54)  Time: 0.769s, 1331.56/s  (0.793s, 1291.83/s)  LR: 6.824e-04  Data: 0.009 (0.017)
Train: 230 [ 300/1251 ( 24%)]  Loss: 3.745 (3.57)  Time: 0.771s, 1327.72/s  (0.792s, 1293.13/s)  LR: 6.824e-04  Data: 0.010 (0.016)
Train: 230 [ 350/1251 ( 28%)]  Loss: 3.722 (3.59)  Time: 0.869s, 1177.92/s  (0.790s, 1296.54/s)  LR: 6.824e-04  Data: 0.009 (0.015)
Train: 230 [ 400/1251 ( 32%)]  Loss: 3.482 (3.58)  Time: 0.773s, 1324.46/s  (0.790s, 1296.17/s)  LR: 6.824e-04  Data: 0.011 (0.014)
Train: 230 [ 450/1251 ( 36%)]  Loss: 3.697 (3.59)  Time: 0.831s, 1232.40/s  (0.789s, 1298.06/s)  LR: 6.824e-04  Data: 0.009 (0.014)
Train: 230 [ 500/1251 ( 40%)]  Loss: 3.395 (3.57)  Time: 0.769s, 1331.65/s  (0.789s, 1297.47/s)  LR: 6.824e-04  Data: 0.009 (0.014)
Train: 230 [ 550/1251 ( 44%)]  Loss: 3.412 (3.56)  Time: 0.772s, 1325.78/s  (0.789s, 1297.53/s)  LR: 6.824e-04  Data: 0.010 (0.013)
Train: 230 [ 600/1251 ( 48%)]  Loss: 3.414 (3.55)  Time: 0.776s, 1319.66/s  (0.789s, 1298.60/s)  LR: 6.824e-04  Data: 0.009 (0.013)
Train: 230 [ 650/1251 ( 52%)]  Loss: 3.404 (3.54)  Time: 0.774s, 1322.87/s  (0.789s, 1298.62/s)  LR: 6.824e-04  Data: 0.010 (0.013)
Train: 230 [ 700/1251 ( 56%)]  Loss: 3.844 (3.56)  Time: 0.831s, 1232.15/s  (0.788s, 1299.33/s)  LR: 6.824e-04  Data: 0.010 (0.013)
Train: 230 [ 750/1251 ( 60%)]  Loss: 3.783 (3.57)  Time: 0.839s, 1220.47/s  (0.789s, 1298.55/s)  LR: 6.824e-04  Data: 0.010 (0.012)
Train: 230 [ 800/1251 ( 64%)]  Loss: 3.726 (3.58)  Time: 0.816s, 1254.39/s  (0.789s, 1298.62/s)  LR: 6.824e-04  Data: 0.010 (0.012)
Train: 230 [ 850/1251 ( 68%)]  Loss: 3.575 (3.58)  Time: 0.843s, 1214.50/s  (0.788s, 1299.24/s)  LR: 6.824e-04  Data: 0.010 (0.012)
Train: 230 [ 900/1251 ( 72%)]  Loss: 3.520 (3.58)  Time: 0.778s, 1316.34/s  (0.788s, 1299.78/s)  LR: 6.824e-04  Data: 0.010 (0.012)
Train: 230 [ 950/1251 ( 76%)]  Loss: 3.317 (3.56)  Time: 0.772s, 1326.17/s  (0.787s, 1300.65/s)  LR: 6.824e-04  Data: 0.010 (0.012)
Train: 230 [1000/1251 ( 80%)]  Loss: 3.753 (3.57)  Time: 0.773s, 1324.30/s  (0.787s, 1301.14/s)  LR: 6.824e-04  Data: 0.012 (0.012)
Train: 230 [1050/1251 ( 84%)]  Loss: 3.385 (3.56)  Time: 0.776s, 1319.80/s  (0.787s, 1301.40/s)  LR: 6.824e-04  Data: 0.010 (0.012)
Train: 230 [1100/1251 ( 88%)]  Loss: 3.628 (3.57)  Time: 0.791s, 1294.07/s  (0.787s, 1301.63/s)  LR: 6.824e-04  Data: 0.013 (0.012)
Train: 230 [1150/1251 ( 92%)]  Loss: 3.623 (3.57)  Time: 0.782s, 1310.15/s  (0.786s, 1302.28/s)  LR: 6.824e-04  Data: 0.010 (0.012)
Train: 230 [1200/1251 ( 96%)]  Loss: 3.600 (3.57)  Time: 0.773s, 1324.07/s  (0.786s, 1302.85/s)  LR: 6.824e-04  Data: 0.010 (0.012)
Train: 230 [1250/1251 (100%)]  Loss: 3.532 (3.57)  Time: 0.760s, 1347.19/s  (0.786s, 1303.11/s)  LR: 6.824e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.531 (1.531)  Loss:  0.7539 (0.7539)  Acc@1: 88.8672 (88.8672)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.194 (0.562)  Loss:  0.8608 (1.3244)  Acc@1: 85.3774 (74.1600)  Acc@5: 96.2264 (92.1820)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-226.pth.tar', 74.40799992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-219.pth.tar', 74.34000014160156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-228.pth.tar', 74.33799990722656)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-224.pth.tar', 74.1619999584961)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-230.pth.tar', 74.15999998291015)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-225.pth.tar', 74.1540000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-213.pth.tar', 74.13599998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-227.pth.tar', 74.12799998535156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-215.pth.tar', 73.97000003662109)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-217.pth.tar', 73.95800006591797)

Train: 231 [   0/1251 (  0%)]  Loss: 3.583 (3.58)  Time: 2.184s,  468.78/s  (2.184s,  468.78/s)  LR: 6.800e-04  Data: 1.413 (1.413)
Train: 231 [  50/1251 (  4%)]  Loss: 3.679 (3.63)  Time: 0.787s, 1300.60/s  (0.814s, 1257.42/s)  LR: 6.800e-04  Data: 0.010 (0.043)
Train: 231 [ 100/1251 (  8%)]  Loss: 3.445 (3.57)  Time: 0.772s, 1325.68/s  (0.799s, 1281.19/s)  LR: 6.800e-04  Data: 0.010 (0.027)
Train: 231 [ 150/1251 ( 12%)]  Loss: 3.713 (3.60)  Time: 0.773s, 1325.51/s  (0.795s, 1288.18/s)  LR: 6.800e-04  Data: 0.010 (0.021)
Train: 231 [ 200/1251 ( 16%)]  Loss: 3.512 (3.59)  Time: 0.775s, 1321.56/s  (0.791s, 1294.03/s)  LR: 6.800e-04  Data: 0.010 (0.019)
Train: 231 [ 250/1251 ( 20%)]  Loss: 3.554 (3.58)  Time: 0.776s, 1320.39/s  (0.790s, 1295.81/s)  LR: 6.800e-04  Data: 0.010 (0.017)
Train: 231 [ 300/1251 ( 24%)]  Loss: 3.758 (3.61)  Time: 0.787s, 1301.76/s  (0.789s, 1298.11/s)  LR: 6.800e-04  Data: 0.011 (0.016)
Train: 231 [ 350/1251 ( 28%)]  Loss: 3.466 (3.59)  Time: 0.775s, 1321.79/s  (0.787s, 1300.48/s)  LR: 6.800e-04  Data: 0.011 (0.015)
Train: 231 [ 400/1251 ( 32%)]  Loss: 3.549 (3.58)  Time: 0.774s, 1323.00/s  (0.786s, 1302.33/s)  LR: 6.800e-04  Data: 0.010 (0.015)
Train: 231 [ 450/1251 ( 36%)]  Loss: 3.516 (3.58)  Time: 0.775s, 1321.84/s  (0.787s, 1300.97/s)  LR: 6.800e-04  Data: 0.010 (0.014)
Train: 231 [ 500/1251 ( 40%)]  Loss: 3.836 (3.60)  Time: 0.815s, 1256.63/s  (0.787s, 1300.80/s)  LR: 6.800e-04  Data: 0.011 (0.014)
Train: 231 [ 550/1251 ( 44%)]  Loss: 3.519 (3.59)  Time: 0.815s, 1256.84/s  (0.788s, 1300.19/s)  LR: 6.800e-04  Data: 0.010 (0.013)
Train: 231 [ 600/1251 ( 48%)]  Loss: 3.682 (3.60)  Time: 0.789s, 1298.36/s  (0.787s, 1301.19/s)  LR: 6.800e-04  Data: 0.010 (0.013)
Train: 231 [ 650/1251 ( 52%)]  Loss: 3.735 (3.61)  Time: 0.780s, 1312.54/s  (0.786s, 1302.13/s)  LR: 6.800e-04  Data: 0.010 (0.013)
Train: 231 [ 700/1251 ( 56%)]  Loss: 3.622 (3.61)  Time: 0.773s, 1324.31/s  (0.786s, 1302.20/s)  LR: 6.800e-04  Data: 0.009 (0.013)
Train: 231 [ 750/1251 ( 60%)]  Loss: 3.702 (3.62)  Time: 0.778s, 1315.58/s  (0.786s, 1302.40/s)  LR: 6.800e-04  Data: 0.010 (0.013)
Train: 231 [ 800/1251 ( 64%)]  Loss: 3.742 (3.62)  Time: 0.827s, 1237.87/s  (0.786s, 1301.97/s)  LR: 6.800e-04  Data: 0.014 (0.012)
Train: 231 [ 850/1251 ( 68%)]  Loss: 3.247 (3.60)  Time: 0.785s, 1304.35/s  (0.787s, 1301.21/s)  LR: 6.800e-04  Data: 0.011 (0.012)
Train: 231 [ 900/1251 ( 72%)]  Loss: 3.561 (3.60)  Time: 0.816s, 1255.25/s  (0.788s, 1300.24/s)  LR: 6.800e-04  Data: 0.010 (0.012)
Train: 231 [ 950/1251 ( 76%)]  Loss: 3.804 (3.61)  Time: 0.814s, 1257.40/s  (0.788s, 1299.40/s)  LR: 6.800e-04  Data: 0.011 (0.012)
Train: 231 [1000/1251 ( 80%)]  Loss: 3.526 (3.61)  Time: 0.775s, 1322.10/s  (0.788s, 1299.52/s)  LR: 6.800e-04  Data: 0.010 (0.012)
Train: 231 [1050/1251 ( 84%)]  Loss: 3.776 (3.61)  Time: 0.772s, 1325.90/s  (0.788s, 1299.89/s)  LR: 6.800e-04  Data: 0.010 (0.012)
Train: 231 [1100/1251 ( 88%)]  Loss: 3.581 (3.61)  Time: 0.779s, 1314.69/s  (0.787s, 1300.59/s)  LR: 6.800e-04  Data: 0.009 (0.012)
Train: 231 [1150/1251 ( 92%)]  Loss: 3.799 (3.62)  Time: 0.787s, 1301.92/s  (0.787s, 1301.15/s)  LR: 6.800e-04  Data: 0.013 (0.012)
Train: 231 [1200/1251 ( 96%)]  Loss: 3.539 (3.62)  Time: 0.781s, 1310.91/s  (0.787s, 1301.58/s)  LR: 6.800e-04  Data: 0.012 (0.012)
Train: 231 [1250/1251 (100%)]  Loss: 3.789 (3.62)  Time: 0.794s, 1288.99/s  (0.787s, 1301.44/s)  LR: 6.800e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.526 (1.526)  Loss:  0.7505 (0.7505)  Acc@1: 88.6719 (88.6719)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.194 (0.579)  Loss:  0.8027 (1.3293)  Acc@1: 85.0236 (73.9120)  Acc@5: 96.8160 (92.1640)
Train: 232 [   0/1251 (  0%)]  Loss: 3.431 (3.43)  Time: 2.373s,  431.51/s  (2.373s,  431.51/s)  LR: 6.775e-04  Data: 1.598 (1.598)
Train: 232 [  50/1251 (  4%)]  Loss: 3.341 (3.39)  Time: 0.774s, 1322.46/s  (0.823s, 1243.64/s)  LR: 6.775e-04  Data: 0.009 (0.045)
Train: 232 [ 100/1251 (  8%)]  Loss: 3.308 (3.36)  Time: 0.772s, 1327.02/s  (0.802s, 1276.72/s)  LR: 6.775e-04  Data: 0.010 (0.027)
Train: 232 [ 150/1251 ( 12%)]  Loss: 3.374 (3.36)  Time: 0.772s, 1327.08/s  (0.794s, 1290.22/s)  LR: 6.775e-04  Data: 0.009 (0.022)
Train: 232 [ 200/1251 ( 16%)]  Loss: 3.280 (3.35)  Time: 0.783s, 1306.98/s  (0.792s, 1292.93/s)  LR: 6.775e-04  Data: 0.010 (0.019)
Train: 232 [ 250/1251 ( 20%)]  Loss: 3.434 (3.36)  Time: 0.772s, 1325.89/s  (0.792s, 1292.25/s)  LR: 6.775e-04  Data: 0.009 (0.017)
Train: 232 [ 300/1251 ( 24%)]  Loss: 3.849 (3.43)  Time: 0.774s, 1322.54/s  (0.790s, 1295.42/s)  LR: 6.775e-04  Data: 0.010 (0.016)
Train: 232 [ 350/1251 ( 28%)]  Loss: 3.249 (3.41)  Time: 0.787s, 1301.92/s  (0.789s, 1297.27/s)  LR: 6.775e-04  Data: 0.012 (0.015)
Train: 232 [ 400/1251 ( 32%)]  Loss: 3.350 (3.40)  Time: 0.771s, 1328.47/s  (0.789s, 1298.39/s)  LR: 6.775e-04  Data: 0.010 (0.014)
Train: 232 [ 450/1251 ( 36%)]  Loss: 3.577 (3.42)  Time: 0.775s, 1321.56/s  (0.789s, 1298.52/s)  LR: 6.775e-04  Data: 0.010 (0.014)
Train: 232 [ 500/1251 ( 40%)]  Loss: 3.655 (3.44)  Time: 0.801s, 1278.51/s  (0.788s, 1299.28/s)  LR: 6.775e-04  Data: 0.009 (0.014)
Train: 232 [ 550/1251 ( 44%)]  Loss: 3.632 (3.46)  Time: 0.772s, 1325.70/s  (0.787s, 1300.45/s)  LR: 6.775e-04  Data: 0.010 (0.013)
Train: 232 [ 600/1251 ( 48%)]  Loss: 3.397 (3.45)  Time: 0.774s, 1323.11/s  (0.788s, 1300.17/s)  LR: 6.775e-04  Data: 0.010 (0.013)
Train: 232 [ 650/1251 ( 52%)]  Loss: 3.733 (3.47)  Time: 0.774s, 1322.32/s  (0.787s, 1300.78/s)  LR: 6.775e-04  Data: 0.010 (0.013)
Train: 232 [ 700/1251 ( 56%)]  Loss: 3.711 (3.49)  Time: 0.782s, 1309.14/s  (0.787s, 1301.71/s)  LR: 6.775e-04  Data: 0.012 (0.013)
Train: 232 [ 750/1251 ( 60%)]  Loss: 3.473 (3.49)  Time: 0.782s, 1309.45/s  (0.787s, 1300.80/s)  LR: 6.775e-04  Data: 0.010 (0.012)
Train: 232 [ 800/1251 ( 64%)]  Loss: 3.217 (3.47)  Time: 0.770s, 1329.48/s  (0.787s, 1301.73/s)  LR: 6.775e-04  Data: 0.010 (0.012)
Train: 232 [ 850/1251 ( 68%)]  Loss: 3.413 (3.47)  Time: 0.772s, 1325.63/s  (0.786s, 1302.39/s)  LR: 6.775e-04  Data: 0.010 (0.012)
Train: 232 [ 900/1251 ( 72%)]  Loss: 3.588 (3.47)  Time: 0.774s, 1322.55/s  (0.786s, 1302.90/s)  LR: 6.775e-04  Data: 0.010 (0.012)
Train: 232 [ 950/1251 ( 76%)]  Loss: 3.340 (3.47)  Time: 0.772s, 1327.12/s  (0.786s, 1303.22/s)  LR: 6.775e-04  Data: 0.010 (0.012)
Train: 232 [1000/1251 ( 80%)]  Loss: 3.397 (3.46)  Time: 0.782s, 1309.16/s  (0.786s, 1303.09/s)  LR: 6.775e-04  Data: 0.009 (0.012)
Train: 232 [1050/1251 ( 84%)]  Loss: 3.409 (3.46)  Time: 0.788s, 1299.19/s  (0.786s, 1303.25/s)  LR: 6.775e-04  Data: 0.010 (0.012)
Train: 232 [1100/1251 ( 88%)]  Loss: 3.570 (3.47)  Time: 0.782s, 1309.66/s  (0.786s, 1303.47/s)  LR: 6.775e-04  Data: 0.009 (0.012)
Train: 232 [1150/1251 ( 92%)]  Loss: 3.841 (3.48)  Time: 0.819s, 1250.71/s  (0.786s, 1303.59/s)  LR: 6.775e-04  Data: 0.009 (0.012)
Train: 232 [1200/1251 ( 96%)]  Loss: 3.876 (3.50)  Time: 0.773s, 1324.67/s  (0.786s, 1302.92/s)  LR: 6.775e-04  Data: 0.010 (0.012)
Train: 232 [1250/1251 (100%)]  Loss: 3.521 (3.50)  Time: 0.762s, 1343.80/s  (0.786s, 1302.41/s)  LR: 6.775e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.559 (1.559)  Loss:  0.7910 (0.7910)  Acc@1: 88.7695 (88.7695)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.194 (0.562)  Loss:  0.9189 (1.3218)  Acc@1: 83.2547 (74.4460)  Acc@5: 96.4623 (92.4800)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-232.pth.tar', 74.44600009521484)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-226.pth.tar', 74.40799992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-219.pth.tar', 74.34000014160156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-228.pth.tar', 74.33799990722656)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-224.pth.tar', 74.1619999584961)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-230.pth.tar', 74.15999998291015)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-225.pth.tar', 74.1540000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-213.pth.tar', 74.13599998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-227.pth.tar', 74.12799998535156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-215.pth.tar', 73.97000003662109)

Train: 233 [   0/1251 (  0%)]  Loss: 3.424 (3.42)  Time: 2.273s,  450.46/s  (2.273s,  450.46/s)  LR: 6.751e-04  Data: 1.493 (1.493)
Train: 233 [  50/1251 (  4%)]  Loss: 3.562 (3.49)  Time: 0.832s, 1230.35/s  (0.817s, 1253.09/s)  LR: 6.751e-04  Data: 0.009 (0.044)
Train: 233 [ 100/1251 (  8%)]  Loss: 3.205 (3.40)  Time: 0.774s, 1322.99/s  (0.802s, 1277.17/s)  LR: 6.751e-04  Data: 0.011 (0.027)
Train: 233 [ 150/1251 ( 12%)]  Loss: 3.588 (3.44)  Time: 0.772s, 1325.75/s  (0.795s, 1288.36/s)  LR: 6.751e-04  Data: 0.010 (0.022)
Train: 233 [ 200/1251 ( 16%)]  Loss: 3.631 (3.48)  Time: 0.775s, 1320.63/s  (0.794s, 1289.22/s)  LR: 6.751e-04  Data: 0.011 (0.019)
Train: 233 [ 250/1251 ( 20%)]  Loss: 3.732 (3.52)  Time: 0.778s, 1315.54/s  (0.796s, 1286.27/s)  LR: 6.751e-04  Data: 0.010 (0.017)
Train: 233 [ 300/1251 ( 24%)]  Loss: 3.777 (3.56)  Time: 0.773s, 1324.35/s  (0.796s, 1286.19/s)  LR: 6.751e-04  Data: 0.010 (0.016)
Train: 233 [ 350/1251 ( 28%)]  Loss: 3.495 (3.55)  Time: 0.780s, 1313.51/s  (0.794s, 1289.05/s)  LR: 6.751e-04  Data: 0.010 (0.015)
Train: 233 [ 400/1251 ( 32%)]  Loss: 3.580 (3.55)  Time: 0.821s, 1247.68/s  (0.793s, 1291.07/s)  LR: 6.751e-04  Data: 0.010 (0.015)
Train: 233 [ 450/1251 ( 36%)]  Loss: 3.529 (3.55)  Time: 0.779s, 1314.24/s  (0.792s, 1293.42/s)  LR: 6.751e-04  Data: 0.010 (0.014)
Train: 233 [ 500/1251 ( 40%)]  Loss: 3.851 (3.58)  Time: 0.772s, 1326.20/s  (0.790s, 1295.41/s)  LR: 6.751e-04  Data: 0.010 (0.014)
Train: 233 [ 550/1251 ( 44%)]  Loss: 3.924 (3.61)  Time: 0.778s, 1315.36/s  (0.790s, 1296.06/s)  LR: 6.751e-04  Data: 0.010 (0.013)
Train: 233 [ 600/1251 ( 48%)]  Loss: 3.860 (3.63)  Time: 0.823s, 1244.80/s  (0.789s, 1297.14/s)  LR: 6.751e-04  Data: 0.010 (0.013)
Train: 233 [ 650/1251 ( 52%)]  Loss: 3.239 (3.60)  Time: 0.775s, 1321.71/s  (0.789s, 1297.74/s)  LR: 6.751e-04  Data: 0.011 (0.013)
Train: 233 [ 700/1251 ( 56%)]  Loss: 3.536 (3.60)  Time: 0.785s, 1304.45/s  (0.789s, 1298.62/s)  LR: 6.751e-04  Data: 0.009 (0.013)
Train: 233 [ 750/1251 ( 60%)]  Loss: 3.723 (3.60)  Time: 0.773s, 1325.49/s  (0.788s, 1299.71/s)  LR: 6.751e-04  Data: 0.010 (0.012)
Train: 233 [ 800/1251 ( 64%)]  Loss: 3.530 (3.60)  Time: 0.796s, 1286.52/s  (0.788s, 1298.88/s)  LR: 6.751e-04  Data: 0.012 (0.012)
Train: 233 [ 850/1251 ( 68%)]  Loss: 3.803 (3.61)  Time: 0.774s, 1322.26/s  (0.788s, 1299.62/s)  LR: 6.751e-04  Data: 0.009 (0.012)
Train: 233 [ 900/1251 ( 72%)]  Loss: 3.421 (3.60)  Time: 0.870s, 1177.32/s  (0.788s, 1299.99/s)  LR: 6.751e-04  Data: 0.010 (0.012)
Train: 233 [ 950/1251 ( 76%)]  Loss: 3.593 (3.60)  Time: 0.777s, 1317.92/s  (0.787s, 1300.55/s)  LR: 6.751e-04  Data: 0.010 (0.012)
Train: 233 [1000/1251 ( 80%)]  Loss: 3.694 (3.60)  Time: 0.782s, 1309.77/s  (0.787s, 1301.45/s)  LR: 6.751e-04  Data: 0.010 (0.012)
Train: 233 [1050/1251 ( 84%)]  Loss: 3.602 (3.60)  Time: 0.772s, 1326.21/s  (0.787s, 1301.54/s)  LR: 6.751e-04  Data: 0.009 (0.012)
Train: 233 [1100/1251 ( 88%)]  Loss: 3.477 (3.60)  Time: 0.781s, 1311.01/s  (0.787s, 1301.42/s)  LR: 6.751e-04  Data: 0.011 (0.012)
Train: 233 [1150/1251 ( 92%)]  Loss: 3.329 (3.59)  Time: 0.777s, 1318.51/s  (0.787s, 1301.87/s)  LR: 6.751e-04  Data: 0.009 (0.012)
Train: 233 [1200/1251 ( 96%)]  Loss: 3.978 (3.60)  Time: 0.781s, 1310.65/s  (0.787s, 1301.41/s)  LR: 6.751e-04  Data: 0.011 (0.012)
Train: 233 [1250/1251 (100%)]  Loss: 3.537 (3.60)  Time: 0.761s, 1345.80/s  (0.787s, 1301.81/s)  LR: 6.751e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.593 (1.593)  Loss:  0.8149 (0.8149)  Acc@1: 89.0625 (89.0625)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.194 (0.562)  Loss:  0.9087 (1.3153)  Acc@1: 83.8443 (74.3220)  Acc@5: 96.5802 (92.4500)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-232.pth.tar', 74.44600009521484)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-226.pth.tar', 74.40799992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-219.pth.tar', 74.34000014160156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-228.pth.tar', 74.33799990722656)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-233.pth.tar', 74.32200009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-224.pth.tar', 74.1619999584961)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-230.pth.tar', 74.15999998291015)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-225.pth.tar', 74.1540000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-213.pth.tar', 74.13599998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-227.pth.tar', 74.12799998535156)

Train: 234 [   0/1251 (  0%)]  Loss: 3.929 (3.93)  Time: 2.312s,  442.96/s  (2.312s,  442.96/s)  LR: 6.727e-04  Data: 1.587 (1.587)
Train: 234 [  50/1251 (  4%)]  Loss: 3.342 (3.64)  Time: 0.772s, 1325.81/s  (0.818s, 1251.84/s)  LR: 6.727e-04  Data: 0.009 (0.047)
Train: 234 [ 100/1251 (  8%)]  Loss: 3.550 (3.61)  Time: 0.774s, 1322.30/s  (0.798s, 1283.42/s)  LR: 6.727e-04  Data: 0.010 (0.029)
Train: 234 [ 150/1251 ( 12%)]  Loss: 3.921 (3.69)  Time: 0.774s, 1323.14/s  (0.792s, 1292.36/s)  LR: 6.727e-04  Data: 0.010 (0.022)
Train: 234 [ 200/1251 ( 16%)]  Loss: 3.894 (3.73)  Time: 0.777s, 1318.64/s  (0.791s, 1293.77/s)  LR: 6.727e-04  Data: 0.010 (0.019)
Train: 234 [ 250/1251 ( 20%)]  Loss: 3.125 (3.63)  Time: 0.775s, 1320.64/s  (0.789s, 1298.48/s)  LR: 6.727e-04  Data: 0.010 (0.017)
Train: 234 [ 300/1251 ( 24%)]  Loss: 3.509 (3.61)  Time: 0.806s, 1270.64/s  (0.789s, 1297.13/s)  LR: 6.727e-04  Data: 0.009 (0.016)
Train: 234 [ 350/1251 ( 28%)]  Loss: 3.782 (3.63)  Time: 0.793s, 1291.02/s  (0.789s, 1298.19/s)  LR: 6.727e-04  Data: 0.010 (0.015)
Train: 234 [ 400/1251 ( 32%)]  Loss: 3.649 (3.63)  Time: 0.783s, 1308.45/s  (0.788s, 1299.49/s)  LR: 6.727e-04  Data: 0.010 (0.015)
Train: 234 [ 450/1251 ( 36%)]  Loss: 3.486 (3.62)  Time: 0.771s, 1328.78/s  (0.788s, 1300.17/s)  LR: 6.727e-04  Data: 0.010 (0.014)
Train: 234 [ 500/1251 ( 40%)]  Loss: 3.250 (3.59)  Time: 0.786s, 1302.11/s  (0.787s, 1301.93/s)  LR: 6.727e-04  Data: 0.010 (0.014)
Train: 234 [ 550/1251 ( 44%)]  Loss: 3.088 (3.54)  Time: 0.771s, 1328.04/s  (0.786s, 1302.66/s)  LR: 6.727e-04  Data: 0.009 (0.013)
Train: 234 [ 600/1251 ( 48%)]  Loss: 3.478 (3.54)  Time: 0.774s, 1323.18/s  (0.785s, 1303.92/s)  LR: 6.727e-04  Data: 0.010 (0.013)
Train: 234 [ 650/1251 ( 52%)]  Loss: 3.585 (3.54)  Time: 0.827s, 1237.63/s  (0.786s, 1302.50/s)  LR: 6.727e-04  Data: 0.010 (0.013)
Train: 234 [ 700/1251 ( 56%)]  Loss: 3.408 (3.53)  Time: 0.774s, 1323.36/s  (0.786s, 1302.96/s)  LR: 6.727e-04  Data: 0.010 (0.013)
Train: 234 [ 750/1251 ( 60%)]  Loss: 3.501 (3.53)  Time: 0.771s, 1327.28/s  (0.786s, 1303.53/s)  LR: 6.727e-04  Data: 0.009 (0.013)
Train: 234 [ 800/1251 ( 64%)]  Loss: 3.542 (3.53)  Time: 0.774s, 1323.06/s  (0.785s, 1304.64/s)  LR: 6.727e-04  Data: 0.009 (0.012)
Train: 234 [ 850/1251 ( 68%)]  Loss: 3.748 (3.54)  Time: 0.791s, 1294.93/s  (0.785s, 1305.22/s)  LR: 6.727e-04  Data: 0.014 (0.012)
Train: 234 [ 900/1251 ( 72%)]  Loss: 3.771 (3.56)  Time: 0.779s, 1313.94/s  (0.784s, 1306.05/s)  LR: 6.727e-04  Data: 0.010 (0.012)
Train: 234 [ 950/1251 ( 76%)]  Loss: 3.298 (3.54)  Time: 0.774s, 1322.58/s  (0.784s, 1306.48/s)  LR: 6.727e-04  Data: 0.010 (0.012)
Train: 234 [1000/1251 ( 80%)]  Loss: 3.565 (3.54)  Time: 0.772s, 1325.73/s  (0.784s, 1306.78/s)  LR: 6.727e-04  Data: 0.010 (0.012)
Train: 234 [1050/1251 ( 84%)]  Loss: 3.386 (3.54)  Time: 0.790s, 1295.45/s  (0.784s, 1305.97/s)  LR: 6.727e-04  Data: 0.009 (0.012)
Train: 234 [1100/1251 ( 88%)]  Loss: 3.595 (3.54)  Time: 0.787s, 1301.39/s  (0.784s, 1305.59/s)  LR: 6.727e-04  Data: 0.009 (0.012)
Train: 234 [1150/1251 ( 92%)]  Loss: 3.337 (3.53)  Time: 0.773s, 1324.29/s  (0.784s, 1306.09/s)  LR: 6.727e-04  Data: 0.010 (0.012)
Train: 234 [1200/1251 ( 96%)]  Loss: 3.296 (3.52)  Time: 0.777s, 1318.09/s  (0.784s, 1305.36/s)  LR: 6.727e-04  Data: 0.010 (0.012)
Train: 234 [1250/1251 (100%)]  Loss: 3.493 (3.52)  Time: 0.757s, 1353.41/s  (0.784s, 1305.72/s)  LR: 6.727e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.548 (1.548)  Loss:  0.8345 (0.8345)  Acc@1: 88.9648 (88.9648)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.194 (0.559)  Loss:  0.9326 (1.4207)  Acc@1: 85.0236 (73.7100)  Acc@5: 96.2264 (91.9600)
Train: 235 [   0/1251 (  0%)]  Loss: 3.526 (3.53)  Time: 2.142s,  478.05/s  (2.142s,  478.05/s)  LR: 6.702e-04  Data: 1.427 (1.427)
Train: 235 [  50/1251 (  4%)]  Loss: 3.890 (3.71)  Time: 0.772s, 1325.84/s  (0.821s, 1247.81/s)  LR: 6.702e-04  Data: 0.010 (0.042)
Train: 235 [ 100/1251 (  8%)]  Loss: 3.404 (3.61)  Time: 0.770s, 1329.10/s  (0.802s, 1277.47/s)  LR: 6.702e-04  Data: 0.009 (0.026)
Train: 235 [ 150/1251 ( 12%)]  Loss: 3.625 (3.61)  Time: 0.826s, 1240.27/s  (0.798s, 1283.11/s)  LR: 6.702e-04  Data: 0.010 (0.021)
Train: 235 [ 200/1251 ( 16%)]  Loss: 3.487 (3.59)  Time: 0.785s, 1303.74/s  (0.796s, 1285.64/s)  LR: 6.702e-04  Data: 0.013 (0.018)
Train: 235 [ 250/1251 ( 20%)]  Loss: 3.432 (3.56)  Time: 0.820s, 1249.37/s  (0.794s, 1290.12/s)  LR: 6.702e-04  Data: 0.012 (0.017)
Train: 235 [ 300/1251 ( 24%)]  Loss: 3.535 (3.56)  Time: 0.772s, 1326.39/s  (0.792s, 1293.18/s)  LR: 6.702e-04  Data: 0.010 (0.016)
Train: 235 [ 350/1251 ( 28%)]  Loss: 3.636 (3.57)  Time: 0.776s, 1319.21/s  (0.791s, 1294.93/s)  LR: 6.702e-04  Data: 0.010 (0.015)
Train: 235 [ 400/1251 ( 32%)]  Loss: 3.662 (3.58)  Time: 0.773s, 1324.70/s  (0.789s, 1297.77/s)  LR: 6.702e-04  Data: 0.010 (0.014)
Train: 235 [ 450/1251 ( 36%)]  Loss: 3.779 (3.60)  Time: 0.777s, 1317.06/s  (0.788s, 1299.07/s)  LR: 6.702e-04  Data: 0.009 (0.014)
Train: 235 [ 500/1251 ( 40%)]  Loss: 3.745 (3.61)  Time: 0.777s, 1318.48/s  (0.788s, 1300.27/s)  LR: 6.702e-04  Data: 0.009 (0.013)
Train: 235 [ 550/1251 ( 44%)]  Loss: 3.855 (3.63)  Time: 0.789s, 1298.37/s  (0.787s, 1300.96/s)  LR: 6.702e-04  Data: 0.014 (0.013)
Train: 235 [ 600/1251 ( 48%)]  Loss: 3.244 (3.60)  Time: 0.771s, 1328.42/s  (0.786s, 1302.29/s)  LR: 6.702e-04  Data: 0.010 (0.013)
Train: 235 [ 650/1251 ( 52%)]  Loss: 3.528 (3.60)  Time: 0.828s, 1236.77/s  (0.788s, 1299.48/s)  LR: 6.702e-04  Data: 0.014 (0.013)
Train: 235 [ 700/1251 ( 56%)]  Loss: 3.842 (3.61)  Time: 0.796s, 1286.62/s  (0.788s, 1300.21/s)  LR: 6.702e-04  Data: 0.010 (0.012)
Train: 235 [ 750/1251 ( 60%)]  Loss: 3.609 (3.61)  Time: 0.807s, 1268.19/s  (0.787s, 1300.97/s)  LR: 6.702e-04  Data: 0.010 (0.012)
Train: 235 [ 800/1251 ( 64%)]  Loss: 3.508 (3.61)  Time: 0.772s, 1326.96/s  (0.787s, 1301.16/s)  LR: 6.702e-04  Data: 0.010 (0.012)
Train: 235 [ 850/1251 ( 68%)]  Loss: 3.565 (3.60)  Time: 0.781s, 1310.99/s  (0.786s, 1302.03/s)  LR: 6.702e-04  Data: 0.012 (0.012)
Train: 235 [ 900/1251 ( 72%)]  Loss: 3.763 (3.61)  Time: 0.772s, 1326.65/s  (0.787s, 1301.81/s)  LR: 6.702e-04  Data: 0.010 (0.012)
Train: 235 [ 950/1251 ( 76%)]  Loss: 3.034 (3.58)  Time: 0.835s, 1225.70/s  (0.787s, 1300.83/s)  LR: 6.702e-04  Data: 0.010 (0.012)
Train: 235 [1000/1251 ( 80%)]  Loss: 3.397 (3.57)  Time: 0.774s, 1322.65/s  (0.788s, 1300.22/s)  LR: 6.702e-04  Data: 0.010 (0.012)
Train: 235 [1050/1251 ( 84%)]  Loss: 3.254 (3.56)  Time: 0.848s, 1208.23/s  (0.787s, 1300.40/s)  LR: 6.702e-04  Data: 0.009 (0.012)
Train: 235 [1100/1251 ( 88%)]  Loss: 3.135 (3.54)  Time: 0.774s, 1322.65/s  (0.787s, 1301.00/s)  LR: 6.702e-04  Data: 0.010 (0.012)
Train: 235 [1150/1251 ( 92%)]  Loss: 3.654 (3.55)  Time: 0.772s, 1325.92/s  (0.787s, 1301.70/s)  LR: 6.702e-04  Data: 0.009 (0.011)
Train: 235 [1200/1251 ( 96%)]  Loss: 3.333 (3.54)  Time: 0.773s, 1324.54/s  (0.786s, 1302.32/s)  LR: 6.702e-04  Data: 0.009 (0.011)
Train: 235 [1250/1251 (100%)]  Loss: 3.174 (3.52)  Time: 0.759s, 1349.08/s  (0.786s, 1302.65/s)  LR: 6.702e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.514 (1.514)  Loss:  0.7036 (0.7036)  Acc@1: 88.5742 (88.5742)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.194 (0.573)  Loss:  0.8325 (1.3007)  Acc@1: 84.7877 (73.8220)  Acc@5: 96.9340 (92.1720)
Train: 236 [   0/1251 (  0%)]  Loss: 3.850 (3.85)  Time: 2.173s,  471.21/s  (2.173s,  471.21/s)  LR: 6.678e-04  Data: 1.445 (1.445)
Train: 236 [  50/1251 (  4%)]  Loss: 3.226 (3.54)  Time: 0.839s, 1220.23/s  (0.825s, 1240.68/s)  LR: 6.678e-04  Data: 0.009 (0.043)
Train: 236 [ 100/1251 (  8%)]  Loss: 3.531 (3.54)  Time: 0.815s, 1256.90/s  (0.807s, 1269.12/s)  LR: 6.678e-04  Data: 0.010 (0.027)
Train: 236 [ 150/1251 ( 12%)]  Loss: 3.095 (3.43)  Time: 0.772s, 1327.04/s  (0.800s, 1280.72/s)  LR: 6.678e-04  Data: 0.010 (0.021)
Train: 236 [ 200/1251 ( 16%)]  Loss: 3.460 (3.43)  Time: 0.790s, 1295.46/s  (0.794s, 1289.08/s)  LR: 6.678e-04  Data: 0.010 (0.018)
Train: 236 [ 250/1251 ( 20%)]  Loss: 2.871 (3.34)  Time: 0.774s, 1323.81/s  (0.794s, 1289.22/s)  LR: 6.678e-04  Data: 0.010 (0.017)
Train: 236 [ 300/1251 ( 24%)]  Loss: 3.292 (3.33)  Time: 0.779s, 1313.67/s  (0.792s, 1293.10/s)  LR: 6.678e-04  Data: 0.010 (0.015)
Train: 236 [ 350/1251 ( 28%)]  Loss: 3.592 (3.36)  Time: 0.771s, 1327.83/s  (0.790s, 1296.31/s)  LR: 6.678e-04  Data: 0.009 (0.015)
Train: 236 [ 400/1251 ( 32%)]  Loss: 3.238 (3.35)  Time: 0.811s, 1261.90/s  (0.789s, 1298.20/s)  LR: 6.678e-04  Data: 0.010 (0.014)
Train: 236 [ 450/1251 ( 36%)]  Loss: 3.605 (3.38)  Time: 0.779s, 1314.49/s  (0.788s, 1299.76/s)  LR: 6.678e-04  Data: 0.010 (0.014)
Train: 236 [ 500/1251 ( 40%)]  Loss: 3.634 (3.40)  Time: 0.805s, 1272.34/s  (0.787s, 1300.61/s)  LR: 6.678e-04  Data: 0.010 (0.013)
Train: 236 [ 550/1251 ( 44%)]  Loss: 3.516 (3.41)  Time: 0.774s, 1323.58/s  (0.787s, 1301.20/s)  LR: 6.678e-04  Data: 0.013 (0.013)
Train: 236 [ 600/1251 ( 48%)]  Loss: 3.713 (3.43)  Time: 0.775s, 1321.00/s  (0.787s, 1300.88/s)  LR: 6.678e-04  Data: 0.009 (0.013)
Train: 236 [ 650/1251 ( 52%)]  Loss: 3.806 (3.46)  Time: 0.780s, 1312.29/s  (0.787s, 1301.69/s)  LR: 6.678e-04  Data: 0.011 (0.013)
Train: 236 [ 700/1251 ( 56%)]  Loss: 3.669 (3.47)  Time: 0.777s, 1317.61/s  (0.786s, 1302.33/s)  LR: 6.678e-04  Data: 0.010 (0.013)
Train: 236 [ 750/1251 ( 60%)]  Loss: 3.421 (3.47)  Time: 0.775s, 1321.58/s  (0.786s, 1301.99/s)  LR: 6.678e-04  Data: 0.009 (0.012)
Train: 236 [ 800/1251 ( 64%)]  Loss: 3.351 (3.46)  Time: 0.784s, 1305.42/s  (0.786s, 1302.57/s)  LR: 6.678e-04  Data: 0.010 (0.012)
Train: 236 [ 850/1251 ( 68%)]  Loss: 3.712 (3.48)  Time: 0.773s, 1324.82/s  (0.786s, 1302.78/s)  LR: 6.678e-04  Data: 0.010 (0.012)
Train: 236 [ 900/1251 ( 72%)]  Loss: 3.903 (3.50)  Time: 0.773s, 1324.09/s  (0.786s, 1303.27/s)  LR: 6.678e-04  Data: 0.010 (0.012)
Train: 236 [ 950/1251 ( 76%)]  Loss: 3.775 (3.51)  Time: 0.779s, 1315.34/s  (0.785s, 1303.81/s)  LR: 6.678e-04  Data: 0.010 (0.012)
Train: 236 [1000/1251 ( 80%)]  Loss: 3.394 (3.51)  Time: 0.790s, 1295.46/s  (0.785s, 1304.34/s)  LR: 6.678e-04  Data: 0.010 (0.012)
Train: 236 [1050/1251 ( 84%)]  Loss: 3.477 (3.51)  Time: 0.771s, 1327.37/s  (0.785s, 1305.02/s)  LR: 6.678e-04  Data: 0.010 (0.012)
Train: 236 [1100/1251 ( 88%)]  Loss: 3.731 (3.52)  Time: 0.772s, 1325.73/s  (0.784s, 1305.44/s)  LR: 6.678e-04  Data: 0.010 (0.012)
Train: 236 [1150/1251 ( 92%)]  Loss: 3.656 (3.52)  Time: 0.776s, 1319.56/s  (0.784s, 1305.67/s)  LR: 6.678e-04  Data: 0.010 (0.012)
Train: 236 [1200/1251 ( 96%)]  Loss: 3.570 (3.52)  Time: 0.773s, 1325.08/s  (0.784s, 1305.92/s)  LR: 6.678e-04  Data: 0.010 (0.012)
Train: 236 [1250/1251 (100%)]  Loss: 3.542 (3.52)  Time: 0.800s, 1280.06/s  (0.784s, 1305.31/s)  LR: 6.678e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.551 (1.551)  Loss:  0.7437 (0.7437)  Acc@1: 88.7695 (88.7695)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.194 (0.561)  Loss:  1.0078 (1.3501)  Acc@1: 82.1934 (74.1020)  Acc@5: 95.8727 (92.2600)
Train: 237 [   0/1251 (  0%)]  Loss: 3.649 (3.65)  Time: 2.188s,  467.93/s  (2.188s,  467.93/s)  LR: 6.653e-04  Data: 1.461 (1.461)
Train: 237 [  50/1251 (  4%)]  Loss: 3.396 (3.52)  Time: 0.771s, 1327.62/s  (0.811s, 1263.34/s)  LR: 6.653e-04  Data: 0.009 (0.041)
Train: 237 [ 100/1251 (  8%)]  Loss: 3.226 (3.42)  Time: 0.774s, 1323.04/s  (0.795s, 1288.14/s)  LR: 6.653e-04  Data: 0.009 (0.025)
Train: 237 [ 150/1251 ( 12%)]  Loss: 3.901 (3.54)  Time: 0.774s, 1323.19/s  (0.789s, 1297.48/s)  LR: 6.653e-04  Data: 0.009 (0.020)
Train: 237 [ 200/1251 ( 16%)]  Loss: 3.865 (3.61)  Time: 0.776s, 1319.16/s  (0.787s, 1300.89/s)  LR: 6.653e-04  Data: 0.010 (0.018)
Train: 237 [ 250/1251 ( 20%)]  Loss: 3.438 (3.58)  Time: 0.774s, 1322.99/s  (0.787s, 1301.63/s)  LR: 6.653e-04  Data: 0.010 (0.016)
Train: 237 [ 300/1251 ( 24%)]  Loss: 3.531 (3.57)  Time: 0.779s, 1314.25/s  (0.786s, 1303.00/s)  LR: 6.653e-04  Data: 0.009 (0.015)
Train: 237 [ 350/1251 ( 28%)]  Loss: 3.452 (3.56)  Time: 0.773s, 1324.00/s  (0.785s, 1304.72/s)  LR: 6.653e-04  Data: 0.010 (0.014)
Train: 237 [ 400/1251 ( 32%)]  Loss: 3.661 (3.57)  Time: 0.776s, 1319.98/s  (0.784s, 1306.06/s)  LR: 6.653e-04  Data: 0.010 (0.014)
Train: 237 [ 450/1251 ( 36%)]  Loss: 3.290 (3.54)  Time: 0.773s, 1324.27/s  (0.785s, 1304.89/s)  LR: 6.653e-04  Data: 0.010 (0.013)
Train: 237 [ 500/1251 ( 40%)]  Loss: 3.662 (3.55)  Time: 0.787s, 1301.10/s  (0.786s, 1303.00/s)  LR: 6.653e-04  Data: 0.013 (0.013)
Train: 237 [ 550/1251 ( 44%)]  Loss: 3.637 (3.56)  Time: 0.773s, 1324.41/s  (0.786s, 1303.15/s)  LR: 6.653e-04  Data: 0.011 (0.013)
Train: 237 [ 600/1251 ( 48%)]  Loss: 3.250 (3.54)  Time: 0.774s, 1323.44/s  (0.785s, 1303.92/s)  LR: 6.653e-04  Data: 0.009 (0.013)
Train: 237 [ 650/1251 ( 52%)]  Loss: 3.562 (3.54)  Time: 0.774s, 1323.68/s  (0.785s, 1304.59/s)  LR: 6.653e-04  Data: 0.009 (0.012)
Train: 237 [ 700/1251 ( 56%)]  Loss: 3.580 (3.54)  Time: 0.785s, 1303.98/s  (0.785s, 1305.25/s)  LR: 6.653e-04  Data: 0.010 (0.012)
Train: 237 [ 750/1251 ( 60%)]  Loss: 3.700 (3.55)  Time: 0.774s, 1322.35/s  (0.784s, 1305.53/s)  LR: 6.653e-04  Data: 0.010 (0.012)
Train: 237 [ 800/1251 ( 64%)]  Loss: 3.482 (3.55)  Time: 0.772s, 1326.00/s  (0.784s, 1305.62/s)  LR: 6.653e-04  Data: 0.009 (0.012)
Train: 237 [ 850/1251 ( 68%)]  Loss: 3.693 (3.55)  Time: 0.815s, 1257.09/s  (0.785s, 1304.15/s)  LR: 6.653e-04  Data: 0.011 (0.012)
Train: 237 [ 900/1251 ( 72%)]  Loss: 3.845 (3.57)  Time: 0.865s, 1184.23/s  (0.785s, 1303.79/s)  LR: 6.653e-04  Data: 0.010 (0.012)
Train: 237 [ 950/1251 ( 76%)]  Loss: 3.748 (3.58)  Time: 0.832s, 1230.17/s  (0.786s, 1303.55/s)  LR: 6.653e-04  Data: 0.010 (0.012)
Train: 237 [1000/1251 ( 80%)]  Loss: 3.259 (3.56)  Time: 0.837s, 1223.71/s  (0.785s, 1303.69/s)  LR: 6.653e-04  Data: 0.009 (0.012)
Train: 237 [1050/1251 ( 84%)]  Loss: 3.615 (3.57)  Time: 0.774s, 1323.64/s  (0.785s, 1303.92/s)  LR: 6.653e-04  Data: 0.010 (0.012)
Train: 237 [1100/1251 ( 88%)]  Loss: 3.477 (3.56)  Time: 0.774s, 1322.21/s  (0.785s, 1303.93/s)  LR: 6.653e-04  Data: 0.011 (0.012)
Train: 237 [1150/1251 ( 92%)]  Loss: 3.490 (3.56)  Time: 0.777s, 1317.79/s  (0.786s, 1303.27/s)  LR: 6.653e-04  Data: 0.010 (0.012)
Train: 237 [1200/1251 ( 96%)]  Loss: 3.719 (3.57)  Time: 0.882s, 1160.78/s  (0.786s, 1303.29/s)  LR: 6.653e-04  Data: 0.010 (0.011)
Train: 237 [1250/1251 (100%)]  Loss: 3.693 (3.57)  Time: 0.768s, 1333.90/s  (0.785s, 1303.94/s)  LR: 6.653e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.555 (1.555)  Loss:  0.6880 (0.6880)  Acc@1: 89.5508 (89.5508)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.194 (0.563)  Loss:  0.9019 (1.2794)  Acc@1: 84.3160 (74.3020)  Acc@5: 95.5189 (92.5160)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-232.pth.tar', 74.44600009521484)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-226.pth.tar', 74.40799992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-219.pth.tar', 74.34000014160156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-228.pth.tar', 74.33799990722656)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-233.pth.tar', 74.32200009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-237.pth.tar', 74.3020000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-224.pth.tar', 74.1619999584961)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-230.pth.tar', 74.15999998291015)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-225.pth.tar', 74.1540000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-213.pth.tar', 74.13599998046875)

Train: 238 [   0/1251 (  0%)]  Loss: 3.548 (3.55)  Time: 2.396s,  427.29/s  (2.396s,  427.29/s)  LR: 6.629e-04  Data: 1.670 (1.670)
Train: 238 [  50/1251 (  4%)]  Loss: 3.699 (3.62)  Time: 0.773s, 1324.96/s  (0.816s, 1254.44/s)  LR: 6.629e-04  Data: 0.010 (0.048)
Train: 238 [ 100/1251 (  8%)]  Loss: 3.598 (3.62)  Time: 0.779s, 1314.31/s  (0.802s, 1276.68/s)  LR: 6.629e-04  Data: 0.013 (0.030)
Train: 238 [ 150/1251 ( 12%)]  Loss: 3.632 (3.62)  Time: 0.773s, 1325.43/s  (0.794s, 1289.79/s)  LR: 6.629e-04  Data: 0.009 (0.023)
Train: 238 [ 200/1251 ( 16%)]  Loss: 3.529 (3.60)  Time: 0.777s, 1318.04/s  (0.790s, 1296.01/s)  LR: 6.629e-04  Data: 0.010 (0.020)
Train: 238 [ 250/1251 ( 20%)]  Loss: 3.625 (3.61)  Time: 0.778s, 1315.77/s  (0.788s, 1299.77/s)  LR: 6.629e-04  Data: 0.009 (0.018)
Train: 238 [ 300/1251 ( 24%)]  Loss: 3.675 (3.62)  Time: 0.772s, 1326.39/s  (0.788s, 1300.04/s)  LR: 6.629e-04  Data: 0.009 (0.017)
Train: 238 [ 350/1251 ( 28%)]  Loss: 3.715 (3.63)  Time: 0.775s, 1320.92/s  (0.788s, 1299.95/s)  LR: 6.629e-04  Data: 0.009 (0.016)
Train: 238 [ 400/1251 ( 32%)]  Loss: 3.204 (3.58)  Time: 0.829s, 1235.91/s  (0.789s, 1297.96/s)  LR: 6.629e-04  Data: 0.014 (0.015)
Train: 238 [ 450/1251 ( 36%)]  Loss: 3.766 (3.60)  Time: 0.774s, 1323.46/s  (0.790s, 1296.88/s)  LR: 6.629e-04  Data: 0.010 (0.014)
Train: 238 [ 500/1251 ( 40%)]  Loss: 3.584 (3.60)  Time: 0.780s, 1312.65/s  (0.789s, 1298.55/s)  LR: 6.629e-04  Data: 0.009 (0.014)
Train: 238 [ 550/1251 ( 44%)]  Loss: 3.459 (3.59)  Time: 0.771s, 1327.92/s  (0.788s, 1299.63/s)  LR: 6.629e-04  Data: 0.010 (0.014)
Train: 238 [ 600/1251 ( 48%)]  Loss: 3.400 (3.57)  Time: 0.827s, 1238.08/s  (0.788s, 1299.13/s)  LR: 6.629e-04  Data: 0.012 (0.013)
Train: 238 [ 650/1251 ( 52%)]  Loss: 3.704 (3.58)  Time: 0.827s, 1238.09/s  (0.790s, 1295.97/s)  LR: 6.629e-04  Data: 0.010 (0.013)
Train: 238 [ 700/1251 ( 56%)]  Loss: 3.166 (3.55)  Time: 0.772s, 1327.12/s  (0.790s, 1296.61/s)  LR: 6.629e-04  Data: 0.010 (0.013)
Train: 238 [ 750/1251 ( 60%)]  Loss: 3.528 (3.55)  Time: 0.773s, 1325.10/s  (0.789s, 1297.42/s)  LR: 6.629e-04  Data: 0.010 (0.013)
Train: 238 [ 800/1251 ( 64%)]  Loss: 3.178 (3.53)  Time: 0.773s, 1324.17/s  (0.789s, 1297.86/s)  LR: 6.629e-04  Data: 0.011 (0.013)
Train: 238 [ 850/1251 ( 68%)]  Loss: 3.600 (3.53)  Time: 0.772s, 1326.75/s  (0.789s, 1297.66/s)  LR: 6.629e-04  Data: 0.010 (0.012)
Train: 238 [ 900/1251 ( 72%)]  Loss: 3.352 (3.52)  Time: 0.777s, 1318.31/s  (0.789s, 1297.83/s)  LR: 6.629e-04  Data: 0.010 (0.012)
Train: 238 [ 950/1251 ( 76%)]  Loss: 3.679 (3.53)  Time: 0.773s, 1324.93/s  (0.789s, 1298.22/s)  LR: 6.629e-04  Data: 0.010 (0.012)
Train: 238 [1000/1251 ( 80%)]  Loss: 3.586 (3.53)  Time: 0.772s, 1326.50/s  (0.789s, 1298.64/s)  LR: 6.629e-04  Data: 0.010 (0.012)
Train: 238 [1050/1251 ( 84%)]  Loss: 3.345 (3.53)  Time: 0.812s, 1261.61/s  (0.788s, 1298.80/s)  LR: 6.629e-04  Data: 0.010 (0.012)
Train: 238 [1100/1251 ( 88%)]  Loss: 3.628 (3.53)  Time: 0.777s, 1317.63/s  (0.789s, 1298.06/s)  LR: 6.629e-04  Data: 0.011 (0.012)
Train: 238 [1150/1251 ( 92%)]  Loss: 3.749 (3.54)  Time: 0.786s, 1303.52/s  (0.788s, 1298.70/s)  LR: 6.629e-04  Data: 0.010 (0.012)
Train: 238 [1200/1251 ( 96%)]  Loss: 3.497 (3.54)  Time: 0.774s, 1323.43/s  (0.788s, 1299.06/s)  LR: 6.629e-04  Data: 0.009 (0.012)
Train: 238 [1250/1251 (100%)]  Loss: 3.611 (3.54)  Time: 0.766s, 1336.74/s  (0.788s, 1299.47/s)  LR: 6.629e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.540 (1.540)  Loss:  0.8271 (0.8271)  Acc@1: 89.7461 (89.7461)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.194 (0.563)  Loss:  0.9741 (1.3835)  Acc@1: 84.7877 (73.8480)  Acc@5: 96.5802 (92.0320)
Train: 239 [   0/1251 (  0%)]  Loss: 3.412 (3.41)  Time: 2.231s,  459.05/s  (2.231s,  459.05/s)  LR: 6.604e-04  Data: 1.496 (1.496)
Train: 239 [  50/1251 (  4%)]  Loss: 3.415 (3.41)  Time: 0.846s, 1210.09/s  (0.824s, 1242.38/s)  LR: 6.604e-04  Data: 0.010 (0.043)
Train: 239 [ 100/1251 (  8%)]  Loss: 3.659 (3.50)  Time: 0.772s, 1325.92/s  (0.803s, 1275.07/s)  LR: 6.604e-04  Data: 0.010 (0.026)
Train: 239 [ 150/1251 ( 12%)]  Loss: 3.327 (3.45)  Time: 0.777s, 1317.97/s  (0.796s, 1286.40/s)  LR: 6.604e-04  Data: 0.011 (0.021)
Train: 239 [ 200/1251 ( 16%)]  Loss: 3.402 (3.44)  Time: 0.773s, 1325.24/s  (0.792s, 1292.24/s)  LR: 6.604e-04  Data: 0.011 (0.018)
Train: 239 [ 250/1251 ( 20%)]  Loss: 3.598 (3.47)  Time: 0.776s, 1320.08/s  (0.790s, 1295.87/s)  LR: 6.604e-04  Data: 0.009 (0.017)
Train: 239 [ 300/1251 ( 24%)]  Loss: 3.432 (3.46)  Time: 0.774s, 1323.60/s  (0.790s, 1295.84/s)  LR: 6.604e-04  Data: 0.009 (0.015)
Train: 239 [ 350/1251 ( 28%)]  Loss: 3.578 (3.48)  Time: 0.791s, 1294.71/s  (0.790s, 1296.64/s)  LR: 6.604e-04  Data: 0.009 (0.015)
Train: 239 [ 400/1251 ( 32%)]  Loss: 3.359 (3.46)  Time: 0.814s, 1258.50/s  (0.789s, 1297.65/s)  LR: 6.604e-04  Data: 0.010 (0.014)
Train: 239 [ 450/1251 ( 36%)]  Loss: 3.525 (3.47)  Time: 0.814s, 1258.58/s  (0.788s, 1299.08/s)  LR: 6.604e-04  Data: 0.009 (0.014)
Train: 239 [ 500/1251 ( 40%)]  Loss: 3.661 (3.49)  Time: 0.774s, 1322.28/s  (0.788s, 1299.46/s)  LR: 6.604e-04  Data: 0.010 (0.013)
Train: 239 [ 550/1251 ( 44%)]  Loss: 3.966 (3.53)  Time: 0.811s, 1262.17/s  (0.788s, 1299.99/s)  LR: 6.604e-04  Data: 0.010 (0.013)
Train: 239 [ 600/1251 ( 48%)]  Loss: 3.704 (3.54)  Time: 0.773s, 1325.16/s  (0.787s, 1300.39/s)  LR: 6.604e-04  Data: 0.009 (0.013)
Train: 239 [ 650/1251 ( 52%)]  Loss: 3.528 (3.54)  Time: 0.776s, 1318.90/s  (0.787s, 1300.97/s)  LR: 6.604e-04  Data: 0.010 (0.013)
Train: 239 [ 700/1251 ( 56%)]  Loss: 3.655 (3.55)  Time: 0.773s, 1323.89/s  (0.786s, 1302.06/s)  LR: 6.604e-04  Data: 0.010 (0.012)
Train: 239 [ 750/1251 ( 60%)]  Loss: 3.633 (3.55)  Time: 0.771s, 1327.80/s  (0.786s, 1302.83/s)  LR: 6.604e-04  Data: 0.010 (0.012)
Train: 239 [ 800/1251 ( 64%)]  Loss: 3.620 (3.56)  Time: 0.778s, 1316.81/s  (0.786s, 1302.77/s)  LR: 6.604e-04  Data: 0.009 (0.012)
Train: 239 [ 850/1251 ( 68%)]  Loss: 3.716 (3.57)  Time: 0.780s, 1313.17/s  (0.786s, 1303.18/s)  LR: 6.604e-04  Data: 0.011 (0.012)
Train: 239 [ 900/1251 ( 72%)]  Loss: 3.774 (3.58)  Time: 0.773s, 1324.64/s  (0.786s, 1302.79/s)  LR: 6.604e-04  Data: 0.010 (0.012)
Train: 239 [ 950/1251 ( 76%)]  Loss: 3.414 (3.57)  Time: 0.773s, 1325.46/s  (0.787s, 1301.81/s)  LR: 6.604e-04  Data: 0.010 (0.012)
Train: 239 [1000/1251 ( 80%)]  Loss: 3.461 (3.56)  Time: 0.776s, 1318.98/s  (0.786s, 1302.37/s)  LR: 6.604e-04  Data: 0.010 (0.012)
Train: 239 [1050/1251 ( 84%)]  Loss: 3.601 (3.57)  Time: 0.783s, 1307.09/s  (0.786s, 1302.83/s)  LR: 6.604e-04  Data: 0.009 (0.012)
Train: 239 [1100/1251 ( 88%)]  Loss: 3.026 (3.54)  Time: 0.770s, 1330.30/s  (0.786s, 1303.22/s)  LR: 6.604e-04  Data: 0.010 (0.012)
Train: 239 [1150/1251 ( 92%)]  Loss: 3.763 (3.55)  Time: 0.779s, 1313.90/s  (0.786s, 1303.22/s)  LR: 6.604e-04  Data: 0.010 (0.011)
Train: 239 [1200/1251 ( 96%)]  Loss: 3.692 (3.56)  Time: 0.857s, 1194.97/s  (0.786s, 1302.83/s)  LR: 6.604e-04  Data: 0.010 (0.011)
Train: 239 [1250/1251 (100%)]  Loss: 3.607 (3.56)  Time: 0.760s, 1346.70/s  (0.786s, 1302.89/s)  LR: 6.604e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.581 (1.581)  Loss:  0.8931 (0.8931)  Acc@1: 89.1602 (89.1602)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.194 (0.571)  Loss:  1.1348 (1.4349)  Acc@1: 82.4292 (74.2240)  Acc@5: 95.9906 (92.3820)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-232.pth.tar', 74.44600009521484)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-226.pth.tar', 74.40799992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-219.pth.tar', 74.34000014160156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-228.pth.tar', 74.33799990722656)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-233.pth.tar', 74.32200009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-237.pth.tar', 74.3020000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-239.pth.tar', 74.22399999511718)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-224.pth.tar', 74.1619999584961)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-230.pth.tar', 74.15999998291015)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-225.pth.tar', 74.1540000390625)

Train: 240 [   0/1251 (  0%)]  Loss: 3.504 (3.50)  Time: 2.371s,  431.81/s  (2.371s,  431.81/s)  LR: 6.580e-04  Data: 1.582 (1.582)
Train: 240 [  50/1251 (  4%)]  Loss: 3.707 (3.61)  Time: 0.778s, 1315.73/s  (0.822s, 1246.46/s)  LR: 6.580e-04  Data: 0.010 (0.043)
Train: 240 [ 100/1251 (  8%)]  Loss: 3.571 (3.59)  Time: 0.772s, 1327.20/s  (0.803s, 1275.01/s)  LR: 6.580e-04  Data: 0.010 (0.027)
Train: 240 [ 150/1251 ( 12%)]  Loss: 3.432 (3.55)  Time: 0.780s, 1313.11/s  (0.796s, 1286.91/s)  LR: 6.580e-04  Data: 0.010 (0.021)
Train: 240 [ 200/1251 ( 16%)]  Loss: 3.224 (3.49)  Time: 0.771s, 1327.62/s  (0.795s, 1288.20/s)  LR: 6.580e-04  Data: 0.010 (0.019)
Train: 240 [ 250/1251 ( 20%)]  Loss: 3.419 (3.48)  Time: 0.782s, 1309.09/s  (0.791s, 1293.79/s)  LR: 6.580e-04  Data: 0.009 (0.017)
Train: 240 [ 300/1251 ( 24%)]  Loss: 3.900 (3.54)  Time: 0.790s, 1296.69/s  (0.790s, 1296.02/s)  LR: 6.580e-04  Data: 0.010 (0.016)
Train: 240 [ 350/1251 ( 28%)]  Loss: 3.301 (3.51)  Time: 0.788s, 1298.67/s  (0.790s, 1296.37/s)  LR: 6.580e-04  Data: 0.010 (0.015)
Train: 240 [ 400/1251 ( 32%)]  Loss: 3.736 (3.53)  Time: 0.771s, 1328.23/s  (0.790s, 1295.51/s)  LR: 6.580e-04  Data: 0.009 (0.014)
Train: 240 [ 450/1251 ( 36%)]  Loss: 3.122 (3.49)  Time: 0.783s, 1308.23/s  (0.791s, 1294.87/s)  LR: 6.580e-04  Data: 0.010 (0.014)
Train: 240 [ 500/1251 ( 40%)]  Loss: 3.574 (3.50)  Time: 0.779s, 1314.81/s  (0.790s, 1296.78/s)  LR: 6.580e-04  Data: 0.010 (0.013)
Train: 240 [ 550/1251 ( 44%)]  Loss: 3.745 (3.52)  Time: 0.774s, 1322.99/s  (0.789s, 1298.50/s)  LR: 6.580e-04  Data: 0.010 (0.013)
Train: 240 [ 600/1251 ( 48%)]  Loss: 3.485 (3.52)  Time: 0.784s, 1305.96/s  (0.788s, 1299.44/s)  LR: 6.580e-04  Data: 0.010 (0.013)
Train: 240 [ 650/1251 ( 52%)]  Loss: 3.523 (3.52)  Time: 0.819s, 1250.02/s  (0.787s, 1300.48/s)  LR: 6.580e-04  Data: 0.010 (0.013)
Train: 240 [ 700/1251 ( 56%)]  Loss: 3.500 (3.52)  Time: 0.775s, 1321.22/s  (0.788s, 1299.15/s)  LR: 6.580e-04  Data: 0.009 (0.013)
Train: 240 [ 750/1251 ( 60%)]  Loss: 3.035 (3.49)  Time: 0.807s, 1268.81/s  (0.789s, 1298.27/s)  LR: 6.580e-04  Data: 0.010 (0.012)
Train: 240 [ 800/1251 ( 64%)]  Loss: 3.473 (3.49)  Time: 0.781s, 1310.85/s  (0.789s, 1298.54/s)  LR: 6.580e-04  Data: 0.010 (0.012)
Train: 240 [ 850/1251 ( 68%)]  Loss: 3.835 (3.50)  Time: 0.777s, 1318.22/s  (0.788s, 1299.10/s)  LR: 6.580e-04  Data: 0.009 (0.012)
Train: 240 [ 900/1251 ( 72%)]  Loss: 3.497 (3.50)  Time: 0.779s, 1314.69/s  (0.788s, 1300.02/s)  LR: 6.580e-04  Data: 0.010 (0.012)
Train: 240 [ 950/1251 ( 76%)]  Loss: 3.334 (3.50)  Time: 0.773s, 1325.07/s  (0.788s, 1299.80/s)  LR: 6.580e-04  Data: 0.012 (0.012)
Train: 240 [1000/1251 ( 80%)]  Loss: 3.563 (3.50)  Time: 0.832s, 1230.71/s  (0.788s, 1300.07/s)  LR: 6.580e-04  Data: 0.010 (0.012)
Train: 240 [1050/1251 ( 84%)]  Loss: 3.738 (3.51)  Time: 0.839s, 1219.88/s  (0.788s, 1299.18/s)  LR: 6.580e-04  Data: 0.012 (0.012)
Train: 240 [1100/1251 ( 88%)]  Loss: 3.521 (3.51)  Time: 0.782s, 1308.80/s  (0.788s, 1299.67/s)  LR: 6.580e-04  Data: 0.010 (0.012)
Train: 240 [1150/1251 ( 92%)]  Loss: 3.687 (3.52)  Time: 0.772s, 1325.86/s  (0.788s, 1299.81/s)  LR: 6.580e-04  Data: 0.010 (0.012)
Train: 240 [1200/1251 ( 96%)]  Loss: 3.119 (3.50)  Time: 0.773s, 1323.90/s  (0.788s, 1299.79/s)  LR: 6.580e-04  Data: 0.010 (0.012)
Train: 240 [1250/1251 (100%)]  Loss: 3.585 (3.51)  Time: 0.764s, 1340.47/s  (0.787s, 1300.48/s)  LR: 6.580e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.560 (1.560)  Loss:  0.6958 (0.6958)  Acc@1: 90.3320 (90.3320)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.193 (0.564)  Loss:  0.8721 (1.3210)  Acc@1: 84.9057 (74.6560)  Acc@5: 96.8160 (92.3660)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-240.pth.tar', 74.6560000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-232.pth.tar', 74.44600009521484)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-226.pth.tar', 74.40799992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-219.pth.tar', 74.34000014160156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-228.pth.tar', 74.33799990722656)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-233.pth.tar', 74.32200009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-237.pth.tar', 74.3020000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-239.pth.tar', 74.22399999511718)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-224.pth.tar', 74.1619999584961)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-230.pth.tar', 74.15999998291015)

Train: 241 [   0/1251 (  0%)]  Loss: 3.615 (3.62)  Time: 2.702s,  378.99/s  (2.702s,  378.99/s)  LR: 6.555e-04  Data: 1.917 (1.917)
Train: 241 [  50/1251 (  4%)]  Loss: 3.515 (3.56)  Time: 0.774s, 1323.03/s  (0.817s, 1253.12/s)  LR: 6.555e-04  Data: 0.010 (0.048)
Train: 241 [ 100/1251 (  8%)]  Loss: 3.709 (3.61)  Time: 0.776s, 1320.30/s  (0.799s, 1282.23/s)  LR: 6.555e-04  Data: 0.010 (0.029)
Train: 241 [ 150/1251 ( 12%)]  Loss: 3.460 (3.57)  Time: 0.781s, 1311.31/s  (0.793s, 1291.69/s)  LR: 6.555e-04  Data: 0.010 (0.023)
Train: 241 [ 200/1251 ( 16%)]  Loss: 4.008 (3.66)  Time: 0.771s, 1327.47/s  (0.790s, 1296.50/s)  LR: 6.555e-04  Data: 0.010 (0.020)
Train: 241 [ 250/1251 ( 20%)]  Loss: 3.720 (3.67)  Time: 0.772s, 1326.42/s  (0.789s, 1298.66/s)  LR: 6.555e-04  Data: 0.010 (0.018)
Train: 241 [ 300/1251 ( 24%)]  Loss: 3.683 (3.67)  Time: 0.845s, 1211.54/s  (0.789s, 1297.44/s)  LR: 6.555e-04  Data: 0.010 (0.017)
Train: 241 [ 350/1251 ( 28%)]  Loss: 3.659 (3.67)  Time: 0.790s, 1296.76/s  (0.788s, 1299.29/s)  LR: 6.555e-04  Data: 0.010 (0.016)
Train: 241 [ 400/1251 ( 32%)]  Loss: 3.331 (3.63)  Time: 0.773s, 1324.05/s  (0.788s, 1299.54/s)  LR: 6.555e-04  Data: 0.010 (0.015)
Train: 241 [ 450/1251 ( 36%)]  Loss: 3.602 (3.63)  Time: 0.773s, 1324.71/s  (0.787s, 1301.59/s)  LR: 6.555e-04  Data: 0.010 (0.015)
Train: 241 [ 500/1251 ( 40%)]  Loss: 4.033 (3.67)  Time: 0.772s, 1326.07/s  (0.786s, 1302.73/s)  LR: 6.555e-04  Data: 0.009 (0.014)
Train: 241 [ 550/1251 ( 44%)]  Loss: 3.637 (3.66)  Time: 0.774s, 1323.64/s  (0.786s, 1303.42/s)  LR: 6.555e-04  Data: 0.010 (0.014)
Train: 241 [ 600/1251 ( 48%)]  Loss: 3.456 (3.65)  Time: 0.775s, 1320.83/s  (0.786s, 1303.31/s)  LR: 6.555e-04  Data: 0.010 (0.014)
Train: 241 [ 650/1251 ( 52%)]  Loss: 3.324 (3.63)  Time: 0.779s, 1315.26/s  (0.785s, 1304.30/s)  LR: 6.555e-04  Data: 0.009 (0.013)
Train: 241 [ 700/1251 ( 56%)]  Loss: 3.755 (3.63)  Time: 0.788s, 1298.80/s  (0.785s, 1304.31/s)  LR: 6.555e-04  Data: 0.013 (0.013)
Train: 241 [ 750/1251 ( 60%)]  Loss: 3.754 (3.64)  Time: 0.782s, 1309.55/s  (0.785s, 1304.79/s)  LR: 6.555e-04  Data: 0.010 (0.013)
Train: 241 [ 800/1251 ( 64%)]  Loss: 3.718 (3.65)  Time: 0.775s, 1321.90/s  (0.785s, 1305.06/s)  LR: 6.555e-04  Data: 0.009 (0.013)
Train: 241 [ 850/1251 ( 68%)]  Loss: 3.351 (3.63)  Time: 0.778s, 1315.96/s  (0.784s, 1305.80/s)  LR: 6.555e-04  Data: 0.010 (0.013)
Train: 241 [ 900/1251 ( 72%)]  Loss: 3.634 (3.63)  Time: 0.848s, 1208.14/s  (0.784s, 1306.36/s)  LR: 6.555e-04  Data: 0.010 (0.012)
Train: 241 [ 950/1251 ( 76%)]  Loss: 3.762 (3.64)  Time: 0.786s, 1302.71/s  (0.784s, 1305.59/s)  LR: 6.555e-04  Data: 0.010 (0.012)
Train: 241 [1000/1251 ( 80%)]  Loss: 3.935 (3.65)  Time: 0.789s, 1297.03/s  (0.785s, 1305.21/s)  LR: 6.555e-04  Data: 0.010 (0.012)
Train: 241 [1050/1251 ( 84%)]  Loss: 3.632 (3.65)  Time: 0.821s, 1248.01/s  (0.785s, 1305.20/s)  LR: 6.555e-04  Data: 0.010 (0.012)
Train: 241 [1100/1251 ( 88%)]  Loss: 3.662 (3.65)  Time: 0.837s, 1223.62/s  (0.784s, 1305.52/s)  LR: 6.555e-04  Data: 0.010 (0.012)
Train: 241 [1150/1251 ( 92%)]  Loss: 3.188 (3.63)  Time: 0.778s, 1315.76/s  (0.784s, 1305.84/s)  LR: 6.555e-04  Data: 0.010 (0.012)
Train: 241 [1200/1251 ( 96%)]  Loss: 3.360 (3.62)  Time: 0.772s, 1327.02/s  (0.785s, 1303.64/s)  LR: 6.555e-04  Data: 0.010 (0.012)
Train: 241 [1250/1251 (100%)]  Loss: 3.277 (3.61)  Time: 0.760s, 1347.06/s  (0.785s, 1304.07/s)  LR: 6.555e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.520 (1.520)  Loss:  0.6660 (0.6660)  Acc@1: 88.9648 (88.9648)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.194 (0.570)  Loss:  0.8618 (1.2594)  Acc@1: 84.5519 (74.2200)  Acc@5: 96.4623 (92.4880)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-240.pth.tar', 74.6560000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-232.pth.tar', 74.44600009521484)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-226.pth.tar', 74.40799992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-219.pth.tar', 74.34000014160156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-228.pth.tar', 74.33799990722656)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-233.pth.tar', 74.32200009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-237.pth.tar', 74.3020000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-239.pth.tar', 74.22399999511718)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-241.pth.tar', 74.2199998828125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-224.pth.tar', 74.1619999584961)

Train: 242 [   0/1251 (  0%)]  Loss: 3.892 (3.89)  Time: 2.247s,  455.68/s  (2.247s,  455.68/s)  LR: 6.530e-04  Data: 1.506 (1.506)
Train: 242 [  50/1251 (  4%)]  Loss: 3.280 (3.59)  Time: 0.775s, 1321.88/s  (0.816s, 1255.63/s)  LR: 6.530e-04  Data: 0.010 (0.046)
Train: 242 [ 100/1251 (  8%)]  Loss: 3.552 (3.57)  Time: 0.772s, 1326.40/s  (0.798s, 1283.91/s)  LR: 6.530e-04  Data: 0.010 (0.028)
Train: 242 [ 150/1251 ( 12%)]  Loss: 3.336 (3.52)  Time: 0.832s, 1230.65/s  (0.793s, 1291.55/s)  LR: 6.530e-04  Data: 0.016 (0.022)
Train: 242 [ 200/1251 ( 16%)]  Loss: 3.497 (3.51)  Time: 0.777s, 1318.66/s  (0.792s, 1293.36/s)  LR: 6.530e-04  Data: 0.010 (0.019)
Train: 242 [ 250/1251 ( 20%)]  Loss: 3.396 (3.49)  Time: 0.771s, 1327.70/s  (0.792s, 1292.54/s)  LR: 6.530e-04  Data: 0.010 (0.017)
Train: 242 [ 300/1251 ( 24%)]  Loss: 3.836 (3.54)  Time: 0.774s, 1323.78/s  (0.791s, 1294.93/s)  LR: 6.530e-04  Data: 0.010 (0.016)
Train: 242 [ 350/1251 ( 28%)]  Loss: 3.463 (3.53)  Time: 0.824s, 1243.20/s  (0.791s, 1294.34/s)  LR: 6.530e-04  Data: 0.010 (0.015)
Train: 242 [ 400/1251 ( 32%)]  Loss: 3.463 (3.52)  Time: 0.770s, 1329.04/s  (0.790s, 1296.04/s)  LR: 6.530e-04  Data: 0.010 (0.015)
Train: 242 [ 450/1251 ( 36%)]  Loss: 3.377 (3.51)  Time: 0.782s, 1309.29/s  (0.789s, 1297.52/s)  LR: 6.530e-04  Data: 0.010 (0.014)
Train: 242 [ 500/1251 ( 40%)]  Loss: 3.818 (3.54)  Time: 0.808s, 1268.03/s  (0.789s, 1298.32/s)  LR: 6.530e-04  Data: 0.010 (0.014)
Train: 242 [ 550/1251 ( 44%)]  Loss: 3.657 (3.55)  Time: 0.773s, 1323.97/s  (0.788s, 1299.32/s)  LR: 6.530e-04  Data: 0.010 (0.014)
Train: 242 [ 600/1251 ( 48%)]  Loss: 3.522 (3.55)  Time: 0.774s, 1322.42/s  (0.789s, 1298.30/s)  LR: 6.530e-04  Data: 0.010 (0.013)
Train: 242 [ 650/1251 ( 52%)]  Loss: 3.688 (3.56)  Time: 0.774s, 1323.70/s  (0.789s, 1297.80/s)  LR: 6.530e-04  Data: 0.010 (0.013)
Train: 242 [ 700/1251 ( 56%)]  Loss: 3.497 (3.55)  Time: 0.779s, 1314.88/s  (0.789s, 1298.37/s)  LR: 6.530e-04  Data: 0.010 (0.013)
Train: 242 [ 750/1251 ( 60%)]  Loss: 3.518 (3.55)  Time: 0.784s, 1306.90/s  (0.788s, 1298.91/s)  LR: 6.530e-04  Data: 0.009 (0.013)
Train: 242 [ 800/1251 ( 64%)]  Loss: 3.102 (3.52)  Time: 0.783s, 1308.17/s  (0.788s, 1299.28/s)  LR: 6.530e-04  Data: 0.009 (0.013)
Train: 242 [ 850/1251 ( 68%)]  Loss: 3.485 (3.52)  Time: 0.782s, 1309.58/s  (0.788s, 1299.92/s)  LR: 6.530e-04  Data: 0.010 (0.012)
Train: 242 [ 900/1251 ( 72%)]  Loss: 3.671 (3.53)  Time: 0.777s, 1317.66/s  (0.787s, 1300.43/s)  LR: 6.530e-04  Data: 0.010 (0.012)
Train: 242 [ 950/1251 ( 76%)]  Loss: 3.178 (3.51)  Time: 0.802s, 1276.30/s  (0.787s, 1300.85/s)  LR: 6.530e-04  Data: 0.009 (0.012)
Train: 242 [1000/1251 ( 80%)]  Loss: 3.590 (3.52)  Time: 0.786s, 1303.15/s  (0.787s, 1300.99/s)  LR: 6.530e-04  Data: 0.010 (0.012)
Train: 242 [1050/1251 ( 84%)]  Loss: 3.602 (3.52)  Time: 0.772s, 1326.92/s  (0.787s, 1301.73/s)  LR: 6.530e-04  Data: 0.010 (0.012)
Train: 242 [1100/1251 ( 88%)]  Loss: 3.686 (3.53)  Time: 0.808s, 1267.05/s  (0.788s, 1300.08/s)  LR: 6.530e-04  Data: 0.011 (0.012)
Train: 242 [1150/1251 ( 92%)]  Loss: 3.397 (3.52)  Time: 0.806s, 1270.87/s  (0.787s, 1300.77/s)  LR: 6.530e-04  Data: 0.009 (0.012)
Train: 242 [1200/1251 ( 96%)]  Loss: 3.410 (3.52)  Time: 0.774s, 1322.64/s  (0.787s, 1301.26/s)  LR: 6.530e-04  Data: 0.010 (0.012)
Train: 242 [1250/1251 (100%)]  Loss: 3.641 (3.52)  Time: 0.768s, 1333.06/s  (0.786s, 1301.99/s)  LR: 6.530e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.520 (1.520)  Loss:  0.6968 (0.6968)  Acc@1: 89.9414 (89.9414)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.193 (0.583)  Loss:  0.8140 (1.2749)  Acc@1: 84.7877 (74.5920)  Acc@5: 96.5802 (92.4460)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-240.pth.tar', 74.6560000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-242.pth.tar', 74.5920001147461)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-232.pth.tar', 74.44600009521484)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-226.pth.tar', 74.40799992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-219.pth.tar', 74.34000014160156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-228.pth.tar', 74.33799990722656)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-233.pth.tar', 74.32200009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-237.pth.tar', 74.3020000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-239.pth.tar', 74.22399999511718)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-241.pth.tar', 74.2199998828125)

Train: 243 [   0/1251 (  0%)]  Loss: 3.653 (3.65)  Time: 2.252s,  454.68/s  (2.252s,  454.68/s)  LR: 6.505e-04  Data: 1.527 (1.527)
Train: 243 [  50/1251 (  4%)]  Loss: 3.670 (3.66)  Time: 0.782s, 1309.95/s  (0.815s, 1256.71/s)  LR: 6.505e-04  Data: 0.010 (0.045)
Train: 243 [ 100/1251 (  8%)]  Loss: 3.657 (3.66)  Time: 0.773s, 1325.11/s  (0.804s, 1273.91/s)  LR: 6.505e-04  Data: 0.009 (0.028)
Train: 243 [ 150/1251 ( 12%)]  Loss: 3.513 (3.62)  Time: 0.891s, 1148.89/s  (0.800s, 1280.60/s)  LR: 6.505e-04  Data: 0.010 (0.022)
Train: 243 [ 200/1251 ( 16%)]  Loss: 3.517 (3.60)  Time: 0.773s, 1325.05/s  (0.796s, 1286.82/s)  LR: 6.505e-04  Data: 0.010 (0.019)
Train: 243 [ 250/1251 ( 20%)]  Loss: 3.261 (3.55)  Time: 0.776s, 1318.86/s  (0.794s, 1289.53/s)  LR: 6.505e-04  Data: 0.011 (0.017)
Train: 243 [ 300/1251 ( 24%)]  Loss: 3.727 (3.57)  Time: 0.774s, 1322.47/s  (0.793s, 1292.04/s)  LR: 6.505e-04  Data: 0.010 (0.016)
Train: 243 [ 350/1251 ( 28%)]  Loss: 3.695 (3.59)  Time: 0.827s, 1238.50/s  (0.791s, 1295.01/s)  LR: 6.505e-04  Data: 0.010 (0.015)
Train: 243 [ 400/1251 ( 32%)]  Loss: 3.542 (3.58)  Time: 0.773s, 1325.23/s  (0.790s, 1295.55/s)  LR: 6.505e-04  Data: 0.009 (0.014)
Train: 243 [ 450/1251 ( 36%)]  Loss: 3.877 (3.61)  Time: 0.774s, 1323.62/s  (0.790s, 1296.64/s)  LR: 6.505e-04  Data: 0.010 (0.014)
Train: 243 [ 500/1251 ( 40%)]  Loss: 3.566 (3.61)  Time: 0.777s, 1317.19/s  (0.790s, 1296.98/s)  LR: 6.505e-04  Data: 0.010 (0.013)
Train: 243 [ 550/1251 ( 44%)]  Loss: 3.669 (3.61)  Time: 0.775s, 1320.81/s  (0.789s, 1298.28/s)  LR: 6.505e-04  Data: 0.010 (0.013)
Train: 243 [ 600/1251 ( 48%)]  Loss: 3.617 (3.61)  Time: 0.810s, 1264.85/s  (0.788s, 1299.76/s)  LR: 6.505e-04  Data: 0.010 (0.013)
Train: 243 [ 650/1251 ( 52%)]  Loss: 3.559 (3.61)  Time: 0.771s, 1328.60/s  (0.788s, 1299.92/s)  LR: 6.505e-04  Data: 0.010 (0.013)
Train: 243 [ 700/1251 ( 56%)]  Loss: 3.121 (3.58)  Time: 0.773s, 1324.15/s  (0.788s, 1299.84/s)  LR: 6.505e-04  Data: 0.010 (0.012)
Train: 243 [ 750/1251 ( 60%)]  Loss: 3.720 (3.59)  Time: 0.855s, 1197.63/s  (0.788s, 1299.58/s)  LR: 6.505e-04  Data: 0.010 (0.012)
Train: 243 [ 800/1251 ( 64%)]  Loss: 3.453 (3.58)  Time: 0.810s, 1264.31/s  (0.788s, 1300.30/s)  LR: 6.505e-04  Data: 0.010 (0.012)
Train: 243 [ 850/1251 ( 68%)]  Loss: 3.547 (3.58)  Time: 0.827s, 1238.28/s  (0.788s, 1299.62/s)  LR: 6.505e-04  Data: 0.010 (0.012)
Train: 243 [ 900/1251 ( 72%)]  Loss: 3.201 (3.56)  Time: 0.780s, 1313.45/s  (0.788s, 1300.26/s)  LR: 6.505e-04  Data: 0.012 (0.012)
Train: 243 [ 950/1251 ( 76%)]  Loss: 3.481 (3.55)  Time: 0.773s, 1323.92/s  (0.788s, 1300.31/s)  LR: 6.505e-04  Data: 0.010 (0.012)
Train: 243 [1000/1251 ( 80%)]  Loss: 3.856 (3.57)  Time: 0.817s, 1253.44/s  (0.788s, 1300.22/s)  LR: 6.505e-04  Data: 0.009 (0.012)
Train: 243 [1050/1251 ( 84%)]  Loss: 3.330 (3.56)  Time: 0.774s, 1323.79/s  (0.787s, 1300.65/s)  LR: 6.505e-04  Data: 0.010 (0.012)
Train: 243 [1100/1251 ( 88%)]  Loss: 3.426 (3.55)  Time: 0.822s, 1245.85/s  (0.787s, 1300.55/s)  LR: 6.505e-04  Data: 0.009 (0.012)
Train: 243 [1150/1251 ( 92%)]  Loss: 4.023 (3.57)  Time: 0.772s, 1326.32/s  (0.787s, 1301.14/s)  LR: 6.505e-04  Data: 0.009 (0.011)
Train: 243 [1200/1251 ( 96%)]  Loss: 3.631 (3.57)  Time: 0.781s, 1310.66/s  (0.787s, 1301.42/s)  LR: 6.505e-04  Data: 0.009 (0.011)
Train: 243 [1250/1251 (100%)]  Loss: 3.198 (3.56)  Time: 0.763s, 1341.71/s  (0.787s, 1301.72/s)  LR: 6.505e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.563 (1.563)  Loss:  0.8198 (0.8198)  Acc@1: 88.6719 (88.6719)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.194 (0.561)  Loss:  0.9180 (1.3718)  Acc@1: 84.0802 (74.0620)  Acc@5: 95.9906 (92.3220)
Train: 244 [   0/1251 (  0%)]  Loss: 3.408 (3.41)  Time: 2.270s,  451.08/s  (2.270s,  451.08/s)  LR: 6.481e-04  Data: 1.534 (1.534)
Train: 244 [  50/1251 (  4%)]  Loss: 3.369 (3.39)  Time: 0.773s, 1324.10/s  (0.814s, 1257.84/s)  LR: 6.481e-04  Data: 0.009 (0.045)
Train: 244 [ 100/1251 (  8%)]  Loss: 3.171 (3.32)  Time: 0.772s, 1327.04/s  (0.808s, 1267.60/s)  LR: 6.481e-04  Data: 0.009 (0.027)
Train: 244 [ 150/1251 ( 12%)]  Loss: 3.303 (3.31)  Time: 0.773s, 1324.78/s  (0.798s, 1283.40/s)  LR: 6.481e-04  Data: 0.009 (0.022)
Train: 244 [ 200/1251 ( 16%)]  Loss: 3.317 (3.31)  Time: 0.789s, 1298.39/s  (0.794s, 1290.02/s)  LR: 6.481e-04  Data: 0.010 (0.019)
Train: 244 [ 250/1251 ( 20%)]  Loss: 3.702 (3.38)  Time: 0.834s, 1228.16/s  (0.794s, 1290.07/s)  LR: 6.481e-04  Data: 0.009 (0.017)
Train: 244 [ 300/1251 ( 24%)]  Loss: 3.182 (3.35)  Time: 0.773s, 1324.01/s  (0.793s, 1290.69/s)  LR: 6.481e-04  Data: 0.010 (0.016)
Train: 244 [ 350/1251 ( 28%)]  Loss: 3.614 (3.38)  Time: 0.772s, 1325.84/s  (0.792s, 1293.42/s)  LR: 6.481e-04  Data: 0.010 (0.015)
Train: 244 [ 400/1251 ( 32%)]  Loss: 3.443 (3.39)  Time: 0.775s, 1321.24/s  (0.791s, 1294.60/s)  LR: 6.481e-04  Data: 0.010 (0.014)
Train: 244 [ 450/1251 ( 36%)]  Loss: 3.553 (3.41)  Time: 0.782s, 1309.19/s  (0.790s, 1296.18/s)  LR: 6.481e-04  Data: 0.009 (0.014)
Train: 244 [ 500/1251 ( 40%)]  Loss: 3.653 (3.43)  Time: 0.838s, 1221.90/s  (0.789s, 1297.88/s)  LR: 6.481e-04  Data: 0.009 (0.013)
Train: 244 [ 550/1251 ( 44%)]  Loss: 3.784 (3.46)  Time: 0.824s, 1242.64/s  (0.788s, 1299.42/s)  LR: 6.481e-04  Data: 0.008 (0.013)
Train: 244 [ 600/1251 ( 48%)]  Loss: 3.355 (3.45)  Time: 0.775s, 1321.06/s  (0.788s, 1300.21/s)  LR: 6.481e-04  Data: 0.013 (0.013)
Train: 244 [ 650/1251 ( 52%)]  Loss: 3.382 (3.45)  Time: 0.785s, 1304.96/s  (0.787s, 1301.35/s)  LR: 6.481e-04  Data: 0.010 (0.013)
Train: 244 [ 700/1251 ( 56%)]  Loss: 3.486 (3.45)  Time: 0.773s, 1324.62/s  (0.787s, 1301.95/s)  LR: 6.481e-04  Data: 0.009 (0.012)
Train: 244 [ 750/1251 ( 60%)]  Loss: 3.112 (3.43)  Time: 0.776s, 1319.43/s  (0.786s, 1302.98/s)  LR: 6.481e-04  Data: 0.010 (0.012)
Train: 244 [ 800/1251 ( 64%)]  Loss: 3.698 (3.44)  Time: 0.771s, 1327.40/s  (0.785s, 1303.79/s)  LR: 6.481e-04  Data: 0.009 (0.012)
Train: 244 [ 850/1251 ( 68%)]  Loss: 3.407 (3.44)  Time: 0.773s, 1325.12/s  (0.786s, 1303.04/s)  LR: 6.481e-04  Data: 0.009 (0.012)
Train: 244 [ 900/1251 ( 72%)]  Loss: 3.437 (3.44)  Time: 0.771s, 1328.41/s  (0.786s, 1303.60/s)  LR: 6.481e-04  Data: 0.009 (0.012)
Train: 244 [ 950/1251 ( 76%)]  Loss: 3.443 (3.44)  Time: 0.787s, 1301.49/s  (0.785s, 1303.88/s)  LR: 6.481e-04  Data: 0.016 (0.012)
Train: 244 [1000/1251 ( 80%)]  Loss: 3.647 (3.45)  Time: 0.787s, 1301.77/s  (0.785s, 1304.31/s)  LR: 6.481e-04  Data: 0.014 (0.012)
Train: 244 [1050/1251 ( 84%)]  Loss: 3.648 (3.46)  Time: 0.770s, 1330.64/s  (0.785s, 1304.78/s)  LR: 6.481e-04  Data: 0.010 (0.012)
Train: 244 [1100/1251 ( 88%)]  Loss: 3.652 (3.47)  Time: 0.773s, 1324.79/s  (0.785s, 1305.23/s)  LR: 6.481e-04  Data: 0.009 (0.012)
Train: 244 [1150/1251 ( 92%)]  Loss: 3.423 (3.47)  Time: 0.811s, 1262.06/s  (0.785s, 1304.45/s)  LR: 6.481e-04  Data: 0.010 (0.011)
Train: 244 [1200/1251 ( 96%)]  Loss: 3.281 (3.46)  Time: 0.773s, 1325.30/s  (0.785s, 1303.79/s)  LR: 6.481e-04  Data: 0.010 (0.011)
Train: 244 [1250/1251 (100%)]  Loss: 3.338 (3.45)  Time: 0.765s, 1338.72/s  (0.785s, 1304.14/s)  LR: 6.481e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.523 (1.523)  Loss:  0.7295 (0.7295)  Acc@1: 88.8672 (88.8672)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.194 (0.561)  Loss:  0.8115 (1.2772)  Acc@1: 83.1368 (74.0560)  Acc@5: 96.3443 (92.2480)
Train: 245 [   0/1251 (  0%)]  Loss: 3.669 (3.67)  Time: 2.350s,  435.74/s  (2.350s,  435.74/s)  LR: 6.456e-04  Data: 1.616 (1.616)
Train: 245 [  50/1251 (  4%)]  Loss: 3.641 (3.66)  Time: 0.776s, 1319.73/s  (0.825s, 1241.20/s)  LR: 6.456e-04  Data: 0.010 (0.047)
Train: 245 [ 100/1251 (  8%)]  Loss: 3.140 (3.48)  Time: 0.777s, 1317.44/s  (0.805s, 1272.26/s)  LR: 6.456e-04  Data: 0.010 (0.029)
Train: 245 [ 150/1251 ( 12%)]  Loss: 3.810 (3.57)  Time: 0.772s, 1327.16/s  (0.802s, 1276.55/s)  LR: 6.456e-04  Data: 0.010 (0.022)
Train: 245 [ 200/1251 ( 16%)]  Loss: 3.844 (3.62)  Time: 0.782s, 1309.11/s  (0.798s, 1283.37/s)  LR: 6.456e-04  Data: 0.010 (0.019)
Train: 245 [ 250/1251 ( 20%)]  Loss: 3.940 (3.67)  Time: 0.771s, 1327.41/s  (0.794s, 1290.03/s)  LR: 6.456e-04  Data: 0.009 (0.017)
Train: 245 [ 300/1251 ( 24%)]  Loss: 3.503 (3.65)  Time: 0.783s, 1308.05/s  (0.791s, 1293.77/s)  LR: 6.456e-04  Data: 0.010 (0.016)
Train: 245 [ 350/1251 ( 28%)]  Loss: 3.595 (3.64)  Time: 0.782s, 1310.13/s  (0.791s, 1295.37/s)  LR: 6.456e-04  Data: 0.009 (0.015)
Train: 245 [ 400/1251 ( 32%)]  Loss: 3.808 (3.66)  Time: 0.790s, 1295.78/s  (0.790s, 1295.99/s)  LR: 6.456e-04  Data: 0.009 (0.015)
Train: 245 [ 450/1251 ( 36%)]  Loss: 3.813 (3.68)  Time: 0.775s, 1320.48/s  (0.789s, 1297.35/s)  LR: 6.456e-04  Data: 0.010 (0.014)
Train: 245 [ 500/1251 ( 40%)]  Loss: 3.687 (3.68)  Time: 0.777s, 1318.54/s  (0.789s, 1298.50/s)  LR: 6.456e-04  Data: 0.010 (0.014)
Train: 245 [ 550/1251 ( 44%)]  Loss: 3.606 (3.67)  Time: 0.771s, 1328.75/s  (0.789s, 1298.42/s)  LR: 6.456e-04  Data: 0.010 (0.013)
Train: 245 [ 600/1251 ( 48%)]  Loss: 3.396 (3.65)  Time: 0.774s, 1322.82/s  (0.788s, 1299.61/s)  LR: 6.456e-04  Data: 0.010 (0.013)
Train: 245 [ 650/1251 ( 52%)]  Loss: 3.514 (3.64)  Time: 0.773s, 1325.06/s  (0.788s, 1298.92/s)  LR: 6.456e-04  Data: 0.010 (0.013)
Train: 245 [ 700/1251 ( 56%)]  Loss: 3.651 (3.64)  Time: 0.841s, 1217.96/s  (0.788s, 1299.57/s)  LR: 6.456e-04  Data: 0.010 (0.013)
Train: 245 [ 750/1251 ( 60%)]  Loss: 3.632 (3.64)  Time: 0.838s, 1221.73/s  (0.788s, 1299.75/s)  LR: 6.456e-04  Data: 0.010 (0.012)
Train: 245 [ 800/1251 ( 64%)]  Loss: 3.284 (3.62)  Time: 0.773s, 1325.37/s  (0.788s, 1299.59/s)  LR: 6.456e-04  Data: 0.011 (0.012)
Train: 245 [ 850/1251 ( 68%)]  Loss: 3.536 (3.61)  Time: 0.772s, 1327.13/s  (0.788s, 1299.62/s)  LR: 6.456e-04  Data: 0.009 (0.012)
Train: 245 [ 900/1251 ( 72%)]  Loss: 3.659 (3.62)  Time: 0.776s, 1320.32/s  (0.788s, 1299.94/s)  LR: 6.456e-04  Data: 0.010 (0.012)
Train: 245 [ 950/1251 ( 76%)]  Loss: 3.740 (3.62)  Time: 0.773s, 1324.31/s  (0.788s, 1300.26/s)  LR: 6.456e-04  Data: 0.010 (0.012)
Train: 245 [1000/1251 ( 80%)]  Loss: 3.005 (3.59)  Time: 0.785s, 1304.76/s  (0.787s, 1300.46/s)  LR: 6.456e-04  Data: 0.010 (0.012)
Train: 245 [1050/1251 ( 84%)]  Loss: 3.325 (3.58)  Time: 0.819s, 1249.88/s  (0.788s, 1300.26/s)  LR: 6.456e-04  Data: 0.010 (0.012)
Train: 245 [1100/1251 ( 88%)]  Loss: 3.379 (3.57)  Time: 0.778s, 1316.47/s  (0.787s, 1300.79/s)  LR: 6.456e-04  Data: 0.010 (0.012)
Train: 245 [1150/1251 ( 92%)]  Loss: 3.616 (3.57)  Time: 0.776s, 1318.76/s  (0.787s, 1300.92/s)  LR: 6.456e-04  Data: 0.010 (0.012)
Train: 245 [1200/1251 ( 96%)]  Loss: 3.459 (3.57)  Time: 0.862s, 1188.02/s  (0.787s, 1300.76/s)  LR: 6.456e-04  Data: 0.010 (0.012)
Train: 245 [1250/1251 (100%)]  Loss: 3.868 (3.58)  Time: 0.758s, 1350.08/s  (0.787s, 1301.05/s)  LR: 6.456e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.527 (1.527)  Loss:  0.7773 (0.7773)  Acc@1: 89.9414 (89.9414)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.194 (0.564)  Loss:  0.8525 (1.3861)  Acc@1: 86.2028 (74.1140)  Acc@5: 97.4057 (92.1920)
Train: 246 [   0/1251 (  0%)]  Loss: 3.514 (3.51)  Time: 2.371s,  431.82/s  (2.371s,  431.82/s)  LR: 6.431e-04  Data: 1.601 (1.601)
Train: 246 [  50/1251 (  4%)]  Loss: 3.646 (3.58)  Time: 0.771s, 1327.53/s  (0.813s, 1259.34/s)  LR: 6.431e-04  Data: 0.010 (0.043)
Train: 246 [ 100/1251 (  8%)]  Loss: 3.655 (3.61)  Time: 0.774s, 1322.20/s  (0.797s, 1284.07/s)  LR: 6.431e-04  Data: 0.011 (0.027)
Train: 246 [ 150/1251 ( 12%)]  Loss: 3.435 (3.56)  Time: 0.773s, 1323.85/s  (0.792s, 1292.56/s)  LR: 6.431e-04  Data: 0.010 (0.021)
Train: 246 [ 200/1251 ( 16%)]  Loss: 3.871 (3.62)  Time: 0.772s, 1326.37/s  (0.790s, 1296.81/s)  LR: 6.431e-04  Data: 0.009 (0.018)
Train: 246 [ 250/1251 ( 20%)]  Loss: 3.466 (3.60)  Time: 0.775s, 1321.74/s  (0.789s, 1298.64/s)  LR: 6.431e-04  Data: 0.010 (0.017)
Train: 246 [ 300/1251 ( 24%)]  Loss: 3.472 (3.58)  Time: 0.783s, 1308.03/s  (0.789s, 1298.61/s)  LR: 6.431e-04  Data: 0.011 (0.016)
Train: 246 [ 350/1251 ( 28%)]  Loss: 3.808 (3.61)  Time: 0.775s, 1321.55/s  (0.788s, 1299.91/s)  LR: 6.431e-04  Data: 0.009 (0.015)
Train: 246 [ 400/1251 ( 32%)]  Loss: 3.622 (3.61)  Time: 0.782s, 1309.80/s  (0.787s, 1301.19/s)  LR: 6.431e-04  Data: 0.010 (0.014)
Train: 246 [ 450/1251 ( 36%)]  Loss: 3.661 (3.62)  Time: 0.773s, 1325.01/s  (0.786s, 1302.78/s)  LR: 6.431e-04  Data: 0.009 (0.014)
Train: 246 [ 500/1251 ( 40%)]  Loss: 3.445 (3.60)  Time: 0.828s, 1236.73/s  (0.787s, 1301.91/s)  LR: 6.431e-04  Data: 0.010 (0.013)
Train: 246 [ 550/1251 ( 44%)]  Loss: 3.680 (3.61)  Time: 0.776s, 1319.88/s  (0.786s, 1302.03/s)  LR: 6.431e-04  Data: 0.009 (0.013)
Train: 246 [ 600/1251 ( 48%)]  Loss: 3.408 (3.59)  Time: 0.781s, 1310.61/s  (0.788s, 1300.17/s)  LR: 6.431e-04  Data: 0.009 (0.013)
Train: 246 [ 650/1251 ( 52%)]  Loss: 3.719 (3.60)  Time: 0.784s, 1305.38/s  (0.788s, 1299.68/s)  LR: 6.431e-04  Data: 0.010 (0.013)
Train: 246 [ 700/1251 ( 56%)]  Loss: 3.388 (3.59)  Time: 0.783s, 1307.95/s  (0.788s, 1299.94/s)  LR: 6.431e-04  Data: 0.012 (0.013)
Train: 246 [ 750/1251 ( 60%)]  Loss: 3.528 (3.58)  Time: 0.780s, 1312.36/s  (0.787s, 1300.90/s)  LR: 6.431e-04  Data: 0.010 (0.012)
Train: 246 [ 800/1251 ( 64%)]  Loss: 3.539 (3.58)  Time: 0.781s, 1311.46/s  (0.788s, 1300.15/s)  LR: 6.431e-04  Data: 0.010 (0.012)
Train: 246 [ 850/1251 ( 68%)]  Loss: 3.272 (3.56)  Time: 0.773s, 1325.24/s  (0.787s, 1300.53/s)  LR: 6.431e-04  Data: 0.010 (0.012)
Train: 246 [ 900/1251 ( 72%)]  Loss: 3.539 (3.56)  Time: 0.808s, 1267.86/s  (0.787s, 1300.48/s)  LR: 6.431e-04  Data: 0.009 (0.012)
Train: 246 [ 950/1251 ( 76%)]  Loss: 3.169 (3.54)  Time: 0.773s, 1324.07/s  (0.787s, 1300.75/s)  LR: 6.431e-04  Data: 0.010 (0.012)
Train: 246 [1000/1251 ( 80%)]  Loss: 3.683 (3.55)  Time: 0.774s, 1323.76/s  (0.787s, 1301.48/s)  LR: 6.431e-04  Data: 0.010 (0.012)
Train: 246 [1050/1251 ( 84%)]  Loss: 3.676 (3.55)  Time: 0.790s, 1296.43/s  (0.787s, 1301.06/s)  LR: 6.431e-04  Data: 0.009 (0.012)
Train: 246 [1100/1251 ( 88%)]  Loss: 3.359 (3.55)  Time: 0.772s, 1326.04/s  (0.787s, 1301.61/s)  LR: 6.431e-04  Data: 0.011 (0.012)
Train: 246 [1150/1251 ( 92%)]  Loss: 3.408 (3.54)  Time: 0.775s, 1322.03/s  (0.787s, 1301.68/s)  LR: 6.431e-04  Data: 0.010 (0.012)
Train: 246 [1200/1251 ( 96%)]  Loss: 3.922 (3.56)  Time: 0.774s, 1323.20/s  (0.786s, 1302.18/s)  LR: 6.431e-04  Data: 0.010 (0.011)
Train: 246 [1250/1251 (100%)]  Loss: 3.507 (3.55)  Time: 0.779s, 1313.97/s  (0.786s, 1302.68/s)  LR: 6.431e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.588 (1.588)  Loss:  0.8774 (0.8774)  Acc@1: 89.8438 (89.8438)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.194 (0.572)  Loss:  0.8589 (1.3659)  Acc@1: 85.9670 (74.2960)  Acc@5: 96.8160 (92.4300)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-240.pth.tar', 74.6560000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-242.pth.tar', 74.5920001147461)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-232.pth.tar', 74.44600009521484)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-226.pth.tar', 74.40799992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-219.pth.tar', 74.34000014160156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-228.pth.tar', 74.33799990722656)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-233.pth.tar', 74.32200009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-237.pth.tar', 74.3020000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-246.pth.tar', 74.29599998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-239.pth.tar', 74.22399999511718)

Train: 247 [   0/1251 (  0%)]  Loss: 3.682 (3.68)  Time: 2.148s,  476.79/s  (2.148s,  476.79/s)  LR: 6.406e-04  Data: 1.409 (1.409)
Train: 247 [  50/1251 (  4%)]  Loss: 3.631 (3.66)  Time: 0.774s, 1323.69/s  (0.815s, 1255.76/s)  LR: 6.406e-04  Data: 0.010 (0.044)
Train: 247 [ 100/1251 (  8%)]  Loss: 3.313 (3.54)  Time: 0.857s, 1195.35/s  (0.805s, 1272.30/s)  LR: 6.406e-04  Data: 0.009 (0.027)
Train: 247 [ 150/1251 ( 12%)]  Loss: 3.481 (3.53)  Time: 0.858s, 1193.52/s  (0.801s, 1278.05/s)  LR: 6.406e-04  Data: 0.010 (0.022)
Train: 247 [ 200/1251 ( 16%)]  Loss: 3.672 (3.56)  Time: 0.779s, 1313.77/s  (0.796s, 1287.16/s)  LR: 6.406e-04  Data: 0.010 (0.019)
Train: 247 [ 250/1251 ( 20%)]  Loss: 3.337 (3.52)  Time: 0.789s, 1297.24/s  (0.793s, 1291.15/s)  LR: 6.406e-04  Data: 0.010 (0.017)
Train: 247 [ 300/1251 ( 24%)]  Loss: 3.818 (3.56)  Time: 0.785s, 1304.13/s  (0.795s, 1288.35/s)  LR: 6.406e-04  Data: 0.012 (0.016)
Train: 247 [ 350/1251 ( 28%)]  Loss: 3.202 (3.52)  Time: 0.772s, 1325.92/s  (0.792s, 1292.24/s)  LR: 6.406e-04  Data: 0.010 (0.015)
Train: 247 [ 400/1251 ( 32%)]  Loss: 3.406 (3.50)  Time: 0.862s, 1188.49/s  (0.794s, 1290.38/s)  LR: 6.406e-04  Data: 0.011 (0.015)
Train: 247 [ 450/1251 ( 36%)]  Loss: 3.560 (3.51)  Time: 0.773s, 1324.85/s  (0.792s, 1292.92/s)  LR: 6.406e-04  Data: 0.010 (0.014)
Train: 247 [ 500/1251 ( 40%)]  Loss: 3.391 (3.50)  Time: 0.775s, 1321.41/s  (0.793s, 1291.20/s)  LR: 6.406e-04  Data: 0.010 (0.014)
Train: 247 [ 550/1251 ( 44%)]  Loss: 3.432 (3.49)  Time: 0.787s, 1300.69/s  (0.792s, 1292.22/s)  LR: 6.406e-04  Data: 0.013 (0.013)
Train: 247 [ 600/1251 ( 48%)]  Loss: 3.402 (3.49)  Time: 0.793s, 1290.59/s  (0.792s, 1293.48/s)  LR: 6.406e-04  Data: 0.009 (0.013)
Train: 247 [ 650/1251 ( 52%)]  Loss: 3.503 (3.49)  Time: 0.777s, 1318.25/s  (0.791s, 1294.25/s)  LR: 6.406e-04  Data: 0.009 (0.013)
Train: 247 [ 700/1251 ( 56%)]  Loss: 3.454 (3.49)  Time: 0.782s, 1309.03/s  (0.790s, 1295.91/s)  LR: 6.406e-04  Data: 0.010 (0.013)
Train: 247 [ 750/1251 ( 60%)]  Loss: 3.645 (3.50)  Time: 0.773s, 1325.32/s  (0.790s, 1296.92/s)  LR: 6.406e-04  Data: 0.010 (0.013)
Train: 247 [ 800/1251 ( 64%)]  Loss: 3.437 (3.49)  Time: 0.775s, 1322.01/s  (0.789s, 1298.00/s)  LR: 6.406e-04  Data: 0.010 (0.012)
Train: 247 [ 850/1251 ( 68%)]  Loss: 3.486 (3.49)  Time: 0.774s, 1322.75/s  (0.788s, 1299.11/s)  LR: 6.406e-04  Data: 0.010 (0.012)
Train: 247 [ 900/1251 ( 72%)]  Loss: 3.793 (3.51)  Time: 0.774s, 1322.69/s  (0.789s, 1298.56/s)  LR: 6.406e-04  Data: 0.010 (0.012)
Train: 247 [ 950/1251 ( 76%)]  Loss: 3.458 (3.51)  Time: 0.774s, 1323.26/s  (0.788s, 1298.97/s)  LR: 6.406e-04  Data: 0.009 (0.012)
Train: 247 [1000/1251 ( 80%)]  Loss: 3.613 (3.51)  Time: 0.777s, 1317.46/s  (0.788s, 1299.71/s)  LR: 6.406e-04  Data: 0.010 (0.012)
Train: 247 [1050/1251 ( 84%)]  Loss: 3.394 (3.50)  Time: 0.704s, 1454.85/s  (0.787s, 1300.48/s)  LR: 6.406e-04  Data: 0.010 (0.012)
Train: 247 [1100/1251 ( 88%)]  Loss: 3.370 (3.50)  Time: 0.774s, 1323.77/s  (0.787s, 1300.87/s)  LR: 6.406e-04  Data: 0.010 (0.012)
Train: 247 [1150/1251 ( 92%)]  Loss: 3.691 (3.51)  Time: 0.773s, 1324.64/s  (0.787s, 1300.49/s)  LR: 6.406e-04  Data: 0.010 (0.012)
Train: 247 [1200/1251 ( 96%)]  Loss: 3.251 (3.50)  Time: 0.772s, 1325.77/s  (0.787s, 1300.89/s)  LR: 6.406e-04  Data: 0.010 (0.012)
Train: 247 [1250/1251 (100%)]  Loss: 3.624 (3.50)  Time: 0.761s, 1346.43/s  (0.787s, 1301.24/s)  LR: 6.406e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.513 (1.513)  Loss:  0.7666 (0.7666)  Acc@1: 90.3320 (90.3320)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.194 (0.578)  Loss:  0.9355 (1.3167)  Acc@1: 84.6698 (74.3960)  Acc@5: 95.9906 (92.5140)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-240.pth.tar', 74.6560000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-242.pth.tar', 74.5920001147461)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-232.pth.tar', 74.44600009521484)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-226.pth.tar', 74.40799992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-247.pth.tar', 74.39600006347656)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-219.pth.tar', 74.34000014160156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-228.pth.tar', 74.33799990722656)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-233.pth.tar', 74.32200009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-237.pth.tar', 74.3020000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-246.pth.tar', 74.29599998046875)

Train: 248 [   0/1251 (  0%)]  Loss: 3.285 (3.28)  Time: 2.310s,  443.32/s  (2.310s,  443.32/s)  LR: 6.381e-04  Data: 1.582 (1.582)
Train: 248 [  50/1251 (  4%)]  Loss: 3.524 (3.40)  Time: 0.778s, 1315.58/s  (0.817s, 1252.75/s)  LR: 6.381e-04  Data: 0.010 (0.049)
Train: 248 [ 100/1251 (  8%)]  Loss: 3.467 (3.43)  Time: 0.772s, 1325.65/s  (0.798s, 1283.38/s)  LR: 6.381e-04  Data: 0.010 (0.030)
Train: 248 [ 150/1251 ( 12%)]  Loss: 3.946 (3.56)  Time: 0.789s, 1297.83/s  (0.791s, 1295.13/s)  LR: 6.381e-04  Data: 0.009 (0.023)
Train: 248 [ 200/1251 ( 16%)]  Loss: 3.223 (3.49)  Time: 0.784s, 1306.57/s  (0.789s, 1297.47/s)  LR: 6.381e-04  Data: 0.013 (0.020)
Train: 248 [ 250/1251 ( 20%)]  Loss: 3.590 (3.51)  Time: 0.827s, 1238.64/s  (0.790s, 1296.32/s)  LR: 6.381e-04  Data: 0.009 (0.018)
Train: 248 [ 300/1251 ( 24%)]  Loss: 3.479 (3.50)  Time: 0.773s, 1324.69/s  (0.793s, 1291.58/s)  LR: 6.381e-04  Data: 0.009 (0.017)
Train: 248 [ 350/1251 ( 28%)]  Loss: 3.617 (3.52)  Time: 0.788s, 1299.23/s  (0.791s, 1294.99/s)  LR: 6.381e-04  Data: 0.012 (0.016)
Train: 248 [ 400/1251 ( 32%)]  Loss: 3.315 (3.49)  Time: 0.774s, 1323.64/s  (0.790s, 1296.64/s)  LR: 6.381e-04  Data: 0.010 (0.015)
Train: 248 [ 450/1251 ( 36%)]  Loss: 3.771 (3.52)  Time: 0.814s, 1257.28/s  (0.789s, 1297.39/s)  LR: 6.381e-04  Data: 0.011 (0.015)
Train: 248 [ 500/1251 ( 40%)]  Loss: 3.641 (3.53)  Time: 0.772s, 1326.31/s  (0.789s, 1298.25/s)  LR: 6.381e-04  Data: 0.010 (0.014)
Train: 248 [ 550/1251 ( 44%)]  Loss: 3.630 (3.54)  Time: 0.770s, 1329.89/s  (0.788s, 1298.72/s)  LR: 6.381e-04  Data: 0.011 (0.014)
Train: 248 [ 600/1251 ( 48%)]  Loss: 3.492 (3.54)  Time: 0.773s, 1324.66/s  (0.788s, 1298.84/s)  LR: 6.381e-04  Data: 0.010 (0.014)
Train: 248 [ 650/1251 ( 52%)]  Loss: 3.643 (3.54)  Time: 0.831s, 1232.83/s  (0.789s, 1298.22/s)  LR: 6.381e-04  Data: 0.010 (0.013)
Train: 248 [ 700/1251 ( 56%)]  Loss: 3.273 (3.53)  Time: 0.774s, 1323.34/s  (0.788s, 1298.90/s)  LR: 6.381e-04  Data: 0.010 (0.013)
Train: 248 [ 750/1251 ( 60%)]  Loss: 3.500 (3.52)  Time: 0.772s, 1326.14/s  (0.788s, 1299.19/s)  LR: 6.381e-04  Data: 0.010 (0.013)
Train: 248 [ 800/1251 ( 64%)]  Loss: 3.676 (3.53)  Time: 0.779s, 1315.13/s  (0.787s, 1300.36/s)  LR: 6.381e-04  Data: 0.010 (0.013)
Train: 248 [ 850/1251 ( 68%)]  Loss: 3.697 (3.54)  Time: 0.773s, 1325.27/s  (0.788s, 1298.94/s)  LR: 6.381e-04  Data: 0.010 (0.013)
Train: 248 [ 900/1251 ( 72%)]  Loss: 3.441 (3.54)  Time: 0.776s, 1320.03/s  (0.788s, 1299.15/s)  LR: 6.381e-04  Data: 0.009 (0.013)
Train: 248 [ 950/1251 ( 76%)]  Loss: 3.356 (3.53)  Time: 0.773s, 1324.85/s  (0.788s, 1299.80/s)  LR: 6.381e-04  Data: 0.010 (0.012)
Train: 248 [1000/1251 ( 80%)]  Loss: 3.529 (3.53)  Time: 0.772s, 1326.80/s  (0.788s, 1300.13/s)  LR: 6.381e-04  Data: 0.010 (0.012)
Train: 248 [1050/1251 ( 84%)]  Loss: 3.176 (3.51)  Time: 0.772s, 1326.04/s  (0.787s, 1300.57/s)  LR: 6.381e-04  Data: 0.010 (0.012)
Train: 248 [1100/1251 ( 88%)]  Loss: 2.881 (3.48)  Time: 0.773s, 1324.44/s  (0.787s, 1300.73/s)  LR: 6.381e-04  Data: 0.010 (0.012)
Train: 248 [1150/1251 ( 92%)]  Loss: 3.495 (3.49)  Time: 0.831s, 1232.35/s  (0.787s, 1300.91/s)  LR: 6.381e-04  Data: 0.010 (0.012)
Train: 248 [1200/1251 ( 96%)]  Loss: 3.429 (3.48)  Time: 0.774s, 1323.10/s  (0.787s, 1301.50/s)  LR: 6.381e-04  Data: 0.010 (0.012)
Train: 248 [1250/1251 (100%)]  Loss: 3.788 (3.49)  Time: 0.761s, 1345.28/s  (0.787s, 1300.96/s)  LR: 6.381e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.549 (1.549)  Loss:  0.7690 (0.7690)  Acc@1: 89.0625 (89.0625)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.193 (0.560)  Loss:  0.8594 (1.2665)  Acc@1: 84.3160 (74.5080)  Acc@5: 96.5802 (92.6200)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-240.pth.tar', 74.6560000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-242.pth.tar', 74.5920001147461)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-248.pth.tar', 74.5080000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-232.pth.tar', 74.44600009521484)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-226.pth.tar', 74.40799992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-247.pth.tar', 74.39600006347656)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-219.pth.tar', 74.34000014160156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-228.pth.tar', 74.33799990722656)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-233.pth.tar', 74.32200009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-237.pth.tar', 74.3020000390625)

Train: 249 [   0/1251 (  0%)]  Loss: 3.666 (3.67)  Time: 2.148s,  476.62/s  (2.148s,  476.62/s)  LR: 6.356e-04  Data: 1.419 (1.419)
Train: 249 [  50/1251 (  4%)]  Loss: 3.553 (3.61)  Time: 0.830s, 1234.43/s  (0.822s, 1245.11/s)  LR: 6.356e-04  Data: 0.010 (0.043)
Train: 249 [ 100/1251 (  8%)]  Loss: 3.535 (3.58)  Time: 0.774s, 1323.04/s  (0.803s, 1274.80/s)  LR: 6.356e-04  Data: 0.010 (0.027)
Train: 249 [ 150/1251 ( 12%)]  Loss: 3.378 (3.53)  Time: 0.773s, 1324.26/s  (0.795s, 1288.58/s)  LR: 6.356e-04  Data: 0.010 (0.021)
Train: 249 [ 200/1251 ( 16%)]  Loss: 3.669 (3.56)  Time: 0.772s, 1326.14/s  (0.792s, 1293.20/s)  LR: 6.356e-04  Data: 0.010 (0.018)
Train: 249 [ 250/1251 ( 20%)]  Loss: 3.482 (3.55)  Time: 0.773s, 1324.94/s  (0.789s, 1297.68/s)  LR: 6.356e-04  Data: 0.011 (0.017)
Train: 249 [ 300/1251 ( 24%)]  Loss: 3.756 (3.58)  Time: 0.787s, 1300.62/s  (0.788s, 1300.30/s)  LR: 6.356e-04  Data: 0.012 (0.016)
Train: 249 [ 350/1251 ( 28%)]  Loss: 3.537 (3.57)  Time: 0.776s, 1319.57/s  (0.786s, 1301.99/s)  LR: 6.356e-04  Data: 0.009 (0.015)
Train: 249 [ 400/1251 ( 32%)]  Loss: 3.312 (3.54)  Time: 0.780s, 1312.48/s  (0.787s, 1300.72/s)  LR: 6.356e-04  Data: 0.010 (0.014)
Train: 249 [ 450/1251 ( 36%)]  Loss: 3.526 (3.54)  Time: 0.782s, 1309.29/s  (0.787s, 1301.25/s)  LR: 6.356e-04  Data: 0.013 (0.014)
Train: 249 [ 500/1251 ( 40%)]  Loss: 3.455 (3.53)  Time: 0.772s, 1326.16/s  (0.786s, 1302.67/s)  LR: 6.356e-04  Data: 0.010 (0.013)
Train: 249 [ 550/1251 ( 44%)]  Loss: 3.608 (3.54)  Time: 0.779s, 1314.95/s  (0.785s, 1303.77/s)  LR: 6.356e-04  Data: 0.010 (0.013)
Train: 249 [ 600/1251 ( 48%)]  Loss: 3.873 (3.57)  Time: 0.847s, 1208.66/s  (0.787s, 1301.87/s)  LR: 6.356e-04  Data: 0.010 (0.013)
Train: 249 [ 650/1251 ( 52%)]  Loss: 3.485 (3.56)  Time: 0.772s, 1326.10/s  (0.786s, 1303.17/s)  LR: 6.356e-04  Data: 0.009 (0.013)
Train: 249 [ 700/1251 ( 56%)]  Loss: 3.613 (3.56)  Time: 0.773s, 1325.03/s  (0.785s, 1303.99/s)  LR: 6.356e-04  Data: 0.009 (0.013)
Train: 249 [ 750/1251 ( 60%)]  Loss: 3.287 (3.55)  Time: 0.776s, 1318.87/s  (0.785s, 1304.66/s)  LR: 6.356e-04  Data: 0.011 (0.012)
Train: 249 [ 800/1251 ( 64%)]  Loss: 3.574 (3.55)  Time: 0.775s, 1321.88/s  (0.784s, 1305.31/s)  LR: 6.356e-04  Data: 0.012 (0.012)
Train: 249 [ 850/1251 ( 68%)]  Loss: 3.739 (3.56)  Time: 0.772s, 1325.58/s  (0.784s, 1305.91/s)  LR: 6.356e-04  Data: 0.009 (0.012)
Train: 249 [ 900/1251 ( 72%)]  Loss: 3.795 (3.57)  Time: 0.773s, 1324.85/s  (0.784s, 1306.52/s)  LR: 6.356e-04  Data: 0.010 (0.012)
Train: 249 [ 950/1251 ( 76%)]  Loss: 3.585 (3.57)  Time: 0.779s, 1314.38/s  (0.784s, 1305.94/s)  LR: 6.356e-04  Data: 0.010 (0.012)
Train: 249 [1000/1251 ( 80%)]  Loss: 3.451 (3.57)  Time: 0.773s, 1324.89/s  (0.785s, 1304.16/s)  LR: 6.356e-04  Data: 0.010 (0.012)
Train: 249 [1050/1251 ( 84%)]  Loss: 3.292 (3.55)  Time: 0.829s, 1235.67/s  (0.785s, 1304.24/s)  LR: 6.356e-04  Data: 0.009 (0.012)
Train: 249 [1100/1251 ( 88%)]  Loss: 3.321 (3.54)  Time: 0.783s, 1308.48/s  (0.785s, 1304.16/s)  LR: 6.356e-04  Data: 0.010 (0.012)
Train: 249 [1150/1251 ( 92%)]  Loss: 3.653 (3.55)  Time: 0.784s, 1305.53/s  (0.785s, 1304.41/s)  LR: 6.356e-04  Data: 0.011 (0.012)
Train: 249 [1200/1251 ( 96%)]  Loss: 3.637 (3.55)  Time: 0.775s, 1321.84/s  (0.785s, 1304.50/s)  LR: 6.356e-04  Data: 0.009 (0.012)
Train: 249 [1250/1251 (100%)]  Loss: 3.828 (3.56)  Time: 0.759s, 1348.30/s  (0.785s, 1304.89/s)  LR: 6.356e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.523 (1.523)  Loss:  0.7280 (0.7280)  Acc@1: 88.4766 (88.4766)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.194 (0.573)  Loss:  0.7905 (1.2794)  Acc@1: 84.0802 (74.5080)  Acc@5: 96.6981 (92.3660)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-240.pth.tar', 74.6560000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-242.pth.tar', 74.5920001147461)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-249.pth.tar', 74.50800006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-248.pth.tar', 74.5080000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-232.pth.tar', 74.44600009521484)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-226.pth.tar', 74.40799992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-247.pth.tar', 74.39600006347656)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-219.pth.tar', 74.34000014160156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-228.pth.tar', 74.33799990722656)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-233.pth.tar', 74.32200009277344)

Train: 250 [   0/1251 (  0%)]  Loss: 3.652 (3.65)  Time: 2.435s,  420.57/s  (2.435s,  420.57/s)  LR: 6.331e-04  Data: 1.700 (1.700)
Train: 250 [  50/1251 (  4%)]  Loss: 3.637 (3.64)  Time: 0.778s, 1316.18/s  (0.811s, 1262.01/s)  LR: 6.331e-04  Data: 0.010 (0.043)
Train: 250 [ 100/1251 (  8%)]  Loss: 3.741 (3.68)  Time: 0.779s, 1314.60/s  (0.796s, 1286.22/s)  LR: 6.331e-04  Data: 0.010 (0.027)
Train: 250 [ 150/1251 ( 12%)]  Loss: 3.642 (3.67)  Time: 0.774s, 1323.24/s  (0.793s, 1291.85/s)  LR: 6.331e-04  Data: 0.010 (0.021)
Train: 250 [ 200/1251 ( 16%)]  Loss: 3.489 (3.63)  Time: 0.772s, 1326.87/s  (0.792s, 1293.75/s)  LR: 6.331e-04  Data: 0.009 (0.018)
Train: 250 [ 250/1251 ( 20%)]  Loss: 3.554 (3.62)  Time: 0.794s, 1289.88/s  (0.793s, 1291.69/s)  LR: 6.331e-04  Data: 0.012 (0.017)
Train: 250 [ 300/1251 ( 24%)]  Loss: 3.453 (3.60)  Time: 0.774s, 1322.72/s  (0.791s, 1295.34/s)  LR: 6.331e-04  Data: 0.010 (0.016)
Train: 250 [ 350/1251 ( 28%)]  Loss: 3.414 (3.57)  Time: 0.776s, 1319.76/s  (0.790s, 1296.43/s)  LR: 6.331e-04  Data: 0.009 (0.015)
Train: 250 [ 400/1251 ( 32%)]  Loss: 3.393 (3.55)  Time: 0.811s, 1263.08/s  (0.789s, 1298.13/s)  LR: 6.331e-04  Data: 0.009 (0.014)
Train: 250 [ 450/1251 ( 36%)]  Loss: 3.615 (3.56)  Time: 0.776s, 1319.69/s  (0.788s, 1299.13/s)  LR: 6.331e-04  Data: 0.010 (0.014)
Train: 250 [ 500/1251 ( 40%)]  Loss: 3.481 (3.55)  Time: 0.774s, 1322.96/s  (0.787s, 1300.95/s)  LR: 6.331e-04  Data: 0.009 (0.013)
Train: 250 [ 550/1251 ( 44%)]  Loss: 3.163 (3.52)  Time: 0.771s, 1327.35/s  (0.787s, 1300.70/s)  LR: 6.331e-04  Data: 0.010 (0.013)
Train: 250 [ 600/1251 ( 48%)]  Loss: 3.692 (3.53)  Time: 0.780s, 1313.18/s  (0.787s, 1301.05/s)  LR: 6.331e-04  Data: 0.011 (0.013)
Train: 250 [ 650/1251 ( 52%)]  Loss: 3.792 (3.55)  Time: 0.833s, 1228.73/s  (0.787s, 1301.30/s)  LR: 6.331e-04  Data: 0.013 (0.013)
Train: 250 [ 700/1251 ( 56%)]  Loss: 3.426 (3.54)  Time: 0.784s, 1306.06/s  (0.787s, 1301.96/s)  LR: 6.331e-04  Data: 0.011 (0.013)
Train: 250 [ 750/1251 ( 60%)]  Loss: 3.444 (3.54)  Time: 0.779s, 1313.93/s  (0.786s, 1302.56/s)  LR: 6.331e-04  Data: 0.009 (0.012)
Train: 250 [ 800/1251 ( 64%)]  Loss: 3.832 (3.55)  Time: 0.772s, 1326.54/s  (0.786s, 1302.31/s)  LR: 6.331e-04  Data: 0.011 (0.012)
Train: 250 [ 850/1251 ( 68%)]  Loss: 3.360 (3.54)  Time: 0.805s, 1272.17/s  (0.786s, 1303.02/s)  LR: 6.331e-04  Data: 0.009 (0.012)
Train: 250 [ 900/1251 ( 72%)]  Loss: 3.468 (3.54)  Time: 0.780s, 1312.67/s  (0.785s, 1303.76/s)  LR: 6.331e-04  Data: 0.010 (0.012)
Train: 250 [ 950/1251 ( 76%)]  Loss: 3.683 (3.55)  Time: 0.773s, 1324.89/s  (0.785s, 1303.80/s)  LR: 6.331e-04  Data: 0.010 (0.012)
Train: 250 [1000/1251 ( 80%)]  Loss: 3.446 (3.54)  Time: 0.774s, 1322.82/s  (0.785s, 1304.23/s)  LR: 6.331e-04  Data: 0.010 (0.012)
Train: 250 [1050/1251 ( 84%)]  Loss: 3.658 (3.55)  Time: 0.773s, 1324.93/s  (0.785s, 1304.56/s)  LR: 6.331e-04  Data: 0.010 (0.012)
Train: 250 [1100/1251 ( 88%)]  Loss: 3.357 (3.54)  Time: 0.773s, 1325.26/s  (0.785s, 1304.63/s)  LR: 6.331e-04  Data: 0.010 (0.012)
Train: 250 [1150/1251 ( 92%)]  Loss: 3.569 (3.54)  Time: 0.772s, 1326.66/s  (0.785s, 1305.05/s)  LR: 6.331e-04  Data: 0.010 (0.012)
Train: 250 [1200/1251 ( 96%)]  Loss: 3.467 (3.54)  Time: 0.772s, 1326.05/s  (0.784s, 1305.55/s)  LR: 6.331e-04  Data: 0.010 (0.012)
Train: 250 [1250/1251 (100%)]  Loss: 3.279 (3.53)  Time: 0.768s, 1332.93/s  (0.784s, 1305.96/s)  LR: 6.331e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.584 (1.584)  Loss:  0.7642 (0.7642)  Acc@1: 88.8672 (88.8672)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.194 (0.564)  Loss:  0.8140 (1.2875)  Acc@1: 85.7311 (74.7680)  Acc@5: 96.4623 (92.6460)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-250.pth.tar', 74.76799987792968)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-240.pth.tar', 74.6560000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-242.pth.tar', 74.5920001147461)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-249.pth.tar', 74.50800006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-248.pth.tar', 74.5080000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-232.pth.tar', 74.44600009521484)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-226.pth.tar', 74.40799992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-247.pth.tar', 74.39600006347656)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-219.pth.tar', 74.34000014160156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-228.pth.tar', 74.33799990722656)

Train: 251 [   0/1251 (  0%)]  Loss: 3.394 (3.39)  Time: 2.206s,  464.28/s  (2.206s,  464.28/s)  LR: 6.306e-04  Data: 1.475 (1.475)
Train: 251 [  50/1251 (  4%)]  Loss: 3.822 (3.61)  Time: 0.783s, 1308.52/s  (0.827s, 1238.76/s)  LR: 6.306e-04  Data: 0.013 (0.043)
Train: 251 [ 100/1251 (  8%)]  Loss: 3.628 (3.61)  Time: 0.785s, 1304.52/s  (0.808s, 1268.05/s)  LR: 6.306e-04  Data: 0.010 (0.027)
Train: 251 [ 150/1251 ( 12%)]  Loss: 3.325 (3.54)  Time: 0.784s, 1306.00/s  (0.801s, 1278.91/s)  LR: 6.306e-04  Data: 0.013 (0.021)
Train: 251 [ 200/1251 ( 16%)]  Loss: 3.405 (3.51)  Time: 0.773s, 1324.31/s  (0.795s, 1287.24/s)  LR: 6.306e-04  Data: 0.010 (0.019)
Train: 251 [ 250/1251 ( 20%)]  Loss: 3.422 (3.50)  Time: 0.773s, 1325.25/s  (0.793s, 1291.85/s)  LR: 6.306e-04  Data: 0.010 (0.017)
Train: 251 [ 300/1251 ( 24%)]  Loss: 3.445 (3.49)  Time: 0.775s, 1321.23/s  (0.793s, 1291.53/s)  LR: 6.306e-04  Data: 0.010 (0.016)
Train: 251 [ 350/1251 ( 28%)]  Loss: 3.575 (3.50)  Time: 0.773s, 1325.39/s  (0.791s, 1294.31/s)  LR: 6.306e-04  Data: 0.010 (0.015)
Train: 251 [ 400/1251 ( 32%)]  Loss: 3.486 (3.50)  Time: 0.774s, 1323.39/s  (0.789s, 1297.05/s)  LR: 6.306e-04  Data: 0.010 (0.014)
Train: 251 [ 450/1251 ( 36%)]  Loss: 3.472 (3.50)  Time: 0.773s, 1324.40/s  (0.789s, 1298.15/s)  LR: 6.306e-04  Data: 0.010 (0.014)
Train: 251 [ 500/1251 ( 40%)]  Loss: 3.211 (3.47)  Time: 0.795s, 1287.66/s  (0.789s, 1298.40/s)  LR: 6.306e-04  Data: 0.010 (0.014)
Train: 251 [ 550/1251 ( 44%)]  Loss: 3.802 (3.50)  Time: 0.774s, 1323.00/s  (0.788s, 1299.48/s)  LR: 6.306e-04  Data: 0.010 (0.013)
Train: 251 [ 600/1251 ( 48%)]  Loss: 3.326 (3.49)  Time: 0.772s, 1325.91/s  (0.788s, 1300.10/s)  LR: 6.306e-04  Data: 0.010 (0.013)
Train: 251 [ 650/1251 ( 52%)]  Loss: 3.694 (3.50)  Time: 0.777s, 1317.72/s  (0.788s, 1300.22/s)  LR: 6.306e-04  Data: 0.010 (0.013)
Train: 251 [ 700/1251 ( 56%)]  Loss: 3.238 (3.48)  Time: 0.778s, 1316.10/s  (0.787s, 1300.72/s)  LR: 6.306e-04  Data: 0.010 (0.013)
Train: 251 [ 750/1251 ( 60%)]  Loss: 3.572 (3.49)  Time: 0.774s, 1322.45/s  (0.787s, 1301.31/s)  LR: 6.306e-04  Data: 0.010 (0.012)
Train: 251 [ 800/1251 ( 64%)]  Loss: 3.228 (3.47)  Time: 0.826s, 1239.85/s  (0.788s, 1299.17/s)  LR: 6.306e-04  Data: 0.014 (0.012)
Train: 251 [ 850/1251 ( 68%)]  Loss: 3.597 (3.48)  Time: 0.773s, 1325.32/s  (0.788s, 1298.88/s)  LR: 6.306e-04  Data: 0.009 (0.012)
Train: 251 [ 900/1251 ( 72%)]  Loss: 3.956 (3.51)  Time: 0.781s, 1310.49/s  (0.788s, 1299.72/s)  LR: 6.306e-04  Data: 0.013 (0.012)
Train: 251 [ 950/1251 ( 76%)]  Loss: 3.355 (3.50)  Time: 0.773s, 1325.44/s  (0.788s, 1299.77/s)  LR: 6.306e-04  Data: 0.010 (0.012)
Train: 251 [1000/1251 ( 80%)]  Loss: 3.761 (3.51)  Time: 0.773s, 1324.37/s  (0.787s, 1300.68/s)  LR: 6.306e-04  Data: 0.010 (0.012)
Train: 251 [1050/1251 ( 84%)]  Loss: 3.916 (3.53)  Time: 0.786s, 1302.66/s  (0.787s, 1301.18/s)  LR: 6.306e-04  Data: 0.010 (0.012)
Train: 251 [1100/1251 ( 88%)]  Loss: 3.924 (3.55)  Time: 0.772s, 1327.09/s  (0.787s, 1301.47/s)  LR: 6.306e-04  Data: 0.010 (0.012)
Train: 251 [1150/1251 ( 92%)]  Loss: 3.314 (3.54)  Time: 0.773s, 1324.53/s  (0.787s, 1301.21/s)  LR: 6.306e-04  Data: 0.011 (0.012)
Train: 251 [1200/1251 ( 96%)]  Loss: 3.449 (3.53)  Time: 0.791s, 1295.11/s  (0.787s, 1301.71/s)  LR: 6.306e-04  Data: 0.012 (0.012)
Train: 251 [1250/1251 (100%)]  Loss: 3.435 (3.53)  Time: 0.770s, 1330.00/s  (0.787s, 1301.65/s)  LR: 6.306e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.525 (1.525)  Loss:  0.8071 (0.8071)  Acc@1: 89.7461 (89.7461)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.194 (0.562)  Loss:  0.9683 (1.3728)  Acc@1: 84.3160 (74.2420)  Acc@5: 96.5802 (92.2540)
Train: 252 [   0/1251 (  0%)]  Loss: 3.349 (3.35)  Time: 2.320s,  441.39/s  (2.320s,  441.39/s)  LR: 6.281e-04  Data: 1.594 (1.594)
Train: 252 [  50/1251 (  4%)]  Loss: 3.140 (3.24)  Time: 0.774s, 1323.01/s  (0.828s, 1236.61/s)  LR: 6.281e-04  Data: 0.009 (0.047)
Train: 252 [ 100/1251 (  8%)]  Loss: 3.820 (3.44)  Time: 0.790s, 1296.02/s  (0.803s, 1275.56/s)  LR: 6.281e-04  Data: 0.009 (0.028)
Train: 252 [ 150/1251 ( 12%)]  Loss: 3.819 (3.53)  Time: 0.773s, 1324.64/s  (0.796s, 1286.83/s)  LR: 6.281e-04  Data: 0.010 (0.022)
Train: 252 [ 200/1251 ( 16%)]  Loss: 3.552 (3.54)  Time: 0.778s, 1316.31/s  (0.792s, 1293.28/s)  LR: 6.281e-04  Data: 0.010 (0.019)
Train: 252 [ 250/1251 ( 20%)]  Loss: 3.449 (3.52)  Time: 0.779s, 1313.77/s  (0.789s, 1297.69/s)  LR: 6.281e-04  Data: 0.011 (0.017)
Train: 252 [ 300/1251 ( 24%)]  Loss: 3.302 (3.49)  Time: 0.772s, 1325.77/s  (0.787s, 1300.45/s)  LR: 6.281e-04  Data: 0.010 (0.016)
Train: 252 [ 350/1251 ( 28%)]  Loss: 3.674 (3.51)  Time: 0.825s, 1240.58/s  (0.786s, 1303.10/s)  LR: 6.281e-04  Data: 0.012 (0.015)
Train: 252 [ 400/1251 ( 32%)]  Loss: 4.031 (3.57)  Time: 0.771s, 1327.37/s  (0.785s, 1304.26/s)  LR: 6.281e-04  Data: 0.010 (0.015)
Train: 252 [ 450/1251 ( 36%)]  Loss: 3.432 (3.56)  Time: 0.784s, 1306.55/s  (0.785s, 1304.32/s)  LR: 6.281e-04  Data: 0.015 (0.014)
Train: 252 [ 500/1251 ( 40%)]  Loss: 3.723 (3.57)  Time: 0.778s, 1315.91/s  (0.785s, 1304.65/s)  LR: 6.281e-04  Data: 0.011 (0.014)
Train: 252 [ 550/1251 ( 44%)]  Loss: 3.476 (3.56)  Time: 0.786s, 1303.27/s  (0.785s, 1304.42/s)  LR: 6.281e-04  Data: 0.011 (0.014)
Train: 252 [ 600/1251 ( 48%)]  Loss: 3.858 (3.59)  Time: 0.778s, 1316.69/s  (0.785s, 1304.67/s)  LR: 6.281e-04  Data: 0.010 (0.013)
Train: 252 [ 650/1251 ( 52%)]  Loss: 3.340 (3.57)  Time: 0.827s, 1237.55/s  (0.785s, 1304.70/s)  LR: 6.281e-04  Data: 0.014 (0.013)
Train: 252 [ 700/1251 ( 56%)]  Loss: 3.571 (3.57)  Time: 0.779s, 1314.00/s  (0.786s, 1302.82/s)  LR: 6.281e-04  Data: 0.010 (0.013)
Train: 252 [ 750/1251 ( 60%)]  Loss: 3.640 (3.57)  Time: 0.780s, 1312.24/s  (0.786s, 1303.26/s)  LR: 6.281e-04  Data: 0.010 (0.013)
Train: 252 [ 800/1251 ( 64%)]  Loss: 3.145 (3.55)  Time: 0.771s, 1328.37/s  (0.786s, 1303.54/s)  LR: 6.281e-04  Data: 0.009 (0.013)
Train: 252 [ 850/1251 ( 68%)]  Loss: 3.735 (3.56)  Time: 0.774s, 1323.13/s  (0.786s, 1303.29/s)  LR: 6.281e-04  Data: 0.010 (0.013)
Train: 252 [ 900/1251 ( 72%)]  Loss: 3.801 (3.57)  Time: 0.774s, 1322.90/s  (0.786s, 1303.28/s)  LR: 6.281e-04  Data: 0.010 (0.012)
Train: 252 [ 950/1251 ( 76%)]  Loss: 3.694 (3.58)  Time: 0.818s, 1252.06/s  (0.785s, 1303.66/s)  LR: 6.281e-04  Data: 0.010 (0.012)
Train: 252 [1000/1251 ( 80%)]  Loss: 3.313 (3.56)  Time: 0.773s, 1325.28/s  (0.785s, 1304.09/s)  LR: 6.281e-04  Data: 0.009 (0.012)
Train: 252 [1050/1251 ( 84%)]  Loss: 3.625 (3.57)  Time: 0.779s, 1313.84/s  (0.786s, 1302.47/s)  LR: 6.281e-04  Data: 0.011 (0.012)
Train: 252 [1100/1251 ( 88%)]  Loss: 3.623 (3.57)  Time: 0.784s, 1306.05/s  (0.787s, 1301.78/s)  LR: 6.281e-04  Data: 0.010 (0.012)
Train: 252 [1150/1251 ( 92%)]  Loss: 3.253 (3.56)  Time: 0.773s, 1323.92/s  (0.787s, 1301.80/s)  LR: 6.281e-04  Data: 0.010 (0.012)
Train: 252 [1200/1251 ( 96%)]  Loss: 3.415 (3.55)  Time: 0.772s, 1326.09/s  (0.786s, 1302.12/s)  LR: 6.281e-04  Data: 0.010 (0.012)
Train: 252 [1250/1251 (100%)]  Loss: 3.579 (3.55)  Time: 0.758s, 1351.10/s  (0.786s, 1302.70/s)  LR: 6.281e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.555 (1.555)  Loss:  0.9048 (0.9048)  Acc@1: 87.8906 (87.8906)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.194 (0.568)  Loss:  0.9507 (1.3644)  Acc@1: 84.0802 (74.3480)  Acc@5: 95.5189 (92.4980)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-250.pth.tar', 74.76799987792968)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-240.pth.tar', 74.6560000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-242.pth.tar', 74.5920001147461)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-249.pth.tar', 74.50800006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-248.pth.tar', 74.5080000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-232.pth.tar', 74.44600009521484)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-226.pth.tar', 74.40799992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-247.pth.tar', 74.39600006347656)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-252.pth.tar', 74.34799993652344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-219.pth.tar', 74.34000014160156)

Train: 253 [   0/1251 (  0%)]  Loss: 3.672 (3.67)  Time: 2.324s,  440.57/s  (2.324s,  440.57/s)  LR: 6.256e-04  Data: 1.593 (1.593)
Train: 253 [  50/1251 (  4%)]  Loss: 3.589 (3.63)  Time: 0.773s, 1325.53/s  (0.818s, 1252.50/s)  LR: 6.256e-04  Data: 0.010 (0.045)
Train: 253 [ 100/1251 (  8%)]  Loss: 3.013 (3.42)  Time: 0.780s, 1313.16/s  (0.798s, 1282.41/s)  LR: 6.256e-04  Data: 0.011 (0.028)
Train: 253 [ 150/1251 ( 12%)]  Loss: 3.455 (3.43)  Time: 0.777s, 1317.21/s  (0.792s, 1293.02/s)  LR: 6.256e-04  Data: 0.010 (0.022)
Train: 253 [ 200/1251 ( 16%)]  Loss: 3.562 (3.46)  Time: 0.775s, 1321.98/s  (0.789s, 1297.95/s)  LR: 6.256e-04  Data: 0.011 (0.019)
Train: 253 [ 250/1251 ( 20%)]  Loss: 3.622 (3.49)  Time: 0.773s, 1325.04/s  (0.788s, 1299.28/s)  LR: 6.256e-04  Data: 0.009 (0.017)
Train: 253 [ 300/1251 ( 24%)]  Loss: 3.665 (3.51)  Time: 0.808s, 1267.94/s  (0.791s, 1294.92/s)  LR: 6.256e-04  Data: 0.010 (0.016)
Train: 253 [ 350/1251 ( 28%)]  Loss: 3.600 (3.52)  Time: 0.784s, 1306.57/s  (0.791s, 1294.40/s)  LR: 6.256e-04  Data: 0.009 (0.015)
Train: 253 [ 400/1251 ( 32%)]  Loss: 3.456 (3.51)  Time: 0.772s, 1326.22/s  (0.790s, 1296.46/s)  LR: 6.256e-04  Data: 0.010 (0.015)
Train: 253 [ 450/1251 ( 36%)]  Loss: 3.635 (3.53)  Time: 0.834s, 1227.61/s  (0.789s, 1298.33/s)  LR: 6.256e-04  Data: 0.009 (0.014)
Train: 253 [ 500/1251 ( 40%)]  Loss: 3.522 (3.53)  Time: 0.790s, 1295.52/s  (0.789s, 1297.87/s)  LR: 6.256e-04  Data: 0.011 (0.014)
Train: 253 [ 550/1251 ( 44%)]  Loss: 3.396 (3.52)  Time: 0.787s, 1301.57/s  (0.789s, 1297.04/s)  LR: 6.256e-04  Data: 0.011 (0.013)
Train: 253 [ 600/1251 ( 48%)]  Loss: 3.394 (3.51)  Time: 0.776s, 1319.51/s  (0.789s, 1297.85/s)  LR: 6.256e-04  Data: 0.010 (0.013)
Train: 253 [ 650/1251 ( 52%)]  Loss: 3.724 (3.52)  Time: 0.773s, 1324.66/s  (0.789s, 1298.53/s)  LR: 6.256e-04  Data: 0.010 (0.013)
Train: 253 [ 700/1251 ( 56%)]  Loss: 3.436 (3.52)  Time: 0.772s, 1326.58/s  (0.788s, 1299.59/s)  LR: 6.256e-04  Data: 0.009 (0.013)
Train: 253 [ 750/1251 ( 60%)]  Loss: 3.467 (3.51)  Time: 0.781s, 1311.98/s  (0.787s, 1300.32/s)  LR: 6.256e-04  Data: 0.010 (0.012)
Train: 253 [ 800/1251 ( 64%)]  Loss: 3.529 (3.51)  Time: 0.780s, 1312.31/s  (0.787s, 1301.29/s)  LR: 6.256e-04  Data: 0.010 (0.012)
Train: 253 [ 850/1251 ( 68%)]  Loss: 3.369 (3.51)  Time: 0.774s, 1323.56/s  (0.786s, 1302.05/s)  LR: 6.256e-04  Data: 0.009 (0.012)
Train: 253 [ 900/1251 ( 72%)]  Loss: 3.625 (3.51)  Time: 0.846s, 1210.14/s  (0.786s, 1302.23/s)  LR: 6.256e-04  Data: 0.011 (0.012)
Train: 253 [ 950/1251 ( 76%)]  Loss: 3.367 (3.50)  Time: 0.775s, 1321.22/s  (0.786s, 1302.59/s)  LR: 6.256e-04  Data: 0.010 (0.012)
Train: 253 [1000/1251 ( 80%)]  Loss: 3.249 (3.49)  Time: 0.778s, 1315.45/s  (0.786s, 1303.30/s)  LR: 6.256e-04  Data: 0.010 (0.012)
Train: 253 [1050/1251 ( 84%)]  Loss: 3.737 (3.50)  Time: 0.815s, 1256.34/s  (0.786s, 1303.34/s)  LR: 6.256e-04  Data: 0.010 (0.012)
Train: 253 [1100/1251 ( 88%)]  Loss: 3.791 (3.52)  Time: 0.772s, 1326.50/s  (0.785s, 1303.92/s)  LR: 6.256e-04  Data: 0.010 (0.012)
Train: 253 [1150/1251 ( 92%)]  Loss: 3.224 (3.50)  Time: 0.770s, 1329.58/s  (0.785s, 1304.05/s)  LR: 6.256e-04  Data: 0.009 (0.012)
Train: 253 [1200/1251 ( 96%)]  Loss: 3.300 (3.50)  Time: 0.773s, 1325.30/s  (0.785s, 1304.28/s)  LR: 6.256e-04  Data: 0.009 (0.012)
Train: 253 [1250/1251 (100%)]  Loss: 3.522 (3.50)  Time: 0.768s, 1333.33/s  (0.785s, 1304.72/s)  LR: 6.256e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.572 (1.572)  Loss:  0.8550 (0.8550)  Acc@1: 88.5742 (88.5742)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.193 (0.565)  Loss:  0.8843 (1.3545)  Acc@1: 84.3160 (74.4720)  Acc@5: 96.8160 (92.4120)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-250.pth.tar', 74.76799987792968)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-240.pth.tar', 74.6560000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-242.pth.tar', 74.5920001147461)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-249.pth.tar', 74.50800006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-248.pth.tar', 74.5080000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-253.pth.tar', 74.4720000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-232.pth.tar', 74.44600009521484)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-226.pth.tar', 74.40799992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-247.pth.tar', 74.39600006347656)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-252.pth.tar', 74.34799993652344)

Train: 254 [   0/1251 (  0%)]  Loss: 3.561 (3.56)  Time: 2.305s,  444.27/s  (2.305s,  444.27/s)  LR: 6.231e-04  Data: 1.555 (1.555)
Train: 254 [  50/1251 (  4%)]  Loss: 3.382 (3.47)  Time: 0.832s, 1230.99/s  (0.818s, 1251.88/s)  LR: 6.231e-04  Data: 0.010 (0.044)
Train: 254 [ 100/1251 (  8%)]  Loss: 3.889 (3.61)  Time: 0.774s, 1323.80/s  (0.799s, 1281.92/s)  LR: 6.231e-04  Data: 0.010 (0.027)
Train: 254 [ 150/1251 ( 12%)]  Loss: 3.675 (3.63)  Time: 0.772s, 1325.73/s  (0.796s, 1286.99/s)  LR: 6.231e-04  Data: 0.010 (0.022)
Train: 254 [ 200/1251 ( 16%)]  Loss: 3.365 (3.57)  Time: 0.776s, 1319.32/s  (0.792s, 1292.16/s)  LR: 6.231e-04  Data: 0.010 (0.019)
Train: 254 [ 250/1251 ( 20%)]  Loss: 3.836 (3.62)  Time: 0.814s, 1257.40/s  (0.793s, 1291.95/s)  LR: 6.231e-04  Data: 0.010 (0.017)
Train: 254 [ 300/1251 ( 24%)]  Loss: 3.517 (3.60)  Time: 0.773s, 1324.71/s  (0.792s, 1293.47/s)  LR: 6.231e-04  Data: 0.009 (0.016)
Train: 254 [ 350/1251 ( 28%)]  Loss: 3.161 (3.55)  Time: 0.772s, 1326.64/s  (0.791s, 1294.57/s)  LR: 6.231e-04  Data: 0.010 (0.015)
Train: 254 [ 400/1251 ( 32%)]  Loss: 3.792 (3.58)  Time: 0.772s, 1326.29/s  (0.790s, 1296.51/s)  LR: 6.231e-04  Data: 0.009 (0.014)
Train: 254 [ 450/1251 ( 36%)]  Loss: 3.419 (3.56)  Time: 0.775s, 1320.46/s  (0.790s, 1296.74/s)  LR: 6.231e-04  Data: 0.010 (0.014)
Train: 254 [ 500/1251 ( 40%)]  Loss: 3.278 (3.53)  Time: 0.784s, 1306.24/s  (0.789s, 1297.97/s)  LR: 6.231e-04  Data: 0.009 (0.014)
Train: 254 [ 550/1251 ( 44%)]  Loss: 3.134 (3.50)  Time: 0.772s, 1326.39/s  (0.789s, 1297.84/s)  LR: 6.231e-04  Data: 0.011 (0.013)
Train: 254 [ 600/1251 ( 48%)]  Loss: 3.536 (3.50)  Time: 0.803s, 1275.82/s  (0.789s, 1297.71/s)  LR: 6.231e-04  Data: 0.010 (0.013)
Train: 254 [ 650/1251 ( 52%)]  Loss: 3.510 (3.50)  Time: 0.774s, 1323.53/s  (0.789s, 1297.48/s)  LR: 6.231e-04  Data: 0.009 (0.013)
Train: 254 [ 700/1251 ( 56%)]  Loss: 3.339 (3.49)  Time: 0.773s, 1325.14/s  (0.788s, 1298.76/s)  LR: 6.231e-04  Data: 0.010 (0.013)
Train: 254 [ 750/1251 ( 60%)]  Loss: 3.482 (3.49)  Time: 0.778s, 1315.80/s  (0.788s, 1298.99/s)  LR: 6.231e-04  Data: 0.009 (0.012)
Train: 254 [ 800/1251 ( 64%)]  Loss: 3.767 (3.51)  Time: 0.833s, 1228.68/s  (0.788s, 1298.99/s)  LR: 6.231e-04  Data: 0.009 (0.012)
Train: 254 [ 850/1251 ( 68%)]  Loss: 3.468 (3.51)  Time: 0.773s, 1325.00/s  (0.788s, 1299.42/s)  LR: 6.231e-04  Data: 0.010 (0.012)
Train: 254 [ 900/1251 ( 72%)]  Loss: 3.714 (3.52)  Time: 0.810s, 1264.31/s  (0.788s, 1299.79/s)  LR: 6.231e-04  Data: 0.009 (0.012)
Train: 254 [ 950/1251 ( 76%)]  Loss: 3.510 (3.52)  Time: 0.773s, 1324.74/s  (0.788s, 1300.09/s)  LR: 6.231e-04  Data: 0.010 (0.012)
Train: 254 [1000/1251 ( 80%)]  Loss: 3.438 (3.51)  Time: 0.775s, 1320.78/s  (0.788s, 1300.09/s)  LR: 6.231e-04  Data: 0.009 (0.012)
Train: 254 [1050/1251 ( 84%)]  Loss: 3.650 (3.52)  Time: 0.781s, 1311.94/s  (0.787s, 1300.53/s)  LR: 6.231e-04  Data: 0.009 (0.012)
Train: 254 [1100/1251 ( 88%)]  Loss: 3.160 (3.50)  Time: 0.703s, 1455.79/s  (0.787s, 1300.92/s)  LR: 6.231e-04  Data: 0.010 (0.012)
Train: 254 [1150/1251 ( 92%)]  Loss: 3.713 (3.51)  Time: 0.773s, 1323.93/s  (0.787s, 1301.37/s)  LR: 6.231e-04  Data: 0.011 (0.012)
Train: 254 [1200/1251 ( 96%)]  Loss: 3.301 (3.50)  Time: 0.787s, 1300.94/s  (0.787s, 1301.95/s)  LR: 6.231e-04  Data: 0.014 (0.012)
Train: 254 [1250/1251 (100%)]  Loss: 3.151 (3.49)  Time: 0.792s, 1293.39/s  (0.786s, 1302.22/s)  LR: 6.231e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.520 (1.520)  Loss:  0.6748 (0.6748)  Acc@1: 90.9180 (90.9180)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.194 (0.560)  Loss:  0.8018 (1.2601)  Acc@1: 85.8491 (74.9640)  Acc@5: 96.6981 (92.8660)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-254.pth.tar', 74.96399992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-250.pth.tar', 74.76799987792968)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-240.pth.tar', 74.6560000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-242.pth.tar', 74.5920001147461)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-249.pth.tar', 74.50800006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-248.pth.tar', 74.5080000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-253.pth.tar', 74.4720000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-232.pth.tar', 74.44600009521484)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-226.pth.tar', 74.40799992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-247.pth.tar', 74.39600006347656)

Train: 255 [   0/1251 (  0%)]  Loss: 3.469 (3.47)  Time: 2.409s,  424.99/s  (2.409s,  424.99/s)  LR: 6.206e-04  Data: 1.675 (1.675)
Train: 255 [  50/1251 (  4%)]  Loss: 3.349 (3.41)  Time: 0.847s, 1208.75/s  (0.821s, 1246.74/s)  LR: 6.206e-04  Data: 0.010 (0.048)
Train: 255 [ 100/1251 (  8%)]  Loss: 3.837 (3.55)  Time: 0.828s, 1237.38/s  (0.810s, 1264.57/s)  LR: 6.206e-04  Data: 0.010 (0.029)
Train: 255 [ 150/1251 ( 12%)]  Loss: 3.842 (3.62)  Time: 0.782s, 1309.23/s  (0.799s, 1281.48/s)  LR: 6.206e-04  Data: 0.010 (0.023)
Train: 255 [ 200/1251 ( 16%)]  Loss: 3.480 (3.60)  Time: 0.776s, 1319.40/s  (0.795s, 1288.69/s)  LR: 6.206e-04  Data: 0.009 (0.020)
Train: 255 [ 250/1251 ( 20%)]  Loss: 3.552 (3.59)  Time: 0.776s, 1318.89/s  (0.792s, 1293.47/s)  LR: 6.206e-04  Data: 0.010 (0.018)
Train: 255 [ 300/1251 ( 24%)]  Loss: 3.217 (3.54)  Time: 0.786s, 1303.49/s  (0.789s, 1297.75/s)  LR: 6.206e-04  Data: 0.010 (0.016)
Train: 255 [ 350/1251 ( 28%)]  Loss: 3.447 (3.52)  Time: 0.782s, 1310.08/s  (0.788s, 1299.75/s)  LR: 6.206e-04  Data: 0.009 (0.016)
Train: 255 [ 400/1251 ( 32%)]  Loss: 3.539 (3.53)  Time: 0.776s, 1320.31/s  (0.787s, 1301.03/s)  LR: 6.206e-04  Data: 0.010 (0.015)
Train: 255 [ 450/1251 ( 36%)]  Loss: 3.340 (3.51)  Time: 0.857s, 1195.08/s  (0.787s, 1301.19/s)  LR: 6.206e-04  Data: 0.011 (0.014)
Train: 255 [ 500/1251 ( 40%)]  Loss: 3.223 (3.48)  Time: 0.774s, 1323.33/s  (0.786s, 1302.13/s)  LR: 6.206e-04  Data: 0.010 (0.014)
Train: 255 [ 550/1251 ( 44%)]  Loss: 3.537 (3.49)  Time: 0.812s, 1260.74/s  (0.786s, 1302.02/s)  LR: 6.206e-04  Data: 0.010 (0.014)
Train: 255 [ 600/1251 ( 48%)]  Loss: 3.118 (3.46)  Time: 0.790s, 1296.98/s  (0.787s, 1301.67/s)  LR: 6.206e-04  Data: 0.010 (0.013)
Train: 255 [ 650/1251 ( 52%)]  Loss: 3.486 (3.46)  Time: 0.773s, 1324.84/s  (0.786s, 1302.80/s)  LR: 6.206e-04  Data: 0.010 (0.013)
Train: 255 [ 700/1251 ( 56%)]  Loss: 3.192 (3.44)  Time: 0.772s, 1327.14/s  (0.786s, 1302.83/s)  LR: 6.206e-04  Data: 0.010 (0.013)
Train: 255 [ 750/1251 ( 60%)]  Loss: 3.445 (3.44)  Time: 0.774s, 1323.27/s  (0.786s, 1302.89/s)  LR: 6.206e-04  Data: 0.011 (0.013)
Train: 255 [ 800/1251 ( 64%)]  Loss: 3.644 (3.45)  Time: 0.771s, 1327.29/s  (0.787s, 1301.90/s)  LR: 6.206e-04  Data: 0.010 (0.012)
Train: 255 [ 850/1251 ( 68%)]  Loss: 3.430 (3.45)  Time: 0.778s, 1315.66/s  (0.787s, 1301.18/s)  LR: 6.206e-04  Data: 0.010 (0.012)
Train: 255 [ 900/1251 ( 72%)]  Loss: 3.463 (3.45)  Time: 0.783s, 1307.20/s  (0.787s, 1301.54/s)  LR: 6.206e-04  Data: 0.010 (0.012)
Train: 255 [ 950/1251 ( 76%)]  Loss: 3.670 (3.46)  Time: 0.772s, 1326.45/s  (0.788s, 1299.97/s)  LR: 6.206e-04  Data: 0.010 (0.012)
Train: 255 [1000/1251 ( 80%)]  Loss: 3.623 (3.47)  Time: 0.774s, 1323.35/s  (0.787s, 1300.46/s)  LR: 6.206e-04  Data: 0.010 (0.012)
Train: 255 [1050/1251 ( 84%)]  Loss: 3.248 (3.46)  Time: 0.776s, 1320.15/s  (0.787s, 1300.88/s)  LR: 6.206e-04  Data: 0.009 (0.012)
Train: 255 [1100/1251 ( 88%)]  Loss: 3.486 (3.46)  Time: 0.773s, 1325.51/s  (0.787s, 1301.31/s)  LR: 6.206e-04  Data: 0.010 (0.012)
Train: 255 [1150/1251 ( 92%)]  Loss: 3.212 (3.45)  Time: 0.776s, 1319.51/s  (0.787s, 1301.28/s)  LR: 6.206e-04  Data: 0.010 (0.012)
Train: 255 [1200/1251 ( 96%)]  Loss: 3.743 (3.46)  Time: 0.774s, 1322.98/s  (0.787s, 1301.05/s)  LR: 6.206e-04  Data: 0.010 (0.012)
Train: 255 [1250/1251 (100%)]  Loss: 3.428 (3.46)  Time: 0.762s, 1344.71/s  (0.787s, 1301.86/s)  LR: 6.206e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.511 (1.511)  Loss:  0.7549 (0.7549)  Acc@1: 89.0625 (89.0625)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.194 (0.574)  Loss:  0.9292 (1.3454)  Acc@1: 85.1415 (74.6120)  Acc@5: 96.5802 (92.6100)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-254.pth.tar', 74.96399992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-250.pth.tar', 74.76799987792968)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-240.pth.tar', 74.6560000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-255.pth.tar', 74.61200000976562)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-242.pth.tar', 74.5920001147461)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-249.pth.tar', 74.50800006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-248.pth.tar', 74.5080000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-253.pth.tar', 74.4720000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-232.pth.tar', 74.44600009521484)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-226.pth.tar', 74.40799992919922)

Train: 256 [   0/1251 (  0%)]  Loss: 3.307 (3.31)  Time: 2.207s,  463.89/s  (2.207s,  463.89/s)  LR: 6.180e-04  Data: 1.481 (1.481)
Train: 256 [  50/1251 (  4%)]  Loss: 3.398 (3.35)  Time: 0.772s, 1325.65/s  (0.813s, 1258.94/s)  LR: 6.180e-04  Data: 0.010 (0.042)
Train: 256 [ 100/1251 (  8%)]  Loss: 3.404 (3.37)  Time: 0.777s, 1317.35/s  (0.803s, 1274.94/s)  LR: 6.180e-04  Data: 0.010 (0.026)
Train: 256 [ 150/1251 ( 12%)]  Loss: 3.237 (3.34)  Time: 0.832s, 1231.04/s  (0.799s, 1281.22/s)  LR: 6.180e-04  Data: 0.010 (0.021)
Train: 256 [ 200/1251 ( 16%)]  Loss: 3.718 (3.41)  Time: 0.777s, 1317.30/s  (0.796s, 1286.88/s)  LR: 6.180e-04  Data: 0.010 (0.018)
Train: 256 [ 250/1251 ( 20%)]  Loss: 3.303 (3.39)  Time: 0.805s, 1272.49/s  (0.793s, 1291.04/s)  LR: 6.180e-04  Data: 0.010 (0.017)
Train: 256 [ 300/1251 ( 24%)]  Loss: 3.326 (3.38)  Time: 0.781s, 1311.17/s  (0.795s, 1287.83/s)  LR: 6.180e-04  Data: 0.012 (0.016)
Train: 256 [ 350/1251 ( 28%)]  Loss: 3.727 (3.43)  Time: 0.778s, 1316.84/s  (0.794s, 1290.14/s)  LR: 6.180e-04  Data: 0.010 (0.015)
Train: 256 [ 400/1251 ( 32%)]  Loss: 3.592 (3.45)  Time: 0.781s, 1311.71/s  (0.792s, 1292.99/s)  LR: 6.180e-04  Data: 0.010 (0.014)
Train: 256 [ 450/1251 ( 36%)]  Loss: 3.668 (3.47)  Time: 0.779s, 1314.79/s  (0.790s, 1295.58/s)  LR: 6.180e-04  Data: 0.010 (0.014)
Train: 256 [ 500/1251 ( 40%)]  Loss: 3.490 (3.47)  Time: 0.772s, 1326.47/s  (0.789s, 1297.56/s)  LR: 6.180e-04  Data: 0.010 (0.013)
Train: 256 [ 550/1251 ( 44%)]  Loss: 3.757 (3.49)  Time: 0.771s, 1327.96/s  (0.790s, 1297.02/s)  LR: 6.180e-04  Data: 0.009 (0.013)
Train: 256 [ 600/1251 ( 48%)]  Loss: 3.831 (3.52)  Time: 0.788s, 1299.49/s  (0.789s, 1298.51/s)  LR: 6.180e-04  Data: 0.010 (0.013)
Train: 256 [ 650/1251 ( 52%)]  Loss: 3.623 (3.53)  Time: 0.775s, 1321.41/s  (0.788s, 1298.89/s)  LR: 6.180e-04  Data: 0.011 (0.013)
Train: 256 [ 700/1251 ( 56%)]  Loss: 3.786 (3.54)  Time: 0.771s, 1328.95/s  (0.788s, 1299.18/s)  LR: 6.180e-04  Data: 0.009 (0.013)
Train: 256 [ 750/1251 ( 60%)]  Loss: 3.537 (3.54)  Time: 0.776s, 1319.59/s  (0.788s, 1300.14/s)  LR: 6.180e-04  Data: 0.010 (0.012)
Train: 256 [ 800/1251 ( 64%)]  Loss: 3.095 (3.52)  Time: 0.773s, 1324.10/s  (0.787s, 1301.21/s)  LR: 6.180e-04  Data: 0.010 (0.012)
Train: 256 [ 850/1251 ( 68%)]  Loss: 3.733 (3.53)  Time: 0.771s, 1327.56/s  (0.787s, 1301.40/s)  LR: 6.180e-04  Data: 0.010 (0.012)
Train: 256 [ 900/1251 ( 72%)]  Loss: 3.344 (3.52)  Time: 0.774s, 1323.11/s  (0.787s, 1301.89/s)  LR: 6.180e-04  Data: 0.009 (0.012)
Train: 256 [ 950/1251 ( 76%)]  Loss: 3.069 (3.50)  Time: 0.774s, 1323.19/s  (0.787s, 1301.55/s)  LR: 6.180e-04  Data: 0.009 (0.012)
Train: 256 [1000/1251 ( 80%)]  Loss: 3.737 (3.51)  Time: 0.778s, 1317.00/s  (0.787s, 1301.90/s)  LR: 6.180e-04  Data: 0.010 (0.012)
Train: 256 [1050/1251 ( 84%)]  Loss: 3.733 (3.52)  Time: 0.775s, 1321.68/s  (0.786s, 1302.25/s)  LR: 6.180e-04  Data: 0.010 (0.012)
Train: 256 [1100/1251 ( 88%)]  Loss: 3.446 (3.52)  Time: 0.783s, 1308.47/s  (0.786s, 1302.52/s)  LR: 6.180e-04  Data: 0.012 (0.012)
Train: 256 [1150/1251 ( 92%)]  Loss: 3.493 (3.51)  Time: 0.768s, 1333.90/s  (0.786s, 1301.99/s)  LR: 6.180e-04  Data: 0.009 (0.012)
Train: 256 [1200/1251 ( 96%)]  Loss: 3.315 (3.51)  Time: 0.773s, 1325.45/s  (0.786s, 1302.50/s)  LR: 6.180e-04  Data: 0.010 (0.012)
Train: 256 [1250/1251 (100%)]  Loss: 3.749 (3.52)  Time: 0.760s, 1348.13/s  (0.787s, 1301.03/s)  LR: 6.180e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.542 (1.542)  Loss:  0.8145 (0.8145)  Acc@1: 88.6719 (88.6719)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.193 (0.567)  Loss:  0.8179 (1.2559)  Acc@1: 85.1415 (74.6160)  Acc@5: 96.5802 (92.5320)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-254.pth.tar', 74.96399992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-250.pth.tar', 74.76799987792968)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-240.pth.tar', 74.6560000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-256.pth.tar', 74.61600000976563)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-255.pth.tar', 74.61200000976562)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-242.pth.tar', 74.5920001147461)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-249.pth.tar', 74.50800006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-248.pth.tar', 74.5080000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-253.pth.tar', 74.4720000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-232.pth.tar', 74.44600009521484)

Train: 257 [   0/1251 (  0%)]  Loss: 3.090 (3.09)  Time: 2.283s,  448.47/s  (2.283s,  448.47/s)  LR: 6.155e-04  Data: 1.557 (1.557)
Train: 257 [  50/1251 (  4%)]  Loss: 3.520 (3.30)  Time: 0.778s, 1316.26/s  (0.825s, 1241.12/s)  LR: 6.155e-04  Data: 0.011 (0.045)
Train: 257 [ 100/1251 (  8%)]  Loss: 3.100 (3.24)  Time: 0.774s, 1322.75/s  (0.805s, 1272.35/s)  LR: 6.155e-04  Data: 0.010 (0.028)
Train: 257 [ 150/1251 ( 12%)]  Loss: 3.655 (3.34)  Time: 0.772s, 1326.86/s  (0.797s, 1284.36/s)  LR: 6.155e-04  Data: 0.009 (0.022)
Train: 257 [ 200/1251 ( 16%)]  Loss: 3.270 (3.33)  Time: 0.772s, 1327.26/s  (0.793s, 1290.88/s)  LR: 6.155e-04  Data: 0.009 (0.019)
Train: 257 [ 250/1251 ( 20%)]  Loss: 3.764 (3.40)  Time: 0.781s, 1311.63/s  (0.792s, 1293.27/s)  LR: 6.155e-04  Data: 0.010 (0.017)
Train: 257 [ 300/1251 ( 24%)]  Loss: 3.513 (3.42)  Time: 0.857s, 1194.30/s  (0.791s, 1293.88/s)  LR: 6.155e-04  Data: 0.011 (0.016)
Train: 257 [ 350/1251 ( 28%)]  Loss: 3.518 (3.43)  Time: 0.774s, 1323.40/s  (0.792s, 1293.04/s)  LR: 6.155e-04  Data: 0.010 (0.015)
Train: 257 [ 400/1251 ( 32%)]  Loss: 3.275 (3.41)  Time: 0.774s, 1322.44/s  (0.791s, 1295.19/s)  LR: 6.155e-04  Data: 0.009 (0.015)
Train: 257 [ 450/1251 ( 36%)]  Loss: 3.557 (3.43)  Time: 0.783s, 1308.60/s  (0.790s, 1295.66/s)  LR: 6.155e-04  Data: 0.009 (0.014)
Train: 257 [ 500/1251 ( 40%)]  Loss: 3.442 (3.43)  Time: 0.774s, 1323.85/s  (0.789s, 1297.32/s)  LR: 6.155e-04  Data: 0.009 (0.014)
Train: 257 [ 550/1251 ( 44%)]  Loss: 3.489 (3.43)  Time: 0.775s, 1321.28/s  (0.789s, 1298.55/s)  LR: 6.155e-04  Data: 0.009 (0.013)
Train: 257 [ 600/1251 ( 48%)]  Loss: 3.727 (3.46)  Time: 0.807s, 1269.05/s  (0.788s, 1299.24/s)  LR: 6.155e-04  Data: 0.010 (0.013)
Train: 257 [ 650/1251 ( 52%)]  Loss: 3.679 (3.47)  Time: 0.775s, 1320.66/s  (0.788s, 1298.72/s)  LR: 6.155e-04  Data: 0.009 (0.013)
Train: 257 [ 700/1251 ( 56%)]  Loss: 3.661 (3.48)  Time: 0.808s, 1267.68/s  (0.788s, 1298.96/s)  LR: 6.155e-04  Data: 0.010 (0.012)
Train: 257 [ 750/1251 ( 60%)]  Loss: 3.952 (3.51)  Time: 0.774s, 1322.77/s  (0.788s, 1299.68/s)  LR: 6.155e-04  Data: 0.010 (0.012)
Train: 257 [ 800/1251 ( 64%)]  Loss: 3.445 (3.51)  Time: 0.788s, 1299.97/s  (0.787s, 1300.33/s)  LR: 6.155e-04  Data: 0.009 (0.012)
Train: 257 [ 850/1251 ( 68%)]  Loss: 3.221 (3.49)  Time: 0.839s, 1220.31/s  (0.787s, 1300.62/s)  LR: 6.155e-04  Data: 0.010 (0.012)
Train: 257 [ 900/1251 ( 72%)]  Loss: 3.688 (3.50)  Time: 0.858s, 1193.95/s  (0.787s, 1301.08/s)  LR: 6.155e-04  Data: 0.012 (0.012)
Train: 257 [ 950/1251 ( 76%)]  Loss: 3.329 (3.49)  Time: 0.775s, 1320.95/s  (0.787s, 1301.36/s)  LR: 6.155e-04  Data: 0.010 (0.012)
Train: 257 [1000/1251 ( 80%)]  Loss: 3.556 (3.50)  Time: 0.775s, 1321.34/s  (0.787s, 1300.74/s)  LR: 6.155e-04  Data: 0.010 (0.012)
Train: 257 [1050/1251 ( 84%)]  Loss: 3.307 (3.49)  Time: 0.773s, 1323.92/s  (0.787s, 1301.23/s)  LR: 6.155e-04  Data: 0.010 (0.012)
Train: 257 [1100/1251 ( 88%)]  Loss: 3.668 (3.50)  Time: 0.770s, 1329.04/s  (0.787s, 1301.41/s)  LR: 6.155e-04  Data: 0.009 (0.012)
Train: 257 [1150/1251 ( 92%)]  Loss: 3.776 (3.51)  Time: 0.775s, 1320.63/s  (0.788s, 1300.14/s)  LR: 6.155e-04  Data: 0.011 (0.012)
Train: 257 [1200/1251 ( 96%)]  Loss: 3.270 (3.50)  Time: 0.815s, 1256.50/s  (0.789s, 1298.29/s)  LR: 6.155e-04  Data: 0.011 (0.012)
Train: 257 [1250/1251 (100%)]  Loss: 3.742 (3.51)  Time: 0.759s, 1348.97/s  (0.789s, 1297.22/s)  LR: 6.155e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.567 (1.567)  Loss:  0.7144 (0.7144)  Acc@1: 90.3320 (90.3320)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.194 (0.569)  Loss:  0.7944 (1.2643)  Acc@1: 86.4387 (74.6040)  Acc@5: 95.8726 (92.4200)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-254.pth.tar', 74.96399992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-250.pth.tar', 74.76799987792968)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-240.pth.tar', 74.6560000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-256.pth.tar', 74.61600000976563)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-255.pth.tar', 74.61200000976562)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-257.pth.tar', 74.60399992675781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-242.pth.tar', 74.5920001147461)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-249.pth.tar', 74.50800006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-248.pth.tar', 74.5080000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-253.pth.tar', 74.4720000390625)

Train: 258 [   0/1251 (  0%)]  Loss: 3.518 (3.52)  Time: 2.313s,  442.65/s  (2.313s,  442.65/s)  LR: 6.130e-04  Data: 1.587 (1.587)
Train: 258 [  50/1251 (  4%)]  Loss: 3.469 (3.49)  Time: 0.780s, 1312.78/s  (0.811s, 1262.35/s)  LR: 6.130e-04  Data: 0.010 (0.043)
Train: 258 [ 100/1251 (  8%)]  Loss: 3.796 (3.59)  Time: 0.851s, 1203.71/s  (0.802s, 1276.62/s)  LR: 6.130e-04  Data: 0.009 (0.027)
Train: 258 [ 150/1251 ( 12%)]  Loss: 3.662 (3.61)  Time: 0.774s, 1323.58/s  (0.807s, 1269.20/s)  LR: 6.130e-04  Data: 0.010 (0.021)
Train: 258 [ 200/1251 ( 16%)]  Loss: 3.678 (3.62)  Time: 0.783s, 1307.14/s  (0.801s, 1279.18/s)  LR: 6.130e-04  Data: 0.009 (0.018)
Train: 258 [ 250/1251 ( 20%)]  Loss: 3.503 (3.60)  Time: 0.816s, 1254.86/s  (0.798s, 1282.78/s)  LR: 6.130e-04  Data: 0.010 (0.016)
Train: 258 [ 300/1251 ( 24%)]  Loss: 3.840 (3.64)  Time: 0.772s, 1326.04/s  (0.797s, 1285.62/s)  LR: 6.130e-04  Data: 0.010 (0.015)
Train: 258 [ 350/1251 ( 28%)]  Loss: 3.560 (3.63)  Time: 0.773s, 1324.90/s  (0.794s, 1289.33/s)  LR: 6.130e-04  Data: 0.010 (0.015)
Train: 258 [ 400/1251 ( 32%)]  Loss: 3.449 (3.61)  Time: 0.773s, 1325.05/s  (0.793s, 1291.02/s)  LR: 6.130e-04  Data: 0.011 (0.014)
Train: 258 [ 450/1251 ( 36%)]  Loss: 3.334 (3.58)  Time: 0.807s, 1269.23/s  (0.794s, 1290.27/s)  LR: 6.130e-04  Data: 0.009 (0.014)
Train: 258 [ 500/1251 ( 40%)]  Loss: 3.679 (3.59)  Time: 0.777s, 1318.46/s  (0.793s, 1290.92/s)  LR: 6.130e-04  Data: 0.010 (0.013)
Train: 258 [ 550/1251 ( 44%)]  Loss: 3.365 (3.57)  Time: 0.773s, 1325.04/s  (0.792s, 1292.66/s)  LR: 6.130e-04  Data: 0.009 (0.013)
Train: 258 [ 600/1251 ( 48%)]  Loss: 3.335 (3.55)  Time: 0.772s, 1326.66/s  (0.791s, 1294.30/s)  LR: 6.130e-04  Data: 0.009 (0.013)
Train: 258 [ 650/1251 ( 52%)]  Loss: 3.805 (3.57)  Time: 0.773s, 1325.27/s  (0.790s, 1295.54/s)  LR: 6.130e-04  Data: 0.010 (0.012)
Train: 258 [ 700/1251 ( 56%)]  Loss: 3.495 (3.57)  Time: 0.773s, 1324.23/s  (0.789s, 1297.23/s)  LR: 6.130e-04  Data: 0.010 (0.012)
Train: 258 [ 750/1251 ( 60%)]  Loss: 3.493 (3.56)  Time: 0.773s, 1325.08/s  (0.789s, 1298.29/s)  LR: 6.130e-04  Data: 0.009 (0.012)
Train: 258 [ 800/1251 ( 64%)]  Loss: 3.438 (3.55)  Time: 0.822s, 1245.10/s  (0.789s, 1298.18/s)  LR: 6.130e-04  Data: 0.009 (0.012)
Train: 258 [ 850/1251 ( 68%)]  Loss: 3.647 (3.56)  Time: 0.774s, 1323.76/s  (0.790s, 1296.18/s)  LR: 6.130e-04  Data: 0.010 (0.012)
Train: 258 [ 900/1251 ( 72%)]  Loss: 3.367 (3.55)  Time: 0.773s, 1324.12/s  (0.790s, 1296.64/s)  LR: 6.130e-04  Data: 0.009 (0.012)
Train: 258 [ 950/1251 ( 76%)]  Loss: 3.752 (3.56)  Time: 0.774s, 1322.85/s  (0.789s, 1297.62/s)  LR: 6.130e-04  Data: 0.010 (0.011)
Train: 258 [1000/1251 ( 80%)]  Loss: 3.049 (3.53)  Time: 0.772s, 1326.88/s  (0.789s, 1298.48/s)  LR: 6.130e-04  Data: 0.009 (0.011)
Train: 258 [1050/1251 ( 84%)]  Loss: 3.579 (3.54)  Time: 0.774s, 1322.18/s  (0.788s, 1299.46/s)  LR: 6.130e-04  Data: 0.009 (0.011)
Train: 258 [1100/1251 ( 88%)]  Loss: 3.722 (3.55)  Time: 0.774s, 1322.47/s  (0.788s, 1300.27/s)  LR: 6.130e-04  Data: 0.009 (0.011)
Train: 258 [1150/1251 ( 92%)]  Loss: 3.723 (3.55)  Time: 0.773s, 1324.66/s  (0.787s, 1301.02/s)  LR: 6.130e-04  Data: 0.010 (0.011)
Train: 258 [1200/1251 ( 96%)]  Loss: 3.232 (3.54)  Time: 0.815s, 1256.86/s  (0.788s, 1299.61/s)  LR: 6.130e-04  Data: 0.010 (0.011)
Train: 258 [1250/1251 (100%)]  Loss: 3.724 (3.55)  Time: 0.797s, 1284.39/s  (0.788s, 1299.03/s)  LR: 6.130e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.573 (1.573)  Loss:  0.9111 (0.9111)  Acc@1: 87.8906 (87.8906)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.195 (0.569)  Loss:  1.0283 (1.4167)  Acc@1: 83.7264 (74.3900)  Acc@5: 96.1085 (92.5060)
Train: 259 [   0/1251 (  0%)]  Loss: 3.548 (3.55)  Time: 2.327s,  439.98/s  (2.327s,  439.98/s)  LR: 6.105e-04  Data: 1.601 (1.601)
Train: 259 [  50/1251 (  4%)]  Loss: 3.775 (3.66)  Time: 0.811s, 1262.55/s  (0.824s, 1242.28/s)  LR: 6.105e-04  Data: 0.011 (0.041)
Train: 259 [ 100/1251 (  8%)]  Loss: 3.622 (3.65)  Time: 0.773s, 1324.37/s  (0.806s, 1270.20/s)  LR: 6.105e-04  Data: 0.010 (0.026)
Train: 259 [ 150/1251 ( 12%)]  Loss: 3.545 (3.62)  Time: 0.772s, 1325.73/s  (0.797s, 1285.15/s)  LR: 6.105e-04  Data: 0.009 (0.020)
Train: 259 [ 200/1251 ( 16%)]  Loss: 3.499 (3.60)  Time: 0.774s, 1323.55/s  (0.794s, 1288.95/s)  LR: 6.105e-04  Data: 0.009 (0.018)
Train: 259 [ 250/1251 ( 20%)]  Loss: 3.819 (3.63)  Time: 0.774s, 1323.74/s  (0.791s, 1294.81/s)  LR: 6.105e-04  Data: 0.010 (0.016)
Train: 259 [ 300/1251 ( 24%)]  Loss: 3.608 (3.63)  Time: 0.772s, 1326.33/s  (0.794s, 1289.69/s)  LR: 6.105e-04  Data: 0.009 (0.015)
Train: 259 [ 350/1251 ( 28%)]  Loss: 3.541 (3.62)  Time: 0.823s, 1244.27/s  (0.793s, 1292.08/s)  LR: 6.105e-04  Data: 0.009 (0.014)
Train: 259 [ 400/1251 ( 32%)]  Loss: 3.475 (3.60)  Time: 0.773s, 1325.46/s  (0.792s, 1293.09/s)  LR: 6.105e-04  Data: 0.010 (0.014)
Train: 259 [ 450/1251 ( 36%)]  Loss: 3.270 (3.57)  Time: 0.773s, 1324.06/s  (0.793s, 1291.92/s)  LR: 6.105e-04  Data: 0.010 (0.013)
Train: 259 [ 500/1251 ( 40%)]  Loss: 3.678 (3.58)  Time: 0.780s, 1313.11/s  (0.791s, 1294.14/s)  LR: 6.105e-04  Data: 0.009 (0.013)
Train: 259 [ 550/1251 ( 44%)]  Loss: 3.711 (3.59)  Time: 0.775s, 1320.53/s  (0.790s, 1295.46/s)  LR: 6.105e-04  Data: 0.010 (0.013)
Train: 259 [ 600/1251 ( 48%)]  Loss: 3.388 (3.58)  Time: 0.778s, 1315.36/s  (0.789s, 1297.42/s)  LR: 6.105e-04  Data: 0.009 (0.012)
Train: 259 [ 650/1251 ( 52%)]  Loss: 3.678 (3.58)  Time: 0.805s, 1272.66/s  (0.789s, 1298.09/s)  LR: 6.105e-04  Data: 0.010 (0.012)
Train: 259 [ 700/1251 ( 56%)]  Loss: 3.591 (3.58)  Time: 0.784s, 1305.78/s  (0.789s, 1297.28/s)  LR: 6.105e-04  Data: 0.011 (0.012)
Train: 259 [ 750/1251 ( 60%)]  Loss: 3.852 (3.60)  Time: 0.778s, 1316.82/s  (0.790s, 1296.65/s)  LR: 6.105e-04  Data: 0.010 (0.012)
Train: 259 [ 800/1251 ( 64%)]  Loss: 3.625 (3.60)  Time: 0.773s, 1324.22/s  (0.789s, 1297.93/s)  LR: 6.105e-04  Data: 0.009 (0.012)
Train: 259 [ 850/1251 ( 68%)]  Loss: 3.429 (3.59)  Time: 0.774s, 1322.21/s  (0.788s, 1299.13/s)  LR: 6.105e-04  Data: 0.010 (0.012)
Train: 259 [ 900/1251 ( 72%)]  Loss: 3.692 (3.60)  Time: 0.774s, 1322.19/s  (0.788s, 1298.75/s)  LR: 6.105e-04  Data: 0.010 (0.012)
Train: 259 [ 950/1251 ( 76%)]  Loss: 3.634 (3.60)  Time: 0.773s, 1324.87/s  (0.788s, 1299.06/s)  LR: 6.105e-04  Data: 0.009 (0.011)
Train: 259 [1000/1251 ( 80%)]  Loss: 3.731 (3.61)  Time: 0.779s, 1314.68/s  (0.788s, 1299.95/s)  LR: 6.105e-04  Data: 0.009 (0.011)
Train: 259 [1050/1251 ( 84%)]  Loss: 3.309 (3.59)  Time: 0.893s, 1147.19/s  (0.787s, 1300.47/s)  LR: 6.105e-04  Data: 0.009 (0.011)
Train: 259 [1100/1251 ( 88%)]  Loss: 3.189 (3.57)  Time: 0.771s, 1327.46/s  (0.787s, 1300.52/s)  LR: 6.105e-04  Data: 0.009 (0.011)
Train: 259 [1150/1251 ( 92%)]  Loss: 3.649 (3.58)  Time: 0.773s, 1325.00/s  (0.787s, 1301.02/s)  LR: 6.105e-04  Data: 0.009 (0.011)
Train: 259 [1200/1251 ( 96%)]  Loss: 3.616 (3.58)  Time: 0.814s, 1257.93/s  (0.788s, 1300.00/s)  LR: 6.105e-04  Data: 0.011 (0.011)
Train: 259 [1250/1251 (100%)]  Loss: 3.625 (3.58)  Time: 0.803s, 1275.25/s  (0.789s, 1298.25/s)  LR: 6.105e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.612 (1.612)  Loss:  0.7861 (0.7861)  Acc@1: 88.5742 (88.5742)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.194 (0.570)  Loss:  0.8403 (1.3305)  Acc@1: 85.6132 (74.5880)  Acc@5: 96.9340 (92.5060)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-254.pth.tar', 74.96399992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-250.pth.tar', 74.76799987792968)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-240.pth.tar', 74.6560000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-256.pth.tar', 74.61600000976563)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-255.pth.tar', 74.61200000976562)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-257.pth.tar', 74.60399992675781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-242.pth.tar', 74.5920001147461)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-259.pth.tar', 74.58800008544922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-249.pth.tar', 74.50800006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-248.pth.tar', 74.5080000390625)

Train: 260 [   0/1251 (  0%)]  Loss: 3.505 (3.51)  Time: 2.193s,  467.04/s  (2.193s,  467.04/s)  LR: 6.079e-04  Data: 1.463 (1.463)
Train: 260 [  50/1251 (  4%)]  Loss: 3.534 (3.52)  Time: 0.778s, 1315.97/s  (0.810s, 1263.82/s)  LR: 6.079e-04  Data: 0.010 (0.043)
Train: 260 [ 100/1251 (  8%)]  Loss: 3.588 (3.54)  Time: 0.773s, 1324.15/s  (0.797s, 1284.10/s)  LR: 6.079e-04  Data: 0.010 (0.027)
Train: 260 [ 150/1251 ( 12%)]  Loss: 3.016 (3.41)  Time: 0.773s, 1324.20/s  (0.795s, 1288.46/s)  LR: 6.079e-04  Data: 0.010 (0.021)
Train: 260 [ 200/1251 ( 16%)]  Loss: 3.585 (3.45)  Time: 0.782s, 1309.41/s  (0.792s, 1292.27/s)  LR: 6.079e-04  Data: 0.012 (0.018)
Train: 260 [ 250/1251 ( 20%)]  Loss: 3.861 (3.51)  Time: 0.781s, 1311.03/s  (0.790s, 1296.20/s)  LR: 6.079e-04  Data: 0.011 (0.017)
Train: 260 [ 300/1251 ( 24%)]  Loss: 3.513 (3.51)  Time: 0.776s, 1320.08/s  (0.788s, 1299.97/s)  LR: 6.079e-04  Data: 0.009 (0.015)
Train: 260 [ 350/1251 ( 28%)]  Loss: 3.792 (3.55)  Time: 0.774s, 1322.34/s  (0.787s, 1301.66/s)  LR: 6.079e-04  Data: 0.010 (0.015)
Train: 260 [ 400/1251 ( 32%)]  Loss: 3.716 (3.57)  Time: 0.786s, 1302.60/s  (0.786s, 1303.39/s)  LR: 6.079e-04  Data: 0.009 (0.014)
Train: 260 [ 450/1251 ( 36%)]  Loss: 3.772 (3.59)  Time: 0.775s, 1320.93/s  (0.785s, 1305.00/s)  LR: 6.079e-04  Data: 0.009 (0.014)
Train: 260 [ 500/1251 ( 40%)]  Loss: 3.334 (3.57)  Time: 0.773s, 1324.32/s  (0.784s, 1306.23/s)  LR: 6.079e-04  Data: 0.011 (0.013)
Train: 260 [ 550/1251 ( 44%)]  Loss: 3.505 (3.56)  Time: 0.780s, 1312.89/s  (0.784s, 1306.27/s)  LR: 6.079e-04  Data: 0.011 (0.013)
Train: 260 [ 600/1251 ( 48%)]  Loss: 3.722 (3.57)  Time: 0.781s, 1311.45/s  (0.784s, 1306.67/s)  LR: 6.079e-04  Data: 0.010 (0.013)
Train: 260 [ 650/1251 ( 52%)]  Loss: 3.776 (3.59)  Time: 0.773s, 1324.66/s  (0.784s, 1306.00/s)  LR: 6.079e-04  Data: 0.010 (0.012)
Train: 260 [ 700/1251 ( 56%)]  Loss: 3.763 (3.60)  Time: 0.784s, 1305.36/s  (0.784s, 1306.81/s)  LR: 6.079e-04  Data: 0.009 (0.012)
Train: 260 [ 750/1251 ( 60%)]  Loss: 3.399 (3.59)  Time: 0.777s, 1318.56/s  (0.784s, 1306.83/s)  LR: 6.079e-04  Data: 0.010 (0.012)
Train: 260 [ 800/1251 ( 64%)]  Loss: 3.650 (3.59)  Time: 0.773s, 1324.72/s  (0.784s, 1306.48/s)  LR: 6.079e-04  Data: 0.011 (0.012)
Train: 260 [ 850/1251 ( 68%)]  Loss: 3.568 (3.59)  Time: 0.775s, 1321.36/s  (0.783s, 1306.97/s)  LR: 6.079e-04  Data: 0.009 (0.012)
Train: 260 [ 900/1251 ( 72%)]  Loss: 3.259 (3.57)  Time: 0.806s, 1269.86/s  (0.784s, 1306.42/s)  LR: 6.079e-04  Data: 0.009 (0.012)
Train: 260 [ 950/1251 ( 76%)]  Loss: 3.148 (3.55)  Time: 0.772s, 1325.74/s  (0.784s, 1305.85/s)  LR: 6.079e-04  Data: 0.009 (0.012)
Train: 260 [1000/1251 ( 80%)]  Loss: 3.750 (3.56)  Time: 0.775s, 1321.10/s  (0.784s, 1306.30/s)  LR: 6.079e-04  Data: 0.009 (0.011)
Train: 260 [1050/1251 ( 84%)]  Loss: 3.322 (3.55)  Time: 0.775s, 1321.01/s  (0.784s, 1306.66/s)  LR: 6.079e-04  Data: 0.010 (0.011)
Train: 260 [1100/1251 ( 88%)]  Loss: 3.335 (3.54)  Time: 0.776s, 1320.37/s  (0.783s, 1307.22/s)  LR: 6.079e-04  Data: 0.010 (0.011)
Train: 260 [1150/1251 ( 92%)]  Loss: 3.647 (3.54)  Time: 0.779s, 1314.02/s  (0.783s, 1307.60/s)  LR: 6.079e-04  Data: 0.013 (0.011)
Train: 260 [1200/1251 ( 96%)]  Loss: 3.681 (3.55)  Time: 0.776s, 1320.16/s  (0.783s, 1307.83/s)  LR: 6.079e-04  Data: 0.010 (0.011)
Train: 260 [1250/1251 (100%)]  Loss: 3.283 (3.54)  Time: 0.766s, 1336.53/s  (0.783s, 1308.18/s)  LR: 6.079e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.525 (1.525)  Loss:  0.6880 (0.6880)  Acc@1: 89.7461 (89.7461)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.193 (0.561)  Loss:  0.7676 (1.2284)  Acc@1: 85.0236 (75.1040)  Acc@5: 96.4623 (92.7180)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-260.pth.tar', 75.10400008789063)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-254.pth.tar', 74.96399992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-250.pth.tar', 74.76799987792968)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-240.pth.tar', 74.6560000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-256.pth.tar', 74.61600000976563)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-255.pth.tar', 74.61200000976562)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-257.pth.tar', 74.60399992675781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-242.pth.tar', 74.5920001147461)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-259.pth.tar', 74.58800008544922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-249.pth.tar', 74.50800006591797)

Train: 261 [   0/1251 (  0%)]  Loss: 3.030 (3.03)  Time: 2.229s,  459.42/s  (2.229s,  459.42/s)  LR: 6.054e-04  Data: 1.501 (1.501)
Train: 261 [  50/1251 (  4%)]  Loss: 3.642 (3.34)  Time: 0.809s, 1265.18/s  (0.824s, 1242.98/s)  LR: 6.054e-04  Data: 0.009 (0.047)
Train: 261 [ 100/1251 (  8%)]  Loss: 3.470 (3.38)  Time: 0.806s, 1269.91/s  (0.811s, 1262.02/s)  LR: 6.054e-04  Data: 0.009 (0.029)
Train: 261 [ 150/1251 ( 12%)]  Loss: 3.182 (3.33)  Time: 0.771s, 1328.42/s  (0.803s, 1275.99/s)  LR: 6.054e-04  Data: 0.010 (0.022)
Train: 261 [ 200/1251 ( 16%)]  Loss: 3.428 (3.35)  Time: 0.779s, 1314.24/s  (0.797s, 1285.36/s)  LR: 6.054e-04  Data: 0.009 (0.019)
Train: 261 [ 250/1251 ( 20%)]  Loss: 3.632 (3.40)  Time: 0.772s, 1325.79/s  (0.794s, 1290.15/s)  LR: 6.054e-04  Data: 0.010 (0.017)
Train: 261 [ 300/1251 ( 24%)]  Loss: 3.592 (3.43)  Time: 0.773s, 1325.04/s  (0.791s, 1294.44/s)  LR: 6.054e-04  Data: 0.009 (0.016)
Train: 261 [ 350/1251 ( 28%)]  Loss: 3.764 (3.47)  Time: 0.770s, 1329.65/s  (0.791s, 1294.53/s)  LR: 6.054e-04  Data: 0.010 (0.015)
Train: 261 [ 400/1251 ( 32%)]  Loss: 3.473 (3.47)  Time: 0.775s, 1321.48/s  (0.790s, 1296.88/s)  LR: 6.054e-04  Data: 0.010 (0.015)
Train: 261 [ 450/1251 ( 36%)]  Loss: 3.493 (3.47)  Time: 0.776s, 1320.42/s  (0.788s, 1298.76/s)  LR: 6.054e-04  Data: 0.009 (0.014)
Train: 261 [ 500/1251 ( 40%)]  Loss: 3.681 (3.49)  Time: 0.821s, 1246.64/s  (0.787s, 1300.43/s)  LR: 6.054e-04  Data: 0.013 (0.014)
Train: 261 [ 550/1251 ( 44%)]  Loss: 3.477 (3.49)  Time: 0.781s, 1311.49/s  (0.787s, 1301.22/s)  LR: 6.054e-04  Data: 0.011 (0.013)
Train: 261 [ 600/1251 ( 48%)]  Loss: 3.838 (3.52)  Time: 0.806s, 1270.69/s  (0.787s, 1301.49/s)  LR: 6.054e-04  Data: 0.009 (0.013)
Train: 261 [ 650/1251 ( 52%)]  Loss: 3.811 (3.54)  Time: 0.779s, 1313.78/s  (0.786s, 1302.63/s)  LR: 6.054e-04  Data: 0.009 (0.013)
Train: 261 [ 700/1251 ( 56%)]  Loss: 3.664 (3.55)  Time: 0.777s, 1317.60/s  (0.786s, 1303.34/s)  LR: 6.054e-04  Data: 0.009 (0.013)
Train: 261 [ 750/1251 ( 60%)]  Loss: 3.649 (3.55)  Time: 0.774s, 1323.85/s  (0.785s, 1303.83/s)  LR: 6.054e-04  Data: 0.009 (0.012)
Train: 261 [ 800/1251 ( 64%)]  Loss: 3.485 (3.55)  Time: 0.776s, 1320.08/s  (0.785s, 1304.03/s)  LR: 6.054e-04  Data: 0.010 (0.012)
Train: 261 [ 850/1251 ( 68%)]  Loss: 3.431 (3.54)  Time: 0.819s, 1250.11/s  (0.785s, 1304.14/s)  LR: 6.054e-04  Data: 0.010 (0.012)
Train: 261 [ 900/1251 ( 72%)]  Loss: 3.396 (3.53)  Time: 0.848s, 1207.76/s  (0.785s, 1304.46/s)  LR: 6.054e-04  Data: 0.010 (0.012)
Train: 261 [ 950/1251 ( 76%)]  Loss: 3.474 (3.53)  Time: 0.805s, 1272.28/s  (0.785s, 1304.84/s)  LR: 6.054e-04  Data: 0.012 (0.012)
Train: 261 [1000/1251 ( 80%)]  Loss: 3.276 (3.52)  Time: 0.777s, 1318.43/s  (0.785s, 1305.24/s)  LR: 6.054e-04  Data: 0.010 (0.012)
Train: 261 [1050/1251 ( 84%)]  Loss: 3.300 (3.51)  Time: 0.778s, 1315.85/s  (0.784s, 1305.80/s)  LR: 6.054e-04  Data: 0.009 (0.012)
Train: 261 [1100/1251 ( 88%)]  Loss: 3.559 (3.51)  Time: 0.843s, 1214.75/s  (0.784s, 1305.97/s)  LR: 6.054e-04  Data: 0.009 (0.012)
Train: 261 [1150/1251 ( 92%)]  Loss: 3.765 (3.52)  Time: 0.773s, 1324.92/s  (0.784s, 1306.38/s)  LR: 6.054e-04  Data: 0.010 (0.011)
Train: 261 [1200/1251 ( 96%)]  Loss: 3.538 (3.52)  Time: 0.773s, 1325.23/s  (0.784s, 1305.53/s)  LR: 6.054e-04  Data: 0.009 (0.011)
Train: 261 [1250/1251 (100%)]  Loss: 3.508 (3.52)  Time: 0.762s, 1344.38/s  (0.784s, 1305.93/s)  LR: 6.054e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.523 (1.523)  Loss:  0.8232 (0.8232)  Acc@1: 89.8438 (89.8438)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.193 (0.563)  Loss:  0.8291 (1.3613)  Acc@1: 85.4953 (74.3480)  Acc@5: 96.8160 (92.4800)
Train: 262 [   0/1251 (  0%)]  Loss: 3.902 (3.90)  Time: 2.217s,  461.89/s  (2.217s,  461.89/s)  LR: 6.028e-04  Data: 1.481 (1.481)
Train: 262 [  50/1251 (  4%)]  Loss: 3.650 (3.78)  Time: 0.790s, 1296.21/s  (0.812s, 1261.07/s)  LR: 6.028e-04  Data: 0.009 (0.041)
Train: 262 [ 100/1251 (  8%)]  Loss: 3.537 (3.70)  Time: 0.772s, 1326.19/s  (0.799s, 1281.95/s)  LR: 6.028e-04  Data: 0.010 (0.026)
Train: 262 [ 150/1251 ( 12%)]  Loss: 3.723 (3.70)  Time: 0.774s, 1323.31/s  (0.796s, 1286.74/s)  LR: 6.028e-04  Data: 0.010 (0.021)
Train: 262 [ 200/1251 ( 16%)]  Loss: 3.768 (3.72)  Time: 0.832s, 1231.38/s  (0.793s, 1290.94/s)  LR: 6.028e-04  Data: 0.009 (0.018)
Train: 262 [ 250/1251 ( 20%)]  Loss: 3.282 (3.64)  Time: 0.788s, 1299.99/s  (0.790s, 1295.39/s)  LR: 6.028e-04  Data: 0.010 (0.016)
Train: 262 [ 300/1251 ( 24%)]  Loss: 3.621 (3.64)  Time: 0.775s, 1321.54/s  (0.790s, 1295.94/s)  LR: 6.028e-04  Data: 0.009 (0.015)
Train: 262 [ 350/1251 ( 28%)]  Loss: 3.529 (3.63)  Time: 0.777s, 1317.35/s  (0.788s, 1298.83/s)  LR: 6.028e-04  Data: 0.009 (0.014)
Train: 262 [ 400/1251 ( 32%)]  Loss: 3.465 (3.61)  Time: 0.808s, 1266.67/s  (0.789s, 1298.54/s)  LR: 6.028e-04  Data: 0.009 (0.014)
Train: 262 [ 450/1251 ( 36%)]  Loss: 3.620 (3.61)  Time: 0.781s, 1310.72/s  (0.790s, 1296.64/s)  LR: 6.028e-04  Data: 0.012 (0.013)
Train: 262 [ 500/1251 ( 40%)]  Loss: 3.018 (3.56)  Time: 0.780s, 1313.38/s  (0.789s, 1298.30/s)  LR: 6.028e-04  Data: 0.010 (0.013)
Train: 262 [ 550/1251 ( 44%)]  Loss: 3.377 (3.54)  Time: 0.773s, 1325.50/s  (0.789s, 1298.66/s)  LR: 6.028e-04  Data: 0.010 (0.013)
Train: 262 [ 600/1251 ( 48%)]  Loss: 3.674 (3.55)  Time: 0.787s, 1300.46/s  (0.789s, 1298.54/s)  LR: 6.028e-04  Data: 0.009 (0.012)
Train: 262 [ 650/1251 ( 52%)]  Loss: 3.431 (3.54)  Time: 0.785s, 1304.87/s  (0.789s, 1297.95/s)  LR: 6.028e-04  Data: 0.010 (0.012)
Train: 262 [ 700/1251 ( 56%)]  Loss: 3.586 (3.55)  Time: 0.774s, 1323.22/s  (0.789s, 1298.43/s)  LR: 6.028e-04  Data: 0.011 (0.012)
Train: 262 [ 750/1251 ( 60%)]  Loss: 3.715 (3.56)  Time: 0.776s, 1319.05/s  (0.788s, 1299.50/s)  LR: 6.028e-04  Data: 0.010 (0.012)
Train: 262 [ 800/1251 ( 64%)]  Loss: 3.645 (3.56)  Time: 0.779s, 1313.68/s  (0.788s, 1299.55/s)  LR: 6.028e-04  Data: 0.010 (0.012)
Train: 262 [ 850/1251 ( 68%)]  Loss: 3.407 (3.55)  Time: 0.813s, 1259.06/s  (0.788s, 1299.81/s)  LR: 6.028e-04  Data: 0.010 (0.012)
Train: 262 [ 900/1251 ( 72%)]  Loss: 3.260 (3.54)  Time: 0.779s, 1314.06/s  (0.788s, 1299.08/s)  LR: 6.028e-04  Data: 0.012 (0.012)
Train: 262 [ 950/1251 ( 76%)]  Loss: 3.502 (3.54)  Time: 0.780s, 1312.10/s  (0.788s, 1299.68/s)  LR: 6.028e-04  Data: 0.009 (0.012)
Train: 262 [1000/1251 ( 80%)]  Loss: 3.563 (3.54)  Time: 0.772s, 1326.59/s  (0.787s, 1300.35/s)  LR: 6.028e-04  Data: 0.009 (0.011)
Train: 262 [1050/1251 ( 84%)]  Loss: 3.673 (3.54)  Time: 0.776s, 1319.27/s  (0.787s, 1300.97/s)  LR: 6.028e-04  Data: 0.010 (0.011)
Train: 262 [1100/1251 ( 88%)]  Loss: 3.543 (3.54)  Time: 0.775s, 1321.25/s  (0.787s, 1300.79/s)  LR: 6.028e-04  Data: 0.010 (0.011)
Train: 262 [1150/1251 ( 92%)]  Loss: 3.474 (3.54)  Time: 0.773s, 1324.30/s  (0.787s, 1301.40/s)  LR: 6.028e-04  Data: 0.009 (0.011)
Train: 262 [1200/1251 ( 96%)]  Loss: 3.449 (3.54)  Time: 0.774s, 1322.68/s  (0.786s, 1302.25/s)  LR: 6.028e-04  Data: 0.010 (0.011)
Train: 262 [1250/1251 (100%)]  Loss: 3.536 (3.54)  Time: 0.762s, 1343.37/s  (0.786s, 1302.49/s)  LR: 6.028e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.517 (1.517)  Loss:  0.7461 (0.7461)  Acc@1: 90.1367 (90.1367)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.194 (0.570)  Loss:  0.7529 (1.2930)  Acc@1: 87.6179 (75.0500)  Acc@5: 97.4057 (92.7560)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-260.pth.tar', 75.10400008789063)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-262.pth.tar', 75.05000018066406)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-254.pth.tar', 74.96399992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-250.pth.tar', 74.76799987792968)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-240.pth.tar', 74.6560000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-256.pth.tar', 74.61600000976563)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-255.pth.tar', 74.61200000976562)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-257.pth.tar', 74.60399992675781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-242.pth.tar', 74.5920001147461)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-259.pth.tar', 74.58800008544922)

Train: 263 [   0/1251 (  0%)]  Loss: 3.676 (3.68)  Time: 2.259s,  453.27/s  (2.259s,  453.27/s)  LR: 6.003e-04  Data: 1.522 (1.522)
Train: 263 [  50/1251 (  4%)]  Loss: 3.450 (3.56)  Time: 0.773s, 1323.88/s  (0.814s, 1257.71/s)  LR: 6.003e-04  Data: 0.010 (0.045)
Train: 263 [ 100/1251 (  8%)]  Loss: 3.483 (3.54)  Time: 0.781s, 1310.81/s  (0.808s, 1266.78/s)  LR: 6.003e-04  Data: 0.011 (0.028)
Train: 263 [ 150/1251 ( 12%)]  Loss: 3.694 (3.58)  Time: 0.782s, 1308.81/s  (0.798s, 1282.49/s)  LR: 6.003e-04  Data: 0.012 (0.022)
Train: 263 [ 200/1251 ( 16%)]  Loss: 3.329 (3.53)  Time: 0.774s, 1323.48/s  (0.794s, 1289.10/s)  LR: 6.003e-04  Data: 0.010 (0.019)
Train: 263 [ 250/1251 ( 20%)]  Loss: 3.494 (3.52)  Time: 0.772s, 1327.05/s  (0.792s, 1293.34/s)  LR: 6.003e-04  Data: 0.009 (0.018)
Train: 263 [ 300/1251 ( 24%)]  Loss: 3.288 (3.49)  Time: 0.778s, 1315.64/s  (0.790s, 1296.99/s)  LR: 6.003e-04  Data: 0.012 (0.016)
Train: 263 [ 350/1251 ( 28%)]  Loss: 3.662 (3.51)  Time: 0.775s, 1321.77/s  (0.789s, 1297.84/s)  LR: 6.003e-04  Data: 0.010 (0.015)
Train: 263 [ 400/1251 ( 32%)]  Loss: 3.490 (3.51)  Time: 0.779s, 1314.82/s  (0.790s, 1296.90/s)  LR: 6.003e-04  Data: 0.009 (0.015)
Train: 263 [ 450/1251 ( 36%)]  Loss: 3.452 (3.50)  Time: 0.777s, 1318.51/s  (0.789s, 1297.07/s)  LR: 6.003e-04  Data: 0.011 (0.014)
Train: 263 [ 500/1251 ( 40%)]  Loss: 2.927 (3.45)  Time: 0.770s, 1329.69/s  (0.790s, 1295.43/s)  LR: 6.003e-04  Data: 0.009 (0.014)
Train: 263 [ 550/1251 ( 44%)]  Loss: 3.687 (3.47)  Time: 0.773s, 1323.86/s  (0.790s, 1296.60/s)  LR: 6.003e-04  Data: 0.009 (0.013)
Train: 263 [ 600/1251 ( 48%)]  Loss: 3.512 (3.47)  Time: 0.776s, 1320.36/s  (0.789s, 1297.83/s)  LR: 6.003e-04  Data: 0.009 (0.013)
Train: 263 [ 650/1251 ( 52%)]  Loss: 3.424 (3.47)  Time: 0.773s, 1325.55/s  (0.788s, 1299.17/s)  LR: 6.003e-04  Data: 0.010 (0.013)
Train: 263 [ 700/1251 ( 56%)]  Loss: 3.450 (3.47)  Time: 0.771s, 1327.64/s  (0.788s, 1299.92/s)  LR: 6.003e-04  Data: 0.010 (0.013)
Train: 263 [ 750/1251 ( 60%)]  Loss: 3.510 (3.47)  Time: 0.808s, 1267.72/s  (0.788s, 1299.73/s)  LR: 6.003e-04  Data: 0.009 (0.012)
Train: 263 [ 800/1251 ( 64%)]  Loss: 3.631 (3.48)  Time: 0.818s, 1251.94/s  (0.788s, 1299.56/s)  LR: 6.003e-04  Data: 0.012 (0.012)
Train: 263 [ 850/1251 ( 68%)]  Loss: 3.536 (3.48)  Time: 0.772s, 1326.89/s  (0.788s, 1299.03/s)  LR: 6.003e-04  Data: 0.010 (0.012)
Train: 263 [ 900/1251 ( 72%)]  Loss: 3.499 (3.48)  Time: 0.831s, 1231.75/s  (0.788s, 1299.33/s)  LR: 6.003e-04  Data: 0.009 (0.012)
Train: 263 [ 950/1251 ( 76%)]  Loss: 3.679 (3.49)  Time: 0.775s, 1321.38/s  (0.789s, 1297.82/s)  LR: 6.003e-04  Data: 0.010 (0.012)
Train: 263 [1000/1251 ( 80%)]  Loss: 3.579 (3.50)  Time: 0.773s, 1325.04/s  (0.789s, 1298.33/s)  LR: 6.003e-04  Data: 0.010 (0.012)
Train: 263 [1050/1251 ( 84%)]  Loss: 3.342 (3.49)  Time: 0.899s, 1139.34/s  (0.789s, 1298.26/s)  LR: 6.003e-04  Data: 0.011 (0.012)
Train: 263 [1100/1251 ( 88%)]  Loss: 3.320 (3.48)  Time: 0.773s, 1325.07/s  (0.789s, 1297.45/s)  LR: 6.003e-04  Data: 0.010 (0.012)
Train: 263 [1150/1251 ( 92%)]  Loss: 3.720 (3.49)  Time: 0.770s, 1329.56/s  (0.789s, 1298.25/s)  LR: 6.003e-04  Data: 0.010 (0.012)
Train: 263 [1200/1251 ( 96%)]  Loss: 3.316 (3.49)  Time: 0.773s, 1325.43/s  (0.788s, 1298.98/s)  LR: 6.003e-04  Data: 0.010 (0.012)
Train: 263 [1250/1251 (100%)]  Loss: 3.494 (3.49)  Time: 0.803s, 1275.44/s  (0.789s, 1297.59/s)  LR: 6.003e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.603 (1.603)  Loss:  0.7480 (0.7480)  Acc@1: 89.0625 (89.0625)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.194 (0.570)  Loss:  0.8135 (1.3123)  Acc@1: 85.1415 (74.4540)  Acc@5: 95.7547 (92.3180)
Train: 264 [   0/1251 (  0%)]  Loss: 3.527 (3.53)  Time: 2.371s,  431.97/s  (2.371s,  431.97/s)  LR: 5.978e-04  Data: 1.647 (1.647)
Train: 264 [  50/1251 (  4%)]  Loss: 3.918 (3.72)  Time: 0.780s, 1313.60/s  (0.819s, 1250.83/s)  LR: 5.978e-04  Data: 0.010 (0.044)
Train: 264 [ 100/1251 (  8%)]  Loss: 3.684 (3.71)  Time: 0.774s, 1323.51/s  (0.799s, 1282.11/s)  LR: 5.978e-04  Data: 0.010 (0.027)
Train: 264 [ 150/1251 ( 12%)]  Loss: 3.685 (3.70)  Time: 0.778s, 1316.23/s  (0.793s, 1291.08/s)  LR: 5.978e-04  Data: 0.009 (0.021)
Train: 264 [ 200/1251 ( 16%)]  Loss: 3.491 (3.66)  Time: 0.774s, 1323.24/s  (0.791s, 1294.82/s)  LR: 5.978e-04  Data: 0.010 (0.019)
Train: 264 [ 250/1251 ( 20%)]  Loss: 3.763 (3.68)  Time: 0.813s, 1259.83/s  (0.789s, 1298.39/s)  LR: 5.978e-04  Data: 0.010 (0.017)
Train: 264 [ 300/1251 ( 24%)]  Loss: 4.109 (3.74)  Time: 0.781s, 1311.60/s  (0.787s, 1300.61/s)  LR: 5.978e-04  Data: 0.011 (0.016)
Train: 264 [ 350/1251 ( 28%)]  Loss: 3.758 (3.74)  Time: 0.787s, 1301.84/s  (0.786s, 1302.03/s)  LR: 5.978e-04  Data: 0.010 (0.015)
Train: 264 [ 400/1251 ( 32%)]  Loss: 3.705 (3.74)  Time: 0.788s, 1298.95/s  (0.785s, 1303.84/s)  LR: 5.978e-04  Data: 0.010 (0.014)
Train: 264 [ 450/1251 ( 36%)]  Loss: 3.461 (3.71)  Time: 0.774s, 1322.18/s  (0.785s, 1304.37/s)  LR: 5.978e-04  Data: 0.011 (0.014)
Train: 264 [ 500/1251 ( 40%)]  Loss: 3.261 (3.67)  Time: 0.774s, 1323.52/s  (0.785s, 1304.77/s)  LR: 5.978e-04  Data: 0.009 (0.013)
Train: 264 [ 550/1251 ( 44%)]  Loss: 3.465 (3.65)  Time: 0.774s, 1323.63/s  (0.784s, 1305.38/s)  LR: 5.978e-04  Data: 0.010 (0.013)
Train: 264 [ 600/1251 ( 48%)]  Loss: 3.613 (3.65)  Time: 0.773s, 1324.10/s  (0.784s, 1306.19/s)  LR: 5.978e-04  Data: 0.010 (0.013)
Train: 264 [ 650/1251 ( 52%)]  Loss: 3.436 (3.63)  Time: 0.774s, 1323.53/s  (0.784s, 1306.78/s)  LR: 5.978e-04  Data: 0.010 (0.012)
Train: 264 [ 700/1251 ( 56%)]  Loss: 3.616 (3.63)  Time: 0.781s, 1310.91/s  (0.783s, 1306.97/s)  LR: 5.978e-04  Data: 0.012 (0.012)
Train: 264 [ 750/1251 ( 60%)]  Loss: 3.508 (3.62)  Time: 0.773s, 1324.07/s  (0.783s, 1307.58/s)  LR: 5.978e-04  Data: 0.010 (0.012)
Train: 264 [ 800/1251 ( 64%)]  Loss: 3.409 (3.61)  Time: 0.773s, 1325.51/s  (0.783s, 1307.86/s)  LR: 5.978e-04  Data: 0.009 (0.012)
Train: 264 [ 850/1251 ( 68%)]  Loss: 3.227 (3.59)  Time: 0.775s, 1320.74/s  (0.783s, 1308.26/s)  LR: 5.978e-04  Data: 0.010 (0.012)
Train: 264 [ 900/1251 ( 72%)]  Loss: 3.457 (3.58)  Time: 0.772s, 1326.91/s  (0.783s, 1308.25/s)  LR: 5.978e-04  Data: 0.009 (0.012)
Train: 264 [ 950/1251 ( 76%)]  Loss: 3.121 (3.56)  Time: 0.826s, 1239.20/s  (0.783s, 1308.42/s)  LR: 5.978e-04  Data: 0.009 (0.012)
Train: 264 [1000/1251 ( 80%)]  Loss: 3.264 (3.55)  Time: 0.770s, 1330.08/s  (0.782s, 1308.88/s)  LR: 5.978e-04  Data: 0.010 (0.012)
Train: 264 [1050/1251 ( 84%)]  Loss: 3.429 (3.54)  Time: 0.821s, 1246.93/s  (0.782s, 1308.76/s)  LR: 5.978e-04  Data: 0.010 (0.011)
Train: 264 [1100/1251 ( 88%)]  Loss: 3.871 (3.56)  Time: 0.785s, 1303.68/s  (0.782s, 1308.71/s)  LR: 5.978e-04  Data: 0.009 (0.011)
Train: 264 [1150/1251 ( 92%)]  Loss: 3.841 (3.57)  Time: 0.775s, 1321.55/s  (0.782s, 1308.98/s)  LR: 5.978e-04  Data: 0.011 (0.011)
Train: 264 [1200/1251 ( 96%)]  Loss: 3.630 (3.57)  Time: 0.782s, 1308.84/s  (0.783s, 1308.34/s)  LR: 5.978e-04  Data: 0.010 (0.011)
Train: 264 [1250/1251 (100%)]  Loss: 3.823 (3.58)  Time: 0.761s, 1346.12/s  (0.784s, 1306.38/s)  LR: 5.978e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.518 (1.518)  Loss:  0.7051 (0.7051)  Acc@1: 89.6484 (89.6484)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.193 (0.561)  Loss:  0.7861 (1.2444)  Acc@1: 86.5566 (74.8900)  Acc@5: 96.6981 (92.6200)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-260.pth.tar', 75.10400008789063)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-262.pth.tar', 75.05000018066406)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-254.pth.tar', 74.96399992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-264.pth.tar', 74.89000010742187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-250.pth.tar', 74.76799987792968)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-240.pth.tar', 74.6560000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-256.pth.tar', 74.61600000976563)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-255.pth.tar', 74.61200000976562)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-257.pth.tar', 74.60399992675781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-242.pth.tar', 74.5920001147461)

Train: 265 [   0/1251 (  0%)]  Loss: 3.742 (3.74)  Time: 2.252s,  454.71/s  (2.252s,  454.71/s)  LR: 5.952e-04  Data: 1.524 (1.524)
Train: 265 [  50/1251 (  4%)]  Loss: 3.650 (3.70)  Time: 0.776s, 1320.37/s  (0.813s, 1259.83/s)  LR: 5.952e-04  Data: 0.009 (0.042)
Train: 265 [ 100/1251 (  8%)]  Loss: 3.432 (3.61)  Time: 0.772s, 1326.55/s  (0.797s, 1284.23/s)  LR: 5.952e-04  Data: 0.009 (0.026)
Train: 265 [ 150/1251 ( 12%)]  Loss: 3.688 (3.63)  Time: 0.778s, 1316.75/s  (0.791s, 1294.42/s)  LR: 5.952e-04  Data: 0.011 (0.021)
Train: 265 [ 200/1251 ( 16%)]  Loss: 3.587 (3.62)  Time: 0.774s, 1323.53/s  (0.788s, 1299.67/s)  LR: 5.952e-04  Data: 0.010 (0.018)
Train: 265 [ 250/1251 ( 20%)]  Loss: 3.511 (3.60)  Time: 0.773s, 1324.65/s  (0.786s, 1303.16/s)  LR: 5.952e-04  Data: 0.009 (0.016)
Train: 265 [ 300/1251 ( 24%)]  Loss: 3.534 (3.59)  Time: 0.888s, 1153.16/s  (0.786s, 1302.99/s)  LR: 5.952e-04  Data: 0.010 (0.015)
Train: 265 [ 350/1251 ( 28%)]  Loss: 3.536 (3.58)  Time: 0.785s, 1304.47/s  (0.786s, 1302.71/s)  LR: 5.952e-04  Data: 0.009 (0.014)
Train: 265 [ 400/1251 ( 32%)]  Loss: 3.687 (3.60)  Time: 0.771s, 1328.63/s  (0.785s, 1304.62/s)  LR: 5.952e-04  Data: 0.010 (0.014)
Train: 265 [ 450/1251 ( 36%)]  Loss: 3.230 (3.56)  Time: 0.773s, 1325.06/s  (0.784s, 1306.16/s)  LR: 5.952e-04  Data: 0.010 (0.013)
Train: 265 [ 500/1251 ( 40%)]  Loss: 3.522 (3.56)  Time: 0.772s, 1325.57/s  (0.785s, 1304.60/s)  LR: 5.952e-04  Data: 0.010 (0.013)
Train: 265 [ 550/1251 ( 44%)]  Loss: 3.888 (3.58)  Time: 0.772s, 1326.30/s  (0.785s, 1305.08/s)  LR: 5.952e-04  Data: 0.010 (0.013)
Train: 265 [ 600/1251 ( 48%)]  Loss: 3.392 (3.57)  Time: 0.775s, 1322.14/s  (0.784s, 1305.67/s)  LR: 5.952e-04  Data: 0.009 (0.012)
Train: 265 [ 650/1251 ( 52%)]  Loss: 3.588 (3.57)  Time: 0.778s, 1316.57/s  (0.784s, 1305.60/s)  LR: 5.952e-04  Data: 0.010 (0.012)
Train: 265 [ 700/1251 ( 56%)]  Loss: 3.624 (3.57)  Time: 0.809s, 1265.70/s  (0.785s, 1304.02/s)  LR: 5.952e-04  Data: 0.009 (0.012)
Train: 265 [ 750/1251 ( 60%)]  Loss: 3.921 (3.60)  Time: 0.782s, 1310.12/s  (0.788s, 1300.24/s)  LR: 5.952e-04  Data: 0.011 (0.012)
Train: 265 [ 800/1251 ( 64%)]  Loss: 3.725 (3.60)  Time: 0.779s, 1314.50/s  (0.787s, 1301.23/s)  LR: 5.952e-04  Data: 0.011 (0.012)
Train: 265 [ 850/1251 ( 68%)]  Loss: 3.765 (3.61)  Time: 0.773s, 1325.17/s  (0.787s, 1301.96/s)  LR: 5.952e-04  Data: 0.010 (0.012)
Train: 265 [ 900/1251 ( 72%)]  Loss: 3.750 (3.62)  Time: 0.818s, 1252.50/s  (0.786s, 1302.47/s)  LR: 5.952e-04  Data: 0.010 (0.012)
Train: 265 [ 950/1251 ( 76%)]  Loss: 3.469 (3.61)  Time: 0.789s, 1298.63/s  (0.786s, 1302.43/s)  LR: 5.952e-04  Data: 0.009 (0.012)
Train: 265 [1000/1251 ( 80%)]  Loss: 3.657 (3.61)  Time: 0.778s, 1316.69/s  (0.786s, 1302.16/s)  LR: 5.952e-04  Data: 0.009 (0.012)
Train: 265 [1050/1251 ( 84%)]  Loss: 3.123 (3.59)  Time: 0.773s, 1324.17/s  (0.786s, 1302.47/s)  LR: 5.952e-04  Data: 0.010 (0.011)
Train: 265 [1100/1251 ( 88%)]  Loss: 3.016 (3.57)  Time: 0.772s, 1326.12/s  (0.786s, 1302.61/s)  LR: 5.952e-04  Data: 0.009 (0.011)
Train: 265 [1150/1251 ( 92%)]  Loss: 3.404 (3.56)  Time: 0.815s, 1257.07/s  (0.788s, 1300.09/s)  LR: 5.952e-04  Data: 0.011 (0.011)
Train: 265 [1200/1251 ( 96%)]  Loss: 3.411 (3.55)  Time: 0.852s, 1202.21/s  (0.787s, 1300.51/s)  LR: 5.952e-04  Data: 0.013 (0.011)
Train: 265 [1250/1251 (100%)]  Loss: 3.463 (3.55)  Time: 0.760s, 1347.73/s  (0.787s, 1300.87/s)  LR: 5.952e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.566 (1.566)  Loss:  0.9077 (0.9077)  Acc@1: 89.0625 (89.0625)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.193 (0.563)  Loss:  0.9805 (1.3774)  Acc@1: 85.9670 (74.3400)  Acc@5: 95.9906 (92.4760)
Train: 266 [   0/1251 (  0%)]  Loss: 3.630 (3.63)  Time: 2.329s,  439.62/s  (2.329s,  439.62/s)  LR: 5.927e-04  Data: 1.598 (1.598)
Train: 266 [  50/1251 (  4%)]  Loss: 3.566 (3.60)  Time: 0.770s, 1329.67/s  (0.820s, 1249.35/s)  LR: 5.927e-04  Data: 0.009 (0.049)
Train: 266 [ 100/1251 (  8%)]  Loss: 3.210 (3.47)  Time: 0.787s, 1301.09/s  (0.800s, 1280.08/s)  LR: 5.927e-04  Data: 0.009 (0.029)
Train: 266 [ 150/1251 ( 12%)]  Loss: 3.704 (3.53)  Time: 0.770s, 1330.19/s  (0.797s, 1284.92/s)  LR: 5.927e-04  Data: 0.010 (0.023)
Train: 266 [ 200/1251 ( 16%)]  Loss: 3.500 (3.52)  Time: 0.772s, 1325.67/s  (0.792s, 1292.20/s)  LR: 5.927e-04  Data: 0.011 (0.020)
Train: 266 [ 250/1251 ( 20%)]  Loss: 3.168 (3.46)  Time: 0.771s, 1327.84/s  (0.790s, 1295.78/s)  LR: 5.927e-04  Data: 0.010 (0.018)
Train: 266 [ 300/1251 ( 24%)]  Loss: 3.774 (3.51)  Time: 0.772s, 1325.86/s  (0.788s, 1299.23/s)  LR: 5.927e-04  Data: 0.010 (0.016)
Train: 266 [ 350/1251 ( 28%)]  Loss: 3.306 (3.48)  Time: 0.777s, 1317.89/s  (0.787s, 1300.66/s)  LR: 5.927e-04  Data: 0.009 (0.015)
Train: 266 [ 400/1251 ( 32%)]  Loss: 3.891 (3.53)  Time: 0.778s, 1316.46/s  (0.786s, 1302.27/s)  LR: 5.927e-04  Data: 0.010 (0.015)
Train: 266 [ 450/1251 ( 36%)]  Loss: 3.464 (3.52)  Time: 0.779s, 1314.00/s  (0.786s, 1303.42/s)  LR: 5.927e-04  Data: 0.010 (0.014)
Train: 266 [ 500/1251 ( 40%)]  Loss: 3.708 (3.54)  Time: 0.773s, 1324.28/s  (0.785s, 1304.44/s)  LR: 5.927e-04  Data: 0.009 (0.014)
Train: 266 [ 550/1251 ( 44%)]  Loss: 3.218 (3.51)  Time: 0.773s, 1324.48/s  (0.785s, 1304.56/s)  LR: 5.927e-04  Data: 0.009 (0.013)
Train: 266 [ 600/1251 ( 48%)]  Loss: 3.685 (3.52)  Time: 0.787s, 1300.75/s  (0.785s, 1304.99/s)  LR: 5.927e-04  Data: 0.009 (0.013)
Train: 266 [ 650/1251 ( 52%)]  Loss: 3.658 (3.53)  Time: 0.774s, 1322.66/s  (0.784s, 1305.81/s)  LR: 5.927e-04  Data: 0.010 (0.013)
Train: 266 [ 700/1251 ( 56%)]  Loss: 3.619 (3.54)  Time: 0.781s, 1310.68/s  (0.784s, 1306.26/s)  LR: 5.927e-04  Data: 0.010 (0.013)
Train: 266 [ 750/1251 ( 60%)]  Loss: 3.451 (3.53)  Time: 0.771s, 1327.29/s  (0.783s, 1307.31/s)  LR: 5.927e-04  Data: 0.009 (0.012)
Train: 266 [ 800/1251 ( 64%)]  Loss: 3.558 (3.54)  Time: 0.816s, 1255.60/s  (0.783s, 1307.48/s)  LR: 5.927e-04  Data: 0.009 (0.012)
Train: 266 [ 850/1251 ( 68%)]  Loss: 3.766 (3.55)  Time: 0.774s, 1323.01/s  (0.784s, 1306.60/s)  LR: 5.927e-04  Data: 0.010 (0.012)
Train: 266 [ 900/1251 ( 72%)]  Loss: 3.630 (3.55)  Time: 0.773s, 1323.97/s  (0.784s, 1305.75/s)  LR: 5.927e-04  Data: 0.009 (0.012)
Train: 266 [ 950/1251 ( 76%)]  Loss: 3.373 (3.54)  Time: 0.782s, 1308.94/s  (0.784s, 1306.04/s)  LR: 5.927e-04  Data: 0.010 (0.012)
Train: 266 [1000/1251 ( 80%)]  Loss: 3.085 (3.52)  Time: 0.780s, 1312.45/s  (0.784s, 1305.82/s)  LR: 5.927e-04  Data: 0.011 (0.012)
Train: 266 [1050/1251 ( 84%)]  Loss: 3.452 (3.52)  Time: 0.811s, 1262.84/s  (0.784s, 1305.85/s)  LR: 5.927e-04  Data: 0.009 (0.012)
Train: 266 [1100/1251 ( 88%)]  Loss: 3.632 (3.52)  Time: 0.826s, 1239.02/s  (0.784s, 1305.97/s)  LR: 5.927e-04  Data: 0.010 (0.012)
Train: 266 [1150/1251 ( 92%)]  Loss: 3.442 (3.52)  Time: 0.775s, 1321.90/s  (0.784s, 1306.45/s)  LR: 5.927e-04  Data: 0.010 (0.012)
Train: 266 [1200/1251 ( 96%)]  Loss: 3.250 (3.51)  Time: 0.774s, 1322.15/s  (0.784s, 1306.78/s)  LR: 5.927e-04  Data: 0.009 (0.011)
Train: 266 [1250/1251 (100%)]  Loss: 3.754 (3.52)  Time: 0.763s, 1342.56/s  (0.784s, 1306.40/s)  LR: 5.927e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.524 (1.524)  Loss:  0.7822 (0.7822)  Acc@1: 90.0391 (90.0391)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.194 (0.564)  Loss:  0.9287 (1.3688)  Acc@1: 85.3774 (74.8540)  Acc@5: 95.7547 (92.7120)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-260.pth.tar', 75.10400008789063)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-262.pth.tar', 75.05000018066406)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-254.pth.tar', 74.96399992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-264.pth.tar', 74.89000010742187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-266.pth.tar', 74.85399998291015)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-250.pth.tar', 74.76799987792968)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-240.pth.tar', 74.6560000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-256.pth.tar', 74.61600000976563)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-255.pth.tar', 74.61200000976562)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-257.pth.tar', 74.60399992675781)

Train: 267 [   0/1251 (  0%)]  Loss: 3.464 (3.46)  Time: 2.359s,  434.03/s  (2.359s,  434.03/s)  LR: 5.901e-04  Data: 1.628 (1.628)
Train: 267 [  50/1251 (  4%)]  Loss: 3.386 (3.43)  Time: 0.772s, 1325.68/s  (0.828s, 1237.38/s)  LR: 5.901e-04  Data: 0.010 (0.047)
Train: 267 [ 100/1251 (  8%)]  Loss: 3.327 (3.39)  Time: 0.777s, 1318.27/s  (0.803s, 1275.71/s)  LR: 5.901e-04  Data: 0.009 (0.028)
Train: 267 [ 150/1251 ( 12%)]  Loss: 3.388 (3.39)  Time: 0.779s, 1314.29/s  (0.795s, 1287.68/s)  LR: 5.901e-04  Data: 0.009 (0.022)
Train: 267 [ 200/1251 ( 16%)]  Loss: 3.500 (3.41)  Time: 0.773s, 1325.22/s  (0.792s, 1292.96/s)  LR: 5.901e-04  Data: 0.009 (0.019)
Train: 267 [ 250/1251 ( 20%)]  Loss: 3.308 (3.40)  Time: 0.775s, 1320.83/s  (0.790s, 1296.53/s)  LR: 5.901e-04  Data: 0.009 (0.017)
Train: 267 [ 300/1251 ( 24%)]  Loss: 3.280 (3.38)  Time: 0.773s, 1324.09/s  (0.789s, 1298.65/s)  LR: 5.901e-04  Data: 0.009 (0.016)
Train: 267 [ 350/1251 ( 28%)]  Loss: 3.586 (3.41)  Time: 0.774s, 1323.66/s  (0.788s, 1299.74/s)  LR: 5.901e-04  Data: 0.010 (0.015)
Train: 267 [ 400/1251 ( 32%)]  Loss: 3.497 (3.42)  Time: 0.773s, 1325.25/s  (0.787s, 1301.26/s)  LR: 5.901e-04  Data: 0.010 (0.014)
Train: 267 [ 450/1251 ( 36%)]  Loss: 3.509 (3.42)  Time: 0.773s, 1324.05/s  (0.787s, 1301.71/s)  LR: 5.901e-04  Data: 0.010 (0.014)
Train: 267 [ 500/1251 ( 40%)]  Loss: 3.461 (3.43)  Time: 0.776s, 1319.25/s  (0.786s, 1302.37/s)  LR: 5.901e-04  Data: 0.009 (0.013)
Train: 267 [ 550/1251 ( 44%)]  Loss: 3.790 (3.46)  Time: 0.782s, 1310.02/s  (0.786s, 1302.68/s)  LR: 5.901e-04  Data: 0.013 (0.013)
Train: 267 [ 600/1251 ( 48%)]  Loss: 3.091 (3.43)  Time: 0.778s, 1316.15/s  (0.786s, 1303.34/s)  LR: 5.901e-04  Data: 0.010 (0.013)
Train: 267 [ 650/1251 ( 52%)]  Loss: 3.158 (3.41)  Time: 0.780s, 1312.11/s  (0.786s, 1303.46/s)  LR: 5.901e-04  Data: 0.011 (0.013)
Train: 267 [ 700/1251 ( 56%)]  Loss: 3.274 (3.40)  Time: 0.774s, 1323.45/s  (0.785s, 1304.10/s)  LR: 5.901e-04  Data: 0.010 (0.013)
Train: 267 [ 750/1251 ( 60%)]  Loss: 3.578 (3.41)  Time: 0.809s, 1266.10/s  (0.785s, 1304.77/s)  LR: 5.901e-04  Data: 0.009 (0.012)
Train: 267 [ 800/1251 ( 64%)]  Loss: 3.409 (3.41)  Time: 0.818s, 1252.29/s  (0.784s, 1305.42/s)  LR: 5.901e-04  Data: 0.009 (0.012)
Train: 267 [ 850/1251 ( 68%)]  Loss: 3.533 (3.42)  Time: 0.775s, 1322.08/s  (0.785s, 1304.05/s)  LR: 5.901e-04  Data: 0.010 (0.012)
Train: 267 [ 900/1251 ( 72%)]  Loss: 3.140 (3.40)  Time: 0.779s, 1313.98/s  (0.785s, 1304.45/s)  LR: 5.901e-04  Data: 0.010 (0.012)
Train: 267 [ 950/1251 ( 76%)]  Loss: 3.808 (3.42)  Time: 0.782s, 1309.23/s  (0.785s, 1304.80/s)  LR: 5.901e-04  Data: 0.010 (0.012)
Train: 267 [1000/1251 ( 80%)]  Loss: 3.712 (3.44)  Time: 0.836s, 1225.08/s  (0.784s, 1305.31/s)  LR: 5.901e-04  Data: 0.009 (0.012)
Train: 267 [1050/1251 ( 84%)]  Loss: 3.660 (3.45)  Time: 0.785s, 1304.25/s  (0.785s, 1304.72/s)  LR: 5.901e-04  Data: 0.010 (0.012)
Train: 267 [1100/1251 ( 88%)]  Loss: 3.560 (3.45)  Time: 0.772s, 1326.26/s  (0.786s, 1303.34/s)  LR: 5.901e-04  Data: 0.010 (0.012)
Train: 267 [1150/1251 ( 92%)]  Loss: 3.752 (3.47)  Time: 0.773s, 1324.40/s  (0.785s, 1304.13/s)  LR: 5.901e-04  Data: 0.010 (0.011)
Train: 267 [1200/1251 ( 96%)]  Loss: 3.152 (3.45)  Time: 0.774s, 1323.62/s  (0.785s, 1304.78/s)  LR: 5.901e-04  Data: 0.010 (0.011)
Train: 267 [1250/1251 (100%)]  Loss: 3.638 (3.46)  Time: 0.785s, 1304.13/s  (0.785s, 1305.27/s)  LR: 5.901e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.512 (1.512)  Loss:  0.8379 (0.8379)  Acc@1: 89.2578 (89.2578)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.194 (0.567)  Loss:  0.9829 (1.3558)  Acc@1: 85.1415 (74.9620)  Acc@5: 95.7547 (92.6160)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-260.pth.tar', 75.10400008789063)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-262.pth.tar', 75.05000018066406)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-254.pth.tar', 74.96399992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-267.pth.tar', 74.96200000976563)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-264.pth.tar', 74.89000010742187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-266.pth.tar', 74.85399998291015)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-250.pth.tar', 74.76799987792968)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-240.pth.tar', 74.6560000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-256.pth.tar', 74.61600000976563)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-255.pth.tar', 74.61200000976562)

Train: 268 [   0/1251 (  0%)]  Loss: 3.567 (3.57)  Time: 2.270s,  451.19/s  (2.270s,  451.19/s)  LR: 5.876e-04  Data: 1.535 (1.535)
Train: 268 [  50/1251 (  4%)]  Loss: 3.121 (3.34)  Time: 0.774s, 1323.67/s  (0.817s, 1253.51/s)  LR: 5.876e-04  Data: 0.010 (0.043)
Train: 268 [ 100/1251 (  8%)]  Loss: 3.771 (3.49)  Time: 0.773s, 1324.57/s  (0.798s, 1282.75/s)  LR: 5.876e-04  Data: 0.010 (0.027)
Train: 268 [ 150/1251 ( 12%)]  Loss: 3.588 (3.51)  Time: 0.776s, 1319.81/s  (0.792s, 1293.06/s)  LR: 5.876e-04  Data: 0.010 (0.021)
Train: 268 [ 200/1251 ( 16%)]  Loss: 3.110 (3.43)  Time: 0.818s, 1252.22/s  (0.792s, 1292.28/s)  LR: 5.876e-04  Data: 0.012 (0.018)
Train: 268 [ 250/1251 ( 20%)]  Loss: 3.186 (3.39)  Time: 0.772s, 1325.99/s  (0.791s, 1293.94/s)  LR: 5.876e-04  Data: 0.009 (0.017)
Train: 268 [ 300/1251 ( 24%)]  Loss: 3.349 (3.38)  Time: 0.779s, 1314.15/s  (0.790s, 1295.63/s)  LR: 5.876e-04  Data: 0.012 (0.015)
Train: 268 [ 350/1251 ( 28%)]  Loss: 3.596 (3.41)  Time: 0.782s, 1308.67/s  (0.789s, 1298.49/s)  LR: 5.876e-04  Data: 0.009 (0.015)
Train: 268 [ 400/1251 ( 32%)]  Loss: 3.906 (3.47)  Time: 0.783s, 1307.88/s  (0.790s, 1296.99/s)  LR: 5.876e-04  Data: 0.010 (0.014)
Train: 268 [ 450/1251 ( 36%)]  Loss: 3.379 (3.46)  Time: 0.783s, 1307.15/s  (0.789s, 1298.41/s)  LR: 5.876e-04  Data: 0.010 (0.014)
Train: 268 [ 500/1251 ( 40%)]  Loss: 3.612 (3.47)  Time: 0.779s, 1313.86/s  (0.789s, 1298.13/s)  LR: 5.876e-04  Data: 0.010 (0.013)
Train: 268 [ 550/1251 ( 44%)]  Loss: 3.941 (3.51)  Time: 0.797s, 1284.09/s  (0.788s, 1298.95/s)  LR: 5.876e-04  Data: 0.010 (0.013)
Train: 268 [ 600/1251 ( 48%)]  Loss: 3.250 (3.49)  Time: 0.830s, 1233.20/s  (0.788s, 1298.84/s)  LR: 5.876e-04  Data: 0.011 (0.013)
Train: 268 [ 650/1251 ( 52%)]  Loss: 3.536 (3.49)  Time: 0.776s, 1318.80/s  (0.788s, 1299.36/s)  LR: 5.876e-04  Data: 0.010 (0.013)
Train: 268 [ 700/1251 ( 56%)]  Loss: 3.558 (3.50)  Time: 0.773s, 1324.05/s  (0.788s, 1300.06/s)  LR: 5.876e-04  Data: 0.009 (0.013)
Train: 268 [ 750/1251 ( 60%)]  Loss: 3.939 (3.53)  Time: 0.771s, 1327.34/s  (0.787s, 1300.84/s)  LR: 5.876e-04  Data: 0.009 (0.012)
Train: 268 [ 800/1251 ( 64%)]  Loss: 3.590 (3.53)  Time: 0.775s, 1321.52/s  (0.787s, 1301.83/s)  LR: 5.876e-04  Data: 0.010 (0.012)
Train: 268 [ 850/1251 ( 68%)]  Loss: 3.850 (3.55)  Time: 0.784s, 1305.34/s  (0.786s, 1302.21/s)  LR: 5.876e-04  Data: 0.011 (0.012)
Train: 268 [ 900/1251 ( 72%)]  Loss: 3.545 (3.55)  Time: 0.773s, 1324.81/s  (0.786s, 1302.75/s)  LR: 5.876e-04  Data: 0.010 (0.012)
Train: 268 [ 950/1251 ( 76%)]  Loss: 3.489 (3.54)  Time: 0.774s, 1323.57/s  (0.786s, 1303.50/s)  LR: 5.876e-04  Data: 0.010 (0.012)
Train: 268 [1000/1251 ( 80%)]  Loss: 3.550 (3.54)  Time: 0.778s, 1315.54/s  (0.785s, 1304.17/s)  LR: 5.876e-04  Data: 0.011 (0.012)
Train: 268 [1050/1251 ( 84%)]  Loss: 3.547 (3.54)  Time: 0.773s, 1324.76/s  (0.785s, 1304.66/s)  LR: 5.876e-04  Data: 0.010 (0.012)
Train: 268 [1100/1251 ( 88%)]  Loss: 3.441 (3.54)  Time: 0.773s, 1325.41/s  (0.785s, 1305.14/s)  LR: 5.876e-04  Data: 0.010 (0.012)
Train: 268 [1150/1251 ( 92%)]  Loss: 3.463 (3.54)  Time: 0.774s, 1322.20/s  (0.784s, 1305.77/s)  LR: 5.876e-04  Data: 0.010 (0.011)
Train: 268 [1200/1251 ( 96%)]  Loss: 3.070 (3.52)  Time: 0.773s, 1324.27/s  (0.784s, 1306.16/s)  LR: 5.876e-04  Data: 0.010 (0.011)
Train: 268 [1250/1251 (100%)]  Loss: 3.558 (3.52)  Time: 0.760s, 1347.42/s  (0.784s, 1306.47/s)  LR: 5.876e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.502 (1.502)  Loss:  0.7812 (0.7812)  Acc@1: 89.8438 (89.8438)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.194 (0.559)  Loss:  0.8047 (1.2770)  Acc@1: 85.6132 (75.0680)  Acc@5: 96.6981 (92.7100)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-260.pth.tar', 75.10400008789063)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-268.pth.tar', 75.06800008544921)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-262.pth.tar', 75.05000018066406)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-254.pth.tar', 74.96399992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-267.pth.tar', 74.96200000976563)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-264.pth.tar', 74.89000010742187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-266.pth.tar', 74.85399998291015)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-250.pth.tar', 74.76799987792968)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-240.pth.tar', 74.6560000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-256.pth.tar', 74.61600000976563)

Train: 269 [   0/1251 (  0%)]  Loss: 3.249 (3.25)  Time: 2.279s,  449.42/s  (2.279s,  449.42/s)  LR: 5.850e-04  Data: 1.546 (1.546)
Train: 269 [  50/1251 (  4%)]  Loss: 3.611 (3.43)  Time: 0.777s, 1317.73/s  (0.830s, 1234.20/s)  LR: 5.850e-04  Data: 0.010 (0.045)
Train: 269 [ 100/1251 (  8%)]  Loss: 4.087 (3.65)  Time: 0.773s, 1325.46/s  (0.805s, 1272.11/s)  LR: 5.850e-04  Data: 0.010 (0.028)
Train: 269 [ 150/1251 ( 12%)]  Loss: 3.568 (3.63)  Time: 0.810s, 1264.81/s  (0.798s, 1283.79/s)  LR: 5.850e-04  Data: 0.010 (0.022)
Train: 269 [ 200/1251 ( 16%)]  Loss: 3.538 (3.61)  Time: 0.770s, 1329.10/s  (0.793s, 1290.76/s)  LR: 5.850e-04  Data: 0.009 (0.019)
Train: 269 [ 250/1251 ( 20%)]  Loss: 3.628 (3.61)  Time: 0.774s, 1322.93/s  (0.790s, 1296.37/s)  LR: 5.850e-04  Data: 0.010 (0.017)
Train: 269 [ 300/1251 ( 24%)]  Loss: 3.620 (3.61)  Time: 0.852s, 1201.24/s  (0.789s, 1298.22/s)  LR: 5.850e-04  Data: 0.010 (0.016)
Train: 269 [ 350/1251 ( 28%)]  Loss: 2.966 (3.53)  Time: 0.783s, 1308.24/s  (0.787s, 1300.65/s)  LR: 5.850e-04  Data: 0.009 (0.015)
Train: 269 [ 400/1251 ( 32%)]  Loss: 3.438 (3.52)  Time: 0.817s, 1253.86/s  (0.787s, 1301.66/s)  LR: 5.850e-04  Data: 0.009 (0.014)
Train: 269 [ 450/1251 ( 36%)]  Loss: 3.812 (3.55)  Time: 0.773s, 1324.03/s  (0.786s, 1303.25/s)  LR: 5.850e-04  Data: 0.010 (0.014)
Train: 269 [ 500/1251 ( 40%)]  Loss: 3.238 (3.52)  Time: 0.773s, 1324.78/s  (0.785s, 1304.26/s)  LR: 5.850e-04  Data: 0.010 (0.013)
Train: 269 [ 550/1251 ( 44%)]  Loss: 3.942 (3.56)  Time: 0.777s, 1317.94/s  (0.785s, 1304.78/s)  LR: 5.850e-04  Data: 0.010 (0.013)
Train: 269 [ 600/1251 ( 48%)]  Loss: 3.612 (3.56)  Time: 0.773s, 1324.94/s  (0.785s, 1305.10/s)  LR: 5.850e-04  Data: 0.010 (0.013)
Train: 269 [ 650/1251 ( 52%)]  Loss: 3.557 (3.56)  Time: 0.773s, 1325.00/s  (0.784s, 1305.66/s)  LR: 5.850e-04  Data: 0.009 (0.013)
Train: 269 [ 700/1251 ( 56%)]  Loss: 3.613 (3.57)  Time: 0.784s, 1305.98/s  (0.784s, 1306.54/s)  LR: 5.850e-04  Data: 0.010 (0.012)
Train: 269 [ 750/1251 ( 60%)]  Loss: 3.831 (3.58)  Time: 0.774s, 1322.35/s  (0.783s, 1307.12/s)  LR: 5.850e-04  Data: 0.009 (0.012)
Train: 269 [ 800/1251 ( 64%)]  Loss: 3.338 (3.57)  Time: 0.790s, 1295.47/s  (0.783s, 1307.86/s)  LR: 5.850e-04  Data: 0.011 (0.012)
Train: 269 [ 850/1251 ( 68%)]  Loss: 3.332 (3.55)  Time: 0.775s, 1321.75/s  (0.783s, 1308.04/s)  LR: 5.850e-04  Data: 0.010 (0.012)
Train: 269 [ 900/1251 ( 72%)]  Loss: 3.849 (3.57)  Time: 0.773s, 1325.54/s  (0.782s, 1308.74/s)  LR: 5.850e-04  Data: 0.010 (0.012)
Train: 269 [ 950/1251 ( 76%)]  Loss: 3.703 (3.58)  Time: 0.774s, 1322.30/s  (0.782s, 1308.96/s)  LR: 5.850e-04  Data: 0.010 (0.012)
Train: 269 [1000/1251 ( 80%)]  Loss: 3.928 (3.59)  Time: 0.794s, 1289.38/s  (0.782s, 1309.44/s)  LR: 5.850e-04  Data: 0.009 (0.012)
Train: 269 [1050/1251 ( 84%)]  Loss: 3.507 (3.59)  Time: 0.772s, 1325.93/s  (0.782s, 1309.85/s)  LR: 5.850e-04  Data: 0.010 (0.012)
Train: 269 [1100/1251 ( 88%)]  Loss: 3.406 (3.58)  Time: 0.772s, 1325.83/s  (0.782s, 1310.10/s)  LR: 5.850e-04  Data: 0.009 (0.011)
Train: 269 [1150/1251 ( 92%)]  Loss: 3.438 (3.58)  Time: 0.799s, 1281.35/s  (0.781s, 1310.38/s)  LR: 5.850e-04  Data: 0.010 (0.011)
Train: 269 [1200/1251 ( 96%)]  Loss: 3.252 (3.56)  Time: 0.776s, 1320.19/s  (0.782s, 1309.25/s)  LR: 5.850e-04  Data: 0.009 (0.011)
Train: 269 [1250/1251 (100%)]  Loss: 3.576 (3.56)  Time: 0.810s, 1264.49/s  (0.783s, 1308.53/s)  LR: 5.850e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.567 (1.567)  Loss:  0.9287 (0.9287)  Acc@1: 89.0625 (89.0625)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.193 (0.584)  Loss:  1.0059 (1.4397)  Acc@1: 84.7877 (74.9840)  Acc@5: 96.5802 (92.8340)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-260.pth.tar', 75.10400008789063)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-268.pth.tar', 75.06800008544921)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-262.pth.tar', 75.05000018066406)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-269.pth.tar', 74.98399998535156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-254.pth.tar', 74.96399992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-267.pth.tar', 74.96200000976563)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-264.pth.tar', 74.89000010742187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-266.pth.tar', 74.85399998291015)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-250.pth.tar', 74.76799987792968)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-240.pth.tar', 74.6560000366211)

Train: 270 [   0/1251 (  0%)]  Loss: 3.571 (3.57)  Time: 2.231s,  459.09/s  (2.231s,  459.09/s)  LR: 5.824e-04  Data: 1.495 (1.495)
Train: 270 [  50/1251 (  4%)]  Loss: 3.692 (3.63)  Time: 0.771s, 1327.88/s  (0.855s, 1197.78/s)  LR: 5.824e-04  Data: 0.010 (0.044)
Train: 270 [ 100/1251 (  8%)]  Loss: 3.457 (3.57)  Time: 0.776s, 1319.94/s  (0.831s, 1231.93/s)  LR: 5.824e-04  Data: 0.010 (0.027)
Train: 270 [ 150/1251 ( 12%)]  Loss: 3.643 (3.59)  Time: 0.772s, 1326.65/s  (0.820s, 1248.34/s)  LR: 5.824e-04  Data: 0.010 (0.021)
Train: 270 [ 200/1251 ( 16%)]  Loss: 3.527 (3.58)  Time: 0.774s, 1323.59/s  (0.811s, 1263.18/s)  LR: 5.824e-04  Data: 0.010 (0.018)
Train: 270 [ 250/1251 ( 20%)]  Loss: 3.636 (3.59)  Time: 0.782s, 1309.23/s  (0.805s, 1272.53/s)  LR: 5.824e-04  Data: 0.010 (0.017)
Train: 270 [ 300/1251 ( 24%)]  Loss: 3.646 (3.60)  Time: 0.774s, 1322.63/s  (0.800s, 1279.30/s)  LR: 5.824e-04  Data: 0.010 (0.016)
Train: 270 [ 350/1251 ( 28%)]  Loss: 3.404 (3.57)  Time: 0.779s, 1315.09/s  (0.797s, 1284.73/s)  LR: 5.824e-04  Data: 0.009 (0.015)
Train: 270 [ 400/1251 ( 32%)]  Loss: 3.374 (3.55)  Time: 0.827s, 1238.89/s  (0.796s, 1286.70/s)  LR: 5.824e-04  Data: 0.014 (0.014)
Train: 270 [ 450/1251 ( 36%)]  Loss: 3.477 (3.54)  Time: 0.791s, 1293.95/s  (0.794s, 1289.61/s)  LR: 5.824e-04  Data: 0.010 (0.014)
Train: 270 [ 500/1251 ( 40%)]  Loss: 3.639 (3.55)  Time: 0.776s, 1320.20/s  (0.793s, 1291.81/s)  LR: 5.824e-04  Data: 0.010 (0.013)
Train: 270 [ 550/1251 ( 44%)]  Loss: 3.444 (3.54)  Time: 0.774s, 1323.48/s  (0.791s, 1293.85/s)  LR: 5.824e-04  Data: 0.010 (0.013)
Train: 270 [ 600/1251 ( 48%)]  Loss: 3.576 (3.55)  Time: 0.784s, 1306.51/s  (0.791s, 1295.26/s)  LR: 5.824e-04  Data: 0.010 (0.013)
Train: 270 [ 650/1251 ( 52%)]  Loss: 3.209 (3.52)  Time: 0.827s, 1238.89/s  (0.792s, 1293.42/s)  LR: 5.824e-04  Data: 0.013 (0.013)
Train: 270 [ 700/1251 ( 56%)]  Loss: 3.519 (3.52)  Time: 0.773s, 1324.52/s  (0.791s, 1294.50/s)  LR: 5.824e-04  Data: 0.010 (0.012)
Train: 270 [ 750/1251 ( 60%)]  Loss: 3.515 (3.52)  Time: 0.773s, 1324.95/s  (0.790s, 1295.70/s)  LR: 5.824e-04  Data: 0.009 (0.012)
Train: 270 [ 800/1251 ( 64%)]  Loss: 3.606 (3.53)  Time: 0.773s, 1323.96/s  (0.789s, 1297.21/s)  LR: 5.824e-04  Data: 0.010 (0.012)
Train: 270 [ 850/1251 ( 68%)]  Loss: 3.773 (3.54)  Time: 0.816s, 1254.34/s  (0.789s, 1297.92/s)  LR: 5.824e-04  Data: 0.010 (0.012)
Train: 270 [ 900/1251 ( 72%)]  Loss: 3.230 (3.52)  Time: 0.772s, 1325.99/s  (0.788s, 1299.03/s)  LR: 5.824e-04  Data: 0.010 (0.012)
Train: 270 [ 950/1251 ( 76%)]  Loss: 3.244 (3.51)  Time: 0.804s, 1273.02/s  (0.788s, 1299.02/s)  LR: 5.824e-04  Data: 0.010 (0.012)
Train: 270 [1000/1251 ( 80%)]  Loss: 3.501 (3.51)  Time: 0.773s, 1323.96/s  (0.788s, 1299.61/s)  LR: 5.824e-04  Data: 0.010 (0.012)
Train: 270 [1050/1251 ( 84%)]  Loss: 3.605 (3.51)  Time: 0.784s, 1305.88/s  (0.788s, 1299.59/s)  LR: 5.824e-04  Data: 0.010 (0.012)
Train: 270 [1100/1251 ( 88%)]  Loss: 3.473 (3.51)  Time: 0.782s, 1308.75/s  (0.788s, 1300.19/s)  LR: 5.824e-04  Data: 0.009 (0.011)
Train: 270 [1150/1251 ( 92%)]  Loss: 3.551 (3.51)  Time: 0.774s, 1322.93/s  (0.787s, 1300.95/s)  LR: 5.824e-04  Data: 0.010 (0.011)
Train: 270 [1200/1251 ( 96%)]  Loss: 3.408 (3.51)  Time: 0.772s, 1325.72/s  (0.787s, 1301.02/s)  LR: 5.824e-04  Data: 0.010 (0.011)
Train: 270 [1250/1251 (100%)]  Loss: 3.616 (3.51)  Time: 0.760s, 1346.54/s  (0.787s, 1301.17/s)  LR: 5.824e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.514 (1.514)  Loss:  0.7983 (0.7983)  Acc@1: 88.4766 (88.4766)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.194 (0.558)  Loss:  0.8403 (1.3358)  Acc@1: 84.9057 (74.6980)  Acc@5: 96.9340 (92.7060)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-260.pth.tar', 75.10400008789063)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-268.pth.tar', 75.06800008544921)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-262.pth.tar', 75.05000018066406)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-269.pth.tar', 74.98399998535156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-254.pth.tar', 74.96399992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-267.pth.tar', 74.96200000976563)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-264.pth.tar', 74.89000010742187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-266.pth.tar', 74.85399998291015)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-250.pth.tar', 74.76799987792968)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-270.pth.tar', 74.6980000366211)

Train: 271 [   0/1251 (  0%)]  Loss: 3.282 (3.28)  Time: 2.190s,  467.65/s  (2.190s,  467.65/s)  LR: 5.799e-04  Data: 1.462 (1.462)
Train: 271 [  50/1251 (  4%)]  Loss: 3.668 (3.47)  Time: 0.773s, 1324.38/s  (0.818s, 1251.15/s)  LR: 5.799e-04  Data: 0.009 (0.045)
Train: 271 [ 100/1251 (  8%)]  Loss: 3.311 (3.42)  Time: 0.779s, 1314.46/s  (0.798s, 1283.54/s)  LR: 5.799e-04  Data: 0.010 (0.027)
Train: 271 [ 150/1251 ( 12%)]  Loss: 3.389 (3.41)  Time: 0.810s, 1264.26/s  (0.799s, 1281.48/s)  LR: 5.799e-04  Data: 0.009 (0.022)
Train: 271 [ 200/1251 ( 16%)]  Loss: 3.560 (3.44)  Time: 0.786s, 1302.93/s  (0.796s, 1286.28/s)  LR: 5.799e-04  Data: 0.010 (0.019)
Train: 271 [ 250/1251 ( 20%)]  Loss: 3.584 (3.47)  Time: 0.778s, 1315.75/s  (0.793s, 1291.51/s)  LR: 5.799e-04  Data: 0.010 (0.017)
Train: 271 [ 300/1251 ( 24%)]  Loss: 3.611 (3.49)  Time: 0.809s, 1266.11/s  (0.794s, 1289.64/s)  LR: 5.799e-04  Data: 0.010 (0.016)
Train: 271 [ 350/1251 ( 28%)]  Loss: 3.603 (3.50)  Time: 0.775s, 1320.68/s  (0.793s, 1291.31/s)  LR: 5.799e-04  Data: 0.009 (0.015)
Train: 271 [ 400/1251 ( 32%)]  Loss: 3.803 (3.53)  Time: 0.775s, 1321.57/s  (0.793s, 1291.42/s)  LR: 5.799e-04  Data: 0.009 (0.014)
Train: 271 [ 450/1251 ( 36%)]  Loss: 3.605 (3.54)  Time: 0.874s, 1171.59/s  (0.793s, 1290.83/s)  LR: 5.799e-04  Data: 0.009 (0.014)
Train: 271 [ 500/1251 ( 40%)]  Loss: 3.585 (3.55)  Time: 0.773s, 1324.52/s  (0.792s, 1292.52/s)  LR: 5.799e-04  Data: 0.010 (0.013)
Train: 271 [ 550/1251 ( 44%)]  Loss: 3.734 (3.56)  Time: 0.773s, 1324.55/s  (0.791s, 1294.08/s)  LR: 5.799e-04  Data: 0.009 (0.013)
Train: 271 [ 600/1251 ( 48%)]  Loss: 3.313 (3.54)  Time: 0.774s, 1323.57/s  (0.790s, 1296.05/s)  LR: 5.799e-04  Data: 0.010 (0.013)
Train: 271 [ 650/1251 ( 52%)]  Loss: 3.459 (3.54)  Time: 0.795s, 1287.38/s  (0.790s, 1296.83/s)  LR: 5.799e-04  Data: 0.010 (0.013)
Train: 271 [ 700/1251 ( 56%)]  Loss: 3.417 (3.53)  Time: 0.773s, 1323.97/s  (0.789s, 1297.52/s)  LR: 5.799e-04  Data: 0.010 (0.012)
Train: 271 [ 750/1251 ( 60%)]  Loss: 3.666 (3.54)  Time: 0.814s, 1257.66/s  (0.789s, 1298.34/s)  LR: 5.799e-04  Data: 0.011 (0.012)
Train: 271 [ 800/1251 ( 64%)]  Loss: 3.647 (3.54)  Time: 0.772s, 1327.22/s  (0.788s, 1298.96/s)  LR: 5.799e-04  Data: 0.009 (0.012)
Train: 271 [ 850/1251 ( 68%)]  Loss: 3.708 (3.55)  Time: 0.781s, 1310.71/s  (0.788s, 1299.88/s)  LR: 5.799e-04  Data: 0.012 (0.012)
Train: 271 [ 900/1251 ( 72%)]  Loss: 3.531 (3.55)  Time: 0.816s, 1255.58/s  (0.788s, 1299.67/s)  LR: 5.799e-04  Data: 0.011 (0.012)
Train: 271 [ 950/1251 ( 76%)]  Loss: 3.585 (3.55)  Time: 0.773s, 1324.66/s  (0.788s, 1299.40/s)  LR: 5.799e-04  Data: 0.009 (0.012)
Train: 271 [1000/1251 ( 80%)]  Loss: 3.593 (3.55)  Time: 0.776s, 1319.78/s  (0.788s, 1299.97/s)  LR: 5.799e-04  Data: 0.010 (0.012)
Train: 271 [1050/1251 ( 84%)]  Loss: 3.820 (3.57)  Time: 0.815s, 1256.76/s  (0.789s, 1298.56/s)  LR: 5.799e-04  Data: 0.011 (0.012)
Train: 271 [1100/1251 ( 88%)]  Loss: 3.700 (3.57)  Time: 0.774s, 1322.54/s  (0.789s, 1298.30/s)  LR: 5.799e-04  Data: 0.009 (0.012)
Train: 271 [1150/1251 ( 92%)]  Loss: 3.433 (3.57)  Time: 0.791s, 1294.83/s  (0.788s, 1299.01/s)  LR: 5.799e-04  Data: 0.010 (0.011)
Train: 271 [1200/1251 ( 96%)]  Loss: 3.466 (3.56)  Time: 0.784s, 1305.85/s  (0.788s, 1299.41/s)  LR: 5.799e-04  Data: 0.010 (0.011)
Train: 271 [1250/1251 (100%)]  Loss: 2.984 (3.54)  Time: 0.772s, 1326.86/s  (0.788s, 1298.86/s)  LR: 5.799e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.502 (1.502)  Loss:  0.7612 (0.7612)  Acc@1: 88.3789 (88.3789)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.194 (0.562)  Loss:  0.9673 (1.3080)  Acc@1: 83.2547 (74.7740)  Acc@5: 95.9906 (92.7040)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-260.pth.tar', 75.10400008789063)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-268.pth.tar', 75.06800008544921)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-262.pth.tar', 75.05000018066406)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-269.pth.tar', 74.98399998535156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-254.pth.tar', 74.96399992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-267.pth.tar', 74.96200000976563)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-264.pth.tar', 74.89000010742187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-266.pth.tar', 74.85399998291015)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-271.pth.tar', 74.77400009521484)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-250.pth.tar', 74.76799987792968)

Train: 272 [   0/1251 (  0%)]  Loss: 3.506 (3.51)  Time: 2.218s,  461.77/s  (2.218s,  461.77/s)  LR: 5.773e-04  Data: 1.471 (1.471)
Train: 272 [  50/1251 (  4%)]  Loss: 3.468 (3.49)  Time: 0.776s, 1320.32/s  (0.811s, 1263.39/s)  LR: 5.773e-04  Data: 0.009 (0.043)
Train: 272 [ 100/1251 (  8%)]  Loss: 3.634 (3.54)  Time: 0.778s, 1316.73/s  (0.796s, 1287.22/s)  LR: 5.773e-04  Data: 0.009 (0.026)
Train: 272 [ 150/1251 ( 12%)]  Loss: 3.116 (3.43)  Time: 0.807s, 1268.14/s  (0.794s, 1290.32/s)  LR: 5.773e-04  Data: 0.010 (0.021)
Train: 272 [ 200/1251 ( 16%)]  Loss: 3.796 (3.50)  Time: 0.817s, 1252.98/s  (0.792s, 1293.20/s)  LR: 5.773e-04  Data: 0.009 (0.018)
Train: 272 [ 250/1251 ( 20%)]  Loss: 3.624 (3.52)  Time: 0.771s, 1327.98/s  (0.790s, 1296.32/s)  LR: 5.773e-04  Data: 0.010 (0.016)
Train: 272 [ 300/1251 ( 24%)]  Loss: 3.435 (3.51)  Time: 0.783s, 1307.63/s  (0.788s, 1298.82/s)  LR: 5.773e-04  Data: 0.009 (0.015)
Train: 272 [ 350/1251 ( 28%)]  Loss: 3.346 (3.49)  Time: 0.783s, 1308.38/s  (0.787s, 1300.76/s)  LR: 5.773e-04  Data: 0.009 (0.015)
Train: 272 [ 400/1251 ( 32%)]  Loss: 3.353 (3.48)  Time: 0.773s, 1324.47/s  (0.786s, 1303.07/s)  LR: 5.773e-04  Data: 0.009 (0.014)
Train: 272 [ 450/1251 ( 36%)]  Loss: 3.731 (3.50)  Time: 0.784s, 1305.74/s  (0.785s, 1304.31/s)  LR: 5.773e-04  Data: 0.010 (0.013)
Train: 272 [ 500/1251 ( 40%)]  Loss: 3.771 (3.53)  Time: 0.774s, 1323.47/s  (0.785s, 1305.03/s)  LR: 5.773e-04  Data: 0.010 (0.013)
Train: 272 [ 550/1251 ( 44%)]  Loss: 3.408 (3.52)  Time: 0.774s, 1323.40/s  (0.784s, 1305.55/s)  LR: 5.773e-04  Data: 0.009 (0.013)
Train: 272 [ 600/1251 ( 48%)]  Loss: 3.744 (3.53)  Time: 0.782s, 1309.76/s  (0.784s, 1306.65/s)  LR: 5.773e-04  Data: 0.010 (0.013)
Train: 272 [ 650/1251 ( 52%)]  Loss: 3.876 (3.56)  Time: 0.774s, 1322.80/s  (0.783s, 1306.98/s)  LR: 5.773e-04  Data: 0.010 (0.012)
Train: 272 [ 700/1251 ( 56%)]  Loss: 3.775 (3.57)  Time: 0.772s, 1326.19/s  (0.783s, 1307.53/s)  LR: 5.773e-04  Data: 0.010 (0.012)
Train: 272 [ 750/1251 ( 60%)]  Loss: 3.786 (3.59)  Time: 0.773s, 1324.96/s  (0.783s, 1308.15/s)  LR: 5.773e-04  Data: 0.010 (0.012)
Train: 272 [ 800/1251 ( 64%)]  Loss: 3.351 (3.57)  Time: 0.773s, 1325.47/s  (0.783s, 1308.32/s)  LR: 5.773e-04  Data: 0.009 (0.012)
Train: 272 [ 850/1251 ( 68%)]  Loss: 3.333 (3.56)  Time: 0.774s, 1323.52/s  (0.783s, 1308.56/s)  LR: 5.773e-04  Data: 0.009 (0.012)
Train: 272 [ 900/1251 ( 72%)]  Loss: 3.704 (3.57)  Time: 0.773s, 1324.18/s  (0.782s, 1308.91/s)  LR: 5.773e-04  Data: 0.010 (0.012)
Train: 272 [ 950/1251 ( 76%)]  Loss: 3.751 (3.58)  Time: 0.773s, 1325.35/s  (0.782s, 1309.31/s)  LR: 5.773e-04  Data: 0.009 (0.012)
Train: 272 [1000/1251 ( 80%)]  Loss: 3.558 (3.57)  Time: 0.779s, 1315.09/s  (0.782s, 1309.50/s)  LR: 5.773e-04  Data: 0.010 (0.011)
Train: 272 [1050/1251 ( 84%)]  Loss: 3.841 (3.59)  Time: 0.828s, 1236.70/s  (0.783s, 1308.55/s)  LR: 5.773e-04  Data: 0.013 (0.011)
Train: 272 [1100/1251 ( 88%)]  Loss: 3.453 (3.58)  Time: 0.773s, 1325.49/s  (0.783s, 1307.94/s)  LR: 5.773e-04  Data: 0.009 (0.011)
Train: 272 [1150/1251 ( 92%)]  Loss: 3.455 (3.58)  Time: 0.778s, 1316.52/s  (0.783s, 1308.17/s)  LR: 5.773e-04  Data: 0.009 (0.011)
Train: 272 [1200/1251 ( 96%)]  Loss: 2.856 (3.55)  Time: 0.775s, 1321.94/s  (0.783s, 1308.48/s)  LR: 5.773e-04  Data: 0.011 (0.011)
Train: 272 [1250/1251 (100%)]  Loss: 3.657 (3.55)  Time: 0.759s, 1348.93/s  (0.782s, 1308.88/s)  LR: 5.773e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.567 (1.567)  Loss:  0.7720 (0.7720)  Acc@1: 89.6484 (89.6484)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.194 (0.565)  Loss:  0.9111 (1.3035)  Acc@1: 83.8443 (74.3360)  Acc@5: 95.4009 (92.5060)
Train: 273 [   0/1251 (  0%)]  Loss: 3.462 (3.46)  Time: 2.185s,  468.65/s  (2.185s,  468.65/s)  LR: 5.747e-04  Data: 1.454 (1.454)
Train: 273 [  50/1251 (  4%)]  Loss: 3.728 (3.59)  Time: 0.803s, 1274.95/s  (0.813s, 1259.49/s)  LR: 5.747e-04  Data: 0.009 (0.043)
Train: 273 [ 100/1251 (  8%)]  Loss: 3.537 (3.58)  Time: 0.815s, 1255.75/s  (0.798s, 1282.43/s)  LR: 5.747e-04  Data: 0.009 (0.026)
Train: 273 [ 150/1251 ( 12%)]  Loss: 3.618 (3.59)  Time: 0.772s, 1326.00/s  (0.793s, 1290.73/s)  LR: 5.747e-04  Data: 0.009 (0.021)
Train: 273 [ 200/1251 ( 16%)]  Loss: 3.535 (3.58)  Time: 0.815s, 1256.93/s  (0.795s, 1288.20/s)  LR: 5.747e-04  Data: 0.010 (0.018)
Train: 273 [ 250/1251 ( 20%)]  Loss: 3.767 (3.61)  Time: 0.774s, 1323.73/s  (0.796s, 1286.21/s)  LR: 5.747e-04  Data: 0.010 (0.017)
Train: 273 [ 300/1251 ( 24%)]  Loss: 3.681 (3.62)  Time: 0.775s, 1320.44/s  (0.794s, 1290.01/s)  LR: 5.747e-04  Data: 0.010 (0.015)
Train: 273 [ 350/1251 ( 28%)]  Loss: 3.580 (3.61)  Time: 0.771s, 1328.93/s  (0.791s, 1293.75/s)  LR: 5.747e-04  Data: 0.009 (0.015)
Train: 273 [ 400/1251 ( 32%)]  Loss: 3.506 (3.60)  Time: 0.780s, 1312.48/s  (0.791s, 1294.89/s)  LR: 5.747e-04  Data: 0.010 (0.014)
Train: 273 [ 450/1251 ( 36%)]  Loss: 3.658 (3.61)  Time: 0.773s, 1325.48/s  (0.790s, 1295.48/s)  LR: 5.747e-04  Data: 0.010 (0.014)
Train: 273 [ 500/1251 ( 40%)]  Loss: 3.539 (3.60)  Time: 0.777s, 1318.25/s  (0.789s, 1297.58/s)  LR: 5.747e-04  Data: 0.009 (0.013)
Train: 273 [ 550/1251 ( 44%)]  Loss: 3.625 (3.60)  Time: 0.772s, 1325.60/s  (0.788s, 1298.98/s)  LR: 5.747e-04  Data: 0.009 (0.013)
Train: 273 [ 600/1251 ( 48%)]  Loss: 3.616 (3.60)  Time: 0.888s, 1152.70/s  (0.788s, 1299.96/s)  LR: 5.747e-04  Data: 0.010 (0.013)
Train: 273 [ 650/1251 ( 52%)]  Loss: 3.536 (3.60)  Time: 0.782s, 1308.92/s  (0.787s, 1300.86/s)  LR: 5.747e-04  Data: 0.012 (0.012)
Train: 273 [ 700/1251 ( 56%)]  Loss: 3.357 (3.58)  Time: 0.815s, 1255.89/s  (0.788s, 1299.88/s)  LR: 5.747e-04  Data: 0.011 (0.012)
Train: 273 [ 750/1251 ( 60%)]  Loss: 3.800 (3.60)  Time: 0.791s, 1295.17/s  (0.788s, 1299.94/s)  LR: 5.747e-04  Data: 0.013 (0.012)
Train: 273 [ 800/1251 ( 64%)]  Loss: 3.880 (3.61)  Time: 0.773s, 1324.15/s  (0.787s, 1300.43/s)  LR: 5.747e-04  Data: 0.010 (0.012)
Train: 273 [ 850/1251 ( 68%)]  Loss: 3.532 (3.61)  Time: 0.772s, 1326.94/s  (0.787s, 1300.71/s)  LR: 5.747e-04  Data: 0.010 (0.012)
Train: 273 [ 900/1251 ( 72%)]  Loss: 3.505 (3.60)  Time: 0.818s, 1251.39/s  (0.787s, 1301.15/s)  LR: 5.747e-04  Data: 0.010 (0.012)
Train: 273 [ 950/1251 ( 76%)]  Loss: 3.421 (3.59)  Time: 0.776s, 1319.70/s  (0.787s, 1300.98/s)  LR: 5.747e-04  Data: 0.010 (0.012)
Train: 273 [1000/1251 ( 80%)]  Loss: 3.677 (3.60)  Time: 0.771s, 1328.53/s  (0.787s, 1301.01/s)  LR: 5.747e-04  Data: 0.010 (0.012)
Train: 273 [1050/1251 ( 84%)]  Loss: 3.194 (3.58)  Time: 0.774s, 1322.67/s  (0.787s, 1301.35/s)  LR: 5.747e-04  Data: 0.011 (0.012)
Train: 273 [1100/1251 ( 88%)]  Loss: 3.469 (3.57)  Time: 0.890s, 1150.47/s  (0.787s, 1301.61/s)  LR: 5.747e-04  Data: 0.010 (0.012)
Train: 273 [1150/1251 ( 92%)]  Loss: 3.197 (3.56)  Time: 0.815s, 1255.81/s  (0.787s, 1301.86/s)  LR: 5.747e-04  Data: 0.010 (0.011)
Train: 273 [1200/1251 ( 96%)]  Loss: 3.643 (3.56)  Time: 0.772s, 1326.62/s  (0.786s, 1302.10/s)  LR: 5.747e-04  Data: 0.010 (0.011)
Train: 273 [1250/1251 (100%)]  Loss: 3.730 (3.57)  Time: 0.771s, 1328.94/s  (0.786s, 1302.36/s)  LR: 5.747e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.559 (1.559)  Loss:  0.7544 (0.7544)  Acc@1: 89.0625 (89.0625)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.195 (0.572)  Loss:  0.8857 (1.2659)  Acc@1: 84.9057 (75.0740)  Acc@5: 95.9906 (92.6880)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-260.pth.tar', 75.10400008789063)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-273.pth.tar', 75.0740000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-268.pth.tar', 75.06800008544921)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-262.pth.tar', 75.05000018066406)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-269.pth.tar', 74.98399998535156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-254.pth.tar', 74.96399992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-267.pth.tar', 74.96200000976563)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-264.pth.tar', 74.89000010742187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-266.pth.tar', 74.85399998291015)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-271.pth.tar', 74.77400009521484)

Train: 274 [   0/1251 (  0%)]  Loss: 3.553 (3.55)  Time: 2.216s,  462.11/s  (2.216s,  462.11/s)  LR: 5.722e-04  Data: 1.485 (1.485)
Train: 274 [  50/1251 (  4%)]  Loss: 3.218 (3.39)  Time: 0.807s, 1269.12/s  (0.846s, 1209.93/s)  LR: 5.722e-04  Data: 0.010 (0.044)
Train: 274 [ 100/1251 (  8%)]  Loss: 3.246 (3.34)  Time: 0.774s, 1322.73/s  (0.821s, 1247.11/s)  LR: 5.722e-04  Data: 0.009 (0.027)
Train: 274 [ 150/1251 ( 12%)]  Loss: 3.224 (3.31)  Time: 0.772s, 1326.29/s  (0.806s, 1270.09/s)  LR: 5.722e-04  Data: 0.010 (0.021)
Train: 274 [ 200/1251 ( 16%)]  Loss: 3.481 (3.34)  Time: 0.778s, 1315.86/s  (0.803s, 1275.47/s)  LR: 5.722e-04  Data: 0.011 (0.018)
Train: 274 [ 250/1251 ( 20%)]  Loss: 3.481 (3.37)  Time: 0.829s, 1235.05/s  (0.798s, 1282.69/s)  LR: 5.722e-04  Data: 0.009 (0.017)
Train: 274 [ 300/1251 ( 24%)]  Loss: 3.540 (3.39)  Time: 0.770s, 1329.33/s  (0.795s, 1288.16/s)  LR: 5.722e-04  Data: 0.010 (0.016)
Train: 274 [ 350/1251 ( 28%)]  Loss: 3.085 (3.35)  Time: 0.772s, 1326.19/s  (0.793s, 1291.71/s)  LR: 5.722e-04  Data: 0.009 (0.015)
Train: 274 [ 400/1251 ( 32%)]  Loss: 3.501 (3.37)  Time: 0.775s, 1321.25/s  (0.791s, 1295.22/s)  LR: 5.722e-04  Data: 0.011 (0.014)
Train: 274 [ 450/1251 ( 36%)]  Loss: 3.708 (3.40)  Time: 0.784s, 1305.34/s  (0.789s, 1297.25/s)  LR: 5.722e-04  Data: 0.010 (0.014)
Train: 274 [ 500/1251 ( 40%)]  Loss: 3.660 (3.43)  Time: 0.776s, 1319.12/s  (0.788s, 1298.83/s)  LR: 5.722e-04  Data: 0.010 (0.013)
Train: 274 [ 550/1251 ( 44%)]  Loss: 3.402 (3.42)  Time: 0.775s, 1321.83/s  (0.788s, 1299.94/s)  LR: 5.722e-04  Data: 0.010 (0.013)
Train: 274 [ 600/1251 ( 48%)]  Loss: 3.435 (3.43)  Time: 0.784s, 1305.45/s  (0.787s, 1301.10/s)  LR: 5.722e-04  Data: 0.010 (0.013)
Train: 274 [ 650/1251 ( 52%)]  Loss: 3.335 (3.42)  Time: 0.795s, 1288.28/s  (0.787s, 1301.70/s)  LR: 5.722e-04  Data: 0.009 (0.013)
Train: 274 [ 700/1251 ( 56%)]  Loss: 3.685 (3.44)  Time: 0.777s, 1318.40/s  (0.786s, 1302.48/s)  LR: 5.722e-04  Data: 0.010 (0.012)
Train: 274 [ 750/1251 ( 60%)]  Loss: 3.383 (3.43)  Time: 0.773s, 1325.12/s  (0.786s, 1303.51/s)  LR: 5.722e-04  Data: 0.011 (0.012)
Train: 274 [ 800/1251 ( 64%)]  Loss: 3.482 (3.44)  Time: 0.774s, 1323.33/s  (0.785s, 1304.48/s)  LR: 5.722e-04  Data: 0.010 (0.012)
Train: 274 [ 850/1251 ( 68%)]  Loss: 3.814 (3.46)  Time: 0.774s, 1322.24/s  (0.785s, 1305.02/s)  LR: 5.722e-04  Data: 0.010 (0.012)
Train: 274 [ 900/1251 ( 72%)]  Loss: 3.693 (3.47)  Time: 0.782s, 1309.05/s  (0.785s, 1305.03/s)  LR: 5.722e-04  Data: 0.010 (0.012)
Train: 274 [ 950/1251 ( 76%)]  Loss: 3.228 (3.46)  Time: 0.778s, 1315.47/s  (0.785s, 1305.21/s)  LR: 5.722e-04  Data: 0.010 (0.012)
Train: 274 [1000/1251 ( 80%)]  Loss: 3.043 (3.44)  Time: 0.773s, 1324.35/s  (0.784s, 1305.45/s)  LR: 5.722e-04  Data: 0.009 (0.012)
Train: 274 [1050/1251 ( 84%)]  Loss: 3.636 (3.45)  Time: 0.783s, 1308.61/s  (0.784s, 1306.06/s)  LR: 5.722e-04  Data: 0.012 (0.011)
Train: 274 [1100/1251 ( 88%)]  Loss: 3.158 (3.43)  Time: 0.775s, 1321.24/s  (0.784s, 1306.17/s)  LR: 5.722e-04  Data: 0.010 (0.011)
Train: 274 [1150/1251 ( 92%)]  Loss: 3.514 (3.44)  Time: 0.825s, 1241.81/s  (0.785s, 1303.65/s)  LR: 5.722e-04  Data: 0.013 (0.011)
Train: 274 [1200/1251 ( 96%)]  Loss: 3.378 (3.44)  Time: 0.826s, 1239.53/s  (0.787s, 1301.05/s)  LR: 5.722e-04  Data: 0.013 (0.011)
Train: 274 [1250/1251 (100%)]  Loss: 3.421 (3.43)  Time: 0.760s, 1346.66/s  (0.788s, 1299.80/s)  LR: 5.722e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.556 (1.556)  Loss:  0.7085 (0.7085)  Acc@1: 90.0391 (90.0391)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.194 (0.562)  Loss:  0.8379 (1.2417)  Acc@1: 84.7877 (75.4680)  Acc@5: 96.1085 (92.9420)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-274.pth.tar', 75.46800011474609)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-260.pth.tar', 75.10400008789063)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-273.pth.tar', 75.0740000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-268.pth.tar', 75.06800008544921)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-262.pth.tar', 75.05000018066406)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-269.pth.tar', 74.98399998535156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-254.pth.tar', 74.96399992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-267.pth.tar', 74.96200000976563)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-264.pth.tar', 74.89000010742187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-266.pth.tar', 74.85399998291015)

Train: 275 [   0/1251 (  0%)]  Loss: 3.747 (3.75)  Time: 2.290s,  447.23/s  (2.290s,  447.23/s)  LR: 5.696e-04  Data: 1.534 (1.534)
Train: 275 [  50/1251 (  4%)]  Loss: 3.468 (3.61)  Time: 0.773s, 1325.35/s  (0.814s, 1258.50/s)  LR: 5.696e-04  Data: 0.009 (0.042)
Train: 275 [ 100/1251 (  8%)]  Loss: 3.708 (3.64)  Time: 0.772s, 1326.21/s  (0.797s, 1284.81/s)  LR: 5.696e-04  Data: 0.010 (0.026)
Train: 275 [ 150/1251 ( 12%)]  Loss: 3.477 (3.60)  Time: 0.896s, 1142.54/s  (0.792s, 1292.23/s)  LR: 5.696e-04  Data: 0.009 (0.020)
Train: 275 [ 200/1251 ( 16%)]  Loss: 3.619 (3.60)  Time: 0.778s, 1316.63/s  (0.789s, 1297.60/s)  LR: 5.696e-04  Data: 0.009 (0.018)
Train: 275 [ 250/1251 ( 20%)]  Loss: 3.521 (3.59)  Time: 0.772s, 1326.99/s  (0.790s, 1295.90/s)  LR: 5.696e-04  Data: 0.010 (0.016)
Train: 275 [ 300/1251 ( 24%)]  Loss: 3.297 (3.55)  Time: 0.784s, 1306.03/s  (0.789s, 1298.43/s)  LR: 5.696e-04  Data: 0.010 (0.015)
Train: 275 [ 350/1251 ( 28%)]  Loss: 3.503 (3.54)  Time: 0.772s, 1326.59/s  (0.787s, 1301.08/s)  LR: 5.696e-04  Data: 0.010 (0.014)
Train: 275 [ 400/1251 ( 32%)]  Loss: 2.984 (3.48)  Time: 0.773s, 1325.13/s  (0.788s, 1299.47/s)  LR: 5.696e-04  Data: 0.009 (0.014)
Train: 275 [ 450/1251 ( 36%)]  Loss: 3.654 (3.50)  Time: 0.808s, 1266.75/s  (0.789s, 1297.67/s)  LR: 5.696e-04  Data: 0.010 (0.013)
Train: 275 [ 500/1251 ( 40%)]  Loss: 3.330 (3.48)  Time: 0.774s, 1323.73/s  (0.789s, 1297.22/s)  LR: 5.696e-04  Data: 0.010 (0.013)
Train: 275 [ 550/1251 ( 44%)]  Loss: 3.315 (3.47)  Time: 0.783s, 1307.06/s  (0.789s, 1298.62/s)  LR: 5.696e-04  Data: 0.009 (0.013)
Train: 275 [ 600/1251 ( 48%)]  Loss: 3.879 (3.50)  Time: 0.778s, 1316.16/s  (0.788s, 1298.98/s)  LR: 5.696e-04  Data: 0.010 (0.012)
Train: 275 [ 650/1251 ( 52%)]  Loss: 3.288 (3.49)  Time: 0.771s, 1327.38/s  (0.788s, 1299.76/s)  LR: 5.696e-04  Data: 0.009 (0.012)
Train: 275 [ 700/1251 ( 56%)]  Loss: 3.341 (3.48)  Time: 0.772s, 1326.50/s  (0.787s, 1300.81/s)  LR: 5.696e-04  Data: 0.010 (0.012)
Train: 275 [ 750/1251 ( 60%)]  Loss: 3.471 (3.48)  Time: 0.773s, 1325.54/s  (0.787s, 1301.39/s)  LR: 5.696e-04  Data: 0.009 (0.012)
Train: 275 [ 800/1251 ( 64%)]  Loss: 3.367 (3.47)  Time: 0.788s, 1300.18/s  (0.787s, 1301.75/s)  LR: 5.696e-04  Data: 0.009 (0.012)
Train: 275 [ 850/1251 ( 68%)]  Loss: 3.464 (3.47)  Time: 0.772s, 1326.14/s  (0.786s, 1302.51/s)  LR: 5.696e-04  Data: 0.009 (0.012)
Train: 275 [ 900/1251 ( 72%)]  Loss: 3.219 (3.46)  Time: 0.772s, 1326.98/s  (0.787s, 1301.29/s)  LR: 5.696e-04  Data: 0.010 (0.012)
Train: 275 [ 950/1251 ( 76%)]  Loss: 3.598 (3.46)  Time: 0.772s, 1325.97/s  (0.787s, 1301.84/s)  LR: 5.696e-04  Data: 0.009 (0.011)
Train: 275 [1000/1251 ( 80%)]  Loss: 3.337 (3.46)  Time: 0.774s, 1322.78/s  (0.787s, 1301.75/s)  LR: 5.696e-04  Data: 0.009 (0.011)
Train: 275 [1050/1251 ( 84%)]  Loss: 2.972 (3.43)  Time: 0.772s, 1326.25/s  (0.786s, 1302.20/s)  LR: 5.696e-04  Data: 0.009 (0.011)
Train: 275 [1100/1251 ( 88%)]  Loss: 3.674 (3.44)  Time: 0.784s, 1306.80/s  (0.786s, 1302.88/s)  LR: 5.696e-04  Data: 0.012 (0.011)
Train: 275 [1150/1251 ( 92%)]  Loss: 3.465 (3.45)  Time: 0.769s, 1331.20/s  (0.786s, 1303.48/s)  LR: 5.696e-04  Data: 0.009 (0.011)
Train: 275 [1200/1251 ( 96%)]  Loss: 3.538 (3.45)  Time: 0.772s, 1326.75/s  (0.785s, 1303.85/s)  LR: 5.696e-04  Data: 0.010 (0.011)
Train: 275 [1250/1251 (100%)]  Loss: 3.550 (3.45)  Time: 0.761s, 1345.74/s  (0.785s, 1304.20/s)  LR: 5.696e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.527 (1.527)  Loss:  0.8252 (0.8252)  Acc@1: 89.4531 (89.4531)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.194 (0.576)  Loss:  0.8770 (1.3439)  Acc@1: 84.3160 (75.1180)  Acc@5: 96.3443 (92.8700)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-274.pth.tar', 75.46800011474609)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-275.pth.tar', 75.11800016845703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-260.pth.tar', 75.10400008789063)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-273.pth.tar', 75.0740000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-268.pth.tar', 75.06800008544921)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-262.pth.tar', 75.05000018066406)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-269.pth.tar', 74.98399998535156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-254.pth.tar', 74.96399992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-267.pth.tar', 74.96200000976563)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-264.pth.tar', 74.89000010742187)

Train: 276 [   0/1251 (  0%)]  Loss: 3.727 (3.73)  Time: 2.299s,  445.47/s  (2.299s,  445.47/s)  LR: 5.670e-04  Data: 1.564 (1.564)
Train: 276 [  50/1251 (  4%)]  Loss: 3.149 (3.44)  Time: 0.774s, 1322.90/s  (0.827s, 1238.33/s)  LR: 5.670e-04  Data: 0.010 (0.047)
Train: 276 [ 100/1251 (  8%)]  Loss: 3.330 (3.40)  Time: 0.772s, 1326.27/s  (0.803s, 1275.96/s)  LR: 5.670e-04  Data: 0.010 (0.029)
Train: 276 [ 150/1251 ( 12%)]  Loss: 3.535 (3.44)  Time: 0.777s, 1318.14/s  (0.797s, 1285.14/s)  LR: 5.670e-04  Data: 0.009 (0.022)
Train: 276 [ 200/1251 ( 16%)]  Loss: 3.233 (3.39)  Time: 0.808s, 1267.30/s  (0.798s, 1283.59/s)  LR: 5.670e-04  Data: 0.009 (0.019)
Train: 276 [ 250/1251 ( 20%)]  Loss: 3.257 (3.37)  Time: 0.774s, 1322.76/s  (0.795s, 1287.60/s)  LR: 5.670e-04  Data: 0.009 (0.017)
Train: 276 [ 300/1251 ( 24%)]  Loss: 3.295 (3.36)  Time: 0.773s, 1324.50/s  (0.792s, 1292.73/s)  LR: 5.670e-04  Data: 0.010 (0.016)
Train: 276 [ 350/1251 ( 28%)]  Loss: 3.906 (3.43)  Time: 0.779s, 1314.29/s  (0.790s, 1296.25/s)  LR: 5.670e-04  Data: 0.010 (0.015)
Train: 276 [ 400/1251 ( 32%)]  Loss: 3.406 (3.43)  Time: 0.773s, 1324.50/s  (0.789s, 1298.61/s)  LR: 5.670e-04  Data: 0.010 (0.014)
Train: 276 [ 450/1251 ( 36%)]  Loss: 3.469 (3.43)  Time: 0.817s, 1254.08/s  (0.788s, 1299.29/s)  LR: 5.670e-04  Data: 0.010 (0.014)
Train: 276 [ 500/1251 ( 40%)]  Loss: 3.521 (3.44)  Time: 0.776s, 1318.75/s  (0.790s, 1296.58/s)  LR: 5.670e-04  Data: 0.010 (0.014)
Train: 276 [ 550/1251 ( 44%)]  Loss: 3.696 (3.46)  Time: 0.773s, 1324.39/s  (0.789s, 1298.18/s)  LR: 5.670e-04  Data: 0.009 (0.013)
Train: 276 [ 600/1251 ( 48%)]  Loss: 3.633 (3.47)  Time: 0.780s, 1313.53/s  (0.788s, 1299.95/s)  LR: 5.670e-04  Data: 0.010 (0.013)
Train: 276 [ 650/1251 ( 52%)]  Loss: 3.765 (3.49)  Time: 0.784s, 1306.22/s  (0.787s, 1300.90/s)  LR: 5.670e-04  Data: 0.010 (0.013)
Train: 276 [ 700/1251 ( 56%)]  Loss: 3.598 (3.50)  Time: 0.772s, 1326.24/s  (0.786s, 1301.99/s)  LR: 5.670e-04  Data: 0.010 (0.012)
Train: 276 [ 750/1251 ( 60%)]  Loss: 3.593 (3.51)  Time: 0.772s, 1326.76/s  (0.786s, 1302.81/s)  LR: 5.670e-04  Data: 0.009 (0.012)
Train: 276 [ 800/1251 ( 64%)]  Loss: 3.413 (3.50)  Time: 0.776s, 1319.85/s  (0.786s, 1303.42/s)  LR: 5.670e-04  Data: 0.009 (0.012)
Train: 276 [ 850/1251 ( 68%)]  Loss: 3.721 (3.51)  Time: 0.817s, 1252.63/s  (0.786s, 1302.30/s)  LR: 5.670e-04  Data: 0.011 (0.012)
Train: 276 [ 900/1251 ( 72%)]  Loss: 3.290 (3.50)  Time: 0.796s, 1286.36/s  (0.787s, 1300.38/s)  LR: 5.670e-04  Data: 0.009 (0.012)
Train: 276 [ 950/1251 ( 76%)]  Loss: 3.354 (3.49)  Time: 0.816s, 1254.25/s  (0.788s, 1300.24/s)  LR: 5.670e-04  Data: 0.010 (0.012)
Train: 276 [1000/1251 ( 80%)]  Loss: 3.889 (3.51)  Time: 0.775s, 1321.43/s  (0.789s, 1298.63/s)  LR: 5.670e-04  Data: 0.011 (0.012)
Train: 276 [1050/1251 ( 84%)]  Loss: 3.774 (3.53)  Time: 0.774s, 1322.43/s  (0.788s, 1299.18/s)  LR: 5.670e-04  Data: 0.010 (0.012)
Train: 276 [1100/1251 ( 88%)]  Loss: 3.552 (3.53)  Time: 0.773s, 1325.55/s  (0.788s, 1299.94/s)  LR: 5.670e-04  Data: 0.010 (0.012)
Train: 276 [1150/1251 ( 92%)]  Loss: 3.465 (3.52)  Time: 0.774s, 1323.28/s  (0.787s, 1300.57/s)  LR: 5.670e-04  Data: 0.010 (0.011)
Train: 276 [1200/1251 ( 96%)]  Loss: 3.318 (3.52)  Time: 0.778s, 1315.85/s  (0.788s, 1300.22/s)  LR: 5.670e-04  Data: 0.010 (0.011)
Train: 276 [1250/1251 (100%)]  Loss: 3.732 (3.52)  Time: 0.766s, 1337.52/s  (0.787s, 1300.45/s)  LR: 5.670e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.534 (1.534)  Loss:  0.7598 (0.7598)  Acc@1: 89.0625 (89.0625)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.194 (0.557)  Loss:  0.9214 (1.3050)  Acc@1: 83.8443 (75.1920)  Acc@5: 96.3443 (92.7300)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-274.pth.tar', 75.46800011474609)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-276.pth.tar', 75.19200009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-275.pth.tar', 75.11800016845703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-260.pth.tar', 75.10400008789063)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-273.pth.tar', 75.0740000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-268.pth.tar', 75.06800008544921)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-262.pth.tar', 75.05000018066406)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-269.pth.tar', 74.98399998535156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-254.pth.tar', 74.96399992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-267.pth.tar', 74.96200000976563)

Train: 277 [   0/1251 (  0%)]  Loss: 3.473 (3.47)  Time: 2.191s,  467.42/s  (2.191s,  467.42/s)  LR: 5.645e-04  Data: 1.462 (1.462)
Train: 277 [  50/1251 (  4%)]  Loss: 3.153 (3.31)  Time: 0.772s, 1326.60/s  (0.810s, 1263.52/s)  LR: 5.645e-04  Data: 0.009 (0.043)
Train: 277 [ 100/1251 (  8%)]  Loss: 3.673 (3.43)  Time: 0.777s, 1318.73/s  (0.798s, 1283.96/s)  LR: 5.645e-04  Data: 0.010 (0.026)
Train: 277 [ 150/1251 ( 12%)]  Loss: 3.268 (3.39)  Time: 0.773s, 1325.45/s  (0.793s, 1291.56/s)  LR: 5.645e-04  Data: 0.009 (0.021)
Train: 277 [ 200/1251 ( 16%)]  Loss: 3.071 (3.33)  Time: 0.772s, 1327.22/s  (0.791s, 1294.10/s)  LR: 5.645e-04  Data: 0.009 (0.018)
Train: 277 [ 250/1251 ( 20%)]  Loss: 3.529 (3.36)  Time: 0.773s, 1325.11/s  (0.789s, 1297.29/s)  LR: 5.645e-04  Data: 0.009 (0.017)
Train: 277 [ 300/1251 ( 24%)]  Loss: 3.566 (3.39)  Time: 0.772s, 1326.11/s  (0.790s, 1295.98/s)  LR: 5.645e-04  Data: 0.009 (0.015)
Train: 277 [ 350/1251 ( 28%)]  Loss: 3.403 (3.39)  Time: 0.772s, 1326.89/s  (0.789s, 1297.11/s)  LR: 5.645e-04  Data: 0.010 (0.015)
Train: 277 [ 400/1251 ( 32%)]  Loss: 3.383 (3.39)  Time: 0.772s, 1327.01/s  (0.788s, 1299.41/s)  LR: 5.645e-04  Data: 0.010 (0.014)
Train: 277 [ 450/1251 ( 36%)]  Loss: 3.511 (3.40)  Time: 0.773s, 1325.14/s  (0.787s, 1300.76/s)  LR: 5.645e-04  Data: 0.009 (0.014)
Train: 277 [ 500/1251 ( 40%)]  Loss: 3.518 (3.41)  Time: 0.772s, 1325.93/s  (0.786s, 1302.12/s)  LR: 5.645e-04  Data: 0.010 (0.013)
Train: 277 [ 550/1251 ( 44%)]  Loss: 3.246 (3.40)  Time: 0.777s, 1318.07/s  (0.786s, 1303.49/s)  LR: 5.645e-04  Data: 0.010 (0.013)
Train: 277 [ 600/1251 ( 48%)]  Loss: 3.515 (3.41)  Time: 0.883s, 1159.74/s  (0.786s, 1302.62/s)  LR: 5.645e-04  Data: 0.009 (0.013)
Train: 277 [ 650/1251 ( 52%)]  Loss: 3.200 (3.39)  Time: 0.772s, 1326.85/s  (0.786s, 1302.41/s)  LR: 5.645e-04  Data: 0.009 (0.012)
Train: 277 [ 700/1251 ( 56%)]  Loss: 3.595 (3.41)  Time: 0.772s, 1326.27/s  (0.785s, 1303.75/s)  LR: 5.645e-04  Data: 0.010 (0.012)
Train: 277 [ 750/1251 ( 60%)]  Loss: 3.363 (3.40)  Time: 0.809s, 1265.84/s  (0.785s, 1304.59/s)  LR: 5.645e-04  Data: 0.010 (0.012)
Train: 277 [ 800/1251 ( 64%)]  Loss: 3.570 (3.41)  Time: 0.803s, 1275.81/s  (0.785s, 1304.63/s)  LR: 5.645e-04  Data: 0.009 (0.012)
Train: 277 [ 850/1251 ( 68%)]  Loss: 3.283 (3.41)  Time: 0.793s, 1291.39/s  (0.784s, 1305.36/s)  LR: 5.645e-04  Data: 0.012 (0.012)
Train: 277 [ 900/1251 ( 72%)]  Loss: 3.732 (3.42)  Time: 0.814s, 1257.58/s  (0.785s, 1304.42/s)  LR: 5.645e-04  Data: 0.011 (0.012)
Train: 277 [ 950/1251 ( 76%)]  Loss: 3.464 (3.43)  Time: 0.773s, 1323.92/s  (0.785s, 1303.73/s)  LR: 5.645e-04  Data: 0.010 (0.012)
Train: 277 [1000/1251 ( 80%)]  Loss: 3.325 (3.42)  Time: 0.772s, 1326.62/s  (0.786s, 1302.85/s)  LR: 5.645e-04  Data: 0.010 (0.012)
Train: 277 [1050/1251 ( 84%)]  Loss: 3.851 (3.44)  Time: 0.771s, 1327.71/s  (0.786s, 1303.31/s)  LR: 5.645e-04  Data: 0.010 (0.011)
Train: 277 [1100/1251 ( 88%)]  Loss: 3.395 (3.44)  Time: 0.776s, 1319.73/s  (0.785s, 1303.75/s)  LR: 5.645e-04  Data: 0.009 (0.011)
Train: 277 [1150/1251 ( 92%)]  Loss: 3.255 (3.43)  Time: 0.785s, 1305.25/s  (0.785s, 1304.13/s)  LR: 5.645e-04  Data: 0.010 (0.011)
Train: 277 [1200/1251 ( 96%)]  Loss: 3.355 (3.43)  Time: 0.773s, 1324.83/s  (0.785s, 1304.58/s)  LR: 5.645e-04  Data: 0.010 (0.011)
Train: 277 [1250/1251 (100%)]  Loss: 3.688 (3.44)  Time: 0.767s, 1335.27/s  (0.785s, 1304.89/s)  LR: 5.645e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.578 (1.578)  Loss:  0.6890 (0.6890)  Acc@1: 89.0625 (89.0625)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.194 (0.568)  Loss:  0.7852 (1.2619)  Acc@1: 84.9057 (75.2700)  Acc@5: 96.8160 (92.9080)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-274.pth.tar', 75.46800011474609)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-277.pth.tar', 75.2700000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-276.pth.tar', 75.19200009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-275.pth.tar', 75.11800016845703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-260.pth.tar', 75.10400008789063)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-273.pth.tar', 75.0740000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-268.pth.tar', 75.06800008544921)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-262.pth.tar', 75.05000018066406)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-269.pth.tar', 74.98399998535156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-254.pth.tar', 74.96399992919922)

Train: 278 [   0/1251 (  0%)]  Loss: 3.583 (3.58)  Time: 2.372s,  431.70/s  (2.372s,  431.70/s)  LR: 5.619e-04  Data: 1.641 (1.641)
Train: 278 [  50/1251 (  4%)]  Loss: 3.421 (3.50)  Time: 0.778s, 1316.21/s  (0.819s, 1250.23/s)  LR: 5.619e-04  Data: 0.010 (0.047)
Train: 278 [ 100/1251 (  8%)]  Loss: 3.273 (3.43)  Time: 0.813s, 1259.15/s  (0.800s, 1279.69/s)  LR: 5.619e-04  Data: 0.010 (0.028)
Train: 278 [ 150/1251 ( 12%)]  Loss: 3.784 (3.52)  Time: 0.816s, 1254.47/s  (0.795s, 1287.50/s)  LR: 5.619e-04  Data: 0.009 (0.022)
Train: 278 [ 200/1251 ( 16%)]  Loss: 3.733 (3.56)  Time: 0.790s, 1296.33/s  (0.793s, 1290.50/s)  LR: 5.619e-04  Data: 0.009 (0.019)
Train: 278 [ 250/1251 ( 20%)]  Loss: 3.206 (3.50)  Time: 0.772s, 1325.80/s  (0.792s, 1292.42/s)  LR: 5.619e-04  Data: 0.010 (0.017)
Train: 278 [ 300/1251 ( 24%)]  Loss: 3.511 (3.50)  Time: 0.783s, 1308.18/s  (0.790s, 1296.18/s)  LR: 5.619e-04  Data: 0.010 (0.016)
Train: 278 [ 350/1251 ( 28%)]  Loss: 3.671 (3.52)  Time: 0.773s, 1325.42/s  (0.789s, 1298.19/s)  LR: 5.619e-04  Data: 0.010 (0.015)
Train: 278 [ 400/1251 ( 32%)]  Loss: 3.590 (3.53)  Time: 0.774s, 1323.10/s  (0.790s, 1295.93/s)  LR: 5.619e-04  Data: 0.009 (0.015)
Train: 278 [ 450/1251 ( 36%)]  Loss: 3.636 (3.54)  Time: 0.774s, 1323.46/s  (0.791s, 1294.48/s)  LR: 5.619e-04  Data: 0.009 (0.014)
Train: 278 [ 500/1251 ( 40%)]  Loss: 3.598 (3.55)  Time: 0.791s, 1295.22/s  (0.790s, 1295.41/s)  LR: 5.619e-04  Data: 0.010 (0.014)
Train: 278 [ 550/1251 ( 44%)]  Loss: 3.561 (3.55)  Time: 0.774s, 1322.89/s  (0.790s, 1296.37/s)  LR: 5.619e-04  Data: 0.010 (0.013)
Train: 278 [ 600/1251 ( 48%)]  Loss: 3.218 (3.52)  Time: 0.776s, 1319.63/s  (0.789s, 1297.70/s)  LR: 5.619e-04  Data: 0.009 (0.013)
Train: 278 [ 650/1251 ( 52%)]  Loss: 3.713 (3.54)  Time: 0.773s, 1324.81/s  (0.788s, 1299.21/s)  LR: 5.619e-04  Data: 0.009 (0.013)
Train: 278 [ 700/1251 ( 56%)]  Loss: 3.383 (3.53)  Time: 0.774s, 1323.50/s  (0.787s, 1300.38/s)  LR: 5.619e-04  Data: 0.010 (0.012)
Train: 278 [ 750/1251 ( 60%)]  Loss: 3.606 (3.53)  Time: 0.777s, 1318.02/s  (0.787s, 1301.44/s)  LR: 5.619e-04  Data: 0.009 (0.012)
Train: 278 [ 800/1251 ( 64%)]  Loss: 3.418 (3.52)  Time: 0.774s, 1323.75/s  (0.786s, 1302.19/s)  LR: 5.619e-04  Data: 0.009 (0.012)
Train: 278 [ 850/1251 ( 68%)]  Loss: 3.473 (3.52)  Time: 0.775s, 1322.04/s  (0.786s, 1302.76/s)  LR: 5.619e-04  Data: 0.009 (0.012)
Train: 278 [ 900/1251 ( 72%)]  Loss: 3.278 (3.51)  Time: 0.786s, 1302.84/s  (0.785s, 1303.67/s)  LR: 5.619e-04  Data: 0.010 (0.012)
Train: 278 [ 950/1251 ( 76%)]  Loss: 3.584 (3.51)  Time: 0.775s, 1321.10/s  (0.785s, 1304.22/s)  LR: 5.619e-04  Data: 0.010 (0.012)
Train: 278 [1000/1251 ( 80%)]  Loss: 3.657 (3.52)  Time: 0.772s, 1327.10/s  (0.785s, 1304.88/s)  LR: 5.619e-04  Data: 0.009 (0.012)
Train: 278 [1050/1251 ( 84%)]  Loss: 3.588 (3.52)  Time: 0.848s, 1208.01/s  (0.785s, 1305.18/s)  LR: 5.619e-04  Data: 0.013 (0.011)
Train: 278 [1100/1251 ( 88%)]  Loss: 3.596 (3.53)  Time: 0.775s, 1322.00/s  (0.784s, 1305.57/s)  LR: 5.619e-04  Data: 0.011 (0.011)
Train: 278 [1150/1251 ( 92%)]  Loss: 3.289 (3.52)  Time: 0.793s, 1291.86/s  (0.784s, 1305.46/s)  LR: 5.619e-04  Data: 0.011 (0.011)
Train: 278 [1200/1251 ( 96%)]  Loss: 3.241 (3.50)  Time: 0.779s, 1313.69/s  (0.784s, 1305.78/s)  LR: 5.619e-04  Data: 0.010 (0.011)
Train: 278 [1250/1251 (100%)]  Loss: 3.731 (3.51)  Time: 0.764s, 1340.08/s  (0.784s, 1305.37/s)  LR: 5.619e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.516 (1.516)  Loss:  0.7856 (0.7856)  Acc@1: 88.3789 (88.3789)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.194 (0.559)  Loss:  0.9463 (1.3326)  Acc@1: 84.1981 (75.1080)  Acc@5: 96.5802 (92.7040)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-274.pth.tar', 75.46800011474609)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-277.pth.tar', 75.2700000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-276.pth.tar', 75.19200009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-275.pth.tar', 75.11800016845703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-278.pth.tar', 75.10799998779297)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-260.pth.tar', 75.10400008789063)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-273.pth.tar', 75.0740000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-268.pth.tar', 75.06800008544921)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-262.pth.tar', 75.05000018066406)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-269.pth.tar', 74.98399998535156)

Train: 279 [   0/1251 (  0%)]  Loss: 3.202 (3.20)  Time: 2.421s,  422.93/s  (2.421s,  422.93/s)  LR: 5.593e-04  Data: 1.688 (1.688)
Train: 279 [  50/1251 (  4%)]  Loss: 3.582 (3.39)  Time: 0.777s, 1317.67/s  (0.826s, 1239.80/s)  LR: 5.593e-04  Data: 0.010 (0.052)
Train: 279 [ 100/1251 (  8%)]  Loss: 3.015 (3.27)  Time: 0.814s, 1257.34/s  (0.824s, 1242.51/s)  LR: 5.593e-04  Data: 0.011 (0.032)
Train: 279 [ 150/1251 ( 12%)]  Loss: 3.585 (3.35)  Time: 0.774s, 1322.89/s  (0.816s, 1255.46/s)  LR: 5.593e-04  Data: 0.011 (0.025)
Train: 279 [ 200/1251 ( 16%)]  Loss: 3.521 (3.38)  Time: 0.776s, 1320.08/s  (0.808s, 1267.36/s)  LR: 5.593e-04  Data: 0.009 (0.021)
Train: 279 [ 250/1251 ( 20%)]  Loss: 3.612 (3.42)  Time: 0.773s, 1325.28/s  (0.804s, 1273.71/s)  LR: 5.593e-04  Data: 0.009 (0.019)
Train: 279 [ 300/1251 ( 24%)]  Loss: 3.339 (3.41)  Time: 0.772s, 1326.41/s  (0.802s, 1276.38/s)  LR: 5.593e-04  Data: 0.009 (0.018)
Train: 279 [ 350/1251 ( 28%)]  Loss: 3.396 (3.41)  Time: 0.774s, 1322.80/s  (0.800s, 1280.45/s)  LR: 5.593e-04  Data: 0.010 (0.016)
Train: 279 [ 400/1251 ( 32%)]  Loss: 3.201 (3.38)  Time: 0.774s, 1323.72/s  (0.797s, 1284.60/s)  LR: 5.593e-04  Data: 0.010 (0.016)
Train: 279 [ 450/1251 ( 36%)]  Loss: 3.763 (3.42)  Time: 0.773s, 1324.10/s  (0.796s, 1286.00/s)  LR: 5.593e-04  Data: 0.010 (0.015)
Train: 279 [ 500/1251 ( 40%)]  Loss: 3.434 (3.42)  Time: 0.781s, 1311.74/s  (0.795s, 1287.65/s)  LR: 5.593e-04  Data: 0.009 (0.014)
Train: 279 [ 550/1251 ( 44%)]  Loss: 3.749 (3.45)  Time: 0.807s, 1269.34/s  (0.795s, 1288.44/s)  LR: 5.593e-04  Data: 0.009 (0.014)
Train: 279 [ 600/1251 ( 48%)]  Loss: 3.290 (3.44)  Time: 0.773s, 1325.14/s  (0.794s, 1289.29/s)  LR: 5.593e-04  Data: 0.010 (0.014)
Train: 279 [ 650/1251 ( 52%)]  Loss: 3.196 (3.42)  Time: 0.780s, 1313.15/s  (0.793s, 1291.18/s)  LR: 5.593e-04  Data: 0.010 (0.013)
Train: 279 [ 700/1251 ( 56%)]  Loss: 3.397 (3.42)  Time: 0.771s, 1327.47/s  (0.793s, 1290.79/s)  LR: 5.593e-04  Data: 0.009 (0.013)
Train: 279 [ 750/1251 ( 60%)]  Loss: 3.384 (3.42)  Time: 0.845s, 1212.39/s  (0.793s, 1291.04/s)  LR: 5.593e-04  Data: 0.012 (0.013)
Train: 279 [ 800/1251 ( 64%)]  Loss: 2.809 (3.38)  Time: 0.795s, 1288.82/s  (0.792s, 1292.18/s)  LR: 5.593e-04  Data: 0.009 (0.013)
Train: 279 [ 850/1251 ( 68%)]  Loss: 3.428 (3.38)  Time: 0.773s, 1325.06/s  (0.792s, 1293.48/s)  LR: 5.593e-04  Data: 0.011 (0.012)
Train: 279 [ 900/1251 ( 72%)]  Loss: 3.645 (3.40)  Time: 0.774s, 1323.05/s  (0.791s, 1294.47/s)  LR: 5.593e-04  Data: 0.009 (0.012)
Train: 279 [ 950/1251 ( 76%)]  Loss: 3.653 (3.41)  Time: 0.772s, 1326.92/s  (0.791s, 1294.95/s)  LR: 5.593e-04  Data: 0.009 (0.012)
Train: 279 [1000/1251 ( 80%)]  Loss: 3.925 (3.43)  Time: 0.772s, 1325.57/s  (0.790s, 1295.76/s)  LR: 5.593e-04  Data: 0.010 (0.012)
Train: 279 [1050/1251 ( 84%)]  Loss: 3.308 (3.43)  Time: 0.823s, 1244.78/s  (0.790s, 1296.61/s)  LR: 5.593e-04  Data: 0.009 (0.012)
Train: 279 [1100/1251 ( 88%)]  Loss: 3.372 (3.43)  Time: 0.778s, 1315.44/s  (0.789s, 1297.11/s)  LR: 5.593e-04  Data: 0.009 (0.012)
Train: 279 [1150/1251 ( 92%)]  Loss: 3.689 (3.44)  Time: 0.779s, 1313.84/s  (0.790s, 1296.48/s)  LR: 5.593e-04  Data: 0.009 (0.012)
Train: 279 [1200/1251 ( 96%)]  Loss: 3.387 (3.44)  Time: 0.773s, 1325.19/s  (0.789s, 1297.17/s)  LR: 5.593e-04  Data: 0.010 (0.012)
Train: 279 [1250/1251 (100%)]  Loss: 3.688 (3.45)  Time: 0.761s, 1345.12/s  (0.789s, 1297.96/s)  LR: 5.593e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.570 (1.570)  Loss:  0.8066 (0.8066)  Acc@1: 89.7461 (89.7461)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.193 (0.564)  Loss:  0.8853 (1.3140)  Acc@1: 84.6698 (74.9000)  Acc@5: 96.6981 (92.7480)
Train: 280 [   0/1251 (  0%)]  Loss: 3.666 (3.67)  Time: 2.181s,  469.61/s  (2.181s,  469.61/s)  LR: 5.567e-04  Data: 1.452 (1.452)
Train: 280 [  50/1251 (  4%)]  Loss: 3.617 (3.64)  Time: 0.773s, 1324.75/s  (0.820s, 1248.51/s)  LR: 5.567e-04  Data: 0.009 (0.044)
Train: 280 [ 100/1251 (  8%)]  Loss: 3.295 (3.53)  Time: 0.774s, 1322.30/s  (0.799s, 1281.23/s)  LR: 5.567e-04  Data: 0.009 (0.027)
Train: 280 [ 150/1251 ( 12%)]  Loss: 3.468 (3.51)  Time: 0.792s, 1292.67/s  (0.795s, 1288.34/s)  LR: 5.567e-04  Data: 0.009 (0.021)
Train: 280 [ 200/1251 ( 16%)]  Loss: 3.722 (3.55)  Time: 0.775s, 1321.74/s  (0.791s, 1294.56/s)  LR: 5.567e-04  Data: 0.009 (0.018)
Train: 280 [ 250/1251 ( 20%)]  Loss: 3.361 (3.52)  Time: 0.782s, 1309.56/s  (0.791s, 1293.81/s)  LR: 5.567e-04  Data: 0.009 (0.017)
Train: 280 [ 300/1251 ( 24%)]  Loss: 3.499 (3.52)  Time: 0.816s, 1254.83/s  (0.790s, 1296.14/s)  LR: 5.567e-04  Data: 0.009 (0.016)
Train: 280 [ 350/1251 ( 28%)]  Loss: 3.633 (3.53)  Time: 0.815s, 1256.60/s  (0.788s, 1298.69/s)  LR: 5.567e-04  Data: 0.011 (0.015)
Train: 280 [ 400/1251 ( 32%)]  Loss: 3.321 (3.51)  Time: 0.784s, 1305.81/s  (0.789s, 1298.50/s)  LR: 5.567e-04  Data: 0.009 (0.014)
Train: 280 [ 450/1251 ( 36%)]  Loss: 3.586 (3.52)  Time: 0.778s, 1316.66/s  (0.787s, 1300.93/s)  LR: 5.567e-04  Data: 0.010 (0.014)
Train: 280 [ 500/1251 ( 40%)]  Loss: 3.473 (3.51)  Time: 0.772s, 1325.85/s  (0.786s, 1302.83/s)  LR: 5.567e-04  Data: 0.009 (0.013)
Train: 280 [ 550/1251 ( 44%)]  Loss: 3.332 (3.50)  Time: 0.777s, 1317.38/s  (0.786s, 1302.55/s)  LR: 5.567e-04  Data: 0.010 (0.013)
Train: 280 [ 600/1251 ( 48%)]  Loss: 3.806 (3.52)  Time: 0.771s, 1327.88/s  (0.786s, 1302.91/s)  LR: 5.567e-04  Data: 0.009 (0.013)
Train: 280 [ 650/1251 ( 52%)]  Loss: 3.643 (3.53)  Time: 0.777s, 1318.26/s  (0.786s, 1303.05/s)  LR: 5.567e-04  Data: 0.010 (0.013)
Train: 280 [ 700/1251 ( 56%)]  Loss: 3.389 (3.52)  Time: 0.787s, 1301.74/s  (0.785s, 1303.98/s)  LR: 5.567e-04  Data: 0.009 (0.012)
Train: 280 [ 750/1251 ( 60%)]  Loss: 3.295 (3.51)  Time: 0.778s, 1315.51/s  (0.785s, 1304.06/s)  LR: 5.567e-04  Data: 0.010 (0.012)
Train: 280 [ 800/1251 ( 64%)]  Loss: 3.561 (3.51)  Time: 0.777s, 1317.35/s  (0.785s, 1304.81/s)  LR: 5.567e-04  Data: 0.011 (0.012)
Train: 280 [ 850/1251 ( 68%)]  Loss: 3.402 (3.50)  Time: 0.851s, 1203.45/s  (0.785s, 1305.12/s)  LR: 5.567e-04  Data: 0.010 (0.012)
Train: 280 [ 900/1251 ( 72%)]  Loss: 3.697 (3.51)  Time: 0.771s, 1328.70/s  (0.785s, 1304.62/s)  LR: 5.567e-04  Data: 0.009 (0.012)
Train: 280 [ 950/1251 ( 76%)]  Loss: 3.287 (3.50)  Time: 0.772s, 1325.89/s  (0.785s, 1304.81/s)  LR: 5.567e-04  Data: 0.009 (0.012)
Train: 280 [1000/1251 ( 80%)]  Loss: 3.340 (3.50)  Time: 0.771s, 1327.49/s  (0.785s, 1303.77/s)  LR: 5.567e-04  Data: 0.010 (0.012)
Train: 280 [1050/1251 ( 84%)]  Loss: 3.461 (3.49)  Time: 0.774s, 1323.69/s  (0.785s, 1304.28/s)  LR: 5.567e-04  Data: 0.009 (0.011)
Train: 280 [1100/1251 ( 88%)]  Loss: 3.347 (3.49)  Time: 0.814s, 1257.33/s  (0.787s, 1301.20/s)  LR: 5.567e-04  Data: 0.011 (0.011)
Train: 280 [1150/1251 ( 92%)]  Loss: 3.064 (3.47)  Time: 0.772s, 1326.92/s  (0.787s, 1301.56/s)  LR: 5.567e-04  Data: 0.010 (0.011)
Train: 280 [1200/1251 ( 96%)]  Loss: 3.451 (3.47)  Time: 0.783s, 1307.95/s  (0.788s, 1300.11/s)  LR: 5.567e-04  Data: 0.009 (0.011)
Train: 280 [1250/1251 (100%)]  Loss: 3.442 (3.47)  Time: 0.758s, 1350.06/s  (0.787s, 1300.58/s)  LR: 5.567e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.528 (1.528)  Loss:  0.6807 (0.6807)  Acc@1: 89.9414 (89.9414)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.193 (0.560)  Loss:  0.8560 (1.3066)  Acc@1: 85.0236 (74.9920)  Acc@5: 96.3443 (92.7140)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-274.pth.tar', 75.46800011474609)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-277.pth.tar', 75.2700000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-276.pth.tar', 75.19200009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-275.pth.tar', 75.11800016845703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-278.pth.tar', 75.10799998779297)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-260.pth.tar', 75.10400008789063)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-273.pth.tar', 75.0740000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-268.pth.tar', 75.06800008544921)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-262.pth.tar', 75.05000018066406)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-280.pth.tar', 74.99200008789063)

Train: 281 [   0/1251 (  0%)]  Loss: 3.664 (3.66)  Time: 2.234s,  458.38/s  (2.234s,  458.38/s)  LR: 5.542e-04  Data: 1.500 (1.500)
Train: 281 [  50/1251 (  4%)]  Loss: 3.788 (3.73)  Time: 0.774s, 1322.92/s  (0.834s, 1228.40/s)  LR: 5.542e-04  Data: 0.010 (0.047)
Train: 281 [ 100/1251 (  8%)]  Loss: 3.568 (3.67)  Time: 0.772s, 1325.83/s  (0.806s, 1269.78/s)  LR: 5.542e-04  Data: 0.009 (0.029)
Train: 281 [ 150/1251 ( 12%)]  Loss: 3.353 (3.59)  Time: 0.774s, 1323.15/s  (0.797s, 1285.08/s)  LR: 5.542e-04  Data: 0.010 (0.023)
Train: 281 [ 200/1251 ( 16%)]  Loss: 3.619 (3.60)  Time: 0.785s, 1304.09/s  (0.794s, 1289.12/s)  LR: 5.542e-04  Data: 0.010 (0.019)
Train: 281 [ 250/1251 ( 20%)]  Loss: 3.367 (3.56)  Time: 0.773s, 1325.28/s  (0.791s, 1294.32/s)  LR: 5.542e-04  Data: 0.010 (0.018)
Train: 281 [ 300/1251 ( 24%)]  Loss: 3.588 (3.56)  Time: 0.773s, 1325.29/s  (0.789s, 1297.86/s)  LR: 5.542e-04  Data: 0.010 (0.016)
Train: 281 [ 350/1251 ( 28%)]  Loss: 3.469 (3.55)  Time: 0.816s, 1255.20/s  (0.790s, 1296.94/s)  LR: 5.542e-04  Data: 0.011 (0.015)
Train: 281 [ 400/1251 ( 32%)]  Loss: 3.565 (3.55)  Time: 0.816s, 1254.43/s  (0.794s, 1289.85/s)  LR: 5.542e-04  Data: 0.011 (0.015)
Train: 281 [ 450/1251 ( 36%)]  Loss: 3.792 (3.58)  Time: 0.773s, 1324.12/s  (0.793s, 1291.97/s)  LR: 5.542e-04  Data: 0.010 (0.014)
Train: 281 [ 500/1251 ( 40%)]  Loss: 3.524 (3.57)  Time: 0.769s, 1332.19/s  (0.793s, 1291.11/s)  LR: 5.542e-04  Data: 0.009 (0.014)
Train: 281 [ 550/1251 ( 44%)]  Loss: 3.532 (3.57)  Time: 0.781s, 1310.88/s  (0.792s, 1292.78/s)  LR: 5.542e-04  Data: 0.011 (0.014)
Train: 281 [ 600/1251 ( 48%)]  Loss: 3.223 (3.54)  Time: 0.807s, 1268.24/s  (0.792s, 1293.07/s)  LR: 5.542e-04  Data: 0.009 (0.013)
Train: 281 [ 650/1251 ( 52%)]  Loss: 3.577 (3.54)  Time: 0.772s, 1325.83/s  (0.791s, 1294.92/s)  LR: 5.542e-04  Data: 0.010 (0.013)
Train: 281 [ 700/1251 ( 56%)]  Loss: 3.324 (3.53)  Time: 0.772s, 1326.08/s  (0.790s, 1296.22/s)  LR: 5.542e-04  Data: 0.010 (0.013)
Train: 281 [ 750/1251 ( 60%)]  Loss: 3.386 (3.52)  Time: 0.773s, 1323.90/s  (0.790s, 1296.74/s)  LR: 5.542e-04  Data: 0.010 (0.013)
Train: 281 [ 800/1251 ( 64%)]  Loss: 3.705 (3.53)  Time: 0.788s, 1300.22/s  (0.789s, 1297.74/s)  LR: 5.542e-04  Data: 0.010 (0.012)
Train: 281 [ 850/1251 ( 68%)]  Loss: 3.377 (3.52)  Time: 0.772s, 1325.69/s  (0.788s, 1298.68/s)  LR: 5.542e-04  Data: 0.009 (0.012)
Train: 281 [ 900/1251 ( 72%)]  Loss: 3.691 (3.53)  Time: 0.774s, 1323.07/s  (0.788s, 1299.46/s)  LR: 5.542e-04  Data: 0.010 (0.012)
Train: 281 [ 950/1251 ( 76%)]  Loss: 3.454 (3.53)  Time: 0.777s, 1318.46/s  (0.788s, 1298.88/s)  LR: 5.542e-04  Data: 0.010 (0.012)
Train: 281 [1000/1251 ( 80%)]  Loss: 3.595 (3.53)  Time: 0.772s, 1326.21/s  (0.788s, 1299.31/s)  LR: 5.542e-04  Data: 0.009 (0.012)
Train: 281 [1050/1251 ( 84%)]  Loss: 3.430 (3.53)  Time: 0.772s, 1325.91/s  (0.788s, 1300.08/s)  LR: 5.542e-04  Data: 0.010 (0.012)
Train: 281 [1100/1251 ( 88%)]  Loss: 3.112 (3.51)  Time: 0.773s, 1324.29/s  (0.788s, 1299.98/s)  LR: 5.542e-04  Data: 0.010 (0.012)
Train: 281 [1150/1251 ( 92%)]  Loss: 3.193 (3.50)  Time: 0.771s, 1327.81/s  (0.787s, 1300.62/s)  LR: 5.542e-04  Data: 0.010 (0.012)
Train: 281 [1200/1251 ( 96%)]  Loss: 3.214 (3.48)  Time: 0.815s, 1256.36/s  (0.787s, 1300.49/s)  LR: 5.542e-04  Data: 0.011 (0.012)
Train: 281 [1250/1251 (100%)]  Loss: 3.499 (3.48)  Time: 0.761s, 1345.62/s  (0.788s, 1299.91/s)  LR: 5.542e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.544 (1.544)  Loss:  0.8555 (0.8555)  Acc@1: 89.0625 (89.0625)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.194 (0.560)  Loss:  0.9258 (1.3113)  Acc@1: 83.9623 (75.2740)  Acc@5: 96.1085 (92.7760)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-274.pth.tar', 75.46800011474609)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-281.pth.tar', 75.27400001464844)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-277.pth.tar', 75.2700000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-276.pth.tar', 75.19200009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-275.pth.tar', 75.11800016845703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-278.pth.tar', 75.10799998779297)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-260.pth.tar', 75.10400008789063)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-273.pth.tar', 75.0740000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-268.pth.tar', 75.06800008544921)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-262.pth.tar', 75.05000018066406)

Train: 282 [   0/1251 (  0%)]  Loss: 3.397 (3.40)  Time: 2.216s,  462.02/s  (2.216s,  462.02/s)  LR: 5.516e-04  Data: 1.489 (1.489)
Train: 282 [  50/1251 (  4%)]  Loss: 3.473 (3.43)  Time: 0.846s, 1210.98/s  (0.828s, 1237.23/s)  LR: 5.516e-04  Data: 0.010 (0.048)
Train: 282 [ 100/1251 (  8%)]  Loss: 3.210 (3.36)  Time: 0.774s, 1323.04/s  (0.805s, 1271.70/s)  LR: 5.516e-04  Data: 0.009 (0.029)
Train: 282 [ 150/1251 ( 12%)]  Loss: 3.510 (3.40)  Time: 0.819s, 1250.25/s  (0.798s, 1283.03/s)  LR: 5.516e-04  Data: 0.011 (0.023)
Train: 282 [ 200/1251 ( 16%)]  Loss: 3.327 (3.38)  Time: 0.771s, 1328.20/s  (0.799s, 1281.19/s)  LR: 5.516e-04  Data: 0.010 (0.020)
Train: 282 [ 250/1251 ( 20%)]  Loss: 3.331 (3.37)  Time: 0.775s, 1322.02/s  (0.795s, 1288.09/s)  LR: 5.516e-04  Data: 0.009 (0.018)
Train: 282 [ 300/1251 ( 24%)]  Loss: 3.709 (3.42)  Time: 0.772s, 1325.73/s  (0.792s, 1292.72/s)  LR: 5.516e-04  Data: 0.010 (0.017)
Train: 282 [ 350/1251 ( 28%)]  Loss: 3.301 (3.41)  Time: 0.774s, 1322.29/s  (0.791s, 1294.77/s)  LR: 5.516e-04  Data: 0.010 (0.016)
Train: 282 [ 400/1251 ( 32%)]  Loss: 3.344 (3.40)  Time: 0.815s, 1257.11/s  (0.792s, 1293.00/s)  LR: 5.516e-04  Data: 0.011 (0.015)
Train: 282 [ 450/1251 ( 36%)]  Loss: 3.417 (3.40)  Time: 0.776s, 1318.98/s  (0.792s, 1293.62/s)  LR: 5.516e-04  Data: 0.010 (0.014)
Train: 282 [ 500/1251 ( 40%)]  Loss: 3.477 (3.41)  Time: 0.774s, 1323.31/s  (0.790s, 1296.25/s)  LR: 5.516e-04  Data: 0.009 (0.014)
Train: 282 [ 550/1251 ( 44%)]  Loss: 3.228 (3.39)  Time: 0.777s, 1318.22/s  (0.789s, 1297.47/s)  LR: 5.516e-04  Data: 0.010 (0.014)
Train: 282 [ 600/1251 ( 48%)]  Loss: 3.603 (3.41)  Time: 0.775s, 1321.93/s  (0.789s, 1298.62/s)  LR: 5.516e-04  Data: 0.010 (0.013)
Train: 282 [ 650/1251 ( 52%)]  Loss: 3.410 (3.41)  Time: 0.773s, 1324.54/s  (0.789s, 1298.05/s)  LR: 5.516e-04  Data: 0.009 (0.013)
Train: 282 [ 700/1251 ( 56%)]  Loss: 3.378 (3.41)  Time: 0.773s, 1323.88/s  (0.788s, 1299.28/s)  LR: 5.516e-04  Data: 0.010 (0.013)
Train: 282 [ 750/1251 ( 60%)]  Loss: 3.784 (3.43)  Time: 0.780s, 1313.33/s  (0.788s, 1299.05/s)  LR: 5.516e-04  Data: 0.010 (0.013)
Train: 282 [ 800/1251 ( 64%)]  Loss: 3.378 (3.43)  Time: 0.774s, 1323.16/s  (0.788s, 1299.97/s)  LR: 5.516e-04  Data: 0.009 (0.012)
Train: 282 [ 850/1251 ( 68%)]  Loss: 3.355 (3.42)  Time: 0.776s, 1319.51/s  (0.787s, 1301.08/s)  LR: 5.516e-04  Data: 0.009 (0.012)
Train: 282 [ 900/1251 ( 72%)]  Loss: 3.411 (3.42)  Time: 0.777s, 1317.74/s  (0.787s, 1300.64/s)  LR: 5.516e-04  Data: 0.009 (0.012)
Train: 282 [ 950/1251 ( 76%)]  Loss: 3.360 (3.42)  Time: 0.774s, 1323.42/s  (0.787s, 1300.74/s)  LR: 5.516e-04  Data: 0.010 (0.012)
Train: 282 [1000/1251 ( 80%)]  Loss: 3.369 (3.42)  Time: 0.829s, 1235.47/s  (0.788s, 1299.43/s)  LR: 5.516e-04  Data: 0.010 (0.012)
Train: 282 [1050/1251 ( 84%)]  Loss: 3.466 (3.42)  Time: 0.774s, 1323.74/s  (0.788s, 1299.45/s)  LR: 5.516e-04  Data: 0.009 (0.012)
Train: 282 [1100/1251 ( 88%)]  Loss: 3.477 (3.42)  Time: 0.781s, 1310.74/s  (0.788s, 1299.86/s)  LR: 5.516e-04  Data: 0.012 (0.012)
Train: 282 [1150/1251 ( 92%)]  Loss: 3.702 (3.43)  Time: 0.776s, 1318.87/s  (0.787s, 1300.32/s)  LR: 5.516e-04  Data: 0.010 (0.012)
Train: 282 [1200/1251 ( 96%)]  Loss: 3.707 (3.44)  Time: 0.779s, 1314.77/s  (0.788s, 1299.69/s)  LR: 5.516e-04  Data: 0.010 (0.012)
Train: 282 [1250/1251 (100%)]  Loss: 3.230 (3.44)  Time: 0.764s, 1340.97/s  (0.788s, 1299.88/s)  LR: 5.516e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.561 (1.561)  Loss:  0.8623 (0.8623)  Acc@1: 88.5742 (88.5742)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.194 (0.558)  Loss:  0.7939 (1.3072)  Acc@1: 85.7311 (74.9300)  Acc@5: 96.4623 (92.6820)
Train: 283 [   0/1251 (  0%)]  Loss: 3.232 (3.23)  Time: 2.375s,  431.19/s  (2.375s,  431.19/s)  LR: 5.490e-04  Data: 1.643 (1.643)
Train: 283 [  50/1251 (  4%)]  Loss: 3.405 (3.32)  Time: 0.772s, 1326.83/s  (0.817s, 1252.72/s)  LR: 5.490e-04  Data: 0.009 (0.047)
Train: 283 [ 100/1251 (  8%)]  Loss: 3.538 (3.39)  Time: 0.773s, 1323.97/s  (0.799s, 1281.86/s)  LR: 5.490e-04  Data: 0.009 (0.028)
Train: 283 [ 150/1251 ( 12%)]  Loss: 3.505 (3.42)  Time: 0.880s, 1163.23/s  (0.793s, 1291.61/s)  LR: 5.490e-04  Data: 0.010 (0.022)
Train: 283 [ 200/1251 ( 16%)]  Loss: 3.548 (3.45)  Time: 0.772s, 1325.65/s  (0.792s, 1292.57/s)  LR: 5.490e-04  Data: 0.010 (0.019)
Train: 283 [ 250/1251 ( 20%)]  Loss: 3.773 (3.50)  Time: 0.773s, 1325.26/s  (0.789s, 1297.19/s)  LR: 5.490e-04  Data: 0.009 (0.017)
Train: 283 [ 300/1251 ( 24%)]  Loss: 3.817 (3.55)  Time: 0.774s, 1323.18/s  (0.788s, 1299.96/s)  LR: 5.490e-04  Data: 0.010 (0.016)
Train: 283 [ 350/1251 ( 28%)]  Loss: 3.457 (3.53)  Time: 0.774s, 1323.34/s  (0.786s, 1302.78/s)  LR: 5.490e-04  Data: 0.009 (0.015)
Train: 283 [ 400/1251 ( 32%)]  Loss: 3.292 (3.51)  Time: 0.773s, 1324.43/s  (0.785s, 1304.10/s)  LR: 5.490e-04  Data: 0.010 (0.014)
Train: 283 [ 450/1251 ( 36%)]  Loss: 3.486 (3.51)  Time: 0.783s, 1307.31/s  (0.786s, 1303.23/s)  LR: 5.490e-04  Data: 0.010 (0.014)
Train: 283 [ 500/1251 ( 40%)]  Loss: 3.533 (3.51)  Time: 0.774s, 1323.71/s  (0.786s, 1303.11/s)  LR: 5.490e-04  Data: 0.010 (0.014)
Train: 283 [ 550/1251 ( 44%)]  Loss: 3.684 (3.52)  Time: 0.783s, 1308.39/s  (0.785s, 1303.64/s)  LR: 5.490e-04  Data: 0.010 (0.013)
Train: 283 [ 600/1251 ( 48%)]  Loss: 3.676 (3.53)  Time: 0.810s, 1264.05/s  (0.786s, 1302.00/s)  LR: 5.490e-04  Data: 0.009 (0.013)
Train: 283 [ 650/1251 ( 52%)]  Loss: 3.544 (3.54)  Time: 0.776s, 1319.56/s  (0.787s, 1301.96/s)  LR: 5.490e-04  Data: 0.009 (0.013)
Train: 283 [ 700/1251 ( 56%)]  Loss: 3.717 (3.55)  Time: 0.811s, 1263.38/s  (0.786s, 1303.07/s)  LR: 5.490e-04  Data: 0.010 (0.012)
Train: 283 [ 750/1251 ( 60%)]  Loss: 3.459 (3.54)  Time: 0.826s, 1239.57/s  (0.786s, 1302.30/s)  LR: 5.490e-04  Data: 0.010 (0.012)
Train: 283 [ 800/1251 ( 64%)]  Loss: 3.358 (3.53)  Time: 0.775s, 1321.14/s  (0.787s, 1300.35/s)  LR: 5.490e-04  Data: 0.010 (0.012)
Train: 283 [ 850/1251 ( 68%)]  Loss: 3.817 (3.55)  Time: 0.773s, 1324.91/s  (0.789s, 1298.54/s)  LR: 5.490e-04  Data: 0.010 (0.012)
Train: 283 [ 900/1251 ( 72%)]  Loss: 3.505 (3.54)  Time: 0.787s, 1300.65/s  (0.788s, 1299.33/s)  LR: 5.490e-04  Data: 0.010 (0.012)
Train: 283 [ 950/1251 ( 76%)]  Loss: 3.457 (3.54)  Time: 0.773s, 1325.34/s  (0.788s, 1299.16/s)  LR: 5.490e-04  Data: 0.009 (0.012)
Train: 283 [1000/1251 ( 80%)]  Loss: 3.575 (3.54)  Time: 0.772s, 1326.74/s  (0.788s, 1300.12/s)  LR: 5.490e-04  Data: 0.010 (0.012)
Train: 283 [1050/1251 ( 84%)]  Loss: 3.282 (3.53)  Time: 0.771s, 1327.36/s  (0.788s, 1300.22/s)  LR: 5.490e-04  Data: 0.010 (0.012)
Train: 283 [1100/1251 ( 88%)]  Loss: 3.478 (3.53)  Time: 0.785s, 1303.83/s  (0.787s, 1300.47/s)  LR: 5.490e-04  Data: 0.009 (0.012)
Train: 283 [1150/1251 ( 92%)]  Loss: 3.691 (3.53)  Time: 0.774s, 1323.30/s  (0.787s, 1301.05/s)  LR: 5.490e-04  Data: 0.010 (0.011)
Train: 283 [1200/1251 ( 96%)]  Loss: 3.863 (3.55)  Time: 0.774s, 1322.87/s  (0.787s, 1301.75/s)  LR: 5.490e-04  Data: 0.010 (0.011)
Train: 283 [1250/1251 (100%)]  Loss: 3.576 (3.55)  Time: 0.757s, 1352.36/s  (0.786s, 1302.05/s)  LR: 5.490e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.514 (1.514)  Loss:  0.8032 (0.8032)  Acc@1: 89.4531 (89.4531)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.194 (0.564)  Loss:  0.8042 (1.3166)  Acc@1: 84.4340 (75.2740)  Acc@5: 97.0519 (92.7360)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-274.pth.tar', 75.46800011474609)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-283.pth.tar', 75.27400009033204)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-281.pth.tar', 75.27400001464844)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-277.pth.tar', 75.2700000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-276.pth.tar', 75.19200009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-275.pth.tar', 75.11800016845703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-278.pth.tar', 75.10799998779297)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-260.pth.tar', 75.10400008789063)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-273.pth.tar', 75.0740000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-268.pth.tar', 75.06800008544921)

Train: 284 [   0/1251 (  0%)]  Loss: 3.190 (3.19)  Time: 2.245s,  456.22/s  (2.245s,  456.22/s)  LR: 5.464e-04  Data: 1.508 (1.508)
Train: 284 [  50/1251 (  4%)]  Loss: 3.423 (3.31)  Time: 0.781s, 1311.11/s  (0.825s, 1241.11/s)  LR: 5.464e-04  Data: 0.011 (0.046)
Train: 284 [ 100/1251 (  8%)]  Loss: 3.459 (3.36)  Time: 0.775s, 1321.06/s  (0.801s, 1277.76/s)  LR: 5.464e-04  Data: 0.010 (0.028)
Train: 284 [ 150/1251 ( 12%)]  Loss: 3.505 (3.39)  Time: 0.784s, 1306.89/s  (0.794s, 1289.74/s)  LR: 5.464e-04  Data: 0.010 (0.022)
Train: 284 [ 200/1251 ( 16%)]  Loss: 3.465 (3.41)  Time: 0.775s, 1321.10/s  (0.792s, 1293.70/s)  LR: 5.464e-04  Data: 0.010 (0.019)
Train: 284 [ 250/1251 ( 20%)]  Loss: 3.423 (3.41)  Time: 0.777s, 1318.36/s  (0.789s, 1297.84/s)  LR: 5.464e-04  Data: 0.010 (0.017)
Train: 284 [ 300/1251 ( 24%)]  Loss: 3.596 (3.44)  Time: 0.785s, 1304.39/s  (0.787s, 1300.69/s)  LR: 5.464e-04  Data: 0.014 (0.016)
Train: 284 [ 350/1251 ( 28%)]  Loss: 3.233 (3.41)  Time: 0.783s, 1307.63/s  (0.787s, 1301.69/s)  LR: 5.464e-04  Data: 0.011 (0.015)
Train: 284 [ 400/1251 ( 32%)]  Loss: 3.675 (3.44)  Time: 0.773s, 1325.13/s  (0.786s, 1302.74/s)  LR: 5.464e-04  Data: 0.011 (0.015)
Train: 284 [ 450/1251 ( 36%)]  Loss: 3.410 (3.44)  Time: 0.800s, 1279.66/s  (0.787s, 1300.65/s)  LR: 5.464e-04  Data: 0.010 (0.014)
Train: 284 [ 500/1251 ( 40%)]  Loss: 3.555 (3.45)  Time: 0.777s, 1318.51/s  (0.787s, 1301.40/s)  LR: 5.464e-04  Data: 0.010 (0.014)
Train: 284 [ 550/1251 ( 44%)]  Loss: 3.462 (3.45)  Time: 0.775s, 1321.56/s  (0.787s, 1301.87/s)  LR: 5.464e-04  Data: 0.010 (0.014)
Train: 284 [ 600/1251 ( 48%)]  Loss: 3.722 (3.47)  Time: 0.775s, 1321.57/s  (0.786s, 1302.57/s)  LR: 5.464e-04  Data: 0.011 (0.013)
Train: 284 [ 650/1251 ( 52%)]  Loss: 3.489 (3.47)  Time: 0.776s, 1319.18/s  (0.787s, 1301.27/s)  LR: 5.464e-04  Data: 0.010 (0.013)
Train: 284 [ 700/1251 ( 56%)]  Loss: 3.586 (3.48)  Time: 0.779s, 1314.14/s  (0.787s, 1301.79/s)  LR: 5.464e-04  Data: 0.010 (0.013)
Train: 284 [ 750/1251 ( 60%)]  Loss: 3.411 (3.48)  Time: 0.774s, 1322.88/s  (0.786s, 1302.38/s)  LR: 5.464e-04  Data: 0.010 (0.013)
Train: 284 [ 800/1251 ( 64%)]  Loss: 3.264 (3.46)  Time: 0.776s, 1320.38/s  (0.786s, 1303.10/s)  LR: 5.464e-04  Data: 0.010 (0.012)
Train: 284 [ 850/1251 ( 68%)]  Loss: 3.447 (3.46)  Time: 0.808s, 1266.64/s  (0.786s, 1303.14/s)  LR: 5.464e-04  Data: 0.009 (0.012)
Train: 284 [ 900/1251 ( 72%)]  Loss: 3.647 (3.47)  Time: 0.776s, 1318.97/s  (0.786s, 1302.77/s)  LR: 5.464e-04  Data: 0.010 (0.012)
Train: 284 [ 950/1251 ( 76%)]  Loss: 3.494 (3.47)  Time: 0.782s, 1309.20/s  (0.786s, 1303.02/s)  LR: 5.464e-04  Data: 0.010 (0.012)
Train: 284 [1000/1251 ( 80%)]  Loss: 3.505 (3.47)  Time: 0.774s, 1322.37/s  (0.785s, 1303.89/s)  LR: 5.464e-04  Data: 0.010 (0.012)
Train: 284 [1050/1251 ( 84%)]  Loss: 3.298 (3.47)  Time: 0.779s, 1314.18/s  (0.785s, 1303.73/s)  LR: 5.464e-04  Data: 0.010 (0.012)
Train: 284 [1100/1251 ( 88%)]  Loss: 3.876 (3.48)  Time: 0.773s, 1325.17/s  (0.785s, 1304.06/s)  LR: 5.464e-04  Data: 0.010 (0.012)
Train: 284 [1150/1251 ( 92%)]  Loss: 3.493 (3.48)  Time: 0.774s, 1323.43/s  (0.785s, 1304.53/s)  LR: 5.464e-04  Data: 0.009 (0.012)
Train: 284 [1200/1251 ( 96%)]  Loss: 3.691 (3.49)  Time: 0.783s, 1307.01/s  (0.785s, 1304.97/s)  LR: 5.464e-04  Data: 0.010 (0.012)
Train: 284 [1250/1251 (100%)]  Loss: 3.218 (3.48)  Time: 0.766s, 1337.35/s  (0.784s, 1305.38/s)  LR: 5.464e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.501 (1.501)  Loss:  0.7920 (0.7920)  Acc@1: 90.6250 (90.6250)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.194 (0.559)  Loss:  0.9082 (1.3532)  Acc@1: 86.2028 (75.1660)  Acc@5: 96.3443 (92.8540)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-274.pth.tar', 75.46800011474609)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-283.pth.tar', 75.27400009033204)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-281.pth.tar', 75.27400001464844)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-277.pth.tar', 75.2700000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-276.pth.tar', 75.19200009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-284.pth.tar', 75.16600008300782)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-275.pth.tar', 75.11800016845703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-278.pth.tar', 75.10799998779297)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-260.pth.tar', 75.10400008789063)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-273.pth.tar', 75.0740000366211)

Train: 285 [   0/1251 (  0%)]  Loss: 3.370 (3.37)  Time: 2.211s,  463.20/s  (2.211s,  463.20/s)  LR: 5.438e-04  Data: 1.482 (1.482)
Train: 285 [  50/1251 (  4%)]  Loss: 3.519 (3.44)  Time: 0.770s, 1329.11/s  (0.814s, 1257.71/s)  LR: 5.438e-04  Data: 0.010 (0.043)
Train: 285 [ 100/1251 (  8%)]  Loss: 3.146 (3.34)  Time: 0.773s, 1324.92/s  (0.801s, 1278.59/s)  LR: 5.438e-04  Data: 0.009 (0.027)
Train: 285 [ 150/1251 ( 12%)]  Loss: 3.629 (3.42)  Time: 0.816s, 1255.38/s  (0.802s, 1276.13/s)  LR: 5.438e-04  Data: 0.012 (0.021)
Train: 285 [ 200/1251 ( 16%)]  Loss: 3.477 (3.43)  Time: 0.772s, 1326.76/s  (0.798s, 1283.72/s)  LR: 5.438e-04  Data: 0.010 (0.018)
Train: 285 [ 250/1251 ( 20%)]  Loss: 3.451 (3.43)  Time: 0.774s, 1322.94/s  (0.794s, 1290.17/s)  LR: 5.438e-04  Data: 0.010 (0.017)
Train: 285 [ 300/1251 ( 24%)]  Loss: 3.495 (3.44)  Time: 0.772s, 1325.94/s  (0.791s, 1294.33/s)  LR: 5.438e-04  Data: 0.010 (0.016)
Train: 285 [ 350/1251 ( 28%)]  Loss: 3.810 (3.49)  Time: 0.777s, 1317.55/s  (0.789s, 1297.08/s)  LR: 5.438e-04  Data: 0.013 (0.015)
Train: 285 [ 400/1251 ( 32%)]  Loss: 3.802 (3.52)  Time: 0.773s, 1325.52/s  (0.789s, 1298.54/s)  LR: 5.438e-04  Data: 0.010 (0.014)
Train: 285 [ 450/1251 ( 36%)]  Loss: 3.632 (3.53)  Time: 0.778s, 1316.18/s  (0.788s, 1299.31/s)  LR: 5.438e-04  Data: 0.009 (0.014)
Train: 285 [ 500/1251 ( 40%)]  Loss: 3.420 (3.52)  Time: 0.772s, 1325.67/s  (0.787s, 1300.38/s)  LR: 5.438e-04  Data: 0.009 (0.013)
Train: 285 [ 550/1251 ( 44%)]  Loss: 3.726 (3.54)  Time: 0.772s, 1326.99/s  (0.787s, 1300.56/s)  LR: 5.438e-04  Data: 0.009 (0.013)
Train: 285 [ 600/1251 ( 48%)]  Loss: 3.648 (3.55)  Time: 0.775s, 1321.64/s  (0.786s, 1302.05/s)  LR: 5.438e-04  Data: 0.010 (0.013)
Train: 285 [ 650/1251 ( 52%)]  Loss: 3.785 (3.56)  Time: 0.775s, 1321.33/s  (0.786s, 1302.87/s)  LR: 5.438e-04  Data: 0.010 (0.013)
Train: 285 [ 700/1251 ( 56%)]  Loss: 3.372 (3.55)  Time: 0.779s, 1314.02/s  (0.785s, 1303.83/s)  LR: 5.438e-04  Data: 0.010 (0.012)
Train: 285 [ 750/1251 ( 60%)]  Loss: 3.586 (3.55)  Time: 0.776s, 1320.38/s  (0.787s, 1301.49/s)  LR: 5.438e-04  Data: 0.010 (0.012)
Train: 285 [ 800/1251 ( 64%)]  Loss: 3.390 (3.54)  Time: 0.773s, 1324.37/s  (0.786s, 1302.25/s)  LR: 5.438e-04  Data: 0.010 (0.012)
Train: 285 [ 850/1251 ( 68%)]  Loss: 3.190 (3.52)  Time: 0.773s, 1325.04/s  (0.786s, 1303.18/s)  LR: 5.438e-04  Data: 0.009 (0.012)
Train: 285 [ 900/1251 ( 72%)]  Loss: 3.395 (3.52)  Time: 0.855s, 1197.71/s  (0.786s, 1303.32/s)  LR: 5.438e-04  Data: 0.010 (0.012)
Train: 285 [ 950/1251 ( 76%)]  Loss: 3.670 (3.53)  Time: 0.774s, 1322.15/s  (0.786s, 1303.47/s)  LR: 5.438e-04  Data: 0.009 (0.012)
Train: 285 [1000/1251 ( 80%)]  Loss: 3.415 (3.52)  Time: 0.773s, 1325.28/s  (0.786s, 1302.91/s)  LR: 5.438e-04  Data: 0.009 (0.012)
Train: 285 [1050/1251 ( 84%)]  Loss: 3.536 (3.52)  Time: 0.812s, 1261.83/s  (0.786s, 1303.14/s)  LR: 5.438e-04  Data: 0.010 (0.011)
Train: 285 [1100/1251 ( 88%)]  Loss: 3.641 (3.53)  Time: 0.814s, 1257.95/s  (0.787s, 1301.87/s)  LR: 5.438e-04  Data: 0.010 (0.011)
Train: 285 [1150/1251 ( 92%)]  Loss: 3.735 (3.54)  Time: 0.772s, 1327.16/s  (0.786s, 1302.54/s)  LR: 5.438e-04  Data: 0.009 (0.011)
Train: 285 [1200/1251 ( 96%)]  Loss: 3.708 (3.54)  Time: 0.773s, 1324.97/s  (0.786s, 1303.06/s)  LR: 5.438e-04  Data: 0.009 (0.011)
Train: 285 [1250/1251 (100%)]  Loss: 3.321 (3.53)  Time: 0.762s, 1343.22/s  (0.786s, 1303.55/s)  LR: 5.438e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.544 (1.544)  Loss:  0.7827 (0.7827)  Acc@1: 88.8672 (88.8672)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.193 (0.571)  Loss:  0.7710 (1.2677)  Acc@1: 85.4953 (75.2020)  Acc@5: 96.8160 (92.7000)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-274.pth.tar', 75.46800011474609)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-283.pth.tar', 75.27400009033204)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-281.pth.tar', 75.27400001464844)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-277.pth.tar', 75.2700000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-285.pth.tar', 75.20200003417969)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-276.pth.tar', 75.19200009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-284.pth.tar', 75.16600008300782)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-275.pth.tar', 75.11800016845703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-278.pth.tar', 75.10799998779297)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-260.pth.tar', 75.10400008789063)

Train: 286 [   0/1251 (  0%)]  Loss: 3.221 (3.22)  Time: 2.273s,  450.44/s  (2.273s,  450.44/s)  LR: 5.413e-04  Data: 1.541 (1.541)
Train: 286 [  50/1251 (  4%)]  Loss: 3.476 (3.35)  Time: 0.773s, 1325.05/s  (0.840s, 1219.05/s)  LR: 5.413e-04  Data: 0.010 (0.043)
Train: 286 [ 100/1251 (  8%)]  Loss: 3.104 (3.27)  Time: 0.772s, 1326.43/s  (0.810s, 1264.04/s)  LR: 5.413e-04  Data: 0.010 (0.027)
Train: 286 [ 150/1251 ( 12%)]  Loss: 3.672 (3.37)  Time: 0.773s, 1323.94/s  (0.801s, 1279.15/s)  LR: 5.413e-04  Data: 0.009 (0.021)
Train: 286 [ 200/1251 ( 16%)]  Loss: 3.143 (3.32)  Time: 0.776s, 1319.97/s  (0.798s, 1283.64/s)  LR: 5.413e-04  Data: 0.011 (0.018)
Train: 286 [ 250/1251 ( 20%)]  Loss: 3.514 (3.35)  Time: 0.773s, 1324.48/s  (0.795s, 1288.84/s)  LR: 5.413e-04  Data: 0.009 (0.017)
Train: 286 [ 300/1251 ( 24%)]  Loss: 3.413 (3.36)  Time: 0.784s, 1306.52/s  (0.792s, 1292.95/s)  LR: 5.413e-04  Data: 0.010 (0.015)
Train: 286 [ 350/1251 ( 28%)]  Loss: 3.534 (3.38)  Time: 0.774s, 1322.69/s  (0.791s, 1294.98/s)  LR: 5.413e-04  Data: 0.010 (0.015)
Train: 286 [ 400/1251 ( 32%)]  Loss: 3.656 (3.41)  Time: 0.783s, 1308.13/s  (0.789s, 1297.66/s)  LR: 5.413e-04  Data: 0.010 (0.014)
Train: 286 [ 450/1251 ( 36%)]  Loss: 3.527 (3.43)  Time: 0.773s, 1324.76/s  (0.788s, 1300.01/s)  LR: 5.413e-04  Data: 0.010 (0.014)
Train: 286 [ 500/1251 ( 40%)]  Loss: 3.586 (3.44)  Time: 0.772s, 1326.13/s  (0.787s, 1301.63/s)  LR: 5.413e-04  Data: 0.010 (0.013)
Train: 286 [ 550/1251 ( 44%)]  Loss: 3.251 (3.42)  Time: 0.775s, 1320.91/s  (0.786s, 1302.34/s)  LR: 5.413e-04  Data: 0.009 (0.013)
Train: 286 [ 600/1251 ( 48%)]  Loss: 3.321 (3.42)  Time: 0.774s, 1322.88/s  (0.786s, 1303.62/s)  LR: 5.413e-04  Data: 0.009 (0.013)
Train: 286 [ 650/1251 ( 52%)]  Loss: 3.335 (3.41)  Time: 0.782s, 1309.08/s  (0.785s, 1304.99/s)  LR: 5.413e-04  Data: 0.009 (0.012)
Train: 286 [ 700/1251 ( 56%)]  Loss: 3.510 (3.42)  Time: 0.827s, 1238.08/s  (0.785s, 1305.27/s)  LR: 5.413e-04  Data: 0.014 (0.012)
Train: 286 [ 750/1251 ( 60%)]  Loss: 3.442 (3.42)  Time: 0.773s, 1324.03/s  (0.787s, 1301.82/s)  LR: 5.413e-04  Data: 0.009 (0.012)
Train: 286 [ 800/1251 ( 64%)]  Loss: 3.359 (3.42)  Time: 0.782s, 1309.17/s  (0.786s, 1302.77/s)  LR: 5.413e-04  Data: 0.010 (0.012)
Train: 286 [ 850/1251 ( 68%)]  Loss: 3.420 (3.42)  Time: 0.773s, 1324.26/s  (0.786s, 1302.62/s)  LR: 5.413e-04  Data: 0.009 (0.012)
Train: 286 [ 900/1251 ( 72%)]  Loss: 3.310 (3.41)  Time: 0.773s, 1325.56/s  (0.786s, 1303.04/s)  LR: 5.413e-04  Data: 0.010 (0.012)
Train: 286 [ 950/1251 ( 76%)]  Loss: 3.437 (3.41)  Time: 0.815s, 1255.86/s  (0.786s, 1303.02/s)  LR: 5.413e-04  Data: 0.010 (0.012)
Train: 286 [1000/1251 ( 80%)]  Loss: 3.594 (3.42)  Time: 0.775s, 1321.84/s  (0.786s, 1302.16/s)  LR: 5.413e-04  Data: 0.009 (0.012)
Train: 286 [1050/1251 ( 84%)]  Loss: 3.551 (3.43)  Time: 0.776s, 1320.17/s  (0.786s, 1302.87/s)  LR: 5.413e-04  Data: 0.010 (0.011)
Train: 286 [1100/1251 ( 88%)]  Loss: 3.465 (3.43)  Time: 0.777s, 1317.27/s  (0.786s, 1302.89/s)  LR: 5.413e-04  Data: 0.009 (0.011)
Train: 286 [1150/1251 ( 92%)]  Loss: 3.796 (3.44)  Time: 0.775s, 1321.95/s  (0.787s, 1301.67/s)  LR: 5.413e-04  Data: 0.010 (0.011)
Train: 286 [1200/1251 ( 96%)]  Loss: 3.634 (3.45)  Time: 0.774s, 1322.85/s  (0.787s, 1301.87/s)  LR: 5.413e-04  Data: 0.009 (0.011)
Train: 286 [1250/1251 (100%)]  Loss: 3.385 (3.45)  Time: 0.770s, 1329.47/s  (0.786s, 1302.30/s)  LR: 5.413e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.542 (1.542)  Loss:  0.7622 (0.7622)  Acc@1: 89.1602 (89.1602)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.194 (0.559)  Loss:  0.8862 (1.2514)  Acc@1: 84.4340 (75.4260)  Acc@5: 95.5189 (92.9160)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-274.pth.tar', 75.46800011474609)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-286.pth.tar', 75.42600009033202)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-283.pth.tar', 75.27400009033204)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-281.pth.tar', 75.27400001464844)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-277.pth.tar', 75.2700000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-285.pth.tar', 75.20200003417969)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-276.pth.tar', 75.19200009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-284.pth.tar', 75.16600008300782)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-275.pth.tar', 75.11800016845703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-278.pth.tar', 75.10799998779297)

Train: 287 [   0/1251 (  0%)]  Loss: 3.359 (3.36)  Time: 2.349s,  435.85/s  (2.349s,  435.85/s)  LR: 5.387e-04  Data: 1.594 (1.594)
Train: 287 [  50/1251 (  4%)]  Loss: 3.488 (3.42)  Time: 0.771s, 1327.72/s  (0.842s, 1216.27/s)  LR: 5.387e-04  Data: 0.009 (0.050)
Train: 287 [ 100/1251 (  8%)]  Loss: 3.332 (3.39)  Time: 0.771s, 1327.63/s  (0.809s, 1266.11/s)  LR: 5.387e-04  Data: 0.009 (0.030)
Train: 287 [ 150/1251 ( 12%)]  Loss: 3.366 (3.39)  Time: 0.787s, 1300.50/s  (0.807s, 1268.77/s)  LR: 5.387e-04  Data: 0.010 (0.023)
Train: 287 [ 200/1251 ( 16%)]  Loss: 3.237 (3.36)  Time: 0.774s, 1323.46/s  (0.801s, 1277.83/s)  LR: 5.387e-04  Data: 0.009 (0.020)
Train: 287 [ 250/1251 ( 20%)]  Loss: 3.604 (3.40)  Time: 0.774s, 1323.69/s  (0.797s, 1285.34/s)  LR: 5.387e-04  Data: 0.010 (0.018)
Train: 287 [ 300/1251 ( 24%)]  Loss: 3.709 (3.44)  Time: 0.787s, 1301.28/s  (0.794s, 1290.44/s)  LR: 5.387e-04  Data: 0.010 (0.017)
Train: 287 [ 350/1251 ( 28%)]  Loss: 3.672 (3.47)  Time: 0.781s, 1311.68/s  (0.792s, 1292.96/s)  LR: 5.387e-04  Data: 0.010 (0.016)
Train: 287 [ 400/1251 ( 32%)]  Loss: 3.499 (3.47)  Time: 0.787s, 1300.73/s  (0.791s, 1295.14/s)  LR: 5.387e-04  Data: 0.009 (0.015)
Train: 287 [ 450/1251 ( 36%)]  Loss: 3.530 (3.48)  Time: 0.772s, 1326.01/s  (0.789s, 1297.49/s)  LR: 5.387e-04  Data: 0.010 (0.014)
Train: 287 [ 500/1251 ( 40%)]  Loss: 3.150 (3.45)  Time: 0.777s, 1318.27/s  (0.788s, 1299.32/s)  LR: 5.387e-04  Data: 0.010 (0.014)
Train: 287 [ 550/1251 ( 44%)]  Loss: 3.533 (3.46)  Time: 0.772s, 1326.66/s  (0.788s, 1299.98/s)  LR: 5.387e-04  Data: 0.009 (0.014)
Train: 287 [ 600/1251 ( 48%)]  Loss: 3.138 (3.43)  Time: 0.772s, 1326.76/s  (0.787s, 1301.38/s)  LR: 5.387e-04  Data: 0.010 (0.013)
Train: 287 [ 650/1251 ( 52%)]  Loss: 3.856 (3.46)  Time: 0.773s, 1324.94/s  (0.786s, 1302.73/s)  LR: 5.387e-04  Data: 0.009 (0.013)
Train: 287 [ 700/1251 ( 56%)]  Loss: 3.430 (3.46)  Time: 0.773s, 1323.93/s  (0.786s, 1303.33/s)  LR: 5.387e-04  Data: 0.010 (0.013)
Train: 287 [ 750/1251 ( 60%)]  Loss: 3.300 (3.45)  Time: 0.867s, 1180.79/s  (0.785s, 1303.69/s)  LR: 5.387e-04  Data: 0.010 (0.013)
Train: 287 [ 800/1251 ( 64%)]  Loss: 3.657 (3.46)  Time: 0.772s, 1325.66/s  (0.785s, 1304.38/s)  LR: 5.387e-04  Data: 0.010 (0.012)
Train: 287 [ 850/1251 ( 68%)]  Loss: 3.390 (3.46)  Time: 0.771s, 1327.39/s  (0.786s, 1302.67/s)  LR: 5.387e-04  Data: 0.010 (0.012)
Train: 287 [ 900/1251 ( 72%)]  Loss: 3.150 (3.44)  Time: 0.817s, 1253.36/s  (0.786s, 1303.32/s)  LR: 5.387e-04  Data: 0.010 (0.012)
Train: 287 [ 950/1251 ( 76%)]  Loss: 3.609 (3.45)  Time: 0.780s, 1312.19/s  (0.785s, 1304.11/s)  LR: 5.387e-04  Data: 0.010 (0.012)
Train: 287 [1000/1251 ( 80%)]  Loss: 3.562 (3.46)  Time: 0.772s, 1325.74/s  (0.785s, 1303.75/s)  LR: 5.387e-04  Data: 0.010 (0.012)
Train: 287 [1050/1251 ( 84%)]  Loss: 3.149 (3.44)  Time: 0.777s, 1318.29/s  (0.785s, 1304.28/s)  LR: 5.387e-04  Data: 0.009 (0.012)
Train: 287 [1100/1251 ( 88%)]  Loss: 3.323 (3.44)  Time: 0.774s, 1323.33/s  (0.785s, 1304.11/s)  LR: 5.387e-04  Data: 0.010 (0.012)
Train: 287 [1150/1251 ( 92%)]  Loss: 3.546 (3.44)  Time: 0.771s, 1327.81/s  (0.785s, 1304.20/s)  LR: 5.387e-04  Data: 0.010 (0.012)
Train: 287 [1200/1251 ( 96%)]  Loss: 3.195 (3.43)  Time: 0.775s, 1321.90/s  (0.785s, 1304.72/s)  LR: 5.387e-04  Data: 0.013 (0.012)
Train: 287 [1250/1251 (100%)]  Loss: 3.222 (3.42)  Time: 0.759s, 1348.57/s  (0.785s, 1304.69/s)  LR: 5.387e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.550 (1.550)  Loss:  0.7720 (0.7720)  Acc@1: 89.2578 (89.2578)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.193 (0.566)  Loss:  0.9297 (1.2984)  Acc@1: 84.6698 (74.7380)  Acc@5: 95.2830 (92.6680)
Train: 288 [   0/1251 (  0%)]  Loss: 3.484 (3.48)  Time: 2.192s,  467.12/s  (2.192s,  467.12/s)  LR: 5.361e-04  Data: 1.462 (1.462)
Train: 288 [  50/1251 (  4%)]  Loss: 3.378 (3.43)  Time: 0.822s, 1246.24/s  (0.816s, 1254.34/s)  LR: 5.361e-04  Data: 0.009 (0.045)
Train: 288 [ 100/1251 (  8%)]  Loss: 3.500 (3.45)  Time: 0.837s, 1223.38/s  (0.809s, 1265.49/s)  LR: 5.361e-04  Data: 0.009 (0.028)
Train: 288 [ 150/1251 ( 12%)]  Loss: 3.632 (3.50)  Time: 0.774s, 1323.59/s  (0.799s, 1282.10/s)  LR: 5.361e-04  Data: 0.009 (0.022)
Train: 288 [ 200/1251 ( 16%)]  Loss: 3.373 (3.47)  Time: 0.854s, 1198.78/s  (0.799s, 1282.28/s)  LR: 5.361e-04  Data: 0.010 (0.019)
Train: 288 [ 250/1251 ( 20%)]  Loss: 3.938 (3.55)  Time: 0.834s, 1228.44/s  (0.796s, 1286.08/s)  LR: 5.361e-04  Data: 0.009 (0.017)
Train: 288 [ 300/1251 ( 24%)]  Loss: 3.516 (3.55)  Time: 0.773s, 1324.94/s  (0.793s, 1290.92/s)  LR: 5.361e-04  Data: 0.009 (0.016)
Train: 288 [ 350/1251 ( 28%)]  Loss: 3.480 (3.54)  Time: 0.772s, 1325.96/s  (0.792s, 1292.97/s)  LR: 5.361e-04  Data: 0.010 (0.015)
Train: 288 [ 400/1251 ( 32%)]  Loss: 3.330 (3.51)  Time: 0.771s, 1328.63/s  (0.790s, 1295.48/s)  LR: 5.361e-04  Data: 0.010 (0.014)
Train: 288 [ 450/1251 ( 36%)]  Loss: 3.528 (3.52)  Time: 0.773s, 1325.01/s  (0.789s, 1297.79/s)  LR: 5.361e-04  Data: 0.009 (0.014)
Train: 288 [ 500/1251 ( 40%)]  Loss: 3.464 (3.51)  Time: 0.777s, 1318.04/s  (0.788s, 1299.46/s)  LR: 5.361e-04  Data: 0.011 (0.013)
Train: 288 [ 550/1251 ( 44%)]  Loss: 3.469 (3.51)  Time: 0.778s, 1316.31/s  (0.788s, 1299.88/s)  LR: 5.361e-04  Data: 0.012 (0.013)
Train: 288 [ 600/1251 ( 48%)]  Loss: 3.208 (3.48)  Time: 0.782s, 1309.77/s  (0.787s, 1300.43/s)  LR: 5.361e-04  Data: 0.010 (0.013)
Train: 288 [ 650/1251 ( 52%)]  Loss: 3.521 (3.49)  Time: 0.772s, 1325.63/s  (0.787s, 1301.75/s)  LR: 5.361e-04  Data: 0.009 (0.013)
Train: 288 [ 700/1251 ( 56%)]  Loss: 3.436 (3.48)  Time: 0.782s, 1309.00/s  (0.786s, 1302.39/s)  LR: 5.361e-04  Data: 0.010 (0.012)
Train: 288 [ 750/1251 ( 60%)]  Loss: 3.674 (3.50)  Time: 0.792s, 1292.68/s  (0.787s, 1301.38/s)  LR: 5.361e-04  Data: 0.009 (0.012)
Train: 288 [ 800/1251 ( 64%)]  Loss: 3.497 (3.50)  Time: 0.773s, 1324.65/s  (0.786s, 1302.62/s)  LR: 5.361e-04  Data: 0.010 (0.012)
Train: 288 [ 850/1251 ( 68%)]  Loss: 3.387 (3.49)  Time: 0.816s, 1255.45/s  (0.786s, 1303.06/s)  LR: 5.361e-04  Data: 0.010 (0.012)
Train: 288 [ 900/1251 ( 72%)]  Loss: 3.359 (3.48)  Time: 0.781s, 1310.40/s  (0.786s, 1303.16/s)  LR: 5.361e-04  Data: 0.012 (0.012)
Train: 288 [ 950/1251 ( 76%)]  Loss: 3.777 (3.50)  Time: 0.774s, 1323.66/s  (0.786s, 1303.53/s)  LR: 5.361e-04  Data: 0.009 (0.012)
Train: 288 [1000/1251 ( 80%)]  Loss: 3.627 (3.50)  Time: 0.778s, 1316.38/s  (0.785s, 1304.57/s)  LR: 5.361e-04  Data: 0.010 (0.012)
Train: 288 [1050/1251 ( 84%)]  Loss: 3.727 (3.51)  Time: 0.773s, 1325.47/s  (0.785s, 1304.83/s)  LR: 5.361e-04  Data: 0.010 (0.012)
Train: 288 [1100/1251 ( 88%)]  Loss: 3.431 (3.51)  Time: 0.775s, 1321.38/s  (0.784s, 1305.44/s)  LR: 5.361e-04  Data: 0.010 (0.011)
Train: 288 [1150/1251 ( 92%)]  Loss: 3.744 (3.52)  Time: 0.779s, 1314.89/s  (0.784s, 1305.99/s)  LR: 5.361e-04  Data: 0.009 (0.011)
Train: 288 [1200/1251 ( 96%)]  Loss: 3.447 (3.52)  Time: 0.784s, 1305.94/s  (0.784s, 1306.55/s)  LR: 5.361e-04  Data: 0.010 (0.011)
Train: 288 [1250/1251 (100%)]  Loss: 3.775 (3.53)  Time: 0.762s, 1343.28/s  (0.784s, 1306.47/s)  LR: 5.361e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.677 (1.677)  Loss:  0.7935 (0.7935)  Acc@1: 89.6484 (89.6484)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.193 (0.572)  Loss:  0.8667 (1.3226)  Acc@1: 84.7877 (74.9740)  Acc@5: 95.8726 (92.8140)
Train: 289 [   0/1251 (  0%)]  Loss: 3.298 (3.30)  Time: 2.199s,  465.64/s  (2.199s,  465.64/s)  LR: 5.335e-04  Data: 1.473 (1.473)
Train: 289 [  50/1251 (  4%)]  Loss: 3.378 (3.34)  Time: 0.782s, 1309.84/s  (0.824s, 1243.42/s)  LR: 5.335e-04  Data: 0.009 (0.044)
Train: 289 [ 100/1251 (  8%)]  Loss: 3.728 (3.47)  Time: 0.772s, 1326.23/s  (0.801s, 1278.59/s)  LR: 5.335e-04  Data: 0.010 (0.027)
Train: 289 [ 150/1251 ( 12%)]  Loss: 3.483 (3.47)  Time: 0.779s, 1315.03/s  (0.795s, 1288.83/s)  LR: 5.335e-04  Data: 0.009 (0.021)
Train: 289 [ 200/1251 ( 16%)]  Loss: 3.484 (3.47)  Time: 0.816s, 1255.29/s  (0.792s, 1292.25/s)  LR: 5.335e-04  Data: 0.009 (0.018)
Train: 289 [ 250/1251 ( 20%)]  Loss: 3.533 (3.48)  Time: 0.784s, 1306.00/s  (0.791s, 1295.16/s)  LR: 5.335e-04  Data: 0.010 (0.017)
Train: 289 [ 300/1251 ( 24%)]  Loss: 3.536 (3.49)  Time: 0.844s, 1212.57/s  (0.789s, 1297.33/s)  LR: 5.335e-04  Data: 0.009 (0.015)
Train: 289 [ 350/1251 ( 28%)]  Loss: 3.626 (3.51)  Time: 0.778s, 1315.52/s  (0.788s, 1299.94/s)  LR: 5.335e-04  Data: 0.009 (0.015)
Train: 289 [ 400/1251 ( 32%)]  Loss: 3.787 (3.54)  Time: 0.772s, 1326.18/s  (0.787s, 1301.28/s)  LR: 5.335e-04  Data: 0.009 (0.014)
Train: 289 [ 450/1251 ( 36%)]  Loss: 3.683 (3.55)  Time: 0.774s, 1322.93/s  (0.786s, 1303.49/s)  LR: 5.335e-04  Data: 0.009 (0.014)
Train: 289 [ 500/1251 ( 40%)]  Loss: 3.210 (3.52)  Time: 0.781s, 1310.39/s  (0.785s, 1304.18/s)  LR: 5.335e-04  Data: 0.010 (0.013)
Train: 289 [ 550/1251 ( 44%)]  Loss: 3.589 (3.53)  Time: 0.772s, 1326.27/s  (0.786s, 1302.22/s)  LR: 5.335e-04  Data: 0.010 (0.013)
Train: 289 [ 600/1251 ( 48%)]  Loss: 3.338 (3.51)  Time: 0.776s, 1319.22/s  (0.786s, 1303.23/s)  LR: 5.335e-04  Data: 0.009 (0.013)
Train: 289 [ 650/1251 ( 52%)]  Loss: 3.526 (3.51)  Time: 0.774s, 1323.02/s  (0.786s, 1302.74/s)  LR: 5.335e-04  Data: 0.010 (0.012)
Train: 289 [ 700/1251 ( 56%)]  Loss: 3.459 (3.51)  Time: 0.775s, 1322.10/s  (0.786s, 1303.52/s)  LR: 5.335e-04  Data: 0.010 (0.012)
Train: 289 [ 750/1251 ( 60%)]  Loss: 3.249 (3.49)  Time: 0.819s, 1250.91/s  (0.785s, 1303.66/s)  LR: 5.335e-04  Data: 0.010 (0.012)
Train: 289 [ 800/1251 ( 64%)]  Loss: 3.614 (3.50)  Time: 0.771s, 1328.81/s  (0.785s, 1304.63/s)  LR: 5.335e-04  Data: 0.011 (0.012)
Train: 289 [ 850/1251 ( 68%)]  Loss: 3.407 (3.50)  Time: 0.778s, 1317.01/s  (0.785s, 1304.92/s)  LR: 5.335e-04  Data: 0.010 (0.012)
Train: 289 [ 900/1251 ( 72%)]  Loss: 3.794 (3.51)  Time: 0.772s, 1326.02/s  (0.785s, 1304.61/s)  LR: 5.335e-04  Data: 0.010 (0.012)
Train: 289 [ 950/1251 ( 76%)]  Loss: 3.326 (3.50)  Time: 0.774s, 1323.28/s  (0.785s, 1304.82/s)  LR: 5.335e-04  Data: 0.010 (0.012)
Train: 289 [1000/1251 ( 80%)]  Loss: 3.562 (3.51)  Time: 0.822s, 1245.78/s  (0.785s, 1303.82/s)  LR: 5.335e-04  Data: 0.018 (0.012)
Train: 289 [1050/1251 ( 84%)]  Loss: 3.467 (3.50)  Time: 0.773s, 1325.24/s  (0.786s, 1302.66/s)  LR: 5.335e-04  Data: 0.009 (0.012)
Train: 289 [1100/1251 ( 88%)]  Loss: 3.500 (3.50)  Time: 0.774s, 1323.62/s  (0.786s, 1303.09/s)  LR: 5.335e-04  Data: 0.010 (0.012)
Train: 289 [1150/1251 ( 92%)]  Loss: 3.446 (3.50)  Time: 0.773s, 1324.30/s  (0.786s, 1303.42/s)  LR: 5.335e-04  Data: 0.009 (0.012)
Train: 289 [1200/1251 ( 96%)]  Loss: 3.394 (3.50)  Time: 0.773s, 1324.52/s  (0.786s, 1303.37/s)  LR: 5.335e-04  Data: 0.010 (0.011)
Train: 289 [1250/1251 (100%)]  Loss: 3.504 (3.50)  Time: 0.762s, 1344.71/s  (0.786s, 1302.75/s)  LR: 5.335e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.522 (1.522)  Loss:  0.7593 (0.7593)  Acc@1: 90.4297 (90.4297)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.194 (0.559)  Loss:  0.8770 (1.3020)  Acc@1: 85.3774 (75.2080)  Acc@5: 96.8160 (92.8480)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-274.pth.tar', 75.46800011474609)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-286.pth.tar', 75.42600009033202)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-283.pth.tar', 75.27400009033204)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-281.pth.tar', 75.27400001464844)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-277.pth.tar', 75.2700000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-289.pth.tar', 75.20800011230469)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-285.pth.tar', 75.20200003417969)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-276.pth.tar', 75.19200009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-284.pth.tar', 75.16600008300782)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-275.pth.tar', 75.11800016845703)

Train: 290 [   0/1251 (  0%)]  Loss: 3.408 (3.41)  Time: 2.281s,  448.98/s  (2.281s,  448.98/s)  LR: 5.309e-04  Data: 1.546 (1.546)
Train: 290 [  50/1251 (  4%)]  Loss: 3.198 (3.30)  Time: 0.814s, 1257.57/s  (0.823s, 1243.54/s)  LR: 5.309e-04  Data: 0.011 (0.042)
Train: 290 [ 100/1251 (  8%)]  Loss: 3.604 (3.40)  Time: 0.809s, 1265.73/s  (0.804s, 1273.73/s)  LR: 5.309e-04  Data: 0.009 (0.026)
Train: 290 [ 150/1251 ( 12%)]  Loss: 3.166 (3.34)  Time: 0.812s, 1260.74/s  (0.797s, 1284.53/s)  LR: 5.309e-04  Data: 0.009 (0.021)
Train: 290 [ 200/1251 ( 16%)]  Loss: 3.552 (3.39)  Time: 0.773s, 1325.47/s  (0.794s, 1289.87/s)  LR: 5.309e-04  Data: 0.010 (0.018)
Train: 290 [ 250/1251 ( 20%)]  Loss: 3.121 (3.34)  Time: 0.774s, 1323.29/s  (0.790s, 1295.76/s)  LR: 5.309e-04  Data: 0.010 (0.017)
Train: 290 [ 300/1251 ( 24%)]  Loss: 3.502 (3.36)  Time: 0.778s, 1316.46/s  (0.789s, 1298.45/s)  LR: 5.309e-04  Data: 0.009 (0.015)
Train: 290 [ 350/1251 ( 28%)]  Loss: 3.538 (3.39)  Time: 0.882s, 1161.42/s  (0.788s, 1299.14/s)  LR: 5.309e-04  Data: 0.009 (0.015)
Train: 290 [ 400/1251 ( 32%)]  Loss: 3.436 (3.39)  Time: 0.826s, 1239.10/s  (0.793s, 1291.70/s)  LR: 5.309e-04  Data: 0.014 (0.014)
Train: 290 [ 450/1251 ( 36%)]  Loss: 3.443 (3.40)  Time: 0.773s, 1324.24/s  (0.793s, 1290.52/s)  LR: 5.309e-04  Data: 0.010 (0.014)
Train: 290 [ 500/1251 ( 40%)]  Loss: 3.557 (3.41)  Time: 0.784s, 1305.44/s  (0.792s, 1292.89/s)  LR: 5.309e-04  Data: 0.013 (0.014)
Train: 290 [ 550/1251 ( 44%)]  Loss: 3.356 (3.41)  Time: 0.783s, 1308.62/s  (0.792s, 1293.38/s)  LR: 5.309e-04  Data: 0.010 (0.013)
Train: 290 [ 600/1251 ( 48%)]  Loss: 3.506 (3.41)  Time: 0.775s, 1322.09/s  (0.791s, 1294.96/s)  LR: 5.309e-04  Data: 0.009 (0.013)
Train: 290 [ 650/1251 ( 52%)]  Loss: 3.099 (3.39)  Time: 0.774s, 1323.35/s  (0.790s, 1296.37/s)  LR: 5.309e-04  Data: 0.010 (0.013)
Train: 290 [ 700/1251 ( 56%)]  Loss: 3.248 (3.38)  Time: 0.777s, 1317.57/s  (0.790s, 1296.36/s)  LR: 5.309e-04  Data: 0.010 (0.013)
Train: 290 [ 750/1251 ( 60%)]  Loss: 3.303 (3.38)  Time: 0.777s, 1317.33/s  (0.789s, 1297.42/s)  LR: 5.309e-04  Data: 0.010 (0.012)
Train: 290 [ 800/1251 ( 64%)]  Loss: 3.600 (3.39)  Time: 0.774s, 1323.47/s  (0.788s, 1298.73/s)  LR: 5.309e-04  Data: 0.010 (0.012)
Train: 290 [ 850/1251 ( 68%)]  Loss: 3.186 (3.38)  Time: 0.783s, 1308.10/s  (0.788s, 1299.69/s)  LR: 5.309e-04  Data: 0.013 (0.012)
Train: 290 [ 900/1251 ( 72%)]  Loss: 3.823 (3.40)  Time: 0.775s, 1321.61/s  (0.788s, 1300.23/s)  LR: 5.309e-04  Data: 0.009 (0.012)
Train: 290 [ 950/1251 ( 76%)]  Loss: 3.551 (3.41)  Time: 0.793s, 1290.95/s  (0.787s, 1301.07/s)  LR: 5.309e-04  Data: 0.009 (0.012)
Train: 290 [1000/1251 ( 80%)]  Loss: 3.442 (3.41)  Time: 0.778s, 1316.01/s  (0.787s, 1301.50/s)  LR: 5.309e-04  Data: 0.010 (0.012)
Train: 290 [1050/1251 ( 84%)]  Loss: 3.668 (3.42)  Time: 0.788s, 1300.06/s  (0.787s, 1301.50/s)  LR: 5.309e-04  Data: 0.009 (0.012)
Train: 290 [1100/1251 ( 88%)]  Loss: 3.279 (3.42)  Time: 0.775s, 1320.82/s  (0.787s, 1301.84/s)  LR: 5.309e-04  Data: 0.010 (0.012)
Train: 290 [1150/1251 ( 92%)]  Loss: 3.433 (3.42)  Time: 0.794s, 1289.83/s  (0.786s, 1302.00/s)  LR: 5.309e-04  Data: 0.010 (0.011)
Train: 290 [1200/1251 ( 96%)]  Loss: 3.335 (3.41)  Time: 0.777s, 1317.96/s  (0.786s, 1302.17/s)  LR: 5.309e-04  Data: 0.011 (0.011)
Train: 290 [1250/1251 (100%)]  Loss: 3.453 (3.42)  Time: 0.758s, 1350.24/s  (0.786s, 1302.37/s)  LR: 5.309e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.509 (1.509)  Loss:  0.7222 (0.7222)  Acc@1: 89.4531 (89.4531)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.194 (0.570)  Loss:  0.8032 (1.2298)  Acc@1: 86.0849 (75.5740)  Acc@5: 97.0519 (92.9560)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-290.pth.tar', 75.57400003173828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-274.pth.tar', 75.46800011474609)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-286.pth.tar', 75.42600009033202)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-283.pth.tar', 75.27400009033204)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-281.pth.tar', 75.27400001464844)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-277.pth.tar', 75.2700000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-289.pth.tar', 75.20800011230469)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-285.pth.tar', 75.20200003417969)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-276.pth.tar', 75.19200009277344)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-284.pth.tar', 75.16600008300782)

Train: 291 [   0/1251 (  0%)]  Loss: 3.196 (3.20)  Time: 2.309s,  443.55/s  (2.309s,  443.55/s)  LR: 5.283e-04  Data: 1.574 (1.574)
Train: 291 [  50/1251 (  4%)]  Loss: 3.647 (3.42)  Time: 0.815s, 1256.10/s  (0.820s, 1249.16/s)  LR: 5.283e-04  Data: 0.009 (0.048)
Train: 291 [ 100/1251 (  8%)]  Loss: 3.426 (3.42)  Time: 0.771s, 1327.45/s  (0.801s, 1278.57/s)  LR: 5.283e-04  Data: 0.009 (0.029)
Train: 291 [ 150/1251 ( 12%)]  Loss: 3.480 (3.44)  Time: 0.778s, 1316.25/s  (0.796s, 1286.93/s)  LR: 5.283e-04  Data: 0.010 (0.022)
Train: 291 [ 200/1251 ( 16%)]  Loss: 3.592 (3.47)  Time: 0.827s, 1237.75/s  (0.797s, 1285.17/s)  LR: 5.283e-04  Data: 0.011 (0.020)
Train: 291 [ 250/1251 ( 20%)]  Loss: 3.306 (3.44)  Time: 0.773s, 1325.47/s  (0.795s, 1287.66/s)  LR: 5.283e-04  Data: 0.010 (0.018)
Train: 291 [ 300/1251 ( 24%)]  Loss: 3.505 (3.45)  Time: 0.773s, 1325.39/s  (0.793s, 1291.11/s)  LR: 5.283e-04  Data: 0.009 (0.016)
Train: 291 [ 350/1251 ( 28%)]  Loss: 3.105 (3.41)  Time: 0.774s, 1323.06/s  (0.791s, 1293.98/s)  LR: 5.283e-04  Data: 0.009 (0.015)
Train: 291 [ 400/1251 ( 32%)]  Loss: 3.692 (3.44)  Time: 0.771s, 1327.41/s  (0.791s, 1294.64/s)  LR: 5.283e-04  Data: 0.009 (0.015)
Train: 291 [ 450/1251 ( 36%)]  Loss: 3.771 (3.47)  Time: 0.775s, 1321.83/s  (0.790s, 1296.97/s)  LR: 5.283e-04  Data: 0.009 (0.014)
Train: 291 [ 500/1251 ( 40%)]  Loss: 3.472 (3.47)  Time: 0.775s, 1321.78/s  (0.789s, 1298.63/s)  LR: 5.283e-04  Data: 0.009 (0.014)
Train: 291 [ 550/1251 ( 44%)]  Loss: 3.105 (3.44)  Time: 0.773s, 1324.75/s  (0.788s, 1299.36/s)  LR: 5.283e-04  Data: 0.010 (0.013)
Train: 291 [ 600/1251 ( 48%)]  Loss: 3.574 (3.45)  Time: 0.812s, 1261.48/s  (0.788s, 1300.01/s)  LR: 5.283e-04  Data: 0.009 (0.013)
Train: 291 [ 650/1251 ( 52%)]  Loss: 3.234 (3.44)  Time: 0.779s, 1313.72/s  (0.787s, 1300.70/s)  LR: 5.283e-04  Data: 0.009 (0.013)
Train: 291 [ 700/1251 ( 56%)]  Loss: 3.597 (3.45)  Time: 0.771s, 1327.36/s  (0.787s, 1301.78/s)  LR: 5.283e-04  Data: 0.010 (0.013)
Train: 291 [ 750/1251 ( 60%)]  Loss: 3.498 (3.45)  Time: 0.774s, 1322.71/s  (0.786s, 1302.56/s)  LR: 5.283e-04  Data: 0.009 (0.012)
Train: 291 [ 800/1251 ( 64%)]  Loss: 3.289 (3.44)  Time: 0.774s, 1323.13/s  (0.786s, 1303.34/s)  LR: 5.283e-04  Data: 0.010 (0.012)
Train: 291 [ 850/1251 ( 68%)]  Loss: 3.811 (3.46)  Time: 0.772s, 1326.11/s  (0.785s, 1303.73/s)  LR: 5.283e-04  Data: 0.010 (0.012)
Train: 291 [ 900/1251 ( 72%)]  Loss: 3.695 (3.47)  Time: 0.784s, 1305.40/s  (0.785s, 1303.98/s)  LR: 5.283e-04  Data: 0.012 (0.012)
Train: 291 [ 950/1251 ( 76%)]  Loss: 3.624 (3.48)  Time: 0.802s, 1276.66/s  (0.786s, 1302.25/s)  LR: 5.283e-04  Data: 0.009 (0.012)
Train: 291 [1000/1251 ( 80%)]  Loss: 3.345 (3.47)  Time: 0.808s, 1267.27/s  (0.786s, 1302.25/s)  LR: 5.283e-04  Data: 0.010 (0.012)
Train: 291 [1050/1251 ( 84%)]  Loss: 3.342 (3.47)  Time: 0.777s, 1318.10/s  (0.786s, 1302.64/s)  LR: 5.283e-04  Data: 0.014 (0.012)
Train: 291 [1100/1251 ( 88%)]  Loss: 3.752 (3.48)  Time: 0.772s, 1327.11/s  (0.786s, 1302.73/s)  LR: 5.283e-04  Data: 0.010 (0.012)
Train: 291 [1150/1251 ( 92%)]  Loss: 3.592 (3.49)  Time: 0.773s, 1324.48/s  (0.786s, 1303.30/s)  LR: 5.283e-04  Data: 0.009 (0.012)
Train: 291 [1200/1251 ( 96%)]  Loss: 3.588 (3.49)  Time: 0.786s, 1303.10/s  (0.786s, 1303.30/s)  LR: 5.283e-04  Data: 0.010 (0.011)
Train: 291 [1250/1251 (100%)]  Loss: 3.182 (3.48)  Time: 0.797s, 1284.95/s  (0.786s, 1303.30/s)  LR: 5.283e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.499 (1.499)  Loss:  0.8188 (0.8188)  Acc@1: 89.5508 (89.5508)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.194 (0.566)  Loss:  0.8555 (1.2881)  Acc@1: 85.3774 (74.8040)  Acc@5: 96.9340 (92.5980)
Train: 292 [   0/1251 (  0%)]  Loss: 3.136 (3.14)  Time: 2.697s,  379.67/s  (2.697s,  379.67/s)  LR: 5.257e-04  Data: 1.963 (1.963)
Train: 292 [  50/1251 (  4%)]  Loss: 3.762 (3.45)  Time: 0.772s, 1325.86/s  (0.819s, 1249.91/s)  LR: 5.257e-04  Data: 0.009 (0.052)
Train: 292 [ 100/1251 (  8%)]  Loss: 3.520 (3.47)  Time: 0.780s, 1312.95/s  (0.800s, 1280.34/s)  LR: 5.257e-04  Data: 0.016 (0.031)
Train: 292 [ 150/1251 ( 12%)]  Loss: 3.261 (3.42)  Time: 0.773s, 1323.90/s  (0.798s, 1283.60/s)  LR: 5.257e-04  Data: 0.010 (0.024)
Train: 292 [ 200/1251 ( 16%)]  Loss: 3.264 (3.39)  Time: 0.771s, 1328.06/s  (0.794s, 1290.15/s)  LR: 5.257e-04  Data: 0.009 (0.021)
Train: 292 [ 250/1251 ( 20%)]  Loss: 3.715 (3.44)  Time: 0.783s, 1307.04/s  (0.790s, 1296.01/s)  LR: 5.257e-04  Data: 0.010 (0.019)
Train: 292 [ 300/1251 ( 24%)]  Loss: 3.434 (3.44)  Time: 0.798s, 1282.94/s  (0.789s, 1298.42/s)  LR: 5.257e-04  Data: 0.012 (0.017)
Train: 292 [ 350/1251 ( 28%)]  Loss: 3.414 (3.44)  Time: 0.784s, 1306.93/s  (0.789s, 1298.01/s)  LR: 5.257e-04  Data: 0.010 (0.016)
Train: 292 [ 400/1251 ( 32%)]  Loss: 3.405 (3.43)  Time: 0.822s, 1246.18/s  (0.788s, 1299.36/s)  LR: 5.257e-04  Data: 0.010 (0.015)
Train: 292 [ 450/1251 ( 36%)]  Loss: 3.390 (3.43)  Time: 0.796s, 1286.94/s  (0.787s, 1301.73/s)  LR: 5.257e-04  Data: 0.009 (0.015)
Train: 292 [ 500/1251 ( 40%)]  Loss: 3.475 (3.43)  Time: 0.826s, 1239.17/s  (0.790s, 1296.83/s)  LR: 5.257e-04  Data: 0.013 (0.014)
Train: 292 [ 550/1251 ( 44%)]  Loss: 3.561 (3.44)  Time: 0.773s, 1325.50/s  (0.790s, 1296.38/s)  LR: 5.257e-04  Data: 0.010 (0.014)
Train: 292 [ 600/1251 ( 48%)]  Loss: 3.630 (3.46)  Time: 0.774s, 1322.96/s  (0.789s, 1297.62/s)  LR: 5.257e-04  Data: 0.009 (0.014)
Train: 292 [ 650/1251 ( 52%)]  Loss: 3.566 (3.47)  Time: 0.782s, 1309.90/s  (0.788s, 1298.80/s)  LR: 5.257e-04  Data: 0.009 (0.013)
Train: 292 [ 700/1251 ( 56%)]  Loss: 3.289 (3.45)  Time: 0.776s, 1320.36/s  (0.788s, 1299.15/s)  LR: 5.257e-04  Data: 0.010 (0.013)
Train: 292 [ 750/1251 ( 60%)]  Loss: 3.526 (3.46)  Time: 0.776s, 1319.90/s  (0.788s, 1299.44/s)  LR: 5.257e-04  Data: 0.010 (0.013)
Train: 292 [ 800/1251 ( 64%)]  Loss: 3.674 (3.47)  Time: 0.773s, 1325.50/s  (0.787s, 1300.36/s)  LR: 5.257e-04  Data: 0.009 (0.013)
Train: 292 [ 850/1251 ( 68%)]  Loss: 3.544 (3.48)  Time: 0.772s, 1327.26/s  (0.788s, 1298.70/s)  LR: 5.257e-04  Data: 0.009 (0.012)
Train: 292 [ 900/1251 ( 72%)]  Loss: 3.505 (3.48)  Time: 0.822s, 1245.28/s  (0.789s, 1298.58/s)  LR: 5.257e-04  Data: 0.015 (0.012)
Train: 292 [ 950/1251 ( 76%)]  Loss: 3.201 (3.46)  Time: 0.800s, 1280.72/s  (0.789s, 1298.63/s)  LR: 5.257e-04  Data: 0.013 (0.012)
Train: 292 [1000/1251 ( 80%)]  Loss: 3.532 (3.47)  Time: 0.833s, 1228.95/s  (0.788s, 1299.05/s)  LR: 5.257e-04  Data: 0.010 (0.012)
Train: 292 [1050/1251 ( 84%)]  Loss: 3.428 (3.46)  Time: 0.814s, 1258.67/s  (0.788s, 1299.00/s)  LR: 5.257e-04  Data: 0.009 (0.012)
Train: 292 [1100/1251 ( 88%)]  Loss: 3.426 (3.46)  Time: 0.784s, 1306.47/s  (0.788s, 1299.41/s)  LR: 5.257e-04  Data: 0.010 (0.012)
Train: 292 [1150/1251 ( 92%)]  Loss: 3.699 (3.47)  Time: 0.772s, 1327.17/s  (0.787s, 1300.33/s)  LR: 5.257e-04  Data: 0.009 (0.012)
Train: 292 [1200/1251 ( 96%)]  Loss: 3.503 (3.47)  Time: 0.832s, 1230.38/s  (0.787s, 1300.38/s)  LR: 5.257e-04  Data: 0.010 (0.012)
Train: 292 [1250/1251 (100%)]  Loss: 3.745 (3.48)  Time: 0.759s, 1349.58/s  (0.787s, 1300.43/s)  LR: 5.257e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.507 (1.507)  Loss:  0.8623 (0.8623)  Acc@1: 88.8672 (88.8672)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.194 (0.563)  Loss:  0.9648 (1.3277)  Acc@1: 84.5519 (75.4180)  Acc@5: 95.4009 (92.8280)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-290.pth.tar', 75.57400003173828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-274.pth.tar', 75.46800011474609)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-286.pth.tar', 75.42600009033202)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-292.pth.tar', 75.41800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-283.pth.tar', 75.27400009033204)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-281.pth.tar', 75.27400001464844)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-277.pth.tar', 75.2700000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-289.pth.tar', 75.20800011230469)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-285.pth.tar', 75.20200003417969)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-276.pth.tar', 75.19200009277344)

Train: 293 [   0/1251 (  0%)]  Loss: 3.618 (3.62)  Time: 2.207s,  463.91/s  (2.207s,  463.91/s)  LR: 5.231e-04  Data: 1.480 (1.480)
Train: 293 [  50/1251 (  4%)]  Loss: 3.389 (3.50)  Time: 0.774s, 1322.23/s  (0.813s, 1259.77/s)  LR: 5.231e-04  Data: 0.010 (0.044)
Train: 293 [ 100/1251 (  8%)]  Loss: 3.309 (3.44)  Time: 0.777s, 1317.79/s  (0.798s, 1282.58/s)  LR: 5.231e-04  Data: 0.010 (0.027)
Train: 293 [ 150/1251 ( 12%)]  Loss: 3.533 (3.46)  Time: 0.773s, 1325.28/s  (0.802s, 1277.57/s)  LR: 5.231e-04  Data: 0.010 (0.021)
Train: 293 [ 200/1251 ( 16%)]  Loss: 3.254 (3.42)  Time: 0.775s, 1321.99/s  (0.799s, 1282.40/s)  LR: 5.231e-04  Data: 0.010 (0.019)
Train: 293 [ 250/1251 ( 20%)]  Loss: 3.211 (3.39)  Time: 0.774s, 1322.97/s  (0.795s, 1288.54/s)  LR: 5.231e-04  Data: 0.009 (0.017)
Train: 293 [ 300/1251 ( 24%)]  Loss: 3.232 (3.36)  Time: 0.773s, 1323.87/s  (0.792s, 1292.98/s)  LR: 5.231e-04  Data: 0.010 (0.016)
Train: 293 [ 350/1251 ( 28%)]  Loss: 2.910 (3.31)  Time: 0.772s, 1325.61/s  (0.792s, 1292.67/s)  LR: 5.231e-04  Data: 0.009 (0.015)
Train: 293 [ 400/1251 ( 32%)]  Loss: 3.392 (3.32)  Time: 0.773s, 1324.45/s  (0.791s, 1295.20/s)  LR: 5.231e-04  Data: 0.010 (0.014)
Train: 293 [ 450/1251 ( 36%)]  Loss: 3.568 (3.34)  Time: 0.772s, 1326.67/s  (0.789s, 1297.08/s)  LR: 5.231e-04  Data: 0.010 (0.014)
Train: 293 [ 500/1251 ( 40%)]  Loss: 3.415 (3.35)  Time: 0.774s, 1322.82/s  (0.789s, 1297.76/s)  LR: 5.231e-04  Data: 0.010 (0.013)
Train: 293 [ 550/1251 ( 44%)]  Loss: 3.452 (3.36)  Time: 0.781s, 1310.84/s  (0.789s, 1297.58/s)  LR: 5.231e-04  Data: 0.010 (0.013)
Train: 293 [ 600/1251 ( 48%)]  Loss: 3.364 (3.36)  Time: 0.832s, 1231.16/s  (0.789s, 1298.29/s)  LR: 5.231e-04  Data: 0.010 (0.013)
Train: 293 [ 650/1251 ( 52%)]  Loss: 3.243 (3.35)  Time: 0.772s, 1325.96/s  (0.789s, 1297.71/s)  LR: 5.231e-04  Data: 0.010 (0.013)
Train: 293 [ 700/1251 ( 56%)]  Loss: 3.502 (3.36)  Time: 0.814s, 1257.94/s  (0.791s, 1294.87/s)  LR: 5.231e-04  Data: 0.011 (0.012)
Train: 293 [ 750/1251 ( 60%)]  Loss: 3.832 (3.39)  Time: 0.772s, 1325.79/s  (0.790s, 1295.46/s)  LR: 5.231e-04  Data: 0.010 (0.012)
Train: 293 [ 800/1251 ( 64%)]  Loss: 3.409 (3.39)  Time: 0.773s, 1325.06/s  (0.790s, 1296.67/s)  LR: 5.231e-04  Data: 0.010 (0.012)
Train: 293 [ 850/1251 ( 68%)]  Loss: 2.743 (3.35)  Time: 0.773s, 1323.96/s  (0.789s, 1297.63/s)  LR: 5.231e-04  Data: 0.010 (0.012)
Train: 293 [ 900/1251 ( 72%)]  Loss: 3.417 (3.36)  Time: 0.809s, 1265.93/s  (0.789s, 1297.89/s)  LR: 5.231e-04  Data: 0.010 (0.012)
Train: 293 [ 950/1251 ( 76%)]  Loss: 3.220 (3.35)  Time: 0.775s, 1321.58/s  (0.788s, 1298.77/s)  LR: 5.231e-04  Data: 0.010 (0.012)
Train: 293 [1000/1251 ( 80%)]  Loss: 3.618 (3.36)  Time: 0.774s, 1323.02/s  (0.788s, 1299.73/s)  LR: 5.231e-04  Data: 0.010 (0.012)
Train: 293 [1050/1251 ( 84%)]  Loss: 3.577 (3.37)  Time: 0.773s, 1323.91/s  (0.788s, 1300.31/s)  LR: 5.231e-04  Data: 0.010 (0.012)
Train: 293 [1100/1251 ( 88%)]  Loss: 3.645 (3.38)  Time: 0.776s, 1318.79/s  (0.788s, 1300.04/s)  LR: 5.231e-04  Data: 0.010 (0.011)
Train: 293 [1150/1251 ( 92%)]  Loss: 3.300 (3.38)  Time: 0.772s, 1326.54/s  (0.787s, 1300.72/s)  LR: 5.231e-04  Data: 0.010 (0.011)
Train: 293 [1200/1251 ( 96%)]  Loss: 3.317 (3.38)  Time: 0.852s, 1202.46/s  (0.787s, 1300.45/s)  LR: 5.231e-04  Data: 0.009 (0.011)
Train: 293 [1250/1251 (100%)]  Loss: 3.529 (3.38)  Time: 0.760s, 1347.19/s  (0.787s, 1301.16/s)  LR: 5.231e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.583 (1.583)  Loss:  0.7031 (0.7031)  Acc@1: 90.6250 (90.6250)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.194 (0.567)  Loss:  0.8721 (1.3083)  Acc@1: 85.9670 (75.6940)  Acc@5: 96.4623 (92.9940)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-293.pth.tar', 75.69400010986328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-290.pth.tar', 75.57400003173828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-274.pth.tar', 75.46800011474609)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-286.pth.tar', 75.42600009033202)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-292.pth.tar', 75.41800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-283.pth.tar', 75.27400009033204)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-281.pth.tar', 75.27400001464844)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-277.pth.tar', 75.2700000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-289.pth.tar', 75.20800011230469)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-285.pth.tar', 75.20200003417969)

Train: 294 [   0/1251 (  0%)]  Loss: 3.383 (3.38)  Time: 2.217s,  461.87/s  (2.217s,  461.87/s)  LR: 5.205e-04  Data: 1.488 (1.488)
Train: 294 [  50/1251 (  4%)]  Loss: 3.466 (3.42)  Time: 0.773s, 1324.42/s  (0.821s, 1247.57/s)  LR: 5.205e-04  Data: 0.010 (0.046)
Train: 294 [ 100/1251 (  8%)]  Loss: 3.436 (3.43)  Time: 0.773s, 1324.34/s  (0.801s, 1278.18/s)  LR: 5.205e-04  Data: 0.010 (0.028)
Train: 294 [ 150/1251 ( 12%)]  Loss: 3.666 (3.49)  Time: 0.773s, 1324.46/s  (0.794s, 1289.19/s)  LR: 5.205e-04  Data: 0.010 (0.022)
Train: 294 [ 200/1251 ( 16%)]  Loss: 3.433 (3.48)  Time: 0.773s, 1325.17/s  (0.791s, 1294.90/s)  LR: 5.205e-04  Data: 0.009 (0.019)
Train: 294 [ 250/1251 ( 20%)]  Loss: 3.096 (3.41)  Time: 0.773s, 1323.98/s  (0.788s, 1298.99/s)  LR: 5.205e-04  Data: 0.008 (0.017)
Train: 294 [ 300/1251 ( 24%)]  Loss: 3.716 (3.46)  Time: 0.774s, 1323.70/s  (0.788s, 1299.18/s)  LR: 5.205e-04  Data: 0.010 (0.016)
Train: 294 [ 350/1251 ( 28%)]  Loss: 3.436 (3.45)  Time: 0.775s, 1321.46/s  (0.786s, 1301.97/s)  LR: 5.205e-04  Data: 0.010 (0.015)
Train: 294 [ 400/1251 ( 32%)]  Loss: 3.547 (3.46)  Time: 0.817s, 1252.70/s  (0.786s, 1303.44/s)  LR: 5.205e-04  Data: 0.010 (0.014)
Train: 294 [ 450/1251 ( 36%)]  Loss: 3.569 (3.47)  Time: 0.816s, 1254.73/s  (0.785s, 1304.60/s)  LR: 5.205e-04  Data: 0.009 (0.014)
Train: 294 [ 500/1251 ( 40%)]  Loss: 3.573 (3.48)  Time: 0.781s, 1311.88/s  (0.784s, 1305.46/s)  LR: 5.205e-04  Data: 0.010 (0.013)
Train: 294 [ 550/1251 ( 44%)]  Loss: 3.375 (3.47)  Time: 0.771s, 1328.11/s  (0.784s, 1305.51/s)  LR: 5.205e-04  Data: 0.010 (0.013)
Train: 294 [ 600/1251 ( 48%)]  Loss: 3.159 (3.45)  Time: 0.773s, 1324.41/s  (0.784s, 1306.18/s)  LR: 5.205e-04  Data: 0.009 (0.013)
Train: 294 [ 650/1251 ( 52%)]  Loss: 3.463 (3.45)  Time: 0.807s, 1268.12/s  (0.784s, 1305.86/s)  LR: 5.205e-04  Data: 0.010 (0.013)
Train: 294 [ 700/1251 ( 56%)]  Loss: 2.821 (3.41)  Time: 0.783s, 1307.01/s  (0.784s, 1306.63/s)  LR: 5.205e-04  Data: 0.009 (0.012)
Train: 294 [ 750/1251 ( 60%)]  Loss: 3.244 (3.40)  Time: 0.775s, 1321.52/s  (0.783s, 1306.99/s)  LR: 5.205e-04  Data: 0.010 (0.012)
Train: 294 [ 800/1251 ( 64%)]  Loss: 3.548 (3.41)  Time: 0.780s, 1312.28/s  (0.784s, 1306.92/s)  LR: 5.205e-04  Data: 0.011 (0.012)
Train: 294 [ 850/1251 ( 68%)]  Loss: 3.561 (3.42)  Time: 0.782s, 1308.98/s  (0.784s, 1306.78/s)  LR: 5.205e-04  Data: 0.011 (0.012)
Train: 294 [ 900/1251 ( 72%)]  Loss: 3.756 (3.43)  Time: 0.837s, 1223.49/s  (0.784s, 1306.72/s)  LR: 5.205e-04  Data: 0.009 (0.012)
Train: 294 [ 950/1251 ( 76%)]  Loss: 3.530 (3.44)  Time: 0.776s, 1319.03/s  (0.783s, 1307.04/s)  LR: 5.205e-04  Data: 0.010 (0.012)
Train: 294 [1000/1251 ( 80%)]  Loss: 3.323 (3.43)  Time: 0.773s, 1325.00/s  (0.785s, 1305.23/s)  LR: 5.205e-04  Data: 0.009 (0.012)
Train: 294 [1050/1251 ( 84%)]  Loss: 3.355 (3.43)  Time: 0.796s, 1285.71/s  (0.784s, 1305.74/s)  LR: 5.205e-04  Data: 0.010 (0.012)
Train: 294 [1100/1251 ( 88%)]  Loss: 3.350 (3.43)  Time: 0.774s, 1322.51/s  (0.784s, 1306.12/s)  LR: 5.205e-04  Data: 0.010 (0.012)
Train: 294 [1150/1251 ( 92%)]  Loss: 3.402 (3.43)  Time: 0.790s, 1296.49/s  (0.784s, 1306.32/s)  LR: 5.205e-04  Data: 0.010 (0.012)
Train: 294 [1200/1251 ( 96%)]  Loss: 3.716 (3.44)  Time: 0.774s, 1323.48/s  (0.784s, 1305.87/s)  LR: 5.205e-04  Data: 0.011 (0.011)
Train: 294 [1250/1251 (100%)]  Loss: 3.353 (3.43)  Time: 0.762s, 1343.24/s  (0.784s, 1306.13/s)  LR: 5.205e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.542 (1.542)  Loss:  0.9395 (0.9395)  Acc@1: 89.2578 (89.2578)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.194 (0.560)  Loss:  0.9883 (1.3747)  Acc@1: 84.0802 (75.2680)  Acc@5: 96.4623 (92.7980)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-293.pth.tar', 75.69400010986328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-290.pth.tar', 75.57400003173828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-274.pth.tar', 75.46800011474609)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-286.pth.tar', 75.42600009033202)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-292.pth.tar', 75.41800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-283.pth.tar', 75.27400009033204)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-281.pth.tar', 75.27400001464844)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-277.pth.tar', 75.2700000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-294.pth.tar', 75.26800006591797)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-289.pth.tar', 75.20800011230469)

Train: 295 [   0/1251 (  0%)]  Loss: 3.731 (3.73)  Time: 2.178s,  470.15/s  (2.178s,  470.15/s)  LR: 5.180e-04  Data: 1.448 (1.448)
Train: 295 [  50/1251 (  4%)]  Loss: 3.594 (3.66)  Time: 0.774s, 1322.49/s  (0.819s, 1250.82/s)  LR: 5.180e-04  Data: 0.009 (0.044)
Train: 295 [ 100/1251 (  8%)]  Loss: 3.261 (3.53)  Time: 0.774s, 1323.22/s  (0.810s, 1264.32/s)  LR: 5.180e-04  Data: 0.010 (0.027)
Train: 295 [ 150/1251 ( 12%)]  Loss: 3.732 (3.58)  Time: 0.774s, 1323.44/s  (0.801s, 1279.15/s)  LR: 5.180e-04  Data: 0.009 (0.022)
Train: 295 [ 200/1251 ( 16%)]  Loss: 3.105 (3.48)  Time: 0.773s, 1323.99/s  (0.796s, 1287.24/s)  LR: 5.180e-04  Data: 0.010 (0.019)
Train: 295 [ 250/1251 ( 20%)]  Loss: 3.315 (3.46)  Time: 0.772s, 1325.81/s  (0.792s, 1293.68/s)  LR: 5.180e-04  Data: 0.009 (0.017)
Train: 295 [ 300/1251 ( 24%)]  Loss: 3.566 (3.47)  Time: 0.778s, 1316.17/s  (0.790s, 1296.23/s)  LR: 5.180e-04  Data: 0.010 (0.016)
Train: 295 [ 350/1251 ( 28%)]  Loss: 3.432 (3.47)  Time: 0.772s, 1326.23/s  (0.789s, 1298.57/s)  LR: 5.180e-04  Data: 0.010 (0.015)
Train: 295 [ 400/1251 ( 32%)]  Loss: 3.217 (3.44)  Time: 0.779s, 1314.37/s  (0.787s, 1300.54/s)  LR: 5.180e-04  Data: 0.010 (0.014)
Train: 295 [ 450/1251 ( 36%)]  Loss: 3.594 (3.45)  Time: 0.814s, 1257.45/s  (0.787s, 1301.01/s)  LR: 5.180e-04  Data: 0.009 (0.014)
Train: 295 [ 500/1251 ( 40%)]  Loss: 3.558 (3.46)  Time: 0.772s, 1326.35/s  (0.788s, 1299.90/s)  LR: 5.180e-04  Data: 0.010 (0.013)
Train: 295 [ 550/1251 ( 44%)]  Loss: 3.190 (3.44)  Time: 0.772s, 1326.61/s  (0.788s, 1299.55/s)  LR: 5.180e-04  Data: 0.010 (0.013)
Train: 295 [ 600/1251 ( 48%)]  Loss: 3.213 (3.42)  Time: 0.774s, 1323.11/s  (0.787s, 1301.15/s)  LR: 5.180e-04  Data: 0.010 (0.013)
Train: 295 [ 650/1251 ( 52%)]  Loss: 3.314 (3.42)  Time: 0.773s, 1324.30/s  (0.787s, 1301.65/s)  LR: 5.180e-04  Data: 0.009 (0.013)
Train: 295 [ 700/1251 ( 56%)]  Loss: 3.412 (3.42)  Time: 0.773s, 1325.15/s  (0.786s, 1302.31/s)  LR: 5.180e-04  Data: 0.009 (0.012)
Train: 295 [ 750/1251 ( 60%)]  Loss: 3.729 (3.44)  Time: 0.774s, 1323.48/s  (0.786s, 1303.28/s)  LR: 5.180e-04  Data: 0.009 (0.012)
Train: 295 [ 800/1251 ( 64%)]  Loss: 3.642 (3.45)  Time: 0.772s, 1326.11/s  (0.786s, 1303.17/s)  LR: 5.180e-04  Data: 0.010 (0.012)
Train: 295 [ 850/1251 ( 68%)]  Loss: 3.416 (3.45)  Time: 0.831s, 1231.71/s  (0.786s, 1303.15/s)  LR: 5.180e-04  Data: 0.009 (0.012)
Train: 295 [ 900/1251 ( 72%)]  Loss: 3.566 (3.45)  Time: 0.773s, 1324.93/s  (0.786s, 1303.21/s)  LR: 5.180e-04  Data: 0.010 (0.012)
Train: 295 [ 950/1251 ( 76%)]  Loss: 3.736 (3.47)  Time: 0.775s, 1321.00/s  (0.785s, 1303.73/s)  LR: 5.180e-04  Data: 0.010 (0.012)
Train: 295 [1000/1251 ( 80%)]  Loss: 3.209 (3.45)  Time: 0.773s, 1325.07/s  (0.785s, 1304.26/s)  LR: 5.180e-04  Data: 0.009 (0.012)
Train: 295 [1050/1251 ( 84%)]  Loss: 3.892 (3.47)  Time: 0.856s, 1195.63/s  (0.785s, 1304.62/s)  LR: 5.180e-04  Data: 0.010 (0.012)
Train: 295 [1100/1251 ( 88%)]  Loss: 3.086 (3.46)  Time: 0.773s, 1325.16/s  (0.786s, 1302.55/s)  LR: 5.180e-04  Data: 0.010 (0.011)
Train: 295 [1150/1251 ( 92%)]  Loss: 3.556 (3.46)  Time: 0.785s, 1304.31/s  (0.787s, 1301.67/s)  LR: 5.180e-04  Data: 0.013 (0.011)
Train: 295 [1200/1251 ( 96%)]  Loss: 3.263 (3.45)  Time: 0.772s, 1327.13/s  (0.787s, 1301.92/s)  LR: 5.180e-04  Data: 0.010 (0.011)
Train: 295 [1250/1251 (100%)]  Loss: 3.374 (3.45)  Time: 0.769s, 1331.73/s  (0.786s, 1302.34/s)  LR: 5.180e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.508 (1.508)  Loss:  0.7051 (0.7051)  Acc@1: 89.4531 (89.4531)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.194 (0.575)  Loss:  0.7998 (1.2296)  Acc@1: 85.7311 (75.6300)  Acc@5: 96.3443 (93.0220)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-293.pth.tar', 75.69400010986328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-295.pth.tar', 75.63000000732421)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-290.pth.tar', 75.57400003173828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-274.pth.tar', 75.46800011474609)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-286.pth.tar', 75.42600009033202)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-292.pth.tar', 75.41800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-283.pth.tar', 75.27400009033204)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-281.pth.tar', 75.27400001464844)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-277.pth.tar', 75.2700000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-294.pth.tar', 75.26800006591797)

Train: 296 [   0/1251 (  0%)]  Loss: 3.479 (3.48)  Time: 2.308s,  443.58/s  (2.308s,  443.58/s)  LR: 5.154e-04  Data: 1.576 (1.576)
Train: 296 [  50/1251 (  4%)]  Loss: 3.250 (3.36)  Time: 0.776s, 1318.98/s  (0.822s, 1245.82/s)  LR: 5.154e-04  Data: 0.009 (0.047)
Train: 296 [ 100/1251 (  8%)]  Loss: 3.176 (3.30)  Time: 0.772s, 1325.57/s  (0.800s, 1279.66/s)  LR: 5.154e-04  Data: 0.010 (0.029)
Train: 296 [ 150/1251 ( 12%)]  Loss: 3.135 (3.26)  Time: 0.807s, 1269.60/s  (0.796s, 1286.14/s)  LR: 5.154e-04  Data: 0.010 (0.022)
Train: 296 [ 200/1251 ( 16%)]  Loss: 3.388 (3.29)  Time: 0.773s, 1323.98/s  (0.794s, 1290.40/s)  LR: 5.154e-04  Data: 0.010 (0.019)
Train: 296 [ 250/1251 ( 20%)]  Loss: 3.327 (3.29)  Time: 0.807s, 1268.90/s  (0.792s, 1293.06/s)  LR: 5.154e-04  Data: 0.012 (0.017)
Train: 296 [ 300/1251 ( 24%)]  Loss: 3.543 (3.33)  Time: 0.774s, 1323.62/s  (0.791s, 1294.20/s)  LR: 5.154e-04  Data: 0.010 (0.016)
Train: 296 [ 350/1251 ( 28%)]  Loss: 3.631 (3.37)  Time: 0.772s, 1326.19/s  (0.789s, 1297.97/s)  LR: 5.154e-04  Data: 0.009 (0.015)
Train: 296 [ 400/1251 ( 32%)]  Loss: 3.306 (3.36)  Time: 0.781s, 1310.38/s  (0.788s, 1300.31/s)  LR: 5.154e-04  Data: 0.009 (0.015)
Train: 296 [ 450/1251 ( 36%)]  Loss: 3.263 (3.35)  Time: 0.781s, 1311.74/s  (0.786s, 1302.27/s)  LR: 5.154e-04  Data: 0.009 (0.014)
Train: 296 [ 500/1251 ( 40%)]  Loss: 3.608 (3.37)  Time: 0.823s, 1244.83/s  (0.786s, 1303.15/s)  LR: 5.154e-04  Data: 0.009 (0.014)
Train: 296 [ 550/1251 ( 44%)]  Loss: 3.487 (3.38)  Time: 0.811s, 1262.74/s  (0.787s, 1300.74/s)  LR: 5.154e-04  Data: 0.009 (0.013)
Train: 296 [ 600/1251 ( 48%)]  Loss: 3.388 (3.38)  Time: 0.777s, 1318.50/s  (0.788s, 1299.69/s)  LR: 5.154e-04  Data: 0.009 (0.013)
Train: 296 [ 650/1251 ( 52%)]  Loss: 3.071 (3.36)  Time: 0.773s, 1324.58/s  (0.787s, 1300.67/s)  LR: 5.154e-04  Data: 0.010 (0.013)
Train: 296 [ 700/1251 ( 56%)]  Loss: 3.520 (3.37)  Time: 0.777s, 1318.46/s  (0.787s, 1301.34/s)  LR: 5.154e-04  Data: 0.009 (0.013)
Train: 296 [ 750/1251 ( 60%)]  Loss: 3.404 (3.37)  Time: 0.775s, 1320.87/s  (0.786s, 1302.42/s)  LR: 5.154e-04  Data: 0.009 (0.012)
Train: 296 [ 800/1251 ( 64%)]  Loss: 3.168 (3.36)  Time: 0.807s, 1268.13/s  (0.787s, 1301.46/s)  LR: 5.154e-04  Data: 0.009 (0.012)
Train: 296 [ 850/1251 ( 68%)]  Loss: 3.437 (3.37)  Time: 0.773s, 1325.50/s  (0.787s, 1300.52/s)  LR: 5.154e-04  Data: 0.011 (0.012)
Train: 296 [ 900/1251 ( 72%)]  Loss: 3.569 (3.38)  Time: 0.784s, 1306.07/s  (0.787s, 1301.09/s)  LR: 5.154e-04  Data: 0.010 (0.012)
Train: 296 [ 950/1251 ( 76%)]  Loss: 3.230 (3.37)  Time: 0.771s, 1327.87/s  (0.787s, 1301.57/s)  LR: 5.154e-04  Data: 0.009 (0.012)
Train: 296 [1000/1251 ( 80%)]  Loss: 3.614 (3.38)  Time: 0.774s, 1322.27/s  (0.786s, 1302.15/s)  LR: 5.154e-04  Data: 0.010 (0.012)
Train: 296 [1050/1251 ( 84%)]  Loss: 3.443 (3.38)  Time: 0.774s, 1323.05/s  (0.787s, 1301.57/s)  LR: 5.154e-04  Data: 0.009 (0.012)
Train: 296 [1100/1251 ( 88%)]  Loss: 3.617 (3.39)  Time: 0.783s, 1308.25/s  (0.787s, 1301.59/s)  LR: 5.154e-04  Data: 0.010 (0.012)
Train: 296 [1150/1251 ( 92%)]  Loss: 3.586 (3.40)  Time: 0.773s, 1324.02/s  (0.786s, 1302.31/s)  LR: 5.154e-04  Data: 0.010 (0.012)
Train: 296 [1200/1251 ( 96%)]  Loss: 3.442 (3.40)  Time: 0.773s, 1324.53/s  (0.786s, 1302.10/s)  LR: 5.154e-04  Data: 0.009 (0.011)
Train: 296 [1250/1251 (100%)]  Loss: 3.668 (3.41)  Time: 0.784s, 1306.41/s  (0.786s, 1302.54/s)  LR: 5.154e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.592 (1.592)  Loss:  0.6851 (0.6851)  Acc@1: 89.5508 (89.5508)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.194 (0.559)  Loss:  0.8530 (1.2193)  Acc@1: 84.5519 (75.8580)  Acc@5: 96.5802 (93.2320)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-296.pth.tar', 75.85800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-293.pth.tar', 75.69400010986328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-295.pth.tar', 75.63000000732421)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-290.pth.tar', 75.57400003173828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-274.pth.tar', 75.46800011474609)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-286.pth.tar', 75.42600009033202)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-292.pth.tar', 75.41800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-283.pth.tar', 75.27400009033204)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-281.pth.tar', 75.27400001464844)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-277.pth.tar', 75.2700000366211)

Train: 297 [   0/1251 (  0%)]  Loss: 3.442 (3.44)  Time: 2.351s,  435.47/s  (2.351s,  435.47/s)  LR: 5.128e-04  Data: 1.578 (1.578)
Train: 297 [  50/1251 (  4%)]  Loss: 3.716 (3.58)  Time: 0.770s, 1329.42/s  (0.828s, 1236.88/s)  LR: 5.128e-04  Data: 0.010 (0.042)
Train: 297 [ 100/1251 (  8%)]  Loss: 3.090 (3.42)  Time: 0.818s, 1251.85/s  (0.806s, 1270.64/s)  LR: 5.128e-04  Data: 0.011 (0.026)
Train: 297 [ 150/1251 ( 12%)]  Loss: 3.873 (3.53)  Time: 0.776s, 1320.09/s  (0.800s, 1280.04/s)  LR: 5.128e-04  Data: 0.010 (0.021)
Train: 297 [ 200/1251 ( 16%)]  Loss: 3.424 (3.51)  Time: 0.818s, 1251.10/s  (0.799s, 1280.95/s)  LR: 5.128e-04  Data: 0.009 (0.018)
Train: 297 [ 250/1251 ( 20%)]  Loss: 3.138 (3.45)  Time: 0.769s, 1331.37/s  (0.796s, 1285.94/s)  LR: 5.128e-04  Data: 0.010 (0.016)
Train: 297 [ 300/1251 ( 24%)]  Loss: 3.839 (3.50)  Time: 0.772s, 1325.87/s  (0.794s, 1289.89/s)  LR: 5.128e-04  Data: 0.010 (0.015)
Train: 297 [ 350/1251 ( 28%)]  Loss: 3.513 (3.50)  Time: 0.825s, 1241.79/s  (0.797s, 1285.25/s)  LR: 5.128e-04  Data: 0.010 (0.015)
Train: 297 [ 400/1251 ( 32%)]  Loss: 3.678 (3.52)  Time: 0.772s, 1326.08/s  (0.795s, 1288.53/s)  LR: 5.128e-04  Data: 0.010 (0.014)
Train: 297 [ 450/1251 ( 36%)]  Loss: 3.074 (3.48)  Time: 0.772s, 1325.57/s  (0.793s, 1291.43/s)  LR: 5.128e-04  Data: 0.009 (0.014)
Train: 297 [ 500/1251 ( 40%)]  Loss: 3.101 (3.44)  Time: 0.808s, 1266.75/s  (0.792s, 1292.78/s)  LR: 5.128e-04  Data: 0.010 (0.013)
Train: 297 [ 550/1251 ( 44%)]  Loss: 3.503 (3.45)  Time: 0.807s, 1268.67/s  (0.794s, 1289.28/s)  LR: 5.128e-04  Data: 0.009 (0.013)
Train: 297 [ 600/1251 ( 48%)]  Loss: 3.412 (3.45)  Time: 0.774s, 1323.76/s  (0.794s, 1289.02/s)  LR: 5.128e-04  Data: 0.009 (0.013)
Train: 297 [ 650/1251 ( 52%)]  Loss: 3.556 (3.45)  Time: 0.781s, 1311.16/s  (0.796s, 1287.10/s)  LR: 5.128e-04  Data: 0.009 (0.013)
Train: 297 [ 700/1251 ( 56%)]  Loss: 3.680 (3.47)  Time: 0.772s, 1326.65/s  (0.795s, 1288.61/s)  LR: 5.128e-04  Data: 0.009 (0.012)
Train: 297 [ 750/1251 ( 60%)]  Loss: 3.427 (3.47)  Time: 0.771s, 1328.57/s  (0.794s, 1289.03/s)  LR: 5.128e-04  Data: 0.008 (0.012)
Train: 297 [ 800/1251 ( 64%)]  Loss: 3.326 (3.46)  Time: 0.784s, 1305.86/s  (0.795s, 1288.35/s)  LR: 5.128e-04  Data: 0.010 (0.012)
Train: 297 [ 850/1251 ( 68%)]  Loss: 3.435 (3.46)  Time: 0.772s, 1325.67/s  (0.794s, 1290.16/s)  LR: 5.128e-04  Data: 0.009 (0.012)
Train: 297 [ 900/1251 ( 72%)]  Loss: 3.510 (3.46)  Time: 0.775s, 1322.09/s  (0.793s, 1291.44/s)  LR: 5.128e-04  Data: 0.009 (0.012)
Train: 297 [ 950/1251 ( 76%)]  Loss: 3.689 (3.47)  Time: 0.808s, 1267.03/s  (0.792s, 1292.32/s)  LR: 5.128e-04  Data: 0.009 (0.012)
Train: 297 [1000/1251 ( 80%)]  Loss: 3.351 (3.47)  Time: 0.808s, 1267.24/s  (0.793s, 1290.93/s)  LR: 5.128e-04  Data: 0.009 (0.011)
Train: 297 [1050/1251 ( 84%)]  Loss: 3.337 (3.46)  Time: 0.780s, 1313.35/s  (0.794s, 1290.41/s)  LR: 5.128e-04  Data: 0.011 (0.011)
Train: 297 [1100/1251 ( 88%)]  Loss: 3.590 (3.47)  Time: 0.775s, 1321.79/s  (0.793s, 1291.16/s)  LR: 5.128e-04  Data: 0.010 (0.011)
Train: 297 [1150/1251 ( 92%)]  Loss: 3.444 (3.46)  Time: 0.773s, 1324.80/s  (0.792s, 1292.32/s)  LR: 5.128e-04  Data: 0.010 (0.011)
Train: 297 [1200/1251 ( 96%)]  Loss: 3.550 (3.47)  Time: 0.789s, 1297.75/s  (0.792s, 1292.96/s)  LR: 5.128e-04  Data: 0.010 (0.011)
Train: 297 [1250/1251 (100%)]  Loss: 2.882 (3.45)  Time: 0.797s, 1284.59/s  (0.792s, 1292.29/s)  LR: 5.128e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.520 (1.520)  Loss:  0.7393 (0.7393)  Acc@1: 89.9414 (89.9414)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.194 (0.568)  Loss:  0.8896 (1.2545)  Acc@1: 83.8443 (75.6260)  Acc@5: 95.4009 (93.0480)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-296.pth.tar', 75.85800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-293.pth.tar', 75.69400010986328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-295.pth.tar', 75.63000000732421)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-297.pth.tar', 75.6259999633789)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-290.pth.tar', 75.57400003173828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-274.pth.tar', 75.46800011474609)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-286.pth.tar', 75.42600009033202)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-292.pth.tar', 75.41800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-283.pth.tar', 75.27400009033204)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-281.pth.tar', 75.27400001464844)

Train: 298 [   0/1251 (  0%)]  Loss: 3.689 (3.69)  Time: 2.224s,  460.33/s  (2.224s,  460.33/s)  LR: 5.102e-04  Data: 1.496 (1.496)
Train: 298 [  50/1251 (  4%)]  Loss: 3.250 (3.47)  Time: 0.774s, 1322.52/s  (0.810s, 1263.47/s)  LR: 5.102e-04  Data: 0.010 (0.044)
Train: 298 [ 100/1251 (  8%)]  Loss: 3.652 (3.53)  Time: 0.775s, 1321.06/s  (0.800s, 1280.60/s)  LR: 5.102e-04  Data: 0.010 (0.027)
Train: 298 [ 150/1251 ( 12%)]  Loss: 3.494 (3.52)  Time: 0.770s, 1329.51/s  (0.793s, 1290.76/s)  LR: 5.102e-04  Data: 0.010 (0.021)
Train: 298 [ 200/1251 ( 16%)]  Loss: 3.080 (3.43)  Time: 0.772s, 1326.73/s  (0.792s, 1293.52/s)  LR: 5.102e-04  Data: 0.009 (0.018)
Train: 298 [ 250/1251 ( 20%)]  Loss: 3.347 (3.42)  Time: 0.774s, 1323.80/s  (0.790s, 1296.56/s)  LR: 5.102e-04  Data: 0.010 (0.017)
Train: 298 [ 300/1251 ( 24%)]  Loss: 3.539 (3.44)  Time: 0.773s, 1325.44/s  (0.790s, 1296.55/s)  LR: 5.102e-04  Data: 0.010 (0.016)
Train: 298 [ 350/1251 ( 28%)]  Loss: 3.630 (3.46)  Time: 0.822s, 1245.99/s  (0.789s, 1298.57/s)  LR: 5.102e-04  Data: 0.009 (0.015)
Train: 298 [ 400/1251 ( 32%)]  Loss: 3.207 (3.43)  Time: 0.775s, 1320.60/s  (0.788s, 1299.33/s)  LR: 5.102e-04  Data: 0.010 (0.014)
Train: 298 [ 450/1251 ( 36%)]  Loss: 3.267 (3.42)  Time: 0.774s, 1322.96/s  (0.787s, 1300.85/s)  LR: 5.102e-04  Data: 0.009 (0.014)
Train: 298 [ 500/1251 ( 40%)]  Loss: 3.616 (3.43)  Time: 0.775s, 1320.78/s  (0.787s, 1301.77/s)  LR: 5.102e-04  Data: 0.010 (0.013)
Train: 298 [ 550/1251 ( 44%)]  Loss: 3.088 (3.40)  Time: 0.822s, 1246.03/s  (0.787s, 1300.88/s)  LR: 5.102e-04  Data: 0.008 (0.013)
Train: 298 [ 600/1251 ( 48%)]  Loss: 3.752 (3.43)  Time: 0.775s, 1322.00/s  (0.787s, 1301.10/s)  LR: 5.102e-04  Data: 0.010 (0.013)
Train: 298 [ 650/1251 ( 52%)]  Loss: 3.255 (3.42)  Time: 0.773s, 1324.72/s  (0.787s, 1301.56/s)  LR: 5.102e-04  Data: 0.010 (0.012)
Train: 298 [ 700/1251 ( 56%)]  Loss: 3.491 (3.42)  Time: 0.779s, 1313.78/s  (0.787s, 1301.43/s)  LR: 5.102e-04  Data: 0.010 (0.012)
Train: 298 [ 750/1251 ( 60%)]  Loss: 3.261 (3.41)  Time: 0.772s, 1326.15/s  (0.787s, 1301.48/s)  LR: 5.102e-04  Data: 0.010 (0.012)
Train: 298 [ 800/1251 ( 64%)]  Loss: 3.841 (3.44)  Time: 0.775s, 1320.98/s  (0.787s, 1301.83/s)  LR: 5.102e-04  Data: 0.009 (0.012)
Train: 298 [ 850/1251 ( 68%)]  Loss: 3.568 (3.45)  Time: 0.783s, 1308.21/s  (0.787s, 1301.89/s)  LR: 5.102e-04  Data: 0.009 (0.012)
Train: 298 [ 900/1251 ( 72%)]  Loss: 3.675 (3.46)  Time: 0.781s, 1311.56/s  (0.786s, 1301.97/s)  LR: 5.102e-04  Data: 0.010 (0.012)
Train: 298 [ 950/1251 ( 76%)]  Loss: 3.349 (3.45)  Time: 0.774s, 1323.75/s  (0.786s, 1302.31/s)  LR: 5.102e-04  Data: 0.011 (0.012)
Train: 298 [1000/1251 ( 80%)]  Loss: 3.309 (3.45)  Time: 0.782s, 1308.99/s  (0.786s, 1302.88/s)  LR: 5.102e-04  Data: 0.010 (0.011)
Train: 298 [1050/1251 ( 84%)]  Loss: 3.105 (3.43)  Time: 0.772s, 1325.69/s  (0.786s, 1303.62/s)  LR: 5.102e-04  Data: 0.010 (0.011)
Train: 298 [1100/1251 ( 88%)]  Loss: 3.604 (3.44)  Time: 0.773s, 1325.55/s  (0.785s, 1303.98/s)  LR: 5.102e-04  Data: 0.009 (0.011)
Train: 298 [1150/1251 ( 92%)]  Loss: 3.732 (3.45)  Time: 0.773s, 1324.96/s  (0.785s, 1304.59/s)  LR: 5.102e-04  Data: 0.010 (0.011)
Train: 298 [1200/1251 ( 96%)]  Loss: 3.476 (3.45)  Time: 0.776s, 1319.10/s  (0.785s, 1304.70/s)  LR: 5.102e-04  Data: 0.010 (0.011)
Train: 298 [1250/1251 (100%)]  Loss: 3.287 (3.44)  Time: 0.767s, 1334.68/s  (0.785s, 1305.13/s)  LR: 5.102e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.513 (1.513)  Loss:  0.7666 (0.7666)  Acc@1: 88.1836 (88.1836)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.194 (0.569)  Loss:  0.9072 (1.2766)  Acc@1: 84.5519 (75.1960)  Acc@5: 95.9906 (93.0420)
Train: 299 [   0/1251 (  0%)]  Loss: 3.652 (3.65)  Time: 2.339s,  437.81/s  (2.339s,  437.81/s)  LR: 5.076e-04  Data: 1.615 (1.615)
Train: 299 [  50/1251 (  4%)]  Loss: 3.732 (3.69)  Time: 0.772s, 1325.69/s  (0.816s, 1255.33/s)  LR: 5.076e-04  Data: 0.009 (0.046)
Train: 299 [ 100/1251 (  8%)]  Loss: 3.422 (3.60)  Time: 0.781s, 1310.98/s  (0.799s, 1281.21/s)  LR: 5.076e-04  Data: 0.009 (0.028)
Train: 299 [ 150/1251 ( 12%)]  Loss: 3.818 (3.66)  Time: 0.778s, 1316.09/s  (0.793s, 1290.83/s)  LR: 5.076e-04  Data: 0.009 (0.022)
Train: 299 [ 200/1251 ( 16%)]  Loss: 2.935 (3.51)  Time: 0.773s, 1325.13/s  (0.792s, 1292.87/s)  LR: 5.076e-04  Data: 0.010 (0.019)
Train: 299 [ 250/1251 ( 20%)]  Loss: 3.456 (3.50)  Time: 0.786s, 1303.08/s  (0.790s, 1296.00/s)  LR: 5.076e-04  Data: 0.010 (0.017)
Train: 299 [ 300/1251 ( 24%)]  Loss: 3.781 (3.54)  Time: 0.773s, 1324.44/s  (0.789s, 1297.06/s)  LR: 5.076e-04  Data: 0.009 (0.016)
Train: 299 [ 350/1251 ( 28%)]  Loss: 3.300 (3.51)  Time: 0.772s, 1326.02/s  (0.788s, 1299.10/s)  LR: 5.076e-04  Data: 0.010 (0.015)
Train: 299 [ 400/1251 ( 32%)]  Loss: 3.682 (3.53)  Time: 0.772s, 1326.14/s  (0.787s, 1300.98/s)  LR: 5.076e-04  Data: 0.010 (0.014)
Train: 299 [ 450/1251 ( 36%)]  Loss: 3.458 (3.52)  Time: 0.773s, 1324.63/s  (0.786s, 1302.62/s)  LR: 5.076e-04  Data: 0.009 (0.014)
Train: 299 [ 500/1251 ( 40%)]  Loss: 3.488 (3.52)  Time: 0.770s, 1329.43/s  (0.785s, 1304.04/s)  LR: 5.076e-04  Data: 0.009 (0.013)
Train: 299 [ 550/1251 ( 44%)]  Loss: 3.371 (3.51)  Time: 0.774s, 1323.58/s  (0.785s, 1304.66/s)  LR: 5.076e-04  Data: 0.011 (0.013)
Train: 299 [ 600/1251 ( 48%)]  Loss: 3.649 (3.52)  Time: 0.782s, 1309.80/s  (0.784s, 1305.34/s)  LR: 5.076e-04  Data: 0.009 (0.013)
Train: 299 [ 650/1251 ( 52%)]  Loss: 3.800 (3.54)  Time: 0.808s, 1267.19/s  (0.784s, 1305.34/s)  LR: 5.076e-04  Data: 0.009 (0.013)
Train: 299 [ 700/1251 ( 56%)]  Loss: 3.579 (3.54)  Time: 0.773s, 1324.65/s  (0.785s, 1303.72/s)  LR: 5.076e-04  Data: 0.009 (0.012)
Train: 299 [ 750/1251 ( 60%)]  Loss: 3.611 (3.55)  Time: 0.774s, 1322.30/s  (0.785s, 1304.40/s)  LR: 5.076e-04  Data: 0.009 (0.012)
Train: 299 [ 800/1251 ( 64%)]  Loss: 3.311 (3.53)  Time: 0.775s, 1321.15/s  (0.785s, 1305.11/s)  LR: 5.076e-04  Data: 0.010 (0.012)
Train: 299 [ 850/1251 ( 68%)]  Loss: 3.250 (3.52)  Time: 0.814s, 1257.61/s  (0.785s, 1303.97/s)  LR: 5.076e-04  Data: 0.011 (0.012)
Train: 299 [ 900/1251 ( 72%)]  Loss: 3.533 (3.52)  Time: 0.824s, 1242.74/s  (0.786s, 1302.18/s)  LR: 5.076e-04  Data: 0.011 (0.012)
Train: 299 [ 950/1251 ( 76%)]  Loss: 3.246 (3.50)  Time: 0.784s, 1306.84/s  (0.786s, 1302.16/s)  LR: 5.076e-04  Data: 0.010 (0.012)
Train: 299 [1000/1251 ( 80%)]  Loss: 3.187 (3.49)  Time: 0.779s, 1314.35/s  (0.786s, 1302.77/s)  LR: 5.076e-04  Data: 0.009 (0.012)
Train: 299 [1050/1251 ( 84%)]  Loss: 3.424 (3.49)  Time: 0.777s, 1318.41/s  (0.787s, 1300.89/s)  LR: 5.076e-04  Data: 0.013 (0.012)
Train: 299 [1100/1251 ( 88%)]  Loss: 3.261 (3.48)  Time: 0.816s, 1254.19/s  (0.788s, 1300.06/s)  LR: 5.076e-04  Data: 0.011 (0.012)
Train: 299 [1150/1251 ( 92%)]  Loss: 3.695 (3.49)  Time: 0.773s, 1324.12/s  (0.788s, 1299.17/s)  LR: 5.076e-04  Data: 0.010 (0.012)
Train: 299 [1200/1251 ( 96%)]  Loss: 3.379 (3.48)  Time: 0.774s, 1323.34/s  (0.788s, 1299.88/s)  LR: 5.076e-04  Data: 0.011 (0.011)
Train: 299 [1250/1251 (100%)]  Loss: 3.719 (3.49)  Time: 0.759s, 1348.98/s  (0.787s, 1300.66/s)  LR: 5.076e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.540 (1.540)  Loss:  0.8086 (0.8086)  Acc@1: 90.2344 (90.2344)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.193 (0.564)  Loss:  0.9155 (1.3172)  Acc@1: 85.4953 (75.2880)  Acc@5: 96.8160 (92.8420)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-296.pth.tar', 75.85800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-293.pth.tar', 75.69400010986328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-295.pth.tar', 75.63000000732421)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-297.pth.tar', 75.6259999633789)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-290.pth.tar', 75.57400003173828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-274.pth.tar', 75.46800011474609)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-286.pth.tar', 75.42600009033202)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-292.pth.tar', 75.41800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-299.pth.tar', 75.28800003417969)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-283.pth.tar', 75.27400009033204)

Train: 300 [   0/1251 (  0%)]  Loss: 3.306 (3.31)  Time: 2.188s,  467.98/s  (2.188s,  467.98/s)  LR: 5.050e-04  Data: 1.438 (1.438)
Train: 300 [  50/1251 (  4%)]  Loss: 3.395 (3.35)  Time: 0.776s, 1318.84/s  (0.820s, 1249.08/s)  LR: 5.050e-04  Data: 0.010 (0.043)
Train: 300 [ 100/1251 (  8%)]  Loss: 3.396 (3.37)  Time: 0.796s, 1286.85/s  (0.801s, 1279.14/s)  LR: 5.050e-04  Data: 0.010 (0.027)
Train: 300 [ 150/1251 ( 12%)]  Loss: 3.167 (3.32)  Time: 0.784s, 1306.81/s  (0.794s, 1290.12/s)  LR: 5.050e-04  Data: 0.010 (0.021)
Train: 300 [ 200/1251 ( 16%)]  Loss: 3.518 (3.36)  Time: 0.793s, 1291.10/s  (0.791s, 1294.30/s)  LR: 5.050e-04  Data: 0.010 (0.019)
Train: 300 [ 250/1251 ( 20%)]  Loss: 3.672 (3.41)  Time: 0.783s, 1307.56/s  (0.790s, 1297.02/s)  LR: 5.050e-04  Data: 0.014 (0.017)
Train: 300 [ 300/1251 ( 24%)]  Loss: 3.548 (3.43)  Time: 0.774s, 1323.04/s  (0.789s, 1298.50/s)  LR: 5.050e-04  Data: 0.010 (0.016)
Train: 300 [ 350/1251 ( 28%)]  Loss: 3.442 (3.43)  Time: 0.790s, 1295.56/s  (0.787s, 1300.93/s)  LR: 5.050e-04  Data: 0.010 (0.015)
Train: 300 [ 400/1251 ( 32%)]  Loss: 3.289 (3.41)  Time: 0.774s, 1322.79/s  (0.786s, 1302.41/s)  LR: 5.050e-04  Data: 0.010 (0.014)
Train: 300 [ 450/1251 ( 36%)]  Loss: 3.420 (3.42)  Time: 0.777s, 1317.47/s  (0.788s, 1298.71/s)  LR: 5.050e-04  Data: 0.012 (0.014)
Train: 300 [ 500/1251 ( 40%)]  Loss: 3.478 (3.42)  Time: 0.774s, 1322.65/s  (0.788s, 1299.04/s)  LR: 5.050e-04  Data: 0.011 (0.014)
Train: 300 [ 550/1251 ( 44%)]  Loss: 3.557 (3.43)  Time: 0.772s, 1325.87/s  (0.788s, 1299.74/s)  LR: 5.050e-04  Data: 0.009 (0.013)
Train: 300 [ 600/1251 ( 48%)]  Loss: 3.180 (3.41)  Time: 0.788s, 1299.28/s  (0.787s, 1301.22/s)  LR: 5.050e-04  Data: 0.009 (0.013)
Train: 300 [ 650/1251 ( 52%)]  Loss: 3.387 (3.41)  Time: 0.774s, 1322.79/s  (0.786s, 1302.40/s)  LR: 5.050e-04  Data: 0.009 (0.013)
Train: 300 [ 700/1251 ( 56%)]  Loss: 2.817 (3.37)  Time: 0.774s, 1323.69/s  (0.786s, 1303.43/s)  LR: 5.050e-04  Data: 0.010 (0.013)
Train: 300 [ 750/1251 ( 60%)]  Loss: 3.454 (3.38)  Time: 0.772s, 1326.60/s  (0.785s, 1304.62/s)  LR: 5.050e-04  Data: 0.009 (0.012)
Train: 300 [ 800/1251 ( 64%)]  Loss: 3.422 (3.38)  Time: 0.785s, 1305.28/s  (0.785s, 1304.46/s)  LR: 5.050e-04  Data: 0.010 (0.012)
Train: 300 [ 850/1251 ( 68%)]  Loss: 3.642 (3.39)  Time: 0.831s, 1232.30/s  (0.785s, 1304.18/s)  LR: 5.050e-04  Data: 0.010 (0.012)
Train: 300 [ 900/1251 ( 72%)]  Loss: 3.409 (3.39)  Time: 0.830s, 1233.04/s  (0.787s, 1301.11/s)  LR: 5.050e-04  Data: 0.013 (0.012)
Train: 300 [ 950/1251 ( 76%)]  Loss: 3.875 (3.42)  Time: 0.814s, 1257.68/s  (0.787s, 1300.65/s)  LR: 5.050e-04  Data: 0.010 (0.012)
Train: 300 [1000/1251 ( 80%)]  Loss: 3.569 (3.43)  Time: 0.788s, 1299.62/s  (0.788s, 1299.76/s)  LR: 5.050e-04  Data: 0.010 (0.012)
Train: 300 [1050/1251 ( 84%)]  Loss: 3.302 (3.42)  Time: 0.774s, 1323.35/s  (0.789s, 1298.37/s)  LR: 5.050e-04  Data: 0.009 (0.012)
Train: 300 [1100/1251 ( 88%)]  Loss: 3.133 (3.41)  Time: 0.774s, 1322.19/s  (0.788s, 1299.36/s)  LR: 5.050e-04  Data: 0.009 (0.012)
Train: 300 [1150/1251 ( 92%)]  Loss: 3.299 (3.40)  Time: 0.827s, 1237.72/s  (0.788s, 1298.71/s)  LR: 5.050e-04  Data: 0.014 (0.012)
Train: 300 [1200/1251 ( 96%)]  Loss: 3.561 (3.41)  Time: 0.772s, 1326.72/s  (0.788s, 1298.81/s)  LR: 5.050e-04  Data: 0.010 (0.011)
Train: 300 [1250/1251 (100%)]  Loss: 3.547 (3.41)  Time: 0.796s, 1286.55/s  (0.788s, 1299.18/s)  LR: 5.050e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.573 (1.573)  Loss:  0.7778 (0.7778)  Acc@1: 87.6953 (87.6953)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.194 (0.565)  Loss:  0.8833 (1.2865)  Acc@1: 85.6132 (75.9600)  Acc@5: 95.9906 (93.1020)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-300.pth.tar', 75.96000008544922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-296.pth.tar', 75.85800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-293.pth.tar', 75.69400010986328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-295.pth.tar', 75.63000000732421)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-297.pth.tar', 75.6259999633789)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-290.pth.tar', 75.57400003173828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-274.pth.tar', 75.46800011474609)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-286.pth.tar', 75.42600009033202)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-292.pth.tar', 75.41800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-299.pth.tar', 75.28800003417969)

Train: 301 [   0/1251 (  0%)]  Loss: 3.402 (3.40)  Time: 2.244s,  456.29/s  (2.244s,  456.29/s)  LR: 5.024e-04  Data: 1.517 (1.517)
Train: 301 [  50/1251 (  4%)]  Loss: 3.510 (3.46)  Time: 0.788s, 1299.39/s  (0.816s, 1254.51/s)  LR: 5.024e-04  Data: 0.015 (0.043)
Train: 301 [ 100/1251 (  8%)]  Loss: 3.635 (3.52)  Time: 0.781s, 1311.64/s  (0.797s, 1284.76/s)  LR: 5.024e-04  Data: 0.012 (0.026)
Train: 301 [ 150/1251 ( 12%)]  Loss: 3.848 (3.60)  Time: 0.810s, 1264.65/s  (0.796s, 1286.30/s)  LR: 5.024e-04  Data: 0.010 (0.021)
Train: 301 [ 200/1251 ( 16%)]  Loss: 3.449 (3.57)  Time: 0.775s, 1321.19/s  (0.794s, 1290.29/s)  LR: 5.024e-04  Data: 0.010 (0.018)
Train: 301 [ 250/1251 ( 20%)]  Loss: 3.327 (3.53)  Time: 0.774s, 1323.68/s  (0.792s, 1292.52/s)  LR: 5.024e-04  Data: 0.010 (0.016)
Train: 301 [ 300/1251 ( 24%)]  Loss: 3.153 (3.47)  Time: 0.773s, 1325.24/s  (0.791s, 1294.71/s)  LR: 5.024e-04  Data: 0.009 (0.015)
Train: 301 [ 350/1251 ( 28%)]  Loss: 3.380 (3.46)  Time: 0.772s, 1325.78/s  (0.789s, 1297.44/s)  LR: 5.024e-04  Data: 0.009 (0.015)
Train: 301 [ 400/1251 ( 32%)]  Loss: 3.576 (3.48)  Time: 0.808s, 1268.07/s  (0.789s, 1297.65/s)  LR: 5.024e-04  Data: 0.010 (0.014)
Train: 301 [ 450/1251 ( 36%)]  Loss: 3.560 (3.48)  Time: 0.782s, 1309.21/s  (0.789s, 1297.79/s)  LR: 5.024e-04  Data: 0.010 (0.013)
Train: 301 [ 500/1251 ( 40%)]  Loss: 3.633 (3.50)  Time: 0.773s, 1325.35/s  (0.788s, 1299.87/s)  LR: 5.024e-04  Data: 0.009 (0.013)
Train: 301 [ 550/1251 ( 44%)]  Loss: 3.437 (3.49)  Time: 0.778s, 1315.92/s  (0.788s, 1300.28/s)  LR: 5.024e-04  Data: 0.009 (0.013)
Train: 301 [ 600/1251 ( 48%)]  Loss: 3.585 (3.50)  Time: 0.774s, 1323.76/s  (0.787s, 1301.11/s)  LR: 5.024e-04  Data: 0.010 (0.013)
Train: 301 [ 650/1251 ( 52%)]  Loss: 3.238 (3.48)  Time: 0.771s, 1328.05/s  (0.787s, 1301.28/s)  LR: 5.024e-04  Data: 0.010 (0.012)
Train: 301 [ 700/1251 ( 56%)]  Loss: 3.195 (3.46)  Time: 0.788s, 1300.16/s  (0.787s, 1300.97/s)  LR: 5.024e-04  Data: 0.009 (0.012)
Train: 301 [ 750/1251 ( 60%)]  Loss: 3.591 (3.47)  Time: 0.775s, 1320.69/s  (0.786s, 1302.11/s)  LR: 5.024e-04  Data: 0.010 (0.012)
Train: 301 [ 800/1251 ( 64%)]  Loss: 3.441 (3.47)  Time: 0.773s, 1324.44/s  (0.787s, 1301.96/s)  LR: 5.024e-04  Data: 0.010 (0.012)
Train: 301 [ 850/1251 ( 68%)]  Loss: 3.708 (3.48)  Time: 0.801s, 1278.04/s  (0.786s, 1302.62/s)  LR: 5.024e-04  Data: 0.010 (0.012)
Train: 301 [ 900/1251 ( 72%)]  Loss: 3.299 (3.47)  Time: 0.774s, 1323.84/s  (0.786s, 1303.19/s)  LR: 5.024e-04  Data: 0.010 (0.012)
Train: 301 [ 950/1251 ( 76%)]  Loss: 3.472 (3.47)  Time: 0.773s, 1324.82/s  (0.785s, 1303.98/s)  LR: 5.024e-04  Data: 0.010 (0.012)
Train: 301 [1000/1251 ( 80%)]  Loss: 3.555 (3.48)  Time: 0.788s, 1300.18/s  (0.786s, 1303.34/s)  LR: 5.024e-04  Data: 0.010 (0.011)
Train: 301 [1050/1251 ( 84%)]  Loss: 3.625 (3.48)  Time: 0.772s, 1326.06/s  (0.787s, 1301.75/s)  LR: 5.024e-04  Data: 0.010 (0.011)
Train: 301 [1100/1251 ( 88%)]  Loss: 3.472 (3.48)  Time: 0.772s, 1325.64/s  (0.786s, 1302.21/s)  LR: 5.024e-04  Data: 0.009 (0.011)
Train: 301 [1150/1251 ( 92%)]  Loss: 3.369 (3.48)  Time: 0.772s, 1326.66/s  (0.786s, 1302.26/s)  LR: 5.024e-04  Data: 0.011 (0.011)
Train: 301 [1200/1251 ( 96%)]  Loss: 3.319 (3.47)  Time: 0.779s, 1313.84/s  (0.786s, 1302.81/s)  LR: 5.024e-04  Data: 0.010 (0.011)
Train: 301 [1250/1251 (100%)]  Loss: 3.366 (3.47)  Time: 0.765s, 1338.52/s  (0.786s, 1303.37/s)  LR: 5.024e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.610 (1.610)  Loss:  0.7939 (0.7939)  Acc@1: 88.6719 (88.6719)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.194 (0.571)  Loss:  0.8608 (1.3298)  Acc@1: 85.6132 (75.5600)  Acc@5: 96.3443 (93.0720)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-300.pth.tar', 75.96000008544922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-296.pth.tar', 75.85800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-293.pth.tar', 75.69400010986328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-295.pth.tar', 75.63000000732421)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-297.pth.tar', 75.6259999633789)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-290.pth.tar', 75.57400003173828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-301.pth.tar', 75.55999995605468)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-274.pth.tar', 75.46800011474609)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-286.pth.tar', 75.42600009033202)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-292.pth.tar', 75.41800001220703)

Train: 302 [   0/1251 (  0%)]  Loss: 3.439 (3.44)  Time: 2.374s,  431.33/s  (2.374s,  431.33/s)  LR: 4.998e-04  Data: 1.641 (1.641)
Train: 302 [  50/1251 (  4%)]  Loss: 3.384 (3.41)  Time: 0.840s, 1218.99/s  (0.820s, 1248.35/s)  LR: 4.998e-04  Data: 0.010 (0.044)
Train: 302 [ 100/1251 (  8%)]  Loss: 3.459 (3.43)  Time: 0.776s, 1320.21/s  (0.801s, 1277.63/s)  LR: 4.998e-04  Data: 0.010 (0.027)
Train: 302 [ 150/1251 ( 12%)]  Loss: 3.531 (3.45)  Time: 0.774s, 1323.63/s  (0.797s, 1285.34/s)  LR: 4.998e-04  Data: 0.010 (0.021)
Train: 302 [ 200/1251 ( 16%)]  Loss: 3.068 (3.38)  Time: 0.778s, 1316.31/s  (0.792s, 1292.15/s)  LR: 4.998e-04  Data: 0.010 (0.018)
Train: 302 [ 250/1251 ( 20%)]  Loss: 3.536 (3.40)  Time: 0.779s, 1314.95/s  (0.791s, 1294.03/s)  LR: 4.998e-04  Data: 0.010 (0.017)
Train: 302 [ 300/1251 ( 24%)]  Loss: 3.176 (3.37)  Time: 0.774s, 1323.38/s  (0.789s, 1298.36/s)  LR: 4.998e-04  Data: 0.010 (0.016)
Train: 302 [ 350/1251 ( 28%)]  Loss: 3.443 (3.38)  Time: 0.783s, 1307.58/s  (0.787s, 1300.91/s)  LR: 4.998e-04  Data: 0.009 (0.015)
Train: 302 [ 400/1251 ( 32%)]  Loss: 3.563 (3.40)  Time: 0.830s, 1233.34/s  (0.786s, 1302.78/s)  LR: 4.998e-04  Data: 0.011 (0.014)
Train: 302 [ 450/1251 ( 36%)]  Loss: 3.567 (3.42)  Time: 0.780s, 1313.52/s  (0.786s, 1303.54/s)  LR: 4.998e-04  Data: 0.010 (0.014)
Train: 302 [ 500/1251 ( 40%)]  Loss: 3.480 (3.42)  Time: 0.779s, 1315.30/s  (0.785s, 1303.98/s)  LR: 4.998e-04  Data: 0.010 (0.013)
Train: 302 [ 550/1251 ( 44%)]  Loss: 3.471 (3.43)  Time: 0.836s, 1225.34/s  (0.787s, 1300.87/s)  LR: 4.998e-04  Data: 0.009 (0.013)
Train: 302 [ 600/1251 ( 48%)]  Loss: 3.455 (3.43)  Time: 0.772s, 1325.84/s  (0.787s, 1301.51/s)  LR: 4.998e-04  Data: 0.010 (0.013)
Train: 302 [ 650/1251 ( 52%)]  Loss: 3.510 (3.43)  Time: 0.774s, 1323.57/s  (0.786s, 1302.57/s)  LR: 4.998e-04  Data: 0.009 (0.012)
Train: 302 [ 700/1251 ( 56%)]  Loss: 3.501 (3.44)  Time: 0.771s, 1328.03/s  (0.786s, 1303.60/s)  LR: 4.998e-04  Data: 0.009 (0.012)
Train: 302 [ 750/1251 ( 60%)]  Loss: 3.622 (3.45)  Time: 0.783s, 1308.00/s  (0.785s, 1303.65/s)  LR: 4.998e-04  Data: 0.010 (0.012)
Train: 302 [ 800/1251 ( 64%)]  Loss: 3.372 (3.45)  Time: 0.778s, 1316.20/s  (0.785s, 1304.56/s)  LR: 4.998e-04  Data: 0.009 (0.012)
Train: 302 [ 850/1251 ( 68%)]  Loss: 3.540 (3.45)  Time: 0.777s, 1317.24/s  (0.785s, 1305.02/s)  LR: 4.998e-04  Data: 0.010 (0.012)
Train: 302 [ 900/1251 ( 72%)]  Loss: 3.525 (3.45)  Time: 0.776s, 1319.98/s  (0.785s, 1305.26/s)  LR: 4.998e-04  Data: 0.009 (0.012)
Train: 302 [ 950/1251 ( 76%)]  Loss: 3.795 (3.47)  Time: 0.796s, 1286.86/s  (0.785s, 1305.04/s)  LR: 4.998e-04  Data: 0.011 (0.012)
Train: 302 [1000/1251 ( 80%)]  Loss: 3.391 (3.47)  Time: 0.771s, 1328.11/s  (0.785s, 1304.51/s)  LR: 4.998e-04  Data: 0.009 (0.011)
Train: 302 [1050/1251 ( 84%)]  Loss: 3.387 (3.46)  Time: 0.771s, 1327.86/s  (0.785s, 1305.10/s)  LR: 4.998e-04  Data: 0.010 (0.011)
Train: 302 [1100/1251 ( 88%)]  Loss: 3.340 (3.46)  Time: 0.776s, 1318.98/s  (0.784s, 1305.76/s)  LR: 4.998e-04  Data: 0.010 (0.011)
Train: 302 [1150/1251 ( 92%)]  Loss: 3.811 (3.47)  Time: 0.812s, 1260.79/s  (0.784s, 1306.23/s)  LR: 4.998e-04  Data: 0.009 (0.011)
Train: 302 [1200/1251 ( 96%)]  Loss: 3.376 (3.47)  Time: 0.789s, 1298.33/s  (0.784s, 1306.17/s)  LR: 4.998e-04  Data: 0.009 (0.011)
Train: 302 [1250/1251 (100%)]  Loss: 3.441 (3.47)  Time: 0.758s, 1350.45/s  (0.784s, 1306.10/s)  LR: 4.998e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.509 (1.509)  Loss:  0.7749 (0.7749)  Acc@1: 89.2578 (89.2578)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.194 (0.563)  Loss:  0.8877 (1.2927)  Acc@1: 84.9057 (75.5360)  Acc@5: 95.7547 (93.0060)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-300.pth.tar', 75.96000008544922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-296.pth.tar', 75.85800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-293.pth.tar', 75.69400010986328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-295.pth.tar', 75.63000000732421)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-297.pth.tar', 75.6259999633789)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-290.pth.tar', 75.57400003173828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-301.pth.tar', 75.55999995605468)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-302.pth.tar', 75.53600003662109)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-274.pth.tar', 75.46800011474609)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-286.pth.tar', 75.42600009033202)

Train: 303 [   0/1251 (  0%)]  Loss: 3.379 (3.38)  Time: 2.356s,  434.59/s  (2.356s,  434.59/s)  LR: 4.972e-04  Data: 1.622 (1.622)
Train: 303 [  50/1251 (  4%)]  Loss: 3.275 (3.33)  Time: 0.772s, 1325.64/s  (0.818s, 1252.01/s)  LR: 4.972e-04  Data: 0.010 (0.045)
Train: 303 [ 100/1251 (  8%)]  Loss: 3.407 (3.35)  Time: 0.810s, 1264.33/s  (0.801s, 1279.00/s)  LR: 4.972e-04  Data: 0.009 (0.027)
Train: 303 [ 150/1251 ( 12%)]  Loss: 3.627 (3.42)  Time: 0.778s, 1316.35/s  (0.795s, 1287.70/s)  LR: 4.972e-04  Data: 0.015 (0.022)
Train: 303 [ 200/1251 ( 16%)]  Loss: 3.209 (3.38)  Time: 0.786s, 1302.47/s  (0.791s, 1294.01/s)  LR: 4.972e-04  Data: 0.010 (0.019)
Train: 303 [ 250/1251 ( 20%)]  Loss: 3.247 (3.36)  Time: 0.772s, 1326.19/s  (0.791s, 1295.20/s)  LR: 4.972e-04  Data: 0.010 (0.017)
Train: 303 [ 300/1251 ( 24%)]  Loss: 3.401 (3.36)  Time: 0.773s, 1325.55/s  (0.789s, 1297.55/s)  LR: 4.972e-04  Data: 0.010 (0.016)
Train: 303 [ 350/1251 ( 28%)]  Loss: 3.807 (3.42)  Time: 0.771s, 1327.36/s  (0.790s, 1296.18/s)  LR: 4.972e-04  Data: 0.010 (0.015)
Train: 303 [ 400/1251 ( 32%)]  Loss: 3.502 (3.43)  Time: 0.773s, 1324.89/s  (0.790s, 1296.42/s)  LR: 4.972e-04  Data: 0.009 (0.014)
Train: 303 [ 450/1251 ( 36%)]  Loss: 3.438 (3.43)  Time: 0.782s, 1309.76/s  (0.791s, 1295.23/s)  LR: 4.972e-04  Data: 0.009 (0.014)
Train: 303 [ 500/1251 ( 40%)]  Loss: 3.665 (3.45)  Time: 0.818s, 1252.42/s  (0.790s, 1296.07/s)  LR: 4.972e-04  Data: 0.009 (0.013)
Train: 303 [ 550/1251 ( 44%)]  Loss: 3.614 (3.46)  Time: 0.776s, 1318.77/s  (0.790s, 1296.64/s)  LR: 4.972e-04  Data: 0.010 (0.013)
Train: 303 [ 600/1251 ( 48%)]  Loss: 3.112 (3.44)  Time: 0.772s, 1325.88/s  (0.789s, 1298.19/s)  LR: 4.972e-04  Data: 0.009 (0.013)
Train: 303 [ 650/1251 ( 52%)]  Loss: 2.952 (3.40)  Time: 0.772s, 1326.12/s  (0.788s, 1299.36/s)  LR: 4.972e-04  Data: 0.009 (0.013)
Train: 303 [ 700/1251 ( 56%)]  Loss: 3.500 (3.41)  Time: 0.783s, 1308.29/s  (0.788s, 1300.22/s)  LR: 4.972e-04  Data: 0.010 (0.012)
Train: 303 [ 750/1251 ( 60%)]  Loss: 2.960 (3.38)  Time: 0.782s, 1309.04/s  (0.787s, 1301.08/s)  LR: 4.972e-04  Data: 0.009 (0.012)
Train: 303 [ 800/1251 ( 64%)]  Loss: 3.669 (3.40)  Time: 0.773s, 1324.94/s  (0.787s, 1301.83/s)  LR: 4.972e-04  Data: 0.011 (0.012)
Train: 303 [ 850/1251 ( 68%)]  Loss: 3.711 (3.42)  Time: 0.815s, 1255.75/s  (0.787s, 1301.23/s)  LR: 4.972e-04  Data: 0.010 (0.012)
Train: 303 [ 900/1251 ( 72%)]  Loss: 3.386 (3.41)  Time: 0.772s, 1326.68/s  (0.787s, 1301.88/s)  LR: 4.972e-04  Data: 0.010 (0.012)
Train: 303 [ 950/1251 ( 76%)]  Loss: 3.492 (3.42)  Time: 0.810s, 1263.48/s  (0.786s, 1302.57/s)  LR: 4.972e-04  Data: 0.010 (0.012)
Train: 303 [1000/1251 ( 80%)]  Loss: 3.705 (3.43)  Time: 0.782s, 1309.70/s  (0.786s, 1303.34/s)  LR: 4.972e-04  Data: 0.010 (0.012)
Train: 303 [1050/1251 ( 84%)]  Loss: 3.628 (3.44)  Time: 0.773s, 1325.52/s  (0.786s, 1303.43/s)  LR: 4.972e-04  Data: 0.010 (0.012)
Train: 303 [1100/1251 ( 88%)]  Loss: 3.716 (3.45)  Time: 0.774s, 1323.26/s  (0.785s, 1303.88/s)  LR: 4.972e-04  Data: 0.011 (0.012)
Train: 303 [1150/1251 ( 92%)]  Loss: 3.582 (3.46)  Time: 0.786s, 1302.61/s  (0.785s, 1304.19/s)  LR: 4.972e-04  Data: 0.010 (0.011)
Train: 303 [1200/1251 ( 96%)]  Loss: 3.512 (3.46)  Time: 0.793s, 1291.24/s  (0.785s, 1304.76/s)  LR: 4.972e-04  Data: 0.010 (0.011)
Train: 303 [1250/1251 (100%)]  Loss: 3.427 (3.46)  Time: 0.761s, 1344.81/s  (0.784s, 1305.34/s)  LR: 4.972e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.493 (1.493)  Loss:  0.7710 (0.7710)  Acc@1: 88.6719 (88.6719)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.194 (0.567)  Loss:  0.8555 (1.3082)  Acc@1: 84.4340 (75.1940)  Acc@5: 95.9906 (92.8160)
Train: 304 [   0/1251 (  0%)]  Loss: 3.549 (3.55)  Time: 2.275s,  450.16/s  (2.275s,  450.16/s)  LR: 4.946e-04  Data: 1.540 (1.540)
Train: 304 [  50/1251 (  4%)]  Loss: 3.477 (3.51)  Time: 0.772s, 1326.11/s  (0.813s, 1259.46/s)  LR: 4.946e-04  Data: 0.010 (0.045)
Train: 304 [ 100/1251 (  8%)]  Loss: 3.257 (3.43)  Time: 0.843s, 1214.02/s  (0.800s, 1280.65/s)  LR: 4.946e-04  Data: 0.010 (0.028)
Train: 304 [ 150/1251 ( 12%)]  Loss: 3.400 (3.42)  Time: 0.775s, 1321.43/s  (0.795s, 1287.32/s)  LR: 4.946e-04  Data: 0.009 (0.022)
Train: 304 [ 200/1251 ( 16%)]  Loss: 3.382 (3.41)  Time: 0.782s, 1310.04/s  (0.792s, 1292.66/s)  LR: 4.946e-04  Data: 0.009 (0.019)
Train: 304 [ 250/1251 ( 20%)]  Loss: 3.419 (3.41)  Time: 0.774s, 1323.49/s  (0.789s, 1297.15/s)  LR: 4.946e-04  Data: 0.010 (0.017)
Train: 304 [ 300/1251 ( 24%)]  Loss: 3.275 (3.39)  Time: 0.783s, 1307.41/s  (0.787s, 1300.37/s)  LR: 4.946e-04  Data: 0.009 (0.016)
Train: 304 [ 350/1251 ( 28%)]  Loss: 3.374 (3.39)  Time: 0.777s, 1317.23/s  (0.786s, 1302.61/s)  LR: 4.946e-04  Data: 0.010 (0.015)
Train: 304 [ 400/1251 ( 32%)]  Loss: 3.001 (3.35)  Time: 0.780s, 1312.98/s  (0.785s, 1304.00/s)  LR: 4.946e-04  Data: 0.011 (0.014)
Train: 304 [ 450/1251 ( 36%)]  Loss: 3.726 (3.39)  Time: 0.773s, 1325.41/s  (0.784s, 1305.38/s)  LR: 4.946e-04  Data: 0.009 (0.014)
Train: 304 [ 500/1251 ( 40%)]  Loss: 3.618 (3.41)  Time: 0.847s, 1209.06/s  (0.785s, 1303.88/s)  LR: 4.946e-04  Data: 0.010 (0.013)
Train: 304 [ 550/1251 ( 44%)]  Loss: 3.481 (3.41)  Time: 0.808s, 1267.47/s  (0.786s, 1302.25/s)  LR: 4.946e-04  Data: 0.009 (0.013)
Train: 304 [ 600/1251 ( 48%)]  Loss: 3.693 (3.43)  Time: 0.780s, 1312.65/s  (0.787s, 1301.44/s)  LR: 4.946e-04  Data: 0.009 (0.013)
Train: 304 [ 650/1251 ( 52%)]  Loss: 3.542 (3.44)  Time: 0.772s, 1326.70/s  (0.786s, 1302.35/s)  LR: 4.946e-04  Data: 0.010 (0.013)
Train: 304 [ 700/1251 ( 56%)]  Loss: 3.297 (3.43)  Time: 0.773s, 1325.26/s  (0.786s, 1303.58/s)  LR: 4.946e-04  Data: 0.010 (0.012)
Train: 304 [ 750/1251 ( 60%)]  Loss: 3.467 (3.43)  Time: 0.835s, 1225.64/s  (0.786s, 1302.71/s)  LR: 4.946e-04  Data: 0.010 (0.012)
Train: 304 [ 800/1251 ( 64%)]  Loss: 3.357 (3.43)  Time: 0.775s, 1321.04/s  (0.786s, 1303.60/s)  LR: 4.946e-04  Data: 0.013 (0.012)
Train: 304 [ 850/1251 ( 68%)]  Loss: 3.241 (3.42)  Time: 0.776s, 1320.06/s  (0.785s, 1304.41/s)  LR: 4.946e-04  Data: 0.010 (0.012)
Train: 304 [ 900/1251 ( 72%)]  Loss: 3.342 (3.42)  Time: 0.775s, 1321.53/s  (0.785s, 1304.32/s)  LR: 4.946e-04  Data: 0.009 (0.012)
Train: 304 [ 950/1251 ( 76%)]  Loss: 3.481 (3.42)  Time: 0.774s, 1323.02/s  (0.785s, 1304.67/s)  LR: 4.946e-04  Data: 0.012 (0.012)
Train: 304 [1000/1251 ( 80%)]  Loss: 3.561 (3.43)  Time: 0.777s, 1318.01/s  (0.785s, 1303.85/s)  LR: 4.946e-04  Data: 0.010 (0.012)
Train: 304 [1050/1251 ( 84%)]  Loss: 3.326 (3.42)  Time: 0.778s, 1315.99/s  (0.785s, 1304.49/s)  LR: 4.946e-04  Data: 0.009 (0.012)
Train: 304 [1100/1251 ( 88%)]  Loss: 3.604 (3.43)  Time: 0.774s, 1322.43/s  (0.785s, 1304.30/s)  LR: 4.946e-04  Data: 0.009 (0.011)
Train: 304 [1150/1251 ( 92%)]  Loss: 3.678 (3.44)  Time: 0.774s, 1322.70/s  (0.785s, 1304.47/s)  LR: 4.946e-04  Data: 0.009 (0.011)
Train: 304 [1200/1251 ( 96%)]  Loss: 3.537 (3.44)  Time: 0.786s, 1302.26/s  (0.785s, 1304.73/s)  LR: 4.946e-04  Data: 0.010 (0.011)
Train: 304 [1250/1251 (100%)]  Loss: 3.405 (3.44)  Time: 0.758s, 1350.55/s  (0.785s, 1305.25/s)  LR: 4.946e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.587 (1.587)  Loss:  0.6396 (0.6396)  Acc@1: 88.7695 (88.7695)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.194 (0.556)  Loss:  0.7549 (1.2188)  Acc@1: 86.5566 (75.8420)  Acc@5: 96.8160 (93.0140)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-300.pth.tar', 75.96000008544922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-296.pth.tar', 75.85800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-304.pth.tar', 75.84200010742188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-293.pth.tar', 75.69400010986328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-295.pth.tar', 75.63000000732421)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-297.pth.tar', 75.6259999633789)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-290.pth.tar', 75.57400003173828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-301.pth.tar', 75.55999995605468)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-302.pth.tar', 75.53600003662109)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-274.pth.tar', 75.46800011474609)

Train: 305 [   0/1251 (  0%)]  Loss: 3.532 (3.53)  Time: 2.236s,  458.05/s  (2.236s,  458.05/s)  LR: 4.920e-04  Data: 1.512 (1.512)
Train: 305 [  50/1251 (  4%)]  Loss: 3.605 (3.57)  Time: 0.771s, 1327.68/s  (0.816s, 1254.85/s)  LR: 4.920e-04  Data: 0.009 (0.044)
Train: 305 [ 100/1251 (  8%)]  Loss: 3.268 (3.47)  Time: 0.837s, 1223.74/s  (0.798s, 1283.58/s)  LR: 4.920e-04  Data: 0.014 (0.027)
Train: 305 [ 150/1251 ( 12%)]  Loss: 3.525 (3.48)  Time: 0.825s, 1240.65/s  (0.792s, 1292.45/s)  LR: 4.920e-04  Data: 0.010 (0.021)
Train: 305 [ 200/1251 ( 16%)]  Loss: 3.709 (3.53)  Time: 0.772s, 1327.03/s  (0.791s, 1294.35/s)  LR: 4.920e-04  Data: 0.010 (0.019)
Train: 305 [ 250/1251 ( 20%)]  Loss: 3.488 (3.52)  Time: 0.781s, 1311.80/s  (0.789s, 1298.21/s)  LR: 4.920e-04  Data: 0.010 (0.017)
Train: 305 [ 300/1251 ( 24%)]  Loss: 3.700 (3.55)  Time: 0.866s, 1181.78/s  (0.788s, 1299.82/s)  LR: 4.920e-04  Data: 0.015 (0.016)
Train: 305 [ 350/1251 ( 28%)]  Loss: 3.405 (3.53)  Time: 0.772s, 1325.67/s  (0.786s, 1302.80/s)  LR: 4.920e-04  Data: 0.010 (0.015)
Train: 305 [ 400/1251 ( 32%)]  Loss: 3.487 (3.52)  Time: 0.783s, 1308.60/s  (0.785s, 1304.11/s)  LR: 4.920e-04  Data: 0.010 (0.014)
Train: 305 [ 450/1251 ( 36%)]  Loss: 3.555 (3.53)  Time: 0.772s, 1326.19/s  (0.786s, 1303.39/s)  LR: 4.920e-04  Data: 0.010 (0.014)
Train: 305 [ 500/1251 ( 40%)]  Loss: 3.498 (3.52)  Time: 0.772s, 1327.05/s  (0.785s, 1304.93/s)  LR: 4.920e-04  Data: 0.009 (0.013)
Train: 305 [ 550/1251 ( 44%)]  Loss: 3.586 (3.53)  Time: 0.772s, 1326.00/s  (0.785s, 1303.79/s)  LR: 4.920e-04  Data: 0.009 (0.013)
Train: 305 [ 600/1251 ( 48%)]  Loss: 3.564 (3.53)  Time: 0.882s, 1160.63/s  (0.786s, 1302.12/s)  LR: 4.920e-04  Data: 0.010 (0.013)
Train: 305 [ 650/1251 ( 52%)]  Loss: 3.680 (3.54)  Time: 0.777s, 1317.12/s  (0.786s, 1302.79/s)  LR: 4.920e-04  Data: 0.010 (0.012)
Train: 305 [ 700/1251 ( 56%)]  Loss: 2.888 (3.50)  Time: 0.809s, 1266.20/s  (0.787s, 1301.76/s)  LR: 4.920e-04  Data: 0.010 (0.012)
Train: 305 [ 750/1251 ( 60%)]  Loss: 3.165 (3.48)  Time: 0.772s, 1325.60/s  (0.787s, 1301.47/s)  LR: 4.920e-04  Data: 0.010 (0.012)
Train: 305 [ 800/1251 ( 64%)]  Loss: 3.196 (3.46)  Time: 0.787s, 1300.95/s  (0.786s, 1302.16/s)  LR: 4.920e-04  Data: 0.010 (0.012)
Train: 305 [ 850/1251 ( 68%)]  Loss: 3.504 (3.46)  Time: 0.783s, 1307.89/s  (0.787s, 1301.87/s)  LR: 4.920e-04  Data: 0.011 (0.012)
Train: 305 [ 900/1251 ( 72%)]  Loss: 3.460 (3.46)  Time: 0.774s, 1323.41/s  (0.786s, 1302.45/s)  LR: 4.920e-04  Data: 0.010 (0.012)
Train: 305 [ 950/1251 ( 76%)]  Loss: 3.404 (3.46)  Time: 0.773s, 1324.60/s  (0.786s, 1302.08/s)  LR: 4.920e-04  Data: 0.010 (0.012)
Train: 305 [1000/1251 ( 80%)]  Loss: 3.340 (3.46)  Time: 0.773s, 1325.48/s  (0.787s, 1301.59/s)  LR: 4.920e-04  Data: 0.009 (0.012)
Train: 305 [1050/1251 ( 84%)]  Loss: 3.323 (3.45)  Time: 0.776s, 1319.77/s  (0.787s, 1301.76/s)  LR: 4.920e-04  Data: 0.010 (0.012)
Train: 305 [1100/1251 ( 88%)]  Loss: 3.634 (3.46)  Time: 0.775s, 1322.08/s  (0.786s, 1302.38/s)  LR: 4.920e-04  Data: 0.009 (0.012)
Train: 305 [1150/1251 ( 92%)]  Loss: 3.607 (3.46)  Time: 0.774s, 1322.40/s  (0.786s, 1302.24/s)  LR: 4.920e-04  Data: 0.009 (0.011)
Train: 305 [1200/1251 ( 96%)]  Loss: 3.523 (3.47)  Time: 0.771s, 1327.68/s  (0.786s, 1302.87/s)  LR: 4.920e-04  Data: 0.009 (0.011)
Train: 305 [1250/1251 (100%)]  Loss: 3.391 (3.46)  Time: 0.760s, 1347.78/s  (0.786s, 1303.50/s)  LR: 4.920e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.575 (1.575)  Loss:  0.6694 (0.6694)  Acc@1: 90.5273 (90.5273)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.193 (0.564)  Loss:  0.7793 (1.2099)  Acc@1: 85.3774 (75.8100)  Acc@5: 95.9906 (93.0560)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-300.pth.tar', 75.96000008544922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-296.pth.tar', 75.85800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-304.pth.tar', 75.84200010742188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-305.pth.tar', 75.80999998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-293.pth.tar', 75.69400010986328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-295.pth.tar', 75.63000000732421)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-297.pth.tar', 75.6259999633789)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-290.pth.tar', 75.57400003173828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-301.pth.tar', 75.55999995605468)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-302.pth.tar', 75.53600003662109)

Train: 306 [   0/1251 (  0%)]  Loss: 3.434 (3.43)  Time: 2.292s,  446.72/s  (2.292s,  446.72/s)  LR: 4.895e-04  Data: 1.556 (1.556)
Train: 306 [  50/1251 (  4%)]  Loss: 3.445 (3.44)  Time: 0.773s, 1323.93/s  (0.814s, 1257.46/s)  LR: 4.895e-04  Data: 0.010 (0.046)
Train: 306 [ 100/1251 (  8%)]  Loss: 3.172 (3.35)  Time: 0.774s, 1323.77/s  (0.802s, 1276.37/s)  LR: 4.895e-04  Data: 0.009 (0.028)
Train: 306 [ 150/1251 ( 12%)]  Loss: 2.836 (3.22)  Time: 0.772s, 1326.59/s  (0.796s, 1287.13/s)  LR: 4.895e-04  Data: 0.010 (0.022)
Train: 306 [ 200/1251 ( 16%)]  Loss: 3.525 (3.28)  Time: 0.775s, 1320.98/s  (0.792s, 1292.36/s)  LR: 4.895e-04  Data: 0.009 (0.019)
Train: 306 [ 250/1251 ( 20%)]  Loss: 3.425 (3.31)  Time: 0.815s, 1256.11/s  (0.792s, 1292.79/s)  LR: 4.895e-04  Data: 0.011 (0.017)
Train: 306 [ 300/1251 ( 24%)]  Loss: 3.425 (3.32)  Time: 0.774s, 1323.67/s  (0.791s, 1295.33/s)  LR: 4.895e-04  Data: 0.010 (0.016)
Train: 306 [ 350/1251 ( 28%)]  Loss: 3.458 (3.34)  Time: 0.772s, 1325.57/s  (0.789s, 1297.83/s)  LR: 4.895e-04  Data: 0.010 (0.015)
Train: 306 [ 400/1251 ( 32%)]  Loss: 3.751 (3.39)  Time: 0.774s, 1323.70/s  (0.788s, 1299.16/s)  LR: 4.895e-04  Data: 0.009 (0.014)
Train: 306 [ 450/1251 ( 36%)]  Loss: 3.417 (3.39)  Time: 0.775s, 1321.37/s  (0.788s, 1299.94/s)  LR: 4.895e-04  Data: 0.010 (0.014)
Train: 306 [ 500/1251 ( 40%)]  Loss: 3.161 (3.37)  Time: 0.777s, 1318.10/s  (0.788s, 1299.84/s)  LR: 4.895e-04  Data: 0.013 (0.014)
Train: 306 [ 550/1251 ( 44%)]  Loss: 3.325 (3.36)  Time: 0.773s, 1324.52/s  (0.790s, 1296.60/s)  LR: 4.895e-04  Data: 0.009 (0.013)
Train: 306 [ 600/1251 ( 48%)]  Loss: 3.550 (3.38)  Time: 0.774s, 1323.11/s  (0.789s, 1298.13/s)  LR: 4.895e-04  Data: 0.009 (0.013)
Train: 306 [ 650/1251 ( 52%)]  Loss: 3.567 (3.39)  Time: 0.808s, 1267.01/s  (0.790s, 1295.65/s)  LR: 4.895e-04  Data: 0.009 (0.013)
Train: 306 [ 700/1251 ( 56%)]  Loss: 3.558 (3.40)  Time: 0.772s, 1327.13/s  (0.790s, 1296.03/s)  LR: 4.895e-04  Data: 0.009 (0.012)
Train: 306 [ 750/1251 ( 60%)]  Loss: 3.521 (3.41)  Time: 0.772s, 1326.17/s  (0.789s, 1297.21/s)  LR: 4.895e-04  Data: 0.010 (0.012)
Train: 306 [ 800/1251 ( 64%)]  Loss: 3.672 (3.43)  Time: 0.784s, 1306.34/s  (0.789s, 1298.11/s)  LR: 4.895e-04  Data: 0.011 (0.012)
Train: 306 [ 850/1251 ( 68%)]  Loss: 3.439 (3.43)  Time: 0.773s, 1324.65/s  (0.788s, 1299.19/s)  LR: 4.895e-04  Data: 0.010 (0.012)
Train: 306 [ 900/1251 ( 72%)]  Loss: 3.002 (3.40)  Time: 0.774s, 1322.45/s  (0.788s, 1300.25/s)  LR: 4.895e-04  Data: 0.010 (0.012)
Train: 306 [ 950/1251 ( 76%)]  Loss: 3.130 (3.39)  Time: 0.775s, 1320.72/s  (0.787s, 1300.73/s)  LR: 4.895e-04  Data: 0.009 (0.012)
Train: 306 [1000/1251 ( 80%)]  Loss: 3.474 (3.39)  Time: 0.773s, 1324.68/s  (0.787s, 1301.41/s)  LR: 4.895e-04  Data: 0.011 (0.012)
Train: 306 [1050/1251 ( 84%)]  Loss: 3.350 (3.39)  Time: 0.774s, 1323.79/s  (0.786s, 1302.17/s)  LR: 4.895e-04  Data: 0.010 (0.012)
Train: 306 [1100/1251 ( 88%)]  Loss: 3.419 (3.39)  Time: 0.773s, 1324.35/s  (0.786s, 1302.92/s)  LR: 4.895e-04  Data: 0.010 (0.011)
Train: 306 [1150/1251 ( 92%)]  Loss: 3.610 (3.40)  Time: 0.793s, 1291.94/s  (0.786s, 1303.22/s)  LR: 4.895e-04  Data: 0.010 (0.011)
Train: 306 [1200/1251 ( 96%)]  Loss: 3.553 (3.41)  Time: 0.774s, 1323.17/s  (0.786s, 1303.22/s)  LR: 4.895e-04  Data: 0.010 (0.011)
Train: 306 [1250/1251 (100%)]  Loss: 3.458 (3.41)  Time: 0.763s, 1341.68/s  (0.786s, 1303.36/s)  LR: 4.895e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.568 (1.568)  Loss:  0.6611 (0.6611)  Acc@1: 90.5273 (90.5273)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.194 (0.561)  Loss:  0.8188 (1.2473)  Acc@1: 85.9670 (75.6600)  Acc@5: 96.5802 (93.0600)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-300.pth.tar', 75.96000008544922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-296.pth.tar', 75.85800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-304.pth.tar', 75.84200010742188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-305.pth.tar', 75.80999998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-293.pth.tar', 75.69400010986328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-306.pth.tar', 75.65999998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-295.pth.tar', 75.63000000732421)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-297.pth.tar', 75.6259999633789)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-290.pth.tar', 75.57400003173828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-301.pth.tar', 75.55999995605468)

Train: 307 [   0/1251 (  0%)]  Loss: 3.503 (3.50)  Time: 2.298s,  445.53/s  (2.298s,  445.53/s)  LR: 4.869e-04  Data: 1.563 (1.563)
Train: 307 [  50/1251 (  4%)]  Loss: 3.428 (3.47)  Time: 0.784s, 1306.82/s  (0.815s, 1255.75/s)  LR: 4.869e-04  Data: 0.009 (0.043)
Train: 307 [ 100/1251 (  8%)]  Loss: 3.682 (3.54)  Time: 0.773s, 1325.10/s  (0.797s, 1284.40/s)  LR: 4.869e-04  Data: 0.010 (0.027)
Train: 307 [ 150/1251 ( 12%)]  Loss: 3.372 (3.50)  Time: 0.776s, 1320.18/s  (0.790s, 1295.77/s)  LR: 4.869e-04  Data: 0.010 (0.021)
Train: 307 [ 200/1251 ( 16%)]  Loss: 3.479 (3.49)  Time: 0.784s, 1306.50/s  (0.788s, 1300.01/s)  LR: 4.869e-04  Data: 0.009 (0.018)
Train: 307 [ 250/1251 ( 20%)]  Loss: 3.622 (3.51)  Time: 0.779s, 1314.86/s  (0.786s, 1302.65/s)  LR: 4.869e-04  Data: 0.010 (0.017)
Train: 307 [ 300/1251 ( 24%)]  Loss: 3.667 (3.54)  Time: 0.773s, 1324.57/s  (0.785s, 1304.91/s)  LR: 4.869e-04  Data: 0.010 (0.015)
Train: 307 [ 350/1251 ( 28%)]  Loss: 3.601 (3.54)  Time: 0.780s, 1313.36/s  (0.783s, 1307.26/s)  LR: 4.869e-04  Data: 0.010 (0.015)
Train: 307 [ 400/1251 ( 32%)]  Loss: 3.498 (3.54)  Time: 0.775s, 1321.46/s  (0.783s, 1307.86/s)  LR: 4.869e-04  Data: 0.009 (0.014)
Train: 307 [ 450/1251 ( 36%)]  Loss: 3.537 (3.54)  Time: 0.815s, 1257.19/s  (0.784s, 1306.33/s)  LR: 4.869e-04  Data: 0.011 (0.014)
Train: 307 [ 500/1251 ( 40%)]  Loss: 3.518 (3.54)  Time: 0.772s, 1325.80/s  (0.787s, 1301.06/s)  LR: 4.869e-04  Data: 0.009 (0.013)
Train: 307 [ 550/1251 ( 44%)]  Loss: 3.452 (3.53)  Time: 0.777s, 1317.41/s  (0.788s, 1299.83/s)  LR: 4.869e-04  Data: 0.010 (0.013)
Train: 307 [ 600/1251 ( 48%)]  Loss: 3.150 (3.50)  Time: 0.773s, 1325.35/s  (0.787s, 1301.01/s)  LR: 4.869e-04  Data: 0.009 (0.013)
Train: 307 [ 650/1251 ( 52%)]  Loss: 3.359 (3.49)  Time: 0.773s, 1324.09/s  (0.787s, 1301.76/s)  LR: 4.869e-04  Data: 0.010 (0.013)
Train: 307 [ 700/1251 ( 56%)]  Loss: 3.628 (3.50)  Time: 0.773s, 1325.53/s  (0.786s, 1303.04/s)  LR: 4.869e-04  Data: 0.009 (0.012)
Train: 307 [ 750/1251 ( 60%)]  Loss: 3.380 (3.49)  Time: 0.773s, 1324.92/s  (0.786s, 1302.24/s)  LR: 4.869e-04  Data: 0.009 (0.012)
Train: 307 [ 800/1251 ( 64%)]  Loss: 3.342 (3.48)  Time: 0.815s, 1256.64/s  (0.786s, 1302.91/s)  LR: 4.869e-04  Data: 0.011 (0.012)
Train: 307 [ 850/1251 ( 68%)]  Loss: 3.725 (3.50)  Time: 0.774s, 1322.29/s  (0.786s, 1303.34/s)  LR: 4.869e-04  Data: 0.009 (0.012)
Train: 307 [ 900/1251 ( 72%)]  Loss: 3.073 (3.47)  Time: 0.772s, 1325.69/s  (0.785s, 1303.68/s)  LR: 4.869e-04  Data: 0.010 (0.012)
Train: 307 [ 950/1251 ( 76%)]  Loss: 3.744 (3.49)  Time: 0.776s, 1319.39/s  (0.786s, 1303.62/s)  LR: 4.869e-04  Data: 0.010 (0.012)
Train: 307 [1000/1251 ( 80%)]  Loss: 3.301 (3.48)  Time: 0.777s, 1317.17/s  (0.785s, 1303.98/s)  LR: 4.869e-04  Data: 0.009 (0.012)
Train: 307 [1050/1251 ( 84%)]  Loss: 3.412 (3.48)  Time: 0.777s, 1317.77/s  (0.785s, 1303.65/s)  LR: 4.869e-04  Data: 0.010 (0.011)
Train: 307 [1100/1251 ( 88%)]  Loss: 3.653 (3.48)  Time: 0.779s, 1314.72/s  (0.785s, 1303.94/s)  LR: 4.869e-04  Data: 0.010 (0.011)
Train: 307 [1150/1251 ( 92%)]  Loss: 3.597 (3.49)  Time: 0.816s, 1254.34/s  (0.786s, 1303.25/s)  LR: 4.869e-04  Data: 0.009 (0.011)
Train: 307 [1200/1251 ( 96%)]  Loss: 3.732 (3.50)  Time: 0.786s, 1302.89/s  (0.786s, 1303.51/s)  LR: 4.869e-04  Data: 0.015 (0.011)
Train: 307 [1250/1251 (100%)]  Loss: 3.168 (3.49)  Time: 0.759s, 1349.35/s  (0.785s, 1303.91/s)  LR: 4.869e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.530 (1.530)  Loss:  0.6963 (0.6963)  Acc@1: 89.1602 (89.1602)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.194 (0.575)  Loss:  0.8345 (1.2245)  Acc@1: 84.1981 (75.6580)  Acc@5: 96.1085 (93.1620)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-300.pth.tar', 75.96000008544922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-296.pth.tar', 75.85800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-304.pth.tar', 75.84200010742188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-305.pth.tar', 75.80999998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-293.pth.tar', 75.69400010986328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-306.pth.tar', 75.65999998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-307.pth.tar', 75.65799998779296)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-295.pth.tar', 75.63000000732421)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-297.pth.tar', 75.6259999633789)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-290.pth.tar', 75.57400003173828)

Train: 308 [   0/1251 (  0%)]  Loss: 3.795 (3.80)  Time: 2.232s,  458.71/s  (2.232s,  458.71/s)  LR: 4.843e-04  Data: 1.499 (1.499)
Train: 308 [  50/1251 (  4%)]  Loss: 3.607 (3.70)  Time: 0.794s, 1290.47/s  (0.827s, 1238.48/s)  LR: 4.843e-04  Data: 0.009 (0.042)
Train: 308 [ 100/1251 (  8%)]  Loss: 3.035 (3.48)  Time: 0.773s, 1325.37/s  (0.804s, 1273.27/s)  LR: 4.843e-04  Data: 0.011 (0.026)
Train: 308 [ 150/1251 ( 12%)]  Loss: 3.707 (3.54)  Time: 0.775s, 1320.89/s  (0.795s, 1288.30/s)  LR: 4.843e-04  Data: 0.010 (0.021)
Train: 308 [ 200/1251 ( 16%)]  Loss: 3.245 (3.48)  Time: 0.774s, 1322.73/s  (0.792s, 1292.59/s)  LR: 4.843e-04  Data: 0.009 (0.018)
Train: 308 [ 250/1251 ( 20%)]  Loss: 3.352 (3.46)  Time: 0.774s, 1323.04/s  (0.789s, 1297.74/s)  LR: 4.843e-04  Data: 0.010 (0.016)
Train: 308 [ 300/1251 ( 24%)]  Loss: 3.285 (3.43)  Time: 0.786s, 1302.20/s  (0.787s, 1300.87/s)  LR: 4.843e-04  Data: 0.009 (0.015)
Train: 308 [ 350/1251 ( 28%)]  Loss: 3.347 (3.42)  Time: 0.774s, 1323.52/s  (0.786s, 1302.93/s)  LR: 4.843e-04  Data: 0.010 (0.014)
Train: 308 [ 400/1251 ( 32%)]  Loss: 3.324 (3.41)  Time: 0.783s, 1308.50/s  (0.786s, 1303.03/s)  LR: 4.843e-04  Data: 0.009 (0.014)
Train: 308 [ 450/1251 ( 36%)]  Loss: 3.587 (3.43)  Time: 0.775s, 1321.12/s  (0.786s, 1303.29/s)  LR: 4.843e-04  Data: 0.010 (0.013)
Train: 308 [ 500/1251 ( 40%)]  Loss: 3.700 (3.45)  Time: 0.817s, 1253.88/s  (0.785s, 1304.28/s)  LR: 4.843e-04  Data: 0.009 (0.013)
Train: 308 [ 550/1251 ( 44%)]  Loss: 3.619 (3.47)  Time: 0.773s, 1324.58/s  (0.785s, 1304.22/s)  LR: 4.843e-04  Data: 0.010 (0.013)
Train: 308 [ 600/1251 ( 48%)]  Loss: 3.590 (3.48)  Time: 0.778s, 1315.57/s  (0.785s, 1304.86/s)  LR: 4.843e-04  Data: 0.009 (0.013)
Train: 308 [ 650/1251 ( 52%)]  Loss: 3.302 (3.46)  Time: 0.773s, 1324.37/s  (0.784s, 1305.63/s)  LR: 4.843e-04  Data: 0.010 (0.012)
Train: 308 [ 700/1251 ( 56%)]  Loss: 3.590 (3.47)  Time: 0.772s, 1327.17/s  (0.784s, 1306.22/s)  LR: 4.843e-04  Data: 0.009 (0.012)
Train: 308 [ 750/1251 ( 60%)]  Loss: 3.590 (3.48)  Time: 0.778s, 1316.90/s  (0.786s, 1302.92/s)  LR: 4.843e-04  Data: 0.010 (0.012)
Train: 308 [ 800/1251 ( 64%)]  Loss: 3.594 (3.49)  Time: 0.770s, 1329.17/s  (0.786s, 1302.07/s)  LR: 4.843e-04  Data: 0.011 (0.012)
Train: 308 [ 850/1251 ( 68%)]  Loss: 3.523 (3.49)  Time: 0.774s, 1322.26/s  (0.786s, 1302.78/s)  LR: 4.843e-04  Data: 0.009 (0.012)
Train: 308 [ 900/1251 ( 72%)]  Loss: 3.640 (3.50)  Time: 0.787s, 1301.17/s  (0.786s, 1302.57/s)  LR: 4.843e-04  Data: 0.009 (0.012)
Train: 308 [ 950/1251 ( 76%)]  Loss: 3.518 (3.50)  Time: 0.774s, 1322.51/s  (0.786s, 1303.13/s)  LR: 4.843e-04  Data: 0.011 (0.012)
Train: 308 [1000/1251 ( 80%)]  Loss: 3.477 (3.50)  Time: 0.820s, 1249.49/s  (0.786s, 1303.43/s)  LR: 4.843e-04  Data: 0.013 (0.011)
Train: 308 [1050/1251 ( 84%)]  Loss: 3.653 (3.50)  Time: 0.776s, 1319.97/s  (0.785s, 1304.14/s)  LR: 4.843e-04  Data: 0.010 (0.011)
Train: 308 [1100/1251 ( 88%)]  Loss: 3.232 (3.49)  Time: 0.787s, 1300.61/s  (0.785s, 1304.79/s)  LR: 4.843e-04  Data: 0.013 (0.011)
Train: 308 [1150/1251 ( 92%)]  Loss: 3.506 (3.49)  Time: 0.775s, 1321.76/s  (0.785s, 1305.19/s)  LR: 4.843e-04  Data: 0.010 (0.011)
Train: 308 [1200/1251 ( 96%)]  Loss: 3.522 (3.49)  Time: 0.774s, 1323.18/s  (0.784s, 1305.44/s)  LR: 4.843e-04  Data: 0.009 (0.011)
Train: 308 [1250/1251 (100%)]  Loss: 3.668 (3.50)  Time: 0.809s, 1266.13/s  (0.784s, 1305.59/s)  LR: 4.843e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.568 (1.568)  Loss:  0.7954 (0.7954)  Acc@1: 89.6484 (89.6484)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.194 (0.568)  Loss:  0.8677 (1.2590)  Acc@1: 85.1415 (75.5620)  Acc@5: 96.3443 (93.2480)
Train: 309 [   0/1251 (  0%)]  Loss: 3.356 (3.36)  Time: 2.139s,  478.80/s  (2.139s,  478.80/s)  LR: 4.817e-04  Data: 1.409 (1.409)
Train: 309 [  50/1251 (  4%)]  Loss: 3.797 (3.58)  Time: 0.839s, 1220.18/s  (0.811s, 1263.25/s)  LR: 4.817e-04  Data: 0.010 (0.040)
Train: 309 [ 100/1251 (  8%)]  Loss: 3.545 (3.57)  Time: 0.818s, 1251.64/s  (0.796s, 1286.58/s)  LR: 4.817e-04  Data: 0.013 (0.025)
Train: 309 [ 150/1251 ( 12%)]  Loss: 3.513 (3.55)  Time: 0.788s, 1298.82/s  (0.796s, 1286.12/s)  LR: 4.817e-04  Data: 0.010 (0.020)
Train: 309 [ 200/1251 ( 16%)]  Loss: 3.445 (3.53)  Time: 0.784s, 1305.32/s  (0.798s, 1283.45/s)  LR: 4.817e-04  Data: 0.010 (0.018)
Train: 309 [ 250/1251 ( 20%)]  Loss: 3.432 (3.51)  Time: 0.776s, 1319.80/s  (0.794s, 1288.97/s)  LR: 4.817e-04  Data: 0.009 (0.016)
Train: 309 [ 300/1251 ( 24%)]  Loss: 3.691 (3.54)  Time: 0.774s, 1323.46/s  (0.793s, 1291.90/s)  LR: 4.817e-04  Data: 0.009 (0.015)
Train: 309 [ 350/1251 ( 28%)]  Loss: 3.418 (3.52)  Time: 0.773s, 1324.27/s  (0.791s, 1294.59/s)  LR: 4.817e-04  Data: 0.010 (0.014)
Train: 309 [ 400/1251 ( 32%)]  Loss: 3.184 (3.49)  Time: 0.772s, 1326.96/s  (0.790s, 1296.77/s)  LR: 4.817e-04  Data: 0.010 (0.014)
Train: 309 [ 450/1251 ( 36%)]  Loss: 3.648 (3.50)  Time: 0.773s, 1325.41/s  (0.788s, 1299.20/s)  LR: 4.817e-04  Data: 0.010 (0.013)
Train: 309 [ 500/1251 ( 40%)]  Loss: 3.286 (3.48)  Time: 0.774s, 1323.06/s  (0.787s, 1301.35/s)  LR: 4.817e-04  Data: 0.009 (0.013)
Train: 309 [ 550/1251 ( 44%)]  Loss: 3.625 (3.49)  Time: 0.781s, 1310.81/s  (0.786s, 1302.43/s)  LR: 4.817e-04  Data: 0.009 (0.013)
Train: 309 [ 600/1251 ( 48%)]  Loss: 3.229 (3.47)  Time: 0.787s, 1301.69/s  (0.786s, 1302.94/s)  LR: 4.817e-04  Data: 0.013 (0.012)
Train: 309 [ 650/1251 ( 52%)]  Loss: 3.776 (3.50)  Time: 0.774s, 1322.66/s  (0.786s, 1302.99/s)  LR: 4.817e-04  Data: 0.009 (0.012)
Train: 309 [ 700/1251 ( 56%)]  Loss: 3.484 (3.50)  Time: 0.812s, 1260.80/s  (0.786s, 1303.16/s)  LR: 4.817e-04  Data: 0.009 (0.012)
Train: 309 [ 750/1251 ( 60%)]  Loss: 3.136 (3.47)  Time: 0.770s, 1329.17/s  (0.786s, 1302.85/s)  LR: 4.817e-04  Data: 0.009 (0.012)
Train: 309 [ 800/1251 ( 64%)]  Loss: 3.633 (3.48)  Time: 0.773s, 1323.95/s  (0.786s, 1303.52/s)  LR: 4.817e-04  Data: 0.010 (0.012)
Train: 309 [ 850/1251 ( 68%)]  Loss: 3.698 (3.49)  Time: 0.825s, 1241.56/s  (0.785s, 1303.77/s)  LR: 4.817e-04  Data: 0.009 (0.012)
Train: 309 [ 900/1251 ( 72%)]  Loss: 3.733 (3.51)  Time: 0.778s, 1316.19/s  (0.785s, 1304.32/s)  LR: 4.817e-04  Data: 0.010 (0.012)
Train: 309 [ 950/1251 ( 76%)]  Loss: 3.434 (3.50)  Time: 0.772s, 1326.05/s  (0.785s, 1304.78/s)  LR: 4.817e-04  Data: 0.010 (0.011)
Train: 309 [1000/1251 ( 80%)]  Loss: 3.377 (3.50)  Time: 0.774s, 1323.61/s  (0.785s, 1305.11/s)  LR: 4.817e-04  Data: 0.010 (0.011)
Train: 309 [1050/1251 ( 84%)]  Loss: 3.493 (3.50)  Time: 0.773s, 1324.23/s  (0.785s, 1304.84/s)  LR: 4.817e-04  Data: 0.009 (0.011)
Train: 309 [1100/1251 ( 88%)]  Loss: 3.354 (3.49)  Time: 0.777s, 1317.61/s  (0.785s, 1305.25/s)  LR: 4.817e-04  Data: 0.010 (0.011)
Train: 309 [1150/1251 ( 92%)]  Loss: 3.206 (3.48)  Time: 0.778s, 1315.57/s  (0.785s, 1304.33/s)  LR: 4.817e-04  Data: 0.010 (0.011)
Train: 309 [1200/1251 ( 96%)]  Loss: 3.570 (3.48)  Time: 0.773s, 1325.29/s  (0.785s, 1304.62/s)  LR: 4.817e-04  Data: 0.009 (0.011)
Train: 309 [1250/1251 (100%)]  Loss: 3.665 (3.49)  Time: 0.760s, 1346.72/s  (0.785s, 1303.96/s)  LR: 4.817e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.573 (1.573)  Loss:  0.7300 (0.7300)  Acc@1: 89.1602 (89.1602)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.193 (0.568)  Loss:  0.8262 (1.2876)  Acc@1: 85.2594 (75.3960)  Acc@5: 96.2264 (93.0140)
Train: 310 [   0/1251 (  0%)]  Loss: 3.470 (3.47)  Time: 2.132s,  480.25/s  (2.132s,  480.25/s)  LR: 4.791e-04  Data: 1.406 (1.406)
Train: 310 [  50/1251 (  4%)]  Loss: 3.643 (3.56)  Time: 0.774s, 1322.94/s  (0.818s, 1251.11/s)  LR: 4.791e-04  Data: 0.010 (0.044)
Train: 310 [ 100/1251 (  8%)]  Loss: 3.102 (3.41)  Time: 0.786s, 1303.15/s  (0.801s, 1277.67/s)  LR: 4.791e-04  Data: 0.011 (0.027)
Train: 310 [ 150/1251 ( 12%)]  Loss: 3.352 (3.39)  Time: 0.774s, 1322.77/s  (0.794s, 1289.77/s)  LR: 4.791e-04  Data: 0.009 (0.021)
Train: 310 [ 200/1251 ( 16%)]  Loss: 3.422 (3.40)  Time: 0.775s, 1321.73/s  (0.792s, 1292.25/s)  LR: 4.791e-04  Data: 0.010 (0.019)
Train: 310 [ 250/1251 ( 20%)]  Loss: 3.611 (3.43)  Time: 0.774s, 1323.79/s  (0.789s, 1297.17/s)  LR: 4.791e-04  Data: 0.009 (0.017)
Train: 310 [ 300/1251 ( 24%)]  Loss: 3.440 (3.43)  Time: 0.773s, 1324.48/s  (0.787s, 1300.83/s)  LR: 4.791e-04  Data: 0.009 (0.016)
Train: 310 [ 350/1251 ( 28%)]  Loss: 3.653 (3.46)  Time: 0.773s, 1325.43/s  (0.786s, 1302.54/s)  LR: 4.791e-04  Data: 0.010 (0.015)
Train: 310 [ 400/1251 ( 32%)]  Loss: 3.461 (3.46)  Time: 0.773s, 1325.01/s  (0.785s, 1303.67/s)  LR: 4.791e-04  Data: 0.009 (0.014)
Train: 310 [ 450/1251 ( 36%)]  Loss: 3.602 (3.48)  Time: 0.773s, 1324.31/s  (0.785s, 1304.73/s)  LR: 4.791e-04  Data: 0.009 (0.014)
Train: 310 [ 500/1251 ( 40%)]  Loss: 3.728 (3.50)  Time: 0.776s, 1319.34/s  (0.784s, 1305.81/s)  LR: 4.791e-04  Data: 0.011 (0.013)
Train: 310 [ 550/1251 ( 44%)]  Loss: 3.773 (3.52)  Time: 0.777s, 1317.86/s  (0.784s, 1306.27/s)  LR: 4.791e-04  Data: 0.009 (0.013)
Train: 310 [ 600/1251 ( 48%)]  Loss: 3.287 (3.50)  Time: 0.822s, 1246.46/s  (0.786s, 1303.40/s)  LR: 4.791e-04  Data: 0.011 (0.013)
Train: 310 [ 650/1251 ( 52%)]  Loss: 3.613 (3.51)  Time: 0.780s, 1313.16/s  (0.785s, 1304.33/s)  LR: 4.791e-04  Data: 0.009 (0.013)
Train: 310 [ 700/1251 ( 56%)]  Loss: 3.225 (3.49)  Time: 0.773s, 1324.98/s  (0.785s, 1305.00/s)  LR: 4.791e-04  Data: 0.009 (0.012)
Train: 310 [ 750/1251 ( 60%)]  Loss: 3.373 (3.48)  Time: 0.783s, 1307.60/s  (0.784s, 1305.69/s)  LR: 4.791e-04  Data: 0.013 (0.012)
Train: 310 [ 800/1251 ( 64%)]  Loss: 3.483 (3.48)  Time: 0.777s, 1317.69/s  (0.784s, 1306.49/s)  LR: 4.791e-04  Data: 0.010 (0.012)
Train: 310 [ 850/1251 ( 68%)]  Loss: 3.021 (3.46)  Time: 0.771s, 1327.33/s  (0.784s, 1306.92/s)  LR: 4.791e-04  Data: 0.010 (0.012)
Train: 310 [ 900/1251 ( 72%)]  Loss: 3.874 (3.48)  Time: 0.772s, 1325.67/s  (0.783s, 1307.14/s)  LR: 4.791e-04  Data: 0.009 (0.012)
Train: 310 [ 950/1251 ( 76%)]  Loss: 3.592 (3.49)  Time: 0.778s, 1315.48/s  (0.783s, 1307.16/s)  LR: 4.791e-04  Data: 0.010 (0.012)
Train: 310 [1000/1251 ( 80%)]  Loss: 3.577 (3.49)  Time: 0.788s, 1298.86/s  (0.783s, 1307.51/s)  LR: 4.791e-04  Data: 0.011 (0.012)
Train: 310 [1050/1251 ( 84%)]  Loss: 3.276 (3.48)  Time: 0.773s, 1324.55/s  (0.783s, 1307.54/s)  LR: 4.791e-04  Data: 0.011 (0.012)
Train: 310 [1100/1251 ( 88%)]  Loss: 3.361 (3.48)  Time: 0.781s, 1310.42/s  (0.783s, 1307.54/s)  LR: 4.791e-04  Data: 0.010 (0.011)
Train: 310 [1150/1251 ( 92%)]  Loss: 3.179 (3.46)  Time: 0.775s, 1321.75/s  (0.784s, 1306.46/s)  LR: 4.791e-04  Data: 0.010 (0.011)
Train: 310 [1200/1251 ( 96%)]  Loss: 3.622 (3.47)  Time: 0.783s, 1307.13/s  (0.784s, 1306.29/s)  LR: 4.791e-04  Data: 0.009 (0.011)
Train: 310 [1250/1251 (100%)]  Loss: 3.436 (3.47)  Time: 0.759s, 1348.29/s  (0.785s, 1305.05/s)  LR: 4.791e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.594 (1.594)  Loss:  0.8569 (0.8569)  Acc@1: 90.6250 (90.6250)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.193 (0.598)  Loss:  0.9536 (1.3431)  Acc@1: 84.4340 (75.6480)  Acc@5: 96.5802 (93.0260)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-300.pth.tar', 75.96000008544922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-296.pth.tar', 75.85800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-304.pth.tar', 75.84200010742188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-305.pth.tar', 75.80999998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-293.pth.tar', 75.69400010986328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-306.pth.tar', 75.65999998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-307.pth.tar', 75.65799998779296)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-310.pth.tar', 75.64800009033203)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-295.pth.tar', 75.63000000732421)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-297.pth.tar', 75.6259999633789)

Train: 311 [   0/1251 (  0%)]  Loss: 3.421 (3.42)  Time: 2.196s,  466.21/s  (2.196s,  466.21/s)  LR: 4.765e-04  Data: 1.466 (1.466)
Train: 311 [  50/1251 (  4%)]  Loss: 3.644 (3.53)  Time: 0.775s, 1322.08/s  (0.816s, 1254.68/s)  LR: 4.765e-04  Data: 0.010 (0.044)
Train: 311 [ 100/1251 (  8%)]  Loss: 3.579 (3.55)  Time: 0.810s, 1263.86/s  (0.805s, 1272.68/s)  LR: 4.765e-04  Data: 0.010 (0.027)
Train: 311 [ 150/1251 ( 12%)]  Loss: 3.528 (3.54)  Time: 0.774s, 1322.41/s  (0.795s, 1287.79/s)  LR: 4.765e-04  Data: 0.010 (0.022)
Train: 311 [ 200/1251 ( 16%)]  Loss: 3.514 (3.54)  Time: 0.772s, 1326.53/s  (0.792s, 1293.26/s)  LR: 4.765e-04  Data: 0.010 (0.019)
Train: 311 [ 250/1251 ( 20%)]  Loss: 3.461 (3.52)  Time: 0.784s, 1305.39/s  (0.789s, 1297.52/s)  LR: 4.765e-04  Data: 0.010 (0.017)
Train: 311 [ 300/1251 ( 24%)]  Loss: 3.387 (3.50)  Time: 0.772s, 1326.16/s  (0.789s, 1297.44/s)  LR: 4.765e-04  Data: 0.009 (0.016)
Train: 311 [ 350/1251 ( 28%)]  Loss: 3.444 (3.50)  Time: 0.775s, 1320.86/s  (0.788s, 1298.93/s)  LR: 4.765e-04  Data: 0.009 (0.015)
Train: 311 [ 400/1251 ( 32%)]  Loss: 3.369 (3.48)  Time: 0.814s, 1257.34/s  (0.789s, 1298.63/s)  LR: 4.765e-04  Data: 0.010 (0.014)
Train: 311 [ 450/1251 ( 36%)]  Loss: 3.709 (3.51)  Time: 0.789s, 1297.96/s  (0.791s, 1294.89/s)  LR: 4.765e-04  Data: 0.010 (0.014)
Train: 311 [ 500/1251 ( 40%)]  Loss: 3.266 (3.48)  Time: 0.808s, 1267.73/s  (0.790s, 1296.12/s)  LR: 4.765e-04  Data: 0.009 (0.013)
Train: 311 [ 550/1251 ( 44%)]  Loss: 3.305 (3.47)  Time: 0.772s, 1327.06/s  (0.791s, 1294.24/s)  LR: 4.765e-04  Data: 0.010 (0.013)
Train: 311 [ 600/1251 ( 48%)]  Loss: 3.391 (3.46)  Time: 0.771s, 1327.83/s  (0.791s, 1295.37/s)  LR: 4.765e-04  Data: 0.010 (0.013)
Train: 311 [ 650/1251 ( 52%)]  Loss: 3.496 (3.47)  Time: 0.776s, 1320.18/s  (0.790s, 1296.31/s)  LR: 4.765e-04  Data: 0.009 (0.013)
Train: 311 [ 700/1251 ( 56%)]  Loss: 3.381 (3.46)  Time: 0.771s, 1328.53/s  (0.791s, 1295.26/s)  LR: 4.765e-04  Data: 0.010 (0.012)
Train: 311 [ 750/1251 ( 60%)]  Loss: 3.762 (3.48)  Time: 0.774s, 1323.75/s  (0.790s, 1296.53/s)  LR: 4.765e-04  Data: 0.009 (0.012)
Train: 311 [ 800/1251 ( 64%)]  Loss: 3.613 (3.49)  Time: 0.774s, 1322.45/s  (0.789s, 1297.11/s)  LR: 4.765e-04  Data: 0.010 (0.012)
Train: 311 [ 850/1251 ( 68%)]  Loss: 3.578 (3.49)  Time: 0.778s, 1316.27/s  (0.789s, 1298.19/s)  LR: 4.765e-04  Data: 0.009 (0.012)
Train: 311 [ 900/1251 ( 72%)]  Loss: 3.182 (3.48)  Time: 0.773s, 1324.83/s  (0.788s, 1299.48/s)  LR: 4.765e-04  Data: 0.010 (0.012)
Train: 311 [ 950/1251 ( 76%)]  Loss: 3.345 (3.47)  Time: 0.773s, 1324.15/s  (0.788s, 1299.42/s)  LR: 4.765e-04  Data: 0.009 (0.012)
Train: 311 [1000/1251 ( 80%)]  Loss: 3.356 (3.46)  Time: 0.785s, 1304.73/s  (0.788s, 1299.99/s)  LR: 4.765e-04  Data: 0.009 (0.012)
Train: 311 [1050/1251 ( 84%)]  Loss: 3.557 (3.47)  Time: 0.784s, 1305.72/s  (0.787s, 1300.46/s)  LR: 4.765e-04  Data: 0.009 (0.012)
Train: 311 [1100/1251 ( 88%)]  Loss: 3.191 (3.46)  Time: 0.773s, 1323.86/s  (0.787s, 1301.23/s)  LR: 4.765e-04  Data: 0.010 (0.011)
Train: 311 [1150/1251 ( 92%)]  Loss: 3.485 (3.46)  Time: 0.773s, 1324.57/s  (0.787s, 1301.95/s)  LR: 4.765e-04  Data: 0.010 (0.011)
Train: 311 [1200/1251 ( 96%)]  Loss: 3.648 (3.46)  Time: 0.807s, 1268.93/s  (0.786s, 1302.10/s)  LR: 4.765e-04  Data: 0.009 (0.011)
Train: 311 [1250/1251 (100%)]  Loss: 3.159 (3.45)  Time: 0.809s, 1265.37/s  (0.786s, 1302.08/s)  LR: 4.765e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.579 (1.579)  Loss:  0.7358 (0.7358)  Acc@1: 89.5508 (89.5508)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.194 (0.563)  Loss:  0.7847 (1.2449)  Acc@1: 85.0236 (75.6280)  Acc@5: 97.1698 (93.1700)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-300.pth.tar', 75.96000008544922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-296.pth.tar', 75.85800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-304.pth.tar', 75.84200010742188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-305.pth.tar', 75.80999998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-293.pth.tar', 75.69400010986328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-306.pth.tar', 75.65999998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-307.pth.tar', 75.65799998779296)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-310.pth.tar', 75.64800009033203)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-295.pth.tar', 75.63000000732421)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-311.pth.tar', 75.62800008789063)

Train: 312 [   0/1251 (  0%)]  Loss: 3.147 (3.15)  Time: 2.361s,  433.63/s  (2.361s,  433.63/s)  LR: 4.739e-04  Data: 1.630 (1.630)
Train: 312 [  50/1251 (  4%)]  Loss: 3.668 (3.41)  Time: 0.773s, 1324.63/s  (0.823s, 1243.61/s)  LR: 4.739e-04  Data: 0.009 (0.048)
Train: 312 [ 100/1251 (  8%)]  Loss: 3.631 (3.48)  Time: 0.788s, 1298.78/s  (0.815s, 1256.05/s)  LR: 4.739e-04  Data: 0.009 (0.029)
Train: 312 [ 150/1251 ( 12%)]  Loss: 3.632 (3.52)  Time: 0.786s, 1303.31/s  (0.807s, 1268.50/s)  LR: 4.739e-04  Data: 0.009 (0.023)
Train: 312 [ 200/1251 ( 16%)]  Loss: 3.440 (3.50)  Time: 0.779s, 1314.62/s  (0.800s, 1279.30/s)  LR: 4.739e-04  Data: 0.010 (0.020)
Train: 312 [ 250/1251 ( 20%)]  Loss: 2.996 (3.42)  Time: 0.771s, 1327.29/s  (0.796s, 1286.61/s)  LR: 4.739e-04  Data: 0.009 (0.018)
Train: 312 [ 300/1251 ( 24%)]  Loss: 3.464 (3.43)  Time: 0.826s, 1240.35/s  (0.799s, 1281.82/s)  LR: 4.739e-04  Data: 0.013 (0.017)
Train: 312 [ 350/1251 ( 28%)]  Loss: 3.566 (3.44)  Time: 0.774s, 1323.33/s  (0.798s, 1283.93/s)  LR: 4.739e-04  Data: 0.010 (0.016)
Train: 312 [ 400/1251 ( 32%)]  Loss: 3.211 (3.42)  Time: 0.773s, 1325.38/s  (0.796s, 1287.14/s)  LR: 4.739e-04  Data: 0.010 (0.015)
Train: 312 [ 450/1251 ( 36%)]  Loss: 3.746 (3.45)  Time: 0.771s, 1328.18/s  (0.794s, 1289.22/s)  LR: 4.739e-04  Data: 0.010 (0.014)
Train: 312 [ 500/1251 ( 40%)]  Loss: 3.025 (3.41)  Time: 0.772s, 1326.20/s  (0.793s, 1291.45/s)  LR: 4.739e-04  Data: 0.010 (0.014)
Train: 312 [ 550/1251 ( 44%)]  Loss: 3.327 (3.40)  Time: 0.773s, 1325.05/s  (0.791s, 1293.95/s)  LR: 4.739e-04  Data: 0.010 (0.014)
Train: 312 [ 600/1251 ( 48%)]  Loss: 3.667 (3.42)  Time: 0.776s, 1320.22/s  (0.792s, 1293.53/s)  LR: 4.739e-04  Data: 0.009 (0.013)
Train: 312 [ 650/1251 ( 52%)]  Loss: 3.361 (3.42)  Time: 0.782s, 1309.09/s  (0.790s, 1295.39/s)  LR: 4.739e-04  Data: 0.009 (0.013)
Train: 312 [ 700/1251 ( 56%)]  Loss: 3.502 (3.43)  Time: 0.778s, 1315.54/s  (0.789s, 1297.03/s)  LR: 4.739e-04  Data: 0.009 (0.013)
Train: 312 [ 750/1251 ( 60%)]  Loss: 3.488 (3.43)  Time: 0.780s, 1312.27/s  (0.790s, 1296.31/s)  LR: 4.739e-04  Data: 0.010 (0.013)
Train: 312 [ 800/1251 ( 64%)]  Loss: 3.224 (3.42)  Time: 0.773s, 1324.40/s  (0.789s, 1297.28/s)  LR: 4.739e-04  Data: 0.010 (0.012)
Train: 312 [ 850/1251 ( 68%)]  Loss: 3.655 (3.43)  Time: 0.778s, 1316.01/s  (0.789s, 1298.21/s)  LR: 4.739e-04  Data: 0.010 (0.012)
Train: 312 [ 900/1251 ( 72%)]  Loss: 3.750 (3.45)  Time: 0.774s, 1322.91/s  (0.788s, 1299.29/s)  LR: 4.739e-04  Data: 0.009 (0.012)
Train: 312 [ 950/1251 ( 76%)]  Loss: 3.217 (3.44)  Time: 0.773s, 1324.07/s  (0.788s, 1300.03/s)  LR: 4.739e-04  Data: 0.010 (0.012)
Train: 312 [1000/1251 ( 80%)]  Loss: 3.329 (3.43)  Time: 0.773s, 1325.10/s  (0.787s, 1300.84/s)  LR: 4.739e-04  Data: 0.009 (0.012)
Train: 312 [1050/1251 ( 84%)]  Loss: 3.584 (3.44)  Time: 0.774s, 1323.56/s  (0.787s, 1301.61/s)  LR: 4.739e-04  Data: 0.010 (0.012)
Train: 312 [1100/1251 ( 88%)]  Loss: 3.100 (3.42)  Time: 0.815s, 1256.88/s  (0.787s, 1301.24/s)  LR: 4.739e-04  Data: 0.011 (0.012)
Train: 312 [1150/1251 ( 92%)]  Loss: 3.604 (3.43)  Time: 0.835s, 1225.63/s  (0.787s, 1301.42/s)  LR: 4.739e-04  Data: 0.010 (0.012)
Train: 312 [1200/1251 ( 96%)]  Loss: 3.297 (3.43)  Time: 0.807s, 1268.46/s  (0.788s, 1299.38/s)  LR: 4.739e-04  Data: 0.010 (0.011)
Train: 312 [1250/1251 (100%)]  Loss: 3.266 (3.42)  Time: 0.759s, 1349.86/s  (0.788s, 1299.03/s)  LR: 4.739e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.527 (1.527)  Loss:  0.7896 (0.7896)  Acc@1: 89.5508 (89.5508)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.194 (0.561)  Loss:  0.8853 (1.3551)  Acc@1: 84.6698 (75.3620)  Acc@5: 96.8160 (93.0580)
Train: 313 [   0/1251 (  0%)]  Loss: 3.563 (3.56)  Time: 2.282s,  448.69/s  (2.282s,  448.69/s)  LR: 4.713e-04  Data: 1.521 (1.521)
Train: 313 [  50/1251 (  4%)]  Loss: 3.490 (3.53)  Time: 0.781s, 1311.61/s  (0.814s, 1258.48/s)  LR: 4.713e-04  Data: 0.010 (0.043)
Train: 313 [ 100/1251 (  8%)]  Loss: 3.347 (3.47)  Time: 0.780s, 1312.70/s  (0.795s, 1287.92/s)  LR: 4.713e-04  Data: 0.010 (0.026)
Train: 313 [ 150/1251 ( 12%)]  Loss: 3.338 (3.43)  Time: 0.773s, 1325.33/s  (0.791s, 1294.53/s)  LR: 4.713e-04  Data: 0.009 (0.021)
Train: 313 [ 200/1251 ( 16%)]  Loss: 3.190 (3.39)  Time: 0.784s, 1306.92/s  (0.790s, 1296.27/s)  LR: 4.713e-04  Data: 0.010 (0.018)
Train: 313 [ 250/1251 ( 20%)]  Loss: 3.702 (3.44)  Time: 0.768s, 1332.58/s  (0.788s, 1300.09/s)  LR: 4.713e-04  Data: 0.010 (0.016)
Train: 313 [ 300/1251 ( 24%)]  Loss: 3.220 (3.41)  Time: 0.778s, 1316.87/s  (0.791s, 1295.04/s)  LR: 4.713e-04  Data: 0.011 (0.015)
Train: 313 [ 350/1251 ( 28%)]  Loss: 3.209 (3.38)  Time: 0.819s, 1250.87/s  (0.790s, 1296.16/s)  LR: 4.713e-04  Data: 0.010 (0.015)
Train: 313 [ 400/1251 ( 32%)]  Loss: 3.230 (3.37)  Time: 0.811s, 1262.82/s  (0.790s, 1295.88/s)  LR: 4.713e-04  Data: 0.010 (0.014)
Train: 313 [ 450/1251 ( 36%)]  Loss: 3.376 (3.37)  Time: 0.776s, 1319.36/s  (0.791s, 1295.30/s)  LR: 4.713e-04  Data: 0.010 (0.014)
Train: 313 [ 500/1251 ( 40%)]  Loss: 3.531 (3.38)  Time: 0.815s, 1256.01/s  (0.790s, 1296.25/s)  LR: 4.713e-04  Data: 0.011 (0.013)
Train: 313 [ 550/1251 ( 44%)]  Loss: 3.311 (3.38)  Time: 0.778s, 1316.54/s  (0.789s, 1297.36/s)  LR: 4.713e-04  Data: 0.009 (0.013)
Train: 313 [ 600/1251 ( 48%)]  Loss: 3.548 (3.39)  Time: 0.866s, 1182.69/s  (0.788s, 1298.73/s)  LR: 4.713e-04  Data: 0.010 (0.013)
Train: 313 [ 650/1251 ( 52%)]  Loss: 3.645 (3.41)  Time: 0.772s, 1326.05/s  (0.788s, 1300.01/s)  LR: 4.713e-04  Data: 0.009 (0.012)
Train: 313 [ 700/1251 ( 56%)]  Loss: 3.274 (3.40)  Time: 0.789s, 1297.31/s  (0.787s, 1300.43/s)  LR: 4.713e-04  Data: 0.010 (0.012)
Train: 313 [ 750/1251 ( 60%)]  Loss: 3.337 (3.39)  Time: 0.785s, 1304.78/s  (0.787s, 1301.41/s)  LR: 4.713e-04  Data: 0.009 (0.012)
Train: 313 [ 800/1251 ( 64%)]  Loss: 3.731 (3.41)  Time: 0.774s, 1323.43/s  (0.786s, 1302.23/s)  LR: 4.713e-04  Data: 0.010 (0.012)
Train: 313 [ 850/1251 ( 68%)]  Loss: 3.292 (3.41)  Time: 0.777s, 1318.11/s  (0.786s, 1302.85/s)  LR: 4.713e-04  Data: 0.009 (0.012)
Train: 313 [ 900/1251 ( 72%)]  Loss: 3.449 (3.41)  Time: 0.851s, 1203.89/s  (0.786s, 1303.15/s)  LR: 4.713e-04  Data: 0.010 (0.012)
Train: 313 [ 950/1251 ( 76%)]  Loss: 3.492 (3.41)  Time: 0.775s, 1321.53/s  (0.786s, 1303.51/s)  LR: 4.713e-04  Data: 0.009 (0.012)
Train: 313 [1000/1251 ( 80%)]  Loss: 3.336 (3.41)  Time: 0.821s, 1246.92/s  (0.785s, 1303.72/s)  LR: 4.713e-04  Data: 0.010 (0.011)
Train: 313 [1050/1251 ( 84%)]  Loss: 3.735 (3.42)  Time: 0.784s, 1305.33/s  (0.785s, 1304.18/s)  LR: 4.713e-04  Data: 0.010 (0.011)
Train: 313 [1100/1251 ( 88%)]  Loss: 3.460 (3.43)  Time: 0.786s, 1303.57/s  (0.785s, 1304.64/s)  LR: 4.713e-04  Data: 0.009 (0.011)
Train: 313 [1150/1251 ( 92%)]  Loss: 3.278 (3.42)  Time: 0.772s, 1325.94/s  (0.785s, 1305.06/s)  LR: 4.713e-04  Data: 0.009 (0.011)
Train: 313 [1200/1251 ( 96%)]  Loss: 3.330 (3.42)  Time: 0.777s, 1317.76/s  (0.785s, 1305.21/s)  LR: 4.713e-04  Data: 0.010 (0.011)
Train: 313 [1250/1251 (100%)]  Loss: 3.436 (3.42)  Time: 0.760s, 1348.14/s  (0.784s, 1305.62/s)  LR: 4.713e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.580 (1.580)  Loss:  0.6777 (0.6777)  Acc@1: 89.6484 (89.6484)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.194 (0.564)  Loss:  0.7949 (1.2505)  Acc@1: 84.1981 (75.7520)  Acc@5: 96.2264 (93.0420)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-300.pth.tar', 75.96000008544922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-296.pth.tar', 75.85800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-304.pth.tar', 75.84200010742188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-305.pth.tar', 75.80999998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-313.pth.tar', 75.75199998779297)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-293.pth.tar', 75.69400010986328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-306.pth.tar', 75.65999998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-307.pth.tar', 75.65799998779296)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-310.pth.tar', 75.64800009033203)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-295.pth.tar', 75.63000000732421)

Train: 314 [   0/1251 (  0%)]  Loss: 3.332 (3.33)  Time: 2.311s,  443.03/s  (2.311s,  443.03/s)  LR: 4.687e-04  Data: 1.582 (1.582)
Train: 314 [  50/1251 (  4%)]  Loss: 3.509 (3.42)  Time: 0.783s, 1308.07/s  (0.812s, 1260.77/s)  LR: 4.687e-04  Data: 0.009 (0.044)
Train: 314 [ 100/1251 (  8%)]  Loss: 3.618 (3.49)  Time: 0.785s, 1305.15/s  (0.798s, 1282.72/s)  LR: 4.687e-04  Data: 0.010 (0.027)
Train: 314 [ 150/1251 ( 12%)]  Loss: 3.629 (3.52)  Time: 0.773s, 1324.01/s  (0.794s, 1290.34/s)  LR: 4.687e-04  Data: 0.009 (0.022)
Train: 314 [ 200/1251 ( 16%)]  Loss: 3.765 (3.57)  Time: 0.783s, 1308.43/s  (0.790s, 1296.12/s)  LR: 4.687e-04  Data: 0.012 (0.019)
Train: 314 [ 250/1251 ( 20%)]  Loss: 3.374 (3.54)  Time: 0.773s, 1324.48/s  (0.787s, 1300.86/s)  LR: 4.687e-04  Data: 0.010 (0.017)
Train: 314 [ 300/1251 ( 24%)]  Loss: 3.253 (3.50)  Time: 0.772s, 1326.09/s  (0.785s, 1304.63/s)  LR: 4.687e-04  Data: 0.010 (0.016)
Train: 314 [ 350/1251 ( 28%)]  Loss: 3.275 (3.47)  Time: 0.772s, 1327.22/s  (0.784s, 1306.45/s)  LR: 4.687e-04  Data: 0.009 (0.015)
Train: 314 [ 400/1251 ( 32%)]  Loss: 3.739 (3.50)  Time: 0.782s, 1309.11/s  (0.783s, 1306.98/s)  LR: 4.687e-04  Data: 0.010 (0.014)
Train: 314 [ 450/1251 ( 36%)]  Loss: 3.247 (3.47)  Time: 0.782s, 1308.71/s  (0.784s, 1305.94/s)  LR: 4.687e-04  Data: 0.010 (0.014)
Train: 314 [ 500/1251 ( 40%)]  Loss: 3.486 (3.48)  Time: 0.774s, 1322.36/s  (0.784s, 1306.22/s)  LR: 4.687e-04  Data: 0.009 (0.013)
Train: 314 [ 550/1251 ( 44%)]  Loss: 3.391 (3.47)  Time: 0.772s, 1326.12/s  (0.784s, 1305.81/s)  LR: 4.687e-04  Data: 0.010 (0.013)
Train: 314 [ 600/1251 ( 48%)]  Loss: 3.552 (3.47)  Time: 0.772s, 1325.71/s  (0.784s, 1305.41/s)  LR: 4.687e-04  Data: 0.010 (0.013)
Train: 314 [ 650/1251 ( 52%)]  Loss: 3.448 (3.47)  Time: 0.772s, 1325.66/s  (0.784s, 1306.48/s)  LR: 4.687e-04  Data: 0.010 (0.013)
Train: 314 [ 700/1251 ( 56%)]  Loss: 3.420 (3.47)  Time: 0.784s, 1305.70/s  (0.783s, 1307.17/s)  LR: 4.687e-04  Data: 0.009 (0.012)
Train: 314 [ 750/1251 ( 60%)]  Loss: 3.403 (3.47)  Time: 0.774s, 1323.74/s  (0.783s, 1307.06/s)  LR: 4.687e-04  Data: 0.009 (0.012)
Train: 314 [ 800/1251 ( 64%)]  Loss: 3.549 (3.47)  Time: 0.779s, 1314.96/s  (0.783s, 1307.18/s)  LR: 4.687e-04  Data: 0.011 (0.012)
Train: 314 [ 850/1251 ( 68%)]  Loss: 3.554 (3.47)  Time: 0.775s, 1320.93/s  (0.783s, 1307.59/s)  LR: 4.687e-04  Data: 0.009 (0.012)
Train: 314 [ 900/1251 ( 72%)]  Loss: 3.670 (3.48)  Time: 0.773s, 1324.60/s  (0.784s, 1306.01/s)  LR: 4.687e-04  Data: 0.009 (0.012)
Train: 314 [ 950/1251 ( 76%)]  Loss: 3.258 (3.47)  Time: 0.771s, 1328.16/s  (0.784s, 1306.66/s)  LR: 4.687e-04  Data: 0.009 (0.012)
Train: 314 [1000/1251 ( 80%)]  Loss: 3.332 (3.47)  Time: 0.788s, 1299.56/s  (0.783s, 1307.20/s)  LR: 4.687e-04  Data: 0.010 (0.012)
Train: 314 [1050/1251 ( 84%)]  Loss: 3.139 (3.45)  Time: 0.773s, 1325.39/s  (0.783s, 1307.46/s)  LR: 4.687e-04  Data: 0.009 (0.011)
Train: 314 [1100/1251 ( 88%)]  Loss: 3.399 (3.45)  Time: 0.777s, 1317.32/s  (0.783s, 1307.44/s)  LR: 4.687e-04  Data: 0.010 (0.011)
Train: 314 [1150/1251 ( 92%)]  Loss: 3.499 (3.45)  Time: 0.772s, 1325.67/s  (0.783s, 1307.39/s)  LR: 4.687e-04  Data: 0.009 (0.011)
Train: 314 [1200/1251 ( 96%)]  Loss: 3.424 (3.45)  Time: 0.776s, 1319.29/s  (0.783s, 1307.47/s)  LR: 4.687e-04  Data: 0.010 (0.011)
Train: 314 [1250/1251 (100%)]  Loss: 3.533 (3.45)  Time: 0.763s, 1341.91/s  (0.784s, 1306.48/s)  LR: 4.687e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.577 (1.577)  Loss:  0.7749 (0.7749)  Acc@1: 89.4531 (89.4531)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.194 (0.563)  Loss:  0.7373 (1.2164)  Acc@1: 85.2594 (75.7040)  Acc@5: 96.6981 (93.2000)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-300.pth.tar', 75.96000008544922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-296.pth.tar', 75.85800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-304.pth.tar', 75.84200010742188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-305.pth.tar', 75.80999998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-313.pth.tar', 75.75199998779297)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-314.pth.tar', 75.70400006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-293.pth.tar', 75.69400010986328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-306.pth.tar', 75.65999998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-307.pth.tar', 75.65799998779296)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-310.pth.tar', 75.64800009033203)

Train: 315 [   0/1251 (  0%)]  Loss: 3.255 (3.25)  Time: 2.349s,  435.96/s  (2.349s,  435.96/s)  LR: 4.662e-04  Data: 1.623 (1.623)
Train: 315 [  50/1251 (  4%)]  Loss: 3.938 (3.60)  Time: 0.773s, 1325.40/s  (0.815s, 1257.05/s)  LR: 4.662e-04  Data: 0.010 (0.046)
Train: 315 [ 100/1251 (  8%)]  Loss: 3.604 (3.60)  Time: 0.794s, 1289.89/s  (0.798s, 1283.62/s)  LR: 4.662e-04  Data: 0.009 (0.028)
Train: 315 [ 150/1251 ( 12%)]  Loss: 3.356 (3.54)  Time: 0.774s, 1323.40/s  (0.793s, 1291.09/s)  LR: 4.662e-04  Data: 0.010 (0.022)
Train: 315 [ 200/1251 ( 16%)]  Loss: 3.631 (3.56)  Time: 0.780s, 1312.46/s  (0.792s, 1293.07/s)  LR: 4.662e-04  Data: 0.010 (0.019)
Train: 315 [ 250/1251 ( 20%)]  Loss: 3.676 (3.58)  Time: 0.776s, 1319.82/s  (0.790s, 1296.79/s)  LR: 4.662e-04  Data: 0.011 (0.017)
Train: 315 [ 300/1251 ( 24%)]  Loss: 3.319 (3.54)  Time: 0.772s, 1326.04/s  (0.789s, 1297.16/s)  LR: 4.662e-04  Data: 0.010 (0.016)
Train: 315 [ 350/1251 ( 28%)]  Loss: 3.445 (3.53)  Time: 0.774s, 1322.90/s  (0.788s, 1299.31/s)  LR: 4.662e-04  Data: 0.010 (0.015)
Train: 315 [ 400/1251 ( 32%)]  Loss: 3.444 (3.52)  Time: 0.783s, 1307.77/s  (0.787s, 1300.90/s)  LR: 4.662e-04  Data: 0.009 (0.014)
Train: 315 [ 450/1251 ( 36%)]  Loss: 3.626 (3.53)  Time: 0.816s, 1255.35/s  (0.786s, 1302.85/s)  LR: 4.662e-04  Data: 0.010 (0.014)
Train: 315 [ 500/1251 ( 40%)]  Loss: 3.503 (3.53)  Time: 0.773s, 1323.96/s  (0.785s, 1303.82/s)  LR: 4.662e-04  Data: 0.011 (0.014)
Train: 315 [ 550/1251 ( 44%)]  Loss: 3.415 (3.52)  Time: 0.774s, 1323.11/s  (0.785s, 1304.03/s)  LR: 4.662e-04  Data: 0.010 (0.013)
Train: 315 [ 600/1251 ( 48%)]  Loss: 3.221 (3.49)  Time: 0.788s, 1299.61/s  (0.785s, 1304.28/s)  LR: 4.662e-04  Data: 0.010 (0.013)
Train: 315 [ 650/1251 ( 52%)]  Loss: 3.497 (3.50)  Time: 0.773s, 1325.42/s  (0.785s, 1304.33/s)  LR: 4.662e-04  Data: 0.010 (0.013)
Train: 315 [ 700/1251 ( 56%)]  Loss: 3.149 (3.47)  Time: 0.773s, 1324.21/s  (0.787s, 1301.49/s)  LR: 4.662e-04  Data: 0.010 (0.013)
Train: 315 [ 750/1251 ( 60%)]  Loss: 3.589 (3.48)  Time: 0.776s, 1319.26/s  (0.786s, 1302.69/s)  LR: 4.662e-04  Data: 0.010 (0.012)
Train: 315 [ 800/1251 ( 64%)]  Loss: 3.379 (3.47)  Time: 0.772s, 1326.37/s  (0.786s, 1302.48/s)  LR: 4.662e-04  Data: 0.009 (0.012)
Train: 315 [ 850/1251 ( 68%)]  Loss: 3.776 (3.49)  Time: 0.830s, 1234.43/s  (0.786s, 1303.02/s)  LR: 4.662e-04  Data: 0.010 (0.012)
Train: 315 [ 900/1251 ( 72%)]  Loss: 3.154 (3.47)  Time: 0.772s, 1326.71/s  (0.786s, 1303.32/s)  LR: 4.662e-04  Data: 0.010 (0.012)
Train: 315 [ 950/1251 ( 76%)]  Loss: 3.195 (3.46)  Time: 0.774s, 1322.73/s  (0.785s, 1303.73/s)  LR: 4.662e-04  Data: 0.010 (0.012)
Train: 315 [1000/1251 ( 80%)]  Loss: 3.549 (3.46)  Time: 0.773s, 1324.76/s  (0.785s, 1303.77/s)  LR: 4.662e-04  Data: 0.010 (0.012)
Train: 315 [1050/1251 ( 84%)]  Loss: 3.459 (3.46)  Time: 0.775s, 1321.93/s  (0.785s, 1303.93/s)  LR: 4.662e-04  Data: 0.009 (0.012)
Train: 315 [1100/1251 ( 88%)]  Loss: 3.598 (3.47)  Time: 0.776s, 1320.25/s  (0.785s, 1304.50/s)  LR: 4.662e-04  Data: 0.009 (0.012)
Train: 315 [1150/1251 ( 92%)]  Loss: 3.458 (3.47)  Time: 0.814s, 1257.51/s  (0.785s, 1305.01/s)  LR: 4.662e-04  Data: 0.011 (0.011)
Train: 315 [1200/1251 ( 96%)]  Loss: 3.628 (3.47)  Time: 0.784s, 1305.78/s  (0.784s, 1305.32/s)  LR: 4.662e-04  Data: 0.009 (0.011)
Train: 315 [1250/1251 (100%)]  Loss: 3.640 (3.48)  Time: 0.768s, 1333.46/s  (0.784s, 1305.41/s)  LR: 4.662e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.561 (1.561)  Loss:  0.8398 (0.8398)  Acc@1: 90.2344 (90.2344)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.194 (0.562)  Loss:  0.9263 (1.3200)  Acc@1: 85.7311 (75.9940)  Acc@5: 96.5802 (93.2360)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-315.pth.tar', 75.99400000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-300.pth.tar', 75.96000008544922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-296.pth.tar', 75.85800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-304.pth.tar', 75.84200010742188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-305.pth.tar', 75.80999998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-313.pth.tar', 75.75199998779297)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-314.pth.tar', 75.70400006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-293.pth.tar', 75.69400010986328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-306.pth.tar', 75.65999998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-307.pth.tar', 75.65799998779296)

Train: 316 [   0/1251 (  0%)]  Loss: 3.634 (3.63)  Time: 2.563s,  399.49/s  (2.563s,  399.49/s)  LR: 4.636e-04  Data: 1.831 (1.831)
Train: 316 [  50/1251 (  4%)]  Loss: 3.118 (3.38)  Time: 0.798s, 1283.98/s  (0.824s, 1242.49/s)  LR: 4.636e-04  Data: 0.010 (0.052)
Train: 316 [ 100/1251 (  8%)]  Loss: 3.653 (3.47)  Time: 0.790s, 1295.40/s  (0.805s, 1271.65/s)  LR: 4.636e-04  Data: 0.012 (0.031)
Train: 316 [ 150/1251 ( 12%)]  Loss: 2.957 (3.34)  Time: 0.820s, 1249.37/s  (0.800s, 1280.46/s)  LR: 4.636e-04  Data: 0.012 (0.024)
Train: 316 [ 200/1251 ( 16%)]  Loss: 3.424 (3.36)  Time: 0.774s, 1322.91/s  (0.804s, 1274.08/s)  LR: 4.636e-04  Data: 0.011 (0.021)
Train: 316 [ 250/1251 ( 20%)]  Loss: 3.224 (3.34)  Time: 0.773s, 1324.75/s  (0.798s, 1282.46/s)  LR: 4.636e-04  Data: 0.010 (0.019)
Train: 316 [ 300/1251 ( 24%)]  Loss: 3.668 (3.38)  Time: 0.775s, 1322.14/s  (0.797s, 1285.02/s)  LR: 4.636e-04  Data: 0.010 (0.018)
Train: 316 [ 350/1251 ( 28%)]  Loss: 3.397 (3.38)  Time: 0.796s, 1286.47/s  (0.795s, 1288.22/s)  LR: 4.636e-04  Data: 0.011 (0.017)
Train: 316 [ 400/1251 ( 32%)]  Loss: 3.807 (3.43)  Time: 0.774s, 1322.84/s  (0.796s, 1286.72/s)  LR: 4.636e-04  Data: 0.010 (0.016)
Train: 316 [ 450/1251 ( 36%)]  Loss: 3.501 (3.44)  Time: 0.773s, 1325.38/s  (0.797s, 1285.54/s)  LR: 4.636e-04  Data: 0.010 (0.015)
Train: 316 [ 500/1251 ( 40%)]  Loss: 3.513 (3.45)  Time: 0.777s, 1317.99/s  (0.795s, 1287.72/s)  LR: 4.636e-04  Data: 0.010 (0.015)
Train: 316 [ 550/1251 ( 44%)]  Loss: 3.785 (3.47)  Time: 0.791s, 1293.80/s  (0.794s, 1289.68/s)  LR: 4.636e-04  Data: 0.010 (0.014)
Train: 316 [ 600/1251 ( 48%)]  Loss: 3.376 (3.47)  Time: 0.778s, 1315.99/s  (0.793s, 1291.20/s)  LR: 4.636e-04  Data: 0.009 (0.014)
Train: 316 [ 650/1251 ( 52%)]  Loss: 3.441 (3.46)  Time: 0.775s, 1321.52/s  (0.793s, 1291.60/s)  LR: 4.636e-04  Data: 0.009 (0.014)
Train: 316 [ 700/1251 ( 56%)]  Loss: 3.333 (3.46)  Time: 0.785s, 1304.39/s  (0.792s, 1293.39/s)  LR: 4.636e-04  Data: 0.010 (0.013)
Train: 316 [ 750/1251 ( 60%)]  Loss: 3.397 (3.45)  Time: 0.773s, 1324.64/s  (0.791s, 1293.91/s)  LR: 4.636e-04  Data: 0.010 (0.013)
Train: 316 [ 800/1251 ( 64%)]  Loss: 3.096 (3.43)  Time: 0.785s, 1304.66/s  (0.791s, 1294.45/s)  LR: 4.636e-04  Data: 0.009 (0.013)
Train: 316 [ 850/1251 ( 68%)]  Loss: 3.096 (3.41)  Time: 0.831s, 1231.90/s  (0.791s, 1295.38/s)  LR: 4.636e-04  Data: 0.009 (0.013)
Train: 316 [ 900/1251 ( 72%)]  Loss: 3.531 (3.42)  Time: 0.772s, 1325.71/s  (0.790s, 1296.49/s)  LR: 4.636e-04  Data: 0.009 (0.013)
Train: 316 [ 950/1251 ( 76%)]  Loss: 3.810 (3.44)  Time: 0.775s, 1320.51/s  (0.789s, 1297.28/s)  LR: 4.636e-04  Data: 0.009 (0.012)
Train: 316 [1000/1251 ( 80%)]  Loss: 3.244 (3.43)  Time: 0.857s, 1194.45/s  (0.789s, 1298.11/s)  LR: 4.636e-04  Data: 0.010 (0.012)
Train: 316 [1050/1251 ( 84%)]  Loss: 3.764 (3.44)  Time: 0.773s, 1324.51/s  (0.788s, 1298.79/s)  LR: 4.636e-04  Data: 0.010 (0.012)
Train: 316 [1100/1251 ( 88%)]  Loss: 3.234 (3.43)  Time: 0.773s, 1324.41/s  (0.788s, 1299.25/s)  LR: 4.636e-04  Data: 0.010 (0.012)
Train: 316 [1150/1251 ( 92%)]  Loss: 3.382 (3.43)  Time: 0.800s, 1279.35/s  (0.788s, 1299.52/s)  LR: 4.636e-04  Data: 0.009 (0.012)
Train: 316 [1200/1251 ( 96%)]  Loss: 3.545 (3.44)  Time: 0.774s, 1322.22/s  (0.788s, 1300.15/s)  LR: 4.636e-04  Data: 0.010 (0.012)
Train: 316 [1250/1251 (100%)]  Loss: 3.389 (3.44)  Time: 0.803s, 1275.47/s  (0.787s, 1300.65/s)  LR: 4.636e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.520 (1.520)  Loss:  0.7065 (0.7065)  Acc@1: 89.3555 (89.3555)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.194 (0.560)  Loss:  0.7964 (1.1992)  Acc@1: 86.2028 (75.8320)  Acc@5: 96.8160 (93.1480)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-315.pth.tar', 75.99400000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-300.pth.tar', 75.96000008544922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-296.pth.tar', 75.85800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-304.pth.tar', 75.84200010742188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-316.pth.tar', 75.83200008300781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-305.pth.tar', 75.80999998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-313.pth.tar', 75.75199998779297)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-314.pth.tar', 75.70400006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-293.pth.tar', 75.69400010986328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-306.pth.tar', 75.65999998046875)

Train: 317 [   0/1251 (  0%)]  Loss: 3.706 (3.71)  Time: 2.475s,  413.77/s  (2.475s,  413.77/s)  LR: 4.610e-04  Data: 1.745 (1.745)
Train: 317 [  50/1251 (  4%)]  Loss: 2.840 (3.27)  Time: 0.773s, 1324.65/s  (0.816s, 1255.63/s)  LR: 4.610e-04  Data: 0.010 (0.048)
Train: 317 [ 100/1251 (  8%)]  Loss: 3.633 (3.39)  Time: 0.817s, 1254.13/s  (0.803s, 1275.17/s)  LR: 4.610e-04  Data: 0.010 (0.029)
Train: 317 [ 150/1251 ( 12%)]  Loss: 3.170 (3.34)  Time: 0.773s, 1325.52/s  (0.807s, 1269.56/s)  LR: 4.610e-04  Data: 0.010 (0.022)
Train: 317 [ 200/1251 ( 16%)]  Loss: 3.700 (3.41)  Time: 0.773s, 1324.49/s  (0.802s, 1276.93/s)  LR: 4.610e-04  Data: 0.009 (0.019)
Train: 317 [ 250/1251 ( 20%)]  Loss: 3.343 (3.40)  Time: 0.772s, 1325.68/s  (0.798s, 1282.98/s)  LR: 4.610e-04  Data: 0.010 (0.017)
Train: 317 [ 300/1251 ( 24%)]  Loss: 3.311 (3.39)  Time: 0.772s, 1326.38/s  (0.797s, 1284.49/s)  LR: 4.610e-04  Data: 0.010 (0.016)
Train: 317 [ 350/1251 ( 28%)]  Loss: 3.578 (3.41)  Time: 0.786s, 1302.70/s  (0.794s, 1289.28/s)  LR: 4.610e-04  Data: 0.010 (0.015)
Train: 317 [ 400/1251 ( 32%)]  Loss: 3.330 (3.40)  Time: 0.776s, 1319.52/s  (0.792s, 1292.34/s)  LR: 4.610e-04  Data: 0.010 (0.014)
Train: 317 [ 450/1251 ( 36%)]  Loss: 3.385 (3.40)  Time: 0.773s, 1324.51/s  (0.791s, 1295.16/s)  LR: 4.610e-04  Data: 0.009 (0.014)
Train: 317 [ 500/1251 ( 40%)]  Loss: 3.481 (3.41)  Time: 0.773s, 1325.29/s  (0.789s, 1297.21/s)  LR: 4.610e-04  Data: 0.010 (0.014)
Train: 317 [ 550/1251 ( 44%)]  Loss: 3.529 (3.42)  Time: 0.779s, 1314.67/s  (0.789s, 1298.36/s)  LR: 4.610e-04  Data: 0.010 (0.013)
Train: 317 [ 600/1251 ( 48%)]  Loss: 3.555 (3.43)  Time: 0.877s, 1167.69/s  (0.788s, 1299.22/s)  LR: 4.610e-04  Data: 0.010 (0.013)
Train: 317 [ 650/1251 ( 52%)]  Loss: 3.499 (3.43)  Time: 0.773s, 1324.85/s  (0.787s, 1300.81/s)  LR: 4.610e-04  Data: 0.009 (0.013)
Train: 317 [ 700/1251 ( 56%)]  Loss: 3.742 (3.45)  Time: 0.772s, 1325.69/s  (0.787s, 1301.84/s)  LR: 4.610e-04  Data: 0.011 (0.012)
Train: 317 [ 750/1251 ( 60%)]  Loss: 3.294 (3.44)  Time: 0.773s, 1324.93/s  (0.786s, 1302.95/s)  LR: 4.610e-04  Data: 0.009 (0.012)
Train: 317 [ 800/1251 ( 64%)]  Loss: 3.577 (3.45)  Time: 0.780s, 1312.88/s  (0.785s, 1304.12/s)  LR: 4.610e-04  Data: 0.010 (0.012)
Train: 317 [ 850/1251 ( 68%)]  Loss: 3.310 (3.44)  Time: 0.807s, 1268.88/s  (0.785s, 1303.98/s)  LR: 4.610e-04  Data: 0.009 (0.012)
Train: 317 [ 900/1251 ( 72%)]  Loss: 3.177 (3.43)  Time: 0.870s, 1176.60/s  (0.786s, 1303.47/s)  LR: 4.610e-04  Data: 0.010 (0.012)
Train: 317 [ 950/1251 ( 76%)]  Loss: 3.758 (3.45)  Time: 0.772s, 1326.86/s  (0.786s, 1303.05/s)  LR: 4.610e-04  Data: 0.010 (0.012)
Train: 317 [1000/1251 ( 80%)]  Loss: 3.372 (3.44)  Time: 0.810s, 1264.06/s  (0.786s, 1302.05/s)  LR: 4.610e-04  Data: 0.009 (0.012)
Train: 317 [1050/1251 ( 84%)]  Loss: 3.381 (3.44)  Time: 0.775s, 1321.70/s  (0.787s, 1300.89/s)  LR: 4.610e-04  Data: 0.010 (0.012)
Train: 317 [1100/1251 ( 88%)]  Loss: 3.496 (3.44)  Time: 0.774s, 1323.03/s  (0.787s, 1301.37/s)  LR: 4.610e-04  Data: 0.010 (0.011)
Train: 317 [1150/1251 ( 92%)]  Loss: 3.504 (3.44)  Time: 0.773s, 1325.23/s  (0.787s, 1301.46/s)  LR: 4.610e-04  Data: 0.009 (0.011)
Train: 317 [1200/1251 ( 96%)]  Loss: 3.483 (3.45)  Time: 0.775s, 1321.81/s  (0.786s, 1302.21/s)  LR: 4.610e-04  Data: 0.009 (0.011)
Train: 317 [1250/1251 (100%)]  Loss: 3.326 (3.44)  Time: 0.763s, 1342.27/s  (0.787s, 1301.57/s)  LR: 4.610e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.609 (1.609)  Loss:  0.7637 (0.7637)  Acc@1: 90.0391 (90.0391)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.194 (0.563)  Loss:  0.8315 (1.2761)  Acc@1: 84.6698 (76.2340)  Acc@5: 96.3443 (93.3160)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-317.pth.tar', 76.2340001928711)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-315.pth.tar', 75.99400000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-300.pth.tar', 75.96000008544922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-296.pth.tar', 75.85800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-304.pth.tar', 75.84200010742188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-316.pth.tar', 75.83200008300781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-305.pth.tar', 75.80999998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-313.pth.tar', 75.75199998779297)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-314.pth.tar', 75.70400006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-293.pth.tar', 75.69400010986328)

Train: 318 [   0/1251 (  0%)]  Loss: 3.158 (3.16)  Time: 2.250s,  455.06/s  (2.250s,  455.06/s)  LR: 4.584e-04  Data: 1.517 (1.517)
Train: 318 [  50/1251 (  4%)]  Loss: 3.375 (3.27)  Time: 0.825s, 1240.62/s  (0.825s, 1241.02/s)  LR: 4.584e-04  Data: 0.013 (0.042)
Train: 318 [ 100/1251 (  8%)]  Loss: 3.543 (3.36)  Time: 0.775s, 1321.98/s  (0.809s, 1265.05/s)  LR: 4.584e-04  Data: 0.010 (0.026)
Train: 318 [ 150/1251 ( 12%)]  Loss: 3.405 (3.37)  Time: 0.772s, 1326.67/s  (0.801s, 1278.17/s)  LR: 4.584e-04  Data: 0.010 (0.021)
Train: 318 [ 200/1251 ( 16%)]  Loss: 3.838 (3.46)  Time: 0.771s, 1328.40/s  (0.797s, 1284.68/s)  LR: 4.584e-04  Data: 0.009 (0.018)
Train: 318 [ 250/1251 ( 20%)]  Loss: 3.701 (3.50)  Time: 0.777s, 1317.06/s  (0.795s, 1288.34/s)  LR: 4.584e-04  Data: 0.010 (0.016)
Train: 318 [ 300/1251 ( 24%)]  Loss: 3.206 (3.46)  Time: 0.807s, 1268.28/s  (0.793s, 1291.71/s)  LR: 4.584e-04  Data: 0.010 (0.015)
Train: 318 [ 350/1251 ( 28%)]  Loss: 3.600 (3.48)  Time: 0.776s, 1319.61/s  (0.792s, 1292.28/s)  LR: 4.584e-04  Data: 0.010 (0.015)
Train: 318 [ 400/1251 ( 32%)]  Loss: 3.311 (3.46)  Time: 0.774s, 1323.55/s  (0.791s, 1294.98/s)  LR: 4.584e-04  Data: 0.010 (0.014)
Train: 318 [ 450/1251 ( 36%)]  Loss: 3.500 (3.46)  Time: 0.775s, 1320.55/s  (0.790s, 1296.35/s)  LR: 4.584e-04  Data: 0.014 (0.013)
Train: 318 [ 500/1251 ( 40%)]  Loss: 3.541 (3.47)  Time: 0.773s, 1325.20/s  (0.789s, 1298.14/s)  LR: 4.584e-04  Data: 0.009 (0.013)
Train: 318 [ 550/1251 ( 44%)]  Loss: 3.321 (3.46)  Time: 0.814s, 1257.31/s  (0.789s, 1298.02/s)  LR: 4.584e-04  Data: 0.010 (0.013)
Train: 318 [ 600/1251 ( 48%)]  Loss: 3.219 (3.44)  Time: 0.773s, 1324.42/s  (0.790s, 1296.42/s)  LR: 4.584e-04  Data: 0.010 (0.013)
Train: 318 [ 650/1251 ( 52%)]  Loss: 3.846 (3.47)  Time: 0.818s, 1251.66/s  (0.789s, 1297.99/s)  LR: 4.584e-04  Data: 0.011 (0.012)
Train: 318 [ 700/1251 ( 56%)]  Loss: 3.612 (3.48)  Time: 0.773s, 1324.56/s  (0.790s, 1296.53/s)  LR: 4.584e-04  Data: 0.010 (0.012)
Train: 318 [ 750/1251 ( 60%)]  Loss: 3.422 (3.47)  Time: 0.773s, 1324.73/s  (0.789s, 1297.69/s)  LR: 4.584e-04  Data: 0.010 (0.012)
Train: 318 [ 800/1251 ( 64%)]  Loss: 3.164 (3.46)  Time: 0.773s, 1324.60/s  (0.789s, 1298.60/s)  LR: 4.584e-04  Data: 0.010 (0.012)
Train: 318 [ 850/1251 ( 68%)]  Loss: 3.266 (3.45)  Time: 0.781s, 1310.63/s  (0.788s, 1299.87/s)  LR: 4.584e-04  Data: 0.010 (0.012)
Train: 318 [ 900/1251 ( 72%)]  Loss: 3.425 (3.44)  Time: 0.779s, 1314.49/s  (0.787s, 1300.93/s)  LR: 4.584e-04  Data: 0.011 (0.012)
Train: 318 [ 950/1251 ( 76%)]  Loss: 3.386 (3.44)  Time: 0.782s, 1309.63/s  (0.787s, 1301.32/s)  LR: 4.584e-04  Data: 0.010 (0.012)
Train: 318 [1000/1251 ( 80%)]  Loss: 3.735 (3.46)  Time: 0.777s, 1317.67/s  (0.786s, 1302.13/s)  LR: 4.584e-04  Data: 0.009 (0.012)
Train: 318 [1050/1251 ( 84%)]  Loss: 3.345 (3.45)  Time: 0.773s, 1325.12/s  (0.786s, 1302.79/s)  LR: 4.584e-04  Data: 0.010 (0.011)
Train: 318 [1100/1251 ( 88%)]  Loss: 3.505 (3.45)  Time: 0.775s, 1320.97/s  (0.786s, 1303.11/s)  LR: 4.584e-04  Data: 0.010 (0.011)
Train: 318 [1150/1251 ( 92%)]  Loss: 3.277 (3.45)  Time: 0.773s, 1325.14/s  (0.786s, 1302.95/s)  LR: 4.584e-04  Data: 0.010 (0.011)
Train: 318 [1200/1251 ( 96%)]  Loss: 3.427 (3.45)  Time: 0.777s, 1318.16/s  (0.786s, 1303.46/s)  LR: 4.584e-04  Data: 0.009 (0.011)
Train: 318 [1250/1251 (100%)]  Loss: 3.422 (3.44)  Time: 0.760s, 1347.79/s  (0.786s, 1303.55/s)  LR: 4.584e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.614 (1.614)  Loss:  0.7305 (0.7305)  Acc@1: 90.6250 (90.6250)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.194 (0.563)  Loss:  0.8564 (1.2666)  Acc@1: 84.9057 (75.9340)  Acc@5: 96.6981 (93.3680)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-317.pth.tar', 76.2340001928711)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-315.pth.tar', 75.99400000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-300.pth.tar', 75.96000008544922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-318.pth.tar', 75.9340000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-296.pth.tar', 75.85800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-304.pth.tar', 75.84200010742188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-316.pth.tar', 75.83200008300781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-305.pth.tar', 75.80999998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-313.pth.tar', 75.75199998779297)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-314.pth.tar', 75.70400006103516)

Train: 319 [   0/1251 (  0%)]  Loss: 3.739 (3.74)  Time: 2.255s,  454.15/s  (2.255s,  454.15/s)  LR: 4.558e-04  Data: 1.522 (1.522)
Train: 319 [  50/1251 (  4%)]  Loss: 3.429 (3.58)  Time: 0.784s, 1306.67/s  (0.817s, 1253.13/s)  LR: 4.558e-04  Data: 0.010 (0.047)
Train: 319 [ 100/1251 (  8%)]  Loss: 3.532 (3.57)  Time: 0.782s, 1309.03/s  (0.797s, 1284.50/s)  LR: 4.558e-04  Data: 0.009 (0.028)
Train: 319 [ 150/1251 ( 12%)]  Loss: 3.405 (3.53)  Time: 0.845s, 1212.00/s  (0.791s, 1294.12/s)  LR: 4.558e-04  Data: 0.009 (0.022)
Train: 319 [ 200/1251 ( 16%)]  Loss: 3.532 (3.53)  Time: 0.777s, 1317.18/s  (0.789s, 1297.89/s)  LR: 4.558e-04  Data: 0.010 (0.019)
Train: 319 [ 250/1251 ( 20%)]  Loss: 2.823 (3.41)  Time: 0.772s, 1326.44/s  (0.787s, 1300.60/s)  LR: 4.558e-04  Data: 0.010 (0.017)
Train: 319 [ 300/1251 ( 24%)]  Loss: 3.437 (3.41)  Time: 0.772s, 1327.24/s  (0.786s, 1303.60/s)  LR: 4.558e-04  Data: 0.010 (0.016)
Train: 319 [ 350/1251 ( 28%)]  Loss: 3.178 (3.38)  Time: 0.774s, 1323.73/s  (0.784s, 1305.55/s)  LR: 4.558e-04  Data: 0.010 (0.015)
Train: 319 [ 400/1251 ( 32%)]  Loss: 3.686 (3.42)  Time: 0.775s, 1322.10/s  (0.784s, 1306.81/s)  LR: 4.558e-04  Data: 0.010 (0.015)
Train: 319 [ 450/1251 ( 36%)]  Loss: 3.452 (3.42)  Time: 0.773s, 1324.24/s  (0.784s, 1306.16/s)  LR: 4.558e-04  Data: 0.009 (0.014)
Train: 319 [ 500/1251 ( 40%)]  Loss: 3.469 (3.43)  Time: 0.772s, 1326.35/s  (0.785s, 1304.29/s)  LR: 4.558e-04  Data: 0.010 (0.014)
Train: 319 [ 550/1251 ( 44%)]  Loss: 3.146 (3.40)  Time: 0.775s, 1320.53/s  (0.786s, 1302.67/s)  LR: 4.558e-04  Data: 0.010 (0.013)
Train: 319 [ 600/1251 ( 48%)]  Loss: 3.676 (3.42)  Time: 0.772s, 1326.14/s  (0.786s, 1302.21/s)  LR: 4.558e-04  Data: 0.009 (0.013)
Train: 319 [ 650/1251 ( 52%)]  Loss: 3.275 (3.41)  Time: 0.773s, 1324.82/s  (0.786s, 1303.55/s)  LR: 4.558e-04  Data: 0.010 (0.013)
Train: 319 [ 700/1251 ( 56%)]  Loss: 3.405 (3.41)  Time: 0.778s, 1316.90/s  (0.787s, 1301.32/s)  LR: 4.558e-04  Data: 0.010 (0.013)
Train: 319 [ 750/1251 ( 60%)]  Loss: 3.234 (3.40)  Time: 0.772s, 1326.05/s  (0.787s, 1301.56/s)  LR: 4.558e-04  Data: 0.010 (0.012)
Train: 319 [ 800/1251 ( 64%)]  Loss: 3.666 (3.42)  Time: 0.808s, 1266.57/s  (0.787s, 1301.48/s)  LR: 4.558e-04  Data: 0.009 (0.012)
Train: 319 [ 850/1251 ( 68%)]  Loss: 3.509 (3.42)  Time: 0.774s, 1323.10/s  (0.786s, 1302.23/s)  LR: 4.558e-04  Data: 0.010 (0.012)
Train: 319 [ 900/1251 ( 72%)]  Loss: 3.089 (3.40)  Time: 0.772s, 1325.84/s  (0.786s, 1302.38/s)  LR: 4.558e-04  Data: 0.009 (0.012)
Train: 319 [ 950/1251 ( 76%)]  Loss: 3.202 (3.39)  Time: 0.791s, 1294.88/s  (0.786s, 1302.21/s)  LR: 4.558e-04  Data: 0.014 (0.012)
Train: 319 [1000/1251 ( 80%)]  Loss: 3.182 (3.38)  Time: 0.806s, 1270.59/s  (0.787s, 1301.95/s)  LR: 4.558e-04  Data: 0.009 (0.012)
Train: 319 [1050/1251 ( 84%)]  Loss: 3.558 (3.39)  Time: 0.772s, 1326.86/s  (0.786s, 1302.33/s)  LR: 4.558e-04  Data: 0.009 (0.012)
Train: 319 [1100/1251 ( 88%)]  Loss: 3.663 (3.40)  Time: 0.774s, 1323.01/s  (0.786s, 1303.05/s)  LR: 4.558e-04  Data: 0.010 (0.012)
Train: 319 [1150/1251 ( 92%)]  Loss: 3.174 (3.39)  Time: 0.770s, 1329.59/s  (0.786s, 1303.61/s)  LR: 4.558e-04  Data: 0.010 (0.012)
Train: 319 [1200/1251 ( 96%)]  Loss: 3.820 (3.41)  Time: 0.780s, 1313.22/s  (0.785s, 1304.31/s)  LR: 4.558e-04  Data: 0.009 (0.011)
Train: 319 [1250/1251 (100%)]  Loss: 3.541 (3.42)  Time: 0.760s, 1347.10/s  (0.785s, 1304.76/s)  LR: 4.558e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.559 (1.559)  Loss:  0.7725 (0.7725)  Acc@1: 90.9180 (90.9180)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.193 (0.563)  Loss:  0.8564 (1.3020)  Acc@1: 86.6745 (75.6380)  Acc@5: 97.2877 (93.1040)
Train: 320 [   0/1251 (  0%)]  Loss: 3.486 (3.49)  Time: 2.228s,  459.60/s  (2.228s,  459.60/s)  LR: 4.533e-04  Data: 1.500 (1.500)
Train: 320 [  50/1251 (  4%)]  Loss: 3.442 (3.46)  Time: 0.772s, 1325.75/s  (0.820s, 1249.01/s)  LR: 4.533e-04  Data: 0.009 (0.044)
Train: 320 [ 100/1251 (  8%)]  Loss: 3.277 (3.40)  Time: 0.776s, 1319.98/s  (0.799s, 1281.58/s)  LR: 4.533e-04  Data: 0.009 (0.027)
Train: 320 [ 150/1251 ( 12%)]  Loss: 3.256 (3.37)  Time: 0.789s, 1298.02/s  (0.797s, 1284.67/s)  LR: 4.533e-04  Data: 0.009 (0.021)
Train: 320 [ 200/1251 ( 16%)]  Loss: 3.506 (3.39)  Time: 0.818s, 1251.95/s  (0.793s, 1291.65/s)  LR: 4.533e-04  Data: 0.010 (0.018)
Train: 320 [ 250/1251 ( 20%)]  Loss: 3.389 (3.39)  Time: 0.840s, 1219.72/s  (0.790s, 1296.15/s)  LR: 4.533e-04  Data: 0.009 (0.017)
Train: 320 [ 300/1251 ( 24%)]  Loss: 3.177 (3.36)  Time: 0.776s, 1320.06/s  (0.788s, 1299.93/s)  LR: 4.533e-04  Data: 0.009 (0.015)
Train: 320 [ 350/1251 ( 28%)]  Loss: 3.765 (3.41)  Time: 0.775s, 1321.72/s  (0.787s, 1301.96/s)  LR: 4.533e-04  Data: 0.009 (0.015)
Train: 320 [ 400/1251 ( 32%)]  Loss: 3.731 (3.45)  Time: 0.772s, 1326.06/s  (0.786s, 1303.36/s)  LR: 4.533e-04  Data: 0.010 (0.014)
Train: 320 [ 450/1251 ( 36%)]  Loss: 3.355 (3.44)  Time: 0.774s, 1323.38/s  (0.785s, 1304.22/s)  LR: 4.533e-04  Data: 0.010 (0.013)
Train: 320 [ 500/1251 ( 40%)]  Loss: 3.449 (3.44)  Time: 0.774s, 1322.27/s  (0.784s, 1305.91/s)  LR: 4.533e-04  Data: 0.009 (0.013)
Train: 320 [ 550/1251 ( 44%)]  Loss: 3.560 (3.45)  Time: 0.774s, 1322.85/s  (0.784s, 1306.39/s)  LR: 4.533e-04  Data: 0.010 (0.013)
Train: 320 [ 600/1251 ( 48%)]  Loss: 3.186 (3.43)  Time: 0.815s, 1255.95/s  (0.784s, 1306.36/s)  LR: 4.533e-04  Data: 0.009 (0.012)
Train: 320 [ 650/1251 ( 52%)]  Loss: 3.976 (3.47)  Time: 0.782s, 1309.92/s  (0.784s, 1306.75/s)  LR: 4.533e-04  Data: 0.010 (0.012)
Train: 320 [ 700/1251 ( 56%)]  Loss: 3.010 (3.44)  Time: 0.778s, 1316.57/s  (0.784s, 1306.82/s)  LR: 4.533e-04  Data: 0.010 (0.012)
Train: 320 [ 750/1251 ( 60%)]  Loss: 3.351 (3.43)  Time: 0.772s, 1326.36/s  (0.783s, 1307.12/s)  LR: 4.533e-04  Data: 0.009 (0.012)
Train: 320 [ 800/1251 ( 64%)]  Loss: 3.328 (3.43)  Time: 0.773s, 1324.34/s  (0.783s, 1307.85/s)  LR: 4.533e-04  Data: 0.009 (0.012)
Train: 320 [ 850/1251 ( 68%)]  Loss: 3.441 (3.43)  Time: 0.785s, 1303.86/s  (0.783s, 1308.33/s)  LR: 4.533e-04  Data: 0.009 (0.012)
Train: 320 [ 900/1251 ( 72%)]  Loss: 3.727 (3.44)  Time: 0.774s, 1322.64/s  (0.782s, 1308.99/s)  LR: 4.533e-04  Data: 0.009 (0.011)
Train: 320 [ 950/1251 ( 76%)]  Loss: 3.378 (3.44)  Time: 0.775s, 1321.10/s  (0.782s, 1308.63/s)  LR: 4.533e-04  Data: 0.009 (0.011)
Train: 320 [1000/1251 ( 80%)]  Loss: 3.425 (3.44)  Time: 0.772s, 1325.58/s  (0.782s, 1308.67/s)  LR: 4.533e-04  Data: 0.010 (0.011)
Train: 320 [1050/1251 ( 84%)]  Loss: 3.664 (3.45)  Time: 0.773s, 1325.09/s  (0.782s, 1308.76/s)  LR: 4.533e-04  Data: 0.010 (0.011)
Train: 320 [1100/1251 ( 88%)]  Loss: 3.537 (3.45)  Time: 0.792s, 1293.47/s  (0.782s, 1308.67/s)  LR: 4.533e-04  Data: 0.012 (0.011)
Train: 320 [1150/1251 ( 92%)]  Loss: 3.494 (3.45)  Time: 0.781s, 1311.90/s  (0.782s, 1308.80/s)  LR: 4.533e-04  Data: 0.010 (0.011)
Train: 320 [1200/1251 ( 96%)]  Loss: 3.552 (3.46)  Time: 0.774s, 1323.20/s  (0.782s, 1309.27/s)  LR: 4.533e-04  Data: 0.009 (0.011)
Train: 320 [1250/1251 (100%)]  Loss: 3.232 (3.45)  Time: 0.762s, 1344.66/s  (0.782s, 1309.20/s)  LR: 4.533e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.504 (1.504)  Loss:  0.6494 (0.6494)  Acc@1: 90.0391 (90.0391)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.194 (0.561)  Loss:  0.7842 (1.1673)  Acc@1: 86.5566 (76.2220)  Acc@5: 96.6981 (93.2180)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-317.pth.tar', 76.2340001928711)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-320.pth.tar', 76.22200010742188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-315.pth.tar', 75.99400000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-300.pth.tar', 75.96000008544922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-318.pth.tar', 75.9340000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-296.pth.tar', 75.85800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-304.pth.tar', 75.84200010742188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-316.pth.tar', 75.83200008300781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-305.pth.tar', 75.80999998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-313.pth.tar', 75.75199998779297)

Train: 321 [   0/1251 (  0%)]  Loss: 3.344 (3.34)  Time: 2.192s,  467.10/s  (2.192s,  467.10/s)  LR: 4.507e-04  Data: 1.462 (1.462)
Train: 321 [  50/1251 (  4%)]  Loss: 3.316 (3.33)  Time: 0.806s, 1270.72/s  (0.838s, 1222.32/s)  LR: 4.507e-04  Data: 0.009 (0.047)
Train: 321 [ 100/1251 (  8%)]  Loss: 3.649 (3.44)  Time: 0.807s, 1269.26/s  (0.820s, 1248.59/s)  LR: 4.507e-04  Data: 0.010 (0.029)
Train: 321 [ 150/1251 ( 12%)]  Loss: 3.275 (3.40)  Time: 0.773s, 1325.17/s  (0.808s, 1267.04/s)  LR: 4.507e-04  Data: 0.010 (0.022)
Train: 321 [ 200/1251 ( 16%)]  Loss: 3.768 (3.47)  Time: 0.773s, 1324.61/s  (0.807s, 1268.53/s)  LR: 4.507e-04  Data: 0.009 (0.019)
Train: 321 [ 250/1251 ( 20%)]  Loss: 3.095 (3.41)  Time: 0.780s, 1313.49/s  (0.804s, 1274.31/s)  LR: 4.507e-04  Data: 0.010 (0.018)
Train: 321 [ 300/1251 ( 24%)]  Loss: 3.113 (3.37)  Time: 0.773s, 1324.43/s  (0.800s, 1280.63/s)  LR: 4.507e-04  Data: 0.009 (0.016)
Train: 321 [ 350/1251 ( 28%)]  Loss: 3.504 (3.38)  Time: 0.769s, 1330.79/s  (0.796s, 1286.11/s)  LR: 4.507e-04  Data: 0.010 (0.015)
Train: 321 [ 400/1251 ( 32%)]  Loss: 3.573 (3.40)  Time: 0.783s, 1307.41/s  (0.794s, 1290.04/s)  LR: 4.507e-04  Data: 0.013 (0.015)
Train: 321 [ 450/1251 ( 36%)]  Loss: 3.672 (3.43)  Time: 0.773s, 1324.75/s  (0.792s, 1293.06/s)  LR: 4.507e-04  Data: 0.010 (0.014)
Train: 321 [ 500/1251 ( 40%)]  Loss: 3.530 (3.44)  Time: 0.774s, 1322.40/s  (0.791s, 1295.23/s)  LR: 4.507e-04  Data: 0.012 (0.014)
Train: 321 [ 550/1251 ( 44%)]  Loss: 3.472 (3.44)  Time: 0.774s, 1322.41/s  (0.790s, 1296.30/s)  LR: 4.507e-04  Data: 0.010 (0.013)
Train: 321 [ 600/1251 ( 48%)]  Loss: 3.518 (3.45)  Time: 0.782s, 1309.78/s  (0.790s, 1296.86/s)  LR: 4.507e-04  Data: 0.010 (0.013)
Train: 321 [ 650/1251 ( 52%)]  Loss: 3.200 (3.43)  Time: 0.773s, 1324.46/s  (0.789s, 1298.39/s)  LR: 4.507e-04  Data: 0.010 (0.013)
Train: 321 [ 700/1251 ( 56%)]  Loss: 3.504 (3.44)  Time: 0.805s, 1271.89/s  (0.788s, 1299.61/s)  LR: 4.507e-04  Data: 0.010 (0.013)
Train: 321 [ 750/1251 ( 60%)]  Loss: 3.588 (3.45)  Time: 0.772s, 1326.17/s  (0.787s, 1300.73/s)  LR: 4.507e-04  Data: 0.010 (0.013)
Train: 321 [ 800/1251 ( 64%)]  Loss: 3.423 (3.44)  Time: 0.808s, 1266.75/s  (0.787s, 1301.57/s)  LR: 4.507e-04  Data: 0.011 (0.012)
Train: 321 [ 850/1251 ( 68%)]  Loss: 3.890 (3.47)  Time: 0.786s, 1303.08/s  (0.786s, 1302.12/s)  LR: 4.507e-04  Data: 0.010 (0.012)
Train: 321 [ 900/1251 ( 72%)]  Loss: 3.309 (3.46)  Time: 0.862s, 1188.58/s  (0.786s, 1302.50/s)  LR: 4.507e-04  Data: 0.009 (0.012)
Train: 321 [ 950/1251 ( 76%)]  Loss: 3.349 (3.45)  Time: 0.772s, 1327.08/s  (0.787s, 1300.91/s)  LR: 4.507e-04  Data: 0.010 (0.012)
Train: 321 [1000/1251 ( 80%)]  Loss: 3.123 (3.44)  Time: 0.772s, 1325.90/s  (0.787s, 1301.10/s)  LR: 4.507e-04  Data: 0.009 (0.012)
Train: 321 [1050/1251 ( 84%)]  Loss: 3.540 (3.44)  Time: 0.819s, 1249.64/s  (0.787s, 1301.25/s)  LR: 4.507e-04  Data: 0.011 (0.012)
Train: 321 [1100/1251 ( 88%)]  Loss: 3.146 (3.43)  Time: 0.773s, 1324.42/s  (0.788s, 1299.48/s)  LR: 4.507e-04  Data: 0.010 (0.012)
Train: 321 [1150/1251 ( 92%)]  Loss: 3.484 (3.43)  Time: 0.788s, 1299.99/s  (0.788s, 1300.00/s)  LR: 4.507e-04  Data: 0.010 (0.012)
Train: 321 [1200/1251 ( 96%)]  Loss: 3.417 (3.43)  Time: 0.893s, 1146.90/s  (0.789s, 1298.57/s)  LR: 4.507e-04  Data: 0.010 (0.012)
Train: 321 [1250/1251 (100%)]  Loss: 3.545 (3.44)  Time: 0.764s, 1340.74/s  (0.788s, 1299.26/s)  LR: 4.507e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.519 (1.519)  Loss:  0.7588 (0.7588)  Acc@1: 90.4297 (90.4297)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.194 (0.571)  Loss:  0.7998 (1.3074)  Acc@1: 86.5566 (76.0240)  Acc@5: 96.5802 (93.2960)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-317.pth.tar', 76.2340001928711)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-320.pth.tar', 76.22200010742188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-321.pth.tar', 76.02399997802735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-315.pth.tar', 75.99400000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-300.pth.tar', 75.96000008544922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-318.pth.tar', 75.9340000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-296.pth.tar', 75.85800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-304.pth.tar', 75.84200010742188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-316.pth.tar', 75.83200008300781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-305.pth.tar', 75.80999998291016)

Train: 322 [   0/1251 (  0%)]  Loss: 3.563 (3.56)  Time: 2.187s,  468.29/s  (2.187s,  468.29/s)  LR: 4.481e-04  Data: 1.453 (1.453)
Train: 322 [  50/1251 (  4%)]  Loss: 3.482 (3.52)  Time: 0.774s, 1322.33/s  (0.816s, 1254.38/s)  LR: 4.481e-04  Data: 0.010 (0.043)
Train: 322 [ 100/1251 (  8%)]  Loss: 3.477 (3.51)  Time: 0.773s, 1325.55/s  (0.802s, 1276.66/s)  LR: 4.481e-04  Data: 0.009 (0.026)
Train: 322 [ 150/1251 ( 12%)]  Loss: 3.626 (3.54)  Time: 0.774s, 1322.78/s  (0.794s, 1289.04/s)  LR: 4.481e-04  Data: 0.009 (0.021)
Train: 322 [ 200/1251 ( 16%)]  Loss: 3.295 (3.49)  Time: 0.772s, 1326.43/s  (0.792s, 1293.46/s)  LR: 4.481e-04  Data: 0.010 (0.018)
Train: 322 [ 250/1251 ( 20%)]  Loss: 3.298 (3.46)  Time: 0.779s, 1315.05/s  (0.789s, 1297.80/s)  LR: 4.481e-04  Data: 0.010 (0.016)
Train: 322 [ 300/1251 ( 24%)]  Loss: 3.672 (3.49)  Time: 0.782s, 1309.66/s  (0.788s, 1298.90/s)  LR: 4.481e-04  Data: 0.012 (0.015)
Train: 322 [ 350/1251 ( 28%)]  Loss: 3.443 (3.48)  Time: 0.787s, 1301.79/s  (0.788s, 1299.68/s)  LR: 4.481e-04  Data: 0.011 (0.015)
Train: 322 [ 400/1251 ( 32%)]  Loss: 3.438 (3.48)  Time: 0.773s, 1323.90/s  (0.787s, 1301.35/s)  LR: 4.481e-04  Data: 0.010 (0.014)
Train: 322 [ 450/1251 ( 36%)]  Loss: 3.530 (3.48)  Time: 0.793s, 1291.65/s  (0.786s, 1303.32/s)  LR: 4.481e-04  Data: 0.011 (0.014)
Train: 322 [ 500/1251 ( 40%)]  Loss: 3.096 (3.45)  Time: 0.776s, 1318.90/s  (0.785s, 1304.67/s)  LR: 4.481e-04  Data: 0.009 (0.013)
Train: 322 [ 550/1251 ( 44%)]  Loss: 3.525 (3.45)  Time: 0.776s, 1319.04/s  (0.785s, 1305.28/s)  LR: 4.481e-04  Data: 0.010 (0.013)
Train: 322 [ 600/1251 ( 48%)]  Loss: 3.239 (3.44)  Time: 0.775s, 1321.09/s  (0.784s, 1306.17/s)  LR: 4.481e-04  Data: 0.009 (0.013)
Train: 322 [ 650/1251 ( 52%)]  Loss: 3.205 (3.42)  Time: 0.773s, 1324.02/s  (0.784s, 1306.72/s)  LR: 4.481e-04  Data: 0.010 (0.012)
Train: 322 [ 700/1251 ( 56%)]  Loss: 3.063 (3.40)  Time: 0.774s, 1322.18/s  (0.783s, 1306.98/s)  LR: 4.481e-04  Data: 0.010 (0.012)
Train: 322 [ 750/1251 ( 60%)]  Loss: 3.345 (3.39)  Time: 0.827s, 1238.15/s  (0.784s, 1305.53/s)  LR: 4.481e-04  Data: 0.013 (0.012)
Train: 322 [ 800/1251 ( 64%)]  Loss: 3.346 (3.39)  Time: 0.832s, 1230.28/s  (0.786s, 1302.52/s)  LR: 4.481e-04  Data: 0.010 (0.012)
Train: 322 [ 850/1251 ( 68%)]  Loss: 3.793 (3.41)  Time: 0.777s, 1317.94/s  (0.786s, 1303.30/s)  LR: 4.481e-04  Data: 0.010 (0.012)
Train: 322 [ 900/1251 ( 72%)]  Loss: 3.457 (3.42)  Time: 0.785s, 1305.02/s  (0.785s, 1304.18/s)  LR: 4.481e-04  Data: 0.010 (0.012)
Train: 322 [ 950/1251 ( 76%)]  Loss: 3.308 (3.41)  Time: 0.776s, 1319.58/s  (0.785s, 1304.73/s)  LR: 4.481e-04  Data: 0.010 (0.012)
Train: 322 [1000/1251 ( 80%)]  Loss: 3.568 (3.42)  Time: 0.780s, 1312.10/s  (0.784s, 1305.30/s)  LR: 4.481e-04  Data: 0.011 (0.012)
Train: 322 [1050/1251 ( 84%)]  Loss: 3.530 (3.42)  Time: 0.781s, 1310.32/s  (0.784s, 1305.59/s)  LR: 4.481e-04  Data: 0.010 (0.012)
Train: 322 [1100/1251 ( 88%)]  Loss: 3.218 (3.41)  Time: 0.828s, 1237.20/s  (0.785s, 1305.05/s)  LR: 4.481e-04  Data: 0.013 (0.011)
Train: 322 [1150/1251 ( 92%)]  Loss: 3.039 (3.40)  Time: 0.790s, 1296.33/s  (0.785s, 1304.60/s)  LR: 4.481e-04  Data: 0.010 (0.011)
Train: 322 [1200/1251 ( 96%)]  Loss: 3.283 (3.39)  Time: 0.773s, 1324.50/s  (0.785s, 1305.03/s)  LR: 4.481e-04  Data: 0.010 (0.011)
Train: 322 [1250/1251 (100%)]  Loss: 3.426 (3.39)  Time: 0.760s, 1348.12/s  (0.784s, 1305.57/s)  LR: 4.481e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.557 (1.557)  Loss:  0.7129 (0.7129)  Acc@1: 90.3320 (90.3320)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.194 (0.561)  Loss:  0.7754 (1.2051)  Acc@1: 85.9670 (76.2520)  Acc@5: 97.7594 (93.3020)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-322.pth.tar', 76.25199998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-317.pth.tar', 76.2340001928711)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-320.pth.tar', 76.22200010742188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-321.pth.tar', 76.02399997802735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-315.pth.tar', 75.99400000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-300.pth.tar', 75.96000008544922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-318.pth.tar', 75.9340000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-296.pth.tar', 75.85800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-304.pth.tar', 75.84200010742188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-316.pth.tar', 75.83200008300781)

Train: 323 [   0/1251 (  0%)]  Loss: 3.676 (3.68)  Time: 2.217s,  461.88/s  (2.217s,  461.88/s)  LR: 4.455e-04  Data: 1.485 (1.485)
Train: 323 [  50/1251 (  4%)]  Loss: 3.268 (3.47)  Time: 0.773s, 1325.10/s  (0.812s, 1261.00/s)  LR: 4.455e-04  Data: 0.010 (0.044)
Train: 323 [ 100/1251 (  8%)]  Loss: 3.322 (3.42)  Time: 0.774s, 1323.16/s  (0.795s, 1287.73/s)  LR: 4.455e-04  Data: 0.010 (0.027)
Train: 323 [ 150/1251 ( 12%)]  Loss: 3.622 (3.47)  Time: 0.775s, 1321.27/s  (0.790s, 1296.64/s)  LR: 4.455e-04  Data: 0.010 (0.021)
Train: 323 [ 200/1251 ( 16%)]  Loss: 3.530 (3.48)  Time: 0.773s, 1325.19/s  (0.788s, 1299.44/s)  LR: 4.455e-04  Data: 0.010 (0.019)
Train: 323 [ 250/1251 ( 20%)]  Loss: 3.360 (3.46)  Time: 0.772s, 1327.20/s  (0.788s, 1299.42/s)  LR: 4.455e-04  Data: 0.010 (0.017)
Train: 323 [ 300/1251 ( 24%)]  Loss: 3.229 (3.43)  Time: 0.786s, 1302.96/s  (0.786s, 1302.04/s)  LR: 4.455e-04  Data: 0.010 (0.016)
Train: 323 [ 350/1251 ( 28%)]  Loss: 3.482 (3.44)  Time: 0.782s, 1309.41/s  (0.785s, 1304.56/s)  LR: 4.455e-04  Data: 0.009 (0.015)
Train: 323 [ 400/1251 ( 32%)]  Loss: 3.483 (3.44)  Time: 0.778s, 1315.57/s  (0.784s, 1306.03/s)  LR: 4.455e-04  Data: 0.009 (0.014)
Train: 323 [ 450/1251 ( 36%)]  Loss: 3.459 (3.44)  Time: 0.849s, 1205.56/s  (0.783s, 1307.24/s)  LR: 4.455e-04  Data: 0.010 (0.014)
Train: 323 [ 500/1251 ( 40%)]  Loss: 3.252 (3.43)  Time: 0.771s, 1327.78/s  (0.783s, 1307.79/s)  LR: 4.455e-04  Data: 0.010 (0.013)
Train: 323 [ 550/1251 ( 44%)]  Loss: 3.136 (3.40)  Time: 0.778s, 1315.38/s  (0.783s, 1307.71/s)  LR: 4.455e-04  Data: 0.009 (0.013)
Train: 323 [ 600/1251 ( 48%)]  Loss: 3.387 (3.40)  Time: 0.773s, 1323.91/s  (0.783s, 1307.66/s)  LR: 4.455e-04  Data: 0.010 (0.013)
Train: 323 [ 650/1251 ( 52%)]  Loss: 3.566 (3.41)  Time: 0.777s, 1317.15/s  (0.783s, 1308.59/s)  LR: 4.455e-04  Data: 0.010 (0.013)
Train: 323 [ 700/1251 ( 56%)]  Loss: 3.511 (3.42)  Time: 0.771s, 1328.44/s  (0.782s, 1309.09/s)  LR: 4.455e-04  Data: 0.009 (0.012)
Train: 323 [ 750/1251 ( 60%)]  Loss: 3.244 (3.41)  Time: 0.779s, 1315.31/s  (0.782s, 1309.38/s)  LR: 4.455e-04  Data: 0.010 (0.012)
Train: 323 [ 800/1251 ( 64%)]  Loss: 3.672 (3.42)  Time: 0.775s, 1320.49/s  (0.782s, 1309.20/s)  LR: 4.455e-04  Data: 0.010 (0.012)
Train: 323 [ 850/1251 ( 68%)]  Loss: 3.359 (3.42)  Time: 0.776s, 1320.02/s  (0.782s, 1309.40/s)  LR: 4.455e-04  Data: 0.009 (0.012)
Train: 323 [ 900/1251 ( 72%)]  Loss: 3.150 (3.41)  Time: 0.780s, 1312.35/s  (0.782s, 1309.19/s)  LR: 4.455e-04  Data: 0.009 (0.012)
Train: 323 [ 950/1251 ( 76%)]  Loss: 3.619 (3.42)  Time: 0.773s, 1325.29/s  (0.782s, 1309.14/s)  LR: 4.455e-04  Data: 0.010 (0.012)
Train: 323 [1000/1251 ( 80%)]  Loss: 3.202 (3.41)  Time: 0.777s, 1318.18/s  (0.782s, 1309.62/s)  LR: 4.455e-04  Data: 0.010 (0.012)
Train: 323 [1050/1251 ( 84%)]  Loss: 3.358 (3.40)  Time: 0.771s, 1328.18/s  (0.782s, 1310.18/s)  LR: 4.455e-04  Data: 0.010 (0.012)
Train: 323 [1100/1251 ( 88%)]  Loss: 3.480 (3.41)  Time: 0.776s, 1319.16/s  (0.781s, 1310.35/s)  LR: 4.455e-04  Data: 0.010 (0.011)
Train: 323 [1150/1251 ( 92%)]  Loss: 3.505 (3.41)  Time: 0.785s, 1304.41/s  (0.781s, 1310.65/s)  LR: 4.455e-04  Data: 0.010 (0.011)
Train: 323 [1200/1251 ( 96%)]  Loss: 3.296 (3.41)  Time: 0.800s, 1280.79/s  (0.781s, 1310.74/s)  LR: 4.455e-04  Data: 0.009 (0.011)
Train: 323 [1250/1251 (100%)]  Loss: 3.344 (3.40)  Time: 0.760s, 1348.02/s  (0.781s, 1311.05/s)  LR: 4.455e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.540 (1.540)  Loss:  0.7544 (0.7544)  Acc@1: 89.6484 (89.6484)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.194 (0.559)  Loss:  0.8418 (1.2157)  Acc@1: 85.2594 (76.4280)  Acc@5: 96.2264 (93.4380)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-323.pth.tar', 76.42800006103515)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-322.pth.tar', 76.25199998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-317.pth.tar', 76.2340001928711)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-320.pth.tar', 76.22200010742188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-321.pth.tar', 76.02399997802735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-315.pth.tar', 75.99400000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-300.pth.tar', 75.96000008544922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-318.pth.tar', 75.9340000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-296.pth.tar', 75.85800001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-304.pth.tar', 75.84200010742188)

Train: 324 [   0/1251 (  0%)]  Loss: 3.408 (3.41)  Time: 2.196s,  466.28/s  (2.196s,  466.28/s)  LR: 4.430e-04  Data: 1.471 (1.471)
Train: 324 [  50/1251 (  4%)]  Loss: 3.181 (3.29)  Time: 0.775s, 1321.52/s  (0.819s, 1250.03/s)  LR: 4.430e-04  Data: 0.011 (0.043)
Train: 324 [ 100/1251 (  8%)]  Loss: 3.460 (3.35)  Time: 0.773s, 1324.10/s  (0.799s, 1281.76/s)  LR: 4.430e-04  Data: 0.009 (0.027)
Train: 324 [ 150/1251 ( 12%)]  Loss: 3.475 (3.38)  Time: 0.772s, 1325.73/s  (0.791s, 1294.60/s)  LR: 4.430e-04  Data: 0.010 (0.021)
Train: 324 [ 200/1251 ( 16%)]  Loss: 3.365 (3.38)  Time: 0.778s, 1316.92/s  (0.790s, 1296.36/s)  LR: 4.430e-04  Data: 0.010 (0.018)
Train: 324 [ 250/1251 ( 20%)]  Loss: 3.384 (3.38)  Time: 0.773s, 1324.36/s  (0.787s, 1300.36/s)  LR: 4.430e-04  Data: 0.010 (0.017)
Train: 324 [ 300/1251 ( 24%)]  Loss: 2.901 (3.31)  Time: 0.783s, 1307.20/s  (0.786s, 1302.03/s)  LR: 4.430e-04  Data: 0.012 (0.016)
Train: 324 [ 350/1251 ( 28%)]  Loss: 3.289 (3.31)  Time: 0.771s, 1327.80/s  (0.786s, 1302.95/s)  LR: 4.430e-04  Data: 0.009 (0.015)
Train: 324 [ 400/1251 ( 32%)]  Loss: 3.008 (3.27)  Time: 0.773s, 1324.96/s  (0.786s, 1303.53/s)  LR: 4.430e-04  Data: 0.009 (0.014)
Train: 324 [ 450/1251 ( 36%)]  Loss: 3.393 (3.29)  Time: 0.775s, 1322.14/s  (0.785s, 1303.69/s)  LR: 4.430e-04  Data: 0.010 (0.014)
Train: 324 [ 500/1251 ( 40%)]  Loss: 3.135 (3.27)  Time: 0.772s, 1326.91/s  (0.785s, 1304.05/s)  LR: 4.430e-04  Data: 0.010 (0.013)
Train: 324 [ 550/1251 ( 44%)]  Loss: 3.570 (3.30)  Time: 0.774s, 1322.28/s  (0.785s, 1304.12/s)  LR: 4.430e-04  Data: 0.010 (0.013)
Train: 324 [ 600/1251 ( 48%)]  Loss: 3.431 (3.31)  Time: 0.774s, 1323.00/s  (0.784s, 1305.39/s)  LR: 4.430e-04  Data: 0.009 (0.013)
Train: 324 [ 650/1251 ( 52%)]  Loss: 3.378 (3.31)  Time: 0.774s, 1323.27/s  (0.784s, 1305.76/s)  LR: 4.430e-04  Data: 0.011 (0.012)
Train: 324 [ 700/1251 ( 56%)]  Loss: 3.301 (3.31)  Time: 0.776s, 1319.79/s  (0.785s, 1304.26/s)  LR: 4.430e-04  Data: 0.009 (0.012)
Train: 324 [ 750/1251 ( 60%)]  Loss: 3.712 (3.34)  Time: 0.773s, 1324.41/s  (0.785s, 1304.98/s)  LR: 4.430e-04  Data: 0.009 (0.012)
Train: 324 [ 800/1251 ( 64%)]  Loss: 3.178 (3.33)  Time: 0.774s, 1323.28/s  (0.785s, 1304.51/s)  LR: 4.430e-04  Data: 0.010 (0.012)
Train: 324 [ 850/1251 ( 68%)]  Loss: 3.758 (3.35)  Time: 0.775s, 1320.91/s  (0.785s, 1304.06/s)  LR: 4.430e-04  Data: 0.010 (0.012)
Train: 324 [ 900/1251 ( 72%)]  Loss: 3.414 (3.35)  Time: 0.773s, 1325.33/s  (0.785s, 1304.73/s)  LR: 4.430e-04  Data: 0.010 (0.012)
Train: 324 [ 950/1251 ( 76%)]  Loss: 3.370 (3.36)  Time: 0.810s, 1263.94/s  (0.786s, 1303.04/s)  LR: 4.430e-04  Data: 0.009 (0.012)
Train: 324 [1000/1251 ( 80%)]  Loss: 3.835 (3.38)  Time: 0.814s, 1258.19/s  (0.787s, 1300.57/s)  LR: 4.430e-04  Data: 0.009 (0.011)
Train: 324 [1050/1251 ( 84%)]  Loss: 3.301 (3.37)  Time: 0.772s, 1326.97/s  (0.788s, 1300.09/s)  LR: 4.430e-04  Data: 0.008 (0.011)
Train: 324 [1100/1251 ( 88%)]  Loss: 3.289 (3.37)  Time: 0.781s, 1311.61/s  (0.788s, 1299.33/s)  LR: 4.430e-04  Data: 0.010 (0.011)
Train: 324 [1150/1251 ( 92%)]  Loss: 3.202 (3.36)  Time: 0.870s, 1176.94/s  (0.788s, 1298.91/s)  LR: 4.430e-04  Data: 0.008 (0.011)
Train: 324 [1200/1251 ( 96%)]  Loss: 3.177 (3.36)  Time: 0.776s, 1319.23/s  (0.788s, 1299.44/s)  LR: 4.430e-04  Data: 0.009 (0.011)
Train: 324 [1250/1251 (100%)]  Loss: 3.512 (3.36)  Time: 0.764s, 1339.64/s  (0.788s, 1298.80/s)  LR: 4.430e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.544 (1.544)  Loss:  0.7632 (0.7632)  Acc@1: 90.0391 (90.0391)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.194 (0.558)  Loss:  0.8130 (1.2564)  Acc@1: 86.6745 (76.0980)  Acc@5: 97.4057 (93.3360)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-323.pth.tar', 76.42800006103515)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-322.pth.tar', 76.25199998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-317.pth.tar', 76.2340001928711)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-320.pth.tar', 76.22200010742188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-324.pth.tar', 76.09800002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-321.pth.tar', 76.02399997802735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-315.pth.tar', 75.99400000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-300.pth.tar', 75.96000008544922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-318.pth.tar', 75.9340000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-296.pth.tar', 75.85800001220703)

Train: 325 [   0/1251 (  0%)]  Loss: 3.301 (3.30)  Time: 2.374s,  431.42/s  (2.374s,  431.42/s)  LR: 4.404e-04  Data: 1.638 (1.638)
Train: 325 [  50/1251 (  4%)]  Loss: 3.298 (3.30)  Time: 0.773s, 1325.20/s  (0.817s, 1253.95/s)  LR: 4.404e-04  Data: 0.009 (0.048)
Train: 325 [ 100/1251 (  8%)]  Loss: 3.418 (3.34)  Time: 0.773s, 1324.16/s  (0.798s, 1282.57/s)  LR: 4.404e-04  Data: 0.009 (0.029)
Train: 325 [ 150/1251 ( 12%)]  Loss: 3.618 (3.41)  Time: 0.807s, 1268.23/s  (0.798s, 1282.53/s)  LR: 4.404e-04  Data: 0.010 (0.023)
Train: 325 [ 200/1251 ( 16%)]  Loss: 3.632 (3.45)  Time: 0.775s, 1321.57/s  (0.800s, 1280.06/s)  LR: 4.404e-04  Data: 0.010 (0.020)
Train: 325 [ 250/1251 ( 20%)]  Loss: 3.427 (3.45)  Time: 0.777s, 1317.62/s  (0.796s, 1286.93/s)  LR: 4.404e-04  Data: 0.010 (0.018)
Train: 325 [ 300/1251 ( 24%)]  Loss: 3.590 (3.47)  Time: 0.773s, 1325.32/s  (0.793s, 1291.51/s)  LR: 4.404e-04  Data: 0.010 (0.016)
Train: 325 [ 350/1251 ( 28%)]  Loss: 3.158 (3.43)  Time: 0.776s, 1320.03/s  (0.791s, 1294.19/s)  LR: 4.404e-04  Data: 0.009 (0.015)
Train: 325 [ 400/1251 ( 32%)]  Loss: 3.495 (3.44)  Time: 0.772s, 1325.90/s  (0.790s, 1296.63/s)  LR: 4.404e-04  Data: 0.009 (0.015)
Train: 325 [ 450/1251 ( 36%)]  Loss: 3.156 (3.41)  Time: 0.772s, 1325.72/s  (0.789s, 1298.20/s)  LR: 4.404e-04  Data: 0.009 (0.014)
Train: 325 [ 500/1251 ( 40%)]  Loss: 3.633 (3.43)  Time: 0.792s, 1293.29/s  (0.788s, 1299.09/s)  LR: 4.404e-04  Data: 0.009 (0.014)
Train: 325 [ 550/1251 ( 44%)]  Loss: 3.162 (3.41)  Time: 0.774s, 1322.55/s  (0.788s, 1299.89/s)  LR: 4.404e-04  Data: 0.010 (0.013)
Train: 325 [ 600/1251 ( 48%)]  Loss: 3.464 (3.41)  Time: 0.772s, 1326.54/s  (0.788s, 1299.28/s)  LR: 4.404e-04  Data: 0.010 (0.013)
Train: 325 [ 650/1251 ( 52%)]  Loss: 3.346 (3.41)  Time: 0.773s, 1325.19/s  (0.787s, 1300.91/s)  LR: 4.404e-04  Data: 0.010 (0.013)
Train: 325 [ 700/1251 ( 56%)]  Loss: 3.152 (3.39)  Time: 0.775s, 1321.25/s  (0.787s, 1301.61/s)  LR: 4.404e-04  Data: 0.010 (0.013)
Train: 325 [ 750/1251 ( 60%)]  Loss: 3.494 (3.40)  Time: 0.775s, 1321.16/s  (0.786s, 1302.50/s)  LR: 4.404e-04  Data: 0.010 (0.012)
Train: 325 [ 800/1251 ( 64%)]  Loss: 3.251 (3.39)  Time: 0.773s, 1324.69/s  (0.786s, 1303.52/s)  LR: 4.404e-04  Data: 0.009 (0.012)
Train: 325 [ 850/1251 ( 68%)]  Loss: 2.995 (3.37)  Time: 0.820s, 1248.54/s  (0.785s, 1303.96/s)  LR: 4.404e-04  Data: 0.009 (0.012)
Train: 325 [ 900/1251 ( 72%)]  Loss: 3.354 (3.37)  Time: 0.875s, 1169.86/s  (0.785s, 1304.34/s)  LR: 4.404e-04  Data: 0.010 (0.012)
Train: 325 [ 950/1251 ( 76%)]  Loss: 3.274 (3.36)  Time: 0.837s, 1223.10/s  (0.785s, 1304.25/s)  LR: 4.404e-04  Data: 0.010 (0.012)
Train: 325 [1000/1251 ( 80%)]  Loss: 3.373 (3.36)  Time: 0.773s, 1324.86/s  (0.785s, 1304.73/s)  LR: 4.404e-04  Data: 0.010 (0.012)
Train: 325 [1050/1251 ( 84%)]  Loss: 3.534 (3.37)  Time: 0.772s, 1326.69/s  (0.784s, 1305.46/s)  LR: 4.404e-04  Data: 0.010 (0.012)
Train: 325 [1100/1251 ( 88%)]  Loss: 3.430 (3.37)  Time: 0.772s, 1325.62/s  (0.784s, 1305.51/s)  LR: 4.404e-04  Data: 0.010 (0.012)
Train: 325 [1150/1251 ( 92%)]  Loss: 3.593 (3.38)  Time: 0.778s, 1315.44/s  (0.784s, 1305.80/s)  LR: 4.404e-04  Data: 0.010 (0.012)
Train: 325 [1200/1251 ( 96%)]  Loss: 3.466 (3.38)  Time: 0.774s, 1322.45/s  (0.785s, 1304.89/s)  LR: 4.404e-04  Data: 0.009 (0.011)
Train: 325 [1250/1251 (100%)]  Loss: 3.774 (3.40)  Time: 0.762s, 1344.57/s  (0.785s, 1304.78/s)  LR: 4.404e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.586 (1.586)  Loss:  0.7461 (0.7461)  Acc@1: 90.9180 (90.9180)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.194 (0.570)  Loss:  0.8159 (1.2548)  Acc@1: 85.6132 (76.1240)  Acc@5: 96.2264 (93.2520)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-323.pth.tar', 76.42800006103515)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-322.pth.tar', 76.25199998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-317.pth.tar', 76.2340001928711)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-320.pth.tar', 76.22200010742188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-325.pth.tar', 76.12399995605469)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-324.pth.tar', 76.09800002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-321.pth.tar', 76.02399997802735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-315.pth.tar', 75.99400000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-300.pth.tar', 75.96000008544922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-318.pth.tar', 75.9340000366211)

Train: 326 [   0/1251 (  0%)]  Loss: 3.250 (3.25)  Time: 2.470s,  414.51/s  (2.470s,  414.51/s)  LR: 4.378e-04  Data: 1.744 (1.744)
Train: 326 [  50/1251 (  4%)]  Loss: 3.341 (3.30)  Time: 0.771s, 1327.50/s  (0.818s, 1252.04/s)  LR: 4.378e-04  Data: 0.010 (0.050)
Train: 326 [ 100/1251 (  8%)]  Loss: 3.352 (3.31)  Time: 0.790s, 1296.69/s  (0.799s, 1281.37/s)  LR: 4.378e-04  Data: 0.010 (0.030)
Train: 326 [ 150/1251 ( 12%)]  Loss: 3.455 (3.35)  Time: 0.784s, 1306.28/s  (0.793s, 1291.88/s)  LR: 4.378e-04  Data: 0.009 (0.023)
Train: 326 [ 200/1251 ( 16%)]  Loss: 3.667 (3.41)  Time: 0.782s, 1309.92/s  (0.789s, 1297.45/s)  LR: 4.378e-04  Data: 0.010 (0.020)
Train: 326 [ 250/1251 ( 20%)]  Loss: 3.560 (3.44)  Time: 0.827s, 1238.76/s  (0.796s, 1286.11/s)  LR: 4.378e-04  Data: 0.013 (0.018)
Train: 326 [ 300/1251 ( 24%)]  Loss: 3.486 (3.44)  Time: 0.774s, 1322.36/s  (0.795s, 1288.07/s)  LR: 4.378e-04  Data: 0.010 (0.017)
Train: 326 [ 350/1251 ( 28%)]  Loss: 3.205 (3.41)  Time: 0.776s, 1318.90/s  (0.793s, 1291.23/s)  LR: 4.378e-04  Data: 0.012 (0.016)
Train: 326 [ 400/1251 ( 32%)]  Loss: 3.342 (3.41)  Time: 0.784s, 1306.65/s  (0.791s, 1294.07/s)  LR: 4.378e-04  Data: 0.010 (0.015)
Train: 326 [ 450/1251 ( 36%)]  Loss: 3.458 (3.41)  Time: 0.812s, 1260.46/s  (0.794s, 1289.20/s)  LR: 4.378e-04  Data: 0.009 (0.015)
Train: 326 [ 500/1251 ( 40%)]  Loss: 3.397 (3.41)  Time: 0.772s, 1326.97/s  (0.793s, 1291.64/s)  LR: 4.378e-04  Data: 0.010 (0.014)
Train: 326 [ 550/1251 ( 44%)]  Loss: 3.540 (3.42)  Time: 0.837s, 1223.01/s  (0.792s, 1293.33/s)  LR: 4.378e-04  Data: 0.010 (0.014)
Train: 326 [ 600/1251 ( 48%)]  Loss: 3.051 (3.39)  Time: 0.776s, 1319.49/s  (0.791s, 1295.17/s)  LR: 4.378e-04  Data: 0.010 (0.013)
Train: 326 [ 650/1251 ( 52%)]  Loss: 3.159 (3.38)  Time: 0.775s, 1320.60/s  (0.790s, 1296.34/s)  LR: 4.378e-04  Data: 0.011 (0.013)
Train: 326 [ 700/1251 ( 56%)]  Loss: 3.474 (3.38)  Time: 0.806s, 1270.11/s  (0.791s, 1294.17/s)  LR: 4.378e-04  Data: 0.010 (0.013)
Train: 326 [ 750/1251 ( 60%)]  Loss: 3.408 (3.38)  Time: 0.773s, 1324.56/s  (0.790s, 1295.45/s)  LR: 4.378e-04  Data: 0.009 (0.013)
Train: 326 [ 800/1251 ( 64%)]  Loss: 3.212 (3.37)  Time: 0.774s, 1323.50/s  (0.791s, 1295.25/s)  LR: 4.378e-04  Data: 0.010 (0.013)
Train: 326 [ 850/1251 ( 68%)]  Loss: 3.227 (3.37)  Time: 0.778s, 1316.84/s  (0.790s, 1296.22/s)  LR: 4.378e-04  Data: 0.010 (0.012)
Train: 326 [ 900/1251 ( 72%)]  Loss: 3.372 (3.37)  Time: 0.774s, 1323.61/s  (0.789s, 1297.71/s)  LR: 4.378e-04  Data: 0.010 (0.012)
Train: 326 [ 950/1251 ( 76%)]  Loss: 3.539 (3.37)  Time: 0.794s, 1290.10/s  (0.789s, 1298.32/s)  LR: 4.378e-04  Data: 0.010 (0.012)
Train: 326 [1000/1251 ( 80%)]  Loss: 3.180 (3.37)  Time: 0.775s, 1322.11/s  (0.789s, 1298.57/s)  LR: 4.378e-04  Data: 0.010 (0.012)
Train: 326 [1050/1251 ( 84%)]  Loss: 3.439 (3.37)  Time: 0.772s, 1325.59/s  (0.788s, 1299.09/s)  LR: 4.378e-04  Data: 0.011 (0.012)
Train: 326 [1100/1251 ( 88%)]  Loss: 3.339 (3.37)  Time: 0.777s, 1317.86/s  (0.788s, 1299.70/s)  LR: 4.378e-04  Data: 0.010 (0.012)
Train: 326 [1150/1251 ( 92%)]  Loss: 3.729 (3.38)  Time: 0.829s, 1235.44/s  (0.789s, 1297.99/s)  LR: 4.378e-04  Data: 0.011 (0.012)
Train: 326 [1200/1251 ( 96%)]  Loss: 3.684 (3.39)  Time: 0.789s, 1298.43/s  (0.789s, 1298.51/s)  LR: 4.378e-04  Data: 0.010 (0.012)
Train: 326 [1250/1251 (100%)]  Loss: 3.213 (3.39)  Time: 0.763s, 1341.39/s  (0.788s, 1299.04/s)  LR: 4.378e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.562 (1.562)  Loss:  0.7158 (0.7158)  Acc@1: 90.3320 (90.3320)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.194 (0.579)  Loss:  0.7275 (1.1923)  Acc@1: 85.9670 (76.1860)  Acc@5: 97.2877 (93.4220)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-323.pth.tar', 76.42800006103515)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-322.pth.tar', 76.25199998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-317.pth.tar', 76.2340001928711)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-320.pth.tar', 76.22200010742188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-326.pth.tar', 76.18599998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-325.pth.tar', 76.12399995605469)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-324.pth.tar', 76.09800002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-321.pth.tar', 76.02399997802735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-315.pth.tar', 75.99400000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-300.pth.tar', 75.96000008544922)

Train: 327 [   0/1251 (  0%)]  Loss: 3.225 (3.23)  Time: 2.203s,  464.88/s  (2.203s,  464.88/s)  LR: 4.353e-04  Data: 1.467 (1.467)
Train: 327 [  50/1251 (  4%)]  Loss: 3.263 (3.24)  Time: 0.772s, 1325.82/s  (0.813s, 1259.32/s)  LR: 4.353e-04  Data: 0.009 (0.043)
Train: 327 [ 100/1251 (  8%)]  Loss: 3.560 (3.35)  Time: 0.771s, 1328.59/s  (0.798s, 1283.59/s)  LR: 4.353e-04  Data: 0.010 (0.027)
Train: 327 [ 150/1251 ( 12%)]  Loss: 3.352 (3.35)  Time: 0.773s, 1324.28/s  (0.792s, 1292.93/s)  LR: 4.353e-04  Data: 0.009 (0.021)
Train: 327 [ 200/1251 ( 16%)]  Loss: 3.000 (3.28)  Time: 0.787s, 1301.95/s  (0.789s, 1298.32/s)  LR: 4.353e-04  Data: 0.011 (0.018)
Train: 327 [ 250/1251 ( 20%)]  Loss: 3.302 (3.28)  Time: 0.773s, 1324.61/s  (0.786s, 1302.24/s)  LR: 4.353e-04  Data: 0.009 (0.017)
Train: 327 [ 300/1251 ( 24%)]  Loss: 3.318 (3.29)  Time: 0.810s, 1264.27/s  (0.785s, 1304.31/s)  LR: 4.353e-04  Data: 0.009 (0.015)
Train: 327 [ 350/1251 ( 28%)]  Loss: 3.615 (3.33)  Time: 0.810s, 1264.76/s  (0.789s, 1298.53/s)  LR: 4.353e-04  Data: 0.010 (0.015)
Train: 327 [ 400/1251 ( 32%)]  Loss: 3.486 (3.35)  Time: 0.775s, 1321.34/s  (0.787s, 1300.47/s)  LR: 4.353e-04  Data: 0.013 (0.014)
Train: 327 [ 450/1251 ( 36%)]  Loss: 3.416 (3.35)  Time: 0.885s, 1157.48/s  (0.787s, 1301.22/s)  LR: 4.353e-04  Data: 0.014 (0.014)
Train: 327 [ 500/1251 ( 40%)]  Loss: 3.364 (3.35)  Time: 0.815s, 1255.99/s  (0.790s, 1295.60/s)  LR: 4.353e-04  Data: 0.011 (0.013)
Train: 327 [ 550/1251 ( 44%)]  Loss: 3.277 (3.35)  Time: 0.790s, 1296.35/s  (0.790s, 1295.62/s)  LR: 4.353e-04  Data: 0.010 (0.013)
Train: 327 [ 600/1251 ( 48%)]  Loss: 3.281 (3.34)  Time: 0.775s, 1321.27/s  (0.789s, 1297.65/s)  LR: 4.353e-04  Data: 0.010 (0.013)
Train: 327 [ 650/1251 ( 52%)]  Loss: 3.246 (3.34)  Time: 0.835s, 1226.82/s  (0.789s, 1297.67/s)  LR: 4.353e-04  Data: 0.011 (0.013)
Train: 327 [ 700/1251 ( 56%)]  Loss: 3.361 (3.34)  Time: 0.777s, 1317.49/s  (0.788s, 1299.07/s)  LR: 4.353e-04  Data: 0.012 (0.012)
Train: 327 [ 750/1251 ( 60%)]  Loss: 3.484 (3.35)  Time: 0.807s, 1269.10/s  (0.789s, 1298.37/s)  LR: 4.353e-04  Data: 0.012 (0.012)
Train: 327 [ 800/1251 ( 64%)]  Loss: 3.566 (3.36)  Time: 0.806s, 1270.96/s  (0.790s, 1295.74/s)  LR: 4.353e-04  Data: 0.013 (0.012)
Train: 327 [ 850/1251 ( 68%)]  Loss: 3.446 (3.36)  Time: 0.775s, 1320.71/s  (0.790s, 1296.69/s)  LR: 4.353e-04  Data: 0.010 (0.012)
Train: 327 [ 900/1251 ( 72%)]  Loss: 3.988 (3.40)  Time: 0.792s, 1293.28/s  (0.790s, 1296.99/s)  LR: 4.353e-04  Data: 0.009 (0.012)
Train: 327 [ 950/1251 ( 76%)]  Loss: 3.308 (3.39)  Time: 0.780s, 1312.83/s  (0.790s, 1296.87/s)  LR: 4.353e-04  Data: 0.010 (0.012)
Train: 327 [1000/1251 ( 80%)]  Loss: 3.273 (3.39)  Time: 0.770s, 1330.64/s  (0.789s, 1297.81/s)  LR: 4.353e-04  Data: 0.010 (0.012)
Train: 327 [1050/1251 ( 84%)]  Loss: 3.437 (3.39)  Time: 0.790s, 1296.30/s  (0.789s, 1297.45/s)  LR: 4.353e-04  Data: 0.009 (0.012)
Train: 327 [1100/1251 ( 88%)]  Loss: 3.510 (3.39)  Time: 0.816s, 1254.42/s  (0.789s, 1297.67/s)  LR: 4.353e-04  Data: 0.010 (0.012)
Train: 327 [1150/1251 ( 92%)]  Loss: 3.643 (3.41)  Time: 0.773s, 1325.36/s  (0.789s, 1297.23/s)  LR: 4.353e-04  Data: 0.010 (0.011)
Train: 327 [1200/1251 ( 96%)]  Loss: 3.456 (3.41)  Time: 0.772s, 1326.89/s  (0.789s, 1297.85/s)  LR: 4.353e-04  Data: 0.010 (0.011)
Train: 327 [1250/1251 (100%)]  Loss: 3.454 (3.41)  Time: 0.763s, 1341.19/s  (0.789s, 1298.55/s)  LR: 4.353e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.534 (1.534)  Loss:  0.6729 (0.6729)  Acc@1: 90.4297 (90.4297)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.194 (0.561)  Loss:  0.7402 (1.1748)  Acc@1: 86.4387 (76.2740)  Acc@5: 96.9340 (93.2140)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-323.pth.tar', 76.42800006103515)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-327.pth.tar', 76.27400005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-322.pth.tar', 76.25199998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-317.pth.tar', 76.2340001928711)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-320.pth.tar', 76.22200010742188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-326.pth.tar', 76.18599998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-325.pth.tar', 76.12399995605469)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-324.pth.tar', 76.09800002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-321.pth.tar', 76.02399997802735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-315.pth.tar', 75.99400000732422)

Train: 328 [   0/1251 (  0%)]  Loss: 3.622 (3.62)  Time: 2.317s,  441.94/s  (2.317s,  441.94/s)  LR: 4.327e-04  Data: 1.582 (1.582)
Train: 328 [  50/1251 (  4%)]  Loss: 3.683 (3.65)  Time: 0.772s, 1326.51/s  (0.817s, 1253.14/s)  LR: 4.327e-04  Data: 0.010 (0.043)
Train: 328 [ 100/1251 (  8%)]  Loss: 3.169 (3.49)  Time: 0.813s, 1259.21/s  (0.810s, 1264.25/s)  LR: 4.327e-04  Data: 0.009 (0.027)
Train: 328 [ 150/1251 ( 12%)]  Loss: 3.355 (3.46)  Time: 0.783s, 1308.00/s  (0.801s, 1277.84/s)  LR: 4.327e-04  Data: 0.010 (0.021)
Train: 328 [ 200/1251 ( 16%)]  Loss: 3.219 (3.41)  Time: 0.774s, 1322.81/s  (0.796s, 1286.23/s)  LR: 4.327e-04  Data: 0.010 (0.018)
Train: 328 [ 250/1251 ( 20%)]  Loss: 3.214 (3.38)  Time: 0.776s, 1319.87/s  (0.793s, 1291.43/s)  LR: 4.327e-04  Data: 0.010 (0.017)
Train: 328 [ 300/1251 ( 24%)]  Loss: 3.111 (3.34)  Time: 0.775s, 1321.60/s  (0.792s, 1292.77/s)  LR: 4.327e-04  Data: 0.010 (0.015)
Train: 328 [ 350/1251 ( 28%)]  Loss: 3.651 (3.38)  Time: 0.773s, 1324.64/s  (0.791s, 1295.04/s)  LR: 4.327e-04  Data: 0.009 (0.015)
Train: 328 [ 400/1251 ( 32%)]  Loss: 3.628 (3.41)  Time: 0.781s, 1311.39/s  (0.790s, 1295.83/s)  LR: 4.327e-04  Data: 0.010 (0.014)
Train: 328 [ 450/1251 ( 36%)]  Loss: 3.090 (3.37)  Time: 0.772s, 1326.28/s  (0.790s, 1295.96/s)  LR: 4.327e-04  Data: 0.009 (0.014)
Train: 328 [ 500/1251 ( 40%)]  Loss: 3.093 (3.35)  Time: 0.778s, 1315.49/s  (0.789s, 1298.05/s)  LR: 4.327e-04  Data: 0.011 (0.013)
Train: 328 [ 550/1251 ( 44%)]  Loss: 3.396 (3.35)  Time: 0.826s, 1239.84/s  (0.792s, 1293.73/s)  LR: 4.327e-04  Data: 0.013 (0.013)
Train: 328 [ 600/1251 ( 48%)]  Loss: 3.400 (3.36)  Time: 0.817s, 1253.86/s  (0.794s, 1289.61/s)  LR: 4.327e-04  Data: 0.011 (0.013)
Train: 328 [ 650/1251 ( 52%)]  Loss: 3.550 (3.37)  Time: 0.785s, 1304.08/s  (0.795s, 1287.74/s)  LR: 4.327e-04  Data: 0.010 (0.013)
Train: 328 [ 700/1251 ( 56%)]  Loss: 3.283 (3.36)  Time: 0.772s, 1326.50/s  (0.794s, 1289.20/s)  LR: 4.327e-04  Data: 0.010 (0.013)
Train: 328 [ 750/1251 ( 60%)]  Loss: 3.837 (3.39)  Time: 0.773s, 1325.26/s  (0.793s, 1290.85/s)  LR: 4.327e-04  Data: 0.009 (0.012)
Train: 328 [ 800/1251 ( 64%)]  Loss: 3.645 (3.41)  Time: 0.773s, 1325.04/s  (0.792s, 1292.68/s)  LR: 4.327e-04  Data: 0.010 (0.012)
Train: 328 [ 850/1251 ( 68%)]  Loss: 3.379 (3.41)  Time: 0.775s, 1321.09/s  (0.791s, 1294.17/s)  LR: 4.327e-04  Data: 0.010 (0.012)
Train: 328 [ 900/1251 ( 72%)]  Loss: 3.133 (3.39)  Time: 0.773s, 1324.86/s  (0.791s, 1295.03/s)  LR: 4.327e-04  Data: 0.009 (0.012)
Train: 328 [ 950/1251 ( 76%)]  Loss: 3.726 (3.41)  Time: 0.775s, 1321.02/s  (0.790s, 1295.46/s)  LR: 4.327e-04  Data: 0.010 (0.012)
Train: 328 [1000/1251 ( 80%)]  Loss: 3.245 (3.40)  Time: 0.786s, 1303.04/s  (0.790s, 1295.72/s)  LR: 4.327e-04  Data: 0.010 (0.012)
Train: 328 [1050/1251 ( 84%)]  Loss: 3.399 (3.40)  Time: 0.775s, 1321.12/s  (0.790s, 1296.40/s)  LR: 4.327e-04  Data: 0.009 (0.012)
Train: 328 [1100/1251 ( 88%)]  Loss: 3.266 (3.40)  Time: 0.777s, 1318.66/s  (0.791s, 1294.84/s)  LR: 4.327e-04  Data: 0.011 (0.012)
Train: 328 [1150/1251 ( 92%)]  Loss: 3.195 (3.39)  Time: 0.779s, 1314.12/s  (0.790s, 1295.75/s)  LR: 4.327e-04  Data: 0.009 (0.012)
Train: 328 [1200/1251 ( 96%)]  Loss: 3.638 (3.40)  Time: 0.774s, 1322.44/s  (0.790s, 1295.60/s)  LR: 4.327e-04  Data: 0.010 (0.011)
Train: 328 [1250/1251 (100%)]  Loss: 3.394 (3.40)  Time: 0.761s, 1345.75/s  (0.790s, 1296.45/s)  LR: 4.327e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.558 (1.558)  Loss:  0.7847 (0.7847)  Acc@1: 90.0391 (90.0391)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.194 (0.557)  Loss:  0.8804 (1.2654)  Acc@1: 84.5519 (75.7880)  Acc@5: 96.2264 (93.3000)
Train: 329 [   0/1251 (  0%)]  Loss: 3.334 (3.33)  Time: 2.250s,  455.18/s  (2.250s,  455.18/s)  LR: 4.301e-04  Data: 1.517 (1.517)
Train: 329 [  50/1251 (  4%)]  Loss: 3.835 (3.58)  Time: 0.776s, 1319.13/s  (0.810s, 1264.13/s)  LR: 4.301e-04  Data: 0.010 (0.043)
Train: 329 [ 100/1251 (  8%)]  Loss: 3.073 (3.41)  Time: 0.816s, 1255.56/s  (0.803s, 1275.90/s)  LR: 4.301e-04  Data: 0.011 (0.027)
Train: 329 [ 150/1251 ( 12%)]  Loss: 3.367 (3.40)  Time: 0.772s, 1325.65/s  (0.799s, 1282.06/s)  LR: 4.301e-04  Data: 0.010 (0.021)
Train: 329 [ 200/1251 ( 16%)]  Loss: 3.111 (3.34)  Time: 0.814s, 1257.37/s  (0.799s, 1281.15/s)  LR: 4.301e-04  Data: 0.011 (0.019)
Train: 329 [ 250/1251 ( 20%)]  Loss: 3.473 (3.37)  Time: 0.811s, 1262.48/s  (0.798s, 1283.31/s)  LR: 4.301e-04  Data: 0.010 (0.017)
Train: 329 [ 300/1251 ( 24%)]  Loss: 2.928 (3.30)  Time: 0.773s, 1324.42/s  (0.796s, 1286.05/s)  LR: 4.301e-04  Data: 0.009 (0.016)
Train: 329 [ 350/1251 ( 28%)]  Loss: 3.405 (3.32)  Time: 0.776s, 1319.44/s  (0.793s, 1290.57/s)  LR: 4.301e-04  Data: 0.010 (0.015)
Train: 329 [ 400/1251 ( 32%)]  Loss: 3.309 (3.31)  Time: 0.773s, 1324.20/s  (0.792s, 1293.05/s)  LR: 4.301e-04  Data: 0.009 (0.014)
Train: 329 [ 450/1251 ( 36%)]  Loss: 3.255 (3.31)  Time: 0.777s, 1318.23/s  (0.791s, 1295.20/s)  LR: 4.301e-04  Data: 0.010 (0.014)
Train: 329 [ 500/1251 ( 40%)]  Loss: 3.744 (3.35)  Time: 0.780s, 1312.24/s  (0.790s, 1295.98/s)  LR: 4.301e-04  Data: 0.011 (0.013)
Train: 329 [ 550/1251 ( 44%)]  Loss: 3.118 (3.33)  Time: 0.774s, 1323.63/s  (0.790s, 1296.48/s)  LR: 4.301e-04  Data: 0.010 (0.013)
Train: 329 [ 600/1251 ( 48%)]  Loss: 3.668 (3.36)  Time: 0.771s, 1329.00/s  (0.789s, 1298.37/s)  LR: 4.301e-04  Data: 0.009 (0.013)
Train: 329 [ 650/1251 ( 52%)]  Loss: 3.572 (3.37)  Time: 0.772s, 1325.99/s  (0.788s, 1299.68/s)  LR: 4.301e-04  Data: 0.010 (0.013)
Train: 329 [ 700/1251 ( 56%)]  Loss: 3.664 (3.39)  Time: 0.787s, 1300.45/s  (0.788s, 1299.46/s)  LR: 4.301e-04  Data: 0.012 (0.012)
Train: 329 [ 750/1251 ( 60%)]  Loss: 3.684 (3.41)  Time: 0.807s, 1268.85/s  (0.790s, 1296.40/s)  LR: 4.301e-04  Data: 0.009 (0.012)
Train: 329 [ 800/1251 ( 64%)]  Loss: 3.648 (3.42)  Time: 0.849s, 1206.48/s  (0.791s, 1293.94/s)  LR: 4.301e-04  Data: 0.009 (0.012)
Train: 329 [ 850/1251 ( 68%)]  Loss: 3.118 (3.41)  Time: 0.777s, 1318.55/s  (0.791s, 1294.51/s)  LR: 4.301e-04  Data: 0.010 (0.012)
Train: 329 [ 900/1251 ( 72%)]  Loss: 3.313 (3.40)  Time: 0.866s, 1182.59/s  (0.791s, 1294.93/s)  LR: 4.301e-04  Data: 0.011 (0.012)
Train: 329 [ 950/1251 ( 76%)]  Loss: 3.534 (3.41)  Time: 0.776s, 1318.92/s  (0.791s, 1294.73/s)  LR: 4.301e-04  Data: 0.009 (0.012)
Train: 329 [1000/1251 ( 80%)]  Loss: 3.599 (3.42)  Time: 0.775s, 1321.24/s  (0.790s, 1295.86/s)  LR: 4.301e-04  Data: 0.009 (0.012)
Train: 329 [1050/1251 ( 84%)]  Loss: 3.432 (3.42)  Time: 0.779s, 1315.12/s  (0.789s, 1297.03/s)  LR: 4.301e-04  Data: 0.009 (0.012)
Train: 329 [1100/1251 ( 88%)]  Loss: 3.367 (3.42)  Time: 0.777s, 1318.72/s  (0.790s, 1296.99/s)  LR: 4.301e-04  Data: 0.009 (0.011)
Train: 329 [1150/1251 ( 92%)]  Loss: 3.801 (3.43)  Time: 0.774s, 1323.03/s  (0.789s, 1297.67/s)  LR: 4.301e-04  Data: 0.009 (0.011)
Train: 329 [1200/1251 ( 96%)]  Loss: 3.520 (3.43)  Time: 0.815s, 1255.96/s  (0.790s, 1296.55/s)  LR: 4.301e-04  Data: 0.011 (0.011)
Train: 329 [1250/1251 (100%)]  Loss: 3.419 (3.43)  Time: 0.760s, 1348.17/s  (0.791s, 1294.47/s)  LR: 4.301e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.525 (1.525)  Loss:  0.7715 (0.7715)  Acc@1: 90.8203 (90.8203)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.194 (0.566)  Loss:  0.8638 (1.2587)  Acc@1: 84.5519 (76.3340)  Acc@5: 97.1698 (93.2680)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-323.pth.tar', 76.42800006103515)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-329.pth.tar', 76.33400001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-327.pth.tar', 76.27400005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-322.pth.tar', 76.25199998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-317.pth.tar', 76.2340001928711)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-320.pth.tar', 76.22200010742188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-326.pth.tar', 76.18599998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-325.pth.tar', 76.12399995605469)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-324.pth.tar', 76.09800002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-321.pth.tar', 76.02399997802735)

Train: 330 [   0/1251 (  0%)]  Loss: 3.406 (3.41)  Time: 2.190s,  467.48/s  (2.190s,  467.48/s)  LR: 4.276e-04  Data: 1.461 (1.461)
Train: 330 [  50/1251 (  4%)]  Loss: 3.619 (3.51)  Time: 0.778s, 1316.06/s  (0.812s, 1261.74/s)  LR: 4.276e-04  Data: 0.009 (0.043)
Train: 330 [ 100/1251 (  8%)]  Loss: 3.486 (3.50)  Time: 0.777s, 1317.58/s  (0.795s, 1287.96/s)  LR: 4.276e-04  Data: 0.010 (0.027)
Train: 330 [ 150/1251 ( 12%)]  Loss: 3.452 (3.49)  Time: 0.785s, 1304.65/s  (0.790s, 1296.10/s)  LR: 4.276e-04  Data: 0.010 (0.021)
Train: 330 [ 200/1251 ( 16%)]  Loss: 3.387 (3.47)  Time: 0.787s, 1301.73/s  (0.787s, 1300.50/s)  LR: 4.276e-04  Data: 0.010 (0.018)
Train: 330 [ 250/1251 ( 20%)]  Loss: 3.459 (3.47)  Time: 0.773s, 1325.00/s  (0.786s, 1303.16/s)  LR: 4.276e-04  Data: 0.009 (0.016)
Train: 330 [ 300/1251 ( 24%)]  Loss: 3.597 (3.49)  Time: 0.774s, 1323.52/s  (0.784s, 1305.56/s)  LR: 4.276e-04  Data: 0.010 (0.015)
Train: 330 [ 350/1251 ( 28%)]  Loss: 3.532 (3.49)  Time: 0.828s, 1236.67/s  (0.787s, 1301.23/s)  LR: 4.276e-04  Data: 0.013 (0.015)
Train: 330 [ 400/1251 ( 32%)]  Loss: 3.330 (3.47)  Time: 0.773s, 1324.38/s  (0.790s, 1295.95/s)  LR: 4.276e-04  Data: 0.009 (0.014)
Train: 330 [ 450/1251 ( 36%)]  Loss: 3.162 (3.44)  Time: 0.780s, 1313.35/s  (0.789s, 1298.17/s)  LR: 4.276e-04  Data: 0.009 (0.014)
Train: 330 [ 500/1251 ( 40%)]  Loss: 3.291 (3.43)  Time: 0.782s, 1309.27/s  (0.787s, 1300.53/s)  LR: 4.276e-04  Data: 0.009 (0.013)
Train: 330 [ 550/1251 ( 44%)]  Loss: 2.936 (3.39)  Time: 0.774s, 1323.29/s  (0.787s, 1301.62/s)  LR: 4.276e-04  Data: 0.010 (0.013)
Train: 330 [ 600/1251 ( 48%)]  Loss: 3.403 (3.39)  Time: 0.774s, 1323.26/s  (0.786s, 1302.38/s)  LR: 4.276e-04  Data: 0.010 (0.013)
Train: 330 [ 650/1251 ( 52%)]  Loss: 3.310 (3.38)  Time: 0.771s, 1327.90/s  (0.786s, 1302.98/s)  LR: 4.276e-04  Data: 0.009 (0.012)
Train: 330 [ 700/1251 ( 56%)]  Loss: 3.365 (3.38)  Time: 0.772s, 1325.69/s  (0.786s, 1303.22/s)  LR: 4.276e-04  Data: 0.010 (0.012)
Train: 330 [ 750/1251 ( 60%)]  Loss: 3.450 (3.39)  Time: 0.785s, 1304.63/s  (0.786s, 1302.81/s)  LR: 4.276e-04  Data: 0.010 (0.012)
Train: 330 [ 800/1251 ( 64%)]  Loss: 3.327 (3.38)  Time: 0.774s, 1322.33/s  (0.786s, 1303.37/s)  LR: 4.276e-04  Data: 0.010 (0.012)
Train: 330 [ 850/1251 ( 68%)]  Loss: 3.240 (3.38)  Time: 0.791s, 1293.86/s  (0.785s, 1304.04/s)  LR: 4.276e-04  Data: 0.010 (0.012)
Train: 330 [ 900/1251 ( 72%)]  Loss: 3.581 (3.39)  Time: 0.779s, 1313.96/s  (0.785s, 1304.63/s)  LR: 4.276e-04  Data: 0.010 (0.012)
Train: 330 [ 950/1251 ( 76%)]  Loss: 3.282 (3.38)  Time: 0.773s, 1325.26/s  (0.785s, 1304.86/s)  LR: 4.276e-04  Data: 0.011 (0.012)
Train: 330 [1000/1251 ( 80%)]  Loss: 3.203 (3.37)  Time: 0.784s, 1306.88/s  (0.785s, 1305.13/s)  LR: 4.276e-04  Data: 0.013 (0.012)
Train: 330 [1050/1251 ( 84%)]  Loss: 3.162 (3.36)  Time: 0.780s, 1312.06/s  (0.785s, 1304.47/s)  LR: 4.276e-04  Data: 0.010 (0.011)
Train: 330 [1100/1251 ( 88%)]  Loss: 3.277 (3.36)  Time: 0.779s, 1315.06/s  (0.786s, 1303.30/s)  LR: 4.276e-04  Data: 0.010 (0.011)
Train: 330 [1150/1251 ( 92%)]  Loss: 3.487 (3.36)  Time: 0.774s, 1323.80/s  (0.785s, 1303.99/s)  LR: 4.276e-04  Data: 0.009 (0.011)
Train: 330 [1200/1251 ( 96%)]  Loss: 3.567 (3.37)  Time: 0.774s, 1323.70/s  (0.785s, 1304.56/s)  LR: 4.276e-04  Data: 0.009 (0.011)
Train: 330 [1250/1251 (100%)]  Loss: 3.183 (3.37)  Time: 0.763s, 1342.36/s  (0.785s, 1304.90/s)  LR: 4.276e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.508 (1.508)  Loss:  0.8159 (0.8159)  Acc@1: 90.9180 (90.9180)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.194 (0.567)  Loss:  0.9126 (1.2716)  Acc@1: 85.7311 (75.9580)  Acc@5: 96.6981 (93.3480)
Train: 331 [   0/1251 (  0%)]  Loss: 3.309 (3.31)  Time: 2.306s,  444.15/s  (2.306s,  444.15/s)  LR: 4.250e-04  Data: 1.577 (1.577)
Train: 331 [  50/1251 (  4%)]  Loss: 3.566 (3.44)  Time: 0.812s, 1260.52/s  (0.818s, 1252.17/s)  LR: 4.250e-04  Data: 0.009 (0.047)
Train: 331 [ 100/1251 (  8%)]  Loss: 3.559 (3.48)  Time: 0.770s, 1329.54/s  (0.813s, 1259.29/s)  LR: 4.250e-04  Data: 0.010 (0.029)
Train: 331 [ 150/1251 ( 12%)]  Loss: 3.541 (3.49)  Time: 0.772s, 1326.31/s  (0.804s, 1273.90/s)  LR: 4.250e-04  Data: 0.010 (0.022)
Train: 331 [ 200/1251 ( 16%)]  Loss: 3.502 (3.50)  Time: 0.772s, 1326.49/s  (0.798s, 1283.98/s)  LR: 4.250e-04  Data: 0.010 (0.019)
Train: 331 [ 250/1251 ( 20%)]  Loss: 3.382 (3.48)  Time: 0.782s, 1309.68/s  (0.795s, 1288.66/s)  LR: 4.250e-04  Data: 0.009 (0.017)
Train: 331 [ 300/1251 ( 24%)]  Loss: 3.474 (3.48)  Time: 0.772s, 1327.09/s  (0.794s, 1290.45/s)  LR: 4.250e-04  Data: 0.010 (0.016)
Train: 331 [ 350/1251 ( 28%)]  Loss: 3.298 (3.45)  Time: 0.772s, 1325.68/s  (0.792s, 1293.67/s)  LR: 4.250e-04  Data: 0.009 (0.015)
Train: 331 [ 400/1251 ( 32%)]  Loss: 3.170 (3.42)  Time: 0.813s, 1259.94/s  (0.791s, 1295.09/s)  LR: 4.250e-04  Data: 0.010 (0.014)
Train: 331 [ 450/1251 ( 36%)]  Loss: 3.129 (3.39)  Time: 0.861s, 1189.75/s  (0.792s, 1293.42/s)  LR: 4.250e-04  Data: 0.010 (0.014)
Train: 331 [ 500/1251 ( 40%)]  Loss: 3.584 (3.41)  Time: 0.809s, 1266.19/s  (0.791s, 1294.18/s)  LR: 4.250e-04  Data: 0.010 (0.014)
Train: 331 [ 550/1251 ( 44%)]  Loss: 3.643 (3.43)  Time: 0.772s, 1326.48/s  (0.791s, 1294.78/s)  LR: 4.250e-04  Data: 0.009 (0.013)
Train: 331 [ 600/1251 ( 48%)]  Loss: 3.349 (3.42)  Time: 0.816s, 1254.32/s  (0.790s, 1295.76/s)  LR: 4.250e-04  Data: 0.010 (0.013)
Train: 331 [ 650/1251 ( 52%)]  Loss: 3.130 (3.40)  Time: 0.773s, 1323.92/s  (0.791s, 1294.84/s)  LR: 4.250e-04  Data: 0.010 (0.013)
Train: 331 [ 700/1251 ( 56%)]  Loss: 3.491 (3.41)  Time: 0.812s, 1260.55/s  (0.790s, 1296.43/s)  LR: 4.250e-04  Data: 0.010 (0.012)
Train: 331 [ 750/1251 ( 60%)]  Loss: 3.485 (3.41)  Time: 0.850s, 1205.03/s  (0.789s, 1297.12/s)  LR: 4.250e-04  Data: 0.010 (0.012)
Train: 331 [ 800/1251 ( 64%)]  Loss: 3.147 (3.40)  Time: 0.782s, 1309.00/s  (0.789s, 1298.16/s)  LR: 4.250e-04  Data: 0.009 (0.012)
Train: 331 [ 850/1251 ( 68%)]  Loss: 3.327 (3.39)  Time: 0.773s, 1324.65/s  (0.788s, 1299.16/s)  LR: 4.250e-04  Data: 0.010 (0.012)
Train: 331 [ 900/1251 ( 72%)]  Loss: 3.301 (3.39)  Time: 0.773s, 1324.80/s  (0.788s, 1300.06/s)  LR: 4.250e-04  Data: 0.010 (0.012)
Train: 331 [ 950/1251 ( 76%)]  Loss: 3.361 (3.39)  Time: 0.771s, 1327.66/s  (0.787s, 1300.80/s)  LR: 4.250e-04  Data: 0.010 (0.012)
Train: 331 [1000/1251 ( 80%)]  Loss: 3.430 (3.39)  Time: 0.772s, 1325.57/s  (0.787s, 1301.26/s)  LR: 4.250e-04  Data: 0.009 (0.012)
Train: 331 [1050/1251 ( 84%)]  Loss: 3.691 (3.40)  Time: 0.771s, 1328.17/s  (0.787s, 1301.55/s)  LR: 4.250e-04  Data: 0.010 (0.012)
Train: 331 [1100/1251 ( 88%)]  Loss: 3.312 (3.40)  Time: 0.775s, 1322.05/s  (0.786s, 1302.03/s)  LR: 4.250e-04  Data: 0.010 (0.011)
Train: 331 [1150/1251 ( 92%)]  Loss: 3.379 (3.40)  Time: 0.777s, 1317.69/s  (0.787s, 1301.55/s)  LR: 4.250e-04  Data: 0.009 (0.011)
Train: 331 [1200/1251 ( 96%)]  Loss: 3.413 (3.40)  Time: 0.773s, 1325.38/s  (0.787s, 1301.88/s)  LR: 4.250e-04  Data: 0.010 (0.011)
Train: 331 [1250/1251 (100%)]  Loss: 3.394 (3.40)  Time: 0.761s, 1346.23/s  (0.786s, 1302.40/s)  LR: 4.250e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.512 (1.512)  Loss:  0.6836 (0.6836)  Acc@1: 90.2344 (90.2344)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.194 (0.569)  Loss:  0.8364 (1.2031)  Acc@1: 84.9057 (76.5620)  Acc@5: 96.5802 (93.5360)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-331.pth.tar', 76.5620000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-323.pth.tar', 76.42800006103515)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-329.pth.tar', 76.33400001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-327.pth.tar', 76.27400005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-322.pth.tar', 76.25199998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-317.pth.tar', 76.2340001928711)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-320.pth.tar', 76.22200010742188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-326.pth.tar', 76.18599998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-325.pth.tar', 76.12399995605469)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-324.pth.tar', 76.09800002929687)

Train: 332 [   0/1251 (  0%)]  Loss: 3.835 (3.84)  Time: 2.241s,  456.84/s  (2.241s,  456.84/s)  LR: 4.224e-04  Data: 1.509 (1.509)
Train: 332 [  50/1251 (  4%)]  Loss: 3.434 (3.63)  Time: 0.775s, 1321.28/s  (0.813s, 1259.89/s)  LR: 4.224e-04  Data: 0.010 (0.044)
Train: 332 [ 100/1251 (  8%)]  Loss: 3.474 (3.58)  Time: 0.790s, 1296.07/s  (0.796s, 1286.06/s)  LR: 4.224e-04  Data: 0.011 (0.027)
Train: 332 [ 150/1251 ( 12%)]  Loss: 3.172 (3.48)  Time: 0.786s, 1302.11/s  (0.791s, 1294.45/s)  LR: 4.224e-04  Data: 0.010 (0.022)
Train: 332 [ 200/1251 ( 16%)]  Loss: 3.479 (3.48)  Time: 0.778s, 1316.10/s  (0.788s, 1299.71/s)  LR: 4.224e-04  Data: 0.010 (0.019)
Train: 332 [ 250/1251 ( 20%)]  Loss: 3.241 (3.44)  Time: 0.774s, 1322.84/s  (0.786s, 1302.24/s)  LR: 4.224e-04  Data: 0.010 (0.017)
Train: 332 [ 300/1251 ( 24%)]  Loss: 3.736 (3.48)  Time: 0.777s, 1317.26/s  (0.785s, 1303.96/s)  LR: 4.224e-04  Data: 0.010 (0.016)
Train: 332 [ 350/1251 ( 28%)]  Loss: 3.290 (3.46)  Time: 0.775s, 1321.23/s  (0.786s, 1302.50/s)  LR: 4.224e-04  Data: 0.010 (0.015)
Train: 332 [ 400/1251 ( 32%)]  Loss: 3.411 (3.45)  Time: 0.772s, 1325.69/s  (0.785s, 1303.71/s)  LR: 4.224e-04  Data: 0.010 (0.014)
Train: 332 [ 450/1251 ( 36%)]  Loss: 3.173 (3.42)  Time: 0.778s, 1317.04/s  (0.785s, 1303.83/s)  LR: 4.224e-04  Data: 0.010 (0.014)
Train: 332 [ 500/1251 ( 40%)]  Loss: 3.570 (3.44)  Time: 0.791s, 1294.86/s  (0.785s, 1304.37/s)  LR: 4.224e-04  Data: 0.009 (0.013)
Train: 332 [ 550/1251 ( 44%)]  Loss: 3.724 (3.46)  Time: 0.831s, 1231.72/s  (0.785s, 1304.72/s)  LR: 4.224e-04  Data: 0.015 (0.013)
Train: 332 [ 600/1251 ( 48%)]  Loss: 3.394 (3.46)  Time: 0.829s, 1235.21/s  (0.785s, 1303.84/s)  LR: 4.224e-04  Data: 0.009 (0.013)
Train: 332 [ 650/1251 ( 52%)]  Loss: 3.550 (3.46)  Time: 0.784s, 1305.70/s  (0.786s, 1303.60/s)  LR: 4.224e-04  Data: 0.010 (0.013)
Train: 332 [ 700/1251 ( 56%)]  Loss: 3.131 (3.44)  Time: 0.776s, 1319.29/s  (0.785s, 1304.09/s)  LR: 4.224e-04  Data: 0.009 (0.012)
Train: 332 [ 750/1251 ( 60%)]  Loss: 3.898 (3.47)  Time: 0.774s, 1323.72/s  (0.785s, 1304.94/s)  LR: 4.224e-04  Data: 0.009 (0.012)
Train: 332 [ 800/1251 ( 64%)]  Loss: 3.421 (3.47)  Time: 0.774s, 1322.65/s  (0.784s, 1305.72/s)  LR: 4.224e-04  Data: 0.011 (0.012)
Train: 332 [ 850/1251 ( 68%)]  Loss: 3.402 (3.46)  Time: 0.807s, 1269.63/s  (0.784s, 1305.72/s)  LR: 4.224e-04  Data: 0.010 (0.012)
Train: 332 [ 900/1251 ( 72%)]  Loss: 3.048 (3.44)  Time: 0.774s, 1322.70/s  (0.784s, 1305.35/s)  LR: 4.224e-04  Data: 0.009 (0.012)
Train: 332 [ 950/1251 ( 76%)]  Loss: 3.524 (3.45)  Time: 0.774s, 1323.71/s  (0.784s, 1305.92/s)  LR: 4.224e-04  Data: 0.009 (0.012)
Train: 332 [1000/1251 ( 80%)]  Loss: 3.339 (3.44)  Time: 0.782s, 1309.38/s  (0.784s, 1306.24/s)  LR: 4.224e-04  Data: 0.009 (0.012)
Train: 332 [1050/1251 ( 84%)]  Loss: 3.403 (3.44)  Time: 0.787s, 1301.48/s  (0.785s, 1304.80/s)  LR: 4.224e-04  Data: 0.010 (0.012)
Train: 332 [1100/1251 ( 88%)]  Loss: 3.220 (3.43)  Time: 0.781s, 1311.01/s  (0.785s, 1304.57/s)  LR: 4.224e-04  Data: 0.010 (0.011)
Train: 332 [1150/1251 ( 92%)]  Loss: 3.246 (3.42)  Time: 0.777s, 1317.33/s  (0.786s, 1303.17/s)  LR: 4.224e-04  Data: 0.009 (0.011)
Train: 332 [1200/1251 ( 96%)]  Loss: 3.348 (3.42)  Time: 0.811s, 1263.20/s  (0.786s, 1303.02/s)  LR: 4.224e-04  Data: 0.010 (0.011)
Train: 332 [1250/1251 (100%)]  Loss: 3.206 (3.41)  Time: 0.763s, 1341.75/s  (0.786s, 1302.96/s)  LR: 4.224e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.526 (1.526)  Loss:  0.7627 (0.7627)  Acc@1: 89.8438 (89.8438)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.193 (0.557)  Loss:  0.8633 (1.2456)  Acc@1: 85.2594 (76.2980)  Acc@5: 96.9340 (93.3660)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-331.pth.tar', 76.5620000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-323.pth.tar', 76.42800006103515)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-329.pth.tar', 76.33400001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-332.pth.tar', 76.29800006103515)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-327.pth.tar', 76.27400005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-322.pth.tar', 76.25199998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-317.pth.tar', 76.2340001928711)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-320.pth.tar', 76.22200010742188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-326.pth.tar', 76.18599998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-325.pth.tar', 76.12399995605469)

Train: 333 [   0/1251 (  0%)]  Loss: 3.209 (3.21)  Time: 2.163s,  473.38/s  (2.163s,  473.38/s)  LR: 4.199e-04  Data: 1.432 (1.432)
Train: 333 [  50/1251 (  4%)]  Loss: 3.262 (3.24)  Time: 0.773s, 1324.49/s  (0.814s, 1257.68/s)  LR: 4.199e-04  Data: 0.009 (0.045)
Train: 333 [ 100/1251 (  8%)]  Loss: 3.319 (3.26)  Time: 0.782s, 1309.97/s  (0.798s, 1282.65/s)  LR: 4.199e-04  Data: 0.011 (0.028)
Train: 333 [ 150/1251 ( 12%)]  Loss: 2.827 (3.15)  Time: 0.772s, 1325.67/s  (0.793s, 1291.53/s)  LR: 4.199e-04  Data: 0.009 (0.022)
Train: 333 [ 200/1251 ( 16%)]  Loss: 3.222 (3.17)  Time: 0.775s, 1322.07/s  (0.790s, 1295.40/s)  LR: 4.199e-04  Data: 0.009 (0.019)
Train: 333 [ 250/1251 ( 20%)]  Loss: 3.267 (3.18)  Time: 0.772s, 1325.64/s  (0.788s, 1299.85/s)  LR: 4.199e-04  Data: 0.010 (0.017)
Train: 333 [ 300/1251 ( 24%)]  Loss: 3.179 (3.18)  Time: 0.775s, 1320.77/s  (0.786s, 1303.00/s)  LR: 4.199e-04  Data: 0.009 (0.016)
Train: 333 [ 350/1251 ( 28%)]  Loss: 3.225 (3.19)  Time: 0.773s, 1324.55/s  (0.785s, 1305.07/s)  LR: 4.199e-04  Data: 0.010 (0.015)
Train: 333 [ 400/1251 ( 32%)]  Loss: 3.111 (3.18)  Time: 0.774s, 1323.77/s  (0.784s, 1306.47/s)  LR: 4.199e-04  Data: 0.009 (0.014)
Train: 333 [ 450/1251 ( 36%)]  Loss: 3.738 (3.24)  Time: 0.773s, 1324.40/s  (0.786s, 1303.23/s)  LR: 4.199e-04  Data: 0.009 (0.014)
Train: 333 [ 500/1251 ( 40%)]  Loss: 3.204 (3.23)  Time: 0.811s, 1262.29/s  (0.785s, 1304.80/s)  LR: 4.199e-04  Data: 0.011 (0.013)
Train: 333 [ 550/1251 ( 44%)]  Loss: 3.277 (3.24)  Time: 0.791s, 1294.03/s  (0.785s, 1303.81/s)  LR: 4.199e-04  Data: 0.009 (0.013)
Train: 333 [ 600/1251 ( 48%)]  Loss: 3.312 (3.24)  Time: 0.773s, 1325.24/s  (0.785s, 1305.13/s)  LR: 4.199e-04  Data: 0.010 (0.013)
Train: 333 [ 650/1251 ( 52%)]  Loss: 3.038 (3.23)  Time: 0.772s, 1326.33/s  (0.784s, 1305.52/s)  LR: 4.199e-04  Data: 0.010 (0.012)
Train: 333 [ 700/1251 ( 56%)]  Loss: 3.362 (3.24)  Time: 0.777s, 1318.16/s  (0.784s, 1306.29/s)  LR: 4.199e-04  Data: 0.010 (0.012)
Train: 333 [ 750/1251 ( 60%)]  Loss: 3.255 (3.24)  Time: 0.774s, 1323.44/s  (0.784s, 1306.63/s)  LR: 4.199e-04  Data: 0.010 (0.012)
Train: 333 [ 800/1251 ( 64%)]  Loss: 3.403 (3.25)  Time: 0.773s, 1324.11/s  (0.784s, 1306.92/s)  LR: 4.199e-04  Data: 0.010 (0.012)
Train: 333 [ 850/1251 ( 68%)]  Loss: 3.299 (3.25)  Time: 0.772s, 1325.71/s  (0.784s, 1306.90/s)  LR: 4.199e-04  Data: 0.010 (0.012)
Train: 333 [ 900/1251 ( 72%)]  Loss: 3.717 (3.28)  Time: 0.842s, 1215.76/s  (0.784s, 1305.41/s)  LR: 4.199e-04  Data: 0.010 (0.012)
Train: 333 [ 950/1251 ( 76%)]  Loss: 3.551 (3.29)  Time: 0.774s, 1322.50/s  (0.784s, 1305.45/s)  LR: 4.199e-04  Data: 0.010 (0.012)
Train: 333 [1000/1251 ( 80%)]  Loss: 3.241 (3.29)  Time: 0.773s, 1324.82/s  (0.785s, 1304.77/s)  LR: 4.199e-04  Data: 0.010 (0.011)
Train: 333 [1050/1251 ( 84%)]  Loss: 3.360 (3.29)  Time: 0.774s, 1323.33/s  (0.784s, 1305.39/s)  LR: 4.199e-04  Data: 0.010 (0.011)
Train: 333 [1100/1251 ( 88%)]  Loss: 3.398 (3.29)  Time: 0.771s, 1328.09/s  (0.784s, 1305.59/s)  LR: 4.199e-04  Data: 0.009 (0.011)
Train: 333 [1150/1251 ( 92%)]  Loss: 3.760 (3.31)  Time: 0.771s, 1327.93/s  (0.784s, 1306.07/s)  LR: 4.199e-04  Data: 0.009 (0.011)
Train: 333 [1200/1251 ( 96%)]  Loss: 3.184 (3.31)  Time: 0.772s, 1326.94/s  (0.784s, 1306.13/s)  LR: 4.199e-04  Data: 0.010 (0.011)
Train: 333 [1250/1251 (100%)]  Loss: 3.597 (3.32)  Time: 0.760s, 1348.22/s  (0.785s, 1304.64/s)  LR: 4.199e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.600 (1.600)  Loss:  0.7603 (0.7603)  Acc@1: 89.1602 (89.1602)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.193 (0.570)  Loss:  0.8809 (1.2240)  Acc@1: 85.8491 (76.3460)  Acc@5: 95.7547 (93.3100)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-331.pth.tar', 76.5620000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-323.pth.tar', 76.42800006103515)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-333.pth.tar', 76.34599992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-329.pth.tar', 76.33400001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-332.pth.tar', 76.29800006103515)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-327.pth.tar', 76.27400005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-322.pth.tar', 76.25199998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-317.pth.tar', 76.2340001928711)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-320.pth.tar', 76.22200010742188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-326.pth.tar', 76.18599998046875)

Train: 334 [   0/1251 (  0%)]  Loss: 3.344 (3.34)  Time: 2.200s,  465.51/s  (2.200s,  465.51/s)  LR: 4.173e-04  Data: 1.468 (1.468)
Train: 334 [  50/1251 (  4%)]  Loss: 3.155 (3.25)  Time: 0.773s, 1324.43/s  (0.815s, 1256.23/s)  LR: 4.173e-04  Data: 0.009 (0.044)
Train: 334 [ 100/1251 (  8%)]  Loss: 3.525 (3.34)  Time: 0.782s, 1309.24/s  (0.804s, 1274.24/s)  LR: 4.173e-04  Data: 0.010 (0.027)
Train: 334 [ 150/1251 ( 12%)]  Loss: 3.495 (3.38)  Time: 0.773s, 1324.94/s  (0.795s, 1288.73/s)  LR: 4.173e-04  Data: 0.010 (0.021)
Train: 334 [ 200/1251 ( 16%)]  Loss: 3.445 (3.39)  Time: 0.782s, 1310.07/s  (0.796s, 1286.39/s)  LR: 4.173e-04  Data: 0.009 (0.018)
Train: 334 [ 250/1251 ( 20%)]  Loss: 3.149 (3.35)  Time: 0.775s, 1320.48/s  (0.793s, 1291.22/s)  LR: 4.173e-04  Data: 0.009 (0.017)
Train: 334 [ 300/1251 ( 24%)]  Loss: 3.597 (3.39)  Time: 0.773s, 1325.33/s  (0.791s, 1295.18/s)  LR: 4.173e-04  Data: 0.010 (0.016)
Train: 334 [ 350/1251 ( 28%)]  Loss: 3.477 (3.40)  Time: 0.771s, 1327.67/s  (0.789s, 1298.30/s)  LR: 4.173e-04  Data: 0.009 (0.015)
Train: 334 [ 400/1251 ( 32%)]  Loss: 3.885 (3.45)  Time: 0.772s, 1327.06/s  (0.787s, 1300.61/s)  LR: 4.173e-04  Data: 0.010 (0.014)
Train: 334 [ 450/1251 ( 36%)]  Loss: 3.389 (3.45)  Time: 0.772s, 1325.83/s  (0.787s, 1301.18/s)  LR: 4.173e-04  Data: 0.009 (0.014)
Train: 334 [ 500/1251 ( 40%)]  Loss: 3.675 (3.47)  Time: 0.773s, 1325.41/s  (0.786s, 1302.99/s)  LR: 4.173e-04  Data: 0.009 (0.013)
Train: 334 [ 550/1251 ( 44%)]  Loss: 3.576 (3.48)  Time: 0.773s, 1324.68/s  (0.786s, 1303.46/s)  LR: 4.173e-04  Data: 0.009 (0.013)
Train: 334 [ 600/1251 ( 48%)]  Loss: 3.514 (3.48)  Time: 0.776s, 1320.26/s  (0.786s, 1303.38/s)  LR: 4.173e-04  Data: 0.010 (0.013)
Train: 334 [ 650/1251 ( 52%)]  Loss: 3.290 (3.47)  Time: 0.774s, 1323.29/s  (0.785s, 1304.11/s)  LR: 4.173e-04  Data: 0.009 (0.012)
Train: 334 [ 700/1251 ( 56%)]  Loss: 3.419 (3.46)  Time: 0.775s, 1321.87/s  (0.785s, 1305.28/s)  LR: 4.173e-04  Data: 0.010 (0.012)
Train: 334 [ 750/1251 ( 60%)]  Loss: 3.688 (3.48)  Time: 0.812s, 1260.65/s  (0.785s, 1304.46/s)  LR: 4.173e-04  Data: 0.009 (0.012)
Train: 334 [ 800/1251 ( 64%)]  Loss: 3.610 (3.48)  Time: 0.776s, 1320.00/s  (0.785s, 1304.94/s)  LR: 4.173e-04  Data: 0.011 (0.012)
Train: 334 [ 850/1251 ( 68%)]  Loss: 3.410 (3.48)  Time: 0.791s, 1295.21/s  (0.784s, 1305.75/s)  LR: 4.173e-04  Data: 0.013 (0.012)
Train: 334 [ 900/1251 ( 72%)]  Loss: 3.543 (3.48)  Time: 0.773s, 1323.99/s  (0.784s, 1306.19/s)  LR: 4.173e-04  Data: 0.009 (0.012)
Train: 334 [ 950/1251 ( 76%)]  Loss: 3.097 (3.46)  Time: 0.775s, 1321.03/s  (0.784s, 1306.41/s)  LR: 4.173e-04  Data: 0.010 (0.012)
Train: 334 [1000/1251 ( 80%)]  Loss: 3.561 (3.47)  Time: 0.779s, 1313.72/s  (0.784s, 1306.90/s)  LR: 4.173e-04  Data: 0.009 (0.012)
Train: 334 [1050/1251 ( 84%)]  Loss: 2.973 (3.45)  Time: 0.773s, 1325.22/s  (0.783s, 1307.11/s)  LR: 4.173e-04  Data: 0.009 (0.011)
Train: 334 [1100/1251 ( 88%)]  Loss: 3.542 (3.45)  Time: 0.776s, 1319.53/s  (0.783s, 1307.34/s)  LR: 4.173e-04  Data: 0.009 (0.011)
Train: 334 [1150/1251 ( 92%)]  Loss: 3.055 (3.43)  Time: 0.772s, 1327.28/s  (0.783s, 1307.92/s)  LR: 4.173e-04  Data: 0.009 (0.011)
Train: 334 [1200/1251 ( 96%)]  Loss: 3.080 (3.42)  Time: 0.774s, 1323.31/s  (0.783s, 1308.07/s)  LR: 4.173e-04  Data: 0.010 (0.011)
Train: 334 [1250/1251 (100%)]  Loss: 3.071 (3.41)  Time: 0.769s, 1331.80/s  (0.783s, 1308.19/s)  LR: 4.173e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.537 (1.537)  Loss:  0.7422 (0.7422)  Acc@1: 90.2344 (90.2344)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.194 (0.559)  Loss:  0.8408 (1.2409)  Acc@1: 87.0283 (76.1820)  Acc@5: 96.1085 (93.1660)
Train: 335 [   0/1251 (  0%)]  Loss: 3.398 (3.40)  Time: 2.212s,  462.84/s  (2.212s,  462.84/s)  LR: 4.148e-04  Data: 1.484 (1.484)
Train: 335 [  50/1251 (  4%)]  Loss: 3.265 (3.33)  Time: 0.777s, 1317.73/s  (0.814s, 1257.34/s)  LR: 4.148e-04  Data: 0.011 (0.045)
Train: 335 [ 100/1251 (  8%)]  Loss: 3.409 (3.36)  Time: 0.798s, 1283.71/s  (0.800s, 1279.79/s)  LR: 4.148e-04  Data: 0.009 (0.028)
Train: 335 [ 150/1251 ( 12%)]  Loss: 3.202 (3.32)  Time: 0.828s, 1237.12/s  (0.796s, 1286.43/s)  LR: 4.148e-04  Data: 0.009 (0.022)
Train: 335 [ 200/1251 ( 16%)]  Loss: 3.618 (3.38)  Time: 0.815s, 1256.04/s  (0.800s, 1279.35/s)  LR: 4.148e-04  Data: 0.011 (0.019)
Train: 335 [ 250/1251 ( 20%)]  Loss: 3.028 (3.32)  Time: 0.771s, 1327.73/s  (0.796s, 1286.50/s)  LR: 4.148e-04  Data: 0.010 (0.017)
Train: 335 [ 300/1251 ( 24%)]  Loss: 3.542 (3.35)  Time: 0.811s, 1261.89/s  (0.793s, 1291.55/s)  LR: 4.148e-04  Data: 0.009 (0.016)
Train: 335 [ 350/1251 ( 28%)]  Loss: 3.344 (3.35)  Time: 0.772s, 1326.16/s  (0.792s, 1292.16/s)  LR: 4.148e-04  Data: 0.009 (0.015)
Train: 335 [ 400/1251 ( 32%)]  Loss: 3.415 (3.36)  Time: 0.773s, 1324.23/s  (0.791s, 1293.87/s)  LR: 4.148e-04  Data: 0.009 (0.014)
Train: 335 [ 450/1251 ( 36%)]  Loss: 3.569 (3.38)  Time: 0.857s, 1194.74/s  (0.790s, 1295.58/s)  LR: 4.148e-04  Data: 0.009 (0.014)
Train: 335 [ 500/1251 ( 40%)]  Loss: 3.330 (3.37)  Time: 0.776s, 1319.25/s  (0.789s, 1297.28/s)  LR: 4.148e-04  Data: 0.009 (0.013)
Train: 335 [ 550/1251 ( 44%)]  Loss: 3.772 (3.41)  Time: 0.779s, 1314.65/s  (0.789s, 1298.01/s)  LR: 4.148e-04  Data: 0.009 (0.013)
Train: 335 [ 600/1251 ( 48%)]  Loss: 2.984 (3.38)  Time: 0.777s, 1317.84/s  (0.788s, 1299.23/s)  LR: 4.148e-04  Data: 0.013 (0.013)
Train: 335 [ 650/1251 ( 52%)]  Loss: 3.656 (3.40)  Time: 0.827s, 1238.39/s  (0.788s, 1300.19/s)  LR: 4.148e-04  Data: 0.009 (0.013)
Train: 335 [ 700/1251 ( 56%)]  Loss: 3.750 (3.42)  Time: 0.772s, 1325.96/s  (0.788s, 1298.87/s)  LR: 4.148e-04  Data: 0.010 (0.012)
Train: 335 [ 750/1251 ( 60%)]  Loss: 3.168 (3.40)  Time: 0.771s, 1328.41/s  (0.788s, 1300.26/s)  LR: 4.148e-04  Data: 0.009 (0.012)
Train: 335 [ 800/1251 ( 64%)]  Loss: 3.370 (3.40)  Time: 0.774s, 1322.58/s  (0.787s, 1300.86/s)  LR: 4.148e-04  Data: 0.009 (0.012)
Train: 335 [ 850/1251 ( 68%)]  Loss: 3.471 (3.41)  Time: 0.777s, 1318.46/s  (0.788s, 1300.06/s)  LR: 4.148e-04  Data: 0.010 (0.012)
Train: 335 [ 900/1251 ( 72%)]  Loss: 3.443 (3.41)  Time: 0.781s, 1310.84/s  (0.788s, 1299.05/s)  LR: 4.148e-04  Data: 0.011 (0.012)
Train: 335 [ 950/1251 ( 76%)]  Loss: 3.448 (3.41)  Time: 0.773s, 1325.33/s  (0.788s, 1299.95/s)  LR: 4.148e-04  Data: 0.009 (0.012)
Train: 335 [1000/1251 ( 80%)]  Loss: 3.500 (3.41)  Time: 0.773s, 1325.00/s  (0.787s, 1300.35/s)  LR: 4.148e-04  Data: 0.010 (0.012)
Train: 335 [1050/1251 ( 84%)]  Loss: 3.879 (3.43)  Time: 0.850s, 1204.49/s  (0.787s, 1301.13/s)  LR: 4.148e-04  Data: 0.010 (0.012)
Train: 335 [1100/1251 ( 88%)]  Loss: 3.656 (3.44)  Time: 0.776s, 1319.21/s  (0.787s, 1301.92/s)  LR: 4.148e-04  Data: 0.009 (0.012)
Train: 335 [1150/1251 ( 92%)]  Loss: 3.583 (3.45)  Time: 0.834s, 1228.30/s  (0.787s, 1301.51/s)  LR: 4.148e-04  Data: 0.017 (0.011)
Train: 335 [1200/1251 ( 96%)]  Loss: 3.671 (3.46)  Time: 0.774s, 1322.95/s  (0.787s, 1301.32/s)  LR: 4.148e-04  Data: 0.009 (0.011)
Train: 335 [1250/1251 (100%)]  Loss: 3.156 (3.45)  Time: 0.765s, 1338.84/s  (0.787s, 1301.84/s)  LR: 4.148e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.517 (1.517)  Loss:  0.5967 (0.5967)  Acc@1: 88.2812 (88.2812)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.194 (0.563)  Loss:  0.7134 (1.1560)  Acc@1: 85.8491 (76.3280)  Acc@5: 96.5802 (93.3260)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-331.pth.tar', 76.5620000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-323.pth.tar', 76.42800006103515)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-333.pth.tar', 76.34599992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-329.pth.tar', 76.33400001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-335.pth.tar', 76.32800005859374)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-332.pth.tar', 76.29800006103515)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-327.pth.tar', 76.27400005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-322.pth.tar', 76.25199998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-317.pth.tar', 76.2340001928711)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-320.pth.tar', 76.22200010742188)

Train: 336 [   0/1251 (  0%)]  Loss: 3.605 (3.61)  Time: 2.214s,  462.54/s  (2.214s,  462.54/s)  LR: 4.122e-04  Data: 1.492 (1.492)
Train: 336 [  50/1251 (  4%)]  Loss: 3.413 (3.51)  Time: 0.814s, 1258.46/s  (0.815s, 1256.09/s)  LR: 4.122e-04  Data: 0.010 (0.044)
Train: 336 [ 100/1251 (  8%)]  Loss: 3.466 (3.49)  Time: 0.772s, 1326.32/s  (0.810s, 1263.98/s)  LR: 4.122e-04  Data: 0.009 (0.027)
Train: 336 [ 150/1251 ( 12%)]  Loss: 3.424 (3.48)  Time: 0.773s, 1324.31/s  (0.800s, 1280.59/s)  LR: 4.122e-04  Data: 0.009 (0.022)
Train: 336 [ 200/1251 ( 16%)]  Loss: 3.056 (3.39)  Time: 0.780s, 1313.43/s  (0.795s, 1288.62/s)  LR: 4.122e-04  Data: 0.010 (0.019)
Train: 336 [ 250/1251 ( 20%)]  Loss: 3.399 (3.39)  Time: 0.781s, 1311.50/s  (0.791s, 1294.15/s)  LR: 4.122e-04  Data: 0.009 (0.017)
Train: 336 [ 300/1251 ( 24%)]  Loss: 3.396 (3.39)  Time: 0.772s, 1327.20/s  (0.790s, 1296.64/s)  LR: 4.122e-04  Data: 0.009 (0.016)
Train: 336 [ 350/1251 ( 28%)]  Loss: 3.557 (3.41)  Time: 0.773s, 1324.33/s  (0.789s, 1298.32/s)  LR: 4.122e-04  Data: 0.010 (0.015)
Train: 336 [ 400/1251 ( 32%)]  Loss: 3.210 (3.39)  Time: 0.784s, 1306.68/s  (0.791s, 1293.81/s)  LR: 4.122e-04  Data: 0.010 (0.014)
Train: 336 [ 450/1251 ( 36%)]  Loss: 3.624 (3.42)  Time: 0.777s, 1317.88/s  (0.791s, 1294.41/s)  LR: 4.122e-04  Data: 0.009 (0.014)
Train: 336 [ 500/1251 ( 40%)]  Loss: 3.591 (3.43)  Time: 0.782s, 1309.65/s  (0.790s, 1296.81/s)  LR: 4.122e-04  Data: 0.009 (0.013)
Train: 336 [ 550/1251 ( 44%)]  Loss: 2.966 (3.39)  Time: 0.773s, 1323.98/s  (0.789s, 1297.42/s)  LR: 4.122e-04  Data: 0.010 (0.013)
Train: 336 [ 600/1251 ( 48%)]  Loss: 3.288 (3.38)  Time: 0.774s, 1322.45/s  (0.788s, 1298.70/s)  LR: 4.122e-04  Data: 0.010 (0.013)
Train: 336 [ 650/1251 ( 52%)]  Loss: 3.549 (3.40)  Time: 0.783s, 1307.57/s  (0.788s, 1298.97/s)  LR: 4.122e-04  Data: 0.009 (0.013)
Train: 336 [ 700/1251 ( 56%)]  Loss: 3.647 (3.41)  Time: 0.833s, 1229.16/s  (0.790s, 1296.46/s)  LR: 4.122e-04  Data: 0.009 (0.012)
Train: 336 [ 750/1251 ( 60%)]  Loss: 3.366 (3.41)  Time: 0.815s, 1256.83/s  (0.790s, 1295.94/s)  LR: 4.122e-04  Data: 0.011 (0.012)
Train: 336 [ 800/1251 ( 64%)]  Loss: 3.448 (3.41)  Time: 0.771s, 1327.42/s  (0.790s, 1296.12/s)  LR: 4.122e-04  Data: 0.009 (0.012)
Train: 336 [ 850/1251 ( 68%)]  Loss: 3.229 (3.40)  Time: 0.775s, 1320.59/s  (0.790s, 1296.37/s)  LR: 4.122e-04  Data: 0.010 (0.012)
Train: 336 [ 900/1251 ( 72%)]  Loss: 3.230 (3.39)  Time: 0.783s, 1307.43/s  (0.789s, 1297.21/s)  LR: 4.122e-04  Data: 0.010 (0.012)
Train: 336 [ 950/1251 ( 76%)]  Loss: 3.279 (3.39)  Time: 0.848s, 1208.20/s  (0.789s, 1297.94/s)  LR: 4.122e-04  Data: 0.012 (0.012)
Train: 336 [1000/1251 ( 80%)]  Loss: 3.333 (3.38)  Time: 0.798s, 1282.80/s  (0.789s, 1298.06/s)  LR: 4.122e-04  Data: 0.010 (0.012)
Train: 336 [1050/1251 ( 84%)]  Loss: 3.478 (3.39)  Time: 0.774s, 1323.44/s  (0.789s, 1298.52/s)  LR: 4.122e-04  Data: 0.010 (0.012)
Train: 336 [1100/1251 ( 88%)]  Loss: 3.538 (3.40)  Time: 0.787s, 1300.83/s  (0.788s, 1299.07/s)  LR: 4.122e-04  Data: 0.011 (0.011)
Train: 336 [1150/1251 ( 92%)]  Loss: 3.466 (3.40)  Time: 0.816s, 1255.03/s  (0.789s, 1298.42/s)  LR: 4.122e-04  Data: 0.014 (0.011)
Train: 336 [1200/1251 ( 96%)]  Loss: 3.318 (3.40)  Time: 0.775s, 1322.04/s  (0.789s, 1297.97/s)  LR: 4.122e-04  Data: 0.009 (0.011)
Train: 336 [1250/1251 (100%)]  Loss: 3.227 (3.39)  Time: 0.759s, 1348.48/s  (0.789s, 1298.64/s)  LR: 4.122e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.537 (1.537)  Loss:  0.7183 (0.7183)  Acc@1: 89.6484 (89.6484)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.193 (0.559)  Loss:  0.8896 (1.2545)  Acc@1: 85.0236 (76.1600)  Acc@5: 96.1085 (93.2300)
Train: 337 [   0/1251 (  0%)]  Loss: 3.668 (3.67)  Time: 2.288s,  447.51/s  (2.288s,  447.51/s)  LR: 4.097e-04  Data: 1.557 (1.557)
Train: 337 [  50/1251 (  4%)]  Loss: 3.507 (3.59)  Time: 0.782s, 1309.53/s  (0.825s, 1241.61/s)  LR: 4.097e-04  Data: 0.009 (0.044)
Train: 337 [ 100/1251 (  8%)]  Loss: 3.124 (3.43)  Time: 0.777s, 1317.56/s  (0.809s, 1266.43/s)  LR: 4.097e-04  Data: 0.010 (0.027)
Train: 337 [ 150/1251 ( 12%)]  Loss: 3.187 (3.37)  Time: 0.775s, 1321.76/s  (0.800s, 1280.23/s)  LR: 4.097e-04  Data: 0.009 (0.021)
Train: 337 [ 200/1251 ( 16%)]  Loss: 3.619 (3.42)  Time: 0.778s, 1316.68/s  (0.796s, 1286.97/s)  LR: 4.097e-04  Data: 0.009 (0.018)
Train: 337 [ 250/1251 ( 20%)]  Loss: 3.469 (3.43)  Time: 0.774s, 1323.70/s  (0.792s, 1293.41/s)  LR: 4.097e-04  Data: 0.011 (0.017)
Train: 337 [ 300/1251 ( 24%)]  Loss: 3.283 (3.41)  Time: 0.771s, 1328.69/s  (0.795s, 1288.40/s)  LR: 4.097e-04  Data: 0.009 (0.016)
Train: 337 [ 350/1251 ( 28%)]  Loss: 3.454 (3.41)  Time: 0.807s, 1268.67/s  (0.795s, 1288.25/s)  LR: 4.097e-04  Data: 0.009 (0.015)
Train: 337 [ 400/1251 ( 32%)]  Loss: 3.376 (3.41)  Time: 0.773s, 1323.93/s  (0.793s, 1290.93/s)  LR: 4.097e-04  Data: 0.009 (0.014)
Train: 337 [ 450/1251 ( 36%)]  Loss: 3.426 (3.41)  Time: 0.775s, 1321.98/s  (0.792s, 1293.05/s)  LR: 4.097e-04  Data: 0.010 (0.014)
Train: 337 [ 500/1251 ( 40%)]  Loss: 3.352 (3.41)  Time: 0.773s, 1325.05/s  (0.791s, 1295.24/s)  LR: 4.097e-04  Data: 0.010 (0.013)
Train: 337 [ 550/1251 ( 44%)]  Loss: 3.417 (3.41)  Time: 0.783s, 1307.65/s  (0.790s, 1296.11/s)  LR: 4.097e-04  Data: 0.010 (0.013)
Train: 337 [ 600/1251 ( 48%)]  Loss: 2.757 (3.36)  Time: 0.772s, 1325.79/s  (0.789s, 1297.69/s)  LR: 4.097e-04  Data: 0.010 (0.013)
Train: 337 [ 650/1251 ( 52%)]  Loss: 3.493 (3.37)  Time: 0.791s, 1294.90/s  (0.789s, 1298.56/s)  LR: 4.097e-04  Data: 0.010 (0.013)
Train: 337 [ 700/1251 ( 56%)]  Loss: 3.340 (3.36)  Time: 0.796s, 1286.34/s  (0.788s, 1298.97/s)  LR: 4.097e-04  Data: 0.013 (0.013)
Train: 337 [ 750/1251 ( 60%)]  Loss: 3.667 (3.38)  Time: 0.773s, 1325.28/s  (0.788s, 1300.28/s)  LR: 4.097e-04  Data: 0.010 (0.012)
Train: 337 [ 800/1251 ( 64%)]  Loss: 3.113 (3.37)  Time: 0.810s, 1264.93/s  (0.787s, 1300.52/s)  LR: 4.097e-04  Data: 0.010 (0.012)
Train: 337 [ 850/1251 ( 68%)]  Loss: 3.311 (3.36)  Time: 0.773s, 1325.04/s  (0.788s, 1299.94/s)  LR: 4.097e-04  Data: 0.010 (0.012)
Train: 337 [ 900/1251 ( 72%)]  Loss: 3.325 (3.36)  Time: 0.778s, 1317.04/s  (0.788s, 1298.77/s)  LR: 4.097e-04  Data: 0.013 (0.012)
Train: 337 [ 950/1251 ( 76%)]  Loss: 3.268 (3.36)  Time: 0.775s, 1321.51/s  (0.790s, 1296.78/s)  LR: 4.097e-04  Data: 0.010 (0.012)
Train: 337 [1000/1251 ( 80%)]  Loss: 3.693 (3.37)  Time: 0.778s, 1315.48/s  (0.789s, 1297.36/s)  LR: 4.097e-04  Data: 0.010 (0.012)
Train: 337 [1050/1251 ( 84%)]  Loss: 3.666 (3.39)  Time: 0.772s, 1325.73/s  (0.789s, 1298.12/s)  LR: 4.097e-04  Data: 0.010 (0.012)
Train: 337 [1100/1251 ( 88%)]  Loss: 3.358 (3.39)  Time: 0.772s, 1325.81/s  (0.788s, 1299.04/s)  LR: 4.097e-04  Data: 0.010 (0.012)
Train: 337 [1150/1251 ( 92%)]  Loss: 3.744 (3.40)  Time: 0.773s, 1324.49/s  (0.788s, 1299.87/s)  LR: 4.097e-04  Data: 0.010 (0.012)
Train: 337 [1200/1251 ( 96%)]  Loss: 3.458 (3.40)  Time: 0.784s, 1305.95/s  (0.788s, 1300.12/s)  LR: 4.097e-04  Data: 0.012 (0.011)
Train: 337 [1250/1251 (100%)]  Loss: 3.042 (3.39)  Time: 0.763s, 1341.29/s  (0.787s, 1300.52/s)  LR: 4.097e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.574 (1.574)  Loss:  0.7080 (0.7080)  Acc@1: 88.9648 (88.9648)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.193 (0.572)  Loss:  0.7852 (1.2073)  Acc@1: 86.4387 (76.4160)  Acc@5: 96.8160 (93.4240)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-331.pth.tar', 76.5620000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-323.pth.tar', 76.42800006103515)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-337.pth.tar', 76.41600005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-333.pth.tar', 76.34599992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-329.pth.tar', 76.33400001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-335.pth.tar', 76.32800005859374)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-332.pth.tar', 76.29800006103515)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-327.pth.tar', 76.27400005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-322.pth.tar', 76.25199998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-317.pth.tar', 76.2340001928711)

Train: 338 [   0/1251 (  0%)]  Loss: 3.656 (3.66)  Time: 2.240s,  457.20/s  (2.240s,  457.20/s)  LR: 4.072e-04  Data: 1.511 (1.511)
Train: 338 [  50/1251 (  4%)]  Loss: 3.120 (3.39)  Time: 0.773s, 1324.65/s  (0.816s, 1255.19/s)  LR: 4.072e-04  Data: 0.010 (0.046)
Train: 338 [ 100/1251 (  8%)]  Loss: 3.576 (3.45)  Time: 0.776s, 1319.27/s  (0.798s, 1283.94/s)  LR: 4.072e-04  Data: 0.010 (0.028)
Train: 338 [ 150/1251 ( 12%)]  Loss: 3.781 (3.53)  Time: 0.774s, 1323.68/s  (0.792s, 1292.73/s)  LR: 4.072e-04  Data: 0.010 (0.022)
Train: 338 [ 200/1251 ( 16%)]  Loss: 3.003 (3.43)  Time: 0.809s, 1265.44/s  (0.789s, 1297.39/s)  LR: 4.072e-04  Data: 0.010 (0.019)
Train: 338 [ 250/1251 ( 20%)]  Loss: 2.976 (3.35)  Time: 0.773s, 1324.91/s  (0.788s, 1299.94/s)  LR: 4.072e-04  Data: 0.010 (0.017)
Train: 338 [ 300/1251 ( 24%)]  Loss: 3.619 (3.39)  Time: 0.788s, 1299.22/s  (0.786s, 1302.11/s)  LR: 4.072e-04  Data: 0.010 (0.016)
Train: 338 [ 350/1251 ( 28%)]  Loss: 3.204 (3.37)  Time: 0.776s, 1320.34/s  (0.786s, 1303.34/s)  LR: 4.072e-04  Data: 0.010 (0.015)
Train: 338 [ 400/1251 ( 32%)]  Loss: 3.087 (3.34)  Time: 0.816s, 1255.42/s  (0.786s, 1303.19/s)  LR: 4.072e-04  Data: 0.011 (0.014)
Train: 338 [ 450/1251 ( 36%)]  Loss: 3.077 (3.31)  Time: 0.775s, 1321.93/s  (0.788s, 1299.64/s)  LR: 4.072e-04  Data: 0.009 (0.014)
Train: 338 [ 500/1251 ( 40%)]  Loss: 2.880 (3.27)  Time: 0.777s, 1318.46/s  (0.787s, 1301.44/s)  LR: 4.072e-04  Data: 0.010 (0.014)
Train: 338 [ 550/1251 ( 44%)]  Loss: 3.309 (3.27)  Time: 0.774s, 1322.97/s  (0.786s, 1302.08/s)  LR: 4.072e-04  Data: 0.010 (0.013)
Train: 338 [ 600/1251 ( 48%)]  Loss: 3.294 (3.28)  Time: 0.774s, 1322.34/s  (0.786s, 1302.44/s)  LR: 4.072e-04  Data: 0.009 (0.013)
Train: 338 [ 650/1251 ( 52%)]  Loss: 3.218 (3.27)  Time: 0.789s, 1297.54/s  (0.787s, 1300.84/s)  LR: 4.072e-04  Data: 0.014 (0.013)
Train: 338 [ 700/1251 ( 56%)]  Loss: 3.339 (3.28)  Time: 0.774s, 1323.46/s  (0.787s, 1301.06/s)  LR: 4.072e-04  Data: 0.010 (0.013)
Train: 338 [ 750/1251 ( 60%)]  Loss: 3.848 (3.31)  Time: 0.781s, 1311.62/s  (0.787s, 1301.56/s)  LR: 4.072e-04  Data: 0.010 (0.012)
Train: 338 [ 800/1251 ( 64%)]  Loss: 3.521 (3.32)  Time: 0.774s, 1322.91/s  (0.786s, 1302.27/s)  LR: 4.072e-04  Data: 0.011 (0.012)
Train: 338 [ 850/1251 ( 68%)]  Loss: 3.504 (3.33)  Time: 0.775s, 1322.08/s  (0.786s, 1302.11/s)  LR: 4.072e-04  Data: 0.009 (0.012)
Train: 338 [ 900/1251 ( 72%)]  Loss: 3.210 (3.33)  Time: 0.774s, 1323.01/s  (0.786s, 1302.20/s)  LR: 4.072e-04  Data: 0.010 (0.012)
Train: 338 [ 950/1251 ( 76%)]  Loss: 3.368 (3.33)  Time: 0.819s, 1251.06/s  (0.786s, 1302.33/s)  LR: 4.072e-04  Data: 0.009 (0.012)
Train: 338 [1000/1251 ( 80%)]  Loss: 3.220 (3.32)  Time: 0.775s, 1321.73/s  (0.786s, 1302.70/s)  LR: 4.072e-04  Data: 0.010 (0.012)
Train: 338 [1050/1251 ( 84%)]  Loss: 3.499 (3.33)  Time: 0.773s, 1324.20/s  (0.786s, 1303.24/s)  LR: 4.072e-04  Data: 0.010 (0.012)
Train: 338 [1100/1251 ( 88%)]  Loss: 3.312 (3.33)  Time: 0.772s, 1325.62/s  (0.785s, 1303.67/s)  LR: 4.072e-04  Data: 0.010 (0.012)
Train: 338 [1150/1251 ( 92%)]  Loss: 3.336 (3.33)  Time: 0.783s, 1308.20/s  (0.786s, 1303.52/s)  LR: 4.072e-04  Data: 0.010 (0.012)
Train: 338 [1200/1251 ( 96%)]  Loss: 3.514 (3.34)  Time: 0.796s, 1286.65/s  (0.785s, 1303.98/s)  LR: 4.072e-04  Data: 0.010 (0.011)
Train: 338 [1250/1251 (100%)]  Loss: 3.262 (3.34)  Time: 0.761s, 1345.58/s  (0.785s, 1303.92/s)  LR: 4.072e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.569 (1.569)  Loss:  0.7192 (0.7192)  Acc@1: 91.3086 (91.3086)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.194 (0.561)  Loss:  0.7319 (1.1837)  Acc@1: 86.2028 (76.5020)  Acc@5: 97.2877 (93.7180)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-331.pth.tar', 76.5620000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-338.pth.tar', 76.50200008300781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-323.pth.tar', 76.42800006103515)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-337.pth.tar', 76.41600005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-333.pth.tar', 76.34599992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-329.pth.tar', 76.33400001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-335.pth.tar', 76.32800005859374)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-332.pth.tar', 76.29800006103515)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-327.pth.tar', 76.27400005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-322.pth.tar', 76.25199998046875)

Train: 339 [   0/1251 (  0%)]  Loss: 3.062 (3.06)  Time: 2.280s,  449.22/s  (2.280s,  449.22/s)  LR: 4.046e-04  Data: 1.546 (1.546)
Train: 339 [  50/1251 (  4%)]  Loss: 3.386 (3.22)  Time: 0.785s, 1303.99/s  (0.825s, 1241.14/s)  LR: 4.046e-04  Data: 0.009 (0.045)
Train: 339 [ 100/1251 (  8%)]  Loss: 3.436 (3.29)  Time: 0.792s, 1293.43/s  (0.808s, 1266.78/s)  LR: 4.046e-04  Data: 0.009 (0.028)
Train: 339 [ 150/1251 ( 12%)]  Loss: 3.805 (3.42)  Time: 0.789s, 1297.56/s  (0.799s, 1281.03/s)  LR: 4.046e-04  Data: 0.012 (0.022)
Train: 339 [ 200/1251 ( 16%)]  Loss: 3.545 (3.45)  Time: 0.773s, 1324.93/s  (0.796s, 1286.92/s)  LR: 4.046e-04  Data: 0.010 (0.019)
Train: 339 [ 250/1251 ( 20%)]  Loss: 3.386 (3.44)  Time: 0.773s, 1324.96/s  (0.792s, 1292.11/s)  LR: 4.046e-04  Data: 0.009 (0.017)
Train: 339 [ 300/1251 ( 24%)]  Loss: 3.712 (3.48)  Time: 0.773s, 1324.31/s  (0.791s, 1294.71/s)  LR: 4.046e-04  Data: 0.009 (0.016)
Train: 339 [ 350/1251 ( 28%)]  Loss: 3.133 (3.43)  Time: 0.809s, 1265.02/s  (0.792s, 1292.86/s)  LR: 4.046e-04  Data: 0.010 (0.015)
Train: 339 [ 400/1251 ( 32%)]  Loss: 3.420 (3.43)  Time: 0.773s, 1325.56/s  (0.795s, 1288.86/s)  LR: 4.046e-04  Data: 0.010 (0.014)
Train: 339 [ 450/1251 ( 36%)]  Loss: 3.305 (3.42)  Time: 0.772s, 1326.49/s  (0.793s, 1291.87/s)  LR: 4.046e-04  Data: 0.009 (0.014)
Train: 339 [ 500/1251 ( 40%)]  Loss: 3.191 (3.40)  Time: 0.780s, 1313.10/s  (0.791s, 1294.00/s)  LR: 4.046e-04  Data: 0.010 (0.014)
Train: 339 [ 550/1251 ( 44%)]  Loss: 3.048 (3.37)  Time: 0.775s, 1321.16/s  (0.791s, 1295.33/s)  LR: 4.046e-04  Data: 0.010 (0.013)
Train: 339 [ 600/1251 ( 48%)]  Loss: 3.522 (3.38)  Time: 0.774s, 1323.73/s  (0.790s, 1296.68/s)  LR: 4.046e-04  Data: 0.010 (0.013)
Train: 339 [ 650/1251 ( 52%)]  Loss: 3.789 (3.41)  Time: 0.787s, 1301.11/s  (0.789s, 1297.53/s)  LR: 4.046e-04  Data: 0.010 (0.013)
Train: 339 [ 700/1251 ( 56%)]  Loss: 3.320 (3.40)  Time: 0.807s, 1268.69/s  (0.789s, 1297.28/s)  LR: 4.046e-04  Data: 0.010 (0.012)
Train: 339 [ 750/1251 ( 60%)]  Loss: 3.549 (3.41)  Time: 0.846s, 1210.76/s  (0.790s, 1297.00/s)  LR: 4.046e-04  Data: 0.009 (0.012)
Train: 339 [ 800/1251 ( 64%)]  Loss: 3.724 (3.43)  Time: 0.778s, 1316.41/s  (0.789s, 1297.94/s)  LR: 4.046e-04  Data: 0.010 (0.012)
Train: 339 [ 850/1251 ( 68%)]  Loss: 3.146 (3.42)  Time: 0.773s, 1325.14/s  (0.789s, 1298.09/s)  LR: 4.046e-04  Data: 0.010 (0.012)
Train: 339 [ 900/1251 ( 72%)]  Loss: 3.293 (3.41)  Time: 0.826s, 1239.44/s  (0.788s, 1299.06/s)  LR: 4.046e-04  Data: 0.009 (0.012)
Train: 339 [ 950/1251 ( 76%)]  Loss: 2.876 (3.38)  Time: 0.773s, 1324.51/s  (0.788s, 1299.33/s)  LR: 4.046e-04  Data: 0.009 (0.012)
Train: 339 [1000/1251 ( 80%)]  Loss: 3.371 (3.38)  Time: 0.773s, 1324.11/s  (0.788s, 1299.54/s)  LR: 4.046e-04  Data: 0.009 (0.012)
Train: 339 [1050/1251 ( 84%)]  Loss: 3.371 (3.38)  Time: 0.774s, 1322.91/s  (0.788s, 1300.18/s)  LR: 4.046e-04  Data: 0.009 (0.012)
Train: 339 [1100/1251 ( 88%)]  Loss: 3.406 (3.38)  Time: 0.783s, 1307.91/s  (0.788s, 1300.08/s)  LR: 4.046e-04  Data: 0.010 (0.012)
Train: 339 [1150/1251 ( 92%)]  Loss: 3.213 (3.38)  Time: 0.845s, 1211.20/s  (0.788s, 1300.02/s)  LR: 4.046e-04  Data: 0.010 (0.011)
Train: 339 [1200/1251 ( 96%)]  Loss: 3.600 (3.38)  Time: 0.776s, 1320.32/s  (0.788s, 1299.57/s)  LR: 4.046e-04  Data: 0.010 (0.011)
Train: 339 [1250/1251 (100%)]  Loss: 3.344 (3.38)  Time: 0.760s, 1347.85/s  (0.788s, 1299.90/s)  LR: 4.046e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.634 (1.634)  Loss:  0.7256 (0.7256)  Acc@1: 90.3320 (90.3320)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.194 (0.573)  Loss:  0.8081 (1.2480)  Acc@1: 86.2028 (76.2140)  Acc@5: 97.0519 (93.4480)
Train: 340 [   0/1251 (  0%)]  Loss: 3.357 (3.36)  Time: 2.260s,  453.17/s  (2.260s,  453.17/s)  LR: 4.021e-04  Data: 1.527 (1.527)
Train: 340 [  50/1251 (  4%)]  Loss: 2.984 (3.17)  Time: 0.827s, 1238.26/s  (0.827s, 1238.18/s)  LR: 4.021e-04  Data: 0.014 (0.045)
Train: 340 [ 100/1251 (  8%)]  Loss: 3.602 (3.31)  Time: 0.779s, 1314.78/s  (0.816s, 1254.77/s)  LR: 4.021e-04  Data: 0.010 (0.028)
Train: 340 [ 150/1251 ( 12%)]  Loss: 3.601 (3.39)  Time: 0.771s, 1328.55/s  (0.804s, 1272.86/s)  LR: 4.021e-04  Data: 0.009 (0.022)
Train: 340 [ 200/1251 ( 16%)]  Loss: 3.425 (3.39)  Time: 0.774s, 1323.07/s  (0.799s, 1281.28/s)  LR: 4.021e-04  Data: 0.010 (0.019)
Train: 340 [ 250/1251 ( 20%)]  Loss: 3.428 (3.40)  Time: 0.774s, 1322.78/s  (0.795s, 1287.48/s)  LR: 4.021e-04  Data: 0.012 (0.017)
Train: 340 [ 300/1251 ( 24%)]  Loss: 3.460 (3.41)  Time: 0.773s, 1325.06/s  (0.792s, 1292.23/s)  LR: 4.021e-04  Data: 0.010 (0.016)
Train: 340 [ 350/1251 ( 28%)]  Loss: 3.578 (3.43)  Time: 0.774s, 1323.67/s  (0.791s, 1294.68/s)  LR: 4.021e-04  Data: 0.009 (0.015)
Train: 340 [ 400/1251 ( 32%)]  Loss: 3.379 (3.42)  Time: 0.775s, 1320.73/s  (0.789s, 1297.62/s)  LR: 4.021e-04  Data: 0.010 (0.014)
Train: 340 [ 450/1251 ( 36%)]  Loss: 2.958 (3.38)  Time: 0.773s, 1324.64/s  (0.788s, 1299.21/s)  LR: 4.021e-04  Data: 0.010 (0.014)
Train: 340 [ 500/1251 ( 40%)]  Loss: 3.702 (3.41)  Time: 0.776s, 1318.94/s  (0.789s, 1298.65/s)  LR: 4.021e-04  Data: 0.009 (0.013)
Train: 340 [ 550/1251 ( 44%)]  Loss: 2.870 (3.36)  Time: 0.774s, 1323.60/s  (0.788s, 1300.07/s)  LR: 4.021e-04  Data: 0.009 (0.013)
Train: 340 [ 600/1251 ( 48%)]  Loss: 3.525 (3.37)  Time: 0.772s, 1325.68/s  (0.788s, 1299.22/s)  LR: 4.021e-04  Data: 0.009 (0.013)
Train: 340 [ 650/1251 ( 52%)]  Loss: 3.658 (3.39)  Time: 0.814s, 1257.31/s  (0.789s, 1298.31/s)  LR: 4.021e-04  Data: 0.011 (0.013)
Train: 340 [ 700/1251 ( 56%)]  Loss: 3.667 (3.41)  Time: 0.814s, 1257.97/s  (0.791s, 1294.57/s)  LR: 4.021e-04  Data: 0.011 (0.012)
Train: 340 [ 750/1251 ( 60%)]  Loss: 3.604 (3.42)  Time: 0.775s, 1321.68/s  (0.791s, 1295.34/s)  LR: 4.021e-04  Data: 0.010 (0.012)
Train: 340 [ 800/1251 ( 64%)]  Loss: 3.183 (3.41)  Time: 0.773s, 1324.61/s  (0.790s, 1296.40/s)  LR: 4.021e-04  Data: 0.010 (0.012)
Train: 340 [ 850/1251 ( 68%)]  Loss: 3.586 (3.42)  Time: 0.770s, 1329.98/s  (0.789s, 1297.48/s)  LR: 4.021e-04  Data: 0.010 (0.012)
Train: 340 [ 900/1251 ( 72%)]  Loss: 3.056 (3.40)  Time: 0.788s, 1299.41/s  (0.789s, 1298.33/s)  LR: 4.021e-04  Data: 0.009 (0.012)
Train: 340 [ 950/1251 ( 76%)]  Loss: 3.248 (3.39)  Time: 0.782s, 1309.74/s  (0.788s, 1298.77/s)  LR: 4.021e-04  Data: 0.010 (0.012)
Train: 340 [1000/1251 ( 80%)]  Loss: 3.304 (3.39)  Time: 0.777s, 1318.45/s  (0.788s, 1299.64/s)  LR: 4.021e-04  Data: 0.009 (0.012)
Train: 340 [1050/1251 ( 84%)]  Loss: 3.465 (3.39)  Time: 0.775s, 1322.06/s  (0.787s, 1300.45/s)  LR: 4.021e-04  Data: 0.009 (0.012)
Train: 340 [1100/1251 ( 88%)]  Loss: 3.604 (3.40)  Time: 0.782s, 1309.62/s  (0.787s, 1300.58/s)  LR: 4.021e-04  Data: 0.010 (0.011)
Train: 340 [1150/1251 ( 92%)]  Loss: 3.693 (3.41)  Time: 0.792s, 1293.19/s  (0.787s, 1301.23/s)  LR: 4.021e-04  Data: 0.009 (0.011)
Train: 340 [1200/1251 ( 96%)]  Loss: 3.409 (3.41)  Time: 0.781s, 1310.61/s  (0.787s, 1301.92/s)  LR: 4.021e-04  Data: 0.010 (0.011)
Train: 340 [1250/1251 (100%)]  Loss: 3.395 (3.41)  Time: 0.771s, 1328.13/s  (0.786s, 1302.33/s)  LR: 4.021e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.543 (1.543)  Loss:  0.8525 (0.8525)  Acc@1: 90.1367 (90.1367)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.194 (0.560)  Loss:  0.9585 (1.3108)  Acc@1: 83.7264 (76.1540)  Acc@5: 96.2264 (93.3640)
Train: 341 [   0/1251 (  0%)]  Loss: 3.415 (3.41)  Time: 2.203s,  464.77/s  (2.203s,  464.77/s)  LR: 3.995e-04  Data: 1.474 (1.474)
Train: 341 [  50/1251 (  4%)]  Loss: 3.610 (3.51)  Time: 0.774s, 1323.38/s  (0.816s, 1254.46/s)  LR: 3.995e-04  Data: 0.009 (0.046)
Train: 341 [ 100/1251 (  8%)]  Loss: 3.392 (3.47)  Time: 0.776s, 1320.17/s  (0.800s, 1279.89/s)  LR: 3.995e-04  Data: 0.010 (0.028)
Train: 341 [ 150/1251 ( 12%)]  Loss: 3.381 (3.45)  Time: 0.823s, 1244.51/s  (0.796s, 1285.65/s)  LR: 3.995e-04  Data: 0.010 (0.022)
Train: 341 [ 200/1251 ( 16%)]  Loss: 3.528 (3.47)  Time: 0.771s, 1327.46/s  (0.793s, 1291.87/s)  LR: 3.995e-04  Data: 0.010 (0.019)
Train: 341 [ 250/1251 ( 20%)]  Loss: 3.274 (3.43)  Time: 0.772s, 1326.07/s  (0.791s, 1294.73/s)  LR: 3.995e-04  Data: 0.009 (0.017)
Train: 341 [ 300/1251 ( 24%)]  Loss: 3.582 (3.45)  Time: 0.851s, 1203.41/s  (0.789s, 1298.30/s)  LR: 3.995e-04  Data: 0.009 (0.016)
Train: 341 [ 350/1251 ( 28%)]  Loss: 3.634 (3.48)  Time: 0.772s, 1325.74/s  (0.788s, 1299.16/s)  LR: 3.995e-04  Data: 0.010 (0.015)
Train: 341 [ 400/1251 ( 32%)]  Loss: 3.263 (3.45)  Time: 0.772s, 1326.67/s  (0.787s, 1300.46/s)  LR: 3.995e-04  Data: 0.010 (0.014)
Train: 341 [ 450/1251 ( 36%)]  Loss: 3.401 (3.45)  Time: 0.775s, 1321.61/s  (0.786s, 1302.05/s)  LR: 3.995e-04  Data: 0.010 (0.014)
Train: 341 [ 500/1251 ( 40%)]  Loss: 3.360 (3.44)  Time: 0.777s, 1318.22/s  (0.786s, 1303.56/s)  LR: 3.995e-04  Data: 0.010 (0.013)
Train: 341 [ 550/1251 ( 44%)]  Loss: 3.613 (3.45)  Time: 0.774s, 1323.32/s  (0.785s, 1304.48/s)  LR: 3.995e-04  Data: 0.009 (0.013)
Train: 341 [ 600/1251 ( 48%)]  Loss: 3.475 (3.46)  Time: 0.772s, 1325.58/s  (0.784s, 1305.65/s)  LR: 3.995e-04  Data: 0.009 (0.013)
Train: 341 [ 650/1251 ( 52%)]  Loss: 3.691 (3.47)  Time: 0.770s, 1329.58/s  (0.784s, 1306.42/s)  LR: 3.995e-04  Data: 0.009 (0.013)
Train: 341 [ 700/1251 ( 56%)]  Loss: 3.538 (3.48)  Time: 0.775s, 1320.80/s  (0.784s, 1306.19/s)  LR: 3.995e-04  Data: 0.010 (0.012)
Train: 341 [ 750/1251 ( 60%)]  Loss: 3.546 (3.48)  Time: 0.775s, 1321.88/s  (0.784s, 1306.54/s)  LR: 3.995e-04  Data: 0.009 (0.012)
Train: 341 [ 800/1251 ( 64%)]  Loss: 3.501 (3.48)  Time: 0.773s, 1324.88/s  (0.783s, 1307.19/s)  LR: 3.995e-04  Data: 0.009 (0.012)
Train: 341 [ 850/1251 ( 68%)]  Loss: 3.283 (3.47)  Time: 0.772s, 1326.64/s  (0.783s, 1307.75/s)  LR: 3.995e-04  Data: 0.009 (0.012)
Train: 341 [ 900/1251 ( 72%)]  Loss: 3.296 (3.46)  Time: 0.799s, 1281.69/s  (0.783s, 1308.22/s)  LR: 3.995e-04  Data: 0.010 (0.012)
Train: 341 [ 950/1251 ( 76%)]  Loss: 3.173 (3.45)  Time: 0.788s, 1298.81/s  (0.783s, 1308.25/s)  LR: 3.995e-04  Data: 0.009 (0.012)
Train: 341 [1000/1251 ( 80%)]  Loss: 3.102 (3.43)  Time: 0.770s, 1330.41/s  (0.783s, 1308.61/s)  LR: 3.995e-04  Data: 0.010 (0.012)
Train: 341 [1050/1251 ( 84%)]  Loss: 3.796 (3.45)  Time: 0.782s, 1310.04/s  (0.782s, 1308.99/s)  LR: 3.995e-04  Data: 0.009 (0.011)
Train: 341 [1100/1251 ( 88%)]  Loss: 3.482 (3.45)  Time: 0.774s, 1323.69/s  (0.782s, 1309.24/s)  LR: 3.995e-04  Data: 0.010 (0.011)
Train: 341 [1150/1251 ( 92%)]  Loss: 3.212 (3.44)  Time: 0.785s, 1305.05/s  (0.783s, 1308.28/s)  LR: 3.995e-04  Data: 0.009 (0.011)
Train: 341 [1200/1251 ( 96%)]  Loss: 3.479 (3.44)  Time: 0.823s, 1243.95/s  (0.783s, 1308.50/s)  LR: 3.995e-04  Data: 0.009 (0.011)
Train: 341 [1250/1251 (100%)]  Loss: 2.941 (3.42)  Time: 0.759s, 1349.03/s  (0.782s, 1308.98/s)  LR: 3.995e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.559 (1.559)  Loss:  0.6890 (0.6890)  Acc@1: 89.5508 (89.5508)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.194 (0.564)  Loss:  0.8335 (1.1904)  Acc@1: 86.3208 (76.4220)  Acc@5: 96.8160 (93.4360)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-331.pth.tar', 76.5620000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-338.pth.tar', 76.50200008300781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-323.pth.tar', 76.42800006103515)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-341.pth.tar', 76.42200000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-337.pth.tar', 76.41600005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-333.pth.tar', 76.34599992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-329.pth.tar', 76.33400001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-335.pth.tar', 76.32800005859374)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-332.pth.tar', 76.29800006103515)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-327.pth.tar', 76.27400005615235)

Train: 342 [   0/1251 (  0%)]  Loss: 3.122 (3.12)  Time: 2.242s,  456.83/s  (2.242s,  456.83/s)  LR: 3.970e-04  Data: 1.512 (1.512)
Train: 342 [  50/1251 (  4%)]  Loss: 3.407 (3.26)  Time: 0.782s, 1309.22/s  (0.813s, 1259.69/s)  LR: 3.970e-04  Data: 0.009 (0.043)
Train: 342 [ 100/1251 (  8%)]  Loss: 2.949 (3.16)  Time: 0.775s, 1320.67/s  (0.796s, 1286.70/s)  LR: 3.970e-04  Data: 0.010 (0.027)
Train: 342 [ 150/1251 ( 12%)]  Loss: 3.470 (3.24)  Time: 0.820s, 1248.81/s  (0.792s, 1292.74/s)  LR: 3.970e-04  Data: 0.011 (0.021)
Train: 342 [ 200/1251 ( 16%)]  Loss: 3.592 (3.31)  Time: 0.776s, 1320.21/s  (0.791s, 1294.74/s)  LR: 3.970e-04  Data: 0.010 (0.018)
Train: 342 [ 250/1251 ( 20%)]  Loss: 3.620 (3.36)  Time: 0.772s, 1326.13/s  (0.791s, 1295.14/s)  LR: 3.970e-04  Data: 0.010 (0.017)
Train: 342 [ 300/1251 ( 24%)]  Loss: 3.263 (3.35)  Time: 0.770s, 1330.64/s  (0.793s, 1291.81/s)  LR: 3.970e-04  Data: 0.010 (0.016)
Train: 342 [ 350/1251 ( 28%)]  Loss: 3.435 (3.36)  Time: 0.774s, 1322.41/s  (0.791s, 1294.26/s)  LR: 3.970e-04  Data: 0.009 (0.015)
Train: 342 [ 400/1251 ( 32%)]  Loss: 3.400 (3.36)  Time: 0.776s, 1320.08/s  (0.789s, 1297.25/s)  LR: 3.970e-04  Data: 0.010 (0.014)
Train: 342 [ 450/1251 ( 36%)]  Loss: 3.745 (3.40)  Time: 0.793s, 1290.87/s  (0.789s, 1298.26/s)  LR: 3.970e-04  Data: 0.010 (0.014)
Train: 342 [ 500/1251 ( 40%)]  Loss: 2.937 (3.36)  Time: 0.784s, 1305.32/s  (0.788s, 1299.75/s)  LR: 3.970e-04  Data: 0.011 (0.013)
Train: 342 [ 550/1251 ( 44%)]  Loss: 3.472 (3.37)  Time: 0.835s, 1226.43/s  (0.790s, 1295.86/s)  LR: 3.970e-04  Data: 0.009 (0.013)
Train: 342 [ 600/1251 ( 48%)]  Loss: 3.643 (3.39)  Time: 0.846s, 1209.71/s  (0.791s, 1294.65/s)  LR: 3.970e-04  Data: 0.010 (0.013)
Train: 342 [ 650/1251 ( 52%)]  Loss: 3.308 (3.38)  Time: 0.825s, 1241.90/s  (0.790s, 1295.51/s)  LR: 3.970e-04  Data: 0.009 (0.013)
Train: 342 [ 700/1251 ( 56%)]  Loss: 3.395 (3.38)  Time: 0.775s, 1322.02/s  (0.790s, 1296.68/s)  LR: 3.970e-04  Data: 0.010 (0.012)
Train: 342 [ 750/1251 ( 60%)]  Loss: 3.314 (3.38)  Time: 0.777s, 1317.31/s  (0.789s, 1297.91/s)  LR: 3.970e-04  Data: 0.010 (0.012)
Train: 342 [ 800/1251 ( 64%)]  Loss: 3.067 (3.36)  Time: 0.776s, 1319.50/s  (0.788s, 1299.15/s)  LR: 3.970e-04  Data: 0.010 (0.012)
Train: 342 [ 850/1251 ( 68%)]  Loss: 3.180 (3.35)  Time: 0.779s, 1315.21/s  (0.788s, 1299.45/s)  LR: 3.970e-04  Data: 0.011 (0.012)
Train: 342 [ 900/1251 ( 72%)]  Loss: 3.717 (3.37)  Time: 0.787s, 1300.53/s  (0.788s, 1299.96/s)  LR: 3.970e-04  Data: 0.010 (0.012)
Train: 342 [ 950/1251 ( 76%)]  Loss: 3.331 (3.37)  Time: 0.774s, 1323.28/s  (0.788s, 1299.74/s)  LR: 3.970e-04  Data: 0.010 (0.012)
Train: 342 [1000/1251 ( 80%)]  Loss: 3.255 (3.36)  Time: 0.774s, 1323.20/s  (0.788s, 1300.30/s)  LR: 3.970e-04  Data: 0.010 (0.012)
Train: 342 [1050/1251 ( 84%)]  Loss: 3.216 (3.36)  Time: 0.787s, 1301.69/s  (0.787s, 1301.01/s)  LR: 3.970e-04  Data: 0.011 (0.012)
Train: 342 [1100/1251 ( 88%)]  Loss: 3.396 (3.36)  Time: 0.827s, 1238.63/s  (0.787s, 1300.90/s)  LR: 3.970e-04  Data: 0.013 (0.012)
Train: 342 [1150/1251 ( 92%)]  Loss: 3.365 (3.36)  Time: 0.776s, 1318.77/s  (0.788s, 1299.24/s)  LR: 3.970e-04  Data: 0.010 (0.012)
Train: 342 [1200/1251 ( 96%)]  Loss: 3.202 (3.35)  Time: 0.781s, 1310.33/s  (0.788s, 1299.67/s)  LR: 3.970e-04  Data: 0.010 (0.012)
Train: 342 [1250/1251 (100%)]  Loss: 2.732 (3.33)  Time: 0.760s, 1346.55/s  (0.788s, 1299.59/s)  LR: 3.970e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.523 (1.523)  Loss:  0.7480 (0.7480)  Acc@1: 89.8438 (89.8438)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.194 (0.576)  Loss:  0.8516 (1.2844)  Acc@1: 87.1462 (76.2040)  Acc@5: 96.6981 (93.5160)
Train: 343 [   0/1251 (  0%)]  Loss: 3.115 (3.11)  Time: 2.303s,  444.54/s  (2.303s,  444.54/s)  LR: 3.945e-04  Data: 1.577 (1.577)
Train: 343 [  50/1251 (  4%)]  Loss: 3.490 (3.30)  Time: 0.773s, 1325.27/s  (0.820s, 1248.36/s)  LR: 3.945e-04  Data: 0.010 (0.046)
Train: 343 [ 100/1251 (  8%)]  Loss: 2.975 (3.19)  Time: 0.807s, 1269.58/s  (0.801s, 1278.90/s)  LR: 3.945e-04  Data: 0.010 (0.028)
Train: 343 [ 150/1251 ( 12%)]  Loss: 3.513 (3.27)  Time: 0.851s, 1203.93/s  (0.796s, 1286.25/s)  LR: 3.945e-04  Data: 0.009 (0.022)
Train: 343 [ 200/1251 ( 16%)]  Loss: 3.391 (3.30)  Time: 0.782s, 1310.15/s  (0.792s, 1293.08/s)  LR: 3.945e-04  Data: 0.010 (0.019)
Train: 343 [ 250/1251 ( 20%)]  Loss: 3.299 (3.30)  Time: 0.792s, 1292.92/s  (0.789s, 1297.77/s)  LR: 3.945e-04  Data: 0.014 (0.017)
Train: 343 [ 300/1251 ( 24%)]  Loss: 3.457 (3.32)  Time: 0.775s, 1321.35/s  (0.788s, 1299.93/s)  LR: 3.945e-04  Data: 0.010 (0.016)
Train: 343 [ 350/1251 ( 28%)]  Loss: 3.502 (3.34)  Time: 0.773s, 1324.07/s  (0.787s, 1301.47/s)  LR: 3.945e-04  Data: 0.010 (0.015)
Train: 343 [ 400/1251 ( 32%)]  Loss: 3.487 (3.36)  Time: 0.771s, 1327.57/s  (0.786s, 1302.55/s)  LR: 3.945e-04  Data: 0.010 (0.014)
Train: 343 [ 450/1251 ( 36%)]  Loss: 3.227 (3.35)  Time: 0.772s, 1326.28/s  (0.785s, 1304.06/s)  LR: 3.945e-04  Data: 0.009 (0.014)
Train: 343 [ 500/1251 ( 40%)]  Loss: 3.343 (3.35)  Time: 0.794s, 1289.85/s  (0.785s, 1304.20/s)  LR: 3.945e-04  Data: 0.010 (0.014)
Train: 343 [ 550/1251 ( 44%)]  Loss: 3.545 (3.36)  Time: 0.773s, 1324.02/s  (0.785s, 1303.73/s)  LR: 3.945e-04  Data: 0.010 (0.013)
Train: 343 [ 600/1251 ( 48%)]  Loss: 3.415 (3.37)  Time: 0.780s, 1312.75/s  (0.787s, 1301.80/s)  LR: 3.945e-04  Data: 0.009 (0.013)
Train: 343 [ 650/1251 ( 52%)]  Loss: 3.295 (3.36)  Time: 0.773s, 1323.98/s  (0.786s, 1302.75/s)  LR: 3.945e-04  Data: 0.009 (0.013)
Train: 343 [ 700/1251 ( 56%)]  Loss: 3.703 (3.38)  Time: 0.774s, 1323.81/s  (0.785s, 1304.02/s)  LR: 3.945e-04  Data: 0.010 (0.013)
Train: 343 [ 750/1251 ( 60%)]  Loss: 3.922 (3.42)  Time: 0.779s, 1315.00/s  (0.785s, 1304.21/s)  LR: 3.945e-04  Data: 0.011 (0.012)
Train: 343 [ 800/1251 ( 64%)]  Loss: 3.200 (3.40)  Time: 0.775s, 1321.73/s  (0.786s, 1303.53/s)  LR: 3.945e-04  Data: 0.010 (0.012)
Train: 343 [ 850/1251 ( 68%)]  Loss: 3.608 (3.42)  Time: 0.772s, 1326.16/s  (0.785s, 1304.01/s)  LR: 3.945e-04  Data: 0.010 (0.012)
Train: 343 [ 900/1251 ( 72%)]  Loss: 3.550 (3.42)  Time: 0.817s, 1252.61/s  (0.785s, 1303.74/s)  LR: 3.945e-04  Data: 0.011 (0.012)
Train: 343 [ 950/1251 ( 76%)]  Loss: 3.545 (3.43)  Time: 0.821s, 1247.28/s  (0.787s, 1301.52/s)  LR: 3.945e-04  Data: 0.009 (0.012)
Train: 343 [1000/1251 ( 80%)]  Loss: 3.302 (3.42)  Time: 0.771s, 1327.78/s  (0.787s, 1301.57/s)  LR: 3.945e-04  Data: 0.009 (0.012)
Train: 343 [1050/1251 ( 84%)]  Loss: 3.430 (3.42)  Time: 0.772s, 1325.71/s  (0.786s, 1302.20/s)  LR: 3.945e-04  Data: 0.010 (0.012)
Train: 343 [1100/1251 ( 88%)]  Loss: 3.470 (3.43)  Time: 0.776s, 1319.73/s  (0.786s, 1302.45/s)  LR: 3.945e-04  Data: 0.009 (0.012)
Train: 343 [1150/1251 ( 92%)]  Loss: 3.513 (3.43)  Time: 0.839s, 1221.20/s  (0.786s, 1303.08/s)  LR: 3.945e-04  Data: 0.009 (0.012)
Train: 343 [1200/1251 ( 96%)]  Loss: 3.654 (3.44)  Time: 0.770s, 1330.12/s  (0.786s, 1303.54/s)  LR: 3.945e-04  Data: 0.010 (0.011)
Train: 343 [1250/1251 (100%)]  Loss: 3.236 (3.43)  Time: 0.758s, 1350.28/s  (0.785s, 1303.96/s)  LR: 3.945e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.568 (1.568)  Loss:  0.6797 (0.6797)  Acc@1: 90.0391 (90.0391)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.194 (0.560)  Loss:  0.7700 (1.1989)  Acc@1: 86.2028 (76.3980)  Acc@5: 97.4057 (93.4900)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-331.pth.tar', 76.5620000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-338.pth.tar', 76.50200008300781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-323.pth.tar', 76.42800006103515)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-341.pth.tar', 76.42200000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-337.pth.tar', 76.41600005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-343.pth.tar', 76.39800008300782)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-333.pth.tar', 76.34599992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-329.pth.tar', 76.33400001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-335.pth.tar', 76.32800005859374)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-332.pth.tar', 76.29800006103515)

Train: 344 [   0/1251 (  0%)]  Loss: 3.552 (3.55)  Time: 2.504s,  408.90/s  (2.504s,  408.90/s)  LR: 3.920e-04  Data: 1.779 (1.779)
Train: 344 [  50/1251 (  4%)]  Loss: 3.363 (3.46)  Time: 0.778s, 1316.48/s  (0.818s, 1251.41/s)  LR: 3.920e-04  Data: 0.009 (0.045)
Train: 344 [ 100/1251 (  8%)]  Loss: 3.269 (3.39)  Time: 0.777s, 1318.64/s  (0.800s, 1279.34/s)  LR: 3.920e-04  Data: 0.010 (0.028)
Train: 344 [ 150/1251 ( 12%)]  Loss: 3.642 (3.46)  Time: 0.773s, 1323.86/s  (0.795s, 1288.38/s)  LR: 3.920e-04  Data: 0.009 (0.022)
Train: 344 [ 200/1251 ( 16%)]  Loss: 3.273 (3.42)  Time: 0.772s, 1326.24/s  (0.794s, 1289.97/s)  LR: 3.920e-04  Data: 0.010 (0.019)
Train: 344 [ 250/1251 ( 20%)]  Loss: 2.757 (3.31)  Time: 0.834s, 1227.56/s  (0.792s, 1292.35/s)  LR: 3.920e-04  Data: 0.009 (0.017)
Train: 344 [ 300/1251 ( 24%)]  Loss: 3.540 (3.34)  Time: 0.779s, 1314.17/s  (0.794s, 1289.51/s)  LR: 3.920e-04  Data: 0.009 (0.016)
Train: 344 [ 350/1251 ( 28%)]  Loss: 3.268 (3.33)  Time: 0.778s, 1315.74/s  (0.792s, 1293.17/s)  LR: 3.920e-04  Data: 0.010 (0.015)
Train: 344 [ 400/1251 ( 32%)]  Loss: 3.274 (3.33)  Time: 0.783s, 1308.52/s  (0.791s, 1295.12/s)  LR: 3.920e-04  Data: 0.010 (0.014)
Train: 344 [ 450/1251 ( 36%)]  Loss: 3.373 (3.33)  Time: 0.780s, 1312.08/s  (0.790s, 1296.34/s)  LR: 3.920e-04  Data: 0.010 (0.014)
Train: 344 [ 500/1251 ( 40%)]  Loss: 3.551 (3.35)  Time: 0.776s, 1318.81/s  (0.789s, 1298.06/s)  LR: 3.920e-04  Data: 0.010 (0.013)
Train: 344 [ 550/1251 ( 44%)]  Loss: 3.144 (3.33)  Time: 0.774s, 1323.10/s  (0.788s, 1299.15/s)  LR: 3.920e-04  Data: 0.010 (0.013)
Train: 344 [ 600/1251 ( 48%)]  Loss: 3.343 (3.33)  Time: 0.773s, 1325.33/s  (0.789s, 1298.18/s)  LR: 3.920e-04  Data: 0.011 (0.013)
Train: 344 [ 650/1251 ( 52%)]  Loss: 2.947 (3.31)  Time: 0.778s, 1316.00/s  (0.788s, 1299.63/s)  LR: 3.920e-04  Data: 0.010 (0.013)
Train: 344 [ 700/1251 ( 56%)]  Loss: 3.200 (3.30)  Time: 0.774s, 1323.51/s  (0.787s, 1300.97/s)  LR: 3.920e-04  Data: 0.010 (0.012)
Train: 344 [ 750/1251 ( 60%)]  Loss: 3.336 (3.30)  Time: 0.772s, 1327.24/s  (0.787s, 1300.65/s)  LR: 3.920e-04  Data: 0.010 (0.012)
Train: 344 [ 800/1251 ( 64%)]  Loss: 3.577 (3.32)  Time: 0.774s, 1323.22/s  (0.787s, 1301.56/s)  LR: 3.920e-04  Data: 0.009 (0.012)
Train: 344 [ 850/1251 ( 68%)]  Loss: 3.137 (3.31)  Time: 0.811s, 1262.44/s  (0.788s, 1299.95/s)  LR: 3.920e-04  Data: 0.009 (0.012)
Train: 344 [ 900/1251 ( 72%)]  Loss: 3.301 (3.31)  Time: 0.805s, 1271.53/s  (0.789s, 1298.16/s)  LR: 3.920e-04  Data: 0.010 (0.012)
Train: 344 [ 950/1251 ( 76%)]  Loss: 3.454 (3.32)  Time: 0.776s, 1319.56/s  (0.789s, 1297.93/s)  LR: 3.920e-04  Data: 0.010 (0.012)
Train: 344 [1000/1251 ( 80%)]  Loss: 3.242 (3.31)  Time: 0.774s, 1323.08/s  (0.788s, 1298.81/s)  LR: 3.920e-04  Data: 0.009 (0.012)
Train: 344 [1050/1251 ( 84%)]  Loss: 3.048 (3.30)  Time: 0.775s, 1320.86/s  (0.788s, 1298.77/s)  LR: 3.920e-04  Data: 0.009 (0.012)
Train: 344 [1100/1251 ( 88%)]  Loss: 3.170 (3.29)  Time: 0.777s, 1318.07/s  (0.788s, 1299.37/s)  LR: 3.920e-04  Data: 0.010 (0.012)
Train: 344 [1150/1251 ( 92%)]  Loss: 3.442 (3.30)  Time: 0.823s, 1243.91/s  (0.788s, 1299.95/s)  LR: 3.920e-04  Data: 0.009 (0.012)
Train: 344 [1200/1251 ( 96%)]  Loss: 3.396 (3.30)  Time: 0.832s, 1230.26/s  (0.788s, 1299.39/s)  LR: 3.920e-04  Data: 0.009 (0.011)
Train: 344 [1250/1251 (100%)]  Loss: 3.325 (3.30)  Time: 0.786s, 1303.40/s  (0.788s, 1299.98/s)  LR: 3.920e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.535 (1.535)  Loss:  0.7388 (0.7388)  Acc@1: 90.3320 (90.3320)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.194 (0.565)  Loss:  0.8721 (1.2867)  Acc@1: 84.5519 (76.4940)  Acc@5: 96.1085 (93.5500)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-331.pth.tar', 76.5620000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-338.pth.tar', 76.50200008300781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-344.pth.tar', 76.49400001220702)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-323.pth.tar', 76.42800006103515)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-341.pth.tar', 76.42200000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-337.pth.tar', 76.41600005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-343.pth.tar', 76.39800008300782)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-333.pth.tar', 76.34599992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-329.pth.tar', 76.33400001220703)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-335.pth.tar', 76.32800005859374)

Train: 345 [   0/1251 (  0%)]  Loss: 3.396 (3.40)  Time: 2.317s,  441.94/s  (2.317s,  441.94/s)  LR: 3.894e-04  Data: 1.584 (1.584)
Train: 345 [  50/1251 (  4%)]  Loss: 3.258 (3.33)  Time: 0.773s, 1324.38/s  (0.815s, 1256.46/s)  LR: 3.894e-04  Data: 0.010 (0.043)
Train: 345 [ 100/1251 (  8%)]  Loss: 3.198 (3.28)  Time: 0.772s, 1326.03/s  (0.798s, 1283.06/s)  LR: 3.894e-04  Data: 0.010 (0.027)
Train: 345 [ 150/1251 ( 12%)]  Loss: 3.442 (3.32)  Time: 0.773s, 1324.25/s  (0.792s, 1292.59/s)  LR: 3.894e-04  Data: 0.010 (0.021)
Train: 345 [ 200/1251 ( 16%)]  Loss: 3.444 (3.35)  Time: 0.770s, 1330.05/s  (0.789s, 1297.13/s)  LR: 3.894e-04  Data: 0.010 (0.018)
Train: 345 [ 250/1251 ( 20%)]  Loss: 3.494 (3.37)  Time: 0.773s, 1324.73/s  (0.787s, 1300.67/s)  LR: 3.894e-04  Data: 0.009 (0.017)
Train: 345 [ 300/1251 ( 24%)]  Loss: 3.367 (3.37)  Time: 0.773s, 1323.94/s  (0.786s, 1302.31/s)  LR: 3.894e-04  Data: 0.010 (0.016)
Train: 345 [ 350/1251 ( 28%)]  Loss: 3.568 (3.40)  Time: 0.774s, 1323.10/s  (0.785s, 1304.62/s)  LR: 3.894e-04  Data: 0.009 (0.015)
Train: 345 [ 400/1251 ( 32%)]  Loss: 3.255 (3.38)  Time: 0.702s, 1457.79/s  (0.784s, 1306.39/s)  LR: 3.894e-04  Data: 0.009 (0.014)
Train: 345 [ 450/1251 ( 36%)]  Loss: 3.596 (3.40)  Time: 0.771s, 1327.81/s  (0.784s, 1306.71/s)  LR: 3.894e-04  Data: 0.009 (0.014)
Train: 345 [ 500/1251 ( 40%)]  Loss: 3.563 (3.42)  Time: 0.776s, 1319.85/s  (0.783s, 1307.38/s)  LR: 3.894e-04  Data: 0.009 (0.013)
Train: 345 [ 550/1251 ( 44%)]  Loss: 3.481 (3.42)  Time: 0.815s, 1256.78/s  (0.784s, 1306.68/s)  LR: 3.894e-04  Data: 0.011 (0.013)
Train: 345 [ 600/1251 ( 48%)]  Loss: 3.412 (3.42)  Time: 0.781s, 1311.32/s  (0.784s, 1305.83/s)  LR: 3.894e-04  Data: 0.010 (0.013)
Train: 345 [ 650/1251 ( 52%)]  Loss: 3.529 (3.43)  Time: 0.772s, 1326.18/s  (0.784s, 1306.34/s)  LR: 3.894e-04  Data: 0.010 (0.012)
Train: 345 [ 700/1251 ( 56%)]  Loss: 3.598 (3.44)  Time: 0.779s, 1313.81/s  (0.783s, 1307.05/s)  LR: 3.894e-04  Data: 0.010 (0.012)
Train: 345 [ 750/1251 ( 60%)]  Loss: 3.781 (3.46)  Time: 0.774s, 1322.15/s  (0.783s, 1307.61/s)  LR: 3.894e-04  Data: 0.010 (0.012)
Train: 345 [ 800/1251 ( 64%)]  Loss: 3.490 (3.46)  Time: 0.783s, 1307.12/s  (0.783s, 1308.00/s)  LR: 3.894e-04  Data: 0.010 (0.012)
Train: 345 [ 850/1251 ( 68%)]  Loss: 3.230 (3.45)  Time: 0.778s, 1316.37/s  (0.783s, 1308.38/s)  LR: 3.894e-04  Data: 0.010 (0.012)
Train: 345 [ 900/1251 ( 72%)]  Loss: 3.643 (3.46)  Time: 0.831s, 1232.66/s  (0.783s, 1307.47/s)  LR: 3.894e-04  Data: 0.009 (0.012)
Train: 345 [ 950/1251 ( 76%)]  Loss: 3.532 (3.46)  Time: 0.772s, 1325.82/s  (0.783s, 1307.63/s)  LR: 3.894e-04  Data: 0.010 (0.012)
Train: 345 [1000/1251 ( 80%)]  Loss: 3.064 (3.44)  Time: 0.792s, 1293.51/s  (0.783s, 1307.36/s)  LR: 3.894e-04  Data: 0.010 (0.012)
Train: 345 [1050/1251 ( 84%)]  Loss: 3.247 (3.44)  Time: 0.770s, 1329.42/s  (0.783s, 1307.68/s)  LR: 3.894e-04  Data: 0.010 (0.011)
Train: 345 [1100/1251 ( 88%)]  Loss: 3.721 (3.45)  Time: 0.772s, 1326.43/s  (0.783s, 1307.83/s)  LR: 3.894e-04  Data: 0.009 (0.011)
Train: 345 [1150/1251 ( 92%)]  Loss: 3.009 (3.43)  Time: 0.780s, 1312.95/s  (0.783s, 1308.04/s)  LR: 3.894e-04  Data: 0.011 (0.011)
Train: 345 [1200/1251 ( 96%)]  Loss: 3.551 (3.43)  Time: 0.848s, 1207.73/s  (0.783s, 1308.12/s)  LR: 3.894e-04  Data: 0.010 (0.011)
Train: 345 [1250/1251 (100%)]  Loss: 3.359 (3.43)  Time: 0.804s, 1274.02/s  (0.783s, 1308.14/s)  LR: 3.894e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.563 (1.563)  Loss:  0.6997 (0.6997)  Acc@1: 89.4531 (89.4531)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.194 (0.584)  Loss:  0.7876 (1.2231)  Acc@1: 85.2594 (76.1700)  Acc@5: 96.5802 (93.4240)
Train: 346 [   0/1251 (  0%)]  Loss: 3.370 (3.37)  Time: 2.334s,  438.69/s  (2.334s,  438.69/s)  LR: 3.869e-04  Data: 1.548 (1.548)
Train: 346 [  50/1251 (  4%)]  Loss: 3.371 (3.37)  Time: 0.776s, 1319.28/s  (0.811s, 1262.67/s)  LR: 3.869e-04  Data: 0.010 (0.042)
Train: 346 [ 100/1251 (  8%)]  Loss: 3.072 (3.27)  Time: 0.773s, 1324.32/s  (0.796s, 1286.84/s)  LR: 3.869e-04  Data: 0.009 (0.026)
Train: 346 [ 150/1251 ( 12%)]  Loss: 3.511 (3.33)  Time: 0.784s, 1306.52/s  (0.790s, 1296.25/s)  LR: 3.869e-04  Data: 0.010 (0.021)
Train: 346 [ 200/1251 ( 16%)]  Loss: 3.603 (3.39)  Time: 0.775s, 1321.18/s  (0.788s, 1298.90/s)  LR: 3.869e-04  Data: 0.010 (0.018)
Train: 346 [ 250/1251 ( 20%)]  Loss: 3.310 (3.37)  Time: 0.780s, 1312.60/s  (0.787s, 1301.50/s)  LR: 3.869e-04  Data: 0.010 (0.016)
Train: 346 [ 300/1251 ( 24%)]  Loss: 3.130 (3.34)  Time: 0.782s, 1310.13/s  (0.786s, 1303.07/s)  LR: 3.869e-04  Data: 0.009 (0.015)
Train: 346 [ 350/1251 ( 28%)]  Loss: 3.402 (3.35)  Time: 0.774s, 1322.60/s  (0.785s, 1304.88/s)  LR: 3.869e-04  Data: 0.009 (0.015)
Train: 346 [ 400/1251 ( 32%)]  Loss: 3.351 (3.35)  Time: 0.775s, 1321.59/s  (0.784s, 1306.31/s)  LR: 3.869e-04  Data: 0.010 (0.014)
Train: 346 [ 450/1251 ( 36%)]  Loss: 3.360 (3.35)  Time: 0.774s, 1323.67/s  (0.784s, 1305.98/s)  LR: 3.869e-04  Data: 0.010 (0.013)
Train: 346 [ 500/1251 ( 40%)]  Loss: 3.472 (3.36)  Time: 0.857s, 1195.53/s  (0.784s, 1306.66/s)  LR: 3.869e-04  Data: 0.010 (0.013)
Train: 346 [ 550/1251 ( 44%)]  Loss: 3.189 (3.35)  Time: 0.781s, 1311.60/s  (0.784s, 1306.77/s)  LR: 3.869e-04  Data: 0.009 (0.013)
Train: 346 [ 600/1251 ( 48%)]  Loss: 3.574 (3.36)  Time: 0.778s, 1316.72/s  (0.783s, 1307.20/s)  LR: 3.869e-04  Data: 0.010 (0.013)
Train: 346 [ 650/1251 ( 52%)]  Loss: 3.619 (3.38)  Time: 0.774s, 1323.50/s  (0.784s, 1305.54/s)  LR: 3.869e-04  Data: 0.010 (0.012)
Train: 346 [ 700/1251 ( 56%)]  Loss: 3.097 (3.36)  Time: 0.842s, 1216.68/s  (0.784s, 1306.20/s)  LR: 3.869e-04  Data: 0.009 (0.012)
Train: 346 [ 750/1251 ( 60%)]  Loss: 3.355 (3.36)  Time: 0.777s, 1318.11/s  (0.784s, 1306.93/s)  LR: 3.869e-04  Data: 0.010 (0.012)
Train: 346 [ 800/1251 ( 64%)]  Loss: 3.144 (3.35)  Time: 0.815s, 1257.18/s  (0.785s, 1304.39/s)  LR: 3.869e-04  Data: 0.011 (0.012)
Train: 346 [ 850/1251 ( 68%)]  Loss: 3.282 (3.35)  Time: 0.815s, 1257.10/s  (0.787s, 1301.21/s)  LR: 3.869e-04  Data: 0.011 (0.012)
Train: 346 [ 900/1251 ( 72%)]  Loss: 3.114 (3.33)  Time: 0.815s, 1256.16/s  (0.789s, 1298.41/s)  LR: 3.869e-04  Data: 0.011 (0.012)
Train: 346 [ 950/1251 ( 76%)]  Loss: 3.133 (3.32)  Time: 0.771s, 1327.37/s  (0.790s, 1296.97/s)  LR: 3.869e-04  Data: 0.009 (0.012)
Train: 346 [1000/1251 ( 80%)]  Loss: 3.391 (3.33)  Time: 0.785s, 1304.52/s  (0.789s, 1297.75/s)  LR: 3.869e-04  Data: 0.010 (0.012)
Train: 346 [1050/1251 ( 84%)]  Loss: 3.382 (3.33)  Time: 0.863s, 1186.93/s  (0.789s, 1298.22/s)  LR: 3.869e-04  Data: 0.009 (0.012)
Train: 346 [1100/1251 ( 88%)]  Loss: 3.419 (3.33)  Time: 0.773s, 1325.37/s  (0.790s, 1296.98/s)  LR: 3.869e-04  Data: 0.010 (0.011)
Train: 346 [1150/1251 ( 92%)]  Loss: 3.227 (3.33)  Time: 0.773s, 1323.97/s  (0.789s, 1297.58/s)  LR: 3.869e-04  Data: 0.010 (0.011)
Train: 346 [1200/1251 ( 96%)]  Loss: 3.299 (3.33)  Time: 0.771s, 1328.14/s  (0.789s, 1297.54/s)  LR: 3.869e-04  Data: 0.010 (0.011)
Train: 346 [1250/1251 (100%)]  Loss: 3.242 (3.32)  Time: 0.796s, 1286.11/s  (0.790s, 1296.92/s)  LR: 3.869e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.561 (1.561)  Loss:  0.6367 (0.6367)  Acc@1: 89.7461 (89.7461)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.194 (0.568)  Loss:  0.7842 (1.1958)  Acc@1: 86.6745 (76.7320)  Acc@5: 96.8160 (93.6100)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-346.pth.tar', 76.73200002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-331.pth.tar', 76.5620000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-338.pth.tar', 76.50200008300781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-344.pth.tar', 76.49400001220702)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-323.pth.tar', 76.42800006103515)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-341.pth.tar', 76.42200000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-337.pth.tar', 76.41600005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-343.pth.tar', 76.39800008300782)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-333.pth.tar', 76.34599992919922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-329.pth.tar', 76.33400001220703)

Train: 347 [   0/1251 (  0%)]  Loss: 3.733 (3.73)  Time: 2.213s,  462.78/s  (2.213s,  462.78/s)  LR: 3.844e-04  Data: 1.481 (1.481)
Train: 347 [  50/1251 (  4%)]  Loss: 3.473 (3.60)  Time: 0.814s, 1257.86/s  (0.818s, 1251.89/s)  LR: 3.844e-04  Data: 0.010 (0.043)
Train: 347 [ 100/1251 (  8%)]  Loss: 3.197 (3.47)  Time: 0.779s, 1314.66/s  (0.800s, 1279.23/s)  LR: 3.844e-04  Data: 0.010 (0.027)
Train: 347 [ 150/1251 ( 12%)]  Loss: 3.166 (3.39)  Time: 0.794s, 1289.21/s  (0.795s, 1287.33/s)  LR: 3.844e-04  Data: 0.009 (0.021)
Train: 347 [ 200/1251 ( 16%)]  Loss: 3.486 (3.41)  Time: 0.773s, 1324.01/s  (0.792s, 1293.46/s)  LR: 3.844e-04  Data: 0.010 (0.018)
Train: 347 [ 250/1251 ( 20%)]  Loss: 3.568 (3.44)  Time: 0.776s, 1319.69/s  (0.790s, 1296.75/s)  LR: 3.844e-04  Data: 0.009 (0.017)
Train: 347 [ 300/1251 ( 24%)]  Loss: 3.521 (3.45)  Time: 0.773s, 1325.37/s  (0.789s, 1297.84/s)  LR: 3.844e-04  Data: 0.010 (0.016)
Train: 347 [ 350/1251 ( 28%)]  Loss: 3.493 (3.45)  Time: 0.770s, 1329.33/s  (0.787s, 1300.37/s)  LR: 3.844e-04  Data: 0.009 (0.015)
Train: 347 [ 400/1251 ( 32%)]  Loss: 3.446 (3.45)  Time: 0.773s, 1324.49/s  (0.788s, 1299.11/s)  LR: 3.844e-04  Data: 0.010 (0.014)
Train: 347 [ 450/1251 ( 36%)]  Loss: 3.213 (3.43)  Time: 0.774s, 1322.83/s  (0.787s, 1300.68/s)  LR: 3.844e-04  Data: 0.010 (0.014)
Train: 347 [ 500/1251 ( 40%)]  Loss: 3.274 (3.42)  Time: 0.772s, 1326.27/s  (0.786s, 1302.01/s)  LR: 3.844e-04  Data: 0.010 (0.013)
Train: 347 [ 550/1251 ( 44%)]  Loss: 3.382 (3.41)  Time: 0.771s, 1327.56/s  (0.787s, 1300.47/s)  LR: 3.844e-04  Data: 0.009 (0.013)
Train: 347 [ 600/1251 ( 48%)]  Loss: 3.707 (3.44)  Time: 0.777s, 1318.34/s  (0.787s, 1300.66/s)  LR: 3.844e-04  Data: 0.012 (0.013)
Train: 347 [ 650/1251 ( 52%)]  Loss: 3.335 (3.43)  Time: 0.783s, 1308.22/s  (0.787s, 1300.36/s)  LR: 3.844e-04  Data: 0.010 (0.013)
Train: 347 [ 700/1251 ( 56%)]  Loss: 3.230 (3.41)  Time: 0.773s, 1324.23/s  (0.787s, 1301.54/s)  LR: 3.844e-04  Data: 0.009 (0.012)
Train: 347 [ 750/1251 ( 60%)]  Loss: 3.401 (3.41)  Time: 0.775s, 1322.12/s  (0.786s, 1302.35/s)  LR: 3.844e-04  Data: 0.010 (0.012)
Train: 347 [ 800/1251 ( 64%)]  Loss: 3.427 (3.41)  Time: 0.774s, 1323.36/s  (0.786s, 1303.37/s)  LR: 3.844e-04  Data: 0.009 (0.012)
Train: 347 [ 850/1251 ( 68%)]  Loss: 3.465 (3.42)  Time: 0.774s, 1322.34/s  (0.785s, 1304.09/s)  LR: 3.844e-04  Data: 0.010 (0.012)
Train: 347 [ 900/1251 ( 72%)]  Loss: 3.270 (3.41)  Time: 0.772s, 1326.43/s  (0.785s, 1304.84/s)  LR: 3.844e-04  Data: 0.009 (0.012)
Train: 347 [ 950/1251 ( 76%)]  Loss: 3.274 (3.40)  Time: 0.783s, 1307.79/s  (0.784s, 1305.39/s)  LR: 3.844e-04  Data: 0.010 (0.012)
Train: 347 [1000/1251 ( 80%)]  Loss: 3.330 (3.40)  Time: 0.813s, 1259.45/s  (0.784s, 1305.38/s)  LR: 3.844e-04  Data: 0.010 (0.012)
Train: 347 [1050/1251 ( 84%)]  Loss: 3.486 (3.40)  Time: 0.773s, 1324.98/s  (0.784s, 1305.90/s)  LR: 3.844e-04  Data: 0.009 (0.012)
Train: 347 [1100/1251 ( 88%)]  Loss: 3.611 (3.41)  Time: 0.773s, 1324.72/s  (0.784s, 1306.22/s)  LR: 3.844e-04  Data: 0.010 (0.011)
Train: 347 [1150/1251 ( 92%)]  Loss: 3.135 (3.40)  Time: 0.778s, 1315.92/s  (0.784s, 1306.44/s)  LR: 3.844e-04  Data: 0.010 (0.011)
Train: 347 [1200/1251 ( 96%)]  Loss: 3.339 (3.40)  Time: 0.787s, 1301.70/s  (0.784s, 1306.80/s)  LR: 3.844e-04  Data: 0.009 (0.011)
Train: 347 [1250/1251 (100%)]  Loss: 3.190 (3.39)  Time: 0.760s, 1347.18/s  (0.783s, 1307.20/s)  LR: 3.844e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.522 (1.522)  Loss:  0.7026 (0.7026)  Acc@1: 89.2578 (89.2578)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.194 (0.576)  Loss:  0.7993 (1.1749)  Acc@1: 84.7877 (76.3720)  Acc@5: 96.5802 (93.3620)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-346.pth.tar', 76.73200002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-331.pth.tar', 76.5620000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-338.pth.tar', 76.50200008300781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-344.pth.tar', 76.49400001220702)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-323.pth.tar', 76.42800006103515)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-341.pth.tar', 76.42200000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-337.pth.tar', 76.41600005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-343.pth.tar', 76.39800008300782)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-347.pth.tar', 76.37199998535156)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-333.pth.tar', 76.34599992919922)

Train: 348 [   0/1251 (  0%)]  Loss: 3.457 (3.46)  Time: 2.271s,  450.98/s  (2.271s,  450.98/s)  LR: 3.819e-04  Data: 1.536 (1.536)
Train: 348 [  50/1251 (  4%)]  Loss: 3.105 (3.28)  Time: 0.774s, 1323.56/s  (0.827s, 1238.79/s)  LR: 3.819e-04  Data: 0.010 (0.057)
Train: 348 [ 100/1251 (  8%)]  Loss: 3.210 (3.26)  Time: 0.772s, 1327.19/s  (0.804s, 1274.26/s)  LR: 3.819e-04  Data: 0.010 (0.034)
Train: 348 [ 150/1251 ( 12%)]  Loss: 3.291 (3.27)  Time: 0.788s, 1300.19/s  (0.795s, 1288.85/s)  LR: 3.819e-04  Data: 0.011 (0.026)
Train: 348 [ 200/1251 ( 16%)]  Loss: 3.040 (3.22)  Time: 0.777s, 1318.04/s  (0.801s, 1278.90/s)  LR: 3.819e-04  Data: 0.012 (0.022)
Train: 348 [ 250/1251 ( 20%)]  Loss: 3.455 (3.26)  Time: 0.801s, 1277.61/s  (0.798s, 1283.55/s)  LR: 3.819e-04  Data: 0.009 (0.020)
Train: 348 [ 300/1251 ( 24%)]  Loss: 3.540 (3.30)  Time: 0.789s, 1298.23/s  (0.796s, 1287.15/s)  LR: 3.819e-04  Data: 0.009 (0.018)
Train: 348 [ 350/1251 ( 28%)]  Loss: 3.459 (3.32)  Time: 0.784s, 1305.62/s  (0.793s, 1290.49/s)  LR: 3.819e-04  Data: 0.009 (0.017)
Train: 348 [ 400/1251 ( 32%)]  Loss: 3.265 (3.31)  Time: 0.772s, 1325.74/s  (0.792s, 1292.98/s)  LR: 3.819e-04  Data: 0.009 (0.016)
Train: 348 [ 450/1251 ( 36%)]  Loss: 3.484 (3.33)  Time: 0.783s, 1308.17/s  (0.790s, 1295.47/s)  LR: 3.819e-04  Data: 0.013 (0.015)
Train: 348 [ 500/1251 ( 40%)]  Loss: 3.306 (3.33)  Time: 0.789s, 1298.60/s  (0.790s, 1297.00/s)  LR: 3.819e-04  Data: 0.010 (0.015)
Train: 348 [ 550/1251 ( 44%)]  Loss: 3.403 (3.33)  Time: 0.772s, 1326.42/s  (0.789s, 1297.93/s)  LR: 3.819e-04  Data: 0.009 (0.014)
Train: 348 [ 600/1251 ( 48%)]  Loss: 3.494 (3.35)  Time: 0.775s, 1321.72/s  (0.788s, 1299.18/s)  LR: 3.819e-04  Data: 0.009 (0.014)
Train: 348 [ 650/1251 ( 52%)]  Loss: 3.352 (3.35)  Time: 0.785s, 1305.08/s  (0.788s, 1300.31/s)  LR: 3.819e-04  Data: 0.013 (0.014)
Train: 348 [ 700/1251 ( 56%)]  Loss: 3.531 (3.36)  Time: 0.777s, 1317.80/s  (0.788s, 1299.68/s)  LR: 3.819e-04  Data: 0.009 (0.013)
Train: 348 [ 750/1251 ( 60%)]  Loss: 3.248 (3.35)  Time: 0.773s, 1324.68/s  (0.787s, 1300.82/s)  LR: 3.819e-04  Data: 0.009 (0.013)
Train: 348 [ 800/1251 ( 64%)]  Loss: 3.045 (3.33)  Time: 0.819s, 1250.09/s  (0.787s, 1301.26/s)  LR: 3.819e-04  Data: 0.010 (0.013)
Train: 348 [ 850/1251 ( 68%)]  Loss: 3.547 (3.35)  Time: 0.776s, 1320.30/s  (0.787s, 1301.44/s)  LR: 3.819e-04  Data: 0.010 (0.013)
Train: 348 [ 900/1251 ( 72%)]  Loss: 3.493 (3.35)  Time: 0.776s, 1320.34/s  (0.787s, 1301.91/s)  LR: 3.819e-04  Data: 0.009 (0.012)
Train: 348 [ 950/1251 ( 76%)]  Loss: 3.621 (3.37)  Time: 0.774s, 1323.06/s  (0.786s, 1302.31/s)  LR: 3.819e-04  Data: 0.010 (0.012)
Train: 348 [1000/1251 ( 80%)]  Loss: 3.345 (3.37)  Time: 0.774s, 1323.28/s  (0.786s, 1302.83/s)  LR: 3.819e-04  Data: 0.009 (0.012)
Train: 348 [1050/1251 ( 84%)]  Loss: 3.446 (3.37)  Time: 0.773s, 1324.70/s  (0.786s, 1303.56/s)  LR: 3.819e-04  Data: 0.009 (0.012)
Train: 348 [1100/1251 ( 88%)]  Loss: 3.379 (3.37)  Time: 0.774s, 1322.43/s  (0.785s, 1304.27/s)  LR: 3.819e-04  Data: 0.009 (0.012)
Train: 348 [1150/1251 ( 92%)]  Loss: 3.559 (3.38)  Time: 0.776s, 1319.18/s  (0.785s, 1304.20/s)  LR: 3.819e-04  Data: 0.010 (0.012)
Train: 348 [1200/1251 ( 96%)]  Loss: 3.785 (3.39)  Time: 0.772s, 1325.89/s  (0.785s, 1304.74/s)  LR: 3.819e-04  Data: 0.009 (0.012)
Train: 348 [1250/1251 (100%)]  Loss: 3.355 (3.39)  Time: 0.761s, 1345.88/s  (0.785s, 1304.50/s)  LR: 3.819e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.636 (1.636)  Loss:  0.7197 (0.7197)  Acc@1: 88.6719 (88.6719)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.193 (0.563)  Loss:  0.7812 (1.2231)  Acc@1: 85.8491 (76.5440)  Acc@5: 96.9340 (93.4700)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-346.pth.tar', 76.73200002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-331.pth.tar', 76.5620000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-348.pth.tar', 76.54400005859375)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-338.pth.tar', 76.50200008300781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-344.pth.tar', 76.49400001220702)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-323.pth.tar', 76.42800006103515)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-341.pth.tar', 76.42200000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-337.pth.tar', 76.41600005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-343.pth.tar', 76.39800008300782)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-347.pth.tar', 76.37199998535156)

Train: 349 [   0/1251 (  0%)]  Loss: 3.411 (3.41)  Time: 2.517s,  406.91/s  (2.517s,  406.91/s)  LR: 3.794e-04  Data: 1.776 (1.776)
Train: 349 [  50/1251 (  4%)]  Loss: 3.347 (3.38)  Time: 0.774s, 1323.62/s  (0.823s, 1244.41/s)  LR: 3.794e-04  Data: 0.009 (0.050)
Train: 349 [ 100/1251 (  8%)]  Loss: 2.952 (3.24)  Time: 0.772s, 1327.25/s  (0.806s, 1269.97/s)  LR: 3.794e-04  Data: 0.009 (0.030)
Train: 349 [ 150/1251 ( 12%)]  Loss: 3.298 (3.25)  Time: 0.777s, 1317.27/s  (0.796s, 1286.04/s)  LR: 3.794e-04  Data: 0.009 (0.023)
Train: 349 [ 200/1251 ( 16%)]  Loss: 2.839 (3.17)  Time: 0.775s, 1321.94/s  (0.792s, 1293.45/s)  LR: 3.794e-04  Data: 0.010 (0.020)
Train: 349 [ 250/1251 ( 20%)]  Loss: 3.226 (3.18)  Time: 0.785s, 1305.25/s  (0.789s, 1297.29/s)  LR: 3.794e-04  Data: 0.010 (0.018)
Train: 349 [ 300/1251 ( 24%)]  Loss: 3.154 (3.18)  Time: 0.831s, 1232.89/s  (0.788s, 1298.78/s)  LR: 3.794e-04  Data: 0.009 (0.017)
Train: 349 [ 350/1251 ( 28%)]  Loss: 3.457 (3.21)  Time: 0.774s, 1322.90/s  (0.790s, 1296.67/s)  LR: 3.794e-04  Data: 0.010 (0.016)
Train: 349 [ 400/1251 ( 32%)]  Loss: 2.883 (3.17)  Time: 0.815s, 1256.95/s  (0.790s, 1296.83/s)  LR: 3.794e-04  Data: 0.009 (0.015)
Train: 349 [ 450/1251 ( 36%)]  Loss: 3.699 (3.23)  Time: 0.774s, 1322.40/s  (0.790s, 1296.27/s)  LR: 3.794e-04  Data: 0.009 (0.014)
Train: 349 [ 500/1251 ( 40%)]  Loss: 3.059 (3.21)  Time: 0.776s, 1320.32/s  (0.788s, 1298.73/s)  LR: 3.794e-04  Data: 0.009 (0.014)
Train: 349 [ 550/1251 ( 44%)]  Loss: 3.299 (3.22)  Time: 0.783s, 1307.76/s  (0.788s, 1299.52/s)  LR: 3.794e-04  Data: 0.009 (0.014)
Train: 349 [ 600/1251 ( 48%)]  Loss: 3.437 (3.24)  Time: 0.845s, 1211.42/s  (0.788s, 1299.18/s)  LR: 3.794e-04  Data: 0.009 (0.013)
Train: 349 [ 650/1251 ( 52%)]  Loss: 3.373 (3.25)  Time: 0.777s, 1317.61/s  (0.788s, 1300.29/s)  LR: 3.794e-04  Data: 0.010 (0.013)
Train: 349 [ 700/1251 ( 56%)]  Loss: 3.476 (3.26)  Time: 0.781s, 1310.71/s  (0.787s, 1301.48/s)  LR: 3.794e-04  Data: 0.009 (0.013)
Train: 349 [ 750/1251 ( 60%)]  Loss: 3.516 (3.28)  Time: 0.775s, 1321.47/s  (0.786s, 1302.31/s)  LR: 3.794e-04  Data: 0.009 (0.013)
Train: 349 [ 800/1251 ( 64%)]  Loss: 3.680 (3.30)  Time: 0.784s, 1306.61/s  (0.786s, 1303.07/s)  LR: 3.794e-04  Data: 0.010 (0.012)
Train: 349 [ 850/1251 ( 68%)]  Loss: 3.560 (3.31)  Time: 0.830s, 1233.80/s  (0.786s, 1303.61/s)  LR: 3.794e-04  Data: 0.009 (0.012)
Train: 349 [ 900/1251 ( 72%)]  Loss: 3.324 (3.32)  Time: 0.783s, 1308.62/s  (0.785s, 1304.51/s)  LR: 3.794e-04  Data: 0.009 (0.012)
Train: 349 [ 950/1251 ( 76%)]  Loss: 3.387 (3.32)  Time: 0.827s, 1238.44/s  (0.786s, 1303.41/s)  LR: 3.794e-04  Data: 0.015 (0.012)
Train: 349 [1000/1251 ( 80%)]  Loss: 3.557 (3.33)  Time: 0.774s, 1323.66/s  (0.786s, 1303.22/s)  LR: 3.794e-04  Data: 0.009 (0.012)
Train: 349 [1050/1251 ( 84%)]  Loss: 3.434 (3.33)  Time: 0.772s, 1326.14/s  (0.785s, 1303.80/s)  LR: 3.794e-04  Data: 0.009 (0.012)
Train: 349 [1100/1251 ( 88%)]  Loss: 3.414 (3.34)  Time: 0.780s, 1312.99/s  (0.786s, 1302.59/s)  LR: 3.794e-04  Data: 0.011 (0.012)
Train: 349 [1150/1251 ( 92%)]  Loss: 3.619 (3.35)  Time: 0.788s, 1299.91/s  (0.786s, 1302.88/s)  LR: 3.794e-04  Data: 0.009 (0.012)
Train: 349 [1200/1251 ( 96%)]  Loss: 3.700 (3.36)  Time: 0.808s, 1266.73/s  (0.786s, 1303.27/s)  LR: 3.794e-04  Data: 0.009 (0.011)
Train: 349 [1250/1251 (100%)]  Loss: 3.356 (3.36)  Time: 0.780s, 1312.98/s  (0.786s, 1303.45/s)  LR: 3.794e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.542 (1.542)  Loss:  0.8120 (0.8120)  Acc@1: 90.8203 (90.8203)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.194 (0.569)  Loss:  0.8120 (1.2647)  Acc@1: 86.7924 (76.3680)  Acc@5: 96.6981 (93.4940)
Train: 350 [   0/1251 (  0%)]  Loss: 3.627 (3.63)  Time: 2.332s,  439.05/s  (2.332s,  439.05/s)  LR: 3.769e-04  Data: 1.602 (1.602)
Train: 350 [  50/1251 (  4%)]  Loss: 3.569 (3.60)  Time: 0.782s, 1309.16/s  (0.817s, 1252.98/s)  LR: 3.769e-04  Data: 0.010 (0.046)
Train: 350 [ 100/1251 (  8%)]  Loss: 3.487 (3.56)  Time: 0.772s, 1327.24/s  (0.803s, 1274.68/s)  LR: 3.769e-04  Data: 0.010 (0.028)
Train: 350 [ 150/1251 ( 12%)]  Loss: 3.294 (3.49)  Time: 0.774s, 1322.92/s  (0.795s, 1288.43/s)  LR: 3.769e-04  Data: 0.009 (0.022)
Train: 350 [ 200/1251 ( 16%)]  Loss: 3.379 (3.47)  Time: 0.776s, 1319.89/s  (0.793s, 1291.56/s)  LR: 3.769e-04  Data: 0.009 (0.019)
Train: 350 [ 250/1251 ( 20%)]  Loss: 3.465 (3.47)  Time: 0.827s, 1237.93/s  (0.793s, 1291.11/s)  LR: 3.769e-04  Data: 0.013 (0.017)
Train: 350 [ 300/1251 ( 24%)]  Loss: 3.515 (3.48)  Time: 0.827s, 1238.85/s  (0.794s, 1290.44/s)  LR: 3.769e-04  Data: 0.014 (0.016)
Train: 350 [ 350/1251 ( 28%)]  Loss: 3.552 (3.49)  Time: 0.778s, 1316.06/s  (0.791s, 1294.16/s)  LR: 3.769e-04  Data: 0.012 (0.015)
Train: 350 [ 400/1251 ( 32%)]  Loss: 3.566 (3.49)  Time: 0.774s, 1323.76/s  (0.790s, 1295.91/s)  LR: 3.769e-04  Data: 0.010 (0.014)
Train: 350 [ 450/1251 ( 36%)]  Loss: 3.500 (3.50)  Time: 0.785s, 1304.56/s  (0.789s, 1297.16/s)  LR: 3.769e-04  Data: 0.010 (0.014)
Train: 350 [ 500/1251 ( 40%)]  Loss: 3.468 (3.49)  Time: 0.784s, 1305.30/s  (0.789s, 1297.43/s)  LR: 3.769e-04  Data: 0.009 (0.014)
Train: 350 [ 550/1251 ( 44%)]  Loss: 3.340 (3.48)  Time: 0.774s, 1323.57/s  (0.791s, 1294.24/s)  LR: 3.769e-04  Data: 0.010 (0.013)
Train: 350 [ 600/1251 ( 48%)]  Loss: 3.434 (3.48)  Time: 0.774s, 1323.73/s  (0.791s, 1293.83/s)  LR: 3.769e-04  Data: 0.012 (0.013)
Train: 350 [ 650/1251 ( 52%)]  Loss: 3.391 (3.47)  Time: 0.784s, 1306.59/s  (0.792s, 1293.64/s)  LR: 3.769e-04  Data: 0.010 (0.013)
Train: 350 [ 700/1251 ( 56%)]  Loss: 3.541 (3.48)  Time: 0.777s, 1317.18/s  (0.791s, 1294.40/s)  LR: 3.769e-04  Data: 0.009 (0.013)
Train: 350 [ 750/1251 ( 60%)]  Loss: 3.390 (3.47)  Time: 0.771s, 1327.55/s  (0.790s, 1295.93/s)  LR: 3.769e-04  Data: 0.008 (0.012)
Train: 350 [ 800/1251 ( 64%)]  Loss: 3.344 (3.46)  Time: 0.772s, 1326.16/s  (0.790s, 1296.16/s)  LR: 3.769e-04  Data: 0.010 (0.012)
Train: 350 [ 850/1251 ( 68%)]  Loss: 3.188 (3.45)  Time: 0.772s, 1326.96/s  (0.789s, 1297.27/s)  LR: 3.769e-04  Data: 0.009 (0.012)
Train: 350 [ 900/1251 ( 72%)]  Loss: 3.343 (3.44)  Time: 0.774s, 1322.26/s  (0.789s, 1298.12/s)  LR: 3.769e-04  Data: 0.010 (0.012)
Train: 350 [ 950/1251 ( 76%)]  Loss: 3.613 (3.45)  Time: 0.772s, 1326.56/s  (0.788s, 1298.94/s)  LR: 3.769e-04  Data: 0.009 (0.012)
Train: 350 [1000/1251 ( 80%)]  Loss: 3.664 (3.46)  Time: 0.783s, 1307.00/s  (0.788s, 1299.28/s)  LR: 3.769e-04  Data: 0.010 (0.012)
Train: 350 [1050/1251 ( 84%)]  Loss: 3.558 (3.46)  Time: 0.808s, 1267.18/s  (0.789s, 1297.55/s)  LR: 3.769e-04  Data: 0.009 (0.012)
Train: 350 [1100/1251 ( 88%)]  Loss: 3.338 (3.46)  Time: 0.812s, 1261.14/s  (0.789s, 1297.02/s)  LR: 3.769e-04  Data: 0.009 (0.012)
Train: 350 [1150/1251 ( 92%)]  Loss: 3.319 (3.45)  Time: 0.815s, 1257.16/s  (0.789s, 1297.07/s)  LR: 3.769e-04  Data: 0.011 (0.011)
Train: 350 [1200/1251 ( 96%)]  Loss: 3.220 (3.44)  Time: 0.827s, 1238.51/s  (0.789s, 1297.53/s)  LR: 3.769e-04  Data: 0.009 (0.011)
Train: 350 [1250/1251 (100%)]  Loss: 3.509 (3.45)  Time: 0.758s, 1350.81/s  (0.789s, 1298.06/s)  LR: 3.769e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.518 (1.518)  Loss:  0.6343 (0.6343)  Acc@1: 89.4531 (89.4531)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.194 (0.564)  Loss:  0.7627 (1.1672)  Acc@1: 86.4387 (76.8780)  Acc@5: 96.8160 (93.6140)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-350.pth.tar', 76.87799992675781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-346.pth.tar', 76.73200002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-331.pth.tar', 76.5620000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-348.pth.tar', 76.54400005859375)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-338.pth.tar', 76.50200008300781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-344.pth.tar', 76.49400001220702)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-323.pth.tar', 76.42800006103515)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-341.pth.tar', 76.42200000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-337.pth.tar', 76.41600005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-343.pth.tar', 76.39800008300782)

Train: 351 [   0/1251 (  0%)]  Loss: 2.918 (2.92)  Time: 2.341s,  437.41/s  (2.341s,  437.41/s)  LR: 3.744e-04  Data: 1.610 (1.610)
Train: 351 [  50/1251 (  4%)]  Loss: 3.292 (3.10)  Time: 0.774s, 1323.83/s  (0.815s, 1256.66/s)  LR: 3.744e-04  Data: 0.010 (0.046)
Train: 351 [ 100/1251 (  8%)]  Loss: 3.227 (3.15)  Time: 0.778s, 1316.63/s  (0.803s, 1275.88/s)  LR: 3.744e-04  Data: 0.009 (0.028)
Train: 351 [ 150/1251 ( 12%)]  Loss: 3.637 (3.27)  Time: 0.773s, 1325.36/s  (0.795s, 1287.86/s)  LR: 3.744e-04  Data: 0.010 (0.022)
Train: 351 [ 200/1251 ( 16%)]  Loss: 3.447 (3.30)  Time: 0.773s, 1325.48/s  (0.793s, 1291.03/s)  LR: 3.744e-04  Data: 0.010 (0.019)
Train: 351 [ 250/1251 ( 20%)]  Loss: 3.425 (3.32)  Time: 0.775s, 1321.54/s  (0.791s, 1294.86/s)  LR: 3.744e-04  Data: 0.010 (0.017)
Train: 351 [ 300/1251 ( 24%)]  Loss: 3.045 (3.28)  Time: 0.780s, 1312.62/s  (0.790s, 1295.79/s)  LR: 3.744e-04  Data: 0.009 (0.016)
Train: 351 [ 350/1251 ( 28%)]  Loss: 3.156 (3.27)  Time: 0.774s, 1322.90/s  (0.791s, 1294.41/s)  LR: 3.744e-04  Data: 0.009 (0.015)
Train: 351 [ 400/1251 ( 32%)]  Loss: 3.404 (3.28)  Time: 0.774s, 1323.30/s  (0.790s, 1296.32/s)  LR: 3.744e-04  Data: 0.009 (0.015)
Train: 351 [ 450/1251 ( 36%)]  Loss: 3.228 (3.28)  Time: 0.774s, 1322.66/s  (0.789s, 1297.60/s)  LR: 3.744e-04  Data: 0.010 (0.014)
Train: 351 [ 500/1251 ( 40%)]  Loss: 3.218 (3.27)  Time: 0.772s, 1325.75/s  (0.790s, 1296.22/s)  LR: 3.744e-04  Data: 0.010 (0.014)
Train: 351 [ 550/1251 ( 44%)]  Loss: 3.358 (3.28)  Time: 0.809s, 1265.73/s  (0.790s, 1296.68/s)  LR: 3.744e-04  Data: 0.009 (0.013)
Train: 351 [ 600/1251 ( 48%)]  Loss: 3.365 (3.29)  Time: 0.774s, 1323.61/s  (0.790s, 1295.57/s)  LR: 3.744e-04  Data: 0.010 (0.013)
Train: 351 [ 650/1251 ( 52%)]  Loss: 3.539 (3.30)  Time: 0.772s, 1326.90/s  (0.790s, 1296.78/s)  LR: 3.744e-04  Data: 0.010 (0.013)
Train: 351 [ 700/1251 ( 56%)]  Loss: 3.679 (3.33)  Time: 0.774s, 1322.30/s  (0.789s, 1297.83/s)  LR: 3.744e-04  Data: 0.010 (0.013)
Train: 351 [ 750/1251 ( 60%)]  Loss: 3.481 (3.34)  Time: 0.862s, 1188.41/s  (0.789s, 1298.43/s)  LR: 3.744e-04  Data: 0.009 (0.012)
Train: 351 [ 800/1251 ( 64%)]  Loss: 3.297 (3.34)  Time: 0.784s, 1306.83/s  (0.788s, 1299.31/s)  LR: 3.744e-04  Data: 0.009 (0.012)
Train: 351 [ 850/1251 ( 68%)]  Loss: 3.357 (3.34)  Time: 0.773s, 1325.29/s  (0.788s, 1300.18/s)  LR: 3.744e-04  Data: 0.009 (0.012)
Train: 351 [ 900/1251 ( 72%)]  Loss: 3.146 (3.33)  Time: 0.780s, 1313.00/s  (0.787s, 1301.10/s)  LR: 3.744e-04  Data: 0.009 (0.012)
Train: 351 [ 950/1251 ( 76%)]  Loss: 3.374 (3.33)  Time: 0.780s, 1313.28/s  (0.787s, 1301.82/s)  LR: 3.744e-04  Data: 0.010 (0.012)
Train: 351 [1000/1251 ( 80%)]  Loss: 3.592 (3.34)  Time: 0.772s, 1326.23/s  (0.787s, 1301.84/s)  LR: 3.744e-04  Data: 0.010 (0.012)
Train: 351 [1050/1251 ( 84%)]  Loss: 3.388 (3.34)  Time: 0.869s, 1178.17/s  (0.787s, 1301.50/s)  LR: 3.744e-04  Data: 0.009 (0.012)
Train: 351 [1100/1251 ( 88%)]  Loss: 3.340 (3.34)  Time: 0.778s, 1315.86/s  (0.787s, 1300.95/s)  LR: 3.744e-04  Data: 0.010 (0.012)
Train: 351 [1150/1251 ( 92%)]  Loss: 3.452 (3.35)  Time: 0.774s, 1323.25/s  (0.787s, 1301.54/s)  LR: 3.744e-04  Data: 0.010 (0.011)
Train: 351 [1200/1251 ( 96%)]  Loss: 3.324 (3.35)  Time: 0.778s, 1316.03/s  (0.786s, 1302.29/s)  LR: 3.744e-04  Data: 0.010 (0.011)
Train: 351 [1250/1251 (100%)]  Loss: 3.543 (3.36)  Time: 0.760s, 1347.71/s  (0.786s, 1302.71/s)  LR: 3.744e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.609 (1.609)  Loss:  0.8247 (0.8247)  Acc@1: 89.2578 (89.2578)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.194 (0.570)  Loss:  0.8462 (1.2906)  Acc@1: 87.1462 (76.7000)  Acc@5: 97.0519 (93.6860)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-350.pth.tar', 76.87799992675781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-346.pth.tar', 76.73200002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-351.pth.tar', 76.70000010498048)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-331.pth.tar', 76.5620000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-348.pth.tar', 76.54400005859375)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-338.pth.tar', 76.50200008300781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-344.pth.tar', 76.49400001220702)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-323.pth.tar', 76.42800006103515)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-341.pth.tar', 76.42200000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-337.pth.tar', 76.41600005615234)

Train: 352 [   0/1251 (  0%)]  Loss: 3.383 (3.38)  Time: 2.248s,  455.55/s  (2.248s,  455.55/s)  LR: 3.719e-04  Data: 1.513 (1.513)
Train: 352 [  50/1251 (  4%)]  Loss: 3.090 (3.24)  Time: 0.787s, 1301.03/s  (0.813s, 1259.17/s)  LR: 3.719e-04  Data: 0.009 (0.046)
Train: 352 [ 100/1251 (  8%)]  Loss: 3.427 (3.30)  Time: 0.775s, 1321.79/s  (0.801s, 1279.06/s)  LR: 3.719e-04  Data: 0.009 (0.028)
Train: 352 [ 150/1251 ( 12%)]  Loss: 3.578 (3.37)  Time: 0.771s, 1328.95/s  (0.794s, 1289.36/s)  LR: 3.719e-04  Data: 0.010 (0.022)
Train: 352 [ 200/1251 ( 16%)]  Loss: 3.598 (3.42)  Time: 0.775s, 1321.50/s  (0.792s, 1292.35/s)  LR: 3.719e-04  Data: 0.009 (0.019)
Train: 352 [ 250/1251 ( 20%)]  Loss: 3.247 (3.39)  Time: 0.786s, 1302.51/s  (0.791s, 1294.11/s)  LR: 3.719e-04  Data: 0.013 (0.017)
Train: 352 [ 300/1251 ( 24%)]  Loss: 3.590 (3.42)  Time: 0.774s, 1323.74/s  (0.789s, 1297.57/s)  LR: 3.719e-04  Data: 0.009 (0.016)
Train: 352 [ 350/1251 ( 28%)]  Loss: 3.762 (3.46)  Time: 0.770s, 1330.54/s  (0.787s, 1300.98/s)  LR: 3.719e-04  Data: 0.010 (0.015)
Train: 352 [ 400/1251 ( 32%)]  Loss: 3.313 (3.44)  Time: 0.772s, 1326.98/s  (0.786s, 1303.28/s)  LR: 3.719e-04  Data: 0.010 (0.015)
Train: 352 [ 450/1251 ( 36%)]  Loss: 3.283 (3.43)  Time: 0.774s, 1323.19/s  (0.786s, 1303.33/s)  LR: 3.719e-04  Data: 0.010 (0.014)
Train: 352 [ 500/1251 ( 40%)]  Loss: 3.506 (3.43)  Time: 0.771s, 1328.64/s  (0.785s, 1303.79/s)  LR: 3.719e-04  Data: 0.009 (0.014)
Train: 352 [ 550/1251 ( 44%)]  Loss: 3.096 (3.41)  Time: 0.774s, 1323.44/s  (0.785s, 1304.12/s)  LR: 3.719e-04  Data: 0.010 (0.013)
Train: 352 [ 600/1251 ( 48%)]  Loss: 3.429 (3.41)  Time: 0.780s, 1313.27/s  (0.785s, 1304.96/s)  LR: 3.719e-04  Data: 0.009 (0.013)
Train: 352 [ 650/1251 ( 52%)]  Loss: 3.106 (3.39)  Time: 0.814s, 1257.51/s  (0.785s, 1303.89/s)  LR: 3.719e-04  Data: 0.011 (0.013)
Train: 352 [ 700/1251 ( 56%)]  Loss: 3.497 (3.39)  Time: 0.774s, 1323.04/s  (0.785s, 1303.84/s)  LR: 3.719e-04  Data: 0.010 (0.013)
Train: 352 [ 750/1251 ( 60%)]  Loss: 3.307 (3.39)  Time: 0.776s, 1320.04/s  (0.785s, 1304.71/s)  LR: 3.719e-04  Data: 0.010 (0.012)
Train: 352 [ 800/1251 ( 64%)]  Loss: 3.446 (3.39)  Time: 0.774s, 1323.30/s  (0.785s, 1305.04/s)  LR: 3.719e-04  Data: 0.010 (0.012)
Train: 352 [ 850/1251 ( 68%)]  Loss: 3.349 (3.39)  Time: 0.772s, 1326.52/s  (0.784s, 1305.47/s)  LR: 3.719e-04  Data: 0.010 (0.012)
Train: 352 [ 900/1251 ( 72%)]  Loss: 3.532 (3.40)  Time: 0.773s, 1324.50/s  (0.785s, 1304.46/s)  LR: 3.719e-04  Data: 0.009 (0.012)
Train: 352 [ 950/1251 ( 76%)]  Loss: 3.810 (3.42)  Time: 0.773s, 1325.51/s  (0.785s, 1303.80/s)  LR: 3.719e-04  Data: 0.010 (0.012)
Train: 352 [1000/1251 ( 80%)]  Loss: 3.738 (3.43)  Time: 0.773s, 1325.26/s  (0.785s, 1304.31/s)  LR: 3.719e-04  Data: 0.010 (0.012)
Train: 352 [1050/1251 ( 84%)]  Loss: 3.114 (3.42)  Time: 0.778s, 1316.78/s  (0.785s, 1304.98/s)  LR: 3.719e-04  Data: 0.011 (0.012)
Train: 352 [1100/1251 ( 88%)]  Loss: 3.521 (3.42)  Time: 0.786s, 1303.00/s  (0.784s, 1305.51/s)  LR: 3.719e-04  Data: 0.008 (0.012)
Train: 352 [1150/1251 ( 92%)]  Loss: 2.909 (3.40)  Time: 0.773s, 1324.65/s  (0.784s, 1305.66/s)  LR: 3.719e-04  Data: 0.009 (0.011)
Train: 352 [1200/1251 ( 96%)]  Loss: 3.762 (3.42)  Time: 0.781s, 1310.51/s  (0.784s, 1305.51/s)  LR: 3.719e-04  Data: 0.012 (0.011)
Train: 352 [1250/1251 (100%)]  Loss: 3.274 (3.41)  Time: 0.764s, 1340.16/s  (0.784s, 1305.30/s)  LR: 3.719e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.521 (1.521)  Loss:  0.8008 (0.8008)  Acc@1: 90.1367 (90.1367)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.194 (0.564)  Loss:  0.8105 (1.2287)  Acc@1: 85.7311 (76.7460)  Acc@5: 95.8726 (93.5720)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-350.pth.tar', 76.87799992675781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-352.pth.tar', 76.74599987792969)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-346.pth.tar', 76.73200002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-351.pth.tar', 76.70000010498048)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-331.pth.tar', 76.5620000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-348.pth.tar', 76.54400005859375)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-338.pth.tar', 76.50200008300781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-344.pth.tar', 76.49400001220702)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-323.pth.tar', 76.42800006103515)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-341.pth.tar', 76.42200000488282)

Train: 353 [   0/1251 (  0%)]  Loss: 3.488 (3.49)  Time: 2.353s,  435.20/s  (2.353s,  435.20/s)  LR: 3.694e-04  Data: 1.606 (1.606)
Train: 353 [  50/1251 (  4%)]  Loss: 3.086 (3.29)  Time: 0.786s, 1302.26/s  (0.818s, 1251.44/s)  LR: 3.694e-04  Data: 0.013 (0.046)
Train: 353 [ 100/1251 (  8%)]  Loss: 3.622 (3.40)  Time: 0.780s, 1312.67/s  (0.799s, 1281.46/s)  LR: 3.694e-04  Data: 0.010 (0.028)
Train: 353 [ 150/1251 ( 12%)]  Loss: 3.353 (3.39)  Time: 0.809s, 1265.88/s  (0.794s, 1289.32/s)  LR: 3.694e-04  Data: 0.009 (0.022)
Train: 353 [ 200/1251 ( 16%)]  Loss: 2.972 (3.30)  Time: 0.774s, 1323.33/s  (0.796s, 1286.98/s)  LR: 3.694e-04  Data: 0.010 (0.019)
Train: 353 [ 250/1251 ( 20%)]  Loss: 3.416 (3.32)  Time: 0.774s, 1323.23/s  (0.793s, 1292.09/s)  LR: 3.694e-04  Data: 0.010 (0.017)
Train: 353 [ 300/1251 ( 24%)]  Loss: 3.395 (3.33)  Time: 0.784s, 1306.80/s  (0.793s, 1290.91/s)  LR: 3.694e-04  Data: 0.010 (0.016)
Train: 353 [ 350/1251 ( 28%)]  Loss: 3.241 (3.32)  Time: 0.841s, 1217.01/s  (0.793s, 1291.10/s)  LR: 3.694e-04  Data: 0.010 (0.015)
Train: 353 [ 400/1251 ( 32%)]  Loss: 3.583 (3.35)  Time: 0.772s, 1325.92/s  (0.792s, 1293.54/s)  LR: 3.694e-04  Data: 0.010 (0.014)
Train: 353 [ 450/1251 ( 36%)]  Loss: 3.249 (3.34)  Time: 0.775s, 1320.60/s  (0.791s, 1294.44/s)  LR: 3.694e-04  Data: 0.011 (0.014)
Train: 353 [ 500/1251 ( 40%)]  Loss: 3.401 (3.35)  Time: 0.772s, 1326.00/s  (0.790s, 1296.20/s)  LR: 3.694e-04  Data: 0.010 (0.014)
Train: 353 [ 550/1251 ( 44%)]  Loss: 3.256 (3.34)  Time: 0.778s, 1316.26/s  (0.789s, 1297.17/s)  LR: 3.694e-04  Data: 0.011 (0.013)
Train: 353 [ 600/1251 ( 48%)]  Loss: 3.083 (3.32)  Time: 0.775s, 1322.05/s  (0.789s, 1298.57/s)  LR: 3.694e-04  Data: 0.011 (0.013)
Train: 353 [ 650/1251 ( 52%)]  Loss: 3.226 (3.31)  Time: 0.777s, 1317.57/s  (0.788s, 1299.42/s)  LR: 3.694e-04  Data: 0.010 (0.013)
Train: 353 [ 700/1251 ( 56%)]  Loss: 3.188 (3.30)  Time: 0.788s, 1299.70/s  (0.788s, 1300.23/s)  LR: 3.694e-04  Data: 0.009 (0.013)
Train: 353 [ 750/1251 ( 60%)]  Loss: 3.113 (3.29)  Time: 0.774s, 1323.02/s  (0.787s, 1301.13/s)  LR: 3.694e-04  Data: 0.010 (0.013)
Train: 353 [ 800/1251 ( 64%)]  Loss: 3.580 (3.31)  Time: 0.778s, 1317.04/s  (0.787s, 1301.26/s)  LR: 3.694e-04  Data: 0.010 (0.012)
Train: 353 [ 850/1251 ( 68%)]  Loss: 3.127 (3.30)  Time: 0.815s, 1256.21/s  (0.787s, 1301.63/s)  LR: 3.694e-04  Data: 0.009 (0.012)
Train: 353 [ 900/1251 ( 72%)]  Loss: 3.409 (3.30)  Time: 0.811s, 1261.95/s  (0.788s, 1299.57/s)  LR: 3.694e-04  Data: 0.009 (0.012)
Train: 353 [ 950/1251 ( 76%)]  Loss: 3.273 (3.30)  Time: 0.772s, 1326.78/s  (0.787s, 1300.33/s)  LR: 3.694e-04  Data: 0.009 (0.012)
Train: 353 [1000/1251 ( 80%)]  Loss: 3.362 (3.31)  Time: 0.776s, 1319.05/s  (0.787s, 1301.22/s)  LR: 3.694e-04  Data: 0.010 (0.012)
Train: 353 [1050/1251 ( 84%)]  Loss: 3.346 (3.31)  Time: 0.807s, 1268.70/s  (0.788s, 1299.98/s)  LR: 3.694e-04  Data: 0.009 (0.012)
Train: 353 [1100/1251 ( 88%)]  Loss: 3.337 (3.31)  Time: 0.772s, 1326.27/s  (0.789s, 1298.62/s)  LR: 3.694e-04  Data: 0.009 (0.012)
Train: 353 [1150/1251 ( 92%)]  Loss: 3.256 (3.31)  Time: 0.776s, 1320.38/s  (0.788s, 1299.34/s)  LR: 3.694e-04  Data: 0.009 (0.012)
Train: 353 [1200/1251 ( 96%)]  Loss: 3.324 (3.31)  Time: 0.769s, 1331.53/s  (0.788s, 1300.27/s)  LR: 3.694e-04  Data: 0.010 (0.012)
Train: 353 [1250/1251 (100%)]  Loss: 3.340 (3.31)  Time: 0.757s, 1352.12/s  (0.787s, 1300.72/s)  LR: 3.694e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.534 (1.534)  Loss:  0.8198 (0.8198)  Acc@1: 91.0156 (91.0156)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.194 (0.573)  Loss:  0.8794 (1.3153)  Acc@1: 85.8491 (76.4220)  Acc@5: 97.1698 (93.4560)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-350.pth.tar', 76.87799992675781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-352.pth.tar', 76.74599987792969)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-346.pth.tar', 76.73200002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-351.pth.tar', 76.70000010498048)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-331.pth.tar', 76.5620000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-348.pth.tar', 76.54400005859375)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-338.pth.tar', 76.50200008300781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-344.pth.tar', 76.49400001220702)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-323.pth.tar', 76.42800006103515)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-353.pth.tar', 76.42200005859375)

Train: 354 [   0/1251 (  0%)]  Loss: 3.497 (3.50)  Time: 2.349s,  435.95/s  (2.349s,  435.95/s)  LR: 3.669e-04  Data: 1.613 (1.613)
Train: 354 [  50/1251 (  4%)]  Loss: 3.254 (3.38)  Time: 0.784s, 1305.52/s  (0.815s, 1256.67/s)  LR: 3.669e-04  Data: 0.011 (0.047)
Train: 354 [ 100/1251 (  8%)]  Loss: 3.267 (3.34)  Time: 0.808s, 1268.00/s  (0.805s, 1272.34/s)  LR: 3.669e-04  Data: 0.009 (0.028)
Train: 354 [ 150/1251 ( 12%)]  Loss: 3.435 (3.36)  Time: 0.808s, 1267.79/s  (0.808s, 1267.40/s)  LR: 3.669e-04  Data: 0.010 (0.022)
Train: 354 [ 200/1251 ( 16%)]  Loss: 3.467 (3.38)  Time: 0.877s, 1167.98/s  (0.810s, 1263.82/s)  LR: 3.669e-04  Data: 0.010 (0.019)
Train: 354 [ 250/1251 ( 20%)]  Loss: 3.224 (3.36)  Time: 0.809s, 1265.20/s  (0.811s, 1261.94/s)  LR: 3.669e-04  Data: 0.010 (0.017)
Train: 354 [ 300/1251 ( 24%)]  Loss: 3.155 (3.33)  Time: 0.832s, 1231.22/s  (0.812s, 1261.43/s)  LR: 3.669e-04  Data: 0.009 (0.016)
Train: 354 [ 350/1251 ( 28%)]  Loss: 3.312 (3.33)  Time: 0.831s, 1232.51/s  (0.812s, 1260.89/s)  LR: 3.669e-04  Data: 0.010 (0.015)
Train: 354 [ 400/1251 ( 32%)]  Loss: 3.340 (3.33)  Time: 0.832s, 1231.17/s  (0.812s, 1260.46/s)  LR: 3.669e-04  Data: 0.010 (0.014)
Train: 354 [ 450/1251 ( 36%)]  Loss: 3.285 (3.32)  Time: 0.802s, 1277.18/s  (0.809s, 1265.48/s)  LR: 3.669e-04  Data: 0.010 (0.014)
Train: 354 [ 500/1251 ( 40%)]  Loss: 3.292 (3.32)  Time: 0.777s, 1318.40/s  (0.806s, 1270.03/s)  LR: 3.669e-04  Data: 0.010 (0.013)
Train: 354 [ 550/1251 ( 44%)]  Loss: 3.285 (3.32)  Time: 0.831s, 1231.60/s  (0.807s, 1268.54/s)  LR: 3.669e-04  Data: 0.009 (0.013)
Train: 354 [ 600/1251 ( 48%)]  Loss: 3.486 (3.33)  Time: 0.783s, 1308.12/s  (0.806s, 1270.56/s)  LR: 3.669e-04  Data: 0.011 (0.013)
Train: 354 [ 650/1251 ( 52%)]  Loss: 3.004 (3.31)  Time: 0.774s, 1322.74/s  (0.804s, 1274.24/s)  LR: 3.669e-04  Data: 0.009 (0.013)
Train: 354 [ 700/1251 ( 56%)]  Loss: 3.161 (3.30)  Time: 0.773s, 1325.31/s  (0.802s, 1276.76/s)  LR: 3.669e-04  Data: 0.010 (0.012)
Train: 354 [ 750/1251 ( 60%)]  Loss: 3.013 (3.28)  Time: 0.773s, 1324.21/s  (0.801s, 1277.94/s)  LR: 3.669e-04  Data: 0.009 (0.012)
Train: 354 [ 800/1251 ( 64%)]  Loss: 3.508 (3.29)  Time: 0.827s, 1238.25/s  (0.800s, 1279.55/s)  LR: 3.669e-04  Data: 0.013 (0.012)
Train: 354 [ 850/1251 ( 68%)]  Loss: 3.537 (3.31)  Time: 0.773s, 1324.58/s  (0.802s, 1277.32/s)  LR: 3.669e-04  Data: 0.009 (0.012)
Train: 354 [ 900/1251 ( 72%)]  Loss: 3.734 (3.33)  Time: 0.774s, 1323.47/s  (0.800s, 1279.35/s)  LR: 3.669e-04  Data: 0.009 (0.012)
Train: 354 [ 950/1251 ( 76%)]  Loss: 3.363 (3.33)  Time: 0.773s, 1324.50/s  (0.800s, 1280.56/s)  LR: 3.669e-04  Data: 0.009 (0.012)
Train: 354 [1000/1251 ( 80%)]  Loss: 3.385 (3.33)  Time: 0.773s, 1324.18/s  (0.798s, 1282.50/s)  LR: 3.669e-04  Data: 0.010 (0.012)
Train: 354 [1050/1251 ( 84%)]  Loss: 3.430 (3.34)  Time: 0.779s, 1314.68/s  (0.797s, 1284.16/s)  LR: 3.669e-04  Data: 0.010 (0.012)
Train: 354 [1100/1251 ( 88%)]  Loss: 3.503 (3.35)  Time: 0.787s, 1300.94/s  (0.797s, 1285.53/s)  LR: 3.669e-04  Data: 0.010 (0.012)
Train: 354 [1150/1251 ( 92%)]  Loss: 3.216 (3.34)  Time: 0.777s, 1317.22/s  (0.796s, 1286.87/s)  LR: 3.669e-04  Data: 0.009 (0.011)
Train: 354 [1200/1251 ( 96%)]  Loss: 3.272 (3.34)  Time: 0.773s, 1325.28/s  (0.795s, 1287.32/s)  LR: 3.669e-04  Data: 0.010 (0.011)
Train: 354 [1250/1251 (100%)]  Loss: 3.754 (3.35)  Time: 0.760s, 1347.61/s  (0.795s, 1288.25/s)  LR: 3.669e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.514 (1.514)  Loss:  0.7573 (0.7573)  Acc@1: 91.0156 (91.0156)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.194 (0.579)  Loss:  0.8726 (1.2792)  Acc@1: 85.6132 (76.4260)  Acc@5: 96.4623 (93.5020)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-350.pth.tar', 76.87799992675781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-352.pth.tar', 76.74599987792969)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-346.pth.tar', 76.73200002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-351.pth.tar', 76.70000010498048)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-331.pth.tar', 76.5620000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-348.pth.tar', 76.54400005859375)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-338.pth.tar', 76.50200008300781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-344.pth.tar', 76.49400001220702)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-323.pth.tar', 76.42800006103515)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-354.pth.tar', 76.42600008544922)

Train: 355 [   0/1251 (  0%)]  Loss: 3.180 (3.18)  Time: 2.301s,  444.99/s  (2.301s,  444.99/s)  LR: 3.644e-04  Data: 1.568 (1.568)
Train: 355 [  50/1251 (  4%)]  Loss: 3.171 (3.18)  Time: 0.777s, 1317.24/s  (0.816s, 1254.79/s)  LR: 3.644e-04  Data: 0.010 (0.044)
Train: 355 [ 100/1251 (  8%)]  Loss: 3.131 (3.16)  Time: 0.773s, 1325.46/s  (0.797s, 1285.08/s)  LR: 3.644e-04  Data: 0.009 (0.027)
Train: 355 [ 150/1251 ( 12%)]  Loss: 3.422 (3.23)  Time: 0.835s, 1226.16/s  (0.793s, 1291.91/s)  LR: 3.644e-04  Data: 0.009 (0.021)
Train: 355 [ 200/1251 ( 16%)]  Loss: 3.437 (3.27)  Time: 0.777s, 1317.49/s  (0.793s, 1291.65/s)  LR: 3.644e-04  Data: 0.010 (0.018)
Train: 355 [ 250/1251 ( 20%)]  Loss: 3.068 (3.23)  Time: 0.773s, 1325.16/s  (0.791s, 1294.60/s)  LR: 3.644e-04  Data: 0.010 (0.017)
Train: 355 [ 300/1251 ( 24%)]  Loss: 3.285 (3.24)  Time: 0.787s, 1300.42/s  (0.790s, 1296.06/s)  LR: 3.644e-04  Data: 0.009 (0.016)
Train: 355 [ 350/1251 ( 28%)]  Loss: 3.540 (3.28)  Time: 0.769s, 1332.11/s  (0.789s, 1297.41/s)  LR: 3.644e-04  Data: 0.009 (0.015)
Train: 355 [ 400/1251 ( 32%)]  Loss: 3.566 (3.31)  Time: 0.772s, 1326.52/s  (0.788s, 1298.98/s)  LR: 3.644e-04  Data: 0.009 (0.014)
Train: 355 [ 450/1251 ( 36%)]  Loss: 3.331 (3.31)  Time: 0.811s, 1262.71/s  (0.787s, 1300.50/s)  LR: 3.644e-04  Data: 0.010 (0.014)
Train: 355 [ 500/1251 ( 40%)]  Loss: 3.242 (3.31)  Time: 0.770s, 1329.17/s  (0.788s, 1300.21/s)  LR: 3.644e-04  Data: 0.010 (0.013)
Train: 355 [ 550/1251 ( 44%)]  Loss: 3.173 (3.30)  Time: 0.771s, 1327.60/s  (0.787s, 1301.62/s)  LR: 3.644e-04  Data: 0.010 (0.013)
Train: 355 [ 600/1251 ( 48%)]  Loss: 3.559 (3.32)  Time: 0.774s, 1323.84/s  (0.786s, 1302.84/s)  LR: 3.644e-04  Data: 0.010 (0.013)
Train: 355 [ 650/1251 ( 52%)]  Loss: 3.262 (3.31)  Time: 0.776s, 1319.31/s  (0.786s, 1303.47/s)  LR: 3.644e-04  Data: 0.010 (0.012)
Train: 355 [ 700/1251 ( 56%)]  Loss: 3.481 (3.32)  Time: 0.777s, 1317.86/s  (0.785s, 1303.89/s)  LR: 3.644e-04  Data: 0.010 (0.012)
Train: 355 [ 750/1251 ( 60%)]  Loss: 3.100 (3.31)  Time: 0.773s, 1324.34/s  (0.785s, 1304.26/s)  LR: 3.644e-04  Data: 0.009 (0.012)
Train: 355 [ 800/1251 ( 64%)]  Loss: 3.538 (3.32)  Time: 0.777s, 1317.43/s  (0.785s, 1305.03/s)  LR: 3.644e-04  Data: 0.009 (0.012)
Train: 355 [ 850/1251 ( 68%)]  Loss: 3.196 (3.32)  Time: 0.771s, 1328.58/s  (0.784s, 1305.70/s)  LR: 3.644e-04  Data: 0.009 (0.012)
Train: 355 [ 900/1251 ( 72%)]  Loss: 3.185 (3.31)  Time: 0.781s, 1310.95/s  (0.784s, 1306.47/s)  LR: 3.644e-04  Data: 0.010 (0.012)
Train: 355 [ 950/1251 ( 76%)]  Loss: 3.287 (3.31)  Time: 0.774s, 1323.37/s  (0.784s, 1306.95/s)  LR: 3.644e-04  Data: 0.010 (0.012)
Train: 355 [1000/1251 ( 80%)]  Loss: 3.491 (3.32)  Time: 0.781s, 1311.70/s  (0.783s, 1307.42/s)  LR: 3.644e-04  Data: 0.009 (0.011)
Train: 355 [1050/1251 ( 84%)]  Loss: 3.705 (3.33)  Time: 0.808s, 1266.70/s  (0.783s, 1307.47/s)  LR: 3.644e-04  Data: 0.009 (0.011)
Train: 355 [1100/1251 ( 88%)]  Loss: 3.302 (3.33)  Time: 0.806s, 1270.49/s  (0.785s, 1305.24/s)  LR: 3.644e-04  Data: 0.010 (0.011)
Train: 355 [1150/1251 ( 92%)]  Loss: 3.222 (3.33)  Time: 0.774s, 1323.40/s  (0.785s, 1304.50/s)  LR: 3.644e-04  Data: 0.010 (0.011)
Train: 355 [1200/1251 ( 96%)]  Loss: 3.364 (3.33)  Time: 0.777s, 1318.03/s  (0.785s, 1304.59/s)  LR: 3.644e-04  Data: 0.009 (0.011)
Train: 355 [1250/1251 (100%)]  Loss: 3.612 (3.34)  Time: 0.759s, 1348.45/s  (0.785s, 1305.03/s)  LR: 3.644e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.607 (1.607)  Loss:  0.7656 (0.7656)  Acc@1: 90.2344 (90.2344)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.194 (0.572)  Loss:  0.9102 (1.2361)  Acc@1: 85.2594 (77.0460)  Acc@5: 96.1085 (93.7440)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-355.pth.tar', 77.04600006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-350.pth.tar', 76.87799992675781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-352.pth.tar', 76.74599987792969)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-346.pth.tar', 76.73200002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-351.pth.tar', 76.70000010498048)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-331.pth.tar', 76.5620000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-348.pth.tar', 76.54400005859375)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-338.pth.tar', 76.50200008300781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-344.pth.tar', 76.49400001220702)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-323.pth.tar', 76.42800006103515)

Train: 356 [   0/1251 (  0%)]  Loss: 3.235 (3.23)  Time: 2.515s,  407.17/s  (2.515s,  407.17/s)  LR: 3.619e-04  Data: 1.788 (1.788)
Train: 356 [  50/1251 (  4%)]  Loss: 3.404 (3.32)  Time: 0.803s, 1275.87/s  (0.839s, 1220.01/s)  LR: 3.619e-04  Data: 0.009 (0.056)
Train: 356 [ 100/1251 (  8%)]  Loss: 3.368 (3.34)  Time: 0.773s, 1324.24/s  (0.810s, 1263.90/s)  LR: 3.619e-04  Data: 0.010 (0.033)
Train: 356 [ 150/1251 ( 12%)]  Loss: 3.442 (3.36)  Time: 0.785s, 1304.16/s  (0.800s, 1279.54/s)  LR: 3.619e-04  Data: 0.010 (0.025)
Train: 356 [ 200/1251 ( 16%)]  Loss: 3.540 (3.40)  Time: 0.778s, 1316.48/s  (0.796s, 1286.81/s)  LR: 3.619e-04  Data: 0.010 (0.022)
Train: 356 [ 250/1251 ( 20%)]  Loss: 3.405 (3.40)  Time: 0.840s, 1218.58/s  (0.794s, 1290.35/s)  LR: 3.619e-04  Data: 0.009 (0.019)
Train: 356 [ 300/1251 ( 24%)]  Loss: 3.405 (3.40)  Time: 0.774s, 1322.84/s  (0.791s, 1294.96/s)  LR: 3.619e-04  Data: 0.010 (0.018)
Train: 356 [ 350/1251 ( 28%)]  Loss: 3.493 (3.41)  Time: 0.781s, 1311.11/s  (0.791s, 1294.06/s)  LR: 3.619e-04  Data: 0.010 (0.016)
Train: 356 [ 400/1251 ( 32%)]  Loss: 3.662 (3.44)  Time: 0.772s, 1325.87/s  (0.790s, 1296.33/s)  LR: 3.619e-04  Data: 0.010 (0.016)
Train: 356 [ 450/1251 ( 36%)]  Loss: 2.986 (3.39)  Time: 0.770s, 1329.52/s  (0.789s, 1298.11/s)  LR: 3.619e-04  Data: 0.010 (0.015)
Train: 356 [ 500/1251 ( 40%)]  Loss: 3.529 (3.41)  Time: 0.779s, 1314.77/s  (0.789s, 1298.42/s)  LR: 3.619e-04  Data: 0.010 (0.015)
Train: 356 [ 550/1251 ( 44%)]  Loss: 3.181 (3.39)  Time: 0.832s, 1231.48/s  (0.788s, 1299.10/s)  LR: 3.619e-04  Data: 0.010 (0.014)
Train: 356 [ 600/1251 ( 48%)]  Loss: 3.199 (3.37)  Time: 0.812s, 1260.59/s  (0.788s, 1299.04/s)  LR: 3.619e-04  Data: 0.010 (0.014)
Train: 356 [ 650/1251 ( 52%)]  Loss: 3.441 (3.38)  Time: 0.782s, 1309.10/s  (0.787s, 1300.58/s)  LR: 3.619e-04  Data: 0.009 (0.013)
Train: 356 [ 700/1251 ( 56%)]  Loss: 3.423 (3.38)  Time: 0.850s, 1204.67/s  (0.788s, 1300.00/s)  LR: 3.619e-04  Data: 0.013 (0.013)
Train: 356 [ 750/1251 ( 60%)]  Loss: 3.546 (3.39)  Time: 0.816s, 1254.24/s  (0.787s, 1300.99/s)  LR: 3.619e-04  Data: 0.010 (0.013)
Train: 356 [ 800/1251 ( 64%)]  Loss: 3.437 (3.39)  Time: 0.774s, 1323.15/s  (0.787s, 1301.80/s)  LR: 3.619e-04  Data: 0.009 (0.013)
Train: 356 [ 850/1251 ( 68%)]  Loss: 3.642 (3.41)  Time: 0.773s, 1324.31/s  (0.786s, 1302.36/s)  LR: 3.619e-04  Data: 0.010 (0.013)
Train: 356 [ 900/1251 ( 72%)]  Loss: 3.373 (3.41)  Time: 0.816s, 1255.17/s  (0.787s, 1301.82/s)  LR: 3.619e-04  Data: 0.011 (0.012)
Train: 356 [ 950/1251 ( 76%)]  Loss: 3.343 (3.40)  Time: 0.776s, 1319.87/s  (0.787s, 1300.77/s)  LR: 3.619e-04  Data: 0.010 (0.012)
Train: 356 [1000/1251 ( 80%)]  Loss: 3.362 (3.40)  Time: 0.781s, 1311.70/s  (0.787s, 1301.56/s)  LR: 3.619e-04  Data: 0.010 (0.012)
Train: 356 [1050/1251 ( 84%)]  Loss: 3.564 (3.41)  Time: 0.785s, 1304.99/s  (0.787s, 1301.51/s)  LR: 3.619e-04  Data: 0.009 (0.012)
Train: 356 [1100/1251 ( 88%)]  Loss: 3.586 (3.42)  Time: 0.807s, 1269.29/s  (0.787s, 1300.64/s)  LR: 3.619e-04  Data: 0.010 (0.012)
Train: 356 [1150/1251 ( 92%)]  Loss: 3.206 (3.41)  Time: 0.773s, 1324.81/s  (0.787s, 1300.99/s)  LR: 3.619e-04  Data: 0.009 (0.012)
Train: 356 [1200/1251 ( 96%)]  Loss: 3.475 (3.41)  Time: 0.775s, 1321.53/s  (0.787s, 1300.83/s)  LR: 3.619e-04  Data: 0.010 (0.012)
Train: 356 [1250/1251 (100%)]  Loss: 3.221 (3.40)  Time: 0.819s, 1249.90/s  (0.787s, 1301.39/s)  LR: 3.619e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.554 (1.554)  Loss:  0.7573 (0.7573)  Acc@1: 90.3320 (90.3320)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.194 (0.571)  Loss:  0.8984 (1.2632)  Acc@1: 85.4953 (76.8220)  Acc@5: 96.8160 (93.7060)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-355.pth.tar', 77.04600006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-350.pth.tar', 76.87799992675781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-356.pth.tar', 76.82200003417968)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-352.pth.tar', 76.74599987792969)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-346.pth.tar', 76.73200002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-351.pth.tar', 76.70000010498048)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-331.pth.tar', 76.5620000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-348.pth.tar', 76.54400005859375)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-338.pth.tar', 76.50200008300781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-344.pth.tar', 76.49400001220702)

Train: 357 [   0/1251 (  0%)]  Loss: 3.265 (3.26)  Time: 2.364s,  433.08/s  (2.364s,  433.08/s)  LR: 3.595e-04  Data: 1.639 (1.639)
Train: 357 [  50/1251 (  4%)]  Loss: 3.148 (3.21)  Time: 0.786s, 1302.01/s  (0.815s, 1256.73/s)  LR: 3.595e-04  Data: 0.011 (0.044)
Train: 357 [ 100/1251 (  8%)]  Loss: 3.618 (3.34)  Time: 0.777s, 1318.41/s  (0.796s, 1286.71/s)  LR: 3.595e-04  Data: 0.010 (0.027)
Train: 357 [ 150/1251 ( 12%)]  Loss: 3.305 (3.33)  Time: 0.772s, 1326.08/s  (0.790s, 1296.67/s)  LR: 3.595e-04  Data: 0.009 (0.022)
Train: 357 [ 200/1251 ( 16%)]  Loss: 3.574 (3.38)  Time: 0.775s, 1321.46/s  (0.788s, 1299.46/s)  LR: 3.595e-04  Data: 0.010 (0.019)
Train: 357 [ 250/1251 ( 20%)]  Loss: 3.260 (3.36)  Time: 0.773s, 1324.47/s  (0.787s, 1301.74/s)  LR: 3.595e-04  Data: 0.010 (0.017)
Train: 357 [ 300/1251 ( 24%)]  Loss: 3.592 (3.39)  Time: 0.810s, 1264.76/s  (0.787s, 1301.95/s)  LR: 3.595e-04  Data: 0.010 (0.016)
Train: 357 [ 350/1251 ( 28%)]  Loss: 3.427 (3.40)  Time: 0.774s, 1323.21/s  (0.787s, 1300.99/s)  LR: 3.595e-04  Data: 0.010 (0.015)
Train: 357 [ 400/1251 ( 32%)]  Loss: 3.436 (3.40)  Time: 0.773s, 1323.93/s  (0.787s, 1300.36/s)  LR: 3.595e-04  Data: 0.009 (0.014)
Train: 357 [ 450/1251 ( 36%)]  Loss: 3.154 (3.38)  Time: 0.774s, 1323.82/s  (0.787s, 1301.30/s)  LR: 3.595e-04  Data: 0.010 (0.014)
Train: 357 [ 500/1251 ( 40%)]  Loss: 3.931 (3.43)  Time: 0.776s, 1319.57/s  (0.786s, 1303.10/s)  LR: 3.595e-04  Data: 0.010 (0.013)
Train: 357 [ 550/1251 ( 44%)]  Loss: 3.096 (3.40)  Time: 0.774s, 1322.95/s  (0.786s, 1302.81/s)  LR: 3.595e-04  Data: 0.010 (0.013)
Train: 357 [ 600/1251 ( 48%)]  Loss: 3.212 (3.39)  Time: 0.813s, 1259.50/s  (0.788s, 1298.87/s)  LR: 3.595e-04  Data: 0.009 (0.013)
Train: 357 [ 650/1251 ( 52%)]  Loss: 2.857 (3.35)  Time: 0.776s, 1319.11/s  (0.791s, 1294.75/s)  LR: 3.595e-04  Data: 0.010 (0.013)
Train: 357 [ 700/1251 ( 56%)]  Loss: 3.481 (3.36)  Time: 0.774s, 1323.45/s  (0.790s, 1295.75/s)  LR: 3.595e-04  Data: 0.009 (0.012)
Train: 357 [ 750/1251 ( 60%)]  Loss: 3.377 (3.36)  Time: 0.771s, 1328.10/s  (0.790s, 1296.78/s)  LR: 3.595e-04  Data: 0.009 (0.012)
Train: 357 [ 800/1251 ( 64%)]  Loss: 3.634 (3.37)  Time: 0.775s, 1321.18/s  (0.789s, 1297.66/s)  LR: 3.595e-04  Data: 0.010 (0.012)
Train: 357 [ 850/1251 ( 68%)]  Loss: 3.371 (3.37)  Time: 0.781s, 1311.09/s  (0.789s, 1298.53/s)  LR: 3.595e-04  Data: 0.009 (0.012)
Train: 357 [ 900/1251 ( 72%)]  Loss: 3.285 (3.37)  Time: 0.855s, 1197.52/s  (0.789s, 1298.07/s)  LR: 3.595e-04  Data: 0.009 (0.012)
Train: 357 [ 950/1251 ( 76%)]  Loss: 3.573 (3.38)  Time: 0.784s, 1306.34/s  (0.788s, 1299.02/s)  LR: 3.595e-04  Data: 0.009 (0.012)
Train: 357 [1000/1251 ( 80%)]  Loss: 3.289 (3.38)  Time: 0.814s, 1257.36/s  (0.788s, 1299.17/s)  LR: 3.595e-04  Data: 0.010 (0.012)
Train: 357 [1050/1251 ( 84%)]  Loss: 3.653 (3.39)  Time: 0.772s, 1326.61/s  (0.788s, 1299.76/s)  LR: 3.595e-04  Data: 0.009 (0.012)
Train: 357 [1100/1251 ( 88%)]  Loss: 3.517 (3.39)  Time: 0.773s, 1324.40/s  (0.788s, 1300.07/s)  LR: 3.595e-04  Data: 0.009 (0.011)
Train: 357 [1150/1251 ( 92%)]  Loss: 3.195 (3.39)  Time: 0.772s, 1325.83/s  (0.787s, 1300.78/s)  LR: 3.595e-04  Data: 0.009 (0.011)
Train: 357 [1200/1251 ( 96%)]  Loss: 3.501 (3.39)  Time: 0.773s, 1324.05/s  (0.787s, 1301.45/s)  LR: 3.595e-04  Data: 0.009 (0.011)
Train: 357 [1250/1251 (100%)]  Loss: 3.745 (3.40)  Time: 0.767s, 1335.36/s  (0.786s, 1302.07/s)  LR: 3.595e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.536 (1.536)  Loss:  0.7422 (0.7422)  Acc@1: 89.9414 (89.9414)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.194 (0.571)  Loss:  0.7920 (1.2620)  Acc@1: 86.2028 (76.4920)  Acc@5: 96.8160 (93.5860)
Train: 358 [   0/1251 (  0%)]  Loss: 3.203 (3.20)  Time: 2.547s,  401.98/s  (2.547s,  401.98/s)  LR: 3.570e-04  Data: 1.784 (1.784)
Train: 358 [  50/1251 (  4%)]  Loss: 3.294 (3.25)  Time: 0.770s, 1329.31/s  (0.816s, 1255.10/s)  LR: 3.570e-04  Data: 0.010 (0.045)
Train: 358 [ 100/1251 (  8%)]  Loss: 3.341 (3.28)  Time: 0.790s, 1295.56/s  (0.799s, 1281.35/s)  LR: 3.570e-04  Data: 0.010 (0.028)
Train: 358 [ 150/1251 ( 12%)]  Loss: 3.096 (3.23)  Time: 0.773s, 1325.54/s  (0.793s, 1291.43/s)  LR: 3.570e-04  Data: 0.009 (0.022)
Train: 358 [ 200/1251 ( 16%)]  Loss: 3.345 (3.26)  Time: 0.773s, 1325.14/s  (0.791s, 1295.11/s)  LR: 3.570e-04  Data: 0.009 (0.019)
Train: 358 [ 250/1251 ( 20%)]  Loss: 3.353 (3.27)  Time: 0.772s, 1325.92/s  (0.791s, 1294.00/s)  LR: 3.570e-04  Data: 0.010 (0.017)
Train: 358 [ 300/1251 ( 24%)]  Loss: 2.996 (3.23)  Time: 0.771s, 1327.51/s  (0.790s, 1297.02/s)  LR: 3.570e-04  Data: 0.010 (0.016)
Train: 358 [ 350/1251 ( 28%)]  Loss: 3.240 (3.23)  Time: 0.773s, 1324.51/s  (0.788s, 1299.30/s)  LR: 3.570e-04  Data: 0.009 (0.015)
Train: 358 [ 400/1251 ( 32%)]  Loss: 3.386 (3.25)  Time: 0.773s, 1323.98/s  (0.787s, 1301.46/s)  LR: 3.570e-04  Data: 0.009 (0.014)
Train: 358 [ 450/1251 ( 36%)]  Loss: 3.602 (3.29)  Time: 0.784s, 1305.62/s  (0.786s, 1303.04/s)  LR: 3.570e-04  Data: 0.009 (0.014)
Train: 358 [ 500/1251 ( 40%)]  Loss: 3.000 (3.26)  Time: 0.791s, 1294.97/s  (0.786s, 1303.31/s)  LR: 3.570e-04  Data: 0.009 (0.013)
Train: 358 [ 550/1251 ( 44%)]  Loss: 3.039 (3.24)  Time: 0.774s, 1323.39/s  (0.785s, 1303.95/s)  LR: 3.570e-04  Data: 0.010 (0.013)
Train: 358 [ 600/1251 ( 48%)]  Loss: 3.380 (3.25)  Time: 0.774s, 1323.63/s  (0.785s, 1305.18/s)  LR: 3.570e-04  Data: 0.010 (0.013)
Train: 358 [ 650/1251 ( 52%)]  Loss: 3.187 (3.25)  Time: 0.782s, 1309.94/s  (0.784s, 1306.22/s)  LR: 3.570e-04  Data: 0.009 (0.013)
Train: 358 [ 700/1251 ( 56%)]  Loss: 3.219 (3.25)  Time: 0.774s, 1323.14/s  (0.784s, 1306.45/s)  LR: 3.570e-04  Data: 0.011 (0.012)
Train: 358 [ 750/1251 ( 60%)]  Loss: 3.589 (3.27)  Time: 0.773s, 1324.12/s  (0.784s, 1305.79/s)  LR: 3.570e-04  Data: 0.009 (0.012)
Train: 358 [ 800/1251 ( 64%)]  Loss: 3.110 (3.26)  Time: 0.772s, 1326.32/s  (0.784s, 1306.30/s)  LR: 3.570e-04  Data: 0.010 (0.012)
Train: 358 [ 850/1251 ( 68%)]  Loss: 3.433 (3.27)  Time: 0.773s, 1325.00/s  (0.784s, 1306.18/s)  LR: 3.570e-04  Data: 0.010 (0.012)
Train: 358 [ 900/1251 ( 72%)]  Loss: 3.422 (3.28)  Time: 0.791s, 1294.55/s  (0.784s, 1305.82/s)  LR: 3.570e-04  Data: 0.010 (0.012)
Train: 358 [ 950/1251 ( 76%)]  Loss: 3.403 (3.28)  Time: 0.793s, 1291.02/s  (0.784s, 1305.80/s)  LR: 3.570e-04  Data: 0.011 (0.012)
Train: 358 [1000/1251 ( 80%)]  Loss: 2.996 (3.27)  Time: 0.773s, 1324.56/s  (0.784s, 1306.47/s)  LR: 3.570e-04  Data: 0.010 (0.012)
Train: 358 [1050/1251 ( 84%)]  Loss: 3.017 (3.26)  Time: 0.774s, 1322.60/s  (0.784s, 1306.31/s)  LR: 3.570e-04  Data: 0.009 (0.012)
Train: 358 [1100/1251 ( 88%)]  Loss: 3.665 (3.27)  Time: 0.787s, 1301.84/s  (0.784s, 1305.66/s)  LR: 3.570e-04  Data: 0.017 (0.012)
Train: 358 [1150/1251 ( 92%)]  Loss: 3.110 (3.27)  Time: 0.775s, 1320.77/s  (0.784s, 1305.56/s)  LR: 3.570e-04  Data: 0.011 (0.012)
Train: 358 [1200/1251 ( 96%)]  Loss: 2.917 (3.25)  Time: 0.774s, 1323.44/s  (0.784s, 1305.38/s)  LR: 3.570e-04  Data: 0.009 (0.011)
Train: 358 [1250/1251 (100%)]  Loss: 3.531 (3.26)  Time: 0.762s, 1344.68/s  (0.784s, 1305.60/s)  LR: 3.570e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.532 (1.532)  Loss:  0.7788 (0.7788)  Acc@1: 89.7461 (89.7461)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.194 (0.565)  Loss:  0.8696 (1.2343)  Acc@1: 84.3160 (76.5960)  Acc@5: 95.4009 (93.4860)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-355.pth.tar', 77.04600006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-350.pth.tar', 76.87799992675781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-356.pth.tar', 76.82200003417968)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-352.pth.tar', 76.74599987792969)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-346.pth.tar', 76.73200002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-351.pth.tar', 76.70000010498048)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-358.pth.tar', 76.5960000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-331.pth.tar', 76.5620000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-348.pth.tar', 76.54400005859375)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-338.pth.tar', 76.50200008300781)

Train: 359 [   0/1251 (  0%)]  Loss: 3.154 (3.15)  Time: 2.465s,  415.47/s  (2.465s,  415.47/s)  LR: 3.545e-04  Data: 1.730 (1.730)
Train: 359 [  50/1251 (  4%)]  Loss: 3.671 (3.41)  Time: 0.771s, 1328.56/s  (0.816s, 1255.67/s)  LR: 3.545e-04  Data: 0.010 (0.045)
Train: 359 [ 100/1251 (  8%)]  Loss: 3.570 (3.46)  Time: 0.773s, 1324.09/s  (0.797s, 1285.45/s)  LR: 3.545e-04  Data: 0.010 (0.027)
Train: 359 [ 150/1251 ( 12%)]  Loss: 3.519 (3.48)  Time: 0.774s, 1322.74/s  (0.791s, 1294.93/s)  LR: 3.545e-04  Data: 0.010 (0.022)
Train: 359 [ 200/1251 ( 16%)]  Loss: 3.412 (3.47)  Time: 0.771s, 1327.72/s  (0.788s, 1298.93/s)  LR: 3.545e-04  Data: 0.009 (0.019)
Train: 359 [ 250/1251 ( 20%)]  Loss: 3.589 (3.49)  Time: 0.830s, 1234.00/s  (0.786s, 1302.12/s)  LR: 3.545e-04  Data: 0.009 (0.017)
Train: 359 [ 300/1251 ( 24%)]  Loss: 3.359 (3.47)  Time: 0.772s, 1326.43/s  (0.786s, 1303.16/s)  LR: 3.545e-04  Data: 0.010 (0.016)
Train: 359 [ 350/1251 ( 28%)]  Loss: 3.412 (3.46)  Time: 0.773s, 1324.07/s  (0.785s, 1304.89/s)  LR: 3.545e-04  Data: 0.009 (0.015)
Train: 359 [ 400/1251 ( 32%)]  Loss: 3.474 (3.46)  Time: 0.774s, 1323.79/s  (0.784s, 1306.33/s)  LR: 3.545e-04  Data: 0.009 (0.014)
Train: 359 [ 450/1251 ( 36%)]  Loss: 3.333 (3.45)  Time: 0.780s, 1312.41/s  (0.783s, 1307.86/s)  LR: 3.545e-04  Data: 0.010 (0.014)
Train: 359 [ 500/1251 ( 40%)]  Loss: 3.525 (3.46)  Time: 0.773s, 1323.90/s  (0.782s, 1308.78/s)  LR: 3.545e-04  Data: 0.010 (0.013)
Train: 359 [ 550/1251 ( 44%)]  Loss: 3.089 (3.43)  Time: 0.778s, 1315.74/s  (0.782s, 1308.68/s)  LR: 3.545e-04  Data: 0.009 (0.013)
Train: 359 [ 600/1251 ( 48%)]  Loss: 3.589 (3.44)  Time: 0.771s, 1328.23/s  (0.782s, 1309.56/s)  LR: 3.545e-04  Data: 0.009 (0.013)
Train: 359 [ 650/1251 ( 52%)]  Loss: 3.522 (3.44)  Time: 0.772s, 1326.05/s  (0.782s, 1309.46/s)  LR: 3.545e-04  Data: 0.010 (0.012)
Train: 359 [ 700/1251 ( 56%)]  Loss: 3.286 (3.43)  Time: 0.773s, 1324.25/s  (0.782s, 1309.92/s)  LR: 3.545e-04  Data: 0.010 (0.012)
Train: 359 [ 750/1251 ( 60%)]  Loss: 3.343 (3.43)  Time: 0.772s, 1326.23/s  (0.782s, 1309.67/s)  LR: 3.545e-04  Data: 0.010 (0.012)
Train: 359 [ 800/1251 ( 64%)]  Loss: 3.485 (3.43)  Time: 0.775s, 1321.38/s  (0.782s, 1309.23/s)  LR: 3.545e-04  Data: 0.010 (0.012)
Train: 359 [ 850/1251 ( 68%)]  Loss: 3.143 (3.42)  Time: 0.771s, 1328.98/s  (0.782s, 1309.50/s)  LR: 3.545e-04  Data: 0.010 (0.012)
Train: 359 [ 900/1251 ( 72%)]  Loss: 3.476 (3.42)  Time: 0.775s, 1321.55/s  (0.782s, 1309.55/s)  LR: 3.545e-04  Data: 0.009 (0.012)
Train: 359 [ 950/1251 ( 76%)]  Loss: 3.199 (3.41)  Time: 0.773s, 1325.56/s  (0.782s, 1309.84/s)  LR: 3.545e-04  Data: 0.010 (0.012)
Train: 359 [1000/1251 ( 80%)]  Loss: 3.369 (3.41)  Time: 0.777s, 1317.82/s  (0.782s, 1309.70/s)  LR: 3.545e-04  Data: 0.010 (0.012)
Train: 359 [1050/1251 ( 84%)]  Loss: 3.135 (3.39)  Time: 0.772s, 1326.74/s  (0.783s, 1308.26/s)  LR: 3.545e-04  Data: 0.010 (0.011)
Train: 359 [1100/1251 ( 88%)]  Loss: 3.425 (3.39)  Time: 0.775s, 1321.57/s  (0.783s, 1308.39/s)  LR: 3.545e-04  Data: 0.010 (0.011)
Train: 359 [1150/1251 ( 92%)]  Loss: 3.600 (3.40)  Time: 0.815s, 1255.82/s  (0.783s, 1307.16/s)  LR: 3.545e-04  Data: 0.012 (0.011)
Train: 359 [1200/1251 ( 96%)]  Loss: 3.668 (3.41)  Time: 0.774s, 1323.72/s  (0.784s, 1305.52/s)  LR: 3.545e-04  Data: 0.009 (0.011)
Train: 359 [1250/1251 (100%)]  Loss: 2.992 (3.40)  Time: 0.760s, 1347.67/s  (0.784s, 1305.84/s)  LR: 3.545e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.543 (1.543)  Loss:  0.6816 (0.6816)  Acc@1: 89.9414 (89.9414)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.194 (0.560)  Loss:  0.7339 (1.1832)  Acc@1: 85.3774 (76.4920)  Acc@5: 96.9340 (93.6060)
Train: 360 [   0/1251 (  0%)]  Loss: 3.185 (3.18)  Time: 2.454s,  417.36/s  (2.454s,  417.36/s)  LR: 3.520e-04  Data: 1.717 (1.717)
Train: 360 [  50/1251 (  4%)]  Loss: 2.862 (3.02)  Time: 0.772s, 1326.35/s  (0.827s, 1237.52/s)  LR: 3.520e-04  Data: 0.009 (0.047)
Train: 360 [ 100/1251 (  8%)]  Loss: 3.339 (3.13)  Time: 0.809s, 1266.09/s  (0.808s, 1267.91/s)  LR: 3.520e-04  Data: 0.010 (0.029)
Train: 360 [ 150/1251 ( 12%)]  Loss: 3.495 (3.22)  Time: 0.806s, 1270.34/s  (0.808s, 1267.22/s)  LR: 3.520e-04  Data: 0.010 (0.022)
Train: 360 [ 200/1251 ( 16%)]  Loss: 3.632 (3.30)  Time: 0.774s, 1322.73/s  (0.805s, 1271.42/s)  LR: 3.520e-04  Data: 0.010 (0.019)
Train: 360 [ 250/1251 ( 20%)]  Loss: 3.736 (3.37)  Time: 0.779s, 1314.02/s  (0.802s, 1276.27/s)  LR: 3.520e-04  Data: 0.011 (0.017)
Train: 360 [ 300/1251 ( 24%)]  Loss: 3.173 (3.35)  Time: 0.774s, 1323.26/s  (0.799s, 1281.34/s)  LR: 3.520e-04  Data: 0.010 (0.016)
Train: 360 [ 350/1251 ( 28%)]  Loss: 2.968 (3.30)  Time: 0.772s, 1325.58/s  (0.798s, 1283.92/s)  LR: 3.520e-04  Data: 0.011 (0.015)
Train: 360 [ 400/1251 ( 32%)]  Loss: 3.403 (3.31)  Time: 0.783s, 1308.33/s  (0.795s, 1287.70/s)  LR: 3.520e-04  Data: 0.010 (0.015)
Train: 360 [ 450/1251 ( 36%)]  Loss: 3.108 (3.29)  Time: 0.821s, 1246.62/s  (0.794s, 1290.08/s)  LR: 3.520e-04  Data: 0.009 (0.014)
Train: 360 [ 500/1251 ( 40%)]  Loss: 3.099 (3.27)  Time: 0.782s, 1309.16/s  (0.794s, 1289.88/s)  LR: 3.520e-04  Data: 0.010 (0.014)
Train: 360 [ 550/1251 ( 44%)]  Loss: 3.106 (3.26)  Time: 0.773s, 1325.00/s  (0.794s, 1290.30/s)  LR: 3.520e-04  Data: 0.010 (0.013)
Train: 360 [ 600/1251 ( 48%)]  Loss: 3.409 (3.27)  Time: 0.774s, 1323.05/s  (0.792s, 1292.71/s)  LR: 3.520e-04  Data: 0.009 (0.013)
Train: 360 [ 650/1251 ( 52%)]  Loss: 3.340 (3.28)  Time: 0.776s, 1320.02/s  (0.791s, 1294.55/s)  LR: 3.520e-04  Data: 0.010 (0.013)
Train: 360 [ 700/1251 ( 56%)]  Loss: 3.090 (3.26)  Time: 0.809s, 1266.16/s  (0.790s, 1295.80/s)  LR: 3.520e-04  Data: 0.009 (0.013)
Train: 360 [ 750/1251 ( 60%)]  Loss: 3.588 (3.28)  Time: 0.783s, 1308.60/s  (0.791s, 1294.58/s)  LR: 3.520e-04  Data: 0.010 (0.012)
Train: 360 [ 800/1251 ( 64%)]  Loss: 3.550 (3.30)  Time: 0.772s, 1326.17/s  (0.790s, 1295.75/s)  LR: 3.520e-04  Data: 0.010 (0.012)
Train: 360 [ 850/1251 ( 68%)]  Loss: 3.359 (3.30)  Time: 0.833s, 1229.56/s  (0.790s, 1296.59/s)  LR: 3.520e-04  Data: 0.009 (0.012)
Train: 360 [ 900/1251 ( 72%)]  Loss: 3.205 (3.30)  Time: 0.815s, 1255.82/s  (0.790s, 1296.28/s)  LR: 3.520e-04  Data: 0.011 (0.012)
Train: 360 [ 950/1251 ( 76%)]  Loss: 3.410 (3.30)  Time: 0.789s, 1297.35/s  (0.790s, 1296.48/s)  LR: 3.520e-04  Data: 0.009 (0.012)
Train: 360 [1000/1251 ( 80%)]  Loss: 3.512 (3.31)  Time: 0.773s, 1324.41/s  (0.790s, 1297.00/s)  LR: 3.520e-04  Data: 0.010 (0.012)
Train: 360 [1050/1251 ( 84%)]  Loss: 3.187 (3.31)  Time: 0.775s, 1320.81/s  (0.789s, 1297.38/s)  LR: 3.520e-04  Data: 0.009 (0.012)
Train: 360 [1100/1251 ( 88%)]  Loss: 3.403 (3.31)  Time: 0.773s, 1324.26/s  (0.789s, 1298.19/s)  LR: 3.520e-04  Data: 0.010 (0.012)
Train: 360 [1150/1251 ( 92%)]  Loss: 3.355 (3.31)  Time: 0.783s, 1308.53/s  (0.788s, 1298.90/s)  LR: 3.520e-04  Data: 0.011 (0.012)
Train: 360 [1200/1251 ( 96%)]  Loss: 3.444 (3.32)  Time: 0.775s, 1320.55/s  (0.788s, 1299.11/s)  LR: 3.520e-04  Data: 0.009 (0.011)
Train: 360 [1250/1251 (100%)]  Loss: 3.349 (3.32)  Time: 0.762s, 1342.98/s  (0.788s, 1299.28/s)  LR: 3.520e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.541 (1.541)  Loss:  0.6904 (0.6904)  Acc@1: 90.4297 (90.4297)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.194 (0.562)  Loss:  0.7422 (1.2032)  Acc@1: 86.3207 (76.6240)  Acc@5: 97.1698 (93.5180)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-355.pth.tar', 77.04600006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-350.pth.tar', 76.87799992675781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-356.pth.tar', 76.82200003417968)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-352.pth.tar', 76.74599987792969)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-346.pth.tar', 76.73200002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-351.pth.tar', 76.70000010498048)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-360.pth.tar', 76.62399987548828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-358.pth.tar', 76.5960000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-331.pth.tar', 76.5620000366211)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-348.pth.tar', 76.54400005859375)

Train: 361 [   0/1251 (  0%)]  Loss: 3.314 (3.31)  Time: 2.320s,  441.29/s  (2.320s,  441.29/s)  LR: 3.496e-04  Data: 1.592 (1.592)
Train: 361 [  50/1251 (  4%)]  Loss: 3.713 (3.51)  Time: 0.772s, 1325.88/s  (0.813s, 1259.55/s)  LR: 3.496e-04  Data: 0.009 (0.045)
Train: 361 [ 100/1251 (  8%)]  Loss: 3.337 (3.45)  Time: 0.773s, 1324.89/s  (0.801s, 1278.98/s)  LR: 3.496e-04  Data: 0.010 (0.028)
Train: 361 [ 150/1251 ( 12%)]  Loss: 2.945 (3.33)  Time: 0.773s, 1324.05/s  (0.794s, 1289.09/s)  LR: 3.496e-04  Data: 0.010 (0.022)
Train: 361 [ 200/1251 ( 16%)]  Loss: 3.170 (3.30)  Time: 0.779s, 1313.75/s  (0.792s, 1293.71/s)  LR: 3.496e-04  Data: 0.009 (0.019)
Train: 361 [ 250/1251 ( 20%)]  Loss: 2.926 (3.23)  Time: 0.817s, 1253.27/s  (0.793s, 1291.36/s)  LR: 3.496e-04  Data: 0.011 (0.017)
Train: 361 [ 300/1251 ( 24%)]  Loss: 3.479 (3.27)  Time: 0.774s, 1322.95/s  (0.794s, 1289.15/s)  LR: 3.496e-04  Data: 0.010 (0.016)
Train: 361 [ 350/1251 ( 28%)]  Loss: 3.514 (3.30)  Time: 0.782s, 1310.03/s  (0.793s, 1290.51/s)  LR: 3.496e-04  Data: 0.009 (0.015)
Train: 361 [ 400/1251 ( 32%)]  Loss: 3.268 (3.30)  Time: 0.816s, 1255.17/s  (0.792s, 1292.20/s)  LR: 3.496e-04  Data: 0.011 (0.015)
Train: 361 [ 450/1251 ( 36%)]  Loss: 3.454 (3.31)  Time: 0.774s, 1323.34/s  (0.792s, 1293.71/s)  LR: 3.496e-04  Data: 0.010 (0.014)
Train: 361 [ 500/1251 ( 40%)]  Loss: 3.234 (3.30)  Time: 0.779s, 1314.91/s  (0.790s, 1296.32/s)  LR: 3.496e-04  Data: 0.013 (0.014)
Train: 361 [ 550/1251 ( 44%)]  Loss: 3.356 (3.31)  Time: 0.771s, 1327.56/s  (0.789s, 1297.06/s)  LR: 3.496e-04  Data: 0.009 (0.013)
Train: 361 [ 600/1251 ( 48%)]  Loss: 3.347 (3.31)  Time: 0.848s, 1207.68/s  (0.789s, 1297.99/s)  LR: 3.496e-04  Data: 0.010 (0.013)
Train: 361 [ 650/1251 ( 52%)]  Loss: 3.292 (3.31)  Time: 0.810s, 1264.74/s  (0.789s, 1298.02/s)  LR: 3.496e-04  Data: 0.010 (0.013)
Train: 361 [ 700/1251 ( 56%)]  Loss: 3.163 (3.30)  Time: 0.815s, 1256.25/s  (0.790s, 1296.47/s)  LR: 3.496e-04  Data: 0.009 (0.013)
Train: 361 [ 750/1251 ( 60%)]  Loss: 2.925 (3.28)  Time: 0.773s, 1324.90/s  (0.790s, 1296.83/s)  LR: 3.496e-04  Data: 0.010 (0.012)
Train: 361 [ 800/1251 ( 64%)]  Loss: 3.530 (3.29)  Time: 0.773s, 1324.07/s  (0.789s, 1298.06/s)  LR: 3.496e-04  Data: 0.009 (0.012)
Train: 361 [ 850/1251 ( 68%)]  Loss: 3.442 (3.30)  Time: 0.808s, 1268.03/s  (0.789s, 1298.06/s)  LR: 3.496e-04  Data: 0.009 (0.012)
Train: 361 [ 900/1251 ( 72%)]  Loss: 3.359 (3.30)  Time: 0.855s, 1197.44/s  (0.789s, 1298.31/s)  LR: 3.496e-04  Data: 0.010 (0.012)
Train: 361 [ 950/1251 ( 76%)]  Loss: 3.326 (3.30)  Time: 0.782s, 1309.65/s  (0.788s, 1299.23/s)  LR: 3.496e-04  Data: 0.010 (0.012)
Train: 361 [1000/1251 ( 80%)]  Loss: 3.387 (3.31)  Time: 0.790s, 1295.61/s  (0.788s, 1299.60/s)  LR: 3.496e-04  Data: 0.010 (0.012)
Train: 361 [1050/1251 ( 84%)]  Loss: 3.102 (3.30)  Time: 0.783s, 1308.13/s  (0.788s, 1299.47/s)  LR: 3.496e-04  Data: 0.010 (0.012)
Train: 361 [1100/1251 ( 88%)]  Loss: 3.373 (3.30)  Time: 0.771s, 1327.34/s  (0.788s, 1299.97/s)  LR: 3.496e-04  Data: 0.010 (0.012)
Train: 361 [1150/1251 ( 92%)]  Loss: 3.325 (3.30)  Time: 0.814s, 1257.94/s  (0.788s, 1299.83/s)  LR: 3.496e-04  Data: 0.010 (0.012)
Train: 361 [1200/1251 ( 96%)]  Loss: 3.577 (3.31)  Time: 0.819s, 1249.90/s  (0.788s, 1299.12/s)  LR: 3.496e-04  Data: 0.009 (0.011)
Train: 361 [1250/1251 (100%)]  Loss: 3.199 (3.31)  Time: 0.761s, 1345.65/s  (0.788s, 1299.84/s)  LR: 3.496e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.546 (1.546)  Loss:  0.6714 (0.6714)  Acc@1: 90.0391 (90.0391)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.193 (0.559)  Loss:  0.8071 (1.2266)  Acc@1: 86.6745 (76.7620)  Acc@5: 97.4057 (93.6140)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-355.pth.tar', 77.04600006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-350.pth.tar', 76.87799992675781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-356.pth.tar', 76.82200003417968)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-361.pth.tar', 76.76200015869141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-352.pth.tar', 76.74599987792969)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-346.pth.tar', 76.73200002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-351.pth.tar', 76.70000010498048)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-360.pth.tar', 76.62399987548828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-358.pth.tar', 76.5960000390625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-331.pth.tar', 76.5620000366211)

Train: 362 [   0/1251 (  0%)]  Loss: 3.240 (3.24)  Time: 2.236s,  457.99/s  (2.236s,  457.99/s)  LR: 3.471e-04  Data: 1.504 (1.504)
Train: 362 [  50/1251 (  4%)]  Loss: 3.414 (3.33)  Time: 0.776s, 1319.89/s  (0.811s, 1263.30/s)  LR: 3.471e-04  Data: 0.010 (0.042)
Train: 362 [ 100/1251 (  8%)]  Loss: 3.759 (3.47)  Time: 0.773s, 1325.01/s  (0.796s, 1285.79/s)  LR: 3.471e-04  Data: 0.009 (0.026)
Train: 362 [ 150/1251 ( 12%)]  Loss: 3.250 (3.42)  Time: 0.773s, 1324.85/s  (0.792s, 1293.51/s)  LR: 3.471e-04  Data: 0.009 (0.021)
Train: 362 [ 200/1251 ( 16%)]  Loss: 3.264 (3.39)  Time: 0.772s, 1325.88/s  (0.790s, 1296.16/s)  LR: 3.471e-04  Data: 0.010 (0.018)
Train: 362 [ 250/1251 ( 20%)]  Loss: 3.521 (3.41)  Time: 0.814s, 1258.16/s  (0.789s, 1297.33/s)  LR: 3.471e-04  Data: 0.010 (0.016)
Train: 362 [ 300/1251 ( 24%)]  Loss: 3.439 (3.41)  Time: 0.770s, 1329.37/s  (0.787s, 1300.45/s)  LR: 3.471e-04  Data: 0.010 (0.015)
Train: 362 [ 350/1251 ( 28%)]  Loss: 3.224 (3.39)  Time: 0.771s, 1328.36/s  (0.788s, 1299.94/s)  LR: 3.471e-04  Data: 0.009 (0.015)
Train: 362 [ 400/1251 ( 32%)]  Loss: 3.371 (3.39)  Time: 0.774s, 1322.20/s  (0.787s, 1301.87/s)  LR: 3.471e-04  Data: 0.010 (0.014)
Train: 362 [ 450/1251 ( 36%)]  Loss: 3.094 (3.36)  Time: 0.774s, 1323.53/s  (0.786s, 1302.92/s)  LR: 3.471e-04  Data: 0.010 (0.013)
Train: 362 [ 500/1251 ( 40%)]  Loss: 3.425 (3.36)  Time: 0.788s, 1299.92/s  (0.786s, 1302.46/s)  LR: 3.471e-04  Data: 0.011 (0.013)
Train: 362 [ 550/1251 ( 44%)]  Loss: 3.699 (3.39)  Time: 0.774s, 1323.20/s  (0.786s, 1302.15/s)  LR: 3.471e-04  Data: 0.010 (0.013)
Train: 362 [ 600/1251 ( 48%)]  Loss: 3.420 (3.39)  Time: 0.775s, 1321.90/s  (0.787s, 1300.68/s)  LR: 3.471e-04  Data: 0.009 (0.013)
Train: 362 [ 650/1251 ( 52%)]  Loss: 3.189 (3.38)  Time: 0.828s, 1237.11/s  (0.788s, 1300.15/s)  LR: 3.471e-04  Data: 0.010 (0.012)
Train: 362 [ 700/1251 ( 56%)]  Loss: 3.384 (3.38)  Time: 0.775s, 1320.88/s  (0.787s, 1301.22/s)  LR: 3.471e-04  Data: 0.009 (0.012)
Train: 362 [ 750/1251 ( 60%)]  Loss: 3.291 (3.37)  Time: 0.774s, 1323.38/s  (0.786s, 1302.29/s)  LR: 3.471e-04  Data: 0.010 (0.012)
Train: 362 [ 800/1251 ( 64%)]  Loss: 3.245 (3.37)  Time: 0.781s, 1311.44/s  (0.786s, 1303.03/s)  LR: 3.471e-04  Data: 0.011 (0.012)
Train: 362 [ 850/1251 ( 68%)]  Loss: 3.414 (3.37)  Time: 0.773s, 1324.65/s  (0.786s, 1303.28/s)  LR: 3.471e-04  Data: 0.009 (0.012)
Train: 362 [ 900/1251 ( 72%)]  Loss: 3.087 (3.35)  Time: 0.773s, 1324.03/s  (0.785s, 1303.67/s)  LR: 3.471e-04  Data: 0.009 (0.012)
Train: 362 [ 950/1251 ( 76%)]  Loss: 3.579 (3.37)  Time: 0.809s, 1265.52/s  (0.786s, 1303.59/s)  LR: 3.471e-04  Data: 0.010 (0.012)
Train: 362 [1000/1251 ( 80%)]  Loss: 3.369 (3.37)  Time: 0.774s, 1322.30/s  (0.785s, 1304.03/s)  LR: 3.471e-04  Data: 0.009 (0.011)
Train: 362 [1050/1251 ( 84%)]  Loss: 3.216 (3.36)  Time: 0.783s, 1308.47/s  (0.785s, 1304.59/s)  LR: 3.471e-04  Data: 0.010 (0.011)
Train: 362 [1100/1251 ( 88%)]  Loss: 2.985 (3.34)  Time: 0.772s, 1326.21/s  (0.785s, 1304.74/s)  LR: 3.471e-04  Data: 0.010 (0.011)
Train: 362 [1150/1251 ( 92%)]  Loss: 3.568 (3.35)  Time: 0.778s, 1316.67/s  (0.786s, 1303.45/s)  LR: 3.471e-04  Data: 0.010 (0.011)
Train: 362 [1200/1251 ( 96%)]  Loss: 3.420 (3.35)  Time: 0.807s, 1268.63/s  (0.786s, 1302.88/s)  LR: 3.471e-04  Data: 0.010 (0.011)
Train: 362 [1250/1251 (100%)]  Loss: 3.413 (3.36)  Time: 0.760s, 1347.44/s  (0.786s, 1303.16/s)  LR: 3.471e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.522 (1.522)  Loss:  0.6777 (0.6777)  Acc@1: 89.8438 (89.8438)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.194 (0.562)  Loss:  0.8105 (1.2504)  Acc@1: 85.8491 (76.8740)  Acc@5: 97.8774 (93.7140)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-355.pth.tar', 77.04600006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-350.pth.tar', 76.87799992675781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-362.pth.tar', 76.87400005859375)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-356.pth.tar', 76.82200003417968)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-361.pth.tar', 76.76200015869141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-352.pth.tar', 76.74599987792969)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-346.pth.tar', 76.73200002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-351.pth.tar', 76.70000010498048)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-360.pth.tar', 76.62399987548828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-358.pth.tar', 76.5960000390625)

Train: 363 [   0/1251 (  0%)]  Loss: 3.517 (3.52)  Time: 2.500s,  409.52/s  (2.500s,  409.52/s)  LR: 3.447e-04  Data: 1.766 (1.766)
Train: 363 [  50/1251 (  4%)]  Loss: 3.219 (3.37)  Time: 0.769s, 1332.10/s  (0.821s, 1247.01/s)  LR: 3.447e-04  Data: 0.010 (0.053)
Train: 363 [ 100/1251 (  8%)]  Loss: 2.956 (3.23)  Time: 0.777s, 1318.17/s  (0.800s, 1280.17/s)  LR: 3.447e-04  Data: 0.014 (0.032)
Train: 363 [ 150/1251 ( 12%)]  Loss: 3.507 (3.30)  Time: 0.778s, 1315.66/s  (0.796s, 1285.66/s)  LR: 3.447e-04  Data: 0.009 (0.024)
Train: 363 [ 200/1251 ( 16%)]  Loss: 3.511 (3.34)  Time: 0.814s, 1257.25/s  (0.797s, 1284.40/s)  LR: 3.447e-04  Data: 0.011 (0.021)
Train: 363 [ 250/1251 ( 20%)]  Loss: 3.401 (3.35)  Time: 0.776s, 1319.15/s  (0.794s, 1289.42/s)  LR: 3.447e-04  Data: 0.010 (0.019)
Train: 363 [ 300/1251 ( 24%)]  Loss: 3.510 (3.37)  Time: 0.781s, 1310.80/s  (0.792s, 1292.90/s)  LR: 3.447e-04  Data: 0.010 (0.017)
Train: 363 [ 350/1251 ( 28%)]  Loss: 3.610 (3.40)  Time: 0.771s, 1328.46/s  (0.794s, 1289.04/s)  LR: 3.447e-04  Data: 0.010 (0.016)
Train: 363 [ 400/1251 ( 32%)]  Loss: 3.494 (3.41)  Time: 0.808s, 1267.67/s  (0.793s, 1291.05/s)  LR: 3.447e-04  Data: 0.009 (0.016)
Train: 363 [ 450/1251 ( 36%)]  Loss: 3.227 (3.40)  Time: 0.771s, 1327.78/s  (0.792s, 1292.14/s)  LR: 3.447e-04  Data: 0.009 (0.015)
Train: 363 [ 500/1251 ( 40%)]  Loss: 3.859 (3.44)  Time: 0.776s, 1319.79/s  (0.791s, 1294.60/s)  LR: 3.447e-04  Data: 0.013 (0.014)
Train: 363 [ 550/1251 ( 44%)]  Loss: 3.202 (3.42)  Time: 0.774s, 1323.12/s  (0.790s, 1296.25/s)  LR: 3.447e-04  Data: 0.010 (0.014)
Train: 363 [ 600/1251 ( 48%)]  Loss: 3.569 (3.43)  Time: 0.776s, 1319.68/s  (0.789s, 1297.55/s)  LR: 3.447e-04  Data: 0.009 (0.014)
Train: 363 [ 650/1251 ( 52%)]  Loss: 3.389 (3.43)  Time: 0.772s, 1326.65/s  (0.788s, 1298.84/s)  LR: 3.447e-04  Data: 0.010 (0.013)
Train: 363 [ 700/1251 ( 56%)]  Loss: 3.279 (3.42)  Time: 0.815s, 1256.42/s  (0.788s, 1298.83/s)  LR: 3.447e-04  Data: 0.011 (0.013)
Train: 363 [ 750/1251 ( 60%)]  Loss: 3.000 (3.39)  Time: 0.779s, 1315.16/s  (0.789s, 1298.39/s)  LR: 3.447e-04  Data: 0.011 (0.013)
Train: 363 [ 800/1251 ( 64%)]  Loss: 3.613 (3.40)  Time: 0.808s, 1267.86/s  (0.789s, 1298.21/s)  LR: 3.447e-04  Data: 0.009 (0.013)
Train: 363 [ 850/1251 ( 68%)]  Loss: 3.268 (3.40)  Time: 0.784s, 1306.80/s  (0.789s, 1298.45/s)  LR: 3.447e-04  Data: 0.013 (0.013)
Train: 363 [ 900/1251 ( 72%)]  Loss: 3.461 (3.40)  Time: 0.773s, 1324.25/s  (0.788s, 1299.00/s)  LR: 3.447e-04  Data: 0.011 (0.012)
Train: 363 [ 950/1251 ( 76%)]  Loss: 3.160 (3.39)  Time: 0.772s, 1326.56/s  (0.788s, 1299.63/s)  LR: 3.447e-04  Data: 0.010 (0.012)
Train: 363 [1000/1251 ( 80%)]  Loss: 3.553 (3.40)  Time: 0.775s, 1321.15/s  (0.787s, 1300.44/s)  LR: 3.447e-04  Data: 0.009 (0.012)
Train: 363 [1050/1251 ( 84%)]  Loss: 3.352 (3.39)  Time: 0.864s, 1185.84/s  (0.787s, 1301.02/s)  LR: 3.447e-04  Data: 0.010 (0.012)
Train: 363 [1100/1251 ( 88%)]  Loss: 3.452 (3.40)  Time: 0.773s, 1325.24/s  (0.787s, 1301.52/s)  LR: 3.447e-04  Data: 0.009 (0.012)
Train: 363 [1150/1251 ( 92%)]  Loss: 3.033 (3.38)  Time: 0.816s, 1254.53/s  (0.786s, 1302.16/s)  LR: 3.447e-04  Data: 0.010 (0.012)
Train: 363 [1200/1251 ( 96%)]  Loss: 3.316 (3.38)  Time: 0.771s, 1327.31/s  (0.786s, 1302.43/s)  LR: 3.447e-04  Data: 0.010 (0.012)
Train: 363 [1250/1251 (100%)]  Loss: 3.088 (3.37)  Time: 0.761s, 1345.44/s  (0.786s, 1302.20/s)  LR: 3.447e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.548 (1.548)  Loss:  0.6289 (0.6289)  Acc@1: 91.4062 (91.4062)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.194 (0.598)  Loss:  0.7598 (1.1454)  Acc@1: 85.3774 (77.1660)  Acc@5: 96.4623 (93.7960)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-363.pth.tar', 77.16599998291015)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-355.pth.tar', 77.04600006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-350.pth.tar', 76.87799992675781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-362.pth.tar', 76.87400005859375)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-356.pth.tar', 76.82200003417968)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-361.pth.tar', 76.76200015869141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-352.pth.tar', 76.74599987792969)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-346.pth.tar', 76.73200002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-351.pth.tar', 76.70000010498048)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-360.pth.tar', 76.62399987548828)

Train: 364 [   0/1251 (  0%)]  Loss: 3.310 (3.31)  Time: 2.328s,  439.89/s  (2.328s,  439.89/s)  LR: 3.422e-04  Data: 1.592 (1.592)
Train: 364 [  50/1251 (  4%)]  Loss: 3.114 (3.21)  Time: 0.795s, 1287.67/s  (0.824s, 1242.79/s)  LR: 3.422e-04  Data: 0.010 (0.046)
Train: 364 [ 100/1251 (  8%)]  Loss: 3.530 (3.32)  Time: 0.772s, 1326.28/s  (0.802s, 1276.05/s)  LR: 3.422e-04  Data: 0.010 (0.028)
Train: 364 [ 150/1251 ( 12%)]  Loss: 3.506 (3.37)  Time: 0.781s, 1310.48/s  (0.795s, 1288.11/s)  LR: 3.422e-04  Data: 0.010 (0.022)
Train: 364 [ 200/1251 ( 16%)]  Loss: 3.600 (3.41)  Time: 0.798s, 1282.51/s  (0.796s, 1286.60/s)  LR: 3.422e-04  Data: 0.009 (0.019)
Train: 364 [ 250/1251 ( 20%)]  Loss: 3.442 (3.42)  Time: 0.783s, 1307.38/s  (0.794s, 1289.41/s)  LR: 3.422e-04  Data: 0.010 (0.018)
Train: 364 [ 300/1251 ( 24%)]  Loss: 3.306 (3.40)  Time: 0.846s, 1211.09/s  (0.793s, 1291.26/s)  LR: 3.422e-04  Data: 0.010 (0.016)
Train: 364 [ 350/1251 ( 28%)]  Loss: 3.282 (3.39)  Time: 0.778s, 1315.38/s  (0.792s, 1293.06/s)  LR: 3.422e-04  Data: 0.010 (0.016)
Train: 364 [ 400/1251 ( 32%)]  Loss: 3.568 (3.41)  Time: 0.783s, 1306.98/s  (0.791s, 1295.12/s)  LR: 3.422e-04  Data: 0.013 (0.015)
Train: 364 [ 450/1251 ( 36%)]  Loss: 3.244 (3.39)  Time: 0.776s, 1319.57/s  (0.790s, 1296.51/s)  LR: 3.422e-04  Data: 0.010 (0.014)
Train: 364 [ 500/1251 ( 40%)]  Loss: 3.363 (3.39)  Time: 0.772s, 1325.79/s  (0.789s, 1297.76/s)  LR: 3.422e-04  Data: 0.010 (0.014)
Train: 364 [ 550/1251 ( 44%)]  Loss: 3.214 (3.37)  Time: 0.775s, 1321.64/s  (0.788s, 1298.98/s)  LR: 3.422e-04  Data: 0.010 (0.014)
Train: 364 [ 600/1251 ( 48%)]  Loss: 3.311 (3.37)  Time: 0.779s, 1315.29/s  (0.787s, 1300.47/s)  LR: 3.422e-04  Data: 0.009 (0.013)
Train: 364 [ 650/1251 ( 52%)]  Loss: 3.410 (3.37)  Time: 0.782s, 1309.52/s  (0.787s, 1301.90/s)  LR: 3.422e-04  Data: 0.010 (0.013)
Train: 364 [ 700/1251 ( 56%)]  Loss: 3.311 (3.37)  Time: 0.773s, 1323.96/s  (0.786s, 1302.89/s)  LR: 3.422e-04  Data: 0.010 (0.013)
Train: 364 [ 750/1251 ( 60%)]  Loss: 3.434 (3.37)  Time: 0.773s, 1325.11/s  (0.786s, 1303.52/s)  LR: 3.422e-04  Data: 0.010 (0.013)
Train: 364 [ 800/1251 ( 64%)]  Loss: 3.432 (3.38)  Time: 0.779s, 1315.24/s  (0.785s, 1304.05/s)  LR: 3.422e-04  Data: 0.010 (0.012)
Train: 364 [ 850/1251 ( 68%)]  Loss: 3.405 (3.38)  Time: 0.773s, 1324.35/s  (0.785s, 1304.25/s)  LR: 3.422e-04  Data: 0.010 (0.012)
Train: 364 [ 900/1251 ( 72%)]  Loss: 2.976 (3.36)  Time: 0.774s, 1323.60/s  (0.785s, 1305.01/s)  LR: 3.422e-04  Data: 0.010 (0.012)
Train: 364 [ 950/1251 ( 76%)]  Loss: 3.786 (3.38)  Time: 0.775s, 1321.28/s  (0.784s, 1305.47/s)  LR: 3.422e-04  Data: 0.009 (0.012)
Train: 364 [1000/1251 ( 80%)]  Loss: 3.402 (3.38)  Time: 0.842s, 1216.83/s  (0.785s, 1304.97/s)  LR: 3.422e-04  Data: 0.010 (0.012)
Train: 364 [1050/1251 ( 84%)]  Loss: 3.497 (3.38)  Time: 0.782s, 1309.49/s  (0.785s, 1304.32/s)  LR: 3.422e-04  Data: 0.011 (0.012)
Train: 364 [1100/1251 ( 88%)]  Loss: 3.266 (3.38)  Time: 0.774s, 1323.04/s  (0.785s, 1304.81/s)  LR: 3.422e-04  Data: 0.010 (0.012)
Train: 364 [1150/1251 ( 92%)]  Loss: 3.442 (3.38)  Time: 0.820s, 1249.44/s  (0.785s, 1305.21/s)  LR: 3.422e-04  Data: 0.015 (0.012)
Train: 364 [1200/1251 ( 96%)]  Loss: 3.348 (3.38)  Time: 0.774s, 1323.85/s  (0.785s, 1305.25/s)  LR: 3.422e-04  Data: 0.010 (0.012)
Train: 364 [1250/1251 (100%)]  Loss: 3.198 (3.37)  Time: 0.760s, 1347.96/s  (0.784s, 1305.55/s)  LR: 3.422e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.568 (1.568)  Loss:  0.7554 (0.7554)  Acc@1: 91.1133 (91.1133)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.194 (0.561)  Loss:  0.8921 (1.2658)  Acc@1: 86.0849 (76.7780)  Acc@5: 96.8160 (93.5700)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-363.pth.tar', 77.16599998291015)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-355.pth.tar', 77.04600006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-350.pth.tar', 76.87799992675781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-362.pth.tar', 76.87400005859375)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-356.pth.tar', 76.82200003417968)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-364.pth.tar', 76.77800003173829)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-361.pth.tar', 76.76200015869141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-352.pth.tar', 76.74599987792969)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-346.pth.tar', 76.73200002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-351.pth.tar', 76.70000010498048)

Train: 365 [   0/1251 (  0%)]  Loss: 3.175 (3.18)  Time: 2.383s,  429.69/s  (2.383s,  429.69/s)  LR: 3.398e-04  Data: 1.652 (1.652)
Train: 365 [  50/1251 (  4%)]  Loss: 3.168 (3.17)  Time: 0.813s, 1259.92/s  (0.817s, 1253.05/s)  LR: 3.398e-04  Data: 0.009 (0.044)
Train: 365 [ 100/1251 (  8%)]  Loss: 3.407 (3.25)  Time: 0.779s, 1314.92/s  (0.799s, 1281.09/s)  LR: 3.398e-04  Data: 0.009 (0.027)
Train: 365 [ 150/1251 ( 12%)]  Loss: 3.516 (3.32)  Time: 0.807s, 1269.65/s  (0.801s, 1278.99/s)  LR: 3.398e-04  Data: 0.010 (0.021)
Train: 365 [ 200/1251 ( 16%)]  Loss: 3.431 (3.34)  Time: 0.779s, 1314.78/s  (0.803s, 1275.19/s)  LR: 3.398e-04  Data: 0.010 (0.018)
Train: 365 [ 250/1251 ( 20%)]  Loss: 3.423 (3.35)  Time: 0.805s, 1272.04/s  (0.804s, 1273.55/s)  LR: 3.398e-04  Data: 0.009 (0.017)
Train: 365 [ 300/1251 ( 24%)]  Loss: 3.087 (3.32)  Time: 0.777s, 1318.45/s  (0.803s, 1275.44/s)  LR: 3.398e-04  Data: 0.010 (0.016)
Train: 365 [ 350/1251 ( 28%)]  Loss: 3.264 (3.31)  Time: 0.772s, 1326.54/s  (0.799s, 1280.88/s)  LR: 3.398e-04  Data: 0.010 (0.015)
Train: 365 [ 400/1251 ( 32%)]  Loss: 3.257 (3.30)  Time: 0.773s, 1324.14/s  (0.797s, 1284.10/s)  LR: 3.398e-04  Data: 0.010 (0.014)
Train: 365 [ 450/1251 ( 36%)]  Loss: 3.450 (3.32)  Time: 0.779s, 1314.39/s  (0.796s, 1286.09/s)  LR: 3.398e-04  Data: 0.009 (0.014)
Train: 365 [ 500/1251 ( 40%)]  Loss: 3.513 (3.34)  Time: 0.771s, 1328.19/s  (0.795s, 1288.58/s)  LR: 3.398e-04  Data: 0.010 (0.013)
Train: 365 [ 550/1251 ( 44%)]  Loss: 3.477 (3.35)  Time: 0.771s, 1327.37/s  (0.793s, 1291.10/s)  LR: 3.398e-04  Data: 0.010 (0.013)
Train: 365 [ 600/1251 ( 48%)]  Loss: 3.315 (3.34)  Time: 0.773s, 1324.14/s  (0.793s, 1291.44/s)  LR: 3.398e-04  Data: 0.009 (0.013)
Train: 365 [ 650/1251 ( 52%)]  Loss: 3.609 (3.36)  Time: 0.792s, 1293.49/s  (0.792s, 1292.96/s)  LR: 3.398e-04  Data: 0.009 (0.013)
Train: 365 [ 700/1251 ( 56%)]  Loss: 3.364 (3.36)  Time: 0.775s, 1321.17/s  (0.791s, 1294.10/s)  LR: 3.398e-04  Data: 0.010 (0.012)
Train: 365 [ 750/1251 ( 60%)]  Loss: 3.432 (3.37)  Time: 0.773s, 1324.67/s  (0.790s, 1295.50/s)  LR: 3.398e-04  Data: 0.010 (0.012)
Train: 365 [ 800/1251 ( 64%)]  Loss: 3.046 (3.35)  Time: 0.773s, 1324.62/s  (0.789s, 1297.03/s)  LR: 3.398e-04  Data: 0.009 (0.012)
Train: 365 [ 850/1251 ( 68%)]  Loss: 3.453 (3.35)  Time: 0.781s, 1311.03/s  (0.789s, 1298.17/s)  LR: 3.398e-04  Data: 0.009 (0.012)
Train: 365 [ 900/1251 ( 72%)]  Loss: 3.360 (3.36)  Time: 0.858s, 1193.11/s  (0.788s, 1299.06/s)  LR: 3.398e-04  Data: 0.010 (0.012)
Train: 365 [ 950/1251 ( 76%)]  Loss: 3.663 (3.37)  Time: 0.773s, 1324.78/s  (0.788s, 1299.86/s)  LR: 3.398e-04  Data: 0.010 (0.012)
Train: 365 [1000/1251 ( 80%)]  Loss: 3.389 (3.37)  Time: 0.815s, 1255.95/s  (0.788s, 1300.18/s)  LR: 3.398e-04  Data: 0.009 (0.012)
Train: 365 [1050/1251 ( 84%)]  Loss: 3.046 (3.36)  Time: 0.776s, 1320.20/s  (0.787s, 1301.06/s)  LR: 3.398e-04  Data: 0.009 (0.011)
Train: 365 [1100/1251 ( 88%)]  Loss: 3.223 (3.35)  Time: 0.776s, 1319.50/s  (0.787s, 1301.72/s)  LR: 3.398e-04  Data: 0.010 (0.011)
Train: 365 [1150/1251 ( 92%)]  Loss: 3.386 (3.35)  Time: 0.815s, 1256.07/s  (0.787s, 1301.22/s)  LR: 3.398e-04  Data: 0.011 (0.011)
Train: 365 [1200/1251 ( 96%)]  Loss: 3.479 (3.36)  Time: 0.887s, 1154.68/s  (0.787s, 1300.73/s)  LR: 3.398e-04  Data: 0.012 (0.011)
Train: 365 [1250/1251 (100%)]  Loss: 3.524 (3.36)  Time: 0.759s, 1348.91/s  (0.788s, 1299.72/s)  LR: 3.398e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.581 (1.581)  Loss:  0.8286 (0.8286)  Acc@1: 90.4297 (90.4297)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.194 (0.565)  Loss:  0.9365 (1.2808)  Acc@1: 85.8491 (76.6980)  Acc@5: 95.9906 (93.5560)
Train: 366 [   0/1251 (  0%)]  Loss: 3.618 (3.62)  Time: 2.433s,  420.81/s  (2.433s,  420.81/s)  LR: 3.373e-04  Data: 1.639 (1.639)
Train: 366 [  50/1251 (  4%)]  Loss: 3.435 (3.53)  Time: 0.838s, 1221.29/s  (0.836s, 1224.64/s)  LR: 3.373e-04  Data: 0.010 (0.044)
Train: 366 [ 100/1251 (  8%)]  Loss: 2.940 (3.33)  Time: 0.772s, 1325.93/s  (0.822s, 1245.70/s)  LR: 3.373e-04  Data: 0.009 (0.028)
Train: 366 [ 150/1251 ( 12%)]  Loss: 3.509 (3.38)  Time: 0.836s, 1225.30/s  (0.818s, 1251.44/s)  LR: 3.373e-04  Data: 0.010 (0.022)
Train: 366 [ 200/1251 ( 16%)]  Loss: 3.668 (3.43)  Time: 0.798s, 1283.81/s  (0.809s, 1266.38/s)  LR: 3.373e-04  Data: 0.010 (0.019)
Train: 366 [ 250/1251 ( 20%)]  Loss: 3.527 (3.45)  Time: 0.775s, 1321.06/s  (0.802s, 1276.08/s)  LR: 3.373e-04  Data: 0.010 (0.017)
Train: 366 [ 300/1251 ( 24%)]  Loss: 3.435 (3.45)  Time: 0.786s, 1302.50/s  (0.800s, 1279.96/s)  LR: 3.373e-04  Data: 0.010 (0.016)
Train: 366 [ 350/1251 ( 28%)]  Loss: 3.338 (3.43)  Time: 0.891s, 1149.35/s  (0.798s, 1283.09/s)  LR: 3.373e-04  Data: 0.009 (0.015)
Train: 366 [ 400/1251 ( 32%)]  Loss: 3.443 (3.43)  Time: 0.773s, 1325.35/s  (0.797s, 1285.27/s)  LR: 3.373e-04  Data: 0.010 (0.014)
Train: 366 [ 450/1251 ( 36%)]  Loss: 3.339 (3.43)  Time: 0.828s, 1236.86/s  (0.798s, 1283.35/s)  LR: 3.373e-04  Data: 0.010 (0.014)
Train: 366 [ 500/1251 ( 40%)]  Loss: 3.531 (3.43)  Time: 0.776s, 1319.73/s  (0.797s, 1285.20/s)  LR: 3.373e-04  Data: 0.010 (0.013)
Train: 366 [ 550/1251 ( 44%)]  Loss: 3.127 (3.41)  Time: 0.778s, 1316.63/s  (0.795s, 1287.62/s)  LR: 3.373e-04  Data: 0.010 (0.013)
Train: 366 [ 600/1251 ( 48%)]  Loss: 3.407 (3.41)  Time: 0.773s, 1324.57/s  (0.794s, 1290.14/s)  LR: 3.373e-04  Data: 0.010 (0.013)
Train: 366 [ 650/1251 ( 52%)]  Loss: 3.498 (3.42)  Time: 0.781s, 1311.76/s  (0.793s, 1291.34/s)  LR: 3.373e-04  Data: 0.010 (0.013)
Train: 366 [ 700/1251 ( 56%)]  Loss: 3.471 (3.42)  Time: 0.778s, 1316.06/s  (0.792s, 1293.13/s)  LR: 3.373e-04  Data: 0.009 (0.012)
Train: 366 [ 750/1251 ( 60%)]  Loss: 3.276 (3.41)  Time: 0.771s, 1328.08/s  (0.791s, 1294.62/s)  LR: 3.373e-04  Data: 0.009 (0.012)
Train: 366 [ 800/1251 ( 64%)]  Loss: 3.131 (3.39)  Time: 0.774s, 1323.18/s  (0.791s, 1295.27/s)  LR: 3.373e-04  Data: 0.010 (0.012)
Train: 366 [ 850/1251 ( 68%)]  Loss: 3.042 (3.37)  Time: 0.774s, 1322.70/s  (0.790s, 1296.41/s)  LR: 3.373e-04  Data: 0.009 (0.012)
Train: 366 [ 900/1251 ( 72%)]  Loss: 3.185 (3.36)  Time: 0.787s, 1300.38/s  (0.789s, 1297.63/s)  LR: 3.373e-04  Data: 0.011 (0.012)
Train: 366 [ 950/1251 ( 76%)]  Loss: 3.004 (3.35)  Time: 0.776s, 1319.38/s  (0.789s, 1298.47/s)  LR: 3.373e-04  Data: 0.009 (0.012)
Train: 366 [1000/1251 ( 80%)]  Loss: 3.394 (3.35)  Time: 0.776s, 1319.34/s  (0.788s, 1299.48/s)  LR: 3.373e-04  Data: 0.009 (0.011)
Train: 366 [1050/1251 ( 84%)]  Loss: 3.398 (3.35)  Time: 0.774s, 1323.19/s  (0.788s, 1300.29/s)  LR: 3.373e-04  Data: 0.009 (0.011)
Train: 366 [1100/1251 ( 88%)]  Loss: 3.571 (3.36)  Time: 0.785s, 1305.20/s  (0.787s, 1300.55/s)  LR: 3.373e-04  Data: 0.010 (0.011)
Train: 366 [1150/1251 ( 92%)]  Loss: 3.426 (3.36)  Time: 0.775s, 1321.18/s  (0.787s, 1300.63/s)  LR: 3.373e-04  Data: 0.009 (0.011)
Train: 366 [1200/1251 ( 96%)]  Loss: 3.639 (3.37)  Time: 0.776s, 1320.39/s  (0.787s, 1301.33/s)  LR: 3.373e-04  Data: 0.010 (0.011)
Train: 366 [1250/1251 (100%)]  Loss: 3.271 (3.37)  Time: 0.762s, 1343.05/s  (0.787s, 1301.82/s)  LR: 3.373e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.620 (1.620)  Loss:  0.6797 (0.6797)  Acc@1: 89.3555 (89.3555)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.194 (0.559)  Loss:  0.8110 (1.1749)  Acc@1: 85.0236 (76.9320)  Acc@5: 96.2264 (93.7080)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-363.pth.tar', 77.16599998291015)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-355.pth.tar', 77.04600006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-366.pth.tar', 76.93200008789063)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-350.pth.tar', 76.87799992675781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-362.pth.tar', 76.87400005859375)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-356.pth.tar', 76.82200003417968)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-364.pth.tar', 76.77800003173829)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-361.pth.tar', 76.76200015869141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-352.pth.tar', 76.74599987792969)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-346.pth.tar', 76.73200002929687)

Train: 367 [   0/1251 (  0%)]  Loss: 3.166 (3.17)  Time: 2.339s,  437.84/s  (2.339s,  437.84/s)  LR: 3.349e-04  Data: 1.523 (1.523)
Train: 367 [  50/1251 (  4%)]  Loss: 3.304 (3.23)  Time: 0.774s, 1323.55/s  (0.813s, 1259.95/s)  LR: 3.349e-04  Data: 0.010 (0.043)
Train: 367 [ 100/1251 (  8%)]  Loss: 3.302 (3.26)  Time: 0.808s, 1267.20/s  (0.808s, 1267.71/s)  LR: 3.349e-04  Data: 0.009 (0.027)
Train: 367 [ 150/1251 ( 12%)]  Loss: 2.899 (3.17)  Time: 0.773s, 1324.69/s  (0.800s, 1279.78/s)  LR: 3.349e-04  Data: 0.010 (0.021)
Train: 367 [ 200/1251 ( 16%)]  Loss: 2.950 (3.12)  Time: 0.775s, 1320.95/s  (0.796s, 1286.81/s)  LR: 3.349e-04  Data: 0.009 (0.018)
Train: 367 [ 250/1251 ( 20%)]  Loss: 3.289 (3.15)  Time: 0.774s, 1322.40/s  (0.793s, 1292.02/s)  LR: 3.349e-04  Data: 0.009 (0.017)
Train: 367 [ 300/1251 ( 24%)]  Loss: 2.837 (3.11)  Time: 0.775s, 1321.53/s  (0.790s, 1295.85/s)  LR: 3.349e-04  Data: 0.010 (0.015)
Train: 367 [ 350/1251 ( 28%)]  Loss: 3.565 (3.16)  Time: 0.773s, 1324.71/s  (0.789s, 1297.96/s)  LR: 3.349e-04  Data: 0.010 (0.015)
Train: 367 [ 400/1251 ( 32%)]  Loss: 3.396 (3.19)  Time: 0.775s, 1320.74/s  (0.788s, 1300.09/s)  LR: 3.349e-04  Data: 0.010 (0.014)
Train: 367 [ 450/1251 ( 36%)]  Loss: 3.488 (3.22)  Time: 0.772s, 1327.04/s  (0.787s, 1300.69/s)  LR: 3.349e-04  Data: 0.009 (0.014)
Train: 367 [ 500/1251 ( 40%)]  Loss: 3.324 (3.23)  Time: 0.807s, 1268.44/s  (0.790s, 1296.64/s)  LR: 3.349e-04  Data: 0.010 (0.013)
Train: 367 [ 550/1251 ( 44%)]  Loss: 2.782 (3.19)  Time: 0.773s, 1324.09/s  (0.789s, 1298.48/s)  LR: 3.349e-04  Data: 0.010 (0.013)
Train: 367 [ 600/1251 ( 48%)]  Loss: 3.558 (3.22)  Time: 0.777s, 1317.78/s  (0.788s, 1300.08/s)  LR: 3.349e-04  Data: 0.009 (0.013)
Train: 367 [ 650/1251 ( 52%)]  Loss: 3.367 (3.23)  Time: 0.773s, 1325.32/s  (0.787s, 1301.14/s)  LR: 3.349e-04  Data: 0.010 (0.012)
Train: 367 [ 700/1251 ( 56%)]  Loss: 3.309 (3.24)  Time: 0.782s, 1309.48/s  (0.787s, 1301.29/s)  LR: 3.349e-04  Data: 0.010 (0.012)
Train: 367 [ 750/1251 ( 60%)]  Loss: 3.300 (3.24)  Time: 0.771s, 1327.81/s  (0.787s, 1301.93/s)  LR: 3.349e-04  Data: 0.009 (0.012)
Train: 367 [ 800/1251 ( 64%)]  Loss: 3.633 (3.26)  Time: 0.772s, 1326.72/s  (0.786s, 1303.00/s)  LR: 3.349e-04  Data: 0.010 (0.012)
Train: 367 [ 850/1251 ( 68%)]  Loss: 3.303 (3.27)  Time: 0.772s, 1326.98/s  (0.785s, 1303.82/s)  LR: 3.349e-04  Data: 0.010 (0.012)
Train: 367 [ 900/1251 ( 72%)]  Loss: 3.426 (3.27)  Time: 0.773s, 1324.98/s  (0.785s, 1304.79/s)  LR: 3.349e-04  Data: 0.009 (0.012)
Train: 367 [ 950/1251 ( 76%)]  Loss: 3.580 (3.29)  Time: 0.784s, 1305.66/s  (0.785s, 1305.22/s)  LR: 3.349e-04  Data: 0.010 (0.012)
Train: 367 [1000/1251 ( 80%)]  Loss: 3.448 (3.30)  Time: 0.771s, 1328.00/s  (0.785s, 1305.20/s)  LR: 3.349e-04  Data: 0.010 (0.011)
Train: 367 [1050/1251 ( 84%)]  Loss: 3.046 (3.29)  Time: 0.842s, 1215.85/s  (0.785s, 1304.36/s)  LR: 3.349e-04  Data: 0.009 (0.011)
Train: 367 [1100/1251 ( 88%)]  Loss: 3.690 (3.30)  Time: 0.812s, 1260.81/s  (0.786s, 1303.51/s)  LR: 3.349e-04  Data: 0.010 (0.011)
Train: 367 [1150/1251 ( 92%)]  Loss: 3.217 (3.30)  Time: 0.787s, 1300.64/s  (0.786s, 1302.53/s)  LR: 3.349e-04  Data: 0.010 (0.011)
Train: 367 [1200/1251 ( 96%)]  Loss: 3.345 (3.30)  Time: 0.773s, 1324.00/s  (0.786s, 1302.94/s)  LR: 3.349e-04  Data: 0.009 (0.011)
Train: 367 [1250/1251 (100%)]  Loss: 3.337 (3.30)  Time: 0.761s, 1345.31/s  (0.786s, 1303.51/s)  LR: 3.349e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.557 (1.557)  Loss:  0.6963 (0.6963)  Acc@1: 90.1367 (90.1367)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.194 (0.564)  Loss:  0.7886 (1.1681)  Acc@1: 85.7311 (77.1960)  Acc@5: 95.9906 (93.7500)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-367.pth.tar', 77.19600000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-363.pth.tar', 77.16599998291015)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-355.pth.tar', 77.04600006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-366.pth.tar', 76.93200008789063)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-350.pth.tar', 76.87799992675781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-362.pth.tar', 76.87400005859375)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-356.pth.tar', 76.82200003417968)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-364.pth.tar', 76.77800003173829)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-361.pth.tar', 76.76200015869141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-352.pth.tar', 76.74599987792969)

Train: 368 [   0/1251 (  0%)]  Loss: 3.099 (3.10)  Time: 2.221s,  461.13/s  (2.221s,  461.13/s)  LR: 3.325e-04  Data: 1.494 (1.494)
Train: 368 [  50/1251 (  4%)]  Loss: 3.303 (3.20)  Time: 0.771s, 1328.24/s  (0.832s, 1230.62/s)  LR: 3.325e-04  Data: 0.010 (0.045)
Train: 368 [ 100/1251 (  8%)]  Loss: 3.170 (3.19)  Time: 0.788s, 1298.70/s  (0.810s, 1264.62/s)  LR: 3.325e-04  Data: 0.010 (0.028)
Train: 368 [ 150/1251 ( 12%)]  Loss: 3.369 (3.24)  Time: 0.775s, 1320.55/s  (0.800s, 1279.50/s)  LR: 3.325e-04  Data: 0.009 (0.022)
Train: 368 [ 200/1251 ( 16%)]  Loss: 3.237 (3.24)  Time: 0.779s, 1313.73/s  (0.795s, 1288.60/s)  LR: 3.325e-04  Data: 0.010 (0.019)
Train: 368 [ 250/1251 ( 20%)]  Loss: 3.353 (3.26)  Time: 0.772s, 1326.57/s  (0.792s, 1293.19/s)  LR: 3.325e-04  Data: 0.011 (0.017)
Train: 368 [ 300/1251 ( 24%)]  Loss: 3.241 (3.25)  Time: 0.785s, 1304.81/s  (0.790s, 1296.36/s)  LR: 3.325e-04  Data: 0.010 (0.016)
Train: 368 [ 350/1251 ( 28%)]  Loss: 3.712 (3.31)  Time: 0.784s, 1306.72/s  (0.788s, 1298.76/s)  LR: 3.325e-04  Data: 0.010 (0.015)
Train: 368 [ 400/1251 ( 32%)]  Loss: 3.432 (3.32)  Time: 0.787s, 1300.68/s  (0.788s, 1300.11/s)  LR: 3.325e-04  Data: 0.010 (0.014)
Train: 368 [ 450/1251 ( 36%)]  Loss: 3.200 (3.31)  Time: 0.773s, 1325.29/s  (0.788s, 1299.87/s)  LR: 3.325e-04  Data: 0.009 (0.014)
Train: 368 [ 500/1251 ( 40%)]  Loss: 3.185 (3.30)  Time: 0.814s, 1257.87/s  (0.789s, 1297.55/s)  LR: 3.325e-04  Data: 0.009 (0.013)
Train: 368 [ 550/1251 ( 44%)]  Loss: 3.422 (3.31)  Time: 0.773s, 1325.07/s  (0.790s, 1296.80/s)  LR: 3.325e-04  Data: 0.010 (0.013)
Train: 368 [ 600/1251 ( 48%)]  Loss: 3.375 (3.32)  Time: 0.773s, 1324.37/s  (0.789s, 1297.65/s)  LR: 3.325e-04  Data: 0.009 (0.013)
Train: 368 [ 650/1251 ( 52%)]  Loss: 3.199 (3.31)  Time: 0.810s, 1264.64/s  (0.789s, 1298.63/s)  LR: 3.325e-04  Data: 0.009 (0.013)
Train: 368 [ 700/1251 ( 56%)]  Loss: 3.677 (3.33)  Time: 0.771s, 1328.35/s  (0.789s, 1297.54/s)  LR: 3.325e-04  Data: 0.009 (0.012)
Train: 368 [ 750/1251 ( 60%)]  Loss: 3.577 (3.35)  Time: 0.782s, 1310.29/s  (0.789s, 1298.21/s)  LR: 3.325e-04  Data: 0.009 (0.012)
Train: 368 [ 800/1251 ( 64%)]  Loss: 3.464 (3.35)  Time: 0.776s, 1319.37/s  (0.788s, 1299.08/s)  LR: 3.325e-04  Data: 0.010 (0.012)
Train: 368 [ 850/1251 ( 68%)]  Loss: 3.682 (3.37)  Time: 0.773s, 1323.99/s  (0.788s, 1299.78/s)  LR: 3.325e-04  Data: 0.009 (0.012)
Train: 368 [ 900/1251 ( 72%)]  Loss: 3.272 (3.37)  Time: 0.844s, 1213.20/s  (0.789s, 1298.57/s)  LR: 3.325e-04  Data: 0.010 (0.012)
Train: 368 [ 950/1251 ( 76%)]  Loss: 3.525 (3.37)  Time: 0.847s, 1208.37/s  (0.790s, 1295.68/s)  LR: 3.325e-04  Data: 0.009 (0.012)
Train: 368 [1000/1251 ( 80%)]  Loss: 3.342 (3.37)  Time: 0.834s, 1227.21/s  (0.791s, 1294.05/s)  LR: 3.325e-04  Data: 0.011 (0.012)
Train: 368 [1050/1251 ( 84%)]  Loss: 3.436 (3.38)  Time: 0.773s, 1324.07/s  (0.792s, 1293.70/s)  LR: 3.325e-04  Data: 0.010 (0.012)
Train: 368 [1100/1251 ( 88%)]  Loss: 3.360 (3.38)  Time: 0.790s, 1296.38/s  (0.791s, 1294.50/s)  LR: 3.325e-04  Data: 0.010 (0.011)
Train: 368 [1150/1251 ( 92%)]  Loss: 3.340 (3.37)  Time: 0.771s, 1327.59/s  (0.790s, 1295.44/s)  LR: 3.325e-04  Data: 0.009 (0.011)
Train: 368 [1200/1251 ( 96%)]  Loss: 2.282 (3.33)  Time: 0.776s, 1319.28/s  (0.790s, 1295.94/s)  LR: 3.325e-04  Data: 0.009 (0.011)
Train: 368 [1250/1251 (100%)]  Loss: 3.471 (3.34)  Time: 0.760s, 1347.27/s  (0.790s, 1296.61/s)  LR: 3.325e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.508 (1.508)  Loss:  0.7505 (0.7505)  Acc@1: 89.8438 (89.8438)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.194 (0.574)  Loss:  0.8154 (1.2113)  Acc@1: 85.9670 (77.1520)  Acc@5: 97.2877 (93.8360)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-367.pth.tar', 77.19600000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-363.pth.tar', 77.16599998291015)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-368.pth.tar', 77.15199998046874)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-355.pth.tar', 77.04600006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-366.pth.tar', 76.93200008789063)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-350.pth.tar', 76.87799992675781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-362.pth.tar', 76.87400005859375)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-356.pth.tar', 76.82200003417968)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-364.pth.tar', 76.77800003173829)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-361.pth.tar', 76.76200015869141)

Train: 369 [   0/1251 (  0%)]  Loss: 3.518 (3.52)  Time: 2.194s,  466.81/s  (2.194s,  466.81/s)  LR: 3.300e-04  Data: 1.465 (1.465)
Train: 369 [  50/1251 (  4%)]  Loss: 3.489 (3.50)  Time: 0.775s, 1321.75/s  (0.814s, 1257.52/s)  LR: 3.300e-04  Data: 0.010 (0.043)
Train: 369 [ 100/1251 (  8%)]  Loss: 3.511 (3.51)  Time: 0.784s, 1306.80/s  (0.802s, 1277.28/s)  LR: 3.300e-04  Data: 0.009 (0.027)
Train: 369 [ 150/1251 ( 12%)]  Loss: 3.380 (3.47)  Time: 0.773s, 1325.39/s  (0.801s, 1278.16/s)  LR: 3.300e-04  Data: 0.010 (0.021)
Train: 369 [ 200/1251 ( 16%)]  Loss: 3.300 (3.44)  Time: 0.782s, 1309.13/s  (0.796s, 1286.93/s)  LR: 3.300e-04  Data: 0.012 (0.018)
Train: 369 [ 250/1251 ( 20%)]  Loss: 3.314 (3.42)  Time: 0.773s, 1324.65/s  (0.794s, 1289.77/s)  LR: 3.300e-04  Data: 0.010 (0.017)
Train: 369 [ 300/1251 ( 24%)]  Loss: 3.435 (3.42)  Time: 0.775s, 1321.20/s  (0.795s, 1287.49/s)  LR: 3.300e-04  Data: 0.009 (0.016)
Train: 369 [ 350/1251 ( 28%)]  Loss: 2.913 (3.36)  Time: 0.773s, 1324.07/s  (0.795s, 1288.73/s)  LR: 3.300e-04  Data: 0.010 (0.015)
Train: 369 [ 400/1251 ( 32%)]  Loss: 3.278 (3.35)  Time: 0.773s, 1325.50/s  (0.793s, 1291.25/s)  LR: 3.300e-04  Data: 0.010 (0.014)
Train: 369 [ 450/1251 ( 36%)]  Loss: 3.229 (3.34)  Time: 0.782s, 1309.98/s  (0.792s, 1293.41/s)  LR: 3.300e-04  Data: 0.009 (0.014)
Train: 369 [ 500/1251 ( 40%)]  Loss: 3.384 (3.34)  Time: 0.786s, 1303.30/s  (0.791s, 1294.64/s)  LR: 3.300e-04  Data: 0.010 (0.013)
Train: 369 [ 550/1251 ( 44%)]  Loss: 3.538 (3.36)  Time: 0.771s, 1328.47/s  (0.790s, 1296.37/s)  LR: 3.300e-04  Data: 0.010 (0.013)
Train: 369 [ 600/1251 ( 48%)]  Loss: 2.931 (3.32)  Time: 0.775s, 1320.65/s  (0.790s, 1296.93/s)  LR: 3.300e-04  Data: 0.010 (0.013)
Train: 369 [ 650/1251 ( 52%)]  Loss: 3.310 (3.32)  Time: 0.781s, 1311.45/s  (0.790s, 1296.96/s)  LR: 3.300e-04  Data: 0.010 (0.013)
Train: 369 [ 700/1251 ( 56%)]  Loss: 3.282 (3.32)  Time: 0.775s, 1321.00/s  (0.789s, 1297.55/s)  LR: 3.300e-04  Data: 0.010 (0.013)
Train: 369 [ 750/1251 ( 60%)]  Loss: 3.276 (3.32)  Time: 0.821s, 1247.28/s  (0.789s, 1297.70/s)  LR: 3.300e-04  Data: 0.010 (0.012)
Train: 369 [ 800/1251 ( 64%)]  Loss: 3.245 (3.31)  Time: 0.813s, 1260.20/s  (0.788s, 1299.07/s)  LR: 3.300e-04  Data: 0.010 (0.012)
Train: 369 [ 850/1251 ( 68%)]  Loss: 3.426 (3.32)  Time: 0.815s, 1256.06/s  (0.788s, 1299.25/s)  LR: 3.300e-04  Data: 0.011 (0.012)
Train: 369 [ 900/1251 ( 72%)]  Loss: 3.165 (3.31)  Time: 0.855s, 1197.98/s  (0.789s, 1297.75/s)  LR: 3.300e-04  Data: 0.010 (0.012)
Train: 369 [ 950/1251 ( 76%)]  Loss: 3.628 (3.33)  Time: 0.783s, 1307.83/s  (0.788s, 1298.71/s)  LR: 3.300e-04  Data: 0.015 (0.012)
Train: 369 [1000/1251 ( 80%)]  Loss: 3.230 (3.32)  Time: 0.773s, 1325.05/s  (0.788s, 1299.46/s)  LR: 3.300e-04  Data: 0.010 (0.012)
Train: 369 [1050/1251 ( 84%)]  Loss: 3.423 (3.33)  Time: 0.774s, 1323.20/s  (0.788s, 1299.84/s)  LR: 3.300e-04  Data: 0.010 (0.012)
Train: 369 [1100/1251 ( 88%)]  Loss: 3.254 (3.32)  Time: 0.778s, 1315.81/s  (0.788s, 1300.16/s)  LR: 3.300e-04  Data: 0.009 (0.012)
Train: 369 [1150/1251 ( 92%)]  Loss: 3.566 (3.33)  Time: 0.781s, 1311.71/s  (0.787s, 1300.71/s)  LR: 3.300e-04  Data: 0.010 (0.012)
Train: 369 [1200/1251 ( 96%)]  Loss: 3.286 (3.33)  Time: 0.773s, 1325.43/s  (0.787s, 1301.38/s)  LR: 3.300e-04  Data: 0.009 (0.012)
Train: 369 [1250/1251 (100%)]  Loss: 3.472 (3.34)  Time: 0.759s, 1349.10/s  (0.786s, 1302.01/s)  LR: 3.300e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.537 (1.537)  Loss:  0.7061 (0.7061)  Acc@1: 90.9180 (90.9180)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.194 (0.567)  Loss:  0.7910 (1.2279)  Acc@1: 87.3821 (77.0860)  Acc@5: 97.4057 (93.7560)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-367.pth.tar', 77.19600000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-363.pth.tar', 77.16599998291015)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-368.pth.tar', 77.15199998046874)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-369.pth.tar', 77.08599994873047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-355.pth.tar', 77.04600006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-366.pth.tar', 76.93200008789063)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-350.pth.tar', 76.87799992675781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-362.pth.tar', 76.87400005859375)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-356.pth.tar', 76.82200003417968)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-364.pth.tar', 76.77800003173829)

Train: 370 [   0/1251 (  0%)]  Loss: 3.109 (3.11)  Time: 2.260s,  453.02/s  (2.260s,  453.02/s)  LR: 3.276e-04  Data: 1.490 (1.490)
Train: 370 [  50/1251 (  4%)]  Loss: 3.484 (3.30)  Time: 0.771s, 1327.91/s  (0.818s, 1252.55/s)  LR: 3.276e-04  Data: 0.010 (0.043)
Train: 370 [ 100/1251 (  8%)]  Loss: 3.588 (3.39)  Time: 0.808s, 1267.67/s  (0.803s, 1274.76/s)  LR: 3.276e-04  Data: 0.009 (0.027)
Train: 370 [ 150/1251 ( 12%)]  Loss: 2.913 (3.27)  Time: 0.774s, 1323.36/s  (0.796s, 1286.08/s)  LR: 3.276e-04  Data: 0.010 (0.021)
Train: 370 [ 200/1251 ( 16%)]  Loss: 3.258 (3.27)  Time: 0.778s, 1316.04/s  (0.794s, 1289.35/s)  LR: 3.276e-04  Data: 0.011 (0.018)
Train: 370 [ 250/1251 ( 20%)]  Loss: 3.103 (3.24)  Time: 0.790s, 1296.18/s  (0.791s, 1294.67/s)  LR: 3.276e-04  Data: 0.009 (0.017)
Train: 370 [ 300/1251 ( 24%)]  Loss: 3.343 (3.26)  Time: 0.781s, 1310.96/s  (0.791s, 1293.94/s)  LR: 3.276e-04  Data: 0.010 (0.015)
Train: 370 [ 350/1251 ( 28%)]  Loss: 3.552 (3.29)  Time: 0.705s, 1451.51/s  (0.789s, 1297.62/s)  LR: 3.276e-04  Data: 0.009 (0.015)
Train: 370 [ 400/1251 ( 32%)]  Loss: 2.814 (3.24)  Time: 0.778s, 1315.56/s  (0.788s, 1298.95/s)  LR: 3.276e-04  Data: 0.010 (0.014)
Train: 370 [ 450/1251 ( 36%)]  Loss: 3.499 (3.27)  Time: 0.790s, 1295.47/s  (0.787s, 1300.95/s)  LR: 3.276e-04  Data: 0.010 (0.014)
Train: 370 [ 500/1251 ( 40%)]  Loss: 3.387 (3.28)  Time: 0.831s, 1231.96/s  (0.788s, 1300.03/s)  LR: 3.276e-04  Data: 0.010 (0.013)
Train: 370 [ 550/1251 ( 44%)]  Loss: 3.232 (3.27)  Time: 0.772s, 1325.88/s  (0.788s, 1299.73/s)  LR: 3.276e-04  Data: 0.010 (0.013)
Train: 370 [ 600/1251 ( 48%)]  Loss: 3.527 (3.29)  Time: 0.781s, 1310.91/s  (0.787s, 1301.29/s)  LR: 3.276e-04  Data: 0.010 (0.013)
Train: 370 [ 650/1251 ( 52%)]  Loss: 3.085 (3.28)  Time: 0.773s, 1324.24/s  (0.787s, 1301.78/s)  LR: 3.276e-04  Data: 0.010 (0.012)
Train: 370 [ 700/1251 ( 56%)]  Loss: 3.241 (3.28)  Time: 0.773s, 1325.29/s  (0.786s, 1302.55/s)  LR: 3.276e-04  Data: 0.010 (0.012)
Train: 370 [ 750/1251 ( 60%)]  Loss: 3.395 (3.28)  Time: 0.773s, 1325.56/s  (0.786s, 1302.47/s)  LR: 3.276e-04  Data: 0.010 (0.012)
Train: 370 [ 800/1251 ( 64%)]  Loss: 3.296 (3.28)  Time: 0.786s, 1303.33/s  (0.786s, 1302.01/s)  LR: 3.276e-04  Data: 0.010 (0.012)
Train: 370 [ 850/1251 ( 68%)]  Loss: 3.145 (3.28)  Time: 0.773s, 1324.09/s  (0.786s, 1302.66/s)  LR: 3.276e-04  Data: 0.010 (0.012)
Train: 370 [ 900/1251 ( 72%)]  Loss: 3.096 (3.27)  Time: 0.775s, 1321.07/s  (0.787s, 1301.72/s)  LR: 3.276e-04  Data: 0.009 (0.012)
Train: 370 [ 950/1251 ( 76%)]  Loss: 3.181 (3.26)  Time: 0.834s, 1227.81/s  (0.786s, 1302.21/s)  LR: 3.276e-04  Data: 0.009 (0.012)
Train: 370 [1000/1251 ( 80%)]  Loss: 3.047 (3.25)  Time: 0.808s, 1267.50/s  (0.788s, 1299.81/s)  LR: 3.276e-04  Data: 0.009 (0.012)
Train: 370 [1050/1251 ( 84%)]  Loss: 3.430 (3.26)  Time: 0.818s, 1251.44/s  (0.788s, 1299.39/s)  LR: 3.276e-04  Data: 0.010 (0.011)
Train: 370 [1100/1251 ( 88%)]  Loss: 3.393 (3.27)  Time: 0.828s, 1236.85/s  (0.788s, 1299.56/s)  LR: 3.276e-04  Data: 0.014 (0.011)
Train: 370 [1150/1251 ( 92%)]  Loss: 3.158 (3.26)  Time: 0.774s, 1322.73/s  (0.788s, 1299.12/s)  LR: 3.276e-04  Data: 0.009 (0.011)
Train: 370 [1200/1251 ( 96%)]  Loss: 3.377 (3.27)  Time: 0.772s, 1325.91/s  (0.788s, 1299.69/s)  LR: 3.276e-04  Data: 0.009 (0.011)
Train: 370 [1250/1251 (100%)]  Loss: 3.461 (3.27)  Time: 0.759s, 1350.03/s  (0.788s, 1300.05/s)  LR: 3.276e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.565 (1.565)  Loss:  0.6934 (0.6934)  Acc@1: 90.5273 (90.5273)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.194 (0.567)  Loss:  0.8193 (1.1953)  Acc@1: 85.3774 (77.1680)  Acc@5: 96.9340 (93.7720)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-367.pth.tar', 77.19600000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-370.pth.tar', 77.16799998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-363.pth.tar', 77.16599998291015)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-368.pth.tar', 77.15199998046874)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-369.pth.tar', 77.08599994873047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-355.pth.tar', 77.04600006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-366.pth.tar', 76.93200008789063)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-350.pth.tar', 76.87799992675781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-362.pth.tar', 76.87400005859375)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-356.pth.tar', 76.82200003417968)

Train: 371 [   0/1251 (  0%)]  Loss: 3.314 (3.31)  Time: 2.279s,  449.39/s  (2.279s,  449.39/s)  LR: 3.252e-04  Data: 1.549 (1.549)
Train: 371 [  50/1251 (  4%)]  Loss: 2.587 (2.95)  Time: 0.832s, 1231.45/s  (0.829s, 1234.93/s)  LR: 3.252e-04  Data: 0.009 (0.045)
Train: 371 [ 100/1251 (  8%)]  Loss: 3.061 (2.99)  Time: 0.773s, 1324.27/s  (0.804s, 1273.40/s)  LR: 3.252e-04  Data: 0.010 (0.028)
Train: 371 [ 150/1251 ( 12%)]  Loss: 3.297 (3.06)  Time: 0.772s, 1325.86/s  (0.797s, 1285.40/s)  LR: 3.252e-04  Data: 0.009 (0.022)
Train: 371 [ 200/1251 ( 16%)]  Loss: 3.480 (3.15)  Time: 0.771s, 1327.89/s  (0.795s, 1288.50/s)  LR: 3.252e-04  Data: 0.010 (0.019)
Train: 371 [ 250/1251 ( 20%)]  Loss: 3.559 (3.22)  Time: 0.774s, 1323.46/s  (0.793s, 1291.60/s)  LR: 3.252e-04  Data: 0.010 (0.017)
Train: 371 [ 300/1251 ( 24%)]  Loss: 3.160 (3.21)  Time: 0.774s, 1323.56/s  (0.790s, 1295.96/s)  LR: 3.252e-04  Data: 0.010 (0.016)
Train: 371 [ 350/1251 ( 28%)]  Loss: 3.633 (3.26)  Time: 0.771s, 1328.15/s  (0.789s, 1298.38/s)  LR: 3.252e-04  Data: 0.010 (0.015)
Train: 371 [ 400/1251 ( 32%)]  Loss: 3.250 (3.26)  Time: 0.815s, 1257.13/s  (0.788s, 1299.81/s)  LR: 3.252e-04  Data: 0.011 (0.014)
Train: 371 [ 450/1251 ( 36%)]  Loss: 3.100 (3.24)  Time: 0.779s, 1314.09/s  (0.788s, 1299.75/s)  LR: 3.252e-04  Data: 0.009 (0.014)
Train: 371 [ 500/1251 ( 40%)]  Loss: 3.098 (3.23)  Time: 0.772s, 1325.91/s  (0.787s, 1300.99/s)  LR: 3.252e-04  Data: 0.009 (0.013)
Train: 371 [ 550/1251 ( 44%)]  Loss: 3.270 (3.23)  Time: 0.774s, 1323.82/s  (0.786s, 1302.49/s)  LR: 3.252e-04  Data: 0.010 (0.013)
Train: 371 [ 600/1251 ( 48%)]  Loss: 3.277 (3.24)  Time: 0.774s, 1323.19/s  (0.786s, 1302.59/s)  LR: 3.252e-04  Data: 0.009 (0.013)
Train: 371 [ 650/1251 ( 52%)]  Loss: 3.354 (3.25)  Time: 0.790s, 1296.61/s  (0.786s, 1303.10/s)  LR: 3.252e-04  Data: 0.010 (0.013)
Train: 371 [ 700/1251 ( 56%)]  Loss: 3.120 (3.24)  Time: 0.771s, 1328.40/s  (0.785s, 1304.15/s)  LR: 3.252e-04  Data: 0.010 (0.012)
Train: 371 [ 750/1251 ( 60%)]  Loss: 3.428 (3.25)  Time: 0.773s, 1325.06/s  (0.785s, 1305.07/s)  LR: 3.252e-04  Data: 0.010 (0.012)
Train: 371 [ 800/1251 ( 64%)]  Loss: 3.394 (3.26)  Time: 0.773s, 1324.86/s  (0.785s, 1304.48/s)  LR: 3.252e-04  Data: 0.009 (0.012)
Train: 371 [ 850/1251 ( 68%)]  Loss: 3.270 (3.26)  Time: 0.773s, 1324.23/s  (0.785s, 1305.11/s)  LR: 3.252e-04  Data: 0.009 (0.012)
Train: 371 [ 900/1251 ( 72%)]  Loss: 3.266 (3.26)  Time: 0.773s, 1325.01/s  (0.785s, 1305.26/s)  LR: 3.252e-04  Data: 0.009 (0.012)
Train: 371 [ 950/1251 ( 76%)]  Loss: 3.417 (3.27)  Time: 0.772s, 1326.10/s  (0.784s, 1305.53/s)  LR: 3.252e-04  Data: 0.010 (0.012)
Train: 371 [1000/1251 ( 80%)]  Loss: 3.380 (3.27)  Time: 0.773s, 1324.77/s  (0.784s, 1305.98/s)  LR: 3.252e-04  Data: 0.009 (0.012)
Train: 371 [1050/1251 ( 84%)]  Loss: 3.218 (3.27)  Time: 0.834s, 1227.80/s  (0.784s, 1306.28/s)  LR: 3.252e-04  Data: 0.010 (0.012)
Train: 371 [1100/1251 ( 88%)]  Loss: 3.304 (3.27)  Time: 0.815s, 1255.91/s  (0.784s, 1306.37/s)  LR: 3.252e-04  Data: 0.011 (0.011)
Train: 371 [1150/1251 ( 92%)]  Loss: 3.300 (3.27)  Time: 0.815s, 1255.70/s  (0.784s, 1306.34/s)  LR: 3.252e-04  Data: 0.011 (0.011)
Train: 371 [1200/1251 ( 96%)]  Loss: 3.370 (3.28)  Time: 0.772s, 1326.69/s  (0.785s, 1305.01/s)  LR: 3.252e-04  Data: 0.009 (0.011)
Train: 371 [1250/1251 (100%)]  Loss: 3.393 (3.28)  Time: 0.804s, 1273.56/s  (0.785s, 1304.87/s)  LR: 3.252e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.523 (1.523)  Loss:  0.8955 (0.8955)  Acc@1: 90.4297 (90.4297)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.194 (0.562)  Loss:  1.0264 (1.3675)  Acc@1: 86.0849 (76.9620)  Acc@5: 97.0519 (93.6780)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-367.pth.tar', 77.19600000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-370.pth.tar', 77.16799998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-363.pth.tar', 77.16599998291015)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-368.pth.tar', 77.15199998046874)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-369.pth.tar', 77.08599994873047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-355.pth.tar', 77.04600006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-371.pth.tar', 76.96200003173828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-366.pth.tar', 76.93200008789063)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-350.pth.tar', 76.87799992675781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-362.pth.tar', 76.87400005859375)

Train: 372 [   0/1251 (  0%)]  Loss: 3.526 (3.53)  Time: 2.322s,  441.01/s  (2.322s,  441.01/s)  LR: 3.228e-04  Data: 1.586 (1.586)
Train: 372 [  50/1251 (  4%)]  Loss: 3.276 (3.40)  Time: 0.774s, 1323.66/s  (0.817s, 1253.57/s)  LR: 3.228e-04  Data: 0.010 (0.044)
Train: 372 [ 100/1251 (  8%)]  Loss: 3.279 (3.36)  Time: 0.771s, 1328.13/s  (0.800s, 1280.24/s)  LR: 3.228e-04  Data: 0.008 (0.027)
Train: 372 [ 150/1251 ( 12%)]  Loss: 3.187 (3.32)  Time: 0.778s, 1315.74/s  (0.797s, 1284.25/s)  LR: 3.228e-04  Data: 0.011 (0.021)
Train: 372 [ 200/1251 ( 16%)]  Loss: 3.399 (3.33)  Time: 0.774s, 1322.52/s  (0.794s, 1290.47/s)  LR: 3.228e-04  Data: 0.009 (0.018)
Train: 372 [ 250/1251 ( 20%)]  Loss: 3.338 (3.33)  Time: 0.785s, 1305.04/s  (0.792s, 1293.69/s)  LR: 3.228e-04  Data: 0.009 (0.017)
Train: 372 [ 300/1251 ( 24%)]  Loss: 3.539 (3.36)  Time: 0.787s, 1300.64/s  (0.789s, 1297.36/s)  LR: 3.228e-04  Data: 0.011 (0.015)
Train: 372 [ 350/1251 ( 28%)]  Loss: 3.818 (3.42)  Time: 0.775s, 1321.89/s  (0.788s, 1299.16/s)  LR: 3.228e-04  Data: 0.011 (0.015)
Train: 372 [ 400/1251 ( 32%)]  Loss: 3.254 (3.40)  Time: 0.785s, 1303.89/s  (0.790s, 1296.76/s)  LR: 3.228e-04  Data: 0.014 (0.014)
Train: 372 [ 450/1251 ( 36%)]  Loss: 3.325 (3.39)  Time: 0.773s, 1324.32/s  (0.789s, 1297.39/s)  LR: 3.228e-04  Data: 0.010 (0.014)
Train: 372 [ 500/1251 ( 40%)]  Loss: 3.271 (3.38)  Time: 0.777s, 1318.70/s  (0.788s, 1299.17/s)  LR: 3.228e-04  Data: 0.009 (0.013)
Train: 372 [ 550/1251 ( 44%)]  Loss: 3.098 (3.36)  Time: 0.790s, 1295.55/s  (0.788s, 1299.26/s)  LR: 3.228e-04  Data: 0.014 (0.013)
Train: 372 [ 600/1251 ( 48%)]  Loss: 3.261 (3.35)  Time: 0.775s, 1320.54/s  (0.787s, 1300.97/s)  LR: 3.228e-04  Data: 0.010 (0.013)
Train: 372 [ 650/1251 ( 52%)]  Loss: 3.116 (3.33)  Time: 0.774s, 1323.13/s  (0.787s, 1301.86/s)  LR: 3.228e-04  Data: 0.010 (0.012)
Train: 372 [ 700/1251 ( 56%)]  Loss: 3.718 (3.36)  Time: 0.774s, 1323.71/s  (0.786s, 1302.52/s)  LR: 3.228e-04  Data: 0.010 (0.012)
Train: 372 [ 750/1251 ( 60%)]  Loss: 3.151 (3.35)  Time: 0.775s, 1321.39/s  (0.786s, 1303.26/s)  LR: 3.228e-04  Data: 0.010 (0.012)
Train: 372 [ 800/1251 ( 64%)]  Loss: 3.464 (3.35)  Time: 0.773s, 1324.76/s  (0.785s, 1304.26/s)  LR: 3.228e-04  Data: 0.010 (0.012)
Train: 372 [ 850/1251 ( 68%)]  Loss: 3.211 (3.35)  Time: 0.791s, 1295.31/s  (0.785s, 1304.80/s)  LR: 3.228e-04  Data: 0.009 (0.012)
Train: 372 [ 900/1251 ( 72%)]  Loss: 3.435 (3.35)  Time: 0.773s, 1324.58/s  (0.785s, 1305.06/s)  LR: 3.228e-04  Data: 0.010 (0.012)
Train: 372 [ 950/1251 ( 76%)]  Loss: 3.124 (3.34)  Time: 0.773s, 1324.40/s  (0.784s, 1305.42/s)  LR: 3.228e-04  Data: 0.009 (0.012)
Train: 372 [1000/1251 ( 80%)]  Loss: 3.115 (3.33)  Time: 0.808s, 1267.49/s  (0.785s, 1305.26/s)  LR: 3.228e-04  Data: 0.009 (0.012)
Train: 372 [1050/1251 ( 84%)]  Loss: 3.177 (3.32)  Time: 0.837s, 1223.15/s  (0.786s, 1303.61/s)  LR: 3.228e-04  Data: 0.010 (0.011)
Train: 372 [1100/1251 ( 88%)]  Loss: 3.061 (3.31)  Time: 0.775s, 1321.73/s  (0.786s, 1303.25/s)  LR: 3.228e-04  Data: 0.009 (0.011)
Train: 372 [1150/1251 ( 92%)]  Loss: 3.168 (3.30)  Time: 0.773s, 1324.34/s  (0.785s, 1303.67/s)  LR: 3.228e-04  Data: 0.010 (0.011)
Train: 372 [1200/1251 ( 96%)]  Loss: 3.217 (3.30)  Time: 0.774s, 1323.00/s  (0.785s, 1303.87/s)  LR: 3.228e-04  Data: 0.010 (0.011)
Train: 372 [1250/1251 (100%)]  Loss: 3.553 (3.31)  Time: 0.761s, 1345.73/s  (0.785s, 1304.44/s)  LR: 3.228e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.535 (1.535)  Loss:  0.8110 (0.8110)  Acc@1: 90.7227 (90.7227)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.194 (0.563)  Loss:  0.9102 (1.2752)  Acc@1: 86.3208 (77.0700)  Acc@5: 97.1698 (93.7920)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-367.pth.tar', 77.19600000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-370.pth.tar', 77.16799998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-363.pth.tar', 77.16599998291015)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-368.pth.tar', 77.15199998046874)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-369.pth.tar', 77.08599994873047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-372.pth.tar', 77.07000013427735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-355.pth.tar', 77.04600006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-371.pth.tar', 76.96200003173828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-366.pth.tar', 76.93200008789063)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-350.pth.tar', 76.87799992675781)

Train: 373 [   0/1251 (  0%)]  Loss: 3.279 (3.28)  Time: 2.319s,  441.51/s  (2.319s,  441.51/s)  LR: 3.204e-04  Data: 1.600 (1.600)
Train: 373 [  50/1251 (  4%)]  Loss: 3.287 (3.28)  Time: 0.772s, 1326.45/s  (0.814s, 1258.05/s)  LR: 3.204e-04  Data: 0.010 (0.048)
Train: 373 [ 100/1251 (  8%)]  Loss: 3.505 (3.36)  Time: 0.815s, 1256.68/s  (0.813s, 1259.98/s)  LR: 3.204e-04  Data: 0.011 (0.030)
Train: 373 [ 150/1251 ( 12%)]  Loss: 3.352 (3.36)  Time: 0.818s, 1252.38/s  (0.805s, 1271.36/s)  LR: 3.204e-04  Data: 0.010 (0.023)
Train: 373 [ 200/1251 ( 16%)]  Loss: 3.069 (3.30)  Time: 0.782s, 1310.23/s  (0.799s, 1281.60/s)  LR: 3.204e-04  Data: 0.009 (0.020)
Train: 373 [ 250/1251 ( 20%)]  Loss: 3.759 (3.38)  Time: 0.784s, 1305.83/s  (0.795s, 1288.33/s)  LR: 3.204e-04  Data: 0.010 (0.018)
Train: 373 [ 300/1251 ( 24%)]  Loss: 3.551 (3.40)  Time: 0.776s, 1319.04/s  (0.795s, 1287.29/s)  LR: 3.204e-04  Data: 0.009 (0.016)
Train: 373 [ 350/1251 ( 28%)]  Loss: 2.949 (3.34)  Time: 0.813s, 1258.87/s  (0.794s, 1290.14/s)  LR: 3.204e-04  Data: 0.010 (0.016)
Train: 373 [ 400/1251 ( 32%)]  Loss: 3.600 (3.37)  Time: 0.790s, 1296.48/s  (0.792s, 1292.44/s)  LR: 3.204e-04  Data: 0.010 (0.015)
Train: 373 [ 450/1251 ( 36%)]  Loss: 3.444 (3.38)  Time: 0.777s, 1318.60/s  (0.791s, 1294.53/s)  LR: 3.204e-04  Data: 0.010 (0.014)
Train: 373 [ 500/1251 ( 40%)]  Loss: 3.160 (3.36)  Time: 0.772s, 1325.96/s  (0.790s, 1296.81/s)  LR: 3.204e-04  Data: 0.009 (0.014)
Train: 373 [ 550/1251 ( 44%)]  Loss: 3.414 (3.36)  Time: 0.773s, 1325.39/s  (0.789s, 1298.39/s)  LR: 3.204e-04  Data: 0.009 (0.013)
Train: 373 [ 600/1251 ( 48%)]  Loss: 3.381 (3.37)  Time: 0.773s, 1324.09/s  (0.789s, 1297.92/s)  LR: 3.204e-04  Data: 0.010 (0.013)
Train: 373 [ 650/1251 ( 52%)]  Loss: 3.373 (3.37)  Time: 0.774s, 1322.52/s  (0.788s, 1299.23/s)  LR: 3.204e-04  Data: 0.010 (0.013)
Train: 373 [ 700/1251 ( 56%)]  Loss: 3.666 (3.39)  Time: 0.773s, 1324.50/s  (0.787s, 1300.42/s)  LR: 3.204e-04  Data: 0.010 (0.013)
Train: 373 [ 750/1251 ( 60%)]  Loss: 3.233 (3.38)  Time: 0.807s, 1268.28/s  (0.787s, 1300.66/s)  LR: 3.204e-04  Data: 0.010 (0.013)
Train: 373 [ 800/1251 ( 64%)]  Loss: 3.800 (3.40)  Time: 0.772s, 1325.60/s  (0.787s, 1301.47/s)  LR: 3.204e-04  Data: 0.010 (0.012)
Train: 373 [ 850/1251 ( 68%)]  Loss: 2.911 (3.37)  Time: 0.771s, 1328.39/s  (0.787s, 1301.42/s)  LR: 3.204e-04  Data: 0.009 (0.012)
Train: 373 [ 900/1251 ( 72%)]  Loss: 3.273 (3.37)  Time: 0.860s, 1191.35/s  (0.786s, 1301.97/s)  LR: 3.204e-04  Data: 0.010 (0.012)
Train: 373 [ 950/1251 ( 76%)]  Loss: 3.135 (3.36)  Time: 0.783s, 1307.39/s  (0.786s, 1302.24/s)  LR: 3.204e-04  Data: 0.010 (0.012)
Train: 373 [1000/1251 ( 80%)]  Loss: 3.599 (3.37)  Time: 0.786s, 1303.04/s  (0.787s, 1301.72/s)  LR: 3.204e-04  Data: 0.010 (0.012)
Train: 373 [1050/1251 ( 84%)]  Loss: 3.457 (3.37)  Time: 0.777s, 1318.69/s  (0.786s, 1302.25/s)  LR: 3.204e-04  Data: 0.010 (0.012)
Train: 373 [1100/1251 ( 88%)]  Loss: 3.201 (3.37)  Time: 0.774s, 1322.85/s  (0.786s, 1302.82/s)  LR: 3.204e-04  Data: 0.010 (0.012)
Train: 373 [1150/1251 ( 92%)]  Loss: 3.174 (3.36)  Time: 0.774s, 1323.17/s  (0.786s, 1302.38/s)  LR: 3.204e-04  Data: 0.010 (0.012)
Train: 373 [1200/1251 ( 96%)]  Loss: 3.402 (3.36)  Time: 0.772s, 1325.94/s  (0.786s, 1302.95/s)  LR: 3.204e-04  Data: 0.010 (0.012)
Train: 373 [1250/1251 (100%)]  Loss: 3.750 (3.37)  Time: 0.760s, 1346.89/s  (0.786s, 1303.36/s)  LR: 3.204e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.586 (1.586)  Loss:  0.7104 (0.7104)  Acc@1: 89.6484 (89.6484)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.194 (0.563)  Loss:  0.8442 (1.2260)  Acc@1: 85.6132 (77.0200)  Acc@5: 96.9340 (93.8360)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-367.pth.tar', 77.19600000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-370.pth.tar', 77.16799998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-363.pth.tar', 77.16599998291015)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-368.pth.tar', 77.15199998046874)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-369.pth.tar', 77.08599994873047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-372.pth.tar', 77.07000013427735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-355.pth.tar', 77.04600006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-373.pth.tar', 77.01999995605469)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-371.pth.tar', 76.96200003173828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-366.pth.tar', 76.93200008789063)

Train: 374 [   0/1251 (  0%)]  Loss: 3.143 (3.14)  Time: 2.417s,  423.64/s  (2.417s,  423.64/s)  LR: 3.180e-04  Data: 1.691 (1.691)
Train: 374 [  50/1251 (  4%)]  Loss: 3.413 (3.28)  Time: 0.777s, 1317.95/s  (0.820s, 1248.37/s)  LR: 3.180e-04  Data: 0.009 (0.049)
Train: 374 [ 100/1251 (  8%)]  Loss: 3.223 (3.26)  Time: 0.814s, 1257.93/s  (0.811s, 1262.35/s)  LR: 3.180e-04  Data: 0.011 (0.030)
Train: 374 [ 150/1251 ( 12%)]  Loss: 3.442 (3.31)  Time: 0.773s, 1324.68/s  (0.802s, 1277.18/s)  LR: 3.180e-04  Data: 0.009 (0.023)
Train: 374 [ 200/1251 ( 16%)]  Loss: 3.122 (3.27)  Time: 0.784s, 1305.86/s  (0.795s, 1287.42/s)  LR: 3.180e-04  Data: 0.015 (0.020)
Train: 374 [ 250/1251 ( 20%)]  Loss: 3.433 (3.30)  Time: 0.772s, 1327.07/s  (0.792s, 1292.89/s)  LR: 3.180e-04  Data: 0.010 (0.018)
Train: 374 [ 300/1251 ( 24%)]  Loss: 3.170 (3.28)  Time: 0.772s, 1326.45/s  (0.790s, 1295.91/s)  LR: 3.180e-04  Data: 0.009 (0.016)
Train: 374 [ 350/1251 ( 28%)]  Loss: 3.048 (3.25)  Time: 0.773s, 1325.24/s  (0.788s, 1299.02/s)  LR: 3.180e-04  Data: 0.009 (0.016)
Train: 374 [ 400/1251 ( 32%)]  Loss: 3.201 (3.24)  Time: 0.780s, 1313.00/s  (0.787s, 1300.79/s)  LR: 3.180e-04  Data: 0.010 (0.015)
Train: 374 [ 450/1251 ( 36%)]  Loss: 2.978 (3.22)  Time: 0.777s, 1317.29/s  (0.787s, 1301.96/s)  LR: 3.180e-04  Data: 0.010 (0.014)
Train: 374 [ 500/1251 ( 40%)]  Loss: 3.528 (3.25)  Time: 0.829s, 1235.95/s  (0.786s, 1302.01/s)  LR: 3.180e-04  Data: 0.009 (0.014)
Train: 374 [ 550/1251 ( 44%)]  Loss: 3.227 (3.24)  Time: 0.857s, 1194.98/s  (0.787s, 1300.94/s)  LR: 3.180e-04  Data: 0.012 (0.013)
Train: 374 [ 600/1251 ( 48%)]  Loss: 3.140 (3.24)  Time: 0.773s, 1325.46/s  (0.787s, 1301.88/s)  LR: 3.180e-04  Data: 0.011 (0.013)
Train: 374 [ 650/1251 ( 52%)]  Loss: 3.316 (3.24)  Time: 0.774s, 1322.59/s  (0.786s, 1302.82/s)  LR: 3.180e-04  Data: 0.010 (0.013)
Train: 374 [ 700/1251 ( 56%)]  Loss: 3.523 (3.26)  Time: 0.806s, 1270.48/s  (0.787s, 1301.80/s)  LR: 3.180e-04  Data: 0.009 (0.013)
Train: 374 [ 750/1251 ( 60%)]  Loss: 3.286 (3.26)  Time: 0.816s, 1255.59/s  (0.787s, 1301.17/s)  LR: 3.180e-04  Data: 0.009 (0.013)
Train: 374 [ 800/1251 ( 64%)]  Loss: 3.334 (3.27)  Time: 0.774s, 1322.76/s  (0.788s, 1300.03/s)  LR: 3.180e-04  Data: 0.010 (0.012)
Train: 374 [ 850/1251 ( 68%)]  Loss: 3.533 (3.28)  Time: 0.789s, 1298.42/s  (0.787s, 1300.58/s)  LR: 3.180e-04  Data: 0.010 (0.012)
Train: 374 [ 900/1251 ( 72%)]  Loss: 3.417 (3.29)  Time: 0.795s, 1288.43/s  (0.787s, 1301.32/s)  LR: 3.180e-04  Data: 0.010 (0.012)
Train: 374 [ 950/1251 ( 76%)]  Loss: 3.362 (3.29)  Time: 0.777s, 1317.43/s  (0.787s, 1301.38/s)  LR: 3.180e-04  Data: 0.011 (0.012)
Train: 374 [1000/1251 ( 80%)]  Loss: 3.242 (3.29)  Time: 0.773s, 1324.64/s  (0.786s, 1302.07/s)  LR: 3.180e-04  Data: 0.010 (0.012)
Train: 374 [1050/1251 ( 84%)]  Loss: 3.494 (3.30)  Time: 0.806s, 1271.14/s  (0.786s, 1302.29/s)  LR: 3.180e-04  Data: 0.010 (0.012)
Train: 374 [1100/1251 ( 88%)]  Loss: 3.446 (3.31)  Time: 0.786s, 1302.89/s  (0.786s, 1302.41/s)  LR: 3.180e-04  Data: 0.013 (0.012)
Train: 374 [1150/1251 ( 92%)]  Loss: 3.293 (3.30)  Time: 0.773s, 1323.97/s  (0.786s, 1302.55/s)  LR: 3.180e-04  Data: 0.010 (0.012)
Train: 374 [1200/1251 ( 96%)]  Loss: 3.197 (3.30)  Time: 0.815s, 1255.81/s  (0.787s, 1301.80/s)  LR: 3.180e-04  Data: 0.011 (0.012)
Train: 374 [1250/1251 (100%)]  Loss: 3.434 (3.31)  Time: 0.759s, 1349.14/s  (0.787s, 1300.71/s)  LR: 3.180e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.553 (1.553)  Loss:  0.6274 (0.6274)  Acc@1: 90.7227 (90.7227)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.194 (0.574)  Loss:  0.8057 (1.1548)  Acc@1: 86.2028 (77.2480)  Acc@5: 96.4623 (93.9140)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-374.pth.tar', 77.24799995361329)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-367.pth.tar', 77.19600000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-370.pth.tar', 77.16799998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-363.pth.tar', 77.16599998291015)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-368.pth.tar', 77.15199998046874)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-369.pth.tar', 77.08599994873047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-372.pth.tar', 77.07000013427735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-355.pth.tar', 77.04600006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-373.pth.tar', 77.01999995605469)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-371.pth.tar', 76.96200003173828)

Train: 375 [   0/1251 (  0%)]  Loss: 3.300 (3.30)  Time: 2.173s,  471.34/s  (2.173s,  471.34/s)  LR: 3.156e-04  Data: 1.442 (1.442)
Train: 375 [  50/1251 (  4%)]  Loss: 3.350 (3.33)  Time: 0.773s, 1324.86/s  (0.816s, 1254.63/s)  LR: 3.156e-04  Data: 0.009 (0.041)
Train: 375 [ 100/1251 (  8%)]  Loss: 3.597 (3.42)  Time: 0.774s, 1323.40/s  (0.811s, 1262.91/s)  LR: 3.156e-04  Data: 0.010 (0.026)
Train: 375 [ 150/1251 ( 12%)]  Loss: 3.113 (3.34)  Time: 0.773s, 1325.01/s  (0.802s, 1276.11/s)  LR: 3.156e-04  Data: 0.010 (0.020)
Train: 375 [ 200/1251 ( 16%)]  Loss: 3.558 (3.38)  Time: 0.776s, 1320.41/s  (0.799s, 1281.95/s)  LR: 3.156e-04  Data: 0.010 (0.018)
Train: 375 [ 250/1251 ( 20%)]  Loss: 3.259 (3.36)  Time: 0.788s, 1299.86/s  (0.794s, 1288.89/s)  LR: 3.156e-04  Data: 0.010 (0.016)
Train: 375 [ 300/1251 ( 24%)]  Loss: 3.519 (3.39)  Time: 0.774s, 1323.70/s  (0.792s, 1293.00/s)  LR: 3.156e-04  Data: 0.011 (0.015)
Train: 375 [ 350/1251 ( 28%)]  Loss: 3.286 (3.37)  Time: 0.814s, 1257.80/s  (0.795s, 1287.26/s)  LR: 3.156e-04  Data: 0.011 (0.014)
Train: 375 [ 400/1251 ( 32%)]  Loss: 3.112 (3.34)  Time: 0.816s, 1255.38/s  (0.799s, 1281.90/s)  LR: 3.156e-04  Data: 0.012 (0.014)
Train: 375 [ 450/1251 ( 36%)]  Loss: 3.621 (3.37)  Time: 0.778s, 1315.39/s  (0.800s, 1279.28/s)  LR: 3.156e-04  Data: 0.015 (0.014)
Train: 375 [ 500/1251 ( 40%)]  Loss: 3.430 (3.38)  Time: 0.773s, 1324.52/s  (0.799s, 1282.33/s)  LR: 3.156e-04  Data: 0.010 (0.013)
Train: 375 [ 550/1251 ( 44%)]  Loss: 3.026 (3.35)  Time: 0.827s, 1238.46/s  (0.797s, 1284.49/s)  LR: 3.156e-04  Data: 0.009 (0.013)
Train: 375 [ 600/1251 ( 48%)]  Loss: 3.576 (3.37)  Time: 0.773s, 1324.53/s  (0.796s, 1286.88/s)  LR: 3.156e-04  Data: 0.010 (0.013)
Train: 375 [ 650/1251 ( 52%)]  Loss: 3.577 (3.38)  Time: 0.781s, 1311.86/s  (0.795s, 1288.69/s)  LR: 3.156e-04  Data: 0.010 (0.013)
Train: 375 [ 700/1251 ( 56%)]  Loss: 3.774 (3.41)  Time: 0.773s, 1324.63/s  (0.794s, 1289.41/s)  LR: 3.156e-04  Data: 0.010 (0.012)
Train: 375 [ 750/1251 ( 60%)]  Loss: 3.334 (3.40)  Time: 0.817s, 1253.53/s  (0.795s, 1287.68/s)  LR: 3.156e-04  Data: 0.009 (0.012)
Train: 375 [ 800/1251 ( 64%)]  Loss: 3.537 (3.41)  Time: 0.776s, 1318.88/s  (0.795s, 1288.06/s)  LR: 3.156e-04  Data: 0.010 (0.012)
Train: 375 [ 850/1251 ( 68%)]  Loss: 3.012 (3.39)  Time: 0.787s, 1300.97/s  (0.794s, 1289.36/s)  LR: 3.156e-04  Data: 0.009 (0.012)
Train: 375 [ 900/1251 ( 72%)]  Loss: 3.192 (3.38)  Time: 0.777s, 1318.70/s  (0.794s, 1290.45/s)  LR: 3.156e-04  Data: 0.009 (0.012)
Train: 375 [ 950/1251 ( 76%)]  Loss: 3.629 (3.39)  Time: 0.785s, 1304.65/s  (0.793s, 1291.08/s)  LR: 3.156e-04  Data: 0.012 (0.012)
Train: 375 [1000/1251 ( 80%)]  Loss: 3.111 (3.38)  Time: 0.772s, 1325.65/s  (0.793s, 1291.22/s)  LR: 3.156e-04  Data: 0.009 (0.012)
Train: 375 [1050/1251 ( 84%)]  Loss: 3.270 (3.37)  Time: 0.772s, 1327.18/s  (0.793s, 1291.93/s)  LR: 3.156e-04  Data: 0.010 (0.012)
Train: 375 [1100/1251 ( 88%)]  Loss: 3.540 (3.38)  Time: 0.772s, 1327.10/s  (0.793s, 1291.65/s)  LR: 3.156e-04  Data: 0.009 (0.012)
Train: 375 [1150/1251 ( 92%)]  Loss: 3.405 (3.38)  Time: 0.780s, 1313.50/s  (0.792s, 1292.29/s)  LR: 3.156e-04  Data: 0.010 (0.011)
Train: 375 [1200/1251 ( 96%)]  Loss: 3.305 (3.38)  Time: 0.772s, 1326.05/s  (0.792s, 1293.15/s)  LR: 3.156e-04  Data: 0.010 (0.011)
Train: 375 [1250/1251 (100%)]  Loss: 3.145 (3.37)  Time: 0.762s, 1344.40/s  (0.791s, 1294.12/s)  LR: 3.156e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.519 (1.519)  Loss:  0.7734 (0.7734)  Acc@1: 89.7461 (89.7461)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.193 (0.562)  Loss:  0.8086 (1.2555)  Acc@1: 86.2028 (77.1180)  Acc@5: 97.1698 (93.8080)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-374.pth.tar', 77.24799995361329)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-367.pth.tar', 77.19600000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-370.pth.tar', 77.16799998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-363.pth.tar', 77.16599998291015)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-368.pth.tar', 77.15199998046874)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-375.pth.tar', 77.11800008300781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-369.pth.tar', 77.08599994873047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-372.pth.tar', 77.07000013427735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-355.pth.tar', 77.04600006103516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-373.pth.tar', 77.01999995605469)

Train: 376 [   0/1251 (  0%)]  Loss: 3.020 (3.02)  Time: 2.515s,  407.20/s  (2.515s,  407.20/s)  LR: 3.132e-04  Data: 1.767 (1.767)
Train: 376 [  50/1251 (  4%)]  Loss: 3.305 (3.16)  Time: 0.774s, 1323.76/s  (0.829s, 1234.61/s)  LR: 3.132e-04  Data: 0.010 (0.044)
Train: 376 [ 100/1251 (  8%)]  Loss: 3.183 (3.17)  Time: 0.778s, 1317.01/s  (0.805s, 1271.77/s)  LR: 3.132e-04  Data: 0.010 (0.027)
Train: 376 [ 150/1251 ( 12%)]  Loss: 3.387 (3.22)  Time: 0.781s, 1311.32/s  (0.797s, 1285.31/s)  LR: 3.132e-04  Data: 0.009 (0.021)
Train: 376 [ 200/1251 ( 16%)]  Loss: 3.487 (3.28)  Time: 0.808s, 1267.50/s  (0.793s, 1290.72/s)  LR: 3.132e-04  Data: 0.010 (0.019)
Train: 376 [ 250/1251 ( 20%)]  Loss: 3.476 (3.31)  Time: 0.773s, 1324.74/s  (0.796s, 1286.68/s)  LR: 3.132e-04  Data: 0.009 (0.017)
Train: 376 [ 300/1251 ( 24%)]  Loss: 3.345 (3.31)  Time: 0.782s, 1309.71/s  (0.794s, 1290.23/s)  LR: 3.132e-04  Data: 0.010 (0.016)
Train: 376 [ 350/1251 ( 28%)]  Loss: 3.086 (3.29)  Time: 0.776s, 1319.96/s  (0.792s, 1293.46/s)  LR: 3.132e-04  Data: 0.010 (0.015)
Train: 376 [ 400/1251 ( 32%)]  Loss: 3.672 (3.33)  Time: 0.774s, 1322.37/s  (0.791s, 1295.04/s)  LR: 3.132e-04  Data: 0.011 (0.014)
Train: 376 [ 450/1251 ( 36%)]  Loss: 3.257 (3.32)  Time: 0.782s, 1308.81/s  (0.790s, 1296.90/s)  LR: 3.132e-04  Data: 0.010 (0.014)
Train: 376 [ 500/1251 ( 40%)]  Loss: 3.363 (3.33)  Time: 0.774s, 1322.87/s  (0.789s, 1298.27/s)  LR: 3.132e-04  Data: 0.010 (0.014)
Train: 376 [ 550/1251 ( 44%)]  Loss: 3.344 (3.33)  Time: 0.774s, 1323.32/s  (0.788s, 1300.15/s)  LR: 3.132e-04  Data: 0.010 (0.013)
Train: 376 [ 600/1251 ( 48%)]  Loss: 3.147 (3.31)  Time: 0.773s, 1324.43/s  (0.787s, 1301.36/s)  LR: 3.132e-04  Data: 0.010 (0.013)
Train: 376 [ 650/1251 ( 52%)]  Loss: 3.113 (3.30)  Time: 0.778s, 1316.82/s  (0.786s, 1302.09/s)  LR: 3.132e-04  Data: 0.010 (0.013)
Train: 376 [ 700/1251 ( 56%)]  Loss: 3.198 (3.29)  Time: 0.776s, 1320.41/s  (0.786s, 1302.30/s)  LR: 3.132e-04  Data: 0.010 (0.013)
Train: 376 [ 750/1251 ( 60%)]  Loss: 3.315 (3.29)  Time: 0.771s, 1327.40/s  (0.786s, 1302.57/s)  LR: 3.132e-04  Data: 0.010 (0.012)
Train: 376 [ 800/1251 ( 64%)]  Loss: 3.159 (3.29)  Time: 0.835s, 1227.00/s  (0.786s, 1302.47/s)  LR: 3.132e-04  Data: 0.010 (0.012)
Train: 376 [ 850/1251 ( 68%)]  Loss: 3.269 (3.28)  Time: 0.780s, 1312.62/s  (0.786s, 1303.24/s)  LR: 3.132e-04  Data: 0.010 (0.012)
Train: 376 [ 900/1251 ( 72%)]  Loss: 3.768 (3.31)  Time: 0.773s, 1324.44/s  (0.786s, 1303.07/s)  LR: 3.132e-04  Data: 0.010 (0.012)
Train: 376 [ 950/1251 ( 76%)]  Loss: 3.095 (3.30)  Time: 0.820s, 1249.49/s  (0.787s, 1300.97/s)  LR: 3.132e-04  Data: 0.010 (0.012)
Train: 376 [1000/1251 ( 80%)]  Loss: 3.562 (3.31)  Time: 0.775s, 1320.83/s  (0.788s, 1299.71/s)  LR: 3.132e-04  Data: 0.009 (0.012)
Train: 376 [1050/1251 ( 84%)]  Loss: 3.213 (3.31)  Time: 0.775s, 1321.77/s  (0.788s, 1299.74/s)  LR: 3.132e-04  Data: 0.011 (0.012)
Train: 376 [1100/1251 ( 88%)]  Loss: 3.425 (3.31)  Time: 0.774s, 1323.70/s  (0.788s, 1300.13/s)  LR: 3.132e-04  Data: 0.010 (0.012)
Train: 376 [1150/1251 ( 92%)]  Loss: 3.476 (3.32)  Time: 0.785s, 1305.17/s  (0.787s, 1300.71/s)  LR: 3.132e-04  Data: 0.013 (0.012)
Train: 376 [1200/1251 ( 96%)]  Loss: 3.435 (3.32)  Time: 0.777s, 1317.42/s  (0.787s, 1301.15/s)  LR: 3.132e-04  Data: 0.010 (0.011)
Train: 376 [1250/1251 (100%)]  Loss: 3.607 (3.33)  Time: 0.807s, 1268.49/s  (0.787s, 1300.86/s)  LR: 3.132e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.530 (1.530)  Loss:  0.6602 (0.6602)  Acc@1: 90.6250 (90.6250)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.194 (0.567)  Loss:  0.6821 (1.1307)  Acc@1: 87.6179 (77.4520)  Acc@5: 97.6415 (93.8880)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-376.pth.tar', 77.451999921875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-374.pth.tar', 77.24799995361329)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-367.pth.tar', 77.19600000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-370.pth.tar', 77.16799998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-363.pth.tar', 77.16599998291015)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-368.pth.tar', 77.15199998046874)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-375.pth.tar', 77.11800008300781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-369.pth.tar', 77.08599994873047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-372.pth.tar', 77.07000013427735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-355.pth.tar', 77.04600006103516)

Train: 377 [   0/1251 (  0%)]  Loss: 3.087 (3.09)  Time: 2.303s,  444.62/s  (2.303s,  444.62/s)  LR: 3.108e-04  Data: 1.577 (1.577)
Train: 377 [  50/1251 (  4%)]  Loss: 3.163 (3.13)  Time: 0.778s, 1316.22/s  (0.822s, 1245.16/s)  LR: 3.108e-04  Data: 0.012 (0.050)
Train: 377 [ 100/1251 (  8%)]  Loss: 3.177 (3.14)  Time: 0.776s, 1320.09/s  (0.800s, 1279.51/s)  LR: 3.108e-04  Data: 0.010 (0.030)
Train: 377 [ 150/1251 ( 12%)]  Loss: 3.195 (3.16)  Time: 0.773s, 1324.68/s  (0.799s, 1281.11/s)  LR: 3.108e-04  Data: 0.010 (0.024)
Train: 377 [ 200/1251 ( 16%)]  Loss: 3.392 (3.20)  Time: 0.772s, 1325.63/s  (0.796s, 1286.03/s)  LR: 3.108e-04  Data: 0.009 (0.020)
Train: 377 [ 250/1251 ( 20%)]  Loss: 3.157 (3.20)  Time: 0.836s, 1224.76/s  (0.794s, 1289.69/s)  LR: 3.108e-04  Data: 0.010 (0.018)
Train: 377 [ 300/1251 ( 24%)]  Loss: 3.180 (3.19)  Time: 0.849s, 1205.99/s  (0.792s, 1293.30/s)  LR: 3.108e-04  Data: 0.011 (0.017)
Train: 377 [ 350/1251 ( 28%)]  Loss: 3.552 (3.24)  Time: 0.799s, 1282.37/s  (0.790s, 1295.99/s)  LR: 3.108e-04  Data: 0.009 (0.016)
Train: 377 [ 400/1251 ( 32%)]  Loss: 3.496 (3.27)  Time: 0.787s, 1301.65/s  (0.789s, 1298.15/s)  LR: 3.108e-04  Data: 0.010 (0.015)
Train: 377 [ 450/1251 ( 36%)]  Loss: 3.234 (3.26)  Time: 0.776s, 1319.01/s  (0.788s, 1299.41/s)  LR: 3.108e-04  Data: 0.010 (0.014)
Train: 377 [ 500/1251 ( 40%)]  Loss: 2.988 (3.24)  Time: 0.770s, 1329.91/s  (0.787s, 1301.57/s)  LR: 3.108e-04  Data: 0.009 (0.014)
Train: 377 [ 550/1251 ( 44%)]  Loss: 3.688 (3.28)  Time: 0.807s, 1269.26/s  (0.786s, 1302.23/s)  LR: 3.108e-04  Data: 0.009 (0.014)
Train: 377 [ 600/1251 ( 48%)]  Loss: 3.154 (3.27)  Time: 0.772s, 1326.15/s  (0.786s, 1303.36/s)  LR: 3.108e-04  Data: 0.009 (0.013)
Train: 377 [ 650/1251 ( 52%)]  Loss: 3.624 (3.29)  Time: 0.773s, 1324.28/s  (0.786s, 1302.93/s)  LR: 3.108e-04  Data: 0.009 (0.013)
Train: 377 [ 700/1251 ( 56%)]  Loss: 3.229 (3.29)  Time: 0.776s, 1320.42/s  (0.785s, 1303.65/s)  LR: 3.108e-04  Data: 0.010 (0.013)
Train: 377 [ 750/1251 ( 60%)]  Loss: 2.947 (3.27)  Time: 0.773s, 1325.51/s  (0.785s, 1304.39/s)  LR: 3.108e-04  Data: 0.010 (0.013)
Train: 377 [ 800/1251 ( 64%)]  Loss: 3.345 (3.27)  Time: 0.777s, 1317.27/s  (0.785s, 1304.79/s)  LR: 3.108e-04  Data: 0.009 (0.012)
Train: 377 [ 850/1251 ( 68%)]  Loss: 3.105 (3.26)  Time: 0.784s, 1306.18/s  (0.785s, 1303.83/s)  LR: 3.108e-04  Data: 0.010 (0.012)
Train: 377 [ 900/1251 ( 72%)]  Loss: 3.462 (3.27)  Time: 0.771s, 1327.66/s  (0.785s, 1304.37/s)  LR: 3.108e-04  Data: 0.009 (0.012)
Train: 377 [ 950/1251 ( 76%)]  Loss: 3.274 (3.27)  Time: 0.772s, 1325.81/s  (0.785s, 1304.67/s)  LR: 3.108e-04  Data: 0.010 (0.012)
Train: 377 [1000/1251 ( 80%)]  Loss: 3.200 (3.27)  Time: 0.774s, 1323.85/s  (0.785s, 1304.21/s)  LR: 3.108e-04  Data: 0.009 (0.012)
Train: 377 [1050/1251 ( 84%)]  Loss: 3.337 (3.27)  Time: 0.782s, 1309.47/s  (0.786s, 1303.50/s)  LR: 3.108e-04  Data: 0.009 (0.012)
Train: 377 [1100/1251 ( 88%)]  Loss: 3.266 (3.27)  Time: 0.787s, 1300.40/s  (0.785s, 1304.00/s)  LR: 3.108e-04  Data: 0.010 (0.012)
Train: 377 [1150/1251 ( 92%)]  Loss: 3.331 (3.27)  Time: 0.773s, 1324.60/s  (0.785s, 1304.23/s)  LR: 3.108e-04  Data: 0.009 (0.012)
Train: 377 [1200/1251 ( 96%)]  Loss: 3.282 (3.27)  Time: 0.790s, 1295.84/s  (0.785s, 1304.81/s)  LR: 3.108e-04  Data: 0.010 (0.012)
Train: 377 [1250/1251 (100%)]  Loss: 3.226 (3.27)  Time: 0.761s, 1345.09/s  (0.785s, 1304.91/s)  LR: 3.108e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.534 (1.534)  Loss:  0.6040 (0.6040)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.194 (0.569)  Loss:  0.7671 (1.1286)  Acc@1: 85.6132 (77.4840)  Acc@5: 97.4057 (93.8580)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-377.pth.tar', 77.48399995605469)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-376.pth.tar', 77.451999921875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-374.pth.tar', 77.24799995361329)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-367.pth.tar', 77.19600000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-370.pth.tar', 77.16799998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-363.pth.tar', 77.16599998291015)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-368.pth.tar', 77.15199998046874)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-375.pth.tar', 77.11800008300781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-369.pth.tar', 77.08599994873047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-372.pth.tar', 77.07000013427735)

Train: 378 [   0/1251 (  0%)]  Loss: 3.222 (3.22)  Time: 2.220s,  461.18/s  (2.220s,  461.18/s)  LR: 3.084e-04  Data: 1.489 (1.489)
Train: 378 [  50/1251 (  4%)]  Loss: 3.241 (3.23)  Time: 0.801s, 1279.02/s  (0.818s, 1251.64/s)  LR: 3.084e-04  Data: 0.009 (0.046)
Train: 378 [ 100/1251 (  8%)]  Loss: 3.218 (3.23)  Time: 0.792s, 1293.54/s  (0.799s, 1281.99/s)  LR: 3.084e-04  Data: 0.010 (0.028)
Train: 378 [ 150/1251 ( 12%)]  Loss: 3.115 (3.20)  Time: 0.777s, 1317.96/s  (0.793s, 1291.51/s)  LR: 3.084e-04  Data: 0.009 (0.022)
Train: 378 [ 200/1251 ( 16%)]  Loss: 3.170 (3.19)  Time: 0.773s, 1324.60/s  (0.790s, 1295.64/s)  LR: 3.084e-04  Data: 0.010 (0.019)
Train: 378 [ 250/1251 ( 20%)]  Loss: 3.738 (3.28)  Time: 0.772s, 1325.62/s  (0.788s, 1299.67/s)  LR: 3.084e-04  Data: 0.010 (0.017)
Train: 378 [ 300/1251 ( 24%)]  Loss: 3.350 (3.29)  Time: 0.775s, 1321.35/s  (0.787s, 1300.74/s)  LR: 3.084e-04  Data: 0.009 (0.016)
Train: 378 [ 350/1251 ( 28%)]  Loss: 2.895 (3.24)  Time: 0.785s, 1304.29/s  (0.786s, 1302.26/s)  LR: 3.084e-04  Data: 0.010 (0.015)
Train: 378 [ 400/1251 ( 32%)]  Loss: 3.514 (3.27)  Time: 0.775s, 1320.68/s  (0.785s, 1304.10/s)  LR: 3.084e-04  Data: 0.013 (0.014)
Train: 378 [ 450/1251 ( 36%)]  Loss: 3.559 (3.30)  Time: 0.773s, 1324.16/s  (0.785s, 1304.59/s)  LR: 3.084e-04  Data: 0.009 (0.014)
Train: 378 [ 500/1251 ( 40%)]  Loss: 3.059 (3.28)  Time: 0.778s, 1316.42/s  (0.785s, 1305.17/s)  LR: 3.084e-04  Data: 0.013 (0.013)
Train: 378 [ 550/1251 ( 44%)]  Loss: 3.301 (3.28)  Time: 0.835s, 1225.81/s  (0.784s, 1305.99/s)  LR: 3.084e-04  Data: 0.009 (0.013)
Train: 378 [ 600/1251 ( 48%)]  Loss: 3.346 (3.29)  Time: 0.775s, 1321.83/s  (0.784s, 1305.85/s)  LR: 3.084e-04  Data: 0.010 (0.013)
Train: 378 [ 650/1251 ( 52%)]  Loss: 3.005 (3.27)  Time: 0.775s, 1321.09/s  (0.784s, 1305.74/s)  LR: 3.084e-04  Data: 0.010 (0.013)
Train: 378 [ 700/1251 ( 56%)]  Loss: 3.149 (3.26)  Time: 0.775s, 1320.92/s  (0.784s, 1306.28/s)  LR: 3.084e-04  Data: 0.011 (0.012)
Train: 378 [ 750/1251 ( 60%)]  Loss: 2.930 (3.24)  Time: 0.777s, 1317.09/s  (0.784s, 1306.29/s)  LR: 3.084e-04  Data: 0.009 (0.012)
Train: 378 [ 800/1251 ( 64%)]  Loss: 3.109 (3.23)  Time: 0.775s, 1320.64/s  (0.784s, 1305.77/s)  LR: 3.084e-04  Data: 0.010 (0.012)
Train: 378 [ 850/1251 ( 68%)]  Loss: 3.438 (3.24)  Time: 0.771s, 1327.86/s  (0.784s, 1305.40/s)  LR: 3.084e-04  Data: 0.009 (0.012)
Train: 378 [ 900/1251 ( 72%)]  Loss: 2.806 (3.22)  Time: 0.776s, 1320.12/s  (0.784s, 1306.03/s)  LR: 3.084e-04  Data: 0.009 (0.012)
Train: 378 [ 950/1251 ( 76%)]  Loss: 3.090 (3.21)  Time: 0.774s, 1323.56/s  (0.784s, 1306.55/s)  LR: 3.084e-04  Data: 0.010 (0.012)
Train: 378 [1000/1251 ( 80%)]  Loss: 3.570 (3.23)  Time: 0.781s, 1311.38/s  (0.784s, 1306.47/s)  LR: 3.084e-04  Data: 0.009 (0.012)
Train: 378 [1050/1251 ( 84%)]  Loss: 3.181 (3.23)  Time: 0.792s, 1292.50/s  (0.784s, 1306.73/s)  LR: 3.084e-04  Data: 0.009 (0.012)
Train: 378 [1100/1251 ( 88%)]  Loss: 3.369 (3.23)  Time: 0.781s, 1311.48/s  (0.784s, 1306.75/s)  LR: 3.084e-04  Data: 0.014 (0.011)
Train: 378 [1150/1251 ( 92%)]  Loss: 3.487 (3.24)  Time: 0.778s, 1315.72/s  (0.784s, 1306.25/s)  LR: 3.084e-04  Data: 0.012 (0.011)
Train: 378 [1200/1251 ( 96%)]  Loss: 3.164 (3.24)  Time: 0.829s, 1234.98/s  (0.784s, 1306.61/s)  LR: 3.084e-04  Data: 0.010 (0.011)
Train: 378 [1250/1251 (100%)]  Loss: 3.157 (3.24)  Time: 0.768s, 1333.94/s  (0.784s, 1306.90/s)  LR: 3.084e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.537 (1.537)  Loss:  0.7520 (0.7520)  Acc@1: 90.8203 (90.8203)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.194 (0.563)  Loss:  0.8545 (1.2231)  Acc@1: 86.6745 (77.1960)  Acc@5: 96.4623 (93.8340)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-377.pth.tar', 77.48399995605469)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-376.pth.tar', 77.451999921875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-374.pth.tar', 77.24799995361329)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-367.pth.tar', 77.19600000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-378.pth.tar', 77.19599989990235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-370.pth.tar', 77.16799998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-363.pth.tar', 77.16599998291015)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-368.pth.tar', 77.15199998046874)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-375.pth.tar', 77.11800008300781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-369.pth.tar', 77.08599994873047)

Train: 379 [   0/1251 (  0%)]  Loss: 3.256 (3.26)  Time: 2.305s,  444.20/s  (2.305s,  444.20/s)  LR: 3.060e-04  Data: 1.574 (1.574)
Train: 379 [  50/1251 (  4%)]  Loss: 2.889 (3.07)  Time: 0.772s, 1326.85/s  (0.824s, 1242.08/s)  LR: 3.060e-04  Data: 0.009 (0.047)
Train: 379 [ 100/1251 (  8%)]  Loss: 3.109 (3.08)  Time: 0.773s, 1325.08/s  (0.803s, 1275.66/s)  LR: 3.060e-04  Data: 0.010 (0.028)
Train: 379 [ 150/1251 ( 12%)]  Loss: 3.242 (3.12)  Time: 0.880s, 1163.60/s  (0.802s, 1276.15/s)  LR: 3.060e-04  Data: 0.011 (0.022)
Train: 379 [ 200/1251 ( 16%)]  Loss: 3.257 (3.15)  Time: 0.807s, 1268.94/s  (0.800s, 1279.34/s)  LR: 3.060e-04  Data: 0.010 (0.019)
Train: 379 [ 250/1251 ( 20%)]  Loss: 3.306 (3.18)  Time: 0.774s, 1322.21/s  (0.798s, 1283.22/s)  LR: 3.060e-04  Data: 0.010 (0.018)
Train: 379 [ 300/1251 ( 24%)]  Loss: 3.581 (3.23)  Time: 0.808s, 1267.53/s  (0.797s, 1284.75/s)  LR: 3.060e-04  Data: 0.009 (0.016)
Train: 379 [ 350/1251 ( 28%)]  Loss: 3.201 (3.23)  Time: 0.773s, 1324.19/s  (0.794s, 1289.07/s)  LR: 3.060e-04  Data: 0.010 (0.015)
Train: 379 [ 400/1251 ( 32%)]  Loss: 3.035 (3.21)  Time: 0.773s, 1324.95/s  (0.793s, 1291.83/s)  LR: 3.060e-04  Data: 0.010 (0.015)
Train: 379 [ 450/1251 ( 36%)]  Loss: 2.891 (3.18)  Time: 0.771s, 1328.11/s  (0.792s, 1293.20/s)  LR: 3.060e-04  Data: 0.009 (0.014)
Train: 379 [ 500/1251 ( 40%)]  Loss: 2.882 (3.15)  Time: 0.773s, 1325.32/s  (0.792s, 1293.67/s)  LR: 3.060e-04  Data: 0.010 (0.014)
Train: 379 [ 550/1251 ( 44%)]  Loss: 3.494 (3.18)  Time: 0.772s, 1326.58/s  (0.791s, 1295.36/s)  LR: 3.060e-04  Data: 0.010 (0.013)
Train: 379 [ 600/1251 ( 48%)]  Loss: 3.405 (3.20)  Time: 0.771s, 1327.85/s  (0.790s, 1296.81/s)  LR: 3.060e-04  Data: 0.010 (0.013)
Train: 379 [ 650/1251 ( 52%)]  Loss: 3.146 (3.19)  Time: 0.813s, 1258.90/s  (0.789s, 1297.87/s)  LR: 3.060e-04  Data: 0.009 (0.013)
Train: 379 [ 700/1251 ( 56%)]  Loss: 3.269 (3.20)  Time: 0.772s, 1327.10/s  (0.789s, 1298.03/s)  LR: 3.060e-04  Data: 0.009 (0.013)
Train: 379 [ 750/1251 ( 60%)]  Loss: 3.150 (3.19)  Time: 0.850s, 1204.68/s  (0.788s, 1298.78/s)  LR: 3.060e-04  Data: 0.011 (0.012)
Train: 379 [ 800/1251 ( 64%)]  Loss: 3.272 (3.20)  Time: 0.784s, 1306.65/s  (0.788s, 1299.45/s)  LR: 3.060e-04  Data: 0.010 (0.012)
Train: 379 [ 850/1251 ( 68%)]  Loss: 3.161 (3.20)  Time: 0.831s, 1231.69/s  (0.788s, 1299.36/s)  LR: 3.060e-04  Data: 0.010 (0.012)
Train: 379 [ 900/1251 ( 72%)]  Loss: 3.362 (3.21)  Time: 0.774s, 1323.57/s  (0.789s, 1297.47/s)  LR: 3.060e-04  Data: 0.010 (0.012)
Train: 379 [ 950/1251 ( 76%)]  Loss: 3.161 (3.20)  Time: 0.805s, 1271.30/s  (0.790s, 1295.38/s)  LR: 3.060e-04  Data: 0.009 (0.012)
Train: 379 [1000/1251 ( 80%)]  Loss: 3.462 (3.22)  Time: 0.781s, 1310.32/s  (0.790s, 1296.36/s)  LR: 3.060e-04  Data: 0.010 (0.012)
Train: 379 [1050/1251 ( 84%)]  Loss: 3.070 (3.21)  Time: 0.775s, 1322.09/s  (0.790s, 1297.00/s)  LR: 3.060e-04  Data: 0.009 (0.012)
Train: 379 [1100/1251 ( 88%)]  Loss: 3.327 (3.21)  Time: 0.772s, 1326.55/s  (0.789s, 1297.76/s)  LR: 3.060e-04  Data: 0.010 (0.012)
Train: 379 [1150/1251 ( 92%)]  Loss: 3.463 (3.22)  Time: 0.776s, 1319.58/s  (0.789s, 1298.04/s)  LR: 3.060e-04  Data: 0.009 (0.012)
Train: 379 [1200/1251 ( 96%)]  Loss: 3.240 (3.23)  Time: 0.771s, 1327.68/s  (0.788s, 1298.76/s)  LR: 3.060e-04  Data: 0.009 (0.011)
Train: 379 [1250/1251 (100%)]  Loss: 3.558 (3.24)  Time: 0.773s, 1324.05/s  (0.789s, 1298.54/s)  LR: 3.060e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.536 (1.536)  Loss:  0.6455 (0.6455)  Acc@1: 90.6250 (90.6250)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.194 (0.573)  Loss:  0.7437 (1.1484)  Acc@1: 86.4387 (77.3740)  Acc@5: 97.0519 (93.8960)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-377.pth.tar', 77.48399995605469)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-376.pth.tar', 77.451999921875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-379.pth.tar', 77.37400005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-374.pth.tar', 77.24799995361329)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-367.pth.tar', 77.19600000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-378.pth.tar', 77.19599989990235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-370.pth.tar', 77.16799998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-363.pth.tar', 77.16599998291015)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-368.pth.tar', 77.15199998046874)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-375.pth.tar', 77.11800008300781)

Train: 380 [   0/1251 (  0%)]  Loss: 3.291 (3.29)  Time: 2.231s,  458.93/s  (2.231s,  458.93/s)  LR: 3.037e-04  Data: 1.494 (1.494)
Train: 380 [  50/1251 (  4%)]  Loss: 3.357 (3.32)  Time: 0.774s, 1322.71/s  (0.822s, 1245.20/s)  LR: 3.037e-04  Data: 0.010 (0.045)
Train: 380 [ 100/1251 (  8%)]  Loss: 3.177 (3.27)  Time: 0.772s, 1326.27/s  (0.800s, 1280.05/s)  LR: 3.037e-04  Data: 0.010 (0.028)
Train: 380 [ 150/1251 ( 12%)]  Loss: 3.564 (3.35)  Time: 0.812s, 1260.37/s  (0.798s, 1283.12/s)  LR: 3.037e-04  Data: 0.010 (0.022)
Train: 380 [ 200/1251 ( 16%)]  Loss: 3.138 (3.31)  Time: 0.816s, 1254.72/s  (0.798s, 1283.41/s)  LR: 3.037e-04  Data: 0.011 (0.019)
Train: 380 [ 250/1251 ( 20%)]  Loss: 3.609 (3.36)  Time: 0.773s, 1324.40/s  (0.795s, 1288.51/s)  LR: 3.037e-04  Data: 0.010 (0.018)
Train: 380 [ 300/1251 ( 24%)]  Loss: 3.309 (3.35)  Time: 0.778s, 1316.84/s  (0.793s, 1291.68/s)  LR: 3.037e-04  Data: 0.010 (0.016)
Train: 380 [ 350/1251 ( 28%)]  Loss: 3.188 (3.33)  Time: 0.774s, 1322.19/s  (0.791s, 1293.96/s)  LR: 3.037e-04  Data: 0.009 (0.015)
Train: 380 [ 400/1251 ( 32%)]  Loss: 3.432 (3.34)  Time: 0.779s, 1314.04/s  (0.790s, 1296.45/s)  LR: 3.037e-04  Data: 0.009 (0.015)
Train: 380 [ 450/1251 ( 36%)]  Loss: 3.409 (3.35)  Time: 0.771s, 1327.85/s  (0.789s, 1298.05/s)  LR: 3.037e-04  Data: 0.009 (0.014)
Train: 380 [ 500/1251 ( 40%)]  Loss: 3.394 (3.35)  Time: 0.824s, 1242.26/s  (0.790s, 1296.93/s)  LR: 3.037e-04  Data: 0.009 (0.014)
Train: 380 [ 550/1251 ( 44%)]  Loss: 3.645 (3.38)  Time: 0.815s, 1256.28/s  (0.791s, 1294.63/s)  LR: 3.037e-04  Data: 0.011 (0.013)
Train: 380 [ 600/1251 ( 48%)]  Loss: 3.344 (3.37)  Time: 0.808s, 1267.02/s  (0.793s, 1291.86/s)  LR: 3.037e-04  Data: 0.010 (0.013)
Train: 380 [ 650/1251 ( 52%)]  Loss: 3.324 (3.37)  Time: 0.787s, 1300.87/s  (0.793s, 1291.01/s)  LR: 3.037e-04  Data: 0.010 (0.013)
Train: 380 [ 700/1251 ( 56%)]  Loss: 2.870 (3.34)  Time: 0.773s, 1325.26/s  (0.792s, 1292.17/s)  LR: 3.037e-04  Data: 0.009 (0.013)
Train: 380 [ 750/1251 ( 60%)]  Loss: 3.754 (3.36)  Time: 0.788s, 1298.88/s  (0.792s, 1293.66/s)  LR: 3.037e-04  Data: 0.009 (0.012)
Train: 380 [ 800/1251 ( 64%)]  Loss: 3.619 (3.38)  Time: 0.782s, 1310.07/s  (0.791s, 1295.08/s)  LR: 3.037e-04  Data: 0.010 (0.012)
Train: 380 [ 850/1251 ( 68%)]  Loss: 3.464 (3.38)  Time: 0.775s, 1321.51/s  (0.790s, 1296.24/s)  LR: 3.037e-04  Data: 0.010 (0.012)
Train: 380 [ 900/1251 ( 72%)]  Loss: 3.241 (3.38)  Time: 0.775s, 1321.70/s  (0.789s, 1297.50/s)  LR: 3.037e-04  Data: 0.010 (0.012)
Train: 380 [ 950/1251 ( 76%)]  Loss: 3.442 (3.38)  Time: 0.819s, 1250.35/s  (0.789s, 1298.42/s)  LR: 3.037e-04  Data: 0.010 (0.012)
Train: 380 [1000/1251 ( 80%)]  Loss: 3.625 (3.39)  Time: 0.774s, 1322.53/s  (0.788s, 1299.42/s)  LR: 3.037e-04  Data: 0.010 (0.012)
Train: 380 [1050/1251 ( 84%)]  Loss: 3.500 (3.40)  Time: 0.773s, 1324.67/s  (0.788s, 1299.89/s)  LR: 3.037e-04  Data: 0.010 (0.012)
Train: 380 [1100/1251 ( 88%)]  Loss: 3.458 (3.40)  Time: 0.794s, 1288.86/s  (0.787s, 1300.72/s)  LR: 3.037e-04  Data: 0.010 (0.012)
Train: 380 [1150/1251 ( 92%)]  Loss: 3.564 (3.40)  Time: 0.790s, 1296.88/s  (0.787s, 1301.27/s)  LR: 3.037e-04  Data: 0.009 (0.011)
Train: 380 [1200/1251 ( 96%)]  Loss: 3.461 (3.41)  Time: 0.775s, 1320.60/s  (0.787s, 1301.88/s)  LR: 3.037e-04  Data: 0.010 (0.011)
Train: 380 [1250/1251 (100%)]  Loss: 3.058 (3.39)  Time: 0.760s, 1346.67/s  (0.787s, 1301.90/s)  LR: 3.037e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.538 (1.538)  Loss:  0.7627 (0.7627)  Acc@1: 90.7227 (90.7227)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.194 (0.564)  Loss:  0.8794 (1.2669)  Acc@1: 86.4387 (77.1920)  Acc@5: 96.5802 (93.8280)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-377.pth.tar', 77.48399995605469)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-376.pth.tar', 77.451999921875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-379.pth.tar', 77.37400005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-374.pth.tar', 77.24799995361329)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-367.pth.tar', 77.19600000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-378.pth.tar', 77.19599989990235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-380.pth.tar', 77.19200005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-370.pth.tar', 77.16799998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-363.pth.tar', 77.16599998291015)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-368.pth.tar', 77.15199998046874)

Train: 381 [   0/1251 (  0%)]  Loss: 3.209 (3.21)  Time: 2.215s,  462.29/s  (2.215s,  462.29/s)  LR: 3.013e-04  Data: 1.486 (1.486)
Train: 381 [  50/1251 (  4%)]  Loss: 3.363 (3.29)  Time: 0.775s, 1320.84/s  (0.825s, 1240.70/s)  LR: 3.013e-04  Data: 0.010 (0.045)
Train: 381 [ 100/1251 (  8%)]  Loss: 3.267 (3.28)  Time: 0.773s, 1324.42/s  (0.802s, 1276.46/s)  LR: 3.013e-04  Data: 0.010 (0.028)
Train: 381 [ 150/1251 ( 12%)]  Loss: 3.384 (3.31)  Time: 0.776s, 1319.98/s  (0.796s, 1287.16/s)  LR: 3.013e-04  Data: 0.010 (0.022)
Train: 381 [ 200/1251 ( 16%)]  Loss: 3.394 (3.32)  Time: 0.817s, 1252.93/s  (0.793s, 1290.83/s)  LR: 3.013e-04  Data: 0.011 (0.019)
Train: 381 [ 250/1251 ( 20%)]  Loss: 3.474 (3.35)  Time: 0.774s, 1323.50/s  (0.795s, 1287.68/s)  LR: 3.013e-04  Data: 0.010 (0.017)
Train: 381 [ 300/1251 ( 24%)]  Loss: 3.076 (3.31)  Time: 0.772s, 1326.84/s  (0.793s, 1291.76/s)  LR: 3.013e-04  Data: 0.010 (0.016)
Train: 381 [ 350/1251 ( 28%)]  Loss: 3.088 (3.28)  Time: 0.775s, 1321.75/s  (0.791s, 1294.55/s)  LR: 3.013e-04  Data: 0.009 (0.015)
Train: 381 [ 400/1251 ( 32%)]  Loss: 3.476 (3.30)  Time: 0.772s, 1326.03/s  (0.791s, 1294.94/s)  LR: 3.013e-04  Data: 0.009 (0.015)
Train: 381 [ 450/1251 ( 36%)]  Loss: 3.569 (3.33)  Time: 0.774s, 1323.66/s  (0.789s, 1297.37/s)  LR: 3.013e-04  Data: 0.010 (0.014)
Train: 381 [ 500/1251 ( 40%)]  Loss: 3.189 (3.32)  Time: 0.772s, 1325.58/s  (0.788s, 1299.05/s)  LR: 3.013e-04  Data: 0.010 (0.014)
Train: 381 [ 550/1251 ( 44%)]  Loss: 3.162 (3.30)  Time: 0.832s, 1230.94/s  (0.788s, 1299.64/s)  LR: 3.013e-04  Data: 0.009 (0.013)
Train: 381 [ 600/1251 ( 48%)]  Loss: 3.322 (3.31)  Time: 0.773s, 1324.57/s  (0.787s, 1300.78/s)  LR: 3.013e-04  Data: 0.011 (0.013)
Train: 381 [ 650/1251 ( 52%)]  Loss: 3.380 (3.31)  Time: 0.774s, 1323.42/s  (0.787s, 1301.70/s)  LR: 3.013e-04  Data: 0.009 (0.013)
Train: 381 [ 700/1251 ( 56%)]  Loss: 3.260 (3.31)  Time: 0.774s, 1322.92/s  (0.786s, 1302.63/s)  LR: 3.013e-04  Data: 0.010 (0.013)
Train: 381 [ 750/1251 ( 60%)]  Loss: 3.070 (3.29)  Time: 0.775s, 1321.11/s  (0.786s, 1302.97/s)  LR: 3.013e-04  Data: 0.010 (0.012)
Train: 381 [ 800/1251 ( 64%)]  Loss: 3.343 (3.30)  Time: 0.776s, 1320.12/s  (0.786s, 1303.12/s)  LR: 3.013e-04  Data: 0.010 (0.012)
Train: 381 [ 850/1251 ( 68%)]  Loss: 3.370 (3.30)  Time: 0.778s, 1316.95/s  (0.786s, 1303.49/s)  LR: 3.013e-04  Data: 0.010 (0.012)
Train: 381 [ 900/1251 ( 72%)]  Loss: 3.539 (3.31)  Time: 0.773s, 1324.82/s  (0.785s, 1303.86/s)  LR: 3.013e-04  Data: 0.010 (0.012)
Train: 381 [ 950/1251 ( 76%)]  Loss: 3.480 (3.32)  Time: 0.772s, 1326.80/s  (0.785s, 1304.29/s)  LR: 3.013e-04  Data: 0.010 (0.012)
Train: 381 [1000/1251 ( 80%)]  Loss: 3.257 (3.32)  Time: 0.776s, 1318.82/s  (0.785s, 1304.73/s)  LR: 3.013e-04  Data: 0.009 (0.012)
Train: 381 [1050/1251 ( 84%)]  Loss: 3.456 (3.32)  Time: 0.778s, 1315.71/s  (0.784s, 1305.30/s)  LR: 3.013e-04  Data: 0.010 (0.012)
Train: 381 [1100/1251 ( 88%)]  Loss: 3.390 (3.33)  Time: 0.784s, 1306.86/s  (0.784s, 1305.70/s)  LR: 3.013e-04  Data: 0.010 (0.012)
Train: 381 [1150/1251 ( 92%)]  Loss: 3.357 (3.33)  Time: 0.780s, 1312.32/s  (0.784s, 1305.64/s)  LR: 3.013e-04  Data: 0.011 (0.011)
Train: 381 [1200/1251 ( 96%)]  Loss: 3.590 (3.34)  Time: 0.871s, 1175.55/s  (0.784s, 1305.67/s)  LR: 3.013e-04  Data: 0.009 (0.011)
Train: 381 [1250/1251 (100%)]  Loss: 3.150 (3.33)  Time: 0.767s, 1334.68/s  (0.784s, 1306.11/s)  LR: 3.013e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.515 (1.515)  Loss:  0.7456 (0.7456)  Acc@1: 91.2109 (91.2109)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.194 (0.560)  Loss:  0.8350 (1.2031)  Acc@1: 86.5566 (77.2380)  Acc@5: 96.9340 (93.9720)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-377.pth.tar', 77.48399995605469)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-376.pth.tar', 77.451999921875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-379.pth.tar', 77.37400005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-374.pth.tar', 77.24799995361329)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-381.pth.tar', 77.23799997802735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-367.pth.tar', 77.19600000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-378.pth.tar', 77.19599989990235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-380.pth.tar', 77.19200005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-370.pth.tar', 77.16799998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-363.pth.tar', 77.16599998291015)

Train: 382 [   0/1251 (  0%)]  Loss: 3.146 (3.15)  Time: 2.133s,  480.09/s  (2.133s,  480.09/s)  LR: 2.989e-04  Data: 1.419 (1.419)
Train: 382 [  50/1251 (  4%)]  Loss: 3.112 (3.13)  Time: 0.812s, 1261.57/s  (0.816s, 1254.18/s)  LR: 2.989e-04  Data: 0.009 (0.042)
Train: 382 [ 100/1251 (  8%)]  Loss: 3.191 (3.15)  Time: 0.789s, 1297.59/s  (0.800s, 1279.61/s)  LR: 2.989e-04  Data: 0.010 (0.026)
Train: 382 [ 150/1251 ( 12%)]  Loss: 3.091 (3.14)  Time: 0.820s, 1249.53/s  (0.800s, 1279.87/s)  LR: 2.989e-04  Data: 0.009 (0.021)
Train: 382 [ 200/1251 ( 16%)]  Loss: 3.161 (3.14)  Time: 0.792s, 1293.55/s  (0.799s, 1282.35/s)  LR: 2.989e-04  Data: 0.010 (0.018)
Train: 382 [ 250/1251 ( 20%)]  Loss: 3.705 (3.23)  Time: 0.772s, 1326.16/s  (0.796s, 1286.76/s)  LR: 2.989e-04  Data: 0.009 (0.016)
Train: 382 [ 300/1251 ( 24%)]  Loss: 3.413 (3.26)  Time: 0.772s, 1326.66/s  (0.793s, 1291.05/s)  LR: 2.989e-04  Data: 0.010 (0.015)
Train: 382 [ 350/1251 ( 28%)]  Loss: 3.420 (3.28)  Time: 0.783s, 1307.80/s  (0.791s, 1293.82/s)  LR: 2.989e-04  Data: 0.010 (0.015)
Train: 382 [ 400/1251 ( 32%)]  Loss: 2.942 (3.24)  Time: 0.773s, 1324.02/s  (0.790s, 1296.38/s)  LR: 2.989e-04  Data: 0.010 (0.014)
Train: 382 [ 450/1251 ( 36%)]  Loss: 3.616 (3.28)  Time: 0.771s, 1327.35/s  (0.789s, 1297.95/s)  LR: 2.989e-04  Data: 0.010 (0.014)
Train: 382 [ 500/1251 ( 40%)]  Loss: 3.488 (3.30)  Time: 0.776s, 1319.59/s  (0.788s, 1300.04/s)  LR: 2.989e-04  Data: 0.009 (0.013)
Train: 382 [ 550/1251 ( 44%)]  Loss: 2.993 (3.27)  Time: 0.773s, 1324.12/s  (0.786s, 1302.00/s)  LR: 2.989e-04  Data: 0.010 (0.013)
Train: 382 [ 600/1251 ( 48%)]  Loss: 3.385 (3.28)  Time: 0.845s, 1211.16/s  (0.786s, 1302.63/s)  LR: 2.989e-04  Data: 0.009 (0.013)
Train: 382 [ 650/1251 ( 52%)]  Loss: 3.277 (3.28)  Time: 0.817s, 1253.04/s  (0.787s, 1300.74/s)  LR: 2.989e-04  Data: 0.012 (0.012)
Train: 382 [ 700/1251 ( 56%)]  Loss: 2.950 (3.26)  Time: 0.782s, 1309.06/s  (0.789s, 1298.07/s)  LR: 2.989e-04  Data: 0.011 (0.012)
Train: 382 [ 750/1251 ( 60%)]  Loss: 3.337 (3.26)  Time: 0.774s, 1322.41/s  (0.788s, 1299.24/s)  LR: 2.989e-04  Data: 0.011 (0.012)
Train: 382 [ 800/1251 ( 64%)]  Loss: 3.538 (3.28)  Time: 0.772s, 1325.95/s  (0.788s, 1299.94/s)  LR: 2.989e-04  Data: 0.010 (0.012)
Train: 382 [ 850/1251 ( 68%)]  Loss: 3.345 (3.28)  Time: 0.773s, 1325.47/s  (0.787s, 1301.12/s)  LR: 2.989e-04  Data: 0.010 (0.012)
Train: 382 [ 900/1251 ( 72%)]  Loss: 3.345 (3.29)  Time: 0.773s, 1324.02/s  (0.786s, 1302.01/s)  LR: 2.989e-04  Data: 0.011 (0.012)
Train: 382 [ 950/1251 ( 76%)]  Loss: 3.502 (3.30)  Time: 0.779s, 1314.48/s  (0.787s, 1301.64/s)  LR: 2.989e-04  Data: 0.010 (0.012)
Train: 382 [1000/1251 ( 80%)]  Loss: 3.042 (3.29)  Time: 0.782s, 1309.96/s  (0.786s, 1302.35/s)  LR: 2.989e-04  Data: 0.009 (0.012)
Train: 382 [1050/1251 ( 84%)]  Loss: 3.433 (3.29)  Time: 0.771s, 1327.93/s  (0.786s, 1302.61/s)  LR: 2.989e-04  Data: 0.009 (0.011)
Train: 382 [1100/1251 ( 88%)]  Loss: 3.489 (3.30)  Time: 0.771s, 1327.73/s  (0.786s, 1303.33/s)  LR: 2.989e-04  Data: 0.010 (0.011)
Train: 382 [1150/1251 ( 92%)]  Loss: 2.916 (3.28)  Time: 0.781s, 1310.70/s  (0.786s, 1303.48/s)  LR: 2.989e-04  Data: 0.010 (0.011)
Train: 382 [1200/1251 ( 96%)]  Loss: 3.186 (3.28)  Time: 0.774s, 1323.00/s  (0.785s, 1304.11/s)  LR: 2.989e-04  Data: 0.010 (0.011)
Train: 382 [1250/1251 (100%)]  Loss: 3.228 (3.28)  Time: 0.804s, 1273.89/s  (0.785s, 1304.29/s)  LR: 2.989e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.521 (1.521)  Loss:  0.7446 (0.7446)  Acc@1: 89.4531 (89.4531)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.194 (0.572)  Loss:  0.8774 (1.2186)  Acc@1: 86.3207 (77.3260)  Acc@5: 97.1698 (93.9160)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-377.pth.tar', 77.48399995605469)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-376.pth.tar', 77.451999921875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-379.pth.tar', 77.37400005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-382.pth.tar', 77.32599987548828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-374.pth.tar', 77.24799995361329)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-381.pth.tar', 77.23799997802735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-367.pth.tar', 77.19600000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-378.pth.tar', 77.19599989990235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-380.pth.tar', 77.19200005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-370.pth.tar', 77.16799998291016)

Train: 383 [   0/1251 (  0%)]  Loss: 3.376 (3.38)  Time: 2.201s,  465.22/s  (2.201s,  465.22/s)  LR: 2.966e-04  Data: 1.487 (1.487)
Train: 383 [  50/1251 (  4%)]  Loss: 3.330 (3.35)  Time: 0.770s, 1329.92/s  (0.814s, 1258.52/s)  LR: 2.966e-04  Data: 0.010 (0.044)
Train: 383 [ 100/1251 (  8%)]  Loss: 3.123 (3.28)  Time: 0.773s, 1323.86/s  (0.804s, 1273.73/s)  LR: 2.966e-04  Data: 0.010 (0.027)
Train: 383 [ 150/1251 ( 12%)]  Loss: 3.279 (3.28)  Time: 0.775s, 1321.90/s  (0.797s, 1284.80/s)  LR: 2.966e-04  Data: 0.010 (0.022)
Train: 383 [ 200/1251 ( 16%)]  Loss: 3.114 (3.24)  Time: 0.771s, 1328.08/s  (0.793s, 1291.53/s)  LR: 2.966e-04  Data: 0.010 (0.019)
Train: 383 [ 250/1251 ( 20%)]  Loss: 3.173 (3.23)  Time: 0.772s, 1325.85/s  (0.790s, 1295.48/s)  LR: 2.966e-04  Data: 0.010 (0.017)
Train: 383 [ 300/1251 ( 24%)]  Loss: 3.560 (3.28)  Time: 0.782s, 1309.48/s  (0.789s, 1298.60/s)  LR: 2.966e-04  Data: 0.010 (0.016)
Train: 383 [ 350/1251 ( 28%)]  Loss: 3.384 (3.29)  Time: 0.772s, 1325.67/s  (0.787s, 1300.73/s)  LR: 2.966e-04  Data: 0.010 (0.015)
Train: 383 [ 400/1251 ( 32%)]  Loss: 3.182 (3.28)  Time: 0.779s, 1314.46/s  (0.786s, 1302.96/s)  LR: 2.966e-04  Data: 0.012 (0.014)
Train: 383 [ 450/1251 ( 36%)]  Loss: 3.225 (3.27)  Time: 0.809s, 1265.10/s  (0.786s, 1303.16/s)  LR: 2.966e-04  Data: 0.009 (0.014)
Train: 383 [ 500/1251 ( 40%)]  Loss: 3.376 (3.28)  Time: 0.830s, 1234.32/s  (0.785s, 1303.68/s)  LR: 2.966e-04  Data: 0.010 (0.014)
Train: 383 [ 550/1251 ( 44%)]  Loss: 3.303 (3.29)  Time: 0.772s, 1326.68/s  (0.785s, 1304.44/s)  LR: 2.966e-04  Data: 0.009 (0.013)
Train: 383 [ 600/1251 ( 48%)]  Loss: 3.278 (3.28)  Time: 0.781s, 1311.34/s  (0.785s, 1304.15/s)  LR: 2.966e-04  Data: 0.010 (0.013)
Train: 383 [ 650/1251 ( 52%)]  Loss: 3.205 (3.28)  Time: 0.798s, 1283.38/s  (0.785s, 1304.92/s)  LR: 2.966e-04  Data: 0.010 (0.013)
Train: 383 [ 700/1251 ( 56%)]  Loss: 3.400 (3.29)  Time: 0.775s, 1320.63/s  (0.784s, 1305.86/s)  LR: 2.966e-04  Data: 0.013 (0.013)
Train: 383 [ 750/1251 ( 60%)]  Loss: 3.617 (3.31)  Time: 0.777s, 1317.45/s  (0.784s, 1306.72/s)  LR: 2.966e-04  Data: 0.010 (0.012)
Train: 383 [ 800/1251 ( 64%)]  Loss: 3.608 (3.33)  Time: 0.773s, 1325.41/s  (0.783s, 1307.27/s)  LR: 2.966e-04  Data: 0.010 (0.012)
Train: 383 [ 850/1251 ( 68%)]  Loss: 3.183 (3.32)  Time: 0.781s, 1311.11/s  (0.783s, 1307.57/s)  LR: 2.966e-04  Data: 0.010 (0.012)
Train: 383 [ 900/1251 ( 72%)]  Loss: 3.255 (3.31)  Time: 0.772s, 1326.38/s  (0.783s, 1307.94/s)  LR: 2.966e-04  Data: 0.010 (0.012)
Train: 383 [ 950/1251 ( 76%)]  Loss: 3.049 (3.30)  Time: 0.773s, 1324.18/s  (0.784s, 1305.52/s)  LR: 2.966e-04  Data: 0.009 (0.012)
Train: 383 [1000/1251 ( 80%)]  Loss: 3.110 (3.29)  Time: 0.773s, 1325.22/s  (0.784s, 1306.09/s)  LR: 2.966e-04  Data: 0.010 (0.012)
Train: 383 [1050/1251 ( 84%)]  Loss: 3.536 (3.30)  Time: 0.773s, 1325.05/s  (0.784s, 1306.74/s)  LR: 2.966e-04  Data: 0.010 (0.012)
Train: 383 [1100/1251 ( 88%)]  Loss: 3.608 (3.32)  Time: 0.773s, 1324.52/s  (0.783s, 1307.21/s)  LR: 2.966e-04  Data: 0.009 (0.012)
Train: 383 [1150/1251 ( 92%)]  Loss: 3.666 (3.33)  Time: 0.774s, 1323.50/s  (0.783s, 1307.27/s)  LR: 2.966e-04  Data: 0.010 (0.012)
Train: 383 [1200/1251 ( 96%)]  Loss: 3.032 (3.32)  Time: 0.774s, 1323.07/s  (0.783s, 1307.52/s)  LR: 2.966e-04  Data: 0.011 (0.011)
Train: 383 [1250/1251 (100%)]  Loss: 3.484 (3.33)  Time: 0.769s, 1332.14/s  (0.783s, 1307.67/s)  LR: 2.966e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.512 (1.512)  Loss:  0.7178 (0.7178)  Acc@1: 91.6992 (91.6992)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.194 (0.567)  Loss:  0.8374 (1.2704)  Acc@1: 86.2028 (77.2520)  Acc@5: 96.6981 (93.9060)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-377.pth.tar', 77.48399995605469)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-376.pth.tar', 77.451999921875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-379.pth.tar', 77.37400005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-382.pth.tar', 77.32599987548828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-383.pth.tar', 77.25199995361328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-374.pth.tar', 77.24799995361329)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-381.pth.tar', 77.23799997802735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-367.pth.tar', 77.19600000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-378.pth.tar', 77.19599989990235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-380.pth.tar', 77.19200005615234)

Train: 384 [   0/1251 (  0%)]  Loss: 3.535 (3.54)  Time: 2.267s,  451.73/s  (2.267s,  451.73/s)  LR: 2.942e-04  Data: 1.531 (1.531)
Train: 384 [  50/1251 (  4%)]  Loss: 3.246 (3.39)  Time: 0.807s, 1269.08/s  (0.816s, 1255.58/s)  LR: 2.942e-04  Data: 0.009 (0.044)
Train: 384 [ 100/1251 (  8%)]  Loss: 3.153 (3.31)  Time: 0.774s, 1323.79/s  (0.804s, 1273.40/s)  LR: 2.942e-04  Data: 0.010 (0.027)
Train: 384 [ 150/1251 ( 12%)]  Loss: 3.239 (3.29)  Time: 0.780s, 1313.20/s  (0.799s, 1280.96/s)  LR: 2.942e-04  Data: 0.010 (0.021)
Train: 384 [ 200/1251 ( 16%)]  Loss: 3.327 (3.30)  Time: 0.776s, 1318.97/s  (0.794s, 1289.90/s)  LR: 2.942e-04  Data: 0.010 (0.018)
Train: 384 [ 250/1251 ( 20%)]  Loss: 3.307 (3.30)  Time: 0.780s, 1312.42/s  (0.793s, 1290.66/s)  LR: 2.942e-04  Data: 0.010 (0.017)
Train: 384 [ 300/1251 ( 24%)]  Loss: 3.181 (3.28)  Time: 0.775s, 1321.83/s  (0.791s, 1294.49/s)  LR: 2.942e-04  Data: 0.010 (0.016)
Train: 384 [ 350/1251 ( 28%)]  Loss: 3.160 (3.27)  Time: 0.772s, 1326.46/s  (0.790s, 1296.80/s)  LR: 2.942e-04  Data: 0.009 (0.015)
Train: 384 [ 400/1251 ( 32%)]  Loss: 3.309 (3.27)  Time: 0.787s, 1301.13/s  (0.788s, 1298.94/s)  LR: 2.942e-04  Data: 0.010 (0.014)
Train: 384 [ 450/1251 ( 36%)]  Loss: 3.392 (3.28)  Time: 0.782s, 1308.71/s  (0.788s, 1299.39/s)  LR: 2.942e-04  Data: 0.010 (0.014)
Train: 384 [ 500/1251 ( 40%)]  Loss: 3.137 (3.27)  Time: 0.773s, 1324.42/s  (0.787s, 1300.77/s)  LR: 2.942e-04  Data: 0.010 (0.013)
Train: 384 [ 550/1251 ( 44%)]  Loss: 3.299 (3.27)  Time: 0.777s, 1318.55/s  (0.786s, 1302.27/s)  LR: 2.942e-04  Data: 0.010 (0.013)
Train: 384 [ 600/1251 ( 48%)]  Loss: 3.334 (3.28)  Time: 0.774s, 1322.75/s  (0.786s, 1303.48/s)  LR: 2.942e-04  Data: 0.010 (0.013)
Train: 384 [ 650/1251 ( 52%)]  Loss: 3.595 (3.30)  Time: 0.786s, 1302.65/s  (0.785s, 1304.08/s)  LR: 2.942e-04  Data: 0.010 (0.013)
Train: 384 [ 700/1251 ( 56%)]  Loss: 3.275 (3.30)  Time: 0.774s, 1323.31/s  (0.785s, 1304.75/s)  LR: 2.942e-04  Data: 0.010 (0.012)
Train: 384 [ 750/1251 ( 60%)]  Loss: 3.174 (3.29)  Time: 0.775s, 1321.73/s  (0.784s, 1305.42/s)  LR: 2.942e-04  Data: 0.010 (0.012)
Train: 384 [ 800/1251 ( 64%)]  Loss: 3.423 (3.30)  Time: 0.773s, 1324.76/s  (0.784s, 1305.98/s)  LR: 2.942e-04  Data: 0.010 (0.012)
Train: 384 [ 850/1251 ( 68%)]  Loss: 3.579 (3.31)  Time: 0.776s, 1319.69/s  (0.784s, 1306.76/s)  LR: 2.942e-04  Data: 0.010 (0.012)
Train: 384 [ 900/1251 ( 72%)]  Loss: 3.525 (3.33)  Time: 0.773s, 1325.11/s  (0.783s, 1307.15/s)  LR: 2.942e-04  Data: 0.010 (0.012)
Train: 384 [ 950/1251 ( 76%)]  Loss: 3.138 (3.32)  Time: 0.772s, 1325.57/s  (0.783s, 1307.11/s)  LR: 2.942e-04  Data: 0.011 (0.012)
Train: 384 [1000/1251 ( 80%)]  Loss: 3.628 (3.33)  Time: 0.816s, 1255.62/s  (0.784s, 1306.47/s)  LR: 2.942e-04  Data: 0.011 (0.012)
Train: 384 [1050/1251 ( 84%)]  Loss: 3.277 (3.33)  Time: 0.773s, 1324.65/s  (0.784s, 1305.65/s)  LR: 2.942e-04  Data: 0.009 (0.012)
Train: 384 [1100/1251 ( 88%)]  Loss: 3.345 (3.33)  Time: 0.782s, 1309.78/s  (0.784s, 1306.10/s)  LR: 2.942e-04  Data: 0.010 (0.012)
Train: 384 [1150/1251 ( 92%)]  Loss: 3.620 (3.34)  Time: 0.772s, 1326.54/s  (0.784s, 1306.45/s)  LR: 2.942e-04  Data: 0.009 (0.011)
Train: 384 [1200/1251 ( 96%)]  Loss: 3.254 (3.34)  Time: 0.773s, 1324.45/s  (0.784s, 1306.77/s)  LR: 2.942e-04  Data: 0.011 (0.011)
Train: 384 [1250/1251 (100%)]  Loss: 3.343 (3.34)  Time: 0.766s, 1336.69/s  (0.783s, 1307.37/s)  LR: 2.942e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.526 (1.526)  Loss:  0.7441 (0.7441)  Acc@1: 90.2344 (90.2344)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.194 (0.564)  Loss:  0.8301 (1.2234)  Acc@1: 85.9670 (77.2980)  Acc@5: 97.5236 (93.9360)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-377.pth.tar', 77.48399995605469)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-376.pth.tar', 77.451999921875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-379.pth.tar', 77.37400005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-382.pth.tar', 77.32599987548828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-384.pth.tar', 77.29799998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-383.pth.tar', 77.25199995361328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-374.pth.tar', 77.24799995361329)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-381.pth.tar', 77.23799997802735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-367.pth.tar', 77.19600000732422)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-378.pth.tar', 77.19599989990235)

Train: 385 [   0/1251 (  0%)]  Loss: 3.240 (3.24)  Time: 2.316s,  442.05/s  (2.316s,  442.05/s)  LR: 2.919e-04  Data: 1.579 (1.579)
Train: 385 [  50/1251 (  4%)]  Loss: 3.238 (3.24)  Time: 0.793s, 1291.62/s  (0.818s, 1251.41/s)  LR: 2.919e-04  Data: 0.013 (0.046)
Train: 385 [ 100/1251 (  8%)]  Loss: 3.249 (3.24)  Time: 0.774s, 1323.65/s  (0.800s, 1280.62/s)  LR: 2.919e-04  Data: 0.009 (0.028)
Train: 385 [ 150/1251 ( 12%)]  Loss: 2.877 (3.15)  Time: 0.773s, 1324.69/s  (0.792s, 1292.12/s)  LR: 2.919e-04  Data: 0.009 (0.022)
Train: 385 [ 200/1251 ( 16%)]  Loss: 3.538 (3.23)  Time: 0.773s, 1324.40/s  (0.791s, 1294.43/s)  LR: 2.919e-04  Data: 0.010 (0.019)
Train: 385 [ 250/1251 ( 20%)]  Loss: 3.629 (3.30)  Time: 0.780s, 1313.51/s  (0.789s, 1297.83/s)  LR: 2.919e-04  Data: 0.010 (0.017)
Train: 385 [ 300/1251 ( 24%)]  Loss: 2.586 (3.19)  Time: 0.781s, 1310.38/s  (0.788s, 1299.23/s)  LR: 2.919e-04  Data: 0.010 (0.016)
Train: 385 [ 350/1251 ( 28%)]  Loss: 3.413 (3.22)  Time: 0.790s, 1295.54/s  (0.787s, 1301.58/s)  LR: 2.919e-04  Data: 0.010 (0.015)
Train: 385 [ 400/1251 ( 32%)]  Loss: 3.411 (3.24)  Time: 0.776s, 1319.32/s  (0.786s, 1302.02/s)  LR: 2.919e-04  Data: 0.010 (0.015)
Train: 385 [ 450/1251 ( 36%)]  Loss: 2.825 (3.20)  Time: 0.793s, 1292.09/s  (0.786s, 1302.29/s)  LR: 2.919e-04  Data: 0.010 (0.014)
Train: 385 [ 500/1251 ( 40%)]  Loss: 3.350 (3.21)  Time: 0.773s, 1324.37/s  (0.786s, 1302.90/s)  LR: 2.919e-04  Data: 0.011 (0.014)
Train: 385 [ 550/1251 ( 44%)]  Loss: 3.223 (3.22)  Time: 0.776s, 1319.55/s  (0.785s, 1304.15/s)  LR: 2.919e-04  Data: 0.010 (0.013)
Train: 385 [ 600/1251 ( 48%)]  Loss: 3.807 (3.26)  Time: 0.787s, 1300.37/s  (0.785s, 1304.09/s)  LR: 2.919e-04  Data: 0.013 (0.013)
Train: 385 [ 650/1251 ( 52%)]  Loss: 3.358 (3.27)  Time: 0.782s, 1309.38/s  (0.785s, 1304.41/s)  LR: 2.919e-04  Data: 0.013 (0.013)
Train: 385 [ 700/1251 ( 56%)]  Loss: 3.232 (3.27)  Time: 0.774s, 1322.99/s  (0.784s, 1305.50/s)  LR: 2.919e-04  Data: 0.010 (0.013)
Train: 385 [ 750/1251 ( 60%)]  Loss: 3.322 (3.27)  Time: 0.773s, 1323.94/s  (0.784s, 1305.50/s)  LR: 2.919e-04  Data: 0.010 (0.013)
Train: 385 [ 800/1251 ( 64%)]  Loss: 3.237 (3.27)  Time: 0.783s, 1307.47/s  (0.784s, 1305.61/s)  LR: 2.919e-04  Data: 0.010 (0.012)
Train: 385 [ 850/1251 ( 68%)]  Loss: 3.254 (3.27)  Time: 0.772s, 1326.88/s  (0.784s, 1305.67/s)  LR: 2.919e-04  Data: 0.010 (0.012)
Train: 385 [ 900/1251 ( 72%)]  Loss: 3.301 (3.27)  Time: 0.782s, 1309.55/s  (0.785s, 1304.68/s)  LR: 2.919e-04  Data: 0.012 (0.012)
Train: 385 [ 950/1251 ( 76%)]  Loss: 3.156 (3.26)  Time: 0.780s, 1312.05/s  (0.784s, 1305.41/s)  LR: 2.919e-04  Data: 0.010 (0.012)
Train: 385 [1000/1251 ( 80%)]  Loss: 3.565 (3.28)  Time: 0.811s, 1262.35/s  (0.785s, 1304.20/s)  LR: 2.919e-04  Data: 0.010 (0.012)
Train: 385 [1050/1251 ( 84%)]  Loss: 3.545 (3.29)  Time: 0.773s, 1324.43/s  (0.785s, 1304.70/s)  LR: 2.919e-04  Data: 0.009 (0.012)
Train: 385 [1100/1251 ( 88%)]  Loss: 3.351 (3.29)  Time: 0.773s, 1323.87/s  (0.785s, 1304.90/s)  LR: 2.919e-04  Data: 0.009 (0.012)
Train: 385 [1150/1251 ( 92%)]  Loss: 3.331 (3.29)  Time: 0.779s, 1314.63/s  (0.785s, 1304.43/s)  LR: 2.919e-04  Data: 0.010 (0.012)
Train: 385 [1200/1251 ( 96%)]  Loss: 3.197 (3.29)  Time: 0.806s, 1271.18/s  (0.785s, 1304.13/s)  LR: 2.919e-04  Data: 0.010 (0.012)
Train: 385 [1250/1251 (100%)]  Loss: 3.279 (3.29)  Time: 0.802s, 1276.69/s  (0.786s, 1303.36/s)  LR: 2.919e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.526 (1.526)  Loss:  0.7070 (0.7070)  Acc@1: 90.3320 (90.3320)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.194 (0.569)  Loss:  0.8145 (1.1714)  Acc@1: 86.3208 (77.4180)  Acc@5: 97.0519 (93.9460)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-377.pth.tar', 77.48399995605469)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-376.pth.tar', 77.451999921875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-385.pth.tar', 77.41800000488281)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-379.pth.tar', 77.37400005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-382.pth.tar', 77.32599987548828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-384.pth.tar', 77.29799998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-383.pth.tar', 77.25199995361328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-374.pth.tar', 77.24799995361329)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-381.pth.tar', 77.23799997802735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-367.pth.tar', 77.19600000732422)

Train: 386 [   0/1251 (  0%)]  Loss: 3.381 (3.38)  Time: 2.203s,  464.79/s  (2.203s,  464.79/s)  LR: 2.896e-04  Data: 1.489 (1.489)
Train: 386 [  50/1251 (  4%)]  Loss: 3.367 (3.37)  Time: 0.778s, 1316.80/s  (0.819s, 1250.68/s)  LR: 2.896e-04  Data: 0.010 (0.044)
Train: 386 [ 100/1251 (  8%)]  Loss: 3.487 (3.41)  Time: 0.776s, 1319.63/s  (0.800s, 1280.29/s)  LR: 2.896e-04  Data: 0.009 (0.027)
Train: 386 [ 150/1251 ( 12%)]  Loss: 3.357 (3.40)  Time: 0.815s, 1256.52/s  (0.797s, 1284.54/s)  LR: 2.896e-04  Data: 0.011 (0.022)
Train: 386 [ 200/1251 ( 16%)]  Loss: 3.135 (3.35)  Time: 0.774s, 1322.32/s  (0.799s, 1280.98/s)  LR: 2.896e-04  Data: 0.009 (0.019)
Train: 386 [ 250/1251 ( 20%)]  Loss: 3.421 (3.36)  Time: 0.774s, 1323.77/s  (0.796s, 1286.63/s)  LR: 2.896e-04  Data: 0.010 (0.017)
Train: 386 [ 300/1251 ( 24%)]  Loss: 3.305 (3.35)  Time: 0.778s, 1316.63/s  (0.794s, 1289.98/s)  LR: 2.896e-04  Data: 0.010 (0.016)
Train: 386 [ 350/1251 ( 28%)]  Loss: 3.530 (3.37)  Time: 0.774s, 1323.20/s  (0.792s, 1293.29/s)  LR: 2.896e-04  Data: 0.010 (0.015)
Train: 386 [ 400/1251 ( 32%)]  Loss: 3.189 (3.35)  Time: 0.774s, 1322.81/s  (0.791s, 1295.04/s)  LR: 2.896e-04  Data: 0.010 (0.014)
Train: 386 [ 450/1251 ( 36%)]  Loss: 3.346 (3.35)  Time: 0.774s, 1323.72/s  (0.790s, 1297.02/s)  LR: 2.896e-04  Data: 0.010 (0.014)
Train: 386 [ 500/1251 ( 40%)]  Loss: 3.400 (3.36)  Time: 0.777s, 1318.36/s  (0.788s, 1298.79/s)  LR: 2.896e-04  Data: 0.010 (0.014)
Train: 386 [ 550/1251 ( 44%)]  Loss: 3.408 (3.36)  Time: 0.771s, 1327.60/s  (0.787s, 1300.36/s)  LR: 2.896e-04  Data: 0.010 (0.013)
Train: 386 [ 600/1251 ( 48%)]  Loss: 3.210 (3.35)  Time: 0.774s, 1323.13/s  (0.787s, 1301.27/s)  LR: 2.896e-04  Data: 0.010 (0.013)
Train: 386 [ 650/1251 ( 52%)]  Loss: 3.239 (3.34)  Time: 0.797s, 1284.39/s  (0.786s, 1302.42/s)  LR: 2.896e-04  Data: 0.010 (0.013)
Train: 386 [ 700/1251 ( 56%)]  Loss: 3.236 (3.33)  Time: 0.780s, 1312.33/s  (0.786s, 1303.06/s)  LR: 2.896e-04  Data: 0.009 (0.013)
Train: 386 [ 750/1251 ( 60%)]  Loss: 3.338 (3.33)  Time: 0.791s, 1294.75/s  (0.785s, 1303.90/s)  LR: 2.896e-04  Data: 0.015 (0.012)
Train: 386 [ 800/1251 ( 64%)]  Loss: 3.301 (3.33)  Time: 0.773s, 1324.04/s  (0.785s, 1303.87/s)  LR: 2.896e-04  Data: 0.009 (0.012)
Train: 386 [ 850/1251 ( 68%)]  Loss: 3.499 (3.34)  Time: 0.820s, 1249.51/s  (0.785s, 1304.50/s)  LR: 2.896e-04  Data: 0.009 (0.012)
Train: 386 [ 900/1251 ( 72%)]  Loss: 3.397 (3.34)  Time: 0.775s, 1320.69/s  (0.785s, 1304.98/s)  LR: 2.896e-04  Data: 0.009 (0.012)
Train: 386 [ 950/1251 ( 76%)]  Loss: 3.249 (3.34)  Time: 0.833s, 1229.72/s  (0.786s, 1303.15/s)  LR: 2.896e-04  Data: 0.010 (0.012)
Train: 386 [1000/1251 ( 80%)]  Loss: 3.298 (3.34)  Time: 0.775s, 1321.46/s  (0.787s, 1301.72/s)  LR: 2.896e-04  Data: 0.009 (0.012)
Train: 386 [1050/1251 ( 84%)]  Loss: 3.302 (3.34)  Time: 0.775s, 1320.90/s  (0.786s, 1302.36/s)  LR: 2.896e-04  Data: 0.010 (0.012)
Train: 386 [1100/1251 ( 88%)]  Loss: 3.133 (3.33)  Time: 0.778s, 1316.98/s  (0.786s, 1302.97/s)  LR: 2.896e-04  Data: 0.009 (0.012)
Train: 386 [1150/1251 ( 92%)]  Loss: 3.539 (3.34)  Time: 0.771s, 1328.16/s  (0.786s, 1303.44/s)  LR: 2.896e-04  Data: 0.010 (0.011)
Train: 386 [1200/1251 ( 96%)]  Loss: 2.961 (3.32)  Time: 0.772s, 1326.91/s  (0.785s, 1304.04/s)  LR: 2.896e-04  Data: 0.009 (0.011)
Train: 386 [1250/1251 (100%)]  Loss: 3.521 (3.33)  Time: 0.759s, 1349.63/s  (0.785s, 1304.36/s)  LR: 2.896e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.556 (1.556)  Loss:  0.7510 (0.7510)  Acc@1: 90.9180 (90.9180)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.194 (0.575)  Loss:  0.8130 (1.2521)  Acc@1: 86.6745 (77.2360)  Acc@5: 97.6415 (93.8780)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-377.pth.tar', 77.48399995605469)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-376.pth.tar', 77.451999921875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-385.pth.tar', 77.41800000488281)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-379.pth.tar', 77.37400005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-382.pth.tar', 77.32599987548828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-384.pth.tar', 77.29799998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-383.pth.tar', 77.25199995361328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-374.pth.tar', 77.24799995361329)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-381.pth.tar', 77.23799997802735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-386.pth.tar', 77.23600002929687)

Train: 387 [   0/1251 (  0%)]  Loss: 3.179 (3.18)  Time: 2.162s,  473.64/s  (2.162s,  473.64/s)  LR: 2.872e-04  Data: 1.434 (1.434)
Train: 387 [  50/1251 (  4%)]  Loss: 3.089 (3.13)  Time: 0.776s, 1319.42/s  (0.813s, 1259.82/s)  LR: 2.872e-04  Data: 0.009 (0.041)
Train: 387 [ 100/1251 (  8%)]  Loss: 3.213 (3.16)  Time: 0.781s, 1311.58/s  (0.797s, 1285.23/s)  LR: 2.872e-04  Data: 0.009 (0.026)
Train: 387 [ 150/1251 ( 12%)]  Loss: 3.488 (3.24)  Time: 0.796s, 1287.02/s  (0.792s, 1293.63/s)  LR: 2.872e-04  Data: 0.009 (0.020)
Train: 387 [ 200/1251 ( 16%)]  Loss: 3.199 (3.23)  Time: 0.772s, 1326.72/s  (0.789s, 1298.33/s)  LR: 2.872e-04  Data: 0.009 (0.018)
Train: 387 [ 250/1251 ( 20%)]  Loss: 3.458 (3.27)  Time: 0.772s, 1326.39/s  (0.789s, 1297.35/s)  LR: 2.872e-04  Data: 0.010 (0.016)
Train: 387 [ 300/1251 ( 24%)]  Loss: 3.582 (3.32)  Time: 0.772s, 1326.74/s  (0.787s, 1300.82/s)  LR: 2.872e-04  Data: 0.010 (0.015)
Train: 387 [ 350/1251 ( 28%)]  Loss: 3.314 (3.32)  Time: 0.775s, 1321.13/s  (0.786s, 1302.41/s)  LR: 2.872e-04  Data: 0.009 (0.014)
Train: 387 [ 400/1251 ( 32%)]  Loss: 2.974 (3.28)  Time: 0.774s, 1323.11/s  (0.785s, 1304.10/s)  LR: 2.872e-04  Data: 0.009 (0.014)
Train: 387 [ 450/1251 ( 36%)]  Loss: 3.377 (3.29)  Time: 0.773s, 1325.47/s  (0.785s, 1304.54/s)  LR: 2.872e-04  Data: 0.009 (0.013)
Train: 387 [ 500/1251 ( 40%)]  Loss: 3.194 (3.28)  Time: 0.780s, 1312.44/s  (0.785s, 1304.45/s)  LR: 2.872e-04  Data: 0.009 (0.013)
Train: 387 [ 550/1251 ( 44%)]  Loss: 3.302 (3.28)  Time: 0.787s, 1300.63/s  (0.785s, 1305.16/s)  LR: 2.872e-04  Data: 0.011 (0.013)
Train: 387 [ 600/1251 ( 48%)]  Loss: 3.163 (3.27)  Time: 0.818s, 1252.54/s  (0.785s, 1305.11/s)  LR: 2.872e-04  Data: 0.009 (0.012)
Train: 387 [ 650/1251 ( 52%)]  Loss: 3.302 (3.27)  Time: 0.808s, 1267.45/s  (0.787s, 1301.83/s)  LR: 2.872e-04  Data: 0.010 (0.012)
Train: 387 [ 700/1251 ( 56%)]  Loss: 3.244 (3.27)  Time: 0.773s, 1324.25/s  (0.787s, 1300.59/s)  LR: 2.872e-04  Data: 0.010 (0.012)
Train: 387 [ 750/1251 ( 60%)]  Loss: 3.317 (3.27)  Time: 0.858s, 1192.87/s  (0.788s, 1299.89/s)  LR: 2.872e-04  Data: 0.009 (0.012)
Train: 387 [ 800/1251 ( 64%)]  Loss: 3.142 (3.27)  Time: 0.790s, 1296.65/s  (0.788s, 1300.28/s)  LR: 2.872e-04  Data: 0.010 (0.012)
Train: 387 [ 850/1251 ( 68%)]  Loss: 2.844 (3.24)  Time: 0.772s, 1326.25/s  (0.787s, 1300.89/s)  LR: 2.872e-04  Data: 0.010 (0.012)
Train: 387 [ 900/1251 ( 72%)]  Loss: 3.153 (3.24)  Time: 0.773s, 1325.20/s  (0.787s, 1301.28/s)  LR: 2.872e-04  Data: 0.009 (0.011)
Train: 387 [ 950/1251 ( 76%)]  Loss: 3.561 (3.25)  Time: 0.774s, 1322.83/s  (0.787s, 1300.66/s)  LR: 2.872e-04  Data: 0.010 (0.011)
Train: 387 [1000/1251 ( 80%)]  Loss: 3.004 (3.24)  Time: 0.772s, 1326.91/s  (0.787s, 1301.39/s)  LR: 2.872e-04  Data: 0.009 (0.011)
Train: 387 [1050/1251 ( 84%)]  Loss: 3.325 (3.25)  Time: 0.773s, 1324.04/s  (0.787s, 1301.89/s)  LR: 2.872e-04  Data: 0.010 (0.011)
Train: 387 [1100/1251 ( 88%)]  Loss: 3.370 (3.25)  Time: 0.807s, 1269.62/s  (0.786s, 1302.14/s)  LR: 2.872e-04  Data: 0.010 (0.011)
Train: 387 [1150/1251 ( 92%)]  Loss: 3.546 (3.26)  Time: 0.774s, 1322.31/s  (0.787s, 1301.32/s)  LR: 2.872e-04  Data: 0.009 (0.011)
Train: 387 [1200/1251 ( 96%)]  Loss: 3.360 (3.27)  Time: 0.772s, 1326.89/s  (0.787s, 1301.77/s)  LR: 2.872e-04  Data: 0.010 (0.011)
Train: 387 [1250/1251 (100%)]  Loss: 3.184 (3.26)  Time: 0.763s, 1341.91/s  (0.786s, 1302.24/s)  LR: 2.872e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.538 (1.538)  Loss:  0.7388 (0.7388)  Acc@1: 90.9180 (90.9180)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.194 (0.570)  Loss:  0.7676 (1.1814)  Acc@1: 86.4387 (77.5840)  Acc@5: 97.5236 (94.0700)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-387.pth.tar', 77.58400005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-377.pth.tar', 77.48399995605469)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-376.pth.tar', 77.451999921875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-385.pth.tar', 77.41800000488281)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-379.pth.tar', 77.37400005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-382.pth.tar', 77.32599987548828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-384.pth.tar', 77.29799998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-383.pth.tar', 77.25199995361328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-374.pth.tar', 77.24799995361329)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-381.pth.tar', 77.23799997802735)

Train: 388 [   0/1251 (  0%)]  Loss: 3.302 (3.30)  Time: 2.174s,  471.12/s  (2.174s,  471.12/s)  LR: 2.849e-04  Data: 1.441 (1.441)
Train: 388 [  50/1251 (  4%)]  Loss: 3.348 (3.33)  Time: 0.773s, 1324.95/s  (0.813s, 1259.59/s)  LR: 2.849e-04  Data: 0.011 (0.045)
Train: 388 [ 100/1251 (  8%)]  Loss: 3.094 (3.25)  Time: 0.773s, 1325.19/s  (0.796s, 1287.10/s)  LR: 2.849e-04  Data: 0.010 (0.028)
Train: 388 [ 150/1251 ( 12%)]  Loss: 3.167 (3.23)  Time: 0.772s, 1326.03/s  (0.791s, 1293.81/s)  LR: 2.849e-04  Data: 0.009 (0.022)
Train: 388 [ 200/1251 ( 16%)]  Loss: 3.289 (3.24)  Time: 0.777s, 1317.75/s  (0.789s, 1298.36/s)  LR: 2.849e-04  Data: 0.011 (0.019)
Train: 388 [ 250/1251 ( 20%)]  Loss: 3.384 (3.26)  Time: 0.815s, 1256.06/s  (0.791s, 1294.60/s)  LR: 2.849e-04  Data: 0.011 (0.017)
Train: 388 [ 300/1251 ( 24%)]  Loss: 3.253 (3.26)  Time: 0.776s, 1319.92/s  (0.794s, 1289.64/s)  LR: 2.849e-04  Data: 0.010 (0.016)
Train: 388 [ 350/1251 ( 28%)]  Loss: 3.110 (3.24)  Time: 0.771s, 1327.74/s  (0.792s, 1292.71/s)  LR: 2.849e-04  Data: 0.009 (0.015)
Train: 388 [ 400/1251 ( 32%)]  Loss: 3.085 (3.23)  Time: 0.777s, 1317.31/s  (0.790s, 1295.59/s)  LR: 2.849e-04  Data: 0.010 (0.014)
Train: 388 [ 450/1251 ( 36%)]  Loss: 3.499 (3.25)  Time: 0.776s, 1319.77/s  (0.789s, 1297.81/s)  LR: 2.849e-04  Data: 0.009 (0.014)
Train: 388 [ 500/1251 ( 40%)]  Loss: 3.150 (3.24)  Time: 0.814s, 1257.92/s  (0.788s, 1299.28/s)  LR: 2.849e-04  Data: 0.010 (0.013)
Train: 388 [ 550/1251 ( 44%)]  Loss: 3.273 (3.25)  Time: 0.778s, 1316.20/s  (0.787s, 1300.62/s)  LR: 2.849e-04  Data: 0.010 (0.013)
Train: 388 [ 600/1251 ( 48%)]  Loss: 3.356 (3.25)  Time: 0.772s, 1325.83/s  (0.787s, 1301.11/s)  LR: 2.849e-04  Data: 0.009 (0.013)
Train: 388 [ 650/1251 ( 52%)]  Loss: 3.346 (3.26)  Time: 0.773s, 1324.30/s  (0.788s, 1299.99/s)  LR: 2.849e-04  Data: 0.010 (0.013)
Train: 388 [ 700/1251 ( 56%)]  Loss: 3.218 (3.26)  Time: 0.776s, 1318.82/s  (0.787s, 1300.52/s)  LR: 2.849e-04  Data: 0.010 (0.012)
Train: 388 [ 750/1251 ( 60%)]  Loss: 3.268 (3.26)  Time: 0.774s, 1323.83/s  (0.787s, 1301.71/s)  LR: 2.849e-04  Data: 0.009 (0.012)
Train: 388 [ 800/1251 ( 64%)]  Loss: 3.179 (3.25)  Time: 0.819s, 1250.45/s  (0.788s, 1299.88/s)  LR: 2.849e-04  Data: 0.010 (0.012)
Train: 388 [ 850/1251 ( 68%)]  Loss: 3.442 (3.26)  Time: 0.775s, 1321.23/s  (0.788s, 1300.00/s)  LR: 2.849e-04  Data: 0.010 (0.012)
Train: 388 [ 900/1251 ( 72%)]  Loss: 3.169 (3.26)  Time: 0.774s, 1323.52/s  (0.787s, 1300.59/s)  LR: 2.849e-04  Data: 0.010 (0.012)
Train: 388 [ 950/1251 ( 76%)]  Loss: 3.396 (3.27)  Time: 0.828s, 1237.45/s  (0.787s, 1300.48/s)  LR: 2.849e-04  Data: 0.014 (0.012)
Train: 388 [1000/1251 ( 80%)]  Loss: 3.438 (3.27)  Time: 0.773s, 1324.32/s  (0.787s, 1300.74/s)  LR: 2.849e-04  Data: 0.009 (0.012)
Train: 388 [1050/1251 ( 84%)]  Loss: 3.108 (3.27)  Time: 0.774s, 1322.56/s  (0.787s, 1301.14/s)  LR: 2.849e-04  Data: 0.010 (0.012)
Train: 388 [1100/1251 ( 88%)]  Loss: 2.796 (3.25)  Time: 0.774s, 1322.22/s  (0.787s, 1301.73/s)  LR: 2.849e-04  Data: 0.009 (0.012)
Train: 388 [1150/1251 ( 92%)]  Loss: 3.191 (3.24)  Time: 0.776s, 1320.04/s  (0.786s, 1302.32/s)  LR: 2.849e-04  Data: 0.010 (0.011)
Train: 388 [1200/1251 ( 96%)]  Loss: 3.069 (3.24)  Time: 0.773s, 1325.04/s  (0.786s, 1303.02/s)  LR: 2.849e-04  Data: 0.009 (0.011)
Train: 388 [1250/1251 (100%)]  Loss: 3.343 (3.24)  Time: 0.765s, 1338.12/s  (0.786s, 1302.56/s)  LR: 2.849e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.490 (1.490)  Loss:  0.7002 (0.7002)  Acc@1: 90.5273 (90.5273)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.194 (0.558)  Loss:  0.7896 (1.2035)  Acc@1: 87.0283 (77.0600)  Acc@5: 97.4057 (93.8320)
Train: 389 [   0/1251 (  0%)]  Loss: 3.085 (3.09)  Time: 2.238s,  457.52/s  (2.238s,  457.52/s)  LR: 2.826e-04  Data: 1.505 (1.505)
Train: 389 [  50/1251 (  4%)]  Loss: 3.406 (3.25)  Time: 0.772s, 1327.07/s  (0.813s, 1259.80/s)  LR: 2.826e-04  Data: 0.009 (0.041)
Train: 389 [ 100/1251 (  8%)]  Loss: 3.253 (3.25)  Time: 0.772s, 1326.17/s  (0.796s, 1285.99/s)  LR: 2.826e-04  Data: 0.009 (0.026)
Train: 389 [ 150/1251 ( 12%)]  Loss: 3.370 (3.28)  Time: 0.773s, 1324.52/s  (0.791s, 1293.88/s)  LR: 2.826e-04  Data: 0.009 (0.021)
Train: 389 [ 200/1251 ( 16%)]  Loss: 2.891 (3.20)  Time: 0.768s, 1332.84/s  (0.788s, 1299.90/s)  LR: 2.826e-04  Data: 0.009 (0.018)
Train: 389 [ 250/1251 ( 20%)]  Loss: 3.506 (3.25)  Time: 0.793s, 1290.84/s  (0.787s, 1300.79/s)  LR: 2.826e-04  Data: 0.010 (0.016)
Train: 389 [ 300/1251 ( 24%)]  Loss: 3.437 (3.28)  Time: 0.808s, 1267.93/s  (0.790s, 1296.98/s)  LR: 2.826e-04  Data: 0.009 (0.015)
Train: 389 [ 350/1251 ( 28%)]  Loss: 3.274 (3.28)  Time: 0.772s, 1325.95/s  (0.789s, 1297.20/s)  LR: 2.826e-04  Data: 0.009 (0.014)
Train: 389 [ 400/1251 ( 32%)]  Loss: 3.388 (3.29)  Time: 0.773s, 1325.28/s  (0.788s, 1299.71/s)  LR: 2.826e-04  Data: 0.010 (0.014)
Train: 389 [ 450/1251 ( 36%)]  Loss: 2.979 (3.26)  Time: 0.772s, 1327.03/s  (0.787s, 1301.50/s)  LR: 2.826e-04  Data: 0.009 (0.013)
Train: 389 [ 500/1251 ( 40%)]  Loss: 3.364 (3.27)  Time: 0.776s, 1319.42/s  (0.786s, 1302.76/s)  LR: 2.826e-04  Data: 0.010 (0.013)
Train: 389 [ 550/1251 ( 44%)]  Loss: 3.607 (3.30)  Time: 0.771s, 1328.98/s  (0.786s, 1303.18/s)  LR: 2.826e-04  Data: 0.010 (0.013)
Train: 389 [ 600/1251 ( 48%)]  Loss: 3.217 (3.29)  Time: 0.781s, 1310.67/s  (0.785s, 1303.65/s)  LR: 2.826e-04  Data: 0.010 (0.012)
Train: 389 [ 650/1251 ( 52%)]  Loss: 3.562 (3.31)  Time: 0.779s, 1315.18/s  (0.785s, 1304.97/s)  LR: 2.826e-04  Data: 0.011 (0.012)
Train: 389 [ 700/1251 ( 56%)]  Loss: 3.168 (3.30)  Time: 0.775s, 1320.65/s  (0.784s, 1305.80/s)  LR: 2.826e-04  Data: 0.010 (0.012)
Train: 389 [ 750/1251 ( 60%)]  Loss: 3.383 (3.31)  Time: 0.826s, 1239.26/s  (0.785s, 1304.99/s)  LR: 2.826e-04  Data: 0.013 (0.012)
Train: 389 [ 800/1251 ( 64%)]  Loss: 3.453 (3.31)  Time: 0.780s, 1313.37/s  (0.786s, 1303.17/s)  LR: 2.826e-04  Data: 0.009 (0.012)
Train: 389 [ 850/1251 ( 68%)]  Loss: 3.275 (3.31)  Time: 0.815s, 1255.68/s  (0.786s, 1302.35/s)  LR: 2.826e-04  Data: 0.011 (0.012)
Train: 389 [ 900/1251 ( 72%)]  Loss: 3.108 (3.30)  Time: 0.774s, 1322.63/s  (0.788s, 1298.87/s)  LR: 2.826e-04  Data: 0.010 (0.012)
Train: 389 [ 950/1251 ( 76%)]  Loss: 3.740 (3.32)  Time: 0.773s, 1324.97/s  (0.788s, 1299.75/s)  LR: 2.826e-04  Data: 0.010 (0.012)
Train: 389 [1000/1251 ( 80%)]  Loss: 3.364 (3.33)  Time: 0.772s, 1325.91/s  (0.787s, 1300.66/s)  LR: 2.826e-04  Data: 0.010 (0.012)
Train: 389 [1050/1251 ( 84%)]  Loss: 3.716 (3.34)  Time: 0.811s, 1262.56/s  (0.788s, 1299.60/s)  LR: 2.826e-04  Data: 0.013 (0.012)
Train: 389 [1100/1251 ( 88%)]  Loss: 2.936 (3.33)  Time: 0.772s, 1326.94/s  (0.789s, 1297.75/s)  LR: 2.826e-04  Data: 0.009 (0.011)
Train: 389 [1150/1251 ( 92%)]  Loss: 3.181 (3.32)  Time: 0.774s, 1322.82/s  (0.789s, 1298.03/s)  LR: 2.826e-04  Data: 0.010 (0.011)
Train: 389 [1200/1251 ( 96%)]  Loss: 3.636 (3.33)  Time: 0.808s, 1266.64/s  (0.789s, 1297.18/s)  LR: 2.826e-04  Data: 0.009 (0.011)
Train: 389 [1250/1251 (100%)]  Loss: 3.028 (3.32)  Time: 0.758s, 1350.58/s  (0.789s, 1297.47/s)  LR: 2.826e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.513 (1.513)  Loss:  0.7368 (0.7368)  Acc@1: 90.8203 (90.8203)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.194 (0.565)  Loss:  0.7773 (1.2109)  Acc@1: 87.6179 (77.6320)  Acc@5: 98.1132 (94.1740)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-389.pth.tar', 77.63200005126953)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-387.pth.tar', 77.58400005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-377.pth.tar', 77.48399995605469)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-376.pth.tar', 77.451999921875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-385.pth.tar', 77.41800000488281)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-379.pth.tar', 77.37400005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-382.pth.tar', 77.32599987548828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-384.pth.tar', 77.29799998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-383.pth.tar', 77.25199995361328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-374.pth.tar', 77.24799995361329)

Train: 390 [   0/1251 (  0%)]  Loss: 3.168 (3.17)  Time: 2.234s,  458.29/s  (2.234s,  458.29/s)  LR: 2.803e-04  Data: 1.500 (1.500)
Train: 390 [  50/1251 (  4%)]  Loss: 3.057 (3.11)  Time: 0.833s, 1229.22/s  (0.818s, 1251.74/s)  LR: 2.803e-04  Data: 0.009 (0.046)
Train: 390 [ 100/1251 (  8%)]  Loss: 3.028 (3.08)  Time: 0.787s, 1301.78/s  (0.802s, 1276.21/s)  LR: 2.803e-04  Data: 0.010 (0.028)
Train: 390 [ 150/1251 ( 12%)]  Loss: 3.158 (3.10)  Time: 0.812s, 1260.48/s  (0.796s, 1285.79/s)  LR: 2.803e-04  Data: 0.010 (0.022)
Train: 390 [ 200/1251 ( 16%)]  Loss: 3.327 (3.15)  Time: 0.773s, 1324.62/s  (0.794s, 1289.22/s)  LR: 2.803e-04  Data: 0.010 (0.019)
Train: 390 [ 250/1251 ( 20%)]  Loss: 3.738 (3.25)  Time: 0.777s, 1318.10/s  (0.791s, 1294.27/s)  LR: 2.803e-04  Data: 0.009 (0.017)
Train: 390 [ 300/1251 ( 24%)]  Loss: 3.696 (3.31)  Time: 0.775s, 1321.10/s  (0.789s, 1297.25/s)  LR: 2.803e-04  Data: 0.010 (0.016)
Train: 390 [ 350/1251 ( 28%)]  Loss: 3.398 (3.32)  Time: 0.781s, 1310.64/s  (0.788s, 1299.60/s)  LR: 2.803e-04  Data: 0.010 (0.015)
Train: 390 [ 400/1251 ( 32%)]  Loss: 3.549 (3.35)  Time: 0.774s, 1322.48/s  (0.787s, 1300.96/s)  LR: 2.803e-04  Data: 0.010 (0.014)
Train: 390 [ 450/1251 ( 36%)]  Loss: 2.909 (3.30)  Time: 0.844s, 1213.67/s  (0.790s, 1295.59/s)  LR: 2.803e-04  Data: 0.010 (0.014)
Train: 390 [ 500/1251 ( 40%)]  Loss: 3.217 (3.30)  Time: 0.836s, 1224.61/s  (0.789s, 1297.34/s)  LR: 2.803e-04  Data: 0.009 (0.013)
Train: 390 [ 550/1251 ( 44%)]  Loss: 3.493 (3.31)  Time: 0.773s, 1324.76/s  (0.790s, 1296.91/s)  LR: 2.803e-04  Data: 0.009 (0.013)
Train: 390 [ 600/1251 ( 48%)]  Loss: 3.619 (3.34)  Time: 0.834s, 1227.45/s  (0.789s, 1298.28/s)  LR: 2.803e-04  Data: 0.009 (0.013)
Train: 390 [ 650/1251 ( 52%)]  Loss: 3.151 (3.32)  Time: 0.805s, 1271.63/s  (0.790s, 1295.95/s)  LR: 2.803e-04  Data: 0.009 (0.013)
Train: 390 [ 700/1251 ( 56%)]  Loss: 3.073 (3.31)  Time: 0.818s, 1251.38/s  (0.792s, 1292.84/s)  LR: 2.803e-04  Data: 0.010 (0.012)
Train: 390 [ 750/1251 ( 60%)]  Loss: 3.140 (3.30)  Time: 0.849s, 1206.74/s  (0.794s, 1290.24/s)  LR: 2.803e-04  Data: 0.011 (0.012)
Train: 390 [ 800/1251 ( 64%)]  Loss: 2.938 (3.27)  Time: 0.773s, 1324.51/s  (0.793s, 1290.54/s)  LR: 2.803e-04  Data: 0.011 (0.012)
Train: 390 [ 850/1251 ( 68%)]  Loss: 3.283 (3.27)  Time: 0.784s, 1305.88/s  (0.793s, 1291.78/s)  LR: 2.803e-04  Data: 0.011 (0.012)
Train: 390 [ 900/1251 ( 72%)]  Loss: 3.136 (3.27)  Time: 0.778s, 1315.57/s  (0.793s, 1291.94/s)  LR: 2.803e-04  Data: 0.014 (0.012)
Train: 390 [ 950/1251 ( 76%)]  Loss: 3.174 (3.26)  Time: 0.772s, 1326.51/s  (0.792s, 1293.08/s)  LR: 2.803e-04  Data: 0.010 (0.012)
Train: 390 [1000/1251 ( 80%)]  Loss: 3.531 (3.28)  Time: 0.786s, 1302.74/s  (0.792s, 1293.45/s)  LR: 2.803e-04  Data: 0.010 (0.012)
Train: 390 [1050/1251 ( 84%)]  Loss: 3.320 (3.28)  Time: 0.776s, 1319.39/s  (0.791s, 1294.06/s)  LR: 2.803e-04  Data: 0.010 (0.012)
Train: 390 [1100/1251 ( 88%)]  Loss: 3.263 (3.28)  Time: 0.774s, 1322.27/s  (0.791s, 1294.72/s)  LR: 2.803e-04  Data: 0.010 (0.012)
Train: 390 [1150/1251 ( 92%)]  Loss: 3.434 (3.28)  Time: 0.774s, 1323.66/s  (0.791s, 1295.38/s)  LR: 2.803e-04  Data: 0.010 (0.012)
Train: 390 [1200/1251 ( 96%)]  Loss: 3.414 (3.29)  Time: 0.786s, 1302.98/s  (0.790s, 1296.17/s)  LR: 2.803e-04  Data: 0.009 (0.012)
Train: 390 [1250/1251 (100%)]  Loss: 3.172 (3.28)  Time: 0.772s, 1326.89/s  (0.790s, 1296.40/s)  LR: 2.803e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.617 (1.617)  Loss:  0.7612 (0.7612)  Acc@1: 90.9180 (90.9180)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.194 (0.563)  Loss:  0.8970 (1.2468)  Acc@1: 85.7311 (77.4600)  Acc@5: 97.0519 (94.0360)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-389.pth.tar', 77.63200005126953)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-387.pth.tar', 77.58400005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-377.pth.tar', 77.48399995605469)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-390.pth.tar', 77.46000013671875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-376.pth.tar', 77.451999921875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-385.pth.tar', 77.41800000488281)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-379.pth.tar', 77.37400005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-382.pth.tar', 77.32599987548828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-384.pth.tar', 77.29799998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-383.pth.tar', 77.25199995361328)

Train: 391 [   0/1251 (  0%)]  Loss: 3.292 (3.29)  Time: 2.217s,  461.94/s  (2.217s,  461.94/s)  LR: 2.780e-04  Data: 1.496 (1.496)
Train: 391 [  50/1251 (  4%)]  Loss: 3.211 (3.25)  Time: 0.773s, 1325.55/s  (0.811s, 1262.00/s)  LR: 2.780e-04  Data: 0.010 (0.043)
Train: 391 [ 100/1251 (  8%)]  Loss: 3.167 (3.22)  Time: 0.772s, 1326.27/s  (0.795s, 1287.87/s)  LR: 2.780e-04  Data: 0.010 (0.026)
Train: 391 [ 150/1251 ( 12%)]  Loss: 3.417 (3.27)  Time: 0.773s, 1324.63/s  (0.792s, 1292.61/s)  LR: 2.780e-04  Data: 0.009 (0.021)
Train: 391 [ 200/1251 ( 16%)]  Loss: 3.652 (3.35)  Time: 0.772s, 1326.56/s  (0.790s, 1296.38/s)  LR: 2.780e-04  Data: 0.010 (0.018)
Train: 391 [ 250/1251 ( 20%)]  Loss: 3.332 (3.35)  Time: 0.777s, 1318.42/s  (0.787s, 1300.55/s)  LR: 2.780e-04  Data: 0.009 (0.016)
Train: 391 [ 300/1251 ( 24%)]  Loss: 3.488 (3.37)  Time: 0.776s, 1319.08/s  (0.786s, 1303.03/s)  LR: 2.780e-04  Data: 0.010 (0.015)
Train: 391 [ 350/1251 ( 28%)]  Loss: 2.915 (3.31)  Time: 0.818s, 1251.25/s  (0.789s, 1297.41/s)  LR: 2.780e-04  Data: 0.009 (0.015)
Train: 391 [ 400/1251 ( 32%)]  Loss: 3.144 (3.29)  Time: 0.776s, 1319.75/s  (0.792s, 1293.60/s)  LR: 2.780e-04  Data: 0.009 (0.014)
Train: 391 [ 450/1251 ( 36%)]  Loss: 3.243 (3.29)  Time: 0.823s, 1244.90/s  (0.791s, 1295.21/s)  LR: 2.780e-04  Data: 0.009 (0.013)
Train: 391 [ 500/1251 ( 40%)]  Loss: 3.706 (3.32)  Time: 0.772s, 1326.13/s  (0.792s, 1293.54/s)  LR: 2.780e-04  Data: 0.010 (0.013)
Train: 391 [ 550/1251 ( 44%)]  Loss: 3.194 (3.31)  Time: 0.775s, 1320.81/s  (0.790s, 1296.01/s)  LR: 2.780e-04  Data: 0.009 (0.013)
Train: 391 [ 600/1251 ( 48%)]  Loss: 3.389 (3.32)  Time: 0.780s, 1312.16/s  (0.789s, 1297.79/s)  LR: 2.780e-04  Data: 0.009 (0.013)
Train: 391 [ 650/1251 ( 52%)]  Loss: 3.233 (3.31)  Time: 0.771s, 1327.62/s  (0.788s, 1298.87/s)  LR: 2.780e-04  Data: 0.009 (0.012)
Train: 391 [ 700/1251 ( 56%)]  Loss: 3.079 (3.30)  Time: 0.789s, 1298.18/s  (0.788s, 1299.97/s)  LR: 2.780e-04  Data: 0.009 (0.012)
Train: 391 [ 750/1251 ( 60%)]  Loss: 3.065 (3.28)  Time: 0.874s, 1171.21/s  (0.787s, 1300.48/s)  LR: 2.780e-04  Data: 0.009 (0.012)
Train: 391 [ 800/1251 ( 64%)]  Loss: 3.730 (3.31)  Time: 0.772s, 1326.40/s  (0.787s, 1301.67/s)  LR: 2.780e-04  Data: 0.010 (0.012)
Train: 391 [ 850/1251 ( 68%)]  Loss: 3.355 (3.31)  Time: 0.816s, 1254.32/s  (0.786s, 1302.58/s)  LR: 2.780e-04  Data: 0.011 (0.012)
Train: 391 [ 900/1251 ( 72%)]  Loss: 3.081 (3.30)  Time: 0.772s, 1326.78/s  (0.786s, 1303.29/s)  LR: 2.780e-04  Data: 0.009 (0.012)
Train: 391 [ 950/1251 ( 76%)]  Loss: 3.010 (3.29)  Time: 0.772s, 1326.65/s  (0.785s, 1304.07/s)  LR: 2.780e-04  Data: 0.009 (0.011)
Train: 391 [1000/1251 ( 80%)]  Loss: 3.334 (3.29)  Time: 0.773s, 1324.04/s  (0.785s, 1304.12/s)  LR: 2.780e-04  Data: 0.010 (0.011)
Train: 391 [1050/1251 ( 84%)]  Loss: 3.570 (3.30)  Time: 0.770s, 1329.81/s  (0.785s, 1304.75/s)  LR: 2.780e-04  Data: 0.009 (0.011)
Train: 391 [1100/1251 ( 88%)]  Loss: 3.431 (3.31)  Time: 0.781s, 1311.26/s  (0.785s, 1305.03/s)  LR: 2.780e-04  Data: 0.015 (0.011)
Train: 391 [1150/1251 ( 92%)]  Loss: 3.492 (3.31)  Time: 0.777s, 1318.67/s  (0.785s, 1305.25/s)  LR: 2.780e-04  Data: 0.009 (0.011)
Train: 391 [1200/1251 ( 96%)]  Loss: 3.377 (3.32)  Time: 0.775s, 1321.51/s  (0.784s, 1305.59/s)  LR: 2.780e-04  Data: 0.009 (0.011)
Train: 391 [1250/1251 (100%)]  Loss: 3.612 (3.33)  Time: 0.760s, 1346.66/s  (0.784s, 1305.90/s)  LR: 2.780e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.594 (1.594)  Loss:  0.7017 (0.7017)  Acc@1: 91.3086 (91.3086)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.194 (0.563)  Loss:  0.7778 (1.2117)  Acc@1: 87.3821 (77.4320)  Acc@5: 97.4057 (94.0840)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-389.pth.tar', 77.63200005126953)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-387.pth.tar', 77.58400005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-377.pth.tar', 77.48399995605469)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-390.pth.tar', 77.46000013671875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-376.pth.tar', 77.451999921875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-391.pth.tar', 77.432000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-385.pth.tar', 77.41800000488281)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-379.pth.tar', 77.37400005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-382.pth.tar', 77.32599987548828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-384.pth.tar', 77.29799998046875)

Train: 392 [   0/1251 (  0%)]  Loss: 3.520 (3.52)  Time: 2.358s,  434.25/s  (2.358s,  434.25/s)  LR: 2.757e-04  Data: 1.623 (1.623)
Train: 392 [  50/1251 (  4%)]  Loss: 3.050 (3.28)  Time: 0.775s, 1320.85/s  (0.816s, 1254.52/s)  LR: 2.757e-04  Data: 0.009 (0.046)
Train: 392 [ 100/1251 (  8%)]  Loss: 3.275 (3.28)  Time: 0.785s, 1303.90/s  (0.798s, 1282.63/s)  LR: 2.757e-04  Data: 0.010 (0.028)
Train: 392 [ 150/1251 ( 12%)]  Loss: 3.224 (3.27)  Time: 0.825s, 1241.20/s  (0.794s, 1290.09/s)  LR: 2.757e-04  Data: 0.013 (0.022)
Train: 392 [ 200/1251 ( 16%)]  Loss: 3.445 (3.30)  Time: 0.773s, 1325.09/s  (0.791s, 1294.02/s)  LR: 2.757e-04  Data: 0.010 (0.019)
Train: 392 [ 250/1251 ( 20%)]  Loss: 3.221 (3.29)  Time: 0.772s, 1327.04/s  (0.789s, 1297.31/s)  LR: 2.757e-04  Data: 0.010 (0.017)
Train: 392 [ 300/1251 ( 24%)]  Loss: 3.194 (3.28)  Time: 0.773s, 1323.93/s  (0.788s, 1300.13/s)  LR: 2.757e-04  Data: 0.009 (0.016)
Train: 392 [ 350/1251 ( 28%)]  Loss: 3.077 (3.25)  Time: 0.773s, 1324.34/s  (0.786s, 1302.00/s)  LR: 2.757e-04  Data: 0.009 (0.015)
Train: 392 [ 400/1251 ( 32%)]  Loss: 3.321 (3.26)  Time: 0.782s, 1309.34/s  (0.785s, 1303.85/s)  LR: 2.757e-04  Data: 0.009 (0.014)
Train: 392 [ 450/1251 ( 36%)]  Loss: 3.219 (3.25)  Time: 0.773s, 1324.05/s  (0.785s, 1304.98/s)  LR: 2.757e-04  Data: 0.009 (0.014)
Train: 392 [ 500/1251 ( 40%)]  Loss: 3.282 (3.26)  Time: 0.818s, 1252.03/s  (0.785s, 1304.47/s)  LR: 2.757e-04  Data: 0.009 (0.013)
Train: 392 [ 550/1251 ( 44%)]  Loss: 3.321 (3.26)  Time: 0.783s, 1308.38/s  (0.786s, 1303.49/s)  LR: 2.757e-04  Data: 0.009 (0.013)
Train: 392 [ 600/1251 ( 48%)]  Loss: 3.412 (3.27)  Time: 0.785s, 1303.75/s  (0.785s, 1304.34/s)  LR: 2.757e-04  Data: 0.010 (0.013)
Train: 392 [ 650/1251 ( 52%)]  Loss: 3.243 (3.27)  Time: 0.772s, 1325.80/s  (0.785s, 1304.14/s)  LR: 2.757e-04  Data: 0.009 (0.013)
Train: 392 [ 700/1251 ( 56%)]  Loss: 3.185 (3.27)  Time: 0.800s, 1280.12/s  (0.785s, 1304.81/s)  LR: 2.757e-04  Data: 0.009 (0.012)
Train: 392 [ 750/1251 ( 60%)]  Loss: 3.124 (3.26)  Time: 0.774s, 1323.44/s  (0.784s, 1305.54/s)  LR: 2.757e-04  Data: 0.009 (0.012)
Train: 392 [ 800/1251 ( 64%)]  Loss: 3.131 (3.25)  Time: 0.771s, 1327.70/s  (0.784s, 1305.54/s)  LR: 2.757e-04  Data: 0.011 (0.012)
Train: 392 [ 850/1251 ( 68%)]  Loss: 3.381 (3.26)  Time: 0.777s, 1317.82/s  (0.784s, 1305.54/s)  LR: 2.757e-04  Data: 0.009 (0.012)
Train: 392 [ 900/1251 ( 72%)]  Loss: 3.600 (3.28)  Time: 0.774s, 1323.28/s  (0.785s, 1305.29/s)  LR: 2.757e-04  Data: 0.010 (0.012)
Train: 392 [ 950/1251 ( 76%)]  Loss: 3.132 (3.27)  Time: 0.775s, 1321.24/s  (0.784s, 1305.47/s)  LR: 2.757e-04  Data: 0.010 (0.012)
Train: 392 [1000/1251 ( 80%)]  Loss: 3.118 (3.26)  Time: 0.772s, 1325.78/s  (0.784s, 1306.01/s)  LR: 2.757e-04  Data: 0.009 (0.012)
Train: 392 [1050/1251 ( 84%)]  Loss: 3.213 (3.26)  Time: 0.775s, 1321.12/s  (0.785s, 1304.90/s)  LR: 2.757e-04  Data: 0.010 (0.012)
Train: 392 [1100/1251 ( 88%)]  Loss: 3.130 (3.25)  Time: 0.780s, 1312.79/s  (0.785s, 1305.24/s)  LR: 2.757e-04  Data: 0.009 (0.011)
Train: 392 [1150/1251 ( 92%)]  Loss: 3.485 (3.26)  Time: 0.773s, 1325.51/s  (0.785s, 1304.32/s)  LR: 2.757e-04  Data: 0.010 (0.011)
Train: 392 [1200/1251 ( 96%)]  Loss: 3.481 (3.27)  Time: 0.794s, 1289.16/s  (0.785s, 1305.00/s)  LR: 2.757e-04  Data: 0.011 (0.011)
Train: 392 [1250/1251 (100%)]  Loss: 2.958 (3.26)  Time: 0.762s, 1343.22/s  (0.784s, 1305.36/s)  LR: 2.757e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.536 (1.536)  Loss:  0.7114 (0.7114)  Acc@1: 90.8203 (90.8203)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.194 (0.556)  Loss:  0.7554 (1.2484)  Acc@1: 86.5566 (77.3460)  Acc@5: 97.2877 (93.8980)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-389.pth.tar', 77.63200005126953)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-387.pth.tar', 77.58400005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-377.pth.tar', 77.48399995605469)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-390.pth.tar', 77.46000013671875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-376.pth.tar', 77.451999921875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-391.pth.tar', 77.432000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-385.pth.tar', 77.41800000488281)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-379.pth.tar', 77.37400005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-392.pth.tar', 77.34599997802735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-382.pth.tar', 77.32599987548828)

Train: 393 [   0/1251 (  0%)]  Loss: 3.466 (3.47)  Time: 2.208s,  463.69/s  (2.208s,  463.69/s)  LR: 2.734e-04  Data: 1.477 (1.477)
Train: 393 [  50/1251 (  4%)]  Loss: 3.590 (3.53)  Time: 0.782s, 1310.17/s  (0.819s, 1249.94/s)  LR: 2.734e-04  Data: 0.009 (0.044)
Train: 393 [ 100/1251 (  8%)]  Loss: 3.205 (3.42)  Time: 0.802s, 1277.28/s  (0.801s, 1278.55/s)  LR: 2.734e-04  Data: 0.009 (0.027)
Train: 393 [ 150/1251 ( 12%)]  Loss: 3.337 (3.40)  Time: 0.775s, 1320.90/s  (0.794s, 1289.52/s)  LR: 2.734e-04  Data: 0.010 (0.021)
Train: 393 [ 200/1251 ( 16%)]  Loss: 3.415 (3.40)  Time: 0.776s, 1320.18/s  (0.791s, 1295.27/s)  LR: 2.734e-04  Data: 0.009 (0.018)
Train: 393 [ 250/1251 ( 20%)]  Loss: 3.340 (3.39)  Time: 0.774s, 1323.27/s  (0.789s, 1297.31/s)  LR: 2.734e-04  Data: 0.010 (0.017)
Train: 393 [ 300/1251 ( 24%)]  Loss: 3.336 (3.38)  Time: 0.774s, 1322.56/s  (0.787s, 1300.88/s)  LR: 2.734e-04  Data: 0.009 (0.015)
Train: 393 [ 350/1251 ( 28%)]  Loss: 3.246 (3.37)  Time: 0.784s, 1306.64/s  (0.787s, 1301.92/s)  LR: 2.734e-04  Data: 0.009 (0.015)
Train: 393 [ 400/1251 ( 32%)]  Loss: 3.239 (3.35)  Time: 0.778s, 1316.55/s  (0.786s, 1303.12/s)  LR: 2.734e-04  Data: 0.010 (0.014)
Train: 393 [ 450/1251 ( 36%)]  Loss: 3.254 (3.34)  Time: 0.773s, 1325.41/s  (0.785s, 1303.66/s)  LR: 2.734e-04  Data: 0.009 (0.014)
Train: 393 [ 500/1251 ( 40%)]  Loss: 2.661 (3.28)  Time: 0.773s, 1324.12/s  (0.785s, 1304.38/s)  LR: 2.734e-04  Data: 0.010 (0.013)
Train: 393 [ 550/1251 ( 44%)]  Loss: 3.292 (3.28)  Time: 0.779s, 1314.32/s  (0.785s, 1304.21/s)  LR: 2.734e-04  Data: 0.010 (0.013)
Train: 393 [ 600/1251 ( 48%)]  Loss: 3.335 (3.29)  Time: 0.786s, 1302.85/s  (0.785s, 1305.23/s)  LR: 2.734e-04  Data: 0.009 (0.013)
Train: 393 [ 650/1251 ( 52%)]  Loss: 3.034 (3.27)  Time: 0.772s, 1326.93/s  (0.784s, 1305.95/s)  LR: 2.734e-04  Data: 0.010 (0.012)
Train: 393 [ 700/1251 ( 56%)]  Loss: 3.275 (3.27)  Time: 0.773s, 1325.41/s  (0.784s, 1306.58/s)  LR: 2.734e-04  Data: 0.010 (0.012)
Train: 393 [ 750/1251 ( 60%)]  Loss: 3.474 (3.28)  Time: 0.772s, 1326.45/s  (0.784s, 1306.73/s)  LR: 2.734e-04  Data: 0.010 (0.012)
Train: 393 [ 800/1251 ( 64%)]  Loss: 3.351 (3.29)  Time: 0.777s, 1317.37/s  (0.784s, 1306.82/s)  LR: 2.734e-04  Data: 0.010 (0.012)
Train: 393 [ 850/1251 ( 68%)]  Loss: 3.064 (3.27)  Time: 0.773s, 1324.75/s  (0.784s, 1306.53/s)  LR: 2.734e-04  Data: 0.009 (0.012)
Train: 393 [ 900/1251 ( 72%)]  Loss: 3.044 (3.26)  Time: 0.783s, 1308.32/s  (0.784s, 1306.79/s)  LR: 2.734e-04  Data: 0.009 (0.012)
Train: 393 [ 950/1251 ( 76%)]  Loss: 3.594 (3.28)  Time: 0.780s, 1312.97/s  (0.783s, 1307.26/s)  LR: 2.734e-04  Data: 0.010 (0.012)
Train: 393 [1000/1251 ( 80%)]  Loss: 3.190 (3.27)  Time: 0.787s, 1300.51/s  (0.783s, 1307.41/s)  LR: 2.734e-04  Data: 0.011 (0.011)
Train: 393 [1050/1251 ( 84%)]  Loss: 3.246 (3.27)  Time: 0.773s, 1325.33/s  (0.783s, 1307.21/s)  LR: 2.734e-04  Data: 0.009 (0.011)
Train: 393 [1100/1251 ( 88%)]  Loss: 3.336 (3.27)  Time: 0.773s, 1324.32/s  (0.783s, 1307.25/s)  LR: 2.734e-04  Data: 0.010 (0.011)
Train: 393 [1150/1251 ( 92%)]  Loss: 3.075 (3.27)  Time: 0.781s, 1310.58/s  (0.783s, 1307.49/s)  LR: 2.734e-04  Data: 0.010 (0.011)
Train: 393 [1200/1251 ( 96%)]  Loss: 3.509 (3.28)  Time: 0.773s, 1325.46/s  (0.783s, 1307.59/s)  LR: 2.734e-04  Data: 0.009 (0.011)
Train: 393 [1250/1251 (100%)]  Loss: 3.472 (3.28)  Time: 0.793s, 1290.58/s  (0.783s, 1307.55/s)  LR: 2.734e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.481 (1.481)  Loss:  0.6406 (0.6406)  Acc@1: 89.7461 (89.7461)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.193 (0.562)  Loss:  0.7251 (1.1424)  Acc@1: 87.1462 (77.7400)  Acc@5: 97.6415 (94.1400)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-393.pth.tar', 77.73999997558593)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-389.pth.tar', 77.63200005126953)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-387.pth.tar', 77.58400005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-377.pth.tar', 77.48399995605469)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-390.pth.tar', 77.46000013671875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-376.pth.tar', 77.451999921875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-391.pth.tar', 77.432000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-385.pth.tar', 77.41800000488281)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-379.pth.tar', 77.37400005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-392.pth.tar', 77.34599997802735)

Train: 394 [   0/1251 (  0%)]  Loss: 2.963 (2.96)  Time: 2.385s,  429.34/s  (2.385s,  429.34/s)  LR: 2.711e-04  Data: 1.658 (1.658)
Train: 394 [  50/1251 (  4%)]  Loss: 3.422 (3.19)  Time: 0.773s, 1325.30/s  (0.813s, 1259.78/s)  LR: 2.711e-04  Data: 0.009 (0.045)
Train: 394 [ 100/1251 (  8%)]  Loss: 3.089 (3.16)  Time: 0.777s, 1317.45/s  (0.796s, 1285.79/s)  LR: 2.711e-04  Data: 0.010 (0.028)
Train: 394 [ 150/1251 ( 12%)]  Loss: 3.395 (3.22)  Time: 0.784s, 1305.74/s  (0.800s, 1279.96/s)  LR: 2.711e-04  Data: 0.015 (0.022)
Train: 394 [ 200/1251 ( 16%)]  Loss: 3.443 (3.26)  Time: 0.781s, 1311.22/s  (0.795s, 1287.83/s)  LR: 2.711e-04  Data: 0.010 (0.019)
Train: 394 [ 250/1251 ( 20%)]  Loss: 3.563 (3.31)  Time: 0.773s, 1324.57/s  (0.793s, 1291.43/s)  LR: 2.711e-04  Data: 0.009 (0.017)
Train: 394 [ 300/1251 ( 24%)]  Loss: 3.239 (3.30)  Time: 0.771s, 1327.88/s  (0.791s, 1295.21/s)  LR: 2.711e-04  Data: 0.009 (0.016)
Train: 394 [ 350/1251 ( 28%)]  Loss: 3.527 (3.33)  Time: 0.780s, 1312.78/s  (0.789s, 1298.55/s)  LR: 2.711e-04  Data: 0.010 (0.015)
Train: 394 [ 400/1251 ( 32%)]  Loss: 3.640 (3.36)  Time: 0.784s, 1306.79/s  (0.788s, 1300.23/s)  LR: 2.711e-04  Data: 0.010 (0.014)
Train: 394 [ 450/1251 ( 36%)]  Loss: 3.529 (3.38)  Time: 0.784s, 1306.71/s  (0.787s, 1301.55/s)  LR: 2.711e-04  Data: 0.010 (0.014)
Train: 394 [ 500/1251 ( 40%)]  Loss: 3.338 (3.38)  Time: 0.807s, 1268.87/s  (0.789s, 1297.57/s)  LR: 2.711e-04  Data: 0.009 (0.014)
Train: 394 [ 550/1251 ( 44%)]  Loss: 3.443 (3.38)  Time: 0.779s, 1313.90/s  (0.790s, 1295.42/s)  LR: 2.711e-04  Data: 0.010 (0.013)
Train: 394 [ 600/1251 ( 48%)]  Loss: 3.207 (3.37)  Time: 0.782s, 1309.16/s  (0.789s, 1297.04/s)  LR: 2.711e-04  Data: 0.010 (0.013)
Train: 394 [ 650/1251 ( 52%)]  Loss: 3.079 (3.35)  Time: 0.781s, 1311.76/s  (0.789s, 1297.65/s)  LR: 2.711e-04  Data: 0.010 (0.013)
Train: 394 [ 700/1251 ( 56%)]  Loss: 3.448 (3.35)  Time: 0.779s, 1313.98/s  (0.789s, 1297.69/s)  LR: 2.711e-04  Data: 0.010 (0.012)
Train: 394 [ 750/1251 ( 60%)]  Loss: 2.862 (3.32)  Time: 0.775s, 1322.05/s  (0.789s, 1298.16/s)  LR: 2.711e-04  Data: 0.010 (0.012)
Train: 394 [ 800/1251 ( 64%)]  Loss: 3.376 (3.33)  Time: 0.776s, 1319.29/s  (0.789s, 1298.33/s)  LR: 2.711e-04  Data: 0.011 (0.012)
Train: 394 [ 850/1251 ( 68%)]  Loss: 3.520 (3.34)  Time: 0.832s, 1230.71/s  (0.789s, 1298.57/s)  LR: 2.711e-04  Data: 0.009 (0.012)
Train: 394 [ 900/1251 ( 72%)]  Loss: 3.363 (3.34)  Time: 0.775s, 1321.92/s  (0.788s, 1299.01/s)  LR: 2.711e-04  Data: 0.010 (0.012)
Train: 394 [ 950/1251 ( 76%)]  Loss: 3.103 (3.33)  Time: 0.774s, 1322.62/s  (0.788s, 1298.83/s)  LR: 2.711e-04  Data: 0.009 (0.012)
Train: 394 [1000/1251 ( 80%)]  Loss: 3.518 (3.34)  Time: 0.772s, 1326.03/s  (0.788s, 1299.54/s)  LR: 2.711e-04  Data: 0.010 (0.012)
Train: 394 [1050/1251 ( 84%)]  Loss: 3.619 (3.35)  Time: 0.809s, 1265.20/s  (0.788s, 1300.14/s)  LR: 2.711e-04  Data: 0.010 (0.012)
Train: 394 [1100/1251 ( 88%)]  Loss: 3.270 (3.35)  Time: 0.784s, 1305.55/s  (0.787s, 1300.71/s)  LR: 2.711e-04  Data: 0.010 (0.011)
Train: 394 [1150/1251 ( 92%)]  Loss: 3.502 (3.35)  Time: 0.773s, 1325.14/s  (0.787s, 1301.12/s)  LR: 2.711e-04  Data: 0.009 (0.011)
Train: 394 [1200/1251 ( 96%)]  Loss: 3.297 (3.35)  Time: 0.774s, 1323.04/s  (0.787s, 1301.57/s)  LR: 2.711e-04  Data: 0.009 (0.011)
Train: 394 [1250/1251 (100%)]  Loss: 3.554 (3.36)  Time: 0.796s, 1287.05/s  (0.787s, 1301.23/s)  LR: 2.711e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.580 (1.580)  Loss:  0.7959 (0.7959)  Acc@1: 89.8438 (89.8438)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.194 (0.570)  Loss:  0.8706 (1.2417)  Acc@1: 85.6132 (77.5040)  Acc@5: 97.2877 (94.0300)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-393.pth.tar', 77.73999997558593)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-389.pth.tar', 77.63200005126953)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-387.pth.tar', 77.58400005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-394.pth.tar', 77.50399995605468)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-377.pth.tar', 77.48399995605469)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-390.pth.tar', 77.46000013671875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-376.pth.tar', 77.451999921875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-391.pth.tar', 77.432000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-385.pth.tar', 77.41800000488281)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-379.pth.tar', 77.37400005615234)

Train: 395 [   0/1251 (  0%)]  Loss: 3.094 (3.09)  Time: 2.530s,  404.74/s  (2.530s,  404.74/s)  LR: 2.688e-04  Data: 1.746 (1.746)
Train: 395 [  50/1251 (  4%)]  Loss: 3.269 (3.18)  Time: 0.773s, 1324.56/s  (0.825s, 1240.51/s)  LR: 2.688e-04  Data: 0.009 (0.052)
Train: 395 [ 100/1251 (  8%)]  Loss: 3.196 (3.19)  Time: 0.826s, 1239.70/s  (0.810s, 1263.53/s)  LR: 2.688e-04  Data: 0.011 (0.031)
Train: 395 [ 150/1251 ( 12%)]  Loss: 3.431 (3.25)  Time: 0.778s, 1316.46/s  (0.805s, 1272.24/s)  LR: 2.688e-04  Data: 0.010 (0.024)
Train: 395 [ 200/1251 ( 16%)]  Loss: 3.356 (3.27)  Time: 0.786s, 1303.14/s  (0.799s, 1281.35/s)  LR: 2.688e-04  Data: 0.009 (0.021)
Train: 395 [ 250/1251 ( 20%)]  Loss: 3.041 (3.23)  Time: 0.818s, 1252.27/s  (0.800s, 1279.48/s)  LR: 2.688e-04  Data: 0.011 (0.019)
Train: 395 [ 300/1251 ( 24%)]  Loss: 3.239 (3.23)  Time: 0.772s, 1325.79/s  (0.798s, 1283.89/s)  LR: 2.688e-04  Data: 0.010 (0.017)
Train: 395 [ 350/1251 ( 28%)]  Loss: 3.140 (3.22)  Time: 0.775s, 1321.40/s  (0.794s, 1288.93/s)  LR: 2.688e-04  Data: 0.010 (0.016)
Train: 395 [ 400/1251 ( 32%)]  Loss: 3.127 (3.21)  Time: 0.772s, 1326.28/s  (0.793s, 1291.71/s)  LR: 2.688e-04  Data: 0.009 (0.015)
Train: 395 [ 450/1251 ( 36%)]  Loss: 3.068 (3.20)  Time: 0.773s, 1323.93/s  (0.791s, 1294.33/s)  LR: 2.688e-04  Data: 0.010 (0.015)
Train: 395 [ 500/1251 ( 40%)]  Loss: 3.204 (3.20)  Time: 0.831s, 1232.35/s  (0.792s, 1293.28/s)  LR: 2.688e-04  Data: 0.010 (0.014)
Train: 395 [ 550/1251 ( 44%)]  Loss: 3.380 (3.21)  Time: 0.772s, 1327.03/s  (0.791s, 1293.86/s)  LR: 2.688e-04  Data: 0.010 (0.014)
Train: 395 [ 600/1251 ( 48%)]  Loss: 3.348 (3.22)  Time: 0.781s, 1311.80/s  (0.791s, 1295.29/s)  LR: 2.688e-04  Data: 0.009 (0.013)
Train: 395 [ 650/1251 ( 52%)]  Loss: 3.372 (3.23)  Time: 0.773s, 1324.58/s  (0.789s, 1297.14/s)  LR: 2.688e-04  Data: 0.010 (0.013)
Train: 395 [ 700/1251 ( 56%)]  Loss: 3.373 (3.24)  Time: 0.806s, 1270.78/s  (0.789s, 1297.55/s)  LR: 2.688e-04  Data: 0.009 (0.013)
Train: 395 [ 750/1251 ( 60%)]  Loss: 3.498 (3.26)  Time: 0.771s, 1328.28/s  (0.788s, 1298.67/s)  LR: 2.688e-04  Data: 0.010 (0.013)
Train: 395 [ 800/1251 ( 64%)]  Loss: 3.820 (3.29)  Time: 0.818s, 1252.05/s  (0.788s, 1299.13/s)  LR: 2.688e-04  Data: 0.010 (0.012)
Train: 395 [ 850/1251 ( 68%)]  Loss: 3.283 (3.29)  Time: 0.784s, 1306.51/s  (0.788s, 1299.71/s)  LR: 2.688e-04  Data: 0.011 (0.012)
Train: 395 [ 900/1251 ( 72%)]  Loss: 3.228 (3.29)  Time: 0.782s, 1309.77/s  (0.787s, 1300.58/s)  LR: 2.688e-04  Data: 0.009 (0.012)
Train: 395 [ 950/1251 ( 76%)]  Loss: 3.511 (3.30)  Time: 0.773s, 1325.27/s  (0.787s, 1301.48/s)  LR: 2.688e-04  Data: 0.009 (0.012)
Train: 395 [1000/1251 ( 80%)]  Loss: 3.340 (3.30)  Time: 0.815s, 1256.55/s  (0.788s, 1299.14/s)  LR: 2.688e-04  Data: 0.011 (0.012)
Train: 395 [1050/1251 ( 84%)]  Loss: 3.345 (3.30)  Time: 0.848s, 1207.20/s  (0.788s, 1299.30/s)  LR: 2.688e-04  Data: 0.010 (0.012)
Train: 395 [1100/1251 ( 88%)]  Loss: 3.411 (3.31)  Time: 0.772s, 1326.03/s  (0.788s, 1300.08/s)  LR: 2.688e-04  Data: 0.010 (0.012)
Train: 395 [1150/1251 ( 92%)]  Loss: 3.298 (3.31)  Time: 0.772s, 1326.21/s  (0.787s, 1300.84/s)  LR: 2.688e-04  Data: 0.009 (0.012)
Train: 395 [1200/1251 ( 96%)]  Loss: 3.317 (3.31)  Time: 0.774s, 1323.23/s  (0.787s, 1301.16/s)  LR: 2.688e-04  Data: 0.010 (0.012)
Train: 395 [1250/1251 (100%)]  Loss: 3.220 (3.30)  Time: 0.761s, 1344.90/s  (0.787s, 1301.47/s)  LR: 2.688e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.519 (1.519)  Loss:  0.7471 (0.7471)  Acc@1: 90.5273 (90.5273)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.194 (0.573)  Loss:  0.8481 (1.2264)  Acc@1: 85.9670 (77.3800)  Acc@5: 97.0519 (94.0760)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-393.pth.tar', 77.73999997558593)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-389.pth.tar', 77.63200005126953)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-387.pth.tar', 77.58400005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-394.pth.tar', 77.50399995605468)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-377.pth.tar', 77.48399995605469)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-390.pth.tar', 77.46000013671875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-376.pth.tar', 77.451999921875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-391.pth.tar', 77.432000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-385.pth.tar', 77.41800000488281)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-395.pth.tar', 77.38000010986327)

Train: 396 [   0/1251 (  0%)]  Loss: 2.926 (2.93)  Time: 2.279s,  449.29/s  (2.279s,  449.29/s)  LR: 2.665e-04  Data: 1.552 (1.552)
Train: 396 [  50/1251 (  4%)]  Loss: 3.291 (3.11)  Time: 0.785s, 1304.40/s  (0.849s, 1206.51/s)  LR: 2.665e-04  Data: 0.011 (0.046)
Train: 396 [ 100/1251 (  8%)]  Loss: 3.324 (3.18)  Time: 0.787s, 1300.90/s  (0.823s, 1243.81/s)  LR: 2.665e-04  Data: 0.010 (0.028)
Train: 396 [ 150/1251 ( 12%)]  Loss: 3.145 (3.17)  Time: 0.790s, 1296.66/s  (0.810s, 1264.07/s)  LR: 2.665e-04  Data: 0.011 (0.022)
Train: 396 [ 200/1251 ( 16%)]  Loss: 3.165 (3.17)  Time: 0.789s, 1298.63/s  (0.804s, 1273.73/s)  LR: 2.665e-04  Data: 0.010 (0.019)
Train: 396 [ 250/1251 ( 20%)]  Loss: 3.493 (3.22)  Time: 0.776s, 1318.95/s  (0.800s, 1280.24/s)  LR: 2.665e-04  Data: 0.010 (0.017)
Train: 396 [ 300/1251 ( 24%)]  Loss: 3.231 (3.22)  Time: 0.795s, 1288.42/s  (0.802s, 1276.82/s)  LR: 2.665e-04  Data: 0.010 (0.016)
Train: 396 [ 350/1251 ( 28%)]  Loss: 3.249 (3.23)  Time: 0.818s, 1251.67/s  (0.800s, 1280.21/s)  LR: 2.665e-04  Data: 0.010 (0.015)
Train: 396 [ 400/1251 ( 32%)]  Loss: 3.329 (3.24)  Time: 0.817s, 1253.94/s  (0.798s, 1282.51/s)  LR: 2.665e-04  Data: 0.010 (0.015)
Train: 396 [ 450/1251 ( 36%)]  Loss: 3.429 (3.26)  Time: 0.780s, 1313.64/s  (0.797s, 1284.86/s)  LR: 2.665e-04  Data: 0.010 (0.014)
Train: 396 [ 500/1251 ( 40%)]  Loss: 3.072 (3.24)  Time: 0.776s, 1320.41/s  (0.796s, 1286.69/s)  LR: 2.665e-04  Data: 0.010 (0.014)
Train: 396 [ 550/1251 ( 44%)]  Loss: 3.422 (3.26)  Time: 0.775s, 1322.02/s  (0.796s, 1287.22/s)  LR: 2.665e-04  Data: 0.010 (0.013)
Train: 396 [ 600/1251 ( 48%)]  Loss: 3.142 (3.25)  Time: 0.857s, 1195.27/s  (0.795s, 1287.42/s)  LR: 2.665e-04  Data: 0.009 (0.013)
Train: 396 [ 650/1251 ( 52%)]  Loss: 3.871 (3.29)  Time: 0.773s, 1325.50/s  (0.794s, 1288.98/s)  LR: 2.665e-04  Data: 0.010 (0.013)
Train: 396 [ 700/1251 ( 56%)]  Loss: 3.105 (3.28)  Time: 0.774s, 1322.56/s  (0.793s, 1291.01/s)  LR: 2.665e-04  Data: 0.010 (0.013)
Train: 396 [ 750/1251 ( 60%)]  Loss: 3.344 (3.28)  Time: 0.774s, 1323.65/s  (0.792s, 1292.47/s)  LR: 2.665e-04  Data: 0.010 (0.012)
Train: 396 [ 800/1251 ( 64%)]  Loss: 3.039 (3.27)  Time: 0.788s, 1299.39/s  (0.792s, 1293.60/s)  LR: 2.665e-04  Data: 0.009 (0.012)
Train: 396 [ 850/1251 ( 68%)]  Loss: 3.528 (3.28)  Time: 0.779s, 1313.95/s  (0.791s, 1294.42/s)  LR: 2.665e-04  Data: 0.010 (0.012)
Train: 396 [ 900/1251 ( 72%)]  Loss: 3.362 (3.29)  Time: 0.807s, 1268.26/s  (0.791s, 1294.69/s)  LR: 2.665e-04  Data: 0.009 (0.012)
Train: 396 [ 950/1251 ( 76%)]  Loss: 3.181 (3.28)  Time: 0.774s, 1323.77/s  (0.791s, 1295.27/s)  LR: 2.665e-04  Data: 0.010 (0.012)
Train: 396 [1000/1251 ( 80%)]  Loss: 3.289 (3.28)  Time: 0.774s, 1323.02/s  (0.791s, 1295.10/s)  LR: 2.665e-04  Data: 0.009 (0.012)
Train: 396 [1050/1251 ( 84%)]  Loss: 3.388 (3.29)  Time: 0.775s, 1321.60/s  (0.790s, 1295.75/s)  LR: 2.665e-04  Data: 0.010 (0.012)
Train: 396 [1100/1251 ( 88%)]  Loss: 3.279 (3.29)  Time: 0.776s, 1319.86/s  (0.790s, 1296.58/s)  LR: 2.665e-04  Data: 0.009 (0.012)
Train: 396 [1150/1251 ( 92%)]  Loss: 3.245 (3.29)  Time: 0.829s, 1235.21/s  (0.790s, 1296.29/s)  LR: 2.665e-04  Data: 0.014 (0.012)
Train: 396 [1200/1251 ( 96%)]  Loss: 3.314 (3.29)  Time: 0.773s, 1325.11/s  (0.790s, 1296.51/s)  LR: 2.665e-04  Data: 0.010 (0.011)
Train: 396 [1250/1251 (100%)]  Loss: 3.401 (3.29)  Time: 0.761s, 1346.24/s  (0.789s, 1297.30/s)  LR: 2.665e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.539 (1.539)  Loss:  0.7139 (0.7139)  Acc@1: 90.6250 (90.6250)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.194 (0.557)  Loss:  0.7588 (1.1631)  Acc@1: 85.4953 (77.9320)  Acc@5: 97.0519 (94.1360)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-396.pth.tar', 77.93199990478516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-393.pth.tar', 77.73999997558593)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-389.pth.tar', 77.63200005126953)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-387.pth.tar', 77.58400005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-394.pth.tar', 77.50399995605468)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-377.pth.tar', 77.48399995605469)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-390.pth.tar', 77.46000013671875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-376.pth.tar', 77.451999921875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-391.pth.tar', 77.432000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-385.pth.tar', 77.41800000488281)

Train: 397 [   0/1251 (  0%)]  Loss: 3.339 (3.34)  Time: 2.238s,  457.47/s  (2.238s,  457.47/s)  LR: 2.643e-04  Data: 1.508 (1.508)
Train: 397 [  50/1251 (  4%)]  Loss: 3.498 (3.42)  Time: 0.772s, 1325.67/s  (0.820s, 1249.27/s)  LR: 2.643e-04  Data: 0.010 (0.045)
Train: 397 [ 100/1251 (  8%)]  Loss: 3.483 (3.44)  Time: 0.773s, 1324.80/s  (0.802s, 1276.89/s)  LR: 2.643e-04  Data: 0.010 (0.028)
Train: 397 [ 150/1251 ( 12%)]  Loss: 3.327 (3.41)  Time: 0.769s, 1330.79/s  (0.793s, 1290.73/s)  LR: 2.643e-04  Data: 0.010 (0.022)
Train: 397 [ 200/1251 ( 16%)]  Loss: 3.154 (3.36)  Time: 0.772s, 1326.48/s  (0.796s, 1286.71/s)  LR: 2.643e-04  Data: 0.010 (0.019)
Train: 397 [ 250/1251 ( 20%)]  Loss: 3.035 (3.31)  Time: 0.779s, 1314.97/s  (0.792s, 1293.01/s)  LR: 2.643e-04  Data: 0.010 (0.017)
Train: 397 [ 300/1251 ( 24%)]  Loss: 3.215 (3.29)  Time: 0.773s, 1325.24/s  (0.790s, 1295.72/s)  LR: 2.643e-04  Data: 0.010 (0.016)
Train: 397 [ 350/1251 ( 28%)]  Loss: 3.344 (3.30)  Time: 0.773s, 1324.69/s  (0.792s, 1292.13/s)  LR: 2.643e-04  Data: 0.010 (0.015)
Train: 397 [ 400/1251 ( 32%)]  Loss: 3.508 (3.32)  Time: 0.808s, 1267.70/s  (0.794s, 1289.55/s)  LR: 2.643e-04  Data: 0.014 (0.014)
Train: 397 [ 450/1251 ( 36%)]  Loss: 3.289 (3.32)  Time: 0.772s, 1326.04/s  (0.794s, 1289.39/s)  LR: 2.643e-04  Data: 0.009 (0.014)
Train: 397 [ 500/1251 ( 40%)]  Loss: 2.964 (3.29)  Time: 0.795s, 1287.97/s  (0.793s, 1291.91/s)  LR: 2.643e-04  Data: 0.010 (0.014)
Train: 397 [ 550/1251 ( 44%)]  Loss: 3.656 (3.32)  Time: 0.775s, 1321.70/s  (0.793s, 1291.05/s)  LR: 2.643e-04  Data: 0.011 (0.013)
Train: 397 [ 600/1251 ( 48%)]  Loss: 3.258 (3.31)  Time: 0.777s, 1317.38/s  (0.792s, 1293.38/s)  LR: 2.643e-04  Data: 0.012 (0.013)
Train: 397 [ 650/1251 ( 52%)]  Loss: 3.297 (3.31)  Time: 0.771s, 1328.06/s  (0.791s, 1295.11/s)  LR: 2.643e-04  Data: 0.010 (0.013)
Train: 397 [ 700/1251 ( 56%)]  Loss: 3.508 (3.32)  Time: 0.774s, 1323.74/s  (0.790s, 1295.49/s)  LR: 2.643e-04  Data: 0.009 (0.012)
Train: 397 [ 750/1251 ( 60%)]  Loss: 3.115 (3.31)  Time: 0.778s, 1316.17/s  (0.790s, 1296.41/s)  LR: 2.643e-04  Data: 0.010 (0.012)
Train: 397 [ 800/1251 ( 64%)]  Loss: 3.330 (3.31)  Time: 0.774s, 1323.31/s  (0.789s, 1297.18/s)  LR: 2.643e-04  Data: 0.010 (0.012)
Train: 397 [ 850/1251 ( 68%)]  Loss: 3.511 (3.32)  Time: 0.773s, 1324.97/s  (0.789s, 1298.34/s)  LR: 2.643e-04  Data: 0.010 (0.012)
Train: 397 [ 900/1251 ( 72%)]  Loss: 3.537 (3.34)  Time: 0.772s, 1326.00/s  (0.789s, 1298.62/s)  LR: 2.643e-04  Data: 0.010 (0.012)
Train: 397 [ 950/1251 ( 76%)]  Loss: 3.252 (3.33)  Time: 0.773s, 1324.01/s  (0.788s, 1299.72/s)  LR: 2.643e-04  Data: 0.009 (0.012)
Train: 397 [1000/1251 ( 80%)]  Loss: 3.229 (3.33)  Time: 0.775s, 1322.10/s  (0.787s, 1300.61/s)  LR: 2.643e-04  Data: 0.009 (0.012)
Train: 397 [1050/1251 ( 84%)]  Loss: 3.660 (3.34)  Time: 0.772s, 1326.33/s  (0.787s, 1301.20/s)  LR: 2.643e-04  Data: 0.010 (0.012)
Train: 397 [1100/1251 ( 88%)]  Loss: 3.295 (3.34)  Time: 0.774s, 1323.80/s  (0.787s, 1301.42/s)  LR: 2.643e-04  Data: 0.009 (0.012)
Train: 397 [1150/1251 ( 92%)]  Loss: 3.332 (3.34)  Time: 0.768s, 1334.13/s  (0.787s, 1301.76/s)  LR: 2.643e-04  Data: 0.010 (0.011)
Train: 397 [1200/1251 ( 96%)]  Loss: 3.268 (3.34)  Time: 0.774s, 1323.39/s  (0.787s, 1301.66/s)  LR: 2.643e-04  Data: 0.009 (0.011)
Train: 397 [1250/1251 (100%)]  Loss: 3.496 (3.34)  Time: 0.761s, 1345.55/s  (0.786s, 1302.11/s)  LR: 2.643e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.626 (1.626)  Loss:  0.7192 (0.7192)  Acc@1: 90.3320 (90.3320)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.194 (0.557)  Loss:  0.8198 (1.2036)  Acc@1: 85.7311 (77.4600)  Acc@5: 97.0519 (94.0160)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-396.pth.tar', 77.93199990478516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-393.pth.tar', 77.73999997558593)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-389.pth.tar', 77.63200005126953)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-387.pth.tar', 77.58400005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-394.pth.tar', 77.50399995605468)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-377.pth.tar', 77.48399995605469)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-390.pth.tar', 77.46000013671875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-397.pth.tar', 77.46000000732423)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-376.pth.tar', 77.451999921875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-391.pth.tar', 77.432000078125)

Train: 398 [   0/1251 (  0%)]  Loss: 3.337 (3.34)  Time: 2.266s,  451.95/s  (2.266s,  451.95/s)  LR: 2.620e-04  Data: 1.536 (1.536)
Train: 398 [  50/1251 (  4%)]  Loss: 2.988 (3.16)  Time: 0.772s, 1326.60/s  (0.813s, 1259.09/s)  LR: 2.620e-04  Data: 0.010 (0.045)
Train: 398 [ 100/1251 (  8%)]  Loss: 3.439 (3.25)  Time: 0.773s, 1325.19/s  (0.797s, 1284.76/s)  LR: 2.620e-04  Data: 0.009 (0.028)
Train: 398 [ 150/1251 ( 12%)]  Loss: 3.108 (3.22)  Time: 0.785s, 1304.06/s  (0.791s, 1295.12/s)  LR: 2.620e-04  Data: 0.010 (0.022)
Train: 398 [ 200/1251 ( 16%)]  Loss: 3.183 (3.21)  Time: 0.772s, 1325.80/s  (0.788s, 1298.83/s)  LR: 2.620e-04  Data: 0.010 (0.019)
Train: 398 [ 250/1251 ( 20%)]  Loss: 3.304 (3.23)  Time: 0.772s, 1326.72/s  (0.787s, 1301.40/s)  LR: 2.620e-04  Data: 0.009 (0.017)
Train: 398 [ 300/1251 ( 24%)]  Loss: 3.645 (3.29)  Time: 0.771s, 1328.87/s  (0.785s, 1304.59/s)  LR: 2.620e-04  Data: 0.010 (0.016)
Train: 398 [ 350/1251 ( 28%)]  Loss: 3.358 (3.30)  Time: 0.777s, 1318.23/s  (0.784s, 1306.43/s)  LR: 2.620e-04  Data: 0.010 (0.015)
Train: 398 [ 400/1251 ( 32%)]  Loss: 3.299 (3.30)  Time: 0.818s, 1251.64/s  (0.787s, 1301.70/s)  LR: 2.620e-04  Data: 0.011 (0.014)
Train: 398 [ 450/1251 ( 36%)]  Loss: 3.039 (3.27)  Time: 0.857s, 1194.71/s  (0.787s, 1300.44/s)  LR: 2.620e-04  Data: 0.011 (0.014)
Train: 398 [ 500/1251 ( 40%)]  Loss: 3.584 (3.30)  Time: 0.772s, 1326.62/s  (0.787s, 1301.48/s)  LR: 2.620e-04  Data: 0.010 (0.014)
Train: 398 [ 550/1251 ( 44%)]  Loss: 2.740 (3.25)  Time: 0.825s, 1241.83/s  (0.789s, 1298.29/s)  LR: 2.620e-04  Data: 0.014 (0.013)
Train: 398 [ 600/1251 ( 48%)]  Loss: 3.211 (3.25)  Time: 0.797s, 1285.20/s  (0.789s, 1297.23/s)  LR: 2.620e-04  Data: 0.010 (0.013)
Train: 398 [ 650/1251 ( 52%)]  Loss: 3.481 (3.27)  Time: 0.781s, 1311.17/s  (0.789s, 1297.98/s)  LR: 2.620e-04  Data: 0.010 (0.013)
Train: 398 [ 700/1251 ( 56%)]  Loss: 3.671 (3.29)  Time: 0.776s, 1320.24/s  (0.788s, 1298.77/s)  LR: 2.620e-04  Data: 0.010 (0.013)
Train: 398 [ 750/1251 ( 60%)]  Loss: 3.412 (3.30)  Time: 0.816s, 1254.84/s  (0.788s, 1299.74/s)  LR: 2.620e-04  Data: 0.013 (0.012)
Train: 398 [ 800/1251 ( 64%)]  Loss: 2.992 (3.28)  Time: 0.774s, 1323.04/s  (0.787s, 1300.58/s)  LR: 2.620e-04  Data: 0.010 (0.012)
Train: 398 [ 850/1251 ( 68%)]  Loss: 3.507 (3.29)  Time: 0.828s, 1237.30/s  (0.788s, 1299.19/s)  LR: 2.620e-04  Data: 0.010 (0.012)
Train: 398 [ 900/1251 ( 72%)]  Loss: 3.643 (3.31)  Time: 0.780s, 1312.19/s  (0.788s, 1299.77/s)  LR: 2.620e-04  Data: 0.010 (0.012)
Train: 398 [ 950/1251 ( 76%)]  Loss: 3.174 (3.31)  Time: 0.834s, 1228.04/s  (0.788s, 1298.77/s)  LR: 2.620e-04  Data: 0.010 (0.012)
Train: 398 [1000/1251 ( 80%)]  Loss: 3.424 (3.31)  Time: 0.775s, 1321.82/s  (0.788s, 1299.24/s)  LR: 2.620e-04  Data: 0.010 (0.012)
Train: 398 [1050/1251 ( 84%)]  Loss: 3.470 (3.32)  Time: 0.775s, 1320.79/s  (0.788s, 1300.20/s)  LR: 2.620e-04  Data: 0.010 (0.012)
Train: 398 [1100/1251 ( 88%)]  Loss: 3.077 (3.31)  Time: 0.781s, 1310.47/s  (0.787s, 1300.86/s)  LR: 2.620e-04  Data: 0.013 (0.012)
Train: 398 [1150/1251 ( 92%)]  Loss: 3.022 (3.30)  Time: 0.775s, 1320.69/s  (0.787s, 1301.31/s)  LR: 2.620e-04  Data: 0.010 (0.012)
Train: 398 [1200/1251 ( 96%)]  Loss: 2.958 (3.28)  Time: 0.774s, 1323.58/s  (0.787s, 1301.00/s)  LR: 2.620e-04  Data: 0.009 (0.012)
Train: 398 [1250/1251 (100%)]  Loss: 3.606 (3.30)  Time: 0.760s, 1346.53/s  (0.787s, 1301.35/s)  LR: 2.620e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.508 (1.508)  Loss:  0.7090 (0.7090)  Acc@1: 89.7461 (89.7461)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.194 (0.558)  Loss:  0.7881 (1.1680)  Acc@1: 86.2028 (77.5440)  Acc@5: 97.1698 (94.0460)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-396.pth.tar', 77.93199990478516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-393.pth.tar', 77.73999997558593)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-389.pth.tar', 77.63200005126953)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-387.pth.tar', 77.58400005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-398.pth.tar', 77.54400008300782)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-394.pth.tar', 77.50399995605468)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-377.pth.tar', 77.48399995605469)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-390.pth.tar', 77.46000013671875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-397.pth.tar', 77.46000000732423)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-376.pth.tar', 77.451999921875)

Train: 399 [   0/1251 (  0%)]  Loss: 3.666 (3.67)  Time: 2.307s,  443.84/s  (2.307s,  443.84/s)  LR: 2.597e-04  Data: 1.563 (1.563)
Train: 399 [  50/1251 (  4%)]  Loss: 2.761 (3.21)  Time: 0.771s, 1328.10/s  (0.816s, 1254.64/s)  LR: 2.597e-04  Data: 0.009 (0.041)
Train: 399 [ 100/1251 (  8%)]  Loss: 3.649 (3.36)  Time: 0.773s, 1325.27/s  (0.799s, 1281.71/s)  LR: 2.597e-04  Data: 0.009 (0.026)
Train: 399 [ 150/1251 ( 12%)]  Loss: 3.517 (3.40)  Time: 0.851s, 1203.11/s  (0.799s, 1282.09/s)  LR: 2.597e-04  Data: 0.009 (0.021)
Train: 399 [ 200/1251 ( 16%)]  Loss: 3.188 (3.36)  Time: 0.774s, 1323.50/s  (0.796s, 1286.09/s)  LR: 2.597e-04  Data: 0.010 (0.018)
Train: 399 [ 250/1251 ( 20%)]  Loss: 2.913 (3.28)  Time: 0.816s, 1254.66/s  (0.798s, 1283.68/s)  LR: 2.597e-04  Data: 0.011 (0.017)
Train: 399 [ 300/1251 ( 24%)]  Loss: 3.397 (3.30)  Time: 0.771s, 1327.60/s  (0.796s, 1286.73/s)  LR: 2.597e-04  Data: 0.010 (0.015)
Train: 399 [ 350/1251 ( 28%)]  Loss: 3.389 (3.31)  Time: 0.774s, 1322.38/s  (0.794s, 1289.90/s)  LR: 2.597e-04  Data: 0.011 (0.015)
Train: 399 [ 400/1251 ( 32%)]  Loss: 3.076 (3.28)  Time: 0.822s, 1245.46/s  (0.796s, 1286.72/s)  LR: 2.597e-04  Data: 0.014 (0.014)
Train: 399 [ 450/1251 ( 36%)]  Loss: 3.100 (3.27)  Time: 0.777s, 1318.41/s  (0.794s, 1289.28/s)  LR: 2.597e-04  Data: 0.010 (0.014)
Train: 399 [ 500/1251 ( 40%)]  Loss: 3.106 (3.25)  Time: 0.773s, 1325.44/s  (0.793s, 1291.87/s)  LR: 2.597e-04  Data: 0.009 (0.013)
Train: 399 [ 550/1251 ( 44%)]  Loss: 3.280 (3.25)  Time: 0.771s, 1327.91/s  (0.791s, 1293.76/s)  LR: 2.597e-04  Data: 0.010 (0.013)
Train: 399 [ 600/1251 ( 48%)]  Loss: 3.093 (3.24)  Time: 0.775s, 1320.57/s  (0.791s, 1294.20/s)  LR: 2.597e-04  Data: 0.010 (0.013)
Train: 399 [ 650/1251 ( 52%)]  Loss: 3.173 (3.24)  Time: 0.771s, 1328.59/s  (0.790s, 1295.72/s)  LR: 2.597e-04  Data: 0.010 (0.012)
Train: 399 [ 700/1251 ( 56%)]  Loss: 3.124 (3.23)  Time: 0.771s, 1328.61/s  (0.790s, 1295.93/s)  LR: 2.597e-04  Data: 0.009 (0.012)
Train: 399 [ 750/1251 ( 60%)]  Loss: 3.531 (3.25)  Time: 0.772s, 1326.32/s  (0.790s, 1296.20/s)  LR: 2.597e-04  Data: 0.010 (0.012)
Train: 399 [ 800/1251 ( 64%)]  Loss: 3.155 (3.24)  Time: 0.774s, 1322.78/s  (0.789s, 1297.31/s)  LR: 2.597e-04  Data: 0.010 (0.012)
Train: 399 [ 850/1251 ( 68%)]  Loss: 3.637 (3.26)  Time: 0.772s, 1326.71/s  (0.789s, 1298.25/s)  LR: 2.597e-04  Data: 0.010 (0.012)
Train: 399 [ 900/1251 ( 72%)]  Loss: 3.546 (3.28)  Time: 0.783s, 1308.12/s  (0.788s, 1299.25/s)  LR: 2.597e-04  Data: 0.009 (0.012)
Train: 399 [ 950/1251 ( 76%)]  Loss: 3.296 (3.28)  Time: 0.773s, 1324.85/s  (0.788s, 1300.30/s)  LR: 2.597e-04  Data: 0.009 (0.012)
Train: 399 [1000/1251 ( 80%)]  Loss: 2.809 (3.26)  Time: 0.772s, 1325.63/s  (0.787s, 1301.12/s)  LR: 2.597e-04  Data: 0.009 (0.011)
Train: 399 [1050/1251 ( 84%)]  Loss: 3.270 (3.26)  Time: 0.853s, 1200.40/s  (0.787s, 1300.78/s)  LR: 2.597e-04  Data: 0.010 (0.011)
Train: 399 [1100/1251 ( 88%)]  Loss: 3.261 (3.26)  Time: 0.776s, 1318.91/s  (0.787s, 1301.44/s)  LR: 2.597e-04  Data: 0.010 (0.011)
Train: 399 [1150/1251 ( 92%)]  Loss: 3.475 (3.27)  Time: 0.777s, 1317.53/s  (0.787s, 1301.96/s)  LR: 2.597e-04  Data: 0.009 (0.011)
Train: 399 [1200/1251 ( 96%)]  Loss: 2.853 (3.25)  Time: 0.776s, 1319.59/s  (0.786s, 1302.40/s)  LR: 2.597e-04  Data: 0.010 (0.011)
Train: 399 [1250/1251 (100%)]  Loss: 2.975 (3.24)  Time: 0.764s, 1340.02/s  (0.786s, 1302.75/s)  LR: 2.597e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.574 (1.574)  Loss:  0.7822 (0.7822)  Acc@1: 90.2344 (90.2344)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.194 (0.567)  Loss:  0.8774 (1.2695)  Acc@1: 87.0283 (77.5280)  Acc@5: 97.5236 (94.0860)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-396.pth.tar', 77.93199990478516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-393.pth.tar', 77.73999997558593)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-389.pth.tar', 77.63200005126953)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-387.pth.tar', 77.58400005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-398.pth.tar', 77.54400008300782)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-399.pth.tar', 77.5279999243164)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-394.pth.tar', 77.50399995605468)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-377.pth.tar', 77.48399995605469)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-390.pth.tar', 77.46000013671875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-397.pth.tar', 77.46000000732423)

Train: 400 [   0/1251 (  0%)]  Loss: 3.106 (3.11)  Time: 2.301s,  444.98/s  (2.301s,  444.98/s)  LR: 2.575e-04  Data: 1.565 (1.565)
Train: 400 [  50/1251 (  4%)]  Loss: 3.166 (3.14)  Time: 0.773s, 1324.14/s  (0.816s, 1254.63/s)  LR: 2.575e-04  Data: 0.009 (0.047)
Train: 400 [ 100/1251 (  8%)]  Loss: 3.453 (3.24)  Time: 0.815s, 1256.57/s  (0.809s, 1266.54/s)  LR: 2.575e-04  Data: 0.011 (0.029)
Train: 400 [ 150/1251 ( 12%)]  Loss: 3.392 (3.28)  Time: 0.787s, 1301.14/s  (0.813s, 1260.03/s)  LR: 2.575e-04  Data: 0.010 (0.023)
Train: 400 [ 200/1251 ( 16%)]  Loss: 3.037 (3.23)  Time: 0.773s, 1324.78/s  (0.804s, 1273.26/s)  LR: 2.575e-04  Data: 0.010 (0.020)
Train: 400 [ 250/1251 ( 20%)]  Loss: 3.488 (3.27)  Time: 0.773s, 1324.33/s  (0.799s, 1281.60/s)  LR: 2.575e-04  Data: 0.010 (0.018)
Train: 400 [ 300/1251 ( 24%)]  Loss: 3.342 (3.28)  Time: 0.781s, 1310.67/s  (0.798s, 1282.66/s)  LR: 2.575e-04  Data: 0.010 (0.016)
Train: 400 [ 350/1251 ( 28%)]  Loss: 3.414 (3.30)  Time: 0.783s, 1307.49/s  (0.796s, 1285.73/s)  LR: 2.575e-04  Data: 0.009 (0.015)
Train: 400 [ 400/1251 ( 32%)]  Loss: 3.141 (3.28)  Time: 0.774s, 1323.62/s  (0.794s, 1289.49/s)  LR: 2.575e-04  Data: 0.011 (0.015)
Train: 400 [ 450/1251 ( 36%)]  Loss: 2.712 (3.23)  Time: 0.780s, 1313.22/s  (0.794s, 1289.07/s)  LR: 2.575e-04  Data: 0.012 (0.014)
Train: 400 [ 500/1251 ( 40%)]  Loss: 3.233 (3.23)  Time: 0.823s, 1244.22/s  (0.797s, 1284.86/s)  LR: 2.575e-04  Data: 0.010 (0.014)
Train: 400 [ 550/1251 ( 44%)]  Loss: 3.276 (3.23)  Time: 0.822s, 1245.37/s  (0.796s, 1286.52/s)  LR: 2.575e-04  Data: 0.009 (0.014)
Train: 400 [ 600/1251 ( 48%)]  Loss: 3.448 (3.25)  Time: 0.774s, 1322.62/s  (0.795s, 1288.68/s)  LR: 2.575e-04  Data: 0.010 (0.013)
Train: 400 [ 650/1251 ( 52%)]  Loss: 3.091 (3.24)  Time: 0.772s, 1326.48/s  (0.794s, 1290.47/s)  LR: 2.575e-04  Data: 0.010 (0.013)
Train: 400 [ 700/1251 ( 56%)]  Loss: 3.392 (3.25)  Time: 0.777s, 1317.91/s  (0.793s, 1291.22/s)  LR: 2.575e-04  Data: 0.010 (0.013)
Train: 400 [ 750/1251 ( 60%)]  Loss: 2.997 (3.23)  Time: 0.774s, 1323.50/s  (0.792s, 1292.85/s)  LR: 2.575e-04  Data: 0.010 (0.013)
Train: 400 [ 800/1251 ( 64%)]  Loss: 3.299 (3.23)  Time: 0.870s, 1177.33/s  (0.792s, 1292.15/s)  LR: 2.575e-04  Data: 0.014 (0.013)
Train: 400 [ 850/1251 ( 68%)]  Loss: 3.405 (3.24)  Time: 0.774s, 1322.70/s  (0.792s, 1292.34/s)  LR: 2.575e-04  Data: 0.010 (0.012)
Train: 400 [ 900/1251 ( 72%)]  Loss: 3.130 (3.24)  Time: 0.774s, 1322.31/s  (0.792s, 1293.49/s)  LR: 2.575e-04  Data: 0.009 (0.012)
Train: 400 [ 950/1251 ( 76%)]  Loss: 3.196 (3.24)  Time: 0.775s, 1321.46/s  (0.791s, 1293.92/s)  LR: 2.575e-04  Data: 0.010 (0.012)
Train: 400 [1000/1251 ( 80%)]  Loss: 3.480 (3.25)  Time: 0.773s, 1324.98/s  (0.791s, 1294.88/s)  LR: 2.575e-04  Data: 0.009 (0.012)
Train: 400 [1050/1251 ( 84%)]  Loss: 3.472 (3.26)  Time: 0.799s, 1280.81/s  (0.790s, 1295.62/s)  LR: 2.575e-04  Data: 0.009 (0.012)
Train: 400 [1100/1251 ( 88%)]  Loss: 3.371 (3.26)  Time: 0.785s, 1304.85/s  (0.790s, 1295.51/s)  LR: 2.575e-04  Data: 0.010 (0.012)
Train: 400 [1150/1251 ( 92%)]  Loss: 3.502 (3.27)  Time: 0.782s, 1309.04/s  (0.790s, 1296.45/s)  LR: 2.575e-04  Data: 0.009 (0.012)
Train: 400 [1200/1251 ( 96%)]  Loss: 3.148 (3.27)  Time: 0.790s, 1295.75/s  (0.790s, 1296.32/s)  LR: 2.575e-04  Data: 0.010 (0.012)
Train: 400 [1250/1251 (100%)]  Loss: 2.942 (3.26)  Time: 0.758s, 1350.39/s  (0.790s, 1296.93/s)  LR: 2.575e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.517 (1.517)  Loss:  0.6743 (0.6743)  Acc@1: 91.5039 (91.5039)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.194 (0.558)  Loss:  0.7139 (1.1457)  Acc@1: 86.9104 (77.8900)  Acc@5: 97.4057 (94.2160)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-396.pth.tar', 77.93199990478516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-400.pth.tar', 77.89000000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-393.pth.tar', 77.73999997558593)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-389.pth.tar', 77.63200005126953)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-387.pth.tar', 77.58400005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-398.pth.tar', 77.54400008300782)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-399.pth.tar', 77.5279999243164)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-394.pth.tar', 77.50399995605468)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-377.pth.tar', 77.48399995605469)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-390.pth.tar', 77.46000013671875)

Train: 401 [   0/1251 (  0%)]  Loss: 2.926 (2.93)  Time: 2.284s,  448.36/s  (2.284s,  448.36/s)  LR: 2.553e-04  Data: 1.557 (1.557)
Train: 401 [  50/1251 (  4%)]  Loss: 3.039 (2.98)  Time: 0.773s, 1325.15/s  (0.819s, 1250.16/s)  LR: 2.553e-04  Data: 0.010 (0.046)
Train: 401 [ 100/1251 (  8%)]  Loss: 3.051 (3.01)  Time: 0.780s, 1313.08/s  (0.800s, 1279.81/s)  LR: 2.553e-04  Data: 0.010 (0.028)
Train: 401 [ 150/1251 ( 12%)]  Loss: 3.324 (3.09)  Time: 0.787s, 1301.32/s  (0.797s, 1285.00/s)  LR: 2.553e-04  Data: 0.009 (0.022)
Train: 401 [ 200/1251 ( 16%)]  Loss: 3.097 (3.09)  Time: 0.779s, 1314.04/s  (0.793s, 1290.90/s)  LR: 2.553e-04  Data: 0.010 (0.019)
Train: 401 [ 250/1251 ( 20%)]  Loss: 3.056 (3.08)  Time: 0.814s, 1258.17/s  (0.793s, 1290.55/s)  LR: 2.553e-04  Data: 0.010 (0.017)
Train: 401 [ 300/1251 ( 24%)]  Loss: 3.578 (3.15)  Time: 0.851s, 1203.43/s  (0.793s, 1290.86/s)  LR: 2.553e-04  Data: 0.010 (0.016)
Train: 401 [ 350/1251 ( 28%)]  Loss: 3.168 (3.15)  Time: 0.797s, 1285.07/s  (0.792s, 1293.13/s)  LR: 2.553e-04  Data: 0.018 (0.015)
Train: 401 [ 400/1251 ( 32%)]  Loss: 3.694 (3.21)  Time: 0.772s, 1326.21/s  (0.791s, 1294.81/s)  LR: 2.553e-04  Data: 0.010 (0.015)
Train: 401 [ 450/1251 ( 36%)]  Loss: 3.763 (3.27)  Time: 0.811s, 1262.30/s  (0.792s, 1293.57/s)  LR: 2.553e-04  Data: 0.010 (0.014)
Train: 401 [ 500/1251 ( 40%)]  Loss: 3.148 (3.26)  Time: 0.775s, 1320.85/s  (0.791s, 1295.02/s)  LR: 2.553e-04  Data: 0.011 (0.014)
Train: 401 [ 550/1251 ( 44%)]  Loss: 3.383 (3.27)  Time: 0.793s, 1292.10/s  (0.790s, 1296.22/s)  LR: 2.553e-04  Data: 0.010 (0.013)
Train: 401 [ 600/1251 ( 48%)]  Loss: 3.209 (3.26)  Time: 0.779s, 1314.26/s  (0.789s, 1297.27/s)  LR: 2.553e-04  Data: 0.011 (0.013)
Train: 401 [ 650/1251 ( 52%)]  Loss: 3.414 (3.28)  Time: 0.776s, 1319.36/s  (0.789s, 1298.47/s)  LR: 2.553e-04  Data: 0.011 (0.013)
Train: 401 [ 700/1251 ( 56%)]  Loss: 3.706 (3.30)  Time: 0.781s, 1311.93/s  (0.788s, 1298.83/s)  LR: 2.553e-04  Data: 0.009 (0.013)
Train: 401 [ 750/1251 ( 60%)]  Loss: 3.349 (3.31)  Time: 0.772s, 1325.81/s  (0.788s, 1299.22/s)  LR: 2.553e-04  Data: 0.010 (0.013)
Train: 401 [ 800/1251 ( 64%)]  Loss: 3.490 (3.32)  Time: 0.815s, 1255.71/s  (0.789s, 1297.79/s)  LR: 2.553e-04  Data: 0.009 (0.012)
Train: 401 [ 850/1251 ( 68%)]  Loss: 3.518 (3.33)  Time: 0.773s, 1324.15/s  (0.789s, 1298.03/s)  LR: 2.553e-04  Data: 0.010 (0.012)
Train: 401 [ 900/1251 ( 72%)]  Loss: 3.094 (3.32)  Time: 0.773s, 1324.15/s  (0.789s, 1298.65/s)  LR: 2.553e-04  Data: 0.010 (0.012)
Train: 401 [ 950/1251 ( 76%)]  Loss: 2.973 (3.30)  Time: 0.778s, 1317.00/s  (0.788s, 1298.79/s)  LR: 2.553e-04  Data: 0.010 (0.012)
Train: 401 [1000/1251 ( 80%)]  Loss: 3.534 (3.31)  Time: 0.772s, 1326.44/s  (0.788s, 1299.07/s)  LR: 2.553e-04  Data: 0.010 (0.012)
Train: 401 [1050/1251 ( 84%)]  Loss: 3.095 (3.30)  Time: 0.774s, 1322.90/s  (0.788s, 1299.13/s)  LR: 2.553e-04  Data: 0.010 (0.012)
Train: 401 [1100/1251 ( 88%)]  Loss: 3.342 (3.30)  Time: 0.777s, 1317.26/s  (0.788s, 1299.46/s)  LR: 2.553e-04  Data: 0.009 (0.012)
Train: 401 [1150/1251 ( 92%)]  Loss: 3.215 (3.30)  Time: 0.772s, 1326.97/s  (0.788s, 1299.96/s)  LR: 2.553e-04  Data: 0.009 (0.012)
Train: 401 [1200/1251 ( 96%)]  Loss: 3.063 (3.29)  Time: 0.849s, 1205.67/s  (0.788s, 1298.87/s)  LR: 2.553e-04  Data: 0.010 (0.012)
Train: 401 [1250/1251 (100%)]  Loss: 2.890 (3.27)  Time: 0.759s, 1349.06/s  (0.789s, 1297.37/s)  LR: 2.553e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.665 (1.665)  Loss:  0.8979 (0.8979)  Acc@1: 90.4297 (90.4297)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.194 (0.558)  Loss:  0.9307 (1.3144)  Acc@1: 86.3208 (77.7340)  Acc@5: 97.2877 (94.1160)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-396.pth.tar', 77.93199990478516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-400.pth.tar', 77.89000000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-393.pth.tar', 77.73999997558593)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-401.pth.tar', 77.73400000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-389.pth.tar', 77.63200005126953)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-387.pth.tar', 77.58400005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-398.pth.tar', 77.54400008300782)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-399.pth.tar', 77.5279999243164)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-394.pth.tar', 77.50399995605468)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-377.pth.tar', 77.48399995605469)

Train: 402 [   0/1251 (  0%)]  Loss: 3.171 (3.17)  Time: 2.185s,  468.68/s  (2.185s,  468.68/s)  LR: 2.530e-04  Data: 1.468 (1.468)
Train: 402 [  50/1251 (  4%)]  Loss: 3.022 (3.10)  Time: 0.775s, 1321.79/s  (0.815s, 1255.98/s)  LR: 2.530e-04  Data: 0.012 (0.043)
Train: 402 [ 100/1251 (  8%)]  Loss: 3.135 (3.11)  Time: 0.782s, 1309.37/s  (0.799s, 1281.39/s)  LR: 2.530e-04  Data: 0.012 (0.026)
Train: 402 [ 150/1251 ( 12%)]  Loss: 3.137 (3.12)  Time: 0.773s, 1323.92/s  (0.792s, 1292.45/s)  LR: 2.530e-04  Data: 0.010 (0.021)
Train: 402 [ 200/1251 ( 16%)]  Loss: 2.973 (3.09)  Time: 0.774s, 1322.31/s  (0.789s, 1297.65/s)  LR: 2.530e-04  Data: 0.010 (0.018)
Train: 402 [ 250/1251 ( 20%)]  Loss: 2.820 (3.04)  Time: 0.780s, 1313.33/s  (0.788s, 1299.20/s)  LR: 2.530e-04  Data: 0.010 (0.016)
Train: 402 [ 300/1251 ( 24%)]  Loss: 3.568 (3.12)  Time: 0.783s, 1307.65/s  (0.787s, 1300.96/s)  LR: 2.530e-04  Data: 0.009 (0.015)
Train: 402 [ 350/1251 ( 28%)]  Loss: 3.561 (3.17)  Time: 0.796s, 1285.81/s  (0.787s, 1300.74/s)  LR: 2.530e-04  Data: 0.012 (0.015)
Train: 402 [ 400/1251 ( 32%)]  Loss: 3.008 (3.16)  Time: 0.780s, 1313.06/s  (0.786s, 1302.54/s)  LR: 2.530e-04  Data: 0.010 (0.014)
Train: 402 [ 450/1251 ( 36%)]  Loss: 3.304 (3.17)  Time: 0.818s, 1251.20/s  (0.786s, 1302.65/s)  LR: 2.530e-04  Data: 0.010 (0.014)
Train: 402 [ 500/1251 ( 40%)]  Loss: 3.244 (3.18)  Time: 0.816s, 1254.64/s  (0.786s, 1302.84/s)  LR: 2.530e-04  Data: 0.009 (0.013)
Train: 402 [ 550/1251 ( 44%)]  Loss: 3.258 (3.18)  Time: 0.773s, 1324.52/s  (0.786s, 1303.49/s)  LR: 2.530e-04  Data: 0.009 (0.013)
Train: 402 [ 600/1251 ( 48%)]  Loss: 3.331 (3.19)  Time: 0.773s, 1324.03/s  (0.786s, 1303.29/s)  LR: 2.530e-04  Data: 0.010 (0.013)
Train: 402 [ 650/1251 ( 52%)]  Loss: 3.403 (3.21)  Time: 0.774s, 1322.36/s  (0.786s, 1303.12/s)  LR: 2.530e-04  Data: 0.009 (0.012)
Train: 402 [ 700/1251 ( 56%)]  Loss: 3.189 (3.21)  Time: 0.773s, 1325.44/s  (0.785s, 1303.87/s)  LR: 2.530e-04  Data: 0.010 (0.012)
Train: 402 [ 750/1251 ( 60%)]  Loss: 3.342 (3.22)  Time: 0.832s, 1231.07/s  (0.786s, 1302.30/s)  LR: 2.530e-04  Data: 0.013 (0.012)
Train: 402 [ 800/1251 ( 64%)]  Loss: 3.418 (3.23)  Time: 0.787s, 1301.37/s  (0.786s, 1302.18/s)  LR: 2.530e-04  Data: 0.014 (0.012)
Train: 402 [ 850/1251 ( 68%)]  Loss: 3.052 (3.22)  Time: 0.773s, 1325.10/s  (0.786s, 1302.90/s)  LR: 2.530e-04  Data: 0.009 (0.012)
Train: 402 [ 900/1251 ( 72%)]  Loss: 3.129 (3.21)  Time: 0.773s, 1324.41/s  (0.786s, 1303.17/s)  LR: 2.530e-04  Data: 0.009 (0.012)
Train: 402 [ 950/1251 ( 76%)]  Loss: 3.492 (3.23)  Time: 0.774s, 1323.51/s  (0.785s, 1303.83/s)  LR: 2.530e-04  Data: 0.009 (0.011)
Train: 402 [1000/1251 ( 80%)]  Loss: 2.816 (3.21)  Time: 0.774s, 1323.74/s  (0.785s, 1304.71/s)  LR: 2.530e-04  Data: 0.010 (0.011)
Train: 402 [1050/1251 ( 84%)]  Loss: 3.112 (3.20)  Time: 0.772s, 1326.36/s  (0.785s, 1305.12/s)  LR: 2.530e-04  Data: 0.011 (0.011)
Train: 402 [1100/1251 ( 88%)]  Loss: 3.604 (3.22)  Time: 0.786s, 1302.88/s  (0.784s, 1305.65/s)  LR: 2.530e-04  Data: 0.011 (0.011)
Train: 402 [1150/1251 ( 92%)]  Loss: 3.261 (3.22)  Time: 0.774s, 1322.46/s  (0.784s, 1306.05/s)  LR: 2.530e-04  Data: 0.009 (0.011)
Train: 402 [1200/1251 ( 96%)]  Loss: 3.181 (3.22)  Time: 0.774s, 1322.20/s  (0.784s, 1306.21/s)  LR: 2.530e-04  Data: 0.009 (0.011)
Train: 402 [1250/1251 (100%)]  Loss: 3.355 (3.23)  Time: 0.763s, 1342.92/s  (0.784s, 1306.57/s)  LR: 2.530e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.524 (1.524)  Loss:  0.6162 (0.6162)  Acc@1: 89.8438 (89.8438)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.194 (0.558)  Loss:  0.6973 (1.0969)  Acc@1: 87.2642 (78.0240)  Acc@5: 97.6415 (94.2540)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-402.pth.tar', 78.02400002685548)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-396.pth.tar', 77.93199990478516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-400.pth.tar', 77.89000000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-393.pth.tar', 77.73999997558593)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-401.pth.tar', 77.73400000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-389.pth.tar', 77.63200005126953)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-387.pth.tar', 77.58400005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-398.pth.tar', 77.54400008300782)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-399.pth.tar', 77.5279999243164)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-394.pth.tar', 77.50399995605468)

Train: 403 [   0/1251 (  0%)]  Loss: 3.122 (3.12)  Time: 2.329s,  439.75/s  (2.329s,  439.75/s)  LR: 2.508e-04  Data: 1.601 (1.601)
Train: 403 [  50/1251 (  4%)]  Loss: 2.934 (3.03)  Time: 0.781s, 1310.50/s  (0.816s, 1255.66/s)  LR: 2.508e-04  Data: 0.010 (0.046)
Train: 403 [ 100/1251 (  8%)]  Loss: 2.973 (3.01)  Time: 0.774s, 1322.23/s  (0.800s, 1279.44/s)  LR: 2.508e-04  Data: 0.009 (0.028)
Train: 403 [ 150/1251 ( 12%)]  Loss: 2.999 (3.01)  Time: 0.772s, 1325.79/s  (0.793s, 1290.97/s)  LR: 2.508e-04  Data: 0.010 (0.022)
Train: 403 [ 200/1251 ( 16%)]  Loss: 3.318 (3.07)  Time: 0.773s, 1324.33/s  (0.789s, 1297.55/s)  LR: 2.508e-04  Data: 0.010 (0.019)
Train: 403 [ 250/1251 ( 20%)]  Loss: 3.200 (3.09)  Time: 0.773s, 1325.22/s  (0.787s, 1300.91/s)  LR: 2.508e-04  Data: 0.010 (0.017)
Train: 403 [ 300/1251 ( 24%)]  Loss: 3.248 (3.11)  Time: 0.787s, 1300.71/s  (0.786s, 1303.00/s)  LR: 2.508e-04  Data: 0.015 (0.016)
Train: 403 [ 350/1251 ( 28%)]  Loss: 3.196 (3.12)  Time: 0.781s, 1311.74/s  (0.785s, 1303.65/s)  LR: 2.508e-04  Data: 0.010 (0.015)
Train: 403 [ 400/1251 ( 32%)]  Loss: 3.175 (3.13)  Time: 0.772s, 1326.87/s  (0.785s, 1303.99/s)  LR: 2.508e-04  Data: 0.009 (0.014)
Train: 403 [ 450/1251 ( 36%)]  Loss: 3.294 (3.15)  Time: 0.814s, 1257.49/s  (0.786s, 1303.20/s)  LR: 2.508e-04  Data: 0.011 (0.014)
Train: 403 [ 500/1251 ( 40%)]  Loss: 2.964 (3.13)  Time: 0.774s, 1323.79/s  (0.786s, 1303.57/s)  LR: 2.508e-04  Data: 0.010 (0.013)
Train: 403 [ 550/1251 ( 44%)]  Loss: 3.442 (3.16)  Time: 0.772s, 1326.68/s  (0.785s, 1304.46/s)  LR: 2.508e-04  Data: 0.010 (0.013)
Train: 403 [ 600/1251 ( 48%)]  Loss: 3.076 (3.15)  Time: 0.773s, 1325.47/s  (0.785s, 1304.64/s)  LR: 2.508e-04  Data: 0.010 (0.013)
Train: 403 [ 650/1251 ( 52%)]  Loss: 3.416 (3.17)  Time: 0.781s, 1310.67/s  (0.785s, 1304.87/s)  LR: 2.508e-04  Data: 0.009 (0.013)
Train: 403 [ 700/1251 ( 56%)]  Loss: 3.262 (3.17)  Time: 0.772s, 1326.02/s  (0.784s, 1305.55/s)  LR: 2.508e-04  Data: 0.009 (0.012)
Train: 403 [ 750/1251 ( 60%)]  Loss: 2.925 (3.16)  Time: 0.772s, 1326.58/s  (0.784s, 1305.91/s)  LR: 2.508e-04  Data: 0.010 (0.012)
Train: 403 [ 800/1251 ( 64%)]  Loss: 3.358 (3.17)  Time: 0.776s, 1319.66/s  (0.784s, 1306.46/s)  LR: 2.508e-04  Data: 0.009 (0.012)
Train: 403 [ 850/1251 ( 68%)]  Loss: 3.472 (3.19)  Time: 0.780s, 1312.42/s  (0.784s, 1306.70/s)  LR: 2.508e-04  Data: 0.010 (0.012)
Train: 403 [ 900/1251 ( 72%)]  Loss: 3.058 (3.18)  Time: 0.772s, 1325.77/s  (0.783s, 1307.36/s)  LR: 2.508e-04  Data: 0.010 (0.012)
Train: 403 [ 950/1251 ( 76%)]  Loss: 3.173 (3.18)  Time: 0.772s, 1325.66/s  (0.783s, 1307.51/s)  LR: 2.508e-04  Data: 0.010 (0.012)
Train: 403 [1000/1251 ( 80%)]  Loss: 3.492 (3.20)  Time: 0.773s, 1324.36/s  (0.783s, 1308.06/s)  LR: 2.508e-04  Data: 0.010 (0.012)
Train: 403 [1050/1251 ( 84%)]  Loss: 2.989 (3.19)  Time: 0.847s, 1209.66/s  (0.783s, 1308.28/s)  LR: 2.508e-04  Data: 0.009 (0.012)
Train: 403 [1100/1251 ( 88%)]  Loss: 3.355 (3.19)  Time: 0.776s, 1319.06/s  (0.783s, 1308.45/s)  LR: 2.508e-04  Data: 0.010 (0.011)
Train: 403 [1150/1251 ( 92%)]  Loss: 3.378 (3.20)  Time: 0.783s, 1308.00/s  (0.782s, 1308.72/s)  LR: 2.508e-04  Data: 0.011 (0.011)
Train: 403 [1200/1251 ( 96%)]  Loss: 3.468 (3.21)  Time: 0.773s, 1324.30/s  (0.782s, 1308.74/s)  LR: 2.508e-04  Data: 0.010 (0.011)
Train: 403 [1250/1251 (100%)]  Loss: 3.256 (3.21)  Time: 0.800s, 1279.97/s  (0.782s, 1309.03/s)  LR: 2.508e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.522 (1.522)  Loss:  0.6973 (0.6973)  Acc@1: 90.6250 (90.6250)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.194 (0.569)  Loss:  0.8164 (1.1719)  Acc@1: 86.7924 (77.9060)  Acc@5: 97.8774 (94.0760)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-402.pth.tar', 78.02400002685548)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-396.pth.tar', 77.93199990478516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-403.pth.tar', 77.90599995117188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-400.pth.tar', 77.89000000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-393.pth.tar', 77.73999997558593)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-401.pth.tar', 77.73400000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-389.pth.tar', 77.63200005126953)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-387.pth.tar', 77.58400005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-398.pth.tar', 77.54400008300782)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-399.pth.tar', 77.5279999243164)

Train: 404 [   0/1251 (  0%)]  Loss: 3.173 (3.17)  Time: 2.189s,  467.84/s  (2.189s,  467.84/s)  LR: 2.486e-04  Data: 1.458 (1.458)
Train: 404 [  50/1251 (  4%)]  Loss: 3.370 (3.27)  Time: 0.773s, 1325.18/s  (0.816s, 1254.30/s)  LR: 2.486e-04  Data: 0.009 (0.043)
Train: 404 [ 100/1251 (  8%)]  Loss: 3.299 (3.28)  Time: 0.838s, 1222.48/s  (0.802s, 1277.48/s)  LR: 2.486e-04  Data: 0.009 (0.027)
Train: 404 [ 150/1251 ( 12%)]  Loss: 3.505 (3.34)  Time: 0.774s, 1322.54/s  (0.796s, 1287.03/s)  LR: 2.486e-04  Data: 0.012 (0.021)
Train: 404 [ 200/1251 ( 16%)]  Loss: 3.140 (3.30)  Time: 0.776s, 1318.92/s  (0.795s, 1287.47/s)  LR: 2.486e-04  Data: 0.013 (0.018)
Train: 404 [ 250/1251 ( 20%)]  Loss: 3.011 (3.25)  Time: 0.782s, 1309.16/s  (0.793s, 1291.91/s)  LR: 2.486e-04  Data: 0.010 (0.017)
Train: 404 [ 300/1251 ( 24%)]  Loss: 3.152 (3.24)  Time: 0.778s, 1315.50/s  (0.791s, 1294.26/s)  LR: 2.486e-04  Data: 0.011 (0.016)
Train: 404 [ 350/1251 ( 28%)]  Loss: 3.396 (3.26)  Time: 0.784s, 1305.64/s  (0.789s, 1297.32/s)  LR: 2.486e-04  Data: 0.010 (0.015)
Train: 404 [ 400/1251 ( 32%)]  Loss: 3.305 (3.26)  Time: 0.775s, 1320.48/s  (0.788s, 1300.01/s)  LR: 2.486e-04  Data: 0.011 (0.014)
Train: 404 [ 450/1251 ( 36%)]  Loss: 3.323 (3.27)  Time: 0.782s, 1310.03/s  (0.787s, 1301.47/s)  LR: 2.486e-04  Data: 0.011 (0.014)
Train: 404 [ 500/1251 ( 40%)]  Loss: 3.010 (3.24)  Time: 0.776s, 1320.11/s  (0.786s, 1302.74/s)  LR: 2.486e-04  Data: 0.010 (0.013)
Train: 404 [ 550/1251 ( 44%)]  Loss: 3.325 (3.25)  Time: 0.773s, 1324.81/s  (0.786s, 1303.43/s)  LR: 2.486e-04  Data: 0.009 (0.013)
Train: 404 [ 600/1251 ( 48%)]  Loss: 3.302 (3.25)  Time: 0.771s, 1327.55/s  (0.785s, 1304.09/s)  LR: 2.486e-04  Data: 0.011 (0.013)
Train: 404 [ 650/1251 ( 52%)]  Loss: 3.043 (3.24)  Time: 0.771s, 1327.82/s  (0.784s, 1305.32/s)  LR: 2.486e-04  Data: 0.010 (0.012)
Train: 404 [ 700/1251 ( 56%)]  Loss: 3.367 (3.25)  Time: 0.807s, 1269.39/s  (0.784s, 1306.13/s)  LR: 2.486e-04  Data: 0.009 (0.012)
Train: 404 [ 750/1251 ( 60%)]  Loss: 3.483 (3.26)  Time: 0.773s, 1324.54/s  (0.784s, 1306.36/s)  LR: 2.486e-04  Data: 0.010 (0.012)
Train: 404 [ 800/1251 ( 64%)]  Loss: 3.339 (3.27)  Time: 0.771s, 1328.25/s  (0.784s, 1306.59/s)  LR: 2.486e-04  Data: 0.011 (0.012)
Train: 404 [ 850/1251 ( 68%)]  Loss: 3.448 (3.28)  Time: 0.784s, 1305.58/s  (0.783s, 1307.17/s)  LR: 2.486e-04  Data: 0.010 (0.012)
Train: 404 [ 900/1251 ( 72%)]  Loss: 3.037 (3.26)  Time: 0.775s, 1320.86/s  (0.783s, 1307.55/s)  LR: 2.486e-04  Data: 0.010 (0.012)
Train: 404 [ 950/1251 ( 76%)]  Loss: 3.494 (3.28)  Time: 0.781s, 1311.01/s  (0.783s, 1307.92/s)  LR: 2.486e-04  Data: 0.009 (0.012)
Train: 404 [1000/1251 ( 80%)]  Loss: 3.502 (3.29)  Time: 0.777s, 1318.50/s  (0.783s, 1308.22/s)  LR: 2.486e-04  Data: 0.009 (0.012)
Train: 404 [1050/1251 ( 84%)]  Loss: 3.400 (3.29)  Time: 0.783s, 1308.21/s  (0.782s, 1308.87/s)  LR: 2.486e-04  Data: 0.009 (0.011)
Train: 404 [1100/1251 ( 88%)]  Loss: 3.499 (3.30)  Time: 0.776s, 1319.01/s  (0.782s, 1309.28/s)  LR: 2.486e-04  Data: 0.009 (0.011)
Train: 404 [1150/1251 ( 92%)]  Loss: 3.202 (3.30)  Time: 0.771s, 1328.59/s  (0.782s, 1309.64/s)  LR: 2.486e-04  Data: 0.010 (0.011)
Train: 404 [1200/1251 ( 96%)]  Loss: 3.068 (3.29)  Time: 0.773s, 1323.97/s  (0.782s, 1308.72/s)  LR: 2.486e-04  Data: 0.009 (0.011)
Train: 404 [1250/1251 (100%)]  Loss: 3.359 (3.29)  Time: 0.762s, 1343.16/s  (0.783s, 1307.87/s)  LR: 2.486e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.537 (1.537)  Loss:  0.7363 (0.7363)  Acc@1: 90.3320 (90.3320)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.194 (0.559)  Loss:  0.8628 (1.1980)  Acc@1: 85.3774 (77.8340)  Acc@5: 96.9340 (94.1100)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-402.pth.tar', 78.02400002685548)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-396.pth.tar', 77.93199990478516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-403.pth.tar', 77.90599995117188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-400.pth.tar', 77.89000000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-404.pth.tar', 77.83399998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-393.pth.tar', 77.73999997558593)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-401.pth.tar', 77.73400000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-389.pth.tar', 77.63200005126953)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-387.pth.tar', 77.58400005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-398.pth.tar', 77.54400008300782)

Train: 405 [   0/1251 (  0%)]  Loss: 3.380 (3.38)  Time: 2.286s,  447.94/s  (2.286s,  447.94/s)  LR: 2.464e-04  Data: 1.553 (1.553)
Train: 405 [  50/1251 (  4%)]  Loss: 3.164 (3.27)  Time: 0.773s, 1325.37/s  (0.813s, 1259.71/s)  LR: 2.464e-04  Data: 0.009 (0.044)
Train: 405 [ 100/1251 (  8%)]  Loss: 3.200 (3.25)  Time: 0.821s, 1248.00/s  (0.802s, 1276.14/s)  LR: 2.464e-04  Data: 0.010 (0.027)
Train: 405 [ 150/1251 ( 12%)]  Loss: 3.166 (3.23)  Time: 0.776s, 1319.64/s  (0.797s, 1284.59/s)  LR: 2.464e-04  Data: 0.010 (0.021)
Train: 405 [ 200/1251 ( 16%)]  Loss: 3.408 (3.26)  Time: 0.773s, 1324.81/s  (0.793s, 1291.45/s)  LR: 2.464e-04  Data: 0.010 (0.018)
Train: 405 [ 250/1251 ( 20%)]  Loss: 3.431 (3.29)  Time: 0.773s, 1324.62/s  (0.792s, 1293.27/s)  LR: 2.464e-04  Data: 0.010 (0.017)
Train: 405 [ 300/1251 ( 24%)]  Loss: 3.446 (3.31)  Time: 0.823s, 1243.89/s  (0.791s, 1294.78/s)  LR: 2.464e-04  Data: 0.010 (0.016)
Train: 405 [ 350/1251 ( 28%)]  Loss: 3.164 (3.29)  Time: 0.805s, 1272.74/s  (0.789s, 1297.10/s)  LR: 2.464e-04  Data: 0.012 (0.015)
Train: 405 [ 400/1251 ( 32%)]  Loss: 3.369 (3.30)  Time: 0.772s, 1326.23/s  (0.790s, 1295.54/s)  LR: 2.464e-04  Data: 0.010 (0.014)
Train: 405 [ 450/1251 ( 36%)]  Loss: 3.541 (3.33)  Time: 0.811s, 1262.07/s  (0.790s, 1296.73/s)  LR: 2.464e-04  Data: 0.009 (0.014)
Train: 405 [ 500/1251 ( 40%)]  Loss: 3.316 (3.33)  Time: 0.782s, 1309.84/s  (0.789s, 1298.49/s)  LR: 2.464e-04  Data: 0.010 (0.013)
Train: 405 [ 550/1251 ( 44%)]  Loss: 3.059 (3.30)  Time: 0.772s, 1325.72/s  (0.789s, 1298.36/s)  LR: 2.464e-04  Data: 0.010 (0.013)
Train: 405 [ 600/1251 ( 48%)]  Loss: 3.045 (3.28)  Time: 0.812s, 1260.88/s  (0.789s, 1297.50/s)  LR: 2.464e-04  Data: 0.010 (0.013)
Train: 405 [ 650/1251 ( 52%)]  Loss: 3.395 (3.29)  Time: 0.773s, 1324.42/s  (0.790s, 1295.95/s)  LR: 2.464e-04  Data: 0.010 (0.013)
Train: 405 [ 700/1251 ( 56%)]  Loss: 3.467 (3.30)  Time: 0.773s, 1324.45/s  (0.789s, 1297.09/s)  LR: 2.464e-04  Data: 0.009 (0.012)
Train: 405 [ 750/1251 ( 60%)]  Loss: 3.339 (3.31)  Time: 0.781s, 1311.54/s  (0.789s, 1297.19/s)  LR: 2.464e-04  Data: 0.010 (0.012)
Train: 405 [ 800/1251 ( 64%)]  Loss: 3.591 (3.32)  Time: 0.816s, 1255.19/s  (0.790s, 1296.66/s)  LR: 2.464e-04  Data: 0.010 (0.012)
Train: 405 [ 850/1251 ( 68%)]  Loss: 3.371 (3.32)  Time: 0.785s, 1304.58/s  (0.789s, 1297.12/s)  LR: 2.464e-04  Data: 0.010 (0.012)
Train: 405 [ 900/1251 ( 72%)]  Loss: 3.088 (3.31)  Time: 0.845s, 1211.41/s  (0.789s, 1297.18/s)  LR: 2.464e-04  Data: 0.010 (0.012)
Train: 405 [ 950/1251 ( 76%)]  Loss: 3.337 (3.31)  Time: 0.833s, 1229.25/s  (0.790s, 1296.38/s)  LR: 2.464e-04  Data: 0.011 (0.012)
Train: 405 [1000/1251 ( 80%)]  Loss: 3.377 (3.32)  Time: 0.781s, 1311.96/s  (0.790s, 1296.29/s)  LR: 2.464e-04  Data: 0.011 (0.012)
Train: 405 [1050/1251 ( 84%)]  Loss: 3.340 (3.32)  Time: 0.781s, 1310.50/s  (0.790s, 1296.57/s)  LR: 2.464e-04  Data: 0.010 (0.012)
Train: 405 [1100/1251 ( 88%)]  Loss: 3.231 (3.31)  Time: 0.776s, 1319.09/s  (0.790s, 1296.65/s)  LR: 2.464e-04  Data: 0.010 (0.011)
Train: 405 [1150/1251 ( 92%)]  Loss: 3.013 (3.30)  Time: 0.773s, 1324.31/s  (0.790s, 1296.86/s)  LR: 2.464e-04  Data: 0.010 (0.011)
Train: 405 [1200/1251 ( 96%)]  Loss: 3.375 (3.30)  Time: 0.773s, 1325.53/s  (0.789s, 1297.61/s)  LR: 2.464e-04  Data: 0.010 (0.011)
Train: 405 [1250/1251 (100%)]  Loss: 3.458 (3.31)  Time: 0.761s, 1346.27/s  (0.789s, 1298.56/s)  LR: 2.464e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.546 (1.546)  Loss:  0.6743 (0.6743)  Acc@1: 90.3320 (90.3320)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.194 (0.565)  Loss:  0.7388 (1.1446)  Acc@1: 86.4387 (77.9040)  Acc@5: 98.1132 (94.1380)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-402.pth.tar', 78.02400002685548)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-396.pth.tar', 77.93199990478516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-403.pth.tar', 77.90599995117188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-405.pth.tar', 77.90400005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-400.pth.tar', 77.89000000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-404.pth.tar', 77.83399998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-393.pth.tar', 77.73999997558593)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-401.pth.tar', 77.73400000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-389.pth.tar', 77.63200005126953)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-387.pth.tar', 77.58400005615235)

Train: 406 [   0/1251 (  0%)]  Loss: 3.301 (3.30)  Time: 2.261s,  452.99/s  (2.261s,  452.99/s)  LR: 2.442e-04  Data: 1.475 (1.475)
Train: 406 [  50/1251 (  4%)]  Loss: 3.150 (3.23)  Time: 0.781s, 1310.61/s  (0.813s, 1259.12/s)  LR: 2.442e-04  Data: 0.009 (0.042)
Train: 406 [ 100/1251 (  8%)]  Loss: 3.018 (3.16)  Time: 0.777s, 1318.05/s  (0.798s, 1282.81/s)  LR: 2.442e-04  Data: 0.010 (0.026)
Train: 406 [ 150/1251 ( 12%)]  Loss: 2.986 (3.11)  Time: 0.827s, 1237.53/s  (0.794s, 1289.10/s)  LR: 2.442e-04  Data: 0.010 (0.021)
Train: 406 [ 200/1251 ( 16%)]  Loss: 3.546 (3.20)  Time: 0.836s, 1225.56/s  (0.793s, 1291.85/s)  LR: 2.442e-04  Data: 0.013 (0.018)
Train: 406 [ 250/1251 ( 20%)]  Loss: 3.320 (3.22)  Time: 0.779s, 1314.30/s  (0.791s, 1294.01/s)  LR: 2.442e-04  Data: 0.012 (0.016)
Train: 406 [ 300/1251 ( 24%)]  Loss: 3.202 (3.22)  Time: 0.773s, 1324.05/s  (0.789s, 1298.24/s)  LR: 2.442e-04  Data: 0.010 (0.015)
Train: 406 [ 350/1251 ( 28%)]  Loss: 3.271 (3.22)  Time: 0.782s, 1310.29/s  (0.788s, 1300.26/s)  LR: 2.442e-04  Data: 0.009 (0.015)
Train: 406 [ 400/1251 ( 32%)]  Loss: 3.492 (3.25)  Time: 0.815s, 1255.94/s  (0.788s, 1299.49/s)  LR: 2.442e-04  Data: 0.011 (0.014)
Train: 406 [ 450/1251 ( 36%)]  Loss: 3.077 (3.24)  Time: 0.774s, 1322.71/s  (0.789s, 1298.23/s)  LR: 2.442e-04  Data: 0.009 (0.014)
Train: 406 [ 500/1251 ( 40%)]  Loss: 3.359 (3.25)  Time: 0.772s, 1326.03/s  (0.790s, 1296.91/s)  LR: 2.442e-04  Data: 0.009 (0.013)
Train: 406 [ 550/1251 ( 44%)]  Loss: 3.395 (3.26)  Time: 0.834s, 1227.75/s  (0.790s, 1296.76/s)  LR: 2.442e-04  Data: 0.010 (0.013)
Train: 406 [ 600/1251 ( 48%)]  Loss: 3.257 (3.26)  Time: 0.779s, 1315.02/s  (0.790s, 1295.81/s)  LR: 2.442e-04  Data: 0.009 (0.013)
Train: 406 [ 650/1251 ( 52%)]  Loss: 3.304 (3.26)  Time: 0.815s, 1256.51/s  (0.790s, 1296.67/s)  LR: 2.442e-04  Data: 0.010 (0.012)
Train: 406 [ 700/1251 ( 56%)]  Loss: 3.359 (3.27)  Time: 0.775s, 1321.68/s  (0.789s, 1297.89/s)  LR: 2.442e-04  Data: 0.010 (0.012)
Train: 406 [ 750/1251 ( 60%)]  Loss: 3.289 (3.27)  Time: 0.816s, 1255.05/s  (0.789s, 1297.68/s)  LR: 2.442e-04  Data: 0.011 (0.012)
Train: 406 [ 800/1251 ( 64%)]  Loss: 3.646 (3.29)  Time: 0.773s, 1325.39/s  (0.789s, 1298.43/s)  LR: 2.442e-04  Data: 0.010 (0.012)
Train: 406 [ 850/1251 ( 68%)]  Loss: 3.160 (3.29)  Time: 0.795s, 1288.28/s  (0.788s, 1298.91/s)  LR: 2.442e-04  Data: 0.012 (0.012)
Train: 406 [ 900/1251 ( 72%)]  Loss: 3.290 (3.29)  Time: 0.827s, 1238.21/s  (0.789s, 1298.51/s)  LR: 2.442e-04  Data: 0.011 (0.012)
Train: 406 [ 950/1251 ( 76%)]  Loss: 3.001 (3.27)  Time: 0.775s, 1321.68/s  (0.788s, 1299.15/s)  LR: 2.442e-04  Data: 0.010 (0.012)
Train: 406 [1000/1251 ( 80%)]  Loss: 3.278 (3.27)  Time: 0.775s, 1321.94/s  (0.788s, 1299.74/s)  LR: 2.442e-04  Data: 0.009 (0.012)
Train: 406 [1050/1251 ( 84%)]  Loss: 3.421 (3.28)  Time: 0.791s, 1293.99/s  (0.788s, 1300.19/s)  LR: 2.442e-04  Data: 0.013 (0.012)
Train: 406 [1100/1251 ( 88%)]  Loss: 3.341 (3.28)  Time: 0.775s, 1321.01/s  (0.787s, 1300.87/s)  LR: 2.442e-04  Data: 0.010 (0.012)
Train: 406 [1150/1251 ( 92%)]  Loss: 3.438 (3.29)  Time: 0.774s, 1322.37/s  (0.787s, 1301.26/s)  LR: 2.442e-04  Data: 0.009 (0.012)
Train: 406 [1200/1251 ( 96%)]  Loss: 3.228 (3.29)  Time: 0.777s, 1317.56/s  (0.787s, 1301.55/s)  LR: 2.442e-04  Data: 0.011 (0.011)
Train: 406 [1250/1251 (100%)]  Loss: 3.504 (3.29)  Time: 0.761s, 1345.61/s  (0.786s, 1302.11/s)  LR: 2.442e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.514 (1.514)  Loss:  0.7866 (0.7866)  Acc@1: 90.6250 (90.6250)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.194 (0.560)  Loss:  0.8281 (1.2192)  Acc@1: 87.1462 (77.9700)  Acc@5: 97.2877 (94.2160)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-402.pth.tar', 78.02400002685548)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-406.pth.tar', 77.96999997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-396.pth.tar', 77.93199990478516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-403.pth.tar', 77.90599995117188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-405.pth.tar', 77.90400005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-400.pth.tar', 77.89000000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-404.pth.tar', 77.83399998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-393.pth.tar', 77.73999997558593)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-401.pth.tar', 77.73400000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-389.pth.tar', 77.63200005126953)

Train: 407 [   0/1251 (  0%)]  Loss: 3.230 (3.23)  Time: 2.244s,  456.30/s  (2.244s,  456.30/s)  LR: 2.420e-04  Data: 1.512 (1.512)
Train: 407 [  50/1251 (  4%)]  Loss: 3.180 (3.20)  Time: 0.788s, 1300.08/s  (0.820s, 1248.49/s)  LR: 2.420e-04  Data: 0.014 (0.044)
Train: 407 [ 100/1251 (  8%)]  Loss: 3.291 (3.23)  Time: 0.771s, 1327.70/s  (0.800s, 1279.60/s)  LR: 2.420e-04  Data: 0.010 (0.027)
Train: 407 [ 150/1251 ( 12%)]  Loss: 3.241 (3.24)  Time: 0.818s, 1251.40/s  (0.794s, 1288.96/s)  LR: 2.420e-04  Data: 0.010 (0.021)
Train: 407 [ 200/1251 ( 16%)]  Loss: 3.431 (3.27)  Time: 0.772s, 1327.26/s  (0.791s, 1293.82/s)  LR: 2.420e-04  Data: 0.009 (0.019)
Train: 407 [ 250/1251 ( 20%)]  Loss: 3.344 (3.29)  Time: 0.772s, 1326.37/s  (0.789s, 1297.90/s)  LR: 2.420e-04  Data: 0.010 (0.017)
Train: 407 [ 300/1251 ( 24%)]  Loss: 3.017 (3.25)  Time: 0.773s, 1325.30/s  (0.788s, 1300.09/s)  LR: 2.420e-04  Data: 0.010 (0.016)
Train: 407 [ 350/1251 ( 28%)]  Loss: 3.510 (3.28)  Time: 0.836s, 1224.66/s  (0.787s, 1301.78/s)  LR: 2.420e-04  Data: 0.009 (0.015)
Train: 407 [ 400/1251 ( 32%)]  Loss: 3.097 (3.26)  Time: 0.776s, 1318.79/s  (0.786s, 1303.19/s)  LR: 2.420e-04  Data: 0.010 (0.014)
Train: 407 [ 450/1251 ( 36%)]  Loss: 3.250 (3.26)  Time: 0.772s, 1326.63/s  (0.785s, 1304.93/s)  LR: 2.420e-04  Data: 0.010 (0.014)
Train: 407 [ 500/1251 ( 40%)]  Loss: 3.500 (3.28)  Time: 0.773s, 1325.08/s  (0.784s, 1306.08/s)  LR: 2.420e-04  Data: 0.010 (0.013)
Train: 407 [ 550/1251 ( 44%)]  Loss: 3.047 (3.26)  Time: 0.772s, 1326.65/s  (0.783s, 1307.31/s)  LR: 2.420e-04  Data: 0.010 (0.013)
Train: 407 [ 600/1251 ( 48%)]  Loss: 3.303 (3.26)  Time: 0.778s, 1316.08/s  (0.783s, 1307.86/s)  LR: 2.420e-04  Data: 0.012 (0.013)
Train: 407 [ 650/1251 ( 52%)]  Loss: 3.357 (3.27)  Time: 0.772s, 1326.65/s  (0.783s, 1308.24/s)  LR: 2.420e-04  Data: 0.010 (0.013)
Train: 407 [ 700/1251 ( 56%)]  Loss: 3.198 (3.27)  Time: 0.776s, 1319.22/s  (0.783s, 1308.31/s)  LR: 2.420e-04  Data: 0.009 (0.012)
Train: 407 [ 750/1251 ( 60%)]  Loss: 3.321 (3.27)  Time: 0.827s, 1238.23/s  (0.782s, 1308.73/s)  LR: 2.420e-04  Data: 0.009 (0.012)
Train: 407 [ 800/1251 ( 64%)]  Loss: 3.254 (3.27)  Time: 0.780s, 1313.10/s  (0.782s, 1309.31/s)  LR: 2.420e-04  Data: 0.009 (0.012)
Train: 407 [ 850/1251 ( 68%)]  Loss: 3.601 (3.29)  Time: 0.779s, 1313.82/s  (0.782s, 1309.65/s)  LR: 2.420e-04  Data: 0.010 (0.012)
Train: 407 [ 900/1251 ( 72%)]  Loss: 3.037 (3.27)  Time: 0.772s, 1326.94/s  (0.782s, 1310.05/s)  LR: 2.420e-04  Data: 0.010 (0.012)
Train: 407 [ 950/1251 ( 76%)]  Loss: 3.297 (3.28)  Time: 0.769s, 1331.15/s  (0.781s, 1310.52/s)  LR: 2.420e-04  Data: 0.010 (0.012)
Train: 407 [1000/1251 ( 80%)]  Loss: 3.217 (3.27)  Time: 0.773s, 1324.08/s  (0.781s, 1310.31/s)  LR: 2.420e-04  Data: 0.010 (0.012)
Train: 407 [1050/1251 ( 84%)]  Loss: 3.564 (3.29)  Time: 0.774s, 1322.94/s  (0.781s, 1310.42/s)  LR: 2.420e-04  Data: 0.010 (0.012)
Train: 407 [1100/1251 ( 88%)]  Loss: 3.205 (3.28)  Time: 0.772s, 1325.78/s  (0.781s, 1310.54/s)  LR: 2.420e-04  Data: 0.010 (0.011)
Train: 407 [1150/1251 ( 92%)]  Loss: 3.434 (3.29)  Time: 0.816s, 1255.13/s  (0.782s, 1310.30/s)  LR: 2.420e-04  Data: 0.011 (0.011)
Train: 407 [1200/1251 ( 96%)]  Loss: 3.663 (3.30)  Time: 0.774s, 1322.72/s  (0.782s, 1308.77/s)  LR: 2.420e-04  Data: 0.010 (0.011)
Train: 407 [1250/1251 (100%)]  Loss: 2.972 (3.29)  Time: 0.759s, 1349.15/s  (0.782s, 1309.12/s)  LR: 2.420e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.566 (1.566)  Loss:  0.6880 (0.6880)  Acc@1: 90.2344 (90.2344)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.193 (0.570)  Loss:  0.8452 (1.1791)  Acc@1: 86.2028 (77.7340)  Acc@5: 97.1698 (94.1060)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-402.pth.tar', 78.02400002685548)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-406.pth.tar', 77.96999997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-396.pth.tar', 77.93199990478516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-403.pth.tar', 77.90599995117188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-405.pth.tar', 77.90400005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-400.pth.tar', 77.89000000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-404.pth.tar', 77.83399998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-393.pth.tar', 77.73999997558593)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-407.pth.tar', 77.73400008300781)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-401.pth.tar', 77.73400000488282)

Train: 408 [   0/1251 (  0%)]  Loss: 3.184 (3.18)  Time: 2.156s,  474.98/s  (2.156s,  474.98/s)  LR: 2.398e-04  Data: 1.441 (1.441)
Train: 408 [  50/1251 (  4%)]  Loss: 3.226 (3.20)  Time: 0.773s, 1324.29/s  (0.814s, 1258.41/s)  LR: 2.398e-04  Data: 0.010 (0.045)
Train: 408 [ 100/1251 (  8%)]  Loss: 3.304 (3.24)  Time: 0.775s, 1321.67/s  (0.797s, 1284.39/s)  LR: 2.398e-04  Data: 0.010 (0.028)
Train: 408 [ 150/1251 ( 12%)]  Loss: 2.819 (3.13)  Time: 0.773s, 1324.76/s  (0.796s, 1286.62/s)  LR: 2.398e-04  Data: 0.010 (0.022)
Train: 408 [ 200/1251 ( 16%)]  Loss: 3.123 (3.13)  Time: 0.783s, 1307.29/s  (0.792s, 1292.31/s)  LR: 2.398e-04  Data: 0.010 (0.019)
Train: 408 [ 250/1251 ( 20%)]  Loss: 3.132 (3.13)  Time: 0.772s, 1326.91/s  (0.790s, 1296.89/s)  LR: 2.398e-04  Data: 0.010 (0.017)
Train: 408 [ 300/1251 ( 24%)]  Loss: 3.199 (3.14)  Time: 0.782s, 1310.30/s  (0.789s, 1297.74/s)  LR: 2.398e-04  Data: 0.009 (0.016)
Train: 408 [ 350/1251 ( 28%)]  Loss: 3.161 (3.14)  Time: 0.794s, 1290.41/s  (0.788s, 1299.35/s)  LR: 2.398e-04  Data: 0.010 (0.015)
Train: 408 [ 400/1251 ( 32%)]  Loss: 3.111 (3.14)  Time: 0.828s, 1236.37/s  (0.788s, 1299.35/s)  LR: 2.398e-04  Data: 0.010 (0.014)
Train: 408 [ 450/1251 ( 36%)]  Loss: 3.011 (3.13)  Time: 0.773s, 1324.41/s  (0.788s, 1300.31/s)  LR: 2.398e-04  Data: 0.010 (0.014)
Train: 408 [ 500/1251 ( 40%)]  Loss: 3.448 (3.16)  Time: 0.775s, 1322.02/s  (0.787s, 1300.81/s)  LR: 2.398e-04  Data: 0.010 (0.014)
Train: 408 [ 550/1251 ( 44%)]  Loss: 3.574 (3.19)  Time: 0.815s, 1256.26/s  (0.788s, 1300.08/s)  LR: 2.398e-04  Data: 0.010 (0.013)
Train: 408 [ 600/1251 ( 48%)]  Loss: 3.069 (3.18)  Time: 0.823s, 1244.69/s  (0.787s, 1300.74/s)  LR: 2.398e-04  Data: 0.010 (0.013)
Train: 408 [ 650/1251 ( 52%)]  Loss: 3.088 (3.17)  Time: 0.774s, 1322.21/s  (0.786s, 1302.03/s)  LR: 2.398e-04  Data: 0.010 (0.013)
Train: 408 [ 700/1251 ( 56%)]  Loss: 3.301 (3.18)  Time: 0.788s, 1298.81/s  (0.786s, 1302.78/s)  LR: 2.398e-04  Data: 0.010 (0.013)
Train: 408 [ 750/1251 ( 60%)]  Loss: 3.295 (3.19)  Time: 0.828s, 1236.01/s  (0.786s, 1302.02/s)  LR: 2.398e-04  Data: 0.010 (0.012)
Train: 408 [ 800/1251 ( 64%)]  Loss: 3.218 (3.19)  Time: 0.773s, 1324.58/s  (0.786s, 1302.21/s)  LR: 2.398e-04  Data: 0.010 (0.012)
Train: 408 [ 850/1251 ( 68%)]  Loss: 3.255 (3.20)  Time: 0.773s, 1324.57/s  (0.786s, 1303.04/s)  LR: 2.398e-04  Data: 0.010 (0.012)
Train: 408 [ 900/1251 ( 72%)]  Loss: 3.132 (3.19)  Time: 0.778s, 1316.57/s  (0.786s, 1303.44/s)  LR: 2.398e-04  Data: 0.010 (0.012)
Train: 408 [ 950/1251 ( 76%)]  Loss: 3.424 (3.20)  Time: 0.785s, 1303.86/s  (0.785s, 1303.93/s)  LR: 2.398e-04  Data: 0.010 (0.012)
Train: 408 [1000/1251 ( 80%)]  Loss: 3.487 (3.22)  Time: 0.774s, 1323.12/s  (0.785s, 1304.57/s)  LR: 2.398e-04  Data: 0.010 (0.012)
Train: 408 [1050/1251 ( 84%)]  Loss: 2.898 (3.20)  Time: 0.778s, 1316.71/s  (0.785s, 1304.61/s)  LR: 2.398e-04  Data: 0.010 (0.012)
Train: 408 [1100/1251 ( 88%)]  Loss: 3.468 (3.21)  Time: 0.812s, 1260.35/s  (0.785s, 1304.92/s)  LR: 2.398e-04  Data: 0.010 (0.012)
Train: 408 [1150/1251 ( 92%)]  Loss: 3.419 (3.22)  Time: 0.773s, 1324.27/s  (0.784s, 1305.50/s)  LR: 2.398e-04  Data: 0.010 (0.012)
Train: 408 [1200/1251 ( 96%)]  Loss: 3.006 (3.21)  Time: 0.774s, 1322.17/s  (0.784s, 1305.87/s)  LR: 2.398e-04  Data: 0.010 (0.011)
Train: 408 [1250/1251 (100%)]  Loss: 3.335 (3.22)  Time: 0.759s, 1348.29/s  (0.784s, 1306.11/s)  LR: 2.398e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.519 (1.519)  Loss:  0.7495 (0.7495)  Acc@1: 90.8203 (90.8203)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.195 (0.562)  Loss:  0.8237 (1.1926)  Acc@1: 86.4387 (78.0320)  Acc@5: 98.1132 (94.2220)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-408.pth.tar', 78.03200005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-402.pth.tar', 78.02400002685548)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-406.pth.tar', 77.96999997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-396.pth.tar', 77.93199990478516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-403.pth.tar', 77.90599995117188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-405.pth.tar', 77.90400005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-400.pth.tar', 77.89000000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-404.pth.tar', 77.83399998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-393.pth.tar', 77.73999997558593)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-407.pth.tar', 77.73400008300781)

Train: 409 [   0/1251 (  0%)]  Loss: 3.372 (3.37)  Time: 2.166s,  472.70/s  (2.166s,  472.70/s)  LR: 2.376e-04  Data: 1.437 (1.437)
Train: 409 [  50/1251 (  4%)]  Loss: 3.226 (3.30)  Time: 0.773s, 1324.65/s  (0.818s, 1251.16/s)  LR: 2.376e-04  Data: 0.010 (0.045)
Train: 409 [ 100/1251 (  8%)]  Loss: 3.341 (3.31)  Time: 0.772s, 1325.97/s  (0.799s, 1282.36/s)  LR: 2.376e-04  Data: 0.009 (0.028)
Train: 409 [ 150/1251 ( 12%)]  Loss: 3.385 (3.33)  Time: 0.782s, 1309.25/s  (0.792s, 1293.11/s)  LR: 2.376e-04  Data: 0.017 (0.022)
Train: 409 [ 200/1251 ( 16%)]  Loss: 3.619 (3.39)  Time: 0.772s, 1326.00/s  (0.789s, 1297.53/s)  LR: 2.376e-04  Data: 0.009 (0.019)
Train: 409 [ 250/1251 ( 20%)]  Loss: 3.116 (3.34)  Time: 0.774s, 1323.65/s  (0.787s, 1301.43/s)  LR: 2.376e-04  Data: 0.010 (0.017)
Train: 409 [ 300/1251 ( 24%)]  Loss: 3.264 (3.33)  Time: 0.854s, 1198.87/s  (0.786s, 1302.36/s)  LR: 2.376e-04  Data: 0.009 (0.016)
Train: 409 [ 350/1251 ( 28%)]  Loss: 3.503 (3.35)  Time: 0.773s, 1325.31/s  (0.786s, 1303.48/s)  LR: 2.376e-04  Data: 0.010 (0.015)
Train: 409 [ 400/1251 ( 32%)]  Loss: 3.311 (3.35)  Time: 0.773s, 1325.06/s  (0.785s, 1304.88/s)  LR: 2.376e-04  Data: 0.009 (0.014)
Train: 409 [ 450/1251 ( 36%)]  Loss: 3.267 (3.34)  Time: 0.795s, 1288.57/s  (0.784s, 1305.85/s)  LR: 2.376e-04  Data: 0.010 (0.014)
Train: 409 [ 500/1251 ( 40%)]  Loss: 3.302 (3.34)  Time: 0.772s, 1327.11/s  (0.785s, 1305.17/s)  LR: 2.376e-04  Data: 0.010 (0.014)
Train: 409 [ 550/1251 ( 44%)]  Loss: 3.167 (3.32)  Time: 0.772s, 1327.09/s  (0.784s, 1306.27/s)  LR: 2.376e-04  Data: 0.009 (0.013)
Train: 409 [ 600/1251 ( 48%)]  Loss: 3.011 (3.30)  Time: 0.773s, 1324.46/s  (0.784s, 1306.95/s)  LR: 2.376e-04  Data: 0.010 (0.013)
Train: 409 [ 650/1251 ( 52%)]  Loss: 3.257 (3.30)  Time: 0.786s, 1303.62/s  (0.784s, 1306.79/s)  LR: 2.376e-04  Data: 0.010 (0.013)
Train: 409 [ 700/1251 ( 56%)]  Loss: 3.216 (3.29)  Time: 0.773s, 1325.17/s  (0.785s, 1304.45/s)  LR: 2.376e-04  Data: 0.009 (0.013)
Train: 409 [ 750/1251 ( 60%)]  Loss: 3.291 (3.29)  Time: 0.815s, 1255.96/s  (0.785s, 1304.48/s)  LR: 2.376e-04  Data: 0.009 (0.012)
Train: 409 [ 800/1251 ( 64%)]  Loss: 3.131 (3.28)  Time: 0.773s, 1325.39/s  (0.785s, 1304.04/s)  LR: 2.376e-04  Data: 0.010 (0.012)
Train: 409 [ 850/1251 ( 68%)]  Loss: 2.937 (3.26)  Time: 0.806s, 1270.61/s  (0.785s, 1304.49/s)  LR: 2.376e-04  Data: 0.010 (0.012)
Train: 409 [ 900/1251 ( 72%)]  Loss: 2.998 (3.25)  Time: 0.864s, 1185.64/s  (0.785s, 1305.06/s)  LR: 2.376e-04  Data: 0.010 (0.012)
Train: 409 [ 950/1251 ( 76%)]  Loss: 3.348 (3.25)  Time: 0.774s, 1323.15/s  (0.784s, 1305.78/s)  LR: 2.376e-04  Data: 0.009 (0.012)
Train: 409 [1000/1251 ( 80%)]  Loss: 3.239 (3.25)  Time: 0.772s, 1326.07/s  (0.784s, 1305.64/s)  LR: 2.376e-04  Data: 0.010 (0.012)
Train: 409 [1050/1251 ( 84%)]  Loss: 3.440 (3.26)  Time: 0.808s, 1267.36/s  (0.784s, 1305.82/s)  LR: 2.376e-04  Data: 0.010 (0.012)
Train: 409 [1100/1251 ( 88%)]  Loss: 3.445 (3.27)  Time: 0.774s, 1323.81/s  (0.785s, 1304.03/s)  LR: 2.376e-04  Data: 0.009 (0.012)
Train: 409 [1150/1251 ( 92%)]  Loss: 3.143 (3.26)  Time: 0.816s, 1255.15/s  (0.785s, 1304.26/s)  LR: 2.376e-04  Data: 0.010 (0.012)
Train: 409 [1200/1251 ( 96%)]  Loss: 3.202 (3.26)  Time: 0.775s, 1322.13/s  (0.786s, 1303.47/s)  LR: 2.376e-04  Data: 0.010 (0.011)
Train: 409 [1250/1251 (100%)]  Loss: 3.046 (3.25)  Time: 0.762s, 1344.41/s  (0.786s, 1303.17/s)  LR: 2.376e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.555 (1.555)  Loss:  0.6685 (0.6685)  Acc@1: 91.3086 (91.3086)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.194 (0.563)  Loss:  0.7231 (1.1387)  Acc@1: 86.7924 (77.8220)  Acc@5: 97.7594 (94.1380)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-408.pth.tar', 78.03200005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-402.pth.tar', 78.02400002685548)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-406.pth.tar', 77.96999997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-396.pth.tar', 77.93199990478516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-403.pth.tar', 77.90599995117188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-405.pth.tar', 77.90400005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-400.pth.tar', 77.89000000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-404.pth.tar', 77.83399998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-409.pth.tar', 77.82199995117188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-393.pth.tar', 77.73999997558593)

Train: 410 [   0/1251 (  0%)]  Loss: 3.056 (3.06)  Time: 2.263s,  452.48/s  (2.263s,  452.48/s)  LR: 2.354e-04  Data: 1.529 (1.529)
Train: 410 [  50/1251 (  4%)]  Loss: 3.239 (3.15)  Time: 0.777s, 1317.84/s  (0.820s, 1248.25/s)  LR: 2.354e-04  Data: 0.010 (0.047)
Train: 410 [ 100/1251 (  8%)]  Loss: 3.043 (3.11)  Time: 0.814s, 1258.02/s  (0.801s, 1278.47/s)  LR: 2.354e-04  Data: 0.010 (0.029)
Train: 410 [ 150/1251 ( 12%)]  Loss: 3.065 (3.10)  Time: 0.774s, 1323.03/s  (0.800s, 1280.47/s)  LR: 2.354e-04  Data: 0.010 (0.022)
Train: 410 [ 200/1251 ( 16%)]  Loss: 3.498 (3.18)  Time: 0.773s, 1324.00/s  (0.796s, 1285.80/s)  LR: 2.354e-04  Data: 0.010 (0.019)
Train: 410 [ 250/1251 ( 20%)]  Loss: 2.916 (3.14)  Time: 0.774s, 1322.40/s  (0.794s, 1289.61/s)  LR: 2.354e-04  Data: 0.009 (0.018)
Train: 410 [ 300/1251 ( 24%)]  Loss: 3.174 (3.14)  Time: 0.775s, 1321.34/s  (0.791s, 1294.58/s)  LR: 2.354e-04  Data: 0.010 (0.016)
Train: 410 [ 350/1251 ( 28%)]  Loss: 3.335 (3.17)  Time: 0.774s, 1323.13/s  (0.789s, 1297.84/s)  LR: 2.354e-04  Data: 0.010 (0.015)
Train: 410 [ 400/1251 ( 32%)]  Loss: 3.245 (3.17)  Time: 0.774s, 1323.48/s  (0.788s, 1299.14/s)  LR: 2.354e-04  Data: 0.009 (0.015)
Train: 410 [ 450/1251 ( 36%)]  Loss: 3.239 (3.18)  Time: 0.774s, 1323.08/s  (0.787s, 1301.12/s)  LR: 2.354e-04  Data: 0.010 (0.014)
Train: 410 [ 500/1251 ( 40%)]  Loss: 3.148 (3.18)  Time: 0.776s, 1319.83/s  (0.786s, 1302.30/s)  LR: 2.354e-04  Data: 0.010 (0.014)
Train: 410 [ 550/1251 ( 44%)]  Loss: 3.212 (3.18)  Time: 0.777s, 1317.05/s  (0.786s, 1302.90/s)  LR: 2.354e-04  Data: 0.010 (0.013)
Train: 410 [ 600/1251 ( 48%)]  Loss: 3.150 (3.18)  Time: 0.776s, 1319.73/s  (0.786s, 1303.35/s)  LR: 2.354e-04  Data: 0.010 (0.013)
Train: 410 [ 650/1251 ( 52%)]  Loss: 3.214 (3.18)  Time: 0.786s, 1302.83/s  (0.785s, 1304.65/s)  LR: 2.354e-04  Data: 0.009 (0.013)
Train: 410 [ 700/1251 ( 56%)]  Loss: 3.258 (3.19)  Time: 0.778s, 1315.39/s  (0.784s, 1305.62/s)  LR: 2.354e-04  Data: 0.009 (0.013)
Train: 410 [ 750/1251 ( 60%)]  Loss: 3.271 (3.19)  Time: 0.773s, 1325.52/s  (0.784s, 1305.83/s)  LR: 2.354e-04  Data: 0.010 (0.012)
Train: 410 [ 800/1251 ( 64%)]  Loss: 3.269 (3.20)  Time: 0.829s, 1234.63/s  (0.784s, 1306.12/s)  LR: 2.354e-04  Data: 0.010 (0.012)
Train: 410 [ 850/1251 ( 68%)]  Loss: 3.185 (3.20)  Time: 0.772s, 1327.02/s  (0.784s, 1306.47/s)  LR: 2.354e-04  Data: 0.010 (0.012)
Train: 410 [ 900/1251 ( 72%)]  Loss: 3.444 (3.21)  Time: 0.775s, 1320.87/s  (0.784s, 1306.53/s)  LR: 2.354e-04  Data: 0.009 (0.012)
Train: 410 [ 950/1251 ( 76%)]  Loss: 3.489 (3.22)  Time: 0.773s, 1325.04/s  (0.784s, 1306.93/s)  LR: 2.354e-04  Data: 0.010 (0.012)
Train: 410 [1000/1251 ( 80%)]  Loss: 3.236 (3.22)  Time: 0.772s, 1325.78/s  (0.783s, 1307.30/s)  LR: 2.354e-04  Data: 0.008 (0.012)
Train: 410 [1050/1251 ( 84%)]  Loss: 3.229 (3.22)  Time: 0.826s, 1239.26/s  (0.784s, 1306.88/s)  LR: 2.354e-04  Data: 0.014 (0.012)
Train: 410 [1100/1251 ( 88%)]  Loss: 3.379 (3.23)  Time: 0.779s, 1314.25/s  (0.784s, 1306.89/s)  LR: 2.354e-04  Data: 0.010 (0.012)
Train: 410 [1150/1251 ( 92%)]  Loss: 3.203 (3.23)  Time: 0.773s, 1324.42/s  (0.783s, 1307.25/s)  LR: 2.354e-04  Data: 0.010 (0.012)
Train: 410 [1200/1251 ( 96%)]  Loss: 3.114 (3.22)  Time: 0.775s, 1322.07/s  (0.784s, 1306.82/s)  LR: 2.354e-04  Data: 0.010 (0.011)
Train: 410 [1250/1251 (100%)]  Loss: 3.201 (3.22)  Time: 0.762s, 1344.38/s  (0.783s, 1307.39/s)  LR: 2.354e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.493 (1.493)  Loss:  0.7612 (0.7612)  Acc@1: 91.6016 (91.6016)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.193 (0.566)  Loss:  0.8003 (1.2177)  Acc@1: 87.1462 (78.2340)  Acc@5: 97.2877 (94.1360)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-410.pth.tar', 78.23400010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-408.pth.tar', 78.03200005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-402.pth.tar', 78.02400002685548)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-406.pth.tar', 77.96999997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-396.pth.tar', 77.93199990478516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-403.pth.tar', 77.90599995117188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-405.pth.tar', 77.90400005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-400.pth.tar', 77.89000000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-404.pth.tar', 77.83399998291016)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-409.pth.tar', 77.82199995117188)

Train: 411 [   0/1251 (  0%)]  Loss: 3.105 (3.10)  Time: 2.437s,  420.16/s  (2.437s,  420.16/s)  LR: 2.332e-04  Data: 1.704 (1.704)
Train: 411 [  50/1251 (  4%)]  Loss: 3.370 (3.24)  Time: 0.775s, 1322.12/s  (0.814s, 1258.36/s)  LR: 2.332e-04  Data: 0.010 (0.046)
Train: 411 [ 100/1251 (  8%)]  Loss: 3.464 (3.31)  Time: 0.773s, 1324.54/s  (0.796s, 1286.26/s)  LR: 2.332e-04  Data: 0.010 (0.028)
Train: 411 [ 150/1251 ( 12%)]  Loss: 2.947 (3.22)  Time: 0.813s, 1259.00/s  (0.790s, 1295.98/s)  LR: 2.332e-04  Data: 0.010 (0.022)
Train: 411 [ 200/1251 ( 16%)]  Loss: 3.548 (3.29)  Time: 0.773s, 1325.31/s  (0.795s, 1288.65/s)  LR: 2.332e-04  Data: 0.010 (0.019)
Train: 411 [ 250/1251 ( 20%)]  Loss: 3.149 (3.26)  Time: 0.772s, 1326.11/s  (0.792s, 1293.56/s)  LR: 2.332e-04  Data: 0.009 (0.017)
Train: 411 [ 300/1251 ( 24%)]  Loss: 3.174 (3.25)  Time: 0.772s, 1325.76/s  (0.791s, 1295.35/s)  LR: 2.332e-04  Data: 0.009 (0.016)
Train: 411 [ 350/1251 ( 28%)]  Loss: 3.083 (3.23)  Time: 0.777s, 1317.62/s  (0.790s, 1296.84/s)  LR: 2.332e-04  Data: 0.010 (0.015)
Train: 411 [ 400/1251 ( 32%)]  Loss: 3.150 (3.22)  Time: 0.793s, 1291.35/s  (0.789s, 1298.11/s)  LR: 2.332e-04  Data: 0.009 (0.014)
Train: 411 [ 450/1251 ( 36%)]  Loss: 3.383 (3.24)  Time: 0.813s, 1259.29/s  (0.788s, 1298.67/s)  LR: 2.332e-04  Data: 0.011 (0.014)
Train: 411 [ 500/1251 ( 40%)]  Loss: 3.306 (3.24)  Time: 0.812s, 1261.42/s  (0.792s, 1293.24/s)  LR: 2.332e-04  Data: 0.010 (0.014)
Train: 411 [ 550/1251 ( 44%)]  Loss: 3.201 (3.24)  Time: 0.768s, 1333.47/s  (0.791s, 1295.03/s)  LR: 2.332e-04  Data: 0.009 (0.013)
Train: 411 [ 600/1251 ( 48%)]  Loss: 3.254 (3.24)  Time: 0.774s, 1323.38/s  (0.790s, 1296.95/s)  LR: 2.332e-04  Data: 0.010 (0.013)
Train: 411 [ 650/1251 ( 52%)]  Loss: 3.675 (3.27)  Time: 0.776s, 1319.98/s  (0.790s, 1296.81/s)  LR: 2.332e-04  Data: 0.010 (0.013)
Train: 411 [ 700/1251 ( 56%)]  Loss: 3.176 (3.27)  Time: 0.813s, 1260.23/s  (0.789s, 1297.86/s)  LR: 2.332e-04  Data: 0.010 (0.013)
Train: 411 [ 750/1251 ( 60%)]  Loss: 3.465 (3.28)  Time: 0.883s, 1160.23/s  (0.789s, 1298.41/s)  LR: 2.332e-04  Data: 0.011 (0.012)
Train: 411 [ 800/1251 ( 64%)]  Loss: 3.614 (3.30)  Time: 0.772s, 1326.83/s  (0.789s, 1298.27/s)  LR: 2.332e-04  Data: 0.010 (0.012)
Train: 411 [ 850/1251 ( 68%)]  Loss: 3.063 (3.28)  Time: 0.771s, 1328.20/s  (0.788s, 1299.28/s)  LR: 2.332e-04  Data: 0.010 (0.012)
Train: 411 [ 900/1251 ( 72%)]  Loss: 3.377 (3.29)  Time: 0.793s, 1291.38/s  (0.787s, 1300.38/s)  LR: 2.332e-04  Data: 0.010 (0.012)
Train: 411 [ 950/1251 ( 76%)]  Loss: 3.340 (3.29)  Time: 0.833s, 1229.83/s  (0.787s, 1300.49/s)  LR: 2.332e-04  Data: 0.010 (0.012)
Train: 411 [1000/1251 ( 80%)]  Loss: 3.135 (3.28)  Time: 0.773s, 1325.11/s  (0.787s, 1300.35/s)  LR: 2.332e-04  Data: 0.009 (0.012)
Train: 411 [1050/1251 ( 84%)]  Loss: 2.973 (3.27)  Time: 0.768s, 1332.93/s  (0.787s, 1301.16/s)  LR: 2.332e-04  Data: 0.009 (0.012)
Train: 411 [1100/1251 ( 88%)]  Loss: 3.373 (3.27)  Time: 0.772s, 1326.01/s  (0.787s, 1301.60/s)  LR: 2.332e-04  Data: 0.010 (0.012)
Train: 411 [1150/1251 ( 92%)]  Loss: 3.361 (3.28)  Time: 0.775s, 1321.42/s  (0.787s, 1301.95/s)  LR: 2.332e-04  Data: 0.009 (0.012)
Train: 411 [1200/1251 ( 96%)]  Loss: 3.120 (3.27)  Time: 0.773s, 1324.32/s  (0.786s, 1302.70/s)  LR: 2.332e-04  Data: 0.011 (0.011)
Train: 411 [1250/1251 (100%)]  Loss: 3.008 (3.26)  Time: 0.766s, 1336.40/s  (0.786s, 1303.05/s)  LR: 2.332e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.592 (1.592)  Loss:  0.7227 (0.7227)  Acc@1: 90.0391 (90.0391)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.194 (0.574)  Loss:  0.8101 (1.1552)  Acc@1: 85.9670 (78.0700)  Acc@5: 96.8160 (94.1980)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-410.pth.tar', 78.23400010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-411.pth.tar', 78.06999998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-408.pth.tar', 78.03200005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-402.pth.tar', 78.02400002685548)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-406.pth.tar', 77.96999997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-396.pth.tar', 77.93199990478516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-403.pth.tar', 77.90599995117188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-405.pth.tar', 77.90400005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-400.pth.tar', 77.89000000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-404.pth.tar', 77.83399998291016)

Train: 412 [   0/1251 (  0%)]  Loss: 3.406 (3.41)  Time: 2.269s,  451.31/s  (2.269s,  451.31/s)  LR: 2.311e-04  Data: 1.539 (1.539)
Train: 412 [  50/1251 (  4%)]  Loss: 3.365 (3.39)  Time: 0.774s, 1322.71/s  (0.821s, 1246.50/s)  LR: 2.311e-04  Data: 0.010 (0.045)
Train: 412 [ 100/1251 (  8%)]  Loss: 3.493 (3.42)  Time: 0.784s, 1306.87/s  (0.800s, 1279.32/s)  LR: 2.311e-04  Data: 0.010 (0.028)
Train: 412 [ 150/1251 ( 12%)]  Loss: 2.924 (3.30)  Time: 0.776s, 1319.17/s  (0.795s, 1288.82/s)  LR: 2.311e-04  Data: 0.010 (0.022)
Train: 412 [ 200/1251 ( 16%)]  Loss: 3.078 (3.25)  Time: 0.775s, 1320.80/s  (0.791s, 1293.95/s)  LR: 2.311e-04  Data: 0.009 (0.019)
Train: 412 [ 250/1251 ( 20%)]  Loss: 2.668 (3.16)  Time: 0.774s, 1323.48/s  (0.789s, 1298.19/s)  LR: 2.311e-04  Data: 0.010 (0.017)
Train: 412 [ 300/1251 ( 24%)]  Loss: 3.533 (3.21)  Time: 0.779s, 1315.17/s  (0.787s, 1301.36/s)  LR: 2.311e-04  Data: 0.010 (0.016)
Train: 412 [ 350/1251 ( 28%)]  Loss: 3.368 (3.23)  Time: 0.774s, 1323.28/s  (0.786s, 1303.62/s)  LR: 2.311e-04  Data: 0.010 (0.015)
Train: 412 [ 400/1251 ( 32%)]  Loss: 3.248 (3.23)  Time: 0.787s, 1301.92/s  (0.785s, 1304.87/s)  LR: 2.311e-04  Data: 0.010 (0.015)
Train: 412 [ 450/1251 ( 36%)]  Loss: 3.370 (3.25)  Time: 0.777s, 1318.47/s  (0.784s, 1305.49/s)  LR: 2.311e-04  Data: 0.010 (0.014)
Train: 412 [ 500/1251 ( 40%)]  Loss: 3.268 (3.25)  Time: 0.823s, 1244.45/s  (0.784s, 1305.49/s)  LR: 2.311e-04  Data: 0.011 (0.014)
Train: 412 [ 550/1251 ( 44%)]  Loss: 3.542 (3.27)  Time: 0.773s, 1324.02/s  (0.786s, 1302.93/s)  LR: 2.311e-04  Data: 0.010 (0.013)
Train: 412 [ 600/1251 ( 48%)]  Loss: 3.413 (3.28)  Time: 0.773s, 1324.31/s  (0.786s, 1303.33/s)  LR: 2.311e-04  Data: 0.010 (0.013)
Train: 412 [ 650/1251 ( 52%)]  Loss: 3.031 (3.26)  Time: 0.790s, 1296.77/s  (0.786s, 1302.97/s)  LR: 2.311e-04  Data: 0.010 (0.013)
Train: 412 [ 700/1251 ( 56%)]  Loss: 3.123 (3.26)  Time: 0.774s, 1323.51/s  (0.786s, 1303.15/s)  LR: 2.311e-04  Data: 0.009 (0.013)
Train: 412 [ 750/1251 ( 60%)]  Loss: 3.494 (3.27)  Time: 0.774s, 1322.90/s  (0.785s, 1303.91/s)  LR: 2.311e-04  Data: 0.010 (0.012)
Train: 412 [ 800/1251 ( 64%)]  Loss: 3.401 (3.28)  Time: 0.782s, 1309.09/s  (0.785s, 1304.22/s)  LR: 2.311e-04  Data: 0.016 (0.012)
Train: 412 [ 850/1251 ( 68%)]  Loss: 3.295 (3.28)  Time: 0.775s, 1321.91/s  (0.785s, 1304.57/s)  LR: 2.311e-04  Data: 0.010 (0.012)
Train: 412 [ 900/1251 ( 72%)]  Loss: 2.914 (3.26)  Time: 0.773s, 1325.35/s  (0.785s, 1304.94/s)  LR: 2.311e-04  Data: 0.009 (0.012)
Train: 412 [ 950/1251 ( 76%)]  Loss: 3.032 (3.25)  Time: 0.775s, 1321.31/s  (0.785s, 1304.15/s)  LR: 2.311e-04  Data: 0.010 (0.012)
Train: 412 [1000/1251 ( 80%)]  Loss: 3.402 (3.26)  Time: 0.774s, 1322.62/s  (0.785s, 1304.33/s)  LR: 2.311e-04  Data: 0.010 (0.012)
Train: 412 [1050/1251 ( 84%)]  Loss: 3.401 (3.26)  Time: 0.777s, 1317.27/s  (0.785s, 1304.98/s)  LR: 2.311e-04  Data: 0.010 (0.012)
Train: 412 [1100/1251 ( 88%)]  Loss: 3.208 (3.26)  Time: 0.784s, 1305.67/s  (0.784s, 1305.38/s)  LR: 2.311e-04  Data: 0.010 (0.012)
Train: 412 [1150/1251 ( 92%)]  Loss: 3.125 (3.25)  Time: 0.776s, 1320.18/s  (0.784s, 1305.91/s)  LR: 2.311e-04  Data: 0.010 (0.012)
Train: 412 [1200/1251 ( 96%)]  Loss: 3.635 (3.27)  Time: 0.771s, 1328.63/s  (0.784s, 1306.01/s)  LR: 2.311e-04  Data: 0.009 (0.011)
Train: 412 [1250/1251 (100%)]  Loss: 2.962 (3.26)  Time: 0.759s, 1349.02/s  (0.784s, 1306.19/s)  LR: 2.311e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.560 (1.560)  Loss:  0.6890 (0.6890)  Acc@1: 91.6992 (91.6992)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.194 (0.560)  Loss:  0.8613 (1.2213)  Acc@1: 86.5566 (77.5500)  Acc@5: 96.6981 (94.0400)
Train: 413 [   0/1251 (  0%)]  Loss: 3.322 (3.32)  Time: 2.203s,  464.73/s  (2.203s,  464.73/s)  LR: 2.289e-04  Data: 1.489 (1.489)
Train: 413 [  50/1251 (  4%)]  Loss: 3.093 (3.21)  Time: 0.776s, 1319.69/s  (0.814s, 1257.39/s)  LR: 2.289e-04  Data: 0.010 (0.046)
Train: 413 [ 100/1251 (  8%)]  Loss: 3.540 (3.32)  Time: 0.789s, 1298.41/s  (0.795s, 1287.32/s)  LR: 2.289e-04  Data: 0.010 (0.028)
Train: 413 [ 150/1251 ( 12%)]  Loss: 3.030 (3.25)  Time: 0.778s, 1316.39/s  (0.790s, 1296.20/s)  LR: 2.289e-04  Data: 0.010 (0.022)
Train: 413 [ 200/1251 ( 16%)]  Loss: 3.284 (3.25)  Time: 0.780s, 1312.42/s  (0.790s, 1296.55/s)  LR: 2.289e-04  Data: 0.010 (0.019)
Train: 413 [ 250/1251 ( 20%)]  Loss: 3.412 (3.28)  Time: 0.782s, 1309.63/s  (0.788s, 1298.97/s)  LR: 2.289e-04  Data: 0.010 (0.017)
Train: 413 [ 300/1251 ( 24%)]  Loss: 3.046 (3.25)  Time: 0.780s, 1313.50/s  (0.787s, 1301.06/s)  LR: 2.289e-04  Data: 0.010 (0.016)
Train: 413 [ 350/1251 ( 28%)]  Loss: 2.934 (3.21)  Time: 0.819s, 1250.88/s  (0.787s, 1301.89/s)  LR: 2.289e-04  Data: 0.010 (0.015)
Train: 413 [ 400/1251 ( 32%)]  Loss: 3.158 (3.20)  Time: 0.773s, 1324.99/s  (0.786s, 1303.46/s)  LR: 2.289e-04  Data: 0.009 (0.014)
Train: 413 [ 450/1251 ( 36%)]  Loss: 3.542 (3.24)  Time: 0.773s, 1324.51/s  (0.785s, 1304.42/s)  LR: 2.289e-04  Data: 0.009 (0.014)
Train: 413 [ 500/1251 ( 40%)]  Loss: 3.055 (3.22)  Time: 0.778s, 1316.28/s  (0.785s, 1303.65/s)  LR: 2.289e-04  Data: 0.009 (0.014)
Train: 413 [ 550/1251 ( 44%)]  Loss: 3.221 (3.22)  Time: 0.780s, 1313.36/s  (0.786s, 1303.54/s)  LR: 2.289e-04  Data: 0.010 (0.013)
Train: 413 [ 600/1251 ( 48%)]  Loss: 3.097 (3.21)  Time: 0.859s, 1192.27/s  (0.785s, 1304.43/s)  LR: 2.289e-04  Data: 0.009 (0.013)
Train: 413 [ 650/1251 ( 52%)]  Loss: 3.113 (3.20)  Time: 0.773s, 1324.64/s  (0.784s, 1305.39/s)  LR: 2.289e-04  Data: 0.009 (0.013)
Train: 413 [ 700/1251 ( 56%)]  Loss: 3.349 (3.21)  Time: 0.774s, 1322.37/s  (0.784s, 1306.20/s)  LR: 2.289e-04  Data: 0.009 (0.012)
Train: 413 [ 750/1251 ( 60%)]  Loss: 3.684 (3.24)  Time: 0.786s, 1302.69/s  (0.784s, 1306.77/s)  LR: 2.289e-04  Data: 0.009 (0.012)
Train: 413 [ 800/1251 ( 64%)]  Loss: 2.852 (3.22)  Time: 0.791s, 1295.22/s  (0.783s, 1307.49/s)  LR: 2.289e-04  Data: 0.010 (0.012)
Train: 413 [ 850/1251 ( 68%)]  Loss: 2.985 (3.21)  Time: 0.773s, 1324.67/s  (0.783s, 1307.98/s)  LR: 2.289e-04  Data: 0.009 (0.012)
Train: 413 [ 900/1251 ( 72%)]  Loss: 3.227 (3.21)  Time: 0.850s, 1204.14/s  (0.784s, 1306.89/s)  LR: 2.289e-04  Data: 0.009 (0.012)
Train: 413 [ 950/1251 ( 76%)]  Loss: 3.038 (3.20)  Time: 0.811s, 1262.47/s  (0.783s, 1307.32/s)  LR: 2.289e-04  Data: 0.010 (0.012)
Train: 413 [1000/1251 ( 80%)]  Loss: 3.175 (3.20)  Time: 0.774s, 1322.16/s  (0.783s, 1307.52/s)  LR: 2.289e-04  Data: 0.009 (0.012)
Train: 413 [1050/1251 ( 84%)]  Loss: 3.184 (3.20)  Time: 0.773s, 1324.97/s  (0.783s, 1308.06/s)  LR: 2.289e-04  Data: 0.009 (0.012)
Train: 413 [1100/1251 ( 88%)]  Loss: 3.250 (3.20)  Time: 0.768s, 1332.75/s  (0.783s, 1308.16/s)  LR: 2.289e-04  Data: 0.010 (0.011)
Train: 413 [1150/1251 ( 92%)]  Loss: 3.155 (3.20)  Time: 0.773s, 1325.10/s  (0.782s, 1308.66/s)  LR: 2.289e-04  Data: 0.010 (0.011)
Train: 413 [1200/1251 ( 96%)]  Loss: 3.060 (3.19)  Time: 0.773s, 1325.56/s  (0.782s, 1308.97/s)  LR: 2.289e-04  Data: 0.010 (0.011)
Train: 413 [1250/1251 (100%)]  Loss: 3.211 (3.19)  Time: 0.761s, 1346.22/s  (0.782s, 1309.24/s)  LR: 2.289e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.544 (1.544)  Loss:  0.7837 (0.7837)  Acc@1: 91.5039 (91.5039)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.194 (0.570)  Loss:  0.8833 (1.2564)  Acc@1: 86.4387 (78.0880)  Acc@5: 97.0519 (94.3600)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-410.pth.tar', 78.23400010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-413.pth.tar', 78.08800005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-411.pth.tar', 78.06999998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-408.pth.tar', 78.03200005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-402.pth.tar', 78.02400002685548)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-406.pth.tar', 77.96999997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-396.pth.tar', 77.93199990478516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-403.pth.tar', 77.90599995117188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-405.pth.tar', 77.90400005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-400.pth.tar', 77.89000000244141)

Train: 414 [   0/1251 (  0%)]  Loss: 2.922 (2.92)  Time: 2.199s,  465.75/s  (2.199s,  465.75/s)  LR: 2.268e-04  Data: 1.470 (1.470)
Train: 414 [  50/1251 (  4%)]  Loss: 3.641 (3.28)  Time: 0.785s, 1304.48/s  (0.823s, 1243.61/s)  LR: 2.268e-04  Data: 0.009 (0.046)
Train: 414 [ 100/1251 (  8%)]  Loss: 3.430 (3.33)  Time: 0.784s, 1306.51/s  (0.818s, 1251.76/s)  LR: 2.268e-04  Data: 0.010 (0.029)
Train: 414 [ 150/1251 ( 12%)]  Loss: 3.521 (3.38)  Time: 0.773s, 1324.49/s  (0.809s, 1265.70/s)  LR: 2.268e-04  Data: 0.010 (0.023)
Train: 414 [ 200/1251 ( 16%)]  Loss: 3.360 (3.37)  Time: 0.775s, 1322.04/s  (0.803s, 1275.74/s)  LR: 2.268e-04  Data: 0.010 (0.020)
Train: 414 [ 250/1251 ( 20%)]  Loss: 3.369 (3.37)  Time: 0.785s, 1304.36/s  (0.798s, 1283.22/s)  LR: 2.268e-04  Data: 0.011 (0.018)
Train: 414 [ 300/1251 ( 24%)]  Loss: 3.196 (3.35)  Time: 0.782s, 1308.73/s  (0.795s, 1287.95/s)  LR: 2.268e-04  Data: 0.010 (0.016)
Train: 414 [ 350/1251 ( 28%)]  Loss: 3.187 (3.33)  Time: 0.789s, 1298.64/s  (0.793s, 1290.65/s)  LR: 2.268e-04  Data: 0.010 (0.015)
Train: 414 [ 400/1251 ( 32%)]  Loss: 3.194 (3.31)  Time: 0.775s, 1320.54/s  (0.792s, 1293.41/s)  LR: 2.268e-04  Data: 0.009 (0.015)
Train: 414 [ 450/1251 ( 36%)]  Loss: 3.569 (3.34)  Time: 0.833s, 1229.41/s  (0.795s, 1287.98/s)  LR: 2.268e-04  Data: 0.009 (0.014)
Train: 414 [ 500/1251 ( 40%)]  Loss: 3.343 (3.34)  Time: 0.774s, 1323.06/s  (0.795s, 1288.51/s)  LR: 2.268e-04  Data: 0.009 (0.014)
Train: 414 [ 550/1251 ( 44%)]  Loss: 3.189 (3.33)  Time: 0.815s, 1256.13/s  (0.795s, 1288.80/s)  LR: 2.268e-04  Data: 0.011 (0.014)
Train: 414 [ 600/1251 ( 48%)]  Loss: 3.285 (3.32)  Time: 0.787s, 1300.87/s  (0.794s, 1290.30/s)  LR: 2.268e-04  Data: 0.010 (0.013)
Train: 414 [ 650/1251 ( 52%)]  Loss: 3.215 (3.32)  Time: 0.776s, 1318.96/s  (0.792s, 1292.13/s)  LR: 2.268e-04  Data: 0.014 (0.013)
Train: 414 [ 700/1251 ( 56%)]  Loss: 3.087 (3.30)  Time: 0.771s, 1328.05/s  (0.791s, 1293.83/s)  LR: 2.268e-04  Data: 0.010 (0.013)
Train: 414 [ 750/1251 ( 60%)]  Loss: 3.644 (3.32)  Time: 0.827s, 1238.60/s  (0.791s, 1294.13/s)  LR: 2.268e-04  Data: 0.014 (0.013)
Train: 414 [ 800/1251 ( 64%)]  Loss: 3.551 (3.34)  Time: 0.814s, 1257.84/s  (0.793s, 1290.51/s)  LR: 2.268e-04  Data: 0.011 (0.013)
Train: 414 [ 850/1251 ( 68%)]  Loss: 3.101 (3.32)  Time: 0.817s, 1253.83/s  (0.795s, 1287.58/s)  LR: 2.268e-04  Data: 0.012 (0.012)
Train: 414 [ 900/1251 ( 72%)]  Loss: 3.275 (3.32)  Time: 0.778s, 1316.47/s  (0.796s, 1285.99/s)  LR: 2.268e-04  Data: 0.010 (0.012)
Train: 414 [ 950/1251 ( 76%)]  Loss: 3.006 (3.30)  Time: 0.772s, 1325.89/s  (0.796s, 1287.03/s)  LR: 2.268e-04  Data: 0.010 (0.012)
Train: 414 [1000/1251 ( 80%)]  Loss: 3.195 (3.30)  Time: 0.832s, 1231.43/s  (0.795s, 1287.89/s)  LR: 2.268e-04  Data: 0.009 (0.012)
Train: 414 [1050/1251 ( 84%)]  Loss: 2.985 (3.28)  Time: 0.776s, 1319.78/s  (0.796s, 1286.75/s)  LR: 2.268e-04  Data: 0.010 (0.012)
Train: 414 [1100/1251 ( 88%)]  Loss: 3.183 (3.28)  Time: 0.816s, 1254.89/s  (0.796s, 1286.41/s)  LR: 2.268e-04  Data: 0.011 (0.012)
Train: 414 [1150/1251 ( 92%)]  Loss: 3.365 (3.28)  Time: 0.781s, 1310.32/s  (0.796s, 1286.98/s)  LR: 2.268e-04  Data: 0.010 (0.012)
Train: 414 [1200/1251 ( 96%)]  Loss: 3.240 (3.28)  Time: 0.831s, 1232.05/s  (0.795s, 1287.70/s)  LR: 2.268e-04  Data: 0.009 (0.012)
Train: 414 [1250/1251 (100%)]  Loss: 3.365 (3.29)  Time: 0.759s, 1349.24/s  (0.796s, 1286.83/s)  LR: 2.268e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.563 (1.563)  Loss:  0.6597 (0.6597)  Acc@1: 91.5039 (91.5039)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.193 (0.564)  Loss:  0.7705 (1.1423)  Acc@1: 86.2028 (78.0520)  Acc@5: 97.4057 (94.2580)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-410.pth.tar', 78.23400010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-413.pth.tar', 78.08800005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-411.pth.tar', 78.06999998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-414.pth.tar', 78.05199995361328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-408.pth.tar', 78.03200005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-402.pth.tar', 78.02400002685548)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-406.pth.tar', 77.96999997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-396.pth.tar', 77.93199990478516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-403.pth.tar', 77.90599995117188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-405.pth.tar', 77.90400005615234)

Train: 415 [   0/1251 (  0%)]  Loss: 3.553 (3.55)  Time: 2.393s,  427.96/s  (2.393s,  427.96/s)  LR: 2.246e-04  Data: 1.646 (1.646)
Train: 415 [  50/1251 (  4%)]  Loss: 3.276 (3.41)  Time: 0.775s, 1322.00/s  (0.828s, 1236.16/s)  LR: 2.246e-04  Data: 0.010 (0.043)
Train: 415 [ 100/1251 (  8%)]  Loss: 3.099 (3.31)  Time: 0.772s, 1325.61/s  (0.811s, 1262.24/s)  LR: 2.246e-04  Data: 0.009 (0.026)
Train: 415 [ 150/1251 ( 12%)]  Loss: 3.494 (3.36)  Time: 0.782s, 1308.94/s  (0.801s, 1278.73/s)  LR: 2.246e-04  Data: 0.010 (0.021)
Train: 415 [ 200/1251 ( 16%)]  Loss: 3.226 (3.33)  Time: 0.772s, 1325.88/s  (0.796s, 1287.24/s)  LR: 2.246e-04  Data: 0.010 (0.018)
Train: 415 [ 250/1251 ( 20%)]  Loss: 3.475 (3.35)  Time: 0.804s, 1274.39/s  (0.793s, 1291.86/s)  LR: 2.246e-04  Data: 0.010 (0.017)
Train: 415 [ 300/1251 ( 24%)]  Loss: 3.184 (3.33)  Time: 0.808s, 1266.88/s  (0.793s, 1291.49/s)  LR: 2.246e-04  Data: 0.009 (0.015)
Train: 415 [ 350/1251 ( 28%)]  Loss: 3.291 (3.32)  Time: 0.829s, 1235.62/s  (0.793s, 1291.88/s)  LR: 2.246e-04  Data: 0.010 (0.015)
Train: 415 [ 400/1251 ( 32%)]  Loss: 3.356 (3.33)  Time: 0.772s, 1326.90/s  (0.793s, 1291.99/s)  LR: 2.246e-04  Data: 0.009 (0.014)
Train: 415 [ 450/1251 ( 36%)]  Loss: 3.214 (3.32)  Time: 0.774s, 1322.63/s  (0.791s, 1294.15/s)  LR: 2.246e-04  Data: 0.010 (0.014)
Train: 415 [ 500/1251 ( 40%)]  Loss: 3.280 (3.31)  Time: 0.771s, 1327.37/s  (0.790s, 1295.72/s)  LR: 2.246e-04  Data: 0.010 (0.013)
Train: 415 [ 550/1251 ( 44%)]  Loss: 3.248 (3.31)  Time: 0.839s, 1219.86/s  (0.791s, 1295.18/s)  LR: 2.246e-04  Data: 0.009 (0.013)
Train: 415 [ 600/1251 ( 48%)]  Loss: 3.033 (3.29)  Time: 0.774s, 1322.23/s  (0.790s, 1296.54/s)  LR: 2.246e-04  Data: 0.009 (0.013)
Train: 415 [ 650/1251 ( 52%)]  Loss: 3.082 (3.27)  Time: 0.815s, 1256.04/s  (0.790s, 1296.03/s)  LR: 2.246e-04  Data: 0.010 (0.012)
Train: 415 [ 700/1251 ( 56%)]  Loss: 3.318 (3.28)  Time: 0.825s, 1240.93/s  (0.790s, 1296.51/s)  LR: 2.246e-04  Data: 0.011 (0.012)
Train: 415 [ 750/1251 ( 60%)]  Loss: 3.107 (3.26)  Time: 0.778s, 1315.71/s  (0.789s, 1297.39/s)  LR: 2.246e-04  Data: 0.010 (0.012)
Train: 415 [ 800/1251 ( 64%)]  Loss: 2.854 (3.24)  Time: 0.773s, 1325.30/s  (0.788s, 1298.76/s)  LR: 2.246e-04  Data: 0.010 (0.012)
Train: 415 [ 850/1251 ( 68%)]  Loss: 3.173 (3.24)  Time: 0.791s, 1295.04/s  (0.789s, 1298.07/s)  LR: 2.246e-04  Data: 0.010 (0.012)
Train: 415 [ 900/1251 ( 72%)]  Loss: 2.811 (3.21)  Time: 0.770s, 1329.81/s  (0.788s, 1299.08/s)  LR: 2.246e-04  Data: 0.010 (0.012)
Train: 415 [ 950/1251 ( 76%)]  Loss: 2.833 (3.20)  Time: 0.772s, 1326.12/s  (0.788s, 1299.76/s)  LR: 2.246e-04  Data: 0.009 (0.012)
Train: 415 [1000/1251 ( 80%)]  Loss: 3.311 (3.20)  Time: 0.779s, 1314.87/s  (0.787s, 1300.46/s)  LR: 2.246e-04  Data: 0.010 (0.012)
Train: 415 [1050/1251 ( 84%)]  Loss: 2.908 (3.19)  Time: 0.873s, 1172.94/s  (0.787s, 1300.54/s)  LR: 2.246e-04  Data: 0.010 (0.011)
Train: 415 [1100/1251 ( 88%)]  Loss: 3.478 (3.20)  Time: 0.808s, 1267.50/s  (0.787s, 1300.42/s)  LR: 2.246e-04  Data: 0.010 (0.011)
Train: 415 [1150/1251 ( 92%)]  Loss: 2.836 (3.18)  Time: 0.785s, 1304.19/s  (0.788s, 1299.90/s)  LR: 2.246e-04  Data: 0.014 (0.011)
Train: 415 [1200/1251 ( 96%)]  Loss: 3.575 (3.20)  Time: 0.773s, 1323.93/s  (0.787s, 1300.71/s)  LR: 2.246e-04  Data: 0.010 (0.011)
Train: 415 [1250/1251 (100%)]  Loss: 3.245 (3.20)  Time: 0.760s, 1347.78/s  (0.787s, 1301.14/s)  LR: 2.246e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.547 (1.547)  Loss:  0.6475 (0.6475)  Acc@1: 91.6016 (91.6016)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.194 (0.568)  Loss:  0.7324 (1.1539)  Acc@1: 86.9104 (77.8440)  Acc@5: 96.4623 (94.2140)
Train: 416 [   0/1251 (  0%)]  Loss: 3.437 (3.44)  Time: 2.293s,  446.66/s  (2.293s,  446.66/s)  LR: 2.225e-04  Data: 1.560 (1.560)
Train: 416 [  50/1251 (  4%)]  Loss: 3.336 (3.39)  Time: 0.774s, 1323.52/s  (0.817s, 1253.16/s)  LR: 2.225e-04  Data: 0.009 (0.043)
Train: 416 [ 100/1251 (  8%)]  Loss: 3.313 (3.36)  Time: 0.826s, 1239.40/s  (0.816s, 1254.43/s)  LR: 2.225e-04  Data: 0.013 (0.027)
Train: 416 [ 150/1251 ( 12%)]  Loss: 3.382 (3.37)  Time: 0.778s, 1316.96/s  (0.805s, 1271.73/s)  LR: 2.225e-04  Data: 0.011 (0.021)
Train: 416 [ 200/1251 ( 16%)]  Loss: 3.262 (3.35)  Time: 0.772s, 1326.99/s  (0.800s, 1280.58/s)  LR: 2.225e-04  Data: 0.010 (0.018)
Train: 416 [ 250/1251 ( 20%)]  Loss: 3.053 (3.30)  Time: 0.772s, 1326.75/s  (0.795s, 1287.71/s)  LR: 2.225e-04  Data: 0.009 (0.017)
Train: 416 [ 300/1251 ( 24%)]  Loss: 3.235 (3.29)  Time: 0.814s, 1258.01/s  (0.795s, 1287.77/s)  LR: 2.225e-04  Data: 0.011 (0.016)
Train: 416 [ 350/1251 ( 28%)]  Loss: 3.384 (3.30)  Time: 0.778s, 1315.61/s  (0.796s, 1285.90/s)  LR: 2.225e-04  Data: 0.010 (0.015)
Train: 416 [ 400/1251 ( 32%)]  Loss: 3.266 (3.30)  Time: 0.836s, 1224.61/s  (0.794s, 1289.15/s)  LR: 2.225e-04  Data: 0.011 (0.014)
Train: 416 [ 450/1251 ( 36%)]  Loss: 3.362 (3.30)  Time: 0.816s, 1255.52/s  (0.793s, 1291.60/s)  LR: 2.225e-04  Data: 0.009 (0.014)
Train: 416 [ 500/1251 ( 40%)]  Loss: 3.144 (3.29)  Time: 0.778s, 1316.60/s  (0.792s, 1292.89/s)  LR: 2.225e-04  Data: 0.010 (0.013)
Train: 416 [ 550/1251 ( 44%)]  Loss: 2.721 (3.24)  Time: 0.841s, 1217.19/s  (0.795s, 1288.68/s)  LR: 2.225e-04  Data: 0.010 (0.013)
Train: 416 [ 600/1251 ( 48%)]  Loss: 3.180 (3.24)  Time: 0.775s, 1320.73/s  (0.794s, 1289.32/s)  LR: 2.225e-04  Data: 0.010 (0.013)
Train: 416 [ 650/1251 ( 52%)]  Loss: 3.231 (3.24)  Time: 0.807s, 1268.41/s  (0.794s, 1289.45/s)  LR: 2.225e-04  Data: 0.010 (0.013)
Train: 416 [ 700/1251 ( 56%)]  Loss: 3.089 (3.23)  Time: 0.788s, 1299.30/s  (0.794s, 1290.09/s)  LR: 2.225e-04  Data: 0.016 (0.012)
Train: 416 [ 750/1251 ( 60%)]  Loss: 2.979 (3.21)  Time: 0.771s, 1327.91/s  (0.793s, 1291.73/s)  LR: 2.225e-04  Data: 0.009 (0.012)
Train: 416 [ 800/1251 ( 64%)]  Loss: 3.338 (3.22)  Time: 0.781s, 1310.99/s  (0.792s, 1292.80/s)  LR: 2.225e-04  Data: 0.010 (0.012)
Train: 416 [ 850/1251 ( 68%)]  Loss: 3.155 (3.21)  Time: 0.781s, 1311.81/s  (0.791s, 1294.07/s)  LR: 2.225e-04  Data: 0.009 (0.012)
Train: 416 [ 900/1251 ( 72%)]  Loss: 3.149 (3.21)  Time: 0.773s, 1325.31/s  (0.791s, 1295.36/s)  LR: 2.225e-04  Data: 0.009 (0.012)
Train: 416 [ 950/1251 ( 76%)]  Loss: 2.745 (3.19)  Time: 0.784s, 1306.07/s  (0.790s, 1296.37/s)  LR: 2.225e-04  Data: 0.009 (0.012)
Train: 416 [1000/1251 ( 80%)]  Loss: 3.343 (3.20)  Time: 0.776s, 1318.99/s  (0.789s, 1297.45/s)  LR: 2.225e-04  Data: 0.009 (0.012)
Train: 416 [1050/1251 ( 84%)]  Loss: 3.451 (3.21)  Time: 0.771s, 1327.39/s  (0.789s, 1298.11/s)  LR: 2.225e-04  Data: 0.009 (0.012)
Train: 416 [1100/1251 ( 88%)]  Loss: 3.503 (3.22)  Time: 0.772s, 1326.58/s  (0.788s, 1298.94/s)  LR: 2.225e-04  Data: 0.010 (0.011)
Train: 416 [1150/1251 ( 92%)]  Loss: 3.074 (3.21)  Time: 0.774s, 1323.47/s  (0.788s, 1299.08/s)  LR: 2.225e-04  Data: 0.009 (0.011)
Train: 416 [1200/1251 ( 96%)]  Loss: 3.225 (3.21)  Time: 0.775s, 1320.75/s  (0.788s, 1299.49/s)  LR: 2.225e-04  Data: 0.009 (0.011)
Train: 416 [1250/1251 (100%)]  Loss: 3.123 (3.21)  Time: 0.777s, 1317.19/s  (0.788s, 1300.14/s)  LR: 2.225e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.530 (1.530)  Loss:  0.7231 (0.7231)  Acc@1: 89.9414 (89.9414)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.194 (0.562)  Loss:  0.7925 (1.1787)  Acc@1: 86.0849 (77.7920)  Acc@5: 96.5802 (94.0420)
Train: 417 [   0/1251 (  0%)]  Loss: 3.108 (3.11)  Time: 2.212s,  462.89/s  (2.212s,  462.89/s)  LR: 2.204e-04  Data: 1.501 (1.501)
Train: 417 [  50/1251 (  4%)]  Loss: 3.324 (3.22)  Time: 0.773s, 1325.38/s  (0.817s, 1253.84/s)  LR: 2.204e-04  Data: 0.009 (0.048)
Train: 417 [ 100/1251 (  8%)]  Loss: 3.379 (3.27)  Time: 0.772s, 1327.06/s  (0.801s, 1278.70/s)  LR: 2.204e-04  Data: 0.010 (0.029)
Train: 417 [ 150/1251 ( 12%)]  Loss: 3.392 (3.30)  Time: 0.773s, 1324.17/s  (0.795s, 1288.59/s)  LR: 2.204e-04  Data: 0.010 (0.023)
Train: 417 [ 200/1251 ( 16%)]  Loss: 3.508 (3.34)  Time: 0.778s, 1315.94/s  (0.790s, 1296.01/s)  LR: 2.204e-04  Data: 0.009 (0.020)
Train: 417 [ 250/1251 ( 20%)]  Loss: 3.464 (3.36)  Time: 0.773s, 1323.94/s  (0.789s, 1298.53/s)  LR: 2.204e-04  Data: 0.009 (0.018)
Train: 417 [ 300/1251 ( 24%)]  Loss: 3.160 (3.33)  Time: 0.771s, 1328.68/s  (0.788s, 1299.21/s)  LR: 2.204e-04  Data: 0.010 (0.016)
Train: 417 [ 350/1251 ( 28%)]  Loss: 3.224 (3.32)  Time: 0.785s, 1303.89/s  (0.788s, 1299.79/s)  LR: 2.204e-04  Data: 0.010 (0.015)
Train: 417 [ 400/1251 ( 32%)]  Loss: 3.211 (3.31)  Time: 0.774s, 1322.40/s  (0.790s, 1296.29/s)  LR: 2.204e-04  Data: 0.010 (0.015)
Train: 417 [ 450/1251 ( 36%)]  Loss: 3.228 (3.30)  Time: 0.774s, 1323.15/s  (0.789s, 1297.63/s)  LR: 2.204e-04  Data: 0.010 (0.014)
Train: 417 [ 500/1251 ( 40%)]  Loss: 3.334 (3.30)  Time: 0.778s, 1316.21/s  (0.788s, 1298.85/s)  LR: 2.204e-04  Data: 0.010 (0.014)
Train: 417 [ 550/1251 ( 44%)]  Loss: 3.056 (3.28)  Time: 0.782s, 1309.64/s  (0.787s, 1300.35/s)  LR: 2.204e-04  Data: 0.009 (0.014)
Train: 417 [ 600/1251 ( 48%)]  Loss: 3.505 (3.30)  Time: 0.856s, 1196.04/s  (0.787s, 1300.40/s)  LR: 2.204e-04  Data: 0.009 (0.013)
Train: 417 [ 650/1251 ( 52%)]  Loss: 3.393 (3.31)  Time: 0.808s, 1267.15/s  (0.788s, 1299.44/s)  LR: 2.204e-04  Data: 0.009 (0.013)
Train: 417 [ 700/1251 ( 56%)]  Loss: 3.227 (3.30)  Time: 0.776s, 1320.25/s  (0.788s, 1300.07/s)  LR: 2.204e-04  Data: 0.009 (0.013)
Train: 417 [ 750/1251 ( 60%)]  Loss: 2.911 (3.28)  Time: 0.773s, 1324.72/s  (0.788s, 1299.77/s)  LR: 2.204e-04  Data: 0.010 (0.013)
Train: 417 [ 800/1251 ( 64%)]  Loss: 3.106 (3.27)  Time: 0.774s, 1322.80/s  (0.787s, 1300.70/s)  LR: 2.204e-04  Data: 0.010 (0.013)
Train: 417 [ 850/1251 ( 68%)]  Loss: 3.449 (3.28)  Time: 0.815s, 1256.09/s  (0.787s, 1301.36/s)  LR: 2.204e-04  Data: 0.010 (0.012)
Train: 417 [ 900/1251 ( 72%)]  Loss: 3.192 (3.27)  Time: 0.773s, 1324.65/s  (0.789s, 1298.52/s)  LR: 2.204e-04  Data: 0.010 (0.012)
Train: 417 [ 950/1251 ( 76%)]  Loss: 3.335 (3.28)  Time: 0.772s, 1326.77/s  (0.788s, 1299.08/s)  LR: 2.204e-04  Data: 0.009 (0.012)
Train: 417 [1000/1251 ( 80%)]  Loss: 3.478 (3.28)  Time: 0.773s, 1324.14/s  (0.788s, 1300.06/s)  LR: 2.204e-04  Data: 0.010 (0.012)
Train: 417 [1050/1251 ( 84%)]  Loss: 3.356 (3.29)  Time: 0.774s, 1323.04/s  (0.787s, 1300.77/s)  LR: 2.204e-04  Data: 0.010 (0.012)
Train: 417 [1100/1251 ( 88%)]  Loss: 3.368 (3.29)  Time: 0.774s, 1323.72/s  (0.787s, 1301.22/s)  LR: 2.204e-04  Data: 0.009 (0.012)
Train: 417 [1150/1251 ( 92%)]  Loss: 2.909 (3.28)  Time: 0.773s, 1324.12/s  (0.787s, 1300.94/s)  LR: 2.204e-04  Data: 0.010 (0.012)
Train: 417 [1200/1251 ( 96%)]  Loss: 3.165 (3.27)  Time: 0.781s, 1310.54/s  (0.787s, 1301.56/s)  LR: 2.204e-04  Data: 0.009 (0.012)
Train: 417 [1250/1251 (100%)]  Loss: 3.112 (3.27)  Time: 0.762s, 1343.09/s  (0.786s, 1302.04/s)  LR: 2.204e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.502 (1.502)  Loss:  0.6802 (0.6802)  Acc@1: 91.5039 (91.5039)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.194 (0.571)  Loss:  0.8086 (1.1772)  Acc@1: 87.1462 (77.9960)  Acc@5: 97.7594 (94.2920)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-410.pth.tar', 78.23400010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-413.pth.tar', 78.08800005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-411.pth.tar', 78.06999998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-414.pth.tar', 78.05199995361328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-408.pth.tar', 78.03200005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-402.pth.tar', 78.02400002685548)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-417.pth.tar', 77.99600010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-406.pth.tar', 77.96999997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-396.pth.tar', 77.93199990478516)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-403.pth.tar', 77.90599995117188)

Train: 418 [   0/1251 (  0%)]  Loss: 2.988 (2.99)  Time: 2.516s,  406.93/s  (2.516s,  406.93/s)  LR: 2.183e-04  Data: 1.786 (1.786)
Train: 418 [  50/1251 (  4%)]  Loss: 3.313 (3.15)  Time: 0.772s, 1325.66/s  (0.817s, 1253.70/s)  LR: 2.183e-04  Data: 0.009 (0.045)
Train: 418 [ 100/1251 (  8%)]  Loss: 3.327 (3.21)  Time: 0.773s, 1325.49/s  (0.798s, 1283.51/s)  LR: 2.183e-04  Data: 0.009 (0.028)
Train: 418 [ 150/1251 ( 12%)]  Loss: 3.315 (3.24)  Time: 0.773s, 1325.52/s  (0.794s, 1290.37/s)  LR: 2.183e-04  Data: 0.009 (0.022)
Train: 418 [ 200/1251 ( 16%)]  Loss: 3.221 (3.23)  Time: 0.773s, 1324.98/s  (0.790s, 1295.59/s)  LR: 2.183e-04  Data: 0.009 (0.019)
Train: 418 [ 250/1251 ( 20%)]  Loss: 3.287 (3.24)  Time: 0.773s, 1324.38/s  (0.788s, 1300.19/s)  LR: 2.183e-04  Data: 0.009 (0.017)
Train: 418 [ 300/1251 ( 24%)]  Loss: 3.401 (3.26)  Time: 0.777s, 1317.19/s  (0.786s, 1302.75/s)  LR: 2.183e-04  Data: 0.010 (0.016)
Train: 418 [ 350/1251 ( 28%)]  Loss: 3.159 (3.25)  Time: 0.815s, 1256.41/s  (0.788s, 1299.23/s)  LR: 2.183e-04  Data: 0.011 (0.015)
Train: 418 [ 400/1251 ( 32%)]  Loss: 3.234 (3.25)  Time: 0.817s, 1252.84/s  (0.787s, 1300.91/s)  LR: 2.183e-04  Data: 0.009 (0.014)
Train: 418 [ 450/1251 ( 36%)]  Loss: 2.948 (3.22)  Time: 0.772s, 1325.76/s  (0.787s, 1301.28/s)  LR: 2.183e-04  Data: 0.010 (0.014)
Train: 418 [ 500/1251 ( 40%)]  Loss: 3.497 (3.24)  Time: 0.832s, 1231.36/s  (0.787s, 1301.43/s)  LR: 2.183e-04  Data: 0.009 (0.013)
Train: 418 [ 550/1251 ( 44%)]  Loss: 3.180 (3.24)  Time: 0.780s, 1312.31/s  (0.787s, 1301.97/s)  LR: 2.183e-04  Data: 0.009 (0.013)
Train: 418 [ 600/1251 ( 48%)]  Loss: 3.177 (3.23)  Time: 0.781s, 1311.10/s  (0.786s, 1303.11/s)  LR: 2.183e-04  Data: 0.010 (0.013)
Train: 418 [ 650/1251 ( 52%)]  Loss: 3.080 (3.22)  Time: 0.772s, 1326.60/s  (0.786s, 1302.56/s)  LR: 2.183e-04  Data: 0.010 (0.013)
Train: 418 [ 700/1251 ( 56%)]  Loss: 3.352 (3.23)  Time: 0.772s, 1325.65/s  (0.787s, 1300.37/s)  LR: 2.183e-04  Data: 0.010 (0.012)
Train: 418 [ 750/1251 ( 60%)]  Loss: 3.551 (3.25)  Time: 0.774s, 1322.79/s  (0.787s, 1301.08/s)  LR: 2.183e-04  Data: 0.009 (0.012)
Train: 418 [ 800/1251 ( 64%)]  Loss: 3.478 (3.27)  Time: 0.780s, 1313.51/s  (0.787s, 1301.43/s)  LR: 2.183e-04  Data: 0.009 (0.012)
Train: 418 [ 850/1251 ( 68%)]  Loss: 3.211 (3.26)  Time: 0.776s, 1319.30/s  (0.787s, 1301.83/s)  LR: 2.183e-04  Data: 0.009 (0.012)
Train: 418 [ 900/1251 ( 72%)]  Loss: 3.215 (3.26)  Time: 0.784s, 1305.98/s  (0.786s, 1302.42/s)  LR: 2.183e-04  Data: 0.010 (0.012)
Train: 418 [ 950/1251 ( 76%)]  Loss: 2.999 (3.25)  Time: 0.777s, 1317.40/s  (0.786s, 1303.18/s)  LR: 2.183e-04  Data: 0.009 (0.012)
Train: 418 [1000/1251 ( 80%)]  Loss: 3.702 (3.27)  Time: 0.773s, 1324.47/s  (0.785s, 1304.02/s)  LR: 2.183e-04  Data: 0.009 (0.012)
Train: 418 [1050/1251 ( 84%)]  Loss: 3.219 (3.27)  Time: 0.777s, 1318.09/s  (0.785s, 1304.52/s)  LR: 2.183e-04  Data: 0.010 (0.011)
Train: 418 [1100/1251 ( 88%)]  Loss: 2.875 (3.25)  Time: 0.814s, 1258.01/s  (0.785s, 1304.80/s)  LR: 2.183e-04  Data: 0.009 (0.011)
Train: 418 [1150/1251 ( 92%)]  Loss: 3.428 (3.26)  Time: 0.772s, 1325.98/s  (0.785s, 1304.40/s)  LR: 2.183e-04  Data: 0.009 (0.011)
Train: 418 [1200/1251 ( 96%)]  Loss: 3.428 (3.26)  Time: 0.777s, 1318.30/s  (0.785s, 1305.04/s)  LR: 2.183e-04  Data: 0.010 (0.011)
Train: 418 [1250/1251 (100%)]  Loss: 3.059 (3.26)  Time: 0.807s, 1269.10/s  (0.785s, 1305.13/s)  LR: 2.183e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.552 (1.552)  Loss:  0.6875 (0.6875)  Acc@1: 90.5273 (90.5273)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.194 (0.559)  Loss:  0.7651 (1.1650)  Acc@1: 87.3821 (78.0940)  Acc@5: 96.9340 (94.2740)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-410.pth.tar', 78.23400010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-418.pth.tar', 78.094000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-413.pth.tar', 78.08800005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-411.pth.tar', 78.06999998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-414.pth.tar', 78.05199995361328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-408.pth.tar', 78.03200005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-402.pth.tar', 78.02400002685548)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-417.pth.tar', 77.99600010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-406.pth.tar', 77.96999997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-396.pth.tar', 77.93199990478516)

Train: 419 [   0/1251 (  0%)]  Loss: 3.089 (3.09)  Time: 2.252s,  454.70/s  (2.252s,  454.70/s)  LR: 2.161e-04  Data: 1.521 (1.521)
Train: 419 [  50/1251 (  4%)]  Loss: 3.332 (3.21)  Time: 0.816s, 1255.55/s  (0.817s, 1253.94/s)  LR: 2.161e-04  Data: 0.010 (0.044)
Train: 419 [ 100/1251 (  8%)]  Loss: 2.888 (3.10)  Time: 0.773s, 1324.83/s  (0.801s, 1278.07/s)  LR: 2.161e-04  Data: 0.010 (0.027)
Train: 419 [ 150/1251 ( 12%)]  Loss: 3.258 (3.14)  Time: 0.771s, 1327.84/s  (0.794s, 1290.45/s)  LR: 2.161e-04  Data: 0.009 (0.021)
Train: 419 [ 200/1251 ( 16%)]  Loss: 3.342 (3.18)  Time: 0.772s, 1326.42/s  (0.789s, 1297.09/s)  LR: 2.161e-04  Data: 0.010 (0.019)
Train: 419 [ 250/1251 ( 20%)]  Loss: 3.228 (3.19)  Time: 0.774s, 1323.34/s  (0.788s, 1300.14/s)  LR: 2.161e-04  Data: 0.010 (0.017)
Train: 419 [ 300/1251 ( 24%)]  Loss: 3.041 (3.17)  Time: 0.774s, 1322.55/s  (0.786s, 1302.84/s)  LR: 2.161e-04  Data: 0.010 (0.016)
Train: 419 [ 350/1251 ( 28%)]  Loss: 3.289 (3.18)  Time: 0.774s, 1323.44/s  (0.785s, 1304.18/s)  LR: 2.161e-04  Data: 0.010 (0.015)
Train: 419 [ 400/1251 ( 32%)]  Loss: 3.451 (3.21)  Time: 0.771s, 1328.23/s  (0.785s, 1305.28/s)  LR: 2.161e-04  Data: 0.010 (0.014)
Train: 419 [ 450/1251 ( 36%)]  Loss: 3.352 (3.23)  Time: 0.785s, 1304.23/s  (0.784s, 1306.14/s)  LR: 2.161e-04  Data: 0.010 (0.014)
Train: 419 [ 500/1251 ( 40%)]  Loss: 3.030 (3.21)  Time: 0.772s, 1326.50/s  (0.784s, 1306.71/s)  LR: 2.161e-04  Data: 0.010 (0.013)
Train: 419 [ 550/1251 ( 44%)]  Loss: 3.157 (3.20)  Time: 0.773s, 1325.04/s  (0.783s, 1307.69/s)  LR: 2.161e-04  Data: 0.010 (0.013)
Train: 419 [ 600/1251 ( 48%)]  Loss: 3.167 (3.20)  Time: 0.773s, 1325.34/s  (0.783s, 1307.51/s)  LR: 2.161e-04  Data: 0.009 (0.013)
Train: 419 [ 650/1251 ( 52%)]  Loss: 2.978 (3.19)  Time: 0.776s, 1320.19/s  (0.783s, 1307.94/s)  LR: 2.161e-04  Data: 0.010 (0.013)
Train: 419 [ 700/1251 ( 56%)]  Loss: 3.507 (3.21)  Time: 0.773s, 1325.45/s  (0.783s, 1307.15/s)  LR: 2.161e-04  Data: 0.009 (0.012)
Train: 419 [ 750/1251 ( 60%)]  Loss: 3.369 (3.22)  Time: 0.773s, 1325.22/s  (0.783s, 1307.44/s)  LR: 2.161e-04  Data: 0.010 (0.012)
Train: 419 [ 800/1251 ( 64%)]  Loss: 3.101 (3.21)  Time: 0.771s, 1327.92/s  (0.783s, 1308.03/s)  LR: 2.161e-04  Data: 0.009 (0.012)
Train: 419 [ 850/1251 ( 68%)]  Loss: 3.160 (3.21)  Time: 0.773s, 1324.31/s  (0.783s, 1307.90/s)  LR: 2.161e-04  Data: 0.009 (0.012)
Train: 419 [ 900/1251 ( 72%)]  Loss: 3.331 (3.21)  Time: 0.776s, 1320.42/s  (0.783s, 1308.44/s)  LR: 2.161e-04  Data: 0.009 (0.012)
Train: 419 [ 950/1251 ( 76%)]  Loss: 3.079 (3.21)  Time: 0.825s, 1241.41/s  (0.783s, 1308.32/s)  LR: 2.161e-04  Data: 0.009 (0.012)
Train: 419 [1000/1251 ( 80%)]  Loss: 2.791 (3.19)  Time: 0.779s, 1314.30/s  (0.783s, 1308.50/s)  LR: 2.161e-04  Data: 0.010 (0.012)
Train: 419 [1050/1251 ( 84%)]  Loss: 3.181 (3.19)  Time: 0.773s, 1325.04/s  (0.783s, 1308.13/s)  LR: 2.161e-04  Data: 0.010 (0.011)
Train: 419 [1100/1251 ( 88%)]  Loss: 3.288 (3.19)  Time: 0.774s, 1323.66/s  (0.783s, 1308.49/s)  LR: 2.161e-04  Data: 0.009 (0.011)
Train: 419 [1150/1251 ( 92%)]  Loss: 3.275 (3.20)  Time: 0.773s, 1325.32/s  (0.782s, 1308.92/s)  LR: 2.161e-04  Data: 0.009 (0.011)
Train: 419 [1200/1251 ( 96%)]  Loss: 3.276 (3.20)  Time: 0.771s, 1328.05/s  (0.782s, 1309.35/s)  LR: 2.161e-04  Data: 0.010 (0.011)
Train: 419 [1250/1251 (100%)]  Loss: 2.925 (3.19)  Time: 0.766s, 1335.95/s  (0.782s, 1309.46/s)  LR: 2.161e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.509 (1.509)  Loss:  0.7773 (0.7773)  Acc@1: 91.4062 (91.4062)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.194 (0.562)  Loss:  0.8716 (1.2359)  Acc@1: 86.4387 (78.0060)  Acc@5: 97.2877 (94.3380)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-410.pth.tar', 78.23400010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-418.pth.tar', 78.094000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-413.pth.tar', 78.08800005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-411.pth.tar', 78.06999998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-414.pth.tar', 78.05199995361328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-408.pth.tar', 78.03200005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-402.pth.tar', 78.02400002685548)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-419.pth.tar', 78.00600005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-417.pth.tar', 77.99600010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-406.pth.tar', 77.96999997558594)

Train: 420 [   0/1251 (  0%)]  Loss: 3.233 (3.23)  Time: 2.336s,  438.29/s  (2.336s,  438.29/s)  LR: 2.140e-04  Data: 1.600 (1.600)
Train: 420 [  50/1251 (  4%)]  Loss: 2.972 (3.10)  Time: 0.838s, 1221.53/s  (0.829s, 1235.46/s)  LR: 2.140e-04  Data: 0.010 (0.041)
Train: 420 [ 100/1251 (  8%)]  Loss: 3.360 (3.19)  Time: 0.784s, 1305.99/s  (0.805s, 1272.67/s)  LR: 2.140e-04  Data: 0.012 (0.026)
Train: 420 [ 150/1251 ( 12%)]  Loss: 3.494 (3.26)  Time: 0.784s, 1306.61/s  (0.796s, 1286.17/s)  LR: 2.140e-04  Data: 0.009 (0.020)
Train: 420 [ 200/1251 ( 16%)]  Loss: 3.241 (3.26)  Time: 0.779s, 1314.36/s  (0.792s, 1293.51/s)  LR: 2.140e-04  Data: 0.009 (0.018)
Train: 420 [ 250/1251 ( 20%)]  Loss: 3.462 (3.29)  Time: 0.773s, 1324.71/s  (0.789s, 1297.94/s)  LR: 2.140e-04  Data: 0.009 (0.016)
Train: 420 [ 300/1251 ( 24%)]  Loss: 2.846 (3.23)  Time: 0.772s, 1326.14/s  (0.789s, 1297.92/s)  LR: 2.140e-04  Data: 0.009 (0.015)
Train: 420 [ 350/1251 ( 28%)]  Loss: 3.238 (3.23)  Time: 0.774s, 1322.81/s  (0.787s, 1300.38/s)  LR: 2.140e-04  Data: 0.010 (0.014)
Train: 420 [ 400/1251 ( 32%)]  Loss: 3.336 (3.24)  Time: 0.774s, 1323.77/s  (0.787s, 1301.46/s)  LR: 2.140e-04  Data: 0.010 (0.014)
Train: 420 [ 450/1251 ( 36%)]  Loss: 3.517 (3.27)  Time: 0.774s, 1323.02/s  (0.786s, 1302.73/s)  LR: 2.140e-04  Data: 0.010 (0.013)
Train: 420 [ 500/1251 ( 40%)]  Loss: 3.500 (3.29)  Time: 0.774s, 1323.85/s  (0.785s, 1303.74/s)  LR: 2.140e-04  Data: 0.009 (0.013)
Train: 420 [ 550/1251 ( 44%)]  Loss: 3.085 (3.27)  Time: 0.780s, 1313.15/s  (0.786s, 1302.70/s)  LR: 2.140e-04  Data: 0.009 (0.013)
Train: 420 [ 600/1251 ( 48%)]  Loss: 3.333 (3.28)  Time: 0.796s, 1285.64/s  (0.785s, 1303.78/s)  LR: 2.140e-04  Data: 0.009 (0.012)
Train: 420 [ 650/1251 ( 52%)]  Loss: 3.357 (3.28)  Time: 0.775s, 1321.46/s  (0.785s, 1304.49/s)  LR: 2.140e-04  Data: 0.011 (0.012)
Train: 420 [ 700/1251 ( 56%)]  Loss: 3.117 (3.27)  Time: 0.815s, 1256.45/s  (0.787s, 1301.90/s)  LR: 2.140e-04  Data: 0.011 (0.012)
Train: 420 [ 750/1251 ( 60%)]  Loss: 3.268 (3.27)  Time: 0.786s, 1302.07/s  (0.788s, 1300.25/s)  LR: 2.140e-04  Data: 0.015 (0.012)
Train: 420 [ 800/1251 ( 64%)]  Loss: 3.112 (3.26)  Time: 0.780s, 1312.12/s  (0.787s, 1301.25/s)  LR: 2.140e-04  Data: 0.009 (0.012)
Train: 420 [ 850/1251 ( 68%)]  Loss: 3.378 (3.27)  Time: 0.816s, 1255.07/s  (0.787s, 1301.63/s)  LR: 2.140e-04  Data: 0.009 (0.012)
Train: 420 [ 900/1251 ( 72%)]  Loss: 3.445 (3.28)  Time: 0.775s, 1321.27/s  (0.786s, 1302.45/s)  LR: 2.140e-04  Data: 0.009 (0.012)
Train: 420 [ 950/1251 ( 76%)]  Loss: 3.286 (3.28)  Time: 0.775s, 1321.24/s  (0.786s, 1302.79/s)  LR: 2.140e-04  Data: 0.010 (0.012)
Train: 420 [1000/1251 ( 80%)]  Loss: 2.949 (3.26)  Time: 0.776s, 1320.05/s  (0.786s, 1303.27/s)  LR: 2.140e-04  Data: 0.010 (0.011)
Train: 420 [1050/1251 ( 84%)]  Loss: 3.439 (3.27)  Time: 0.774s, 1323.44/s  (0.786s, 1303.56/s)  LR: 2.140e-04  Data: 0.010 (0.011)
Train: 420 [1100/1251 ( 88%)]  Loss: 3.193 (3.27)  Time: 0.773s, 1325.34/s  (0.785s, 1304.20/s)  LR: 2.140e-04  Data: 0.009 (0.011)
Train: 420 [1150/1251 ( 92%)]  Loss: 3.147 (3.26)  Time: 0.791s, 1295.13/s  (0.785s, 1304.53/s)  LR: 2.140e-04  Data: 0.009 (0.011)
Train: 420 [1200/1251 ( 96%)]  Loss: 3.130 (3.26)  Time: 0.848s, 1207.68/s  (0.785s, 1304.65/s)  LR: 2.140e-04  Data: 0.009 (0.011)
Train: 420 [1250/1251 (100%)]  Loss: 3.287 (3.26)  Time: 0.768s, 1333.80/s  (0.785s, 1305.07/s)  LR: 2.140e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.576 (1.576)  Loss:  0.7881 (0.7881)  Acc@1: 91.0156 (91.0156)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.193 (0.558)  Loss:  0.8604 (1.2464)  Acc@1: 86.4387 (77.9940)  Acc@5: 96.8160 (94.2720)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-410.pth.tar', 78.23400010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-418.pth.tar', 78.094000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-413.pth.tar', 78.08800005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-411.pth.tar', 78.06999998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-414.pth.tar', 78.05199995361328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-408.pth.tar', 78.03200005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-402.pth.tar', 78.02400002685548)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-419.pth.tar', 78.00600005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-417.pth.tar', 77.99600010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-420.pth.tar', 77.99399992675781)

Train: 421 [   0/1251 (  0%)]  Loss: 3.169 (3.17)  Time: 2.212s,  462.87/s  (2.212s,  462.87/s)  LR: 2.120e-04  Data: 1.426 (1.426)
Train: 421 [  50/1251 (  4%)]  Loss: 3.262 (3.22)  Time: 0.807s, 1268.29/s  (0.823s, 1244.36/s)  LR: 2.120e-04  Data: 0.010 (0.042)
Train: 421 [ 100/1251 (  8%)]  Loss: 3.111 (3.18)  Time: 0.777s, 1317.86/s  (0.802s, 1277.32/s)  LR: 2.120e-04  Data: 0.010 (0.026)
Train: 421 [ 150/1251 ( 12%)]  Loss: 3.187 (3.18)  Time: 0.773s, 1324.18/s  (0.795s, 1287.49/s)  LR: 2.120e-04  Data: 0.010 (0.021)
Train: 421 [ 200/1251 ( 16%)]  Loss: 3.249 (3.20)  Time: 0.773s, 1325.05/s  (0.791s, 1294.36/s)  LR: 2.120e-04  Data: 0.010 (0.018)
Train: 421 [ 250/1251 ( 20%)]  Loss: 3.568 (3.26)  Time: 0.773s, 1324.73/s  (0.788s, 1298.68/s)  LR: 2.120e-04  Data: 0.010 (0.017)
Train: 421 [ 300/1251 ( 24%)]  Loss: 3.206 (3.25)  Time: 0.776s, 1319.62/s  (0.787s, 1300.94/s)  LR: 2.120e-04  Data: 0.009 (0.015)
Train: 421 [ 350/1251 ( 28%)]  Loss: 2.946 (3.21)  Time: 0.781s, 1311.76/s  (0.786s, 1302.50/s)  LR: 2.120e-04  Data: 0.010 (0.015)
Train: 421 [ 400/1251 ( 32%)]  Loss: 3.184 (3.21)  Time: 0.773s, 1324.41/s  (0.785s, 1303.73/s)  LR: 2.120e-04  Data: 0.009 (0.014)
Train: 421 [ 450/1251 ( 36%)]  Loss: 3.196 (3.21)  Time: 0.773s, 1325.31/s  (0.785s, 1305.15/s)  LR: 2.120e-04  Data: 0.009 (0.014)
Train: 421 [ 500/1251 ( 40%)]  Loss: 2.892 (3.18)  Time: 0.774s, 1323.66/s  (0.784s, 1305.88/s)  LR: 2.120e-04  Data: 0.010 (0.013)
Train: 421 [ 550/1251 ( 44%)]  Loss: 3.310 (3.19)  Time: 0.773s, 1324.91/s  (0.784s, 1306.07/s)  LR: 2.120e-04  Data: 0.009 (0.013)
Train: 421 [ 600/1251 ( 48%)]  Loss: 2.992 (3.17)  Time: 0.783s, 1307.26/s  (0.783s, 1307.01/s)  LR: 2.120e-04  Data: 0.010 (0.013)
Train: 421 [ 650/1251 ( 52%)]  Loss: 2.992 (3.16)  Time: 0.773s, 1325.12/s  (0.783s, 1307.77/s)  LR: 2.120e-04  Data: 0.010 (0.012)
Train: 421 [ 700/1251 ( 56%)]  Loss: 3.196 (3.16)  Time: 0.774s, 1323.84/s  (0.783s, 1308.40/s)  LR: 2.120e-04  Data: 0.009 (0.012)
Train: 421 [ 750/1251 ( 60%)]  Loss: 3.525 (3.19)  Time: 0.777s, 1318.66/s  (0.783s, 1308.58/s)  LR: 2.120e-04  Data: 0.009 (0.012)
Train: 421 [ 800/1251 ( 64%)]  Loss: 3.158 (3.18)  Time: 0.774s, 1323.75/s  (0.782s, 1309.29/s)  LR: 2.120e-04  Data: 0.010 (0.012)
Train: 421 [ 850/1251 ( 68%)]  Loss: 3.281 (3.19)  Time: 0.784s, 1306.76/s  (0.782s, 1309.32/s)  LR: 2.120e-04  Data: 0.013 (0.012)
Train: 421 [ 900/1251 ( 72%)]  Loss: 3.328 (3.20)  Time: 0.885s, 1157.56/s  (0.783s, 1307.85/s)  LR: 2.120e-04  Data: 0.011 (0.012)
Train: 421 [ 950/1251 ( 76%)]  Loss: 3.270 (3.20)  Time: 0.775s, 1321.61/s  (0.785s, 1304.85/s)  LR: 2.120e-04  Data: 0.009 (0.012)
Train: 421 [1000/1251 ( 80%)]  Loss: 2.890 (3.19)  Time: 0.774s, 1323.66/s  (0.784s, 1305.38/s)  LR: 2.120e-04  Data: 0.009 (0.012)
Train: 421 [1050/1251 ( 84%)]  Loss: 3.483 (3.20)  Time: 0.773s, 1325.12/s  (0.785s, 1305.03/s)  LR: 2.120e-04  Data: 0.010 (0.011)
Train: 421 [1100/1251 ( 88%)]  Loss: 3.157 (3.20)  Time: 0.815s, 1256.42/s  (0.785s, 1304.18/s)  LR: 2.120e-04  Data: 0.009 (0.011)
Train: 421 [1150/1251 ( 92%)]  Loss: 3.384 (3.21)  Time: 0.817s, 1254.02/s  (0.787s, 1301.30/s)  LR: 2.120e-04  Data: 0.011 (0.011)
Train: 421 [1200/1251 ( 96%)]  Loss: 3.326 (3.21)  Time: 0.862s, 1188.35/s  (0.787s, 1301.41/s)  LR: 2.120e-04  Data: 0.010 (0.011)
Train: 421 [1250/1251 (100%)]  Loss: 3.061 (3.20)  Time: 0.760s, 1346.88/s  (0.786s, 1302.18/s)  LR: 2.120e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.546 (1.546)  Loss:  0.6484 (0.6484)  Acc@1: 90.5273 (90.5273)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.194 (0.560)  Loss:  0.7632 (1.1202)  Acc@1: 86.9104 (78.1780)  Acc@5: 97.5236 (94.4640)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-410.pth.tar', 78.23400010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-421.pth.tar', 78.17800000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-418.pth.tar', 78.094000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-413.pth.tar', 78.08800005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-411.pth.tar', 78.06999998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-414.pth.tar', 78.05199995361328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-408.pth.tar', 78.03200005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-402.pth.tar', 78.02400002685548)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-419.pth.tar', 78.00600005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-417.pth.tar', 77.99600010498047)

Train: 422 [   0/1251 (  0%)]  Loss: 3.538 (3.54)  Time: 2.210s,  463.36/s  (2.210s,  463.36/s)  LR: 2.099e-04  Data: 1.495 (1.495)
Train: 422 [  50/1251 (  4%)]  Loss: 2.914 (3.23)  Time: 0.818s, 1252.32/s  (0.822s, 1246.03/s)  LR: 2.099e-04  Data: 0.009 (0.046)
Train: 422 [ 100/1251 (  8%)]  Loss: 2.956 (3.14)  Time: 0.772s, 1327.02/s  (0.802s, 1276.25/s)  LR: 2.099e-04  Data: 0.009 (0.028)
Train: 422 [ 150/1251 ( 12%)]  Loss: 2.633 (3.01)  Time: 0.775s, 1321.63/s  (0.795s, 1288.65/s)  LR: 2.099e-04  Data: 0.010 (0.022)
Train: 422 [ 200/1251 ( 16%)]  Loss: 3.417 (3.09)  Time: 0.774s, 1323.06/s  (0.791s, 1293.92/s)  LR: 2.099e-04  Data: 0.010 (0.019)
Train: 422 [ 250/1251 ( 20%)]  Loss: 3.297 (3.13)  Time: 0.776s, 1319.86/s  (0.788s, 1299.00/s)  LR: 2.099e-04  Data: 0.009 (0.017)
Train: 422 [ 300/1251 ( 24%)]  Loss: 3.484 (3.18)  Time: 0.774s, 1322.65/s  (0.787s, 1301.96/s)  LR: 2.099e-04  Data: 0.010 (0.016)
Train: 422 [ 350/1251 ( 28%)]  Loss: 3.340 (3.20)  Time: 0.773s, 1324.26/s  (0.785s, 1303.95/s)  LR: 2.099e-04  Data: 0.010 (0.015)
Train: 422 [ 400/1251 ( 32%)]  Loss: 3.163 (3.19)  Time: 0.774s, 1323.68/s  (0.786s, 1303.49/s)  LR: 2.099e-04  Data: 0.010 (0.014)
Train: 422 [ 450/1251 ( 36%)]  Loss: 3.246 (3.20)  Time: 0.774s, 1323.33/s  (0.785s, 1305.12/s)  LR: 2.099e-04  Data: 0.010 (0.014)
Train: 422 [ 500/1251 ( 40%)]  Loss: 3.090 (3.19)  Time: 0.773s, 1324.71/s  (0.784s, 1305.47/s)  LR: 2.099e-04  Data: 0.010 (0.013)
Train: 422 [ 550/1251 ( 44%)]  Loss: 3.249 (3.19)  Time: 0.777s, 1317.25/s  (0.784s, 1306.29/s)  LR: 2.099e-04  Data: 0.010 (0.013)
Train: 422 [ 600/1251 ( 48%)]  Loss: 3.162 (3.19)  Time: 0.774s, 1322.80/s  (0.783s, 1307.16/s)  LR: 2.099e-04  Data: 0.009 (0.013)
Train: 422 [ 650/1251 ( 52%)]  Loss: 3.495 (3.21)  Time: 0.786s, 1303.37/s  (0.783s, 1308.13/s)  LR: 2.099e-04  Data: 0.010 (0.012)
Train: 422 [ 700/1251 ( 56%)]  Loss: 2.846 (3.19)  Time: 0.826s, 1239.47/s  (0.783s, 1307.02/s)  LR: 2.099e-04  Data: 0.013 (0.012)
Train: 422 [ 750/1251 ( 60%)]  Loss: 3.242 (3.19)  Time: 0.815s, 1257.18/s  (0.785s, 1304.42/s)  LR: 2.099e-04  Data: 0.009 (0.012)
Train: 422 [ 800/1251 ( 64%)]  Loss: 3.217 (3.19)  Time: 0.774s, 1322.32/s  (0.785s, 1303.98/s)  LR: 2.099e-04  Data: 0.010 (0.012)
Train: 422 [ 850/1251 ( 68%)]  Loss: 3.271 (3.20)  Time: 0.784s, 1306.65/s  (0.786s, 1303.28/s)  LR: 2.099e-04  Data: 0.012 (0.012)
Train: 422 [ 900/1251 ( 72%)]  Loss: 2.778 (3.18)  Time: 0.774s, 1322.33/s  (0.786s, 1303.48/s)  LR: 2.099e-04  Data: 0.010 (0.012)
Train: 422 [ 950/1251 ( 76%)]  Loss: 3.310 (3.18)  Time: 0.814s, 1258.27/s  (0.786s, 1302.59/s)  LR: 2.099e-04  Data: 0.010 (0.012)
Train: 422 [1000/1251 ( 80%)]  Loss: 3.213 (3.18)  Time: 0.776s, 1320.21/s  (0.786s, 1302.15/s)  LR: 2.099e-04  Data: 0.010 (0.012)
Train: 422 [1050/1251 ( 84%)]  Loss: 3.135 (3.18)  Time: 0.785s, 1305.09/s  (0.786s, 1302.49/s)  LR: 2.099e-04  Data: 0.010 (0.012)
Train: 422 [1100/1251 ( 88%)]  Loss: 3.212 (3.18)  Time: 0.774s, 1322.98/s  (0.786s, 1302.87/s)  LR: 2.099e-04  Data: 0.011 (0.012)
Train: 422 [1150/1251 ( 92%)]  Loss: 3.494 (3.20)  Time: 0.776s, 1319.29/s  (0.786s, 1303.12/s)  LR: 2.099e-04  Data: 0.009 (0.012)
Train: 422 [1200/1251 ( 96%)]  Loss: 3.438 (3.21)  Time: 0.780s, 1312.67/s  (0.786s, 1303.33/s)  LR: 2.099e-04  Data: 0.011 (0.012)
Train: 422 [1250/1251 (100%)]  Loss: 3.335 (3.21)  Time: 0.761s, 1346.20/s  (0.785s, 1303.85/s)  LR: 2.099e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.502 (1.502)  Loss:  0.6582 (0.6582)  Acc@1: 91.1133 (91.1133)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.194 (0.566)  Loss:  0.7979 (1.1447)  Acc@1: 87.1462 (78.3440)  Acc@5: 96.9340 (94.3820)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-422.pth.tar', 78.34400010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-410.pth.tar', 78.23400010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-421.pth.tar', 78.17800000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-418.pth.tar', 78.094000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-413.pth.tar', 78.08800005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-411.pth.tar', 78.06999998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-414.pth.tar', 78.05199995361328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-408.pth.tar', 78.03200005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-402.pth.tar', 78.02400002685548)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-419.pth.tar', 78.00600005615235)

Train: 423 [   0/1251 (  0%)]  Loss: 3.039 (3.04)  Time: 2.447s,  418.39/s  (2.447s,  418.39/s)  LR: 2.078e-04  Data: 1.663 (1.663)
Train: 423 [  50/1251 (  4%)]  Loss: 3.043 (3.04)  Time: 0.773s, 1325.29/s  (0.816s, 1255.36/s)  LR: 2.078e-04  Data: 0.009 (0.042)
Train: 423 [ 100/1251 (  8%)]  Loss: 3.278 (3.12)  Time: 0.777s, 1318.64/s  (0.799s, 1282.07/s)  LR: 2.078e-04  Data: 0.010 (0.026)
Train: 423 [ 150/1251 ( 12%)]  Loss: 3.236 (3.15)  Time: 0.797s, 1285.40/s  (0.793s, 1292.01/s)  LR: 2.078e-04  Data: 0.009 (0.021)
Train: 423 [ 200/1251 ( 16%)]  Loss: 3.353 (3.19)  Time: 0.774s, 1322.86/s  (0.790s, 1295.76/s)  LR: 2.078e-04  Data: 0.009 (0.018)
Train: 423 [ 250/1251 ( 20%)]  Loss: 2.964 (3.15)  Time: 0.772s, 1326.38/s  (0.788s, 1299.89/s)  LR: 2.078e-04  Data: 0.009 (0.016)
Train: 423 [ 300/1251 ( 24%)]  Loss: 3.370 (3.18)  Time: 0.773s, 1324.97/s  (0.786s, 1302.06/s)  LR: 2.078e-04  Data: 0.009 (0.015)
Train: 423 [ 350/1251 ( 28%)]  Loss: 3.435 (3.21)  Time: 0.778s, 1316.99/s  (0.785s, 1304.02/s)  LR: 2.078e-04  Data: 0.009 (0.015)
Train: 423 [ 400/1251 ( 32%)]  Loss: 2.987 (3.19)  Time: 0.820s, 1248.02/s  (0.789s, 1298.63/s)  LR: 2.078e-04  Data: 0.011 (0.014)
Train: 423 [ 450/1251 ( 36%)]  Loss: 3.275 (3.20)  Time: 0.814s, 1258.23/s  (0.789s, 1297.66/s)  LR: 2.078e-04  Data: 0.009 (0.014)
Train: 423 [ 500/1251 ( 40%)]  Loss: 2.865 (3.17)  Time: 0.772s, 1326.34/s  (0.789s, 1297.05/s)  LR: 2.078e-04  Data: 0.010 (0.013)
Train: 423 [ 550/1251 ( 44%)]  Loss: 2.988 (3.15)  Time: 0.813s, 1259.34/s  (0.789s, 1297.73/s)  LR: 2.078e-04  Data: 0.009 (0.013)
Train: 423 [ 600/1251 ( 48%)]  Loss: 3.408 (3.17)  Time: 0.772s, 1325.84/s  (0.788s, 1298.86/s)  LR: 2.078e-04  Data: 0.009 (0.013)
Train: 423 [ 650/1251 ( 52%)]  Loss: 3.338 (3.18)  Time: 0.774s, 1323.79/s  (0.788s, 1299.84/s)  LR: 2.078e-04  Data: 0.010 (0.012)
Train: 423 [ 700/1251 ( 56%)]  Loss: 3.199 (3.19)  Time: 0.806s, 1269.84/s  (0.788s, 1298.80/s)  LR: 2.078e-04  Data: 0.010 (0.012)
Train: 423 [ 750/1251 ( 60%)]  Loss: 3.617 (3.21)  Time: 0.775s, 1321.49/s  (0.788s, 1299.64/s)  LR: 2.078e-04  Data: 0.009 (0.012)
Train: 423 [ 800/1251 ( 64%)]  Loss: 3.341 (3.22)  Time: 0.775s, 1320.57/s  (0.787s, 1300.73/s)  LR: 2.078e-04  Data: 0.011 (0.012)
Train: 423 [ 850/1251 ( 68%)]  Loss: 3.289 (3.22)  Time: 0.775s, 1321.75/s  (0.787s, 1301.48/s)  LR: 2.078e-04  Data: 0.012 (0.012)
Train: 423 [ 900/1251 ( 72%)]  Loss: 3.064 (3.22)  Time: 0.773s, 1324.57/s  (0.786s, 1302.34/s)  LR: 2.078e-04  Data: 0.009 (0.012)
Train: 423 [ 950/1251 ( 76%)]  Loss: 3.310 (3.22)  Time: 0.771s, 1328.61/s  (0.786s, 1302.92/s)  LR: 2.078e-04  Data: 0.009 (0.012)
Train: 423 [1000/1251 ( 80%)]  Loss: 3.392 (3.23)  Time: 0.874s, 1171.97/s  (0.786s, 1303.20/s)  LR: 2.078e-04  Data: 0.010 (0.011)
Train: 423 [1050/1251 ( 84%)]  Loss: 3.100 (3.22)  Time: 0.815s, 1256.55/s  (0.786s, 1302.59/s)  LR: 2.078e-04  Data: 0.009 (0.011)
Train: 423 [1100/1251 ( 88%)]  Loss: 3.087 (3.22)  Time: 0.771s, 1327.58/s  (0.786s, 1303.06/s)  LR: 2.078e-04  Data: 0.010 (0.011)
Train: 423 [1150/1251 ( 92%)]  Loss: 3.415 (3.22)  Time: 0.773s, 1325.37/s  (0.785s, 1303.78/s)  LR: 2.078e-04  Data: 0.009 (0.011)
Train: 423 [1200/1251 ( 96%)]  Loss: 3.256 (3.23)  Time: 0.775s, 1322.02/s  (0.785s, 1304.21/s)  LR: 2.078e-04  Data: 0.010 (0.011)
Train: 423 [1250/1251 (100%)]  Loss: 3.062 (3.22)  Time: 0.763s, 1342.87/s  (0.785s, 1304.82/s)  LR: 2.078e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.533 (1.533)  Loss:  0.6782 (0.6782)  Acc@1: 90.0391 (90.0391)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.194 (0.583)  Loss:  0.7515 (1.1600)  Acc@1: 86.0849 (78.2380)  Acc@5: 97.5236 (94.2960)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-422.pth.tar', 78.34400010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-423.pth.tar', 78.23800016113282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-410.pth.tar', 78.23400010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-421.pth.tar', 78.17800000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-418.pth.tar', 78.094000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-413.pth.tar', 78.08800005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-411.pth.tar', 78.06999998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-414.pth.tar', 78.05199995361328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-408.pth.tar', 78.03200005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-402.pth.tar', 78.02400002685548)

Train: 424 [   0/1251 (  0%)]  Loss: 3.284 (3.28)  Time: 2.360s,  433.93/s  (2.360s,  433.93/s)  LR: 2.057e-04  Data: 1.635 (1.635)
Train: 424 [  50/1251 (  4%)]  Loss: 3.010 (3.15)  Time: 0.775s, 1320.62/s  (0.823s, 1244.35/s)  LR: 2.057e-04  Data: 0.009 (0.045)
Train: 424 [ 100/1251 (  8%)]  Loss: 3.061 (3.12)  Time: 0.772s, 1325.63/s  (0.802s, 1277.23/s)  LR: 2.057e-04  Data: 0.010 (0.028)
Train: 424 [ 150/1251 ( 12%)]  Loss: 3.400 (3.19)  Time: 0.780s, 1313.57/s  (0.794s, 1289.00/s)  LR: 2.057e-04  Data: 0.009 (0.022)
Train: 424 [ 200/1251 ( 16%)]  Loss: 3.118 (3.17)  Time: 0.774s, 1322.98/s  (0.792s, 1293.06/s)  LR: 2.057e-04  Data: 0.010 (0.019)
Train: 424 [ 250/1251 ( 20%)]  Loss: 3.315 (3.20)  Time: 0.773s, 1325.31/s  (0.790s, 1295.58/s)  LR: 2.057e-04  Data: 0.010 (0.017)
Train: 424 [ 300/1251 ( 24%)]  Loss: 2.855 (3.15)  Time: 0.773s, 1324.04/s  (0.788s, 1299.00/s)  LR: 2.057e-04  Data: 0.010 (0.016)
Train: 424 [ 350/1251 ( 28%)]  Loss: 3.225 (3.16)  Time: 0.777s, 1317.81/s  (0.787s, 1301.31/s)  LR: 2.057e-04  Data: 0.009 (0.015)
Train: 424 [ 400/1251 ( 32%)]  Loss: 3.351 (3.18)  Time: 0.774s, 1323.60/s  (0.786s, 1302.48/s)  LR: 2.057e-04  Data: 0.009 (0.014)
Train: 424 [ 450/1251 ( 36%)]  Loss: 2.884 (3.15)  Time: 0.772s, 1326.16/s  (0.785s, 1304.19/s)  LR: 2.057e-04  Data: 0.009 (0.014)
Train: 424 [ 500/1251 ( 40%)]  Loss: 2.887 (3.13)  Time: 0.773s, 1324.98/s  (0.785s, 1304.85/s)  LR: 2.057e-04  Data: 0.009 (0.013)
Train: 424 [ 550/1251 ( 44%)]  Loss: 3.477 (3.16)  Time: 0.774s, 1322.36/s  (0.784s, 1305.87/s)  LR: 2.057e-04  Data: 0.010 (0.013)
Train: 424 [ 600/1251 ( 48%)]  Loss: 3.181 (3.16)  Time: 0.789s, 1298.14/s  (0.784s, 1306.82/s)  LR: 2.057e-04  Data: 0.010 (0.013)
Train: 424 [ 650/1251 ( 52%)]  Loss: 3.491 (3.18)  Time: 0.792s, 1293.22/s  (0.784s, 1306.58/s)  LR: 2.057e-04  Data: 0.013 (0.013)
Train: 424 [ 700/1251 ( 56%)]  Loss: 3.649 (3.21)  Time: 0.783s, 1307.34/s  (0.784s, 1306.77/s)  LR: 2.057e-04  Data: 0.010 (0.012)
Train: 424 [ 750/1251 ( 60%)]  Loss: 2.920 (3.19)  Time: 0.773s, 1323.99/s  (0.784s, 1306.56/s)  LR: 2.057e-04  Data: 0.010 (0.012)
Train: 424 [ 800/1251 ( 64%)]  Loss: 3.396 (3.21)  Time: 0.780s, 1313.49/s  (0.783s, 1307.02/s)  LR: 2.057e-04  Data: 0.015 (0.012)
Train: 424 [ 850/1251 ( 68%)]  Loss: 3.342 (3.21)  Time: 0.773s, 1324.10/s  (0.784s, 1306.09/s)  LR: 2.057e-04  Data: 0.010 (0.012)
Train: 424 [ 900/1251 ( 72%)]  Loss: 3.194 (3.21)  Time: 0.774s, 1323.68/s  (0.784s, 1305.84/s)  LR: 2.057e-04  Data: 0.009 (0.012)
Train: 424 [ 950/1251 ( 76%)]  Loss: 3.362 (3.22)  Time: 0.771s, 1327.96/s  (0.784s, 1306.21/s)  LR: 2.057e-04  Data: 0.010 (0.012)
Train: 424 [1000/1251 ( 80%)]  Loss: 3.288 (3.22)  Time: 0.772s, 1325.79/s  (0.784s, 1306.85/s)  LR: 2.057e-04  Data: 0.009 (0.012)
Train: 424 [1050/1251 ( 84%)]  Loss: 3.288 (3.23)  Time: 0.775s, 1320.80/s  (0.784s, 1306.88/s)  LR: 2.057e-04  Data: 0.010 (0.012)
Train: 424 [1100/1251 ( 88%)]  Loss: 3.170 (3.22)  Time: 0.814s, 1257.24/s  (0.784s, 1305.88/s)  LR: 2.057e-04  Data: 0.011 (0.011)
Train: 424 [1150/1251 ( 92%)]  Loss: 3.326 (3.23)  Time: 0.778s, 1315.65/s  (0.784s, 1305.33/s)  LR: 2.057e-04  Data: 0.009 (0.011)
Train: 424 [1200/1251 ( 96%)]  Loss: 3.049 (3.22)  Time: 0.774s, 1323.21/s  (0.784s, 1305.60/s)  LR: 2.057e-04  Data: 0.009 (0.011)
Train: 424 [1250/1251 (100%)]  Loss: 3.362 (3.23)  Time: 0.759s, 1349.39/s  (0.784s, 1306.04/s)  LR: 2.057e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.532 (1.532)  Loss:  0.8042 (0.8042)  Acc@1: 91.0156 (91.0156)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.194 (0.566)  Loss:  0.7739 (1.1827)  Acc@1: 86.9104 (78.3180)  Acc@5: 97.4057 (94.3720)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-422.pth.tar', 78.34400010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-424.pth.tar', 78.31800000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-423.pth.tar', 78.23800016113282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-410.pth.tar', 78.23400010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-421.pth.tar', 78.17800000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-418.pth.tar', 78.094000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-413.pth.tar', 78.08800005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-411.pth.tar', 78.06999998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-414.pth.tar', 78.05199995361328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-408.pth.tar', 78.03200005615234)

Train: 425 [   0/1251 (  0%)]  Loss: 3.195 (3.19)  Time: 2.265s,  452.02/s  (2.265s,  452.02/s)  LR: 2.037e-04  Data: 1.528 (1.528)
Train: 425 [  50/1251 (  4%)]  Loss: 3.403 (3.30)  Time: 0.772s, 1326.91/s  (0.813s, 1259.05/s)  LR: 2.037e-04  Data: 0.009 (0.042)
Train: 425 [ 100/1251 (  8%)]  Loss: 2.916 (3.17)  Time: 0.773s, 1324.79/s  (0.796s, 1286.97/s)  LR: 2.037e-04  Data: 0.009 (0.026)
Train: 425 [ 150/1251 ( 12%)]  Loss: 3.203 (3.18)  Time: 0.776s, 1319.73/s  (0.794s, 1289.06/s)  LR: 2.037e-04  Data: 0.011 (0.021)
Train: 425 [ 200/1251 ( 16%)]  Loss: 3.141 (3.17)  Time: 0.779s, 1313.84/s  (0.793s, 1291.99/s)  LR: 2.037e-04  Data: 0.009 (0.018)
Train: 425 [ 250/1251 ( 20%)]  Loss: 3.313 (3.19)  Time: 0.806s, 1270.66/s  (0.790s, 1296.45/s)  LR: 2.037e-04  Data: 0.009 (0.016)
Train: 425 [ 300/1251 ( 24%)]  Loss: 3.133 (3.19)  Time: 0.773s, 1324.53/s  (0.789s, 1298.39/s)  LR: 2.037e-04  Data: 0.010 (0.015)
Train: 425 [ 350/1251 ( 28%)]  Loss: 3.423 (3.22)  Time: 0.771s, 1328.22/s  (0.788s, 1299.67/s)  LR: 2.037e-04  Data: 0.009 (0.014)
Train: 425 [ 400/1251 ( 32%)]  Loss: 3.162 (3.21)  Time: 0.777s, 1317.47/s  (0.787s, 1300.73/s)  LR: 2.037e-04  Data: 0.009 (0.014)
Train: 425 [ 450/1251 ( 36%)]  Loss: 3.280 (3.22)  Time: 0.773s, 1324.11/s  (0.786s, 1302.05/s)  LR: 2.037e-04  Data: 0.010 (0.013)
Train: 425 [ 500/1251 ( 40%)]  Loss: 3.516 (3.24)  Time: 0.773s, 1324.54/s  (0.786s, 1302.12/s)  LR: 2.037e-04  Data: 0.009 (0.013)
Train: 425 [ 550/1251 ( 44%)]  Loss: 3.270 (3.25)  Time: 0.775s, 1321.41/s  (0.786s, 1303.08/s)  LR: 2.037e-04  Data: 0.010 (0.013)
Train: 425 [ 600/1251 ( 48%)]  Loss: 3.154 (3.24)  Time: 0.771s, 1328.60/s  (0.786s, 1303.48/s)  LR: 2.037e-04  Data: 0.009 (0.013)
Train: 425 [ 650/1251 ( 52%)]  Loss: 3.195 (3.24)  Time: 0.775s, 1321.24/s  (0.785s, 1304.23/s)  LR: 2.037e-04  Data: 0.010 (0.012)
Train: 425 [ 700/1251 ( 56%)]  Loss: 3.137 (3.23)  Time: 0.772s, 1325.71/s  (0.785s, 1303.75/s)  LR: 2.037e-04  Data: 0.010 (0.012)
Train: 425 [ 750/1251 ( 60%)]  Loss: 3.249 (3.23)  Time: 0.776s, 1319.11/s  (0.786s, 1302.90/s)  LR: 2.037e-04  Data: 0.009 (0.012)
Train: 425 [ 800/1251 ( 64%)]  Loss: 3.386 (3.24)  Time: 0.778s, 1316.24/s  (0.786s, 1303.40/s)  LR: 2.037e-04  Data: 0.009 (0.012)
Train: 425 [ 850/1251 ( 68%)]  Loss: 3.060 (3.23)  Time: 0.784s, 1306.44/s  (0.785s, 1303.80/s)  LR: 2.037e-04  Data: 0.009 (0.012)
Train: 425 [ 900/1251 ( 72%)]  Loss: 2.974 (3.22)  Time: 0.854s, 1199.73/s  (0.785s, 1304.37/s)  LR: 2.037e-04  Data: 0.010 (0.012)
Train: 425 [ 950/1251 ( 76%)]  Loss: 3.284 (3.22)  Time: 0.811s, 1263.26/s  (0.785s, 1304.23/s)  LR: 2.037e-04  Data: 0.009 (0.011)
Train: 425 [1000/1251 ( 80%)]  Loss: 3.149 (3.22)  Time: 0.772s, 1327.22/s  (0.786s, 1302.67/s)  LR: 2.037e-04  Data: 0.009 (0.011)
Train: 425 [1050/1251 ( 84%)]  Loss: 3.214 (3.22)  Time: 0.774s, 1323.65/s  (0.786s, 1303.08/s)  LR: 2.037e-04  Data: 0.009 (0.011)
Train: 425 [1100/1251 ( 88%)]  Loss: 3.058 (3.21)  Time: 0.779s, 1314.49/s  (0.786s, 1303.21/s)  LR: 2.037e-04  Data: 0.010 (0.011)
Train: 425 [1150/1251 ( 92%)]  Loss: 3.294 (3.21)  Time: 0.775s, 1320.57/s  (0.785s, 1303.83/s)  LR: 2.037e-04  Data: 0.009 (0.011)
Train: 425 [1200/1251 ( 96%)]  Loss: 3.190 (3.21)  Time: 0.773s, 1325.36/s  (0.785s, 1303.92/s)  LR: 2.037e-04  Data: 0.009 (0.011)
Train: 425 [1250/1251 (100%)]  Loss: 3.259 (3.21)  Time: 0.760s, 1347.41/s  (0.785s, 1304.17/s)  LR: 2.037e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.574 (1.574)  Loss:  0.6768 (0.6768)  Acc@1: 91.6992 (91.6992)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.194 (0.568)  Loss:  0.7095 (1.1398)  Acc@1: 87.9717 (78.3380)  Acc@5: 97.8774 (94.3420)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-422.pth.tar', 78.34400010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-425.pth.tar', 78.3380000756836)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-424.pth.tar', 78.31800000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-423.pth.tar', 78.23800016113282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-410.pth.tar', 78.23400010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-421.pth.tar', 78.17800000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-418.pth.tar', 78.094000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-413.pth.tar', 78.08800005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-411.pth.tar', 78.06999998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-414.pth.tar', 78.05199995361328)

Train: 426 [   0/1251 (  0%)]  Loss: 3.279 (3.28)  Time: 2.248s,  455.42/s  (2.248s,  455.42/s)  LR: 2.016e-04  Data: 1.521 (1.521)
Train: 426 [  50/1251 (  4%)]  Loss: 3.463 (3.37)  Time: 0.775s, 1321.57/s  (0.820s, 1248.93/s)  LR: 2.016e-04  Data: 0.010 (0.046)
Train: 426 [ 100/1251 (  8%)]  Loss: 3.157 (3.30)  Time: 0.781s, 1311.83/s  (0.802s, 1276.50/s)  LR: 2.016e-04  Data: 0.010 (0.028)
Train: 426 [ 150/1251 ( 12%)]  Loss: 3.088 (3.25)  Time: 0.775s, 1321.95/s  (0.796s, 1286.79/s)  LR: 2.016e-04  Data: 0.010 (0.022)
Train: 426 [ 200/1251 ( 16%)]  Loss: 3.154 (3.23)  Time: 0.778s, 1315.62/s  (0.792s, 1293.10/s)  LR: 2.016e-04  Data: 0.010 (0.019)
Train: 426 [ 250/1251 ( 20%)]  Loss: 2.941 (3.18)  Time: 0.773s, 1324.05/s  (0.789s, 1297.54/s)  LR: 2.016e-04  Data: 0.010 (0.017)
Train: 426 [ 300/1251 ( 24%)]  Loss: 3.229 (3.19)  Time: 0.777s, 1318.18/s  (0.787s, 1300.63/s)  LR: 2.016e-04  Data: 0.009 (0.016)
Train: 426 [ 350/1251 ( 28%)]  Loss: 3.174 (3.19)  Time: 0.774s, 1323.68/s  (0.788s, 1298.89/s)  LR: 2.016e-04  Data: 0.010 (0.015)
Train: 426 [ 400/1251 ( 32%)]  Loss: 3.184 (3.19)  Time: 0.776s, 1319.72/s  (0.789s, 1297.81/s)  LR: 2.016e-04  Data: 0.010 (0.014)
Train: 426 [ 450/1251 ( 36%)]  Loss: 3.230 (3.19)  Time: 0.773s, 1323.99/s  (0.788s, 1299.06/s)  LR: 2.016e-04  Data: 0.010 (0.014)
Train: 426 [ 500/1251 ( 40%)]  Loss: 3.549 (3.22)  Time: 0.781s, 1311.77/s  (0.787s, 1300.37/s)  LR: 2.016e-04  Data: 0.012 (0.014)
Train: 426 [ 550/1251 ( 44%)]  Loss: 3.080 (3.21)  Time: 0.780s, 1312.54/s  (0.787s, 1301.83/s)  LR: 2.016e-04  Data: 0.009 (0.013)
Train: 426 [ 600/1251 ( 48%)]  Loss: 3.501 (3.23)  Time: 0.774s, 1322.68/s  (0.786s, 1302.32/s)  LR: 2.016e-04  Data: 0.009 (0.013)
Train: 426 [ 650/1251 ( 52%)]  Loss: 3.305 (3.24)  Time: 0.826s, 1239.67/s  (0.787s, 1301.09/s)  LR: 2.016e-04  Data: 0.013 (0.013)
Train: 426 [ 700/1251 ( 56%)]  Loss: 3.430 (3.25)  Time: 0.782s, 1309.86/s  (0.789s, 1298.27/s)  LR: 2.016e-04  Data: 0.010 (0.013)
Train: 426 [ 750/1251 ( 60%)]  Loss: 3.195 (3.25)  Time: 0.777s, 1317.72/s  (0.789s, 1298.30/s)  LR: 2.016e-04  Data: 0.010 (0.012)
Train: 426 [ 800/1251 ( 64%)]  Loss: 3.221 (3.25)  Time: 0.772s, 1327.26/s  (0.788s, 1298.94/s)  LR: 2.016e-04  Data: 0.009 (0.012)
Train: 426 [ 850/1251 ( 68%)]  Loss: 3.081 (3.24)  Time: 0.772s, 1326.82/s  (0.788s, 1299.60/s)  LR: 2.016e-04  Data: 0.009 (0.012)
Train: 426 [ 900/1251 ( 72%)]  Loss: 3.290 (3.24)  Time: 0.773s, 1324.79/s  (0.787s, 1300.60/s)  LR: 2.016e-04  Data: 0.010 (0.012)
Train: 426 [ 950/1251 ( 76%)]  Loss: 2.953 (3.23)  Time: 0.774s, 1323.27/s  (0.787s, 1301.39/s)  LR: 2.016e-04  Data: 0.009 (0.012)
Train: 426 [1000/1251 ( 80%)]  Loss: 3.262 (3.23)  Time: 0.773s, 1325.36/s  (0.786s, 1302.17/s)  LR: 2.016e-04  Data: 0.009 (0.012)
Train: 426 [1050/1251 ( 84%)]  Loss: 3.236 (3.23)  Time: 0.829s, 1234.58/s  (0.786s, 1302.54/s)  LR: 2.016e-04  Data: 0.014 (0.012)
Train: 426 [1100/1251 ( 88%)]  Loss: 3.477 (3.24)  Time: 0.777s, 1318.69/s  (0.786s, 1301.98/s)  LR: 2.016e-04  Data: 0.009 (0.012)
Train: 426 [1150/1251 ( 92%)]  Loss: 3.135 (3.23)  Time: 0.776s, 1319.18/s  (0.786s, 1302.61/s)  LR: 2.016e-04  Data: 0.009 (0.012)
Train: 426 [1200/1251 ( 96%)]  Loss: 3.064 (3.23)  Time: 0.776s, 1320.35/s  (0.786s, 1302.64/s)  LR: 2.016e-04  Data: 0.010 (0.011)
Train: 426 [1250/1251 (100%)]  Loss: 2.777 (3.21)  Time: 0.791s, 1293.80/s  (0.786s, 1302.82/s)  LR: 2.016e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.609 (1.609)  Loss:  0.6206 (0.6206)  Acc@1: 91.0156 (91.0156)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.194 (0.579)  Loss:  0.7690 (1.1059)  Acc@1: 85.9670 (78.4660)  Acc@5: 97.6415 (94.3240)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-426.pth.tar', 78.46600010986329)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-422.pth.tar', 78.34400010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-425.pth.tar', 78.3380000756836)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-424.pth.tar', 78.31800000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-423.pth.tar', 78.23800016113282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-410.pth.tar', 78.23400010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-421.pth.tar', 78.17800000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-418.pth.tar', 78.094000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-413.pth.tar', 78.08800005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-411.pth.tar', 78.06999998046875)

Train: 427 [   0/1251 (  0%)]  Loss: 3.101 (3.10)  Time: 2.183s,  469.03/s  (2.183s,  469.03/s)  LR: 1.996e-04  Data: 1.450 (1.450)
Train: 427 [  50/1251 (  4%)]  Loss: 2.973 (3.04)  Time: 0.772s, 1325.77/s  (0.812s, 1260.74/s)  LR: 1.996e-04  Data: 0.010 (0.044)
Train: 427 [ 100/1251 (  8%)]  Loss: 3.219 (3.10)  Time: 0.781s, 1311.90/s  (0.796s, 1286.36/s)  LR: 1.996e-04  Data: 0.009 (0.027)
Train: 427 [ 150/1251 ( 12%)]  Loss: 2.977 (3.07)  Time: 0.853s, 1200.80/s  (0.791s, 1294.75/s)  LR: 1.996e-04  Data: 0.010 (0.022)
Train: 427 [ 200/1251 ( 16%)]  Loss: 3.406 (3.14)  Time: 0.806s, 1270.01/s  (0.791s, 1294.71/s)  LR: 1.996e-04  Data: 0.010 (0.019)
Train: 427 [ 250/1251 ( 20%)]  Loss: 3.391 (3.18)  Time: 0.826s, 1240.27/s  (0.789s, 1297.53/s)  LR: 1.996e-04  Data: 0.010 (0.017)
Train: 427 [ 300/1251 ( 24%)]  Loss: 3.550 (3.23)  Time: 0.773s, 1324.99/s  (0.788s, 1299.09/s)  LR: 1.996e-04  Data: 0.009 (0.016)
Train: 427 [ 350/1251 ( 28%)]  Loss: 2.938 (3.19)  Time: 0.770s, 1330.62/s  (0.792s, 1293.21/s)  LR: 1.996e-04  Data: 0.010 (0.015)
Train: 427 [ 400/1251 ( 32%)]  Loss: 2.995 (3.17)  Time: 0.776s, 1319.73/s  (0.790s, 1296.04/s)  LR: 1.996e-04  Data: 0.009 (0.014)
Train: 427 [ 450/1251 ( 36%)]  Loss: 3.265 (3.18)  Time: 0.851s, 1202.89/s  (0.789s, 1298.20/s)  LR: 1.996e-04  Data: 0.010 (0.014)
Train: 427 [ 500/1251 ( 40%)]  Loss: 3.104 (3.17)  Time: 0.785s, 1304.32/s  (0.788s, 1299.89/s)  LR: 1.996e-04  Data: 0.010 (0.013)
Train: 427 [ 550/1251 ( 44%)]  Loss: 3.333 (3.19)  Time: 0.775s, 1321.50/s  (0.787s, 1300.91/s)  LR: 1.996e-04  Data: 0.010 (0.013)
Train: 427 [ 600/1251 ( 48%)]  Loss: 3.314 (3.20)  Time: 0.772s, 1326.75/s  (0.786s, 1302.47/s)  LR: 1.996e-04  Data: 0.010 (0.013)
Train: 427 [ 650/1251 ( 52%)]  Loss: 3.575 (3.22)  Time: 0.779s, 1313.70/s  (0.786s, 1302.94/s)  LR: 1.996e-04  Data: 0.010 (0.013)
Train: 427 [ 700/1251 ( 56%)]  Loss: 3.200 (3.22)  Time: 0.815s, 1256.62/s  (0.787s, 1301.47/s)  LR: 1.996e-04  Data: 0.010 (0.012)
Train: 427 [ 750/1251 ( 60%)]  Loss: 3.216 (3.22)  Time: 0.790s, 1297.01/s  (0.786s, 1302.18/s)  LR: 1.996e-04  Data: 0.010 (0.012)
Train: 427 [ 800/1251 ( 64%)]  Loss: 3.313 (3.23)  Time: 0.805s, 1272.31/s  (0.786s, 1302.12/s)  LR: 1.996e-04  Data: 0.010 (0.012)
Train: 427 [ 850/1251 ( 68%)]  Loss: 3.382 (3.24)  Time: 0.772s, 1325.65/s  (0.786s, 1302.40/s)  LR: 1.996e-04  Data: 0.010 (0.012)
Train: 427 [ 900/1251 ( 72%)]  Loss: 3.421 (3.25)  Time: 0.772s, 1326.08/s  (0.786s, 1302.93/s)  LR: 1.996e-04  Data: 0.010 (0.012)
Train: 427 [ 950/1251 ( 76%)]  Loss: 3.332 (3.25)  Time: 0.778s, 1315.42/s  (0.785s, 1303.93/s)  LR: 1.996e-04  Data: 0.009 (0.012)
Train: 427 [1000/1251 ( 80%)]  Loss: 3.326 (3.25)  Time: 0.829s, 1235.29/s  (0.786s, 1303.33/s)  LR: 1.996e-04  Data: 0.010 (0.012)
Train: 427 [1050/1251 ( 84%)]  Loss: 3.667 (3.27)  Time: 0.780s, 1312.77/s  (0.785s, 1303.84/s)  LR: 1.996e-04  Data: 0.009 (0.011)
Train: 427 [1100/1251 ( 88%)]  Loss: 3.465 (3.28)  Time: 0.809s, 1265.95/s  (0.786s, 1302.07/s)  LR: 1.996e-04  Data: 0.009 (0.011)
Train: 427 [1150/1251 ( 92%)]  Loss: 3.567 (3.29)  Time: 0.771s, 1327.90/s  (0.786s, 1302.61/s)  LR: 1.996e-04  Data: 0.010 (0.011)
Train: 427 [1200/1251 ( 96%)]  Loss: 3.160 (3.29)  Time: 0.775s, 1321.08/s  (0.786s, 1302.72/s)  LR: 1.996e-04  Data: 0.010 (0.011)
Train: 427 [1250/1251 (100%)]  Loss: 3.192 (3.28)  Time: 0.762s, 1344.53/s  (0.786s, 1302.99/s)  LR: 1.996e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.531 (1.531)  Loss:  0.6484 (0.6484)  Acc@1: 91.6016 (91.6016)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.194 (0.597)  Loss:  0.7363 (1.1242)  Acc@1: 86.0849 (78.3440)  Acc@5: 97.0519 (94.2500)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-426.pth.tar', 78.46600010986329)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-422.pth.tar', 78.34400010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-427.pth.tar', 78.34400003173828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-425.pth.tar', 78.3380000756836)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-424.pth.tar', 78.31800000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-423.pth.tar', 78.23800016113282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-410.pth.tar', 78.23400010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-421.pth.tar', 78.17800000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-418.pth.tar', 78.094000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-413.pth.tar', 78.08800005615234)

Train: 428 [   0/1251 (  0%)]  Loss: 3.241 (3.24)  Time: 2.244s,  456.32/s  (2.244s,  456.32/s)  LR: 1.975e-04  Data: 1.511 (1.511)
Train: 428 [  50/1251 (  4%)]  Loss: 3.375 (3.31)  Time: 0.782s, 1309.70/s  (0.817s, 1253.53/s)  LR: 1.975e-04  Data: 0.010 (0.044)
Train: 428 [ 100/1251 (  8%)]  Loss: 3.197 (3.27)  Time: 0.774s, 1322.95/s  (0.806s, 1270.47/s)  LR: 1.975e-04  Data: 0.010 (0.028)
Train: 428 [ 150/1251 ( 12%)]  Loss: 3.150 (3.24)  Time: 0.772s, 1325.66/s  (0.797s, 1284.04/s)  LR: 1.975e-04  Data: 0.011 (0.022)
Train: 428 [ 200/1251 ( 16%)]  Loss: 3.247 (3.24)  Time: 0.779s, 1314.45/s  (0.793s, 1291.51/s)  LR: 1.975e-04  Data: 0.009 (0.019)
Train: 428 [ 250/1251 ( 20%)]  Loss: 3.099 (3.22)  Time: 0.788s, 1300.28/s  (0.794s, 1290.17/s)  LR: 1.975e-04  Data: 0.010 (0.017)
Train: 428 [ 300/1251 ( 24%)]  Loss: 3.147 (3.21)  Time: 0.776s, 1319.51/s  (0.791s, 1294.01/s)  LR: 1.975e-04  Data: 0.010 (0.016)
Train: 428 [ 350/1251 ( 28%)]  Loss: 3.130 (3.20)  Time: 0.818s, 1251.76/s  (0.792s, 1293.35/s)  LR: 1.975e-04  Data: 0.010 (0.015)
Train: 428 [ 400/1251 ( 32%)]  Loss: 2.792 (3.15)  Time: 0.782s, 1309.39/s  (0.794s, 1290.34/s)  LR: 1.975e-04  Data: 0.009 (0.015)
Train: 428 [ 450/1251 ( 36%)]  Loss: 3.228 (3.16)  Time: 0.775s, 1321.20/s  (0.793s, 1291.54/s)  LR: 1.975e-04  Data: 0.010 (0.014)
Train: 428 [ 500/1251 ( 40%)]  Loss: 3.295 (3.17)  Time: 0.784s, 1306.55/s  (0.792s, 1293.40/s)  LR: 1.975e-04  Data: 0.010 (0.014)
Train: 428 [ 550/1251 ( 44%)]  Loss: 3.001 (3.16)  Time: 0.785s, 1305.11/s  (0.791s, 1294.50/s)  LR: 1.975e-04  Data: 0.010 (0.013)
Train: 428 [ 600/1251 ( 48%)]  Loss: 3.528 (3.19)  Time: 0.783s, 1307.30/s  (0.790s, 1296.42/s)  LR: 1.975e-04  Data: 0.009 (0.013)
Train: 428 [ 650/1251 ( 52%)]  Loss: 3.348 (3.20)  Time: 0.774s, 1322.87/s  (0.789s, 1297.16/s)  LR: 1.975e-04  Data: 0.010 (0.013)
Train: 428 [ 700/1251 ( 56%)]  Loss: 3.385 (3.21)  Time: 0.789s, 1298.09/s  (0.789s, 1298.27/s)  LR: 1.975e-04  Data: 0.010 (0.013)
Train: 428 [ 750/1251 ( 60%)]  Loss: 3.239 (3.21)  Time: 0.782s, 1309.18/s  (0.788s, 1299.28/s)  LR: 1.975e-04  Data: 0.010 (0.012)
Train: 428 [ 800/1251 ( 64%)]  Loss: 3.455 (3.23)  Time: 0.780s, 1312.39/s  (0.788s, 1300.30/s)  LR: 1.975e-04  Data: 0.010 (0.012)
Train: 428 [ 850/1251 ( 68%)]  Loss: 3.177 (3.22)  Time: 0.880s, 1164.09/s  (0.787s, 1300.76/s)  LR: 1.975e-04  Data: 0.013 (0.012)
Train: 428 [ 900/1251 ( 72%)]  Loss: 3.445 (3.24)  Time: 0.773s, 1325.25/s  (0.787s, 1301.48/s)  LR: 1.975e-04  Data: 0.010 (0.012)
Train: 428 [ 950/1251 ( 76%)]  Loss: 2.977 (3.22)  Time: 0.773s, 1325.43/s  (0.787s, 1301.42/s)  LR: 1.975e-04  Data: 0.010 (0.012)
Train: 428 [1000/1251 ( 80%)]  Loss: 3.059 (3.22)  Time: 0.778s, 1315.39/s  (0.786s, 1302.05/s)  LR: 1.975e-04  Data: 0.009 (0.012)
Train: 428 [1050/1251 ( 84%)]  Loss: 2.956 (3.20)  Time: 0.780s, 1313.04/s  (0.786s, 1302.57/s)  LR: 1.975e-04  Data: 0.010 (0.012)
Train: 428 [1100/1251 ( 88%)]  Loss: 3.141 (3.20)  Time: 0.811s, 1263.20/s  (0.787s, 1301.23/s)  LR: 1.975e-04  Data: 0.009 (0.012)
Train: 428 [1150/1251 ( 92%)]  Loss: 2.798 (3.18)  Time: 0.773s, 1324.09/s  (0.787s, 1301.05/s)  LR: 1.975e-04  Data: 0.010 (0.012)
Train: 428 [1200/1251 ( 96%)]  Loss: 3.348 (3.19)  Time: 0.770s, 1329.22/s  (0.787s, 1300.98/s)  LR: 1.975e-04  Data: 0.010 (0.012)
Train: 428 [1250/1251 (100%)]  Loss: 3.002 (3.18)  Time: 0.761s, 1345.33/s  (0.787s, 1301.62/s)  LR: 1.975e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.545 (1.545)  Loss:  0.6021 (0.6021)  Acc@1: 90.5273 (90.5273)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.193 (0.568)  Loss:  0.7144 (1.0779)  Acc@1: 86.5566 (78.4140)  Acc@5: 96.9340 (94.4960)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-426.pth.tar', 78.46600010986329)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-428.pth.tar', 78.41400010742187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-422.pth.tar', 78.34400010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-427.pth.tar', 78.34400003173828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-425.pth.tar', 78.3380000756836)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-424.pth.tar', 78.31800000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-423.pth.tar', 78.23800016113282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-410.pth.tar', 78.23400010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-421.pth.tar', 78.17800000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-418.pth.tar', 78.094000078125)

Train: 429 [   0/1251 (  0%)]  Loss: 3.083 (3.08)  Time: 2.307s,  443.90/s  (2.307s,  443.90/s)  LR: 1.955e-04  Data: 1.586 (1.586)
Train: 429 [  50/1251 (  4%)]  Loss: 3.313 (3.20)  Time: 0.771s, 1327.45/s  (0.810s, 1263.72/s)  LR: 1.955e-04  Data: 0.009 (0.042)
Train: 429 [ 100/1251 (  8%)]  Loss: 3.451 (3.28)  Time: 0.824s, 1242.74/s  (0.798s, 1283.96/s)  LR: 1.955e-04  Data: 0.012 (0.026)
Train: 429 [ 150/1251 ( 12%)]  Loss: 3.383 (3.31)  Time: 0.823s, 1244.24/s  (0.800s, 1279.82/s)  LR: 1.955e-04  Data: 0.010 (0.021)
Train: 429 [ 200/1251 ( 16%)]  Loss: 2.981 (3.24)  Time: 0.772s, 1327.03/s  (0.799s, 1281.49/s)  LR: 1.955e-04  Data: 0.010 (0.018)
Train: 429 [ 250/1251 ( 20%)]  Loss: 3.081 (3.22)  Time: 0.772s, 1325.92/s  (0.796s, 1286.56/s)  LR: 1.955e-04  Data: 0.010 (0.016)
Train: 429 [ 300/1251 ( 24%)]  Loss: 3.612 (3.27)  Time: 0.817s, 1252.95/s  (0.794s, 1290.36/s)  LR: 1.955e-04  Data: 0.009 (0.015)
Train: 429 [ 350/1251 ( 28%)]  Loss: 3.279 (3.27)  Time: 0.772s, 1326.62/s  (0.792s, 1292.93/s)  LR: 1.955e-04  Data: 0.010 (0.015)
Train: 429 [ 400/1251 ( 32%)]  Loss: 3.569 (3.31)  Time: 0.777s, 1317.88/s  (0.790s, 1295.99/s)  LR: 1.955e-04  Data: 0.010 (0.014)
Train: 429 [ 450/1251 ( 36%)]  Loss: 3.377 (3.31)  Time: 0.807s, 1268.49/s  (0.790s, 1295.88/s)  LR: 1.955e-04  Data: 0.010 (0.014)
Train: 429 [ 500/1251 ( 40%)]  Loss: 2.797 (3.27)  Time: 0.771s, 1328.24/s  (0.791s, 1295.21/s)  LR: 1.955e-04  Data: 0.009 (0.013)
Train: 429 [ 550/1251 ( 44%)]  Loss: 3.368 (3.27)  Time: 0.771s, 1328.52/s  (0.791s, 1294.51/s)  LR: 1.955e-04  Data: 0.010 (0.013)
Train: 429 [ 600/1251 ( 48%)]  Loss: 3.001 (3.25)  Time: 0.859s, 1191.79/s  (0.791s, 1295.26/s)  LR: 1.955e-04  Data: 0.010 (0.013)
Train: 429 [ 650/1251 ( 52%)]  Loss: 3.078 (3.24)  Time: 0.785s, 1304.80/s  (0.790s, 1296.44/s)  LR: 1.955e-04  Data: 0.012 (0.012)
Train: 429 [ 700/1251 ( 56%)]  Loss: 3.081 (3.23)  Time: 0.772s, 1326.10/s  (0.789s, 1297.76/s)  LR: 1.955e-04  Data: 0.010 (0.012)
Train: 429 [ 750/1251 ( 60%)]  Loss: 3.276 (3.23)  Time: 0.773s, 1324.54/s  (0.788s, 1299.12/s)  LR: 1.955e-04  Data: 0.010 (0.012)
Train: 429 [ 800/1251 ( 64%)]  Loss: 3.308 (3.24)  Time: 0.773s, 1325.27/s  (0.787s, 1300.35/s)  LR: 1.955e-04  Data: 0.010 (0.012)
Train: 429 [ 850/1251 ( 68%)]  Loss: 2.714 (3.21)  Time: 0.809s, 1265.54/s  (0.788s, 1299.50/s)  LR: 1.955e-04  Data: 0.010 (0.012)
Train: 429 [ 900/1251 ( 72%)]  Loss: 3.178 (3.21)  Time: 0.773s, 1325.02/s  (0.788s, 1299.79/s)  LR: 1.955e-04  Data: 0.009 (0.012)
Train: 429 [ 950/1251 ( 76%)]  Loss: 3.334 (3.21)  Time: 0.791s, 1294.68/s  (0.787s, 1300.62/s)  LR: 1.955e-04  Data: 0.010 (0.012)
Train: 429 [1000/1251 ( 80%)]  Loss: 3.022 (3.20)  Time: 0.773s, 1325.28/s  (0.787s, 1300.82/s)  LR: 1.955e-04  Data: 0.010 (0.012)
Train: 429 [1050/1251 ( 84%)]  Loss: 3.485 (3.22)  Time: 0.772s, 1325.70/s  (0.787s, 1301.68/s)  LR: 1.955e-04  Data: 0.010 (0.011)
Train: 429 [1100/1251 ( 88%)]  Loss: 3.508 (3.23)  Time: 0.772s, 1327.07/s  (0.787s, 1301.50/s)  LR: 1.955e-04  Data: 0.010 (0.011)
Train: 429 [1150/1251 ( 92%)]  Loss: 3.498 (3.24)  Time: 0.774s, 1323.19/s  (0.786s, 1302.11/s)  LR: 1.955e-04  Data: 0.010 (0.011)
Train: 429 [1200/1251 ( 96%)]  Loss: 3.327 (3.24)  Time: 0.771s, 1328.42/s  (0.786s, 1302.49/s)  LR: 1.955e-04  Data: 0.010 (0.011)
Train: 429 [1250/1251 (100%)]  Loss: 3.082 (3.24)  Time: 0.760s, 1346.77/s  (0.786s, 1302.75/s)  LR: 1.955e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.559 (1.559)  Loss:  0.6484 (0.6484)  Acc@1: 90.7227 (90.7227)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.194 (0.568)  Loss:  0.7388 (1.0966)  Acc@1: 86.2028 (78.5680)  Acc@5: 97.2877 (94.5340)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-429.pth.tar', 78.56800008300782)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-426.pth.tar', 78.46600010986329)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-428.pth.tar', 78.41400010742187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-422.pth.tar', 78.34400010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-427.pth.tar', 78.34400003173828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-425.pth.tar', 78.3380000756836)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-424.pth.tar', 78.31800000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-423.pth.tar', 78.23800016113282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-410.pth.tar', 78.23400010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-421.pth.tar', 78.17800000244141)

Train: 430 [   0/1251 (  0%)]  Loss: 3.137 (3.14)  Time: 2.280s,  449.08/s  (2.280s,  449.08/s)  LR: 1.935e-04  Data: 1.559 (1.559)
Train: 430 [  50/1251 (  4%)]  Loss: 3.100 (3.12)  Time: 0.773s, 1324.56/s  (0.815s, 1256.19/s)  LR: 1.935e-04  Data: 0.010 (0.046)
Train: 430 [ 100/1251 (  8%)]  Loss: 3.477 (3.24)  Time: 0.790s, 1295.92/s  (0.800s, 1279.94/s)  LR: 1.935e-04  Data: 0.010 (0.028)
Train: 430 [ 150/1251 ( 12%)]  Loss: 3.207 (3.23)  Time: 0.781s, 1311.90/s  (0.795s, 1288.77/s)  LR: 1.935e-04  Data: 0.010 (0.022)
Train: 430 [ 200/1251 ( 16%)]  Loss: 3.034 (3.19)  Time: 0.813s, 1259.16/s  (0.790s, 1296.04/s)  LR: 1.935e-04  Data: 0.010 (0.019)
Train: 430 [ 250/1251 ( 20%)]  Loss: 2.966 (3.15)  Time: 0.785s, 1304.18/s  (0.788s, 1299.36/s)  LR: 1.935e-04  Data: 0.010 (0.017)
Train: 430 [ 300/1251 ( 24%)]  Loss: 3.174 (3.16)  Time: 0.794s, 1289.24/s  (0.786s, 1302.02/s)  LR: 1.935e-04  Data: 0.012 (0.016)
Train: 430 [ 350/1251 ( 28%)]  Loss: 3.244 (3.17)  Time: 0.816s, 1254.27/s  (0.786s, 1302.49/s)  LR: 1.935e-04  Data: 0.010 (0.015)
Train: 430 [ 400/1251 ( 32%)]  Loss: 3.624 (3.22)  Time: 0.788s, 1299.34/s  (0.785s, 1304.20/s)  LR: 1.935e-04  Data: 0.009 (0.014)
Train: 430 [ 450/1251 ( 36%)]  Loss: 2.856 (3.18)  Time: 0.779s, 1314.76/s  (0.787s, 1300.33/s)  LR: 1.935e-04  Data: 0.010 (0.014)
Train: 430 [ 500/1251 ( 40%)]  Loss: 3.146 (3.18)  Time: 0.834s, 1227.65/s  (0.788s, 1298.89/s)  LR: 1.935e-04  Data: 0.009 (0.014)
Train: 430 [ 550/1251 ( 44%)]  Loss: 3.054 (3.17)  Time: 0.773s, 1324.10/s  (0.790s, 1296.32/s)  LR: 1.935e-04  Data: 0.009 (0.013)
Train: 430 [ 600/1251 ( 48%)]  Loss: 3.282 (3.18)  Time: 0.775s, 1321.98/s  (0.789s, 1297.91/s)  LR: 1.935e-04  Data: 0.010 (0.013)
Train: 430 [ 650/1251 ( 52%)]  Loss: 3.021 (3.17)  Time: 0.782s, 1309.11/s  (0.788s, 1299.35/s)  LR: 1.935e-04  Data: 0.009 (0.013)
Train: 430 [ 700/1251 ( 56%)]  Loss: 3.430 (3.18)  Time: 0.809s, 1265.76/s  (0.789s, 1297.46/s)  LR: 1.935e-04  Data: 0.009 (0.013)
Train: 430 [ 750/1251 ( 60%)]  Loss: 2.993 (3.17)  Time: 0.779s, 1313.69/s  (0.789s, 1298.12/s)  LR: 1.935e-04  Data: 0.010 (0.012)
Train: 430 [ 800/1251 ( 64%)]  Loss: 3.008 (3.16)  Time: 0.774s, 1322.33/s  (0.788s, 1299.13/s)  LR: 1.935e-04  Data: 0.010 (0.012)
Train: 430 [ 850/1251 ( 68%)]  Loss: 3.198 (3.16)  Time: 0.823s, 1243.51/s  (0.788s, 1300.02/s)  LR: 1.935e-04  Data: 0.010 (0.012)
Train: 430 [ 900/1251 ( 72%)]  Loss: 3.151 (3.16)  Time: 0.773s, 1323.89/s  (0.787s, 1301.06/s)  LR: 1.935e-04  Data: 0.010 (0.012)
Train: 430 [ 950/1251 ( 76%)]  Loss: 3.148 (3.16)  Time: 0.773s, 1325.52/s  (0.787s, 1301.59/s)  LR: 1.935e-04  Data: 0.010 (0.012)
Train: 430 [1000/1251 ( 80%)]  Loss: 3.409 (3.17)  Time: 0.774s, 1323.15/s  (0.786s, 1302.39/s)  LR: 1.935e-04  Data: 0.010 (0.012)
Train: 430 [1050/1251 ( 84%)]  Loss: 3.312 (3.18)  Time: 0.777s, 1317.77/s  (0.786s, 1302.99/s)  LR: 1.935e-04  Data: 0.010 (0.012)
Train: 430 [1100/1251 ( 88%)]  Loss: 3.002 (3.17)  Time: 0.775s, 1321.23/s  (0.786s, 1303.45/s)  LR: 1.935e-04  Data: 0.009 (0.012)
Train: 430 [1150/1251 ( 92%)]  Loss: 3.300 (3.18)  Time: 0.782s, 1309.63/s  (0.785s, 1303.99/s)  LR: 1.935e-04  Data: 0.010 (0.012)
Train: 430 [1200/1251 ( 96%)]  Loss: 3.162 (3.18)  Time: 0.834s, 1227.33/s  (0.785s, 1304.07/s)  LR: 1.935e-04  Data: 0.011 (0.011)
Train: 430 [1250/1251 (100%)]  Loss: 2.850 (3.16)  Time: 0.782s, 1308.71/s  (0.785s, 1304.07/s)  LR: 1.935e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.519 (1.519)  Loss:  0.6694 (0.6694)  Acc@1: 90.5273 (90.5273)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.194 (0.563)  Loss:  0.7925 (1.1197)  Acc@1: 86.6745 (78.6880)  Acc@5: 96.5802 (94.5140)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-430.pth.tar', 78.68800002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-429.pth.tar', 78.56800008300782)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-426.pth.tar', 78.46600010986329)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-428.pth.tar', 78.41400010742187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-422.pth.tar', 78.34400010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-427.pth.tar', 78.34400003173828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-425.pth.tar', 78.3380000756836)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-424.pth.tar', 78.31800000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-423.pth.tar', 78.23800016113282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-410.pth.tar', 78.23400010498047)

Train: 431 [   0/1251 (  0%)]  Loss: 3.129 (3.13)  Time: 2.376s,  431.04/s  (2.376s,  431.04/s)  LR: 1.915e-04  Data: 1.596 (1.596)
Train: 431 [  50/1251 (  4%)]  Loss: 3.269 (3.20)  Time: 0.780s, 1312.91/s  (0.850s, 1204.33/s)  LR: 1.915e-04  Data: 0.010 (0.044)
Train: 431 [ 100/1251 (  8%)]  Loss: 3.332 (3.24)  Time: 0.786s, 1303.29/s  (0.816s, 1255.52/s)  LR: 1.915e-04  Data: 0.010 (0.027)
Train: 431 [ 150/1251 ( 12%)]  Loss: 3.458 (3.30)  Time: 0.774s, 1322.21/s  (0.805s, 1272.00/s)  LR: 1.915e-04  Data: 0.010 (0.022)
Train: 431 [ 200/1251 ( 16%)]  Loss: 3.352 (3.31)  Time: 0.777s, 1318.55/s  (0.800s, 1280.30/s)  LR: 1.915e-04  Data: 0.010 (0.019)
Train: 431 [ 250/1251 ( 20%)]  Loss: 3.283 (3.30)  Time: 0.773s, 1324.64/s  (0.796s, 1285.69/s)  LR: 1.915e-04  Data: 0.010 (0.017)
Train: 431 [ 300/1251 ( 24%)]  Loss: 2.714 (3.22)  Time: 0.782s, 1309.91/s  (0.794s, 1290.04/s)  LR: 1.915e-04  Data: 0.010 (0.016)
Train: 431 [ 350/1251 ( 28%)]  Loss: 3.327 (3.23)  Time: 0.775s, 1321.18/s  (0.792s, 1293.25/s)  LR: 1.915e-04  Data: 0.010 (0.015)
Train: 431 [ 400/1251 ( 32%)]  Loss: 3.475 (3.26)  Time: 0.773s, 1325.12/s  (0.790s, 1295.44/s)  LR: 1.915e-04  Data: 0.010 (0.014)
Train: 431 [ 450/1251 ( 36%)]  Loss: 3.529 (3.29)  Time: 0.776s, 1318.85/s  (0.789s, 1297.22/s)  LR: 1.915e-04  Data: 0.009 (0.014)
Train: 431 [ 500/1251 ( 40%)]  Loss: 3.397 (3.30)  Time: 0.776s, 1319.42/s  (0.789s, 1297.89/s)  LR: 1.915e-04  Data: 0.010 (0.013)
Train: 431 [ 550/1251 ( 44%)]  Loss: 3.301 (3.30)  Time: 0.810s, 1264.66/s  (0.790s, 1297.02/s)  LR: 1.915e-04  Data: 0.010 (0.013)
Train: 431 [ 600/1251 ( 48%)]  Loss: 3.042 (3.28)  Time: 0.814s, 1258.05/s  (0.793s, 1291.99/s)  LR: 1.915e-04  Data: 0.010 (0.013)
Train: 431 [ 650/1251 ( 52%)]  Loss: 2.999 (3.26)  Time: 0.778s, 1316.43/s  (0.793s, 1291.26/s)  LR: 1.915e-04  Data: 0.009 (0.013)
Train: 431 [ 700/1251 ( 56%)]  Loss: 3.321 (3.26)  Time: 0.770s, 1329.63/s  (0.793s, 1292.10/s)  LR: 1.915e-04  Data: 0.009 (0.013)
Train: 431 [ 750/1251 ( 60%)]  Loss: 3.477 (3.28)  Time: 0.773s, 1325.30/s  (0.792s, 1293.39/s)  LR: 1.915e-04  Data: 0.010 (0.012)
Train: 431 [ 800/1251 ( 64%)]  Loss: 3.449 (3.29)  Time: 0.778s, 1315.99/s  (0.792s, 1293.65/s)  LR: 1.915e-04  Data: 0.010 (0.012)
Train: 431 [ 850/1251 ( 68%)]  Loss: 3.143 (3.28)  Time: 0.808s, 1267.38/s  (0.791s, 1294.04/s)  LR: 1.915e-04  Data: 0.010 (0.012)
Train: 431 [ 900/1251 ( 72%)]  Loss: 3.135 (3.27)  Time: 0.807s, 1268.32/s  (0.792s, 1292.41/s)  LR: 1.915e-04  Data: 0.010 (0.012)
Train: 431 [ 950/1251 ( 76%)]  Loss: 3.020 (3.26)  Time: 0.812s, 1261.07/s  (0.792s, 1292.75/s)  LR: 1.915e-04  Data: 0.010 (0.012)
Train: 431 [1000/1251 ( 80%)]  Loss: 2.994 (3.25)  Time: 0.775s, 1321.15/s  (0.792s, 1293.55/s)  LR: 1.915e-04  Data: 0.009 (0.012)
Train: 431 [1050/1251 ( 84%)]  Loss: 3.287 (3.25)  Time: 0.808s, 1266.92/s  (0.791s, 1294.62/s)  LR: 1.915e-04  Data: 0.009 (0.012)
Train: 431 [1100/1251 ( 88%)]  Loss: 3.414 (3.25)  Time: 0.772s, 1325.93/s  (0.791s, 1294.06/s)  LR: 1.915e-04  Data: 0.009 (0.012)
Train: 431 [1150/1251 ( 92%)]  Loss: 3.299 (3.26)  Time: 0.770s, 1329.83/s  (0.791s, 1294.69/s)  LR: 1.915e-04  Data: 0.010 (0.012)
Train: 431 [1200/1251 ( 96%)]  Loss: 3.397 (3.26)  Time: 0.778s, 1316.99/s  (0.791s, 1295.13/s)  LR: 1.915e-04  Data: 0.009 (0.011)
Train: 431 [1250/1251 (100%)]  Loss: 3.030 (3.25)  Time: 0.761s, 1345.42/s  (0.790s, 1295.46/s)  LR: 1.915e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.564 (1.564)  Loss:  0.6890 (0.6890)  Acc@1: 91.6992 (91.6992)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.194 (0.562)  Loss:  0.8408 (1.1700)  Acc@1: 86.9104 (78.6360)  Acc@5: 97.0519 (94.5980)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-430.pth.tar', 78.68800002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-431.pth.tar', 78.6360000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-429.pth.tar', 78.56800008300782)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-426.pth.tar', 78.46600010986329)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-428.pth.tar', 78.41400010742187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-422.pth.tar', 78.34400010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-427.pth.tar', 78.34400003173828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-425.pth.tar', 78.3380000756836)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-424.pth.tar', 78.31800000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-423.pth.tar', 78.23800016113282)

Train: 432 [   0/1251 (  0%)]  Loss: 3.434 (3.43)  Time: 2.227s,  459.89/s  (2.227s,  459.89/s)  LR: 1.895e-04  Data: 1.512 (1.512)
Train: 432 [  50/1251 (  4%)]  Loss: 3.097 (3.27)  Time: 0.775s, 1320.71/s  (0.811s, 1262.94/s)  LR: 1.895e-04  Data: 0.010 (0.044)
Train: 432 [ 100/1251 (  8%)]  Loss: 3.146 (3.23)  Time: 0.772s, 1325.85/s  (0.802s, 1276.27/s)  LR: 1.895e-04  Data: 0.009 (0.027)
Train: 432 [ 150/1251 ( 12%)]  Loss: 3.350 (3.26)  Time: 0.779s, 1314.55/s  (0.795s, 1288.01/s)  LR: 1.895e-04  Data: 0.010 (0.021)
Train: 432 [ 200/1251 ( 16%)]  Loss: 3.147 (3.23)  Time: 0.783s, 1307.96/s  (0.791s, 1293.89/s)  LR: 1.895e-04  Data: 0.010 (0.018)
Train: 432 [ 250/1251 ( 20%)]  Loss: 3.174 (3.22)  Time: 0.774s, 1322.57/s  (0.790s, 1296.41/s)  LR: 1.895e-04  Data: 0.009 (0.017)
Train: 432 [ 300/1251 ( 24%)]  Loss: 3.086 (3.20)  Time: 0.776s, 1318.74/s  (0.789s, 1298.45/s)  LR: 1.895e-04  Data: 0.010 (0.016)
Train: 432 [ 350/1251 ( 28%)]  Loss: 3.101 (3.19)  Time: 0.778s, 1316.66/s  (0.787s, 1300.40/s)  LR: 1.895e-04  Data: 0.014 (0.015)
Train: 432 [ 400/1251 ( 32%)]  Loss: 3.262 (3.20)  Time: 0.773s, 1324.86/s  (0.786s, 1302.17/s)  LR: 1.895e-04  Data: 0.011 (0.014)
Train: 432 [ 450/1251 ( 36%)]  Loss: 3.328 (3.21)  Time: 0.828s, 1236.21/s  (0.786s, 1303.35/s)  LR: 1.895e-04  Data: 0.010 (0.014)
Train: 432 [ 500/1251 ( 40%)]  Loss: 3.255 (3.22)  Time: 0.774s, 1323.18/s  (0.785s, 1304.58/s)  LR: 1.895e-04  Data: 0.011 (0.013)
Train: 432 [ 550/1251 ( 44%)]  Loss: 3.490 (3.24)  Time: 0.817s, 1252.79/s  (0.785s, 1304.06/s)  LR: 1.895e-04  Data: 0.012 (0.013)
Train: 432 [ 600/1251 ( 48%)]  Loss: 3.469 (3.26)  Time: 0.775s, 1321.84/s  (0.785s, 1304.58/s)  LR: 1.895e-04  Data: 0.010 (0.013)
Train: 432 [ 650/1251 ( 52%)]  Loss: 3.060 (3.24)  Time: 0.810s, 1263.77/s  (0.785s, 1305.02/s)  LR: 1.895e-04  Data: 0.010 (0.013)
Train: 432 [ 700/1251 ( 56%)]  Loss: 3.212 (3.24)  Time: 0.782s, 1308.92/s  (0.785s, 1305.24/s)  LR: 1.895e-04  Data: 0.010 (0.012)
Train: 432 [ 750/1251 ( 60%)]  Loss: 3.392 (3.25)  Time: 0.774s, 1322.18/s  (0.784s, 1305.90/s)  LR: 1.895e-04  Data: 0.010 (0.012)
Train: 432 [ 800/1251 ( 64%)]  Loss: 3.369 (3.26)  Time: 0.826s, 1239.17/s  (0.785s, 1304.38/s)  LR: 1.895e-04  Data: 0.014 (0.012)
Train: 432 [ 850/1251 ( 68%)]  Loss: 2.967 (3.24)  Time: 0.775s, 1320.92/s  (0.785s, 1304.07/s)  LR: 1.895e-04  Data: 0.010 (0.012)
Train: 432 [ 900/1251 ( 72%)]  Loss: 3.484 (3.25)  Time: 0.773s, 1324.14/s  (0.785s, 1304.32/s)  LR: 1.895e-04  Data: 0.009 (0.012)
Train: 432 [ 950/1251 ( 76%)]  Loss: 3.226 (3.25)  Time: 0.815s, 1256.73/s  (0.785s, 1304.02/s)  LR: 1.895e-04  Data: 0.011 (0.012)
Train: 432 [1000/1251 ( 80%)]  Loss: 3.056 (3.24)  Time: 0.773s, 1324.59/s  (0.786s, 1302.18/s)  LR: 1.895e-04  Data: 0.010 (0.012)
Train: 432 [1050/1251 ( 84%)]  Loss: 3.619 (3.26)  Time: 0.773s, 1324.37/s  (0.786s, 1302.79/s)  LR: 1.895e-04  Data: 0.010 (0.012)
Train: 432 [1100/1251 ( 88%)]  Loss: 3.232 (3.26)  Time: 0.773s, 1324.49/s  (0.786s, 1303.34/s)  LR: 1.895e-04  Data: 0.011 (0.012)
Train: 432 [1150/1251 ( 92%)]  Loss: 3.300 (3.26)  Time: 0.823s, 1244.26/s  (0.786s, 1302.61/s)  LR: 1.895e-04  Data: 0.010 (0.011)
Train: 432 [1200/1251 ( 96%)]  Loss: 3.057 (3.25)  Time: 0.772s, 1327.02/s  (0.786s, 1302.72/s)  LR: 1.895e-04  Data: 0.010 (0.011)
Train: 432 [1250/1251 (100%)]  Loss: 3.127 (3.25)  Time: 0.760s, 1347.28/s  (0.787s, 1301.10/s)  LR: 1.895e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.553 (1.553)  Loss:  0.7227 (0.7227)  Acc@1: 90.8203 (90.8203)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.194 (0.569)  Loss:  0.8516 (1.1974)  Acc@1: 85.9670 (78.3820)  Acc@5: 96.6981 (94.4360)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-430.pth.tar', 78.68800002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-431.pth.tar', 78.6360000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-429.pth.tar', 78.56800008300782)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-426.pth.tar', 78.46600010986329)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-428.pth.tar', 78.41400010742187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-432.pth.tar', 78.38200010986328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-422.pth.tar', 78.34400010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-427.pth.tar', 78.34400003173828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-425.pth.tar', 78.3380000756836)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-424.pth.tar', 78.31800000244141)

Train: 433 [   0/1251 (  0%)]  Loss: 3.069 (3.07)  Time: 2.181s,  469.52/s  (2.181s,  469.52/s)  LR: 1.875e-04  Data: 1.466 (1.466)
Train: 433 [  50/1251 (  4%)]  Loss: 3.433 (3.25)  Time: 0.796s, 1287.15/s  (0.811s, 1262.32/s)  LR: 1.875e-04  Data: 0.010 (0.042)
Train: 433 [ 100/1251 (  8%)]  Loss: 3.054 (3.19)  Time: 0.825s, 1240.81/s  (0.797s, 1284.10/s)  LR: 1.875e-04  Data: 0.010 (0.026)
Train: 433 [ 150/1251 ( 12%)]  Loss: 3.522 (3.27)  Time: 0.772s, 1326.19/s  (0.794s, 1289.07/s)  LR: 1.875e-04  Data: 0.010 (0.021)
Train: 433 [ 200/1251 ( 16%)]  Loss: 3.401 (3.30)  Time: 0.781s, 1311.29/s  (0.791s, 1294.70/s)  LR: 1.875e-04  Data: 0.010 (0.018)
Train: 433 [ 250/1251 ( 20%)]  Loss: 2.944 (3.24)  Time: 0.772s, 1326.84/s  (0.792s, 1292.12/s)  LR: 1.875e-04  Data: 0.009 (0.016)
Train: 433 [ 300/1251 ( 24%)]  Loss: 3.384 (3.26)  Time: 0.779s, 1314.07/s  (0.791s, 1295.27/s)  LR: 1.875e-04  Data: 0.009 (0.015)
Train: 433 [ 350/1251 ( 28%)]  Loss: 3.037 (3.23)  Time: 0.783s, 1308.36/s  (0.789s, 1298.33/s)  LR: 1.875e-04  Data: 0.009 (0.014)
Train: 433 [ 400/1251 ( 32%)]  Loss: 3.104 (3.22)  Time: 0.815s, 1257.07/s  (0.790s, 1296.70/s)  LR: 1.875e-04  Data: 0.010 (0.014)
Train: 433 [ 450/1251 ( 36%)]  Loss: 3.147 (3.21)  Time: 0.779s, 1314.23/s  (0.789s, 1297.87/s)  LR: 1.875e-04  Data: 0.010 (0.014)
Train: 433 [ 500/1251 ( 40%)]  Loss: 3.154 (3.20)  Time: 0.772s, 1326.35/s  (0.789s, 1298.42/s)  LR: 1.875e-04  Data: 0.010 (0.013)
Train: 433 [ 550/1251 ( 44%)]  Loss: 3.405 (3.22)  Time: 0.774s, 1323.27/s  (0.788s, 1298.76/s)  LR: 1.875e-04  Data: 0.010 (0.013)
Train: 433 [ 600/1251 ( 48%)]  Loss: 3.277 (3.23)  Time: 0.775s, 1320.64/s  (0.789s, 1297.46/s)  LR: 1.875e-04  Data: 0.012 (0.013)
Train: 433 [ 650/1251 ( 52%)]  Loss: 3.514 (3.25)  Time: 0.790s, 1295.63/s  (0.789s, 1298.52/s)  LR: 1.875e-04  Data: 0.012 (0.013)
Train: 433 [ 700/1251 ( 56%)]  Loss: 2.720 (3.21)  Time: 0.776s, 1319.05/s  (0.788s, 1299.87/s)  LR: 1.875e-04  Data: 0.010 (0.013)
Train: 433 [ 750/1251 ( 60%)]  Loss: 3.157 (3.21)  Time: 0.773s, 1324.32/s  (0.787s, 1300.69/s)  LR: 1.875e-04  Data: 0.010 (0.012)
Train: 433 [ 800/1251 ( 64%)]  Loss: 3.138 (3.20)  Time: 0.807s, 1268.74/s  (0.787s, 1301.23/s)  LR: 1.875e-04  Data: 0.010 (0.012)
Train: 433 [ 850/1251 ( 68%)]  Loss: 3.035 (3.19)  Time: 0.774s, 1323.61/s  (0.787s, 1300.73/s)  LR: 1.875e-04  Data: 0.009 (0.012)
Train: 433 [ 900/1251 ( 72%)]  Loss: 3.102 (3.19)  Time: 0.774s, 1322.66/s  (0.787s, 1301.51/s)  LR: 1.875e-04  Data: 0.010 (0.012)
Train: 433 [ 950/1251 ( 76%)]  Loss: 3.446 (3.20)  Time: 0.774s, 1323.55/s  (0.787s, 1301.77/s)  LR: 1.875e-04  Data: 0.010 (0.012)
Train: 433 [1000/1251 ( 80%)]  Loss: 3.041 (3.19)  Time: 0.780s, 1313.44/s  (0.788s, 1300.25/s)  LR: 1.875e-04  Data: 0.009 (0.012)
Train: 433 [1050/1251 ( 84%)]  Loss: 3.338 (3.20)  Time: 0.773s, 1324.28/s  (0.787s, 1301.22/s)  LR: 1.875e-04  Data: 0.010 (0.012)
Train: 433 [1100/1251 ( 88%)]  Loss: 3.176 (3.20)  Time: 0.781s, 1311.23/s  (0.787s, 1301.87/s)  LR: 1.875e-04  Data: 0.009 (0.012)
Train: 433 [1150/1251 ( 92%)]  Loss: 3.408 (3.21)  Time: 0.773s, 1324.81/s  (0.786s, 1302.37/s)  LR: 1.875e-04  Data: 0.010 (0.012)
Train: 433 [1200/1251 ( 96%)]  Loss: 3.309 (3.21)  Time: 0.772s, 1326.45/s  (0.786s, 1302.70/s)  LR: 1.875e-04  Data: 0.009 (0.011)
Train: 433 [1250/1251 (100%)]  Loss: 2.878 (3.20)  Time: 0.769s, 1331.64/s  (0.786s, 1302.68/s)  LR: 1.875e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.531 (1.531)  Loss:  0.6973 (0.6973)  Acc@1: 90.6250 (90.6250)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.194 (0.566)  Loss:  0.7690 (1.1565)  Acc@1: 86.3208 (78.3020)  Acc@5: 97.7594 (94.5160)
Train: 434 [   0/1251 (  0%)]  Loss: 3.392 (3.39)  Time: 2.426s,  422.15/s  (2.426s,  422.15/s)  LR: 1.855e-04  Data: 1.698 (1.698)
Train: 434 [  50/1251 (  4%)]  Loss: 3.218 (3.30)  Time: 0.773s, 1324.37/s  (0.823s, 1244.47/s)  LR: 1.855e-04  Data: 0.010 (0.047)
Train: 434 [ 100/1251 (  8%)]  Loss: 3.014 (3.21)  Time: 0.772s, 1327.00/s  (0.803s, 1274.52/s)  LR: 1.855e-04  Data: 0.009 (0.029)
Train: 434 [ 150/1251 ( 12%)]  Loss: 3.511 (3.28)  Time: 0.775s, 1320.79/s  (0.795s, 1288.11/s)  LR: 1.855e-04  Data: 0.010 (0.023)
Train: 434 [ 200/1251 ( 16%)]  Loss: 2.828 (3.19)  Time: 0.773s, 1324.51/s  (0.791s, 1294.48/s)  LR: 1.855e-04  Data: 0.010 (0.019)
Train: 434 [ 250/1251 ( 20%)]  Loss: 3.047 (3.17)  Time: 0.773s, 1323.92/s  (0.788s, 1298.81/s)  LR: 1.855e-04  Data: 0.010 (0.018)
Train: 434 [ 300/1251 ( 24%)]  Loss: 3.521 (3.22)  Time: 0.774s, 1323.84/s  (0.787s, 1300.91/s)  LR: 1.855e-04  Data: 0.010 (0.016)
Train: 434 [ 350/1251 ( 28%)]  Loss: 3.243 (3.22)  Time: 0.821s, 1246.56/s  (0.788s, 1298.80/s)  LR: 1.855e-04  Data: 0.010 (0.015)
Train: 434 [ 400/1251 ( 32%)]  Loss: 2.952 (3.19)  Time: 0.775s, 1321.25/s  (0.788s, 1299.36/s)  LR: 1.855e-04  Data: 0.010 (0.015)
Train: 434 [ 450/1251 ( 36%)]  Loss: 2.953 (3.17)  Time: 0.775s, 1320.67/s  (0.787s, 1300.59/s)  LR: 1.855e-04  Data: 0.010 (0.014)
Train: 434 [ 500/1251 ( 40%)]  Loss: 3.199 (3.17)  Time: 0.789s, 1297.96/s  (0.787s, 1300.43/s)  LR: 1.855e-04  Data: 0.010 (0.014)
Train: 434 [ 550/1251 ( 44%)]  Loss: 3.366 (3.19)  Time: 0.774s, 1322.39/s  (0.787s, 1301.17/s)  LR: 1.855e-04  Data: 0.009 (0.013)
Train: 434 [ 600/1251 ( 48%)]  Loss: 2.737 (3.15)  Time: 0.775s, 1321.29/s  (0.787s, 1301.23/s)  LR: 1.855e-04  Data: 0.010 (0.013)
Train: 434 [ 650/1251 ( 52%)]  Loss: 3.188 (3.15)  Time: 0.784s, 1306.40/s  (0.787s, 1301.90/s)  LR: 1.855e-04  Data: 0.010 (0.013)
Train: 434 [ 700/1251 ( 56%)]  Loss: 2.740 (3.13)  Time: 0.768s, 1333.05/s  (0.786s, 1302.15/s)  LR: 1.855e-04  Data: 0.009 (0.013)
Train: 434 [ 750/1251 ( 60%)]  Loss: 3.178 (3.13)  Time: 0.776s, 1319.92/s  (0.786s, 1303.11/s)  LR: 1.855e-04  Data: 0.010 (0.012)
Train: 434 [ 800/1251 ( 64%)]  Loss: 3.221 (3.14)  Time: 0.772s, 1326.08/s  (0.786s, 1303.37/s)  LR: 1.855e-04  Data: 0.009 (0.012)
Train: 434 [ 850/1251 ( 68%)]  Loss: 3.456 (3.15)  Time: 0.779s, 1314.11/s  (0.786s, 1303.58/s)  LR: 1.855e-04  Data: 0.011 (0.012)
Train: 434 [ 900/1251 ( 72%)]  Loss: 3.006 (3.15)  Time: 0.783s, 1308.40/s  (0.785s, 1304.13/s)  LR: 1.855e-04  Data: 0.010 (0.012)
Train: 434 [ 950/1251 ( 76%)]  Loss: 3.079 (3.14)  Time: 0.779s, 1314.62/s  (0.786s, 1302.42/s)  LR: 1.855e-04  Data: 0.010 (0.012)
Train: 434 [1000/1251 ( 80%)]  Loss: 3.329 (3.15)  Time: 0.777s, 1317.76/s  (0.786s, 1302.55/s)  LR: 1.855e-04  Data: 0.010 (0.012)
Train: 434 [1050/1251 ( 84%)]  Loss: 2.994 (3.14)  Time: 0.818s, 1251.12/s  (0.787s, 1301.86/s)  LR: 1.855e-04  Data: 0.010 (0.012)
Train: 434 [1100/1251 ( 88%)]  Loss: 3.144 (3.14)  Time: 0.774s, 1323.07/s  (0.786s, 1302.52/s)  LR: 1.855e-04  Data: 0.010 (0.012)
Train: 434 [1150/1251 ( 92%)]  Loss: 3.499 (3.16)  Time: 0.827s, 1238.19/s  (0.787s, 1301.62/s)  LR: 1.855e-04  Data: 0.014 (0.012)
Train: 434 [1200/1251 ( 96%)]  Loss: 3.385 (3.17)  Time: 0.774s, 1322.58/s  (0.786s, 1302.25/s)  LR: 1.855e-04  Data: 0.010 (0.011)
Train: 434 [1250/1251 (100%)]  Loss: 3.083 (3.16)  Time: 0.760s, 1346.68/s  (0.786s, 1302.62/s)  LR: 1.855e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.544 (1.544)  Loss:  0.6533 (0.6533)  Acc@1: 91.1133 (91.1133)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.194 (0.570)  Loss:  0.7432 (1.0884)  Acc@1: 86.3208 (78.5560)  Acc@5: 97.0519 (94.5440)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-430.pth.tar', 78.68800002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-431.pth.tar', 78.6360000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-429.pth.tar', 78.56800008300782)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-434.pth.tar', 78.55600000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-426.pth.tar', 78.46600010986329)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-428.pth.tar', 78.41400010742187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-432.pth.tar', 78.38200010986328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-422.pth.tar', 78.34400010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-427.pth.tar', 78.34400003173828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-425.pth.tar', 78.3380000756836)

Train: 435 [   0/1251 (  0%)]  Loss: 3.033 (3.03)  Time: 2.333s,  438.85/s  (2.333s,  438.85/s)  LR: 1.835e-04  Data: 1.603 (1.603)
Train: 435 [  50/1251 (  4%)]  Loss: 3.246 (3.14)  Time: 0.775s, 1321.39/s  (0.820s, 1249.34/s)  LR: 1.835e-04  Data: 0.009 (0.049)
Train: 435 [ 100/1251 (  8%)]  Loss: 2.896 (3.06)  Time: 0.770s, 1329.76/s  (0.800s, 1279.95/s)  LR: 1.835e-04  Data: 0.010 (0.030)
Train: 435 [ 150/1251 ( 12%)]  Loss: 3.390 (3.14)  Time: 0.816s, 1254.33/s  (0.801s, 1277.91/s)  LR: 1.835e-04  Data: 0.009 (0.023)
Train: 435 [ 200/1251 ( 16%)]  Loss: 3.311 (3.17)  Time: 0.772s, 1326.03/s  (0.800s, 1279.86/s)  LR: 1.835e-04  Data: 0.010 (0.020)
Train: 435 [ 250/1251 ( 20%)]  Loss: 3.203 (3.18)  Time: 0.780s, 1313.46/s  (0.797s, 1285.21/s)  LR: 1.835e-04  Data: 0.009 (0.018)
Train: 435 [ 300/1251 ( 24%)]  Loss: 3.601 (3.24)  Time: 0.772s, 1326.74/s  (0.797s, 1285.55/s)  LR: 1.835e-04  Data: 0.010 (0.017)
Train: 435 [ 350/1251 ( 28%)]  Loss: 3.352 (3.25)  Time: 0.818s, 1252.32/s  (0.794s, 1288.96/s)  LR: 1.835e-04  Data: 0.009 (0.016)
Train: 435 [ 400/1251 ( 32%)]  Loss: 3.233 (3.25)  Time: 0.809s, 1266.11/s  (0.793s, 1290.66/s)  LR: 1.835e-04  Data: 0.010 (0.015)
Train: 435 [ 450/1251 ( 36%)]  Loss: 3.567 (3.28)  Time: 0.847s, 1208.39/s  (0.793s, 1291.03/s)  LR: 1.835e-04  Data: 0.009 (0.015)
Train: 435 [ 500/1251 ( 40%)]  Loss: 3.146 (3.27)  Time: 0.814s, 1257.28/s  (0.793s, 1290.93/s)  LR: 1.835e-04  Data: 0.010 (0.014)
Train: 435 [ 550/1251 ( 44%)]  Loss: 2.977 (3.25)  Time: 0.786s, 1302.76/s  (0.792s, 1292.92/s)  LR: 1.835e-04  Data: 0.009 (0.014)
Train: 435 [ 600/1251 ( 48%)]  Loss: 3.392 (3.26)  Time: 0.787s, 1300.63/s  (0.791s, 1294.17/s)  LR: 1.835e-04  Data: 0.009 (0.013)
Train: 435 [ 650/1251 ( 52%)]  Loss: 3.118 (3.25)  Time: 0.815s, 1256.99/s  (0.791s, 1293.93/s)  LR: 1.835e-04  Data: 0.009 (0.013)
Train: 435 [ 700/1251 ( 56%)]  Loss: 2.998 (3.23)  Time: 0.774s, 1322.65/s  (0.791s, 1295.06/s)  LR: 1.835e-04  Data: 0.010 (0.013)
Train: 435 [ 750/1251 ( 60%)]  Loss: 3.325 (3.24)  Time: 0.805s, 1271.65/s  (0.790s, 1295.91/s)  LR: 1.835e-04  Data: 0.010 (0.013)
Train: 435 [ 800/1251 ( 64%)]  Loss: 3.189 (3.23)  Time: 0.773s, 1324.14/s  (0.790s, 1296.18/s)  LR: 1.835e-04  Data: 0.010 (0.012)
Train: 435 [ 850/1251 ( 68%)]  Loss: 3.375 (3.24)  Time: 0.777s, 1317.58/s  (0.790s, 1296.79/s)  LR: 1.835e-04  Data: 0.010 (0.012)
Train: 435 [ 900/1251 ( 72%)]  Loss: 3.246 (3.24)  Time: 0.776s, 1319.56/s  (0.790s, 1296.10/s)  LR: 1.835e-04  Data: 0.009 (0.012)
Train: 435 [ 950/1251 ( 76%)]  Loss: 3.129 (3.24)  Time: 0.772s, 1326.88/s  (0.790s, 1296.86/s)  LR: 1.835e-04  Data: 0.010 (0.012)
Train: 435 [1000/1251 ( 80%)]  Loss: 3.493 (3.25)  Time: 0.773s, 1324.33/s  (0.789s, 1297.83/s)  LR: 1.835e-04  Data: 0.011 (0.012)
Train: 435 [1050/1251 ( 84%)]  Loss: 3.075 (3.24)  Time: 0.817s, 1252.88/s  (0.790s, 1296.43/s)  LR: 1.835e-04  Data: 0.009 (0.012)
Train: 435 [1100/1251 ( 88%)]  Loss: 3.297 (3.24)  Time: 0.814s, 1258.16/s  (0.791s, 1293.87/s)  LR: 1.835e-04  Data: 0.010 (0.012)
Train: 435 [1150/1251 ( 92%)]  Loss: 2.829 (3.23)  Time: 0.773s, 1324.89/s  (0.792s, 1293.12/s)  LR: 1.835e-04  Data: 0.009 (0.012)
Train: 435 [1200/1251 ( 96%)]  Loss: 3.118 (3.22)  Time: 0.772s, 1325.60/s  (0.792s, 1293.49/s)  LR: 1.835e-04  Data: 0.009 (0.012)
Train: 435 [1250/1251 (100%)]  Loss: 3.067 (3.22)  Time: 0.761s, 1346.32/s  (0.792s, 1293.66/s)  LR: 1.835e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.517 (1.517)  Loss:  0.7866 (0.7866)  Acc@1: 91.0156 (91.0156)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.194 (0.564)  Loss:  0.8462 (1.2304)  Acc@1: 86.2028 (78.2900)  Acc@5: 97.0519 (94.3800)
Train: 436 [   0/1251 (  0%)]  Loss: 3.107 (3.11)  Time: 2.214s,  462.52/s  (2.214s,  462.52/s)  LR: 1.816e-04  Data: 1.445 (1.445)
Train: 436 [  50/1251 (  4%)]  Loss: 3.370 (3.24)  Time: 0.774s, 1323.72/s  (0.826s, 1240.09/s)  LR: 1.816e-04  Data: 0.009 (0.045)
Train: 436 [ 100/1251 (  8%)]  Loss: 2.818 (3.10)  Time: 0.812s, 1260.49/s  (0.809s, 1266.17/s)  LR: 1.816e-04  Data: 0.009 (0.028)
Train: 436 [ 150/1251 ( 12%)]  Loss: 3.334 (3.16)  Time: 0.778s, 1316.72/s  (0.802s, 1276.37/s)  LR: 1.816e-04  Data: 0.009 (0.022)
Train: 436 [ 200/1251 ( 16%)]  Loss: 3.292 (3.18)  Time: 0.816s, 1255.34/s  (0.801s, 1278.73/s)  LR: 1.816e-04  Data: 0.011 (0.019)
Train: 436 [ 250/1251 ( 20%)]  Loss: 3.071 (3.17)  Time: 0.773s, 1324.93/s  (0.798s, 1282.81/s)  LR: 1.816e-04  Data: 0.009 (0.017)
Train: 436 [ 300/1251 ( 24%)]  Loss: 3.547 (3.22)  Time: 0.770s, 1329.68/s  (0.795s, 1287.45/s)  LR: 1.816e-04  Data: 0.008 (0.016)
Train: 436 [ 350/1251 ( 28%)]  Loss: 2.926 (3.18)  Time: 0.772s, 1325.68/s  (0.793s, 1291.10/s)  LR: 1.816e-04  Data: 0.009 (0.015)
Train: 436 [ 400/1251 ( 32%)]  Loss: 3.056 (3.17)  Time: 0.779s, 1313.95/s  (0.792s, 1293.48/s)  LR: 1.816e-04  Data: 0.015 (0.014)
Train: 436 [ 450/1251 ( 36%)]  Loss: 3.373 (3.19)  Time: 0.773s, 1325.54/s  (0.790s, 1296.06/s)  LR: 1.816e-04  Data: 0.009 (0.014)
Train: 436 [ 500/1251 ( 40%)]  Loss: 3.264 (3.20)  Time: 0.773s, 1324.47/s  (0.791s, 1295.00/s)  LR: 1.816e-04  Data: 0.010 (0.013)
Train: 436 [ 550/1251 ( 44%)]  Loss: 3.104 (3.19)  Time: 0.779s, 1314.56/s  (0.790s, 1296.65/s)  LR: 1.816e-04  Data: 0.009 (0.013)
Train: 436 [ 600/1251 ( 48%)]  Loss: 3.017 (3.18)  Time: 0.846s, 1210.44/s  (0.789s, 1297.70/s)  LR: 1.816e-04  Data: 0.009 (0.013)
Train: 436 [ 650/1251 ( 52%)]  Loss: 3.135 (3.17)  Time: 0.775s, 1321.48/s  (0.789s, 1298.14/s)  LR: 1.816e-04  Data: 0.009 (0.013)
Train: 436 [ 700/1251 ( 56%)]  Loss: 3.159 (3.17)  Time: 0.775s, 1321.89/s  (0.788s, 1299.13/s)  LR: 1.816e-04  Data: 0.009 (0.012)
Train: 436 [ 750/1251 ( 60%)]  Loss: 3.022 (3.16)  Time: 0.774s, 1322.58/s  (0.788s, 1300.14/s)  LR: 1.816e-04  Data: 0.010 (0.012)
Train: 436 [ 800/1251 ( 64%)]  Loss: 2.985 (3.15)  Time: 0.797s, 1284.70/s  (0.787s, 1300.64/s)  LR: 1.816e-04  Data: 0.010 (0.012)
Train: 436 [ 850/1251 ( 68%)]  Loss: 3.148 (3.15)  Time: 0.773s, 1324.01/s  (0.787s, 1301.76/s)  LR: 1.816e-04  Data: 0.009 (0.012)
Train: 436 [ 900/1251 ( 72%)]  Loss: 3.449 (3.17)  Time: 0.773s, 1324.74/s  (0.786s, 1302.43/s)  LR: 1.816e-04  Data: 0.010 (0.012)
Train: 436 [ 950/1251 ( 76%)]  Loss: 3.175 (3.17)  Time: 0.771s, 1327.76/s  (0.786s, 1302.41/s)  LR: 1.816e-04  Data: 0.009 (0.012)
Train: 436 [1000/1251 ( 80%)]  Loss: 3.296 (3.17)  Time: 0.785s, 1304.71/s  (0.786s, 1302.65/s)  LR: 1.816e-04  Data: 0.010 (0.012)
Train: 436 [1050/1251 ( 84%)]  Loss: 3.189 (3.17)  Time: 0.774s, 1322.35/s  (0.786s, 1303.16/s)  LR: 1.816e-04  Data: 0.010 (0.011)
Train: 436 [1100/1251 ( 88%)]  Loss: 3.008 (3.17)  Time: 0.782s, 1309.29/s  (0.786s, 1302.26/s)  LR: 1.816e-04  Data: 0.010 (0.011)
Train: 436 [1150/1251 ( 92%)]  Loss: 3.121 (3.17)  Time: 0.777s, 1318.46/s  (0.786s, 1302.07/s)  LR: 1.816e-04  Data: 0.009 (0.011)
Train: 436 [1200/1251 ( 96%)]  Loss: 3.334 (3.17)  Time: 0.779s, 1313.88/s  (0.787s, 1301.73/s)  LR: 1.816e-04  Data: 0.009 (0.011)
Train: 436 [1250/1251 (100%)]  Loss: 3.388 (3.18)  Time: 0.832s, 1230.44/s  (0.786s, 1302.30/s)  LR: 1.816e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.512 (1.512)  Loss:  0.6719 (0.6719)  Acc@1: 91.3086 (91.3086)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.194 (0.562)  Loss:  0.8169 (1.2053)  Acc@1: 86.3208 (78.3520)  Acc@5: 97.5236 (94.4820)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-430.pth.tar', 78.68800002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-431.pth.tar', 78.6360000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-429.pth.tar', 78.56800008300782)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-434.pth.tar', 78.55600000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-426.pth.tar', 78.46600010986329)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-428.pth.tar', 78.41400010742187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-432.pth.tar', 78.38200010986328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-436.pth.tar', 78.35200000488281)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-422.pth.tar', 78.34400010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-427.pth.tar', 78.34400003173828)

Train: 437 [   0/1251 (  0%)]  Loss: 2.946 (2.95)  Time: 2.258s,  453.49/s  (2.258s,  453.49/s)  LR: 1.796e-04  Data: 1.529 (1.529)
Train: 437 [  50/1251 (  4%)]  Loss: 3.500 (3.22)  Time: 0.773s, 1324.48/s  (0.813s, 1259.14/s)  LR: 1.796e-04  Data: 0.009 (0.044)
Train: 437 [ 100/1251 (  8%)]  Loss: 3.231 (3.23)  Time: 0.773s, 1325.34/s  (0.801s, 1279.00/s)  LR: 1.796e-04  Data: 0.010 (0.027)
Train: 437 [ 150/1251 ( 12%)]  Loss: 3.214 (3.22)  Time: 0.798s, 1283.71/s  (0.793s, 1290.82/s)  LR: 1.796e-04  Data: 0.009 (0.021)
Train: 437 [ 200/1251 ( 16%)]  Loss: 2.958 (3.17)  Time: 0.805s, 1271.29/s  (0.794s, 1290.31/s)  LR: 1.796e-04  Data: 0.009 (0.019)
Train: 437 [ 250/1251 ( 20%)]  Loss: 2.960 (3.13)  Time: 0.822s, 1246.23/s  (0.795s, 1288.81/s)  LR: 1.796e-04  Data: 0.010 (0.017)
Train: 437 [ 300/1251 ( 24%)]  Loss: 3.576 (3.20)  Time: 0.770s, 1330.65/s  (0.793s, 1291.23/s)  LR: 1.796e-04  Data: 0.009 (0.016)
Train: 437 [ 350/1251 ( 28%)]  Loss: 3.174 (3.19)  Time: 0.776s, 1319.49/s  (0.791s, 1294.44/s)  LR: 1.796e-04  Data: 0.009 (0.015)
Train: 437 [ 400/1251 ( 32%)]  Loss: 2.959 (3.17)  Time: 0.771s, 1327.97/s  (0.790s, 1296.76/s)  LR: 1.796e-04  Data: 0.010 (0.014)
Train: 437 [ 450/1251 ( 36%)]  Loss: 3.130 (3.16)  Time: 0.771s, 1328.34/s  (0.788s, 1298.96/s)  LR: 1.796e-04  Data: 0.010 (0.014)
Train: 437 [ 500/1251 ( 40%)]  Loss: 3.351 (3.18)  Time: 0.771s, 1327.43/s  (0.787s, 1300.97/s)  LR: 1.796e-04  Data: 0.010 (0.013)
Train: 437 [ 550/1251 ( 44%)]  Loss: 3.302 (3.19)  Time: 0.816s, 1254.34/s  (0.787s, 1300.70/s)  LR: 1.796e-04  Data: 0.009 (0.013)
Train: 437 [ 600/1251 ( 48%)]  Loss: 3.392 (3.21)  Time: 0.861s, 1189.36/s  (0.790s, 1296.14/s)  LR: 1.796e-04  Data: 0.009 (0.013)
Train: 437 [ 650/1251 ( 52%)]  Loss: 2.939 (3.19)  Time: 0.776s, 1320.05/s  (0.789s, 1297.83/s)  LR: 1.796e-04  Data: 0.010 (0.012)
Train: 437 [ 700/1251 ( 56%)]  Loss: 3.259 (3.19)  Time: 0.816s, 1255.31/s  (0.791s, 1294.80/s)  LR: 1.796e-04  Data: 0.011 (0.012)
Train: 437 [ 750/1251 ( 60%)]  Loss: 3.113 (3.19)  Time: 0.771s, 1328.06/s  (0.790s, 1295.78/s)  LR: 1.796e-04  Data: 0.009 (0.012)
Train: 437 [ 800/1251 ( 64%)]  Loss: 3.222 (3.19)  Time: 0.772s, 1326.06/s  (0.790s, 1296.70/s)  LR: 1.796e-04  Data: 0.009 (0.012)
Train: 437 [ 850/1251 ( 68%)]  Loss: 3.184 (3.19)  Time: 0.773s, 1324.58/s  (0.790s, 1296.44/s)  LR: 1.796e-04  Data: 0.010 (0.012)
Train: 437 [ 900/1251 ( 72%)]  Loss: 3.298 (3.20)  Time: 0.773s, 1325.09/s  (0.789s, 1297.67/s)  LR: 1.796e-04  Data: 0.009 (0.012)
Train: 437 [ 950/1251 ( 76%)]  Loss: 3.127 (3.19)  Time: 0.771s, 1328.26/s  (0.789s, 1298.25/s)  LR: 1.796e-04  Data: 0.010 (0.012)
Train: 437 [1000/1251 ( 80%)]  Loss: 3.138 (3.19)  Time: 0.774s, 1323.54/s  (0.788s, 1298.80/s)  LR: 1.796e-04  Data: 0.010 (0.012)
Train: 437 [1050/1251 ( 84%)]  Loss: 3.524 (3.20)  Time: 0.810s, 1264.83/s  (0.789s, 1298.61/s)  LR: 1.796e-04  Data: 0.010 (0.011)
Train: 437 [1100/1251 ( 88%)]  Loss: 3.055 (3.20)  Time: 0.771s, 1328.07/s  (0.789s, 1297.23/s)  LR: 1.796e-04  Data: 0.009 (0.011)
Train: 437 [1150/1251 ( 92%)]  Loss: 3.177 (3.20)  Time: 0.774s, 1322.43/s  (0.789s, 1297.82/s)  LR: 1.796e-04  Data: 0.009 (0.011)
Train: 437 [1200/1251 ( 96%)]  Loss: 2.996 (3.19)  Time: 0.778s, 1316.26/s  (0.789s, 1298.16/s)  LR: 1.796e-04  Data: 0.010 (0.011)
Train: 437 [1250/1251 (100%)]  Loss: 3.276 (3.19)  Time: 0.770s, 1329.63/s  (0.788s, 1298.95/s)  LR: 1.796e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.580 (1.580)  Loss:  0.7866 (0.7866)  Acc@1: 91.7969 (91.7969)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.194 (0.561)  Loss:  0.9546 (1.2396)  Acc@1: 85.8491 (78.2080)  Acc@5: 97.6415 (94.3380)
Train: 438 [   0/1251 (  0%)]  Loss: 2.898 (2.90)  Time: 2.424s,  422.46/s  (2.424s,  422.46/s)  LR: 1.777e-04  Data: 1.697 (1.697)
Train: 438 [  50/1251 (  4%)]  Loss: 3.071 (2.98)  Time: 0.774s, 1322.68/s  (0.818s, 1251.19/s)  LR: 1.777e-04  Data: 0.011 (0.048)
Train: 438 [ 100/1251 (  8%)]  Loss: 3.155 (3.04)  Time: 0.780s, 1312.36/s  (0.800s, 1280.41/s)  LR: 1.777e-04  Data: 0.010 (0.029)
Train: 438 [ 150/1251 ( 12%)]  Loss: 3.069 (3.05)  Time: 0.788s, 1299.68/s  (0.801s, 1278.85/s)  LR: 1.777e-04  Data: 0.009 (0.023)
Train: 438 [ 200/1251 ( 16%)]  Loss: 3.301 (3.10)  Time: 0.773s, 1324.97/s  (0.796s, 1285.88/s)  LR: 1.777e-04  Data: 0.010 (0.019)
Train: 438 [ 250/1251 ( 20%)]  Loss: 3.409 (3.15)  Time: 0.772s, 1327.09/s  (0.793s, 1291.18/s)  LR: 1.777e-04  Data: 0.010 (0.018)
Train: 438 [ 300/1251 ( 24%)]  Loss: 3.232 (3.16)  Time: 0.773s, 1325.33/s  (0.792s, 1293.57/s)  LR: 1.777e-04  Data: 0.009 (0.016)
Train: 438 [ 350/1251 ( 28%)]  Loss: 3.140 (3.16)  Time: 0.780s, 1312.97/s  (0.790s, 1296.67/s)  LR: 1.777e-04  Data: 0.010 (0.015)
Train: 438 [ 400/1251 ( 32%)]  Loss: 2.891 (3.13)  Time: 0.773s, 1325.39/s  (0.789s, 1298.46/s)  LR: 1.777e-04  Data: 0.010 (0.015)
Train: 438 [ 450/1251 ( 36%)]  Loss: 3.165 (3.13)  Time: 0.780s, 1312.80/s  (0.789s, 1297.88/s)  LR: 1.777e-04  Data: 0.009 (0.014)
Train: 438 [ 500/1251 ( 40%)]  Loss: 3.333 (3.15)  Time: 0.777s, 1318.36/s  (0.788s, 1299.61/s)  LR: 1.777e-04  Data: 0.009 (0.014)
Train: 438 [ 550/1251 ( 44%)]  Loss: 2.939 (3.13)  Time: 0.773s, 1325.14/s  (0.787s, 1300.90/s)  LR: 1.777e-04  Data: 0.010 (0.013)
Train: 438 [ 600/1251 ( 48%)]  Loss: 3.060 (3.13)  Time: 0.772s, 1326.36/s  (0.787s, 1300.49/s)  LR: 1.777e-04  Data: 0.009 (0.013)
Train: 438 [ 650/1251 ( 52%)]  Loss: 3.149 (3.13)  Time: 0.773s, 1324.07/s  (0.787s, 1300.67/s)  LR: 1.777e-04  Data: 0.009 (0.013)
Train: 438 [ 700/1251 ( 56%)]  Loss: 3.124 (3.13)  Time: 0.773s, 1324.65/s  (0.787s, 1301.95/s)  LR: 1.777e-04  Data: 0.010 (0.012)
Train: 438 [ 750/1251 ( 60%)]  Loss: 3.421 (3.15)  Time: 0.772s, 1325.65/s  (0.786s, 1302.84/s)  LR: 1.777e-04  Data: 0.010 (0.012)
Train: 438 [ 800/1251 ( 64%)]  Loss: 3.354 (3.16)  Time: 0.780s, 1312.05/s  (0.786s, 1303.29/s)  LR: 1.777e-04  Data: 0.010 (0.012)
Train: 438 [ 850/1251 ( 68%)]  Loss: 3.028 (3.15)  Time: 0.789s, 1297.16/s  (0.785s, 1303.75/s)  LR: 1.777e-04  Data: 0.014 (0.012)
Train: 438 [ 900/1251 ( 72%)]  Loss: 3.403 (3.17)  Time: 0.797s, 1285.57/s  (0.785s, 1304.17/s)  LR: 1.777e-04  Data: 0.010 (0.012)
Train: 438 [ 950/1251 ( 76%)]  Loss: 3.332 (3.17)  Time: 0.774s, 1322.84/s  (0.785s, 1304.62/s)  LR: 1.777e-04  Data: 0.010 (0.012)
Train: 438 [1000/1251 ( 80%)]  Loss: 3.152 (3.17)  Time: 0.816s, 1255.10/s  (0.785s, 1304.70/s)  LR: 1.777e-04  Data: 0.011 (0.012)
Train: 438 [1050/1251 ( 84%)]  Loss: 3.228 (3.18)  Time: 0.774s, 1323.76/s  (0.785s, 1304.83/s)  LR: 1.777e-04  Data: 0.010 (0.012)
Train: 438 [1100/1251 ( 88%)]  Loss: 3.207 (3.18)  Time: 0.813s, 1259.34/s  (0.785s, 1305.03/s)  LR: 1.777e-04  Data: 0.010 (0.012)
Train: 438 [1150/1251 ( 92%)]  Loss: 3.455 (3.19)  Time: 0.807s, 1269.44/s  (0.785s, 1304.98/s)  LR: 1.777e-04  Data: 0.010 (0.012)
Train: 438 [1200/1251 ( 96%)]  Loss: 3.371 (3.20)  Time: 0.776s, 1319.33/s  (0.785s, 1303.98/s)  LR: 1.777e-04  Data: 0.010 (0.012)
Train: 438 [1250/1251 (100%)]  Loss: 3.319 (3.20)  Time: 0.759s, 1348.58/s  (0.785s, 1304.64/s)  LR: 1.777e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.570 (1.570)  Loss:  0.5933 (0.5933)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.194 (0.560)  Loss:  0.7124 (1.0988)  Acc@1: 86.9104 (78.4840)  Acc@5: 98.3491 (94.5020)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-430.pth.tar', 78.68800002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-431.pth.tar', 78.6360000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-429.pth.tar', 78.56800008300782)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-434.pth.tar', 78.55600000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-438.pth.tar', 78.48399987304687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-426.pth.tar', 78.46600010986329)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-428.pth.tar', 78.41400010742187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-432.pth.tar', 78.38200010986328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-436.pth.tar', 78.35200000488281)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-422.pth.tar', 78.34400010498047)

Train: 439 [   0/1251 (  0%)]  Loss: 3.108 (3.11)  Time: 2.182s,  469.38/s  (2.182s,  469.38/s)  LR: 1.757e-04  Data: 1.430 (1.430)
Train: 439 [  50/1251 (  4%)]  Loss: 3.225 (3.17)  Time: 0.772s, 1325.59/s  (0.814s, 1258.02/s)  LR: 1.757e-04  Data: 0.011 (0.042)
Train: 439 [ 100/1251 (  8%)]  Loss: 3.290 (3.21)  Time: 0.783s, 1307.26/s  (0.798s, 1282.99/s)  LR: 1.757e-04  Data: 0.010 (0.026)
Train: 439 [ 150/1251 ( 12%)]  Loss: 3.136 (3.19)  Time: 0.794s, 1290.19/s  (0.795s, 1288.33/s)  LR: 1.757e-04  Data: 0.011 (0.021)
Train: 439 [ 200/1251 ( 16%)]  Loss: 3.439 (3.24)  Time: 0.773s, 1325.03/s  (0.791s, 1294.30/s)  LR: 1.757e-04  Data: 0.010 (0.018)
Train: 439 [ 250/1251 ( 20%)]  Loss: 3.355 (3.26)  Time: 0.784s, 1306.22/s  (0.791s, 1294.86/s)  LR: 1.757e-04  Data: 0.011 (0.017)
Train: 439 [ 300/1251 ( 24%)]  Loss: 3.000 (3.22)  Time: 0.778s, 1316.96/s  (0.790s, 1296.87/s)  LR: 1.757e-04  Data: 0.010 (0.016)
Train: 439 [ 350/1251 ( 28%)]  Loss: 3.467 (3.25)  Time: 0.806s, 1270.37/s  (0.790s, 1296.44/s)  LR: 1.757e-04  Data: 0.010 (0.015)
Train: 439 [ 400/1251 ( 32%)]  Loss: 3.645 (3.30)  Time: 0.777s, 1317.26/s  (0.790s, 1296.39/s)  LR: 1.757e-04  Data: 0.011 (0.014)
Train: 439 [ 450/1251 ( 36%)]  Loss: 3.335 (3.30)  Time: 0.772s, 1326.26/s  (0.789s, 1297.86/s)  LR: 1.757e-04  Data: 0.010 (0.014)
Train: 439 [ 500/1251 ( 40%)]  Loss: 3.192 (3.29)  Time: 0.814s, 1257.70/s  (0.789s, 1298.39/s)  LR: 1.757e-04  Data: 0.011 (0.013)
Train: 439 [ 550/1251 ( 44%)]  Loss: 3.252 (3.29)  Time: 0.773s, 1324.15/s  (0.789s, 1298.04/s)  LR: 1.757e-04  Data: 0.010 (0.013)
Train: 439 [ 600/1251 ( 48%)]  Loss: 3.152 (3.28)  Time: 0.790s, 1295.63/s  (0.788s, 1299.45/s)  LR: 1.757e-04  Data: 0.010 (0.013)
Train: 439 [ 650/1251 ( 52%)]  Loss: 3.198 (3.27)  Time: 0.786s, 1303.23/s  (0.787s, 1300.89/s)  LR: 1.757e-04  Data: 0.011 (0.013)
Train: 439 [ 700/1251 ( 56%)]  Loss: 3.260 (3.27)  Time: 0.773s, 1325.39/s  (0.787s, 1301.54/s)  LR: 1.757e-04  Data: 0.010 (0.012)
Train: 439 [ 750/1251 ( 60%)]  Loss: 3.223 (3.27)  Time: 0.876s, 1168.78/s  (0.786s, 1302.25/s)  LR: 1.757e-04  Data: 0.010 (0.012)
Train: 439 [ 800/1251 ( 64%)]  Loss: 3.379 (3.27)  Time: 0.773s, 1325.22/s  (0.786s, 1303.13/s)  LR: 1.757e-04  Data: 0.010 (0.012)
Train: 439 [ 850/1251 ( 68%)]  Loss: 2.869 (3.25)  Time: 0.811s, 1263.03/s  (0.787s, 1300.63/s)  LR: 1.757e-04  Data: 0.010 (0.012)
Train: 439 [ 900/1251 ( 72%)]  Loss: 3.289 (3.25)  Time: 0.779s, 1314.95/s  (0.787s, 1301.62/s)  LR: 1.757e-04  Data: 0.010 (0.012)
Train: 439 [ 950/1251 ( 76%)]  Loss: 2.997 (3.24)  Time: 0.776s, 1319.99/s  (0.787s, 1301.92/s)  LR: 1.757e-04  Data: 0.010 (0.012)
Train: 439 [1000/1251 ( 80%)]  Loss: 3.194 (3.24)  Time: 0.829s, 1234.70/s  (0.786s, 1302.37/s)  LR: 1.757e-04  Data: 0.010 (0.012)
Train: 439 [1050/1251 ( 84%)]  Loss: 3.318 (3.24)  Time: 0.773s, 1324.45/s  (0.786s, 1302.98/s)  LR: 1.757e-04  Data: 0.010 (0.012)
Train: 439 [1100/1251 ( 88%)]  Loss: 2.965 (3.23)  Time: 0.784s, 1306.45/s  (0.786s, 1302.87/s)  LR: 1.757e-04  Data: 0.010 (0.011)
Train: 439 [1150/1251 ( 92%)]  Loss: 2.818 (3.21)  Time: 0.778s, 1315.98/s  (0.786s, 1303.32/s)  LR: 1.757e-04  Data: 0.009 (0.011)
Train: 439 [1200/1251 ( 96%)]  Loss: 3.129 (3.21)  Time: 0.770s, 1329.25/s  (0.786s, 1302.58/s)  LR: 1.757e-04  Data: 0.009 (0.011)
Train: 439 [1250/1251 (100%)]  Loss: 3.013 (3.20)  Time: 0.762s, 1344.05/s  (0.786s, 1303.02/s)  LR: 1.757e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.543 (1.543)  Loss:  0.5889 (0.5889)  Acc@1: 91.5039 (91.5039)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.194 (0.566)  Loss:  0.7510 (1.0790)  Acc@1: 86.6745 (78.8040)  Acc@5: 97.1698 (94.5120)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-439.pth.tar', 78.80400002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-430.pth.tar', 78.68800002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-431.pth.tar', 78.6360000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-429.pth.tar', 78.56800008300782)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-434.pth.tar', 78.55600000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-438.pth.tar', 78.48399987304687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-426.pth.tar', 78.46600010986329)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-428.pth.tar', 78.41400010742187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-432.pth.tar', 78.38200010986328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-436.pth.tar', 78.35200000488281)

Train: 440 [   0/1251 (  0%)]  Loss: 2.955 (2.95)  Time: 2.253s,  454.59/s  (2.253s,  454.59/s)  LR: 1.738e-04  Data: 1.517 (1.517)
Train: 440 [  50/1251 (  4%)]  Loss: 3.100 (3.03)  Time: 0.785s, 1304.14/s  (0.814s, 1258.73/s)  LR: 1.738e-04  Data: 0.010 (0.045)
Train: 440 [ 100/1251 (  8%)]  Loss: 2.893 (2.98)  Time: 0.773s, 1324.68/s  (0.798s, 1283.40/s)  LR: 1.738e-04  Data: 0.010 (0.028)
Train: 440 [ 150/1251 ( 12%)]  Loss: 2.979 (2.98)  Time: 0.776s, 1320.05/s  (0.794s, 1288.93/s)  LR: 1.738e-04  Data: 0.010 (0.022)
Train: 440 [ 200/1251 ( 16%)]  Loss: 2.964 (2.98)  Time: 0.772s, 1325.94/s  (0.791s, 1295.20/s)  LR: 1.738e-04  Data: 0.010 (0.019)
Train: 440 [ 250/1251 ( 20%)]  Loss: 3.596 (3.08)  Time: 0.774s, 1322.40/s  (0.788s, 1299.96/s)  LR: 1.738e-04  Data: 0.009 (0.017)
Train: 440 [ 300/1251 ( 24%)]  Loss: 2.806 (3.04)  Time: 0.775s, 1321.75/s  (0.786s, 1302.87/s)  LR: 1.738e-04  Data: 0.010 (0.016)
Train: 440 [ 350/1251 ( 28%)]  Loss: 3.122 (3.05)  Time: 0.774s, 1322.57/s  (0.785s, 1305.07/s)  LR: 1.738e-04  Data: 0.010 (0.015)
Train: 440 [ 400/1251 ( 32%)]  Loss: 3.164 (3.06)  Time: 0.774s, 1322.74/s  (0.784s, 1306.04/s)  LR: 1.738e-04  Data: 0.009 (0.014)
Train: 440 [ 450/1251 ( 36%)]  Loss: 3.313 (3.09)  Time: 0.773s, 1325.34/s  (0.783s, 1307.29/s)  LR: 1.738e-04  Data: 0.011 (0.014)
Train: 440 [ 500/1251 ( 40%)]  Loss: 2.799 (3.06)  Time: 0.774s, 1322.95/s  (0.784s, 1305.90/s)  LR: 1.738e-04  Data: 0.010 (0.013)
Train: 440 [ 550/1251 ( 44%)]  Loss: 3.355 (3.09)  Time: 0.772s, 1326.89/s  (0.784s, 1306.80/s)  LR: 1.738e-04  Data: 0.010 (0.013)
Train: 440 [ 600/1251 ( 48%)]  Loss: 3.109 (3.09)  Time: 0.778s, 1316.76/s  (0.783s, 1307.40/s)  LR: 1.738e-04  Data: 0.009 (0.013)
Train: 440 [ 650/1251 ( 52%)]  Loss: 3.197 (3.10)  Time: 0.828s, 1236.41/s  (0.784s, 1305.62/s)  LR: 1.738e-04  Data: 0.011 (0.013)
Train: 440 [ 700/1251 ( 56%)]  Loss: 3.246 (3.11)  Time: 0.827s, 1238.64/s  (0.786s, 1302.18/s)  LR: 1.738e-04  Data: 0.010 (0.013)
Train: 440 [ 750/1251 ( 60%)]  Loss: 3.167 (3.11)  Time: 0.772s, 1325.61/s  (0.787s, 1301.15/s)  LR: 1.738e-04  Data: 0.009 (0.012)
Train: 440 [ 800/1251 ( 64%)]  Loss: 3.305 (3.12)  Time: 0.775s, 1320.55/s  (0.786s, 1301.97/s)  LR: 1.738e-04  Data: 0.014 (0.012)
Train: 440 [ 850/1251 ( 68%)]  Loss: 3.219 (3.13)  Time: 0.793s, 1291.43/s  (0.786s, 1302.41/s)  LR: 1.738e-04  Data: 0.010 (0.012)
Train: 440 [ 900/1251 ( 72%)]  Loss: 3.410 (3.14)  Time: 0.772s, 1325.94/s  (0.786s, 1302.88/s)  LR: 1.738e-04  Data: 0.009 (0.012)
Train: 440 [ 950/1251 ( 76%)]  Loss: 2.861 (3.13)  Time: 0.773s, 1325.04/s  (0.786s, 1303.51/s)  LR: 1.738e-04  Data: 0.010 (0.012)
Train: 440 [1000/1251 ( 80%)]  Loss: 3.261 (3.13)  Time: 0.774s, 1322.56/s  (0.785s, 1303.93/s)  LR: 1.738e-04  Data: 0.009 (0.012)
Train: 440 [1050/1251 ( 84%)]  Loss: 3.360 (3.14)  Time: 0.783s, 1307.20/s  (0.786s, 1302.46/s)  LR: 1.738e-04  Data: 0.014 (0.012)
Train: 440 [1100/1251 ( 88%)]  Loss: 3.165 (3.15)  Time: 0.774s, 1323.68/s  (0.786s, 1302.96/s)  LR: 1.738e-04  Data: 0.010 (0.012)
Train: 440 [1150/1251 ( 92%)]  Loss: 3.304 (3.15)  Time: 0.815s, 1257.12/s  (0.787s, 1301.60/s)  LR: 1.738e-04  Data: 0.011 (0.012)
Train: 440 [1200/1251 ( 96%)]  Loss: 3.418 (3.16)  Time: 0.774s, 1322.28/s  (0.786s, 1302.16/s)  LR: 1.738e-04  Data: 0.009 (0.011)
Train: 440 [1250/1251 (100%)]  Loss: 3.292 (3.17)  Time: 0.759s, 1349.01/s  (0.786s, 1302.94/s)  LR: 1.738e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.560 (1.560)  Loss:  0.7061 (0.7061)  Acc@1: 91.4062 (91.4062)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.194 (0.559)  Loss:  0.7856 (1.1238)  Acc@1: 86.6745 (78.7300)  Acc@5: 97.9953 (94.5840)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-439.pth.tar', 78.80400002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-440.pth.tar', 78.72999989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-430.pth.tar', 78.68800002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-431.pth.tar', 78.6360000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-429.pth.tar', 78.56800008300782)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-434.pth.tar', 78.55600000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-438.pth.tar', 78.48399987304687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-426.pth.tar', 78.46600010986329)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-428.pth.tar', 78.41400010742187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-432.pth.tar', 78.38200010986328)

Train: 441 [   0/1251 (  0%)]  Loss: 3.077 (3.08)  Time: 2.467s,  415.12/s  (2.467s,  415.12/s)  LR: 1.719e-04  Data: 1.694 (1.694)
Train: 441 [  50/1251 (  4%)]  Loss: 2.858 (2.97)  Time: 0.776s, 1319.59/s  (0.822s, 1245.76/s)  LR: 1.719e-04  Data: 0.010 (0.043)
Train: 441 [ 100/1251 (  8%)]  Loss: 3.329 (3.09)  Time: 0.792s, 1293.54/s  (0.802s, 1277.25/s)  LR: 1.719e-04  Data: 0.010 (0.027)
Train: 441 [ 150/1251 ( 12%)]  Loss: 3.349 (3.15)  Time: 0.772s, 1326.45/s  (0.793s, 1290.49/s)  LR: 1.719e-04  Data: 0.009 (0.021)
Train: 441 [ 200/1251 ( 16%)]  Loss: 2.832 (3.09)  Time: 0.815s, 1256.52/s  (0.792s, 1292.25/s)  LR: 1.719e-04  Data: 0.011 (0.018)
Train: 441 [ 250/1251 ( 20%)]  Loss: 3.292 (3.12)  Time: 0.771s, 1327.29/s  (0.791s, 1294.58/s)  LR: 1.719e-04  Data: 0.010 (0.017)
Train: 441 [ 300/1251 ( 24%)]  Loss: 3.171 (3.13)  Time: 0.775s, 1320.45/s  (0.789s, 1297.70/s)  LR: 1.719e-04  Data: 0.010 (0.016)
Train: 441 [ 350/1251 ( 28%)]  Loss: 3.403 (3.16)  Time: 0.772s, 1325.68/s  (0.788s, 1299.13/s)  LR: 1.719e-04  Data: 0.009 (0.015)
Train: 441 [ 400/1251 ( 32%)]  Loss: 3.043 (3.15)  Time: 0.773s, 1324.52/s  (0.787s, 1300.46/s)  LR: 1.719e-04  Data: 0.010 (0.014)
Train: 441 [ 450/1251 ( 36%)]  Loss: 3.082 (3.14)  Time: 0.771s, 1327.74/s  (0.787s, 1301.91/s)  LR: 1.719e-04  Data: 0.010 (0.014)
Train: 441 [ 500/1251 ( 40%)]  Loss: 3.333 (3.16)  Time: 0.773s, 1325.46/s  (0.788s, 1300.10/s)  LR: 1.719e-04  Data: 0.009 (0.013)
Train: 441 [ 550/1251 ( 44%)]  Loss: 3.297 (3.17)  Time: 0.785s, 1303.65/s  (0.788s, 1300.06/s)  LR: 1.719e-04  Data: 0.010 (0.013)
Train: 441 [ 600/1251 ( 48%)]  Loss: 3.123 (3.17)  Time: 0.775s, 1321.02/s  (0.787s, 1300.51/s)  LR: 1.719e-04  Data: 0.010 (0.013)
Train: 441 [ 650/1251 ( 52%)]  Loss: 2.867 (3.15)  Time: 0.780s, 1312.92/s  (0.787s, 1301.35/s)  LR: 1.719e-04  Data: 0.010 (0.013)
Train: 441 [ 700/1251 ( 56%)]  Loss: 3.091 (3.14)  Time: 0.830s, 1234.03/s  (0.787s, 1301.77/s)  LR: 1.719e-04  Data: 0.009 (0.012)
Train: 441 [ 750/1251 ( 60%)]  Loss: 3.370 (3.16)  Time: 0.795s, 1287.40/s  (0.787s, 1301.09/s)  LR: 1.719e-04  Data: 0.010 (0.012)
Train: 441 [ 800/1251 ( 64%)]  Loss: 3.031 (3.15)  Time: 0.774s, 1323.52/s  (0.787s, 1301.94/s)  LR: 1.719e-04  Data: 0.009 (0.012)
Train: 441 [ 850/1251 ( 68%)]  Loss: 3.080 (3.15)  Time: 0.792s, 1293.07/s  (0.786s, 1302.57/s)  LR: 1.719e-04  Data: 0.009 (0.012)
Train: 441 [ 900/1251 ( 72%)]  Loss: 3.263 (3.15)  Time: 0.773s, 1324.94/s  (0.786s, 1302.75/s)  LR: 1.719e-04  Data: 0.010 (0.012)
Train: 441 [ 950/1251 ( 76%)]  Loss: 3.299 (3.16)  Time: 0.792s, 1292.21/s  (0.786s, 1303.04/s)  LR: 1.719e-04  Data: 0.013 (0.012)
Train: 441 [1000/1251 ( 80%)]  Loss: 3.135 (3.16)  Time: 0.773s, 1324.32/s  (0.786s, 1302.46/s)  LR: 1.719e-04  Data: 0.010 (0.012)
Train: 441 [1050/1251 ( 84%)]  Loss: 3.577 (3.18)  Time: 0.777s, 1317.25/s  (0.787s, 1300.71/s)  LR: 1.719e-04  Data: 0.012 (0.012)
Train: 441 [1100/1251 ( 88%)]  Loss: 2.865 (3.16)  Time: 0.783s, 1307.30/s  (0.788s, 1300.28/s)  LR: 1.719e-04  Data: 0.010 (0.012)
Train: 441 [1150/1251 ( 92%)]  Loss: 3.438 (3.18)  Time: 0.772s, 1325.80/s  (0.788s, 1299.98/s)  LR: 1.719e-04  Data: 0.009 (0.012)
Train: 441 [1200/1251 ( 96%)]  Loss: 3.589 (3.19)  Time: 0.826s, 1239.52/s  (0.788s, 1299.78/s)  LR: 1.719e-04  Data: 0.009 (0.011)
Train: 441 [1250/1251 (100%)]  Loss: 3.579 (3.21)  Time: 0.760s, 1347.71/s  (0.788s, 1299.64/s)  LR: 1.719e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.575 (1.575)  Loss:  0.6904 (0.6904)  Acc@1: 91.5039 (91.5039)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.194 (0.567)  Loss:  0.7881 (1.1544)  Acc@1: 86.3208 (78.7760)  Acc@5: 97.6415 (94.5360)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-439.pth.tar', 78.80400002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-441.pth.tar', 78.77600000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-440.pth.tar', 78.72999989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-430.pth.tar', 78.68800002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-431.pth.tar', 78.6360000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-429.pth.tar', 78.56800008300782)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-434.pth.tar', 78.55600000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-438.pth.tar', 78.48399987304687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-426.pth.tar', 78.46600010986329)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-428.pth.tar', 78.41400010742187)

Train: 442 [   0/1251 (  0%)]  Loss: 3.356 (3.36)  Time: 2.372s,  431.61/s  (2.372s,  431.61/s)  LR: 1.699e-04  Data: 1.631 (1.631)
Train: 442 [  50/1251 (  4%)]  Loss: 3.262 (3.31)  Time: 0.815s, 1256.95/s  (0.828s, 1237.14/s)  LR: 1.699e-04  Data: 0.011 (0.049)
Train: 442 [ 100/1251 (  8%)]  Loss: 3.227 (3.28)  Time: 0.772s, 1326.61/s  (0.813s, 1260.20/s)  LR: 1.699e-04  Data: 0.010 (0.030)
Train: 442 [ 150/1251 ( 12%)]  Loss: 3.499 (3.34)  Time: 0.783s, 1307.68/s  (0.802s, 1276.25/s)  LR: 1.699e-04  Data: 0.011 (0.024)
Train: 442 [ 200/1251 ( 16%)]  Loss: 3.002 (3.27)  Time: 0.780s, 1313.02/s  (0.801s, 1278.34/s)  LR: 1.699e-04  Data: 0.010 (0.020)
Train: 442 [ 250/1251 ( 20%)]  Loss: 3.067 (3.24)  Time: 0.772s, 1326.12/s  (0.798s, 1283.20/s)  LR: 1.699e-04  Data: 0.009 (0.018)
Train: 442 [ 300/1251 ( 24%)]  Loss: 3.317 (3.25)  Time: 0.783s, 1308.26/s  (0.795s, 1288.36/s)  LR: 1.699e-04  Data: 0.011 (0.017)
Train: 442 [ 350/1251 ( 28%)]  Loss: 3.011 (3.22)  Time: 0.773s, 1324.19/s  (0.793s, 1291.45/s)  LR: 1.699e-04  Data: 0.010 (0.016)
Train: 442 [ 400/1251 ( 32%)]  Loss: 3.143 (3.21)  Time: 0.815s, 1256.01/s  (0.795s, 1288.51/s)  LR: 1.699e-04  Data: 0.012 (0.015)
Train: 442 [ 450/1251 ( 36%)]  Loss: 3.057 (3.19)  Time: 0.775s, 1322.09/s  (0.793s, 1290.65/s)  LR: 1.699e-04  Data: 0.009 (0.015)
Train: 442 [ 500/1251 ( 40%)]  Loss: 3.299 (3.20)  Time: 0.775s, 1321.96/s  (0.792s, 1292.99/s)  LR: 1.699e-04  Data: 0.009 (0.014)
Train: 442 [ 550/1251 ( 44%)]  Loss: 3.151 (3.20)  Time: 0.774s, 1323.52/s  (0.791s, 1295.13/s)  LR: 1.699e-04  Data: 0.009 (0.014)
Train: 442 [ 600/1251 ( 48%)]  Loss: 3.367 (3.21)  Time: 0.776s, 1319.97/s  (0.792s, 1292.50/s)  LR: 1.699e-04  Data: 0.010 (0.013)
Train: 442 [ 650/1251 ( 52%)]  Loss: 2.868 (3.19)  Time: 0.779s, 1315.05/s  (0.791s, 1294.17/s)  LR: 1.699e-04  Data: 0.010 (0.013)
Train: 442 [ 700/1251 ( 56%)]  Loss: 3.292 (3.19)  Time: 0.835s, 1227.04/s  (0.790s, 1295.42/s)  LR: 1.699e-04  Data: 0.009 (0.013)
Train: 442 [ 750/1251 ( 60%)]  Loss: 3.210 (3.20)  Time: 0.854s, 1199.66/s  (0.791s, 1295.14/s)  LR: 1.699e-04  Data: 0.010 (0.013)
Train: 442 [ 800/1251 ( 64%)]  Loss: 2.993 (3.18)  Time: 0.776s, 1319.62/s  (0.790s, 1295.53/s)  LR: 1.699e-04  Data: 0.010 (0.012)
Train: 442 [ 850/1251 ( 68%)]  Loss: 3.208 (3.18)  Time: 0.774s, 1322.46/s  (0.790s, 1295.39/s)  LR: 1.699e-04  Data: 0.010 (0.012)
Train: 442 [ 900/1251 ( 72%)]  Loss: 2.925 (3.17)  Time: 0.775s, 1321.74/s  (0.790s, 1296.31/s)  LR: 1.699e-04  Data: 0.009 (0.012)
Train: 442 [ 950/1251 ( 76%)]  Loss: 3.220 (3.17)  Time: 0.774s, 1322.34/s  (0.789s, 1297.27/s)  LR: 1.699e-04  Data: 0.010 (0.012)
Train: 442 [1000/1251 ( 80%)]  Loss: 3.252 (3.18)  Time: 0.800s, 1280.78/s  (0.789s, 1297.82/s)  LR: 1.699e-04  Data: 0.009 (0.012)
Train: 442 [1050/1251 ( 84%)]  Loss: 3.359 (3.19)  Time: 0.779s, 1315.15/s  (0.788s, 1298.68/s)  LR: 1.699e-04  Data: 0.010 (0.012)
Train: 442 [1100/1251 ( 88%)]  Loss: 2.952 (3.18)  Time: 0.782s, 1310.21/s  (0.789s, 1297.69/s)  LR: 1.699e-04  Data: 0.009 (0.012)
Train: 442 [1150/1251 ( 92%)]  Loss: 3.304 (3.18)  Time: 0.775s, 1322.12/s  (0.789s, 1298.43/s)  LR: 1.699e-04  Data: 0.010 (0.012)
Train: 442 [1200/1251 ( 96%)]  Loss: 3.011 (3.17)  Time: 0.772s, 1327.09/s  (0.789s, 1298.55/s)  LR: 1.699e-04  Data: 0.009 (0.012)
Train: 442 [1250/1251 (100%)]  Loss: 2.874 (3.16)  Time: 0.775s, 1320.68/s  (0.788s, 1299.17/s)  LR: 1.699e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.596 (1.596)  Loss:  0.6431 (0.6431)  Acc@1: 90.0391 (90.0391)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.193 (0.556)  Loss:  0.7251 (1.1198)  Acc@1: 86.9104 (78.6320)  Acc@5: 97.8774 (94.4260)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-439.pth.tar', 78.80400002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-441.pth.tar', 78.77600000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-440.pth.tar', 78.72999989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-430.pth.tar', 78.68800002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-431.pth.tar', 78.6360000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-442.pth.tar', 78.6320000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-429.pth.tar', 78.56800008300782)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-434.pth.tar', 78.55600000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-438.pth.tar', 78.48399987304687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-426.pth.tar', 78.46600010986329)

Train: 443 [   0/1251 (  0%)]  Loss: 3.049 (3.05)  Time: 2.342s,  437.19/s  (2.342s,  437.19/s)  LR: 1.680e-04  Data: 1.613 (1.613)
Train: 443 [  50/1251 (  4%)]  Loss: 3.356 (3.20)  Time: 0.772s, 1327.04/s  (0.819s, 1250.67/s)  LR: 1.680e-04  Data: 0.010 (0.044)
Train: 443 [ 100/1251 (  8%)]  Loss: 3.311 (3.24)  Time: 0.787s, 1300.73/s  (0.800s, 1279.44/s)  LR: 1.680e-04  Data: 0.010 (0.027)
Train: 443 [ 150/1251 ( 12%)]  Loss: 3.097 (3.20)  Time: 0.774s, 1323.50/s  (0.793s, 1292.01/s)  LR: 1.680e-04  Data: 0.010 (0.021)
Train: 443 [ 200/1251 ( 16%)]  Loss: 2.966 (3.16)  Time: 0.790s, 1296.32/s  (0.790s, 1296.70/s)  LR: 1.680e-04  Data: 0.012 (0.019)
Train: 443 [ 250/1251 ( 20%)]  Loss: 3.453 (3.21)  Time: 0.785s, 1304.82/s  (0.787s, 1301.10/s)  LR: 1.680e-04  Data: 0.010 (0.017)
Train: 443 [ 300/1251 ( 24%)]  Loss: 3.086 (3.19)  Time: 0.772s, 1326.69/s  (0.786s, 1303.61/s)  LR: 1.680e-04  Data: 0.010 (0.016)
Train: 443 [ 350/1251 ( 28%)]  Loss: 3.345 (3.21)  Time: 0.773s, 1324.82/s  (0.786s, 1302.96/s)  LR: 1.680e-04  Data: 0.009 (0.015)
Train: 443 [ 400/1251 ( 32%)]  Loss: 3.429 (3.23)  Time: 0.771s, 1327.62/s  (0.786s, 1303.56/s)  LR: 1.680e-04  Data: 0.010 (0.014)
Train: 443 [ 450/1251 ( 36%)]  Loss: 3.187 (3.23)  Time: 0.771s, 1328.97/s  (0.785s, 1305.02/s)  LR: 1.680e-04  Data: 0.009 (0.014)
Train: 443 [ 500/1251 ( 40%)]  Loss: 3.573 (3.26)  Time: 0.785s, 1304.53/s  (0.784s, 1305.74/s)  LR: 1.680e-04  Data: 0.010 (0.013)
Train: 443 [ 550/1251 ( 44%)]  Loss: 3.455 (3.28)  Time: 0.772s, 1326.16/s  (0.786s, 1302.30/s)  LR: 1.680e-04  Data: 0.011 (0.013)
Train: 443 [ 600/1251 ( 48%)]  Loss: 3.100 (3.26)  Time: 0.778s, 1316.78/s  (0.786s, 1303.33/s)  LR: 1.680e-04  Data: 0.010 (0.013)
Train: 443 [ 650/1251 ( 52%)]  Loss: 3.451 (3.28)  Time: 0.772s, 1325.75/s  (0.785s, 1304.46/s)  LR: 1.680e-04  Data: 0.009 (0.013)
Train: 443 [ 700/1251 ( 56%)]  Loss: 2.927 (3.25)  Time: 0.782s, 1309.46/s  (0.784s, 1305.50/s)  LR: 1.680e-04  Data: 0.012 (0.012)
Train: 443 [ 750/1251 ( 60%)]  Loss: 3.256 (3.25)  Time: 0.772s, 1327.04/s  (0.785s, 1304.64/s)  LR: 1.680e-04  Data: 0.010 (0.012)
Train: 443 [ 800/1251 ( 64%)]  Loss: 3.080 (3.24)  Time: 0.807s, 1268.72/s  (0.785s, 1305.22/s)  LR: 1.680e-04  Data: 0.010 (0.012)
Train: 443 [ 850/1251 ( 68%)]  Loss: 3.384 (3.25)  Time: 0.774s, 1323.06/s  (0.785s, 1304.23/s)  LR: 1.680e-04  Data: 0.010 (0.012)
Train: 443 [ 900/1251 ( 72%)]  Loss: 3.207 (3.25)  Time: 0.772s, 1326.06/s  (0.785s, 1304.73/s)  LR: 1.680e-04  Data: 0.009 (0.012)
Train: 443 [ 950/1251 ( 76%)]  Loss: 3.239 (3.25)  Time: 0.778s, 1316.65/s  (0.785s, 1305.24/s)  LR: 1.680e-04  Data: 0.009 (0.012)
Train: 443 [1000/1251 ( 80%)]  Loss: 3.264 (3.25)  Time: 0.773s, 1324.97/s  (0.785s, 1305.08/s)  LR: 1.680e-04  Data: 0.010 (0.012)
Train: 443 [1050/1251 ( 84%)]  Loss: 3.224 (3.25)  Time: 0.772s, 1326.35/s  (0.785s, 1305.21/s)  LR: 1.680e-04  Data: 0.010 (0.012)
Train: 443 [1100/1251 ( 88%)]  Loss: 3.161 (3.24)  Time: 0.773s, 1325.24/s  (0.784s, 1305.38/s)  LR: 1.680e-04  Data: 0.010 (0.012)
Train: 443 [1150/1251 ( 92%)]  Loss: 3.289 (3.25)  Time: 0.775s, 1320.72/s  (0.784s, 1305.91/s)  LR: 1.680e-04  Data: 0.009 (0.011)
Train: 443 [1200/1251 ( 96%)]  Loss: 3.277 (3.25)  Time: 0.773s, 1325.47/s  (0.784s, 1306.09/s)  LR: 1.680e-04  Data: 0.009 (0.011)
Train: 443 [1250/1251 (100%)]  Loss: 3.188 (3.24)  Time: 0.758s, 1351.35/s  (0.784s, 1306.25/s)  LR: 1.680e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.557 (1.557)  Loss:  0.6255 (0.6255)  Acc@1: 91.4062 (91.4062)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.194 (0.561)  Loss:  0.7378 (1.1214)  Acc@1: 86.5566 (78.5200)  Acc@5: 97.8774 (94.4940)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-439.pth.tar', 78.80400002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-441.pth.tar', 78.77600000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-440.pth.tar', 78.72999989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-430.pth.tar', 78.68800002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-431.pth.tar', 78.6360000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-442.pth.tar', 78.6320000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-429.pth.tar', 78.56800008300782)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-434.pth.tar', 78.55600000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-443.pth.tar', 78.51999997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-438.pth.tar', 78.48399987304687)

Train: 444 [   0/1251 (  0%)]  Loss: 2.990 (2.99)  Time: 2.271s,  450.86/s  (2.271s,  450.86/s)  LR: 1.661e-04  Data: 1.537 (1.537)
Train: 444 [  50/1251 (  4%)]  Loss: 3.102 (3.05)  Time: 0.777s, 1317.83/s  (0.815s, 1256.55/s)  LR: 1.661e-04  Data: 0.010 (0.044)
Train: 444 [ 100/1251 (  8%)]  Loss: 2.927 (3.01)  Time: 0.840s, 1219.70/s  (0.806s, 1270.48/s)  LR: 1.661e-04  Data: 0.015 (0.028)
Train: 444 [ 150/1251 ( 12%)]  Loss: 2.917 (2.98)  Time: 0.784s, 1306.51/s  (0.810s, 1263.89/s)  LR: 1.661e-04  Data: 0.013 (0.022)
Train: 444 [ 200/1251 ( 16%)]  Loss: 3.529 (3.09)  Time: 0.773s, 1324.80/s  (0.803s, 1275.54/s)  LR: 1.661e-04  Data: 0.010 (0.019)
Train: 444 [ 250/1251 ( 20%)]  Loss: 3.283 (3.12)  Time: 0.775s, 1321.48/s  (0.799s, 1281.94/s)  LR: 1.661e-04  Data: 0.010 (0.018)
Train: 444 [ 300/1251 ( 24%)]  Loss: 3.287 (3.15)  Time: 0.774s, 1323.18/s  (0.796s, 1285.73/s)  LR: 1.661e-04  Data: 0.010 (0.016)
Train: 444 [ 350/1251 ( 28%)]  Loss: 3.101 (3.14)  Time: 0.817s, 1253.89/s  (0.795s, 1288.56/s)  LR: 1.661e-04  Data: 0.011 (0.016)
Train: 444 [ 400/1251 ( 32%)]  Loss: 2.687 (3.09)  Time: 0.773s, 1325.15/s  (0.794s, 1289.46/s)  LR: 1.661e-04  Data: 0.010 (0.015)
Train: 444 [ 450/1251 ( 36%)]  Loss: 3.485 (3.13)  Time: 0.774s, 1323.49/s  (0.792s, 1292.50/s)  LR: 1.661e-04  Data: 0.010 (0.014)
Train: 444 [ 500/1251 ( 40%)]  Loss: 3.385 (3.15)  Time: 0.786s, 1302.97/s  (0.792s, 1293.08/s)  LR: 1.661e-04  Data: 0.013 (0.014)
Train: 444 [ 550/1251 ( 44%)]  Loss: 3.364 (3.17)  Time: 0.778s, 1315.60/s  (0.793s, 1291.48/s)  LR: 1.661e-04  Data: 0.010 (0.014)
Train: 444 [ 600/1251 ( 48%)]  Loss: 3.436 (3.19)  Time: 0.774s, 1323.56/s  (0.793s, 1291.76/s)  LR: 1.661e-04  Data: 0.009 (0.013)
Train: 444 [ 650/1251 ( 52%)]  Loss: 3.114 (3.19)  Time: 0.780s, 1312.34/s  (0.792s, 1292.99/s)  LR: 1.661e-04  Data: 0.009 (0.013)
Train: 444 [ 700/1251 ( 56%)]  Loss: 3.294 (3.19)  Time: 0.810s, 1263.62/s  (0.792s, 1292.91/s)  LR: 1.661e-04  Data: 0.010 (0.013)
Train: 444 [ 750/1251 ( 60%)]  Loss: 3.015 (3.18)  Time: 0.774s, 1322.58/s  (0.792s, 1293.53/s)  LR: 1.661e-04  Data: 0.010 (0.013)
Train: 444 [ 800/1251 ( 64%)]  Loss: 3.202 (3.18)  Time: 0.776s, 1319.26/s  (0.791s, 1294.57/s)  LR: 1.661e-04  Data: 0.010 (0.012)
Train: 444 [ 850/1251 ( 68%)]  Loss: 3.158 (3.18)  Time: 0.774s, 1323.64/s  (0.790s, 1295.42/s)  LR: 1.661e-04  Data: 0.009 (0.012)
Train: 444 [ 900/1251 ( 72%)]  Loss: 3.470 (3.20)  Time: 0.773s, 1325.14/s  (0.790s, 1295.58/s)  LR: 1.661e-04  Data: 0.009 (0.012)
Train: 444 [ 950/1251 ( 76%)]  Loss: 3.213 (3.20)  Time: 0.780s, 1312.76/s  (0.790s, 1296.42/s)  LR: 1.661e-04  Data: 0.009 (0.012)
Train: 444 [1000/1251 ( 80%)]  Loss: 3.216 (3.20)  Time: 0.772s, 1325.93/s  (0.789s, 1297.08/s)  LR: 1.661e-04  Data: 0.009 (0.012)
Train: 444 [1050/1251 ( 84%)]  Loss: 3.221 (3.20)  Time: 0.775s, 1321.40/s  (0.789s, 1298.07/s)  LR: 1.661e-04  Data: 0.010 (0.012)
Train: 444 [1100/1251 ( 88%)]  Loss: 3.071 (3.19)  Time: 0.780s, 1312.98/s  (0.789s, 1297.60/s)  LR: 1.661e-04  Data: 0.010 (0.012)
Train: 444 [1150/1251 ( 92%)]  Loss: 2.995 (3.19)  Time: 0.772s, 1326.00/s  (0.789s, 1298.22/s)  LR: 1.661e-04  Data: 0.009 (0.012)
Train: 444 [1200/1251 ( 96%)]  Loss: 3.217 (3.19)  Time: 0.774s, 1322.32/s  (0.788s, 1298.95/s)  LR: 1.661e-04  Data: 0.009 (0.012)
Train: 444 [1250/1251 (100%)]  Loss: 3.213 (3.19)  Time: 0.759s, 1348.59/s  (0.788s, 1299.59/s)  LR: 1.661e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.570 (1.570)  Loss:  0.7383 (0.7383)  Acc@1: 91.0156 (91.0156)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.194 (0.563)  Loss:  0.8809 (1.2355)  Acc@1: 86.7924 (78.6100)  Acc@5: 97.4057 (94.5480)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-439.pth.tar', 78.80400002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-441.pth.tar', 78.77600000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-440.pth.tar', 78.72999989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-430.pth.tar', 78.68800002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-431.pth.tar', 78.6360000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-442.pth.tar', 78.6320000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-444.pth.tar', 78.60999995117187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-429.pth.tar', 78.56800008300782)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-434.pth.tar', 78.55600000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-443.pth.tar', 78.51999997802734)

Train: 445 [   0/1251 (  0%)]  Loss: 3.778 (3.78)  Time: 2.259s,  453.23/s  (2.259s,  453.23/s)  LR: 1.643e-04  Data: 1.530 (1.530)
Train: 445 [  50/1251 (  4%)]  Loss: 2.945 (3.36)  Time: 0.773s, 1325.50/s  (0.820s, 1248.55/s)  LR: 1.643e-04  Data: 0.009 (0.045)
Train: 445 [ 100/1251 (  8%)]  Loss: 3.211 (3.31)  Time: 0.782s, 1308.67/s  (0.802s, 1277.45/s)  LR: 1.643e-04  Data: 0.010 (0.027)
Train: 445 [ 150/1251 ( 12%)]  Loss: 3.036 (3.24)  Time: 0.773s, 1325.18/s  (0.801s, 1279.01/s)  LR: 1.643e-04  Data: 0.010 (0.022)
Train: 445 [ 200/1251 ( 16%)]  Loss: 3.043 (3.20)  Time: 0.815s, 1256.54/s  (0.797s, 1284.61/s)  LR: 1.643e-04  Data: 0.011 (0.019)
Train: 445 [ 250/1251 ( 20%)]  Loss: 3.196 (3.20)  Time: 0.787s, 1301.44/s  (0.795s, 1287.97/s)  LR: 1.643e-04  Data: 0.010 (0.017)
Train: 445 [ 300/1251 ( 24%)]  Loss: 3.137 (3.19)  Time: 0.772s, 1326.76/s  (0.792s, 1292.61/s)  LR: 1.643e-04  Data: 0.010 (0.016)
Train: 445 [ 350/1251 ( 28%)]  Loss: 3.181 (3.19)  Time: 0.773s, 1324.92/s  (0.790s, 1296.56/s)  LR: 1.643e-04  Data: 0.009 (0.015)
Train: 445 [ 400/1251 ( 32%)]  Loss: 3.220 (3.19)  Time: 0.774s, 1323.44/s  (0.789s, 1297.96/s)  LR: 1.643e-04  Data: 0.009 (0.014)
Train: 445 [ 450/1251 ( 36%)]  Loss: 3.288 (3.20)  Time: 0.773s, 1324.33/s  (0.788s, 1299.94/s)  LR: 1.643e-04  Data: 0.010 (0.014)
Train: 445 [ 500/1251 ( 40%)]  Loss: 3.281 (3.21)  Time: 0.775s, 1322.00/s  (0.787s, 1301.37/s)  LR: 1.643e-04  Data: 0.009 (0.013)
Train: 445 [ 550/1251 ( 44%)]  Loss: 3.141 (3.20)  Time: 0.775s, 1321.88/s  (0.786s, 1301.99/s)  LR: 1.643e-04  Data: 0.010 (0.013)
Train: 445 [ 600/1251 ( 48%)]  Loss: 3.345 (3.22)  Time: 0.778s, 1315.62/s  (0.786s, 1302.89/s)  LR: 1.643e-04  Data: 0.010 (0.013)
Train: 445 [ 650/1251 ( 52%)]  Loss: 3.080 (3.21)  Time: 0.779s, 1313.99/s  (0.786s, 1302.39/s)  LR: 1.643e-04  Data: 0.013 (0.013)
Train: 445 [ 700/1251 ( 56%)]  Loss: 3.243 (3.21)  Time: 0.775s, 1322.05/s  (0.786s, 1303.54/s)  LR: 1.643e-04  Data: 0.010 (0.012)
Train: 445 [ 750/1251 ( 60%)]  Loss: 3.345 (3.22)  Time: 0.777s, 1317.94/s  (0.786s, 1303.12/s)  LR: 1.643e-04  Data: 0.010 (0.012)
Train: 445 [ 800/1251 ( 64%)]  Loss: 2.893 (3.20)  Time: 0.772s, 1327.24/s  (0.785s, 1303.63/s)  LR: 1.643e-04  Data: 0.010 (0.012)
Train: 445 [ 850/1251 ( 68%)]  Loss: 3.145 (3.19)  Time: 0.780s, 1313.19/s  (0.785s, 1304.34/s)  LR: 1.643e-04  Data: 0.009 (0.012)
Train: 445 [ 900/1251 ( 72%)]  Loss: 3.342 (3.20)  Time: 0.775s, 1321.54/s  (0.785s, 1304.54/s)  LR: 1.643e-04  Data: 0.009 (0.012)
Train: 445 [ 950/1251 ( 76%)]  Loss: 3.146 (3.20)  Time: 0.776s, 1319.57/s  (0.785s, 1305.20/s)  LR: 1.643e-04  Data: 0.009 (0.012)
Train: 445 [1000/1251 ( 80%)]  Loss: 3.090 (3.19)  Time: 0.773s, 1325.03/s  (0.784s, 1305.80/s)  LR: 1.643e-04  Data: 0.009 (0.012)
Train: 445 [1050/1251 ( 84%)]  Loss: 3.212 (3.20)  Time: 0.789s, 1298.28/s  (0.784s, 1306.09/s)  LR: 1.643e-04  Data: 0.013 (0.012)
Train: 445 [1100/1251 ( 88%)]  Loss: 3.593 (3.21)  Time: 0.780s, 1312.37/s  (0.784s, 1306.31/s)  LR: 1.643e-04  Data: 0.009 (0.011)
Train: 445 [1150/1251 ( 92%)]  Loss: 3.121 (3.21)  Time: 0.775s, 1320.59/s  (0.784s, 1306.33/s)  LR: 1.643e-04  Data: 0.010 (0.011)
Train: 445 [1200/1251 ( 96%)]  Loss: 3.168 (3.21)  Time: 0.775s, 1321.06/s  (0.784s, 1306.74/s)  LR: 1.643e-04  Data: 0.009 (0.011)
Train: 445 [1250/1251 (100%)]  Loss: 3.228 (3.21)  Time: 0.759s, 1348.29/s  (0.783s, 1307.10/s)  LR: 1.643e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.524 (1.524)  Loss:  0.6836 (0.6836)  Acc@1: 91.5039 (91.5039)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.194 (0.561)  Loss:  0.7256 (1.1376)  Acc@1: 86.6745 (78.5420)  Acc@5: 97.2877 (94.5240)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-439.pth.tar', 78.80400002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-441.pth.tar', 78.77600000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-440.pth.tar', 78.72999989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-430.pth.tar', 78.68800002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-431.pth.tar', 78.6360000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-442.pth.tar', 78.6320000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-444.pth.tar', 78.60999995117187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-429.pth.tar', 78.56800008300782)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-434.pth.tar', 78.55600000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-445.pth.tar', 78.54200002929687)

Train: 446 [   0/1251 (  0%)]  Loss: 3.126 (3.13)  Time: 2.369s,  432.31/s  (2.369s,  432.31/s)  LR: 1.624e-04  Data: 1.636 (1.636)
Train: 446 [  50/1251 (  4%)]  Loss: 3.037 (3.08)  Time: 0.814s, 1257.29/s  (0.837s, 1222.79/s)  LR: 1.624e-04  Data: 0.011 (0.047)
Train: 446 [ 100/1251 (  8%)]  Loss: 2.918 (3.03)  Time: 0.781s, 1311.21/s  (0.820s, 1248.42/s)  LR: 1.624e-04  Data: 0.010 (0.029)
Train: 446 [ 150/1251 ( 12%)]  Loss: 3.244 (3.08)  Time: 0.775s, 1321.94/s  (0.816s, 1254.95/s)  LR: 1.624e-04  Data: 0.009 (0.023)
Train: 446 [ 200/1251 ( 16%)]  Loss: 2.562 (2.98)  Time: 0.778s, 1316.86/s  (0.808s, 1267.94/s)  LR: 1.624e-04  Data: 0.009 (0.020)
Train: 446 [ 250/1251 ( 20%)]  Loss: 3.273 (3.03)  Time: 0.776s, 1319.85/s  (0.802s, 1276.10/s)  LR: 1.624e-04  Data: 0.009 (0.018)
Train: 446 [ 300/1251 ( 24%)]  Loss: 3.525 (3.10)  Time: 0.783s, 1306.98/s  (0.799s, 1281.48/s)  LR: 1.624e-04  Data: 0.010 (0.017)
Train: 446 [ 350/1251 ( 28%)]  Loss: 3.446 (3.14)  Time: 0.783s, 1307.71/s  (0.796s, 1285.67/s)  LR: 1.624e-04  Data: 0.010 (0.016)
Train: 446 [ 400/1251 ( 32%)]  Loss: 2.910 (3.12)  Time: 0.778s, 1316.47/s  (0.796s, 1286.39/s)  LR: 1.624e-04  Data: 0.010 (0.015)
Train: 446 [ 450/1251 ( 36%)]  Loss: 3.159 (3.12)  Time: 0.773s, 1324.78/s  (0.794s, 1289.22/s)  LR: 1.624e-04  Data: 0.009 (0.014)
Train: 446 [ 500/1251 ( 40%)]  Loss: 3.382 (3.14)  Time: 0.817s, 1253.67/s  (0.793s, 1291.66/s)  LR: 1.624e-04  Data: 0.010 (0.014)
Train: 446 [ 550/1251 ( 44%)]  Loss: 3.016 (3.13)  Time: 0.776s, 1319.81/s  (0.792s, 1292.88/s)  LR: 1.624e-04  Data: 0.010 (0.014)
Train: 446 [ 600/1251 ( 48%)]  Loss: 3.065 (3.13)  Time: 0.805s, 1271.33/s  (0.791s, 1293.85/s)  LR: 1.624e-04  Data: 0.010 (0.013)
Train: 446 [ 650/1251 ( 52%)]  Loss: 3.159 (3.13)  Time: 0.777s, 1318.59/s  (0.790s, 1295.59/s)  LR: 1.624e-04  Data: 0.009 (0.013)
Train: 446 [ 700/1251 ( 56%)]  Loss: 3.099 (3.13)  Time: 0.817s, 1253.87/s  (0.790s, 1296.47/s)  LR: 1.624e-04  Data: 0.009 (0.013)
Train: 446 [ 750/1251 ( 60%)]  Loss: 3.159 (3.13)  Time: 0.783s, 1308.35/s  (0.790s, 1296.48/s)  LR: 1.624e-04  Data: 0.010 (0.013)
Train: 446 [ 800/1251 ( 64%)]  Loss: 3.356 (3.14)  Time: 0.780s, 1313.41/s  (0.789s, 1297.78/s)  LR: 1.624e-04  Data: 0.009 (0.012)
Train: 446 [ 850/1251 ( 68%)]  Loss: 3.405 (3.16)  Time: 0.773s, 1323.88/s  (0.788s, 1298.78/s)  LR: 1.624e-04  Data: 0.010 (0.012)
Train: 446 [ 900/1251 ( 72%)]  Loss: 2.940 (3.15)  Time: 0.775s, 1321.13/s  (0.788s, 1299.55/s)  LR: 1.624e-04  Data: 0.009 (0.012)
Train: 446 [ 950/1251 ( 76%)]  Loss: 3.364 (3.16)  Time: 0.772s, 1326.32/s  (0.788s, 1300.12/s)  LR: 1.624e-04  Data: 0.011 (0.012)
Train: 446 [1000/1251 ( 80%)]  Loss: 3.100 (3.15)  Time: 0.773s, 1324.60/s  (0.787s, 1300.90/s)  LR: 1.624e-04  Data: 0.009 (0.012)
Train: 446 [1050/1251 ( 84%)]  Loss: 3.332 (3.16)  Time: 0.777s, 1317.58/s  (0.787s, 1301.73/s)  LR: 1.624e-04  Data: 0.010 (0.012)
Train: 446 [1100/1251 ( 88%)]  Loss: 3.218 (3.17)  Time: 0.774s, 1323.63/s  (0.786s, 1302.10/s)  LR: 1.624e-04  Data: 0.010 (0.012)
Train: 446 [1150/1251 ( 92%)]  Loss: 3.090 (3.16)  Time: 0.815s, 1256.40/s  (0.787s, 1300.51/s)  LR: 1.624e-04  Data: 0.011 (0.012)
Train: 446 [1200/1251 ( 96%)]  Loss: 3.225 (3.16)  Time: 0.774s, 1323.04/s  (0.787s, 1300.44/s)  LR: 1.624e-04  Data: 0.009 (0.012)
Train: 446 [1250/1251 (100%)]  Loss: 3.350 (3.17)  Time: 0.760s, 1348.11/s  (0.787s, 1300.92/s)  LR: 1.624e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.525 (1.525)  Loss:  0.6714 (0.6714)  Acc@1: 91.8945 (91.8945)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.194 (0.565)  Loss:  0.8218 (1.1520)  Acc@1: 86.4387 (78.7020)  Acc@5: 97.0519 (94.5360)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-439.pth.tar', 78.80400002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-441.pth.tar', 78.77600000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-440.pth.tar', 78.72999989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-446.pth.tar', 78.70200005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-430.pth.tar', 78.68800002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-431.pth.tar', 78.6360000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-442.pth.tar', 78.6320000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-444.pth.tar', 78.60999995117187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-429.pth.tar', 78.56800008300782)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-434.pth.tar', 78.55600000488282)

Train: 447 [   0/1251 (  0%)]  Loss: 3.157 (3.16)  Time: 2.411s,  424.78/s  (2.411s,  424.78/s)  LR: 1.605e-04  Data: 1.555 (1.555)
Train: 447 [  50/1251 (  4%)]  Loss: 3.401 (3.28)  Time: 0.773s, 1323.87/s  (0.815s, 1255.80/s)  LR: 1.605e-04  Data: 0.009 (0.042)
Train: 447 [ 100/1251 (  8%)]  Loss: 3.457 (3.34)  Time: 0.781s, 1311.62/s  (0.798s, 1283.18/s)  LR: 1.605e-04  Data: 0.011 (0.026)
Train: 447 [ 150/1251 ( 12%)]  Loss: 3.336 (3.34)  Time: 0.867s, 1181.59/s  (0.793s, 1292.05/s)  LR: 1.605e-04  Data: 0.010 (0.021)
Train: 447 [ 200/1251 ( 16%)]  Loss: 3.143 (3.30)  Time: 0.774s, 1323.62/s  (0.790s, 1296.83/s)  LR: 1.605e-04  Data: 0.009 (0.018)
Train: 447 [ 250/1251 ( 20%)]  Loss: 3.252 (3.29)  Time: 0.803s, 1274.68/s  (0.792s, 1292.71/s)  LR: 1.605e-04  Data: 0.010 (0.016)
Train: 447 [ 300/1251 ( 24%)]  Loss: 3.129 (3.27)  Time: 0.810s, 1263.96/s  (0.792s, 1292.48/s)  LR: 1.605e-04  Data: 0.009 (0.015)
Train: 447 [ 350/1251 ( 28%)]  Loss: 2.979 (3.23)  Time: 0.815s, 1257.09/s  (0.796s, 1286.76/s)  LR: 1.605e-04  Data: 0.011 (0.014)
Train: 447 [ 400/1251 ( 32%)]  Loss: 3.000 (3.21)  Time: 0.774s, 1322.65/s  (0.795s, 1287.85/s)  LR: 1.605e-04  Data: 0.009 (0.014)
Train: 447 [ 450/1251 ( 36%)]  Loss: 3.127 (3.20)  Time: 0.784s, 1306.26/s  (0.794s, 1290.21/s)  LR: 1.605e-04  Data: 0.009 (0.013)
Train: 447 [ 500/1251 ( 40%)]  Loss: 3.032 (3.18)  Time: 0.792s, 1292.14/s  (0.793s, 1290.70/s)  LR: 1.605e-04  Data: 0.009 (0.013)
Train: 447 [ 550/1251 ( 44%)]  Loss: 3.409 (3.20)  Time: 0.780s, 1313.37/s  (0.793s, 1290.80/s)  LR: 1.605e-04  Data: 0.009 (0.013)
Train: 447 [ 600/1251 ( 48%)]  Loss: 2.914 (3.18)  Time: 0.772s, 1326.23/s  (0.793s, 1291.81/s)  LR: 1.605e-04  Data: 0.010 (0.013)
Train: 447 [ 650/1251 ( 52%)]  Loss: 3.571 (3.21)  Time: 0.773s, 1324.87/s  (0.792s, 1292.23/s)  LR: 1.605e-04  Data: 0.010 (0.012)
Train: 447 [ 700/1251 ( 56%)]  Loss: 2.828 (3.18)  Time: 0.771s, 1328.30/s  (0.792s, 1292.46/s)  LR: 1.605e-04  Data: 0.009 (0.012)
Train: 447 [ 750/1251 ( 60%)]  Loss: 3.210 (3.18)  Time: 0.773s, 1324.36/s  (0.792s, 1293.57/s)  LR: 1.605e-04  Data: 0.009 (0.012)
Train: 447 [ 800/1251 ( 64%)]  Loss: 2.772 (3.16)  Time: 0.774s, 1323.80/s  (0.791s, 1294.90/s)  LR: 1.605e-04  Data: 0.009 (0.012)
Train: 447 [ 850/1251 ( 68%)]  Loss: 3.405 (3.17)  Time: 0.772s, 1325.82/s  (0.790s, 1295.78/s)  LR: 1.605e-04  Data: 0.009 (0.012)
Train: 447 [ 900/1251 ( 72%)]  Loss: 3.174 (3.17)  Time: 0.772s, 1325.73/s  (0.790s, 1296.24/s)  LR: 1.605e-04  Data: 0.009 (0.012)
Train: 447 [ 950/1251 ( 76%)]  Loss: 3.043 (3.17)  Time: 0.772s, 1327.16/s  (0.789s, 1297.43/s)  LR: 1.605e-04  Data: 0.010 (0.011)
Train: 447 [1000/1251 ( 80%)]  Loss: 3.131 (3.17)  Time: 0.771s, 1327.30/s  (0.789s, 1298.46/s)  LR: 1.605e-04  Data: 0.009 (0.011)
Train: 447 [1050/1251 ( 84%)]  Loss: 3.408 (3.18)  Time: 0.851s, 1203.20/s  (0.788s, 1298.95/s)  LR: 1.605e-04  Data: 0.009 (0.011)
Train: 447 [1100/1251 ( 88%)]  Loss: 3.408 (3.19)  Time: 0.773s, 1324.68/s  (0.788s, 1299.49/s)  LR: 1.605e-04  Data: 0.010 (0.011)
Train: 447 [1150/1251 ( 92%)]  Loss: 3.120 (3.18)  Time: 0.771s, 1327.95/s  (0.788s, 1300.19/s)  LR: 1.605e-04  Data: 0.009 (0.011)
Train: 447 [1200/1251 ( 96%)]  Loss: 3.026 (3.18)  Time: 0.789s, 1297.22/s  (0.788s, 1300.05/s)  LR: 1.605e-04  Data: 0.011 (0.011)
Train: 447 [1250/1251 (100%)]  Loss: 3.067 (3.17)  Time: 0.764s, 1339.45/s  (0.788s, 1300.30/s)  LR: 1.605e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.521 (1.521)  Loss:  0.6919 (0.6919)  Acc@1: 91.0156 (91.0156)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.194 (0.563)  Loss:  0.8022 (1.1730)  Acc@1: 87.1462 (78.6620)  Acc@5: 97.8774 (94.4640)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-439.pth.tar', 78.80400002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-441.pth.tar', 78.77600000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-440.pth.tar', 78.72999989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-446.pth.tar', 78.70200005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-430.pth.tar', 78.68800002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-447.pth.tar', 78.66199997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-431.pth.tar', 78.6360000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-442.pth.tar', 78.6320000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-444.pth.tar', 78.60999995117187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-429.pth.tar', 78.56800008300782)

Train: 448 [   0/1251 (  0%)]  Loss: 3.257 (3.26)  Time: 2.300s,  445.17/s  (2.300s,  445.17/s)  LR: 1.587e-04  Data: 1.567 (1.567)
Train: 448 [  50/1251 (  4%)]  Loss: 3.476 (3.37)  Time: 0.818s, 1251.57/s  (0.846s, 1210.83/s)  LR: 1.587e-04  Data: 0.011 (0.045)
Train: 448 [ 100/1251 (  8%)]  Loss: 3.046 (3.26)  Time: 0.776s, 1319.37/s  (0.830s, 1233.14/s)  LR: 1.587e-04  Data: 0.009 (0.028)
Train: 448 [ 150/1251 ( 12%)]  Loss: 3.445 (3.31)  Time: 0.773s, 1325.43/s  (0.816s, 1255.02/s)  LR: 1.587e-04  Data: 0.010 (0.022)
Train: 448 [ 200/1251 ( 16%)]  Loss: 3.078 (3.26)  Time: 0.776s, 1318.74/s  (0.807s, 1269.41/s)  LR: 1.587e-04  Data: 0.009 (0.019)
Train: 448 [ 250/1251 ( 20%)]  Loss: 3.060 (3.23)  Time: 0.776s, 1318.94/s  (0.803s, 1275.96/s)  LR: 1.587e-04  Data: 0.011 (0.017)
Train: 448 [ 300/1251 ( 24%)]  Loss: 3.455 (3.26)  Time: 0.797s, 1284.82/s  (0.799s, 1280.82/s)  LR: 1.587e-04  Data: 0.010 (0.016)
Train: 448 [ 350/1251 ( 28%)]  Loss: 3.169 (3.25)  Time: 0.789s, 1297.24/s  (0.797s, 1284.32/s)  LR: 1.587e-04  Data: 0.009 (0.015)
Train: 448 [ 400/1251 ( 32%)]  Loss: 3.193 (3.24)  Time: 0.773s, 1323.97/s  (0.796s, 1286.30/s)  LR: 1.587e-04  Data: 0.010 (0.014)
Train: 448 [ 450/1251 ( 36%)]  Loss: 3.108 (3.23)  Time: 0.780s, 1313.63/s  (0.794s, 1289.74/s)  LR: 1.587e-04  Data: 0.009 (0.014)
Train: 448 [ 500/1251 ( 40%)]  Loss: 3.108 (3.22)  Time: 0.775s, 1321.64/s  (0.792s, 1292.18/s)  LR: 1.587e-04  Data: 0.010 (0.014)
Train: 448 [ 550/1251 ( 44%)]  Loss: 3.159 (3.21)  Time: 0.815s, 1256.59/s  (0.792s, 1293.07/s)  LR: 1.587e-04  Data: 0.009 (0.013)
Train: 448 [ 600/1251 ( 48%)]  Loss: 2.830 (3.18)  Time: 0.876s, 1168.67/s  (0.792s, 1293.66/s)  LR: 1.587e-04  Data: 0.010 (0.013)
Train: 448 [ 650/1251 ( 52%)]  Loss: 3.413 (3.20)  Time: 0.833s, 1228.95/s  (0.791s, 1293.92/s)  LR: 1.587e-04  Data: 0.010 (0.013)
Train: 448 [ 700/1251 ( 56%)]  Loss: 3.304 (3.21)  Time: 0.774s, 1323.50/s  (0.791s, 1294.65/s)  LR: 1.587e-04  Data: 0.009 (0.012)
Train: 448 [ 750/1251 ( 60%)]  Loss: 3.271 (3.21)  Time: 0.815s, 1256.70/s  (0.791s, 1294.07/s)  LR: 1.587e-04  Data: 0.010 (0.012)
Train: 448 [ 800/1251 ( 64%)]  Loss: 2.829 (3.19)  Time: 0.783s, 1308.47/s  (0.790s, 1295.39/s)  LR: 1.587e-04  Data: 0.010 (0.012)
Train: 448 [ 850/1251 ( 68%)]  Loss: 3.311 (3.20)  Time: 0.774s, 1322.93/s  (0.791s, 1294.66/s)  LR: 1.587e-04  Data: 0.009 (0.012)
Train: 448 [ 900/1251 ( 72%)]  Loss: 3.549 (3.21)  Time: 0.772s, 1325.88/s  (0.791s, 1295.13/s)  LR: 1.587e-04  Data: 0.010 (0.012)
Train: 448 [ 950/1251 ( 76%)]  Loss: 3.241 (3.22)  Time: 0.769s, 1332.08/s  (0.790s, 1295.78/s)  LR: 1.587e-04  Data: 0.010 (0.012)
Train: 448 [1000/1251 ( 80%)]  Loss: 2.985 (3.20)  Time: 0.785s, 1303.91/s  (0.790s, 1296.81/s)  LR: 1.587e-04  Data: 0.010 (0.012)
Train: 448 [1050/1251 ( 84%)]  Loss: 3.048 (3.20)  Time: 0.787s, 1301.08/s  (0.789s, 1297.30/s)  LR: 1.587e-04  Data: 0.010 (0.012)
Train: 448 [1100/1251 ( 88%)]  Loss: 3.100 (3.19)  Time: 0.774s, 1322.35/s  (0.789s, 1297.63/s)  LR: 1.587e-04  Data: 0.010 (0.012)
Train: 448 [1150/1251 ( 92%)]  Loss: 3.222 (3.19)  Time: 0.813s, 1259.20/s  (0.789s, 1297.50/s)  LR: 1.587e-04  Data: 0.010 (0.011)
Train: 448 [1200/1251 ( 96%)]  Loss: 3.133 (3.19)  Time: 0.774s, 1322.73/s  (0.789s, 1297.43/s)  LR: 1.587e-04  Data: 0.010 (0.011)
Train: 448 [1250/1251 (100%)]  Loss: 3.129 (3.19)  Time: 0.758s, 1350.38/s  (0.790s, 1296.47/s)  LR: 1.587e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.521 (1.521)  Loss:  0.6870 (0.6870)  Acc@1: 91.7969 (91.7969)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.193 (0.566)  Loss:  0.8154 (1.1387)  Acc@1: 86.9104 (78.8160)  Acc@5: 97.1698 (94.5480)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-448.pth.tar', 78.81600000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-439.pth.tar', 78.80400002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-441.pth.tar', 78.77600000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-440.pth.tar', 78.72999989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-446.pth.tar', 78.70200005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-430.pth.tar', 78.68800002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-447.pth.tar', 78.66199997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-431.pth.tar', 78.6360000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-442.pth.tar', 78.6320000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-444.pth.tar', 78.60999995117187)

Train: 449 [   0/1251 (  0%)]  Loss: 3.190 (3.19)  Time: 2.387s,  429.02/s  (2.387s,  429.02/s)  LR: 1.568e-04  Data: 1.642 (1.642)
Train: 449 [  50/1251 (  4%)]  Loss: 3.198 (3.19)  Time: 0.775s, 1321.75/s  (0.835s, 1226.58/s)  LR: 1.568e-04  Data: 0.010 (0.047)
Train: 449 [ 100/1251 (  8%)]  Loss: 3.514 (3.30)  Time: 0.837s, 1223.92/s  (0.825s, 1241.26/s)  LR: 1.568e-04  Data: 0.011 (0.029)
Train: 449 [ 150/1251 ( 12%)]  Loss: 3.270 (3.29)  Time: 0.816s, 1254.21/s  (0.816s, 1255.05/s)  LR: 1.568e-04  Data: 0.013 (0.023)
Train: 449 [ 200/1251 ( 16%)]  Loss: 3.061 (3.25)  Time: 0.774s, 1322.63/s  (0.807s, 1268.63/s)  LR: 1.568e-04  Data: 0.010 (0.020)
Train: 449 [ 250/1251 ( 20%)]  Loss: 3.191 (3.24)  Time: 0.774s, 1322.16/s  (0.804s, 1273.81/s)  LR: 1.568e-04  Data: 0.010 (0.018)
Train: 449 [ 300/1251 ( 24%)]  Loss: 3.159 (3.23)  Time: 0.784s, 1306.01/s  (0.800s, 1279.81/s)  LR: 1.568e-04  Data: 0.014 (0.017)
Train: 449 [ 350/1251 ( 28%)]  Loss: 3.513 (3.26)  Time: 0.769s, 1330.84/s  (0.797s, 1284.95/s)  LR: 1.568e-04  Data: 0.009 (0.016)
Train: 449 [ 400/1251 ( 32%)]  Loss: 3.479 (3.29)  Time: 0.773s, 1324.21/s  (0.796s, 1286.18/s)  LR: 1.568e-04  Data: 0.010 (0.015)
Train: 449 [ 450/1251 ( 36%)]  Loss: 3.243 (3.28)  Time: 0.773s, 1324.60/s  (0.795s, 1288.64/s)  LR: 1.568e-04  Data: 0.009 (0.014)
Train: 449 [ 500/1251 ( 40%)]  Loss: 2.979 (3.25)  Time: 0.774s, 1323.80/s  (0.793s, 1290.85/s)  LR: 1.568e-04  Data: 0.010 (0.014)
Train: 449 [ 550/1251 ( 44%)]  Loss: 3.250 (3.25)  Time: 0.782s, 1309.59/s  (0.793s, 1291.79/s)  LR: 1.568e-04  Data: 0.009 (0.014)
Train: 449 [ 600/1251 ( 48%)]  Loss: 3.149 (3.25)  Time: 0.773s, 1325.50/s  (0.792s, 1293.63/s)  LR: 1.568e-04  Data: 0.010 (0.013)
Train: 449 [ 650/1251 ( 52%)]  Loss: 3.159 (3.24)  Time: 0.785s, 1305.08/s  (0.790s, 1295.40/s)  LR: 1.568e-04  Data: 0.010 (0.013)
Train: 449 [ 700/1251 ( 56%)]  Loss: 3.062 (3.23)  Time: 0.802s, 1277.21/s  (0.790s, 1295.53/s)  LR: 1.568e-04  Data: 0.010 (0.013)
Train: 449 [ 750/1251 ( 60%)]  Loss: 3.403 (3.24)  Time: 0.795s, 1287.80/s  (0.790s, 1295.57/s)  LR: 1.568e-04  Data: 0.013 (0.013)
Train: 449 [ 800/1251 ( 64%)]  Loss: 3.396 (3.25)  Time: 0.809s, 1265.23/s  (0.790s, 1296.37/s)  LR: 1.568e-04  Data: 0.009 (0.013)
Train: 449 [ 850/1251 ( 68%)]  Loss: 2.849 (3.23)  Time: 0.773s, 1324.71/s  (0.790s, 1296.43/s)  LR: 1.568e-04  Data: 0.010 (0.012)
Train: 449 [ 900/1251 ( 72%)]  Loss: 2.994 (3.21)  Time: 0.859s, 1192.07/s  (0.790s, 1296.53/s)  LR: 1.568e-04  Data: 0.010 (0.012)
Train: 449 [ 950/1251 ( 76%)]  Loss: 3.116 (3.21)  Time: 0.774s, 1322.99/s  (0.789s, 1297.48/s)  LR: 1.568e-04  Data: 0.010 (0.012)
Train: 449 [1000/1251 ( 80%)]  Loss: 3.305 (3.21)  Time: 0.774s, 1322.88/s  (0.790s, 1296.13/s)  LR: 1.568e-04  Data: 0.009 (0.012)
Train: 449 [1050/1251 ( 84%)]  Loss: 3.059 (3.21)  Time: 0.772s, 1325.64/s  (0.790s, 1296.90/s)  LR: 1.568e-04  Data: 0.010 (0.012)
Train: 449 [1100/1251 ( 88%)]  Loss: 3.037 (3.20)  Time: 0.771s, 1328.56/s  (0.789s, 1297.50/s)  LR: 1.568e-04  Data: 0.010 (0.012)
Train: 449 [1150/1251 ( 92%)]  Loss: 3.042 (3.19)  Time: 0.777s, 1317.18/s  (0.789s, 1298.25/s)  LR: 1.568e-04  Data: 0.009 (0.012)
Train: 449 [1200/1251 ( 96%)]  Loss: 3.310 (3.20)  Time: 0.782s, 1310.10/s  (0.788s, 1298.73/s)  LR: 1.568e-04  Data: 0.010 (0.012)
Train: 449 [1250/1251 (100%)]  Loss: 3.285 (3.20)  Time: 0.765s, 1338.03/s  (0.788s, 1299.03/s)  LR: 1.568e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.599 (1.599)  Loss:  0.7114 (0.7114)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.194 (0.564)  Loss:  0.8232 (1.1773)  Acc@1: 86.5566 (78.7980)  Acc@5: 96.6981 (94.6480)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-448.pth.tar', 78.81600000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-439.pth.tar', 78.80400002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-449.pth.tar', 78.79800010742187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-441.pth.tar', 78.77600000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-440.pth.tar', 78.72999989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-446.pth.tar', 78.70200005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-430.pth.tar', 78.68800002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-447.pth.tar', 78.66199997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-431.pth.tar', 78.6360000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-442.pth.tar', 78.6320000024414)

Train: 450 [   0/1251 (  0%)]  Loss: 3.552 (3.55)  Time: 2.256s,  453.85/s  (2.256s,  453.85/s)  LR: 1.550e-04  Data: 1.523 (1.523)
Train: 450 [  50/1251 (  4%)]  Loss: 3.071 (3.31)  Time: 0.771s, 1328.29/s  (0.816s, 1254.56/s)  LR: 1.550e-04  Data: 0.009 (0.045)
Train: 450 [ 100/1251 (  8%)]  Loss: 3.377 (3.33)  Time: 0.775s, 1321.09/s  (0.797s, 1284.52/s)  LR: 1.550e-04  Data: 0.009 (0.027)
Train: 450 [ 150/1251 ( 12%)]  Loss: 3.095 (3.27)  Time: 0.775s, 1320.88/s  (0.794s, 1289.09/s)  LR: 1.550e-04  Data: 0.009 (0.022)
Train: 450 [ 200/1251 ( 16%)]  Loss: 3.450 (3.31)  Time: 0.776s, 1319.58/s  (0.790s, 1295.78/s)  LR: 1.550e-04  Data: 0.010 (0.019)
Train: 450 [ 250/1251 ( 20%)]  Loss: 3.118 (3.28)  Time: 0.777s, 1317.45/s  (0.788s, 1298.96/s)  LR: 1.550e-04  Data: 0.010 (0.017)
Train: 450 [ 300/1251 ( 24%)]  Loss: 3.213 (3.27)  Time: 0.806s, 1269.75/s  (0.789s, 1298.19/s)  LR: 1.550e-04  Data: 0.009 (0.016)
Train: 450 [ 350/1251 ( 28%)]  Loss: 3.430 (3.29)  Time: 0.774s, 1322.41/s  (0.789s, 1297.40/s)  LR: 1.550e-04  Data: 0.010 (0.015)
Train: 450 [ 400/1251 ( 32%)]  Loss: 3.270 (3.29)  Time: 0.782s, 1310.08/s  (0.788s, 1298.96/s)  LR: 1.550e-04  Data: 0.012 (0.014)
Train: 450 [ 450/1251 ( 36%)]  Loss: 3.250 (3.28)  Time: 0.817s, 1253.67/s  (0.790s, 1295.44/s)  LR: 1.550e-04  Data: 0.013 (0.014)
Train: 450 [ 500/1251 ( 40%)]  Loss: 3.322 (3.29)  Time: 0.785s, 1305.15/s  (0.789s, 1297.40/s)  LR: 1.550e-04  Data: 0.010 (0.013)
Train: 450 [ 550/1251 ( 44%)]  Loss: 3.205 (3.28)  Time: 0.779s, 1315.32/s  (0.788s, 1298.96/s)  LR: 1.550e-04  Data: 0.009 (0.013)
Train: 450 [ 600/1251 ( 48%)]  Loss: 3.002 (3.26)  Time: 0.810s, 1264.81/s  (0.789s, 1297.88/s)  LR: 1.550e-04  Data: 0.009 (0.013)
Train: 450 [ 650/1251 ( 52%)]  Loss: 3.129 (3.25)  Time: 0.779s, 1313.80/s  (0.788s, 1299.12/s)  LR: 1.550e-04  Data: 0.009 (0.013)
Train: 450 [ 700/1251 ( 56%)]  Loss: 3.305 (3.25)  Time: 0.783s, 1308.28/s  (0.788s, 1299.55/s)  LR: 1.550e-04  Data: 0.009 (0.012)
Train: 450 [ 750/1251 ( 60%)]  Loss: 2.673 (3.22)  Time: 0.775s, 1321.74/s  (0.788s, 1299.85/s)  LR: 1.550e-04  Data: 0.010 (0.012)
Train: 450 [ 800/1251 ( 64%)]  Loss: 3.174 (3.21)  Time: 0.776s, 1319.58/s  (0.787s, 1300.70/s)  LR: 1.550e-04  Data: 0.011 (0.012)
Train: 450 [ 850/1251 ( 68%)]  Loss: 3.396 (3.22)  Time: 0.775s, 1321.63/s  (0.787s, 1301.71/s)  LR: 1.550e-04  Data: 0.010 (0.012)
Train: 450 [ 900/1251 ( 72%)]  Loss: 3.417 (3.23)  Time: 0.808s, 1267.92/s  (0.787s, 1301.46/s)  LR: 1.550e-04  Data: 0.010 (0.012)
Train: 450 [ 950/1251 ( 76%)]  Loss: 3.374 (3.24)  Time: 0.773s, 1325.30/s  (0.786s, 1302.00/s)  LR: 1.550e-04  Data: 0.009 (0.012)
Train: 450 [1000/1251 ( 80%)]  Loss: 3.034 (3.23)  Time: 0.773s, 1324.23/s  (0.786s, 1302.80/s)  LR: 1.550e-04  Data: 0.010 (0.012)
Train: 450 [1050/1251 ( 84%)]  Loss: 3.129 (3.23)  Time: 0.778s, 1316.65/s  (0.786s, 1303.11/s)  LR: 1.550e-04  Data: 0.010 (0.012)
Train: 450 [1100/1251 ( 88%)]  Loss: 3.185 (3.22)  Time: 0.773s, 1325.21/s  (0.787s, 1301.95/s)  LR: 1.550e-04  Data: 0.010 (0.011)
Train: 450 [1150/1251 ( 92%)]  Loss: 3.159 (3.22)  Time: 0.783s, 1307.11/s  (0.787s, 1301.59/s)  LR: 1.550e-04  Data: 0.010 (0.011)
Train: 450 [1200/1251 ( 96%)]  Loss: 2.884 (3.21)  Time: 0.773s, 1324.60/s  (0.786s, 1302.32/s)  LR: 1.550e-04  Data: 0.010 (0.011)
Train: 450 [1250/1251 (100%)]  Loss: 3.412 (3.22)  Time: 0.760s, 1347.82/s  (0.786s, 1302.55/s)  LR: 1.550e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.572 (1.572)  Loss:  0.7388 (0.7388)  Acc@1: 91.2109 (91.2109)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.194 (0.559)  Loss:  0.8022 (1.1917)  Acc@1: 87.1462 (78.8300)  Acc@5: 97.1698 (94.5740)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-450.pth.tar', 78.82999997558593)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-448.pth.tar', 78.81600000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-439.pth.tar', 78.80400002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-449.pth.tar', 78.79800010742187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-441.pth.tar', 78.77600000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-440.pth.tar', 78.72999989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-446.pth.tar', 78.70200005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-430.pth.tar', 78.68800002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-447.pth.tar', 78.66199997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-431.pth.tar', 78.6360000024414)

Train: 451 [   0/1251 (  0%)]  Loss: 3.376 (3.38)  Time: 2.196s,  466.38/s  (2.196s,  466.38/s)  LR: 1.532e-04  Data: 1.466 (1.466)
Train: 451 [  50/1251 (  4%)]  Loss: 3.105 (3.24)  Time: 0.772s, 1326.00/s  (0.817s, 1253.04/s)  LR: 1.532e-04  Data: 0.010 (0.042)
Train: 451 [ 100/1251 (  8%)]  Loss: 3.078 (3.19)  Time: 0.773s, 1324.68/s  (0.802s, 1277.22/s)  LR: 1.532e-04  Data: 0.010 (0.026)
Train: 451 [ 150/1251 ( 12%)]  Loss: 3.311 (3.22)  Time: 0.868s, 1180.22/s  (0.795s, 1287.54/s)  LR: 1.532e-04  Data: 0.009 (0.021)
Train: 451 [ 200/1251 ( 16%)]  Loss: 2.784 (3.13)  Time: 0.773s, 1325.39/s  (0.792s, 1292.32/s)  LR: 1.532e-04  Data: 0.010 (0.018)
Train: 451 [ 250/1251 ( 20%)]  Loss: 3.100 (3.13)  Time: 0.794s, 1290.06/s  (0.791s, 1293.81/s)  LR: 1.532e-04  Data: 0.009 (0.017)
Train: 451 [ 300/1251 ( 24%)]  Loss: 2.902 (3.09)  Time: 0.775s, 1322.12/s  (0.791s, 1294.34/s)  LR: 1.532e-04  Data: 0.010 (0.016)
Train: 451 [ 350/1251 ( 28%)]  Loss: 2.941 (3.07)  Time: 0.774s, 1323.69/s  (0.789s, 1297.18/s)  LR: 1.532e-04  Data: 0.010 (0.015)
Train: 451 [ 400/1251 ( 32%)]  Loss: 3.266 (3.10)  Time: 0.853s, 1200.08/s  (0.789s, 1298.40/s)  LR: 1.532e-04  Data: 0.010 (0.014)
Train: 451 [ 450/1251 ( 36%)]  Loss: 3.141 (3.10)  Time: 0.772s, 1326.65/s  (0.787s, 1300.55/s)  LR: 1.532e-04  Data: 0.009 (0.014)
Train: 451 [ 500/1251 ( 40%)]  Loss: 3.453 (3.13)  Time: 0.860s, 1190.15/s  (0.789s, 1297.54/s)  LR: 1.532e-04  Data: 0.010 (0.013)
Train: 451 [ 550/1251 ( 44%)]  Loss: 3.298 (3.15)  Time: 0.772s, 1325.82/s  (0.788s, 1298.76/s)  LR: 1.532e-04  Data: 0.009 (0.013)
Train: 451 [ 600/1251 ( 48%)]  Loss: 3.353 (3.16)  Time: 0.778s, 1315.59/s  (0.787s, 1300.33/s)  LR: 1.532e-04  Data: 0.010 (0.013)
Train: 451 [ 650/1251 ( 52%)]  Loss: 2.801 (3.14)  Time: 0.779s, 1315.22/s  (0.787s, 1300.44/s)  LR: 1.532e-04  Data: 0.010 (0.013)
Train: 451 [ 700/1251 ( 56%)]  Loss: 3.358 (3.15)  Time: 0.773s, 1324.03/s  (0.787s, 1301.26/s)  LR: 1.532e-04  Data: 0.010 (0.012)
Train: 451 [ 750/1251 ( 60%)]  Loss: 3.569 (3.18)  Time: 0.772s, 1325.99/s  (0.786s, 1302.13/s)  LR: 1.532e-04  Data: 0.010 (0.012)
Train: 451 [ 800/1251 ( 64%)]  Loss: 3.003 (3.17)  Time: 0.773s, 1324.90/s  (0.786s, 1302.89/s)  LR: 1.532e-04  Data: 0.010 (0.012)
Train: 451 [ 850/1251 ( 68%)]  Loss: 3.321 (3.18)  Time: 0.773s, 1325.08/s  (0.786s, 1303.18/s)  LR: 1.532e-04  Data: 0.010 (0.012)
Train: 451 [ 900/1251 ( 72%)]  Loss: 3.471 (3.19)  Time: 0.774s, 1322.63/s  (0.785s, 1303.88/s)  LR: 1.532e-04  Data: 0.010 (0.012)
Train: 451 [ 950/1251 ( 76%)]  Loss: 3.400 (3.20)  Time: 0.781s, 1311.93/s  (0.785s, 1304.49/s)  LR: 1.532e-04  Data: 0.009 (0.012)
Train: 451 [1000/1251 ( 80%)]  Loss: 3.366 (3.21)  Time: 0.773s, 1325.20/s  (0.785s, 1305.14/s)  LR: 1.532e-04  Data: 0.009 (0.012)
Train: 451 [1050/1251 ( 84%)]  Loss: 3.333 (3.21)  Time: 0.782s, 1309.59/s  (0.784s, 1305.51/s)  LR: 1.532e-04  Data: 0.010 (0.012)
Train: 451 [1100/1251 ( 88%)]  Loss: 3.469 (3.23)  Time: 0.772s, 1326.28/s  (0.784s, 1305.72/s)  LR: 1.532e-04  Data: 0.010 (0.011)
Train: 451 [1150/1251 ( 92%)]  Loss: 3.105 (3.22)  Time: 0.773s, 1324.84/s  (0.784s, 1306.03/s)  LR: 1.532e-04  Data: 0.010 (0.011)
Train: 451 [1200/1251 ( 96%)]  Loss: 3.155 (3.22)  Time: 0.772s, 1325.71/s  (0.784s, 1306.32/s)  LR: 1.532e-04  Data: 0.010 (0.011)
Train: 451 [1250/1251 (100%)]  Loss: 2.726 (3.20)  Time: 0.760s, 1347.95/s  (0.784s, 1306.65/s)  LR: 1.532e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.561 (1.561)  Loss:  0.5957 (0.5957)  Acc@1: 91.4062 (91.4062)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.194 (0.576)  Loss:  0.7617 (1.0493)  Acc@1: 85.6132 (78.9980)  Acc@5: 96.9340 (94.7700)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-451.pth.tar', 78.99800008544922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-450.pth.tar', 78.82999997558593)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-448.pth.tar', 78.81600000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-439.pth.tar', 78.80400002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-449.pth.tar', 78.79800010742187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-441.pth.tar', 78.77600000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-440.pth.tar', 78.72999989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-446.pth.tar', 78.70200005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-430.pth.tar', 78.68800002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-447.pth.tar', 78.66199997558594)

Train: 452 [   0/1251 (  0%)]  Loss: 3.049 (3.05)  Time: 2.336s,  438.27/s  (2.336s,  438.27/s)  LR: 1.513e-04  Data: 1.601 (1.601)
Train: 452 [  50/1251 (  4%)]  Loss: 3.152 (3.10)  Time: 0.774s, 1322.60/s  (0.819s, 1249.65/s)  LR: 1.513e-04  Data: 0.010 (0.049)
Train: 452 [ 100/1251 (  8%)]  Loss: 3.002 (3.07)  Time: 0.773s, 1325.56/s  (0.800s, 1279.95/s)  LR: 1.513e-04  Data: 0.009 (0.030)
Train: 452 [ 150/1251 ( 12%)]  Loss: 3.133 (3.08)  Time: 0.776s, 1319.63/s  (0.794s, 1289.82/s)  LR: 1.513e-04  Data: 0.009 (0.023)
Train: 452 [ 200/1251 ( 16%)]  Loss: 3.026 (3.07)  Time: 0.811s, 1262.42/s  (0.800s, 1280.01/s)  LR: 1.513e-04  Data: 0.010 (0.020)
Train: 452 [ 250/1251 ( 20%)]  Loss: 3.363 (3.12)  Time: 0.783s, 1308.12/s  (0.796s, 1286.84/s)  LR: 1.513e-04  Data: 0.009 (0.018)
Train: 452 [ 300/1251 ( 24%)]  Loss: 3.114 (3.12)  Time: 0.777s, 1318.38/s  (0.793s, 1291.72/s)  LR: 1.513e-04  Data: 0.010 (0.016)
Train: 452 [ 350/1251 ( 28%)]  Loss: 2.995 (3.10)  Time: 0.773s, 1324.55/s  (0.791s, 1294.35/s)  LR: 1.513e-04  Data: 0.010 (0.016)
Train: 452 [ 400/1251 ( 32%)]  Loss: 3.081 (3.10)  Time: 0.781s, 1310.49/s  (0.794s, 1290.41/s)  LR: 1.513e-04  Data: 0.012 (0.015)
Train: 452 [ 450/1251 ( 36%)]  Loss: 2.972 (3.09)  Time: 0.786s, 1302.21/s  (0.792s, 1292.60/s)  LR: 1.513e-04  Data: 0.010 (0.014)
Train: 452 [ 500/1251 ( 40%)]  Loss: 2.707 (3.05)  Time: 0.773s, 1324.46/s  (0.791s, 1294.70/s)  LR: 1.513e-04  Data: 0.009 (0.014)
Train: 452 [ 550/1251 ( 44%)]  Loss: 3.126 (3.06)  Time: 0.776s, 1320.13/s  (0.790s, 1296.22/s)  LR: 1.513e-04  Data: 0.009 (0.014)
Train: 452 [ 600/1251 ( 48%)]  Loss: 2.966 (3.05)  Time: 0.773s, 1324.37/s  (0.789s, 1297.09/s)  LR: 1.513e-04  Data: 0.009 (0.013)
Train: 452 [ 650/1251 ( 52%)]  Loss: 2.739 (3.03)  Time: 0.788s, 1298.77/s  (0.789s, 1298.22/s)  LR: 1.513e-04  Data: 0.009 (0.013)
Train: 452 [ 700/1251 ( 56%)]  Loss: 3.284 (3.05)  Time: 0.780s, 1312.04/s  (0.788s, 1298.94/s)  LR: 1.513e-04  Data: 0.012 (0.013)
Train: 452 [ 750/1251 ( 60%)]  Loss: 3.409 (3.07)  Time: 0.808s, 1266.97/s  (0.788s, 1299.95/s)  LR: 1.513e-04  Data: 0.010 (0.013)
Train: 452 [ 800/1251 ( 64%)]  Loss: 3.180 (3.08)  Time: 0.775s, 1321.80/s  (0.788s, 1299.66/s)  LR: 1.513e-04  Data: 0.011 (0.012)
Train: 452 [ 850/1251 ( 68%)]  Loss: 3.205 (3.08)  Time: 0.774s, 1323.73/s  (0.789s, 1298.66/s)  LR: 1.513e-04  Data: 0.010 (0.012)
Train: 452 [ 900/1251 ( 72%)]  Loss: 2.958 (3.08)  Time: 0.774s, 1322.58/s  (0.788s, 1299.10/s)  LR: 1.513e-04  Data: 0.010 (0.012)
Train: 452 [ 950/1251 ( 76%)]  Loss: 3.112 (3.08)  Time: 0.773s, 1324.02/s  (0.788s, 1298.75/s)  LR: 1.513e-04  Data: 0.011 (0.012)
Train: 452 [1000/1251 ( 80%)]  Loss: 3.010 (3.08)  Time: 0.774s, 1322.48/s  (0.789s, 1297.42/s)  LR: 1.513e-04  Data: 0.010 (0.012)
Train: 452 [1050/1251 ( 84%)]  Loss: 3.276 (3.08)  Time: 0.831s, 1232.85/s  (0.789s, 1298.07/s)  LR: 1.513e-04  Data: 0.015 (0.012)
Train: 452 [1100/1251 ( 88%)]  Loss: 3.261 (3.09)  Time: 0.800s, 1279.91/s  (0.788s, 1298.69/s)  LR: 1.513e-04  Data: 0.010 (0.012)
Train: 452 [1150/1251 ( 92%)]  Loss: 3.273 (3.10)  Time: 0.774s, 1322.32/s  (0.790s, 1296.88/s)  LR: 1.513e-04  Data: 0.010 (0.012)
Train: 452 [1200/1251 ( 96%)]  Loss: 3.046 (3.10)  Time: 0.780s, 1312.87/s  (0.789s, 1297.85/s)  LR: 1.513e-04  Data: 0.009 (0.012)
Train: 452 [1250/1251 (100%)]  Loss: 2.898 (3.09)  Time: 0.761s, 1345.28/s  (0.789s, 1298.41/s)  LR: 1.513e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.547 (1.547)  Loss:  0.6313 (0.6313)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.194 (0.564)  Loss:  0.7485 (1.1180)  Acc@1: 87.0283 (78.7260)  Acc@5: 97.7594 (94.6600)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-451.pth.tar', 78.99800008544922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-450.pth.tar', 78.82999997558593)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-448.pth.tar', 78.81600000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-439.pth.tar', 78.80400002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-449.pth.tar', 78.79800010742187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-441.pth.tar', 78.77600000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-440.pth.tar', 78.72999989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-452.pth.tar', 78.72600005371093)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-446.pth.tar', 78.70200005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-430.pth.tar', 78.68800002929687)

Train: 453 [   0/1251 (  0%)]  Loss: 3.289 (3.29)  Time: 2.258s,  453.51/s  (2.258s,  453.51/s)  LR: 1.495e-04  Data: 1.531 (1.531)
Train: 453 [  50/1251 (  4%)]  Loss: 2.999 (3.14)  Time: 0.778s, 1316.24/s  (0.823s, 1244.72/s)  LR: 1.495e-04  Data: 0.013 (0.045)
Train: 453 [ 100/1251 (  8%)]  Loss: 3.166 (3.15)  Time: 0.772s, 1326.58/s  (0.802s, 1276.27/s)  LR: 1.495e-04  Data: 0.009 (0.028)
Train: 453 [ 150/1251 ( 12%)]  Loss: 3.464 (3.23)  Time: 0.773s, 1323.93/s  (0.795s, 1287.75/s)  LR: 1.495e-04  Data: 0.009 (0.022)
Train: 453 [ 200/1251 ( 16%)]  Loss: 3.217 (3.23)  Time: 0.773s, 1325.17/s  (0.791s, 1294.47/s)  LR: 1.495e-04  Data: 0.010 (0.019)
Train: 453 [ 250/1251 ( 20%)]  Loss: 3.028 (3.19)  Time: 0.771s, 1328.48/s  (0.789s, 1298.30/s)  LR: 1.495e-04  Data: 0.010 (0.017)
Train: 453 [ 300/1251 ( 24%)]  Loss: 3.299 (3.21)  Time: 0.778s, 1315.50/s  (0.787s, 1301.14/s)  LR: 1.495e-04  Data: 0.009 (0.016)
Train: 453 [ 350/1251 ( 28%)]  Loss: 3.026 (3.19)  Time: 0.776s, 1319.42/s  (0.787s, 1301.94/s)  LR: 1.495e-04  Data: 0.009 (0.015)
Train: 453 [ 400/1251 ( 32%)]  Loss: 3.056 (3.17)  Time: 0.773s, 1325.46/s  (0.785s, 1303.92/s)  LR: 1.495e-04  Data: 0.010 (0.014)
Train: 453 [ 450/1251 ( 36%)]  Loss: 3.370 (3.19)  Time: 0.773s, 1324.28/s  (0.785s, 1305.03/s)  LR: 1.495e-04  Data: 0.010 (0.014)
Train: 453 [ 500/1251 ( 40%)]  Loss: 3.164 (3.19)  Time: 0.771s, 1327.53/s  (0.784s, 1305.97/s)  LR: 1.495e-04  Data: 0.010 (0.013)
Train: 453 [ 550/1251 ( 44%)]  Loss: 3.463 (3.21)  Time: 0.775s, 1320.48/s  (0.784s, 1306.57/s)  LR: 1.495e-04  Data: 0.010 (0.013)
Train: 453 [ 600/1251 ( 48%)]  Loss: 2.728 (3.17)  Time: 0.772s, 1325.62/s  (0.784s, 1306.94/s)  LR: 1.495e-04  Data: 0.010 (0.013)
Train: 453 [ 650/1251 ( 52%)]  Loss: 3.207 (3.18)  Time: 0.774s, 1323.54/s  (0.783s, 1307.48/s)  LR: 1.495e-04  Data: 0.010 (0.013)
Train: 453 [ 700/1251 ( 56%)]  Loss: 2.831 (3.15)  Time: 0.815s, 1256.92/s  (0.785s, 1304.55/s)  LR: 1.495e-04  Data: 0.011 (0.012)
Train: 453 [ 750/1251 ( 60%)]  Loss: 3.318 (3.16)  Time: 0.773s, 1324.15/s  (0.785s, 1304.10/s)  LR: 1.495e-04  Data: 0.010 (0.012)
Train: 453 [ 800/1251 ( 64%)]  Loss: 2.857 (3.15)  Time: 0.772s, 1325.72/s  (0.785s, 1304.15/s)  LR: 1.495e-04  Data: 0.009 (0.012)
Train: 453 [ 850/1251 ( 68%)]  Loss: 3.374 (3.16)  Time: 0.783s, 1307.19/s  (0.785s, 1304.69/s)  LR: 1.495e-04  Data: 0.013 (0.012)
Train: 453 [ 900/1251 ( 72%)]  Loss: 3.222 (3.16)  Time: 0.852s, 1201.67/s  (0.785s, 1305.09/s)  LR: 1.495e-04  Data: 0.009 (0.012)
Train: 453 [ 950/1251 ( 76%)]  Loss: 3.236 (3.17)  Time: 0.807s, 1269.18/s  (0.786s, 1303.35/s)  LR: 1.495e-04  Data: 0.009 (0.012)
Train: 453 [1000/1251 ( 80%)]  Loss: 3.460 (3.18)  Time: 0.814s, 1257.83/s  (0.786s, 1302.28/s)  LR: 1.495e-04  Data: 0.011 (0.012)
Train: 453 [1050/1251 ( 84%)]  Loss: 3.097 (3.18)  Time: 0.773s, 1324.86/s  (0.786s, 1302.44/s)  LR: 1.495e-04  Data: 0.009 (0.012)
Train: 453 [1100/1251 ( 88%)]  Loss: 3.232 (3.18)  Time: 0.773s, 1325.40/s  (0.786s, 1302.69/s)  LR: 1.495e-04  Data: 0.009 (0.011)
Train: 453 [1150/1251 ( 92%)]  Loss: 2.912 (3.17)  Time: 0.815s, 1256.08/s  (0.787s, 1301.90/s)  LR: 1.495e-04  Data: 0.010 (0.011)
Train: 453 [1200/1251 ( 96%)]  Loss: 3.425 (3.18)  Time: 0.773s, 1325.14/s  (0.786s, 1302.11/s)  LR: 1.495e-04  Data: 0.010 (0.011)
Train: 453 [1250/1251 (100%)]  Loss: 3.251 (3.18)  Time: 0.772s, 1326.65/s  (0.787s, 1301.88/s)  LR: 1.495e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.533 (1.533)  Loss:  0.6738 (0.6738)  Acc@1: 91.7969 (91.7969)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.194 (0.563)  Loss:  0.8384 (1.1757)  Acc@1: 87.0283 (78.8740)  Acc@5: 97.7594 (94.6580)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-451.pth.tar', 78.99800008544922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-453.pth.tar', 78.87400018310547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-450.pth.tar', 78.82999997558593)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-448.pth.tar', 78.81600000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-439.pth.tar', 78.80400002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-449.pth.tar', 78.79800010742187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-441.pth.tar', 78.77600000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-440.pth.tar', 78.72999989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-452.pth.tar', 78.72600005371093)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-446.pth.tar', 78.70200005615234)

Train: 454 [   0/1251 (  0%)]  Loss: 3.393 (3.39)  Time: 2.336s,  438.27/s  (2.336s,  438.27/s)  LR: 1.477e-04  Data: 1.603 (1.603)
Train: 454 [  50/1251 (  4%)]  Loss: 3.434 (3.41)  Time: 0.773s, 1324.63/s  (0.817s, 1252.74/s)  LR: 1.477e-04  Data: 0.010 (0.047)
Train: 454 [ 100/1251 (  8%)]  Loss: 3.132 (3.32)  Time: 0.781s, 1310.84/s  (0.800s, 1280.13/s)  LR: 1.477e-04  Data: 0.009 (0.028)
Train: 454 [ 150/1251 ( 12%)]  Loss: 3.050 (3.25)  Time: 0.775s, 1321.72/s  (0.793s, 1291.74/s)  LR: 1.477e-04  Data: 0.010 (0.022)
Train: 454 [ 200/1251 ( 16%)]  Loss: 3.211 (3.24)  Time: 0.786s, 1302.11/s  (0.792s, 1293.60/s)  LR: 1.477e-04  Data: 0.010 (0.019)
Train: 454 [ 250/1251 ( 20%)]  Loss: 3.099 (3.22)  Time: 0.774s, 1322.95/s  (0.789s, 1298.64/s)  LR: 1.477e-04  Data: 0.009 (0.017)
Train: 454 [ 300/1251 ( 24%)]  Loss: 3.084 (3.20)  Time: 0.775s, 1322.03/s  (0.788s, 1299.21/s)  LR: 1.477e-04  Data: 0.009 (0.016)
Train: 454 [ 350/1251 ( 28%)]  Loss: 3.115 (3.19)  Time: 0.776s, 1319.89/s  (0.791s, 1295.14/s)  LR: 1.477e-04  Data: 0.009 (0.015)
Train: 454 [ 400/1251 ( 32%)]  Loss: 3.311 (3.20)  Time: 0.826s, 1240.10/s  (0.789s, 1297.14/s)  LR: 1.477e-04  Data: 0.009 (0.014)
Train: 454 [ 450/1251 ( 36%)]  Loss: 3.179 (3.20)  Time: 0.822s, 1245.17/s  (0.789s, 1298.43/s)  LR: 1.477e-04  Data: 0.010 (0.014)
Train: 454 [ 500/1251 ( 40%)]  Loss: 3.129 (3.19)  Time: 0.773s, 1324.48/s  (0.788s, 1299.33/s)  LR: 1.477e-04  Data: 0.009 (0.013)
Train: 454 [ 550/1251 ( 44%)]  Loss: 3.306 (3.20)  Time: 0.774s, 1322.59/s  (0.789s, 1298.54/s)  LR: 1.477e-04  Data: 0.010 (0.013)
Train: 454 [ 600/1251 ( 48%)]  Loss: 2.754 (3.17)  Time: 0.783s, 1307.15/s  (0.788s, 1299.37/s)  LR: 1.477e-04  Data: 0.010 (0.013)
Train: 454 [ 650/1251 ( 52%)]  Loss: 2.918 (3.15)  Time: 0.804s, 1273.96/s  (0.788s, 1300.26/s)  LR: 1.477e-04  Data: 0.009 (0.013)
Train: 454 [ 700/1251 ( 56%)]  Loss: 3.016 (3.14)  Time: 0.774s, 1323.06/s  (0.787s, 1301.43/s)  LR: 1.477e-04  Data: 0.010 (0.012)
Train: 454 [ 750/1251 ( 60%)]  Loss: 2.771 (3.12)  Time: 0.774s, 1323.04/s  (0.786s, 1302.27/s)  LR: 1.477e-04  Data: 0.010 (0.012)
Train: 454 [ 800/1251 ( 64%)]  Loss: 3.272 (3.13)  Time: 0.788s, 1300.00/s  (0.786s, 1303.14/s)  LR: 1.477e-04  Data: 0.010 (0.012)
Train: 454 [ 850/1251 ( 68%)]  Loss: 3.298 (3.14)  Time: 0.774s, 1323.82/s  (0.785s, 1303.68/s)  LR: 1.477e-04  Data: 0.009 (0.012)
Train: 454 [ 900/1251 ( 72%)]  Loss: 2.853 (3.12)  Time: 0.773s, 1324.98/s  (0.785s, 1304.13/s)  LR: 1.477e-04  Data: 0.010 (0.012)
Train: 454 [ 950/1251 ( 76%)]  Loss: 2.714 (3.10)  Time: 0.790s, 1295.63/s  (0.786s, 1303.40/s)  LR: 1.477e-04  Data: 0.010 (0.012)
Train: 454 [1000/1251 ( 80%)]  Loss: 3.043 (3.10)  Time: 0.791s, 1294.47/s  (0.786s, 1303.52/s)  LR: 1.477e-04  Data: 0.011 (0.012)
Train: 454 [1050/1251 ( 84%)]  Loss: 3.320 (3.11)  Time: 0.772s, 1326.36/s  (0.785s, 1303.99/s)  LR: 1.477e-04  Data: 0.010 (0.012)
Train: 454 [1100/1251 ( 88%)]  Loss: 3.342 (3.12)  Time: 0.789s, 1297.83/s  (0.785s, 1304.38/s)  LR: 1.477e-04  Data: 0.011 (0.011)
Train: 454 [1150/1251 ( 92%)]  Loss: 3.200 (3.12)  Time: 0.773s, 1324.82/s  (0.785s, 1304.58/s)  LR: 1.477e-04  Data: 0.009 (0.011)
Train: 454 [1200/1251 ( 96%)]  Loss: 2.791 (3.11)  Time: 0.778s, 1316.82/s  (0.786s, 1303.48/s)  LR: 1.477e-04  Data: 0.010 (0.011)
Train: 454 [1250/1251 (100%)]  Loss: 2.984 (3.10)  Time: 0.760s, 1346.83/s  (0.786s, 1303.54/s)  LR: 1.477e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.510 (1.510)  Loss:  0.6572 (0.6572)  Acc@1: 91.4062 (91.4062)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.194 (0.566)  Loss:  0.8066 (1.1285)  Acc@1: 85.4953 (78.9040)  Acc@5: 96.8160 (94.5440)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-451.pth.tar', 78.99800008544922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-454.pth.tar', 78.90400003417969)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-453.pth.tar', 78.87400018310547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-450.pth.tar', 78.82999997558593)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-448.pth.tar', 78.81600000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-439.pth.tar', 78.80400002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-449.pth.tar', 78.79800010742187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-441.pth.tar', 78.77600000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-440.pth.tar', 78.72999989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-452.pth.tar', 78.72600005371093)

Train: 455 [   0/1251 (  0%)]  Loss: 3.055 (3.06)  Time: 2.293s,  446.66/s  (2.293s,  446.66/s)  LR: 1.459e-04  Data: 1.566 (1.566)
Train: 455 [  50/1251 (  4%)]  Loss: 3.013 (3.03)  Time: 0.774s, 1323.04/s  (0.828s, 1236.32/s)  LR: 1.459e-04  Data: 0.009 (0.046)
Train: 455 [ 100/1251 (  8%)]  Loss: 3.051 (3.04)  Time: 0.779s, 1315.25/s  (0.817s, 1254.11/s)  LR: 1.459e-04  Data: 0.010 (0.028)
Train: 455 [ 150/1251 ( 12%)]  Loss: 2.936 (3.01)  Time: 0.809s, 1265.03/s  (0.808s, 1266.99/s)  LR: 1.459e-04  Data: 0.009 (0.022)
Train: 455 [ 200/1251 ( 16%)]  Loss: 3.124 (3.04)  Time: 0.772s, 1326.07/s  (0.802s, 1277.47/s)  LR: 1.459e-04  Data: 0.009 (0.019)
Train: 455 [ 250/1251 ( 20%)]  Loss: 3.343 (3.09)  Time: 0.773s, 1324.44/s  (0.798s, 1282.55/s)  LR: 1.459e-04  Data: 0.010 (0.017)
Train: 455 [ 300/1251 ( 24%)]  Loss: 3.368 (3.13)  Time: 0.778s, 1316.91/s  (0.796s, 1287.08/s)  LR: 1.459e-04  Data: 0.009 (0.016)
Train: 455 [ 350/1251 ( 28%)]  Loss: 3.049 (3.12)  Time: 0.776s, 1319.42/s  (0.794s, 1289.57/s)  LR: 1.459e-04  Data: 0.009 (0.015)
Train: 455 [ 400/1251 ( 32%)]  Loss: 3.228 (3.13)  Time: 0.772s, 1325.62/s  (0.793s, 1290.50/s)  LR: 1.459e-04  Data: 0.009 (0.015)
Train: 455 [ 450/1251 ( 36%)]  Loss: 3.229 (3.14)  Time: 0.775s, 1320.76/s  (0.792s, 1292.99/s)  LR: 1.459e-04  Data: 0.010 (0.014)
Train: 455 [ 500/1251 ( 40%)]  Loss: 3.411 (3.16)  Time: 0.783s, 1307.11/s  (0.791s, 1294.58/s)  LR: 1.459e-04  Data: 0.010 (0.014)
Train: 455 [ 550/1251 ( 44%)]  Loss: 3.187 (3.17)  Time: 0.786s, 1303.05/s  (0.790s, 1296.12/s)  LR: 1.459e-04  Data: 0.009 (0.013)
Train: 455 [ 600/1251 ( 48%)]  Loss: 3.461 (3.19)  Time: 0.775s, 1320.62/s  (0.789s, 1297.07/s)  LR: 1.459e-04  Data: 0.009 (0.013)
Train: 455 [ 650/1251 ( 52%)]  Loss: 2.803 (3.16)  Time: 0.777s, 1317.77/s  (0.788s, 1298.93/s)  LR: 1.459e-04  Data: 0.009 (0.013)
Train: 455 [ 700/1251 ( 56%)]  Loss: 3.259 (3.17)  Time: 0.774s, 1322.55/s  (0.789s, 1298.20/s)  LR: 1.459e-04  Data: 0.010 (0.012)
Train: 455 [ 750/1251 ( 60%)]  Loss: 3.183 (3.17)  Time: 0.785s, 1304.80/s  (0.788s, 1298.96/s)  LR: 1.459e-04  Data: 0.009 (0.012)
Train: 455 [ 800/1251 ( 64%)]  Loss: 3.243 (3.17)  Time: 0.835s, 1226.44/s  (0.788s, 1299.48/s)  LR: 1.459e-04  Data: 0.010 (0.012)
Train: 455 [ 850/1251 ( 68%)]  Loss: 3.062 (3.17)  Time: 0.802s, 1277.29/s  (0.788s, 1299.24/s)  LR: 1.459e-04  Data: 0.009 (0.012)
Train: 455 [ 900/1251 ( 72%)]  Loss: 3.181 (3.17)  Time: 0.773s, 1324.46/s  (0.789s, 1298.05/s)  LR: 1.459e-04  Data: 0.010 (0.012)
Train: 455 [ 950/1251 ( 76%)]  Loss: 3.223 (3.17)  Time: 0.771s, 1327.76/s  (0.789s, 1297.92/s)  LR: 1.459e-04  Data: 0.009 (0.012)
Train: 455 [1000/1251 ( 80%)]  Loss: 3.228 (3.17)  Time: 0.772s, 1325.70/s  (0.788s, 1298.96/s)  LR: 1.459e-04  Data: 0.009 (0.012)
Train: 455 [1050/1251 ( 84%)]  Loss: 3.132 (3.17)  Time: 0.850s, 1204.41/s  (0.788s, 1299.67/s)  LR: 1.459e-04  Data: 0.009 (0.012)
Train: 455 [1100/1251 ( 88%)]  Loss: 3.319 (3.18)  Time: 0.776s, 1318.77/s  (0.788s, 1300.14/s)  LR: 1.459e-04  Data: 0.010 (0.011)
Train: 455 [1150/1251 ( 92%)]  Loss: 3.185 (3.18)  Time: 0.773s, 1324.81/s  (0.787s, 1300.68/s)  LR: 1.459e-04  Data: 0.009 (0.011)
Train: 455 [1200/1251 ( 96%)]  Loss: 3.506 (3.19)  Time: 0.772s, 1326.75/s  (0.787s, 1301.34/s)  LR: 1.459e-04  Data: 0.009 (0.011)
Train: 455 [1250/1251 (100%)]  Loss: 2.869 (3.18)  Time: 0.763s, 1342.92/s  (0.787s, 1301.78/s)  LR: 1.459e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.565 (1.565)  Loss:  0.6963 (0.6963)  Acc@1: 91.7969 (91.7969)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.194 (0.557)  Loss:  0.7974 (1.1492)  Acc@1: 86.9104 (79.1080)  Acc@5: 97.6415 (94.6540)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-455.pth.tar', 79.10799987304688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-451.pth.tar', 78.99800008544922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-454.pth.tar', 78.90400003417969)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-453.pth.tar', 78.87400018310547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-450.pth.tar', 78.82999997558593)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-448.pth.tar', 78.81600000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-439.pth.tar', 78.80400002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-449.pth.tar', 78.79800010742187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-441.pth.tar', 78.77600000488282)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-440.pth.tar', 78.72999989990234)

Train: 456 [   0/1251 (  0%)]  Loss: 3.304 (3.30)  Time: 2.188s,  467.95/s  (2.188s,  467.95/s)  LR: 1.442e-04  Data: 1.460 (1.460)
Train: 456 [  50/1251 (  4%)]  Loss: 3.129 (3.22)  Time: 0.782s, 1308.67/s  (0.816s, 1255.00/s)  LR: 1.442e-04  Data: 0.010 (0.045)
Train: 456 [ 100/1251 (  8%)]  Loss: 2.874 (3.10)  Time: 0.773s, 1325.26/s  (0.799s, 1281.75/s)  LR: 1.442e-04  Data: 0.009 (0.028)
Train: 456 [ 150/1251 ( 12%)]  Loss: 3.318 (3.16)  Time: 0.808s, 1268.00/s  (0.796s, 1287.17/s)  LR: 1.442e-04  Data: 0.009 (0.022)
Train: 456 [ 200/1251 ( 16%)]  Loss: 3.002 (3.13)  Time: 0.774s, 1323.71/s  (0.801s, 1279.03/s)  LR: 1.442e-04  Data: 0.010 (0.019)
Train: 456 [ 250/1251 ( 20%)]  Loss: 3.155 (3.13)  Time: 0.777s, 1318.38/s  (0.797s, 1285.06/s)  LR: 1.442e-04  Data: 0.010 (0.017)
Train: 456 [ 300/1251 ( 24%)]  Loss: 3.140 (3.13)  Time: 0.777s, 1318.32/s  (0.794s, 1290.27/s)  LR: 1.442e-04  Data: 0.011 (0.016)
Train: 456 [ 350/1251 ( 28%)]  Loss: 3.249 (3.15)  Time: 0.775s, 1321.03/s  (0.793s, 1291.07/s)  LR: 1.442e-04  Data: 0.009 (0.015)
Train: 456 [ 400/1251 ( 32%)]  Loss: 3.196 (3.15)  Time: 0.783s, 1308.21/s  (0.791s, 1293.89/s)  LR: 1.442e-04  Data: 0.009 (0.014)
Train: 456 [ 450/1251 ( 36%)]  Loss: 3.230 (3.16)  Time: 0.833s, 1229.78/s  (0.791s, 1293.91/s)  LR: 1.442e-04  Data: 0.009 (0.014)
Train: 456 [ 500/1251 ( 40%)]  Loss: 3.054 (3.15)  Time: 0.789s, 1298.20/s  (0.791s, 1294.06/s)  LR: 1.442e-04  Data: 0.009 (0.013)
Train: 456 [ 550/1251 ( 44%)]  Loss: 3.209 (3.16)  Time: 0.778s, 1316.47/s  (0.790s, 1295.42/s)  LR: 1.442e-04  Data: 0.010 (0.013)
Train: 456 [ 600/1251 ( 48%)]  Loss: 3.106 (3.15)  Time: 0.775s, 1320.61/s  (0.790s, 1296.84/s)  LR: 1.442e-04  Data: 0.009 (0.013)
Train: 456 [ 650/1251 ( 52%)]  Loss: 3.010 (3.14)  Time: 0.774s, 1323.68/s  (0.789s, 1297.64/s)  LR: 1.442e-04  Data: 0.010 (0.012)
Train: 456 [ 700/1251 ( 56%)]  Loss: 3.095 (3.14)  Time: 0.771s, 1328.92/s  (0.788s, 1299.05/s)  LR: 1.442e-04  Data: 0.009 (0.012)
Train: 456 [ 750/1251 ( 60%)]  Loss: 3.456 (3.16)  Time: 0.777s, 1318.41/s  (0.788s, 1300.27/s)  LR: 1.442e-04  Data: 0.010 (0.012)
Train: 456 [ 800/1251 ( 64%)]  Loss: 3.350 (3.17)  Time: 0.773s, 1324.40/s  (0.787s, 1301.13/s)  LR: 1.442e-04  Data: 0.010 (0.012)
Train: 456 [ 850/1251 ( 68%)]  Loss: 3.018 (3.16)  Time: 0.775s, 1320.71/s  (0.787s, 1301.79/s)  LR: 1.442e-04  Data: 0.010 (0.012)
Train: 456 [ 900/1251 ( 72%)]  Loss: 3.206 (3.16)  Time: 0.774s, 1323.69/s  (0.786s, 1302.10/s)  LR: 1.442e-04  Data: 0.009 (0.012)
Train: 456 [ 950/1251 ( 76%)]  Loss: 3.265 (3.17)  Time: 0.774s, 1323.23/s  (0.786s, 1302.56/s)  LR: 1.442e-04  Data: 0.009 (0.012)
Train: 456 [1000/1251 ( 80%)]  Loss: 3.268 (3.17)  Time: 0.815s, 1256.58/s  (0.787s, 1301.90/s)  LR: 1.442e-04  Data: 0.011 (0.011)
Train: 456 [1050/1251 ( 84%)]  Loss: 3.300 (3.18)  Time: 0.773s, 1324.37/s  (0.786s, 1302.28/s)  LR: 1.442e-04  Data: 0.009 (0.011)
Train: 456 [1100/1251 ( 88%)]  Loss: 3.252 (3.18)  Time: 0.774s, 1322.65/s  (0.786s, 1302.76/s)  LR: 1.442e-04  Data: 0.009 (0.011)
Train: 456 [1150/1251 ( 92%)]  Loss: 2.710 (3.16)  Time: 0.773s, 1324.46/s  (0.786s, 1303.37/s)  LR: 1.442e-04  Data: 0.010 (0.011)
Train: 456 [1200/1251 ( 96%)]  Loss: 2.868 (3.15)  Time: 0.774s, 1323.30/s  (0.785s, 1303.81/s)  LR: 1.442e-04  Data: 0.009 (0.011)
Train: 456 [1250/1251 (100%)]  Loss: 2.960 (3.14)  Time: 0.799s, 1281.35/s  (0.785s, 1304.17/s)  LR: 1.442e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.582 (1.582)  Loss:  0.5645 (0.5645)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.194 (0.563)  Loss:  0.7324 (1.0663)  Acc@1: 86.6745 (79.0460)  Acc@5: 97.1698 (94.7360)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-455.pth.tar', 79.10799987304688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-456.pth.tar', 79.04600015869141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-451.pth.tar', 78.99800008544922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-454.pth.tar', 78.90400003417969)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-453.pth.tar', 78.87400018310547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-450.pth.tar', 78.82999997558593)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-448.pth.tar', 78.81600000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-439.pth.tar', 78.80400002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-449.pth.tar', 78.79800010742187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-441.pth.tar', 78.77600000488282)

Train: 457 [   0/1251 (  0%)]  Loss: 3.164 (3.16)  Time: 2.401s,  426.54/s  (2.401s,  426.54/s)  LR: 1.424e-04  Data: 1.670 (1.670)
Train: 457 [  50/1251 (  4%)]  Loss: 3.248 (3.21)  Time: 0.772s, 1326.46/s  (0.814s, 1258.21/s)  LR: 1.424e-04  Data: 0.009 (0.045)
Train: 457 [ 100/1251 (  8%)]  Loss: 3.302 (3.24)  Time: 0.772s, 1326.53/s  (0.797s, 1284.85/s)  LR: 1.424e-04  Data: 0.009 (0.028)
Train: 457 [ 150/1251 ( 12%)]  Loss: 2.890 (3.15)  Time: 0.773s, 1324.50/s  (0.791s, 1295.23/s)  LR: 1.424e-04  Data: 0.010 (0.022)
Train: 457 [ 200/1251 ( 16%)]  Loss: 3.203 (3.16)  Time: 0.772s, 1326.46/s  (0.788s, 1300.02/s)  LR: 1.424e-04  Data: 0.009 (0.019)
Train: 457 [ 250/1251 ( 20%)]  Loss: 3.272 (3.18)  Time: 0.774s, 1323.10/s  (0.785s, 1304.58/s)  LR: 1.424e-04  Data: 0.010 (0.017)
Train: 457 [ 300/1251 ( 24%)]  Loss: 3.232 (3.19)  Time: 0.772s, 1326.67/s  (0.783s, 1307.36/s)  LR: 1.424e-04  Data: 0.009 (0.016)
Train: 457 [ 350/1251 ( 28%)]  Loss: 2.792 (3.14)  Time: 0.816s, 1254.23/s  (0.783s, 1308.27/s)  LR: 1.424e-04  Data: 0.009 (0.015)
Train: 457 [ 400/1251 ( 32%)]  Loss: 3.341 (3.16)  Time: 0.777s, 1317.39/s  (0.782s, 1309.08/s)  LR: 1.424e-04  Data: 0.010 (0.014)
Train: 457 [ 450/1251 ( 36%)]  Loss: 3.276 (3.17)  Time: 0.776s, 1319.80/s  (0.783s, 1308.31/s)  LR: 1.424e-04  Data: 0.009 (0.014)
Train: 457 [ 500/1251 ( 40%)]  Loss: 3.140 (3.17)  Time: 0.772s, 1326.81/s  (0.783s, 1308.54/s)  LR: 1.424e-04  Data: 0.009 (0.013)
Train: 457 [ 550/1251 ( 44%)]  Loss: 3.227 (3.17)  Time: 0.773s, 1325.17/s  (0.782s, 1309.28/s)  LR: 1.424e-04  Data: 0.009 (0.013)
Train: 457 [ 600/1251 ( 48%)]  Loss: 3.123 (3.17)  Time: 0.773s, 1325.37/s  (0.782s, 1310.09/s)  LR: 1.424e-04  Data: 0.011 (0.013)
Train: 457 [ 650/1251 ( 52%)]  Loss: 2.899 (3.15)  Time: 0.775s, 1321.87/s  (0.783s, 1308.39/s)  LR: 1.424e-04  Data: 0.009 (0.012)
Train: 457 [ 700/1251 ( 56%)]  Loss: 3.161 (3.15)  Time: 0.774s, 1323.52/s  (0.782s, 1308.74/s)  LR: 1.424e-04  Data: 0.009 (0.012)
Train: 457 [ 750/1251 ( 60%)]  Loss: 3.262 (3.16)  Time: 0.772s, 1326.05/s  (0.782s, 1308.86/s)  LR: 1.424e-04  Data: 0.010 (0.012)
Train: 457 [ 800/1251 ( 64%)]  Loss: 3.248 (3.16)  Time: 0.776s, 1318.83/s  (0.782s, 1309.12/s)  LR: 1.424e-04  Data: 0.011 (0.012)
Train: 457 [ 850/1251 ( 68%)]  Loss: 3.147 (3.16)  Time: 0.773s, 1325.50/s  (0.782s, 1309.19/s)  LR: 1.424e-04  Data: 0.010 (0.012)
Train: 457 [ 900/1251 ( 72%)]  Loss: 3.310 (3.17)  Time: 0.780s, 1312.77/s  (0.782s, 1309.08/s)  LR: 1.424e-04  Data: 0.010 (0.012)
Train: 457 [ 950/1251 ( 76%)]  Loss: 3.225 (3.17)  Time: 0.773s, 1324.55/s  (0.782s, 1309.29/s)  LR: 1.424e-04  Data: 0.009 (0.012)
Train: 457 [1000/1251 ( 80%)]  Loss: 3.423 (3.19)  Time: 0.787s, 1300.66/s  (0.782s, 1309.45/s)  LR: 1.424e-04  Data: 0.009 (0.011)
Train: 457 [1050/1251 ( 84%)]  Loss: 3.187 (3.19)  Time: 0.771s, 1328.62/s  (0.782s, 1309.71/s)  LR: 1.424e-04  Data: 0.009 (0.011)
Train: 457 [1100/1251 ( 88%)]  Loss: 3.064 (3.18)  Time: 0.774s, 1322.88/s  (0.782s, 1308.84/s)  LR: 1.424e-04  Data: 0.009 (0.011)
Train: 457 [1150/1251 ( 92%)]  Loss: 3.291 (3.18)  Time: 0.776s, 1318.85/s  (0.782s, 1308.89/s)  LR: 1.424e-04  Data: 0.009 (0.011)
Train: 457 [1200/1251 ( 96%)]  Loss: 3.083 (3.18)  Time: 0.773s, 1324.07/s  (0.782s, 1309.37/s)  LR: 1.424e-04  Data: 0.009 (0.011)
Train: 457 [1250/1251 (100%)]  Loss: 3.030 (3.17)  Time: 0.763s, 1342.03/s  (0.782s, 1309.55/s)  LR: 1.424e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.499 (1.499)  Loss:  0.6704 (0.6704)  Acc@1: 92.3828 (92.3828)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.194 (0.566)  Loss:  0.8330 (1.1653)  Acc@1: 86.4387 (78.8820)  Acc@5: 97.0519 (94.7360)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-455.pth.tar', 79.10799987304688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-456.pth.tar', 79.04600015869141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-451.pth.tar', 78.99800008544922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-454.pth.tar', 78.90400003417969)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-457.pth.tar', 78.88200005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-453.pth.tar', 78.87400018310547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-450.pth.tar', 78.82999997558593)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-448.pth.tar', 78.81600000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-439.pth.tar', 78.80400002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-449.pth.tar', 78.79800010742187)

Train: 458 [   0/1251 (  0%)]  Loss: 3.282 (3.28)  Time: 2.377s,  430.75/s  (2.377s,  430.75/s)  LR: 1.406e-04  Data: 1.642 (1.642)
Train: 458 [  50/1251 (  4%)]  Loss: 2.693 (2.99)  Time: 0.770s, 1329.44/s  (0.815s, 1256.67/s)  LR: 1.406e-04  Data: 0.009 (0.045)
Train: 458 [ 100/1251 (  8%)]  Loss: 3.028 (3.00)  Time: 0.773s, 1325.37/s  (0.798s, 1283.28/s)  LR: 1.406e-04  Data: 0.009 (0.028)
Train: 458 [ 150/1251 ( 12%)]  Loss: 3.341 (3.09)  Time: 0.772s, 1325.76/s  (0.792s, 1292.31/s)  LR: 1.406e-04  Data: 0.010 (0.022)
Train: 458 [ 200/1251 ( 16%)]  Loss: 3.003 (3.07)  Time: 0.785s, 1305.11/s  (0.790s, 1296.74/s)  LR: 1.406e-04  Data: 0.009 (0.019)
Train: 458 [ 250/1251 ( 20%)]  Loss: 2.941 (3.05)  Time: 0.782s, 1308.66/s  (0.788s, 1300.09/s)  LR: 1.406e-04  Data: 0.011 (0.017)
Train: 458 [ 300/1251 ( 24%)]  Loss: 3.271 (3.08)  Time: 0.819s, 1250.46/s  (0.786s, 1302.78/s)  LR: 1.406e-04  Data: 0.011 (0.016)
Train: 458 [ 350/1251 ( 28%)]  Loss: 3.240 (3.10)  Time: 0.772s, 1326.43/s  (0.785s, 1304.20/s)  LR: 1.406e-04  Data: 0.009 (0.015)
Train: 458 [ 400/1251 ( 32%)]  Loss: 3.298 (3.12)  Time: 0.776s, 1320.30/s  (0.786s, 1303.00/s)  LR: 1.406e-04  Data: 0.010 (0.014)
Train: 458 [ 450/1251 ( 36%)]  Loss: 3.249 (3.13)  Time: 0.786s, 1302.93/s  (0.788s, 1299.79/s)  LR: 1.406e-04  Data: 0.010 (0.014)
Train: 458 [ 500/1251 ( 40%)]  Loss: 2.902 (3.11)  Time: 0.774s, 1322.63/s  (0.787s, 1301.02/s)  LR: 1.406e-04  Data: 0.010 (0.013)
Train: 458 [ 550/1251 ( 44%)]  Loss: 2.968 (3.10)  Time: 0.777s, 1317.62/s  (0.789s, 1297.57/s)  LR: 1.406e-04  Data: 0.010 (0.013)
Train: 458 [ 600/1251 ( 48%)]  Loss: 3.154 (3.11)  Time: 0.780s, 1312.15/s  (0.788s, 1298.79/s)  LR: 1.406e-04  Data: 0.009 (0.013)
Train: 458 [ 650/1251 ( 52%)]  Loss: 3.380 (3.12)  Time: 0.773s, 1323.94/s  (0.788s, 1298.81/s)  LR: 1.406e-04  Data: 0.010 (0.012)
Train: 458 [ 700/1251 ( 56%)]  Loss: 2.785 (3.10)  Time: 0.846s, 1209.84/s  (0.788s, 1299.26/s)  LR: 1.406e-04  Data: 0.009 (0.012)
Train: 458 [ 750/1251 ( 60%)]  Loss: 3.172 (3.11)  Time: 0.815s, 1255.82/s  (0.789s, 1298.12/s)  LR: 1.406e-04  Data: 0.011 (0.012)
Train: 458 [ 800/1251 ( 64%)]  Loss: 3.052 (3.10)  Time: 0.781s, 1310.87/s  (0.789s, 1298.33/s)  LR: 1.406e-04  Data: 0.010 (0.012)
Train: 458 [ 850/1251 ( 68%)]  Loss: 3.093 (3.10)  Time: 0.783s, 1307.22/s  (0.788s, 1299.09/s)  LR: 1.406e-04  Data: 0.009 (0.012)
Train: 458 [ 900/1251 ( 72%)]  Loss: 3.067 (3.10)  Time: 0.768s, 1332.54/s  (0.788s, 1299.56/s)  LR: 1.406e-04  Data: 0.009 (0.012)
Train: 458 [ 950/1251 ( 76%)]  Loss: 3.062 (3.10)  Time: 0.773s, 1325.03/s  (0.788s, 1300.07/s)  LR: 1.406e-04  Data: 0.009 (0.012)
Train: 458 [1000/1251 ( 80%)]  Loss: 3.079 (3.10)  Time: 0.772s, 1326.70/s  (0.787s, 1300.84/s)  LR: 1.406e-04  Data: 0.009 (0.011)
Train: 458 [1050/1251 ( 84%)]  Loss: 3.256 (3.11)  Time: 0.773s, 1325.37/s  (0.787s, 1301.21/s)  LR: 1.406e-04  Data: 0.009 (0.011)
Train: 458 [1100/1251 ( 88%)]  Loss: 2.754 (3.09)  Time: 0.838s, 1221.28/s  (0.787s, 1300.89/s)  LR: 1.406e-04  Data: 0.009 (0.011)
Train: 458 [1150/1251 ( 92%)]  Loss: 2.926 (3.08)  Time: 0.782s, 1308.76/s  (0.787s, 1301.22/s)  LR: 1.406e-04  Data: 0.012 (0.011)
Train: 458 [1200/1251 ( 96%)]  Loss: 3.095 (3.08)  Time: 0.777s, 1318.09/s  (0.787s, 1301.02/s)  LR: 1.406e-04  Data: 0.010 (0.011)
Train: 458 [1250/1251 (100%)]  Loss: 3.001 (3.08)  Time: 0.761s, 1346.45/s  (0.787s, 1301.30/s)  LR: 1.406e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.561 (1.561)  Loss:  0.5796 (0.5796)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.194 (0.566)  Loss:  0.7251 (1.0744)  Acc@1: 87.1462 (79.2260)  Acc@5: 97.5236 (94.8100)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-458.pth.tar', 79.22599997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-455.pth.tar', 79.10799987304688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-456.pth.tar', 79.04600015869141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-451.pth.tar', 78.99800008544922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-454.pth.tar', 78.90400003417969)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-457.pth.tar', 78.88200005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-453.pth.tar', 78.87400018310547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-450.pth.tar', 78.82999997558593)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-448.pth.tar', 78.81600000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-439.pth.tar', 78.80400002929687)

Train: 459 [   0/1251 (  0%)]  Loss: 2.847 (2.85)  Time: 2.506s,  408.66/s  (2.506s,  408.66/s)  LR: 1.389e-04  Data: 1.743 (1.743)
Train: 459 [  50/1251 (  4%)]  Loss: 2.845 (2.85)  Time: 0.775s, 1321.65/s  (0.855s, 1197.92/s)  LR: 1.389e-04  Data: 0.009 (0.047)
Train: 459 [ 100/1251 (  8%)]  Loss: 3.235 (2.98)  Time: 0.771s, 1327.31/s  (0.816s, 1255.16/s)  LR: 1.389e-04  Data: 0.010 (0.029)
Train: 459 [ 150/1251 ( 12%)]  Loss: 2.884 (2.95)  Time: 0.861s, 1188.96/s  (0.809s, 1265.00/s)  LR: 1.389e-04  Data: 0.010 (0.023)
Train: 459 [ 200/1251 ( 16%)]  Loss: 3.254 (3.01)  Time: 0.787s, 1301.84/s  (0.802s, 1277.33/s)  LR: 1.389e-04  Data: 0.010 (0.020)
Train: 459 [ 250/1251 ( 20%)]  Loss: 2.974 (3.01)  Time: 0.773s, 1324.59/s  (0.801s, 1278.45/s)  LR: 1.389e-04  Data: 0.010 (0.018)
Train: 459 [ 300/1251 ( 24%)]  Loss: 3.066 (3.01)  Time: 0.776s, 1319.98/s  (0.797s, 1284.15/s)  LR: 1.389e-04  Data: 0.010 (0.016)
Train: 459 [ 350/1251 ( 28%)]  Loss: 3.242 (3.04)  Time: 0.815s, 1257.05/s  (0.797s, 1285.22/s)  LR: 1.389e-04  Data: 0.009 (0.015)
Train: 459 [ 400/1251 ( 32%)]  Loss: 3.117 (3.05)  Time: 0.830s, 1233.40/s  (0.796s, 1285.93/s)  LR: 1.389e-04  Data: 0.009 (0.015)
Train: 459 [ 450/1251 ( 36%)]  Loss: 3.098 (3.06)  Time: 0.778s, 1316.87/s  (0.796s, 1285.78/s)  LR: 1.389e-04  Data: 0.009 (0.014)
Train: 459 [ 500/1251 ( 40%)]  Loss: 3.189 (3.07)  Time: 0.775s, 1321.72/s  (0.795s, 1287.55/s)  LR: 1.389e-04  Data: 0.009 (0.014)
Train: 459 [ 550/1251 ( 44%)]  Loss: 3.104 (3.07)  Time: 0.775s, 1322.14/s  (0.796s, 1286.95/s)  LR: 1.389e-04  Data: 0.010 (0.013)
Train: 459 [ 600/1251 ( 48%)]  Loss: 3.366 (3.09)  Time: 0.773s, 1325.18/s  (0.794s, 1289.37/s)  LR: 1.389e-04  Data: 0.010 (0.013)
Train: 459 [ 650/1251 ( 52%)]  Loss: 3.374 (3.11)  Time: 0.772s, 1325.83/s  (0.793s, 1291.53/s)  LR: 1.389e-04  Data: 0.009 (0.013)
Train: 459 [ 700/1251 ( 56%)]  Loss: 3.218 (3.12)  Time: 0.773s, 1324.00/s  (0.792s, 1292.48/s)  LR: 1.389e-04  Data: 0.010 (0.012)
Train: 459 [ 750/1251 ( 60%)]  Loss: 2.782 (3.10)  Time: 0.773s, 1325.35/s  (0.792s, 1292.90/s)  LR: 1.389e-04  Data: 0.009 (0.012)
Train: 459 [ 800/1251 ( 64%)]  Loss: 3.290 (3.11)  Time: 0.774s, 1323.80/s  (0.791s, 1294.40/s)  LR: 1.389e-04  Data: 0.009 (0.012)
Train: 459 [ 850/1251 ( 68%)]  Loss: 3.188 (3.12)  Time: 0.781s, 1311.63/s  (0.790s, 1295.68/s)  LR: 1.389e-04  Data: 0.010 (0.012)
Train: 459 [ 900/1251 ( 72%)]  Loss: 3.255 (3.12)  Time: 0.772s, 1325.66/s  (0.790s, 1296.63/s)  LR: 1.389e-04  Data: 0.010 (0.012)
Train: 459 [ 950/1251 ( 76%)]  Loss: 3.119 (3.12)  Time: 0.772s, 1326.21/s  (0.789s, 1297.60/s)  LR: 1.389e-04  Data: 0.009 (0.012)
Train: 459 [1000/1251 ( 80%)]  Loss: 3.248 (3.13)  Time: 0.780s, 1312.05/s  (0.789s, 1298.61/s)  LR: 1.389e-04  Data: 0.012 (0.012)
Train: 459 [1050/1251 ( 84%)]  Loss: 3.053 (3.12)  Time: 0.848s, 1207.53/s  (0.790s, 1296.98/s)  LR: 1.389e-04  Data: 0.010 (0.012)
Train: 459 [1100/1251 ( 88%)]  Loss: 3.279 (3.13)  Time: 0.780s, 1312.65/s  (0.789s, 1297.44/s)  LR: 1.389e-04  Data: 0.009 (0.012)
Train: 459 [1150/1251 ( 92%)]  Loss: 3.102 (3.13)  Time: 0.810s, 1263.50/s  (0.790s, 1296.52/s)  LR: 1.389e-04  Data: 0.009 (0.011)
Train: 459 [1200/1251 ( 96%)]  Loss: 2.983 (3.12)  Time: 0.774s, 1322.51/s  (0.790s, 1296.74/s)  LR: 1.389e-04  Data: 0.010 (0.011)
Train: 459 [1250/1251 (100%)]  Loss: 3.461 (3.14)  Time: 0.760s, 1347.98/s  (0.790s, 1296.91/s)  LR: 1.389e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.504 (1.504)  Loss:  0.5645 (0.5645)  Acc@1: 91.8945 (91.8945)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.194 (0.570)  Loss:  0.6650 (1.0706)  Acc@1: 87.2642 (79.0260)  Acc@5: 97.7594 (94.6740)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-458.pth.tar', 79.22599997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-455.pth.tar', 79.10799987304688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-456.pth.tar', 79.04600015869141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-459.pth.tar', 79.02600015625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-451.pth.tar', 78.99800008544922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-454.pth.tar', 78.90400003417969)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-457.pth.tar', 78.88200005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-453.pth.tar', 78.87400018310547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-450.pth.tar', 78.82999997558593)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-448.pth.tar', 78.81600000244141)

Train: 460 [   0/1251 (  0%)]  Loss: 3.240 (3.24)  Time: 2.344s,  436.81/s  (2.344s,  436.81/s)  LR: 1.371e-04  Data: 1.600 (1.600)
Train: 460 [  50/1251 (  4%)]  Loss: 3.039 (3.14)  Time: 0.773s, 1324.69/s  (0.828s, 1236.76/s)  LR: 1.371e-04  Data: 0.011 (0.044)
Train: 460 [ 100/1251 (  8%)]  Loss: 2.790 (3.02)  Time: 0.803s, 1275.36/s  (0.806s, 1270.10/s)  LR: 1.371e-04  Data: 0.011 (0.027)
Train: 460 [ 150/1251 ( 12%)]  Loss: 3.437 (3.13)  Time: 0.777s, 1318.68/s  (0.799s, 1280.97/s)  LR: 1.371e-04  Data: 0.009 (0.022)
Train: 460 [ 200/1251 ( 16%)]  Loss: 3.125 (3.13)  Time: 0.772s, 1327.20/s  (0.795s, 1288.33/s)  LR: 1.371e-04  Data: 0.010 (0.019)
Train: 460 [ 250/1251 ( 20%)]  Loss: 3.368 (3.17)  Time: 0.782s, 1309.66/s  (0.792s, 1293.13/s)  LR: 1.371e-04  Data: 0.010 (0.017)
Train: 460 [ 300/1251 ( 24%)]  Loss: 3.439 (3.21)  Time: 0.774s, 1323.54/s  (0.791s, 1293.99/s)  LR: 1.371e-04  Data: 0.010 (0.016)
Train: 460 [ 350/1251 ( 28%)]  Loss: 3.424 (3.23)  Time: 0.778s, 1315.39/s  (0.791s, 1294.94/s)  LR: 1.371e-04  Data: 0.009 (0.015)
Train: 460 [ 400/1251 ( 32%)]  Loss: 3.281 (3.24)  Time: 0.773s, 1324.44/s  (0.790s, 1295.43/s)  LR: 1.371e-04  Data: 0.010 (0.015)
Train: 460 [ 450/1251 ( 36%)]  Loss: 3.335 (3.25)  Time: 0.773s, 1325.48/s  (0.789s, 1297.64/s)  LR: 1.371e-04  Data: 0.010 (0.014)
Train: 460 [ 500/1251 ( 40%)]  Loss: 3.000 (3.23)  Time: 0.776s, 1319.64/s  (0.788s, 1299.54/s)  LR: 1.371e-04  Data: 0.010 (0.014)
Train: 460 [ 550/1251 ( 44%)]  Loss: 3.579 (3.25)  Time: 0.773s, 1323.96/s  (0.787s, 1301.36/s)  LR: 1.371e-04  Data: 0.010 (0.013)
Train: 460 [ 600/1251 ( 48%)]  Loss: 2.720 (3.21)  Time: 0.773s, 1324.14/s  (0.786s, 1302.79/s)  LR: 1.371e-04  Data: 0.010 (0.013)
Train: 460 [ 650/1251 ( 52%)]  Loss: 3.000 (3.20)  Time: 0.774s, 1323.25/s  (0.785s, 1304.08/s)  LR: 1.371e-04  Data: 0.011 (0.013)
Train: 460 [ 700/1251 ( 56%)]  Loss: 2.915 (3.18)  Time: 0.774s, 1323.12/s  (0.785s, 1304.94/s)  LR: 1.371e-04  Data: 0.010 (0.013)
Train: 460 [ 750/1251 ( 60%)]  Loss: 3.098 (3.17)  Time: 0.773s, 1324.94/s  (0.784s, 1306.00/s)  LR: 1.371e-04  Data: 0.011 (0.012)
Train: 460 [ 800/1251 ( 64%)]  Loss: 2.720 (3.15)  Time: 0.775s, 1321.83/s  (0.783s, 1307.10/s)  LR: 1.371e-04  Data: 0.010 (0.012)
Train: 460 [ 850/1251 ( 68%)]  Loss: 2.947 (3.14)  Time: 0.773s, 1324.41/s  (0.783s, 1307.42/s)  LR: 1.371e-04  Data: 0.010 (0.012)
Train: 460 [ 900/1251 ( 72%)]  Loss: 3.015 (3.13)  Time: 0.797s, 1284.98/s  (0.783s, 1307.86/s)  LR: 1.371e-04  Data: 0.010 (0.012)
Train: 460 [ 950/1251 ( 76%)]  Loss: 2.774 (3.11)  Time: 0.815s, 1257.06/s  (0.783s, 1308.48/s)  LR: 1.371e-04  Data: 0.010 (0.012)
Train: 460 [1000/1251 ( 80%)]  Loss: 2.895 (3.10)  Time: 0.774s, 1322.17/s  (0.782s, 1308.75/s)  LR: 1.371e-04  Data: 0.010 (0.012)
Train: 460 [1050/1251 ( 84%)]  Loss: 3.100 (3.10)  Time: 0.776s, 1319.90/s  (0.782s, 1308.71/s)  LR: 1.371e-04  Data: 0.010 (0.012)
Train: 460 [1100/1251 ( 88%)]  Loss: 3.218 (3.11)  Time: 0.773s, 1325.17/s  (0.782s, 1309.06/s)  LR: 1.371e-04  Data: 0.010 (0.012)
Train: 460 [1150/1251 ( 92%)]  Loss: 3.233 (3.11)  Time: 0.774s, 1323.44/s  (0.782s, 1309.19/s)  LR: 1.371e-04  Data: 0.010 (0.012)
Train: 460 [1200/1251 ( 96%)]  Loss: 2.813 (3.10)  Time: 0.774s, 1322.62/s  (0.782s, 1309.06/s)  LR: 1.371e-04  Data: 0.010 (0.012)
Train: 460 [1250/1251 (100%)]  Loss: 3.143 (3.10)  Time: 0.757s, 1353.53/s  (0.782s, 1309.40/s)  LR: 1.371e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.545 (1.545)  Loss:  0.7041 (0.7041)  Acc@1: 91.4062 (91.4062)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.194 (0.557)  Loss:  0.7920 (1.1573)  Acc@1: 86.6745 (78.8420)  Acc@5: 97.0519 (94.7220)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-458.pth.tar', 79.22599997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-455.pth.tar', 79.10799987304688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-456.pth.tar', 79.04600015869141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-459.pth.tar', 79.02600015625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-451.pth.tar', 78.99800008544922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-454.pth.tar', 78.90400003417969)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-457.pth.tar', 78.88200005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-453.pth.tar', 78.87400018310547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-460.pth.tar', 78.8420001586914)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-450.pth.tar', 78.82999997558593)

Train: 461 [   0/1251 (  0%)]  Loss: 3.290 (3.29)  Time: 2.262s,  452.62/s  (2.262s,  452.62/s)  LR: 1.354e-04  Data: 1.533 (1.533)
Train: 461 [  50/1251 (  4%)]  Loss: 3.112 (3.20)  Time: 0.773s, 1324.25/s  (0.819s, 1250.72/s)  LR: 1.354e-04  Data: 0.010 (0.045)
Train: 461 [ 100/1251 (  8%)]  Loss: 3.379 (3.26)  Time: 0.807s, 1268.78/s  (0.800s, 1279.99/s)  LR: 1.354e-04  Data: 0.010 (0.027)
Train: 461 [ 150/1251 ( 12%)]  Loss: 3.034 (3.20)  Time: 0.772s, 1326.34/s  (0.793s, 1290.83/s)  LR: 1.354e-04  Data: 0.010 (0.022)
Train: 461 [ 200/1251 ( 16%)]  Loss: 3.038 (3.17)  Time: 0.774s, 1323.03/s  (0.789s, 1297.03/s)  LR: 1.354e-04  Data: 0.010 (0.019)
Train: 461 [ 250/1251 ( 20%)]  Loss: 3.156 (3.17)  Time: 0.775s, 1321.01/s  (0.787s, 1301.25/s)  LR: 1.354e-04  Data: 0.010 (0.017)
Train: 461 [ 300/1251 ( 24%)]  Loss: 3.337 (3.19)  Time: 0.846s, 1210.14/s  (0.786s, 1303.47/s)  LR: 1.354e-04  Data: 0.010 (0.016)
Train: 461 [ 350/1251 ( 28%)]  Loss: 2.872 (3.15)  Time: 0.773s, 1324.59/s  (0.785s, 1304.94/s)  LR: 1.354e-04  Data: 0.010 (0.015)
Train: 461 [ 400/1251 ( 32%)]  Loss: 3.073 (3.14)  Time: 0.814s, 1257.91/s  (0.784s, 1306.64/s)  LR: 1.354e-04  Data: 0.010 (0.014)
Train: 461 [ 450/1251 ( 36%)]  Loss: 3.320 (3.16)  Time: 0.772s, 1326.38/s  (0.783s, 1307.37/s)  LR: 1.354e-04  Data: 0.010 (0.014)
Train: 461 [ 500/1251 ( 40%)]  Loss: 3.104 (3.16)  Time: 0.774s, 1323.26/s  (0.783s, 1308.22/s)  LR: 1.354e-04  Data: 0.010 (0.013)
Train: 461 [ 550/1251 ( 44%)]  Loss: 3.003 (3.14)  Time: 0.773s, 1323.93/s  (0.782s, 1309.26/s)  LR: 1.354e-04  Data: 0.010 (0.013)
Train: 461 [ 600/1251 ( 48%)]  Loss: 3.130 (3.14)  Time: 0.775s, 1321.65/s  (0.782s, 1309.67/s)  LR: 1.354e-04  Data: 0.010 (0.013)
Train: 461 [ 650/1251 ( 52%)]  Loss: 3.499 (3.17)  Time: 0.774s, 1323.20/s  (0.782s, 1309.81/s)  LR: 1.354e-04  Data: 0.010 (0.013)
Train: 461 [ 700/1251 ( 56%)]  Loss: 3.258 (3.17)  Time: 0.775s, 1322.12/s  (0.782s, 1309.37/s)  LR: 1.354e-04  Data: 0.010 (0.012)
Train: 461 [ 750/1251 ( 60%)]  Loss: 2.542 (3.13)  Time: 0.775s, 1321.53/s  (0.782s, 1309.70/s)  LR: 1.354e-04  Data: 0.010 (0.012)
Train: 461 [ 800/1251 ( 64%)]  Loss: 2.688 (3.11)  Time: 0.772s, 1326.47/s  (0.782s, 1309.84/s)  LR: 1.354e-04  Data: 0.010 (0.012)
Train: 461 [ 850/1251 ( 68%)]  Loss: 3.312 (3.12)  Time: 0.773s, 1325.00/s  (0.782s, 1310.25/s)  LR: 1.354e-04  Data: 0.010 (0.012)
Train: 461 [ 900/1251 ( 72%)]  Loss: 3.501 (3.14)  Time: 0.773s, 1324.73/s  (0.781s, 1310.60/s)  LR: 1.354e-04  Data: 0.010 (0.012)
Train: 461 [ 950/1251 ( 76%)]  Loss: 2.796 (3.12)  Time: 0.813s, 1259.82/s  (0.781s, 1311.12/s)  LR: 1.354e-04  Data: 0.010 (0.012)
Train: 461 [1000/1251 ( 80%)]  Loss: 3.320 (3.13)  Time: 0.817s, 1254.05/s  (0.781s, 1311.43/s)  LR: 1.354e-04  Data: 0.010 (0.012)
Train: 461 [1050/1251 ( 84%)]  Loss: 3.474 (3.15)  Time: 0.772s, 1326.63/s  (0.781s, 1311.19/s)  LR: 1.354e-04  Data: 0.010 (0.012)
Train: 461 [1100/1251 ( 88%)]  Loss: 3.113 (3.15)  Time: 0.811s, 1262.52/s  (0.781s, 1311.30/s)  LR: 1.354e-04  Data: 0.010 (0.012)
Train: 461 [1150/1251 ( 92%)]  Loss: 3.302 (3.15)  Time: 0.810s, 1263.88/s  (0.781s, 1311.35/s)  LR: 1.354e-04  Data: 0.010 (0.011)
Train: 461 [1200/1251 ( 96%)]  Loss: 2.729 (3.14)  Time: 0.857s, 1195.49/s  (0.781s, 1311.73/s)  LR: 1.354e-04  Data: 0.010 (0.011)
Train: 461 [1250/1251 (100%)]  Loss: 3.331 (3.14)  Time: 0.758s, 1350.20/s  (0.780s, 1312.06/s)  LR: 1.354e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.498 (1.498)  Loss:  0.6270 (0.6270)  Acc@1: 91.6992 (91.6992)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.194 (0.555)  Loss:  0.7163 (1.0946)  Acc@1: 87.5000 (79.2340)  Acc@5: 97.9953 (94.7960)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-461.pth.tar', 79.234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-458.pth.tar', 79.22599997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-455.pth.tar', 79.10799987304688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-456.pth.tar', 79.04600015869141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-459.pth.tar', 79.02600015625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-451.pth.tar', 78.99800008544922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-454.pth.tar', 78.90400003417969)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-457.pth.tar', 78.88200005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-453.pth.tar', 78.87400018310547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-460.pth.tar', 78.8420001586914)

Train: 462 [   0/1251 (  0%)]  Loss: 3.200 (3.20)  Time: 2.330s,  439.44/s  (2.330s,  439.44/s)  LR: 1.337e-04  Data: 1.604 (1.604)
Train: 462 [  50/1251 (  4%)]  Loss: 3.211 (3.21)  Time: 0.777s, 1318.56/s  (0.814s, 1257.85/s)  LR: 1.337e-04  Data: 0.010 (0.048)
Train: 462 [ 100/1251 (  8%)]  Loss: 3.378 (3.26)  Time: 0.775s, 1321.92/s  (0.797s, 1284.07/s)  LR: 1.337e-04  Data: 0.010 (0.029)
Train: 462 [ 150/1251 ( 12%)]  Loss: 3.101 (3.22)  Time: 0.778s, 1316.77/s  (0.792s, 1293.25/s)  LR: 1.337e-04  Data: 0.010 (0.023)
Train: 462 [ 200/1251 ( 16%)]  Loss: 3.201 (3.22)  Time: 0.773s, 1324.06/s  (0.789s, 1297.18/s)  LR: 1.337e-04  Data: 0.010 (0.020)
Train: 462 [ 250/1251 ( 20%)]  Loss: 3.151 (3.21)  Time: 0.779s, 1314.56/s  (0.788s, 1299.90/s)  LR: 1.337e-04  Data: 0.010 (0.018)
Train: 462 [ 300/1251 ( 24%)]  Loss: 2.861 (3.16)  Time: 0.773s, 1324.12/s  (0.786s, 1303.00/s)  LR: 1.337e-04  Data: 0.010 (0.016)
Train: 462 [ 350/1251 ( 28%)]  Loss: 2.873 (3.12)  Time: 0.774s, 1323.21/s  (0.785s, 1303.65/s)  LR: 1.337e-04  Data: 0.010 (0.015)
Train: 462 [ 400/1251 ( 32%)]  Loss: 2.920 (3.10)  Time: 0.795s, 1287.93/s  (0.785s, 1304.42/s)  LR: 1.337e-04  Data: 0.010 (0.015)
Train: 462 [ 450/1251 ( 36%)]  Loss: 3.091 (3.10)  Time: 0.774s, 1322.68/s  (0.784s, 1305.81/s)  LR: 1.337e-04  Data: 0.010 (0.014)
Train: 462 [ 500/1251 ( 40%)]  Loss: 2.967 (3.09)  Time: 0.774s, 1323.36/s  (0.784s, 1306.28/s)  LR: 1.337e-04  Data: 0.010 (0.014)
Train: 462 [ 550/1251 ( 44%)]  Loss: 3.127 (3.09)  Time: 0.775s, 1321.61/s  (0.784s, 1306.90/s)  LR: 1.337e-04  Data: 0.010 (0.013)
Train: 462 [ 600/1251 ( 48%)]  Loss: 2.950 (3.08)  Time: 0.775s, 1322.14/s  (0.783s, 1307.95/s)  LR: 1.337e-04  Data: 0.010 (0.013)
Train: 462 [ 650/1251 ( 52%)]  Loss: 2.936 (3.07)  Time: 0.775s, 1320.80/s  (0.783s, 1308.36/s)  LR: 1.337e-04  Data: 0.010 (0.013)
Train: 462 [ 700/1251 ( 56%)]  Loss: 3.431 (3.09)  Time: 0.774s, 1323.75/s  (0.782s, 1308.75/s)  LR: 1.337e-04  Data: 0.010 (0.013)
Train: 462 [ 750/1251 ( 60%)]  Loss: 2.944 (3.08)  Time: 0.775s, 1322.03/s  (0.782s, 1308.89/s)  LR: 1.337e-04  Data: 0.010 (0.013)
Train: 462 [ 800/1251 ( 64%)]  Loss: 3.399 (3.10)  Time: 0.775s, 1321.69/s  (0.782s, 1309.01/s)  LR: 1.337e-04  Data: 0.010 (0.012)
Train: 462 [ 850/1251 ( 68%)]  Loss: 3.205 (3.11)  Time: 0.796s, 1285.85/s  (0.782s, 1309.26/s)  LR: 1.337e-04  Data: 0.010 (0.012)
Train: 462 [ 900/1251 ( 72%)]  Loss: 3.221 (3.11)  Time: 0.777s, 1318.16/s  (0.782s, 1309.62/s)  LR: 1.337e-04  Data: 0.015 (0.012)
Train: 462 [ 950/1251 ( 76%)]  Loss: 2.921 (3.10)  Time: 0.815s, 1256.35/s  (0.782s, 1309.47/s)  LR: 1.337e-04  Data: 0.010 (0.012)
Train: 462 [1000/1251 ( 80%)]  Loss: 3.271 (3.11)  Time: 0.776s, 1319.03/s  (0.782s, 1309.78/s)  LR: 1.337e-04  Data: 0.010 (0.012)
Train: 462 [1050/1251 ( 84%)]  Loss: 2.975 (3.11)  Time: 0.773s, 1325.40/s  (0.782s, 1310.14/s)  LR: 1.337e-04  Data: 0.010 (0.012)
Train: 462 [1100/1251 ( 88%)]  Loss: 3.052 (3.10)  Time: 0.773s, 1324.39/s  (0.781s, 1310.54/s)  LR: 1.337e-04  Data: 0.010 (0.012)
Train: 462 [1150/1251 ( 92%)]  Loss: 3.144 (3.11)  Time: 0.774s, 1322.91/s  (0.781s, 1310.72/s)  LR: 1.337e-04  Data: 0.010 (0.012)
Train: 462 [1200/1251 ( 96%)]  Loss: 2.919 (3.10)  Time: 0.808s, 1266.67/s  (0.781s, 1310.79/s)  LR: 1.337e-04  Data: 0.010 (0.012)
Train: 462 [1250/1251 (100%)]  Loss: 3.142 (3.10)  Time: 0.757s, 1352.73/s  (0.781s, 1310.83/s)  LR: 1.337e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.536 (1.536)  Loss:  0.6548 (0.6548)  Acc@1: 91.8945 (91.8945)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.194 (0.560)  Loss:  0.7793 (1.1019)  Acc@1: 85.6132 (79.1660)  Acc@5: 97.5236 (94.7880)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-461.pth.tar', 79.234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-458.pth.tar', 79.22599997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-462.pth.tar', 79.16599995605469)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-455.pth.tar', 79.10799987304688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-456.pth.tar', 79.04600015869141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-459.pth.tar', 79.02600015625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-451.pth.tar', 78.99800008544922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-454.pth.tar', 78.90400003417969)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-457.pth.tar', 78.88200005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-453.pth.tar', 78.87400018310547)

Train: 463 [   0/1251 (  0%)]  Loss: 3.407 (3.41)  Time: 2.341s,  437.38/s  (2.341s,  437.38/s)  LR: 1.320e-04  Data: 1.609 (1.609)
Train: 463 [  50/1251 (  4%)]  Loss: 2.970 (3.19)  Time: 0.812s, 1260.93/s  (0.809s, 1265.73/s)  LR: 1.320e-04  Data: 0.010 (0.044)
Train: 463 [ 100/1251 (  8%)]  Loss: 2.889 (3.09)  Time: 0.810s, 1263.49/s  (0.795s, 1288.32/s)  LR: 1.320e-04  Data: 0.010 (0.027)
Train: 463 [ 150/1251 ( 12%)]  Loss: 3.330 (3.15)  Time: 0.772s, 1326.79/s  (0.789s, 1297.09/s)  LR: 1.320e-04  Data: 0.010 (0.022)
Train: 463 [ 200/1251 ( 16%)]  Loss: 3.338 (3.19)  Time: 0.772s, 1326.78/s  (0.787s, 1301.05/s)  LR: 1.320e-04  Data: 0.010 (0.019)
Train: 463 [ 250/1251 ( 20%)]  Loss: 3.263 (3.20)  Time: 0.816s, 1255.28/s  (0.785s, 1305.20/s)  LR: 1.320e-04  Data: 0.010 (0.017)
Train: 463 [ 300/1251 ( 24%)]  Loss: 3.185 (3.20)  Time: 0.773s, 1325.28/s  (0.784s, 1306.64/s)  LR: 1.320e-04  Data: 0.010 (0.016)
Train: 463 [ 350/1251 ( 28%)]  Loss: 3.164 (3.19)  Time: 0.775s, 1321.77/s  (0.783s, 1307.24/s)  LR: 1.320e-04  Data: 0.010 (0.015)
Train: 463 [ 400/1251 ( 32%)]  Loss: 3.145 (3.19)  Time: 0.811s, 1262.13/s  (0.783s, 1308.42/s)  LR: 1.320e-04  Data: 0.010 (0.014)
Train: 463 [ 450/1251 ( 36%)]  Loss: 3.136 (3.18)  Time: 0.773s, 1325.23/s  (0.782s, 1308.84/s)  LR: 1.320e-04  Data: 0.010 (0.014)
Train: 463 [ 500/1251 ( 40%)]  Loss: 2.922 (3.16)  Time: 0.776s, 1320.06/s  (0.782s, 1309.35/s)  LR: 1.320e-04  Data: 0.010 (0.014)
Train: 463 [ 550/1251 ( 44%)]  Loss: 3.507 (3.19)  Time: 0.775s, 1321.47/s  (0.782s, 1309.03/s)  LR: 1.320e-04  Data: 0.010 (0.013)
Train: 463 [ 600/1251 ( 48%)]  Loss: 3.456 (3.21)  Time: 0.773s, 1324.34/s  (0.782s, 1309.54/s)  LR: 1.320e-04  Data: 0.010 (0.013)
Train: 463 [ 650/1251 ( 52%)]  Loss: 2.976 (3.19)  Time: 0.812s, 1261.20/s  (0.782s, 1309.85/s)  LR: 1.320e-04  Data: 0.010 (0.013)
Train: 463 [ 700/1251 ( 56%)]  Loss: 3.181 (3.19)  Time: 0.772s, 1326.28/s  (0.781s, 1310.41/s)  LR: 1.320e-04  Data: 0.010 (0.013)
Train: 463 [ 750/1251 ( 60%)]  Loss: 3.281 (3.20)  Time: 0.775s, 1321.47/s  (0.781s, 1310.95/s)  LR: 1.320e-04  Data: 0.010 (0.012)
Train: 463 [ 800/1251 ( 64%)]  Loss: 2.948 (3.18)  Time: 0.810s, 1264.11/s  (0.781s, 1310.76/s)  LR: 1.320e-04  Data: 0.010 (0.012)
Train: 463 [ 850/1251 ( 68%)]  Loss: 3.146 (3.18)  Time: 0.810s, 1264.57/s  (0.781s, 1311.03/s)  LR: 1.320e-04  Data: 0.010 (0.012)
Train: 463 [ 900/1251 ( 72%)]  Loss: 3.181 (3.18)  Time: 0.773s, 1324.83/s  (0.781s, 1311.27/s)  LR: 1.320e-04  Data: 0.009 (0.012)
Train: 463 [ 950/1251 ( 76%)]  Loss: 3.290 (3.19)  Time: 0.772s, 1326.78/s  (0.781s, 1311.70/s)  LR: 1.320e-04  Data: 0.010 (0.012)
Train: 463 [1000/1251 ( 80%)]  Loss: 3.212 (3.19)  Time: 0.771s, 1327.91/s  (0.781s, 1311.92/s)  LR: 1.320e-04  Data: 0.010 (0.012)
Train: 463 [1050/1251 ( 84%)]  Loss: 3.121 (3.18)  Time: 0.772s, 1326.62/s  (0.781s, 1311.98/s)  LR: 1.320e-04  Data: 0.010 (0.012)
Train: 463 [1100/1251 ( 88%)]  Loss: 2.893 (3.17)  Time: 0.773s, 1325.54/s  (0.780s, 1312.13/s)  LR: 1.320e-04  Data: 0.010 (0.012)
Train: 463 [1150/1251 ( 92%)]  Loss: 3.050 (3.17)  Time: 0.772s, 1326.97/s  (0.780s, 1312.39/s)  LR: 1.320e-04  Data: 0.010 (0.011)
Train: 463 [1200/1251 ( 96%)]  Loss: 3.382 (3.18)  Time: 0.773s, 1325.13/s  (0.780s, 1312.40/s)  LR: 1.320e-04  Data: 0.010 (0.011)
Train: 463 [1250/1251 (100%)]  Loss: 2.708 (3.16)  Time: 0.757s, 1352.98/s  (0.780s, 1312.65/s)  LR: 1.320e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.494 (1.494)  Loss:  0.6963 (0.6963)  Acc@1: 91.2109 (91.2109)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.193 (0.553)  Loss:  0.8301 (1.1686)  Acc@1: 86.4387 (79.1100)  Acc@5: 97.4057 (94.8000)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-461.pth.tar', 79.234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-458.pth.tar', 79.22599997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-462.pth.tar', 79.16599995605469)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-463.pth.tar', 79.11000005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-455.pth.tar', 79.10799987304688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-456.pth.tar', 79.04600015869141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-459.pth.tar', 79.02600015625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-451.pth.tar', 78.99800008544922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-454.pth.tar', 78.90400003417969)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-457.pth.tar', 78.88200005615235)

Train: 464 [   0/1251 (  0%)]  Loss: 3.045 (3.05)  Time: 2.269s,  451.21/s  (2.269s,  451.21/s)  LR: 1.303e-04  Data: 1.540 (1.540)
Train: 464 [  50/1251 (  4%)]  Loss: 2.940 (2.99)  Time: 0.776s, 1320.05/s  (0.809s, 1265.85/s)  LR: 1.303e-04  Data: 0.010 (0.042)
Train: 464 [ 100/1251 (  8%)]  Loss: 2.999 (2.99)  Time: 0.774s, 1323.62/s  (0.796s, 1287.06/s)  LR: 1.303e-04  Data: 0.010 (0.026)
Train: 464 [ 150/1251 ( 12%)]  Loss: 3.190 (3.04)  Time: 0.775s, 1320.58/s  (0.790s, 1295.85/s)  LR: 1.303e-04  Data: 0.010 (0.021)
Train: 464 [ 200/1251 ( 16%)]  Loss: 3.400 (3.11)  Time: 0.776s, 1320.27/s  (0.787s, 1301.08/s)  LR: 1.303e-04  Data: 0.010 (0.018)
Train: 464 [ 250/1251 ( 20%)]  Loss: 3.569 (3.19)  Time: 0.810s, 1264.00/s  (0.785s, 1304.57/s)  LR: 1.303e-04  Data: 0.010 (0.017)
Train: 464 [ 300/1251 ( 24%)]  Loss: 3.177 (3.19)  Time: 0.777s, 1317.41/s  (0.785s, 1304.52/s)  LR: 1.303e-04  Data: 0.010 (0.015)
Train: 464 [ 350/1251 ( 28%)]  Loss: 3.082 (3.18)  Time: 0.775s, 1320.45/s  (0.785s, 1304.83/s)  LR: 1.303e-04  Data: 0.010 (0.015)
Train: 464 [ 400/1251 ( 32%)]  Loss: 3.220 (3.18)  Time: 0.774s, 1323.67/s  (0.785s, 1305.00/s)  LR: 1.303e-04  Data: 0.010 (0.014)
Train: 464 [ 450/1251 ( 36%)]  Loss: 3.383 (3.20)  Time: 0.799s, 1280.96/s  (0.785s, 1304.64/s)  LR: 1.303e-04  Data: 0.010 (0.014)
Train: 464 [ 500/1251 ( 40%)]  Loss: 2.989 (3.18)  Time: 0.773s, 1324.20/s  (0.784s, 1305.62/s)  LR: 1.303e-04  Data: 0.010 (0.013)
Train: 464 [ 550/1251 ( 44%)]  Loss: 3.042 (3.17)  Time: 0.776s, 1319.30/s  (0.784s, 1306.09/s)  LR: 1.303e-04  Data: 0.010 (0.013)
Train: 464 [ 600/1251 ( 48%)]  Loss: 3.315 (3.18)  Time: 0.777s, 1317.68/s  (0.784s, 1306.93/s)  LR: 1.303e-04  Data: 0.010 (0.013)
Train: 464 [ 650/1251 ( 52%)]  Loss: 3.265 (3.19)  Time: 0.772s, 1326.55/s  (0.783s, 1307.79/s)  LR: 1.303e-04  Data: 0.010 (0.012)
Train: 464 [ 700/1251 ( 56%)]  Loss: 2.941 (3.17)  Time: 0.813s, 1260.12/s  (0.783s, 1307.24/s)  LR: 1.303e-04  Data: 0.013 (0.012)
Train: 464 [ 750/1251 ( 60%)]  Loss: 3.203 (3.17)  Time: 0.810s, 1264.52/s  (0.783s, 1307.64/s)  LR: 1.303e-04  Data: 0.010 (0.012)
Train: 464 [ 800/1251 ( 64%)]  Loss: 3.324 (3.18)  Time: 0.774s, 1323.45/s  (0.783s, 1308.20/s)  LR: 1.303e-04  Data: 0.010 (0.012)
Train: 464 [ 850/1251 ( 68%)]  Loss: 3.368 (3.19)  Time: 0.773s, 1325.47/s  (0.782s, 1308.88/s)  LR: 1.303e-04  Data: 0.010 (0.012)
Train: 464 [ 900/1251 ( 72%)]  Loss: 3.362 (3.20)  Time: 0.774s, 1322.49/s  (0.782s, 1309.29/s)  LR: 1.303e-04  Data: 0.010 (0.012)
Train: 464 [ 950/1251 ( 76%)]  Loss: 2.648 (3.17)  Time: 0.774s, 1323.54/s  (0.782s, 1309.94/s)  LR: 1.303e-04  Data: 0.010 (0.012)
Train: 464 [1000/1251 ( 80%)]  Loss: 3.053 (3.17)  Time: 0.773s, 1324.83/s  (0.781s, 1310.38/s)  LR: 1.303e-04  Data: 0.009 (0.012)
Train: 464 [1050/1251 ( 84%)]  Loss: 2.819 (3.15)  Time: 0.810s, 1264.21/s  (0.782s, 1310.15/s)  LR: 1.303e-04  Data: 0.010 (0.011)
Train: 464 [1100/1251 ( 88%)]  Loss: 3.040 (3.15)  Time: 0.775s, 1320.71/s  (0.781s, 1310.51/s)  LR: 1.303e-04  Data: 0.009 (0.011)
Train: 464 [1150/1251 ( 92%)]  Loss: 3.061 (3.14)  Time: 0.774s, 1323.15/s  (0.781s, 1311.09/s)  LR: 1.303e-04  Data: 0.010 (0.011)
Train: 464 [1200/1251 ( 96%)]  Loss: 3.201 (3.15)  Time: 0.776s, 1319.60/s  (0.781s, 1311.07/s)  LR: 1.303e-04  Data: 0.010 (0.011)
Train: 464 [1250/1251 (100%)]  Loss: 2.947 (3.14)  Time: 0.758s, 1351.50/s  (0.781s, 1311.19/s)  LR: 1.303e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.542 (1.542)  Loss:  0.6328 (0.6328)  Acc@1: 91.3086 (91.3086)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.194 (0.559)  Loss:  0.6953 (1.0740)  Acc@1: 86.9104 (79.4520)  Acc@5: 97.4057 (94.6820)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-464.pth.tar', 79.45200000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-461.pth.tar', 79.234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-458.pth.tar', 79.22599997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-462.pth.tar', 79.16599995605469)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-463.pth.tar', 79.11000005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-455.pth.tar', 79.10799987304688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-456.pth.tar', 79.04600015869141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-459.pth.tar', 79.02600015625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-451.pth.tar', 78.99800008544922)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-454.pth.tar', 78.90400003417969)

Train: 465 [   0/1251 (  0%)]  Loss: 2.976 (2.98)  Time: 2.193s,  467.04/s  (2.193s,  467.04/s)  LR: 1.286e-04  Data: 1.478 (1.478)
Train: 465 [  50/1251 (  4%)]  Loss: 2.807 (2.89)  Time: 0.773s, 1325.05/s  (0.817s, 1253.02/s)  LR: 1.286e-04  Data: 0.010 (0.048)
Train: 465 [ 100/1251 (  8%)]  Loss: 2.935 (2.91)  Time: 0.772s, 1325.70/s  (0.797s, 1284.34/s)  LR: 1.286e-04  Data: 0.010 (0.029)
Train: 465 [ 150/1251 ( 12%)]  Loss: 2.879 (2.90)  Time: 0.774s, 1322.17/s  (0.792s, 1292.95/s)  LR: 1.286e-04  Data: 0.010 (0.023)
Train: 465 [ 200/1251 ( 16%)]  Loss: 3.375 (2.99)  Time: 0.773s, 1325.52/s  (0.789s, 1297.55/s)  LR: 1.286e-04  Data: 0.010 (0.020)
Train: 465 [ 250/1251 ( 20%)]  Loss: 3.290 (3.04)  Time: 0.772s, 1326.05/s  (0.787s, 1301.05/s)  LR: 1.286e-04  Data: 0.010 (0.018)
Train: 465 [ 300/1251 ( 24%)]  Loss: 3.175 (3.06)  Time: 0.776s, 1319.11/s  (0.785s, 1304.10/s)  LR: 1.286e-04  Data: 0.010 (0.016)
Train: 465 [ 350/1251 ( 28%)]  Loss: 3.232 (3.08)  Time: 0.771s, 1327.97/s  (0.785s, 1305.23/s)  LR: 1.286e-04  Data: 0.010 (0.015)
Train: 465 [ 400/1251 ( 32%)]  Loss: 3.276 (3.10)  Time: 0.773s, 1324.37/s  (0.784s, 1306.58/s)  LR: 1.286e-04  Data: 0.009 (0.015)
Train: 465 [ 450/1251 ( 36%)]  Loss: 2.989 (3.09)  Time: 0.772s, 1326.72/s  (0.783s, 1307.27/s)  LR: 1.286e-04  Data: 0.010 (0.014)
Train: 465 [ 500/1251 ( 40%)]  Loss: 3.196 (3.10)  Time: 0.778s, 1315.50/s  (0.783s, 1307.57/s)  LR: 1.286e-04  Data: 0.010 (0.014)
Train: 465 [ 550/1251 ( 44%)]  Loss: 2.997 (3.09)  Time: 0.774s, 1323.82/s  (0.782s, 1308.69/s)  LR: 1.286e-04  Data: 0.010 (0.013)
Train: 465 [ 600/1251 ( 48%)]  Loss: 3.093 (3.09)  Time: 0.773s, 1325.51/s  (0.782s, 1308.88/s)  LR: 1.286e-04  Data: 0.010 (0.013)
Train: 465 [ 650/1251 ( 52%)]  Loss: 2.861 (3.08)  Time: 0.812s, 1261.52/s  (0.782s, 1309.16/s)  LR: 1.286e-04  Data: 0.010 (0.013)
Train: 465 [ 700/1251 ( 56%)]  Loss: 3.326 (3.09)  Time: 0.773s, 1325.33/s  (0.782s, 1309.09/s)  LR: 1.286e-04  Data: 0.010 (0.013)
Train: 465 [ 750/1251 ( 60%)]  Loss: 2.674 (3.07)  Time: 0.774s, 1322.61/s  (0.782s, 1309.23/s)  LR: 1.286e-04  Data: 0.010 (0.012)
Train: 465 [ 800/1251 ( 64%)]  Loss: 3.178 (3.07)  Time: 0.775s, 1321.40/s  (0.782s, 1310.02/s)  LR: 1.286e-04  Data: 0.010 (0.012)
Train: 465 [ 850/1251 ( 68%)]  Loss: 3.034 (3.07)  Time: 0.771s, 1327.29/s  (0.782s, 1310.10/s)  LR: 1.286e-04  Data: 0.010 (0.012)
Train: 465 [ 900/1251 ( 72%)]  Loss: 3.410 (3.09)  Time: 0.813s, 1259.49/s  (0.782s, 1309.55/s)  LR: 1.286e-04  Data: 0.010 (0.012)
Train: 465 [ 950/1251 ( 76%)]  Loss: 3.059 (3.09)  Time: 0.775s, 1321.65/s  (0.782s, 1309.20/s)  LR: 1.286e-04  Data: 0.010 (0.012)
Train: 465 [1000/1251 ( 80%)]  Loss: 3.205 (3.09)  Time: 0.772s, 1325.83/s  (0.782s, 1309.46/s)  LR: 1.286e-04  Data: 0.009 (0.012)
Train: 465 [1050/1251 ( 84%)]  Loss: 2.825 (3.08)  Time: 0.774s, 1323.36/s  (0.782s, 1309.45/s)  LR: 1.286e-04  Data: 0.010 (0.012)
Train: 465 [1100/1251 ( 88%)]  Loss: 3.326 (3.09)  Time: 0.816s, 1255.51/s  (0.782s, 1309.08/s)  LR: 1.286e-04  Data: 0.010 (0.012)
Train: 465 [1150/1251 ( 92%)]  Loss: 3.019 (3.09)  Time: 0.775s, 1321.20/s  (0.782s, 1309.39/s)  LR: 1.286e-04  Data: 0.010 (0.012)
Train: 465 [1200/1251 ( 96%)]  Loss: 3.036 (3.09)  Time: 0.812s, 1261.24/s  (0.782s, 1309.57/s)  LR: 1.286e-04  Data: 0.010 (0.011)
Train: 465 [1250/1251 (100%)]  Loss: 3.473 (3.10)  Time: 0.756s, 1354.37/s  (0.782s, 1309.84/s)  LR: 1.286e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.500 (1.500)  Loss:  0.6455 (0.6455)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.193 (0.553)  Loss:  0.7227 (1.0990)  Acc@1: 86.5566 (79.2920)  Acc@5: 97.9953 (94.8240)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-464.pth.tar', 79.45200000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-465.pth.tar', 79.29199997802735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-461.pth.tar', 79.234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-458.pth.tar', 79.22599997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-462.pth.tar', 79.16599995605469)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-463.pth.tar', 79.11000005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-455.pth.tar', 79.10799987304688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-456.pth.tar', 79.04600015869141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-459.pth.tar', 79.02600015625)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-451.pth.tar', 78.99800008544922)

Train: 466 [   0/1251 (  0%)]  Loss: 3.056 (3.06)  Time: 2.131s,  480.43/s  (2.131s,  480.43/s)  LR: 1.269e-04  Data: 1.417 (1.417)
Train: 466 [  50/1251 (  4%)]  Loss: 2.651 (2.85)  Time: 0.775s, 1321.02/s  (0.809s, 1266.46/s)  LR: 1.269e-04  Data: 0.010 (0.042)
Train: 466 [ 100/1251 (  8%)]  Loss: 2.843 (2.85)  Time: 0.774s, 1322.66/s  (0.791s, 1293.83/s)  LR: 1.269e-04  Data: 0.010 (0.026)
Train: 466 [ 150/1251 ( 12%)]  Loss: 3.142 (2.92)  Time: 0.774s, 1323.59/s  (0.788s, 1299.96/s)  LR: 1.269e-04  Data: 0.009 (0.021)
Train: 466 [ 200/1251 ( 16%)]  Loss: 3.357 (3.01)  Time: 0.775s, 1322.06/s  (0.787s, 1300.35/s)  LR: 1.269e-04  Data: 0.010 (0.018)
Train: 466 [ 250/1251 ( 20%)]  Loss: 3.033 (3.01)  Time: 0.775s, 1321.95/s  (0.786s, 1302.79/s)  LR: 1.269e-04  Data: 0.010 (0.016)
Train: 466 [ 300/1251 ( 24%)]  Loss: 3.251 (3.05)  Time: 0.773s, 1324.12/s  (0.785s, 1304.55/s)  LR: 1.269e-04  Data: 0.011 (0.015)
Train: 466 [ 350/1251 ( 28%)]  Loss: 2.955 (3.04)  Time: 0.772s, 1326.33/s  (0.784s, 1306.55/s)  LR: 1.269e-04  Data: 0.010 (0.014)
Train: 466 [ 400/1251 ( 32%)]  Loss: 3.305 (3.07)  Time: 0.773s, 1324.72/s  (0.783s, 1307.24/s)  LR: 1.269e-04  Data: 0.010 (0.014)
Train: 466 [ 450/1251 ( 36%)]  Loss: 3.280 (3.09)  Time: 0.773s, 1325.09/s  (0.783s, 1307.14/s)  LR: 1.269e-04  Data: 0.010 (0.013)
Train: 466 [ 500/1251 ( 40%)]  Loss: 3.208 (3.10)  Time: 0.775s, 1320.94/s  (0.783s, 1307.76/s)  LR: 1.269e-04  Data: 0.010 (0.013)
Train: 466 [ 550/1251 ( 44%)]  Loss: 3.212 (3.11)  Time: 0.776s, 1319.06/s  (0.783s, 1307.07/s)  LR: 1.269e-04  Data: 0.009 (0.013)
Train: 466 [ 600/1251 ( 48%)]  Loss: 3.338 (3.13)  Time: 0.773s, 1324.63/s  (0.783s, 1307.44/s)  LR: 1.269e-04  Data: 0.010 (0.013)
Train: 466 [ 650/1251 ( 52%)]  Loss: 3.084 (3.12)  Time: 0.774s, 1322.50/s  (0.783s, 1307.91/s)  LR: 1.269e-04  Data: 0.010 (0.012)
Train: 466 [ 700/1251 ( 56%)]  Loss: 3.170 (3.13)  Time: 0.810s, 1264.88/s  (0.782s, 1308.72/s)  LR: 1.269e-04  Data: 0.010 (0.012)
Train: 466 [ 750/1251 ( 60%)]  Loss: 2.665 (3.10)  Time: 0.811s, 1263.05/s  (0.782s, 1308.76/s)  LR: 1.269e-04  Data: 0.010 (0.012)
Train: 466 [ 800/1251 ( 64%)]  Loss: 3.087 (3.10)  Time: 0.808s, 1268.08/s  (0.782s, 1309.26/s)  LR: 1.269e-04  Data: 0.010 (0.012)
Train: 466 [ 850/1251 ( 68%)]  Loss: 3.016 (3.09)  Time: 0.774s, 1322.70/s  (0.782s, 1309.45/s)  LR: 1.269e-04  Data: 0.010 (0.012)
Train: 466 [ 900/1251 ( 72%)]  Loss: 2.997 (3.09)  Time: 0.775s, 1321.99/s  (0.782s, 1309.45/s)  LR: 1.269e-04  Data: 0.010 (0.012)
Train: 466 [ 950/1251 ( 76%)]  Loss: 3.088 (3.09)  Time: 0.776s, 1319.15/s  (0.782s, 1309.40/s)  LR: 1.269e-04  Data: 0.010 (0.012)
Train: 466 [1000/1251 ( 80%)]  Loss: 3.035 (3.08)  Time: 0.811s, 1262.20/s  (0.782s, 1309.67/s)  LR: 1.269e-04  Data: 0.009 (0.012)
Train: 466 [1050/1251 ( 84%)]  Loss: 3.207 (3.09)  Time: 0.776s, 1319.86/s  (0.782s, 1309.89/s)  LR: 1.269e-04  Data: 0.010 (0.011)
Train: 466 [1100/1251 ( 88%)]  Loss: 3.292 (3.10)  Time: 0.774s, 1322.19/s  (0.782s, 1310.21/s)  LR: 1.269e-04  Data: 0.010 (0.011)
Train: 466 [1150/1251 ( 92%)]  Loss: 2.874 (3.09)  Time: 0.797s, 1285.00/s  (0.781s, 1310.42/s)  LR: 1.269e-04  Data: 0.011 (0.011)
Train: 466 [1200/1251 ( 96%)]  Loss: 3.272 (3.10)  Time: 0.774s, 1323.28/s  (0.781s, 1310.64/s)  LR: 1.269e-04  Data: 0.010 (0.011)
Train: 466 [1250/1251 (100%)]  Loss: 2.677 (3.08)  Time: 0.757s, 1353.14/s  (0.781s, 1310.64/s)  LR: 1.269e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.490 (1.490)  Loss:  0.6445 (0.6445)  Acc@1: 91.6016 (91.6016)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.194 (0.557)  Loss:  0.7344 (1.1076)  Acc@1: 87.0283 (79.2600)  Acc@5: 97.4057 (94.7640)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-464.pth.tar', 79.45200000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-465.pth.tar', 79.29199997802735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-466.pth.tar', 79.2599999243164)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-461.pth.tar', 79.234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-458.pth.tar', 79.22599997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-462.pth.tar', 79.16599995605469)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-463.pth.tar', 79.11000005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-455.pth.tar', 79.10799987304688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-456.pth.tar', 79.04600015869141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-459.pth.tar', 79.02600015625)

Train: 467 [   0/1251 (  0%)]  Loss: 3.004 (3.00)  Time: 2.608s,  392.61/s  (2.608s,  392.61/s)  LR: 1.253e-04  Data: 1.879 (1.879)
Train: 467 [  50/1251 (  4%)]  Loss: 3.033 (3.02)  Time: 0.772s, 1326.30/s  (0.820s, 1249.17/s)  LR: 1.253e-04  Data: 0.010 (0.047)
Train: 467 [ 100/1251 (  8%)]  Loss: 3.185 (3.07)  Time: 0.807s, 1269.40/s  (0.806s, 1270.62/s)  LR: 1.253e-04  Data: 0.010 (0.028)
Train: 467 [ 150/1251 ( 12%)]  Loss: 3.003 (3.06)  Time: 0.775s, 1321.02/s  (0.800s, 1280.60/s)  LR: 1.253e-04  Data: 0.010 (0.022)
Train: 467 [ 200/1251 ( 16%)]  Loss: 3.458 (3.14)  Time: 0.772s, 1326.43/s  (0.798s, 1283.26/s)  LR: 1.253e-04  Data: 0.010 (0.019)
Train: 467 [ 250/1251 ( 20%)]  Loss: 3.359 (3.17)  Time: 0.775s, 1320.68/s  (0.797s, 1284.76/s)  LR: 1.253e-04  Data: 0.010 (0.017)
Train: 467 [ 300/1251 ( 24%)]  Loss: 3.249 (3.18)  Time: 0.772s, 1326.91/s  (0.796s, 1287.07/s)  LR: 1.253e-04  Data: 0.010 (0.016)
Train: 467 [ 350/1251 ( 28%)]  Loss: 3.364 (3.21)  Time: 0.773s, 1324.41/s  (0.794s, 1289.28/s)  LR: 1.253e-04  Data: 0.010 (0.015)
Train: 467 [ 400/1251 ( 32%)]  Loss: 3.008 (3.18)  Time: 0.772s, 1326.13/s  (0.794s, 1290.35/s)  LR: 1.253e-04  Data: 0.010 (0.015)
Train: 467 [ 450/1251 ( 36%)]  Loss: 2.674 (3.13)  Time: 0.772s, 1326.92/s  (0.793s, 1291.29/s)  LR: 1.253e-04  Data: 0.010 (0.014)
Train: 467 [ 500/1251 ( 40%)]  Loss: 3.124 (3.13)  Time: 0.772s, 1327.04/s  (0.792s, 1293.01/s)  LR: 1.253e-04  Data: 0.010 (0.014)
Train: 467 [ 550/1251 ( 44%)]  Loss: 3.093 (3.13)  Time: 0.772s, 1326.59/s  (0.791s, 1294.89/s)  LR: 1.253e-04  Data: 0.010 (0.013)
Train: 467 [ 600/1251 ( 48%)]  Loss: 3.413 (3.15)  Time: 0.771s, 1328.98/s  (0.790s, 1297.01/s)  LR: 1.253e-04  Data: 0.010 (0.013)
Train: 467 [ 650/1251 ( 52%)]  Loss: 3.164 (3.15)  Time: 0.770s, 1330.41/s  (0.788s, 1298.85/s)  LR: 1.253e-04  Data: 0.010 (0.013)
Train: 467 [ 700/1251 ( 56%)]  Loss: 3.099 (3.15)  Time: 0.775s, 1321.64/s  (0.788s, 1300.10/s)  LR: 1.253e-04  Data: 0.010 (0.013)
Train: 467 [ 750/1251 ( 60%)]  Loss: 2.972 (3.14)  Time: 0.846s, 1210.80/s  (0.788s, 1299.42/s)  LR: 1.253e-04  Data: 0.010 (0.012)
Train: 467 [ 800/1251 ( 64%)]  Loss: 3.285 (3.15)  Time: 0.807s, 1269.29/s  (0.788s, 1299.80/s)  LR: 1.253e-04  Data: 0.010 (0.012)
Train: 467 [ 850/1251 ( 68%)]  Loss: 2.879 (3.13)  Time: 0.775s, 1321.79/s  (0.787s, 1300.53/s)  LR: 1.253e-04  Data: 0.010 (0.012)
Train: 467 [ 900/1251 ( 72%)]  Loss: 3.100 (3.13)  Time: 0.811s, 1263.32/s  (0.788s, 1299.66/s)  LR: 1.253e-04  Data: 0.010 (0.012)
Train: 467 [ 950/1251 ( 76%)]  Loss: 3.157 (3.13)  Time: 0.776s, 1318.78/s  (0.788s, 1299.06/s)  LR: 1.253e-04  Data: 0.010 (0.012)
Train: 467 [1000/1251 ( 80%)]  Loss: 3.158 (3.13)  Time: 0.773s, 1324.65/s  (0.788s, 1299.41/s)  LR: 1.253e-04  Data: 0.010 (0.012)
Train: 467 [1050/1251 ( 84%)]  Loss: 3.212 (3.14)  Time: 0.807s, 1268.66/s  (0.788s, 1299.56/s)  LR: 1.253e-04  Data: 0.010 (0.012)
Train: 467 [1100/1251 ( 88%)]  Loss: 2.846 (3.12)  Time: 0.775s, 1322.09/s  (0.787s, 1300.36/s)  LR: 1.253e-04  Data: 0.010 (0.012)
Train: 467 [1150/1251 ( 92%)]  Loss: 3.139 (3.12)  Time: 0.772s, 1327.01/s  (0.787s, 1300.32/s)  LR: 1.253e-04  Data: 0.010 (0.012)
Train: 467 [1200/1251 ( 96%)]  Loss: 2.863 (3.11)  Time: 0.772s, 1326.34/s  (0.787s, 1300.99/s)  LR: 1.253e-04  Data: 0.010 (0.011)
Train: 467 [1250/1251 (100%)]  Loss: 2.793 (3.10)  Time: 0.790s, 1296.33/s  (0.787s, 1300.69/s)  LR: 1.253e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.498 (1.498)  Loss:  0.6758 (0.6758)  Acc@1: 91.6992 (91.6992)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.194 (0.555)  Loss:  0.7622 (1.1108)  Acc@1: 86.6745 (79.5140)  Acc@5: 97.4057 (94.8920)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-467.pth.tar', 79.51399989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-464.pth.tar', 79.45200000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-465.pth.tar', 79.29199997802735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-466.pth.tar', 79.2599999243164)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-461.pth.tar', 79.234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-458.pth.tar', 79.22599997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-462.pth.tar', 79.16599995605469)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-463.pth.tar', 79.11000005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-455.pth.tar', 79.10799987304688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-456.pth.tar', 79.04600015869141)

Train: 468 [   0/1251 (  0%)]  Loss: 3.039 (3.04)  Time: 2.161s,  473.86/s  (2.161s,  473.86/s)  LR: 1.236e-04  Data: 1.447 (1.447)
Train: 468 [  50/1251 (  4%)]  Loss: 3.343 (3.19)  Time: 0.772s, 1325.91/s  (0.814s, 1258.69/s)  LR: 1.236e-04  Data: 0.010 (0.039)
Train: 468 [ 100/1251 (  8%)]  Loss: 2.786 (3.06)  Time: 0.773s, 1324.86/s  (0.802s, 1277.38/s)  LR: 1.236e-04  Data: 0.009 (0.024)
Train: 468 [ 150/1251 ( 12%)]  Loss: 3.084 (3.06)  Time: 0.806s, 1271.24/s  (0.796s, 1286.89/s)  LR: 1.236e-04  Data: 0.010 (0.020)
Train: 468 [ 200/1251 ( 16%)]  Loss: 2.976 (3.05)  Time: 0.771s, 1328.76/s  (0.792s, 1292.13/s)  LR: 1.236e-04  Data: 0.009 (0.017)
Train: 468 [ 250/1251 ( 20%)]  Loss: 3.079 (3.05)  Time: 0.773s, 1324.39/s  (0.791s, 1293.76/s)  LR: 1.236e-04  Data: 0.010 (0.016)
Train: 468 [ 300/1251 ( 24%)]  Loss: 3.051 (3.05)  Time: 0.778s, 1316.39/s  (0.791s, 1294.01/s)  LR: 1.236e-04  Data: 0.009 (0.015)
Train: 468 [ 350/1251 ( 28%)]  Loss: 3.181 (3.07)  Time: 0.771s, 1327.38/s  (0.792s, 1293.57/s)  LR: 1.236e-04  Data: 0.010 (0.014)
Train: 468 [ 400/1251 ( 32%)]  Loss: 3.003 (3.06)  Time: 0.807s, 1268.59/s  (0.791s, 1294.38/s)  LR: 1.236e-04  Data: 0.010 (0.013)
Train: 468 [ 450/1251 ( 36%)]  Loss: 3.345 (3.09)  Time: 0.773s, 1324.94/s  (0.789s, 1297.18/s)  LR: 1.236e-04  Data: 0.009 (0.013)
Train: 468 [ 500/1251 ( 40%)]  Loss: 3.041 (3.08)  Time: 0.773s, 1324.61/s  (0.788s, 1299.70/s)  LR: 1.236e-04  Data: 0.010 (0.013)
Train: 468 [ 550/1251 ( 44%)]  Loss: 3.122 (3.09)  Time: 0.772s, 1326.74/s  (0.788s, 1299.87/s)  LR: 1.236e-04  Data: 0.010 (0.012)
Train: 468 [ 600/1251 ( 48%)]  Loss: 2.910 (3.07)  Time: 0.771s, 1327.38/s  (0.788s, 1298.93/s)  LR: 1.236e-04  Data: 0.010 (0.012)
Train: 468 [ 650/1251 ( 52%)]  Loss: 3.166 (3.08)  Time: 0.806s, 1271.25/s  (0.789s, 1298.02/s)  LR: 1.236e-04  Data: 0.010 (0.012)
Train: 468 [ 700/1251 ( 56%)]  Loss: 2.794 (3.06)  Time: 0.808s, 1267.03/s  (0.789s, 1298.22/s)  LR: 1.236e-04  Data: 0.010 (0.012)
Train: 468 [ 750/1251 ( 60%)]  Loss: 3.056 (3.06)  Time: 0.773s, 1324.55/s  (0.788s, 1299.37/s)  LR: 1.236e-04  Data: 0.010 (0.012)
Train: 468 [ 800/1251 ( 64%)]  Loss: 2.949 (3.05)  Time: 0.774s, 1323.70/s  (0.788s, 1299.38/s)  LR: 1.236e-04  Data: 0.010 (0.012)
Train: 468 [ 850/1251 ( 68%)]  Loss: 3.081 (3.06)  Time: 0.806s, 1269.86/s  (0.789s, 1298.18/s)  LR: 1.236e-04  Data: 0.010 (0.012)
Train: 468 [ 900/1251 ( 72%)]  Loss: 2.597 (3.03)  Time: 0.773s, 1325.31/s  (0.789s, 1297.62/s)  LR: 1.236e-04  Data: 0.010 (0.011)
Train: 468 [ 950/1251 ( 76%)]  Loss: 3.038 (3.03)  Time: 0.808s, 1267.01/s  (0.789s, 1297.30/s)  LR: 1.236e-04  Data: 0.010 (0.011)
Train: 468 [1000/1251 ( 80%)]  Loss: 3.298 (3.04)  Time: 0.774s, 1322.32/s  (0.789s, 1297.37/s)  LR: 1.236e-04  Data: 0.010 (0.011)
Train: 468 [1050/1251 ( 84%)]  Loss: 2.983 (3.04)  Time: 0.809s, 1265.24/s  (0.790s, 1296.26/s)  LR: 1.236e-04  Data: 0.010 (0.011)
Train: 468 [1100/1251 ( 88%)]  Loss: 3.345 (3.06)  Time: 0.807s, 1269.63/s  (0.790s, 1295.98/s)  LR: 1.236e-04  Data: 0.010 (0.011)
Train: 468 [1150/1251 ( 92%)]  Loss: 3.040 (3.05)  Time: 0.805s, 1271.38/s  (0.790s, 1296.34/s)  LR: 1.236e-04  Data: 0.010 (0.011)
Train: 468 [1200/1251 ( 96%)]  Loss: 3.151 (3.06)  Time: 0.773s, 1325.02/s  (0.790s, 1295.43/s)  LR: 1.236e-04  Data: 0.010 (0.011)
Train: 468 [1250/1251 (100%)]  Loss: 3.022 (3.06)  Time: 0.792s, 1292.37/s  (0.791s, 1295.22/s)  LR: 1.236e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.520 (1.520)  Loss:  0.5986 (0.5986)  Acc@1: 91.5039 (91.5039)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.194 (0.562)  Loss:  0.7012 (1.0531)  Acc@1: 86.3208 (79.3260)  Acc@5: 97.8774 (94.7460)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-467.pth.tar', 79.51399989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-464.pth.tar', 79.45200000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-468.pth.tar', 79.32600013427735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-465.pth.tar', 79.29199997802735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-466.pth.tar', 79.2599999243164)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-461.pth.tar', 79.234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-458.pth.tar', 79.22599997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-462.pth.tar', 79.16599995605469)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-463.pth.tar', 79.11000005615234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-455.pth.tar', 79.10799987304688)

Train: 469 [   0/1251 (  0%)]  Loss: 2.872 (2.87)  Time: 2.415s,  424.02/s  (2.415s,  424.02/s)  LR: 1.219e-04  Data: 1.684 (1.684)
Train: 469 [  50/1251 (  4%)]  Loss: 3.110 (2.99)  Time: 0.808s, 1268.07/s  (0.841s, 1217.53/s)  LR: 1.219e-04  Data: 0.010 (0.044)
Train: 469 [ 100/1251 (  8%)]  Loss: 2.961 (2.98)  Time: 0.808s, 1267.91/s  (0.816s, 1254.28/s)  LR: 1.219e-04  Data: 0.010 (0.027)
Train: 469 [ 150/1251 ( 12%)]  Loss: 2.800 (2.94)  Time: 0.807s, 1268.92/s  (0.812s, 1260.41/s)  LR: 1.219e-04  Data: 0.010 (0.021)
Train: 469 [ 200/1251 ( 16%)]  Loss: 3.022 (2.95)  Time: 0.808s, 1267.30/s  (0.806s, 1269.83/s)  LR: 1.219e-04  Data: 0.010 (0.019)
Train: 469 [ 250/1251 ( 20%)]  Loss: 3.342 (3.02)  Time: 0.808s, 1267.98/s  (0.804s, 1273.53/s)  LR: 1.219e-04  Data: 0.010 (0.017)
Train: 469 [ 300/1251 ( 24%)]  Loss: 2.771 (2.98)  Time: 0.773s, 1324.31/s  (0.799s, 1281.28/s)  LR: 1.219e-04  Data: 0.010 (0.016)
Train: 469 [ 350/1251 ( 28%)]  Loss: 2.729 (2.95)  Time: 0.776s, 1319.00/s  (0.796s, 1286.11/s)  LR: 1.219e-04  Data: 0.010 (0.015)
Train: 469 [ 400/1251 ( 32%)]  Loss: 3.239 (2.98)  Time: 0.771s, 1327.85/s  (0.794s, 1290.05/s)  LR: 1.219e-04  Data: 0.010 (0.014)
Train: 469 [ 450/1251 ( 36%)]  Loss: 3.156 (3.00)  Time: 0.771s, 1328.28/s  (0.792s, 1292.36/s)  LR: 1.219e-04  Data: 0.010 (0.014)
Train: 469 [ 500/1251 ( 40%)]  Loss: 2.966 (3.00)  Time: 0.808s, 1267.68/s  (0.792s, 1293.70/s)  LR: 1.219e-04  Data: 0.010 (0.013)
Train: 469 [ 550/1251 ( 44%)]  Loss: 3.296 (3.02)  Time: 0.773s, 1324.88/s  (0.790s, 1295.62/s)  LR: 1.219e-04  Data: 0.010 (0.013)
Train: 469 [ 600/1251 ( 48%)]  Loss: 3.359 (3.05)  Time: 0.774s, 1323.16/s  (0.791s, 1294.73/s)  LR: 1.219e-04  Data: 0.010 (0.013)
Train: 469 [ 650/1251 ( 52%)]  Loss: 3.097 (3.05)  Time: 0.771s, 1327.43/s  (0.790s, 1296.08/s)  LR: 1.219e-04  Data: 0.010 (0.013)
Train: 469 [ 700/1251 ( 56%)]  Loss: 3.319 (3.07)  Time: 0.774s, 1322.35/s  (0.789s, 1297.60/s)  LR: 1.219e-04  Data: 0.010 (0.012)
Train: 469 [ 750/1251 ( 60%)]  Loss: 2.984 (3.06)  Time: 0.811s, 1262.93/s  (0.789s, 1298.10/s)  LR: 1.219e-04  Data: 0.010 (0.012)
Train: 469 [ 800/1251 ( 64%)]  Loss: 3.194 (3.07)  Time: 0.772s, 1326.82/s  (0.789s, 1298.64/s)  LR: 1.219e-04  Data: 0.010 (0.012)
Train: 469 [ 850/1251 ( 68%)]  Loss: 3.303 (3.08)  Time: 0.774s, 1322.50/s  (0.789s, 1298.26/s)  LR: 1.219e-04  Data: 0.010 (0.012)
Train: 469 [ 900/1251 ( 72%)]  Loss: 3.100 (3.09)  Time: 0.771s, 1327.59/s  (0.788s, 1298.81/s)  LR: 1.219e-04  Data: 0.010 (0.012)
Train: 469 [ 950/1251 ( 76%)]  Loss: 3.426 (3.10)  Time: 0.807s, 1268.24/s  (0.788s, 1298.95/s)  LR: 1.219e-04  Data: 0.010 (0.012)
Train: 469 [1000/1251 ( 80%)]  Loss: 2.876 (3.09)  Time: 0.772s, 1327.27/s  (0.788s, 1299.37/s)  LR: 1.219e-04  Data: 0.010 (0.012)
Train: 469 [1050/1251 ( 84%)]  Loss: 2.987 (3.09)  Time: 0.808s, 1267.88/s  (0.788s, 1299.22/s)  LR: 1.219e-04  Data: 0.010 (0.012)
Train: 469 [1100/1251 ( 88%)]  Loss: 3.206 (3.09)  Time: 0.811s, 1262.48/s  (0.788s, 1299.05/s)  LR: 1.219e-04  Data: 0.010 (0.012)
Train: 469 [1150/1251 ( 92%)]  Loss: 3.060 (3.09)  Time: 0.772s, 1326.40/s  (0.788s, 1298.88/s)  LR: 1.219e-04  Data: 0.010 (0.011)
Train: 469 [1200/1251 ( 96%)]  Loss: 3.046 (3.09)  Time: 0.860s, 1191.08/s  (0.788s, 1299.51/s)  LR: 1.219e-04  Data: 0.010 (0.011)
Train: 469 [1250/1251 (100%)]  Loss: 3.391 (3.10)  Time: 0.756s, 1355.24/s  (0.788s, 1299.82/s)  LR: 1.219e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.549 (1.549)  Loss:  0.6963 (0.6963)  Acc@1: 91.4062 (91.4062)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.194 (0.557)  Loss:  0.7871 (1.1412)  Acc@1: 86.3208 (79.3320)  Acc@5: 97.6415 (94.8200)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-467.pth.tar', 79.51399989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-464.pth.tar', 79.45200000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-469.pth.tar', 79.33200013427734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-468.pth.tar', 79.32600013427735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-465.pth.tar', 79.29199997802735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-466.pth.tar', 79.2599999243164)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-461.pth.tar', 79.234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-458.pth.tar', 79.22599997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-462.pth.tar', 79.16599995605469)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-463.pth.tar', 79.11000005615234)

Train: 470 [   0/1251 (  0%)]  Loss: 3.156 (3.16)  Time: 2.231s,  458.93/s  (2.231s,  458.93/s)  LR: 1.203e-04  Data: 1.503 (1.503)
Train: 470 [  50/1251 (  4%)]  Loss: 3.316 (3.24)  Time: 0.775s, 1320.77/s  (0.824s, 1242.66/s)  LR: 1.203e-04  Data: 0.010 (0.045)
Train: 470 [ 100/1251 (  8%)]  Loss: 3.195 (3.22)  Time: 0.807s, 1269.29/s  (0.811s, 1262.63/s)  LR: 1.203e-04  Data: 0.010 (0.028)
Train: 470 [ 150/1251 ( 12%)]  Loss: 3.334 (3.25)  Time: 0.807s, 1268.85/s  (0.808s, 1267.89/s)  LR: 1.203e-04  Data: 0.010 (0.022)
Train: 470 [ 200/1251 ( 16%)]  Loss: 3.375 (3.28)  Time: 0.807s, 1269.66/s  (0.804s, 1273.97/s)  LR: 1.203e-04  Data: 0.010 (0.019)
Train: 470 [ 250/1251 ( 20%)]  Loss: 2.817 (3.20)  Time: 0.806s, 1269.74/s  (0.800s, 1279.76/s)  LR: 1.203e-04  Data: 0.010 (0.017)
Train: 470 [ 300/1251 ( 24%)]  Loss: 2.749 (3.13)  Time: 0.807s, 1269.39/s  (0.796s, 1285.63/s)  LR: 1.203e-04  Data: 0.010 (0.016)
Train: 470 [ 350/1251 ( 28%)]  Loss: 3.071 (3.13)  Time: 0.773s, 1324.86/s  (0.796s, 1287.16/s)  LR: 1.203e-04  Data: 0.010 (0.015)
Train: 470 [ 400/1251 ( 32%)]  Loss: 3.292 (3.14)  Time: 0.772s, 1326.52/s  (0.795s, 1288.29/s)  LR: 1.203e-04  Data: 0.009 (0.014)
Train: 470 [ 450/1251 ( 36%)]  Loss: 3.003 (3.13)  Time: 0.807s, 1268.92/s  (0.794s, 1289.64/s)  LR: 1.203e-04  Data: 0.010 (0.014)
Train: 470 [ 500/1251 ( 40%)]  Loss: 3.029 (3.12)  Time: 0.772s, 1326.59/s  (0.794s, 1290.25/s)  LR: 1.203e-04  Data: 0.010 (0.013)
Train: 470 [ 550/1251 ( 44%)]  Loss: 2.949 (3.11)  Time: 0.809s, 1265.55/s  (0.793s, 1290.95/s)  LR: 1.203e-04  Data: 0.010 (0.013)
Train: 470 [ 600/1251 ( 48%)]  Loss: 3.480 (3.14)  Time: 0.772s, 1326.74/s  (0.792s, 1292.15/s)  LR: 1.203e-04  Data: 0.010 (0.013)
Train: 470 [ 650/1251 ( 52%)]  Loss: 3.216 (3.14)  Time: 0.774s, 1322.29/s  (0.793s, 1291.65/s)  LR: 1.203e-04  Data: 0.009 (0.013)
Train: 470 [ 700/1251 ( 56%)]  Loss: 3.159 (3.14)  Time: 0.806s, 1269.70/s  (0.793s, 1291.52/s)  LR: 1.203e-04  Data: 0.010 (0.012)
Train: 470 [ 750/1251 ( 60%)]  Loss: 2.835 (3.12)  Time: 0.773s, 1325.39/s  (0.793s, 1291.83/s)  LR: 1.203e-04  Data: 0.010 (0.012)
Train: 470 [ 800/1251 ( 64%)]  Loss: 3.431 (3.14)  Time: 0.809s, 1265.39/s  (0.793s, 1291.54/s)  LR: 1.203e-04  Data: 0.010 (0.012)
Train: 470 [ 850/1251 ( 68%)]  Loss: 3.092 (3.14)  Time: 0.774s, 1322.18/s  (0.793s, 1292.08/s)  LR: 1.203e-04  Data: 0.010 (0.012)
Train: 470 [ 900/1251 ( 72%)]  Loss: 3.324 (3.15)  Time: 0.809s, 1266.11/s  (0.792s, 1292.59/s)  LR: 1.203e-04  Data: 0.010 (0.012)
Train: 470 [ 950/1251 ( 76%)]  Loss: 3.257 (3.15)  Time: 0.772s, 1325.59/s  (0.792s, 1292.40/s)  LR: 1.203e-04  Data: 0.010 (0.012)
Train: 470 [1000/1251 ( 80%)]  Loss: 3.157 (3.15)  Time: 0.810s, 1264.05/s  (0.792s, 1292.93/s)  LR: 1.203e-04  Data: 0.010 (0.012)
Train: 470 [1050/1251 ( 84%)]  Loss: 2.913 (3.14)  Time: 0.811s, 1262.02/s  (0.792s, 1292.30/s)  LR: 1.203e-04  Data: 0.010 (0.012)
Train: 470 [1100/1251 ( 88%)]  Loss: 3.297 (3.15)  Time: 0.769s, 1331.11/s  (0.792s, 1292.91/s)  LR: 1.203e-04  Data: 0.010 (0.011)
Train: 470 [1150/1251 ( 92%)]  Loss: 3.303 (3.16)  Time: 0.808s, 1268.05/s  (0.792s, 1292.79/s)  LR: 1.203e-04  Data: 0.010 (0.011)
Train: 470 [1200/1251 ( 96%)]  Loss: 2.878 (3.15)  Time: 0.772s, 1325.83/s  (0.792s, 1293.18/s)  LR: 1.203e-04  Data: 0.010 (0.011)
Train: 470 [1250/1251 (100%)]  Loss: 2.905 (3.14)  Time: 0.758s, 1351.71/s  (0.792s, 1292.84/s)  LR: 1.203e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.612 (1.612)  Loss:  0.6436 (0.6436)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.194 (0.557)  Loss:  0.7075 (1.0837)  Acc@1: 86.6745 (79.6180)  Acc@5: 98.3491 (94.8800)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-470.pth.tar', 79.61799989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-467.pth.tar', 79.51399989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-464.pth.tar', 79.45200000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-469.pth.tar', 79.33200013427734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-468.pth.tar', 79.32600013427735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-465.pth.tar', 79.29199997802735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-466.pth.tar', 79.2599999243164)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-461.pth.tar', 79.234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-458.pth.tar', 79.22599997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-462.pth.tar', 79.16599995605469)

Train: 471 [   0/1251 (  0%)]  Loss: 2.850 (2.85)  Time: 2.364s,  433.22/s  (2.364s,  433.22/s)  LR: 1.187e-04  Data: 1.631 (1.631)
Train: 471 [  50/1251 (  4%)]  Loss: 2.841 (2.85)  Time: 0.802s, 1277.22/s  (0.819s, 1250.28/s)  LR: 1.187e-04  Data: 0.010 (0.048)
Train: 471 [ 100/1251 (  8%)]  Loss: 3.110 (2.93)  Time: 0.776s, 1319.25/s  (0.807s, 1269.42/s)  LR: 1.187e-04  Data: 0.010 (0.029)
Train: 471 [ 150/1251 ( 12%)]  Loss: 2.884 (2.92)  Time: 0.813s, 1259.46/s  (0.804s, 1273.76/s)  LR: 1.187e-04  Data: 0.010 (0.023)
Train: 471 [ 200/1251 ( 16%)]  Loss: 3.049 (2.95)  Time: 0.775s, 1321.81/s  (0.803s, 1274.98/s)  LR: 1.187e-04  Data: 0.010 (0.020)
Train: 471 [ 250/1251 ( 20%)]  Loss: 3.060 (2.97)  Time: 0.772s, 1327.19/s  (0.799s, 1281.70/s)  LR: 1.187e-04  Data: 0.010 (0.018)
Train: 471 [ 300/1251 ( 24%)]  Loss: 3.096 (2.98)  Time: 0.808s, 1267.96/s  (0.796s, 1285.64/s)  LR: 1.187e-04  Data: 0.010 (0.016)
Train: 471 [ 350/1251 ( 28%)]  Loss: 3.336 (3.03)  Time: 0.772s, 1327.27/s  (0.794s, 1289.84/s)  LR: 1.187e-04  Data: 0.010 (0.015)
Train: 471 [ 400/1251 ( 32%)]  Loss: 2.977 (3.02)  Time: 0.774s, 1322.53/s  (0.792s, 1293.65/s)  LR: 1.187e-04  Data: 0.009 (0.015)
Train: 471 [ 450/1251 ( 36%)]  Loss: 2.888 (3.01)  Time: 0.812s, 1261.71/s  (0.791s, 1293.77/s)  LR: 1.187e-04  Data: 0.010 (0.014)
Train: 471 [ 500/1251 ( 40%)]  Loss: 2.979 (3.01)  Time: 0.773s, 1325.37/s  (0.793s, 1291.86/s)  LR: 1.187e-04  Data: 0.010 (0.014)
Train: 471 [ 550/1251 ( 44%)]  Loss: 3.094 (3.01)  Time: 0.809s, 1265.79/s  (0.793s, 1291.76/s)  LR: 1.187e-04  Data: 0.010 (0.013)
Train: 471 [ 600/1251 ( 48%)]  Loss: 3.408 (3.04)  Time: 0.774s, 1322.82/s  (0.792s, 1292.36/s)  LR: 1.187e-04  Data: 0.010 (0.013)
Train: 471 [ 650/1251 ( 52%)]  Loss: 3.092 (3.05)  Time: 0.774s, 1322.21/s  (0.793s, 1292.04/s)  LR: 1.187e-04  Data: 0.010 (0.013)
Train: 471 [ 700/1251 ( 56%)]  Loss: 2.876 (3.04)  Time: 0.772s, 1326.49/s  (0.792s, 1292.27/s)  LR: 1.187e-04  Data: 0.010 (0.013)
Train: 471 [ 750/1251 ( 60%)]  Loss: 3.224 (3.05)  Time: 0.771s, 1328.28/s  (0.792s, 1292.46/s)  LR: 1.187e-04  Data: 0.010 (0.012)
Train: 471 [ 800/1251 ( 64%)]  Loss: 2.965 (3.04)  Time: 0.775s, 1322.00/s  (0.792s, 1293.33/s)  LR: 1.187e-04  Data: 0.010 (0.012)
Train: 471 [ 850/1251 ( 68%)]  Loss: 3.063 (3.04)  Time: 0.810s, 1264.81/s  (0.791s, 1294.21/s)  LR: 1.187e-04  Data: 0.010 (0.012)
Train: 471 [ 900/1251 ( 72%)]  Loss: 2.862 (3.03)  Time: 0.774s, 1322.33/s  (0.791s, 1294.77/s)  LR: 1.187e-04  Data: 0.010 (0.012)
Train: 471 [ 950/1251 ( 76%)]  Loss: 2.930 (3.03)  Time: 0.780s, 1312.80/s  (0.791s, 1294.77/s)  LR: 1.187e-04  Data: 0.012 (0.012)
Train: 471 [1000/1251 ( 80%)]  Loss: 2.972 (3.03)  Time: 0.775s, 1321.20/s  (0.790s, 1295.62/s)  LR: 1.187e-04  Data: 0.010 (0.012)
Train: 471 [1050/1251 ( 84%)]  Loss: 3.241 (3.04)  Time: 0.807s, 1268.62/s  (0.791s, 1295.22/s)  LR: 1.187e-04  Data: 0.010 (0.012)
Train: 471 [1100/1251 ( 88%)]  Loss: 2.884 (3.03)  Time: 0.772s, 1326.32/s  (0.790s, 1295.73/s)  LR: 1.187e-04  Data: 0.010 (0.012)
Train: 471 [1150/1251 ( 92%)]  Loss: 3.389 (3.04)  Time: 0.775s, 1321.33/s  (0.790s, 1296.08/s)  LR: 1.187e-04  Data: 0.010 (0.012)
Train: 471 [1200/1251 ( 96%)]  Loss: 3.154 (3.05)  Time: 0.808s, 1267.87/s  (0.790s, 1295.45/s)  LR: 1.187e-04  Data: 0.010 (0.011)
Train: 471 [1250/1251 (100%)]  Loss: 2.813 (3.04)  Time: 0.757s, 1353.58/s  (0.790s, 1295.90/s)  LR: 1.187e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.519 (1.519)  Loss:  0.5425 (0.5425)  Acc@1: 91.3086 (91.3086)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.194 (0.552)  Loss:  0.6523 (1.0030)  Acc@1: 86.7924 (79.6820)  Acc@5: 97.6415 (94.9200)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-471.pth.tar', 79.68199995117187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-470.pth.tar', 79.61799989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-467.pth.tar', 79.51399989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-464.pth.tar', 79.45200000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-469.pth.tar', 79.33200013427734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-468.pth.tar', 79.32600013427735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-465.pth.tar', 79.29199997802735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-466.pth.tar', 79.2599999243164)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-461.pth.tar', 79.234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-458.pth.tar', 79.22599997558594)

Train: 472 [   0/1251 (  0%)]  Loss: 2.976 (2.98)  Time: 2.239s,  457.29/s  (2.239s,  457.29/s)  LR: 1.171e-04  Data: 1.526 (1.526)
Train: 472 [  50/1251 (  4%)]  Loss: 2.911 (2.94)  Time: 0.772s, 1325.96/s  (0.823s, 1244.47/s)  LR: 1.171e-04  Data: 0.010 (0.045)
Train: 472 [ 100/1251 (  8%)]  Loss: 2.891 (2.93)  Time: 0.801s, 1278.63/s  (0.805s, 1272.23/s)  LR: 1.171e-04  Data: 0.010 (0.027)
Train: 472 [ 150/1251 ( 12%)]  Loss: 3.092 (2.97)  Time: 0.772s, 1327.19/s  (0.802s, 1276.10/s)  LR: 1.171e-04  Data: 0.010 (0.022)
Train: 472 [ 200/1251 ( 16%)]  Loss: 3.125 (3.00)  Time: 0.773s, 1324.48/s  (0.795s, 1287.93/s)  LR: 1.171e-04  Data: 0.010 (0.019)
Train: 472 [ 250/1251 ( 20%)]  Loss: 2.919 (2.99)  Time: 0.808s, 1267.94/s  (0.793s, 1291.87/s)  LR: 1.171e-04  Data: 0.010 (0.017)
Train: 472 [ 300/1251 ( 24%)]  Loss: 2.898 (2.97)  Time: 0.772s, 1326.33/s  (0.791s, 1294.96/s)  LR: 1.171e-04  Data: 0.010 (0.016)
Train: 472 [ 350/1251 ( 28%)]  Loss: 3.385 (3.02)  Time: 0.774s, 1323.64/s  (0.793s, 1291.69/s)  LR: 1.171e-04  Data: 0.010 (0.015)
Train: 472 [ 400/1251 ( 32%)]  Loss: 3.150 (3.04)  Time: 0.771s, 1328.39/s  (0.791s, 1294.49/s)  LR: 1.171e-04  Data: 0.010 (0.014)
Train: 472 [ 450/1251 ( 36%)]  Loss: 3.147 (3.05)  Time: 0.772s, 1326.97/s  (0.789s, 1297.26/s)  LR: 1.171e-04  Data: 0.010 (0.014)
Train: 472 [ 500/1251 ( 40%)]  Loss: 3.013 (3.05)  Time: 0.771s, 1328.34/s  (0.788s, 1298.71/s)  LR: 1.171e-04  Data: 0.010 (0.014)
Train: 472 [ 550/1251 ( 44%)]  Loss: 3.211 (3.06)  Time: 0.772s, 1326.99/s  (0.788s, 1299.70/s)  LR: 1.171e-04  Data: 0.010 (0.013)
Train: 472 [ 600/1251 ( 48%)]  Loss: 3.100 (3.06)  Time: 0.772s, 1326.68/s  (0.787s, 1301.49/s)  LR: 1.171e-04  Data: 0.010 (0.013)
Train: 472 [ 650/1251 ( 52%)]  Loss: 3.384 (3.09)  Time: 0.807s, 1268.88/s  (0.787s, 1301.06/s)  LR: 1.171e-04  Data: 0.010 (0.013)
Train: 472 [ 700/1251 ( 56%)]  Loss: 3.159 (3.09)  Time: 0.806s, 1270.16/s  (0.787s, 1300.84/s)  LR: 1.171e-04  Data: 0.010 (0.012)
Train: 472 [ 750/1251 ( 60%)]  Loss: 3.217 (3.10)  Time: 0.809s, 1265.66/s  (0.788s, 1300.02/s)  LR: 1.171e-04  Data: 0.010 (0.012)
Train: 472 [ 800/1251 ( 64%)]  Loss: 2.774 (3.08)  Time: 0.773s, 1324.59/s  (0.787s, 1300.83/s)  LR: 1.171e-04  Data: 0.010 (0.012)
Train: 472 [ 850/1251 ( 68%)]  Loss: 3.044 (3.08)  Time: 0.813s, 1260.19/s  (0.787s, 1300.37/s)  LR: 1.171e-04  Data: 0.010 (0.012)
Train: 472 [ 900/1251 ( 72%)]  Loss: 3.215 (3.08)  Time: 0.772s, 1326.60/s  (0.787s, 1301.18/s)  LR: 1.171e-04  Data: 0.010 (0.012)
Train: 472 [ 950/1251 ( 76%)]  Loss: 3.540 (3.11)  Time: 0.811s, 1262.53/s  (0.787s, 1301.61/s)  LR: 1.171e-04  Data: 0.010 (0.012)
Train: 472 [1000/1251 ( 80%)]  Loss: 3.441 (3.12)  Time: 0.772s, 1326.04/s  (0.787s, 1301.77/s)  LR: 1.171e-04  Data: 0.010 (0.012)
Train: 472 [1050/1251 ( 84%)]  Loss: 3.007 (3.12)  Time: 0.772s, 1326.01/s  (0.787s, 1301.63/s)  LR: 1.171e-04  Data: 0.010 (0.012)
Train: 472 [1100/1251 ( 88%)]  Loss: 3.275 (3.12)  Time: 0.772s, 1326.60/s  (0.787s, 1301.48/s)  LR: 1.171e-04  Data: 0.010 (0.012)
Train: 472 [1150/1251 ( 92%)]  Loss: 3.272 (3.13)  Time: 0.809s, 1265.72/s  (0.787s, 1301.78/s)  LR: 1.171e-04  Data: 0.010 (0.011)
Train: 472 [1200/1251 ( 96%)]  Loss: 3.389 (3.14)  Time: 0.807s, 1268.28/s  (0.787s, 1301.12/s)  LR: 1.171e-04  Data: 0.010 (0.011)
Train: 472 [1250/1251 (100%)]  Loss: 3.074 (3.14)  Time: 0.758s, 1350.05/s  (0.787s, 1301.16/s)  LR: 1.171e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.493 (1.493)  Loss:  0.7324 (0.7324)  Acc@1: 91.2109 (91.2109)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.193 (0.567)  Loss:  0.7690 (1.1370)  Acc@1: 87.0283 (79.2520)  Acc@5: 97.7594 (94.8040)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-471.pth.tar', 79.68199995117187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-470.pth.tar', 79.61799989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-467.pth.tar', 79.51399989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-464.pth.tar', 79.45200000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-469.pth.tar', 79.33200013427734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-468.pth.tar', 79.32600013427735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-465.pth.tar', 79.29199997802735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-466.pth.tar', 79.2599999243164)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-472.pth.tar', 79.25200005371094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-461.pth.tar', 79.234)

Train: 473 [   0/1251 (  0%)]  Loss: 3.023 (3.02)  Time: 2.147s,  477.01/s  (2.147s,  477.01/s)  LR: 1.155e-04  Data: 1.433 (1.433)
Train: 473 [  50/1251 (  4%)]  Loss: 3.114 (3.07)  Time: 0.775s, 1321.82/s  (0.810s, 1263.86/s)  LR: 1.155e-04  Data: 0.011 (0.041)
Train: 473 [ 100/1251 (  8%)]  Loss: 2.994 (3.04)  Time: 0.811s, 1262.62/s  (0.799s, 1281.03/s)  LR: 1.155e-04  Data: 0.010 (0.026)
Train: 473 [ 150/1251 ( 12%)]  Loss: 3.291 (3.11)  Time: 0.775s, 1321.75/s  (0.796s, 1287.01/s)  LR: 1.155e-04  Data: 0.010 (0.020)
Train: 473 [ 200/1251 ( 16%)]  Loss: 3.092 (3.10)  Time: 0.771s, 1328.74/s  (0.795s, 1287.70/s)  LR: 1.155e-04  Data: 0.010 (0.018)
Train: 473 [ 250/1251 ( 20%)]  Loss: 3.067 (3.10)  Time: 0.808s, 1267.85/s  (0.794s, 1289.12/s)  LR: 1.155e-04  Data: 0.010 (0.016)
Train: 473 [ 300/1251 ( 24%)]  Loss: 3.370 (3.14)  Time: 0.776s, 1320.17/s  (0.794s, 1289.65/s)  LR: 1.155e-04  Data: 0.010 (0.015)
Train: 473 [ 350/1251 ( 28%)]  Loss: 2.922 (3.11)  Time: 0.808s, 1267.91/s  (0.793s, 1290.79/s)  LR: 1.155e-04  Data: 0.010 (0.014)
Train: 473 [ 400/1251 ( 32%)]  Loss: 3.114 (3.11)  Time: 0.773s, 1324.97/s  (0.792s, 1293.43/s)  LR: 1.155e-04  Data: 0.010 (0.014)
Train: 473 [ 450/1251 ( 36%)]  Loss: 2.765 (3.08)  Time: 0.771s, 1328.28/s  (0.790s, 1295.50/s)  LR: 1.155e-04  Data: 0.010 (0.013)
Train: 473 [ 500/1251 ( 40%)]  Loss: 3.026 (3.07)  Time: 0.773s, 1324.71/s  (0.790s, 1296.55/s)  LR: 1.155e-04  Data: 0.010 (0.013)
Train: 473 [ 550/1251 ( 44%)]  Loss: 3.147 (3.08)  Time: 0.776s, 1318.92/s  (0.790s, 1296.70/s)  LR: 1.155e-04  Data: 0.010 (0.013)
Train: 473 [ 600/1251 ( 48%)]  Loss: 3.265 (3.09)  Time: 0.772s, 1326.62/s  (0.790s, 1296.01/s)  LR: 1.155e-04  Data: 0.010 (0.013)
Train: 473 [ 650/1251 ( 52%)]  Loss: 2.796 (3.07)  Time: 0.771s, 1327.45/s  (0.790s, 1296.07/s)  LR: 1.155e-04  Data: 0.010 (0.012)
Train: 473 [ 700/1251 ( 56%)]  Loss: 3.265 (3.08)  Time: 0.807s, 1268.87/s  (0.789s, 1297.77/s)  LR: 1.155e-04  Data: 0.010 (0.012)
Train: 473 [ 750/1251 ( 60%)]  Loss: 3.491 (3.11)  Time: 0.809s, 1266.01/s  (0.789s, 1297.26/s)  LR: 1.155e-04  Data: 0.010 (0.012)
Train: 473 [ 800/1251 ( 64%)]  Loss: 2.952 (3.10)  Time: 0.773s, 1324.47/s  (0.789s, 1297.45/s)  LR: 1.155e-04  Data: 0.010 (0.012)
Train: 473 [ 850/1251 ( 68%)]  Loss: 3.371 (3.11)  Time: 0.776s, 1320.13/s  (0.789s, 1298.18/s)  LR: 1.155e-04  Data: 0.010 (0.012)
Train: 473 [ 900/1251 ( 72%)]  Loss: 3.270 (3.12)  Time: 0.772s, 1326.06/s  (0.789s, 1297.88/s)  LR: 1.155e-04  Data: 0.010 (0.012)
Train: 473 [ 950/1251 ( 76%)]  Loss: 3.525 (3.14)  Time: 0.774s, 1322.48/s  (0.789s, 1298.24/s)  LR: 1.155e-04  Data: 0.010 (0.012)
Train: 473 [1000/1251 ( 80%)]  Loss: 3.060 (3.14)  Time: 0.776s, 1319.98/s  (0.788s, 1298.81/s)  LR: 1.155e-04  Data: 0.010 (0.011)
Train: 473 [1050/1251 ( 84%)]  Loss: 2.987 (3.13)  Time: 0.772s, 1325.77/s  (0.789s, 1298.35/s)  LR: 1.155e-04  Data: 0.010 (0.011)
Train: 473 [1100/1251 ( 88%)]  Loss: 3.098 (3.13)  Time: 0.772s, 1327.06/s  (0.788s, 1298.72/s)  LR: 1.155e-04  Data: 0.010 (0.011)
Train: 473 [1150/1251 ( 92%)]  Loss: 3.019 (3.13)  Time: 0.772s, 1325.71/s  (0.788s, 1299.24/s)  LR: 1.155e-04  Data: 0.010 (0.011)
Train: 473 [1200/1251 ( 96%)]  Loss: 3.122 (3.13)  Time: 0.808s, 1267.26/s  (0.788s, 1298.98/s)  LR: 1.155e-04  Data: 0.010 (0.011)
Train: 473 [1250/1251 (100%)]  Loss: 3.291 (3.13)  Time: 0.793s, 1290.85/s  (0.788s, 1298.92/s)  LR: 1.155e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.605 (1.605)  Loss:  0.7373 (0.7373)  Acc@1: 91.6016 (91.6016)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.193 (0.557)  Loss:  0.8110 (1.1805)  Acc@1: 87.1462 (79.0780)  Acc@5: 97.7594 (94.7740)
Train: 474 [   0/1251 (  0%)]  Loss: 2.938 (2.94)  Time: 2.080s,  492.35/s  (2.080s,  492.35/s)  LR: 1.139e-04  Data: 1.365 (1.365)
Train: 474 [  50/1251 (  4%)]  Loss: 3.458 (3.20)  Time: 0.808s, 1267.95/s  (0.822s, 1246.18/s)  LR: 1.139e-04  Data: 0.010 (0.041)
Train: 474 [ 100/1251 (  8%)]  Loss: 2.904 (3.10)  Time: 0.806s, 1270.34/s  (0.806s, 1270.59/s)  LR: 1.139e-04  Data: 0.010 (0.026)
Train: 474 [ 150/1251 ( 12%)]  Loss: 2.991 (3.07)  Time: 0.775s, 1321.44/s  (0.802s, 1276.66/s)  LR: 1.139e-04  Data: 0.010 (0.021)
Train: 474 [ 200/1251 ( 16%)]  Loss: 3.119 (3.08)  Time: 0.772s, 1326.38/s  (0.801s, 1279.06/s)  LR: 1.139e-04  Data: 0.010 (0.018)
Train: 474 [ 250/1251 ( 20%)]  Loss: 3.408 (3.14)  Time: 0.772s, 1325.60/s  (0.796s, 1287.05/s)  LR: 1.139e-04  Data: 0.010 (0.016)
Train: 474 [ 300/1251 ( 24%)]  Loss: 3.420 (3.18)  Time: 0.805s, 1271.64/s  (0.794s, 1290.16/s)  LR: 1.139e-04  Data: 0.010 (0.015)
Train: 474 [ 350/1251 ( 28%)]  Loss: 3.142 (3.17)  Time: 0.776s, 1318.94/s  (0.794s, 1290.34/s)  LR: 1.139e-04  Data: 0.010 (0.014)
Train: 474 [ 400/1251 ( 32%)]  Loss: 3.065 (3.16)  Time: 0.772s, 1326.47/s  (0.792s, 1292.74/s)  LR: 1.139e-04  Data: 0.010 (0.014)
Train: 474 [ 450/1251 ( 36%)]  Loss: 3.114 (3.16)  Time: 0.772s, 1325.83/s  (0.792s, 1293.42/s)  LR: 1.139e-04  Data: 0.010 (0.013)
Train: 474 [ 500/1251 ( 40%)]  Loss: 3.028 (3.14)  Time: 0.813s, 1259.42/s  (0.793s, 1291.60/s)  LR: 1.139e-04  Data: 0.010 (0.013)
Train: 474 [ 550/1251 ( 44%)]  Loss: 3.031 (3.13)  Time: 0.808s, 1268.05/s  (0.792s, 1293.27/s)  LR: 1.139e-04  Data: 0.010 (0.013)
Train: 474 [ 600/1251 ( 48%)]  Loss: 2.979 (3.12)  Time: 0.771s, 1327.81/s  (0.791s, 1294.09/s)  LR: 1.139e-04  Data: 0.009 (0.013)
Train: 474 [ 650/1251 ( 52%)]  Loss: 3.170 (3.13)  Time: 0.772s, 1326.48/s  (0.791s, 1294.84/s)  LR: 1.139e-04  Data: 0.010 (0.012)
Train: 474 [ 700/1251 ( 56%)]  Loss: 2.912 (3.11)  Time: 0.775s, 1320.48/s  (0.790s, 1296.18/s)  LR: 1.139e-04  Data: 0.010 (0.012)
Train: 474 [ 750/1251 ( 60%)]  Loss: 3.379 (3.13)  Time: 0.773s, 1325.02/s  (0.789s, 1297.21/s)  LR: 1.139e-04  Data: 0.010 (0.012)
Train: 474 [ 800/1251 ( 64%)]  Loss: 3.442 (3.15)  Time: 0.772s, 1327.06/s  (0.789s, 1297.36/s)  LR: 1.139e-04  Data: 0.010 (0.012)
Train: 474 [ 850/1251 ( 68%)]  Loss: 3.274 (3.15)  Time: 0.772s, 1326.84/s  (0.789s, 1297.74/s)  LR: 1.139e-04  Data: 0.010 (0.012)
Train: 474 [ 900/1251 ( 72%)]  Loss: 3.256 (3.16)  Time: 0.776s, 1320.37/s  (0.789s, 1298.25/s)  LR: 1.139e-04  Data: 0.010 (0.012)
Train: 474 [ 950/1251 ( 76%)]  Loss: 3.069 (3.15)  Time: 0.771s, 1327.47/s  (0.789s, 1297.76/s)  LR: 1.139e-04  Data: 0.010 (0.012)
Train: 474 [1000/1251 ( 80%)]  Loss: 3.398 (3.17)  Time: 0.774s, 1322.94/s  (0.789s, 1298.32/s)  LR: 1.139e-04  Data: 0.010 (0.011)
Train: 474 [1050/1251 ( 84%)]  Loss: 2.795 (3.15)  Time: 0.773s, 1324.24/s  (0.789s, 1298.56/s)  LR: 1.139e-04  Data: 0.010 (0.011)
Train: 474 [1100/1251 ( 88%)]  Loss: 3.294 (3.16)  Time: 0.772s, 1326.43/s  (0.789s, 1298.65/s)  LR: 1.139e-04  Data: 0.010 (0.011)
Train: 474 [1150/1251 ( 92%)]  Loss: 3.217 (3.16)  Time: 0.772s, 1325.73/s  (0.789s, 1298.64/s)  LR: 1.139e-04  Data: 0.010 (0.011)
Train: 474 [1200/1251 ( 96%)]  Loss: 3.345 (3.17)  Time: 0.775s, 1322.05/s  (0.789s, 1298.66/s)  LR: 1.139e-04  Data: 0.010 (0.011)
Train: 474 [1250/1251 (100%)]  Loss: 2.870 (3.15)  Time: 0.791s, 1294.18/s  (0.789s, 1298.66/s)  LR: 1.139e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.564 (1.564)  Loss:  0.6221 (0.6221)  Acc@1: 91.8945 (91.8945)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.194 (0.569)  Loss:  0.7021 (1.0945)  Acc@1: 86.6745 (79.5580)  Acc@5: 97.7594 (94.8820)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-471.pth.tar', 79.68199995117187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-470.pth.tar', 79.61799989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-474.pth.tar', 79.55800002929688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-467.pth.tar', 79.51399989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-464.pth.tar', 79.45200000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-469.pth.tar', 79.33200013427734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-468.pth.tar', 79.32600013427735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-465.pth.tar', 79.29199997802735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-466.pth.tar', 79.2599999243164)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-472.pth.tar', 79.25200005371094)

Train: 475 [   0/1251 (  0%)]  Loss: 3.053 (3.05)  Time: 2.240s,  457.05/s  (2.240s,  457.05/s)  LR: 1.123e-04  Data: 1.513 (1.513)
Train: 475 [  50/1251 (  4%)]  Loss: 2.880 (2.97)  Time: 0.808s, 1266.98/s  (0.836s, 1224.27/s)  LR: 1.123e-04  Data: 0.010 (0.040)
Train: 475 [ 100/1251 (  8%)]  Loss: 3.422 (3.12)  Time: 0.810s, 1263.60/s  (0.813s, 1260.02/s)  LR: 1.123e-04  Data: 0.011 (0.025)
Train: 475 [ 150/1251 ( 12%)]  Loss: 2.972 (3.08)  Time: 0.779s, 1314.06/s  (0.808s, 1267.87/s)  LR: 1.123e-04  Data: 0.010 (0.020)
Train: 475 [ 200/1251 ( 16%)]  Loss: 3.257 (3.12)  Time: 0.808s, 1267.21/s  (0.803s, 1274.66/s)  LR: 1.123e-04  Data: 0.010 (0.017)
Train: 475 [ 250/1251 ( 20%)]  Loss: 2.620 (3.03)  Time: 0.808s, 1267.20/s  (0.803s, 1275.15/s)  LR: 1.123e-04  Data: 0.010 (0.016)
Train: 475 [ 300/1251 ( 24%)]  Loss: 2.943 (3.02)  Time: 0.775s, 1321.30/s  (0.802s, 1277.25/s)  LR: 1.123e-04  Data: 0.010 (0.015)
Train: 475 [ 350/1251 ( 28%)]  Loss: 3.264 (3.05)  Time: 0.807s, 1269.06/s  (0.799s, 1281.70/s)  LR: 1.123e-04  Data: 0.010 (0.014)
Train: 475 [ 400/1251 ( 32%)]  Loss: 2.946 (3.04)  Time: 0.813s, 1258.91/s  (0.798s, 1282.70/s)  LR: 1.123e-04  Data: 0.010 (0.014)
Train: 475 [ 450/1251 ( 36%)]  Loss: 3.218 (3.06)  Time: 0.876s, 1169.10/s  (0.798s, 1282.98/s)  LR: 1.123e-04  Data: 0.010 (0.013)
Train: 475 [ 500/1251 ( 40%)]  Loss: 3.065 (3.06)  Time: 0.807s, 1268.32/s  (0.798s, 1283.39/s)  LR: 1.123e-04  Data: 0.010 (0.013)
Train: 475 [ 550/1251 ( 44%)]  Loss: 3.098 (3.06)  Time: 0.809s, 1265.74/s  (0.797s, 1284.27/s)  LR: 1.123e-04  Data: 0.010 (0.013)
Train: 475 [ 600/1251 ( 48%)]  Loss: 3.050 (3.06)  Time: 0.807s, 1268.97/s  (0.797s, 1285.53/s)  LR: 1.123e-04  Data: 0.010 (0.012)
Train: 475 [ 650/1251 ( 52%)]  Loss: 3.155 (3.07)  Time: 0.772s, 1326.11/s  (0.796s, 1285.92/s)  LR: 1.123e-04  Data: 0.010 (0.012)
Train: 475 [ 700/1251 ( 56%)]  Loss: 3.187 (3.08)  Time: 0.808s, 1266.67/s  (0.797s, 1285.58/s)  LR: 1.123e-04  Data: 0.010 (0.012)
Train: 475 [ 750/1251 ( 60%)]  Loss: 2.825 (3.06)  Time: 0.774s, 1322.26/s  (0.796s, 1286.12/s)  LR: 1.123e-04  Data: 0.010 (0.012)
Train: 475 [ 800/1251 ( 64%)]  Loss: 2.918 (3.05)  Time: 0.771s, 1327.61/s  (0.796s, 1286.96/s)  LR: 1.123e-04  Data: 0.010 (0.012)
Train: 475 [ 850/1251 ( 68%)]  Loss: 3.192 (3.06)  Time: 0.772s, 1326.38/s  (0.795s, 1288.65/s)  LR: 1.123e-04  Data: 0.010 (0.012)
Train: 475 [ 900/1251 ( 72%)]  Loss: 2.727 (3.04)  Time: 0.808s, 1267.45/s  (0.794s, 1289.22/s)  LR: 1.123e-04  Data: 0.010 (0.012)
Train: 475 [ 950/1251 ( 76%)]  Loss: 3.393 (3.06)  Time: 0.772s, 1326.88/s  (0.795s, 1288.82/s)  LR: 1.123e-04  Data: 0.010 (0.011)
Train: 475 [1000/1251 ( 80%)]  Loss: 2.914 (3.05)  Time: 0.808s, 1266.99/s  (0.794s, 1289.59/s)  LR: 1.123e-04  Data: 0.010 (0.011)
Train: 475 [1050/1251 ( 84%)]  Loss: 3.344 (3.07)  Time: 0.775s, 1321.88/s  (0.794s, 1290.44/s)  LR: 1.123e-04  Data: 0.010 (0.011)
Train: 475 [1100/1251 ( 88%)]  Loss: 3.242 (3.07)  Time: 0.812s, 1260.41/s  (0.793s, 1291.18/s)  LR: 1.123e-04  Data: 0.010 (0.011)
Train: 475 [1150/1251 ( 92%)]  Loss: 2.834 (3.06)  Time: 0.774s, 1323.64/s  (0.793s, 1292.01/s)  LR: 1.123e-04  Data: 0.010 (0.011)
Train: 475 [1200/1251 ( 96%)]  Loss: 3.217 (3.07)  Time: 0.774s, 1322.75/s  (0.792s, 1292.49/s)  LR: 1.123e-04  Data: 0.010 (0.011)
Train: 475 [1250/1251 (100%)]  Loss: 2.961 (3.07)  Time: 0.757s, 1352.89/s  (0.792s, 1293.31/s)  LR: 1.123e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.557 (1.557)  Loss:  0.6567 (0.6567)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.193 (0.561)  Loss:  0.7588 (1.1210)  Acc@1: 86.5566 (79.3320)  Acc@5: 97.2877 (94.9580)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-471.pth.tar', 79.68199995117187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-470.pth.tar', 79.61799989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-474.pth.tar', 79.55800002929688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-467.pth.tar', 79.51399989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-464.pth.tar', 79.45200000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-469.pth.tar', 79.33200013427734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-475.pth.tar', 79.33199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-468.pth.tar', 79.32600013427735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-465.pth.tar', 79.29199997802735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-466.pth.tar', 79.2599999243164)

Train: 476 [   0/1251 (  0%)]  Loss: 3.150 (3.15)  Time: 2.352s,  435.34/s  (2.352s,  435.34/s)  LR: 1.107e-04  Data: 1.627 (1.627)
Train: 476 [  50/1251 (  4%)]  Loss: 3.300 (3.23)  Time: 0.772s, 1326.98/s  (0.816s, 1254.30/s)  LR: 1.107e-04  Data: 0.010 (0.044)
Train: 476 [ 100/1251 (  8%)]  Loss: 2.838 (3.10)  Time: 0.773s, 1324.83/s  (0.805s, 1272.53/s)  LR: 1.107e-04  Data: 0.010 (0.027)
Train: 476 [ 150/1251 ( 12%)]  Loss: 3.075 (3.09)  Time: 0.809s, 1265.98/s  (0.799s, 1281.04/s)  LR: 1.107e-04  Data: 0.010 (0.021)
Train: 476 [ 200/1251 ( 16%)]  Loss: 3.467 (3.17)  Time: 0.810s, 1264.18/s  (0.798s, 1282.52/s)  LR: 1.107e-04  Data: 0.010 (0.019)
Train: 476 [ 250/1251 ( 20%)]  Loss: 2.898 (3.12)  Time: 0.772s, 1326.77/s  (0.796s, 1287.16/s)  LR: 1.107e-04  Data: 0.010 (0.017)
Train: 476 [ 300/1251 ( 24%)]  Loss: 3.105 (3.12)  Time: 0.772s, 1327.05/s  (0.796s, 1286.76/s)  LR: 1.107e-04  Data: 0.010 (0.016)
Train: 476 [ 350/1251 ( 28%)]  Loss: 3.246 (3.14)  Time: 0.773s, 1323.91/s  (0.795s, 1288.26/s)  LR: 1.107e-04  Data: 0.010 (0.015)
Train: 476 [ 400/1251 ( 32%)]  Loss: 3.024 (3.12)  Time: 0.772s, 1326.94/s  (0.795s, 1288.03/s)  LR: 1.107e-04  Data: 0.010 (0.014)
Train: 476 [ 450/1251 ( 36%)]  Loss: 3.050 (3.12)  Time: 0.771s, 1327.45/s  (0.795s, 1287.33/s)  LR: 1.107e-04  Data: 0.010 (0.014)
Train: 476 [ 500/1251 ( 40%)]  Loss: 2.581 (3.07)  Time: 0.811s, 1262.36/s  (0.795s, 1287.49/s)  LR: 1.107e-04  Data: 0.010 (0.013)
Train: 476 [ 550/1251 ( 44%)]  Loss: 3.262 (3.08)  Time: 0.771s, 1328.54/s  (0.794s, 1289.97/s)  LR: 1.107e-04  Data: 0.010 (0.013)
Train: 476 [ 600/1251 ( 48%)]  Loss: 3.198 (3.09)  Time: 0.771s, 1328.17/s  (0.792s, 1292.70/s)  LR: 1.107e-04  Data: 0.010 (0.013)
Train: 476 [ 650/1251 ( 52%)]  Loss: 2.958 (3.08)  Time: 0.772s, 1325.67/s  (0.792s, 1293.43/s)  LR: 1.107e-04  Data: 0.010 (0.013)
Train: 476 [ 700/1251 ( 56%)]  Loss: 3.185 (3.09)  Time: 0.773s, 1325.56/s  (0.792s, 1293.23/s)  LR: 1.107e-04  Data: 0.010 (0.012)
Train: 476 [ 750/1251 ( 60%)]  Loss: 2.906 (3.08)  Time: 0.806s, 1270.28/s  (0.791s, 1293.92/s)  LR: 1.107e-04  Data: 0.010 (0.012)
Train: 476 [ 800/1251 ( 64%)]  Loss: 3.464 (3.10)  Time: 0.776s, 1320.09/s  (0.792s, 1293.39/s)  LR: 1.107e-04  Data: 0.010 (0.012)
Train: 476 [ 850/1251 ( 68%)]  Loss: 3.261 (3.11)  Time: 0.771s, 1328.75/s  (0.791s, 1294.65/s)  LR: 1.107e-04  Data: 0.010 (0.012)
Train: 476 [ 900/1251 ( 72%)]  Loss: 3.026 (3.10)  Time: 0.805s, 1271.95/s  (0.791s, 1294.67/s)  LR: 1.107e-04  Data: 0.010 (0.012)
Train: 476 [ 950/1251 ( 76%)]  Loss: 3.160 (3.11)  Time: 0.772s, 1325.60/s  (0.791s, 1294.53/s)  LR: 1.107e-04  Data: 0.010 (0.012)
Train: 476 [1000/1251 ( 80%)]  Loss: 2.861 (3.10)  Time: 0.773s, 1324.23/s  (0.791s, 1294.88/s)  LR: 1.107e-04  Data: 0.010 (0.012)
Train: 476 [1050/1251 ( 84%)]  Loss: 2.833 (3.08)  Time: 0.783s, 1307.35/s  (0.790s, 1295.73/s)  LR: 1.107e-04  Data: 0.010 (0.012)
Train: 476 [1100/1251 ( 88%)]  Loss: 3.355 (3.10)  Time: 0.772s, 1326.98/s  (0.790s, 1296.28/s)  LR: 1.107e-04  Data: 0.010 (0.012)
Train: 476 [1150/1251 ( 92%)]  Loss: 2.949 (3.09)  Time: 0.773s, 1325.24/s  (0.789s, 1297.23/s)  LR: 1.107e-04  Data: 0.009 (0.011)
Train: 476 [1200/1251 ( 96%)]  Loss: 3.147 (3.09)  Time: 0.773s, 1325.20/s  (0.789s, 1297.58/s)  LR: 1.107e-04  Data: 0.010 (0.011)
Train: 476 [1250/1251 (100%)]  Loss: 3.017 (3.09)  Time: 0.791s, 1294.17/s  (0.789s, 1298.33/s)  LR: 1.107e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.550 (1.550)  Loss:  0.6450 (0.6450)  Acc@1: 91.8945 (91.8945)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.194 (0.563)  Loss:  0.7900 (1.1203)  Acc@1: 86.5566 (79.5640)  Acc@5: 98.3491 (94.9380)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-471.pth.tar', 79.68199995117187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-470.pth.tar', 79.61799989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-476.pth.tar', 79.56399997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-474.pth.tar', 79.55800002929688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-467.pth.tar', 79.51399989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-464.pth.tar', 79.45200000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-469.pth.tar', 79.33200013427734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-475.pth.tar', 79.33199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-468.pth.tar', 79.32600013427735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-465.pth.tar', 79.29199997802735)

Train: 477 [   0/1251 (  0%)]  Loss: 2.905 (2.90)  Time: 2.224s,  460.53/s  (2.224s,  460.53/s)  LR: 1.092e-04  Data: 1.494 (1.494)
Train: 477 [  50/1251 (  4%)]  Loss: 3.303 (3.10)  Time: 0.810s, 1264.17/s  (0.816s, 1255.24/s)  LR: 1.092e-04  Data: 0.010 (0.044)
Train: 477 [ 100/1251 (  8%)]  Loss: 3.013 (3.07)  Time: 0.807s, 1268.49/s  (0.803s, 1274.94/s)  LR: 1.092e-04  Data: 0.010 (0.027)
Train: 477 [ 150/1251 ( 12%)]  Loss: 3.384 (3.15)  Time: 0.775s, 1321.56/s  (0.799s, 1282.03/s)  LR: 1.092e-04  Data: 0.010 (0.021)
Train: 477 [ 200/1251 ( 16%)]  Loss: 3.359 (3.19)  Time: 0.808s, 1267.11/s  (0.795s, 1288.69/s)  LR: 1.092e-04  Data: 0.010 (0.018)
Train: 477 [ 250/1251 ( 20%)]  Loss: 3.374 (3.22)  Time: 0.773s, 1324.49/s  (0.794s, 1289.75/s)  LR: 1.092e-04  Data: 0.010 (0.017)
Train: 477 [ 300/1251 ( 24%)]  Loss: 2.977 (3.19)  Time: 0.856s, 1195.77/s  (0.791s, 1294.10/s)  LR: 1.092e-04  Data: 0.010 (0.016)
Train: 477 [ 350/1251 ( 28%)]  Loss: 3.028 (3.17)  Time: 0.775s, 1321.22/s  (0.790s, 1295.79/s)  LR: 1.092e-04  Data: 0.010 (0.015)
Train: 477 [ 400/1251 ( 32%)]  Loss: 3.251 (3.18)  Time: 0.773s, 1325.33/s  (0.789s, 1298.50/s)  LR: 1.092e-04  Data: 0.009 (0.014)
Train: 477 [ 450/1251 ( 36%)]  Loss: 2.970 (3.16)  Time: 0.808s, 1267.52/s  (0.790s, 1296.78/s)  LR: 1.092e-04  Data: 0.010 (0.014)
Train: 477 [ 500/1251 ( 40%)]  Loss: 3.210 (3.16)  Time: 0.774s, 1322.29/s  (0.791s, 1294.93/s)  LR: 1.092e-04  Data: 0.010 (0.013)
Train: 477 [ 550/1251 ( 44%)]  Loss: 3.151 (3.16)  Time: 0.773s, 1324.61/s  (0.790s, 1295.91/s)  LR: 1.092e-04  Data: 0.010 (0.013)
Train: 477 [ 600/1251 ( 48%)]  Loss: 2.872 (3.14)  Time: 0.810s, 1264.01/s  (0.790s, 1296.70/s)  LR: 1.092e-04  Data: 0.010 (0.013)
Train: 477 [ 650/1251 ( 52%)]  Loss: 3.196 (3.14)  Time: 0.807s, 1268.62/s  (0.790s, 1295.91/s)  LR: 1.092e-04  Data: 0.010 (0.012)
Train: 477 [ 700/1251 ( 56%)]  Loss: 2.783 (3.12)  Time: 0.773s, 1324.24/s  (0.791s, 1295.07/s)  LR: 1.092e-04  Data: 0.010 (0.012)
Train: 477 [ 750/1251 ( 60%)]  Loss: 3.334 (3.13)  Time: 0.807s, 1269.03/s  (0.791s, 1294.25/s)  LR: 1.092e-04  Data: 0.010 (0.012)
Train: 477 [ 800/1251 ( 64%)]  Loss: 2.784 (3.11)  Time: 0.774s, 1323.52/s  (0.792s, 1293.02/s)  LR: 1.092e-04  Data: 0.010 (0.012)
Train: 477 [ 850/1251 ( 68%)]  Loss: 2.890 (3.10)  Time: 0.808s, 1267.73/s  (0.792s, 1292.53/s)  LR: 1.092e-04  Data: 0.010 (0.012)
Train: 477 [ 900/1251 ( 72%)]  Loss: 2.919 (3.09)  Time: 0.807s, 1269.24/s  (0.792s, 1292.46/s)  LR: 1.092e-04  Data: 0.010 (0.012)
Train: 477 [ 950/1251 ( 76%)]  Loss: 3.364 (3.10)  Time: 0.775s, 1321.46/s  (0.793s, 1291.98/s)  LR: 1.092e-04  Data: 0.010 (0.012)
Train: 477 [1000/1251 ( 80%)]  Loss: 3.181 (3.11)  Time: 0.775s, 1321.71/s  (0.793s, 1291.41/s)  LR: 1.092e-04  Data: 0.010 (0.012)
Train: 477 [1050/1251 ( 84%)]  Loss: 3.033 (3.10)  Time: 0.774s, 1323.53/s  (0.792s, 1292.13/s)  LR: 1.092e-04  Data: 0.010 (0.012)
Train: 477 [1100/1251 ( 88%)]  Loss: 3.260 (3.11)  Time: 0.807s, 1269.14/s  (0.792s, 1292.28/s)  LR: 1.092e-04  Data: 0.010 (0.011)
Train: 477 [1150/1251 ( 92%)]  Loss: 3.232 (3.12)  Time: 0.813s, 1260.00/s  (0.793s, 1292.06/s)  LR: 1.092e-04  Data: 0.011 (0.011)
Train: 477 [1200/1251 ( 96%)]  Loss: 3.143 (3.12)  Time: 0.849s, 1206.33/s  (0.793s, 1291.47/s)  LR: 1.092e-04  Data: 0.010 (0.011)
Train: 477 [1250/1251 (100%)]  Loss: 3.206 (3.12)  Time: 0.791s, 1294.01/s  (0.792s, 1292.21/s)  LR: 1.092e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.622 (1.622)  Loss:  0.6704 (0.6704)  Acc@1: 91.5039 (91.5039)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.194 (0.556)  Loss:  0.7598 (1.1247)  Acc@1: 87.2642 (79.4600)  Acc@5: 97.7594 (94.8540)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-471.pth.tar', 79.68199995117187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-470.pth.tar', 79.61799989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-476.pth.tar', 79.56399997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-474.pth.tar', 79.55800002929688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-467.pth.tar', 79.51399989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-477.pth.tar', 79.46000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-464.pth.tar', 79.45200000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-469.pth.tar', 79.33200013427734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-475.pth.tar', 79.33199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-468.pth.tar', 79.32600013427735)

Train: 478 [   0/1251 (  0%)]  Loss: 3.305 (3.31)  Time: 2.172s,  471.35/s  (2.172s,  471.35/s)  LR: 1.076e-04  Data: 1.459 (1.459)
Train: 478 [  50/1251 (  4%)]  Loss: 3.369 (3.34)  Time: 0.813s, 1260.04/s  (0.821s, 1247.33/s)  LR: 1.076e-04  Data: 0.010 (0.041)
Train: 478 [ 100/1251 (  8%)]  Loss: 3.211 (3.29)  Time: 0.806s, 1270.85/s  (0.803s, 1274.91/s)  LR: 1.076e-04  Data: 0.010 (0.026)
Train: 478 [ 150/1251 ( 12%)]  Loss: 3.421 (3.33)  Time: 0.774s, 1322.75/s  (0.796s, 1286.24/s)  LR: 1.076e-04  Data: 0.010 (0.020)
Train: 478 [ 200/1251 ( 16%)]  Loss: 3.300 (3.32)  Time: 0.806s, 1269.90/s  (0.795s, 1288.63/s)  LR: 1.076e-04  Data: 0.010 (0.018)
Train: 478 [ 250/1251 ( 20%)]  Loss: 3.281 (3.31)  Time: 0.806s, 1270.14/s  (0.796s, 1286.89/s)  LR: 1.076e-04  Data: 0.010 (0.016)
Train: 478 [ 300/1251 ( 24%)]  Loss: 3.055 (3.28)  Time: 0.772s, 1326.67/s  (0.795s, 1288.06/s)  LR: 1.076e-04  Data: 0.010 (0.015)
Train: 478 [ 350/1251 ( 28%)]  Loss: 2.778 (3.22)  Time: 0.773s, 1325.33/s  (0.794s, 1289.33/s)  LR: 1.076e-04  Data: 0.010 (0.014)
Train: 478 [ 400/1251 ( 32%)]  Loss: 3.163 (3.21)  Time: 0.813s, 1260.02/s  (0.794s, 1289.06/s)  LR: 1.076e-04  Data: 0.010 (0.014)
Train: 478 [ 450/1251 ( 36%)]  Loss: 3.101 (3.20)  Time: 0.773s, 1325.55/s  (0.793s, 1290.72/s)  LR: 1.076e-04  Data: 0.010 (0.013)
Train: 478 [ 500/1251 ( 40%)]  Loss: 2.992 (3.18)  Time: 0.807s, 1269.05/s  (0.793s, 1290.52/s)  LR: 1.076e-04  Data: 0.010 (0.013)
Train: 478 [ 550/1251 ( 44%)]  Loss: 2.794 (3.15)  Time: 0.809s, 1266.30/s  (0.793s, 1290.99/s)  LR: 1.076e-04  Data: 0.010 (0.013)
Train: 478 [ 600/1251 ( 48%)]  Loss: 3.373 (3.16)  Time: 0.772s, 1325.85/s  (0.793s, 1290.55/s)  LR: 1.076e-04  Data: 0.010 (0.013)
Train: 478 [ 650/1251 ( 52%)]  Loss: 2.969 (3.15)  Time: 0.771s, 1327.80/s  (0.793s, 1291.00/s)  LR: 1.076e-04  Data: 0.010 (0.012)
Train: 478 [ 700/1251 ( 56%)]  Loss: 3.127 (3.15)  Time: 0.772s, 1326.11/s  (0.792s, 1292.33/s)  LR: 1.076e-04  Data: 0.010 (0.012)
Train: 478 [ 750/1251 ( 60%)]  Loss: 3.103 (3.15)  Time: 0.776s, 1319.48/s  (0.792s, 1292.89/s)  LR: 1.076e-04  Data: 0.010 (0.012)
Train: 478 [ 800/1251 ( 64%)]  Loss: 3.230 (3.15)  Time: 0.774s, 1323.75/s  (0.792s, 1292.98/s)  LR: 1.076e-04  Data: 0.010 (0.012)
Train: 478 [ 850/1251 ( 68%)]  Loss: 3.176 (3.15)  Time: 0.807s, 1269.22/s  (0.791s, 1293.99/s)  LR: 1.076e-04  Data: 0.010 (0.012)
Train: 478 [ 900/1251 ( 72%)]  Loss: 3.176 (3.15)  Time: 0.774s, 1322.18/s  (0.791s, 1294.48/s)  LR: 1.076e-04  Data: 0.010 (0.012)
Train: 478 [ 950/1251 ( 76%)]  Loss: 3.054 (3.15)  Time: 0.773s, 1325.11/s  (0.790s, 1295.88/s)  LR: 1.076e-04  Data: 0.010 (0.012)
Train: 478 [1000/1251 ( 80%)]  Loss: 2.777 (3.13)  Time: 0.807s, 1269.49/s  (0.790s, 1296.43/s)  LR: 1.076e-04  Data: 0.010 (0.011)
Train: 478 [1050/1251 ( 84%)]  Loss: 3.180 (3.13)  Time: 0.773s, 1324.98/s  (0.790s, 1296.72/s)  LR: 1.076e-04  Data: 0.010 (0.011)
Train: 478 [1100/1251 ( 88%)]  Loss: 3.273 (3.14)  Time: 0.772s, 1326.58/s  (0.789s, 1297.78/s)  LR: 1.076e-04  Data: 0.009 (0.011)
Train: 478 [1150/1251 ( 92%)]  Loss: 3.289 (3.15)  Time: 0.777s, 1317.45/s  (0.789s, 1298.04/s)  LR: 1.076e-04  Data: 0.010 (0.011)
Train: 478 [1200/1251 ( 96%)]  Loss: 3.252 (3.15)  Time: 0.806s, 1270.93/s  (0.789s, 1297.94/s)  LR: 1.076e-04  Data: 0.010 (0.011)
Train: 478 [1250/1251 (100%)]  Loss: 3.108 (3.15)  Time: 0.789s, 1297.44/s  (0.790s, 1297.00/s)  LR: 1.076e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.528 (1.528)  Loss:  0.7197 (0.7197)  Acc@1: 91.1133 (91.1133)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.194 (0.567)  Loss:  0.7944 (1.1482)  Acc@1: 86.9104 (79.5080)  Acc@5: 97.8774 (94.8720)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-471.pth.tar', 79.68199995117187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-470.pth.tar', 79.61799989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-476.pth.tar', 79.56399997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-474.pth.tar', 79.55800002929688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-467.pth.tar', 79.51399989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-478.pth.tar', 79.5080000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-477.pth.tar', 79.46000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-464.pth.tar', 79.45200000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-469.pth.tar', 79.33200013427734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-475.pth.tar', 79.33199997802734)

Train: 479 [   0/1251 (  0%)]  Loss: 2.891 (2.89)  Time: 2.266s,  451.88/s  (2.266s,  451.88/s)  LR: 1.061e-04  Data: 1.536 (1.536)
Train: 479 [  50/1251 (  4%)]  Loss: 3.020 (2.96)  Time: 0.772s, 1325.89/s  (0.810s, 1264.29/s)  LR: 1.061e-04  Data: 0.010 (0.044)
Train: 479 [ 100/1251 (  8%)]  Loss: 3.225 (3.05)  Time: 0.770s, 1329.17/s  (0.801s, 1278.60/s)  LR: 1.061e-04  Data: 0.010 (0.027)
Train: 479 [ 150/1251 ( 12%)]  Loss: 3.166 (3.08)  Time: 0.775s, 1321.05/s  (0.793s, 1291.32/s)  LR: 1.061e-04  Data: 0.010 (0.022)
Train: 479 [ 200/1251 ( 16%)]  Loss: 2.985 (3.06)  Time: 0.772s, 1325.97/s  (0.792s, 1292.44/s)  LR: 1.061e-04  Data: 0.010 (0.019)
Train: 479 [ 250/1251 ( 20%)]  Loss: 2.866 (3.03)  Time: 0.808s, 1266.68/s  (0.791s, 1294.08/s)  LR: 1.061e-04  Data: 0.010 (0.017)
Train: 479 [ 300/1251 ( 24%)]  Loss: 2.880 (3.00)  Time: 0.772s, 1326.94/s  (0.792s, 1292.45/s)  LR: 1.061e-04  Data: 0.010 (0.016)
Train: 479 [ 350/1251 ( 28%)]  Loss: 3.166 (3.03)  Time: 0.772s, 1326.31/s  (0.792s, 1293.30/s)  LR: 1.061e-04  Data: 0.010 (0.015)
Train: 479 [ 400/1251 ( 32%)]  Loss: 2.849 (3.01)  Time: 0.775s, 1321.22/s  (0.791s, 1294.97/s)  LR: 1.061e-04  Data: 0.010 (0.014)
Train: 479 [ 450/1251 ( 36%)]  Loss: 2.906 (3.00)  Time: 0.857s, 1194.41/s  (0.791s, 1294.70/s)  LR: 1.061e-04  Data: 0.010 (0.014)
Train: 479 [ 500/1251 ( 40%)]  Loss: 2.888 (2.99)  Time: 0.779s, 1314.67/s  (0.791s, 1294.83/s)  LR: 1.061e-04  Data: 0.010 (0.013)
Train: 479 [ 550/1251 ( 44%)]  Loss: 2.918 (2.98)  Time: 0.772s, 1326.56/s  (0.790s, 1295.77/s)  LR: 1.061e-04  Data: 0.010 (0.013)
Train: 479 [ 600/1251 ( 48%)]  Loss: 3.257 (3.00)  Time: 0.771s, 1328.87/s  (0.790s, 1296.97/s)  LR: 1.061e-04  Data: 0.010 (0.013)
Train: 479 [ 650/1251 ( 52%)]  Loss: 3.046 (3.00)  Time: 0.772s, 1326.03/s  (0.789s, 1298.32/s)  LR: 1.061e-04  Data: 0.010 (0.013)
Train: 479 [ 700/1251 ( 56%)]  Loss: 2.819 (2.99)  Time: 0.810s, 1264.01/s  (0.788s, 1298.98/s)  LR: 1.061e-04  Data: 0.010 (0.012)
Train: 479 [ 750/1251 ( 60%)]  Loss: 3.091 (3.00)  Time: 0.775s, 1321.25/s  (0.789s, 1298.13/s)  LR: 1.061e-04  Data: 0.010 (0.012)
Train: 479 [ 800/1251 ( 64%)]  Loss: 3.119 (3.01)  Time: 0.808s, 1267.71/s  (0.789s, 1298.62/s)  LR: 1.061e-04  Data: 0.010 (0.012)
Train: 479 [ 850/1251 ( 68%)]  Loss: 3.127 (3.01)  Time: 0.772s, 1326.35/s  (0.789s, 1298.44/s)  LR: 1.061e-04  Data: 0.010 (0.012)
Train: 479 [ 900/1251 ( 72%)]  Loss: 3.142 (3.02)  Time: 0.774s, 1322.33/s  (0.788s, 1299.11/s)  LR: 1.061e-04  Data: 0.010 (0.012)
Train: 479 [ 950/1251 ( 76%)]  Loss: 3.149 (3.03)  Time: 0.772s, 1327.07/s  (0.788s, 1299.21/s)  LR: 1.061e-04  Data: 0.010 (0.012)
Train: 479 [1000/1251 ( 80%)]  Loss: 2.870 (3.02)  Time: 0.772s, 1326.74/s  (0.789s, 1298.16/s)  LR: 1.061e-04  Data: 0.010 (0.012)
Train: 479 [1050/1251 ( 84%)]  Loss: 3.198 (3.03)  Time: 0.806s, 1269.94/s  (0.789s, 1297.44/s)  LR: 1.061e-04  Data: 0.010 (0.012)
Train: 479 [1100/1251 ( 88%)]  Loss: 2.992 (3.02)  Time: 0.772s, 1325.84/s  (0.789s, 1297.24/s)  LR: 1.061e-04  Data: 0.010 (0.011)
Train: 479 [1150/1251 ( 92%)]  Loss: 2.718 (3.01)  Time: 0.807s, 1268.60/s  (0.790s, 1296.61/s)  LR: 1.061e-04  Data: 0.010 (0.011)
Train: 479 [1200/1251 ( 96%)]  Loss: 2.889 (3.01)  Time: 0.775s, 1320.86/s  (0.790s, 1296.64/s)  LR: 1.061e-04  Data: 0.010 (0.011)
Train: 479 [1250/1251 (100%)]  Loss: 2.968 (3.01)  Time: 0.791s, 1294.08/s  (0.790s, 1295.54/s)  LR: 1.061e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.548 (1.548)  Loss:  0.6562 (0.6562)  Acc@1: 91.6992 (91.6992)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.194 (0.556)  Loss:  0.7324 (1.0784)  Acc@1: 86.9104 (79.5200)  Acc@5: 97.5236 (94.9400)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-471.pth.tar', 79.68199995117187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-470.pth.tar', 79.61799989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-476.pth.tar', 79.56399997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-474.pth.tar', 79.55800002929688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-479.pth.tar', 79.5200000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-467.pth.tar', 79.51399989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-478.pth.tar', 79.5080000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-477.pth.tar', 79.46000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-464.pth.tar', 79.45200000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-469.pth.tar', 79.33200013427734)

Train: 480 [   0/1251 (  0%)]  Loss: 2.954 (2.95)  Time: 2.331s,  439.34/s  (2.331s,  439.34/s)  LR: 1.045e-04  Data: 1.611 (1.611)
Train: 480 [  50/1251 (  4%)]  Loss: 3.326 (3.14)  Time: 0.806s, 1270.14/s  (0.814s, 1257.63/s)  LR: 1.045e-04  Data: 0.010 (0.041)
Train: 480 [ 100/1251 (  8%)]  Loss: 3.015 (3.10)  Time: 0.820s, 1249.39/s  (0.804s, 1274.29/s)  LR: 1.045e-04  Data: 0.010 (0.026)
Train: 480 [ 150/1251 ( 12%)]  Loss: 3.354 (3.16)  Time: 0.808s, 1267.83/s  (0.800s, 1279.35/s)  LR: 1.045e-04  Data: 0.010 (0.020)
Train: 480 [ 200/1251 ( 16%)]  Loss: 3.111 (3.15)  Time: 0.772s, 1326.15/s  (0.797s, 1284.08/s)  LR: 1.045e-04  Data: 0.010 (0.018)
Train: 480 [ 250/1251 ( 20%)]  Loss: 3.184 (3.16)  Time: 0.820s, 1248.42/s  (0.797s, 1285.41/s)  LR: 1.045e-04  Data: 0.010 (0.016)
Train: 480 [ 300/1251 ( 24%)]  Loss: 3.157 (3.16)  Time: 0.811s, 1263.02/s  (0.795s, 1287.36/s)  LR: 1.045e-04  Data: 0.010 (0.015)
Train: 480 [ 350/1251 ( 28%)]  Loss: 2.989 (3.14)  Time: 0.812s, 1260.45/s  (0.796s, 1287.21/s)  LR: 1.045e-04  Data: 0.010 (0.014)
Train: 480 [ 400/1251 ( 32%)]  Loss: 3.106 (3.13)  Time: 0.772s, 1326.40/s  (0.795s, 1287.93/s)  LR: 1.045e-04  Data: 0.010 (0.014)
Train: 480 [ 450/1251 ( 36%)]  Loss: 3.260 (3.15)  Time: 0.806s, 1269.86/s  (0.795s, 1287.99/s)  LR: 1.045e-04  Data: 0.010 (0.013)
Train: 480 [ 500/1251 ( 40%)]  Loss: 3.253 (3.16)  Time: 0.807s, 1269.09/s  (0.795s, 1287.50/s)  LR: 1.045e-04  Data: 0.010 (0.013)
Train: 480 [ 550/1251 ( 44%)]  Loss: 3.011 (3.14)  Time: 0.808s, 1267.24/s  (0.795s, 1288.25/s)  LR: 1.045e-04  Data: 0.010 (0.013)
Train: 480 [ 600/1251 ( 48%)]  Loss: 3.023 (3.13)  Time: 0.805s, 1271.43/s  (0.795s, 1288.36/s)  LR: 1.045e-04  Data: 0.010 (0.013)
Train: 480 [ 650/1251 ( 52%)]  Loss: 2.966 (3.12)  Time: 0.805s, 1271.44/s  (0.795s, 1288.64/s)  LR: 1.045e-04  Data: 0.010 (0.012)
Train: 480 [ 700/1251 ( 56%)]  Loss: 3.193 (3.13)  Time: 0.774s, 1323.11/s  (0.794s, 1289.10/s)  LR: 1.045e-04  Data: 0.010 (0.012)
Train: 480 [ 750/1251 ( 60%)]  Loss: 3.176 (3.13)  Time: 0.771s, 1328.10/s  (0.793s, 1290.69/s)  LR: 1.045e-04  Data: 0.010 (0.012)
Train: 480 [ 800/1251 ( 64%)]  Loss: 3.069 (3.13)  Time: 0.772s, 1325.85/s  (0.793s, 1291.46/s)  LR: 1.045e-04  Data: 0.010 (0.012)
Train: 480 [ 850/1251 ( 68%)]  Loss: 3.056 (3.12)  Time: 0.774s, 1322.66/s  (0.793s, 1291.08/s)  LR: 1.045e-04  Data: 0.010 (0.012)
Train: 480 [ 900/1251 ( 72%)]  Loss: 3.139 (3.12)  Time: 0.772s, 1326.93/s  (0.792s, 1292.55/s)  LR: 1.045e-04  Data: 0.010 (0.012)
Train: 480 [ 950/1251 ( 76%)]  Loss: 3.259 (3.13)  Time: 0.803s, 1275.74/s  (0.792s, 1292.78/s)  LR: 1.045e-04  Data: 0.010 (0.012)
Train: 480 [1000/1251 ( 80%)]  Loss: 3.032 (3.13)  Time: 0.806s, 1271.18/s  (0.791s, 1294.05/s)  LR: 1.045e-04  Data: 0.010 (0.011)
Train: 480 [1050/1251 ( 84%)]  Loss: 3.366 (3.14)  Time: 0.772s, 1326.91/s  (0.791s, 1294.41/s)  LR: 1.045e-04  Data: 0.009 (0.011)
Train: 480 [1100/1251 ( 88%)]  Loss: 3.006 (3.13)  Time: 0.776s, 1319.89/s  (0.791s, 1295.01/s)  LR: 1.045e-04  Data: 0.010 (0.011)
Train: 480 [1150/1251 ( 92%)]  Loss: 2.973 (3.12)  Time: 0.775s, 1321.48/s  (0.791s, 1294.83/s)  LR: 1.045e-04  Data: 0.010 (0.011)
Train: 480 [1200/1251 ( 96%)]  Loss: 3.420 (3.14)  Time: 0.777s, 1318.31/s  (0.791s, 1294.88/s)  LR: 1.045e-04  Data: 0.010 (0.011)
Train: 480 [1250/1251 (100%)]  Loss: 3.198 (3.14)  Time: 0.789s, 1297.88/s  (0.790s, 1295.42/s)  LR: 1.045e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.546 (1.546)  Loss:  0.7607 (0.7607)  Acc@1: 91.4062 (91.4062)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.194 (0.571)  Loss:  0.8237 (1.1886)  Acc@1: 86.6745 (79.4920)  Acc@5: 97.9953 (94.8860)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-471.pth.tar', 79.68199995117187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-470.pth.tar', 79.61799989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-476.pth.tar', 79.56399997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-474.pth.tar', 79.55800002929688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-479.pth.tar', 79.5200000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-467.pth.tar', 79.51399989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-478.pth.tar', 79.5080000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-480.pth.tar', 79.49199989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-477.pth.tar', 79.46000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-464.pth.tar', 79.45200000244141)

Train: 481 [   0/1251 (  0%)]  Loss: 2.937 (2.94)  Time: 2.550s,  401.57/s  (2.550s,  401.57/s)  LR: 1.030e-04  Data: 1.819 (1.819)
Train: 481 [  50/1251 (  4%)]  Loss: 2.898 (2.92)  Time: 0.807s, 1269.06/s  (0.825s, 1241.12/s)  LR: 1.030e-04  Data: 0.010 (0.049)
Train: 481 [ 100/1251 (  8%)]  Loss: 2.762 (2.87)  Time: 0.775s, 1321.74/s  (0.809s, 1265.73/s)  LR: 1.030e-04  Data: 0.010 (0.029)
Train: 481 [ 150/1251 ( 12%)]  Loss: 3.569 (3.04)  Time: 0.812s, 1261.36/s  (0.802s, 1277.11/s)  LR: 1.030e-04  Data: 0.010 (0.023)
Train: 481 [ 200/1251 ( 16%)]  Loss: 3.290 (3.09)  Time: 0.772s, 1326.64/s  (0.798s, 1282.79/s)  LR: 1.030e-04  Data: 0.010 (0.020)
Train: 481 [ 250/1251 ( 20%)]  Loss: 3.080 (3.09)  Time: 0.775s, 1321.96/s  (0.797s, 1285.60/s)  LR: 1.030e-04  Data: 0.010 (0.018)
Train: 481 [ 300/1251 ( 24%)]  Loss: 3.277 (3.12)  Time: 0.856s, 1195.81/s  (0.795s, 1288.71/s)  LR: 1.030e-04  Data: 0.010 (0.016)
Train: 481 [ 350/1251 ( 28%)]  Loss: 2.707 (3.07)  Time: 0.807s, 1268.82/s  (0.795s, 1288.09/s)  LR: 1.030e-04  Data: 0.010 (0.015)
Train: 481 [ 400/1251 ( 32%)]  Loss: 2.968 (3.05)  Time: 0.800s, 1279.63/s  (0.794s, 1289.25/s)  LR: 1.030e-04  Data: 0.010 (0.015)
Train: 481 [ 450/1251 ( 36%)]  Loss: 3.227 (3.07)  Time: 0.772s, 1325.92/s  (0.794s, 1290.01/s)  LR: 1.030e-04  Data: 0.010 (0.014)
Train: 481 [ 500/1251 ( 40%)]  Loss: 2.980 (3.06)  Time: 0.773s, 1324.48/s  (0.793s, 1291.47/s)  LR: 1.030e-04  Data: 0.010 (0.014)
Train: 481 [ 550/1251 ( 44%)]  Loss: 3.241 (3.08)  Time: 0.807s, 1269.07/s  (0.791s, 1293.77/s)  LR: 1.030e-04  Data: 0.010 (0.013)
Train: 481 [ 600/1251 ( 48%)]  Loss: 3.134 (3.08)  Time: 0.808s, 1266.88/s  (0.792s, 1292.88/s)  LR: 1.030e-04  Data: 0.010 (0.013)
Train: 481 [ 650/1251 ( 52%)]  Loss: 2.710 (3.06)  Time: 0.807s, 1268.16/s  (0.792s, 1293.50/s)  LR: 1.030e-04  Data: 0.010 (0.013)
Train: 481 [ 700/1251 ( 56%)]  Loss: 2.872 (3.04)  Time: 0.772s, 1326.07/s  (0.792s, 1293.01/s)  LR: 1.030e-04  Data: 0.010 (0.013)
Train: 481 [ 750/1251 ( 60%)]  Loss: 3.251 (3.06)  Time: 0.808s, 1267.81/s  (0.792s, 1292.51/s)  LR: 1.030e-04  Data: 0.010 (0.012)
Train: 481 [ 800/1251 ( 64%)]  Loss: 3.337 (3.07)  Time: 0.771s, 1328.44/s  (0.793s, 1291.88/s)  LR: 1.030e-04  Data: 0.010 (0.012)
Train: 481 [ 850/1251 ( 68%)]  Loss: 3.233 (3.08)  Time: 0.771s, 1327.76/s  (0.792s, 1292.62/s)  LR: 1.030e-04  Data: 0.010 (0.012)
Train: 481 [ 900/1251 ( 72%)]  Loss: 2.994 (3.08)  Time: 0.807s, 1268.64/s  (0.792s, 1292.30/s)  LR: 1.030e-04  Data: 0.010 (0.012)
Train: 481 [ 950/1251 ( 76%)]  Loss: 3.293 (3.09)  Time: 0.808s, 1267.02/s  (0.793s, 1291.98/s)  LR: 1.030e-04  Data: 0.010 (0.012)
Train: 481 [1000/1251 ( 80%)]  Loss: 2.893 (3.08)  Time: 0.808s, 1267.88/s  (0.793s, 1291.58/s)  LR: 1.030e-04  Data: 0.010 (0.012)
Train: 481 [1050/1251 ( 84%)]  Loss: 2.947 (3.07)  Time: 0.807s, 1268.72/s  (0.793s, 1291.36/s)  LR: 1.030e-04  Data: 0.010 (0.012)
Train: 481 [1100/1251 ( 88%)]  Loss: 3.341 (3.08)  Time: 0.808s, 1268.08/s  (0.793s, 1292.00/s)  LR: 1.030e-04  Data: 0.010 (0.012)
Train: 481 [1150/1251 ( 92%)]  Loss: 3.097 (3.08)  Time: 0.813s, 1260.18/s  (0.792s, 1292.41/s)  LR: 1.030e-04  Data: 0.010 (0.011)
Train: 481 [1200/1251 ( 96%)]  Loss: 3.058 (3.08)  Time: 0.771s, 1327.82/s  (0.792s, 1292.30/s)  LR: 1.030e-04  Data: 0.010 (0.011)
Train: 481 [1250/1251 (100%)]  Loss: 2.806 (3.07)  Time: 0.792s, 1293.62/s  (0.793s, 1291.71/s)  LR: 1.030e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.539 (1.539)  Loss:  0.6826 (0.6826)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.194 (0.557)  Loss:  0.7876 (1.1243)  Acc@1: 87.1462 (79.5060)  Acc@5: 97.7594 (95.0040)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-471.pth.tar', 79.68199995117187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-470.pth.tar', 79.61799989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-476.pth.tar', 79.56399997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-474.pth.tar', 79.55800002929688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-479.pth.tar', 79.5200000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-467.pth.tar', 79.51399989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-478.pth.tar', 79.5080000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-481.pth.tar', 79.50599997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-480.pth.tar', 79.49199989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-477.pth.tar', 79.46000002685547)

Train: 482 [   0/1251 (  0%)]  Loss: 2.684 (2.68)  Time: 2.332s,  439.11/s  (2.332s,  439.11/s)  LR: 1.015e-04  Data: 1.598 (1.598)
Train: 482 [  50/1251 (  4%)]  Loss: 3.253 (2.97)  Time: 0.806s, 1270.12/s  (0.821s, 1248.01/s)  LR: 1.015e-04  Data: 0.009 (0.045)
Train: 482 [ 100/1251 (  8%)]  Loss: 3.084 (3.01)  Time: 0.807s, 1268.97/s  (0.808s, 1267.94/s)  LR: 1.015e-04  Data: 0.010 (0.027)
Train: 482 [ 150/1251 ( 12%)]  Loss: 3.185 (3.05)  Time: 0.772s, 1327.26/s  (0.800s, 1280.22/s)  LR: 1.015e-04  Data: 0.010 (0.022)
Train: 482 [ 200/1251 ( 16%)]  Loss: 3.435 (3.13)  Time: 0.807s, 1268.55/s  (0.798s, 1283.08/s)  LR: 1.015e-04  Data: 0.010 (0.019)
Train: 482 [ 250/1251 ( 20%)]  Loss: 3.082 (3.12)  Time: 0.774s, 1322.53/s  (0.795s, 1287.54/s)  LR: 1.015e-04  Data: 0.010 (0.017)
Train: 482 [ 300/1251 ( 24%)]  Loss: 2.925 (3.09)  Time: 0.773s, 1325.55/s  (0.794s, 1290.22/s)  LR: 1.015e-04  Data: 0.010 (0.016)
Train: 482 [ 350/1251 ( 28%)]  Loss: 3.229 (3.11)  Time: 0.807s, 1269.58/s  (0.792s, 1293.39/s)  LR: 1.015e-04  Data: 0.010 (0.015)
Train: 482 [ 400/1251 ( 32%)]  Loss: 3.283 (3.13)  Time: 0.807s, 1269.63/s  (0.793s, 1291.58/s)  LR: 1.015e-04  Data: 0.010 (0.014)
Train: 482 [ 450/1251 ( 36%)]  Loss: 3.159 (3.13)  Time: 0.808s, 1266.97/s  (0.794s, 1289.69/s)  LR: 1.015e-04  Data: 0.010 (0.014)
Train: 482 [ 500/1251 ( 40%)]  Loss: 3.171 (3.14)  Time: 0.772s, 1326.31/s  (0.794s, 1289.21/s)  LR: 1.015e-04  Data: 0.010 (0.013)
Train: 482 [ 550/1251 ( 44%)]  Loss: 3.201 (3.14)  Time: 0.771s, 1327.55/s  (0.793s, 1290.52/s)  LR: 1.015e-04  Data: 0.010 (0.013)
Train: 482 [ 600/1251 ( 48%)]  Loss: 3.291 (3.15)  Time: 0.776s, 1318.95/s  (0.793s, 1292.06/s)  LR: 1.015e-04  Data: 0.010 (0.013)
Train: 482 [ 650/1251 ( 52%)]  Loss: 3.053 (3.15)  Time: 0.773s, 1325.05/s  (0.792s, 1292.56/s)  LR: 1.015e-04  Data: 0.010 (0.013)
Train: 482 [ 700/1251 ( 56%)]  Loss: 2.953 (3.13)  Time: 0.808s, 1266.99/s  (0.791s, 1293.89/s)  LR: 1.015e-04  Data: 0.010 (0.012)
Train: 482 [ 750/1251 ( 60%)]  Loss: 3.242 (3.14)  Time: 0.772s, 1326.40/s  (0.791s, 1293.82/s)  LR: 1.015e-04  Data: 0.010 (0.012)
Train: 482 [ 800/1251 ( 64%)]  Loss: 3.004 (3.13)  Time: 0.772s, 1326.26/s  (0.791s, 1293.84/s)  LR: 1.015e-04  Data: 0.009 (0.012)
Train: 482 [ 850/1251 ( 68%)]  Loss: 3.219 (3.14)  Time: 0.772s, 1326.38/s  (0.792s, 1292.93/s)  LR: 1.015e-04  Data: 0.010 (0.012)
Train: 482 [ 900/1251 ( 72%)]  Loss: 3.174 (3.14)  Time: 0.807s, 1268.32/s  (0.792s, 1293.05/s)  LR: 1.015e-04  Data: 0.010 (0.012)
Train: 482 [ 950/1251 ( 76%)]  Loss: 3.053 (3.13)  Time: 0.772s, 1325.96/s  (0.791s, 1294.09/s)  LR: 1.015e-04  Data: 0.010 (0.012)
Train: 482 [1000/1251 ( 80%)]  Loss: 2.849 (3.12)  Time: 0.774s, 1323.13/s  (0.791s, 1293.75/s)  LR: 1.015e-04  Data: 0.010 (0.012)
Train: 482 [1050/1251 ( 84%)]  Loss: 3.216 (3.12)  Time: 0.808s, 1267.61/s  (0.792s, 1293.64/s)  LR: 1.015e-04  Data: 0.010 (0.011)
Train: 482 [1100/1251 ( 88%)]  Loss: 3.319 (3.13)  Time: 0.807s, 1269.10/s  (0.791s, 1294.55/s)  LR: 1.015e-04  Data: 0.010 (0.011)
Train: 482 [1150/1251 ( 92%)]  Loss: 3.227 (3.14)  Time: 0.772s, 1326.77/s  (0.791s, 1295.11/s)  LR: 1.015e-04  Data: 0.010 (0.011)
Train: 482 [1200/1251 ( 96%)]  Loss: 2.862 (3.13)  Time: 0.771s, 1327.72/s  (0.791s, 1294.95/s)  LR: 1.015e-04  Data: 0.010 (0.011)
Train: 482 [1250/1251 (100%)]  Loss: 2.855 (3.12)  Time: 0.755s, 1355.78/s  (0.791s, 1295.13/s)  LR: 1.015e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.530 (1.530)  Loss:  0.6572 (0.6572)  Acc@1: 91.7969 (91.7969)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.194 (0.561)  Loss:  0.7524 (1.1119)  Acc@1: 87.0283 (79.3680)  Acc@5: 97.9953 (94.8920)
Train: 483 [   0/1251 (  0%)]  Loss: 2.901 (2.90)  Time: 2.145s,  477.37/s  (2.145s,  477.37/s)  LR: 1.000e-04  Data: 1.431 (1.431)
Train: 483 [  50/1251 (  4%)]  Loss: 3.246 (3.07)  Time: 0.775s, 1321.45/s  (0.811s, 1263.19/s)  LR: 1.000e-04  Data: 0.010 (0.042)
Train: 483 [ 100/1251 (  8%)]  Loss: 3.130 (3.09)  Time: 0.810s, 1264.41/s  (0.801s, 1278.07/s)  LR: 1.000e-04  Data: 0.010 (0.026)
Train: 483 [ 150/1251 ( 12%)]  Loss: 2.894 (3.04)  Time: 0.771s, 1327.89/s  (0.797s, 1285.32/s)  LR: 1.000e-04  Data: 0.010 (0.021)
Train: 483 [ 200/1251 ( 16%)]  Loss: 3.208 (3.08)  Time: 0.807s, 1268.86/s  (0.794s, 1289.87/s)  LR: 1.000e-04  Data: 0.010 (0.018)
Train: 483 [ 250/1251 ( 20%)]  Loss: 2.714 (3.02)  Time: 0.773s, 1324.36/s  (0.794s, 1290.45/s)  LR: 1.000e-04  Data: 0.010 (0.016)
Train: 483 [ 300/1251 ( 24%)]  Loss: 2.917 (3.00)  Time: 0.807s, 1268.58/s  (0.794s, 1289.42/s)  LR: 1.000e-04  Data: 0.010 (0.015)
Train: 483 [ 350/1251 ( 28%)]  Loss: 2.994 (3.00)  Time: 0.807s, 1268.52/s  (0.793s, 1290.86/s)  LR: 1.000e-04  Data: 0.010 (0.015)
Train: 483 [ 400/1251 ( 32%)]  Loss: 3.369 (3.04)  Time: 0.772s, 1325.80/s  (0.793s, 1290.56/s)  LR: 1.000e-04  Data: 0.010 (0.014)
Train: 483 [ 450/1251 ( 36%)]  Loss: 3.099 (3.05)  Time: 0.773s, 1325.40/s  (0.793s, 1291.56/s)  LR: 1.000e-04  Data: 0.010 (0.014)
Train: 483 [ 500/1251 ( 40%)]  Loss: 3.296 (3.07)  Time: 0.807s, 1269.61/s  (0.792s, 1293.70/s)  LR: 1.000e-04  Data: 0.010 (0.013)
Train: 483 [ 550/1251 ( 44%)]  Loss: 2.701 (3.04)  Time: 0.772s, 1326.60/s  (0.792s, 1292.24/s)  LR: 1.000e-04  Data: 0.010 (0.013)
Train: 483 [ 600/1251 ( 48%)]  Loss: 2.839 (3.02)  Time: 0.807s, 1268.36/s  (0.793s, 1291.23/s)  LR: 1.000e-04  Data: 0.010 (0.013)
Train: 483 [ 650/1251 ( 52%)]  Loss: 3.279 (3.04)  Time: 0.774s, 1322.82/s  (0.794s, 1289.95/s)  LR: 1.000e-04  Data: 0.010 (0.012)
Train: 483 [ 700/1251 ( 56%)]  Loss: 3.134 (3.05)  Time: 0.807s, 1268.98/s  (0.794s, 1289.95/s)  LR: 1.000e-04  Data: 0.010 (0.012)
Train: 483 [ 750/1251 ( 60%)]  Loss: 3.044 (3.05)  Time: 0.775s, 1321.27/s  (0.794s, 1290.14/s)  LR: 1.000e-04  Data: 0.010 (0.012)
Train: 483 [ 800/1251 ( 64%)]  Loss: 3.233 (3.06)  Time: 0.807s, 1268.70/s  (0.794s, 1290.13/s)  LR: 1.000e-04  Data: 0.010 (0.012)
Train: 483 [ 850/1251 ( 68%)]  Loss: 2.711 (3.04)  Time: 0.773s, 1324.34/s  (0.794s, 1289.83/s)  LR: 1.000e-04  Data: 0.010 (0.012)
Train: 483 [ 900/1251 ( 72%)]  Loss: 3.401 (3.06)  Time: 0.774s, 1322.15/s  (0.794s, 1289.59/s)  LR: 1.000e-04  Data: 0.010 (0.012)
Train: 483 [ 950/1251 ( 76%)]  Loss: 3.199 (3.07)  Time: 0.775s, 1321.71/s  (0.794s, 1289.09/s)  LR: 1.000e-04  Data: 0.010 (0.012)
Train: 483 [1000/1251 ( 80%)]  Loss: 2.849 (3.06)  Time: 0.808s, 1267.89/s  (0.794s, 1288.99/s)  LR: 1.000e-04  Data: 0.010 (0.012)
Train: 483 [1050/1251 ( 84%)]  Loss: 2.952 (3.05)  Time: 0.770s, 1329.32/s  (0.794s, 1289.58/s)  LR: 1.000e-04  Data: 0.010 (0.011)
Train: 483 [1100/1251 ( 88%)]  Loss: 3.283 (3.06)  Time: 0.808s, 1267.02/s  (0.794s, 1290.05/s)  LR: 1.000e-04  Data: 0.010 (0.011)
Train: 483 [1150/1251 ( 92%)]  Loss: 3.284 (3.07)  Time: 0.772s, 1326.06/s  (0.794s, 1290.13/s)  LR: 1.000e-04  Data: 0.010 (0.011)
Train: 483 [1200/1251 ( 96%)]  Loss: 2.865 (3.06)  Time: 0.772s, 1325.77/s  (0.794s, 1290.41/s)  LR: 1.000e-04  Data: 0.010 (0.011)
Train: 483 [1250/1251 (100%)]  Loss: 3.019 (3.06)  Time: 0.792s, 1292.86/s  (0.794s, 1290.27/s)  LR: 1.000e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.527 (1.527)  Loss:  0.7178 (0.7178)  Acc@1: 91.8945 (91.8945)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.194 (0.559)  Loss:  0.8223 (1.1555)  Acc@1: 85.9670 (79.6680)  Acc@5: 97.5236 (94.9480)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-471.pth.tar', 79.68199995117187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-483.pth.tar', 79.66799998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-470.pth.tar', 79.61799989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-476.pth.tar', 79.56399997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-474.pth.tar', 79.55800002929688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-479.pth.tar', 79.5200000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-467.pth.tar', 79.51399989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-478.pth.tar', 79.5080000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-481.pth.tar', 79.50599997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-480.pth.tar', 79.49199989990234)

Train: 484 [   0/1251 (  0%)]  Loss: 3.092 (3.09)  Time: 2.146s,  477.27/s  (2.146s,  477.27/s)  LR: 9.853e-05  Data: 1.414 (1.414)
Train: 484 [  50/1251 (  4%)]  Loss: 3.389 (3.24)  Time: 0.793s, 1290.94/s  (0.826s, 1239.33/s)  LR: 9.853e-05  Data: 0.010 (0.040)
Train: 484 [ 100/1251 (  8%)]  Loss: 3.015 (3.17)  Time: 0.807s, 1269.60/s  (0.815s, 1256.41/s)  LR: 9.853e-05  Data: 0.010 (0.025)
Train: 484 [ 150/1251 ( 12%)]  Loss: 3.132 (3.16)  Time: 0.772s, 1326.62/s  (0.808s, 1267.96/s)  LR: 9.853e-05  Data: 0.010 (0.020)
Train: 484 [ 200/1251 ( 16%)]  Loss: 3.064 (3.14)  Time: 0.809s, 1266.31/s  (0.802s, 1277.03/s)  LR: 9.853e-05  Data: 0.010 (0.018)
Train: 484 [ 250/1251 ( 20%)]  Loss: 3.194 (3.15)  Time: 0.807s, 1269.61/s  (0.801s, 1277.80/s)  LR: 9.853e-05  Data: 0.010 (0.016)
Train: 484 [ 300/1251 ( 24%)]  Loss: 3.025 (3.13)  Time: 0.807s, 1269.48/s  (0.799s, 1281.59/s)  LR: 9.853e-05  Data: 0.010 (0.015)
Train: 484 [ 350/1251 ( 28%)]  Loss: 3.143 (3.13)  Time: 0.806s, 1270.91/s  (0.798s, 1283.80/s)  LR: 9.853e-05  Data: 0.010 (0.014)
Train: 484 [ 400/1251 ( 32%)]  Loss: 3.119 (3.13)  Time: 0.807s, 1269.48/s  (0.798s, 1283.15/s)  LR: 9.853e-05  Data: 0.010 (0.014)
Train: 484 [ 450/1251 ( 36%)]  Loss: 3.285 (3.15)  Time: 0.806s, 1269.69/s  (0.798s, 1283.39/s)  LR: 9.853e-05  Data: 0.010 (0.013)
Train: 484 [ 500/1251 ( 40%)]  Loss: 2.853 (3.12)  Time: 0.772s, 1325.75/s  (0.796s, 1286.26/s)  LR: 9.853e-05  Data: 0.010 (0.013)
Train: 484 [ 550/1251 ( 44%)]  Loss: 3.007 (3.11)  Time: 0.772s, 1325.85/s  (0.794s, 1289.17/s)  LR: 9.853e-05  Data: 0.010 (0.013)
Train: 484 [ 600/1251 ( 48%)]  Loss: 3.047 (3.11)  Time: 0.808s, 1267.17/s  (0.793s, 1290.53/s)  LR: 9.853e-05  Data: 0.010 (0.012)
Train: 484 [ 650/1251 ( 52%)]  Loss: 2.981 (3.10)  Time: 0.809s, 1265.86/s  (0.793s, 1290.90/s)  LR: 9.853e-05  Data: 0.010 (0.012)
Train: 484 [ 700/1251 ( 56%)]  Loss: 3.205 (3.10)  Time: 0.807s, 1269.33/s  (0.793s, 1291.68/s)  LR: 9.853e-05  Data: 0.010 (0.012)
Train: 484 [ 750/1251 ( 60%)]  Loss: 2.811 (3.09)  Time: 0.808s, 1267.21/s  (0.793s, 1291.79/s)  LR: 9.853e-05  Data: 0.010 (0.012)
Train: 484 [ 800/1251 ( 64%)]  Loss: 3.211 (3.09)  Time: 0.805s, 1271.33/s  (0.793s, 1291.93/s)  LR: 9.853e-05  Data: 0.010 (0.012)
Train: 484 [ 850/1251 ( 68%)]  Loss: 2.885 (3.08)  Time: 0.771s, 1328.41/s  (0.792s, 1292.22/s)  LR: 9.853e-05  Data: 0.010 (0.012)
Train: 484 [ 900/1251 ( 72%)]  Loss: 3.421 (3.10)  Time: 0.808s, 1267.28/s  (0.793s, 1291.43/s)  LR: 9.853e-05  Data: 0.010 (0.012)
Train: 484 [ 950/1251 ( 76%)]  Loss: 3.033 (3.10)  Time: 0.811s, 1262.31/s  (0.793s, 1290.91/s)  LR: 9.853e-05  Data: 0.010 (0.012)
Train: 484 [1000/1251 ( 80%)]  Loss: 3.115 (3.10)  Time: 0.809s, 1266.25/s  (0.794s, 1290.32/s)  LR: 9.853e-05  Data: 0.010 (0.011)
Train: 484 [1050/1251 ( 84%)]  Loss: 3.059 (3.09)  Time: 0.773s, 1324.76/s  (0.793s, 1290.67/s)  LR: 9.853e-05  Data: 0.010 (0.011)
Train: 484 [1100/1251 ( 88%)]  Loss: 3.005 (3.09)  Time: 0.771s, 1327.86/s  (0.793s, 1291.80/s)  LR: 9.853e-05  Data: 0.009 (0.011)
Train: 484 [1150/1251 ( 92%)]  Loss: 3.382 (3.10)  Time: 0.772s, 1326.81/s  (0.793s, 1291.88/s)  LR: 9.853e-05  Data: 0.010 (0.011)
Train: 484 [1200/1251 ( 96%)]  Loss: 3.168 (3.11)  Time: 0.809s, 1266.26/s  (0.793s, 1291.67/s)  LR: 9.853e-05  Data: 0.010 (0.011)
Train: 484 [1250/1251 (100%)]  Loss: 3.265 (3.11)  Time: 0.798s, 1282.85/s  (0.793s, 1291.13/s)  LR: 9.853e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.560 (1.560)  Loss:  0.7139 (0.7139)  Acc@1: 91.6016 (91.6016)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.194 (0.559)  Loss:  0.7905 (1.1235)  Acc@1: 86.0849 (79.5780)  Acc@5: 98.1132 (95.0060)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-471.pth.tar', 79.68199995117187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-483.pth.tar', 79.66799998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-470.pth.tar', 79.61799989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-484.pth.tar', 79.57800003173828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-476.pth.tar', 79.56399997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-474.pth.tar', 79.55800002929688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-479.pth.tar', 79.5200000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-467.pth.tar', 79.51399989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-478.pth.tar', 79.5080000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-481.pth.tar', 79.50599997558594)

Train: 485 [   0/1251 (  0%)]  Loss: 2.907 (2.91)  Time: 2.202s,  464.96/s  (2.202s,  464.96/s)  LR: 9.706e-05  Data: 1.470 (1.470)
Train: 485 [  50/1251 (  4%)]  Loss: 3.245 (3.08)  Time: 0.774s, 1323.29/s  (0.821s, 1247.83/s)  LR: 9.706e-05  Data: 0.010 (0.043)
Train: 485 [ 100/1251 (  8%)]  Loss: 2.769 (2.97)  Time: 0.812s, 1260.65/s  (0.809s, 1266.24/s)  LR: 9.706e-05  Data: 0.010 (0.027)
Train: 485 [ 150/1251 ( 12%)]  Loss: 2.991 (2.98)  Time: 0.773s, 1325.49/s  (0.803s, 1275.55/s)  LR: 9.706e-05  Data: 0.010 (0.021)
Train: 485 [ 200/1251 ( 16%)]  Loss: 3.033 (2.99)  Time: 0.807s, 1268.31/s  (0.798s, 1282.82/s)  LR: 9.706e-05  Data: 0.010 (0.018)
Train: 485 [ 250/1251 ( 20%)]  Loss: 2.823 (2.96)  Time: 0.774s, 1323.23/s  (0.795s, 1288.04/s)  LR: 9.706e-05  Data: 0.010 (0.017)
Train: 485 [ 300/1251 ( 24%)]  Loss: 3.435 (3.03)  Time: 0.772s, 1326.14/s  (0.793s, 1291.70/s)  LR: 9.706e-05  Data: 0.010 (0.016)
Train: 485 [ 350/1251 ( 28%)]  Loss: 3.293 (3.06)  Time: 0.773s, 1325.54/s  (0.792s, 1293.63/s)  LR: 9.706e-05  Data: 0.010 (0.015)
Train: 485 [ 400/1251 ( 32%)]  Loss: 3.024 (3.06)  Time: 0.774s, 1322.71/s  (0.792s, 1292.19/s)  LR: 9.706e-05  Data: 0.010 (0.014)
Train: 485 [ 450/1251 ( 36%)]  Loss: 3.064 (3.06)  Time: 0.774s, 1322.55/s  (0.792s, 1293.05/s)  LR: 9.706e-05  Data: 0.010 (0.014)
Train: 485 [ 500/1251 ( 40%)]  Loss: 3.237 (3.07)  Time: 0.775s, 1322.03/s  (0.791s, 1294.06/s)  LR: 9.706e-05  Data: 0.010 (0.013)
Train: 485 [ 550/1251 ( 44%)]  Loss: 3.161 (3.08)  Time: 0.773s, 1323.90/s  (0.790s, 1296.15/s)  LR: 9.706e-05  Data: 0.010 (0.013)
Train: 485 [ 600/1251 ( 48%)]  Loss: 3.117 (3.08)  Time: 0.772s, 1326.68/s  (0.789s, 1297.06/s)  LR: 9.706e-05  Data: 0.010 (0.013)
Train: 485 [ 650/1251 ( 52%)]  Loss: 3.193 (3.09)  Time: 0.772s, 1327.13/s  (0.789s, 1297.55/s)  LR: 9.706e-05  Data: 0.010 (0.012)
Train: 485 [ 700/1251 ( 56%)]  Loss: 2.865 (3.08)  Time: 0.771s, 1327.98/s  (0.789s, 1297.56/s)  LR: 9.706e-05  Data: 0.010 (0.012)
Train: 485 [ 750/1251 ( 60%)]  Loss: 3.262 (3.09)  Time: 0.773s, 1325.14/s  (0.790s, 1296.43/s)  LR: 9.706e-05  Data: 0.010 (0.012)
Train: 485 [ 800/1251 ( 64%)]  Loss: 3.177 (3.09)  Time: 0.775s, 1321.50/s  (0.790s, 1295.60/s)  LR: 9.706e-05  Data: 0.010 (0.012)
Train: 485 [ 850/1251 ( 68%)]  Loss: 3.435 (3.11)  Time: 0.772s, 1326.29/s  (0.790s, 1295.97/s)  LR: 9.706e-05  Data: 0.010 (0.012)
Train: 485 [ 900/1251 ( 72%)]  Loss: 2.940 (3.10)  Time: 0.880s, 1163.48/s  (0.790s, 1295.51/s)  LR: 9.706e-05  Data: 0.010 (0.012)
Train: 485 [ 950/1251 ( 76%)]  Loss: 2.936 (3.10)  Time: 0.772s, 1326.26/s  (0.791s, 1295.15/s)  LR: 9.706e-05  Data: 0.010 (0.012)
Train: 485 [1000/1251 ( 80%)]  Loss: 3.255 (3.10)  Time: 0.807s, 1268.16/s  (0.791s, 1295.01/s)  LR: 9.706e-05  Data: 0.010 (0.012)
Train: 485 [1050/1251 ( 84%)]  Loss: 3.105 (3.10)  Time: 0.774s, 1323.79/s  (0.791s, 1295.24/s)  LR: 9.706e-05  Data: 0.010 (0.012)
Train: 485 [1100/1251 ( 88%)]  Loss: 2.809 (3.09)  Time: 0.807s, 1268.46/s  (0.791s, 1295.31/s)  LR: 9.706e-05  Data: 0.010 (0.011)
Train: 485 [1150/1251 ( 92%)]  Loss: 3.344 (3.10)  Time: 0.774s, 1322.39/s  (0.790s, 1296.25/s)  LR: 9.706e-05  Data: 0.010 (0.011)
Train: 485 [1200/1251 ( 96%)]  Loss: 3.119 (3.10)  Time: 0.808s, 1267.56/s  (0.790s, 1295.85/s)  LR: 9.706e-05  Data: 0.009 (0.011)
Train: 485 [1250/1251 (100%)]  Loss: 3.133 (3.10)  Time: 0.758s, 1351.39/s  (0.790s, 1296.32/s)  LR: 9.706e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.593 (1.593)  Loss:  0.7227 (0.7227)  Acc@1: 91.6016 (91.6016)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.194 (0.558)  Loss:  0.7861 (1.1490)  Acc@1: 87.9717 (79.6160)  Acc@5: 97.6415 (94.9360)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-471.pth.tar', 79.68199995117187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-483.pth.tar', 79.66799998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-470.pth.tar', 79.61799989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-485.pth.tar', 79.61599994628907)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-484.pth.tar', 79.57800003173828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-476.pth.tar', 79.56399997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-474.pth.tar', 79.55800002929688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-479.pth.tar', 79.5200000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-467.pth.tar', 79.51399989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-478.pth.tar', 79.5080000024414)

Train: 486 [   0/1251 (  0%)]  Loss: 3.084 (3.08)  Time: 2.284s,  448.26/s  (2.284s,  448.26/s)  LR: 9.560e-05  Data: 1.565 (1.565)
Train: 486 [  50/1251 (  4%)]  Loss: 3.023 (3.05)  Time: 0.808s, 1267.81/s  (0.829s, 1234.94/s)  LR: 9.560e-05  Data: 0.010 (0.042)
Train: 486 [ 100/1251 (  8%)]  Loss: 2.908 (3.00)  Time: 0.809s, 1266.21/s  (0.808s, 1267.55/s)  LR: 9.560e-05  Data: 0.010 (0.026)
Train: 486 [ 150/1251 ( 12%)]  Loss: 3.142 (3.04)  Time: 0.806s, 1269.70/s  (0.801s, 1278.27/s)  LR: 9.560e-05  Data: 0.010 (0.021)
Train: 486 [ 200/1251 ( 16%)]  Loss: 2.818 (3.00)  Time: 0.806s, 1270.08/s  (0.800s, 1280.79/s)  LR: 9.560e-05  Data: 0.010 (0.018)
Train: 486 [ 250/1251 ( 20%)]  Loss: 3.042 (3.00)  Time: 0.807s, 1268.36/s  (0.797s, 1284.70/s)  LR: 9.560e-05  Data: 0.010 (0.016)
Train: 486 [ 300/1251 ( 24%)]  Loss: 3.100 (3.02)  Time: 0.781s, 1311.56/s  (0.795s, 1288.10/s)  LR: 9.560e-05  Data: 0.010 (0.015)
Train: 486 [ 350/1251 ( 28%)]  Loss: 3.189 (3.04)  Time: 0.772s, 1326.35/s  (0.793s, 1291.53/s)  LR: 9.560e-05  Data: 0.010 (0.014)
Train: 486 [ 400/1251 ( 32%)]  Loss: 2.885 (3.02)  Time: 0.773s, 1324.91/s  (0.793s, 1291.69/s)  LR: 9.560e-05  Data: 0.010 (0.014)
Train: 486 [ 450/1251 ( 36%)]  Loss: 3.023 (3.02)  Time: 0.806s, 1270.63/s  (0.793s, 1291.89/s)  LR: 9.560e-05  Data: 0.010 (0.013)
Train: 486 [ 500/1251 ( 40%)]  Loss: 3.141 (3.03)  Time: 0.809s, 1266.10/s  (0.792s, 1292.79/s)  LR: 9.560e-05  Data: 0.009 (0.013)
Train: 486 [ 550/1251 ( 44%)]  Loss: 3.229 (3.05)  Time: 0.807s, 1269.62/s  (0.791s, 1294.99/s)  LR: 9.560e-05  Data: 0.010 (0.013)
Train: 486 [ 600/1251 ( 48%)]  Loss: 3.192 (3.06)  Time: 0.779s, 1313.81/s  (0.793s, 1291.42/s)  LR: 9.560e-05  Data: 0.010 (0.013)
Train: 486 [ 650/1251 ( 52%)]  Loss: 2.855 (3.05)  Time: 0.779s, 1314.86/s  (0.792s, 1292.36/s)  LR: 9.560e-05  Data: 0.010 (0.012)
Train: 486 [ 700/1251 ( 56%)]  Loss: 3.347 (3.07)  Time: 0.779s, 1315.01/s  (0.794s, 1289.93/s)  LR: 9.560e-05  Data: 0.010 (0.012)
Train: 486 [ 750/1251 ( 60%)]  Loss: 3.348 (3.08)  Time: 0.791s, 1293.76/s  (0.794s, 1289.94/s)  LR: 9.560e-05  Data: 0.012 (0.012)
Train: 486 [ 800/1251 ( 64%)]  Loss: 3.179 (3.09)  Time: 0.820s, 1249.36/s  (0.794s, 1288.99/s)  LR: 9.560e-05  Data: 0.010 (0.012)
Train: 486 [ 850/1251 ( 68%)]  Loss: 3.321 (3.10)  Time: 0.811s, 1263.27/s  (0.795s, 1287.25/s)  LR: 9.560e-05  Data: 0.010 (0.012)
Train: 486 [ 900/1251 ( 72%)]  Loss: 2.778 (3.08)  Time: 0.773s, 1324.80/s  (0.795s, 1287.93/s)  LR: 9.560e-05  Data: 0.009 (0.012)
Train: 486 [ 950/1251 ( 76%)]  Loss: 3.071 (3.08)  Time: 0.782s, 1310.26/s  (0.794s, 1288.93/s)  LR: 9.560e-05  Data: 0.009 (0.012)
Train: 486 [1000/1251 ( 80%)]  Loss: 2.990 (3.08)  Time: 0.807s, 1268.42/s  (0.795s, 1288.77/s)  LR: 9.560e-05  Data: 0.010 (0.012)
Train: 486 [1050/1251 ( 84%)]  Loss: 2.942 (3.07)  Time: 0.820s, 1248.90/s  (0.795s, 1288.33/s)  LR: 9.560e-05  Data: 0.009 (0.011)
Train: 486 [1100/1251 ( 88%)]  Loss: 3.352 (3.09)  Time: 0.809s, 1265.64/s  (0.795s, 1288.47/s)  LR: 9.560e-05  Data: 0.010 (0.011)
Train: 486 [1150/1251 ( 92%)]  Loss: 3.214 (3.09)  Time: 0.822s, 1245.86/s  (0.795s, 1288.08/s)  LR: 9.560e-05  Data: 0.010 (0.011)
Train: 486 [1200/1251 ( 96%)]  Loss: 3.018 (3.09)  Time: 0.810s, 1264.96/s  (0.795s, 1287.47/s)  LR: 9.560e-05  Data: 0.010 (0.011)
Train: 486 [1250/1251 (100%)]  Loss: 3.430 (3.10)  Time: 0.758s, 1350.38/s  (0.795s, 1288.21/s)  LR: 9.560e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.610 (1.610)  Loss:  0.6758 (0.6758)  Acc@1: 90.7227 (90.7227)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.194 (0.566)  Loss:  0.7803 (1.1175)  Acc@1: 86.5566 (79.7220)  Acc@5: 97.8774 (94.9320)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-486.pth.tar', 79.72199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-471.pth.tar', 79.68199995117187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-483.pth.tar', 79.66799998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-470.pth.tar', 79.61799989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-485.pth.tar', 79.61599994628907)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-484.pth.tar', 79.57800003173828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-476.pth.tar', 79.56399997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-474.pth.tar', 79.55800002929688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-479.pth.tar', 79.5200000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-467.pth.tar', 79.51399989990234)

Train: 487 [   0/1251 (  0%)]  Loss: 2.962 (2.96)  Time: 2.374s,  431.28/s  (2.374s,  431.28/s)  LR: 9.414e-05  Data: 1.604 (1.604)
Train: 487 [  50/1251 (  4%)]  Loss: 2.787 (2.87)  Time: 0.840s, 1219.72/s  (0.823s, 1244.00/s)  LR: 9.414e-05  Data: 0.010 (0.049)
Train: 487 [ 100/1251 (  8%)]  Loss: 3.006 (2.92)  Time: 0.771s, 1327.85/s  (0.808s, 1267.05/s)  LR: 9.414e-05  Data: 0.010 (0.030)
Train: 487 [ 150/1251 ( 12%)]  Loss: 3.305 (3.01)  Time: 0.851s, 1203.06/s  (0.801s, 1278.97/s)  LR: 9.414e-05  Data: 0.010 (0.023)
Train: 487 [ 200/1251 ( 16%)]  Loss: 2.857 (2.98)  Time: 0.808s, 1266.80/s  (0.796s, 1285.95/s)  LR: 9.414e-05  Data: 0.009 (0.020)
Train: 487 [ 250/1251 ( 20%)]  Loss: 2.993 (2.98)  Time: 0.772s, 1326.08/s  (0.798s, 1282.76/s)  LR: 9.414e-05  Data: 0.009 (0.018)
Train: 487 [ 300/1251 ( 24%)]  Loss: 3.356 (3.04)  Time: 0.776s, 1319.79/s  (0.796s, 1286.86/s)  LR: 9.414e-05  Data: 0.011 (0.017)
Train: 487 [ 350/1251 ( 28%)]  Loss: 3.233 (3.06)  Time: 0.778s, 1316.41/s  (0.794s, 1290.05/s)  LR: 9.414e-05  Data: 0.010 (0.016)
Train: 487 [ 400/1251 ( 32%)]  Loss: 3.128 (3.07)  Time: 0.785s, 1304.34/s  (0.795s, 1287.52/s)  LR: 9.414e-05  Data: 0.010 (0.015)
Train: 487 [ 450/1251 ( 36%)]  Loss: 2.921 (3.05)  Time: 0.774s, 1323.25/s  (0.794s, 1289.98/s)  LR: 9.414e-05  Data: 0.009 (0.015)
Train: 487 [ 500/1251 ( 40%)]  Loss: 3.133 (3.06)  Time: 0.806s, 1270.55/s  (0.794s, 1290.44/s)  LR: 9.414e-05  Data: 0.010 (0.014)
Train: 487 [ 550/1251 ( 44%)]  Loss: 2.827 (3.04)  Time: 0.772s, 1325.65/s  (0.794s, 1289.99/s)  LR: 9.414e-05  Data: 0.009 (0.014)
Train: 487 [ 600/1251 ( 48%)]  Loss: 2.933 (3.03)  Time: 0.774s, 1322.62/s  (0.792s, 1292.12/s)  LR: 9.414e-05  Data: 0.009 (0.013)
Train: 487 [ 650/1251 ( 52%)]  Loss: 3.221 (3.05)  Time: 0.773s, 1325.19/s  (0.792s, 1293.63/s)  LR: 9.414e-05  Data: 0.010 (0.013)
Train: 487 [ 700/1251 ( 56%)]  Loss: 3.033 (3.05)  Time: 0.788s, 1299.20/s  (0.791s, 1294.07/s)  LR: 9.414e-05  Data: 0.010 (0.013)
Train: 487 [ 750/1251 ( 60%)]  Loss: 3.182 (3.05)  Time: 0.773s, 1325.50/s  (0.791s, 1294.84/s)  LR: 9.414e-05  Data: 0.010 (0.013)
Train: 487 [ 800/1251 ( 64%)]  Loss: 3.285 (3.07)  Time: 0.776s, 1319.91/s  (0.790s, 1295.54/s)  LR: 9.414e-05  Data: 0.009 (0.013)
Train: 487 [ 850/1251 ( 68%)]  Loss: 3.013 (3.07)  Time: 0.772s, 1326.01/s  (0.791s, 1295.34/s)  LR: 9.414e-05  Data: 0.010 (0.012)
Train: 487 [ 900/1251 ( 72%)]  Loss: 3.196 (3.07)  Time: 0.773s, 1324.93/s  (0.790s, 1295.78/s)  LR: 9.414e-05  Data: 0.009 (0.012)
Train: 487 [ 950/1251 ( 76%)]  Loss: 3.118 (3.07)  Time: 0.807s, 1268.58/s  (0.790s, 1295.76/s)  LR: 9.414e-05  Data: 0.009 (0.012)
Train: 487 [1000/1251 ( 80%)]  Loss: 3.015 (3.07)  Time: 0.773s, 1324.80/s  (0.790s, 1295.61/s)  LR: 9.414e-05  Data: 0.010 (0.012)
Train: 487 [1050/1251 ( 84%)]  Loss: 3.145 (3.08)  Time: 0.861s, 1188.99/s  (0.791s, 1294.96/s)  LR: 9.414e-05  Data: 0.012 (0.012)
Train: 487 [1100/1251 ( 88%)]  Loss: 3.040 (3.07)  Time: 0.773s, 1324.42/s  (0.790s, 1295.76/s)  LR: 9.414e-05  Data: 0.010 (0.012)
Train: 487 [1150/1251 ( 92%)]  Loss: 3.025 (3.07)  Time: 0.778s, 1316.73/s  (0.790s, 1295.73/s)  LR: 9.414e-05  Data: 0.010 (0.012)
Train: 487 [1200/1251 ( 96%)]  Loss: 3.206 (3.08)  Time: 0.849s, 1205.70/s  (0.791s, 1295.08/s)  LR: 9.414e-05  Data: 0.010 (0.012)
Train: 487 [1250/1251 (100%)]  Loss: 3.059 (3.08)  Time: 0.795s, 1288.35/s  (0.791s, 1294.87/s)  LR: 9.414e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.618 (1.618)  Loss:  0.5996 (0.5996)  Acc@1: 91.6016 (91.6016)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.194 (0.567)  Loss:  0.7051 (1.0728)  Acc@1: 86.9104 (79.7460)  Acc@5: 97.8774 (95.0260)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-487.pth.tar', 79.74600013183594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-486.pth.tar', 79.72199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-471.pth.tar', 79.68199995117187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-483.pth.tar', 79.66799998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-470.pth.tar', 79.61799989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-485.pth.tar', 79.61599994628907)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-484.pth.tar', 79.57800003173828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-476.pth.tar', 79.56399997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-474.pth.tar', 79.55800002929688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-479.pth.tar', 79.5200000024414)

Train: 488 [   0/1251 (  0%)]  Loss: 3.107 (3.11)  Time: 2.296s,  446.04/s  (2.296s,  446.04/s)  LR: 9.270e-05  Data: 1.568 (1.568)
Train: 488 [  50/1251 (  4%)]  Loss: 3.205 (3.16)  Time: 0.794s, 1290.08/s  (0.815s, 1256.18/s)  LR: 9.270e-05  Data: 0.010 (0.044)
Train: 488 [ 100/1251 (  8%)]  Loss: 3.010 (3.11)  Time: 0.816s, 1255.55/s  (0.799s, 1281.44/s)  LR: 9.270e-05  Data: 0.011 (0.027)
Train: 488 [ 150/1251 ( 12%)]  Loss: 2.731 (3.01)  Time: 0.807s, 1268.12/s  (0.799s, 1281.30/s)  LR: 9.270e-05  Data: 0.010 (0.022)
Train: 488 [ 200/1251 ( 16%)]  Loss: 3.175 (3.05)  Time: 0.780s, 1312.34/s  (0.796s, 1286.21/s)  LR: 9.270e-05  Data: 0.010 (0.019)
Train: 488 [ 250/1251 ( 20%)]  Loss: 3.091 (3.05)  Time: 0.809s, 1265.34/s  (0.796s, 1286.05/s)  LR: 9.270e-05  Data: 0.010 (0.017)
Train: 488 [ 300/1251 ( 24%)]  Loss: 2.848 (3.02)  Time: 0.772s, 1326.66/s  (0.797s, 1284.43/s)  LR: 9.270e-05  Data: 0.009 (0.016)
Train: 488 [ 350/1251 ( 28%)]  Loss: 3.476 (3.08)  Time: 0.808s, 1267.30/s  (0.797s, 1285.55/s)  LR: 9.270e-05  Data: 0.010 (0.015)
Train: 488 [ 400/1251 ( 32%)]  Loss: 3.271 (3.10)  Time: 0.780s, 1312.53/s  (0.796s, 1287.15/s)  LR: 9.270e-05  Data: 0.009 (0.014)
Train: 488 [ 450/1251 ( 36%)]  Loss: 3.015 (3.09)  Time: 0.780s, 1312.36/s  (0.794s, 1290.23/s)  LR: 9.270e-05  Data: 0.010 (0.014)
Train: 488 [ 500/1251 ( 40%)]  Loss: 3.211 (3.10)  Time: 0.819s, 1250.79/s  (0.793s, 1291.37/s)  LR: 9.270e-05  Data: 0.009 (0.014)
Train: 488 [ 550/1251 ( 44%)]  Loss: 3.416 (3.13)  Time: 0.772s, 1326.22/s  (0.792s, 1292.17/s)  LR: 9.270e-05  Data: 0.009 (0.013)
Train: 488 [ 600/1251 ( 48%)]  Loss: 3.171 (3.13)  Time: 0.833s, 1228.76/s  (0.792s, 1292.29/s)  LR: 9.270e-05  Data: 0.010 (0.013)
Train: 488 [ 650/1251 ( 52%)]  Loss: 3.027 (3.13)  Time: 0.773s, 1324.16/s  (0.792s, 1292.50/s)  LR: 9.270e-05  Data: 0.010 (0.013)
Train: 488 [ 700/1251 ( 56%)]  Loss: 3.324 (3.14)  Time: 0.791s, 1294.15/s  (0.792s, 1292.96/s)  LR: 9.270e-05  Data: 0.012 (0.012)
Train: 488 [ 750/1251 ( 60%)]  Loss: 3.202 (3.14)  Time: 0.774s, 1323.49/s  (0.791s, 1294.48/s)  LR: 9.270e-05  Data: 0.010 (0.012)
Train: 488 [ 800/1251 ( 64%)]  Loss: 3.418 (3.16)  Time: 0.772s, 1326.90/s  (0.790s, 1295.83/s)  LR: 9.270e-05  Data: 0.009 (0.012)
Train: 488 [ 850/1251 ( 68%)]  Loss: 3.200 (3.16)  Time: 0.806s, 1270.14/s  (0.790s, 1295.48/s)  LR: 9.270e-05  Data: 0.010 (0.012)
Train: 488 [ 900/1251 ( 72%)]  Loss: 3.356 (3.17)  Time: 0.786s, 1302.60/s  (0.791s, 1295.04/s)  LR: 9.270e-05  Data: 0.010 (0.012)
Train: 488 [ 950/1251 ( 76%)]  Loss: 2.947 (3.16)  Time: 0.809s, 1265.29/s  (0.792s, 1293.33/s)  LR: 9.270e-05  Data: 0.009 (0.012)
Train: 488 [1000/1251 ( 80%)]  Loss: 3.141 (3.16)  Time: 0.776s, 1320.40/s  (0.791s, 1294.22/s)  LR: 9.270e-05  Data: 0.010 (0.012)
Train: 488 [1050/1251 ( 84%)]  Loss: 2.920 (3.15)  Time: 0.772s, 1326.58/s  (0.791s, 1294.98/s)  LR: 9.270e-05  Data: 0.010 (0.012)
Train: 488 [1100/1251 ( 88%)]  Loss: 3.310 (3.16)  Time: 0.781s, 1311.08/s  (0.790s, 1295.44/s)  LR: 9.270e-05  Data: 0.010 (0.012)
Train: 488 [1150/1251 ( 92%)]  Loss: 2.723 (3.14)  Time: 0.776s, 1320.07/s  (0.790s, 1296.18/s)  LR: 9.270e-05  Data: 0.009 (0.012)
Train: 488 [1200/1251 ( 96%)]  Loss: 3.043 (3.13)  Time: 0.784s, 1306.74/s  (0.790s, 1295.59/s)  LR: 9.270e-05  Data: 0.010 (0.011)
Train: 488 [1250/1251 (100%)]  Loss: 3.033 (3.13)  Time: 0.798s, 1283.50/s  (0.790s, 1295.81/s)  LR: 9.270e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.599 (1.599)  Loss:  0.6636 (0.6636)  Acc@1: 91.6016 (91.6016)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.194 (0.566)  Loss:  0.7686 (1.1173)  Acc@1: 86.9104 (79.6860)  Acc@5: 97.9953 (95.0340)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-487.pth.tar', 79.74600013183594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-486.pth.tar', 79.72199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-488.pth.tar', 79.6860000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-471.pth.tar', 79.68199995117187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-483.pth.tar', 79.66799998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-470.pth.tar', 79.61799989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-485.pth.tar', 79.61599994628907)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-484.pth.tar', 79.57800003173828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-476.pth.tar', 79.56399997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-474.pth.tar', 79.55800002929688)

Train: 489 [   0/1251 (  0%)]  Loss: 2.886 (2.89)  Time: 2.252s,  454.77/s  (2.252s,  454.77/s)  LR: 9.128e-05  Data: 1.497 (1.497)
Train: 489 [  50/1251 (  4%)]  Loss: 3.311 (3.10)  Time: 0.846s, 1211.01/s  (0.811s, 1261.88/s)  LR: 9.128e-05  Data: 0.010 (0.043)
Train: 489 [ 100/1251 (  8%)]  Loss: 3.093 (3.10)  Time: 0.808s, 1267.40/s  (0.801s, 1278.82/s)  LR: 9.128e-05  Data: 0.010 (0.027)
Train: 489 [ 150/1251 ( 12%)]  Loss: 2.878 (3.04)  Time: 0.771s, 1328.98/s  (0.803s, 1275.58/s)  LR: 9.128e-05  Data: 0.010 (0.021)
Train: 489 [ 200/1251 ( 16%)]  Loss: 3.059 (3.05)  Time: 0.807s, 1268.20/s  (0.802s, 1276.89/s)  LR: 9.128e-05  Data: 0.009 (0.018)
Train: 489 [ 250/1251 ( 20%)]  Loss: 3.121 (3.06)  Time: 0.811s, 1262.57/s  (0.801s, 1278.51/s)  LR: 9.128e-05  Data: 0.010 (0.017)
Train: 489 [ 300/1251 ( 24%)]  Loss: 2.918 (3.04)  Time: 0.803s, 1275.71/s  (0.801s, 1278.95/s)  LR: 9.128e-05  Data: 0.010 (0.016)
Train: 489 [ 350/1251 ( 28%)]  Loss: 3.293 (3.07)  Time: 0.775s, 1322.04/s  (0.801s, 1278.24/s)  LR: 9.128e-05  Data: 0.010 (0.015)
Train: 489 [ 400/1251 ( 32%)]  Loss: 2.962 (3.06)  Time: 0.773s, 1324.32/s  (0.801s, 1278.36/s)  LR: 9.128e-05  Data: 0.010 (0.014)
Train: 489 [ 450/1251 ( 36%)]  Loss: 2.758 (3.03)  Time: 0.812s, 1261.45/s  (0.800s, 1280.09/s)  LR: 9.128e-05  Data: 0.010 (0.014)
Train: 489 [ 500/1251 ( 40%)]  Loss: 3.227 (3.05)  Time: 0.781s, 1311.41/s  (0.798s, 1282.94/s)  LR: 9.128e-05  Data: 0.010 (0.013)
Train: 489 [ 550/1251 ( 44%)]  Loss: 3.043 (3.05)  Time: 0.772s, 1325.82/s  (0.797s, 1285.17/s)  LR: 9.128e-05  Data: 0.010 (0.013)
Train: 489 [ 600/1251 ( 48%)]  Loss: 2.864 (3.03)  Time: 0.812s, 1261.50/s  (0.797s, 1285.01/s)  LR: 9.128e-05  Data: 0.010 (0.013)
Train: 489 [ 650/1251 ( 52%)]  Loss: 3.053 (3.03)  Time: 0.815s, 1255.92/s  (0.797s, 1284.94/s)  LR: 9.128e-05  Data: 0.010 (0.013)
Train: 489 [ 700/1251 ( 56%)]  Loss: 3.067 (3.04)  Time: 0.771s, 1328.48/s  (0.796s, 1285.64/s)  LR: 9.128e-05  Data: 0.010 (0.012)
Train: 489 [ 750/1251 ( 60%)]  Loss: 3.091 (3.04)  Time: 0.785s, 1305.13/s  (0.797s, 1285.37/s)  LR: 9.128e-05  Data: 0.010 (0.012)
Train: 489 [ 800/1251 ( 64%)]  Loss: 3.010 (3.04)  Time: 0.775s, 1320.99/s  (0.796s, 1286.52/s)  LR: 9.128e-05  Data: 0.010 (0.012)
Train: 489 [ 850/1251 ( 68%)]  Loss: 3.058 (3.04)  Time: 0.773s, 1323.97/s  (0.796s, 1287.22/s)  LR: 9.128e-05  Data: 0.010 (0.012)
Train: 489 [ 900/1251 ( 72%)]  Loss: 2.789 (3.03)  Time: 0.809s, 1266.09/s  (0.795s, 1287.80/s)  LR: 9.128e-05  Data: 0.011 (0.012)
Train: 489 [ 950/1251 ( 76%)]  Loss: 2.872 (3.02)  Time: 0.784s, 1305.91/s  (0.795s, 1288.12/s)  LR: 9.128e-05  Data: 0.011 (0.012)
Train: 489 [1000/1251 ( 80%)]  Loss: 3.351 (3.03)  Time: 0.795s, 1288.30/s  (0.795s, 1288.28/s)  LR: 9.128e-05  Data: 0.012 (0.012)
Train: 489 [1050/1251 ( 84%)]  Loss: 2.984 (3.03)  Time: 0.782s, 1308.66/s  (0.795s, 1288.32/s)  LR: 9.128e-05  Data: 0.010 (0.012)
Train: 489 [1100/1251 ( 88%)]  Loss: 2.975 (3.03)  Time: 0.776s, 1319.07/s  (0.795s, 1288.76/s)  LR: 9.128e-05  Data: 0.010 (0.012)
Train: 489 [1150/1251 ( 92%)]  Loss: 3.150 (3.03)  Time: 0.815s, 1257.20/s  (0.795s, 1287.67/s)  LR: 9.128e-05  Data: 0.011 (0.012)
Train: 489 [1200/1251 ( 96%)]  Loss: 2.990 (3.03)  Time: 0.896s, 1142.88/s  (0.795s, 1287.29/s)  LR: 9.128e-05  Data: 0.010 (0.011)
Train: 489 [1250/1251 (100%)]  Loss: 3.057 (3.03)  Time: 0.802s, 1277.15/s  (0.795s, 1287.56/s)  LR: 9.128e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.615 (1.615)  Loss:  0.6597 (0.6597)  Acc@1: 91.8945 (91.8945)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.194 (0.565)  Loss:  0.7412 (1.1105)  Acc@1: 86.5566 (79.8200)  Acc@5: 97.2877 (95.0840)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-489.pth.tar', 79.81999997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-487.pth.tar', 79.74600013183594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-486.pth.tar', 79.72199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-488.pth.tar', 79.6860000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-471.pth.tar', 79.68199995117187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-483.pth.tar', 79.66799998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-470.pth.tar', 79.61799989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-485.pth.tar', 79.61599994628907)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-484.pth.tar', 79.57800003173828)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-476.pth.tar', 79.56399997802734)

Train: 490 [   0/1251 (  0%)]  Loss: 2.839 (2.84)  Time: 2.154s,  475.38/s  (2.154s,  475.38/s)  LR: 8.986e-05  Data: 1.422 (1.422)
Train: 490 [  50/1251 (  4%)]  Loss: 3.064 (2.95)  Time: 0.809s, 1266.21/s  (0.829s, 1235.37/s)  LR: 8.986e-05  Data: 0.010 (0.041)
Train: 490 [ 100/1251 (  8%)]  Loss: 3.105 (3.00)  Time: 0.810s, 1264.18/s  (0.811s, 1262.26/s)  LR: 8.986e-05  Data: 0.010 (0.026)
Train: 490 [ 150/1251 ( 12%)]  Loss: 2.843 (2.96)  Time: 0.808s, 1267.06/s  (0.808s, 1267.67/s)  LR: 8.986e-05  Data: 0.010 (0.020)
Train: 490 [ 200/1251 ( 16%)]  Loss: 2.992 (2.97)  Time: 0.809s, 1266.18/s  (0.803s, 1274.82/s)  LR: 8.986e-05  Data: 0.010 (0.018)
Train: 490 [ 250/1251 ( 20%)]  Loss: 3.314 (3.03)  Time: 0.809s, 1266.23/s  (0.800s, 1279.79/s)  LR: 8.986e-05  Data: 0.010 (0.016)
Train: 490 [ 300/1251 ( 24%)]  Loss: 2.797 (2.99)  Time: 0.778s, 1316.79/s  (0.800s, 1280.63/s)  LR: 8.986e-05  Data: 0.010 (0.015)
Train: 490 [ 350/1251 ( 28%)]  Loss: 3.073 (3.00)  Time: 0.792s, 1292.22/s  (0.798s, 1282.86/s)  LR: 8.986e-05  Data: 0.010 (0.014)
Train: 490 [ 400/1251 ( 32%)]  Loss: 2.861 (2.99)  Time: 0.782s, 1310.25/s  (0.798s, 1283.14/s)  LR: 8.986e-05  Data: 0.009 (0.014)
Train: 490 [ 450/1251 ( 36%)]  Loss: 2.781 (2.97)  Time: 0.784s, 1305.72/s  (0.798s, 1283.12/s)  LR: 8.986e-05  Data: 0.010 (0.013)
Train: 490 [ 500/1251 ( 40%)]  Loss: 3.155 (2.98)  Time: 0.782s, 1309.37/s  (0.797s, 1284.44/s)  LR: 8.986e-05  Data: 0.010 (0.013)
Train: 490 [ 550/1251 ( 44%)]  Loss: 2.983 (2.98)  Time: 0.781s, 1310.87/s  (0.796s, 1286.35/s)  LR: 8.986e-05  Data: 0.010 (0.013)
Train: 490 [ 600/1251 ( 48%)]  Loss: 3.056 (2.99)  Time: 0.842s, 1216.39/s  (0.796s, 1286.45/s)  LR: 8.986e-05  Data: 0.010 (0.013)
Train: 490 [ 650/1251 ( 52%)]  Loss: 3.235 (3.01)  Time: 0.809s, 1265.95/s  (0.795s, 1287.84/s)  LR: 8.986e-05  Data: 0.010 (0.012)
Train: 490 [ 700/1251 ( 56%)]  Loss: 3.027 (3.01)  Time: 0.816s, 1254.95/s  (0.796s, 1286.89/s)  LR: 8.986e-05  Data: 0.012 (0.012)
Train: 490 [ 750/1251 ( 60%)]  Loss: 3.340 (3.03)  Time: 0.773s, 1325.07/s  (0.796s, 1286.85/s)  LR: 8.986e-05  Data: 0.009 (0.012)
Train: 490 [ 800/1251 ( 64%)]  Loss: 3.024 (3.03)  Time: 0.826s, 1239.22/s  (0.796s, 1286.43/s)  LR: 8.986e-05  Data: 0.009 (0.012)
Train: 490 [ 850/1251 ( 68%)]  Loss: 3.083 (3.03)  Time: 0.808s, 1267.42/s  (0.795s, 1287.51/s)  LR: 8.986e-05  Data: 0.010 (0.012)
Train: 490 [ 900/1251 ( 72%)]  Loss: 3.100 (3.04)  Time: 0.808s, 1267.99/s  (0.795s, 1288.09/s)  LR: 8.986e-05  Data: 0.010 (0.012)
Train: 490 [ 950/1251 ( 76%)]  Loss: 2.950 (3.03)  Time: 0.771s, 1328.25/s  (0.795s, 1288.80/s)  LR: 8.986e-05  Data: 0.009 (0.012)
Train: 490 [1000/1251 ( 80%)]  Loss: 3.128 (3.04)  Time: 0.793s, 1292.05/s  (0.794s, 1289.93/s)  LR: 8.986e-05  Data: 0.010 (0.012)
Train: 490 [1050/1251 ( 84%)]  Loss: 3.275 (3.05)  Time: 0.804s, 1274.18/s  (0.793s, 1290.65/s)  LR: 8.986e-05  Data: 0.010 (0.011)
Train: 490 [1100/1251 ( 88%)]  Loss: 2.881 (3.04)  Time: 0.808s, 1267.56/s  (0.793s, 1290.95/s)  LR: 8.986e-05  Data: 0.012 (0.011)
Train: 490 [1150/1251 ( 92%)]  Loss: 3.031 (3.04)  Time: 0.815s, 1256.96/s  (0.793s, 1290.67/s)  LR: 8.986e-05  Data: 0.010 (0.011)
Train: 490 [1200/1251 ( 96%)]  Loss: 2.935 (3.03)  Time: 0.772s, 1326.52/s  (0.793s, 1291.65/s)  LR: 8.986e-05  Data: 0.010 (0.011)
Train: 490 [1250/1251 (100%)]  Loss: 2.921 (3.03)  Time: 0.765s, 1339.08/s  (0.793s, 1291.38/s)  LR: 8.986e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.617 (1.617)  Loss:  0.6206 (0.6206)  Acc@1: 91.8945 (91.8945)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.194 (0.562)  Loss:  0.7368 (1.0688)  Acc@1: 87.0283 (79.8320)  Acc@5: 97.7594 (95.0540)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-490.pth.tar', 79.83200005371094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-489.pth.tar', 79.81999997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-487.pth.tar', 79.74600013183594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-486.pth.tar', 79.72199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-488.pth.tar', 79.6860000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-471.pth.tar', 79.68199995117187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-483.pth.tar', 79.66799998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-470.pth.tar', 79.61799989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-485.pth.tar', 79.61599994628907)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-484.pth.tar', 79.57800003173828)

Train: 491 [   0/1251 (  0%)]  Loss: 3.176 (3.18)  Time: 2.276s,  449.95/s  (2.276s,  449.95/s)  LR: 8.845e-05  Data: 1.545 (1.545)
Train: 491 [  50/1251 (  4%)]  Loss: 2.988 (3.08)  Time: 0.773s, 1325.09/s  (0.834s, 1227.24/s)  LR: 8.845e-05  Data: 0.010 (0.045)
Train: 491 [ 100/1251 (  8%)]  Loss: 2.812 (2.99)  Time: 0.808s, 1266.87/s  (0.816s, 1254.31/s)  LR: 8.845e-05  Data: 0.010 (0.028)
Train: 491 [ 150/1251 ( 12%)]  Loss: 3.166 (3.04)  Time: 0.774s, 1322.88/s  (0.808s, 1267.04/s)  LR: 8.845e-05  Data: 0.010 (0.022)
Train: 491 [ 200/1251 ( 16%)]  Loss: 3.023 (3.03)  Time: 0.773s, 1324.04/s  (0.802s, 1277.28/s)  LR: 8.845e-05  Data: 0.011 (0.019)
Train: 491 [ 250/1251 ( 20%)]  Loss: 3.375 (3.09)  Time: 0.774s, 1322.89/s  (0.798s, 1282.43/s)  LR: 8.845e-05  Data: 0.011 (0.017)
Train: 491 [ 300/1251 ( 24%)]  Loss: 3.123 (3.09)  Time: 0.774s, 1323.65/s  (0.795s, 1287.92/s)  LR: 8.845e-05  Data: 0.009 (0.016)
Train: 491 [ 350/1251 ( 28%)]  Loss: 3.228 (3.11)  Time: 0.776s, 1320.14/s  (0.793s, 1291.84/s)  LR: 8.845e-05  Data: 0.009 (0.015)
Train: 491 [ 400/1251 ( 32%)]  Loss: 3.169 (3.12)  Time: 0.773s, 1324.64/s  (0.793s, 1290.61/s)  LR: 8.845e-05  Data: 0.009 (0.015)
Train: 491 [ 450/1251 ( 36%)]  Loss: 3.206 (3.13)  Time: 0.773s, 1325.34/s  (0.793s, 1291.58/s)  LR: 8.845e-05  Data: 0.010 (0.014)
Train: 491 [ 500/1251 ( 40%)]  Loss: 3.293 (3.14)  Time: 0.772s, 1326.40/s  (0.794s, 1290.27/s)  LR: 8.845e-05  Data: 0.010 (0.014)
Train: 491 [ 550/1251 ( 44%)]  Loss: 3.140 (3.14)  Time: 0.807s, 1268.24/s  (0.793s, 1291.01/s)  LR: 8.845e-05  Data: 0.010 (0.013)
Train: 491 [ 600/1251 ( 48%)]  Loss: 3.638 (3.18)  Time: 0.775s, 1320.81/s  (0.793s, 1291.31/s)  LR: 8.845e-05  Data: 0.010 (0.013)
Train: 491 [ 650/1251 ( 52%)]  Loss: 3.212 (3.18)  Time: 0.773s, 1324.53/s  (0.793s, 1292.02/s)  LR: 8.845e-05  Data: 0.010 (0.013)
Train: 491 [ 700/1251 ( 56%)]  Loss: 3.136 (3.18)  Time: 0.819s, 1249.80/s  (0.793s, 1290.52/s)  LR: 8.845e-05  Data: 0.010 (0.013)
Train: 491 [ 750/1251 ( 60%)]  Loss: 3.385 (3.19)  Time: 0.771s, 1327.40/s  (0.793s, 1290.97/s)  LR: 8.845e-05  Data: 0.010 (0.012)
Train: 491 [ 800/1251 ( 64%)]  Loss: 3.288 (3.20)  Time: 0.786s, 1302.18/s  (0.793s, 1290.86/s)  LR: 8.845e-05  Data: 0.010 (0.012)
Train: 491 [ 850/1251 ( 68%)]  Loss: 2.993 (3.19)  Time: 0.776s, 1319.77/s  (0.793s, 1291.87/s)  LR: 8.845e-05  Data: 0.010 (0.012)
Train: 491 [ 900/1251 ( 72%)]  Loss: 3.238 (3.19)  Time: 0.774s, 1323.54/s  (0.792s, 1292.85/s)  LR: 8.845e-05  Data: 0.010 (0.012)
Train: 491 [ 950/1251 ( 76%)]  Loss: 3.202 (3.19)  Time: 0.775s, 1321.54/s  (0.791s, 1294.11/s)  LR: 8.845e-05  Data: 0.011 (0.012)
Train: 491 [1000/1251 ( 80%)]  Loss: 3.168 (3.19)  Time: 0.782s, 1310.23/s  (0.791s, 1294.67/s)  LR: 8.845e-05  Data: 0.010 (0.012)
Train: 491 [1050/1251 ( 84%)]  Loss: 3.082 (3.18)  Time: 0.785s, 1304.86/s  (0.791s, 1294.94/s)  LR: 8.845e-05  Data: 0.010 (0.012)
Train: 491 [1100/1251 ( 88%)]  Loss: 2.722 (3.16)  Time: 0.805s, 1272.78/s  (0.792s, 1293.50/s)  LR: 8.845e-05  Data: 0.009 (0.012)
Train: 491 [1150/1251 ( 92%)]  Loss: 2.800 (3.15)  Time: 0.855s, 1198.05/s  (0.792s, 1292.83/s)  LR: 8.845e-05  Data: 0.010 (0.012)
Train: 491 [1200/1251 ( 96%)]  Loss: 2.939 (3.14)  Time: 0.786s, 1303.15/s  (0.792s, 1293.15/s)  LR: 8.845e-05  Data: 0.010 (0.012)
Train: 491 [1250/1251 (100%)]  Loss: 2.914 (3.13)  Time: 0.834s, 1228.13/s  (0.792s, 1293.56/s)  LR: 8.845e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.547 (1.547)  Loss:  0.6289 (0.6289)  Acc@1: 91.6992 (91.6992)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.194 (0.575)  Loss:  0.6724 (1.0600)  Acc@1: 87.2642 (79.8400)  Acc@5: 98.5849 (95.0220)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-491.pth.tar', 79.84000002685546)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-490.pth.tar', 79.83200005371094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-489.pth.tar', 79.81999997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-487.pth.tar', 79.74600013183594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-486.pth.tar', 79.72199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-488.pth.tar', 79.6860000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-471.pth.tar', 79.68199995117187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-483.pth.tar', 79.66799998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-470.pth.tar', 79.61799989990234)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-485.pth.tar', 79.61599994628907)

Train: 492 [   0/1251 (  0%)]  Loss: 2.793 (2.79)  Time: 2.294s,  446.35/s  (2.294s,  446.35/s)  LR: 8.706e-05  Data: 1.558 (1.558)
Train: 492 [  50/1251 (  4%)]  Loss: 3.212 (3.00)  Time: 0.769s, 1331.23/s  (0.828s, 1237.36/s)  LR: 8.706e-05  Data: 0.009 (0.043)
Train: 492 [ 100/1251 (  8%)]  Loss: 3.009 (3.00)  Time: 0.829s, 1235.37/s  (0.809s, 1265.81/s)  LR: 8.706e-05  Data: 0.009 (0.026)
Train: 492 [ 150/1251 ( 12%)]  Loss: 3.182 (3.05)  Time: 0.773s, 1324.11/s  (0.803s, 1275.45/s)  LR: 8.706e-05  Data: 0.010 (0.021)
Train: 492 [ 200/1251 ( 16%)]  Loss: 2.637 (2.97)  Time: 0.773s, 1324.11/s  (0.798s, 1282.60/s)  LR: 8.706e-05  Data: 0.009 (0.018)
Train: 492 [ 250/1251 ( 20%)]  Loss: 3.139 (3.00)  Time: 0.811s, 1262.20/s  (0.796s, 1286.78/s)  LR: 8.706e-05  Data: 0.010 (0.016)
Train: 492 [ 300/1251 ( 24%)]  Loss: 3.024 (3.00)  Time: 0.777s, 1317.16/s  (0.795s, 1287.25/s)  LR: 8.706e-05  Data: 0.009 (0.015)
Train: 492 [ 350/1251 ( 28%)]  Loss: 3.085 (3.01)  Time: 0.773s, 1324.72/s  (0.794s, 1290.29/s)  LR: 8.706e-05  Data: 0.010 (0.014)
Train: 492 [ 400/1251 ( 32%)]  Loss: 3.015 (3.01)  Time: 0.772s, 1325.95/s  (0.792s, 1293.27/s)  LR: 8.706e-05  Data: 0.010 (0.014)
Train: 492 [ 450/1251 ( 36%)]  Loss: 2.721 (2.98)  Time: 0.784s, 1306.05/s  (0.792s, 1293.53/s)  LR: 8.706e-05  Data: 0.009 (0.013)
Train: 492 [ 500/1251 ( 40%)]  Loss: 3.100 (2.99)  Time: 0.779s, 1314.88/s  (0.792s, 1293.54/s)  LR: 8.706e-05  Data: 0.010 (0.013)
Train: 492 [ 550/1251 ( 44%)]  Loss: 3.132 (3.00)  Time: 0.811s, 1262.62/s  (0.792s, 1293.74/s)  LR: 8.706e-05  Data: 0.009 (0.013)
Train: 492 [ 600/1251 ( 48%)]  Loss: 3.254 (3.02)  Time: 0.812s, 1260.70/s  (0.791s, 1294.07/s)  LR: 8.706e-05  Data: 0.009 (0.012)
Train: 492 [ 650/1251 ( 52%)]  Loss: 3.034 (3.02)  Time: 0.781s, 1310.51/s  (0.791s, 1295.26/s)  LR: 8.706e-05  Data: 0.011 (0.012)
Train: 492 [ 700/1251 ( 56%)]  Loss: 3.031 (3.02)  Time: 0.776s, 1319.16/s  (0.790s, 1296.11/s)  LR: 8.706e-05  Data: 0.010 (0.012)
Train: 492 [ 750/1251 ( 60%)]  Loss: 2.876 (3.02)  Time: 0.833s, 1228.63/s  (0.790s, 1295.98/s)  LR: 8.706e-05  Data: 0.009 (0.012)
Train: 492 [ 800/1251 ( 64%)]  Loss: 2.916 (3.01)  Time: 0.787s, 1300.49/s  (0.790s, 1295.46/s)  LR: 8.706e-05  Data: 0.009 (0.012)
Train: 492 [ 850/1251 ( 68%)]  Loss: 3.148 (3.02)  Time: 0.815s, 1256.49/s  (0.791s, 1294.51/s)  LR: 8.706e-05  Data: 0.012 (0.011)
Train: 492 [ 900/1251 ( 72%)]  Loss: 3.090 (3.02)  Time: 0.773s, 1325.47/s  (0.790s, 1295.40/s)  LR: 8.706e-05  Data: 0.009 (0.011)
Train: 492 [ 950/1251 ( 76%)]  Loss: 3.215 (3.03)  Time: 0.807s, 1268.87/s  (0.790s, 1295.54/s)  LR: 8.706e-05  Data: 0.009 (0.011)
Train: 492 [1000/1251 ( 80%)]  Loss: 3.099 (3.03)  Time: 0.772s, 1327.21/s  (0.790s, 1296.33/s)  LR: 8.706e-05  Data: 0.009 (0.011)
Train: 492 [1050/1251 ( 84%)]  Loss: 2.889 (3.03)  Time: 0.771s, 1327.60/s  (0.789s, 1297.36/s)  LR: 8.706e-05  Data: 0.009 (0.011)
Train: 492 [1100/1251 ( 88%)]  Loss: 2.777 (3.02)  Time: 0.806s, 1270.16/s  (0.790s, 1296.47/s)  LR: 8.706e-05  Data: 0.009 (0.011)
Train: 492 [1150/1251 ( 92%)]  Loss: 2.931 (3.01)  Time: 0.816s, 1255.18/s  (0.790s, 1296.60/s)  LR: 8.706e-05  Data: 0.009 (0.011)
Train: 492 [1200/1251 ( 96%)]  Loss: 3.198 (3.02)  Time: 0.774s, 1323.38/s  (0.790s, 1296.88/s)  LR: 8.706e-05  Data: 0.009 (0.011)
Train: 492 [1250/1251 (100%)]  Loss: 3.216 (3.03)  Time: 0.796s, 1286.44/s  (0.790s, 1296.94/s)  LR: 8.706e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.687 (1.687)  Loss:  0.6035 (0.6035)  Acc@1: 91.5039 (91.5039)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.194 (0.565)  Loss:  0.6753 (1.0278)  Acc@1: 87.1462 (79.8600)  Acc@5: 98.1132 (95.0500)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-492.pth.tar', 79.86000010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-491.pth.tar', 79.84000002685546)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-490.pth.tar', 79.83200005371094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-489.pth.tar', 79.81999997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-487.pth.tar', 79.74600013183594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-486.pth.tar', 79.72199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-488.pth.tar', 79.6860000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-471.pth.tar', 79.68199995117187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-483.pth.tar', 79.66799998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-470.pth.tar', 79.61799989990234)

Train: 493 [   0/1251 (  0%)]  Loss: 3.260 (3.26)  Time: 2.515s,  407.11/s  (2.515s,  407.11/s)  LR: 8.567e-05  Data: 1.715 (1.715)
Train: 493 [  50/1251 (  4%)]  Loss: 3.223 (3.24)  Time: 0.826s, 1238.97/s  (0.831s, 1232.20/s)  LR: 8.567e-05  Data: 0.010 (0.048)
Train: 493 [ 100/1251 (  8%)]  Loss: 2.871 (3.12)  Time: 0.771s, 1327.35/s  (0.808s, 1268.09/s)  LR: 8.567e-05  Data: 0.010 (0.029)
Train: 493 [ 150/1251 ( 12%)]  Loss: 2.757 (3.03)  Time: 0.811s, 1262.51/s  (0.800s, 1279.59/s)  LR: 8.567e-05  Data: 0.009 (0.023)
Train: 493 [ 200/1251 ( 16%)]  Loss: 3.264 (3.07)  Time: 0.814s, 1257.85/s  (0.803s, 1275.61/s)  LR: 8.567e-05  Data: 0.011 (0.020)
Train: 493 [ 250/1251 ( 20%)]  Loss: 3.176 (3.09)  Time: 0.783s, 1308.43/s  (0.801s, 1278.75/s)  LR: 8.567e-05  Data: 0.009 (0.018)
Train: 493 [ 300/1251 ( 24%)]  Loss: 2.981 (3.08)  Time: 0.862s, 1188.35/s  (0.797s, 1284.08/s)  LR: 8.567e-05  Data: 0.009 (0.016)
Train: 493 [ 350/1251 ( 28%)]  Loss: 3.107 (3.08)  Time: 0.776s, 1319.39/s  (0.796s, 1286.58/s)  LR: 8.567e-05  Data: 0.009 (0.015)
Train: 493 [ 400/1251 ( 32%)]  Loss: 3.297 (3.10)  Time: 0.809s, 1266.21/s  (0.795s, 1287.81/s)  LR: 8.567e-05  Data: 0.010 (0.015)
Train: 493 [ 450/1251 ( 36%)]  Loss: 2.922 (3.09)  Time: 0.809s, 1265.22/s  (0.794s, 1289.56/s)  LR: 8.567e-05  Data: 0.009 (0.014)
Train: 493 [ 500/1251 ( 40%)]  Loss: 2.841 (3.06)  Time: 0.776s, 1320.34/s  (0.794s, 1289.35/s)  LR: 8.567e-05  Data: 0.010 (0.014)
Train: 493 [ 550/1251 ( 44%)]  Loss: 2.893 (3.05)  Time: 0.782s, 1309.06/s  (0.794s, 1290.02/s)  LR: 8.567e-05  Data: 0.009 (0.013)
Train: 493 [ 600/1251 ( 48%)]  Loss: 3.279 (3.07)  Time: 0.863s, 1186.69/s  (0.793s, 1290.93/s)  LR: 8.567e-05  Data: 0.016 (0.013)
Train: 493 [ 650/1251 ( 52%)]  Loss: 2.935 (3.06)  Time: 0.809s, 1265.04/s  (0.794s, 1290.45/s)  LR: 8.567e-05  Data: 0.009 (0.013)
Train: 493 [ 700/1251 ( 56%)]  Loss: 3.159 (3.06)  Time: 0.774s, 1322.61/s  (0.794s, 1289.53/s)  LR: 8.567e-05  Data: 0.010 (0.013)
Train: 493 [ 750/1251 ( 60%)]  Loss: 3.050 (3.06)  Time: 0.774s, 1322.27/s  (0.793s, 1290.50/s)  LR: 8.567e-05  Data: 0.010 (0.012)
Train: 493 [ 800/1251 ( 64%)]  Loss: 3.223 (3.07)  Time: 0.773s, 1325.01/s  (0.792s, 1292.36/s)  LR: 8.567e-05  Data: 0.009 (0.012)
Train: 493 [ 850/1251 ( 68%)]  Loss: 2.821 (3.06)  Time: 0.773s, 1324.54/s  (0.792s, 1292.41/s)  LR: 8.567e-05  Data: 0.009 (0.012)
Train: 493 [ 900/1251 ( 72%)]  Loss: 3.150 (3.06)  Time: 0.779s, 1315.06/s  (0.792s, 1292.47/s)  LR: 8.567e-05  Data: 0.009 (0.012)
Train: 493 [ 950/1251 ( 76%)]  Loss: 3.212 (3.07)  Time: 0.776s, 1319.23/s  (0.792s, 1293.11/s)  LR: 8.567e-05  Data: 0.010 (0.012)
Train: 493 [1000/1251 ( 80%)]  Loss: 3.263 (3.08)  Time: 0.808s, 1267.51/s  (0.792s, 1293.26/s)  LR: 8.567e-05  Data: 0.012 (0.012)
Train: 493 [1050/1251 ( 84%)]  Loss: 2.983 (3.08)  Time: 0.773s, 1325.45/s  (0.792s, 1293.69/s)  LR: 8.567e-05  Data: 0.009 (0.012)
Train: 493 [1100/1251 ( 88%)]  Loss: 2.619 (3.06)  Time: 0.779s, 1313.95/s  (0.792s, 1293.59/s)  LR: 8.567e-05  Data: 0.009 (0.011)
Train: 493 [1150/1251 ( 92%)]  Loss: 2.941 (3.05)  Time: 0.779s, 1314.34/s  (0.792s, 1293.19/s)  LR: 8.567e-05  Data: 0.010 (0.011)
Train: 493 [1200/1251 ( 96%)]  Loss: 3.444 (3.07)  Time: 0.774s, 1323.41/s  (0.792s, 1293.11/s)  LR: 8.567e-05  Data: 0.009 (0.011)
Train: 493 [1250/1251 (100%)]  Loss: 3.014 (3.06)  Time: 0.762s, 1344.56/s  (0.792s, 1293.55/s)  LR: 8.567e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.600 (1.600)  Loss:  0.7085 (0.7085)  Acc@1: 91.5039 (91.5039)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.194 (0.568)  Loss:  0.8086 (1.1632)  Acc@1: 85.7311 (79.6660)  Acc@5: 98.1132 (95.0660)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-492.pth.tar', 79.86000010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-491.pth.tar', 79.84000002685546)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-490.pth.tar', 79.83200005371094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-489.pth.tar', 79.81999997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-487.pth.tar', 79.74600013183594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-486.pth.tar', 79.72199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-488.pth.tar', 79.6860000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-471.pth.tar', 79.68199995117187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-483.pth.tar', 79.66799998046875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-493.pth.tar', 79.66599987792969)

Train: 494 [   0/1251 (  0%)]  Loss: 3.161 (3.16)  Time: 2.343s,  437.03/s  (2.343s,  437.03/s)  LR: 8.430e-05  Data: 1.575 (1.575)
Train: 494 [  50/1251 (  4%)]  Loss: 2.777 (2.97)  Time: 0.772s, 1325.79/s  (0.816s, 1255.27/s)  LR: 8.430e-05  Data: 0.009 (0.044)
Train: 494 [ 100/1251 (  8%)]  Loss: 3.265 (3.07)  Time: 0.809s, 1266.12/s  (0.801s, 1277.98/s)  LR: 8.430e-05  Data: 0.010 (0.027)
Train: 494 [ 150/1251 ( 12%)]  Loss: 3.002 (3.05)  Time: 0.771s, 1327.55/s  (0.799s, 1282.33/s)  LR: 8.430e-05  Data: 0.010 (0.021)
Train: 494 [ 200/1251 ( 16%)]  Loss: 3.108 (3.06)  Time: 0.781s, 1311.24/s  (0.796s, 1286.78/s)  LR: 8.430e-05  Data: 0.010 (0.019)
Train: 494 [ 250/1251 ( 20%)]  Loss: 3.130 (3.07)  Time: 0.774s, 1323.70/s  (0.792s, 1292.89/s)  LR: 8.430e-05  Data: 0.010 (0.017)
Train: 494 [ 300/1251 ( 24%)]  Loss: 3.004 (3.06)  Time: 0.773s, 1325.40/s  (0.792s, 1292.62/s)  LR: 8.430e-05  Data: 0.009 (0.016)
Train: 494 [ 350/1251 ( 28%)]  Loss: 3.231 (3.08)  Time: 0.773s, 1325.17/s  (0.793s, 1291.95/s)  LR: 8.430e-05  Data: 0.009 (0.015)
Train: 494 [ 400/1251 ( 32%)]  Loss: 3.022 (3.08)  Time: 0.773s, 1325.17/s  (0.793s, 1291.56/s)  LR: 8.430e-05  Data: 0.009 (0.014)
Train: 494 [ 450/1251 ( 36%)]  Loss: 3.209 (3.09)  Time: 0.779s, 1314.04/s  (0.791s, 1293.91/s)  LR: 8.430e-05  Data: 0.010 (0.014)
Train: 494 [ 500/1251 ( 40%)]  Loss: 2.857 (3.07)  Time: 0.773s, 1324.64/s  (0.790s, 1295.83/s)  LR: 8.430e-05  Data: 0.009 (0.013)
Train: 494 [ 550/1251 ( 44%)]  Loss: 2.968 (3.06)  Time: 0.805s, 1271.50/s  (0.790s, 1296.80/s)  LR: 8.430e-05  Data: 0.009 (0.013)
Train: 494 [ 600/1251 ( 48%)]  Loss: 3.069 (3.06)  Time: 0.780s, 1313.61/s  (0.790s, 1296.69/s)  LR: 8.430e-05  Data: 0.009 (0.013)
Train: 494 [ 650/1251 ( 52%)]  Loss: 3.096 (3.06)  Time: 0.772s, 1326.56/s  (0.790s, 1296.48/s)  LR: 8.430e-05  Data: 0.009 (0.012)
Train: 494 [ 700/1251 ( 56%)]  Loss: 3.044 (3.06)  Time: 0.773s, 1325.04/s  (0.789s, 1297.17/s)  LR: 8.430e-05  Data: 0.010 (0.012)
Train: 494 [ 750/1251 ( 60%)]  Loss: 2.987 (3.06)  Time: 0.771s, 1327.76/s  (0.789s, 1297.40/s)  LR: 8.430e-05  Data: 0.009 (0.012)
Train: 494 [ 800/1251 ( 64%)]  Loss: 3.310 (3.07)  Time: 0.773s, 1323.98/s  (0.789s, 1297.61/s)  LR: 8.430e-05  Data: 0.010 (0.012)
Train: 494 [ 850/1251 ( 68%)]  Loss: 3.187 (3.08)  Time: 0.776s, 1319.39/s  (0.790s, 1296.66/s)  LR: 8.430e-05  Data: 0.010 (0.012)
Train: 494 [ 900/1251 ( 72%)]  Loss: 3.259 (3.09)  Time: 0.773s, 1324.05/s  (0.789s, 1297.20/s)  LR: 8.430e-05  Data: 0.010 (0.012)
Train: 494 [ 950/1251 ( 76%)]  Loss: 3.405 (3.10)  Time: 0.807s, 1269.68/s  (0.789s, 1297.05/s)  LR: 8.430e-05  Data: 0.009 (0.012)
Train: 494 [1000/1251 ( 80%)]  Loss: 3.222 (3.11)  Time: 0.815s, 1256.81/s  (0.789s, 1297.24/s)  LR: 8.430e-05  Data: 0.011 (0.012)
Train: 494 [1050/1251 ( 84%)]  Loss: 2.912 (3.10)  Time: 0.777s, 1318.45/s  (0.789s, 1297.21/s)  LR: 8.430e-05  Data: 0.010 (0.011)
Train: 494 [1100/1251 ( 88%)]  Loss: 2.820 (3.09)  Time: 0.810s, 1264.37/s  (0.790s, 1296.81/s)  LR: 8.430e-05  Data: 0.010 (0.011)
Train: 494 [1150/1251 ( 92%)]  Loss: 2.820 (3.08)  Time: 0.772s, 1325.85/s  (0.790s, 1296.79/s)  LR: 8.430e-05  Data: 0.009 (0.011)
Train: 494 [1200/1251 ( 96%)]  Loss: 3.144 (3.08)  Time: 0.817s, 1252.71/s  (0.790s, 1296.82/s)  LR: 8.430e-05  Data: 0.009 (0.011)
Train: 494 [1250/1251 (100%)]  Loss: 2.952 (3.08)  Time: 0.810s, 1264.22/s  (0.790s, 1296.34/s)  LR: 8.430e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.525 (1.525)  Loss:  0.6357 (0.6357)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.194 (0.572)  Loss:  0.7568 (1.0694)  Acc@1: 86.5566 (80.0020)  Acc@5: 98.5849 (95.1000)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-494.pth.tar', 80.00199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-492.pth.tar', 79.86000010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-491.pth.tar', 79.84000002685546)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-490.pth.tar', 79.83200005371094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-489.pth.tar', 79.81999997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-487.pth.tar', 79.74600013183594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-486.pth.tar', 79.72199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-488.pth.tar', 79.6860000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-471.pth.tar', 79.68199995117187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-483.pth.tar', 79.66799998046875)

Train: 495 [   0/1251 (  0%)]  Loss: 2.932 (2.93)  Time: 2.332s,  439.05/s  (2.332s,  439.05/s)  LR: 8.294e-05  Data: 1.598 (1.598)
Train: 495 [  50/1251 (  4%)]  Loss: 3.061 (3.00)  Time: 0.783s, 1307.12/s  (0.820s, 1248.47/s)  LR: 8.294e-05  Data: 0.010 (0.046)
Train: 495 [ 100/1251 (  8%)]  Loss: 3.033 (3.01)  Time: 0.814s, 1257.70/s  (0.810s, 1264.25/s)  LR: 8.294e-05  Data: 0.011 (0.028)
Train: 495 [ 150/1251 ( 12%)]  Loss: 3.097 (3.03)  Time: 0.845s, 1212.45/s  (0.808s, 1267.02/s)  LR: 8.294e-05  Data: 0.009 (0.022)
Train: 495 [ 200/1251 ( 16%)]  Loss: 3.156 (3.06)  Time: 0.815s, 1256.04/s  (0.803s, 1275.71/s)  LR: 8.294e-05  Data: 0.009 (0.019)
Train: 495 [ 250/1251 ( 20%)]  Loss: 3.075 (3.06)  Time: 0.809s, 1265.85/s  (0.799s, 1281.35/s)  LR: 8.294e-05  Data: 0.010 (0.017)
Train: 495 [ 300/1251 ( 24%)]  Loss: 2.823 (3.03)  Time: 0.781s, 1310.42/s  (0.796s, 1285.63/s)  LR: 8.294e-05  Data: 0.010 (0.016)
Train: 495 [ 350/1251 ( 28%)]  Loss: 2.701 (2.98)  Time: 0.817s, 1252.71/s  (0.796s, 1286.09/s)  LR: 8.294e-05  Data: 0.011 (0.015)
Train: 495 [ 400/1251 ( 32%)]  Loss: 3.175 (3.01)  Time: 0.779s, 1314.16/s  (0.795s, 1287.29/s)  LR: 8.294e-05  Data: 0.009 (0.014)
Train: 495 [ 450/1251 ( 36%)]  Loss: 2.899 (3.00)  Time: 0.914s, 1120.75/s  (0.796s, 1286.49/s)  LR: 8.294e-05  Data: 0.010 (0.014)
Train: 495 [ 500/1251 ( 40%)]  Loss: 3.181 (3.01)  Time: 0.808s, 1267.91/s  (0.796s, 1286.82/s)  LR: 8.294e-05  Data: 0.009 (0.013)
Train: 495 [ 550/1251 ( 44%)]  Loss: 2.897 (3.00)  Time: 0.853s, 1200.85/s  (0.795s, 1287.84/s)  LR: 8.294e-05  Data: 0.009 (0.013)
Train: 495 [ 600/1251 ( 48%)]  Loss: 3.104 (3.01)  Time: 0.776s, 1319.63/s  (0.795s, 1287.92/s)  LR: 8.294e-05  Data: 0.009 (0.013)
Train: 495 [ 650/1251 ( 52%)]  Loss: 3.310 (3.03)  Time: 0.788s, 1299.57/s  (0.794s, 1289.32/s)  LR: 8.294e-05  Data: 0.009 (0.012)
Train: 495 [ 700/1251 ( 56%)]  Loss: 2.832 (3.02)  Time: 0.802s, 1276.32/s  (0.795s, 1288.69/s)  LR: 8.294e-05  Data: 0.010 (0.012)
Train: 495 [ 750/1251 ( 60%)]  Loss: 3.112 (3.02)  Time: 0.819s, 1251.06/s  (0.794s, 1289.96/s)  LR: 8.294e-05  Data: 0.010 (0.012)
Train: 495 [ 800/1251 ( 64%)]  Loss: 3.049 (3.03)  Time: 0.773s, 1324.82/s  (0.793s, 1291.56/s)  LR: 8.294e-05  Data: 0.009 (0.012)
Train: 495 [ 850/1251 ( 68%)]  Loss: 3.150 (3.03)  Time: 0.820s, 1249.19/s  (0.793s, 1291.81/s)  LR: 8.294e-05  Data: 0.009 (0.012)
Train: 495 [ 900/1251 ( 72%)]  Loss: 2.983 (3.03)  Time: 0.772s, 1326.10/s  (0.793s, 1292.05/s)  LR: 8.294e-05  Data: 0.010 (0.012)
Train: 495 [ 950/1251 ( 76%)]  Loss: 3.024 (3.03)  Time: 0.783s, 1307.71/s  (0.792s, 1292.95/s)  LR: 8.294e-05  Data: 0.009 (0.012)
Train: 495 [1000/1251 ( 80%)]  Loss: 2.898 (3.02)  Time: 0.775s, 1320.56/s  (0.792s, 1292.64/s)  LR: 8.294e-05  Data: 0.009 (0.011)
Train: 495 [1050/1251 ( 84%)]  Loss: 3.314 (3.04)  Time: 0.782s, 1308.72/s  (0.792s, 1292.14/s)  LR: 8.294e-05  Data: 0.009 (0.011)
Train: 495 [1100/1251 ( 88%)]  Loss: 2.939 (3.03)  Time: 0.807s, 1269.46/s  (0.793s, 1291.78/s)  LR: 8.294e-05  Data: 0.009 (0.011)
Train: 495 [1150/1251 ( 92%)]  Loss: 2.959 (3.03)  Time: 0.819s, 1250.20/s  (0.793s, 1291.45/s)  LR: 8.294e-05  Data: 0.009 (0.011)
Train: 495 [1200/1251 ( 96%)]  Loss: 3.103 (3.03)  Time: 0.802s, 1277.12/s  (0.793s, 1291.22/s)  LR: 8.294e-05  Data: 0.009 (0.011)
Train: 495 [1250/1251 (100%)]  Loss: 2.839 (3.02)  Time: 0.761s, 1345.38/s  (0.793s, 1291.43/s)  LR: 8.294e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.649 (1.649)  Loss:  0.6528 (0.6528)  Acc@1: 92.2852 (92.2852)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.194 (0.566)  Loss:  0.7759 (1.1183)  Acc@1: 86.2028 (79.6980)  Acc@5: 97.5236 (94.9380)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-494.pth.tar', 80.00199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-492.pth.tar', 79.86000010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-491.pth.tar', 79.84000002685546)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-490.pth.tar', 79.83200005371094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-489.pth.tar', 79.81999997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-487.pth.tar', 79.74600013183594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-486.pth.tar', 79.72199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-495.pth.tar', 79.69799995361328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-488.pth.tar', 79.6860000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-471.pth.tar', 79.68199995117187)

Train: 496 [   0/1251 (  0%)]  Loss: 3.057 (3.06)  Time: 2.238s,  457.48/s  (2.238s,  457.48/s)  LR: 8.159e-05  Data: 1.510 (1.510)
Train: 496 [  50/1251 (  4%)]  Loss: 3.253 (3.15)  Time: 0.824s, 1243.30/s  (0.819s, 1249.74/s)  LR: 8.159e-05  Data: 0.011 (0.045)
Train: 496 [ 100/1251 (  8%)]  Loss: 3.273 (3.19)  Time: 0.829s, 1234.62/s  (0.807s, 1268.15/s)  LR: 8.159e-05  Data: 0.010 (0.028)
Train: 496 [ 150/1251 ( 12%)]  Loss: 2.979 (3.14)  Time: 0.814s, 1258.56/s  (0.801s, 1278.26/s)  LR: 8.159e-05  Data: 0.011 (0.022)
Train: 496 [ 200/1251 ( 16%)]  Loss: 3.051 (3.12)  Time: 0.773s, 1325.34/s  (0.798s, 1282.45/s)  LR: 8.159e-05  Data: 0.010 (0.019)
Train: 496 [ 250/1251 ( 20%)]  Loss: 3.097 (3.12)  Time: 0.828s, 1236.87/s  (0.799s, 1281.88/s)  LR: 8.159e-05  Data: 0.010 (0.017)
Train: 496 [ 300/1251 ( 24%)]  Loss: 3.145 (3.12)  Time: 0.808s, 1268.09/s  (0.797s, 1284.77/s)  LR: 8.159e-05  Data: 0.010 (0.016)
Train: 496 [ 350/1251 ( 28%)]  Loss: 3.004 (3.11)  Time: 0.810s, 1263.80/s  (0.797s, 1285.33/s)  LR: 8.159e-05  Data: 0.009 (0.015)
Train: 496 [ 400/1251 ( 32%)]  Loss: 3.442 (3.14)  Time: 0.788s, 1300.27/s  (0.795s, 1287.50/s)  LR: 8.159e-05  Data: 0.011 (0.015)
Train: 496 [ 450/1251 ( 36%)]  Loss: 2.971 (3.13)  Time: 0.815s, 1256.21/s  (0.796s, 1287.04/s)  LR: 8.159e-05  Data: 0.012 (0.014)
Train: 496 [ 500/1251 ( 40%)]  Loss: 2.946 (3.11)  Time: 0.769s, 1330.93/s  (0.795s, 1287.64/s)  LR: 8.159e-05  Data: 0.009 (0.014)
Train: 496 [ 550/1251 ( 44%)]  Loss: 2.902 (3.09)  Time: 0.773s, 1324.26/s  (0.796s, 1287.09/s)  LR: 8.159e-05  Data: 0.010 (0.013)
Train: 496 [ 600/1251 ( 48%)]  Loss: 2.827 (3.07)  Time: 0.823s, 1244.22/s  (0.795s, 1288.31/s)  LR: 8.159e-05  Data: 0.010 (0.013)
Train: 496 [ 650/1251 ( 52%)]  Loss: 2.874 (3.06)  Time: 0.774s, 1323.38/s  (0.794s, 1289.41/s)  LR: 8.159e-05  Data: 0.009 (0.013)
Train: 496 [ 700/1251 ( 56%)]  Loss: 3.184 (3.07)  Time: 0.773s, 1324.72/s  (0.795s, 1288.54/s)  LR: 8.159e-05  Data: 0.010 (0.013)
Train: 496 [ 750/1251 ( 60%)]  Loss: 3.283 (3.08)  Time: 0.807s, 1269.37/s  (0.795s, 1288.67/s)  LR: 8.159e-05  Data: 0.010 (0.012)
Train: 496 [ 800/1251 ( 64%)]  Loss: 3.130 (3.08)  Time: 0.781s, 1310.36/s  (0.794s, 1289.04/s)  LR: 8.159e-05  Data: 0.010 (0.012)
Train: 496 [ 850/1251 ( 68%)]  Loss: 3.107 (3.08)  Time: 0.776s, 1319.02/s  (0.794s, 1290.46/s)  LR: 8.159e-05  Data: 0.010 (0.012)
Train: 496 [ 900/1251 ( 72%)]  Loss: 3.391 (3.10)  Time: 0.773s, 1324.25/s  (0.793s, 1290.88/s)  LR: 8.159e-05  Data: 0.009 (0.012)
Train: 496 [ 950/1251 ( 76%)]  Loss: 3.308 (3.11)  Time: 0.815s, 1256.64/s  (0.794s, 1289.98/s)  LR: 8.159e-05  Data: 0.011 (0.012)
Train: 496 [1000/1251 ( 80%)]  Loss: 2.813 (3.10)  Time: 0.823s, 1244.22/s  (0.794s, 1289.34/s)  LR: 8.159e-05  Data: 0.010 (0.012)
Train: 496 [1050/1251 ( 84%)]  Loss: 3.126 (3.10)  Time: 0.772s, 1325.63/s  (0.794s, 1289.99/s)  LR: 8.159e-05  Data: 0.010 (0.012)
Train: 496 [1100/1251 ( 88%)]  Loss: 3.153 (3.10)  Time: 0.868s, 1179.60/s  (0.793s, 1290.84/s)  LR: 8.159e-05  Data: 0.009 (0.012)
Train: 496 [1150/1251 ( 92%)]  Loss: 3.007 (3.10)  Time: 0.811s, 1262.33/s  (0.793s, 1290.97/s)  LR: 8.159e-05  Data: 0.009 (0.012)
Train: 496 [1200/1251 ( 96%)]  Loss: 2.973 (3.09)  Time: 0.779s, 1315.30/s  (0.793s, 1291.19/s)  LR: 8.159e-05  Data: 0.010 (0.011)
Train: 496 [1250/1251 (100%)]  Loss: 3.206 (3.10)  Time: 0.789s, 1298.01/s  (0.793s, 1291.72/s)  LR: 8.159e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.557 (1.557)  Loss:  0.6816 (0.6816)  Acc@1: 91.3086 (91.3086)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.193 (0.568)  Loss:  0.7603 (1.1227)  Acc@1: 87.2642 (79.8480)  Acc@5: 97.9953 (94.9880)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-494.pth.tar', 80.00199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-492.pth.tar', 79.86000010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-496.pth.tar', 79.84800002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-491.pth.tar', 79.84000002685546)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-490.pth.tar', 79.83200005371094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-489.pth.tar', 79.81999997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-487.pth.tar', 79.74600013183594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-486.pth.tar', 79.72199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-495.pth.tar', 79.69799995361328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-488.pth.tar', 79.6860000024414)

Train: 497 [   0/1251 (  0%)]  Loss: 3.070 (3.07)  Time: 2.233s,  458.55/s  (2.233s,  458.55/s)  LR: 8.026e-05  Data: 1.508 (1.508)
Train: 497 [  50/1251 (  4%)]  Loss: 3.044 (3.06)  Time: 0.777s, 1317.71/s  (0.823s, 1244.60/s)  LR: 8.026e-05  Data: 0.010 (0.044)
Train: 497 [ 100/1251 (  8%)]  Loss: 2.797 (2.97)  Time: 0.772s, 1327.18/s  (0.804s, 1274.04/s)  LR: 8.026e-05  Data: 0.010 (0.027)
Train: 497 [ 150/1251 ( 12%)]  Loss: 3.001 (2.98)  Time: 0.808s, 1267.10/s  (0.801s, 1278.77/s)  LR: 8.026e-05  Data: 0.010 (0.022)
Train: 497 [ 200/1251 ( 16%)]  Loss: 3.162 (3.01)  Time: 0.820s, 1248.08/s  (0.805s, 1271.95/s)  LR: 8.026e-05  Data: 0.011 (0.019)
Train: 497 [ 250/1251 ( 20%)]  Loss: 3.405 (3.08)  Time: 0.773s, 1325.09/s  (0.806s, 1269.93/s)  LR: 8.026e-05  Data: 0.010 (0.017)
Train: 497 [ 300/1251 ( 24%)]  Loss: 3.017 (3.07)  Time: 0.773s, 1324.76/s  (0.803s, 1275.45/s)  LR: 8.026e-05  Data: 0.010 (0.016)
Train: 497 [ 350/1251 ( 28%)]  Loss: 3.042 (3.07)  Time: 0.822s, 1246.32/s  (0.802s, 1276.25/s)  LR: 8.026e-05  Data: 0.009 (0.015)
Train: 497 [ 400/1251 ( 32%)]  Loss: 3.096 (3.07)  Time: 0.796s, 1286.41/s  (0.803s, 1275.08/s)  LR: 8.026e-05  Data: 0.016 (0.015)
Train: 497 [ 450/1251 ( 36%)]  Loss: 3.020 (3.07)  Time: 0.825s, 1241.94/s  (0.804s, 1274.33/s)  LR: 8.026e-05  Data: 0.009 (0.014)
Train: 497 [ 500/1251 ( 40%)]  Loss: 2.899 (3.05)  Time: 0.807s, 1268.57/s  (0.804s, 1272.93/s)  LR: 8.026e-05  Data: 0.010 (0.014)
Train: 497 [ 550/1251 ( 44%)]  Loss: 2.851 (3.03)  Time: 0.792s, 1292.19/s  (0.803s, 1274.47/s)  LR: 8.026e-05  Data: 0.010 (0.013)
Train: 497 [ 600/1251 ( 48%)]  Loss: 3.089 (3.04)  Time: 0.780s, 1312.76/s  (0.803s, 1275.71/s)  LR: 8.026e-05  Data: 0.014 (0.013)
Train: 497 [ 650/1251 ( 52%)]  Loss: 3.215 (3.05)  Time: 0.774s, 1323.26/s  (0.802s, 1276.14/s)  LR: 8.026e-05  Data: 0.010 (0.013)
Train: 497 [ 700/1251 ( 56%)]  Loss: 3.040 (3.05)  Time: 0.787s, 1300.48/s  (0.801s, 1278.60/s)  LR: 8.026e-05  Data: 0.010 (0.013)
Train: 497 [ 750/1251 ( 60%)]  Loss: 3.133 (3.06)  Time: 0.773s, 1323.87/s  (0.800s, 1279.55/s)  LR: 8.026e-05  Data: 0.010 (0.013)
Train: 497 [ 800/1251 ( 64%)]  Loss: 3.224 (3.07)  Time: 0.781s, 1310.62/s  (0.799s, 1281.51/s)  LR: 8.026e-05  Data: 0.010 (0.012)
Train: 497 [ 850/1251 ( 68%)]  Loss: 2.761 (3.05)  Time: 0.772s, 1327.01/s  (0.799s, 1282.32/s)  LR: 8.026e-05  Data: 0.010 (0.012)
Train: 497 [ 900/1251 ( 72%)]  Loss: 2.892 (3.04)  Time: 0.815s, 1257.06/s  (0.798s, 1282.53/s)  LR: 8.026e-05  Data: 0.011 (0.012)
Train: 497 [ 950/1251 ( 76%)]  Loss: 3.075 (3.04)  Time: 0.815s, 1256.53/s  (0.798s, 1282.83/s)  LR: 8.026e-05  Data: 0.009 (0.012)
Train: 497 [1000/1251 ( 80%)]  Loss: 3.276 (3.05)  Time: 0.819s, 1250.42/s  (0.798s, 1283.12/s)  LR: 8.026e-05  Data: 0.010 (0.012)
Train: 497 [1050/1251 ( 84%)]  Loss: 2.880 (3.05)  Time: 0.814s, 1257.26/s  (0.798s, 1283.72/s)  LR: 8.026e-05  Data: 0.009 (0.012)
Train: 497 [1100/1251 ( 88%)]  Loss: 3.332 (3.06)  Time: 0.808s, 1267.21/s  (0.797s, 1284.30/s)  LR: 8.026e-05  Data: 0.009 (0.012)
Train: 497 [1150/1251 ( 92%)]  Loss: 3.094 (3.06)  Time: 0.808s, 1267.53/s  (0.797s, 1285.08/s)  LR: 8.026e-05  Data: 0.010 (0.012)
Train: 497 [1200/1251 ( 96%)]  Loss: 3.008 (3.06)  Time: 0.775s, 1320.83/s  (0.796s, 1285.94/s)  LR: 8.026e-05  Data: 0.010 (0.011)
Train: 497 [1250/1251 (100%)]  Loss: 3.120 (3.06)  Time: 0.804s, 1273.65/s  (0.796s, 1286.69/s)  LR: 8.026e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.617 (1.617)  Loss:  0.6602 (0.6602)  Acc@1: 91.6992 (91.6992)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.194 (0.565)  Loss:  0.7866 (1.1501)  Acc@1: 86.4387 (79.7060)  Acc@5: 97.5236 (95.1220)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-494.pth.tar', 80.00199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-492.pth.tar', 79.86000010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-496.pth.tar', 79.84800002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-491.pth.tar', 79.84000002685546)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-490.pth.tar', 79.83200005371094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-489.pth.tar', 79.81999997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-487.pth.tar', 79.74600013183594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-486.pth.tar', 79.72199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-497.pth.tar', 79.70600005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-495.pth.tar', 79.69799995361328)

Train: 498 [   0/1251 (  0%)]  Loss: 3.128 (3.13)  Time: 2.194s,  466.63/s  (2.194s,  466.63/s)  LR: 7.893e-05  Data: 1.474 (1.474)
Train: 498 [  50/1251 (  4%)]  Loss: 2.882 (3.01)  Time: 0.814s, 1257.72/s  (0.828s, 1236.67/s)  LR: 7.893e-05  Data: 0.011 (0.044)
Train: 498 [ 100/1251 (  8%)]  Loss: 2.770 (2.93)  Time: 0.774s, 1322.64/s  (0.809s, 1265.14/s)  LR: 7.893e-05  Data: 0.010 (0.027)
Train: 498 [ 150/1251 ( 12%)]  Loss: 2.871 (2.91)  Time: 0.773s, 1325.06/s  (0.801s, 1278.55/s)  LR: 7.893e-05  Data: 0.009 (0.021)
Train: 498 [ 200/1251 ( 16%)]  Loss: 2.994 (2.93)  Time: 0.807s, 1268.17/s  (0.796s, 1286.11/s)  LR: 7.893e-05  Data: 0.010 (0.019)
Train: 498 [ 250/1251 ( 20%)]  Loss: 3.097 (2.96)  Time: 0.776s, 1319.57/s  (0.794s, 1289.65/s)  LR: 7.893e-05  Data: 0.010 (0.017)
Train: 498 [ 300/1251 ( 24%)]  Loss: 3.307 (3.01)  Time: 0.771s, 1327.37/s  (0.791s, 1294.29/s)  LR: 7.893e-05  Data: 0.010 (0.016)
Train: 498 [ 350/1251 ( 28%)]  Loss: 3.201 (3.03)  Time: 0.772s, 1326.92/s  (0.791s, 1294.60/s)  LR: 7.893e-05  Data: 0.009 (0.015)
Train: 498 [ 400/1251 ( 32%)]  Loss: 3.181 (3.05)  Time: 0.772s, 1325.95/s  (0.792s, 1293.08/s)  LR: 7.893e-05  Data: 0.009 (0.014)
Train: 498 [ 450/1251 ( 36%)]  Loss: 2.973 (3.04)  Time: 0.820s, 1248.76/s  (0.793s, 1291.50/s)  LR: 7.893e-05  Data: 0.011 (0.014)
Train: 498 [ 500/1251 ( 40%)]  Loss: 3.065 (3.04)  Time: 0.773s, 1324.62/s  (0.793s, 1291.51/s)  LR: 7.893e-05  Data: 0.010 (0.013)
Train: 498 [ 550/1251 ( 44%)]  Loss: 2.924 (3.03)  Time: 0.823s, 1244.52/s  (0.793s, 1291.63/s)  LR: 7.893e-05  Data: 0.010 (0.013)
Train: 498 [ 600/1251 ( 48%)]  Loss: 3.109 (3.04)  Time: 0.807s, 1269.43/s  (0.792s, 1292.12/s)  LR: 7.893e-05  Data: 0.010 (0.013)
Train: 498 [ 650/1251 ( 52%)]  Loss: 3.243 (3.05)  Time: 0.773s, 1324.92/s  (0.792s, 1293.03/s)  LR: 7.893e-05  Data: 0.009 (0.012)
Train: 498 [ 700/1251 ( 56%)]  Loss: 3.079 (3.05)  Time: 0.809s, 1265.45/s  (0.792s, 1293.73/s)  LR: 7.893e-05  Data: 0.010 (0.012)
Train: 498 [ 750/1251 ( 60%)]  Loss: 2.687 (3.03)  Time: 0.773s, 1324.19/s  (0.791s, 1294.70/s)  LR: 7.893e-05  Data: 0.009 (0.012)
Train: 498 [ 800/1251 ( 64%)]  Loss: 3.298 (3.05)  Time: 0.773s, 1324.36/s  (0.791s, 1294.99/s)  LR: 7.893e-05  Data: 0.009 (0.012)
Train: 498 [ 850/1251 ( 68%)]  Loss: 2.895 (3.04)  Time: 0.773s, 1324.03/s  (0.791s, 1294.72/s)  LR: 7.893e-05  Data: 0.009 (0.012)
Train: 498 [ 900/1251 ( 72%)]  Loss: 3.020 (3.04)  Time: 0.772s, 1327.15/s  (0.790s, 1295.57/s)  LR: 7.893e-05  Data: 0.009 (0.012)
Train: 498 [ 950/1251 ( 76%)]  Loss: 3.093 (3.04)  Time: 0.775s, 1322.06/s  (0.791s, 1294.10/s)  LR: 7.893e-05  Data: 0.010 (0.012)
Train: 498 [1000/1251 ( 80%)]  Loss: 3.247 (3.05)  Time: 0.814s, 1258.14/s  (0.792s, 1293.23/s)  LR: 7.893e-05  Data: 0.011 (0.012)
Train: 498 [1050/1251 ( 84%)]  Loss: 2.710 (3.04)  Time: 0.774s, 1322.65/s  (0.792s, 1293.10/s)  LR: 7.893e-05  Data: 0.009 (0.011)
Train: 498 [1100/1251 ( 88%)]  Loss: 2.770 (3.02)  Time: 0.809s, 1266.06/s  (0.792s, 1292.25/s)  LR: 7.893e-05  Data: 0.010 (0.011)
Train: 498 [1150/1251 ( 92%)]  Loss: 3.200 (3.03)  Time: 0.774s, 1323.25/s  (0.792s, 1292.45/s)  LR: 7.893e-05  Data: 0.009 (0.011)
Train: 498 [1200/1251 ( 96%)]  Loss: 3.280 (3.04)  Time: 0.777s, 1317.22/s  (0.792s, 1292.78/s)  LR: 7.893e-05  Data: 0.012 (0.011)
Train: 498 [1250/1251 (100%)]  Loss: 2.903 (3.04)  Time: 0.759s, 1348.83/s  (0.792s, 1292.63/s)  LR: 7.893e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.531 (1.531)  Loss:  0.6060 (0.6060)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.194 (0.572)  Loss:  0.7129 (1.0543)  Acc@1: 86.6745 (80.1080)  Acc@5: 97.6415 (95.1560)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-498.pth.tar', 80.10800002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-494.pth.tar', 80.00199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-492.pth.tar', 79.86000010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-496.pth.tar', 79.84800002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-491.pth.tar', 79.84000002685546)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-490.pth.tar', 79.83200005371094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-489.pth.tar', 79.81999997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-487.pth.tar', 79.74600013183594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-486.pth.tar', 79.72199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-497.pth.tar', 79.70600005615235)

Train: 499 [   0/1251 (  0%)]  Loss: 3.174 (3.17)  Time: 2.305s,  444.30/s  (2.305s,  444.30/s)  LR: 7.762e-05  Data: 1.579 (1.579)
Train: 499 [  50/1251 (  4%)]  Loss: 2.891 (3.03)  Time: 0.774s, 1323.00/s  (0.835s, 1226.13/s)  LR: 7.762e-05  Data: 0.009 (0.045)
Train: 499 [ 100/1251 (  8%)]  Loss: 3.125 (3.06)  Time: 0.822s, 1245.76/s  (0.826s, 1239.39/s)  LR: 7.762e-05  Data: 0.011 (0.028)
Train: 499 [ 150/1251 ( 12%)]  Loss: 2.938 (3.03)  Time: 0.814s, 1258.25/s  (0.821s, 1246.87/s)  LR: 7.762e-05  Data: 0.011 (0.023)
Train: 499 [ 200/1251 ( 16%)]  Loss: 3.502 (3.13)  Time: 0.772s, 1326.09/s  (0.815s, 1257.03/s)  LR: 7.762e-05  Data: 0.009 (0.020)
Train: 499 [ 250/1251 ( 20%)]  Loss: 3.176 (3.13)  Time: 0.773s, 1324.83/s  (0.808s, 1266.57/s)  LR: 7.762e-05  Data: 0.010 (0.018)
Train: 499 [ 300/1251 ( 24%)]  Loss: 2.936 (3.11)  Time: 0.804s, 1273.46/s  (0.807s, 1268.47/s)  LR: 7.762e-05  Data: 0.010 (0.016)
Train: 499 [ 350/1251 ( 28%)]  Loss: 3.036 (3.10)  Time: 0.781s, 1310.48/s  (0.805s, 1271.30/s)  LR: 7.762e-05  Data: 0.010 (0.015)
Train: 499 [ 400/1251 ( 32%)]  Loss: 3.150 (3.10)  Time: 0.802s, 1276.38/s  (0.806s, 1270.03/s)  LR: 7.762e-05  Data: 0.017 (0.015)
Train: 499 [ 450/1251 ( 36%)]  Loss: 2.920 (3.08)  Time: 0.814s, 1257.95/s  (0.806s, 1270.84/s)  LR: 7.762e-05  Data: 0.011 (0.014)
Train: 499 [ 500/1251 ( 40%)]  Loss: 2.821 (3.06)  Time: 0.774s, 1322.90/s  (0.804s, 1273.61/s)  LR: 7.762e-05  Data: 0.010 (0.014)
Train: 499 [ 550/1251 ( 44%)]  Loss: 3.108 (3.06)  Time: 0.772s, 1327.01/s  (0.802s, 1276.55/s)  LR: 7.762e-05  Data: 0.010 (0.014)
Train: 499 [ 600/1251 ( 48%)]  Loss: 3.260 (3.08)  Time: 0.784s, 1305.67/s  (0.802s, 1276.10/s)  LR: 7.762e-05  Data: 0.010 (0.013)
Train: 499 [ 650/1251 ( 52%)]  Loss: 3.068 (3.08)  Time: 0.785s, 1304.53/s  (0.801s, 1278.25/s)  LR: 7.762e-05  Data: 0.009 (0.013)
Train: 499 [ 700/1251 ( 56%)]  Loss: 2.984 (3.07)  Time: 0.772s, 1325.91/s  (0.800s, 1279.54/s)  LR: 7.762e-05  Data: 0.010 (0.013)
Train: 499 [ 750/1251 ( 60%)]  Loss: 3.019 (3.07)  Time: 0.809s, 1266.29/s  (0.799s, 1280.90/s)  LR: 7.762e-05  Data: 0.010 (0.013)
Train: 499 [ 800/1251 ( 64%)]  Loss: 3.031 (3.07)  Time: 0.780s, 1312.35/s  (0.799s, 1281.54/s)  LR: 7.762e-05  Data: 0.012 (0.012)
Train: 499 [ 850/1251 ( 68%)]  Loss: 2.913 (3.06)  Time: 0.772s, 1326.47/s  (0.798s, 1282.80/s)  LR: 7.762e-05  Data: 0.009 (0.012)
Train: 499 [ 900/1251 ( 72%)]  Loss: 3.154 (3.06)  Time: 0.772s, 1327.00/s  (0.798s, 1283.94/s)  LR: 7.762e-05  Data: 0.009 (0.012)
Train: 499 [ 950/1251 ( 76%)]  Loss: 3.222 (3.07)  Time: 0.806s, 1270.22/s  (0.797s, 1284.55/s)  LR: 7.762e-05  Data: 0.009 (0.012)
Train: 499 [1000/1251 ( 80%)]  Loss: 3.219 (3.08)  Time: 0.826s, 1240.24/s  (0.797s, 1284.04/s)  LR: 7.762e-05  Data: 0.012 (0.012)
Train: 499 [1050/1251 ( 84%)]  Loss: 3.251 (3.09)  Time: 0.805s, 1271.58/s  (0.797s, 1284.50/s)  LR: 7.762e-05  Data: 0.009 (0.012)
Train: 499 [1100/1251 ( 88%)]  Loss: 2.550 (3.06)  Time: 0.778s, 1316.64/s  (0.797s, 1284.98/s)  LR: 7.762e-05  Data: 0.010 (0.012)
Train: 499 [1150/1251 ( 92%)]  Loss: 2.964 (3.06)  Time: 0.838s, 1221.72/s  (0.798s, 1283.60/s)  LR: 7.762e-05  Data: 0.017 (0.012)
Train: 499 [1200/1251 ( 96%)]  Loss: 3.257 (3.07)  Time: 0.814s, 1258.69/s  (0.799s, 1281.71/s)  LR: 7.762e-05  Data: 0.010 (0.012)
Train: 499 [1250/1251 (100%)]  Loss: 3.130 (3.07)  Time: 0.762s, 1343.00/s  (0.799s, 1281.81/s)  LR: 7.762e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.558 (1.558)  Loss:  0.6367 (0.6367)  Acc@1: 93.2617 (93.2617)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.194 (0.566)  Loss:  0.7617 (1.1110)  Acc@1: 86.9104 (79.8320)  Acc@5: 98.1132 (95.0920)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-498.pth.tar', 80.10800002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-494.pth.tar', 80.00199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-492.pth.tar', 79.86000010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-496.pth.tar', 79.84800002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-491.pth.tar', 79.84000002685546)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-490.pth.tar', 79.83200005371094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-499.pth.tar', 79.8320000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-489.pth.tar', 79.81999997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-487.pth.tar', 79.74600013183594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-486.pth.tar', 79.72199997802734)

Train: 500 [   0/1251 (  0%)]  Loss: 3.423 (3.42)  Time: 2.202s,  464.96/s  (2.202s,  464.96/s)  LR: 7.632e-05  Data: 1.472 (1.472)
Train: 500 [  50/1251 (  4%)]  Loss: 2.966 (3.19)  Time: 0.785s, 1305.19/s  (0.833s, 1229.32/s)  LR: 7.632e-05  Data: 0.009 (0.045)
Train: 500 [ 100/1251 (  8%)]  Loss: 3.006 (3.13)  Time: 0.807s, 1268.68/s  (0.812s, 1260.71/s)  LR: 7.632e-05  Data: 0.009 (0.028)
Train: 500 [ 150/1251 ( 12%)]  Loss: 2.834 (3.06)  Time: 0.810s, 1264.49/s  (0.810s, 1264.00/s)  LR: 7.632e-05  Data: 0.009 (0.022)
Train: 500 [ 200/1251 ( 16%)]  Loss: 3.271 (3.10)  Time: 0.774s, 1323.64/s  (0.806s, 1271.23/s)  LR: 7.632e-05  Data: 0.009 (0.019)
Train: 500 [ 250/1251 ( 20%)]  Loss: 2.985 (3.08)  Time: 0.773s, 1324.48/s  (0.803s, 1275.57/s)  LR: 7.632e-05  Data: 0.010 (0.017)
Train: 500 [ 300/1251 ( 24%)]  Loss: 3.367 (3.12)  Time: 0.812s, 1261.74/s  (0.802s, 1277.40/s)  LR: 7.632e-05  Data: 0.009 (0.016)
Train: 500 [ 350/1251 ( 28%)]  Loss: 2.972 (3.10)  Time: 0.775s, 1321.90/s  (0.800s, 1279.67/s)  LR: 7.632e-05  Data: 0.009 (0.015)
Train: 500 [ 400/1251 ( 32%)]  Loss: 3.337 (3.13)  Time: 0.809s, 1266.50/s  (0.800s, 1280.70/s)  LR: 7.632e-05  Data: 0.010 (0.014)
Train: 500 [ 450/1251 ( 36%)]  Loss: 3.163 (3.13)  Time: 0.796s, 1286.15/s  (0.799s, 1281.59/s)  LR: 7.632e-05  Data: 0.010 (0.014)
Train: 500 [ 500/1251 ( 40%)]  Loss: 2.789 (3.10)  Time: 0.814s, 1257.57/s  (0.798s, 1282.54/s)  LR: 7.632e-05  Data: 0.011 (0.013)
Train: 500 [ 550/1251 ( 44%)]  Loss: 2.655 (3.06)  Time: 0.775s, 1320.90/s  (0.797s, 1285.07/s)  LR: 7.632e-05  Data: 0.009 (0.013)
Train: 500 [ 600/1251 ( 48%)]  Loss: 3.336 (3.08)  Time: 0.783s, 1308.15/s  (0.796s, 1286.97/s)  LR: 7.632e-05  Data: 0.009 (0.013)
Train: 500 [ 650/1251 ( 52%)]  Loss: 3.329 (3.10)  Time: 0.806s, 1269.88/s  (0.795s, 1288.82/s)  LR: 7.632e-05  Data: 0.009 (0.012)
Train: 500 [ 700/1251 ( 56%)]  Loss: 2.967 (3.09)  Time: 0.773s, 1324.52/s  (0.793s, 1290.65/s)  LR: 7.632e-05  Data: 0.009 (0.012)
Train: 500 [ 750/1251 ( 60%)]  Loss: 2.986 (3.09)  Time: 0.809s, 1265.82/s  (0.793s, 1291.04/s)  LR: 7.632e-05  Data: 0.014 (0.012)
Train: 500 [ 800/1251 ( 64%)]  Loss: 3.257 (3.10)  Time: 0.777s, 1318.43/s  (0.793s, 1291.27/s)  LR: 7.632e-05  Data: 0.010 (0.012)
Train: 500 [ 850/1251 ( 68%)]  Loss: 3.146 (3.10)  Time: 0.807s, 1268.78/s  (0.792s, 1292.18/s)  LR: 7.632e-05  Data: 0.009 (0.012)
Train: 500 [ 900/1251 ( 72%)]  Loss: 3.049 (3.10)  Time: 0.772s, 1325.88/s  (0.793s, 1291.72/s)  LR: 7.632e-05  Data: 0.010 (0.012)
Train: 500 [ 950/1251 ( 76%)]  Loss: 3.148 (3.10)  Time: 0.807s, 1268.14/s  (0.793s, 1291.17/s)  LR: 7.632e-05  Data: 0.009 (0.012)
Train: 500 [1000/1251 ( 80%)]  Loss: 3.282 (3.11)  Time: 0.812s, 1260.95/s  (0.793s, 1291.57/s)  LR: 7.632e-05  Data: 0.009 (0.011)
Train: 500 [1050/1251 ( 84%)]  Loss: 3.238 (3.11)  Time: 0.773s, 1324.91/s  (0.792s, 1292.19/s)  LR: 7.632e-05  Data: 0.009 (0.011)
Train: 500 [1100/1251 ( 88%)]  Loss: 3.482 (3.13)  Time: 0.773s, 1325.57/s  (0.792s, 1292.66/s)  LR: 7.632e-05  Data: 0.009 (0.011)
Train: 500 [1150/1251 ( 92%)]  Loss: 2.980 (3.12)  Time: 0.772s, 1325.73/s  (0.792s, 1292.25/s)  LR: 7.632e-05  Data: 0.010 (0.011)
Train: 500 [1200/1251 ( 96%)]  Loss: 2.982 (3.12)  Time: 0.798s, 1283.73/s  (0.793s, 1291.58/s)  LR: 7.632e-05  Data: 0.009 (0.011)
Train: 500 [1250/1251 (100%)]  Loss: 3.024 (3.11)  Time: 0.805s, 1272.77/s  (0.793s, 1291.43/s)  LR: 7.632e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.592 (1.592)  Loss:  0.6616 (0.6616)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.194 (0.564)  Loss:  0.7939 (1.1087)  Acc@1: 86.4387 (79.8960)  Acc@5: 97.8774 (95.1320)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-498.pth.tar', 80.10800002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-494.pth.tar', 80.00199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-500.pth.tar', 79.89600005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-492.pth.tar', 79.86000010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-496.pth.tar', 79.84800002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-491.pth.tar', 79.84000002685546)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-490.pth.tar', 79.83200005371094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-499.pth.tar', 79.8320000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-489.pth.tar', 79.81999997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-487.pth.tar', 79.74600013183594)

Train: 501 [   0/1251 (  0%)]  Loss: 2.781 (2.78)  Time: 2.243s,  456.48/s  (2.243s,  456.48/s)  LR: 7.503e-05  Data: 1.521 (1.521)
Train: 501 [  50/1251 (  4%)]  Loss: 3.125 (2.95)  Time: 0.774s, 1323.35/s  (0.825s, 1241.88/s)  LR: 7.503e-05  Data: 0.009 (0.047)
Train: 501 [ 100/1251 (  8%)]  Loss: 2.915 (2.94)  Time: 0.771s, 1328.24/s  (0.805s, 1272.24/s)  LR: 7.503e-05  Data: 0.009 (0.028)
Train: 501 [ 150/1251 ( 12%)]  Loss: 3.170 (3.00)  Time: 0.774s, 1322.22/s  (0.798s, 1282.89/s)  LR: 7.503e-05  Data: 0.009 (0.022)
Train: 501 [ 200/1251 ( 16%)]  Loss: 3.040 (3.01)  Time: 0.772s, 1326.04/s  (0.797s, 1285.15/s)  LR: 7.503e-05  Data: 0.010 (0.019)
Train: 501 [ 250/1251 ( 20%)]  Loss: 3.042 (3.01)  Time: 0.834s, 1227.85/s  (0.796s, 1286.24/s)  LR: 7.503e-05  Data: 0.017 (0.017)
Train: 501 [ 300/1251 ( 24%)]  Loss: 3.114 (3.03)  Time: 0.796s, 1286.46/s  (0.798s, 1283.48/s)  LR: 7.503e-05  Data: 0.010 (0.016)
Train: 501 [ 350/1251 ( 28%)]  Loss: 3.376 (3.07)  Time: 0.808s, 1266.68/s  (0.797s, 1284.10/s)  LR: 7.503e-05  Data: 0.009 (0.015)
Train: 501 [ 400/1251 ( 32%)]  Loss: 3.049 (3.07)  Time: 0.783s, 1307.33/s  (0.796s, 1287.10/s)  LR: 7.503e-05  Data: 0.009 (0.014)
Train: 501 [ 450/1251 ( 36%)]  Loss: 3.260 (3.09)  Time: 0.806s, 1271.02/s  (0.796s, 1287.04/s)  LR: 7.503e-05  Data: 0.009 (0.014)
Train: 501 [ 500/1251 ( 40%)]  Loss: 3.113 (3.09)  Time: 0.774s, 1322.99/s  (0.795s, 1288.60/s)  LR: 7.503e-05  Data: 0.010 (0.014)
Train: 501 [ 550/1251 ( 44%)]  Loss: 2.849 (3.07)  Time: 0.773s, 1324.16/s  (0.794s, 1289.91/s)  LR: 7.503e-05  Data: 0.010 (0.013)
Train: 501 [ 600/1251 ( 48%)]  Loss: 2.763 (3.05)  Time: 0.805s, 1271.53/s  (0.794s, 1290.32/s)  LR: 7.503e-05  Data: 0.010 (0.013)
Train: 501 [ 650/1251 ( 52%)]  Loss: 3.140 (3.05)  Time: 0.773s, 1325.02/s  (0.793s, 1291.75/s)  LR: 7.503e-05  Data: 0.009 (0.013)
Train: 501 [ 700/1251 ( 56%)]  Loss: 3.232 (3.06)  Time: 0.781s, 1311.22/s  (0.793s, 1292.11/s)  LR: 7.503e-05  Data: 0.011 (0.012)
Train: 501 [ 750/1251 ( 60%)]  Loss: 2.945 (3.06)  Time: 0.815s, 1256.96/s  (0.794s, 1289.55/s)  LR: 7.503e-05  Data: 0.009 (0.012)
Train: 501 [ 800/1251 ( 64%)]  Loss: 3.283 (3.07)  Time: 0.775s, 1322.00/s  (0.794s, 1289.52/s)  LR: 7.503e-05  Data: 0.010 (0.012)
Train: 501 [ 850/1251 ( 68%)]  Loss: 3.041 (3.07)  Time: 0.771s, 1327.67/s  (0.794s, 1289.15/s)  LR: 7.503e-05  Data: 0.009 (0.012)
Train: 501 [ 900/1251 ( 72%)]  Loss: 3.181 (3.07)  Time: 0.775s, 1321.74/s  (0.795s, 1288.33/s)  LR: 7.503e-05  Data: 0.009 (0.012)
Train: 501 [ 950/1251 ( 76%)]  Loss: 3.088 (3.08)  Time: 0.772s, 1325.96/s  (0.794s, 1289.35/s)  LR: 7.503e-05  Data: 0.009 (0.012)
Train: 501 [1000/1251 ( 80%)]  Loss: 3.104 (3.08)  Time: 0.774s, 1322.81/s  (0.794s, 1289.29/s)  LR: 7.503e-05  Data: 0.010 (0.012)
Train: 501 [1050/1251 ( 84%)]  Loss: 2.993 (3.07)  Time: 0.777s, 1317.48/s  (0.794s, 1290.24/s)  LR: 7.503e-05  Data: 0.010 (0.012)
Train: 501 [1100/1251 ( 88%)]  Loss: 3.207 (3.08)  Time: 0.775s, 1321.66/s  (0.793s, 1291.23/s)  LR: 7.503e-05  Data: 0.009 (0.011)
Train: 501 [1150/1251 ( 92%)]  Loss: 2.855 (3.07)  Time: 0.773s, 1325.46/s  (0.794s, 1289.90/s)  LR: 7.503e-05  Data: 0.009 (0.011)
Train: 501 [1200/1251 ( 96%)]  Loss: 3.167 (3.07)  Time: 0.850s, 1204.83/s  (0.793s, 1290.66/s)  LR: 7.503e-05  Data: 0.010 (0.011)
Train: 501 [1250/1251 (100%)]  Loss: 3.005 (3.07)  Time: 0.767s, 1335.26/s  (0.793s, 1291.47/s)  LR: 7.503e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.614 (1.614)  Loss:  0.6177 (0.6177)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.194 (0.563)  Loss:  0.7695 (1.1022)  Acc@1: 86.2028 (79.9460)  Acc@5: 98.2311 (95.1300)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-498.pth.tar', 80.10800002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-494.pth.tar', 80.00199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-501.pth.tar', 79.94599995361328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-500.pth.tar', 79.89600005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-492.pth.tar', 79.86000010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-496.pth.tar', 79.84800002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-491.pth.tar', 79.84000002685546)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-490.pth.tar', 79.83200005371094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-499.pth.tar', 79.8320000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-489.pth.tar', 79.81999997802734)

Train: 502 [   0/1251 (  0%)]  Loss: 3.015 (3.01)  Time: 2.176s,  470.61/s  (2.176s,  470.61/s)  LR: 7.375e-05  Data: 1.444 (1.444)
Train: 502 [  50/1251 (  4%)]  Loss: 3.085 (3.05)  Time: 0.775s, 1321.50/s  (0.824s, 1243.42/s)  LR: 7.375e-05  Data: 0.010 (0.044)
Train: 502 [ 100/1251 (  8%)]  Loss: 3.219 (3.11)  Time: 0.778s, 1315.88/s  (0.805s, 1271.85/s)  LR: 7.375e-05  Data: 0.011 (0.027)
Train: 502 [ 150/1251 ( 12%)]  Loss: 2.990 (3.08)  Time: 0.873s, 1172.68/s  (0.799s, 1281.22/s)  LR: 7.375e-05  Data: 0.009 (0.021)
Train: 502 [ 200/1251 ( 16%)]  Loss: 2.922 (3.05)  Time: 0.776s, 1319.72/s  (0.799s, 1282.00/s)  LR: 7.375e-05  Data: 0.010 (0.018)
Train: 502 [ 250/1251 ( 20%)]  Loss: 3.057 (3.05)  Time: 0.780s, 1312.02/s  (0.798s, 1283.85/s)  LR: 7.375e-05  Data: 0.009 (0.017)
Train: 502 [ 300/1251 ( 24%)]  Loss: 2.862 (3.02)  Time: 0.772s, 1327.10/s  (0.796s, 1287.08/s)  LR: 7.375e-05  Data: 0.009 (0.016)
Train: 502 [ 350/1251 ( 28%)]  Loss: 3.042 (3.02)  Time: 0.809s, 1265.92/s  (0.796s, 1286.52/s)  LR: 7.375e-05  Data: 0.010 (0.015)
Train: 502 [ 400/1251 ( 32%)]  Loss: 3.310 (3.06)  Time: 0.772s, 1326.42/s  (0.796s, 1286.62/s)  LR: 7.375e-05  Data: 0.009 (0.014)
Train: 502 [ 450/1251 ( 36%)]  Loss: 2.852 (3.04)  Time: 0.807s, 1269.33/s  (0.796s, 1286.66/s)  LR: 7.375e-05  Data: 0.010 (0.014)
Train: 502 [ 500/1251 ( 40%)]  Loss: 2.815 (3.02)  Time: 0.773s, 1325.07/s  (0.795s, 1288.23/s)  LR: 7.375e-05  Data: 0.010 (0.013)
Train: 502 [ 550/1251 ( 44%)]  Loss: 3.118 (3.02)  Time: 0.772s, 1325.79/s  (0.794s, 1288.89/s)  LR: 7.375e-05  Data: 0.010 (0.013)
Train: 502 [ 600/1251 ( 48%)]  Loss: 3.003 (3.02)  Time: 0.773s, 1325.54/s  (0.794s, 1289.96/s)  LR: 7.375e-05  Data: 0.009 (0.013)
Train: 502 [ 650/1251 ( 52%)]  Loss: 2.765 (3.00)  Time: 0.774s, 1323.41/s  (0.793s, 1290.76/s)  LR: 7.375e-05  Data: 0.010 (0.012)
Train: 502 [ 700/1251 ( 56%)]  Loss: 3.243 (3.02)  Time: 0.788s, 1299.47/s  (0.793s, 1291.57/s)  LR: 7.375e-05  Data: 0.009 (0.012)
Train: 502 [ 750/1251 ( 60%)]  Loss: 2.737 (3.00)  Time: 0.788s, 1299.33/s  (0.793s, 1291.23/s)  LR: 7.375e-05  Data: 0.009 (0.012)
Train: 502 [ 800/1251 ( 64%)]  Loss: 3.246 (3.02)  Time: 0.890s, 1150.27/s  (0.794s, 1289.87/s)  LR: 7.375e-05  Data: 0.010 (0.012)
Train: 502 [ 850/1251 ( 68%)]  Loss: 3.147 (3.02)  Time: 0.775s, 1321.32/s  (0.793s, 1290.91/s)  LR: 7.375e-05  Data: 0.009 (0.012)
Train: 502 [ 900/1251 ( 72%)]  Loss: 2.895 (3.02)  Time: 0.815s, 1256.13/s  (0.794s, 1290.44/s)  LR: 7.375e-05  Data: 0.011 (0.012)
Train: 502 [ 950/1251 ( 76%)]  Loss: 3.196 (3.03)  Time: 0.810s, 1263.91/s  (0.794s, 1288.90/s)  LR: 7.375e-05  Data: 0.009 (0.012)
Train: 502 [1000/1251 ( 80%)]  Loss: 3.034 (3.03)  Time: 0.775s, 1320.45/s  (0.794s, 1289.64/s)  LR: 7.375e-05  Data: 0.009 (0.012)
Train: 502 [1050/1251 ( 84%)]  Loss: 3.156 (3.03)  Time: 0.801s, 1278.25/s  (0.794s, 1289.43/s)  LR: 7.375e-05  Data: 0.010 (0.012)
Train: 502 [1100/1251 ( 88%)]  Loss: 3.156 (3.04)  Time: 0.825s, 1241.32/s  (0.794s, 1289.85/s)  LR: 7.375e-05  Data: 0.010 (0.012)
Train: 502 [1150/1251 ( 92%)]  Loss: 2.757 (3.03)  Time: 0.797s, 1285.51/s  (0.794s, 1289.83/s)  LR: 7.375e-05  Data: 0.016 (0.011)
Train: 502 [1200/1251 ( 96%)]  Loss: 2.998 (3.02)  Time: 0.772s, 1325.81/s  (0.793s, 1290.62/s)  LR: 7.375e-05  Data: 0.010 (0.011)
Train: 502 [1250/1251 (100%)]  Loss: 3.042 (3.03)  Time: 0.810s, 1264.37/s  (0.794s, 1290.41/s)  LR: 7.375e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.627 (1.627)  Loss:  0.6221 (0.6221)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.194 (0.582)  Loss:  0.7432 (1.0721)  Acc@1: 86.5566 (80.2420)  Acc@5: 97.7594 (95.2200)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-502.pth.tar', 80.24199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-498.pth.tar', 80.10800002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-494.pth.tar', 80.00199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-501.pth.tar', 79.94599995361328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-500.pth.tar', 79.89600005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-492.pth.tar', 79.86000010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-496.pth.tar', 79.84800002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-491.pth.tar', 79.84000002685546)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-490.pth.tar', 79.83200005371094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-499.pth.tar', 79.8320000024414)

Train: 503 [   0/1251 (  0%)]  Loss: 2.708 (2.71)  Time: 2.321s,  441.25/s  (2.321s,  441.25/s)  LR: 7.248e-05  Data: 1.590 (1.590)
Train: 503 [  50/1251 (  4%)]  Loss: 3.242 (2.98)  Time: 0.774s, 1323.63/s  (0.818s, 1251.09/s)  LR: 7.248e-05  Data: 0.010 (0.047)
Train: 503 [ 100/1251 (  8%)]  Loss: 3.031 (2.99)  Time: 0.807s, 1269.56/s  (0.804s, 1272.86/s)  LR: 7.248e-05  Data: 0.011 (0.029)
Train: 503 [ 150/1251 ( 12%)]  Loss: 3.302 (3.07)  Time: 0.774s, 1323.35/s  (0.801s, 1277.98/s)  LR: 7.248e-05  Data: 0.010 (0.023)
Train: 503 [ 200/1251 ( 16%)]  Loss: 2.956 (3.05)  Time: 0.775s, 1322.09/s  (0.800s, 1280.60/s)  LR: 7.248e-05  Data: 0.010 (0.019)
Train: 503 [ 250/1251 ( 20%)]  Loss: 3.283 (3.09)  Time: 0.773s, 1325.00/s  (0.798s, 1282.79/s)  LR: 7.248e-05  Data: 0.010 (0.017)
Train: 503 [ 300/1251 ( 24%)]  Loss: 3.285 (3.12)  Time: 0.811s, 1263.33/s  (0.796s, 1286.33/s)  LR: 7.248e-05  Data: 0.010 (0.016)
Train: 503 [ 350/1251 ( 28%)]  Loss: 3.094 (3.11)  Time: 0.775s, 1321.29/s  (0.794s, 1288.99/s)  LR: 7.248e-05  Data: 0.010 (0.015)
Train: 503 [ 400/1251 ( 32%)]  Loss: 3.032 (3.10)  Time: 0.773s, 1324.96/s  (0.795s, 1288.04/s)  LR: 7.248e-05  Data: 0.010 (0.015)
Train: 503 [ 450/1251 ( 36%)]  Loss: 3.213 (3.11)  Time: 0.856s, 1195.84/s  (0.793s, 1290.55/s)  LR: 7.248e-05  Data: 0.010 (0.014)
Train: 503 [ 500/1251 ( 40%)]  Loss: 3.082 (3.11)  Time: 0.785s, 1303.80/s  (0.795s, 1287.98/s)  LR: 7.248e-05  Data: 0.009 (0.014)
Train: 503 [ 550/1251 ( 44%)]  Loss: 2.986 (3.10)  Time: 0.773s, 1324.22/s  (0.793s, 1290.54/s)  LR: 7.248e-05  Data: 0.010 (0.013)
Train: 503 [ 600/1251 ( 48%)]  Loss: 3.210 (3.11)  Time: 0.772s, 1326.29/s  (0.792s, 1292.54/s)  LR: 7.248e-05  Data: 0.009 (0.013)
Train: 503 [ 650/1251 ( 52%)]  Loss: 2.981 (3.10)  Time: 0.774s, 1323.22/s  (0.792s, 1293.53/s)  LR: 7.248e-05  Data: 0.010 (0.013)
Train: 503 [ 700/1251 ( 56%)]  Loss: 2.826 (3.08)  Time: 0.808s, 1267.49/s  (0.792s, 1292.77/s)  LR: 7.248e-05  Data: 0.009 (0.013)
Train: 503 [ 750/1251 ( 60%)]  Loss: 3.092 (3.08)  Time: 0.796s, 1286.63/s  (0.792s, 1292.30/s)  LR: 7.248e-05  Data: 0.010 (0.012)
Train: 503 [ 800/1251 ( 64%)]  Loss: 3.268 (3.09)  Time: 0.790s, 1296.27/s  (0.792s, 1292.46/s)  LR: 7.248e-05  Data: 0.010 (0.012)
Train: 503 [ 850/1251 ( 68%)]  Loss: 2.760 (3.07)  Time: 0.799s, 1281.30/s  (0.793s, 1290.90/s)  LR: 7.248e-05  Data: 0.009 (0.012)
Train: 503 [ 900/1251 ( 72%)]  Loss: 2.990 (3.07)  Time: 0.814s, 1257.43/s  (0.793s, 1290.81/s)  LR: 7.248e-05  Data: 0.011 (0.012)
Train: 503 [ 950/1251 ( 76%)]  Loss: 2.907 (3.06)  Time: 0.773s, 1325.30/s  (0.793s, 1291.51/s)  LR: 7.248e-05  Data: 0.009 (0.012)
Train: 503 [1000/1251 ( 80%)]  Loss: 3.131 (3.07)  Time: 0.780s, 1312.96/s  (0.792s, 1292.15/s)  LR: 7.248e-05  Data: 0.012 (0.012)
Train: 503 [1050/1251 ( 84%)]  Loss: 3.216 (3.07)  Time: 0.809s, 1266.45/s  (0.793s, 1290.91/s)  LR: 7.248e-05  Data: 0.009 (0.012)
Train: 503 [1100/1251 ( 88%)]  Loss: 2.959 (3.07)  Time: 0.773s, 1324.69/s  (0.793s, 1290.56/s)  LR: 7.248e-05  Data: 0.009 (0.012)
Train: 503 [1150/1251 ( 92%)]  Loss: 3.095 (3.07)  Time: 0.773s, 1323.93/s  (0.794s, 1289.76/s)  LR: 7.248e-05  Data: 0.009 (0.012)
Train: 503 [1200/1251 ( 96%)]  Loss: 2.807 (3.06)  Time: 0.780s, 1312.21/s  (0.793s, 1290.62/s)  LR: 7.248e-05  Data: 0.010 (0.011)
Train: 503 [1250/1251 (100%)]  Loss: 3.358 (3.07)  Time: 0.761s, 1345.77/s  (0.793s, 1291.39/s)  LR: 7.248e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.576 (1.576)  Loss:  0.6860 (0.6860)  Acc@1: 92.2852 (92.2852)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.193 (0.574)  Loss:  0.7852 (1.1325)  Acc@1: 86.9104 (80.0140)  Acc@5: 97.7594 (95.0960)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-502.pth.tar', 80.24199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-498.pth.tar', 80.10800002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-503.pth.tar', 80.0140000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-494.pth.tar', 80.00199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-501.pth.tar', 79.94599995361328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-500.pth.tar', 79.89600005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-492.pth.tar', 79.86000010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-496.pth.tar', 79.84800002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-491.pth.tar', 79.84000002685546)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-490.pth.tar', 79.83200005371094)

Train: 504 [   0/1251 (  0%)]  Loss: 2.956 (2.96)  Time: 2.386s,  429.14/s  (2.386s,  429.14/s)  LR: 7.123e-05  Data: 1.648 (1.648)
Train: 504 [  50/1251 (  4%)]  Loss: 3.003 (2.98)  Time: 0.773s, 1325.07/s  (0.827s, 1237.99/s)  LR: 7.123e-05  Data: 0.010 (0.047)
Train: 504 [ 100/1251 (  8%)]  Loss: 3.014 (2.99)  Time: 0.771s, 1327.91/s  (0.812s, 1261.36/s)  LR: 7.123e-05  Data: 0.010 (0.029)
Train: 504 [ 150/1251 ( 12%)]  Loss: 2.974 (2.99)  Time: 0.809s, 1265.04/s  (0.807s, 1269.33/s)  LR: 7.123e-05  Data: 0.009 (0.022)
Train: 504 [ 200/1251 ( 16%)]  Loss: 2.942 (2.98)  Time: 0.774s, 1322.68/s  (0.805s, 1272.22/s)  LR: 7.123e-05  Data: 0.010 (0.019)
Train: 504 [ 250/1251 ( 20%)]  Loss: 2.855 (2.96)  Time: 0.775s, 1321.20/s  (0.801s, 1278.24/s)  LR: 7.123e-05  Data: 0.010 (0.017)
Train: 504 [ 300/1251 ( 24%)]  Loss: 3.378 (3.02)  Time: 0.783s, 1307.12/s  (0.799s, 1281.41/s)  LR: 7.123e-05  Data: 0.010 (0.016)
Train: 504 [ 350/1251 ( 28%)]  Loss: 2.961 (3.01)  Time: 0.815s, 1255.94/s  (0.798s, 1282.78/s)  LR: 7.123e-05  Data: 0.010 (0.015)
Train: 504 [ 400/1251 ( 32%)]  Loss: 3.121 (3.02)  Time: 0.778s, 1315.58/s  (0.797s, 1284.52/s)  LR: 7.123e-05  Data: 0.010 (0.015)
Train: 504 [ 450/1251 ( 36%)]  Loss: 2.703 (2.99)  Time: 0.833s, 1229.68/s  (0.797s, 1284.38/s)  LR: 7.123e-05  Data: 0.009 (0.014)
Train: 504 [ 500/1251 ( 40%)]  Loss: 3.094 (3.00)  Time: 0.811s, 1262.38/s  (0.796s, 1285.63/s)  LR: 7.123e-05  Data: 0.013 (0.014)
Train: 504 [ 550/1251 ( 44%)]  Loss: 2.828 (2.99)  Time: 0.773s, 1324.81/s  (0.796s, 1286.80/s)  LR: 7.123e-05  Data: 0.009 (0.013)
Train: 504 [ 600/1251 ( 48%)]  Loss: 3.199 (3.00)  Time: 0.808s, 1268.09/s  (0.796s, 1287.11/s)  LR: 7.123e-05  Data: 0.010 (0.013)
Train: 504 [ 650/1251 ( 52%)]  Loss: 2.936 (3.00)  Time: 0.775s, 1321.42/s  (0.796s, 1287.13/s)  LR: 7.123e-05  Data: 0.009 (0.013)
Train: 504 [ 700/1251 ( 56%)]  Loss: 2.641 (2.97)  Time: 0.808s, 1268.02/s  (0.795s, 1288.19/s)  LR: 7.123e-05  Data: 0.009 (0.012)
Train: 504 [ 750/1251 ( 60%)]  Loss: 2.834 (2.96)  Time: 0.780s, 1313.09/s  (0.795s, 1287.85/s)  LR: 7.123e-05  Data: 0.010 (0.012)
Train: 504 [ 800/1251 ( 64%)]  Loss: 2.828 (2.96)  Time: 0.777s, 1318.28/s  (0.795s, 1288.56/s)  LR: 7.123e-05  Data: 0.009 (0.012)
Train: 504 [ 850/1251 ( 68%)]  Loss: 2.855 (2.95)  Time: 0.773s, 1325.26/s  (0.794s, 1289.33/s)  LR: 7.123e-05  Data: 0.010 (0.012)
Train: 504 [ 900/1251 ( 72%)]  Loss: 2.924 (2.95)  Time: 0.785s, 1304.77/s  (0.794s, 1289.43/s)  LR: 7.123e-05  Data: 0.010 (0.012)
Train: 504 [ 950/1251 ( 76%)]  Loss: 2.830 (2.94)  Time: 0.807s, 1269.40/s  (0.794s, 1289.00/s)  LR: 7.123e-05  Data: 0.009 (0.012)
Train: 504 [1000/1251 ( 80%)]  Loss: 2.964 (2.94)  Time: 0.815s, 1257.18/s  (0.795s, 1288.75/s)  LR: 7.123e-05  Data: 0.011 (0.012)
Train: 504 [1050/1251 ( 84%)]  Loss: 3.234 (2.96)  Time: 0.772s, 1326.73/s  (0.794s, 1289.88/s)  LR: 7.123e-05  Data: 0.010 (0.012)
Train: 504 [1100/1251 ( 88%)]  Loss: 2.806 (2.95)  Time: 0.806s, 1271.19/s  (0.793s, 1290.65/s)  LR: 7.123e-05  Data: 0.011 (0.012)
Train: 504 [1150/1251 ( 92%)]  Loss: 2.763 (2.94)  Time: 0.775s, 1321.86/s  (0.793s, 1290.88/s)  LR: 7.123e-05  Data: 0.010 (0.011)
Train: 504 [1200/1251 ( 96%)]  Loss: 2.989 (2.95)  Time: 0.772s, 1326.43/s  (0.793s, 1291.71/s)  LR: 7.123e-05  Data: 0.009 (0.011)
Train: 504 [1250/1251 (100%)]  Loss: 2.988 (2.95)  Time: 0.761s, 1344.94/s  (0.793s, 1291.68/s)  LR: 7.123e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.694 (1.694)  Loss:  0.6416 (0.6416)  Acc@1: 91.7969 (91.7969)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.194 (0.572)  Loss:  0.7402 (1.0831)  Acc@1: 86.6745 (80.0840)  Acc@5: 97.9953 (95.2320)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-502.pth.tar', 80.24199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-498.pth.tar', 80.10800002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-504.pth.tar', 80.08400002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-503.pth.tar', 80.0140000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-494.pth.tar', 80.00199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-501.pth.tar', 79.94599995361328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-500.pth.tar', 79.89600005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-492.pth.tar', 79.86000010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-496.pth.tar', 79.84800002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-491.pth.tar', 79.84000002685546)

Train: 505 [   0/1251 (  0%)]  Loss: 3.267 (3.27)  Time: 2.277s,  449.64/s  (2.277s,  449.64/s)  LR: 6.999e-05  Data: 1.542 (1.542)
Train: 505 [  50/1251 (  4%)]  Loss: 3.250 (3.26)  Time: 0.815s, 1256.24/s  (0.843s, 1215.19/s)  LR: 6.999e-05  Data: 0.011 (0.050)
Train: 505 [ 100/1251 (  8%)]  Loss: 3.185 (3.23)  Time: 0.774s, 1323.77/s  (0.816s, 1255.19/s)  LR: 6.999e-05  Data: 0.009 (0.030)
Train: 505 [ 150/1251 ( 12%)]  Loss: 3.018 (3.18)  Time: 0.808s, 1266.71/s  (0.810s, 1264.34/s)  LR: 6.999e-05  Data: 0.010 (0.023)
Train: 505 [ 200/1251 ( 16%)]  Loss: 3.378 (3.22)  Time: 0.814s, 1257.58/s  (0.809s, 1265.00/s)  LR: 6.999e-05  Data: 0.009 (0.020)
Train: 505 [ 250/1251 ( 20%)]  Loss: 2.935 (3.17)  Time: 0.791s, 1294.39/s  (0.807s, 1269.21/s)  LR: 6.999e-05  Data: 0.010 (0.018)
Train: 505 [ 300/1251 ( 24%)]  Loss: 2.887 (3.13)  Time: 0.773s, 1324.05/s  (0.806s, 1271.17/s)  LR: 6.999e-05  Data: 0.010 (0.017)
Train: 505 [ 350/1251 ( 28%)]  Loss: 2.799 (3.09)  Time: 0.781s, 1311.09/s  (0.803s, 1275.26/s)  LR: 6.999e-05  Data: 0.014 (0.016)
Train: 505 [ 400/1251 ( 32%)]  Loss: 3.093 (3.09)  Time: 0.784s, 1305.33/s  (0.800s, 1280.27/s)  LR: 6.999e-05  Data: 0.009 (0.015)
Train: 505 [ 450/1251 ( 36%)]  Loss: 2.985 (3.08)  Time: 0.792s, 1293.01/s  (0.798s, 1282.72/s)  LR: 6.999e-05  Data: 0.009 (0.014)
Train: 505 [ 500/1251 ( 40%)]  Loss: 2.937 (3.07)  Time: 0.807s, 1268.88/s  (0.797s, 1284.68/s)  LR: 6.999e-05  Data: 0.009 (0.014)
Train: 505 [ 550/1251 ( 44%)]  Loss: 2.857 (3.05)  Time: 0.774s, 1323.24/s  (0.796s, 1286.43/s)  LR: 6.999e-05  Data: 0.010 (0.014)
Train: 505 [ 600/1251 ( 48%)]  Loss: 3.294 (3.07)  Time: 0.784s, 1306.48/s  (0.796s, 1286.94/s)  LR: 6.999e-05  Data: 0.010 (0.013)
Train: 505 [ 650/1251 ( 52%)]  Loss: 3.002 (3.06)  Time: 0.805s, 1271.95/s  (0.796s, 1287.00/s)  LR: 6.999e-05  Data: 0.009 (0.013)
Train: 505 [ 700/1251 ( 56%)]  Loss: 3.250 (3.08)  Time: 0.806s, 1271.06/s  (0.797s, 1285.32/s)  LR: 6.999e-05  Data: 0.009 (0.013)
Train: 505 [ 750/1251 ( 60%)]  Loss: 3.112 (3.08)  Time: 0.809s, 1266.21/s  (0.797s, 1284.34/s)  LR: 6.999e-05  Data: 0.010 (0.013)
Train: 505 [ 800/1251 ( 64%)]  Loss: 2.715 (3.06)  Time: 0.774s, 1322.87/s  (0.797s, 1285.62/s)  LR: 6.999e-05  Data: 0.010 (0.012)
Train: 505 [ 850/1251 ( 68%)]  Loss: 3.267 (3.07)  Time: 0.776s, 1320.29/s  (0.796s, 1286.68/s)  LR: 6.999e-05  Data: 0.010 (0.012)
Train: 505 [ 900/1251 ( 72%)]  Loss: 3.044 (3.07)  Time: 0.786s, 1302.01/s  (0.795s, 1287.51/s)  LR: 6.999e-05  Data: 0.010 (0.012)
Train: 505 [ 950/1251 ( 76%)]  Loss: 3.041 (3.07)  Time: 0.773s, 1324.57/s  (0.795s, 1288.12/s)  LR: 6.999e-05  Data: 0.010 (0.012)
Train: 505 [1000/1251 ( 80%)]  Loss: 3.101 (3.07)  Time: 0.775s, 1321.07/s  (0.795s, 1288.82/s)  LR: 6.999e-05  Data: 0.010 (0.012)
Train: 505 [1050/1251 ( 84%)]  Loss: 2.968 (3.06)  Time: 0.772s, 1326.12/s  (0.794s, 1289.76/s)  LR: 6.999e-05  Data: 0.009 (0.012)
Train: 505 [1100/1251 ( 88%)]  Loss: 3.192 (3.07)  Time: 0.817s, 1253.96/s  (0.794s, 1289.38/s)  LR: 6.999e-05  Data: 0.011 (0.012)
Train: 505 [1150/1251 ( 92%)]  Loss: 3.001 (3.07)  Time: 0.775s, 1322.05/s  (0.794s, 1289.75/s)  LR: 6.999e-05  Data: 0.010 (0.012)
Train: 505 [1200/1251 ( 96%)]  Loss: 3.141 (3.07)  Time: 0.772s, 1325.64/s  (0.793s, 1290.80/s)  LR: 6.999e-05  Data: 0.010 (0.012)
Train: 505 [1250/1251 (100%)]  Loss: 3.185 (3.07)  Time: 0.801s, 1278.79/s  (0.794s, 1290.14/s)  LR: 6.999e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.577 (1.577)  Loss:  0.6460 (0.6460)  Acc@1: 92.2852 (92.2852)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.193 (0.561)  Loss:  0.7007 (1.0962)  Acc@1: 87.5000 (80.0540)  Acc@5: 98.2311 (95.0900)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-502.pth.tar', 80.24199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-498.pth.tar', 80.10800002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-504.pth.tar', 80.08400002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-505.pth.tar', 80.054)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-503.pth.tar', 80.0140000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-494.pth.tar', 80.00199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-501.pth.tar', 79.94599995361328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-500.pth.tar', 79.89600005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-492.pth.tar', 79.86000010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-496.pth.tar', 79.84800002685547)

Train: 506 [   0/1251 (  0%)]  Loss: 3.167 (3.17)  Time: 2.272s,  450.63/s  (2.272s,  450.63/s)  LR: 6.875e-05  Data: 1.553 (1.553)
Train: 506 [  50/1251 (  4%)]  Loss: 2.630 (2.90)  Time: 0.772s, 1325.88/s  (0.823s, 1244.76/s)  LR: 6.875e-05  Data: 0.010 (0.045)
Train: 506 [ 100/1251 (  8%)]  Loss: 2.972 (2.92)  Time: 0.818s, 1251.57/s  (0.820s, 1248.64/s)  LR: 6.875e-05  Data: 0.011 (0.028)
Train: 506 [ 150/1251 ( 12%)]  Loss: 3.033 (2.95)  Time: 0.777s, 1317.05/s  (0.816s, 1255.37/s)  LR: 6.875e-05  Data: 0.009 (0.022)
Train: 506 [ 200/1251 ( 16%)]  Loss: 2.956 (2.95)  Time: 0.773s, 1325.22/s  (0.807s, 1268.58/s)  LR: 6.875e-05  Data: 0.010 (0.019)
Train: 506 [ 250/1251 ( 20%)]  Loss: 3.267 (3.00)  Time: 0.831s, 1232.53/s  (0.802s, 1276.26/s)  LR: 6.875e-05  Data: 0.009 (0.017)
Train: 506 [ 300/1251 ( 24%)]  Loss: 3.061 (3.01)  Time: 0.779s, 1314.94/s  (0.803s, 1275.73/s)  LR: 6.875e-05  Data: 0.009 (0.016)
Train: 506 [ 350/1251 ( 28%)]  Loss: 2.851 (2.99)  Time: 0.807s, 1268.99/s  (0.802s, 1276.92/s)  LR: 6.875e-05  Data: 0.009 (0.015)
Train: 506 [ 400/1251 ( 32%)]  Loss: 3.039 (3.00)  Time: 0.773s, 1325.30/s  (0.801s, 1279.17/s)  LR: 6.875e-05  Data: 0.010 (0.015)
Train: 506 [ 450/1251 ( 36%)]  Loss: 2.945 (2.99)  Time: 0.807s, 1268.90/s  (0.800s, 1279.64/s)  LR: 6.875e-05  Data: 0.009 (0.014)
Train: 506 [ 500/1251 ( 40%)]  Loss: 3.423 (3.03)  Time: 0.807s, 1268.36/s  (0.800s, 1280.04/s)  LR: 6.875e-05  Data: 0.009 (0.014)
Train: 506 [ 550/1251 ( 44%)]  Loss: 3.154 (3.04)  Time: 0.772s, 1326.25/s  (0.799s, 1281.15/s)  LR: 6.875e-05  Data: 0.010 (0.013)
Train: 506 [ 600/1251 ( 48%)]  Loss: 2.937 (3.03)  Time: 0.772s, 1325.60/s  (0.799s, 1281.20/s)  LR: 6.875e-05  Data: 0.009 (0.013)
Train: 506 [ 650/1251 ( 52%)]  Loss: 2.823 (3.02)  Time: 0.828s, 1236.61/s  (0.799s, 1281.28/s)  LR: 6.875e-05  Data: 0.014 (0.013)
Train: 506 [ 700/1251 ( 56%)]  Loss: 3.015 (3.02)  Time: 0.809s, 1265.01/s  (0.799s, 1281.55/s)  LR: 6.875e-05  Data: 0.010 (0.013)
Train: 506 [ 750/1251 ( 60%)]  Loss: 3.125 (3.02)  Time: 0.806s, 1269.82/s  (0.799s, 1282.00/s)  LR: 6.875e-05  Data: 0.010 (0.012)
Train: 506 [ 800/1251 ( 64%)]  Loss: 3.238 (3.04)  Time: 0.772s, 1326.54/s  (0.798s, 1283.04/s)  LR: 6.875e-05  Data: 0.010 (0.012)
Train: 506 [ 850/1251 ( 68%)]  Loss: 2.670 (3.02)  Time: 0.773s, 1324.76/s  (0.797s, 1284.42/s)  LR: 6.875e-05  Data: 0.010 (0.012)
Train: 506 [ 900/1251 ( 72%)]  Loss: 3.005 (3.02)  Time: 0.773s, 1325.07/s  (0.797s, 1284.90/s)  LR: 6.875e-05  Data: 0.010 (0.012)
Train: 506 [ 950/1251 ( 76%)]  Loss: 3.272 (3.03)  Time: 0.808s, 1267.30/s  (0.797s, 1284.57/s)  LR: 6.875e-05  Data: 0.009 (0.012)
Train: 506 [1000/1251 ( 80%)]  Loss: 2.928 (3.02)  Time: 0.774s, 1323.30/s  (0.798s, 1283.44/s)  LR: 6.875e-05  Data: 0.010 (0.012)
Train: 506 [1050/1251 ( 84%)]  Loss: 3.070 (3.03)  Time: 0.807s, 1269.11/s  (0.798s, 1283.79/s)  LR: 6.875e-05  Data: 0.009 (0.012)
Train: 506 [1100/1251 ( 88%)]  Loss: 3.074 (3.03)  Time: 0.773s, 1324.83/s  (0.797s, 1284.84/s)  LR: 6.875e-05  Data: 0.010 (0.012)
Train: 506 [1150/1251 ( 92%)]  Loss: 3.022 (3.03)  Time: 0.808s, 1268.10/s  (0.797s, 1285.34/s)  LR: 6.875e-05  Data: 0.009 (0.012)
Train: 506 [1200/1251 ( 96%)]  Loss: 2.743 (3.02)  Time: 0.845s, 1211.43/s  (0.797s, 1285.40/s)  LR: 6.875e-05  Data: 0.010 (0.011)
Train: 506 [1250/1251 (100%)]  Loss: 3.149 (3.02)  Time: 0.759s, 1349.07/s  (0.796s, 1286.13/s)  LR: 6.875e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.559 (1.559)  Loss:  0.5806 (0.5806)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.194 (0.569)  Loss:  0.6758 (1.0421)  Acc@1: 86.9104 (80.1520)  Acc@5: 98.4670 (95.1240)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-502.pth.tar', 80.24199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-506.pth.tar', 80.15199987304688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-498.pth.tar', 80.10800002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-504.pth.tar', 80.08400002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-505.pth.tar', 80.054)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-503.pth.tar', 80.0140000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-494.pth.tar', 80.00199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-501.pth.tar', 79.94599995361328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-500.pth.tar', 79.89600005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-492.pth.tar', 79.86000010498047)

Train: 507 [   0/1251 (  0%)]  Loss: 3.135 (3.13)  Time: 2.206s,  464.23/s  (2.206s,  464.23/s)  LR: 6.754e-05  Data: 1.479 (1.479)
Train: 507 [  50/1251 (  4%)]  Loss: 2.720 (2.93)  Time: 0.785s, 1304.29/s  (0.818s, 1251.55/s)  LR: 6.754e-05  Data: 0.009 (0.045)
Train: 507 [ 100/1251 (  8%)]  Loss: 3.008 (2.95)  Time: 0.704s, 1454.68/s  (0.800s, 1280.02/s)  LR: 6.754e-05  Data: 0.009 (0.027)
Train: 507 [ 150/1251 ( 12%)]  Loss: 3.225 (3.02)  Time: 0.896s, 1142.78/s  (0.799s, 1281.93/s)  LR: 6.754e-05  Data: 0.010 (0.021)
Train: 507 [ 200/1251 ( 16%)]  Loss: 3.076 (3.03)  Time: 0.812s, 1261.06/s  (0.795s, 1288.07/s)  LR: 6.754e-05  Data: 0.009 (0.019)
Train: 507 [ 250/1251 ( 20%)]  Loss: 3.362 (3.09)  Time: 0.773s, 1324.79/s  (0.796s, 1287.24/s)  LR: 6.754e-05  Data: 0.010 (0.017)
Train: 507 [ 300/1251 ( 24%)]  Loss: 3.178 (3.10)  Time: 0.773s, 1324.67/s  (0.793s, 1290.49/s)  LR: 6.754e-05  Data: 0.010 (0.016)
Train: 507 [ 350/1251 ( 28%)]  Loss: 2.956 (3.08)  Time: 0.780s, 1312.51/s  (0.791s, 1294.06/s)  LR: 6.754e-05  Data: 0.009 (0.015)
Train: 507 [ 400/1251 ( 32%)]  Loss: 3.139 (3.09)  Time: 0.800s, 1279.98/s  (0.791s, 1294.17/s)  LR: 6.754e-05  Data: 0.009 (0.014)
Train: 507 [ 450/1251 ( 36%)]  Loss: 2.741 (3.05)  Time: 0.769s, 1330.85/s  (0.791s, 1294.63/s)  LR: 6.754e-05  Data: 0.010 (0.014)
Train: 507 [ 500/1251 ( 40%)]  Loss: 3.111 (3.06)  Time: 0.774s, 1322.27/s  (0.791s, 1294.31/s)  LR: 6.754e-05  Data: 0.010 (0.013)
Train: 507 [ 550/1251 ( 44%)]  Loss: 3.001 (3.05)  Time: 0.820s, 1249.33/s  (0.792s, 1293.60/s)  LR: 6.754e-05  Data: 0.009 (0.013)
Train: 507 [ 600/1251 ( 48%)]  Loss: 3.177 (3.06)  Time: 0.823s, 1243.87/s  (0.791s, 1294.76/s)  LR: 6.754e-05  Data: 0.010 (0.013)
Train: 507 [ 650/1251 ( 52%)]  Loss: 2.772 (3.04)  Time: 0.772s, 1326.20/s  (0.790s, 1295.44/s)  LR: 6.754e-05  Data: 0.010 (0.013)
Train: 507 [ 700/1251 ( 56%)]  Loss: 3.101 (3.05)  Time: 0.772s, 1325.94/s  (0.791s, 1294.54/s)  LR: 6.754e-05  Data: 0.010 (0.012)
Train: 507 [ 750/1251 ( 60%)]  Loss: 3.077 (3.05)  Time: 0.785s, 1303.92/s  (0.791s, 1294.66/s)  LR: 6.754e-05  Data: 0.010 (0.012)
Train: 507 [ 800/1251 ( 64%)]  Loss: 2.807 (3.03)  Time: 0.775s, 1322.14/s  (0.791s, 1294.41/s)  LR: 6.754e-05  Data: 0.010 (0.012)
Train: 507 [ 850/1251 ( 68%)]  Loss: 3.224 (3.05)  Time: 0.815s, 1257.02/s  (0.792s, 1292.74/s)  LR: 6.754e-05  Data: 0.011 (0.012)
Train: 507 [ 900/1251 ( 72%)]  Loss: 2.894 (3.04)  Time: 0.773s, 1325.11/s  (0.792s, 1292.19/s)  LR: 6.754e-05  Data: 0.010 (0.012)
Train: 507 [ 950/1251 ( 76%)]  Loss: 3.239 (3.05)  Time: 0.779s, 1314.03/s  (0.793s, 1291.99/s)  LR: 6.754e-05  Data: 0.009 (0.012)
Train: 507 [1000/1251 ( 80%)]  Loss: 3.304 (3.06)  Time: 0.809s, 1266.33/s  (0.792s, 1292.84/s)  LR: 6.754e-05  Data: 0.009 (0.012)
Train: 507 [1050/1251 ( 84%)]  Loss: 3.094 (3.06)  Time: 0.772s, 1325.99/s  (0.792s, 1293.20/s)  LR: 6.754e-05  Data: 0.010 (0.012)
Train: 507 [1100/1251 ( 88%)]  Loss: 3.115 (3.06)  Time: 0.773s, 1325.30/s  (0.792s, 1292.79/s)  LR: 6.754e-05  Data: 0.009 (0.011)
Train: 507 [1150/1251 ( 92%)]  Loss: 3.284 (3.07)  Time: 0.772s, 1327.10/s  (0.792s, 1293.49/s)  LR: 6.754e-05  Data: 0.009 (0.011)
Train: 507 [1200/1251 ( 96%)]  Loss: 2.783 (3.06)  Time: 0.777s, 1317.70/s  (0.792s, 1292.75/s)  LR: 6.754e-05  Data: 0.010 (0.011)
Train: 507 [1250/1251 (100%)]  Loss: 2.921 (3.06)  Time: 0.772s, 1327.22/s  (0.792s, 1292.75/s)  LR: 6.754e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.551 (1.551)  Loss:  0.5869 (0.5869)  Acc@1: 91.7969 (91.7969)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.194 (0.564)  Loss:  0.6992 (1.0370)  Acc@1: 86.7925 (80.2440)  Acc@5: 98.1132 (95.1380)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-507.pth.tar', 80.24400008056641)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-502.pth.tar', 80.24199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-506.pth.tar', 80.15199987304688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-498.pth.tar', 80.10800002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-504.pth.tar', 80.08400002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-505.pth.tar', 80.054)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-503.pth.tar', 80.0140000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-494.pth.tar', 80.00199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-501.pth.tar', 79.94599995361328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-500.pth.tar', 79.89600005615235)

Train: 508 [   0/1251 (  0%)]  Loss: 2.588 (2.59)  Time: 2.210s,  463.37/s  (2.210s,  463.37/s)  LR: 6.633e-05  Data: 1.485 (1.485)
Train: 508 [  50/1251 (  4%)]  Loss: 2.890 (2.74)  Time: 0.774s, 1323.73/s  (0.815s, 1256.54/s)  LR: 6.633e-05  Data: 0.010 (0.043)
Train: 508 [ 100/1251 (  8%)]  Loss: 3.280 (2.92)  Time: 0.774s, 1323.16/s  (0.804s, 1273.85/s)  LR: 6.633e-05  Data: 0.010 (0.026)
Train: 508 [ 150/1251 ( 12%)]  Loss: 3.020 (2.94)  Time: 0.825s, 1240.54/s  (0.798s, 1282.48/s)  LR: 6.633e-05  Data: 0.009 (0.021)
Train: 508 [ 200/1251 ( 16%)]  Loss: 3.227 (3.00)  Time: 0.816s, 1254.90/s  (0.799s, 1282.03/s)  LR: 6.633e-05  Data: 0.009 (0.018)
Train: 508 [ 250/1251 ( 20%)]  Loss: 3.064 (3.01)  Time: 0.782s, 1309.34/s  (0.799s, 1280.94/s)  LR: 6.633e-05  Data: 0.009 (0.016)
Train: 508 [ 300/1251 ( 24%)]  Loss: 3.474 (3.08)  Time: 0.772s, 1326.65/s  (0.797s, 1284.50/s)  LR: 6.633e-05  Data: 0.010 (0.015)
Train: 508 [ 350/1251 ( 28%)]  Loss: 2.993 (3.07)  Time: 0.775s, 1321.64/s  (0.796s, 1285.96/s)  LR: 6.633e-05  Data: 0.009 (0.014)
Train: 508 [ 400/1251 ( 32%)]  Loss: 2.864 (3.04)  Time: 0.771s, 1328.61/s  (0.794s, 1288.96/s)  LR: 6.633e-05  Data: 0.009 (0.014)
Train: 508 [ 450/1251 ( 36%)]  Loss: 3.103 (3.05)  Time: 0.851s, 1202.65/s  (0.793s, 1290.80/s)  LR: 6.633e-05  Data: 0.010 (0.013)
Train: 508 [ 500/1251 ( 40%)]  Loss: 2.673 (3.02)  Time: 0.863s, 1186.45/s  (0.796s, 1287.06/s)  LR: 6.633e-05  Data: 0.010 (0.013)
Train: 508 [ 550/1251 ( 44%)]  Loss: 3.160 (3.03)  Time: 0.783s, 1307.92/s  (0.797s, 1284.29/s)  LR: 6.633e-05  Data: 0.018 (0.013)
Train: 508 [ 600/1251 ( 48%)]  Loss: 3.064 (3.03)  Time: 0.785s, 1305.19/s  (0.797s, 1284.52/s)  LR: 6.633e-05  Data: 0.009 (0.013)
Train: 508 [ 650/1251 ( 52%)]  Loss: 2.905 (3.02)  Time: 0.785s, 1304.13/s  (0.796s, 1285.83/s)  LR: 6.633e-05  Data: 0.010 (0.013)
Train: 508 [ 700/1251 ( 56%)]  Loss: 3.127 (3.03)  Time: 0.775s, 1320.69/s  (0.796s, 1286.38/s)  LR: 6.633e-05  Data: 0.010 (0.012)
Train: 508 [ 750/1251 ( 60%)]  Loss: 2.986 (3.03)  Time: 0.773s, 1323.93/s  (0.795s, 1287.62/s)  LR: 6.633e-05  Data: 0.010 (0.012)
Train: 508 [ 800/1251 ( 64%)]  Loss: 3.097 (3.03)  Time: 0.820s, 1249.01/s  (0.795s, 1287.70/s)  LR: 6.633e-05  Data: 0.010 (0.012)
Train: 508 [ 850/1251 ( 68%)]  Loss: 2.935 (3.03)  Time: 0.807s, 1268.99/s  (0.796s, 1286.70/s)  LR: 6.633e-05  Data: 0.010 (0.012)
Train: 508 [ 900/1251 ( 72%)]  Loss: 3.220 (3.04)  Time: 0.867s, 1181.42/s  (0.796s, 1286.27/s)  LR: 6.633e-05  Data: 0.010 (0.012)
Train: 508 [ 950/1251 ( 76%)]  Loss: 3.113 (3.04)  Time: 0.810s, 1264.02/s  (0.796s, 1286.44/s)  LR: 6.633e-05  Data: 0.010 (0.012)
Train: 508 [1000/1251 ( 80%)]  Loss: 2.885 (3.03)  Time: 0.773s, 1324.50/s  (0.796s, 1286.81/s)  LR: 6.633e-05  Data: 0.010 (0.012)
Train: 508 [1050/1251 ( 84%)]  Loss: 2.839 (3.02)  Time: 0.773s, 1324.23/s  (0.795s, 1287.94/s)  LR: 6.633e-05  Data: 0.010 (0.012)
Train: 508 [1100/1251 ( 88%)]  Loss: 3.158 (3.03)  Time: 0.807s, 1269.57/s  (0.795s, 1288.42/s)  LR: 6.633e-05  Data: 0.010 (0.012)
Train: 508 [1150/1251 ( 92%)]  Loss: 2.907 (3.02)  Time: 0.773s, 1325.25/s  (0.794s, 1289.32/s)  LR: 6.633e-05  Data: 0.010 (0.011)
Train: 508 [1200/1251 ( 96%)]  Loss: 3.397 (3.04)  Time: 0.816s, 1254.76/s  (0.794s, 1289.90/s)  LR: 6.633e-05  Data: 0.010 (0.011)
Train: 508 [1250/1251 (100%)]  Loss: 2.977 (3.04)  Time: 0.797s, 1284.62/s  (0.794s, 1290.09/s)  LR: 6.633e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.613 (1.613)  Loss:  0.6138 (0.6138)  Acc@1: 91.6016 (91.6016)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.194 (0.567)  Loss:  0.7070 (1.0584)  Acc@1: 85.8491 (79.9720)  Acc@5: 98.1132 (95.1000)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-507.pth.tar', 80.24400008056641)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-502.pth.tar', 80.24199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-506.pth.tar', 80.15199987304688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-498.pth.tar', 80.10800002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-504.pth.tar', 80.08400002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-505.pth.tar', 80.054)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-503.pth.tar', 80.0140000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-494.pth.tar', 80.00199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-508.pth.tar', 79.97199992919921)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-501.pth.tar', 79.94599995361328)

Train: 509 [   0/1251 (  0%)]  Loss: 2.981 (2.98)  Time: 2.359s,  434.14/s  (2.359s,  434.14/s)  LR: 6.513e-05  Data: 1.632 (1.632)
Train: 509 [  50/1251 (  4%)]  Loss: 3.049 (3.02)  Time: 0.775s, 1321.40/s  (0.830s, 1233.08/s)  LR: 6.513e-05  Data: 0.010 (0.045)
Train: 509 [ 100/1251 (  8%)]  Loss: 3.142 (3.06)  Time: 0.774s, 1323.55/s  (0.811s, 1262.02/s)  LR: 6.513e-05  Data: 0.010 (0.027)
Train: 509 [ 150/1251 ( 12%)]  Loss: 2.822 (3.00)  Time: 0.809s, 1265.04/s  (0.805s, 1271.78/s)  LR: 6.513e-05  Data: 0.009 (0.022)
Train: 509 [ 200/1251 ( 16%)]  Loss: 2.840 (2.97)  Time: 0.819s, 1250.44/s  (0.804s, 1273.43/s)  LR: 6.513e-05  Data: 0.009 (0.019)
Train: 509 [ 250/1251 ( 20%)]  Loss: 2.885 (2.95)  Time: 0.774s, 1323.20/s  (0.801s, 1278.75/s)  LR: 6.513e-05  Data: 0.009 (0.017)
Train: 509 [ 300/1251 ( 24%)]  Loss: 3.043 (2.97)  Time: 0.802s, 1276.04/s  (0.800s, 1279.24/s)  LR: 6.513e-05  Data: 0.009 (0.016)
Train: 509 [ 350/1251 ( 28%)]  Loss: 2.892 (2.96)  Time: 0.826s, 1240.16/s  (0.801s, 1278.06/s)  LR: 6.513e-05  Data: 0.010 (0.015)
Train: 509 [ 400/1251 ( 32%)]  Loss: 3.015 (2.96)  Time: 0.778s, 1316.38/s  (0.801s, 1278.40/s)  LR: 6.513e-05  Data: 0.009 (0.014)
Train: 509 [ 450/1251 ( 36%)]  Loss: 2.963 (2.96)  Time: 0.772s, 1325.78/s  (0.801s, 1278.83/s)  LR: 6.513e-05  Data: 0.010 (0.014)
Train: 509 [ 500/1251 ( 40%)]  Loss: 3.161 (2.98)  Time: 0.771s, 1327.53/s  (0.799s, 1281.35/s)  LR: 6.513e-05  Data: 0.009 (0.013)
Train: 509 [ 550/1251 ( 44%)]  Loss: 2.895 (2.97)  Time: 0.805s, 1272.37/s  (0.799s, 1281.99/s)  LR: 6.513e-05  Data: 0.010 (0.013)
Train: 509 [ 600/1251 ( 48%)]  Loss: 3.145 (2.99)  Time: 0.790s, 1295.78/s  (0.799s, 1281.33/s)  LR: 6.513e-05  Data: 0.010 (0.013)
Train: 509 [ 650/1251 ( 52%)]  Loss: 3.039 (2.99)  Time: 0.772s, 1326.14/s  (0.798s, 1283.59/s)  LR: 6.513e-05  Data: 0.010 (0.013)
Train: 509 [ 700/1251 ( 56%)]  Loss: 2.934 (2.99)  Time: 0.778s, 1315.60/s  (0.798s, 1283.21/s)  LR: 6.513e-05  Data: 0.010 (0.012)
Train: 509 [ 750/1251 ( 60%)]  Loss: 2.845 (2.98)  Time: 0.772s, 1325.59/s  (0.798s, 1283.99/s)  LR: 6.513e-05  Data: 0.010 (0.012)
Train: 509 [ 800/1251 ( 64%)]  Loss: 3.211 (2.99)  Time: 0.772s, 1326.16/s  (0.797s, 1285.11/s)  LR: 6.513e-05  Data: 0.009 (0.012)
Train: 509 [ 850/1251 ( 68%)]  Loss: 3.141 (3.00)  Time: 0.773s, 1324.71/s  (0.796s, 1286.11/s)  LR: 6.513e-05  Data: 0.009 (0.012)
Train: 509 [ 900/1251 ( 72%)]  Loss: 3.130 (3.01)  Time: 0.772s, 1325.94/s  (0.796s, 1287.14/s)  LR: 6.513e-05  Data: 0.010 (0.012)
Train: 509 [ 950/1251 ( 76%)]  Loss: 3.206 (3.02)  Time: 0.815s, 1256.13/s  (0.796s, 1286.79/s)  LR: 6.513e-05  Data: 0.011 (0.012)
Train: 509 [1000/1251 ( 80%)]  Loss: 2.802 (3.01)  Time: 0.780s, 1312.96/s  (0.796s, 1286.57/s)  LR: 6.513e-05  Data: 0.010 (0.012)
Train: 509 [1050/1251 ( 84%)]  Loss: 2.875 (3.00)  Time: 0.867s, 1181.47/s  (0.796s, 1286.17/s)  LR: 6.513e-05  Data: 0.010 (0.012)
Train: 509 [1100/1251 ( 88%)]  Loss: 3.055 (3.00)  Time: 0.807s, 1269.12/s  (0.796s, 1286.43/s)  LR: 6.513e-05  Data: 0.010 (0.012)
Train: 509 [1150/1251 ( 92%)]  Loss: 3.125 (3.01)  Time: 0.776s, 1318.74/s  (0.796s, 1287.23/s)  LR: 6.513e-05  Data: 0.010 (0.011)
Train: 509 [1200/1251 ( 96%)]  Loss: 3.082 (3.01)  Time: 0.806s, 1270.78/s  (0.795s, 1287.47/s)  LR: 6.513e-05  Data: 0.009 (0.011)
Train: 509 [1250/1251 (100%)]  Loss: 3.083 (3.01)  Time: 0.786s, 1303.11/s  (0.795s, 1287.28/s)  LR: 6.513e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.606 (1.606)  Loss:  0.6533 (0.6533)  Acc@1: 91.7969 (91.7969)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.194 (0.568)  Loss:  0.7305 (1.0989)  Acc@1: 87.3821 (80.2000)  Acc@5: 98.3491 (95.2420)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-507.pth.tar', 80.24400008056641)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-502.pth.tar', 80.24199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-509.pth.tar', 80.19999994873046)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-506.pth.tar', 80.15199987304688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-498.pth.tar', 80.10800002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-504.pth.tar', 80.08400002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-505.pth.tar', 80.054)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-503.pth.tar', 80.0140000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-494.pth.tar', 80.00199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-508.pth.tar', 79.97199992919921)

Train: 510 [   0/1251 (  0%)]  Loss: 3.147 (3.15)  Time: 2.302s,  444.74/s  (2.302s,  444.74/s)  LR: 6.395e-05  Data: 1.572 (1.572)
Train: 510 [  50/1251 (  4%)]  Loss: 3.065 (3.11)  Time: 0.819s, 1250.06/s  (0.832s, 1231.43/s)  LR: 6.395e-05  Data: 0.009 (0.043)
Train: 510 [ 100/1251 (  8%)]  Loss: 2.961 (3.06)  Time: 0.786s, 1303.37/s  (0.810s, 1264.79/s)  LR: 6.395e-05  Data: 0.010 (0.027)
Train: 510 [ 150/1251 ( 12%)]  Loss: 2.966 (3.03)  Time: 0.772s, 1326.77/s  (0.801s, 1277.76/s)  LR: 6.395e-05  Data: 0.009 (0.021)
Train: 510 [ 200/1251 ( 16%)]  Loss: 3.257 (3.08)  Time: 0.815s, 1257.10/s  (0.802s, 1277.17/s)  LR: 6.395e-05  Data: 0.011 (0.018)
Train: 510 [ 250/1251 ( 20%)]  Loss: 3.009 (3.07)  Time: 0.770s, 1330.70/s  (0.801s, 1279.10/s)  LR: 6.395e-05  Data: 0.009 (0.017)
Train: 510 [ 300/1251 ( 24%)]  Loss: 3.096 (3.07)  Time: 0.819s, 1249.77/s  (0.800s, 1280.27/s)  LR: 6.395e-05  Data: 0.013 (0.016)
Train: 510 [ 350/1251 ( 28%)]  Loss: 3.382 (3.11)  Time: 0.811s, 1262.59/s  (0.798s, 1283.19/s)  LR: 6.395e-05  Data: 0.010 (0.015)
Train: 510 [ 400/1251 ( 32%)]  Loss: 3.004 (3.10)  Time: 0.772s, 1326.78/s  (0.797s, 1284.85/s)  LR: 6.395e-05  Data: 0.010 (0.014)
Train: 510 [ 450/1251 ( 36%)]  Loss: 3.288 (3.12)  Time: 0.773s, 1324.57/s  (0.798s, 1283.84/s)  LR: 6.395e-05  Data: 0.010 (0.014)
Train: 510 [ 500/1251 ( 40%)]  Loss: 2.847 (3.09)  Time: 0.815s, 1256.05/s  (0.797s, 1284.92/s)  LR: 6.395e-05  Data: 0.011 (0.013)
Train: 510 [ 550/1251 ( 44%)]  Loss: 3.091 (3.09)  Time: 0.808s, 1268.11/s  (0.797s, 1284.83/s)  LR: 6.395e-05  Data: 0.010 (0.013)
Train: 510 [ 600/1251 ( 48%)]  Loss: 3.162 (3.10)  Time: 0.807s, 1269.31/s  (0.796s, 1285.66/s)  LR: 6.395e-05  Data: 0.010 (0.013)
Train: 510 [ 650/1251 ( 52%)]  Loss: 2.994 (3.09)  Time: 0.771s, 1327.61/s  (0.797s, 1285.24/s)  LR: 6.395e-05  Data: 0.010 (0.013)
Train: 510 [ 700/1251 ( 56%)]  Loss: 2.935 (3.08)  Time: 0.807s, 1269.50/s  (0.796s, 1286.02/s)  LR: 6.395e-05  Data: 0.009 (0.012)
Train: 510 [ 750/1251 ( 60%)]  Loss: 3.156 (3.09)  Time: 0.807s, 1268.36/s  (0.796s, 1286.12/s)  LR: 6.395e-05  Data: 0.010 (0.012)
Train: 510 [ 800/1251 ( 64%)]  Loss: 2.757 (3.07)  Time: 0.814s, 1257.30/s  (0.796s, 1286.88/s)  LR: 6.395e-05  Data: 0.011 (0.012)
Train: 510 [ 850/1251 ( 68%)]  Loss: 2.975 (3.06)  Time: 0.808s, 1267.64/s  (0.796s, 1286.44/s)  LR: 6.395e-05  Data: 0.010 (0.012)
Train: 510 [ 900/1251 ( 72%)]  Loss: 2.975 (3.06)  Time: 0.808s, 1267.14/s  (0.797s, 1285.40/s)  LR: 6.395e-05  Data: 0.010 (0.012)
Train: 510 [ 950/1251 ( 76%)]  Loss: 3.044 (3.06)  Time: 0.771s, 1328.19/s  (0.796s, 1286.31/s)  LR: 6.395e-05  Data: 0.009 (0.012)
Train: 510 [1000/1251 ( 80%)]  Loss: 3.027 (3.05)  Time: 0.772s, 1325.83/s  (0.795s, 1287.40/s)  LR: 6.395e-05  Data: 0.010 (0.012)
Train: 510 [1050/1251 ( 84%)]  Loss: 3.135 (3.06)  Time: 0.778s, 1315.48/s  (0.796s, 1286.64/s)  LR: 6.395e-05  Data: 0.010 (0.012)
Train: 510 [1100/1251 ( 88%)]  Loss: 3.130 (3.06)  Time: 0.774s, 1322.64/s  (0.796s, 1286.90/s)  LR: 6.395e-05  Data: 0.010 (0.011)
Train: 510 [1150/1251 ( 92%)]  Loss: 2.879 (3.05)  Time: 0.774s, 1322.57/s  (0.796s, 1286.48/s)  LR: 6.395e-05  Data: 0.009 (0.011)
Train: 510 [1200/1251 ( 96%)]  Loss: 3.356 (3.07)  Time: 0.781s, 1311.34/s  (0.796s, 1286.12/s)  LR: 6.395e-05  Data: 0.010 (0.011)
Train: 510 [1250/1251 (100%)]  Loss: 2.803 (3.06)  Time: 0.760s, 1347.49/s  (0.796s, 1286.83/s)  LR: 6.395e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.544 (1.544)  Loss:  0.7134 (0.7134)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.194 (0.577)  Loss:  0.7632 (1.1126)  Acc@1: 86.3208 (80.1340)  Acc@5: 98.1132 (95.2380)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-507.pth.tar', 80.24400008056641)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-502.pth.tar', 80.24199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-509.pth.tar', 80.19999994873046)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-506.pth.tar', 80.15199987304688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-510.pth.tar', 80.13400000488281)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-498.pth.tar', 80.10800002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-504.pth.tar', 80.08400002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-505.pth.tar', 80.054)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-503.pth.tar', 80.0140000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-494.pth.tar', 80.00199997802734)

Train: 511 [   0/1251 (  0%)]  Loss: 3.017 (3.02)  Time: 2.189s,  467.81/s  (2.189s,  467.81/s)  LR: 6.278e-05  Data: 1.474 (1.474)
Train: 511 [  50/1251 (  4%)]  Loss: 3.224 (3.12)  Time: 0.774s, 1322.61/s  (0.824s, 1242.06/s)  LR: 6.278e-05  Data: 0.010 (0.045)
Train: 511 [ 100/1251 (  8%)]  Loss: 3.002 (3.08)  Time: 0.786s, 1302.11/s  (0.805s, 1271.89/s)  LR: 6.278e-05  Data: 0.009 (0.028)
Train: 511 [ 150/1251 ( 12%)]  Loss: 2.916 (3.04)  Time: 0.796s, 1286.80/s  (0.799s, 1281.39/s)  LR: 6.278e-05  Data: 0.009 (0.022)
Train: 511 [ 200/1251 ( 16%)]  Loss: 2.776 (2.99)  Time: 0.832s, 1230.09/s  (0.799s, 1281.76/s)  LR: 6.278e-05  Data: 0.011 (0.019)
Train: 511 [ 250/1251 ( 20%)]  Loss: 3.130 (3.01)  Time: 0.779s, 1315.31/s  (0.797s, 1284.72/s)  LR: 6.278e-05  Data: 0.010 (0.017)
Train: 511 [ 300/1251 ( 24%)]  Loss: 2.829 (2.98)  Time: 0.815s, 1255.83/s  (0.798s, 1283.15/s)  LR: 6.278e-05  Data: 0.011 (0.016)
Train: 511 [ 350/1251 ( 28%)]  Loss: 3.125 (3.00)  Time: 0.772s, 1326.43/s  (0.798s, 1283.76/s)  LR: 6.278e-05  Data: 0.010 (0.015)
Train: 511 [ 400/1251 ( 32%)]  Loss: 2.921 (2.99)  Time: 0.814s, 1258.07/s  (0.797s, 1284.72/s)  LR: 6.278e-05  Data: 0.011 (0.014)
Train: 511 [ 450/1251 ( 36%)]  Loss: 2.875 (2.98)  Time: 0.772s, 1326.10/s  (0.795s, 1287.29/s)  LR: 6.278e-05  Data: 0.010 (0.014)
Train: 511 [ 500/1251 ( 40%)]  Loss: 3.262 (3.01)  Time: 0.871s, 1175.12/s  (0.795s, 1287.68/s)  LR: 6.278e-05  Data: 0.010 (0.014)
Train: 511 [ 550/1251 ( 44%)]  Loss: 2.923 (3.00)  Time: 0.808s, 1267.04/s  (0.796s, 1286.98/s)  LR: 6.278e-05  Data: 0.009 (0.013)
Train: 511 [ 600/1251 ( 48%)]  Loss: 3.031 (3.00)  Time: 0.804s, 1273.61/s  (0.795s, 1288.47/s)  LR: 6.278e-05  Data: 0.009 (0.013)
Train: 511 [ 650/1251 ( 52%)]  Loss: 2.977 (3.00)  Time: 0.773s, 1324.35/s  (0.795s, 1288.48/s)  LR: 6.278e-05  Data: 0.009 (0.013)
Train: 511 [ 700/1251 ( 56%)]  Loss: 2.822 (2.99)  Time: 0.823s, 1243.88/s  (0.795s, 1287.73/s)  LR: 6.278e-05  Data: 0.009 (0.013)
Train: 511 [ 750/1251 ( 60%)]  Loss: 3.090 (3.00)  Time: 0.895s, 1144.58/s  (0.796s, 1287.06/s)  LR: 6.278e-05  Data: 0.010 (0.012)
Train: 511 [ 800/1251 ( 64%)]  Loss: 2.919 (2.99)  Time: 0.774s, 1323.64/s  (0.795s, 1288.27/s)  LR: 6.278e-05  Data: 0.010 (0.012)
Train: 511 [ 850/1251 ( 68%)]  Loss: 3.176 (3.00)  Time: 0.786s, 1302.58/s  (0.794s, 1289.16/s)  LR: 6.278e-05  Data: 0.010 (0.012)
Train: 511 [ 900/1251 ( 72%)]  Loss: 3.406 (3.02)  Time: 0.807s, 1269.13/s  (0.794s, 1290.28/s)  LR: 6.278e-05  Data: 0.010 (0.012)
Train: 511 [ 950/1251 ( 76%)]  Loss: 3.192 (3.03)  Time: 0.776s, 1319.74/s  (0.793s, 1290.99/s)  LR: 6.278e-05  Data: 0.009 (0.012)
Train: 511 [1000/1251 ( 80%)]  Loss: 2.896 (3.02)  Time: 0.778s, 1316.90/s  (0.793s, 1291.41/s)  LR: 6.278e-05  Data: 0.010 (0.012)
Train: 511 [1050/1251 ( 84%)]  Loss: 2.856 (3.02)  Time: 0.776s, 1319.29/s  (0.792s, 1292.53/s)  LR: 6.278e-05  Data: 0.010 (0.012)
Train: 511 [1100/1251 ( 88%)]  Loss: 3.134 (3.02)  Time: 0.807s, 1268.49/s  (0.792s, 1292.92/s)  LR: 6.278e-05  Data: 0.010 (0.012)
Train: 511 [1150/1251 ( 92%)]  Loss: 3.008 (3.02)  Time: 0.809s, 1265.84/s  (0.792s, 1292.71/s)  LR: 6.278e-05  Data: 0.009 (0.012)
Train: 511 [1200/1251 ( 96%)]  Loss: 3.250 (3.03)  Time: 0.777s, 1318.11/s  (0.792s, 1292.63/s)  LR: 6.278e-05  Data: 0.010 (0.011)
Train: 511 [1250/1251 (100%)]  Loss: 3.471 (3.05)  Time: 0.761s, 1345.22/s  (0.792s, 1292.42/s)  LR: 6.278e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.504 (1.504)  Loss:  0.6880 (0.6880)  Acc@1: 92.3828 (92.3828)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.193 (0.573)  Loss:  0.7622 (1.0973)  Acc@1: 86.2028 (80.2280)  Acc@5: 98.2311 (95.2720)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-507.pth.tar', 80.24400008056641)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-502.pth.tar', 80.24199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-511.pth.tar', 80.22799995361328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-509.pth.tar', 80.19999994873046)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-506.pth.tar', 80.15199987304688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-510.pth.tar', 80.13400000488281)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-498.pth.tar', 80.10800002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-504.pth.tar', 80.08400002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-505.pth.tar', 80.054)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-503.pth.tar', 80.0140000024414)

Train: 512 [   0/1251 (  0%)]  Loss: 3.074 (3.07)  Time: 2.340s,  437.58/s  (2.340s,  437.58/s)  LR: 6.162e-05  Data: 1.611 (1.611)
Train: 512 [  50/1251 (  4%)]  Loss: 2.911 (2.99)  Time: 0.778s, 1315.86/s  (0.824s, 1242.80/s)  LR: 6.162e-05  Data: 0.009 (0.045)
Train: 512 [ 100/1251 (  8%)]  Loss: 3.315 (3.10)  Time: 0.772s, 1326.16/s  (0.804s, 1273.17/s)  LR: 6.162e-05  Data: 0.009 (0.028)
Train: 512 [ 150/1251 ( 12%)]  Loss: 3.247 (3.14)  Time: 0.773s, 1324.39/s  (0.795s, 1287.81/s)  LR: 6.162e-05  Data: 0.010 (0.022)
Train: 512 [ 200/1251 ( 16%)]  Loss: 3.288 (3.17)  Time: 0.777s, 1317.99/s  (0.794s, 1290.02/s)  LR: 6.162e-05  Data: 0.010 (0.019)
Train: 512 [ 250/1251 ( 20%)]  Loss: 3.180 (3.17)  Time: 0.776s, 1318.76/s  (0.796s, 1286.59/s)  LR: 6.162e-05  Data: 0.013 (0.017)
Train: 512 [ 300/1251 ( 24%)]  Loss: 3.134 (3.16)  Time: 0.772s, 1325.84/s  (0.794s, 1289.41/s)  LR: 6.162e-05  Data: 0.010 (0.016)
Train: 512 [ 350/1251 ( 28%)]  Loss: 3.063 (3.15)  Time: 0.830s, 1234.35/s  (0.794s, 1290.08/s)  LR: 6.162e-05  Data: 0.009 (0.015)
Train: 512 [ 400/1251 ( 32%)]  Loss: 3.123 (3.15)  Time: 0.773s, 1324.31/s  (0.794s, 1289.53/s)  LR: 6.162e-05  Data: 0.010 (0.014)
Train: 512 [ 450/1251 ( 36%)]  Loss: 3.002 (3.13)  Time: 0.774s, 1323.59/s  (0.794s, 1289.74/s)  LR: 6.162e-05  Data: 0.010 (0.014)
Train: 512 [ 500/1251 ( 40%)]  Loss: 2.902 (3.11)  Time: 0.774s, 1322.22/s  (0.793s, 1291.10/s)  LR: 6.162e-05  Data: 0.009 (0.013)
Train: 512 [ 550/1251 ( 44%)]  Loss: 2.924 (3.10)  Time: 0.773s, 1324.12/s  (0.792s, 1292.21/s)  LR: 6.162e-05  Data: 0.009 (0.013)
Train: 512 [ 600/1251 ( 48%)]  Loss: 2.880 (3.08)  Time: 0.776s, 1319.04/s  (0.793s, 1291.71/s)  LR: 6.162e-05  Data: 0.009 (0.013)
Train: 512 [ 650/1251 ( 52%)]  Loss: 2.867 (3.07)  Time: 0.814s, 1257.48/s  (0.793s, 1291.94/s)  LR: 6.162e-05  Data: 0.010 (0.013)
Train: 512 [ 700/1251 ( 56%)]  Loss: 3.252 (3.08)  Time: 0.815s, 1256.25/s  (0.793s, 1291.95/s)  LR: 6.162e-05  Data: 0.010 (0.012)
Train: 512 [ 750/1251 ( 60%)]  Loss: 3.029 (3.07)  Time: 0.784s, 1306.05/s  (0.794s, 1290.22/s)  LR: 6.162e-05  Data: 0.010 (0.012)
Train: 512 [ 800/1251 ( 64%)]  Loss: 2.674 (3.05)  Time: 0.785s, 1303.93/s  (0.794s, 1290.46/s)  LR: 6.162e-05  Data: 0.009 (0.012)
Train: 512 [ 850/1251 ( 68%)]  Loss: 3.334 (3.07)  Time: 0.807s, 1268.19/s  (0.793s, 1290.51/s)  LR: 6.162e-05  Data: 0.010 (0.012)
Train: 512 [ 900/1251 ( 72%)]  Loss: 3.302 (3.08)  Time: 0.778s, 1316.05/s  (0.794s, 1290.06/s)  LR: 6.162e-05  Data: 0.010 (0.012)
Train: 512 [ 950/1251 ( 76%)]  Loss: 3.027 (3.08)  Time: 0.780s, 1313.51/s  (0.794s, 1290.00/s)  LR: 6.162e-05  Data: 0.010 (0.012)
Train: 512 [1000/1251 ( 80%)]  Loss: 2.761 (3.06)  Time: 0.808s, 1266.82/s  (0.794s, 1290.10/s)  LR: 6.162e-05  Data: 0.011 (0.012)
Train: 512 [1050/1251 ( 84%)]  Loss: 3.006 (3.06)  Time: 0.809s, 1265.93/s  (0.793s, 1290.75/s)  LR: 6.162e-05  Data: 0.010 (0.012)
Train: 512 [1100/1251 ( 88%)]  Loss: 3.089 (3.06)  Time: 0.826s, 1239.08/s  (0.793s, 1290.51/s)  LR: 6.162e-05  Data: 0.010 (0.011)
Train: 512 [1150/1251 ( 92%)]  Loss: 2.962 (3.06)  Time: 0.818s, 1252.10/s  (0.794s, 1290.29/s)  LR: 6.162e-05  Data: 0.011 (0.011)
Train: 512 [1200/1251 ( 96%)]  Loss: 3.240 (3.06)  Time: 0.775s, 1321.30/s  (0.794s, 1289.84/s)  LR: 6.162e-05  Data: 0.010 (0.011)
Train: 512 [1250/1251 (100%)]  Loss: 3.151 (3.07)  Time: 0.763s, 1342.66/s  (0.793s, 1290.70/s)  LR: 6.162e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.602 (1.602)  Loss:  0.7334 (0.7334)  Acc@1: 91.5039 (91.5039)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.194 (0.574)  Loss:  0.7949 (1.1632)  Acc@1: 87.1462 (80.1240)  Acc@5: 97.8774 (95.2100)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-507.pth.tar', 80.24400008056641)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-502.pth.tar', 80.24199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-511.pth.tar', 80.22799995361328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-509.pth.tar', 80.19999994873046)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-506.pth.tar', 80.15199987304688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-510.pth.tar', 80.13400000488281)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-512.pth.tar', 80.12400010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-498.pth.tar', 80.10800002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-504.pth.tar', 80.08400002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-505.pth.tar', 80.054)

Train: 513 [   0/1251 (  0%)]  Loss: 3.104 (3.10)  Time: 2.216s,  462.14/s  (2.216s,  462.14/s)  LR: 6.048e-05  Data: 1.484 (1.484)
Train: 513 [  50/1251 (  4%)]  Loss: 2.983 (3.04)  Time: 0.782s, 1308.82/s  (0.825s, 1241.62/s)  LR: 6.048e-05  Data: 0.009 (0.044)
Train: 513 [ 100/1251 (  8%)]  Loss: 3.171 (3.09)  Time: 0.774s, 1323.21/s  (0.804s, 1273.31/s)  LR: 6.048e-05  Data: 0.010 (0.027)
Train: 513 [ 150/1251 ( 12%)]  Loss: 2.850 (3.03)  Time: 0.839s, 1219.86/s  (0.804s, 1273.96/s)  LR: 6.048e-05  Data: 0.009 (0.021)
Train: 513 [ 200/1251 ( 16%)]  Loss: 3.086 (3.04)  Time: 0.771s, 1328.75/s  (0.797s, 1285.05/s)  LR: 6.048e-05  Data: 0.009 (0.018)
Train: 513 [ 250/1251 ( 20%)]  Loss: 2.846 (3.01)  Time: 0.776s, 1319.33/s  (0.796s, 1285.77/s)  LR: 6.048e-05  Data: 0.013 (0.017)
Train: 513 [ 300/1251 ( 24%)]  Loss: 3.107 (3.02)  Time: 0.947s, 1080.90/s  (0.796s, 1286.34/s)  LR: 6.048e-05  Data: 0.009 (0.015)
Train: 513 [ 350/1251 ( 28%)]  Loss: 2.669 (2.98)  Time: 0.775s, 1320.77/s  (0.794s, 1289.51/s)  LR: 6.048e-05  Data: 0.009 (0.015)
Train: 513 [ 400/1251 ( 32%)]  Loss: 2.704 (2.95)  Time: 0.773s, 1325.30/s  (0.793s, 1292.11/s)  LR: 6.048e-05  Data: 0.010 (0.014)
Train: 513 [ 450/1251 ( 36%)]  Loss: 3.056 (2.96)  Time: 0.803s, 1275.39/s  (0.792s, 1293.49/s)  LR: 6.048e-05  Data: 0.010 (0.014)
Train: 513 [ 500/1251 ( 40%)]  Loss: 2.659 (2.93)  Time: 0.808s, 1267.91/s  (0.791s, 1294.07/s)  LR: 6.048e-05  Data: 0.009 (0.013)
Train: 513 [ 550/1251 ( 44%)]  Loss: 3.100 (2.94)  Time: 0.808s, 1267.45/s  (0.793s, 1291.75/s)  LR: 6.048e-05  Data: 0.009 (0.013)
Train: 513 [ 600/1251 ( 48%)]  Loss: 2.908 (2.94)  Time: 0.807s, 1269.57/s  (0.794s, 1289.59/s)  LR: 6.048e-05  Data: 0.009 (0.013)
Train: 513 [ 650/1251 ( 52%)]  Loss: 3.093 (2.95)  Time: 0.836s, 1224.37/s  (0.795s, 1287.43/s)  LR: 6.048e-05  Data: 0.010 (0.012)
Train: 513 [ 700/1251 ( 56%)]  Loss: 2.656 (2.93)  Time: 0.815s, 1256.05/s  (0.795s, 1288.66/s)  LR: 6.048e-05  Data: 0.010 (0.012)
Train: 513 [ 750/1251 ( 60%)]  Loss: 2.775 (2.92)  Time: 0.774s, 1322.59/s  (0.794s, 1289.17/s)  LR: 6.048e-05  Data: 0.010 (0.012)
Train: 513 [ 800/1251 ( 64%)]  Loss: 2.836 (2.92)  Time: 0.773s, 1324.91/s  (0.793s, 1290.68/s)  LR: 6.048e-05  Data: 0.010 (0.012)
Train: 513 [ 850/1251 ( 68%)]  Loss: 3.326 (2.94)  Time: 0.807s, 1268.16/s  (0.793s, 1290.78/s)  LR: 6.048e-05  Data: 0.009 (0.012)
Train: 513 [ 900/1251 ( 72%)]  Loss: 2.937 (2.94)  Time: 0.823s, 1244.46/s  (0.793s, 1290.90/s)  LR: 6.048e-05  Data: 0.010 (0.012)
Train: 513 [ 950/1251 ( 76%)]  Loss: 3.121 (2.95)  Time: 0.806s, 1270.39/s  (0.794s, 1288.98/s)  LR: 6.048e-05  Data: 0.009 (0.012)
Train: 513 [1000/1251 ( 80%)]  Loss: 3.058 (2.95)  Time: 0.773s, 1325.22/s  (0.794s, 1290.12/s)  LR: 6.048e-05  Data: 0.009 (0.011)
Train: 513 [1050/1251 ( 84%)]  Loss: 3.082 (2.96)  Time: 0.810s, 1264.95/s  (0.794s, 1290.07/s)  LR: 6.048e-05  Data: 0.009 (0.011)
Train: 513 [1100/1251 ( 88%)]  Loss: 3.198 (2.97)  Time: 0.808s, 1267.45/s  (0.793s, 1290.85/s)  LR: 6.048e-05  Data: 0.010 (0.011)
Train: 513 [1150/1251 ( 92%)]  Loss: 3.158 (2.98)  Time: 0.773s, 1324.18/s  (0.793s, 1290.98/s)  LR: 6.048e-05  Data: 0.010 (0.011)
Train: 513 [1200/1251 ( 96%)]  Loss: 3.145 (2.99)  Time: 0.779s, 1315.22/s  (0.793s, 1291.36/s)  LR: 6.048e-05  Data: 0.010 (0.011)
Train: 513 [1250/1251 (100%)]  Loss: 3.329 (3.00)  Time: 0.766s, 1336.98/s  (0.793s, 1292.02/s)  LR: 6.048e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.573 (1.573)  Loss:  0.5869 (0.5869)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.194 (0.567)  Loss:  0.7070 (1.0375)  Acc@1: 86.4387 (80.3320)  Acc@5: 97.8774 (95.2540)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-513.pth.tar', 80.33199992675782)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-507.pth.tar', 80.24400008056641)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-502.pth.tar', 80.24199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-511.pth.tar', 80.22799995361328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-509.pth.tar', 80.19999994873046)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-506.pth.tar', 80.15199987304688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-510.pth.tar', 80.13400000488281)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-512.pth.tar', 80.12400010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-498.pth.tar', 80.10800002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-504.pth.tar', 80.08400002929687)

Train: 514 [   0/1251 (  0%)]  Loss: 3.111 (3.11)  Time: 2.291s,  446.95/s  (2.291s,  446.95/s)  LR: 5.934e-05  Data: 1.562 (1.562)
Train: 514 [  50/1251 (  4%)]  Loss: 3.100 (3.11)  Time: 0.771s, 1328.97/s  (0.817s, 1253.56/s)  LR: 5.934e-05  Data: 0.010 (0.045)
Train: 514 [ 100/1251 (  8%)]  Loss: 3.175 (3.13)  Time: 0.816s, 1255.00/s  (0.809s, 1265.23/s)  LR: 5.934e-05  Data: 0.011 (0.028)
Train: 514 [ 150/1251 ( 12%)]  Loss: 3.145 (3.13)  Time: 0.808s, 1266.94/s  (0.812s, 1261.72/s)  LR: 5.934e-05  Data: 0.010 (0.023)
Train: 514 [ 200/1251 ( 16%)]  Loss: 3.147 (3.14)  Time: 0.773s, 1324.84/s  (0.806s, 1270.72/s)  LR: 5.934e-05  Data: 0.010 (0.019)
Train: 514 [ 250/1251 ( 20%)]  Loss: 3.025 (3.12)  Time: 0.776s, 1320.21/s  (0.803s, 1275.66/s)  LR: 5.934e-05  Data: 0.010 (0.018)
Train: 514 [ 300/1251 ( 24%)]  Loss: 2.902 (3.09)  Time: 0.778s, 1316.12/s  (0.801s, 1277.79/s)  LR: 5.934e-05  Data: 0.010 (0.016)
Train: 514 [ 350/1251 ( 28%)]  Loss: 3.094 (3.09)  Time: 0.783s, 1307.12/s  (0.801s, 1278.41/s)  LR: 5.934e-05  Data: 0.010 (0.015)
Train: 514 [ 400/1251 ( 32%)]  Loss: 2.928 (3.07)  Time: 0.773s, 1323.88/s  (0.800s, 1280.63/s)  LR: 5.934e-05  Data: 0.009 (0.015)
Train: 514 [ 450/1251 ( 36%)]  Loss: 2.972 (3.06)  Time: 0.773s, 1324.81/s  (0.797s, 1284.19/s)  LR: 5.934e-05  Data: 0.010 (0.014)
Train: 514 [ 500/1251 ( 40%)]  Loss: 2.889 (3.04)  Time: 0.803s, 1274.83/s  (0.796s, 1286.80/s)  LR: 5.934e-05  Data: 0.010 (0.014)
Train: 514 [ 550/1251 ( 44%)]  Loss: 2.934 (3.04)  Time: 0.772s, 1325.72/s  (0.795s, 1287.92/s)  LR: 5.934e-05  Data: 0.009 (0.013)
Train: 514 [ 600/1251 ( 48%)]  Loss: 3.022 (3.03)  Time: 0.772s, 1325.66/s  (0.794s, 1289.76/s)  LR: 5.934e-05  Data: 0.010 (0.013)
Train: 514 [ 650/1251 ( 52%)]  Loss: 2.849 (3.02)  Time: 0.814s, 1257.48/s  (0.794s, 1289.82/s)  LR: 5.934e-05  Data: 0.011 (0.013)
Train: 514 [ 700/1251 ( 56%)]  Loss: 3.148 (3.03)  Time: 0.774s, 1322.65/s  (0.794s, 1289.20/s)  LR: 5.934e-05  Data: 0.011 (0.013)
Train: 514 [ 750/1251 ( 60%)]  Loss: 3.199 (3.04)  Time: 0.773s, 1324.39/s  (0.794s, 1289.34/s)  LR: 5.934e-05  Data: 0.010 (0.013)
Train: 514 [ 800/1251 ( 64%)]  Loss: 2.773 (3.02)  Time: 0.773s, 1325.56/s  (0.793s, 1290.58/s)  LR: 5.934e-05  Data: 0.010 (0.012)
Train: 514 [ 850/1251 ( 68%)]  Loss: 3.297 (3.04)  Time: 0.772s, 1326.90/s  (0.793s, 1291.57/s)  LR: 5.934e-05  Data: 0.010 (0.012)
Train: 514 [ 900/1251 ( 72%)]  Loss: 3.099 (3.04)  Time: 0.773s, 1325.27/s  (0.793s, 1291.90/s)  LR: 5.934e-05  Data: 0.009 (0.012)
Train: 514 [ 950/1251 ( 76%)]  Loss: 3.067 (3.04)  Time: 0.807s, 1268.67/s  (0.793s, 1291.57/s)  LR: 5.934e-05  Data: 0.010 (0.012)
Train: 514 [1000/1251 ( 80%)]  Loss: 3.208 (3.05)  Time: 0.808s, 1267.57/s  (0.793s, 1290.83/s)  LR: 5.934e-05  Data: 0.009 (0.012)
Train: 514 [1050/1251 ( 84%)]  Loss: 3.267 (3.06)  Time: 0.776s, 1320.11/s  (0.793s, 1290.79/s)  LR: 5.934e-05  Data: 0.013 (0.012)
Train: 514 [1100/1251 ( 88%)]  Loss: 2.945 (3.06)  Time: 0.810s, 1263.80/s  (0.794s, 1290.08/s)  LR: 5.934e-05  Data: 0.010 (0.012)
Train: 514 [1150/1251 ( 92%)]  Loss: 3.027 (3.06)  Time: 0.772s, 1326.05/s  (0.793s, 1290.59/s)  LR: 5.934e-05  Data: 0.009 (0.012)
Train: 514 [1200/1251 ( 96%)]  Loss: 2.846 (3.05)  Time: 0.781s, 1311.40/s  (0.794s, 1290.30/s)  LR: 5.934e-05  Data: 0.010 (0.012)
Train: 514 [1250/1251 (100%)]  Loss: 3.035 (3.05)  Time: 0.773s, 1325.00/s  (0.794s, 1289.40/s)  LR: 5.934e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.579 (1.579)  Loss:  0.7056 (0.7056)  Acc@1: 91.7969 (91.7969)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.194 (0.569)  Loss:  0.7466 (1.1394)  Acc@1: 87.5000 (80.0440)  Acc@5: 97.9953 (95.1920)
Train: 515 [   0/1251 (  0%)]  Loss: 2.966 (2.97)  Time: 2.227s,  459.73/s  (2.227s,  459.73/s)  LR: 5.822e-05  Data: 1.433 (1.433)
Train: 515 [  50/1251 (  4%)]  Loss: 3.021 (2.99)  Time: 0.803s, 1274.92/s  (0.825s, 1240.54/s)  LR: 5.822e-05  Data: 0.010 (0.040)
Train: 515 [ 100/1251 (  8%)]  Loss: 2.618 (2.87)  Time: 0.773s, 1323.86/s  (0.808s, 1266.60/s)  LR: 5.822e-05  Data: 0.009 (0.025)
Train: 515 [ 150/1251 ( 12%)]  Loss: 3.046 (2.91)  Time: 0.775s, 1320.92/s  (0.803s, 1275.28/s)  LR: 5.822e-05  Data: 0.009 (0.020)
Train: 515 [ 200/1251 ( 16%)]  Loss: 3.084 (2.95)  Time: 0.773s, 1325.09/s  (0.797s, 1285.23/s)  LR: 5.822e-05  Data: 0.010 (0.018)
Train: 515 [ 250/1251 ( 20%)]  Loss: 3.066 (2.97)  Time: 0.792s, 1292.91/s  (0.800s, 1280.41/s)  LR: 5.822e-05  Data: 0.010 (0.016)
Train: 515 [ 300/1251 ( 24%)]  Loss: 3.255 (3.01)  Time: 0.830s, 1233.30/s  (0.797s, 1284.95/s)  LR: 5.822e-05  Data: 0.016 (0.015)
Train: 515 [ 350/1251 ( 28%)]  Loss: 3.176 (3.03)  Time: 0.777s, 1318.15/s  (0.796s, 1286.43/s)  LR: 5.822e-05  Data: 0.010 (0.015)
Train: 515 [ 400/1251 ( 32%)]  Loss: 3.113 (3.04)  Time: 0.773s, 1324.90/s  (0.796s, 1286.10/s)  LR: 5.822e-05  Data: 0.010 (0.014)
Train: 515 [ 450/1251 ( 36%)]  Loss: 3.056 (3.04)  Time: 0.806s, 1270.55/s  (0.795s, 1288.27/s)  LR: 5.822e-05  Data: 0.009 (0.013)
Train: 515 [ 500/1251 ( 40%)]  Loss: 2.910 (3.03)  Time: 0.773s, 1325.47/s  (0.795s, 1288.36/s)  LR: 5.822e-05  Data: 0.010 (0.013)
Train: 515 [ 550/1251 ( 44%)]  Loss: 2.991 (3.03)  Time: 0.857s, 1194.86/s  (0.795s, 1288.33/s)  LR: 5.822e-05  Data: 0.012 (0.013)
Train: 515 [ 600/1251 ( 48%)]  Loss: 3.089 (3.03)  Time: 0.808s, 1267.53/s  (0.795s, 1287.45/s)  LR: 5.822e-05  Data: 0.009 (0.013)
Train: 515 [ 650/1251 ( 52%)]  Loss: 3.066 (3.03)  Time: 0.776s, 1319.20/s  (0.794s, 1289.27/s)  LR: 5.822e-05  Data: 0.009 (0.012)
Train: 515 [ 700/1251 ( 56%)]  Loss: 3.117 (3.04)  Time: 0.774s, 1323.34/s  (0.795s, 1288.76/s)  LR: 5.822e-05  Data: 0.010 (0.012)
Train: 515 [ 750/1251 ( 60%)]  Loss: 2.963 (3.03)  Time: 0.773s, 1323.99/s  (0.794s, 1290.24/s)  LR: 5.822e-05  Data: 0.010 (0.012)
Train: 515 [ 800/1251 ( 64%)]  Loss: 2.704 (3.01)  Time: 0.817s, 1253.92/s  (0.795s, 1288.76/s)  LR: 5.822e-05  Data: 0.010 (0.012)
Train: 515 [ 850/1251 ( 68%)]  Loss: 2.872 (3.01)  Time: 0.774s, 1323.45/s  (0.794s, 1288.89/s)  LR: 5.822e-05  Data: 0.010 (0.012)
Train: 515 [ 900/1251 ( 72%)]  Loss: 3.111 (3.01)  Time: 0.775s, 1320.83/s  (0.795s, 1288.62/s)  LR: 5.822e-05  Data: 0.010 (0.012)
Train: 515 [ 950/1251 ( 76%)]  Loss: 2.990 (3.01)  Time: 0.775s, 1321.16/s  (0.794s, 1289.08/s)  LR: 5.822e-05  Data: 0.009 (0.012)
Train: 515 [1000/1251 ( 80%)]  Loss: 3.056 (3.01)  Time: 0.805s, 1271.41/s  (0.794s, 1289.23/s)  LR: 5.822e-05  Data: 0.010 (0.011)
Train: 515 [1050/1251 ( 84%)]  Loss: 3.182 (3.02)  Time: 0.794s, 1290.15/s  (0.794s, 1289.33/s)  LR: 5.822e-05  Data: 0.009 (0.011)
Train: 515 [1100/1251 ( 88%)]  Loss: 3.285 (3.03)  Time: 0.781s, 1310.93/s  (0.794s, 1289.07/s)  LR: 5.822e-05  Data: 0.010 (0.011)
Train: 515 [1150/1251 ( 92%)]  Loss: 3.199 (3.04)  Time: 0.788s, 1299.56/s  (0.794s, 1289.60/s)  LR: 5.822e-05  Data: 0.009 (0.011)
Train: 515 [1200/1251 ( 96%)]  Loss: 3.089 (3.04)  Time: 0.806s, 1270.86/s  (0.794s, 1289.92/s)  LR: 5.822e-05  Data: 0.009 (0.011)
Train: 515 [1250/1251 (100%)]  Loss: 2.986 (3.04)  Time: 0.761s, 1345.16/s  (0.794s, 1289.60/s)  LR: 5.822e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.626 (1.626)  Loss:  0.6489 (0.6489)  Acc@1: 91.8945 (91.8945)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.194 (0.568)  Loss:  0.7021 (1.0582)  Acc@1: 87.6179 (80.3560)  Acc@5: 97.8774 (95.2400)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-515.pth.tar', 80.35600005126953)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-513.pth.tar', 80.33199992675782)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-507.pth.tar', 80.24400008056641)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-502.pth.tar', 80.24199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-511.pth.tar', 80.22799995361328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-509.pth.tar', 80.19999994873046)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-506.pth.tar', 80.15199987304688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-510.pth.tar', 80.13400000488281)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-512.pth.tar', 80.12400010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-498.pth.tar', 80.10800002929687)

Train: 516 [   0/1251 (  0%)]  Loss: 2.820 (2.82)  Time: 2.253s,  454.49/s  (2.253s,  454.49/s)  LR: 5.711e-05  Data: 1.519 (1.519)
Train: 516 [  50/1251 (  4%)]  Loss: 3.250 (3.04)  Time: 0.780s, 1312.71/s  (0.826s, 1239.85/s)  LR: 5.711e-05  Data: 0.010 (0.044)
Train: 516 [ 100/1251 (  8%)]  Loss: 3.057 (3.04)  Time: 0.773s, 1324.72/s  (0.808s, 1267.54/s)  LR: 5.711e-05  Data: 0.009 (0.027)
Train: 516 [ 150/1251 ( 12%)]  Loss: 3.121 (3.06)  Time: 0.809s, 1265.30/s  (0.801s, 1278.13/s)  LR: 5.711e-05  Data: 0.010 (0.021)
Train: 516 [ 200/1251 ( 16%)]  Loss: 2.851 (3.02)  Time: 0.773s, 1324.51/s  (0.798s, 1283.92/s)  LR: 5.711e-05  Data: 0.010 (0.019)
Train: 516 [ 250/1251 ( 20%)]  Loss: 3.363 (3.08)  Time: 0.808s, 1266.71/s  (0.797s, 1284.46/s)  LR: 5.711e-05  Data: 0.009 (0.017)
Train: 516 [ 300/1251 ( 24%)]  Loss: 2.870 (3.05)  Time: 0.773s, 1325.00/s  (0.796s, 1286.97/s)  LR: 5.711e-05  Data: 0.010 (0.016)
Train: 516 [ 350/1251 ( 28%)]  Loss: 2.953 (3.04)  Time: 0.774s, 1322.20/s  (0.794s, 1289.11/s)  LR: 5.711e-05  Data: 0.009 (0.015)
Train: 516 [ 400/1251 ( 32%)]  Loss: 2.831 (3.01)  Time: 0.772s, 1327.09/s  (0.795s, 1288.83/s)  LR: 5.711e-05  Data: 0.010 (0.014)
Train: 516 [ 450/1251 ( 36%)]  Loss: 2.848 (3.00)  Time: 0.814s, 1258.01/s  (0.795s, 1287.28/s)  LR: 5.711e-05  Data: 0.009 (0.014)
Train: 516 [ 500/1251 ( 40%)]  Loss: 2.838 (2.98)  Time: 0.814s, 1257.82/s  (0.795s, 1287.67/s)  LR: 5.711e-05  Data: 0.010 (0.013)
Train: 516 [ 550/1251 ( 44%)]  Loss: 3.020 (2.99)  Time: 0.809s, 1266.52/s  (0.796s, 1287.07/s)  LR: 5.711e-05  Data: 0.010 (0.013)
Train: 516 [ 600/1251 ( 48%)]  Loss: 3.042 (2.99)  Time: 0.773s, 1324.64/s  (0.795s, 1287.81/s)  LR: 5.711e-05  Data: 0.009 (0.013)
Train: 516 [ 650/1251 ( 52%)]  Loss: 2.571 (2.96)  Time: 0.781s, 1311.43/s  (0.794s, 1289.05/s)  LR: 5.711e-05  Data: 0.010 (0.012)
Train: 516 [ 700/1251 ( 56%)]  Loss: 2.872 (2.95)  Time: 0.777s, 1317.70/s  (0.794s, 1289.07/s)  LR: 5.711e-05  Data: 0.009 (0.012)
Train: 516 [ 750/1251 ( 60%)]  Loss: 2.763 (2.94)  Time: 0.820s, 1248.67/s  (0.794s, 1289.76/s)  LR: 5.711e-05  Data: 0.009 (0.012)
Train: 516 [ 800/1251 ( 64%)]  Loss: 3.089 (2.95)  Time: 0.806s, 1269.94/s  (0.795s, 1288.74/s)  LR: 5.711e-05  Data: 0.009 (0.012)
Train: 516 [ 850/1251 ( 68%)]  Loss: 2.935 (2.95)  Time: 0.807s, 1268.31/s  (0.794s, 1288.94/s)  LR: 5.711e-05  Data: 0.010 (0.012)
Train: 516 [ 900/1251 ( 72%)]  Loss: 3.289 (2.97)  Time: 0.777s, 1318.12/s  (0.794s, 1289.76/s)  LR: 5.711e-05  Data: 0.009 (0.012)
Train: 516 [ 950/1251 ( 76%)]  Loss: 3.023 (2.97)  Time: 0.807s, 1268.96/s  (0.794s, 1289.94/s)  LR: 5.711e-05  Data: 0.009 (0.012)
Train: 516 [1000/1251 ( 80%)]  Loss: 2.980 (2.97)  Time: 0.774s, 1322.87/s  (0.793s, 1290.66/s)  LR: 5.711e-05  Data: 0.010 (0.012)
Train: 516 [1050/1251 ( 84%)]  Loss: 2.814 (2.96)  Time: 0.842s, 1215.86/s  (0.793s, 1291.31/s)  LR: 5.711e-05  Data: 0.009 (0.011)
Train: 516 [1100/1251 ( 88%)]  Loss: 2.722 (2.95)  Time: 0.845s, 1211.36/s  (0.793s, 1290.62/s)  LR: 5.711e-05  Data: 0.014 (0.011)
Train: 516 [1150/1251 ( 92%)]  Loss: 3.296 (2.97)  Time: 0.808s, 1266.86/s  (0.794s, 1290.47/s)  LR: 5.711e-05  Data: 0.009 (0.011)
Train: 516 [1200/1251 ( 96%)]  Loss: 3.226 (2.98)  Time: 0.811s, 1262.94/s  (0.794s, 1290.20/s)  LR: 5.711e-05  Data: 0.009 (0.011)
Train: 516 [1250/1251 (100%)]  Loss: 2.912 (2.98)  Time: 0.761s, 1346.17/s  (0.794s, 1289.71/s)  LR: 5.711e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.587 (1.587)  Loss:  0.6899 (0.6899)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.194 (0.567)  Loss:  0.7866 (1.1408)  Acc@1: 87.5000 (80.2480)  Acc@5: 98.1132 (95.2320)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-515.pth.tar', 80.35600005126953)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-513.pth.tar', 80.33199992675782)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-516.pth.tar', 80.24799987060547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-507.pth.tar', 80.24400008056641)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-502.pth.tar', 80.24199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-511.pth.tar', 80.22799995361328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-509.pth.tar', 80.19999994873046)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-506.pth.tar', 80.15199987304688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-510.pth.tar', 80.13400000488281)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-512.pth.tar', 80.12400010498047)

Train: 517 [   0/1251 (  0%)]  Loss: 2.849 (2.85)  Time: 2.292s,  446.77/s  (2.292s,  446.77/s)  LR: 5.601e-05  Data: 1.564 (1.564)
Train: 517 [  50/1251 (  4%)]  Loss: 2.840 (2.84)  Time: 0.775s, 1321.11/s  (0.820s, 1248.82/s)  LR: 5.601e-05  Data: 0.010 (0.045)
Train: 517 [ 100/1251 (  8%)]  Loss: 3.192 (2.96)  Time: 0.814s, 1258.60/s  (0.810s, 1264.66/s)  LR: 5.601e-05  Data: 0.011 (0.028)
Train: 517 [ 150/1251 ( 12%)]  Loss: 2.871 (2.94)  Time: 0.794s, 1290.17/s  (0.808s, 1267.85/s)  LR: 5.601e-05  Data: 0.010 (0.022)
Train: 517 [ 200/1251 ( 16%)]  Loss: 3.151 (2.98)  Time: 0.776s, 1318.79/s  (0.803s, 1275.37/s)  LR: 5.601e-05  Data: 0.010 (0.019)
Train: 517 [ 250/1251 ( 20%)]  Loss: 3.306 (3.03)  Time: 0.773s, 1324.52/s  (0.801s, 1278.94/s)  LR: 5.601e-05  Data: 0.010 (0.017)
Train: 517 [ 300/1251 ( 24%)]  Loss: 3.100 (3.04)  Time: 0.775s, 1320.76/s  (0.798s, 1282.53/s)  LR: 5.601e-05  Data: 0.010 (0.016)
Train: 517 [ 350/1251 ( 28%)]  Loss: 3.076 (3.05)  Time: 0.809s, 1266.47/s  (0.797s, 1285.12/s)  LR: 5.601e-05  Data: 0.010 (0.015)
Train: 517 [ 400/1251 ( 32%)]  Loss: 2.981 (3.04)  Time: 0.787s, 1300.36/s  (0.797s, 1285.11/s)  LR: 5.601e-05  Data: 0.009 (0.014)
Train: 517 [ 450/1251 ( 36%)]  Loss: 3.364 (3.07)  Time: 0.809s, 1266.40/s  (0.796s, 1286.30/s)  LR: 5.601e-05  Data: 0.010 (0.014)
Train: 517 [ 500/1251 ( 40%)]  Loss: 3.193 (3.08)  Time: 0.790s, 1296.26/s  (0.796s, 1286.60/s)  LR: 5.601e-05  Data: 0.009 (0.014)
Train: 517 [ 550/1251 ( 44%)]  Loss: 3.126 (3.09)  Time: 0.806s, 1269.95/s  (0.795s, 1288.67/s)  LR: 5.601e-05  Data: 0.010 (0.013)
Train: 517 [ 600/1251 ( 48%)]  Loss: 3.310 (3.10)  Time: 0.773s, 1325.38/s  (0.795s, 1287.87/s)  LR: 5.601e-05  Data: 0.010 (0.013)
Train: 517 [ 650/1251 ( 52%)]  Loss: 2.940 (3.09)  Time: 0.814s, 1257.71/s  (0.795s, 1287.51/s)  LR: 5.601e-05  Data: 0.010 (0.013)
Train: 517 [ 700/1251 ( 56%)]  Loss: 2.740 (3.07)  Time: 0.781s, 1310.65/s  (0.796s, 1286.49/s)  LR: 5.601e-05  Data: 0.010 (0.012)
Train: 517 [ 750/1251 ( 60%)]  Loss: 3.295 (3.08)  Time: 0.807s, 1268.97/s  (0.796s, 1285.87/s)  LR: 5.601e-05  Data: 0.010 (0.012)
Train: 517 [ 800/1251 ( 64%)]  Loss: 2.965 (3.08)  Time: 0.772s, 1325.75/s  (0.797s, 1285.15/s)  LR: 5.601e-05  Data: 0.010 (0.012)
Train: 517 [ 850/1251 ( 68%)]  Loss: 2.837 (3.06)  Time: 0.772s, 1325.61/s  (0.797s, 1285.39/s)  LR: 5.601e-05  Data: 0.010 (0.012)
Train: 517 [ 900/1251 ( 72%)]  Loss: 3.076 (3.06)  Time: 0.789s, 1298.43/s  (0.797s, 1284.95/s)  LR: 5.601e-05  Data: 0.010 (0.012)
Train: 517 [ 950/1251 ( 76%)]  Loss: 2.536 (3.04)  Time: 0.805s, 1272.27/s  (0.797s, 1284.91/s)  LR: 5.601e-05  Data: 0.010 (0.012)
Train: 517 [1000/1251 ( 80%)]  Loss: 2.953 (3.03)  Time: 0.807s, 1268.37/s  (0.797s, 1285.47/s)  LR: 5.601e-05  Data: 0.012 (0.012)
Train: 517 [1050/1251 ( 84%)]  Loss: 3.151 (3.04)  Time: 0.814s, 1257.31/s  (0.797s, 1284.29/s)  LR: 5.601e-05  Data: 0.009 (0.012)
Train: 517 [1100/1251 ( 88%)]  Loss: 2.928 (3.03)  Time: 0.771s, 1328.66/s  (0.798s, 1283.02/s)  LR: 5.601e-05  Data: 0.010 (0.012)
Train: 517 [1150/1251 ( 92%)]  Loss: 3.046 (3.03)  Time: 0.771s, 1327.36/s  (0.798s, 1283.15/s)  LR: 5.601e-05  Data: 0.009 (0.012)
Train: 517 [1200/1251 ( 96%)]  Loss: 2.870 (3.03)  Time: 0.860s, 1190.24/s  (0.798s, 1282.83/s)  LR: 5.601e-05  Data: 0.010 (0.011)
Train: 517 [1250/1251 (100%)]  Loss: 3.164 (3.03)  Time: 0.763s, 1342.61/s  (0.798s, 1283.21/s)  LR: 5.601e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.622 (1.622)  Loss:  0.6821 (0.6821)  Acc@1: 91.8945 (91.8945)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.194 (0.570)  Loss:  0.7510 (1.1122)  Acc@1: 87.3821 (80.1460)  Acc@5: 98.1132 (95.2760)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-515.pth.tar', 80.35600005126953)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-513.pth.tar', 80.33199992675782)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-516.pth.tar', 80.24799987060547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-507.pth.tar', 80.24400008056641)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-502.pth.tar', 80.24199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-511.pth.tar', 80.22799995361328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-509.pth.tar', 80.19999994873046)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-506.pth.tar', 80.15199987304688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-517.pth.tar', 80.14599994873046)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-510.pth.tar', 80.13400000488281)

Train: 518 [   0/1251 (  0%)]  Loss: 2.705 (2.70)  Time: 2.443s,  419.11/s  (2.443s,  419.11/s)  LR: 5.493e-05  Data: 1.717 (1.717)
Train: 518 [  50/1251 (  4%)]  Loss: 2.680 (2.69)  Time: 0.772s, 1325.79/s  (0.826s, 1239.66/s)  LR: 5.493e-05  Data: 0.009 (0.046)
Train: 518 [ 100/1251 (  8%)]  Loss: 3.004 (2.80)  Time: 0.806s, 1270.19/s  (0.810s, 1264.62/s)  LR: 5.493e-05  Data: 0.010 (0.028)
Train: 518 [ 150/1251 ( 12%)]  Loss: 2.887 (2.82)  Time: 0.784s, 1305.84/s  (0.801s, 1279.05/s)  LR: 5.493e-05  Data: 0.009 (0.022)
Train: 518 [ 200/1251 ( 16%)]  Loss: 2.715 (2.80)  Time: 0.806s, 1269.92/s  (0.798s, 1282.89/s)  LR: 5.493e-05  Data: 0.009 (0.019)
Train: 518 [ 250/1251 ( 20%)]  Loss: 2.844 (2.81)  Time: 0.773s, 1324.58/s  (0.797s, 1285.05/s)  LR: 5.493e-05  Data: 0.009 (0.017)
Train: 518 [ 300/1251 ( 24%)]  Loss: 2.508 (2.76)  Time: 0.780s, 1313.38/s  (0.795s, 1288.71/s)  LR: 5.493e-05  Data: 0.010 (0.016)
Train: 518 [ 350/1251 ( 28%)]  Loss: 3.299 (2.83)  Time: 0.807s, 1268.80/s  (0.794s, 1290.25/s)  LR: 5.493e-05  Data: 0.009 (0.015)
Train: 518 [ 400/1251 ( 32%)]  Loss: 3.174 (2.87)  Time: 0.774s, 1323.25/s  (0.793s, 1290.86/s)  LR: 5.493e-05  Data: 0.010 (0.014)
Train: 518 [ 450/1251 ( 36%)]  Loss: 2.859 (2.87)  Time: 0.811s, 1261.91/s  (0.794s, 1290.28/s)  LR: 5.493e-05  Data: 0.009 (0.014)
Train: 518 [ 500/1251 ( 40%)]  Loss: 3.070 (2.89)  Time: 0.775s, 1320.75/s  (0.794s, 1289.91/s)  LR: 5.493e-05  Data: 0.009 (0.013)
Train: 518 [ 550/1251 ( 44%)]  Loss: 2.512 (2.85)  Time: 0.771s, 1328.21/s  (0.794s, 1289.31/s)  LR: 5.493e-05  Data: 0.010 (0.013)
Train: 518 [ 600/1251 ( 48%)]  Loss: 2.902 (2.86)  Time: 0.826s, 1239.99/s  (0.794s, 1290.42/s)  LR: 5.493e-05  Data: 0.009 (0.013)
Train: 518 [ 650/1251 ( 52%)]  Loss: 3.170 (2.88)  Time: 0.772s, 1325.78/s  (0.792s, 1292.33/s)  LR: 5.493e-05  Data: 0.010 (0.013)
Train: 518 [ 700/1251 ( 56%)]  Loss: 2.876 (2.88)  Time: 0.785s, 1304.03/s  (0.792s, 1293.47/s)  LR: 5.493e-05  Data: 0.009 (0.012)
Train: 518 [ 750/1251 ( 60%)]  Loss: 2.913 (2.88)  Time: 0.771s, 1327.95/s  (0.791s, 1294.52/s)  LR: 5.493e-05  Data: 0.010 (0.012)
Train: 518 [ 800/1251 ( 64%)]  Loss: 3.212 (2.90)  Time: 0.773s, 1324.73/s  (0.791s, 1294.69/s)  LR: 5.493e-05  Data: 0.010 (0.012)
Train: 518 [ 850/1251 ( 68%)]  Loss: 3.186 (2.92)  Time: 0.839s, 1219.93/s  (0.790s, 1295.82/s)  LR: 5.493e-05  Data: 0.010 (0.012)
Train: 518 [ 900/1251 ( 72%)]  Loss: 2.942 (2.92)  Time: 0.790s, 1296.38/s  (0.790s, 1295.59/s)  LR: 5.493e-05  Data: 0.009 (0.012)
Train: 518 [ 950/1251 ( 76%)]  Loss: 3.498 (2.95)  Time: 0.817s, 1253.10/s  (0.791s, 1293.87/s)  LR: 5.493e-05  Data: 0.009 (0.012)
Train: 518 [1000/1251 ( 80%)]  Loss: 3.107 (2.96)  Time: 0.773s, 1324.75/s  (0.791s, 1294.36/s)  LR: 5.493e-05  Data: 0.010 (0.012)
Train: 518 [1050/1251 ( 84%)]  Loss: 2.783 (2.95)  Time: 0.774s, 1322.25/s  (0.791s, 1295.19/s)  LR: 5.493e-05  Data: 0.010 (0.012)
Train: 518 [1100/1251 ( 88%)]  Loss: 3.074 (2.95)  Time: 0.806s, 1270.48/s  (0.791s, 1294.58/s)  LR: 5.493e-05  Data: 0.009 (0.011)
Train: 518 [1150/1251 ( 92%)]  Loss: 3.142 (2.96)  Time: 0.777s, 1317.91/s  (0.792s, 1293.53/s)  LR: 5.493e-05  Data: 0.010 (0.011)
Train: 518 [1200/1251 ( 96%)]  Loss: 3.049 (2.96)  Time: 0.772s, 1326.76/s  (0.791s, 1293.91/s)  LR: 5.493e-05  Data: 0.009 (0.011)
Train: 518 [1250/1251 (100%)]  Loss: 3.110 (2.97)  Time: 0.760s, 1347.26/s  (0.792s, 1293.36/s)  LR: 5.493e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.539 (1.539)  Loss:  0.5845 (0.5845)  Acc@1: 92.2852 (92.2852)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.194 (0.561)  Loss:  0.6631 (1.0323)  Acc@1: 87.2642 (80.4100)  Acc@5: 98.1132 (95.3660)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-518.pth.tar', 80.41000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-515.pth.tar', 80.35600005126953)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-513.pth.tar', 80.33199992675782)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-516.pth.tar', 80.24799987060547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-507.pth.tar', 80.24400008056641)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-502.pth.tar', 80.24199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-511.pth.tar', 80.22799995361328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-509.pth.tar', 80.19999994873046)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-506.pth.tar', 80.15199987304688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-517.pth.tar', 80.14599994873046)

Train: 519 [   0/1251 (  0%)]  Loss: 3.204 (3.20)  Time: 2.204s,  464.55/s  (2.204s,  464.55/s)  LR: 5.386e-05  Data: 1.475 (1.475)
Train: 519 [  50/1251 (  4%)]  Loss: 2.816 (3.01)  Time: 0.776s, 1320.38/s  (0.824s, 1242.73/s)  LR: 5.386e-05  Data: 0.010 (0.042)
Train: 519 [ 100/1251 (  8%)]  Loss: 3.102 (3.04)  Time: 0.808s, 1266.83/s  (0.809s, 1265.02/s)  LR: 5.386e-05  Data: 0.010 (0.026)
Train: 519 [ 150/1251 ( 12%)]  Loss: 3.044 (3.04)  Time: 0.773s, 1325.07/s  (0.802s, 1276.30/s)  LR: 5.386e-05  Data: 0.010 (0.021)
Train: 519 [ 200/1251 ( 16%)]  Loss: 2.961 (3.03)  Time: 0.816s, 1255.52/s  (0.803s, 1275.72/s)  LR: 5.386e-05  Data: 0.012 (0.018)
Train: 519 [ 250/1251 ( 20%)]  Loss: 2.958 (3.01)  Time: 0.776s, 1319.19/s  (0.803s, 1275.72/s)  LR: 5.386e-05  Data: 0.010 (0.017)
Train: 519 [ 300/1251 ( 24%)]  Loss: 2.927 (3.00)  Time: 0.809s, 1266.06/s  (0.799s, 1281.74/s)  LR: 5.386e-05  Data: 0.010 (0.015)
Train: 519 [ 350/1251 ( 28%)]  Loss: 3.071 (3.01)  Time: 0.788s, 1299.36/s  (0.796s, 1285.64/s)  LR: 5.386e-05  Data: 0.010 (0.015)
Train: 519 [ 400/1251 ( 32%)]  Loss: 2.870 (2.99)  Time: 0.786s, 1302.61/s  (0.795s, 1288.54/s)  LR: 5.386e-05  Data: 0.010 (0.014)
Train: 519 [ 450/1251 ( 36%)]  Loss: 2.806 (2.98)  Time: 0.777s, 1318.65/s  (0.795s, 1287.87/s)  LR: 5.386e-05  Data: 0.009 (0.014)
Train: 519 [ 500/1251 ( 40%)]  Loss: 2.625 (2.94)  Time: 0.789s, 1297.63/s  (0.795s, 1287.93/s)  LR: 5.386e-05  Data: 0.010 (0.013)
Train: 519 [ 550/1251 ( 44%)]  Loss: 3.170 (2.96)  Time: 0.786s, 1303.12/s  (0.796s, 1287.22/s)  LR: 5.386e-05  Data: 0.010 (0.013)
Train: 519 [ 600/1251 ( 48%)]  Loss: 3.015 (2.97)  Time: 0.779s, 1314.32/s  (0.795s, 1288.68/s)  LR: 5.386e-05  Data: 0.010 (0.013)
Train: 519 [ 650/1251 ( 52%)]  Loss: 3.258 (2.99)  Time: 0.775s, 1322.02/s  (0.793s, 1290.76/s)  LR: 5.386e-05  Data: 0.011 (0.013)
Train: 519 [ 700/1251 ( 56%)]  Loss: 3.010 (2.99)  Time: 0.778s, 1317.03/s  (0.792s, 1292.28/s)  LR: 5.386e-05  Data: 0.010 (0.012)
Train: 519 [ 750/1251 ( 60%)]  Loss: 3.173 (3.00)  Time: 0.862s, 1187.80/s  (0.792s, 1293.01/s)  LR: 5.386e-05  Data: 0.010 (0.012)
Train: 519 [ 800/1251 ( 64%)]  Loss: 2.924 (3.00)  Time: 0.779s, 1315.31/s  (0.792s, 1293.46/s)  LR: 5.386e-05  Data: 0.010 (0.012)
Train: 519 [ 850/1251 ( 68%)]  Loss: 2.922 (2.99)  Time: 0.823s, 1244.29/s  (0.791s, 1293.87/s)  LR: 5.386e-05  Data: 0.010 (0.012)
Train: 519 [ 900/1251 ( 72%)]  Loss: 3.200 (3.00)  Time: 0.784s, 1305.44/s  (0.792s, 1293.23/s)  LR: 5.386e-05  Data: 0.010 (0.012)
Train: 519 [ 950/1251 ( 76%)]  Loss: 3.161 (3.01)  Time: 0.773s, 1325.44/s  (0.791s, 1293.89/s)  LR: 5.386e-05  Data: 0.010 (0.012)
Train: 519 [1000/1251 ( 80%)]  Loss: 3.213 (3.02)  Time: 0.809s, 1266.37/s  (0.791s, 1294.21/s)  LR: 5.386e-05  Data: 0.010 (0.012)
Train: 519 [1050/1251 ( 84%)]  Loss: 2.979 (3.02)  Time: 0.807s, 1268.38/s  (0.791s, 1294.50/s)  LR: 5.386e-05  Data: 0.010 (0.012)
Train: 519 [1100/1251 ( 88%)]  Loss: 3.012 (3.02)  Time: 0.773s, 1325.20/s  (0.791s, 1294.21/s)  LR: 5.386e-05  Data: 0.010 (0.012)
Train: 519 [1150/1251 ( 92%)]  Loss: 3.021 (3.02)  Time: 0.787s, 1301.75/s  (0.791s, 1294.62/s)  LR: 5.386e-05  Data: 0.009 (0.011)
Train: 519 [1200/1251 ( 96%)]  Loss: 2.891 (3.01)  Time: 0.805s, 1272.31/s  (0.791s, 1293.89/s)  LR: 5.386e-05  Data: 0.010 (0.011)
Train: 519 [1250/1251 (100%)]  Loss: 2.775 (3.00)  Time: 0.760s, 1348.20/s  (0.791s, 1294.37/s)  LR: 5.386e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.605 (1.605)  Loss:  0.6289 (0.6289)  Acc@1: 91.6016 (91.6016)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.194 (0.574)  Loss:  0.7368 (1.0625)  Acc@1: 87.0283 (80.3680)  Acc@5: 97.9953 (95.2660)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-518.pth.tar', 80.41000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-519.pth.tar', 80.36799992431641)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-515.pth.tar', 80.35600005126953)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-513.pth.tar', 80.33199992675782)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-516.pth.tar', 80.24799987060547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-507.pth.tar', 80.24400008056641)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-502.pth.tar', 80.24199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-511.pth.tar', 80.22799995361328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-509.pth.tar', 80.19999994873046)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-506.pth.tar', 80.15199987304688)

Train: 520 [   0/1251 (  0%)]  Loss: 2.787 (2.79)  Time: 2.250s,  455.18/s  (2.250s,  455.18/s)  LR: 5.279e-05  Data: 1.519 (1.519)
Train: 520 [  50/1251 (  4%)]  Loss: 2.941 (2.86)  Time: 0.807s, 1268.49/s  (0.811s, 1263.33/s)  LR: 5.279e-05  Data: 0.009 (0.043)
Train: 520 [ 100/1251 (  8%)]  Loss: 2.810 (2.85)  Time: 0.809s, 1265.74/s  (0.802s, 1276.18/s)  LR: 5.279e-05  Data: 0.010 (0.027)
Train: 520 [ 150/1251 ( 12%)]  Loss: 3.100 (2.91)  Time: 0.814s, 1257.79/s  (0.805s, 1271.97/s)  LR: 5.279e-05  Data: 0.011 (0.022)
Train: 520 [ 200/1251 ( 16%)]  Loss: 2.695 (2.87)  Time: 0.815s, 1256.34/s  (0.809s, 1266.44/s)  LR: 5.279e-05  Data: 0.011 (0.019)
Train: 520 [ 250/1251 ( 20%)]  Loss: 2.830 (2.86)  Time: 0.807s, 1269.34/s  (0.808s, 1266.98/s)  LR: 5.279e-05  Data: 0.009 (0.017)
Train: 520 [ 300/1251 ( 24%)]  Loss: 2.932 (2.87)  Time: 0.807s, 1269.09/s  (0.806s, 1270.02/s)  LR: 5.279e-05  Data: 0.010 (0.016)
Train: 520 [ 350/1251 ( 28%)]  Loss: 2.929 (2.88)  Time: 0.807s, 1268.31/s  (0.805s, 1271.66/s)  LR: 5.279e-05  Data: 0.010 (0.015)
Train: 520 [ 400/1251 ( 32%)]  Loss: 2.967 (2.89)  Time: 0.807s, 1269.11/s  (0.804s, 1274.00/s)  LR: 5.279e-05  Data: 0.010 (0.015)
Train: 520 [ 450/1251 ( 36%)]  Loss: 2.917 (2.89)  Time: 0.776s, 1320.27/s  (0.804s, 1273.09/s)  LR: 5.279e-05  Data: 0.010 (0.014)
Train: 520 [ 500/1251 ( 40%)]  Loss: 2.892 (2.89)  Time: 0.808s, 1266.80/s  (0.804s, 1272.95/s)  LR: 5.279e-05  Data: 0.010 (0.014)
Train: 520 [ 550/1251 ( 44%)]  Loss: 3.140 (2.91)  Time: 0.774s, 1323.78/s  (0.804s, 1273.94/s)  LR: 5.279e-05  Data: 0.010 (0.013)
Train: 520 [ 600/1251 ( 48%)]  Loss: 3.181 (2.93)  Time: 0.807s, 1268.20/s  (0.802s, 1276.74/s)  LR: 5.279e-05  Data: 0.010 (0.013)
Train: 520 [ 650/1251 ( 52%)]  Loss: 3.014 (2.94)  Time: 0.771s, 1327.80/s  (0.802s, 1277.50/s)  LR: 5.279e-05  Data: 0.009 (0.013)
Train: 520 [ 700/1251 ( 56%)]  Loss: 3.123 (2.95)  Time: 0.803s, 1275.80/s  (0.800s, 1279.41/s)  LR: 5.279e-05  Data: 0.010 (0.013)
Train: 520 [ 750/1251 ( 60%)]  Loss: 3.021 (2.95)  Time: 0.811s, 1263.16/s  (0.801s, 1278.18/s)  LR: 5.279e-05  Data: 0.009 (0.012)
Train: 520 [ 800/1251 ( 64%)]  Loss: 2.755 (2.94)  Time: 0.845s, 1212.14/s  (0.801s, 1278.78/s)  LR: 5.279e-05  Data: 0.010 (0.012)
Train: 520 [ 850/1251 ( 68%)]  Loss: 2.727 (2.93)  Time: 0.771s, 1327.32/s  (0.800s, 1280.18/s)  LR: 5.279e-05  Data: 0.009 (0.012)
Train: 520 [ 900/1251 ( 72%)]  Loss: 3.030 (2.94)  Time: 0.812s, 1260.42/s  (0.799s, 1282.10/s)  LR: 5.279e-05  Data: 0.009 (0.012)
Train: 520 [ 950/1251 ( 76%)]  Loss: 2.811 (2.93)  Time: 0.814s, 1257.37/s  (0.799s, 1281.90/s)  LR: 5.279e-05  Data: 0.010 (0.012)
Train: 520 [1000/1251 ( 80%)]  Loss: 2.691 (2.92)  Time: 0.774s, 1323.21/s  (0.798s, 1282.83/s)  LR: 5.279e-05  Data: 0.010 (0.012)
Train: 520 [1050/1251 ( 84%)]  Loss: 3.085 (2.93)  Time: 0.815s, 1256.30/s  (0.798s, 1283.04/s)  LR: 5.279e-05  Data: 0.011 (0.012)
Train: 520 [1100/1251 ( 88%)]  Loss: 2.922 (2.93)  Time: 0.808s, 1267.63/s  (0.798s, 1283.65/s)  LR: 5.279e-05  Data: 0.010 (0.012)
Train: 520 [1150/1251 ( 92%)]  Loss: 3.143 (2.94)  Time: 0.817s, 1253.43/s  (0.797s, 1284.38/s)  LR: 5.279e-05  Data: 0.012 (0.012)
Train: 520 [1200/1251 ( 96%)]  Loss: 3.221 (2.95)  Time: 0.772s, 1326.61/s  (0.798s, 1283.28/s)  LR: 5.279e-05  Data: 0.010 (0.012)
Train: 520 [1250/1251 (100%)]  Loss: 2.868 (2.94)  Time: 0.761s, 1345.26/s  (0.798s, 1283.42/s)  LR: 5.279e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.548 (1.548)  Loss:  0.6460 (0.6460)  Acc@1: 91.4062 (91.4062)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.194 (0.564)  Loss:  0.7173 (1.0527)  Acc@1: 87.5000 (80.3340)  Acc@5: 98.3491 (95.2780)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-518.pth.tar', 80.41000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-519.pth.tar', 80.36799992431641)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-515.pth.tar', 80.35600005126953)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-520.pth.tar', 80.33400012939452)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-513.pth.tar', 80.33199992675782)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-516.pth.tar', 80.24799987060547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-507.pth.tar', 80.24400008056641)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-502.pth.tar', 80.24199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-511.pth.tar', 80.22799995361328)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-509.pth.tar', 80.19999994873046)

Train: 521 [   0/1251 (  0%)]  Loss: 2.783 (2.78)  Time: 2.278s,  449.49/s  (2.278s,  449.49/s)  LR: 5.175e-05  Data: 1.548 (1.548)
Train: 521 [  50/1251 (  4%)]  Loss: 3.083 (2.93)  Time: 0.784s, 1306.58/s  (0.817s, 1253.40/s)  LR: 5.175e-05  Data: 0.012 (0.044)
Train: 521 [ 100/1251 (  8%)]  Loss: 3.080 (2.98)  Time: 0.775s, 1321.07/s  (0.802s, 1277.11/s)  LR: 5.175e-05  Data: 0.010 (0.027)
Train: 521 [ 150/1251 ( 12%)]  Loss: 2.623 (2.89)  Time: 0.781s, 1311.57/s  (0.800s, 1279.43/s)  LR: 5.175e-05  Data: 0.010 (0.021)
Train: 521 [ 200/1251 ( 16%)]  Loss: 2.390 (2.79)  Time: 0.774s, 1323.50/s  (0.797s, 1285.11/s)  LR: 5.175e-05  Data: 0.010 (0.018)
Train: 521 [ 250/1251 ( 20%)]  Loss: 2.803 (2.79)  Time: 0.809s, 1266.24/s  (0.793s, 1290.65/s)  LR: 5.175e-05  Data: 0.011 (0.017)
Train: 521 [ 300/1251 ( 24%)]  Loss: 2.711 (2.78)  Time: 0.814s, 1257.90/s  (0.795s, 1288.29/s)  LR: 5.175e-05  Data: 0.011 (0.016)
Train: 521 [ 350/1251 ( 28%)]  Loss: 2.772 (2.78)  Time: 0.782s, 1308.66/s  (0.797s, 1285.29/s)  LR: 5.175e-05  Data: 0.010 (0.015)
Train: 521 [ 400/1251 ( 32%)]  Loss: 3.194 (2.83)  Time: 0.778s, 1315.80/s  (0.797s, 1285.52/s)  LR: 5.175e-05  Data: 0.010 (0.014)
Train: 521 [ 450/1251 ( 36%)]  Loss: 3.155 (2.86)  Time: 0.776s, 1319.22/s  (0.796s, 1286.50/s)  LR: 5.175e-05  Data: 0.009 (0.014)
Train: 521 [ 500/1251 ( 40%)]  Loss: 3.234 (2.89)  Time: 0.786s, 1302.28/s  (0.795s, 1288.23/s)  LR: 5.175e-05  Data: 0.009 (0.013)
Train: 521 [ 550/1251 ( 44%)]  Loss: 3.312 (2.93)  Time: 0.827s, 1238.67/s  (0.795s, 1288.80/s)  LR: 5.175e-05  Data: 0.009 (0.013)
Train: 521 [ 600/1251 ( 48%)]  Loss: 2.789 (2.92)  Time: 0.806s, 1270.37/s  (0.795s, 1288.76/s)  LR: 5.175e-05  Data: 0.010 (0.013)
Train: 521 [ 650/1251 ( 52%)]  Loss: 2.862 (2.91)  Time: 0.817s, 1253.27/s  (0.795s, 1288.18/s)  LR: 5.175e-05  Data: 0.011 (0.013)
Train: 521 [ 700/1251 ( 56%)]  Loss: 3.190 (2.93)  Time: 0.775s, 1322.09/s  (0.794s, 1289.63/s)  LR: 5.175e-05  Data: 0.010 (0.012)
Train: 521 [ 750/1251 ( 60%)]  Loss: 3.196 (2.95)  Time: 0.781s, 1310.59/s  (0.794s, 1290.21/s)  LR: 5.175e-05  Data: 0.010 (0.012)
Train: 521 [ 800/1251 ( 64%)]  Loss: 3.038 (2.95)  Time: 0.774s, 1322.24/s  (0.793s, 1291.32/s)  LR: 5.175e-05  Data: 0.010 (0.012)
Train: 521 [ 850/1251 ( 68%)]  Loss: 2.922 (2.95)  Time: 0.808s, 1266.64/s  (0.793s, 1291.06/s)  LR: 5.175e-05  Data: 0.009 (0.012)
Train: 521 [ 900/1251 ( 72%)]  Loss: 3.195 (2.96)  Time: 0.773s, 1324.05/s  (0.794s, 1290.36/s)  LR: 5.175e-05  Data: 0.010 (0.012)
Train: 521 [ 950/1251 ( 76%)]  Loss: 2.961 (2.96)  Time: 0.819s, 1250.02/s  (0.793s, 1290.56/s)  LR: 5.175e-05  Data: 0.010 (0.012)
Train: 521 [1000/1251 ( 80%)]  Loss: 2.958 (2.96)  Time: 0.787s, 1300.61/s  (0.793s, 1291.70/s)  LR: 5.175e-05  Data: 0.010 (0.012)
Train: 521 [1050/1251 ( 84%)]  Loss: 2.691 (2.95)  Time: 0.783s, 1307.47/s  (0.793s, 1291.58/s)  LR: 5.175e-05  Data: 0.009 (0.011)
Train: 521 [1100/1251 ( 88%)]  Loss: 2.888 (2.95)  Time: 0.772s, 1326.26/s  (0.793s, 1291.97/s)  LR: 5.175e-05  Data: 0.010 (0.011)
Train: 521 [1150/1251 ( 92%)]  Loss: 2.793 (2.94)  Time: 0.778s, 1316.25/s  (0.792s, 1292.21/s)  LR: 5.175e-05  Data: 0.010 (0.011)
Train: 521 [1200/1251 ( 96%)]  Loss: 2.555 (2.93)  Time: 0.817s, 1252.87/s  (0.793s, 1291.45/s)  LR: 5.175e-05  Data: 0.011 (0.011)
Train: 521 [1250/1251 (100%)]  Loss: 2.833 (2.92)  Time: 0.804s, 1273.94/s  (0.793s, 1291.42/s)  LR: 5.175e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.544 (1.544)  Loss:  0.6489 (0.6489)  Acc@1: 91.8945 (91.8945)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.193 (0.567)  Loss:  0.7202 (1.0625)  Acc@1: 86.7924 (80.3540)  Acc@5: 98.2311 (95.3500)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-518.pth.tar', 80.41000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-519.pth.tar', 80.36799992431641)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-515.pth.tar', 80.35600005126953)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-521.pth.tar', 80.35399995117187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-520.pth.tar', 80.33400012939452)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-513.pth.tar', 80.33199992675782)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-516.pth.tar', 80.24799987060547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-507.pth.tar', 80.24400008056641)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-502.pth.tar', 80.24199997802734)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-511.pth.tar', 80.22799995361328)

Train: 522 [   0/1251 (  0%)]  Loss: 2.981 (2.98)  Time: 2.614s,  391.73/s  (2.614s,  391.73/s)  LR: 5.071e-05  Data: 1.883 (1.883)
Train: 522 [  50/1251 (  4%)]  Loss: 2.919 (2.95)  Time: 0.821s, 1246.56/s  (0.818s, 1252.25/s)  LR: 5.071e-05  Data: 0.009 (0.047)
Train: 522 [ 100/1251 (  8%)]  Loss: 3.207 (3.04)  Time: 0.770s, 1329.59/s  (0.809s, 1265.79/s)  LR: 5.071e-05  Data: 0.010 (0.029)
Train: 522 [ 150/1251 ( 12%)]  Loss: 3.081 (3.05)  Time: 0.774s, 1323.46/s  (0.804s, 1273.97/s)  LR: 5.071e-05  Data: 0.010 (0.022)
Train: 522 [ 200/1251 ( 16%)]  Loss: 3.081 (3.05)  Time: 0.781s, 1310.64/s  (0.801s, 1279.16/s)  LR: 5.071e-05  Data: 0.009 (0.019)
Train: 522 [ 250/1251 ( 20%)]  Loss: 2.895 (3.03)  Time: 0.773s, 1324.75/s  (0.797s, 1285.58/s)  LR: 5.071e-05  Data: 0.009 (0.017)
Train: 522 [ 300/1251 ( 24%)]  Loss: 3.131 (3.04)  Time: 0.784s, 1305.66/s  (0.794s, 1289.52/s)  LR: 5.071e-05  Data: 0.009 (0.016)
Train: 522 [ 350/1251 ( 28%)]  Loss: 3.015 (3.04)  Time: 0.780s, 1312.98/s  (0.793s, 1292.08/s)  LR: 5.071e-05  Data: 0.010 (0.015)
Train: 522 [ 400/1251 ( 32%)]  Loss: 3.070 (3.04)  Time: 0.807s, 1269.62/s  (0.791s, 1294.18/s)  LR: 5.071e-05  Data: 0.010 (0.015)
Train: 522 [ 450/1251 ( 36%)]  Loss: 3.344 (3.07)  Time: 0.773s, 1324.53/s  (0.790s, 1295.64/s)  LR: 5.071e-05  Data: 0.009 (0.014)
Train: 522 [ 500/1251 ( 40%)]  Loss: 2.845 (3.05)  Time: 0.773s, 1324.41/s  (0.791s, 1294.05/s)  LR: 5.071e-05  Data: 0.010 (0.014)
Train: 522 [ 550/1251 ( 44%)]  Loss: 2.878 (3.04)  Time: 0.774s, 1322.44/s  (0.792s, 1293.02/s)  LR: 5.071e-05  Data: 0.011 (0.013)
Train: 522 [ 600/1251 ( 48%)]  Loss: 2.930 (3.03)  Time: 0.815s, 1256.37/s  (0.792s, 1293.16/s)  LR: 5.071e-05  Data: 0.011 (0.013)
Train: 522 [ 650/1251 ( 52%)]  Loss: 3.016 (3.03)  Time: 0.785s, 1304.64/s  (0.792s, 1292.46/s)  LR: 5.071e-05  Data: 0.009 (0.013)
Train: 522 [ 700/1251 ( 56%)]  Loss: 2.871 (3.02)  Time: 0.775s, 1321.05/s  (0.792s, 1293.44/s)  LR: 5.071e-05  Data: 0.010 (0.013)
Train: 522 [ 750/1251 ( 60%)]  Loss: 2.937 (3.01)  Time: 0.809s, 1265.17/s  (0.792s, 1292.14/s)  LR: 5.071e-05  Data: 0.010 (0.012)
Train: 522 [ 800/1251 ( 64%)]  Loss: 2.866 (3.00)  Time: 0.816s, 1255.43/s  (0.793s, 1291.51/s)  LR: 5.071e-05  Data: 0.011 (0.012)
Train: 522 [ 850/1251 ( 68%)]  Loss: 2.616 (2.98)  Time: 0.814s, 1257.67/s  (0.793s, 1290.58/s)  LR: 5.071e-05  Data: 0.011 (0.012)
Train: 522 [ 900/1251 ( 72%)]  Loss: 3.364 (3.00)  Time: 0.791s, 1294.84/s  (0.795s, 1288.76/s)  LR: 5.071e-05  Data: 0.009 (0.012)
Train: 522 [ 950/1251 ( 76%)]  Loss: 3.066 (3.01)  Time: 0.772s, 1326.08/s  (0.796s, 1287.20/s)  LR: 5.071e-05  Data: 0.010 (0.012)
Train: 522 [1000/1251 ( 80%)]  Loss: 2.967 (3.00)  Time: 0.811s, 1263.14/s  (0.795s, 1287.56/s)  LR: 5.071e-05  Data: 0.010 (0.012)
Train: 522 [1050/1251 ( 84%)]  Loss: 2.842 (3.00)  Time: 0.812s, 1260.68/s  (0.796s, 1286.77/s)  LR: 5.071e-05  Data: 0.010 (0.012)
Train: 522 [1100/1251 ( 88%)]  Loss: 2.641 (2.98)  Time: 0.777s, 1318.21/s  (0.795s, 1287.81/s)  LR: 5.071e-05  Data: 0.010 (0.012)
Train: 522 [1150/1251 ( 92%)]  Loss: 2.985 (2.98)  Time: 0.815s, 1257.21/s  (0.795s, 1287.65/s)  LR: 5.071e-05  Data: 0.011 (0.012)
Train: 522 [1200/1251 ( 96%)]  Loss: 3.315 (2.99)  Time: 0.779s, 1313.93/s  (0.796s, 1285.98/s)  LR: 5.071e-05  Data: 0.010 (0.012)
Train: 522 [1250/1251 (100%)]  Loss: 2.674 (2.98)  Time: 0.801s, 1277.87/s  (0.796s, 1285.92/s)  LR: 5.071e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.557 (1.557)  Loss:  0.5894 (0.5894)  Acc@1: 91.8945 (91.8945)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.194 (0.571)  Loss:  0.7178 (1.0196)  Acc@1: 87.0283 (80.4500)  Acc@5: 97.6415 (95.2900)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-522.pth.tar', 80.45000005371094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-518.pth.tar', 80.41000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-519.pth.tar', 80.36799992431641)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-515.pth.tar', 80.35600005126953)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-521.pth.tar', 80.35399995117187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-520.pth.tar', 80.33400012939452)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-513.pth.tar', 80.33199992675782)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-516.pth.tar', 80.24799987060547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-507.pth.tar', 80.24400008056641)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-502.pth.tar', 80.24199997802734)

Train: 523 [   0/1251 (  0%)]  Loss: 3.203 (3.20)  Time: 2.179s,  469.85/s  (2.179s,  469.85/s)  LR: 4.969e-05  Data: 1.466 (1.466)
Train: 523 [  50/1251 (  4%)]  Loss: 3.107 (3.16)  Time: 0.774s, 1323.51/s  (0.815s, 1256.37/s)  LR: 4.969e-05  Data: 0.009 (0.040)
Train: 523 [ 100/1251 (  8%)]  Loss: 3.279 (3.20)  Time: 0.807s, 1268.69/s  (0.804s, 1274.39/s)  LR: 4.969e-05  Data: 0.010 (0.025)
Train: 523 [ 150/1251 ( 12%)]  Loss: 3.053 (3.16)  Time: 0.866s, 1182.83/s  (0.797s, 1284.16/s)  LR: 4.969e-05  Data: 0.009 (0.020)
Train: 523 [ 200/1251 ( 16%)]  Loss: 2.919 (3.11)  Time: 0.778s, 1316.95/s  (0.797s, 1284.99/s)  LR: 4.969e-05  Data: 0.010 (0.018)
Train: 523 [ 250/1251 ( 20%)]  Loss: 3.206 (3.13)  Time: 0.809s, 1265.38/s  (0.797s, 1284.16/s)  LR: 4.969e-05  Data: 0.010 (0.016)
Train: 523 [ 300/1251 ( 24%)]  Loss: 2.996 (3.11)  Time: 0.785s, 1303.96/s  (0.798s, 1282.64/s)  LR: 4.969e-05  Data: 0.009 (0.015)
Train: 523 [ 350/1251 ( 28%)]  Loss: 3.020 (3.10)  Time: 0.782s, 1309.96/s  (0.797s, 1285.17/s)  LR: 4.969e-05  Data: 0.010 (0.014)
Train: 523 [ 400/1251 ( 32%)]  Loss: 3.331 (3.12)  Time: 0.777s, 1318.44/s  (0.795s, 1287.93/s)  LR: 4.969e-05  Data: 0.010 (0.014)
Train: 523 [ 450/1251 ( 36%)]  Loss: 2.999 (3.11)  Time: 0.788s, 1299.59/s  (0.794s, 1289.56/s)  LR: 4.969e-05  Data: 0.012 (0.013)
Train: 523 [ 500/1251 ( 40%)]  Loss: 2.938 (3.10)  Time: 0.807s, 1269.21/s  (0.795s, 1288.11/s)  LR: 4.969e-05  Data: 0.010 (0.013)
Train: 523 [ 550/1251 ( 44%)]  Loss: 3.073 (3.09)  Time: 0.817s, 1252.83/s  (0.795s, 1287.26/s)  LR: 4.969e-05  Data: 0.009 (0.013)
Train: 523 [ 600/1251 ( 48%)]  Loss: 3.211 (3.10)  Time: 0.831s, 1232.67/s  (0.795s, 1287.30/s)  LR: 4.969e-05  Data: 0.017 (0.012)
Train: 523 [ 650/1251 ( 52%)]  Loss: 2.911 (3.09)  Time: 0.776s, 1318.90/s  (0.795s, 1288.07/s)  LR: 4.969e-05  Data: 0.009 (0.012)
Train: 523 [ 700/1251 ( 56%)]  Loss: 3.123 (3.09)  Time: 0.773s, 1324.14/s  (0.794s, 1289.09/s)  LR: 4.969e-05  Data: 0.010 (0.012)
Train: 523 [ 750/1251 ( 60%)]  Loss: 3.014 (3.09)  Time: 0.774s, 1322.75/s  (0.794s, 1290.08/s)  LR: 4.969e-05  Data: 0.010 (0.012)
Train: 523 [ 800/1251 ( 64%)]  Loss: 2.768 (3.07)  Time: 0.856s, 1196.19/s  (0.793s, 1291.03/s)  LR: 4.969e-05  Data: 0.009 (0.012)
Train: 523 [ 850/1251 ( 68%)]  Loss: 2.978 (3.06)  Time: 0.778s, 1316.13/s  (0.793s, 1291.14/s)  LR: 4.969e-05  Data: 0.010 (0.012)
Train: 523 [ 900/1251 ( 72%)]  Loss: 3.128 (3.07)  Time: 0.807s, 1268.71/s  (0.793s, 1291.53/s)  LR: 4.969e-05  Data: 0.010 (0.012)
Train: 523 [ 950/1251 ( 76%)]  Loss: 3.128 (3.07)  Time: 0.832s, 1230.73/s  (0.793s, 1290.59/s)  LR: 4.969e-05  Data: 0.015 (0.011)
Train: 523 [1000/1251 ( 80%)]  Loss: 3.181 (3.07)  Time: 0.773s, 1325.30/s  (0.794s, 1289.49/s)  LR: 4.969e-05  Data: 0.010 (0.011)
Train: 523 [1050/1251 ( 84%)]  Loss: 3.000 (3.07)  Time: 0.782s, 1308.68/s  (0.794s, 1289.47/s)  LR: 4.969e-05  Data: 0.010 (0.011)
Train: 523 [1100/1251 ( 88%)]  Loss: 3.106 (3.07)  Time: 0.776s, 1319.09/s  (0.794s, 1289.75/s)  LR: 4.969e-05  Data: 0.010 (0.011)
Train: 523 [1150/1251 ( 92%)]  Loss: 2.665 (3.06)  Time: 0.815s, 1256.23/s  (0.794s, 1289.93/s)  LR: 4.969e-05  Data: 0.009 (0.011)
Train: 523 [1200/1251 ( 96%)]  Loss: 3.037 (3.05)  Time: 0.773s, 1324.07/s  (0.794s, 1290.23/s)  LR: 4.969e-05  Data: 0.010 (0.011)
Train: 523 [1250/1251 (100%)]  Loss: 2.941 (3.05)  Time: 0.766s, 1336.46/s  (0.793s, 1290.51/s)  LR: 4.969e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.553 (1.553)  Loss:  0.6118 (0.6118)  Acc@1: 92.3828 (92.3828)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.194 (0.567)  Loss:  0.6909 (1.0464)  Acc@1: 87.0283 (80.4180)  Acc@5: 97.9953 (95.3040)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-522.pth.tar', 80.45000005371094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-523.pth.tar', 80.4179999243164)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-518.pth.tar', 80.41000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-519.pth.tar', 80.36799992431641)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-515.pth.tar', 80.35600005126953)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-521.pth.tar', 80.35399995117187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-520.pth.tar', 80.33400012939452)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-513.pth.tar', 80.33199992675782)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-516.pth.tar', 80.24799987060547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-507.pth.tar', 80.24400008056641)

Train: 524 [   0/1251 (  0%)]  Loss: 3.218 (3.22)  Time: 2.325s,  440.48/s  (2.325s,  440.48/s)  LR: 4.868e-05  Data: 1.535 (1.535)
Train: 524 [  50/1251 (  4%)]  Loss: 3.000 (3.11)  Time: 0.807s, 1268.20/s  (0.820s, 1249.43/s)  LR: 4.868e-05  Data: 0.010 (0.044)
Train: 524 [ 100/1251 (  8%)]  Loss: 3.294 (3.17)  Time: 0.776s, 1319.89/s  (0.807s, 1269.32/s)  LR: 4.868e-05  Data: 0.010 (0.027)
Train: 524 [ 150/1251 ( 12%)]  Loss: 2.603 (3.03)  Time: 0.772s, 1325.71/s  (0.798s, 1282.44/s)  LR: 4.868e-05  Data: 0.009 (0.021)
Train: 524 [ 200/1251 ( 16%)]  Loss: 2.750 (2.97)  Time: 0.772s, 1326.10/s  (0.796s, 1287.17/s)  LR: 4.868e-05  Data: 0.010 (0.018)
Train: 524 [ 250/1251 ( 20%)]  Loss: 2.881 (2.96)  Time: 0.776s, 1319.08/s  (0.794s, 1290.20/s)  LR: 4.868e-05  Data: 0.009 (0.017)
Train: 524 [ 300/1251 ( 24%)]  Loss: 3.047 (2.97)  Time: 0.772s, 1326.53/s  (0.793s, 1291.67/s)  LR: 4.868e-05  Data: 0.010 (0.016)
Train: 524 [ 350/1251 ( 28%)]  Loss: 3.102 (2.99)  Time: 0.807s, 1268.32/s  (0.792s, 1292.17/s)  LR: 4.868e-05  Data: 0.009 (0.015)
Train: 524 [ 400/1251 ( 32%)]  Loss: 2.935 (2.98)  Time: 0.775s, 1321.07/s  (0.793s, 1291.69/s)  LR: 4.868e-05  Data: 0.009 (0.014)
Train: 524 [ 450/1251 ( 36%)]  Loss: 3.048 (2.99)  Time: 0.809s, 1266.12/s  (0.793s, 1291.39/s)  LR: 4.868e-05  Data: 0.009 (0.014)
Train: 524 [ 500/1251 ( 40%)]  Loss: 2.854 (2.98)  Time: 0.807s, 1269.42/s  (0.793s, 1292.06/s)  LR: 4.868e-05  Data: 0.009 (0.013)
Train: 524 [ 550/1251 ( 44%)]  Loss: 2.953 (2.97)  Time: 0.773s, 1325.20/s  (0.792s, 1293.34/s)  LR: 4.868e-05  Data: 0.010 (0.013)
Train: 524 [ 600/1251 ( 48%)]  Loss: 3.024 (2.98)  Time: 0.772s, 1325.60/s  (0.791s, 1295.30/s)  LR: 4.868e-05  Data: 0.010 (0.013)
Train: 524 [ 650/1251 ( 52%)]  Loss: 2.912 (2.97)  Time: 0.772s, 1326.03/s  (0.791s, 1294.40/s)  LR: 4.868e-05  Data: 0.010 (0.012)
Train: 524 [ 700/1251 ( 56%)]  Loss: 3.261 (2.99)  Time: 0.784s, 1306.84/s  (0.790s, 1295.79/s)  LR: 4.868e-05  Data: 0.010 (0.012)
Train: 524 [ 750/1251 ( 60%)]  Loss: 3.088 (3.00)  Time: 0.774s, 1323.56/s  (0.791s, 1294.66/s)  LR: 4.868e-05  Data: 0.010 (0.012)
Train: 524 [ 800/1251 ( 64%)]  Loss: 2.963 (3.00)  Time: 0.788s, 1299.98/s  (0.792s, 1293.35/s)  LR: 4.868e-05  Data: 0.009 (0.012)
Train: 524 [ 850/1251 ( 68%)]  Loss: 3.131 (3.00)  Time: 0.816s, 1255.53/s  (0.792s, 1292.72/s)  LR: 4.868e-05  Data: 0.011 (0.012)
Train: 524 [ 900/1251 ( 72%)]  Loss: 2.704 (2.99)  Time: 0.771s, 1327.38/s  (0.792s, 1292.83/s)  LR: 4.868e-05  Data: 0.010 (0.012)
Train: 524 [ 950/1251 ( 76%)]  Loss: 2.952 (2.99)  Time: 0.837s, 1222.98/s  (0.793s, 1291.87/s)  LR: 4.868e-05  Data: 0.010 (0.012)
Train: 524 [1000/1251 ( 80%)]  Loss: 2.991 (2.99)  Time: 0.778s, 1315.59/s  (0.793s, 1292.07/s)  LR: 4.868e-05  Data: 0.009 (0.012)
Train: 524 [1050/1251 ( 84%)]  Loss: 2.925 (2.98)  Time: 0.810s, 1264.83/s  (0.793s, 1292.08/s)  LR: 4.868e-05  Data: 0.010 (0.011)
Train: 524 [1100/1251 ( 88%)]  Loss: 2.624 (2.97)  Time: 0.806s, 1270.50/s  (0.793s, 1292.10/s)  LR: 4.868e-05  Data: 0.009 (0.011)
Train: 524 [1150/1251 ( 92%)]  Loss: 3.150 (2.98)  Time: 0.826s, 1240.09/s  (0.793s, 1290.67/s)  LR: 4.868e-05  Data: 0.010 (0.011)
Train: 524 [1200/1251 ( 96%)]  Loss: 2.932 (2.97)  Time: 0.823s, 1244.80/s  (0.793s, 1291.08/s)  LR: 4.868e-05  Data: 0.011 (0.011)
Train: 524 [1250/1251 (100%)]  Loss: 2.752 (2.97)  Time: 0.762s, 1343.27/s  (0.793s, 1291.85/s)  LR: 4.868e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.558 (1.558)  Loss:  0.5713 (0.5713)  Acc@1: 91.8945 (91.8945)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.194 (0.585)  Loss:  0.6523 (1.0028)  Acc@1: 87.1462 (80.4180)  Acc@5: 97.8774 (95.3600)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-522.pth.tar', 80.45000005371094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-524.pth.tar', 80.41799997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-523.pth.tar', 80.4179999243164)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-518.pth.tar', 80.41000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-519.pth.tar', 80.36799992431641)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-515.pth.tar', 80.35600005126953)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-521.pth.tar', 80.35399995117187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-520.pth.tar', 80.33400012939452)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-513.pth.tar', 80.33199992675782)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-516.pth.tar', 80.24799987060547)

Train: 525 [   0/1251 (  0%)]  Loss: 2.686 (2.69)  Time: 2.436s,  420.41/s  (2.436s,  420.41/s)  LR: 4.768e-05  Data: 1.703 (1.703)
Train: 525 [  50/1251 (  4%)]  Loss: 3.144 (2.92)  Time: 0.856s, 1196.36/s  (0.829s, 1235.26/s)  LR: 4.768e-05  Data: 0.010 (0.046)
Train: 525 [ 100/1251 (  8%)]  Loss: 3.046 (2.96)  Time: 0.797s, 1284.46/s  (0.814s, 1257.61/s)  LR: 4.768e-05  Data: 0.016 (0.028)
Train: 525 [ 150/1251 ( 12%)]  Loss: 3.117 (3.00)  Time: 0.773s, 1324.83/s  (0.804s, 1273.61/s)  LR: 4.768e-05  Data: 0.010 (0.022)
Train: 525 [ 200/1251 ( 16%)]  Loss: 2.933 (2.99)  Time: 0.770s, 1329.42/s  (0.798s, 1283.03/s)  LR: 4.768e-05  Data: 0.009 (0.019)
Train: 525 [ 250/1251 ( 20%)]  Loss: 2.766 (2.95)  Time: 0.773s, 1324.84/s  (0.794s, 1289.33/s)  LR: 4.768e-05  Data: 0.010 (0.017)
Train: 525 [ 300/1251 ( 24%)]  Loss: 3.002 (2.96)  Time: 0.777s, 1318.26/s  (0.794s, 1290.39/s)  LR: 4.768e-05  Data: 0.010 (0.016)
Train: 525 [ 350/1251 ( 28%)]  Loss: 3.274 (3.00)  Time: 0.774s, 1323.42/s  (0.794s, 1289.57/s)  LR: 4.768e-05  Data: 0.010 (0.015)
Train: 525 [ 400/1251 ( 32%)]  Loss: 3.083 (3.01)  Time: 0.808s, 1266.59/s  (0.795s, 1288.18/s)  LR: 4.768e-05  Data: 0.010 (0.015)
Train: 525 [ 450/1251 ( 36%)]  Loss: 2.772 (2.98)  Time: 0.773s, 1325.55/s  (0.795s, 1288.72/s)  LR: 4.768e-05  Data: 0.010 (0.014)
Train: 525 [ 500/1251 ( 40%)]  Loss: 2.889 (2.97)  Time: 0.810s, 1264.33/s  (0.794s, 1289.66/s)  LR: 4.768e-05  Data: 0.012 (0.014)
Train: 525 [ 550/1251 ( 44%)]  Loss: 3.088 (2.98)  Time: 0.772s, 1326.35/s  (0.793s, 1290.66/s)  LR: 4.768e-05  Data: 0.009 (0.013)
Train: 525 [ 600/1251 ( 48%)]  Loss: 3.210 (3.00)  Time: 0.769s, 1330.86/s  (0.794s, 1289.97/s)  LR: 4.768e-05  Data: 0.009 (0.013)
Train: 525 [ 650/1251 ( 52%)]  Loss: 3.460 (3.03)  Time: 0.806s, 1270.57/s  (0.794s, 1289.31/s)  LR: 4.768e-05  Data: 0.010 (0.013)
Train: 525 [ 700/1251 ( 56%)]  Loss: 3.222 (3.05)  Time: 0.774s, 1323.66/s  (0.794s, 1289.64/s)  LR: 4.768e-05  Data: 0.009 (0.013)
Train: 525 [ 750/1251 ( 60%)]  Loss: 2.992 (3.04)  Time: 0.783s, 1308.08/s  (0.794s, 1290.15/s)  LR: 4.768e-05  Data: 0.010 (0.012)
Train: 525 [ 800/1251 ( 64%)]  Loss: 2.866 (3.03)  Time: 0.808s, 1266.89/s  (0.793s, 1290.56/s)  LR: 4.768e-05  Data: 0.009 (0.012)
Train: 525 [ 850/1251 ( 68%)]  Loss: 2.749 (3.02)  Time: 0.822s, 1245.90/s  (0.794s, 1288.90/s)  LR: 4.768e-05  Data: 0.010 (0.012)
Train: 525 [ 900/1251 ( 72%)]  Loss: 2.769 (3.00)  Time: 0.811s, 1263.30/s  (0.794s, 1288.90/s)  LR: 4.768e-05  Data: 0.010 (0.012)
Train: 525 [ 950/1251 ( 76%)]  Loss: 3.176 (3.01)  Time: 0.774s, 1322.61/s  (0.794s, 1289.79/s)  LR: 4.768e-05  Data: 0.009 (0.012)
Train: 525 [1000/1251 ( 80%)]  Loss: 2.925 (3.01)  Time: 0.792s, 1292.48/s  (0.793s, 1291.07/s)  LR: 4.768e-05  Data: 0.013 (0.012)
Train: 525 [1050/1251 ( 84%)]  Loss: 3.218 (3.02)  Time: 0.772s, 1325.87/s  (0.793s, 1291.27/s)  LR: 4.768e-05  Data: 0.009 (0.012)
Train: 525 [1100/1251 ( 88%)]  Loss: 3.147 (3.02)  Time: 0.803s, 1274.88/s  (0.793s, 1290.70/s)  LR: 4.768e-05  Data: 0.009 (0.012)
Train: 525 [1150/1251 ( 92%)]  Loss: 3.139 (3.03)  Time: 0.875s, 1170.66/s  (0.793s, 1291.05/s)  LR: 4.768e-05  Data: 0.009 (0.011)
Train: 525 [1200/1251 ( 96%)]  Loss: 2.933 (3.02)  Time: 0.786s, 1302.56/s  (0.793s, 1291.84/s)  LR: 4.768e-05  Data: 0.012 (0.011)
Train: 525 [1250/1251 (100%)]  Loss: 3.050 (3.03)  Time: 0.805s, 1271.63/s  (0.793s, 1291.88/s)  LR: 4.768e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.556 (1.556)  Loss:  0.6577 (0.6577)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.194 (0.562)  Loss:  0.7207 (1.0877)  Acc@1: 87.5000 (80.3640)  Acc@5: 98.3491 (95.3380)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-522.pth.tar', 80.45000005371094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-524.pth.tar', 80.41799997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-523.pth.tar', 80.4179999243164)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-518.pth.tar', 80.41000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-519.pth.tar', 80.36799992431641)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-525.pth.tar', 80.364)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-515.pth.tar', 80.35600005126953)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-521.pth.tar', 80.35399995117187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-520.pth.tar', 80.33400012939452)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-513.pth.tar', 80.33199992675782)

Train: 526 [   0/1251 (  0%)]  Loss: 3.103 (3.10)  Time: 2.232s,  458.77/s  (2.232s,  458.77/s)  LR: 4.669e-05  Data: 1.500 (1.500)
Train: 526 [  50/1251 (  4%)]  Loss: 3.051 (3.08)  Time: 0.786s, 1302.78/s  (0.818s, 1251.89/s)  LR: 4.669e-05  Data: 0.010 (0.042)
Train: 526 [ 100/1251 (  8%)]  Loss: 3.056 (3.07)  Time: 0.808s, 1267.05/s  (0.801s, 1277.83/s)  LR: 4.669e-05  Data: 0.009 (0.026)
Train: 526 [ 150/1251 ( 12%)]  Loss: 3.043 (3.06)  Time: 0.808s, 1267.86/s  (0.797s, 1284.06/s)  LR: 4.669e-05  Data: 0.009 (0.021)
Train: 526 [ 200/1251 ( 16%)]  Loss: 3.110 (3.07)  Time: 0.774s, 1322.88/s  (0.795s, 1287.60/s)  LR: 4.669e-05  Data: 0.009 (0.018)
Train: 526 [ 250/1251 ( 20%)]  Loss: 2.854 (3.04)  Time: 0.784s, 1305.36/s  (0.793s, 1290.55/s)  LR: 4.669e-05  Data: 0.010 (0.016)
Train: 526 [ 300/1251 ( 24%)]  Loss: 3.057 (3.04)  Time: 0.819s, 1250.33/s  (0.792s, 1292.16/s)  LR: 4.669e-05  Data: 0.009 (0.015)
Train: 526 [ 350/1251 ( 28%)]  Loss: 3.048 (3.04)  Time: 0.809s, 1265.77/s  (0.791s, 1294.02/s)  LR: 4.669e-05  Data: 0.009 (0.014)
Train: 526 [ 400/1251 ( 32%)]  Loss: 3.005 (3.04)  Time: 0.772s, 1326.93/s  (0.791s, 1293.89/s)  LR: 4.669e-05  Data: 0.010 (0.014)
Train: 526 [ 450/1251 ( 36%)]  Loss: 3.070 (3.04)  Time: 0.784s, 1305.74/s  (0.792s, 1292.85/s)  LR: 4.669e-05  Data: 0.010 (0.013)
Train: 526 [ 500/1251 ( 40%)]  Loss: 3.079 (3.04)  Time: 0.806s, 1270.17/s  (0.792s, 1292.19/s)  LR: 4.669e-05  Data: 0.009 (0.013)
Train: 526 [ 550/1251 ( 44%)]  Loss: 2.895 (3.03)  Time: 0.807s, 1268.88/s  (0.793s, 1291.17/s)  LR: 4.669e-05  Data: 0.010 (0.013)
Train: 526 [ 600/1251 ( 48%)]  Loss: 3.074 (3.03)  Time: 0.808s, 1267.72/s  (0.794s, 1290.25/s)  LR: 4.669e-05  Data: 0.010 (0.012)
Train: 526 [ 650/1251 ( 52%)]  Loss: 2.904 (3.02)  Time: 0.821s, 1247.73/s  (0.793s, 1291.09/s)  LR: 4.669e-05  Data: 0.010 (0.012)
Train: 526 [ 700/1251 ( 56%)]  Loss: 2.993 (3.02)  Time: 0.820s, 1249.25/s  (0.793s, 1290.52/s)  LR: 4.669e-05  Data: 0.011 (0.012)
Train: 526 [ 750/1251 ( 60%)]  Loss: 2.855 (3.01)  Time: 0.788s, 1299.57/s  (0.793s, 1290.55/s)  LR: 4.669e-05  Data: 0.014 (0.012)
Train: 526 [ 800/1251 ( 64%)]  Loss: 2.831 (3.00)  Time: 0.814s, 1258.01/s  (0.793s, 1291.19/s)  LR: 4.669e-05  Data: 0.010 (0.012)
Train: 526 [ 850/1251 ( 68%)]  Loss: 2.818 (2.99)  Time: 0.780s, 1312.28/s  (0.793s, 1291.29/s)  LR: 4.669e-05  Data: 0.009 (0.012)
Train: 526 [ 900/1251 ( 72%)]  Loss: 3.145 (3.00)  Time: 0.773s, 1325.25/s  (0.793s, 1291.53/s)  LR: 4.669e-05  Data: 0.010 (0.012)
Train: 526 [ 950/1251 ( 76%)]  Loss: 2.546 (2.98)  Time: 0.781s, 1310.52/s  (0.792s, 1292.15/s)  LR: 4.669e-05  Data: 0.009 (0.012)
Train: 526 [1000/1251 ( 80%)]  Loss: 2.976 (2.98)  Time: 0.772s, 1325.62/s  (0.793s, 1291.46/s)  LR: 4.669e-05  Data: 0.009 (0.011)
Train: 526 [1050/1251 ( 84%)]  Loss: 3.294 (2.99)  Time: 0.807s, 1269.35/s  (0.793s, 1291.73/s)  LR: 4.669e-05  Data: 0.010 (0.011)
Train: 526 [1100/1251 ( 88%)]  Loss: 2.853 (2.99)  Time: 0.809s, 1265.27/s  (0.793s, 1291.53/s)  LR: 4.669e-05  Data: 0.010 (0.011)
Train: 526 [1150/1251 ( 92%)]  Loss: 2.904 (2.98)  Time: 0.815s, 1256.31/s  (0.793s, 1291.26/s)  LR: 4.669e-05  Data: 0.011 (0.011)
Train: 526 [1200/1251 ( 96%)]  Loss: 2.577 (2.97)  Time: 0.773s, 1325.37/s  (0.793s, 1291.75/s)  LR: 4.669e-05  Data: 0.010 (0.011)
Train: 526 [1250/1251 (100%)]  Loss: 3.111 (2.97)  Time: 0.768s, 1332.64/s  (0.793s, 1291.90/s)  LR: 4.669e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.606 (1.606)  Loss:  0.5981 (0.5981)  Acc@1: 92.3828 (92.3828)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.194 (0.565)  Loss:  0.7100 (1.0393)  Acc@1: 86.5566 (80.5720)  Acc@5: 97.7594 (95.2800)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-526.pth.tar', 80.57199997802735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-522.pth.tar', 80.45000005371094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-524.pth.tar', 80.41799997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-523.pth.tar', 80.4179999243164)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-518.pth.tar', 80.41000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-519.pth.tar', 80.36799992431641)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-525.pth.tar', 80.364)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-515.pth.tar', 80.35600005126953)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-521.pth.tar', 80.35399995117187)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-520.pth.tar', 80.33400012939452)

Train: 527 [   0/1251 (  0%)]  Loss: 3.332 (3.33)  Time: 2.174s,  470.92/s  (2.174s,  470.92/s)  LR: 4.572e-05  Data: 1.459 (1.459)
Train: 527 [  50/1251 (  4%)]  Loss: 2.430 (2.88)  Time: 0.810s, 1264.88/s  (0.834s, 1227.33/s)  LR: 4.572e-05  Data: 0.010 (0.044)
Train: 527 [ 100/1251 (  8%)]  Loss: 3.078 (2.95)  Time: 0.816s, 1254.21/s  (0.817s, 1253.01/s)  LR: 4.572e-05  Data: 0.009 (0.027)
Train: 527 [ 150/1251 ( 12%)]  Loss: 3.238 (3.02)  Time: 0.774s, 1322.39/s  (0.809s, 1266.37/s)  LR: 4.572e-05  Data: 0.010 (0.021)
Train: 527 [ 200/1251 ( 16%)]  Loss: 2.833 (2.98)  Time: 0.778s, 1316.22/s  (0.802s, 1277.39/s)  LR: 4.572e-05  Data: 0.009 (0.019)
Train: 527 [ 250/1251 ( 20%)]  Loss: 2.996 (2.98)  Time: 0.773s, 1324.34/s  (0.798s, 1282.51/s)  LR: 4.572e-05  Data: 0.010 (0.017)
Train: 527 [ 300/1251 ( 24%)]  Loss: 2.792 (2.96)  Time: 0.810s, 1264.30/s  (0.798s, 1283.98/s)  LR: 4.572e-05  Data: 0.010 (0.016)
Train: 527 [ 350/1251 ( 28%)]  Loss: 3.012 (2.96)  Time: 0.780s, 1313.33/s  (0.797s, 1284.19/s)  LR: 4.572e-05  Data: 0.010 (0.015)
Train: 527 [ 400/1251 ( 32%)]  Loss: 3.132 (2.98)  Time: 0.773s, 1325.23/s  (0.796s, 1285.73/s)  LR: 4.572e-05  Data: 0.010 (0.014)
Train: 527 [ 450/1251 ( 36%)]  Loss: 2.785 (2.96)  Time: 0.827s, 1238.12/s  (0.797s, 1285.07/s)  LR: 4.572e-05  Data: 0.010 (0.014)
Train: 527 [ 500/1251 ( 40%)]  Loss: 2.905 (2.96)  Time: 0.773s, 1324.00/s  (0.797s, 1284.32/s)  LR: 4.572e-05  Data: 0.011 (0.013)
Train: 527 [ 550/1251 ( 44%)]  Loss: 3.074 (2.97)  Time: 0.810s, 1264.10/s  (0.797s, 1284.78/s)  LR: 4.572e-05  Data: 0.010 (0.013)
Train: 527 [ 600/1251 ( 48%)]  Loss: 2.939 (2.97)  Time: 0.784s, 1305.30/s  (0.797s, 1284.02/s)  LR: 4.572e-05  Data: 0.010 (0.013)
Train: 527 [ 650/1251 ( 52%)]  Loss: 2.968 (2.97)  Time: 0.772s, 1326.77/s  (0.797s, 1284.93/s)  LR: 4.572e-05  Data: 0.010 (0.013)
Train: 527 [ 700/1251 ( 56%)]  Loss: 3.132 (2.98)  Time: 0.815s, 1256.43/s  (0.797s, 1285.32/s)  LR: 4.572e-05  Data: 0.010 (0.012)
Train: 527 [ 750/1251 ( 60%)]  Loss: 3.218 (2.99)  Time: 0.808s, 1267.16/s  (0.797s, 1284.55/s)  LR: 4.572e-05  Data: 0.010 (0.012)
Train: 527 [ 800/1251 ( 64%)]  Loss: 2.978 (2.99)  Time: 0.781s, 1310.78/s  (0.797s, 1285.30/s)  LR: 4.572e-05  Data: 0.010 (0.012)
Train: 527 [ 850/1251 ( 68%)]  Loss: 2.789 (2.98)  Time: 0.772s, 1326.26/s  (0.796s, 1286.10/s)  LR: 4.572e-05  Data: 0.010 (0.012)
Train: 527 [ 900/1251 ( 72%)]  Loss: 3.159 (2.99)  Time: 0.808s, 1266.66/s  (0.796s, 1286.11/s)  LR: 4.572e-05  Data: 0.010 (0.012)
Train: 527 [ 950/1251 ( 76%)]  Loss: 3.035 (2.99)  Time: 0.776s, 1319.92/s  (0.796s, 1286.08/s)  LR: 4.572e-05  Data: 0.010 (0.012)
Train: 527 [1000/1251 ( 80%)]  Loss: 2.992 (2.99)  Time: 0.808s, 1267.17/s  (0.796s, 1286.66/s)  LR: 4.572e-05  Data: 0.010 (0.012)
Train: 527 [1050/1251 ( 84%)]  Loss: 3.185 (3.00)  Time: 0.831s, 1231.96/s  (0.796s, 1286.85/s)  LR: 4.572e-05  Data: 0.016 (0.012)
Train: 527 [1100/1251 ( 88%)]  Loss: 2.905 (3.00)  Time: 0.778s, 1316.31/s  (0.795s, 1287.74/s)  LR: 4.572e-05  Data: 0.010 (0.011)
Train: 527 [1150/1251 ( 92%)]  Loss: 2.685 (2.98)  Time: 0.789s, 1298.34/s  (0.795s, 1288.40/s)  LR: 4.572e-05  Data: 0.014 (0.011)
Train: 527 [1200/1251 ( 96%)]  Loss: 2.650 (2.97)  Time: 0.771s, 1328.17/s  (0.794s, 1289.10/s)  LR: 4.572e-05  Data: 0.009 (0.011)
Train: 527 [1250/1251 (100%)]  Loss: 2.844 (2.96)  Time: 0.795s, 1288.69/s  (0.794s, 1289.09/s)  LR: 4.572e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.566 (1.566)  Loss:  0.6216 (0.6216)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.194 (0.580)  Loss:  0.6953 (1.0581)  Acc@1: 86.9104 (80.5000)  Acc@5: 98.1132 (95.2720)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-526.pth.tar', 80.57199997802735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-527.pth.tar', 80.50000000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-522.pth.tar', 80.45000005371094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-524.pth.tar', 80.41799997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-523.pth.tar', 80.4179999243164)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-518.pth.tar', 80.41000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-519.pth.tar', 80.36799992431641)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-525.pth.tar', 80.364)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-515.pth.tar', 80.35600005126953)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-521.pth.tar', 80.35399995117187)

Train: 528 [   0/1251 (  0%)]  Loss: 2.814 (2.81)  Time: 2.343s,  437.14/s  (2.343s,  437.14/s)  LR: 4.476e-05  Data: 1.609 (1.609)
Train: 528 [  50/1251 (  4%)]  Loss: 3.032 (2.92)  Time: 0.806s, 1269.79/s  (0.831s, 1232.99/s)  LR: 4.476e-05  Data: 0.009 (0.046)
Train: 528 [ 100/1251 (  8%)]  Loss: 3.262 (3.04)  Time: 0.772s, 1325.69/s  (0.811s, 1262.72/s)  LR: 4.476e-05  Data: 0.010 (0.028)
Train: 528 [ 150/1251 ( 12%)]  Loss: 3.095 (3.05)  Time: 0.806s, 1270.26/s  (0.802s, 1277.42/s)  LR: 4.476e-05  Data: 0.010 (0.022)
Train: 528 [ 200/1251 ( 16%)]  Loss: 2.868 (3.01)  Time: 0.805s, 1271.55/s  (0.804s, 1273.06/s)  LR: 4.476e-05  Data: 0.009 (0.019)
Train: 528 [ 250/1251 ( 20%)]  Loss: 2.868 (2.99)  Time: 0.854s, 1198.70/s  (0.802s, 1277.22/s)  LR: 4.476e-05  Data: 0.010 (0.017)
Train: 528 [ 300/1251 ( 24%)]  Loss: 2.950 (2.98)  Time: 0.805s, 1271.61/s  (0.799s, 1280.99/s)  LR: 4.476e-05  Data: 0.009 (0.016)
Train: 528 [ 350/1251 ( 28%)]  Loss: 3.206 (3.01)  Time: 0.807s, 1269.25/s  (0.800s, 1280.09/s)  LR: 4.476e-05  Data: 0.009 (0.015)
Train: 528 [ 400/1251 ( 32%)]  Loss: 3.133 (3.03)  Time: 0.774s, 1322.27/s  (0.800s, 1280.04/s)  LR: 4.476e-05  Data: 0.009 (0.014)
Train: 528 [ 450/1251 ( 36%)]  Loss: 3.166 (3.04)  Time: 0.815s, 1256.31/s  (0.799s, 1282.30/s)  LR: 4.476e-05  Data: 0.010 (0.014)
Train: 528 [ 500/1251 ( 40%)]  Loss: 2.831 (3.02)  Time: 0.814s, 1257.58/s  (0.799s, 1281.55/s)  LR: 4.476e-05  Data: 0.011 (0.013)
Train: 528 [ 550/1251 ( 44%)]  Loss: 3.068 (3.02)  Time: 0.773s, 1324.35/s  (0.800s, 1280.74/s)  LR: 4.476e-05  Data: 0.010 (0.013)
Train: 528 [ 600/1251 ( 48%)]  Loss: 3.206 (3.04)  Time: 0.779s, 1314.49/s  (0.799s, 1282.23/s)  LR: 4.476e-05  Data: 0.009 (0.013)
Train: 528 [ 650/1251 ( 52%)]  Loss: 3.103 (3.04)  Time: 0.775s, 1320.62/s  (0.797s, 1284.74/s)  LR: 4.476e-05  Data: 0.009 (0.013)
Train: 528 [ 700/1251 ( 56%)]  Loss: 2.922 (3.03)  Time: 0.781s, 1310.75/s  (0.796s, 1286.56/s)  LR: 4.476e-05  Data: 0.009 (0.012)
Train: 528 [ 750/1251 ( 60%)]  Loss: 3.272 (3.05)  Time: 0.884s, 1158.66/s  (0.796s, 1286.97/s)  LR: 4.476e-05  Data: 0.009 (0.012)
Train: 528 [ 800/1251 ( 64%)]  Loss: 2.862 (3.04)  Time: 0.787s, 1301.68/s  (0.796s, 1286.03/s)  LR: 4.476e-05  Data: 0.009 (0.012)
Train: 528 [ 850/1251 ( 68%)]  Loss: 2.802 (3.03)  Time: 0.808s, 1267.85/s  (0.796s, 1286.78/s)  LR: 4.476e-05  Data: 0.009 (0.012)
Train: 528 [ 900/1251 ( 72%)]  Loss: 2.858 (3.02)  Time: 0.816s, 1255.17/s  (0.796s, 1286.51/s)  LR: 4.476e-05  Data: 0.011 (0.012)
Train: 528 [ 950/1251 ( 76%)]  Loss: 3.152 (3.02)  Time: 0.826s, 1239.59/s  (0.797s, 1284.57/s)  LR: 4.476e-05  Data: 0.010 (0.012)
Train: 528 [1000/1251 ( 80%)]  Loss: 3.084 (3.03)  Time: 0.806s, 1270.99/s  (0.797s, 1284.89/s)  LR: 4.476e-05  Data: 0.010 (0.012)
Train: 528 [1050/1251 ( 84%)]  Loss: 3.096 (3.03)  Time: 0.824s, 1243.38/s  (0.797s, 1285.37/s)  LR: 4.476e-05  Data: 0.009 (0.012)
Train: 528 [1100/1251 ( 88%)]  Loss: 3.256 (3.04)  Time: 0.813s, 1260.02/s  (0.796s, 1285.80/s)  LR: 4.476e-05  Data: 0.010 (0.011)
Train: 528 [1150/1251 ( 92%)]  Loss: 2.915 (3.03)  Time: 0.807s, 1268.94/s  (0.797s, 1285.01/s)  LR: 4.476e-05  Data: 0.009 (0.011)
Train: 528 [1200/1251 ( 96%)]  Loss: 3.106 (3.04)  Time: 0.870s, 1176.89/s  (0.797s, 1285.34/s)  LR: 4.476e-05  Data: 0.010 (0.011)
Train: 528 [1250/1251 (100%)]  Loss: 2.787 (3.03)  Time: 0.795s, 1288.15/s  (0.796s, 1285.77/s)  LR: 4.476e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.625 (1.625)  Loss:  0.6226 (0.6226)  Acc@1: 91.6016 (91.6016)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.194 (0.564)  Loss:  0.7300 (1.0642)  Acc@1: 86.4387 (80.3340)  Acc@5: 97.6415 (95.2940)
Train: 529 [   0/1251 (  0%)]  Loss: 2.918 (2.92)  Time: 2.365s,  432.96/s  (2.365s,  432.96/s)  LR: 4.381e-05  Data: 1.639 (1.639)
Train: 529 [  50/1251 (  4%)]  Loss: 2.940 (2.93)  Time: 0.787s, 1300.90/s  (0.818s, 1251.12/s)  LR: 4.381e-05  Data: 0.009 (0.044)
Train: 529 [ 100/1251 (  8%)]  Loss: 2.957 (2.94)  Time: 0.812s, 1261.77/s  (0.807s, 1268.64/s)  LR: 4.381e-05  Data: 0.010 (0.027)
Train: 529 [ 150/1251 ( 12%)]  Loss: 3.062 (2.97)  Time: 0.783s, 1308.50/s  (0.804s, 1272.98/s)  LR: 4.381e-05  Data: 0.013 (0.021)
Train: 529 [ 200/1251 ( 16%)]  Loss: 2.846 (2.94)  Time: 0.771s, 1327.59/s  (0.800s, 1280.46/s)  LR: 4.381e-05  Data: 0.009 (0.018)
Train: 529 [ 250/1251 ( 20%)]  Loss: 3.261 (3.00)  Time: 0.798s, 1283.61/s  (0.798s, 1282.44/s)  LR: 4.381e-05  Data: 0.009 (0.017)
Train: 529 [ 300/1251 ( 24%)]  Loss: 2.620 (2.94)  Time: 0.782s, 1308.89/s  (0.798s, 1283.93/s)  LR: 4.381e-05  Data: 0.010 (0.015)
Train: 529 [ 350/1251 ( 28%)]  Loss: 3.111 (2.96)  Time: 0.772s, 1326.02/s  (0.796s, 1285.87/s)  LR: 4.381e-05  Data: 0.010 (0.015)
Train: 529 [ 400/1251 ( 32%)]  Loss: 3.120 (2.98)  Time: 0.810s, 1264.69/s  (0.797s, 1284.95/s)  LR: 4.381e-05  Data: 0.009 (0.014)
Train: 529 [ 450/1251 ( 36%)]  Loss: 3.016 (2.99)  Time: 0.787s, 1300.65/s  (0.796s, 1285.78/s)  LR: 4.381e-05  Data: 0.009 (0.014)
Train: 529 [ 500/1251 ( 40%)]  Loss: 3.074 (2.99)  Time: 0.774s, 1323.84/s  (0.796s, 1286.77/s)  LR: 4.381e-05  Data: 0.009 (0.013)
Train: 529 [ 550/1251 ( 44%)]  Loss: 3.270 (3.02)  Time: 0.811s, 1262.64/s  (0.795s, 1288.06/s)  LR: 4.381e-05  Data: 0.010 (0.013)
Train: 529 [ 600/1251 ( 48%)]  Loss: 2.979 (3.01)  Time: 0.780s, 1312.09/s  (0.794s, 1289.03/s)  LR: 4.381e-05  Data: 0.009 (0.013)
Train: 529 [ 650/1251 ( 52%)]  Loss: 3.044 (3.02)  Time: 0.774s, 1323.58/s  (0.794s, 1289.12/s)  LR: 4.381e-05  Data: 0.010 (0.012)
Train: 529 [ 700/1251 ( 56%)]  Loss: 2.984 (3.01)  Time: 0.776s, 1319.82/s  (0.795s, 1288.74/s)  LR: 4.381e-05  Data: 0.009 (0.012)
Train: 529 [ 750/1251 ( 60%)]  Loss: 3.207 (3.03)  Time: 0.820s, 1249.43/s  (0.794s, 1289.35/s)  LR: 4.381e-05  Data: 0.009 (0.012)
Train: 529 [ 800/1251 ( 64%)]  Loss: 3.193 (3.04)  Time: 0.809s, 1266.46/s  (0.794s, 1289.13/s)  LR: 4.381e-05  Data: 0.010 (0.012)
Train: 529 [ 850/1251 ( 68%)]  Loss: 2.612 (3.01)  Time: 0.809s, 1266.15/s  (0.794s, 1290.10/s)  LR: 4.381e-05  Data: 0.010 (0.012)
Train: 529 [ 900/1251 ( 72%)]  Loss: 2.695 (3.00)  Time: 0.810s, 1264.86/s  (0.794s, 1289.20/s)  LR: 4.381e-05  Data: 0.009 (0.012)
Train: 529 [ 950/1251 ( 76%)]  Loss: 3.109 (3.00)  Time: 0.811s, 1262.73/s  (0.795s, 1288.61/s)  LR: 4.381e-05  Data: 0.010 (0.011)
Train: 529 [1000/1251 ( 80%)]  Loss: 2.715 (2.99)  Time: 0.794s, 1289.45/s  (0.795s, 1288.27/s)  LR: 4.381e-05  Data: 0.009 (0.011)
Train: 529 [1050/1251 ( 84%)]  Loss: 3.112 (2.99)  Time: 0.772s, 1325.77/s  (0.794s, 1288.97/s)  LR: 4.381e-05  Data: 0.009 (0.011)
Train: 529 [1100/1251 ( 88%)]  Loss: 3.063 (3.00)  Time: 0.773s, 1324.70/s  (0.795s, 1288.44/s)  LR: 4.381e-05  Data: 0.010 (0.011)
Train: 529 [1150/1251 ( 92%)]  Loss: 2.935 (2.99)  Time: 0.805s, 1271.81/s  (0.795s, 1288.16/s)  LR: 4.381e-05  Data: 0.009 (0.011)
Train: 529 [1200/1251 ( 96%)]  Loss: 2.722 (2.98)  Time: 0.823s, 1244.82/s  (0.795s, 1287.75/s)  LR: 4.381e-05  Data: 0.010 (0.011)
Train: 529 [1250/1251 (100%)]  Loss: 3.070 (2.99)  Time: 0.796s, 1286.00/s  (0.796s, 1286.78/s)  LR: 4.381e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.559 (1.559)  Loss:  0.7217 (0.7217)  Acc@1: 91.7969 (91.7969)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.194 (0.562)  Loss:  0.8232 (1.1676)  Acc@1: 87.2642 (80.3820)  Acc@5: 98.1132 (95.3000)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-526.pth.tar', 80.57199997802735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-527.pth.tar', 80.50000000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-522.pth.tar', 80.45000005371094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-524.pth.tar', 80.41799997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-523.pth.tar', 80.4179999243164)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-518.pth.tar', 80.41000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-529.pth.tar', 80.38200002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-519.pth.tar', 80.36799992431641)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-525.pth.tar', 80.364)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-515.pth.tar', 80.35600005126953)

Train: 530 [   0/1251 (  0%)]  Loss: 2.706 (2.71)  Time: 2.267s,  451.73/s  (2.267s,  451.73/s)  LR: 4.288e-05  Data: 1.530 (1.530)
Train: 530 [  50/1251 (  4%)]  Loss: 2.819 (2.76)  Time: 0.775s, 1321.27/s  (0.817s, 1252.76/s)  LR: 4.288e-05  Data: 0.009 (0.045)
Train: 530 [ 100/1251 (  8%)]  Loss: 2.962 (2.83)  Time: 0.774s, 1323.40/s  (0.803s, 1274.52/s)  LR: 4.288e-05  Data: 0.010 (0.028)
Train: 530 [ 150/1251 ( 12%)]  Loss: 3.049 (2.88)  Time: 0.806s, 1270.93/s  (0.801s, 1277.93/s)  LR: 4.288e-05  Data: 0.009 (0.022)
Train: 530 [ 200/1251 ( 16%)]  Loss: 2.931 (2.89)  Time: 0.777s, 1317.42/s  (0.803s, 1275.72/s)  LR: 4.288e-05  Data: 0.009 (0.019)
Train: 530 [ 250/1251 ( 20%)]  Loss: 3.097 (2.93)  Time: 0.773s, 1325.03/s  (0.800s, 1280.16/s)  LR: 4.288e-05  Data: 0.010 (0.017)
Train: 530 [ 300/1251 ( 24%)]  Loss: 2.613 (2.88)  Time: 0.812s, 1261.06/s  (0.799s, 1281.71/s)  LR: 4.288e-05  Data: 0.010 (0.016)
Train: 530 [ 350/1251 ( 28%)]  Loss: 3.063 (2.90)  Time: 0.776s, 1319.65/s  (0.797s, 1284.17/s)  LR: 4.288e-05  Data: 0.009 (0.015)
Train: 530 [ 400/1251 ( 32%)]  Loss: 3.093 (2.93)  Time: 0.820s, 1249.21/s  (0.797s, 1284.79/s)  LR: 4.288e-05  Data: 0.009 (0.014)
Train: 530 [ 450/1251 ( 36%)]  Loss: 2.851 (2.92)  Time: 0.776s, 1318.96/s  (0.796s, 1285.78/s)  LR: 4.288e-05  Data: 0.010 (0.014)
Train: 530 [ 500/1251 ( 40%)]  Loss: 2.987 (2.92)  Time: 0.773s, 1325.54/s  (0.796s, 1286.68/s)  LR: 4.288e-05  Data: 0.010 (0.013)
Train: 530 [ 550/1251 ( 44%)]  Loss: 2.942 (2.93)  Time: 0.808s, 1266.83/s  (0.796s, 1286.62/s)  LR: 4.288e-05  Data: 0.010 (0.013)
Train: 530 [ 600/1251 ( 48%)]  Loss: 3.163 (2.94)  Time: 0.809s, 1266.31/s  (0.796s, 1286.68/s)  LR: 4.288e-05  Data: 0.010 (0.013)
Train: 530 [ 650/1251 ( 52%)]  Loss: 3.033 (2.95)  Time: 0.786s, 1303.40/s  (0.795s, 1288.57/s)  LR: 4.288e-05  Data: 0.010 (0.013)
Train: 530 [ 700/1251 ( 56%)]  Loss: 3.014 (2.95)  Time: 0.774s, 1322.89/s  (0.794s, 1289.46/s)  LR: 4.288e-05  Data: 0.010 (0.012)
Train: 530 [ 750/1251 ( 60%)]  Loss: 2.912 (2.95)  Time: 0.774s, 1322.83/s  (0.794s, 1289.88/s)  LR: 4.288e-05  Data: 0.009 (0.012)
Train: 530 [ 800/1251 ( 64%)]  Loss: 2.947 (2.95)  Time: 0.788s, 1299.95/s  (0.794s, 1289.15/s)  LR: 4.288e-05  Data: 0.010 (0.012)
Train: 530 [ 850/1251 ( 68%)]  Loss: 2.967 (2.95)  Time: 0.774s, 1323.31/s  (0.794s, 1289.09/s)  LR: 4.288e-05  Data: 0.010 (0.012)
Train: 530 [ 900/1251 ( 72%)]  Loss: 2.690 (2.94)  Time: 0.788s, 1299.82/s  (0.795s, 1288.59/s)  LR: 4.288e-05  Data: 0.009 (0.012)
Train: 530 [ 950/1251 ( 76%)]  Loss: 3.248 (2.95)  Time: 0.797s, 1284.15/s  (0.795s, 1287.26/s)  LR: 4.288e-05  Data: 0.014 (0.012)
Train: 530 [1000/1251 ( 80%)]  Loss: 3.017 (2.96)  Time: 0.779s, 1314.98/s  (0.796s, 1286.58/s)  LR: 4.288e-05  Data: 0.010 (0.012)
Train: 530 [1050/1251 ( 84%)]  Loss: 2.846 (2.95)  Time: 0.772s, 1325.97/s  (0.796s, 1286.34/s)  LR: 4.288e-05  Data: 0.010 (0.012)
Train: 530 [1100/1251 ( 88%)]  Loss: 2.682 (2.94)  Time: 0.816s, 1254.77/s  (0.796s, 1286.29/s)  LR: 4.288e-05  Data: 0.010 (0.012)
Train: 530 [1150/1251 ( 92%)]  Loss: 2.864 (2.94)  Time: 0.781s, 1310.38/s  (0.796s, 1285.96/s)  LR: 4.288e-05  Data: 0.010 (0.012)
Train: 530 [1200/1251 ( 96%)]  Loss: 3.065 (2.94)  Time: 0.807s, 1268.25/s  (0.796s, 1287.02/s)  LR: 4.288e-05  Data: 0.011 (0.012)
Train: 530 [1250/1251 (100%)]  Loss: 2.986 (2.94)  Time: 0.766s, 1337.25/s  (0.795s, 1287.29/s)  LR: 4.288e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.550 (1.550)  Loss:  0.6079 (0.6079)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.194 (0.588)  Loss:  0.7031 (1.0443)  Acc@1: 86.6745 (80.6140)  Acc@5: 98.2311 (95.3000)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-530.pth.tar', 80.61400002929688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-526.pth.tar', 80.57199997802735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-527.pth.tar', 80.50000000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-522.pth.tar', 80.45000005371094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-524.pth.tar', 80.41799997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-523.pth.tar', 80.4179999243164)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-518.pth.tar', 80.41000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-529.pth.tar', 80.38200002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-519.pth.tar', 80.36799992431641)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-525.pth.tar', 80.364)

Train: 531 [   0/1251 (  0%)]  Loss: 2.538 (2.54)  Time: 2.190s,  467.55/s  (2.190s,  467.55/s)  LR: 4.196e-05  Data: 1.458 (1.458)
Train: 531 [  50/1251 (  4%)]  Loss: 2.994 (2.77)  Time: 0.774s, 1322.71/s  (0.819s, 1250.94/s)  LR: 4.196e-05  Data: 0.010 (0.044)
Train: 531 [ 100/1251 (  8%)]  Loss: 3.024 (2.85)  Time: 0.773s, 1323.86/s  (0.802s, 1276.03/s)  LR: 4.196e-05  Data: 0.011 (0.027)
Train: 531 [ 150/1251 ( 12%)]  Loss: 2.843 (2.85)  Time: 0.811s, 1262.08/s  (0.797s, 1284.38/s)  LR: 4.196e-05  Data: 0.010 (0.021)
Train: 531 [ 200/1251 ( 16%)]  Loss: 2.751 (2.83)  Time: 0.776s, 1320.21/s  (0.795s, 1288.48/s)  LR: 4.196e-05  Data: 0.010 (0.018)
Train: 531 [ 250/1251 ( 20%)]  Loss: 3.006 (2.86)  Time: 0.774s, 1323.48/s  (0.794s, 1289.43/s)  LR: 4.196e-05  Data: 0.010 (0.017)
Train: 531 [ 300/1251 ( 24%)]  Loss: 3.145 (2.90)  Time: 0.807s, 1269.25/s  (0.794s, 1290.09/s)  LR: 4.196e-05  Data: 0.010 (0.016)
Train: 531 [ 350/1251 ( 28%)]  Loss: 3.195 (2.94)  Time: 0.794s, 1290.11/s  (0.793s, 1290.73/s)  LR: 4.196e-05  Data: 0.011 (0.015)
Train: 531 [ 400/1251 ( 32%)]  Loss: 2.931 (2.94)  Time: 0.809s, 1266.52/s  (0.794s, 1290.27/s)  LR: 4.196e-05  Data: 0.010 (0.014)
Train: 531 [ 450/1251 ( 36%)]  Loss: 2.706 (2.91)  Time: 0.775s, 1321.63/s  (0.792s, 1292.20/s)  LR: 4.196e-05  Data: 0.009 (0.014)
Train: 531 [ 500/1251 ( 40%)]  Loss: 2.662 (2.89)  Time: 0.816s, 1255.43/s  (0.792s, 1292.69/s)  LR: 4.196e-05  Data: 0.010 (0.013)
Train: 531 [ 550/1251 ( 44%)]  Loss: 2.866 (2.89)  Time: 0.775s, 1321.93/s  (0.792s, 1292.82/s)  LR: 4.196e-05  Data: 0.010 (0.013)
Train: 531 [ 600/1251 ( 48%)]  Loss: 3.003 (2.90)  Time: 0.777s, 1318.27/s  (0.792s, 1292.47/s)  LR: 4.196e-05  Data: 0.009 (0.013)
Train: 531 [ 650/1251 ( 52%)]  Loss: 3.154 (2.92)  Time: 0.775s, 1321.58/s  (0.792s, 1292.75/s)  LR: 4.196e-05  Data: 0.009 (0.013)
Train: 531 [ 700/1251 ( 56%)]  Loss: 3.043 (2.92)  Time: 0.773s, 1324.10/s  (0.791s, 1294.26/s)  LR: 4.196e-05  Data: 0.010 (0.012)
Train: 531 [ 750/1251 ( 60%)]  Loss: 3.092 (2.93)  Time: 0.867s, 1181.29/s  (0.791s, 1295.19/s)  LR: 4.196e-05  Data: 0.010 (0.012)
Train: 531 [ 800/1251 ( 64%)]  Loss: 2.884 (2.93)  Time: 0.806s, 1270.22/s  (0.791s, 1294.28/s)  LR: 4.196e-05  Data: 0.010 (0.012)
Train: 531 [ 850/1251 ( 68%)]  Loss: 2.904 (2.93)  Time: 0.817s, 1253.89/s  (0.793s, 1292.09/s)  LR: 4.196e-05  Data: 0.010 (0.012)
Train: 531 [ 900/1251 ( 72%)]  Loss: 2.989 (2.93)  Time: 0.785s, 1304.56/s  (0.793s, 1291.29/s)  LR: 4.196e-05  Data: 0.010 (0.012)
Train: 531 [ 950/1251 ( 76%)]  Loss: 3.281 (2.95)  Time: 0.821s, 1247.94/s  (0.793s, 1290.80/s)  LR: 4.196e-05  Data: 0.009 (0.012)
Train: 531 [1000/1251 ( 80%)]  Loss: 3.210 (2.96)  Time: 0.812s, 1261.45/s  (0.793s, 1290.70/s)  LR: 4.196e-05  Data: 0.009 (0.012)
Train: 531 [1050/1251 ( 84%)]  Loss: 2.917 (2.96)  Time: 0.816s, 1254.88/s  (0.793s, 1290.50/s)  LR: 4.196e-05  Data: 0.010 (0.012)
Train: 531 [1100/1251 ( 88%)]  Loss: 2.849 (2.96)  Time: 0.774s, 1323.02/s  (0.794s, 1289.85/s)  LR: 4.196e-05  Data: 0.009 (0.012)
Train: 531 [1150/1251 ( 92%)]  Loss: 2.931 (2.95)  Time: 0.776s, 1319.82/s  (0.794s, 1289.70/s)  LR: 4.196e-05  Data: 0.010 (0.012)
Train: 531 [1200/1251 ( 96%)]  Loss: 2.915 (2.95)  Time: 0.809s, 1265.61/s  (0.794s, 1289.34/s)  LR: 4.196e-05  Data: 0.009 (0.011)
Train: 531 [1250/1251 (100%)]  Loss: 3.026 (2.96)  Time: 0.760s, 1347.80/s  (0.794s, 1289.18/s)  LR: 4.196e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.636 (1.636)  Loss:  0.7251 (0.7251)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.194 (0.580)  Loss:  0.7959 (1.1644)  Acc@1: 87.9717 (80.3460)  Acc@5: 97.9953 (95.2680)
Train: 532 [   0/1251 (  0%)]  Loss: 3.104 (3.10)  Time: 2.283s,  448.48/s  (2.283s,  448.48/s)  LR: 4.105e-05  Data: 1.556 (1.556)
Train: 532 [  50/1251 (  4%)]  Loss: 3.082 (3.09)  Time: 0.807s, 1269.23/s  (0.836s, 1225.59/s)  LR: 4.105e-05  Data: 0.009 (0.045)
Train: 532 [ 100/1251 (  8%)]  Loss: 3.069 (3.09)  Time: 0.772s, 1326.01/s  (0.823s, 1244.16/s)  LR: 4.105e-05  Data: 0.009 (0.027)
Train: 532 [ 150/1251 ( 12%)]  Loss: 3.252 (3.13)  Time: 0.779s, 1314.63/s  (0.813s, 1259.57/s)  LR: 4.105e-05  Data: 0.010 (0.021)
Train: 532 [ 200/1251 ( 16%)]  Loss: 2.879 (3.08)  Time: 0.776s, 1320.28/s  (0.807s, 1269.33/s)  LR: 4.105e-05  Data: 0.010 (0.019)
Train: 532 [ 250/1251 ( 20%)]  Loss: 3.062 (3.07)  Time: 0.773s, 1324.51/s  (0.804s, 1274.33/s)  LR: 4.105e-05  Data: 0.009 (0.017)
Train: 532 [ 300/1251 ( 24%)]  Loss: 2.902 (3.05)  Time: 0.784s, 1306.80/s  (0.799s, 1281.17/s)  LR: 4.105e-05  Data: 0.009 (0.016)
Train: 532 [ 350/1251 ( 28%)]  Loss: 2.898 (3.03)  Time: 0.808s, 1267.52/s  (0.798s, 1283.46/s)  LR: 4.105e-05  Data: 0.009 (0.015)
Train: 532 [ 400/1251 ( 32%)]  Loss: 3.175 (3.05)  Time: 0.816s, 1254.69/s  (0.798s, 1283.84/s)  LR: 4.105e-05  Data: 0.011 (0.014)
Train: 532 [ 450/1251 ( 36%)]  Loss: 3.065 (3.05)  Time: 0.814s, 1258.68/s  (0.797s, 1284.35/s)  LR: 4.105e-05  Data: 0.010 (0.014)
Train: 532 [ 500/1251 ( 40%)]  Loss: 3.125 (3.06)  Time: 0.775s, 1321.02/s  (0.797s, 1284.03/s)  LR: 4.105e-05  Data: 0.010 (0.013)
Train: 532 [ 550/1251 ( 44%)]  Loss: 3.176 (3.07)  Time: 0.809s, 1265.58/s  (0.797s, 1285.58/s)  LR: 4.105e-05  Data: 0.009 (0.013)
Train: 532 [ 600/1251 ( 48%)]  Loss: 2.728 (3.04)  Time: 0.773s, 1325.26/s  (0.796s, 1286.17/s)  LR: 4.105e-05  Data: 0.009 (0.013)
Train: 532 [ 650/1251 ( 52%)]  Loss: 3.220 (3.05)  Time: 0.806s, 1269.84/s  (0.796s, 1286.04/s)  LR: 4.105e-05  Data: 0.009 (0.012)
Train: 532 [ 700/1251 ( 56%)]  Loss: 2.878 (3.04)  Time: 0.779s, 1314.14/s  (0.795s, 1287.63/s)  LR: 4.105e-05  Data: 0.010 (0.012)
Train: 532 [ 750/1251 ( 60%)]  Loss: 3.032 (3.04)  Time: 0.772s, 1327.26/s  (0.795s, 1288.55/s)  LR: 4.105e-05  Data: 0.009 (0.012)
Train: 532 [ 800/1251 ( 64%)]  Loss: 3.007 (3.04)  Time: 0.772s, 1327.00/s  (0.794s, 1289.08/s)  LR: 4.105e-05  Data: 0.010 (0.012)
Train: 532 [ 850/1251 ( 68%)]  Loss: 3.004 (3.04)  Time: 0.774s, 1323.58/s  (0.793s, 1290.51/s)  LR: 4.105e-05  Data: 0.009 (0.012)
Train: 532 [ 900/1251 ( 72%)]  Loss: 3.073 (3.04)  Time: 0.772s, 1326.56/s  (0.794s, 1289.90/s)  LR: 4.105e-05  Data: 0.009 (0.012)
Train: 532 [ 950/1251 ( 76%)]  Loss: 3.122 (3.04)  Time: 0.776s, 1319.96/s  (0.794s, 1289.76/s)  LR: 4.105e-05  Data: 0.009 (0.012)
Train: 532 [1000/1251 ( 80%)]  Loss: 2.887 (3.04)  Time: 0.816s, 1255.40/s  (0.794s, 1289.72/s)  LR: 4.105e-05  Data: 0.010 (0.011)
Train: 532 [1050/1251 ( 84%)]  Loss: 3.167 (3.04)  Time: 0.776s, 1320.17/s  (0.794s, 1289.62/s)  LR: 4.105e-05  Data: 0.010 (0.011)
Train: 532 [1100/1251 ( 88%)]  Loss: 3.238 (3.05)  Time: 0.804s, 1274.17/s  (0.794s, 1289.50/s)  LR: 4.105e-05  Data: 0.010 (0.011)
Train: 532 [1150/1251 ( 92%)]  Loss: 2.860 (3.04)  Time: 0.797s, 1284.70/s  (0.794s, 1290.14/s)  LR: 4.105e-05  Data: 0.009 (0.011)
Train: 532 [1200/1251 ( 96%)]  Loss: 2.990 (3.04)  Time: 0.806s, 1270.92/s  (0.794s, 1289.64/s)  LR: 4.105e-05  Data: 0.009 (0.011)
Train: 532 [1250/1251 (100%)]  Loss: 2.719 (3.03)  Time: 0.795s, 1287.69/s  (0.794s, 1289.97/s)  LR: 4.105e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.551 (1.551)  Loss:  0.5879 (0.5879)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.194 (0.571)  Loss:  0.6997 (1.0359)  Acc@1: 86.9104 (80.4720)  Acc@5: 98.3491 (95.3600)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-530.pth.tar', 80.61400002929688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-526.pth.tar', 80.57199997802735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-527.pth.tar', 80.50000000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-532.pth.tar', 80.4720000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-522.pth.tar', 80.45000005371094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-524.pth.tar', 80.41799997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-523.pth.tar', 80.4179999243164)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-518.pth.tar', 80.41000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-529.pth.tar', 80.38200002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-519.pth.tar', 80.36799992431641)

Train: 533 [   0/1251 (  0%)]  Loss: 3.301 (3.30)  Time: 2.288s,  447.52/s  (2.288s,  447.52/s)  LR: 4.015e-05  Data: 1.553 (1.553)
Train: 533 [  50/1251 (  4%)]  Loss: 2.837 (3.07)  Time: 0.816s, 1255.59/s  (0.824s, 1242.79/s)  LR: 4.015e-05  Data: 0.011 (0.044)
Train: 533 [ 100/1251 (  8%)]  Loss: 2.955 (3.03)  Time: 0.811s, 1262.06/s  (0.810s, 1263.98/s)  LR: 4.015e-05  Data: 0.009 (0.027)
Train: 533 [ 150/1251 ( 12%)]  Loss: 3.022 (3.03)  Time: 0.772s, 1325.71/s  (0.809s, 1265.93/s)  LR: 4.015e-05  Data: 0.010 (0.022)
Train: 533 [ 200/1251 ( 16%)]  Loss: 3.014 (3.03)  Time: 0.776s, 1320.16/s  (0.804s, 1273.77/s)  LR: 4.015e-05  Data: 0.010 (0.019)
Train: 533 [ 250/1251 ( 20%)]  Loss: 3.115 (3.04)  Time: 0.774s, 1323.74/s  (0.802s, 1277.50/s)  LR: 4.015e-05  Data: 0.009 (0.017)
Train: 533 [ 300/1251 ( 24%)]  Loss: 3.038 (3.04)  Time: 0.798s, 1283.01/s  (0.800s, 1280.51/s)  LR: 4.015e-05  Data: 0.009 (0.016)
Train: 533 [ 350/1251 ( 28%)]  Loss: 2.926 (3.03)  Time: 0.812s, 1261.29/s  (0.798s, 1283.92/s)  LR: 4.015e-05  Data: 0.010 (0.015)
Train: 533 [ 400/1251 ( 32%)]  Loss: 2.909 (3.01)  Time: 0.807s, 1269.56/s  (0.800s, 1280.39/s)  LR: 4.015e-05  Data: 0.010 (0.014)
Train: 533 [ 450/1251 ( 36%)]  Loss: 3.051 (3.02)  Time: 0.807s, 1269.46/s  (0.800s, 1279.93/s)  LR: 4.015e-05  Data: 0.010 (0.014)
Train: 533 [ 500/1251 ( 40%)]  Loss: 2.895 (3.01)  Time: 0.797s, 1284.90/s  (0.800s, 1280.16/s)  LR: 4.015e-05  Data: 0.012 (0.013)
Train: 533 [ 550/1251 ( 44%)]  Loss: 2.949 (3.00)  Time: 0.813s, 1259.70/s  (0.799s, 1281.43/s)  LR: 4.015e-05  Data: 0.010 (0.013)
Train: 533 [ 600/1251 ( 48%)]  Loss: 2.724 (2.98)  Time: 0.773s, 1324.83/s  (0.798s, 1283.57/s)  LR: 4.015e-05  Data: 0.010 (0.013)
Train: 533 [ 650/1251 ( 52%)]  Loss: 3.132 (2.99)  Time: 0.786s, 1302.47/s  (0.797s, 1285.49/s)  LR: 4.015e-05  Data: 0.010 (0.012)
Train: 533 [ 700/1251 ( 56%)]  Loss: 3.042 (2.99)  Time: 0.774s, 1323.30/s  (0.797s, 1285.00/s)  LR: 4.015e-05  Data: 0.010 (0.012)
Train: 533 [ 750/1251 ( 60%)]  Loss: 2.761 (2.98)  Time: 0.785s, 1304.77/s  (0.796s, 1285.96/s)  LR: 4.015e-05  Data: 0.010 (0.012)
Train: 533 [ 800/1251 ( 64%)]  Loss: 3.242 (2.99)  Time: 0.774s, 1323.69/s  (0.796s, 1285.86/s)  LR: 4.015e-05  Data: 0.009 (0.012)
Train: 533 [ 850/1251 ( 68%)]  Loss: 3.120 (3.00)  Time: 0.815s, 1256.62/s  (0.797s, 1285.01/s)  LR: 4.015e-05  Data: 0.011 (0.012)
Train: 533 [ 900/1251 ( 72%)]  Loss: 2.898 (3.00)  Time: 0.799s, 1280.85/s  (0.797s, 1285.61/s)  LR: 4.015e-05  Data: 0.009 (0.012)
Train: 533 [ 950/1251 ( 76%)]  Loss: 3.124 (3.00)  Time: 0.774s, 1322.25/s  (0.796s, 1286.21/s)  LR: 4.015e-05  Data: 0.009 (0.012)
Train: 533 [1000/1251 ( 80%)]  Loss: 3.064 (3.01)  Time: 0.773s, 1323.97/s  (0.795s, 1287.42/s)  LR: 4.015e-05  Data: 0.009 (0.012)
Train: 533 [1050/1251 ( 84%)]  Loss: 3.358 (3.02)  Time: 0.773s, 1325.26/s  (0.795s, 1288.43/s)  LR: 4.015e-05  Data: 0.010 (0.011)
Train: 533 [1100/1251 ( 88%)]  Loss: 2.608 (3.00)  Time: 0.811s, 1262.64/s  (0.794s, 1289.33/s)  LR: 4.015e-05  Data: 0.010 (0.011)
Train: 533 [1150/1251 ( 92%)]  Loss: 2.966 (3.00)  Time: 0.808s, 1266.90/s  (0.794s, 1289.37/s)  LR: 4.015e-05  Data: 0.010 (0.011)
Train: 533 [1200/1251 ( 96%)]  Loss: 2.899 (3.00)  Time: 0.773s, 1325.32/s  (0.794s, 1289.69/s)  LR: 4.015e-05  Data: 0.010 (0.011)
Train: 533 [1250/1251 (100%)]  Loss: 2.984 (3.00)  Time: 0.797s, 1285.14/s  (0.794s, 1290.05/s)  LR: 4.015e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.645 (1.645)  Loss:  0.6089 (0.6089)  Acc@1: 92.3828 (92.3828)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.194 (0.562)  Loss:  0.6992 (1.0383)  Acc@1: 87.3821 (80.5600)  Acc@5: 98.3491 (95.3880)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-530.pth.tar', 80.61400002929688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-526.pth.tar', 80.57199997802735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-533.pth.tar', 80.55999994873046)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-527.pth.tar', 80.50000000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-532.pth.tar', 80.4720000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-522.pth.tar', 80.45000005371094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-524.pth.tar', 80.41799997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-523.pth.tar', 80.4179999243164)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-518.pth.tar', 80.41000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-529.pth.tar', 80.38200002685547)

Train: 534 [   0/1251 (  0%)]  Loss: 3.075 (3.08)  Time: 2.551s,  401.40/s  (2.551s,  401.40/s)  LR: 3.926e-05  Data: 1.828 (1.828)
Train: 534 [  50/1251 (  4%)]  Loss: 2.755 (2.92)  Time: 0.807s, 1268.89/s  (0.844s, 1212.65/s)  LR: 3.926e-05  Data: 0.009 (0.045)
Train: 534 [ 100/1251 (  8%)]  Loss: 2.687 (2.84)  Time: 0.773s, 1324.50/s  (0.822s, 1246.30/s)  LR: 3.926e-05  Data: 0.009 (0.028)
Train: 534 [ 150/1251 ( 12%)]  Loss: 2.905 (2.86)  Time: 0.773s, 1324.66/s  (0.807s, 1268.70/s)  LR: 3.926e-05  Data: 0.010 (0.022)
Train: 534 [ 200/1251 ( 16%)]  Loss: 2.780 (2.84)  Time: 0.773s, 1324.15/s  (0.805s, 1272.03/s)  LR: 3.926e-05  Data: 0.009 (0.019)
Train: 534 [ 250/1251 ( 20%)]  Loss: 2.802 (2.83)  Time: 0.815s, 1256.20/s  (0.803s, 1276.01/s)  LR: 3.926e-05  Data: 0.011 (0.017)
Train: 534 [ 300/1251 ( 24%)]  Loss: 2.901 (2.84)  Time: 0.784s, 1305.89/s  (0.800s, 1280.54/s)  LR: 3.926e-05  Data: 0.013 (0.016)
Train: 534 [ 350/1251 ( 28%)]  Loss: 3.037 (2.87)  Time: 0.808s, 1267.57/s  (0.798s, 1283.21/s)  LR: 3.926e-05  Data: 0.009 (0.015)
Train: 534 [ 400/1251 ( 32%)]  Loss: 2.504 (2.83)  Time: 0.784s, 1305.90/s  (0.798s, 1283.85/s)  LR: 3.926e-05  Data: 0.010 (0.014)
Train: 534 [ 450/1251 ( 36%)]  Loss: 3.018 (2.85)  Time: 0.809s, 1266.05/s  (0.798s, 1283.16/s)  LR: 3.926e-05  Data: 0.009 (0.014)
Train: 534 [ 500/1251 ( 40%)]  Loss: 2.919 (2.85)  Time: 0.812s, 1261.21/s  (0.799s, 1282.29/s)  LR: 3.926e-05  Data: 0.010 (0.013)
Train: 534 [ 550/1251 ( 44%)]  Loss: 2.837 (2.85)  Time: 0.809s, 1265.13/s  (0.798s, 1282.41/s)  LR: 3.926e-05  Data: 0.009 (0.013)
Train: 534 [ 600/1251 ( 48%)]  Loss: 3.187 (2.88)  Time: 0.808s, 1268.09/s  (0.798s, 1282.74/s)  LR: 3.926e-05  Data: 0.009 (0.013)
Train: 534 [ 650/1251 ( 52%)]  Loss: 3.261 (2.90)  Time: 0.806s, 1270.08/s  (0.798s, 1283.91/s)  LR: 3.926e-05  Data: 0.009 (0.013)
Train: 534 [ 700/1251 ( 56%)]  Loss: 2.637 (2.89)  Time: 0.844s, 1212.89/s  (0.797s, 1284.46/s)  LR: 3.926e-05  Data: 0.010 (0.012)
Train: 534 [ 750/1251 ( 60%)]  Loss: 3.052 (2.90)  Time: 0.773s, 1325.50/s  (0.796s, 1285.63/s)  LR: 3.926e-05  Data: 0.010 (0.012)
Train: 534 [ 800/1251 ( 64%)]  Loss: 2.664 (2.88)  Time: 0.770s, 1329.67/s  (0.796s, 1286.83/s)  LR: 3.926e-05  Data: 0.010 (0.012)
Train: 534 [ 850/1251 ( 68%)]  Loss: 3.336 (2.91)  Time: 0.773s, 1324.01/s  (0.796s, 1286.36/s)  LR: 3.926e-05  Data: 0.010 (0.012)
Train: 534 [ 900/1251 ( 72%)]  Loss: 2.680 (2.90)  Time: 0.782s, 1308.90/s  (0.795s, 1287.80/s)  LR: 3.926e-05  Data: 0.009 (0.012)
Train: 534 [ 950/1251 ( 76%)]  Loss: 3.110 (2.91)  Time: 0.808s, 1267.95/s  (0.795s, 1288.36/s)  LR: 3.926e-05  Data: 0.010 (0.012)
Train: 534 [1000/1251 ( 80%)]  Loss: 3.092 (2.92)  Time: 0.815s, 1256.66/s  (0.795s, 1288.53/s)  LR: 3.926e-05  Data: 0.010 (0.012)
Train: 534 [1050/1251 ( 84%)]  Loss: 2.957 (2.92)  Time: 0.772s, 1326.30/s  (0.794s, 1288.93/s)  LR: 3.926e-05  Data: 0.010 (0.012)
Train: 534 [1100/1251 ( 88%)]  Loss: 2.993 (2.92)  Time: 0.775s, 1321.16/s  (0.794s, 1289.69/s)  LR: 3.926e-05  Data: 0.010 (0.011)
Train: 534 [1150/1251 ( 92%)]  Loss: 3.191 (2.93)  Time: 0.770s, 1329.48/s  (0.794s, 1290.10/s)  LR: 3.926e-05  Data: 0.010 (0.011)
Train: 534 [1200/1251 ( 96%)]  Loss: 2.860 (2.93)  Time: 0.780s, 1313.47/s  (0.794s, 1290.40/s)  LR: 3.926e-05  Data: 0.010 (0.011)
Train: 534 [1250/1251 (100%)]  Loss: 2.979 (2.93)  Time: 0.794s, 1290.00/s  (0.793s, 1290.57/s)  LR: 3.926e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.538 (1.538)  Loss:  0.5850 (0.5850)  Acc@1: 92.3828 (92.3828)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.194 (0.563)  Loss:  0.7168 (1.0458)  Acc@1: 87.5000 (80.5240)  Acc@5: 98.2311 (95.4220)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-530.pth.tar', 80.61400002929688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-526.pth.tar', 80.57199997802735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-533.pth.tar', 80.55999994873046)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-534.pth.tar', 80.524)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-527.pth.tar', 80.50000000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-532.pth.tar', 80.4720000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-522.pth.tar', 80.45000005371094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-524.pth.tar', 80.41799997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-523.pth.tar', 80.4179999243164)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-518.pth.tar', 80.41000002685547)

Train: 535 [   0/1251 (  0%)]  Loss: 3.319 (3.32)  Time: 2.403s,  426.20/s  (2.403s,  426.20/s)  LR: 3.839e-05  Data: 1.672 (1.672)
Train: 535 [  50/1251 (  4%)]  Loss: 2.950 (3.13)  Time: 0.811s, 1263.20/s  (0.822s, 1245.31/s)  LR: 3.839e-05  Data: 0.012 (0.042)
Train: 535 [ 100/1251 (  8%)]  Loss: 2.904 (3.06)  Time: 0.823s, 1244.93/s  (0.805s, 1271.27/s)  LR: 3.839e-05  Data: 0.009 (0.026)
Train: 535 [ 150/1251 ( 12%)]  Loss: 3.218 (3.10)  Time: 0.810s, 1264.33/s  (0.805s, 1271.43/s)  LR: 3.839e-05  Data: 0.010 (0.021)
Train: 535 [ 200/1251 ( 16%)]  Loss: 3.301 (3.14)  Time: 0.774s, 1322.85/s  (0.801s, 1278.42/s)  LR: 3.839e-05  Data: 0.010 (0.018)
Train: 535 [ 250/1251 ( 20%)]  Loss: 2.993 (3.11)  Time: 0.784s, 1305.59/s  (0.799s, 1280.97/s)  LR: 3.839e-05  Data: 0.011 (0.016)
Train: 535 [ 300/1251 ( 24%)]  Loss: 2.840 (3.08)  Time: 0.773s, 1324.95/s  (0.796s, 1285.79/s)  LR: 3.839e-05  Data: 0.010 (0.015)
Train: 535 [ 350/1251 ( 28%)]  Loss: 3.038 (3.07)  Time: 0.808s, 1266.71/s  (0.798s, 1282.74/s)  LR: 3.839e-05  Data: 0.010 (0.015)
Train: 535 [ 400/1251 ( 32%)]  Loss: 2.812 (3.04)  Time: 0.779s, 1314.15/s  (0.799s, 1281.78/s)  LR: 3.839e-05  Data: 0.013 (0.014)
Train: 535 [ 450/1251 ( 36%)]  Loss: 2.910 (3.03)  Time: 0.773s, 1324.82/s  (0.798s, 1282.98/s)  LR: 3.839e-05  Data: 0.009 (0.013)
Train: 535 [ 500/1251 ( 40%)]  Loss: 2.988 (3.02)  Time: 0.802s, 1276.16/s  (0.797s, 1284.64/s)  LR: 3.839e-05  Data: 0.009 (0.013)
Train: 535 [ 550/1251 ( 44%)]  Loss: 2.791 (3.01)  Time: 0.772s, 1326.97/s  (0.797s, 1285.41/s)  LR: 3.839e-05  Data: 0.010 (0.013)
Train: 535 [ 600/1251 ( 48%)]  Loss: 3.117 (3.01)  Time: 0.772s, 1326.94/s  (0.798s, 1283.37/s)  LR: 3.839e-05  Data: 0.009 (0.013)
Train: 535 [ 650/1251 ( 52%)]  Loss: 2.788 (3.00)  Time: 0.807s, 1269.28/s  (0.797s, 1284.56/s)  LR: 3.839e-05  Data: 0.010 (0.012)
Train: 535 [ 700/1251 ( 56%)]  Loss: 2.921 (2.99)  Time: 0.773s, 1324.80/s  (0.796s, 1285.96/s)  LR: 3.839e-05  Data: 0.010 (0.012)
Train: 535 [ 750/1251 ( 60%)]  Loss: 3.219 (3.01)  Time: 0.786s, 1302.88/s  (0.796s, 1287.16/s)  LR: 3.839e-05  Data: 0.010 (0.012)
Train: 535 [ 800/1251 ( 64%)]  Loss: 2.626 (2.98)  Time: 0.789s, 1297.74/s  (0.795s, 1287.40/s)  LR: 3.839e-05  Data: 0.010 (0.012)
Train: 535 [ 850/1251 ( 68%)]  Loss: 2.786 (2.97)  Time: 0.772s, 1327.18/s  (0.795s, 1288.17/s)  LR: 3.839e-05  Data: 0.009 (0.012)
Train: 535 [ 900/1251 ( 72%)]  Loss: 3.129 (2.98)  Time: 0.808s, 1266.62/s  (0.795s, 1288.19/s)  LR: 3.839e-05  Data: 0.009 (0.012)
Train: 535 [ 950/1251 ( 76%)]  Loss: 3.072 (2.99)  Time: 0.771s, 1328.12/s  (0.795s, 1288.64/s)  LR: 3.839e-05  Data: 0.010 (0.012)
Train: 535 [1000/1251 ( 80%)]  Loss: 2.983 (2.99)  Time: 0.775s, 1320.65/s  (0.795s, 1288.64/s)  LR: 3.839e-05  Data: 0.009 (0.011)
Train: 535 [1050/1251 ( 84%)]  Loss: 2.970 (2.99)  Time: 0.786s, 1302.01/s  (0.795s, 1288.51/s)  LR: 3.839e-05  Data: 0.013 (0.011)
Train: 535 [1100/1251 ( 88%)]  Loss: 3.007 (2.99)  Time: 0.782s, 1310.06/s  (0.794s, 1289.31/s)  LR: 3.839e-05  Data: 0.010 (0.011)
Train: 535 [1150/1251 ( 92%)]  Loss: 2.846 (2.98)  Time: 0.830s, 1234.08/s  (0.794s, 1289.28/s)  LR: 3.839e-05  Data: 0.015 (0.011)
Train: 535 [1200/1251 ( 96%)]  Loss: 3.107 (2.99)  Time: 0.813s, 1259.43/s  (0.795s, 1288.72/s)  LR: 3.839e-05  Data: 0.009 (0.011)
Train: 535 [1250/1251 (100%)]  Loss: 3.175 (2.99)  Time: 0.797s, 1284.97/s  (0.795s, 1288.00/s)  LR: 3.839e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.562 (1.562)  Loss:  0.5400 (0.5400)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.194 (0.562)  Loss:  0.6553 (1.0067)  Acc@1: 87.5000 (80.6740)  Acc@5: 98.3491 (95.4400)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-535.pth.tar', 80.67399987060547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-530.pth.tar', 80.61400002929688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-526.pth.tar', 80.57199997802735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-533.pth.tar', 80.55999994873046)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-534.pth.tar', 80.524)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-527.pth.tar', 80.50000000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-532.pth.tar', 80.4720000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-522.pth.tar', 80.45000005371094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-524.pth.tar', 80.41799997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-523.pth.tar', 80.4179999243164)

Train: 536 [   0/1251 (  0%)]  Loss: 2.656 (2.66)  Time: 2.140s,  478.41/s  (2.140s,  478.41/s)  LR: 3.753e-05  Data: 1.407 (1.407)
Train: 536 [  50/1251 (  4%)]  Loss: 2.977 (2.82)  Time: 0.773s, 1324.73/s  (0.822s, 1246.04/s)  LR: 3.753e-05  Data: 0.010 (0.041)
Train: 536 [ 100/1251 (  8%)]  Loss: 2.603 (2.75)  Time: 0.791s, 1295.28/s  (0.805s, 1271.39/s)  LR: 3.753e-05  Data: 0.009 (0.025)
Train: 536 [ 150/1251 ( 12%)]  Loss: 3.024 (2.81)  Time: 0.809s, 1265.17/s  (0.802s, 1276.09/s)  LR: 3.753e-05  Data: 0.010 (0.020)
Train: 536 [ 200/1251 ( 16%)]  Loss: 2.837 (2.82)  Time: 0.777s, 1317.96/s  (0.800s, 1279.89/s)  LR: 3.753e-05  Data: 0.010 (0.018)
Train: 536 [ 250/1251 ( 20%)]  Loss: 2.836 (2.82)  Time: 0.774s, 1323.11/s  (0.797s, 1284.86/s)  LR: 3.753e-05  Data: 0.009 (0.016)
Train: 536 [ 300/1251 ( 24%)]  Loss: 3.296 (2.89)  Time: 0.808s, 1266.93/s  (0.795s, 1288.11/s)  LR: 3.753e-05  Data: 0.010 (0.015)
Train: 536 [ 350/1251 ( 28%)]  Loss: 3.212 (2.93)  Time: 0.773s, 1324.50/s  (0.795s, 1287.55/s)  LR: 3.753e-05  Data: 0.010 (0.015)
Train: 536 [ 400/1251 ( 32%)]  Loss: 2.731 (2.91)  Time: 0.812s, 1260.43/s  (0.795s, 1287.67/s)  LR: 3.753e-05  Data: 0.010 (0.014)
Train: 536 [ 450/1251 ( 36%)]  Loss: 3.036 (2.92)  Time: 0.816s, 1254.68/s  (0.796s, 1285.88/s)  LR: 3.753e-05  Data: 0.011 (0.014)
Train: 536 [ 500/1251 ( 40%)]  Loss: 2.798 (2.91)  Time: 0.776s, 1320.03/s  (0.797s, 1285.15/s)  LR: 3.753e-05  Data: 0.010 (0.013)
Train: 536 [ 550/1251 ( 44%)]  Loss: 2.948 (2.91)  Time: 0.770s, 1330.31/s  (0.796s, 1286.53/s)  LR: 3.753e-05  Data: 0.010 (0.013)
Train: 536 [ 600/1251 ( 48%)]  Loss: 3.039 (2.92)  Time: 0.817s, 1253.86/s  (0.796s, 1286.26/s)  LR: 3.753e-05  Data: 0.010 (0.013)
Train: 536 [ 650/1251 ( 52%)]  Loss: 2.883 (2.92)  Time: 0.772s, 1326.65/s  (0.796s, 1287.20/s)  LR: 3.753e-05  Data: 0.010 (0.013)
Train: 536 [ 700/1251 ( 56%)]  Loss: 2.890 (2.92)  Time: 0.842s, 1216.11/s  (0.794s, 1289.31/s)  LR: 3.753e-05  Data: 0.009 (0.012)
Train: 536 [ 750/1251 ( 60%)]  Loss: 3.375 (2.95)  Time: 0.772s, 1326.77/s  (0.794s, 1289.24/s)  LR: 3.753e-05  Data: 0.010 (0.012)
Train: 536 [ 800/1251 ( 64%)]  Loss: 2.591 (2.93)  Time: 0.785s, 1303.71/s  (0.794s, 1289.40/s)  LR: 3.753e-05  Data: 0.010 (0.012)
Train: 536 [ 850/1251 ( 68%)]  Loss: 3.204 (2.94)  Time: 0.817s, 1253.06/s  (0.794s, 1289.05/s)  LR: 3.753e-05  Data: 0.009 (0.012)
Train: 536 [ 900/1251 ( 72%)]  Loss: 2.854 (2.94)  Time: 0.778s, 1316.53/s  (0.794s, 1289.89/s)  LR: 3.753e-05  Data: 0.010 (0.012)
Train: 536 [ 950/1251 ( 76%)]  Loss: 2.721 (2.93)  Time: 0.825s, 1240.58/s  (0.794s, 1289.22/s)  LR: 3.753e-05  Data: 0.009 (0.012)
Train: 536 [1000/1251 ( 80%)]  Loss: 2.763 (2.92)  Time: 0.816s, 1255.32/s  (0.795s, 1288.71/s)  LR: 3.753e-05  Data: 0.011 (0.012)
Train: 536 [1050/1251 ( 84%)]  Loss: 3.082 (2.93)  Time: 0.775s, 1321.47/s  (0.794s, 1289.00/s)  LR: 3.753e-05  Data: 0.010 (0.012)
Train: 536 [1100/1251 ( 88%)]  Loss: 3.014 (2.93)  Time: 0.848s, 1207.43/s  (0.794s, 1289.97/s)  LR: 3.753e-05  Data: 0.014 (0.011)
Train: 536 [1150/1251 ( 92%)]  Loss: 3.149 (2.94)  Time: 0.823s, 1244.70/s  (0.794s, 1290.48/s)  LR: 3.753e-05  Data: 0.009 (0.011)
Train: 536 [1200/1251 ( 96%)]  Loss: 2.687 (2.93)  Time: 0.785s, 1305.18/s  (0.794s, 1290.13/s)  LR: 3.753e-05  Data: 0.010 (0.011)
Train: 536 [1250/1251 (100%)]  Loss: 2.988 (2.93)  Time: 0.760s, 1347.01/s  (0.794s, 1290.38/s)  LR: 3.753e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.521 (1.521)  Loss:  0.6392 (0.6392)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.194 (0.571)  Loss:  0.7515 (1.0789)  Acc@1: 87.1462 (80.3980)  Acc@5: 97.9953 (95.3000)
Train: 537 [   0/1251 (  0%)]  Loss: 2.791 (2.79)  Time: 2.245s,  456.17/s  (2.245s,  456.17/s)  LR: 3.669e-05  Data: 1.491 (1.491)
Train: 537 [  50/1251 (  4%)]  Loss: 3.099 (2.95)  Time: 0.774s, 1322.27/s  (0.824s, 1243.24/s)  LR: 3.669e-05  Data: 0.009 (0.045)
Train: 537 [ 100/1251 (  8%)]  Loss: 3.122 (3.00)  Time: 0.774s, 1322.67/s  (0.804s, 1274.11/s)  LR: 3.669e-05  Data: 0.009 (0.028)
Train: 537 [ 150/1251 ( 12%)]  Loss: 2.604 (2.90)  Time: 0.784s, 1305.38/s  (0.797s, 1284.59/s)  LR: 3.669e-05  Data: 0.009 (0.022)
Train: 537 [ 200/1251 ( 16%)]  Loss: 3.002 (2.92)  Time: 0.815s, 1256.00/s  (0.798s, 1282.89/s)  LR: 3.669e-05  Data: 0.011 (0.019)
Train: 537 [ 250/1251 ( 20%)]  Loss: 3.138 (2.96)  Time: 0.774s, 1323.84/s  (0.797s, 1284.92/s)  LR: 3.669e-05  Data: 0.009 (0.017)
Train: 537 [ 300/1251 ( 24%)]  Loss: 2.971 (2.96)  Time: 0.782s, 1308.63/s  (0.794s, 1289.07/s)  LR: 3.669e-05  Data: 0.009 (0.016)
Train: 537 [ 350/1251 ( 28%)]  Loss: 3.036 (2.97)  Time: 0.807s, 1268.47/s  (0.795s, 1288.29/s)  LR: 3.669e-05  Data: 0.009 (0.015)
Train: 537 [ 400/1251 ( 32%)]  Loss: 3.147 (2.99)  Time: 0.785s, 1303.64/s  (0.796s, 1287.06/s)  LR: 3.669e-05  Data: 0.010 (0.015)
Train: 537 [ 450/1251 ( 36%)]  Loss: 2.912 (2.98)  Time: 0.803s, 1275.01/s  (0.794s, 1289.39/s)  LR: 3.669e-05  Data: 0.009 (0.014)
Train: 537 [ 500/1251 ( 40%)]  Loss: 2.962 (2.98)  Time: 0.785s, 1304.90/s  (0.793s, 1291.33/s)  LR: 3.669e-05  Data: 0.010 (0.014)
Train: 537 [ 550/1251 ( 44%)]  Loss: 3.226 (3.00)  Time: 0.809s, 1266.01/s  (0.793s, 1291.21/s)  LR: 3.669e-05  Data: 0.009 (0.013)
Train: 537 [ 600/1251 ( 48%)]  Loss: 3.278 (3.02)  Time: 0.781s, 1310.34/s  (0.793s, 1290.62/s)  LR: 3.669e-05  Data: 0.010 (0.013)
Train: 537 [ 650/1251 ( 52%)]  Loss: 3.008 (3.02)  Time: 0.807s, 1268.56/s  (0.793s, 1291.84/s)  LR: 3.669e-05  Data: 0.010 (0.013)
Train: 537 [ 700/1251 ( 56%)]  Loss: 3.101 (3.03)  Time: 0.776s, 1319.40/s  (0.792s, 1292.62/s)  LR: 3.669e-05  Data: 0.010 (0.013)
Train: 537 [ 750/1251 ( 60%)]  Loss: 3.091 (3.03)  Time: 0.783s, 1307.74/s  (0.793s, 1291.70/s)  LR: 3.669e-05  Data: 0.010 (0.012)
Train: 537 [ 800/1251 ( 64%)]  Loss: 2.881 (3.02)  Time: 0.783s, 1307.62/s  (0.792s, 1292.58/s)  LR: 3.669e-05  Data: 0.010 (0.012)
Train: 537 [ 850/1251 ( 68%)]  Loss: 3.048 (3.02)  Time: 0.819s, 1250.12/s  (0.792s, 1292.78/s)  LR: 3.669e-05  Data: 0.010 (0.012)
Train: 537 [ 900/1251 ( 72%)]  Loss: 3.104 (3.03)  Time: 0.813s, 1258.77/s  (0.793s, 1291.63/s)  LR: 3.669e-05  Data: 0.011 (0.012)
Train: 537 [ 950/1251 ( 76%)]  Loss: 3.002 (3.03)  Time: 0.787s, 1300.85/s  (0.793s, 1292.09/s)  LR: 3.669e-05  Data: 0.010 (0.012)
Train: 537 [1000/1251 ( 80%)]  Loss: 3.001 (3.02)  Time: 0.772s, 1325.70/s  (0.792s, 1292.61/s)  LR: 3.669e-05  Data: 0.010 (0.012)
Train: 537 [1050/1251 ( 84%)]  Loss: 3.019 (3.02)  Time: 0.775s, 1321.78/s  (0.792s, 1292.71/s)  LR: 3.669e-05  Data: 0.010 (0.012)
Train: 537 [1100/1251 ( 88%)]  Loss: 3.048 (3.03)  Time: 0.781s, 1311.13/s  (0.792s, 1292.93/s)  LR: 3.669e-05  Data: 0.010 (0.012)
Train: 537 [1150/1251 ( 92%)]  Loss: 3.037 (3.03)  Time: 0.782s, 1310.13/s  (0.792s, 1292.55/s)  LR: 3.669e-05  Data: 0.010 (0.012)
Train: 537 [1200/1251 ( 96%)]  Loss: 2.743 (3.01)  Time: 0.807s, 1268.82/s  (0.793s, 1292.10/s)  LR: 3.669e-05  Data: 0.009 (0.011)
Train: 537 [1250/1251 (100%)]  Loss: 2.849 (3.01)  Time: 0.767s, 1335.28/s  (0.792s, 1292.38/s)  LR: 3.669e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.699 (1.699)  Loss:  0.6045 (0.6045)  Acc@1: 92.6758 (92.6758)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.194 (0.566)  Loss:  0.7007 (1.0447)  Acc@1: 87.2642 (80.6020)  Acc@5: 98.1132 (95.3780)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-535.pth.tar', 80.67399987060547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-530.pth.tar', 80.61400002929688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-537.pth.tar', 80.60200002685546)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-526.pth.tar', 80.57199997802735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-533.pth.tar', 80.55999994873046)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-534.pth.tar', 80.524)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-527.pth.tar', 80.50000000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-532.pth.tar', 80.4720000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-522.pth.tar', 80.45000005371094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-524.pth.tar', 80.41799997558594)

Train: 538 [   0/1251 (  0%)]  Loss: 3.023 (3.02)  Time: 2.415s,  424.02/s  (2.415s,  424.02/s)  LR: 3.585e-05  Data: 1.689 (1.689)
Train: 538 [  50/1251 (  4%)]  Loss: 2.699 (2.86)  Time: 0.773s, 1325.35/s  (0.827s, 1237.87/s)  LR: 3.585e-05  Data: 0.010 (0.049)
Train: 538 [ 100/1251 (  8%)]  Loss: 2.961 (2.89)  Time: 0.773s, 1325.28/s  (0.804s, 1273.08/s)  LR: 3.585e-05  Data: 0.009 (0.030)
Train: 538 [ 150/1251 ( 12%)]  Loss: 3.206 (2.97)  Time: 0.775s, 1321.18/s  (0.799s, 1281.46/s)  LR: 3.585e-05  Data: 0.011 (0.023)
Train: 538 [ 200/1251 ( 16%)]  Loss: 2.582 (2.89)  Time: 0.815s, 1255.82/s  (0.797s, 1284.71/s)  LR: 3.585e-05  Data: 0.009 (0.020)
Train: 538 [ 250/1251 ( 20%)]  Loss: 3.116 (2.93)  Time: 0.773s, 1325.13/s  (0.796s, 1287.05/s)  LR: 3.585e-05  Data: 0.010 (0.018)
Train: 538 [ 300/1251 ( 24%)]  Loss: 2.722 (2.90)  Time: 0.776s, 1318.99/s  (0.794s, 1290.27/s)  LR: 3.585e-05  Data: 0.010 (0.017)
Train: 538 [ 350/1251 ( 28%)]  Loss: 2.767 (2.88)  Time: 0.831s, 1232.59/s  (0.793s, 1291.44/s)  LR: 3.585e-05  Data: 0.009 (0.016)
Train: 538 [ 400/1251 ( 32%)]  Loss: 2.492 (2.84)  Time: 0.771s, 1327.88/s  (0.792s, 1293.53/s)  LR: 3.585e-05  Data: 0.009 (0.015)
Train: 538 [ 450/1251 ( 36%)]  Loss: 2.897 (2.85)  Time: 0.806s, 1270.49/s  (0.791s, 1294.01/s)  LR: 3.585e-05  Data: 0.009 (0.014)
Train: 538 [ 500/1251 ( 40%)]  Loss: 2.781 (2.84)  Time: 0.770s, 1329.11/s  (0.792s, 1292.44/s)  LR: 3.585e-05  Data: 0.009 (0.014)
Train: 538 [ 550/1251 ( 44%)]  Loss: 2.802 (2.84)  Time: 0.779s, 1314.68/s  (0.792s, 1293.30/s)  LR: 3.585e-05  Data: 0.010 (0.014)
Train: 538 [ 600/1251 ( 48%)]  Loss: 2.971 (2.85)  Time: 0.814s, 1257.66/s  (0.793s, 1291.85/s)  LR: 3.585e-05  Data: 0.011 (0.013)
Train: 538 [ 650/1251 ( 52%)]  Loss: 2.860 (2.85)  Time: 0.772s, 1326.29/s  (0.792s, 1292.93/s)  LR: 3.585e-05  Data: 0.010 (0.013)
Train: 538 [ 700/1251 ( 56%)]  Loss: 2.865 (2.85)  Time: 0.773s, 1325.14/s  (0.792s, 1292.93/s)  LR: 3.585e-05  Data: 0.010 (0.013)
Train: 538 [ 750/1251 ( 60%)]  Loss: 3.301 (2.88)  Time: 0.776s, 1319.82/s  (0.792s, 1293.38/s)  LR: 3.585e-05  Data: 0.009 (0.013)
Train: 538 [ 800/1251 ( 64%)]  Loss: 3.115 (2.89)  Time: 0.781s, 1310.93/s  (0.791s, 1294.46/s)  LR: 3.585e-05  Data: 0.009 (0.012)
Train: 538 [ 850/1251 ( 68%)]  Loss: 2.905 (2.89)  Time: 0.773s, 1324.22/s  (0.791s, 1294.44/s)  LR: 3.585e-05  Data: 0.010 (0.012)
Train: 538 [ 900/1251 ( 72%)]  Loss: 2.923 (2.89)  Time: 0.817s, 1253.78/s  (0.791s, 1294.30/s)  LR: 3.585e-05  Data: 0.011 (0.012)
Train: 538 [ 950/1251 ( 76%)]  Loss: 2.728 (2.89)  Time: 0.772s, 1326.17/s  (0.791s, 1294.16/s)  LR: 3.585e-05  Data: 0.009 (0.012)
Train: 538 [1000/1251 ( 80%)]  Loss: 2.836 (2.88)  Time: 0.769s, 1330.79/s  (0.791s, 1294.08/s)  LR: 3.585e-05  Data: 0.009 (0.012)
Train: 538 [1050/1251 ( 84%)]  Loss: 3.111 (2.89)  Time: 0.773s, 1325.01/s  (0.791s, 1294.63/s)  LR: 3.585e-05  Data: 0.009 (0.012)
Train: 538 [1100/1251 ( 88%)]  Loss: 3.104 (2.90)  Time: 0.819s, 1250.24/s  (0.791s, 1294.50/s)  LR: 3.585e-05  Data: 0.013 (0.012)
Train: 538 [1150/1251 ( 92%)]  Loss: 3.060 (2.91)  Time: 0.773s, 1324.75/s  (0.792s, 1293.64/s)  LR: 3.585e-05  Data: 0.010 (0.012)
Train: 538 [1200/1251 ( 96%)]  Loss: 2.727 (2.90)  Time: 0.784s, 1305.76/s  (0.792s, 1292.70/s)  LR: 3.585e-05  Data: 0.010 (0.012)
Train: 538 [1250/1251 (100%)]  Loss: 2.844 (2.90)  Time: 0.759s, 1348.34/s  (0.792s, 1292.86/s)  LR: 3.585e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.518 (1.518)  Loss:  0.6265 (0.6265)  Acc@1: 93.1641 (93.1641)  Acc@5: 98.7305 (98.7305)
Test: [  48/48]  Time: 0.194 (0.563)  Loss:  0.7305 (1.0778)  Acc@1: 86.9104 (80.5320)  Acc@5: 98.2311 (95.3420)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-535.pth.tar', 80.67399987060547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-530.pth.tar', 80.61400002929688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-537.pth.tar', 80.60200002685546)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-526.pth.tar', 80.57199997802735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-533.pth.tar', 80.55999994873046)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-538.pth.tar', 80.53199987304687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-534.pth.tar', 80.524)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-527.pth.tar', 80.50000000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-532.pth.tar', 80.4720000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-522.pth.tar', 80.45000005371094)

Train: 539 [   0/1251 (  0%)]  Loss: 2.701 (2.70)  Time: 2.225s,  460.29/s  (2.225s,  460.29/s)  LR: 3.503e-05  Data: 1.471 (1.471)
Train: 539 [  50/1251 (  4%)]  Loss: 3.230 (2.97)  Time: 0.801s, 1279.03/s  (0.820s, 1248.40/s)  LR: 3.503e-05  Data: 0.009 (0.045)
Train: 539 [ 100/1251 (  8%)]  Loss: 2.747 (2.89)  Time: 0.816s, 1255.15/s  (0.802s, 1276.59/s)  LR: 3.503e-05  Data: 0.010 (0.028)
Train: 539 [ 150/1251 ( 12%)]  Loss: 3.099 (2.94)  Time: 0.808s, 1266.69/s  (0.798s, 1282.57/s)  LR: 3.503e-05  Data: 0.009 (0.022)
Train: 539 [ 200/1251 ( 16%)]  Loss: 3.206 (3.00)  Time: 0.774s, 1323.64/s  (0.795s, 1287.83/s)  LR: 3.503e-05  Data: 0.010 (0.019)
Train: 539 [ 250/1251 ( 20%)]  Loss: 3.031 (3.00)  Time: 0.775s, 1321.78/s  (0.792s, 1293.41/s)  LR: 3.503e-05  Data: 0.009 (0.017)
Train: 539 [ 300/1251 ( 24%)]  Loss: 2.539 (2.94)  Time: 0.799s, 1282.02/s  (0.791s, 1294.14/s)  LR: 3.503e-05  Data: 0.010 (0.016)
Train: 539 [ 350/1251 ( 28%)]  Loss: 3.081 (2.95)  Time: 0.771s, 1328.74/s  (0.791s, 1294.50/s)  LR: 3.503e-05  Data: 0.010 (0.015)
Train: 539 [ 400/1251 ( 32%)]  Loss: 2.864 (2.94)  Time: 0.773s, 1324.32/s  (0.790s, 1296.49/s)  LR: 3.503e-05  Data: 0.010 (0.014)
Train: 539 [ 450/1251 ( 36%)]  Loss: 2.904 (2.94)  Time: 0.772s, 1325.83/s  (0.789s, 1297.81/s)  LR: 3.503e-05  Data: 0.009 (0.014)
Train: 539 [ 500/1251 ( 40%)]  Loss: 2.945 (2.94)  Time: 0.786s, 1302.18/s  (0.788s, 1299.27/s)  LR: 3.503e-05  Data: 0.010 (0.013)
Train: 539 [ 550/1251 ( 44%)]  Loss: 3.074 (2.95)  Time: 0.783s, 1307.16/s  (0.787s, 1300.47/s)  LR: 3.503e-05  Data: 0.011 (0.013)
Train: 539 [ 600/1251 ( 48%)]  Loss: 3.008 (2.96)  Time: 0.772s, 1326.20/s  (0.787s, 1300.68/s)  LR: 3.503e-05  Data: 0.010 (0.013)
Train: 539 [ 650/1251 ( 52%)]  Loss: 3.193 (2.97)  Time: 0.771s, 1328.78/s  (0.787s, 1300.94/s)  LR: 3.503e-05  Data: 0.010 (0.013)
Train: 539 [ 700/1251 ( 56%)]  Loss: 2.940 (2.97)  Time: 0.772s, 1325.94/s  (0.788s, 1299.40/s)  LR: 3.503e-05  Data: 0.010 (0.012)
Train: 539 [ 750/1251 ( 60%)]  Loss: 3.035 (2.97)  Time: 0.784s, 1306.89/s  (0.789s, 1297.98/s)  LR: 3.503e-05  Data: 0.010 (0.012)
Train: 539 [ 800/1251 ( 64%)]  Loss: 2.769 (2.96)  Time: 0.850s, 1204.35/s  (0.788s, 1298.75/s)  LR: 3.503e-05  Data: 0.010 (0.012)
Train: 539 [ 850/1251 ( 68%)]  Loss: 2.806 (2.95)  Time: 0.807s, 1268.52/s  (0.789s, 1298.42/s)  LR: 3.503e-05  Data: 0.009 (0.012)
Train: 539 [ 900/1251 ( 72%)]  Loss: 2.782 (2.95)  Time: 0.805s, 1272.26/s  (0.789s, 1297.23/s)  LR: 3.503e-05  Data: 0.010 (0.012)
Train: 539 [ 950/1251 ( 76%)]  Loss: 3.050 (2.95)  Time: 0.772s, 1327.05/s  (0.790s, 1296.21/s)  LR: 3.503e-05  Data: 0.010 (0.012)
Train: 539 [1000/1251 ( 80%)]  Loss: 3.154 (2.96)  Time: 0.776s, 1319.68/s  (0.789s, 1297.11/s)  LR: 3.503e-05  Data: 0.009 (0.012)
Train: 539 [1050/1251 ( 84%)]  Loss: 2.939 (2.96)  Time: 0.790s, 1296.42/s  (0.789s, 1297.42/s)  LR: 3.503e-05  Data: 0.012 (0.012)
Train: 539 [1100/1251 ( 88%)]  Loss: 2.874 (2.96)  Time: 0.808s, 1267.74/s  (0.789s, 1297.22/s)  LR: 3.503e-05  Data: 0.009 (0.011)
Train: 539 [1150/1251 ( 92%)]  Loss: 2.773 (2.95)  Time: 0.774s, 1323.32/s  (0.789s, 1297.49/s)  LR: 3.503e-05  Data: 0.010 (0.011)
Train: 539 [1200/1251 ( 96%)]  Loss: 2.884 (2.95)  Time: 0.817s, 1252.94/s  (0.789s, 1297.53/s)  LR: 3.503e-05  Data: 0.012 (0.011)
Train: 539 [1250/1251 (100%)]  Loss: 3.025 (2.95)  Time: 0.813s, 1259.45/s  (0.790s, 1295.57/s)  LR: 3.503e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.513 (1.513)  Loss:  0.6289 (0.6289)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.194 (0.568)  Loss:  0.7607 (1.0793)  Acc@1: 87.3821 (80.4700)  Acc@5: 98.2311 (95.2840)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-535.pth.tar', 80.67399987060547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-530.pth.tar', 80.61400002929688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-537.pth.tar', 80.60200002685546)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-526.pth.tar', 80.57199997802735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-533.pth.tar', 80.55999994873046)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-538.pth.tar', 80.53199987304687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-534.pth.tar', 80.524)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-527.pth.tar', 80.50000000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-532.pth.tar', 80.4720000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-539.pth.tar', 80.46999994873048)

Train: 540 [   0/1251 (  0%)]  Loss: 2.817 (2.82)  Time: 2.393s,  427.88/s  (2.393s,  427.88/s)  LR: 3.423e-05  Data: 1.623 (1.623)
Train: 540 [  50/1251 (  4%)]  Loss: 3.268 (3.04)  Time: 0.772s, 1325.90/s  (0.826s, 1240.32/s)  LR: 3.423e-05  Data: 0.010 (0.046)
Train: 540 [ 100/1251 (  8%)]  Loss: 3.066 (3.05)  Time: 0.823s, 1244.63/s  (0.806s, 1270.97/s)  LR: 3.423e-05  Data: 0.012 (0.028)
Train: 540 [ 150/1251 ( 12%)]  Loss: 2.948 (3.02)  Time: 0.809s, 1266.29/s  (0.800s, 1279.40/s)  LR: 3.423e-05  Data: 0.010 (0.022)
Train: 540 [ 200/1251 ( 16%)]  Loss: 2.746 (2.97)  Time: 0.774s, 1323.76/s  (0.799s, 1282.21/s)  LR: 3.423e-05  Data: 0.009 (0.019)
Train: 540 [ 250/1251 ( 20%)]  Loss: 2.643 (2.91)  Time: 0.773s, 1324.14/s  (0.795s, 1288.85/s)  LR: 3.423e-05  Data: 0.009 (0.017)
Train: 540 [ 300/1251 ( 24%)]  Loss: 3.015 (2.93)  Time: 0.771s, 1327.52/s  (0.795s, 1288.49/s)  LR: 3.423e-05  Data: 0.009 (0.016)
Train: 540 [ 350/1251 ( 28%)]  Loss: 2.950 (2.93)  Time: 0.781s, 1311.22/s  (0.793s, 1290.92/s)  LR: 3.423e-05  Data: 0.010 (0.015)
Train: 540 [ 400/1251 ( 32%)]  Loss: 2.826 (2.92)  Time: 0.774s, 1323.05/s  (0.792s, 1293.04/s)  LR: 3.423e-05  Data: 0.009 (0.014)
Train: 540 [ 450/1251 ( 36%)]  Loss: 2.896 (2.92)  Time: 0.771s, 1327.68/s  (0.791s, 1295.33/s)  LR: 3.423e-05  Data: 0.010 (0.014)
Train: 540 [ 500/1251 ( 40%)]  Loss: 3.125 (2.94)  Time: 0.772s, 1325.68/s  (0.790s, 1296.60/s)  LR: 3.423e-05  Data: 0.009 (0.013)
Train: 540 [ 550/1251 ( 44%)]  Loss: 3.087 (2.95)  Time: 0.775s, 1322.05/s  (0.789s, 1297.16/s)  LR: 3.423e-05  Data: 0.010 (0.013)
Train: 540 [ 600/1251 ( 48%)]  Loss: 2.475 (2.91)  Time: 0.770s, 1329.55/s  (0.788s, 1298.75/s)  LR: 3.423e-05  Data: 0.010 (0.013)
Train: 540 [ 650/1251 ( 52%)]  Loss: 3.107 (2.93)  Time: 0.785s, 1304.81/s  (0.789s, 1297.91/s)  LR: 3.423e-05  Data: 0.011 (0.013)
Train: 540 [ 700/1251 ( 56%)]  Loss: 3.016 (2.93)  Time: 0.814s, 1257.90/s  (0.791s, 1294.98/s)  LR: 3.423e-05  Data: 0.011 (0.013)
Train: 540 [ 750/1251 ( 60%)]  Loss: 3.214 (2.95)  Time: 0.783s, 1307.10/s  (0.790s, 1295.62/s)  LR: 3.423e-05  Data: 0.012 (0.012)
Train: 540 [ 800/1251 ( 64%)]  Loss: 2.687 (2.93)  Time: 0.809s, 1266.00/s  (0.790s, 1296.14/s)  LR: 3.423e-05  Data: 0.011 (0.012)
Train: 540 [ 850/1251 ( 68%)]  Loss: 2.947 (2.94)  Time: 0.780s, 1312.89/s  (0.790s, 1296.54/s)  LR: 3.423e-05  Data: 0.009 (0.012)
Train: 540 [ 900/1251 ( 72%)]  Loss: 3.035 (2.94)  Time: 0.775s, 1320.89/s  (0.790s, 1296.96/s)  LR: 3.423e-05  Data: 0.009 (0.012)
Train: 540 [ 950/1251 ( 76%)]  Loss: 3.122 (2.95)  Time: 0.831s, 1232.96/s  (0.789s, 1297.39/s)  LR: 3.423e-05  Data: 0.010 (0.012)
Train: 540 [1000/1251 ( 80%)]  Loss: 2.876 (2.95)  Time: 0.773s, 1325.47/s  (0.789s, 1298.09/s)  LR: 3.423e-05  Data: 0.010 (0.012)
Train: 540 [1050/1251 ( 84%)]  Loss: 3.009 (2.95)  Time: 0.825s, 1240.97/s  (0.788s, 1298.75/s)  LR: 3.423e-05  Data: 0.009 (0.012)
Train: 540 [1100/1251 ( 88%)]  Loss: 2.763 (2.94)  Time: 0.787s, 1300.50/s  (0.788s, 1298.97/s)  LR: 3.423e-05  Data: 0.011 (0.012)
Train: 540 [1150/1251 ( 92%)]  Loss: 2.486 (2.92)  Time: 0.819s, 1250.50/s  (0.788s, 1298.69/s)  LR: 3.423e-05  Data: 0.010 (0.011)
Train: 540 [1200/1251 ( 96%)]  Loss: 2.970 (2.92)  Time: 0.856s, 1195.68/s  (0.789s, 1297.70/s)  LR: 3.423e-05  Data: 0.009 (0.011)
Train: 540 [1250/1251 (100%)]  Loss: 3.131 (2.93)  Time: 0.760s, 1347.84/s  (0.789s, 1297.43/s)  LR: 3.423e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.562 (1.562)  Loss:  0.6836 (0.6836)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.194 (0.563)  Loss:  0.7842 (1.1274)  Acc@1: 87.1462 (80.5120)  Acc@5: 98.3491 (95.3340)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-535.pth.tar', 80.67399987060547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-530.pth.tar', 80.61400002929688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-537.pth.tar', 80.60200002685546)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-526.pth.tar', 80.57199997802735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-533.pth.tar', 80.55999994873046)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-538.pth.tar', 80.53199987304687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-534.pth.tar', 80.524)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-540.pth.tar', 80.51199997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-527.pth.tar', 80.50000000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-532.pth.tar', 80.4720000024414)

Train: 541 [   0/1251 (  0%)]  Loss: 3.119 (3.12)  Time: 2.775s,  369.03/s  (2.775s,  369.03/s)  LR: 3.343e-05  Data: 2.019 (2.019)
Train: 541 [  50/1251 (  4%)]  Loss: 3.334 (3.23)  Time: 0.807s, 1268.88/s  (0.823s, 1243.81/s)  LR: 3.343e-05  Data: 0.010 (0.049)
Train: 541 [ 100/1251 (  8%)]  Loss: 2.942 (3.13)  Time: 0.778s, 1315.77/s  (0.803s, 1275.29/s)  LR: 3.343e-05  Data: 0.009 (0.030)
Train: 541 [ 150/1251 ( 12%)]  Loss: 2.940 (3.08)  Time: 0.772s, 1327.27/s  (0.795s, 1287.46/s)  LR: 3.343e-05  Data: 0.009 (0.023)
Train: 541 [ 200/1251 ( 16%)]  Loss: 2.910 (3.05)  Time: 0.772s, 1325.61/s  (0.793s, 1291.20/s)  LR: 3.343e-05  Data: 0.010 (0.020)
Train: 541 [ 250/1251 ( 20%)]  Loss: 2.714 (2.99)  Time: 0.783s, 1308.53/s  (0.792s, 1292.93/s)  LR: 3.343e-05  Data: 0.013 (0.018)
Train: 541 [ 300/1251 ( 24%)]  Loss: 3.049 (3.00)  Time: 0.809s, 1265.90/s  (0.792s, 1292.39/s)  LR: 3.343e-05  Data: 0.010 (0.016)
Train: 541 [ 350/1251 ( 28%)]  Loss: 2.639 (2.96)  Time: 0.808s, 1267.93/s  (0.795s, 1288.24/s)  LR: 3.343e-05  Data: 0.010 (0.015)
Train: 541 [ 400/1251 ( 32%)]  Loss: 2.979 (2.96)  Time: 0.771s, 1327.79/s  (0.794s, 1290.29/s)  LR: 3.343e-05  Data: 0.009 (0.015)
Train: 541 [ 450/1251 ( 36%)]  Loss: 2.886 (2.95)  Time: 0.775s, 1322.01/s  (0.792s, 1293.06/s)  LR: 3.343e-05  Data: 0.013 (0.014)
Train: 541 [ 500/1251 ( 40%)]  Loss: 2.936 (2.95)  Time: 0.811s, 1263.11/s  (0.793s, 1291.21/s)  LR: 3.343e-05  Data: 0.008 (0.014)
Train: 541 [ 550/1251 ( 44%)]  Loss: 2.765 (2.93)  Time: 0.822s, 1245.46/s  (0.793s, 1290.79/s)  LR: 3.343e-05  Data: 0.009 (0.013)
Train: 541 [ 600/1251 ( 48%)]  Loss: 3.078 (2.95)  Time: 0.809s, 1265.50/s  (0.793s, 1291.47/s)  LR: 3.343e-05  Data: 0.008 (0.013)
Train: 541 [ 650/1251 ( 52%)]  Loss: 3.017 (2.95)  Time: 0.806s, 1270.19/s  (0.793s, 1291.94/s)  LR: 3.343e-05  Data: 0.009 (0.013)
Train: 541 [ 700/1251 ( 56%)]  Loss: 2.631 (2.93)  Time: 0.771s, 1328.16/s  (0.793s, 1291.16/s)  LR: 3.343e-05  Data: 0.009 (0.013)
Train: 541 [ 750/1251 ( 60%)]  Loss: 3.060 (2.94)  Time: 0.833s, 1229.03/s  (0.792s, 1292.55/s)  LR: 3.343e-05  Data: 0.009 (0.012)
Train: 541 [ 800/1251 ( 64%)]  Loss: 3.182 (2.95)  Time: 0.776s, 1319.11/s  (0.792s, 1293.13/s)  LR: 3.343e-05  Data: 0.009 (0.012)
Train: 541 [ 850/1251 ( 68%)]  Loss: 2.872 (2.95)  Time: 0.808s, 1268.03/s  (0.793s, 1291.81/s)  LR: 3.343e-05  Data: 0.010 (0.012)
Train: 541 [ 900/1251 ( 72%)]  Loss: 2.968 (2.95)  Time: 0.782s, 1309.53/s  (0.792s, 1292.23/s)  LR: 3.343e-05  Data: 0.009 (0.012)
Train: 541 [ 950/1251 ( 76%)]  Loss: 3.037 (2.95)  Time: 0.772s, 1325.68/s  (0.792s, 1292.93/s)  LR: 3.343e-05  Data: 0.010 (0.012)
Train: 541 [1000/1251 ( 80%)]  Loss: 3.045 (2.96)  Time: 0.779s, 1314.82/s  (0.791s, 1294.25/s)  LR: 3.343e-05  Data: 0.010 (0.012)
Train: 541 [1050/1251 ( 84%)]  Loss: 3.111 (2.96)  Time: 0.840s, 1218.41/s  (0.791s, 1293.86/s)  LR: 3.343e-05  Data: 0.010 (0.012)
Train: 541 [1100/1251 ( 88%)]  Loss: 2.921 (2.96)  Time: 0.775s, 1321.20/s  (0.792s, 1293.64/s)  LR: 3.343e-05  Data: 0.010 (0.012)
Train: 541 [1150/1251 ( 92%)]  Loss: 2.972 (2.96)  Time: 0.816s, 1254.14/s  (0.792s, 1292.81/s)  LR: 3.343e-05  Data: 0.010 (0.012)
Train: 541 [1200/1251 ( 96%)]  Loss: 2.926 (2.96)  Time: 0.772s, 1326.04/s  (0.792s, 1292.70/s)  LR: 3.343e-05  Data: 0.010 (0.012)
Train: 541 [1250/1251 (100%)]  Loss: 2.927 (2.96)  Time: 0.760s, 1347.59/s  (0.792s, 1293.55/s)  LR: 3.343e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.617 (1.617)  Loss:  0.7119 (0.7119)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.194 (0.561)  Loss:  0.7964 (1.1691)  Acc@1: 86.9104 (80.5520)  Acc@5: 98.1132 (95.3520)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-535.pth.tar', 80.67399987060547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-530.pth.tar', 80.61400002929688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-537.pth.tar', 80.60200002685546)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-526.pth.tar', 80.57199997802735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-533.pth.tar', 80.55999994873046)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-541.pth.tar', 80.5520000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-538.pth.tar', 80.53199987304687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-534.pth.tar', 80.524)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-540.pth.tar', 80.51199997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-527.pth.tar', 80.50000000244141)

Train: 542 [   0/1251 (  0%)]  Loss: 3.129 (3.13)  Time: 2.342s,  437.17/s  (2.342s,  437.17/s)  LR: 3.265e-05  Data: 1.559 (1.559)
Train: 542 [  50/1251 (  4%)]  Loss: 3.186 (3.16)  Time: 0.774s, 1323.51/s  (0.821s, 1247.40/s)  LR: 3.265e-05  Data: 0.010 (0.049)
Train: 542 [ 100/1251 (  8%)]  Loss: 2.884 (3.07)  Time: 0.770s, 1329.14/s  (0.800s, 1279.51/s)  LR: 3.265e-05  Data: 0.010 (0.030)
Train: 542 [ 150/1251 ( 12%)]  Loss: 2.747 (2.99)  Time: 0.830s, 1233.56/s  (0.805s, 1271.55/s)  LR: 3.265e-05  Data: 0.010 (0.023)
Train: 542 [ 200/1251 ( 16%)]  Loss: 3.018 (2.99)  Time: 0.771s, 1328.12/s  (0.799s, 1280.92/s)  LR: 3.265e-05  Data: 0.010 (0.020)
Train: 542 [ 250/1251 ( 20%)]  Loss: 3.050 (3.00)  Time: 0.772s, 1326.96/s  (0.795s, 1287.62/s)  LR: 3.265e-05  Data: 0.010 (0.018)
Train: 542 [ 300/1251 ( 24%)]  Loss: 3.097 (3.02)  Time: 0.772s, 1326.67/s  (0.793s, 1290.88/s)  LR: 3.265e-05  Data: 0.010 (0.017)
Train: 542 [ 350/1251 ( 28%)]  Loss: 2.735 (2.98)  Time: 0.798s, 1283.39/s  (0.791s, 1294.51/s)  LR: 3.265e-05  Data: 0.010 (0.016)
Train: 542 [ 400/1251 ( 32%)]  Loss: 3.056 (2.99)  Time: 0.814s, 1257.72/s  (0.794s, 1289.32/s)  LR: 3.265e-05  Data: 0.011 (0.015)
Train: 542 [ 450/1251 ( 36%)]  Loss: 2.831 (2.97)  Time: 0.814s, 1257.35/s  (0.797s, 1285.54/s)  LR: 3.265e-05  Data: 0.011 (0.015)
Train: 542 [ 500/1251 ( 40%)]  Loss: 3.093 (2.98)  Time: 0.771s, 1327.95/s  (0.798s, 1282.95/s)  LR: 3.265e-05  Data: 0.010 (0.014)
Train: 542 [ 550/1251 ( 44%)]  Loss: 2.594 (2.95)  Time: 0.785s, 1304.16/s  (0.796s, 1285.65/s)  LR: 3.265e-05  Data: 0.009 (0.014)
Train: 542 [ 600/1251 ( 48%)]  Loss: 2.953 (2.95)  Time: 0.771s, 1327.37/s  (0.795s, 1287.78/s)  LR: 3.265e-05  Data: 0.010 (0.014)
Train: 542 [ 650/1251 ( 52%)]  Loss: 2.946 (2.95)  Time: 0.777s, 1317.79/s  (0.795s, 1287.91/s)  LR: 3.265e-05  Data: 0.011 (0.013)
Train: 542 [ 700/1251 ( 56%)]  Loss: 3.122 (2.96)  Time: 0.779s, 1314.07/s  (0.794s, 1289.88/s)  LR: 3.265e-05  Data: 0.010 (0.013)
Train: 542 [ 750/1251 ( 60%)]  Loss: 2.776 (2.95)  Time: 0.791s, 1294.95/s  (0.794s, 1290.32/s)  LR: 3.265e-05  Data: 0.013 (0.013)
Train: 542 [ 800/1251 ( 64%)]  Loss: 2.879 (2.95)  Time: 0.827s, 1237.94/s  (0.793s, 1291.94/s)  LR: 3.265e-05  Data: 0.010 (0.013)
Train: 542 [ 850/1251 ( 68%)]  Loss: 2.757 (2.94)  Time: 0.769s, 1332.07/s  (0.792s, 1292.36/s)  LR: 3.265e-05  Data: 0.010 (0.013)
Train: 542 [ 900/1251 ( 72%)]  Loss: 3.073 (2.94)  Time: 0.780s, 1313.51/s  (0.791s, 1293.87/s)  LR: 3.265e-05  Data: 0.010 (0.012)
Train: 542 [ 950/1251 ( 76%)]  Loss: 3.324 (2.96)  Time: 0.787s, 1301.81/s  (0.791s, 1294.87/s)  LR: 3.265e-05  Data: 0.010 (0.012)
Train: 542 [1000/1251 ( 80%)]  Loss: 3.272 (2.98)  Time: 0.773s, 1324.19/s  (0.791s, 1294.61/s)  LR: 3.265e-05  Data: 0.010 (0.012)
Train: 542 [1050/1251 ( 84%)]  Loss: 2.623 (2.96)  Time: 0.805s, 1271.97/s  (0.790s, 1295.49/s)  LR: 3.265e-05  Data: 0.010 (0.012)
Train: 542 [1100/1251 ( 88%)]  Loss: 3.014 (2.96)  Time: 0.774s, 1322.21/s  (0.791s, 1295.33/s)  LR: 3.265e-05  Data: 0.010 (0.012)
Train: 542 [1150/1251 ( 92%)]  Loss: 3.138 (2.97)  Time: 0.783s, 1308.45/s  (0.790s, 1296.31/s)  LR: 3.265e-05  Data: 0.009 (0.012)
Train: 542 [1200/1251 ( 96%)]  Loss: 3.202 (2.98)  Time: 0.807s, 1269.18/s  (0.790s, 1296.40/s)  LR: 3.265e-05  Data: 0.010 (0.012)
Train: 542 [1250/1251 (100%)]  Loss: 2.637 (2.97)  Time: 0.808s, 1268.09/s  (0.790s, 1295.72/s)  LR: 3.265e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.561 (1.561)  Loss:  0.7031 (0.7031)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.194 (0.571)  Loss:  0.7598 (1.1159)  Acc@1: 87.3821 (80.5700)  Acc@5: 98.3491 (95.3720)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-535.pth.tar', 80.67399987060547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-530.pth.tar', 80.61400002929688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-537.pth.tar', 80.60200002685546)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-526.pth.tar', 80.57199997802735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-542.pth.tar', 80.56999994873047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-533.pth.tar', 80.55999994873046)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-541.pth.tar', 80.5520000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-538.pth.tar', 80.53199987304687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-534.pth.tar', 80.524)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-540.pth.tar', 80.51199997558594)

Train: 543 [   0/1251 (  0%)]  Loss: 3.067 (3.07)  Time: 2.272s,  450.69/s  (2.272s,  450.69/s)  LR: 3.188e-05  Data: 1.535 (1.535)
Train: 543 [  50/1251 (  4%)]  Loss: 2.879 (2.97)  Time: 0.775s, 1322.05/s  (0.822s, 1245.20/s)  LR: 3.188e-05  Data: 0.011 (0.046)
Train: 543 [ 100/1251 (  8%)]  Loss: 2.980 (2.98)  Time: 0.773s, 1325.56/s  (0.803s, 1274.59/s)  LR: 3.188e-05  Data: 0.010 (0.028)
Train: 543 [ 150/1251 ( 12%)]  Loss: 2.689 (2.90)  Time: 0.850s, 1204.87/s  (0.796s, 1285.72/s)  LR: 3.188e-05  Data: 0.010 (0.022)
Train: 543 [ 200/1251 ( 16%)]  Loss: 3.101 (2.94)  Time: 0.792s, 1293.30/s  (0.792s, 1293.52/s)  LR: 3.188e-05  Data: 0.012 (0.019)
Train: 543 [ 250/1251 ( 20%)]  Loss: 3.139 (2.98)  Time: 0.772s, 1325.66/s  (0.789s, 1297.12/s)  LR: 3.188e-05  Data: 0.010 (0.017)
Train: 543 [ 300/1251 ( 24%)]  Loss: 3.245 (3.01)  Time: 0.773s, 1325.07/s  (0.791s, 1294.88/s)  LR: 3.188e-05  Data: 0.010 (0.016)
Train: 543 [ 350/1251 ( 28%)]  Loss: 3.228 (3.04)  Time: 0.778s, 1317.00/s  (0.790s, 1296.61/s)  LR: 3.188e-05  Data: 0.010 (0.015)
Train: 543 [ 400/1251 ( 32%)]  Loss: 2.959 (3.03)  Time: 0.779s, 1315.18/s  (0.789s, 1297.97/s)  LR: 3.188e-05  Data: 0.010 (0.015)
Train: 543 [ 450/1251 ( 36%)]  Loss: 2.926 (3.02)  Time: 0.772s, 1327.22/s  (0.789s, 1298.22/s)  LR: 3.188e-05  Data: 0.009 (0.014)
Train: 543 [ 500/1251 ( 40%)]  Loss: 2.879 (3.01)  Time: 0.772s, 1326.46/s  (0.789s, 1298.31/s)  LR: 3.188e-05  Data: 0.010 (0.014)
Train: 543 [ 550/1251 ( 44%)]  Loss: 3.153 (3.02)  Time: 0.778s, 1315.80/s  (0.789s, 1298.58/s)  LR: 3.188e-05  Data: 0.010 (0.013)
Train: 543 [ 600/1251 ( 48%)]  Loss: 3.026 (3.02)  Time: 0.782s, 1309.54/s  (0.790s, 1295.46/s)  LR: 3.188e-05  Data: 0.009 (0.013)
Train: 543 [ 650/1251 ( 52%)]  Loss: 2.888 (3.01)  Time: 0.772s, 1327.19/s  (0.790s, 1296.01/s)  LR: 3.188e-05  Data: 0.010 (0.013)
Train: 543 [ 700/1251 ( 56%)]  Loss: 3.073 (3.02)  Time: 0.786s, 1303.56/s  (0.790s, 1296.15/s)  LR: 3.188e-05  Data: 0.009 (0.013)
Train: 543 [ 750/1251 ( 60%)]  Loss: 2.963 (3.01)  Time: 0.786s, 1302.49/s  (0.789s, 1297.26/s)  LR: 3.188e-05  Data: 0.010 (0.012)
Train: 543 [ 800/1251 ( 64%)]  Loss: 2.830 (3.00)  Time: 0.776s, 1319.64/s  (0.789s, 1298.35/s)  LR: 3.188e-05  Data: 0.010 (0.012)
Train: 543 [ 850/1251 ( 68%)]  Loss: 2.983 (3.00)  Time: 0.772s, 1326.53/s  (0.788s, 1299.21/s)  LR: 3.188e-05  Data: 0.010 (0.012)
Train: 543 [ 900/1251 ( 72%)]  Loss: 3.009 (3.00)  Time: 0.782s, 1309.33/s  (0.789s, 1297.47/s)  LR: 3.188e-05  Data: 0.010 (0.012)
Train: 543 [ 950/1251 ( 76%)]  Loss: 2.737 (2.99)  Time: 0.778s, 1316.90/s  (0.789s, 1298.39/s)  LR: 3.188e-05  Data: 0.009 (0.012)
Train: 543 [1000/1251 ( 80%)]  Loss: 2.683 (2.97)  Time: 0.818s, 1251.24/s  (0.789s, 1298.36/s)  LR: 3.188e-05  Data: 0.011 (0.012)
Train: 543 [1050/1251 ( 84%)]  Loss: 3.178 (2.98)  Time: 0.867s, 1181.55/s  (0.789s, 1297.75/s)  LR: 3.188e-05  Data: 0.009 (0.012)
Train: 543 [1100/1251 ( 88%)]  Loss: 3.062 (2.99)  Time: 0.807s, 1268.83/s  (0.789s, 1297.59/s)  LR: 3.188e-05  Data: 0.010 (0.012)
Train: 543 [1150/1251 ( 92%)]  Loss: 3.146 (2.99)  Time: 0.818s, 1251.97/s  (0.790s, 1296.24/s)  LR: 3.188e-05  Data: 0.011 (0.012)
Train: 543 [1200/1251 ( 96%)]  Loss: 2.737 (2.98)  Time: 0.789s, 1297.34/s  (0.790s, 1295.88/s)  LR: 3.188e-05  Data: 0.010 (0.012)
Train: 543 [1250/1251 (100%)]  Loss: 2.857 (2.98)  Time: 0.795s, 1288.86/s  (0.790s, 1295.76/s)  LR: 3.188e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.568 (1.568)  Loss:  0.6680 (0.6680)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.194 (0.562)  Loss:  0.7783 (1.1143)  Acc@1: 87.1462 (80.5440)  Acc@5: 97.8774 (95.3080)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-535.pth.tar', 80.67399987060547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-530.pth.tar', 80.61400002929688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-537.pth.tar', 80.60200002685546)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-526.pth.tar', 80.57199997802735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-542.pth.tar', 80.56999994873047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-533.pth.tar', 80.55999994873046)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-541.pth.tar', 80.5520000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-543.pth.tar', 80.54400010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-538.pth.tar', 80.53199987304687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-534.pth.tar', 80.524)

Train: 544 [   0/1251 (  0%)]  Loss: 2.767 (2.77)  Time: 2.269s,  451.26/s  (2.269s,  451.26/s)  LR: 3.113e-05  Data: 1.539 (1.539)
Train: 544 [  50/1251 (  4%)]  Loss: 3.090 (2.93)  Time: 0.772s, 1326.78/s  (0.821s, 1246.87/s)  LR: 3.113e-05  Data: 0.010 (0.046)
Train: 544 [ 100/1251 (  8%)]  Loss: 2.965 (2.94)  Time: 0.808s, 1267.69/s  (0.806s, 1271.22/s)  LR: 3.113e-05  Data: 0.010 (0.028)
Train: 544 [ 150/1251 ( 12%)]  Loss: 3.060 (2.97)  Time: 0.784s, 1306.33/s  (0.802s, 1276.34/s)  LR: 3.113e-05  Data: 0.011 (0.022)
Train: 544 [ 200/1251 ( 16%)]  Loss: 2.934 (2.96)  Time: 0.805s, 1272.65/s  (0.799s, 1281.89/s)  LR: 3.113e-05  Data: 0.009 (0.019)
Train: 544 [ 250/1251 ( 20%)]  Loss: 3.000 (2.97)  Time: 0.775s, 1320.79/s  (0.796s, 1286.31/s)  LR: 3.113e-05  Data: 0.010 (0.017)
Train: 544 [ 300/1251 ( 24%)]  Loss: 2.788 (2.94)  Time: 0.776s, 1319.20/s  (0.794s, 1290.28/s)  LR: 3.113e-05  Data: 0.010 (0.016)
Train: 544 [ 350/1251 ( 28%)]  Loss: 3.043 (2.96)  Time: 0.769s, 1331.94/s  (0.792s, 1293.04/s)  LR: 3.113e-05  Data: 0.010 (0.015)
Train: 544 [ 400/1251 ( 32%)]  Loss: 2.951 (2.96)  Time: 0.807s, 1268.30/s  (0.791s, 1294.92/s)  LR: 3.113e-05  Data: 0.009 (0.015)
Train: 544 [ 450/1251 ( 36%)]  Loss: 2.982 (2.96)  Time: 0.772s, 1326.78/s  (0.789s, 1297.08/s)  LR: 3.113e-05  Data: 0.010 (0.014)
Train: 544 [ 500/1251 ( 40%)]  Loss: 3.174 (2.98)  Time: 0.816s, 1255.36/s  (0.791s, 1295.31/s)  LR: 3.113e-05  Data: 0.011 (0.014)
Train: 544 [ 550/1251 ( 44%)]  Loss: 2.869 (2.97)  Time: 0.791s, 1294.91/s  (0.790s, 1296.05/s)  LR: 3.113e-05  Data: 0.011 (0.013)
Train: 544 [ 600/1251 ( 48%)]  Loss: 2.963 (2.97)  Time: 0.773s, 1325.03/s  (0.789s, 1298.06/s)  LR: 3.113e-05  Data: 0.010 (0.013)
Train: 544 [ 650/1251 ( 52%)]  Loss: 2.624 (2.94)  Time: 0.771s, 1327.50/s  (0.788s, 1299.78/s)  LR: 3.113e-05  Data: 0.009 (0.013)
Train: 544 [ 700/1251 ( 56%)]  Loss: 3.304 (2.97)  Time: 0.843s, 1214.65/s  (0.787s, 1300.47/s)  LR: 3.113e-05  Data: 0.009 (0.013)
Train: 544 [ 750/1251 ( 60%)]  Loss: 2.862 (2.96)  Time: 0.769s, 1331.18/s  (0.787s, 1301.02/s)  LR: 3.113e-05  Data: 0.010 (0.012)
Train: 544 [ 800/1251 ( 64%)]  Loss: 2.654 (2.94)  Time: 0.773s, 1324.24/s  (0.787s, 1301.72/s)  LR: 3.113e-05  Data: 0.010 (0.012)
Train: 544 [ 850/1251 ( 68%)]  Loss: 2.712 (2.93)  Time: 0.811s, 1262.72/s  (0.787s, 1301.87/s)  LR: 3.113e-05  Data: 0.010 (0.012)
Train: 544 [ 900/1251 ( 72%)]  Loss: 2.940 (2.93)  Time: 0.791s, 1293.76/s  (0.787s, 1301.32/s)  LR: 3.113e-05  Data: 0.010 (0.012)
Train: 544 [ 950/1251 ( 76%)]  Loss: 2.819 (2.93)  Time: 0.815s, 1257.02/s  (0.788s, 1299.77/s)  LR: 3.113e-05  Data: 0.011 (0.012)
Train: 544 [1000/1251 ( 80%)]  Loss: 2.744 (2.92)  Time: 0.770s, 1329.25/s  (0.787s, 1300.43/s)  LR: 3.113e-05  Data: 0.010 (0.012)
Train: 544 [1050/1251 ( 84%)]  Loss: 3.024 (2.92)  Time: 0.771s, 1328.53/s  (0.787s, 1301.00/s)  LR: 3.113e-05  Data: 0.010 (0.012)
Train: 544 [1100/1251 ( 88%)]  Loss: 2.878 (2.92)  Time: 0.806s, 1270.36/s  (0.787s, 1301.05/s)  LR: 3.113e-05  Data: 0.010 (0.012)
Train: 544 [1150/1251 ( 92%)]  Loss: 2.930 (2.92)  Time: 0.778s, 1316.30/s  (0.787s, 1300.97/s)  LR: 3.113e-05  Data: 0.013 (0.012)
Train: 544 [1200/1251 ( 96%)]  Loss: 2.600 (2.91)  Time: 0.775s, 1322.13/s  (0.787s, 1301.50/s)  LR: 3.113e-05  Data: 0.010 (0.012)
Train: 544 [1250/1251 (100%)]  Loss: 3.007 (2.91)  Time: 0.795s, 1288.45/s  (0.787s, 1301.11/s)  LR: 3.113e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.567 (1.567)  Loss:  0.6631 (0.6631)  Acc@1: 92.3828 (92.3828)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.194 (0.575)  Loss:  0.7588 (1.1184)  Acc@1: 88.3255 (80.6000)  Acc@5: 98.2311 (95.4380)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-535.pth.tar', 80.67399987060547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-530.pth.tar', 80.61400002929688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-537.pth.tar', 80.60200002685546)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-544.pth.tar', 80.59999997070312)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-526.pth.tar', 80.57199997802735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-542.pth.tar', 80.56999994873047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-533.pth.tar', 80.55999994873046)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-541.pth.tar', 80.5520000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-543.pth.tar', 80.54400010498047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-538.pth.tar', 80.53199987304687)

Train: 545 [   0/1251 (  0%)]  Loss: 2.644 (2.64)  Time: 2.268s,  451.47/s  (2.268s,  451.47/s)  LR: 3.038e-05  Data: 1.532 (1.532)
Train: 545 [  50/1251 (  4%)]  Loss: 2.523 (2.58)  Time: 0.773s, 1325.46/s  (0.817s, 1252.86/s)  LR: 3.038e-05  Data: 0.010 (0.043)
Train: 545 [ 100/1251 (  8%)]  Loss: 2.828 (2.67)  Time: 0.772s, 1326.37/s  (0.804s, 1272.86/s)  LR: 3.038e-05  Data: 0.011 (0.026)
Train: 545 [ 150/1251 ( 12%)]  Loss: 2.760 (2.69)  Time: 0.785s, 1304.95/s  (0.798s, 1283.94/s)  LR: 3.038e-05  Data: 0.009 (0.021)
Train: 545 [ 200/1251 ( 16%)]  Loss: 3.081 (2.77)  Time: 0.814s, 1257.79/s  (0.795s, 1288.49/s)  LR: 3.038e-05  Data: 0.011 (0.018)
Train: 545 [ 250/1251 ( 20%)]  Loss: 2.642 (2.75)  Time: 0.771s, 1327.63/s  (0.792s, 1292.81/s)  LR: 3.038e-05  Data: 0.009 (0.017)
Train: 545 [ 300/1251 ( 24%)]  Loss: 3.088 (2.80)  Time: 0.881s, 1162.96/s  (0.790s, 1295.62/s)  LR: 3.038e-05  Data: 0.009 (0.015)
Train: 545 [ 350/1251 ( 28%)]  Loss: 3.050 (2.83)  Time: 0.777s, 1318.65/s  (0.791s, 1293.90/s)  LR: 3.038e-05  Data: 0.009 (0.015)
Train: 545 [ 400/1251 ( 32%)]  Loss: 3.192 (2.87)  Time: 0.771s, 1327.46/s  (0.791s, 1294.14/s)  LR: 3.038e-05  Data: 0.009 (0.014)
Train: 545 [ 450/1251 ( 36%)]  Loss: 2.810 (2.86)  Time: 0.805s, 1271.30/s  (0.790s, 1295.45/s)  LR: 3.038e-05  Data: 0.009 (0.014)
Train: 545 [ 500/1251 ( 40%)]  Loss: 2.912 (2.87)  Time: 0.834s, 1228.44/s  (0.792s, 1293.23/s)  LR: 3.038e-05  Data: 0.009 (0.013)
Train: 545 [ 550/1251 ( 44%)]  Loss: 2.877 (2.87)  Time: 0.845s, 1212.09/s  (0.791s, 1293.96/s)  LR: 3.038e-05  Data: 0.010 (0.013)
Train: 545 [ 600/1251 ( 48%)]  Loss: 2.932 (2.87)  Time: 0.804s, 1273.52/s  (0.792s, 1293.28/s)  LR: 3.038e-05  Data: 0.010 (0.013)
Train: 545 [ 650/1251 ( 52%)]  Loss: 2.867 (2.87)  Time: 0.787s, 1301.43/s  (0.791s, 1294.97/s)  LR: 3.038e-05  Data: 0.010 (0.012)
Train: 545 [ 700/1251 ( 56%)]  Loss: 2.741 (2.86)  Time: 0.772s, 1327.09/s  (0.790s, 1295.69/s)  LR: 3.038e-05  Data: 0.010 (0.012)
Train: 545 [ 750/1251 ( 60%)]  Loss: 2.935 (2.87)  Time: 0.780s, 1312.68/s  (0.790s, 1296.71/s)  LR: 3.038e-05  Data: 0.010 (0.012)
Train: 545 [ 800/1251 ( 64%)]  Loss: 3.219 (2.89)  Time: 0.783s, 1308.12/s  (0.789s, 1297.69/s)  LR: 3.038e-05  Data: 0.010 (0.012)
Train: 545 [ 850/1251 ( 68%)]  Loss: 2.929 (2.89)  Time: 0.818s, 1252.48/s  (0.789s, 1297.51/s)  LR: 3.038e-05  Data: 0.009 (0.012)
Train: 545 [ 900/1251 ( 72%)]  Loss: 2.729 (2.88)  Time: 0.772s, 1327.11/s  (0.789s, 1298.30/s)  LR: 3.038e-05  Data: 0.010 (0.012)
Train: 545 [ 950/1251 ( 76%)]  Loss: 2.907 (2.88)  Time: 0.775s, 1321.35/s  (0.789s, 1297.71/s)  LR: 3.038e-05  Data: 0.010 (0.012)
Train: 545 [1000/1251 ( 80%)]  Loss: 2.927 (2.89)  Time: 0.775s, 1321.53/s  (0.789s, 1298.29/s)  LR: 3.038e-05  Data: 0.010 (0.012)
Train: 545 [1050/1251 ( 84%)]  Loss: 2.992 (2.89)  Time: 0.773s, 1324.36/s  (0.789s, 1297.77/s)  LR: 3.038e-05  Data: 0.009 (0.011)
Train: 545 [1100/1251 ( 88%)]  Loss: 2.972 (2.89)  Time: 0.773s, 1325.20/s  (0.789s, 1297.78/s)  LR: 3.038e-05  Data: 0.009 (0.011)
Train: 545 [1150/1251 ( 92%)]  Loss: 2.587 (2.88)  Time: 0.772s, 1326.37/s  (0.788s, 1298.83/s)  LR: 3.038e-05  Data: 0.010 (0.011)
Train: 545 [1200/1251 ( 96%)]  Loss: 3.153 (2.89)  Time: 0.771s, 1327.70/s  (0.788s, 1299.19/s)  LR: 3.038e-05  Data: 0.010 (0.011)
Train: 545 [1250/1251 (100%)]  Loss: 2.768 (2.89)  Time: 0.797s, 1285.28/s  (0.788s, 1298.86/s)  LR: 3.038e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.546 (1.546)  Loss:  0.6196 (0.6196)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.194 (0.569)  Loss:  0.7178 (1.0618)  Acc@1: 86.4387 (80.5720)  Acc@5: 98.2311 (95.4640)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-535.pth.tar', 80.67399987060547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-530.pth.tar', 80.61400002929688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-537.pth.tar', 80.60200002685546)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-544.pth.tar', 80.59999997070312)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-545.pth.tar', 80.57200005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-526.pth.tar', 80.57199997802735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-542.pth.tar', 80.56999994873047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-533.pth.tar', 80.55999994873046)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-541.pth.tar', 80.5520000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-543.pth.tar', 80.54400010498047)

Train: 546 [   0/1251 (  0%)]  Loss: 2.803 (2.80)  Time: 2.192s,  467.16/s  (2.192s,  467.16/s)  LR: 2.965e-05  Data: 1.477 (1.477)
Train: 546 [  50/1251 (  4%)]  Loss: 3.085 (2.94)  Time: 0.785s, 1304.37/s  (0.813s, 1260.24/s)  LR: 2.965e-05  Data: 0.010 (0.043)
Train: 546 [ 100/1251 (  8%)]  Loss: 2.970 (2.95)  Time: 0.782s, 1309.87/s  (0.804s, 1273.68/s)  LR: 2.965e-05  Data: 0.012 (0.027)
Train: 546 [ 150/1251 ( 12%)]  Loss: 2.558 (2.85)  Time: 0.834s, 1227.92/s  (0.803s, 1275.40/s)  LR: 2.965e-05  Data: 0.009 (0.021)
Train: 546 [ 200/1251 ( 16%)]  Loss: 2.872 (2.86)  Time: 0.771s, 1327.97/s  (0.798s, 1283.41/s)  LR: 2.965e-05  Data: 0.009 (0.018)
Train: 546 [ 250/1251 ( 20%)]  Loss: 2.783 (2.85)  Time: 0.796s, 1287.08/s  (0.799s, 1281.55/s)  LR: 2.965e-05  Data: 0.010 (0.017)
Train: 546 [ 300/1251 ( 24%)]  Loss: 2.958 (2.86)  Time: 0.773s, 1325.21/s  (0.796s, 1286.82/s)  LR: 2.965e-05  Data: 0.010 (0.016)
Train: 546 [ 350/1251 ( 28%)]  Loss: 3.109 (2.89)  Time: 0.772s, 1326.51/s  (0.794s, 1289.71/s)  LR: 2.965e-05  Data: 0.010 (0.015)
Train: 546 [ 400/1251 ( 32%)]  Loss: 2.980 (2.90)  Time: 0.772s, 1326.17/s  (0.792s, 1292.51/s)  LR: 2.965e-05  Data: 0.010 (0.014)
Train: 546 [ 450/1251 ( 36%)]  Loss: 2.984 (2.91)  Time: 0.771s, 1328.35/s  (0.791s, 1295.33/s)  LR: 2.965e-05  Data: 0.009 (0.014)
Train: 546 [ 500/1251 ( 40%)]  Loss: 2.980 (2.92)  Time: 0.784s, 1305.67/s  (0.791s, 1294.28/s)  LR: 2.965e-05  Data: 0.009 (0.013)
Train: 546 [ 550/1251 ( 44%)]  Loss: 3.332 (2.95)  Time: 0.773s, 1325.32/s  (0.791s, 1294.75/s)  LR: 2.965e-05  Data: 0.010 (0.013)
Train: 546 [ 600/1251 ( 48%)]  Loss: 3.210 (2.97)  Time: 0.814s, 1257.62/s  (0.791s, 1295.27/s)  LR: 2.965e-05  Data: 0.011 (0.013)
Train: 546 [ 650/1251 ( 52%)]  Loss: 2.987 (2.97)  Time: 0.773s, 1325.32/s  (0.791s, 1295.11/s)  LR: 2.965e-05  Data: 0.010 (0.013)
Train: 546 [ 700/1251 ( 56%)]  Loss: 2.893 (2.97)  Time: 0.807s, 1268.74/s  (0.790s, 1295.70/s)  LR: 2.965e-05  Data: 0.009 (0.012)
Train: 546 [ 750/1251 ( 60%)]  Loss: 3.126 (2.98)  Time: 0.774s, 1323.27/s  (0.791s, 1294.19/s)  LR: 2.965e-05  Data: 0.010 (0.012)
Train: 546 [ 800/1251 ( 64%)]  Loss: 2.886 (2.97)  Time: 0.814s, 1257.74/s  (0.792s, 1292.82/s)  LR: 2.965e-05  Data: 0.011 (0.012)
Train: 546 [ 850/1251 ( 68%)]  Loss: 3.050 (2.98)  Time: 0.815s, 1256.88/s  (0.794s, 1290.04/s)  LR: 2.965e-05  Data: 0.011 (0.012)
Train: 546 [ 900/1251 ( 72%)]  Loss: 2.965 (2.98)  Time: 0.774s, 1323.78/s  (0.794s, 1289.60/s)  LR: 2.965e-05  Data: 0.009 (0.012)
Train: 546 [ 950/1251 ( 76%)]  Loss: 2.940 (2.97)  Time: 0.808s, 1267.79/s  (0.794s, 1289.14/s)  LR: 2.965e-05  Data: 0.009 (0.012)
Train: 546 [1000/1251 ( 80%)]  Loss: 3.079 (2.98)  Time: 0.771s, 1328.58/s  (0.794s, 1290.27/s)  LR: 2.965e-05  Data: 0.010 (0.012)
Train: 546 [1050/1251 ( 84%)]  Loss: 2.947 (2.98)  Time: 0.772s, 1327.21/s  (0.793s, 1290.89/s)  LR: 2.965e-05  Data: 0.010 (0.012)
Train: 546 [1100/1251 ( 88%)]  Loss: 3.002 (2.98)  Time: 0.771s, 1327.99/s  (0.793s, 1291.14/s)  LR: 2.965e-05  Data: 0.010 (0.012)
Train: 546 [1150/1251 ( 92%)]  Loss: 3.258 (2.99)  Time: 0.770s, 1329.04/s  (0.794s, 1290.28/s)  LR: 2.965e-05  Data: 0.010 (0.012)
Train: 546 [1200/1251 ( 96%)]  Loss: 2.837 (2.98)  Time: 0.773s, 1324.86/s  (0.793s, 1291.21/s)  LR: 2.965e-05  Data: 0.010 (0.011)
Train: 546 [1250/1251 (100%)]  Loss: 3.128 (2.99)  Time: 0.762s, 1343.03/s  (0.793s, 1290.72/s)  LR: 2.965e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.534 (1.534)  Loss:  0.6523 (0.6523)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.194 (0.564)  Loss:  0.7734 (1.1012)  Acc@1: 87.3821 (80.7260)  Acc@5: 98.2311 (95.5040)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-546.pth.tar', 80.726000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-535.pth.tar', 80.67399987060547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-530.pth.tar', 80.61400002929688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-537.pth.tar', 80.60200002685546)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-544.pth.tar', 80.59999997070312)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-545.pth.tar', 80.57200005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-526.pth.tar', 80.57199997802735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-542.pth.tar', 80.56999994873047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-533.pth.tar', 80.55999994873046)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-541.pth.tar', 80.5520000024414)

Train: 547 [   0/1251 (  0%)]  Loss: 2.908 (2.91)  Time: 2.124s,  482.13/s  (2.124s,  482.13/s)  LR: 2.894e-05  Data: 1.410 (1.410)
Train: 547 [  50/1251 (  4%)]  Loss: 2.940 (2.92)  Time: 0.771s, 1328.72/s  (0.818s, 1251.79/s)  LR: 2.894e-05  Data: 0.009 (0.042)
Train: 547 [ 100/1251 (  8%)]  Loss: 2.936 (2.93)  Time: 0.772s, 1326.80/s  (0.801s, 1278.20/s)  LR: 2.894e-05  Data: 0.009 (0.026)
Train: 547 [ 150/1251 ( 12%)]  Loss: 3.241 (3.01)  Time: 0.814s, 1257.44/s  (0.800s, 1280.06/s)  LR: 2.894e-05  Data: 0.011 (0.021)
Train: 547 [ 200/1251 ( 16%)]  Loss: 3.232 (3.05)  Time: 0.814s, 1258.40/s  (0.798s, 1283.43/s)  LR: 2.894e-05  Data: 0.010 (0.018)
Train: 547 [ 250/1251 ( 20%)]  Loss: 2.981 (3.04)  Time: 0.788s, 1299.62/s  (0.799s, 1281.89/s)  LR: 2.894e-05  Data: 0.009 (0.017)
Train: 547 [ 300/1251 ( 24%)]  Loss: 3.012 (3.04)  Time: 0.772s, 1326.36/s  (0.795s, 1287.27/s)  LR: 2.894e-05  Data: 0.010 (0.015)
Train: 547 [ 350/1251 ( 28%)]  Loss: 3.234 (3.06)  Time: 0.775s, 1321.84/s  (0.794s, 1289.69/s)  LR: 2.894e-05  Data: 0.010 (0.015)
Train: 547 [ 400/1251 ( 32%)]  Loss: 3.240 (3.08)  Time: 0.775s, 1320.62/s  (0.792s, 1292.30/s)  LR: 2.894e-05  Data: 0.010 (0.014)
Train: 547 [ 450/1251 ( 36%)]  Loss: 2.971 (3.07)  Time: 0.780s, 1312.74/s  (0.791s, 1294.47/s)  LR: 2.894e-05  Data: 0.010 (0.014)
Train: 547 [ 500/1251 ( 40%)]  Loss: 3.216 (3.08)  Time: 0.815s, 1257.00/s  (0.790s, 1296.28/s)  LR: 2.894e-05  Data: 0.010 (0.013)
Train: 547 [ 550/1251 ( 44%)]  Loss: 2.845 (3.06)  Time: 0.777s, 1318.48/s  (0.789s, 1297.66/s)  LR: 2.894e-05  Data: 0.010 (0.013)
Train: 547 [ 600/1251 ( 48%)]  Loss: 3.231 (3.08)  Time: 0.778s, 1316.27/s  (0.789s, 1297.68/s)  LR: 2.894e-05  Data: 0.010 (0.013)
Train: 547 [ 650/1251 ( 52%)]  Loss: 2.778 (3.05)  Time: 0.773s, 1324.50/s  (0.789s, 1298.19/s)  LR: 2.894e-05  Data: 0.010 (0.013)
Train: 547 [ 700/1251 ( 56%)]  Loss: 2.979 (3.05)  Time: 0.823s, 1243.82/s  (0.789s, 1297.99/s)  LR: 2.894e-05  Data: 0.011 (0.013)
Train: 547 [ 750/1251 ( 60%)]  Loss: 2.902 (3.04)  Time: 0.779s, 1314.69/s  (0.789s, 1297.79/s)  LR: 2.894e-05  Data: 0.011 (0.012)
Train: 547 [ 800/1251 ( 64%)]  Loss: 3.067 (3.04)  Time: 0.827s, 1237.53/s  (0.788s, 1298.71/s)  LR: 2.894e-05  Data: 0.010 (0.012)
Train: 547 [ 850/1251 ( 68%)]  Loss: 3.009 (3.04)  Time: 0.772s, 1325.61/s  (0.789s, 1297.64/s)  LR: 2.894e-05  Data: 0.010 (0.012)
Train: 547 [ 900/1251 ( 72%)]  Loss: 2.678 (3.02)  Time: 0.808s, 1268.09/s  (0.790s, 1296.02/s)  LR: 2.894e-05  Data: 0.010 (0.012)
Train: 547 [ 950/1251 ( 76%)]  Loss: 2.996 (3.02)  Time: 0.806s, 1270.43/s  (0.790s, 1296.57/s)  LR: 2.894e-05  Data: 0.012 (0.012)
Train: 547 [1000/1251 ( 80%)]  Loss: 2.854 (3.01)  Time: 0.775s, 1321.05/s  (0.790s, 1296.28/s)  LR: 2.894e-05  Data: 0.010 (0.012)
Train: 547 [1050/1251 ( 84%)]  Loss: 3.162 (3.02)  Time: 0.806s, 1271.21/s  (0.790s, 1296.45/s)  LR: 2.894e-05  Data: 0.010 (0.012)
Train: 547 [1100/1251 ( 88%)]  Loss: 2.704 (3.01)  Time: 0.774s, 1323.82/s  (0.789s, 1297.48/s)  LR: 2.894e-05  Data: 0.010 (0.012)
Train: 547 [1150/1251 ( 92%)]  Loss: 2.994 (3.00)  Time: 0.778s, 1316.76/s  (0.789s, 1297.35/s)  LR: 2.894e-05  Data: 0.010 (0.012)
Train: 547 [1200/1251 ( 96%)]  Loss: 2.946 (3.00)  Time: 0.770s, 1329.14/s  (0.789s, 1298.36/s)  LR: 2.894e-05  Data: 0.010 (0.012)
Train: 547 [1250/1251 (100%)]  Loss: 3.168 (3.01)  Time: 0.798s, 1283.40/s  (0.789s, 1298.50/s)  LR: 2.894e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.544 (1.544)  Loss:  0.6353 (0.6353)  Acc@1: 92.6758 (92.6758)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.194 (0.575)  Loss:  0.7383 (1.0778)  Acc@1: 87.2642 (80.6100)  Acc@5: 97.9953 (95.3380)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-546.pth.tar', 80.726000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-535.pth.tar', 80.67399987060547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-530.pth.tar', 80.61400002929688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-547.pth.tar', 80.61000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-537.pth.tar', 80.60200002685546)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-544.pth.tar', 80.59999997070312)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-545.pth.tar', 80.57200005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-526.pth.tar', 80.57199997802735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-542.pth.tar', 80.56999994873047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-533.pth.tar', 80.55999994873046)

Train: 548 [   0/1251 (  0%)]  Loss: 3.134 (3.13)  Time: 2.238s,  457.50/s  (2.238s,  457.50/s)  LR: 2.823e-05  Data: 1.518 (1.518)
Train: 548 [  50/1251 (  4%)]  Loss: 2.986 (3.06)  Time: 0.817s, 1253.93/s  (0.820s, 1248.03/s)  LR: 2.823e-05  Data: 0.010 (0.046)
Train: 548 [ 100/1251 (  8%)]  Loss: 2.920 (3.01)  Time: 0.779s, 1314.52/s  (0.804s, 1272.89/s)  LR: 2.823e-05  Data: 0.010 (0.028)
Train: 548 [ 150/1251 ( 12%)]  Loss: 3.083 (3.03)  Time: 0.810s, 1264.90/s  (0.800s, 1279.85/s)  LR: 2.823e-05  Data: 0.010 (0.022)
Train: 548 [ 200/1251 ( 16%)]  Loss: 3.012 (3.03)  Time: 0.774s, 1322.21/s  (0.796s, 1286.33/s)  LR: 2.823e-05  Data: 0.010 (0.019)
Train: 548 [ 250/1251 ( 20%)]  Loss: 2.778 (2.99)  Time: 0.773s, 1325.39/s  (0.792s, 1292.58/s)  LR: 2.823e-05  Data: 0.010 (0.017)
Train: 548 [ 300/1251 ( 24%)]  Loss: 2.900 (2.97)  Time: 0.781s, 1311.02/s  (0.790s, 1296.27/s)  LR: 2.823e-05  Data: 0.010 (0.016)
Train: 548 [ 350/1251 ( 28%)]  Loss: 2.873 (2.96)  Time: 0.771s, 1328.70/s  (0.789s, 1298.37/s)  LR: 2.823e-05  Data: 0.009 (0.015)
Train: 548 [ 400/1251 ( 32%)]  Loss: 3.054 (2.97)  Time: 0.826s, 1240.04/s  (0.787s, 1301.10/s)  LR: 2.823e-05  Data: 0.010 (0.015)
Train: 548 [ 450/1251 ( 36%)]  Loss: 2.823 (2.96)  Time: 0.772s, 1325.91/s  (0.787s, 1301.72/s)  LR: 2.823e-05  Data: 0.010 (0.014)
Train: 548 [ 500/1251 ( 40%)]  Loss: 3.194 (2.98)  Time: 0.837s, 1222.72/s  (0.789s, 1297.63/s)  LR: 2.823e-05  Data: 0.010 (0.014)
Train: 548 [ 550/1251 ( 44%)]  Loss: 3.046 (2.98)  Time: 0.815s, 1255.94/s  (0.790s, 1295.90/s)  LR: 2.823e-05  Data: 0.010 (0.014)
Train: 548 [ 600/1251 ( 48%)]  Loss: 2.605 (2.95)  Time: 0.779s, 1314.21/s  (0.790s, 1295.74/s)  LR: 2.823e-05  Data: 0.010 (0.013)
Train: 548 [ 650/1251 ( 52%)]  Loss: 2.953 (2.95)  Time: 0.837s, 1223.81/s  (0.791s, 1293.90/s)  LR: 2.823e-05  Data: 0.010 (0.013)
Train: 548 [ 700/1251 ( 56%)]  Loss: 3.071 (2.96)  Time: 0.811s, 1261.93/s  (0.791s, 1295.08/s)  LR: 2.823e-05  Data: 0.014 (0.013)
Train: 548 [ 750/1251 ( 60%)]  Loss: 2.930 (2.96)  Time: 0.773s, 1324.18/s  (0.790s, 1296.13/s)  LR: 2.823e-05  Data: 0.013 (0.013)
Train: 548 [ 800/1251 ( 64%)]  Loss: 3.049 (2.97)  Time: 0.771s, 1328.09/s  (0.790s, 1296.94/s)  LR: 2.823e-05  Data: 0.010 (0.013)
Train: 548 [ 850/1251 ( 68%)]  Loss: 2.918 (2.96)  Time: 0.773s, 1325.49/s  (0.789s, 1297.76/s)  LR: 2.823e-05  Data: 0.010 (0.012)
Train: 548 [ 900/1251 ( 72%)]  Loss: 2.927 (2.96)  Time: 0.773s, 1324.31/s  (0.790s, 1296.83/s)  LR: 2.823e-05  Data: 0.010 (0.012)
Train: 548 [ 950/1251 ( 76%)]  Loss: 3.132 (2.97)  Time: 0.773s, 1325.12/s  (0.789s, 1297.87/s)  LR: 2.823e-05  Data: 0.010 (0.012)
Train: 548 [1000/1251 ( 80%)]  Loss: 2.846 (2.96)  Time: 0.775s, 1321.80/s  (0.789s, 1297.87/s)  LR: 2.823e-05  Data: 0.010 (0.012)
Train: 548 [1050/1251 ( 84%)]  Loss: 3.156 (2.97)  Time: 0.777s, 1318.53/s  (0.789s, 1297.96/s)  LR: 2.823e-05  Data: 0.010 (0.012)
Train: 548 [1100/1251 ( 88%)]  Loss: 2.926 (2.97)  Time: 0.773s, 1325.01/s  (0.789s, 1297.49/s)  LR: 2.823e-05  Data: 0.010 (0.012)
Train: 548 [1150/1251 ( 92%)]  Loss: 2.843 (2.96)  Time: 0.834s, 1227.66/s  (0.789s, 1297.40/s)  LR: 2.823e-05  Data: 0.009 (0.012)
Train: 548 [1200/1251 ( 96%)]  Loss: 2.767 (2.96)  Time: 0.770s, 1329.12/s  (0.789s, 1297.85/s)  LR: 2.823e-05  Data: 0.010 (0.012)
Train: 548 [1250/1251 (100%)]  Loss: 2.796 (2.95)  Time: 0.798s, 1283.56/s  (0.789s, 1297.55/s)  LR: 2.823e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.676 (1.676)  Loss:  0.6416 (0.6416)  Acc@1: 92.2852 (92.2852)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.194 (0.581)  Loss:  0.7451 (1.0626)  Acc@1: 87.0283 (80.6780)  Acc@5: 97.9953 (95.4420)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-546.pth.tar', 80.726000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-548.pth.tar', 80.67800005371093)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-535.pth.tar', 80.67399987060547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-530.pth.tar', 80.61400002929688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-547.pth.tar', 80.61000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-537.pth.tar', 80.60200002685546)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-544.pth.tar', 80.59999997070312)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-545.pth.tar', 80.57200005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-526.pth.tar', 80.57199997802735)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-542.pth.tar', 80.56999994873047)

Train: 549 [   0/1251 (  0%)]  Loss: 2.981 (2.98)  Time: 2.331s,  439.28/s  (2.331s,  439.28/s)  LR: 2.754e-05  Data: 1.603 (1.603)
Train: 549 [  50/1251 (  4%)]  Loss: 3.108 (3.04)  Time: 0.772s, 1326.34/s  (0.814s, 1257.63/s)  LR: 2.754e-05  Data: 0.010 (0.044)
Train: 549 [ 100/1251 (  8%)]  Loss: 3.062 (3.05)  Time: 0.822s, 1246.20/s  (0.799s, 1281.27/s)  LR: 2.754e-05  Data: 0.010 (0.027)
Train: 549 [ 150/1251 ( 12%)]  Loss: 2.700 (2.96)  Time: 0.771s, 1327.96/s  (0.797s, 1284.57/s)  LR: 2.754e-05  Data: 0.010 (0.022)
Train: 549 [ 200/1251 ( 16%)]  Loss: 3.116 (2.99)  Time: 0.808s, 1268.04/s  (0.798s, 1282.42/s)  LR: 2.754e-05  Data: 0.010 (0.019)
Train: 549 [ 250/1251 ( 20%)]  Loss: 2.777 (2.96)  Time: 0.821s, 1247.12/s  (0.801s, 1278.59/s)  LR: 2.754e-05  Data: 0.010 (0.017)
Train: 549 [ 300/1251 ( 24%)]  Loss: 3.141 (2.98)  Time: 0.868s, 1179.87/s  (0.798s, 1283.26/s)  LR: 2.754e-05  Data: 0.009 (0.016)
Train: 549 [ 350/1251 ( 28%)]  Loss: 3.198 (3.01)  Time: 0.807s, 1268.76/s  (0.797s, 1284.99/s)  LR: 2.754e-05  Data: 0.010 (0.015)
Train: 549 [ 400/1251 ( 32%)]  Loss: 2.650 (2.97)  Time: 0.806s, 1270.48/s  (0.798s, 1282.74/s)  LR: 2.754e-05  Data: 0.010 (0.014)
Train: 549 [ 450/1251 ( 36%)]  Loss: 3.178 (2.99)  Time: 0.815s, 1256.69/s  (0.797s, 1284.85/s)  LR: 2.754e-05  Data: 0.011 (0.014)
Train: 549 [ 500/1251 ( 40%)]  Loss: 3.202 (3.01)  Time: 0.812s, 1261.21/s  (0.798s, 1282.75/s)  LR: 2.754e-05  Data: 0.009 (0.013)
Train: 549 [ 550/1251 ( 44%)]  Loss: 2.874 (3.00)  Time: 0.808s, 1267.52/s  (0.798s, 1282.79/s)  LR: 2.754e-05  Data: 0.010 (0.013)
Train: 549 [ 600/1251 ( 48%)]  Loss: 3.084 (3.01)  Time: 0.888s, 1153.79/s  (0.799s, 1282.05/s)  LR: 2.754e-05  Data: 0.010 (0.013)
Train: 549 [ 650/1251 ( 52%)]  Loss: 2.891 (3.00)  Time: 0.779s, 1314.53/s  (0.797s, 1284.14/s)  LR: 2.754e-05  Data: 0.009 (0.013)
Train: 549 [ 700/1251 ( 56%)]  Loss: 3.062 (3.00)  Time: 0.770s, 1329.02/s  (0.797s, 1284.49/s)  LR: 2.754e-05  Data: 0.010 (0.012)
Train: 549 [ 750/1251 ( 60%)]  Loss: 2.800 (2.99)  Time: 0.773s, 1323.92/s  (0.796s, 1286.73/s)  LR: 2.754e-05  Data: 0.010 (0.012)
Train: 549 [ 800/1251 ( 64%)]  Loss: 2.942 (2.99)  Time: 0.785s, 1304.34/s  (0.795s, 1288.31/s)  LR: 2.754e-05  Data: 0.010 (0.012)
Train: 549 [ 850/1251 ( 68%)]  Loss: 2.583 (2.96)  Time: 0.816s, 1254.94/s  (0.795s, 1288.49/s)  LR: 2.754e-05  Data: 0.010 (0.012)
Train: 549 [ 900/1251 ( 72%)]  Loss: 3.151 (2.97)  Time: 0.798s, 1283.91/s  (0.794s, 1288.88/s)  LR: 2.754e-05  Data: 0.010 (0.012)
Train: 549 [ 950/1251 ( 76%)]  Loss: 2.883 (2.97)  Time: 0.773s, 1324.48/s  (0.794s, 1290.30/s)  LR: 2.754e-05  Data: 0.009 (0.012)
Train: 549 [1000/1251 ( 80%)]  Loss: 2.813 (2.96)  Time: 0.772s, 1327.02/s  (0.793s, 1291.45/s)  LR: 2.754e-05  Data: 0.009 (0.012)
Train: 549 [1050/1251 ( 84%)]  Loss: 3.057 (2.97)  Time: 0.818s, 1251.86/s  (0.793s, 1292.05/s)  LR: 2.754e-05  Data: 0.010 (0.012)
Train: 549 [1100/1251 ( 88%)]  Loss: 2.953 (2.97)  Time: 0.770s, 1330.44/s  (0.792s, 1292.76/s)  LR: 2.754e-05  Data: 0.009 (0.012)
Train: 549 [1150/1251 ( 92%)]  Loss: 3.060 (2.97)  Time: 0.773s, 1324.73/s  (0.792s, 1293.45/s)  LR: 2.754e-05  Data: 0.010 (0.011)
Train: 549 [1200/1251 ( 96%)]  Loss: 3.159 (2.98)  Time: 0.851s, 1203.06/s  (0.791s, 1293.97/s)  LR: 2.754e-05  Data: 0.009 (0.011)
Train: 549 [1250/1251 (100%)]  Loss: 2.821 (2.97)  Time: 0.797s, 1284.06/s  (0.792s, 1293.09/s)  LR: 2.754e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.606 (1.606)  Loss:  0.6304 (0.6304)  Acc@1: 92.6758 (92.6758)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.194 (0.563)  Loss:  0.7295 (1.0737)  Acc@1: 87.8538 (80.6600)  Acc@5: 97.8774 (95.4300)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-546.pth.tar', 80.726000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-548.pth.tar', 80.67800005371093)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-535.pth.tar', 80.67399987060547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-549.pth.tar', 80.65999989501952)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-530.pth.tar', 80.61400002929688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-547.pth.tar', 80.61000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-537.pth.tar', 80.60200002685546)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-544.pth.tar', 80.59999997070312)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-545.pth.tar', 80.57200005615235)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-526.pth.tar', 80.57199997802735)

Train: 550 [   0/1251 (  0%)]  Loss: 3.290 (3.29)  Time: 2.433s,  420.80/s  (2.433s,  420.80/s)  LR: 2.687e-05  Data: 1.661 (1.661)
Train: 550 [  50/1251 (  4%)]  Loss: 3.206 (3.25)  Time: 0.772s, 1325.85/s  (0.816s, 1255.61/s)  LR: 2.687e-05  Data: 0.009 (0.044)
Train: 550 [ 100/1251 (  8%)]  Loss: 2.707 (3.07)  Time: 0.773s, 1324.91/s  (0.803s, 1274.64/s)  LR: 2.687e-05  Data: 0.010 (0.027)
Train: 550 [ 150/1251 ( 12%)]  Loss: 2.869 (3.02)  Time: 0.799s, 1281.81/s  (0.799s, 1281.53/s)  LR: 2.687e-05  Data: 0.009 (0.022)
Train: 550 [ 200/1251 ( 16%)]  Loss: 2.820 (2.98)  Time: 0.772s, 1326.43/s  (0.797s, 1284.17/s)  LR: 2.687e-05  Data: 0.009 (0.019)
Train: 550 [ 250/1251 ( 20%)]  Loss: 2.947 (2.97)  Time: 0.814s, 1257.33/s  (0.796s, 1286.59/s)  LR: 2.687e-05  Data: 0.011 (0.017)
Train: 550 [ 300/1251 ( 24%)]  Loss: 2.810 (2.95)  Time: 0.780s, 1312.18/s  (0.795s, 1288.01/s)  LR: 2.687e-05  Data: 0.010 (0.016)
Train: 550 [ 350/1251 ( 28%)]  Loss: 2.811 (2.93)  Time: 0.771s, 1328.63/s  (0.792s, 1292.35/s)  LR: 2.687e-05  Data: 0.010 (0.015)
Train: 550 [ 400/1251 ( 32%)]  Loss: 2.850 (2.92)  Time: 0.774s, 1323.79/s  (0.791s, 1294.27/s)  LR: 2.687e-05  Data: 0.011 (0.014)
Train: 550 [ 450/1251 ( 36%)]  Loss: 3.119 (2.94)  Time: 0.780s, 1313.02/s  (0.792s, 1293.67/s)  LR: 2.687e-05  Data: 0.010 (0.014)
Train: 550 [ 500/1251 ( 40%)]  Loss: 2.904 (2.94)  Time: 0.771s, 1327.68/s  (0.792s, 1293.63/s)  LR: 2.687e-05  Data: 0.010 (0.014)
Train: 550 [ 550/1251 ( 44%)]  Loss: 2.909 (2.94)  Time: 0.776s, 1318.87/s  (0.791s, 1295.05/s)  LR: 2.687e-05  Data: 0.010 (0.013)
Train: 550 [ 600/1251 ( 48%)]  Loss: 3.116 (2.95)  Time: 0.772s, 1325.89/s  (0.790s, 1296.86/s)  LR: 2.687e-05  Data: 0.010 (0.013)
Train: 550 [ 650/1251 ( 52%)]  Loss: 2.704 (2.93)  Time: 0.773s, 1324.08/s  (0.789s, 1298.07/s)  LR: 2.687e-05  Data: 0.009 (0.013)
Train: 550 [ 700/1251 ( 56%)]  Loss: 3.183 (2.95)  Time: 0.809s, 1265.58/s  (0.789s, 1298.32/s)  LR: 2.687e-05  Data: 0.010 (0.012)
Train: 550 [ 750/1251 ( 60%)]  Loss: 2.644 (2.93)  Time: 0.774s, 1323.42/s  (0.789s, 1298.18/s)  LR: 2.687e-05  Data: 0.010 (0.012)
Train: 550 [ 800/1251 ( 64%)]  Loss: 2.972 (2.93)  Time: 0.814s, 1258.57/s  (0.789s, 1297.27/s)  LR: 2.687e-05  Data: 0.012 (0.012)
Train: 550 [ 850/1251 ( 68%)]  Loss: 2.925 (2.93)  Time: 0.808s, 1267.23/s  (0.789s, 1297.69/s)  LR: 2.687e-05  Data: 0.010 (0.012)
Train: 550 [ 900/1251 ( 72%)]  Loss: 2.923 (2.93)  Time: 0.780s, 1312.53/s  (0.789s, 1298.49/s)  LR: 2.687e-05  Data: 0.009 (0.012)
Train: 550 [ 950/1251 ( 76%)]  Loss: 3.157 (2.94)  Time: 0.807s, 1268.14/s  (0.789s, 1298.53/s)  LR: 2.687e-05  Data: 0.010 (0.012)
Train: 550 [1000/1251 ( 80%)]  Loss: 2.701 (2.93)  Time: 0.769s, 1330.88/s  (0.788s, 1299.51/s)  LR: 2.687e-05  Data: 0.010 (0.012)
Train: 550 [1050/1251 ( 84%)]  Loss: 2.864 (2.93)  Time: 0.815s, 1255.75/s  (0.788s, 1299.99/s)  LR: 2.687e-05  Data: 0.011 (0.012)
Train: 550 [1100/1251 ( 88%)]  Loss: 2.725 (2.92)  Time: 0.773s, 1324.72/s  (0.788s, 1299.58/s)  LR: 2.687e-05  Data: 0.010 (0.012)
Train: 550 [1150/1251 ( 92%)]  Loss: 2.892 (2.92)  Time: 0.807s, 1268.47/s  (0.788s, 1300.10/s)  LR: 2.687e-05  Data: 0.010 (0.011)
Train: 550 [1200/1251 ( 96%)]  Loss: 2.723 (2.91)  Time: 0.773s, 1325.17/s  (0.788s, 1299.21/s)  LR: 2.687e-05  Data: 0.010 (0.011)
Train: 550 [1250/1251 (100%)]  Loss: 3.010 (2.91)  Time: 0.761s, 1345.84/s  (0.788s, 1299.40/s)  LR: 2.687e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.597 (1.597)  Loss:  0.5845 (0.5845)  Acc@1: 92.6758 (92.6758)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.194 (0.566)  Loss:  0.6938 (1.0385)  Acc@1: 87.3821 (80.7160)  Acc@5: 98.2311 (95.3800)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-546.pth.tar', 80.726000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-550.pth.tar', 80.71599994873047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-548.pth.tar', 80.67800005371093)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-535.pth.tar', 80.67399987060547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-549.pth.tar', 80.65999989501952)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-530.pth.tar', 80.61400002929688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-547.pth.tar', 80.61000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-537.pth.tar', 80.60200002685546)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-544.pth.tar', 80.59999997070312)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-545.pth.tar', 80.57200005615235)

Train: 551 [   0/1251 (  0%)]  Loss: 2.931 (2.93)  Time: 2.419s,  423.27/s  (2.419s,  423.27/s)  LR: 2.620e-05  Data: 1.693 (1.693)
Train: 551 [  50/1251 (  4%)]  Loss: 2.681 (2.81)  Time: 0.772s, 1326.10/s  (0.815s, 1255.84/s)  LR: 2.620e-05  Data: 0.010 (0.048)
Train: 551 [ 100/1251 (  8%)]  Loss: 2.965 (2.86)  Time: 0.784s, 1306.71/s  (0.800s, 1280.77/s)  LR: 2.620e-05  Data: 0.011 (0.029)
Train: 551 [ 150/1251 ( 12%)]  Loss: 3.004 (2.90)  Time: 0.772s, 1326.62/s  (0.795s, 1287.27/s)  LR: 2.620e-05  Data: 0.009 (0.023)
Train: 551 [ 200/1251 ( 16%)]  Loss: 2.845 (2.89)  Time: 0.791s, 1295.07/s  (0.794s, 1290.45/s)  LR: 2.620e-05  Data: 0.010 (0.019)
Train: 551 [ 250/1251 ( 20%)]  Loss: 2.390 (2.80)  Time: 0.811s, 1262.37/s  (0.792s, 1293.01/s)  LR: 2.620e-05  Data: 0.010 (0.018)
Train: 551 [ 300/1251 ( 24%)]  Loss: 2.940 (2.82)  Time: 0.790s, 1295.53/s  (0.791s, 1294.80/s)  LR: 2.620e-05  Data: 0.010 (0.016)
Train: 551 [ 350/1251 ( 28%)]  Loss: 2.923 (2.83)  Time: 0.774s, 1323.41/s  (0.791s, 1295.23/s)  LR: 2.620e-05  Data: 0.010 (0.015)
Train: 551 [ 400/1251 ( 32%)]  Loss: 3.142 (2.87)  Time: 0.777s, 1318.27/s  (0.790s, 1296.90/s)  LR: 2.620e-05  Data: 0.010 (0.015)
Train: 551 [ 450/1251 ( 36%)]  Loss: 3.182 (2.90)  Time: 0.825s, 1241.71/s  (0.791s, 1294.94/s)  LR: 2.620e-05  Data: 0.014 (0.014)
Train: 551 [ 500/1251 ( 40%)]  Loss: 3.150 (2.92)  Time: 0.777s, 1318.34/s  (0.793s, 1291.36/s)  LR: 2.620e-05  Data: 0.011 (0.014)
Train: 551 [ 550/1251 ( 44%)]  Loss: 2.991 (2.93)  Time: 0.820s, 1249.45/s  (0.793s, 1291.31/s)  LR: 2.620e-05  Data: 0.011 (0.014)
Train: 551 [ 600/1251 ( 48%)]  Loss: 2.694 (2.91)  Time: 0.776s, 1320.00/s  (0.792s, 1292.65/s)  LR: 2.620e-05  Data: 0.009 (0.013)
Train: 551 [ 650/1251 ( 52%)]  Loss: 2.930 (2.91)  Time: 0.814s, 1257.41/s  (0.793s, 1291.24/s)  LR: 2.620e-05  Data: 0.011 (0.013)
Train: 551 [ 700/1251 ( 56%)]  Loss: 3.011 (2.92)  Time: 0.782s, 1308.89/s  (0.793s, 1291.83/s)  LR: 2.620e-05  Data: 0.013 (0.013)
Train: 551 [ 750/1251 ( 60%)]  Loss: 2.797 (2.91)  Time: 0.771s, 1327.52/s  (0.792s, 1292.20/s)  LR: 2.620e-05  Data: 0.010 (0.013)
Train: 551 [ 800/1251 ( 64%)]  Loss: 2.935 (2.91)  Time: 0.772s, 1326.91/s  (0.792s, 1293.25/s)  LR: 2.620e-05  Data: 0.010 (0.012)
Train: 551 [ 850/1251 ( 68%)]  Loss: 2.895 (2.91)  Time: 0.778s, 1315.68/s  (0.791s, 1294.67/s)  LR: 2.620e-05  Data: 0.010 (0.012)
Train: 551 [ 900/1251 ( 72%)]  Loss: 2.961 (2.91)  Time: 0.774s, 1322.90/s  (0.791s, 1295.30/s)  LR: 2.620e-05  Data: 0.009 (0.012)
Train: 551 [ 950/1251 ( 76%)]  Loss: 2.950 (2.92)  Time: 0.778s, 1316.14/s  (0.790s, 1295.62/s)  LR: 2.620e-05  Data: 0.010 (0.012)
Train: 551 [1000/1251 ( 80%)]  Loss: 2.974 (2.92)  Time: 0.813s, 1260.31/s  (0.791s, 1294.72/s)  LR: 2.620e-05  Data: 0.010 (0.012)
Train: 551 [1050/1251 ( 84%)]  Loss: 3.096 (2.93)  Time: 0.812s, 1261.12/s  (0.791s, 1294.93/s)  LR: 2.620e-05  Data: 0.009 (0.012)
Train: 551 [1100/1251 ( 88%)]  Loss: 3.238 (2.94)  Time: 0.771s, 1327.75/s  (0.791s, 1294.25/s)  LR: 2.620e-05  Data: 0.009 (0.012)
Train: 551 [1150/1251 ( 92%)]  Loss: 2.952 (2.94)  Time: 0.773s, 1324.00/s  (0.791s, 1294.98/s)  LR: 2.620e-05  Data: 0.009 (0.012)
Train: 551 [1200/1251 ( 96%)]  Loss: 2.997 (2.94)  Time: 0.775s, 1321.27/s  (0.791s, 1294.83/s)  LR: 2.620e-05  Data: 0.010 (0.012)
Train: 551 [1250/1251 (100%)]  Loss: 3.020 (2.95)  Time: 0.761s, 1345.54/s  (0.790s, 1295.71/s)  LR: 2.620e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.569 (1.569)  Loss:  0.6265 (0.6265)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.194 (0.561)  Loss:  0.7256 (1.0784)  Acc@1: 87.8538 (80.7260)  Acc@5: 97.9953 (95.4640)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-546.pth.tar', 80.726000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-551.pth.tar', 80.72600002441406)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-550.pth.tar', 80.71599994873047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-548.pth.tar', 80.67800005371093)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-535.pth.tar', 80.67399987060547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-549.pth.tar', 80.65999989501952)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-530.pth.tar', 80.61400002929688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-547.pth.tar', 80.61000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-537.pth.tar', 80.60200002685546)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-544.pth.tar', 80.59999997070312)

Train: 552 [   0/1251 (  0%)]  Loss: 3.389 (3.39)  Time: 2.398s,  427.07/s  (2.398s,  427.07/s)  LR: 2.555e-05  Data: 1.673 (1.673)
Train: 552 [  50/1251 (  4%)]  Loss: 2.731 (3.06)  Time: 0.773s, 1325.08/s  (0.819s, 1249.63/s)  LR: 2.555e-05  Data: 0.011 (0.045)
Train: 552 [ 100/1251 (  8%)]  Loss: 3.146 (3.09)  Time: 0.804s, 1273.37/s  (0.803s, 1275.99/s)  LR: 2.555e-05  Data: 0.010 (0.028)
Train: 552 [ 150/1251 ( 12%)]  Loss: 2.975 (3.06)  Time: 0.822s, 1245.19/s  (0.805s, 1272.08/s)  LR: 2.555e-05  Data: 0.010 (0.022)
Train: 552 [ 200/1251 ( 16%)]  Loss: 2.898 (3.03)  Time: 0.791s, 1295.25/s  (0.800s, 1279.49/s)  LR: 2.555e-05  Data: 0.010 (0.019)
Train: 552 [ 250/1251 ( 20%)]  Loss: 2.856 (3.00)  Time: 0.781s, 1311.11/s  (0.796s, 1286.29/s)  LR: 2.555e-05  Data: 0.012 (0.017)
Train: 552 [ 300/1251 ( 24%)]  Loss: 3.305 (3.04)  Time: 0.771s, 1327.37/s  (0.795s, 1287.94/s)  LR: 2.555e-05  Data: 0.010 (0.016)
Train: 552 [ 350/1251 ( 28%)]  Loss: 3.002 (3.04)  Time: 0.771s, 1328.64/s  (0.793s, 1291.12/s)  LR: 2.555e-05  Data: 0.009 (0.015)
Train: 552 [ 400/1251 ( 32%)]  Loss: 2.775 (3.01)  Time: 0.784s, 1306.46/s  (0.791s, 1294.52/s)  LR: 2.555e-05  Data: 0.010 (0.015)
Train: 552 [ 450/1251 ( 36%)]  Loss: 3.087 (3.02)  Time: 0.806s, 1270.15/s  (0.793s, 1291.40/s)  LR: 2.555e-05  Data: 0.010 (0.014)
Train: 552 [ 500/1251 ( 40%)]  Loss: 3.135 (3.03)  Time: 0.781s, 1311.93/s  (0.793s, 1290.95/s)  LR: 2.555e-05  Data: 0.010 (0.014)
Train: 552 [ 550/1251 ( 44%)]  Loss: 2.992 (3.02)  Time: 0.772s, 1325.78/s  (0.792s, 1293.03/s)  LR: 2.555e-05  Data: 0.010 (0.013)
Train: 552 [ 600/1251 ( 48%)]  Loss: 3.008 (3.02)  Time: 0.771s, 1328.85/s  (0.791s, 1294.66/s)  LR: 2.555e-05  Data: 0.009 (0.013)
Train: 552 [ 650/1251 ( 52%)]  Loss: 2.985 (3.02)  Time: 0.792s, 1292.54/s  (0.790s, 1296.17/s)  LR: 2.555e-05  Data: 0.010 (0.013)
Train: 552 [ 700/1251 ( 56%)]  Loss: 2.950 (3.02)  Time: 0.779s, 1314.98/s  (0.791s, 1294.91/s)  LR: 2.555e-05  Data: 0.009 (0.013)
Train: 552 [ 750/1251 ( 60%)]  Loss: 2.903 (3.01)  Time: 0.776s, 1319.92/s  (0.790s, 1295.98/s)  LR: 2.555e-05  Data: 0.013 (0.012)
Train: 552 [ 800/1251 ( 64%)]  Loss: 2.926 (3.00)  Time: 0.774s, 1323.21/s  (0.790s, 1296.46/s)  LR: 2.555e-05  Data: 0.012 (0.012)
Train: 552 [ 850/1251 ( 68%)]  Loss: 2.963 (3.00)  Time: 0.771s, 1327.60/s  (0.789s, 1297.47/s)  LR: 2.555e-05  Data: 0.010 (0.012)
Train: 552 [ 900/1251 ( 72%)]  Loss: 3.156 (3.01)  Time: 0.780s, 1312.12/s  (0.789s, 1298.10/s)  LR: 2.555e-05  Data: 0.010 (0.012)
Train: 552 [ 950/1251 ( 76%)]  Loss: 2.949 (3.01)  Time: 0.811s, 1263.19/s  (0.790s, 1296.65/s)  LR: 2.555e-05  Data: 0.010 (0.012)
Train: 552 [1000/1251 ( 80%)]  Loss: 2.788 (3.00)  Time: 0.778s, 1315.71/s  (0.789s, 1297.13/s)  LR: 2.555e-05  Data: 0.010 (0.012)
Train: 552 [1050/1251 ( 84%)]  Loss: 2.746 (2.98)  Time: 0.785s, 1305.00/s  (0.789s, 1297.93/s)  LR: 2.555e-05  Data: 0.012 (0.012)
Train: 552 [1100/1251 ( 88%)]  Loss: 3.166 (2.99)  Time: 0.775s, 1322.11/s  (0.789s, 1297.66/s)  LR: 2.555e-05  Data: 0.010 (0.012)
Train: 552 [1150/1251 ( 92%)]  Loss: 3.155 (3.00)  Time: 0.772s, 1325.88/s  (0.789s, 1298.33/s)  LR: 2.555e-05  Data: 0.010 (0.012)
Train: 552 [1200/1251 ( 96%)]  Loss: 2.907 (3.00)  Time: 0.793s, 1291.98/s  (0.788s, 1298.77/s)  LR: 2.555e-05  Data: 0.010 (0.012)
Train: 552 [1250/1251 (100%)]  Loss: 2.789 (2.99)  Time: 0.765s, 1339.10/s  (0.788s, 1298.99/s)  LR: 2.555e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.672 (1.672)  Loss:  0.5752 (0.5752)  Acc@1: 92.2852 (92.2852)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.194 (0.585)  Loss:  0.6992 (1.0284)  Acc@1: 86.9104 (80.8300)  Acc@5: 98.4670 (95.4420)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-552.pth.tar', 80.83000000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-546.pth.tar', 80.726000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-551.pth.tar', 80.72600002441406)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-550.pth.tar', 80.71599994873047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-548.pth.tar', 80.67800005371093)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-535.pth.tar', 80.67399987060547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-549.pth.tar', 80.65999989501952)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-530.pth.tar', 80.61400002929688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-547.pth.tar', 80.61000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-537.pth.tar', 80.60200002685546)

Train: 553 [   0/1251 (  0%)]  Loss: 2.873 (2.87)  Time: 2.367s,  432.68/s  (2.367s,  432.68/s)  LR: 2.491e-05  Data: 1.633 (1.633)
Train: 553 [  50/1251 (  4%)]  Loss: 3.051 (2.96)  Time: 0.778s, 1316.09/s  (0.809s, 1265.16/s)  LR: 2.491e-05  Data: 0.010 (0.044)
Train: 553 [ 100/1251 (  8%)]  Loss: 3.026 (2.98)  Time: 0.819s, 1250.52/s  (0.803s, 1274.79/s)  LR: 2.491e-05  Data: 0.010 (0.027)
Train: 553 [ 150/1251 ( 12%)]  Loss: 2.784 (2.93)  Time: 0.784s, 1305.44/s  (0.800s, 1280.43/s)  LR: 2.491e-05  Data: 0.010 (0.021)
Train: 553 [ 200/1251 ( 16%)]  Loss: 3.138 (2.97)  Time: 0.818s, 1252.22/s  (0.798s, 1283.41/s)  LR: 2.491e-05  Data: 0.013 (0.019)
Train: 553 [ 250/1251 ( 20%)]  Loss: 2.844 (2.95)  Time: 0.772s, 1326.60/s  (0.795s, 1287.84/s)  LR: 2.491e-05  Data: 0.010 (0.017)
Train: 553 [ 300/1251 ( 24%)]  Loss: 2.963 (2.95)  Time: 0.821s, 1246.60/s  (0.794s, 1289.65/s)  LR: 2.491e-05  Data: 0.010 (0.016)
Train: 553 [ 350/1251 ( 28%)]  Loss: 2.810 (2.94)  Time: 0.787s, 1300.51/s  (0.794s, 1290.20/s)  LR: 2.491e-05  Data: 0.009 (0.015)
Train: 553 [ 400/1251 ( 32%)]  Loss: 3.025 (2.95)  Time: 0.777s, 1317.25/s  (0.792s, 1293.18/s)  LR: 2.491e-05  Data: 0.010 (0.014)
Train: 553 [ 450/1251 ( 36%)]  Loss: 3.166 (2.97)  Time: 0.783s, 1307.17/s  (0.790s, 1295.87/s)  LR: 2.491e-05  Data: 0.010 (0.014)
Train: 553 [ 500/1251 ( 40%)]  Loss: 2.835 (2.96)  Time: 0.772s, 1325.95/s  (0.789s, 1297.25/s)  LR: 2.491e-05  Data: 0.010 (0.014)
Train: 553 [ 550/1251 ( 44%)]  Loss: 3.155 (2.97)  Time: 0.807s, 1268.79/s  (0.788s, 1298.91/s)  LR: 2.491e-05  Data: 0.009 (0.013)
Train: 553 [ 600/1251 ( 48%)]  Loss: 2.756 (2.96)  Time: 0.813s, 1259.39/s  (0.790s, 1296.34/s)  LR: 2.491e-05  Data: 0.010 (0.013)
Train: 553 [ 650/1251 ( 52%)]  Loss: 3.239 (2.98)  Time: 0.810s, 1264.21/s  (0.790s, 1296.53/s)  LR: 2.491e-05  Data: 0.010 (0.013)
Train: 553 [ 700/1251 ( 56%)]  Loss: 3.038 (2.98)  Time: 0.777s, 1318.63/s  (0.790s, 1296.62/s)  LR: 2.491e-05  Data: 0.010 (0.013)
Train: 553 [ 750/1251 ( 60%)]  Loss: 2.736 (2.96)  Time: 0.771s, 1328.18/s  (0.789s, 1297.42/s)  LR: 2.491e-05  Data: 0.009 (0.012)
Train: 553 [ 800/1251 ( 64%)]  Loss: 2.862 (2.96)  Time: 0.818s, 1252.10/s  (0.790s, 1296.67/s)  LR: 2.491e-05  Data: 0.010 (0.012)
Train: 553 [ 850/1251 ( 68%)]  Loss: 2.768 (2.95)  Time: 0.785s, 1303.79/s  (0.790s, 1296.49/s)  LR: 2.491e-05  Data: 0.010 (0.012)
Train: 553 [ 900/1251 ( 72%)]  Loss: 2.934 (2.95)  Time: 0.772s, 1326.47/s  (0.790s, 1296.91/s)  LR: 2.491e-05  Data: 0.010 (0.012)
Train: 553 [ 950/1251 ( 76%)]  Loss: 2.882 (2.94)  Time: 0.806s, 1270.15/s  (0.789s, 1297.53/s)  LR: 2.491e-05  Data: 0.009 (0.012)
Train: 553 [1000/1251 ( 80%)]  Loss: 3.122 (2.95)  Time: 0.772s, 1325.57/s  (0.789s, 1297.04/s)  LR: 2.491e-05  Data: 0.010 (0.012)
Train: 553 [1050/1251 ( 84%)]  Loss: 2.998 (2.95)  Time: 0.773s, 1325.29/s  (0.789s, 1297.48/s)  LR: 2.491e-05  Data: 0.009 (0.012)
Train: 553 [1100/1251 ( 88%)]  Loss: 3.045 (2.96)  Time: 0.787s, 1301.16/s  (0.789s, 1298.29/s)  LR: 2.491e-05  Data: 0.010 (0.012)
Train: 553 [1150/1251 ( 92%)]  Loss: 3.031 (2.96)  Time: 0.778s, 1316.16/s  (0.789s, 1298.51/s)  LR: 2.491e-05  Data: 0.010 (0.012)
Train: 553 [1200/1251 ( 96%)]  Loss: 2.980 (2.96)  Time: 0.772s, 1326.31/s  (0.789s, 1298.57/s)  LR: 2.491e-05  Data: 0.010 (0.012)
Train: 553 [1250/1251 (100%)]  Loss: 3.057 (2.97)  Time: 0.772s, 1325.72/s  (0.788s, 1299.20/s)  LR: 2.491e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.529 (1.529)  Loss:  0.5957 (0.5957)  Acc@1: 92.3828 (92.3828)  Acc@5: 98.7305 (98.7305)
Test: [  48/48]  Time: 0.194 (0.574)  Loss:  0.6973 (1.0365)  Acc@1: 87.1462 (80.6920)  Acc@5: 98.2311 (95.4300)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-552.pth.tar', 80.83000000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-546.pth.tar', 80.726000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-551.pth.tar', 80.72600002441406)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-550.pth.tar', 80.71599994873047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-553.pth.tar', 80.69199997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-548.pth.tar', 80.67800005371093)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-535.pth.tar', 80.67399987060547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-549.pth.tar', 80.65999989501952)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-530.pth.tar', 80.61400002929688)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-547.pth.tar', 80.61000002685547)

Train: 554 [   0/1251 (  0%)]  Loss: 3.262 (3.26)  Time: 2.258s,  453.48/s  (2.258s,  453.48/s)  LR: 2.429e-05  Data: 1.525 (1.525)
Train: 554 [  50/1251 (  4%)]  Loss: 2.932 (3.10)  Time: 0.777s, 1318.61/s  (0.839s, 1220.67/s)  LR: 2.429e-05  Data: 0.010 (0.046)
Train: 554 [ 100/1251 (  8%)]  Loss: 3.234 (3.14)  Time: 0.813s, 1259.98/s  (0.814s, 1258.14/s)  LR: 2.429e-05  Data: 0.009 (0.028)
Train: 554 [ 150/1251 ( 12%)]  Loss: 2.569 (3.00)  Time: 0.771s, 1328.25/s  (0.806s, 1271.01/s)  LR: 2.429e-05  Data: 0.009 (0.022)
Train: 554 [ 200/1251 ( 16%)]  Loss: 2.904 (2.98)  Time: 0.804s, 1273.48/s  (0.803s, 1274.48/s)  LR: 2.429e-05  Data: 0.010 (0.019)
Train: 554 [ 250/1251 ( 20%)]  Loss: 3.141 (3.01)  Time: 0.771s, 1327.33/s  (0.800s, 1280.01/s)  LR: 2.429e-05  Data: 0.009 (0.017)
Train: 554 [ 300/1251 ( 24%)]  Loss: 2.712 (2.96)  Time: 0.770s, 1329.19/s  (0.800s, 1279.45/s)  LR: 2.429e-05  Data: 0.009 (0.016)
Train: 554 [ 350/1251 ( 28%)]  Loss: 3.114 (2.98)  Time: 0.825s, 1241.70/s  (0.799s, 1281.55/s)  LR: 2.429e-05  Data: 0.010 (0.015)
Train: 554 [ 400/1251 ( 32%)]  Loss: 2.923 (2.98)  Time: 0.769s, 1331.61/s  (0.798s, 1283.65/s)  LR: 2.429e-05  Data: 0.010 (0.014)
Train: 554 [ 450/1251 ( 36%)]  Loss: 3.024 (2.98)  Time: 0.775s, 1320.48/s  (0.797s, 1284.82/s)  LR: 2.429e-05  Data: 0.009 (0.014)
Train: 554 [ 500/1251 ( 40%)]  Loss: 2.788 (2.96)  Time: 0.772s, 1325.80/s  (0.797s, 1284.86/s)  LR: 2.429e-05  Data: 0.010 (0.014)
Train: 554 [ 550/1251 ( 44%)]  Loss: 2.852 (2.95)  Time: 0.780s, 1312.19/s  (0.795s, 1287.37/s)  LR: 2.429e-05  Data: 0.010 (0.013)
Train: 554 [ 600/1251 ( 48%)]  Loss: 2.755 (2.94)  Time: 0.776s, 1320.03/s  (0.795s, 1288.47/s)  LR: 2.429e-05  Data: 0.011 (0.013)
Train: 554 [ 650/1251 ( 52%)]  Loss: 2.944 (2.94)  Time: 0.771s, 1328.37/s  (0.794s, 1290.30/s)  LR: 2.429e-05  Data: 0.010 (0.013)
Train: 554 [ 700/1251 ( 56%)]  Loss: 2.677 (2.92)  Time: 0.773s, 1324.38/s  (0.793s, 1291.49/s)  LR: 2.429e-05  Data: 0.009 (0.013)
Train: 554 [ 750/1251 ( 60%)]  Loss: 2.807 (2.91)  Time: 0.816s, 1255.42/s  (0.793s, 1292.11/s)  LR: 2.429e-05  Data: 0.012 (0.012)
Train: 554 [ 800/1251 ( 64%)]  Loss: 2.623 (2.90)  Time: 0.771s, 1328.67/s  (0.792s, 1293.46/s)  LR: 2.429e-05  Data: 0.009 (0.012)
Train: 554 [ 850/1251 ( 68%)]  Loss: 2.468 (2.87)  Time: 0.858s, 1193.92/s  (0.791s, 1294.04/s)  LR: 2.429e-05  Data: 0.010 (0.012)
Train: 554 [ 900/1251 ( 72%)]  Loss: 3.047 (2.88)  Time: 0.815s, 1256.63/s  (0.791s, 1294.03/s)  LR: 2.429e-05  Data: 0.011 (0.012)
Train: 554 [ 950/1251 ( 76%)]  Loss: 2.777 (2.88)  Time: 0.772s, 1325.62/s  (0.791s, 1295.26/s)  LR: 2.429e-05  Data: 0.009 (0.012)
Train: 554 [1000/1251 ( 80%)]  Loss: 2.820 (2.87)  Time: 0.815s, 1255.75/s  (0.790s, 1295.87/s)  LR: 2.429e-05  Data: 0.011 (0.012)
Train: 554 [1050/1251 ( 84%)]  Loss: 3.252 (2.89)  Time: 0.853s, 1200.49/s  (0.790s, 1296.42/s)  LR: 2.429e-05  Data: 0.009 (0.012)
Train: 554 [1100/1251 ( 88%)]  Loss: 2.585 (2.88)  Time: 0.778s, 1316.09/s  (0.790s, 1295.72/s)  LR: 2.429e-05  Data: 0.009 (0.012)
Train: 554 [1150/1251 ( 92%)]  Loss: 2.609 (2.87)  Time: 0.790s, 1295.75/s  (0.791s, 1294.97/s)  LR: 2.429e-05  Data: 0.016 (0.011)
Train: 554 [1200/1251 ( 96%)]  Loss: 3.100 (2.88)  Time: 0.774s, 1323.44/s  (0.790s, 1295.70/s)  LR: 2.429e-05  Data: 0.010 (0.011)
Train: 554 [1250/1251 (100%)]  Loss: 2.904 (2.88)  Time: 0.794s, 1290.15/s  (0.790s, 1296.60/s)  LR: 2.429e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.537 (1.537)  Loss:  0.6235 (0.6235)  Acc@1: 92.2852 (92.2852)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.194 (0.591)  Loss:  0.7153 (1.0559)  Acc@1: 86.9104 (80.8340)  Acc@5: 98.3491 (95.4160)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-554.pth.tar', 80.8340000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-552.pth.tar', 80.83000000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-546.pth.tar', 80.726000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-551.pth.tar', 80.72600002441406)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-550.pth.tar', 80.71599994873047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-553.pth.tar', 80.69199997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-548.pth.tar', 80.67800005371093)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-535.pth.tar', 80.67399987060547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-549.pth.tar', 80.65999989501952)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-530.pth.tar', 80.61400002929688)

Train: 555 [   0/1251 (  0%)]  Loss: 2.600 (2.60)  Time: 2.340s,  437.54/s  (2.340s,  437.54/s)  LR: 2.368e-05  Data: 1.601 (1.601)
Train: 555 [  50/1251 (  4%)]  Loss: 3.000 (2.80)  Time: 0.772s, 1326.12/s  (0.816s, 1254.66/s)  LR: 2.368e-05  Data: 0.010 (0.045)
Train: 555 [ 100/1251 (  8%)]  Loss: 2.908 (2.84)  Time: 0.781s, 1311.09/s  (0.803s, 1274.50/s)  LR: 2.368e-05  Data: 0.012 (0.028)
Train: 555 [ 150/1251 ( 12%)]  Loss: 2.966 (2.87)  Time: 0.773s, 1325.42/s  (0.800s, 1279.47/s)  LR: 2.368e-05  Data: 0.010 (0.022)
Train: 555 [ 200/1251 ( 16%)]  Loss: 2.740 (2.84)  Time: 0.772s, 1326.85/s  (0.799s, 1281.61/s)  LR: 2.368e-05  Data: 0.009 (0.019)
Train: 555 [ 250/1251 ( 20%)]  Loss: 2.906 (2.85)  Time: 0.774s, 1323.80/s  (0.802s, 1276.63/s)  LR: 2.368e-05  Data: 0.010 (0.017)
Train: 555 [ 300/1251 ( 24%)]  Loss: 2.648 (2.82)  Time: 0.773s, 1324.42/s  (0.800s, 1279.32/s)  LR: 2.368e-05  Data: 0.010 (0.016)
Train: 555 [ 350/1251 ( 28%)]  Loss: 2.720 (2.81)  Time: 0.771s, 1327.79/s  (0.798s, 1283.25/s)  LR: 2.368e-05  Data: 0.009 (0.015)
Train: 555 [ 400/1251 ( 32%)]  Loss: 3.134 (2.85)  Time: 0.773s, 1324.95/s  (0.797s, 1285.40/s)  LR: 2.368e-05  Data: 0.009 (0.015)
Train: 555 [ 450/1251 ( 36%)]  Loss: 3.048 (2.87)  Time: 0.776s, 1318.74/s  (0.795s, 1287.26/s)  LR: 2.368e-05  Data: 0.010 (0.014)
Train: 555 [ 500/1251 ( 40%)]  Loss: 3.053 (2.88)  Time: 0.787s, 1300.69/s  (0.795s, 1288.27/s)  LR: 2.368e-05  Data: 0.009 (0.014)
Train: 555 [ 550/1251 ( 44%)]  Loss: 2.910 (2.89)  Time: 0.776s, 1319.29/s  (0.794s, 1290.44/s)  LR: 2.368e-05  Data: 0.010 (0.013)
Train: 555 [ 600/1251 ( 48%)]  Loss: 3.175 (2.91)  Time: 0.807s, 1269.19/s  (0.793s, 1290.69/s)  LR: 2.368e-05  Data: 0.013 (0.013)
Train: 555 [ 650/1251 ( 52%)]  Loss: 2.881 (2.91)  Time: 0.821s, 1247.53/s  (0.793s, 1291.25/s)  LR: 2.368e-05  Data: 0.010 (0.013)
Train: 555 [ 700/1251 ( 56%)]  Loss: 2.963 (2.91)  Time: 0.774s, 1323.48/s  (0.792s, 1292.19/s)  LR: 2.368e-05  Data: 0.010 (0.013)
Train: 555 [ 750/1251 ( 60%)]  Loss: 3.309 (2.94)  Time: 0.777s, 1317.65/s  (0.792s, 1292.90/s)  LR: 2.368e-05  Data: 0.010 (0.012)
Train: 555 [ 800/1251 ( 64%)]  Loss: 2.977 (2.94)  Time: 0.809s, 1265.39/s  (0.791s, 1293.99/s)  LR: 2.368e-05  Data: 0.009 (0.012)
Train: 555 [ 850/1251 ( 68%)]  Loss: 2.898 (2.94)  Time: 0.780s, 1312.32/s  (0.791s, 1295.08/s)  LR: 2.368e-05  Data: 0.010 (0.012)
Train: 555 [ 900/1251 ( 72%)]  Loss: 2.783 (2.93)  Time: 0.771s, 1327.30/s  (0.790s, 1296.24/s)  LR: 2.368e-05  Data: 0.010 (0.012)
Train: 555 [ 950/1251 ( 76%)]  Loss: 2.754 (2.92)  Time: 0.773s, 1324.46/s  (0.790s, 1296.80/s)  LR: 2.368e-05  Data: 0.010 (0.012)
Train: 555 [1000/1251 ( 80%)]  Loss: 3.011 (2.92)  Time: 0.816s, 1254.17/s  (0.790s, 1296.44/s)  LR: 2.368e-05  Data: 0.012 (0.012)
Train: 555 [1050/1251 ( 84%)]  Loss: 2.883 (2.92)  Time: 0.774s, 1323.39/s  (0.790s, 1296.46/s)  LR: 2.368e-05  Data: 0.010 (0.012)
Train: 555 [1100/1251 ( 88%)]  Loss: 2.606 (2.91)  Time: 0.769s, 1331.54/s  (0.789s, 1297.33/s)  LR: 2.368e-05  Data: 0.009 (0.012)
Train: 555 [1150/1251 ( 92%)]  Loss: 2.922 (2.91)  Time: 0.779s, 1313.93/s  (0.789s, 1297.57/s)  LR: 2.368e-05  Data: 0.009 (0.012)
Train: 555 [1200/1251 ( 96%)]  Loss: 2.730 (2.90)  Time: 0.775s, 1321.15/s  (0.790s, 1296.25/s)  LR: 2.368e-05  Data: 0.010 (0.012)
Train: 555 [1250/1251 (100%)]  Loss: 3.017 (2.91)  Time: 0.767s, 1335.92/s  (0.789s, 1297.24/s)  LR: 2.368e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.555 (1.555)  Loss:  0.5991 (0.5991)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.7305 (98.7305)
Test: [  48/48]  Time: 0.194 (0.562)  Loss:  0.7041 (1.0253)  Acc@1: 86.9104 (80.8020)  Acc@5: 98.3491 (95.4580)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-554.pth.tar', 80.8340000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-552.pth.tar', 80.83000000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-555.pth.tar', 80.80200013183594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-546.pth.tar', 80.726000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-551.pth.tar', 80.72600002441406)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-550.pth.tar', 80.71599994873047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-553.pth.tar', 80.69199997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-548.pth.tar', 80.67800005371093)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-535.pth.tar', 80.67399987060547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-549.pth.tar', 80.65999989501952)

Train: 556 [   0/1251 (  0%)]  Loss: 2.742 (2.74)  Time: 2.439s,  419.78/s  (2.439s,  419.78/s)  LR: 2.308e-05  Data: 1.654 (1.654)
Train: 556 [  50/1251 (  4%)]  Loss: 2.960 (2.85)  Time: 0.815s, 1256.58/s  (0.838s, 1222.50/s)  LR: 2.308e-05  Data: 0.009 (0.047)
Train: 556 [ 100/1251 (  8%)]  Loss: 3.040 (2.91)  Time: 0.775s, 1321.03/s  (0.814s, 1257.81/s)  LR: 2.308e-05  Data: 0.010 (0.029)
Train: 556 [ 150/1251 ( 12%)]  Loss: 3.141 (2.97)  Time: 0.772s, 1327.08/s  (0.804s, 1274.08/s)  LR: 2.308e-05  Data: 0.010 (0.023)
Train: 556 [ 200/1251 ( 16%)]  Loss: 2.732 (2.92)  Time: 0.775s, 1320.61/s  (0.800s, 1280.64/s)  LR: 2.308e-05  Data: 0.011 (0.019)
Train: 556 [ 250/1251 ( 20%)]  Loss: 2.761 (2.90)  Time: 0.806s, 1269.71/s  (0.797s, 1284.17/s)  LR: 2.308e-05  Data: 0.010 (0.018)
Train: 556 [ 300/1251 ( 24%)]  Loss: 3.005 (2.91)  Time: 0.809s, 1266.21/s  (0.795s, 1287.80/s)  LR: 2.308e-05  Data: 0.011 (0.016)
Train: 556 [ 350/1251 ( 28%)]  Loss: 2.791 (2.90)  Time: 0.772s, 1325.82/s  (0.793s, 1291.34/s)  LR: 2.308e-05  Data: 0.010 (0.015)
Train: 556 [ 400/1251 ( 32%)]  Loss: 2.544 (2.86)  Time: 0.771s, 1328.63/s  (0.792s, 1292.87/s)  LR: 2.308e-05  Data: 0.009 (0.015)
Train: 556 [ 450/1251 ( 36%)]  Loss: 3.078 (2.88)  Time: 0.801s, 1278.88/s  (0.791s, 1294.79/s)  LR: 2.308e-05  Data: 0.010 (0.014)
Train: 556 [ 500/1251 ( 40%)]  Loss: 3.138 (2.90)  Time: 0.773s, 1324.01/s  (0.789s, 1297.38/s)  LR: 2.308e-05  Data: 0.010 (0.014)
Train: 556 [ 550/1251 ( 44%)]  Loss: 2.695 (2.89)  Time: 0.771s, 1328.57/s  (0.788s, 1298.99/s)  LR: 2.308e-05  Data: 0.010 (0.013)
Train: 556 [ 600/1251 ( 48%)]  Loss: 2.954 (2.89)  Time: 0.772s, 1326.33/s  (0.788s, 1300.22/s)  LR: 2.308e-05  Data: 0.010 (0.013)
Train: 556 [ 650/1251 ( 52%)]  Loss: 2.896 (2.89)  Time: 0.784s, 1305.32/s  (0.787s, 1301.19/s)  LR: 2.308e-05  Data: 0.010 (0.013)
Train: 556 [ 700/1251 ( 56%)]  Loss: 3.199 (2.91)  Time: 0.783s, 1308.34/s  (0.788s, 1300.16/s)  LR: 2.308e-05  Data: 0.009 (0.013)
Train: 556 [ 750/1251 ( 60%)]  Loss: 2.822 (2.91)  Time: 0.807s, 1268.58/s  (0.788s, 1299.62/s)  LR: 2.308e-05  Data: 0.009 (0.012)
Train: 556 [ 800/1251 ( 64%)]  Loss: 3.036 (2.91)  Time: 0.775s, 1321.85/s  (0.788s, 1298.99/s)  LR: 2.308e-05  Data: 0.010 (0.012)
Train: 556 [ 850/1251 ( 68%)]  Loss: 2.954 (2.92)  Time: 0.778s, 1315.76/s  (0.788s, 1300.13/s)  LR: 2.308e-05  Data: 0.009 (0.012)
Train: 556 [ 900/1251 ( 72%)]  Loss: 3.219 (2.93)  Time: 0.772s, 1326.52/s  (0.787s, 1300.62/s)  LR: 2.308e-05  Data: 0.010 (0.012)
Train: 556 [ 950/1251 ( 76%)]  Loss: 3.083 (2.94)  Time: 0.772s, 1325.61/s  (0.787s, 1301.47/s)  LR: 2.308e-05  Data: 0.010 (0.012)
Train: 556 [1000/1251 ( 80%)]  Loss: 2.886 (2.94)  Time: 0.776s, 1320.40/s  (0.786s, 1302.25/s)  LR: 2.308e-05  Data: 0.010 (0.012)
Train: 556 [1050/1251 ( 84%)]  Loss: 2.896 (2.94)  Time: 0.809s, 1265.48/s  (0.786s, 1302.70/s)  LR: 2.308e-05  Data: 0.010 (0.012)
Train: 556 [1100/1251 ( 88%)]  Loss: 2.747 (2.93)  Time: 0.788s, 1300.26/s  (0.786s, 1302.56/s)  LR: 2.308e-05  Data: 0.009 (0.012)
Train: 556 [1150/1251 ( 92%)]  Loss: 2.695 (2.92)  Time: 0.816s, 1254.31/s  (0.787s, 1301.85/s)  LR: 2.308e-05  Data: 0.011 (0.012)
Train: 556 [1200/1251 ( 96%)]  Loss: 2.762 (2.91)  Time: 0.774s, 1323.01/s  (0.787s, 1301.13/s)  LR: 2.308e-05  Data: 0.011 (0.011)
Train: 556 [1250/1251 (100%)]  Loss: 3.055 (2.92)  Time: 0.779s, 1314.47/s  (0.787s, 1300.66/s)  LR: 2.308e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.613 (1.613)  Loss:  0.6641 (0.6641)  Acc@1: 91.6992 (91.6992)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.193 (0.570)  Loss:  0.7598 (1.1090)  Acc@1: 87.2642 (80.6720)  Acc@5: 98.1132 (95.4760)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-554.pth.tar', 80.8340000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-552.pth.tar', 80.83000000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-555.pth.tar', 80.80200013183594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-546.pth.tar', 80.726000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-551.pth.tar', 80.72600002441406)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-550.pth.tar', 80.71599994873047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-553.pth.tar', 80.69199997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-548.pth.tar', 80.67800005371093)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-535.pth.tar', 80.67399987060547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-556.pth.tar', 80.67200002685547)

Train: 557 [   0/1251 (  0%)]  Loss: 3.064 (3.06)  Time: 2.357s,  434.44/s  (2.357s,  434.44/s)  LR: 2.249e-05  Data: 1.631 (1.631)
Train: 557 [  50/1251 (  4%)]  Loss: 2.706 (2.89)  Time: 0.773s, 1324.94/s  (0.831s, 1232.33/s)  LR: 2.249e-05  Data: 0.011 (0.047)
Train: 557 [ 100/1251 (  8%)]  Loss: 2.890 (2.89)  Time: 0.771s, 1327.32/s  (0.807s, 1268.12/s)  LR: 2.249e-05  Data: 0.010 (0.029)
Train: 557 [ 150/1251 ( 12%)]  Loss: 2.600 (2.82)  Time: 0.772s, 1326.68/s  (0.798s, 1282.79/s)  LR: 2.249e-05  Data: 0.010 (0.022)
Train: 557 [ 200/1251 ( 16%)]  Loss: 3.163 (2.88)  Time: 0.819s, 1249.57/s  (0.795s, 1287.64/s)  LR: 2.249e-05  Data: 0.010 (0.019)
Train: 557 [ 250/1251 ( 20%)]  Loss: 2.542 (2.83)  Time: 0.772s, 1326.61/s  (0.794s, 1289.99/s)  LR: 2.249e-05  Data: 0.010 (0.017)
Train: 557 [ 300/1251 ( 24%)]  Loss: 2.993 (2.85)  Time: 0.771s, 1327.99/s  (0.792s, 1293.69/s)  LR: 2.249e-05  Data: 0.010 (0.016)
Train: 557 [ 350/1251 ( 28%)]  Loss: 2.928 (2.86)  Time: 0.785s, 1304.65/s  (0.790s, 1296.31/s)  LR: 2.249e-05  Data: 0.009 (0.015)
Train: 557 [ 400/1251 ( 32%)]  Loss: 2.803 (2.85)  Time: 0.782s, 1310.29/s  (0.790s, 1296.98/s)  LR: 2.249e-05  Data: 0.012 (0.015)
Train: 557 [ 450/1251 ( 36%)]  Loss: 2.893 (2.86)  Time: 0.773s, 1324.28/s  (0.789s, 1298.09/s)  LR: 2.249e-05  Data: 0.010 (0.014)
Train: 557 [ 500/1251 ( 40%)]  Loss: 3.096 (2.88)  Time: 0.778s, 1315.41/s  (0.787s, 1300.34/s)  LR: 2.249e-05  Data: 0.010 (0.014)
Train: 557 [ 550/1251 ( 44%)]  Loss: 2.918 (2.88)  Time: 0.773s, 1324.72/s  (0.787s, 1300.44/s)  LR: 2.249e-05  Data: 0.009 (0.013)
Train: 557 [ 600/1251 ( 48%)]  Loss: 2.700 (2.87)  Time: 0.772s, 1327.10/s  (0.787s, 1301.13/s)  LR: 2.249e-05  Data: 0.009 (0.013)
Train: 557 [ 650/1251 ( 52%)]  Loss: 3.065 (2.88)  Time: 0.771s, 1328.24/s  (0.787s, 1301.80/s)  LR: 2.249e-05  Data: 0.009 (0.013)
Train: 557 [ 700/1251 ( 56%)]  Loss: 2.923 (2.89)  Time: 0.770s, 1330.02/s  (0.786s, 1302.84/s)  LR: 2.249e-05  Data: 0.010 (0.013)
Train: 557 [ 750/1251 ( 60%)]  Loss: 3.110 (2.90)  Time: 0.775s, 1320.92/s  (0.786s, 1303.08/s)  LR: 2.249e-05  Data: 0.009 (0.012)
Train: 557 [ 800/1251 ( 64%)]  Loss: 2.798 (2.89)  Time: 0.771s, 1328.56/s  (0.786s, 1303.40/s)  LR: 2.249e-05  Data: 0.010 (0.012)
Train: 557 [ 850/1251 ( 68%)]  Loss: 2.876 (2.89)  Time: 0.775s, 1321.99/s  (0.785s, 1303.89/s)  LR: 2.249e-05  Data: 0.010 (0.012)
Train: 557 [ 900/1251 ( 72%)]  Loss: 3.082 (2.90)  Time: 0.776s, 1319.71/s  (0.786s, 1302.84/s)  LR: 2.249e-05  Data: 0.010 (0.012)
Train: 557 [ 950/1251 ( 76%)]  Loss: 3.153 (2.92)  Time: 0.770s, 1330.10/s  (0.786s, 1302.93/s)  LR: 2.249e-05  Data: 0.010 (0.012)
Train: 557 [1000/1251 ( 80%)]  Loss: 2.968 (2.92)  Time: 0.807s, 1269.02/s  (0.786s, 1302.46/s)  LR: 2.249e-05  Data: 0.010 (0.012)
Train: 557 [1050/1251 ( 84%)]  Loss: 2.860 (2.92)  Time: 0.773s, 1323.94/s  (0.787s, 1301.86/s)  LR: 2.249e-05  Data: 0.010 (0.012)
Train: 557 [1100/1251 ( 88%)]  Loss: 3.005 (2.92)  Time: 0.773s, 1325.15/s  (0.787s, 1301.65/s)  LR: 2.249e-05  Data: 0.009 (0.012)
Train: 557 [1150/1251 ( 92%)]  Loss: 2.983 (2.92)  Time: 0.787s, 1301.80/s  (0.787s, 1300.90/s)  LR: 2.249e-05  Data: 0.009 (0.011)
Train: 557 [1200/1251 ( 96%)]  Loss: 2.983 (2.92)  Time: 0.815s, 1256.79/s  (0.788s, 1299.42/s)  LR: 2.249e-05  Data: 0.009 (0.011)
Train: 557 [1250/1251 (100%)]  Loss: 3.011 (2.93)  Time: 0.778s, 1316.24/s  (0.789s, 1298.30/s)  LR: 2.249e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.559 (1.559)  Loss:  0.6831 (0.6831)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.7305 (98.7305)
Test: [  48/48]  Time: 0.194 (0.570)  Loss:  0.7661 (1.1117)  Acc@1: 87.1462 (80.8140)  Acc@5: 97.8774 (95.4580)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-554.pth.tar', 80.8340000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-552.pth.tar', 80.83000000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-557.pth.tar', 80.81399984619141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-555.pth.tar', 80.80200013183594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-546.pth.tar', 80.726000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-551.pth.tar', 80.72600002441406)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-550.pth.tar', 80.71599994873047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-553.pth.tar', 80.69199997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-548.pth.tar', 80.67800005371093)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-535.pth.tar', 80.67399987060547)

Train: 558 [   0/1251 (  0%)]  Loss: 3.072 (3.07)  Time: 2.412s,  424.60/s  (2.412s,  424.60/s)  LR: 2.192e-05  Data: 1.674 (1.674)
Train: 558 [  50/1251 (  4%)]  Loss: 2.826 (2.95)  Time: 0.815s, 1257.02/s  (0.832s, 1231.08/s)  LR: 2.192e-05  Data: 0.009 (0.043)
Train: 558 [ 100/1251 (  8%)]  Loss: 2.740 (2.88)  Time: 0.772s, 1326.90/s  (0.812s, 1260.96/s)  LR: 2.192e-05  Data: 0.009 (0.026)
Train: 558 [ 150/1251 ( 12%)]  Loss: 3.140 (2.94)  Time: 0.779s, 1314.70/s  (0.805s, 1272.11/s)  LR: 2.192e-05  Data: 0.010 (0.021)
Train: 558 [ 200/1251 ( 16%)]  Loss: 2.720 (2.90)  Time: 0.771s, 1328.73/s  (0.803s, 1275.51/s)  LR: 2.192e-05  Data: 0.010 (0.018)
Train: 558 [ 250/1251 ( 20%)]  Loss: 3.035 (2.92)  Time: 0.772s, 1326.15/s  (0.797s, 1284.02/s)  LR: 2.192e-05  Data: 0.009 (0.016)
Train: 558 [ 300/1251 ( 24%)]  Loss: 2.943 (2.93)  Time: 0.780s, 1312.36/s  (0.794s, 1290.12/s)  LR: 2.192e-05  Data: 0.012 (0.015)
Train: 558 [ 350/1251 ( 28%)]  Loss: 2.991 (2.93)  Time: 0.814s, 1258.64/s  (0.795s, 1288.14/s)  LR: 2.192e-05  Data: 0.011 (0.015)
Train: 558 [ 400/1251 ( 32%)]  Loss: 2.715 (2.91)  Time: 0.807s, 1268.28/s  (0.795s, 1288.05/s)  LR: 2.192e-05  Data: 0.010 (0.014)
Train: 558 [ 450/1251 ( 36%)]  Loss: 3.000 (2.92)  Time: 0.775s, 1320.69/s  (0.796s, 1286.35/s)  LR: 2.192e-05  Data: 0.009 (0.014)
Train: 558 [ 500/1251 ( 40%)]  Loss: 2.933 (2.92)  Time: 0.772s, 1326.54/s  (0.795s, 1288.20/s)  LR: 2.192e-05  Data: 0.010 (0.013)
Train: 558 [ 550/1251 ( 44%)]  Loss: 2.969 (2.92)  Time: 0.778s, 1316.54/s  (0.795s, 1288.44/s)  LR: 2.192e-05  Data: 0.009 (0.013)
Train: 558 [ 600/1251 ( 48%)]  Loss: 3.101 (2.94)  Time: 0.779s, 1314.71/s  (0.794s, 1290.35/s)  LR: 2.192e-05  Data: 0.010 (0.013)
Train: 558 [ 650/1251 ( 52%)]  Loss: 2.909 (2.94)  Time: 0.787s, 1300.88/s  (0.793s, 1290.94/s)  LR: 2.192e-05  Data: 0.009 (0.012)
Train: 558 [ 700/1251 ( 56%)]  Loss: 3.078 (2.94)  Time: 0.788s, 1299.49/s  (0.792s, 1292.40/s)  LR: 2.192e-05  Data: 0.010 (0.012)
Train: 558 [ 750/1251 ( 60%)]  Loss: 3.163 (2.96)  Time: 0.816s, 1254.56/s  (0.793s, 1291.39/s)  LR: 2.192e-05  Data: 0.011 (0.012)
Train: 558 [ 800/1251 ( 64%)]  Loss: 2.707 (2.94)  Time: 0.837s, 1222.91/s  (0.794s, 1289.39/s)  LR: 2.192e-05  Data: 0.018 (0.012)
Train: 558 [ 850/1251 ( 68%)]  Loss: 2.977 (2.95)  Time: 0.799s, 1282.40/s  (0.795s, 1288.22/s)  LR: 2.192e-05  Data: 0.010 (0.012)
Train: 558 [ 900/1251 ( 72%)]  Loss: 2.724 (2.93)  Time: 0.808s, 1267.85/s  (0.794s, 1289.26/s)  LR: 2.192e-05  Data: 0.010 (0.012)
Train: 558 [ 950/1251 ( 76%)]  Loss: 3.080 (2.94)  Time: 0.788s, 1299.11/s  (0.795s, 1288.82/s)  LR: 2.192e-05  Data: 0.010 (0.012)
Train: 558 [1000/1251 ( 80%)]  Loss: 3.198 (2.95)  Time: 0.823s, 1244.03/s  (0.795s, 1288.61/s)  LR: 2.192e-05  Data: 0.009 (0.012)
Train: 558 [1050/1251 ( 84%)]  Loss: 2.930 (2.95)  Time: 0.783s, 1307.00/s  (0.795s, 1288.39/s)  LR: 2.192e-05  Data: 0.011 (0.012)
Train: 558 [1100/1251 ( 88%)]  Loss: 2.793 (2.95)  Time: 0.775s, 1321.76/s  (0.794s, 1289.26/s)  LR: 2.192e-05  Data: 0.010 (0.012)
Train: 558 [1150/1251 ( 92%)]  Loss: 2.883 (2.94)  Time: 0.799s, 1281.64/s  (0.794s, 1289.99/s)  LR: 2.192e-05  Data: 0.009 (0.012)
Train: 558 [1200/1251 ( 96%)]  Loss: 2.628 (2.93)  Time: 0.775s, 1321.50/s  (0.793s, 1290.90/s)  LR: 2.192e-05  Data: 0.010 (0.011)
Train: 558 [1250/1251 (100%)]  Loss: 2.907 (2.93)  Time: 0.760s, 1346.98/s  (0.793s, 1290.88/s)  LR: 2.192e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.581 (1.581)  Loss:  0.6704 (0.6704)  Acc@1: 92.3828 (92.3828)  Acc@5: 98.7305 (98.7305)
Test: [  48/48]  Time: 0.194 (0.560)  Loss:  0.7749 (1.1126)  Acc@1: 86.7924 (80.7220)  Acc@5: 97.9953 (95.4200)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-554.pth.tar', 80.8340000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-552.pth.tar', 80.83000000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-557.pth.tar', 80.81399984619141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-555.pth.tar', 80.80200013183594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-546.pth.tar', 80.726000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-551.pth.tar', 80.72600002441406)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-558.pth.tar', 80.72199995117188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-550.pth.tar', 80.71599994873047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-553.pth.tar', 80.69199997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-548.pth.tar', 80.67800005371093)

Train: 559 [   0/1251 (  0%)]  Loss: 3.170 (3.17)  Time: 2.136s,  479.51/s  (2.136s,  479.51/s)  LR: 2.136e-05  Data: 1.422 (1.422)
Train: 559 [  50/1251 (  4%)]  Loss: 3.174 (3.17)  Time: 0.771s, 1327.32/s  (0.812s, 1260.31/s)  LR: 2.136e-05  Data: 0.010 (0.043)
Train: 559 [ 100/1251 (  8%)]  Loss: 2.963 (3.10)  Time: 0.772s, 1326.60/s  (0.795s, 1287.38/s)  LR: 2.136e-05  Data: 0.010 (0.027)
Train: 559 [ 150/1251 ( 12%)]  Loss: 2.914 (3.06)  Time: 0.770s, 1330.61/s  (0.790s, 1296.31/s)  LR: 2.136e-05  Data: 0.010 (0.021)
Train: 559 [ 200/1251 ( 16%)]  Loss: 3.052 (3.05)  Time: 0.771s, 1327.67/s  (0.788s, 1299.02/s)  LR: 2.136e-05  Data: 0.010 (0.019)
Train: 559 [ 250/1251 ( 20%)]  Loss: 3.072 (3.06)  Time: 0.770s, 1330.07/s  (0.786s, 1302.13/s)  LR: 2.136e-05  Data: 0.010 (0.017)
Train: 559 [ 300/1251 ( 24%)]  Loss: 2.795 (3.02)  Time: 0.780s, 1313.09/s  (0.786s, 1302.92/s)  LR: 2.136e-05  Data: 0.010 (0.016)
Train: 559 [ 350/1251 ( 28%)]  Loss: 2.907 (3.01)  Time: 0.770s, 1329.22/s  (0.789s, 1298.58/s)  LR: 2.136e-05  Data: 0.010 (0.015)
Train: 559 [ 400/1251 ( 32%)]  Loss: 2.788 (2.98)  Time: 0.779s, 1314.43/s  (0.787s, 1300.97/s)  LR: 2.136e-05  Data: 0.010 (0.014)
Train: 559 [ 450/1251 ( 36%)]  Loss: 2.956 (2.98)  Time: 0.773s, 1325.30/s  (0.786s, 1303.18/s)  LR: 2.136e-05  Data: 0.010 (0.014)
Train: 559 [ 500/1251 ( 40%)]  Loss: 2.779 (2.96)  Time: 0.812s, 1261.41/s  (0.785s, 1303.96/s)  LR: 2.136e-05  Data: 0.010 (0.013)
Train: 559 [ 550/1251 ( 44%)]  Loss: 2.748 (2.94)  Time: 0.807s, 1268.14/s  (0.787s, 1301.47/s)  LR: 2.136e-05  Data: 0.010 (0.013)
Train: 559 [ 600/1251 ( 48%)]  Loss: 2.971 (2.95)  Time: 0.784s, 1305.75/s  (0.789s, 1298.64/s)  LR: 2.136e-05  Data: 0.010 (0.013)
Train: 559 [ 650/1251 ( 52%)]  Loss: 3.352 (2.97)  Time: 0.844s, 1213.92/s  (0.789s, 1297.40/s)  LR: 2.136e-05  Data: 0.010 (0.013)
Train: 559 [ 700/1251 ( 56%)]  Loss: 2.955 (2.97)  Time: 0.776s, 1320.27/s  (0.788s, 1298.91/s)  LR: 2.136e-05  Data: 0.010 (0.012)
Train: 559 [ 750/1251 ( 60%)]  Loss: 3.220 (2.99)  Time: 0.812s, 1261.81/s  (0.788s, 1299.89/s)  LR: 2.136e-05  Data: 0.010 (0.012)
Train: 559 [ 800/1251 ( 64%)]  Loss: 3.291 (3.01)  Time: 0.774s, 1323.67/s  (0.787s, 1300.83/s)  LR: 2.136e-05  Data: 0.010 (0.012)
Train: 559 [ 850/1251 ( 68%)]  Loss: 2.581 (2.98)  Time: 0.804s, 1273.65/s  (0.787s, 1301.18/s)  LR: 2.136e-05  Data: 0.010 (0.012)
Train: 559 [ 900/1251 ( 72%)]  Loss: 3.025 (2.98)  Time: 0.815s, 1257.17/s  (0.788s, 1298.89/s)  LR: 2.136e-05  Data: 0.011 (0.012)
Train: 559 [ 950/1251 ( 76%)]  Loss: 2.955 (2.98)  Time: 0.774s, 1323.05/s  (0.788s, 1298.67/s)  LR: 2.136e-05  Data: 0.011 (0.012)
Train: 559 [1000/1251 ( 80%)]  Loss: 3.027 (2.99)  Time: 0.782s, 1309.67/s  (0.789s, 1298.20/s)  LR: 2.136e-05  Data: 0.009 (0.012)
Train: 559 [1050/1251 ( 84%)]  Loss: 2.836 (2.98)  Time: 0.772s, 1326.46/s  (0.788s, 1298.97/s)  LR: 2.136e-05  Data: 0.009 (0.012)
Train: 559 [1100/1251 ( 88%)]  Loss: 2.974 (2.98)  Time: 0.774s, 1322.16/s  (0.788s, 1299.89/s)  LR: 2.136e-05  Data: 0.010 (0.012)
Train: 559 [1150/1251 ( 92%)]  Loss: 2.960 (2.98)  Time: 0.814s, 1257.93/s  (0.788s, 1299.96/s)  LR: 2.136e-05  Data: 0.010 (0.012)
Train: 559 [1200/1251 ( 96%)]  Loss: 3.242 (2.99)  Time: 0.816s, 1255.38/s  (0.789s, 1298.04/s)  LR: 2.136e-05  Data: 0.011 (0.012)
Train: 559 [1250/1251 (100%)]  Loss: 2.588 (2.97)  Time: 0.761s, 1345.61/s  (0.789s, 1297.94/s)  LR: 2.136e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.550 (1.550)  Loss:  0.5620 (0.5620)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.194 (0.564)  Loss:  0.6846 (1.0178)  Acc@1: 87.6179 (80.8500)  Acc@5: 98.3491 (95.4720)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-559.pth.tar', 80.849999921875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-554.pth.tar', 80.8340000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-552.pth.tar', 80.83000000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-557.pth.tar', 80.81399984619141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-555.pth.tar', 80.80200013183594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-546.pth.tar', 80.726000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-551.pth.tar', 80.72600002441406)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-558.pth.tar', 80.72199995117188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-550.pth.tar', 80.71599994873047)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-553.pth.tar', 80.69199997558594)

Train: 560 [   0/1251 (  0%)]  Loss: 2.933 (2.93)  Time: 2.343s,  437.10/s  (2.343s,  437.10/s)  LR: 2.082e-05  Data: 1.533 (1.533)
Train: 560 [  50/1251 (  4%)]  Loss: 2.822 (2.88)  Time: 0.815s, 1255.92/s  (0.827s, 1238.26/s)  LR: 2.082e-05  Data: 0.011 (0.041)
Train: 560 [ 100/1251 (  8%)]  Loss: 2.625 (2.79)  Time: 0.771s, 1327.42/s  (0.816s, 1255.10/s)  LR: 2.082e-05  Data: 0.010 (0.026)
Train: 560 [ 150/1251 ( 12%)]  Loss: 2.774 (2.79)  Time: 0.773s, 1324.99/s  (0.806s, 1270.18/s)  LR: 2.082e-05  Data: 0.010 (0.021)
Train: 560 [ 200/1251 ( 16%)]  Loss: 2.992 (2.83)  Time: 0.776s, 1320.11/s  (0.799s, 1281.11/s)  LR: 2.082e-05  Data: 0.009 (0.018)
Train: 560 [ 250/1251 ( 20%)]  Loss: 2.798 (2.82)  Time: 0.781s, 1311.62/s  (0.796s, 1286.35/s)  LR: 2.082e-05  Data: 0.012 (0.016)
Train: 560 [ 300/1251 ( 24%)]  Loss: 2.927 (2.84)  Time: 0.772s, 1326.86/s  (0.793s, 1291.11/s)  LR: 2.082e-05  Data: 0.010 (0.015)
Train: 560 [ 350/1251 ( 28%)]  Loss: 2.934 (2.85)  Time: 0.820s, 1248.73/s  (0.792s, 1293.57/s)  LR: 2.082e-05  Data: 0.011 (0.015)
Train: 560 [ 400/1251 ( 32%)]  Loss: 2.956 (2.86)  Time: 0.816s, 1255.66/s  (0.793s, 1291.16/s)  LR: 2.082e-05  Data: 0.011 (0.014)
Train: 560 [ 450/1251 ( 36%)]  Loss: 2.837 (2.86)  Time: 0.776s, 1318.91/s  (0.794s, 1289.60/s)  LR: 2.082e-05  Data: 0.010 (0.014)
Train: 560 [ 500/1251 ( 40%)]  Loss: 2.956 (2.87)  Time: 0.781s, 1310.49/s  (0.792s, 1292.33/s)  LR: 2.082e-05  Data: 0.009 (0.013)
Train: 560 [ 550/1251 ( 44%)]  Loss: 2.931 (2.87)  Time: 0.771s, 1328.73/s  (0.791s, 1294.49/s)  LR: 2.082e-05  Data: 0.010 (0.013)
Train: 560 [ 600/1251 ( 48%)]  Loss: 3.027 (2.89)  Time: 0.771s, 1327.32/s  (0.790s, 1296.59/s)  LR: 2.082e-05  Data: 0.009 (0.013)
Train: 560 [ 650/1251 ( 52%)]  Loss: 2.620 (2.87)  Time: 0.781s, 1311.12/s  (0.790s, 1295.78/s)  LR: 2.082e-05  Data: 0.010 (0.013)
Train: 560 [ 700/1251 ( 56%)]  Loss: 3.047 (2.88)  Time: 0.808s, 1267.93/s  (0.790s, 1296.37/s)  LR: 2.082e-05  Data: 0.010 (0.012)
Train: 560 [ 750/1251 ( 60%)]  Loss: 2.716 (2.87)  Time: 0.772s, 1326.59/s  (0.791s, 1294.50/s)  LR: 2.082e-05  Data: 0.009 (0.012)
Train: 560 [ 800/1251 ( 64%)]  Loss: 2.968 (2.87)  Time: 0.778s, 1316.56/s  (0.791s, 1295.28/s)  LR: 2.082e-05  Data: 0.010 (0.012)
Train: 560 [ 850/1251 ( 68%)]  Loss: 2.880 (2.87)  Time: 0.849s, 1206.13/s  (0.791s, 1293.89/s)  LR: 2.082e-05  Data: 0.011 (0.012)
Train: 560 [ 900/1251 ( 72%)]  Loss: 3.002 (2.88)  Time: 0.842s, 1215.61/s  (0.792s, 1292.23/s)  LR: 2.082e-05  Data: 0.010 (0.012)
Train: 560 [ 950/1251 ( 76%)]  Loss: 2.710 (2.87)  Time: 0.814s, 1257.50/s  (0.792s, 1293.61/s)  LR: 2.082e-05  Data: 0.010 (0.012)
Train: 560 [1000/1251 ( 80%)]  Loss: 2.802 (2.87)  Time: 0.770s, 1329.03/s  (0.791s, 1294.69/s)  LR: 2.082e-05  Data: 0.009 (0.012)
Train: 560 [1050/1251 ( 84%)]  Loss: 2.804 (2.87)  Time: 0.797s, 1284.50/s  (0.791s, 1295.05/s)  LR: 2.082e-05  Data: 0.009 (0.012)
Train: 560 [1100/1251 ( 88%)]  Loss: 3.027 (2.87)  Time: 0.818s, 1252.37/s  (0.791s, 1294.73/s)  LR: 2.082e-05  Data: 0.012 (0.012)
Train: 560 [1150/1251 ( 92%)]  Loss: 2.777 (2.87)  Time: 0.834s, 1227.25/s  (0.790s, 1295.44/s)  LR: 2.082e-05  Data: 0.010 (0.011)
Train: 560 [1200/1251 ( 96%)]  Loss: 2.945 (2.87)  Time: 0.774s, 1323.80/s  (0.790s, 1296.15/s)  LR: 2.082e-05  Data: 0.009 (0.011)
Train: 560 [1250/1251 (100%)]  Loss: 3.218 (2.89)  Time: 0.761s, 1345.95/s  (0.789s, 1297.09/s)  LR: 2.082e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.575 (1.575)  Loss:  0.6504 (0.6504)  Acc@1: 93.0664 (93.0664)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.194 (0.562)  Loss:  0.7446 (1.0926)  Acc@1: 87.3821 (80.7860)  Acc@5: 98.1132 (95.4560)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-559.pth.tar', 80.849999921875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-554.pth.tar', 80.8340000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-552.pth.tar', 80.83000000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-557.pth.tar', 80.81399984619141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-555.pth.tar', 80.80200013183594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-560.pth.tar', 80.78599994873046)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-546.pth.tar', 80.726000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-551.pth.tar', 80.72600002441406)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-558.pth.tar', 80.72199995117188)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-550.pth.tar', 80.71599994873047)

Train: 561 [   0/1251 (  0%)]  Loss: 3.196 (3.20)  Time: 2.326s,  440.29/s  (2.326s,  440.29/s)  LR: 2.028e-05  Data: 1.590 (1.590)
Train: 561 [  50/1251 (  4%)]  Loss: 2.958 (3.08)  Time: 0.820s, 1249.42/s  (0.824s, 1243.42/s)  LR: 2.028e-05  Data: 0.009 (0.049)
Train: 561 [ 100/1251 (  8%)]  Loss: 3.210 (3.12)  Time: 0.785s, 1303.99/s  (0.811s, 1263.28/s)  LR: 2.028e-05  Data: 0.009 (0.029)
Train: 561 [ 150/1251 ( 12%)]  Loss: 3.031 (3.10)  Time: 0.779s, 1314.23/s  (0.803s, 1275.22/s)  LR: 2.028e-05  Data: 0.010 (0.023)
Train: 561 [ 200/1251 ( 16%)]  Loss: 2.697 (3.02)  Time: 0.794s, 1288.91/s  (0.799s, 1282.19/s)  LR: 2.028e-05  Data: 0.009 (0.020)
Train: 561 [ 250/1251 ( 20%)]  Loss: 2.214 (2.88)  Time: 0.787s, 1301.51/s  (0.795s, 1287.45/s)  LR: 2.028e-05  Data: 0.010 (0.018)
Train: 561 [ 300/1251 ( 24%)]  Loss: 2.860 (2.88)  Time: 0.775s, 1320.85/s  (0.793s, 1291.24/s)  LR: 2.028e-05  Data: 0.010 (0.016)
Train: 561 [ 350/1251 ( 28%)]  Loss: 3.139 (2.91)  Time: 0.850s, 1204.94/s  (0.795s, 1287.60/s)  LR: 2.028e-05  Data: 0.011 (0.016)
Train: 561 [ 400/1251 ( 32%)]  Loss: 2.926 (2.91)  Time: 0.771s, 1327.82/s  (0.793s, 1290.81/s)  LR: 2.028e-05  Data: 0.010 (0.015)
Train: 561 [ 450/1251 ( 36%)]  Loss: 2.969 (2.92)  Time: 0.783s, 1308.47/s  (0.792s, 1292.70/s)  LR: 2.028e-05  Data: 0.009 (0.014)
Train: 561 [ 500/1251 ( 40%)]  Loss: 2.522 (2.88)  Time: 0.808s, 1267.21/s  (0.791s, 1294.85/s)  LR: 2.028e-05  Data: 0.009 (0.014)
Train: 561 [ 550/1251 ( 44%)]  Loss: 2.879 (2.88)  Time: 0.773s, 1325.15/s  (0.790s, 1296.74/s)  LR: 2.028e-05  Data: 0.010 (0.014)
Train: 561 [ 600/1251 ( 48%)]  Loss: 2.507 (2.85)  Time: 0.855s, 1197.47/s  (0.790s, 1295.82/s)  LR: 2.028e-05  Data: 0.011 (0.013)
Train: 561 [ 650/1251 ( 52%)]  Loss: 3.225 (2.88)  Time: 0.828s, 1237.13/s  (0.790s, 1296.73/s)  LR: 2.028e-05  Data: 0.010 (0.013)
Train: 561 [ 700/1251 ( 56%)]  Loss: 3.158 (2.90)  Time: 0.855s, 1197.18/s  (0.790s, 1296.87/s)  LR: 2.028e-05  Data: 0.010 (0.013)
Train: 561 [ 750/1251 ( 60%)]  Loss: 2.763 (2.89)  Time: 0.810s, 1264.27/s  (0.790s, 1296.28/s)  LR: 2.028e-05  Data: 0.009 (0.013)
Train: 561 [ 800/1251 ( 64%)]  Loss: 2.833 (2.89)  Time: 0.773s, 1324.52/s  (0.789s, 1297.05/s)  LR: 2.028e-05  Data: 0.010 (0.012)
Train: 561 [ 850/1251 ( 68%)]  Loss: 3.020 (2.89)  Time: 0.773s, 1325.28/s  (0.789s, 1297.82/s)  LR: 2.028e-05  Data: 0.010 (0.012)
Train: 561 [ 900/1251 ( 72%)]  Loss: 3.065 (2.90)  Time: 0.853s, 1199.96/s  (0.789s, 1297.40/s)  LR: 2.028e-05  Data: 0.009 (0.012)
Train: 561 [ 950/1251 ( 76%)]  Loss: 2.581 (2.89)  Time: 0.810s, 1264.38/s  (0.789s, 1297.20/s)  LR: 2.028e-05  Data: 0.009 (0.012)
Train: 561 [1000/1251 ( 80%)]  Loss: 3.045 (2.90)  Time: 0.816s, 1255.15/s  (0.790s, 1296.85/s)  LR: 2.028e-05  Data: 0.011 (0.012)
Train: 561 [1050/1251 ( 84%)]  Loss: 2.698 (2.89)  Time: 0.776s, 1319.62/s  (0.790s, 1295.72/s)  LR: 2.028e-05  Data: 0.010 (0.012)
Train: 561 [1100/1251 ( 88%)]  Loss: 2.970 (2.89)  Time: 0.809s, 1265.99/s  (0.791s, 1295.22/s)  LR: 2.028e-05  Data: 0.009 (0.012)
Train: 561 [1150/1251 ( 92%)]  Loss: 2.951 (2.89)  Time: 0.807s, 1269.04/s  (0.791s, 1293.90/s)  LR: 2.028e-05  Data: 0.009 (0.012)
Train: 561 [1200/1251 ( 96%)]  Loss: 2.857 (2.89)  Time: 0.772s, 1325.81/s  (0.791s, 1294.28/s)  LR: 2.028e-05  Data: 0.010 (0.012)
Train: 561 [1250/1251 (100%)]  Loss: 2.947 (2.89)  Time: 0.760s, 1346.90/s  (0.791s, 1294.80/s)  LR: 2.028e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.636 (1.636)  Loss:  0.6655 (0.6655)  Acc@1: 92.6758 (92.6758)  Acc@5: 98.7305 (98.7305)
Test: [  48/48]  Time: 0.195 (0.571)  Loss:  0.7700 (1.1049)  Acc@1: 87.1462 (80.8220)  Acc@5: 98.2311 (95.4600)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-559.pth.tar', 80.849999921875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-554.pth.tar', 80.8340000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-552.pth.tar', 80.83000000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-561.pth.tar', 80.82199997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-557.pth.tar', 80.81399984619141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-555.pth.tar', 80.80200013183594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-560.pth.tar', 80.78599994873046)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-546.pth.tar', 80.726000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-551.pth.tar', 80.72600002441406)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-558.pth.tar', 80.72199995117188)

Train: 562 [   0/1251 (  0%)]  Loss: 2.701 (2.70)  Time: 2.361s,  433.74/s  (2.361s,  433.74/s)  LR: 1.977e-05  Data: 1.626 (1.626)
Train: 562 [  50/1251 (  4%)]  Loss: 3.064 (2.88)  Time: 0.778s, 1316.98/s  (0.828s, 1237.25/s)  LR: 1.977e-05  Data: 0.010 (0.047)
Train: 562 [ 100/1251 (  8%)]  Loss: 2.849 (2.87)  Time: 0.782s, 1309.07/s  (0.806s, 1271.02/s)  LR: 1.977e-05  Data: 0.010 (0.028)
Train: 562 [ 150/1251 ( 12%)]  Loss: 2.794 (2.85)  Time: 0.798s, 1282.74/s  (0.801s, 1277.68/s)  LR: 1.977e-05  Data: 0.010 (0.022)
Train: 562 [ 200/1251 ( 16%)]  Loss: 2.727 (2.83)  Time: 0.771s, 1328.32/s  (0.795s, 1287.74/s)  LR: 1.977e-05  Data: 0.010 (0.019)
Train: 562 [ 250/1251 ( 20%)]  Loss: 2.997 (2.86)  Time: 0.772s, 1327.01/s  (0.792s, 1292.53/s)  LR: 1.977e-05  Data: 0.010 (0.017)
Train: 562 [ 300/1251 ( 24%)]  Loss: 3.146 (2.90)  Time: 0.789s, 1297.34/s  (0.790s, 1295.94/s)  LR: 1.977e-05  Data: 0.010 (0.016)
Train: 562 [ 350/1251 ( 28%)]  Loss: 2.943 (2.90)  Time: 0.778s, 1316.67/s  (0.789s, 1297.99/s)  LR: 1.977e-05  Data: 0.010 (0.015)
Train: 562 [ 400/1251 ( 32%)]  Loss: 2.854 (2.90)  Time: 0.819s, 1250.35/s  (0.789s, 1297.94/s)  LR: 1.977e-05  Data: 0.011 (0.015)
Train: 562 [ 450/1251 ( 36%)]  Loss: 2.817 (2.89)  Time: 0.782s, 1309.77/s  (0.791s, 1294.61/s)  LR: 1.977e-05  Data: 0.010 (0.014)
Train: 562 [ 500/1251 ( 40%)]  Loss: 3.125 (2.91)  Time: 0.772s, 1325.97/s  (0.790s, 1295.76/s)  LR: 1.977e-05  Data: 0.010 (0.014)
Train: 562 [ 550/1251 ( 44%)]  Loss: 3.049 (2.92)  Time: 0.799s, 1282.00/s  (0.790s, 1296.51/s)  LR: 1.977e-05  Data: 0.009 (0.013)
Train: 562 [ 600/1251 ( 48%)]  Loss: 2.960 (2.93)  Time: 0.821s, 1246.67/s  (0.791s, 1294.59/s)  LR: 1.977e-05  Data: 0.010 (0.013)
Train: 562 [ 650/1251 ( 52%)]  Loss: 2.879 (2.92)  Time: 0.771s, 1327.83/s  (0.791s, 1294.08/s)  LR: 1.977e-05  Data: 0.010 (0.013)
Train: 562 [ 700/1251 ( 56%)]  Loss: 2.967 (2.92)  Time: 0.815s, 1256.72/s  (0.791s, 1294.10/s)  LR: 1.977e-05  Data: 0.011 (0.013)
Train: 562 [ 750/1251 ( 60%)]  Loss: 3.204 (2.94)  Time: 0.782s, 1309.35/s  (0.791s, 1294.51/s)  LR: 1.977e-05  Data: 0.012 (0.013)
Train: 562 [ 800/1251 ( 64%)]  Loss: 2.889 (2.94)  Time: 0.815s, 1255.79/s  (0.791s, 1293.98/s)  LR: 1.977e-05  Data: 0.010 (0.012)
Train: 562 [ 850/1251 ( 68%)]  Loss: 2.569 (2.92)  Time: 0.771s, 1328.95/s  (0.791s, 1294.07/s)  LR: 1.977e-05  Data: 0.009 (0.012)
Train: 562 [ 900/1251 ( 72%)]  Loss: 2.677 (2.91)  Time: 0.771s, 1329.00/s  (0.791s, 1294.50/s)  LR: 1.977e-05  Data: 0.010 (0.012)
Train: 562 [ 950/1251 ( 76%)]  Loss: 3.151 (2.92)  Time: 0.779s, 1314.57/s  (0.790s, 1295.76/s)  LR: 1.977e-05  Data: 0.009 (0.012)
Train: 562 [1000/1251 ( 80%)]  Loss: 2.963 (2.92)  Time: 0.776s, 1318.96/s  (0.790s, 1296.20/s)  LR: 1.977e-05  Data: 0.010 (0.012)
Train: 562 [1050/1251 ( 84%)]  Loss: 2.851 (2.92)  Time: 0.772s, 1326.25/s  (0.790s, 1296.56/s)  LR: 1.977e-05  Data: 0.010 (0.012)
Train: 562 [1100/1251 ( 88%)]  Loss: 3.078 (2.92)  Time: 0.785s, 1304.86/s  (0.789s, 1297.11/s)  LR: 1.977e-05  Data: 0.010 (0.012)
Train: 562 [1150/1251 ( 92%)]  Loss: 2.887 (2.92)  Time: 0.784s, 1305.37/s  (0.789s, 1297.16/s)  LR: 1.977e-05  Data: 0.010 (0.012)
Train: 562 [1200/1251 ( 96%)]  Loss: 2.786 (2.92)  Time: 0.780s, 1312.54/s  (0.789s, 1297.03/s)  LR: 1.977e-05  Data: 0.009 (0.012)
Train: 562 [1250/1251 (100%)]  Loss: 2.897 (2.92)  Time: 0.770s, 1329.56/s  (0.789s, 1297.98/s)  LR: 1.977e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.597 (1.597)  Loss:  0.6675 (0.6675)  Acc@1: 92.6758 (92.6758)  Acc@5: 98.7305 (98.7305)
Test: [  48/48]  Time: 0.194 (0.568)  Loss:  0.7705 (1.1142)  Acc@1: 87.0283 (80.6840)  Acc@5: 97.9953 (95.4380)
Train: 563 [   0/1251 (  0%)]  Loss: 2.974 (2.97)  Time: 2.117s,  483.68/s  (2.117s,  483.68/s)  LR: 1.926e-05  Data: 1.405 (1.405)
Train: 563 [  50/1251 (  4%)]  Loss: 2.900 (2.94)  Time: 0.772s, 1326.84/s  (0.819s, 1250.21/s)  LR: 1.926e-05  Data: 0.010 (0.044)
Train: 563 [ 100/1251 (  8%)]  Loss: 2.864 (2.91)  Time: 0.780s, 1313.08/s  (0.800s, 1279.84/s)  LR: 1.926e-05  Data: 0.009 (0.027)
Train: 563 [ 150/1251 ( 12%)]  Loss: 2.645 (2.85)  Time: 0.779s, 1314.25/s  (0.794s, 1290.04/s)  LR: 1.926e-05  Data: 0.010 (0.021)
Train: 563 [ 200/1251 ( 16%)]  Loss: 2.795 (2.84)  Time: 0.773s, 1323.92/s  (0.796s, 1286.70/s)  LR: 1.926e-05  Data: 0.009 (0.019)
Train: 563 [ 250/1251 ( 20%)]  Loss: 2.944 (2.85)  Time: 0.807s, 1269.00/s  (0.794s, 1289.83/s)  LR: 1.926e-05  Data: 0.009 (0.017)
Train: 563 [ 300/1251 ( 24%)]  Loss: 3.062 (2.88)  Time: 0.773s, 1324.54/s  (0.794s, 1290.10/s)  LR: 1.926e-05  Data: 0.011 (0.016)
Train: 563 [ 350/1251 ( 28%)]  Loss: 2.742 (2.87)  Time: 0.785s, 1305.16/s  (0.792s, 1292.80/s)  LR: 1.926e-05  Data: 0.013 (0.015)
Train: 563 [ 400/1251 ( 32%)]  Loss: 2.801 (2.86)  Time: 0.778s, 1316.02/s  (0.790s, 1295.85/s)  LR: 1.926e-05  Data: 0.010 (0.014)
Train: 563 [ 450/1251 ( 36%)]  Loss: 2.749 (2.85)  Time: 0.784s, 1306.41/s  (0.789s, 1298.16/s)  LR: 1.926e-05  Data: 0.010 (0.014)
Train: 563 [ 500/1251 ( 40%)]  Loss: 3.046 (2.87)  Time: 0.834s, 1228.17/s  (0.788s, 1299.52/s)  LR: 1.926e-05  Data: 0.010 (0.013)
Train: 563 [ 550/1251 ( 44%)]  Loss: 2.504 (2.84)  Time: 0.783s, 1307.56/s  (0.789s, 1297.63/s)  LR: 1.926e-05  Data: 0.009 (0.013)
Train: 563 [ 600/1251 ( 48%)]  Loss: 2.937 (2.84)  Time: 0.775s, 1321.56/s  (0.789s, 1298.65/s)  LR: 1.926e-05  Data: 0.010 (0.013)
Train: 563 [ 650/1251 ( 52%)]  Loss: 2.710 (2.83)  Time: 0.773s, 1324.88/s  (0.787s, 1300.32/s)  LR: 1.926e-05  Data: 0.010 (0.012)
Train: 563 [ 700/1251 ( 56%)]  Loss: 2.665 (2.82)  Time: 0.781s, 1311.81/s  (0.787s, 1301.80/s)  LR: 1.926e-05  Data: 0.011 (0.012)
Train: 563 [ 750/1251 ( 60%)]  Loss: 3.002 (2.83)  Time: 0.782s, 1309.95/s  (0.786s, 1302.59/s)  LR: 1.926e-05  Data: 0.013 (0.012)
Train: 563 [ 800/1251 ( 64%)]  Loss: 3.005 (2.84)  Time: 0.780s, 1312.43/s  (0.786s, 1302.99/s)  LR: 1.926e-05  Data: 0.009 (0.012)
Train: 563 [ 850/1251 ( 68%)]  Loss: 3.149 (2.86)  Time: 0.781s, 1310.69/s  (0.787s, 1301.54/s)  LR: 1.926e-05  Data: 0.010 (0.012)
Train: 563 [ 900/1251 ( 72%)]  Loss: 2.661 (2.85)  Time: 0.771s, 1328.51/s  (0.787s, 1301.94/s)  LR: 1.926e-05  Data: 0.010 (0.012)
Train: 563 [ 950/1251 ( 76%)]  Loss: 3.179 (2.87)  Time: 0.773s, 1325.18/s  (0.786s, 1302.38/s)  LR: 1.926e-05  Data: 0.010 (0.012)
Train: 563 [1000/1251 ( 80%)]  Loss: 2.839 (2.87)  Time: 0.770s, 1329.98/s  (0.786s, 1302.47/s)  LR: 1.926e-05  Data: 0.010 (0.012)
Train: 563 [1050/1251 ( 84%)]  Loss: 2.895 (2.87)  Time: 0.791s, 1295.29/s  (0.786s, 1303.08/s)  LR: 1.926e-05  Data: 0.009 (0.011)
Train: 563 [1100/1251 ( 88%)]  Loss: 2.951 (2.87)  Time: 0.772s, 1326.83/s  (0.785s, 1303.74/s)  LR: 1.926e-05  Data: 0.010 (0.011)
Train: 563 [1150/1251 ( 92%)]  Loss: 3.081 (2.88)  Time: 0.770s, 1329.52/s  (0.786s, 1303.03/s)  LR: 1.926e-05  Data: 0.010 (0.011)
Train: 563 [1200/1251 ( 96%)]  Loss: 2.882 (2.88)  Time: 0.797s, 1284.43/s  (0.786s, 1302.58/s)  LR: 1.926e-05  Data: 0.009 (0.011)
Train: 563 [1250/1251 (100%)]  Loss: 3.311 (2.90)  Time: 0.760s, 1346.82/s  (0.786s, 1303.11/s)  LR: 1.926e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.581 (1.581)  Loss:  0.6602 (0.6602)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.194 (0.567)  Loss:  0.7603 (1.0979)  Acc@1: 87.5000 (80.8460)  Acc@5: 98.2311 (95.5180)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-559.pth.tar', 80.849999921875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-563.pth.tar', 80.846)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-554.pth.tar', 80.8340000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-552.pth.tar', 80.83000000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-561.pth.tar', 80.82199997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-557.pth.tar', 80.81399984619141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-555.pth.tar', 80.80200013183594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-560.pth.tar', 80.78599994873046)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-546.pth.tar', 80.726000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-551.pth.tar', 80.72600002441406)

Train: 564 [   0/1251 (  0%)]  Loss: 2.559 (2.56)  Time: 2.230s,  459.12/s  (2.230s,  459.12/s)  LR: 1.877e-05  Data: 1.502 (1.502)
Train: 564 [  50/1251 (  4%)]  Loss: 3.150 (2.85)  Time: 0.778s, 1315.68/s  (0.837s, 1222.92/s)  LR: 1.877e-05  Data: 0.010 (0.046)
Train: 564 [ 100/1251 (  8%)]  Loss: 2.546 (2.75)  Time: 0.814s, 1258.16/s  (0.809s, 1265.21/s)  LR: 1.877e-05  Data: 0.011 (0.028)
Train: 564 [ 150/1251 ( 12%)]  Loss: 2.781 (2.76)  Time: 0.772s, 1326.51/s  (0.801s, 1278.25/s)  LR: 1.877e-05  Data: 0.010 (0.022)
Train: 564 [ 200/1251 ( 16%)]  Loss: 2.740 (2.76)  Time: 0.772s, 1327.21/s  (0.798s, 1283.11/s)  LR: 1.877e-05  Data: 0.010 (0.019)
Train: 564 [ 250/1251 ( 20%)]  Loss: 3.097 (2.81)  Time: 0.779s, 1313.95/s  (0.795s, 1288.08/s)  LR: 1.877e-05  Data: 0.016 (0.017)
Train: 564 [ 300/1251 ( 24%)]  Loss: 2.894 (2.82)  Time: 0.775s, 1320.86/s  (0.794s, 1290.48/s)  LR: 1.877e-05  Data: 0.010 (0.016)
Train: 564 [ 350/1251 ( 28%)]  Loss: 2.578 (2.79)  Time: 0.775s, 1320.95/s  (0.792s, 1292.59/s)  LR: 1.877e-05  Data: 0.010 (0.015)
Train: 564 [ 400/1251 ( 32%)]  Loss: 2.818 (2.80)  Time: 0.870s, 1176.72/s  (0.793s, 1291.70/s)  LR: 1.877e-05  Data: 0.010 (0.015)
Train: 564 [ 450/1251 ( 36%)]  Loss: 2.931 (2.81)  Time: 0.812s, 1261.71/s  (0.794s, 1290.40/s)  LR: 1.877e-05  Data: 0.010 (0.014)
Train: 564 [ 500/1251 ( 40%)]  Loss: 3.037 (2.83)  Time: 0.783s, 1308.00/s  (0.793s, 1291.92/s)  LR: 1.877e-05  Data: 0.010 (0.014)
Train: 564 [ 550/1251 ( 44%)]  Loss: 3.121 (2.85)  Time: 0.776s, 1319.23/s  (0.792s, 1293.43/s)  LR: 1.877e-05  Data: 0.013 (0.014)
Train: 564 [ 600/1251 ( 48%)]  Loss: 2.720 (2.84)  Time: 0.772s, 1325.59/s  (0.792s, 1292.33/s)  LR: 1.877e-05  Data: 0.010 (0.013)
Train: 564 [ 650/1251 ( 52%)]  Loss: 3.062 (2.86)  Time: 0.774s, 1322.68/s  (0.792s, 1293.54/s)  LR: 1.877e-05  Data: 0.010 (0.013)
Train: 564 [ 700/1251 ( 56%)]  Loss: 2.773 (2.85)  Time: 0.771s, 1327.68/s  (0.791s, 1294.44/s)  LR: 1.877e-05  Data: 0.010 (0.013)
Train: 564 [ 750/1251 ( 60%)]  Loss: 2.927 (2.86)  Time: 0.784s, 1305.55/s  (0.791s, 1294.83/s)  LR: 1.877e-05  Data: 0.009 (0.013)
Train: 564 [ 800/1251 ( 64%)]  Loss: 2.723 (2.85)  Time: 0.772s, 1326.35/s  (0.790s, 1296.10/s)  LR: 1.877e-05  Data: 0.010 (0.012)
Train: 564 [ 850/1251 ( 68%)]  Loss: 3.111 (2.86)  Time: 0.782s, 1309.84/s  (0.789s, 1297.19/s)  LR: 1.877e-05  Data: 0.010 (0.012)
Train: 564 [ 900/1251 ( 72%)]  Loss: 3.111 (2.88)  Time: 0.783s, 1308.29/s  (0.789s, 1298.15/s)  LR: 1.877e-05  Data: 0.010 (0.012)
Train: 564 [ 950/1251 ( 76%)]  Loss: 2.947 (2.88)  Time: 0.773s, 1325.47/s  (0.789s, 1298.39/s)  LR: 1.877e-05  Data: 0.010 (0.012)
Train: 564 [1000/1251 ( 80%)]  Loss: 2.694 (2.87)  Time: 0.774s, 1323.72/s  (0.788s, 1299.16/s)  LR: 1.877e-05  Data: 0.010 (0.012)
Train: 564 [1050/1251 ( 84%)]  Loss: 3.035 (2.88)  Time: 0.777s, 1318.57/s  (0.788s, 1299.62/s)  LR: 1.877e-05  Data: 0.010 (0.012)
Train: 564 [1100/1251 ( 88%)]  Loss: 3.082 (2.89)  Time: 0.773s, 1325.28/s  (0.788s, 1299.93/s)  LR: 1.877e-05  Data: 0.010 (0.012)
Train: 564 [1150/1251 ( 92%)]  Loss: 2.706 (2.88)  Time: 0.773s, 1325.21/s  (0.788s, 1299.70/s)  LR: 1.877e-05  Data: 0.009 (0.012)
Train: 564 [1200/1251 ( 96%)]  Loss: 2.848 (2.88)  Time: 0.778s, 1316.42/s  (0.788s, 1299.27/s)  LR: 1.877e-05  Data: 0.010 (0.012)
Train: 564 [1250/1251 (100%)]  Loss: 3.033 (2.89)  Time: 0.759s, 1348.92/s  (0.788s, 1299.70/s)  LR: 1.877e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.554 (1.554)  Loss:  0.6543 (0.6543)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.8281 (98.8281)
Test: [  48/48]  Time: 0.194 (0.564)  Loss:  0.7422 (1.0796)  Acc@1: 87.2642 (80.8820)  Acc@5: 98.2311 (95.5200)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-564.pth.tar', 80.88200002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-559.pth.tar', 80.849999921875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-563.pth.tar', 80.846)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-554.pth.tar', 80.8340000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-552.pth.tar', 80.83000000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-561.pth.tar', 80.82199997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-557.pth.tar', 80.81399984619141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-555.pth.tar', 80.80200013183594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-560.pth.tar', 80.78599994873046)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-546.pth.tar', 80.726000078125)

Train: 565 [   0/1251 (  0%)]  Loss: 2.765 (2.77)  Time: 2.230s,  459.12/s  (2.230s,  459.12/s)  LR: 1.829e-05  Data: 1.498 (1.498)
Train: 565 [  50/1251 (  4%)]  Loss: 3.063 (2.91)  Time: 0.772s, 1326.14/s  (0.824s, 1242.06/s)  LR: 1.829e-05  Data: 0.009 (0.042)
Train: 565 [ 100/1251 (  8%)]  Loss: 2.740 (2.86)  Time: 0.806s, 1270.01/s  (0.807s, 1268.58/s)  LR: 1.829e-05  Data: 0.010 (0.026)
Train: 565 [ 150/1251 ( 12%)]  Loss: 2.886 (2.86)  Time: 0.783s, 1307.70/s  (0.798s, 1283.94/s)  LR: 1.829e-05  Data: 0.010 (0.021)
Train: 565 [ 200/1251 ( 16%)]  Loss: 2.945 (2.88)  Time: 0.776s, 1320.18/s  (0.797s, 1285.34/s)  LR: 1.829e-05  Data: 0.010 (0.018)
Train: 565 [ 250/1251 ( 20%)]  Loss: 2.860 (2.88)  Time: 0.807s, 1268.55/s  (0.798s, 1282.60/s)  LR: 1.829e-05  Data: 0.009 (0.016)
Train: 565 [ 300/1251 ( 24%)]  Loss: 2.765 (2.86)  Time: 0.772s, 1326.13/s  (0.797s, 1285.03/s)  LR: 1.829e-05  Data: 0.010 (0.015)
Train: 565 [ 350/1251 ( 28%)]  Loss: 3.241 (2.91)  Time: 0.807s, 1269.53/s  (0.795s, 1287.56/s)  LR: 1.829e-05  Data: 0.010 (0.015)
Train: 565 [ 400/1251 ( 32%)]  Loss: 2.607 (2.87)  Time: 0.797s, 1284.64/s  (0.795s, 1287.72/s)  LR: 1.829e-05  Data: 0.010 (0.014)
Train: 565 [ 450/1251 ( 36%)]  Loss: 2.805 (2.87)  Time: 0.809s, 1266.18/s  (0.794s, 1289.63/s)  LR: 1.829e-05  Data: 0.010 (0.013)
Train: 565 [ 500/1251 ( 40%)]  Loss: 3.041 (2.88)  Time: 0.772s, 1326.63/s  (0.793s, 1290.62/s)  LR: 1.829e-05  Data: 0.010 (0.013)
Train: 565 [ 550/1251 ( 44%)]  Loss: 2.786 (2.88)  Time: 0.771s, 1327.91/s  (0.793s, 1291.49/s)  LR: 1.829e-05  Data: 0.009 (0.013)
Train: 565 [ 600/1251 ( 48%)]  Loss: 2.722 (2.86)  Time: 0.771s, 1327.79/s  (0.793s, 1290.94/s)  LR: 1.829e-05  Data: 0.010 (0.013)
Train: 565 [ 650/1251 ( 52%)]  Loss: 2.624 (2.85)  Time: 0.774s, 1322.45/s  (0.792s, 1293.03/s)  LR: 1.829e-05  Data: 0.012 (0.012)
Train: 565 [ 700/1251 ( 56%)]  Loss: 2.635 (2.83)  Time: 0.773s, 1325.30/s  (0.792s, 1293.51/s)  LR: 1.829e-05  Data: 0.009 (0.012)
Train: 565 [ 750/1251 ( 60%)]  Loss: 2.416 (2.81)  Time: 0.779s, 1314.30/s  (0.792s, 1292.67/s)  LR: 1.829e-05  Data: 0.010 (0.012)
Train: 565 [ 800/1251 ( 64%)]  Loss: 2.583 (2.79)  Time: 0.779s, 1314.87/s  (0.792s, 1292.58/s)  LR: 1.829e-05  Data: 0.009 (0.012)
Train: 565 [ 850/1251 ( 68%)]  Loss: 2.995 (2.80)  Time: 0.775s, 1321.56/s  (0.792s, 1293.48/s)  LR: 1.829e-05  Data: 0.009 (0.012)
Train: 565 [ 900/1251 ( 72%)]  Loss: 3.188 (2.82)  Time: 0.869s, 1177.80/s  (0.792s, 1292.70/s)  LR: 1.829e-05  Data: 0.010 (0.012)
Train: 565 [ 950/1251 ( 76%)]  Loss: 2.863 (2.83)  Time: 0.773s, 1325.30/s  (0.793s, 1292.04/s)  LR: 1.829e-05  Data: 0.009 (0.012)
Train: 565 [1000/1251 ( 80%)]  Loss: 2.793 (2.82)  Time: 0.818s, 1252.58/s  (0.792s, 1292.96/s)  LR: 1.829e-05  Data: 0.010 (0.012)
Train: 565 [1050/1251 ( 84%)]  Loss: 2.775 (2.82)  Time: 0.779s, 1313.75/s  (0.792s, 1293.63/s)  LR: 1.829e-05  Data: 0.013 (0.011)
Train: 565 [1100/1251 ( 88%)]  Loss: 2.863 (2.82)  Time: 0.819s, 1250.44/s  (0.791s, 1294.05/s)  LR: 1.829e-05  Data: 0.010 (0.011)
Train: 565 [1150/1251 ( 92%)]  Loss: 3.325 (2.85)  Time: 0.781s, 1311.80/s  (0.791s, 1294.70/s)  LR: 1.829e-05  Data: 0.013 (0.011)
Train: 565 [1200/1251 ( 96%)]  Loss: 3.018 (2.85)  Time: 0.783s, 1307.96/s  (0.790s, 1295.50/s)  LR: 1.829e-05  Data: 0.009 (0.011)
Train: 565 [1250/1251 (100%)]  Loss: 3.037 (2.86)  Time: 0.795s, 1287.94/s  (0.790s, 1295.75/s)  LR: 1.829e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.631 (1.631)  Loss:  0.6367 (0.6367)  Acc@1: 91.8945 (91.8945)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.194 (0.572)  Loss:  0.7373 (1.0601)  Acc@1: 87.5000 (80.8780)  Acc@5: 98.3491 (95.4900)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-564.pth.tar', 80.88200002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-565.pth.tar', 80.878)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-559.pth.tar', 80.849999921875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-563.pth.tar', 80.846)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-554.pth.tar', 80.8340000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-552.pth.tar', 80.83000000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-561.pth.tar', 80.82199997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-557.pth.tar', 80.81399984619141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-555.pth.tar', 80.80200013183594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-560.pth.tar', 80.78599994873046)

Train: 566 [   0/1251 (  0%)]  Loss: 3.066 (3.07)  Time: 2.177s,  470.40/s  (2.177s,  470.40/s)  LR: 1.782e-05  Data: 1.443 (1.443)
Train: 566 [  50/1251 (  4%)]  Loss: 2.993 (3.03)  Time: 0.793s, 1291.64/s  (0.835s, 1226.49/s)  LR: 1.782e-05  Data: 0.010 (0.043)
Train: 566 [ 100/1251 (  8%)]  Loss: 2.911 (2.99)  Time: 0.809s, 1266.31/s  (0.813s, 1259.48/s)  LR: 1.782e-05  Data: 0.009 (0.027)
Train: 566 [ 150/1251 ( 12%)]  Loss: 2.950 (2.98)  Time: 0.776s, 1319.88/s  (0.805s, 1271.80/s)  LR: 1.782e-05  Data: 0.009 (0.021)
Train: 566 [ 200/1251 ( 16%)]  Loss: 2.826 (2.95)  Time: 0.778s, 1316.88/s  (0.800s, 1280.76/s)  LR: 1.782e-05  Data: 0.010 (0.018)
Train: 566 [ 250/1251 ( 20%)]  Loss: 2.895 (2.94)  Time: 0.811s, 1262.59/s  (0.795s, 1287.73/s)  LR: 1.782e-05  Data: 0.010 (0.017)
Train: 566 [ 300/1251 ( 24%)]  Loss: 2.740 (2.91)  Time: 0.772s, 1325.57/s  (0.792s, 1292.32/s)  LR: 1.782e-05  Data: 0.010 (0.016)
Train: 566 [ 350/1251 ( 28%)]  Loss: 3.046 (2.93)  Time: 0.781s, 1311.26/s  (0.791s, 1293.77/s)  LR: 1.782e-05  Data: 0.010 (0.015)
Train: 566 [ 400/1251 ( 32%)]  Loss: 2.981 (2.93)  Time: 0.773s, 1324.99/s  (0.791s, 1294.29/s)  LR: 1.782e-05  Data: 0.009 (0.014)
Train: 566 [ 450/1251 ( 36%)]  Loss: 2.861 (2.93)  Time: 0.774s, 1322.65/s  (0.790s, 1296.56/s)  LR: 1.782e-05  Data: 0.010 (0.014)
Train: 566 [ 500/1251 ( 40%)]  Loss: 3.102 (2.94)  Time: 0.814s, 1257.41/s  (0.791s, 1295.06/s)  LR: 1.782e-05  Data: 0.010 (0.013)
Train: 566 [ 550/1251 ( 44%)]  Loss: 2.806 (2.93)  Time: 0.771s, 1328.45/s  (0.793s, 1291.45/s)  LR: 1.782e-05  Data: 0.010 (0.013)
Train: 566 [ 600/1251 ( 48%)]  Loss: 2.859 (2.93)  Time: 0.806s, 1270.29/s  (0.793s, 1292.04/s)  LR: 1.782e-05  Data: 0.009 (0.013)
Train: 566 [ 650/1251 ( 52%)]  Loss: 3.390 (2.96)  Time: 0.815s, 1257.10/s  (0.792s, 1292.24/s)  LR: 1.782e-05  Data: 0.009 (0.012)
Train: 566 [ 700/1251 ( 56%)]  Loss: 2.897 (2.95)  Time: 0.792s, 1292.64/s  (0.792s, 1292.90/s)  LR: 1.782e-05  Data: 0.010 (0.012)
Train: 566 [ 750/1251 ( 60%)]  Loss: 2.939 (2.95)  Time: 0.804s, 1274.40/s  (0.792s, 1293.74/s)  LR: 1.782e-05  Data: 0.010 (0.012)
Train: 566 [ 800/1251 ( 64%)]  Loss: 3.150 (2.97)  Time: 0.774s, 1323.04/s  (0.792s, 1292.43/s)  LR: 1.782e-05  Data: 0.010 (0.012)
Train: 566 [ 850/1251 ( 68%)]  Loss: 3.220 (2.98)  Time: 0.778s, 1316.04/s  (0.792s, 1293.71/s)  LR: 1.782e-05  Data: 0.010 (0.012)
Train: 566 [ 900/1251 ( 72%)]  Loss: 2.677 (2.96)  Time: 0.772s, 1327.15/s  (0.791s, 1294.52/s)  LR: 1.782e-05  Data: 0.009 (0.012)
Train: 566 [ 950/1251 ( 76%)]  Loss: 3.221 (2.98)  Time: 0.808s, 1267.43/s  (0.791s, 1294.22/s)  LR: 1.782e-05  Data: 0.010 (0.012)
Train: 566 [1000/1251 ( 80%)]  Loss: 2.653 (2.96)  Time: 0.773s, 1324.58/s  (0.791s, 1295.00/s)  LR: 1.782e-05  Data: 0.010 (0.012)
Train: 566 [1050/1251 ( 84%)]  Loss: 2.854 (2.96)  Time: 0.825s, 1241.47/s  (0.790s, 1295.51/s)  LR: 1.782e-05  Data: 0.009 (0.011)
Train: 566 [1100/1251 ( 88%)]  Loss: 3.156 (2.96)  Time: 0.813s, 1259.42/s  (0.791s, 1294.39/s)  LR: 1.782e-05  Data: 0.010 (0.011)
Train: 566 [1150/1251 ( 92%)]  Loss: 3.140 (2.97)  Time: 0.773s, 1324.67/s  (0.791s, 1295.19/s)  LR: 1.782e-05  Data: 0.010 (0.011)
Train: 566 [1200/1251 ( 96%)]  Loss: 2.881 (2.97)  Time: 0.773s, 1325.43/s  (0.790s, 1295.60/s)  LR: 1.782e-05  Data: 0.009 (0.011)
Train: 566 [1250/1251 (100%)]  Loss: 2.690 (2.96)  Time: 0.762s, 1344.23/s  (0.790s, 1296.35/s)  LR: 1.782e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.568 (1.568)  Loss:  0.5928 (0.5928)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.194 (0.568)  Loss:  0.6943 (1.0146)  Acc@1: 87.0283 (80.9840)  Acc@5: 98.2311 (95.5240)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-566.pth.tar', 80.98399992431641)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-564.pth.tar', 80.88200002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-565.pth.tar', 80.878)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-559.pth.tar', 80.849999921875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-563.pth.tar', 80.846)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-554.pth.tar', 80.8340000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-552.pth.tar', 80.83000000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-561.pth.tar', 80.82199997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-557.pth.tar', 80.81399984619141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-555.pth.tar', 80.80200013183594)

Train: 567 [   0/1251 (  0%)]  Loss: 2.771 (2.77)  Time: 2.383s,  429.64/s  (2.383s,  429.64/s)  LR: 1.737e-05  Data: 1.652 (1.652)
Train: 567 [  50/1251 (  4%)]  Loss: 2.802 (2.79)  Time: 0.807s, 1268.43/s  (0.826s, 1239.63/s)  LR: 1.737e-05  Data: 0.010 (0.048)
Train: 567 [ 100/1251 (  8%)]  Loss: 3.055 (2.88)  Time: 0.815s, 1256.70/s  (0.809s, 1265.59/s)  LR: 1.737e-05  Data: 0.010 (0.029)
Train: 567 [ 150/1251 ( 12%)]  Loss: 2.948 (2.89)  Time: 0.775s, 1320.85/s  (0.804s, 1273.76/s)  LR: 1.737e-05  Data: 0.010 (0.023)
Train: 567 [ 200/1251 ( 16%)]  Loss: 3.251 (2.97)  Time: 0.773s, 1324.79/s  (0.801s, 1278.05/s)  LR: 1.737e-05  Data: 0.009 (0.019)
Train: 567 [ 250/1251 ( 20%)]  Loss: 2.727 (2.93)  Time: 0.768s, 1332.97/s  (0.798s, 1283.46/s)  LR: 1.737e-05  Data: 0.009 (0.018)
Train: 567 [ 300/1251 ( 24%)]  Loss: 2.955 (2.93)  Time: 0.774s, 1323.57/s  (0.795s, 1288.26/s)  LR: 1.737e-05  Data: 0.009 (0.016)
Train: 567 [ 350/1251 ( 28%)]  Loss: 2.839 (2.92)  Time: 0.772s, 1326.68/s  (0.793s, 1290.74/s)  LR: 1.737e-05  Data: 0.009 (0.015)
Train: 567 [ 400/1251 ( 32%)]  Loss: 2.937 (2.92)  Time: 0.827s, 1238.04/s  (0.793s, 1292.02/s)  LR: 1.737e-05  Data: 0.009 (0.015)
Train: 567 [ 450/1251 ( 36%)]  Loss: 2.635 (2.89)  Time: 0.773s, 1325.25/s  (0.791s, 1294.28/s)  LR: 1.737e-05  Data: 0.009 (0.014)
Train: 567 [ 500/1251 ( 40%)]  Loss: 3.195 (2.92)  Time: 0.780s, 1312.44/s  (0.791s, 1295.07/s)  LR: 1.737e-05  Data: 0.010 (0.014)
Train: 567 [ 550/1251 ( 44%)]  Loss: 3.020 (2.93)  Time: 0.772s, 1325.86/s  (0.790s, 1295.83/s)  LR: 1.737e-05  Data: 0.010 (0.013)
Train: 567 [ 600/1251 ( 48%)]  Loss: 3.136 (2.94)  Time: 0.807s, 1269.17/s  (0.790s, 1295.55/s)  LR: 1.737e-05  Data: 0.009 (0.013)
Train: 567 [ 650/1251 ( 52%)]  Loss: 2.978 (2.95)  Time: 0.773s, 1323.99/s  (0.790s, 1296.52/s)  LR: 1.737e-05  Data: 0.010 (0.013)
Train: 567 [ 700/1251 ( 56%)]  Loss: 2.964 (2.95)  Time: 0.774s, 1322.66/s  (0.790s, 1296.75/s)  LR: 1.737e-05  Data: 0.009 (0.013)
Train: 567 [ 750/1251 ( 60%)]  Loss: 3.032 (2.95)  Time: 0.792s, 1293.54/s  (0.789s, 1297.65/s)  LR: 1.737e-05  Data: 0.009 (0.012)
Train: 567 [ 800/1251 ( 64%)]  Loss: 2.613 (2.93)  Time: 0.781s, 1311.93/s  (0.789s, 1298.25/s)  LR: 1.737e-05  Data: 0.010 (0.012)
Train: 567 [ 850/1251 ( 68%)]  Loss: 2.814 (2.93)  Time: 0.777s, 1317.85/s  (0.789s, 1298.66/s)  LR: 1.737e-05  Data: 0.009 (0.012)
Train: 567 [ 900/1251 ( 72%)]  Loss: 3.037 (2.93)  Time: 0.773s, 1324.49/s  (0.788s, 1299.47/s)  LR: 1.737e-05  Data: 0.009 (0.012)
Train: 567 [ 950/1251 ( 76%)]  Loss: 2.932 (2.93)  Time: 0.777s, 1317.66/s  (0.788s, 1299.66/s)  LR: 1.737e-05  Data: 0.009 (0.012)
Train: 567 [1000/1251 ( 80%)]  Loss: 2.960 (2.93)  Time: 0.807s, 1268.60/s  (0.788s, 1299.67/s)  LR: 1.737e-05  Data: 0.009 (0.012)
Train: 567 [1050/1251 ( 84%)]  Loss: 2.881 (2.93)  Time: 0.776s, 1319.55/s  (0.788s, 1299.50/s)  LR: 1.737e-05  Data: 0.010 (0.012)
Train: 567 [1100/1251 ( 88%)]  Loss: 3.089 (2.94)  Time: 0.781s, 1310.93/s  (0.788s, 1299.27/s)  LR: 1.737e-05  Data: 0.009 (0.012)
Train: 567 [1150/1251 ( 92%)]  Loss: 2.969 (2.94)  Time: 0.809s, 1266.20/s  (0.789s, 1298.45/s)  LR: 1.737e-05  Data: 0.010 (0.012)
Train: 567 [1200/1251 ( 96%)]  Loss: 3.115 (2.95)  Time: 0.773s, 1325.01/s  (0.789s, 1297.92/s)  LR: 1.737e-05  Data: 0.009 (0.011)
Train: 567 [1250/1251 (100%)]  Loss: 3.025 (2.95)  Time: 0.759s, 1348.71/s  (0.789s, 1298.15/s)  LR: 1.737e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.618 (1.618)  Loss:  0.6548 (0.6548)  Acc@1: 92.3828 (92.3828)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.194 (0.563)  Loss:  0.7422 (1.0900)  Acc@1: 87.6179 (80.9120)  Acc@5: 98.1132 (95.4860)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-566.pth.tar', 80.98399992431641)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-567.pth.tar', 80.91200005126953)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-564.pth.tar', 80.88200002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-565.pth.tar', 80.878)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-559.pth.tar', 80.849999921875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-563.pth.tar', 80.846)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-554.pth.tar', 80.8340000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-552.pth.tar', 80.83000000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-561.pth.tar', 80.82199997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-557.pth.tar', 80.81399984619141)

Train: 568 [   0/1251 (  0%)]  Loss: 2.853 (2.85)  Time: 2.281s,  448.96/s  (2.281s,  448.96/s)  LR: 1.693e-05  Data: 1.550 (1.550)
Train: 568 [  50/1251 (  4%)]  Loss: 3.205 (3.03)  Time: 0.773s, 1325.36/s  (0.819s, 1250.11/s)  LR: 1.693e-05  Data: 0.010 (0.046)
Train: 568 [ 100/1251 (  8%)]  Loss: 2.933 (3.00)  Time: 0.821s, 1247.04/s  (0.800s, 1280.69/s)  LR: 1.693e-05  Data: 0.010 (0.028)
Train: 568 [ 150/1251 ( 12%)]  Loss: 2.853 (2.96)  Time: 0.868s, 1179.51/s  (0.795s, 1287.89/s)  LR: 1.693e-05  Data: 0.010 (0.022)
Train: 568 [ 200/1251 ( 16%)]  Loss: 2.867 (2.94)  Time: 0.841s, 1217.58/s  (0.796s, 1286.23/s)  LR: 1.693e-05  Data: 0.009 (0.019)
Train: 568 [ 250/1251 ( 20%)]  Loss: 2.961 (2.95)  Time: 0.789s, 1297.58/s  (0.799s, 1281.39/s)  LR: 1.693e-05  Data: 0.010 (0.017)
Train: 568 [ 300/1251 ( 24%)]  Loss: 2.666 (2.91)  Time: 0.773s, 1324.54/s  (0.800s, 1279.52/s)  LR: 1.693e-05  Data: 0.009 (0.016)
Train: 568 [ 350/1251 ( 28%)]  Loss: 2.986 (2.92)  Time: 0.779s, 1314.44/s  (0.798s, 1283.36/s)  LR: 1.693e-05  Data: 0.010 (0.015)
Train: 568 [ 400/1251 ( 32%)]  Loss: 3.072 (2.93)  Time: 0.785s, 1303.72/s  (0.796s, 1285.98/s)  LR: 1.693e-05  Data: 0.011 (0.014)
Train: 568 [ 450/1251 ( 36%)]  Loss: 2.805 (2.92)  Time: 0.784s, 1306.06/s  (0.795s, 1288.86/s)  LR: 1.693e-05  Data: 0.011 (0.014)
Train: 568 [ 500/1251 ( 40%)]  Loss: 2.878 (2.92)  Time: 0.773s, 1325.51/s  (0.793s, 1291.31/s)  LR: 1.693e-05  Data: 0.009 (0.013)
Train: 568 [ 550/1251 ( 44%)]  Loss: 2.703 (2.90)  Time: 0.780s, 1312.57/s  (0.793s, 1291.27/s)  LR: 1.693e-05  Data: 0.010 (0.013)
Train: 568 [ 600/1251 ( 48%)]  Loss: 2.767 (2.89)  Time: 0.772s, 1326.89/s  (0.792s, 1292.67/s)  LR: 1.693e-05  Data: 0.009 (0.013)
Train: 568 [ 650/1251 ( 52%)]  Loss: 2.889 (2.89)  Time: 0.770s, 1330.15/s  (0.791s, 1294.51/s)  LR: 1.693e-05  Data: 0.010 (0.013)
Train: 568 [ 700/1251 ( 56%)]  Loss: 2.889 (2.89)  Time: 0.808s, 1267.73/s  (0.790s, 1295.57/s)  LR: 1.693e-05  Data: 0.010 (0.012)
Train: 568 [ 750/1251 ( 60%)]  Loss: 2.780 (2.88)  Time: 0.778s, 1315.88/s  (0.790s, 1295.43/s)  LR: 1.693e-05  Data: 0.010 (0.012)
Train: 568 [ 800/1251 ( 64%)]  Loss: 2.699 (2.87)  Time: 0.855s, 1197.13/s  (0.790s, 1296.54/s)  LR: 1.693e-05  Data: 0.009 (0.012)
Train: 568 [ 850/1251 ( 68%)]  Loss: 2.807 (2.87)  Time: 0.772s, 1326.00/s  (0.789s, 1297.51/s)  LR: 1.693e-05  Data: 0.010 (0.012)
Train: 568 [ 900/1251 ( 72%)]  Loss: 2.988 (2.87)  Time: 0.806s, 1270.20/s  (0.789s, 1297.62/s)  LR: 1.693e-05  Data: 0.010 (0.012)
Train: 568 [ 950/1251 ( 76%)]  Loss: 2.920 (2.88)  Time: 0.818s, 1251.11/s  (0.790s, 1296.46/s)  LR: 1.693e-05  Data: 0.009 (0.012)
Train: 568 [1000/1251 ( 80%)]  Loss: 2.911 (2.88)  Time: 0.771s, 1328.21/s  (0.791s, 1295.20/s)  LR: 1.693e-05  Data: 0.010 (0.012)
Train: 568 [1050/1251 ( 84%)]  Loss: 3.099 (2.89)  Time: 0.771s, 1327.51/s  (0.790s, 1295.75/s)  LR: 1.693e-05  Data: 0.010 (0.012)
Train: 568 [1100/1251 ( 88%)]  Loss: 2.565 (2.87)  Time: 0.775s, 1321.21/s  (0.790s, 1296.21/s)  LR: 1.693e-05  Data: 0.010 (0.011)
Train: 568 [1150/1251 ( 92%)]  Loss: 2.879 (2.87)  Time: 0.771s, 1327.63/s  (0.790s, 1295.93/s)  LR: 1.693e-05  Data: 0.010 (0.011)
Train: 568 [1200/1251 ( 96%)]  Loss: 3.004 (2.88)  Time: 0.839s, 1220.32/s  (0.790s, 1296.61/s)  LR: 1.693e-05  Data: 0.010 (0.011)
Train: 568 [1250/1251 (100%)]  Loss: 2.933 (2.88)  Time: 0.758s, 1351.63/s  (0.789s, 1297.31/s)  LR: 1.693e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.570 (1.570)  Loss:  0.6367 (0.6367)  Acc@1: 92.3828 (92.3828)  Acc@5: 98.7305 (98.7305)
Test: [  48/48]  Time: 0.194 (0.567)  Loss:  0.7432 (1.0863)  Acc@1: 87.2642 (80.9400)  Acc@5: 98.1132 (95.5480)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-566.pth.tar', 80.98399992431641)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-568.pth.tar', 80.94000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-567.pth.tar', 80.91200005126953)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-564.pth.tar', 80.88200002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-565.pth.tar', 80.878)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-559.pth.tar', 80.849999921875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-563.pth.tar', 80.846)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-554.pth.tar', 80.8340000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-552.pth.tar', 80.83000000244141)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-561.pth.tar', 80.82199997558594)

Train: 569 [   0/1251 (  0%)]  Loss: 3.239 (3.24)  Time: 2.435s,  420.50/s  (2.435s,  420.50/s)  LR: 1.651e-05  Data: 1.703 (1.703)
Train: 569 [  50/1251 (  4%)]  Loss: 2.915 (3.08)  Time: 0.815s, 1256.97/s  (0.833s, 1230.02/s)  LR: 1.651e-05  Data: 0.011 (0.046)
Train: 569 [ 100/1251 (  8%)]  Loss: 2.916 (3.02)  Time: 0.808s, 1267.57/s  (0.810s, 1263.74/s)  LR: 1.651e-05  Data: 0.009 (0.028)
Train: 569 [ 150/1251 ( 12%)]  Loss: 3.013 (3.02)  Time: 0.806s, 1269.78/s  (0.810s, 1264.61/s)  LR: 1.651e-05  Data: 0.009 (0.022)
Train: 569 [ 200/1251 ( 16%)]  Loss: 3.092 (3.04)  Time: 0.770s, 1329.60/s  (0.808s, 1268.02/s)  LR: 1.651e-05  Data: 0.009 (0.019)
Train: 569 [ 250/1251 ( 20%)]  Loss: 2.918 (3.02)  Time: 0.782s, 1310.00/s  (0.802s, 1276.17/s)  LR: 1.651e-05  Data: 0.013 (0.017)
Train: 569 [ 300/1251 ( 24%)]  Loss: 2.916 (3.00)  Time: 0.781s, 1311.95/s  (0.799s, 1282.34/s)  LR: 1.651e-05  Data: 0.008 (0.016)
Train: 569 [ 350/1251 ( 28%)]  Loss: 3.154 (3.02)  Time: 0.777s, 1317.85/s  (0.797s, 1284.39/s)  LR: 1.651e-05  Data: 0.009 (0.015)
Train: 569 [ 400/1251 ( 32%)]  Loss: 3.213 (3.04)  Time: 0.831s, 1231.72/s  (0.797s, 1284.89/s)  LR: 1.651e-05  Data: 0.009 (0.014)
Train: 569 [ 450/1251 ( 36%)]  Loss: 2.976 (3.04)  Time: 0.815s, 1255.86/s  (0.796s, 1287.19/s)  LR: 1.651e-05  Data: 0.011 (0.014)
Train: 569 [ 500/1251 ( 40%)]  Loss: 2.913 (3.02)  Time: 0.781s, 1310.42/s  (0.794s, 1289.94/s)  LR: 1.651e-05  Data: 0.010 (0.013)
Train: 569 [ 550/1251 ( 44%)]  Loss: 2.724 (3.00)  Time: 0.773s, 1324.69/s  (0.793s, 1291.64/s)  LR: 1.651e-05  Data: 0.009 (0.013)
Train: 569 [ 600/1251 ( 48%)]  Loss: 3.001 (3.00)  Time: 0.781s, 1311.59/s  (0.793s, 1292.08/s)  LR: 1.651e-05  Data: 0.009 (0.013)
Train: 569 [ 650/1251 ( 52%)]  Loss: 2.702 (2.98)  Time: 0.773s, 1325.14/s  (0.791s, 1294.07/s)  LR: 1.651e-05  Data: 0.009 (0.013)
Train: 569 [ 700/1251 ( 56%)]  Loss: 3.106 (2.99)  Time: 0.772s, 1326.70/s  (0.790s, 1295.54/s)  LR: 1.651e-05  Data: 0.011 (0.012)
Train: 569 [ 750/1251 ( 60%)]  Loss: 2.782 (2.97)  Time: 0.797s, 1285.61/s  (0.790s, 1296.30/s)  LR: 1.651e-05  Data: 0.010 (0.012)
Train: 569 [ 800/1251 ( 64%)]  Loss: 2.565 (2.95)  Time: 0.811s, 1262.07/s  (0.791s, 1294.89/s)  LR: 1.651e-05  Data: 0.010 (0.012)
Train: 569 [ 850/1251 ( 68%)]  Loss: 2.959 (2.95)  Time: 0.772s, 1326.71/s  (0.790s, 1295.48/s)  LR: 1.651e-05  Data: 0.010 (0.012)
Train: 569 [ 900/1251 ( 72%)]  Loss: 2.953 (2.95)  Time: 0.866s, 1182.97/s  (0.791s, 1295.32/s)  LR: 1.651e-05  Data: 0.010 (0.012)
Train: 569 [ 950/1251 ( 76%)]  Loss: 3.020 (2.95)  Time: 0.776s, 1318.97/s  (0.790s, 1295.62/s)  LR: 1.651e-05  Data: 0.009 (0.012)
Train: 569 [1000/1251 ( 80%)]  Loss: 2.929 (2.95)  Time: 0.816s, 1255.03/s  (0.790s, 1295.89/s)  LR: 1.651e-05  Data: 0.011 (0.012)
Train: 569 [1050/1251 ( 84%)]  Loss: 3.053 (2.96)  Time: 0.783s, 1307.06/s  (0.790s, 1296.03/s)  LR: 1.651e-05  Data: 0.010 (0.012)
Train: 569 [1100/1251 ( 88%)]  Loss: 2.944 (2.96)  Time: 0.775s, 1320.97/s  (0.790s, 1296.10/s)  LR: 1.651e-05  Data: 0.009 (0.012)
Train: 569 [1150/1251 ( 92%)]  Loss: 3.157 (2.96)  Time: 0.772s, 1326.64/s  (0.790s, 1296.36/s)  LR: 1.651e-05  Data: 0.009 (0.011)
Train: 569 [1200/1251 ( 96%)]  Loss: 2.938 (2.96)  Time: 0.775s, 1321.13/s  (0.790s, 1296.77/s)  LR: 1.651e-05  Data: 0.011 (0.011)
Train: 569 [1250/1251 (100%)]  Loss: 2.935 (2.96)  Time: 0.759s, 1349.59/s  (0.789s, 1297.18/s)  LR: 1.651e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.610 (1.610)  Loss:  0.6401 (0.6401)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.194 (0.583)  Loss:  0.7563 (1.0831)  Acc@1: 87.3821 (80.8740)  Acc@5: 98.3491 (95.5000)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-566.pth.tar', 80.98399992431641)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-568.pth.tar', 80.94000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-567.pth.tar', 80.91200005126953)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-564.pth.tar', 80.88200002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-565.pth.tar', 80.878)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-569.pth.tar', 80.874000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-559.pth.tar', 80.849999921875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-563.pth.tar', 80.846)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-554.pth.tar', 80.8340000024414)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-552.pth.tar', 80.83000000244141)

Train: 570 [   0/1251 (  0%)]  Loss: 3.074 (3.07)  Time: 2.147s,  476.88/s  (2.147s,  476.88/s)  LR: 1.609e-05  Data: 1.416 (1.416)
Train: 570 [  50/1251 (  4%)]  Loss: 2.602 (2.84)  Time: 0.775s, 1320.98/s  (0.829s, 1235.06/s)  LR: 1.609e-05  Data: 0.010 (0.043)
Train: 570 [ 100/1251 (  8%)]  Loss: 2.935 (2.87)  Time: 0.781s, 1311.06/s  (0.806s, 1270.48/s)  LR: 1.609e-05  Data: 0.010 (0.027)
Train: 570 [ 150/1251 ( 12%)]  Loss: 2.860 (2.87)  Time: 0.823s, 1244.47/s  (0.797s, 1284.66/s)  LR: 1.609e-05  Data: 0.010 (0.021)
Train: 570 [ 200/1251 ( 16%)]  Loss: 2.718 (2.84)  Time: 0.772s, 1326.14/s  (0.798s, 1283.61/s)  LR: 1.609e-05  Data: 0.010 (0.019)
Train: 570 [ 250/1251 ( 20%)]  Loss: 2.820 (2.83)  Time: 0.773s, 1324.55/s  (0.796s, 1286.48/s)  LR: 1.609e-05  Data: 0.010 (0.017)
Train: 570 [ 300/1251 ( 24%)]  Loss: 3.094 (2.87)  Time: 0.770s, 1330.02/s  (0.792s, 1292.18/s)  LR: 1.609e-05  Data: 0.009 (0.016)
Train: 570 [ 350/1251 ( 28%)]  Loss: 3.218 (2.92)  Time: 0.806s, 1269.71/s  (0.791s, 1294.17/s)  LR: 1.609e-05  Data: 0.010 (0.015)
Train: 570 [ 400/1251 ( 32%)]  Loss: 2.792 (2.90)  Time: 0.780s, 1313.18/s  (0.790s, 1296.57/s)  LR: 1.609e-05  Data: 0.010 (0.014)
Train: 570 [ 450/1251 ( 36%)]  Loss: 3.112 (2.92)  Time: 0.792s, 1292.69/s  (0.791s, 1294.86/s)  LR: 1.609e-05  Data: 0.009 (0.014)
Train: 570 [ 500/1251 ( 40%)]  Loss: 2.969 (2.93)  Time: 0.772s, 1326.57/s  (0.790s, 1296.56/s)  LR: 1.609e-05  Data: 0.010 (0.013)
Train: 570 [ 550/1251 ( 44%)]  Loss: 3.204 (2.95)  Time: 0.771s, 1328.22/s  (0.789s, 1297.49/s)  LR: 1.609e-05  Data: 0.010 (0.013)
Train: 570 [ 600/1251 ( 48%)]  Loss: 2.845 (2.94)  Time: 0.776s, 1320.19/s  (0.790s, 1296.74/s)  LR: 1.609e-05  Data: 0.012 (0.013)
Train: 570 [ 650/1251 ( 52%)]  Loss: 2.999 (2.95)  Time: 0.807s, 1269.41/s  (0.790s, 1296.13/s)  LR: 1.609e-05  Data: 0.010 (0.013)
Train: 570 [ 700/1251 ( 56%)]  Loss: 2.644 (2.93)  Time: 0.785s, 1303.78/s  (0.789s, 1297.42/s)  LR: 1.609e-05  Data: 0.010 (0.012)
Train: 570 [ 750/1251 ( 60%)]  Loss: 2.681 (2.91)  Time: 0.773s, 1324.52/s  (0.789s, 1298.00/s)  LR: 1.609e-05  Data: 0.010 (0.012)
Train: 570 [ 800/1251 ( 64%)]  Loss: 2.838 (2.91)  Time: 0.787s, 1300.87/s  (0.789s, 1298.45/s)  LR: 1.609e-05  Data: 0.010 (0.012)
Train: 570 [ 850/1251 ( 68%)]  Loss: 3.053 (2.91)  Time: 0.771s, 1327.38/s  (0.788s, 1299.05/s)  LR: 1.609e-05  Data: 0.010 (0.012)
Train: 570 [ 900/1251 ( 72%)]  Loss: 3.010 (2.92)  Time: 0.809s, 1265.11/s  (0.789s, 1298.65/s)  LR: 1.609e-05  Data: 0.010 (0.012)
Train: 570 [ 950/1251 ( 76%)]  Loss: 3.027 (2.92)  Time: 0.772s, 1326.93/s  (0.788s, 1299.66/s)  LR: 1.609e-05  Data: 0.010 (0.012)
Train: 570 [1000/1251 ( 80%)]  Loss: 3.043 (2.93)  Time: 0.780s, 1313.33/s  (0.788s, 1299.76/s)  LR: 1.609e-05  Data: 0.009 (0.012)
Train: 570 [1050/1251 ( 84%)]  Loss: 3.171 (2.94)  Time: 0.776s, 1319.02/s  (0.788s, 1299.89/s)  LR: 1.609e-05  Data: 0.010 (0.012)
Train: 570 [1100/1251 ( 88%)]  Loss: 3.305 (2.96)  Time: 0.779s, 1314.62/s  (0.788s, 1299.02/s)  LR: 1.609e-05  Data: 0.009 (0.011)
Train: 570 [1150/1251 ( 92%)]  Loss: 2.814 (2.95)  Time: 0.771s, 1327.58/s  (0.788s, 1299.05/s)  LR: 1.609e-05  Data: 0.010 (0.011)
Train: 570 [1200/1251 ( 96%)]  Loss: 2.613 (2.94)  Time: 0.815s, 1256.39/s  (0.788s, 1299.48/s)  LR: 1.609e-05  Data: 0.011 (0.011)
Train: 570 [1250/1251 (100%)]  Loss: 3.012 (2.94)  Time: 0.762s, 1343.63/s  (0.788s, 1298.97/s)  LR: 1.609e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.578 (1.578)  Loss:  0.5718 (0.5718)  Acc@1: 92.3828 (92.3828)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.194 (0.566)  Loss:  0.6992 (1.0100)  Acc@1: 87.1462 (81.0100)  Acc@5: 98.2311 (95.4960)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-570.pth.tar', 81.00999997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-566.pth.tar', 80.98399992431641)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-568.pth.tar', 80.94000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-567.pth.tar', 80.91200005126953)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-564.pth.tar', 80.88200002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-565.pth.tar', 80.878)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-569.pth.tar', 80.874000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-559.pth.tar', 80.849999921875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-563.pth.tar', 80.846)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-554.pth.tar', 80.8340000024414)

Train: 571 [   0/1251 (  0%)]  Loss: 2.906 (2.91)  Time: 2.667s,  383.93/s  (2.667s,  383.93/s)  LR: 1.570e-05  Data: 1.936 (1.936)
Train: 571 [  50/1251 (  4%)]  Loss: 2.849 (2.88)  Time: 0.782s, 1309.32/s  (0.852s, 1202.42/s)  LR: 1.570e-05  Data: 0.012 (0.052)
Train: 571 [ 100/1251 (  8%)]  Loss: 3.097 (2.95)  Time: 0.773s, 1324.20/s  (0.816s, 1254.46/s)  LR: 1.570e-05  Data: 0.010 (0.031)
Train: 571 [ 150/1251 ( 12%)]  Loss: 3.014 (2.97)  Time: 0.810s, 1263.91/s  (0.809s, 1265.11/s)  LR: 1.570e-05  Data: 0.010 (0.024)
Train: 571 [ 200/1251 ( 16%)]  Loss: 3.127 (3.00)  Time: 0.786s, 1302.61/s  (0.803s, 1275.29/s)  LR: 1.570e-05  Data: 0.014 (0.021)
Train: 571 [ 250/1251 ( 20%)]  Loss: 2.770 (2.96)  Time: 0.809s, 1265.99/s  (0.798s, 1282.57/s)  LR: 1.570e-05  Data: 0.010 (0.019)
Train: 571 [ 300/1251 ( 24%)]  Loss: 3.113 (2.98)  Time: 0.775s, 1321.94/s  (0.798s, 1282.97/s)  LR: 1.570e-05  Data: 0.009 (0.017)
Train: 571 [ 350/1251 ( 28%)]  Loss: 2.797 (2.96)  Time: 0.771s, 1328.79/s  (0.795s, 1287.88/s)  LR: 1.570e-05  Data: 0.009 (0.016)
Train: 571 [ 400/1251 ( 32%)]  Loss: 2.889 (2.95)  Time: 0.775s, 1322.01/s  (0.793s, 1290.71/s)  LR: 1.570e-05  Data: 0.009 (0.015)
Train: 571 [ 450/1251 ( 36%)]  Loss: 3.123 (2.97)  Time: 0.850s, 1205.10/s  (0.793s, 1291.53/s)  LR: 1.570e-05  Data: 0.010 (0.015)
Train: 571 [ 500/1251 ( 40%)]  Loss: 3.028 (2.97)  Time: 0.791s, 1294.27/s  (0.792s, 1293.37/s)  LR: 1.570e-05  Data: 0.009 (0.014)
Train: 571 [ 550/1251 ( 44%)]  Loss: 3.293 (3.00)  Time: 0.772s, 1326.90/s  (0.793s, 1290.51/s)  LR: 1.570e-05  Data: 0.010 (0.014)
Train: 571 [ 600/1251 ( 48%)]  Loss: 3.103 (3.01)  Time: 0.788s, 1300.09/s  (0.793s, 1291.26/s)  LR: 1.570e-05  Data: 0.011 (0.014)
Train: 571 [ 650/1251 ( 52%)]  Loss: 3.220 (3.02)  Time: 0.833s, 1229.12/s  (0.792s, 1292.39/s)  LR: 1.570e-05  Data: 0.010 (0.013)
Train: 571 [ 700/1251 ( 56%)]  Loss: 2.716 (3.00)  Time: 0.771s, 1328.63/s  (0.791s, 1294.05/s)  LR: 1.570e-05  Data: 0.010 (0.013)
Train: 571 [ 750/1251 ( 60%)]  Loss: 2.447 (2.97)  Time: 0.777s, 1318.59/s  (0.791s, 1295.14/s)  LR: 1.570e-05  Data: 0.010 (0.013)
Train: 571 [ 800/1251 ( 64%)]  Loss: 2.805 (2.96)  Time: 0.776s, 1319.38/s  (0.790s, 1295.94/s)  LR: 1.570e-05  Data: 0.010 (0.013)
Train: 571 [ 850/1251 ( 68%)]  Loss: 2.515 (2.93)  Time: 0.776s, 1320.28/s  (0.789s, 1297.06/s)  LR: 1.570e-05  Data: 0.009 (0.013)
Train: 571 [ 900/1251 ( 72%)]  Loss: 2.757 (2.92)  Time: 0.778s, 1315.46/s  (0.789s, 1297.79/s)  LR: 1.570e-05  Data: 0.010 (0.012)
Train: 571 [ 950/1251 ( 76%)]  Loss: 3.170 (2.94)  Time: 0.772s, 1325.83/s  (0.789s, 1297.21/s)  LR: 1.570e-05  Data: 0.009 (0.012)
Train: 571 [1000/1251 ( 80%)]  Loss: 2.710 (2.93)  Time: 0.778s, 1316.11/s  (0.789s, 1297.95/s)  LR: 1.570e-05  Data: 0.010 (0.012)
Train: 571 [1050/1251 ( 84%)]  Loss: 2.353 (2.90)  Time: 0.785s, 1305.04/s  (0.789s, 1298.28/s)  LR: 1.570e-05  Data: 0.010 (0.012)
Train: 571 [1100/1251 ( 88%)]  Loss: 3.082 (2.91)  Time: 0.832s, 1231.27/s  (0.789s, 1298.05/s)  LR: 1.570e-05  Data: 0.010 (0.012)
Train: 571 [1150/1251 ( 92%)]  Loss: 2.729 (2.90)  Time: 0.778s, 1316.07/s  (0.790s, 1296.46/s)  LR: 1.570e-05  Data: 0.010 (0.012)
Train: 571 [1200/1251 ( 96%)]  Loss: 2.593 (2.89)  Time: 0.794s, 1288.90/s  (0.790s, 1296.28/s)  LR: 1.570e-05  Data: 0.010 (0.012)
Train: 571 [1250/1251 (100%)]  Loss: 3.085 (2.90)  Time: 0.765s, 1338.12/s  (0.790s, 1296.60/s)  LR: 1.570e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.576 (1.576)  Loss:  0.6328 (0.6328)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.194 (0.566)  Loss:  0.7593 (1.0788)  Acc@1: 87.6179 (80.9160)  Acc@5: 97.9953 (95.4960)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-570.pth.tar', 81.00999997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-566.pth.tar', 80.98399992431641)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-568.pth.tar', 80.94000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-571.pth.tar', 80.91600018066406)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-567.pth.tar', 80.91200005126953)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-564.pth.tar', 80.88200002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-565.pth.tar', 80.878)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-569.pth.tar', 80.874000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-559.pth.tar', 80.849999921875)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-563.pth.tar', 80.846)

Train: 572 [   0/1251 (  0%)]  Loss: 2.757 (2.76)  Time: 2.382s,  429.82/s  (2.382s,  429.82/s)  LR: 1.531e-05  Data: 1.595 (1.595)
Train: 572 [  50/1251 (  4%)]  Loss: 2.622 (2.69)  Time: 0.780s, 1313.25/s  (0.836s, 1224.78/s)  LR: 1.531e-05  Data: 0.010 (0.045)
Train: 572 [ 100/1251 (  8%)]  Loss: 3.031 (2.80)  Time: 0.815s, 1256.95/s  (0.816s, 1255.09/s)  LR: 1.531e-05  Data: 0.011 (0.028)
Train: 572 [ 150/1251 ( 12%)]  Loss: 2.887 (2.82)  Time: 0.773s, 1324.44/s  (0.815s, 1255.96/s)  LR: 1.531e-05  Data: 0.009 (0.022)
Train: 572 [ 200/1251 ( 16%)]  Loss: 2.632 (2.79)  Time: 0.807s, 1268.89/s  (0.810s, 1264.40/s)  LR: 1.531e-05  Data: 0.009 (0.019)
Train: 572 [ 250/1251 ( 20%)]  Loss: 2.959 (2.81)  Time: 0.772s, 1325.70/s  (0.804s, 1273.43/s)  LR: 1.531e-05  Data: 0.009 (0.017)
Train: 572 [ 300/1251 ( 24%)]  Loss: 2.892 (2.83)  Time: 0.772s, 1326.68/s  (0.800s, 1279.36/s)  LR: 1.531e-05  Data: 0.010 (0.016)
Train: 572 [ 350/1251 ( 28%)]  Loss: 2.986 (2.85)  Time: 0.772s, 1326.78/s  (0.798s, 1282.56/s)  LR: 1.531e-05  Data: 0.010 (0.015)
Train: 572 [ 400/1251 ( 32%)]  Loss: 3.203 (2.89)  Time: 0.789s, 1298.64/s  (0.797s, 1284.92/s)  LR: 1.531e-05  Data: 0.009 (0.015)
Train: 572 [ 450/1251 ( 36%)]  Loss: 3.020 (2.90)  Time: 0.820s, 1248.85/s  (0.795s, 1287.74/s)  LR: 1.531e-05  Data: 0.009 (0.014)
Train: 572 [ 500/1251 ( 40%)]  Loss: 2.786 (2.89)  Time: 0.771s, 1328.61/s  (0.793s, 1290.94/s)  LR: 1.531e-05  Data: 0.010 (0.014)
Train: 572 [ 550/1251 ( 44%)]  Loss: 2.724 (2.87)  Time: 0.772s, 1325.62/s  (0.792s, 1292.31/s)  LR: 1.531e-05  Data: 0.010 (0.013)
Train: 572 [ 600/1251 ( 48%)]  Loss: 2.727 (2.86)  Time: 0.815s, 1256.97/s  (0.792s, 1292.65/s)  LR: 1.531e-05  Data: 0.011 (0.013)
Train: 572 [ 650/1251 ( 52%)]  Loss: 3.002 (2.87)  Time: 0.777s, 1317.56/s  (0.792s, 1292.31/s)  LR: 1.531e-05  Data: 0.009 (0.013)
Train: 572 [ 700/1251 ( 56%)]  Loss: 2.742 (2.86)  Time: 0.809s, 1265.07/s  (0.792s, 1292.99/s)  LR: 1.531e-05  Data: 0.010 (0.013)
Train: 572 [ 750/1251 ( 60%)]  Loss: 3.102 (2.88)  Time: 0.780s, 1312.65/s  (0.793s, 1291.69/s)  LR: 1.531e-05  Data: 0.011 (0.012)
Train: 572 [ 800/1251 ( 64%)]  Loss: 2.861 (2.88)  Time: 0.774s, 1323.64/s  (0.793s, 1291.60/s)  LR: 1.531e-05  Data: 0.010 (0.012)
Train: 572 [ 850/1251 ( 68%)]  Loss: 2.796 (2.87)  Time: 0.774s, 1323.70/s  (0.792s, 1293.09/s)  LR: 1.531e-05  Data: 0.010 (0.012)
Train: 572 [ 900/1251 ( 72%)]  Loss: 3.024 (2.88)  Time: 0.774s, 1322.71/s  (0.792s, 1293.61/s)  LR: 1.531e-05  Data: 0.010 (0.012)
Train: 572 [ 950/1251 ( 76%)]  Loss: 3.093 (2.89)  Time: 0.821s, 1247.71/s  (0.792s, 1293.36/s)  LR: 1.531e-05  Data: 0.010 (0.012)
Train: 572 [1000/1251 ( 80%)]  Loss: 3.124 (2.90)  Time: 0.795s, 1287.98/s  (0.792s, 1292.27/s)  LR: 1.531e-05  Data: 0.010 (0.012)
Train: 572 [1050/1251 ( 84%)]  Loss: 3.249 (2.92)  Time: 0.773s, 1324.54/s  (0.792s, 1293.42/s)  LR: 1.531e-05  Data: 0.010 (0.012)
Train: 572 [1100/1251 ( 88%)]  Loss: 3.139 (2.93)  Time: 0.808s, 1267.34/s  (0.792s, 1292.71/s)  LR: 1.531e-05  Data: 0.009 (0.012)
Train: 572 [1150/1251 ( 92%)]  Loss: 2.943 (2.93)  Time: 0.772s, 1325.85/s  (0.792s, 1292.51/s)  LR: 1.531e-05  Data: 0.009 (0.012)
Train: 572 [1200/1251 ( 96%)]  Loss: 2.923 (2.93)  Time: 0.772s, 1327.10/s  (0.792s, 1293.61/s)  LR: 1.531e-05  Data: 0.010 (0.011)
Train: 572 [1250/1251 (100%)]  Loss: 2.850 (2.93)  Time: 0.770s, 1329.03/s  (0.791s, 1294.35/s)  LR: 1.531e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.544 (1.544)  Loss:  0.6191 (0.6191)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.194 (0.568)  Loss:  0.7324 (1.0559)  Acc@1: 87.3821 (80.8340)  Acc@5: 98.1132 (95.4720)
Train: 573 [   0/1251 (  0%)]  Loss: 2.751 (2.75)  Time: 2.213s,  462.72/s  (2.213s,  462.72/s)  LR: 1.494e-05  Data: 1.498 (1.498)
Train: 573 [  50/1251 (  4%)]  Loss: 2.959 (2.86)  Time: 0.815s, 1256.91/s  (0.850s, 1204.56/s)  LR: 1.494e-05  Data: 0.011 (0.046)
Train: 573 [ 100/1251 (  8%)]  Loss: 2.877 (2.86)  Time: 0.824s, 1243.25/s  (0.819s, 1250.31/s)  LR: 1.494e-05  Data: 0.009 (0.028)
Train: 573 [ 150/1251 ( 12%)]  Loss: 2.672 (2.81)  Time: 0.806s, 1270.13/s  (0.811s, 1262.01/s)  LR: 1.494e-05  Data: 0.012 (0.022)
Train: 573 [ 200/1251 ( 16%)]  Loss: 3.085 (2.87)  Time: 0.811s, 1263.19/s  (0.809s, 1265.96/s)  LR: 1.494e-05  Data: 0.010 (0.019)
Train: 573 [ 250/1251 ( 20%)]  Loss: 2.910 (2.88)  Time: 0.784s, 1306.60/s  (0.804s, 1272.97/s)  LR: 1.494e-05  Data: 0.010 (0.017)
Train: 573 [ 300/1251 ( 24%)]  Loss: 3.160 (2.92)  Time: 0.814s, 1258.55/s  (0.802s, 1277.18/s)  LR: 1.494e-05  Data: 0.010 (0.016)
Train: 573 [ 350/1251 ( 28%)]  Loss: 3.091 (2.94)  Time: 0.787s, 1301.50/s  (0.803s, 1276.01/s)  LR: 1.494e-05  Data: 0.011 (0.015)
Train: 573 [ 400/1251 ( 32%)]  Loss: 2.539 (2.89)  Time: 0.785s, 1303.73/s  (0.801s, 1277.78/s)  LR: 1.494e-05  Data: 0.011 (0.015)
Train: 573 [ 450/1251 ( 36%)]  Loss: 2.963 (2.90)  Time: 0.773s, 1324.79/s  (0.799s, 1281.69/s)  LR: 1.494e-05  Data: 0.010 (0.014)
Train: 573 [ 500/1251 ( 40%)]  Loss: 2.636 (2.88)  Time: 0.770s, 1329.39/s  (0.799s, 1281.89/s)  LR: 1.494e-05  Data: 0.009 (0.014)
Train: 573 [ 550/1251 ( 44%)]  Loss: 2.775 (2.87)  Time: 0.776s, 1319.56/s  (0.798s, 1283.86/s)  LR: 1.494e-05  Data: 0.010 (0.013)
Train: 573 [ 600/1251 ( 48%)]  Loss: 3.178 (2.89)  Time: 0.772s, 1326.22/s  (0.797s, 1284.78/s)  LR: 1.494e-05  Data: 0.009 (0.013)
Train: 573 [ 650/1251 ( 52%)]  Loss: 3.242 (2.92)  Time: 0.826s, 1240.03/s  (0.798s, 1283.34/s)  LR: 1.494e-05  Data: 0.010 (0.013)
Train: 573 [ 700/1251 ( 56%)]  Loss: 3.041 (2.93)  Time: 0.772s, 1326.08/s  (0.797s, 1284.31/s)  LR: 1.494e-05  Data: 0.010 (0.013)
Train: 573 [ 750/1251 ( 60%)]  Loss: 2.908 (2.92)  Time: 0.807s, 1269.15/s  (0.798s, 1283.13/s)  LR: 1.494e-05  Data: 0.010 (0.013)
Train: 573 [ 800/1251 ( 64%)]  Loss: 3.036 (2.93)  Time: 0.774s, 1323.18/s  (0.797s, 1284.85/s)  LR: 1.494e-05  Data: 0.010 (0.012)
Train: 573 [ 850/1251 ( 68%)]  Loss: 3.103 (2.94)  Time: 0.834s, 1228.53/s  (0.797s, 1284.80/s)  LR: 1.494e-05  Data: 0.009 (0.012)
Train: 573 [ 900/1251 ( 72%)]  Loss: 2.910 (2.94)  Time: 0.907s, 1128.86/s  (0.798s, 1283.39/s)  LR: 1.494e-05  Data: 0.009 (0.012)
Train: 573 [ 950/1251 ( 76%)]  Loss: 3.093 (2.95)  Time: 0.783s, 1307.60/s  (0.798s, 1283.77/s)  LR: 1.494e-05  Data: 0.010 (0.012)
Train: 573 [1000/1251 ( 80%)]  Loss: 2.975 (2.95)  Time: 0.773s, 1324.78/s  (0.797s, 1284.63/s)  LR: 1.494e-05  Data: 0.010 (0.012)
Train: 573 [1050/1251 ( 84%)]  Loss: 2.884 (2.94)  Time: 0.772s, 1327.14/s  (0.797s, 1285.57/s)  LR: 1.494e-05  Data: 0.010 (0.012)
Train: 573 [1100/1251 ( 88%)]  Loss: 3.115 (2.95)  Time: 0.806s, 1270.32/s  (0.797s, 1285.04/s)  LR: 1.494e-05  Data: 0.009 (0.012)
Train: 573 [1150/1251 ( 92%)]  Loss: 2.963 (2.95)  Time: 0.787s, 1300.48/s  (0.797s, 1285.57/s)  LR: 1.494e-05  Data: 0.010 (0.012)
Train: 573 [1200/1251 ( 96%)]  Loss: 2.970 (2.95)  Time: 0.774s, 1323.71/s  (0.796s, 1286.12/s)  LR: 1.494e-05  Data: 0.011 (0.012)
Train: 573 [1250/1251 (100%)]  Loss: 3.069 (2.96)  Time: 0.762s, 1343.77/s  (0.796s, 1285.98/s)  LR: 1.494e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.570 (1.570)  Loss:  0.6445 (0.6445)  Acc@1: 91.8945 (91.8945)  Acc@5: 98.7305 (98.7305)
Test: [  48/48]  Time: 0.194 (0.566)  Loss:  0.7373 (1.0742)  Acc@1: 87.6179 (80.9880)  Acc@5: 98.3491 (95.4800)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-570.pth.tar', 81.00999997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-573.pth.tar', 80.98800005126954)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-566.pth.tar', 80.98399992431641)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-568.pth.tar', 80.94000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-571.pth.tar', 80.91600018066406)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-567.pth.tar', 80.91200005126953)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-564.pth.tar', 80.88200002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-565.pth.tar', 80.878)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-569.pth.tar', 80.874000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-559.pth.tar', 80.849999921875)

Train: 574 [   0/1251 (  0%)]  Loss: 3.000 (3.00)  Time: 2.133s,  480.02/s  (2.133s,  480.02/s)  LR: 1.458e-05  Data: 1.419 (1.419)
Train: 574 [  50/1251 (  4%)]  Loss: 2.891 (2.95)  Time: 0.851s, 1203.37/s  (0.819s, 1249.86/s)  LR: 1.458e-05  Data: 0.009 (0.045)
Train: 574 [ 100/1251 (  8%)]  Loss: 2.711 (2.87)  Time: 0.772s, 1325.82/s  (0.800s, 1279.22/s)  LR: 1.458e-05  Data: 0.010 (0.028)
Train: 574 [ 150/1251 ( 12%)]  Loss: 3.108 (2.93)  Time: 0.810s, 1264.38/s  (0.799s, 1282.10/s)  LR: 1.458e-05  Data: 0.009 (0.022)
Train: 574 [ 200/1251 ( 16%)]  Loss: 2.882 (2.92)  Time: 0.771s, 1328.27/s  (0.796s, 1286.12/s)  LR: 1.458e-05  Data: 0.009 (0.019)
Train: 574 [ 250/1251 ( 20%)]  Loss: 3.399 (3.00)  Time: 0.773s, 1324.04/s  (0.793s, 1290.75/s)  LR: 1.458e-05  Data: 0.009 (0.017)
Train: 574 [ 300/1251 ( 24%)]  Loss: 2.659 (2.95)  Time: 0.779s, 1314.77/s  (0.792s, 1293.10/s)  LR: 1.458e-05  Data: 0.010 (0.016)
Train: 574 [ 350/1251 ( 28%)]  Loss: 2.898 (2.94)  Time: 0.811s, 1263.33/s  (0.791s, 1294.76/s)  LR: 1.458e-05  Data: 0.010 (0.015)
Train: 574 [ 400/1251 ( 32%)]  Loss: 3.180 (2.97)  Time: 0.779s, 1314.86/s  (0.789s, 1297.08/s)  LR: 1.458e-05  Data: 0.009 (0.014)
Train: 574 [ 450/1251 ( 36%)]  Loss: 3.039 (2.98)  Time: 0.807s, 1269.20/s  (0.790s, 1296.14/s)  LR: 1.458e-05  Data: 0.010 (0.014)
Train: 574 [ 500/1251 ( 40%)]  Loss: 2.811 (2.96)  Time: 0.773s, 1324.71/s  (0.789s, 1297.76/s)  LR: 1.458e-05  Data: 0.010 (0.013)
Train: 574 [ 550/1251 ( 44%)]  Loss: 2.866 (2.95)  Time: 0.772s, 1326.42/s  (0.788s, 1298.96/s)  LR: 1.458e-05  Data: 0.010 (0.013)
Train: 574 [ 600/1251 ( 48%)]  Loss: 2.672 (2.93)  Time: 0.784s, 1306.16/s  (0.788s, 1300.29/s)  LR: 1.458e-05  Data: 0.010 (0.013)
Train: 574 [ 650/1251 ( 52%)]  Loss: 2.891 (2.93)  Time: 0.823s, 1244.16/s  (0.788s, 1299.87/s)  LR: 1.458e-05  Data: 0.011 (0.013)
Train: 574 [ 700/1251 ( 56%)]  Loss: 2.986 (2.93)  Time: 0.814s, 1257.25/s  (0.788s, 1299.74/s)  LR: 1.458e-05  Data: 0.011 (0.013)
Train: 574 [ 750/1251 ( 60%)]  Loss: 2.908 (2.93)  Time: 0.770s, 1330.29/s  (0.789s, 1298.51/s)  LR: 1.458e-05  Data: 0.010 (0.012)
Train: 574 [ 800/1251 ( 64%)]  Loss: 2.829 (2.93)  Time: 0.783s, 1307.68/s  (0.788s, 1299.21/s)  LR: 1.458e-05  Data: 0.010 (0.012)
Train: 574 [ 850/1251 ( 68%)]  Loss: 3.075 (2.93)  Time: 0.780s, 1313.62/s  (0.788s, 1299.30/s)  LR: 1.458e-05  Data: 0.013 (0.012)
Train: 574 [ 900/1251 ( 72%)]  Loss: 2.990 (2.94)  Time: 0.776s, 1319.23/s  (0.788s, 1299.93/s)  LR: 1.458e-05  Data: 0.010 (0.012)
Train: 574 [ 950/1251 ( 76%)]  Loss: 3.307 (2.96)  Time: 0.771s, 1327.77/s  (0.787s, 1300.90/s)  LR: 1.458e-05  Data: 0.009 (0.012)
Train: 574 [1000/1251 ( 80%)]  Loss: 3.032 (2.96)  Time: 0.771s, 1327.91/s  (0.787s, 1301.46/s)  LR: 1.458e-05  Data: 0.010 (0.012)
Train: 574 [1050/1251 ( 84%)]  Loss: 3.119 (2.97)  Time: 0.771s, 1328.36/s  (0.787s, 1301.60/s)  LR: 1.458e-05  Data: 0.009 (0.012)
Train: 574 [1100/1251 ( 88%)]  Loss: 3.099 (2.97)  Time: 0.770s, 1330.56/s  (0.787s, 1301.07/s)  LR: 1.458e-05  Data: 0.009 (0.012)
Train: 574 [1150/1251 ( 92%)]  Loss: 2.879 (2.97)  Time: 0.818s, 1251.38/s  (0.787s, 1300.68/s)  LR: 1.458e-05  Data: 0.009 (0.012)
Train: 574 [1200/1251 ( 96%)]  Loss: 2.973 (2.97)  Time: 0.771s, 1327.42/s  (0.788s, 1299.25/s)  LR: 1.458e-05  Data: 0.010 (0.011)
Train: 574 [1250/1251 (100%)]  Loss: 3.018 (2.97)  Time: 0.759s, 1349.76/s  (0.788s, 1299.60/s)  LR: 1.458e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.530 (1.530)  Loss:  0.6582 (0.6582)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.8281 (98.8281)
Test: [  48/48]  Time: 0.194 (0.575)  Loss:  0.7646 (1.0990)  Acc@1: 87.1462 (80.7960)  Acc@5: 98.3491 (95.4860)
Train: 575 [   0/1251 (  0%)]  Loss: 2.965 (2.97)  Time: 2.285s,  448.23/s  (2.285s,  448.23/s)  LR: 1.423e-05  Data: 1.564 (1.564)
Train: 575 [  50/1251 (  4%)]  Loss: 3.384 (3.17)  Time: 0.772s, 1326.24/s  (0.815s, 1255.68/s)  LR: 1.423e-05  Data: 0.009 (0.046)
Train: 575 [ 100/1251 (  8%)]  Loss: 2.470 (2.94)  Time: 0.772s, 1326.67/s  (0.801s, 1278.74/s)  LR: 1.423e-05  Data: 0.010 (0.028)
Train: 575 [ 150/1251 ( 12%)]  Loss: 3.083 (2.98)  Time: 0.774s, 1322.17/s  (0.796s, 1286.04/s)  LR: 1.423e-05  Data: 0.010 (0.022)
Train: 575 [ 200/1251 ( 16%)]  Loss: 3.067 (2.99)  Time: 0.822s, 1245.53/s  (0.801s, 1278.19/s)  LR: 1.423e-05  Data: 0.010 (0.019)
Train: 575 [ 250/1251 ( 20%)]  Loss: 3.061 (3.01)  Time: 0.771s, 1327.66/s  (0.803s, 1276.00/s)  LR: 1.423e-05  Data: 0.010 (0.017)
Train: 575 [ 300/1251 ( 24%)]  Loss: 2.947 (3.00)  Time: 0.827s, 1237.61/s  (0.803s, 1275.04/s)  LR: 1.423e-05  Data: 0.013 (0.016)
Train: 575 [ 350/1251 ( 28%)]  Loss: 2.854 (2.98)  Time: 0.783s, 1308.26/s  (0.804s, 1273.91/s)  LR: 1.423e-05  Data: 0.010 (0.015)
Train: 575 [ 400/1251 ( 32%)]  Loss: 3.024 (2.98)  Time: 0.776s, 1320.20/s  (0.802s, 1277.34/s)  LR: 1.423e-05  Data: 0.010 (0.014)
Train: 575 [ 450/1251 ( 36%)]  Loss: 2.824 (2.97)  Time: 0.816s, 1254.19/s  (0.800s, 1280.17/s)  LR: 1.423e-05  Data: 0.010 (0.014)
Train: 575 [ 500/1251 ( 40%)]  Loss: 2.747 (2.95)  Time: 0.775s, 1321.08/s  (0.799s, 1282.35/s)  LR: 1.423e-05  Data: 0.010 (0.014)
Train: 575 [ 550/1251 ( 44%)]  Loss: 3.103 (2.96)  Time: 0.812s, 1261.21/s  (0.798s, 1283.07/s)  LR: 1.423e-05  Data: 0.009 (0.013)
Train: 575 [ 600/1251 ( 48%)]  Loss: 2.895 (2.96)  Time: 0.829s, 1235.03/s  (0.800s, 1280.33/s)  LR: 1.423e-05  Data: 0.012 (0.013)
Train: 575 [ 650/1251 ( 52%)]  Loss: 3.107 (2.97)  Time: 0.771s, 1328.24/s  (0.799s, 1281.60/s)  LR: 1.423e-05  Data: 0.010 (0.013)
Train: 575 [ 700/1251 ( 56%)]  Loss: 2.958 (2.97)  Time: 0.777s, 1317.32/s  (0.797s, 1284.15/s)  LR: 1.423e-05  Data: 0.009 (0.013)
Train: 575 [ 750/1251 ( 60%)]  Loss: 2.969 (2.97)  Time: 0.790s, 1296.34/s  (0.797s, 1285.57/s)  LR: 1.423e-05  Data: 0.010 (0.012)
Train: 575 [ 800/1251 ( 64%)]  Loss: 3.101 (2.97)  Time: 0.787s, 1301.26/s  (0.796s, 1286.91/s)  LR: 1.423e-05  Data: 0.010 (0.012)
Train: 575 [ 850/1251 ( 68%)]  Loss: 3.035 (2.98)  Time: 0.774s, 1322.84/s  (0.795s, 1288.22/s)  LR: 1.423e-05  Data: 0.010 (0.012)
Train: 575 [ 900/1251 ( 72%)]  Loss: 3.003 (2.98)  Time: 0.773s, 1324.12/s  (0.794s, 1289.42/s)  LR: 1.423e-05  Data: 0.010 (0.012)
Train: 575 [ 950/1251 ( 76%)]  Loss: 2.922 (2.98)  Time: 0.774s, 1323.78/s  (0.794s, 1290.38/s)  LR: 1.423e-05  Data: 0.010 (0.012)
Train: 575 [1000/1251 ( 80%)]  Loss: 3.008 (2.98)  Time: 0.780s, 1312.68/s  (0.793s, 1291.29/s)  LR: 1.423e-05  Data: 0.011 (0.012)
Train: 575 [1050/1251 ( 84%)]  Loss: 2.723 (2.97)  Time: 0.778s, 1317.04/s  (0.792s, 1292.13/s)  LR: 1.423e-05  Data: 0.010 (0.012)
Train: 575 [1100/1251 ( 88%)]  Loss: 3.081 (2.97)  Time: 0.787s, 1301.94/s  (0.792s, 1292.30/s)  LR: 1.423e-05  Data: 0.009 (0.012)
Train: 575 [1150/1251 ( 92%)]  Loss: 2.845 (2.97)  Time: 0.773s, 1323.93/s  (0.792s, 1293.46/s)  LR: 1.423e-05  Data: 0.010 (0.012)
Train: 575 [1200/1251 ( 96%)]  Loss: 2.952 (2.97)  Time: 0.773s, 1323.95/s  (0.791s, 1294.49/s)  LR: 1.423e-05  Data: 0.010 (0.012)
Train: 575 [1250/1251 (100%)]  Loss: 3.257 (2.98)  Time: 0.808s, 1267.49/s  (0.791s, 1294.77/s)  LR: 1.423e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.543 (1.543)  Loss:  0.6904 (0.6904)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.193 (0.568)  Loss:  0.7808 (1.1137)  Acc@1: 87.2642 (80.8300)  Acc@5: 98.3491 (95.5320)
Train: 576 [   0/1251 (  0%)]  Loss: 3.121 (3.12)  Time: 2.208s,  463.86/s  (2.208s,  463.86/s)  LR: 1.390e-05  Data: 1.476 (1.476)
Train: 576 [  50/1251 (  4%)]  Loss: 2.971 (3.05)  Time: 0.776s, 1318.83/s  (0.828s, 1236.77/s)  LR: 1.390e-05  Data: 0.013 (0.045)
Train: 576 [ 100/1251 (  8%)]  Loss: 2.735 (2.94)  Time: 0.774s, 1323.65/s  (0.815s, 1257.07/s)  LR: 1.390e-05  Data: 0.009 (0.028)
Train: 576 [ 150/1251 ( 12%)]  Loss: 3.011 (2.96)  Time: 0.771s, 1328.10/s  (0.804s, 1273.25/s)  LR: 1.390e-05  Data: 0.010 (0.022)
Train: 576 [ 200/1251 ( 16%)]  Loss: 2.845 (2.94)  Time: 0.780s, 1312.57/s  (0.799s, 1281.06/s)  LR: 1.390e-05  Data: 0.010 (0.019)
Train: 576 [ 250/1251 ( 20%)]  Loss: 3.250 (2.99)  Time: 0.808s, 1267.24/s  (0.798s, 1283.95/s)  LR: 1.390e-05  Data: 0.010 (0.017)
Train: 576 [ 300/1251 ( 24%)]  Loss: 3.129 (3.01)  Time: 0.796s, 1286.51/s  (0.796s, 1287.14/s)  LR: 1.390e-05  Data: 0.010 (0.016)
Train: 576 [ 350/1251 ( 28%)]  Loss: 2.855 (2.99)  Time: 0.808s, 1266.99/s  (0.796s, 1286.06/s)  LR: 1.390e-05  Data: 0.009 (0.015)
Train: 576 [ 400/1251 ( 32%)]  Loss: 2.691 (2.96)  Time: 0.810s, 1264.19/s  (0.796s, 1286.26/s)  LR: 1.390e-05  Data: 0.010 (0.014)
Train: 576 [ 450/1251 ( 36%)]  Loss: 2.876 (2.95)  Time: 0.768s, 1333.26/s  (0.796s, 1286.41/s)  LR: 1.390e-05  Data: 0.010 (0.014)
Train: 576 [ 500/1251 ( 40%)]  Loss: 2.952 (2.95)  Time: 0.772s, 1326.84/s  (0.794s, 1289.57/s)  LR: 1.390e-05  Data: 0.010 (0.014)
Train: 576 [ 550/1251 ( 44%)]  Loss: 3.040 (2.96)  Time: 0.773s, 1325.15/s  (0.794s, 1289.85/s)  LR: 1.390e-05  Data: 0.010 (0.013)
Train: 576 [ 600/1251 ( 48%)]  Loss: 3.293 (2.98)  Time: 0.834s, 1227.12/s  (0.794s, 1289.91/s)  LR: 1.390e-05  Data: 0.016 (0.013)
Train: 576 [ 650/1251 ( 52%)]  Loss: 2.634 (2.96)  Time: 0.770s, 1330.38/s  (0.793s, 1290.89/s)  LR: 1.390e-05  Data: 0.009 (0.013)
Train: 576 [ 700/1251 ( 56%)]  Loss: 2.744 (2.94)  Time: 0.815s, 1255.90/s  (0.793s, 1291.96/s)  LR: 1.390e-05  Data: 0.011 (0.013)
Train: 576 [ 750/1251 ( 60%)]  Loss: 3.076 (2.95)  Time: 0.769s, 1331.57/s  (0.793s, 1291.43/s)  LR: 1.390e-05  Data: 0.009 (0.012)
Train: 576 [ 800/1251 ( 64%)]  Loss: 3.073 (2.96)  Time: 0.785s, 1304.06/s  (0.792s, 1292.62/s)  LR: 1.390e-05  Data: 0.010 (0.012)
Train: 576 [ 850/1251 ( 68%)]  Loss: 2.997 (2.96)  Time: 0.783s, 1307.01/s  (0.791s, 1293.95/s)  LR: 1.390e-05  Data: 0.010 (0.012)
Train: 576 [ 900/1251 ( 72%)]  Loss: 2.908 (2.96)  Time: 0.795s, 1288.09/s  (0.791s, 1294.26/s)  LR: 1.390e-05  Data: 0.009 (0.012)
Train: 576 [ 950/1251 ( 76%)]  Loss: 2.422 (2.93)  Time: 0.805s, 1271.91/s  (0.791s, 1295.00/s)  LR: 1.390e-05  Data: 0.009 (0.012)
Train: 576 [1000/1251 ( 80%)]  Loss: 2.757 (2.92)  Time: 0.826s, 1239.05/s  (0.791s, 1294.91/s)  LR: 1.390e-05  Data: 0.014 (0.012)
Train: 576 [1050/1251 ( 84%)]  Loss: 2.899 (2.92)  Time: 0.772s, 1325.91/s  (0.791s, 1294.36/s)  LR: 1.390e-05  Data: 0.010 (0.012)
Train: 576 [1100/1251 ( 88%)]  Loss: 2.905 (2.92)  Time: 0.779s, 1314.79/s  (0.791s, 1294.95/s)  LR: 1.390e-05  Data: 0.009 (0.012)
Train: 576 [1150/1251 ( 92%)]  Loss: 2.924 (2.92)  Time: 0.772s, 1325.57/s  (0.790s, 1295.92/s)  LR: 1.390e-05  Data: 0.010 (0.012)
Train: 576 [1200/1251 ( 96%)]  Loss: 2.879 (2.92)  Time: 0.808s, 1266.63/s  (0.791s, 1295.06/s)  LR: 1.390e-05  Data: 0.009 (0.011)
Train: 576 [1250/1251 (100%)]  Loss: 3.008 (2.92)  Time: 0.761s, 1344.83/s  (0.791s, 1295.37/s)  LR: 1.390e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.551 (1.551)  Loss:  0.6367 (0.6367)  Acc@1: 92.2852 (92.2852)  Acc@5: 98.7305 (98.7305)
Test: [  48/48]  Time: 0.194 (0.572)  Loss:  0.7373 (1.0820)  Acc@1: 87.7359 (80.9520)  Acc@5: 98.2311 (95.4800)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-570.pth.tar', 81.00999997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-573.pth.tar', 80.98800005126954)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-566.pth.tar', 80.98399992431641)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-576.pth.tar', 80.95200010253906)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-568.pth.tar', 80.94000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-571.pth.tar', 80.91600018066406)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-567.pth.tar', 80.91200005126953)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-564.pth.tar', 80.88200002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-565.pth.tar', 80.878)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-569.pth.tar', 80.874000078125)

Train: 577 [   0/1251 (  0%)]  Loss: 3.352 (3.35)  Time: 2.275s,  450.01/s  (2.275s,  450.01/s)  LR: 1.359e-05  Data: 1.545 (1.545)
Train: 577 [  50/1251 (  4%)]  Loss: 2.833 (3.09)  Time: 0.771s, 1327.93/s  (0.829s, 1235.59/s)  LR: 1.359e-05  Data: 0.010 (0.047)
Train: 577 [ 100/1251 (  8%)]  Loss: 2.688 (2.96)  Time: 0.826s, 1240.34/s  (0.809s, 1265.54/s)  LR: 1.359e-05  Data: 0.014 (0.029)
Train: 577 [ 150/1251 ( 12%)]  Loss: 2.848 (2.93)  Time: 0.809s, 1265.42/s  (0.809s, 1265.73/s)  LR: 1.359e-05  Data: 0.010 (0.022)
Train: 577 [ 200/1251 ( 16%)]  Loss: 2.737 (2.89)  Time: 0.784s, 1305.64/s  (0.805s, 1271.72/s)  LR: 1.359e-05  Data: 0.010 (0.019)
Train: 577 [ 250/1251 ( 20%)]  Loss: 3.098 (2.93)  Time: 0.780s, 1312.36/s  (0.802s, 1277.10/s)  LR: 1.359e-05  Data: 0.009 (0.017)
Train: 577 [ 300/1251 ( 24%)]  Loss: 3.053 (2.94)  Time: 0.772s, 1325.84/s  (0.797s, 1284.47/s)  LR: 1.359e-05  Data: 0.009 (0.016)
Train: 577 [ 350/1251 ( 28%)]  Loss: 2.946 (2.94)  Time: 0.807s, 1268.70/s  (0.795s, 1287.76/s)  LR: 1.359e-05  Data: 0.010 (0.015)
Train: 577 [ 400/1251 ( 32%)]  Loss: 2.906 (2.94)  Time: 0.771s, 1328.06/s  (0.795s, 1287.36/s)  LR: 1.359e-05  Data: 0.009 (0.015)
Train: 577 [ 450/1251 ( 36%)]  Loss: 2.803 (2.93)  Time: 0.817s, 1253.14/s  (0.794s, 1288.88/s)  LR: 1.359e-05  Data: 0.011 (0.014)
Train: 577 [ 500/1251 ( 40%)]  Loss: 3.133 (2.95)  Time: 0.769s, 1331.62/s  (0.795s, 1288.29/s)  LR: 1.359e-05  Data: 0.010 (0.014)
Train: 577 [ 550/1251 ( 44%)]  Loss: 3.073 (2.96)  Time: 0.771s, 1327.95/s  (0.794s, 1289.36/s)  LR: 1.359e-05  Data: 0.010 (0.013)
Train: 577 [ 600/1251 ( 48%)]  Loss: 3.068 (2.96)  Time: 0.771s, 1328.55/s  (0.795s, 1288.02/s)  LR: 1.359e-05  Data: 0.010 (0.013)
Train: 577 [ 650/1251 ( 52%)]  Loss: 2.818 (2.95)  Time: 0.774s, 1323.35/s  (0.794s, 1288.91/s)  LR: 1.359e-05  Data: 0.010 (0.013)
Train: 577 [ 700/1251 ( 56%)]  Loss: 2.847 (2.95)  Time: 0.773s, 1324.28/s  (0.793s, 1290.71/s)  LR: 1.359e-05  Data: 0.010 (0.013)
Train: 577 [ 750/1251 ( 60%)]  Loss: 2.749 (2.93)  Time: 0.772s, 1326.38/s  (0.792s, 1292.57/s)  LR: 1.359e-05  Data: 0.010 (0.013)
Train: 577 [ 800/1251 ( 64%)]  Loss: 2.905 (2.93)  Time: 0.779s, 1314.53/s  (0.793s, 1291.37/s)  LR: 1.359e-05  Data: 0.009 (0.012)
Train: 577 [ 850/1251 ( 68%)]  Loss: 3.018 (2.94)  Time: 0.773s, 1324.35/s  (0.792s, 1292.71/s)  LR: 1.359e-05  Data: 0.010 (0.012)
Train: 577 [ 900/1251 ( 72%)]  Loss: 2.913 (2.94)  Time: 0.887s, 1154.90/s  (0.792s, 1292.28/s)  LR: 1.359e-05  Data: 0.011 (0.012)
Train: 577 [ 950/1251 ( 76%)]  Loss: 2.817 (2.93)  Time: 0.778s, 1316.43/s  (0.793s, 1292.03/s)  LR: 1.359e-05  Data: 0.010 (0.012)
Train: 577 [1000/1251 ( 80%)]  Loss: 2.622 (2.92)  Time: 0.790s, 1296.94/s  (0.793s, 1291.93/s)  LR: 1.359e-05  Data: 0.010 (0.012)
Train: 577 [1050/1251 ( 84%)]  Loss: 2.986 (2.92)  Time: 0.810s, 1264.82/s  (0.792s, 1292.39/s)  LR: 1.359e-05  Data: 0.010 (0.012)
Train: 577 [1100/1251 ( 88%)]  Loss: 3.047 (2.92)  Time: 0.770s, 1330.41/s  (0.792s, 1292.44/s)  LR: 1.359e-05  Data: 0.010 (0.012)
Train: 577 [1150/1251 ( 92%)]  Loss: 3.271 (2.94)  Time: 0.822s, 1245.63/s  (0.792s, 1292.74/s)  LR: 1.359e-05  Data: 0.010 (0.012)
Train: 577 [1200/1251 ( 96%)]  Loss: 2.889 (2.94)  Time: 0.852s, 1202.39/s  (0.792s, 1292.37/s)  LR: 1.359e-05  Data: 0.009 (0.012)
Train: 577 [1250/1251 (100%)]  Loss: 2.913 (2.94)  Time: 0.765s, 1338.77/s  (0.793s, 1291.72/s)  LR: 1.359e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.604 (1.604)  Loss:  0.5952 (0.5952)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.194 (0.567)  Loss:  0.7080 (1.0416)  Acc@1: 87.2642 (81.0700)  Acc@5: 98.3491 (95.5760)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-577.pth.tar', 81.07000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-570.pth.tar', 81.00999997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-573.pth.tar', 80.98800005126954)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-566.pth.tar', 80.98399992431641)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-576.pth.tar', 80.95200010253906)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-568.pth.tar', 80.94000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-571.pth.tar', 80.91600018066406)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-567.pth.tar', 80.91200005126953)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-564.pth.tar', 80.88200002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-565.pth.tar', 80.878)

Train: 578 [   0/1251 (  0%)]  Loss: 2.914 (2.91)  Time: 2.284s,  448.24/s  (2.284s,  448.24/s)  LR: 1.328e-05  Data: 1.518 (1.518)
Train: 578 [  50/1251 (  4%)]  Loss: 3.087 (3.00)  Time: 0.796s, 1287.12/s  (0.822s, 1245.90/s)  LR: 1.328e-05  Data: 0.010 (0.045)
Train: 578 [ 100/1251 (  8%)]  Loss: 2.928 (2.98)  Time: 0.866s, 1182.45/s  (0.816s, 1255.21/s)  LR: 1.328e-05  Data: 0.009 (0.028)
Train: 578 [ 150/1251 ( 12%)]  Loss: 2.953 (2.97)  Time: 0.773s, 1325.51/s  (0.805s, 1272.29/s)  LR: 1.328e-05  Data: 0.010 (0.022)
Train: 578 [ 200/1251 ( 16%)]  Loss: 3.050 (2.99)  Time: 0.771s, 1327.79/s  (0.799s, 1282.06/s)  LR: 1.328e-05  Data: 0.010 (0.019)
Train: 578 [ 250/1251 ( 20%)]  Loss: 2.552 (2.91)  Time: 0.777s, 1317.31/s  (0.796s, 1285.76/s)  LR: 1.328e-05  Data: 0.010 (0.017)
Train: 578 [ 300/1251 ( 24%)]  Loss: 2.955 (2.92)  Time: 0.826s, 1239.94/s  (0.795s, 1287.60/s)  LR: 1.328e-05  Data: 0.012 (0.016)
Train: 578 [ 350/1251 ( 28%)]  Loss: 2.843 (2.91)  Time: 0.829s, 1235.12/s  (0.795s, 1287.74/s)  LR: 1.328e-05  Data: 0.010 (0.015)
Train: 578 [ 400/1251 ( 32%)]  Loss: 2.726 (2.89)  Time: 0.779s, 1315.12/s  (0.794s, 1289.22/s)  LR: 1.328e-05  Data: 0.010 (0.015)
Train: 578 [ 450/1251 ( 36%)]  Loss: 2.916 (2.89)  Time: 0.896s, 1142.35/s  (0.794s, 1289.30/s)  LR: 1.328e-05  Data: 0.011 (0.014)
Train: 578 [ 500/1251 ( 40%)]  Loss: 2.793 (2.88)  Time: 0.787s, 1300.49/s  (0.793s, 1291.23/s)  LR: 1.328e-05  Data: 0.010 (0.014)
Train: 578 [ 550/1251 ( 44%)]  Loss: 2.771 (2.87)  Time: 0.773s, 1325.53/s  (0.794s, 1290.06/s)  LR: 1.328e-05  Data: 0.010 (0.013)
Train: 578 [ 600/1251 ( 48%)]  Loss: 2.662 (2.86)  Time: 0.771s, 1328.06/s  (0.793s, 1290.98/s)  LR: 1.328e-05  Data: 0.010 (0.013)
Train: 578 [ 650/1251 ( 52%)]  Loss: 2.866 (2.86)  Time: 0.782s, 1309.71/s  (0.792s, 1292.84/s)  LR: 1.328e-05  Data: 0.010 (0.013)
Train: 578 [ 700/1251 ( 56%)]  Loss: 3.163 (2.88)  Time: 0.812s, 1260.95/s  (0.792s, 1292.27/s)  LR: 1.328e-05  Data: 0.010 (0.013)
Train: 578 [ 750/1251 ( 60%)]  Loss: 2.511 (2.86)  Time: 0.776s, 1319.35/s  (0.793s, 1291.30/s)  LR: 1.328e-05  Data: 0.011 (0.012)
Train: 578 [ 800/1251 ( 64%)]  Loss: 2.987 (2.86)  Time: 0.773s, 1324.85/s  (0.792s, 1292.69/s)  LR: 1.328e-05  Data: 0.010 (0.012)
Train: 578 [ 850/1251 ( 68%)]  Loss: 2.977 (2.87)  Time: 0.769s, 1331.60/s  (0.791s, 1294.10/s)  LR: 1.328e-05  Data: 0.010 (0.012)
Train: 578 [ 900/1251 ( 72%)]  Loss: 3.076 (2.88)  Time: 0.826s, 1240.14/s  (0.791s, 1294.77/s)  LR: 1.328e-05  Data: 0.009 (0.012)
Train: 578 [ 950/1251 ( 76%)]  Loss: 2.822 (2.88)  Time: 0.807s, 1269.14/s  (0.791s, 1295.11/s)  LR: 1.328e-05  Data: 0.009 (0.012)
Train: 578 [1000/1251 ( 80%)]  Loss: 2.862 (2.88)  Time: 0.783s, 1307.84/s  (0.791s, 1295.12/s)  LR: 1.328e-05  Data: 0.010 (0.012)
Train: 578 [1050/1251 ( 84%)]  Loss: 2.836 (2.87)  Time: 0.816s, 1255.03/s  (0.791s, 1295.10/s)  LR: 1.328e-05  Data: 0.009 (0.012)
Train: 578 [1100/1251 ( 88%)]  Loss: 2.902 (2.88)  Time: 0.790s, 1295.49/s  (0.790s, 1295.52/s)  LR: 1.328e-05  Data: 0.015 (0.012)
Train: 578 [1150/1251 ( 92%)]  Loss: 2.476 (2.86)  Time: 0.819s, 1250.17/s  (0.791s, 1295.28/s)  LR: 1.328e-05  Data: 0.010 (0.012)
Train: 578 [1200/1251 ( 96%)]  Loss: 2.963 (2.86)  Time: 0.792s, 1293.56/s  (0.791s, 1295.25/s)  LR: 1.328e-05  Data: 0.009 (0.012)
Train: 578 [1250/1251 (100%)]  Loss: 2.905 (2.87)  Time: 0.760s, 1347.82/s  (0.790s, 1295.90/s)  LR: 1.328e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.535 (1.535)  Loss:  0.6533 (0.6533)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.7305 (98.7305)
Test: [  48/48]  Time: 0.194 (0.567)  Loss:  0.7705 (1.0896)  Acc@1: 87.0283 (81.0280)  Acc@5: 98.4670 (95.5360)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-577.pth.tar', 81.07000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-578.pth.tar', 81.02800005371094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-570.pth.tar', 81.00999997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-573.pth.tar', 80.98800005126954)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-566.pth.tar', 80.98399992431641)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-576.pth.tar', 80.95200010253906)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-568.pth.tar', 80.94000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-571.pth.tar', 80.91600018066406)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-567.pth.tar', 80.91200005126953)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-564.pth.tar', 80.88200002685547)

Train: 579 [   0/1251 (  0%)]  Loss: 3.180 (3.18)  Time: 2.323s,  440.87/s  (2.323s,  440.87/s)  LR: 1.299e-05  Data: 1.537 (1.537)
Train: 579 [  50/1251 (  4%)]  Loss: 2.817 (3.00)  Time: 0.773s, 1324.01/s  (0.818s, 1251.50/s)  LR: 1.299e-05  Data: 0.010 (0.046)
Train: 579 [ 100/1251 (  8%)]  Loss: 3.208 (3.07)  Time: 0.807s, 1269.39/s  (0.803s, 1275.95/s)  LR: 1.299e-05  Data: 0.010 (0.028)
Train: 579 [ 150/1251 ( 12%)]  Loss: 3.087 (3.07)  Time: 0.881s, 1162.64/s  (0.801s, 1278.54/s)  LR: 1.299e-05  Data: 0.010 (0.022)
Train: 579 [ 200/1251 ( 16%)]  Loss: 2.747 (3.01)  Time: 0.821s, 1247.31/s  (0.800s, 1280.22/s)  LR: 1.299e-05  Data: 0.010 (0.019)
Train: 579 [ 250/1251 ( 20%)]  Loss: 2.729 (2.96)  Time: 0.775s, 1321.87/s  (0.798s, 1283.90/s)  LR: 1.299e-05  Data: 0.010 (0.017)
Train: 579 [ 300/1251 ( 24%)]  Loss: 2.942 (2.96)  Time: 0.780s, 1312.92/s  (0.795s, 1288.68/s)  LR: 1.299e-05  Data: 0.010 (0.016)
Train: 579 [ 350/1251 ( 28%)]  Loss: 2.899 (2.95)  Time: 0.773s, 1324.52/s  (0.793s, 1290.75/s)  LR: 1.299e-05  Data: 0.009 (0.015)
Train: 579 [ 400/1251 ( 32%)]  Loss: 2.690 (2.92)  Time: 0.771s, 1328.27/s  (0.791s, 1293.84/s)  LR: 1.299e-05  Data: 0.009 (0.014)
Train: 579 [ 450/1251 ( 36%)]  Loss: 2.656 (2.90)  Time: 0.783s, 1307.59/s  (0.790s, 1296.24/s)  LR: 1.299e-05  Data: 0.010 (0.014)
Train: 579 [ 500/1251 ( 40%)]  Loss: 2.860 (2.89)  Time: 0.780s, 1313.35/s  (0.791s, 1293.95/s)  LR: 1.299e-05  Data: 0.010 (0.013)
Train: 579 [ 550/1251 ( 44%)]  Loss: 2.818 (2.89)  Time: 0.827s, 1238.26/s  (0.792s, 1293.43/s)  LR: 1.299e-05  Data: 0.009 (0.013)
Train: 579 [ 600/1251 ( 48%)]  Loss: 2.942 (2.89)  Time: 0.859s, 1191.51/s  (0.793s, 1291.29/s)  LR: 1.299e-05  Data: 0.010 (0.013)
Train: 579 [ 650/1251 ( 52%)]  Loss: 2.791 (2.88)  Time: 0.773s, 1325.18/s  (0.792s, 1292.87/s)  LR: 1.299e-05  Data: 0.009 (0.013)
Train: 579 [ 700/1251 ( 56%)]  Loss: 3.042 (2.89)  Time: 0.771s, 1327.71/s  (0.792s, 1293.23/s)  LR: 1.299e-05  Data: 0.010 (0.013)
Train: 579 [ 750/1251 ( 60%)]  Loss: 3.093 (2.91)  Time: 0.773s, 1323.87/s  (0.791s, 1294.65/s)  LR: 1.299e-05  Data: 0.010 (0.012)
Train: 579 [ 800/1251 ( 64%)]  Loss: 2.808 (2.90)  Time: 0.775s, 1321.96/s  (0.790s, 1295.53/s)  LR: 1.299e-05  Data: 0.010 (0.012)
Train: 579 [ 850/1251 ( 68%)]  Loss: 3.211 (2.92)  Time: 0.810s, 1264.45/s  (0.790s, 1295.75/s)  LR: 1.299e-05  Data: 0.010 (0.012)
Train: 579 [ 900/1251 ( 72%)]  Loss: 3.200 (2.93)  Time: 0.813s, 1259.11/s  (0.790s, 1296.06/s)  LR: 1.299e-05  Data: 0.009 (0.012)
Train: 579 [ 950/1251 ( 76%)]  Loss: 2.880 (2.93)  Time: 0.778s, 1315.99/s  (0.790s, 1296.28/s)  LR: 1.299e-05  Data: 0.011 (0.012)
Train: 579 [1000/1251 ( 80%)]  Loss: 3.256 (2.95)  Time: 0.782s, 1309.31/s  (0.790s, 1295.95/s)  LR: 1.299e-05  Data: 0.010 (0.012)
Train: 579 [1050/1251 ( 84%)]  Loss: 2.649 (2.93)  Time: 0.772s, 1325.92/s  (0.790s, 1295.87/s)  LR: 1.299e-05  Data: 0.010 (0.012)
Train: 579 [1100/1251 ( 88%)]  Loss: 2.828 (2.93)  Time: 0.814s, 1257.88/s  (0.790s, 1295.92/s)  LR: 1.299e-05  Data: 0.009 (0.012)
Train: 579 [1150/1251 ( 92%)]  Loss: 2.913 (2.93)  Time: 0.772s, 1326.98/s  (0.791s, 1295.09/s)  LR: 1.299e-05  Data: 0.010 (0.011)
Train: 579 [1200/1251 ( 96%)]  Loss: 2.804 (2.92)  Time: 0.814s, 1257.90/s  (0.791s, 1295.24/s)  LR: 1.299e-05  Data: 0.011 (0.011)
Train: 579 [1250/1251 (100%)]  Loss: 3.054 (2.93)  Time: 0.769s, 1331.19/s  (0.790s, 1295.83/s)  LR: 1.299e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.552 (1.552)  Loss:  0.6465 (0.6465)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.194 (0.565)  Loss:  0.7378 (1.0759)  Acc@1: 87.5000 (81.0300)  Acc@5: 98.3491 (95.5400)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-577.pth.tar', 81.07000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-579.pth.tar', 81.03)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-578.pth.tar', 81.02800005371094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-570.pth.tar', 81.00999997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-573.pth.tar', 80.98800005126954)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-566.pth.tar', 80.98399992431641)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-576.pth.tar', 80.95200010253906)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-568.pth.tar', 80.94000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-571.pth.tar', 80.91600018066406)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-567.pth.tar', 80.91200005126953)

Train: 580 [   0/1251 (  0%)]  Loss: 3.225 (3.23)  Time: 2.547s,  402.01/s  (2.547s,  402.01/s)  LR: 1.271e-05  Data: 1.810 (1.810)
Train: 580 [  50/1251 (  4%)]  Loss: 2.783 (3.00)  Time: 0.832s, 1230.19/s  (0.822s, 1245.45/s)  LR: 1.271e-05  Data: 0.010 (0.049)
Train: 580 [ 100/1251 (  8%)]  Loss: 2.898 (2.97)  Time: 0.783s, 1307.47/s  (0.811s, 1262.12/s)  LR: 1.271e-05  Data: 0.009 (0.030)
Train: 580 [ 150/1251 ( 12%)]  Loss: 3.053 (2.99)  Time: 0.784s, 1306.50/s  (0.800s, 1280.15/s)  LR: 1.271e-05  Data: 0.010 (0.023)
Train: 580 [ 200/1251 ( 16%)]  Loss: 3.036 (3.00)  Time: 0.770s, 1330.24/s  (0.797s, 1284.75/s)  LR: 1.271e-05  Data: 0.010 (0.020)
Train: 580 [ 250/1251 ( 20%)]  Loss: 3.036 (3.01)  Time: 0.772s, 1326.60/s  (0.795s, 1287.79/s)  LR: 1.271e-05  Data: 0.010 (0.018)
Train: 580 [ 300/1251 ( 24%)]  Loss: 2.931 (2.99)  Time: 0.826s, 1239.58/s  (0.794s, 1289.54/s)  LR: 1.271e-05  Data: 0.010 (0.017)
Train: 580 [ 350/1251 ( 28%)]  Loss: 2.876 (2.98)  Time: 0.808s, 1268.02/s  (0.792s, 1292.61/s)  LR: 1.271e-05  Data: 0.009 (0.016)
Train: 580 [ 400/1251 ( 32%)]  Loss: 2.792 (2.96)  Time: 0.815s, 1256.48/s  (0.795s, 1287.57/s)  LR: 1.271e-05  Data: 0.011 (0.015)
Train: 580 [ 450/1251 ( 36%)]  Loss: 2.861 (2.95)  Time: 0.771s, 1328.18/s  (0.795s, 1288.73/s)  LR: 1.271e-05  Data: 0.009 (0.014)
Train: 580 [ 500/1251 ( 40%)]  Loss: 2.702 (2.93)  Time: 0.771s, 1327.63/s  (0.794s, 1289.40/s)  LR: 1.271e-05  Data: 0.010 (0.014)
Train: 580 [ 550/1251 ( 44%)]  Loss: 2.679 (2.91)  Time: 0.774s, 1323.55/s  (0.793s, 1291.17/s)  LR: 1.271e-05  Data: 0.010 (0.014)
Train: 580 [ 600/1251 ( 48%)]  Loss: 2.812 (2.90)  Time: 0.776s, 1320.38/s  (0.792s, 1293.06/s)  LR: 1.271e-05  Data: 0.010 (0.013)
Train: 580 [ 650/1251 ( 52%)]  Loss: 2.872 (2.90)  Time: 0.862s, 1187.55/s  (0.792s, 1293.09/s)  LR: 1.271e-05  Data: 0.009 (0.013)
Train: 580 [ 700/1251 ( 56%)]  Loss: 2.898 (2.90)  Time: 0.880s, 1164.00/s  (0.792s, 1293.04/s)  LR: 1.271e-05  Data: 0.009 (0.013)
Train: 580 [ 750/1251 ( 60%)]  Loss: 3.115 (2.91)  Time: 0.777s, 1318.57/s  (0.791s, 1293.79/s)  LR: 1.271e-05  Data: 0.011 (0.013)
Train: 580 [ 800/1251 ( 64%)]  Loss: 2.705 (2.90)  Time: 0.856s, 1195.80/s  (0.792s, 1293.38/s)  LR: 1.271e-05  Data: 0.011 (0.013)
Train: 580 [ 850/1251 ( 68%)]  Loss: 3.059 (2.91)  Time: 0.783s, 1307.78/s  (0.791s, 1294.90/s)  LR: 1.271e-05  Data: 0.009 (0.012)
Train: 580 [ 900/1251 ( 72%)]  Loss: 2.863 (2.91)  Time: 0.776s, 1319.89/s  (0.791s, 1294.98/s)  LR: 1.271e-05  Data: 0.010 (0.012)
Train: 580 [ 950/1251 ( 76%)]  Loss: 2.967 (2.91)  Time: 0.775s, 1321.65/s  (0.790s, 1296.02/s)  LR: 1.271e-05  Data: 0.011 (0.012)
Train: 580 [1000/1251 ( 80%)]  Loss: 2.815 (2.90)  Time: 0.770s, 1330.12/s  (0.790s, 1296.62/s)  LR: 1.271e-05  Data: 0.010 (0.012)
Train: 580 [1050/1251 ( 84%)]  Loss: 2.994 (2.91)  Time: 0.772s, 1326.64/s  (0.790s, 1296.41/s)  LR: 1.271e-05  Data: 0.009 (0.012)
Train: 580 [1100/1251 ( 88%)]  Loss: 2.856 (2.91)  Time: 0.789s, 1297.62/s  (0.791s, 1295.35/s)  LR: 1.271e-05  Data: 0.012 (0.012)
Train: 580 [1150/1251 ( 92%)]  Loss: 3.009 (2.91)  Time: 0.834s, 1227.86/s  (0.791s, 1294.52/s)  LR: 1.271e-05  Data: 0.009 (0.012)
Train: 580 [1200/1251 ( 96%)]  Loss: 3.060 (2.92)  Time: 0.771s, 1327.34/s  (0.791s, 1294.72/s)  LR: 1.271e-05  Data: 0.010 (0.012)
Train: 580 [1250/1251 (100%)]  Loss: 2.977 (2.92)  Time: 0.794s, 1288.95/s  (0.791s, 1294.90/s)  LR: 1.271e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.552 (1.552)  Loss:  0.7056 (0.7056)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.7305 (98.7305)
Test: [  48/48]  Time: 0.194 (0.562)  Loss:  0.7891 (1.1390)  Acc@1: 87.1462 (80.8640)  Acc@5: 98.2311 (95.5140)
Train: 581 [   0/1251 (  0%)]  Loss: 3.250 (3.25)  Time: 2.224s,  460.35/s  (2.224s,  460.35/s)  LR: 1.245e-05  Data: 1.473 (1.473)
Train: 581 [  50/1251 (  4%)]  Loss: 2.706 (2.98)  Time: 0.815s, 1257.13/s  (0.825s, 1240.61/s)  LR: 1.245e-05  Data: 0.011 (0.042)
Train: 581 [ 100/1251 (  8%)]  Loss: 2.912 (2.96)  Time: 0.774s, 1322.19/s  (0.809s, 1266.41/s)  LR: 1.245e-05  Data: 0.010 (0.026)
Train: 581 [ 150/1251 ( 12%)]  Loss: 2.891 (2.94)  Time: 0.814s, 1257.32/s  (0.805s, 1272.34/s)  LR: 1.245e-05  Data: 0.011 (0.021)
Train: 581 [ 200/1251 ( 16%)]  Loss: 3.067 (2.97)  Time: 0.790s, 1296.28/s  (0.799s, 1281.23/s)  LR: 1.245e-05  Data: 0.009 (0.018)
Train: 581 [ 250/1251 ( 20%)]  Loss: 2.866 (2.95)  Time: 0.779s, 1314.02/s  (0.796s, 1286.44/s)  LR: 1.245e-05  Data: 0.009 (0.016)
Train: 581 [ 300/1251 ( 24%)]  Loss: 3.043 (2.96)  Time: 0.786s, 1302.94/s  (0.795s, 1288.41/s)  LR: 1.245e-05  Data: 0.010 (0.015)
Train: 581 [ 350/1251 ( 28%)]  Loss: 2.656 (2.92)  Time: 0.787s, 1300.79/s  (0.793s, 1291.31/s)  LR: 1.245e-05  Data: 0.010 (0.015)
Train: 581 [ 400/1251 ( 32%)]  Loss: 2.768 (2.91)  Time: 0.800s, 1280.39/s  (0.792s, 1293.12/s)  LR: 1.245e-05  Data: 0.011 (0.014)
Train: 581 [ 450/1251 ( 36%)]  Loss: 2.909 (2.91)  Time: 0.783s, 1307.13/s  (0.791s, 1293.85/s)  LR: 1.245e-05  Data: 0.010 (0.014)
Train: 581 [ 500/1251 ( 40%)]  Loss: 3.030 (2.92)  Time: 0.813s, 1258.86/s  (0.791s, 1294.31/s)  LR: 1.245e-05  Data: 0.011 (0.013)
Train: 581 [ 550/1251 ( 44%)]  Loss: 2.809 (2.91)  Time: 0.783s, 1307.97/s  (0.791s, 1295.21/s)  LR: 1.245e-05  Data: 0.010 (0.013)
Train: 581 [ 600/1251 ( 48%)]  Loss: 2.642 (2.89)  Time: 0.772s, 1326.24/s  (0.790s, 1296.84/s)  LR: 1.245e-05  Data: 0.010 (0.013)
Train: 581 [ 650/1251 ( 52%)]  Loss: 3.050 (2.90)  Time: 0.815s, 1256.40/s  (0.790s, 1295.65/s)  LR: 1.245e-05  Data: 0.011 (0.013)
Train: 581 [ 700/1251 ( 56%)]  Loss: 2.677 (2.89)  Time: 0.774s, 1323.65/s  (0.790s, 1295.66/s)  LR: 1.245e-05  Data: 0.010 (0.013)
Train: 581 [ 750/1251 ( 60%)]  Loss: 3.179 (2.90)  Time: 0.771s, 1328.36/s  (0.790s, 1296.45/s)  LR: 1.245e-05  Data: 0.009 (0.012)
Train: 581 [ 800/1251 ( 64%)]  Loss: 2.452 (2.88)  Time: 0.808s, 1267.26/s  (0.790s, 1295.39/s)  LR: 1.245e-05  Data: 0.009 (0.012)
Train: 581 [ 850/1251 ( 68%)]  Loss: 2.539 (2.86)  Time: 0.816s, 1254.79/s  (0.791s, 1294.03/s)  LR: 1.245e-05  Data: 0.011 (0.012)
Train: 581 [ 900/1251 ( 72%)]  Loss: 2.777 (2.85)  Time: 0.857s, 1195.41/s  (0.793s, 1291.54/s)  LR: 1.245e-05  Data: 0.010 (0.012)
Train: 581 [ 950/1251 ( 76%)]  Loss: 2.810 (2.85)  Time: 0.775s, 1320.46/s  (0.793s, 1291.74/s)  LR: 1.245e-05  Data: 0.010 (0.012)
Train: 581 [1000/1251 ( 80%)]  Loss: 2.968 (2.86)  Time: 0.782s, 1310.07/s  (0.793s, 1291.21/s)  LR: 1.245e-05  Data: 0.010 (0.012)
Train: 581 [1050/1251 ( 84%)]  Loss: 2.872 (2.86)  Time: 0.773s, 1325.36/s  (0.793s, 1291.08/s)  LR: 1.245e-05  Data: 0.009 (0.012)
Train: 581 [1100/1251 ( 88%)]  Loss: 2.942 (2.86)  Time: 0.775s, 1321.72/s  (0.793s, 1291.79/s)  LR: 1.245e-05  Data: 0.010 (0.012)
Train: 581 [1150/1251 ( 92%)]  Loss: 3.009 (2.87)  Time: 0.774s, 1323.07/s  (0.792s, 1293.00/s)  LR: 1.245e-05  Data: 0.010 (0.012)
Train: 581 [1200/1251 ( 96%)]  Loss: 2.818 (2.87)  Time: 0.783s, 1307.12/s  (0.792s, 1293.04/s)  LR: 1.245e-05  Data: 0.010 (0.011)
Train: 581 [1250/1251 (100%)]  Loss: 2.585 (2.85)  Time: 0.764s, 1340.63/s  (0.792s, 1293.72/s)  LR: 1.245e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.617 (1.617)  Loss:  0.6304 (0.6304)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.7305 (98.7305)
Test: [  48/48]  Time: 0.194 (0.561)  Loss:  0.7402 (1.0658)  Acc@1: 86.6745 (81.0720)  Acc@5: 98.2311 (95.5660)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-581.pth.tar', 81.07200002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-577.pth.tar', 81.07000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-579.pth.tar', 81.03)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-578.pth.tar', 81.02800005371094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-570.pth.tar', 81.00999997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-573.pth.tar', 80.98800005126954)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-566.pth.tar', 80.98399992431641)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-576.pth.tar', 80.95200010253906)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-568.pth.tar', 80.94000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-571.pth.tar', 80.91600018066406)

Train: 582 [   0/1251 (  0%)]  Loss: 3.170 (3.17)  Time: 2.258s,  453.43/s  (2.258s,  453.43/s)  LR: 1.220e-05  Data: 1.483 (1.483)
Train: 582 [  50/1251 (  4%)]  Loss: 2.953 (3.06)  Time: 0.815s, 1257.19/s  (0.841s, 1217.56/s)  LR: 1.220e-05  Data: 0.011 (0.043)
Train: 582 [ 100/1251 (  8%)]  Loss: 3.181 (3.10)  Time: 0.774s, 1322.94/s  (0.816s, 1254.34/s)  LR: 1.220e-05  Data: 0.010 (0.027)
Train: 582 [ 150/1251 ( 12%)]  Loss: 3.141 (3.11)  Time: 0.774s, 1322.24/s  (0.807s, 1269.03/s)  LR: 1.220e-05  Data: 0.012 (0.022)
Train: 582 [ 200/1251 ( 16%)]  Loss: 2.768 (3.04)  Time: 0.774s, 1323.04/s  (0.801s, 1278.61/s)  LR: 1.220e-05  Data: 0.010 (0.019)
Train: 582 [ 250/1251 ( 20%)]  Loss: 2.955 (3.03)  Time: 0.780s, 1312.49/s  (0.798s, 1283.58/s)  LR: 1.220e-05  Data: 0.009 (0.017)
Train: 582 [ 300/1251 ( 24%)]  Loss: 2.588 (2.97)  Time: 0.774s, 1323.71/s  (0.796s, 1285.72/s)  LR: 1.220e-05  Data: 0.010 (0.016)
Train: 582 [ 350/1251 ( 28%)]  Loss: 2.843 (2.95)  Time: 0.798s, 1282.95/s  (0.796s, 1286.20/s)  LR: 1.220e-05  Data: 0.014 (0.015)
Train: 582 [ 400/1251 ( 32%)]  Loss: 2.949 (2.95)  Time: 0.772s, 1326.03/s  (0.795s, 1287.66/s)  LR: 1.220e-05  Data: 0.009 (0.014)
Train: 582 [ 450/1251 ( 36%)]  Loss: 2.810 (2.94)  Time: 0.784s, 1306.87/s  (0.795s, 1288.08/s)  LR: 1.220e-05  Data: 0.010 (0.014)
Train: 582 [ 500/1251 ( 40%)]  Loss: 2.938 (2.94)  Time: 0.774s, 1323.10/s  (0.794s, 1290.05/s)  LR: 1.220e-05  Data: 0.010 (0.013)
Train: 582 [ 550/1251 ( 44%)]  Loss: 2.564 (2.90)  Time: 0.779s, 1314.21/s  (0.792s, 1292.32/s)  LR: 1.220e-05  Data: 0.010 (0.013)
Train: 582 [ 600/1251 ( 48%)]  Loss: 3.068 (2.92)  Time: 0.771s, 1327.44/s  (0.792s, 1293.70/s)  LR: 1.220e-05  Data: 0.009 (0.013)
Train: 582 [ 650/1251 ( 52%)]  Loss: 2.699 (2.90)  Time: 0.808s, 1266.68/s  (0.791s, 1294.13/s)  LR: 1.220e-05  Data: 0.010 (0.013)
Train: 582 [ 700/1251 ( 56%)]  Loss: 2.768 (2.89)  Time: 0.783s, 1307.76/s  (0.791s, 1294.83/s)  LR: 1.220e-05  Data: 0.010 (0.012)
Train: 582 [ 750/1251 ( 60%)]  Loss: 2.974 (2.90)  Time: 0.807s, 1269.38/s  (0.790s, 1295.98/s)  LR: 1.220e-05  Data: 0.011 (0.012)
Train: 582 [ 800/1251 ( 64%)]  Loss: 3.170 (2.91)  Time: 0.808s, 1267.09/s  (0.791s, 1295.15/s)  LR: 1.220e-05  Data: 0.010 (0.012)
Train: 582 [ 850/1251 ( 68%)]  Loss: 2.970 (2.92)  Time: 0.773s, 1324.91/s  (0.790s, 1296.27/s)  LR: 1.220e-05  Data: 0.010 (0.012)
Train: 582 [ 900/1251 ( 72%)]  Loss: 3.075 (2.93)  Time: 0.772s, 1325.89/s  (0.789s, 1297.46/s)  LR: 1.220e-05  Data: 0.010 (0.012)
Train: 582 [ 950/1251 ( 76%)]  Loss: 2.865 (2.92)  Time: 0.780s, 1312.63/s  (0.789s, 1298.02/s)  LR: 1.220e-05  Data: 0.013 (0.012)
Train: 582 [1000/1251 ( 80%)]  Loss: 3.142 (2.93)  Time: 0.773s, 1323.95/s  (0.789s, 1297.79/s)  LR: 1.220e-05  Data: 0.009 (0.012)
Train: 582 [1050/1251 ( 84%)]  Loss: 2.541 (2.91)  Time: 0.813s, 1258.81/s  (0.790s, 1296.66/s)  LR: 1.220e-05  Data: 0.012 (0.012)
Train: 582 [1100/1251 ( 88%)]  Loss: 2.595 (2.90)  Time: 0.772s, 1325.84/s  (0.790s, 1296.52/s)  LR: 1.220e-05  Data: 0.010 (0.011)
Train: 582 [1150/1251 ( 92%)]  Loss: 3.217 (2.91)  Time: 0.775s, 1321.68/s  (0.789s, 1297.03/s)  LR: 1.220e-05  Data: 0.010 (0.011)
Train: 582 [1200/1251 ( 96%)]  Loss: 2.791 (2.91)  Time: 0.815s, 1256.34/s  (0.790s, 1295.78/s)  LR: 1.220e-05  Data: 0.011 (0.011)
Train: 582 [1250/1251 (100%)]  Loss: 2.805 (2.91)  Time: 0.760s, 1347.63/s  (0.791s, 1294.19/s)  LR: 1.220e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.549 (1.549)  Loss:  0.6104 (0.6104)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.7305 (98.7305)
Test: [  48/48]  Time: 0.194 (0.562)  Loss:  0.7217 (1.0461)  Acc@1: 87.2642 (80.9840)  Acc@5: 98.2311 (95.5000)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-581.pth.tar', 81.07200002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-577.pth.tar', 81.07000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-579.pth.tar', 81.03)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-578.pth.tar', 81.02800005371094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-570.pth.tar', 81.00999997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-573.pth.tar', 80.98800005126954)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-582.pth.tar', 80.98400002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-566.pth.tar', 80.98399992431641)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-576.pth.tar', 80.95200010253906)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-568.pth.tar', 80.94000002685547)

Train: 583 [   0/1251 (  0%)]  Loss: 2.646 (2.65)  Time: 2.276s,  449.88/s  (2.276s,  449.88/s)  LR: 1.196e-05  Data: 1.548 (1.548)
Train: 583 [  50/1251 (  4%)]  Loss: 2.588 (2.62)  Time: 0.777s, 1317.23/s  (0.817s, 1252.73/s)  LR: 1.196e-05  Data: 0.011 (0.044)
Train: 583 [ 100/1251 (  8%)]  Loss: 3.264 (2.83)  Time: 0.772s, 1326.40/s  (0.807s, 1269.02/s)  LR: 1.196e-05  Data: 0.010 (0.027)
Train: 583 [ 150/1251 ( 12%)]  Loss: 3.235 (2.93)  Time: 0.774s, 1323.60/s  (0.800s, 1280.70/s)  LR: 1.196e-05  Data: 0.009 (0.021)
Train: 583 [ 200/1251 ( 16%)]  Loss: 2.834 (2.91)  Time: 0.773s, 1324.98/s  (0.797s, 1284.63/s)  LR: 1.196e-05  Data: 0.010 (0.019)
Train: 583 [ 250/1251 ( 20%)]  Loss: 2.861 (2.90)  Time: 0.773s, 1323.93/s  (0.794s, 1290.05/s)  LR: 1.196e-05  Data: 0.010 (0.017)
Train: 583 [ 300/1251 ( 24%)]  Loss: 2.753 (2.88)  Time: 0.771s, 1328.10/s  (0.794s, 1290.28/s)  LR: 1.196e-05  Data: 0.009 (0.016)
Train: 583 [ 350/1251 ( 28%)]  Loss: 2.825 (2.88)  Time: 0.773s, 1324.81/s  (0.792s, 1292.65/s)  LR: 1.196e-05  Data: 0.010 (0.015)
Train: 583 [ 400/1251 ( 32%)]  Loss: 2.857 (2.87)  Time: 0.778s, 1316.87/s  (0.792s, 1292.92/s)  LR: 1.196e-05  Data: 0.010 (0.014)
Train: 583 [ 450/1251 ( 36%)]  Loss: 2.737 (2.86)  Time: 0.772s, 1326.83/s  (0.791s, 1295.07/s)  LR: 1.196e-05  Data: 0.010 (0.014)
Train: 583 [ 500/1251 ( 40%)]  Loss: 2.901 (2.86)  Time: 0.772s, 1326.04/s  (0.791s, 1295.34/s)  LR: 1.196e-05  Data: 0.010 (0.013)
Train: 583 [ 550/1251 ( 44%)]  Loss: 2.961 (2.87)  Time: 0.773s, 1325.08/s  (0.790s, 1295.91/s)  LR: 1.196e-05  Data: 0.010 (0.013)
Train: 583 [ 600/1251 ( 48%)]  Loss: 2.994 (2.88)  Time: 0.773s, 1325.38/s  (0.790s, 1296.67/s)  LR: 1.196e-05  Data: 0.010 (0.013)
Train: 583 [ 650/1251 ( 52%)]  Loss: 2.646 (2.86)  Time: 0.782s, 1308.86/s  (0.790s, 1296.55/s)  LR: 1.196e-05  Data: 0.010 (0.013)
Train: 583 [ 700/1251 ( 56%)]  Loss: 2.942 (2.87)  Time: 0.793s, 1291.45/s  (0.789s, 1297.73/s)  LR: 1.196e-05  Data: 0.010 (0.012)
Train: 583 [ 750/1251 ( 60%)]  Loss: 3.016 (2.88)  Time: 0.771s, 1328.93/s  (0.789s, 1298.26/s)  LR: 1.196e-05  Data: 0.010 (0.012)
Train: 583 [ 800/1251 ( 64%)]  Loss: 2.702 (2.87)  Time: 0.771s, 1327.42/s  (0.788s, 1298.71/s)  LR: 1.196e-05  Data: 0.009 (0.012)
Train: 583 [ 850/1251 ( 68%)]  Loss: 2.951 (2.87)  Time: 0.773s, 1324.92/s  (0.788s, 1299.71/s)  LR: 1.196e-05  Data: 0.010 (0.012)
Train: 583 [ 900/1251 ( 72%)]  Loss: 2.929 (2.88)  Time: 0.812s, 1260.87/s  (0.787s, 1300.54/s)  LR: 1.196e-05  Data: 0.012 (0.012)
Train: 583 [ 950/1251 ( 76%)]  Loss: 2.896 (2.88)  Time: 0.772s, 1327.22/s  (0.787s, 1300.93/s)  LR: 1.196e-05  Data: 0.010 (0.012)
Train: 583 [1000/1251 ( 80%)]  Loss: 2.977 (2.88)  Time: 0.777s, 1318.71/s  (0.787s, 1301.71/s)  LR: 1.196e-05  Data: 0.010 (0.012)
Train: 583 [1050/1251 ( 84%)]  Loss: 3.240 (2.90)  Time: 0.773s, 1324.54/s  (0.787s, 1300.67/s)  LR: 1.196e-05  Data: 0.009 (0.012)
Train: 583 [1100/1251 ( 88%)]  Loss: 3.116 (2.91)  Time: 0.780s, 1312.41/s  (0.787s, 1300.75/s)  LR: 1.196e-05  Data: 0.011 (0.011)
Train: 583 [1150/1251 ( 92%)]  Loss: 2.837 (2.90)  Time: 0.870s, 1177.04/s  (0.788s, 1299.60/s)  LR: 1.196e-05  Data: 0.016 (0.011)
Train: 583 [1200/1251 ( 96%)]  Loss: 2.899 (2.90)  Time: 0.775s, 1322.11/s  (0.788s, 1299.52/s)  LR: 1.196e-05  Data: 0.009 (0.011)
Train: 583 [1250/1251 (100%)]  Loss: 3.059 (2.91)  Time: 0.758s, 1350.71/s  (0.789s, 1298.38/s)  LR: 1.196e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.567 (1.567)  Loss:  0.6519 (0.6519)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.7305 (98.7305)
Test: [  48/48]  Time: 0.194 (0.571)  Loss:  0.7583 (1.0896)  Acc@1: 87.3821 (80.9080)  Acc@5: 98.2311 (95.5460)
Train: 584 [   0/1251 (  0%)]  Loss: 2.838 (2.84)  Time: 2.266s,  451.88/s  (2.266s,  451.88/s)  LR: 1.174e-05  Data: 1.532 (1.532)
Train: 584 [  50/1251 (  4%)]  Loss: 3.210 (3.02)  Time: 0.772s, 1326.32/s  (0.817s, 1252.92/s)  LR: 1.174e-05  Data: 0.009 (0.043)
Train: 584 [ 100/1251 (  8%)]  Loss: 2.993 (3.01)  Time: 0.780s, 1312.87/s  (0.798s, 1283.61/s)  LR: 1.174e-05  Data: 0.010 (0.026)
Train: 584 [ 150/1251 ( 12%)]  Loss: 3.093 (3.03)  Time: 0.776s, 1320.15/s  (0.797s, 1285.60/s)  LR: 1.174e-05  Data: 0.009 (0.021)
Train: 584 [ 200/1251 ( 16%)]  Loss: 2.864 (3.00)  Time: 0.815s, 1256.95/s  (0.793s, 1291.65/s)  LR: 1.174e-05  Data: 0.009 (0.018)
Train: 584 [ 250/1251 ( 20%)]  Loss: 3.031 (3.00)  Time: 0.801s, 1278.98/s  (0.793s, 1290.74/s)  LR: 1.174e-05  Data: 0.009 (0.016)
Train: 584 [ 300/1251 ( 24%)]  Loss: 3.067 (3.01)  Time: 0.786s, 1302.49/s  (0.792s, 1292.12/s)  LR: 1.174e-05  Data: 0.010 (0.015)
Train: 584 [ 350/1251 ( 28%)]  Loss: 2.810 (2.99)  Time: 0.837s, 1223.13/s  (0.793s, 1290.82/s)  LR: 1.174e-05  Data: 0.013 (0.015)
Train: 584 [ 400/1251 ( 32%)]  Loss: 2.787 (2.97)  Time: 0.774s, 1322.32/s  (0.795s, 1288.86/s)  LR: 1.174e-05  Data: 0.009 (0.014)
Train: 584 [ 450/1251 ( 36%)]  Loss: 2.985 (2.97)  Time: 0.815s, 1256.80/s  (0.797s, 1285.37/s)  LR: 1.174e-05  Data: 0.011 (0.014)
Train: 584 [ 500/1251 ( 40%)]  Loss: 2.967 (2.97)  Time: 0.816s, 1254.45/s  (0.796s, 1286.12/s)  LR: 1.174e-05  Data: 0.009 (0.013)
Train: 584 [ 550/1251 ( 44%)]  Loss: 2.812 (2.95)  Time: 0.774s, 1323.63/s  (0.796s, 1285.71/s)  LR: 1.174e-05  Data: 0.009 (0.013)
Train: 584 [ 600/1251 ( 48%)]  Loss: 3.019 (2.96)  Time: 0.786s, 1303.29/s  (0.796s, 1286.51/s)  LR: 1.174e-05  Data: 0.010 (0.013)
Train: 584 [ 650/1251 ( 52%)]  Loss: 2.977 (2.96)  Time: 0.774s, 1322.83/s  (0.795s, 1287.70/s)  LR: 1.174e-05  Data: 0.009 (0.012)
Train: 584 [ 700/1251 ( 56%)]  Loss: 2.950 (2.96)  Time: 0.773s, 1324.50/s  (0.795s, 1287.93/s)  LR: 1.174e-05  Data: 0.010 (0.012)
Train: 584 [ 750/1251 ( 60%)]  Loss: 2.852 (2.95)  Time: 0.790s, 1295.55/s  (0.794s, 1289.43/s)  LR: 1.174e-05  Data: 0.009 (0.012)
Train: 584 [ 800/1251 ( 64%)]  Loss: 2.885 (2.95)  Time: 0.773s, 1324.65/s  (0.793s, 1290.85/s)  LR: 1.174e-05  Data: 0.009 (0.012)
Train: 584 [ 850/1251 ( 68%)]  Loss: 3.225 (2.96)  Time: 0.773s, 1324.77/s  (0.793s, 1292.06/s)  LR: 1.174e-05  Data: 0.009 (0.012)
Train: 584 [ 900/1251 ( 72%)]  Loss: 2.684 (2.95)  Time: 0.773s, 1324.17/s  (0.792s, 1292.49/s)  LR: 1.174e-05  Data: 0.010 (0.012)
Train: 584 [ 950/1251 ( 76%)]  Loss: 2.739 (2.94)  Time: 0.783s, 1308.35/s  (0.792s, 1293.43/s)  LR: 1.174e-05  Data: 0.010 (0.012)
Train: 584 [1000/1251 ( 80%)]  Loss: 2.771 (2.93)  Time: 0.771s, 1327.39/s  (0.791s, 1294.20/s)  LR: 1.174e-05  Data: 0.010 (0.011)
Train: 584 [1050/1251 ( 84%)]  Loss: 2.464 (2.91)  Time: 0.777s, 1318.09/s  (0.791s, 1294.24/s)  LR: 1.174e-05  Data: 0.010 (0.011)
Train: 584 [1100/1251 ( 88%)]  Loss: 2.954 (2.91)  Time: 0.815s, 1256.37/s  (0.792s, 1293.49/s)  LR: 1.174e-05  Data: 0.011 (0.011)
Train: 584 [1150/1251 ( 92%)]  Loss: 3.082 (2.92)  Time: 0.817s, 1254.08/s  (0.792s, 1292.30/s)  LR: 1.174e-05  Data: 0.010 (0.011)
Train: 584 [1200/1251 ( 96%)]  Loss: 2.714 (2.91)  Time: 0.772s, 1325.87/s  (0.793s, 1291.29/s)  LR: 1.174e-05  Data: 0.010 (0.011)
Train: 584 [1250/1251 (100%)]  Loss: 2.774 (2.91)  Time: 0.802s, 1276.91/s  (0.793s, 1291.55/s)  LR: 1.174e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.564 (1.564)  Loss:  0.6387 (0.6387)  Acc@1: 92.6758 (92.6758)  Acc@5: 98.8281 (98.8281)
Test: [  48/48]  Time: 0.194 (0.563)  Loss:  0.7500 (1.0652)  Acc@1: 87.7358 (81.0580)  Acc@5: 98.1132 (95.5620)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-581.pth.tar', 81.07200002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-577.pth.tar', 81.07000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-584.pth.tar', 81.05799997314453)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-579.pth.tar', 81.03)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-578.pth.tar', 81.02800005371094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-570.pth.tar', 81.00999997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-573.pth.tar', 80.98800005126954)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-582.pth.tar', 80.98400002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-566.pth.tar', 80.98399992431641)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-576.pth.tar', 80.95200010253906)

Train: 585 [   0/1251 (  0%)]  Loss: 2.839 (2.84)  Time: 2.252s,  454.71/s  (2.252s,  454.71/s)  LR: 1.153e-05  Data: 1.525 (1.525)
Train: 585 [  50/1251 (  4%)]  Loss: 2.515 (2.68)  Time: 0.806s, 1269.93/s  (0.816s, 1254.99/s)  LR: 1.153e-05  Data: 0.010 (0.044)
Train: 585 [ 100/1251 (  8%)]  Loss: 2.943 (2.77)  Time: 0.771s, 1327.55/s  (0.810s, 1264.73/s)  LR: 1.153e-05  Data: 0.010 (0.027)
Train: 585 [ 150/1251 ( 12%)]  Loss: 3.065 (2.84)  Time: 0.774s, 1322.66/s  (0.803s, 1275.24/s)  LR: 1.153e-05  Data: 0.009 (0.021)
Train: 585 [ 200/1251 ( 16%)]  Loss: 2.850 (2.84)  Time: 0.807s, 1268.43/s  (0.798s, 1283.10/s)  LR: 1.153e-05  Data: 0.009 (0.018)
Train: 585 [ 250/1251 ( 20%)]  Loss: 2.938 (2.86)  Time: 0.784s, 1306.25/s  (0.797s, 1284.42/s)  LR: 1.153e-05  Data: 0.009 (0.017)
Train: 585 [ 300/1251 ( 24%)]  Loss: 2.938 (2.87)  Time: 0.775s, 1320.79/s  (0.794s, 1289.40/s)  LR: 1.153e-05  Data: 0.010 (0.015)
Train: 585 [ 350/1251 ( 28%)]  Loss: 2.719 (2.85)  Time: 0.778s, 1316.74/s  (0.793s, 1290.89/s)  LR: 1.153e-05  Data: 0.009 (0.015)
Train: 585 [ 400/1251 ( 32%)]  Loss: 2.955 (2.86)  Time: 0.778s, 1315.71/s  (0.795s, 1288.80/s)  LR: 1.153e-05  Data: 0.009 (0.014)
Train: 585 [ 450/1251 ( 36%)]  Loss: 2.773 (2.85)  Time: 0.778s, 1316.71/s  (0.794s, 1289.88/s)  LR: 1.153e-05  Data: 0.010 (0.013)
Train: 585 [ 500/1251 ( 40%)]  Loss: 2.855 (2.85)  Time: 0.787s, 1301.21/s  (0.794s, 1289.67/s)  LR: 1.153e-05  Data: 0.010 (0.013)
Train: 585 [ 550/1251 ( 44%)]  Loss: 3.133 (2.88)  Time: 0.807s, 1269.04/s  (0.794s, 1289.00/s)  LR: 1.153e-05  Data: 0.010 (0.013)
Train: 585 [ 600/1251 ( 48%)]  Loss: 3.055 (2.89)  Time: 0.801s, 1278.41/s  (0.795s, 1288.16/s)  LR: 1.153e-05  Data: 0.009 (0.013)
Train: 585 [ 650/1251 ( 52%)]  Loss: 2.698 (2.88)  Time: 0.813s, 1259.11/s  (0.794s, 1289.00/s)  LR: 1.153e-05  Data: 0.009 (0.012)
Train: 585 [ 700/1251 ( 56%)]  Loss: 2.535 (2.85)  Time: 0.770s, 1330.59/s  (0.794s, 1288.96/s)  LR: 1.153e-05  Data: 0.009 (0.012)
Train: 585 [ 750/1251 ( 60%)]  Loss: 3.065 (2.87)  Time: 0.787s, 1301.90/s  (0.794s, 1289.63/s)  LR: 1.153e-05  Data: 0.009 (0.012)
Train: 585 [ 800/1251 ( 64%)]  Loss: 3.014 (2.88)  Time: 0.777s, 1317.93/s  (0.793s, 1290.96/s)  LR: 1.153e-05  Data: 0.010 (0.012)
Train: 585 [ 850/1251 ( 68%)]  Loss: 3.229 (2.90)  Time: 0.774s, 1323.45/s  (0.793s, 1292.07/s)  LR: 1.153e-05  Data: 0.009 (0.012)
Train: 585 [ 900/1251 ( 72%)]  Loss: 3.155 (2.91)  Time: 0.853s, 1201.16/s  (0.792s, 1292.67/s)  LR: 1.153e-05  Data: 0.011 (0.012)
Train: 585 [ 950/1251 ( 76%)]  Loss: 2.801 (2.90)  Time: 0.772s, 1326.64/s  (0.792s, 1293.65/s)  LR: 1.153e-05  Data: 0.009 (0.011)
Train: 585 [1000/1251 ( 80%)]  Loss: 3.047 (2.91)  Time: 0.825s, 1241.57/s  (0.793s, 1291.95/s)  LR: 1.153e-05  Data: 0.012 (0.011)
Train: 585 [1050/1251 ( 84%)]  Loss: 2.819 (2.91)  Time: 0.784s, 1306.72/s  (0.792s, 1292.73/s)  LR: 1.153e-05  Data: 0.010 (0.011)
Train: 585 [1100/1251 ( 88%)]  Loss: 2.886 (2.91)  Time: 0.776s, 1319.65/s  (0.792s, 1293.49/s)  LR: 1.153e-05  Data: 0.010 (0.011)
Train: 585 [1150/1251 ( 92%)]  Loss: 2.798 (2.90)  Time: 0.787s, 1300.43/s  (0.791s, 1294.11/s)  LR: 1.153e-05  Data: 0.010 (0.011)
Train: 585 [1200/1251 ( 96%)]  Loss: 2.891 (2.90)  Time: 0.891s, 1149.08/s  (0.792s, 1292.81/s)  LR: 1.153e-05  Data: 0.009 (0.011)
Train: 585 [1250/1251 (100%)]  Loss: 3.146 (2.91)  Time: 0.811s, 1261.90/s  (0.792s, 1293.00/s)  LR: 1.153e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.564 (1.564)  Loss:  0.5859 (0.5859)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.193 (0.569)  Loss:  0.6982 (1.0234)  Acc@1: 87.0283 (81.0860)  Acc@5: 97.9953 (95.5740)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-585.pth.tar', 81.08600005371093)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-581.pth.tar', 81.07200002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-577.pth.tar', 81.07000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-584.pth.tar', 81.05799997314453)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-579.pth.tar', 81.03)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-578.pth.tar', 81.02800005371094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-570.pth.tar', 81.00999997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-573.pth.tar', 80.98800005126954)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-582.pth.tar', 80.98400002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-566.pth.tar', 80.98399992431641)

Train: 586 [   0/1251 (  0%)]  Loss: 2.643 (2.64)  Time: 2.274s,  450.23/s  (2.274s,  450.23/s)  LR: 1.133e-05  Data: 1.544 (1.544)
Train: 586 [  50/1251 (  4%)]  Loss: 2.738 (2.69)  Time: 0.773s, 1325.36/s  (0.822s, 1245.79/s)  LR: 1.133e-05  Data: 0.009 (0.043)
Train: 586 [ 100/1251 (  8%)]  Loss: 3.008 (2.80)  Time: 0.775s, 1320.80/s  (0.805s, 1272.46/s)  LR: 1.133e-05  Data: 0.010 (0.027)
Train: 586 [ 150/1251 ( 12%)]  Loss: 2.893 (2.82)  Time: 0.845s, 1211.85/s  (0.801s, 1278.62/s)  LR: 1.133e-05  Data: 0.010 (0.021)
Train: 586 [ 200/1251 ( 16%)]  Loss: 2.896 (2.84)  Time: 0.792s, 1293.10/s  (0.796s, 1286.13/s)  LR: 1.133e-05  Data: 0.010 (0.018)
Train: 586 [ 250/1251 ( 20%)]  Loss: 3.105 (2.88)  Time: 0.782s, 1310.11/s  (0.794s, 1289.20/s)  LR: 1.133e-05  Data: 0.010 (0.017)
Train: 586 [ 300/1251 ( 24%)]  Loss: 2.706 (2.86)  Time: 0.780s, 1313.60/s  (0.793s, 1291.97/s)  LR: 1.133e-05  Data: 0.009 (0.015)
Train: 586 [ 350/1251 ( 28%)]  Loss: 2.905 (2.86)  Time: 0.779s, 1314.82/s  (0.791s, 1293.99/s)  LR: 1.133e-05  Data: 0.009 (0.015)
Train: 586 [ 400/1251 ( 32%)]  Loss: 2.933 (2.87)  Time: 0.808s, 1267.01/s  (0.792s, 1292.35/s)  LR: 1.133e-05  Data: 0.010 (0.014)
Train: 586 [ 450/1251 ( 36%)]  Loss: 2.970 (2.88)  Time: 0.771s, 1327.86/s  (0.792s, 1293.74/s)  LR: 1.133e-05  Data: 0.010 (0.014)
Train: 586 [ 500/1251 ( 40%)]  Loss: 2.949 (2.89)  Time: 0.773s, 1324.83/s  (0.792s, 1293.63/s)  LR: 1.133e-05  Data: 0.009 (0.013)
Train: 586 [ 550/1251 ( 44%)]  Loss: 2.808 (2.88)  Time: 0.782s, 1309.69/s  (0.792s, 1292.31/s)  LR: 1.133e-05  Data: 0.009 (0.013)
Train: 586 [ 600/1251 ( 48%)]  Loss: 2.538 (2.85)  Time: 0.771s, 1327.98/s  (0.791s, 1293.96/s)  LR: 1.133e-05  Data: 0.010 (0.013)
Train: 586 [ 650/1251 ( 52%)]  Loss: 3.188 (2.88)  Time: 0.776s, 1319.91/s  (0.791s, 1294.64/s)  LR: 1.133e-05  Data: 0.010 (0.012)
Train: 586 [ 700/1251 ( 56%)]  Loss: 3.081 (2.89)  Time: 0.805s, 1272.49/s  (0.791s, 1294.64/s)  LR: 1.133e-05  Data: 0.009 (0.012)
Train: 586 [ 750/1251 ( 60%)]  Loss: 2.613 (2.87)  Time: 0.791s, 1294.12/s  (0.791s, 1293.94/s)  LR: 1.133e-05  Data: 0.014 (0.012)
Train: 586 [ 800/1251 ( 64%)]  Loss: 2.987 (2.88)  Time: 0.775s, 1320.73/s  (0.791s, 1293.97/s)  LR: 1.133e-05  Data: 0.011 (0.012)
Train: 586 [ 850/1251 ( 68%)]  Loss: 2.866 (2.88)  Time: 0.785s, 1304.65/s  (0.791s, 1294.40/s)  LR: 1.133e-05  Data: 0.009 (0.012)
Train: 586 [ 900/1251 ( 72%)]  Loss: 2.936 (2.88)  Time: 0.772s, 1326.85/s  (0.791s, 1294.57/s)  LR: 1.133e-05  Data: 0.010 (0.012)
Train: 586 [ 950/1251 ( 76%)]  Loss: 3.193 (2.90)  Time: 0.773s, 1325.07/s  (0.791s, 1294.68/s)  LR: 1.133e-05  Data: 0.009 (0.012)
Train: 586 [1000/1251 ( 80%)]  Loss: 3.061 (2.91)  Time: 0.773s, 1324.89/s  (0.791s, 1294.94/s)  LR: 1.133e-05  Data: 0.011 (0.011)
Train: 586 [1050/1251 ( 84%)]  Loss: 2.767 (2.90)  Time: 0.773s, 1325.42/s  (0.791s, 1295.38/s)  LR: 1.133e-05  Data: 0.009 (0.011)
Train: 586 [1100/1251 ( 88%)]  Loss: 2.881 (2.90)  Time: 0.822s, 1245.09/s  (0.791s, 1294.12/s)  LR: 1.133e-05  Data: 0.010 (0.011)
Train: 586 [1150/1251 ( 92%)]  Loss: 3.025 (2.90)  Time: 0.820s, 1248.70/s  (0.792s, 1293.72/s)  LR: 1.133e-05  Data: 0.012 (0.011)
Train: 586 [1200/1251 ( 96%)]  Loss: 2.974 (2.91)  Time: 0.784s, 1305.74/s  (0.792s, 1292.41/s)  LR: 1.133e-05  Data: 0.010 (0.011)
Train: 586 [1250/1251 (100%)]  Loss: 3.173 (2.92)  Time: 0.770s, 1330.50/s  (0.792s, 1292.67/s)  LR: 1.133e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.598 (1.598)  Loss:  0.6992 (0.6992)  Acc@1: 92.6758 (92.6758)  Acc@5: 98.7305 (98.7305)
Test: [  48/48]  Time: 0.194 (0.569)  Loss:  0.7915 (1.1386)  Acc@1: 87.1462 (80.8460)  Acc@5: 98.2311 (95.4380)
Train: 587 [   0/1251 (  0%)]  Loss: 2.616 (2.62)  Time: 2.396s,  427.38/s  (2.396s,  427.38/s)  LR: 1.115e-05  Data: 1.663 (1.663)
Train: 587 [  50/1251 (  4%)]  Loss: 3.127 (2.87)  Time: 0.773s, 1325.17/s  (0.820s, 1248.26/s)  LR: 1.115e-05  Data: 0.010 (0.048)
Train: 587 [ 100/1251 (  8%)]  Loss: 2.991 (2.91)  Time: 0.772s, 1327.14/s  (0.810s, 1264.83/s)  LR: 1.115e-05  Data: 0.009 (0.029)
Train: 587 [ 150/1251 ( 12%)]  Loss: 2.603 (2.83)  Time: 0.777s, 1318.24/s  (0.801s, 1278.62/s)  LR: 1.115e-05  Data: 0.010 (0.023)
Train: 587 [ 200/1251 ( 16%)]  Loss: 2.997 (2.87)  Time: 0.789s, 1297.90/s  (0.798s, 1283.03/s)  LR: 1.115e-05  Data: 0.013 (0.020)
Train: 587 [ 250/1251 ( 20%)]  Loss: 2.983 (2.89)  Time: 0.789s, 1297.87/s  (0.795s, 1288.75/s)  LR: 1.115e-05  Data: 0.010 (0.018)
Train: 587 [ 300/1251 ( 24%)]  Loss: 2.951 (2.90)  Time: 0.805s, 1271.65/s  (0.794s, 1289.67/s)  LR: 1.115e-05  Data: 0.010 (0.016)
Train: 587 [ 350/1251 ( 28%)]  Loss: 3.028 (2.91)  Time: 0.814s, 1257.85/s  (0.793s, 1291.89/s)  LR: 1.115e-05  Data: 0.011 (0.015)
Train: 587 [ 400/1251 ( 32%)]  Loss: 2.869 (2.91)  Time: 0.772s, 1327.03/s  (0.791s, 1295.12/s)  LR: 1.115e-05  Data: 0.010 (0.015)
Train: 587 [ 450/1251 ( 36%)]  Loss: 2.949 (2.91)  Time: 0.816s, 1255.25/s  (0.790s, 1296.39/s)  LR: 1.115e-05  Data: 0.009 (0.014)
Train: 587 [ 500/1251 ( 40%)]  Loss: 3.147 (2.93)  Time: 0.775s, 1320.68/s  (0.791s, 1294.68/s)  LR: 1.115e-05  Data: 0.010 (0.014)
Train: 587 [ 550/1251 ( 44%)]  Loss: 2.951 (2.93)  Time: 0.769s, 1332.08/s  (0.791s, 1295.08/s)  LR: 1.115e-05  Data: 0.010 (0.013)
Train: 587 [ 600/1251 ( 48%)]  Loss: 2.934 (2.93)  Time: 0.771s, 1328.92/s  (0.792s, 1293.29/s)  LR: 1.115e-05  Data: 0.010 (0.013)
Train: 587 [ 650/1251 ( 52%)]  Loss: 3.000 (2.94)  Time: 0.772s, 1326.51/s  (0.791s, 1294.69/s)  LR: 1.115e-05  Data: 0.010 (0.013)
Train: 587 [ 700/1251 ( 56%)]  Loss: 2.618 (2.92)  Time: 0.773s, 1325.10/s  (0.791s, 1295.16/s)  LR: 1.115e-05  Data: 0.010 (0.013)
Train: 587 [ 750/1251 ( 60%)]  Loss: 3.061 (2.93)  Time: 0.773s, 1324.82/s  (0.791s, 1294.53/s)  LR: 1.115e-05  Data: 0.010 (0.013)
Train: 587 [ 800/1251 ( 64%)]  Loss: 2.635 (2.91)  Time: 0.829s, 1235.19/s  (0.791s, 1294.74/s)  LR: 1.115e-05  Data: 0.010 (0.013)
Train: 587 [ 850/1251 ( 68%)]  Loss: 3.040 (2.92)  Time: 0.772s, 1326.23/s  (0.791s, 1294.82/s)  LR: 1.115e-05  Data: 0.010 (0.012)
Train: 587 [ 900/1251 ( 72%)]  Loss: 2.719 (2.91)  Time: 0.807s, 1268.52/s  (0.790s, 1296.03/s)  LR: 1.115e-05  Data: 0.010 (0.012)
Train: 587 [ 950/1251 ( 76%)]  Loss: 2.746 (2.90)  Time: 0.774s, 1322.59/s  (0.789s, 1297.03/s)  LR: 1.115e-05  Data: 0.011 (0.012)
Train: 587 [1000/1251 ( 80%)]  Loss: 2.976 (2.90)  Time: 0.780s, 1312.97/s  (0.789s, 1297.90/s)  LR: 1.115e-05  Data: 0.011 (0.012)
Train: 587 [1050/1251 ( 84%)]  Loss: 2.889 (2.90)  Time: 0.772s, 1327.01/s  (0.789s, 1297.28/s)  LR: 1.115e-05  Data: 0.010 (0.012)
Train: 587 [1100/1251 ( 88%)]  Loss: 3.098 (2.91)  Time: 0.772s, 1325.78/s  (0.789s, 1297.88/s)  LR: 1.115e-05  Data: 0.010 (0.012)
Train: 587 [1150/1251 ( 92%)]  Loss: 2.989 (2.91)  Time: 0.781s, 1310.96/s  (0.788s, 1298.77/s)  LR: 1.115e-05  Data: 0.010 (0.012)
Train: 587 [1200/1251 ( 96%)]  Loss: 3.245 (2.93)  Time: 0.779s, 1314.60/s  (0.788s, 1299.28/s)  LR: 1.115e-05  Data: 0.009 (0.012)
Train: 587 [1250/1251 (100%)]  Loss: 3.045 (2.93)  Time: 0.849s, 1206.66/s  (0.788s, 1299.07/s)  LR: 1.115e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.591 (1.591)  Loss:  0.6831 (0.6831)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.194 (0.571)  Loss:  0.7812 (1.1235)  Acc@1: 87.1462 (80.8540)  Acc@5: 98.4670 (95.5500)
Train: 588 [   0/1251 (  0%)]  Loss: 2.862 (2.86)  Time: 2.296s,  445.98/s  (2.296s,  445.98/s)  LR: 1.098e-05  Data: 1.569 (1.569)
Train: 588 [  50/1251 (  4%)]  Loss: 2.967 (2.91)  Time: 0.778s, 1316.13/s  (0.817s, 1253.37/s)  LR: 1.098e-05  Data: 0.010 (0.046)
Train: 588 [ 100/1251 (  8%)]  Loss: 2.690 (2.84)  Time: 0.855s, 1197.33/s  (0.798s, 1282.64/s)  LR: 1.098e-05  Data: 0.011 (0.028)
Train: 588 [ 150/1251 ( 12%)]  Loss: 2.901 (2.86)  Time: 0.786s, 1302.89/s  (0.793s, 1291.19/s)  LR: 1.098e-05  Data: 0.010 (0.022)
Train: 588 [ 200/1251 ( 16%)]  Loss: 2.893 (2.86)  Time: 0.809s, 1265.80/s  (0.794s, 1289.31/s)  LR: 1.098e-05  Data: 0.010 (0.019)
Train: 588 [ 250/1251 ( 20%)]  Loss: 2.915 (2.87)  Time: 0.771s, 1328.64/s  (0.791s, 1293.99/s)  LR: 1.098e-05  Data: 0.010 (0.018)
Train: 588 [ 300/1251 ( 24%)]  Loss: 2.604 (2.83)  Time: 0.774s, 1323.67/s  (0.791s, 1294.78/s)  LR: 1.098e-05  Data: 0.010 (0.016)
Train: 588 [ 350/1251 ( 28%)]  Loss: 2.859 (2.84)  Time: 0.772s, 1325.69/s  (0.792s, 1293.54/s)  LR: 1.098e-05  Data: 0.009 (0.015)
Train: 588 [ 400/1251 ( 32%)]  Loss: 2.889 (2.84)  Time: 0.822s, 1246.15/s  (0.791s, 1294.16/s)  LR: 1.098e-05  Data: 0.009 (0.015)
Train: 588 [ 450/1251 ( 36%)]  Loss: 2.948 (2.85)  Time: 0.779s, 1314.97/s  (0.790s, 1296.08/s)  LR: 1.098e-05  Data: 0.012 (0.014)
Train: 588 [ 500/1251 ( 40%)]  Loss: 2.771 (2.85)  Time: 0.782s, 1310.23/s  (0.789s, 1297.06/s)  LR: 1.098e-05  Data: 0.012 (0.014)
Train: 588 [ 550/1251 ( 44%)]  Loss: 2.479 (2.82)  Time: 0.773s, 1325.20/s  (0.789s, 1298.28/s)  LR: 1.098e-05  Data: 0.009 (0.013)
Train: 588 [ 600/1251 ( 48%)]  Loss: 3.083 (2.84)  Time: 0.771s, 1327.61/s  (0.788s, 1299.20/s)  LR: 1.098e-05  Data: 0.009 (0.013)
Train: 588 [ 650/1251 ( 52%)]  Loss: 2.901 (2.84)  Time: 0.810s, 1263.77/s  (0.788s, 1299.08/s)  LR: 1.098e-05  Data: 0.010 (0.013)
Train: 588 [ 700/1251 ( 56%)]  Loss: 2.953 (2.85)  Time: 0.773s, 1324.77/s  (0.789s, 1298.09/s)  LR: 1.098e-05  Data: 0.010 (0.013)
Train: 588 [ 750/1251 ( 60%)]  Loss: 3.109 (2.86)  Time: 0.774s, 1322.23/s  (0.789s, 1297.61/s)  LR: 1.098e-05  Data: 0.010 (0.012)
Train: 588 [ 800/1251 ( 64%)]  Loss: 3.124 (2.88)  Time: 0.773s, 1325.02/s  (0.789s, 1298.46/s)  LR: 1.098e-05  Data: 0.010 (0.012)
Train: 588 [ 850/1251 ( 68%)]  Loss: 2.773 (2.87)  Time: 0.771s, 1328.30/s  (0.790s, 1296.97/s)  LR: 1.098e-05  Data: 0.009 (0.012)
Train: 588 [ 900/1251 ( 72%)]  Loss: 2.666 (2.86)  Time: 0.785s, 1305.05/s  (0.790s, 1296.87/s)  LR: 1.098e-05  Data: 0.009 (0.012)
Train: 588 [ 950/1251 ( 76%)]  Loss: 2.866 (2.86)  Time: 0.781s, 1311.46/s  (0.791s, 1295.27/s)  LR: 1.098e-05  Data: 0.013 (0.012)
Train: 588 [1000/1251 ( 80%)]  Loss: 2.922 (2.87)  Time: 0.780s, 1312.57/s  (0.790s, 1295.73/s)  LR: 1.098e-05  Data: 0.009 (0.012)
Train: 588 [1050/1251 ( 84%)]  Loss: 3.041 (2.87)  Time: 0.772s, 1325.77/s  (0.790s, 1296.54/s)  LR: 1.098e-05  Data: 0.009 (0.012)
Train: 588 [1100/1251 ( 88%)]  Loss: 2.899 (2.87)  Time: 0.777s, 1317.51/s  (0.789s, 1297.18/s)  LR: 1.098e-05  Data: 0.013 (0.012)
Train: 588 [1150/1251 ( 92%)]  Loss: 2.955 (2.88)  Time: 0.774s, 1323.17/s  (0.789s, 1297.94/s)  LR: 1.098e-05  Data: 0.009 (0.012)
Train: 588 [1200/1251 ( 96%)]  Loss: 2.944 (2.88)  Time: 0.772s, 1327.20/s  (0.790s, 1296.93/s)  LR: 1.098e-05  Data: 0.009 (0.011)
Train: 588 [1250/1251 (100%)]  Loss: 2.476 (2.87)  Time: 0.795s, 1287.66/s  (0.789s, 1297.22/s)  LR: 1.098e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.555 (1.555)  Loss:  0.6104 (0.6104)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.193 (0.575)  Loss:  0.7241 (1.0446)  Acc@1: 87.0283 (81.0220)  Acc@5: 98.1132 (95.5340)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-585.pth.tar', 81.08600005371093)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-581.pth.tar', 81.07200002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-577.pth.tar', 81.07000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-584.pth.tar', 81.05799997314453)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-579.pth.tar', 81.03)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-578.pth.tar', 81.02800005371094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-588.pth.tar', 81.02200005371094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-570.pth.tar', 81.00999997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-573.pth.tar', 80.98800005126954)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-582.pth.tar', 80.98400002685547)

Train: 589 [   0/1251 (  0%)]  Loss: 3.088 (3.09)  Time: 2.196s,  466.33/s  (2.196s,  466.33/s)  LR: 1.082e-05  Data: 1.462 (1.462)
Train: 589 [  50/1251 (  4%)]  Loss: 2.840 (2.96)  Time: 0.814s, 1258.09/s  (0.827s, 1238.84/s)  LR: 1.082e-05  Data: 0.010 (0.044)
Train: 589 [ 100/1251 (  8%)]  Loss: 3.018 (2.98)  Time: 0.817s, 1253.91/s  (0.818s, 1251.53/s)  LR: 1.082e-05  Data: 0.012 (0.028)
Train: 589 [ 150/1251 ( 12%)]  Loss: 2.761 (2.93)  Time: 0.811s, 1262.32/s  (0.808s, 1267.78/s)  LR: 1.082e-05  Data: 0.010 (0.022)
Train: 589 [ 200/1251 ( 16%)]  Loss: 2.701 (2.88)  Time: 0.773s, 1324.29/s  (0.803s, 1274.58/s)  LR: 1.082e-05  Data: 0.010 (0.019)
Train: 589 [ 250/1251 ( 20%)]  Loss: 2.582 (2.83)  Time: 0.772s, 1325.87/s  (0.798s, 1283.65/s)  LR: 1.082e-05  Data: 0.010 (0.017)
Train: 589 [ 300/1251 ( 24%)]  Loss: 2.778 (2.82)  Time: 0.809s, 1266.24/s  (0.795s, 1287.79/s)  LR: 1.082e-05  Data: 0.009 (0.016)
Train: 589 [ 350/1251 ( 28%)]  Loss: 2.753 (2.82)  Time: 0.773s, 1323.94/s  (0.796s, 1287.11/s)  LR: 1.082e-05  Data: 0.010 (0.015)
Train: 589 [ 400/1251 ( 32%)]  Loss: 2.718 (2.80)  Time: 0.807s, 1269.56/s  (0.794s, 1289.39/s)  LR: 1.082e-05  Data: 0.010 (0.014)
Train: 589 [ 450/1251 ( 36%)]  Loss: 3.018 (2.83)  Time: 0.774s, 1323.19/s  (0.793s, 1291.04/s)  LR: 1.082e-05  Data: 0.009 (0.014)
Train: 589 [ 500/1251 ( 40%)]  Loss: 3.029 (2.84)  Time: 0.773s, 1324.51/s  (0.793s, 1291.68/s)  LR: 1.082e-05  Data: 0.010 (0.014)
Train: 589 [ 550/1251 ( 44%)]  Loss: 2.958 (2.85)  Time: 0.783s, 1306.98/s  (0.794s, 1290.13/s)  LR: 1.082e-05  Data: 0.010 (0.013)
Train: 589 [ 600/1251 ( 48%)]  Loss: 2.824 (2.85)  Time: 0.817s, 1252.61/s  (0.794s, 1289.84/s)  LR: 1.082e-05  Data: 0.012 (0.013)
Train: 589 [ 650/1251 ( 52%)]  Loss: 3.099 (2.87)  Time: 0.773s, 1325.15/s  (0.795s, 1288.35/s)  LR: 1.082e-05  Data: 0.010 (0.013)
Train: 589 [ 700/1251 ( 56%)]  Loss: 2.912 (2.87)  Time: 0.821s, 1246.84/s  (0.794s, 1289.58/s)  LR: 1.082e-05  Data: 0.012 (0.013)
Train: 589 [ 750/1251 ( 60%)]  Loss: 2.835 (2.87)  Time: 0.783s, 1308.00/s  (0.795s, 1288.11/s)  LR: 1.082e-05  Data: 0.010 (0.012)
Train: 589 [ 800/1251 ( 64%)]  Loss: 2.729 (2.86)  Time: 0.806s, 1271.01/s  (0.795s, 1288.09/s)  LR: 1.082e-05  Data: 0.009 (0.012)
Train: 589 [ 850/1251 ( 68%)]  Loss: 2.901 (2.86)  Time: 0.801s, 1278.91/s  (0.795s, 1288.66/s)  LR: 1.082e-05  Data: 0.009 (0.012)
Train: 589 [ 900/1251 ( 72%)]  Loss: 2.841 (2.86)  Time: 0.858s, 1194.00/s  (0.794s, 1290.09/s)  LR: 1.082e-05  Data: 0.010 (0.012)
Train: 589 [ 950/1251 ( 76%)]  Loss: 2.910 (2.86)  Time: 0.820s, 1249.40/s  (0.794s, 1289.77/s)  LR: 1.082e-05  Data: 0.015 (0.012)
Train: 589 [1000/1251 ( 80%)]  Loss: 3.168 (2.88)  Time: 0.809s, 1266.13/s  (0.794s, 1289.94/s)  LR: 1.082e-05  Data: 0.009 (0.012)
Train: 589 [1050/1251 ( 84%)]  Loss: 2.676 (2.87)  Time: 0.775s, 1321.19/s  (0.793s, 1291.06/s)  LR: 1.082e-05  Data: 0.010 (0.012)
Train: 589 [1100/1251 ( 88%)]  Loss: 2.890 (2.87)  Time: 0.777s, 1318.60/s  (0.793s, 1291.98/s)  LR: 1.082e-05  Data: 0.009 (0.012)
Train: 589 [1150/1251 ( 92%)]  Loss: 2.719 (2.86)  Time: 0.769s, 1331.33/s  (0.792s, 1292.16/s)  LR: 1.082e-05  Data: 0.009 (0.012)
Train: 589 [1200/1251 ( 96%)]  Loss: 2.244 (2.84)  Time: 0.861s, 1188.66/s  (0.793s, 1291.29/s)  LR: 1.082e-05  Data: 0.009 (0.012)
Train: 589 [1250/1251 (100%)]  Loss: 2.969 (2.84)  Time: 0.770s, 1329.84/s  (0.793s, 1290.65/s)  LR: 1.082e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.578 (1.578)  Loss:  0.6406 (0.6406)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.193 (0.575)  Loss:  0.7412 (1.0660)  Acc@1: 87.1462 (80.8680)  Acc@5: 98.2311 (95.5200)
Train: 590 [   0/1251 (  0%)]  Loss: 3.180 (3.18)  Time: 2.450s,  417.93/s  (2.450s,  417.93/s)  LR: 1.068e-05  Data: 1.729 (1.729)
Train: 590 [  50/1251 (  4%)]  Loss: 3.040 (3.11)  Time: 0.807s, 1269.03/s  (0.821s, 1246.56/s)  LR: 1.068e-05  Data: 0.009 (0.044)
Train: 590 [ 100/1251 (  8%)]  Loss: 3.127 (3.12)  Time: 0.793s, 1291.82/s  (0.809s, 1265.45/s)  LR: 1.068e-05  Data: 0.009 (0.027)
Train: 590 [ 150/1251 ( 12%)]  Loss: 3.041 (3.10)  Time: 0.772s, 1325.98/s  (0.800s, 1279.66/s)  LR: 1.068e-05  Data: 0.010 (0.021)
Train: 590 [ 200/1251 ( 16%)]  Loss: 3.166 (3.11)  Time: 0.804s, 1273.44/s  (0.798s, 1282.41/s)  LR: 1.068e-05  Data: 0.009 (0.018)
Train: 590 [ 250/1251 ( 20%)]  Loss: 3.002 (3.09)  Time: 0.774s, 1323.13/s  (0.799s, 1281.27/s)  LR: 1.068e-05  Data: 0.009 (0.017)
Train: 590 [ 300/1251 ( 24%)]  Loss: 2.814 (3.05)  Time: 0.769s, 1331.45/s  (0.797s, 1284.35/s)  LR: 1.068e-05  Data: 0.010 (0.016)
Train: 590 [ 350/1251 ( 28%)]  Loss: 2.995 (3.05)  Time: 0.815s, 1256.83/s  (0.795s, 1288.47/s)  LR: 1.068e-05  Data: 0.010 (0.015)
Train: 590 [ 400/1251 ( 32%)]  Loss: 3.161 (3.06)  Time: 0.773s, 1324.87/s  (0.793s, 1290.78/s)  LR: 1.068e-05  Data: 0.010 (0.014)
Train: 590 [ 450/1251 ( 36%)]  Loss: 2.935 (3.05)  Time: 0.781s, 1310.73/s  (0.792s, 1293.53/s)  LR: 1.068e-05  Data: 0.010 (0.014)
Train: 590 [ 500/1251 ( 40%)]  Loss: 2.922 (3.03)  Time: 0.771s, 1327.80/s  (0.791s, 1294.70/s)  LR: 1.068e-05  Data: 0.010 (0.013)
Train: 590 [ 550/1251 ( 44%)]  Loss: 2.780 (3.01)  Time: 0.819s, 1250.19/s  (0.791s, 1294.56/s)  LR: 1.068e-05  Data: 0.009 (0.013)
Train: 590 [ 600/1251 ( 48%)]  Loss: 2.574 (2.98)  Time: 0.777s, 1318.38/s  (0.792s, 1292.60/s)  LR: 1.068e-05  Data: 0.010 (0.013)
Train: 590 [ 650/1251 ( 52%)]  Loss: 2.934 (2.98)  Time: 0.770s, 1329.28/s  (0.791s, 1294.05/s)  LR: 1.068e-05  Data: 0.009 (0.013)
Train: 590 [ 700/1251 ( 56%)]  Loss: 3.007 (2.98)  Time: 0.798s, 1282.44/s  (0.791s, 1293.77/s)  LR: 1.068e-05  Data: 0.010 (0.012)
Train: 590 [ 750/1251 ( 60%)]  Loss: 2.897 (2.97)  Time: 0.816s, 1255.37/s  (0.791s, 1294.45/s)  LR: 1.068e-05  Data: 0.010 (0.012)
Train: 590 [ 800/1251 ( 64%)]  Loss: 2.807 (2.96)  Time: 0.772s, 1326.40/s  (0.790s, 1295.46/s)  LR: 1.068e-05  Data: 0.010 (0.012)
Train: 590 [ 850/1251 ( 68%)]  Loss: 3.124 (2.97)  Time: 0.807s, 1269.20/s  (0.791s, 1294.80/s)  LR: 1.068e-05  Data: 0.010 (0.012)
Train: 590 [ 900/1251 ( 72%)]  Loss: 2.875 (2.97)  Time: 0.776s, 1318.98/s  (0.792s, 1293.43/s)  LR: 1.068e-05  Data: 0.009 (0.012)
Train: 590 [ 950/1251 ( 76%)]  Loss: 3.028 (2.97)  Time: 0.781s, 1310.99/s  (0.792s, 1293.07/s)  LR: 1.068e-05  Data: 0.011 (0.012)
Train: 590 [1000/1251 ( 80%)]  Loss: 3.201 (2.98)  Time: 0.827s, 1238.67/s  (0.793s, 1291.23/s)  LR: 1.068e-05  Data: 0.010 (0.012)
Train: 590 [1050/1251 ( 84%)]  Loss: 3.034 (2.98)  Time: 0.789s, 1298.40/s  (0.793s, 1290.66/s)  LR: 1.068e-05  Data: 0.010 (0.012)
Train: 590 [1100/1251 ( 88%)]  Loss: 3.220 (2.99)  Time: 0.817s, 1253.86/s  (0.793s, 1290.98/s)  LR: 1.068e-05  Data: 0.011 (0.012)
Train: 590 [1150/1251 ( 92%)]  Loss: 2.875 (2.99)  Time: 0.786s, 1302.99/s  (0.793s, 1291.49/s)  LR: 1.068e-05  Data: 0.009 (0.012)
Train: 590 [1200/1251 ( 96%)]  Loss: 2.785 (2.98)  Time: 0.776s, 1320.07/s  (0.792s, 1292.55/s)  LR: 1.068e-05  Data: 0.010 (0.011)
Train: 590 [1250/1251 (100%)]  Loss: 2.987 (2.98)  Time: 0.760s, 1347.90/s  (0.792s, 1293.42/s)  LR: 1.068e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.535 (1.535)  Loss:  0.6670 (0.6670)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.194 (0.561)  Loss:  0.7603 (1.0974)  Acc@1: 87.0283 (80.9460)  Acc@5: 98.2311 (95.4960)
Train: 591 [   0/1251 (  0%)]  Loss: 3.052 (3.05)  Time: 2.445s,  418.87/s  (2.445s,  418.87/s)  LR: 1.055e-05  Data: 1.701 (1.701)
Train: 591 [  50/1251 (  4%)]  Loss: 2.782 (2.92)  Time: 0.784s, 1305.68/s  (0.829s, 1235.85/s)  LR: 1.055e-05  Data: 0.009 (0.047)
Train: 591 [ 100/1251 (  8%)]  Loss: 3.122 (2.99)  Time: 0.779s, 1315.31/s  (0.818s, 1251.50/s)  LR: 1.055e-05  Data: 0.009 (0.029)
Train: 591 [ 150/1251 ( 12%)]  Loss: 3.020 (2.99)  Time: 0.772s, 1325.96/s  (0.806s, 1270.15/s)  LR: 1.055e-05  Data: 0.010 (0.022)
Train: 591 [ 200/1251 ( 16%)]  Loss: 3.008 (3.00)  Time: 0.820s, 1249.24/s  (0.800s, 1279.76/s)  LR: 1.055e-05  Data: 0.009 (0.019)
Train: 591 [ 250/1251 ( 20%)]  Loss: 2.721 (2.95)  Time: 0.807s, 1268.29/s  (0.796s, 1286.23/s)  LR: 1.055e-05  Data: 0.010 (0.017)
Train: 591 [ 300/1251 ( 24%)]  Loss: 3.136 (2.98)  Time: 0.799s, 1281.61/s  (0.796s, 1287.24/s)  LR: 1.055e-05  Data: 0.010 (0.016)
Train: 591 [ 350/1251 ( 28%)]  Loss: 3.022 (2.98)  Time: 0.776s, 1319.36/s  (0.795s, 1287.63/s)  LR: 1.055e-05  Data: 0.010 (0.015)
Train: 591 [ 400/1251 ( 32%)]  Loss: 2.724 (2.95)  Time: 0.772s, 1327.24/s  (0.795s, 1287.42/s)  LR: 1.055e-05  Data: 0.010 (0.015)
Train: 591 [ 450/1251 ( 36%)]  Loss: 2.750 (2.93)  Time: 0.864s, 1185.34/s  (0.795s, 1287.33/s)  LR: 1.055e-05  Data: 0.010 (0.014)
Train: 591 [ 500/1251 ( 40%)]  Loss: 2.650 (2.91)  Time: 0.770s, 1329.42/s  (0.795s, 1288.75/s)  LR: 1.055e-05  Data: 0.010 (0.014)
Train: 591 [ 550/1251 ( 44%)]  Loss: 2.906 (2.91)  Time: 0.842s, 1215.98/s  (0.795s, 1288.12/s)  LR: 1.055e-05  Data: 0.009 (0.013)
Train: 591 [ 600/1251 ( 48%)]  Loss: 3.049 (2.92)  Time: 0.818s, 1252.50/s  (0.797s, 1285.40/s)  LR: 1.055e-05  Data: 0.010 (0.013)
Train: 591 [ 650/1251 ( 52%)]  Loss: 2.835 (2.91)  Time: 0.772s, 1325.98/s  (0.798s, 1283.53/s)  LR: 1.055e-05  Data: 0.009 (0.013)
Train: 591 [ 700/1251 ( 56%)]  Loss: 2.881 (2.91)  Time: 0.773s, 1324.73/s  (0.797s, 1285.08/s)  LR: 1.055e-05  Data: 0.010 (0.013)
Train: 591 [ 750/1251 ( 60%)]  Loss: 2.978 (2.91)  Time: 0.789s, 1298.46/s  (0.796s, 1286.22/s)  LR: 1.055e-05  Data: 0.010 (0.013)
Train: 591 [ 800/1251 ( 64%)]  Loss: 2.911 (2.91)  Time: 0.820s, 1248.47/s  (0.795s, 1287.37/s)  LR: 1.055e-05  Data: 0.013 (0.012)
Train: 591 [ 850/1251 ( 68%)]  Loss: 2.727 (2.90)  Time: 0.776s, 1319.79/s  (0.795s, 1287.34/s)  LR: 1.055e-05  Data: 0.010 (0.012)
Train: 591 [ 900/1251 ( 72%)]  Loss: 2.601 (2.89)  Time: 0.773s, 1324.82/s  (0.794s, 1288.98/s)  LR: 1.055e-05  Data: 0.010 (0.012)
Train: 591 [ 950/1251 ( 76%)]  Loss: 2.881 (2.89)  Time: 0.779s, 1314.49/s  (0.794s, 1289.92/s)  LR: 1.055e-05  Data: 0.010 (0.012)
Train: 591 [1000/1251 ( 80%)]  Loss: 2.717 (2.88)  Time: 0.816s, 1254.49/s  (0.794s, 1290.37/s)  LR: 1.055e-05  Data: 0.011 (0.012)
Train: 591 [1050/1251 ( 84%)]  Loss: 3.119 (2.89)  Time: 0.868s, 1179.55/s  (0.793s, 1291.46/s)  LR: 1.055e-05  Data: 0.009 (0.012)
Train: 591 [1100/1251 ( 88%)]  Loss: 2.757 (2.88)  Time: 0.829s, 1235.17/s  (0.793s, 1292.09/s)  LR: 1.055e-05  Data: 0.009 (0.012)
Train: 591 [1150/1251 ( 92%)]  Loss: 2.823 (2.88)  Time: 0.815s, 1256.00/s  (0.793s, 1291.97/s)  LR: 1.055e-05  Data: 0.011 (0.012)
Train: 591 [1200/1251 ( 96%)]  Loss: 2.766 (2.88)  Time: 0.780s, 1312.00/s  (0.793s, 1291.88/s)  LR: 1.055e-05  Data: 0.011 (0.012)
Train: 591 [1250/1251 (100%)]  Loss: 2.901 (2.88)  Time: 0.760s, 1346.56/s  (0.793s, 1292.06/s)  LR: 1.055e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.603 (1.603)  Loss:  0.6816 (0.6816)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.194 (0.566)  Loss:  0.7871 (1.1075)  Acc@1: 87.3821 (81.0660)  Acc@5: 98.1132 (95.5040)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-585.pth.tar', 81.08600005371093)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-581.pth.tar', 81.07200002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-577.pth.tar', 81.07000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-591.pth.tar', 81.06599994873046)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-584.pth.tar', 81.05799997314453)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-579.pth.tar', 81.03)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-578.pth.tar', 81.02800005371094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-588.pth.tar', 81.02200005371094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-570.pth.tar', 81.00999997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-573.pth.tar', 80.98800005126954)

Train: 592 [   0/1251 (  0%)]  Loss: 2.922 (2.92)  Time: 2.426s,  422.16/s  (2.426s,  422.16/s)  LR: 1.043e-05  Data: 1.692 (1.692)
Train: 592 [  50/1251 (  4%)]  Loss: 3.029 (2.98)  Time: 0.771s, 1328.80/s  (0.818s, 1251.91/s)  LR: 1.043e-05  Data: 0.010 (0.047)
Train: 592 [ 100/1251 (  8%)]  Loss: 2.808 (2.92)  Time: 0.771s, 1327.90/s  (0.800s, 1279.64/s)  LR: 1.043e-05  Data: 0.010 (0.029)
Train: 592 [ 150/1251 ( 12%)]  Loss: 2.905 (2.92)  Time: 0.855s, 1198.23/s  (0.799s, 1281.49/s)  LR: 1.043e-05  Data: 0.010 (0.022)
Train: 592 [ 200/1251 ( 16%)]  Loss: 3.067 (2.95)  Time: 0.772s, 1326.37/s  (0.798s, 1282.67/s)  LR: 1.043e-05  Data: 0.010 (0.019)
Train: 592 [ 250/1251 ( 20%)]  Loss: 3.072 (2.97)  Time: 0.773s, 1325.40/s  (0.795s, 1288.77/s)  LR: 1.043e-05  Data: 0.010 (0.017)
Train: 592 [ 300/1251 ( 24%)]  Loss: 2.649 (2.92)  Time: 0.815s, 1255.95/s  (0.795s, 1287.51/s)  LR: 1.043e-05  Data: 0.010 (0.016)
Train: 592 [ 350/1251 ( 28%)]  Loss: 2.836 (2.91)  Time: 0.777s, 1318.67/s  (0.794s, 1289.98/s)  LR: 1.043e-05  Data: 0.011 (0.015)
Train: 592 [ 400/1251 ( 32%)]  Loss: 2.636 (2.88)  Time: 0.771s, 1328.57/s  (0.792s, 1292.84/s)  LR: 1.043e-05  Data: 0.009 (0.015)
Train: 592 [ 450/1251 ( 36%)]  Loss: 2.797 (2.87)  Time: 0.818s, 1252.32/s  (0.791s, 1294.57/s)  LR: 1.043e-05  Data: 0.012 (0.014)
Train: 592 [ 500/1251 ( 40%)]  Loss: 2.797 (2.87)  Time: 0.772s, 1326.60/s  (0.791s, 1294.03/s)  LR: 1.043e-05  Data: 0.009 (0.014)
Train: 592 [ 550/1251 ( 44%)]  Loss: 2.496 (2.83)  Time: 0.786s, 1303.53/s  (0.790s, 1295.49/s)  LR: 1.043e-05  Data: 0.009 (0.013)
Train: 592 [ 600/1251 ( 48%)]  Loss: 2.739 (2.83)  Time: 0.817s, 1253.50/s  (0.790s, 1295.59/s)  LR: 1.043e-05  Data: 0.009 (0.013)
Train: 592 [ 650/1251 ( 52%)]  Loss: 2.889 (2.83)  Time: 0.772s, 1326.16/s  (0.791s, 1295.17/s)  LR: 1.043e-05  Data: 0.010 (0.013)
Train: 592 [ 700/1251 ( 56%)]  Loss: 3.099 (2.85)  Time: 0.793s, 1290.76/s  (0.790s, 1295.79/s)  LR: 1.043e-05  Data: 0.012 (0.013)
Train: 592 [ 750/1251 ( 60%)]  Loss: 2.910 (2.85)  Time: 0.771s, 1328.83/s  (0.791s, 1294.48/s)  LR: 1.043e-05  Data: 0.010 (0.013)
Train: 592 [ 800/1251 ( 64%)]  Loss: 2.804 (2.85)  Time: 0.776s, 1319.88/s  (0.792s, 1293.48/s)  LR: 1.043e-05  Data: 0.010 (0.012)
Train: 592 [ 850/1251 ( 68%)]  Loss: 2.997 (2.86)  Time: 0.785s, 1305.18/s  (0.793s, 1292.07/s)  LR: 1.043e-05  Data: 0.013 (0.012)
Train: 592 [ 900/1251 ( 72%)]  Loss: 2.887 (2.86)  Time: 0.775s, 1320.57/s  (0.792s, 1292.35/s)  LR: 1.043e-05  Data: 0.010 (0.012)
Train: 592 [ 950/1251 ( 76%)]  Loss: 2.903 (2.86)  Time: 0.773s, 1324.24/s  (0.792s, 1292.97/s)  LR: 1.043e-05  Data: 0.010 (0.012)
Train: 592 [1000/1251 ( 80%)]  Loss: 3.001 (2.87)  Time: 0.814s, 1258.05/s  (0.793s, 1290.97/s)  LR: 1.043e-05  Data: 0.010 (0.012)
Train: 592 [1050/1251 ( 84%)]  Loss: 2.919 (2.87)  Time: 0.777s, 1317.96/s  (0.793s, 1291.52/s)  LR: 1.043e-05  Data: 0.010 (0.012)
Train: 592 [1100/1251 ( 88%)]  Loss: 3.003 (2.88)  Time: 0.775s, 1320.65/s  (0.792s, 1292.20/s)  LR: 1.043e-05  Data: 0.009 (0.012)
Train: 592 [1150/1251 ( 92%)]  Loss: 2.813 (2.87)  Time: 0.780s, 1313.25/s  (0.792s, 1292.82/s)  LR: 1.043e-05  Data: 0.010 (0.012)
Train: 592 [1200/1251 ( 96%)]  Loss: 3.223 (2.89)  Time: 0.783s, 1307.50/s  (0.792s, 1292.67/s)  LR: 1.043e-05  Data: 0.012 (0.012)
Train: 592 [1250/1251 (100%)]  Loss: 2.888 (2.89)  Time: 0.804s, 1273.29/s  (0.792s, 1293.33/s)  LR: 1.043e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.513 (1.513)  Loss:  0.6572 (0.6572)  Acc@1: 92.2852 (92.2852)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.194 (0.562)  Loss:  0.7476 (1.0834)  Acc@1: 87.2642 (80.9080)  Acc@5: 98.1132 (95.5320)
Train: 593 [   0/1251 (  0%)]  Loss: 3.112 (3.11)  Time: 2.235s,  458.18/s  (2.235s,  458.18/s)  LR: 1.033e-05  Data: 1.519 (1.519)
Train: 593 [  50/1251 (  4%)]  Loss: 3.185 (3.15)  Time: 0.775s, 1322.07/s  (0.827s, 1238.48/s)  LR: 1.033e-05  Data: 0.010 (0.042)
Train: 593 [ 100/1251 (  8%)]  Loss: 2.784 (3.03)  Time: 0.772s, 1326.74/s  (0.808s, 1266.90/s)  LR: 1.033e-05  Data: 0.010 (0.026)
Train: 593 [ 150/1251 ( 12%)]  Loss: 3.206 (3.07)  Time: 0.771s, 1327.79/s  (0.799s, 1282.07/s)  LR: 1.033e-05  Data: 0.009 (0.021)
Train: 593 [ 200/1251 ( 16%)]  Loss: 3.233 (3.10)  Time: 0.772s, 1325.65/s  (0.793s, 1290.75/s)  LR: 1.033e-05  Data: 0.009 (0.018)
Train: 593 [ 250/1251 ( 20%)]  Loss: 2.798 (3.05)  Time: 0.807s, 1269.16/s  (0.795s, 1288.49/s)  LR: 1.033e-05  Data: 0.010 (0.016)
Train: 593 [ 300/1251 ( 24%)]  Loss: 3.066 (3.05)  Time: 0.777s, 1318.00/s  (0.791s, 1293.76/s)  LR: 1.033e-05  Data: 0.010 (0.015)
Train: 593 [ 350/1251 ( 28%)]  Loss: 2.897 (3.04)  Time: 0.773s, 1324.17/s  (0.792s, 1293.29/s)  LR: 1.033e-05  Data: 0.010 (0.015)
Train: 593 [ 400/1251 ( 32%)]  Loss: 2.764 (3.00)  Time: 0.773s, 1324.33/s  (0.791s, 1294.21/s)  LR: 1.033e-05  Data: 0.010 (0.014)
Train: 593 [ 450/1251 ( 36%)]  Loss: 3.035 (3.01)  Time: 0.774s, 1322.30/s  (0.791s, 1294.12/s)  LR: 1.033e-05  Data: 0.009 (0.013)
Train: 593 [ 500/1251 ( 40%)]  Loss: 3.071 (3.01)  Time: 0.780s, 1312.45/s  (0.792s, 1293.60/s)  LR: 1.033e-05  Data: 0.010 (0.013)
Train: 593 [ 550/1251 ( 44%)]  Loss: 3.105 (3.02)  Time: 0.823s, 1244.01/s  (0.792s, 1292.47/s)  LR: 1.033e-05  Data: 0.010 (0.013)
Train: 593 [ 600/1251 ( 48%)]  Loss: 2.789 (3.00)  Time: 0.771s, 1327.41/s  (0.793s, 1292.02/s)  LR: 1.033e-05  Data: 0.010 (0.013)
Train: 593 [ 650/1251 ( 52%)]  Loss: 2.470 (2.97)  Time: 0.832s, 1230.03/s  (0.792s, 1292.75/s)  LR: 1.033e-05  Data: 0.010 (0.012)
Train: 593 [ 700/1251 ( 56%)]  Loss: 2.837 (2.96)  Time: 0.771s, 1328.16/s  (0.792s, 1292.41/s)  LR: 1.033e-05  Data: 0.009 (0.012)
Train: 593 [ 750/1251 ( 60%)]  Loss: 3.018 (2.96)  Time: 0.774s, 1322.62/s  (0.793s, 1291.60/s)  LR: 1.033e-05  Data: 0.010 (0.012)
Train: 593 [ 800/1251 ( 64%)]  Loss: 3.173 (2.97)  Time: 0.813s, 1259.89/s  (0.793s, 1290.63/s)  LR: 1.033e-05  Data: 0.010 (0.012)
Train: 593 [ 850/1251 ( 68%)]  Loss: 2.875 (2.97)  Time: 0.816s, 1255.62/s  (0.795s, 1288.73/s)  LR: 1.033e-05  Data: 0.011 (0.012)
Train: 593 [ 900/1251 ( 72%)]  Loss: 2.883 (2.96)  Time: 0.774s, 1323.73/s  (0.794s, 1289.41/s)  LR: 1.033e-05  Data: 0.010 (0.012)
Train: 593 [ 950/1251 ( 76%)]  Loss: 2.614 (2.95)  Time: 0.774s, 1323.14/s  (0.794s, 1290.19/s)  LR: 1.033e-05  Data: 0.009 (0.012)
Train: 593 [1000/1251 ( 80%)]  Loss: 2.815 (2.94)  Time: 0.780s, 1312.05/s  (0.793s, 1291.38/s)  LR: 1.033e-05  Data: 0.010 (0.012)
Train: 593 [1050/1251 ( 84%)]  Loss: 2.599 (2.92)  Time: 0.783s, 1307.21/s  (0.793s, 1291.98/s)  LR: 1.033e-05  Data: 0.011 (0.012)
Train: 593 [1100/1251 ( 88%)]  Loss: 2.876 (2.92)  Time: 0.782s, 1308.79/s  (0.792s, 1292.17/s)  LR: 1.033e-05  Data: 0.014 (0.012)
Train: 593 [1150/1251 ( 92%)]  Loss: 2.763 (2.92)  Time: 0.775s, 1320.86/s  (0.792s, 1292.81/s)  LR: 1.033e-05  Data: 0.010 (0.011)
Train: 593 [1200/1251 ( 96%)]  Loss: 2.887 (2.91)  Time: 0.773s, 1324.44/s  (0.792s, 1293.57/s)  LR: 1.033e-05  Data: 0.010 (0.011)
Train: 593 [1250/1251 (100%)]  Loss: 2.742 (2.91)  Time: 0.758s, 1350.17/s  (0.792s, 1293.56/s)  LR: 1.033e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.640 (1.640)  Loss:  0.6021 (0.6021)  Acc@1: 92.3828 (92.3828)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.194 (0.565)  Loss:  0.7017 (1.0233)  Acc@1: 87.5000 (80.9580)  Acc@5: 98.1132 (95.5280)
Train: 594 [   0/1251 (  0%)]  Loss: 2.805 (2.81)  Time: 2.289s,  447.43/s  (2.289s,  447.43/s)  LR: 1.024e-05  Data: 1.551 (1.551)
Train: 594 [  50/1251 (  4%)]  Loss: 2.780 (2.79)  Time: 0.791s, 1294.14/s  (0.816s, 1254.39/s)  LR: 1.024e-05  Data: 0.010 (0.046)
Train: 594 [ 100/1251 (  8%)]  Loss: 2.936 (2.84)  Time: 0.777s, 1317.85/s  (0.807s, 1269.24/s)  LR: 1.024e-05  Data: 0.010 (0.029)
Train: 594 [ 150/1251 ( 12%)]  Loss: 3.027 (2.89)  Time: 0.790s, 1295.70/s  (0.801s, 1278.55/s)  LR: 1.024e-05  Data: 0.009 (0.022)
Train: 594 [ 200/1251 ( 16%)]  Loss: 2.901 (2.89)  Time: 0.772s, 1326.25/s  (0.798s, 1283.13/s)  LR: 1.024e-05  Data: 0.010 (0.019)
Train: 594 [ 250/1251 ( 20%)]  Loss: 2.795 (2.87)  Time: 0.789s, 1297.78/s  (0.795s, 1287.57/s)  LR: 1.024e-05  Data: 0.011 (0.017)
Train: 594 [ 300/1251 ( 24%)]  Loss: 2.959 (2.89)  Time: 0.863s, 1186.74/s  (0.793s, 1291.09/s)  LR: 1.024e-05  Data: 0.010 (0.016)
Train: 594 [ 350/1251 ( 28%)]  Loss: 3.010 (2.90)  Time: 0.771s, 1328.21/s  (0.791s, 1294.72/s)  LR: 1.024e-05  Data: 0.009 (0.015)
Train: 594 [ 400/1251 ( 32%)]  Loss: 2.731 (2.88)  Time: 0.772s, 1326.21/s  (0.791s, 1294.53/s)  LR: 1.024e-05  Data: 0.010 (0.015)
Train: 594 [ 450/1251 ( 36%)]  Loss: 3.227 (2.92)  Time: 0.876s, 1169.33/s  (0.790s, 1295.55/s)  LR: 1.024e-05  Data: 0.010 (0.014)
Train: 594 [ 500/1251 ( 40%)]  Loss: 2.908 (2.92)  Time: 0.773s, 1325.26/s  (0.790s, 1296.27/s)  LR: 1.024e-05  Data: 0.010 (0.014)
Train: 594 [ 550/1251 ( 44%)]  Loss: 3.166 (2.94)  Time: 0.772s, 1325.86/s  (0.789s, 1298.13/s)  LR: 1.024e-05  Data: 0.010 (0.013)
Train: 594 [ 600/1251 ( 48%)]  Loss: 2.539 (2.91)  Time: 0.771s, 1327.79/s  (0.790s, 1296.26/s)  LR: 1.024e-05  Data: 0.010 (0.013)
Train: 594 [ 650/1251 ( 52%)]  Loss: 3.061 (2.92)  Time: 0.771s, 1328.34/s  (0.789s, 1297.44/s)  LR: 1.024e-05  Data: 0.009 (0.013)
Train: 594 [ 700/1251 ( 56%)]  Loss: 3.099 (2.93)  Time: 0.770s, 1329.93/s  (0.788s, 1298.89/s)  LR: 1.024e-05  Data: 0.009 (0.013)
Train: 594 [ 750/1251 ( 60%)]  Loss: 2.895 (2.93)  Time: 0.773s, 1324.88/s  (0.788s, 1299.26/s)  LR: 1.024e-05  Data: 0.009 (0.012)
Train: 594 [ 800/1251 ( 64%)]  Loss: 3.009 (2.93)  Time: 0.785s, 1303.83/s  (0.788s, 1299.95/s)  LR: 1.024e-05  Data: 0.010 (0.012)
Train: 594 [ 850/1251 ( 68%)]  Loss: 2.927 (2.93)  Time: 0.771s, 1327.38/s  (0.787s, 1300.81/s)  LR: 1.024e-05  Data: 0.010 (0.012)
Train: 594 [ 900/1251 ( 72%)]  Loss: 2.969 (2.93)  Time: 0.775s, 1320.84/s  (0.787s, 1300.89/s)  LR: 1.024e-05  Data: 0.009 (0.012)
Train: 594 [ 950/1251 ( 76%)]  Loss: 2.847 (2.93)  Time: 0.777s, 1317.76/s  (0.787s, 1301.69/s)  LR: 1.024e-05  Data: 0.010 (0.012)
Train: 594 [1000/1251 ( 80%)]  Loss: 3.132 (2.94)  Time: 0.774s, 1323.21/s  (0.786s, 1302.27/s)  LR: 1.024e-05  Data: 0.009 (0.012)
Train: 594 [1050/1251 ( 84%)]  Loss: 3.130 (2.95)  Time: 0.771s, 1328.83/s  (0.786s, 1302.42/s)  LR: 1.024e-05  Data: 0.009 (0.012)
Train: 594 [1100/1251 ( 88%)]  Loss: 3.043 (2.95)  Time: 0.773s, 1325.27/s  (0.786s, 1302.54/s)  LR: 1.024e-05  Data: 0.010 (0.012)
Train: 594 [1150/1251 ( 92%)]  Loss: 2.799 (2.95)  Time: 0.786s, 1302.96/s  (0.786s, 1302.57/s)  LR: 1.024e-05  Data: 0.010 (0.011)
Train: 594 [1200/1251 ( 96%)]  Loss: 2.858 (2.94)  Time: 0.824s, 1243.01/s  (0.787s, 1300.96/s)  LR: 1.024e-05  Data: 0.010 (0.011)
Train: 594 [1250/1251 (100%)]  Loss: 3.222 (2.95)  Time: 0.762s, 1343.47/s  (0.787s, 1301.03/s)  LR: 1.024e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.542 (1.542)  Loss:  0.6738 (0.6738)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.7305 (98.7305)
Test: [  48/48]  Time: 0.194 (0.570)  Loss:  0.7607 (1.0922)  Acc@1: 87.3821 (80.9620)  Acc@5: 98.4670 (95.5140)
Train: 595 [   0/1251 (  0%)]  Loss: 2.933 (2.93)  Time: 2.327s,  440.07/s  (2.327s,  440.07/s)  LR: 1.017e-05  Data: 1.599 (1.599)
Train: 595 [  50/1251 (  4%)]  Loss: 3.046 (2.99)  Time: 0.785s, 1304.21/s  (0.828s, 1236.06/s)  LR: 1.017e-05  Data: 0.010 (0.044)
Train: 595 [ 100/1251 (  8%)]  Loss: 2.905 (2.96)  Time: 0.772s, 1326.30/s  (0.808s, 1267.49/s)  LR: 1.017e-05  Data: 0.010 (0.027)
Train: 595 [ 150/1251 ( 12%)]  Loss: 3.132 (3.00)  Time: 0.811s, 1262.11/s  (0.799s, 1281.47/s)  LR: 1.017e-05  Data: 0.010 (0.022)
Train: 595 [ 200/1251 ( 16%)]  Loss: 2.897 (2.98)  Time: 0.808s, 1267.44/s  (0.797s, 1284.45/s)  LR: 1.017e-05  Data: 0.010 (0.019)
Train: 595 [ 250/1251 ( 20%)]  Loss: 2.948 (2.98)  Time: 0.814s, 1258.03/s  (0.795s, 1288.09/s)  LR: 1.017e-05  Data: 0.011 (0.017)
Train: 595 [ 300/1251 ( 24%)]  Loss: 2.693 (2.94)  Time: 0.779s, 1314.85/s  (0.793s, 1291.33/s)  LR: 1.017e-05  Data: 0.010 (0.016)
Train: 595 [ 350/1251 ( 28%)]  Loss: 3.046 (2.95)  Time: 0.780s, 1313.05/s  (0.792s, 1292.84/s)  LR: 1.017e-05  Data: 0.010 (0.015)
Train: 595 [ 400/1251 ( 32%)]  Loss: 2.777 (2.93)  Time: 0.813s, 1260.03/s  (0.792s, 1292.45/s)  LR: 1.017e-05  Data: 0.010 (0.015)
Train: 595 [ 450/1251 ( 36%)]  Loss: 2.980 (2.94)  Time: 0.868s, 1179.23/s  (0.792s, 1293.53/s)  LR: 1.017e-05  Data: 0.010 (0.014)
Train: 595 [ 500/1251 ( 40%)]  Loss: 2.859 (2.93)  Time: 0.795s, 1288.28/s  (0.791s, 1294.45/s)  LR: 1.017e-05  Data: 0.010 (0.014)
Train: 595 [ 550/1251 ( 44%)]  Loss: 3.028 (2.94)  Time: 0.774s, 1323.57/s  (0.790s, 1296.43/s)  LR: 1.017e-05  Data: 0.010 (0.013)
Train: 595 [ 600/1251 ( 48%)]  Loss: 2.716 (2.92)  Time: 0.773s, 1324.67/s  (0.790s, 1295.45/s)  LR: 1.017e-05  Data: 0.010 (0.013)
Train: 595 [ 650/1251 ( 52%)]  Loss: 3.121 (2.93)  Time: 0.830s, 1233.84/s  (0.791s, 1294.20/s)  LR: 1.017e-05  Data: 0.010 (0.013)
Train: 595 [ 700/1251 ( 56%)]  Loss: 2.773 (2.92)  Time: 0.778s, 1315.51/s  (0.791s, 1294.23/s)  LR: 1.017e-05  Data: 0.010 (0.013)
Train: 595 [ 750/1251 ( 60%)]  Loss: 3.012 (2.93)  Time: 0.869s, 1178.57/s  (0.791s, 1293.84/s)  LR: 1.017e-05  Data: 0.009 (0.013)
Train: 595 [ 800/1251 ( 64%)]  Loss: 2.764 (2.92)  Time: 0.797s, 1284.81/s  (0.792s, 1293.52/s)  LR: 1.017e-05  Data: 0.015 (0.012)
Train: 595 [ 850/1251 ( 68%)]  Loss: 3.023 (2.93)  Time: 0.773s, 1324.54/s  (0.791s, 1294.71/s)  LR: 1.017e-05  Data: 0.010 (0.012)
Train: 595 [ 900/1251 ( 72%)]  Loss: 2.595 (2.91)  Time: 0.776s, 1319.48/s  (0.791s, 1294.62/s)  LR: 1.017e-05  Data: 0.009 (0.012)
Train: 595 [ 950/1251 ( 76%)]  Loss: 3.073 (2.92)  Time: 0.770s, 1330.10/s  (0.791s, 1295.16/s)  LR: 1.017e-05  Data: 0.009 (0.012)
Train: 595 [1000/1251 ( 80%)]  Loss: 2.942 (2.92)  Time: 0.776s, 1319.22/s  (0.791s, 1294.00/s)  LR: 1.017e-05  Data: 0.009 (0.012)
Train: 595 [1050/1251 ( 84%)]  Loss: 2.938 (2.92)  Time: 0.788s, 1298.73/s  (0.791s, 1294.14/s)  LR: 1.017e-05  Data: 0.010 (0.012)
Train: 595 [1100/1251 ( 88%)]  Loss: 2.788 (2.91)  Time: 0.774s, 1323.66/s  (0.792s, 1293.60/s)  LR: 1.017e-05  Data: 0.010 (0.012)
Train: 595 [1150/1251 ( 92%)]  Loss: 2.751 (2.91)  Time: 0.863s, 1187.01/s  (0.792s, 1292.63/s)  LR: 1.017e-05  Data: 0.009 (0.012)
Train: 595 [1200/1251 ( 96%)]  Loss: 3.147 (2.92)  Time: 0.771s, 1327.37/s  (0.792s, 1293.29/s)  LR: 1.017e-05  Data: 0.010 (0.012)
Train: 595 [1250/1251 (100%)]  Loss: 3.080 (2.92)  Time: 0.759s, 1349.38/s  (0.793s, 1292.11/s)  LR: 1.017e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.579 (1.579)  Loss:  0.6431 (0.6431)  Acc@1: 92.2852 (92.2852)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.194 (0.583)  Loss:  0.7490 (1.0814)  Acc@1: 87.1462 (81.0060)  Acc@5: 98.1132 (95.5240)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-585.pth.tar', 81.08600005371093)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-581.pth.tar', 81.07200002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-577.pth.tar', 81.07000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-591.pth.tar', 81.06599994873046)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-584.pth.tar', 81.05799997314453)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-579.pth.tar', 81.03)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-578.pth.tar', 81.02800005371094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-588.pth.tar', 81.02200005371094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-570.pth.tar', 81.00999997558594)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-595.pth.tar', 81.00600010498047)

Train: 596 [   0/1251 (  0%)]  Loss: 2.949 (2.95)  Time: 2.183s,  469.01/s  (2.183s,  469.01/s)  LR: 1.011e-05  Data: 1.463 (1.463)
Train: 596 [  50/1251 (  4%)]  Loss: 3.040 (2.99)  Time: 0.812s, 1260.51/s  (0.822s, 1245.98/s)  LR: 1.011e-05  Data: 0.012 (0.044)
Train: 596 [ 100/1251 (  8%)]  Loss: 2.900 (2.96)  Time: 0.772s, 1326.83/s  (0.803s, 1275.11/s)  LR: 1.011e-05  Data: 0.010 (0.027)
Train: 596 [ 150/1251 ( 12%)]  Loss: 2.993 (2.97)  Time: 0.815s, 1257.17/s  (0.798s, 1283.18/s)  LR: 1.011e-05  Data: 0.011 (0.022)
Train: 596 [ 200/1251 ( 16%)]  Loss: 2.577 (2.89)  Time: 0.773s, 1324.71/s  (0.795s, 1288.63/s)  LR: 1.011e-05  Data: 0.011 (0.019)
Train: 596 [ 250/1251 ( 20%)]  Loss: 2.990 (2.91)  Time: 0.810s, 1264.62/s  (0.793s, 1292.01/s)  LR: 1.011e-05  Data: 0.009 (0.017)
Train: 596 [ 300/1251 ( 24%)]  Loss: 2.914 (2.91)  Time: 0.785s, 1304.19/s  (0.791s, 1294.10/s)  LR: 1.011e-05  Data: 0.011 (0.016)
Train: 596 [ 350/1251 ( 28%)]  Loss: 2.778 (2.89)  Time: 0.818s, 1252.41/s  (0.792s, 1292.35/s)  LR: 1.011e-05  Data: 0.010 (0.015)
Train: 596 [ 400/1251 ( 32%)]  Loss: 2.783 (2.88)  Time: 0.772s, 1325.67/s  (0.791s, 1293.80/s)  LR: 1.011e-05  Data: 0.010 (0.014)
Train: 596 [ 450/1251 ( 36%)]  Loss: 2.907 (2.88)  Time: 0.770s, 1329.01/s  (0.790s, 1295.39/s)  LR: 1.011e-05  Data: 0.010 (0.014)
Train: 596 [ 500/1251 ( 40%)]  Loss: 3.164 (2.91)  Time: 0.807s, 1269.53/s  (0.790s, 1296.82/s)  LR: 1.011e-05  Data: 0.010 (0.014)
Train: 596 [ 550/1251 ( 44%)]  Loss: 3.227 (2.94)  Time: 0.781s, 1310.46/s  (0.791s, 1295.36/s)  LR: 1.011e-05  Data: 0.010 (0.013)
Train: 596 [ 600/1251 ( 48%)]  Loss: 2.807 (2.93)  Time: 0.811s, 1262.78/s  (0.790s, 1295.94/s)  LR: 1.011e-05  Data: 0.009 (0.013)
Train: 596 [ 650/1251 ( 52%)]  Loss: 2.716 (2.91)  Time: 0.773s, 1325.44/s  (0.790s, 1296.70/s)  LR: 1.011e-05  Data: 0.010 (0.013)
Train: 596 [ 700/1251 ( 56%)]  Loss: 3.024 (2.92)  Time: 0.774s, 1323.60/s  (0.790s, 1296.84/s)  LR: 1.011e-05  Data: 0.010 (0.013)
Train: 596 [ 750/1251 ( 60%)]  Loss: 2.859 (2.91)  Time: 0.772s, 1326.58/s  (0.789s, 1298.13/s)  LR: 1.011e-05  Data: 0.010 (0.012)
Train: 596 [ 800/1251 ( 64%)]  Loss: 3.006 (2.92)  Time: 0.783s, 1307.31/s  (0.788s, 1299.39/s)  LR: 1.011e-05  Data: 0.010 (0.012)
Train: 596 [ 850/1251 ( 68%)]  Loss: 3.045 (2.93)  Time: 0.773s, 1325.00/s  (0.788s, 1300.05/s)  LR: 1.011e-05  Data: 0.010 (0.012)
Train: 596 [ 900/1251 ( 72%)]  Loss: 3.058 (2.93)  Time: 0.818s, 1251.16/s  (0.788s, 1299.20/s)  LR: 1.011e-05  Data: 0.010 (0.012)
Train: 596 [ 950/1251 ( 76%)]  Loss: 2.881 (2.93)  Time: 0.769s, 1332.29/s  (0.788s, 1299.97/s)  LR: 1.011e-05  Data: 0.010 (0.012)
Train: 596 [1000/1251 ( 80%)]  Loss: 2.652 (2.92)  Time: 0.781s, 1311.06/s  (0.787s, 1300.39/s)  LR: 1.011e-05  Data: 0.012 (0.012)
Train: 596 [1050/1251 ( 84%)]  Loss: 3.038 (2.92)  Time: 0.820s, 1248.20/s  (0.788s, 1300.11/s)  LR: 1.011e-05  Data: 0.009 (0.012)
Train: 596 [1100/1251 ( 88%)]  Loss: 2.729 (2.91)  Time: 0.787s, 1301.19/s  (0.788s, 1299.88/s)  LR: 1.011e-05  Data: 0.010 (0.012)
Train: 596 [1150/1251 ( 92%)]  Loss: 2.613 (2.90)  Time: 0.804s, 1273.73/s  (0.788s, 1300.06/s)  LR: 1.011e-05  Data: 0.009 (0.012)
Train: 596 [1200/1251 ( 96%)]  Loss: 2.867 (2.90)  Time: 0.771s, 1328.44/s  (0.787s, 1300.88/s)  LR: 1.011e-05  Data: 0.010 (0.012)
Train: 596 [1250/1251 (100%)]  Loss: 2.941 (2.90)  Time: 0.805s, 1272.08/s  (0.788s, 1300.16/s)  LR: 1.011e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.537 (1.537)  Loss:  0.6670 (0.6670)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.194 (0.563)  Loss:  0.7539 (1.0853)  Acc@1: 87.5000 (80.9600)  Acc@5: 98.1132 (95.5140)
Train: 597 [   0/1251 (  0%)]  Loss: 3.025 (3.03)  Time: 2.413s,  424.30/s  (2.413s,  424.30/s)  LR: 1.006e-05  Data: 1.687 (1.687)
Train: 597 [  50/1251 (  4%)]  Loss: 3.232 (3.13)  Time: 0.772s, 1326.75/s  (0.817s, 1253.34/s)  LR: 1.006e-05  Data: 0.010 (0.043)
Train: 597 [ 100/1251 (  8%)]  Loss: 2.995 (3.08)  Time: 0.773s, 1324.91/s  (0.800s, 1279.68/s)  LR: 1.006e-05  Data: 0.010 (0.027)
Train: 597 [ 150/1251 ( 12%)]  Loss: 2.789 (3.01)  Time: 0.803s, 1275.03/s  (0.795s, 1288.37/s)  LR: 1.006e-05  Data: 0.010 (0.021)
Train: 597 [ 200/1251 ( 16%)]  Loss: 2.680 (2.94)  Time: 0.808s, 1267.87/s  (0.799s, 1281.90/s)  LR: 1.006e-05  Data: 0.010 (0.018)
Train: 597 [ 250/1251 ( 20%)]  Loss: 2.984 (2.95)  Time: 0.776s, 1319.41/s  (0.796s, 1285.90/s)  LR: 1.006e-05  Data: 0.010 (0.017)
Train: 597 [ 300/1251 ( 24%)]  Loss: 2.866 (2.94)  Time: 0.852s, 1201.97/s  (0.793s, 1290.79/s)  LR: 1.006e-05  Data: 0.010 (0.016)
Train: 597 [ 350/1251 ( 28%)]  Loss: 3.014 (2.95)  Time: 0.796s, 1286.96/s  (0.792s, 1292.59/s)  LR: 1.006e-05  Data: 0.009 (0.015)
Train: 597 [ 400/1251 ( 32%)]  Loss: 2.725 (2.92)  Time: 0.771s, 1327.70/s  (0.795s, 1288.75/s)  LR: 1.006e-05  Data: 0.010 (0.014)
Train: 597 [ 450/1251 ( 36%)]  Loss: 3.087 (2.94)  Time: 0.809s, 1265.63/s  (0.794s, 1288.92/s)  LR: 1.006e-05  Data: 0.010 (0.014)
Train: 597 [ 500/1251 ( 40%)]  Loss: 3.318 (2.97)  Time: 0.858s, 1194.02/s  (0.794s, 1290.47/s)  LR: 1.006e-05  Data: 0.010 (0.013)
Train: 597 [ 550/1251 ( 44%)]  Loss: 2.724 (2.95)  Time: 0.772s, 1325.92/s  (0.794s, 1290.30/s)  LR: 1.006e-05  Data: 0.010 (0.013)
Train: 597 [ 600/1251 ( 48%)]  Loss: 3.077 (2.96)  Time: 0.771s, 1328.46/s  (0.793s, 1291.58/s)  LR: 1.006e-05  Data: 0.010 (0.013)
Train: 597 [ 650/1251 ( 52%)]  Loss: 2.989 (2.96)  Time: 0.778s, 1316.78/s  (0.793s, 1291.43/s)  LR: 1.006e-05  Data: 0.010 (0.013)
Train: 597 [ 700/1251 ( 56%)]  Loss: 2.920 (2.96)  Time: 0.772s, 1325.88/s  (0.792s, 1292.49/s)  LR: 1.006e-05  Data: 0.010 (0.012)
Train: 597 [ 750/1251 ( 60%)]  Loss: 2.867 (2.96)  Time: 0.810s, 1263.42/s  (0.792s, 1293.13/s)  LR: 1.006e-05  Data: 0.010 (0.012)
Train: 597 [ 800/1251 ( 64%)]  Loss: 2.743 (2.94)  Time: 0.784s, 1305.68/s  (0.791s, 1293.86/s)  LR: 1.006e-05  Data: 0.010 (0.012)
Train: 597 [ 850/1251 ( 68%)]  Loss: 2.747 (2.93)  Time: 0.772s, 1326.02/s  (0.791s, 1294.80/s)  LR: 1.006e-05  Data: 0.010 (0.012)
Train: 597 [ 900/1251 ( 72%)]  Loss: 2.870 (2.93)  Time: 0.790s, 1296.84/s  (0.790s, 1295.60/s)  LR: 1.006e-05  Data: 0.010 (0.012)
Train: 597 [ 950/1251 ( 76%)]  Loss: 3.167 (2.94)  Time: 0.804s, 1273.73/s  (0.791s, 1294.31/s)  LR: 1.006e-05  Data: 0.010 (0.012)
Train: 597 [1000/1251 ( 80%)]  Loss: 3.107 (2.95)  Time: 0.777s, 1318.16/s  (0.792s, 1293.08/s)  LR: 1.006e-05  Data: 0.009 (0.012)
Train: 597 [1050/1251 ( 84%)]  Loss: 2.539 (2.93)  Time: 0.806s, 1270.05/s  (0.792s, 1293.30/s)  LR: 1.006e-05  Data: 0.010 (0.012)
Train: 597 [1100/1251 ( 88%)]  Loss: 2.656 (2.92)  Time: 0.772s, 1326.86/s  (0.792s, 1293.16/s)  LR: 1.006e-05  Data: 0.009 (0.012)
Train: 597 [1150/1251 ( 92%)]  Loss: 2.473 (2.90)  Time: 0.774s, 1323.80/s  (0.792s, 1292.55/s)  LR: 1.006e-05  Data: 0.009 (0.011)
Train: 597 [1200/1251 ( 96%)]  Loss: 2.418 (2.88)  Time: 0.808s, 1267.75/s  (0.792s, 1292.28/s)  LR: 1.006e-05  Data: 0.010 (0.011)
Train: 597 [1250/1251 (100%)]  Loss: 2.890 (2.88)  Time: 0.774s, 1322.42/s  (0.792s, 1292.13/s)  LR: 1.006e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.616 (1.616)  Loss:  0.5776 (0.5776)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.193 (0.575)  Loss:  0.6855 (0.9979)  Acc@1: 87.2641 (81.0440)  Acc@5: 97.9953 (95.5560)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-585.pth.tar', 81.08600005371093)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-581.pth.tar', 81.07200002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-577.pth.tar', 81.07000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-591.pth.tar', 81.06599994873046)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-584.pth.tar', 81.05799997314453)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-597.pth.tar', 81.04399989746094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-579.pth.tar', 81.03)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-578.pth.tar', 81.02800005371094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-588.pth.tar', 81.02200005371094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-570.pth.tar', 81.00999997558594)

Train: 598 [   0/1251 (  0%)]  Loss: 2.669 (2.67)  Time: 2.386s,  429.17/s  (2.386s,  429.17/s)  LR: 1.003e-05  Data: 1.604 (1.604)
Train: 598 [  50/1251 (  4%)]  Loss: 3.189 (2.93)  Time: 0.772s, 1326.41/s  (0.825s, 1241.62/s)  LR: 1.003e-05  Data: 0.009 (0.045)
Train: 598 [ 100/1251 (  8%)]  Loss: 2.875 (2.91)  Time: 0.769s, 1331.28/s  (0.808s, 1267.42/s)  LR: 1.003e-05  Data: 0.010 (0.028)
Train: 598 [ 150/1251 ( 12%)]  Loss: 2.972 (2.93)  Time: 0.772s, 1326.09/s  (0.798s, 1282.84/s)  LR: 1.003e-05  Data: 0.010 (0.022)
Train: 598 [ 200/1251 ( 16%)]  Loss: 2.867 (2.91)  Time: 0.779s, 1314.77/s  (0.795s, 1287.44/s)  LR: 1.003e-05  Data: 0.010 (0.019)
Train: 598 [ 250/1251 ( 20%)]  Loss: 2.643 (2.87)  Time: 0.815s, 1256.40/s  (0.794s, 1289.09/s)  LR: 1.003e-05  Data: 0.011 (0.017)
Train: 598 [ 300/1251 ( 24%)]  Loss: 2.553 (2.82)  Time: 0.773s, 1324.22/s  (0.795s, 1288.53/s)  LR: 1.003e-05  Data: 0.009 (0.016)
Train: 598 [ 350/1251 ( 28%)]  Loss: 2.958 (2.84)  Time: 0.817s, 1253.15/s  (0.794s, 1290.21/s)  LR: 1.003e-05  Data: 0.010 (0.015)
Train: 598 [ 400/1251 ( 32%)]  Loss: 2.876 (2.84)  Time: 0.778s, 1316.19/s  (0.792s, 1293.16/s)  LR: 1.003e-05  Data: 0.012 (0.015)
Train: 598 [ 450/1251 ( 36%)]  Loss: 2.807 (2.84)  Time: 0.776s, 1320.38/s  (0.791s, 1295.30/s)  LR: 1.003e-05  Data: 0.010 (0.014)
Train: 598 [ 500/1251 ( 40%)]  Loss: 3.175 (2.87)  Time: 0.775s, 1322.07/s  (0.789s, 1297.53/s)  LR: 1.003e-05  Data: 0.012 (0.014)
Train: 598 [ 550/1251 ( 44%)]  Loss: 2.972 (2.88)  Time: 0.807s, 1269.22/s  (0.789s, 1298.02/s)  LR: 1.003e-05  Data: 0.009 (0.014)
Train: 598 [ 600/1251 ( 48%)]  Loss: 3.108 (2.90)  Time: 0.808s, 1267.51/s  (0.789s, 1297.70/s)  LR: 1.003e-05  Data: 0.010 (0.013)
Train: 598 [ 650/1251 ( 52%)]  Loss: 2.843 (2.89)  Time: 0.772s, 1326.06/s  (0.788s, 1298.69/s)  LR: 1.003e-05  Data: 0.010 (0.013)
Train: 598 [ 700/1251 ( 56%)]  Loss: 2.967 (2.90)  Time: 0.775s, 1322.01/s  (0.788s, 1298.85/s)  LR: 1.003e-05  Data: 0.009 (0.013)
Train: 598 [ 750/1251 ( 60%)]  Loss: 3.059 (2.91)  Time: 0.781s, 1310.64/s  (0.788s, 1300.32/s)  LR: 1.003e-05  Data: 0.010 (0.013)
Train: 598 [ 800/1251 ( 64%)]  Loss: 3.118 (2.92)  Time: 0.775s, 1321.72/s  (0.788s, 1300.29/s)  LR: 1.003e-05  Data: 0.009 (0.012)
Train: 598 [ 850/1251 ( 68%)]  Loss: 2.774 (2.91)  Time: 0.776s, 1319.96/s  (0.787s, 1301.29/s)  LR: 1.003e-05  Data: 0.009 (0.012)
Train: 598 [ 900/1251 ( 72%)]  Loss: 2.994 (2.92)  Time: 0.771s, 1328.36/s  (0.788s, 1300.09/s)  LR: 1.003e-05  Data: 0.010 (0.012)
Train: 598 [ 950/1251 ( 76%)]  Loss: 2.951 (2.92)  Time: 0.772s, 1325.94/s  (0.788s, 1299.42/s)  LR: 1.003e-05  Data: 0.010 (0.012)
Train: 598 [1000/1251 ( 80%)]  Loss: 2.664 (2.91)  Time: 0.797s, 1284.78/s  (0.788s, 1300.06/s)  LR: 1.003e-05  Data: 0.012 (0.012)
Train: 598 [1050/1251 ( 84%)]  Loss: 3.093 (2.91)  Time: 0.778s, 1315.54/s  (0.787s, 1300.66/s)  LR: 1.003e-05  Data: 0.013 (0.012)
Train: 598 [1100/1251 ( 88%)]  Loss: 2.981 (2.92)  Time: 0.779s, 1313.83/s  (0.787s, 1300.76/s)  LR: 1.003e-05  Data: 0.010 (0.012)
Train: 598 [1150/1251 ( 92%)]  Loss: 2.733 (2.91)  Time: 0.809s, 1265.22/s  (0.787s, 1300.91/s)  LR: 1.003e-05  Data: 0.009 (0.012)
Train: 598 [1200/1251 ( 96%)]  Loss: 2.556 (2.90)  Time: 0.775s, 1321.19/s  (0.787s, 1300.60/s)  LR: 1.003e-05  Data: 0.010 (0.012)
Train: 598 [1250/1251 (100%)]  Loss: 2.980 (2.90)  Time: 0.760s, 1347.33/s  (0.787s, 1300.34/s)  LR: 1.003e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.574 (1.574)  Loss:  0.6318 (0.6318)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.194 (0.570)  Loss:  0.7354 (1.0621)  Acc@1: 86.9104 (81.0120)  Acc@5: 98.2311 (95.5420)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-585.pth.tar', 81.08600005371093)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-581.pth.tar', 81.07200002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-577.pth.tar', 81.07000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-591.pth.tar', 81.06599994873046)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-584.pth.tar', 81.05799997314453)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-597.pth.tar', 81.04399989746094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-579.pth.tar', 81.03)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-578.pth.tar', 81.02800005371094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-588.pth.tar', 81.02200005371094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-598.pth.tar', 81.01200000244141)

Train: 599 [   0/1251 (  0%)]  Loss: 2.702 (2.70)  Time: 2.319s,  441.55/s  (2.319s,  441.55/s)  LR: 1.001e-05  Data: 1.592 (1.592)
Train: 599 [  50/1251 (  4%)]  Loss: 2.908 (2.81)  Time: 0.773s, 1324.77/s  (0.813s, 1259.82/s)  LR: 1.001e-05  Data: 0.010 (0.044)
Train: 599 [ 100/1251 (  8%)]  Loss: 2.799 (2.80)  Time: 0.782s, 1310.20/s  (0.795s, 1287.33/s)  LR: 1.001e-05  Data: 0.009 (0.027)
Train: 599 [ 150/1251 ( 12%)]  Loss: 2.924 (2.83)  Time: 0.773s, 1325.11/s  (0.796s, 1286.81/s)  LR: 1.001e-05  Data: 0.010 (0.021)
Train: 599 [ 200/1251 ( 16%)]  Loss: 2.955 (2.86)  Time: 0.773s, 1325.41/s  (0.791s, 1294.35/s)  LR: 1.001e-05  Data: 0.010 (0.018)
Train: 599 [ 250/1251 ( 20%)]  Loss: 2.627 (2.82)  Time: 0.770s, 1330.64/s  (0.789s, 1297.14/s)  LR: 1.001e-05  Data: 0.009 (0.017)
Train: 599 [ 300/1251 ( 24%)]  Loss: 2.900 (2.83)  Time: 0.774s, 1322.44/s  (0.789s, 1297.68/s)  LR: 1.001e-05  Data: 0.010 (0.016)
Train: 599 [ 350/1251 ( 28%)]  Loss: 2.576 (2.80)  Time: 0.820s, 1249.52/s  (0.789s, 1298.11/s)  LR: 1.001e-05  Data: 0.009 (0.015)
Train: 599 [ 400/1251 ( 32%)]  Loss: 2.949 (2.82)  Time: 0.775s, 1321.49/s  (0.788s, 1300.07/s)  LR: 1.001e-05  Data: 0.010 (0.014)
Train: 599 [ 450/1251 ( 36%)]  Loss: 2.892 (2.82)  Time: 0.817s, 1252.62/s  (0.791s, 1295.30/s)  LR: 1.001e-05  Data: 0.011 (0.014)
Train: 599 [ 500/1251 ( 40%)]  Loss: 3.017 (2.84)  Time: 0.776s, 1319.33/s  (0.791s, 1294.17/s)  LR: 1.001e-05  Data: 0.009 (0.013)
Train: 599 [ 550/1251 ( 44%)]  Loss: 3.116 (2.86)  Time: 0.772s, 1326.60/s  (0.790s, 1296.44/s)  LR: 1.001e-05  Data: 0.009 (0.013)
Train: 599 [ 600/1251 ( 48%)]  Loss: 3.013 (2.88)  Time: 0.772s, 1327.28/s  (0.789s, 1298.22/s)  LR: 1.001e-05  Data: 0.010 (0.013)
Train: 599 [ 650/1251 ( 52%)]  Loss: 2.900 (2.88)  Time: 0.772s, 1326.03/s  (0.788s, 1299.58/s)  LR: 1.001e-05  Data: 0.009 (0.013)
Train: 599 [ 700/1251 ( 56%)]  Loss: 2.875 (2.88)  Time: 0.837s, 1223.70/s  (0.788s, 1298.99/s)  LR: 1.001e-05  Data: 0.009 (0.012)
Train: 599 [ 750/1251 ( 60%)]  Loss: 2.616 (2.86)  Time: 0.773s, 1325.14/s  (0.788s, 1300.08/s)  LR: 1.001e-05  Data: 0.010 (0.012)
Train: 599 [ 800/1251 ( 64%)]  Loss: 2.838 (2.86)  Time: 0.827s, 1238.02/s  (0.789s, 1298.19/s)  LR: 1.001e-05  Data: 0.014 (0.012)
Train: 599 [ 850/1251 ( 68%)]  Loss: 2.826 (2.86)  Time: 0.777s, 1317.09/s  (0.789s, 1298.21/s)  LR: 1.001e-05  Data: 0.009 (0.012)
Train: 599 [ 900/1251 ( 72%)]  Loss: 2.863 (2.86)  Time: 0.828s, 1236.78/s  (0.789s, 1298.36/s)  LR: 1.001e-05  Data: 0.010 (0.012)
Train: 599 [ 950/1251 ( 76%)]  Loss: 2.707 (2.85)  Time: 0.773s, 1325.20/s  (0.788s, 1299.07/s)  LR: 1.001e-05  Data: 0.010 (0.012)
Train: 599 [1000/1251 ( 80%)]  Loss: 2.835 (2.85)  Time: 0.769s, 1331.20/s  (0.788s, 1299.46/s)  LR: 1.001e-05  Data: 0.009 (0.012)
Train: 599 [1050/1251 ( 84%)]  Loss: 3.054 (2.86)  Time: 0.787s, 1300.38/s  (0.788s, 1299.47/s)  LR: 1.001e-05  Data: 0.015 (0.012)
Train: 599 [1100/1251 ( 88%)]  Loss: 2.955 (2.86)  Time: 0.778s, 1315.50/s  (0.788s, 1299.60/s)  LR: 1.001e-05  Data: 0.010 (0.012)
Train: 599 [1150/1251 ( 92%)]  Loss: 2.671 (2.85)  Time: 0.777s, 1318.34/s  (0.788s, 1299.94/s)  LR: 1.001e-05  Data: 0.010 (0.011)
Train: 599 [1200/1251 ( 96%)]  Loss: 2.894 (2.86)  Time: 0.773s, 1324.04/s  (0.787s, 1300.54/s)  LR: 1.001e-05  Data: 0.010 (0.011)
Train: 599 [1250/1251 (100%)]  Loss: 2.739 (2.85)  Time: 0.768s, 1333.74/s  (0.787s, 1300.95/s)  LR: 1.001e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.583 (1.583)  Loss:  0.5234 (0.5234)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.194 (0.566)  Loss:  0.6367 (0.9496)  Acc@1: 87.5000 (81.1760)  Acc@5: 98.2311 (95.5660)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-599.pth.tar', 81.17599987060547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-585.pth.tar', 81.08600005371093)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-581.pth.tar', 81.07200002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-577.pth.tar', 81.07000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-591.pth.tar', 81.06599994873046)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-584.pth.tar', 81.05799997314453)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-597.pth.tar', 81.04399989746094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-579.pth.tar', 81.03)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-578.pth.tar', 81.02800005371094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-588.pth.tar', 81.02200005371094)

Train: 600 [   0/1251 (  0%)]  Loss: 2.814 (2.81)  Time: 2.517s,  406.91/s  (2.517s,  406.91/s)  LR: 1.000e-05  Data: 1.769 (1.769)
Train: 600 [  50/1251 (  4%)]  Loss: 3.006 (2.91)  Time: 0.811s, 1263.26/s  (0.829s, 1235.55/s)  LR: 1.000e-05  Data: 0.010 (0.046)
Train: 600 [ 100/1251 (  8%)]  Loss: 2.797 (2.87)  Time: 0.775s, 1320.58/s  (0.813s, 1259.88/s)  LR: 1.000e-05  Data: 0.009 (0.028)
Train: 600 [ 150/1251 ( 12%)]  Loss: 2.655 (2.82)  Time: 0.769s, 1331.05/s  (0.800s, 1279.29/s)  LR: 1.000e-05  Data: 0.010 (0.022)
Train: 600 [ 200/1251 ( 16%)]  Loss: 2.967 (2.85)  Time: 0.815s, 1256.67/s  (0.799s, 1282.40/s)  LR: 1.000e-05  Data: 0.011 (0.019)
Train: 600 [ 250/1251 ( 20%)]  Loss: 2.883 (2.85)  Time: 0.773s, 1324.00/s  (0.797s, 1284.48/s)  LR: 1.000e-05  Data: 0.010 (0.017)
Train: 600 [ 300/1251 ( 24%)]  Loss: 2.939 (2.87)  Time: 0.782s, 1309.37/s  (0.794s, 1289.90/s)  LR: 1.000e-05  Data: 0.010 (0.016)
Train: 600 [ 350/1251 ( 28%)]  Loss: 2.920 (2.87)  Time: 0.815s, 1256.58/s  (0.797s, 1285.58/s)  LR: 1.000e-05  Data: 0.011 (0.015)
Train: 600 [ 400/1251 ( 32%)]  Loss: 2.852 (2.87)  Time: 0.815s, 1257.19/s  (0.799s, 1281.25/s)  LR: 1.000e-05  Data: 0.011 (0.015)
Train: 600 [ 450/1251 ( 36%)]  Loss: 3.026 (2.89)  Time: 0.787s, 1301.38/s  (0.799s, 1281.14/s)  LR: 1.000e-05  Data: 0.009 (0.014)
Train: 600 [ 500/1251 ( 40%)]  Loss: 2.880 (2.89)  Time: 0.775s, 1321.43/s  (0.797s, 1284.20/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 600 [ 550/1251 ( 44%)]  Loss: 3.107 (2.90)  Time: 0.777s, 1318.48/s  (0.796s, 1286.15/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 600 [ 600/1251 ( 48%)]  Loss: 2.817 (2.90)  Time: 0.774s, 1323.13/s  (0.795s, 1288.41/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 600 [ 650/1251 ( 52%)]  Loss: 3.053 (2.91)  Time: 0.807s, 1269.39/s  (0.793s, 1290.59/s)  LR: 1.000e-05  Data: 0.009 (0.013)
Train: 600 [ 700/1251 ( 56%)]  Loss: 2.823 (2.90)  Time: 0.780s, 1313.53/s  (0.792s, 1292.38/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 600 [ 750/1251 ( 60%)]  Loss: 3.090 (2.91)  Time: 0.773s, 1324.74/s  (0.791s, 1294.32/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 600 [ 800/1251 ( 64%)]  Loss: 2.853 (2.91)  Time: 0.774s, 1323.70/s  (0.791s, 1295.38/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 600 [ 850/1251 ( 68%)]  Loss: 3.099 (2.92)  Time: 0.832s, 1231.19/s  (0.790s, 1295.95/s)  LR: 1.000e-05  Data: 0.014 (0.012)
Train: 600 [ 900/1251 ( 72%)]  Loss: 2.834 (2.92)  Time: 0.806s, 1270.80/s  (0.790s, 1296.43/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 600 [ 950/1251 ( 76%)]  Loss: 2.741 (2.91)  Time: 0.774s, 1322.79/s  (0.789s, 1297.03/s)  LR: 1.000e-05  Data: 0.009 (0.012)
Train: 600 [1000/1251 ( 80%)]  Loss: 2.946 (2.91)  Time: 0.783s, 1307.92/s  (0.789s, 1297.54/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 600 [1050/1251 ( 84%)]  Loss: 2.980 (2.91)  Time: 0.775s, 1321.13/s  (0.789s, 1297.85/s)  LR: 1.000e-05  Data: 0.009 (0.012)
Train: 600 [1100/1251 ( 88%)]  Loss: 2.878 (2.91)  Time: 0.783s, 1308.20/s  (0.789s, 1298.34/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 600 [1150/1251 ( 92%)]  Loss: 2.727 (2.90)  Time: 0.815s, 1257.21/s  (0.789s, 1297.18/s)  LR: 1.000e-05  Data: 0.009 (0.012)
Train: 600 [1200/1251 ( 96%)]  Loss: 2.798 (2.90)  Time: 0.774s, 1323.58/s  (0.789s, 1297.09/s)  LR: 1.000e-05  Data: 0.009 (0.012)
Train: 600 [1250/1251 (100%)]  Loss: 2.700 (2.89)  Time: 0.760s, 1346.73/s  (0.790s, 1296.94/s)  LR: 1.000e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.566 (1.566)  Loss:  0.5693 (0.5693)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.194 (0.564)  Loss:  0.6836 (1.0050)  Acc@1: 87.3821 (81.0620)  Acc@5: 98.3491 (95.5720)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-599.pth.tar', 81.17599987060547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-585.pth.tar', 81.08600005371093)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-581.pth.tar', 81.07200002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-577.pth.tar', 81.07000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-591.pth.tar', 81.06599994873046)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-600.pth.tar', 81.062000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-584.pth.tar', 81.05799997314453)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-597.pth.tar', 81.04399989746094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-579.pth.tar', 81.03)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-578.pth.tar', 81.02800005371094)

Train: 601 [   0/1251 (  0%)]  Loss: 2.700 (2.70)  Time: 2.395s,  427.57/s  (2.395s,  427.57/s)  LR: 1.000e-05  Data: 1.659 (1.659)
Train: 601 [  50/1251 (  4%)]  Loss: 2.667 (2.68)  Time: 0.818s, 1251.59/s  (0.851s, 1203.83/s)  LR: 1.000e-05  Data: 0.011 (0.044)
Train: 601 [ 100/1251 (  8%)]  Loss: 2.891 (2.75)  Time: 0.807s, 1269.09/s  (0.820s, 1248.62/s)  LR: 1.000e-05  Data: 0.010 (0.027)
Train: 601 [ 150/1251 ( 12%)]  Loss: 2.468 (2.68)  Time: 0.774s, 1322.32/s  (0.817s, 1253.97/s)  LR: 1.000e-05  Data: 0.009 (0.022)
Train: 601 [ 200/1251 ( 16%)]  Loss: 2.765 (2.70)  Time: 0.774s, 1323.79/s  (0.808s, 1267.64/s)  LR: 1.000e-05  Data: 0.010 (0.019)
Train: 601 [ 250/1251 ( 20%)]  Loss: 2.812 (2.72)  Time: 0.805s, 1271.44/s  (0.805s, 1272.36/s)  LR: 1.000e-05  Data: 0.010 (0.017)
Train: 601 [ 300/1251 ( 24%)]  Loss: 2.696 (2.71)  Time: 0.772s, 1325.85/s  (0.801s, 1278.36/s)  LR: 1.000e-05  Data: 0.010 (0.016)
Train: 601 [ 350/1251 ( 28%)]  Loss: 2.897 (2.74)  Time: 0.809s, 1265.75/s  (0.798s, 1282.54/s)  LR: 1.000e-05  Data: 0.010 (0.015)
Train: 601 [ 400/1251 ( 32%)]  Loss: 2.818 (2.75)  Time: 0.793s, 1292.00/s  (0.797s, 1284.39/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 601 [ 450/1251 ( 36%)]  Loss: 2.779 (2.75)  Time: 0.775s, 1320.83/s  (0.795s, 1287.48/s)  LR: 1.000e-05  Data: 0.009 (0.014)
Train: 601 [ 500/1251 ( 40%)]  Loss: 3.322 (2.80)  Time: 0.780s, 1312.65/s  (0.795s, 1287.86/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 601 [ 550/1251 ( 44%)]  Loss: 2.534 (2.78)  Time: 0.773s, 1324.79/s  (0.794s, 1290.12/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 601 [ 600/1251 ( 48%)]  Loss: 3.109 (2.80)  Time: 0.772s, 1326.26/s  (0.793s, 1291.87/s)  LR: 1.000e-05  Data: 0.009 (0.013)
Train: 601 [ 650/1251 ( 52%)]  Loss: 2.959 (2.82)  Time: 0.807s, 1269.48/s  (0.792s, 1292.14/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 601 [ 700/1251 ( 56%)]  Loss: 2.696 (2.81)  Time: 0.789s, 1297.08/s  (0.792s, 1292.64/s)  LR: 1.000e-05  Data: 0.009 (0.012)
Train: 601 [ 750/1251 ( 60%)]  Loss: 2.814 (2.81)  Time: 0.772s, 1327.24/s  (0.791s, 1294.17/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 601 [ 800/1251 ( 64%)]  Loss: 2.805 (2.81)  Time: 0.800s, 1279.64/s  (0.791s, 1294.16/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 601 [ 850/1251 ( 68%)]  Loss: 3.212 (2.83)  Time: 0.824s, 1242.16/s  (0.791s, 1294.75/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 601 [ 900/1251 ( 72%)]  Loss: 3.063 (2.84)  Time: 0.770s, 1330.14/s  (0.791s, 1295.07/s)  LR: 1.000e-05  Data: 0.009 (0.012)
Train: 601 [ 950/1251 ( 76%)]  Loss: 3.026 (2.85)  Time: 0.771s, 1327.75/s  (0.790s, 1295.58/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 601 [1000/1251 ( 80%)]  Loss: 2.926 (2.86)  Time: 0.815s, 1256.82/s  (0.792s, 1293.18/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 601 [1050/1251 ( 84%)]  Loss: 2.670 (2.85)  Time: 0.818s, 1251.56/s  (0.793s, 1291.26/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 601 [1100/1251 ( 88%)]  Loss: 3.138 (2.86)  Time: 0.815s, 1257.00/s  (0.794s, 1289.70/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 601 [1150/1251 ( 92%)]  Loss: 2.853 (2.86)  Time: 0.772s, 1326.78/s  (0.794s, 1289.12/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 601 [1200/1251 ( 96%)]  Loss: 3.295 (2.88)  Time: 0.781s, 1310.43/s  (0.794s, 1289.40/s)  LR: 1.000e-05  Data: 0.012 (0.011)
Train: 601 [1250/1251 (100%)]  Loss: 3.046 (2.88)  Time: 0.796s, 1286.90/s  (0.794s, 1289.78/s)  LR: 1.000e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.562 (1.562)  Loss:  0.7070 (0.7070)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.194 (0.562)  Loss:  0.7974 (1.1230)  Acc@1: 87.5000 (80.9560)  Acc@5: 98.3491 (95.5560)
Train: 602 [   0/1251 (  0%)]  Loss: 2.834 (2.83)  Time: 2.116s,  483.85/s  (2.116s,  483.85/s)  LR: 1.000e-05  Data: 1.401 (1.401)
Train: 602 [  50/1251 (  4%)]  Loss: 2.986 (2.91)  Time: 0.814s, 1257.39/s  (0.816s, 1255.06/s)  LR: 1.000e-05  Data: 0.011 (0.042)
Train: 602 [ 100/1251 (  8%)]  Loss: 2.810 (2.88)  Time: 0.770s, 1330.01/s  (0.812s, 1261.35/s)  LR: 1.000e-05  Data: 0.009 (0.026)
Train: 602 [ 150/1251 ( 12%)]  Loss: 2.941 (2.89)  Time: 0.773s, 1324.67/s  (0.801s, 1279.03/s)  LR: 1.000e-05  Data: 0.010 (0.021)
Train: 602 [ 200/1251 ( 16%)]  Loss: 2.928 (2.90)  Time: 0.786s, 1302.62/s  (0.796s, 1286.83/s)  LR: 1.000e-05  Data: 0.009 (0.018)
Train: 602 [ 250/1251 ( 20%)]  Loss: 2.691 (2.87)  Time: 0.806s, 1269.85/s  (0.794s, 1290.00/s)  LR: 1.000e-05  Data: 0.009 (0.016)
Train: 602 [ 300/1251 ( 24%)]  Loss: 2.671 (2.84)  Time: 0.835s, 1226.47/s  (0.796s, 1285.63/s)  LR: 1.000e-05  Data: 0.010 (0.015)
Train: 602 [ 350/1251 ( 28%)]  Loss: 2.787 (2.83)  Time: 0.772s, 1325.66/s  (0.795s, 1288.11/s)  LR: 1.000e-05  Data: 0.010 (0.015)
Train: 602 [ 400/1251 ( 32%)]  Loss: 2.918 (2.84)  Time: 0.780s, 1312.15/s  (0.793s, 1291.38/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 602 [ 450/1251 ( 36%)]  Loss: 2.806 (2.84)  Time: 0.809s, 1265.77/s  (0.791s, 1293.84/s)  LR: 1.000e-05  Data: 0.009 (0.013)
Train: 602 [ 500/1251 ( 40%)]  Loss: 2.940 (2.85)  Time: 0.770s, 1329.03/s  (0.791s, 1294.54/s)  LR: 1.000e-05  Data: 0.009 (0.013)
Train: 602 [ 550/1251 ( 44%)]  Loss: 3.036 (2.86)  Time: 0.777s, 1317.49/s  (0.791s, 1294.96/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 602 [ 600/1251 ( 48%)]  Loss: 2.697 (2.85)  Time: 0.784s, 1305.82/s  (0.790s, 1295.87/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 602 [ 650/1251 ( 52%)]  Loss: 2.693 (2.84)  Time: 0.773s, 1325.23/s  (0.790s, 1296.87/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 602 [ 700/1251 ( 56%)]  Loss: 2.735 (2.83)  Time: 0.773s, 1324.07/s  (0.789s, 1297.80/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 602 [ 750/1251 ( 60%)]  Loss: 3.218 (2.86)  Time: 0.776s, 1319.02/s  (0.790s, 1295.64/s)  LR: 1.000e-05  Data: 0.009 (0.012)
Train: 602 [ 800/1251 ( 64%)]  Loss: 2.871 (2.86)  Time: 0.773s, 1325.02/s  (0.790s, 1296.57/s)  LR: 1.000e-05  Data: 0.009 (0.012)
Train: 602 [ 850/1251 ( 68%)]  Loss: 2.932 (2.86)  Time: 0.770s, 1330.21/s  (0.789s, 1297.35/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 602 [ 900/1251 ( 72%)]  Loss: 3.135 (2.88)  Time: 0.772s, 1327.01/s  (0.789s, 1298.61/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 602 [ 950/1251 ( 76%)]  Loss: 2.906 (2.88)  Time: 0.777s, 1318.21/s  (0.788s, 1298.87/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 602 [1000/1251 ( 80%)]  Loss: 2.643 (2.87)  Time: 0.773s, 1324.42/s  (0.788s, 1299.20/s)  LR: 1.000e-05  Data: 0.010 (0.011)
Train: 602 [1050/1251 ( 84%)]  Loss: 3.036 (2.87)  Time: 0.807s, 1268.53/s  (0.788s, 1299.50/s)  LR: 1.000e-05  Data: 0.012 (0.011)
Train: 602 [1100/1251 ( 88%)]  Loss: 3.011 (2.88)  Time: 0.772s, 1327.19/s  (0.788s, 1299.41/s)  LR: 1.000e-05  Data: 0.010 (0.011)
Train: 602 [1150/1251 ( 92%)]  Loss: 2.444 (2.86)  Time: 0.773s, 1324.42/s  (0.788s, 1300.24/s)  LR: 1.000e-05  Data: 0.010 (0.011)
Train: 602 [1200/1251 ( 96%)]  Loss: 3.128 (2.87)  Time: 0.804s, 1273.16/s  (0.787s, 1300.81/s)  LR: 1.000e-05  Data: 0.010 (0.011)
Train: 602 [1250/1251 (100%)]  Loss: 2.919 (2.87)  Time: 0.760s, 1346.74/s  (0.787s, 1301.44/s)  LR: 1.000e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.606 (1.606)  Loss:  0.6299 (0.6299)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.194 (0.565)  Loss:  0.7314 (1.0536)  Acc@1: 87.2642 (80.9600)  Acc@5: 98.2311 (95.5360)
Train: 603 [   0/1251 (  0%)]  Loss: 3.030 (3.03)  Time: 2.300s,  445.15/s  (2.300s,  445.15/s)  LR: 1.000e-05  Data: 1.572 (1.572)
Train: 603 [  50/1251 (  4%)]  Loss: 2.818 (2.92)  Time: 0.772s, 1327.25/s  (0.821s, 1247.59/s)  LR: 1.000e-05  Data: 0.010 (0.044)
Train: 603 [ 100/1251 (  8%)]  Loss: 2.726 (2.86)  Time: 0.774s, 1323.45/s  (0.797s, 1284.09/s)  LR: 1.000e-05  Data: 0.009 (0.027)
Train: 603 [ 150/1251 ( 12%)]  Loss: 2.945 (2.88)  Time: 0.813s, 1259.35/s  (0.798s, 1283.95/s)  LR: 1.000e-05  Data: 0.009 (0.021)
Train: 603 [ 200/1251 ( 16%)]  Loss: 3.347 (2.97)  Time: 0.781s, 1310.46/s  (0.793s, 1291.20/s)  LR: 1.000e-05  Data: 0.009 (0.018)
Train: 603 [ 250/1251 ( 20%)]  Loss: 3.081 (2.99)  Time: 0.773s, 1325.49/s  (0.793s, 1291.27/s)  LR: 1.000e-05  Data: 0.010 (0.017)
Train: 603 [ 300/1251 ( 24%)]  Loss: 2.967 (2.99)  Time: 0.799s, 1281.83/s  (0.795s, 1288.56/s)  LR: 1.000e-05  Data: 0.012 (0.015)
Train: 603 [ 350/1251 ( 28%)]  Loss: 2.769 (2.96)  Time: 0.772s, 1326.98/s  (0.793s, 1291.83/s)  LR: 1.000e-05  Data: 0.009 (0.015)
Train: 603 [ 400/1251 ( 32%)]  Loss: 2.968 (2.96)  Time: 0.772s, 1326.98/s  (0.792s, 1293.49/s)  LR: 1.000e-05  Data: 0.009 (0.014)
Train: 603 [ 450/1251 ( 36%)]  Loss: 2.857 (2.95)  Time: 0.773s, 1324.48/s  (0.790s, 1295.61/s)  LR: 1.000e-05  Data: 0.009 (0.014)
Train: 603 [ 500/1251 ( 40%)]  Loss: 2.977 (2.95)  Time: 0.773s, 1324.31/s  (0.789s, 1297.81/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 603 [ 550/1251 ( 44%)]  Loss: 3.093 (2.96)  Time: 0.774s, 1323.00/s  (0.788s, 1299.85/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 603 [ 600/1251 ( 48%)]  Loss: 2.960 (2.96)  Time: 0.774s, 1323.62/s  (0.787s, 1301.45/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 603 [ 650/1251 ( 52%)]  Loss: 2.912 (2.96)  Time: 0.797s, 1284.46/s  (0.787s, 1301.67/s)  LR: 1.000e-05  Data: 0.009 (0.012)
Train: 603 [ 700/1251 ( 56%)]  Loss: 2.950 (2.96)  Time: 0.774s, 1323.53/s  (0.787s, 1301.66/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 603 [ 750/1251 ( 60%)]  Loss: 2.947 (2.96)  Time: 0.867s, 1181.61/s  (0.787s, 1300.85/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 603 [ 800/1251 ( 64%)]  Loss: 2.817 (2.95)  Time: 0.773s, 1325.46/s  (0.787s, 1301.15/s)  LR: 1.000e-05  Data: 0.009 (0.012)
Train: 603 [ 850/1251 ( 68%)]  Loss: 2.591 (2.93)  Time: 0.772s, 1325.70/s  (0.787s, 1301.68/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 603 [ 900/1251 ( 72%)]  Loss: 2.802 (2.92)  Time: 0.783s, 1308.48/s  (0.786s, 1302.72/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 603 [ 950/1251 ( 76%)]  Loss: 2.853 (2.92)  Time: 0.772s, 1326.96/s  (0.786s, 1303.07/s)  LR: 1.000e-05  Data: 0.009 (0.012)
Train: 603 [1000/1251 ( 80%)]  Loss: 2.706 (2.91)  Time: 0.770s, 1330.00/s  (0.785s, 1303.92/s)  LR: 1.000e-05  Data: 0.010 (0.011)
Train: 603 [1050/1251 ( 84%)]  Loss: 2.767 (2.90)  Time: 0.784s, 1306.53/s  (0.785s, 1304.15/s)  LR: 1.000e-05  Data: 0.009 (0.011)
Train: 603 [1100/1251 ( 88%)]  Loss: 3.132 (2.91)  Time: 0.807s, 1268.76/s  (0.785s, 1303.78/s)  LR: 1.000e-05  Data: 0.011 (0.011)
Train: 603 [1150/1251 ( 92%)]  Loss: 2.578 (2.90)  Time: 0.773s, 1325.39/s  (0.786s, 1303.51/s)  LR: 1.000e-05  Data: 0.012 (0.011)
Train: 603 [1200/1251 ( 96%)]  Loss: 2.780 (2.89)  Time: 0.773s, 1325.23/s  (0.786s, 1303.58/s)  LR: 1.000e-05  Data: 0.010 (0.011)
Train: 603 [1250/1251 (100%)]  Loss: 2.564 (2.88)  Time: 0.767s, 1335.70/s  (0.786s, 1303.60/s)  LR: 1.000e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.657 (1.657)  Loss:  0.6318 (0.6318)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.194 (0.581)  Loss:  0.7422 (1.0701)  Acc@1: 86.9104 (80.9420)  Acc@5: 98.1132 (95.5520)
Train: 604 [   0/1251 (  0%)]  Loss: 3.027 (3.03)  Time: 2.430s,  421.43/s  (2.430s,  421.43/s)  LR: 1.000e-05  Data: 1.655 (1.655)
Train: 604 [  50/1251 (  4%)]  Loss: 2.863 (2.94)  Time: 0.792s, 1293.07/s  (0.853s, 1201.14/s)  LR: 1.000e-05  Data: 0.010 (0.053)
Train: 604 [ 100/1251 (  8%)]  Loss: 2.818 (2.90)  Time: 0.774s, 1323.80/s  (0.815s, 1255.94/s)  LR: 1.000e-05  Data: 0.010 (0.032)
Train: 604 [ 150/1251 ( 12%)]  Loss: 2.829 (2.88)  Time: 0.786s, 1302.45/s  (0.808s, 1268.09/s)  LR: 1.000e-05  Data: 0.009 (0.025)
Train: 604 [ 200/1251 ( 16%)]  Loss: 2.846 (2.88)  Time: 0.785s, 1304.80/s  (0.802s, 1277.42/s)  LR: 1.000e-05  Data: 0.010 (0.021)
Train: 604 [ 250/1251 ( 20%)]  Loss: 3.065 (2.91)  Time: 0.808s, 1267.08/s  (0.800s, 1279.33/s)  LR: 1.000e-05  Data: 0.010 (0.019)
Train: 604 [ 300/1251 ( 24%)]  Loss: 2.848 (2.90)  Time: 0.777s, 1317.51/s  (0.797s, 1284.05/s)  LR: 1.000e-05  Data: 0.011 (0.018)
Train: 604 [ 350/1251 ( 28%)]  Loss: 2.924 (2.90)  Time: 0.772s, 1326.02/s  (0.799s, 1282.17/s)  LR: 1.000e-05  Data: 0.009 (0.017)
Train: 604 [ 400/1251 ( 32%)]  Loss: 2.883 (2.90)  Time: 0.769s, 1330.92/s  (0.797s, 1285.39/s)  LR: 1.000e-05  Data: 0.010 (0.016)
Train: 604 [ 450/1251 ( 36%)]  Loss: 2.812 (2.89)  Time: 0.772s, 1326.54/s  (0.795s, 1288.53/s)  LR: 1.000e-05  Data: 0.010 (0.015)
Train: 604 [ 500/1251 ( 40%)]  Loss: 2.694 (2.87)  Time: 0.775s, 1321.82/s  (0.794s, 1289.42/s)  LR: 1.000e-05  Data: 0.010 (0.015)
Train: 604 [ 550/1251 ( 44%)]  Loss: 3.102 (2.89)  Time: 0.776s, 1318.89/s  (0.793s, 1290.97/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 604 [ 600/1251 ( 48%)]  Loss: 2.820 (2.89)  Time: 0.778s, 1316.03/s  (0.794s, 1289.86/s)  LR: 1.000e-05  Data: 0.009 (0.014)
Train: 604 [ 650/1251 ( 52%)]  Loss: 3.138 (2.90)  Time: 0.775s, 1320.61/s  (0.794s, 1290.28/s)  LR: 1.000e-05  Data: 0.009 (0.013)
Train: 604 [ 700/1251 ( 56%)]  Loss: 2.958 (2.91)  Time: 0.822s, 1246.02/s  (0.794s, 1289.31/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 604 [ 750/1251 ( 60%)]  Loss: 2.987 (2.91)  Time: 0.786s, 1302.55/s  (0.794s, 1289.26/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 604 [ 800/1251 ( 64%)]  Loss: 3.003 (2.92)  Time: 0.773s, 1323.99/s  (0.793s, 1290.53/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 604 [ 850/1251 ( 68%)]  Loss: 2.816 (2.91)  Time: 0.820s, 1248.17/s  (0.793s, 1291.46/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 604 [ 900/1251 ( 72%)]  Loss: 2.572 (2.90)  Time: 0.816s, 1254.51/s  (0.794s, 1289.39/s)  LR: 1.000e-05  Data: 0.009 (0.013)
Train: 604 [ 950/1251 ( 76%)]  Loss: 2.727 (2.89)  Time: 0.772s, 1327.14/s  (0.795s, 1287.72/s)  LR: 1.000e-05  Data: 0.009 (0.012)
Train: 604 [1000/1251 ( 80%)]  Loss: 2.884 (2.89)  Time: 0.812s, 1261.07/s  (0.796s, 1286.89/s)  LR: 1.000e-05  Data: 0.009 (0.012)
Train: 604 [1050/1251 ( 84%)]  Loss: 2.966 (2.89)  Time: 0.786s, 1302.83/s  (0.795s, 1287.92/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 604 [1100/1251 ( 88%)]  Loss: 2.733 (2.88)  Time: 0.775s, 1321.37/s  (0.794s, 1289.14/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 604 [1150/1251 ( 92%)]  Loss: 2.836 (2.88)  Time: 0.773s, 1325.19/s  (0.794s, 1289.68/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 604 [1200/1251 ( 96%)]  Loss: 2.915 (2.88)  Time: 0.773s, 1324.54/s  (0.794s, 1290.29/s)  LR: 1.000e-05  Data: 0.009 (0.012)
Train: 604 [1250/1251 (100%)]  Loss: 2.725 (2.88)  Time: 0.763s, 1341.95/s  (0.793s, 1291.41/s)  LR: 1.000e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.651 (1.651)  Loss:  0.6714 (0.6714)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.194 (0.566)  Loss:  0.7666 (1.0960)  Acc@1: 87.3821 (80.9800)  Acc@5: 98.2311 (95.5460)
Train: 605 [   0/1251 (  0%)]  Loss: 2.585 (2.58)  Time: 2.164s,  473.22/s  (2.164s,  473.22/s)  LR: 1.000e-05  Data: 1.429 (1.429)
Train: 605 [  50/1251 (  4%)]  Loss: 2.851 (2.72)  Time: 0.773s, 1324.11/s  (0.823s, 1244.08/s)  LR: 1.000e-05  Data: 0.010 (0.048)
Train: 605 [ 100/1251 (  8%)]  Loss: 2.650 (2.70)  Time: 0.809s, 1265.87/s  (0.801s, 1278.41/s)  LR: 1.000e-05  Data: 0.010 (0.029)
Train: 605 [ 150/1251 ( 12%)]  Loss: 2.825 (2.73)  Time: 0.773s, 1324.94/s  (0.794s, 1289.23/s)  LR: 1.000e-05  Data: 0.010 (0.023)
Train: 605 [ 200/1251 ( 16%)]  Loss: 2.864 (2.75)  Time: 0.779s, 1315.34/s  (0.790s, 1295.76/s)  LR: 1.000e-05  Data: 0.010 (0.020)
Train: 605 [ 250/1251 ( 20%)]  Loss: 2.397 (2.70)  Time: 0.773s, 1325.06/s  (0.789s, 1297.52/s)  LR: 1.000e-05  Data: 0.009 (0.018)
Train: 605 [ 300/1251 ( 24%)]  Loss: 2.834 (2.71)  Time: 0.783s, 1308.23/s  (0.788s, 1299.52/s)  LR: 1.000e-05  Data: 0.009 (0.016)
Train: 605 [ 350/1251 ( 28%)]  Loss: 2.986 (2.75)  Time: 0.808s, 1267.63/s  (0.789s, 1297.66/s)  LR: 1.000e-05  Data: 0.010 (0.015)
Train: 605 [ 400/1251 ( 32%)]  Loss: 2.720 (2.75)  Time: 0.809s, 1265.94/s  (0.791s, 1295.11/s)  LR: 1.000e-05  Data: 0.010 (0.015)
Train: 605 [ 450/1251 ( 36%)]  Loss: 2.827 (2.75)  Time: 0.774s, 1323.37/s  (0.791s, 1295.19/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 605 [ 500/1251 ( 40%)]  Loss: 2.930 (2.77)  Time: 0.771s, 1327.42/s  (0.790s, 1296.81/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 605 [ 550/1251 ( 44%)]  Loss: 2.966 (2.79)  Time: 0.775s, 1321.34/s  (0.789s, 1298.66/s)  LR: 1.000e-05  Data: 0.009 (0.013)
Train: 605 [ 600/1251 ( 48%)]  Loss: 2.930 (2.80)  Time: 0.803s, 1275.45/s  (0.788s, 1298.75/s)  LR: 1.000e-05  Data: 0.009 (0.013)
Train: 605 [ 650/1251 ( 52%)]  Loss: 3.270 (2.83)  Time: 0.773s, 1325.47/s  (0.790s, 1296.01/s)  LR: 1.000e-05  Data: 0.009 (0.013)
Train: 605 [ 700/1251 ( 56%)]  Loss: 3.100 (2.85)  Time: 0.774s, 1323.24/s  (0.789s, 1297.80/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 605 [ 750/1251 ( 60%)]  Loss: 2.974 (2.86)  Time: 0.773s, 1325.38/s  (0.790s, 1295.74/s)  LR: 1.000e-05  Data: 0.009 (0.012)
Train: 605 [ 800/1251 ( 64%)]  Loss: 2.942 (2.86)  Time: 0.834s, 1227.45/s  (0.791s, 1295.38/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 605 [ 850/1251 ( 68%)]  Loss: 2.791 (2.86)  Time: 0.770s, 1329.59/s  (0.790s, 1296.30/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 605 [ 900/1251 ( 72%)]  Loss: 2.880 (2.86)  Time: 0.790s, 1296.48/s  (0.790s, 1296.35/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 605 [ 950/1251 ( 76%)]  Loss: 3.272 (2.88)  Time: 0.803s, 1274.44/s  (0.790s, 1295.73/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 605 [1000/1251 ( 80%)]  Loss: 3.079 (2.89)  Time: 0.774s, 1323.54/s  (0.790s, 1295.63/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 605 [1050/1251 ( 84%)]  Loss: 2.770 (2.88)  Time: 0.772s, 1326.72/s  (0.790s, 1296.49/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 605 [1100/1251 ( 88%)]  Loss: 3.219 (2.90)  Time: 0.773s, 1323.86/s  (0.790s, 1296.85/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 605 [1150/1251 ( 92%)]  Loss: 2.866 (2.90)  Time: 0.825s, 1241.22/s  (0.790s, 1296.43/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 605 [1200/1251 ( 96%)]  Loss: 2.511 (2.88)  Time: 0.771s, 1328.80/s  (0.789s, 1297.19/s)  LR: 1.000e-05  Data: 0.009 (0.012)
Train: 605 [1250/1251 (100%)]  Loss: 2.836 (2.88)  Time: 0.805s, 1272.83/s  (0.790s, 1296.58/s)  LR: 1.000e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.599 (1.599)  Loss:  0.6118 (0.6118)  Acc@1: 92.3828 (92.3828)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.194 (0.575)  Loss:  0.7231 (1.0475)  Acc@1: 87.5000 (81.0720)  Acc@5: 98.3491 (95.5260)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-599.pth.tar', 81.17599987060547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-585.pth.tar', 81.08600005371093)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-581.pth.tar', 81.07200002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-605.pth.tar', 81.072)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-577.pth.tar', 81.07000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-591.pth.tar', 81.06599994873046)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-600.pth.tar', 81.062000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-584.pth.tar', 81.05799997314453)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-597.pth.tar', 81.04399989746094)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-579.pth.tar', 81.03)

Train: 606 [   0/1251 (  0%)]  Loss: 3.116 (3.12)  Time: 2.303s,  444.66/s  (2.303s,  444.66/s)  LR: 1.000e-05  Data: 1.512 (1.512)
Train: 606 [  50/1251 (  4%)]  Loss: 2.885 (3.00)  Time: 0.819s, 1249.95/s  (0.817s, 1253.76/s)  LR: 1.000e-05  Data: 0.010 (0.045)
Train: 606 [ 100/1251 (  8%)]  Loss: 2.969 (2.99)  Time: 0.775s, 1321.62/s  (0.798s, 1282.48/s)  LR: 1.000e-05  Data: 0.011 (0.027)
Train: 606 [ 150/1251 ( 12%)]  Loss: 3.067 (3.01)  Time: 0.773s, 1325.41/s  (0.793s, 1292.02/s)  LR: 1.000e-05  Data: 0.010 (0.022)
Train: 606 [ 200/1251 ( 16%)]  Loss: 3.049 (3.02)  Time: 0.809s, 1265.56/s  (0.793s, 1291.06/s)  LR: 1.000e-05  Data: 0.010 (0.019)
Train: 606 [ 250/1251 ( 20%)]  Loss: 3.097 (3.03)  Time: 0.773s, 1325.53/s  (0.792s, 1293.72/s)  LR: 1.000e-05  Data: 0.010 (0.017)
Train: 606 [ 300/1251 ( 24%)]  Loss: 3.064 (3.04)  Time: 0.775s, 1322.01/s  (0.790s, 1295.76/s)  LR: 1.000e-05  Data: 0.009 (0.016)
Train: 606 [ 350/1251 ( 28%)]  Loss: 2.824 (3.01)  Time: 0.815s, 1256.94/s  (0.790s, 1296.60/s)  LR: 1.000e-05  Data: 0.011 (0.015)
Train: 606 [ 400/1251 ( 32%)]  Loss: 2.669 (2.97)  Time: 0.774s, 1322.97/s  (0.788s, 1298.99/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 606 [ 450/1251 ( 36%)]  Loss: 2.902 (2.96)  Time: 0.814s, 1258.39/s  (0.788s, 1300.13/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 606 [ 500/1251 ( 40%)]  Loss: 2.758 (2.95)  Time: 0.807s, 1268.32/s  (0.789s, 1298.17/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 606 [ 550/1251 ( 44%)]  Loss: 2.810 (2.93)  Time: 0.820s, 1249.53/s  (0.789s, 1297.99/s)  LR: 1.000e-05  Data: 0.009 (0.013)
Train: 606 [ 600/1251 ( 48%)]  Loss: 2.952 (2.94)  Time: 0.807s, 1269.48/s  (0.790s, 1295.89/s)  LR: 1.000e-05  Data: 0.009 (0.013)
Train: 606 [ 650/1251 ( 52%)]  Loss: 2.880 (2.93)  Time: 0.841s, 1218.08/s  (0.792s, 1292.98/s)  LR: 1.000e-05  Data: 0.009 (0.013)
Train: 606 [ 700/1251 ( 56%)]  Loss: 2.875 (2.93)  Time: 0.819s, 1249.61/s  (0.792s, 1293.00/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 606 [ 750/1251 ( 60%)]  Loss: 2.456 (2.90)  Time: 0.819s, 1250.27/s  (0.793s, 1291.89/s)  LR: 1.000e-05  Data: 0.009 (0.012)
Train: 606 [ 800/1251 ( 64%)]  Loss: 2.697 (2.89)  Time: 0.787s, 1301.27/s  (0.793s, 1290.79/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 606 [ 850/1251 ( 68%)]  Loss: 3.004 (2.89)  Time: 0.772s, 1325.79/s  (0.794s, 1290.46/s)  LR: 1.000e-05  Data: 0.009 (0.012)
Train: 606 [ 900/1251 ( 72%)]  Loss: 2.852 (2.89)  Time: 0.776s, 1320.03/s  (0.793s, 1291.59/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 606 [ 950/1251 ( 76%)]  Loss: 3.164 (2.90)  Time: 0.807s, 1269.23/s  (0.793s, 1291.21/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 606 [1000/1251 ( 80%)]  Loss: 2.825 (2.90)  Time: 0.772s, 1326.97/s  (0.793s, 1291.37/s)  LR: 1.000e-05  Data: 0.009 (0.012)
Train: 606 [1050/1251 ( 84%)]  Loss: 2.936 (2.90)  Time: 0.777s, 1318.03/s  (0.793s, 1291.43/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 606 [1100/1251 ( 88%)]  Loss: 2.654 (2.89)  Time: 0.783s, 1307.31/s  (0.792s, 1292.76/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 606 [1150/1251 ( 92%)]  Loss: 2.701 (2.88)  Time: 0.834s, 1228.37/s  (0.792s, 1293.00/s)  LR: 1.000e-05  Data: 0.011 (0.011)
Train: 606 [1200/1251 ( 96%)]  Loss: 2.722 (2.88)  Time: 0.771s, 1327.43/s  (0.792s, 1292.67/s)  LR: 1.000e-05  Data: 0.010 (0.011)
Train: 606 [1250/1251 (100%)]  Loss: 2.672 (2.87)  Time: 0.758s, 1350.78/s  (0.791s, 1293.80/s)  LR: 1.000e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.612 (1.612)  Loss:  0.6011 (0.6011)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.194 (0.570)  Loss:  0.7002 (1.0265)  Acc@1: 87.5000 (81.0460)  Acc@5: 98.4670 (95.5560)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-599.pth.tar', 81.17599987060547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-585.pth.tar', 81.08600005371093)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-581.pth.tar', 81.07200002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-605.pth.tar', 81.072)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-577.pth.tar', 81.07000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-591.pth.tar', 81.06599994873046)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-600.pth.tar', 81.062000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-584.pth.tar', 81.05799997314453)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-606.pth.tar', 81.04600012939453)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-597.pth.tar', 81.04399989746094)

Train: 607 [   0/1251 (  0%)]  Loss: 3.150 (3.15)  Time: 2.258s,  453.46/s  (2.258s,  453.46/s)  LR: 1.000e-05  Data: 1.533 (1.533)
Train: 607 [  50/1251 (  4%)]  Loss: 2.918 (3.03)  Time: 0.777s, 1318.38/s  (0.816s, 1254.16/s)  LR: 1.000e-05  Data: 0.010 (0.045)
Train: 607 [ 100/1251 (  8%)]  Loss: 3.098 (3.06)  Time: 0.774s, 1323.82/s  (0.798s, 1282.50/s)  LR: 1.000e-05  Data: 0.010 (0.028)
Train: 607 [ 150/1251 ( 12%)]  Loss: 2.746 (2.98)  Time: 0.778s, 1316.54/s  (0.796s, 1287.13/s)  LR: 1.000e-05  Data: 0.010 (0.022)
Train: 607 [ 200/1251 ( 16%)]  Loss: 2.626 (2.91)  Time: 0.773s, 1324.81/s  (0.793s, 1291.18/s)  LR: 1.000e-05  Data: 0.009 (0.019)
Train: 607 [ 250/1251 ( 20%)]  Loss: 2.668 (2.87)  Time: 0.814s, 1257.89/s  (0.792s, 1292.63/s)  LR: 1.000e-05  Data: 0.011 (0.017)
Train: 607 [ 300/1251 ( 24%)]  Loss: 3.035 (2.89)  Time: 0.779s, 1314.91/s  (0.791s, 1295.02/s)  LR: 1.000e-05  Data: 0.014 (0.016)
Train: 607 [ 350/1251 ( 28%)]  Loss: 2.619 (2.86)  Time: 0.775s, 1321.31/s  (0.791s, 1294.95/s)  LR: 1.000e-05  Data: 0.009 (0.015)
Train: 607 [ 400/1251 ( 32%)]  Loss: 3.193 (2.89)  Time: 0.771s, 1327.73/s  (0.790s, 1296.38/s)  LR: 1.000e-05  Data: 0.009 (0.014)
Train: 607 [ 450/1251 ( 36%)]  Loss: 3.143 (2.92)  Time: 0.784s, 1306.27/s  (0.789s, 1297.13/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 607 [ 500/1251 ( 40%)]  Loss: 2.851 (2.91)  Time: 0.778s, 1316.87/s  (0.789s, 1297.30/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 607 [ 550/1251 ( 44%)]  Loss: 3.029 (2.92)  Time: 0.774s, 1322.35/s  (0.788s, 1299.07/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 607 [ 600/1251 ( 48%)]  Loss: 2.987 (2.93)  Time: 0.811s, 1261.90/s  (0.788s, 1299.24/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 607 [ 650/1251 ( 52%)]  Loss: 3.014 (2.93)  Time: 0.773s, 1325.54/s  (0.787s, 1300.53/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 607 [ 700/1251 ( 56%)]  Loss: 3.093 (2.94)  Time: 0.774s, 1323.11/s  (0.787s, 1301.23/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 607 [ 750/1251 ( 60%)]  Loss: 2.276 (2.90)  Time: 0.771s, 1328.98/s  (0.787s, 1300.54/s)  LR: 1.000e-05  Data: 0.009 (0.012)
Train: 607 [ 800/1251 ( 64%)]  Loss: 2.943 (2.91)  Time: 0.772s, 1325.61/s  (0.788s, 1299.73/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 607 [ 850/1251 ( 68%)]  Loss: 2.775 (2.90)  Time: 0.831s, 1232.50/s  (0.788s, 1299.77/s)  LR: 1.000e-05  Data: 0.013 (0.012)
Train: 607 [ 900/1251 ( 72%)]  Loss: 2.995 (2.90)  Time: 0.775s, 1321.40/s  (0.787s, 1300.65/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 607 [ 950/1251 ( 76%)]  Loss: 3.021 (2.91)  Time: 0.777s, 1318.33/s  (0.787s, 1301.06/s)  LR: 1.000e-05  Data: 0.009 (0.012)
Train: 607 [1000/1251 ( 80%)]  Loss: 2.758 (2.90)  Time: 0.770s, 1329.53/s  (0.787s, 1301.09/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 607 [1050/1251 ( 84%)]  Loss: 2.887 (2.90)  Time: 0.772s, 1326.76/s  (0.787s, 1300.43/s)  LR: 1.000e-05  Data: 0.009 (0.012)
Train: 607 [1100/1251 ( 88%)]  Loss: 3.065 (2.91)  Time: 0.773s, 1324.08/s  (0.788s, 1299.27/s)  LR: 1.000e-05  Data: 0.010 (0.011)
Train: 607 [1150/1251 ( 92%)]  Loss: 3.073 (2.92)  Time: 0.815s, 1256.92/s  (0.788s, 1299.18/s)  LR: 1.000e-05  Data: 0.011 (0.011)
Train: 607 [1200/1251 ( 96%)]  Loss: 2.575 (2.90)  Time: 0.769s, 1330.98/s  (0.788s, 1298.86/s)  LR: 1.000e-05  Data: 0.009 (0.011)
Train: 607 [1250/1251 (100%)]  Loss: 2.933 (2.90)  Time: 0.795s, 1288.20/s  (0.788s, 1299.17/s)  LR: 1.000e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.547 (1.547)  Loss:  0.6318 (0.6318)  Acc@1: 92.2852 (92.2852)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.194 (0.566)  Loss:  0.7207 (1.0511)  Acc@1: 87.3821 (81.0000)  Acc@5: 98.2311 (95.5500)
Train: 608 [   0/1251 (  0%)]  Loss: 2.960 (2.96)  Time: 2.391s,  428.24/s  (2.391s,  428.24/s)  LR: 1.000e-05  Data: 1.556 (1.556)
Train: 608 [  50/1251 (  4%)]  Loss: 3.123 (3.04)  Time: 0.773s, 1324.96/s  (0.837s, 1223.15/s)  LR: 1.000e-05  Data: 0.009 (0.044)
Train: 608 [ 100/1251 (  8%)]  Loss: 2.682 (2.92)  Time: 0.772s, 1326.69/s  (0.817s, 1253.64/s)  LR: 1.000e-05  Data: 0.010 (0.027)
Train: 608 [ 150/1251 ( 12%)]  Loss: 3.288 (3.01)  Time: 0.818s, 1252.35/s  (0.814s, 1258.06/s)  LR: 1.000e-05  Data: 0.009 (0.021)
Train: 608 [ 200/1251 ( 16%)]  Loss: 2.988 (3.01)  Time: 0.774s, 1323.19/s  (0.807s, 1268.74/s)  LR: 1.000e-05  Data: 0.010 (0.019)
Train: 608 [ 250/1251 ( 20%)]  Loss: 2.884 (2.99)  Time: 0.810s, 1264.72/s  (0.802s, 1276.79/s)  LR: 1.000e-05  Data: 0.010 (0.017)
Train: 608 [ 300/1251 ( 24%)]  Loss: 3.025 (2.99)  Time: 0.778s, 1316.21/s  (0.798s, 1282.55/s)  LR: 1.000e-05  Data: 0.009 (0.016)
Train: 608 [ 350/1251 ( 28%)]  Loss: 3.045 (3.00)  Time: 0.802s, 1276.26/s  (0.796s, 1285.73/s)  LR: 1.000e-05  Data: 0.009 (0.015)
Train: 608 [ 400/1251 ( 32%)]  Loss: 2.943 (2.99)  Time: 0.815s, 1256.21/s  (0.795s, 1288.05/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 608 [ 450/1251 ( 36%)]  Loss: 3.130 (3.01)  Time: 0.872s, 1174.73/s  (0.795s, 1288.68/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 608 [ 500/1251 ( 40%)]  Loss: 3.264 (3.03)  Time: 0.782s, 1309.57/s  (0.793s, 1290.99/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 608 [ 550/1251 ( 44%)]  Loss: 2.830 (3.01)  Time: 0.815s, 1256.77/s  (0.794s, 1290.00/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 608 [ 600/1251 ( 48%)]  Loss: 3.146 (3.02)  Time: 0.774s, 1323.42/s  (0.793s, 1290.56/s)  LR: 1.000e-05  Data: 0.009 (0.013)
Train: 608 [ 650/1251 ( 52%)]  Loss: 2.652 (3.00)  Time: 0.815s, 1256.29/s  (0.793s, 1291.08/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 608 [ 700/1251 ( 56%)]  Loss: 2.828 (2.99)  Time: 0.773s, 1324.85/s  (0.795s, 1288.81/s)  LR: 1.000e-05  Data: 0.009 (0.012)
Train: 608 [ 750/1251 ( 60%)]  Loss: 2.949 (2.98)  Time: 0.771s, 1327.76/s  (0.795s, 1288.42/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 608 [ 800/1251 ( 64%)]  Loss: 2.940 (2.98)  Time: 0.806s, 1270.83/s  (0.794s, 1288.97/s)  LR: 1.000e-05  Data: 0.009 (0.012)
Train: 608 [ 850/1251 ( 68%)]  Loss: 2.764 (2.97)  Time: 0.773s, 1325.38/s  (0.794s, 1289.05/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 608 [ 900/1251 ( 72%)]  Loss: 2.947 (2.97)  Time: 0.776s, 1319.78/s  (0.794s, 1290.15/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 608 [ 950/1251 ( 76%)]  Loss: 2.928 (2.97)  Time: 0.769s, 1331.44/s  (0.794s, 1290.37/s)  LR: 1.000e-05  Data: 0.009 (0.012)
Train: 608 [1000/1251 ( 80%)]  Loss: 2.986 (2.97)  Time: 0.808s, 1267.48/s  (0.794s, 1289.72/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 608 [1050/1251 ( 84%)]  Loss: 2.980 (2.97)  Time: 0.782s, 1308.72/s  (0.795s, 1288.49/s)  LR: 1.000e-05  Data: 0.009 (0.011)
Train: 608 [1100/1251 ( 88%)]  Loss: 2.941 (2.97)  Time: 0.858s, 1193.68/s  (0.795s, 1288.29/s)  LR: 1.000e-05  Data: 0.014 (0.011)
Train: 608 [1150/1251 ( 92%)]  Loss: 2.812 (2.96)  Time: 0.810s, 1263.43/s  (0.794s, 1289.00/s)  LR: 1.000e-05  Data: 0.010 (0.011)
Train: 608 [1200/1251 ( 96%)]  Loss: 2.904 (2.96)  Time: 0.848s, 1207.71/s  (0.795s, 1288.70/s)  LR: 1.000e-05  Data: 0.010 (0.011)
Train: 608 [1250/1251 (100%)]  Loss: 2.902 (2.96)  Time: 0.799s, 1281.97/s  (0.795s, 1288.21/s)  LR: 1.000e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.552 (1.552)  Loss:  0.6494 (0.6494)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.7305 (98.7305)
Test: [  48/48]  Time: 0.194 (0.570)  Loss:  0.7441 (1.0747)  Acc@1: 87.6179 (80.9840)  Acc@5: 98.1132 (95.5940)
Train: 609 [   0/1251 (  0%)]  Loss: 2.983 (2.98)  Time: 2.282s,  448.77/s  (2.282s,  448.77/s)  LR: 1.000e-05  Data: 1.553 (1.553)
Train: 609 [  50/1251 (  4%)]  Loss: 2.582 (2.78)  Time: 0.779s, 1315.17/s  (0.832s, 1230.87/s)  LR: 1.000e-05  Data: 0.010 (0.045)
Train: 609 [ 100/1251 (  8%)]  Loss: 2.987 (2.85)  Time: 0.771s, 1328.98/s  (0.805s, 1272.78/s)  LR: 1.000e-05  Data: 0.009 (0.027)
Train: 609 [ 150/1251 ( 12%)]  Loss: 2.735 (2.82)  Time: 0.770s, 1329.28/s  (0.798s, 1283.12/s)  LR: 1.000e-05  Data: 0.009 (0.022)
Train: 609 [ 200/1251 ( 16%)]  Loss: 2.655 (2.79)  Time: 0.775s, 1321.10/s  (0.793s, 1290.73/s)  LR: 1.000e-05  Data: 0.013 (0.019)
Train: 609 [ 250/1251 ( 20%)]  Loss: 3.032 (2.83)  Time: 0.780s, 1313.18/s  (0.790s, 1295.81/s)  LR: 1.000e-05  Data: 0.010 (0.017)
Train: 609 [ 300/1251 ( 24%)]  Loss: 2.968 (2.85)  Time: 0.771s, 1327.50/s  (0.789s, 1298.62/s)  LR: 1.000e-05  Data: 0.010 (0.016)
Train: 609 [ 350/1251 ( 28%)]  Loss: 3.001 (2.87)  Time: 0.778s, 1315.68/s  (0.787s, 1301.54/s)  LR: 1.000e-05  Data: 0.009 (0.015)
Train: 609 [ 400/1251 ( 32%)]  Loss: 3.211 (2.91)  Time: 0.806s, 1270.21/s  (0.787s, 1301.90/s)  LR: 1.000e-05  Data: 0.009 (0.014)
Train: 609 [ 450/1251 ( 36%)]  Loss: 2.940 (2.91)  Time: 0.773s, 1324.97/s  (0.786s, 1302.08/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 609 [ 500/1251 ( 40%)]  Loss: 2.842 (2.90)  Time: 0.787s, 1301.94/s  (0.787s, 1301.79/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 609 [ 550/1251 ( 44%)]  Loss: 2.977 (2.91)  Time: 0.857s, 1195.49/s  (0.786s, 1302.01/s)  LR: 1.000e-05  Data: 0.009 (0.013)
Train: 609 [ 600/1251 ( 48%)]  Loss: 2.802 (2.90)  Time: 0.808s, 1267.31/s  (0.789s, 1297.82/s)  LR: 1.000e-05  Data: 0.009 (0.013)
Train: 609 [ 650/1251 ( 52%)]  Loss: 3.077 (2.91)  Time: 0.858s, 1194.08/s  (0.790s, 1296.16/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 609 [ 700/1251 ( 56%)]  Loss: 3.106 (2.93)  Time: 0.773s, 1325.08/s  (0.791s, 1295.14/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 609 [ 750/1251 ( 60%)]  Loss: 3.126 (2.94)  Time: 0.773s, 1324.44/s  (0.791s, 1294.91/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 609 [ 800/1251 ( 64%)]  Loss: 3.076 (2.95)  Time: 0.797s, 1284.97/s  (0.790s, 1295.96/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 609 [ 850/1251 ( 68%)]  Loss: 2.765 (2.94)  Time: 0.817s, 1253.42/s  (0.790s, 1295.41/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 609 [ 900/1251 ( 72%)]  Loss: 2.635 (2.92)  Time: 0.773s, 1325.45/s  (0.790s, 1296.13/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 609 [ 950/1251 ( 76%)]  Loss: 2.667 (2.91)  Time: 0.773s, 1324.88/s  (0.790s, 1296.93/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 609 [1000/1251 ( 80%)]  Loss: 3.250 (2.92)  Time: 0.776s, 1319.44/s  (0.789s, 1297.74/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 609 [1050/1251 ( 84%)]  Loss: 2.901 (2.92)  Time: 0.774s, 1323.12/s  (0.789s, 1298.29/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 609 [1100/1251 ( 88%)]  Loss: 2.783 (2.92)  Time: 0.801s, 1278.20/s  (0.789s, 1298.54/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 609 [1150/1251 ( 92%)]  Loss: 2.765 (2.91)  Time: 0.772s, 1325.69/s  (0.789s, 1298.52/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 609 [1200/1251 ( 96%)]  Loss: 3.065 (2.92)  Time: 0.776s, 1320.28/s  (0.788s, 1299.04/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 609 [1250/1251 (100%)]  Loss: 2.789 (2.91)  Time: 0.767s, 1335.44/s  (0.788s, 1299.78/s)  LR: 1.000e-05  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.505 (1.505)  Loss:  0.5698 (0.5698)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.7305 (98.7305)
Test: [  48/48]  Time: 0.194 (0.566)  Loss:  0.6641 (0.9887)  Acc@1: 87.5000 (81.1220)  Acc@5: 98.1132 (95.5960)
Current checkpoints:
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-599.pth.tar', 81.17599987060547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-609.pth.tar', 81.122)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-585.pth.tar', 81.08600005371093)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-581.pth.tar', 81.07200002929687)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-605.pth.tar', 81.072)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-577.pth.tar', 81.07000002685547)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-591.pth.tar', 81.06599994873046)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-600.pth.tar', 81.062000078125)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-584.pth.tar', 81.05799997314453)
 ('./output/train/20220327-162107-hrnet32-224/checkpoint-606.pth.tar', 81.04600012939453)

*** Best metric: 81.17599987060547 (epoch 599)
