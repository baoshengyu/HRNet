/home/baoshengyu/anaconda3/lib/python3.7/site-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 6
Added key: store_based_barrier_key:1 to store for rank: 3
Added key: store_based_barrier_key:1 to store for rank: 7
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 4
Added key: store_based_barrier_key:1 to store for rank: 5
Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 4, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 6, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 5, total 8.
Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 8.
Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 7, total 8.
Model hrnet18 created, param count:21301174
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.875
AMP not enabled. Training in float32.
Using native Torch DistributedDataParallel.
Scheduled epochs: 310
Train: 0 [   0/1251 (  0%)]  Loss: 6.949 (6.95)  Time: 7.907s,  129.51/s  (7.907s,  129.51/s)  LR: 1.000e-06  Data: 1.687 (1.687)
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Train: 0 [  50/1251 (  4%)]  Loss: 6.944 (6.95)  Time: 0.782s, 1309.78/s  (0.973s, 1052.39/s)  LR: 1.000e-06  Data: 0.011 (0.043)
Train: 0 [ 100/1251 (  8%)]  Loss: 6.921 (6.94)  Time: 0.782s, 1309.67/s  (0.880s, 1163.93/s)  LR: 1.000e-06  Data: 0.010 (0.027)
Train: 0 [ 150/1251 ( 12%)]  Loss: 6.928 (6.94)  Time: 0.782s, 1310.25/s  (0.850s, 1204.63/s)  LR: 1.000e-06  Data: 0.009 (0.022)
Train: 0 [ 200/1251 ( 16%)]  Loss: 6.924 (6.93)  Time: 0.821s, 1246.55/s  (0.839s, 1219.87/s)  LR: 1.000e-06  Data: 0.011 (0.019)
Train: 0 [ 250/1251 ( 20%)]  Loss: 6.927 (6.93)  Time: 0.816s, 1254.27/s  (0.835s, 1225.78/s)  LR: 1.000e-06  Data: 0.011 (0.017)
Train: 0 [ 300/1251 ( 24%)]  Loss: 6.937 (6.93)  Time: 0.781s, 1311.88/s  (0.830s, 1234.43/s)  LR: 1.000e-06  Data: 0.010 (0.016)
Train: 0 [ 350/1251 ( 28%)]  Loss: 6.924 (6.93)  Time: 0.792s, 1292.56/s  (0.823s, 1244.11/s)  LR: 1.000e-06  Data: 0.010 (0.015)
Train: 0 [ 400/1251 ( 32%)]  Loss: 6.917 (6.93)  Time: 0.779s, 1314.34/s  (0.819s, 1250.71/s)  LR: 1.000e-06  Data: 0.009 (0.015)
Train: 0 [ 450/1251 ( 36%)]  Loss: 6.926 (6.93)  Time: 0.782s, 1310.07/s  (0.815s, 1256.31/s)  LR: 1.000e-06  Data: 0.010 (0.014)
Train: 0 [ 500/1251 ( 40%)]  Loss: 6.933 (6.93)  Time: 0.780s, 1313.47/s  (0.812s, 1260.68/s)  LR: 1.000e-06  Data: 0.010 (0.014)
Train: 0 [ 550/1251 ( 44%)]  Loss: 6.915 (6.93)  Time: 0.820s, 1248.95/s  (0.810s, 1263.85/s)  LR: 1.000e-06  Data: 0.009 (0.013)
Train: 0 [ 600/1251 ( 48%)]  Loss: 6.916 (6.93)  Time: 0.778s, 1315.82/s  (0.808s, 1266.91/s)  LR: 1.000e-06  Data: 0.009 (0.013)
Train: 0 [ 650/1251 ( 52%)]  Loss: 6.927 (6.93)  Time: 0.780s, 1313.48/s  (0.806s, 1269.95/s)  LR: 1.000e-06  Data: 0.009 (0.013)
Train: 0 [ 700/1251 ( 56%)]  Loss: 6.922 (6.93)  Time: 0.816s, 1254.33/s  (0.806s, 1271.02/s)  LR: 1.000e-06  Data: 0.010 (0.012)
Train: 0 [ 750/1251 ( 60%)]  Loss: 6.912 (6.93)  Time: 0.779s, 1314.94/s  (0.804s, 1273.58/s)  LR: 1.000e-06  Data: 0.009 (0.012)
Train: 0 [ 800/1251 ( 64%)]  Loss: 6.915 (6.93)  Time: 0.782s, 1308.67/s  (0.804s, 1273.92/s)  LR: 1.000e-06  Data: 0.010 (0.012)
Train: 0 [ 850/1251 ( 68%)]  Loss: 6.912 (6.92)  Time: 0.780s, 1312.56/s  (0.802s, 1276.18/s)  LR: 1.000e-06  Data: 0.010 (0.012)
Train: 0 [ 900/1251 ( 72%)]  Loss: 6.915 (6.92)  Time: 0.780s, 1313.36/s  (0.801s, 1277.80/s)  LR: 1.000e-06  Data: 0.011 (0.012)
Train: 0 [ 950/1251 ( 76%)]  Loss: 6.918 (6.92)  Time: 0.780s, 1312.21/s  (0.800s, 1279.33/s)  LR: 1.000e-06  Data: 0.011 (0.012)
Train: 0 [1000/1251 ( 80%)]  Loss: 6.908 (6.92)  Time: 0.786s, 1302.01/s  (0.799s, 1280.86/s)  LR: 1.000e-06  Data: 0.013 (0.012)
Train: 0 [1050/1251 ( 84%)]  Loss: 6.911 (6.92)  Time: 0.778s, 1316.56/s  (0.799s, 1282.24/s)  LR: 1.000e-06  Data: 0.011 (0.012)
Train: 0 [1100/1251 ( 88%)]  Loss: 6.913 (6.92)  Time: 0.780s, 1312.69/s  (0.798s, 1283.33/s)  LR: 1.000e-06  Data: 0.013 (0.012)
Train: 0 [1150/1251 ( 92%)]  Loss: 6.915 (6.92)  Time: 0.782s, 1310.12/s  (0.798s, 1283.24/s)  LR: 1.000e-06  Data: 0.009 (0.012)
Train: 0 [1200/1251 ( 96%)]  Loss: 6.921 (6.92)  Time: 0.817s, 1253.25/s  (0.798s, 1283.48/s)  LR: 1.000e-06  Data: 0.011 (0.011)
Train: 0 [1250/1251 (100%)]  Loss: 6.900 (6.92)  Time: 0.768s, 1333.76/s  (0.798s, 1282.98/s)  LR: 1.000e-06  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.992 (1.992)  Loss:  6.8060 (6.8060)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  1.5625 ( 1.5625)
Test: [  48/48]  Time: 3.065 (0.629)  Loss:  6.8243 (6.9026)  Acc@1:  1.4151 ( 0.1760)  Acc@5:  2.8302 ( 0.6760)
Current checkpoints:
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-0.pth.tar', 0.17600000061035156)

Train: 1 [   0/1251 (  0%)]  Loss: 6.910 (6.91)  Time: 3.024s,  338.57/s  (3.024s,  338.57/s)  LR: 2.008e-04  Data: 1.571 (1.571)
Train: 1 [  50/1251 (  4%)]  Loss: 6.914 (6.91)  Time: 0.778s, 1316.50/s  (0.832s, 1231.48/s)  LR: 2.008e-04  Data: 0.011 (0.041)
Train: 1 [ 100/1251 (  8%)]  Loss: 6.908 (6.91)  Time: 0.783s, 1308.45/s  (0.809s, 1265.55/s)  LR: 2.008e-04  Data: 0.011 (0.025)
Train: 1 [ 150/1251 ( 12%)]  Loss: 6.880 (6.90)  Time: 0.779s, 1314.80/s  (0.802s, 1276.93/s)  LR: 2.008e-04  Data: 0.011 (0.020)
Train: 1 [ 200/1251 ( 16%)]  Loss: 6.891 (6.90)  Time: 0.819s, 1250.75/s  (0.799s, 1280.90/s)  LR: 2.008e-04  Data: 0.010 (0.018)
Train: 1 [ 250/1251 ( 20%)]  Loss: 6.861 (6.89)  Time: 0.817s, 1254.09/s  (0.796s, 1285.98/s)  LR: 2.008e-04  Data: 0.011 (0.016)
Train: 1 [ 300/1251 ( 24%)]  Loss: 6.850 (6.89)  Time: 0.780s, 1313.46/s  (0.795s, 1287.74/s)  LR: 2.008e-04  Data: 0.010 (0.015)
Train: 1 [ 350/1251 ( 28%)]  Loss: 6.788 (6.88)  Time: 0.780s, 1313.35/s  (0.793s, 1291.10/s)  LR: 2.008e-04  Data: 0.010 (0.014)
Train: 1 [ 400/1251 ( 32%)]  Loss: 6.747 (6.86)  Time: 0.789s, 1298.26/s  (0.792s, 1292.33/s)  LR: 2.008e-04  Data: 0.012 (0.014)
Train: 1 [ 450/1251 ( 36%)]  Loss: 6.723 (6.85)  Time: 0.779s, 1313.77/s  (0.792s, 1293.29/s)  LR: 2.008e-04  Data: 0.009 (0.013)
Train: 1 [ 500/1251 ( 40%)]  Loss: 6.791 (6.84)  Time: 0.811s, 1261.87/s  (0.791s, 1294.01/s)  LR: 2.008e-04  Data: 0.010 (0.013)
Train: 1 [ 550/1251 ( 44%)]  Loss: 6.740 (6.83)  Time: 0.780s, 1312.58/s  (0.792s, 1292.48/s)  LR: 2.008e-04  Data: 0.010 (0.013)
Train: 1 [ 600/1251 ( 48%)]  Loss: 6.738 (6.83)  Time: 0.790s, 1295.80/s  (0.791s, 1293.90/s)  LR: 2.008e-04  Data: 0.010 (0.013)
Train: 1 [ 650/1251 ( 52%)]  Loss: 6.667 (6.81)  Time: 0.803s, 1275.31/s  (0.792s, 1293.70/s)  LR: 2.008e-04  Data: 0.009 (0.012)
Train: 1 [ 700/1251 ( 56%)]  Loss: 6.621 (6.80)  Time: 0.822s, 1246.38/s  (0.793s, 1291.98/s)  LR: 2.008e-04  Data: 0.010 (0.012)
Train: 1 [ 750/1251 ( 60%)]  Loss: 6.707 (6.80)  Time: 0.781s, 1310.50/s  (0.792s, 1293.10/s)  LR: 2.008e-04  Data: 0.010 (0.012)
Train: 1 [ 800/1251 ( 64%)]  Loss: 6.619 (6.79)  Time: 0.779s, 1314.64/s  (0.791s, 1293.76/s)  LR: 2.008e-04  Data: 0.009 (0.012)
Train: 1 [ 850/1251 ( 68%)]  Loss: 6.634 (6.78)  Time: 0.780s, 1312.78/s  (0.791s, 1293.99/s)  LR: 2.008e-04  Data: 0.009 (0.012)
Train: 1 [ 900/1251 ( 72%)]  Loss: 6.655 (6.77)  Time: 0.778s, 1315.79/s  (0.791s, 1294.99/s)  LR: 2.008e-04  Data: 0.009 (0.012)
Train: 1 [ 950/1251 ( 76%)]  Loss: 6.661 (6.77)  Time: 0.780s, 1312.53/s  (0.790s, 1295.93/s)  LR: 2.008e-04  Data: 0.010 (0.012)
Train: 1 [1000/1251 ( 80%)]  Loss: 6.588 (6.76)  Time: 0.782s, 1309.88/s  (0.791s, 1295.27/s)  LR: 2.008e-04  Data: 0.010 (0.012)
Train: 1 [1050/1251 ( 84%)]  Loss: 6.598 (6.75)  Time: 0.782s, 1309.72/s  (0.790s, 1296.00/s)  LR: 2.008e-04  Data: 0.011 (0.012)
Train: 1 [1100/1251 ( 88%)]  Loss: 6.467 (6.74)  Time: 0.780s, 1312.81/s  (0.790s, 1296.73/s)  LR: 2.008e-04  Data: 0.010 (0.012)
Train: 1 [1150/1251 ( 92%)]  Loss: 6.417 (6.72)  Time: 0.811s, 1262.38/s  (0.789s, 1297.29/s)  LR: 2.008e-04  Data: 0.012 (0.011)
Train: 1 [1200/1251 ( 96%)]  Loss: 6.605 (6.72)  Time: 0.781s, 1311.39/s  (0.789s, 1297.68/s)  LR: 2.008e-04  Data: 0.009 (0.011)
Train: 1 [1250/1251 (100%)]  Loss: 6.406 (6.71)  Time: 0.771s, 1327.37/s  (0.790s, 1296.91/s)  LR: 2.008e-04  Data: 0.000 (0.011)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.522 (1.522)  Loss:  5.7218 (5.7218)  Acc@1:  0.2930 ( 0.2930)  Acc@5: 11.7188 (11.7188)
Test: [  48/48]  Time: 0.171 (0.569)  Loss:  5.0273 (5.8105)  Acc@1: 18.0425 ( 3.1280)  Acc@5: 29.9528 (10.9000)
Current checkpoints:
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-1.pth.tar', 3.128000015869141)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-0.pth.tar', 0.17600000061035156)

Train: 2 [   0/1251 (  0%)]  Loss: 6.597 (6.60)  Time: 2.292s,  446.87/s  (2.292s,  446.87/s)  LR: 4.006e-04  Data: 1.554 (1.554)
Train: 2 [  50/1251 (  4%)]  Loss: 6.529 (6.56)  Time: 0.780s, 1312.07/s  (0.824s, 1242.69/s)  LR: 4.006e-04  Data: 0.011 (0.047)
Train: 2 [ 100/1251 (  8%)]  Loss: 6.496 (6.54)  Time: 0.785s, 1303.78/s  (0.808s, 1267.76/s)  LR: 4.006e-04  Data: 0.010 (0.029)
Train: 2 [ 150/1251 ( 12%)]  Loss: 6.611 (6.56)  Time: 0.787s, 1300.95/s  (0.804s, 1273.70/s)  LR: 4.006e-04  Data: 0.009 (0.023)
Train: 2 [ 200/1251 ( 16%)]  Loss: 6.506 (6.55)  Time: 0.818s, 1251.45/s  (0.800s, 1280.07/s)  LR: 4.006e-04  Data: 0.011 (0.019)
Train: 2 [ 250/1251 ( 20%)]  Loss: 6.434 (6.53)  Time: 0.825s, 1241.42/s  (0.800s, 1279.54/s)  LR: 4.006e-04  Data: 0.010 (0.018)
Train: 2 [ 300/1251 ( 24%)]  Loss: 6.532 (6.53)  Time: 0.814s, 1258.67/s  (0.800s, 1279.46/s)  LR: 4.006e-04  Data: 0.009 (0.016)
Train: 2 [ 350/1251 ( 28%)]  Loss: 6.501 (6.53)  Time: 0.781s, 1311.00/s  (0.798s, 1283.25/s)  LR: 4.006e-04  Data: 0.010 (0.015)
Train: 2 [ 400/1251 ( 32%)]  Loss: 6.563 (6.53)  Time: 0.778s, 1316.78/s  (0.796s, 1285.71/s)  LR: 4.006e-04  Data: 0.010 (0.015)
Train: 2 [ 450/1251 ( 36%)]  Loss: 6.476 (6.52)  Time: 0.780s, 1312.84/s  (0.795s, 1287.82/s)  LR: 4.006e-04  Data: 0.011 (0.014)
Train: 2 [ 500/1251 ( 40%)]  Loss: 6.358 (6.51)  Time: 0.790s, 1296.86/s  (0.794s, 1289.29/s)  LR: 4.006e-04  Data: 0.010 (0.014)
Train: 2 [ 550/1251 ( 44%)]  Loss: 6.318 (6.49)  Time: 0.848s, 1207.65/s  (0.793s, 1291.11/s)  LR: 4.006e-04  Data: 0.010 (0.013)
Train: 2 [ 600/1251 ( 48%)]  Loss: 6.330 (6.48)  Time: 0.780s, 1313.41/s  (0.793s, 1291.32/s)  LR: 4.006e-04  Data: 0.010 (0.013)
Train: 2 [ 650/1251 ( 52%)]  Loss: 6.290 (6.47)  Time: 0.791s, 1294.28/s  (0.792s, 1292.22/s)  LR: 4.006e-04  Data: 0.010 (0.013)
Train: 2 [ 700/1251 ( 56%)]  Loss: 6.130 (6.44)  Time: 0.779s, 1314.24/s  (0.792s, 1293.52/s)  LR: 4.006e-04  Data: 0.011 (0.013)
Train: 2 [ 750/1251 ( 60%)]  Loss: 6.306 (6.44)  Time: 0.780s, 1312.91/s  (0.791s, 1294.55/s)  LR: 4.006e-04  Data: 0.010 (0.013)
Train: 2 [ 800/1251 ( 64%)]  Loss: 6.320 (6.43)  Time: 0.778s, 1316.16/s  (0.791s, 1295.18/s)  LR: 4.006e-04  Data: 0.011 (0.012)
Train: 2 [ 850/1251 ( 68%)]  Loss: 6.342 (6.42)  Time: 0.780s, 1312.57/s  (0.790s, 1295.85/s)  LR: 4.006e-04  Data: 0.010 (0.012)
Train: 2 [ 900/1251 ( 72%)]  Loss: 6.253 (6.42)  Time: 0.781s, 1311.45/s  (0.790s, 1296.63/s)  LR: 4.006e-04  Data: 0.010 (0.012)
Train: 2 [ 950/1251 ( 76%)]  Loss: 6.201 (6.40)  Time: 0.782s, 1310.27/s  (0.789s, 1297.45/s)  LR: 4.006e-04  Data: 0.009 (0.012)
Train: 2 [1000/1251 ( 80%)]  Loss: 6.202 (6.40)  Time: 0.778s, 1315.44/s  (0.789s, 1298.10/s)  LR: 4.006e-04  Data: 0.010 (0.012)
Train: 2 [1050/1251 ( 84%)]  Loss: 6.370 (6.39)  Time: 0.778s, 1316.07/s  (0.789s, 1298.66/s)  LR: 4.006e-04  Data: 0.011 (0.012)
Train: 2 [1100/1251 ( 88%)]  Loss: 6.291 (6.39)  Time: 0.780s, 1313.38/s  (0.788s, 1299.24/s)  LR: 4.006e-04  Data: 0.010 (0.012)
Train: 2 [1150/1251 ( 92%)]  Loss: 6.159 (6.38)  Time: 0.823s, 1244.13/s  (0.789s, 1297.50/s)  LR: 4.006e-04  Data: 0.013 (0.012)
Train: 2 [1200/1251 ( 96%)]  Loss: 6.146 (6.37)  Time: 0.790s, 1295.41/s  (0.790s, 1296.66/s)  LR: 4.006e-04  Data: 0.009 (0.012)
Train: 2 [1250/1251 (100%)]  Loss: 5.991 (6.36)  Time: 0.776s, 1320.42/s  (0.790s, 1296.21/s)  LR: 4.006e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.520 (1.520)  Loss:  5.0434 (5.0434)  Acc@1:  9.5703 ( 9.5703)  Acc@5: 26.0742 (26.0742)
Test: [  48/48]  Time: 0.172 (0.561)  Loss:  4.4932 (5.1787)  Acc@1: 19.3396 ( 7.6080)  Acc@5: 36.9104 (21.3640)
Current checkpoints:
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-2.pth.tar', 7.6079999975585935)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-1.pth.tar', 3.128000015869141)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-0.pth.tar', 0.17600000061035156)

Train: 3 [   0/1251 (  0%)]  Loss: 6.213 (6.21)  Time: 2.350s,  435.70/s  (2.350s,  435.70/s)  LR: 6.004e-04  Data: 1.611 (1.611)
Train: 3 [  50/1251 (  4%)]  Loss: 6.304 (6.26)  Time: 0.825s, 1241.63/s  (0.830s, 1233.97/s)  LR: 6.004e-04  Data: 0.010 (0.046)
Train: 3 [ 100/1251 (  8%)]  Loss: 6.238 (6.25)  Time: 0.780s, 1312.56/s  (0.813s, 1259.85/s)  LR: 6.004e-04  Data: 0.010 (0.028)
Train: 3 [ 150/1251 ( 12%)]  Loss: 6.236 (6.25)  Time: 0.783s, 1308.13/s  (0.803s, 1274.58/s)  LR: 6.004e-04  Data: 0.010 (0.022)
Train: 3 [ 200/1251 ( 16%)]  Loss: 6.196 (6.24)  Time: 0.780s, 1312.31/s  (0.798s, 1283.12/s)  LR: 6.004e-04  Data: 0.010 (0.019)
Train: 3 [ 250/1251 ( 20%)]  Loss: 6.216 (6.23)  Time: 0.781s, 1311.08/s  (0.795s, 1288.82/s)  LR: 6.004e-04  Data: 0.011 (0.017)
Train: 3 [ 300/1251 ( 24%)]  Loss: 5.960 (6.19)  Time: 0.823s, 1244.42/s  (0.793s, 1291.21/s)  LR: 6.004e-04  Data: 0.009 (0.016)
Train: 3 [ 350/1251 ( 28%)]  Loss: 6.301 (6.21)  Time: 0.820s, 1248.61/s  (0.797s, 1285.00/s)  LR: 6.004e-04  Data: 0.011 (0.015)
Train: 3 [ 400/1251 ( 32%)]  Loss: 6.157 (6.20)  Time: 0.814s, 1257.80/s  (0.797s, 1284.18/s)  LR: 6.004e-04  Data: 0.009 (0.015)
Train: 3 [ 450/1251 ( 36%)]  Loss: 6.039 (6.19)  Time: 0.793s, 1290.93/s  (0.797s, 1285.37/s)  LR: 6.004e-04  Data: 0.009 (0.014)
Train: 3 [ 500/1251 ( 40%)]  Loss: 6.084 (6.18)  Time: 0.780s, 1312.82/s  (0.795s, 1287.77/s)  LR: 6.004e-04  Data: 0.010 (0.014)
Train: 3 [ 550/1251 ( 44%)]  Loss: 6.163 (6.18)  Time: 0.786s, 1302.39/s  (0.794s, 1289.22/s)  LR: 6.004e-04  Data: 0.010 (0.013)
Train: 3 [ 600/1251 ( 48%)]  Loss: 6.264 (6.18)  Time: 0.816s, 1255.30/s  (0.795s, 1287.96/s)  LR: 6.004e-04  Data: 0.011 (0.013)
Train: 3 [ 650/1251 ( 52%)]  Loss: 6.216 (6.18)  Time: 0.818s, 1251.18/s  (0.797s, 1285.42/s)  LR: 6.004e-04  Data: 0.011 (0.013)
Train: 3 [ 700/1251 ( 56%)]  Loss: 5.955 (6.17)  Time: 0.780s, 1313.09/s  (0.797s, 1284.27/s)  LR: 6.004e-04  Data: 0.011 (0.013)
Train: 3 [ 750/1251 ( 60%)]  Loss: 5.998 (6.16)  Time: 0.775s, 1322.13/s  (0.796s, 1286.00/s)  LR: 6.004e-04  Data: 0.009 (0.013)
Train: 3 [ 800/1251 ( 64%)]  Loss: 5.866 (6.14)  Time: 0.781s, 1311.61/s  (0.796s, 1287.14/s)  LR: 6.004e-04  Data: 0.011 (0.012)
Train: 3 [ 850/1251 ( 68%)]  Loss: 6.076 (6.14)  Time: 0.779s, 1314.24/s  (0.795s, 1288.20/s)  LR: 6.004e-04  Data: 0.010 (0.012)
Train: 3 [ 900/1251 ( 72%)]  Loss: 5.986 (6.13)  Time: 0.779s, 1314.45/s  (0.794s, 1289.40/s)  LR: 6.004e-04  Data: 0.009 (0.012)
Train: 3 [ 950/1251 ( 76%)]  Loss: 5.659 (6.11)  Time: 0.781s, 1311.70/s  (0.795s, 1288.32/s)  LR: 6.004e-04  Data: 0.009 (0.012)
Train: 3 [1000/1251 ( 80%)]  Loss: 6.046 (6.10)  Time: 0.822s, 1246.30/s  (0.794s, 1288.94/s)  LR: 6.004e-04  Data: 0.009 (0.012)
Train: 3 [1050/1251 ( 84%)]  Loss: 6.184 (6.11)  Time: 0.782s, 1309.95/s  (0.794s, 1289.83/s)  LR: 6.004e-04  Data: 0.010 (0.012)
Train: 3 [1100/1251 ( 88%)]  Loss: 5.719 (6.09)  Time: 0.795s, 1287.90/s  (0.794s, 1290.47/s)  LR: 6.004e-04  Data: 0.012 (0.012)
Train: 3 [1150/1251 ( 92%)]  Loss: 5.618 (6.07)  Time: 0.778s, 1316.88/s  (0.793s, 1290.59/s)  LR: 6.004e-04  Data: 0.010 (0.012)
Train: 3 [1200/1251 ( 96%)]  Loss: 5.945 (6.07)  Time: 0.782s, 1309.54/s  (0.793s, 1291.44/s)  LR: 6.004e-04  Data: 0.009 (0.012)
Train: 3 [1250/1251 (100%)]  Loss: 5.965 (6.06)  Time: 0.768s, 1332.96/s  (0.793s, 1292.03/s)  LR: 6.004e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.490 (1.490)  Loss:  4.3157 (4.3157)  Acc@1: 14.3555 (14.3555)  Acc@5: 34.8633 (34.8633)
Test: [  48/48]  Time: 0.172 (0.565)  Loss:  3.1777 (4.4561)  Acc@1: 38.5613 (15.4520)  Acc@5: 62.8538 (36.5360)
Current checkpoints:
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-3.pth.tar', 15.452000008544921)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-2.pth.tar', 7.6079999975585935)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-1.pth.tar', 3.128000015869141)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-0.pth.tar', 0.17600000061035156)

Train: 4 [   0/1251 (  0%)]  Loss: 5.863 (5.86)  Time: 2.230s,  459.27/s  (2.230s,  459.27/s)  LR: 8.002e-04  Data: 1.493 (1.493)
Train: 4 [  50/1251 (  4%)]  Loss: 6.016 (5.94)  Time: 0.777s, 1318.61/s  (0.839s, 1220.21/s)  LR: 8.002e-04  Data: 0.010 (0.045)
Train: 4 [ 100/1251 (  8%)]  Loss: 5.921 (5.93)  Time: 0.780s, 1313.27/s  (0.812s, 1261.69/s)  LR: 8.002e-04  Data: 0.009 (0.028)
Train: 4 [ 150/1251 ( 12%)]  Loss: 5.951 (5.94)  Time: 0.783s, 1307.32/s  (0.802s, 1276.25/s)  LR: 8.002e-04  Data: 0.010 (0.022)
Train: 4 [ 200/1251 ( 16%)]  Loss: 5.696 (5.89)  Time: 0.785s, 1305.04/s  (0.798s, 1283.40/s)  LR: 8.002e-04  Data: 0.009 (0.019)
Train: 4 [ 250/1251 ( 20%)]  Loss: 5.896 (5.89)  Time: 0.798s, 1283.34/s  (0.796s, 1287.22/s)  LR: 8.002e-04  Data: 0.009 (0.017)
Train: 4 [ 300/1251 ( 24%)]  Loss: 5.990 (5.90)  Time: 0.817s, 1252.87/s  (0.797s, 1285.10/s)  LR: 8.002e-04  Data: 0.010 (0.016)
Train: 4 [ 350/1251 ( 28%)]  Loss: 5.892 (5.90)  Time: 0.779s, 1313.98/s  (0.795s, 1287.55/s)  LR: 8.002e-04  Data: 0.009 (0.015)
Train: 4 [ 400/1251 ( 32%)]  Loss: 5.886 (5.90)  Time: 0.784s, 1306.81/s  (0.794s, 1289.54/s)  LR: 8.002e-04  Data: 0.011 (0.015)
Train: 4 [ 450/1251 ( 36%)]  Loss: 6.048 (5.92)  Time: 0.791s, 1295.36/s  (0.793s, 1291.77/s)  LR: 8.002e-04  Data: 0.011 (0.014)
Train: 4 [ 500/1251 ( 40%)]  Loss: 5.695 (5.90)  Time: 0.788s, 1299.94/s  (0.792s, 1293.49/s)  LR: 8.002e-04  Data: 0.010 (0.014)
Train: 4 [ 550/1251 ( 44%)]  Loss: 5.782 (5.89)  Time: 0.818s, 1251.62/s  (0.794s, 1290.31/s)  LR: 8.002e-04  Data: 0.011 (0.013)
Train: 4 [ 600/1251 ( 48%)]  Loss: 5.699 (5.87)  Time: 0.818s, 1251.66/s  (0.795s, 1287.43/s)  LR: 8.002e-04  Data: 0.010 (0.013)
Train: 4 [ 650/1251 ( 52%)]  Loss: 5.689 (5.86)  Time: 0.789s, 1297.79/s  (0.796s, 1286.67/s)  LR: 8.002e-04  Data: 0.011 (0.013)
Train: 4 [ 700/1251 ( 56%)]  Loss: 5.853 (5.86)  Time: 0.863s, 1186.84/s  (0.795s, 1287.70/s)  LR: 8.002e-04  Data: 0.010 (0.013)
Train: 4 [ 750/1251 ( 60%)]  Loss: 5.779 (5.85)  Time: 0.797s, 1285.41/s  (0.795s, 1288.65/s)  LR: 8.002e-04  Data: 0.009 (0.013)
Train: 4 [ 800/1251 ( 64%)]  Loss: 5.695 (5.84)  Time: 0.780s, 1312.96/s  (0.794s, 1289.75/s)  LR: 8.002e-04  Data: 0.011 (0.013)
Train: 4 [ 850/1251 ( 68%)]  Loss: 5.744 (5.84)  Time: 0.807s, 1269.47/s  (0.793s, 1290.67/s)  LR: 8.002e-04  Data: 0.010 (0.012)
Train: 4 [ 900/1251 ( 72%)]  Loss: 5.934 (5.84)  Time: 0.784s, 1306.50/s  (0.794s, 1289.48/s)  LR: 8.002e-04  Data: 0.009 (0.012)
Train: 4 [ 950/1251 ( 76%)]  Loss: 5.678 (5.84)  Time: 0.783s, 1308.15/s  (0.794s, 1290.32/s)  LR: 8.002e-04  Data: 0.010 (0.012)
Train: 4 [1000/1251 ( 80%)]  Loss: 5.462 (5.82)  Time: 0.782s, 1309.95/s  (0.793s, 1290.96/s)  LR: 8.002e-04  Data: 0.011 (0.012)
Train: 4 [1050/1251 ( 84%)]  Loss: 5.586 (5.81)  Time: 0.780s, 1312.69/s  (0.793s, 1291.76/s)  LR: 8.002e-04  Data: 0.011 (0.012)
Train: 4 [1100/1251 ( 88%)]  Loss: 5.758 (5.80)  Time: 0.819s, 1250.84/s  (0.793s, 1291.45/s)  LR: 8.002e-04  Data: 0.009 (0.012)
Train: 4 [1150/1251 ( 92%)]  Loss: 5.721 (5.80)  Time: 0.781s, 1311.70/s  (0.793s, 1291.03/s)  LR: 8.002e-04  Data: 0.010 (0.012)
Train: 4 [1200/1251 ( 96%)]  Loss: 5.690 (5.80)  Time: 0.778s, 1316.65/s  (0.793s, 1291.77/s)  LR: 8.002e-04  Data: 0.011 (0.012)
Train: 4 [1250/1251 (100%)]  Loss: 5.697 (5.79)  Time: 0.770s, 1330.33/s  (0.792s, 1292.28/s)  LR: 8.002e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.557 (1.557)  Loss:  3.2894 (3.2894)  Acc@1: 31.3477 (31.3477)  Acc@5: 63.0859 (63.0859)
Test: [  48/48]  Time: 0.172 (0.567)  Loss:  2.8672 (3.9763)  Acc@1: 48.4670 (21.1660)  Acc@5: 68.3962 (45.0940)
Current checkpoints:
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-4.pth.tar', 21.166000045166015)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-3.pth.tar', 15.452000008544921)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-2.pth.tar', 7.6079999975585935)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-1.pth.tar', 3.128000015869141)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-0.pth.tar', 0.17600000061035156)

Train: 5 [   0/1251 (  0%)]  Loss: 5.589 (5.59)  Time: 2.302s,  444.84/s  (2.302s,  444.84/s)  LR: 9.993e-04  Data: 1.566 (1.566)
Train: 5 [  50/1251 (  4%)]  Loss: 5.656 (5.62)  Time: 0.780s, 1312.82/s  (0.820s, 1248.36/s)  LR: 9.993e-04  Data: 0.009 (0.043)
Train: 5 [ 100/1251 (  8%)]  Loss: 5.655 (5.63)  Time: 0.779s, 1315.07/s  (0.801s, 1278.91/s)  LR: 9.993e-04  Data: 0.010 (0.027)
Train: 5 [ 150/1251 ( 12%)]  Loss: 5.648 (5.64)  Time: 0.820s, 1248.75/s  (0.799s, 1281.92/s)  LR: 9.993e-04  Data: 0.011 (0.021)
Train: 5 [ 200/1251 ( 16%)]  Loss: 5.490 (5.61)  Time: 0.779s, 1314.93/s  (0.801s, 1278.03/s)  LR: 9.993e-04  Data: 0.010 (0.019)
Train: 5 [ 250/1251 ( 20%)]  Loss: 5.515 (5.59)  Time: 0.781s, 1310.85/s  (0.799s, 1281.28/s)  LR: 9.993e-04  Data: 0.011 (0.017)
Train: 5 [ 300/1251 ( 24%)]  Loss: 5.435 (5.57)  Time: 0.787s, 1300.95/s  (0.798s, 1283.76/s)  LR: 9.993e-04  Data: 0.011 (0.016)
Train: 5 [ 350/1251 ( 28%)]  Loss: 5.586 (5.57)  Time: 0.817s, 1253.00/s  (0.797s, 1284.73/s)  LR: 9.993e-04  Data: 0.011 (0.015)
Train: 5 [ 400/1251 ( 32%)]  Loss: 5.771 (5.59)  Time: 0.780s, 1312.03/s  (0.796s, 1285.86/s)  LR: 9.993e-04  Data: 0.011 (0.015)
Train: 5 [ 450/1251 ( 36%)]  Loss: 5.580 (5.59)  Time: 0.780s, 1313.03/s  (0.795s, 1288.58/s)  LR: 9.993e-04  Data: 0.011 (0.014)
Train: 5 [ 500/1251 ( 40%)]  Loss: 5.489 (5.58)  Time: 0.784s, 1306.91/s  (0.794s, 1289.49/s)  LR: 9.993e-04  Data: 0.011 (0.014)
Train: 5 [ 550/1251 ( 44%)]  Loss: 5.593 (5.58)  Time: 0.783s, 1307.22/s  (0.796s, 1286.08/s)  LR: 9.993e-04  Data: 0.010 (0.014)
Train: 5 [ 600/1251 ( 48%)]  Loss: 5.419 (5.57)  Time: 0.781s, 1311.69/s  (0.797s, 1284.11/s)  LR: 9.993e-04  Data: 0.010 (0.013)
Train: 5 [ 650/1251 ( 52%)]  Loss: 5.685 (5.58)  Time: 0.788s, 1299.74/s  (0.796s, 1285.91/s)  LR: 9.993e-04  Data: 0.009 (0.013)
Train: 5 [ 700/1251 ( 56%)]  Loss: 5.326 (5.56)  Time: 0.819s, 1250.16/s  (0.796s, 1286.18/s)  LR: 9.993e-04  Data: 0.011 (0.013)
Train: 5 [ 750/1251 ( 60%)]  Loss: 5.869 (5.58)  Time: 0.782s, 1309.15/s  (0.797s, 1284.68/s)  LR: 9.993e-04  Data: 0.011 (0.013)
Train: 5 [ 800/1251 ( 64%)]  Loss: 4.985 (5.55)  Time: 0.780s, 1312.32/s  (0.797s, 1285.13/s)  LR: 9.993e-04  Data: 0.011 (0.013)
Train: 5 [ 850/1251 ( 68%)]  Loss: 5.721 (5.56)  Time: 0.788s, 1300.22/s  (0.796s, 1286.23/s)  LR: 9.993e-04  Data: 0.010 (0.012)
Train: 5 [ 900/1251 ( 72%)]  Loss: 5.619 (5.56)  Time: 0.779s, 1314.79/s  (0.796s, 1286.72/s)  LR: 9.993e-04  Data: 0.011 (0.012)
Train: 5 [ 950/1251 ( 76%)]  Loss: 5.557 (5.56)  Time: 0.783s, 1308.20/s  (0.795s, 1287.87/s)  LR: 9.993e-04  Data: 0.011 (0.012)
Train: 5 [1000/1251 ( 80%)]  Loss: 5.178 (5.54)  Time: 0.778s, 1316.65/s  (0.795s, 1288.80/s)  LR: 9.993e-04  Data: 0.010 (0.012)
Train: 5 [1050/1251 ( 84%)]  Loss: 5.455 (5.54)  Time: 0.811s, 1263.25/s  (0.794s, 1289.48/s)  LR: 9.993e-04  Data: 0.011 (0.012)
Train: 5 [1100/1251 ( 88%)]  Loss: 5.621 (5.54)  Time: 0.794s, 1290.43/s  (0.794s, 1289.97/s)  LR: 9.993e-04  Data: 0.009 (0.012)
Train: 5 [1150/1251 ( 92%)]  Loss: 5.255 (5.53)  Time: 0.781s, 1310.69/s  (0.794s, 1290.39/s)  LR: 9.993e-04  Data: 0.010 (0.012)
Train: 5 [1200/1251 ( 96%)]  Loss: 5.426 (5.52)  Time: 0.787s, 1301.15/s  (0.793s, 1290.80/s)  LR: 9.993e-04  Data: 0.012 (0.012)
Train: 5 [1250/1251 (100%)]  Loss: 5.061 (5.51)  Time: 0.804s, 1273.56/s  (0.794s, 1290.18/s)  LR: 9.993e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.540 (1.540)  Loss:  2.6524 (2.6524)  Acc@1: 42.8711 (42.8711)  Acc@5: 73.6328 (73.6328)
Test: [  48/48]  Time: 0.171 (0.575)  Loss:  2.3972 (3.4685)  Acc@1: 50.1179 (28.7460)  Acc@5: 72.7594 (55.3020)
Current checkpoints:
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-5.pth.tar', 28.74600005126953)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-4.pth.tar', 21.166000045166015)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-3.pth.tar', 15.452000008544921)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-2.pth.tar', 7.6079999975585935)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-1.pth.tar', 3.128000015869141)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-0.pth.tar', 0.17600000061035156)

Train: 6 [   0/1251 (  0%)]  Loss: 5.280 (5.28)  Time: 2.218s,  461.66/s  (2.218s,  461.66/s)  LR: 9.990e-04  Data: 1.483 (1.483)
Train: 6 [  50/1251 (  4%)]  Loss: 5.559 (5.42)  Time: 0.780s, 1312.70/s  (0.815s, 1257.15/s)  LR: 9.990e-04  Data: 0.008 (0.044)
Train: 6 [ 100/1251 (  8%)]  Loss: 5.512 (5.45)  Time: 0.780s, 1312.97/s  (0.801s, 1278.09/s)  LR: 9.990e-04  Data: 0.011 (0.027)
Train: 6 [ 150/1251 ( 12%)]  Loss: 5.220 (5.39)  Time: 0.783s, 1307.01/s  (0.797s, 1285.52/s)  LR: 9.990e-04  Data: 0.010 (0.021)
Train: 6 [ 200/1251 ( 16%)]  Loss: 5.464 (5.41)  Time: 0.782s, 1310.20/s  (0.794s, 1289.14/s)  LR: 9.990e-04  Data: 0.010 (0.018)
Train: 6 [ 250/1251 ( 20%)]  Loss: 5.486 (5.42)  Time: 0.818s, 1251.42/s  (0.797s, 1285.50/s)  LR: 9.990e-04  Data: 0.011 (0.017)
Train: 6 [ 300/1251 ( 24%)]  Loss: 5.530 (5.44)  Time: 0.781s, 1311.46/s  (0.796s, 1287.21/s)  LR: 9.990e-04  Data: 0.009 (0.016)
Train: 6 [ 350/1251 ( 28%)]  Loss: 5.229 (5.41)  Time: 0.780s, 1312.84/s  (0.794s, 1289.70/s)  LR: 9.990e-04  Data: 0.010 (0.015)
Train: 6 [ 400/1251 ( 32%)]  Loss: 5.137 (5.38)  Time: 0.821s, 1247.59/s  (0.793s, 1290.90/s)  LR: 9.990e-04  Data: 0.011 (0.014)
Train: 6 [ 450/1251 ( 36%)]  Loss: 5.306 (5.37)  Time: 0.779s, 1314.13/s  (0.793s, 1291.80/s)  LR: 9.990e-04  Data: 0.010 (0.014)
Train: 6 [ 500/1251 ( 40%)]  Loss: 5.415 (5.38)  Time: 0.819s, 1250.58/s  (0.793s, 1292.04/s)  LR: 9.990e-04  Data: 0.011 (0.013)
Train: 6 [ 550/1251 ( 44%)]  Loss: 5.576 (5.39)  Time: 0.816s, 1254.58/s  (0.795s, 1288.34/s)  LR: 9.990e-04  Data: 0.010 (0.013)
Train: 6 [ 600/1251 ( 48%)]  Loss: 5.534 (5.40)  Time: 0.779s, 1314.81/s  (0.796s, 1286.87/s)  LR: 9.990e-04  Data: 0.010 (0.013)
Train: 6 [ 650/1251 ( 52%)]  Loss: 5.376 (5.40)  Time: 0.783s, 1308.06/s  (0.795s, 1288.29/s)  LR: 9.990e-04  Data: 0.009 (0.013)
Train: 6 [ 700/1251 ( 56%)]  Loss: 5.272 (5.39)  Time: 0.779s, 1314.84/s  (0.794s, 1289.49/s)  LR: 9.990e-04  Data: 0.010 (0.013)
Train: 6 [ 750/1251 ( 60%)]  Loss: 5.231 (5.38)  Time: 0.781s, 1310.69/s  (0.793s, 1290.73/s)  LR: 9.990e-04  Data: 0.010 (0.012)
Train: 6 [ 800/1251 ( 64%)]  Loss: 4.960 (5.36)  Time: 0.780s, 1313.48/s  (0.793s, 1291.95/s)  LR: 9.990e-04  Data: 0.011 (0.012)
Train: 6 [ 850/1251 ( 68%)]  Loss: 5.245 (5.35)  Time: 0.781s, 1310.83/s  (0.792s, 1293.00/s)  LR: 9.990e-04  Data: 0.010 (0.012)
Train: 6 [ 900/1251 ( 72%)]  Loss: 5.366 (5.35)  Time: 0.780s, 1312.90/s  (0.793s, 1291.70/s)  LR: 9.990e-04  Data: 0.011 (0.012)
Train: 6 [ 950/1251 ( 76%)]  Loss: 5.371 (5.35)  Time: 0.788s, 1299.84/s  (0.792s, 1292.64/s)  LR: 9.990e-04  Data: 0.010 (0.012)
Train: 6 [1000/1251 ( 80%)]  Loss: 5.333 (5.35)  Time: 0.834s, 1228.29/s  (0.793s, 1290.73/s)  LR: 9.990e-04  Data: 0.011 (0.012)
Train: 6 [1050/1251 ( 84%)]  Loss: 5.015 (5.34)  Time: 0.784s, 1305.82/s  (0.793s, 1290.55/s)  LR: 9.990e-04  Data: 0.010 (0.012)
Train: 6 [1100/1251 ( 88%)]  Loss: 5.181 (5.33)  Time: 0.780s, 1313.45/s  (0.793s, 1290.85/s)  LR: 9.990e-04  Data: 0.010 (0.012)
Train: 6 [1150/1251 ( 92%)]  Loss: 5.116 (5.32)  Time: 0.792s, 1292.73/s  (0.793s, 1290.86/s)  LR: 9.990e-04  Data: 0.010 (0.012)
Train: 6 [1200/1251 ( 96%)]  Loss: 5.372 (5.32)  Time: 0.790s, 1296.02/s  (0.794s, 1290.37/s)  LR: 9.990e-04  Data: 0.012 (0.012)
Train: 6 [1250/1251 (100%)]  Loss: 5.274 (5.32)  Time: 0.768s, 1332.93/s  (0.793s, 1291.21/s)  LR: 9.990e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.547 (1.547)  Loss:  2.8928 (2.8928)  Acc@1: 44.7266 (44.7266)  Acc@5: 74.0234 (74.0234)
Test: [  48/48]  Time: 0.171 (0.561)  Loss:  2.7150 (3.3615)  Acc@1: 53.7736 (33.9620)  Acc@5: 74.6462 (60.9340)
Current checkpoints:
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-6.pth.tar', 33.961999958496094)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-5.pth.tar', 28.74600005126953)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-4.pth.tar', 21.166000045166015)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-3.pth.tar', 15.452000008544921)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-2.pth.tar', 7.6079999975585935)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-1.pth.tar', 3.128000015869141)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-0.pth.tar', 0.17600000061035156)

Train: 7 [   0/1251 (  0%)]  Loss: 5.235 (5.24)  Time: 2.293s,  446.56/s  (2.293s,  446.56/s)  LR: 9.987e-04  Data: 1.554 (1.554)
Train: 7 [  50/1251 (  4%)]  Loss: 5.053 (5.14)  Time: 0.781s, 1311.34/s  (0.821s, 1247.27/s)  LR: 9.987e-04  Data: 0.011 (0.045)
Train: 7 [ 100/1251 (  8%)]  Loss: 5.284 (5.19)  Time: 0.782s, 1309.26/s  (0.803s, 1275.05/s)  LR: 9.987e-04  Data: 0.011 (0.028)
Train: 7 [ 150/1251 ( 12%)]  Loss: 4.988 (5.14)  Time: 0.781s, 1311.85/s  (0.797s, 1285.23/s)  LR: 9.987e-04  Data: 0.009 (0.022)
Train: 7 [ 200/1251 ( 16%)]  Loss: 4.964 (5.10)  Time: 0.779s, 1315.13/s  (0.794s, 1289.88/s)  LR: 9.987e-04  Data: 0.010 (0.019)
Train: 7 [ 250/1251 ( 20%)]  Loss: 4.972 (5.08)  Time: 0.809s, 1266.23/s  (0.792s, 1293.24/s)  LR: 9.987e-04  Data: 0.011 (0.017)
Train: 7 [ 300/1251 ( 24%)]  Loss: 5.408 (5.13)  Time: 0.781s, 1311.88/s  (0.791s, 1293.98/s)  LR: 9.987e-04  Data: 0.010 (0.016)
Train: 7 [ 350/1251 ( 28%)]  Loss: 5.302 (5.15)  Time: 0.815s, 1255.85/s  (0.794s, 1290.34/s)  LR: 9.987e-04  Data: 0.010 (0.015)
Train: 7 [ 400/1251 ( 32%)]  Loss: 5.297 (5.17)  Time: 0.779s, 1313.79/s  (0.794s, 1289.57/s)  LR: 9.987e-04  Data: 0.011 (0.015)
Train: 7 [ 450/1251 ( 36%)]  Loss: 4.963 (5.15)  Time: 0.825s, 1240.80/s  (0.795s, 1287.33/s)  LR: 9.987e-04  Data: 0.012 (0.014)
Train: 7 [ 500/1251 ( 40%)]  Loss: 5.507 (5.18)  Time: 0.784s, 1306.63/s  (0.795s, 1288.57/s)  LR: 9.987e-04  Data: 0.010 (0.014)
Train: 7 [ 550/1251 ( 44%)]  Loss: 5.453 (5.20)  Time: 0.803s, 1275.38/s  (0.795s, 1288.58/s)  LR: 9.987e-04  Data: 0.008 (0.013)
Train: 7 [ 600/1251 ( 48%)]  Loss: 5.186 (5.20)  Time: 0.779s, 1315.01/s  (0.794s, 1289.81/s)  LR: 9.987e-04  Data: 0.009 (0.013)
Train: 7 [ 650/1251 ( 52%)]  Loss: 5.037 (5.19)  Time: 0.791s, 1294.47/s  (0.793s, 1291.25/s)  LR: 9.987e-04  Data: 0.010 (0.013)
Train: 7 [ 700/1251 ( 56%)]  Loss: 5.307 (5.20)  Time: 0.782s, 1310.18/s  (0.792s, 1292.50/s)  LR: 9.987e-04  Data: 0.012 (0.013)
Train: 7 [ 750/1251 ( 60%)]  Loss: 4.899 (5.18)  Time: 0.795s, 1287.98/s  (0.792s, 1293.28/s)  LR: 9.987e-04  Data: 0.010 (0.013)
Train: 7 [ 800/1251 ( 64%)]  Loss: 5.225 (5.18)  Time: 0.780s, 1313.31/s  (0.792s, 1293.47/s)  LR: 9.987e-04  Data: 0.010 (0.012)
Train: 7 [ 850/1251 ( 68%)]  Loss: 5.026 (5.17)  Time: 0.777s, 1318.00/s  (0.791s, 1294.12/s)  LR: 9.987e-04  Data: 0.011 (0.012)
Train: 7 [ 900/1251 ( 72%)]  Loss: 5.012 (5.16)  Time: 0.818s, 1252.56/s  (0.792s, 1292.83/s)  LR: 9.987e-04  Data: 0.011 (0.012)
Train: 7 [ 950/1251 ( 76%)]  Loss: 5.399 (5.18)  Time: 0.831s, 1232.69/s  (0.793s, 1290.53/s)  LR: 9.987e-04  Data: 0.010 (0.012)
Train: 7 [1000/1251 ( 80%)]  Loss: 5.254 (5.18)  Time: 0.824s, 1243.18/s  (0.795s, 1288.50/s)  LR: 9.987e-04  Data: 0.010 (0.012)
Train: 7 [1050/1251 ( 84%)]  Loss: 5.182 (5.18)  Time: 0.780s, 1312.79/s  (0.795s, 1288.42/s)  LR: 9.987e-04  Data: 0.012 (0.012)
Train: 7 [1100/1251 ( 88%)]  Loss: 5.263 (5.18)  Time: 0.780s, 1312.85/s  (0.794s, 1289.27/s)  LR: 9.987e-04  Data: 0.009 (0.012)
Train: 7 [1150/1251 ( 92%)]  Loss: 4.990 (5.18)  Time: 0.778s, 1316.28/s  (0.795s, 1288.80/s)  LR: 9.987e-04  Data: 0.010 (0.012)
Train: 7 [1200/1251 ( 96%)]  Loss: 5.619 (5.19)  Time: 0.782s, 1310.01/s  (0.794s, 1289.54/s)  LR: 9.987e-04  Data: 0.011 (0.012)
Train: 7 [1250/1251 (100%)]  Loss: 5.478 (5.20)  Time: 0.768s, 1332.68/s  (0.794s, 1290.40/s)  LR: 9.987e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.506 (1.506)  Loss:  2.0347 (2.0347)  Acc@1: 57.0312 (57.0312)  Acc@5: 84.0820 (84.0820)
Test: [  48/48]  Time: 0.172 (0.574)  Loss:  1.8306 (2.9323)  Acc@1: 63.9151 (38.5300)  Acc@5: 82.9009 (65.6820)
Current checkpoints:
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-7.pth.tar', 38.52999996826172)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-6.pth.tar', 33.961999958496094)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-5.pth.tar', 28.74600005126953)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-4.pth.tar', 21.166000045166015)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-3.pth.tar', 15.452000008544921)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-2.pth.tar', 7.6079999975585935)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-1.pth.tar', 3.128000015869141)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-0.pth.tar', 0.17600000061035156)

Train: 8 [   0/1251 (  0%)]  Loss: 5.282 (5.28)  Time: 2.420s,  423.16/s  (2.420s,  423.16/s)  LR: 9.983e-04  Data: 1.622 (1.622)
Train: 8 [  50/1251 (  4%)]  Loss: 4.924 (5.10)  Time: 0.819s, 1251.04/s  (0.835s, 1225.99/s)  LR: 9.983e-04  Data: 0.009 (0.047)
Train: 8 [ 100/1251 (  8%)]  Loss: 5.395 (5.20)  Time: 0.792s, 1293.52/s  (0.817s, 1253.22/s)  LR: 9.983e-04  Data: 0.010 (0.028)
Train: 8 [ 150/1251 ( 12%)]  Loss: 4.925 (5.13)  Time: 0.779s, 1314.60/s  (0.805s, 1271.47/s)  LR: 9.983e-04  Data: 0.011 (0.022)
Train: 8 [ 200/1251 ( 16%)]  Loss: 5.088 (5.12)  Time: 0.789s, 1297.43/s  (0.800s, 1280.75/s)  LR: 9.983e-04  Data: 0.009 (0.019)
Train: 8 [ 250/1251 ( 20%)]  Loss: 4.764 (5.06)  Time: 0.785s, 1304.38/s  (0.797s, 1285.09/s)  LR: 9.983e-04  Data: 0.011 (0.018)
Train: 8 [ 300/1251 ( 24%)]  Loss: 5.388 (5.11)  Time: 0.779s, 1314.44/s  (0.795s, 1287.52/s)  LR: 9.983e-04  Data: 0.010 (0.016)
Train: 8 [ 350/1251 ( 28%)]  Loss: 4.946 (5.09)  Time: 0.840s, 1218.87/s  (0.797s, 1285.62/s)  LR: 9.983e-04  Data: 0.010 (0.016)
Train: 8 [ 400/1251 ( 32%)]  Loss: 4.914 (5.07)  Time: 0.782s, 1309.74/s  (0.797s, 1284.76/s)  LR: 9.983e-04  Data: 0.011 (0.015)
Train: 8 [ 450/1251 ( 36%)]  Loss: 4.985 (5.06)  Time: 0.779s, 1315.24/s  (0.796s, 1287.18/s)  LR: 9.983e-04  Data: 0.010 (0.014)
Train: 8 [ 500/1251 ( 40%)]  Loss: 5.173 (5.07)  Time: 0.788s, 1299.02/s  (0.794s, 1289.16/s)  LR: 9.983e-04  Data: 0.011 (0.014)
Train: 8 [ 550/1251 ( 44%)]  Loss: 4.908 (5.06)  Time: 0.779s, 1314.54/s  (0.793s, 1291.11/s)  LR: 9.983e-04  Data: 0.010 (0.014)
Train: 8 [ 600/1251 ( 48%)]  Loss: 5.084 (5.06)  Time: 0.777s, 1317.90/s  (0.793s, 1290.79/s)  LR: 9.983e-04  Data: 0.010 (0.013)
Train: 8 [ 650/1251 ( 52%)]  Loss: 5.309 (5.08)  Time: 0.780s, 1312.53/s  (0.792s, 1292.22/s)  LR: 9.983e-04  Data: 0.009 (0.013)
Train: 8 [ 700/1251 ( 56%)]  Loss: 5.184 (5.08)  Time: 0.822s, 1245.82/s  (0.793s, 1291.84/s)  LR: 9.983e-04  Data: 0.010 (0.013)
Train: 8 [ 750/1251 ( 60%)]  Loss: 4.922 (5.07)  Time: 0.780s, 1312.39/s  (0.793s, 1290.58/s)  LR: 9.983e-04  Data: 0.010 (0.013)
Train: 8 [ 800/1251 ( 64%)]  Loss: 5.143 (5.08)  Time: 0.781s, 1311.26/s  (0.793s, 1291.61/s)  LR: 9.983e-04  Data: 0.011 (0.013)
Train: 8 [ 850/1251 ( 68%)]  Loss: 5.286 (5.09)  Time: 0.780s, 1312.89/s  (0.792s, 1292.26/s)  LR: 9.983e-04  Data: 0.009 (0.012)
Train: 8 [ 900/1251 ( 72%)]  Loss: 4.850 (5.08)  Time: 0.779s, 1313.79/s  (0.792s, 1293.02/s)  LR: 9.983e-04  Data: 0.010 (0.012)
Train: 8 [ 950/1251 ( 76%)]  Loss: 4.597 (5.05)  Time: 0.779s, 1314.36/s  (0.792s, 1293.31/s)  LR: 9.983e-04  Data: 0.009 (0.012)
Train: 8 [1000/1251 ( 80%)]  Loss: 4.966 (5.05)  Time: 0.778s, 1316.98/s  (0.791s, 1293.92/s)  LR: 9.983e-04  Data: 0.011 (0.012)
Train: 8 [1050/1251 ( 84%)]  Loss: 4.909 (5.04)  Time: 0.778s, 1315.37/s  (0.791s, 1294.23/s)  LR: 9.983e-04  Data: 0.010 (0.012)
Train: 8 [1100/1251 ( 88%)]  Loss: 5.019 (5.04)  Time: 0.779s, 1315.07/s  (0.792s, 1293.00/s)  LR: 9.983e-04  Data: 0.010 (0.012)
Train: 8 [1150/1251 ( 92%)]  Loss: 4.714 (5.03)  Time: 0.781s, 1311.65/s  (0.792s, 1293.43/s)  LR: 9.983e-04  Data: 0.011 (0.012)
Train: 8 [1200/1251 ( 96%)]  Loss: 5.379 (5.04)  Time: 0.797s, 1284.09/s  (0.792s, 1293.13/s)  LR: 9.983e-04  Data: 0.009 (0.012)
Train: 8 [1250/1251 (100%)]  Loss: 5.202 (5.05)  Time: 0.818s, 1252.56/s  (0.792s, 1293.24/s)  LR: 9.983e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.494 (1.494)  Loss:  1.9985 (1.9985)  Acc@1: 58.7891 (58.7891)  Acc@5: 83.2031 (83.2031)
Test: [  48/48]  Time: 0.171 (0.561)  Loss:  2.1141 (2.8405)  Acc@1: 61.2028 (40.7840)  Acc@5: 76.8868 (67.8480)
Current checkpoints:
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-8.pth.tar', 40.78400001831055)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-7.pth.tar', 38.52999996826172)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-6.pth.tar', 33.961999958496094)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-5.pth.tar', 28.74600005126953)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-4.pth.tar', 21.166000045166015)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-3.pth.tar', 15.452000008544921)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-2.pth.tar', 7.6079999975585935)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-1.pth.tar', 3.128000015869141)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-0.pth.tar', 0.17600000061035156)

Train: 9 [   0/1251 (  0%)]  Loss: 4.722 (4.72)  Time: 2.364s,  433.15/s  (2.364s,  433.15/s)  LR: 9.978e-04  Data: 1.626 (1.626)
Train: 9 [  50/1251 (  4%)]  Loss: 4.923 (4.82)  Time: 0.779s, 1313.95/s  (0.829s, 1234.99/s)  LR: 9.978e-04  Data: 0.011 (0.047)
Train: 9 [ 100/1251 (  8%)]  Loss: 4.353 (4.67)  Time: 0.791s, 1294.87/s  (0.808s, 1267.94/s)  LR: 9.978e-04  Data: 0.010 (0.029)
Train: 9 [ 150/1251 ( 12%)]  Loss: 5.124 (4.78)  Time: 0.822s, 1245.97/s  (0.810s, 1264.31/s)  LR: 9.978e-04  Data: 0.010 (0.023)
Train: 9 [ 200/1251 ( 16%)]  Loss: 4.970 (4.82)  Time: 0.779s, 1314.55/s  (0.806s, 1270.68/s)  LR: 9.978e-04  Data: 0.010 (0.020)
Train: 9 [ 250/1251 ( 20%)]  Loss: 4.869 (4.83)  Time: 0.781s, 1311.66/s  (0.804s, 1273.01/s)  LR: 9.978e-04  Data: 0.010 (0.018)
Train: 9 [ 300/1251 ( 24%)]  Loss: 4.858 (4.83)  Time: 0.817s, 1253.45/s  (0.804s, 1273.08/s)  LR: 9.978e-04  Data: 0.011 (0.016)
Train: 9 [ 350/1251 ( 28%)]  Loss: 4.616 (4.80)  Time: 0.778s, 1316.99/s  (0.803s, 1275.92/s)  LR: 9.978e-04  Data: 0.010 (0.016)
Train: 9 [ 400/1251 ( 32%)]  Loss: 4.992 (4.83)  Time: 0.793s, 1290.71/s  (0.801s, 1278.32/s)  LR: 9.978e-04  Data: 0.010 (0.015)
Train: 9 [ 450/1251 ( 36%)]  Loss: 5.057 (4.85)  Time: 0.779s, 1314.22/s  (0.799s, 1281.18/s)  LR: 9.978e-04  Data: 0.011 (0.014)
Train: 9 [ 500/1251 ( 40%)]  Loss: 4.760 (4.84)  Time: 0.780s, 1313.60/s  (0.797s, 1284.04/s)  LR: 9.978e-04  Data: 0.011 (0.014)
Train: 9 [ 550/1251 ( 44%)]  Loss: 5.133 (4.86)  Time: 0.800s, 1279.24/s  (0.796s, 1285.76/s)  LR: 9.978e-04  Data: 0.011 (0.014)
Train: 9 [ 600/1251 ( 48%)]  Loss: 4.765 (4.86)  Time: 0.780s, 1312.59/s  (0.795s, 1287.64/s)  LR: 9.978e-04  Data: 0.010 (0.013)
Train: 9 [ 650/1251 ( 52%)]  Loss: 4.484 (4.83)  Time: 0.783s, 1307.98/s  (0.794s, 1289.31/s)  LR: 9.978e-04  Data: 0.009 (0.013)
Train: 9 [ 700/1251 ( 56%)]  Loss: 5.073 (4.85)  Time: 0.786s, 1303.04/s  (0.793s, 1290.53/s)  LR: 9.978e-04  Data: 0.009 (0.013)
Train: 9 [ 750/1251 ( 60%)]  Loss: 5.185 (4.87)  Time: 0.777s, 1317.18/s  (0.793s, 1290.73/s)  LR: 9.978e-04  Data: 0.010 (0.013)
Train: 9 [ 800/1251 ( 64%)]  Loss: 4.773 (4.86)  Time: 0.779s, 1314.58/s  (0.793s, 1291.78/s)  LR: 9.978e-04  Data: 0.010 (0.013)
Train: 9 [ 850/1251 ( 68%)]  Loss: 4.654 (4.85)  Time: 0.820s, 1249.29/s  (0.794s, 1289.36/s)  LR: 9.978e-04  Data: 0.010 (0.012)
Train: 9 [ 900/1251 ( 72%)]  Loss: 4.957 (4.86)  Time: 0.779s, 1315.20/s  (0.794s, 1289.67/s)  LR: 9.978e-04  Data: 0.010 (0.012)
Train: 9 [ 950/1251 ( 76%)]  Loss: 5.195 (4.87)  Time: 0.779s, 1314.60/s  (0.795s, 1288.44/s)  LR: 9.978e-04  Data: 0.010 (0.012)
Train: 9 [1000/1251 ( 80%)]  Loss: 4.944 (4.88)  Time: 0.780s, 1313.41/s  (0.795s, 1288.07/s)  LR: 9.978e-04  Data: 0.011 (0.012)
Train: 9 [1050/1251 ( 84%)]  Loss: 4.899 (4.88)  Time: 0.782s, 1309.10/s  (0.794s, 1288.94/s)  LR: 9.978e-04  Data: 0.011 (0.012)
Train: 9 [1100/1251 ( 88%)]  Loss: 4.773 (4.87)  Time: 0.791s, 1295.26/s  (0.794s, 1289.65/s)  LR: 9.978e-04  Data: 0.009 (0.012)
Train: 9 [1150/1251 ( 92%)]  Loss: 4.969 (4.88)  Time: 0.778s, 1316.51/s  (0.794s, 1289.97/s)  LR: 9.978e-04  Data: 0.011 (0.012)
Train: 9 [1200/1251 ( 96%)]  Loss: 4.705 (4.87)  Time: 0.828s, 1236.25/s  (0.794s, 1290.16/s)  LR: 9.978e-04  Data: 0.011 (0.012)
Train: 9 [1250/1251 (100%)]  Loss: 4.853 (4.87)  Time: 0.769s, 1331.51/s  (0.794s, 1290.41/s)  LR: 9.978e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.509 (1.509)  Loss:  2.1721 (2.1721)  Acc@1: 62.7930 (62.7930)  Acc@5: 86.0352 (86.0352)
Test: [  48/48]  Time: 0.171 (0.573)  Loss:  1.9533 (2.7147)  Acc@1: 64.7406 (45.5880)  Acc@5: 84.3160 (72.3760)
Current checkpoints:
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-9.pth.tar', 45.58800006835938)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-8.pth.tar', 40.78400001831055)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-7.pth.tar', 38.52999996826172)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-6.pth.tar', 33.961999958496094)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-5.pth.tar', 28.74600005126953)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-4.pth.tar', 21.166000045166015)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-3.pth.tar', 15.452000008544921)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-2.pth.tar', 7.6079999975585935)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-1.pth.tar', 3.128000015869141)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-0.pth.tar', 0.17600000061035156)

Train: 10 [   0/1251 (  0%)]  Loss: 4.860 (4.86)  Time: 2.346s,  436.46/s  (2.346s,  436.46/s)  LR: 9.973e-04  Data: 1.602 (1.602)
Train: 10 [  50/1251 (  4%)]  Loss: 4.699 (4.78)  Time: 0.832s, 1230.69/s  (0.822s, 1245.46/s)  LR: 9.973e-04  Data: 0.011 (0.042)
Train: 10 [ 100/1251 (  8%)]  Loss: 4.863 (4.81)  Time: 0.830s, 1234.20/s  (0.812s, 1261.43/s)  LR: 9.973e-04  Data: 0.010 (0.026)
Train: 10 [ 150/1251 ( 12%)]  Loss: 4.601 (4.76)  Time: 0.817s, 1252.79/s  (0.808s, 1266.79/s)  LR: 9.973e-04  Data: 0.011 (0.021)
Train: 10 [ 200/1251 ( 16%)]  Loss: 4.971 (4.80)  Time: 0.792s, 1293.69/s  (0.811s, 1262.80/s)  LR: 9.973e-04  Data: 0.010 (0.018)
Train: 10 [ 250/1251 ( 20%)]  Loss: 4.636 (4.77)  Time: 0.780s, 1312.73/s  (0.806s, 1269.99/s)  LR: 9.973e-04  Data: 0.010 (0.017)
Train: 10 [ 300/1251 ( 24%)]  Loss: 4.667 (4.76)  Time: 0.787s, 1301.49/s  (0.803s, 1275.42/s)  LR: 9.973e-04  Data: 0.009 (0.015)
Train: 10 [ 350/1251 ( 28%)]  Loss: 4.903 (4.78)  Time: 0.782s, 1308.77/s  (0.800s, 1280.17/s)  LR: 9.973e-04  Data: 0.011 (0.015)
Train: 10 [ 400/1251 ( 32%)]  Loss: 4.882 (4.79)  Time: 0.786s, 1303.51/s  (0.798s, 1283.01/s)  LR: 9.973e-04  Data: 0.010 (0.014)
Train: 10 [ 450/1251 ( 36%)]  Loss: 4.863 (4.79)  Time: 0.781s, 1311.26/s  (0.796s, 1285.79/s)  LR: 9.973e-04  Data: 0.011 (0.014)
Train: 10 [ 500/1251 ( 40%)]  Loss: 5.110 (4.82)  Time: 0.780s, 1312.83/s  (0.795s, 1287.57/s)  LR: 9.973e-04  Data: 0.011 (0.013)
Train: 10 [ 550/1251 ( 44%)]  Loss: 4.962 (4.83)  Time: 0.784s, 1306.28/s  (0.795s, 1288.17/s)  LR: 9.973e-04  Data: 0.010 (0.013)
Train: 10 [ 600/1251 ( 48%)]  Loss: 5.061 (4.85)  Time: 0.783s, 1308.36/s  (0.794s, 1289.63/s)  LR: 9.973e-04  Data: 0.012 (0.013)
Train: 10 [ 650/1251 ( 52%)]  Loss: 4.783 (4.85)  Time: 0.777s, 1318.48/s  (0.793s, 1291.27/s)  LR: 9.973e-04  Data: 0.010 (0.013)
Train: 10 [ 700/1251 ( 56%)]  Loss: 4.708 (4.84)  Time: 0.780s, 1313.29/s  (0.793s, 1291.76/s)  LR: 9.973e-04  Data: 0.011 (0.012)
Train: 10 [ 750/1251 ( 60%)]  Loss: 4.595 (4.82)  Time: 0.779s, 1314.24/s  (0.792s, 1292.92/s)  LR: 9.973e-04  Data: 0.010 (0.012)
Train: 10 [ 800/1251 ( 64%)]  Loss: 4.861 (4.83)  Time: 0.781s, 1310.42/s  (0.791s, 1294.02/s)  LR: 9.973e-04  Data: 0.011 (0.012)
Train: 10 [ 850/1251 ( 68%)]  Loss: 4.769 (4.82)  Time: 0.782s, 1309.50/s  (0.791s, 1294.83/s)  LR: 9.973e-04  Data: 0.011 (0.012)
Train: 10 [ 900/1251 ( 72%)]  Loss: 4.683 (4.81)  Time: 0.784s, 1306.51/s  (0.791s, 1295.14/s)  LR: 9.973e-04  Data: 0.011 (0.012)
Train: 10 [ 950/1251 ( 76%)]  Loss: 4.612 (4.80)  Time: 0.785s, 1304.82/s  (0.790s, 1295.74/s)  LR: 9.973e-04  Data: 0.011 (0.012)
Train: 10 [1000/1251 ( 80%)]  Loss: 4.927 (4.81)  Time: 0.783s, 1307.85/s  (0.790s, 1296.33/s)  LR: 9.973e-04  Data: 0.009 (0.012)
Train: 10 [1050/1251 ( 84%)]  Loss: 4.785 (4.81)  Time: 0.781s, 1311.25/s  (0.790s, 1296.87/s)  LR: 9.973e-04  Data: 0.011 (0.012)
Train: 10 [1100/1251 ( 88%)]  Loss: 4.739 (4.81)  Time: 0.780s, 1312.09/s  (0.789s, 1297.14/s)  LR: 9.973e-04  Data: 0.011 (0.012)
Train: 10 [1150/1251 ( 92%)]  Loss: 4.834 (4.81)  Time: 0.784s, 1306.35/s  (0.789s, 1297.44/s)  LR: 9.973e-04  Data: 0.017 (0.012)
Train: 10 [1200/1251 ( 96%)]  Loss: 4.604 (4.80)  Time: 0.782s, 1309.91/s  (0.789s, 1297.84/s)  LR: 9.973e-04  Data: 0.011 (0.012)
Train: 10 [1250/1251 (100%)]  Loss: 4.765 (4.80)  Time: 0.777s, 1318.14/s  (0.789s, 1297.53/s)  LR: 9.973e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.575 (1.575)  Loss:  1.5194 (1.5194)  Acc@1: 70.3125 (70.3125)  Acc@5: 89.1602 (89.1602)
Test: [  48/48]  Time: 0.171 (0.558)  Loss:  1.4601 (2.5030)  Acc@1: 70.7547 (48.4620)  Acc@5: 88.2076 (74.5160)
Current checkpoints:
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-10.pth.tar', 48.46199996582031)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-9.pth.tar', 45.58800006835938)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-8.pth.tar', 40.78400001831055)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-7.pth.tar', 38.52999996826172)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-6.pth.tar', 33.961999958496094)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-5.pth.tar', 28.74600005126953)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-4.pth.tar', 21.166000045166015)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-3.pth.tar', 15.452000008544921)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-2.pth.tar', 7.6079999975585935)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-1.pth.tar', 3.128000015869141)

Train: 11 [   0/1251 (  0%)]  Loss: 4.402 (4.40)  Time: 2.334s,  438.67/s  (2.334s,  438.67/s)  LR: 9.967e-04  Data: 1.598 (1.598)
Train: 11 [  50/1251 (  4%)]  Loss: 4.698 (4.55)  Time: 0.819s, 1250.55/s  (0.830s, 1233.27/s)  LR: 9.967e-04  Data: 0.011 (0.046)
Train: 11 [ 100/1251 (  8%)]  Loss: 4.714 (4.60)  Time: 0.778s, 1315.88/s  (0.813s, 1259.57/s)  LR: 9.967e-04  Data: 0.010 (0.028)
Train: 11 [ 150/1251 ( 12%)]  Loss: 4.493 (4.58)  Time: 0.780s, 1312.51/s  (0.803s, 1274.44/s)  LR: 9.967e-04  Data: 0.011 (0.022)
Train: 11 [ 200/1251 ( 16%)]  Loss: 4.625 (4.59)  Time: 0.779s, 1314.18/s  (0.798s, 1282.53/s)  LR: 9.967e-04  Data: 0.010 (0.019)
Train: 11 [ 250/1251 ( 20%)]  Loss: 4.712 (4.61)  Time: 0.817s, 1253.55/s  (0.797s, 1285.08/s)  LR: 9.967e-04  Data: 0.009 (0.017)
Train: 11 [ 300/1251 ( 24%)]  Loss: 4.567 (4.60)  Time: 0.779s, 1314.14/s  (0.795s, 1288.49/s)  LR: 9.967e-04  Data: 0.010 (0.016)
Train: 11 [ 350/1251 ( 28%)]  Loss: 4.753 (4.62)  Time: 0.779s, 1314.48/s  (0.793s, 1290.58/s)  LR: 9.967e-04  Data: 0.009 (0.015)
Train: 11 [ 400/1251 ( 32%)]  Loss: 4.695 (4.63)  Time: 0.781s, 1311.59/s  (0.793s, 1291.14/s)  LR: 9.967e-04  Data: 0.011 (0.015)
Train: 11 [ 450/1251 ( 36%)]  Loss: 4.533 (4.62)  Time: 0.781s, 1310.99/s  (0.792s, 1293.51/s)  LR: 9.967e-04  Data: 0.010 (0.014)
Train: 11 [ 500/1251 ( 40%)]  Loss: 4.851 (4.64)  Time: 0.780s, 1313.35/s  (0.790s, 1295.42/s)  LR: 9.967e-04  Data: 0.009 (0.014)
Train: 11 [ 550/1251 ( 44%)]  Loss: 4.413 (4.62)  Time: 0.780s, 1313.12/s  (0.790s, 1296.30/s)  LR: 9.967e-04  Data: 0.011 (0.013)
Train: 11 [ 600/1251 ( 48%)]  Loss: 4.810 (4.64)  Time: 0.780s, 1312.25/s  (0.789s, 1297.35/s)  LR: 9.967e-04  Data: 0.010 (0.013)
Train: 11 [ 650/1251 ( 52%)]  Loss: 4.567 (4.63)  Time: 0.785s, 1304.79/s  (0.789s, 1298.27/s)  LR: 9.967e-04  Data: 0.011 (0.013)
Train: 11 [ 700/1251 ( 56%)]  Loss: 4.564 (4.63)  Time: 0.788s, 1300.20/s  (0.788s, 1298.72/s)  LR: 9.967e-04  Data: 0.009 (0.013)
Train: 11 [ 750/1251 ( 60%)]  Loss: 5.026 (4.65)  Time: 0.791s, 1294.07/s  (0.788s, 1299.17/s)  LR: 9.967e-04  Data: 0.009 (0.013)
Train: 11 [ 800/1251 ( 64%)]  Loss: 4.480 (4.64)  Time: 0.781s, 1311.61/s  (0.788s, 1299.80/s)  LR: 9.967e-04  Data: 0.011 (0.012)
Train: 11 [ 850/1251 ( 68%)]  Loss: 4.698 (4.64)  Time: 0.778s, 1315.68/s  (0.787s, 1300.33/s)  LR: 9.967e-04  Data: 0.010 (0.012)
Train: 11 [ 900/1251 ( 72%)]  Loss: 4.423 (4.63)  Time: 0.780s, 1313.45/s  (0.787s, 1300.96/s)  LR: 9.967e-04  Data: 0.010 (0.012)
Train: 11 [ 950/1251 ( 76%)]  Loss: 4.922 (4.65)  Time: 0.780s, 1313.48/s  (0.787s, 1301.37/s)  LR: 9.967e-04  Data: 0.011 (0.012)
Train: 11 [1000/1251 ( 80%)]  Loss: 4.748 (4.65)  Time: 0.782s, 1309.12/s  (0.787s, 1301.81/s)  LR: 9.967e-04  Data: 0.011 (0.012)
Train: 11 [1050/1251 ( 84%)]  Loss: 4.825 (4.66)  Time: 0.796s, 1286.61/s  (0.787s, 1301.91/s)  LR: 9.967e-04  Data: 0.011 (0.012)
Train: 11 [1100/1251 ( 88%)]  Loss: 4.559 (4.66)  Time: 0.781s, 1311.36/s  (0.786s, 1302.15/s)  LR: 9.967e-04  Data: 0.010 (0.012)
Train: 11 [1150/1251 ( 92%)]  Loss: 4.617 (4.65)  Time: 0.789s, 1297.69/s  (0.786s, 1302.40/s)  LR: 9.967e-04  Data: 0.009 (0.012)
Train: 11 [1200/1251 ( 96%)]  Loss: 4.374 (4.64)  Time: 0.781s, 1311.02/s  (0.786s, 1302.51/s)  LR: 9.967e-04  Data: 0.010 (0.012)
Train: 11 [1250/1251 (100%)]  Loss: 4.617 (4.64)  Time: 0.768s, 1332.60/s  (0.786s, 1302.83/s)  LR: 9.967e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.501 (1.501)  Loss:  1.5364 (1.5364)  Acc@1: 72.3633 (72.3633)  Acc@5: 90.4297 (90.4297)
Test: [  48/48]  Time: 0.171 (0.587)  Loss:  1.7469 (2.4805)  Acc@1: 68.9858 (49.4780)  Acc@5: 86.9104 (75.8440)
Current checkpoints:
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-11.pth.tar', 49.477999973144534)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-10.pth.tar', 48.46199996582031)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-9.pth.tar', 45.58800006835938)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-8.pth.tar', 40.78400001831055)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-7.pth.tar', 38.52999996826172)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-6.pth.tar', 33.961999958496094)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-5.pth.tar', 28.74600005126953)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-4.pth.tar', 21.166000045166015)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-3.pth.tar', 15.452000008544921)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-2.pth.tar', 7.6079999975585935)

Train: 12 [   0/1251 (  0%)]  Loss: 4.691 (4.69)  Time: 2.325s,  440.38/s  (2.325s,  440.38/s)  LR: 9.961e-04  Data: 1.588 (1.588)
Train: 12 [  50/1251 (  4%)]  Loss: 4.437 (4.56)  Time: 0.779s, 1313.96/s  (0.822s, 1245.10/s)  LR: 9.961e-04  Data: 0.010 (0.045)
Train: 12 [ 100/1251 (  8%)]  Loss: 4.457 (4.53)  Time: 0.855s, 1197.17/s  (0.803s, 1274.84/s)  LR: 9.961e-04  Data: 0.011 (0.028)
Train: 12 [ 150/1251 ( 12%)]  Loss: 4.757 (4.59)  Time: 0.779s, 1313.70/s  (0.799s, 1282.16/s)  LR: 9.961e-04  Data: 0.010 (0.022)
Train: 12 [ 200/1251 ( 16%)]  Loss: 4.661 (4.60)  Time: 0.782s, 1309.53/s  (0.795s, 1288.26/s)  LR: 9.961e-04  Data: 0.011 (0.019)
Train: 12 [ 250/1251 ( 20%)]  Loss: 4.780 (4.63)  Time: 0.780s, 1313.50/s  (0.793s, 1291.97/s)  LR: 9.961e-04  Data: 0.010 (0.017)
Train: 12 [ 300/1251 ( 24%)]  Loss: 4.900 (4.67)  Time: 0.782s, 1308.93/s  (0.791s, 1294.69/s)  LR: 9.961e-04  Data: 0.010 (0.016)
Train: 12 [ 350/1251 ( 28%)]  Loss: 4.773 (4.68)  Time: 0.778s, 1315.40/s  (0.790s, 1296.27/s)  LR: 9.961e-04  Data: 0.010 (0.015)
Train: 12 [ 400/1251 ( 32%)]  Loss: 4.563 (4.67)  Time: 0.781s, 1311.73/s  (0.789s, 1297.42/s)  LR: 9.961e-04  Data: 0.011 (0.015)
Train: 12 [ 450/1251 ( 36%)]  Loss: 4.570 (4.66)  Time: 0.779s, 1314.70/s  (0.789s, 1298.49/s)  LR: 9.961e-04  Data: 0.010 (0.014)
Train: 12 [ 500/1251 ( 40%)]  Loss: 4.536 (4.65)  Time: 0.780s, 1313.30/s  (0.788s, 1299.01/s)  LR: 9.961e-04  Data: 0.010 (0.014)
Train: 12 [ 550/1251 ( 44%)]  Loss: 4.623 (4.65)  Time: 0.780s, 1312.49/s  (0.788s, 1300.16/s)  LR: 9.961e-04  Data: 0.011 (0.014)
Train: 12 [ 600/1251 ( 48%)]  Loss: 4.782 (4.66)  Time: 0.780s, 1312.60/s  (0.787s, 1301.09/s)  LR: 9.961e-04  Data: 0.011 (0.013)
Train: 12 [ 650/1251 ( 52%)]  Loss: 4.740 (4.66)  Time: 0.785s, 1305.04/s  (0.787s, 1301.73/s)  LR: 9.961e-04  Data: 0.011 (0.013)
Train: 12 [ 700/1251 ( 56%)]  Loss: 4.453 (4.65)  Time: 0.809s, 1265.17/s  (0.787s, 1301.87/s)  LR: 9.961e-04  Data: 0.011 (0.013)
Train: 12 [ 750/1251 ( 60%)]  Loss: 4.578 (4.64)  Time: 0.782s, 1308.76/s  (0.787s, 1301.91/s)  LR: 9.961e-04  Data: 0.009 (0.013)
Train: 12 [ 800/1251 ( 64%)]  Loss: 4.863 (4.66)  Time: 0.781s, 1311.71/s  (0.787s, 1301.84/s)  LR: 9.961e-04  Data: 0.010 (0.012)
Train: 12 [ 850/1251 ( 68%)]  Loss: 4.534 (4.65)  Time: 0.781s, 1310.46/s  (0.787s, 1301.32/s)  LR: 9.961e-04  Data: 0.010 (0.012)
Train: 12 [ 900/1251 ( 72%)]  Loss: 4.375 (4.64)  Time: 0.785s, 1304.16/s  (0.787s, 1301.76/s)  LR: 9.961e-04  Data: 0.010 (0.012)
Train: 12 [ 950/1251 ( 76%)]  Loss: 4.576 (4.63)  Time: 0.778s, 1315.50/s  (0.786s, 1302.24/s)  LR: 9.961e-04  Data: 0.011 (0.012)
Train: 12 [1000/1251 ( 80%)]  Loss: 4.772 (4.64)  Time: 0.822s, 1245.48/s  (0.787s, 1301.65/s)  LR: 9.961e-04  Data: 0.013 (0.012)
Train: 12 [1050/1251 ( 84%)]  Loss: 4.495 (4.63)  Time: 0.780s, 1313.35/s  (0.787s, 1300.42/s)  LR: 9.961e-04  Data: 0.010 (0.012)
Train: 12 [1100/1251 ( 88%)]  Loss: 4.264 (4.62)  Time: 0.777s, 1318.50/s  (0.787s, 1300.86/s)  LR: 9.961e-04  Data: 0.010 (0.012)
Train: 12 [1150/1251 ( 92%)]  Loss: 4.870 (4.63)  Time: 0.817s, 1253.94/s  (0.788s, 1299.79/s)  LR: 9.961e-04  Data: 0.010 (0.012)
Train: 12 [1200/1251 ( 96%)]  Loss: 4.828 (4.64)  Time: 0.782s, 1308.70/s  (0.788s, 1298.80/s)  LR: 9.961e-04  Data: 0.011 (0.012)
Train: 12 [1250/1251 (100%)]  Loss: 4.454 (4.63)  Time: 0.768s, 1332.58/s  (0.788s, 1299.15/s)  LR: 9.961e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.514 (1.514)  Loss:  1.5788 (1.5788)  Acc@1: 73.7305 (73.7305)  Acc@5: 90.4297 (90.4297)
Test: [  48/48]  Time: 0.173 (0.566)  Loss:  1.7176 (2.4502)  Acc@1: 72.2877 (51.3420)  Acc@5: 88.9151 (76.5900)
Current checkpoints:
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-12.pth.tar', 51.34200011474609)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-11.pth.tar', 49.477999973144534)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-10.pth.tar', 48.46199996582031)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-9.pth.tar', 45.58800006835938)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-8.pth.tar', 40.78400001831055)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-7.pth.tar', 38.52999996826172)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-6.pth.tar', 33.961999958496094)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-5.pth.tar', 28.74600005126953)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-4.pth.tar', 21.166000045166015)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-3.pth.tar', 15.452000008544921)

Train: 13 [   0/1251 (  0%)]  Loss: 4.797 (4.80)  Time: 2.295s,  446.18/s  (2.295s,  446.18/s)  LR: 9.954e-04  Data: 1.545 (1.545)
Train: 13 [  50/1251 (  4%)]  Loss: 4.733 (4.76)  Time: 0.789s, 1298.28/s  (0.818s, 1251.49/s)  LR: 9.954e-04  Data: 0.010 (0.045)
Train: 13 [ 100/1251 (  8%)]  Loss: 4.560 (4.70)  Time: 0.780s, 1312.37/s  (0.800s, 1279.52/s)  LR: 9.954e-04  Data: 0.009 (0.028)
Train: 13 [ 150/1251 ( 12%)]  Loss: 4.286 (4.59)  Time: 0.790s, 1296.24/s  (0.796s, 1286.25/s)  LR: 9.954e-04  Data: 0.011 (0.022)
Train: 13 [ 200/1251 ( 16%)]  Loss: 4.742 (4.62)  Time: 0.793s, 1291.43/s  (0.796s, 1286.81/s)  LR: 9.954e-04  Data: 0.010 (0.019)
Train: 13 [ 250/1251 ( 20%)]  Loss: 4.623 (4.62)  Time: 0.779s, 1314.78/s  (0.794s, 1289.88/s)  LR: 9.954e-04  Data: 0.010 (0.017)
Train: 13 [ 300/1251 ( 24%)]  Loss: 4.525 (4.61)  Time: 0.818s, 1251.22/s  (0.793s, 1291.09/s)  LR: 9.954e-04  Data: 0.011 (0.016)
Train: 13 [ 350/1251 ( 28%)]  Loss: 4.819 (4.64)  Time: 0.821s, 1247.96/s  (0.792s, 1292.96/s)  LR: 9.954e-04  Data: 0.010 (0.015)
Train: 13 [ 400/1251 ( 32%)]  Loss: 4.547 (4.63)  Time: 0.778s, 1316.58/s  (0.792s, 1292.68/s)  LR: 9.954e-04  Data: 0.010 (0.015)
Train: 13 [ 450/1251 ( 36%)]  Loss: 4.558 (4.62)  Time: 0.784s, 1305.74/s  (0.791s, 1294.12/s)  LR: 9.954e-04  Data: 0.010 (0.014)
Train: 13 [ 500/1251 ( 40%)]  Loss: 4.582 (4.62)  Time: 0.781s, 1311.68/s  (0.790s, 1295.79/s)  LR: 9.954e-04  Data: 0.009 (0.014)
Train: 13 [ 550/1251 ( 44%)]  Loss: 4.665 (4.62)  Time: 0.783s, 1306.96/s  (0.789s, 1297.16/s)  LR: 9.954e-04  Data: 0.011 (0.013)
Train: 13 [ 600/1251 ( 48%)]  Loss: 4.337 (4.60)  Time: 0.783s, 1307.41/s  (0.789s, 1298.00/s)  LR: 9.954e-04  Data: 0.009 (0.013)
Train: 13 [ 650/1251 ( 52%)]  Loss: 4.378 (4.58)  Time: 0.779s, 1314.33/s  (0.789s, 1298.66/s)  LR: 9.954e-04  Data: 0.009 (0.013)
Train: 13 [ 700/1251 ( 56%)]  Loss: 4.738 (4.59)  Time: 0.779s, 1314.21/s  (0.788s, 1299.60/s)  LR: 9.954e-04  Data: 0.009 (0.013)
Train: 13 [ 750/1251 ( 60%)]  Loss: 4.784 (4.60)  Time: 0.779s, 1314.13/s  (0.788s, 1300.30/s)  LR: 9.954e-04  Data: 0.010 (0.012)
Train: 13 [ 800/1251 ( 64%)]  Loss: 4.220 (4.58)  Time: 0.780s, 1312.45/s  (0.787s, 1301.03/s)  LR: 9.954e-04  Data: 0.010 (0.012)
Train: 13 [ 850/1251 ( 68%)]  Loss: 4.872 (4.60)  Time: 0.794s, 1289.38/s  (0.787s, 1301.39/s)  LR: 9.954e-04  Data: 0.009 (0.012)
Train: 13 [ 900/1251 ( 72%)]  Loss: 4.712 (4.60)  Time: 0.780s, 1313.17/s  (0.787s, 1301.76/s)  LR: 9.954e-04  Data: 0.009 (0.012)
Train: 13 [ 950/1251 ( 76%)]  Loss: 4.714 (4.61)  Time: 0.813s, 1259.78/s  (0.788s, 1300.27/s)  LR: 9.954e-04  Data: 0.011 (0.012)
Train: 13 [1000/1251 ( 80%)]  Loss: 4.527 (4.61)  Time: 0.781s, 1311.95/s  (0.788s, 1299.66/s)  LR: 9.954e-04  Data: 0.009 (0.012)
Train: 13 [1050/1251 ( 84%)]  Loss: 4.542 (4.60)  Time: 0.780s, 1313.59/s  (0.788s, 1300.03/s)  LR: 9.954e-04  Data: 0.010 (0.012)
Train: 13 [1100/1251 ( 88%)]  Loss: 4.789 (4.61)  Time: 0.850s, 1204.51/s  (0.788s, 1299.87/s)  LR: 9.954e-04  Data: 0.011 (0.012)
Train: 13 [1150/1251 ( 92%)]  Loss: 4.671 (4.61)  Time: 0.819s, 1250.86/s  (0.789s, 1298.17/s)  LR: 9.954e-04  Data: 0.010 (0.012)
Train: 13 [1200/1251 ( 96%)]  Loss: 4.385 (4.60)  Time: 0.779s, 1314.53/s  (0.789s, 1297.87/s)  LR: 9.954e-04  Data: 0.010 (0.012)
Train: 13 [1250/1251 (100%)]  Loss: 4.148 (4.59)  Time: 0.769s, 1330.77/s  (0.789s, 1298.14/s)  LR: 9.954e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.548 (1.548)  Loss:  1.6045 (1.6045)  Acc@1: 68.2617 (68.2617)  Acc@5: 88.4766 (88.4766)
Test: [  48/48]  Time: 0.171 (0.570)  Loss:  1.5429 (2.3458)  Acc@1: 71.4623 (51.9100)  Acc@5: 88.7972 (77.6600)
Current checkpoints:
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-13.pth.tar', 51.91000001464844)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-12.pth.tar', 51.34200011474609)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-11.pth.tar', 49.477999973144534)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-10.pth.tar', 48.46199996582031)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-9.pth.tar', 45.58800006835938)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-8.pth.tar', 40.78400001831055)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-7.pth.tar', 38.52999996826172)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-6.pth.tar', 33.961999958496094)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-5.pth.tar', 28.74600005126953)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-4.pth.tar', 21.166000045166015)

Train: 14 [   0/1251 (  0%)]  Loss: 4.520 (4.52)  Time: 2.276s,  449.91/s  (2.276s,  449.91/s)  LR: 9.947e-04  Data: 1.539 (1.539)
Train: 14 [  50/1251 (  4%)]  Loss: 4.580 (4.55)  Time: 0.780s, 1312.93/s  (0.826s, 1240.12/s)  LR: 9.947e-04  Data: 0.010 (0.051)
Train: 14 [ 100/1251 (  8%)]  Loss: 4.516 (4.54)  Time: 0.786s, 1302.70/s  (0.805s, 1272.67/s)  LR: 9.947e-04  Data: 0.012 (0.031)
Train: 14 [ 150/1251 ( 12%)]  Loss: 4.654 (4.57)  Time: 0.779s, 1314.44/s  (0.797s, 1285.47/s)  LR: 9.947e-04  Data: 0.009 (0.024)
Train: 14 [ 200/1251 ( 16%)]  Loss: 4.617 (4.58)  Time: 0.821s, 1247.85/s  (0.800s, 1280.61/s)  LR: 9.947e-04  Data: 0.010 (0.021)
Train: 14 [ 250/1251 ( 20%)]  Loss: 4.585 (4.58)  Time: 0.818s, 1252.55/s  (0.803s, 1275.18/s)  LR: 9.947e-04  Data: 0.011 (0.019)
Train: 14 [ 300/1251 ( 24%)]  Loss: 4.306 (4.54)  Time: 0.815s, 1255.81/s  (0.805s, 1271.83/s)  LR: 9.947e-04  Data: 0.011 (0.017)
Train: 14 [ 350/1251 ( 28%)]  Loss: 4.833 (4.58)  Time: 0.779s, 1313.69/s  (0.805s, 1271.39/s)  LR: 9.947e-04  Data: 0.010 (0.016)
Train: 14 [ 400/1251 ( 32%)]  Loss: 4.635 (4.58)  Time: 0.823s, 1244.56/s  (0.803s, 1275.18/s)  LR: 9.947e-04  Data: 0.010 (0.015)
Train: 14 [ 450/1251 ( 36%)]  Loss: 4.180 (4.54)  Time: 0.779s, 1313.89/s  (0.803s, 1275.03/s)  LR: 9.947e-04  Data: 0.010 (0.015)
Train: 14 [ 500/1251 ( 40%)]  Loss: 4.601 (4.55)  Time: 0.815s, 1256.47/s  (0.804s, 1274.17/s)  LR: 9.947e-04  Data: 0.010 (0.014)
Train: 14 [ 550/1251 ( 44%)]  Loss: 4.783 (4.57)  Time: 0.782s, 1310.22/s  (0.804s, 1273.69/s)  LR: 9.947e-04  Data: 0.010 (0.014)
Train: 14 [ 600/1251 ( 48%)]  Loss: 4.616 (4.57)  Time: 0.802s, 1276.72/s  (0.803s, 1274.91/s)  LR: 9.947e-04  Data: 0.010 (0.014)
Train: 14 [ 650/1251 ( 52%)]  Loss: 4.640 (4.58)  Time: 0.779s, 1313.84/s  (0.804s, 1274.42/s)  LR: 9.947e-04  Data: 0.010 (0.013)
Train: 14 [ 700/1251 ( 56%)]  Loss: 4.545 (4.57)  Time: 0.780s, 1312.20/s  (0.804s, 1274.25/s)  LR: 9.947e-04  Data: 0.010 (0.013)
Train: 14 [ 750/1251 ( 60%)]  Loss: 4.832 (4.59)  Time: 0.819s, 1250.90/s  (0.803s, 1275.74/s)  LR: 9.947e-04  Data: 0.009 (0.013)
Train: 14 [ 800/1251 ( 64%)]  Loss: 4.189 (4.57)  Time: 0.820s, 1248.76/s  (0.802s, 1276.92/s)  LR: 9.947e-04  Data: 0.010 (0.013)
Train: 14 [ 850/1251 ( 68%)]  Loss: 4.580 (4.57)  Time: 0.786s, 1302.84/s  (0.801s, 1277.90/s)  LR: 9.947e-04  Data: 0.010 (0.013)
Train: 14 [ 900/1251 ( 72%)]  Loss: 4.959 (4.59)  Time: 0.790s, 1295.57/s  (0.800s, 1279.57/s)  LR: 9.947e-04  Data: 0.010 (0.013)
Train: 14 [ 950/1251 ( 76%)]  Loss: 4.764 (4.60)  Time: 0.779s, 1314.52/s  (0.800s, 1280.57/s)  LR: 9.947e-04  Data: 0.010 (0.012)
Train: 14 [1000/1251 ( 80%)]  Loss: 4.396 (4.59)  Time: 0.815s, 1256.37/s  (0.799s, 1280.89/s)  LR: 9.947e-04  Data: 0.011 (0.012)
Train: 14 [1050/1251 ( 84%)]  Loss: 4.429 (4.58)  Time: 0.814s, 1257.91/s  (0.800s, 1279.66/s)  LR: 9.947e-04  Data: 0.011 (0.012)
Train: 14 [1100/1251 ( 88%)]  Loss: 4.244 (4.57)  Time: 0.818s, 1252.07/s  (0.801s, 1278.39/s)  LR: 9.947e-04  Data: 0.010 (0.012)
Train: 14 [1150/1251 ( 92%)]  Loss: 4.582 (4.57)  Time: 0.818s, 1252.02/s  (0.802s, 1277.23/s)  LR: 9.947e-04  Data: 0.010 (0.012)
Train: 14 [1200/1251 ( 96%)]  Loss: 4.915 (4.58)  Time: 0.778s, 1316.03/s  (0.802s, 1277.46/s)  LR: 9.947e-04  Data: 0.010 (0.012)
Train: 14 [1250/1251 (100%)]  Loss: 4.484 (4.58)  Time: 0.768s, 1332.48/s  (0.801s, 1278.80/s)  LR: 9.947e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.540 (1.540)  Loss:  1.4467 (1.4467)  Acc@1: 74.1211 (74.1211)  Acc@5: 90.3320 (90.3320)
Test: [  48/48]  Time: 0.171 (0.562)  Loss:  1.4795 (2.2525)  Acc@1: 73.4670 (54.3000)  Acc@5: 88.5613 (79.1520)
Current checkpoints:
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-14.pth.tar', 54.29999998046875)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-13.pth.tar', 51.91000001464844)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-12.pth.tar', 51.34200011474609)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-11.pth.tar', 49.477999973144534)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-10.pth.tar', 48.46199996582031)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-9.pth.tar', 45.58800006835938)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-8.pth.tar', 40.78400001831055)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-7.pth.tar', 38.52999996826172)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-6.pth.tar', 33.961999958496094)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-5.pth.tar', 28.74600005126953)

Train: 15 [   0/1251 (  0%)]  Loss: 4.511 (4.51)  Time: 2.416s,  423.84/s  (2.416s,  423.84/s)  LR: 9.939e-04  Data: 1.680 (1.680)
Train: 15 [  50/1251 (  4%)]  Loss: 4.025 (4.27)  Time: 0.818s, 1252.34/s  (0.824s, 1242.37/s)  LR: 9.939e-04  Data: 0.009 (0.044)
Train: 15 [ 100/1251 (  8%)]  Loss: 4.639 (4.39)  Time: 0.801s, 1277.87/s  (0.806s, 1270.32/s)  LR: 9.939e-04  Data: 0.011 (0.027)
Train: 15 [ 150/1251 ( 12%)]  Loss: 4.175 (4.34)  Time: 0.814s, 1257.35/s  (0.804s, 1273.07/s)  LR: 9.939e-04  Data: 0.010 (0.021)
Train: 15 [ 200/1251 ( 16%)]  Loss: 4.748 (4.42)  Time: 0.780s, 1312.58/s  (0.805s, 1272.61/s)  LR: 9.939e-04  Data: 0.011 (0.019)
Train: 15 [ 250/1251 ( 20%)]  Loss: 4.572 (4.44)  Time: 0.790s, 1295.90/s  (0.801s, 1277.81/s)  LR: 9.939e-04  Data: 0.011 (0.017)
Train: 15 [ 300/1251 ( 24%)]  Loss: 4.740 (4.49)  Time: 0.793s, 1291.86/s  (0.800s, 1279.35/s)  LR: 9.939e-04  Data: 0.010 (0.016)
Train: 15 [ 350/1251 ( 28%)]  Loss: 4.476 (4.49)  Time: 0.783s, 1307.43/s  (0.799s, 1281.09/s)  LR: 9.939e-04  Data: 0.010 (0.015)
Train: 15 [ 400/1251 ( 32%)]  Loss: 4.166 (4.45)  Time: 0.780s, 1313.16/s  (0.799s, 1282.28/s)  LR: 9.939e-04  Data: 0.010 (0.014)
Train: 15 [ 450/1251 ( 36%)]  Loss: 4.433 (4.45)  Time: 0.780s, 1312.06/s  (0.797s, 1285.24/s)  LR: 9.939e-04  Data: 0.011 (0.014)
Train: 15 [ 500/1251 ( 40%)]  Loss: 4.414 (4.45)  Time: 0.782s, 1309.65/s  (0.796s, 1286.27/s)  LR: 9.939e-04  Data: 0.011 (0.014)
Train: 15 [ 550/1251 ( 44%)]  Loss: 4.461 (4.45)  Time: 0.779s, 1313.94/s  (0.795s, 1287.48/s)  LR: 9.939e-04  Data: 0.010 (0.013)
Train: 15 [ 600/1251 ( 48%)]  Loss: 4.472 (4.45)  Time: 0.779s, 1314.91/s  (0.796s, 1286.74/s)  LR: 9.939e-04  Data: 0.009 (0.013)
Train: 15 [ 650/1251 ( 52%)]  Loss: 4.551 (4.46)  Time: 0.820s, 1248.64/s  (0.795s, 1287.59/s)  LR: 9.939e-04  Data: 0.010 (0.013)
Train: 15 [ 700/1251 ( 56%)]  Loss: 4.792 (4.48)  Time: 0.781s, 1311.76/s  (0.795s, 1288.39/s)  LR: 9.939e-04  Data: 0.010 (0.013)
Train: 15 [ 750/1251 ( 60%)]  Loss: 4.452 (4.48)  Time: 0.814s, 1258.21/s  (0.794s, 1289.76/s)  LR: 9.939e-04  Data: 0.009 (0.012)
Train: 15 [ 800/1251 ( 64%)]  Loss: 4.581 (4.48)  Time: 0.783s, 1308.09/s  (0.794s, 1290.15/s)  LR: 9.939e-04  Data: 0.010 (0.012)
Train: 15 [ 850/1251 ( 68%)]  Loss: 4.398 (4.48)  Time: 0.778s, 1316.87/s  (0.793s, 1291.16/s)  LR: 9.939e-04  Data: 0.011 (0.012)
Train: 15 [ 900/1251 ( 72%)]  Loss: 4.634 (4.49)  Time: 0.778s, 1315.42/s  (0.793s, 1291.78/s)  LR: 9.939e-04  Data: 0.010 (0.012)
Train: 15 [ 950/1251 ( 76%)]  Loss: 4.308 (4.48)  Time: 0.781s, 1311.84/s  (0.792s, 1292.65/s)  LR: 9.939e-04  Data: 0.011 (0.012)
Train: 15 [1000/1251 ( 80%)]  Loss: 4.545 (4.48)  Time: 0.782s, 1309.67/s  (0.792s, 1293.64/s)  LR: 9.939e-04  Data: 0.010 (0.012)
Train: 15 [1050/1251 ( 84%)]  Loss: 4.482 (4.48)  Time: 0.777s, 1318.53/s  (0.791s, 1294.24/s)  LR: 9.939e-04  Data: 0.010 (0.012)
Train: 15 [1100/1251 ( 88%)]  Loss: 4.442 (4.48)  Time: 0.816s, 1254.19/s  (0.792s, 1293.32/s)  LR: 9.939e-04  Data: 0.010 (0.012)
Train: 15 [1150/1251 ( 92%)]  Loss: 4.913 (4.50)  Time: 0.806s, 1270.82/s  (0.792s, 1292.85/s)  LR: 9.939e-04  Data: 0.010 (0.012)
Train: 15 [1200/1251 ( 96%)]  Loss: 4.701 (4.51)  Time: 0.782s, 1309.22/s  (0.792s, 1293.54/s)  LR: 9.939e-04  Data: 0.010 (0.012)
Train: 15 [1250/1251 (100%)]  Loss: 4.589 (4.51)  Time: 0.805s, 1272.66/s  (0.793s, 1292.11/s)  LR: 9.939e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.543 (1.543)  Loss:  1.4707 (1.4707)  Acc@1: 75.9766 (75.9766)  Acc@5: 91.2109 (91.2109)
Test: [  48/48]  Time: 0.172 (0.569)  Loss:  1.5240 (2.1901)  Acc@1: 72.6415 (55.2720)  Acc@5: 89.5047 (79.8660)
Current checkpoints:
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-15.pth.tar', 55.27200000976563)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-14.pth.tar', 54.29999998046875)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-13.pth.tar', 51.91000001464844)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-12.pth.tar', 51.34200011474609)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-11.pth.tar', 49.477999973144534)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-10.pth.tar', 48.46199996582031)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-9.pth.tar', 45.58800006835938)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-8.pth.tar', 40.78400001831055)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-7.pth.tar', 38.52999996826172)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-6.pth.tar', 33.961999958496094)

Train: 16 [   0/1251 (  0%)]  Loss: 4.225 (4.22)  Time: 2.305s,  444.34/s  (2.305s,  444.34/s)  LR: 9.931e-04  Data: 1.569 (1.569)
Train: 16 [  50/1251 (  4%)]  Loss: 4.569 (4.40)  Time: 0.781s, 1311.80/s  (0.817s, 1252.74/s)  LR: 9.931e-04  Data: 0.010 (0.044)
Train: 16 [ 100/1251 (  8%)]  Loss: 4.447 (4.41)  Time: 0.783s, 1308.15/s  (0.799s, 1280.81/s)  LR: 9.931e-04  Data: 0.011 (0.028)
Train: 16 [ 150/1251 ( 12%)]  Loss: 4.495 (4.43)  Time: 0.836s, 1224.17/s  (0.793s, 1290.60/s)  LR: 9.931e-04  Data: 0.011 (0.022)
Train: 16 [ 200/1251 ( 16%)]  Loss: 4.498 (4.45)  Time: 0.778s, 1315.98/s  (0.790s, 1295.69/s)  LR: 9.931e-04  Data: 0.011 (0.019)
Train: 16 [ 250/1251 ( 20%)]  Loss: 4.514 (4.46)  Time: 0.780s, 1313.07/s  (0.789s, 1298.47/s)  LR: 9.931e-04  Data: 0.011 (0.017)
Train: 16 [ 300/1251 ( 24%)]  Loss: 3.981 (4.39)  Time: 0.784s, 1305.46/s  (0.787s, 1300.65/s)  LR: 9.931e-04  Data: 0.011 (0.016)
Train: 16 [ 350/1251 ( 28%)]  Loss: 4.748 (4.43)  Time: 0.813s, 1259.58/s  (0.789s, 1298.20/s)  LR: 9.931e-04  Data: 0.010 (0.015)
Train: 16 [ 400/1251 ( 32%)]  Loss: 4.418 (4.43)  Time: 0.779s, 1314.73/s  (0.788s, 1299.35/s)  LR: 9.931e-04  Data: 0.010 (0.015)
Train: 16 [ 450/1251 ( 36%)]  Loss: 4.423 (4.43)  Time: 0.789s, 1297.39/s  (0.787s, 1300.66/s)  LR: 9.931e-04  Data: 0.009 (0.014)
Train: 16 [ 500/1251 ( 40%)]  Loss: 3.924 (4.39)  Time: 0.811s, 1262.96/s  (0.787s, 1300.84/s)  LR: 9.931e-04  Data: 0.010 (0.014)
Train: 16 [ 550/1251 ( 44%)]  Loss: 4.616 (4.40)  Time: 0.779s, 1314.90/s  (0.788s, 1299.78/s)  LR: 9.931e-04  Data: 0.010 (0.013)
Train: 16 [ 600/1251 ( 48%)]  Loss: 4.262 (4.39)  Time: 0.780s, 1313.63/s  (0.787s, 1300.72/s)  LR: 9.931e-04  Data: 0.011 (0.013)
Train: 16 [ 650/1251 ( 52%)]  Loss: 4.192 (4.38)  Time: 0.781s, 1310.81/s  (0.789s, 1298.14/s)  LR: 9.931e-04  Data: 0.009 (0.013)
Train: 16 [ 700/1251 ( 56%)]  Loss: 4.162 (4.36)  Time: 0.834s, 1227.29/s  (0.788s, 1298.72/s)  LR: 9.931e-04  Data: 0.010 (0.013)
Train: 16 [ 750/1251 ( 60%)]  Loss: 4.156 (4.35)  Time: 0.801s, 1278.99/s  (0.789s, 1298.33/s)  LR: 9.931e-04  Data: 0.009 (0.013)
Train: 16 [ 800/1251 ( 64%)]  Loss: 4.742 (4.37)  Time: 0.782s, 1309.19/s  (0.789s, 1298.30/s)  LR: 9.931e-04  Data: 0.011 (0.012)
Train: 16 [ 850/1251 ( 68%)]  Loss: 4.700 (4.39)  Time: 0.779s, 1315.02/s  (0.788s, 1298.99/s)  LR: 9.931e-04  Data: 0.010 (0.012)
Train: 16 [ 900/1251 ( 72%)]  Loss: 4.477 (4.40)  Time: 0.788s, 1298.99/s  (0.788s, 1299.73/s)  LR: 9.931e-04  Data: 0.011 (0.012)
Train: 16 [ 950/1251 ( 76%)]  Loss: 4.471 (4.40)  Time: 0.780s, 1312.30/s  (0.787s, 1300.39/s)  LR: 9.931e-04  Data: 0.010 (0.012)
Train: 16 [1000/1251 ( 80%)]  Loss: 4.145 (4.39)  Time: 0.788s, 1298.93/s  (0.787s, 1300.67/s)  LR: 9.931e-04  Data: 0.010 (0.012)
Train: 16 [1050/1251 ( 84%)]  Loss: 4.654 (4.40)  Time: 0.779s, 1314.57/s  (0.787s, 1300.87/s)  LR: 9.931e-04  Data: 0.009 (0.012)
Train: 16 [1100/1251 ( 88%)]  Loss: 4.236 (4.39)  Time: 0.802s, 1277.08/s  (0.787s, 1301.06/s)  LR: 9.931e-04  Data: 0.013 (0.012)
Train: 16 [1150/1251 ( 92%)]  Loss: 4.636 (4.40)  Time: 0.776s, 1320.16/s  (0.787s, 1300.84/s)  LR: 9.931e-04  Data: 0.010 (0.012)
Train: 16 [1200/1251 ( 96%)]  Loss: 4.259 (4.40)  Time: 0.825s, 1241.54/s  (0.788s, 1299.96/s)  LR: 9.931e-04  Data: 0.011 (0.012)
Train: 16 [1250/1251 (100%)]  Loss: 4.760 (4.41)  Time: 0.767s, 1335.87/s  (0.788s, 1299.60/s)  LR: 9.931e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.561 (1.561)  Loss:  1.2685 (1.2685)  Acc@1: 76.2695 (76.2695)  Acc@5: 91.7969 (91.7969)
Test: [  48/48]  Time: 0.172 (0.565)  Loss:  1.3451 (2.1524)  Acc@1: 77.1226 (57.0900)  Acc@5: 90.8019 (81.0360)
Current checkpoints:
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-16.pth.tar', 57.08999988769531)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-15.pth.tar', 55.27200000976563)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-14.pth.tar', 54.29999998046875)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-13.pth.tar', 51.91000001464844)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-12.pth.tar', 51.34200011474609)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-11.pth.tar', 49.477999973144534)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-10.pth.tar', 48.46199996582031)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-9.pth.tar', 45.58800006835938)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-8.pth.tar', 40.78400001831055)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-7.pth.tar', 38.52999996826172)

Train: 17 [   0/1251 (  0%)]  Loss: 4.466 (4.47)  Time: 2.396s,  427.39/s  (2.396s,  427.39/s)  LR: 9.922e-04  Data: 1.656 (1.656)
Train: 17 [  50/1251 (  4%)]  Loss: 4.145 (4.31)  Time: 0.782s, 1309.82/s  (0.820s, 1248.09/s)  LR: 9.922e-04  Data: 0.010 (0.047)
Train: 17 [ 100/1251 (  8%)]  Loss: 4.511 (4.37)  Time: 0.803s, 1275.03/s  (0.806s, 1270.04/s)  LR: 9.922e-04  Data: 0.010 (0.029)
Train: 17 [ 150/1251 ( 12%)]  Loss: 4.620 (4.44)  Time: 0.778s, 1316.45/s  (0.800s, 1280.78/s)  LR: 9.922e-04  Data: 0.010 (0.023)
Train: 17 [ 200/1251 ( 16%)]  Loss: 4.628 (4.47)  Time: 0.781s, 1310.61/s  (0.796s, 1286.98/s)  LR: 9.922e-04  Data: 0.009 (0.020)
Train: 17 [ 250/1251 ( 20%)]  Loss: 4.477 (4.47)  Time: 0.798s, 1283.84/s  (0.793s, 1290.88/s)  LR: 9.922e-04  Data: 0.009 (0.018)
Train: 17 [ 300/1251 ( 24%)]  Loss: 4.272 (4.45)  Time: 0.801s, 1279.03/s  (0.792s, 1292.40/s)  LR: 9.922e-04  Data: 0.010 (0.017)
Train: 17 [ 350/1251 ( 28%)]  Loss: 4.131 (4.41)  Time: 0.780s, 1313.23/s  (0.791s, 1294.10/s)  LR: 9.922e-04  Data: 0.011 (0.016)
Train: 17 [ 400/1251 ( 32%)]  Loss: 4.114 (4.37)  Time: 0.778s, 1315.43/s  (0.790s, 1295.52/s)  LR: 9.922e-04  Data: 0.010 (0.015)
Train: 17 [ 450/1251 ( 36%)]  Loss: 4.583 (4.39)  Time: 0.817s, 1253.65/s  (0.792s, 1293.25/s)  LR: 9.922e-04  Data: 0.011 (0.015)
Train: 17 [ 500/1251 ( 40%)]  Loss: 4.704 (4.42)  Time: 0.817s, 1254.00/s  (0.795s, 1288.11/s)  LR: 9.922e-04  Data: 0.011 (0.014)
Train: 17 [ 550/1251 ( 44%)]  Loss: 4.297 (4.41)  Time: 0.779s, 1314.48/s  (0.795s, 1287.55/s)  LR: 9.922e-04  Data: 0.010 (0.014)
Train: 17 [ 600/1251 ( 48%)]  Loss: 4.561 (4.42)  Time: 0.816s, 1254.88/s  (0.796s, 1286.85/s)  LR: 9.922e-04  Data: 0.011 (0.014)
Train: 17 [ 650/1251 ( 52%)]  Loss: 4.245 (4.41)  Time: 0.826s, 1239.15/s  (0.796s, 1286.56/s)  LR: 9.922e-04  Data: 0.010 (0.013)
Train: 17 [ 700/1251 ( 56%)]  Loss: 4.675 (4.43)  Time: 0.781s, 1311.28/s  (0.795s, 1288.36/s)  LR: 9.922e-04  Data: 0.011 (0.013)
Train: 17 [ 750/1251 ( 60%)]  Loss: 4.790 (4.45)  Time: 0.779s, 1314.24/s  (0.794s, 1289.70/s)  LR: 9.922e-04  Data: 0.011 (0.013)
Train: 17 [ 800/1251 ( 64%)]  Loss: 4.475 (4.45)  Time: 0.780s, 1313.60/s  (0.793s, 1290.71/s)  LR: 9.922e-04  Data: 0.011 (0.013)
Train: 17 [ 850/1251 ( 68%)]  Loss: 4.452 (4.45)  Time: 0.818s, 1251.31/s  (0.793s, 1290.65/s)  LR: 9.922e-04  Data: 0.010 (0.013)
Train: 17 [ 900/1251 ( 72%)]  Loss: 4.807 (4.47)  Time: 0.789s, 1298.25/s  (0.793s, 1291.16/s)  LR: 9.922e-04  Data: 0.009 (0.013)
Train: 17 [ 950/1251 ( 76%)]  Loss: 4.398 (4.47)  Time: 0.827s, 1238.55/s  (0.793s, 1291.18/s)  LR: 9.922e-04  Data: 0.011 (0.012)
Train: 17 [1000/1251 ( 80%)]  Loss: 4.317 (4.46)  Time: 0.782s, 1309.61/s  (0.793s, 1291.96/s)  LR: 9.922e-04  Data: 0.010 (0.012)
Train: 17 [1050/1251 ( 84%)]  Loss: 4.502 (4.46)  Time: 0.779s, 1314.24/s  (0.792s, 1292.19/s)  LR: 9.922e-04  Data: 0.009 (0.012)
Train: 17 [1100/1251 ( 88%)]  Loss: 4.145 (4.45)  Time: 0.781s, 1311.15/s  (0.793s, 1291.29/s)  LR: 9.922e-04  Data: 0.011 (0.012)
Train: 17 [1150/1251 ( 92%)]  Loss: 4.384 (4.45)  Time: 0.781s, 1311.24/s  (0.793s, 1291.71/s)  LR: 9.922e-04  Data: 0.010 (0.012)
Train: 17 [1200/1251 ( 96%)]  Loss: 4.205 (4.44)  Time: 0.791s, 1294.62/s  (0.792s, 1292.32/s)  LR: 9.922e-04  Data: 0.011 (0.012)
Train: 17 [1250/1251 (100%)]  Loss: 4.808 (4.45)  Time: 0.805s, 1271.51/s  (0.793s, 1292.10/s)  LR: 9.922e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.595 (1.595)  Loss:  1.5346 (1.5346)  Acc@1: 75.4883 (75.4883)  Acc@5: 91.6016 (91.6016)
Test: [  48/48]  Time: 0.171 (0.575)  Loss:  1.4948 (2.1345)  Acc@1: 72.9953 (57.5220)  Acc@5: 89.9764 (81.5640)
Current checkpoints:
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-17.pth.tar', 57.52200003417969)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-16.pth.tar', 57.08999988769531)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-15.pth.tar', 55.27200000976563)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-14.pth.tar', 54.29999998046875)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-13.pth.tar', 51.91000001464844)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-12.pth.tar', 51.34200011474609)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-11.pth.tar', 49.477999973144534)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-10.pth.tar', 48.46199996582031)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-9.pth.tar', 45.58800006835938)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-8.pth.tar', 40.78400001831055)

Train: 18 [   0/1251 (  0%)]  Loss: 4.281 (4.28)  Time: 2.513s,  407.46/s  (2.513s,  407.46/s)  LR: 9.912e-04  Data: 1.762 (1.762)
Train: 18 [  50/1251 (  4%)]  Loss: 4.443 (4.36)  Time: 0.780s, 1312.22/s  (0.821s, 1247.06/s)  LR: 9.912e-04  Data: 0.011 (0.048)
Train: 18 [ 100/1251 (  8%)]  Loss: 4.490 (4.40)  Time: 0.780s, 1312.58/s  (0.803s, 1274.56/s)  LR: 9.912e-04  Data: 0.009 (0.029)
Train: 18 [ 150/1251 ( 12%)]  Loss: 4.455 (4.42)  Time: 0.780s, 1311.98/s  (0.796s, 1286.66/s)  LR: 9.912e-04  Data: 0.011 (0.023)
Train: 18 [ 200/1251 ( 16%)]  Loss: 3.999 (4.33)  Time: 0.782s, 1310.09/s  (0.792s, 1292.29/s)  LR: 9.912e-04  Data: 0.009 (0.020)
Train: 18 [ 250/1251 ( 20%)]  Loss: 4.635 (4.38)  Time: 0.780s, 1312.48/s  (0.790s, 1296.06/s)  LR: 9.912e-04  Data: 0.011 (0.018)
Train: 18 [ 300/1251 ( 24%)]  Loss: 4.454 (4.39)  Time: 0.819s, 1249.78/s  (0.792s, 1292.65/s)  LR: 9.912e-04  Data: 0.011 (0.017)
Train: 18 [ 350/1251 ( 28%)]  Loss: 4.224 (4.37)  Time: 0.822s, 1246.17/s  (0.795s, 1287.81/s)  LR: 9.912e-04  Data: 0.010 (0.016)
Train: 18 [ 400/1251 ( 32%)]  Loss: 4.524 (4.39)  Time: 0.782s, 1309.92/s  (0.795s, 1288.17/s)  LR: 9.912e-04  Data: 0.009 (0.015)
Train: 18 [ 450/1251 ( 36%)]  Loss: 4.630 (4.41)  Time: 0.782s, 1309.81/s  (0.793s, 1290.54/s)  LR: 9.912e-04  Data: 0.011 (0.015)
Train: 18 [ 500/1251 ( 40%)]  Loss: 4.037 (4.38)  Time: 0.781s, 1310.84/s  (0.792s, 1292.38/s)  LR: 9.912e-04  Data: 0.011 (0.014)
Train: 18 [ 550/1251 ( 44%)]  Loss: 4.290 (4.37)  Time: 0.819s, 1249.55/s  (0.793s, 1290.65/s)  LR: 9.912e-04  Data: 0.011 (0.014)
Train: 18 [ 600/1251 ( 48%)]  Loss: 4.219 (4.36)  Time: 0.780s, 1313.26/s  (0.794s, 1290.37/s)  LR: 9.912e-04  Data: 0.011 (0.014)
Train: 18 [ 650/1251 ( 52%)]  Loss: 4.663 (4.38)  Time: 0.785s, 1304.77/s  (0.793s, 1290.90/s)  LR: 9.912e-04  Data: 0.009 (0.013)
Train: 18 [ 700/1251 ( 56%)]  Loss: 3.953 (4.35)  Time: 0.801s, 1278.27/s  (0.793s, 1290.56/s)  LR: 9.912e-04  Data: 0.009 (0.013)
Train: 18 [ 750/1251 ( 60%)]  Loss: 4.330 (4.35)  Time: 0.779s, 1314.70/s  (0.794s, 1290.44/s)  LR: 9.912e-04  Data: 0.010 (0.013)
Train: 18 [ 800/1251 ( 64%)]  Loss: 4.630 (4.37)  Time: 0.778s, 1315.93/s  (0.793s, 1291.57/s)  LR: 9.912e-04  Data: 0.010 (0.013)
Train: 18 [ 850/1251 ( 68%)]  Loss: 4.366 (4.37)  Time: 0.782s, 1309.04/s  (0.792s, 1292.66/s)  LR: 9.912e-04  Data: 0.010 (0.013)
Train: 18 [ 900/1251 ( 72%)]  Loss: 4.121 (4.35)  Time: 0.779s, 1313.97/s  (0.791s, 1293.78/s)  LR: 9.912e-04  Data: 0.011 (0.013)
Train: 18 [ 950/1251 ( 76%)]  Loss: 4.354 (4.35)  Time: 0.780s, 1313.30/s  (0.791s, 1294.64/s)  LR: 9.912e-04  Data: 0.011 (0.012)
Train: 18 [1000/1251 ( 80%)]  Loss: 4.270 (4.35)  Time: 0.782s, 1310.24/s  (0.791s, 1295.01/s)  LR: 9.912e-04  Data: 0.010 (0.012)
Train: 18 [1050/1251 ( 84%)]  Loss: 4.635 (4.36)  Time: 0.781s, 1310.48/s  (0.791s, 1295.31/s)  LR: 9.912e-04  Data: 0.010 (0.012)
Train: 18 [1100/1251 ( 88%)]  Loss: 4.700 (4.38)  Time: 0.820s, 1249.00/s  (0.791s, 1295.21/s)  LR: 9.912e-04  Data: 0.011 (0.012)
Train: 18 [1150/1251 ( 92%)]  Loss: 4.466 (4.38)  Time: 0.779s, 1313.78/s  (0.791s, 1294.87/s)  LR: 9.912e-04  Data: 0.011 (0.012)
Train: 18 [1200/1251 ( 96%)]  Loss: 4.190 (4.37)  Time: 0.779s, 1314.42/s  (0.791s, 1295.11/s)  LR: 9.912e-04  Data: 0.010 (0.012)
Train: 18 [1250/1251 (100%)]  Loss: 4.123 (4.36)  Time: 0.771s, 1327.51/s  (0.791s, 1295.35/s)  LR: 9.912e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.616 (1.616)  Loss:  1.2224 (1.2224)  Acc@1: 79.0039 (79.0039)  Acc@5: 94.1406 (94.1406)
Test: [  48/48]  Time: 0.171 (0.559)  Loss:  1.4437 (2.1257)  Acc@1: 75.3538 (58.5320)  Acc@5: 90.2123 (82.3200)
Current checkpoints:
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-18.pth.tar', 58.53200002441406)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-17.pth.tar', 57.52200003417969)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-16.pth.tar', 57.08999988769531)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-15.pth.tar', 55.27200000976563)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-14.pth.tar', 54.29999998046875)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-13.pth.tar', 51.91000001464844)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-12.pth.tar', 51.34200011474609)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-11.pth.tar', 49.477999973144534)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-10.pth.tar', 48.46199996582031)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-9.pth.tar', 45.58800006835938)

Train: 19 [   0/1251 (  0%)]  Loss: 4.860 (4.86)  Time: 2.269s,  451.32/s  (2.269s,  451.32/s)  LR: 9.902e-04  Data: 1.533 (1.533)
Train: 19 [  50/1251 (  4%)]  Loss: 4.510 (4.68)  Time: 0.782s, 1309.68/s  (0.824s, 1241.97/s)  LR: 9.902e-04  Data: 0.009 (0.045)
Train: 19 [ 100/1251 (  8%)]  Loss: 4.206 (4.53)  Time: 0.781s, 1311.01/s  (0.805s, 1272.72/s)  LR: 9.902e-04  Data: 0.010 (0.028)
Train: 19 [ 150/1251 ( 12%)]  Loss: 4.387 (4.49)  Time: 0.783s, 1307.76/s  (0.799s, 1281.96/s)  LR: 9.902e-04  Data: 0.011 (0.022)
Train: 19 [ 200/1251 ( 16%)]  Loss: 4.760 (4.54)  Time: 0.821s, 1247.51/s  (0.804s, 1274.16/s)  LR: 9.902e-04  Data: 0.009 (0.019)
Train: 19 [ 250/1251 ( 20%)]  Loss: 4.517 (4.54)  Time: 0.821s, 1246.95/s  (0.806s, 1270.38/s)  LR: 9.902e-04  Data: 0.011 (0.018)
Train: 19 [ 300/1251 ( 24%)]  Loss: 4.159 (4.49)  Time: 0.783s, 1308.25/s  (0.806s, 1270.96/s)  LR: 9.902e-04  Data: 0.009 (0.016)
Train: 19 [ 350/1251 ( 28%)]  Loss: 4.727 (4.52)  Time: 0.780s, 1313.06/s  (0.803s, 1275.35/s)  LR: 9.902e-04  Data: 0.010 (0.016)
Train: 19 [ 400/1251 ( 32%)]  Loss: 4.057 (4.46)  Time: 0.785s, 1304.94/s  (0.800s, 1279.34/s)  LR: 9.902e-04  Data: 0.009 (0.015)
Train: 19 [ 450/1251 ( 36%)]  Loss: 4.213 (4.44)  Time: 0.777s, 1317.29/s  (0.800s, 1280.11/s)  LR: 9.902e-04  Data: 0.011 (0.014)
Train: 19 [ 500/1251 ( 40%)]  Loss: 4.329 (4.43)  Time: 0.779s, 1315.00/s  (0.799s, 1281.09/s)  LR: 9.902e-04  Data: 0.010 (0.014)
Train: 19 [ 550/1251 ( 44%)]  Loss: 4.069 (4.40)  Time: 0.781s, 1311.54/s  (0.798s, 1282.53/s)  LR: 9.902e-04  Data: 0.014 (0.014)
Train: 19 [ 600/1251 ( 48%)]  Loss: 4.615 (4.42)  Time: 0.782s, 1309.58/s  (0.797s, 1284.44/s)  LR: 9.902e-04  Data: 0.010 (0.013)
Train: 19 [ 650/1251 ( 52%)]  Loss: 4.559 (4.43)  Time: 0.819s, 1249.75/s  (0.798s, 1283.62/s)  LR: 9.902e-04  Data: 0.015 (0.013)
Train: 19 [ 700/1251 ( 56%)]  Loss: 4.571 (4.44)  Time: 0.779s, 1314.73/s  (0.797s, 1284.98/s)  LR: 9.902e-04  Data: 0.011 (0.013)
Train: 19 [ 750/1251 ( 60%)]  Loss: 4.789 (4.46)  Time: 0.840s, 1218.99/s  (0.797s, 1284.99/s)  LR: 9.902e-04  Data: 0.010 (0.013)
Train: 19 [ 800/1251 ( 64%)]  Loss: 4.256 (4.45)  Time: 0.778s, 1316.38/s  (0.797s, 1284.19/s)  LR: 9.902e-04  Data: 0.011 (0.013)
Train: 19 [ 850/1251 ( 68%)]  Loss: 4.825 (4.47)  Time: 0.781s, 1311.93/s  (0.797s, 1285.15/s)  LR: 9.902e-04  Data: 0.012 (0.013)
Train: 19 [ 900/1251 ( 72%)]  Loss: 4.258 (4.46)  Time: 0.785s, 1305.21/s  (0.796s, 1286.44/s)  LR: 9.902e-04  Data: 0.010 (0.012)
Train: 19 [ 950/1251 ( 76%)]  Loss: 4.334 (4.45)  Time: 0.779s, 1315.07/s  (0.795s, 1287.48/s)  LR: 9.902e-04  Data: 0.009 (0.012)
Train: 19 [1000/1251 ( 80%)]  Loss: 4.645 (4.46)  Time: 0.780s, 1313.34/s  (0.795s, 1288.47/s)  LR: 9.902e-04  Data: 0.010 (0.012)
Train: 19 [1050/1251 ( 84%)]  Loss: 4.373 (4.46)  Time: 0.778s, 1316.24/s  (0.794s, 1289.48/s)  LR: 9.902e-04  Data: 0.010 (0.012)
Train: 19 [1100/1251 ( 88%)]  Loss: 4.545 (4.46)  Time: 0.792s, 1292.69/s  (0.794s, 1290.17/s)  LR: 9.902e-04  Data: 0.009 (0.012)
Train: 19 [1150/1251 ( 92%)]  Loss: 4.319 (4.45)  Time: 0.817s, 1253.05/s  (0.795s, 1288.70/s)  LR: 9.902e-04  Data: 0.011 (0.012)
Train: 19 [1200/1251 ( 96%)]  Loss: 4.143 (4.44)  Time: 0.789s, 1298.14/s  (0.795s, 1288.70/s)  LR: 9.902e-04  Data: 0.009 (0.012)
Train: 19 [1250/1251 (100%)]  Loss: 4.337 (4.44)  Time: 0.768s, 1334.16/s  (0.794s, 1289.61/s)  LR: 9.902e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.605 (1.605)  Loss:  1.3028 (1.3028)  Acc@1: 78.3203 (78.3203)  Acc@5: 92.8711 (92.8711)
Test: [  48/48]  Time: 0.171 (0.571)  Loss:  1.5875 (2.0322)  Acc@1: 74.0566 (59.5960)  Acc@5: 88.2075 (82.8860)
Current checkpoints:
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-19.pth.tar', 59.59599997802734)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-18.pth.tar', 58.53200002441406)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-17.pth.tar', 57.52200003417969)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-16.pth.tar', 57.08999988769531)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-15.pth.tar', 55.27200000976563)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-14.pth.tar', 54.29999998046875)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-13.pth.tar', 51.91000001464844)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-12.pth.tar', 51.34200011474609)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-11.pth.tar', 49.477999973144534)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-10.pth.tar', 48.46199996582031)

Train: 20 [   0/1251 (  0%)]  Loss: 4.511 (4.51)  Time: 2.361s,  433.71/s  (2.361s,  433.71/s)  LR: 9.892e-04  Data: 1.625 (1.625)
Train: 20 [  50/1251 (  4%)]  Loss: 4.533 (4.52)  Time: 0.796s, 1287.04/s  (0.833s, 1229.15/s)  LR: 9.892e-04  Data: 0.011 (0.042)
Train: 20 [ 100/1251 (  8%)]  Loss: 4.449 (4.50)  Time: 0.783s, 1308.47/s  (0.807s, 1269.05/s)  LR: 9.892e-04  Data: 0.011 (0.026)
Train: 20 [ 150/1251 ( 12%)]  Loss: 4.125 (4.40)  Time: 0.795s, 1288.71/s  (0.808s, 1267.38/s)  LR: 9.892e-04  Data: 0.009 (0.021)
Train: 20 [ 200/1251 ( 16%)]  Loss: 4.310 (4.39)  Time: 0.780s, 1312.02/s  (0.802s, 1276.13/s)  LR: 9.892e-04  Data: 0.011 (0.018)
Train: 20 [ 250/1251 ( 20%)]  Loss: 4.273 (4.37)  Time: 0.785s, 1304.29/s  (0.800s, 1280.02/s)  LR: 9.892e-04  Data: 0.009 (0.017)
Train: 20 [ 300/1251 ( 24%)]  Loss: 4.368 (4.37)  Time: 0.789s, 1297.23/s  (0.798s, 1283.88/s)  LR: 9.892e-04  Data: 0.011 (0.016)
Train: 20 [ 350/1251 ( 28%)]  Loss: 4.338 (4.36)  Time: 0.783s, 1307.15/s  (0.796s, 1285.95/s)  LR: 9.892e-04  Data: 0.010 (0.015)
Train: 20 [ 400/1251 ( 32%)]  Loss: 4.301 (4.36)  Time: 0.778s, 1316.93/s  (0.796s, 1286.44/s)  LR: 9.892e-04  Data: 0.010 (0.014)
Train: 20 [ 450/1251 ( 36%)]  Loss: 4.660 (4.39)  Time: 0.778s, 1315.86/s  (0.795s, 1288.41/s)  LR: 9.892e-04  Data: 0.009 (0.014)
Train: 20 [ 500/1251 ( 40%)]  Loss: 4.541 (4.40)  Time: 0.789s, 1298.60/s  (0.794s, 1290.28/s)  LR: 9.892e-04  Data: 0.010 (0.014)
Train: 20 [ 550/1251 ( 44%)]  Loss: 4.412 (4.40)  Time: 0.781s, 1311.95/s  (0.793s, 1291.45/s)  LR: 9.892e-04  Data: 0.010 (0.013)
Train: 20 [ 600/1251 ( 48%)]  Loss: 4.598 (4.42)  Time: 0.791s, 1294.85/s  (0.793s, 1291.85/s)  LR: 9.892e-04  Data: 0.011 (0.013)
Train: 20 [ 650/1251 ( 52%)]  Loss: 4.024 (4.39)  Time: 0.778s, 1315.82/s  (0.793s, 1291.68/s)  LR: 9.892e-04  Data: 0.010 (0.013)
Train: 20 [ 700/1251 ( 56%)]  Loss: 4.526 (4.40)  Time: 0.819s, 1250.05/s  (0.794s, 1290.23/s)  LR: 9.892e-04  Data: 0.011 (0.013)
Train: 20 [ 750/1251 ( 60%)]  Loss: 4.355 (4.40)  Time: 0.780s, 1312.82/s  (0.793s, 1291.17/s)  LR: 9.892e-04  Data: 0.010 (0.012)
Train: 20 [ 800/1251 ( 64%)]  Loss: 4.420 (4.40)  Time: 0.823s, 1244.67/s  (0.793s, 1291.99/s)  LR: 9.892e-04  Data: 0.011 (0.012)
Train: 20 [ 850/1251 ( 68%)]  Loss: 4.438 (4.40)  Time: 0.780s, 1312.98/s  (0.792s, 1292.38/s)  LR: 9.892e-04  Data: 0.011 (0.012)
Train: 20 [ 900/1251 ( 72%)]  Loss: 4.085 (4.38)  Time: 0.785s, 1303.63/s  (0.793s, 1292.06/s)  LR: 9.892e-04  Data: 0.011 (0.012)
Train: 20 [ 950/1251 ( 76%)]  Loss: 4.594 (4.39)  Time: 0.812s, 1260.52/s  (0.793s, 1292.07/s)  LR: 9.892e-04  Data: 0.009 (0.012)
Train: 20 [1000/1251 ( 80%)]  Loss: 4.451 (4.40)  Time: 0.780s, 1312.48/s  (0.793s, 1291.79/s)  LR: 9.892e-04  Data: 0.009 (0.012)
Train: 20 [1050/1251 ( 84%)]  Loss: 4.273 (4.39)  Time: 0.782s, 1310.19/s  (0.792s, 1292.49/s)  LR: 9.892e-04  Data: 0.009 (0.012)
Train: 20 [1100/1251 ( 88%)]  Loss: 4.402 (4.39)  Time: 0.819s, 1250.35/s  (0.792s, 1292.65/s)  LR: 9.892e-04  Data: 0.011 (0.012)
Train: 20 [1150/1251 ( 92%)]  Loss: 4.419 (4.39)  Time: 0.832s, 1230.59/s  (0.793s, 1290.64/s)  LR: 9.892e-04  Data: 0.011 (0.012)
Train: 20 [1200/1251 ( 96%)]  Loss: 4.395 (4.39)  Time: 0.780s, 1312.64/s  (0.794s, 1289.61/s)  LR: 9.892e-04  Data: 0.011 (0.012)
Train: 20 [1250/1251 (100%)]  Loss: 4.162 (4.38)  Time: 0.766s, 1336.92/s  (0.794s, 1290.48/s)  LR: 9.892e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.521 (1.521)  Loss:  1.2426 (1.2426)  Acc@1: 77.9297 (77.9297)  Acc@5: 94.0430 (94.0430)
Test: [  48/48]  Time: 0.172 (0.559)  Loss:  1.4336 (2.0907)  Acc@1: 76.1792 (59.1800)  Acc@5: 90.9198 (82.5220)
Current checkpoints:
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-19.pth.tar', 59.59599997802734)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-20.pth.tar', 59.179999995117186)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-18.pth.tar', 58.53200002441406)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-17.pth.tar', 57.52200003417969)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-16.pth.tar', 57.08999988769531)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-15.pth.tar', 55.27200000976563)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-14.pth.tar', 54.29999998046875)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-13.pth.tar', 51.91000001464844)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-12.pth.tar', 51.34200011474609)
 ('./output/train/20220201-174628-hrnet18-224/checkpoint-11.pth.tar', 49.477999973144534)

Train: 21 [   0/1251 (  0%)]  Loss: 4.416 (4.42)  Time: 2.348s,  436.15/s  (2.348s,  436.15/s)  LR: 9.881e-04  Data: 1.611 (1.611)
Train: 21 [  50/1251 (  4%)]  Loss: 4.001 (4.21)  Time: 0.778s, 1316.72/s  (0.834s, 1227.75/s)  LR: 9.881e-04  Data: 0.010 (0.048)
Train: 21 [ 100/1251 (  8%)]  Loss: 4.138 (4.19)  Time: 0.780s, 1312.81/s  (0.812s, 1261.37/s)  LR: 9.881e-04  Data: 0.012 (0.029)
Train: 21 [ 150/1251 ( 12%)]  Loss: 4.516 (4.27)  Time: 0.787s, 1300.40/s  (0.803s, 1275.99/s)  LR: 9.881e-04  Data: 0.009 (0.023)
Train: 21 [ 200/1251 ( 16%)]  Loss: 4.527 (4.32)  Time: 0.779s, 1313.72/s  (0.801s, 1278.94/s)  LR: 9.881e-04  Data: 0.011 (0.020)
Train: 21 [ 250/1251 ( 20%)]  Loss: 4.323 (4.32)  Time: 0.782s, 1309.94/s  (0.798s, 1283.23/s)  LR: 9.881e-04  Data: 0.009 (0.018)
WARNING:torch.distributed.elastic.agent.server.api:Received 15 death signal, shutting down workers
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 81872 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 81873 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 81874 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 81875 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 81876 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 81877 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 81878 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 81879 closing signal SIGTERM
Traceback (most recent call last):
  File "/home/baoshengyu/anaconda3/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/baoshengyu/anaconda3/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/baoshengyu/anaconda3/lib/python3.7/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/home/baoshengyu/anaconda3/lib/python3.7/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/home/baoshengyu/anaconda3/lib/python3.7/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/home/baoshengyu/anaconda3/lib/python3.7/site-packages/torch/distributed/run.py", line 713, in run
    )(*cmd_args)
  File "/home/baoshengyu/anaconda3/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/baoshengyu/anaconda3/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 252, in launch_agent
    result = agent.run()
  File "/home/baoshengyu/anaconda3/lib/python3.7/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/baoshengyu/anaconda3/lib/python3.7/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/baoshengyu/anaconda3/lib/python3.7/site-packages/torch/distributed/elastic/agent/server/api.py", line 843, in _invoke_run
    time.sleep(monitor_interval)
  File "/home/baoshengyu/anaconda3/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 60, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 81858 got signal: 15
/home/baoshengyu/anaconda3/lib/python3.7/site-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 3
Added key: store_based_barrier_key:1 to store for rank: 5
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 6
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 4
Added key: store_based_barrier_key:1 to store for rank: 7
Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 7, total 8.
Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 6, total 8.
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 8.
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 4, total 8.
Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 5, total 8.
Model hrnet18 created, param count:21301174
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.875
AMP not enabled. Training in float32.
Restoring model state from checkpoint...
Restoring optimizer state from checkpoint...
Loaded checkpoint 'output/train/20220201-174628-hrnet18-224/last.pth.tar' (epoch 20)
Using native Torch DistributedDataParallel.
Scheduled epochs: 310
Train: 21 [   0/1251 (  0%)]  Loss: 4.117 (4.12)  Time: 8.775s,  116.69/s  (8.775s,  116.69/s)  LR: 9.881e-04  Data: 2.005 (2.005)
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Train: 21 [  50/1251 (  4%)]  Loss: 4.019 (4.07)  Time: 0.807s, 1269.00/s  (0.988s, 1036.73/s)  LR: 9.881e-04  Data: 0.012 (0.050)
Train: 21 [ 100/1251 (  8%)]  Loss: 4.587 (4.24)  Time: 0.821s, 1247.17/s  (0.896s, 1142.82/s)  LR: 9.881e-04  Data: 0.011 (0.031)
Train: 21 [ 150/1251 ( 12%)]  Loss: 3.954 (4.17)  Time: 0.791s, 1294.43/s  (0.864s, 1185.73/s)  LR: 9.881e-04  Data: 0.011 (0.025)
Train: 21 [ 200/1251 ( 16%)]  Loss: 4.524 (4.24)  Time: 0.823s, 1244.84/s  (0.850s, 1204.96/s)  LR: 9.881e-04  Data: 0.012 (0.021)
Train: 21 [ 250/1251 ( 20%)]  Loss: 4.480 (4.28)  Time: 0.781s, 1311.23/s  (0.839s, 1219.78/s)  LR: 9.881e-04  Data: 0.011 (0.019)
Train: 21 [ 300/1251 ( 24%)]  Loss: 4.577 (4.32)  Time: 0.782s, 1309.56/s  (0.832s, 1230.27/s)  LR: 9.881e-04  Data: 0.012 (0.018)
Train: 21 [ 350/1251 ( 28%)]  Loss: 4.221 (4.31)  Time: 0.777s, 1317.32/s  (0.827s, 1238.69/s)  LR: 9.881e-04  Data: 0.011 (0.017)
Train: 21 [ 400/1251 ( 32%)]  Loss: 4.332 (4.31)  Time: 0.780s, 1312.67/s  (0.822s, 1245.19/s)  LR: 9.881e-04  Data: 0.011 (0.016)
Train: 21 [ 450/1251 ( 36%)]  Loss: 4.542 (4.34)  Time: 0.814s, 1257.69/s  (0.820s, 1248.61/s)  LR: 9.881e-04  Data: 0.011 (0.016)
Train: 21 [ 500/1251 ( 40%)]  Loss: 4.722 (4.37)  Time: 0.791s, 1294.32/s  (0.817s, 1253.13/s)  LR: 9.881e-04  Data: 0.011 (0.015)
Train: 21 [ 550/1251 ( 44%)]  Loss: 4.170 (4.35)  Time: 0.822s, 1245.91/s  (0.815s, 1256.32/s)  LR: 9.881e-04  Data: 0.011 (0.015)
Train: 21 [ 600/1251 ( 48%)]  Loss: 4.304 (4.35)  Time: 0.822s, 1245.44/s  (0.813s, 1259.01/s)  LR: 9.881e-04  Data: 0.011 (0.014)
Train: 21 [ 650/1251 ( 52%)]  Loss: 4.132 (4.33)  Time: 0.786s, 1302.19/s  (0.812s, 1260.62/s)  LR: 9.881e-04  Data: 0.013 (0.014)
Train: 21 [ 700/1251 ( 56%)]  Loss: 4.562 (4.35)  Time: 0.776s, 1319.79/s  (0.811s, 1263.24/s)  LR: 9.881e-04  Data: 0.011 (0.014)
Train: 21 [ 750/1251 ( 60%)]  Loss: 4.614 (4.37)  Time: 0.786s, 1302.97/s  (0.809s, 1265.22/s)  LR: 9.881e-04  Data: 0.011 (0.014)
Train: 21 [ 800/1251 ( 64%)]  Loss: 4.654 (4.38)  Time: 0.780s, 1313.65/s  (0.808s, 1267.51/s)  LR: 9.881e-04  Data: 0.011 (0.014)
Train: 21 [ 850/1251 ( 68%)]  Loss: 4.190 (4.37)  Time: 0.793s, 1291.79/s  (0.807s, 1269.52/s)  LR: 9.881e-04  Data: 0.011 (0.013)
Train: 21 [ 900/1251 ( 72%)]  Loss: 4.377 (4.37)  Time: 0.782s, 1309.29/s  (0.806s, 1271.17/s)  LR: 9.881e-04  Data: 0.011 (0.013)
Train: 21 [ 950/1251 ( 76%)]  Loss: 4.181 (4.36)  Time: 0.810s, 1263.79/s  (0.806s, 1270.87/s)  LR: 9.881e-04  Data: 0.012 (0.013)
Train: 21 [1000/1251 ( 80%)]  Loss: 3.999 (4.35)  Time: 0.798s, 1282.59/s  (0.806s, 1271.16/s)  LR: 9.881e-04  Data: 0.011 (0.013)
Train: 21 [1050/1251 ( 84%)]  Loss: 4.766 (4.36)  Time: 0.778s, 1316.83/s  (0.805s, 1271.78/s)  LR: 9.881e-04  Data: 0.011 (0.013)
Train: 21 [1100/1251 ( 88%)]  Loss: 4.636 (4.38)  Time: 0.882s, 1161.07/s  (0.806s, 1270.69/s)  LR: 9.881e-04  Data: 0.011 (0.013)
Train: 21 [1150/1251 ( 92%)]  Loss: 4.332 (4.37)  Time: 0.777s, 1317.57/s  (0.805s, 1272.33/s)  LR: 9.881e-04  Data: 0.011 (0.013)
Train: 21 [1200/1251 ( 96%)]  Loss: 4.362 (4.37)  Time: 0.814s, 1257.93/s  (0.805s, 1272.25/s)  LR: 9.881e-04  Data: 0.010 (0.013)
Train: 21 [1250/1251 (100%)]  Loss: 4.271 (4.37)  Time: 0.770s, 1329.65/s  (0.805s, 1272.38/s)  LR: 9.881e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.915 (1.915)  Loss:  1.3794 (1.3794)  Acc@1: 76.8555 (76.8555)  Acc@5: 93.5547 (93.5547)
Test: [  48/48]  Time: 2.971 (0.624)  Loss:  1.4398 (1.9849)  Acc@1: 75.0000 (60.1500)  Acc@5: 90.6840 (83.4840)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-21.pth.tar', 60.15)

Train: 22 [   0/1251 (  0%)]  Loss: 4.121 (4.12)  Time: 3.154s,  324.69/s  (3.154s,  324.69/s)  LR: 9.869e-04  Data: 1.934 (1.934)
Train: 22 [  50/1251 (  4%)]  Loss: 4.331 (4.23)  Time: 0.841s, 1217.85/s  (0.834s, 1228.13/s)  LR: 9.869e-04  Data: 0.011 (0.049)
Train: 22 [ 100/1251 (  8%)]  Loss: 4.333 (4.26)  Time: 0.779s, 1314.17/s  (0.809s, 1265.42/s)  LR: 9.869e-04  Data: 0.011 (0.030)
Train: 22 [ 150/1251 ( 12%)]  Loss: 4.422 (4.30)  Time: 0.783s, 1308.54/s  (0.800s, 1279.67/s)  LR: 9.869e-04  Data: 0.010 (0.023)
Train: 22 [ 200/1251 ( 16%)]  Loss: 4.274 (4.30)  Time: 0.778s, 1315.68/s  (0.796s, 1285.63/s)  LR: 9.869e-04  Data: 0.010 (0.020)
Train: 22 [ 250/1251 ( 20%)]  Loss: 4.424 (4.32)  Time: 0.779s, 1314.29/s  (0.793s, 1290.66/s)  LR: 9.869e-04  Data: 0.010 (0.018)
Train: 22 [ 300/1251 ( 24%)]  Loss: 4.515 (4.35)  Time: 0.780s, 1313.02/s  (0.792s, 1293.35/s)  LR: 9.869e-04  Data: 0.011 (0.017)
Train: 22 [ 350/1251 ( 28%)]  Loss: 4.198 (4.33)  Time: 0.780s, 1312.84/s  (0.791s, 1294.45/s)  LR: 9.869e-04  Data: 0.011 (0.016)
Train: 22 [ 400/1251 ( 32%)]  Loss: 4.015 (4.29)  Time: 0.779s, 1314.15/s  (0.790s, 1295.66/s)  LR: 9.869e-04  Data: 0.011 (0.016)
Train: 22 [ 450/1251 ( 36%)]  Loss: 4.064 (4.27)  Time: 0.779s, 1314.25/s  (0.789s, 1297.71/s)  LR: 9.869e-04  Data: 0.011 (0.015)
Train: 22 [ 500/1251 ( 40%)]  Loss: 4.586 (4.30)  Time: 0.781s, 1311.74/s  (0.790s, 1296.73/s)  LR: 9.869e-04  Data: 0.011 (0.015)
Train: 22 [ 550/1251 ( 44%)]  Loss: 4.062 (4.28)  Time: 0.783s, 1307.41/s  (0.790s, 1296.66/s)  LR: 9.869e-04  Data: 0.010 (0.014)
Train: 22 [ 600/1251 ( 48%)]  Loss: 4.693 (4.31)  Time: 0.779s, 1314.88/s  (0.789s, 1297.81/s)  LR: 9.869e-04  Data: 0.011 (0.014)
Train: 22 [ 650/1251 ( 52%)]  Loss: 3.979 (4.29)  Time: 0.781s, 1310.62/s  (0.790s, 1296.99/s)  LR: 9.869e-04  Data: 0.010 (0.014)
Train: 22 [ 700/1251 ( 56%)]  Loss: 4.389 (4.29)  Time: 0.821s, 1247.48/s  (0.790s, 1296.85/s)  LR: 9.869e-04  Data: 0.011 (0.014)
Train: 22 [ 750/1251 ( 60%)]  Loss: 4.483 (4.31)  Time: 0.778s, 1316.53/s  (0.790s, 1296.39/s)  LR: 9.869e-04  Data: 0.011 (0.013)
Train: 22 [ 800/1251 ( 64%)]  Loss: 4.511 (4.32)  Time: 0.779s, 1314.08/s  (0.789s, 1297.36/s)  LR: 9.869e-04  Data: 0.011 (0.013)
Train: 22 [ 850/1251 ( 68%)]  Loss: 4.444 (4.32)  Time: 0.829s, 1234.88/s  (0.789s, 1297.40/s)  LR: 9.869e-04  Data: 0.011 (0.013)
Train: 22 [ 900/1251 ( 72%)]  Loss: 4.629 (4.34)  Time: 0.780s, 1312.23/s  (0.790s, 1296.81/s)  LR: 9.869e-04  Data: 0.011 (0.013)
Train: 22 [ 950/1251 ( 76%)]  Loss: 4.464 (4.35)  Time: 0.779s, 1315.23/s  (0.789s, 1297.04/s)  LR: 9.869e-04  Data: 0.011 (0.013)
Train: 22 [1000/1251 ( 80%)]  Loss: 4.506 (4.35)  Time: 0.778s, 1315.46/s  (0.789s, 1297.50/s)  LR: 9.869e-04  Data: 0.011 (0.013)
Train: 22 [1050/1251 ( 84%)]  Loss: 4.135 (4.34)  Time: 0.781s, 1311.87/s  (0.789s, 1297.91/s)  LR: 9.869e-04  Data: 0.012 (0.013)
Train: 22 [1100/1251 ( 88%)]  Loss: 4.188 (4.34)  Time: 0.777s, 1317.99/s  (0.789s, 1298.34/s)  LR: 9.869e-04  Data: 0.012 (0.013)
Train: 22 [1150/1251 ( 92%)]  Loss: 3.762 (4.31)  Time: 0.781s, 1311.64/s  (0.789s, 1298.59/s)  LR: 9.869e-04  Data: 0.012 (0.013)
Train: 22 [1200/1251 ( 96%)]  Loss: 4.581 (4.32)  Time: 0.813s, 1260.01/s  (0.789s, 1298.20/s)  LR: 9.869e-04  Data: 0.011 (0.013)
Train: 22 [1250/1251 (100%)]  Loss: 3.868 (4.31)  Time: 0.768s, 1332.50/s  (0.789s, 1298.01/s)  LR: 9.869e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.563 (1.563)  Loss:  1.2478 (1.2478)  Acc@1: 78.7109 (78.7109)  Acc@5: 93.2617 (93.2617)
Test: [  48/48]  Time: 0.172 (0.562)  Loss:  1.1773 (1.9387)  Acc@1: 77.4764 (61.2280)  Acc@5: 91.8632 (84.5440)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-22.pth.tar', 61.228000041503904)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-21.pth.tar', 60.15)

Train: 23 [   0/1251 (  0%)]  Loss: 4.349 (4.35)  Time: 2.241s,  456.92/s  (2.241s,  456.92/s)  LR: 9.857e-04  Data: 1.507 (1.507)
Train: 23 [  50/1251 (  4%)]  Loss: 4.076 (4.21)  Time: 0.777s, 1317.08/s  (0.820s, 1248.81/s)  LR: 9.857e-04  Data: 0.011 (0.045)
Train: 23 [ 100/1251 (  8%)]  Loss: 4.408 (4.28)  Time: 0.780s, 1312.69/s  (0.803s, 1275.13/s)  LR: 9.857e-04  Data: 0.011 (0.028)
Train: 23 [ 150/1251 ( 12%)]  Loss: 4.513 (4.34)  Time: 0.787s, 1300.56/s  (0.796s, 1285.65/s)  LR: 9.857e-04  Data: 0.011 (0.022)
Train: 23 [ 200/1251 ( 16%)]  Loss: 4.266 (4.32)  Time: 0.839s, 1219.97/s  (0.795s, 1287.77/s)  LR: 9.857e-04  Data: 0.011 (0.019)
Train: 23 [ 250/1251 ( 20%)]  Loss: 4.030 (4.27)  Time: 0.785s, 1305.14/s  (0.795s, 1288.67/s)  LR: 9.857e-04  Data: 0.010 (0.018)
Train: 23 [ 300/1251 ( 24%)]  Loss: 4.283 (4.28)  Time: 0.779s, 1315.25/s  (0.793s, 1291.87/s)  LR: 9.857e-04  Data: 0.010 (0.016)
Train: 23 [ 350/1251 ( 28%)]  Loss: 4.383 (4.29)  Time: 0.784s, 1306.02/s  (0.792s, 1293.39/s)  LR: 9.857e-04  Data: 0.011 (0.016)
Train: 23 [ 400/1251 ( 32%)]  Loss: 4.694 (4.33)  Time: 0.780s, 1312.18/s  (0.791s, 1294.42/s)  LR: 9.857e-04  Data: 0.011 (0.015)
Train: 23 [ 450/1251 ( 36%)]  Loss: 4.581 (4.36)  Time: 0.792s, 1292.30/s  (0.790s, 1295.73/s)  LR: 9.857e-04  Data: 0.010 (0.015)
Train: 23 [ 500/1251 ( 40%)]  Loss: 4.193 (4.34)  Time: 0.781s, 1311.78/s  (0.791s, 1293.89/s)  LR: 9.857e-04  Data: 0.011 (0.014)
Train: 23 [ 550/1251 ( 44%)]  Loss: 4.109 (4.32)  Time: 0.829s, 1234.50/s  (0.791s, 1294.45/s)  LR: 9.857e-04  Data: 0.011 (0.014)
Train: 23 [ 600/1251 ( 48%)]  Loss: 4.392 (4.33)  Time: 0.814s, 1258.07/s  (0.791s, 1294.04/s)  LR: 9.857e-04  Data: 0.011 (0.014)
Train: 23 [ 650/1251 ( 52%)]  Loss: 4.122 (4.31)  Time: 0.780s, 1313.17/s  (0.791s, 1294.96/s)  LR: 9.857e-04  Data: 0.010 (0.014)
Train: 23 [ 700/1251 ( 56%)]  Loss: 3.743 (4.28)  Time: 0.781s, 1311.09/s  (0.790s, 1296.07/s)  LR: 9.857e-04  Data: 0.011 (0.013)
Train: 23 [ 750/1251 ( 60%)]  Loss: 4.344 (4.28)  Time: 0.778s, 1316.11/s  (0.790s, 1296.76/s)  LR: 9.857e-04  Data: 0.010 (0.013)
Train: 23 [ 800/1251 ( 64%)]  Loss: 4.292 (4.28)  Time: 0.781s, 1310.78/s  (0.790s, 1296.89/s)  LR: 9.857e-04  Data: 0.011 (0.013)
Train: 23 [ 850/1251 ( 68%)]  Loss: 4.354 (4.29)  Time: 0.781s, 1311.88/s  (0.789s, 1297.16/s)  LR: 9.857e-04  Data: 0.011 (0.013)
Train: 23 [ 900/1251 ( 72%)]  Loss: 4.440 (4.29)  Time: 0.776s, 1319.24/s  (0.790s, 1296.67/s)  LR: 9.857e-04  Data: 0.011 (0.013)
Train: 23 [ 950/1251 ( 76%)]  Loss: 3.950 (4.28)  Time: 0.804s, 1273.00/s  (0.791s, 1294.77/s)  LR: 9.857e-04  Data: 0.010 (0.013)
Train: 23 [1000/1251 ( 80%)]  Loss: 4.288 (4.28)  Time: 0.813s, 1260.06/s  (0.792s, 1292.61/s)  LR: 9.857e-04  Data: 0.011 (0.013)
Train: 23 [1050/1251 ( 84%)]  Loss: 4.580 (4.29)  Time: 0.779s, 1315.22/s  (0.792s, 1293.05/s)  LR: 9.857e-04  Data: 0.010 (0.013)
Train: 23 [1100/1251 ( 88%)]  Loss: 4.523 (4.30)  Time: 0.786s, 1303.50/s  (0.792s, 1293.17/s)  LR: 9.857e-04  Data: 0.010 (0.012)
Train: 23 [1150/1251 ( 92%)]  Loss: 4.292 (4.30)  Time: 0.812s, 1260.84/s  (0.793s, 1291.81/s)  LR: 9.857e-04  Data: 0.011 (0.012)
Train: 23 [1200/1251 ( 96%)]  Loss: 4.177 (4.30)  Time: 0.779s, 1314.29/s  (0.793s, 1291.01/s)  LR: 9.857e-04  Data: 0.011 (0.012)
Train: 23 [1250/1251 (100%)]  Loss: 3.942 (4.28)  Time: 0.797s, 1284.37/s  (0.793s, 1291.37/s)  LR: 9.857e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.573 (1.573)  Loss:  1.2443 (1.2443)  Acc@1: 79.1016 (79.1016)  Acc@5: 93.3594 (93.3594)
Test: [  48/48]  Time: 0.172 (0.558)  Loss:  1.2204 (1.9037)  Acc@1: 76.7689 (61.4220)  Acc@5: 92.8066 (84.4980)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-23.pth.tar', 61.422000122070315)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-22.pth.tar', 61.228000041503904)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-21.pth.tar', 60.15)

Train: 24 [   0/1251 (  0%)]  Loss: 4.271 (4.27)  Time: 2.191s,  467.27/s  (2.191s,  467.27/s)  LR: 9.844e-04  Data: 1.452 (1.452)
Train: 24 [  50/1251 (  4%)]  Loss: 4.354 (4.31)  Time: 0.827s, 1238.34/s  (0.834s, 1227.81/s)  LR: 9.844e-04  Data: 0.011 (0.044)
Train: 24 [ 100/1251 (  8%)]  Loss: 4.430 (4.35)  Time: 0.836s, 1225.05/s  (0.820s, 1249.01/s)  LR: 9.844e-04  Data: 0.011 (0.028)
Train: 24 [ 150/1251 ( 12%)]  Loss: 4.431 (4.37)  Time: 0.776s, 1318.83/s  (0.808s, 1267.16/s)  LR: 9.844e-04  Data: 0.011 (0.022)
Train: 24 [ 200/1251 ( 16%)]  Loss: 4.346 (4.37)  Time: 0.779s, 1314.66/s  (0.803s, 1275.97/s)  LR: 9.844e-04  Data: 0.012 (0.019)
Train: 24 [ 250/1251 ( 20%)]  Loss: 4.353 (4.36)  Time: 0.794s, 1288.97/s  (0.799s, 1282.10/s)  LR: 9.844e-04  Data: 0.028 (0.018)
Train: 24 [ 300/1251 ( 24%)]  Loss: 3.751 (4.28)  Time: 0.778s, 1316.30/s  (0.796s, 1285.81/s)  LR: 9.844e-04  Data: 0.011 (0.017)
Train: 24 [ 350/1251 ( 28%)]  Loss: 4.617 (4.32)  Time: 0.779s, 1314.95/s  (0.797s, 1284.80/s)  LR: 9.844e-04  Data: 0.011 (0.016)
Train: 24 [ 400/1251 ( 32%)]  Loss: 4.186 (4.30)  Time: 0.777s, 1318.60/s  (0.796s, 1286.24/s)  LR: 9.844e-04  Data: 0.011 (0.015)
Train: 24 [ 450/1251 ( 36%)]  Loss: 4.198 (4.29)  Time: 0.777s, 1317.83/s  (0.796s, 1285.74/s)  LR: 9.844e-04  Data: 0.011 (0.015)
Train: 24 [ 500/1251 ( 40%)]  Loss: 4.276 (4.29)  Time: 0.778s, 1315.48/s  (0.796s, 1287.19/s)  LR: 9.844e-04  Data: 0.012 (0.014)
Train: 24 [ 550/1251 ( 44%)]  Loss: 4.399 (4.30)  Time: 0.825s, 1240.97/s  (0.796s, 1286.86/s)  LR: 9.844e-04  Data: 0.012 (0.014)
Train: 24 [ 600/1251 ( 48%)]  Loss: 4.627 (4.33)  Time: 0.779s, 1314.89/s  (0.795s, 1288.73/s)  LR: 9.844e-04  Data: 0.012 (0.014)
Train: 24 [ 650/1251 ( 52%)]  Loss: 4.581 (4.34)  Time: 0.796s, 1286.20/s  (0.794s, 1290.04/s)  LR: 9.844e-04  Data: 0.011 (0.014)
Train: 24 [ 700/1251 ( 56%)]  Loss: 4.103 (4.33)  Time: 0.781s, 1311.33/s  (0.794s, 1290.46/s)  LR: 9.844e-04  Data: 0.011 (0.014)
Train: 24 [ 750/1251 ( 60%)]  Loss: 4.188 (4.32)  Time: 0.780s, 1312.51/s  (0.793s, 1290.88/s)  LR: 9.844e-04  Data: 0.011 (0.013)
Train: 24 [ 800/1251 ( 64%)]  Loss: 3.973 (4.30)  Time: 0.798s, 1283.12/s  (0.793s, 1291.82/s)  LR: 9.844e-04  Data: 0.011 (0.013)
Train: 24 [ 850/1251 ( 68%)]  Loss: 4.501 (4.31)  Time: 0.781s, 1311.12/s  (0.792s, 1292.93/s)  LR: 9.844e-04  Data: 0.011 (0.013)
Train: 24 [ 900/1251 ( 72%)]  Loss: 4.263 (4.31)  Time: 0.781s, 1311.06/s  (0.792s, 1292.99/s)  LR: 9.844e-04  Data: 0.010 (0.013)
Train: 24 [ 950/1251 ( 76%)]  Loss: 3.820 (4.28)  Time: 0.780s, 1312.64/s  (0.793s, 1292.02/s)  LR: 9.844e-04  Data: 0.010 (0.013)
Train: 24 [1000/1251 ( 80%)]  Loss: 4.275 (4.28)  Time: 0.792s, 1292.25/s  (0.793s, 1292.07/s)  LR: 9.844e-04  Data: 0.011 (0.013)
Train: 24 [1050/1251 ( 84%)]  Loss: 4.517 (4.29)  Time: 0.777s, 1317.91/s  (0.793s, 1291.75/s)  LR: 9.844e-04  Data: 0.012 (0.013)
Train: 24 [1100/1251 ( 88%)]  Loss: 3.873 (4.28)  Time: 0.842s, 1216.45/s  (0.794s, 1290.38/s)  LR: 9.844e-04  Data: 0.011 (0.013)
Train: 24 [1150/1251 ( 92%)]  Loss: 3.884 (4.26)  Time: 0.779s, 1314.30/s  (0.793s, 1291.24/s)  LR: 9.844e-04  Data: 0.011 (0.012)
Train: 24 [1200/1251 ( 96%)]  Loss: 4.320 (4.26)  Time: 0.778s, 1316.50/s  (0.793s, 1291.73/s)  LR: 9.844e-04  Data: 0.011 (0.012)
Train: 24 [1250/1251 (100%)]  Loss: 4.314 (4.26)  Time: 0.785s, 1304.99/s  (0.793s, 1291.90/s)  LR: 9.844e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.524 (1.524)  Loss:  1.1950 (1.1950)  Acc@1: 77.9297 (77.9297)  Acc@5: 92.8711 (92.8711)
Test: [  48/48]  Time: 0.172 (0.566)  Loss:  1.2213 (1.9357)  Acc@1: 76.7689 (61.0380)  Acc@5: 91.6273 (84.0300)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-23.pth.tar', 61.422000122070315)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-22.pth.tar', 61.228000041503904)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-24.pth.tar', 61.038000122070315)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-21.pth.tar', 60.15)

Train: 25 [   0/1251 (  0%)]  Loss: 4.402 (4.40)  Time: 2.235s,  458.24/s  (2.235s,  458.24/s)  LR: 9.831e-04  Data: 1.500 (1.500)
Train: 25 [  50/1251 (  4%)]  Loss: 4.384 (4.39)  Time: 0.782s, 1310.00/s  (0.814s, 1258.24/s)  LR: 9.831e-04  Data: 0.011 (0.044)
Train: 25 [ 100/1251 (  8%)]  Loss: 4.501 (4.43)  Time: 0.802s, 1277.39/s  (0.802s, 1277.03/s)  LR: 9.831e-04  Data: 0.011 (0.028)
Train: 25 [ 150/1251 ( 12%)]  Loss: 4.303 (4.40)  Time: 0.778s, 1316.51/s  (0.802s, 1276.46/s)  LR: 9.831e-04  Data: 0.010 (0.022)
Train: 25 [ 200/1251 ( 16%)]  Loss: 3.936 (4.31)  Time: 0.780s, 1313.54/s  (0.803s, 1275.27/s)  LR: 9.831e-04  Data: 0.012 (0.019)
Train: 25 [ 250/1251 ( 20%)]  Loss: 4.210 (4.29)  Time: 0.791s, 1294.00/s  (0.800s, 1280.57/s)  LR: 9.831e-04  Data: 0.011 (0.017)
Train: 25 [ 300/1251 ( 24%)]  Loss: 4.327 (4.29)  Time: 0.777s, 1317.43/s  (0.798s, 1282.95/s)  LR: 9.831e-04  Data: 0.011 (0.016)
Train: 25 [ 350/1251 ( 28%)]  Loss: 4.376 (4.30)  Time: 0.814s, 1258.51/s  (0.797s, 1285.53/s)  LR: 9.831e-04  Data: 0.010 (0.016)
Train: 25 [ 400/1251 ( 32%)]  Loss: 4.390 (4.31)  Time: 0.779s, 1313.81/s  (0.798s, 1283.64/s)  LR: 9.831e-04  Data: 0.011 (0.015)
Train: 25 [ 450/1251 ( 36%)]  Loss: 4.525 (4.34)  Time: 0.781s, 1310.39/s  (0.797s, 1285.09/s)  LR: 9.831e-04  Data: 0.012 (0.015)
Train: 25 [ 500/1251 ( 40%)]  Loss: 4.125 (4.32)  Time: 0.828s, 1236.87/s  (0.797s, 1284.50/s)  LR: 9.831e-04  Data: 0.011 (0.014)
Train: 25 [ 550/1251 ( 44%)]  Loss: 4.364 (4.32)  Time: 0.788s, 1299.13/s  (0.797s, 1285.36/s)  LR: 9.831e-04  Data: 0.011 (0.014)
Train: 25 [ 600/1251 ( 48%)]  Loss: 4.078 (4.30)  Time: 0.782s, 1308.82/s  (0.796s, 1286.87/s)  LR: 9.831e-04  Data: 0.011 (0.014)
Train: 25 [ 650/1251 ( 52%)]  Loss: 3.886 (4.27)  Time: 0.778s, 1315.56/s  (0.795s, 1288.37/s)  LR: 9.831e-04  Data: 0.011 (0.013)
Train: 25 [ 700/1251 ( 56%)]  Loss: 4.464 (4.28)  Time: 0.777s, 1317.49/s  (0.794s, 1290.03/s)  LR: 9.831e-04  Data: 0.011 (0.013)
Train: 25 [ 750/1251 ( 60%)]  Loss: 4.263 (4.28)  Time: 0.807s, 1268.15/s  (0.794s, 1289.62/s)  LR: 9.831e-04  Data: 0.011 (0.013)
Train: 25 [ 800/1251 ( 64%)]  Loss: 4.396 (4.29)  Time: 0.783s, 1307.58/s  (0.794s, 1289.30/s)  LR: 9.831e-04  Data: 0.011 (0.013)
Train: 25 [ 850/1251 ( 68%)]  Loss: 4.201 (4.29)  Time: 0.806s, 1270.11/s  (0.794s, 1289.80/s)  LR: 9.831e-04  Data: 0.012 (0.013)
Train: 25 [ 900/1251 ( 72%)]  Loss: 4.430 (4.29)  Time: 0.777s, 1317.47/s  (0.793s, 1290.70/s)  LR: 9.831e-04  Data: 0.011 (0.013)
Train: 25 [ 950/1251 ( 76%)]  Loss: 3.983 (4.28)  Time: 0.816s, 1255.63/s  (0.794s, 1289.81/s)  LR: 9.831e-04  Data: 0.010 (0.013)
Train: 25 [1000/1251 ( 80%)]  Loss: 3.808 (4.25)  Time: 0.814s, 1258.35/s  (0.793s, 1290.49/s)  LR: 9.831e-04  Data: 0.012 (0.013)
Train: 25 [1050/1251 ( 84%)]  Loss: 4.179 (4.25)  Time: 0.790s, 1296.26/s  (0.794s, 1290.37/s)  LR: 9.831e-04  Data: 0.012 (0.012)
Train: 25 [1100/1251 ( 88%)]  Loss: 4.434 (4.26)  Time: 0.780s, 1313.06/s  (0.794s, 1289.45/s)  LR: 9.831e-04  Data: 0.011 (0.012)
Train: 25 [1150/1251 ( 92%)]  Loss: 4.269 (4.26)  Time: 0.778s, 1315.87/s  (0.794s, 1289.98/s)  LR: 9.831e-04  Data: 0.010 (0.012)
Train: 25 [1200/1251 ( 96%)]  Loss: 4.469 (4.27)  Time: 0.776s, 1319.58/s  (0.794s, 1289.86/s)  LR: 9.831e-04  Data: 0.011 (0.012)
Train: 25 [1250/1251 (100%)]  Loss: 4.314 (4.27)  Time: 0.770s, 1329.50/s  (0.794s, 1290.07/s)  LR: 9.831e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.541 (1.541)  Loss:  1.1855 (1.1855)  Acc@1: 83.6914 (83.6914)  Acc@5: 95.1172 (95.1172)
Test: [  48/48]  Time: 0.172 (0.558)  Loss:  1.3423 (1.9934)  Acc@1: 77.1227 (62.1480)  Acc@5: 91.0377 (85.0260)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-25.pth.tar', 62.14800014648438)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-23.pth.tar', 61.422000122070315)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-22.pth.tar', 61.228000041503904)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-24.pth.tar', 61.038000122070315)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-21.pth.tar', 60.15)

Train: 26 [   0/1251 (  0%)]  Loss: 4.324 (4.32)  Time: 2.193s,  466.93/s  (2.193s,  466.93/s)  LR: 9.818e-04  Data: 1.458 (1.458)
Train: 26 [  50/1251 (  4%)]  Loss: 4.401 (4.36)  Time: 0.779s, 1314.65/s  (0.818s, 1252.00/s)  LR: 9.818e-04  Data: 0.010 (0.043)
Train: 26 [ 100/1251 (  8%)]  Loss: 4.151 (4.29)  Time: 0.788s, 1300.27/s  (0.802s, 1276.77/s)  LR: 9.818e-04  Data: 0.011 (0.027)
Train: 26 [ 150/1251 ( 12%)]  Loss: 4.414 (4.32)  Time: 0.796s, 1286.86/s  (0.800s, 1279.42/s)  LR: 9.818e-04  Data: 0.011 (0.022)
Train: 26 [ 200/1251 ( 16%)]  Loss: 4.041 (4.27)  Time: 0.779s, 1315.07/s  (0.798s, 1283.35/s)  LR: 9.818e-04  Data: 0.011 (0.019)
Train: 26 [ 250/1251 ( 20%)]  Loss: 3.930 (4.21)  Time: 0.843s, 1214.60/s  (0.797s, 1285.46/s)  LR: 9.818e-04  Data: 0.011 (0.017)
Train: 26 [ 300/1251 ( 24%)]  Loss: 4.072 (4.19)  Time: 0.821s, 1247.25/s  (0.799s, 1282.17/s)  LR: 9.818e-04  Data: 0.012 (0.016)
Train: 26 [ 350/1251 ( 28%)]  Loss: 4.092 (4.18)  Time: 0.777s, 1317.55/s  (0.797s, 1284.57/s)  LR: 9.818e-04  Data: 0.010 (0.016)
Train: 26 [ 400/1251 ( 32%)]  Loss: 4.544 (4.22)  Time: 0.817s, 1253.74/s  (0.797s, 1284.28/s)  LR: 9.818e-04  Data: 0.011 (0.015)
Train: 26 [ 450/1251 ( 36%)]  Loss: 4.462 (4.24)  Time: 0.835s, 1225.99/s  (0.797s, 1284.12/s)  LR: 9.818e-04  Data: 0.011 (0.014)
Train: 26 [ 500/1251 ( 40%)]  Loss: 4.249 (4.24)  Time: 0.791s, 1294.46/s  (0.797s, 1284.65/s)  LR: 9.818e-04  Data: 0.010 (0.014)
Train: 26 [ 550/1251 ( 44%)]  Loss: 4.119 (4.23)  Time: 0.781s, 1310.65/s  (0.798s, 1283.47/s)  LR: 9.818e-04  Data: 0.011 (0.014)
Train: 26 [ 600/1251 ( 48%)]  Loss: 4.061 (4.22)  Time: 0.777s, 1317.07/s  (0.797s, 1285.09/s)  LR: 9.818e-04  Data: 0.011 (0.014)
Train: 26 [ 650/1251 ( 52%)]  Loss: 4.494 (4.24)  Time: 0.781s, 1310.87/s  (0.796s, 1286.61/s)  LR: 9.818e-04  Data: 0.010 (0.013)
Train: 26 [ 700/1251 ( 56%)]  Loss: 3.972 (4.22)  Time: 0.780s, 1313.08/s  (0.795s, 1288.10/s)  LR: 9.818e-04  Data: 0.011 (0.013)
Train: 26 [ 750/1251 ( 60%)]  Loss: 4.574 (4.24)  Time: 0.811s, 1261.86/s  (0.797s, 1285.44/s)  LR: 9.818e-04  Data: 0.011 (0.013)
Train: 26 [ 800/1251 ( 64%)]  Loss: 3.616 (4.21)  Time: 0.835s, 1226.46/s  (0.798s, 1283.13/s)  LR: 9.818e-04  Data: 0.011 (0.013)
Train: 26 [ 850/1251 ( 68%)]  Loss: 4.519 (4.22)  Time: 0.825s, 1241.22/s  (0.798s, 1283.17/s)  LR: 9.818e-04  Data: 0.011 (0.013)
Train: 26 [ 900/1251 ( 72%)]  Loss: 4.557 (4.24)  Time: 0.924s, 1108.11/s  (0.798s, 1282.43/s)  LR: 9.818e-04  Data: 0.011 (0.013)
Train: 26 [ 950/1251 ( 76%)]  Loss: 4.396 (4.25)  Time: 0.776s, 1319.19/s  (0.799s, 1281.30/s)  LR: 9.818e-04  Data: 0.011 (0.013)
Train: 26 [1000/1251 ( 80%)]  Loss: 3.886 (4.23)  Time: 0.779s, 1314.21/s  (0.800s, 1280.04/s)  LR: 9.818e-04  Data: 0.010 (0.012)
Train: 26 [1050/1251 ( 84%)]  Loss: 4.323 (4.24)  Time: 0.833s, 1229.36/s  (0.800s, 1280.72/s)  LR: 9.818e-04  Data: 0.011 (0.012)
Train: 26 [1100/1251 ( 88%)]  Loss: 4.567 (4.25)  Time: 0.780s, 1312.89/s  (0.799s, 1281.06/s)  LR: 9.818e-04  Data: 0.011 (0.012)
Train: 26 [1150/1251 ( 92%)]  Loss: 4.026 (4.24)  Time: 0.779s, 1315.06/s  (0.799s, 1281.66/s)  LR: 9.818e-04  Data: 0.011 (0.012)
Train: 26 [1200/1251 ( 96%)]  Loss: 4.263 (4.24)  Time: 0.778s, 1315.87/s  (0.799s, 1282.08/s)  LR: 9.818e-04  Data: 0.012 (0.012)
Train: 26 [1250/1251 (100%)]  Loss: 3.750 (4.22)  Time: 0.763s, 1342.90/s  (0.798s, 1283.16/s)  LR: 9.818e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.545 (1.545)  Loss:  1.1518 (1.1518)  Acc@1: 78.2227 (78.2227)  Acc@5: 93.4570 (93.4570)
Test: [  48/48]  Time: 0.172 (0.566)  Loss:  1.0370 (1.8171)  Acc@1: 80.1887 (61.9220)  Acc@5: 93.3962 (84.8400)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-25.pth.tar', 62.14800014648438)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-26.pth.tar', 61.92200005615234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-23.pth.tar', 61.422000122070315)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-22.pth.tar', 61.228000041503904)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-24.pth.tar', 61.038000122070315)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-21.pth.tar', 60.15)

Train: 27 [   0/1251 (  0%)]  Loss: 4.003 (4.00)  Time: 2.345s,  436.59/s  (2.345s,  436.59/s)  LR: 9.803e-04  Data: 1.610 (1.610)
Train: 27 [  50/1251 (  4%)]  Loss: 4.547 (4.27)  Time: 0.780s, 1312.99/s  (0.822s, 1245.21/s)  LR: 9.803e-04  Data: 0.011 (0.047)
Train: 27 [ 100/1251 (  8%)]  Loss: 4.334 (4.29)  Time: 0.817s, 1254.08/s  (0.816s, 1254.84/s)  LR: 9.803e-04  Data: 0.011 (0.029)
Train: 27 [ 150/1251 ( 12%)]  Loss: 4.230 (4.28)  Time: 0.796s, 1286.84/s  (0.807s, 1269.48/s)  LR: 9.803e-04  Data: 0.011 (0.023)
Train: 27 [ 200/1251 ( 16%)]  Loss: 4.410 (4.30)  Time: 0.776s, 1319.40/s  (0.802s, 1276.93/s)  LR: 9.803e-04  Data: 0.011 (0.020)
Train: 27 [ 250/1251 ( 20%)]  Loss: 4.356 (4.31)  Time: 0.787s, 1301.56/s  (0.799s, 1281.24/s)  LR: 9.803e-04  Data: 0.011 (0.018)
Train: 27 [ 300/1251 ( 24%)]  Loss: 4.522 (4.34)  Time: 0.777s, 1317.29/s  (0.797s, 1285.04/s)  LR: 9.803e-04  Data: 0.011 (0.017)
Train: 27 [ 350/1251 ( 28%)]  Loss: 4.177 (4.32)  Time: 0.781s, 1311.57/s  (0.797s, 1285.40/s)  LR: 9.803e-04  Data: 0.011 (0.016)
Train: 27 [ 400/1251 ( 32%)]  Loss: 3.991 (4.29)  Time: 0.774s, 1323.27/s  (0.796s, 1287.02/s)  LR: 9.803e-04  Data: 0.011 (0.015)
Train: 27 [ 450/1251 ( 36%)]  Loss: 4.268 (4.28)  Time: 0.833s, 1229.98/s  (0.794s, 1288.89/s)  LR: 9.803e-04  Data: 0.010 (0.015)
Train: 27 [ 500/1251 ( 40%)]  Loss: 4.370 (4.29)  Time: 0.778s, 1316.47/s  (0.794s, 1289.21/s)  LR: 9.803e-04  Data: 0.011 (0.014)
Train: 27 [ 550/1251 ( 44%)]  Loss: 4.591 (4.32)  Time: 0.778s, 1316.82/s  (0.793s, 1290.80/s)  LR: 9.803e-04  Data: 0.011 (0.014)
Train: 27 [ 600/1251 ( 48%)]  Loss: 4.509 (4.33)  Time: 0.781s, 1311.74/s  (0.794s, 1289.30/s)  LR: 9.803e-04  Data: 0.011 (0.014)
Train: 27 [ 650/1251 ( 52%)]  Loss: 4.374 (4.33)  Time: 0.823s, 1243.52/s  (0.794s, 1289.51/s)  LR: 9.803e-04  Data: 0.011 (0.014)
Train: 27 [ 700/1251 ( 56%)]  Loss: 4.274 (4.33)  Time: 0.774s, 1323.37/s  (0.794s, 1289.22/s)  LR: 9.803e-04  Data: 0.010 (0.013)
Train: 27 [ 750/1251 ( 60%)]  Loss: 4.215 (4.32)  Time: 0.779s, 1314.05/s  (0.794s, 1289.84/s)  LR: 9.803e-04  Data: 0.011 (0.013)
Train: 27 [ 800/1251 ( 64%)]  Loss: 3.938 (4.30)  Time: 0.790s, 1296.32/s  (0.794s, 1290.21/s)  LR: 9.803e-04  Data: 0.011 (0.013)
Train: 27 [ 850/1251 ( 68%)]  Loss: 4.188 (4.29)  Time: 0.779s, 1314.91/s  (0.793s, 1291.54/s)  LR: 9.803e-04  Data: 0.011 (0.013)
Train: 27 [ 900/1251 ( 72%)]  Loss: 4.407 (4.30)  Time: 0.778s, 1315.84/s  (0.792s, 1292.80/s)  LR: 9.803e-04  Data: 0.011 (0.013)
Train: 27 [ 950/1251 ( 76%)]  Loss: 4.328 (4.30)  Time: 0.830s, 1234.07/s  (0.793s, 1291.98/s)  LR: 9.803e-04  Data: 0.011 (0.013)
Train: 27 [1000/1251 ( 80%)]  Loss: 4.513 (4.31)  Time: 0.780s, 1312.49/s  (0.792s, 1292.70/s)  LR: 9.803e-04  Data: 0.010 (0.013)
Train: 27 [1050/1251 ( 84%)]  Loss: 3.992 (4.30)  Time: 0.818s, 1252.36/s  (0.792s, 1293.42/s)  LR: 9.803e-04  Data: 0.010 (0.013)
Train: 27 [1100/1251 ( 88%)]  Loss: 4.050 (4.29)  Time: 0.832s, 1230.08/s  (0.792s, 1293.50/s)  LR: 9.803e-04  Data: 0.010 (0.012)
Train: 27 [1150/1251 ( 92%)]  Loss: 4.010 (4.27)  Time: 0.780s, 1312.38/s  (0.792s, 1293.54/s)  LR: 9.803e-04  Data: 0.011 (0.012)
Train: 27 [1200/1251 ( 96%)]  Loss: 4.403 (4.28)  Time: 0.777s, 1317.26/s  (0.792s, 1293.16/s)  LR: 9.803e-04  Data: 0.010 (0.012)
Train: 27 [1250/1251 (100%)]  Loss: 4.277 (4.28)  Time: 0.769s, 1330.80/s  (0.792s, 1293.33/s)  LR: 9.803e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.492 (1.492)  Loss:  1.2398 (1.2398)  Acc@1: 82.4219 (82.4219)  Acc@5: 94.8242 (94.8242)
Test: [  48/48]  Time: 0.172 (0.560)  Loss:  1.3326 (1.8820)  Acc@1: 76.4151 (62.9520)  Acc@5: 90.6840 (85.3240)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-27.pth.tar', 62.95199996826172)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-25.pth.tar', 62.14800014648438)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-26.pth.tar', 61.92200005615234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-23.pth.tar', 61.422000122070315)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-22.pth.tar', 61.228000041503904)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-24.pth.tar', 61.038000122070315)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-21.pth.tar', 60.15)

Train: 28 [   0/1251 (  0%)]  Loss: 4.307 (4.31)  Time: 2.328s,  439.83/s  (2.328s,  439.83/s)  LR: 9.789e-04  Data: 1.591 (1.591)
Train: 28 [  50/1251 (  4%)]  Loss: 3.947 (4.13)  Time: 0.779s, 1314.45/s  (0.820s, 1249.13/s)  LR: 9.789e-04  Data: 0.011 (0.048)
Train: 28 [ 100/1251 (  8%)]  Loss: 4.342 (4.20)  Time: 0.782s, 1309.41/s  (0.803s, 1275.34/s)  LR: 9.789e-04  Data: 0.010 (0.030)
Train: 28 [ 150/1251 ( 12%)]  Loss: 4.015 (4.15)  Time: 0.780s, 1313.15/s  (0.802s, 1277.54/s)  LR: 9.789e-04  Data: 0.011 (0.023)
Train: 28 [ 200/1251 ( 16%)]  Loss: 3.975 (4.12)  Time: 0.789s, 1297.52/s  (0.798s, 1283.02/s)  LR: 9.789e-04  Data: 0.010 (0.020)
Train: 28 [ 250/1251 ( 20%)]  Loss: 3.738 (4.05)  Time: 0.790s, 1296.65/s  (0.797s, 1284.52/s)  LR: 9.789e-04  Data: 0.015 (0.018)
Train: 28 [ 300/1251 ( 24%)]  Loss: 4.430 (4.11)  Time: 0.780s, 1312.02/s  (0.796s, 1286.43/s)  LR: 9.789e-04  Data: 0.011 (0.017)
Train: 28 [ 350/1251 ( 28%)]  Loss: 4.328 (4.14)  Time: 0.782s, 1309.51/s  (0.794s, 1289.13/s)  LR: 9.789e-04  Data: 0.011 (0.016)
Train: 28 [ 400/1251 ( 32%)]  Loss: 4.283 (4.15)  Time: 0.812s, 1260.80/s  (0.793s, 1291.44/s)  LR: 9.789e-04  Data: 0.010 (0.016)
Train: 28 [ 450/1251 ( 36%)]  Loss: 3.879 (4.12)  Time: 0.777s, 1317.62/s  (0.793s, 1291.21/s)  LR: 9.789e-04  Data: 0.011 (0.015)
Train: 28 [ 500/1251 ( 40%)]  Loss: 4.458 (4.15)  Time: 0.777s, 1317.65/s  (0.793s, 1291.31/s)  LR: 9.789e-04  Data: 0.011 (0.015)
Train: 28 [ 550/1251 ( 44%)]  Loss: 4.705 (4.20)  Time: 0.785s, 1304.93/s  (0.793s, 1291.82/s)  LR: 9.789e-04  Data: 0.010 (0.014)
Train: 28 [ 600/1251 ( 48%)]  Loss: 4.401 (4.22)  Time: 0.781s, 1311.35/s  (0.793s, 1291.64/s)  LR: 9.789e-04  Data: 0.012 (0.014)
Train: 28 [ 650/1251 ( 52%)]  Loss: 3.949 (4.20)  Time: 0.779s, 1313.97/s  (0.792s, 1292.66/s)  LR: 9.789e-04  Data: 0.011 (0.014)
Train: 28 [ 700/1251 ( 56%)]  Loss: 4.219 (4.20)  Time: 0.786s, 1302.51/s  (0.792s, 1293.42/s)  LR: 9.789e-04  Data: 0.012 (0.014)
Train: 28 [ 750/1251 ( 60%)]  Loss: 3.940 (4.18)  Time: 0.789s, 1298.14/s  (0.792s, 1292.63/s)  LR: 9.789e-04  Data: 0.011 (0.014)
Train: 28 [ 800/1251 ( 64%)]  Loss: 4.239 (4.19)  Time: 0.782s, 1310.20/s  (0.792s, 1293.53/s)  LR: 9.789e-04  Data: 0.010 (0.013)
Train: 28 [ 850/1251 ( 68%)]  Loss: 4.068 (4.18)  Time: 0.777s, 1317.54/s  (0.792s, 1292.99/s)  LR: 9.789e-04  Data: 0.010 (0.013)
Train: 28 [ 900/1251 ( 72%)]  Loss: 4.051 (4.17)  Time: 0.834s, 1228.43/s  (0.792s, 1292.43/s)  LR: 9.789e-04  Data: 0.012 (0.013)
Train: 28 [ 950/1251 ( 76%)]  Loss: 4.516 (4.19)  Time: 0.779s, 1314.02/s  (0.793s, 1292.03/s)  LR: 9.789e-04  Data: 0.011 (0.013)
Train: 28 [1000/1251 ( 80%)]  Loss: 4.440 (4.20)  Time: 0.835s, 1225.87/s  (0.792s, 1292.63/s)  LR: 9.789e-04  Data: 0.011 (0.013)
Train: 28 [1050/1251 ( 84%)]  Loss: 4.249 (4.20)  Time: 0.776s, 1320.02/s  (0.793s, 1292.01/s)  LR: 9.789e-04  Data: 0.010 (0.013)
Train: 28 [1100/1251 ( 88%)]  Loss: 4.678 (4.22)  Time: 0.778s, 1316.63/s  (0.793s, 1291.32/s)  LR: 9.789e-04  Data: 0.011 (0.013)
Train: 28 [1150/1251 ( 92%)]  Loss: 4.169 (4.22)  Time: 0.784s, 1305.71/s  (0.793s, 1291.43/s)  LR: 9.789e-04  Data: 0.012 (0.013)
Train: 28 [1200/1251 ( 96%)]  Loss: 4.743 (4.24)  Time: 0.790s, 1296.52/s  (0.793s, 1291.92/s)  LR: 9.789e-04  Data: 0.011 (0.013)
Train: 28 [1250/1251 (100%)]  Loss: 4.488 (4.25)  Time: 0.774s, 1323.60/s  (0.793s, 1291.77/s)  LR: 9.789e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.559 (1.559)  Loss:  1.3624 (1.3624)  Acc@1: 79.5898 (79.5898)  Acc@5: 93.0664 (93.0664)
Test: [  48/48]  Time: 0.172 (0.556)  Loss:  1.2488 (1.8793)  Acc@1: 75.9434 (62.3960)  Acc@5: 91.1557 (85.1540)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-27.pth.tar', 62.95199996826172)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-28.pth.tar', 62.396000021972654)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-25.pth.tar', 62.14800014648438)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-26.pth.tar', 61.92200005615234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-23.pth.tar', 61.422000122070315)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-22.pth.tar', 61.228000041503904)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-24.pth.tar', 61.038000122070315)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-21.pth.tar', 60.15)

Train: 29 [   0/1251 (  0%)]  Loss: 4.425 (4.42)  Time: 2.400s,  426.73/s  (2.400s,  426.73/s)  LR: 9.773e-04  Data: 1.663 (1.663)
Train: 29 [  50/1251 (  4%)]  Loss: 4.097 (4.26)  Time: 0.803s, 1275.63/s  (0.822s, 1245.94/s)  LR: 9.773e-04  Data: 0.015 (0.048)
Train: 29 [ 100/1251 (  8%)]  Loss: 4.474 (4.33)  Time: 0.780s, 1313.04/s  (0.803s, 1274.49/s)  LR: 9.773e-04  Data: 0.010 (0.030)
Train: 29 [ 150/1251 ( 12%)]  Loss: 4.000 (4.25)  Time: 0.780s, 1312.66/s  (0.796s, 1285.65/s)  LR: 9.773e-04  Data: 0.012 (0.024)
Train: 29 [ 200/1251 ( 16%)]  Loss: 4.395 (4.28)  Time: 0.780s, 1313.11/s  (0.793s, 1290.84/s)  LR: 9.773e-04  Data: 0.011 (0.020)
Train: 29 [ 250/1251 ( 20%)]  Loss: 3.775 (4.19)  Time: 0.778s, 1315.78/s  (0.792s, 1293.69/s)  LR: 9.773e-04  Data: 0.010 (0.019)
Train: 29 [ 300/1251 ( 24%)]  Loss: 4.617 (4.25)  Time: 0.802s, 1276.54/s  (0.793s, 1291.09/s)  LR: 9.773e-04  Data: 0.011 (0.017)
Train: 29 [ 350/1251 ( 28%)]  Loss: 4.207 (4.25)  Time: 0.839s, 1220.34/s  (0.793s, 1291.74/s)  LR: 9.773e-04  Data: 0.010 (0.016)
Train: 29 [ 400/1251 ( 32%)]  Loss: 4.178 (4.24)  Time: 0.789s, 1298.15/s  (0.792s, 1292.34/s)  LR: 9.773e-04  Data: 0.011 (0.016)
Train: 29 [ 450/1251 ( 36%)]  Loss: 4.149 (4.23)  Time: 0.778s, 1315.75/s  (0.793s, 1290.77/s)  LR: 9.773e-04  Data: 0.012 (0.015)
Train: 29 [ 500/1251 ( 40%)]  Loss: 4.502 (4.26)  Time: 0.791s, 1294.35/s  (0.793s, 1291.81/s)  LR: 9.773e-04  Data: 0.010 (0.015)
Train: 29 [ 550/1251 ( 44%)]  Loss: 4.181 (4.25)  Time: 0.780s, 1313.47/s  (0.793s, 1291.11/s)  LR: 9.773e-04  Data: 0.010 (0.014)
Train: 29 [ 600/1251 ( 48%)]  Loss: 4.165 (4.24)  Time: 0.787s, 1300.62/s  (0.792s, 1292.41/s)  LR: 9.773e-04  Data: 0.010 (0.014)
Train: 29 [ 650/1251 ( 52%)]  Loss: 4.637 (4.27)  Time: 0.778s, 1315.69/s  (0.791s, 1293.86/s)  LR: 9.773e-04  Data: 0.010 (0.014)
Train: 29 [ 700/1251 ( 56%)]  Loss: 4.476 (4.29)  Time: 0.778s, 1315.43/s  (0.791s, 1294.24/s)  LR: 9.773e-04  Data: 0.011 (0.014)
Train: 29 [ 750/1251 ( 60%)]  Loss: 4.163 (4.28)  Time: 0.777s, 1317.79/s  (0.791s, 1294.44/s)  LR: 9.773e-04  Data: 0.010 (0.013)
Train: 29 [ 800/1251 ( 64%)]  Loss: 4.223 (4.27)  Time: 0.779s, 1315.29/s  (0.790s, 1295.65/s)  LR: 9.773e-04  Data: 0.011 (0.013)
Train: 29 [ 850/1251 ( 68%)]  Loss: 4.356 (4.28)  Time: 0.777s, 1317.38/s  (0.791s, 1295.30/s)  LR: 9.773e-04  Data: 0.011 (0.013)
Train: 29 [ 900/1251 ( 72%)]  Loss: 4.090 (4.27)  Time: 0.792s, 1292.88/s  (0.791s, 1294.18/s)  LR: 9.773e-04  Data: 0.011 (0.013)
Train: 29 [ 950/1251 ( 76%)]  Loss: 3.878 (4.25)  Time: 0.802s, 1276.43/s  (0.791s, 1294.91/s)  LR: 9.773e-04  Data: 0.010 (0.013)
Train: 29 [1000/1251 ( 80%)]  Loss: 4.116 (4.24)  Time: 0.796s, 1285.99/s  (0.791s, 1294.46/s)  LR: 9.773e-04  Data: 0.011 (0.013)
Train: 29 [1050/1251 ( 84%)]  Loss: 3.991 (4.23)  Time: 0.783s, 1307.87/s  (0.791s, 1294.54/s)  LR: 9.773e-04  Data: 0.010 (0.013)
Train: 29 [1100/1251 ( 88%)]  Loss: 4.192 (4.23)  Time: 0.786s, 1302.21/s  (0.791s, 1294.17/s)  LR: 9.773e-04  Data: 0.011 (0.013)
Train: 29 [1150/1251 ( 92%)]  Loss: 3.939 (4.22)  Time: 0.778s, 1316.94/s  (0.791s, 1294.33/s)  LR: 9.773e-04  Data: 0.011 (0.013)
Train: 29 [1200/1251 ( 96%)]  Loss: 4.553 (4.23)  Time: 0.782s, 1309.63/s  (0.791s, 1294.42/s)  LR: 9.773e-04  Data: 0.010 (0.012)
Train: 29 [1250/1251 (100%)]  Loss: 4.429 (4.24)  Time: 0.768s, 1333.19/s  (0.791s, 1295.01/s)  LR: 9.773e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.510 (1.510)  Loss:  1.1490 (1.1490)  Acc@1: 80.6641 (80.6641)  Acc@5: 95.3125 (95.3125)
Test: [  48/48]  Time: 0.172 (0.560)  Loss:  1.2065 (1.8371)  Acc@1: 80.6604 (63.2440)  Acc@5: 93.3962 (85.8480)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-29.pth.tar', 63.24400013183594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-27.pth.tar', 62.95199996826172)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-28.pth.tar', 62.396000021972654)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-25.pth.tar', 62.14800014648438)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-26.pth.tar', 61.92200005615234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-23.pth.tar', 61.422000122070315)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-22.pth.tar', 61.228000041503904)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-24.pth.tar', 61.038000122070315)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-21.pth.tar', 60.15)

Train: 30 [   0/1251 (  0%)]  Loss: 3.964 (3.96)  Time: 2.538s,  403.54/s  (2.538s,  403.54/s)  LR: 9.758e-04  Data: 1.788 (1.788)
Train: 30 [  50/1251 (  4%)]  Loss: 4.062 (4.01)  Time: 0.785s, 1304.10/s  (0.838s, 1221.57/s)  LR: 9.758e-04  Data: 0.011 (0.055)
Train: 30 [ 100/1251 (  8%)]  Loss: 3.586 (3.87)  Time: 0.783s, 1308.17/s  (0.816s, 1255.10/s)  LR: 9.758e-04  Data: 0.010 (0.033)
Train: 30 [ 150/1251 ( 12%)]  Loss: 4.270 (3.97)  Time: 0.780s, 1312.15/s  (0.805s, 1271.32/s)  LR: 9.758e-04  Data: 0.011 (0.026)
Train: 30 [ 200/1251 ( 16%)]  Loss: 4.146 (4.01)  Time: 0.828s, 1236.15/s  (0.803s, 1275.95/s)  LR: 9.758e-04  Data: 0.010 (0.022)
Train: 30 [ 250/1251 ( 20%)]  Loss: 4.134 (4.03)  Time: 0.790s, 1295.43/s  (0.802s, 1277.50/s)  LR: 9.758e-04  Data: 0.011 (0.020)
Train: 30 [ 300/1251 ( 24%)]  Loss: 4.190 (4.05)  Time: 0.781s, 1311.48/s  (0.798s, 1282.51/s)  LR: 9.758e-04  Data: 0.010 (0.018)
Train: 30 [ 350/1251 ( 28%)]  Loss: 4.013 (4.05)  Time: 0.779s, 1315.26/s  (0.796s, 1285.89/s)  LR: 9.758e-04  Data: 0.010 (0.017)
Train: 30 [ 400/1251 ( 32%)]  Loss: 4.108 (4.05)  Time: 0.778s, 1315.69/s  (0.797s, 1285.43/s)  LR: 9.758e-04  Data: 0.010 (0.016)
Train: 30 [ 450/1251 ( 36%)]  Loss: 4.502 (4.10)  Time: 0.776s, 1319.90/s  (0.797s, 1284.45/s)  LR: 9.758e-04  Data: 0.011 (0.016)
Train: 30 [ 500/1251 ( 40%)]  Loss: 4.227 (4.11)  Time: 0.781s, 1311.79/s  (0.796s, 1285.69/s)  LR: 9.758e-04  Data: 0.011 (0.015)
Train: 30 [ 550/1251 ( 44%)]  Loss: 4.439 (4.14)  Time: 0.805s, 1272.03/s  (0.796s, 1286.04/s)  LR: 9.758e-04  Data: 0.010 (0.015)
Train: 30 [ 600/1251 ( 48%)]  Loss: 3.983 (4.12)  Time: 0.783s, 1307.07/s  (0.795s, 1287.50/s)  LR: 9.758e-04  Data: 0.011 (0.015)
Train: 30 [ 650/1251 ( 52%)]  Loss: 3.940 (4.11)  Time: 0.780s, 1312.04/s  (0.795s, 1288.31/s)  LR: 9.758e-04  Data: 0.010 (0.014)
Train: 30 [ 700/1251 ( 56%)]  Loss: 4.304 (4.12)  Time: 0.809s, 1265.13/s  (0.794s, 1289.11/s)  LR: 9.758e-04  Data: 0.011 (0.014)
Train: 30 [ 750/1251 ( 60%)]  Loss: 4.408 (4.14)  Time: 0.840s, 1219.41/s  (0.794s, 1289.03/s)  LR: 9.758e-04  Data: 0.011 (0.014)
Train: 30 [ 800/1251 ( 64%)]  Loss: 4.128 (4.14)  Time: 0.837s, 1223.90/s  (0.795s, 1288.58/s)  LR: 9.758e-04  Data: 0.010 (0.014)
Train: 30 [ 850/1251 ( 68%)]  Loss: 3.969 (4.13)  Time: 0.812s, 1260.32/s  (0.796s, 1285.87/s)  LR: 9.758e-04  Data: 0.011 (0.013)
Train: 30 [ 900/1251 ( 72%)]  Loss: 4.331 (4.14)  Time: 0.778s, 1315.84/s  (0.796s, 1286.27/s)  LR: 9.758e-04  Data: 0.011 (0.013)
Train: 30 [ 950/1251 ( 76%)]  Loss: 4.547 (4.16)  Time: 0.778s, 1316.71/s  (0.795s, 1287.39/s)  LR: 9.758e-04  Data: 0.011 (0.013)
Train: 30 [1000/1251 ( 80%)]  Loss: 4.245 (4.17)  Time: 0.782s, 1308.66/s  (0.795s, 1288.41/s)  LR: 9.758e-04  Data: 0.010 (0.013)
Train: 30 [1050/1251 ( 84%)]  Loss: 4.136 (4.17)  Time: 0.795s, 1288.31/s  (0.794s, 1289.11/s)  LR: 9.758e-04  Data: 0.018 (0.013)
Train: 30 [1100/1251 ( 88%)]  Loss: 4.194 (4.17)  Time: 0.850s, 1205.36/s  (0.794s, 1289.53/s)  LR: 9.758e-04  Data: 0.010 (0.013)
Train: 30 [1150/1251 ( 92%)]  Loss: 4.433 (4.18)  Time: 0.790s, 1295.41/s  (0.794s, 1288.93/s)  LR: 9.758e-04  Data: 0.010 (0.013)
Train: 30 [1200/1251 ( 96%)]  Loss: 4.063 (4.17)  Time: 0.779s, 1314.31/s  (0.794s, 1289.18/s)  LR: 9.758e-04  Data: 0.011 (0.013)
Train: 30 [1250/1251 (100%)]  Loss: 4.038 (4.17)  Time: 0.770s, 1330.19/s  (0.794s, 1289.65/s)  LR: 9.758e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.511 (1.511)  Loss:  1.2241 (1.2241)  Acc@1: 80.3711 (80.3711)  Acc@5: 93.8477 (93.8477)
Test: [  48/48]  Time: 0.172 (0.564)  Loss:  1.1300 (1.8907)  Acc@1: 80.1887 (62.8620)  Acc@5: 93.0424 (85.2820)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-29.pth.tar', 63.24400013183594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-27.pth.tar', 62.95199996826172)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-30.pth.tar', 62.86199992675781)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-28.pth.tar', 62.396000021972654)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-25.pth.tar', 62.14800014648438)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-26.pth.tar', 61.92200005615234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-23.pth.tar', 61.422000122070315)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-22.pth.tar', 61.228000041503904)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-24.pth.tar', 61.038000122070315)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-21.pth.tar', 60.15)

Train: 31 [   0/1251 (  0%)]  Loss: 4.333 (4.33)  Time: 2.217s,  461.87/s  (2.217s,  461.87/s)  LR: 9.741e-04  Data: 1.481 (1.481)
Train: 31 [  50/1251 (  4%)]  Loss: 4.095 (4.21)  Time: 0.789s, 1297.09/s  (0.849s, 1205.66/s)  LR: 9.741e-04  Data: 0.013 (0.049)
Train: 31 [ 100/1251 (  8%)]  Loss: 4.158 (4.20)  Time: 0.780s, 1312.15/s  (0.817s, 1253.20/s)  LR: 9.741e-04  Data: 0.010 (0.030)
Train: 31 [ 150/1251 ( 12%)]  Loss: 4.161 (4.19)  Time: 0.787s, 1301.76/s  (0.811s, 1262.79/s)  LR: 9.741e-04  Data: 0.010 (0.024)
Train: 31 [ 200/1251 ( 16%)]  Loss: 4.271 (4.20)  Time: 0.776s, 1318.97/s  (0.805s, 1271.35/s)  LR: 9.741e-04  Data: 0.011 (0.021)
Train: 31 [ 250/1251 ( 20%)]  Loss: 4.164 (4.20)  Time: 0.784s, 1305.91/s  (0.805s, 1272.39/s)  LR: 9.741e-04  Data: 0.011 (0.019)
Train: 31 [ 300/1251 ( 24%)]  Loss: 3.891 (4.15)  Time: 0.785s, 1304.90/s  (0.802s, 1277.25/s)  LR: 9.741e-04  Data: 0.012 (0.017)
Train: 31 [ 350/1251 ( 28%)]  Loss: 4.314 (4.17)  Time: 0.831s, 1232.56/s  (0.800s, 1279.84/s)  LR: 9.741e-04  Data: 0.011 (0.016)
Train: 31 [ 400/1251 ( 32%)]  Loss: 4.183 (4.17)  Time: 0.783s, 1307.19/s  (0.799s, 1282.36/s)  LR: 9.741e-04  Data: 0.011 (0.016)
Train: 31 [ 450/1251 ( 36%)]  Loss: 4.232 (4.18)  Time: 0.803s, 1275.35/s  (0.799s, 1281.61/s)  LR: 9.741e-04  Data: 0.011 (0.015)
Train: 31 [ 500/1251 ( 40%)]  Loss: 4.407 (4.20)  Time: 0.824s, 1243.30/s  (0.799s, 1282.19/s)  LR: 9.741e-04  Data: 0.011 (0.015)
Train: 31 [ 550/1251 ( 44%)]  Loss: 4.528 (4.23)  Time: 0.777s, 1318.10/s  (0.798s, 1282.43/s)  LR: 9.741e-04  Data: 0.010 (0.014)
Train: 31 [ 600/1251 ( 48%)]  Loss: 4.448 (4.24)  Time: 0.778s, 1316.13/s  (0.798s, 1283.34/s)  LR: 9.741e-04  Data: 0.011 (0.014)
Train: 31 [ 650/1251 ( 52%)]  Loss: 4.252 (4.25)  Time: 0.776s, 1319.35/s  (0.798s, 1283.10/s)  LR: 9.741e-04  Data: 0.011 (0.014)
Train: 31 [ 700/1251 ( 56%)]  Loss: 4.013 (4.23)  Time: 0.779s, 1314.94/s  (0.798s, 1283.48/s)  LR: 9.741e-04  Data: 0.010 (0.014)
Train: 31 [ 750/1251 ( 60%)]  Loss: 4.044 (4.22)  Time: 0.778s, 1316.25/s  (0.798s, 1283.69/s)  LR: 9.741e-04  Data: 0.011 (0.013)
Train: 31 [ 800/1251 ( 64%)]  Loss: 4.258 (4.22)  Time: 0.802s, 1277.25/s  (0.799s, 1282.31/s)  LR: 9.741e-04  Data: 0.011 (0.013)
Train: 31 [ 850/1251 ( 68%)]  Loss: 3.969 (4.21)  Time: 0.791s, 1294.42/s  (0.798s, 1282.54/s)  LR: 9.741e-04  Data: 0.010 (0.013)
Train: 31 [ 900/1251 ( 72%)]  Loss: 4.198 (4.21)  Time: 0.777s, 1317.43/s  (0.798s, 1283.99/s)  LR: 9.741e-04  Data: 0.011 (0.013)
Train: 31 [ 950/1251 ( 76%)]  Loss: 3.986 (4.20)  Time: 0.813s, 1260.13/s  (0.798s, 1283.36/s)  LR: 9.741e-04  Data: 0.010 (0.013)
Train: 31 [1000/1251 ( 80%)]  Loss: 4.356 (4.20)  Time: 0.819s, 1250.61/s  (0.798s, 1283.31/s)  LR: 9.741e-04  Data: 0.011 (0.013)
Train: 31 [1050/1251 ( 84%)]  Loss: 4.076 (4.20)  Time: 0.802s, 1276.97/s  (0.798s, 1283.10/s)  LR: 9.741e-04  Data: 0.011 (0.013)
Train: 31 [1100/1251 ( 88%)]  Loss: 4.192 (4.20)  Time: 0.786s, 1302.64/s  (0.798s, 1283.80/s)  LR: 9.741e-04  Data: 0.011 (0.013)
Train: 31 [1150/1251 ( 92%)]  Loss: 4.333 (4.20)  Time: 0.779s, 1313.71/s  (0.797s, 1284.90/s)  LR: 9.741e-04  Data: 0.011 (0.012)
Train: 31 [1200/1251 ( 96%)]  Loss: 4.038 (4.20)  Time: 0.786s, 1301.99/s  (0.796s, 1285.98/s)  LR: 9.741e-04  Data: 0.011 (0.012)
Train: 31 [1250/1251 (100%)]  Loss: 4.175 (4.20)  Time: 0.801s, 1277.80/s  (0.796s, 1286.39/s)  LR: 9.741e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.554 (1.554)  Loss:  1.1429 (1.1429)  Acc@1: 82.4219 (82.4219)  Acc@5: 95.2148 (95.2148)
Test: [  48/48]  Time: 0.172 (0.565)  Loss:  1.2007 (1.8807)  Acc@1: 79.3632 (62.4620)  Acc@5: 91.8632 (84.9240)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-29.pth.tar', 63.24400013183594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-27.pth.tar', 62.95199996826172)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-30.pth.tar', 62.86199992675781)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-31.pth.tar', 62.46200008544922)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-28.pth.tar', 62.396000021972654)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-25.pth.tar', 62.14800014648438)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-26.pth.tar', 61.92200005615234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-23.pth.tar', 61.422000122070315)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-22.pth.tar', 61.228000041503904)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-24.pth.tar', 61.038000122070315)

Train: 32 [   0/1251 (  0%)]  Loss: 3.879 (3.88)  Time: 2.316s,  442.19/s  (2.316s,  442.19/s)  LR: 9.725e-04  Data: 1.523 (1.523)
Train: 32 [  50/1251 (  4%)]  Loss: 4.251 (4.07)  Time: 0.779s, 1314.87/s  (0.840s, 1218.42/s)  LR: 9.725e-04  Data: 0.012 (0.046)
Train: 32 [ 100/1251 (  8%)]  Loss: 4.178 (4.10)  Time: 0.839s, 1220.09/s  (0.816s, 1255.36/s)  LR: 9.725e-04  Data: 0.010 (0.029)
Train: 32 [ 150/1251 ( 12%)]  Loss: 3.979 (4.07)  Time: 0.791s, 1294.73/s  (0.806s, 1270.47/s)  LR: 9.725e-04  Data: 0.011 (0.023)
Train: 32 [ 200/1251 ( 16%)]  Loss: 4.165 (4.09)  Time: 0.778s, 1316.16/s  (0.801s, 1278.70/s)  LR: 9.725e-04  Data: 0.011 (0.020)
Train: 32 [ 250/1251 ( 20%)]  Loss: 3.926 (4.06)  Time: 0.778s, 1316.21/s  (0.799s, 1281.90/s)  LR: 9.725e-04  Data: 0.010 (0.018)
Train: 32 [ 300/1251 ( 24%)]  Loss: 3.903 (4.04)  Time: 0.782s, 1309.43/s  (0.796s, 1286.63/s)  LR: 9.725e-04  Data: 0.011 (0.017)
Train: 32 [ 350/1251 ( 28%)]  Loss: 4.244 (4.07)  Time: 0.814s, 1257.57/s  (0.794s, 1289.44/s)  LR: 9.725e-04  Data: 0.011 (0.016)
Train: 32 [ 400/1251 ( 32%)]  Loss: 4.249 (4.09)  Time: 0.842s, 1215.72/s  (0.798s, 1283.18/s)  LR: 9.725e-04  Data: 0.011 (0.015)
Train: 32 [ 450/1251 ( 36%)]  Loss: 3.937 (4.07)  Time: 0.776s, 1319.53/s  (0.800s, 1279.31/s)  LR: 9.725e-04  Data: 0.010 (0.015)
Train: 32 [ 500/1251 ( 40%)]  Loss: 4.358 (4.10)  Time: 0.784s, 1306.79/s  (0.800s, 1280.55/s)  LR: 9.725e-04  Data: 0.010 (0.015)
Train: 32 [ 550/1251 ( 44%)]  Loss: 3.962 (4.09)  Time: 0.846s, 1210.77/s  (0.802s, 1276.38/s)  LR: 9.725e-04  Data: 0.011 (0.014)
Train: 32 [ 600/1251 ( 48%)]  Loss: 4.276 (4.10)  Time: 0.846s, 1210.61/s  (0.805s, 1272.74/s)  LR: 9.725e-04  Data: 0.012 (0.014)
Train: 32 [ 650/1251 ( 52%)]  Loss: 3.911 (4.09)  Time: 0.779s, 1313.72/s  (0.805s, 1271.71/s)  LR: 9.725e-04  Data: 0.010 (0.014)
Train: 32 [ 700/1251 ( 56%)]  Loss: 4.114 (4.09)  Time: 0.817s, 1252.61/s  (0.804s, 1274.39/s)  LR: 9.725e-04  Data: 0.011 (0.014)
Train: 32 [ 750/1251 ( 60%)]  Loss: 4.517 (4.12)  Time: 0.778s, 1315.63/s  (0.802s, 1276.12/s)  LR: 9.725e-04  Data: 0.010 (0.013)
Train: 32 [ 800/1251 ( 64%)]  Loss: 3.979 (4.11)  Time: 0.776s, 1319.47/s  (0.801s, 1277.76/s)  LR: 9.725e-04  Data: 0.011 (0.013)
Train: 32 [ 850/1251 ( 68%)]  Loss: 4.124 (4.11)  Time: 0.783s, 1307.12/s  (0.801s, 1278.43/s)  LR: 9.725e-04  Data: 0.011 (0.013)
Train: 32 [ 900/1251 ( 72%)]  Loss: 3.821 (4.09)  Time: 0.780s, 1313.52/s  (0.800s, 1279.71/s)  LR: 9.725e-04  Data: 0.011 (0.013)
Train: 32 [ 950/1251 ( 76%)]  Loss: 4.466 (4.11)  Time: 0.778s, 1315.82/s  (0.800s, 1280.70/s)  LR: 9.725e-04  Data: 0.012 (0.013)
Train: 32 [1000/1251 ( 80%)]  Loss: 4.346 (4.12)  Time: 0.777s, 1318.48/s  (0.799s, 1281.97/s)  LR: 9.725e-04  Data: 0.010 (0.013)
Train: 32 [1050/1251 ( 84%)]  Loss: 4.466 (4.14)  Time: 0.820s, 1249.29/s  (0.799s, 1281.27/s)  LR: 9.725e-04  Data: 0.010 (0.013)
Train: 32 [1100/1251 ( 88%)]  Loss: 4.151 (4.14)  Time: 0.781s, 1311.85/s  (0.799s, 1281.16/s)  LR: 9.725e-04  Data: 0.011 (0.013)
Train: 32 [1150/1251 ( 92%)]  Loss: 4.261 (4.14)  Time: 0.779s, 1313.78/s  (0.799s, 1282.15/s)  LR: 9.725e-04  Data: 0.010 (0.012)
Train: 32 [1200/1251 ( 96%)]  Loss: 3.900 (4.13)  Time: 0.820s, 1248.06/s  (0.799s, 1281.92/s)  LR: 9.725e-04  Data: 0.014 (0.012)
Train: 32 [1250/1251 (100%)]  Loss: 4.121 (4.13)  Time: 0.800s, 1280.27/s  (0.799s, 1281.65/s)  LR: 9.725e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.567 (1.567)  Loss:  1.1480 (1.1480)  Acc@1: 83.0078 (83.0078)  Acc@5: 95.3125 (95.3125)
Test: [  48/48]  Time: 0.172 (0.562)  Loss:  1.1976 (1.9558)  Acc@1: 77.8302 (62.3860)  Acc@5: 93.8679 (85.2000)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-29.pth.tar', 63.24400013183594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-27.pth.tar', 62.95199996826172)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-30.pth.tar', 62.86199992675781)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-31.pth.tar', 62.46200008544922)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-28.pth.tar', 62.396000021972654)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-32.pth.tar', 62.38600006591797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-25.pth.tar', 62.14800014648438)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-26.pth.tar', 61.92200005615234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-23.pth.tar', 61.422000122070315)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-22.pth.tar', 61.228000041503904)

Train: 33 [   0/1251 (  0%)]  Loss: 4.221 (4.22)  Time: 2.332s,  439.06/s  (2.332s,  439.06/s)  LR: 9.707e-04  Data: 1.597 (1.597)
Train: 33 [  50/1251 (  4%)]  Loss: 3.718 (3.97)  Time: 0.813s, 1259.43/s  (0.819s, 1250.76/s)  LR: 9.707e-04  Data: 0.010 (0.045)
Train: 33 [ 100/1251 (  8%)]  Loss: 4.083 (4.01)  Time: 0.776s, 1319.44/s  (0.811s, 1261.95/s)  LR: 9.707e-04  Data: 0.012 (0.028)
Train: 33 [ 150/1251 ( 12%)]  Loss: 4.307 (4.08)  Time: 0.807s, 1268.72/s  (0.806s, 1270.44/s)  LR: 9.707e-04  Data: 0.010 (0.022)
Train: 33 [ 200/1251 ( 16%)]  Loss: 3.994 (4.06)  Time: 0.780s, 1313.31/s  (0.806s, 1270.40/s)  LR: 9.707e-04  Data: 0.010 (0.019)
Train: 33 [ 250/1251 ( 20%)]  Loss: 4.138 (4.08)  Time: 0.777s, 1317.13/s  (0.803s, 1275.41/s)  LR: 9.707e-04  Data: 0.011 (0.018)
Train: 33 [ 300/1251 ( 24%)]  Loss: 4.391 (4.12)  Time: 0.778s, 1317.00/s  (0.802s, 1276.25/s)  LR: 9.707e-04  Data: 0.011 (0.017)
Train: 33 [ 350/1251 ( 28%)]  Loss: 4.262 (4.14)  Time: 0.785s, 1304.03/s  (0.800s, 1280.53/s)  LR: 9.707e-04  Data: 0.011 (0.016)
Train: 33 [ 400/1251 ( 32%)]  Loss: 4.198 (4.15)  Time: 0.779s, 1314.34/s  (0.798s, 1283.94/s)  LR: 9.707e-04  Data: 0.010 (0.015)
Train: 33 [ 450/1251 ( 36%)]  Loss: 4.034 (4.13)  Time: 0.781s, 1311.45/s  (0.797s, 1284.59/s)  LR: 9.707e-04  Data: 0.010 (0.015)
Train: 33 [ 500/1251 ( 40%)]  Loss: 4.052 (4.13)  Time: 0.817s, 1252.64/s  (0.797s, 1284.39/s)  LR: 9.707e-04  Data: 0.011 (0.014)
Train: 33 [ 550/1251 ( 44%)]  Loss: 4.145 (4.13)  Time: 0.818s, 1252.22/s  (0.800s, 1280.38/s)  LR: 9.707e-04  Data: 0.011 (0.014)
Train: 33 [ 600/1251 ( 48%)]  Loss: 4.299 (4.14)  Time: 0.780s, 1312.18/s  (0.799s, 1281.81/s)  LR: 9.707e-04  Data: 0.010 (0.014)
Train: 33 [ 650/1251 ( 52%)]  Loss: 4.180 (4.14)  Time: 0.788s, 1298.85/s  (0.798s, 1283.69/s)  LR: 9.707e-04  Data: 0.011 (0.014)
Train: 33 [ 700/1251 ( 56%)]  Loss: 4.050 (4.14)  Time: 0.776s, 1319.40/s  (0.798s, 1283.96/s)  LR: 9.707e-04  Data: 0.011 (0.013)
Train: 33 [ 750/1251 ( 60%)]  Loss: 4.165 (4.14)  Time: 0.775s, 1320.45/s  (0.798s, 1282.70/s)  LR: 9.707e-04  Data: 0.011 (0.013)
Train: 33 [ 800/1251 ( 64%)]  Loss: 4.396 (4.15)  Time: 0.778s, 1315.69/s  (0.797s, 1284.28/s)  LR: 9.707e-04  Data: 0.010 (0.013)
Train: 33 [ 850/1251 ( 68%)]  Loss: 4.039 (4.15)  Time: 0.780s, 1313.32/s  (0.797s, 1284.74/s)  LR: 9.707e-04  Data: 0.011 (0.013)
Train: 33 [ 900/1251 ( 72%)]  Loss: 3.951 (4.14)  Time: 0.816s, 1254.48/s  (0.797s, 1284.38/s)  LR: 9.707e-04  Data: 0.013 (0.013)
Train: 33 [ 950/1251 ( 76%)]  Loss: 4.152 (4.14)  Time: 0.833s, 1228.81/s  (0.798s, 1283.54/s)  LR: 9.707e-04  Data: 0.015 (0.013)
Train: 33 [1000/1251 ( 80%)]  Loss: 4.221 (4.14)  Time: 0.779s, 1314.64/s  (0.798s, 1283.42/s)  LR: 9.707e-04  Data: 0.011 (0.013)
Train: 33 [1050/1251 ( 84%)]  Loss: 4.049 (4.14)  Time: 0.799s, 1282.18/s  (0.797s, 1284.61/s)  LR: 9.707e-04  Data: 0.010 (0.012)
Train: 33 [1100/1251 ( 88%)]  Loss: 3.787 (4.12)  Time: 0.786s, 1303.45/s  (0.798s, 1283.94/s)  LR: 9.707e-04  Data: 0.011 (0.012)
Train: 33 [1150/1251 ( 92%)]  Loss: 4.385 (4.13)  Time: 0.808s, 1267.09/s  (0.797s, 1284.15/s)  LR: 9.707e-04  Data: 0.011 (0.012)
Train: 33 [1200/1251 ( 96%)]  Loss: 4.264 (4.14)  Time: 0.829s, 1235.42/s  (0.797s, 1284.52/s)  LR: 9.707e-04  Data: 0.010 (0.012)
Train: 33 [1250/1251 (100%)]  Loss: 3.885 (4.13)  Time: 0.769s, 1330.86/s  (0.797s, 1285.02/s)  LR: 9.707e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.507 (1.507)  Loss:  1.3085 (1.3085)  Acc@1: 80.1758 (80.1758)  Acc@5: 93.5547 (93.5547)
Test: [  48/48]  Time: 0.172 (0.566)  Loss:  1.1338 (1.9130)  Acc@1: 80.6604 (63.7420)  Acc@5: 93.5142 (86.0800)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-33.pth.tar', 63.74200000244141)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-29.pth.tar', 63.24400013183594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-27.pth.tar', 62.95199996826172)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-30.pth.tar', 62.86199992675781)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-31.pth.tar', 62.46200008544922)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-28.pth.tar', 62.396000021972654)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-32.pth.tar', 62.38600006591797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-25.pth.tar', 62.14800014648438)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-26.pth.tar', 61.92200005615234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-23.pth.tar', 61.422000122070315)

Train: 34 [   0/1251 (  0%)]  Loss: 4.248 (4.25)  Time: 2.299s,  445.37/s  (2.299s,  445.37/s)  LR: 9.690e-04  Data: 1.550 (1.550)
Train: 34 [  50/1251 (  4%)]  Loss: 4.349 (4.30)  Time: 0.820s, 1248.74/s  (0.828s, 1235.98/s)  LR: 9.690e-04  Data: 0.011 (0.046)
Train: 34 [ 100/1251 (  8%)]  Loss: 3.906 (4.17)  Time: 0.779s, 1313.85/s  (0.809s, 1266.20/s)  LR: 9.690e-04  Data: 0.011 (0.029)
Train: 34 [ 150/1251 ( 12%)]  Loss: 4.037 (4.13)  Time: 0.785s, 1304.17/s  (0.802s, 1277.34/s)  LR: 9.690e-04  Data: 0.011 (0.023)
Train: 34 [ 200/1251 ( 16%)]  Loss: 4.217 (4.15)  Time: 0.791s, 1295.03/s  (0.799s, 1281.91/s)  LR: 9.690e-04  Data: 0.010 (0.020)
Train: 34 [ 250/1251 ( 20%)]  Loss: 4.218 (4.16)  Time: 0.781s, 1311.17/s  (0.796s, 1285.81/s)  LR: 9.690e-04  Data: 0.010 (0.018)
Train: 34 [ 300/1251 ( 24%)]  Loss: 4.033 (4.14)  Time: 0.777s, 1317.42/s  (0.794s, 1289.75/s)  LR: 9.690e-04  Data: 0.011 (0.017)
Train: 34 [ 350/1251 ( 28%)]  Loss: 4.470 (4.18)  Time: 0.780s, 1313.21/s  (0.793s, 1291.32/s)  LR: 9.690e-04  Data: 0.011 (0.016)
Train: 34 [ 400/1251 ( 32%)]  Loss: 4.177 (4.18)  Time: 0.780s, 1312.38/s  (0.792s, 1292.14/s)  LR: 9.690e-04  Data: 0.010 (0.015)
Train: 34 [ 450/1251 ( 36%)]  Loss: 4.398 (4.21)  Time: 0.780s, 1312.98/s  (0.792s, 1292.96/s)  LR: 9.690e-04  Data: 0.011 (0.015)
Train: 34 [ 500/1251 ( 40%)]  Loss: 4.182 (4.20)  Time: 0.777s, 1317.56/s  (0.791s, 1294.63/s)  LR: 9.690e-04  Data: 0.011 (0.014)
Train: 34 [ 550/1251 ( 44%)]  Loss: 4.266 (4.21)  Time: 0.784s, 1305.64/s  (0.790s, 1295.76/s)  LR: 9.690e-04  Data: 0.011 (0.014)
Train: 34 [ 600/1251 ( 48%)]  Loss: 3.958 (4.19)  Time: 0.813s, 1259.69/s  (0.790s, 1295.93/s)  LR: 9.690e-04  Data: 0.011 (0.014)
Train: 34 [ 650/1251 ( 52%)]  Loss: 4.188 (4.19)  Time: 0.781s, 1310.32/s  (0.791s, 1295.38/s)  LR: 9.690e-04  Data: 0.012 (0.014)
Train: 34 [ 700/1251 ( 56%)]  Loss: 4.325 (4.20)  Time: 0.778s, 1315.46/s  (0.790s, 1296.02/s)  LR: 9.690e-04  Data: 0.011 (0.013)
Train: 34 [ 750/1251 ( 60%)]  Loss: 4.501 (4.22)  Time: 0.779s, 1315.35/s  (0.790s, 1296.17/s)  LR: 9.690e-04  Data: 0.011 (0.013)
Train: 34 [ 800/1251 ( 64%)]  Loss: 3.682 (4.19)  Time: 0.805s, 1272.08/s  (0.790s, 1296.73/s)  LR: 9.690e-04  Data: 0.010 (0.013)
Train: 34 [ 850/1251 ( 68%)]  Loss: 4.465 (4.20)  Time: 0.792s, 1293.01/s  (0.791s, 1295.16/s)  LR: 9.690e-04  Data: 0.011 (0.013)
Train: 34 [ 900/1251 ( 72%)]  Loss: 4.304 (4.21)  Time: 0.788s, 1299.37/s  (0.792s, 1292.45/s)  LR: 9.690e-04  Data: 0.010 (0.013)
Train: 34 [ 950/1251 ( 76%)]  Loss: 4.140 (4.20)  Time: 0.778s, 1316.72/s  (0.792s, 1292.79/s)  LR: 9.690e-04  Data: 0.011 (0.013)
Train: 34 [1000/1251 ( 80%)]  Loss: 4.116 (4.20)  Time: 0.779s, 1314.35/s  (0.792s, 1293.49/s)  LR: 9.690e-04  Data: 0.011 (0.013)
Train: 34 [1050/1251 ( 84%)]  Loss: 4.083 (4.19)  Time: 0.780s, 1313.41/s  (0.791s, 1294.03/s)  LR: 9.690e-04  Data: 0.011 (0.013)
Train: 34 [1100/1251 ( 88%)]  Loss: 4.195 (4.19)  Time: 0.781s, 1310.44/s  (0.792s, 1293.49/s)  LR: 9.690e-04  Data: 0.010 (0.013)
Train: 34 [1150/1251 ( 92%)]  Loss: 4.338 (4.20)  Time: 0.777s, 1317.28/s  (0.793s, 1291.88/s)  LR: 9.690e-04  Data: 0.010 (0.012)
Train: 34 [1200/1251 ( 96%)]  Loss: 3.759 (4.18)  Time: 0.778s, 1316.60/s  (0.793s, 1291.85/s)  LR: 9.690e-04  Data: 0.010 (0.012)
Train: 34 [1250/1251 (100%)]  Loss: 3.678 (4.16)  Time: 0.801s, 1278.54/s  (0.793s, 1291.53/s)  LR: 9.690e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.501 (1.501)  Loss:  1.1533 (1.1533)  Acc@1: 81.0547 (81.0547)  Acc@5: 93.6523 (93.6523)
Test: [  48/48]  Time: 0.172 (0.565)  Loss:  1.2567 (1.8163)  Acc@1: 78.7736 (63.7100)  Acc@5: 92.6887 (85.8920)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-33.pth.tar', 63.74200000244141)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-34.pth.tar', 63.71000008789063)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-29.pth.tar', 63.24400013183594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-27.pth.tar', 62.95199996826172)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-30.pth.tar', 62.86199992675781)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-31.pth.tar', 62.46200008544922)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-28.pth.tar', 62.396000021972654)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-32.pth.tar', 62.38600006591797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-25.pth.tar', 62.14800014648438)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-26.pth.tar', 61.92200005615234)

Train: 35 [   0/1251 (  0%)]  Loss: 4.148 (4.15)  Time: 2.311s,  443.00/s  (2.311s,  443.00/s)  LR: 9.671e-04  Data: 1.574 (1.574)
Train: 35 [  50/1251 (  4%)]  Loss: 4.202 (4.18)  Time: 0.830s, 1234.03/s  (0.859s, 1192.34/s)  LR: 9.671e-04  Data: 0.011 (0.046)
Train: 35 [ 100/1251 (  8%)]  Loss: 4.190 (4.18)  Time: 0.822s, 1245.57/s  (0.843s, 1214.08/s)  LR: 9.671e-04  Data: 0.011 (0.029)
Train: 35 [ 150/1251 ( 12%)]  Loss: 4.082 (4.16)  Time: 0.780s, 1312.33/s  (0.825s, 1241.31/s)  LR: 9.671e-04  Data: 0.012 (0.023)
Train: 35 [ 200/1251 ( 16%)]  Loss: 4.241 (4.17)  Time: 0.774s, 1323.36/s  (0.815s, 1256.31/s)  LR: 9.671e-04  Data: 0.011 (0.020)
Train: 35 [ 250/1251 ( 20%)]  Loss: 4.193 (4.18)  Time: 0.778s, 1316.62/s  (0.809s, 1265.80/s)  LR: 9.671e-04  Data: 0.011 (0.019)
Train: 35 [ 300/1251 ( 24%)]  Loss: 4.069 (4.16)  Time: 0.781s, 1310.40/s  (0.805s, 1272.49/s)  LR: 9.671e-04  Data: 0.010 (0.017)
Train: 35 [ 350/1251 ( 28%)]  Loss: 4.497 (4.20)  Time: 0.779s, 1314.23/s  (0.801s, 1278.18/s)  LR: 9.671e-04  Data: 0.011 (0.016)
Train: 35 [ 400/1251 ( 32%)]  Loss: 4.351 (4.22)  Time: 0.777s, 1317.30/s  (0.799s, 1281.22/s)  LR: 9.671e-04  Data: 0.011 (0.016)
Train: 35 [ 450/1251 ( 36%)]  Loss: 3.808 (4.18)  Time: 0.777s, 1317.59/s  (0.798s, 1282.56/s)  LR: 9.671e-04  Data: 0.010 (0.015)
Train: 35 [ 500/1251 ( 40%)]  Loss: 4.316 (4.19)  Time: 0.834s, 1227.95/s  (0.797s, 1285.29/s)  LR: 9.671e-04  Data: 0.011 (0.015)
Train: 35 [ 550/1251 ( 44%)]  Loss: 4.321 (4.20)  Time: 0.778s, 1315.73/s  (0.796s, 1286.18/s)  LR: 9.671e-04  Data: 0.010 (0.014)
Train: 35 [ 600/1251 ( 48%)]  Loss: 4.249 (4.21)  Time: 0.779s, 1315.05/s  (0.795s, 1287.69/s)  LR: 9.671e-04  Data: 0.010 (0.014)
Train: 35 [ 650/1251 ( 52%)]  Loss: 4.210 (4.21)  Time: 0.783s, 1308.62/s  (0.794s, 1289.45/s)  LR: 9.671e-04  Data: 0.010 (0.014)
Train: 35 [ 700/1251 ( 56%)]  Loss: 4.054 (4.20)  Time: 0.813s, 1260.31/s  (0.794s, 1289.89/s)  LR: 9.671e-04  Data: 0.011 (0.014)
Train: 35 [ 750/1251 ( 60%)]  Loss: 4.427 (4.21)  Time: 0.778s, 1316.43/s  (0.793s, 1290.91/s)  LR: 9.671e-04  Data: 0.011 (0.013)
Train: 35 [ 800/1251 ( 64%)]  Loss: 3.962 (4.20)  Time: 0.797s, 1284.72/s  (0.793s, 1290.83/s)  LR: 9.671e-04  Data: 0.011 (0.013)
Train: 35 [ 850/1251 ( 68%)]  Loss: 4.240 (4.20)  Time: 0.780s, 1313.55/s  (0.793s, 1291.00/s)  LR: 9.671e-04  Data: 0.011 (0.013)
Train: 35 [ 900/1251 ( 72%)]  Loss: 4.386 (4.21)  Time: 0.777s, 1318.09/s  (0.793s, 1291.67/s)  LR: 9.671e-04  Data: 0.010 (0.013)
Train: 35 [ 950/1251 ( 76%)]  Loss: 4.439 (4.22)  Time: 0.781s, 1311.86/s  (0.792s, 1292.12/s)  LR: 9.671e-04  Data: 0.010 (0.013)
Train: 35 [1000/1251 ( 80%)]  Loss: 4.068 (4.21)  Time: 0.778s, 1316.16/s  (0.792s, 1292.98/s)  LR: 9.671e-04  Data: 0.011 (0.013)
Train: 35 [1050/1251 ( 84%)]  Loss: 4.232 (4.21)  Time: 0.777s, 1317.36/s  (0.792s, 1292.46/s)  LR: 9.671e-04  Data: 0.011 (0.013)
Train: 35 [1100/1251 ( 88%)]  Loss: 4.131 (4.21)  Time: 0.802s, 1276.63/s  (0.792s, 1292.87/s)  LR: 9.671e-04  Data: 0.011 (0.013)
Train: 35 [1150/1251 ( 92%)]  Loss: 4.115 (4.21)  Time: 0.775s, 1320.61/s  (0.792s, 1293.41/s)  LR: 9.671e-04  Data: 0.011 (0.012)
Train: 35 [1200/1251 ( 96%)]  Loss: 4.575 (4.22)  Time: 0.817s, 1253.36/s  (0.792s, 1292.88/s)  LR: 9.671e-04  Data: 0.011 (0.012)
Train: 35 [1250/1251 (100%)]  Loss: 3.941 (4.21)  Time: 0.767s, 1334.40/s  (0.792s, 1292.52/s)  LR: 9.671e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.551 (1.551)  Loss:  1.1866 (1.1866)  Acc@1: 83.2031 (83.2031)  Acc@5: 94.3359 (94.3359)
Test: [  48/48]  Time: 0.172 (0.569)  Loss:  1.1983 (1.8345)  Acc@1: 80.6604 (64.2740)  Acc@5: 93.5142 (86.4580)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-35.pth.tar', 64.27400000244141)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-33.pth.tar', 63.74200000244141)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-34.pth.tar', 63.71000008789063)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-29.pth.tar', 63.24400013183594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-27.pth.tar', 62.95199996826172)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-30.pth.tar', 62.86199992675781)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-31.pth.tar', 62.46200008544922)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-28.pth.tar', 62.396000021972654)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-32.pth.tar', 62.38600006591797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-25.pth.tar', 62.14800014648438)

Train: 36 [   0/1251 (  0%)]  Loss: 4.217 (4.22)  Time: 2.210s,  463.43/s  (2.210s,  463.43/s)  LR: 9.652e-04  Data: 1.476 (1.476)
Train: 36 [  50/1251 (  4%)]  Loss: 3.633 (3.93)  Time: 0.815s, 1255.68/s  (0.825s, 1241.00/s)  LR: 9.652e-04  Data: 0.011 (0.042)
Train: 36 [ 100/1251 (  8%)]  Loss: 4.244 (4.03)  Time: 0.835s, 1226.68/s  (0.817s, 1254.10/s)  LR: 9.652e-04  Data: 0.014 (0.027)
Train: 36 [ 150/1251 ( 12%)]  Loss: 3.962 (4.01)  Time: 0.847s, 1209.46/s  (0.819s, 1249.61/s)  LR: 9.652e-04  Data: 0.010 (0.022)
Train: 36 [ 200/1251 ( 16%)]  Loss: 4.261 (4.06)  Time: 0.781s, 1311.46/s  (0.818s, 1251.67/s)  LR: 9.652e-04  Data: 0.010 (0.019)
Train: 36 [ 250/1251 ( 20%)]  Loss: 4.382 (4.12)  Time: 0.779s, 1314.08/s  (0.812s, 1261.73/s)  LR: 9.652e-04  Data: 0.011 (0.017)
Train: 36 [ 300/1251 ( 24%)]  Loss: 4.167 (4.12)  Time: 0.791s, 1294.03/s  (0.807s, 1268.31/s)  LR: 9.652e-04  Data: 0.011 (0.016)
Train: 36 [ 350/1251 ( 28%)]  Loss: 4.161 (4.13)  Time: 0.783s, 1307.56/s  (0.807s, 1269.38/s)  LR: 9.652e-04  Data: 0.010 (0.016)
Train: 36 [ 400/1251 ( 32%)]  Loss: 3.899 (4.10)  Time: 0.845s, 1212.54/s  (0.806s, 1270.26/s)  LR: 9.652e-04  Data: 0.010 (0.015)
Train: 36 [ 450/1251 ( 36%)]  Loss: 3.997 (4.09)  Time: 0.795s, 1287.51/s  (0.805s, 1272.11/s)  LR: 9.652e-04  Data: 0.011 (0.015)
Train: 36 [ 500/1251 ( 40%)]  Loss: 4.167 (4.10)  Time: 0.816s, 1255.15/s  (0.803s, 1275.41/s)  LR: 9.652e-04  Data: 0.011 (0.014)
Train: 36 [ 550/1251 ( 44%)]  Loss: 4.048 (4.09)  Time: 0.782s, 1309.08/s  (0.801s, 1278.19/s)  LR: 9.652e-04  Data: 0.011 (0.014)
Train: 36 [ 600/1251 ( 48%)]  Loss: 4.093 (4.09)  Time: 0.778s, 1316.89/s  (0.800s, 1280.24/s)  LR: 9.652e-04  Data: 0.011 (0.014)
Train: 36 [ 650/1251 ( 52%)]  Loss: 4.045 (4.09)  Time: 0.778s, 1315.86/s  (0.799s, 1282.16/s)  LR: 9.652e-04  Data: 0.010 (0.013)
Train: 36 [ 700/1251 ( 56%)]  Loss: 4.302 (4.11)  Time: 0.814s, 1257.33/s  (0.799s, 1281.18/s)  LR: 9.652e-04  Data: 0.010 (0.013)
Train: 36 [ 750/1251 ( 60%)]  Loss: 4.231 (4.11)  Time: 0.779s, 1313.68/s  (0.800s, 1280.65/s)  LR: 9.652e-04  Data: 0.011 (0.013)
Train: 36 [ 800/1251 ( 64%)]  Loss: 4.173 (4.12)  Time: 0.781s, 1310.34/s  (0.799s, 1281.27/s)  LR: 9.652e-04  Data: 0.011 (0.013)
Train: 36 [ 850/1251 ( 68%)]  Loss: 4.015 (4.11)  Time: 0.782s, 1310.09/s  (0.799s, 1281.69/s)  LR: 9.652e-04  Data: 0.011 (0.013)
Train: 36 [ 900/1251 ( 72%)]  Loss: 4.053 (4.11)  Time: 0.790s, 1295.92/s  (0.798s, 1282.71/s)  LR: 9.652e-04  Data: 0.011 (0.013)
Train: 36 [ 950/1251 ( 76%)]  Loss: 4.212 (4.11)  Time: 0.782s, 1309.84/s  (0.798s, 1283.95/s)  LR: 9.652e-04  Data: 0.011 (0.013)
Train: 36 [1000/1251 ( 80%)]  Loss: 4.182 (4.12)  Time: 0.784s, 1306.82/s  (0.797s, 1284.51/s)  LR: 9.652e-04  Data: 0.011 (0.013)
Train: 36 [1050/1251 ( 84%)]  Loss: 4.027 (4.11)  Time: 0.783s, 1307.75/s  (0.797s, 1284.66/s)  LR: 9.652e-04  Data: 0.011 (0.013)
Train: 36 [1100/1251 ( 88%)]  Loss: 4.288 (4.12)  Time: 0.787s, 1301.36/s  (0.797s, 1285.10/s)  LR: 9.652e-04  Data: 0.011 (0.012)
Train: 36 [1150/1251 ( 92%)]  Loss: 4.649 (4.14)  Time: 0.841s, 1218.11/s  (0.796s, 1285.71/s)  LR: 9.652e-04  Data: 0.010 (0.012)
Train: 36 [1200/1251 ( 96%)]  Loss: 4.276 (4.15)  Time: 0.788s, 1300.18/s  (0.797s, 1285.42/s)  LR: 9.652e-04  Data: 0.010 (0.012)
Train: 36 [1250/1251 (100%)]  Loss: 4.095 (4.15)  Time: 0.781s, 1311.21/s  (0.796s, 1286.17/s)  LR: 9.652e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.499 (1.499)  Loss:  1.1020 (1.1020)  Acc@1: 82.6172 (82.6172)  Acc@5: 94.7266 (94.7266)
Test: [  48/48]  Time: 0.172 (0.561)  Loss:  0.9461 (1.6904)  Acc@1: 81.7217 (65.0680)  Acc@5: 95.6368 (86.8200)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-36.pth.tar', 65.06799994628906)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-35.pth.tar', 64.27400000244141)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-33.pth.tar', 63.74200000244141)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-34.pth.tar', 63.71000008789063)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-29.pth.tar', 63.24400013183594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-27.pth.tar', 62.95199996826172)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-30.pth.tar', 62.86199992675781)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-31.pth.tar', 62.46200008544922)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-28.pth.tar', 62.396000021972654)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-32.pth.tar', 62.38600006591797)

Train: 37 [   0/1251 (  0%)]  Loss: 3.721 (3.72)  Time: 2.342s,  437.31/s  (2.342s,  437.31/s)  LR: 9.633e-04  Data: 1.606 (1.606)
Train: 37 [  50/1251 (  4%)]  Loss: 4.140 (3.93)  Time: 0.821s, 1247.28/s  (0.835s, 1226.83/s)  LR: 9.633e-04  Data: 0.011 (0.046)
Train: 37 [ 100/1251 (  8%)]  Loss: 3.998 (3.95)  Time: 0.820s, 1249.02/s  (0.813s, 1259.38/s)  LR: 9.633e-04  Data: 0.010 (0.029)
Train: 37 [ 150/1251 ( 12%)]  Loss: 4.048 (3.98)  Time: 0.776s, 1319.61/s  (0.805s, 1272.60/s)  LR: 9.633e-04  Data: 0.010 (0.023)
Train: 37 [ 200/1251 ( 16%)]  Loss: 4.085 (4.00)  Time: 0.782s, 1309.51/s  (0.799s, 1281.25/s)  LR: 9.633e-04  Data: 0.011 (0.020)
Train: 37 [ 250/1251 ( 20%)]  Loss: 4.061 (4.01)  Time: 0.817s, 1253.27/s  (0.798s, 1283.89/s)  LR: 9.633e-04  Data: 0.011 (0.018)
Train: 37 [ 300/1251 ( 24%)]  Loss: 3.786 (3.98)  Time: 0.778s, 1315.49/s  (0.798s, 1283.62/s)  LR: 9.633e-04  Data: 0.011 (0.017)
Train: 37 [ 350/1251 ( 28%)]  Loss: 4.291 (4.02)  Time: 0.780s, 1313.42/s  (0.796s, 1286.98/s)  LR: 9.633e-04  Data: 0.011 (0.016)
Train: 37 [ 400/1251 ( 32%)]  Loss: 4.117 (4.03)  Time: 0.787s, 1301.17/s  (0.795s, 1288.79/s)  LR: 9.633e-04  Data: 0.012 (0.015)
Train: 37 [ 450/1251 ( 36%)]  Loss: 4.008 (4.03)  Time: 0.776s, 1319.21/s  (0.794s, 1290.14/s)  LR: 9.633e-04  Data: 0.010 (0.015)
Train: 37 [ 500/1251 ( 40%)]  Loss: 3.536 (3.98)  Time: 0.779s, 1313.83/s  (0.792s, 1292.25/s)  LR: 9.633e-04  Data: 0.011 (0.015)
Train: 37 [ 550/1251 ( 44%)]  Loss: 4.390 (4.02)  Time: 0.781s, 1310.39/s  (0.791s, 1293.75/s)  LR: 9.633e-04  Data: 0.012 (0.014)
Train: 37 [ 600/1251 ( 48%)]  Loss: 4.038 (4.02)  Time: 0.823s, 1244.47/s  (0.792s, 1293.63/s)  LR: 9.633e-04  Data: 0.010 (0.014)
Train: 37 [ 650/1251 ( 52%)]  Loss: 3.762 (4.00)  Time: 0.778s, 1315.40/s  (0.791s, 1294.90/s)  LR: 9.633e-04  Data: 0.011 (0.014)
Train: 37 [ 700/1251 ( 56%)]  Loss: 3.911 (3.99)  Time: 0.812s, 1260.66/s  (0.790s, 1295.61/s)  LR: 9.633e-04  Data: 0.011 (0.014)
Train: 37 [ 750/1251 ( 60%)]  Loss: 3.844 (3.98)  Time: 0.780s, 1312.39/s  (0.792s, 1293.37/s)  LR: 9.633e-04  Data: 0.010 (0.013)
Train: 37 [ 800/1251 ( 64%)]  Loss: 4.285 (4.00)  Time: 0.817s, 1253.42/s  (0.792s, 1293.20/s)  LR: 9.633e-04  Data: 0.011 (0.013)
Train: 37 [ 850/1251 ( 68%)]  Loss: 4.320 (4.02)  Time: 0.814s, 1257.39/s  (0.793s, 1291.38/s)  LR: 9.633e-04  Data: 0.011 (0.013)
Train: 37 [ 900/1251 ( 72%)]  Loss: 4.106 (4.02)  Time: 0.779s, 1313.74/s  (0.793s, 1291.24/s)  LR: 9.633e-04  Data: 0.011 (0.013)
Train: 37 [ 950/1251 ( 76%)]  Loss: 3.882 (4.02)  Time: 0.779s, 1314.58/s  (0.792s, 1292.37/s)  LR: 9.633e-04  Data: 0.011 (0.013)
Train: 37 [1000/1251 ( 80%)]  Loss: 3.784 (4.01)  Time: 0.813s, 1259.06/s  (0.793s, 1291.88/s)  LR: 9.633e-04  Data: 0.011 (0.013)
Train: 37 [1050/1251 ( 84%)]  Loss: 4.437 (4.03)  Time: 0.781s, 1311.12/s  (0.794s, 1290.26/s)  LR: 9.633e-04  Data: 0.011 (0.013)
Train: 37 [1100/1251 ( 88%)]  Loss: 3.813 (4.02)  Time: 0.784s, 1306.41/s  (0.793s, 1291.07/s)  LR: 9.633e-04  Data: 0.010 (0.013)
Train: 37 [1150/1251 ( 92%)]  Loss: 4.308 (4.03)  Time: 0.777s, 1317.62/s  (0.793s, 1291.83/s)  LR: 9.633e-04  Data: 0.011 (0.013)
Train: 37 [1200/1251 ( 96%)]  Loss: 3.902 (4.02)  Time: 0.783s, 1307.42/s  (0.792s, 1292.14/s)  LR: 9.633e-04  Data: 0.014 (0.013)
Train: 37 [1250/1251 (100%)]  Loss: 4.386 (4.04)  Time: 0.770s, 1329.64/s  (0.792s, 1292.21/s)  LR: 9.633e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.611 (1.611)  Loss:  1.1629 (1.1629)  Acc@1: 80.9570 (80.9570)  Acc@5: 94.4336 (94.4336)
Test: [  48/48]  Time: 0.172 (0.564)  Loss:  1.3511 (1.8234)  Acc@1: 78.0660 (64.5980)  Acc@5: 91.7453 (86.4620)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-36.pth.tar', 65.06799994628906)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-37.pth.tar', 64.59799990966796)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-35.pth.tar', 64.27400000244141)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-33.pth.tar', 63.74200000244141)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-34.pth.tar', 63.71000008789063)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-29.pth.tar', 63.24400013183594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-27.pth.tar', 62.95199996826172)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-30.pth.tar', 62.86199992675781)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-31.pth.tar', 62.46200008544922)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-28.pth.tar', 62.396000021972654)

Train: 38 [   0/1251 (  0%)]  Loss: 4.266 (4.27)  Time: 2.278s,  449.51/s  (2.278s,  449.51/s)  LR: 9.613e-04  Data: 1.541 (1.541)
Train: 38 [  50/1251 (  4%)]  Loss: 3.831 (4.05)  Time: 0.779s, 1315.03/s  (0.826s, 1240.09/s)  LR: 9.613e-04  Data: 0.012 (0.047)
Train: 38 [ 100/1251 (  8%)]  Loss: 4.250 (4.12)  Time: 0.776s, 1319.57/s  (0.809s, 1265.94/s)  LR: 9.613e-04  Data: 0.011 (0.029)
Train: 38 [ 150/1251 ( 12%)]  Loss: 4.122 (4.12)  Time: 0.785s, 1304.07/s  (0.801s, 1277.95/s)  LR: 9.613e-04  Data: 0.011 (0.023)
Train: 38 [ 200/1251 ( 16%)]  Loss: 4.338 (4.16)  Time: 0.777s, 1317.39/s  (0.797s, 1284.28/s)  LR: 9.613e-04  Data: 0.011 (0.020)
Train: 38 [ 250/1251 ( 20%)]  Loss: 4.200 (4.17)  Time: 0.778s, 1315.76/s  (0.796s, 1286.83/s)  LR: 9.613e-04  Data: 0.011 (0.018)
Train: 38 [ 300/1251 ( 24%)]  Loss: 3.988 (4.14)  Time: 0.777s, 1318.55/s  (0.795s, 1288.72/s)  LR: 9.613e-04  Data: 0.011 (0.017)
Train: 38 [ 350/1251 ( 28%)]  Loss: 3.897 (4.11)  Time: 0.788s, 1298.81/s  (0.793s, 1290.77/s)  LR: 9.613e-04  Data: 0.011 (0.016)
Train: 38 [ 400/1251 ( 32%)]  Loss: 3.829 (4.08)  Time: 0.779s, 1315.13/s  (0.793s, 1290.59/s)  LR: 9.613e-04  Data: 0.011 (0.016)
Train: 38 [ 450/1251 ( 36%)]  Loss: 4.210 (4.09)  Time: 0.780s, 1312.13/s  (0.794s, 1290.39/s)  LR: 9.613e-04  Data: 0.011 (0.015)
Train: 38 [ 500/1251 ( 40%)]  Loss: 4.265 (4.11)  Time: 0.780s, 1312.54/s  (0.792s, 1292.24/s)  LR: 9.613e-04  Data: 0.012 (0.015)
Train: 38 [ 550/1251 ( 44%)]  Loss: 3.891 (4.09)  Time: 0.842s, 1215.50/s  (0.793s, 1292.07/s)  LR: 9.613e-04  Data: 0.011 (0.014)
Train: 38 [ 600/1251 ( 48%)]  Loss: 4.134 (4.09)  Time: 0.776s, 1319.74/s  (0.792s, 1292.71/s)  LR: 9.613e-04  Data: 0.011 (0.014)
Train: 38 [ 650/1251 ( 52%)]  Loss: 3.970 (4.09)  Time: 0.779s, 1313.76/s  (0.792s, 1292.52/s)  LR: 9.613e-04  Data: 0.011 (0.014)
Train: 38 [ 700/1251 ( 56%)]  Loss: 4.507 (4.11)  Time: 0.778s, 1315.99/s  (0.792s, 1292.75/s)  LR: 9.613e-04  Data: 0.011 (0.014)
Train: 38 [ 750/1251 ( 60%)]  Loss: 4.236 (4.12)  Time: 0.834s, 1227.70/s  (0.792s, 1292.33/s)  LR: 9.613e-04  Data: 0.011 (0.014)
Train: 38 [ 800/1251 ( 64%)]  Loss: 4.193 (4.13)  Time: 0.813s, 1259.80/s  (0.794s, 1289.78/s)  LR: 9.613e-04  Data: 0.011 (0.013)
Train: 38 [ 850/1251 ( 68%)]  Loss: 4.087 (4.12)  Time: 0.835s, 1225.90/s  (0.796s, 1287.04/s)  LR: 9.613e-04  Data: 0.010 (0.013)
Train: 38 [ 900/1251 ( 72%)]  Loss: 4.489 (4.14)  Time: 0.778s, 1315.93/s  (0.796s, 1286.66/s)  LR: 9.613e-04  Data: 0.011 (0.013)
Train: 38 [ 950/1251 ( 76%)]  Loss: 4.205 (4.15)  Time: 0.830s, 1233.29/s  (0.796s, 1286.96/s)  LR: 9.613e-04  Data: 0.011 (0.013)
Train: 38 [1000/1251 ( 80%)]  Loss: 3.918 (4.13)  Time: 0.848s, 1207.53/s  (0.795s, 1287.54/s)  LR: 9.613e-04  Data: 0.011 (0.013)
Train: 38 [1050/1251 ( 84%)]  Loss: 4.283 (4.14)  Time: 0.785s, 1304.12/s  (0.796s, 1286.70/s)  LR: 9.613e-04  Data: 0.011 (0.013)
Train: 38 [1100/1251 ( 88%)]  Loss: 3.945 (4.13)  Time: 0.840s, 1218.99/s  (0.796s, 1286.04/s)  LR: 9.613e-04  Data: 0.012 (0.013)
Train: 38 [1150/1251 ( 92%)]  Loss: 4.061 (4.13)  Time: 0.793s, 1290.73/s  (0.796s, 1287.22/s)  LR: 9.613e-04  Data: 0.011 (0.013)
Train: 38 [1200/1251 ( 96%)]  Loss: 3.971 (4.12)  Time: 0.783s, 1308.22/s  (0.795s, 1287.94/s)  LR: 9.613e-04  Data: 0.011 (0.013)
Train: 38 [1250/1251 (100%)]  Loss: 4.427 (4.14)  Time: 0.769s, 1331.39/s  (0.795s, 1287.39/s)  LR: 9.613e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.540 (1.540)  Loss:  1.2527 (1.2527)  Acc@1: 81.9336 (81.9336)  Acc@5: 94.0430 (94.0430)
Test: [  48/48]  Time: 0.172 (0.564)  Loss:  1.3279 (1.9185)  Acc@1: 79.9528 (63.9420)  Acc@5: 91.8632 (85.7360)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-36.pth.tar', 65.06799994628906)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-37.pth.tar', 64.59799990966796)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-35.pth.tar', 64.27400000244141)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-38.pth.tar', 63.94200008300781)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-33.pth.tar', 63.74200000244141)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-34.pth.tar', 63.71000008789063)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-29.pth.tar', 63.24400013183594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-27.pth.tar', 62.95199996826172)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-30.pth.tar', 62.86199992675781)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-31.pth.tar', 62.46200008544922)

Train: 39 [   0/1251 (  0%)]  Loss: 3.909 (3.91)  Time: 2.309s,  443.57/s  (2.309s,  443.57/s)  LR: 9.593e-04  Data: 1.571 (1.571)
Train: 39 [  50/1251 (  4%)]  Loss: 4.183 (4.05)  Time: 0.822s, 1245.44/s  (0.841s, 1217.91/s)  LR: 9.593e-04  Data: 0.011 (0.045)
Train: 39 [ 100/1251 (  8%)]  Loss: 4.242 (4.11)  Time: 0.780s, 1312.97/s  (0.814s, 1257.93/s)  LR: 9.593e-04  Data: 0.012 (0.028)
Train: 39 [ 150/1251 ( 12%)]  Loss: 4.093 (4.11)  Time: 0.778s, 1316.54/s  (0.805s, 1272.02/s)  LR: 9.593e-04  Data: 0.010 (0.023)
Train: 39 [ 200/1251 ( 16%)]  Loss: 3.761 (4.04)  Time: 0.781s, 1311.24/s  (0.799s, 1280.96/s)  LR: 9.593e-04  Data: 0.011 (0.020)
Train: 39 [ 250/1251 ( 20%)]  Loss: 4.417 (4.10)  Time: 0.776s, 1319.56/s  (0.800s, 1279.26/s)  LR: 9.593e-04  Data: 0.011 (0.018)
Train: 39 [ 300/1251 ( 24%)]  Loss: 4.207 (4.12)  Time: 0.779s, 1314.73/s  (0.798s, 1282.58/s)  LR: 9.593e-04  Data: 0.010 (0.017)
Train: 39 [ 350/1251 ( 28%)]  Loss: 3.961 (4.10)  Time: 0.816s, 1254.40/s  (0.798s, 1282.99/s)  LR: 9.593e-04  Data: 0.011 (0.016)
Train: 39 [ 400/1251 ( 32%)]  Loss: 4.152 (4.10)  Time: 0.778s, 1315.51/s  (0.798s, 1283.60/s)  LR: 9.593e-04  Data: 0.011 (0.015)
Train: 39 [ 450/1251 ( 36%)]  Loss: 4.121 (4.10)  Time: 0.814s, 1257.87/s  (0.797s, 1284.68/s)  LR: 9.593e-04  Data: 0.012 (0.015)
Train: 39 [ 500/1251 ( 40%)]  Loss: 3.795 (4.08)  Time: 0.777s, 1318.13/s  (0.796s, 1286.91/s)  LR: 9.593e-04  Data: 0.011 (0.015)
Train: 39 [ 550/1251 ( 44%)]  Loss: 4.178 (4.08)  Time: 0.778s, 1316.65/s  (0.796s, 1286.60/s)  LR: 9.593e-04  Data: 0.011 (0.014)
Train: 39 [ 600/1251 ( 48%)]  Loss: 3.937 (4.07)  Time: 0.793s, 1291.15/s  (0.795s, 1288.24/s)  LR: 9.593e-04  Data: 0.012 (0.014)
Train: 39 [ 650/1251 ( 52%)]  Loss: 4.492 (4.10)  Time: 0.814s, 1257.67/s  (0.796s, 1286.67/s)  LR: 9.593e-04  Data: 0.011 (0.014)
Train: 39 [ 700/1251 ( 56%)]  Loss: 3.759 (4.08)  Time: 0.779s, 1314.45/s  (0.796s, 1286.87/s)  LR: 9.593e-04  Data: 0.011 (0.014)
Train: 39 [ 750/1251 ( 60%)]  Loss: 4.214 (4.09)  Time: 0.778s, 1316.86/s  (0.796s, 1286.47/s)  LR: 9.593e-04  Data: 0.012 (0.013)
Train: 39 [ 800/1251 ( 64%)]  Loss: 4.260 (4.10)  Time: 0.793s, 1291.73/s  (0.797s, 1284.17/s)  LR: 9.593e-04  Data: 0.011 (0.013)
Train: 39 [ 850/1251 ( 68%)]  Loss: 4.130 (4.10)  Time: 0.779s, 1314.79/s  (0.796s, 1285.68/s)  LR: 9.593e-04  Data: 0.012 (0.013)
Train: 39 [ 900/1251 ( 72%)]  Loss: 3.913 (4.09)  Time: 0.779s, 1314.72/s  (0.797s, 1285.34/s)  LR: 9.593e-04  Data: 0.011 (0.013)
Train: 39 [ 950/1251 ( 76%)]  Loss: 3.980 (4.09)  Time: 0.777s, 1317.16/s  (0.796s, 1285.81/s)  LR: 9.593e-04  Data: 0.011 (0.013)
Train: 39 [1000/1251 ( 80%)]  Loss: 4.096 (4.09)  Time: 0.779s, 1314.29/s  (0.796s, 1286.67/s)  LR: 9.593e-04  Data: 0.011 (0.013)
Train: 39 [1050/1251 ( 84%)]  Loss: 4.397 (4.10)  Time: 0.780s, 1312.17/s  (0.795s, 1287.62/s)  LR: 9.593e-04  Data: 0.011 (0.013)
Train: 39 [1100/1251 ( 88%)]  Loss: 4.477 (4.12)  Time: 0.836s, 1224.56/s  (0.795s, 1288.13/s)  LR: 9.593e-04  Data: 0.011 (0.013)
Train: 39 [1150/1251 ( 92%)]  Loss: 4.162 (4.12)  Time: 0.781s, 1311.78/s  (0.795s, 1288.66/s)  LR: 9.593e-04  Data: 0.011 (0.013)
Train: 39 [1200/1251 ( 96%)]  Loss: 3.839 (4.11)  Time: 0.780s, 1312.27/s  (0.795s, 1288.57/s)  LR: 9.593e-04  Data: 0.011 (0.013)
Train: 39 [1250/1251 (100%)]  Loss: 3.895 (4.10)  Time: 0.768s, 1332.91/s  (0.795s, 1288.22/s)  LR: 9.593e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.568 (1.568)  Loss:  1.2592 (1.2592)  Acc@1: 82.4219 (82.4219)  Acc@5: 95.5078 (95.5078)
Test: [  48/48]  Time: 0.172 (0.557)  Loss:  1.4029 (1.8848)  Acc@1: 78.5377 (65.0100)  Acc@5: 92.2170 (86.7760)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-36.pth.tar', 65.06799994628906)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-39.pth.tar', 65.00999998535156)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-37.pth.tar', 64.59799990966796)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-35.pth.tar', 64.27400000244141)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-38.pth.tar', 63.94200008300781)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-33.pth.tar', 63.74200000244141)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-34.pth.tar', 63.71000008789063)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-29.pth.tar', 63.24400013183594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-27.pth.tar', 62.95199996826172)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-30.pth.tar', 62.86199992675781)

Train: 40 [   0/1251 (  0%)]  Loss: 4.509 (4.51)  Time: 2.332s,  439.18/s  (2.332s,  439.18/s)  LR: 9.572e-04  Data: 1.596 (1.596)
Train: 40 [  50/1251 (  4%)]  Loss: 4.155 (4.33)  Time: 0.811s, 1263.42/s  (0.834s, 1228.01/s)  LR: 9.572e-04  Data: 0.010 (0.048)
Train: 40 [ 100/1251 (  8%)]  Loss: 3.799 (4.15)  Time: 0.780s, 1313.56/s  (0.813s, 1259.79/s)  LR: 9.572e-04  Data: 0.011 (0.030)
Train: 40 [ 150/1251 ( 12%)]  Loss: 4.202 (4.17)  Time: 0.838s, 1222.67/s  (0.804s, 1274.03/s)  LR: 9.572e-04  Data: 0.011 (0.023)
Train: 40 [ 200/1251 ( 16%)]  Loss: 4.540 (4.24)  Time: 0.778s, 1316.34/s  (0.807s, 1269.46/s)  LR: 9.572e-04  Data: 0.012 (0.020)
Train: 40 [ 250/1251 ( 20%)]  Loss: 4.150 (4.23)  Time: 0.777s, 1317.83/s  (0.806s, 1271.13/s)  LR: 9.572e-04  Data: 0.011 (0.019)
Train: 40 [ 300/1251 ( 24%)]  Loss: 4.000 (4.19)  Time: 0.788s, 1300.04/s  (0.802s, 1276.76/s)  LR: 9.572e-04  Data: 0.011 (0.017)
Train: 40 [ 350/1251 ( 28%)]  Loss: 4.522 (4.23)  Time: 0.778s, 1315.89/s  (0.801s, 1277.64/s)  LR: 9.572e-04  Data: 0.011 (0.016)
Train: 40 [ 400/1251 ( 32%)]  Loss: 3.678 (4.17)  Time: 0.786s, 1302.05/s  (0.799s, 1280.93/s)  LR: 9.572e-04  Data: 0.012 (0.016)
Train: 40 [ 450/1251 ( 36%)]  Loss: 4.071 (4.16)  Time: 0.835s, 1227.00/s  (0.800s, 1280.52/s)  LR: 9.572e-04  Data: 0.010 (0.015)
Train: 40 [ 500/1251 ( 40%)]  Loss: 4.202 (4.17)  Time: 0.781s, 1311.38/s  (0.800s, 1280.68/s)  LR: 9.572e-04  Data: 0.011 (0.015)
Train: 40 [ 550/1251 ( 44%)]  Loss: 3.911 (4.14)  Time: 0.779s, 1313.80/s  (0.799s, 1280.95/s)  LR: 9.572e-04  Data: 0.011 (0.015)
Train: 40 [ 600/1251 ( 48%)]  Loss: 4.283 (4.16)  Time: 0.815s, 1256.55/s  (0.798s, 1282.49/s)  LR: 9.572e-04  Data: 0.011 (0.014)
Train: 40 [ 650/1251 ( 52%)]  Loss: 4.119 (4.15)  Time: 0.783s, 1308.27/s  (0.798s, 1283.78/s)  LR: 9.572e-04  Data: 0.011 (0.014)
Train: 40 [ 700/1251 ( 56%)]  Loss: 4.073 (4.15)  Time: 0.778s, 1316.27/s  (0.797s, 1284.72/s)  LR: 9.572e-04  Data: 0.010 (0.014)
Train: 40 [ 750/1251 ( 60%)]  Loss: 4.535 (4.17)  Time: 0.777s, 1317.80/s  (0.798s, 1283.85/s)  LR: 9.572e-04  Data: 0.010 (0.014)
Train: 40 [ 800/1251 ( 64%)]  Loss: 4.170 (4.17)  Time: 0.792s, 1293.17/s  (0.797s, 1285.06/s)  LR: 9.572e-04  Data: 0.011 (0.013)
Train: 40 [ 850/1251 ( 68%)]  Loss: 4.619 (4.20)  Time: 0.809s, 1265.83/s  (0.798s, 1283.65/s)  LR: 9.572e-04  Data: 0.014 (0.013)
Train: 40 [ 900/1251 ( 72%)]  Loss: 4.034 (4.19)  Time: 0.775s, 1321.04/s  (0.797s, 1284.56/s)  LR: 9.572e-04  Data: 0.011 (0.013)
Train: 40 [ 950/1251 ( 76%)]  Loss: 3.986 (4.18)  Time: 0.826s, 1240.25/s  (0.797s, 1284.97/s)  LR: 9.572e-04  Data: 0.011 (0.013)
Train: 40 [1000/1251 ( 80%)]  Loss: 4.342 (4.19)  Time: 0.779s, 1314.63/s  (0.797s, 1285.55/s)  LR: 9.572e-04  Data: 0.011 (0.013)
Train: 40 [1050/1251 ( 84%)]  Loss: 4.061 (4.18)  Time: 0.834s, 1227.41/s  (0.797s, 1284.59/s)  LR: 9.572e-04  Data: 0.012 (0.013)
Train: 40 [1100/1251 ( 88%)]  Loss: 4.294 (4.19)  Time: 0.778s, 1316.96/s  (0.797s, 1284.21/s)  LR: 9.572e-04  Data: 0.011 (0.013)
Train: 40 [1150/1251 ( 92%)]  Loss: 4.069 (4.18)  Time: 0.850s, 1204.74/s  (0.798s, 1283.93/s)  LR: 9.572e-04  Data: 0.011 (0.013)
Train: 40 [1200/1251 ( 96%)]  Loss: 3.953 (4.17)  Time: 0.782s, 1309.72/s  (0.798s, 1283.89/s)  LR: 9.572e-04  Data: 0.011 (0.013)
Train: 40 [1250/1251 (100%)]  Loss: 3.976 (4.16)  Time: 0.770s, 1330.66/s  (0.797s, 1284.80/s)  LR: 9.572e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.565 (1.565)  Loss:  1.0299 (1.0299)  Acc@1: 82.5195 (82.5195)  Acc@5: 94.2383 (94.2383)
Test: [  48/48]  Time: 0.172 (0.566)  Loss:  1.1349 (1.7044)  Acc@1: 78.4198 (64.9060)  Acc@5: 92.4528 (86.5540)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-36.pth.tar', 65.06799994628906)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-39.pth.tar', 65.00999998535156)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-40.pth.tar', 64.90599993408203)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-37.pth.tar', 64.59799990966796)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-35.pth.tar', 64.27400000244141)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-38.pth.tar', 63.94200008300781)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-33.pth.tar', 63.74200000244141)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-34.pth.tar', 63.71000008789063)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-29.pth.tar', 63.24400013183594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-27.pth.tar', 62.95199996826172)

Train: 41 [   0/1251 (  0%)]  Loss: 4.232 (4.23)  Time: 2.299s,  445.32/s  (2.299s,  445.32/s)  LR: 9.551e-04  Data: 1.566 (1.566)
Train: 41 [  50/1251 (  4%)]  Loss: 4.201 (4.22)  Time: 0.791s, 1294.56/s  (0.832s, 1230.63/s)  LR: 9.551e-04  Data: 0.012 (0.052)
Train: 41 [ 100/1251 (  8%)]  Loss: 4.097 (4.18)  Time: 0.778s, 1316.38/s  (0.823s, 1244.40/s)  LR: 9.551e-04  Data: 0.011 (0.032)
Train: 41 [ 150/1251 ( 12%)]  Loss: 4.106 (4.16)  Time: 0.780s, 1313.56/s  (0.814s, 1257.66/s)  LR: 9.551e-04  Data: 0.011 (0.025)
Train: 41 [ 200/1251 ( 16%)]  Loss: 4.092 (4.15)  Time: 0.775s, 1321.21/s  (0.807s, 1269.52/s)  LR: 9.551e-04  Data: 0.011 (0.022)
Train: 41 [ 250/1251 ( 20%)]  Loss: 3.886 (4.10)  Time: 0.786s, 1302.66/s  (0.802s, 1277.09/s)  LR: 9.551e-04  Data: 0.011 (0.020)
Train: 41 [ 300/1251 ( 24%)]  Loss: 4.231 (4.12)  Time: 0.822s, 1245.68/s  (0.800s, 1279.63/s)  LR: 9.551e-04  Data: 0.012 (0.018)
Train: 41 [ 350/1251 ( 28%)]  Loss: 4.218 (4.13)  Time: 0.792s, 1292.35/s  (0.801s, 1277.89/s)  LR: 9.551e-04  Data: 0.011 (0.017)
Train: 41 [ 400/1251 ( 32%)]  Loss: 4.054 (4.12)  Time: 0.785s, 1305.21/s  (0.800s, 1280.30/s)  LR: 9.551e-04  Data: 0.012 (0.017)
Train: 41 [ 450/1251 ( 36%)]  Loss: 4.194 (4.13)  Time: 0.780s, 1312.54/s  (0.798s, 1283.06/s)  LR: 9.551e-04  Data: 0.011 (0.016)
Train: 41 [ 500/1251 ( 40%)]  Loss: 4.447 (4.16)  Time: 0.779s, 1314.20/s  (0.796s, 1285.65/s)  LR: 9.551e-04  Data: 0.011 (0.016)
Train: 41 [ 550/1251 ( 44%)]  Loss: 4.167 (4.16)  Time: 0.778s, 1315.97/s  (0.796s, 1286.04/s)  LR: 9.551e-04  Data: 0.010 (0.015)
Train: 41 [ 600/1251 ( 48%)]  Loss: 4.320 (4.17)  Time: 0.813s, 1259.93/s  (0.797s, 1284.51/s)  LR: 9.551e-04  Data: 0.011 (0.015)
Train: 41 [ 650/1251 ( 52%)]  Loss: 3.922 (4.15)  Time: 0.805s, 1272.40/s  (0.797s, 1285.25/s)  LR: 9.551e-04  Data: 0.011 (0.014)
Train: 41 [ 700/1251 ( 56%)]  Loss: 4.325 (4.17)  Time: 0.807s, 1268.60/s  (0.797s, 1285.10/s)  LR: 9.551e-04  Data: 0.011 (0.014)
Train: 41 [ 750/1251 ( 60%)]  Loss: 4.034 (4.16)  Time: 0.814s, 1257.90/s  (0.797s, 1285.29/s)  LR: 9.551e-04  Data: 0.011 (0.014)
Train: 41 [ 800/1251 ( 64%)]  Loss: 4.200 (4.16)  Time: 0.778s, 1315.74/s  (0.798s, 1283.66/s)  LR: 9.551e-04  Data: 0.011 (0.014)
Train: 41 [ 850/1251 ( 68%)]  Loss: 4.042 (4.15)  Time: 0.778s, 1316.15/s  (0.797s, 1285.09/s)  LR: 9.551e-04  Data: 0.010 (0.014)
Train: 41 [ 900/1251 ( 72%)]  Loss: 3.950 (4.14)  Time: 0.790s, 1296.69/s  (0.797s, 1284.78/s)  LR: 9.551e-04  Data: 0.011 (0.013)
Train: 41 [ 950/1251 ( 76%)]  Loss: 4.289 (4.15)  Time: 0.774s, 1322.51/s  (0.797s, 1284.15/s)  LR: 9.551e-04  Data: 0.011 (0.013)
Train: 41 [1000/1251 ( 80%)]  Loss: 4.294 (4.16)  Time: 0.777s, 1317.28/s  (0.797s, 1285.36/s)  LR: 9.551e-04  Data: 0.011 (0.013)
Train: 41 [1050/1251 ( 84%)]  Loss: 4.014 (4.15)  Time: 0.786s, 1302.99/s  (0.796s, 1286.10/s)  LR: 9.551e-04  Data: 0.011 (0.013)
Train: 41 [1100/1251 ( 88%)]  Loss: 4.133 (4.15)  Time: 0.779s, 1314.46/s  (0.797s, 1284.90/s)  LR: 9.551e-04  Data: 0.010 (0.013)
Train: 41 [1150/1251 ( 92%)]  Loss: 4.310 (4.16)  Time: 0.779s, 1313.88/s  (0.796s, 1285.84/s)  LR: 9.551e-04  Data: 0.011 (0.013)
Train: 41 [1200/1251 ( 96%)]  Loss: 4.141 (4.16)  Time: 0.814s, 1258.19/s  (0.796s, 1285.87/s)  LR: 9.551e-04  Data: 0.010 (0.013)
Train: 41 [1250/1251 (100%)]  Loss: 3.902 (4.15)  Time: 0.768s, 1332.71/s  (0.796s, 1286.28/s)  LR: 9.551e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.528 (1.528)  Loss:  1.0174 (1.0174)  Acc@1: 82.3242 (82.3242)  Acc@5: 96.2891 (96.2891)
Test: [  48/48]  Time: 0.172 (0.561)  Loss:  1.1583 (1.7159)  Acc@1: 82.1934 (65.7160)  Acc@5: 93.7500 (87.1900)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-41.pth.tar', 65.71600002197266)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-36.pth.tar', 65.06799994628906)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-39.pth.tar', 65.00999998535156)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-40.pth.tar', 64.90599993408203)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-37.pth.tar', 64.59799990966796)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-35.pth.tar', 64.27400000244141)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-38.pth.tar', 63.94200008300781)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-33.pth.tar', 63.74200000244141)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-34.pth.tar', 63.71000008789063)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-29.pth.tar', 63.24400013183594)

Train: 42 [   0/1251 (  0%)]  Loss: 4.136 (4.14)  Time: 2.250s,  455.09/s  (2.250s,  455.09/s)  LR: 9.529e-04  Data: 1.513 (1.513)
Train: 42 [  50/1251 (  4%)]  Loss: 3.876 (4.01)  Time: 0.779s, 1314.06/s  (0.841s, 1217.36/s)  LR: 9.529e-04  Data: 0.011 (0.051)
Train: 42 [ 100/1251 (  8%)]  Loss: 3.932 (3.98)  Time: 0.786s, 1303.43/s  (0.812s, 1261.58/s)  LR: 9.529e-04  Data: 0.014 (0.031)
Train: 42 [ 150/1251 ( 12%)]  Loss: 4.260 (4.05)  Time: 0.779s, 1313.80/s  (0.806s, 1269.87/s)  LR: 9.529e-04  Data: 0.010 (0.025)
Train: 42 [ 200/1251 ( 16%)]  Loss: 4.192 (4.08)  Time: 0.781s, 1310.59/s  (0.803s, 1275.89/s)  LR: 9.529e-04  Data: 0.010 (0.021)
Train: 42 [ 250/1251 ( 20%)]  Loss: 4.054 (4.07)  Time: 0.783s, 1308.11/s  (0.801s, 1278.75/s)  LR: 9.529e-04  Data: 0.011 (0.019)
Train: 42 [ 300/1251 ( 24%)]  Loss: 4.354 (4.11)  Time: 0.828s, 1236.90/s  (0.800s, 1279.65/s)  LR: 9.529e-04  Data: 0.011 (0.018)
Train: 42 [ 350/1251 ( 28%)]  Loss: 4.064 (4.11)  Time: 0.776s, 1318.91/s  (0.800s, 1280.03/s)  LR: 9.529e-04  Data: 0.011 (0.017)
Train: 42 [ 400/1251 ( 32%)]  Loss: 4.028 (4.10)  Time: 0.796s, 1286.09/s  (0.799s, 1281.63/s)  LR: 9.529e-04  Data: 0.011 (0.016)
Train: 42 [ 450/1251 ( 36%)]  Loss: 4.274 (4.12)  Time: 0.780s, 1313.39/s  (0.798s, 1283.73/s)  LR: 9.529e-04  Data: 0.011 (0.016)
Train: 42 [ 500/1251 ( 40%)]  Loss: 4.166 (4.12)  Time: 0.805s, 1271.67/s  (0.797s, 1285.57/s)  LR: 9.529e-04  Data: 0.011 (0.015)
Train: 42 [ 550/1251 ( 44%)]  Loss: 4.124 (4.12)  Time: 0.777s, 1318.52/s  (0.796s, 1286.65/s)  LR: 9.529e-04  Data: 0.011 (0.015)
Train: 42 [ 600/1251 ( 48%)]  Loss: 4.003 (4.11)  Time: 0.779s, 1314.75/s  (0.795s, 1287.91/s)  LR: 9.529e-04  Data: 0.011 (0.014)
Train: 42 [ 650/1251 ( 52%)]  Loss: 4.344 (4.13)  Time: 0.778s, 1315.54/s  (0.794s, 1288.97/s)  LR: 9.529e-04  Data: 0.011 (0.014)
Train: 42 [ 700/1251 ( 56%)]  Loss: 4.082 (4.13)  Time: 0.810s, 1264.70/s  (0.795s, 1288.42/s)  LR: 9.529e-04  Data: 0.011 (0.014)
Train: 42 [ 750/1251 ( 60%)]  Loss: 4.427 (4.14)  Time: 0.779s, 1315.08/s  (0.795s, 1288.11/s)  LR: 9.529e-04  Data: 0.011 (0.014)
Train: 42 [ 800/1251 ( 64%)]  Loss: 4.276 (4.15)  Time: 0.780s, 1312.81/s  (0.795s, 1288.18/s)  LR: 9.529e-04  Data: 0.011 (0.014)
Train: 42 [ 850/1251 ( 68%)]  Loss: 4.005 (4.14)  Time: 0.799s, 1281.16/s  (0.795s, 1287.46/s)  LR: 9.529e-04  Data: 0.013 (0.013)
Train: 42 [ 900/1251 ( 72%)]  Loss: 4.036 (4.14)  Time: 0.826s, 1239.56/s  (0.795s, 1287.59/s)  LR: 9.529e-04  Data: 0.011 (0.013)
Train: 42 [ 950/1251 ( 76%)]  Loss: 3.762 (4.12)  Time: 0.811s, 1262.50/s  (0.795s, 1287.88/s)  LR: 9.529e-04  Data: 0.011 (0.013)
Train: 42 [1000/1251 ( 80%)]  Loss: 4.062 (4.12)  Time: 0.815s, 1256.11/s  (0.795s, 1288.39/s)  LR: 9.529e-04  Data: 0.011 (0.013)
Train: 42 [1050/1251 ( 84%)]  Loss: 4.093 (4.12)  Time: 0.822s, 1245.90/s  (0.796s, 1286.43/s)  LR: 9.529e-04  Data: 0.012 (0.013)
Train: 42 [1100/1251 ( 88%)]  Loss: 3.994 (4.11)  Time: 0.808s, 1267.81/s  (0.796s, 1285.76/s)  LR: 9.529e-04  Data: 0.011 (0.013)
Train: 42 [1150/1251 ( 92%)]  Loss: 4.006 (4.11)  Time: 0.802s, 1276.74/s  (0.796s, 1285.99/s)  LR: 9.529e-04  Data: 0.011 (0.013)
Train: 42 [1200/1251 ( 96%)]  Loss: 4.337 (4.12)  Time: 0.780s, 1312.15/s  (0.796s, 1286.34/s)  LR: 9.529e-04  Data: 0.011 (0.013)
Train: 42 [1250/1251 (100%)]  Loss: 4.100 (4.11)  Time: 0.768s, 1333.69/s  (0.796s, 1287.23/s)  LR: 9.529e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.541 (1.541)  Loss:  1.3112 (1.3112)  Acc@1: 80.5664 (80.5664)  Acc@5: 94.4336 (94.4336)
Test: [  48/48]  Time: 0.172 (0.566)  Loss:  1.3097 (1.8792)  Acc@1: 80.3066 (65.2000)  Acc@5: 93.7500 (86.8640)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-41.pth.tar', 65.71600002197266)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-42.pth.tar', 65.20000010742187)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-36.pth.tar', 65.06799994628906)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-39.pth.tar', 65.00999998535156)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-40.pth.tar', 64.90599993408203)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-37.pth.tar', 64.59799990966796)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-35.pth.tar', 64.27400000244141)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-38.pth.tar', 63.94200008300781)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-33.pth.tar', 63.74200000244141)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-34.pth.tar', 63.71000008789063)

Train: 43 [   0/1251 (  0%)]  Loss: 3.884 (3.88)  Time: 2.460s,  416.33/s  (2.460s,  416.33/s)  LR: 9.507e-04  Data: 1.723 (1.723)
Train: 43 [  50/1251 (  4%)]  Loss: 3.873 (3.88)  Time: 0.800s, 1279.49/s  (0.841s, 1218.21/s)  LR: 9.507e-04  Data: 0.011 (0.048)
Train: 43 [ 100/1251 (  8%)]  Loss: 3.853 (3.87)  Time: 0.831s, 1232.81/s  (0.814s, 1258.65/s)  LR: 9.507e-04  Data: 0.012 (0.030)
Train: 43 [ 150/1251 ( 12%)]  Loss: 4.076 (3.92)  Time: 0.777s, 1317.45/s  (0.806s, 1270.21/s)  LR: 9.507e-04  Data: 0.011 (0.024)
Train: 43 [ 200/1251 ( 16%)]  Loss: 4.062 (3.95)  Time: 0.781s, 1311.80/s  (0.800s, 1279.55/s)  LR: 9.507e-04  Data: 0.010 (0.021)
Train: 43 [ 250/1251 ( 20%)]  Loss: 4.371 (4.02)  Time: 0.779s, 1314.24/s  (0.797s, 1284.75/s)  LR: 9.507e-04  Data: 0.011 (0.019)
Train: 43 [ 300/1251 ( 24%)]  Loss: 4.059 (4.03)  Time: 0.780s, 1313.60/s  (0.796s, 1286.24/s)  LR: 9.507e-04  Data: 0.011 (0.017)
Train: 43 [ 350/1251 ( 28%)]  Loss: 4.313 (4.06)  Time: 0.809s, 1266.22/s  (0.795s, 1288.27/s)  LR: 9.507e-04  Data: 0.010 (0.017)
Train: 43 [ 400/1251 ( 32%)]  Loss: 4.361 (4.09)  Time: 0.779s, 1314.94/s  (0.793s, 1291.20/s)  LR: 9.507e-04  Data: 0.011 (0.016)
Train: 43 [ 450/1251 ( 36%)]  Loss: 4.300 (4.12)  Time: 0.778s, 1316.04/s  (0.792s, 1293.09/s)  LR: 9.507e-04  Data: 0.011 (0.015)
Train: 43 [ 500/1251 ( 40%)]  Loss: 3.965 (4.10)  Time: 0.778s, 1316.00/s  (0.791s, 1294.44/s)  LR: 9.507e-04  Data: 0.010 (0.015)
Train: 43 [ 550/1251 ( 44%)]  Loss: 4.238 (4.11)  Time: 0.786s, 1301.99/s  (0.790s, 1295.88/s)  LR: 9.507e-04  Data: 0.011 (0.015)
Train: 43 [ 600/1251 ( 48%)]  Loss: 4.072 (4.11)  Time: 0.794s, 1288.90/s  (0.790s, 1296.79/s)  LR: 9.507e-04  Data: 0.011 (0.014)
Train: 43 [ 650/1251 ( 52%)]  Loss: 4.163 (4.11)  Time: 0.783s, 1307.71/s  (0.790s, 1295.99/s)  LR: 9.507e-04  Data: 0.012 (0.014)
Train: 43 [ 700/1251 ( 56%)]  Loss: 3.967 (4.10)  Time: 0.784s, 1306.15/s  (0.790s, 1296.43/s)  LR: 9.507e-04  Data: 0.010 (0.014)
Train: 43 [ 750/1251 ( 60%)]  Loss: 4.420 (4.12)  Time: 0.781s, 1310.46/s  (0.790s, 1296.64/s)  LR: 9.507e-04  Data: 0.010 (0.014)
Train: 43 [ 800/1251 ( 64%)]  Loss: 3.828 (4.11)  Time: 0.782s, 1309.99/s  (0.790s, 1296.13/s)  LR: 9.507e-04  Data: 0.011 (0.013)
Train: 43 [ 850/1251 ( 68%)]  Loss: 4.031 (4.10)  Time: 0.780s, 1313.57/s  (0.790s, 1297.01/s)  LR: 9.507e-04  Data: 0.010 (0.013)
Train: 43 [ 900/1251 ( 72%)]  Loss: 4.288 (4.11)  Time: 0.777s, 1317.76/s  (0.790s, 1295.98/s)  LR: 9.507e-04  Data: 0.011 (0.013)
Train: 43 [ 950/1251 ( 76%)]  Loss: 4.086 (4.11)  Time: 0.780s, 1312.66/s  (0.790s, 1296.04/s)  LR: 9.507e-04  Data: 0.012 (0.013)
Train: 43 [1000/1251 ( 80%)]  Loss: 4.174 (4.11)  Time: 0.780s, 1312.85/s  (0.790s, 1296.18/s)  LR: 9.507e-04  Data: 0.011 (0.013)
Train: 43 [1050/1251 ( 84%)]  Loss: 4.031 (4.11)  Time: 0.780s, 1312.95/s  (0.790s, 1296.29/s)  LR: 9.507e-04  Data: 0.010 (0.013)
Train: 43 [1100/1251 ( 88%)]  Loss: 4.195 (4.11)  Time: 0.822s, 1246.09/s  (0.790s, 1296.72/s)  LR: 9.507e-04  Data: 0.011 (0.013)
Train: 43 [1150/1251 ( 92%)]  Loss: 4.278 (4.12)  Time: 0.775s, 1321.18/s  (0.790s, 1295.69/s)  LR: 9.507e-04  Data: 0.010 (0.013)
Train: 43 [1200/1251 ( 96%)]  Loss: 4.329 (4.13)  Time: 0.777s, 1318.36/s  (0.790s, 1296.27/s)  LR: 9.507e-04  Data: 0.011 (0.013)
Train: 43 [1250/1251 (100%)]  Loss: 4.044 (4.13)  Time: 0.767s, 1334.69/s  (0.790s, 1296.68/s)  LR: 9.507e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.558 (1.558)  Loss:  1.0821 (1.0821)  Acc@1: 81.6406 (81.6406)  Acc@5: 94.7266 (94.7266)
Test: [  48/48]  Time: 0.172 (0.562)  Loss:  1.1583 (1.7833)  Acc@1: 80.5425 (65.0280)  Acc@5: 92.0991 (86.7640)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-41.pth.tar', 65.71600002197266)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-42.pth.tar', 65.20000010742187)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-36.pth.tar', 65.06799994628906)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-43.pth.tar', 65.02800020996094)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-39.pth.tar', 65.00999998535156)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-40.pth.tar', 64.90599993408203)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-37.pth.tar', 64.59799990966796)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-35.pth.tar', 64.27400000244141)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-38.pth.tar', 63.94200008300781)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-33.pth.tar', 63.74200000244141)

Train: 44 [   0/1251 (  0%)]  Loss: 4.265 (4.26)  Time: 2.251s,  455.01/s  (2.251s,  455.01/s)  LR: 9.484e-04  Data: 1.516 (1.516)
Train: 44 [  50/1251 (  4%)]  Loss: 3.617 (3.94)  Time: 0.781s, 1310.66/s  (0.815s, 1256.42/s)  LR: 9.484e-04  Data: 0.011 (0.045)
Train: 44 [ 100/1251 (  8%)]  Loss: 4.106 (4.00)  Time: 0.841s, 1217.50/s  (0.808s, 1267.52/s)  LR: 9.484e-04  Data: 0.010 (0.028)
Train: 44 [ 150/1251 ( 12%)]  Loss: 3.961 (3.99)  Time: 0.802s, 1276.36/s  (0.806s, 1269.85/s)  LR: 9.484e-04  Data: 0.011 (0.022)
Train: 44 [ 200/1251 ( 16%)]  Loss: 3.983 (3.99)  Time: 0.777s, 1318.48/s  (0.800s, 1279.53/s)  LR: 9.484e-04  Data: 0.011 (0.020)
Train: 44 [ 250/1251 ( 20%)]  Loss: 4.063 (4.00)  Time: 0.785s, 1304.38/s  (0.797s, 1284.77/s)  LR: 9.484e-04  Data: 0.011 (0.018)
Train: 44 [ 300/1251 ( 24%)]  Loss: 4.113 (4.02)  Time: 0.791s, 1294.55/s  (0.795s, 1287.41/s)  LR: 9.484e-04  Data: 0.012 (0.017)
Train: 44 [ 350/1251 ( 28%)]  Loss: 4.091 (4.02)  Time: 0.841s, 1217.05/s  (0.797s, 1285.02/s)  LR: 9.484e-04  Data: 0.012 (0.016)
Train: 44 [ 400/1251 ( 32%)]  Loss: 4.014 (4.02)  Time: 0.791s, 1295.35/s  (0.798s, 1283.00/s)  LR: 9.484e-04  Data: 0.015 (0.015)
Train: 44 [ 450/1251 ( 36%)]  Loss: 3.816 (4.00)  Time: 0.782s, 1309.54/s  (0.798s, 1282.88/s)  LR: 9.484e-04  Data: 0.011 (0.015)
Train: 44 [ 500/1251 ( 40%)]  Loss: 4.105 (4.01)  Time: 0.812s, 1260.71/s  (0.799s, 1282.08/s)  LR: 9.484e-04  Data: 0.011 (0.015)
Train: 44 [ 550/1251 ( 44%)]  Loss: 4.591 (4.06)  Time: 0.779s, 1314.05/s  (0.799s, 1281.07/s)  LR: 9.484e-04  Data: 0.012 (0.014)
Train: 44 [ 600/1251 ( 48%)]  Loss: 4.390 (4.09)  Time: 0.812s, 1260.83/s  (0.799s, 1281.59/s)  LR: 9.484e-04  Data: 0.011 (0.014)
Train: 44 [ 650/1251 ( 52%)]  Loss: 4.366 (4.11)  Time: 0.834s, 1227.79/s  (0.799s, 1281.11/s)  LR: 9.484e-04  Data: 0.011 (0.014)
Train: 44 [ 700/1251 ( 56%)]  Loss: 4.266 (4.12)  Time: 0.778s, 1316.75/s  (0.798s, 1282.42/s)  LR: 9.484e-04  Data: 0.010 (0.014)
Train: 44 [ 750/1251 ( 60%)]  Loss: 4.382 (4.13)  Time: 0.812s, 1261.11/s  (0.800s, 1279.98/s)  LR: 9.484e-04  Data: 0.012 (0.013)
Train: 44 [ 800/1251 ( 64%)]  Loss: 4.220 (4.14)  Time: 0.839s, 1220.67/s  (0.800s, 1279.72/s)  LR: 9.484e-04  Data: 0.011 (0.013)
Train: 44 [ 850/1251 ( 68%)]  Loss: 4.381 (4.15)  Time: 0.786s, 1302.27/s  (0.799s, 1280.98/s)  LR: 9.484e-04  Data: 0.011 (0.013)
Train: 44 [ 900/1251 ( 72%)]  Loss: 3.931 (4.14)  Time: 0.779s, 1313.79/s  (0.798s, 1282.41/s)  LR: 9.484e-04  Data: 0.011 (0.013)
Train: 44 [ 950/1251 ( 76%)]  Loss: 4.180 (4.14)  Time: 0.778s, 1315.38/s  (0.798s, 1283.22/s)  LR: 9.484e-04  Data: 0.011 (0.013)
Train: 44 [1000/1251 ( 80%)]  Loss: 4.272 (4.15)  Time: 0.829s, 1234.55/s  (0.798s, 1283.94/s)  LR: 9.484e-04  Data: 0.011 (0.013)
Train: 44 [1050/1251 ( 84%)]  Loss: 4.141 (4.15)  Time: 0.778s, 1315.69/s  (0.798s, 1283.49/s)  LR: 9.484e-04  Data: 0.011 (0.013)
Train: 44 [1100/1251 ( 88%)]  Loss: 4.163 (4.15)  Time: 0.845s, 1211.84/s  (0.797s, 1284.27/s)  LR: 9.484e-04  Data: 0.011 (0.013)
Train: 44 [1150/1251 ( 92%)]  Loss: 3.935 (4.14)  Time: 0.778s, 1316.36/s  (0.797s, 1285.07/s)  LR: 9.484e-04  Data: 0.011 (0.013)
Train: 44 [1200/1251 ( 96%)]  Loss: 3.487 (4.11)  Time: 0.817s, 1253.84/s  (0.797s, 1284.78/s)  LR: 9.484e-04  Data: 0.011 (0.013)
Train: 44 [1250/1251 (100%)]  Loss: 4.000 (4.11)  Time: 0.779s, 1313.74/s  (0.797s, 1284.53/s)  LR: 9.484e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.499 (1.499)  Loss:  1.0122 (1.0122)  Acc@1: 83.4961 (83.4961)  Acc@5: 95.4102 (95.4102)
Test: [  48/48]  Time: 0.172 (0.558)  Loss:  1.0143 (1.7334)  Acc@1: 81.2500 (65.0720)  Acc@5: 95.0472 (86.8520)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-41.pth.tar', 65.71600002197266)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-42.pth.tar', 65.20000010742187)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-44.pth.tar', 65.072)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-36.pth.tar', 65.06799994628906)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-43.pth.tar', 65.02800020996094)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-39.pth.tar', 65.00999998535156)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-40.pth.tar', 64.90599993408203)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-37.pth.tar', 64.59799990966796)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-35.pth.tar', 64.27400000244141)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-38.pth.tar', 63.94200008300781)

Train: 45 [   0/1251 (  0%)]  Loss: 4.004 (4.00)  Time: 2.276s,  449.87/s  (2.276s,  449.87/s)  LR: 9.460e-04  Data: 1.496 (1.496)
Train: 45 [  50/1251 (  4%)]  Loss: 3.715 (3.86)  Time: 0.781s, 1311.79/s  (0.839s, 1219.99/s)  LR: 9.460e-04  Data: 0.011 (0.043)
Train: 45 [ 100/1251 (  8%)]  Loss: 3.988 (3.90)  Time: 0.779s, 1313.83/s  (0.818s, 1251.12/s)  LR: 9.460e-04  Data: 0.012 (0.027)
Train: 45 [ 150/1251 ( 12%)]  Loss: 3.937 (3.91)  Time: 0.819s, 1250.68/s  (0.811s, 1262.49/s)  LR: 9.460e-04  Data: 0.010 (0.022)
Train: 45 [ 200/1251 ( 16%)]  Loss: 4.061 (3.94)  Time: 0.776s, 1319.28/s  (0.809s, 1266.07/s)  LR: 9.460e-04  Data: 0.011 (0.019)
Train: 45 [ 250/1251 ( 20%)]  Loss: 3.868 (3.93)  Time: 0.821s, 1246.95/s  (0.805s, 1272.24/s)  LR: 9.460e-04  Data: 0.011 (0.018)
Train: 45 [ 300/1251 ( 24%)]  Loss: 4.085 (3.95)  Time: 0.778s, 1316.31/s  (0.803s, 1275.27/s)  LR: 9.460e-04  Data: 0.011 (0.017)
Train: 45 [ 350/1251 ( 28%)]  Loss: 4.013 (3.96)  Time: 0.812s, 1260.39/s  (0.802s, 1276.25/s)  LR: 9.460e-04  Data: 0.010 (0.016)
Train: 45 [ 400/1251 ( 32%)]  Loss: 3.978 (3.96)  Time: 0.790s, 1295.78/s  (0.802s, 1277.19/s)  LR: 9.460e-04  Data: 0.011 (0.015)
Train: 45 [ 450/1251 ( 36%)]  Loss: 4.221 (3.99)  Time: 0.778s, 1315.94/s  (0.800s, 1280.35/s)  LR: 9.460e-04  Data: 0.010 (0.015)
Train: 45 [ 500/1251 ( 40%)]  Loss: 4.312 (4.02)  Time: 0.781s, 1311.58/s  (0.798s, 1283.16/s)  LR: 9.460e-04  Data: 0.010 (0.014)
Train: 45 [ 550/1251 ( 44%)]  Loss: 3.842 (4.00)  Time: 0.778s, 1315.48/s  (0.797s, 1285.31/s)  LR: 9.460e-04  Data: 0.011 (0.014)
Train: 45 [ 600/1251 ( 48%)]  Loss: 3.926 (4.00)  Time: 0.780s, 1313.11/s  (0.795s, 1287.40/s)  LR: 9.460e-04  Data: 0.011 (0.014)
Train: 45 [ 650/1251 ( 52%)]  Loss: 4.095 (4.00)  Time: 0.779s, 1314.92/s  (0.795s, 1288.40/s)  LR: 9.460e-04  Data: 0.010 (0.014)
Train: 45 [ 700/1251 ( 56%)]  Loss: 3.999 (4.00)  Time: 0.779s, 1313.94/s  (0.794s, 1288.99/s)  LR: 9.460e-04  Data: 0.010 (0.013)
Train: 45 [ 750/1251 ( 60%)]  Loss: 3.991 (4.00)  Time: 0.802s, 1276.49/s  (0.794s, 1289.84/s)  LR: 9.460e-04  Data: 0.011 (0.013)
Train: 45 [ 800/1251 ( 64%)]  Loss: 4.208 (4.01)  Time: 0.778s, 1316.75/s  (0.793s, 1291.14/s)  LR: 9.460e-04  Data: 0.010 (0.013)
Train: 45 [ 850/1251 ( 68%)]  Loss: 3.563 (3.99)  Time: 0.780s, 1312.72/s  (0.792s, 1292.43/s)  LR: 9.460e-04  Data: 0.011 (0.013)
Train: 45 [ 900/1251 ( 72%)]  Loss: 4.054 (3.99)  Time: 0.786s, 1302.17/s  (0.792s, 1293.36/s)  LR: 9.460e-04  Data: 0.011 (0.013)
Train: 45 [ 950/1251 ( 76%)]  Loss: 4.333 (4.01)  Time: 0.831s, 1232.48/s  (0.791s, 1293.95/s)  LR: 9.460e-04  Data: 0.011 (0.013)
Train: 45 [1000/1251 ( 80%)]  Loss: 4.201 (4.02)  Time: 0.813s, 1258.82/s  (0.791s, 1293.99/s)  LR: 9.460e-04  Data: 0.011 (0.013)
Train: 45 [1050/1251 ( 84%)]  Loss: 4.332 (4.03)  Time: 0.789s, 1297.99/s  (0.791s, 1294.14/s)  LR: 9.460e-04  Data: 0.011 (0.013)
Train: 45 [1100/1251 ( 88%)]  Loss: 4.089 (4.04)  Time: 0.779s, 1314.11/s  (0.791s, 1294.44/s)  LR: 9.460e-04  Data: 0.011 (0.012)
Train: 45 [1150/1251 ( 92%)]  Loss: 4.237 (4.04)  Time: 0.780s, 1313.12/s  (0.791s, 1295.09/s)  LR: 9.460e-04  Data: 0.010 (0.012)
Train: 45 [1200/1251 ( 96%)]  Loss: 4.489 (4.06)  Time: 0.805s, 1271.30/s  (0.791s, 1294.64/s)  LR: 9.460e-04  Data: 0.010 (0.012)
Train: 45 [1250/1251 (100%)]  Loss: 3.801 (4.05)  Time: 0.817s, 1252.97/s  (0.791s, 1294.00/s)  LR: 9.460e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.555 (1.555)  Loss:  1.1432 (1.1432)  Acc@1: 82.9102 (82.9102)  Acc@5: 94.7266 (94.7266)
Test: [  48/48]  Time: 0.172 (0.559)  Loss:  1.1634 (1.7018)  Acc@1: 79.8349 (66.6820)  Acc@5: 93.3962 (87.6900)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-45.pth.tar', 66.68200003173828)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-41.pth.tar', 65.71600002197266)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-42.pth.tar', 65.20000010742187)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-44.pth.tar', 65.072)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-36.pth.tar', 65.06799994628906)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-43.pth.tar', 65.02800020996094)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-39.pth.tar', 65.00999998535156)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-40.pth.tar', 64.90599993408203)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-37.pth.tar', 64.59799990966796)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-35.pth.tar', 64.27400000244141)

Train: 46 [   0/1251 (  0%)]  Loss: 4.096 (4.10)  Time: 2.258s,  453.50/s  (2.258s,  453.50/s)  LR: 9.437e-04  Data: 1.462 (1.462)
Train: 46 [  50/1251 (  4%)]  Loss: 4.287 (4.19)  Time: 0.803s, 1275.95/s  (0.824s, 1243.04/s)  LR: 9.437e-04  Data: 0.011 (0.043)
Train: 46 [ 100/1251 (  8%)]  Loss: 3.807 (4.06)  Time: 0.783s, 1307.34/s  (0.807s, 1268.37/s)  LR: 9.437e-04  Data: 0.011 (0.027)
Train: 46 [ 150/1251 ( 12%)]  Loss: 3.653 (3.96)  Time: 0.776s, 1319.00/s  (0.801s, 1277.61/s)  LR: 9.437e-04  Data: 0.011 (0.022)
Train: 46 [ 200/1251 ( 16%)]  Loss: 3.883 (3.95)  Time: 0.780s, 1312.00/s  (0.797s, 1284.91/s)  LR: 9.437e-04  Data: 0.011 (0.019)
Train: 46 [ 250/1251 ( 20%)]  Loss: 3.857 (3.93)  Time: 0.781s, 1311.21/s  (0.794s, 1290.27/s)  LR: 9.437e-04  Data: 0.011 (0.017)
Train: 46 [ 300/1251 ( 24%)]  Loss: 3.626 (3.89)  Time: 0.779s, 1315.05/s  (0.791s, 1293.99/s)  LR: 9.437e-04  Data: 0.011 (0.016)
Train: 46 [ 350/1251 ( 28%)]  Loss: 3.945 (3.89)  Time: 0.836s, 1224.65/s  (0.792s, 1293.71/s)  LR: 9.437e-04  Data: 0.011 (0.016)
Train: 46 [ 400/1251 ( 32%)]  Loss: 3.546 (3.86)  Time: 0.776s, 1318.94/s  (0.792s, 1292.54/s)  LR: 9.437e-04  Data: 0.011 (0.015)
Train: 46 [ 450/1251 ( 36%)]  Loss: 4.414 (3.91)  Time: 0.775s, 1321.65/s  (0.793s, 1291.79/s)  LR: 9.437e-04  Data: 0.010 (0.015)
Train: 46 [ 500/1251 ( 40%)]  Loss: 3.865 (3.91)  Time: 0.790s, 1295.40/s  (0.792s, 1293.30/s)  LR: 9.437e-04  Data: 0.011 (0.014)
Train: 46 [ 550/1251 ( 44%)]  Loss: 4.175 (3.93)  Time: 0.778s, 1316.05/s  (0.792s, 1293.23/s)  LR: 9.437e-04  Data: 0.011 (0.014)
Train: 46 [ 600/1251 ( 48%)]  Loss: 4.129 (3.94)  Time: 0.843s, 1214.00/s  (0.793s, 1291.08/s)  LR: 9.437e-04  Data: 0.011 (0.014)
Train: 46 [ 650/1251 ( 52%)]  Loss: 4.370 (3.98)  Time: 0.784s, 1305.62/s  (0.793s, 1291.97/s)  LR: 9.437e-04  Data: 0.010 (0.013)
Train: 46 [ 700/1251 ( 56%)]  Loss: 4.201 (3.99)  Time: 0.883s, 1159.79/s  (0.793s, 1291.37/s)  LR: 9.437e-04  Data: 0.010 (0.013)
Train: 46 [ 750/1251 ( 60%)]  Loss: 4.100 (4.00)  Time: 0.782s, 1309.55/s  (0.793s, 1291.39/s)  LR: 9.437e-04  Data: 0.012 (0.013)
Train: 46 [ 800/1251 ( 64%)]  Loss: 4.069 (4.00)  Time: 0.780s, 1312.21/s  (0.793s, 1290.59/s)  LR: 9.437e-04  Data: 0.011 (0.013)
Train: 46 [ 850/1251 ( 68%)]  Loss: 4.368 (4.02)  Time: 0.792s, 1292.90/s  (0.793s, 1291.55/s)  LR: 9.437e-04  Data: 0.011 (0.013)
Train: 46 [ 900/1251 ( 72%)]  Loss: 4.027 (4.02)  Time: 0.812s, 1261.48/s  (0.793s, 1291.59/s)  LR: 9.437e-04  Data: 0.011 (0.013)
Train: 46 [ 950/1251 ( 76%)]  Loss: 4.019 (4.02)  Time: 0.825s, 1240.89/s  (0.794s, 1290.16/s)  LR: 9.437e-04  Data: 0.011 (0.013)
Train: 46 [1000/1251 ( 80%)]  Loss: 4.237 (4.03)  Time: 0.840s, 1219.13/s  (0.794s, 1290.33/s)  LR: 9.437e-04  Data: 0.011 (0.013)
Train: 46 [1050/1251 ( 84%)]  Loss: 3.781 (4.02)  Time: 0.843s, 1215.34/s  (0.794s, 1289.51/s)  LR: 9.437e-04  Data: 0.011 (0.013)
Train: 46 [1100/1251 ( 88%)]  Loss: 4.170 (4.03)  Time: 0.815s, 1257.14/s  (0.794s, 1289.06/s)  LR: 9.437e-04  Data: 0.012 (0.012)
Train: 46 [1150/1251 ( 92%)]  Loss: 4.303 (4.04)  Time: 0.775s, 1320.46/s  (0.794s, 1288.99/s)  LR: 9.437e-04  Data: 0.011 (0.012)
Train: 46 [1200/1251 ( 96%)]  Loss: 3.836 (4.03)  Time: 0.778s, 1316.70/s  (0.794s, 1289.13/s)  LR: 9.437e-04  Data: 0.011 (0.012)
Train: 46 [1250/1251 (100%)]  Loss: 4.126 (4.03)  Time: 0.796s, 1287.03/s  (0.794s, 1289.22/s)  LR: 9.437e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.503 (1.503)  Loss:  0.9292 (0.9292)  Acc@1: 84.2773 (84.2773)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.172 (0.564)  Loss:  1.0540 (1.7037)  Acc@1: 80.7783 (65.8560)  Acc@5: 94.3396 (87.1800)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-45.pth.tar', 66.68200003173828)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-46.pth.tar', 65.85599992431641)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-41.pth.tar', 65.71600002197266)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-42.pth.tar', 65.20000010742187)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-44.pth.tar', 65.072)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-36.pth.tar', 65.06799994628906)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-43.pth.tar', 65.02800020996094)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-39.pth.tar', 65.00999998535156)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-40.pth.tar', 64.90599993408203)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-37.pth.tar', 64.59799990966796)

Train: 47 [   0/1251 (  0%)]  Loss: 3.903 (3.90)  Time: 2.196s,  466.27/s  (2.196s,  466.27/s)  LR: 9.412e-04  Data: 1.458 (1.458)
Train: 47 [  50/1251 (  4%)]  Loss: 4.200 (4.05)  Time: 0.782s, 1308.79/s  (0.847s, 1208.73/s)  LR: 9.412e-04  Data: 0.014 (0.044)
Train: 47 [ 100/1251 (  8%)]  Loss: 3.631 (3.91)  Time: 0.777s, 1317.57/s  (0.816s, 1254.67/s)  LR: 9.412e-04  Data: 0.010 (0.028)
Train: 47 [ 150/1251 ( 12%)]  Loss: 3.823 (3.89)  Time: 0.775s, 1321.72/s  (0.805s, 1272.00/s)  LR: 9.412e-04  Data: 0.010 (0.022)
Train: 47 [ 200/1251 ( 16%)]  Loss: 4.033 (3.92)  Time: 0.777s, 1317.24/s  (0.802s, 1276.05/s)  LR: 9.412e-04  Data: 0.011 (0.019)
Train: 47 [ 250/1251 ( 20%)]  Loss: 4.049 (3.94)  Time: 0.777s, 1317.59/s  (0.799s, 1282.39/s)  LR: 9.412e-04  Data: 0.012 (0.018)
Train: 47 [ 300/1251 ( 24%)]  Loss: 3.967 (3.94)  Time: 0.777s, 1317.17/s  (0.798s, 1284.01/s)  LR: 9.412e-04  Data: 0.011 (0.017)
Train: 47 [ 350/1251 ( 28%)]  Loss: 3.927 (3.94)  Time: 0.836s, 1225.32/s  (0.800s, 1280.06/s)  LR: 9.412e-04  Data: 0.012 (0.016)
Train: 47 [ 400/1251 ( 32%)]  Loss: 4.018 (3.95)  Time: 0.796s, 1287.08/s  (0.800s, 1279.22/s)  LR: 9.412e-04  Data: 0.012 (0.015)
Train: 47 [ 450/1251 ( 36%)]  Loss: 4.235 (3.98)  Time: 0.786s, 1302.91/s  (0.801s, 1279.18/s)  LR: 9.412e-04  Data: 0.011 (0.015)
Train: 47 [ 500/1251 ( 40%)]  Loss: 3.821 (3.96)  Time: 0.849s, 1206.43/s  (0.799s, 1281.62/s)  LR: 9.412e-04  Data: 0.011 (0.015)
Train: 47 [ 550/1251 ( 44%)]  Loss: 4.532 (4.01)  Time: 0.777s, 1318.07/s  (0.798s, 1283.62/s)  LR: 9.412e-04  Data: 0.011 (0.014)
Train: 47 [ 600/1251 ( 48%)]  Loss: 4.379 (4.04)  Time: 0.786s, 1303.52/s  (0.797s, 1285.24/s)  LR: 9.412e-04  Data: 0.012 (0.014)
Train: 47 [ 650/1251 ( 52%)]  Loss: 4.085 (4.04)  Time: 0.833s, 1229.19/s  (0.797s, 1285.41/s)  LR: 9.412e-04  Data: 0.011 (0.014)
Train: 47 [ 700/1251 ( 56%)]  Loss: 3.946 (4.04)  Time: 0.778s, 1316.03/s  (0.797s, 1285.09/s)  LR: 9.412e-04  Data: 0.011 (0.014)
Train: 47 [ 750/1251 ( 60%)]  Loss: 4.328 (4.05)  Time: 0.777s, 1318.48/s  (0.797s, 1285.27/s)  LR: 9.412e-04  Data: 0.010 (0.013)
Train: 47 [ 800/1251 ( 64%)]  Loss: 4.415 (4.08)  Time: 0.813s, 1258.92/s  (0.796s, 1285.84/s)  LR: 9.412e-04  Data: 0.010 (0.013)
Train: 47 [ 850/1251 ( 68%)]  Loss: 3.937 (4.07)  Time: 0.787s, 1301.77/s  (0.796s, 1286.44/s)  LR: 9.412e-04  Data: 0.010 (0.013)
Train: 47 [ 900/1251 ( 72%)]  Loss: 3.659 (4.05)  Time: 0.831s, 1232.45/s  (0.796s, 1286.86/s)  LR: 9.412e-04  Data: 0.011 (0.013)
Train: 47 [ 950/1251 ( 76%)]  Loss: 4.222 (4.06)  Time: 0.777s, 1318.49/s  (0.795s, 1287.54/s)  LR: 9.412e-04  Data: 0.011 (0.013)
Train: 47 [1000/1251 ( 80%)]  Loss: 4.003 (4.05)  Time: 0.779s, 1314.27/s  (0.795s, 1287.37/s)  LR: 9.412e-04  Data: 0.011 (0.013)
Train: 47 [1050/1251 ( 84%)]  Loss: 4.171 (4.06)  Time: 0.781s, 1311.41/s  (0.795s, 1288.35/s)  LR: 9.412e-04  Data: 0.010 (0.013)
Train: 47 [1100/1251 ( 88%)]  Loss: 3.881 (4.05)  Time: 0.777s, 1317.19/s  (0.794s, 1289.13/s)  LR: 9.412e-04  Data: 0.011 (0.013)
Train: 47 [1150/1251 ( 92%)]  Loss: 4.309 (4.06)  Time: 0.782s, 1309.96/s  (0.794s, 1289.42/s)  LR: 9.412e-04  Data: 0.011 (0.013)
Train: 47 [1200/1251 ( 96%)]  Loss: 4.257 (4.07)  Time: 0.778s, 1316.50/s  (0.794s, 1290.20/s)  LR: 9.412e-04  Data: 0.010 (0.012)
Train: 47 [1250/1251 (100%)]  Loss: 4.121 (4.07)  Time: 0.770s, 1330.36/s  (0.793s, 1290.74/s)  LR: 9.412e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.570 (1.570)  Loss:  1.0421 (1.0421)  Acc@1: 83.0078 (83.0078)  Acc@5: 94.5312 (94.5312)
Test: [  48/48]  Time: 0.172 (0.568)  Loss:  1.1928 (1.6934)  Acc@1: 80.3066 (66.1880)  Acc@5: 92.4528 (87.4240)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-45.pth.tar', 66.68200003173828)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-47.pth.tar', 66.18800010742187)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-46.pth.tar', 65.85599992431641)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-41.pth.tar', 65.71600002197266)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-42.pth.tar', 65.20000010742187)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-44.pth.tar', 65.072)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-36.pth.tar', 65.06799994628906)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-43.pth.tar', 65.02800020996094)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-39.pth.tar', 65.00999998535156)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-40.pth.tar', 64.90599993408203)

Train: 48 [   0/1251 (  0%)]  Loss: 3.796 (3.80)  Time: 2.376s,  430.95/s  (2.376s,  430.95/s)  LR: 9.388e-04  Data: 1.642 (1.642)
Train: 48 [  50/1251 (  4%)]  Loss: 3.687 (3.74)  Time: 0.779s, 1315.00/s  (0.827s, 1238.81/s)  LR: 9.388e-04  Data: 0.010 (0.048)
Train: 48 [ 100/1251 (  8%)]  Loss: 4.362 (3.95)  Time: 0.779s, 1314.06/s  (0.809s, 1265.46/s)  LR: 9.388e-04  Data: 0.011 (0.030)
Train: 48 [ 150/1251 ( 12%)]  Loss: 4.211 (4.01)  Time: 0.794s, 1289.57/s  (0.802s, 1276.87/s)  LR: 9.388e-04  Data: 0.011 (0.023)
Train: 48 [ 200/1251 ( 16%)]  Loss: 3.953 (4.00)  Time: 0.786s, 1302.27/s  (0.798s, 1282.95/s)  LR: 9.388e-04  Data: 0.011 (0.020)
Train: 48 [ 250/1251 ( 20%)]  Loss: 3.910 (3.99)  Time: 0.779s, 1314.98/s  (0.797s, 1284.31/s)  LR: 9.388e-04  Data: 0.010 (0.018)
Train: 48 [ 300/1251 ( 24%)]  Loss: 4.220 (4.02)  Time: 0.844s, 1212.77/s  (0.801s, 1279.18/s)  LR: 9.388e-04  Data: 0.010 (0.017)
Train: 48 [ 350/1251 ( 28%)]  Loss: 3.622 (3.97)  Time: 0.783s, 1308.26/s  (0.799s, 1281.85/s)  LR: 9.388e-04  Data: 0.011 (0.016)
Train: 48 [ 400/1251 ( 32%)]  Loss: 4.104 (3.99)  Time: 0.781s, 1310.77/s  (0.797s, 1284.70/s)  LR: 9.388e-04  Data: 0.010 (0.016)
Train: 48 [ 450/1251 ( 36%)]  Loss: 4.529 (4.04)  Time: 0.778s, 1315.69/s  (0.795s, 1287.68/s)  LR: 9.388e-04  Data: 0.011 (0.015)
Train: 48 [ 500/1251 ( 40%)]  Loss: 4.045 (4.04)  Time: 0.780s, 1312.58/s  (0.794s, 1289.84/s)  LR: 9.388e-04  Data: 0.010 (0.015)
Train: 48 [ 550/1251 ( 44%)]  Loss: 3.792 (4.02)  Time: 0.778s, 1316.91/s  (0.794s, 1288.89/s)  LR: 9.388e-04  Data: 0.011 (0.014)
Train: 48 [ 600/1251 ( 48%)]  Loss: 3.788 (4.00)  Time: 0.779s, 1315.14/s  (0.794s, 1289.17/s)  LR: 9.388e-04  Data: 0.011 (0.014)
Train: 48 [ 650/1251 ( 52%)]  Loss: 3.931 (4.00)  Time: 0.779s, 1313.81/s  (0.794s, 1289.92/s)  LR: 9.388e-04  Data: 0.011 (0.014)
Train: 48 [ 700/1251 ( 56%)]  Loss: 4.372 (4.02)  Time: 0.780s, 1313.19/s  (0.793s, 1290.78/s)  LR: 9.388e-04  Data: 0.011 (0.014)
Train: 48 [ 750/1251 ( 60%)]  Loss: 4.408 (4.05)  Time: 0.782s, 1309.19/s  (0.794s, 1289.33/s)  LR: 9.388e-04  Data: 0.011 (0.013)
Train: 48 [ 800/1251 ( 64%)]  Loss: 4.124 (4.05)  Time: 0.795s, 1288.45/s  (0.795s, 1288.81/s)  LR: 9.388e-04  Data: 0.011 (0.013)
Train: 48 [ 850/1251 ( 68%)]  Loss: 4.389 (4.07)  Time: 0.780s, 1312.46/s  (0.794s, 1289.03/s)  LR: 9.388e-04  Data: 0.011 (0.013)
Train: 48 [ 900/1251 ( 72%)]  Loss: 4.049 (4.07)  Time: 0.793s, 1292.07/s  (0.794s, 1290.21/s)  LR: 9.388e-04  Data: 0.011 (0.013)
Train: 48 [ 950/1251 ( 76%)]  Loss: 4.076 (4.07)  Time: 0.780s, 1313.56/s  (0.793s, 1290.92/s)  LR: 9.388e-04  Data: 0.011 (0.013)
Train: 48 [1000/1251 ( 80%)]  Loss: 4.018 (4.07)  Time: 0.781s, 1311.82/s  (0.793s, 1291.60/s)  LR: 9.388e-04  Data: 0.010 (0.013)
Train: 48 [1050/1251 ( 84%)]  Loss: 3.837 (4.06)  Time: 0.790s, 1296.35/s  (0.792s, 1292.16/s)  LR: 9.388e-04  Data: 0.010 (0.013)
Train: 48 [1100/1251 ( 88%)]  Loss: 4.142 (4.06)  Time: 0.853s, 1200.30/s  (0.793s, 1291.87/s)  LR: 9.388e-04  Data: 0.010 (0.013)
Train: 48 [1150/1251 ( 92%)]  Loss: 4.041 (4.06)  Time: 0.779s, 1314.49/s  (0.793s, 1290.79/s)  LR: 9.388e-04  Data: 0.011 (0.013)
Train: 48 [1200/1251 ( 96%)]  Loss: 4.166 (4.06)  Time: 0.840s, 1219.52/s  (0.793s, 1291.36/s)  LR: 9.388e-04  Data: 0.011 (0.013)
Train: 48 [1250/1251 (100%)]  Loss: 3.963 (4.06)  Time: 0.769s, 1330.82/s  (0.793s, 1292.02/s)  LR: 9.388e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.514 (1.514)  Loss:  1.0557 (1.0557)  Acc@1: 83.6914 (83.6914)  Acc@5: 95.1172 (95.1172)
Test: [  48/48]  Time: 0.172 (0.561)  Loss:  1.0652 (1.7081)  Acc@1: 79.3632 (65.5840)  Acc@5: 94.8113 (87.1260)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-45.pth.tar', 66.68200003173828)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-47.pth.tar', 66.18800010742187)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-46.pth.tar', 65.85599992431641)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-41.pth.tar', 65.71600002197266)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-48.pth.tar', 65.58399995605468)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-42.pth.tar', 65.20000010742187)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-44.pth.tar', 65.072)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-36.pth.tar', 65.06799994628906)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-43.pth.tar', 65.02800020996094)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-39.pth.tar', 65.00999998535156)

Train: 49 [   0/1251 (  0%)]  Loss: 3.952 (3.95)  Time: 2.337s,  438.20/s  (2.337s,  438.20/s)  LR: 9.363e-04  Data: 1.602 (1.602)
Train: 49 [  50/1251 (  4%)]  Loss: 3.607 (3.78)  Time: 0.779s, 1314.97/s  (0.828s, 1237.05/s)  LR: 9.363e-04  Data: 0.011 (0.046)
Train: 49 [ 100/1251 (  8%)]  Loss: 4.246 (3.93)  Time: 0.787s, 1301.13/s  (0.808s, 1267.90/s)  LR: 9.363e-04  Data: 0.011 (0.029)
Train: 49 [ 150/1251 ( 12%)]  Loss: 3.993 (3.95)  Time: 0.778s, 1315.64/s  (0.800s, 1280.14/s)  LR: 9.363e-04  Data: 0.011 (0.023)
Train: 49 [ 200/1251 ( 16%)]  Loss: 4.033 (3.97)  Time: 0.776s, 1318.84/s  (0.799s, 1281.85/s)  LR: 9.363e-04  Data: 0.011 (0.020)
Train: 49 [ 250/1251 ( 20%)]  Loss: 3.868 (3.95)  Time: 0.820s, 1248.71/s  (0.800s, 1280.62/s)  LR: 9.363e-04  Data: 0.012 (0.018)
Train: 49 [ 300/1251 ( 24%)]  Loss: 4.133 (3.98)  Time: 0.783s, 1307.85/s  (0.798s, 1282.43/s)  LR: 9.363e-04  Data: 0.011 (0.017)
Train: 49 [ 350/1251 ( 28%)]  Loss: 4.186 (4.00)  Time: 0.778s, 1316.35/s  (0.799s, 1281.76/s)  LR: 9.363e-04  Data: 0.011 (0.016)
Train: 49 [ 400/1251 ( 32%)]  Loss: 3.943 (4.00)  Time: 0.780s, 1313.37/s  (0.797s, 1284.72/s)  LR: 9.363e-04  Data: 0.011 (0.015)
Train: 49 [ 450/1251 ( 36%)]  Loss: 4.068 (4.00)  Time: 0.779s, 1314.92/s  (0.796s, 1286.49/s)  LR: 9.363e-04  Data: 0.010 (0.015)
Train: 49 [ 500/1251 ( 40%)]  Loss: 3.612 (3.97)  Time: 0.819s, 1249.87/s  (0.795s, 1287.52/s)  LR: 9.363e-04  Data: 0.012 (0.015)
Train: 49 [ 550/1251 ( 44%)]  Loss: 3.970 (3.97)  Time: 0.823s, 1243.89/s  (0.796s, 1286.35/s)  LR: 9.363e-04  Data: 0.011 (0.014)
Train: 49 [ 600/1251 ( 48%)]  Loss: 4.130 (3.98)  Time: 0.779s, 1314.83/s  (0.795s, 1287.41/s)  LR: 9.363e-04  Data: 0.011 (0.014)
Train: 49 [ 650/1251 ( 52%)]  Loss: 4.167 (3.99)  Time: 0.785s, 1304.54/s  (0.794s, 1289.07/s)  LR: 9.363e-04  Data: 0.010 (0.014)
Train: 49 [ 700/1251 ( 56%)]  Loss: 4.313 (4.01)  Time: 0.787s, 1301.47/s  (0.794s, 1290.02/s)  LR: 9.363e-04  Data: 0.011 (0.014)
Train: 49 [ 750/1251 ( 60%)]  Loss: 4.129 (4.02)  Time: 0.778s, 1315.70/s  (0.793s, 1290.91/s)  LR: 9.363e-04  Data: 0.011 (0.013)
Train: 49 [ 800/1251 ( 64%)]  Loss: 4.181 (4.03)  Time: 0.778s, 1316.19/s  (0.793s, 1291.53/s)  LR: 9.363e-04  Data: 0.011 (0.013)
Train: 49 [ 850/1251 ( 68%)]  Loss: 4.054 (4.03)  Time: 0.783s, 1307.64/s  (0.792s, 1292.14/s)  LR: 9.363e-04  Data: 0.010 (0.013)
Train: 49 [ 900/1251 ( 72%)]  Loss: 3.794 (4.02)  Time: 0.779s, 1314.97/s  (0.793s, 1291.65/s)  LR: 9.363e-04  Data: 0.010 (0.013)
Train: 49 [ 950/1251 ( 76%)]  Loss: 4.267 (4.03)  Time: 0.790s, 1296.74/s  (0.793s, 1291.52/s)  LR: 9.363e-04  Data: 0.011 (0.013)
Train: 49 [1000/1251 ( 80%)]  Loss: 4.031 (4.03)  Time: 0.811s, 1262.23/s  (0.793s, 1291.29/s)  LR: 9.363e-04  Data: 0.010 (0.013)
Train: 49 [1050/1251 ( 84%)]  Loss: 3.823 (4.02)  Time: 0.777s, 1317.56/s  (0.793s, 1291.37/s)  LR: 9.363e-04  Data: 0.010 (0.013)
Train: 49 [1100/1251 ( 88%)]  Loss: 4.243 (4.03)  Time: 0.814s, 1257.56/s  (0.793s, 1292.04/s)  LR: 9.363e-04  Data: 0.014 (0.013)
Train: 49 [1150/1251 ( 92%)]  Loss: 3.689 (4.02)  Time: 0.781s, 1311.89/s  (0.792s, 1292.76/s)  LR: 9.363e-04  Data: 0.011 (0.012)
Train: 49 [1200/1251 ( 96%)]  Loss: 4.102 (4.02)  Time: 0.786s, 1303.56/s  (0.792s, 1292.97/s)  LR: 9.363e-04  Data: 0.011 (0.012)
Train: 49 [1250/1251 (100%)]  Loss: 3.893 (4.02)  Time: 0.764s, 1340.34/s  (0.792s, 1293.57/s)  LR: 9.363e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.533 (1.533)  Loss:  1.0827 (1.0827)  Acc@1: 84.3750 (84.3750)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.172 (0.554)  Loss:  1.1255 (1.8623)  Acc@1: 81.1321 (66.0920)  Acc@5: 95.0472 (87.0660)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-45.pth.tar', 66.68200003173828)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-47.pth.tar', 66.18800010742187)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-49.pth.tar', 66.092000078125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-46.pth.tar', 65.85599992431641)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-41.pth.tar', 65.71600002197266)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-48.pth.tar', 65.58399995605468)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-42.pth.tar', 65.20000010742187)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-44.pth.tar', 65.072)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-36.pth.tar', 65.06799994628906)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-43.pth.tar', 65.02800020996094)

Train: 50 [   0/1251 (  0%)]  Loss: 4.324 (4.32)  Time: 2.311s,  443.06/s  (2.311s,  443.06/s)  LR: 9.337e-04  Data: 1.575 (1.575)
Train: 50 [  50/1251 (  4%)]  Loss: 4.197 (4.26)  Time: 0.840s, 1218.76/s  (0.839s, 1219.97/s)  LR: 9.337e-04  Data: 0.011 (0.044)
Train: 50 [ 100/1251 (  8%)]  Loss: 3.951 (4.16)  Time: 0.779s, 1314.73/s  (0.818s, 1252.51/s)  LR: 9.337e-04  Data: 0.011 (0.028)
Train: 50 [ 150/1251 ( 12%)]  Loss: 4.092 (4.14)  Time: 0.791s, 1294.40/s  (0.809s, 1265.60/s)  LR: 9.337e-04  Data: 0.010 (0.022)
Train: 50 [ 200/1251 ( 16%)]  Loss: 4.364 (4.19)  Time: 0.786s, 1302.36/s  (0.805s, 1272.46/s)  LR: 9.337e-04  Data: 0.011 (0.019)
Train: 50 [ 250/1251 ( 20%)]  Loss: 4.034 (4.16)  Time: 0.852s, 1201.21/s  (0.805s, 1271.86/s)  LR: 9.337e-04  Data: 0.013 (0.018)
Train: 50 [ 300/1251 ( 24%)]  Loss: 3.719 (4.10)  Time: 0.777s, 1318.18/s  (0.805s, 1271.42/s)  LR: 9.337e-04  Data: 0.012 (0.017)
Train: 50 [ 350/1251 ( 28%)]  Loss: 3.800 (4.06)  Time: 0.779s, 1315.17/s  (0.802s, 1277.25/s)  LR: 9.337e-04  Data: 0.010 (0.016)
Train: 50 [ 400/1251 ( 32%)]  Loss: 3.874 (4.04)  Time: 0.785s, 1304.77/s  (0.799s, 1280.99/s)  LR: 9.337e-04  Data: 0.011 (0.015)
Train: 50 [ 450/1251 ( 36%)]  Loss: 3.801 (4.02)  Time: 0.804s, 1274.41/s  (0.799s, 1282.10/s)  LR: 9.337e-04  Data: 0.011 (0.015)
Train: 50 [ 500/1251 ( 40%)]  Loss: 3.807 (4.00)  Time: 0.779s, 1315.20/s  (0.799s, 1282.21/s)  LR: 9.337e-04  Data: 0.011 (0.014)
Train: 50 [ 550/1251 ( 44%)]  Loss: 3.952 (3.99)  Time: 0.814s, 1258.03/s  (0.797s, 1284.38/s)  LR: 9.337e-04  Data: 0.010 (0.014)
Train: 50 [ 600/1251 ( 48%)]  Loss: 4.319 (4.02)  Time: 0.827s, 1238.52/s  (0.797s, 1284.25/s)  LR: 9.337e-04  Data: 0.011 (0.014)
Train: 50 [ 650/1251 ( 52%)]  Loss: 3.897 (4.01)  Time: 0.849s, 1206.72/s  (0.799s, 1280.83/s)  LR: 9.337e-04  Data: 0.014 (0.014)
Train: 50 [ 700/1251 ( 56%)]  Loss: 3.703 (3.99)  Time: 0.834s, 1228.20/s  (0.801s, 1278.38/s)  LR: 9.337e-04  Data: 0.010 (0.013)
Train: 50 [ 750/1251 ( 60%)]  Loss: 4.371 (4.01)  Time: 0.786s, 1302.38/s  (0.800s, 1279.87/s)  LR: 9.337e-04  Data: 0.011 (0.013)
Train: 50 [ 800/1251 ( 64%)]  Loss: 4.393 (4.04)  Time: 0.775s, 1320.56/s  (0.799s, 1281.10/s)  LR: 9.337e-04  Data: 0.011 (0.013)
Train: 50 [ 850/1251 ( 68%)]  Loss: 4.051 (4.04)  Time: 0.795s, 1288.00/s  (0.798s, 1282.72/s)  LR: 9.337e-04  Data: 0.011 (0.013)
Train: 50 [ 900/1251 ( 72%)]  Loss: 4.321 (4.05)  Time: 0.813s, 1259.50/s  (0.798s, 1283.71/s)  LR: 9.337e-04  Data: 0.011 (0.013)
Train: 50 [ 950/1251 ( 76%)]  Loss: 4.050 (4.05)  Time: 0.813s, 1259.77/s  (0.798s, 1283.76/s)  LR: 9.337e-04  Data: 0.011 (0.013)
Train: 50 [1000/1251 ( 80%)]  Loss: 4.016 (4.05)  Time: 0.798s, 1283.37/s  (0.799s, 1282.30/s)  LR: 9.337e-04  Data: 0.011 (0.013)
Train: 50 [1050/1251 ( 84%)]  Loss: 4.574 (4.07)  Time: 0.803s, 1274.97/s  (0.799s, 1282.33/s)  LR: 9.337e-04  Data: 0.010 (0.013)
Train: 50 [1100/1251 ( 88%)]  Loss: 3.871 (4.06)  Time: 0.811s, 1262.93/s  (0.799s, 1282.11/s)  LR: 9.337e-04  Data: 0.011 (0.013)
Train: 50 [1150/1251 ( 92%)]  Loss: 4.328 (4.08)  Time: 0.781s, 1310.83/s  (0.798s, 1282.51/s)  LR: 9.337e-04  Data: 0.010 (0.012)
Train: 50 [1200/1251 ( 96%)]  Loss: 3.872 (4.07)  Time: 0.820s, 1248.90/s  (0.798s, 1283.18/s)  LR: 9.337e-04  Data: 0.012 (0.012)
Train: 50 [1250/1251 (100%)]  Loss: 4.026 (4.07)  Time: 0.768s, 1333.89/s  (0.798s, 1283.04/s)  LR: 9.337e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.535 (1.535)  Loss:  1.0421 (1.0421)  Acc@1: 86.2305 (86.2305)  Acc@5: 96.0938 (96.0938)
Test: [  48/48]  Time: 0.172 (0.567)  Loss:  1.0284 (1.7001)  Acc@1: 82.0755 (67.3480)  Acc@5: 95.1651 (88.0580)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-50.pth.tar', 67.34800010009765)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-45.pth.tar', 66.68200003173828)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-47.pth.tar', 66.18800010742187)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-49.pth.tar', 66.092000078125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-46.pth.tar', 65.85599992431641)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-41.pth.tar', 65.71600002197266)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-48.pth.tar', 65.58399995605468)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-42.pth.tar', 65.20000010742187)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-44.pth.tar', 65.072)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-36.pth.tar', 65.06799994628906)

Train: 51 [   0/1251 (  0%)]  Loss: 4.308 (4.31)  Time: 2.377s,  430.81/s  (2.377s,  430.81/s)  LR: 9.311e-04  Data: 1.641 (1.641)
Train: 51 [  50/1251 (  4%)]  Loss: 4.106 (4.21)  Time: 0.788s, 1299.89/s  (0.830s, 1234.34/s)  LR: 9.311e-04  Data: 0.014 (0.051)
Train: 51 [ 100/1251 (  8%)]  Loss: 3.585 (4.00)  Time: 0.778s, 1315.40/s  (0.810s, 1264.41/s)  LR: 9.311e-04  Data: 0.011 (0.031)
Train: 51 [ 150/1251 ( 12%)]  Loss: 3.620 (3.90)  Time: 0.801s, 1278.47/s  (0.802s, 1276.60/s)  LR: 9.311e-04  Data: 0.012 (0.025)
Train: 51 [ 200/1251 ( 16%)]  Loss: 4.295 (3.98)  Time: 0.812s, 1261.76/s  (0.806s, 1270.42/s)  LR: 9.311e-04  Data: 0.011 (0.021)
Train: 51 [ 250/1251 ( 20%)]  Loss: 4.090 (4.00)  Time: 0.781s, 1310.86/s  (0.804s, 1273.12/s)  LR: 9.311e-04  Data: 0.011 (0.019)
Train: 51 [ 300/1251 ( 24%)]  Loss: 4.034 (4.01)  Time: 0.785s, 1304.79/s  (0.804s, 1273.73/s)  LR: 9.311e-04  Data: 0.010 (0.018)
Train: 51 [ 350/1251 ( 28%)]  Loss: 4.340 (4.05)  Time: 0.782s, 1309.45/s  (0.801s, 1278.31/s)  LR: 9.311e-04  Data: 0.012 (0.017)
Train: 51 [ 400/1251 ( 32%)]  Loss: 4.013 (4.04)  Time: 0.786s, 1303.08/s  (0.801s, 1278.40/s)  LR: 9.311e-04  Data: 0.011 (0.016)
Train: 51 [ 450/1251 ( 36%)]  Loss: 4.492 (4.09)  Time: 0.813s, 1259.29/s  (0.800s, 1280.25/s)  LR: 9.311e-04  Data: 0.011 (0.016)
Train: 51 [ 500/1251 ( 40%)]  Loss: 4.040 (4.08)  Time: 0.779s, 1313.92/s  (0.800s, 1280.65/s)  LR: 9.311e-04  Data: 0.010 (0.015)
Train: 51 [ 550/1251 ( 44%)]  Loss: 4.052 (4.08)  Time: 0.778s, 1315.40/s  (0.798s, 1282.58/s)  LR: 9.311e-04  Data: 0.011 (0.015)
Train: 51 [ 600/1251 ( 48%)]  Loss: 3.973 (4.07)  Time: 0.778s, 1316.42/s  (0.798s, 1283.27/s)  LR: 9.311e-04  Data: 0.011 (0.014)
Train: 51 [ 650/1251 ( 52%)]  Loss: 4.044 (4.07)  Time: 0.782s, 1308.87/s  (0.798s, 1283.81/s)  LR: 9.311e-04  Data: 0.012 (0.014)
Train: 51 [ 700/1251 ( 56%)]  Loss: 4.199 (4.08)  Time: 0.823s, 1244.45/s  (0.799s, 1281.61/s)  LR: 9.311e-04  Data: 0.012 (0.014)
Train: 51 [ 750/1251 ( 60%)]  Loss: 3.917 (4.07)  Time: 0.788s, 1298.94/s  (0.798s, 1282.41/s)  LR: 9.311e-04  Data: 0.011 (0.014)
Train: 51 [ 800/1251 ( 64%)]  Loss: 4.163 (4.07)  Time: 0.814s, 1258.58/s  (0.798s, 1282.73/s)  LR: 9.311e-04  Data: 0.011 (0.014)
Train: 51 [ 850/1251 ( 68%)]  Loss: 3.974 (4.07)  Time: 0.841s, 1217.89/s  (0.798s, 1283.80/s)  LR: 9.311e-04  Data: 0.011 (0.013)
Train: 51 [ 900/1251 ( 72%)]  Loss: 4.311 (4.08)  Time: 0.778s, 1316.55/s  (0.797s, 1284.71/s)  LR: 9.311e-04  Data: 0.012 (0.013)
Train: 51 [ 950/1251 ( 76%)]  Loss: 4.031 (4.08)  Time: 0.819s, 1250.07/s  (0.796s, 1285.73/s)  LR: 9.311e-04  Data: 0.011 (0.013)
Train: 51 [1000/1251 ( 80%)]  Loss: 3.895 (4.07)  Time: 0.781s, 1311.70/s  (0.796s, 1286.31/s)  LR: 9.311e-04  Data: 0.010 (0.013)
Train: 51 [1050/1251 ( 84%)]  Loss: 3.637 (4.05)  Time: 0.815s, 1256.17/s  (0.796s, 1286.54/s)  LR: 9.311e-04  Data: 0.011 (0.013)
Train: 51 [1100/1251 ( 88%)]  Loss: 4.249 (4.06)  Time: 0.842s, 1215.43/s  (0.797s, 1285.51/s)  LR: 9.311e-04  Data: 0.011 (0.013)
Train: 51 [1150/1251 ( 92%)]  Loss: 3.955 (4.06)  Time: 0.792s, 1293.24/s  (0.797s, 1284.48/s)  LR: 9.311e-04  Data: 0.011 (0.013)
Train: 51 [1200/1251 ( 96%)]  Loss: 3.982 (4.05)  Time: 0.779s, 1314.62/s  (0.797s, 1284.46/s)  LR: 9.311e-04  Data: 0.011 (0.013)
Train: 51 [1250/1251 (100%)]  Loss: 4.585 (4.07)  Time: 0.771s, 1328.02/s  (0.797s, 1285.24/s)  LR: 9.311e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.553 (1.553)  Loss:  1.0941 (1.0941)  Acc@1: 84.3750 (84.3750)  Acc@5: 94.8242 (94.8242)
Test: [  48/48]  Time: 0.172 (0.562)  Loss:  1.2579 (1.7449)  Acc@1: 81.3679 (67.0560)  Acc@5: 93.6321 (87.8680)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-50.pth.tar', 67.34800010009765)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-51.pth.tar', 67.05600005126954)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-45.pth.tar', 66.68200003173828)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-47.pth.tar', 66.18800010742187)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-49.pth.tar', 66.092000078125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-46.pth.tar', 65.85599992431641)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-41.pth.tar', 65.71600002197266)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-48.pth.tar', 65.58399995605468)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-42.pth.tar', 65.20000010742187)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-44.pth.tar', 65.072)

Train: 52 [   0/1251 (  0%)]  Loss: 3.856 (3.86)  Time: 2.342s,  437.25/s  (2.342s,  437.25/s)  LR: 9.284e-04  Data: 1.605 (1.605)
Train: 52 [  50/1251 (  4%)]  Loss: 4.328 (4.09)  Time: 0.780s, 1313.02/s  (0.816s, 1254.65/s)  LR: 9.284e-04  Data: 0.011 (0.046)
Train: 52 [ 100/1251 (  8%)]  Loss: 3.877 (4.02)  Time: 0.821s, 1246.88/s  (0.810s, 1264.94/s)  LR: 9.284e-04  Data: 0.011 (0.029)
Train: 52 [ 150/1251 ( 12%)]  Loss: 4.379 (4.11)  Time: 0.795s, 1288.49/s  (0.804s, 1273.12/s)  LR: 9.284e-04  Data: 0.010 (0.023)
Train: 52 [ 200/1251 ( 16%)]  Loss: 4.204 (4.13)  Time: 0.788s, 1298.72/s  (0.799s, 1281.02/s)  LR: 9.284e-04  Data: 0.011 (0.020)
Train: 52 [ 250/1251 ( 20%)]  Loss: 4.374 (4.17)  Time: 0.800s, 1280.20/s  (0.799s, 1282.27/s)  LR: 9.284e-04  Data: 0.011 (0.018)
Train: 52 [ 300/1251 ( 24%)]  Loss: 3.965 (4.14)  Time: 0.817s, 1253.48/s  (0.796s, 1286.21/s)  LR: 9.284e-04  Data: 0.011 (0.017)
Train: 52 [ 350/1251 ( 28%)]  Loss: 4.442 (4.18)  Time: 0.781s, 1311.09/s  (0.795s, 1287.38/s)  LR: 9.284e-04  Data: 0.011 (0.016)
Train: 52 [ 400/1251 ( 32%)]  Loss: 3.908 (4.15)  Time: 0.793s, 1290.90/s  (0.795s, 1287.94/s)  LR: 9.284e-04  Data: 0.010 (0.015)
Train: 52 [ 450/1251 ( 36%)]  Loss: 3.989 (4.13)  Time: 0.779s, 1314.22/s  (0.795s, 1288.30/s)  LR: 9.284e-04  Data: 0.010 (0.015)
Train: 52 [ 500/1251 ( 40%)]  Loss: 4.129 (4.13)  Time: 0.779s, 1314.71/s  (0.794s, 1289.99/s)  LR: 9.284e-04  Data: 0.011 (0.014)
Train: 52 [ 550/1251 ( 44%)]  Loss: 3.965 (4.12)  Time: 0.777s, 1317.46/s  (0.793s, 1291.41/s)  LR: 9.284e-04  Data: 0.011 (0.014)
Train: 52 [ 600/1251 ( 48%)]  Loss: 3.829 (4.10)  Time: 0.776s, 1319.38/s  (0.793s, 1291.91/s)  LR: 9.284e-04  Data: 0.010 (0.014)
Train: 52 [ 650/1251 ( 52%)]  Loss: 3.741 (4.07)  Time: 0.777s, 1317.21/s  (0.793s, 1290.82/s)  LR: 9.284e-04  Data: 0.012 (0.014)
Train: 52 [ 700/1251 ( 56%)]  Loss: 4.314 (4.09)  Time: 0.780s, 1312.82/s  (0.793s, 1290.77/s)  LR: 9.284e-04  Data: 0.011 (0.013)
Train: 52 [ 750/1251 ( 60%)]  Loss: 3.853 (4.07)  Time: 0.778s, 1316.09/s  (0.793s, 1291.99/s)  LR: 9.284e-04  Data: 0.010 (0.013)
Train: 52 [ 800/1251 ( 64%)]  Loss: 3.494 (4.04)  Time: 0.831s, 1231.58/s  (0.793s, 1291.57/s)  LR: 9.284e-04  Data: 0.011 (0.013)
Train: 52 [ 850/1251 ( 68%)]  Loss: 4.177 (4.05)  Time: 0.830s, 1234.22/s  (0.793s, 1290.61/s)  LR: 9.284e-04  Data: 0.011 (0.013)
Train: 52 [ 900/1251 ( 72%)]  Loss: 4.182 (4.05)  Time: 0.781s, 1311.17/s  (0.793s, 1291.28/s)  LR: 9.284e-04  Data: 0.011 (0.013)
Train: 52 [ 950/1251 ( 76%)]  Loss: 4.329 (4.07)  Time: 0.848s, 1207.61/s  (0.793s, 1291.41/s)  LR: 9.284e-04  Data: 0.011 (0.013)
Train: 52 [1000/1251 ( 80%)]  Loss: 4.071 (4.07)  Time: 0.798s, 1283.92/s  (0.793s, 1291.94/s)  LR: 9.284e-04  Data: 0.010 (0.013)
Train: 52 [1050/1251 ( 84%)]  Loss: 3.943 (4.06)  Time: 0.839s, 1220.89/s  (0.792s, 1292.14/s)  LR: 9.284e-04  Data: 0.011 (0.013)
Train: 52 [1100/1251 ( 88%)]  Loss: 4.032 (4.06)  Time: 0.844s, 1213.04/s  (0.793s, 1291.15/s)  LR: 9.284e-04  Data: 0.011 (0.012)
Train: 52 [1150/1251 ( 92%)]  Loss: 3.944 (4.06)  Time: 0.831s, 1231.93/s  (0.794s, 1289.70/s)  LR: 9.284e-04  Data: 0.011 (0.012)
Train: 52 [1200/1251 ( 96%)]  Loss: 4.140 (4.06)  Time: 0.779s, 1314.46/s  (0.794s, 1289.78/s)  LR: 9.284e-04  Data: 0.011 (0.012)
Train: 52 [1250/1251 (100%)]  Loss: 4.018 (4.06)  Time: 0.781s, 1310.45/s  (0.794s, 1290.12/s)  LR: 9.284e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.549 (1.549)  Loss:  0.9985 (0.9985)  Acc@1: 84.0820 (84.0820)  Acc@5: 95.7031 (95.7031)
Test: [  48/48]  Time: 0.172 (0.566)  Loss:  0.9093 (1.6652)  Acc@1: 82.5472 (66.9220)  Acc@5: 95.1651 (87.9260)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-50.pth.tar', 67.34800010009765)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-51.pth.tar', 67.05600005126954)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-52.pth.tar', 66.92200017578125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-45.pth.tar', 66.68200003173828)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-47.pth.tar', 66.18800010742187)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-49.pth.tar', 66.092000078125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-46.pth.tar', 65.85599992431641)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-41.pth.tar', 65.71600002197266)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-48.pth.tar', 65.58399995605468)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-42.pth.tar', 65.20000010742187)

Train: 53 [   0/1251 (  0%)]  Loss: 3.903 (3.90)  Time: 2.660s,  384.93/s  (2.660s,  384.93/s)  LR: 9.257e-04  Data: 1.908 (1.908)
Train: 53 [  50/1251 (  4%)]  Loss: 4.167 (4.03)  Time: 0.812s, 1261.59/s  (0.838s, 1221.86/s)  LR: 9.257e-04  Data: 0.010 (0.052)
Train: 53 [ 100/1251 (  8%)]  Loss: 4.189 (4.09)  Time: 0.817s, 1253.01/s  (0.815s, 1257.05/s)  LR: 9.257e-04  Data: 0.015 (0.032)
Train: 53 [ 150/1251 ( 12%)]  Loss: 3.823 (4.02)  Time: 0.780s, 1313.23/s  (0.806s, 1270.54/s)  LR: 9.257e-04  Data: 0.010 (0.025)
Train: 53 [ 200/1251 ( 16%)]  Loss: 3.922 (4.00)  Time: 0.780s, 1312.88/s  (0.802s, 1276.28/s)  LR: 9.257e-04  Data: 0.011 (0.021)
Train: 53 [ 250/1251 ( 20%)]  Loss: 4.114 (4.02)  Time: 0.782s, 1310.00/s  (0.800s, 1279.28/s)  LR: 9.257e-04  Data: 0.011 (0.019)
Train: 53 [ 300/1251 ( 24%)]  Loss: 4.089 (4.03)  Time: 0.820s, 1248.63/s  (0.801s, 1278.15/s)  LR: 9.257e-04  Data: 0.011 (0.018)
Train: 53 [ 350/1251 ( 28%)]  Loss: 4.313 (4.07)  Time: 0.878s, 1166.86/s  (0.801s, 1277.78/s)  LR: 9.257e-04  Data: 0.011 (0.017)
Train: 53 [ 400/1251 ( 32%)]  Loss: 4.323 (4.09)  Time: 0.778s, 1315.47/s  (0.800s, 1279.96/s)  LR: 9.257e-04  Data: 0.010 (0.016)
Train: 53 [ 450/1251 ( 36%)]  Loss: 3.435 (4.03)  Time: 0.777s, 1318.06/s  (0.798s, 1282.82/s)  LR: 9.257e-04  Data: 0.011 (0.016)
Train: 53 [ 500/1251 ( 40%)]  Loss: 4.004 (4.03)  Time: 0.786s, 1302.75/s  (0.797s, 1284.08/s)  LR: 9.257e-04  Data: 0.011 (0.015)
Train: 53 [ 550/1251 ( 44%)]  Loss: 3.897 (4.01)  Time: 0.783s, 1307.99/s  (0.797s, 1285.22/s)  LR: 9.257e-04  Data: 0.011 (0.015)
Train: 53 [ 600/1251 ( 48%)]  Loss: 4.236 (4.03)  Time: 0.820s, 1249.02/s  (0.796s, 1286.45/s)  LR: 9.257e-04  Data: 0.011 (0.014)
Train: 53 [ 650/1251 ( 52%)]  Loss: 4.101 (4.04)  Time: 0.783s, 1307.87/s  (0.795s, 1288.17/s)  LR: 9.257e-04  Data: 0.011 (0.014)
Train: 53 [ 700/1251 ( 56%)]  Loss: 3.933 (4.03)  Time: 0.776s, 1318.99/s  (0.794s, 1289.35/s)  LR: 9.257e-04  Data: 0.011 (0.014)
Train: 53 [ 750/1251 ( 60%)]  Loss: 4.228 (4.04)  Time: 0.778s, 1316.08/s  (0.794s, 1290.28/s)  LR: 9.257e-04  Data: 0.012 (0.014)
Train: 53 [ 800/1251 ( 64%)]  Loss: 3.839 (4.03)  Time: 0.786s, 1302.95/s  (0.793s, 1291.08/s)  LR: 9.257e-04  Data: 0.011 (0.014)
Train: 53 [ 850/1251 ( 68%)]  Loss: 4.184 (4.04)  Time: 0.779s, 1313.70/s  (0.794s, 1289.88/s)  LR: 9.257e-04  Data: 0.011 (0.014)
Train: 53 [ 900/1251 ( 72%)]  Loss: 4.076 (4.04)  Time: 0.797s, 1284.50/s  (0.794s, 1290.14/s)  LR: 9.257e-04  Data: 0.012 (0.013)
Train: 53 [ 950/1251 ( 76%)]  Loss: 3.917 (4.03)  Time: 0.791s, 1294.12/s  (0.794s, 1289.72/s)  LR: 9.257e-04  Data: 0.011 (0.013)
Train: 53 [1000/1251 ( 80%)]  Loss: 3.933 (4.03)  Time: 0.781s, 1310.57/s  (0.794s, 1290.04/s)  LR: 9.257e-04  Data: 0.011 (0.013)
Train: 53 [1050/1251 ( 84%)]  Loss: 3.996 (4.03)  Time: 0.805s, 1272.38/s  (0.794s, 1290.42/s)  LR: 9.257e-04  Data: 0.011 (0.013)
Train: 53 [1100/1251 ( 88%)]  Loss: 3.909 (4.02)  Time: 0.798s, 1283.01/s  (0.794s, 1290.33/s)  LR: 9.257e-04  Data: 0.011 (0.013)
Train: 53 [1150/1251 ( 92%)]  Loss: 4.227 (4.03)  Time: 0.780s, 1312.90/s  (0.793s, 1290.83/s)  LR: 9.257e-04  Data: 0.011 (0.013)
Train: 53 [1200/1251 ( 96%)]  Loss: 4.131 (4.04)  Time: 0.778s, 1316.49/s  (0.793s, 1291.65/s)  LR: 9.257e-04  Data: 0.012 (0.013)
Train: 53 [1250/1251 (100%)]  Loss: 3.727 (4.02)  Time: 0.804s, 1274.28/s  (0.793s, 1291.56/s)  LR: 9.257e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.537 (1.537)  Loss:  1.2672 (1.2672)  Acc@1: 80.7617 (80.7617)  Acc@5: 94.3359 (94.3359)
Test: [  48/48]  Time: 0.172 (0.561)  Loss:  1.0897 (1.7678)  Acc@1: 83.4906 (66.5460)  Acc@5: 95.0472 (87.4660)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-50.pth.tar', 67.34800010009765)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-51.pth.tar', 67.05600005126954)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-52.pth.tar', 66.92200017578125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-45.pth.tar', 66.68200003173828)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-53.pth.tar', 66.54600006835938)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-47.pth.tar', 66.18800010742187)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-49.pth.tar', 66.092000078125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-46.pth.tar', 65.85599992431641)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-41.pth.tar', 65.71600002197266)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-48.pth.tar', 65.58399995605468)

Train: 54 [   0/1251 (  0%)]  Loss: 3.727 (3.73)  Time: 2.371s,  431.97/s  (2.371s,  431.97/s)  LR: 9.229e-04  Data: 1.577 (1.577)
Train: 54 [  50/1251 (  4%)]  Loss: 4.149 (3.94)  Time: 0.778s, 1315.75/s  (0.818s, 1251.12/s)  LR: 9.229e-04  Data: 0.011 (0.045)
Train: 54 [ 100/1251 (  8%)]  Loss: 4.126 (4.00)  Time: 0.836s, 1225.18/s  (0.805s, 1272.21/s)  LR: 9.229e-04  Data: 0.011 (0.028)
Train: 54 [ 150/1251 ( 12%)]  Loss: 4.379 (4.10)  Time: 0.812s, 1260.88/s  (0.811s, 1263.38/s)  LR: 9.229e-04  Data: 0.011 (0.022)
Train: 54 [ 200/1251 ( 16%)]  Loss: 3.865 (4.05)  Time: 0.782s, 1309.08/s  (0.804s, 1273.67/s)  LR: 9.229e-04  Data: 0.011 (0.020)
Train: 54 [ 250/1251 ( 20%)]  Loss: 3.889 (4.02)  Time: 0.782s, 1309.70/s  (0.799s, 1281.07/s)  LR: 9.229e-04  Data: 0.011 (0.018)
Train: 54 [ 300/1251 ( 24%)]  Loss: 3.934 (4.01)  Time: 0.778s, 1315.84/s  (0.796s, 1285.81/s)  LR: 9.229e-04  Data: 0.011 (0.017)
Train: 54 [ 350/1251 ( 28%)]  Loss: 4.054 (4.02)  Time: 0.793s, 1291.85/s  (0.795s, 1287.62/s)  LR: 9.229e-04  Data: 0.014 (0.016)
Train: 54 [ 400/1251 ( 32%)]  Loss: 3.979 (4.01)  Time: 0.779s, 1314.22/s  (0.794s, 1289.40/s)  LR: 9.229e-04  Data: 0.011 (0.015)
Train: 54 [ 450/1251 ( 36%)]  Loss: 4.091 (4.02)  Time: 0.779s, 1313.72/s  (0.793s, 1291.86/s)  LR: 9.229e-04  Data: 0.010 (0.015)
Train: 54 [ 500/1251 ( 40%)]  Loss: 3.957 (4.01)  Time: 0.781s, 1310.68/s  (0.792s, 1292.35/s)  LR: 9.229e-04  Data: 0.011 (0.014)
Train: 54 [ 550/1251 ( 44%)]  Loss: 4.199 (4.03)  Time: 0.826s, 1240.37/s  (0.793s, 1291.98/s)  LR: 9.229e-04  Data: 0.010 (0.014)
Train: 54 [ 600/1251 ( 48%)]  Loss: 4.386 (4.06)  Time: 0.780s, 1313.24/s  (0.793s, 1291.11/s)  LR: 9.229e-04  Data: 0.011 (0.014)
Train: 54 [ 650/1251 ( 52%)]  Loss: 4.154 (4.06)  Time: 0.798s, 1283.24/s  (0.793s, 1291.04/s)  LR: 9.229e-04  Data: 0.011 (0.014)
Train: 54 [ 700/1251 ( 56%)]  Loss: 4.178 (4.07)  Time: 0.831s, 1232.40/s  (0.793s, 1290.54/s)  LR: 9.229e-04  Data: 0.014 (0.014)
Train: 54 [ 750/1251 ( 60%)]  Loss: 4.071 (4.07)  Time: 0.778s, 1315.84/s  (0.794s, 1290.21/s)  LR: 9.229e-04  Data: 0.011 (0.013)
Train: 54 [ 800/1251 ( 64%)]  Loss: 4.065 (4.07)  Time: 0.779s, 1314.70/s  (0.793s, 1290.89/s)  LR: 9.229e-04  Data: 0.011 (0.013)
Train: 54 [ 850/1251 ( 68%)]  Loss: 3.948 (4.06)  Time: 0.795s, 1288.67/s  (0.793s, 1291.86/s)  LR: 9.229e-04  Data: 0.011 (0.013)
Train: 54 [ 900/1251 ( 72%)]  Loss: 3.946 (4.06)  Time: 0.789s, 1297.66/s  (0.792s, 1292.53/s)  LR: 9.229e-04  Data: 0.011 (0.013)
Train: 54 [ 950/1251 ( 76%)]  Loss: 4.303 (4.07)  Time: 0.784s, 1305.40/s  (0.793s, 1291.15/s)  LR: 9.229e-04  Data: 0.011 (0.013)
Train: 54 [1000/1251 ( 80%)]  Loss: 4.076 (4.07)  Time: 0.835s, 1226.49/s  (0.793s, 1291.24/s)  LR: 9.229e-04  Data: 0.010 (0.013)
Train: 54 [1050/1251 ( 84%)]  Loss: 3.685 (4.05)  Time: 0.835s, 1226.21/s  (0.793s, 1291.45/s)  LR: 9.229e-04  Data: 0.011 (0.013)
Train: 54 [1100/1251 ( 88%)]  Loss: 4.138 (4.06)  Time: 0.778s, 1316.80/s  (0.792s, 1292.23/s)  LR: 9.229e-04  Data: 0.011 (0.013)
Train: 54 [1150/1251 ( 92%)]  Loss: 4.466 (4.07)  Time: 0.812s, 1261.37/s  (0.792s, 1292.75/s)  LR: 9.229e-04  Data: 0.011 (0.013)
Train: 54 [1200/1251 ( 96%)]  Loss: 3.967 (4.07)  Time: 0.777s, 1317.73/s  (0.792s, 1292.48/s)  LR: 9.229e-04  Data: 0.011 (0.012)
Train: 54 [1250/1251 (100%)]  Loss: 3.991 (4.07)  Time: 0.767s, 1335.22/s  (0.792s, 1292.98/s)  LR: 9.229e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.537 (1.537)  Loss:  1.1662 (1.1662)  Acc@1: 82.7148 (82.7148)  Acc@5: 94.1406 (94.1406)
Test: [  48/48]  Time: 0.172 (0.567)  Loss:  1.1323 (1.6677)  Acc@1: 80.4245 (66.5360)  Acc@5: 93.9859 (87.7680)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-50.pth.tar', 67.34800010009765)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-51.pth.tar', 67.05600005126954)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-52.pth.tar', 66.92200017578125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-45.pth.tar', 66.68200003173828)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-53.pth.tar', 66.54600006835938)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-54.pth.tar', 66.53600002929687)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-47.pth.tar', 66.18800010742187)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-49.pth.tar', 66.092000078125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-46.pth.tar', 65.85599992431641)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-41.pth.tar', 65.71600002197266)

Train: 55 [   0/1251 (  0%)]  Loss: 4.120 (4.12)  Time: 2.276s,  449.93/s  (2.276s,  449.93/s)  LR: 9.201e-04  Data: 1.540 (1.540)
Train: 55 [  50/1251 (  4%)]  Loss: 4.094 (4.11)  Time: 0.789s, 1298.07/s  (0.823s, 1243.57/s)  LR: 9.201e-04  Data: 0.011 (0.044)
Train: 55 [ 100/1251 (  8%)]  Loss: 3.780 (4.00)  Time: 0.806s, 1271.11/s  (0.810s, 1264.47/s)  LR: 9.201e-04  Data: 0.011 (0.028)
Train: 55 [ 150/1251 ( 12%)]  Loss: 4.241 (4.06)  Time: 0.780s, 1312.22/s  (0.805s, 1272.62/s)  LR: 9.201e-04  Data: 0.010 (0.022)
Train: 55 [ 200/1251 ( 16%)]  Loss: 4.242 (4.10)  Time: 0.801s, 1278.73/s  (0.801s, 1277.82/s)  LR: 9.201e-04  Data: 0.011 (0.019)
Train: 55 [ 250/1251 ( 20%)]  Loss: 3.906 (4.06)  Time: 0.780s, 1313.18/s  (0.798s, 1282.73/s)  LR: 9.201e-04  Data: 0.011 (0.018)
Train: 55 [ 300/1251 ( 24%)]  Loss: 4.312 (4.10)  Time: 0.779s, 1315.27/s  (0.796s, 1286.27/s)  LR: 9.201e-04  Data: 0.011 (0.017)
Train: 55 [ 350/1251 ( 28%)]  Loss: 3.983 (4.08)  Time: 0.814s, 1257.77/s  (0.797s, 1284.09/s)  LR: 9.201e-04  Data: 0.012 (0.016)
Train: 55 [ 400/1251 ( 32%)]  Loss: 3.468 (4.02)  Time: 0.778s, 1316.40/s  (0.796s, 1286.74/s)  LR: 9.201e-04  Data: 0.011 (0.015)
Train: 55 [ 450/1251 ( 36%)]  Loss: 4.085 (4.02)  Time: 0.833s, 1229.16/s  (0.795s, 1287.81/s)  LR: 9.201e-04  Data: 0.011 (0.015)
Train: 55 [ 500/1251 ( 40%)]  Loss: 4.116 (4.03)  Time: 0.811s, 1261.90/s  (0.798s, 1283.92/s)  LR: 9.201e-04  Data: 0.010 (0.014)
Train: 55 [ 550/1251 ( 44%)]  Loss: 3.883 (4.02)  Time: 0.812s, 1261.81/s  (0.800s, 1280.19/s)  LR: 9.201e-04  Data: 0.010 (0.014)
Train: 55 [ 600/1251 ( 48%)]  Loss: 4.154 (4.03)  Time: 0.778s, 1316.34/s  (0.800s, 1279.63/s)  LR: 9.201e-04  Data: 0.012 (0.014)
Train: 55 [ 650/1251 ( 52%)]  Loss: 3.974 (4.03)  Time: 0.780s, 1312.54/s  (0.799s, 1281.15/s)  LR: 9.201e-04  Data: 0.011 (0.014)
Train: 55 [ 700/1251 ( 56%)]  Loss: 4.024 (4.03)  Time: 0.776s, 1318.77/s  (0.799s, 1281.82/s)  LR: 9.201e-04  Data: 0.011 (0.013)
Train: 55 [ 750/1251 ( 60%)]  Loss: 3.730 (4.01)  Time: 0.817s, 1252.70/s  (0.798s, 1282.45/s)  LR: 9.201e-04  Data: 0.012 (0.013)
Train: 55 [ 800/1251 ( 64%)]  Loss: 4.063 (4.01)  Time: 0.778s, 1316.68/s  (0.798s, 1283.48/s)  LR: 9.201e-04  Data: 0.011 (0.013)
Train: 55 [ 850/1251 ( 68%)]  Loss: 3.785 (4.00)  Time: 0.785s, 1304.19/s  (0.799s, 1281.87/s)  LR: 9.201e-04  Data: 0.011 (0.013)
Train: 55 [ 900/1251 ( 72%)]  Loss: 4.142 (4.01)  Time: 0.808s, 1267.16/s  (0.798s, 1283.30/s)  LR: 9.201e-04  Data: 0.011 (0.013)
Train: 55 [ 950/1251 ( 76%)]  Loss: 4.166 (4.01)  Time: 0.777s, 1318.04/s  (0.798s, 1284.00/s)  LR: 9.201e-04  Data: 0.011 (0.013)
Train: 55 [1000/1251 ( 80%)]  Loss: 3.472 (3.99)  Time: 0.820s, 1248.79/s  (0.798s, 1283.22/s)  LR: 9.201e-04  Data: 0.011 (0.013)
Train: 55 [1050/1251 ( 84%)]  Loss: 3.907 (3.98)  Time: 0.790s, 1296.92/s  (0.798s, 1283.45/s)  LR: 9.201e-04  Data: 0.011 (0.013)
Train: 55 [1100/1251 ( 88%)]  Loss: 4.401 (4.00)  Time: 0.816s, 1254.63/s  (0.798s, 1282.95/s)  LR: 9.201e-04  Data: 0.011 (0.013)
Train: 55 [1150/1251 ( 92%)]  Loss: 4.184 (4.01)  Time: 0.779s, 1313.90/s  (0.799s, 1281.99/s)  LR: 9.201e-04  Data: 0.011 (0.012)
Train: 55 [1200/1251 ( 96%)]  Loss: 4.244 (4.02)  Time: 0.790s, 1296.90/s  (0.798s, 1282.77/s)  LR: 9.201e-04  Data: 0.011 (0.012)
Train: 55 [1250/1251 (100%)]  Loss: 3.539 (4.00)  Time: 0.792s, 1293.14/s  (0.798s, 1283.46/s)  LR: 9.201e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.519 (1.519)  Loss:  1.0640 (1.0640)  Acc@1: 83.8867 (83.8867)  Acc@5: 95.6055 (95.6055)
Test: [  48/48]  Time: 0.172 (0.569)  Loss:  1.4005 (1.7696)  Acc@1: 78.4198 (66.9840)  Acc@5: 92.5708 (87.7580)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-50.pth.tar', 67.34800010009765)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-51.pth.tar', 67.05600005126954)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-55.pth.tar', 66.98400006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-52.pth.tar', 66.92200017578125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-45.pth.tar', 66.68200003173828)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-53.pth.tar', 66.54600006835938)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-54.pth.tar', 66.53600002929687)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-47.pth.tar', 66.18800010742187)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-49.pth.tar', 66.092000078125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-46.pth.tar', 65.85599992431641)

Train: 56 [   0/1251 (  0%)]  Loss: 4.072 (4.07)  Time: 2.223s,  460.61/s  (2.223s,  460.61/s)  LR: 9.173e-04  Data: 1.486 (1.486)
Train: 56 [  50/1251 (  4%)]  Loss: 4.034 (4.05)  Time: 0.833s, 1229.89/s  (0.856s, 1196.81/s)  LR: 9.173e-04  Data: 0.011 (0.044)
Train: 56 [ 100/1251 (  8%)]  Loss: 4.229 (4.11)  Time: 0.779s, 1313.95/s  (0.824s, 1242.82/s)  LR: 9.173e-04  Data: 0.011 (0.028)
Train: 56 [ 150/1251 ( 12%)]  Loss: 3.649 (4.00)  Time: 0.781s, 1310.31/s  (0.817s, 1253.95/s)  LR: 9.173e-04  Data: 0.011 (0.022)
Train: 56 [ 200/1251 ( 16%)]  Loss: 3.718 (3.94)  Time: 0.794s, 1289.09/s  (0.812s, 1261.37/s)  LR: 9.173e-04  Data: 0.011 (0.019)
Train: 56 [ 250/1251 ( 20%)]  Loss: 3.875 (3.93)  Time: 0.784s, 1305.42/s  (0.806s, 1270.34/s)  LR: 9.173e-04  Data: 0.011 (0.018)
Train: 56 [ 300/1251 ( 24%)]  Loss: 3.905 (3.93)  Time: 0.780s, 1313.37/s  (0.802s, 1276.13/s)  LR: 9.173e-04  Data: 0.011 (0.017)
Train: 56 [ 350/1251 ( 28%)]  Loss: 4.083 (3.95)  Time: 0.811s, 1262.95/s  (0.801s, 1278.32/s)  LR: 9.173e-04  Data: 0.011 (0.016)
Train: 56 [ 400/1251 ( 32%)]  Loss: 4.461 (4.00)  Time: 0.847s, 1208.29/s  (0.803s, 1275.10/s)  LR: 9.173e-04  Data: 0.011 (0.015)
Train: 56 [ 450/1251 ( 36%)]  Loss: 3.836 (3.99)  Time: 0.781s, 1311.75/s  (0.803s, 1275.40/s)  LR: 9.173e-04  Data: 0.011 (0.015)
Train: 56 [ 500/1251 ( 40%)]  Loss: 4.490 (4.03)  Time: 0.786s, 1303.34/s  (0.801s, 1277.87/s)  LR: 9.173e-04  Data: 0.011 (0.014)
Train: 56 [ 550/1251 ( 44%)]  Loss: 3.647 (4.00)  Time: 0.855s, 1198.15/s  (0.801s, 1277.75/s)  LR: 9.173e-04  Data: 0.014 (0.014)
Train: 56 [ 600/1251 ( 48%)]  Loss: 3.893 (3.99)  Time: 0.830s, 1234.45/s  (0.802s, 1276.82/s)  LR: 9.173e-04  Data: 0.010 (0.014)
Train: 56 [ 650/1251 ( 52%)]  Loss: 4.055 (4.00)  Time: 0.786s, 1302.01/s  (0.803s, 1275.97/s)  LR: 9.173e-04  Data: 0.010 (0.014)
Train: 56 [ 700/1251 ( 56%)]  Loss: 4.013 (4.00)  Time: 0.818s, 1252.28/s  (0.802s, 1276.37/s)  LR: 9.173e-04  Data: 0.011 (0.013)
Train: 56 [ 750/1251 ( 60%)]  Loss: 4.020 (4.00)  Time: 0.784s, 1306.77/s  (0.801s, 1278.04/s)  LR: 9.173e-04  Data: 0.011 (0.013)
Train: 56 [ 800/1251 ( 64%)]  Loss: 4.210 (4.01)  Time: 0.786s, 1302.45/s  (0.803s, 1275.90/s)  LR: 9.173e-04  Data: 0.013 (0.013)
Train: 56 [ 850/1251 ( 68%)]  Loss: 4.201 (4.02)  Time: 0.813s, 1259.88/s  (0.803s, 1275.37/s)  LR: 9.173e-04  Data: 0.012 (0.013)
Train: 56 [ 900/1251 ( 72%)]  Loss: 4.395 (4.04)  Time: 0.776s, 1318.82/s  (0.803s, 1275.66/s)  LR: 9.173e-04  Data: 0.011 (0.013)
Train: 56 [ 950/1251 ( 76%)]  Loss: 3.979 (4.04)  Time: 0.782s, 1309.62/s  (0.802s, 1276.99/s)  LR: 9.173e-04  Data: 0.011 (0.013)
Train: 56 [1000/1251 ( 80%)]  Loss: 4.375 (4.05)  Time: 0.837s, 1223.10/s  (0.802s, 1277.10/s)  LR: 9.173e-04  Data: 0.011 (0.013)
Train: 56 [1050/1251 ( 84%)]  Loss: 4.139 (4.06)  Time: 0.780s, 1313.15/s  (0.801s, 1278.15/s)  LR: 9.173e-04  Data: 0.011 (0.013)
Train: 56 [1100/1251 ( 88%)]  Loss: 3.703 (4.04)  Time: 0.826s, 1239.30/s  (0.801s, 1278.39/s)  LR: 9.173e-04  Data: 0.011 (0.013)
Train: 56 [1150/1251 ( 92%)]  Loss: 3.687 (4.03)  Time: 0.818s, 1252.27/s  (0.801s, 1278.95/s)  LR: 9.173e-04  Data: 0.011 (0.012)
Train: 56 [1200/1251 ( 96%)]  Loss: 4.202 (4.03)  Time: 0.776s, 1319.13/s  (0.800s, 1279.64/s)  LR: 9.173e-04  Data: 0.011 (0.012)
Train: 56 [1250/1251 (100%)]  Loss: 3.999 (4.03)  Time: 0.801s, 1278.33/s  (0.800s, 1279.49/s)  LR: 9.173e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.565 (1.565)  Loss:  0.9503 (0.9503)  Acc@1: 84.2773 (84.2773)  Acc@5: 95.6055 (95.6055)
Test: [  48/48]  Time: 0.172 (0.563)  Loss:  1.1412 (1.6875)  Acc@1: 79.5991 (67.2320)  Acc@5: 93.5142 (88.2540)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-50.pth.tar', 67.34800010009765)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-56.pth.tar', 67.23200005859375)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-51.pth.tar', 67.05600005126954)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-55.pth.tar', 66.98400006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-52.pth.tar', 66.92200017578125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-45.pth.tar', 66.68200003173828)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-53.pth.tar', 66.54600006835938)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-54.pth.tar', 66.53600002929687)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-47.pth.tar', 66.18800010742187)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-49.pth.tar', 66.092000078125)

Train: 57 [   0/1251 (  0%)]  Loss: 3.976 (3.98)  Time: 2.239s,  457.39/s  (2.239s,  457.39/s)  LR: 9.144e-04  Data: 1.503 (1.503)
Train: 57 [  50/1251 (  4%)]  Loss: 3.633 (3.80)  Time: 0.818s, 1251.40/s  (0.836s, 1225.33/s)  LR: 9.144e-04  Data: 0.011 (0.044)
Train: 57 [ 100/1251 (  8%)]  Loss: 4.208 (3.94)  Time: 0.828s, 1236.31/s  (0.828s, 1236.59/s)  LR: 9.144e-04  Data: 0.010 (0.028)
Train: 57 [ 150/1251 ( 12%)]  Loss: 3.808 (3.91)  Time: 0.803s, 1275.37/s  (0.825s, 1241.93/s)  LR: 9.144e-04  Data: 0.011 (0.022)
Train: 57 [ 200/1251 ( 16%)]  Loss: 3.781 (3.88)  Time: 0.817s, 1252.94/s  (0.817s, 1254.03/s)  LR: 9.144e-04  Data: 0.012 (0.019)
Train: 57 [ 250/1251 ( 20%)]  Loss: 3.834 (3.87)  Time: 0.779s, 1314.43/s  (0.814s, 1258.02/s)  LR: 9.144e-04  Data: 0.011 (0.018)
Train: 57 [ 300/1251 ( 24%)]  Loss: 4.194 (3.92)  Time: 0.779s, 1313.70/s  (0.813s, 1259.06/s)  LR: 9.144e-04  Data: 0.011 (0.017)
Train: 57 [ 350/1251 ( 28%)]  Loss: 3.980 (3.93)  Time: 0.784s, 1305.66/s  (0.809s, 1265.83/s)  LR: 9.144e-04  Data: 0.010 (0.016)
Train: 57 [ 400/1251 ( 32%)]  Loss: 3.659 (3.90)  Time: 0.819s, 1250.78/s  (0.807s, 1269.36/s)  LR: 9.144e-04  Data: 0.011 (0.015)
Train: 57 [ 450/1251 ( 36%)]  Loss: 3.971 (3.90)  Time: 0.778s, 1315.46/s  (0.809s, 1265.50/s)  LR: 9.144e-04  Data: 0.010 (0.015)
Train: 57 [ 500/1251 ( 40%)]  Loss: 4.162 (3.93)  Time: 0.780s, 1313.58/s  (0.807s, 1268.26/s)  LR: 9.144e-04  Data: 0.010 (0.014)
Train: 57 [ 550/1251 ( 44%)]  Loss: 3.825 (3.92)  Time: 0.819s, 1250.67/s  (0.807s, 1269.59/s)  LR: 9.144e-04  Data: 0.011 (0.014)
Train: 57 [ 600/1251 ( 48%)]  Loss: 3.858 (3.91)  Time: 0.779s, 1315.18/s  (0.807s, 1268.37/s)  LR: 9.144e-04  Data: 0.011 (0.014)
Train: 57 [ 650/1251 ( 52%)]  Loss: 4.025 (3.92)  Time: 0.778s, 1315.86/s  (0.806s, 1270.91/s)  LR: 9.144e-04  Data: 0.011 (0.014)
Train: 57 [ 700/1251 ( 56%)]  Loss: 3.860 (3.92)  Time: 0.777s, 1317.46/s  (0.804s, 1273.00/s)  LR: 9.144e-04  Data: 0.012 (0.013)
Train: 57 [ 750/1251 ( 60%)]  Loss: 3.736 (3.91)  Time: 0.784s, 1306.30/s  (0.804s, 1273.48/s)  LR: 9.144e-04  Data: 0.010 (0.013)
Train: 57 [ 800/1251 ( 64%)]  Loss: 4.225 (3.93)  Time: 0.809s, 1265.62/s  (0.803s, 1275.33/s)  LR: 9.144e-04  Data: 0.011 (0.013)
Train: 57 [ 850/1251 ( 68%)]  Loss: 3.973 (3.93)  Time: 0.817s, 1252.81/s  (0.803s, 1275.28/s)  LR: 9.144e-04  Data: 0.012 (0.013)
Train: 57 [ 900/1251 ( 72%)]  Loss: 3.885 (3.93)  Time: 0.779s, 1314.41/s  (0.802s, 1276.24/s)  LR: 9.144e-04  Data: 0.011 (0.013)
Train: 57 [ 950/1251 ( 76%)]  Loss: 4.154 (3.94)  Time: 0.825s, 1240.72/s  (0.801s, 1277.71/s)  LR: 9.144e-04  Data: 0.011 (0.013)
Train: 57 [1000/1251 ( 80%)]  Loss: 3.840 (3.93)  Time: 0.779s, 1314.69/s  (0.801s, 1278.93/s)  LR: 9.144e-04  Data: 0.011 (0.013)
Train: 57 [1050/1251 ( 84%)]  Loss: 4.128 (3.94)  Time: 0.793s, 1291.05/s  (0.800s, 1279.40/s)  LR: 9.144e-04  Data: 0.011 (0.013)
Train: 57 [1100/1251 ( 88%)]  Loss: 4.106 (3.95)  Time: 0.781s, 1311.83/s  (0.800s, 1279.83/s)  LR: 9.144e-04  Data: 0.011 (0.013)
Train: 57 [1150/1251 ( 92%)]  Loss: 3.950 (3.95)  Time: 0.821s, 1247.89/s  (0.799s, 1280.96/s)  LR: 9.144e-04  Data: 0.010 (0.013)
Train: 57 [1200/1251 ( 96%)]  Loss: 3.768 (3.94)  Time: 0.777s, 1317.51/s  (0.799s, 1281.25/s)  LR: 9.144e-04  Data: 0.011 (0.012)
Train: 57 [1250/1251 (100%)]  Loss: 4.299 (3.96)  Time: 0.780s, 1313.16/s  (0.799s, 1281.83/s)  LR: 9.144e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.513 (1.513)  Loss:  0.9338 (0.9338)  Acc@1: 84.1797 (84.1797)  Acc@5: 95.8984 (95.8984)
Test: [  48/48]  Time: 0.172 (0.563)  Loss:  1.1672 (1.6929)  Acc@1: 79.5991 (66.8580)  Acc@5: 92.4528 (87.8460)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-50.pth.tar', 67.34800010009765)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-56.pth.tar', 67.23200005859375)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-51.pth.tar', 67.05600005126954)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-55.pth.tar', 66.98400006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-52.pth.tar', 66.92200017578125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-57.pth.tar', 66.85799992919922)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-45.pth.tar', 66.68200003173828)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-53.pth.tar', 66.54600006835938)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-54.pth.tar', 66.53600002929687)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-47.pth.tar', 66.18800010742187)

Train: 58 [   0/1251 (  0%)]  Loss: 4.479 (4.48)  Time: 2.462s,  415.95/s  (2.462s,  415.95/s)  LR: 9.115e-04  Data: 1.674 (1.674)
Train: 58 [  50/1251 (  4%)]  Loss: 4.354 (4.42)  Time: 0.816s, 1254.16/s  (0.834s, 1228.02/s)  LR: 9.115e-04  Data: 0.011 (0.050)
Train: 58 [ 100/1251 (  8%)]  Loss: 3.909 (4.25)  Time: 0.823s, 1243.90/s  (0.814s, 1257.57/s)  LR: 9.115e-04  Data: 0.011 (0.030)
Train: 58 [ 150/1251 ( 12%)]  Loss: 3.978 (4.18)  Time: 0.786s, 1302.86/s  (0.807s, 1269.10/s)  LR: 9.115e-04  Data: 0.011 (0.024)
Train: 58 [ 200/1251 ( 16%)]  Loss: 4.110 (4.17)  Time: 0.783s, 1307.72/s  (0.801s, 1279.00/s)  LR: 9.115e-04  Data: 0.011 (0.021)
Train: 58 [ 250/1251 ( 20%)]  Loss: 3.946 (4.13)  Time: 0.779s, 1314.67/s  (0.798s, 1282.86/s)  LR: 9.115e-04  Data: 0.012 (0.019)
Train: 58 [ 300/1251 ( 24%)]  Loss: 4.100 (4.13)  Time: 0.781s, 1311.64/s  (0.795s, 1287.57/s)  LR: 9.115e-04  Data: 0.010 (0.018)
Train: 58 [ 350/1251 ( 28%)]  Loss: 3.612 (4.06)  Time: 0.791s, 1295.09/s  (0.794s, 1290.45/s)  LR: 9.115e-04  Data: 0.011 (0.017)
Train: 58 [ 400/1251 ( 32%)]  Loss: 3.876 (4.04)  Time: 0.780s, 1313.38/s  (0.794s, 1289.44/s)  LR: 9.115e-04  Data: 0.011 (0.016)
Train: 58 [ 450/1251 ( 36%)]  Loss: 4.027 (4.04)  Time: 0.783s, 1307.40/s  (0.794s, 1289.25/s)  LR: 9.115e-04  Data: 0.012 (0.015)
Train: 58 [ 500/1251 ( 40%)]  Loss: 4.009 (4.04)  Time: 0.782s, 1309.16/s  (0.793s, 1290.67/s)  LR: 9.115e-04  Data: 0.010 (0.015)
Train: 58 [ 550/1251 ( 44%)]  Loss: 4.147 (4.05)  Time: 0.778s, 1316.92/s  (0.794s, 1289.87/s)  LR: 9.115e-04  Data: 0.011 (0.015)
Train: 58 [ 600/1251 ( 48%)]  Loss: 3.992 (4.04)  Time: 0.779s, 1314.99/s  (0.793s, 1291.23/s)  LR: 9.115e-04  Data: 0.011 (0.014)
Train: 58 [ 650/1251 ( 52%)]  Loss: 3.835 (4.03)  Time: 0.781s, 1311.13/s  (0.792s, 1292.12/s)  LR: 9.115e-04  Data: 0.011 (0.014)
Train: 58 [ 700/1251 ( 56%)]  Loss: 3.750 (4.01)  Time: 0.835s, 1226.58/s  (0.792s, 1292.38/s)  LR: 9.115e-04  Data: 0.011 (0.014)
Train: 58 [ 750/1251 ( 60%)]  Loss: 3.664 (3.99)  Time: 0.780s, 1312.76/s  (0.793s, 1290.97/s)  LR: 9.115e-04  Data: 0.011 (0.014)
Train: 58 [ 800/1251 ( 64%)]  Loss: 4.068 (3.99)  Time: 0.780s, 1313.58/s  (0.793s, 1291.58/s)  LR: 9.115e-04  Data: 0.011 (0.014)
Train: 58 [ 850/1251 ( 68%)]  Loss: 4.179 (4.00)  Time: 0.788s, 1299.20/s  (0.793s, 1291.94/s)  LR: 9.115e-04  Data: 0.010 (0.013)
Train: 58 [ 900/1251 ( 72%)]  Loss: 3.545 (3.98)  Time: 0.780s, 1312.45/s  (0.792s, 1292.59/s)  LR: 9.115e-04  Data: 0.011 (0.013)
Train: 58 [ 950/1251 ( 76%)]  Loss: 4.325 (4.00)  Time: 0.778s, 1315.55/s  (0.792s, 1293.39/s)  LR: 9.115e-04  Data: 0.011 (0.013)
Train: 58 [1000/1251 ( 80%)]  Loss: 4.095 (4.00)  Time: 0.790s, 1296.75/s  (0.791s, 1294.25/s)  LR: 9.115e-04  Data: 0.011 (0.013)
Train: 58 [1050/1251 ( 84%)]  Loss: 3.893 (4.00)  Time: 0.781s, 1311.36/s  (0.791s, 1295.16/s)  LR: 9.115e-04  Data: 0.011 (0.013)
Train: 58 [1100/1251 ( 88%)]  Loss: 4.278 (4.01)  Time: 0.803s, 1275.61/s  (0.791s, 1295.10/s)  LR: 9.115e-04  Data: 0.012 (0.013)
Train: 58 [1150/1251 ( 92%)]  Loss: 4.043 (4.01)  Time: 0.780s, 1313.52/s  (0.791s, 1295.29/s)  LR: 9.115e-04  Data: 0.011 (0.013)
Train: 58 [1200/1251 ( 96%)]  Loss: 4.518 (4.03)  Time: 0.781s, 1310.54/s  (0.790s, 1295.62/s)  LR: 9.115e-04  Data: 0.011 (0.013)
Train: 58 [1250/1251 (100%)]  Loss: 4.395 (4.04)  Time: 0.768s, 1333.89/s  (0.790s, 1296.26/s)  LR: 9.115e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.519 (1.519)  Loss:  1.0072 (1.0072)  Acc@1: 85.3516 (85.3516)  Acc@5: 95.4102 (95.4102)
Test: [  48/48]  Time: 0.172 (0.563)  Loss:  0.9572 (1.6316)  Acc@1: 83.4906 (67.2860)  Acc@5: 95.5189 (88.1500)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-50.pth.tar', 67.34800010009765)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-58.pth.tar', 67.28600006835937)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-56.pth.tar', 67.23200005859375)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-51.pth.tar', 67.05600005126954)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-55.pth.tar', 66.98400006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-52.pth.tar', 66.92200017578125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-57.pth.tar', 66.85799992919922)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-45.pth.tar', 66.68200003173828)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-53.pth.tar', 66.54600006835938)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-54.pth.tar', 66.53600002929687)

Train: 59 [   0/1251 (  0%)]  Loss: 4.064 (4.06)  Time: 2.261s,  452.92/s  (2.261s,  452.92/s)  LR: 9.085e-04  Data: 1.525 (1.525)
Train: 59 [  50/1251 (  4%)]  Loss: 4.121 (4.09)  Time: 0.800s, 1279.86/s  (0.832s, 1231.08/s)  LR: 9.085e-04  Data: 0.011 (0.046)
Train: 59 [ 100/1251 (  8%)]  Loss: 4.139 (4.11)  Time: 0.788s, 1299.05/s  (0.811s, 1262.55/s)  LR: 9.085e-04  Data: 0.010 (0.028)
Train: 59 [ 150/1251 ( 12%)]  Loss: 4.347 (4.17)  Time: 0.777s, 1317.80/s  (0.804s, 1272.89/s)  LR: 9.085e-04  Data: 0.011 (0.023)
Train: 59 [ 200/1251 ( 16%)]  Loss: 3.380 (4.01)  Time: 0.787s, 1301.73/s  (0.801s, 1277.86/s)  LR: 9.085e-04  Data: 0.011 (0.020)
Train: 59 [ 250/1251 ( 20%)]  Loss: 4.143 (4.03)  Time: 0.778s, 1315.99/s  (0.799s, 1282.06/s)  LR: 9.085e-04  Data: 0.011 (0.018)
Train: 59 [ 300/1251 ( 24%)]  Loss: 3.582 (3.97)  Time: 0.777s, 1317.11/s  (0.796s, 1286.79/s)  LR: 9.085e-04  Data: 0.011 (0.017)
Train: 59 [ 350/1251 ( 28%)]  Loss: 3.670 (3.93)  Time: 0.776s, 1319.72/s  (0.794s, 1289.66/s)  LR: 9.085e-04  Data: 0.011 (0.016)
Train: 59 [ 400/1251 ( 32%)]  Loss: 3.766 (3.91)  Time: 0.780s, 1313.34/s  (0.793s, 1291.98/s)  LR: 9.085e-04  Data: 0.011 (0.015)
Train: 59 [ 450/1251 ( 36%)]  Loss: 3.909 (3.91)  Time: 0.778s, 1316.53/s  (0.792s, 1293.27/s)  LR: 9.085e-04  Data: 0.011 (0.015)
Train: 59 [ 500/1251 ( 40%)]  Loss: 3.978 (3.92)  Time: 0.821s, 1247.36/s  (0.791s, 1294.06/s)  LR: 9.085e-04  Data: 0.011 (0.014)
Train: 59 [ 550/1251 ( 44%)]  Loss: 4.398 (3.96)  Time: 0.780s, 1313.65/s  (0.791s, 1293.99/s)  LR: 9.085e-04  Data: 0.010 (0.014)
Train: 59 [ 600/1251 ( 48%)]  Loss: 4.274 (3.98)  Time: 0.813s, 1259.12/s  (0.791s, 1293.84/s)  LR: 9.085e-04  Data: 0.010 (0.014)
Train: 59 [ 650/1251 ( 52%)]  Loss: 4.360 (4.01)  Time: 0.778s, 1316.94/s  (0.791s, 1294.74/s)  LR: 9.085e-04  Data: 0.011 (0.014)
Train: 59 [ 700/1251 ( 56%)]  Loss: 3.924 (4.00)  Time: 0.776s, 1319.33/s  (0.790s, 1295.51/s)  LR: 9.085e-04  Data: 0.011 (0.014)
Train: 59 [ 750/1251 ( 60%)]  Loss: 3.981 (4.00)  Time: 0.812s, 1261.32/s  (0.792s, 1293.51/s)  LR: 9.085e-04  Data: 0.011 (0.013)
Train: 59 [ 800/1251 ( 64%)]  Loss: 3.935 (4.00)  Time: 0.788s, 1299.47/s  (0.792s, 1293.02/s)  LR: 9.085e-04  Data: 0.012 (0.013)
Train: 59 [ 850/1251 ( 68%)]  Loss: 3.652 (3.98)  Time: 0.786s, 1302.97/s  (0.792s, 1293.65/s)  LR: 9.085e-04  Data: 0.011 (0.013)
Train: 59 [ 900/1251 ( 72%)]  Loss: 3.990 (3.98)  Time: 0.781s, 1310.79/s  (0.791s, 1294.09/s)  LR: 9.085e-04  Data: 0.012 (0.013)
Train: 59 [ 950/1251 ( 76%)]  Loss: 3.687 (3.96)  Time: 0.777s, 1317.17/s  (0.791s, 1294.49/s)  LR: 9.085e-04  Data: 0.011 (0.013)
Train: 59 [1000/1251 ( 80%)]  Loss: 3.749 (3.95)  Time: 0.789s, 1297.17/s  (0.791s, 1295.06/s)  LR: 9.085e-04  Data: 0.012 (0.013)
Train: 59 [1050/1251 ( 84%)]  Loss: 4.045 (3.96)  Time: 0.779s, 1315.00/s  (0.790s, 1295.71/s)  LR: 9.085e-04  Data: 0.012 (0.013)
Train: 59 [1100/1251 ( 88%)]  Loss: 3.631 (3.94)  Time: 0.817s, 1253.79/s  (0.790s, 1296.04/s)  LR: 9.085e-04  Data: 0.011 (0.013)
Train: 59 [1150/1251 ( 92%)]  Loss: 4.246 (3.96)  Time: 0.783s, 1308.38/s  (0.791s, 1294.67/s)  LR: 9.085e-04  Data: 0.010 (0.013)
Train: 59 [1200/1251 ( 96%)]  Loss: 3.482 (3.94)  Time: 0.778s, 1316.30/s  (0.791s, 1295.08/s)  LR: 9.085e-04  Data: 0.011 (0.013)
Train: 59 [1250/1251 (100%)]  Loss: 3.837 (3.93)  Time: 0.769s, 1331.09/s  (0.791s, 1294.86/s)  LR: 9.085e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.560 (1.560)  Loss:  1.1040 (1.1040)  Acc@1: 82.9102 (82.9102)  Acc@5: 95.6055 (95.6055)
Test: [  48/48]  Time: 0.173 (0.569)  Loss:  1.1479 (1.7163)  Acc@1: 81.9576 (67.7220)  Acc@5: 95.4009 (87.9220)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-59.pth.tar', 67.72200004882812)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-50.pth.tar', 67.34800010009765)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-58.pth.tar', 67.28600006835937)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-56.pth.tar', 67.23200005859375)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-51.pth.tar', 67.05600005126954)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-55.pth.tar', 66.98400006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-52.pth.tar', 66.92200017578125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-57.pth.tar', 66.85799992919922)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-45.pth.tar', 66.68200003173828)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-53.pth.tar', 66.54600006835938)

Train: 60 [   0/1251 (  0%)]  Loss: 4.047 (4.05)  Time: 2.314s,  442.48/s  (2.314s,  442.48/s)  LR: 9.055e-04  Data: 1.579 (1.579)
Train: 60 [  50/1251 (  4%)]  Loss: 4.186 (4.12)  Time: 0.834s, 1228.40/s  (0.838s, 1221.30/s)  LR: 9.055e-04  Data: 0.011 (0.048)
Train: 60 [ 100/1251 (  8%)]  Loss: 4.430 (4.22)  Time: 0.813s, 1258.99/s  (0.818s, 1251.84/s)  LR: 9.055e-04  Data: 0.011 (0.030)
Train: 60 [ 150/1251 ( 12%)]  Loss: 3.828 (4.12)  Time: 0.784s, 1306.09/s  (0.806s, 1270.11/s)  LR: 9.055e-04  Data: 0.011 (0.023)
Train: 60 [ 200/1251 ( 16%)]  Loss: 4.026 (4.10)  Time: 0.777s, 1317.76/s  (0.806s, 1269.76/s)  LR: 9.055e-04  Data: 0.010 (0.020)
Train: 60 [ 250/1251 ( 20%)]  Loss: 3.348 (3.98)  Time: 0.778s, 1316.26/s  (0.802s, 1276.90/s)  LR: 9.055e-04  Data: 0.011 (0.018)
Train: 60 [ 300/1251 ( 24%)]  Loss: 3.622 (3.93)  Time: 0.821s, 1247.90/s  (0.806s, 1271.21/s)  LR: 9.055e-04  Data: 0.011 (0.017)
Train: 60 [ 350/1251 ( 28%)]  Loss: 4.394 (3.99)  Time: 0.819s, 1250.97/s  (0.803s, 1274.43/s)  LR: 9.055e-04  Data: 0.011 (0.016)
Train: 60 [ 400/1251 ( 32%)]  Loss: 4.283 (4.02)  Time: 0.780s, 1313.08/s  (0.803s, 1275.53/s)  LR: 9.055e-04  Data: 0.011 (0.016)
Train: 60 [ 450/1251 ( 36%)]  Loss: 3.629 (3.98)  Time: 0.779s, 1314.08/s  (0.801s, 1279.05/s)  LR: 9.055e-04  Data: 0.010 (0.015)
Train: 60 [ 500/1251 ( 40%)]  Loss: 3.474 (3.93)  Time: 0.784s, 1305.53/s  (0.800s, 1280.42/s)  LR: 9.055e-04  Data: 0.012 (0.015)
Train: 60 [ 550/1251 ( 44%)]  Loss: 3.927 (3.93)  Time: 0.779s, 1313.94/s  (0.800s, 1280.53/s)  LR: 9.055e-04  Data: 0.011 (0.014)
Train: 60 [ 600/1251 ( 48%)]  Loss: 3.946 (3.93)  Time: 0.798s, 1282.63/s  (0.799s, 1282.22/s)  LR: 9.055e-04  Data: 0.011 (0.014)
Train: 60 [ 650/1251 ( 52%)]  Loss: 3.813 (3.93)  Time: 0.780s, 1312.21/s  (0.798s, 1283.20/s)  LR: 9.055e-04  Data: 0.010 (0.014)
Train: 60 [ 700/1251 ( 56%)]  Loss: 3.914 (3.92)  Time: 0.779s, 1313.88/s  (0.797s, 1284.77/s)  LR: 9.055e-04  Data: 0.011 (0.014)
Train: 60 [ 750/1251 ( 60%)]  Loss: 3.957 (3.93)  Time: 0.781s, 1311.67/s  (0.796s, 1286.14/s)  LR: 9.055e-04  Data: 0.011 (0.013)
Train: 60 [ 800/1251 ( 64%)]  Loss: 4.100 (3.94)  Time: 0.778s, 1316.00/s  (0.795s, 1287.81/s)  LR: 9.055e-04  Data: 0.011 (0.013)
Train: 60 [ 850/1251 ( 68%)]  Loss: 3.792 (3.93)  Time: 0.791s, 1294.35/s  (0.794s, 1289.02/s)  LR: 9.055e-04  Data: 0.011 (0.013)
Train: 60 [ 900/1251 ( 72%)]  Loss: 4.087 (3.94)  Time: 0.778s, 1316.53/s  (0.794s, 1290.16/s)  LR: 9.055e-04  Data: 0.011 (0.013)
Train: 60 [ 950/1251 ( 76%)]  Loss: 4.110 (3.95)  Time: 0.782s, 1309.03/s  (0.794s, 1290.17/s)  LR: 9.055e-04  Data: 0.011 (0.013)
Train: 60 [1000/1251 ( 80%)]  Loss: 4.168 (3.96)  Time: 0.778s, 1316.08/s  (0.794s, 1290.46/s)  LR: 9.055e-04  Data: 0.011 (0.013)
Train: 60 [1050/1251 ( 84%)]  Loss: 4.120 (3.96)  Time: 0.780s, 1312.67/s  (0.793s, 1291.15/s)  LR: 9.055e-04  Data: 0.011 (0.013)
Train: 60 [1100/1251 ( 88%)]  Loss: 3.988 (3.96)  Time: 0.840s, 1219.25/s  (0.793s, 1291.65/s)  LR: 9.055e-04  Data: 0.011 (0.013)
Train: 60 [1150/1251 ( 92%)]  Loss: 3.505 (3.95)  Time: 0.801s, 1279.12/s  (0.793s, 1291.16/s)  LR: 9.055e-04  Data: 0.011 (0.013)
Train: 60 [1200/1251 ( 96%)]  Loss: 3.889 (3.94)  Time: 0.779s, 1314.03/s  (0.793s, 1291.60/s)  LR: 9.055e-04  Data: 0.011 (0.012)
Train: 60 [1250/1251 (100%)]  Loss: 4.312 (3.96)  Time: 0.768s, 1332.90/s  (0.793s, 1291.92/s)  LR: 9.055e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.495 (1.495)  Loss:  1.0236 (1.0236)  Acc@1: 85.3516 (85.3516)  Acc@5: 95.3125 (95.3125)
Test: [  48/48]  Time: 0.172 (0.563)  Loss:  1.1466 (1.7360)  Acc@1: 81.7217 (66.5460)  Acc@5: 93.2783 (87.4480)
Train: 61 [   0/1251 (  0%)]  Loss: 4.171 (4.17)  Time: 2.308s,  443.69/s  (2.308s,  443.69/s)  LR: 9.024e-04  Data: 1.559 (1.559)
Train: 61 [  50/1251 (  4%)]  Loss: 3.809 (3.99)  Time: 0.781s, 1310.67/s  (0.820s, 1249.48/s)  LR: 9.024e-04  Data: 0.011 (0.045)
Train: 61 [ 100/1251 (  8%)]  Loss: 4.037 (4.01)  Time: 0.784s, 1306.17/s  (0.801s, 1278.56/s)  LR: 9.024e-04  Data: 0.012 (0.028)
Train: 61 [ 150/1251 ( 12%)]  Loss: 4.122 (4.03)  Time: 0.802s, 1276.08/s  (0.798s, 1282.98/s)  LR: 9.024e-04  Data: 0.011 (0.023)
Train: 61 [ 200/1251 ( 16%)]  Loss: 4.015 (4.03)  Time: 0.873s, 1172.77/s  (0.799s, 1281.93/s)  LR: 9.024e-04  Data: 0.013 (0.020)
Train: 61 [ 250/1251 ( 20%)]  Loss: 4.035 (4.03)  Time: 0.779s, 1315.21/s  (0.800s, 1279.77/s)  LR: 9.024e-04  Data: 0.011 (0.018)
Train: 61 [ 300/1251 ( 24%)]  Loss: 4.184 (4.05)  Time: 0.779s, 1314.45/s  (0.799s, 1281.66/s)  LR: 9.024e-04  Data: 0.011 (0.017)
Train: 61 [ 350/1251 ( 28%)]  Loss: 4.162 (4.07)  Time: 0.816s, 1254.19/s  (0.800s, 1279.77/s)  LR: 9.024e-04  Data: 0.011 (0.016)
Train: 61 [ 400/1251 ( 32%)]  Loss: 3.814 (4.04)  Time: 0.778s, 1316.30/s  (0.801s, 1277.76/s)  LR: 9.024e-04  Data: 0.011 (0.015)
Train: 61 [ 450/1251 ( 36%)]  Loss: 3.941 (4.03)  Time: 0.779s, 1314.23/s  (0.801s, 1278.76/s)  LR: 9.024e-04  Data: 0.010 (0.015)
Train: 61 [ 500/1251 ( 40%)]  Loss: 3.831 (4.01)  Time: 0.836s, 1224.69/s  (0.801s, 1278.72/s)  LR: 9.024e-04  Data: 0.011 (0.014)
Train: 61 [ 550/1251 ( 44%)]  Loss: 3.992 (4.01)  Time: 0.781s, 1311.75/s  (0.800s, 1279.37/s)  LR: 9.024e-04  Data: 0.011 (0.014)
Train: 61 [ 600/1251 ( 48%)]  Loss: 4.065 (4.01)  Time: 0.815s, 1255.88/s  (0.799s, 1280.89/s)  LR: 9.024e-04  Data: 0.012 (0.014)
Train: 61 [ 650/1251 ( 52%)]  Loss: 3.880 (4.00)  Time: 0.779s, 1314.76/s  (0.801s, 1278.65/s)  LR: 9.024e-04  Data: 0.011 (0.014)
Train: 61 [ 700/1251 ( 56%)]  Loss: 4.057 (4.01)  Time: 0.779s, 1314.90/s  (0.800s, 1280.66/s)  LR: 9.024e-04  Data: 0.011 (0.013)
Train: 61 [ 750/1251 ( 60%)]  Loss: 4.174 (4.02)  Time: 0.777s, 1318.47/s  (0.799s, 1281.97/s)  LR: 9.024e-04  Data: 0.011 (0.013)
Train: 61 [ 800/1251 ( 64%)]  Loss: 3.932 (4.01)  Time: 0.788s, 1300.16/s  (0.799s, 1280.80/s)  LR: 9.024e-04  Data: 0.012 (0.013)
Train: 61 [ 850/1251 ( 68%)]  Loss: 3.801 (4.00)  Time: 0.816s, 1255.63/s  (0.800s, 1280.57/s)  LR: 9.024e-04  Data: 0.011 (0.013)
Train: 61 [ 900/1251 ( 72%)]  Loss: 3.661 (3.98)  Time: 0.781s, 1311.46/s  (0.799s, 1282.14/s)  LR: 9.024e-04  Data: 0.010 (0.013)
Train: 61 [ 950/1251 ( 76%)]  Loss: 4.322 (4.00)  Time: 0.820s, 1249.14/s  (0.798s, 1282.87/s)  LR: 9.024e-04  Data: 0.011 (0.013)
Train: 61 [1000/1251 ( 80%)]  Loss: 4.202 (4.01)  Time: 0.815s, 1256.85/s  (0.797s, 1284.08/s)  LR: 9.024e-04  Data: 0.011 (0.013)
Train: 61 [1050/1251 ( 84%)]  Loss: 4.368 (4.03)  Time: 0.814s, 1257.25/s  (0.797s, 1284.78/s)  LR: 9.024e-04  Data: 0.011 (0.013)
Train: 61 [1100/1251 ( 88%)]  Loss: 3.938 (4.02)  Time: 0.778s, 1316.46/s  (0.797s, 1285.23/s)  LR: 9.024e-04  Data: 0.011 (0.013)
Train: 61 [1150/1251 ( 92%)]  Loss: 3.982 (4.02)  Time: 0.779s, 1314.47/s  (0.797s, 1285.44/s)  LR: 9.024e-04  Data: 0.011 (0.012)
Train: 61 [1200/1251 ( 96%)]  Loss: 4.180 (4.03)  Time: 0.780s, 1313.21/s  (0.796s, 1286.41/s)  LR: 9.024e-04  Data: 0.010 (0.012)
Train: 61 [1250/1251 (100%)]  Loss: 3.909 (4.02)  Time: 0.769s, 1331.18/s  (0.795s, 1287.38/s)  LR: 9.024e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.522 (1.522)  Loss:  0.9951 (0.9951)  Acc@1: 84.5703 (84.5703)  Acc@5: 95.4102 (95.4102)
Test: [  48/48]  Time: 0.172 (0.571)  Loss:  1.2191 (1.7701)  Acc@1: 80.0708 (65.7520)  Acc@5: 93.2783 (87.0820)
Train: 62 [   0/1251 (  0%)]  Loss: 4.220 (4.22)  Time: 2.220s,  461.24/s  (2.220s,  461.24/s)  LR: 8.993e-04  Data: 1.483 (1.483)
Train: 62 [  50/1251 (  4%)]  Loss: 4.229 (4.22)  Time: 0.792s, 1293.13/s  (0.819s, 1250.01/s)  LR: 8.993e-04  Data: 0.011 (0.045)
Train: 62 [ 100/1251 (  8%)]  Loss: 3.991 (4.15)  Time: 0.819s, 1250.05/s  (0.806s, 1270.59/s)  LR: 8.993e-04  Data: 0.010 (0.028)
Train: 62 [ 150/1251 ( 12%)]  Loss: 4.307 (4.19)  Time: 0.782s, 1309.97/s  (0.806s, 1270.59/s)  LR: 8.993e-04  Data: 0.011 (0.022)
Train: 62 [ 200/1251 ( 16%)]  Loss: 4.106 (4.17)  Time: 0.779s, 1313.69/s  (0.803s, 1274.64/s)  LR: 8.993e-04  Data: 0.011 (0.020)
Train: 62 [ 250/1251 ( 20%)]  Loss: 4.064 (4.15)  Time: 0.782s, 1308.80/s  (0.801s, 1278.71/s)  LR: 8.993e-04  Data: 0.011 (0.018)
Train: 62 [ 300/1251 ( 24%)]  Loss: 3.970 (4.13)  Time: 0.777s, 1317.22/s  (0.799s, 1282.15/s)  LR: 8.993e-04  Data: 0.011 (0.017)
Train: 62 [ 350/1251 ( 28%)]  Loss: 4.181 (4.13)  Time: 0.844s, 1213.82/s  (0.799s, 1281.88/s)  LR: 8.993e-04  Data: 0.011 (0.016)
Train: 62 [ 400/1251 ( 32%)]  Loss: 3.690 (4.08)  Time: 0.778s, 1316.73/s  (0.800s, 1280.51/s)  LR: 8.993e-04  Data: 0.011 (0.015)
Train: 62 [ 450/1251 ( 36%)]  Loss: 4.412 (4.12)  Time: 0.779s, 1313.87/s  (0.798s, 1283.42/s)  LR: 8.993e-04  Data: 0.011 (0.015)
Train: 62 [ 500/1251 ( 40%)]  Loss: 3.885 (4.10)  Time: 0.781s, 1311.32/s  (0.796s, 1286.01/s)  LR: 8.993e-04  Data: 0.011 (0.014)
Train: 62 [ 550/1251 ( 44%)]  Loss: 4.332 (4.12)  Time: 0.829s, 1235.77/s  (0.795s, 1287.33/s)  LR: 8.993e-04  Data: 0.011 (0.014)
Train: 62 [ 600/1251 ( 48%)]  Loss: 4.129 (4.12)  Time: 0.836s, 1225.55/s  (0.796s, 1286.39/s)  LR: 8.993e-04  Data: 0.012 (0.014)
Train: 62 [ 650/1251 ( 52%)]  Loss: 3.861 (4.10)  Time: 0.834s, 1228.33/s  (0.798s, 1283.89/s)  LR: 8.993e-04  Data: 0.011 (0.014)
Train: 62 [ 700/1251 ( 56%)]  Loss: 3.910 (4.09)  Time: 0.780s, 1312.90/s  (0.798s, 1283.63/s)  LR: 8.993e-04  Data: 0.011 (0.013)
Train: 62 [ 750/1251 ( 60%)]  Loss: 4.217 (4.09)  Time: 0.779s, 1315.26/s  (0.798s, 1283.49/s)  LR: 8.993e-04  Data: 0.010 (0.013)
Train: 62 [ 800/1251 ( 64%)]  Loss: 3.802 (4.08)  Time: 0.779s, 1314.18/s  (0.797s, 1285.01/s)  LR: 8.993e-04  Data: 0.011 (0.013)
Train: 62 [ 850/1251 ( 68%)]  Loss: 3.879 (4.07)  Time: 0.844s, 1213.26/s  (0.797s, 1285.59/s)  LR: 8.993e-04  Data: 0.011 (0.013)
Train: 62 [ 900/1251 ( 72%)]  Loss: 4.056 (4.07)  Time: 0.812s, 1261.26/s  (0.797s, 1285.21/s)  LR: 8.993e-04  Data: 0.011 (0.013)
Train: 62 [ 950/1251 ( 76%)]  Loss: 4.153 (4.07)  Time: 0.817s, 1253.36/s  (0.797s, 1285.05/s)  LR: 8.993e-04  Data: 0.011 (0.013)
Train: 62 [1000/1251 ( 80%)]  Loss: 4.053 (4.07)  Time: 0.798s, 1282.48/s  (0.798s, 1283.13/s)  LR: 8.993e-04  Data: 0.011 (0.013)
Train: 62 [1050/1251 ( 84%)]  Loss: 4.261 (4.08)  Time: 0.776s, 1319.12/s  (0.798s, 1283.54/s)  LR: 8.993e-04  Data: 0.010 (0.013)
Train: 62 [1100/1251 ( 88%)]  Loss: 3.632 (4.06)  Time: 0.777s, 1318.46/s  (0.798s, 1283.95/s)  LR: 8.993e-04  Data: 0.011 (0.013)
Train: 62 [1150/1251 ( 92%)]  Loss: 4.154 (4.06)  Time: 0.780s, 1312.27/s  (0.797s, 1284.61/s)  LR: 8.993e-04  Data: 0.011 (0.012)
Train: 62 [1200/1251 ( 96%)]  Loss: 3.794 (4.05)  Time: 0.789s, 1298.00/s  (0.797s, 1284.50/s)  LR: 8.993e-04  Data: 0.011 (0.012)
Train: 62 [1250/1251 (100%)]  Loss: 3.927 (4.05)  Time: 0.776s, 1319.63/s  (0.797s, 1284.23/s)  LR: 8.993e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.608 (1.608)  Loss:  0.9992 (0.9992)  Acc@1: 85.4492 (85.4492)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.172 (0.562)  Loss:  1.0517 (1.6921)  Acc@1: 81.0142 (67.3580)  Acc@5: 94.9293 (87.9040)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-59.pth.tar', 67.72200004882812)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-62.pth.tar', 67.35800002685546)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-50.pth.tar', 67.34800010009765)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-58.pth.tar', 67.28600006835937)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-56.pth.tar', 67.23200005859375)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-51.pth.tar', 67.05600005126954)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-55.pth.tar', 66.98400006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-52.pth.tar', 66.92200017578125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-57.pth.tar', 66.85799992919922)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-45.pth.tar', 66.68200003173828)

Train: 63 [   0/1251 (  0%)]  Loss: 4.135 (4.13)  Time: 2.243s,  456.56/s  (2.243s,  456.56/s)  LR: 8.961e-04  Data: 1.507 (1.507)
Train: 63 [  50/1251 (  4%)]  Loss: 3.810 (3.97)  Time: 0.780s, 1313.26/s  (0.818s, 1252.58/s)  LR: 8.961e-04  Data: 0.011 (0.044)
Train: 63 [ 100/1251 (  8%)]  Loss: 4.244 (4.06)  Time: 0.776s, 1318.95/s  (0.804s, 1274.26/s)  LR: 8.961e-04  Data: 0.011 (0.028)
Train: 63 [ 150/1251 ( 12%)]  Loss: 3.701 (3.97)  Time: 0.815s, 1256.46/s  (0.797s, 1284.63/s)  LR: 8.961e-04  Data: 0.012 (0.022)
Train: 63 [ 200/1251 ( 16%)]  Loss: 4.078 (3.99)  Time: 0.779s, 1314.42/s  (0.794s, 1290.07/s)  LR: 8.961e-04  Data: 0.011 (0.019)
Train: 63 [ 250/1251 ( 20%)]  Loss: 3.884 (3.98)  Time: 0.814s, 1258.54/s  (0.799s, 1280.90/s)  LR: 8.961e-04  Data: 0.011 (0.018)
Train: 63 [ 300/1251 ( 24%)]  Loss: 3.750 (3.94)  Time: 0.790s, 1296.30/s  (0.798s, 1283.13/s)  LR: 8.961e-04  Data: 0.011 (0.017)
Train: 63 [ 350/1251 ( 28%)]  Loss: 3.899 (3.94)  Time: 0.812s, 1261.28/s  (0.798s, 1283.77/s)  LR: 8.961e-04  Data: 0.011 (0.016)
Train: 63 [ 400/1251 ( 32%)]  Loss: 3.858 (3.93)  Time: 0.778s, 1315.84/s  (0.797s, 1284.39/s)  LR: 8.961e-04  Data: 0.011 (0.015)
Train: 63 [ 450/1251 ( 36%)]  Loss: 3.943 (3.93)  Time: 0.785s, 1304.61/s  (0.798s, 1283.71/s)  LR: 8.961e-04  Data: 0.011 (0.015)
Train: 63 [ 500/1251 ( 40%)]  Loss: 4.126 (3.95)  Time: 0.778s, 1316.95/s  (0.798s, 1284.01/s)  LR: 8.961e-04  Data: 0.010 (0.014)
Train: 63 [ 550/1251 ( 44%)]  Loss: 3.887 (3.94)  Time: 0.776s, 1319.62/s  (0.796s, 1286.29/s)  LR: 8.961e-04  Data: 0.010 (0.014)
Train: 63 [ 600/1251 ( 48%)]  Loss: 4.074 (3.95)  Time: 0.777s, 1318.16/s  (0.796s, 1286.23/s)  LR: 8.961e-04  Data: 0.011 (0.014)
Train: 63 [ 650/1251 ( 52%)]  Loss: 3.782 (3.94)  Time: 0.779s, 1314.98/s  (0.795s, 1287.51/s)  LR: 8.961e-04  Data: 0.011 (0.014)
Train: 63 [ 700/1251 ( 56%)]  Loss: 4.241 (3.96)  Time: 0.806s, 1269.80/s  (0.796s, 1285.66/s)  LR: 8.961e-04  Data: 0.010 (0.013)
Train: 63 [ 750/1251 ( 60%)]  Loss: 4.386 (3.99)  Time: 0.784s, 1306.57/s  (0.797s, 1285.47/s)  LR: 8.961e-04  Data: 0.012 (0.013)
Train: 63 [ 800/1251 ( 64%)]  Loss: 4.057 (3.99)  Time: 0.831s, 1232.96/s  (0.797s, 1284.54/s)  LR: 8.961e-04  Data: 0.011 (0.013)
Train: 63 [ 850/1251 ( 68%)]  Loss: 3.816 (3.98)  Time: 0.778s, 1316.53/s  (0.798s, 1283.37/s)  LR: 8.961e-04  Data: 0.011 (0.013)
Train: 63 [ 900/1251 ( 72%)]  Loss: 3.731 (3.97)  Time: 0.780s, 1313.13/s  (0.798s, 1283.67/s)  LR: 8.961e-04  Data: 0.011 (0.013)
Train: 63 [ 950/1251 ( 76%)]  Loss: 3.922 (3.97)  Time: 0.779s, 1314.56/s  (0.797s, 1285.21/s)  LR: 8.961e-04  Data: 0.011 (0.013)
Train: 63 [1000/1251 ( 80%)]  Loss: 3.861 (3.96)  Time: 0.820s, 1248.94/s  (0.797s, 1285.46/s)  LR: 8.961e-04  Data: 0.012 (0.013)
Train: 63 [1050/1251 ( 84%)]  Loss: 4.043 (3.96)  Time: 0.793s, 1292.07/s  (0.797s, 1285.28/s)  LR: 8.961e-04  Data: 0.012 (0.013)
Train: 63 [1100/1251 ( 88%)]  Loss: 4.172 (3.97)  Time: 0.877s, 1167.47/s  (0.798s, 1283.50/s)  LR: 8.961e-04  Data: 0.011 (0.013)
Train: 63 [1150/1251 ( 92%)]  Loss: 3.950 (3.97)  Time: 0.785s, 1304.12/s  (0.797s, 1284.16/s)  LR: 8.961e-04  Data: 0.012 (0.013)
Train: 63 [1200/1251 ( 96%)]  Loss: 4.080 (3.98)  Time: 0.779s, 1313.72/s  (0.797s, 1285.02/s)  LR: 8.961e-04  Data: 0.011 (0.012)
Train: 63 [1250/1251 (100%)]  Loss: 3.973 (3.98)  Time: 0.770s, 1329.40/s  (0.796s, 1286.13/s)  LR: 8.961e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.623 (1.623)  Loss:  1.0009 (1.0009)  Acc@1: 86.6211 (86.6211)  Acc@5: 95.6055 (95.6055)
Test: [  48/48]  Time: 0.172 (0.569)  Loss:  1.0874 (1.6956)  Acc@1: 80.6604 (67.4960)  Acc@5: 94.3396 (88.3400)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-59.pth.tar', 67.72200004882812)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-63.pth.tar', 67.4960000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-62.pth.tar', 67.35800002685546)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-50.pth.tar', 67.34800010009765)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-58.pth.tar', 67.28600006835937)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-56.pth.tar', 67.23200005859375)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-51.pth.tar', 67.05600005126954)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-55.pth.tar', 66.98400006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-52.pth.tar', 66.92200017578125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-57.pth.tar', 66.85799992919922)

Train: 64 [   0/1251 (  0%)]  Loss: 4.334 (4.33)  Time: 2.341s,  437.48/s  (2.341s,  437.48/s)  LR: 8.929e-04  Data: 1.605 (1.605)
Train: 64 [  50/1251 (  4%)]  Loss: 4.088 (4.21)  Time: 0.837s, 1223.05/s  (0.824s, 1242.44/s)  LR: 8.929e-04  Data: 0.010 (0.046)
Train: 64 [ 100/1251 (  8%)]  Loss: 4.264 (4.23)  Time: 0.831s, 1232.64/s  (0.812s, 1261.74/s)  LR: 8.929e-04  Data: 0.011 (0.029)
Train: 64 [ 150/1251 ( 12%)]  Loss: 4.236 (4.23)  Time: 0.778s, 1316.61/s  (0.804s, 1273.53/s)  LR: 8.929e-04  Data: 0.011 (0.023)
Train: 64 [ 200/1251 ( 16%)]  Loss: 4.302 (4.24)  Time: 0.779s, 1315.05/s  (0.799s, 1281.76/s)  LR: 8.929e-04  Data: 0.011 (0.020)
Train: 64 [ 250/1251 ( 20%)]  Loss: 3.932 (4.19)  Time: 0.777s, 1317.18/s  (0.796s, 1285.98/s)  LR: 8.929e-04  Data: 0.011 (0.018)
Train: 64 [ 300/1251 ( 24%)]  Loss: 4.024 (4.17)  Time: 0.781s, 1311.76/s  (0.794s, 1289.56/s)  LR: 8.929e-04  Data: 0.011 (0.017)
Train: 64 [ 350/1251 ( 28%)]  Loss: 3.789 (4.12)  Time: 0.779s, 1314.33/s  (0.794s, 1290.18/s)  LR: 8.929e-04  Data: 0.011 (0.016)
Train: 64 [ 400/1251 ( 32%)]  Loss: 4.232 (4.13)  Time: 0.780s, 1313.11/s  (0.793s, 1291.08/s)  LR: 8.929e-04  Data: 0.011 (0.015)
Train: 64 [ 450/1251 ( 36%)]  Loss: 4.159 (4.14)  Time: 0.787s, 1300.73/s  (0.793s, 1291.08/s)  LR: 8.929e-04  Data: 0.011 (0.015)
Train: 64 [ 500/1251 ( 40%)]  Loss: 4.376 (4.16)  Time: 0.779s, 1313.89/s  (0.793s, 1290.61/s)  LR: 8.929e-04  Data: 0.010 (0.014)
Train: 64 [ 550/1251 ( 44%)]  Loss: 3.612 (4.11)  Time: 0.777s, 1317.73/s  (0.793s, 1291.07/s)  LR: 8.929e-04  Data: 0.010 (0.014)
Train: 64 [ 600/1251 ( 48%)]  Loss: 4.212 (4.12)  Time: 0.789s, 1297.27/s  (0.793s, 1292.08/s)  LR: 8.929e-04  Data: 0.012 (0.014)
Train: 64 [ 650/1251 ( 52%)]  Loss: 4.261 (4.13)  Time: 0.779s, 1313.95/s  (0.792s, 1293.56/s)  LR: 8.929e-04  Data: 0.010 (0.014)
Train: 64 [ 700/1251 ( 56%)]  Loss: 3.771 (4.11)  Time: 0.820s, 1249.13/s  (0.792s, 1292.98/s)  LR: 8.929e-04  Data: 0.011 (0.013)
Train: 64 [ 750/1251 ( 60%)]  Loss: 4.040 (4.10)  Time: 0.780s, 1313.12/s  (0.792s, 1292.26/s)  LR: 8.929e-04  Data: 0.011 (0.013)
Train: 64 [ 800/1251 ( 64%)]  Loss: 4.160 (4.11)  Time: 0.779s, 1314.89/s  (0.792s, 1293.43/s)  LR: 8.929e-04  Data: 0.011 (0.013)
Train: 64 [ 850/1251 ( 68%)]  Loss: 4.146 (4.11)  Time: 0.779s, 1314.47/s  (0.791s, 1293.85/s)  LR: 8.929e-04  Data: 0.011 (0.013)
Train: 64 [ 900/1251 ( 72%)]  Loss: 4.226 (4.11)  Time: 0.778s, 1316.03/s  (0.792s, 1293.53/s)  LR: 8.929e-04  Data: 0.011 (0.013)
Train: 64 [ 950/1251 ( 76%)]  Loss: 3.903 (4.10)  Time: 0.778s, 1316.70/s  (0.791s, 1293.89/s)  LR: 8.929e-04  Data: 0.010 (0.013)
Train: 64 [1000/1251 ( 80%)]  Loss: 3.904 (4.09)  Time: 0.778s, 1316.16/s  (0.791s, 1294.90/s)  LR: 8.929e-04  Data: 0.011 (0.013)
Train: 64 [1050/1251 ( 84%)]  Loss: 3.719 (4.08)  Time: 0.837s, 1223.84/s  (0.791s, 1294.22/s)  LR: 8.929e-04  Data: 0.011 (0.013)
Train: 64 [1100/1251 ( 88%)]  Loss: 3.635 (4.06)  Time: 0.780s, 1313.03/s  (0.791s, 1294.37/s)  LR: 8.929e-04  Data: 0.011 (0.013)
Train: 64 [1150/1251 ( 92%)]  Loss: 4.059 (4.06)  Time: 0.776s, 1319.66/s  (0.791s, 1294.74/s)  LR: 8.929e-04  Data: 0.011 (0.012)
Train: 64 [1200/1251 ( 96%)]  Loss: 4.026 (4.06)  Time: 0.779s, 1313.95/s  (0.791s, 1295.18/s)  LR: 8.929e-04  Data: 0.011 (0.012)
Train: 64 [1250/1251 (100%)]  Loss: 3.917 (4.05)  Time: 0.765s, 1338.25/s  (0.791s, 1295.11/s)  LR: 8.929e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.517 (1.517)  Loss:  1.2846 (1.2846)  Acc@1: 83.8867 (83.8867)  Acc@5: 95.0195 (95.0195)
Test: [  48/48]  Time: 0.172 (0.568)  Loss:  1.0881 (1.7582)  Acc@1: 80.8962 (66.0140)  Acc@5: 94.1038 (87.4300)
Train: 65 [   0/1251 (  0%)]  Loss: 3.983 (3.98)  Time: 2.328s,  439.79/s  (2.328s,  439.79/s)  LR: 8.897e-04  Data: 1.593 (1.593)
Train: 65 [  50/1251 (  4%)]  Loss: 3.903 (3.94)  Time: 0.834s, 1227.82/s  (0.841s, 1217.28/s)  LR: 8.897e-04  Data: 0.011 (0.048)
Train: 65 [ 100/1251 (  8%)]  Loss: 3.480 (3.79)  Time: 0.777s, 1318.18/s  (0.818s, 1251.78/s)  LR: 8.897e-04  Data: 0.010 (0.029)
Train: 65 [ 150/1251 ( 12%)]  Loss: 4.118 (3.87)  Time: 0.785s, 1305.19/s  (0.809s, 1266.12/s)  LR: 8.897e-04  Data: 0.010 (0.023)
Train: 65 [ 200/1251 ( 16%)]  Loss: 4.119 (3.92)  Time: 0.811s, 1262.57/s  (0.810s, 1264.87/s)  LR: 8.897e-04  Data: 0.011 (0.020)
Train: 65 [ 250/1251 ( 20%)]  Loss: 4.136 (3.96)  Time: 0.774s, 1322.72/s  (0.806s, 1270.91/s)  LR: 8.897e-04  Data: 0.011 (0.018)
Train: 65 [ 300/1251 ( 24%)]  Loss: 3.858 (3.94)  Time: 0.779s, 1315.03/s  (0.802s, 1276.23/s)  LR: 8.897e-04  Data: 0.011 (0.017)
Train: 65 [ 350/1251 ( 28%)]  Loss: 4.274 (3.98)  Time: 0.824s, 1242.39/s  (0.801s, 1279.04/s)  LR: 8.897e-04  Data: 0.011 (0.016)
Train: 65 [ 400/1251 ( 32%)]  Loss: 4.233 (4.01)  Time: 0.780s, 1313.41/s  (0.799s, 1280.83/s)  LR: 8.897e-04  Data: 0.011 (0.016)
Train: 65 [ 450/1251 ( 36%)]  Loss: 4.238 (4.03)  Time: 0.790s, 1295.66/s  (0.797s, 1284.16/s)  LR: 8.897e-04  Data: 0.012 (0.015)
Train: 65 [ 500/1251 ( 40%)]  Loss: 3.954 (4.03)  Time: 0.786s, 1303.08/s  (0.796s, 1286.13/s)  LR: 8.897e-04  Data: 0.012 (0.015)
Train: 65 [ 550/1251 ( 44%)]  Loss: 3.967 (4.02)  Time: 0.779s, 1315.02/s  (0.795s, 1288.24/s)  LR: 8.897e-04  Data: 0.011 (0.014)
Train: 65 [ 600/1251 ( 48%)]  Loss: 4.156 (4.03)  Time: 0.778s, 1316.59/s  (0.795s, 1288.72/s)  LR: 8.897e-04  Data: 0.011 (0.014)
Train: 65 [ 650/1251 ( 52%)]  Loss: 4.056 (4.03)  Time: 0.779s, 1314.89/s  (0.794s, 1290.02/s)  LR: 8.897e-04  Data: 0.011 (0.014)
Train: 65 [ 700/1251 ( 56%)]  Loss: 4.143 (4.04)  Time: 0.777s, 1317.83/s  (0.794s, 1290.02/s)  LR: 8.897e-04  Data: 0.011 (0.014)
Train: 65 [ 750/1251 ( 60%)]  Loss: 3.687 (4.02)  Time: 0.787s, 1301.70/s  (0.794s, 1288.91/s)  LR: 8.897e-04  Data: 0.011 (0.013)
Train: 65 [ 800/1251 ( 64%)]  Loss: 3.816 (4.01)  Time: 0.783s, 1308.44/s  (0.794s, 1290.22/s)  LR: 8.897e-04  Data: 0.010 (0.013)
Train: 65 [ 850/1251 ( 68%)]  Loss: 3.752 (3.99)  Time: 0.779s, 1314.60/s  (0.793s, 1291.46/s)  LR: 8.897e-04  Data: 0.010 (0.013)
Train: 65 [ 900/1251 ( 72%)]  Loss: 4.137 (4.00)  Time: 0.777s, 1317.46/s  (0.792s, 1292.14/s)  LR: 8.897e-04  Data: 0.011 (0.013)
Train: 65 [ 950/1251 ( 76%)]  Loss: 4.040 (4.00)  Time: 0.781s, 1311.94/s  (0.793s, 1291.91/s)  LR: 8.897e-04  Data: 0.011 (0.013)
Train: 65 [1000/1251 ( 80%)]  Loss: 3.685 (3.99)  Time: 0.783s, 1308.40/s  (0.792s, 1292.70/s)  LR: 8.897e-04  Data: 0.012 (0.013)
Train: 65 [1050/1251 ( 84%)]  Loss: 3.565 (3.97)  Time: 0.778s, 1315.51/s  (0.792s, 1293.35/s)  LR: 8.897e-04  Data: 0.011 (0.013)
Train: 65 [1100/1251 ( 88%)]  Loss: 4.222 (3.98)  Time: 0.781s, 1311.95/s  (0.792s, 1293.71/s)  LR: 8.897e-04  Data: 0.011 (0.013)
Train: 65 [1150/1251 ( 92%)]  Loss: 3.917 (3.98)  Time: 0.781s, 1311.60/s  (0.791s, 1294.14/s)  LR: 8.897e-04  Data: 0.011 (0.013)
Train: 65 [1200/1251 ( 96%)]  Loss: 3.647 (3.96)  Time: 0.780s, 1313.56/s  (0.791s, 1294.75/s)  LR: 8.897e-04  Data: 0.011 (0.013)
Train: 65 [1250/1251 (100%)]  Loss: 3.611 (3.95)  Time: 0.786s, 1302.04/s  (0.791s, 1295.30/s)  LR: 8.897e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.589 (1.589)  Loss:  0.9200 (0.9200)  Acc@1: 85.8398 (85.8398)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.172 (0.563)  Loss:  1.0888 (1.6318)  Acc@1: 81.1321 (67.2580)  Acc@5: 94.3396 (88.1100)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-59.pth.tar', 67.72200004882812)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-63.pth.tar', 67.4960000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-62.pth.tar', 67.35800002685546)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-50.pth.tar', 67.34800010009765)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-58.pth.tar', 67.28600006835937)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-65.pth.tar', 67.258000078125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-56.pth.tar', 67.23200005859375)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-51.pth.tar', 67.05600005126954)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-55.pth.tar', 66.98400006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-52.pth.tar', 66.92200017578125)

Train: 66 [   0/1251 (  0%)]  Loss: 3.691 (3.69)  Time: 2.302s,  444.90/s  (2.302s,  444.90/s)  LR: 8.864e-04  Data: 1.568 (1.568)
Train: 66 [  50/1251 (  4%)]  Loss: 3.869 (3.78)  Time: 0.781s, 1310.34/s  (0.825s, 1241.68/s)  LR: 8.864e-04  Data: 0.010 (0.044)
Train: 66 [ 100/1251 (  8%)]  Loss: 4.344 (3.97)  Time: 0.849s, 1206.78/s  (0.812s, 1260.91/s)  LR: 8.864e-04  Data: 0.011 (0.027)
Train: 66 [ 150/1251 ( 12%)]  Loss: 3.939 (3.96)  Time: 0.827s, 1238.35/s  (0.807s, 1269.16/s)  LR: 8.864e-04  Data: 0.011 (0.022)
Train: 66 [ 200/1251 ( 16%)]  Loss: 3.816 (3.93)  Time: 0.837s, 1223.75/s  (0.807s, 1268.25/s)  LR: 8.864e-04  Data: 0.011 (0.019)
Train: 66 [ 250/1251 ( 20%)]  Loss: 4.278 (3.99)  Time: 0.777s, 1318.29/s  (0.803s, 1275.38/s)  LR: 8.864e-04  Data: 0.011 (0.017)
Train: 66 [ 300/1251 ( 24%)]  Loss: 3.798 (3.96)  Time: 0.843s, 1214.65/s  (0.802s, 1277.52/s)  LR: 8.864e-04  Data: 0.010 (0.016)
Train: 66 [ 350/1251 ( 28%)]  Loss: 3.794 (3.94)  Time: 0.780s, 1312.93/s  (0.799s, 1280.83/s)  LR: 8.864e-04  Data: 0.011 (0.016)
Train: 66 [ 400/1251 ( 32%)]  Loss: 3.775 (3.92)  Time: 0.777s, 1317.92/s  (0.798s, 1283.81/s)  LR: 8.864e-04  Data: 0.010 (0.015)
Train: 66 [ 450/1251 ( 36%)]  Loss: 4.245 (3.95)  Time: 0.841s, 1218.02/s  (0.800s, 1280.19/s)  LR: 8.864e-04  Data: 0.011 (0.015)
Train: 66 [ 500/1251 ( 40%)]  Loss: 4.181 (3.98)  Time: 0.780s, 1312.54/s  (0.800s, 1279.74/s)  LR: 8.864e-04  Data: 0.011 (0.014)
Train: 66 [ 550/1251 ( 44%)]  Loss: 4.147 (3.99)  Time: 0.778s, 1316.69/s  (0.799s, 1281.63/s)  LR: 8.864e-04  Data: 0.011 (0.014)
Train: 66 [ 600/1251 ( 48%)]  Loss: 3.722 (3.97)  Time: 0.830s, 1233.96/s  (0.798s, 1283.25/s)  LR: 8.864e-04  Data: 0.011 (0.014)
Train: 66 [ 650/1251 ( 52%)]  Loss: 3.605 (3.94)  Time: 0.781s, 1310.61/s  (0.798s, 1283.16/s)  LR: 8.864e-04  Data: 0.011 (0.013)
Train: 66 [ 700/1251 ( 56%)]  Loss: 3.903 (3.94)  Time: 0.780s, 1312.67/s  (0.797s, 1284.44/s)  LR: 8.864e-04  Data: 0.012 (0.013)
Train: 66 [ 750/1251 ( 60%)]  Loss: 3.781 (3.93)  Time: 0.810s, 1263.85/s  (0.796s, 1286.03/s)  LR: 8.864e-04  Data: 0.011 (0.013)
Train: 66 [ 800/1251 ( 64%)]  Loss: 4.033 (3.94)  Time: 0.832s, 1230.13/s  (0.797s, 1284.36/s)  LR: 8.864e-04  Data: 0.011 (0.013)
Train: 66 [ 850/1251 ( 68%)]  Loss: 3.997 (3.94)  Time: 0.792s, 1292.79/s  (0.797s, 1284.91/s)  LR: 8.864e-04  Data: 0.011 (0.013)
Train: 66 [ 900/1251 ( 72%)]  Loss: 3.981 (3.94)  Time: 0.779s, 1315.32/s  (0.798s, 1283.97/s)  LR: 8.864e-04  Data: 0.010 (0.013)
Train: 66 [ 950/1251 ( 76%)]  Loss: 4.185 (3.95)  Time: 0.796s, 1286.94/s  (0.797s, 1284.39/s)  LR: 8.864e-04  Data: 0.011 (0.013)
Train: 66 [1000/1251 ( 80%)]  Loss: 4.186 (3.97)  Time: 0.808s, 1267.96/s  (0.798s, 1283.30/s)  LR: 8.864e-04  Data: 0.011 (0.013)
Train: 66 [1050/1251 ( 84%)]  Loss: 4.161 (3.97)  Time: 0.780s, 1313.07/s  (0.797s, 1284.08/s)  LR: 8.864e-04  Data: 0.011 (0.012)
Train: 66 [1100/1251 ( 88%)]  Loss: 3.848 (3.97)  Time: 0.779s, 1315.31/s  (0.797s, 1284.81/s)  LR: 8.864e-04  Data: 0.011 (0.012)
Train: 66 [1150/1251 ( 92%)]  Loss: 4.000 (3.97)  Time: 0.839s, 1220.53/s  (0.797s, 1285.19/s)  LR: 8.864e-04  Data: 0.011 (0.012)
Train: 66 [1200/1251 ( 96%)]  Loss: 4.413 (3.99)  Time: 0.822s, 1246.21/s  (0.798s, 1283.42/s)  LR: 8.864e-04  Data: 0.012 (0.012)
Train: 66 [1250/1251 (100%)]  Loss: 4.261 (4.00)  Time: 0.779s, 1314.83/s  (0.798s, 1283.32/s)  LR: 8.864e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.511 (1.511)  Loss:  1.1736 (1.1736)  Acc@1: 83.7891 (83.7891)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.172 (0.572)  Loss:  1.2226 (1.8148)  Acc@1: 82.7830 (66.7560)  Acc@5: 94.8113 (87.7560)
Train: 67 [   0/1251 (  0%)]  Loss: 4.136 (4.14)  Time: 2.218s,  461.64/s  (2.218s,  461.64/s)  LR: 8.831e-04  Data: 1.482 (1.482)
Train: 67 [  50/1251 (  4%)]  Loss: 4.099 (4.12)  Time: 0.780s, 1313.56/s  (0.831s, 1232.47/s)  LR: 8.831e-04  Data: 0.011 (0.049)
Train: 67 [ 100/1251 (  8%)]  Loss: 4.154 (4.13)  Time: 0.813s, 1259.70/s  (0.819s, 1250.48/s)  LR: 8.831e-04  Data: 0.011 (0.030)
Train: 67 [ 150/1251 ( 12%)]  Loss: 3.479 (3.97)  Time: 0.780s, 1312.91/s  (0.810s, 1264.58/s)  LR: 8.831e-04  Data: 0.011 (0.024)
Train: 67 [ 200/1251 ( 16%)]  Loss: 4.129 (4.00)  Time: 0.778s, 1316.10/s  (0.805s, 1272.48/s)  LR: 8.831e-04  Data: 0.010 (0.020)
Train: 67 [ 250/1251 ( 20%)]  Loss: 4.007 (4.00)  Time: 0.814s, 1258.65/s  (0.805s, 1272.70/s)  LR: 8.831e-04  Data: 0.010 (0.019)
Train: 67 [ 300/1251 ( 24%)]  Loss: 3.986 (4.00)  Time: 0.778s, 1315.89/s  (0.805s, 1272.33/s)  LR: 8.831e-04  Data: 0.011 (0.017)
Train: 67 [ 350/1251 ( 28%)]  Loss: 3.536 (3.94)  Time: 0.817s, 1253.87/s  (0.803s, 1275.22/s)  LR: 8.831e-04  Data: 0.010 (0.016)
Train: 67 [ 400/1251 ( 32%)]  Loss: 4.001 (3.95)  Time: 0.777s, 1318.06/s  (0.801s, 1278.58/s)  LR: 8.831e-04  Data: 0.012 (0.016)
Train: 67 [ 450/1251 ( 36%)]  Loss: 3.973 (3.95)  Time: 0.817s, 1253.63/s  (0.800s, 1279.75/s)  LR: 8.831e-04  Data: 0.012 (0.015)
Train: 67 [ 500/1251 ( 40%)]  Loss: 3.588 (3.92)  Time: 0.808s, 1267.30/s  (0.800s, 1279.36/s)  LR: 8.831e-04  Data: 0.011 (0.015)
Train: 67 [ 550/1251 ( 44%)]  Loss: 4.212 (3.94)  Time: 0.818s, 1252.46/s  (0.799s, 1280.80/s)  LR: 8.831e-04  Data: 0.011 (0.014)
Train: 67 [ 600/1251 ( 48%)]  Loss: 4.190 (3.96)  Time: 0.779s, 1314.71/s  (0.800s, 1279.73/s)  LR: 8.831e-04  Data: 0.010 (0.014)
Train: 67 [ 650/1251 ( 52%)]  Loss: 4.008 (3.96)  Time: 0.785s, 1304.74/s  (0.800s, 1280.16/s)  LR: 8.831e-04  Data: 0.011 (0.014)
Train: 67 [ 700/1251 ( 56%)]  Loss: 4.399 (3.99)  Time: 0.775s, 1321.48/s  (0.799s, 1281.56/s)  LR: 8.831e-04  Data: 0.011 (0.014)
Train: 67 [ 750/1251 ( 60%)]  Loss: 4.119 (4.00)  Time: 0.780s, 1313.23/s  (0.798s, 1283.36/s)  LR: 8.831e-04  Data: 0.011 (0.014)
Train: 67 [ 800/1251 ( 64%)]  Loss: 4.142 (4.01)  Time: 0.776s, 1318.94/s  (0.798s, 1283.25/s)  LR: 8.831e-04  Data: 0.011 (0.013)
Train: 67 [ 850/1251 ( 68%)]  Loss: 4.159 (4.02)  Time: 0.777s, 1317.09/s  (0.797s, 1284.76/s)  LR: 8.831e-04  Data: 0.011 (0.013)
Train: 67 [ 900/1251 ( 72%)]  Loss: 4.384 (4.04)  Time: 0.786s, 1303.42/s  (0.796s, 1285.84/s)  LR: 8.831e-04  Data: 0.010 (0.013)
Train: 67 [ 950/1251 ( 76%)]  Loss: 3.741 (4.02)  Time: 0.778s, 1316.08/s  (0.796s, 1285.72/s)  LR: 8.831e-04  Data: 0.011 (0.013)
Train: 67 [1000/1251 ( 80%)]  Loss: 3.891 (4.02)  Time: 0.814s, 1257.21/s  (0.796s, 1285.91/s)  LR: 8.831e-04  Data: 0.011 (0.013)
Train: 67 [1050/1251 ( 84%)]  Loss: 4.065 (4.02)  Time: 0.815s, 1256.56/s  (0.796s, 1286.53/s)  LR: 8.831e-04  Data: 0.012 (0.013)
Train: 67 [1100/1251 ( 88%)]  Loss: 3.991 (4.02)  Time: 0.778s, 1316.35/s  (0.796s, 1287.07/s)  LR: 8.831e-04  Data: 0.011 (0.013)
Train: 67 [1150/1251 ( 92%)]  Loss: 3.890 (4.01)  Time: 0.778s, 1315.40/s  (0.795s, 1288.21/s)  LR: 8.831e-04  Data: 0.011 (0.013)
Train: 67 [1200/1251 ( 96%)]  Loss: 3.771 (4.00)  Time: 0.782s, 1309.61/s  (0.794s, 1289.02/s)  LR: 8.831e-04  Data: 0.011 (0.013)
Train: 67 [1250/1251 (100%)]  Loss: 4.228 (4.01)  Time: 0.767s, 1334.50/s  (0.794s, 1289.59/s)  LR: 8.831e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.548 (1.548)  Loss:  1.3169 (1.3169)  Acc@1: 83.1055 (83.1055)  Acc@5: 95.0195 (95.0195)
Test: [  48/48]  Time: 0.172 (0.568)  Loss:  1.1789 (1.8174)  Acc@1: 83.0189 (67.7480)  Acc@5: 95.2830 (88.6300)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-67.pth.tar', 67.74799999267579)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-59.pth.tar', 67.72200004882812)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-63.pth.tar', 67.4960000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-62.pth.tar', 67.35800002685546)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-50.pth.tar', 67.34800010009765)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-58.pth.tar', 67.28600006835937)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-65.pth.tar', 67.258000078125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-56.pth.tar', 67.23200005859375)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-51.pth.tar', 67.05600005126954)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-55.pth.tar', 66.98400006347656)

Train: 68 [   0/1251 (  0%)]  Loss: 4.080 (4.08)  Time: 2.368s,  432.38/s  (2.368s,  432.38/s)  LR: 8.797e-04  Data: 1.635 (1.635)
Train: 68 [  50/1251 (  4%)]  Loss: 4.051 (4.07)  Time: 0.778s, 1316.58/s  (0.835s, 1226.51/s)  LR: 8.797e-04  Data: 0.011 (0.049)
Train: 68 [ 100/1251 (  8%)]  Loss: 3.942 (4.02)  Time: 0.777s, 1317.32/s  (0.811s, 1263.35/s)  LR: 8.797e-04  Data: 0.011 (0.030)
Train: 68 [ 150/1251 ( 12%)]  Loss: 4.125 (4.05)  Time: 0.781s, 1311.83/s  (0.804s, 1274.33/s)  LR: 8.797e-04  Data: 0.011 (0.024)
Train: 68 [ 200/1251 ( 16%)]  Loss: 3.500 (3.94)  Time: 0.838s, 1222.05/s  (0.804s, 1273.47/s)  LR: 8.797e-04  Data: 0.010 (0.021)
Train: 68 [ 250/1251 ( 20%)]  Loss: 4.315 (4.00)  Time: 0.779s, 1314.77/s  (0.802s, 1276.21/s)  LR: 8.797e-04  Data: 0.010 (0.019)
Train: 68 [ 300/1251 ( 24%)]  Loss: 3.703 (3.96)  Time: 0.847s, 1209.05/s  (0.807s, 1269.23/s)  LR: 8.797e-04  Data: 0.011 (0.017)
Train: 68 [ 350/1251 ( 28%)]  Loss: 3.793 (3.94)  Time: 0.811s, 1262.65/s  (0.806s, 1270.72/s)  LR: 8.797e-04  Data: 0.011 (0.017)
Train: 68 [ 400/1251 ( 32%)]  Loss: 4.014 (3.95)  Time: 0.812s, 1261.68/s  (0.806s, 1270.13/s)  LR: 8.797e-04  Data: 0.010 (0.016)
Train: 68 [ 450/1251 ( 36%)]  Loss: 3.679 (3.92)  Time: 0.778s, 1315.91/s  (0.805s, 1272.52/s)  LR: 8.797e-04  Data: 0.011 (0.015)
Train: 68 [ 500/1251 ( 40%)]  Loss: 4.240 (3.95)  Time: 0.860s, 1191.02/s  (0.806s, 1270.12/s)  LR: 8.797e-04  Data: 0.015 (0.015)
Train: 68 [ 550/1251 ( 44%)]  Loss: 3.711 (3.93)  Time: 0.780s, 1312.69/s  (0.805s, 1271.85/s)  LR: 8.797e-04  Data: 0.011 (0.015)
Train: 68 [ 600/1251 ( 48%)]  Loss: 3.922 (3.93)  Time: 0.780s, 1312.09/s  (0.805s, 1272.81/s)  LR: 8.797e-04  Data: 0.011 (0.014)
Train: 68 [ 650/1251 ( 52%)]  Loss: 4.234 (3.95)  Time: 0.778s, 1315.75/s  (0.803s, 1275.78/s)  LR: 8.797e-04  Data: 0.011 (0.014)
Train: 68 [ 700/1251 ( 56%)]  Loss: 3.357 (3.91)  Time: 0.796s, 1286.89/s  (0.801s, 1277.63/s)  LR: 8.797e-04  Data: 0.010 (0.014)
Train: 68 [ 750/1251 ( 60%)]  Loss: 3.494 (3.89)  Time: 0.779s, 1314.49/s  (0.801s, 1277.86/s)  LR: 8.797e-04  Data: 0.011 (0.014)
Train: 68 [ 800/1251 ( 64%)]  Loss: 3.839 (3.88)  Time: 0.778s, 1316.52/s  (0.801s, 1278.18/s)  LR: 8.797e-04  Data: 0.010 (0.013)
Train: 68 [ 850/1251 ( 68%)]  Loss: 3.937 (3.89)  Time: 0.779s, 1315.33/s  (0.800s, 1279.61/s)  LR: 8.797e-04  Data: 0.011 (0.013)
Train: 68 [ 900/1251 ( 72%)]  Loss: 4.024 (3.89)  Time: 0.778s, 1315.78/s  (0.799s, 1281.34/s)  LR: 8.797e-04  Data: 0.010 (0.013)
Train: 68 [ 950/1251 ( 76%)]  Loss: 3.708 (3.88)  Time: 0.777s, 1318.37/s  (0.798s, 1282.90/s)  LR: 8.797e-04  Data: 0.011 (0.013)
Train: 68 [1000/1251 ( 80%)]  Loss: 3.855 (3.88)  Time: 0.779s, 1315.28/s  (0.797s, 1284.23/s)  LR: 8.797e-04  Data: 0.011 (0.013)
Train: 68 [1050/1251 ( 84%)]  Loss: 4.082 (3.89)  Time: 0.779s, 1315.07/s  (0.797s, 1284.82/s)  LR: 8.797e-04  Data: 0.011 (0.013)
Train: 68 [1100/1251 ( 88%)]  Loss: 4.101 (3.90)  Time: 0.831s, 1231.69/s  (0.797s, 1285.08/s)  LR: 8.797e-04  Data: 0.011 (0.013)
Train: 68 [1150/1251 ( 92%)]  Loss: 3.748 (3.89)  Time: 0.780s, 1312.96/s  (0.797s, 1285.49/s)  LR: 8.797e-04  Data: 0.011 (0.013)
Train: 68 [1200/1251 ( 96%)]  Loss: 3.713 (3.89)  Time: 0.779s, 1315.32/s  (0.796s, 1285.92/s)  LR: 8.797e-04  Data: 0.010 (0.013)
Train: 68 [1250/1251 (100%)]  Loss: 3.938 (3.89)  Time: 0.773s, 1325.38/s  (0.796s, 1286.88/s)  LR: 8.797e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.502 (1.502)  Loss:  1.0126 (1.0126)  Acc@1: 85.2539 (85.2539)  Acc@5: 95.6055 (95.6055)
Test: [  48/48]  Time: 0.172 (0.566)  Loss:  1.1143 (1.6176)  Acc@1: 80.7783 (68.4280)  Acc@5: 93.6321 (89.0380)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-68.pth.tar', 68.42800005371093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-67.pth.tar', 67.74799999267579)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-59.pth.tar', 67.72200004882812)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-63.pth.tar', 67.4960000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-62.pth.tar', 67.35800002685546)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-50.pth.tar', 67.34800010009765)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-58.pth.tar', 67.28600006835937)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-65.pth.tar', 67.258000078125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-56.pth.tar', 67.23200005859375)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-51.pth.tar', 67.05600005126954)

Train: 69 [   0/1251 (  0%)]  Loss: 4.194 (4.19)  Time: 2.294s,  446.46/s  (2.294s,  446.46/s)  LR: 8.763e-04  Data: 1.559 (1.559)
Train: 69 [  50/1251 (  4%)]  Loss: 4.048 (4.12)  Time: 0.831s, 1232.24/s  (0.824s, 1243.43/s)  LR: 8.763e-04  Data: 0.011 (0.049)
Train: 69 [ 100/1251 (  8%)]  Loss: 3.887 (4.04)  Time: 0.787s, 1301.48/s  (0.807s, 1269.37/s)  LR: 8.763e-04  Data: 0.012 (0.030)
Train: 69 [ 150/1251 ( 12%)]  Loss: 4.157 (4.07)  Time: 0.778s, 1316.92/s  (0.798s, 1283.15/s)  LR: 8.763e-04  Data: 0.010 (0.024)
Train: 69 [ 200/1251 ( 16%)]  Loss: 4.268 (4.11)  Time: 0.779s, 1315.04/s  (0.798s, 1283.41/s)  LR: 8.763e-04  Data: 0.011 (0.020)
Train: 69 [ 250/1251 ( 20%)]  Loss: 3.788 (4.06)  Time: 0.780s, 1313.61/s  (0.795s, 1288.23/s)  LR: 8.763e-04  Data: 0.010 (0.019)
Train: 69 [ 300/1251 ( 24%)]  Loss: 3.962 (4.04)  Time: 0.818s, 1252.48/s  (0.795s, 1288.03/s)  LR: 8.763e-04  Data: 0.011 (0.017)
Train: 69 [ 350/1251 ( 28%)]  Loss: 4.112 (4.05)  Time: 0.815s, 1256.42/s  (0.798s, 1283.65/s)  LR: 8.763e-04  Data: 0.011 (0.016)
Train: 69 [ 400/1251 ( 32%)]  Loss: 3.732 (4.02)  Time: 0.777s, 1317.87/s  (0.797s, 1285.03/s)  LR: 8.763e-04  Data: 0.011 (0.016)
Train: 69 [ 450/1251 ( 36%)]  Loss: 3.949 (4.01)  Time: 0.777s, 1318.68/s  (0.795s, 1287.34/s)  LR: 8.763e-04  Data: 0.010 (0.015)
Train: 69 [ 500/1251 ( 40%)]  Loss: 3.861 (4.00)  Time: 0.778s, 1315.65/s  (0.794s, 1289.84/s)  LR: 8.763e-04  Data: 0.011 (0.015)
Train: 69 [ 550/1251 ( 44%)]  Loss: 3.822 (3.98)  Time: 0.813s, 1260.29/s  (0.795s, 1288.80/s)  LR: 8.763e-04  Data: 0.010 (0.014)
Train: 69 [ 600/1251 ( 48%)]  Loss: 3.967 (3.98)  Time: 0.779s, 1315.11/s  (0.794s, 1290.22/s)  LR: 8.763e-04  Data: 0.011 (0.014)
Train: 69 [ 650/1251 ( 52%)]  Loss: 4.068 (3.99)  Time: 0.781s, 1310.47/s  (0.793s, 1291.27/s)  LR: 8.763e-04  Data: 0.010 (0.014)
Train: 69 [ 700/1251 ( 56%)]  Loss: 4.018 (3.99)  Time: 0.776s, 1319.03/s  (0.792s, 1292.52/s)  LR: 8.763e-04  Data: 0.010 (0.014)
Train: 69 [ 750/1251 ( 60%)]  Loss: 3.506 (3.96)  Time: 0.784s, 1306.03/s  (0.792s, 1293.30/s)  LR: 8.763e-04  Data: 0.011 (0.014)
Train: 69 [ 800/1251 ( 64%)]  Loss: 3.908 (3.96)  Time: 0.778s, 1316.33/s  (0.791s, 1294.00/s)  LR: 8.763e-04  Data: 0.012 (0.013)
Train: 69 [ 850/1251 ( 68%)]  Loss: 4.055 (3.96)  Time: 0.824s, 1243.37/s  (0.791s, 1294.20/s)  LR: 8.763e-04  Data: 0.011 (0.013)
Train: 69 [ 900/1251 ( 72%)]  Loss: 3.886 (3.96)  Time: 0.816s, 1254.74/s  (0.792s, 1293.24/s)  LR: 8.763e-04  Data: 0.011 (0.013)
Train: 69 [ 950/1251 ( 76%)]  Loss: 3.834 (3.95)  Time: 0.826s, 1239.82/s  (0.791s, 1293.79/s)  LR: 8.763e-04  Data: 0.011 (0.013)
Train: 69 [1000/1251 ( 80%)]  Loss: 4.379 (3.97)  Time: 0.812s, 1260.99/s  (0.792s, 1292.76/s)  LR: 8.763e-04  Data: 0.011 (0.013)
Train: 69 [1050/1251 ( 84%)]  Loss: 3.707 (3.96)  Time: 0.775s, 1321.07/s  (0.792s, 1292.55/s)  LR: 8.763e-04  Data: 0.011 (0.013)
Train: 69 [1100/1251 ( 88%)]  Loss: 4.092 (3.97)  Time: 0.776s, 1319.15/s  (0.792s, 1293.32/s)  LR: 8.763e-04  Data: 0.011 (0.013)
Train: 69 [1150/1251 ( 92%)]  Loss: 3.767 (3.96)  Time: 0.813s, 1259.14/s  (0.792s, 1292.64/s)  LR: 8.763e-04  Data: 0.010 (0.013)
Train: 69 [1200/1251 ( 96%)]  Loss: 3.962 (3.96)  Time: 0.849s, 1206.36/s  (0.794s, 1290.38/s)  LR: 8.763e-04  Data: 0.010 (0.013)
Train: 69 [1250/1251 (100%)]  Loss: 4.209 (3.97)  Time: 0.819s, 1250.05/s  (0.793s, 1290.73/s)  LR: 8.763e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.560 (1.560)  Loss:  1.0066 (1.0066)  Acc@1: 85.4492 (85.4492)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.172 (0.565)  Loss:  1.1548 (1.6542)  Acc@1: 80.6604 (68.6360)  Acc@5: 95.0472 (89.0540)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-69.pth.tar', 68.6360000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-68.pth.tar', 68.42800005371093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-67.pth.tar', 67.74799999267579)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-59.pth.tar', 67.72200004882812)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-63.pth.tar', 67.4960000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-62.pth.tar', 67.35800002685546)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-50.pth.tar', 67.34800010009765)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-58.pth.tar', 67.28600006835937)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-65.pth.tar', 67.258000078125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-56.pth.tar', 67.23200005859375)

Train: 70 [   0/1251 (  0%)]  Loss: 3.950 (3.95)  Time: 2.301s,  445.09/s  (2.301s,  445.09/s)  LR: 8.729e-04  Data: 1.565 (1.565)
Train: 70 [  50/1251 (  4%)]  Loss: 4.265 (4.11)  Time: 0.830s, 1233.99/s  (0.837s, 1223.09/s)  LR: 8.729e-04  Data: 0.011 (0.051)
Train: 70 [ 100/1251 (  8%)]  Loss: 3.754 (3.99)  Time: 0.777s, 1317.85/s  (0.812s, 1261.28/s)  LR: 8.729e-04  Data: 0.011 (0.031)
Train: 70 [ 150/1251 ( 12%)]  Loss: 4.176 (4.04)  Time: 0.781s, 1311.08/s  (0.805s, 1272.13/s)  LR: 8.729e-04  Data: 0.011 (0.025)
Train: 70 [ 200/1251 ( 16%)]  Loss: 3.954 (4.02)  Time: 0.780s, 1312.41/s  (0.802s, 1276.68/s)  LR: 8.729e-04  Data: 0.011 (0.021)
Train: 70 [ 250/1251 ( 20%)]  Loss: 3.933 (4.01)  Time: 0.837s, 1223.77/s  (0.799s, 1282.03/s)  LR: 8.729e-04  Data: 0.011 (0.019)
Train: 70 [ 300/1251 ( 24%)]  Loss: 3.672 (3.96)  Time: 0.779s, 1314.34/s  (0.798s, 1283.30/s)  LR: 8.729e-04  Data: 0.011 (0.018)
Train: 70 [ 350/1251 ( 28%)]  Loss: 3.740 (3.93)  Time: 0.780s, 1313.65/s  (0.797s, 1284.69/s)  LR: 8.729e-04  Data: 0.011 (0.017)
Train: 70 [ 400/1251 ( 32%)]  Loss: 4.077 (3.95)  Time: 0.786s, 1303.30/s  (0.796s, 1286.77/s)  LR: 8.729e-04  Data: 0.011 (0.016)
Train: 70 [ 450/1251 ( 36%)]  Loss: 3.586 (3.91)  Time: 0.779s, 1314.67/s  (0.795s, 1288.56/s)  LR: 8.729e-04  Data: 0.011 (0.016)
Train: 70 [ 500/1251 ( 40%)]  Loss: 3.865 (3.91)  Time: 0.789s, 1297.35/s  (0.794s, 1289.05/s)  LR: 8.729e-04  Data: 0.011 (0.015)
Train: 70 [ 550/1251 ( 44%)]  Loss: 3.978 (3.91)  Time: 0.779s, 1314.23/s  (0.794s, 1289.08/s)  LR: 8.729e-04  Data: 0.012 (0.015)
Train: 70 [ 600/1251 ( 48%)]  Loss: 3.892 (3.91)  Time: 0.778s, 1315.55/s  (0.793s, 1290.77/s)  LR: 8.729e-04  Data: 0.010 (0.014)
Train: 70 [ 650/1251 ( 52%)]  Loss: 4.097 (3.92)  Time: 0.780s, 1312.96/s  (0.792s, 1292.40/s)  LR: 8.729e-04  Data: 0.011 (0.014)
Train: 70 [ 700/1251 ( 56%)]  Loss: 4.053 (3.93)  Time: 0.777s, 1317.05/s  (0.792s, 1292.59/s)  LR: 8.729e-04  Data: 0.010 (0.014)
Train: 70 [ 750/1251 ( 60%)]  Loss: 4.383 (3.96)  Time: 0.782s, 1309.91/s  (0.792s, 1293.38/s)  LR: 8.729e-04  Data: 0.011 (0.014)
Train: 70 [ 800/1251 ( 64%)]  Loss: 4.024 (3.96)  Time: 0.822s, 1245.29/s  (0.791s, 1293.94/s)  LR: 8.729e-04  Data: 0.010 (0.014)
Train: 70 [ 850/1251 ( 68%)]  Loss: 3.937 (3.96)  Time: 0.805s, 1272.81/s  (0.791s, 1294.22/s)  LR: 8.729e-04  Data: 0.011 (0.013)
Train: 70 [ 900/1251 ( 72%)]  Loss: 4.163 (3.97)  Time: 0.780s, 1313.43/s  (0.791s, 1294.88/s)  LR: 8.729e-04  Data: 0.011 (0.013)
Train: 70 [ 950/1251 ( 76%)]  Loss: 4.145 (3.98)  Time: 0.786s, 1302.15/s  (0.790s, 1295.43/s)  LR: 8.729e-04  Data: 0.015 (0.013)
Train: 70 [1000/1251 ( 80%)]  Loss: 4.051 (3.99)  Time: 0.778s, 1315.51/s  (0.790s, 1295.58/s)  LR: 8.729e-04  Data: 0.010 (0.013)
Train: 70 [1050/1251 ( 84%)]  Loss: 3.965 (3.98)  Time: 0.819s, 1250.89/s  (0.790s, 1295.52/s)  LR: 8.729e-04  Data: 0.011 (0.013)
Train: 70 [1100/1251 ( 88%)]  Loss: 3.632 (3.97)  Time: 0.780s, 1313.65/s  (0.792s, 1293.30/s)  LR: 8.729e-04  Data: 0.011 (0.013)
Train: 70 [1150/1251 ( 92%)]  Loss: 3.942 (3.97)  Time: 0.780s, 1312.34/s  (0.792s, 1293.39/s)  LR: 8.729e-04  Data: 0.010 (0.013)
Train: 70 [1200/1251 ( 96%)]  Loss: 3.879 (3.96)  Time: 0.836s, 1224.21/s  (0.792s, 1292.79/s)  LR: 8.729e-04  Data: 0.010 (0.013)
Train: 70 [1250/1251 (100%)]  Loss: 3.964 (3.96)  Time: 0.767s, 1334.41/s  (0.792s, 1293.16/s)  LR: 8.729e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.536 (1.536)  Loss:  0.9241 (0.9241)  Acc@1: 83.4961 (83.4961)  Acc@5: 96.0938 (96.0938)
Test: [  48/48]  Time: 0.172 (0.561)  Loss:  1.0145 (1.6304)  Acc@1: 82.3113 (68.2280)  Acc@5: 94.3396 (88.6780)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-69.pth.tar', 68.6360000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-68.pth.tar', 68.42800005371093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-70.pth.tar', 68.22800007324219)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-67.pth.tar', 67.74799999267579)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-59.pth.tar', 67.72200004882812)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-63.pth.tar', 67.4960000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-62.pth.tar', 67.35800002685546)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-50.pth.tar', 67.34800010009765)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-58.pth.tar', 67.28600006835937)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-65.pth.tar', 67.258000078125)

Train: 71 [   0/1251 (  0%)]  Loss: 3.604 (3.60)  Time: 2.230s,  459.15/s  (2.230s,  459.15/s)  LR: 8.694e-04  Data: 1.493 (1.493)
Train: 71 [  50/1251 (  4%)]  Loss: 4.149 (3.88)  Time: 0.780s, 1312.45/s  (0.823s, 1244.20/s)  LR: 8.694e-04  Data: 0.011 (0.048)
Train: 71 [ 100/1251 (  8%)]  Loss: 4.053 (3.94)  Time: 0.778s, 1316.99/s  (0.804s, 1274.32/s)  LR: 8.694e-04  Data: 0.010 (0.030)
Train: 71 [ 150/1251 ( 12%)]  Loss: 4.131 (3.98)  Time: 0.815s, 1255.97/s  (0.801s, 1278.01/s)  LR: 8.694e-04  Data: 0.011 (0.024)
Train: 71 [ 200/1251 ( 16%)]  Loss: 4.267 (4.04)  Time: 0.816s, 1254.50/s  (0.800s, 1280.24/s)  LR: 8.694e-04  Data: 0.011 (0.020)
Train: 71 [ 250/1251 ( 20%)]  Loss: 4.217 (4.07)  Time: 0.785s, 1304.57/s  (0.799s, 1281.00/s)  LR: 8.694e-04  Data: 0.011 (0.019)
Train: 71 [ 300/1251 ( 24%)]  Loss: 4.116 (4.08)  Time: 0.777s, 1318.14/s  (0.797s, 1285.52/s)  LR: 8.694e-04  Data: 0.011 (0.017)
Train: 71 [ 350/1251 ( 28%)]  Loss: 3.803 (4.04)  Time: 0.775s, 1320.83/s  (0.800s, 1280.53/s)  LR: 8.694e-04  Data: 0.011 (0.016)
Train: 71 [ 400/1251 ( 32%)]  Loss: 4.012 (4.04)  Time: 0.778s, 1316.06/s  (0.797s, 1284.28/s)  LR: 8.694e-04  Data: 0.011 (0.016)
Train: 71 [ 450/1251 ( 36%)]  Loss: 3.751 (4.01)  Time: 0.778s, 1316.67/s  (0.797s, 1284.68/s)  LR: 8.694e-04  Data: 0.011 (0.015)
Train: 71 [ 500/1251 ( 40%)]  Loss: 4.204 (4.03)  Time: 0.780s, 1312.09/s  (0.796s, 1285.80/s)  LR: 8.694e-04  Data: 0.010 (0.015)
Train: 71 [ 550/1251 ( 44%)]  Loss: 3.661 (4.00)  Time: 0.780s, 1312.51/s  (0.795s, 1287.79/s)  LR: 8.694e-04  Data: 0.011 (0.014)
Train: 71 [ 600/1251 ( 48%)]  Loss: 3.918 (3.99)  Time: 0.812s, 1260.92/s  (0.794s, 1289.47/s)  LR: 8.694e-04  Data: 0.011 (0.014)
Train: 71 [ 650/1251 ( 52%)]  Loss: 4.084 (4.00)  Time: 0.784s, 1306.09/s  (0.794s, 1290.29/s)  LR: 8.694e-04  Data: 0.011 (0.014)
Train: 71 [ 700/1251 ( 56%)]  Loss: 3.527 (3.97)  Time: 0.822s, 1245.47/s  (0.794s, 1289.87/s)  LR: 8.694e-04  Data: 0.010 (0.014)
Train: 71 [ 750/1251 ( 60%)]  Loss: 4.039 (3.97)  Time: 0.783s, 1307.74/s  (0.793s, 1290.87/s)  LR: 8.694e-04  Data: 0.011 (0.013)
Train: 71 [ 800/1251 ( 64%)]  Loss: 4.094 (3.98)  Time: 0.812s, 1261.17/s  (0.793s, 1291.65/s)  LR: 8.694e-04  Data: 0.011 (0.013)
Train: 71 [ 850/1251 ( 68%)]  Loss: 4.099 (3.98)  Time: 0.794s, 1290.41/s  (0.793s, 1290.56/s)  LR: 8.694e-04  Data: 0.011 (0.013)
Train: 71 [ 900/1251 ( 72%)]  Loss: 3.902 (3.98)  Time: 0.791s, 1294.26/s  (0.794s, 1290.11/s)  LR: 8.694e-04  Data: 0.013 (0.013)
Train: 71 [ 950/1251 ( 76%)]  Loss: 4.034 (3.98)  Time: 0.780s, 1312.30/s  (0.794s, 1290.43/s)  LR: 8.694e-04  Data: 0.010 (0.013)
Train: 71 [1000/1251 ( 80%)]  Loss: 3.734 (3.97)  Time: 0.814s, 1258.08/s  (0.794s, 1290.48/s)  LR: 8.694e-04  Data: 0.011 (0.013)
Train: 71 [1050/1251 ( 84%)]  Loss: 4.344 (3.99)  Time: 0.778s, 1316.19/s  (0.794s, 1290.47/s)  LR: 8.694e-04  Data: 0.011 (0.013)
Train: 71 [1100/1251 ( 88%)]  Loss: 3.942 (3.99)  Time: 0.780s, 1313.15/s  (0.793s, 1291.09/s)  LR: 8.694e-04  Data: 0.011 (0.013)
Train: 71 [1150/1251 ( 92%)]  Loss: 4.151 (3.99)  Time: 0.780s, 1312.37/s  (0.793s, 1291.56/s)  LR: 8.694e-04  Data: 0.011 (0.013)
Train: 71 [1200/1251 ( 96%)]  Loss: 4.094 (4.00)  Time: 0.780s, 1313.17/s  (0.793s, 1292.04/s)  LR: 8.694e-04  Data: 0.011 (0.013)
Train: 71 [1250/1251 (100%)]  Loss: 3.829 (3.99)  Time: 0.768s, 1333.86/s  (0.792s, 1292.49/s)  LR: 8.694e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.506 (1.506)  Loss:  0.9260 (0.9260)  Acc@1: 83.3984 (83.3984)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.172 (0.559)  Loss:  1.1107 (1.5993)  Acc@1: 78.5377 (68.0160)  Acc@5: 93.5142 (88.5740)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-69.pth.tar', 68.6360000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-68.pth.tar', 68.42800005371093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-70.pth.tar', 68.22800007324219)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-71.pth.tar', 68.01600011474609)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-67.pth.tar', 67.74799999267579)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-59.pth.tar', 67.72200004882812)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-63.pth.tar', 67.4960000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-62.pth.tar', 67.35800002685546)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-50.pth.tar', 67.34800010009765)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-58.pth.tar', 67.28600006835937)

Train: 72 [   0/1251 (  0%)]  Loss: 4.576 (4.58)  Time: 2.379s,  430.46/s  (2.379s,  430.46/s)  LR: 8.658e-04  Data: 1.643 (1.643)
Train: 72 [  50/1251 (  4%)]  Loss: 4.033 (4.30)  Time: 0.784s, 1305.58/s  (0.825s, 1240.82/s)  LR: 8.658e-04  Data: 0.012 (0.049)
Train: 72 [ 100/1251 (  8%)]  Loss: 4.347 (4.32)  Time: 0.778s, 1315.58/s  (0.809s, 1265.30/s)  LR: 8.658e-04  Data: 0.011 (0.031)
Train: 72 [ 150/1251 ( 12%)]  Loss: 4.050 (4.25)  Time: 0.780s, 1313.57/s  (0.800s, 1280.07/s)  LR: 8.658e-04  Data: 0.011 (0.024)
Train: 72 [ 200/1251 ( 16%)]  Loss: 3.771 (4.16)  Time: 0.790s, 1296.72/s  (0.796s, 1286.04/s)  LR: 8.658e-04  Data: 0.011 (0.021)
Train: 72 [ 250/1251 ( 20%)]  Loss: 3.623 (4.07)  Time: 0.778s, 1316.24/s  (0.793s, 1291.47/s)  LR: 8.658e-04  Data: 0.010 (0.019)
Train: 72 [ 300/1251 ( 24%)]  Loss: 3.862 (4.04)  Time: 0.781s, 1311.04/s  (0.791s, 1294.67/s)  LR: 8.658e-04  Data: 0.011 (0.018)
Train: 72 [ 350/1251 ( 28%)]  Loss: 4.420 (4.09)  Time: 0.793s, 1291.97/s  (0.790s, 1295.45/s)  LR: 8.658e-04  Data: 0.011 (0.017)
Train: 72 [ 400/1251 ( 32%)]  Loss: 3.865 (4.06)  Time: 0.782s, 1310.13/s  (0.791s, 1294.41/s)  LR: 8.658e-04  Data: 0.011 (0.016)
Train: 72 [ 450/1251 ( 36%)]  Loss: 3.990 (4.05)  Time: 0.838s, 1221.69/s  (0.794s, 1290.31/s)  LR: 8.658e-04  Data: 0.011 (0.015)
Train: 72 [ 500/1251 ( 40%)]  Loss: 4.240 (4.07)  Time: 0.784s, 1305.87/s  (0.793s, 1291.69/s)  LR: 8.658e-04  Data: 0.011 (0.015)
Train: 72 [ 550/1251 ( 44%)]  Loss: 3.653 (4.04)  Time: 0.778s, 1316.45/s  (0.792s, 1293.42/s)  LR: 8.658e-04  Data: 0.010 (0.014)
Train: 72 [ 600/1251 ( 48%)]  Loss: 3.506 (4.00)  Time: 0.842s, 1216.40/s  (0.792s, 1293.30/s)  LR: 8.658e-04  Data: 0.011 (0.014)
Train: 72 [ 650/1251 ( 52%)]  Loss: 3.959 (3.99)  Time: 0.775s, 1320.53/s  (0.792s, 1293.39/s)  LR: 8.658e-04  Data: 0.010 (0.014)
Train: 72 [ 700/1251 ( 56%)]  Loss: 4.038 (4.00)  Time: 0.789s, 1297.78/s  (0.792s, 1293.11/s)  LR: 8.658e-04  Data: 0.011 (0.014)
Train: 72 [ 750/1251 ( 60%)]  Loss: 3.816 (3.98)  Time: 0.779s, 1314.60/s  (0.792s, 1292.92/s)  LR: 8.658e-04  Data: 0.011 (0.014)
Train: 72 [ 800/1251 ( 64%)]  Loss: 3.984 (3.98)  Time: 0.781s, 1311.79/s  (0.792s, 1293.61/s)  LR: 8.658e-04  Data: 0.011 (0.013)
Train: 72 [ 850/1251 ( 68%)]  Loss: 3.842 (3.98)  Time: 0.797s, 1285.57/s  (0.792s, 1292.69/s)  LR: 8.658e-04  Data: 0.011 (0.013)
Train: 72 [ 900/1251 ( 72%)]  Loss: 4.022 (3.98)  Time: 0.819s, 1250.63/s  (0.792s, 1292.58/s)  LR: 8.658e-04  Data: 0.010 (0.013)
Train: 72 [ 950/1251 ( 76%)]  Loss: 4.187 (3.99)  Time: 0.777s, 1317.34/s  (0.792s, 1292.22/s)  LR: 8.658e-04  Data: 0.011 (0.013)
Train: 72 [1000/1251 ( 80%)]  Loss: 4.161 (4.00)  Time: 0.779s, 1315.34/s  (0.792s, 1292.59/s)  LR: 8.658e-04  Data: 0.011 (0.013)
Train: 72 [1050/1251 ( 84%)]  Loss: 3.906 (3.99)  Time: 0.788s, 1298.76/s  (0.792s, 1293.09/s)  LR: 8.658e-04  Data: 0.011 (0.013)
Train: 72 [1100/1251 ( 88%)]  Loss: 4.073 (4.00)  Time: 0.778s, 1316.28/s  (0.792s, 1293.03/s)  LR: 8.658e-04  Data: 0.011 (0.013)
Train: 72 [1150/1251 ( 92%)]  Loss: 3.733 (3.99)  Time: 0.782s, 1309.75/s  (0.792s, 1292.76/s)  LR: 8.658e-04  Data: 0.011 (0.013)
Train: 72 [1200/1251 ( 96%)]  Loss: 3.987 (3.99)  Time: 0.779s, 1315.26/s  (0.792s, 1293.57/s)  LR: 8.658e-04  Data: 0.011 (0.013)
Train: 72 [1250/1251 (100%)]  Loss: 4.000 (3.99)  Time: 0.774s, 1322.95/s  (0.792s, 1293.33/s)  LR: 8.658e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.549 (1.549)  Loss:  1.1567 (1.1567)  Acc@1: 86.1328 (86.1328)  Acc@5: 96.1914 (96.1914)
Test: [  48/48]  Time: 0.172 (0.563)  Loss:  1.1337 (1.7302)  Acc@1: 82.0755 (67.7620)  Acc@5: 94.9292 (88.5240)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-69.pth.tar', 68.6360000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-68.pth.tar', 68.42800005371093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-70.pth.tar', 68.22800007324219)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-71.pth.tar', 68.01600011474609)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-72.pth.tar', 67.76199997070313)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-67.pth.tar', 67.74799999267579)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-59.pth.tar', 67.72200004882812)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-63.pth.tar', 67.4960000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-62.pth.tar', 67.35800002685546)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-50.pth.tar', 67.34800010009765)

Train: 73 [   0/1251 (  0%)]  Loss: 4.111 (4.11)  Time: 2.251s,  454.90/s  (2.251s,  454.90/s)  LR: 8.623e-04  Data: 1.514 (1.514)
Train: 73 [  50/1251 (  4%)]  Loss: 3.738 (3.92)  Time: 0.812s, 1261.65/s  (0.824s, 1243.16/s)  LR: 8.623e-04  Data: 0.011 (0.041)
Train: 73 [ 100/1251 (  8%)]  Loss: 3.721 (3.86)  Time: 0.776s, 1318.85/s  (0.818s, 1251.12/s)  LR: 8.623e-04  Data: 0.011 (0.026)
Train: 73 [ 150/1251 ( 12%)]  Loss: 3.925 (3.87)  Time: 0.782s, 1310.11/s  (0.808s, 1267.04/s)  LR: 8.623e-04  Data: 0.010 (0.021)
Train: 73 [ 200/1251 ( 16%)]  Loss: 4.073 (3.91)  Time: 0.779s, 1315.05/s  (0.803s, 1274.85/s)  LR: 8.623e-04  Data: 0.012 (0.019)
Train: 73 [ 250/1251 ( 20%)]  Loss: 4.128 (3.95)  Time: 0.780s, 1312.92/s  (0.801s, 1277.62/s)  LR: 8.623e-04  Data: 0.011 (0.017)
Train: 73 [ 300/1251 ( 24%)]  Loss: 4.124 (3.97)  Time: 0.802s, 1276.18/s  (0.802s, 1276.83/s)  LR: 8.623e-04  Data: 0.011 (0.016)
Train: 73 [ 350/1251 ( 28%)]  Loss: 3.722 (3.94)  Time: 0.785s, 1303.72/s  (0.802s, 1276.97/s)  LR: 8.623e-04  Data: 0.011 (0.015)
Train: 73 [ 400/1251 ( 32%)]  Loss: 3.517 (3.90)  Time: 0.816s, 1255.32/s  (0.801s, 1278.13/s)  LR: 8.623e-04  Data: 0.011 (0.015)
Train: 73 [ 450/1251 ( 36%)]  Loss: 4.155 (3.92)  Time: 0.783s, 1308.05/s  (0.800s, 1279.22/s)  LR: 8.623e-04  Data: 0.012 (0.014)
Train: 73 [ 500/1251 ( 40%)]  Loss: 4.067 (3.93)  Time: 0.780s, 1313.46/s  (0.799s, 1281.39/s)  LR: 8.623e-04  Data: 0.011 (0.014)
Train: 73 [ 550/1251 ( 44%)]  Loss: 3.874 (3.93)  Time: 0.779s, 1314.13/s  (0.797s, 1284.06/s)  LR: 8.623e-04  Data: 0.011 (0.014)
Train: 73 [ 600/1251 ( 48%)]  Loss: 3.851 (3.92)  Time: 0.778s, 1316.95/s  (0.796s, 1285.93/s)  LR: 8.623e-04  Data: 0.012 (0.014)
Train: 73 [ 650/1251 ( 52%)]  Loss: 3.649 (3.90)  Time: 0.815s, 1256.18/s  (0.796s, 1285.92/s)  LR: 8.623e-04  Data: 0.011 (0.013)
Train: 73 [ 700/1251 ( 56%)]  Loss: 3.734 (3.89)  Time: 0.800s, 1280.17/s  (0.796s, 1287.03/s)  LR: 8.623e-04  Data: 0.012 (0.013)
Train: 73 [ 750/1251 ( 60%)]  Loss: 3.932 (3.90)  Time: 0.807s, 1269.48/s  (0.795s, 1288.29/s)  LR: 8.623e-04  Data: 0.011 (0.013)
Train: 73 [ 800/1251 ( 64%)]  Loss: 4.086 (3.91)  Time: 0.778s, 1316.01/s  (0.794s, 1288.97/s)  LR: 8.623e-04  Data: 0.011 (0.013)
Train: 73 [ 850/1251 ( 68%)]  Loss: 3.853 (3.90)  Time: 0.796s, 1286.83/s  (0.794s, 1289.55/s)  LR: 8.623e-04  Data: 0.011 (0.013)
Train: 73 [ 900/1251 ( 72%)]  Loss: 3.888 (3.90)  Time: 0.790s, 1296.38/s  (0.794s, 1289.83/s)  LR: 8.623e-04  Data: 0.011 (0.013)
Train: 73 [ 950/1251 ( 76%)]  Loss: 3.803 (3.90)  Time: 0.820s, 1249.22/s  (0.794s, 1289.62/s)  LR: 8.623e-04  Data: 0.011 (0.013)
Train: 73 [1000/1251 ( 80%)]  Loss: 3.883 (3.90)  Time: 0.815s, 1255.86/s  (0.794s, 1289.50/s)  LR: 8.623e-04  Data: 0.011 (0.012)
Train: 73 [1050/1251 ( 84%)]  Loss: 3.746 (3.89)  Time: 0.780s, 1313.56/s  (0.794s, 1289.62/s)  LR: 8.623e-04  Data: 0.011 (0.012)
Train: 73 [1100/1251 ( 88%)]  Loss: 4.179 (3.90)  Time: 0.835s, 1226.49/s  (0.794s, 1290.01/s)  LR: 8.623e-04  Data: 0.011 (0.012)
Train: 73 [1150/1251 ( 92%)]  Loss: 3.850 (3.90)  Time: 0.778s, 1315.66/s  (0.794s, 1290.22/s)  LR: 8.623e-04  Data: 0.012 (0.012)
Train: 73 [1200/1251 ( 96%)]  Loss: 4.033 (3.91)  Time: 0.815s, 1255.96/s  (0.794s, 1289.67/s)  LR: 8.623e-04  Data: 0.011 (0.012)
Train: 73 [1250/1251 (100%)]  Loss: 4.129 (3.91)  Time: 0.768s, 1332.56/s  (0.794s, 1289.81/s)  LR: 8.623e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.552 (1.552)  Loss:  1.0236 (1.0236)  Acc@1: 86.1328 (86.1328)  Acc@5: 95.7031 (95.7031)
Test: [  48/48]  Time: 0.172 (0.558)  Loss:  1.0369 (1.6499)  Acc@1: 81.4859 (68.0740)  Acc@5: 94.8113 (88.6420)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-69.pth.tar', 68.6360000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-68.pth.tar', 68.42800005371093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-70.pth.tar', 68.22800007324219)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-73.pth.tar', 68.07400010253906)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-71.pth.tar', 68.01600011474609)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-72.pth.tar', 67.76199997070313)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-67.pth.tar', 67.74799999267579)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-59.pth.tar', 67.72200004882812)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-63.pth.tar', 67.4960000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-62.pth.tar', 67.35800002685546)

Train: 74 [   0/1251 (  0%)]  Loss: 3.578 (3.58)  Time: 2.229s,  459.37/s  (2.229s,  459.37/s)  LR: 8.587e-04  Data: 1.494 (1.494)
Train: 74 [  50/1251 (  4%)]  Loss: 4.087 (3.83)  Time: 0.778s, 1316.71/s  (0.820s, 1248.61/s)  LR: 8.587e-04  Data: 0.011 (0.044)
Train: 74 [ 100/1251 (  8%)]  Loss: 4.114 (3.93)  Time: 0.814s, 1257.80/s  (0.816s, 1255.14/s)  LR: 8.587e-04  Data: 0.011 (0.027)
Train: 74 [ 150/1251 ( 12%)]  Loss: 4.129 (3.98)  Time: 0.779s, 1315.06/s  (0.808s, 1267.86/s)  LR: 8.587e-04  Data: 0.011 (0.022)
Train: 74 [ 200/1251 ( 16%)]  Loss: 3.693 (3.92)  Time: 0.807s, 1269.35/s  (0.803s, 1275.12/s)  LR: 8.587e-04  Data: 0.010 (0.019)
Train: 74 [ 250/1251 ( 20%)]  Loss: 3.459 (3.84)  Time: 0.776s, 1318.94/s  (0.800s, 1279.83/s)  LR: 8.587e-04  Data: 0.011 (0.018)
Train: 74 [ 300/1251 ( 24%)]  Loss: 4.100 (3.88)  Time: 0.789s, 1297.11/s  (0.798s, 1283.40/s)  LR: 8.587e-04  Data: 0.011 (0.016)
Train: 74 [ 350/1251 ( 28%)]  Loss: 3.472 (3.83)  Time: 0.780s, 1312.98/s  (0.796s, 1286.35/s)  LR: 8.587e-04  Data: 0.012 (0.016)
Train: 74 [ 400/1251 ( 32%)]  Loss: 3.453 (3.79)  Time: 0.777s, 1318.12/s  (0.795s, 1287.40/s)  LR: 8.587e-04  Data: 0.011 (0.015)
Train: 74 [ 450/1251 ( 36%)]  Loss: 3.888 (3.80)  Time: 0.778s, 1316.50/s  (0.794s, 1289.31/s)  LR: 8.587e-04  Data: 0.012 (0.015)
Train: 74 [ 500/1251 ( 40%)]  Loss: 4.163 (3.83)  Time: 0.791s, 1294.32/s  (0.793s, 1290.88/s)  LR: 8.587e-04  Data: 0.011 (0.014)
Train: 74 [ 550/1251 ( 44%)]  Loss: 4.047 (3.85)  Time: 0.781s, 1310.75/s  (0.793s, 1291.33/s)  LR: 8.587e-04  Data: 0.011 (0.014)
Train: 74 [ 600/1251 ( 48%)]  Loss: 3.751 (3.84)  Time: 0.779s, 1314.73/s  (0.793s, 1291.96/s)  LR: 8.587e-04  Data: 0.010 (0.014)
Train: 74 [ 650/1251 ( 52%)]  Loss: 3.853 (3.84)  Time: 0.815s, 1256.52/s  (0.792s, 1292.54/s)  LR: 8.587e-04  Data: 0.013 (0.013)
Train: 74 [ 700/1251 ( 56%)]  Loss: 4.163 (3.86)  Time: 0.774s, 1323.01/s  (0.793s, 1291.94/s)  LR: 8.587e-04  Data: 0.010 (0.013)
Train: 74 [ 750/1251 ( 60%)]  Loss: 4.107 (3.88)  Time: 0.779s, 1314.76/s  (0.792s, 1292.78/s)  LR: 8.587e-04  Data: 0.011 (0.013)
Train: 74 [ 800/1251 ( 64%)]  Loss: 4.072 (3.89)  Time: 0.782s, 1309.21/s  (0.792s, 1293.61/s)  LR: 8.587e-04  Data: 0.011 (0.013)
Train: 74 [ 850/1251 ( 68%)]  Loss: 3.369 (3.86)  Time: 0.778s, 1316.23/s  (0.792s, 1292.16/s)  LR: 8.587e-04  Data: 0.011 (0.013)
Train: 74 [ 900/1251 ( 72%)]  Loss: 3.990 (3.87)  Time: 0.782s, 1309.76/s  (0.792s, 1292.26/s)  LR: 8.587e-04  Data: 0.011 (0.013)
Train: 74 [ 950/1251 ( 76%)]  Loss: 4.098 (3.88)  Time: 0.779s, 1314.76/s  (0.792s, 1293.11/s)  LR: 8.587e-04  Data: 0.011 (0.013)
Train: 74 [1000/1251 ( 80%)]  Loss: 3.879 (3.88)  Time: 0.774s, 1322.96/s  (0.792s, 1292.83/s)  LR: 8.587e-04  Data: 0.010 (0.013)
Train: 74 [1050/1251 ( 84%)]  Loss: 3.717 (3.87)  Time: 0.779s, 1314.24/s  (0.792s, 1292.37/s)  LR: 8.587e-04  Data: 0.011 (0.013)
Train: 74 [1100/1251 ( 88%)]  Loss: 3.933 (3.87)  Time: 0.781s, 1311.07/s  (0.792s, 1293.02/s)  LR: 8.587e-04  Data: 0.012 (0.012)
Train: 74 [1150/1251 ( 92%)]  Loss: 3.538 (3.86)  Time: 0.781s, 1311.84/s  (0.792s, 1293.17/s)  LR: 8.587e-04  Data: 0.011 (0.012)
Train: 74 [1200/1251 ( 96%)]  Loss: 3.805 (3.86)  Time: 0.779s, 1315.35/s  (0.792s, 1293.37/s)  LR: 8.587e-04  Data: 0.010 (0.012)
Train: 74 [1250/1251 (100%)]  Loss: 4.119 (3.87)  Time: 0.802s, 1276.21/s  (0.792s, 1293.05/s)  LR: 8.587e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.589 (1.589)  Loss:  1.0125 (1.0125)  Acc@1: 85.8398 (85.8398)  Acc@5: 96.1914 (96.1914)
Test: [  48/48]  Time: 0.172 (0.568)  Loss:  1.0712 (1.6716)  Acc@1: 81.3679 (68.1640)  Acc@5: 94.3396 (88.4260)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-69.pth.tar', 68.6360000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-68.pth.tar', 68.42800005371093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-70.pth.tar', 68.22800007324219)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-74.pth.tar', 68.16400005126953)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-73.pth.tar', 68.07400010253906)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-71.pth.tar', 68.01600011474609)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-72.pth.tar', 67.76199997070313)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-67.pth.tar', 67.74799999267579)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-59.pth.tar', 67.72200004882812)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-63.pth.tar', 67.4960000024414)

Train: 75 [   0/1251 (  0%)]  Loss: 3.949 (3.95)  Time: 2.316s,  442.07/s  (2.316s,  442.07/s)  LR: 8.550e-04  Data: 1.582 (1.582)
Train: 75 [  50/1251 (  4%)]  Loss: 4.324 (4.14)  Time: 0.781s, 1311.59/s  (0.829s, 1235.85/s)  LR: 8.550e-04  Data: 0.014 (0.045)
Train: 75 [ 100/1251 (  8%)]  Loss: 4.091 (4.12)  Time: 0.822s, 1245.40/s  (0.813s, 1259.77/s)  LR: 8.550e-04  Data: 0.012 (0.028)
Train: 75 [ 150/1251 ( 12%)]  Loss: 3.821 (4.05)  Time: 0.816s, 1255.51/s  (0.808s, 1267.56/s)  LR: 8.550e-04  Data: 0.011 (0.022)
Train: 75 [ 200/1251 ( 16%)]  Loss: 4.086 (4.05)  Time: 0.815s, 1257.09/s  (0.804s, 1274.06/s)  LR: 8.550e-04  Data: 0.010 (0.019)
Train: 75 [ 250/1251 ( 20%)]  Loss: 3.862 (4.02)  Time: 0.797s, 1284.08/s  (0.804s, 1273.40/s)  LR: 8.550e-04  Data: 0.011 (0.018)
Train: 75 [ 300/1251 ( 24%)]  Loss: 4.110 (4.03)  Time: 0.781s, 1310.46/s  (0.802s, 1276.65/s)  LR: 8.550e-04  Data: 0.011 (0.017)
Train: 75 [ 350/1251 ( 28%)]  Loss: 3.979 (4.03)  Time: 0.778s, 1316.53/s  (0.799s, 1281.15/s)  LR: 8.550e-04  Data: 0.010 (0.016)
Train: 75 [ 400/1251 ( 32%)]  Loss: 3.871 (4.01)  Time: 0.777s, 1317.69/s  (0.797s, 1284.14/s)  LR: 8.550e-04  Data: 0.011 (0.015)
Train: 75 [ 450/1251 ( 36%)]  Loss: 3.642 (3.97)  Time: 0.794s, 1289.63/s  (0.796s, 1286.51/s)  LR: 8.550e-04  Data: 0.011 (0.015)
Train: 75 [ 500/1251 ( 40%)]  Loss: 3.926 (3.97)  Time: 0.775s, 1320.81/s  (0.797s, 1284.96/s)  LR: 8.550e-04  Data: 0.010 (0.014)
Train: 75 [ 550/1251 ( 44%)]  Loss: 3.752 (3.95)  Time: 0.812s, 1261.28/s  (0.797s, 1285.05/s)  LR: 8.550e-04  Data: 0.011 (0.014)
Train: 75 [ 600/1251 ( 48%)]  Loss: 4.186 (3.97)  Time: 0.812s, 1260.60/s  (0.799s, 1281.86/s)  LR: 8.550e-04  Data: 0.011 (0.014)
Train: 75 [ 650/1251 ( 52%)]  Loss: 3.834 (3.96)  Time: 0.821s, 1247.00/s  (0.798s, 1282.82/s)  LR: 8.550e-04  Data: 0.011 (0.014)
Train: 75 [ 700/1251 ( 56%)]  Loss: 3.961 (3.96)  Time: 0.778s, 1316.86/s  (0.799s, 1282.05/s)  LR: 8.550e-04  Data: 0.010 (0.013)
Train: 75 [ 750/1251 ( 60%)]  Loss: 3.833 (3.95)  Time: 0.778s, 1315.69/s  (0.798s, 1282.86/s)  LR: 8.550e-04  Data: 0.010 (0.013)
Train: 75 [ 800/1251 ( 64%)]  Loss: 3.887 (3.95)  Time: 0.820s, 1249.24/s  (0.799s, 1281.60/s)  LR: 8.550e-04  Data: 0.011 (0.013)
Train: 75 [ 850/1251 ( 68%)]  Loss: 3.736 (3.94)  Time: 0.810s, 1264.32/s  (0.800s, 1280.62/s)  LR: 8.550e-04  Data: 0.011 (0.013)
Train: 75 [ 900/1251 ( 72%)]  Loss: 4.138 (3.95)  Time: 0.776s, 1319.65/s  (0.799s, 1281.55/s)  LR: 8.550e-04  Data: 0.010 (0.013)
Train: 75 [ 950/1251 ( 76%)]  Loss: 4.240 (3.96)  Time: 0.777s, 1317.84/s  (0.798s, 1282.68/s)  LR: 8.550e-04  Data: 0.010 (0.013)
Train: 75 [1000/1251 ( 80%)]  Loss: 4.137 (3.97)  Time: 0.781s, 1310.57/s  (0.798s, 1283.13/s)  LR: 8.550e-04  Data: 0.011 (0.013)
Train: 75 [1050/1251 ( 84%)]  Loss: 3.998 (3.97)  Time: 0.815s, 1255.93/s  (0.798s, 1282.96/s)  LR: 8.550e-04  Data: 0.011 (0.013)
Train: 75 [1100/1251 ( 88%)]  Loss: 3.878 (3.97)  Time: 0.844s, 1213.54/s  (0.798s, 1283.60/s)  LR: 8.550e-04  Data: 0.011 (0.012)
Train: 75 [1150/1251 ( 92%)]  Loss: 3.917 (3.97)  Time: 0.786s, 1302.51/s  (0.797s, 1284.69/s)  LR: 8.550e-04  Data: 0.011 (0.012)
Train: 75 [1200/1251 ( 96%)]  Loss: 3.903 (3.96)  Time: 0.777s, 1317.18/s  (0.797s, 1285.03/s)  LR: 8.550e-04  Data: 0.011 (0.012)
Train: 75 [1250/1251 (100%)]  Loss: 3.802 (3.96)  Time: 0.802s, 1276.90/s  (0.797s, 1285.39/s)  LR: 8.550e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.541 (1.541)  Loss:  1.0871 (1.0871)  Acc@1: 84.6680 (84.6680)  Acc@5: 95.8008 (95.8008)
Test: [  48/48]  Time: 0.172 (0.562)  Loss:  1.1105 (1.6669)  Acc@1: 79.7170 (67.7660)  Acc@5: 93.9858 (88.3860)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-69.pth.tar', 68.6360000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-68.pth.tar', 68.42800005371093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-70.pth.tar', 68.22800007324219)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-74.pth.tar', 68.16400005126953)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-73.pth.tar', 68.07400010253906)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-71.pth.tar', 68.01600011474609)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-75.pth.tar', 67.76599998046875)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-72.pth.tar', 67.76199997070313)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-67.pth.tar', 67.74799999267579)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-59.pth.tar', 67.72200004882812)

Train: 76 [   0/1251 (  0%)]  Loss: 3.569 (3.57)  Time: 2.375s,  431.08/s  (2.375s,  431.08/s)  LR: 8.513e-04  Data: 1.642 (1.642)
Train: 76 [  50/1251 (  4%)]  Loss: 3.747 (3.66)  Time: 0.778s, 1315.50/s  (0.820s, 1249.32/s)  LR: 8.513e-04  Data: 0.012 (0.043)
Train: 76 [ 100/1251 (  8%)]  Loss: 4.107 (3.81)  Time: 0.779s, 1315.09/s  (0.802s, 1276.58/s)  LR: 8.513e-04  Data: 0.011 (0.027)
Train: 76 [ 150/1251 ( 12%)]  Loss: 3.918 (3.84)  Time: 0.785s, 1304.64/s  (0.798s, 1283.66/s)  LR: 8.513e-04  Data: 0.012 (0.022)
Train: 76 [ 200/1251 ( 16%)]  Loss: 3.528 (3.77)  Time: 0.791s, 1293.98/s  (0.796s, 1286.27/s)  LR: 8.513e-04  Data: 0.011 (0.019)
Train: 76 [ 250/1251 ( 20%)]  Loss: 4.214 (3.85)  Time: 0.782s, 1308.97/s  (0.794s, 1289.46/s)  LR: 8.513e-04  Data: 0.011 (0.018)
Train: 76 [ 300/1251 ( 24%)]  Loss: 4.301 (3.91)  Time: 0.781s, 1311.78/s  (0.794s, 1288.90/s)  LR: 8.513e-04  Data: 0.011 (0.016)
Train: 76 [ 350/1251 ( 28%)]  Loss: 3.627 (3.88)  Time: 0.794s, 1290.06/s  (0.794s, 1289.67/s)  LR: 8.513e-04  Data: 0.010 (0.016)
Train: 76 [ 400/1251 ( 32%)]  Loss: 3.783 (3.87)  Time: 0.835s, 1226.85/s  (0.794s, 1289.95/s)  LR: 8.513e-04  Data: 0.010 (0.015)
Train: 76 [ 450/1251 ( 36%)]  Loss: 3.550 (3.83)  Time: 0.778s, 1316.70/s  (0.794s, 1289.21/s)  LR: 8.513e-04  Data: 0.011 (0.015)
Train: 76 [ 500/1251 ( 40%)]  Loss: 3.941 (3.84)  Time: 0.777s, 1318.00/s  (0.793s, 1291.10/s)  LR: 8.513e-04  Data: 0.011 (0.014)
Train: 76 [ 550/1251 ( 44%)]  Loss: 4.082 (3.86)  Time: 0.778s, 1316.41/s  (0.793s, 1291.99/s)  LR: 8.513e-04  Data: 0.011 (0.014)
Train: 76 [ 600/1251 ( 48%)]  Loss: 3.996 (3.87)  Time: 0.841s, 1218.04/s  (0.793s, 1291.88/s)  LR: 8.513e-04  Data: 0.011 (0.014)
Train: 76 [ 650/1251 ( 52%)]  Loss: 4.041 (3.89)  Time: 0.778s, 1315.62/s  (0.792s, 1292.34/s)  LR: 8.513e-04  Data: 0.010 (0.013)
Train: 76 [ 700/1251 ( 56%)]  Loss: 4.178 (3.91)  Time: 0.778s, 1316.54/s  (0.793s, 1290.85/s)  LR: 8.513e-04  Data: 0.011 (0.013)
Train: 76 [ 750/1251 ( 60%)]  Loss: 4.252 (3.93)  Time: 0.777s, 1317.22/s  (0.794s, 1290.17/s)  LR: 8.513e-04  Data: 0.011 (0.013)
Train: 76 [ 800/1251 ( 64%)]  Loss: 4.283 (3.95)  Time: 0.844s, 1213.52/s  (0.794s, 1289.97/s)  LR: 8.513e-04  Data: 0.011 (0.013)
Train: 76 [ 850/1251 ( 68%)]  Loss: 3.708 (3.93)  Time: 0.845s, 1212.08/s  (0.795s, 1287.76/s)  LR: 8.513e-04  Data: 0.011 (0.013)
Train: 76 [ 900/1251 ( 72%)]  Loss: 3.950 (3.94)  Time: 0.838s, 1221.72/s  (0.797s, 1285.27/s)  LR: 8.513e-04  Data: 0.011 (0.013)
Train: 76 [ 950/1251 ( 76%)]  Loss: 4.078 (3.94)  Time: 0.780s, 1312.30/s  (0.797s, 1285.49/s)  LR: 8.513e-04  Data: 0.012 (0.013)
Train: 76 [1000/1251 ( 80%)]  Loss: 4.055 (3.95)  Time: 0.779s, 1315.08/s  (0.796s, 1285.91/s)  LR: 8.513e-04  Data: 0.011 (0.013)
Train: 76 [1050/1251 ( 84%)]  Loss: 3.773 (3.94)  Time: 0.779s, 1313.67/s  (0.796s, 1287.06/s)  LR: 8.513e-04  Data: 0.011 (0.012)
Train: 76 [1100/1251 ( 88%)]  Loss: 4.181 (3.95)  Time: 0.844s, 1212.98/s  (0.796s, 1287.09/s)  LR: 8.513e-04  Data: 0.011 (0.012)
Train: 76 [1150/1251 ( 92%)]  Loss: 4.285 (3.96)  Time: 0.777s, 1317.80/s  (0.795s, 1287.84/s)  LR: 8.513e-04  Data: 0.010 (0.012)
Train: 76 [1200/1251 ( 96%)]  Loss: 4.020 (3.97)  Time: 0.780s, 1312.60/s  (0.795s, 1288.68/s)  LR: 8.513e-04  Data: 0.011 (0.012)
Train: 76 [1250/1251 (100%)]  Loss: 3.658 (3.95)  Time: 0.807s, 1268.46/s  (0.796s, 1287.23/s)  LR: 8.513e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.508 (1.508)  Loss:  1.1197 (1.1197)  Acc@1: 84.6680 (84.6680)  Acc@5: 95.0195 (95.0195)
Test: [  48/48]  Time: 0.172 (0.565)  Loss:  1.2331 (1.7329)  Acc@1: 81.2500 (67.4800)  Acc@5: 92.3349 (88.0660)
Train: 77 [   0/1251 (  0%)]  Loss: 3.963 (3.96)  Time: 2.393s,  427.97/s  (2.393s,  427.97/s)  LR: 8.476e-04  Data: 1.655 (1.655)
Train: 77 [  50/1251 (  4%)]  Loss: 3.691 (3.83)  Time: 0.774s, 1323.59/s  (0.830s, 1233.73/s)  LR: 8.476e-04  Data: 0.010 (0.047)
Train: 77 [ 100/1251 (  8%)]  Loss: 3.990 (3.88)  Time: 0.818s, 1251.29/s  (0.821s, 1247.74/s)  LR: 8.476e-04  Data: 0.012 (0.029)
Train: 77 [ 150/1251 ( 12%)]  Loss: 4.070 (3.93)  Time: 0.815s, 1256.28/s  (0.812s, 1261.28/s)  LR: 8.476e-04  Data: 0.011 (0.023)
Train: 77 [ 200/1251 ( 16%)]  Loss: 3.871 (3.92)  Time: 0.778s, 1315.90/s  (0.810s, 1263.58/s)  LR: 8.476e-04  Data: 0.011 (0.020)
Train: 77 [ 250/1251 ( 20%)]  Loss: 3.731 (3.89)  Time: 0.787s, 1301.97/s  (0.805s, 1272.16/s)  LR: 8.476e-04  Data: 0.011 (0.018)
Train: 77 [ 300/1251 ( 24%)]  Loss: 3.806 (3.87)  Time: 0.776s, 1318.81/s  (0.802s, 1277.23/s)  LR: 8.476e-04  Data: 0.011 (0.017)
Train: 77 [ 350/1251 ( 28%)]  Loss: 3.909 (3.88)  Time: 0.844s, 1212.75/s  (0.802s, 1276.19/s)  LR: 8.476e-04  Data: 0.011 (0.016)
Train: 77 [ 400/1251 ( 32%)]  Loss: 3.454 (3.83)  Time: 0.777s, 1317.44/s  (0.801s, 1278.78/s)  LR: 8.476e-04  Data: 0.010 (0.016)
Train: 77 [ 450/1251 ( 36%)]  Loss: 3.637 (3.81)  Time: 0.819s, 1250.18/s  (0.800s, 1280.28/s)  LR: 8.476e-04  Data: 0.011 (0.015)
Train: 77 [ 500/1251 ( 40%)]  Loss: 3.775 (3.81)  Time: 0.780s, 1313.59/s  (0.798s, 1282.73/s)  LR: 8.476e-04  Data: 0.011 (0.015)
Train: 77 [ 550/1251 ( 44%)]  Loss: 3.466 (3.78)  Time: 0.782s, 1309.09/s  (0.797s, 1285.27/s)  LR: 8.476e-04  Data: 0.012 (0.014)
Train: 77 [ 600/1251 ( 48%)]  Loss: 3.784 (3.78)  Time: 0.777s, 1317.84/s  (0.796s, 1286.95/s)  LR: 8.476e-04  Data: 0.010 (0.014)
Train: 77 [ 650/1251 ( 52%)]  Loss: 3.862 (3.79)  Time: 0.879s, 1164.92/s  (0.795s, 1288.23/s)  LR: 8.476e-04  Data: 0.011 (0.014)
Train: 77 [ 700/1251 ( 56%)]  Loss: 4.171 (3.81)  Time: 0.777s, 1317.92/s  (0.796s, 1286.09/s)  LR: 8.476e-04  Data: 0.010 (0.014)
Train: 77 [ 750/1251 ( 60%)]  Loss: 3.602 (3.80)  Time: 0.776s, 1318.83/s  (0.795s, 1287.46/s)  LR: 8.476e-04  Data: 0.011 (0.013)
Train: 77 [ 800/1251 ( 64%)]  Loss: 4.027 (3.81)  Time: 0.777s, 1317.05/s  (0.795s, 1287.32/s)  LR: 8.476e-04  Data: 0.010 (0.013)
Train: 77 [ 850/1251 ( 68%)]  Loss: 3.710 (3.81)  Time: 0.782s, 1309.54/s  (0.795s, 1288.57/s)  LR: 8.476e-04  Data: 0.010 (0.013)
Train: 77 [ 900/1251 ( 72%)]  Loss: 3.904 (3.81)  Time: 0.816s, 1255.23/s  (0.795s, 1288.33/s)  LR: 8.476e-04  Data: 0.011 (0.013)
Train: 77 [ 950/1251 ( 76%)]  Loss: 3.556 (3.80)  Time: 0.813s, 1258.96/s  (0.796s, 1286.60/s)  LR: 8.476e-04  Data: 0.011 (0.013)
Train: 77 [1000/1251 ( 80%)]  Loss: 4.353 (3.83)  Time: 0.777s, 1317.12/s  (0.796s, 1286.65/s)  LR: 8.476e-04  Data: 0.011 (0.013)
Train: 77 [1050/1251 ( 84%)]  Loss: 4.224 (3.84)  Time: 0.788s, 1300.16/s  (0.796s, 1286.35/s)  LR: 8.476e-04  Data: 0.011 (0.013)
Train: 77 [1100/1251 ( 88%)]  Loss: 3.728 (3.84)  Time: 0.823s, 1244.65/s  (0.796s, 1285.86/s)  LR: 8.476e-04  Data: 0.010 (0.013)
Train: 77 [1150/1251 ( 92%)]  Loss: 3.906 (3.84)  Time: 0.778s, 1315.76/s  (0.796s, 1286.69/s)  LR: 8.476e-04  Data: 0.011 (0.013)
Train: 77 [1200/1251 ( 96%)]  Loss: 4.192 (3.86)  Time: 0.776s, 1318.93/s  (0.795s, 1287.49/s)  LR: 8.476e-04  Data: 0.011 (0.012)
Train: 77 [1250/1251 (100%)]  Loss: 3.851 (3.86)  Time: 0.765s, 1337.75/s  (0.795s, 1288.46/s)  LR: 8.476e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.513 (1.513)  Loss:  0.9843 (0.9843)  Acc@1: 85.7422 (85.7422)  Acc@5: 95.4102 (95.4102)
Test: [  48/48]  Time: 0.172 (0.558)  Loss:  1.1335 (1.6402)  Acc@1: 82.9009 (69.0340)  Acc@5: 94.3396 (89.0380)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-77.pth.tar', 69.03400007080079)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-69.pth.tar', 68.6360000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-68.pth.tar', 68.42800005371093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-70.pth.tar', 68.22800007324219)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-74.pth.tar', 68.16400005126953)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-73.pth.tar', 68.07400010253906)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-71.pth.tar', 68.01600011474609)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-75.pth.tar', 67.76599998046875)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-72.pth.tar', 67.76199997070313)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-67.pth.tar', 67.74799999267579)

Train: 78 [   0/1251 (  0%)]  Loss: 3.700 (3.70)  Time: 2.366s,  432.77/s  (2.366s,  432.77/s)  LR: 8.439e-04  Data: 1.568 (1.568)
Train: 78 [  50/1251 (  4%)]  Loss: 3.830 (3.76)  Time: 0.776s, 1319.40/s  (0.822s, 1246.26/s)  LR: 8.439e-04  Data: 0.011 (0.048)
Train: 78 [ 100/1251 (  8%)]  Loss: 4.101 (3.88)  Time: 0.777s, 1318.64/s  (0.804s, 1273.89/s)  LR: 8.439e-04  Data: 0.011 (0.030)
Train: 78 [ 150/1251 ( 12%)]  Loss: 4.099 (3.93)  Time: 0.779s, 1313.80/s  (0.797s, 1284.18/s)  LR: 8.439e-04  Data: 0.012 (0.024)
Train: 78 [ 200/1251 ( 16%)]  Loss: 4.081 (3.96)  Time: 0.777s, 1317.38/s  (0.795s, 1288.84/s)  LR: 8.439e-04  Data: 0.011 (0.021)
Train: 78 [ 250/1251 ( 20%)]  Loss: 4.033 (3.97)  Time: 0.789s, 1297.07/s  (0.800s, 1279.55/s)  LR: 8.439e-04  Data: 0.011 (0.019)
Train: 78 [ 300/1251 ( 24%)]  Loss: 3.843 (3.96)  Time: 0.786s, 1302.08/s  (0.798s, 1282.93/s)  LR: 8.439e-04  Data: 0.011 (0.018)
Train: 78 [ 350/1251 ( 28%)]  Loss: 3.785 (3.93)  Time: 0.788s, 1298.96/s  (0.796s, 1285.98/s)  LR: 8.439e-04  Data: 0.010 (0.017)
Train: 78 [ 400/1251 ( 32%)]  Loss: 4.053 (3.95)  Time: 0.776s, 1318.86/s  (0.796s, 1286.20/s)  LR: 8.439e-04  Data: 0.011 (0.016)
Train: 78 [ 450/1251 ( 36%)]  Loss: 3.463 (3.90)  Time: 0.779s, 1313.69/s  (0.796s, 1286.49/s)  LR: 8.439e-04  Data: 0.011 (0.016)
Train: 78 [ 500/1251 ( 40%)]  Loss: 3.953 (3.90)  Time: 0.778s, 1315.58/s  (0.796s, 1286.94/s)  LR: 8.439e-04  Data: 0.011 (0.015)
Train: 78 [ 550/1251 ( 44%)]  Loss: 3.895 (3.90)  Time: 0.778s, 1315.51/s  (0.796s, 1287.00/s)  LR: 8.439e-04  Data: 0.011 (0.015)
Train: 78 [ 600/1251 ( 48%)]  Loss: 3.664 (3.88)  Time: 0.781s, 1311.86/s  (0.797s, 1284.86/s)  LR: 8.439e-04  Data: 0.010 (0.014)
Train: 78 [ 650/1251 ( 52%)]  Loss: 4.110 (3.90)  Time: 0.791s, 1295.17/s  (0.797s, 1284.68/s)  LR: 8.439e-04  Data: 0.011 (0.014)
Train: 78 [ 700/1251 ( 56%)]  Loss: 4.171 (3.92)  Time: 0.828s, 1236.66/s  (0.798s, 1282.73/s)  LR: 8.439e-04  Data: 0.011 (0.014)
Train: 78 [ 750/1251 ( 60%)]  Loss: 4.135 (3.93)  Time: 0.799s, 1281.54/s  (0.798s, 1282.79/s)  LR: 8.439e-04  Data: 0.011 (0.014)
Train: 78 [ 800/1251 ( 64%)]  Loss: 3.846 (3.93)  Time: 0.838s, 1222.67/s  (0.798s, 1282.50/s)  LR: 8.439e-04  Data: 0.010 (0.013)
Train: 78 [ 850/1251 ( 68%)]  Loss: 4.069 (3.94)  Time: 0.783s, 1307.97/s  (0.798s, 1283.45/s)  LR: 8.439e-04  Data: 0.012 (0.013)
Train: 78 [ 900/1251 ( 72%)]  Loss: 4.079 (3.94)  Time: 0.791s, 1294.50/s  (0.798s, 1283.97/s)  LR: 8.439e-04  Data: 0.010 (0.013)
Train: 78 [ 950/1251 ( 76%)]  Loss: 3.643 (3.93)  Time: 0.781s, 1310.38/s  (0.798s, 1283.81/s)  LR: 8.439e-04  Data: 0.011 (0.013)
Train: 78 [1000/1251 ( 80%)]  Loss: 3.785 (3.92)  Time: 0.776s, 1319.40/s  (0.797s, 1284.92/s)  LR: 8.439e-04  Data: 0.011 (0.013)
Train: 78 [1050/1251 ( 84%)]  Loss: 3.652 (3.91)  Time: 0.781s, 1311.85/s  (0.796s, 1285.69/s)  LR: 8.439e-04  Data: 0.010 (0.013)
Train: 78 [1100/1251 ( 88%)]  Loss: 3.724 (3.90)  Time: 0.776s, 1318.88/s  (0.797s, 1285.22/s)  LR: 8.439e-04  Data: 0.011 (0.013)
Train: 78 [1150/1251 ( 92%)]  Loss: 4.165 (3.91)  Time: 0.801s, 1278.79/s  (0.797s, 1285.31/s)  LR: 8.439e-04  Data: 0.011 (0.013)
Train: 78 [1200/1251 ( 96%)]  Loss: 3.807 (3.91)  Time: 0.778s, 1315.39/s  (0.796s, 1285.80/s)  LR: 8.439e-04  Data: 0.011 (0.013)
Train: 78 [1250/1251 (100%)]  Loss: 3.858 (3.91)  Time: 0.807s, 1268.16/s  (0.797s, 1285.08/s)  LR: 8.439e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.503 (1.503)  Loss:  0.9554 (0.9554)  Acc@1: 85.8398 (85.8398)  Acc@5: 95.8008 (95.8008)
Test: [  48/48]  Time: 0.172 (0.561)  Loss:  0.9414 (1.5757)  Acc@1: 81.8396 (69.1980)  Acc@5: 93.7500 (89.1000)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-78.pth.tar', 69.1979999975586)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-77.pth.tar', 69.03400007080079)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-69.pth.tar', 68.6360000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-68.pth.tar', 68.42800005371093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-70.pth.tar', 68.22800007324219)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-74.pth.tar', 68.16400005126953)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-73.pth.tar', 68.07400010253906)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-71.pth.tar', 68.01600011474609)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-75.pth.tar', 67.76599998046875)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-72.pth.tar', 67.76199997070313)

Train: 79 [   0/1251 (  0%)]  Loss: 3.981 (3.98)  Time: 2.332s,  439.16/s  (2.332s,  439.16/s)  LR: 8.401e-04  Data: 1.596 (1.596)
Train: 79 [  50/1251 (  4%)]  Loss: 3.968 (3.97)  Time: 0.778s, 1316.95/s  (0.843s, 1214.04/s)  LR: 8.401e-04  Data: 0.011 (0.043)
Train: 79 [ 100/1251 (  8%)]  Loss: 3.831 (3.93)  Time: 0.783s, 1307.94/s  (0.814s, 1258.14/s)  LR: 8.401e-04  Data: 0.011 (0.027)
Train: 79 [ 150/1251 ( 12%)]  Loss: 4.038 (3.95)  Time: 0.798s, 1282.95/s  (0.810s, 1263.58/s)  LR: 8.401e-04  Data: 0.012 (0.022)
Train: 79 [ 200/1251 ( 16%)]  Loss: 4.146 (3.99)  Time: 0.779s, 1314.25/s  (0.804s, 1273.51/s)  LR: 8.401e-04  Data: 0.010 (0.019)
Train: 79 [ 250/1251 ( 20%)]  Loss: 3.988 (3.99)  Time: 0.778s, 1316.40/s  (0.799s, 1280.96/s)  LR: 8.401e-04  Data: 0.011 (0.017)
Train: 79 [ 300/1251 ( 24%)]  Loss: 3.805 (3.97)  Time: 0.816s, 1254.30/s  (0.801s, 1278.50/s)  LR: 8.401e-04  Data: 0.012 (0.016)
Train: 79 [ 350/1251 ( 28%)]  Loss: 3.830 (3.95)  Time: 0.777s, 1318.11/s  (0.800s, 1279.51/s)  LR: 8.401e-04  Data: 0.011 (0.015)
Train: 79 [ 400/1251 ( 32%)]  Loss: 3.891 (3.94)  Time: 0.790s, 1295.87/s  (0.799s, 1282.20/s)  LR: 8.401e-04  Data: 0.011 (0.015)
Train: 79 [ 450/1251 ( 36%)]  Loss: 3.888 (3.94)  Time: 0.777s, 1318.60/s  (0.797s, 1285.25/s)  LR: 8.401e-04  Data: 0.011 (0.014)
Train: 79 [ 500/1251 ( 40%)]  Loss: 4.115 (3.95)  Time: 0.779s, 1314.72/s  (0.796s, 1286.16/s)  LR: 8.401e-04  Data: 0.011 (0.014)
Train: 79 [ 550/1251 ( 44%)]  Loss: 3.817 (3.94)  Time: 0.796s, 1286.12/s  (0.796s, 1285.65/s)  LR: 8.401e-04  Data: 0.011 (0.014)
Train: 79 [ 600/1251 ( 48%)]  Loss: 4.235 (3.96)  Time: 0.822s, 1245.28/s  (0.797s, 1285.19/s)  LR: 8.401e-04  Data: 0.012 (0.014)
Train: 79 [ 650/1251 ( 52%)]  Loss: 3.846 (3.96)  Time: 0.777s, 1317.60/s  (0.798s, 1283.86/s)  LR: 8.401e-04  Data: 0.011 (0.013)
Train: 79 [ 700/1251 ( 56%)]  Loss: 3.749 (3.94)  Time: 0.795s, 1288.64/s  (0.797s, 1284.67/s)  LR: 8.401e-04  Data: 0.011 (0.013)
Train: 79 [ 750/1251 ( 60%)]  Loss: 3.838 (3.94)  Time: 0.782s, 1309.31/s  (0.796s, 1285.64/s)  LR: 8.401e-04  Data: 0.011 (0.013)
Train: 79 [ 800/1251 ( 64%)]  Loss: 4.005 (3.94)  Time: 0.816s, 1255.19/s  (0.797s, 1285.32/s)  LR: 8.401e-04  Data: 0.011 (0.013)
Train: 79 [ 850/1251 ( 68%)]  Loss: 3.787 (3.93)  Time: 0.820s, 1248.53/s  (0.797s, 1284.36/s)  LR: 8.401e-04  Data: 0.011 (0.013)
Train: 79 [ 900/1251 ( 72%)]  Loss: 3.841 (3.93)  Time: 0.776s, 1319.31/s  (0.798s, 1283.97/s)  LR: 8.401e-04  Data: 0.011 (0.013)
Train: 79 [ 950/1251 ( 76%)]  Loss: 3.741 (3.92)  Time: 0.776s, 1319.54/s  (0.797s, 1285.50/s)  LR: 8.401e-04  Data: 0.011 (0.013)
Train: 79 [1000/1251 ( 80%)]  Loss: 3.940 (3.92)  Time: 0.811s, 1262.78/s  (0.796s, 1286.19/s)  LR: 8.401e-04  Data: 0.011 (0.013)
Train: 79 [1050/1251 ( 84%)]  Loss: 3.992 (3.92)  Time: 0.778s, 1315.79/s  (0.796s, 1286.92/s)  LR: 8.401e-04  Data: 0.011 (0.012)
Train: 79 [1100/1251 ( 88%)]  Loss: 3.859 (3.92)  Time: 0.776s, 1320.19/s  (0.795s, 1287.99/s)  LR: 8.401e-04  Data: 0.011 (0.012)
Train: 79 [1150/1251 ( 92%)]  Loss: 3.890 (3.92)  Time: 0.777s, 1317.27/s  (0.795s, 1288.57/s)  LR: 8.401e-04  Data: 0.011 (0.012)
Train: 79 [1200/1251 ( 96%)]  Loss: 3.876 (3.92)  Time: 0.792s, 1293.73/s  (0.794s, 1289.47/s)  LR: 8.401e-04  Data: 0.010 (0.012)
Train: 79 [1250/1251 (100%)]  Loss: 3.860 (3.91)  Time: 0.770s, 1330.12/s  (0.794s, 1288.89/s)  LR: 8.401e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.505 (1.505)  Loss:  1.1341 (1.1341)  Acc@1: 86.8164 (86.8164)  Acc@5: 96.0938 (96.0938)
Test: [  48/48]  Time: 0.172 (0.565)  Loss:  1.0984 (1.6561)  Acc@1: 81.4859 (68.8120)  Acc@5: 94.9293 (89.1200)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-78.pth.tar', 69.1979999975586)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-77.pth.tar', 69.03400007080079)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-79.pth.tar', 68.81200010253906)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-69.pth.tar', 68.6360000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-68.pth.tar', 68.42800005371093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-70.pth.tar', 68.22800007324219)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-74.pth.tar', 68.16400005126953)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-73.pth.tar', 68.07400010253906)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-71.pth.tar', 68.01600011474609)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-75.pth.tar', 67.76599998046875)

Train: 80 [   0/1251 (  0%)]  Loss: 4.081 (4.08)  Time: 2.409s,  425.15/s  (2.409s,  425.15/s)  LR: 8.362e-04  Data: 1.657 (1.657)
Train: 80 [  50/1251 (  4%)]  Loss: 3.942 (4.01)  Time: 0.820s, 1248.06/s  (0.833s, 1229.67/s)  LR: 8.362e-04  Data: 0.011 (0.046)
Train: 80 [ 100/1251 (  8%)]  Loss: 4.231 (4.08)  Time: 0.780s, 1313.64/s  (0.814s, 1257.39/s)  LR: 8.362e-04  Data: 0.011 (0.029)
Train: 80 [ 150/1251 ( 12%)]  Loss: 4.225 (4.12)  Time: 0.837s, 1223.96/s  (0.805s, 1272.24/s)  LR: 8.362e-04  Data: 0.010 (0.023)
Train: 80 [ 200/1251 ( 16%)]  Loss: 3.962 (4.09)  Time: 0.777s, 1318.52/s  (0.807s, 1268.34/s)  LR: 8.362e-04  Data: 0.011 (0.020)
Train: 80 [ 250/1251 ( 20%)]  Loss: 3.923 (4.06)  Time: 0.839s, 1220.48/s  (0.803s, 1275.22/s)  LR: 8.362e-04  Data: 0.011 (0.018)
Train: 80 [ 300/1251 ( 24%)]  Loss: 3.698 (4.01)  Time: 0.781s, 1310.66/s  (0.805s, 1272.32/s)  LR: 8.362e-04  Data: 0.011 (0.017)
Train: 80 [ 350/1251 ( 28%)]  Loss: 3.903 (4.00)  Time: 0.840s, 1219.19/s  (0.803s, 1274.68/s)  LR: 8.362e-04  Data: 0.010 (0.016)
Train: 80 [ 400/1251 ( 32%)]  Loss: 3.816 (3.98)  Time: 0.779s, 1314.62/s  (0.806s, 1269.87/s)  LR: 8.362e-04  Data: 0.011 (0.016)
Train: 80 [ 450/1251 ( 36%)]  Loss: 4.303 (4.01)  Time: 0.780s, 1313.29/s  (0.804s, 1272.97/s)  LR: 8.362e-04  Data: 0.010 (0.015)
Train: 80 [ 500/1251 ( 40%)]  Loss: 3.954 (4.00)  Time: 0.778s, 1316.79/s  (0.803s, 1275.14/s)  LR: 8.362e-04  Data: 0.011 (0.015)
Train: 80 [ 550/1251 ( 44%)]  Loss: 4.144 (4.02)  Time: 0.792s, 1293.44/s  (0.801s, 1277.66/s)  LR: 8.362e-04  Data: 0.012 (0.014)
Train: 80 [ 600/1251 ( 48%)]  Loss: 3.935 (4.01)  Time: 0.779s, 1314.64/s  (0.801s, 1278.65/s)  LR: 8.362e-04  Data: 0.011 (0.014)
Train: 80 [ 650/1251 ( 52%)]  Loss: 4.009 (4.01)  Time: 0.776s, 1319.49/s  (0.800s, 1279.55/s)  LR: 8.362e-04  Data: 0.010 (0.014)
Train: 80 [ 700/1251 ( 56%)]  Loss: 3.767 (3.99)  Time: 0.775s, 1321.62/s  (0.799s, 1281.63/s)  LR: 8.362e-04  Data: 0.010 (0.014)
Train: 80 [ 750/1251 ( 60%)]  Loss: 4.035 (4.00)  Time: 0.776s, 1319.52/s  (0.800s, 1280.46/s)  LR: 8.362e-04  Data: 0.011 (0.013)
Train: 80 [ 800/1251 ( 64%)]  Loss: 3.690 (3.98)  Time: 0.850s, 1204.96/s  (0.800s, 1279.79/s)  LR: 8.362e-04  Data: 0.011 (0.013)
Train: 80 [ 850/1251 ( 68%)]  Loss: 3.719 (3.96)  Time: 0.786s, 1302.94/s  (0.800s, 1280.54/s)  LR: 8.362e-04  Data: 0.013 (0.013)
Train: 80 [ 900/1251 ( 72%)]  Loss: 3.928 (3.96)  Time: 0.779s, 1314.39/s  (0.799s, 1282.02/s)  LR: 8.362e-04  Data: 0.011 (0.013)
Train: 80 [ 950/1251 ( 76%)]  Loss: 4.049 (3.97)  Time: 0.803s, 1275.93/s  (0.798s, 1282.97/s)  LR: 8.362e-04  Data: 0.011 (0.013)
Train: 80 [1000/1251 ( 80%)]  Loss: 4.237 (3.98)  Time: 0.780s, 1313.06/s  (0.798s, 1282.95/s)  LR: 8.362e-04  Data: 0.011 (0.013)
Train: 80 [1050/1251 ( 84%)]  Loss: 3.788 (3.97)  Time: 0.785s, 1304.63/s  (0.797s, 1284.30/s)  LR: 8.362e-04  Data: 0.011 (0.013)
Train: 80 [1100/1251 ( 88%)]  Loss: 4.654 (4.00)  Time: 0.778s, 1315.96/s  (0.797s, 1285.01/s)  LR: 8.362e-04  Data: 0.010 (0.013)
Train: 80 [1150/1251 ( 92%)]  Loss: 3.799 (3.99)  Time: 0.778s, 1316.76/s  (0.796s, 1285.68/s)  LR: 8.362e-04  Data: 0.011 (0.013)
Train: 80 [1200/1251 ( 96%)]  Loss: 3.781 (3.98)  Time: 0.781s, 1311.52/s  (0.796s, 1286.54/s)  LR: 8.362e-04  Data: 0.011 (0.013)
Train: 80 [1250/1251 (100%)]  Loss: 3.620 (3.97)  Time: 0.769s, 1330.83/s  (0.795s, 1287.46/s)  LR: 8.362e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.510 (1.510)  Loss:  0.8863 (0.8863)  Acc@1: 84.8633 (84.8633)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.172 (0.564)  Loss:  1.0038 (1.5870)  Acc@1: 81.9575 (68.3900)  Acc@5: 94.6934 (88.7660)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-78.pth.tar', 69.1979999975586)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-77.pth.tar', 69.03400007080079)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-79.pth.tar', 68.81200010253906)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-69.pth.tar', 68.6360000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-68.pth.tar', 68.42800005371093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-80.pth.tar', 68.38999991943359)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-70.pth.tar', 68.22800007324219)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-74.pth.tar', 68.16400005126953)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-73.pth.tar', 68.07400010253906)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-71.pth.tar', 68.01600011474609)

Train: 81 [   0/1251 (  0%)]  Loss: 4.229 (4.23)  Time: 2.227s,  459.83/s  (2.227s,  459.83/s)  LR: 8.323e-04  Data: 1.493 (1.493)
Train: 81 [  50/1251 (  4%)]  Loss: 3.690 (3.96)  Time: 0.780s, 1313.03/s  (0.818s, 1251.44/s)  LR: 8.323e-04  Data: 0.011 (0.043)
Train: 81 [ 100/1251 (  8%)]  Loss: 3.557 (3.83)  Time: 0.776s, 1318.88/s  (0.801s, 1278.70/s)  LR: 8.323e-04  Data: 0.010 (0.027)
Train: 81 [ 150/1251 ( 12%)]  Loss: 4.133 (3.90)  Time: 0.784s, 1305.45/s  (0.796s, 1287.19/s)  LR: 8.323e-04  Data: 0.010 (0.022)
Train: 81 [ 200/1251 ( 16%)]  Loss: 3.996 (3.92)  Time: 0.777s, 1318.19/s  (0.795s, 1287.78/s)  LR: 8.323e-04  Data: 0.010 (0.019)
Train: 81 [ 250/1251 ( 20%)]  Loss: 3.763 (3.89)  Time: 0.823s, 1244.57/s  (0.797s, 1284.51/s)  LR: 8.323e-04  Data: 0.012 (0.017)
Train: 81 [ 300/1251 ( 24%)]  Loss: 4.048 (3.92)  Time: 0.778s, 1316.04/s  (0.797s, 1285.42/s)  LR: 8.323e-04  Data: 0.011 (0.016)
Train: 81 [ 350/1251 ( 28%)]  Loss: 4.137 (3.94)  Time: 0.777s, 1317.10/s  (0.795s, 1288.08/s)  LR: 8.323e-04  Data: 0.010 (0.016)
Train: 81 [ 400/1251 ( 32%)]  Loss: 4.288 (3.98)  Time: 0.778s, 1315.40/s  (0.794s, 1290.38/s)  LR: 8.323e-04  Data: 0.011 (0.015)
Train: 81 [ 450/1251 ( 36%)]  Loss: 3.963 (3.98)  Time: 0.783s, 1307.22/s  (0.796s, 1286.33/s)  LR: 8.323e-04  Data: 0.011 (0.015)
Train: 81 [ 500/1251 ( 40%)]  Loss: 3.457 (3.93)  Time: 0.813s, 1260.00/s  (0.795s, 1287.47/s)  LR: 8.323e-04  Data: 0.012 (0.014)
Train: 81 [ 550/1251 ( 44%)]  Loss: 3.774 (3.92)  Time: 0.821s, 1247.39/s  (0.797s, 1285.24/s)  LR: 8.323e-04  Data: 0.011 (0.014)
Train: 81 [ 600/1251 ( 48%)]  Loss: 3.747 (3.91)  Time: 0.780s, 1313.12/s  (0.796s, 1285.72/s)  LR: 8.323e-04  Data: 0.011 (0.014)
Train: 81 [ 650/1251 ( 52%)]  Loss: 3.595 (3.88)  Time: 0.783s, 1307.93/s  (0.797s, 1285.35/s)  LR: 8.323e-04  Data: 0.011 (0.013)
Train: 81 [ 700/1251 ( 56%)]  Loss: 3.798 (3.88)  Time: 0.777s, 1318.46/s  (0.798s, 1283.87/s)  LR: 8.323e-04  Data: 0.011 (0.013)
Train: 81 [ 750/1251 ( 60%)]  Loss: 3.828 (3.88)  Time: 0.783s, 1307.01/s  (0.797s, 1284.66/s)  LR: 8.323e-04  Data: 0.011 (0.013)
Train: 81 [ 800/1251 ( 64%)]  Loss: 3.948 (3.88)  Time: 0.786s, 1303.12/s  (0.798s, 1283.78/s)  LR: 8.323e-04  Data: 0.012 (0.013)
Train: 81 [ 850/1251 ( 68%)]  Loss: 4.003 (3.89)  Time: 0.779s, 1314.35/s  (0.797s, 1285.05/s)  LR: 8.323e-04  Data: 0.011 (0.013)
Train: 81 [ 900/1251 ( 72%)]  Loss: 3.933 (3.89)  Time: 0.778s, 1316.33/s  (0.797s, 1285.56/s)  LR: 8.323e-04  Data: 0.011 (0.013)
Train: 81 [ 950/1251 ( 76%)]  Loss: 4.056 (3.90)  Time: 0.784s, 1306.16/s  (0.796s, 1286.10/s)  LR: 8.323e-04  Data: 0.010 (0.013)
Train: 81 [1000/1251 ( 80%)]  Loss: 4.043 (3.90)  Time: 0.776s, 1319.46/s  (0.796s, 1286.96/s)  LR: 8.323e-04  Data: 0.010 (0.013)
Train: 81 [1050/1251 ( 84%)]  Loss: 4.343 (3.92)  Time: 0.777s, 1317.04/s  (0.795s, 1287.56/s)  LR: 8.323e-04  Data: 0.011 (0.012)
Train: 81 [1100/1251 ( 88%)]  Loss: 3.624 (3.91)  Time: 0.777s, 1318.00/s  (0.795s, 1288.25/s)  LR: 8.323e-04  Data: 0.010 (0.012)
Train: 81 [1150/1251 ( 92%)]  Loss: 3.855 (3.91)  Time: 0.817s, 1253.03/s  (0.794s, 1289.03/s)  LR: 8.323e-04  Data: 0.011 (0.012)
Train: 81 [1200/1251 ( 96%)]  Loss: 4.172 (3.92)  Time: 0.816s, 1254.32/s  (0.795s, 1288.44/s)  LR: 8.323e-04  Data: 0.010 (0.012)
Train: 81 [1250/1251 (100%)]  Loss: 3.815 (3.92)  Time: 0.800s, 1279.87/s  (0.796s, 1286.74/s)  LR: 8.323e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.506 (1.506)  Loss:  0.8266 (0.8266)  Acc@1: 87.8906 (87.8906)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.172 (0.559)  Loss:  1.0254 (1.5483)  Acc@1: 81.9576 (69.3460)  Acc@5: 95.5189 (89.3700)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-81.pth.tar', 69.34600004882813)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-78.pth.tar', 69.1979999975586)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-77.pth.tar', 69.03400007080079)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-79.pth.tar', 68.81200010253906)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-69.pth.tar', 68.6360000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-68.pth.tar', 68.42800005371093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-80.pth.tar', 68.38999991943359)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-70.pth.tar', 68.22800007324219)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-74.pth.tar', 68.16400005126953)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-73.pth.tar', 68.07400010253906)

Train: 82 [   0/1251 (  0%)]  Loss: 3.794 (3.79)  Time: 2.258s,  453.49/s  (2.258s,  453.49/s)  LR: 8.284e-04  Data: 1.524 (1.524)
Train: 82 [  50/1251 (  4%)]  Loss: 4.019 (3.91)  Time: 0.782s, 1308.87/s  (0.818s, 1252.25/s)  LR: 8.284e-04  Data: 0.014 (0.045)
Train: 82 [ 100/1251 (  8%)]  Loss: 3.960 (3.92)  Time: 0.778s, 1317.03/s  (0.805s, 1271.87/s)  LR: 8.284e-04  Data: 0.010 (0.028)
Train: 82 [ 150/1251 ( 12%)]  Loss: 3.926 (3.92)  Time: 0.847s, 1209.08/s  (0.802s, 1277.41/s)  LR: 8.284e-04  Data: 0.011 (0.023)
Train: 82 [ 200/1251 ( 16%)]  Loss: 4.144 (3.97)  Time: 0.816s, 1254.58/s  (0.802s, 1276.76/s)  LR: 8.284e-04  Data: 0.010 (0.020)
Train: 82 [ 250/1251 ( 20%)]  Loss: 3.921 (3.96)  Time: 0.780s, 1313.22/s  (0.803s, 1275.90/s)  LR: 8.284e-04  Data: 0.011 (0.018)
Train: 82 [ 300/1251 ( 24%)]  Loss: 3.734 (3.93)  Time: 0.778s, 1316.44/s  (0.799s, 1280.97/s)  LR: 8.284e-04  Data: 0.010 (0.017)
Train: 82 [ 350/1251 ( 28%)]  Loss: 3.970 (3.93)  Time: 0.841s, 1217.28/s  (0.798s, 1282.69/s)  LR: 8.284e-04  Data: 0.011 (0.016)
Train: 82 [ 400/1251 ( 32%)]  Loss: 4.115 (3.95)  Time: 0.776s, 1319.12/s  (0.799s, 1281.50/s)  LR: 8.284e-04  Data: 0.010 (0.015)
Train: 82 [ 450/1251 ( 36%)]  Loss: 3.862 (3.94)  Time: 0.782s, 1308.89/s  (0.797s, 1284.02/s)  LR: 8.284e-04  Data: 0.010 (0.015)
Train: 82 [ 500/1251 ( 40%)]  Loss: 4.170 (3.96)  Time: 0.779s, 1315.16/s  (0.796s, 1285.90/s)  LR: 8.284e-04  Data: 0.010 (0.015)
Train: 82 [ 550/1251 ( 44%)]  Loss: 4.251 (3.99)  Time: 0.784s, 1305.63/s  (0.796s, 1286.18/s)  LR: 8.284e-04  Data: 0.011 (0.014)
Train: 82 [ 600/1251 ( 48%)]  Loss: 3.811 (3.98)  Time: 0.779s, 1314.53/s  (0.795s, 1287.86/s)  LR: 8.284e-04  Data: 0.010 (0.014)
Train: 82 [ 650/1251 ( 52%)]  Loss: 3.970 (3.97)  Time: 0.780s, 1312.15/s  (0.794s, 1289.51/s)  LR: 8.284e-04  Data: 0.010 (0.014)
Train: 82 [ 700/1251 ( 56%)]  Loss: 3.987 (3.98)  Time: 0.844s, 1213.72/s  (0.796s, 1286.89/s)  LR: 8.284e-04  Data: 0.011 (0.014)
Train: 82 [ 750/1251 ( 60%)]  Loss: 3.492 (3.95)  Time: 0.781s, 1311.67/s  (0.797s, 1284.67/s)  LR: 8.284e-04  Data: 0.011 (0.013)
Train: 82 [ 800/1251 ( 64%)]  Loss: 3.727 (3.93)  Time: 0.837s, 1222.73/s  (0.798s, 1283.91/s)  LR: 8.284e-04  Data: 0.011 (0.013)
Train: 82 [ 850/1251 ( 68%)]  Loss: 4.029 (3.94)  Time: 0.799s, 1281.95/s  (0.798s, 1283.88/s)  LR: 8.284e-04  Data: 0.011 (0.013)
Train: 82 [ 900/1251 ( 72%)]  Loss: 4.080 (3.95)  Time: 0.833s, 1228.57/s  (0.798s, 1282.65/s)  LR: 8.284e-04  Data: 0.009 (0.013)
Train: 82 [ 950/1251 ( 76%)]  Loss: 3.445 (3.92)  Time: 0.776s, 1319.61/s  (0.798s, 1283.29/s)  LR: 8.284e-04  Data: 0.010 (0.013)
Train: 82 [1000/1251 ( 80%)]  Loss: 4.003 (3.92)  Time: 0.781s, 1310.85/s  (0.797s, 1284.66/s)  LR: 8.284e-04  Data: 0.011 (0.013)
Train: 82 [1050/1251 ( 84%)]  Loss: 3.895 (3.92)  Time: 0.840s, 1219.14/s  (0.797s, 1285.61/s)  LR: 8.284e-04  Data: 0.012 (0.013)
Train: 82 [1100/1251 ( 88%)]  Loss: 3.665 (3.91)  Time: 0.780s, 1312.21/s  (0.796s, 1286.04/s)  LR: 8.284e-04  Data: 0.011 (0.013)
Train: 82 [1150/1251 ( 92%)]  Loss: 3.804 (3.91)  Time: 0.785s, 1304.84/s  (0.796s, 1286.48/s)  LR: 8.284e-04  Data: 0.011 (0.013)
Train: 82 [1200/1251 ( 96%)]  Loss: 3.729 (3.90)  Time: 0.787s, 1301.95/s  (0.796s, 1286.91/s)  LR: 8.284e-04  Data: 0.011 (0.013)
Train: 82 [1250/1251 (100%)]  Loss: 3.908 (3.90)  Time: 0.770s, 1330.20/s  (0.795s, 1287.72/s)  LR: 8.284e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.503 (1.503)  Loss:  1.1185 (1.1185)  Acc@1: 84.7656 (84.7656)  Acc@5: 94.8242 (94.8242)
Test: [  48/48]  Time: 0.172 (0.568)  Loss:  1.2681 (1.7116)  Acc@1: 81.3679 (68.5240)  Acc@5: 94.9292 (88.9220)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-81.pth.tar', 69.34600004882813)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-78.pth.tar', 69.1979999975586)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-77.pth.tar', 69.03400007080079)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-79.pth.tar', 68.81200010253906)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-69.pth.tar', 68.6360000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-82.pth.tar', 68.52400005126952)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-68.pth.tar', 68.42800005371093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-80.pth.tar', 68.38999991943359)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-70.pth.tar', 68.22800007324219)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-74.pth.tar', 68.16400005126953)

Train: 83 [   0/1251 (  0%)]  Loss: 3.748 (3.75)  Time: 2.305s,  444.16/s  (2.305s,  444.16/s)  LR: 8.245e-04  Data: 1.572 (1.572)
Train: 83 [  50/1251 (  4%)]  Loss: 3.882 (3.82)  Time: 0.784s, 1306.88/s  (0.821s, 1246.88/s)  LR: 8.245e-04  Data: 0.011 (0.046)
Train: 83 [ 100/1251 (  8%)]  Loss: 3.867 (3.83)  Time: 0.778s, 1315.86/s  (0.801s, 1278.84/s)  LR: 8.245e-04  Data: 0.010 (0.029)
Train: 83 [ 150/1251 ( 12%)]  Loss: 3.770 (3.82)  Time: 0.809s, 1266.06/s  (0.797s, 1285.00/s)  LR: 8.245e-04  Data: 0.010 (0.023)
Train: 83 [ 200/1251 ( 16%)]  Loss: 3.915 (3.84)  Time: 0.808s, 1267.34/s  (0.794s, 1289.89/s)  LR: 8.245e-04  Data: 0.010 (0.020)
Train: 83 [ 250/1251 ( 20%)]  Loss: 3.730 (3.82)  Time: 0.786s, 1303.36/s  (0.799s, 1281.31/s)  LR: 8.245e-04  Data: 0.010 (0.018)
Train: 83 [ 300/1251 ( 24%)]  Loss: 3.950 (3.84)  Time: 0.815s, 1256.39/s  (0.797s, 1284.79/s)  LR: 8.245e-04  Data: 0.011 (0.017)
Train: 83 [ 350/1251 ( 28%)]  Loss: 4.306 (3.90)  Time: 0.782s, 1310.29/s  (0.795s, 1287.26/s)  LR: 8.245e-04  Data: 0.010 (0.016)
Train: 83 [ 400/1251 ( 32%)]  Loss: 4.196 (3.93)  Time: 0.780s, 1313.65/s  (0.797s, 1285.04/s)  LR: 8.245e-04  Data: 0.011 (0.015)
Train: 83 [ 450/1251 ( 36%)]  Loss: 3.794 (3.92)  Time: 0.814s, 1257.80/s  (0.796s, 1287.18/s)  LR: 8.245e-04  Data: 0.011 (0.015)
Train: 83 [ 500/1251 ( 40%)]  Loss: 3.945 (3.92)  Time: 0.830s, 1234.44/s  (0.795s, 1288.75/s)  LR: 8.245e-04  Data: 0.011 (0.015)
Train: 83 [ 550/1251 ( 44%)]  Loss: 3.898 (3.92)  Time: 0.778s, 1316.75/s  (0.795s, 1288.83/s)  LR: 8.245e-04  Data: 0.011 (0.014)
Train: 83 [ 600/1251 ( 48%)]  Loss: 4.256 (3.94)  Time: 0.795s, 1287.97/s  (0.794s, 1289.81/s)  LR: 8.245e-04  Data: 0.012 (0.014)
Train: 83 [ 650/1251 ( 52%)]  Loss: 4.289 (3.97)  Time: 0.816s, 1254.26/s  (0.794s, 1288.98/s)  LR: 8.245e-04  Data: 0.012 (0.014)
Train: 83 [ 700/1251 ( 56%)]  Loss: 3.947 (3.97)  Time: 0.777s, 1318.11/s  (0.794s, 1289.99/s)  LR: 8.245e-04  Data: 0.010 (0.014)
Train: 83 [ 750/1251 ( 60%)]  Loss: 3.934 (3.96)  Time: 0.826s, 1240.20/s  (0.793s, 1290.91/s)  LR: 8.245e-04  Data: 0.010 (0.013)
Train: 83 [ 800/1251 ( 64%)]  Loss: 3.925 (3.96)  Time: 0.783s, 1307.62/s  (0.794s, 1290.07/s)  LR: 8.245e-04  Data: 0.010 (0.013)
Train: 83 [ 850/1251 ( 68%)]  Loss: 3.617 (3.94)  Time: 0.803s, 1274.77/s  (0.794s, 1289.72/s)  LR: 8.245e-04  Data: 0.011 (0.013)
Train: 83 [ 900/1251 ( 72%)]  Loss: 4.014 (3.95)  Time: 0.777s, 1317.34/s  (0.794s, 1290.41/s)  LR: 8.245e-04  Data: 0.010 (0.013)
Train: 83 [ 950/1251 ( 76%)]  Loss: 3.712 (3.93)  Time: 0.794s, 1290.11/s  (0.794s, 1289.63/s)  LR: 8.245e-04  Data: 0.011 (0.013)
Train: 83 [1000/1251 ( 80%)]  Loss: 3.693 (3.92)  Time: 0.812s, 1261.23/s  (0.794s, 1288.90/s)  LR: 8.245e-04  Data: 0.011 (0.013)
Train: 83 [1050/1251 ( 84%)]  Loss: 3.774 (3.92)  Time: 0.815s, 1256.97/s  (0.796s, 1286.76/s)  LR: 8.245e-04  Data: 0.011 (0.013)
Train: 83 [1100/1251 ( 88%)]  Loss: 3.958 (3.92)  Time: 0.819s, 1250.34/s  (0.797s, 1284.58/s)  LR: 8.245e-04  Data: 0.011 (0.013)
Train: 83 [1150/1251 ( 92%)]  Loss: 4.147 (3.93)  Time: 0.777s, 1318.10/s  (0.797s, 1284.31/s)  LR: 8.245e-04  Data: 0.010 (0.012)
Train: 83 [1200/1251 ( 96%)]  Loss: 4.248 (3.94)  Time: 0.777s, 1317.07/s  (0.797s, 1284.98/s)  LR: 8.245e-04  Data: 0.011 (0.012)
Train: 83 [1250/1251 (100%)]  Loss: 4.031 (3.94)  Time: 0.776s, 1319.42/s  (0.797s, 1285.59/s)  LR: 8.245e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.576 (1.576)  Loss:  1.0434 (1.0434)  Acc@1: 84.8633 (84.8633)  Acc@5: 94.8242 (94.8242)
Test: [  48/48]  Time: 0.172 (0.558)  Loss:  1.1405 (1.5870)  Acc@1: 79.0094 (68.4060)  Acc@5: 93.3962 (88.8460)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-81.pth.tar', 69.34600004882813)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-78.pth.tar', 69.1979999975586)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-77.pth.tar', 69.03400007080079)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-79.pth.tar', 68.81200010253906)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-69.pth.tar', 68.6360000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-82.pth.tar', 68.52400005126952)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-68.pth.tar', 68.42800005371093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-83.pth.tar', 68.40599993164062)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-80.pth.tar', 68.38999991943359)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-70.pth.tar', 68.22800007324219)

Train: 84 [   0/1251 (  0%)]  Loss: 4.032 (4.03)  Time: 2.232s,  458.70/s  (2.232s,  458.70/s)  LR: 8.205e-04  Data: 1.497 (1.497)
Train: 84 [  50/1251 (  4%)]  Loss: 3.861 (3.95)  Time: 0.843s, 1214.95/s  (0.841s, 1217.67/s)  LR: 8.205e-04  Data: 0.011 (0.045)
Train: 84 [ 100/1251 (  8%)]  Loss: 3.907 (3.93)  Time: 0.789s, 1297.22/s  (0.816s, 1254.82/s)  LR: 8.205e-04  Data: 0.011 (0.028)
Train: 84 [ 150/1251 ( 12%)]  Loss: 3.961 (3.94)  Time: 0.785s, 1304.76/s  (0.811s, 1262.30/s)  LR: 8.205e-04  Data: 0.011 (0.023)
Train: 84 [ 200/1251 ( 16%)]  Loss: 3.979 (3.95)  Time: 0.780s, 1312.97/s  (0.806s, 1270.95/s)  LR: 8.205e-04  Data: 0.011 (0.020)
Train: 84 [ 250/1251 ( 20%)]  Loss: 3.816 (3.93)  Time: 0.790s, 1296.64/s  (0.801s, 1278.39/s)  LR: 8.205e-04  Data: 0.013 (0.018)
Train: 84 [ 300/1251 ( 24%)]  Loss: 3.968 (3.93)  Time: 0.797s, 1284.01/s  (0.798s, 1282.64/s)  LR: 8.205e-04  Data: 0.011 (0.017)
Train: 84 [ 350/1251 ( 28%)]  Loss: 4.046 (3.95)  Time: 0.779s, 1314.44/s  (0.796s, 1286.21/s)  LR: 8.205e-04  Data: 0.011 (0.016)
Train: 84 [ 400/1251 ( 32%)]  Loss: 3.984 (3.95)  Time: 0.779s, 1313.76/s  (0.795s, 1288.85/s)  LR: 8.205e-04  Data: 0.011 (0.016)
Train: 84 [ 450/1251 ( 36%)]  Loss: 4.309 (3.99)  Time: 0.778s, 1315.88/s  (0.793s, 1291.08/s)  LR: 8.205e-04  Data: 0.011 (0.015)
Train: 84 [ 500/1251 ( 40%)]  Loss: 4.207 (4.01)  Time: 0.787s, 1300.42/s  (0.792s, 1292.31/s)  LR: 8.205e-04  Data: 0.017 (0.015)
Train: 84 [ 550/1251 ( 44%)]  Loss: 3.606 (3.97)  Time: 0.775s, 1321.03/s  (0.792s, 1293.41/s)  LR: 8.205e-04  Data: 0.011 (0.015)
Train: 84 [ 600/1251 ( 48%)]  Loss: 4.101 (3.98)  Time: 0.777s, 1317.23/s  (0.791s, 1294.65/s)  LR: 8.205e-04  Data: 0.010 (0.014)
Train: 84 [ 650/1251 ( 52%)]  Loss: 3.832 (3.97)  Time: 0.838s, 1222.61/s  (0.791s, 1294.94/s)  LR: 8.205e-04  Data: 0.010 (0.014)
Train: 84 [ 700/1251 ( 56%)]  Loss: 3.803 (3.96)  Time: 0.776s, 1319.17/s  (0.792s, 1293.57/s)  LR: 8.205e-04  Data: 0.011 (0.014)
Train: 84 [ 750/1251 ( 60%)]  Loss: 3.934 (3.96)  Time: 0.777s, 1318.29/s  (0.791s, 1294.42/s)  LR: 8.205e-04  Data: 0.011 (0.014)
Train: 84 [ 800/1251 ( 64%)]  Loss: 3.718 (3.94)  Time: 0.780s, 1312.70/s  (0.792s, 1293.59/s)  LR: 8.205e-04  Data: 0.010 (0.013)
Train: 84 [ 850/1251 ( 68%)]  Loss: 3.793 (3.94)  Time: 0.797s, 1284.33/s  (0.791s, 1294.34/s)  LR: 8.205e-04  Data: 0.011 (0.013)
Train: 84 [ 900/1251 ( 72%)]  Loss: 3.936 (3.94)  Time: 0.789s, 1297.43/s  (0.791s, 1294.29/s)  LR: 8.205e-04  Data: 0.011 (0.013)
Train: 84 [ 950/1251 ( 76%)]  Loss: 4.165 (3.95)  Time: 0.800s, 1280.71/s  (0.792s, 1293.50/s)  LR: 8.205e-04  Data: 0.010 (0.013)
Train: 84 [1000/1251 ( 80%)]  Loss: 3.839 (3.94)  Time: 0.778s, 1316.02/s  (0.792s, 1293.14/s)  LR: 8.205e-04  Data: 0.011 (0.013)
Train: 84 [1050/1251 ( 84%)]  Loss: 3.882 (3.94)  Time: 0.779s, 1314.17/s  (0.792s, 1292.49/s)  LR: 8.205e-04  Data: 0.011 (0.013)
Train: 84 [1100/1251 ( 88%)]  Loss: 3.669 (3.93)  Time: 0.777s, 1317.10/s  (0.793s, 1291.66/s)  LR: 8.205e-04  Data: 0.011 (0.013)
Train: 84 [1150/1251 ( 92%)]  Loss: 4.244 (3.94)  Time: 0.831s, 1231.81/s  (0.792s, 1292.32/s)  LR: 8.205e-04  Data: 0.010 (0.013)
Train: 84 [1200/1251 ( 96%)]  Loss: 3.520 (3.92)  Time: 0.780s, 1313.51/s  (0.793s, 1291.47/s)  LR: 8.205e-04  Data: 0.011 (0.013)
Train: 84 [1250/1251 (100%)]  Loss: 4.189 (3.93)  Time: 0.797s, 1285.19/s  (0.793s, 1291.54/s)  LR: 8.205e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.502 (1.502)  Loss:  0.8945 (0.8945)  Acc@1: 84.9609 (84.9609)  Acc@5: 96.2891 (96.2891)
Test: [  48/48]  Time: 0.172 (0.570)  Loss:  1.1231 (1.6139)  Acc@1: 80.6604 (69.0660)  Acc@5: 95.1651 (89.1900)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-81.pth.tar', 69.34600004882813)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-78.pth.tar', 69.1979999975586)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-84.pth.tar', 69.06600013183593)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-77.pth.tar', 69.03400007080079)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-79.pth.tar', 68.81200010253906)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-69.pth.tar', 68.6360000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-82.pth.tar', 68.52400005126952)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-68.pth.tar', 68.42800005371093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-83.pth.tar', 68.40599993164062)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-80.pth.tar', 68.38999991943359)

Train: 85 [   0/1251 (  0%)]  Loss: 3.845 (3.84)  Time: 2.253s,  454.47/s  (2.253s,  454.47/s)  LR: 8.165e-04  Data: 1.519 (1.519)
Train: 85 [  50/1251 (  4%)]  Loss: 3.917 (3.88)  Time: 0.779s, 1314.04/s  (0.830s, 1234.03/s)  LR: 8.165e-04  Data: 0.011 (0.050)
Train: 85 [ 100/1251 (  8%)]  Loss: 3.658 (3.81)  Time: 0.783s, 1308.14/s  (0.808s, 1266.57/s)  LR: 8.165e-04  Data: 0.010 (0.031)
Train: 85 [ 150/1251 ( 12%)]  Loss: 3.709 (3.78)  Time: 0.781s, 1310.40/s  (0.801s, 1277.82/s)  LR: 8.165e-04  Data: 0.011 (0.024)
Train: 85 [ 200/1251 ( 16%)]  Loss: 4.237 (3.87)  Time: 0.783s, 1308.24/s  (0.797s, 1284.17/s)  LR: 8.165e-04  Data: 0.011 (0.021)
Train: 85 [ 250/1251 ( 20%)]  Loss: 3.814 (3.86)  Time: 0.838s, 1221.95/s  (0.795s, 1287.99/s)  LR: 8.165e-04  Data: 0.012 (0.019)
Train: 85 [ 300/1251 ( 24%)]  Loss: 3.752 (3.85)  Time: 0.777s, 1318.64/s  (0.793s, 1290.95/s)  LR: 8.165e-04  Data: 0.011 (0.018)
Train: 85 [ 350/1251 ( 28%)]  Loss: 3.948 (3.86)  Time: 0.779s, 1315.13/s  (0.792s, 1292.17/s)  LR: 8.165e-04  Data: 0.011 (0.017)
Train: 85 [ 400/1251 ( 32%)]  Loss: 3.951 (3.87)  Time: 0.777s, 1317.95/s  (0.792s, 1293.22/s)  LR: 8.165e-04  Data: 0.011 (0.016)
Train: 85 [ 450/1251 ( 36%)]  Loss: 3.548 (3.84)  Time: 0.779s, 1314.00/s  (0.791s, 1294.18/s)  LR: 8.165e-04  Data: 0.012 (0.015)
Train: 85 [ 500/1251 ( 40%)]  Loss: 3.914 (3.85)  Time: 0.793s, 1291.00/s  (0.791s, 1294.54/s)  LR: 8.165e-04  Data: 0.011 (0.015)
Train: 85 [ 550/1251 ( 44%)]  Loss: 3.470 (3.81)  Time: 0.779s, 1314.21/s  (0.791s, 1294.56/s)  LR: 8.165e-04  Data: 0.011 (0.015)
Train: 85 [ 600/1251 ( 48%)]  Loss: 4.099 (3.84)  Time: 0.800s, 1279.94/s  (0.791s, 1294.81/s)  LR: 8.165e-04  Data: 0.011 (0.014)
Train: 85 [ 650/1251 ( 52%)]  Loss: 3.763 (3.83)  Time: 0.823s, 1244.71/s  (0.791s, 1294.73/s)  LR: 8.165e-04  Data: 0.011 (0.014)
Train: 85 [ 700/1251 ( 56%)]  Loss: 4.273 (3.86)  Time: 0.813s, 1259.60/s  (0.793s, 1290.52/s)  LR: 8.165e-04  Data: 0.011 (0.014)
Train: 85 [ 750/1251 ( 60%)]  Loss: 4.002 (3.87)  Time: 0.815s, 1256.70/s  (0.795s, 1288.08/s)  LR: 8.165e-04  Data: 0.011 (0.014)
Train: 85 [ 800/1251 ( 64%)]  Loss: 3.746 (3.86)  Time: 0.783s, 1308.02/s  (0.795s, 1287.52/s)  LR: 8.165e-04  Data: 0.011 (0.014)
Train: 85 [ 850/1251 ( 68%)]  Loss: 3.896 (3.86)  Time: 0.782s, 1309.91/s  (0.795s, 1288.39/s)  LR: 8.165e-04  Data: 0.011 (0.013)
Train: 85 [ 900/1251 ( 72%)]  Loss: 3.758 (3.86)  Time: 0.778s, 1315.61/s  (0.794s, 1289.66/s)  LR: 8.165e-04  Data: 0.011 (0.013)
Train: 85 [ 950/1251 ( 76%)]  Loss: 4.210 (3.88)  Time: 0.817s, 1253.69/s  (0.794s, 1290.31/s)  LR: 8.165e-04  Data: 0.011 (0.013)
Train: 85 [1000/1251 ( 80%)]  Loss: 4.010 (3.88)  Time: 0.779s, 1314.81/s  (0.794s, 1290.44/s)  LR: 8.165e-04  Data: 0.011 (0.013)
Train: 85 [1050/1251 ( 84%)]  Loss: 3.921 (3.88)  Time: 0.780s, 1312.39/s  (0.793s, 1291.26/s)  LR: 8.165e-04  Data: 0.011 (0.013)
Train: 85 [1100/1251 ( 88%)]  Loss: 3.887 (3.88)  Time: 0.776s, 1319.01/s  (0.793s, 1290.81/s)  LR: 8.165e-04  Data: 0.011 (0.013)
Train: 85 [1150/1251 ( 92%)]  Loss: 3.808 (3.88)  Time: 0.778s, 1316.90/s  (0.793s, 1291.44/s)  LR: 8.165e-04  Data: 0.011 (0.013)
Train: 85 [1200/1251 ( 96%)]  Loss: 4.066 (3.89)  Time: 0.797s, 1285.23/s  (0.793s, 1291.13/s)  LR: 8.165e-04  Data: 0.011 (0.013)
Train: 85 [1250/1251 (100%)]  Loss: 3.622 (3.88)  Time: 0.785s, 1304.91/s  (0.793s, 1291.08/s)  LR: 8.165e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.484 (1.484)  Loss:  0.9480 (0.9480)  Acc@1: 83.8867 (83.8867)  Acc@5: 95.4102 (95.4102)
Test: [  48/48]  Time: 0.172 (0.575)  Loss:  1.1200 (1.6017)  Acc@1: 81.6038 (67.7120)  Acc@5: 93.1604 (88.4800)
Train: 86 [   0/1251 (  0%)]  Loss: 3.882 (3.88)  Time: 2.233s,  458.49/s  (2.233s,  458.49/s)  LR: 8.125e-04  Data: 1.496 (1.496)
Train: 86 [  50/1251 (  4%)]  Loss: 3.951 (3.92)  Time: 0.777s, 1317.10/s  (0.820s, 1248.85/s)  LR: 8.125e-04  Data: 0.010 (0.044)
Train: 86 [ 100/1251 (  8%)]  Loss: 3.921 (3.92)  Time: 0.820s, 1249.38/s  (0.805s, 1271.62/s)  LR: 8.125e-04  Data: 0.011 (0.028)
Train: 86 [ 150/1251 ( 12%)]  Loss: 4.026 (3.95)  Time: 0.790s, 1295.53/s  (0.801s, 1278.65/s)  LR: 8.125e-04  Data: 0.013 (0.022)
Train: 86 [ 200/1251 ( 16%)]  Loss: 3.483 (3.85)  Time: 0.827s, 1238.59/s  (0.798s, 1283.25/s)  LR: 8.125e-04  Data: 0.012 (0.020)
Train: 86 [ 250/1251 ( 20%)]  Loss: 4.079 (3.89)  Time: 0.778s, 1315.65/s  (0.797s, 1285.49/s)  LR: 8.125e-04  Data: 0.010 (0.018)
Train: 86 [ 300/1251 ( 24%)]  Loss: 3.872 (3.89)  Time: 0.779s, 1314.37/s  (0.795s, 1287.77/s)  LR: 8.125e-04  Data: 0.011 (0.017)
Train: 86 [ 350/1251 ( 28%)]  Loss: 4.470 (3.96)  Time: 0.780s, 1313.60/s  (0.793s, 1290.53/s)  LR: 8.125e-04  Data: 0.011 (0.016)
Train: 86 [ 400/1251 ( 32%)]  Loss: 3.784 (3.94)  Time: 0.787s, 1301.76/s  (0.792s, 1292.60/s)  LR: 8.125e-04  Data: 0.010 (0.015)
Train: 86 [ 450/1251 ( 36%)]  Loss: 3.913 (3.94)  Time: 0.819s, 1249.92/s  (0.792s, 1292.43/s)  LR: 8.125e-04  Data: 0.011 (0.015)
Train: 86 [ 500/1251 ( 40%)]  Loss: 4.010 (3.94)  Time: 0.779s, 1315.07/s  (0.792s, 1293.14/s)  LR: 8.125e-04  Data: 0.011 (0.014)
Train: 86 [ 550/1251 ( 44%)]  Loss: 4.061 (3.95)  Time: 0.778s, 1316.37/s  (0.791s, 1293.89/s)  LR: 8.125e-04  Data: 0.011 (0.014)
Train: 86 [ 600/1251 ( 48%)]  Loss: 3.914 (3.95)  Time: 0.781s, 1311.56/s  (0.791s, 1293.89/s)  LR: 8.125e-04  Data: 0.010 (0.014)
Train: 86 [ 650/1251 ( 52%)]  Loss: 3.884 (3.95)  Time: 0.778s, 1316.30/s  (0.792s, 1293.56/s)  LR: 8.125e-04  Data: 0.011 (0.013)
Train: 86 [ 700/1251 ( 56%)]  Loss: 3.834 (3.94)  Time: 0.839s, 1220.49/s  (0.792s, 1293.01/s)  LR: 8.125e-04  Data: 0.011 (0.013)
Train: 86 [ 750/1251 ( 60%)]  Loss: 3.944 (3.94)  Time: 0.778s, 1316.72/s  (0.793s, 1290.57/s)  LR: 8.125e-04  Data: 0.011 (0.013)
Train: 86 [ 800/1251 ( 64%)]  Loss: 3.750 (3.93)  Time: 0.778s, 1316.73/s  (0.793s, 1291.10/s)  LR: 8.125e-04  Data: 0.010 (0.013)
Train: 86 [ 850/1251 ( 68%)]  Loss: 3.806 (3.92)  Time: 0.781s, 1310.32/s  (0.792s, 1292.30/s)  LR: 8.125e-04  Data: 0.011 (0.013)
Train: 86 [ 900/1251 ( 72%)]  Loss: 3.940 (3.92)  Time: 0.815s, 1256.51/s  (0.792s, 1292.60/s)  LR: 8.125e-04  Data: 0.011 (0.013)
Train: 86 [ 950/1251 ( 76%)]  Loss: 4.205 (3.94)  Time: 0.781s, 1311.61/s  (0.793s, 1291.13/s)  LR: 8.125e-04  Data: 0.011 (0.013)
Train: 86 [1000/1251 ( 80%)]  Loss: 3.747 (3.93)  Time: 0.839s, 1220.36/s  (0.793s, 1291.82/s)  LR: 8.125e-04  Data: 0.011 (0.013)
Train: 86 [1050/1251 ( 84%)]  Loss: 4.102 (3.94)  Time: 0.787s, 1300.84/s  (0.792s, 1292.33/s)  LR: 8.125e-04  Data: 0.014 (0.013)
Train: 86 [1100/1251 ( 88%)]  Loss: 3.915 (3.93)  Time: 0.778s, 1315.98/s  (0.792s, 1292.96/s)  LR: 8.125e-04  Data: 0.011 (0.012)
Train: 86 [1150/1251 ( 92%)]  Loss: 4.030 (3.94)  Time: 0.780s, 1312.26/s  (0.792s, 1292.40/s)  LR: 8.125e-04  Data: 0.011 (0.012)
Train: 86 [1200/1251 ( 96%)]  Loss: 4.078 (3.94)  Time: 0.776s, 1320.38/s  (0.793s, 1291.63/s)  LR: 8.125e-04  Data: 0.010 (0.012)
Train: 86 [1250/1251 (100%)]  Loss: 3.872 (3.94)  Time: 0.798s, 1283.36/s  (0.793s, 1290.66/s)  LR: 8.125e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.559 (1.559)  Loss:  1.0494 (1.0494)  Acc@1: 84.4727 (84.4727)  Acc@5: 94.8242 (94.8242)
Test: [  48/48]  Time: 0.172 (0.565)  Loss:  1.1170 (1.6474)  Acc@1: 80.0708 (67.7040)  Acc@5: 94.8113 (88.5020)
Train: 87 [   0/1251 (  0%)]  Loss: 3.619 (3.62)  Time: 2.350s,  435.80/s  (2.350s,  435.80/s)  LR: 8.084e-04  Data: 1.615 (1.615)
Train: 87 [  50/1251 (  4%)]  Loss: 3.788 (3.70)  Time: 0.786s, 1303.60/s  (0.826s, 1239.96/s)  LR: 8.084e-04  Data: 0.011 (0.046)
Train: 87 [ 100/1251 (  8%)]  Loss: 3.838 (3.75)  Time: 0.798s, 1282.76/s  (0.805s, 1271.77/s)  LR: 8.084e-04  Data: 0.010 (0.029)
Train: 87 [ 150/1251 ( 12%)]  Loss: 3.855 (3.77)  Time: 0.782s, 1309.24/s  (0.798s, 1283.70/s)  LR: 8.084e-04  Data: 0.011 (0.023)
Train: 87 [ 200/1251 ( 16%)]  Loss: 3.611 (3.74)  Time: 0.779s, 1313.87/s  (0.795s, 1287.29/s)  LR: 8.084e-04  Data: 0.011 (0.020)
Train: 87 [ 250/1251 ( 20%)]  Loss: 3.928 (3.77)  Time: 0.777s, 1318.60/s  (0.793s, 1291.23/s)  LR: 8.084e-04  Data: 0.010 (0.018)
Train: 87 [ 300/1251 ( 24%)]  Loss: 3.532 (3.74)  Time: 0.854s, 1198.67/s  (0.793s, 1291.73/s)  LR: 8.084e-04  Data: 0.010 (0.017)
Train: 87 [ 350/1251 ( 28%)]  Loss: 3.883 (3.76)  Time: 0.785s, 1303.66/s  (0.792s, 1293.16/s)  LR: 8.084e-04  Data: 0.012 (0.016)
Train: 87 [ 400/1251 ( 32%)]  Loss: 3.744 (3.76)  Time: 0.779s, 1314.81/s  (0.794s, 1290.05/s)  LR: 8.084e-04  Data: 0.010 (0.015)
Train: 87 [ 450/1251 ( 36%)]  Loss: 4.004 (3.78)  Time: 0.778s, 1316.87/s  (0.793s, 1291.82/s)  LR: 8.084e-04  Data: 0.011 (0.015)
Train: 87 [ 500/1251 ( 40%)]  Loss: 3.934 (3.79)  Time: 0.782s, 1310.05/s  (0.792s, 1293.14/s)  LR: 8.084e-04  Data: 0.012 (0.015)
Train: 87 [ 550/1251 ( 44%)]  Loss: 3.917 (3.80)  Time: 0.793s, 1290.71/s  (0.792s, 1292.82/s)  LR: 8.084e-04  Data: 0.011 (0.014)
Train: 87 [ 600/1251 ( 48%)]  Loss: 3.897 (3.81)  Time: 0.776s, 1319.17/s  (0.791s, 1294.24/s)  LR: 8.084e-04  Data: 0.011 (0.014)
Train: 87 [ 650/1251 ( 52%)]  Loss: 4.212 (3.84)  Time: 0.777s, 1318.36/s  (0.792s, 1292.77/s)  LR: 8.084e-04  Data: 0.011 (0.014)
Train: 87 [ 700/1251 ( 56%)]  Loss: 3.695 (3.83)  Time: 0.777s, 1317.13/s  (0.792s, 1292.32/s)  LR: 8.084e-04  Data: 0.011 (0.014)
Train: 87 [ 750/1251 ( 60%)]  Loss: 4.094 (3.85)  Time: 0.778s, 1315.39/s  (0.792s, 1293.02/s)  LR: 8.084e-04  Data: 0.012 (0.013)
Train: 87 [ 800/1251 ( 64%)]  Loss: 4.033 (3.86)  Time: 0.833s, 1229.23/s  (0.792s, 1293.02/s)  LR: 8.084e-04  Data: 0.010 (0.013)
Train: 87 [ 850/1251 ( 68%)]  Loss: 3.641 (3.85)  Time: 0.822s, 1245.58/s  (0.792s, 1293.10/s)  LR: 8.084e-04  Data: 0.012 (0.013)
Train: 87 [ 900/1251 ( 72%)]  Loss: 3.821 (3.84)  Time: 0.777s, 1317.46/s  (0.792s, 1293.72/s)  LR: 8.084e-04  Data: 0.011 (0.013)
Train: 87 [ 950/1251 ( 76%)]  Loss: 4.050 (3.85)  Time: 0.801s, 1278.49/s  (0.792s, 1293.02/s)  LR: 8.084e-04  Data: 0.011 (0.013)
Train: 87 [1000/1251 ( 80%)]  Loss: 4.130 (3.87)  Time: 0.811s, 1262.06/s  (0.792s, 1292.89/s)  LR: 8.084e-04  Data: 0.010 (0.013)
Train: 87 [1050/1251 ( 84%)]  Loss: 4.242 (3.88)  Time: 0.778s, 1316.16/s  (0.792s, 1292.39/s)  LR: 8.084e-04  Data: 0.011 (0.013)
Train: 87 [1100/1251 ( 88%)]  Loss: 4.269 (3.90)  Time: 0.839s, 1220.93/s  (0.793s, 1292.09/s)  LR: 8.084e-04  Data: 0.010 (0.013)
Train: 87 [1150/1251 ( 92%)]  Loss: 3.741 (3.89)  Time: 0.816s, 1254.17/s  (0.793s, 1291.76/s)  LR: 8.084e-04  Data: 0.013 (0.013)
Train: 87 [1200/1251 ( 96%)]  Loss: 4.044 (3.90)  Time: 0.780s, 1313.35/s  (0.793s, 1290.60/s)  LR: 8.084e-04  Data: 0.012 (0.012)
Train: 87 [1250/1251 (100%)]  Loss: 4.352 (3.92)  Time: 0.782s, 1309.28/s  (0.793s, 1290.83/s)  LR: 8.084e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.540 (1.540)  Loss:  1.1219 (1.1219)  Acc@1: 84.8633 (84.8633)  Acc@5: 94.5312 (94.5312)
Test: [  48/48]  Time: 0.172 (0.558)  Loss:  1.1713 (1.6657)  Acc@1: 80.6604 (68.8760)  Acc@5: 93.1604 (89.2440)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-81.pth.tar', 69.34600004882813)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-78.pth.tar', 69.1979999975586)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-84.pth.tar', 69.06600013183593)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-77.pth.tar', 69.03400007080079)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-87.pth.tar', 68.8760000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-79.pth.tar', 68.81200010253906)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-69.pth.tar', 68.6360000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-82.pth.tar', 68.52400005126952)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-68.pth.tar', 68.42800005371093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-83.pth.tar', 68.40599993164062)

Train: 88 [   0/1251 (  0%)]  Loss: 3.928 (3.93)  Time: 2.342s,  437.16/s  (2.342s,  437.16/s)  LR: 8.043e-04  Data: 1.608 (1.608)
Train: 88 [  50/1251 (  4%)]  Loss: 3.621 (3.77)  Time: 0.784s, 1305.71/s  (0.824s, 1243.24/s)  LR: 8.043e-04  Data: 0.012 (0.046)
Train: 88 [ 100/1251 (  8%)]  Loss: 3.883 (3.81)  Time: 0.776s, 1319.31/s  (0.808s, 1267.60/s)  LR: 8.043e-04  Data: 0.011 (0.029)
Train: 88 [ 150/1251 ( 12%)]  Loss: 3.675 (3.78)  Time: 0.850s, 1205.18/s  (0.804s, 1273.23/s)  LR: 8.043e-04  Data: 0.011 (0.023)
Train: 88 [ 200/1251 ( 16%)]  Loss: 3.577 (3.74)  Time: 0.784s, 1306.68/s  (0.801s, 1277.77/s)  LR: 8.043e-04  Data: 0.011 (0.020)
Train: 88 [ 250/1251 ( 20%)]  Loss: 4.037 (3.79)  Time: 0.839s, 1219.95/s  (0.803s, 1275.59/s)  LR: 8.043e-04  Data: 0.011 (0.018)
Train: 88 [ 300/1251 ( 24%)]  Loss: 3.200 (3.70)  Time: 0.820s, 1249.28/s  (0.800s, 1279.81/s)  LR: 8.043e-04  Data: 0.011 (0.017)
Train: 88 [ 350/1251 ( 28%)]  Loss: 4.281 (3.78)  Time: 0.780s, 1312.15/s  (0.797s, 1284.15/s)  LR: 8.043e-04  Data: 0.011 (0.016)
Train: 88 [ 400/1251 ( 32%)]  Loss: 4.030 (3.80)  Time: 0.779s, 1315.22/s  (0.796s, 1286.92/s)  LR: 8.043e-04  Data: 0.011 (0.016)
Train: 88 [ 450/1251 ( 36%)]  Loss: 4.427 (3.87)  Time: 0.778s, 1316.27/s  (0.794s, 1289.71/s)  LR: 8.043e-04  Data: 0.011 (0.015)
Train: 88 [ 500/1251 ( 40%)]  Loss: 3.632 (3.84)  Time: 0.800s, 1279.61/s  (0.794s, 1289.38/s)  LR: 8.043e-04  Data: 0.011 (0.015)
Train: 88 [ 550/1251 ( 44%)]  Loss: 4.161 (3.87)  Time: 0.814s, 1257.94/s  (0.794s, 1289.30/s)  LR: 8.043e-04  Data: 0.011 (0.014)
Train: 88 [ 600/1251 ( 48%)]  Loss: 3.658 (3.85)  Time: 0.789s, 1298.27/s  (0.794s, 1289.11/s)  LR: 8.043e-04  Data: 0.011 (0.014)
Train: 88 [ 650/1251 ( 52%)]  Loss: 3.890 (3.86)  Time: 0.838s, 1222.19/s  (0.795s, 1288.83/s)  LR: 8.043e-04  Data: 0.011 (0.014)
Train: 88 [ 700/1251 ( 56%)]  Loss: 3.806 (3.85)  Time: 0.799s, 1281.59/s  (0.796s, 1286.57/s)  LR: 8.043e-04  Data: 0.011 (0.014)
Train: 88 [ 750/1251 ( 60%)]  Loss: 3.932 (3.86)  Time: 0.780s, 1312.70/s  (0.796s, 1286.38/s)  LR: 8.043e-04  Data: 0.011 (0.013)
Train: 88 [ 800/1251 ( 64%)]  Loss: 3.765 (3.85)  Time: 0.838s, 1222.37/s  (0.796s, 1286.17/s)  LR: 8.043e-04  Data: 0.011 (0.013)
Train: 88 [ 850/1251 ( 68%)]  Loss: 3.770 (3.85)  Time: 0.783s, 1307.45/s  (0.796s, 1287.20/s)  LR: 8.043e-04  Data: 0.011 (0.013)
Train: 88 [ 900/1251 ( 72%)]  Loss: 4.021 (3.86)  Time: 0.782s, 1309.11/s  (0.795s, 1288.01/s)  LR: 8.043e-04  Data: 0.011 (0.013)
Train: 88 [ 950/1251 ( 76%)]  Loss: 3.741 (3.85)  Time: 0.779s, 1314.06/s  (0.795s, 1288.63/s)  LR: 8.043e-04  Data: 0.010 (0.013)
Train: 88 [1000/1251 ( 80%)]  Loss: 4.239 (3.87)  Time: 0.785s, 1304.01/s  (0.794s, 1289.00/s)  LR: 8.043e-04  Data: 0.012 (0.013)
Train: 88 [1050/1251 ( 84%)]  Loss: 3.648 (3.86)  Time: 0.824s, 1242.95/s  (0.795s, 1287.67/s)  LR: 8.043e-04  Data: 0.011 (0.013)
Train: 88 [1100/1251 ( 88%)]  Loss: 3.731 (3.85)  Time: 0.805s, 1271.97/s  (0.795s, 1287.59/s)  LR: 8.043e-04  Data: 0.011 (0.013)
Train: 88 [1150/1251 ( 92%)]  Loss: 4.064 (3.86)  Time: 0.788s, 1300.27/s  (0.795s, 1287.63/s)  LR: 8.043e-04  Data: 0.011 (0.013)
Train: 88 [1200/1251 ( 96%)]  Loss: 3.728 (3.86)  Time: 0.813s, 1259.55/s  (0.795s, 1287.62/s)  LR: 8.043e-04  Data: 0.012 (0.013)
Train: 88 [1250/1251 (100%)]  Loss: 4.158 (3.87)  Time: 0.798s, 1283.46/s  (0.796s, 1286.30/s)  LR: 8.043e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.513 (1.513)  Loss:  1.3162 (1.3162)  Acc@1: 83.1055 (83.1055)  Acc@5: 95.1172 (95.1172)
Test: [  48/48]  Time: 0.172 (0.562)  Loss:  1.4044 (1.7779)  Acc@1: 81.2500 (68.2320)  Acc@5: 93.2783 (88.6320)
Train: 89 [   0/1251 (  0%)]  Loss: 3.789 (3.79)  Time: 2.480s,  412.84/s  (2.480s,  412.84/s)  LR: 8.001e-04  Data: 1.746 (1.746)
Train: 89 [  50/1251 (  4%)]  Loss: 3.929 (3.86)  Time: 0.809s, 1265.66/s  (0.845s, 1211.43/s)  LR: 8.001e-04  Data: 0.010 (0.049)
Train: 89 [ 100/1251 (  8%)]  Loss: 3.387 (3.70)  Time: 0.790s, 1295.53/s  (0.818s, 1251.33/s)  LR: 8.001e-04  Data: 0.011 (0.030)
Train: 89 [ 150/1251 ( 12%)]  Loss: 4.086 (3.80)  Time: 0.781s, 1310.64/s  (0.807s, 1269.55/s)  LR: 8.001e-04  Data: 0.011 (0.024)
Train: 89 [ 200/1251 ( 16%)]  Loss: 3.837 (3.81)  Time: 0.808s, 1267.19/s  (0.809s, 1266.36/s)  LR: 8.001e-04  Data: 0.011 (0.020)
Train: 89 [ 250/1251 ( 20%)]  Loss: 3.712 (3.79)  Time: 0.817s, 1253.49/s  (0.810s, 1264.72/s)  LR: 8.001e-04  Data: 0.010 (0.018)
Train: 89 [ 300/1251 ( 24%)]  Loss: 3.303 (3.72)  Time: 0.780s, 1312.66/s  (0.807s, 1269.14/s)  LR: 8.001e-04  Data: 0.011 (0.017)
Train: 89 [ 350/1251 ( 28%)]  Loss: 3.672 (3.71)  Time: 0.834s, 1228.29/s  (0.804s, 1274.12/s)  LR: 8.001e-04  Data: 0.011 (0.016)
Train: 89 [ 400/1251 ( 32%)]  Loss: 3.993 (3.75)  Time: 0.778s, 1315.69/s  (0.801s, 1278.33/s)  LR: 8.001e-04  Data: 0.011 (0.016)
Train: 89 [ 450/1251 ( 36%)]  Loss: 4.104 (3.78)  Time: 0.811s, 1262.82/s  (0.801s, 1278.65/s)  LR: 8.001e-04  Data: 0.011 (0.015)
Train: 89 [ 500/1251 ( 40%)]  Loss: 4.311 (3.83)  Time: 0.832s, 1230.04/s  (0.801s, 1278.45/s)  LR: 8.001e-04  Data: 0.011 (0.015)
Train: 89 [ 550/1251 ( 44%)]  Loss: 4.072 (3.85)  Time: 0.828s, 1237.24/s  (0.802s, 1277.18/s)  LR: 8.001e-04  Data: 0.011 (0.014)
Train: 89 [ 600/1251 ( 48%)]  Loss: 3.909 (3.85)  Time: 0.775s, 1321.12/s  (0.802s, 1277.50/s)  LR: 8.001e-04  Data: 0.010 (0.014)
Train: 89 [ 650/1251 ( 52%)]  Loss: 3.429 (3.82)  Time: 0.840s, 1219.18/s  (0.801s, 1278.07/s)  LR: 8.001e-04  Data: 0.010 (0.014)
Train: 89 [ 700/1251 ( 56%)]  Loss: 3.851 (3.83)  Time: 0.775s, 1320.50/s  (0.800s, 1279.98/s)  LR: 8.001e-04  Data: 0.011 (0.014)
Train: 89 [ 750/1251 ( 60%)]  Loss: 3.927 (3.83)  Time: 0.778s, 1315.76/s  (0.799s, 1281.85/s)  LR: 8.001e-04  Data: 0.011 (0.013)
Train: 89 [ 800/1251 ( 64%)]  Loss: 3.976 (3.84)  Time: 0.812s, 1260.94/s  (0.799s, 1282.21/s)  LR: 8.001e-04  Data: 0.011 (0.013)
Train: 89 [ 850/1251 ( 68%)]  Loss: 3.875 (3.84)  Time: 0.778s, 1316.52/s  (0.799s, 1281.82/s)  LR: 8.001e-04  Data: 0.010 (0.013)
Train: 89 [ 900/1251 ( 72%)]  Loss: 3.751 (3.84)  Time: 0.779s, 1314.55/s  (0.798s, 1282.89/s)  LR: 8.001e-04  Data: 0.011 (0.013)
Train: 89 [ 950/1251 ( 76%)]  Loss: 3.771 (3.83)  Time: 0.780s, 1312.81/s  (0.798s, 1283.98/s)  LR: 8.001e-04  Data: 0.011 (0.013)
Train: 89 [1000/1251 ( 80%)]  Loss: 4.293 (3.86)  Time: 0.795s, 1287.26/s  (0.797s, 1284.57/s)  LR: 8.001e-04  Data: 0.011 (0.013)
Train: 89 [1050/1251 ( 84%)]  Loss: 4.030 (3.86)  Time: 0.777s, 1317.20/s  (0.797s, 1284.91/s)  LR: 8.001e-04  Data: 0.010 (0.013)
Train: 89 [1100/1251 ( 88%)]  Loss: 3.984 (3.87)  Time: 0.781s, 1310.97/s  (0.797s, 1285.45/s)  LR: 8.001e-04  Data: 0.011 (0.013)
Train: 89 [1150/1251 ( 92%)]  Loss: 3.896 (3.87)  Time: 0.779s, 1314.30/s  (0.796s, 1285.77/s)  LR: 8.001e-04  Data: 0.011 (0.012)
Train: 89 [1200/1251 ( 96%)]  Loss: 4.026 (3.88)  Time: 0.780s, 1312.01/s  (0.796s, 1286.17/s)  LR: 8.001e-04  Data: 0.011 (0.012)
Train: 89 [1250/1251 (100%)]  Loss: 3.970 (3.88)  Time: 0.767s, 1334.92/s  (0.796s, 1286.13/s)  LR: 8.001e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.537 (1.537)  Loss:  1.0121 (1.0121)  Acc@1: 85.3516 (85.3516)  Acc@5: 95.7031 (95.7031)
Test: [  48/48]  Time: 0.172 (0.567)  Loss:  1.0249 (1.6441)  Acc@1: 82.6651 (68.6660)  Acc@5: 95.1651 (88.9800)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-81.pth.tar', 69.34600004882813)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-78.pth.tar', 69.1979999975586)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-84.pth.tar', 69.06600013183593)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-77.pth.tar', 69.03400007080079)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-87.pth.tar', 68.8760000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-79.pth.tar', 68.81200010253906)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-89.pth.tar', 68.66599996826172)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-69.pth.tar', 68.6360000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-82.pth.tar', 68.52400005126952)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-68.pth.tar', 68.42800005371093)

Train: 90 [   0/1251 (  0%)]  Loss: 4.353 (4.35)  Time: 2.286s,  447.94/s  (2.286s,  447.94/s)  LR: 7.960e-04  Data: 1.473 (1.473)
Train: 90 [  50/1251 (  4%)]  Loss: 3.646 (4.00)  Time: 0.777s, 1317.55/s  (0.825s, 1241.36/s)  LR: 7.960e-04  Data: 0.011 (0.044)
Train: 90 [ 100/1251 (  8%)]  Loss: 3.874 (3.96)  Time: 0.799s, 1281.77/s  (0.812s, 1261.20/s)  LR: 7.960e-04  Data: 0.011 (0.027)
Train: 90 [ 150/1251 ( 12%)]  Loss: 3.944 (3.95)  Time: 0.815s, 1256.61/s  (0.804s, 1273.19/s)  LR: 7.960e-04  Data: 0.011 (0.022)
Train: 90 [ 200/1251 ( 16%)]  Loss: 3.992 (3.96)  Time: 0.810s, 1264.03/s  (0.800s, 1279.80/s)  LR: 7.960e-04  Data: 0.011 (0.019)
Train: 90 [ 250/1251 ( 20%)]  Loss: 3.811 (3.94)  Time: 0.777s, 1317.07/s  (0.799s, 1282.08/s)  LR: 7.960e-04  Data: 0.011 (0.018)
Train: 90 [ 300/1251 ( 24%)]  Loss: 3.917 (3.93)  Time: 0.844s, 1212.65/s  (0.797s, 1285.02/s)  LR: 7.960e-04  Data: 0.010 (0.017)
Train: 90 [ 350/1251 ( 28%)]  Loss: 3.734 (3.91)  Time: 0.778s, 1315.55/s  (0.798s, 1283.81/s)  LR: 7.960e-04  Data: 0.011 (0.016)
Train: 90 [ 400/1251 ( 32%)]  Loss: 3.983 (3.92)  Time: 0.783s, 1307.14/s  (0.796s, 1287.02/s)  LR: 7.960e-04  Data: 0.011 (0.015)
Train: 90 [ 450/1251 ( 36%)]  Loss: 3.538 (3.88)  Time: 0.777s, 1318.36/s  (0.794s, 1289.02/s)  LR: 7.960e-04  Data: 0.011 (0.015)
Train: 90 [ 500/1251 ( 40%)]  Loss: 4.060 (3.90)  Time: 0.783s, 1307.87/s  (0.794s, 1290.10/s)  LR: 7.960e-04  Data: 0.011 (0.015)
Train: 90 [ 550/1251 ( 44%)]  Loss: 3.893 (3.90)  Time: 0.778s, 1315.68/s  (0.795s, 1288.30/s)  LR: 7.960e-04  Data: 0.011 (0.014)
Train: 90 [ 600/1251 ( 48%)]  Loss: 3.883 (3.89)  Time: 0.778s, 1316.00/s  (0.794s, 1289.86/s)  LR: 7.960e-04  Data: 0.011 (0.014)
Train: 90 [ 650/1251 ( 52%)]  Loss: 4.021 (3.90)  Time: 0.777s, 1317.12/s  (0.793s, 1291.17/s)  LR: 7.960e-04  Data: 0.011 (0.014)
Train: 90 [ 700/1251 ( 56%)]  Loss: 3.969 (3.91)  Time: 0.778s, 1316.15/s  (0.793s, 1292.05/s)  LR: 7.960e-04  Data: 0.011 (0.014)
Train: 90 [ 750/1251 ( 60%)]  Loss: 3.975 (3.91)  Time: 0.788s, 1298.74/s  (0.792s, 1293.16/s)  LR: 7.960e-04  Data: 0.011 (0.014)
Train: 90 [ 800/1251 ( 64%)]  Loss: 4.134 (3.93)  Time: 0.840s, 1219.10/s  (0.793s, 1291.91/s)  LR: 7.960e-04  Data: 0.010 (0.013)
Train: 90 [ 850/1251 ( 68%)]  Loss: 3.682 (3.91)  Time: 0.781s, 1311.01/s  (0.793s, 1291.06/s)  LR: 7.960e-04  Data: 0.010 (0.013)
Train: 90 [ 900/1251 ( 72%)]  Loss: 3.996 (3.92)  Time: 0.788s, 1300.20/s  (0.793s, 1291.85/s)  LR: 7.960e-04  Data: 0.011 (0.013)
Train: 90 [ 950/1251 ( 76%)]  Loss: 3.420 (3.89)  Time: 0.779s, 1315.34/s  (0.793s, 1291.73/s)  LR: 7.960e-04  Data: 0.011 (0.013)
Train: 90 [1000/1251 ( 80%)]  Loss: 3.879 (3.89)  Time: 0.777s, 1318.00/s  (0.792s, 1292.26/s)  LR: 7.960e-04  Data: 0.011 (0.013)
Train: 90 [1050/1251 ( 84%)]  Loss: 3.856 (3.89)  Time: 0.782s, 1309.22/s  (0.792s, 1292.20/s)  LR: 7.960e-04  Data: 0.011 (0.013)
Train: 90 [1100/1251 ( 88%)]  Loss: 3.966 (3.89)  Time: 0.846s, 1209.96/s  (0.793s, 1292.01/s)  LR: 7.960e-04  Data: 0.011 (0.013)
Train: 90 [1150/1251 ( 92%)]  Loss: 4.021 (3.90)  Time: 0.812s, 1260.42/s  (0.793s, 1291.08/s)  LR: 7.960e-04  Data: 0.010 (0.013)
Train: 90 [1200/1251 ( 96%)]  Loss: 4.173 (3.91)  Time: 0.815s, 1255.75/s  (0.794s, 1289.49/s)  LR: 7.960e-04  Data: 0.010 (0.012)
Train: 90 [1250/1251 (100%)]  Loss: 3.515 (3.89)  Time: 0.768s, 1333.47/s  (0.794s, 1289.17/s)  LR: 7.960e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.537 (1.537)  Loss:  1.0256 (1.0256)  Acc@1: 84.5703 (84.5703)  Acc@5: 95.0195 (95.0195)
Test: [  48/48]  Time: 0.172 (0.567)  Loss:  0.9996 (1.6588)  Acc@1: 83.2547 (68.6800)  Acc@5: 96.4623 (89.0260)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-81.pth.tar', 69.34600004882813)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-78.pth.tar', 69.1979999975586)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-84.pth.tar', 69.06600013183593)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-77.pth.tar', 69.03400007080079)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-87.pth.tar', 68.8760000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-79.pth.tar', 68.81200010253906)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-90.pth.tar', 68.67999996582031)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-89.pth.tar', 68.66599996826172)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-69.pth.tar', 68.6360000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-82.pth.tar', 68.52400005126952)

Train: 91 [   0/1251 (  0%)]  Loss: 3.799 (3.80)  Time: 2.260s,  453.14/s  (2.260s,  453.14/s)  LR: 7.917e-04  Data: 1.524 (1.524)
Train: 91 [  50/1251 (  4%)]  Loss: 4.218 (4.01)  Time: 0.812s, 1261.40/s  (0.843s, 1214.54/s)  LR: 7.917e-04  Data: 0.010 (0.046)
Train: 91 [ 100/1251 (  8%)]  Loss: 3.902 (3.97)  Time: 0.792s, 1292.35/s  (0.824s, 1242.14/s)  LR: 7.917e-04  Data: 0.014 (0.029)
Train: 91 [ 150/1251 ( 12%)]  Loss: 3.835 (3.94)  Time: 0.812s, 1260.55/s  (0.818s, 1252.52/s)  LR: 7.917e-04  Data: 0.011 (0.023)
Train: 91 [ 200/1251 ( 16%)]  Loss: 3.874 (3.93)  Time: 0.794s, 1289.77/s  (0.812s, 1261.29/s)  LR: 7.917e-04  Data: 0.011 (0.020)
Train: 91 [ 250/1251 ( 20%)]  Loss: 3.419 (3.84)  Time: 0.780s, 1312.05/s  (0.807s, 1268.41/s)  LR: 7.917e-04  Data: 0.011 (0.018)
Train: 91 [ 300/1251 ( 24%)]  Loss: 3.678 (3.82)  Time: 0.780s, 1313.49/s  (0.804s, 1274.36/s)  LR: 7.917e-04  Data: 0.012 (0.017)
Train: 91 [ 350/1251 ( 28%)]  Loss: 3.959 (3.84)  Time: 0.776s, 1319.80/s  (0.801s, 1278.37/s)  LR: 7.917e-04  Data: 0.010 (0.016)
Train: 91 [ 400/1251 ( 32%)]  Loss: 4.011 (3.86)  Time: 0.797s, 1284.38/s  (0.800s, 1279.53/s)  LR: 7.917e-04  Data: 0.011 (0.015)
Train: 91 [ 450/1251 ( 36%)]  Loss: 4.135 (3.88)  Time: 0.780s, 1312.39/s  (0.800s, 1280.48/s)  LR: 7.917e-04  Data: 0.011 (0.015)
Train: 91 [ 500/1251 ( 40%)]  Loss: 4.140 (3.91)  Time: 0.778s, 1315.49/s  (0.798s, 1283.43/s)  LR: 7.917e-04  Data: 0.011 (0.015)
Train: 91 [ 550/1251 ( 44%)]  Loss: 3.869 (3.90)  Time: 0.823s, 1244.29/s  (0.799s, 1281.68/s)  LR: 7.917e-04  Data: 0.012 (0.014)
Train: 91 [ 600/1251 ( 48%)]  Loss: 3.844 (3.90)  Time: 0.778s, 1316.85/s  (0.798s, 1283.58/s)  LR: 7.917e-04  Data: 0.011 (0.014)
Train: 91 [ 650/1251 ( 52%)]  Loss: 3.966 (3.90)  Time: 0.787s, 1301.76/s  (0.798s, 1283.75/s)  LR: 7.917e-04  Data: 0.012 (0.014)
Train: 91 [ 700/1251 ( 56%)]  Loss: 3.918 (3.90)  Time: 0.781s, 1311.52/s  (0.797s, 1285.23/s)  LR: 7.917e-04  Data: 0.011 (0.014)
Train: 91 [ 750/1251 ( 60%)]  Loss: 3.683 (3.89)  Time: 0.779s, 1314.84/s  (0.797s, 1285.02/s)  LR: 7.917e-04  Data: 0.011 (0.013)
Train: 91 [ 800/1251 ( 64%)]  Loss: 4.046 (3.90)  Time: 0.818s, 1252.31/s  (0.798s, 1283.99/s)  LR: 7.917e-04  Data: 0.011 (0.013)
Train: 91 [ 850/1251 ( 68%)]  Loss: 3.842 (3.90)  Time: 0.807s, 1268.45/s  (0.798s, 1283.91/s)  LR: 7.917e-04  Data: 0.012 (0.013)
Train: 91 [ 900/1251 ( 72%)]  Loss: 4.140 (3.91)  Time: 0.808s, 1268.07/s  (0.798s, 1283.76/s)  LR: 7.917e-04  Data: 0.012 (0.013)
Train: 91 [ 950/1251 ( 76%)]  Loss: 3.960 (3.91)  Time: 0.782s, 1309.92/s  (0.798s, 1283.73/s)  LR: 7.917e-04  Data: 0.010 (0.013)
Train: 91 [1000/1251 ( 80%)]  Loss: 3.589 (3.90)  Time: 0.784s, 1306.33/s  (0.798s, 1283.69/s)  LR: 7.917e-04  Data: 0.010 (0.013)
Train: 91 [1050/1251 ( 84%)]  Loss: 3.980 (3.90)  Time: 0.814s, 1258.34/s  (0.798s, 1283.43/s)  LR: 7.917e-04  Data: 0.012 (0.013)
Train: 91 [1100/1251 ( 88%)]  Loss: 3.577 (3.89)  Time: 0.781s, 1310.48/s  (0.798s, 1283.13/s)  LR: 7.917e-04  Data: 0.010 (0.013)
Train: 91 [1150/1251 ( 92%)]  Loss: 3.980 (3.89)  Time: 0.778s, 1315.98/s  (0.797s, 1284.34/s)  LR: 7.917e-04  Data: 0.011 (0.013)
Train: 91 [1200/1251 ( 96%)]  Loss: 4.230 (3.90)  Time: 0.806s, 1270.97/s  (0.797s, 1284.74/s)  LR: 7.917e-04  Data: 0.011 (0.012)
Train: 91 [1250/1251 (100%)]  Loss: 4.169 (3.91)  Time: 0.769s, 1331.76/s  (0.797s, 1285.61/s)  LR: 7.917e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.515 (1.515)  Loss:  0.9886 (0.9886)  Acc@1: 84.1797 (84.1797)  Acc@5: 93.8477 (93.8477)
Test: [  48/48]  Time: 0.172 (0.564)  Loss:  0.9459 (1.5541)  Acc@1: 82.1934 (68.9960)  Acc@5: 94.6934 (89.0340)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-81.pth.tar', 69.34600004882813)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-78.pth.tar', 69.1979999975586)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-84.pth.tar', 69.06600013183593)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-77.pth.tar', 69.03400007080079)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-91.pth.tar', 68.99600002197266)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-87.pth.tar', 68.8760000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-79.pth.tar', 68.81200010253906)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-90.pth.tar', 68.67999996582031)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-89.pth.tar', 68.66599996826172)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-69.pth.tar', 68.6360000024414)

Train: 92 [   0/1251 (  0%)]  Loss: 3.694 (3.69)  Time: 2.433s,  420.92/s  (2.433s,  420.92/s)  LR: 7.875e-04  Data: 1.695 (1.695)
Train: 92 [  50/1251 (  4%)]  Loss: 3.910 (3.80)  Time: 0.780s, 1313.63/s  (0.830s, 1233.71/s)  LR: 7.875e-04  Data: 0.011 (0.044)
Train: 92 [ 100/1251 (  8%)]  Loss: 3.816 (3.81)  Time: 0.827s, 1238.27/s  (0.808s, 1267.84/s)  LR: 7.875e-04  Data: 0.011 (0.028)
Train: 92 [ 150/1251 ( 12%)]  Loss: 3.987 (3.85)  Time: 0.782s, 1309.46/s  (0.810s, 1264.53/s)  LR: 7.875e-04  Data: 0.010 (0.022)
Train: 92 [ 200/1251 ( 16%)]  Loss: 3.570 (3.80)  Time: 0.789s, 1298.55/s  (0.804s, 1274.31/s)  LR: 7.875e-04  Data: 0.011 (0.019)
Train: 92 [ 250/1251 ( 20%)]  Loss: 3.956 (3.82)  Time: 0.780s, 1313.40/s  (0.800s, 1280.75/s)  LR: 7.875e-04  Data: 0.010 (0.018)
Train: 92 [ 300/1251 ( 24%)]  Loss: 3.657 (3.80)  Time: 0.778s, 1316.61/s  (0.798s, 1283.01/s)  LR: 7.875e-04  Data: 0.010 (0.017)
Train: 92 [ 350/1251 ( 28%)]  Loss: 3.814 (3.80)  Time: 0.857s, 1194.21/s  (0.797s, 1285.60/s)  LR: 7.875e-04  Data: 0.010 (0.016)
Train: 92 [ 400/1251 ( 32%)]  Loss: 4.016 (3.82)  Time: 0.788s, 1299.28/s  (0.796s, 1285.68/s)  LR: 7.875e-04  Data: 0.010 (0.015)
Train: 92 [ 450/1251 ( 36%)]  Loss: 3.699 (3.81)  Time: 0.841s, 1218.11/s  (0.796s, 1285.85/s)  LR: 7.875e-04  Data: 0.011 (0.015)
Train: 92 [ 500/1251 ( 40%)]  Loss: 3.741 (3.81)  Time: 0.840s, 1219.38/s  (0.799s, 1281.44/s)  LR: 7.875e-04  Data: 0.012 (0.014)
Train: 92 [ 550/1251 ( 44%)]  Loss: 3.830 (3.81)  Time: 0.790s, 1296.31/s  (0.800s, 1280.74/s)  LR: 7.875e-04  Data: 0.010 (0.014)
Train: 92 [ 600/1251 ( 48%)]  Loss: 3.905 (3.82)  Time: 0.777s, 1317.12/s  (0.799s, 1281.39/s)  LR: 7.875e-04  Data: 0.011 (0.014)
Train: 92 [ 650/1251 ( 52%)]  Loss: 3.859 (3.82)  Time: 0.795s, 1287.86/s  (0.799s, 1282.15/s)  LR: 7.875e-04  Data: 0.011 (0.014)
Train: 92 [ 700/1251 ( 56%)]  Loss: 3.920 (3.82)  Time: 0.796s, 1286.49/s  (0.798s, 1283.06/s)  LR: 7.875e-04  Data: 0.010 (0.013)
Train: 92 [ 750/1251 ( 60%)]  Loss: 3.953 (3.83)  Time: 0.776s, 1320.12/s  (0.797s, 1284.12/s)  LR: 7.875e-04  Data: 0.012 (0.013)
Train: 92 [ 800/1251 ( 64%)]  Loss: 3.918 (3.84)  Time: 0.781s, 1311.96/s  (0.797s, 1285.17/s)  LR: 7.875e-04  Data: 0.010 (0.013)
Train: 92 [ 850/1251 ( 68%)]  Loss: 4.066 (3.85)  Time: 0.818s, 1252.19/s  (0.796s, 1285.76/s)  LR: 7.875e-04  Data: 0.010 (0.013)
Train: 92 [ 900/1251 ( 72%)]  Loss: 3.504 (3.83)  Time: 0.777s, 1317.27/s  (0.797s, 1284.98/s)  LR: 7.875e-04  Data: 0.010 (0.013)
Train: 92 [ 950/1251 ( 76%)]  Loss: 3.688 (3.83)  Time: 0.816s, 1254.32/s  (0.798s, 1283.78/s)  LR: 7.875e-04  Data: 0.011 (0.013)
Train: 92 [1000/1251 ( 80%)]  Loss: 3.879 (3.83)  Time: 0.815s, 1256.18/s  (0.798s, 1282.62/s)  LR: 7.875e-04  Data: 0.010 (0.013)
Train: 92 [1050/1251 ( 84%)]  Loss: 3.909 (3.83)  Time: 0.842s, 1216.58/s  (0.799s, 1281.82/s)  LR: 7.875e-04  Data: 0.011 (0.013)
Train: 92 [1100/1251 ( 88%)]  Loss: 4.216 (3.85)  Time: 0.787s, 1301.18/s  (0.799s, 1281.36/s)  LR: 7.875e-04  Data: 0.011 (0.012)
Train: 92 [1150/1251 ( 92%)]  Loss: 3.613 (3.84)  Time: 0.779s, 1314.44/s  (0.798s, 1282.42/s)  LR: 7.875e-04  Data: 0.011 (0.012)
Train: 92 [1200/1251 ( 96%)]  Loss: 3.929 (3.84)  Time: 0.841s, 1217.52/s  (0.799s, 1281.26/s)  LR: 7.875e-04  Data: 0.011 (0.012)
Train: 92 [1250/1251 (100%)]  Loss: 3.728 (3.84)  Time: 0.797s, 1284.93/s  (0.800s, 1280.57/s)  LR: 7.875e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.537 (1.537)  Loss:  0.8066 (0.8066)  Acc@1: 86.5234 (86.5234)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.172 (0.561)  Loss:  0.9123 (1.4714)  Acc@1: 84.5519 (70.1680)  Acc@5: 95.2830 (89.5560)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-92.pth.tar', 70.16800001220703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-81.pth.tar', 69.34600004882813)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-78.pth.tar', 69.1979999975586)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-84.pth.tar', 69.06600013183593)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-77.pth.tar', 69.03400007080079)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-91.pth.tar', 68.99600002197266)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-87.pth.tar', 68.8760000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-79.pth.tar', 68.81200010253906)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-90.pth.tar', 68.67999996582031)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-89.pth.tar', 68.66599996826172)

Train: 93 [   0/1251 (  0%)]  Loss: 3.716 (3.72)  Time: 2.331s,  439.26/s  (2.331s,  439.26/s)  LR: 7.832e-04  Data: 1.539 (1.539)
Train: 93 [  50/1251 (  4%)]  Loss: 4.257 (3.99)  Time: 0.791s, 1294.92/s  (0.818s, 1251.94/s)  LR: 7.832e-04  Data: 0.017 (0.044)
Train: 93 [ 100/1251 (  8%)]  Loss: 3.834 (3.94)  Time: 0.813s, 1259.25/s  (0.804s, 1274.15/s)  LR: 7.832e-04  Data: 0.011 (0.028)
Train: 93 [ 150/1251 ( 12%)]  Loss: 3.937 (3.94)  Time: 0.780s, 1313.23/s  (0.798s, 1282.48/s)  LR: 7.832e-04  Data: 0.011 (0.022)
Train: 93 [ 200/1251 ( 16%)]  Loss: 4.085 (3.97)  Time: 0.809s, 1265.07/s  (0.799s, 1281.22/s)  LR: 7.832e-04  Data: 0.011 (0.019)
Train: 93 [ 250/1251 ( 20%)]  Loss: 3.472 (3.88)  Time: 0.780s, 1312.74/s  (0.797s, 1285.53/s)  LR: 7.832e-04  Data: 0.010 (0.018)
Train: 93 [ 300/1251 ( 24%)]  Loss: 3.949 (3.89)  Time: 0.776s, 1319.28/s  (0.796s, 1286.61/s)  LR: 7.832e-04  Data: 0.011 (0.017)
Train: 93 [ 350/1251 ( 28%)]  Loss: 3.894 (3.89)  Time: 0.786s, 1302.69/s  (0.794s, 1289.02/s)  LR: 7.832e-04  Data: 0.010 (0.016)
Train: 93 [ 400/1251 ( 32%)]  Loss: 3.999 (3.90)  Time: 0.779s, 1314.41/s  (0.794s, 1290.06/s)  LR: 7.832e-04  Data: 0.010 (0.015)
Train: 93 [ 450/1251 ( 36%)]  Loss: 4.149 (3.93)  Time: 0.777s, 1317.25/s  (0.793s, 1290.72/s)  LR: 7.832e-04  Data: 0.011 (0.015)
Train: 93 [ 500/1251 ( 40%)]  Loss: 4.485 (3.98)  Time: 0.779s, 1314.67/s  (0.793s, 1291.80/s)  LR: 7.832e-04  Data: 0.011 (0.014)
Train: 93 [ 550/1251 ( 44%)]  Loss: 3.876 (3.97)  Time: 0.816s, 1255.31/s  (0.793s, 1290.96/s)  LR: 7.832e-04  Data: 0.010 (0.014)
Train: 93 [ 600/1251 ( 48%)]  Loss: 4.108 (3.98)  Time: 0.777s, 1318.40/s  (0.795s, 1288.21/s)  LR: 7.832e-04  Data: 0.011 (0.014)
Train: 93 [ 650/1251 ( 52%)]  Loss: 3.992 (3.98)  Time: 0.837s, 1223.01/s  (0.794s, 1289.40/s)  LR: 7.832e-04  Data: 0.010 (0.014)
Train: 93 [ 700/1251 ( 56%)]  Loss: 3.780 (3.97)  Time: 0.778s, 1315.70/s  (0.793s, 1290.99/s)  LR: 7.832e-04  Data: 0.010 (0.013)
Train: 93 [ 750/1251 ( 60%)]  Loss: 3.513 (3.94)  Time: 0.775s, 1321.58/s  (0.793s, 1291.95/s)  LR: 7.832e-04  Data: 0.011 (0.013)
Train: 93 [ 800/1251 ( 64%)]  Loss: 4.093 (3.95)  Time: 0.784s, 1306.04/s  (0.793s, 1291.98/s)  LR: 7.832e-04  Data: 0.013 (0.013)
Train: 93 [ 850/1251 ( 68%)]  Loss: 3.566 (3.93)  Time: 0.781s, 1311.67/s  (0.792s, 1293.06/s)  LR: 7.832e-04  Data: 0.011 (0.013)
Train: 93 [ 900/1251 ( 72%)]  Loss: 3.867 (3.92)  Time: 0.813s, 1259.80/s  (0.792s, 1292.61/s)  LR: 7.832e-04  Data: 0.010 (0.013)
Train: 93 [ 950/1251 ( 76%)]  Loss: 3.920 (3.92)  Time: 0.777s, 1317.54/s  (0.792s, 1293.17/s)  LR: 7.832e-04  Data: 0.010 (0.013)
Train: 93 [1000/1251 ( 80%)]  Loss: 3.754 (3.92)  Time: 0.815s, 1255.83/s  (0.792s, 1292.84/s)  LR: 7.832e-04  Data: 0.010 (0.013)
Train: 93 [1050/1251 ( 84%)]  Loss: 4.014 (3.92)  Time: 0.777s, 1317.40/s  (0.792s, 1293.44/s)  LR: 7.832e-04  Data: 0.010 (0.012)
Train: 93 [1100/1251 ( 88%)]  Loss: 3.861 (3.92)  Time: 0.815s, 1255.99/s  (0.792s, 1292.32/s)  LR: 7.832e-04  Data: 0.010 (0.012)
Train: 93 [1150/1251 ( 92%)]  Loss: 3.531 (3.90)  Time: 0.788s, 1299.65/s  (0.793s, 1292.09/s)  LR: 7.832e-04  Data: 0.011 (0.012)
Train: 93 [1200/1251 ( 96%)]  Loss: 4.044 (3.91)  Time: 0.809s, 1265.55/s  (0.792s, 1292.58/s)  LR: 7.832e-04  Data: 0.011 (0.012)
Train: 93 [1250/1251 (100%)]  Loss: 4.226 (3.92)  Time: 0.767s, 1335.31/s  (0.792s, 1293.10/s)  LR: 7.832e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.516 (1.516)  Loss:  1.0237 (1.0237)  Acc@1: 85.9375 (85.9375)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.172 (0.563)  Loss:  0.9955 (1.5793)  Acc@1: 82.9009 (69.7960)  Acc@5: 94.4576 (89.6140)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-92.pth.tar', 70.16800001220703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-93.pth.tar', 69.79600007080079)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-81.pth.tar', 69.34600004882813)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-78.pth.tar', 69.1979999975586)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-84.pth.tar', 69.06600013183593)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-77.pth.tar', 69.03400007080079)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-91.pth.tar', 68.99600002197266)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-87.pth.tar', 68.8760000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-79.pth.tar', 68.81200010253906)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-90.pth.tar', 68.67999996582031)

Train: 94 [   0/1251 (  0%)]  Loss: 4.061 (4.06)  Time: 2.349s,  435.92/s  (2.349s,  435.92/s)  LR: 7.789e-04  Data: 1.614 (1.614)
Train: 94 [  50/1251 (  4%)]  Loss: 3.797 (3.93)  Time: 0.780s, 1313.28/s  (0.831s, 1232.01/s)  LR: 7.789e-04  Data: 0.010 (0.052)
Train: 94 [ 100/1251 (  8%)]  Loss: 3.720 (3.86)  Time: 0.782s, 1309.21/s  (0.818s, 1252.49/s)  LR: 7.789e-04  Data: 0.011 (0.031)
Train: 94 [ 150/1251 ( 12%)]  Loss: 3.990 (3.89)  Time: 0.780s, 1313.07/s  (0.809s, 1265.51/s)  LR: 7.789e-04  Data: 0.010 (0.025)
Train: 94 [ 200/1251 ( 16%)]  Loss: 3.851 (3.88)  Time: 0.801s, 1277.82/s  (0.807s, 1268.93/s)  LR: 7.789e-04  Data: 0.011 (0.021)
Train: 94 [ 250/1251 ( 20%)]  Loss: 4.415 (3.97)  Time: 0.782s, 1309.82/s  (0.802s, 1276.71/s)  LR: 7.789e-04  Data: 0.011 (0.019)
Train: 94 [ 300/1251 ( 24%)]  Loss: 3.788 (3.95)  Time: 0.777s, 1317.50/s  (0.799s, 1280.82/s)  LR: 7.789e-04  Data: 0.011 (0.018)
Train: 94 [ 350/1251 ( 28%)]  Loss: 3.899 (3.94)  Time: 0.781s, 1311.71/s  (0.797s, 1284.44/s)  LR: 7.789e-04  Data: 0.011 (0.017)
Train: 94 [ 400/1251 ( 32%)]  Loss: 3.894 (3.94)  Time: 0.781s, 1311.73/s  (0.796s, 1286.26/s)  LR: 7.789e-04  Data: 0.011 (0.016)
Train: 94 [ 450/1251 ( 36%)]  Loss: 3.979 (3.94)  Time: 0.778s, 1315.40/s  (0.795s, 1287.86/s)  LR: 7.789e-04  Data: 0.011 (0.015)
Train: 94 [ 500/1251 ( 40%)]  Loss: 4.093 (3.95)  Time: 0.842s, 1216.10/s  (0.798s, 1284.00/s)  LR: 7.789e-04  Data: 0.012 (0.015)
Train: 94 [ 550/1251 ( 44%)]  Loss: 3.831 (3.94)  Time: 0.806s, 1270.70/s  (0.798s, 1282.42/s)  LR: 7.789e-04  Data: 0.011 (0.015)
Train: 94 [ 600/1251 ( 48%)]  Loss: 3.751 (3.93)  Time: 0.781s, 1311.35/s  (0.797s, 1284.50/s)  LR: 7.789e-04  Data: 0.011 (0.014)
Train: 94 [ 650/1251 ( 52%)]  Loss: 3.981 (3.93)  Time: 0.777s, 1318.27/s  (0.796s, 1286.02/s)  LR: 7.789e-04  Data: 0.011 (0.014)
Train: 94 [ 700/1251 ( 56%)]  Loss: 4.130 (3.95)  Time: 0.779s, 1314.74/s  (0.795s, 1287.27/s)  LR: 7.789e-04  Data: 0.011 (0.014)
Train: 94 [ 750/1251 ( 60%)]  Loss: 3.896 (3.94)  Time: 0.782s, 1310.24/s  (0.795s, 1288.37/s)  LR: 7.789e-04  Data: 0.011 (0.014)
Train: 94 [ 800/1251 ( 64%)]  Loss: 4.022 (3.95)  Time: 0.787s, 1301.13/s  (0.795s, 1288.85/s)  LR: 7.789e-04  Data: 0.012 (0.013)
Train: 94 [ 850/1251 ( 68%)]  Loss: 3.685 (3.93)  Time: 0.834s, 1228.42/s  (0.795s, 1288.36/s)  LR: 7.789e-04  Data: 0.010 (0.013)
Train: 94 [ 900/1251 ( 72%)]  Loss: 3.929 (3.93)  Time: 0.813s, 1259.33/s  (0.795s, 1288.56/s)  LR: 7.789e-04  Data: 0.011 (0.013)
Train: 94 [ 950/1251 ( 76%)]  Loss: 4.065 (3.94)  Time: 0.777s, 1317.47/s  (0.795s, 1287.53/s)  LR: 7.789e-04  Data: 0.011 (0.013)
Train: 94 [1000/1251 ( 80%)]  Loss: 3.575 (3.92)  Time: 0.842s, 1215.74/s  (0.795s, 1287.75/s)  LR: 7.789e-04  Data: 0.012 (0.013)
Train: 94 [1050/1251 ( 84%)]  Loss: 4.166 (3.93)  Time: 0.851s, 1203.97/s  (0.796s, 1286.58/s)  LR: 7.789e-04  Data: 0.010 (0.013)
Train: 94 [1100/1251 ( 88%)]  Loss: 3.786 (3.93)  Time: 0.780s, 1312.02/s  (0.796s, 1286.51/s)  LR: 7.789e-04  Data: 0.011 (0.013)
Train: 94 [1150/1251 ( 92%)]  Loss: 3.963 (3.93)  Time: 0.787s, 1301.64/s  (0.796s, 1287.08/s)  LR: 7.789e-04  Data: 0.010 (0.013)
Train: 94 [1200/1251 ( 96%)]  Loss: 3.881 (3.93)  Time: 0.777s, 1317.23/s  (0.795s, 1288.06/s)  LR: 7.789e-04  Data: 0.011 (0.013)
Train: 94 [1250/1251 (100%)]  Loss: 3.814 (3.92)  Time: 0.769s, 1332.26/s  (0.794s, 1288.90/s)  LR: 7.789e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.551 (1.551)  Loss:  0.9633 (0.9633)  Acc@1: 87.0117 (87.0117)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.172 (0.561)  Loss:  1.0257 (1.5722)  Acc@1: 82.4292 (69.9740)  Acc@5: 95.2830 (89.5760)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-92.pth.tar', 70.16800001220703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-94.pth.tar', 69.97399999511718)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-93.pth.tar', 69.79600007080079)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-81.pth.tar', 69.34600004882813)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-78.pth.tar', 69.1979999975586)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-84.pth.tar', 69.06600013183593)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-77.pth.tar', 69.03400007080079)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-91.pth.tar', 68.99600002197266)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-87.pth.tar', 68.8760000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-79.pth.tar', 68.81200010253906)

Train: 95 [   0/1251 (  0%)]  Loss: 3.806 (3.81)  Time: 2.582s,  396.57/s  (2.582s,  396.57/s)  LR: 7.746e-04  Data: 1.845 (1.845)
Train: 95 [  50/1251 (  4%)]  Loss: 3.679 (3.74)  Time: 0.796s, 1287.12/s  (0.824s, 1242.07/s)  LR: 7.746e-04  Data: 0.010 (0.054)
Train: 95 [ 100/1251 (  8%)]  Loss: 3.673 (3.72)  Time: 0.829s, 1234.67/s  (0.811s, 1262.76/s)  LR: 7.746e-04  Data: 0.011 (0.033)
Train: 95 [ 150/1251 ( 12%)]  Loss: 3.689 (3.71)  Time: 0.793s, 1290.60/s  (0.804s, 1273.83/s)  LR: 7.746e-04  Data: 0.010 (0.025)
Train: 95 [ 200/1251 ( 16%)]  Loss: 3.738 (3.72)  Time: 0.823s, 1244.02/s  (0.805s, 1272.44/s)  LR: 7.746e-04  Data: 0.011 (0.022)
Train: 95 [ 250/1251 ( 20%)]  Loss: 3.707 (3.72)  Time: 0.834s, 1227.54/s  (0.804s, 1273.39/s)  LR: 7.746e-04  Data: 0.011 (0.019)
Train: 95 [ 300/1251 ( 24%)]  Loss: 3.434 (3.68)  Time: 0.826s, 1239.28/s  (0.802s, 1276.46/s)  LR: 7.746e-04  Data: 0.010 (0.018)
Train: 95 [ 350/1251 ( 28%)]  Loss: 4.013 (3.72)  Time: 0.817s, 1253.93/s  (0.803s, 1275.72/s)  LR: 7.746e-04  Data: 0.012 (0.017)
Train: 95 [ 400/1251 ( 32%)]  Loss: 4.034 (3.75)  Time: 0.778s, 1316.69/s  (0.800s, 1279.27/s)  LR: 7.746e-04  Data: 0.010 (0.016)
Train: 95 [ 450/1251 ( 36%)]  Loss: 3.720 (3.75)  Time: 0.779s, 1314.31/s  (0.798s, 1282.49/s)  LR: 7.746e-04  Data: 0.016 (0.016)
Train: 95 [ 500/1251 ( 40%)]  Loss: 3.653 (3.74)  Time: 0.861s, 1189.75/s  (0.798s, 1283.89/s)  LR: 7.746e-04  Data: 0.011 (0.015)
Train: 95 [ 550/1251 ( 44%)]  Loss: 4.091 (3.77)  Time: 0.776s, 1320.10/s  (0.797s, 1284.06/s)  LR: 7.746e-04  Data: 0.011 (0.015)
Train: 95 [ 600/1251 ( 48%)]  Loss: 4.001 (3.79)  Time: 0.805s, 1272.17/s  (0.798s, 1283.77/s)  LR: 7.746e-04  Data: 0.011 (0.015)
Train: 95 [ 650/1251 ( 52%)]  Loss: 3.609 (3.77)  Time: 0.784s, 1306.50/s  (0.797s, 1284.73/s)  LR: 7.746e-04  Data: 0.012 (0.014)
Train: 95 [ 700/1251 ( 56%)]  Loss: 4.020 (3.79)  Time: 0.778s, 1316.60/s  (0.797s, 1285.10/s)  LR: 7.746e-04  Data: 0.011 (0.014)
Train: 95 [ 750/1251 ( 60%)]  Loss: 4.340 (3.83)  Time: 0.814s, 1257.26/s  (0.797s, 1284.40/s)  LR: 7.746e-04  Data: 0.010 (0.014)
Train: 95 [ 800/1251 ( 64%)]  Loss: 3.795 (3.82)  Time: 0.844s, 1213.16/s  (0.797s, 1285.37/s)  LR: 7.746e-04  Data: 0.011 (0.014)
Train: 95 [ 850/1251 ( 68%)]  Loss: 4.115 (3.84)  Time: 0.835s, 1226.02/s  (0.796s, 1285.83/s)  LR: 7.746e-04  Data: 0.010 (0.013)
Train: 95 [ 900/1251 ( 72%)]  Loss: 3.701 (3.83)  Time: 0.819s, 1250.75/s  (0.797s, 1285.07/s)  LR: 7.746e-04  Data: 0.011 (0.013)
Train: 95 [ 950/1251 ( 76%)]  Loss: 3.771 (3.83)  Time: 0.780s, 1312.21/s  (0.797s, 1284.36/s)  LR: 7.746e-04  Data: 0.012 (0.013)
Train: 95 [1000/1251 ( 80%)]  Loss: 3.930 (3.83)  Time: 0.777s, 1317.21/s  (0.796s, 1285.67/s)  LR: 7.746e-04  Data: 0.011 (0.013)
Train: 95 [1050/1251 ( 84%)]  Loss: 4.403 (3.86)  Time: 0.814s, 1258.02/s  (0.796s, 1286.30/s)  LR: 7.746e-04  Data: 0.010 (0.013)
Train: 95 [1100/1251 ( 88%)]  Loss: 3.613 (3.85)  Time: 0.778s, 1316.26/s  (0.796s, 1285.85/s)  LR: 7.746e-04  Data: 0.010 (0.013)
Train: 95 [1150/1251 ( 92%)]  Loss: 3.672 (3.84)  Time: 0.813s, 1259.89/s  (0.796s, 1286.45/s)  LR: 7.746e-04  Data: 0.011 (0.013)
Train: 95 [1200/1251 ( 96%)]  Loss: 3.934 (3.85)  Time: 0.775s, 1320.68/s  (0.796s, 1286.78/s)  LR: 7.746e-04  Data: 0.011 (0.013)
Train: 95 [1250/1251 (100%)]  Loss: 4.136 (3.86)  Time: 0.770s, 1329.63/s  (0.796s, 1286.93/s)  LR: 7.746e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.507 (1.507)  Loss:  0.8517 (0.8517)  Acc@1: 86.2305 (86.2305)  Acc@5: 96.1914 (96.1914)
Test: [  48/48]  Time: 0.172 (0.574)  Loss:  0.8230 (1.4743)  Acc@1: 83.7264 (69.6600)  Acc@5: 95.6368 (89.4560)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-92.pth.tar', 70.16800001220703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-94.pth.tar', 69.97399999511718)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-93.pth.tar', 69.79600007080079)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-95.pth.tar', 69.66000017089844)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-81.pth.tar', 69.34600004882813)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-78.pth.tar', 69.1979999975586)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-84.pth.tar', 69.06600013183593)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-77.pth.tar', 69.03400007080079)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-91.pth.tar', 68.99600002197266)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-87.pth.tar', 68.8760000024414)

Train: 96 [   0/1251 (  0%)]  Loss: 4.134 (4.13)  Time: 2.291s,  446.88/s  (2.291s,  446.88/s)  LR: 7.702e-04  Data: 1.498 (1.498)
Train: 96 [  50/1251 (  4%)]  Loss: 3.831 (3.98)  Time: 0.777s, 1317.10/s  (0.825s, 1240.93/s)  LR: 7.702e-04  Data: 0.011 (0.047)
Train: 96 [ 100/1251 (  8%)]  Loss: 3.927 (3.96)  Time: 0.779s, 1314.09/s  (0.804s, 1272.93/s)  LR: 7.702e-04  Data: 0.011 (0.029)
Train: 96 [ 150/1251 ( 12%)]  Loss: 4.179 (4.02)  Time: 0.850s, 1204.08/s  (0.804s, 1273.73/s)  LR: 7.702e-04  Data: 0.010 (0.023)
Train: 96 [ 200/1251 ( 16%)]  Loss: 3.798 (3.97)  Time: 0.778s, 1316.78/s  (0.801s, 1278.94/s)  LR: 7.702e-04  Data: 0.011 (0.020)
Train: 96 [ 250/1251 ( 20%)]  Loss: 3.993 (3.98)  Time: 0.778s, 1315.89/s  (0.796s, 1285.81/s)  LR: 7.702e-04  Data: 0.011 (0.018)
Train: 96 [ 300/1251 ( 24%)]  Loss: 3.487 (3.91)  Time: 0.785s, 1303.75/s  (0.798s, 1283.85/s)  LR: 7.702e-04  Data: 0.011 (0.017)
Train: 96 [ 350/1251 ( 28%)]  Loss: 3.538 (3.86)  Time: 0.789s, 1297.41/s  (0.796s, 1286.31/s)  LR: 7.702e-04  Data: 0.014 (0.016)
Train: 96 [ 400/1251 ( 32%)]  Loss: 4.012 (3.88)  Time: 0.779s, 1314.52/s  (0.795s, 1287.97/s)  LR: 7.702e-04  Data: 0.010 (0.015)
Train: 96 [ 450/1251 ( 36%)]  Loss: 4.259 (3.92)  Time: 0.811s, 1262.98/s  (0.794s, 1289.38/s)  LR: 7.702e-04  Data: 0.011 (0.015)
Train: 96 [ 500/1251 ( 40%)]  Loss: 4.027 (3.93)  Time: 0.792s, 1292.93/s  (0.794s, 1289.94/s)  LR: 7.702e-04  Data: 0.011 (0.015)
Train: 96 [ 550/1251 ( 44%)]  Loss: 4.019 (3.93)  Time: 0.780s, 1312.61/s  (0.793s, 1291.04/s)  LR: 7.702e-04  Data: 0.012 (0.014)
Train: 96 [ 600/1251 ( 48%)]  Loss: 3.966 (3.94)  Time: 0.858s, 1193.93/s  (0.793s, 1290.54/s)  LR: 7.702e-04  Data: 0.011 (0.014)
Train: 96 [ 650/1251 ( 52%)]  Loss: 3.663 (3.92)  Time: 0.795s, 1287.81/s  (0.794s, 1288.93/s)  LR: 7.702e-04  Data: 0.012 (0.014)
Train: 96 [ 700/1251 ( 56%)]  Loss: 3.521 (3.89)  Time: 0.780s, 1312.14/s  (0.794s, 1290.11/s)  LR: 7.702e-04  Data: 0.012 (0.014)
Train: 96 [ 750/1251 ( 60%)]  Loss: 3.833 (3.89)  Time: 0.848s, 1207.00/s  (0.794s, 1289.81/s)  LR: 7.702e-04  Data: 0.012 (0.014)
Train: 96 [ 800/1251 ( 64%)]  Loss: 3.202 (3.85)  Time: 0.837s, 1223.97/s  (0.794s, 1290.06/s)  LR: 7.702e-04  Data: 0.012 (0.013)
Train: 96 [ 850/1251 ( 68%)]  Loss: 4.040 (3.86)  Time: 0.851s, 1203.62/s  (0.796s, 1286.48/s)  LR: 7.702e-04  Data: 0.011 (0.013)
Train: 96 [ 900/1251 ( 72%)]  Loss: 3.919 (3.86)  Time: 0.800s, 1280.35/s  (0.796s, 1286.82/s)  LR: 7.702e-04  Data: 0.014 (0.013)
Train: 96 [ 950/1251 ( 76%)]  Loss: 3.925 (3.86)  Time: 0.784s, 1305.90/s  (0.796s, 1286.27/s)  LR: 7.702e-04  Data: 0.010 (0.013)
Train: 96 [1000/1251 ( 80%)]  Loss: 3.951 (3.87)  Time: 0.776s, 1320.25/s  (0.796s, 1285.93/s)  LR: 7.702e-04  Data: 0.011 (0.013)
Train: 96 [1050/1251 ( 84%)]  Loss: 3.971 (3.87)  Time: 0.778s, 1316.69/s  (0.796s, 1286.67/s)  LR: 7.702e-04  Data: 0.011 (0.013)
Train: 96 [1100/1251 ( 88%)]  Loss: 4.257 (3.89)  Time: 0.775s, 1321.82/s  (0.796s, 1287.15/s)  LR: 7.702e-04  Data: 0.011 (0.013)
Train: 96 [1150/1251 ( 92%)]  Loss: 4.010 (3.89)  Time: 0.848s, 1207.95/s  (0.797s, 1284.84/s)  LR: 7.702e-04  Data: 0.011 (0.013)
Train: 96 [1200/1251 ( 96%)]  Loss: 4.059 (3.90)  Time: 0.781s, 1311.42/s  (0.797s, 1284.94/s)  LR: 7.702e-04  Data: 0.012 (0.013)
Train: 96 [1250/1251 (100%)]  Loss: 4.001 (3.90)  Time: 0.779s, 1314.05/s  (0.797s, 1284.94/s)  LR: 7.702e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.643 (1.643)  Loss:  0.8773 (0.8773)  Acc@1: 87.5977 (87.5977)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.172 (0.572)  Loss:  1.0186 (1.6058)  Acc@1: 83.0189 (69.7840)  Acc@5: 94.6934 (89.6140)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-92.pth.tar', 70.16800001220703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-94.pth.tar', 69.97399999511718)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-93.pth.tar', 69.79600007080079)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-96.pth.tar', 69.78399999267579)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-95.pth.tar', 69.66000017089844)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-81.pth.tar', 69.34600004882813)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-78.pth.tar', 69.1979999975586)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-84.pth.tar', 69.06600013183593)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-77.pth.tar', 69.03400007080079)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-91.pth.tar', 68.99600002197266)

Train: 97 [   0/1251 (  0%)]  Loss: 3.898 (3.90)  Time: 2.301s,  444.99/s  (2.301s,  444.99/s)  LR: 7.658e-04  Data: 1.567 (1.567)
Train: 97 [  50/1251 (  4%)]  Loss: 3.924 (3.91)  Time: 0.782s, 1309.95/s  (0.823s, 1243.79/s)  LR: 7.658e-04  Data: 0.011 (0.048)
Train: 97 [ 100/1251 (  8%)]  Loss: 4.184 (4.00)  Time: 0.802s, 1277.07/s  (0.810s, 1264.76/s)  LR: 7.658e-04  Data: 0.011 (0.029)
Train: 97 [ 150/1251 ( 12%)]  Loss: 3.962 (3.99)  Time: 0.779s, 1313.81/s  (0.802s, 1277.57/s)  LR: 7.658e-04  Data: 0.011 (0.023)
Train: 97 [ 200/1251 ( 16%)]  Loss: 3.938 (3.98)  Time: 0.782s, 1309.12/s  (0.797s, 1284.15/s)  LR: 7.658e-04  Data: 0.011 (0.020)
Train: 97 [ 250/1251 ( 20%)]  Loss: 3.654 (3.93)  Time: 0.800s, 1280.43/s  (0.797s, 1284.16/s)  LR: 7.658e-04  Data: 0.012 (0.018)
Train: 97 [ 300/1251 ( 24%)]  Loss: 3.695 (3.89)  Time: 0.778s, 1315.36/s  (0.799s, 1281.99/s)  LR: 7.658e-04  Data: 0.011 (0.017)
Train: 97 [ 350/1251 ( 28%)]  Loss: 3.662 (3.86)  Time: 0.779s, 1315.08/s  (0.797s, 1285.09/s)  LR: 7.658e-04  Data: 0.010 (0.016)
Train: 97 [ 400/1251 ( 32%)]  Loss: 3.530 (3.83)  Time: 0.775s, 1320.68/s  (0.796s, 1285.79/s)  LR: 7.658e-04  Data: 0.011 (0.016)
Train: 97 [ 450/1251 ( 36%)]  Loss: 3.863 (3.83)  Time: 0.779s, 1314.95/s  (0.795s, 1287.65/s)  LR: 7.658e-04  Data: 0.011 (0.015)
Train: 97 [ 500/1251 ( 40%)]  Loss: 3.758 (3.82)  Time: 0.777s, 1318.71/s  (0.794s, 1289.51/s)  LR: 7.658e-04  Data: 0.011 (0.015)
Train: 97 [ 550/1251 ( 44%)]  Loss: 3.666 (3.81)  Time: 0.782s, 1309.38/s  (0.793s, 1291.38/s)  LR: 7.658e-04  Data: 0.012 (0.015)
Train: 97 [ 600/1251 ( 48%)]  Loss: 4.068 (3.83)  Time: 0.797s, 1285.29/s  (0.792s, 1292.30/s)  LR: 7.658e-04  Data: 0.011 (0.014)
Train: 97 [ 650/1251 ( 52%)]  Loss: 3.806 (3.83)  Time: 0.814s, 1258.08/s  (0.793s, 1290.82/s)  LR: 7.658e-04  Data: 0.011 (0.014)
Train: 97 [ 700/1251 ( 56%)]  Loss: 3.874 (3.83)  Time: 0.836s, 1224.79/s  (0.793s, 1290.85/s)  LR: 7.658e-04  Data: 0.011 (0.014)
Train: 97 [ 750/1251 ( 60%)]  Loss: 3.986 (3.84)  Time: 0.777s, 1317.13/s  (0.793s, 1291.89/s)  LR: 7.658e-04  Data: 0.011 (0.014)
Train: 97 [ 800/1251 ( 64%)]  Loss: 3.899 (3.85)  Time: 0.815s, 1256.73/s  (0.794s, 1290.20/s)  LR: 7.658e-04  Data: 0.012 (0.013)
Train: 97 [ 850/1251 ( 68%)]  Loss: 3.316 (3.82)  Time: 0.833s, 1229.28/s  (0.794s, 1289.42/s)  LR: 7.658e-04  Data: 0.011 (0.013)
Train: 97 [ 900/1251 ( 72%)]  Loss: 3.497 (3.80)  Time: 0.780s, 1312.34/s  (0.794s, 1290.28/s)  LR: 7.658e-04  Data: 0.012 (0.013)
Train: 97 [ 950/1251 ( 76%)]  Loss: 3.768 (3.80)  Time: 0.778s, 1316.58/s  (0.793s, 1291.23/s)  LR: 7.658e-04  Data: 0.010 (0.013)
Train: 97 [1000/1251 ( 80%)]  Loss: 4.169 (3.82)  Time: 0.779s, 1314.12/s  (0.793s, 1290.93/s)  LR: 7.658e-04  Data: 0.012 (0.013)
Train: 97 [1050/1251 ( 84%)]  Loss: 3.926 (3.82)  Time: 0.782s, 1309.54/s  (0.793s, 1291.66/s)  LR: 7.658e-04  Data: 0.011 (0.013)
Train: 97 [1100/1251 ( 88%)]  Loss: 4.241 (3.84)  Time: 0.790s, 1296.67/s  (0.792s, 1292.37/s)  LR: 7.658e-04  Data: 0.011 (0.013)
Train: 97 [1150/1251 ( 92%)]  Loss: 3.487 (3.82)  Time: 0.793s, 1291.29/s  (0.792s, 1292.77/s)  LR: 7.658e-04  Data: 0.010 (0.013)
Train: 97 [1200/1251 ( 96%)]  Loss: 4.135 (3.84)  Time: 0.779s, 1314.02/s  (0.792s, 1293.15/s)  LR: 7.658e-04  Data: 0.010 (0.013)
Train: 97 [1250/1251 (100%)]  Loss: 3.679 (3.83)  Time: 0.776s, 1319.44/s  (0.792s, 1292.82/s)  LR: 7.658e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.581 (1.581)  Loss:  0.8988 (0.8988)  Acc@1: 85.0586 (85.0586)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.172 (0.569)  Loss:  0.9704 (1.5173)  Acc@1: 81.6038 (69.6300)  Acc@5: 95.0472 (89.6200)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-92.pth.tar', 70.16800001220703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-94.pth.tar', 69.97399999511718)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-93.pth.tar', 69.79600007080079)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-96.pth.tar', 69.78399999267579)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-95.pth.tar', 69.66000017089844)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-97.pth.tar', 69.6300001538086)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-81.pth.tar', 69.34600004882813)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-78.pth.tar', 69.1979999975586)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-84.pth.tar', 69.06600013183593)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-77.pth.tar', 69.03400007080079)

Train: 98 [   0/1251 (  0%)]  Loss: 3.668 (3.67)  Time: 2.254s,  454.29/s  (2.254s,  454.29/s)  LR: 7.614e-04  Data: 1.520 (1.520)
Train: 98 [  50/1251 (  4%)]  Loss: 3.752 (3.71)  Time: 0.813s, 1260.09/s  (0.836s, 1224.71/s)  LR: 7.614e-04  Data: 0.011 (0.045)
Train: 98 [ 100/1251 (  8%)]  Loss: 3.956 (3.79)  Time: 0.806s, 1271.02/s  (0.812s, 1260.43/s)  LR: 7.614e-04  Data: 0.011 (0.028)
Train: 98 [ 150/1251 ( 12%)]  Loss: 3.508 (3.72)  Time: 0.784s, 1305.78/s  (0.804s, 1272.98/s)  LR: 7.614e-04  Data: 0.011 (0.023)
Train: 98 [ 200/1251 ( 16%)]  Loss: 4.026 (3.78)  Time: 0.774s, 1322.44/s  (0.804s, 1272.98/s)  LR: 7.614e-04  Data: 0.011 (0.020)
Train: 98 [ 250/1251 ( 20%)]  Loss: 3.872 (3.80)  Time: 0.777s, 1317.05/s  (0.804s, 1273.09/s)  LR: 7.614e-04  Data: 0.011 (0.018)
Train: 98 [ 300/1251 ( 24%)]  Loss: 3.773 (3.79)  Time: 0.780s, 1313.17/s  (0.802s, 1276.23/s)  LR: 7.614e-04  Data: 0.011 (0.017)
Train: 98 [ 350/1251 ( 28%)]  Loss: 4.098 (3.83)  Time: 0.779s, 1314.60/s  (0.802s, 1276.95/s)  LR: 7.614e-04  Data: 0.011 (0.016)
Train: 98 [ 400/1251 ( 32%)]  Loss: 3.820 (3.83)  Time: 0.775s, 1320.82/s  (0.800s, 1279.63/s)  LR: 7.614e-04  Data: 0.011 (0.015)
Train: 98 [ 450/1251 ( 36%)]  Loss: 3.682 (3.82)  Time: 0.778s, 1315.55/s  (0.800s, 1279.40/s)  LR: 7.614e-04  Data: 0.010 (0.015)
Train: 98 [ 500/1251 ( 40%)]  Loss: 3.459 (3.78)  Time: 0.852s, 1201.95/s  (0.802s, 1276.86/s)  LR: 7.614e-04  Data: 0.011 (0.015)
Train: 98 [ 550/1251 ( 44%)]  Loss: 3.779 (3.78)  Time: 0.776s, 1319.07/s  (0.804s, 1274.03/s)  LR: 7.614e-04  Data: 0.011 (0.014)
Train: 98 [ 600/1251 ( 48%)]  Loss: 4.395 (3.83)  Time: 0.777s, 1318.10/s  (0.802s, 1276.44/s)  LR: 7.614e-04  Data: 0.011 (0.014)
Train: 98 [ 650/1251 ( 52%)]  Loss: 3.951 (3.84)  Time: 0.803s, 1275.35/s  (0.801s, 1278.49/s)  LR: 7.614e-04  Data: 0.011 (0.014)
Train: 98 [ 700/1251 ( 56%)]  Loss: 3.919 (3.84)  Time: 0.777s, 1317.12/s  (0.800s, 1279.30/s)  LR: 7.614e-04  Data: 0.011 (0.014)
Train: 98 [ 750/1251 ( 60%)]  Loss: 3.787 (3.84)  Time: 0.778s, 1316.60/s  (0.800s, 1280.46/s)  LR: 7.614e-04  Data: 0.011 (0.013)
Train: 98 [ 800/1251 ( 64%)]  Loss: 3.971 (3.85)  Time: 0.780s, 1312.39/s  (0.799s, 1281.67/s)  LR: 7.614e-04  Data: 0.010 (0.013)
Train: 98 [ 850/1251 ( 68%)]  Loss: 4.025 (3.86)  Time: 0.833s, 1229.94/s  (0.799s, 1281.78/s)  LR: 7.614e-04  Data: 0.011 (0.013)
Train: 98 [ 900/1251 ( 72%)]  Loss: 3.985 (3.86)  Time: 0.790s, 1295.87/s  (0.798s, 1282.95/s)  LR: 7.614e-04  Data: 0.011 (0.013)
Train: 98 [ 950/1251 ( 76%)]  Loss: 3.731 (3.86)  Time: 0.779s, 1314.30/s  (0.798s, 1283.64/s)  LR: 7.614e-04  Data: 0.011 (0.013)
Train: 98 [1000/1251 ( 80%)]  Loss: 3.481 (3.84)  Time: 0.838s, 1222.61/s  (0.799s, 1281.97/s)  LR: 7.614e-04  Data: 0.011 (0.013)
Train: 98 [1050/1251 ( 84%)]  Loss: 4.205 (3.86)  Time: 0.804s, 1274.02/s  (0.798s, 1282.79/s)  LR: 7.614e-04  Data: 0.011 (0.013)
Train: 98 [1100/1251 ( 88%)]  Loss: 3.947 (3.86)  Time: 0.777s, 1317.45/s  (0.798s, 1283.25/s)  LR: 7.614e-04  Data: 0.010 (0.013)
Train: 98 [1150/1251 ( 92%)]  Loss: 3.552 (3.85)  Time: 0.794s, 1289.70/s  (0.797s, 1284.24/s)  LR: 7.614e-04  Data: 0.010 (0.013)
Train: 98 [1200/1251 ( 96%)]  Loss: 3.990 (3.85)  Time: 0.779s, 1314.48/s  (0.797s, 1284.59/s)  LR: 7.614e-04  Data: 0.011 (0.012)
Train: 98 [1250/1251 (100%)]  Loss: 4.008 (3.86)  Time: 0.770s, 1329.63/s  (0.797s, 1285.21/s)  LR: 7.614e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.549 (1.549)  Loss:  0.9446 (0.9446)  Acc@1: 87.6953 (87.6953)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.172 (0.566)  Loss:  1.0886 (1.6412)  Acc@1: 82.1934 (69.2100)  Acc@5: 95.5189 (89.3440)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-92.pth.tar', 70.16800001220703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-94.pth.tar', 69.97399999511718)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-93.pth.tar', 69.79600007080079)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-96.pth.tar', 69.78399999267579)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-95.pth.tar', 69.66000017089844)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-97.pth.tar', 69.6300001538086)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-81.pth.tar', 69.34600004882813)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-98.pth.tar', 69.20999989257813)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-78.pth.tar', 69.1979999975586)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-84.pth.tar', 69.06600013183593)

Train: 99 [   0/1251 (  0%)]  Loss: 3.526 (3.53)  Time: 2.306s,  444.10/s  (2.306s,  444.10/s)  LR: 7.570e-04  Data: 1.570 (1.570)
Train: 99 [  50/1251 (  4%)]  Loss: 4.066 (3.80)  Time: 0.820s, 1248.82/s  (0.848s, 1207.67/s)  LR: 7.570e-04  Data: 0.011 (0.044)
Train: 99 [ 100/1251 (  8%)]  Loss: 3.607 (3.73)  Time: 0.815s, 1256.12/s  (0.837s, 1223.81/s)  LR: 7.570e-04  Data: 0.012 (0.028)
Train: 99 [ 150/1251 ( 12%)]  Loss: 3.929 (3.78)  Time: 0.819s, 1250.13/s  (0.832s, 1230.37/s)  LR: 7.570e-04  Data: 0.011 (0.023)
Train: 99 [ 200/1251 ( 16%)]  Loss: 3.707 (3.77)  Time: 0.775s, 1320.53/s  (0.822s, 1245.26/s)  LR: 7.570e-04  Data: 0.011 (0.020)
Train: 99 [ 250/1251 ( 20%)]  Loss: 4.138 (3.83)  Time: 0.794s, 1289.26/s  (0.817s, 1253.04/s)  LR: 7.570e-04  Data: 0.011 (0.018)
Train: 99 [ 300/1251 ( 24%)]  Loss: 4.146 (3.87)  Time: 0.792s, 1293.63/s  (0.814s, 1257.83/s)  LR: 7.570e-04  Data: 0.011 (0.017)
Train: 99 [ 350/1251 ( 28%)]  Loss: 3.861 (3.87)  Time: 0.811s, 1262.27/s  (0.812s, 1260.46/s)  LR: 7.570e-04  Data: 0.011 (0.016)
Train: 99 [ 400/1251 ( 32%)]  Loss: 3.859 (3.87)  Time: 0.781s, 1310.80/s  (0.809s, 1265.52/s)  LR: 7.570e-04  Data: 0.011 (0.015)
Train: 99 [ 450/1251 ( 36%)]  Loss: 3.568 (3.84)  Time: 0.785s, 1303.68/s  (0.808s, 1266.84/s)  LR: 7.570e-04  Data: 0.011 (0.015)
Train: 99 [ 500/1251 ( 40%)]  Loss: 4.107 (3.86)  Time: 0.825s, 1241.70/s  (0.806s, 1270.30/s)  LR: 7.570e-04  Data: 0.014 (0.015)
Train: 99 [ 550/1251 ( 44%)]  Loss: 3.878 (3.87)  Time: 0.812s, 1260.52/s  (0.805s, 1272.37/s)  LR: 7.570e-04  Data: 0.010 (0.014)
Train: 99 [ 600/1251 ( 48%)]  Loss: 3.941 (3.87)  Time: 0.799s, 1281.91/s  (0.803s, 1275.10/s)  LR: 7.570e-04  Data: 0.011 (0.014)
Train: 99 [ 650/1251 ( 52%)]  Loss: 3.695 (3.86)  Time: 0.783s, 1308.01/s  (0.801s, 1277.66/s)  LR: 7.570e-04  Data: 0.010 (0.014)
Train: 99 [ 700/1251 ( 56%)]  Loss: 4.235 (3.88)  Time: 0.780s, 1312.93/s  (0.801s, 1279.15/s)  LR: 7.570e-04  Data: 0.011 (0.014)
Train: 99 [ 750/1251 ( 60%)]  Loss: 3.811 (3.88)  Time: 0.819s, 1250.72/s  (0.802s, 1277.15/s)  LR: 7.570e-04  Data: 0.011 (0.013)
Train: 99 [ 800/1251 ( 64%)]  Loss: 4.344 (3.91)  Time: 0.778s, 1316.16/s  (0.802s, 1277.34/s)  LR: 7.570e-04  Data: 0.011 (0.013)
Train: 99 [ 850/1251 ( 68%)]  Loss: 3.826 (3.90)  Time: 0.779s, 1313.77/s  (0.801s, 1278.67/s)  LR: 7.570e-04  Data: 0.011 (0.013)
Train: 99 [ 900/1251 ( 72%)]  Loss: 3.668 (3.89)  Time: 0.778s, 1316.23/s  (0.800s, 1280.02/s)  LR: 7.570e-04  Data: 0.011 (0.013)
Train: 99 [ 950/1251 ( 76%)]  Loss: 3.953 (3.89)  Time: 0.780s, 1312.53/s  (0.800s, 1280.70/s)  LR: 7.570e-04  Data: 0.011 (0.013)
Train: 99 [1000/1251 ( 80%)]  Loss: 4.015 (3.90)  Time: 0.782s, 1310.28/s  (0.799s, 1281.43/s)  LR: 7.570e-04  Data: 0.011 (0.013)
Train: 99 [1050/1251 ( 84%)]  Loss: 3.566 (3.88)  Time: 0.789s, 1298.24/s  (0.798s, 1282.53/s)  LR: 7.570e-04  Data: 0.010 (0.013)
Train: 99 [1100/1251 ( 88%)]  Loss: 3.954 (3.89)  Time: 0.813s, 1259.84/s  (0.798s, 1282.63/s)  LR: 7.570e-04  Data: 0.011 (0.013)
Train: 99 [1150/1251 ( 92%)]  Loss: 3.480 (3.87)  Time: 0.781s, 1311.82/s  (0.798s, 1283.25/s)  LR: 7.570e-04  Data: 0.010 (0.013)
Train: 99 [1200/1251 ( 96%)]  Loss: 3.947 (3.87)  Time: 0.780s, 1312.27/s  (0.798s, 1283.95/s)  LR: 7.570e-04  Data: 0.011 (0.013)
Train: 99 [1250/1251 (100%)]  Loss: 4.188 (3.89)  Time: 0.775s, 1322.13/s  (0.797s, 1284.77/s)  LR: 7.570e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.548 (1.548)  Loss:  1.0447 (1.0447)  Acc@1: 84.7656 (84.7656)  Acc@5: 95.5078 (95.5078)
Test: [  48/48]  Time: 0.172 (0.569)  Loss:  1.0829 (1.6617)  Acc@1: 82.5472 (69.1620)  Acc@5: 94.6934 (89.3200)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-92.pth.tar', 70.16800001220703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-94.pth.tar', 69.97399999511718)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-93.pth.tar', 69.79600007080079)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-96.pth.tar', 69.78399999267579)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-95.pth.tar', 69.66000017089844)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-97.pth.tar', 69.6300001538086)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-81.pth.tar', 69.34600004882813)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-98.pth.tar', 69.20999989257813)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-78.pth.tar', 69.1979999975586)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-99.pth.tar', 69.16200004638672)

Train: 100 [   0/1251 (  0%)]  Loss: 3.847 (3.85)  Time: 2.294s,  446.48/s  (2.294s,  446.48/s)  LR: 7.525e-04  Data: 1.559 (1.559)
Train: 100 [  50/1251 (  4%)]  Loss: 3.742 (3.79)  Time: 0.837s, 1222.97/s  (0.831s, 1232.30/s)  LR: 7.525e-04  Data: 0.011 (0.047)
Train: 100 [ 100/1251 (  8%)]  Loss: 3.834 (3.81)  Time: 0.822s, 1245.86/s  (0.817s, 1253.25/s)  LR: 7.525e-04  Data: 0.011 (0.029)
Train: 100 [ 150/1251 ( 12%)]  Loss: 3.630 (3.76)  Time: 0.780s, 1312.39/s  (0.807s, 1268.97/s)  LR: 7.525e-04  Data: 0.011 (0.023)
Train: 100 [ 200/1251 ( 16%)]  Loss: 3.961 (3.80)  Time: 0.779s, 1314.87/s  (0.801s, 1278.07/s)  LR: 7.525e-04  Data: 0.011 (0.020)
Train: 100 [ 250/1251 ( 20%)]  Loss: 3.626 (3.77)  Time: 0.778s, 1315.53/s  (0.799s, 1281.67/s)  LR: 7.525e-04  Data: 0.011 (0.018)
Train: 100 [ 300/1251 ( 24%)]  Loss: 3.971 (3.80)  Time: 0.836s, 1225.18/s  (0.801s, 1277.81/s)  LR: 7.525e-04  Data: 0.011 (0.017)
Train: 100 [ 350/1251 ( 28%)]  Loss: 3.820 (3.80)  Time: 0.778s, 1315.43/s  (0.801s, 1278.47/s)  LR: 7.525e-04  Data: 0.011 (0.016)
Train: 100 [ 400/1251 ( 32%)]  Loss: 3.613 (3.78)  Time: 0.823s, 1244.47/s  (0.801s, 1278.99/s)  LR: 7.525e-04  Data: 0.011 (0.016)
Train: 100 [ 450/1251 ( 36%)]  Loss: 3.886 (3.79)  Time: 0.787s, 1301.96/s  (0.799s, 1281.00/s)  LR: 7.525e-04  Data: 0.011 (0.015)
Train: 100 [ 500/1251 ( 40%)]  Loss: 4.312 (3.84)  Time: 0.816s, 1254.43/s  (0.799s, 1281.24/s)  LR: 7.525e-04  Data: 0.011 (0.015)
Train: 100 [ 550/1251 ( 44%)]  Loss: 3.769 (3.83)  Time: 0.777s, 1317.19/s  (0.801s, 1278.30/s)  LR: 7.525e-04  Data: 0.010 (0.014)
Train: 100 [ 600/1251 ( 48%)]  Loss: 3.935 (3.84)  Time: 0.812s, 1261.49/s  (0.801s, 1278.54/s)  LR: 7.525e-04  Data: 0.011 (0.014)
Train: 100 [ 650/1251 ( 52%)]  Loss: 3.693 (3.83)  Time: 0.780s, 1312.39/s  (0.801s, 1279.00/s)  LR: 7.525e-04  Data: 0.012 (0.014)
Train: 100 [ 700/1251 ( 56%)]  Loss: 3.673 (3.82)  Time: 0.778s, 1316.33/s  (0.799s, 1281.18/s)  LR: 7.525e-04  Data: 0.010 (0.014)
Train: 100 [ 750/1251 ( 60%)]  Loss: 3.759 (3.82)  Time: 0.779s, 1313.85/s  (0.798s, 1282.82/s)  LR: 7.525e-04  Data: 0.010 (0.013)
Train: 100 [ 800/1251 ( 64%)]  Loss: 4.125 (3.84)  Time: 0.780s, 1312.19/s  (0.797s, 1284.04/s)  LR: 7.525e-04  Data: 0.011 (0.013)
Train: 100 [ 850/1251 ( 68%)]  Loss: 4.052 (3.85)  Time: 0.814s, 1258.51/s  (0.797s, 1284.02/s)  LR: 7.525e-04  Data: 0.010 (0.013)
Train: 100 [ 900/1251 ( 72%)]  Loss: 3.637 (3.84)  Time: 0.829s, 1234.74/s  (0.798s, 1282.65/s)  LR: 7.525e-04  Data: 0.011 (0.013)
Train: 100 [ 950/1251 ( 76%)]  Loss: 3.980 (3.84)  Time: 0.817s, 1253.40/s  (0.798s, 1282.74/s)  LR: 7.525e-04  Data: 0.012 (0.013)
Train: 100 [1000/1251 ( 80%)]  Loss: 3.508 (3.83)  Time: 0.779s, 1314.06/s  (0.798s, 1283.01/s)  LR: 7.525e-04  Data: 0.011 (0.013)
Train: 100 [1050/1251 ( 84%)]  Loss: 3.780 (3.83)  Time: 0.781s, 1311.87/s  (0.797s, 1284.26/s)  LR: 7.525e-04  Data: 0.012 (0.013)
Train: 100 [1100/1251 ( 88%)]  Loss: 3.896 (3.83)  Time: 0.838s, 1222.31/s  (0.797s, 1284.73/s)  LR: 7.525e-04  Data: 0.011 (0.013)
Train: 100 [1150/1251 ( 92%)]  Loss: 3.591 (3.82)  Time: 0.780s, 1313.37/s  (0.797s, 1285.18/s)  LR: 7.525e-04  Data: 0.011 (0.013)
Train: 100 [1200/1251 ( 96%)]  Loss: 3.850 (3.82)  Time: 0.780s, 1312.93/s  (0.796s, 1285.66/s)  LR: 7.525e-04  Data: 0.011 (0.013)
Train: 100 [1250/1251 (100%)]  Loss: 3.875 (3.82)  Time: 0.767s, 1335.48/s  (0.796s, 1286.52/s)  LR: 7.525e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.588 (1.588)  Loss:  1.1147 (1.1147)  Acc@1: 85.6445 (85.6445)  Acc@5: 96.0938 (96.0938)
Test: [  48/48]  Time: 0.172 (0.568)  Loss:  1.1876 (1.6182)  Acc@1: 83.0189 (69.8380)  Acc@5: 95.4009 (89.5140)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-92.pth.tar', 70.16800001220703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-94.pth.tar', 69.97399999511718)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-100.pth.tar', 69.83799999267578)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-93.pth.tar', 69.79600007080079)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-96.pth.tar', 69.78399999267579)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-95.pth.tar', 69.66000017089844)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-97.pth.tar', 69.6300001538086)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-81.pth.tar', 69.34600004882813)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-98.pth.tar', 69.20999989257813)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-78.pth.tar', 69.1979999975586)

Train: 101 [   0/1251 (  0%)]  Loss: 3.781 (3.78)  Time: 2.373s,  431.52/s  (2.373s,  431.52/s)  LR: 7.480e-04  Data: 1.635 (1.635)
Train: 101 [  50/1251 (  4%)]  Loss: 3.524 (3.65)  Time: 0.780s, 1313.46/s  (0.823s, 1244.98/s)  LR: 7.480e-04  Data: 0.011 (0.045)
Train: 101 [ 100/1251 (  8%)]  Loss: 3.710 (3.67)  Time: 0.777s, 1317.41/s  (0.804s, 1273.52/s)  LR: 7.480e-04  Data: 0.010 (0.028)
Train: 101 [ 150/1251 ( 12%)]  Loss: 3.875 (3.72)  Time: 0.786s, 1303.13/s  (0.797s, 1284.54/s)  LR: 7.480e-04  Data: 0.011 (0.022)
Train: 101 [ 200/1251 ( 16%)]  Loss: 4.068 (3.79)  Time: 0.783s, 1307.76/s  (0.794s, 1289.80/s)  LR: 7.480e-04  Data: 0.010 (0.019)
Train: 101 [ 250/1251 ( 20%)]  Loss: 3.959 (3.82)  Time: 0.777s, 1318.07/s  (0.792s, 1292.50/s)  LR: 7.480e-04  Data: 0.011 (0.018)
Train: 101 [ 300/1251 ( 24%)]  Loss: 3.654 (3.80)  Time: 0.780s, 1313.05/s  (0.794s, 1289.82/s)  LR: 7.480e-04  Data: 0.011 (0.017)
Train: 101 [ 350/1251 ( 28%)]  Loss: 3.583 (3.77)  Time: 0.793s, 1291.03/s  (0.793s, 1291.05/s)  LR: 7.480e-04  Data: 0.012 (0.016)
Train: 101 [ 400/1251 ( 32%)]  Loss: 3.819 (3.77)  Time: 0.779s, 1313.73/s  (0.792s, 1293.02/s)  LR: 7.480e-04  Data: 0.011 (0.015)
Train: 101 [ 450/1251 ( 36%)]  Loss: 3.988 (3.80)  Time: 0.783s, 1306.98/s  (0.791s, 1293.99/s)  LR: 7.480e-04  Data: 0.011 (0.015)
Train: 101 [ 500/1251 ( 40%)]  Loss: 3.918 (3.81)  Time: 0.812s, 1261.31/s  (0.793s, 1291.14/s)  LR: 7.480e-04  Data: 0.011 (0.014)
Train: 101 [ 550/1251 ( 44%)]  Loss: 3.972 (3.82)  Time: 0.812s, 1261.71/s  (0.793s, 1290.99/s)  LR: 7.480e-04  Data: 0.011 (0.014)
Train: 101 [ 600/1251 ( 48%)]  Loss: 3.994 (3.83)  Time: 0.790s, 1296.31/s  (0.793s, 1290.58/s)  LR: 7.480e-04  Data: 0.015 (0.014)
Train: 101 [ 650/1251 ( 52%)]  Loss: 4.007 (3.85)  Time: 0.821s, 1247.51/s  (0.794s, 1289.31/s)  LR: 7.480e-04  Data: 0.011 (0.014)
Train: 101 [ 700/1251 ( 56%)]  Loss: 3.913 (3.85)  Time: 0.779s, 1315.32/s  (0.795s, 1287.42/s)  LR: 7.480e-04  Data: 0.011 (0.013)
Train: 101 [ 750/1251 ( 60%)]  Loss: 4.078 (3.87)  Time: 0.778s, 1316.39/s  (0.796s, 1286.31/s)  LR: 7.480e-04  Data: 0.010 (0.013)
Train: 101 [ 800/1251 ( 64%)]  Loss: 3.625 (3.85)  Time: 0.781s, 1310.41/s  (0.796s, 1287.00/s)  LR: 7.480e-04  Data: 0.011 (0.013)
Train: 101 [ 850/1251 ( 68%)]  Loss: 3.663 (3.84)  Time: 0.777s, 1318.17/s  (0.795s, 1287.56/s)  LR: 7.480e-04  Data: 0.011 (0.013)
Train: 101 [ 900/1251 ( 72%)]  Loss: 4.046 (3.85)  Time: 0.833s, 1229.04/s  (0.795s, 1287.53/s)  LR: 7.480e-04  Data: 0.011 (0.013)
Train: 101 [ 950/1251 ( 76%)]  Loss: 4.138 (3.87)  Time: 0.813s, 1259.34/s  (0.797s, 1285.44/s)  LR: 7.480e-04  Data: 0.011 (0.013)
Train: 101 [1000/1251 ( 80%)]  Loss: 4.092 (3.88)  Time: 0.789s, 1297.94/s  (0.797s, 1285.58/s)  LR: 7.480e-04  Data: 0.012 (0.013)
Train: 101 [1050/1251 ( 84%)]  Loss: 4.004 (3.88)  Time: 0.778s, 1315.71/s  (0.796s, 1285.98/s)  LR: 7.480e-04  Data: 0.011 (0.013)
Train: 101 [1100/1251 ( 88%)]  Loss: 3.442 (3.86)  Time: 0.794s, 1289.87/s  (0.796s, 1286.79/s)  LR: 7.480e-04  Data: 0.011 (0.013)
Train: 101 [1150/1251 ( 92%)]  Loss: 4.025 (3.87)  Time: 0.830s, 1234.33/s  (0.795s, 1287.44/s)  LR: 7.480e-04  Data: 0.011 (0.012)
Train: 101 [1200/1251 ( 96%)]  Loss: 4.007 (3.88)  Time: 0.800s, 1280.17/s  (0.795s, 1287.88/s)  LR: 7.480e-04  Data: 0.011 (0.012)
Train: 101 [1250/1251 (100%)]  Loss: 3.733 (3.87)  Time: 0.805s, 1271.61/s  (0.796s, 1287.08/s)  LR: 7.480e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.497 (1.497)  Loss:  0.9597 (0.9597)  Acc@1: 86.8164 (86.8164)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.172 (0.558)  Loss:  1.1365 (1.5902)  Acc@1: 81.3679 (69.5000)  Acc@5: 94.8113 (89.4440)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-92.pth.tar', 70.16800001220703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-94.pth.tar', 69.97399999511718)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-100.pth.tar', 69.83799999267578)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-93.pth.tar', 69.79600007080079)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-96.pth.tar', 69.78399999267579)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-95.pth.tar', 69.66000017089844)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-97.pth.tar', 69.6300001538086)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-101.pth.tar', 69.50000005126954)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-81.pth.tar', 69.34600004882813)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-98.pth.tar', 69.20999989257813)

Train: 102 [   0/1251 (  0%)]  Loss: 3.743 (3.74)  Time: 2.305s,  444.31/s  (2.305s,  444.31/s)  LR: 7.435e-04  Data: 1.508 (1.508)
Train: 102 [  50/1251 (  4%)]  Loss: 3.513 (3.63)  Time: 0.779s, 1315.15/s  (0.815s, 1255.95/s)  LR: 7.435e-04  Data: 0.011 (0.046)
Train: 102 [ 100/1251 (  8%)]  Loss: 3.516 (3.59)  Time: 0.780s, 1312.19/s  (0.803s, 1274.72/s)  LR: 7.435e-04  Data: 0.012 (0.029)
Train: 102 [ 150/1251 ( 12%)]  Loss: 4.111 (3.72)  Time: 0.791s, 1294.95/s  (0.797s, 1285.00/s)  LR: 7.435e-04  Data: 0.011 (0.023)
Train: 102 [ 200/1251 ( 16%)]  Loss: 3.658 (3.71)  Time: 0.786s, 1303.10/s  (0.797s, 1284.39/s)  LR: 7.435e-04  Data: 0.011 (0.020)
Train: 102 [ 250/1251 ( 20%)]  Loss: 3.420 (3.66)  Time: 0.780s, 1312.20/s  (0.795s, 1287.60/s)  LR: 7.435e-04  Data: 0.011 (0.018)
Train: 102 [ 300/1251 ( 24%)]  Loss: 3.699 (3.67)  Time: 0.779s, 1315.07/s  (0.796s, 1286.36/s)  LR: 7.435e-04  Data: 0.011 (0.017)
Train: 102 [ 350/1251 ( 28%)]  Loss: 3.898 (3.69)  Time: 0.815s, 1256.67/s  (0.796s, 1286.57/s)  LR: 7.435e-04  Data: 0.010 (0.016)
Train: 102 [ 400/1251 ( 32%)]  Loss: 3.747 (3.70)  Time: 0.779s, 1315.00/s  (0.795s, 1288.27/s)  LR: 7.435e-04  Data: 0.012 (0.016)
Train: 102 [ 450/1251 ( 36%)]  Loss: 3.710 (3.70)  Time: 0.783s, 1307.88/s  (0.795s, 1287.51/s)  LR: 7.435e-04  Data: 0.011 (0.015)
Train: 102 [ 500/1251 ( 40%)]  Loss: 3.850 (3.71)  Time: 0.780s, 1312.93/s  (0.796s, 1286.67/s)  LR: 7.435e-04  Data: 0.011 (0.015)
Train: 102 [ 550/1251 ( 44%)]  Loss: 4.020 (3.74)  Time: 0.779s, 1314.59/s  (0.795s, 1288.59/s)  LR: 7.435e-04  Data: 0.011 (0.014)
Train: 102 [ 600/1251 ( 48%)]  Loss: 3.535 (3.72)  Time: 0.813s, 1260.20/s  (0.794s, 1289.89/s)  LR: 7.435e-04  Data: 0.011 (0.014)
Train: 102 [ 650/1251 ( 52%)]  Loss: 4.113 (3.75)  Time: 0.778s, 1316.36/s  (0.793s, 1290.68/s)  LR: 7.435e-04  Data: 0.011 (0.014)
Train: 102 [ 700/1251 ( 56%)]  Loss: 3.649 (3.75)  Time: 0.796s, 1285.71/s  (0.794s, 1289.31/s)  LR: 7.435e-04  Data: 0.011 (0.014)
Train: 102 [ 750/1251 ( 60%)]  Loss: 4.010 (3.76)  Time: 0.830s, 1234.43/s  (0.794s, 1289.05/s)  LR: 7.435e-04  Data: 0.012 (0.013)
Train: 102 [ 800/1251 ( 64%)]  Loss: 3.893 (3.77)  Time: 0.781s, 1311.44/s  (0.796s, 1287.04/s)  LR: 7.435e-04  Data: 0.011 (0.013)
Train: 102 [ 850/1251 ( 68%)]  Loss: 4.242 (3.80)  Time: 0.790s, 1295.86/s  (0.797s, 1285.43/s)  LR: 7.435e-04  Data: 0.012 (0.013)
Train: 102 [ 900/1251 ( 72%)]  Loss: 3.475 (3.78)  Time: 0.781s, 1311.64/s  (0.797s, 1285.49/s)  LR: 7.435e-04  Data: 0.011 (0.013)
Train: 102 [ 950/1251 ( 76%)]  Loss: 3.829 (3.78)  Time: 0.781s, 1311.07/s  (0.796s, 1286.71/s)  LR: 7.435e-04  Data: 0.011 (0.013)
Train: 102 [1000/1251 ( 80%)]  Loss: 3.869 (3.79)  Time: 0.819s, 1249.90/s  (0.796s, 1287.16/s)  LR: 7.435e-04  Data: 0.011 (0.013)
Train: 102 [1050/1251 ( 84%)]  Loss: 3.945 (3.79)  Time: 0.780s, 1312.45/s  (0.795s, 1288.06/s)  LR: 7.435e-04  Data: 0.012 (0.013)
Train: 102 [1100/1251 ( 88%)]  Loss: 4.214 (3.81)  Time: 0.779s, 1314.11/s  (0.794s, 1288.97/s)  LR: 7.435e-04  Data: 0.011 (0.013)
Train: 102 [1150/1251 ( 92%)]  Loss: 3.936 (3.82)  Time: 0.815s, 1255.79/s  (0.795s, 1288.12/s)  LR: 7.435e-04  Data: 0.011 (0.013)
Train: 102 [1200/1251 ( 96%)]  Loss: 3.672 (3.81)  Time: 0.779s, 1314.18/s  (0.796s, 1286.60/s)  LR: 7.435e-04  Data: 0.011 (0.013)
Train: 102 [1250/1251 (100%)]  Loss: 4.169 (3.82)  Time: 0.808s, 1267.20/s  (0.797s, 1285.37/s)  LR: 7.435e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.533 (1.533)  Loss:  0.9142 (0.9142)  Acc@1: 86.7188 (86.7188)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.172 (0.568)  Loss:  0.9824 (1.5731)  Acc@1: 83.2547 (70.5040)  Acc@5: 95.5189 (89.9860)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-102.pth.tar', 70.50400009521485)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-92.pth.tar', 70.16800001220703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-94.pth.tar', 69.97399999511718)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-100.pth.tar', 69.83799999267578)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-93.pth.tar', 69.79600007080079)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-96.pth.tar', 69.78399999267579)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-95.pth.tar', 69.66000017089844)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-97.pth.tar', 69.6300001538086)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-101.pth.tar', 69.50000005126954)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-81.pth.tar', 69.34600004882813)

Train: 103 [   0/1251 (  0%)]  Loss: 3.971 (3.97)  Time: 2.364s,  433.15/s  (2.364s,  433.15/s)  LR: 7.389e-04  Data: 1.628 (1.628)
Train: 103 [  50/1251 (  4%)]  Loss: 3.490 (3.73)  Time: 0.813s, 1260.21/s  (0.826s, 1239.42/s)  LR: 7.389e-04  Data: 0.011 (0.047)
Train: 103 [ 100/1251 (  8%)]  Loss: 3.887 (3.78)  Time: 0.796s, 1286.15/s  (0.813s, 1260.26/s)  LR: 7.389e-04  Data: 0.011 (0.029)
Train: 103 [ 150/1251 ( 12%)]  Loss: 3.413 (3.69)  Time: 0.786s, 1302.83/s  (0.807s, 1268.90/s)  LR: 7.389e-04  Data: 0.011 (0.023)
Train: 103 [ 200/1251 ( 16%)]  Loss: 4.288 (3.81)  Time: 0.780s, 1312.76/s  (0.804s, 1273.89/s)  LR: 7.389e-04  Data: 0.011 (0.020)
Train: 103 [ 250/1251 ( 20%)]  Loss: 4.128 (3.86)  Time: 0.820s, 1248.60/s  (0.804s, 1273.41/s)  LR: 7.389e-04  Data: 0.011 (0.018)
Train: 103 [ 300/1251 ( 24%)]  Loss: 3.955 (3.88)  Time: 0.817s, 1252.74/s  (0.802s, 1276.24/s)  LR: 7.389e-04  Data: 0.010 (0.017)
Train: 103 [ 350/1251 ( 28%)]  Loss: 3.646 (3.85)  Time: 0.775s, 1321.93/s  (0.804s, 1273.38/s)  LR: 7.389e-04  Data: 0.011 (0.016)
Train: 103 [ 400/1251 ( 32%)]  Loss: 3.738 (3.84)  Time: 0.778s, 1316.67/s  (0.804s, 1273.79/s)  LR: 7.389e-04  Data: 0.011 (0.016)
Train: 103 [ 450/1251 ( 36%)]  Loss: 3.582 (3.81)  Time: 0.781s, 1311.34/s  (0.801s, 1277.74/s)  LR: 7.389e-04  Data: 0.011 (0.015)
Train: 103 [ 500/1251 ( 40%)]  Loss: 3.837 (3.81)  Time: 0.815s, 1256.27/s  (0.801s, 1278.83/s)  LR: 7.389e-04  Data: 0.011 (0.015)
Train: 103 [ 550/1251 ( 44%)]  Loss: 4.023 (3.83)  Time: 0.815s, 1255.82/s  (0.801s, 1278.95/s)  LR: 7.389e-04  Data: 0.010 (0.014)
Train: 103 [ 600/1251 ( 48%)]  Loss: 3.901 (3.84)  Time: 0.779s, 1314.00/s  (0.801s, 1278.87/s)  LR: 7.389e-04  Data: 0.011 (0.014)
Train: 103 [ 650/1251 ( 52%)]  Loss: 4.028 (3.85)  Time: 0.792s, 1293.70/s  (0.801s, 1278.06/s)  LR: 7.389e-04  Data: 0.011 (0.014)
Train: 103 [ 700/1251 ( 56%)]  Loss: 3.792 (3.85)  Time: 0.779s, 1314.94/s  (0.800s, 1279.55/s)  LR: 7.389e-04  Data: 0.011 (0.014)
Train: 103 [ 750/1251 ( 60%)]  Loss: 4.114 (3.86)  Time: 0.780s, 1312.25/s  (0.801s, 1278.33/s)  LR: 7.389e-04  Data: 0.011 (0.013)
Train: 103 [ 800/1251 ( 64%)]  Loss: 3.941 (3.87)  Time: 0.805s, 1272.71/s  (0.800s, 1279.67/s)  LR: 7.389e-04  Data: 0.010 (0.013)
Train: 103 [ 850/1251 ( 68%)]  Loss: 3.753 (3.86)  Time: 0.779s, 1314.76/s  (0.799s, 1280.98/s)  LR: 7.389e-04  Data: 0.011 (0.013)
Train: 103 [ 900/1251 ( 72%)]  Loss: 4.101 (3.87)  Time: 0.781s, 1310.67/s  (0.799s, 1282.29/s)  LR: 7.389e-04  Data: 0.011 (0.013)
Train: 103 [ 950/1251 ( 76%)]  Loss: 4.008 (3.88)  Time: 0.780s, 1312.97/s  (0.798s, 1283.43/s)  LR: 7.389e-04  Data: 0.012 (0.013)
Train: 103 [1000/1251 ( 80%)]  Loss: 4.202 (3.90)  Time: 0.777s, 1317.83/s  (0.797s, 1284.42/s)  LR: 7.389e-04  Data: 0.011 (0.013)
Train: 103 [1050/1251 ( 84%)]  Loss: 3.625 (3.88)  Time: 0.817s, 1254.12/s  (0.798s, 1282.93/s)  LR: 7.389e-04  Data: 0.011 (0.013)
Train: 103 [1100/1251 ( 88%)]  Loss: 4.030 (3.89)  Time: 0.782s, 1309.61/s  (0.798s, 1282.98/s)  LR: 7.389e-04  Data: 0.011 (0.013)
Train: 103 [1150/1251 ( 92%)]  Loss: 3.858 (3.89)  Time: 0.785s, 1304.79/s  (0.798s, 1283.58/s)  LR: 7.389e-04  Data: 0.013 (0.013)
Train: 103 [1200/1251 ( 96%)]  Loss: 3.828 (3.89)  Time: 0.779s, 1314.71/s  (0.797s, 1284.26/s)  LR: 7.389e-04  Data: 0.010 (0.012)
Train: 103 [1250/1251 (100%)]  Loss: 3.935 (3.89)  Time: 0.771s, 1328.60/s  (0.797s, 1285.39/s)  LR: 7.389e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.513 (1.513)  Loss:  0.9958 (0.9958)  Acc@1: 84.7656 (84.7656)  Acc@5: 95.7031 (95.7031)
Test: [  48/48]  Time: 0.172 (0.561)  Loss:  1.0981 (1.5245)  Acc@1: 82.1934 (69.9100)  Acc@5: 94.1038 (89.7680)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-102.pth.tar', 70.50400009521485)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-92.pth.tar', 70.16800001220703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-94.pth.tar', 69.97399999511718)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-103.pth.tar', 69.91000002197265)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-100.pth.tar', 69.83799999267578)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-93.pth.tar', 69.79600007080079)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-96.pth.tar', 69.78399999267579)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-95.pth.tar', 69.66000017089844)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-97.pth.tar', 69.6300001538086)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-101.pth.tar', 69.50000005126954)

Train: 104 [   0/1251 (  0%)]  Loss: 3.765 (3.77)  Time: 2.304s,  444.49/s  (2.304s,  444.49/s)  LR: 7.343e-04  Data: 1.568 (1.568)
Train: 104 [  50/1251 (  4%)]  Loss: 4.114 (3.94)  Time: 0.778s, 1316.55/s  (0.828s, 1237.43/s)  LR: 7.343e-04  Data: 0.011 (0.046)
Train: 104 [ 100/1251 (  8%)]  Loss: 3.925 (3.93)  Time: 0.834s, 1227.40/s  (0.807s, 1269.06/s)  LR: 7.343e-04  Data: 0.011 (0.029)
Train: 104 [ 150/1251 ( 12%)]  Loss: 3.767 (3.89)  Time: 0.782s, 1310.14/s  (0.803s, 1274.58/s)  LR: 7.343e-04  Data: 0.011 (0.023)
Train: 104 [ 200/1251 ( 16%)]  Loss: 3.867 (3.89)  Time: 0.789s, 1298.24/s  (0.800s, 1280.17/s)  LR: 7.343e-04  Data: 0.012 (0.020)
Train: 104 [ 250/1251 ( 20%)]  Loss: 3.940 (3.90)  Time: 0.802s, 1277.57/s  (0.797s, 1284.67/s)  LR: 7.343e-04  Data: 0.011 (0.018)
Train: 104 [ 300/1251 ( 24%)]  Loss: 4.341 (3.96)  Time: 0.794s, 1289.37/s  (0.796s, 1286.55/s)  LR: 7.343e-04  Data: 0.011 (0.017)
Train: 104 [ 350/1251 ( 28%)]  Loss: 4.009 (3.97)  Time: 0.843s, 1214.71/s  (0.799s, 1281.87/s)  LR: 7.343e-04  Data: 0.012 (0.016)
Train: 104 [ 400/1251 ( 32%)]  Loss: 4.187 (3.99)  Time: 0.779s, 1314.43/s  (0.799s, 1281.90/s)  LR: 7.343e-04  Data: 0.011 (0.016)
Train: 104 [ 450/1251 ( 36%)]  Loss: 3.902 (3.98)  Time: 0.778s, 1315.92/s  (0.798s, 1283.85/s)  LR: 7.343e-04  Data: 0.011 (0.015)
Train: 104 [ 500/1251 ( 40%)]  Loss: 3.786 (3.96)  Time: 0.777s, 1318.05/s  (0.797s, 1285.52/s)  LR: 7.343e-04  Data: 0.010 (0.015)
Train: 104 [ 550/1251 ( 44%)]  Loss: 3.662 (3.94)  Time: 0.779s, 1314.99/s  (0.796s, 1287.09/s)  LR: 7.343e-04  Data: 0.012 (0.014)
Train: 104 [ 600/1251 ( 48%)]  Loss: 3.767 (3.93)  Time: 0.780s, 1312.20/s  (0.795s, 1288.40/s)  LR: 7.343e-04  Data: 0.011 (0.014)
Train: 104 [ 650/1251 ( 52%)]  Loss: 3.788 (3.92)  Time: 0.786s, 1302.61/s  (0.794s, 1289.27/s)  LR: 7.343e-04  Data: 0.011 (0.014)
Train: 104 [ 700/1251 ( 56%)]  Loss: 3.719 (3.90)  Time: 0.780s, 1312.43/s  (0.794s, 1289.08/s)  LR: 7.343e-04  Data: 0.011 (0.014)
Train: 104 [ 750/1251 ( 60%)]  Loss: 3.741 (3.89)  Time: 0.784s, 1306.72/s  (0.795s, 1287.35/s)  LR: 7.343e-04  Data: 0.011 (0.013)
Train: 104 [ 800/1251 ( 64%)]  Loss: 3.930 (3.89)  Time: 0.781s, 1310.82/s  (0.795s, 1288.14/s)  LR: 7.343e-04  Data: 0.012 (0.013)
Train: 104 [ 850/1251 ( 68%)]  Loss: 3.735 (3.89)  Time: 0.783s, 1307.72/s  (0.795s, 1288.65/s)  LR: 7.343e-04  Data: 0.011 (0.013)
Train: 104 [ 900/1251 ( 72%)]  Loss: 4.040 (3.89)  Time: 0.791s, 1295.11/s  (0.795s, 1288.11/s)  LR: 7.343e-04  Data: 0.011 (0.013)
Train: 104 [ 950/1251 ( 76%)]  Loss: 3.787 (3.89)  Time: 0.779s, 1314.19/s  (0.795s, 1287.64/s)  LR: 7.343e-04  Data: 0.011 (0.013)
Train: 104 [1000/1251 ( 80%)]  Loss: 4.023 (3.90)  Time: 0.792s, 1292.43/s  (0.795s, 1288.34/s)  LR: 7.343e-04  Data: 0.012 (0.013)
Train: 104 [1050/1251 ( 84%)]  Loss: 3.762 (3.89)  Time: 0.789s, 1297.40/s  (0.795s, 1288.23/s)  LR: 7.343e-04  Data: 0.011 (0.013)
Train: 104 [1100/1251 ( 88%)]  Loss: 3.858 (3.89)  Time: 0.780s, 1313.10/s  (0.794s, 1289.14/s)  LR: 7.343e-04  Data: 0.011 (0.013)
Train: 104 [1150/1251 ( 92%)]  Loss: 3.603 (3.88)  Time: 0.782s, 1309.80/s  (0.794s, 1289.81/s)  LR: 7.343e-04  Data: 0.011 (0.013)
Train: 104 [1200/1251 ( 96%)]  Loss: 3.720 (3.87)  Time: 0.778s, 1316.80/s  (0.793s, 1290.68/s)  LR: 7.343e-04  Data: 0.011 (0.013)
Train: 104 [1250/1251 (100%)]  Loss: 4.125 (3.88)  Time: 0.772s, 1326.93/s  (0.793s, 1290.54/s)  LR: 7.343e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.532 (1.532)  Loss:  1.0959 (1.0959)  Acc@1: 85.0586 (85.0586)  Acc@5: 95.4102 (95.4102)
Test: [  48/48]  Time: 0.172 (0.560)  Loss:  1.0612 (1.6781)  Acc@1: 83.3726 (69.5860)  Acc@5: 95.2830 (89.3580)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-102.pth.tar', 70.50400009521485)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-92.pth.tar', 70.16800001220703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-94.pth.tar', 69.97399999511718)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-103.pth.tar', 69.91000002197265)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-100.pth.tar', 69.83799999267578)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-93.pth.tar', 69.79600007080079)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-96.pth.tar', 69.78399999267579)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-95.pth.tar', 69.66000017089844)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-97.pth.tar', 69.6300001538086)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-104.pth.tar', 69.58599988769531)

Train: 105 [   0/1251 (  0%)]  Loss: 3.920 (3.92)  Time: 2.259s,  453.30/s  (2.259s,  453.30/s)  LR: 7.297e-04  Data: 1.524 (1.524)
Train: 105 [  50/1251 (  4%)]  Loss: 3.799 (3.86)  Time: 0.789s, 1298.58/s  (0.848s, 1207.80/s)  LR: 7.297e-04  Data: 0.011 (0.047)
Train: 105 [ 100/1251 (  8%)]  Loss: 3.795 (3.84)  Time: 0.786s, 1303.38/s  (0.818s, 1252.39/s)  LR: 7.297e-04  Data: 0.010 (0.029)
Train: 105 [ 150/1251 ( 12%)]  Loss: 4.018 (3.88)  Time: 0.787s, 1300.33/s  (0.807s, 1268.50/s)  LR: 7.297e-04  Data: 0.012 (0.023)
Train: 105 [ 200/1251 ( 16%)]  Loss: 4.100 (3.93)  Time: 0.789s, 1297.16/s  (0.802s, 1277.47/s)  LR: 7.297e-04  Data: 0.011 (0.020)
Train: 105 [ 250/1251 ( 20%)]  Loss: 3.670 (3.88)  Time: 0.798s, 1282.63/s  (0.799s, 1282.00/s)  LR: 7.297e-04  Data: 0.011 (0.018)
Train: 105 [ 300/1251 ( 24%)]  Loss: 3.956 (3.89)  Time: 0.815s, 1257.20/s  (0.800s, 1280.15/s)  LR: 7.297e-04  Data: 0.013 (0.017)
Train: 105 [ 350/1251 ( 28%)]  Loss: 4.111 (3.92)  Time: 0.786s, 1302.30/s  (0.799s, 1280.84/s)  LR: 7.297e-04  Data: 0.011 (0.016)
Train: 105 [ 400/1251 ( 32%)]  Loss: 3.813 (3.91)  Time: 0.822s, 1245.57/s  (0.798s, 1282.74/s)  LR: 7.297e-04  Data: 0.011 (0.016)
Train: 105 [ 450/1251 ( 36%)]  Loss: 4.114 (3.93)  Time: 0.777s, 1318.42/s  (0.800s, 1279.20/s)  LR: 7.297e-04  Data: 0.011 (0.015)
Train: 105 [ 500/1251 ( 40%)]  Loss: 3.795 (3.92)  Time: 0.784s, 1306.01/s  (0.799s, 1281.05/s)  LR: 7.297e-04  Data: 0.010 (0.015)
Train: 105 [ 550/1251 ( 44%)]  Loss: 4.134 (3.94)  Time: 0.778s, 1316.76/s  (0.798s, 1283.25/s)  LR: 7.297e-04  Data: 0.011 (0.014)
Train: 105 [ 600/1251 ( 48%)]  Loss: 3.938 (3.94)  Time: 0.785s, 1304.69/s  (0.797s, 1284.57/s)  LR: 7.297e-04  Data: 0.013 (0.014)
Train: 105 [ 650/1251 ( 52%)]  Loss: 3.829 (3.93)  Time: 0.834s, 1227.57/s  (0.796s, 1285.71/s)  LR: 7.297e-04  Data: 0.012 (0.014)
Train: 105 [ 700/1251 ( 56%)]  Loss: 3.767 (3.92)  Time: 0.783s, 1307.16/s  (0.796s, 1286.65/s)  LR: 7.297e-04  Data: 0.011 (0.014)
Train: 105 [ 750/1251 ( 60%)]  Loss: 3.492 (3.89)  Time: 0.800s, 1280.25/s  (0.796s, 1286.52/s)  LR: 7.297e-04  Data: 0.011 (0.013)
Train: 105 [ 800/1251 ( 64%)]  Loss: 3.921 (3.89)  Time: 0.816s, 1254.85/s  (0.796s, 1286.52/s)  LR: 7.297e-04  Data: 0.011 (0.013)
Train: 105 [ 850/1251 ( 68%)]  Loss: 4.011 (3.90)  Time: 0.817s, 1254.10/s  (0.795s, 1287.42/s)  LR: 7.297e-04  Data: 0.011 (0.013)
Train: 105 [ 900/1251 ( 72%)]  Loss: 4.202 (3.92)  Time: 0.801s, 1278.80/s  (0.795s, 1287.92/s)  LR: 7.297e-04  Data: 0.010 (0.013)
Train: 105 [ 950/1251 ( 76%)]  Loss: 3.991 (3.92)  Time: 0.779s, 1314.44/s  (0.795s, 1288.60/s)  LR: 7.297e-04  Data: 0.011 (0.013)
Train: 105 [1000/1251 ( 80%)]  Loss: 3.391 (3.89)  Time: 0.789s, 1298.41/s  (0.794s, 1289.24/s)  LR: 7.297e-04  Data: 0.011 (0.013)
Train: 105 [1050/1251 ( 84%)]  Loss: 3.953 (3.90)  Time: 0.780s, 1312.21/s  (0.794s, 1290.25/s)  LR: 7.297e-04  Data: 0.012 (0.013)
Train: 105 [1100/1251 ( 88%)]  Loss: 3.994 (3.90)  Time: 0.778s, 1315.57/s  (0.794s, 1290.46/s)  LR: 7.297e-04  Data: 0.012 (0.013)
Train: 105 [1150/1251 ( 92%)]  Loss: 4.070 (3.91)  Time: 0.812s, 1261.82/s  (0.794s, 1289.98/s)  LR: 7.297e-04  Data: 0.012 (0.013)
Train: 105 [1200/1251 ( 96%)]  Loss: 3.701 (3.90)  Time: 0.783s, 1308.60/s  (0.794s, 1289.29/s)  LR: 7.297e-04  Data: 0.010 (0.013)
Train: 105 [1250/1251 (100%)]  Loss: 3.971 (3.90)  Time: 0.795s, 1288.04/s  (0.794s, 1289.23/s)  LR: 7.297e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.567 (1.567)  Loss:  0.8897 (0.8897)  Acc@1: 86.0352 (86.0352)  Acc@5: 96.2891 (96.2891)
Test: [  48/48]  Time: 0.172 (0.558)  Loss:  1.0417 (1.5350)  Acc@1: 83.1368 (70.0880)  Acc@5: 95.5189 (89.7680)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-102.pth.tar', 70.50400009521485)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-92.pth.tar', 70.16800001220703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-105.pth.tar', 70.08800004394531)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-94.pth.tar', 69.97399999511718)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-103.pth.tar', 69.91000002197265)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-100.pth.tar', 69.83799999267578)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-93.pth.tar', 69.79600007080079)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-96.pth.tar', 69.78399999267579)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-95.pth.tar', 69.66000017089844)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-97.pth.tar', 69.6300001538086)

Train: 106 [   0/1251 (  0%)]  Loss: 4.146 (4.15)  Time: 2.266s,  451.93/s  (2.266s,  451.93/s)  LR: 7.251e-04  Data: 1.531 (1.531)
Train: 106 [  50/1251 (  4%)]  Loss: 4.362 (4.25)  Time: 0.813s, 1259.41/s  (0.844s, 1213.84/s)  LR: 7.251e-04  Data: 0.010 (0.051)
Train: 106 [ 100/1251 (  8%)]  Loss: 3.914 (4.14)  Time: 0.835s, 1226.75/s  (0.827s, 1238.15/s)  LR: 7.251e-04  Data: 0.011 (0.031)
Train: 106 [ 150/1251 ( 12%)]  Loss: 3.770 (4.05)  Time: 0.785s, 1305.28/s  (0.815s, 1256.18/s)  LR: 7.251e-04  Data: 0.011 (0.025)
Train: 106 [ 200/1251 ( 16%)]  Loss: 3.578 (3.95)  Time: 0.786s, 1302.65/s  (0.809s, 1265.57/s)  LR: 7.251e-04  Data: 0.011 (0.021)
Train: 106 [ 250/1251 ( 20%)]  Loss: 4.169 (3.99)  Time: 0.781s, 1311.23/s  (0.804s, 1273.35/s)  LR: 7.251e-04  Data: 0.011 (0.019)
Train: 106 [ 300/1251 ( 24%)]  Loss: 3.967 (3.99)  Time: 0.829s, 1235.77/s  (0.801s, 1278.08/s)  LR: 7.251e-04  Data: 0.013 (0.018)
Train: 106 [ 350/1251 ( 28%)]  Loss: 4.078 (4.00)  Time: 0.789s, 1298.34/s  (0.803s, 1275.98/s)  LR: 7.251e-04  Data: 0.014 (0.017)
Train: 106 [ 400/1251 ( 32%)]  Loss: 3.760 (3.97)  Time: 0.780s, 1312.28/s  (0.801s, 1278.38/s)  LR: 7.251e-04  Data: 0.011 (0.016)
Train: 106 [ 450/1251 ( 36%)]  Loss: 3.742 (3.95)  Time: 0.777s, 1317.80/s  (0.800s, 1279.84/s)  LR: 7.251e-04  Data: 0.011 (0.016)
Train: 106 [ 500/1251 ( 40%)]  Loss: 4.081 (3.96)  Time: 0.780s, 1312.69/s  (0.798s, 1282.43/s)  LR: 7.251e-04  Data: 0.012 (0.015)
Train: 106 [ 550/1251 ( 44%)]  Loss: 3.749 (3.94)  Time: 0.844s, 1212.63/s  (0.799s, 1281.25/s)  LR: 7.251e-04  Data: 0.011 (0.015)
Train: 106 [ 600/1251 ( 48%)]  Loss: 3.992 (3.95)  Time: 0.818s, 1252.40/s  (0.801s, 1278.03/s)  LR: 7.251e-04  Data: 0.011 (0.014)
Train: 106 [ 650/1251 ( 52%)]  Loss: 3.864 (3.94)  Time: 0.779s, 1314.82/s  (0.800s, 1279.35/s)  LR: 7.251e-04  Data: 0.011 (0.014)
Train: 106 [ 700/1251 ( 56%)]  Loss: 4.073 (3.95)  Time: 0.779s, 1314.57/s  (0.799s, 1280.96/s)  LR: 7.251e-04  Data: 0.011 (0.014)
Train: 106 [ 750/1251 ( 60%)]  Loss: 3.706 (3.93)  Time: 0.834s, 1227.27/s  (0.799s, 1282.29/s)  LR: 7.251e-04  Data: 0.011 (0.014)
Train: 106 [ 800/1251 ( 64%)]  Loss: 3.631 (3.92)  Time: 0.835s, 1225.94/s  (0.798s, 1283.22/s)  LR: 7.251e-04  Data: 0.011 (0.014)
Train: 106 [ 850/1251 ( 68%)]  Loss: 3.859 (3.91)  Time: 0.780s, 1312.49/s  (0.797s, 1284.31/s)  LR: 7.251e-04  Data: 0.011 (0.013)
Train: 106 [ 900/1251 ( 72%)]  Loss: 3.925 (3.91)  Time: 0.788s, 1300.31/s  (0.797s, 1285.19/s)  LR: 7.251e-04  Data: 0.011 (0.013)
Train: 106 [ 950/1251 ( 76%)]  Loss: 3.999 (3.92)  Time: 0.778s, 1316.83/s  (0.797s, 1285.45/s)  LR: 7.251e-04  Data: 0.011 (0.013)
Train: 106 [1000/1251 ( 80%)]  Loss: 3.847 (3.91)  Time: 0.779s, 1314.25/s  (0.796s, 1285.97/s)  LR: 7.251e-04  Data: 0.010 (0.013)
Train: 106 [1050/1251 ( 84%)]  Loss: 3.752 (3.91)  Time: 0.779s, 1315.01/s  (0.796s, 1285.96/s)  LR: 7.251e-04  Data: 0.011 (0.013)
Train: 106 [1100/1251 ( 88%)]  Loss: 4.308 (3.92)  Time: 0.784s, 1306.34/s  (0.796s, 1286.52/s)  LR: 7.251e-04  Data: 0.012 (0.013)
Train: 106 [1150/1251 ( 92%)]  Loss: 4.238 (3.94)  Time: 0.779s, 1314.48/s  (0.796s, 1286.54/s)  LR: 7.251e-04  Data: 0.011 (0.013)
Train: 106 [1200/1251 ( 96%)]  Loss: 3.819 (3.93)  Time: 0.776s, 1319.54/s  (0.796s, 1286.36/s)  LR: 7.251e-04  Data: 0.012 (0.013)
Train: 106 [1250/1251 (100%)]  Loss: 3.704 (3.92)  Time: 0.799s, 1281.43/s  (0.796s, 1286.45/s)  LR: 7.251e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.566 (1.566)  Loss:  0.9688 (0.9688)  Acc@1: 86.6211 (86.6211)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.172 (0.567)  Loss:  1.2260 (1.6312)  Acc@1: 82.4293 (69.3900)  Acc@5: 95.4009 (89.5160)
Train: 107 [   0/1251 (  0%)]  Loss: 4.063 (4.06)  Time: 2.288s,  447.52/s  (2.288s,  447.52/s)  LR: 7.204e-04  Data: 1.548 (1.548)
Train: 107 [  50/1251 (  4%)]  Loss: 4.052 (4.06)  Time: 0.781s, 1311.24/s  (0.821s, 1247.23/s)  LR: 7.204e-04  Data: 0.010 (0.046)
Train: 107 [ 100/1251 (  8%)]  Loss: 3.975 (4.03)  Time: 0.779s, 1314.67/s  (0.805s, 1271.82/s)  LR: 7.204e-04  Data: 0.011 (0.028)
Train: 107 [ 150/1251 ( 12%)]  Loss: 4.314 (4.10)  Time: 0.778s, 1316.70/s  (0.798s, 1283.46/s)  LR: 7.204e-04  Data: 0.011 (0.022)
Train: 107 [ 200/1251 ( 16%)]  Loss: 3.643 (4.01)  Time: 0.819s, 1249.98/s  (0.798s, 1283.39/s)  LR: 7.204e-04  Data: 0.011 (0.020)
Train: 107 [ 250/1251 ( 20%)]  Loss: 3.863 (3.99)  Time: 0.777s, 1317.92/s  (0.795s, 1287.35/s)  LR: 7.204e-04  Data: 0.010 (0.018)
Train: 107 [ 300/1251 ( 24%)]  Loss: 3.966 (3.98)  Time: 0.780s, 1312.85/s  (0.794s, 1290.29/s)  LR: 7.204e-04  Data: 0.011 (0.017)
Train: 107 [ 350/1251 ( 28%)]  Loss: 3.941 (3.98)  Time: 0.803s, 1274.94/s  (0.795s, 1288.18/s)  LR: 7.204e-04  Data: 0.011 (0.016)
Train: 107 [ 400/1251 ( 32%)]  Loss: 3.320 (3.90)  Time: 0.793s, 1291.09/s  (0.793s, 1290.57/s)  LR: 7.204e-04  Data: 0.011 (0.015)
Train: 107 [ 450/1251 ( 36%)]  Loss: 3.493 (3.86)  Time: 0.784s, 1305.35/s  (0.793s, 1292.07/s)  LR: 7.204e-04  Data: 0.010 (0.015)
Train: 107 [ 500/1251 ( 40%)]  Loss: 3.945 (3.87)  Time: 0.779s, 1313.92/s  (0.792s, 1293.02/s)  LR: 7.204e-04  Data: 0.011 (0.014)
Train: 107 [ 550/1251 ( 44%)]  Loss: 3.798 (3.86)  Time: 0.778s, 1315.76/s  (0.792s, 1292.15/s)  LR: 7.204e-04  Data: 0.011 (0.014)
Train: 107 [ 600/1251 ( 48%)]  Loss: 3.740 (3.85)  Time: 0.778s, 1316.19/s  (0.792s, 1293.16/s)  LR: 7.204e-04  Data: 0.011 (0.014)
Train: 107 [ 650/1251 ( 52%)]  Loss: 4.163 (3.88)  Time: 0.826s, 1239.28/s  (0.791s, 1294.21/s)  LR: 7.204e-04  Data: 0.010 (0.014)
Train: 107 [ 700/1251 ( 56%)]  Loss: 4.141 (3.89)  Time: 0.779s, 1314.85/s  (0.791s, 1294.04/s)  LR: 7.204e-04  Data: 0.012 (0.013)
Train: 107 [ 750/1251 ( 60%)]  Loss: 3.576 (3.87)  Time: 0.788s, 1299.14/s  (0.791s, 1294.40/s)  LR: 7.204e-04  Data: 0.012 (0.013)
Train: 107 [ 800/1251 ( 64%)]  Loss: 3.826 (3.87)  Time: 0.849s, 1205.89/s  (0.791s, 1294.66/s)  LR: 7.204e-04  Data: 0.011 (0.013)
Train: 107 [ 850/1251 ( 68%)]  Loss: 3.736 (3.86)  Time: 0.775s, 1320.67/s  (0.791s, 1295.38/s)  LR: 7.204e-04  Data: 0.010 (0.013)
Train: 107 [ 900/1251 ( 72%)]  Loss: 3.783 (3.86)  Time: 0.816s, 1254.56/s  (0.791s, 1294.98/s)  LR: 7.204e-04  Data: 0.011 (0.013)
Train: 107 [ 950/1251 ( 76%)]  Loss: 3.972 (3.87)  Time: 0.779s, 1315.08/s  (0.792s, 1293.39/s)  LR: 7.204e-04  Data: 0.011 (0.013)
Train: 107 [1000/1251 ( 80%)]  Loss: 3.370 (3.84)  Time: 0.786s, 1303.46/s  (0.792s, 1292.20/s)  LR: 7.204e-04  Data: 0.014 (0.013)
Train: 107 [1050/1251 ( 84%)]  Loss: 3.732 (3.84)  Time: 0.784s, 1305.70/s  (0.792s, 1292.20/s)  LR: 7.204e-04  Data: 0.010 (0.013)
Train: 107 [1100/1251 ( 88%)]  Loss: 3.964 (3.84)  Time: 0.778s, 1315.67/s  (0.793s, 1290.90/s)  LR: 7.204e-04  Data: 0.012 (0.013)
Train: 107 [1150/1251 ( 92%)]  Loss: 3.712 (3.84)  Time: 0.775s, 1321.04/s  (0.793s, 1291.02/s)  LR: 7.204e-04  Data: 0.010 (0.013)
Train: 107 [1200/1251 ( 96%)]  Loss: 4.132 (3.85)  Time: 0.779s, 1314.87/s  (0.793s, 1290.50/s)  LR: 7.204e-04  Data: 0.011 (0.012)
Train: 107 [1250/1251 (100%)]  Loss: 3.777 (3.85)  Time: 0.767s, 1334.97/s  (0.793s, 1291.04/s)  LR: 7.204e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.513 (1.513)  Loss:  0.9893 (0.9893)  Acc@1: 86.5234 (86.5234)  Acc@5: 95.6055 (95.6055)
Test: [  48/48]  Time: 0.172 (0.563)  Loss:  1.0982 (1.6362)  Acc@1: 83.0189 (69.8820)  Acc@5: 95.7547 (89.7440)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-102.pth.tar', 70.50400009521485)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-92.pth.tar', 70.16800001220703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-105.pth.tar', 70.08800004394531)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-94.pth.tar', 69.97399999511718)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-103.pth.tar', 69.91000002197265)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-107.pth.tar', 69.88199999267579)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-100.pth.tar', 69.83799999267578)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-93.pth.tar', 69.79600007080079)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-96.pth.tar', 69.78399999267579)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-95.pth.tar', 69.66000017089844)

Train: 108 [   0/1251 (  0%)]  Loss: 3.637 (3.64)  Time: 2.185s,  468.65/s  (2.185s,  468.65/s)  LR: 7.158e-04  Data: 1.449 (1.449)
Train: 108 [  50/1251 (  4%)]  Loss: 4.112 (3.87)  Time: 0.805s, 1271.71/s  (0.829s, 1235.65/s)  LR: 7.158e-04  Data: 0.011 (0.044)
Train: 108 [ 100/1251 (  8%)]  Loss: 3.894 (3.88)  Time: 0.778s, 1315.55/s  (0.823s, 1244.31/s)  LR: 7.158e-04  Data: 0.011 (0.028)
Train: 108 [ 150/1251 ( 12%)]  Loss: 3.742 (3.85)  Time: 0.779s, 1314.98/s  (0.811s, 1262.91/s)  LR: 7.158e-04  Data: 0.011 (0.022)
Train: 108 [ 200/1251 ( 16%)]  Loss: 3.598 (3.80)  Time: 0.793s, 1291.22/s  (0.808s, 1267.74/s)  LR: 7.158e-04  Data: 0.011 (0.020)
Train: 108 [ 250/1251 ( 20%)]  Loss: 3.583 (3.76)  Time: 0.787s, 1301.53/s  (0.804s, 1273.01/s)  LR: 7.158e-04  Data: 0.011 (0.018)
Train: 108 [ 300/1251 ( 24%)]  Loss: 3.794 (3.77)  Time: 0.798s, 1282.61/s  (0.804s, 1274.28/s)  LR: 7.158e-04  Data: 0.011 (0.017)
Train: 108 [ 350/1251 ( 28%)]  Loss: 4.118 (3.81)  Time: 0.790s, 1295.78/s  (0.801s, 1277.98/s)  LR: 7.158e-04  Data: 0.012 (0.016)
Train: 108 [ 400/1251 ( 32%)]  Loss: 3.432 (3.77)  Time: 0.778s, 1316.72/s  (0.800s, 1279.74/s)  LR: 7.158e-04  Data: 0.011 (0.015)
Train: 108 [ 450/1251 ( 36%)]  Loss: 3.582 (3.75)  Time: 0.779s, 1314.44/s  (0.798s, 1283.00/s)  LR: 7.158e-04  Data: 0.011 (0.015)
Train: 108 [ 500/1251 ( 40%)]  Loss: 3.530 (3.73)  Time: 0.780s, 1312.94/s  (0.796s, 1285.74/s)  LR: 7.158e-04  Data: 0.011 (0.015)
Train: 108 [ 550/1251 ( 44%)]  Loss: 3.814 (3.74)  Time: 0.781s, 1311.43/s  (0.795s, 1288.14/s)  LR: 7.158e-04  Data: 0.011 (0.014)
Train: 108 [ 600/1251 ( 48%)]  Loss: 3.825 (3.74)  Time: 0.779s, 1313.90/s  (0.794s, 1289.52/s)  LR: 7.158e-04  Data: 0.011 (0.014)
Train: 108 [ 650/1251 ( 52%)]  Loss: 4.022 (3.76)  Time: 0.782s, 1310.04/s  (0.793s, 1290.74/s)  LR: 7.158e-04  Data: 0.012 (0.014)
Train: 108 [ 700/1251 ( 56%)]  Loss: 3.860 (3.77)  Time: 0.846s, 1210.37/s  (0.794s, 1290.22/s)  LR: 7.158e-04  Data: 0.011 (0.014)
Train: 108 [ 750/1251 ( 60%)]  Loss: 3.835 (3.77)  Time: 0.827s, 1238.52/s  (0.795s, 1288.76/s)  LR: 7.158e-04  Data: 0.012 (0.013)
Train: 108 [ 800/1251 ( 64%)]  Loss: 3.769 (3.77)  Time: 0.781s, 1311.81/s  (0.795s, 1287.77/s)  LR: 7.158e-04  Data: 0.010 (0.013)
Train: 108 [ 850/1251 ( 68%)]  Loss: 3.699 (3.77)  Time: 0.780s, 1313.65/s  (0.795s, 1288.78/s)  LR: 7.158e-04  Data: 0.011 (0.013)
Train: 108 [ 900/1251 ( 72%)]  Loss: 3.842 (3.77)  Time: 0.780s, 1313.31/s  (0.794s, 1289.92/s)  LR: 7.158e-04  Data: 0.011 (0.013)
Train: 108 [ 950/1251 ( 76%)]  Loss: 3.876 (3.78)  Time: 0.840s, 1218.72/s  (0.795s, 1287.99/s)  LR: 7.158e-04  Data: 0.011 (0.013)
Train: 108 [1000/1251 ( 80%)]  Loss: 3.789 (3.78)  Time: 0.832s, 1231.45/s  (0.796s, 1285.78/s)  LR: 7.158e-04  Data: 0.011 (0.013)
Train: 108 [1050/1251 ( 84%)]  Loss: 3.920 (3.79)  Time: 0.851s, 1203.24/s  (0.796s, 1285.71/s)  LR: 7.158e-04  Data: 0.011 (0.013)
Train: 108 [1100/1251 ( 88%)]  Loss: 3.743 (3.78)  Time: 0.779s, 1313.92/s  (0.797s, 1285.36/s)  LR: 7.158e-04  Data: 0.011 (0.013)
Train: 108 [1150/1251 ( 92%)]  Loss: 3.550 (3.77)  Time: 0.817s, 1253.15/s  (0.797s, 1284.76/s)  LR: 7.158e-04  Data: 0.012 (0.013)
Train: 108 [1200/1251 ( 96%)]  Loss: 3.926 (3.78)  Time: 0.776s, 1319.85/s  (0.797s, 1285.42/s)  LR: 7.158e-04  Data: 0.011 (0.013)
Train: 108 [1250/1251 (100%)]  Loss: 4.087 (3.79)  Time: 0.768s, 1333.88/s  (0.796s, 1286.30/s)  LR: 7.158e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.531 (1.531)  Loss:  1.0386 (1.0386)  Acc@1: 86.2305 (86.2305)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.172 (0.563)  Loss:  1.0036 (1.5542)  Acc@1: 82.5472 (70.3040)  Acc@5: 94.9293 (89.8080)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-102.pth.tar', 70.50400009521485)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-108.pth.tar', 70.30399991699218)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-92.pth.tar', 70.16800001220703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-105.pth.tar', 70.08800004394531)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-94.pth.tar', 69.97399999511718)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-103.pth.tar', 69.91000002197265)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-107.pth.tar', 69.88199999267579)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-100.pth.tar', 69.83799999267578)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-93.pth.tar', 69.79600007080079)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-96.pth.tar', 69.78399999267579)

Train: 109 [   0/1251 (  0%)]  Loss: 3.504 (3.50)  Time: 2.240s,  457.11/s  (2.240s,  457.11/s)  LR: 7.111e-04  Data: 1.506 (1.506)
Train: 109 [  50/1251 (  4%)]  Loss: 3.640 (3.57)  Time: 0.836s, 1224.52/s  (0.813s, 1259.29/s)  LR: 7.111e-04  Data: 0.011 (0.045)
Train: 109 [ 100/1251 (  8%)]  Loss: 3.797 (3.65)  Time: 0.816s, 1255.50/s  (0.799s, 1281.30/s)  LR: 7.111e-04  Data: 0.011 (0.028)
Train: 109 [ 150/1251 ( 12%)]  Loss: 3.428 (3.59)  Time: 0.813s, 1259.24/s  (0.804s, 1274.07/s)  LR: 7.111e-04  Data: 0.011 (0.022)
Train: 109 [ 200/1251 ( 16%)]  Loss: 4.103 (3.69)  Time: 0.778s, 1316.87/s  (0.806s, 1270.65/s)  LR: 7.111e-04  Data: 0.011 (0.020)
Train: 109 [ 250/1251 ( 20%)]  Loss: 3.882 (3.73)  Time: 0.778s, 1316.38/s  (0.800s, 1279.52/s)  LR: 7.111e-04  Data: 0.011 (0.018)
Train: 109 [ 300/1251 ( 24%)]  Loss: 4.173 (3.79)  Time: 0.780s, 1313.42/s  (0.797s, 1285.57/s)  LR: 7.111e-04  Data: 0.011 (0.017)
Train: 109 [ 350/1251 ( 28%)]  Loss: 4.355 (3.86)  Time: 0.775s, 1320.72/s  (0.794s, 1289.68/s)  LR: 7.111e-04  Data: 0.011 (0.016)
Train: 109 [ 400/1251 ( 32%)]  Loss: 4.043 (3.88)  Time: 0.778s, 1315.54/s  (0.793s, 1291.85/s)  LR: 7.111e-04  Data: 0.011 (0.015)
Train: 109 [ 450/1251 ( 36%)]  Loss: 3.878 (3.88)  Time: 0.778s, 1316.50/s  (0.792s, 1292.39/s)  LR: 7.111e-04  Data: 0.011 (0.015)
Train: 109 [ 500/1251 ( 40%)]  Loss: 3.769 (3.87)  Time: 0.777s, 1318.68/s  (0.791s, 1294.61/s)  LR: 7.111e-04  Data: 0.011 (0.014)
/home/baoshengyu/anaconda3/lib/python3.7/site-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Traceback (most recent call last):
  File "/home/baoshengyu/anaconda3/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/baoshengyu/anaconda3/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/baoshengyu/anaconda3/lib/python3.7/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/home/baoshengyu/anaconda3/lib/python3.7/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/home/baoshengyu/anaconda3/lib/python3.7/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/home/baoshengyu/anaconda3/lib/python3.7/site-packages/torch/distributed/run.py", line 713, in run
    )(*cmd_args)
  File "/home/baoshengyu/anaconda3/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/baoshengyu/anaconda3/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 252, in launch_agent
    result = agent.run()
  File "/home/baoshengyu/anaconda3/lib/python3.7/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/baoshengyu/anaconda3/lib/python3.7/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/baoshengyu/anaconda3/lib/python3.7/site-packages/torch/distributed/elastic/agent/server/api.py", line 837, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/baoshengyu/anaconda3/lib/python3.7/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/baoshengyu/anaconda3/lib/python3.7/site-packages/torch/distributed/elastic/agent/server/api.py", line 678, in _initialize_workers
    self._rendezvous(worker_group)
  File "/home/baoshengyu/anaconda3/lib/python3.7/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/baoshengyu/anaconda3/lib/python3.7/site-packages/torch/distributed/elastic/agent/server/api.py", line 538, in _rendezvous
    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()
  File "/home/baoshengyu/anaconda3/lib/python3.7/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 61, in next_rendezvous
    multi_tenant=True,
RuntimeError: Address already in use
Train: 109 [ 550/1251 ( 44%)]  Loss: 4.066 (3.89)  Time: 0.779s, 1314.63/s  (0.790s, 1296.25/s)  LR: 7.111e-04  Data: 0.011 (0.014)
Train: 109 [ 600/1251 ( 48%)]  Loss: 4.062 (3.90)  Time: 0.777s, 1317.45/s  (0.789s, 1297.92/s)  LR: 7.111e-04  Data: 0.011 (0.014)
Train: 109 [ 650/1251 ( 52%)]  Loss: 3.885 (3.90)  Time: 0.781s, 1311.94/s  (0.788s, 1299.34/s)  LR: 7.111e-04  Data: 0.011 (0.014)
Train: 109 [ 700/1251 ( 56%)]  Loss: 3.573 (3.88)  Time: 0.778s, 1316.86/s  (0.787s, 1300.55/s)  LR: 7.111e-04  Data: 0.011 (0.013)
Train: 109 [ 750/1251 ( 60%)]  Loss: 3.857 (3.88)  Time: 0.812s, 1261.02/s  (0.788s, 1299.04/s)  LR: 7.111e-04  Data: 0.011 (0.013)
Train: 109 [ 800/1251 ( 64%)]  Loss: 4.018 (3.88)  Time: 0.813s, 1258.89/s  (0.790s, 1296.43/s)  LR: 7.111e-04  Data: 0.011 (0.013)
Train: 109 [ 850/1251 ( 68%)]  Loss: 3.582 (3.87)  Time: 0.778s, 1315.53/s  (0.791s, 1295.34/s)  LR: 7.111e-04  Data: 0.011 (0.013)
Train: 109 [ 900/1251 ( 72%)]  Loss: 4.099 (3.88)  Time: 0.777s, 1318.05/s  (0.790s, 1296.42/s)  LR: 7.111e-04  Data: 0.011 (0.013)
Train: 109 [ 950/1251 ( 76%)]  Loss: 3.866 (3.88)  Time: 0.779s, 1314.58/s  (0.790s, 1296.63/s)  LR: 7.111e-04  Data: 0.011 (0.013)
Train: 109 [1000/1251 ( 80%)]  Loss: 3.239 (3.85)  Time: 0.777s, 1317.63/s  (0.789s, 1297.61/s)  LR: 7.111e-04  Data: 0.011 (0.013)
Train: 109 [1050/1251 ( 84%)]  Loss: 4.152 (3.86)  Time: 0.780s, 1313.65/s  (0.789s, 1298.50/s)  LR: 7.111e-04  Data: 0.011 (0.013)
Train: 109 [1100/1251 ( 88%)]  Loss: 3.899 (3.86)  Time: 0.777s, 1318.01/s  (0.789s, 1297.70/s)  LR: 7.111e-04  Data: 0.011 (0.013)
Train: 109 [1150/1251 ( 92%)]  Loss: 3.787 (3.86)  Time: 0.780s, 1313.17/s  (0.789s, 1298.36/s)  LR: 7.111e-04  Data: 0.011 (0.012)
Train: 109 [1200/1251 ( 96%)]  Loss: 3.907 (3.86)  Time: 0.810s, 1263.98/s  (0.789s, 1297.74/s)  LR: 7.111e-04  Data: 0.011 (0.012)
Train: 109 [1250/1251 (100%)]  Loss: 3.807 (3.86)  Time: 0.829s, 1235.28/s  (0.790s, 1297.02/s)  LR: 7.111e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.485 (1.485)  Loss:  1.0129 (1.0129)  Acc@1: 86.1328 (86.1328)  Acc@5: 96.2891 (96.2891)
Test: [  48/48]  Time: 0.172 (0.554)  Loss:  0.9174 (1.5295)  Acc@1: 83.8443 (71.0600)  Acc@5: 95.9906 (90.2600)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-109.pth.tar', 71.0599999633789)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-102.pth.tar', 70.50400009521485)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-108.pth.tar', 70.30399991699218)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-92.pth.tar', 70.16800001220703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-105.pth.tar', 70.08800004394531)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-94.pth.tar', 69.97399999511718)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-103.pth.tar', 69.91000002197265)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-107.pth.tar', 69.88199999267579)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-100.pth.tar', 69.83799999267578)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-93.pth.tar', 69.79600007080079)

Train: 110 [   0/1251 (  0%)]  Loss: 3.671 (3.67)  Time: 2.234s,  458.44/s  (2.234s,  458.44/s)  LR: 7.063e-04  Data: 1.498 (1.498)
Train: 110 [  50/1251 (  4%)]  Loss: 4.038 (3.85)  Time: 0.811s, 1262.10/s  (0.813s, 1259.95/s)  LR: 7.063e-04  Data: 0.011 (0.043)
Train: 110 [ 100/1251 (  8%)]  Loss: 3.843 (3.85)  Time: 0.775s, 1320.45/s  (0.801s, 1278.75/s)  LR: 7.063e-04  Data: 0.011 (0.027)
Train: 110 [ 150/1251 ( 12%)]  Loss: 3.576 (3.78)  Time: 0.780s, 1312.69/s  (0.794s, 1289.39/s)  LR: 7.063e-04  Data: 0.011 (0.022)
Train: 110 [ 200/1251 ( 16%)]  Loss: 3.745 (3.77)  Time: 0.778s, 1315.40/s  (0.791s, 1293.82/s)  LR: 7.063e-04  Data: 0.011 (0.019)
Train: 110 [ 250/1251 ( 20%)]  Loss: 3.767 (3.77)  Time: 0.781s, 1311.14/s  (0.789s, 1297.77/s)  LR: 7.063e-04  Data: 0.011 (0.018)
Train: 110 [ 300/1251 ( 24%)]  Loss: 3.974 (3.80)  Time: 0.778s, 1316.71/s  (0.788s, 1300.05/s)  LR: 7.063e-04  Data: 0.011 (0.017)
Train: 110 [ 350/1251 ( 28%)]  Loss: 3.901 (3.81)  Time: 0.777s, 1317.54/s  (0.786s, 1302.13/s)  LR: 7.063e-04  Data: 0.011 (0.016)
Train: 110 [ 400/1251 ( 32%)]  Loss: 4.146 (3.85)  Time: 0.776s, 1319.62/s  (0.786s, 1303.48/s)  LR: 7.063e-04  Data: 0.011 (0.015)
Train: 110 [ 450/1251 ( 36%)]  Loss: 3.450 (3.81)  Time: 0.778s, 1316.23/s  (0.785s, 1304.90/s)  LR: 7.063e-04  Data: 0.011 (0.015)
Train: 110 [ 500/1251 ( 40%)]  Loss: 3.533 (3.79)  Time: 0.777s, 1317.50/s  (0.785s, 1304.89/s)  LR: 7.063e-04  Data: 0.011 (0.014)
Train: 110 [ 550/1251 ( 44%)]  Loss: 4.054 (3.81)  Time: 0.776s, 1318.81/s  (0.785s, 1304.32/s)  LR: 7.063e-04  Data: 0.010 (0.014)
Train: 110 [ 600/1251 ( 48%)]  Loss: 3.848 (3.81)  Time: 0.810s, 1264.76/s  (0.785s, 1305.03/s)  LR: 7.063e-04  Data: 0.011 (0.014)
Train: 110 [ 650/1251 ( 52%)]  Loss: 3.981 (3.82)  Time: 0.812s, 1260.87/s  (0.787s, 1301.13/s)  LR: 7.063e-04  Data: 0.011 (0.014)
Train: 110 [ 700/1251 ( 56%)]  Loss: 3.580 (3.81)  Time: 0.777s, 1318.49/s  (0.788s, 1299.68/s)  LR: 7.063e-04  Data: 0.011 (0.013)
Train: 110 [ 750/1251 ( 60%)]  Loss: 4.189 (3.83)  Time: 0.812s, 1261.14/s  (0.788s, 1298.69/s)  LR: 7.063e-04  Data: 0.011 (0.013)
Train: 110 [ 800/1251 ( 64%)]  Loss: 3.854 (3.83)  Time: 0.778s, 1315.72/s  (0.789s, 1298.65/s)  LR: 7.063e-04  Data: 0.010 (0.013)
Train: 110 [ 850/1251 ( 68%)]  Loss: 3.606 (3.82)  Time: 0.780s, 1312.84/s  (0.789s, 1297.97/s)  LR: 7.063e-04  Data: 0.011 (0.013)
Train: 110 [ 900/1251 ( 72%)]  Loss: 3.805 (3.82)  Time: 0.780s, 1312.85/s  (0.789s, 1297.24/s)  LR: 7.063e-04  Data: 0.011 (0.013)
Train: 110 [ 950/1251 ( 76%)]  Loss: 3.712 (3.81)  Time: 0.777s, 1318.26/s  (0.790s, 1296.47/s)  LR: 7.063e-04  Data: 0.011 (0.013)
Train: 110 [1000/1251 ( 80%)]  Loss: 3.796 (3.81)  Time: 0.780s, 1312.46/s  (0.790s, 1296.05/s)  LR: 7.063e-04  Data: 0.010 (0.013)
Train: 110 [1050/1251 ( 84%)]  Loss: 3.882 (3.82)  Time: 0.795s, 1288.63/s  (0.790s, 1296.38/s)  LR: 7.063e-04  Data: 0.010 (0.013)
Train: 110 [1100/1251 ( 88%)]  Loss: 3.965 (3.82)  Time: 0.834s, 1227.41/s  (0.790s, 1296.55/s)  LR: 7.063e-04  Data: 0.011 (0.013)
Train: 110 [1150/1251 ( 92%)]  Loss: 3.633 (3.81)  Time: 0.778s, 1315.62/s  (0.790s, 1295.82/s)  LR: 7.063e-04  Data: 0.011 (0.012)
Train: 110 [1200/1251 ( 96%)]  Loss: 3.762 (3.81)  Time: 0.778s, 1316.37/s  (0.790s, 1296.04/s)  LR: 7.063e-04  Data: 0.011 (0.012)
Train: 110 [1250/1251 (100%)]  Loss: 4.000 (3.82)  Time: 0.784s, 1305.69/s  (0.790s, 1296.35/s)  LR: 7.063e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.592 (1.592)  Loss:  0.9461 (0.9461)  Acc@1: 85.5469 (85.5469)  Acc@5: 96.0938 (96.0938)
Test: [  48/48]  Time: 0.172 (0.555)  Loss:  1.1013 (1.5498)  Acc@1: 81.4859 (70.0220)  Acc@5: 94.6934 (89.8020)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-109.pth.tar', 71.0599999633789)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-102.pth.tar', 70.50400009521485)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-108.pth.tar', 70.30399991699218)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-92.pth.tar', 70.16800001220703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-105.pth.tar', 70.08800004394531)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-110.pth.tar', 70.02200010253907)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-94.pth.tar', 69.97399999511718)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-103.pth.tar', 69.91000002197265)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-107.pth.tar', 69.88199999267579)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-100.pth.tar', 69.83799999267578)

Train: 111 [   0/1251 (  0%)]  Loss: 3.796 (3.80)  Time: 2.443s,  419.15/s  (2.443s,  419.15/s)  LR: 7.016e-04  Data: 1.709 (1.709)
Train: 111 [  50/1251 (  4%)]  Loss: 3.839 (3.82)  Time: 0.779s, 1314.17/s  (0.828s, 1237.12/s)  LR: 7.016e-04  Data: 0.011 (0.047)
Train: 111 [ 100/1251 (  8%)]  Loss: 3.380 (3.67)  Time: 0.775s, 1321.26/s  (0.812s, 1261.64/s)  LR: 7.016e-04  Data: 0.010 (0.029)
Train: 111 [ 150/1251 ( 12%)]  Loss: 3.862 (3.72)  Time: 0.779s, 1313.93/s  (0.808s, 1267.44/s)  LR: 7.016e-04  Data: 0.011 (0.023)
Train: 111 [ 200/1251 ( 16%)]  Loss: 4.118 (3.80)  Time: 0.777s, 1317.40/s  (0.806s, 1271.25/s)  LR: 7.016e-04  Data: 0.011 (0.020)
Train: 111 [ 250/1251 ( 20%)]  Loss: 3.964 (3.83)  Time: 0.800s, 1280.39/s  (0.803s, 1275.17/s)  LR: 7.016e-04  Data: 0.011 (0.018)
Train: 111 [ 300/1251 ( 24%)]  Loss: 3.869 (3.83)  Time: 0.834s, 1228.50/s  (0.802s, 1276.88/s)  LR: 7.016e-04  Data: 0.011 (0.017)
Train: 111 [ 350/1251 ( 28%)]  Loss: 4.105 (3.87)  Time: 0.868s, 1179.87/s  (0.800s, 1279.72/s)  LR: 7.016e-04  Data: 0.011 (0.016)
Train: 111 [ 400/1251 ( 32%)]  Loss: 4.378 (3.92)  Time: 0.819s, 1249.62/s  (0.800s, 1280.71/s)  LR: 7.016e-04  Data: 0.011 (0.016)
Train: 111 [ 450/1251 ( 36%)]  Loss: 3.934 (3.92)  Time: 0.778s, 1316.52/s  (0.798s, 1283.31/s)  LR: 7.016e-04  Data: 0.010 (0.015)
Train: 111 [ 500/1251 ( 40%)]  Loss: 3.677 (3.90)  Time: 0.816s, 1255.38/s  (0.797s, 1284.91/s)  LR: 7.016e-04  Data: 0.011 (0.015)
Train: 111 [ 550/1251 ( 44%)]  Loss: 4.211 (3.93)  Time: 0.782s, 1308.96/s  (0.797s, 1285.53/s)  LR: 7.016e-04  Data: 0.011 (0.014)
Train: 111 [ 600/1251 ( 48%)]  Loss: 3.789 (3.92)  Time: 0.778s, 1317.01/s  (0.796s, 1285.71/s)  LR: 7.016e-04  Data: 0.011 (0.014)
Train: 111 [ 650/1251 ( 52%)]  Loss: 3.939 (3.92)  Time: 0.777s, 1317.52/s  (0.796s, 1286.09/s)  LR: 7.016e-04  Data: 0.011 (0.014)
Train: 111 [ 700/1251 ( 56%)]  Loss: 3.920 (3.92)  Time: 0.777s, 1317.65/s  (0.796s, 1286.04/s)  LR: 7.016e-04  Data: 0.011 (0.014)
Train: 111 [ 750/1251 ( 60%)]  Loss: 4.025 (3.93)  Time: 0.778s, 1315.52/s  (0.796s, 1286.47/s)  LR: 7.016e-04  Data: 0.012 (0.013)
Train: 111 [ 800/1251 ( 64%)]  Loss: 3.617 (3.91)  Time: 0.826s, 1239.77/s  (0.796s, 1286.56/s)  LR: 7.016e-04  Data: 0.011 (0.013)
Train: 111 [ 850/1251 ( 68%)]  Loss: 3.589 (3.89)  Time: 0.777s, 1317.85/s  (0.796s, 1285.82/s)  LR: 7.016e-04  Data: 0.012 (0.013)
Train: 111 [ 900/1251 ( 72%)]  Loss: 4.033 (3.90)  Time: 0.779s, 1314.58/s  (0.796s, 1286.16/s)  LR: 7.016e-04  Data: 0.011 (0.013)
Train: 111 [ 950/1251 ( 76%)]  Loss: 3.829 (3.89)  Time: 0.776s, 1319.64/s  (0.796s, 1286.55/s)  LR: 7.016e-04  Data: 0.011 (0.013)
Train: 111 [1000/1251 ( 80%)]  Loss: 4.056 (3.90)  Time: 0.811s, 1262.83/s  (0.796s, 1286.94/s)  LR: 7.016e-04  Data: 0.011 (0.013)
Train: 111 [1050/1251 ( 84%)]  Loss: 3.700 (3.89)  Time: 0.805s, 1272.80/s  (0.796s, 1286.86/s)  LR: 7.016e-04  Data: 0.011 (0.013)
Train: 111 [1100/1251 ( 88%)]  Loss: 3.905 (3.89)  Time: 0.781s, 1310.32/s  (0.796s, 1287.06/s)  LR: 7.016e-04  Data: 0.011 (0.013)
Train: 111 [1150/1251 ( 92%)]  Loss: 3.668 (3.88)  Time: 0.848s, 1207.41/s  (0.795s, 1287.33/s)  LR: 7.016e-04  Data: 0.012 (0.013)
Train: 111 [1200/1251 ( 96%)]  Loss: 3.860 (3.88)  Time: 0.776s, 1319.78/s  (0.795s, 1287.54/s)  LR: 7.016e-04  Data: 0.011 (0.013)
Train: 111 [1250/1251 (100%)]  Loss: 3.741 (3.88)  Time: 0.770s, 1330.22/s  (0.795s, 1287.40/s)  LR: 7.016e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.550 (1.550)  Loss:  1.0251 (1.0251)  Acc@1: 84.2773 (84.2773)  Acc@5: 95.8984 (95.8984)
Test: [  48/48]  Time: 0.172 (0.559)  Loss:  0.9667 (1.5387)  Acc@1: 83.3726 (70.3520)  Acc@5: 95.8727 (89.7520)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-109.pth.tar', 71.0599999633789)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-102.pth.tar', 70.50400009521485)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-111.pth.tar', 70.35200001708985)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-108.pth.tar', 70.30399991699218)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-92.pth.tar', 70.16800001220703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-105.pth.tar', 70.08800004394531)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-110.pth.tar', 70.02200010253907)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-94.pth.tar', 69.97399999511718)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-103.pth.tar', 69.91000002197265)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-107.pth.tar', 69.88199999267579)

Train: 112 [   0/1251 (  0%)]  Loss: 3.899 (3.90)  Time: 2.246s,  455.93/s  (2.246s,  455.93/s)  LR: 6.968e-04  Data: 1.510 (1.510)
Train: 112 [  50/1251 (  4%)]  Loss: 3.826 (3.86)  Time: 0.778s, 1315.36/s  (0.826s, 1239.22/s)  LR: 6.968e-04  Data: 0.011 (0.048)
Train: 112 [ 100/1251 (  8%)]  Loss: 3.858 (3.86)  Time: 0.780s, 1313.37/s  (0.808s, 1267.52/s)  LR: 6.968e-04  Data: 0.011 (0.030)
Train: 112 [ 150/1251 ( 12%)]  Loss: 3.806 (3.85)  Time: 0.779s, 1314.61/s  (0.803s, 1275.02/s)  LR: 6.968e-04  Data: 0.011 (0.023)
Train: 112 [ 200/1251 ( 16%)]  Loss: 3.765 (3.83)  Time: 0.800s, 1279.84/s  (0.801s, 1278.17/s)  LR: 6.968e-04  Data: 0.011 (0.020)
Train: 112 [ 250/1251 ( 20%)]  Loss: 4.142 (3.88)  Time: 0.778s, 1316.02/s  (0.802s, 1276.76/s)  LR: 6.968e-04  Data: 0.011 (0.019)
Train: 112 [ 300/1251 ( 24%)]  Loss: 3.260 (3.79)  Time: 0.778s, 1315.62/s  (0.799s, 1281.34/s)  LR: 6.968e-04  Data: 0.011 (0.017)
Train: 112 [ 350/1251 ( 28%)]  Loss: 3.822 (3.80)  Time: 0.778s, 1316.30/s  (0.799s, 1281.70/s)  LR: 6.968e-04  Data: 0.011 (0.016)
Train: 112 [ 400/1251 ( 32%)]  Loss: 3.902 (3.81)  Time: 0.777s, 1318.27/s  (0.798s, 1283.63/s)  LR: 6.968e-04  Data: 0.011 (0.016)
Train: 112 [ 450/1251 ( 36%)]  Loss: 4.011 (3.83)  Time: 0.811s, 1262.36/s  (0.798s, 1284.01/s)  LR: 6.968e-04  Data: 0.011 (0.015)
Train: 112 [ 500/1251 ( 40%)]  Loss: 4.362 (3.88)  Time: 0.778s, 1316.47/s  (0.798s, 1283.86/s)  LR: 6.968e-04  Data: 0.011 (0.015)
Train: 112 [ 550/1251 ( 44%)]  Loss: 3.782 (3.87)  Time: 0.776s, 1319.31/s  (0.798s, 1282.94/s)  LR: 6.968e-04  Data: 0.011 (0.014)
Train: 112 [ 600/1251 ( 48%)]  Loss: 3.889 (3.87)  Time: 0.783s, 1308.04/s  (0.797s, 1284.31/s)  LR: 6.968e-04  Data: 0.011 (0.014)
Train: 112 [ 650/1251 ( 52%)]  Loss: 3.799 (3.87)  Time: 0.842s, 1216.02/s  (0.799s, 1282.39/s)  LR: 6.968e-04  Data: 0.011 (0.014)
Train: 112 [ 700/1251 ( 56%)]  Loss: 4.068 (3.88)  Time: 0.778s, 1316.77/s  (0.798s, 1283.27/s)  LR: 6.968e-04  Data: 0.011 (0.014)
Train: 112 [ 750/1251 ( 60%)]  Loss: 3.827 (3.88)  Time: 0.808s, 1267.42/s  (0.797s, 1284.02/s)  LR: 6.968e-04  Data: 0.011 (0.014)
Train: 112 [ 800/1251 ( 64%)]  Loss: 3.838 (3.87)  Time: 0.776s, 1319.93/s  (0.797s, 1284.99/s)  LR: 6.968e-04  Data: 0.011 (0.013)
Train: 112 [ 850/1251 ( 68%)]  Loss: 4.063 (3.88)  Time: 0.784s, 1306.87/s  (0.797s, 1285.53/s)  LR: 6.968e-04  Data: 0.014 (0.013)
Train: 112 [ 900/1251 ( 72%)]  Loss: 4.065 (3.89)  Time: 0.842s, 1216.77/s  (0.796s, 1285.81/s)  LR: 6.968e-04  Data: 0.013 (0.013)
Train: 112 [ 950/1251 ( 76%)]  Loss: 3.601 (3.88)  Time: 0.776s, 1319.94/s  (0.797s, 1285.25/s)  LR: 6.968e-04  Data: 0.011 (0.013)
Train: 112 [1000/1251 ( 80%)]  Loss: 3.976 (3.88)  Time: 0.837s, 1223.64/s  (0.797s, 1285.00/s)  LR: 6.968e-04  Data: 0.011 (0.013)
Train: 112 [1050/1251 ( 84%)]  Loss: 3.768 (3.88)  Time: 0.779s, 1314.65/s  (0.796s, 1285.72/s)  LR: 6.968e-04  Data: 0.011 (0.013)
Train: 112 [1100/1251 ( 88%)]  Loss: 3.919 (3.88)  Time: 0.805s, 1272.13/s  (0.797s, 1285.44/s)  LR: 6.968e-04  Data: 0.011 (0.013)
Train: 112 [1150/1251 ( 92%)]  Loss: 3.924 (3.88)  Time: 0.778s, 1316.62/s  (0.797s, 1285.27/s)  LR: 6.968e-04  Data: 0.011 (0.013)
Train: 112 [1200/1251 ( 96%)]  Loss: 4.050 (3.89)  Time: 0.788s, 1299.37/s  (0.797s, 1285.58/s)  LR: 6.968e-04  Data: 0.011 (0.013)
Train: 112 [1250/1251 (100%)]  Loss: 3.670 (3.88)  Time: 0.768s, 1332.68/s  (0.796s, 1285.88/s)  LR: 6.968e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.511 (1.511)  Loss:  0.8624 (0.8624)  Acc@1: 85.6445 (85.6445)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.172 (0.564)  Loss:  0.9681 (1.5211)  Acc@1: 82.5472 (70.1860)  Acc@5: 95.7547 (90.1120)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-109.pth.tar', 71.0599999633789)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-102.pth.tar', 70.50400009521485)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-111.pth.tar', 70.35200001708985)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-108.pth.tar', 70.30399991699218)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-112.pth.tar', 70.18600004638672)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-92.pth.tar', 70.16800001220703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-105.pth.tar', 70.08800004394531)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-110.pth.tar', 70.02200010253907)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-94.pth.tar', 69.97399999511718)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-103.pth.tar', 69.91000002197265)

Train: 113 [   0/1251 (  0%)]  Loss: 3.631 (3.63)  Time: 2.378s,  430.60/s  (2.378s,  430.60/s)  LR: 6.920e-04  Data: 1.643 (1.643)
Train: 113 [  50/1251 (  4%)]  Loss: 4.034 (3.83)  Time: 0.780s, 1312.08/s  (0.836s, 1224.79/s)  LR: 6.920e-04  Data: 0.010 (0.046)
Train: 113 [ 100/1251 (  8%)]  Loss: 3.479 (3.71)  Time: 0.777s, 1317.35/s  (0.811s, 1262.62/s)  LR: 6.920e-04  Data: 0.011 (0.029)
Train: 113 [ 150/1251 ( 12%)]  Loss: 3.966 (3.78)  Time: 0.777s, 1317.29/s  (0.807s, 1268.59/s)  LR: 6.920e-04  Data: 0.011 (0.023)
Train: 113 [ 200/1251 ( 16%)]  Loss: 3.639 (3.75)  Time: 0.805s, 1272.59/s  (0.804s, 1273.29/s)  LR: 6.920e-04  Data: 0.011 (0.020)
Train: 113 [ 250/1251 ( 20%)]  Loss: 4.063 (3.80)  Time: 0.795s, 1287.77/s  (0.807s, 1268.74/s)  LR: 6.920e-04  Data: 0.013 (0.018)
Train: 113 [ 300/1251 ( 24%)]  Loss: 3.707 (3.79)  Time: 0.778s, 1315.74/s  (0.808s, 1267.09/s)  LR: 6.920e-04  Data: 0.011 (0.017)
Train: 113 [ 350/1251 ( 28%)]  Loss: 3.890 (3.80)  Time: 0.775s, 1321.90/s  (0.807s, 1268.23/s)  LR: 6.920e-04  Data: 0.011 (0.016)
Train: 113 [ 400/1251 ( 32%)]  Loss: 3.910 (3.81)  Time: 0.777s, 1317.08/s  (0.807s, 1269.68/s)  LR: 6.920e-04  Data: 0.011 (0.015)
Train: 113 [ 450/1251 ( 36%)]  Loss: 3.734 (3.81)  Time: 0.777s, 1318.32/s  (0.805s, 1272.58/s)  LR: 6.920e-04  Data: 0.011 (0.015)
Train: 113 [ 500/1251 ( 40%)]  Loss: 3.395 (3.77)  Time: 0.813s, 1259.44/s  (0.803s, 1275.19/s)  LR: 6.920e-04  Data: 0.011 (0.015)
Train: 113 [ 550/1251 ( 44%)]  Loss: 3.790 (3.77)  Time: 0.814s, 1258.51/s  (0.802s, 1276.62/s)  LR: 6.920e-04  Data: 0.011 (0.014)
Train: 113 [ 600/1251 ( 48%)]  Loss: 3.997 (3.79)  Time: 0.778s, 1315.74/s  (0.802s, 1276.64/s)  LR: 6.920e-04  Data: 0.011 (0.014)
Train: 113 [ 650/1251 ( 52%)]  Loss: 4.161 (3.81)  Time: 0.820s, 1248.41/s  (0.801s, 1278.30/s)  LR: 6.920e-04  Data: 0.012 (0.014)
Train: 113 [ 700/1251 ( 56%)]  Loss: 3.653 (3.80)  Time: 0.780s, 1312.67/s  (0.801s, 1278.69/s)  LR: 6.920e-04  Data: 0.011 (0.013)
Train: 113 [ 750/1251 ( 60%)]  Loss: 3.399 (3.78)  Time: 0.776s, 1318.98/s  (0.800s, 1279.46/s)  LR: 6.920e-04  Data: 0.011 (0.013)
Train: 113 [ 800/1251 ( 64%)]  Loss: 3.650 (3.77)  Time: 0.792s, 1293.53/s  (0.800s, 1279.48/s)  LR: 6.920e-04  Data: 0.011 (0.013)
Train: 113 [ 850/1251 ( 68%)]  Loss: 3.615 (3.76)  Time: 0.814s, 1257.80/s  (0.800s, 1279.75/s)  LR: 6.920e-04  Data: 0.011 (0.013)
Train: 113 [ 900/1251 ( 72%)]  Loss: 4.013 (3.78)  Time: 0.815s, 1256.61/s  (0.801s, 1279.14/s)  LR: 6.920e-04  Data: 0.011 (0.013)
Train: 113 [ 950/1251 ( 76%)]  Loss: 3.723 (3.77)  Time: 0.851s, 1203.48/s  (0.800s, 1279.27/s)  LR: 6.920e-04  Data: 0.011 (0.013)
Train: 113 [1000/1251 ( 80%)]  Loss: 3.772 (3.77)  Time: 0.775s, 1321.58/s  (0.800s, 1280.11/s)  LR: 6.920e-04  Data: 0.011 (0.013)
Train: 113 [1050/1251 ( 84%)]  Loss: 3.735 (3.77)  Time: 0.778s, 1316.21/s  (0.800s, 1280.44/s)  LR: 6.920e-04  Data: 0.010 (0.013)
Train: 113 [1100/1251 ( 88%)]  Loss: 3.873 (3.78)  Time: 0.779s, 1314.40/s  (0.800s, 1280.77/s)  LR: 6.920e-04  Data: 0.011 (0.013)
Train: 113 [1150/1251 ( 92%)]  Loss: 3.810 (3.78)  Time: 0.779s, 1314.34/s  (0.799s, 1281.49/s)  LR: 6.920e-04  Data: 0.011 (0.013)
Train: 113 [1200/1251 ( 96%)]  Loss: 4.342 (3.80)  Time: 0.789s, 1297.40/s  (0.799s, 1281.88/s)  LR: 6.920e-04  Data: 0.011 (0.012)
Train: 113 [1250/1251 (100%)]  Loss: 4.067 (3.81)  Time: 0.767s, 1335.86/s  (0.798s, 1282.69/s)  LR: 6.920e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.496 (1.496)  Loss:  0.9247 (0.9247)  Acc@1: 85.7422 (85.7422)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.172 (0.560)  Loss:  1.0086 (1.5939)  Acc@1: 81.8396 (70.3940)  Acc@5: 95.4009 (89.8260)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-109.pth.tar', 71.0599999633789)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-102.pth.tar', 70.50400009521485)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-113.pth.tar', 70.3939999975586)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-111.pth.tar', 70.35200001708985)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-108.pth.tar', 70.30399991699218)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-112.pth.tar', 70.18600004638672)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-92.pth.tar', 70.16800001220703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-105.pth.tar', 70.08800004394531)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-110.pth.tar', 70.02200010253907)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-94.pth.tar', 69.97399999511718)

Train: 114 [   0/1251 (  0%)]  Loss: 4.009 (4.01)  Time: 2.293s,  446.61/s  (2.293s,  446.61/s)  LR: 6.872e-04  Data: 1.500 (1.500)
Train: 114 [  50/1251 (  4%)]  Loss: 3.591 (3.80)  Time: 0.776s, 1319.27/s  (0.824s, 1241.97/s)  LR: 6.872e-04  Data: 0.010 (0.043)
Train: 114 [ 100/1251 (  8%)]  Loss: 3.584 (3.73)  Time: 0.779s, 1314.58/s  (0.809s, 1266.12/s)  LR: 6.872e-04  Data: 0.011 (0.027)
Train: 114 [ 150/1251 ( 12%)]  Loss: 3.789 (3.74)  Time: 0.779s, 1314.75/s  (0.806s, 1269.69/s)  LR: 6.872e-04  Data: 0.011 (0.022)
Train: 114 [ 200/1251 ( 16%)]  Loss: 3.734 (3.74)  Time: 0.780s, 1312.96/s  (0.805s, 1271.31/s)  LR: 6.872e-04  Data: 0.011 (0.019)
Train: 114 [ 250/1251 ( 20%)]  Loss: 4.097 (3.80)  Time: 0.797s, 1285.13/s  (0.803s, 1275.89/s)  LR: 6.872e-04  Data: 0.011 (0.018)
Train: 114 [ 300/1251 ( 24%)]  Loss: 3.438 (3.75)  Time: 0.779s, 1315.24/s  (0.801s, 1277.66/s)  LR: 6.872e-04  Data: 0.011 (0.016)
Train: 114 [ 350/1251 ( 28%)]  Loss: 3.596 (3.73)  Time: 0.779s, 1313.82/s  (0.801s, 1279.17/s)  LR: 6.872e-04  Data: 0.011 (0.016)
Train: 114 [ 400/1251 ( 32%)]  Loss: 3.901 (3.75)  Time: 0.780s, 1313.60/s  (0.800s, 1280.19/s)  LR: 6.872e-04  Data: 0.011 (0.015)
Train: 114 [ 450/1251 ( 36%)]  Loss: 3.931 (3.77)  Time: 0.832s, 1231.40/s  (0.799s, 1281.82/s)  LR: 6.872e-04  Data: 0.011 (0.015)
Train: 114 [ 500/1251 ( 40%)]  Loss: 4.029 (3.79)  Time: 0.777s, 1317.76/s  (0.800s, 1280.55/s)  LR: 6.872e-04  Data: 0.012 (0.014)
Train: 114 [ 550/1251 ( 44%)]  Loss: 3.771 (3.79)  Time: 0.778s, 1316.20/s  (0.800s, 1279.59/s)  LR: 6.872e-04  Data: 0.011 (0.014)
Train: 114 [ 600/1251 ( 48%)]  Loss: 3.869 (3.80)  Time: 0.835s, 1225.65/s  (0.800s, 1280.13/s)  LR: 6.872e-04  Data: 0.010 (0.014)
Train: 114 [ 650/1251 ( 52%)]  Loss: 3.463 (3.77)  Time: 0.832s, 1230.42/s  (0.802s, 1277.41/s)  LR: 6.872e-04  Data: 0.011 (0.014)
Train: 114 [ 700/1251 ( 56%)]  Loss: 3.917 (3.78)  Time: 0.814s, 1258.70/s  (0.801s, 1278.33/s)  LR: 6.872e-04  Data: 0.011 (0.013)
Train: 114 [ 750/1251 ( 60%)]  Loss: 3.982 (3.79)  Time: 0.779s, 1313.67/s  (0.800s, 1279.22/s)  LR: 6.872e-04  Data: 0.011 (0.013)
Train: 114 [ 800/1251 ( 64%)]  Loss: 3.881 (3.80)  Time: 0.819s, 1250.52/s  (0.800s, 1279.90/s)  LR: 6.872e-04  Data: 0.015 (0.013)
Train: 114 [ 850/1251 ( 68%)]  Loss: 4.122 (3.82)  Time: 0.778s, 1317.00/s  (0.801s, 1278.87/s)  LR: 6.872e-04  Data: 0.011 (0.013)
Train: 114 [ 900/1251 ( 72%)]  Loss: 4.093 (3.83)  Time: 0.779s, 1314.18/s  (0.800s, 1280.72/s)  LR: 6.872e-04  Data: 0.011 (0.013)
Train: 114 [ 950/1251 ( 76%)]  Loss: 3.822 (3.83)  Time: 0.776s, 1319.17/s  (0.798s, 1282.46/s)  LR: 6.872e-04  Data: 0.011 (0.013)
Train: 114 [1000/1251 ( 80%)]  Loss: 3.876 (3.83)  Time: 0.778s, 1315.51/s  (0.797s, 1284.06/s)  LR: 6.872e-04  Data: 0.011 (0.013)
Train: 114 [1050/1251 ( 84%)]  Loss: 4.018 (3.84)  Time: 0.780s, 1312.79/s  (0.797s, 1285.54/s)  LR: 6.872e-04  Data: 0.011 (0.013)
Train: 114 [1100/1251 ( 88%)]  Loss: 3.874 (3.84)  Time: 0.777s, 1317.61/s  (0.796s, 1286.85/s)  LR: 6.872e-04  Data: 0.011 (0.013)
Train: 114 [1150/1251 ( 92%)]  Loss: 3.382 (3.82)  Time: 0.777s, 1318.35/s  (0.795s, 1287.74/s)  LR: 6.872e-04  Data: 0.011 (0.012)
Train: 114 [1200/1251 ( 96%)]  Loss: 4.036 (3.83)  Time: 0.776s, 1318.94/s  (0.795s, 1288.83/s)  LR: 6.872e-04  Data: 0.011 (0.012)
Train: 114 [1250/1251 (100%)]  Loss: 3.691 (3.83)  Time: 0.764s, 1339.95/s  (0.794s, 1289.79/s)  LR: 6.872e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.527 (1.527)  Loss:  0.9108 (0.9108)  Acc@1: 87.0117 (87.0117)  Acc@5: 96.1914 (96.1914)
Test: [  48/48]  Time: 0.171 (0.554)  Loss:  0.9956 (1.5475)  Acc@1: 84.1981 (69.8700)  Acc@5: 95.7547 (89.3900)
Train: 115 [   0/1251 (  0%)]  Loss: 3.809 (3.81)  Time: 2.183s,  469.05/s  (2.183s,  469.05/s)  LR: 6.824e-04  Data: 1.448 (1.448)
Train: 115 [  50/1251 (  4%)]  Loss: 3.705 (3.76)  Time: 0.777s, 1318.06/s  (0.824s, 1242.05/s)  LR: 6.824e-04  Data: 0.011 (0.044)
Train: 115 [ 100/1251 (  8%)]  Loss: 3.659 (3.72)  Time: 0.776s, 1319.84/s  (0.802s, 1277.48/s)  LR: 6.824e-04  Data: 0.011 (0.028)
Train: 115 [ 150/1251 ( 12%)]  Loss: 3.784 (3.74)  Time: 0.778s, 1316.89/s  (0.794s, 1289.64/s)  LR: 6.824e-04  Data: 0.011 (0.022)
Train: 115 [ 200/1251 ( 16%)]  Loss: 4.039 (3.80)  Time: 0.778s, 1315.91/s  (0.790s, 1296.37/s)  LR: 6.824e-04  Data: 0.011 (0.019)
Train: 115 [ 250/1251 ( 20%)]  Loss: 3.794 (3.80)  Time: 0.776s, 1319.93/s  (0.788s, 1300.15/s)  LR: 6.824e-04  Data: 0.011 (0.018)
Train: 115 [ 300/1251 ( 24%)]  Loss: 3.611 (3.77)  Time: 0.778s, 1316.50/s  (0.786s, 1302.60/s)  LR: 6.824e-04  Data: 0.011 (0.017)
Train: 115 [ 350/1251 ( 28%)]  Loss: 3.876 (3.78)  Time: 0.777s, 1317.32/s  (0.785s, 1303.88/s)  LR: 6.824e-04  Data: 0.011 (0.016)
Train: 115 [ 400/1251 ( 32%)]  Loss: 4.010 (3.81)  Time: 0.778s, 1316.90/s  (0.785s, 1305.22/s)  LR: 6.824e-04  Data: 0.011 (0.015)
Train: 115 [ 450/1251 ( 36%)]  Loss: 3.823 (3.81)  Time: 0.778s, 1315.42/s  (0.784s, 1305.95/s)  LR: 6.824e-04  Data: 0.011 (0.015)
Train: 115 [ 500/1251 ( 40%)]  Loss: 3.721 (3.80)  Time: 0.777s, 1317.15/s  (0.784s, 1306.95/s)  LR: 6.824e-04  Data: 0.011 (0.014)
Train: 115 [ 550/1251 ( 44%)]  Loss: 3.650 (3.79)  Time: 0.781s, 1311.23/s  (0.783s, 1307.79/s)  LR: 6.824e-04  Data: 0.011 (0.014)
Train: 115 [ 600/1251 ( 48%)]  Loss: 3.816 (3.79)  Time: 0.777s, 1317.28/s  (0.783s, 1308.47/s)  LR: 6.824e-04  Data: 0.011 (0.014)
Train: 115 [ 650/1251 ( 52%)]  Loss: 3.859 (3.80)  Time: 0.777s, 1317.81/s  (0.782s, 1309.11/s)  LR: 6.824e-04  Data: 0.011 (0.014)
Train: 115 [ 700/1251 ( 56%)]  Loss: 3.924 (3.81)  Time: 0.776s, 1319.35/s  (0.782s, 1309.30/s)  LR: 6.824e-04  Data: 0.011 (0.013)
Train: 115 [ 750/1251 ( 60%)]  Loss: 3.534 (3.79)  Time: 0.775s, 1321.38/s  (0.782s, 1308.73/s)  LR: 6.824e-04  Data: 0.011 (0.013)
Train: 115 [ 800/1251 ( 64%)]  Loss: 4.070 (3.80)  Time: 0.778s, 1315.85/s  (0.782s, 1309.07/s)  LR: 6.824e-04  Data: 0.011 (0.013)
Train: 115 [ 850/1251 ( 68%)]  Loss: 3.809 (3.81)  Time: 0.777s, 1317.68/s  (0.783s, 1308.47/s)  LR: 6.824e-04  Data: 0.011 (0.013)
Train: 115 [ 900/1251 ( 72%)]  Loss: 3.880 (3.81)  Time: 0.779s, 1313.95/s  (0.782s, 1308.96/s)  LR: 6.824e-04  Data: 0.011 (0.013)
Train: 115 [ 950/1251 ( 76%)]  Loss: 3.592 (3.80)  Time: 0.810s, 1263.72/s  (0.782s, 1309.15/s)  LR: 6.824e-04  Data: 0.011 (0.013)
Train: 115 [1000/1251 ( 80%)]  Loss: 3.882 (3.80)  Time: 0.776s, 1319.13/s  (0.783s, 1307.99/s)  LR: 6.824e-04  Data: 0.011 (0.013)
Train: 115 [1050/1251 ( 84%)]  Loss: 3.723 (3.80)  Time: 0.813s, 1259.14/s  (0.783s, 1307.58/s)  LR: 6.824e-04  Data: 0.011 (0.013)
Train: 115 [1100/1251 ( 88%)]  Loss: 3.638 (3.79)  Time: 0.810s, 1264.85/s  (0.784s, 1305.38/s)  LR: 6.824e-04  Data: 0.011 (0.013)
Train: 115 [1150/1251 ( 92%)]  Loss: 3.441 (3.78)  Time: 0.777s, 1317.26/s  (0.784s, 1305.50/s)  LR: 6.824e-04  Data: 0.011 (0.012)
Train: 115 [1200/1251 ( 96%)]  Loss: 3.907 (3.78)  Time: 0.779s, 1314.45/s  (0.784s, 1305.82/s)  LR: 6.824e-04  Data: 0.011 (0.012)
Train: 115 [1250/1251 (100%)]  Loss: 3.882 (3.79)  Time: 0.764s, 1340.04/s  (0.784s, 1306.29/s)  LR: 6.824e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.481 (1.481)  Loss:  0.8504 (0.8504)  Acc@1: 86.7188 (86.7188)  Acc@5: 95.8008 (95.8008)
Test: [  48/48]  Time: 0.172 (0.558)  Loss:  1.0248 (1.4840)  Acc@1: 82.1934 (70.9860)  Acc@5: 93.7500 (90.2460)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-109.pth.tar', 71.0599999633789)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-115.pth.tar', 70.98600002197266)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-102.pth.tar', 70.50400009521485)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-113.pth.tar', 70.3939999975586)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-111.pth.tar', 70.35200001708985)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-108.pth.tar', 70.30399991699218)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-112.pth.tar', 70.18600004638672)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-92.pth.tar', 70.16800001220703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-105.pth.tar', 70.08800004394531)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-110.pth.tar', 70.02200010253907)

Train: 116 [   0/1251 (  0%)]  Loss: 4.112 (4.11)  Time: 2.196s,  466.40/s  (2.196s,  466.40/s)  LR: 6.775e-04  Data: 1.461 (1.461)
Train: 116 [  50/1251 (  4%)]  Loss: 4.099 (4.11)  Time: 0.778s, 1317.01/s  (0.815s, 1255.92/s)  LR: 6.775e-04  Data: 0.011 (0.043)
Train: 116 [ 100/1251 (  8%)]  Loss: 3.504 (3.91)  Time: 0.778s, 1316.81/s  (0.797s, 1285.52/s)  LR: 6.775e-04  Data: 0.011 (0.027)
Train: 116 [ 150/1251 ( 12%)]  Loss: 3.844 (3.89)  Time: 0.810s, 1264.55/s  (0.793s, 1290.75/s)  LR: 6.775e-04  Data: 0.011 (0.022)
Train: 116 [ 200/1251 ( 16%)]  Loss: 3.791 (3.87)  Time: 0.777s, 1317.37/s  (0.797s, 1285.53/s)  LR: 6.775e-04  Data: 0.011 (0.019)
Train: 116 [ 250/1251 ( 20%)]  Loss: 3.919 (3.88)  Time: 0.779s, 1314.46/s  (0.793s, 1291.38/s)  LR: 6.775e-04  Data: 0.011 (0.017)
Train: 116 [ 300/1251 ( 24%)]  Loss: 3.786 (3.87)  Time: 0.777s, 1317.33/s  (0.791s, 1295.22/s)  LR: 6.775e-04  Data: 0.011 (0.016)
Train: 116 [ 350/1251 ( 28%)]  Loss: 3.981 (3.88)  Time: 0.779s, 1314.99/s  (0.789s, 1298.26/s)  LR: 6.775e-04  Data: 0.011 (0.016)
Train: 116 [ 400/1251 ( 32%)]  Loss: 3.977 (3.89)  Time: 0.777s, 1317.11/s  (0.788s, 1300.22/s)  LR: 6.775e-04  Data: 0.011 (0.015)
Train: 116 [ 450/1251 ( 36%)]  Loss: 3.713 (3.87)  Time: 0.813s, 1259.06/s  (0.789s, 1297.63/s)  LR: 6.775e-04  Data: 0.011 (0.015)
Train: 116 [ 500/1251 ( 40%)]  Loss: 4.023 (3.89)  Time: 0.777s, 1317.19/s  (0.789s, 1297.86/s)  LR: 6.775e-04  Data: 0.011 (0.014)
Train: 116 [ 550/1251 ( 44%)]  Loss: 3.511 (3.85)  Time: 0.814s, 1258.37/s  (0.789s, 1298.12/s)  LR: 6.775e-04  Data: 0.011 (0.014)
Train: 116 [ 600/1251 ( 48%)]  Loss: 3.884 (3.86)  Time: 0.815s, 1256.80/s  (0.791s, 1294.61/s)  LR: 6.775e-04  Data: 0.011 (0.014)
Train: 116 [ 650/1251 ( 52%)]  Loss: 3.576 (3.84)  Time: 0.814s, 1258.73/s  (0.793s, 1291.67/s)  LR: 6.775e-04  Data: 0.011 (0.014)
Train: 116 [ 700/1251 ( 56%)]  Loss: 3.912 (3.84)  Time: 0.777s, 1317.53/s  (0.793s, 1291.53/s)  LR: 6.775e-04  Data: 0.011 (0.013)
Train: 116 [ 750/1251 ( 60%)]  Loss: 4.026 (3.85)  Time: 0.776s, 1319.09/s  (0.792s, 1292.95/s)  LR: 6.775e-04  Data: 0.011 (0.013)
Train: 116 [ 800/1251 ( 64%)]  Loss: 4.081 (3.87)  Time: 0.817s, 1253.45/s  (0.792s, 1292.76/s)  LR: 6.775e-04  Data: 0.011 (0.013)
Train: 116 [ 850/1251 ( 68%)]  Loss: 3.968 (3.87)  Time: 0.778s, 1316.78/s  (0.792s, 1293.40/s)  LR: 6.775e-04  Data: 0.011 (0.013)
Train: 116 [ 900/1251 ( 72%)]  Loss: 3.597 (3.86)  Time: 0.778s, 1316.96/s  (0.791s, 1294.50/s)  LR: 6.775e-04  Data: 0.011 (0.013)
Train: 116 [ 950/1251 ( 76%)]  Loss: 3.744 (3.85)  Time: 0.777s, 1317.99/s  (0.790s, 1295.66/s)  LR: 6.775e-04  Data: 0.011 (0.013)
Train: 116 [1000/1251 ( 80%)]  Loss: 3.904 (3.85)  Time: 0.845s, 1211.45/s  (0.790s, 1295.52/s)  LR: 6.775e-04  Data: 0.011 (0.013)
Train: 116 [1050/1251 ( 84%)]  Loss: 3.242 (3.83)  Time: 0.775s, 1320.60/s  (0.791s, 1294.11/s)  LR: 6.775e-04  Data: 0.011 (0.013)
Train: 116 [1100/1251 ( 88%)]  Loss: 4.023 (3.84)  Time: 0.812s, 1260.54/s  (0.792s, 1293.66/s)  LR: 6.775e-04  Data: 0.011 (0.013)
Train: 116 [1150/1251 ( 92%)]  Loss: 4.066 (3.85)  Time: 0.778s, 1315.91/s  (0.791s, 1294.22/s)  LR: 6.775e-04  Data: 0.011 (0.012)
Train: 116 [1200/1251 ( 96%)]  Loss: 3.881 (3.85)  Time: 0.775s, 1320.62/s  (0.791s, 1295.08/s)  LR: 6.775e-04  Data: 0.011 (0.012)
Train: 116 [1250/1251 (100%)]  Loss: 3.454 (3.83)  Time: 0.800s, 1279.64/s  (0.790s, 1295.45/s)  LR: 6.775e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.486 (1.486)  Loss:  1.0076 (1.0076)  Acc@1: 85.7422 (85.7422)  Acc@5: 95.8008 (95.8008)
Test: [  48/48]  Time: 0.172 (0.552)  Loss:  1.0995 (1.5680)  Acc@1: 82.0755 (70.3220)  Acc@5: 94.6934 (89.8020)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-109.pth.tar', 71.0599999633789)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-115.pth.tar', 70.98600002197266)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-102.pth.tar', 70.50400009521485)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-113.pth.tar', 70.3939999975586)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-111.pth.tar', 70.35200001708985)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-116.pth.tar', 70.32199997070313)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-108.pth.tar', 70.30399991699218)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-112.pth.tar', 70.18600004638672)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-92.pth.tar', 70.16800001220703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-105.pth.tar', 70.08800004394531)

Train: 117 [   0/1251 (  0%)]  Loss: 4.075 (4.08)  Time: 2.263s,  452.51/s  (2.263s,  452.51/s)  LR: 6.727e-04  Data: 1.527 (1.527)
Train: 117 [  50/1251 (  4%)]  Loss: 3.801 (3.94)  Time: 0.827s, 1238.08/s  (0.811s, 1263.25/s)  LR: 6.727e-04  Data: 0.011 (0.043)
Train: 117 [ 100/1251 (  8%)]  Loss: 3.875 (3.92)  Time: 0.812s, 1261.64/s  (0.802s, 1277.47/s)  LR: 6.727e-04  Data: 0.011 (0.027)
Train: 117 [ 150/1251 ( 12%)]  Loss: 3.974 (3.93)  Time: 0.779s, 1315.29/s  (0.795s, 1288.00/s)  LR: 6.727e-04  Data: 0.011 (0.022)
Train: 117 [ 200/1251 ( 16%)]  Loss: 3.803 (3.91)  Time: 0.778s, 1316.65/s  (0.791s, 1295.13/s)  LR: 6.727e-04  Data: 0.011 (0.019)
Train: 117 [ 250/1251 ( 20%)]  Loss: 4.096 (3.94)  Time: 0.778s, 1316.83/s  (0.788s, 1299.13/s)  LR: 6.727e-04  Data: 0.011 (0.017)
Train: 117 [ 300/1251 ( 24%)]  Loss: 3.807 (3.92)  Time: 0.777s, 1317.19/s  (0.787s, 1301.65/s)  LR: 6.727e-04  Data: 0.011 (0.016)
Train: 117 [ 350/1251 ( 28%)]  Loss: 3.877 (3.91)  Time: 0.777s, 1318.44/s  (0.786s, 1303.18/s)  LR: 6.727e-04  Data: 0.011 (0.016)
Train: 117 [ 400/1251 ( 32%)]  Loss: 4.151 (3.94)  Time: 0.776s, 1320.39/s  (0.785s, 1304.67/s)  LR: 6.727e-04  Data: 0.011 (0.015)
Train: 117 [ 450/1251 ( 36%)]  Loss: 3.400 (3.89)  Time: 0.820s, 1248.90/s  (0.785s, 1304.47/s)  LR: 6.727e-04  Data: 0.011 (0.015)
Train: 117 [ 500/1251 ( 40%)]  Loss: 3.881 (3.89)  Time: 0.778s, 1316.77/s  (0.788s, 1299.69/s)  LR: 6.727e-04  Data: 0.011 (0.014)
Train: 117 [ 550/1251 ( 44%)]  Loss: 3.818 (3.88)  Time: 0.780s, 1312.35/s  (0.787s, 1301.23/s)  LR: 6.727e-04  Data: 0.011 (0.014)
Train: 117 [ 600/1251 ( 48%)]  Loss: 3.785 (3.87)  Time: 0.780s, 1313.57/s  (0.787s, 1301.85/s)  LR: 6.727e-04  Data: 0.011 (0.014)
Train: 117 [ 650/1251 ( 52%)]  Loss: 3.861 (3.87)  Time: 0.851s, 1203.08/s  (0.786s, 1302.80/s)  LR: 6.727e-04  Data: 0.011 (0.014)
Train: 117 [ 700/1251 ( 56%)]  Loss: 3.865 (3.87)  Time: 0.822s, 1245.29/s  (0.786s, 1302.35/s)  LR: 6.727e-04  Data: 0.011 (0.013)
Train: 117 [ 750/1251 ( 60%)]  Loss: 3.539 (3.85)  Time: 0.777s, 1317.06/s  (0.787s, 1300.97/s)  LR: 6.727e-04  Data: 0.011 (0.013)
Train: 117 [ 800/1251 ( 64%)]  Loss: 3.393 (3.82)  Time: 0.777s, 1318.01/s  (0.787s, 1301.68/s)  LR: 6.727e-04  Data: 0.011 (0.013)
Train: 117 [ 850/1251 ( 68%)]  Loss: 4.105 (3.84)  Time: 0.777s, 1317.17/s  (0.786s, 1302.59/s)  LR: 6.727e-04  Data: 0.011 (0.013)
Train: 117 [ 900/1251 ( 72%)]  Loss: 3.357 (3.81)  Time: 0.778s, 1316.74/s  (0.786s, 1303.38/s)  LR: 6.727e-04  Data: 0.011 (0.013)
Train: 117 [ 950/1251 ( 76%)]  Loss: 3.607 (3.80)  Time: 0.779s, 1315.26/s  (0.785s, 1304.00/s)  LR: 6.727e-04  Data: 0.011 (0.013)
Train: 117 [1000/1251 ( 80%)]  Loss: 3.758 (3.80)  Time: 0.813s, 1260.18/s  (0.786s, 1302.46/s)  LR: 6.727e-04  Data: 0.011 (0.013)
Train: 117 [1050/1251 ( 84%)]  Loss: 4.032 (3.81)  Time: 0.778s, 1316.51/s  (0.787s, 1301.92/s)  LR: 6.727e-04  Data: 0.011 (0.013)
Train: 117 [1100/1251 ( 88%)]  Loss: 3.616 (3.80)  Time: 0.777s, 1318.40/s  (0.786s, 1302.53/s)  LR: 6.727e-04  Data: 0.011 (0.013)
Train: 117 [1150/1251 ( 92%)]  Loss: 3.673 (3.80)  Time: 0.777s, 1318.20/s  (0.786s, 1302.94/s)  LR: 6.727e-04  Data: 0.011 (0.012)
Train: 117 [1200/1251 ( 96%)]  Loss: 4.019 (3.81)  Time: 0.785s, 1304.43/s  (0.786s, 1302.62/s)  LR: 6.727e-04  Data: 0.011 (0.012)
Train: 117 [1250/1251 (100%)]  Loss: 4.061 (3.82)  Time: 0.764s, 1339.62/s  (0.786s, 1302.64/s)  LR: 6.727e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.529 (1.529)  Loss:  0.8823 (0.8823)  Acc@1: 86.3281 (86.3281)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.171 (0.553)  Loss:  0.9732 (1.5014)  Acc@1: 83.4906 (70.6720)  Acc@5: 94.9293 (90.3120)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-109.pth.tar', 71.0599999633789)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-115.pth.tar', 70.98600002197266)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-117.pth.tar', 70.67199993896485)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-102.pth.tar', 70.50400009521485)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-113.pth.tar', 70.3939999975586)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-111.pth.tar', 70.35200001708985)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-116.pth.tar', 70.32199997070313)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-108.pth.tar', 70.30399991699218)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-112.pth.tar', 70.18600004638672)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-92.pth.tar', 70.16800001220703)

Train: 118 [   0/1251 (  0%)]  Loss: 3.301 (3.30)  Time: 2.199s,  465.73/s  (2.199s,  465.73/s)  LR: 6.678e-04  Data: 1.466 (1.466)
Train: 118 [  50/1251 (  4%)]  Loss: 4.061 (3.68)  Time: 0.780s, 1312.41/s  (0.823s, 1244.86/s)  LR: 6.678e-04  Data: 0.011 (0.050)
Train: 118 [ 100/1251 (  8%)]  Loss: 3.342 (3.57)  Time: 0.777s, 1318.21/s  (0.801s, 1278.81/s)  LR: 6.678e-04  Data: 0.011 (0.031)
Train: 118 [ 150/1251 ( 12%)]  Loss: 3.720 (3.61)  Time: 0.816s, 1254.70/s  (0.800s, 1279.54/s)  LR: 6.678e-04  Data: 0.011 (0.024)
Train: 118 [ 200/1251 ( 16%)]  Loss: 3.878 (3.66)  Time: 0.777s, 1317.40/s  (0.797s, 1285.61/s)  LR: 6.678e-04  Data: 0.011 (0.021)
Train: 118 [ 250/1251 ( 20%)]  Loss: 3.899 (3.70)  Time: 0.777s, 1317.25/s  (0.794s, 1289.90/s)  LR: 6.678e-04  Data: 0.011 (0.019)
Train: 118 [ 300/1251 ( 24%)]  Loss: 3.907 (3.73)  Time: 0.777s, 1318.21/s  (0.794s, 1289.42/s)  LR: 6.678e-04  Data: 0.011 (0.018)
Train: 118 [ 350/1251 ( 28%)]  Loss: 3.441 (3.69)  Time: 0.776s, 1319.75/s  (0.792s, 1293.21/s)  LR: 6.678e-04  Data: 0.011 (0.017)
Train: 118 [ 400/1251 ( 32%)]  Loss: 3.930 (3.72)  Time: 0.781s, 1311.82/s  (0.790s, 1295.83/s)  LR: 6.678e-04  Data: 0.011 (0.016)
Train: 118 [ 450/1251 ( 36%)]  Loss: 3.893 (3.74)  Time: 0.778s, 1315.42/s  (0.789s, 1297.93/s)  LR: 6.678e-04  Data: 0.011 (0.015)
Train: 118 [ 500/1251 ( 40%)]  Loss: 3.950 (3.76)  Time: 0.779s, 1314.68/s  (0.790s, 1296.86/s)  LR: 6.678e-04  Data: 0.011 (0.015)
Train: 118 [ 550/1251 ( 44%)]  Loss: 3.915 (3.77)  Time: 0.779s, 1314.19/s  (0.789s, 1298.50/s)  LR: 6.678e-04  Data: 0.011 (0.015)
Train: 118 [ 600/1251 ( 48%)]  Loss: 4.117 (3.80)  Time: 0.778s, 1316.32/s  (0.788s, 1299.87/s)  LR: 6.678e-04  Data: 0.011 (0.014)
Train: 118 [ 650/1251 ( 52%)]  Loss: 3.608 (3.78)  Time: 0.806s, 1271.10/s  (0.787s, 1300.92/s)  LR: 6.678e-04  Data: 0.011 (0.014)
Train: 118 [ 700/1251 ( 56%)]  Loss: 3.409 (3.76)  Time: 0.820s, 1249.31/s  (0.788s, 1299.49/s)  LR: 6.678e-04  Data: 0.012 (0.014)
Train: 118 [ 750/1251 ( 60%)]  Loss: 4.000 (3.77)  Time: 0.818s, 1251.51/s  (0.789s, 1298.38/s)  LR: 6.678e-04  Data: 0.011 (0.014)
Train: 118 [ 800/1251 ( 64%)]  Loss: 3.440 (3.75)  Time: 0.820s, 1249.10/s  (0.791s, 1295.16/s)  LR: 6.678e-04  Data: 0.012 (0.014)
Train: 118 [ 850/1251 ( 68%)]  Loss: 3.602 (3.75)  Time: 0.778s, 1316.59/s  (0.790s, 1296.19/s)  LR: 6.678e-04  Data: 0.011 (0.013)
Train: 118 [ 900/1251 ( 72%)]  Loss: 3.706 (3.74)  Time: 0.777s, 1318.69/s  (0.789s, 1297.14/s)  LR: 6.678e-04  Data: 0.011 (0.013)
Train: 118 [ 950/1251 ( 76%)]  Loss: 3.821 (3.75)  Time: 0.816s, 1255.55/s  (0.789s, 1297.80/s)  LR: 6.678e-04  Data: 0.011 (0.013)
Train: 118 [1000/1251 ( 80%)]  Loss: 3.950 (3.76)  Time: 0.778s, 1316.75/s  (0.789s, 1298.29/s)  LR: 6.678e-04  Data: 0.011 (0.013)
Train: 118 [1050/1251 ( 84%)]  Loss: 3.538 (3.75)  Time: 0.779s, 1315.15/s  (0.788s, 1299.16/s)  LR: 6.678e-04  Data: 0.011 (0.013)
Train: 118 [1100/1251 ( 88%)]  Loss: 3.956 (3.76)  Time: 0.779s, 1314.73/s  (0.788s, 1299.95/s)  LR: 6.678e-04  Data: 0.011 (0.013)
Train: 118 [1150/1251 ( 92%)]  Loss: 3.646 (3.75)  Time: 0.778s, 1315.54/s  (0.787s, 1300.53/s)  LR: 6.678e-04  Data: 0.011 (0.013)
Train: 118 [1200/1251 ( 96%)]  Loss: 4.067 (3.76)  Time: 0.775s, 1320.64/s  (0.787s, 1301.04/s)  LR: 6.678e-04  Data: 0.011 (0.013)
Train: 118 [1250/1251 (100%)]  Loss: 3.902 (3.77)  Time: 0.764s, 1340.87/s  (0.787s, 1301.57/s)  LR: 6.678e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.506 (1.506)  Loss:  1.0071 (1.0071)  Acc@1: 86.2305 (86.2305)  Acc@5: 95.9961 (95.9961)
Test: [  48/48]  Time: 0.172 (0.553)  Loss:  0.9830 (1.5457)  Acc@1: 83.8443 (70.5460)  Acc@5: 94.9292 (90.1400)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-109.pth.tar', 71.0599999633789)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-115.pth.tar', 70.98600002197266)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-117.pth.tar', 70.67199993896485)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-118.pth.tar', 70.5459999633789)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-102.pth.tar', 70.50400009521485)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-113.pth.tar', 70.3939999975586)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-111.pth.tar', 70.35200001708985)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-116.pth.tar', 70.32199997070313)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-108.pth.tar', 70.30399991699218)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-112.pth.tar', 70.18600004638672)

Train: 119 [   0/1251 (  0%)]  Loss: 3.894 (3.89)  Time: 2.145s,  477.42/s  (2.145s,  477.42/s)  LR: 6.629e-04  Data: 1.406 (1.406)
Train: 119 [  50/1251 (  4%)]  Loss: 3.982 (3.94)  Time: 0.811s, 1262.78/s  (0.837s, 1223.75/s)  LR: 6.629e-04  Data: 0.011 (0.042)
Train: 119 [ 100/1251 (  8%)]  Loss: 4.016 (3.96)  Time: 0.818s, 1252.33/s  (0.826s, 1239.27/s)  LR: 6.629e-04  Data: 0.011 (0.027)
Train: 119 [ 150/1251 ( 12%)]  Loss: 3.547 (3.86)  Time: 0.815s, 1257.03/s  (0.822s, 1245.29/s)  LR: 6.629e-04  Data: 0.011 (0.022)
Train: 119 [ 200/1251 ( 16%)]  Loss: 3.911 (3.87)  Time: 0.777s, 1318.05/s  (0.812s, 1260.54/s)  LR: 6.629e-04  Data: 0.011 (0.019)
Train: 119 [ 250/1251 ( 20%)]  Loss: 4.122 (3.91)  Time: 0.777s, 1317.22/s  (0.808s, 1267.76/s)  LR: 6.629e-04  Data: 0.011 (0.018)
Train: 119 [ 300/1251 ( 24%)]  Loss: 3.649 (3.87)  Time: 0.813s, 1260.31/s  (0.805s, 1272.21/s)  LR: 6.629e-04  Data: 0.011 (0.017)
Train: 119 [ 350/1251 ( 28%)]  Loss: 4.252 (3.92)  Time: 0.812s, 1260.89/s  (0.802s, 1276.72/s)  LR: 6.629e-04  Data: 0.011 (0.016)
Train: 119 [ 400/1251 ( 32%)]  Loss: 3.949 (3.92)  Time: 0.814s, 1257.22/s  (0.801s, 1278.81/s)  LR: 6.629e-04  Data: 0.011 (0.015)
Train: 119 [ 450/1251 ( 36%)]  Loss: 4.145 (3.95)  Time: 0.813s, 1259.76/s  (0.799s, 1281.35/s)  LR: 6.629e-04  Data: 0.011 (0.015)
Train: 119 [ 500/1251 ( 40%)]  Loss: 3.758 (3.93)  Time: 0.813s, 1260.13/s  (0.800s, 1279.43/s)  LR: 6.629e-04  Data: 0.011 (0.014)
Train: 119 [ 550/1251 ( 44%)]  Loss: 3.985 (3.93)  Time: 0.776s, 1320.12/s  (0.798s, 1282.77/s)  LR: 6.629e-04  Data: 0.011 (0.014)
Train: 119 [ 600/1251 ( 48%)]  Loss: 3.215 (3.88)  Time: 0.777s, 1317.54/s  (0.797s, 1285.45/s)  LR: 6.629e-04  Data: 0.011 (0.014)
Train: 119 [ 650/1251 ( 52%)]  Loss: 3.451 (3.85)  Time: 0.776s, 1319.27/s  (0.795s, 1287.59/s)  LR: 6.629e-04  Data: 0.011 (0.014)
Train: 119 [ 700/1251 ( 56%)]  Loss: 3.950 (3.86)  Time: 0.775s, 1320.72/s  (0.795s, 1288.74/s)  LR: 6.629e-04  Data: 0.011 (0.013)
Train: 119 [ 750/1251 ( 60%)]  Loss: 3.824 (3.85)  Time: 0.817s, 1253.60/s  (0.795s, 1288.84/s)  LR: 6.629e-04  Data: 0.011 (0.013)
Train: 119 [ 800/1251 ( 64%)]  Loss: 4.029 (3.86)  Time: 0.860s, 1190.64/s  (0.796s, 1286.95/s)  LR: 6.629e-04  Data: 0.011 (0.013)
Train: 119 [ 850/1251 ( 68%)]  Loss: 3.705 (3.85)  Time: 0.777s, 1318.38/s  (0.795s, 1288.39/s)  LR: 6.629e-04  Data: 0.011 (0.013)
Train: 119 [ 900/1251 ( 72%)]  Loss: 3.874 (3.86)  Time: 0.779s, 1315.10/s  (0.794s, 1289.93/s)  LR: 6.629e-04  Data: 0.011 (0.013)
Train: 119 [ 950/1251 ( 76%)]  Loss: 3.581 (3.84)  Time: 0.777s, 1317.22/s  (0.793s, 1291.25/s)  LR: 6.629e-04  Data: 0.011 (0.013)
Train: 119 [1000/1251 ( 80%)]  Loss: 4.059 (3.85)  Time: 0.817s, 1253.01/s  (0.794s, 1289.47/s)  LR: 6.629e-04  Data: 0.012 (0.013)
Train: 119 [1050/1251 ( 84%)]  Loss: 3.811 (3.85)  Time: 0.780s, 1313.03/s  (0.794s, 1288.91/s)  LR: 6.629e-04  Data: 0.011 (0.013)
Train: 119 [1100/1251 ( 88%)]  Loss: 4.074 (3.86)  Time: 0.778s, 1316.67/s  (0.794s, 1290.07/s)  LR: 6.629e-04  Data: 0.011 (0.013)
Train: 119 [1150/1251 ( 92%)]  Loss: 3.714 (3.85)  Time: 0.815s, 1256.32/s  (0.793s, 1290.86/s)  LR: 6.629e-04  Data: 0.011 (0.012)
Train: 119 [1200/1251 ( 96%)]  Loss: 3.646 (3.85)  Time: 0.777s, 1318.05/s  (0.793s, 1291.31/s)  LR: 6.629e-04  Data: 0.011 (0.012)
Train: 119 [1250/1251 (100%)]  Loss: 3.924 (3.85)  Time: 0.765s, 1338.85/s  (0.792s, 1292.34/s)  LR: 6.629e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.506 (1.506)  Loss:  0.9888 (0.9888)  Acc@1: 84.9609 (84.9609)  Acc@5: 95.4102 (95.4102)
Test: [  48/48]  Time: 0.171 (0.548)  Loss:  0.9822 (1.5594)  Acc@1: 82.6651 (70.3220)  Acc@5: 95.8726 (89.8400)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-109.pth.tar', 71.0599999633789)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-115.pth.tar', 70.98600002197266)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-117.pth.tar', 70.67199993896485)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-118.pth.tar', 70.5459999633789)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-102.pth.tar', 70.50400009521485)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-113.pth.tar', 70.3939999975586)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-111.pth.tar', 70.35200001708985)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-116.pth.tar', 70.32199997070313)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-119.pth.tar', 70.32199996826172)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-108.pth.tar', 70.30399991699218)

Train: 120 [   0/1251 (  0%)]  Loss: 3.902 (3.90)  Time: 2.411s,  424.71/s  (2.411s,  424.71/s)  LR: 6.580e-04  Data: 1.678 (1.678)
Train: 120 [  50/1251 (  4%)]  Loss: 3.880 (3.89)  Time: 0.780s, 1313.07/s  (0.813s, 1259.52/s)  LR: 6.580e-04  Data: 0.011 (0.044)
Train: 120 [ 100/1251 (  8%)]  Loss: 3.735 (3.84)  Time: 0.782s, 1309.38/s  (0.796s, 1286.08/s)  LR: 6.580e-04  Data: 0.011 (0.028)
Train: 120 [ 150/1251 ( 12%)]  Loss: 3.995 (3.88)  Time: 0.810s, 1263.92/s  (0.793s, 1291.43/s)  LR: 6.580e-04  Data: 0.012 (0.022)
Train: 120 [ 200/1251 ( 16%)]  Loss: 3.763 (3.86)  Time: 0.782s, 1310.27/s  (0.792s, 1293.37/s)  LR: 6.580e-04  Data: 0.011 (0.019)
Train: 120 [ 250/1251 ( 20%)]  Loss: 3.681 (3.83)  Time: 0.777s, 1318.37/s  (0.789s, 1297.73/s)  LR: 6.580e-04  Data: 0.011 (0.018)
Train: 120 [ 300/1251 ( 24%)]  Loss: 3.871 (3.83)  Time: 0.776s, 1319.48/s  (0.787s, 1300.58/s)  LR: 6.580e-04  Data: 0.011 (0.017)
Train: 120 [ 350/1251 ( 28%)]  Loss: 3.650 (3.81)  Time: 0.778s, 1315.81/s  (0.786s, 1302.90/s)  LR: 6.580e-04  Data: 0.011 (0.016)
Train: 120 [ 400/1251 ( 32%)]  Loss: 3.851 (3.81)  Time: 0.779s, 1315.27/s  (0.785s, 1304.62/s)  LR: 6.580e-04  Data: 0.011 (0.015)
Train: 120 [ 450/1251 ( 36%)]  Loss: 3.623 (3.80)  Time: 0.780s, 1313.19/s  (0.785s, 1304.52/s)  LR: 6.580e-04  Data: 0.011 (0.015)
Train: 120 [ 500/1251 ( 40%)]  Loss: 3.639 (3.78)  Time: 0.776s, 1319.27/s  (0.785s, 1304.21/s)  LR: 6.580e-04  Data: 0.011 (0.014)
Train: 120 [ 550/1251 ( 44%)]  Loss: 3.600 (3.77)  Time: 0.778s, 1316.88/s  (0.785s, 1305.23/s)  LR: 6.580e-04  Data: 0.011 (0.014)
Train: 120 [ 600/1251 ( 48%)]  Loss: 3.934 (3.78)  Time: 0.776s, 1319.28/s  (0.784s, 1306.16/s)  LR: 6.580e-04  Data: 0.011 (0.014)
Train: 120 [ 650/1251 ( 52%)]  Loss: 4.025 (3.80)  Time: 0.777s, 1318.14/s  (0.784s, 1306.73/s)  LR: 6.580e-04  Data: 0.011 (0.014)
Train: 120 [ 700/1251 ( 56%)]  Loss: 3.594 (3.78)  Time: 0.777s, 1318.62/s  (0.783s, 1307.46/s)  LR: 6.580e-04  Data: 0.011 (0.013)
Train: 120 [ 750/1251 ( 60%)]  Loss: 3.738 (3.78)  Time: 0.777s, 1318.67/s  (0.783s, 1308.09/s)  LR: 6.580e-04  Data: 0.011 (0.013)
Train: 120 [ 800/1251 ( 64%)]  Loss: 4.183 (3.80)  Time: 0.776s, 1319.00/s  (0.784s, 1306.66/s)  LR: 6.580e-04  Data: 0.011 (0.013)
Train: 120 [ 850/1251 ( 68%)]  Loss: 3.790 (3.80)  Time: 0.777s, 1318.19/s  (0.783s, 1307.03/s)  LR: 6.580e-04  Data: 0.011 (0.013)
Train: 120 [ 900/1251 ( 72%)]  Loss: 3.685 (3.80)  Time: 0.777s, 1317.40/s  (0.783s, 1307.41/s)  LR: 6.580e-04  Data: 0.011 (0.013)
Train: 120 [ 950/1251 ( 76%)]  Loss: 3.738 (3.79)  Time: 0.778s, 1316.18/s  (0.783s, 1307.17/s)  LR: 6.580e-04  Data: 0.011 (0.013)
Train: 120 [1000/1251 ( 80%)]  Loss: 3.766 (3.79)  Time: 0.777s, 1317.50/s  (0.783s, 1307.39/s)  LR: 6.580e-04  Data: 0.011 (0.013)
Train: 120 [1050/1251 ( 84%)]  Loss: 3.602 (3.78)  Time: 0.778s, 1316.19/s  (0.783s, 1307.63/s)  LR: 6.580e-04  Data: 0.011 (0.013)
Train: 120 [1100/1251 ( 88%)]  Loss: 3.436 (3.77)  Time: 0.778s, 1317.01/s  (0.783s, 1308.04/s)  LR: 6.580e-04  Data: 0.011 (0.013)
Train: 120 [1150/1251 ( 92%)]  Loss: 3.794 (3.77)  Time: 0.778s, 1316.75/s  (0.783s, 1308.36/s)  LR: 6.580e-04  Data: 0.011 (0.013)
Train: 120 [1200/1251 ( 96%)]  Loss: 3.729 (3.77)  Time: 0.778s, 1316.81/s  (0.783s, 1308.56/s)  LR: 6.580e-04  Data: 0.011 (0.012)
Train: 120 [1250/1251 (100%)]  Loss: 3.943 (3.77)  Time: 0.762s, 1343.23/s  (0.782s, 1308.74/s)  LR: 6.580e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.481 (1.481)  Loss:  0.8942 (0.8942)  Acc@1: 86.5234 (86.5234)  Acc@5: 96.1914 (96.1914)
Test: [  48/48]  Time: 0.172 (0.554)  Loss:  0.9779 (1.5240)  Acc@1: 82.5472 (70.9920)  Acc@5: 95.2830 (90.0600)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-109.pth.tar', 71.0599999633789)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-120.pth.tar', 70.99200004638672)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-115.pth.tar', 70.98600002197266)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-117.pth.tar', 70.67199993896485)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-118.pth.tar', 70.5459999633789)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-102.pth.tar', 70.50400009521485)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-113.pth.tar', 70.3939999975586)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-111.pth.tar', 70.35200001708985)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-116.pth.tar', 70.32199997070313)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-119.pth.tar', 70.32199996826172)

Train: 121 [   0/1251 (  0%)]  Loss: 4.043 (4.04)  Time: 2.379s,  430.37/s  (2.379s,  430.37/s)  LR: 6.530e-04  Data: 1.646 (1.646)
Train: 121 [  50/1251 (  4%)]  Loss: 3.826 (3.93)  Time: 0.777s, 1317.12/s  (0.818s, 1251.31/s)  LR: 6.530e-04  Data: 0.011 (0.053)
Train: 121 [ 100/1251 (  8%)]  Loss: 3.743 (3.87)  Time: 0.811s, 1262.67/s  (0.813s, 1259.07/s)  LR: 6.530e-04  Data: 0.011 (0.032)
Train: 121 [ 150/1251 ( 12%)]  Loss: 3.535 (3.79)  Time: 0.777s, 1317.43/s  (0.801s, 1277.64/s)  LR: 6.530e-04  Data: 0.011 (0.025)
Train: 121 [ 200/1251 ( 16%)]  Loss: 3.984 (3.83)  Time: 0.780s, 1312.98/s  (0.796s, 1287.14/s)  LR: 6.530e-04  Data: 0.011 (0.022)
Train: 121 [ 250/1251 ( 20%)]  Loss: 3.725 (3.81)  Time: 0.776s, 1319.32/s  (0.792s, 1292.69/s)  LR: 6.530e-04  Data: 0.011 (0.019)
Train: 121 [ 300/1251 ( 24%)]  Loss: 3.486 (3.76)  Time: 0.814s, 1258.12/s  (0.793s, 1290.90/s)  LR: 6.530e-04  Data: 0.011 (0.018)
Train: 121 [ 350/1251 ( 28%)]  Loss: 3.978 (3.79)  Time: 0.777s, 1317.67/s  (0.792s, 1292.50/s)  LR: 6.530e-04  Data: 0.011 (0.017)
Train: 121 [ 400/1251 ( 32%)]  Loss: 4.207 (3.84)  Time: 0.777s, 1318.71/s  (0.790s, 1295.50/s)  LR: 6.530e-04  Data: 0.011 (0.016)
Train: 121 [ 450/1251 ( 36%)]  Loss: 3.971 (3.85)  Time: 0.813s, 1259.09/s  (0.790s, 1296.38/s)  LR: 6.530e-04  Data: 0.011 (0.016)
Train: 121 [ 500/1251 ( 40%)]  Loss: 3.544 (3.82)  Time: 0.778s, 1316.63/s  (0.790s, 1296.54/s)  LR: 6.530e-04  Data: 0.011 (0.015)
Train: 121 [ 550/1251 ( 44%)]  Loss: 4.184 (3.85)  Time: 0.815s, 1256.68/s  (0.791s, 1294.30/s)  LR: 6.530e-04  Data: 0.012 (0.015)
Train: 121 [ 600/1251 ( 48%)]  Loss: 3.822 (3.85)  Time: 0.779s, 1315.32/s  (0.790s, 1295.66/s)  LR: 6.530e-04  Data: 0.011 (0.015)
Train: 121 [ 650/1251 ( 52%)]  Loss: 3.707 (3.84)  Time: 0.837s, 1223.48/s  (0.789s, 1297.08/s)  LR: 6.530e-04  Data: 0.011 (0.014)
Train: 121 [ 700/1251 ( 56%)]  Loss: 3.984 (3.85)  Time: 0.777s, 1318.10/s  (0.789s, 1298.11/s)  LR: 6.530e-04  Data: 0.011 (0.014)
Train: 121 [ 750/1251 ( 60%)]  Loss: 3.569 (3.83)  Time: 0.778s, 1316.82/s  (0.788s, 1299.33/s)  LR: 6.530e-04  Data: 0.011 (0.014)
Train: 121 [ 800/1251 ( 64%)]  Loss: 3.666 (3.82)  Time: 0.779s, 1314.53/s  (0.788s, 1300.28/s)  LR: 6.530e-04  Data: 0.011 (0.014)
Train: 121 [ 850/1251 ( 68%)]  Loss: 4.056 (3.84)  Time: 0.779s, 1315.33/s  (0.787s, 1301.04/s)  LR: 6.530e-04  Data: 0.011 (0.014)
Train: 121 [ 900/1251 ( 72%)]  Loss: 3.969 (3.84)  Time: 0.778s, 1315.60/s  (0.787s, 1301.88/s)  LR: 6.530e-04  Data: 0.011 (0.013)
Train: 121 [ 950/1251 ( 76%)]  Loss: 3.550 (3.83)  Time: 0.778s, 1315.88/s  (0.786s, 1302.56/s)  LR: 6.530e-04  Data: 0.011 (0.013)
Train: 121 [1000/1251 ( 80%)]  Loss: 3.998 (3.84)  Time: 0.778s, 1316.76/s  (0.786s, 1303.28/s)  LR: 6.530e-04  Data: 0.011 (0.013)
Train: 121 [1050/1251 ( 84%)]  Loss: 3.997 (3.84)  Time: 0.776s, 1318.80/s  (0.785s, 1303.63/s)  LR: 6.530e-04  Data: 0.011 (0.013)
Train: 121 [1100/1251 ( 88%)]  Loss: 3.805 (3.84)  Time: 0.780s, 1313.09/s  (0.785s, 1304.17/s)  LR: 6.530e-04  Data: 0.011 (0.013)
Train: 121 [1150/1251 ( 92%)]  Loss: 3.826 (3.84)  Time: 0.778s, 1315.92/s  (0.785s, 1304.62/s)  LR: 6.530e-04  Data: 0.011 (0.013)
Train: 121 [1200/1251 ( 96%)]  Loss: 3.406 (3.82)  Time: 0.821s, 1246.67/s  (0.786s, 1302.73/s)  LR: 6.530e-04  Data: 0.011 (0.013)
Train: 121 [1250/1251 (100%)]  Loss: 3.707 (3.82)  Time: 0.800s, 1279.44/s  (0.786s, 1302.11/s)  LR: 6.530e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.494 (1.494)  Loss:  1.0683 (1.0683)  Acc@1: 85.3516 (85.3516)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.171 (0.562)  Loss:  0.9049 (1.5457)  Acc@1: 83.6085 (70.6380)  Acc@5: 96.2264 (90.1740)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-109.pth.tar', 71.0599999633789)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-120.pth.tar', 70.99200004638672)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-115.pth.tar', 70.98600002197266)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-117.pth.tar', 70.67199993896485)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-121.pth.tar', 70.63799999023438)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-118.pth.tar', 70.5459999633789)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-102.pth.tar', 70.50400009521485)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-113.pth.tar', 70.3939999975586)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-111.pth.tar', 70.35200001708985)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-116.pth.tar', 70.32199997070313)

Train: 122 [   0/1251 (  0%)]  Loss: 3.738 (3.74)  Time: 2.185s,  468.62/s  (2.185s,  468.62/s)  LR: 6.481e-04  Data: 1.452 (1.452)
Train: 122 [  50/1251 (  4%)]  Loss: 3.727 (3.73)  Time: 0.778s, 1316.61/s  (0.813s, 1260.13/s)  LR: 6.481e-04  Data: 0.011 (0.046)
Train: 122 [ 100/1251 (  8%)]  Loss: 3.845 (3.77)  Time: 0.811s, 1262.85/s  (0.798s, 1282.93/s)  LR: 6.481e-04  Data: 0.011 (0.028)
Train: 122 [ 150/1251 ( 12%)]  Loss: 3.655 (3.74)  Time: 0.777s, 1318.29/s  (0.798s, 1283.90/s)  LR: 6.481e-04  Data: 0.011 (0.023)
Train: 122 [ 200/1251 ( 16%)]  Loss: 3.505 (3.69)  Time: 0.777s, 1317.50/s  (0.793s, 1291.68/s)  LR: 6.481e-04  Data: 0.011 (0.020)
Train: 122 [ 250/1251 ( 20%)]  Loss: 3.298 (3.63)  Time: 0.777s, 1317.89/s  (0.790s, 1296.28/s)  LR: 6.481e-04  Data: 0.011 (0.018)
Train: 122 [ 300/1251 ( 24%)]  Loss: 4.145 (3.70)  Time: 0.778s, 1316.64/s  (0.788s, 1299.40/s)  LR: 6.481e-04  Data: 0.011 (0.017)
Train: 122 [ 350/1251 ( 28%)]  Loss: 3.691 (3.70)  Time: 0.778s, 1315.63/s  (0.787s, 1301.87/s)  LR: 6.481e-04  Data: 0.011 (0.016)
Train: 122 [ 400/1251 ( 32%)]  Loss: 3.548 (3.68)  Time: 0.777s, 1317.60/s  (0.786s, 1303.46/s)  LR: 6.481e-04  Data: 0.011 (0.015)
Train: 122 [ 450/1251 ( 36%)]  Loss: 3.513 (3.67)  Time: 0.777s, 1318.05/s  (0.785s, 1304.94/s)  LR: 6.481e-04  Data: 0.011 (0.015)
Train: 122 [ 500/1251 ( 40%)]  Loss: 3.515 (3.65)  Time: 0.779s, 1314.98/s  (0.784s, 1305.76/s)  LR: 6.481e-04  Data: 0.011 (0.015)
Train: 122 [ 550/1251 ( 44%)]  Loss: 3.811 (3.67)  Time: 0.777s, 1317.71/s  (0.784s, 1306.48/s)  LR: 6.481e-04  Data: 0.011 (0.014)
Train: 122 [ 600/1251 ( 48%)]  Loss: 3.647 (3.66)  Time: 0.806s, 1270.71/s  (0.783s, 1307.12/s)  LR: 6.481e-04  Data: 0.011 (0.014)
Train: 122 [ 650/1251 ( 52%)]  Loss: 3.412 (3.65)  Time: 0.776s, 1318.98/s  (0.784s, 1306.89/s)  LR: 6.481e-04  Data: 0.011 (0.014)
Train: 122 [ 700/1251 ( 56%)]  Loss: 4.001 (3.67)  Time: 0.777s, 1317.66/s  (0.783s, 1307.57/s)  LR: 6.481e-04  Data: 0.011 (0.014)
Train: 122 [ 750/1251 ( 60%)]  Loss: 3.786 (3.68)  Time: 0.779s, 1315.30/s  (0.783s, 1308.15/s)  LR: 6.481e-04  Data: 0.011 (0.013)
Train: 122 [ 800/1251 ( 64%)]  Loss: 3.705 (3.68)  Time: 0.778s, 1315.68/s  (0.783s, 1308.61/s)  LR: 6.481e-04  Data: 0.011 (0.013)
Train: 122 [ 850/1251 ( 68%)]  Loss: 3.959 (3.69)  Time: 0.779s, 1315.15/s  (0.782s, 1308.89/s)  LR: 6.481e-04  Data: 0.011 (0.013)
Train: 122 [ 900/1251 ( 72%)]  Loss: 4.122 (3.72)  Time: 0.776s, 1319.53/s  (0.782s, 1309.16/s)  LR: 6.481e-04  Data: 0.011 (0.013)
Train: 122 [ 950/1251 ( 76%)]  Loss: 4.009 (3.73)  Time: 0.777s, 1318.57/s  (0.782s, 1309.45/s)  LR: 6.481e-04  Data: 0.011 (0.013)
Train: 122 [1000/1251 ( 80%)]  Loss: 3.885 (3.74)  Time: 0.778s, 1316.53/s  (0.782s, 1309.75/s)  LR: 6.481e-04  Data: 0.011 (0.013)
Train: 122 [1050/1251 ( 84%)]  Loss: 3.489 (3.73)  Time: 0.784s, 1306.80/s  (0.782s, 1310.04/s)  LR: 6.481e-04  Data: 0.011 (0.013)
Train: 122 [1100/1251 ( 88%)]  Loss: 3.886 (3.73)  Time: 0.780s, 1312.80/s  (0.781s, 1310.34/s)  LR: 6.481e-04  Data: 0.011 (0.013)
Train: 122 [1150/1251 ( 92%)]  Loss: 3.806 (3.74)  Time: 0.778s, 1315.89/s  (0.781s, 1310.54/s)  LR: 6.481e-04  Data: 0.011 (0.013)
Train: 122 [1200/1251 ( 96%)]  Loss: 3.697 (3.74)  Time: 0.779s, 1314.11/s  (0.782s, 1309.37/s)  LR: 6.481e-04  Data: 0.011 (0.013)
Train: 122 [1250/1251 (100%)]  Loss: 3.830 (3.74)  Time: 0.762s, 1343.49/s  (0.782s, 1309.55/s)  LR: 6.481e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.480 (1.480)  Loss:  1.0458 (1.0458)  Acc@1: 84.7656 (84.7656)  Acc@5: 95.4102 (95.4102)
Test: [  48/48]  Time: 0.172 (0.552)  Loss:  1.0276 (1.5179)  Acc@1: 82.0755 (70.8700)  Acc@5: 95.0472 (89.9780)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-109.pth.tar', 71.0599999633789)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-120.pth.tar', 70.99200004638672)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-115.pth.tar', 70.98600002197266)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-122.pth.tar', 70.86999997070312)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-117.pth.tar', 70.67199993896485)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-121.pth.tar', 70.63799999023438)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-118.pth.tar', 70.5459999633789)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-102.pth.tar', 70.50400009521485)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-113.pth.tar', 70.3939999975586)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-111.pth.tar', 70.35200001708985)

Train: 123 [   0/1251 (  0%)]  Loss: 3.854 (3.85)  Time: 2.274s,  450.35/s  (2.274s,  450.35/s)  LR: 6.431e-04  Data: 1.479 (1.479)
Train: 123 [  50/1251 (  4%)]  Loss: 3.719 (3.79)  Time: 0.776s, 1319.57/s  (0.811s, 1262.26/s)  LR: 6.431e-04  Data: 0.011 (0.043)
Train: 123 [ 100/1251 (  8%)]  Loss: 3.575 (3.72)  Time: 0.779s, 1314.37/s  (0.796s, 1286.75/s)  LR: 6.431e-04  Data: 0.011 (0.027)
Train: 123 [ 150/1251 ( 12%)]  Loss: 3.717 (3.72)  Time: 0.778s, 1315.36/s  (0.790s, 1296.50/s)  LR: 6.431e-04  Data: 0.011 (0.022)
Train: 123 [ 200/1251 ( 16%)]  Loss: 4.086 (3.79)  Time: 0.778s, 1317.03/s  (0.788s, 1299.30/s)  LR: 6.431e-04  Data: 0.011 (0.019)
Train: 123 [ 250/1251 ( 20%)]  Loss: 3.982 (3.82)  Time: 0.776s, 1320.34/s  (0.786s, 1302.42/s)  LR: 6.431e-04  Data: 0.011 (0.018)
Train: 123 [ 300/1251 ( 24%)]  Loss: 4.101 (3.86)  Time: 0.777s, 1317.22/s  (0.785s, 1304.41/s)  LR: 6.431e-04  Data: 0.011 (0.016)
Train: 123 [ 350/1251 ( 28%)]  Loss: 3.559 (3.82)  Time: 0.777s, 1318.65/s  (0.784s, 1305.51/s)  LR: 6.431e-04  Data: 0.011 (0.016)
Train: 123 [ 400/1251 ( 32%)]  Loss: 3.846 (3.83)  Time: 0.778s, 1316.16/s  (0.784s, 1306.88/s)  LR: 6.431e-04  Data: 0.011 (0.015)
Train: 123 [ 450/1251 ( 36%)]  Loss: 3.790 (3.82)  Time: 0.777s, 1317.35/s  (0.783s, 1307.81/s)  LR: 6.431e-04  Data: 0.012 (0.015)
Train: 123 [ 500/1251 ( 40%)]  Loss: 3.999 (3.84)  Time: 0.776s, 1320.16/s  (0.783s, 1308.43/s)  LR: 6.431e-04  Data: 0.011 (0.014)
Train: 123 [ 550/1251 ( 44%)]  Loss: 3.921 (3.85)  Time: 0.777s, 1317.90/s  (0.782s, 1309.16/s)  LR: 6.431e-04  Data: 0.011 (0.014)
Train: 123 [ 600/1251 ( 48%)]  Loss: 3.770 (3.84)  Time: 0.777s, 1317.62/s  (0.782s, 1309.52/s)  LR: 6.431e-04  Data: 0.011 (0.014)
Train: 123 [ 650/1251 ( 52%)]  Loss: 4.065 (3.86)  Time: 0.775s, 1321.43/s  (0.782s, 1309.82/s)  LR: 6.431e-04  Data: 0.011 (0.014)
Train: 123 [ 700/1251 ( 56%)]  Loss: 3.796 (3.85)  Time: 0.777s, 1317.49/s  (0.782s, 1310.10/s)  LR: 6.431e-04  Data: 0.011 (0.013)
Train: 123 [ 750/1251 ( 60%)]  Loss: 3.850 (3.85)  Time: 0.778s, 1315.59/s  (0.781s, 1310.44/s)  LR: 6.431e-04  Data: 0.011 (0.013)
Train: 123 [ 800/1251 ( 64%)]  Loss: 3.526 (3.83)  Time: 0.776s, 1318.84/s  (0.781s, 1310.73/s)  LR: 6.431e-04  Data: 0.011 (0.013)
Train: 123 [ 850/1251 ( 68%)]  Loss: 3.726 (3.83)  Time: 0.777s, 1317.65/s  (0.781s, 1310.90/s)  LR: 6.431e-04  Data: 0.011 (0.013)
Train: 123 [ 900/1251 ( 72%)]  Loss: 3.701 (3.82)  Time: 0.778s, 1315.99/s  (0.781s, 1311.24/s)  LR: 6.431e-04  Data: 0.011 (0.013)
Train: 123 [ 950/1251 ( 76%)]  Loss: 3.630 (3.81)  Time: 0.777s, 1317.62/s  (0.781s, 1310.69/s)  LR: 6.431e-04  Data: 0.012 (0.013)
Train: 123 [1000/1251 ( 80%)]  Loss: 3.429 (3.79)  Time: 0.777s, 1318.67/s  (0.781s, 1311.00/s)  LR: 6.431e-04  Data: 0.011 (0.013)
Train: 123 [1050/1251 ( 84%)]  Loss: 3.860 (3.80)  Time: 0.776s, 1319.07/s  (0.781s, 1311.02/s)  LR: 6.431e-04  Data: 0.011 (0.013)
Train: 123 [1100/1251 ( 88%)]  Loss: 3.879 (3.80)  Time: 0.777s, 1318.30/s  (0.781s, 1311.11/s)  LR: 6.431e-04  Data: 0.011 (0.013)
Train: 123 [1150/1251 ( 92%)]  Loss: 3.995 (3.81)  Time: 0.778s, 1317.02/s  (0.781s, 1311.28/s)  LR: 6.431e-04  Data: 0.011 (0.012)
Train: 123 [1200/1251 ( 96%)]  Loss: 3.362 (3.79)  Time: 0.778s, 1315.85/s  (0.781s, 1311.30/s)  LR: 6.431e-04  Data: 0.011 (0.012)
Train: 123 [1250/1251 (100%)]  Loss: 4.142 (3.80)  Time: 0.765s, 1338.23/s  (0.781s, 1311.56/s)  LR: 6.431e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.483 (1.483)  Loss:  0.8433 (0.8433)  Acc@1: 86.4258 (86.4258)  Acc@5: 96.0938 (96.0938)
Test: [  48/48]  Time: 0.172 (0.553)  Loss:  0.9680 (1.4972)  Acc@1: 83.4906 (71.1540)  Acc@5: 95.6368 (90.2860)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-123.pth.tar', 71.15400006835938)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-109.pth.tar', 71.0599999633789)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-120.pth.tar', 70.99200004638672)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-115.pth.tar', 70.98600002197266)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-122.pth.tar', 70.86999997070312)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-117.pth.tar', 70.67199993896485)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-121.pth.tar', 70.63799999023438)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-118.pth.tar', 70.5459999633789)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-102.pth.tar', 70.50400009521485)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-113.pth.tar', 70.3939999975586)

Train: 124 [   0/1251 (  0%)]  Loss: 3.765 (3.76)  Time: 2.252s,  454.69/s  (2.252s,  454.69/s)  LR: 6.381e-04  Data: 1.517 (1.517)
Train: 124 [  50/1251 (  4%)]  Loss: 3.830 (3.80)  Time: 0.778s, 1316.89/s  (0.817s, 1253.09/s)  LR: 6.381e-04  Data: 0.011 (0.050)
Train: 124 [ 100/1251 (  8%)]  Loss: 3.593 (3.73)  Time: 0.777s, 1317.17/s  (0.801s, 1278.18/s)  LR: 6.381e-04  Data: 0.010 (0.031)
Train: 124 [ 150/1251 ( 12%)]  Loss: 4.031 (3.80)  Time: 0.784s, 1306.26/s  (0.799s, 1281.14/s)  LR: 6.381e-04  Data: 0.013 (0.024)
Train: 124 [ 200/1251 ( 16%)]  Loss: 3.495 (3.74)  Time: 0.776s, 1319.47/s  (0.798s, 1282.48/s)  LR: 6.381e-04  Data: 0.011 (0.021)
Train: 124 [ 250/1251 ( 20%)]  Loss: 3.707 (3.74)  Time: 0.813s, 1259.78/s  (0.799s, 1282.10/s)  LR: 6.381e-04  Data: 0.010 (0.019)
Train: 124 [ 300/1251 ( 24%)]  Loss: 3.666 (3.73)  Time: 0.781s, 1311.78/s  (0.799s, 1282.34/s)  LR: 6.381e-04  Data: 0.011 (0.018)
Train: 124 [ 350/1251 ( 28%)]  Loss: 3.664 (3.72)  Time: 0.821s, 1246.89/s  (0.801s, 1279.04/s)  LR: 6.381e-04  Data: 0.011 (0.017)
Train: 124 [ 400/1251 ( 32%)]  Loss: 3.720 (3.72)  Time: 0.786s, 1302.10/s  (0.802s, 1277.13/s)  LR: 6.381e-04  Data: 0.013 (0.016)
Train: 124 [ 450/1251 ( 36%)]  Loss: 3.637 (3.71)  Time: 0.804s, 1273.78/s  (0.802s, 1277.53/s)  LR: 6.381e-04  Data: 0.011 (0.016)
Train: 124 [ 500/1251 ( 40%)]  Loss: 3.691 (3.71)  Time: 0.825s, 1241.42/s  (0.801s, 1278.74/s)  LR: 6.381e-04  Data: 0.011 (0.015)
Train: 124 [ 550/1251 ( 44%)]  Loss: 3.727 (3.71)  Time: 0.818s, 1252.15/s  (0.801s, 1279.07/s)  LR: 6.381e-04  Data: 0.011 (0.015)
Train: 124 [ 600/1251 ( 48%)]  Loss: 3.512 (3.70)  Time: 0.835s, 1226.38/s  (0.801s, 1279.12/s)  LR: 6.381e-04  Data: 0.011 (0.014)
Train: 124 [ 650/1251 ( 52%)]  Loss: 3.537 (3.68)  Time: 0.801s, 1277.62/s  (0.800s, 1280.09/s)  LR: 6.381e-04  Data: 0.011 (0.014)
Train: 124 [ 700/1251 ( 56%)]  Loss: 3.856 (3.70)  Time: 0.791s, 1293.81/s  (0.799s, 1281.25/s)  LR: 6.381e-04  Data: 0.011 (0.014)
Train: 124 [ 750/1251 ( 60%)]  Loss: 3.563 (3.69)  Time: 0.777s, 1318.05/s  (0.799s, 1281.90/s)  LR: 6.381e-04  Data: 0.011 (0.014)
Train: 124 [ 800/1251 ( 64%)]  Loss: 3.873 (3.70)  Time: 0.821s, 1247.74/s  (0.798s, 1282.47/s)  LR: 6.381e-04  Data: 0.010 (0.014)
Train: 124 [ 850/1251 ( 68%)]  Loss: 3.520 (3.69)  Time: 0.779s, 1314.21/s  (0.798s, 1283.01/s)  LR: 6.381e-04  Data: 0.012 (0.013)
Train: 124 [ 900/1251 ( 72%)]  Loss: 3.816 (3.69)  Time: 0.812s, 1261.23/s  (0.798s, 1283.00/s)  LR: 6.381e-04  Data: 0.011 (0.013)
Train: 124 [ 950/1251 ( 76%)]  Loss: 3.892 (3.70)  Time: 0.796s, 1285.69/s  (0.798s, 1282.91/s)  LR: 6.381e-04  Data: 0.015 (0.013)
Train: 124 [1000/1251 ( 80%)]  Loss: 4.259 (3.73)  Time: 0.778s, 1315.51/s  (0.798s, 1283.00/s)  LR: 6.381e-04  Data: 0.011 (0.013)
Train: 124 [1050/1251 ( 84%)]  Loss: 3.954 (3.74)  Time: 0.781s, 1311.58/s  (0.798s, 1283.19/s)  LR: 6.381e-04  Data: 0.016 (0.013)
Train: 124 [1100/1251 ( 88%)]  Loss: 3.889 (3.75)  Time: 0.804s, 1273.11/s  (0.798s, 1282.79/s)  LR: 6.381e-04  Data: 0.011 (0.013)
Train: 124 [1150/1251 ( 92%)]  Loss: 3.762 (3.75)  Time: 0.774s, 1322.90/s  (0.798s, 1282.75/s)  LR: 6.381e-04  Data: 0.010 (0.013)
Train: 124 [1200/1251 ( 96%)]  Loss: 3.925 (3.76)  Time: 0.791s, 1294.07/s  (0.798s, 1282.92/s)  LR: 6.381e-04  Data: 0.011 (0.013)
Train: 124 [1250/1251 (100%)]  Loss: 3.789 (3.76)  Time: 0.771s, 1327.69/s  (0.798s, 1283.07/s)  LR: 6.381e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.564 (1.564)  Loss:  0.9207 (0.9207)  Acc@1: 87.2070 (87.2070)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.172 (0.566)  Loss:  0.9250 (1.4801)  Acc@1: 84.7877 (71.3380)  Acc@5: 95.5189 (90.2240)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-124.pth.tar', 71.3380001147461)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-123.pth.tar', 71.15400006835938)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-109.pth.tar', 71.0599999633789)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-120.pth.tar', 70.99200004638672)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-115.pth.tar', 70.98600002197266)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-122.pth.tar', 70.86999997070312)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-117.pth.tar', 70.67199993896485)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-121.pth.tar', 70.63799999023438)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-118.pth.tar', 70.5459999633789)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-102.pth.tar', 70.50400009521485)

Train: 125 [   0/1251 (  0%)]  Loss: 3.711 (3.71)  Time: 2.344s,  436.92/s  (2.344s,  436.92/s)  LR: 6.331e-04  Data: 1.549 (1.549)
Train: 125 [  50/1251 (  4%)]  Loss: 3.699 (3.70)  Time: 0.832s, 1230.18/s  (0.839s, 1221.09/s)  LR: 6.331e-04  Data: 0.011 (0.047)
Train: 125 [ 100/1251 (  8%)]  Loss: 3.956 (3.79)  Time: 0.789s, 1297.96/s  (0.818s, 1251.35/s)  LR: 6.331e-04  Data: 0.016 (0.029)
Train: 125 [ 150/1251 ( 12%)]  Loss: 3.760 (3.78)  Time: 0.789s, 1297.88/s  (0.810s, 1264.53/s)  LR: 6.331e-04  Data: 0.011 (0.023)
Train: 125 [ 200/1251 ( 16%)]  Loss: 3.373 (3.70)  Time: 0.825s, 1241.54/s  (0.807s, 1268.49/s)  LR: 6.331e-04  Data: 0.011 (0.020)
Train: 125 [ 250/1251 ( 20%)]  Loss: 4.080 (3.76)  Time: 0.779s, 1314.94/s  (0.807s, 1269.08/s)  LR: 6.331e-04  Data: 0.011 (0.019)
Train: 125 [ 300/1251 ( 24%)]  Loss: 4.057 (3.81)  Time: 0.777s, 1317.56/s  (0.805s, 1271.96/s)  LR: 6.331e-04  Data: 0.011 (0.017)
Train: 125 [ 350/1251 ( 28%)]  Loss: 3.584 (3.78)  Time: 0.877s, 1167.52/s  (0.803s, 1274.45/s)  LR: 6.331e-04  Data: 0.013 (0.016)
Train: 125 [ 400/1251 ( 32%)]  Loss: 4.054 (3.81)  Time: 0.777s, 1317.34/s  (0.802s, 1276.24/s)  LR: 6.331e-04  Data: 0.013 (0.016)
Train: 125 [ 450/1251 ( 36%)]  Loss: 3.339 (3.76)  Time: 0.783s, 1308.20/s  (0.801s, 1278.08/s)  LR: 6.331e-04  Data: 0.011 (0.015)
Train: 125 [ 500/1251 ( 40%)]  Loss: 3.646 (3.75)  Time: 0.777s, 1318.60/s  (0.801s, 1278.25/s)  LR: 6.331e-04  Data: 0.011 (0.015)
Train: 125 [ 550/1251 ( 44%)]  Loss: 4.008 (3.77)  Time: 0.776s, 1319.37/s  (0.801s, 1278.84/s)  LR: 6.331e-04  Data: 0.011 (0.015)
Train: 125 [ 600/1251 ( 48%)]  Loss: 4.033 (3.79)  Time: 0.779s, 1314.18/s  (0.800s, 1280.34/s)  LR: 6.331e-04  Data: 0.011 (0.014)
Train: 125 [ 650/1251 ( 52%)]  Loss: 4.015 (3.81)  Time: 0.780s, 1312.09/s  (0.799s, 1281.13/s)  LR: 6.331e-04  Data: 0.011 (0.014)
Train: 125 [ 700/1251 ( 56%)]  Loss: 4.003 (3.82)  Time: 0.834s, 1227.60/s  (0.798s, 1282.42/s)  LR: 6.331e-04  Data: 0.011 (0.014)
Train: 125 [ 750/1251 ( 60%)]  Loss: 3.398 (3.79)  Time: 0.784s, 1306.52/s  (0.798s, 1283.25/s)  LR: 6.331e-04  Data: 0.010 (0.014)
Train: 125 [ 800/1251 ( 64%)]  Loss: 3.902 (3.80)  Time: 0.783s, 1307.94/s  (0.797s, 1284.04/s)  LR: 6.331e-04  Data: 0.011 (0.014)
Train: 125 [ 850/1251 ( 68%)]  Loss: 4.117 (3.82)  Time: 0.786s, 1303.24/s  (0.797s, 1284.16/s)  LR: 6.331e-04  Data: 0.012 (0.013)
Train: 125 [ 900/1251 ( 72%)]  Loss: 3.641 (3.81)  Time: 0.786s, 1302.63/s  (0.797s, 1284.28/s)  LR: 6.331e-04  Data: 0.014 (0.013)
Train: 125 [ 950/1251 ( 76%)]  Loss: 3.788 (3.81)  Time: 0.895s, 1143.71/s  (0.797s, 1284.91/s)  LR: 6.331e-04  Data: 0.010 (0.013)
Train: 125 [1000/1251 ( 80%)]  Loss: 3.750 (3.81)  Time: 0.838s, 1222.18/s  (0.797s, 1284.52/s)  LR: 6.331e-04  Data: 0.011 (0.013)
Train: 125 [1050/1251 ( 84%)]  Loss: 3.948 (3.81)  Time: 0.777s, 1318.17/s  (0.797s, 1284.46/s)  LR: 6.331e-04  Data: 0.011 (0.013)
Train: 125 [1100/1251 ( 88%)]  Loss: 4.222 (3.83)  Time: 0.776s, 1319.33/s  (0.797s, 1284.79/s)  LR: 6.331e-04  Data: 0.010 (0.013)
Train: 125 [1150/1251 ( 92%)]  Loss: 4.015 (3.84)  Time: 0.818s, 1251.53/s  (0.797s, 1284.88/s)  LR: 6.331e-04  Data: 0.011 (0.013)
Train: 125 [1200/1251 ( 96%)]  Loss: 3.832 (3.84)  Time: 0.779s, 1314.36/s  (0.797s, 1285.15/s)  LR: 6.331e-04  Data: 0.012 (0.013)
Train: 125 [1250/1251 (100%)]  Loss: 3.975 (3.84)  Time: 0.769s, 1332.06/s  (0.796s, 1285.70/s)  LR: 6.331e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.542 (1.542)  Loss:  0.8842 (0.8842)  Acc@1: 87.3047 (87.3047)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.172 (0.563)  Loss:  1.0984 (1.5493)  Acc@1: 82.0755 (70.6640)  Acc@5: 94.9292 (90.1840)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-124.pth.tar', 71.3380001147461)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-123.pth.tar', 71.15400006835938)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-109.pth.tar', 71.0599999633789)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-120.pth.tar', 70.99200004638672)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-115.pth.tar', 70.98600002197266)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-122.pth.tar', 70.86999997070312)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-117.pth.tar', 70.67199993896485)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-125.pth.tar', 70.66399997070313)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-121.pth.tar', 70.63799999023438)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-118.pth.tar', 70.5459999633789)

Train: 126 [   0/1251 (  0%)]  Loss: 3.657 (3.66)  Time: 2.365s,  432.97/s  (2.365s,  432.97/s)  LR: 6.281e-04  Data: 1.568 (1.568)
Train: 126 [  50/1251 (  4%)]  Loss: 3.901 (3.78)  Time: 0.780s, 1312.50/s  (0.831s, 1231.79/s)  LR: 6.281e-04  Data: 0.011 (0.045)
Train: 126 [ 100/1251 (  8%)]  Loss: 3.502 (3.69)  Time: 0.778s, 1316.24/s  (0.811s, 1262.08/s)  LR: 6.281e-04  Data: 0.011 (0.028)
Train: 126 [ 150/1251 ( 12%)]  Loss: 3.320 (3.60)  Time: 0.784s, 1305.36/s  (0.804s, 1272.94/s)  LR: 6.281e-04  Data: 0.011 (0.022)
Train: 126 [ 200/1251 ( 16%)]  Loss: 4.190 (3.71)  Time: 0.808s, 1267.09/s  (0.803s, 1274.53/s)  LR: 6.281e-04  Data: 0.011 (0.020)
Train: 126 [ 250/1251 ( 20%)]  Loss: 3.969 (3.76)  Time: 0.790s, 1295.46/s  (0.802s, 1276.23/s)  LR: 6.281e-04  Data: 0.011 (0.018)
Train: 126 [ 300/1251 ( 24%)]  Loss: 3.949 (3.78)  Time: 0.779s, 1313.78/s  (0.801s, 1277.97/s)  LR: 6.281e-04  Data: 0.011 (0.017)
Train: 126 [ 350/1251 ( 28%)]  Loss: 4.000 (3.81)  Time: 0.830s, 1234.43/s  (0.801s, 1278.06/s)  LR: 6.281e-04  Data: 0.011 (0.016)
Train: 126 [ 400/1251 ( 32%)]  Loss: 3.989 (3.83)  Time: 0.784s, 1306.70/s  (0.800s, 1279.67/s)  LR: 6.281e-04  Data: 0.013 (0.016)
Train: 126 [ 450/1251 ( 36%)]  Loss: 3.658 (3.81)  Time: 0.779s, 1314.16/s  (0.800s, 1280.55/s)  LR: 6.281e-04  Data: 0.011 (0.015)
Train: 126 [ 500/1251 ( 40%)]  Loss: 3.726 (3.81)  Time: 0.804s, 1273.04/s  (0.799s, 1281.44/s)  LR: 6.281e-04  Data: 0.011 (0.015)
Train: 126 [ 550/1251 ( 44%)]  Loss: 3.861 (3.81)  Time: 0.784s, 1306.83/s  (0.799s, 1281.54/s)  LR: 6.281e-04  Data: 0.011 (0.014)
Train: 126 [ 600/1251 ( 48%)]  Loss: 4.234 (3.84)  Time: 0.828s, 1236.79/s  (0.800s, 1280.46/s)  LR: 6.281e-04  Data: 0.014 (0.014)
Train: 126 [ 650/1251 ( 52%)]  Loss: 3.726 (3.83)  Time: 0.777s, 1318.37/s  (0.800s, 1280.38/s)  LR: 6.281e-04  Data: 0.010 (0.014)
Train: 126 [ 700/1251 ( 56%)]  Loss: 4.130 (3.85)  Time: 0.817s, 1253.12/s  (0.799s, 1281.27/s)  LR: 6.281e-04  Data: 0.011 (0.014)
Train: 126 [ 750/1251 ( 60%)]  Loss: 3.744 (3.85)  Time: 0.779s, 1314.23/s  (0.798s, 1282.56/s)  LR: 6.281e-04  Data: 0.011 (0.014)
Train: 126 [ 800/1251 ( 64%)]  Loss: 3.897 (3.85)  Time: 0.780s, 1312.94/s  (0.798s, 1283.66/s)  LR: 6.281e-04  Data: 0.011 (0.013)
Train: 126 [ 850/1251 ( 68%)]  Loss: 4.022 (3.86)  Time: 0.777s, 1317.09/s  (0.797s, 1284.39/s)  LR: 6.281e-04  Data: 0.011 (0.013)
Train: 126 [ 900/1251 ( 72%)]  Loss: 3.611 (3.85)  Time: 0.840s, 1219.20/s  (0.797s, 1284.81/s)  LR: 6.281e-04  Data: 0.011 (0.013)
Train: 126 [ 950/1251 ( 76%)]  Loss: 3.911 (3.85)  Time: 0.792s, 1293.72/s  (0.797s, 1285.55/s)  LR: 6.281e-04  Data: 0.011 (0.013)
Train: 126 [1000/1251 ( 80%)]  Loss: 3.934 (3.85)  Time: 0.784s, 1305.94/s  (0.796s, 1285.69/s)  LR: 6.281e-04  Data: 0.010 (0.013)
Train: 126 [1050/1251 ( 84%)]  Loss: 3.969 (3.86)  Time: 0.778s, 1315.65/s  (0.796s, 1285.83/s)  LR: 6.281e-04  Data: 0.011 (0.013)
Train: 126 [1100/1251 ( 88%)]  Loss: 3.889 (3.86)  Time: 0.776s, 1319.98/s  (0.796s, 1285.90/s)  LR: 6.281e-04  Data: 0.011 (0.013)
Train: 126 [1150/1251 ( 92%)]  Loss: 3.832 (3.86)  Time: 0.827s, 1238.04/s  (0.796s, 1286.01/s)  LR: 6.281e-04  Data: 0.012 (0.013)
Train: 126 [1200/1251 ( 96%)]  Loss: 3.584 (3.85)  Time: 0.780s, 1313.29/s  (0.797s, 1285.56/s)  LR: 6.281e-04  Data: 0.012 (0.013)
Train: 126 [1250/1251 (100%)]  Loss: 3.918 (3.85)  Time: 0.764s, 1340.41/s  (0.796s, 1285.73/s)  LR: 6.281e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.590 (1.590)  Loss:  1.0350 (1.0350)  Acc@1: 86.8164 (86.8164)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.172 (0.564)  Loss:  1.0088 (1.6089)  Acc@1: 83.7264 (71.3900)  Acc@5: 96.3443 (90.4480)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-126.pth.tar', 71.38999991210937)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-124.pth.tar', 71.3380001147461)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-123.pth.tar', 71.15400006835938)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-109.pth.tar', 71.0599999633789)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-120.pth.tar', 70.99200004638672)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-115.pth.tar', 70.98600002197266)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-122.pth.tar', 70.86999997070312)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-117.pth.tar', 70.67199993896485)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-125.pth.tar', 70.66399997070313)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-121.pth.tar', 70.63799999023438)

Train: 127 [   0/1251 (  0%)]  Loss: 3.731 (3.73)  Time: 2.379s,  430.43/s  (2.379s,  430.43/s)  LR: 6.231e-04  Data: 1.643 (1.643)
Train: 127 [  50/1251 (  4%)]  Loss: 3.615 (3.67)  Time: 0.778s, 1316.99/s  (0.832s, 1230.31/s)  LR: 6.231e-04  Data: 0.011 (0.052)
Train: 127 [ 100/1251 (  8%)]  Loss: 3.767 (3.70)  Time: 0.783s, 1308.37/s  (0.814s, 1257.26/s)  LR: 6.231e-04  Data: 0.011 (0.032)
Train: 127 [ 150/1251 ( 12%)]  Loss: 3.522 (3.66)  Time: 0.778s, 1316.61/s  (0.809s, 1266.54/s)  LR: 6.231e-04  Data: 0.011 (0.025)
Train: 127 [ 200/1251 ( 16%)]  Loss: 3.725 (3.67)  Time: 0.830s, 1234.31/s  (0.805s, 1271.83/s)  LR: 6.231e-04  Data: 0.011 (0.021)
Train: 127 [ 250/1251 ( 20%)]  Loss: 3.574 (3.66)  Time: 0.776s, 1319.72/s  (0.804s, 1274.38/s)  LR: 6.231e-04  Data: 0.011 (0.019)
Train: 127 [ 300/1251 ( 24%)]  Loss: 3.868 (3.69)  Time: 0.801s, 1279.00/s  (0.802s, 1276.47/s)  LR: 6.231e-04  Data: 0.011 (0.018)
Train: 127 [ 350/1251 ( 28%)]  Loss: 3.713 (3.69)  Time: 0.828s, 1236.85/s  (0.802s, 1276.99/s)  LR: 6.231e-04  Data: 0.011 (0.017)
Train: 127 [ 400/1251 ( 32%)]  Loss: 3.590 (3.68)  Time: 0.829s, 1234.95/s  (0.802s, 1276.53/s)  LR: 6.231e-04  Data: 0.011 (0.016)
Train: 127 [ 450/1251 ( 36%)]  Loss: 3.925 (3.70)  Time: 0.780s, 1312.31/s  (0.801s, 1278.74/s)  LR: 6.231e-04  Data: 0.010 (0.016)
Train: 127 [ 500/1251 ( 40%)]  Loss: 3.640 (3.70)  Time: 0.835s, 1226.55/s  (0.800s, 1279.59/s)  LR: 6.231e-04  Data: 0.010 (0.015)
Train: 127 [ 550/1251 ( 44%)]  Loss: 3.823 (3.71)  Time: 0.785s, 1303.76/s  (0.800s, 1280.38/s)  LR: 6.231e-04  Data: 0.011 (0.015)
Train: 127 [ 600/1251 ( 48%)]  Loss: 3.691 (3.71)  Time: 0.794s, 1289.41/s  (0.799s, 1281.38/s)  LR: 6.231e-04  Data: 0.011 (0.015)
Train: 127 [ 650/1251 ( 52%)]  Loss: 3.932 (3.72)  Time: 0.845s, 1212.06/s  (0.798s, 1282.54/s)  LR: 6.231e-04  Data: 0.011 (0.014)
Train: 127 [ 700/1251 ( 56%)]  Loss: 3.335 (3.70)  Time: 0.794s, 1290.13/s  (0.798s, 1282.76/s)  LR: 6.231e-04  Data: 0.012 (0.014)
Train: 127 [ 750/1251 ( 60%)]  Loss: 3.531 (3.69)  Time: 0.806s, 1269.80/s  (0.798s, 1283.31/s)  LR: 6.231e-04  Data: 0.011 (0.014)
Train: 127 [ 800/1251 ( 64%)]  Loss: 4.153 (3.71)  Time: 0.828s, 1236.24/s  (0.798s, 1283.63/s)  LR: 6.231e-04  Data: 0.013 (0.014)
Train: 127 [ 850/1251 ( 68%)]  Loss: 3.954 (3.73)  Time: 0.799s, 1281.16/s  (0.798s, 1284.00/s)  LR: 6.231e-04  Data: 0.012 (0.014)
Train: 127 [ 900/1251 ( 72%)]  Loss: 3.765 (3.73)  Time: 0.776s, 1320.10/s  (0.798s, 1284.00/s)  LR: 6.231e-04  Data: 0.011 (0.014)
Train: 127 [ 950/1251 ( 76%)]  Loss: 3.840 (3.73)  Time: 0.809s, 1266.14/s  (0.798s, 1283.79/s)  LR: 6.231e-04  Data: 0.011 (0.013)
Train: 127 [1000/1251 ( 80%)]  Loss: 3.780 (3.74)  Time: 0.798s, 1283.77/s  (0.798s, 1283.93/s)  LR: 6.231e-04  Data: 0.011 (0.013)
Train: 127 [1050/1251 ( 84%)]  Loss: 3.793 (3.74)  Time: 0.787s, 1300.63/s  (0.797s, 1284.07/s)  LR: 6.231e-04  Data: 0.011 (0.013)
Train: 127 [1100/1251 ( 88%)]  Loss: 3.711 (3.74)  Time: 0.806s, 1270.30/s  (0.797s, 1284.69/s)  LR: 6.231e-04  Data: 0.011 (0.013)
Train: 127 [1150/1251 ( 92%)]  Loss: 3.158 (3.71)  Time: 0.804s, 1273.94/s  (0.797s, 1284.73/s)  LR: 6.231e-04  Data: 0.011 (0.013)
Train: 127 [1200/1251 ( 96%)]  Loss: 3.741 (3.72)  Time: 0.780s, 1313.29/s  (0.797s, 1285.54/s)  LR: 6.231e-04  Data: 0.010 (0.013)
Train: 127 [1250/1251 (100%)]  Loss: 3.733 (3.72)  Time: 0.766s, 1336.11/s  (0.796s, 1285.67/s)  LR: 6.231e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.548 (1.548)  Loss:  1.0041 (1.0041)  Acc@1: 87.2070 (87.2070)  Acc@5: 96.0938 (96.0938)
Test: [  48/48]  Time: 0.171 (0.564)  Loss:  1.0488 (1.5402)  Acc@1: 83.0189 (70.9400)  Acc@5: 95.0472 (90.2620)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-126.pth.tar', 71.38999991210937)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-124.pth.tar', 71.3380001147461)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-123.pth.tar', 71.15400006835938)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-109.pth.tar', 71.0599999633789)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-120.pth.tar', 70.99200004638672)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-115.pth.tar', 70.98600002197266)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-127.pth.tar', 70.93999999267578)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-122.pth.tar', 70.86999997070312)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-117.pth.tar', 70.67199993896485)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-125.pth.tar', 70.66399997070313)

Train: 128 [   0/1251 (  0%)]  Loss: 3.698 (3.70)  Time: 2.257s,  453.66/s  (2.257s,  453.66/s)  LR: 6.180e-04  Data: 1.520 (1.520)
Train: 128 [  50/1251 (  4%)]  Loss: 3.888 (3.79)  Time: 0.779s, 1314.26/s  (0.825s, 1241.40/s)  LR: 6.180e-04  Data: 0.011 (0.045)
Train: 128 [ 100/1251 (  8%)]  Loss: 3.854 (3.81)  Time: 0.817s, 1253.74/s  (0.809s, 1265.75/s)  LR: 6.180e-04  Data: 0.011 (0.028)
Train: 128 [ 150/1251 ( 12%)]  Loss: 3.577 (3.75)  Time: 0.782s, 1310.17/s  (0.806s, 1270.77/s)  LR: 6.180e-04  Data: 0.011 (0.023)
Train: 128 [ 200/1251 ( 16%)]  Loss: 3.533 (3.71)  Time: 0.778s, 1316.19/s  (0.804s, 1274.35/s)  LR: 6.180e-04  Data: 0.011 (0.020)
Train: 128 [ 250/1251 ( 20%)]  Loss: 3.862 (3.74)  Time: 0.808s, 1266.68/s  (0.802s, 1277.06/s)  LR: 6.180e-04  Data: 0.012 (0.018)
Train: 128 [ 300/1251 ( 24%)]  Loss: 4.099 (3.79)  Time: 0.777s, 1317.37/s  (0.801s, 1278.30/s)  LR: 6.180e-04  Data: 0.012 (0.017)
Train: 128 [ 350/1251 ( 28%)]  Loss: 3.927 (3.80)  Time: 0.842s, 1215.52/s  (0.800s, 1280.30/s)  LR: 6.180e-04  Data: 0.011 (0.016)
Train: 128 [ 400/1251 ( 32%)]  Loss: 3.818 (3.81)  Time: 0.778s, 1315.63/s  (0.800s, 1280.46/s)  LR: 6.180e-04  Data: 0.011 (0.016)
Train: 128 [ 450/1251 ( 36%)]  Loss: 3.360 (3.76)  Time: 0.785s, 1304.94/s  (0.799s, 1282.11/s)  LR: 6.180e-04  Data: 0.011 (0.015)
Train: 128 [ 500/1251 ( 40%)]  Loss: 3.636 (3.75)  Time: 0.820s, 1248.09/s  (0.798s, 1282.52/s)  LR: 6.180e-04  Data: 0.011 (0.015)
Train: 128 [ 550/1251 ( 44%)]  Loss: 3.275 (3.71)  Time: 0.792s, 1293.17/s  (0.798s, 1282.86/s)  LR: 6.180e-04  Data: 0.011 (0.015)
Train: 128 [ 600/1251 ( 48%)]  Loss: 3.550 (3.70)  Time: 0.846s, 1209.98/s  (0.799s, 1281.64/s)  LR: 6.180e-04  Data: 0.012 (0.014)
Train: 128 [ 650/1251 ( 52%)]  Loss: 4.140 (3.73)  Time: 0.806s, 1270.59/s  (0.798s, 1283.13/s)  LR: 6.180e-04  Data: 0.011 (0.014)
Train: 128 [ 700/1251 ( 56%)]  Loss: 3.694 (3.73)  Time: 0.784s, 1305.83/s  (0.798s, 1283.54/s)  LR: 6.180e-04  Data: 0.011 (0.014)
Train: 128 [ 750/1251 ( 60%)]  Loss: 3.876 (3.74)  Time: 0.790s, 1296.74/s  (0.797s, 1284.11/s)  LR: 6.180e-04  Data: 0.011 (0.014)
Train: 128 [ 800/1251 ( 64%)]  Loss: 4.008 (3.75)  Time: 0.783s, 1308.32/s  (0.797s, 1284.66/s)  LR: 6.180e-04  Data: 0.011 (0.014)
Train: 128 [ 850/1251 ( 68%)]  Loss: 3.517 (3.74)  Time: 0.781s, 1311.69/s  (0.797s, 1285.32/s)  LR: 6.180e-04  Data: 0.011 (0.013)
Train: 128 [ 900/1251 ( 72%)]  Loss: 3.529 (3.73)  Time: 0.789s, 1297.91/s  (0.797s, 1285.24/s)  LR: 6.180e-04  Data: 0.011 (0.013)
Train: 128 [ 950/1251 ( 76%)]  Loss: 3.521 (3.72)  Time: 0.777s, 1318.24/s  (0.797s, 1285.48/s)  LR: 6.180e-04  Data: 0.011 (0.013)
Train: 128 [1000/1251 ( 80%)]  Loss: 3.937 (3.73)  Time: 0.778s, 1316.62/s  (0.797s, 1285.56/s)  LR: 6.180e-04  Data: 0.011 (0.013)
Train: 128 [1050/1251 ( 84%)]  Loss: 3.950 (3.74)  Time: 0.780s, 1312.81/s  (0.797s, 1285.41/s)  LR: 6.180e-04  Data: 0.011 (0.013)
Train: 128 [1100/1251 ( 88%)]  Loss: 3.925 (3.75)  Time: 0.782s, 1309.88/s  (0.797s, 1285.26/s)  LR: 6.180e-04  Data: 0.011 (0.013)
Train: 128 [1150/1251 ( 92%)]  Loss: 3.524 (3.74)  Time: 0.817s, 1253.76/s  (0.797s, 1284.51/s)  LR: 6.180e-04  Data: 0.010 (0.013)
Train: 128 [1200/1251 ( 96%)]  Loss: 3.662 (3.73)  Time: 0.776s, 1319.85/s  (0.798s, 1283.71/s)  LR: 6.180e-04  Data: 0.011 (0.013)
Train: 128 [1250/1251 (100%)]  Loss: 3.835 (3.74)  Time: 0.771s, 1328.96/s  (0.797s, 1284.02/s)  LR: 6.180e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.526 (1.526)  Loss:  0.9333 (0.9333)  Acc@1: 87.3047 (87.3047)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.172 (0.560)  Loss:  0.9130 (1.4923)  Acc@1: 84.6698 (71.7180)  Acc@5: 94.9292 (90.7580)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-128.pth.tar', 71.71800006347657)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-126.pth.tar', 71.38999991210937)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-124.pth.tar', 71.3380001147461)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-123.pth.tar', 71.15400006835938)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-109.pth.tar', 71.0599999633789)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-120.pth.tar', 70.99200004638672)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-115.pth.tar', 70.98600002197266)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-127.pth.tar', 70.93999999267578)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-122.pth.tar', 70.86999997070312)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-117.pth.tar', 70.67199993896485)

Train: 129 [   0/1251 (  0%)]  Loss: 3.934 (3.93)  Time: 2.441s,  419.57/s  (2.441s,  419.57/s)  LR: 6.130e-04  Data: 1.649 (1.649)
Train: 129 [  50/1251 (  4%)]  Loss: 3.926 (3.93)  Time: 0.779s, 1315.23/s  (0.832s, 1230.27/s)  LR: 6.130e-04  Data: 0.012 (0.048)
Train: 129 [ 100/1251 (  8%)]  Loss: 3.957 (3.94)  Time: 0.820s, 1248.27/s  (0.816s, 1255.05/s)  LR: 6.130e-04  Data: 0.012 (0.030)
Train: 129 [ 150/1251 ( 12%)]  Loss: 3.997 (3.95)  Time: 0.777s, 1318.26/s  (0.813s, 1259.82/s)  LR: 6.130e-04  Data: 0.011 (0.024)
Train: 129 [ 200/1251 ( 16%)]  Loss: 3.756 (3.91)  Time: 0.779s, 1314.46/s  (0.809s, 1265.60/s)  LR: 6.130e-04  Data: 0.011 (0.021)
Train: 129 [ 250/1251 ( 20%)]  Loss: 3.553 (3.85)  Time: 0.835s, 1226.93/s  (0.807s, 1268.72/s)  LR: 6.130e-04  Data: 0.011 (0.019)
Train: 129 [ 300/1251 ( 24%)]  Loss: 3.816 (3.85)  Time: 0.832s, 1230.81/s  (0.806s, 1270.53/s)  LR: 6.130e-04  Data: 0.011 (0.018)
Train: 129 [ 350/1251 ( 28%)]  Loss: 3.860 (3.85)  Time: 0.840s, 1218.87/s  (0.804s, 1272.91/s)  LR: 6.130e-04  Data: 0.011 (0.017)
Train: 129 [ 400/1251 ( 32%)]  Loss: 3.443 (3.80)  Time: 0.775s, 1321.02/s  (0.804s, 1273.34/s)  LR: 6.130e-04  Data: 0.010 (0.016)
Train: 129 [ 450/1251 ( 36%)]  Loss: 3.895 (3.81)  Time: 0.774s, 1323.05/s  (0.804s, 1274.33/s)  LR: 6.130e-04  Data: 0.010 (0.015)
Train: 129 [ 500/1251 ( 40%)]  Loss: 3.535 (3.79)  Time: 0.829s, 1235.86/s  (0.803s, 1275.71/s)  LR: 6.130e-04  Data: 0.011 (0.015)
Train: 129 [ 550/1251 ( 44%)]  Loss: 3.912 (3.80)  Time: 0.845s, 1211.99/s  (0.802s, 1276.55/s)  LR: 6.130e-04  Data: 0.014 (0.015)
Train: 129 [ 600/1251 ( 48%)]  Loss: 3.513 (3.78)  Time: 0.780s, 1312.00/s  (0.802s, 1277.57/s)  LR: 6.130e-04  Data: 0.011 (0.014)
Train: 129 [ 650/1251 ( 52%)]  Loss: 3.977 (3.79)  Time: 0.827s, 1238.02/s  (0.801s, 1278.66/s)  LR: 6.130e-04  Data: 0.010 (0.014)
Train: 129 [ 700/1251 ( 56%)]  Loss: 3.999 (3.80)  Time: 0.776s, 1320.22/s  (0.800s, 1279.70/s)  LR: 6.130e-04  Data: 0.012 (0.014)
Train: 129 [ 750/1251 ( 60%)]  Loss: 3.831 (3.81)  Time: 0.778s, 1315.58/s  (0.800s, 1280.56/s)  LR: 6.130e-04  Data: 0.012 (0.014)
Train: 129 [ 800/1251 ( 64%)]  Loss: 3.730 (3.80)  Time: 0.839s, 1219.96/s  (0.799s, 1281.29/s)  LR: 6.130e-04  Data: 0.012 (0.014)
Train: 129 [ 850/1251 ( 68%)]  Loss: 3.594 (3.79)  Time: 0.790s, 1296.54/s  (0.799s, 1281.62/s)  LR: 6.130e-04  Data: 0.016 (0.014)
Train: 129 [ 900/1251 ( 72%)]  Loss: 3.914 (3.80)  Time: 0.778s, 1316.84/s  (0.799s, 1282.06/s)  LR: 6.130e-04  Data: 0.011 (0.013)
Train: 129 [ 950/1251 ( 76%)]  Loss: 3.803 (3.80)  Time: 0.780s, 1313.40/s  (0.799s, 1282.04/s)  LR: 6.130e-04  Data: 0.012 (0.013)
Train: 129 [1000/1251 ( 80%)]  Loss: 3.588 (3.79)  Time: 0.791s, 1293.79/s  (0.798s, 1282.68/s)  LR: 6.130e-04  Data: 0.011 (0.013)
Train: 129 [1050/1251 ( 84%)]  Loss: 3.852 (3.79)  Time: 0.805s, 1271.74/s  (0.798s, 1282.97/s)  LR: 6.130e-04  Data: 0.011 (0.013)
Train: 129 [1100/1251 ( 88%)]  Loss: 3.687 (3.79)  Time: 0.780s, 1313.58/s  (0.798s, 1283.17/s)  LR: 6.130e-04  Data: 0.011 (0.013)
Train: 129 [1150/1251 ( 92%)]  Loss: 3.881 (3.79)  Time: 0.781s, 1311.91/s  (0.798s, 1283.87/s)  LR: 6.130e-04  Data: 0.011 (0.013)
Train: 129 [1200/1251 ( 96%)]  Loss: 3.763 (3.79)  Time: 0.827s, 1238.69/s  (0.797s, 1284.11/s)  LR: 6.130e-04  Data: 0.012 (0.013)
Train: 129 [1250/1251 (100%)]  Loss: 3.579 (3.78)  Time: 0.810s, 1264.29/s  (0.797s, 1284.32/s)  LR: 6.130e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.600 (1.600)  Loss:  0.9170 (0.9170)  Acc@1: 86.8164 (86.8164)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.172 (0.568)  Loss:  0.9420 (1.5007)  Acc@1: 84.3160 (71.4700)  Acc@5: 95.7547 (90.4460)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-128.pth.tar', 71.71800006347657)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-129.pth.tar', 71.47000016845703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-126.pth.tar', 71.38999991210937)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-124.pth.tar', 71.3380001147461)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-123.pth.tar', 71.15400006835938)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-109.pth.tar', 71.0599999633789)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-120.pth.tar', 70.99200004638672)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-115.pth.tar', 70.98600002197266)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-127.pth.tar', 70.93999999267578)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-122.pth.tar', 70.86999997070312)

Train: 130 [   0/1251 (  0%)]  Loss: 3.758 (3.76)  Time: 2.309s,  443.57/s  (2.309s,  443.57/s)  LR: 6.079e-04  Data: 1.546 (1.546)
Train: 130 [  50/1251 (  4%)]  Loss: 4.004 (3.88)  Time: 0.780s, 1312.65/s  (0.831s, 1232.01/s)  LR: 6.079e-04  Data: 0.011 (0.049)
Train: 130 [ 100/1251 (  8%)]  Loss: 4.052 (3.94)  Time: 0.778s, 1315.52/s  (0.813s, 1259.85/s)  LR: 6.079e-04  Data: 0.011 (0.030)
Train: 130 [ 150/1251 ( 12%)]  Loss: 3.294 (3.78)  Time: 0.832s, 1231.17/s  (0.806s, 1270.63/s)  LR: 6.079e-04  Data: 0.012 (0.024)
Train: 130 [ 200/1251 ( 16%)]  Loss: 3.901 (3.80)  Time: 0.780s, 1312.37/s  (0.801s, 1277.81/s)  LR: 6.079e-04  Data: 0.011 (0.021)
Train: 130 [ 250/1251 ( 20%)]  Loss: 3.752 (3.79)  Time: 0.777s, 1317.91/s  (0.799s, 1281.26/s)  LR: 6.079e-04  Data: 0.011 (0.019)
Train: 130 [ 300/1251 ( 24%)]  Loss: 3.551 (3.76)  Time: 0.777s, 1317.11/s  (0.799s, 1282.39/s)  LR: 6.079e-04  Data: 0.011 (0.018)
Train: 130 [ 350/1251 ( 28%)]  Loss: 3.858 (3.77)  Time: 0.834s, 1228.41/s  (0.800s, 1280.19/s)  LR: 6.079e-04  Data: 0.011 (0.017)
Train: 130 [ 400/1251 ( 32%)]  Loss: 3.719 (3.77)  Time: 0.808s, 1267.86/s  (0.799s, 1281.03/s)  LR: 6.079e-04  Data: 0.011 (0.016)
Train: 130 [ 450/1251 ( 36%)]  Loss: 3.877 (3.78)  Time: 0.776s, 1319.76/s  (0.799s, 1281.73/s)  LR: 6.079e-04  Data: 0.011 (0.016)
Train: 130 [ 500/1251 ( 40%)]  Loss: 3.870 (3.78)  Time: 0.790s, 1295.46/s  (0.798s, 1282.48/s)  LR: 6.079e-04  Data: 0.011 (0.015)
Train: 130 [ 550/1251 ( 44%)]  Loss: 3.661 (3.77)  Time: 0.780s, 1313.45/s  (0.799s, 1282.30/s)  LR: 6.079e-04  Data: 0.011 (0.015)
Train: 130 [ 600/1251 ( 48%)]  Loss: 3.616 (3.76)  Time: 0.823s, 1243.89/s  (0.798s, 1282.71/s)  LR: 6.079e-04  Data: 0.011 (0.015)
Train: 130 [ 650/1251 ( 52%)]  Loss: 3.927 (3.77)  Time: 0.789s, 1297.49/s  (0.798s, 1282.79/s)  LR: 6.079e-04  Data: 0.016 (0.014)
Train: 130 [ 700/1251 ( 56%)]  Loss: 3.642 (3.77)  Time: 0.822s, 1245.62/s  (0.798s, 1282.47/s)  LR: 6.079e-04  Data: 0.011 (0.014)
Train: 130 [ 750/1251 ( 60%)]  Loss: 3.633 (3.76)  Time: 0.868s, 1179.60/s  (0.799s, 1282.36/s)  LR: 6.079e-04  Data: 0.011 (0.014)
Train: 130 [ 800/1251 ( 64%)]  Loss: 4.296 (3.79)  Time: 0.810s, 1264.51/s  (0.798s, 1282.80/s)  LR: 6.079e-04  Data: 0.012 (0.014)
Train: 130 [ 850/1251 ( 68%)]  Loss: 3.855 (3.79)  Time: 0.779s, 1314.93/s  (0.798s, 1283.37/s)  LR: 6.079e-04  Data: 0.011 (0.014)
Train: 130 [ 900/1251 ( 72%)]  Loss: 3.918 (3.80)  Time: 0.778s, 1316.88/s  (0.798s, 1283.07/s)  LR: 6.079e-04  Data: 0.012 (0.014)
Train: 130 [ 950/1251 ( 76%)]  Loss: 4.087 (3.81)  Time: 0.833s, 1229.85/s  (0.798s, 1283.65/s)  LR: 6.079e-04  Data: 0.011 (0.013)
Train: 130 [1000/1251 ( 80%)]  Loss: 3.818 (3.81)  Time: 0.780s, 1313.39/s  (0.798s, 1283.90/s)  LR: 6.079e-04  Data: 0.011 (0.013)
Train: 130 [1050/1251 ( 84%)]  Loss: 3.870 (3.82)  Time: 0.778s, 1315.81/s  (0.797s, 1284.34/s)  LR: 6.079e-04  Data: 0.012 (0.013)
Train: 130 [1100/1251 ( 88%)]  Loss: 3.452 (3.80)  Time: 0.779s, 1314.40/s  (0.797s, 1284.62/s)  LR: 6.079e-04  Data: 0.011 (0.013)
Train: 130 [1150/1251 ( 92%)]  Loss: 3.939 (3.81)  Time: 0.795s, 1287.35/s  (0.797s, 1285.03/s)  LR: 6.079e-04  Data: 0.011 (0.013)
Train: 130 [1200/1251 ( 96%)]  Loss: 3.641 (3.80)  Time: 0.784s, 1305.91/s  (0.797s, 1285.51/s)  LR: 6.079e-04  Data: 0.011 (0.013)
Train: 130 [1250/1251 (100%)]  Loss: 3.728 (3.80)  Time: 0.770s, 1329.95/s  (0.796s, 1285.93/s)  LR: 6.079e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.609 (1.609)  Loss:  1.1575 (1.1575)  Acc@1: 86.8164 (86.8164)  Acc@5: 95.8008 (95.8008)
Test: [  48/48]  Time: 0.172 (0.564)  Loss:  1.0804 (1.5861)  Acc@1: 82.7830 (71.2140)  Acc@5: 95.0472 (90.2940)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-128.pth.tar', 71.71800006347657)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-129.pth.tar', 71.47000016845703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-126.pth.tar', 71.38999991210937)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-124.pth.tar', 71.3380001147461)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-130.pth.tar', 71.21400001953126)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-123.pth.tar', 71.15400006835938)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-109.pth.tar', 71.0599999633789)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-120.pth.tar', 70.99200004638672)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-115.pth.tar', 70.98600002197266)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-127.pth.tar', 70.93999999267578)

Train: 131 [   0/1251 (  0%)]  Loss: 4.081 (4.08)  Time: 2.290s,  447.20/s  (2.290s,  447.20/s)  LR: 6.028e-04  Data: 1.556 (1.556)
Train: 131 [  50/1251 (  4%)]  Loss: 3.395 (3.74)  Time: 0.823s, 1244.54/s  (0.833s, 1229.06/s)  LR: 6.028e-04  Data: 0.011 (0.047)
Train: 131 [ 100/1251 (  8%)]  Loss: 3.963 (3.81)  Time: 0.782s, 1309.56/s  (0.816s, 1254.72/s)  LR: 6.028e-04  Data: 0.011 (0.030)
Train: 131 [ 150/1251 ( 12%)]  Loss: 3.975 (3.85)  Time: 0.820s, 1248.81/s  (0.809s, 1266.31/s)  LR: 6.028e-04  Data: 0.013 (0.024)
Train: 131 [ 200/1251 ( 16%)]  Loss: 4.043 (3.89)  Time: 0.833s, 1229.30/s  (0.805s, 1271.87/s)  LR: 6.028e-04  Data: 0.011 (0.021)
Train: 131 [ 250/1251 ( 20%)]  Loss: 3.691 (3.86)  Time: 0.777s, 1317.27/s  (0.804s, 1273.29/s)  LR: 6.028e-04  Data: 0.011 (0.019)
Train: 131 [ 300/1251 ( 24%)]  Loss: 3.684 (3.83)  Time: 0.837s, 1223.76/s  (0.803s, 1274.88/s)  LR: 6.028e-04  Data: 0.012 (0.018)
Train: 131 [ 350/1251 ( 28%)]  Loss: 3.589 (3.80)  Time: 0.776s, 1319.48/s  (0.803s, 1274.43/s)  LR: 6.028e-04  Data: 0.011 (0.017)
Train: 131 [ 400/1251 ( 32%)]  Loss: 3.899 (3.81)  Time: 0.778s, 1317.03/s  (0.804s, 1273.27/s)  LR: 6.028e-04  Data: 0.011 (0.016)
Train: 131 [ 450/1251 ( 36%)]  Loss: 3.618 (3.79)  Time: 0.783s, 1307.73/s  (0.803s, 1274.79/s)  LR: 6.028e-04  Data: 0.011 (0.016)
Train: 131 [ 500/1251 ( 40%)]  Loss: 3.756 (3.79)  Time: 0.798s, 1283.45/s  (0.802s, 1276.13/s)  LR: 6.028e-04  Data: 0.011 (0.015)
Train: 131 [ 550/1251 ( 44%)]  Loss: 3.797 (3.79)  Time: 0.781s, 1311.40/s  (0.802s, 1276.47/s)  LR: 6.028e-04  Data: 0.011 (0.015)
Train: 131 [ 600/1251 ( 48%)]  Loss: 3.815 (3.79)  Time: 0.803s, 1274.44/s  (0.801s, 1278.47/s)  LR: 6.028e-04  Data: 0.011 (0.015)
Train: 131 [ 650/1251 ( 52%)]  Loss: 3.783 (3.79)  Time: 0.789s, 1298.60/s  (0.800s, 1279.80/s)  LR: 6.028e-04  Data: 0.011 (0.014)
Train: 131 [ 700/1251 ( 56%)]  Loss: 4.210 (3.82)  Time: 0.778s, 1316.02/s  (0.799s, 1281.21/s)  LR: 6.028e-04  Data: 0.011 (0.014)
Train: 131 [ 750/1251 ( 60%)]  Loss: 3.807 (3.82)  Time: 0.810s, 1264.35/s  (0.799s, 1281.91/s)  LR: 6.028e-04  Data: 0.014 (0.014)
Train: 131 [ 800/1251 ( 64%)]  Loss: 3.479 (3.80)  Time: 0.792s, 1293.66/s  (0.798s, 1282.41/s)  LR: 6.028e-04  Data: 0.011 (0.014)
Train: 131 [ 850/1251 ( 68%)]  Loss: 4.199 (3.82)  Time: 0.787s, 1300.63/s  (0.798s, 1282.80/s)  LR: 6.028e-04  Data: 0.015 (0.014)
Train: 131 [ 900/1251 ( 72%)]  Loss: 3.664 (3.81)  Time: 0.779s, 1314.51/s  (0.798s, 1283.30/s)  LR: 6.028e-04  Data: 0.011 (0.014)
Train: 131 [ 950/1251 ( 76%)]  Loss: 3.649 (3.80)  Time: 0.865s, 1184.09/s  (0.798s, 1283.54/s)  LR: 6.028e-04  Data: 0.011 (0.013)
Train: 131 [1000/1251 ( 80%)]  Loss: 3.745 (3.80)  Time: 0.780s, 1312.47/s  (0.798s, 1283.60/s)  LR: 6.028e-04  Data: 0.011 (0.013)
Train: 131 [1050/1251 ( 84%)]  Loss: 3.730 (3.80)  Time: 0.789s, 1298.40/s  (0.798s, 1283.77/s)  LR: 6.028e-04  Data: 0.011 (0.013)
Train: 131 [1100/1251 ( 88%)]  Loss: 3.790 (3.80)  Time: 0.897s, 1141.91/s  (0.798s, 1283.55/s)  LR: 6.028e-04  Data: 0.011 (0.013)
Train: 131 [1150/1251 ( 92%)]  Loss: 4.094 (3.81)  Time: 0.798s, 1283.20/s  (0.798s, 1283.00/s)  LR: 6.028e-04  Data: 0.011 (0.013)
Train: 131 [1200/1251 ( 96%)]  Loss: 4.054 (3.82)  Time: 0.789s, 1298.18/s  (0.798s, 1282.88/s)  LR: 6.028e-04  Data: 0.011 (0.013)
Train: 131 [1250/1251 (100%)]  Loss: 3.972 (3.83)  Time: 0.804s, 1274.26/s  (0.798s, 1283.34/s)  LR: 6.028e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.517 (1.517)  Loss:  0.8643 (0.8643)  Acc@1: 87.1094 (87.1094)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.172 (0.561)  Loss:  0.9013 (1.4695)  Acc@1: 83.8443 (71.4820)  Acc@5: 95.5189 (90.3600)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-128.pth.tar', 71.71800006347657)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-131.pth.tar', 71.48200009277343)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-129.pth.tar', 71.47000016845703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-126.pth.tar', 71.38999991210937)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-124.pth.tar', 71.3380001147461)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-130.pth.tar', 71.21400001953126)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-123.pth.tar', 71.15400006835938)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-109.pth.tar', 71.0599999633789)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-120.pth.tar', 70.99200004638672)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-115.pth.tar', 70.98600002197266)

Train: 132 [   0/1251 (  0%)]  Loss: 4.007 (4.01)  Time: 2.328s,  439.93/s  (2.328s,  439.93/s)  LR: 5.978e-04  Data: 1.550 (1.550)
Train: 132 [  50/1251 (  4%)]  Loss: 3.758 (3.88)  Time: 0.819s, 1250.49/s  (0.833s, 1229.35/s)  LR: 5.978e-04  Data: 0.011 (0.045)
Train: 132 [ 100/1251 (  8%)]  Loss: 3.816 (3.86)  Time: 0.827s, 1238.25/s  (0.814s, 1258.33/s)  LR: 5.978e-04  Data: 0.011 (0.028)
Train: 132 [ 150/1251 ( 12%)]  Loss: 3.646 (3.81)  Time: 0.775s, 1320.95/s  (0.809s, 1265.80/s)  LR: 5.978e-04  Data: 0.011 (0.023)
Train: 132 [ 200/1251 ( 16%)]  Loss: 3.829 (3.81)  Time: 0.804s, 1273.09/s  (0.806s, 1270.98/s)  LR: 5.978e-04  Data: 0.011 (0.020)
Train: 132 [ 250/1251 ( 20%)]  Loss: 3.633 (3.78)  Time: 0.798s, 1282.86/s  (0.803s, 1274.95/s)  LR: 5.978e-04  Data: 0.011 (0.018)
Train: 132 [ 300/1251 ( 24%)]  Loss: 4.002 (3.81)  Time: 0.778s, 1315.52/s  (0.801s, 1277.84/s)  LR: 5.978e-04  Data: 0.011 (0.017)
Train: 132 [ 350/1251 ( 28%)]  Loss: 3.699 (3.80)  Time: 0.791s, 1294.96/s  (0.800s, 1280.56/s)  LR: 5.978e-04  Data: 0.014 (0.016)
Train: 132 [ 400/1251 ( 32%)]  Loss: 3.614 (3.78)  Time: 0.779s, 1314.41/s  (0.800s, 1280.63/s)  LR: 5.978e-04  Data: 0.011 (0.016)
Train: 132 [ 450/1251 ( 36%)]  Loss: 3.727 (3.77)  Time: 0.781s, 1311.79/s  (0.799s, 1281.95/s)  LR: 5.978e-04  Data: 0.011 (0.015)
Train: 132 [ 500/1251 ( 40%)]  Loss: 3.762 (3.77)  Time: 0.804s, 1273.63/s  (0.798s, 1282.53/s)  LR: 5.978e-04  Data: 0.013 (0.015)
Train: 132 [ 550/1251 ( 44%)]  Loss: 3.620 (3.76)  Time: 0.822s, 1246.06/s  (0.798s, 1282.46/s)  LR: 5.978e-04  Data: 0.011 (0.015)
Train: 132 [ 600/1251 ( 48%)]  Loss: 3.556 (3.74)  Time: 0.809s, 1266.22/s  (0.798s, 1282.59/s)  LR: 5.978e-04  Data: 0.017 (0.014)
Train: 132 [ 650/1251 ( 52%)]  Loss: 3.981 (3.76)  Time: 0.838s, 1222.36/s  (0.798s, 1282.53/s)  LR: 5.978e-04  Data: 0.014 (0.014)
Train: 132 [ 700/1251 ( 56%)]  Loss: 4.100 (3.78)  Time: 0.774s, 1323.73/s  (0.798s, 1282.76/s)  LR: 5.978e-04  Data: 0.010 (0.014)
Train: 132 [ 750/1251 ( 60%)]  Loss: 3.849 (3.79)  Time: 0.776s, 1319.90/s  (0.798s, 1282.60/s)  LR: 5.978e-04  Data: 0.011 (0.014)
Train: 132 [ 800/1251 ( 64%)]  Loss: 3.536 (3.77)  Time: 0.853s, 1200.63/s  (0.798s, 1282.73/s)  LR: 5.978e-04  Data: 0.011 (0.014)
Train: 132 [ 850/1251 ( 68%)]  Loss: 3.721 (3.77)  Time: 0.778s, 1315.67/s  (0.798s, 1282.94/s)  LR: 5.978e-04  Data: 0.011 (0.013)
Train: 132 [ 900/1251 ( 72%)]  Loss: 3.909 (3.78)  Time: 0.778s, 1316.09/s  (0.798s, 1283.34/s)  LR: 5.978e-04  Data: 0.011 (0.013)
Train: 132 [ 950/1251 ( 76%)]  Loss: 3.879 (3.78)  Time: 0.830s, 1233.40/s  (0.798s, 1283.17/s)  LR: 5.978e-04  Data: 0.011 (0.013)
Train: 132 [1000/1251 ( 80%)]  Loss: 3.779 (3.78)  Time: 0.779s, 1313.90/s  (0.798s, 1283.68/s)  LR: 5.978e-04  Data: 0.011 (0.013)
Train: 132 [1050/1251 ( 84%)]  Loss: 3.621 (3.77)  Time: 0.802s, 1276.93/s  (0.798s, 1283.94/s)  LR: 5.978e-04  Data: 0.011 (0.013)
Train: 132 [1100/1251 ( 88%)]  Loss: 3.929 (3.78)  Time: 0.790s, 1295.80/s  (0.797s, 1284.41/s)  LR: 5.978e-04  Data: 0.013 (0.013)
Train: 132 [1150/1251 ( 92%)]  Loss: 3.844 (3.78)  Time: 0.793s, 1290.81/s  (0.797s, 1284.43/s)  LR: 5.978e-04  Data: 0.011 (0.013)
Train: 132 [1200/1251 ( 96%)]  Loss: 3.775 (3.78)  Time: 0.786s, 1302.39/s  (0.797s, 1284.66/s)  LR: 5.978e-04  Data: 0.015 (0.013)
Train: 132 [1250/1251 (100%)]  Loss: 3.523 (3.77)  Time: 0.798s, 1283.51/s  (0.797s, 1284.66/s)  LR: 5.978e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.586 (1.586)  Loss:  0.9120 (0.9120)  Acc@1: 87.8906 (87.8906)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.172 (0.568)  Loss:  0.8965 (1.4699)  Acc@1: 84.6698 (71.7980)  Acc@5: 95.5189 (90.6680)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-132.pth.tar', 71.79800006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-128.pth.tar', 71.71800006347657)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-131.pth.tar', 71.48200009277343)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-129.pth.tar', 71.47000016845703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-126.pth.tar', 71.38999991210937)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-124.pth.tar', 71.3380001147461)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-130.pth.tar', 71.21400001953126)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-123.pth.tar', 71.15400006835938)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-109.pth.tar', 71.0599999633789)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-120.pth.tar', 70.99200004638672)

Train: 133 [   0/1251 (  0%)]  Loss: 3.738 (3.74)  Time: 2.291s,  446.95/s  (2.291s,  446.95/s)  LR: 5.927e-04  Data: 1.552 (1.552)
Train: 133 [  50/1251 (  4%)]  Loss: 3.672 (3.70)  Time: 0.822s, 1245.83/s  (0.827s, 1238.55/s)  LR: 5.927e-04  Data: 0.011 (0.043)
Train: 133 [ 100/1251 (  8%)]  Loss: 3.992 (3.80)  Time: 0.790s, 1296.65/s  (0.816s, 1254.85/s)  LR: 5.927e-04  Data: 0.011 (0.028)
Train: 133 [ 150/1251 ( 12%)]  Loss: 3.536 (3.73)  Time: 0.775s, 1321.04/s  (0.808s, 1267.05/s)  LR: 5.927e-04  Data: 0.011 (0.022)
Train: 133 [ 200/1251 ( 16%)]  Loss: 3.564 (3.70)  Time: 0.803s, 1275.95/s  (0.804s, 1272.88/s)  LR: 5.927e-04  Data: 0.010 (0.019)
Train: 133 [ 250/1251 ( 20%)]  Loss: 3.811 (3.72)  Time: 0.837s, 1222.94/s  (0.803s, 1275.21/s)  LR: 5.927e-04  Data: 0.011 (0.018)
Train: 133 [ 300/1251 ( 24%)]  Loss: 3.980 (3.76)  Time: 0.782s, 1309.03/s  (0.801s, 1278.46/s)  LR: 5.927e-04  Data: 0.011 (0.017)
Train: 133 [ 350/1251 ( 28%)]  Loss: 3.061 (3.67)  Time: 0.783s, 1307.76/s  (0.800s, 1280.06/s)  LR: 5.927e-04  Data: 0.013 (0.016)
Train: 133 [ 400/1251 ( 32%)]  Loss: 3.586 (3.66)  Time: 0.809s, 1266.39/s  (0.799s, 1281.95/s)  LR: 5.927e-04  Data: 0.011 (0.016)
Train: 133 [ 450/1251 ( 36%)]  Loss: 3.440 (3.64)  Time: 0.791s, 1293.82/s  (0.798s, 1283.21/s)  LR: 5.927e-04  Data: 0.011 (0.015)
Train: 133 [ 500/1251 ( 40%)]  Loss: 3.642 (3.64)  Time: 0.804s, 1274.02/s  (0.798s, 1283.22/s)  LR: 5.927e-04  Data: 0.011 (0.015)
Train: 133 [ 550/1251 ( 44%)]  Loss: 3.706 (3.64)  Time: 0.801s, 1278.01/s  (0.797s, 1284.07/s)  LR: 5.927e-04  Data: 0.011 (0.014)
Train: 133 [ 600/1251 ( 48%)]  Loss: 3.631 (3.64)  Time: 0.776s, 1319.23/s  (0.797s, 1284.28/s)  LR: 5.927e-04  Data: 0.011 (0.014)
Train: 133 [ 650/1251 ( 52%)]  Loss: 3.679 (3.65)  Time: 0.780s, 1313.11/s  (0.797s, 1284.80/s)  LR: 5.927e-04  Data: 0.011 (0.014)
Train: 133 [ 700/1251 ( 56%)]  Loss: 4.055 (3.67)  Time: 0.832s, 1230.37/s  (0.797s, 1284.69/s)  LR: 5.927e-04  Data: 0.011 (0.014)
Train: 133 [ 750/1251 ( 60%)]  Loss: 3.514 (3.66)  Time: 0.783s, 1307.55/s  (0.797s, 1284.40/s)  LR: 5.927e-04  Data: 0.016 (0.014)
Train: 133 [ 800/1251 ( 64%)]  Loss: 3.887 (3.68)  Time: 0.834s, 1227.42/s  (0.797s, 1284.40/s)  LR: 5.927e-04  Data: 0.011 (0.014)
Train: 133 [ 850/1251 ( 68%)]  Loss: 3.790 (3.68)  Time: 0.818s, 1251.76/s  (0.797s, 1284.83/s)  LR: 5.927e-04  Data: 0.011 (0.013)
Train: 133 [ 900/1251 ( 72%)]  Loss: 3.435 (3.67)  Time: 0.798s, 1282.77/s  (0.797s, 1284.95/s)  LR: 5.927e-04  Data: 0.011 (0.013)
Train: 133 [ 950/1251 ( 76%)]  Loss: 4.098 (3.69)  Time: 0.836s, 1225.55/s  (0.797s, 1284.15/s)  LR: 5.927e-04  Data: 0.011 (0.013)
Train: 133 [1000/1251 ( 80%)]  Loss: 3.958 (3.70)  Time: 0.805s, 1272.50/s  (0.797s, 1284.62/s)  LR: 5.927e-04  Data: 0.011 (0.013)
Train: 133 [1050/1251 ( 84%)]  Loss: 4.027 (3.72)  Time: 0.775s, 1320.97/s  (0.797s, 1284.21/s)  LR: 5.927e-04  Data: 0.011 (0.013)
Train: 133 [1100/1251 ( 88%)]  Loss: 3.739 (3.72)  Time: 0.811s, 1262.40/s  (0.797s, 1284.43/s)  LR: 5.927e-04  Data: 0.011 (0.013)
Train: 133 [1150/1251 ( 92%)]  Loss: 3.720 (3.72)  Time: 0.783s, 1307.89/s  (0.797s, 1284.65/s)  LR: 5.927e-04  Data: 0.011 (0.013)
Train: 133 [1200/1251 ( 96%)]  Loss: 3.797 (3.72)  Time: 0.787s, 1301.37/s  (0.797s, 1284.60/s)  LR: 5.927e-04  Data: 0.014 (0.013)
Train: 133 [1250/1251 (100%)]  Loss: 3.927 (3.73)  Time: 0.769s, 1331.83/s  (0.797s, 1285.15/s)  LR: 5.927e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.502 (1.502)  Loss:  0.9932 (0.9932)  Acc@1: 86.1328 (86.1328)  Acc@5: 96.0938 (96.0938)
Test: [  48/48]  Time: 0.172 (0.565)  Loss:  1.0748 (1.5378)  Acc@1: 83.3726 (71.2380)  Acc@5: 95.6368 (90.3660)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-132.pth.tar', 71.79800006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-128.pth.tar', 71.71800006347657)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-131.pth.tar', 71.48200009277343)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-129.pth.tar', 71.47000016845703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-126.pth.tar', 71.38999991210937)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-124.pth.tar', 71.3380001147461)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-133.pth.tar', 71.23800001708985)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-130.pth.tar', 71.21400001953126)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-123.pth.tar', 71.15400006835938)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-109.pth.tar', 71.0599999633789)

Train: 134 [   0/1251 (  0%)]  Loss: 3.883 (3.88)  Time: 2.491s,  411.09/s  (2.491s,  411.09/s)  LR: 5.876e-04  Data: 1.731 (1.731)
Train: 134 [  50/1251 (  4%)]  Loss: 3.807 (3.84)  Time: 0.777s, 1317.46/s  (0.830s, 1233.92/s)  LR: 5.876e-04  Data: 0.011 (0.047)
Train: 134 [ 100/1251 (  8%)]  Loss: 4.078 (3.92)  Time: 0.793s, 1291.60/s  (0.811s, 1262.19/s)  LR: 5.876e-04  Data: 0.011 (0.029)
Train: 134 [ 150/1251 ( 12%)]  Loss: 3.963 (3.93)  Time: 0.858s, 1194.15/s  (0.809s, 1266.34/s)  LR: 5.876e-04  Data: 0.014 (0.023)
Train: 134 [ 200/1251 ( 16%)]  Loss: 3.702 (3.89)  Time: 0.801s, 1277.81/s  (0.805s, 1272.68/s)  LR: 5.876e-04  Data: 0.012 (0.020)
Train: 134 [ 250/1251 ( 20%)]  Loss: 3.930 (3.89)  Time: 0.776s, 1319.01/s  (0.802s, 1276.19/s)  LR: 5.876e-04  Data: 0.011 (0.019)
Train: 134 [ 300/1251 ( 24%)]  Loss: 3.810 (3.88)  Time: 0.780s, 1313.12/s  (0.802s, 1277.41/s)  LR: 5.876e-04  Data: 0.011 (0.017)
Train: 134 [ 350/1251 ( 28%)]  Loss: 3.923 (3.89)  Time: 0.777s, 1317.90/s  (0.800s, 1279.83/s)  LR: 5.876e-04  Data: 0.011 (0.017)
Train: 134 [ 400/1251 ( 32%)]  Loss: 3.853 (3.88)  Time: 0.781s, 1311.95/s  (0.799s, 1280.87/s)  LR: 5.876e-04  Data: 0.011 (0.016)
Train: 134 [ 450/1251 ( 36%)]  Loss: 3.926 (3.89)  Time: 0.781s, 1310.89/s  (0.798s, 1282.82/s)  LR: 5.876e-04  Data: 0.013 (0.015)
Train: 134 [ 500/1251 ( 40%)]  Loss: 3.614 (3.86)  Time: 0.779s, 1315.12/s  (0.798s, 1283.80/s)  LR: 5.876e-04  Data: 0.011 (0.015)
Train: 134 [ 550/1251 ( 44%)]  Loss: 3.668 (3.85)  Time: 0.778s, 1315.54/s  (0.797s, 1285.55/s)  LR: 5.876e-04  Data: 0.011 (0.015)
Train: 134 [ 600/1251 ( 48%)]  Loss: 3.871 (3.85)  Time: 0.777s, 1318.21/s  (0.796s, 1285.85/s)  LR: 5.876e-04  Data: 0.012 (0.014)
Train: 134 [ 650/1251 ( 52%)]  Loss: 3.594 (3.83)  Time: 0.776s, 1320.21/s  (0.796s, 1286.09/s)  LR: 5.876e-04  Data: 0.011 (0.014)
Train: 134 [ 700/1251 ( 56%)]  Loss: 3.879 (3.83)  Time: 0.788s, 1299.73/s  (0.796s, 1286.47/s)  LR: 5.876e-04  Data: 0.012 (0.014)
Train: 134 [ 750/1251 ( 60%)]  Loss: 3.851 (3.83)  Time: 0.786s, 1303.34/s  (0.796s, 1286.55/s)  LR: 5.876e-04  Data: 0.014 (0.014)
Train: 134 [ 800/1251 ( 64%)]  Loss: 4.135 (3.85)  Time: 0.776s, 1319.28/s  (0.796s, 1286.98/s)  LR: 5.876e-04  Data: 0.011 (0.014)
Train: 134 [ 850/1251 ( 68%)]  Loss: 3.758 (3.85)  Time: 0.776s, 1320.01/s  (0.796s, 1286.97/s)  LR: 5.876e-04  Data: 0.011 (0.014)
Train: 134 [ 900/1251 ( 72%)]  Loss: 3.901 (3.85)  Time: 0.805s, 1272.73/s  (0.796s, 1287.18/s)  LR: 5.876e-04  Data: 0.011 (0.013)
Train: 134 [ 950/1251 ( 76%)]  Loss: 3.728 (3.84)  Time: 0.780s, 1312.34/s  (0.796s, 1287.16/s)  LR: 5.876e-04  Data: 0.011 (0.013)
Train: 134 [1000/1251 ( 80%)]  Loss: 4.206 (3.86)  Time: 0.861s, 1189.60/s  (0.796s, 1287.11/s)  LR: 5.876e-04  Data: 0.011 (0.013)
Train: 134 [1050/1251 ( 84%)]  Loss: 4.122 (3.87)  Time: 0.778s, 1315.80/s  (0.796s, 1287.15/s)  LR: 5.876e-04  Data: 0.011 (0.013)
Train: 134 [1100/1251 ( 88%)]  Loss: 3.660 (3.86)  Time: 0.778s, 1315.86/s  (0.796s, 1286.92/s)  LR: 5.876e-04  Data: 0.011 (0.013)
Train: 134 [1150/1251 ( 92%)]  Loss: 3.850 (3.86)  Time: 0.779s, 1314.44/s  (0.796s, 1287.02/s)  LR: 5.876e-04  Data: 0.011 (0.013)
Train: 134 [1200/1251 ( 96%)]  Loss: 4.070 (3.87)  Time: 0.780s, 1313.50/s  (0.796s, 1286.98/s)  LR: 5.876e-04  Data: 0.011 (0.013)
Train: 134 [1250/1251 (100%)]  Loss: 3.427 (3.85)  Time: 0.808s, 1266.63/s  (0.796s, 1286.97/s)  LR: 5.876e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.629 (1.629)  Loss:  0.8838 (0.8838)  Acc@1: 87.5977 (87.5977)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.172 (0.561)  Loss:  1.1002 (1.5287)  Acc@1: 81.3679 (71.2180)  Acc@5: 95.2830 (90.4560)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-132.pth.tar', 71.79800006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-128.pth.tar', 71.71800006347657)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-131.pth.tar', 71.48200009277343)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-129.pth.tar', 71.47000016845703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-126.pth.tar', 71.38999991210937)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-124.pth.tar', 71.3380001147461)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-133.pth.tar', 71.23800001708985)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-134.pth.tar', 71.21800005126953)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-130.pth.tar', 71.21400001953126)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-123.pth.tar', 71.15400006835938)

Train: 135 [   0/1251 (  0%)]  Loss: 3.729 (3.73)  Time: 2.346s,  436.52/s  (2.346s,  436.52/s)  LR: 5.824e-04  Data: 1.610 (1.610)
Train: 135 [  50/1251 (  4%)]  Loss: 3.806 (3.77)  Time: 0.777s, 1318.42/s  (0.835s, 1226.22/s)  LR: 5.824e-04  Data: 0.011 (0.053)
Train: 135 [ 100/1251 (  8%)]  Loss: 3.638 (3.72)  Time: 0.780s, 1312.70/s  (0.819s, 1250.83/s)  LR: 5.824e-04  Data: 0.011 (0.032)
Train: 135 [ 150/1251 ( 12%)]  Loss: 4.030 (3.80)  Time: 0.777s, 1317.59/s  (0.811s, 1262.96/s)  LR: 5.824e-04  Data: 0.011 (0.025)
Train: 135 [ 200/1251 ( 16%)]  Loss: 3.807 (3.80)  Time: 0.837s, 1223.90/s  (0.807s, 1268.72/s)  LR: 5.824e-04  Data: 0.011 (0.022)
Train: 135 [ 250/1251 ( 20%)]  Loss: 3.766 (3.80)  Time: 0.798s, 1283.17/s  (0.806s, 1270.76/s)  LR: 5.824e-04  Data: 0.011 (0.020)
Train: 135 [ 300/1251 ( 24%)]  Loss: 3.774 (3.79)  Time: 0.780s, 1313.61/s  (0.804s, 1274.09/s)  LR: 5.824e-04  Data: 0.011 (0.018)
Train: 135 [ 350/1251 ( 28%)]  Loss: 3.681 (3.78)  Time: 0.821s, 1247.53/s  (0.802s, 1277.04/s)  LR: 5.824e-04  Data: 0.012 (0.017)
Train: 135 [ 400/1251 ( 32%)]  Loss: 3.780 (3.78)  Time: 0.778s, 1315.59/s  (0.800s, 1279.58/s)  LR: 5.824e-04  Data: 0.011 (0.017)
Train: 135 [ 450/1251 ( 36%)]  Loss: 3.974 (3.80)  Time: 0.780s, 1313.02/s  (0.801s, 1278.82/s)  LR: 5.824e-04  Data: 0.011 (0.016)
Train: 135 [ 500/1251 ( 40%)]  Loss: 3.831 (3.80)  Time: 0.779s, 1313.86/s  (0.799s, 1281.38/s)  LR: 5.824e-04  Data: 0.011 (0.016)
Train: 135 [ 550/1251 ( 44%)]  Loss: 3.737 (3.80)  Time: 0.784s, 1306.84/s  (0.799s, 1281.25/s)  LR: 5.824e-04  Data: 0.011 (0.015)
Train: 135 [ 600/1251 ( 48%)]  Loss: 3.698 (3.79)  Time: 0.808s, 1267.67/s  (0.799s, 1282.31/s)  LR: 5.824e-04  Data: 0.011 (0.015)
Train: 135 [ 650/1251 ( 52%)]  Loss: 3.578 (3.77)  Time: 0.775s, 1320.74/s  (0.798s, 1282.79/s)  LR: 5.824e-04  Data: 0.011 (0.015)
Train: 135 [ 700/1251 ( 56%)]  Loss: 4.041 (3.79)  Time: 0.802s, 1277.11/s  (0.798s, 1283.23/s)  LR: 5.824e-04  Data: 0.011 (0.014)
Train: 135 [ 750/1251 ( 60%)]  Loss: 3.907 (3.80)  Time: 0.887s, 1154.32/s  (0.798s, 1282.82/s)  LR: 5.824e-04  Data: 0.014 (0.014)
Train: 135 [ 800/1251 ( 64%)]  Loss: 3.692 (3.79)  Time: 0.784s, 1306.89/s  (0.798s, 1283.20/s)  LR: 5.824e-04  Data: 0.011 (0.014)
Train: 135 [ 850/1251 ( 68%)]  Loss: 3.635 (3.78)  Time: 0.779s, 1315.20/s  (0.798s, 1283.58/s)  LR: 5.824e-04  Data: 0.011 (0.014)
Train: 135 [ 900/1251 ( 72%)]  Loss: 4.001 (3.79)  Time: 0.822s, 1245.61/s  (0.798s, 1283.85/s)  LR: 5.824e-04  Data: 0.011 (0.014)
Train: 135 [ 950/1251 ( 76%)]  Loss: 3.608 (3.79)  Time: 0.778s, 1315.71/s  (0.797s, 1284.50/s)  LR: 5.824e-04  Data: 0.011 (0.014)
Train: 135 [1000/1251 ( 80%)]  Loss: 3.620 (3.78)  Time: 0.778s, 1316.58/s  (0.797s, 1284.63/s)  LR: 5.824e-04  Data: 0.011 (0.014)
Train: 135 [1050/1251 ( 84%)]  Loss: 3.414 (3.76)  Time: 0.776s, 1319.90/s  (0.797s, 1284.63/s)  LR: 5.824e-04  Data: 0.011 (0.013)
Train: 135 [1100/1251 ( 88%)]  Loss: 3.764 (3.76)  Time: 0.804s, 1273.92/s  (0.797s, 1284.96/s)  LR: 5.824e-04  Data: 0.011 (0.013)
Train: 135 [1150/1251 ( 92%)]  Loss: 3.422 (3.75)  Time: 0.779s, 1314.15/s  (0.797s, 1285.44/s)  LR: 5.824e-04  Data: 0.011 (0.013)
Train: 135 [1200/1251 ( 96%)]  Loss: 3.647 (3.74)  Time: 0.803s, 1275.02/s  (0.796s, 1285.66/s)  LR: 5.824e-04  Data: 0.011 (0.013)
Train: 135 [1250/1251 (100%)]  Loss: 3.607 (3.74)  Time: 0.770s, 1330.68/s  (0.796s, 1286.19/s)  LR: 5.824e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.589 (1.589)  Loss:  1.0477 (1.0477)  Acc@1: 86.1328 (86.1328)  Acc@5: 95.9961 (95.9961)
Test: [  48/48]  Time: 0.172 (0.562)  Loss:  1.0632 (1.5953)  Acc@1: 83.6085 (70.5080)  Acc@5: 95.1651 (89.8940)
Train: 136 [   0/1251 (  0%)]  Loss: 3.815 (3.81)  Time: 2.396s,  427.42/s  (2.396s,  427.42/s)  LR: 5.773e-04  Data: 1.606 (1.606)
Train: 136 [  50/1251 (  4%)]  Loss: 3.609 (3.71)  Time: 0.822s, 1245.87/s  (0.831s, 1231.82/s)  LR: 5.773e-04  Data: 0.011 (0.046)
Train: 136 [ 100/1251 (  8%)]  Loss: 3.584 (3.67)  Time: 0.779s, 1314.60/s  (0.813s, 1259.63/s)  LR: 5.773e-04  Data: 0.011 (0.029)
Train: 136 [ 150/1251 ( 12%)]  Loss: 4.036 (3.76)  Time: 0.831s, 1231.93/s  (0.811s, 1262.52/s)  LR: 5.773e-04  Data: 0.011 (0.023)
Train: 136 [ 200/1251 ( 16%)]  Loss: 3.773 (3.76)  Time: 0.780s, 1313.19/s  (0.804s, 1273.75/s)  LR: 5.773e-04  Data: 0.011 (0.020)
Train: 136 [ 250/1251 ( 20%)]  Loss: 4.084 (3.82)  Time: 0.778s, 1316.15/s  (0.799s, 1281.75/s)  LR: 5.773e-04  Data: 0.011 (0.018)
Train: 136 [ 300/1251 ( 24%)]  Loss: 3.725 (3.80)  Time: 0.779s, 1314.87/s  (0.795s, 1287.48/s)  LR: 5.773e-04  Data: 0.011 (0.017)
Train: 136 [ 350/1251 ( 28%)]  Loss: 3.555 (3.77)  Time: 0.776s, 1319.77/s  (0.793s, 1291.42/s)  LR: 5.773e-04  Data: 0.011 (0.016)
Train: 136 [ 400/1251 ( 32%)]  Loss: 4.042 (3.80)  Time: 0.812s, 1261.30/s  (0.793s, 1291.20/s)  LR: 5.773e-04  Data: 0.011 (0.016)
Train: 136 [ 450/1251 ( 36%)]  Loss: 3.684 (3.79)  Time: 0.776s, 1319.14/s  (0.792s, 1292.70/s)  LR: 5.773e-04  Data: 0.012 (0.015)
Train: 136 [ 500/1251 ( 40%)]  Loss: 3.716 (3.78)  Time: 0.778s, 1316.40/s  (0.791s, 1294.76/s)  LR: 5.773e-04  Data: 0.011 (0.015)
Train: 136 [ 550/1251 ( 44%)]  Loss: 3.718 (3.78)  Time: 0.811s, 1262.11/s  (0.792s, 1292.40/s)  LR: 5.773e-04  Data: 0.011 (0.014)
Train: 136 [ 600/1251 ( 48%)]  Loss: 3.591 (3.76)  Time: 0.810s, 1264.92/s  (0.794s, 1289.70/s)  LR: 5.773e-04  Data: 0.011 (0.014)
Train: 136 [ 650/1251 ( 52%)]  Loss: 3.678 (3.76)  Time: 0.809s, 1265.48/s  (0.795s, 1287.61/s)  LR: 5.773e-04  Data: 0.011 (0.014)
Train: 136 [ 700/1251 ( 56%)]  Loss: 3.762 (3.76)  Time: 0.813s, 1260.01/s  (0.796s, 1285.72/s)  LR: 5.773e-04  Data: 0.011 (0.014)
Train: 136 [ 750/1251 ( 60%)]  Loss: 3.800 (3.76)  Time: 0.813s, 1259.06/s  (0.797s, 1284.17/s)  LR: 5.773e-04  Data: 0.011 (0.014)
Train: 136 [ 800/1251 ( 64%)]  Loss: 3.875 (3.77)  Time: 0.778s, 1315.66/s  (0.798s, 1283.63/s)  LR: 5.773e-04  Data: 0.011 (0.013)
Train: 136 [ 850/1251 ( 68%)]  Loss: 3.912 (3.78)  Time: 0.816s, 1254.42/s  (0.798s, 1283.90/s)  LR: 5.773e-04  Data: 0.011 (0.013)
Train: 136 [ 900/1251 ( 72%)]  Loss: 3.924 (3.78)  Time: 0.811s, 1263.12/s  (0.797s, 1284.58/s)  LR: 5.773e-04  Data: 0.011 (0.013)
Train: 136 [ 950/1251 ( 76%)]  Loss: 3.923 (3.79)  Time: 0.811s, 1262.81/s  (0.798s, 1283.28/s)  LR: 5.773e-04  Data: 0.011 (0.013)
Train: 136 [1000/1251 ( 80%)]  Loss: 3.921 (3.80)  Time: 0.778s, 1316.45/s  (0.797s, 1284.38/s)  LR: 5.773e-04  Data: 0.011 (0.013)
Train: 136 [1050/1251 ( 84%)]  Loss: 3.753 (3.79)  Time: 0.776s, 1319.22/s  (0.796s, 1285.80/s)  LR: 5.773e-04  Data: 0.011 (0.013)
Train: 136 [1100/1251 ( 88%)]  Loss: 3.708 (3.79)  Time: 0.776s, 1318.80/s  (0.796s, 1287.16/s)  LR: 5.773e-04  Data: 0.011 (0.013)
Train: 136 [1150/1251 ( 92%)]  Loss: 4.070 (3.80)  Time: 0.778s, 1315.80/s  (0.795s, 1288.27/s)  LR: 5.773e-04  Data: 0.011 (0.013)
Train: 136 [1200/1251 ( 96%)]  Loss: 3.935 (3.81)  Time: 0.805s, 1271.37/s  (0.794s, 1289.30/s)  LR: 5.773e-04  Data: 0.011 (0.013)
Train: 136 [1250/1251 (100%)]  Loss: 4.218 (3.82)  Time: 0.765s, 1339.02/s  (0.794s, 1289.40/s)  LR: 5.773e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.531 (1.531)  Loss:  0.9365 (0.9365)  Acc@1: 87.1094 (87.1094)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.172 (0.559)  Loss:  1.0292 (1.5470)  Acc@1: 82.5472 (71.4720)  Acc@5: 95.0472 (90.5060)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-132.pth.tar', 71.79800006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-128.pth.tar', 71.71800006347657)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-131.pth.tar', 71.48200009277343)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-136.pth.tar', 71.47200004638673)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-129.pth.tar', 71.47000016845703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-126.pth.tar', 71.38999991210937)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-124.pth.tar', 71.3380001147461)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-133.pth.tar', 71.23800001708985)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-134.pth.tar', 71.21800005126953)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-130.pth.tar', 71.21400001953126)

Train: 137 [   0/1251 (  0%)]  Loss: 3.881 (3.88)  Time: 2.279s,  449.35/s  (2.279s,  449.35/s)  LR: 5.722e-04  Data: 1.544 (1.544)
Train: 137 [  50/1251 (  4%)]  Loss: 3.612 (3.75)  Time: 0.777s, 1317.73/s  (0.813s, 1259.05/s)  LR: 5.722e-04  Data: 0.011 (0.044)
Train: 137 [ 100/1251 (  8%)]  Loss: 3.574 (3.69)  Time: 0.777s, 1318.05/s  (0.796s, 1286.75/s)  LR: 5.722e-04  Data: 0.011 (0.028)
Train: 137 [ 150/1251 ( 12%)]  Loss: 3.743 (3.70)  Time: 0.778s, 1316.08/s  (0.790s, 1295.80/s)  LR: 5.722e-04  Data: 0.011 (0.022)
Train: 137 [ 200/1251 ( 16%)]  Loss: 3.658 (3.69)  Time: 0.777s, 1318.15/s  (0.787s, 1300.93/s)  LR: 5.722e-04  Data: 0.011 (0.019)
Train: 137 [ 250/1251 ( 20%)]  Loss: 3.847 (3.72)  Time: 0.777s, 1317.19/s  (0.785s, 1303.71/s)  LR: 5.722e-04  Data: 0.011 (0.018)
Train: 137 [ 300/1251 ( 24%)]  Loss: 4.024 (3.76)  Time: 0.777s, 1317.95/s  (0.784s, 1305.49/s)  LR: 5.722e-04  Data: 0.011 (0.017)
Train: 137 [ 350/1251 ( 28%)]  Loss: 3.593 (3.74)  Time: 0.780s, 1312.60/s  (0.784s, 1306.70/s)  LR: 5.722e-04  Data: 0.011 (0.016)
Train: 137 [ 400/1251 ( 32%)]  Loss: 3.746 (3.74)  Time: 0.818s, 1251.23/s  (0.788s, 1299.77/s)  LR: 5.722e-04  Data: 0.011 (0.015)
Train: 137 [ 450/1251 ( 36%)]  Loss: 3.337 (3.70)  Time: 0.779s, 1314.88/s  (0.790s, 1296.46/s)  LR: 5.722e-04  Data: 0.011 (0.015)
Train: 137 [ 500/1251 ( 40%)]  Loss: 4.024 (3.73)  Time: 0.779s, 1314.82/s  (0.789s, 1298.39/s)  LR: 5.722e-04  Data: 0.011 (0.014)
Train: 137 [ 550/1251 ( 44%)]  Loss: 3.676 (3.73)  Time: 0.778s, 1316.82/s  (0.788s, 1299.78/s)  LR: 5.722e-04  Data: 0.011 (0.014)
Train: 137 [ 600/1251 ( 48%)]  Loss: 3.544 (3.71)  Time: 0.777s, 1317.58/s  (0.787s, 1301.20/s)  LR: 5.722e-04  Data: 0.011 (0.014)
Train: 137 [ 650/1251 ( 52%)]  Loss: 3.813 (3.72)  Time: 0.778s, 1316.56/s  (0.787s, 1301.42/s)  LR: 5.722e-04  Data: 0.011 (0.014)
Train: 137 [ 700/1251 ( 56%)]  Loss: 3.969 (3.74)  Time: 0.777s, 1317.08/s  (0.786s, 1302.29/s)  LR: 5.722e-04  Data: 0.011 (0.013)
Train: 137 [ 750/1251 ( 60%)]  Loss: 3.510 (3.72)  Time: 0.778s, 1316.32/s  (0.786s, 1303.03/s)  LR: 5.722e-04  Data: 0.011 (0.013)
Train: 137 [ 800/1251 ( 64%)]  Loss: 3.682 (3.72)  Time: 0.777s, 1318.10/s  (0.785s, 1303.78/s)  LR: 5.722e-04  Data: 0.011 (0.013)
Train: 137 [ 850/1251 ( 68%)]  Loss: 3.622 (3.71)  Time: 0.778s, 1316.49/s  (0.785s, 1304.55/s)  LR: 5.722e-04  Data: 0.011 (0.013)
Train: 137 [ 900/1251 ( 72%)]  Loss: 3.969 (3.73)  Time: 0.777s, 1317.63/s  (0.785s, 1305.18/s)  LR: 5.722e-04  Data: 0.011 (0.013)
Train: 137 [ 950/1251 ( 76%)]  Loss: 3.776 (3.73)  Time: 0.776s, 1319.23/s  (0.785s, 1304.14/s)  LR: 5.722e-04  Data: 0.011 (0.013)
Train: 137 [1000/1251 ( 80%)]  Loss: 3.813 (3.73)  Time: 0.831s, 1232.38/s  (0.785s, 1304.41/s)  LR: 5.722e-04  Data: 0.011 (0.013)
Train: 137 [1050/1251 ( 84%)]  Loss: 4.080 (3.75)  Time: 0.778s, 1316.21/s  (0.785s, 1304.90/s)  LR: 5.722e-04  Data: 0.011 (0.013)
Train: 137 [1100/1251 ( 88%)]  Loss: 3.540 (3.74)  Time: 0.778s, 1315.80/s  (0.784s, 1305.37/s)  LR: 5.722e-04  Data: 0.011 (0.013)
Train: 137 [1150/1251 ( 92%)]  Loss: 4.047 (3.75)  Time: 0.815s, 1256.46/s  (0.784s, 1305.43/s)  LR: 5.722e-04  Data: 0.011 (0.012)
Train: 137 [1200/1251 ( 96%)]  Loss: 3.460 (3.74)  Time: 0.778s, 1316.33/s  (0.785s, 1304.28/s)  LR: 5.722e-04  Data: 0.011 (0.012)
Train: 137 [1250/1251 (100%)]  Loss: 4.174 (3.76)  Time: 0.765s, 1338.51/s  (0.785s, 1304.20/s)  LR: 5.722e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.517 (1.517)  Loss:  0.8628 (0.8628)  Acc@1: 88.6719 (88.6719)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.172 (0.553)  Loss:  1.0711 (1.5133)  Acc@1: 82.9009 (71.7080)  Acc@5: 95.8727 (90.5540)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-132.pth.tar', 71.79800006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-128.pth.tar', 71.71800006347657)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-137.pth.tar', 71.70800007080078)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-131.pth.tar', 71.48200009277343)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-136.pth.tar', 71.47200004638673)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-129.pth.tar', 71.47000016845703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-126.pth.tar', 71.38999991210937)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-124.pth.tar', 71.3380001147461)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-133.pth.tar', 71.23800001708985)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-134.pth.tar', 71.21800005126953)

Train: 138 [   0/1251 (  0%)]  Loss: 3.743 (3.74)  Time: 2.367s,  432.55/s  (2.367s,  432.55/s)  LR: 5.670e-04  Data: 1.578 (1.578)
Train: 138 [  50/1251 (  4%)]  Loss: 3.806 (3.77)  Time: 0.777s, 1317.24/s  (0.818s, 1251.53/s)  LR: 5.670e-04  Data: 0.011 (0.050)
Train: 138 [ 100/1251 (  8%)]  Loss: 3.913 (3.82)  Time: 0.778s, 1315.82/s  (0.800s, 1280.66/s)  LR: 5.670e-04  Data: 0.011 (0.031)
Train: 138 [ 150/1251 ( 12%)]  Loss: 3.365 (3.71)  Time: 0.778s, 1316.64/s  (0.793s, 1291.56/s)  LR: 5.670e-04  Data: 0.011 (0.024)
Train: 138 [ 200/1251 ( 16%)]  Loss: 3.235 (3.61)  Time: 0.778s, 1316.43/s  (0.790s, 1296.74/s)  LR: 5.670e-04  Data: 0.011 (0.021)
Train: 138 [ 250/1251 ( 20%)]  Loss: 3.532 (3.60)  Time: 0.778s, 1317.02/s  (0.788s, 1300.04/s)  LR: 5.670e-04  Data: 0.011 (0.019)
Train: 138 [ 300/1251 ( 24%)]  Loss: 3.791 (3.63)  Time: 0.777s, 1317.89/s  (0.786s, 1302.84/s)  LR: 5.670e-04  Data: 0.011 (0.018)
Train: 138 [ 350/1251 ( 28%)]  Loss: 3.642 (3.63)  Time: 0.776s, 1319.61/s  (0.785s, 1304.62/s)  LR: 5.670e-04  Data: 0.011 (0.017)
Train: 138 [ 400/1251 ( 32%)]  Loss: 3.831 (3.65)  Time: 0.776s, 1319.32/s  (0.784s, 1305.68/s)  LR: 5.670e-04  Data: 0.011 (0.016)
Train: 138 [ 450/1251 ( 36%)]  Loss: 3.751 (3.66)  Time: 0.774s, 1322.98/s  (0.784s, 1306.35/s)  LR: 5.670e-04  Data: 0.011 (0.015)
Train: 138 [ 500/1251 ( 40%)]  Loss: 3.714 (3.67)  Time: 0.776s, 1318.98/s  (0.785s, 1305.05/s)  LR: 5.670e-04  Data: 0.011 (0.015)
Train: 138 [ 550/1251 ( 44%)]  Loss: 3.818 (3.68)  Time: 0.775s, 1320.59/s  (0.784s, 1305.88/s)  LR: 5.670e-04  Data: 0.011 (0.015)
Train: 138 [ 600/1251 ( 48%)]  Loss: 3.830 (3.69)  Time: 0.778s, 1315.46/s  (0.784s, 1306.36/s)  LR: 5.670e-04  Data: 0.011 (0.014)
Train: 138 [ 650/1251 ( 52%)]  Loss: 3.471 (3.67)  Time: 0.779s, 1313.98/s  (0.783s, 1307.11/s)  LR: 5.670e-04  Data: 0.011 (0.014)
Train: 138 [ 700/1251 ( 56%)]  Loss: 3.975 (3.69)  Time: 0.811s, 1263.27/s  (0.785s, 1305.13/s)  LR: 5.670e-04  Data: 0.011 (0.014)
Train: 138 [ 750/1251 ( 60%)]  Loss: 3.637 (3.69)  Time: 0.776s, 1319.34/s  (0.785s, 1304.67/s)  LR: 5.670e-04  Data: 0.011 (0.014)
Train: 138 [ 800/1251 ( 64%)]  Loss: 3.818 (3.70)  Time: 0.810s, 1264.41/s  (0.785s, 1305.21/s)  LR: 5.670e-04  Data: 0.011 (0.014)
Train: 138 [ 850/1251 ( 68%)]  Loss: 3.981 (3.71)  Time: 0.776s, 1320.25/s  (0.785s, 1304.23/s)  LR: 5.670e-04  Data: 0.011 (0.013)
Train: 138 [ 900/1251 ( 72%)]  Loss: 3.715 (3.71)  Time: 0.813s, 1259.89/s  (0.785s, 1304.02/s)  LR: 5.670e-04  Data: 0.011 (0.013)
Train: 138 [ 950/1251 ( 76%)]  Loss: 3.943 (3.73)  Time: 0.778s, 1316.53/s  (0.786s, 1303.36/s)  LR: 5.670e-04  Data: 0.011 (0.013)
Train: 138 [1000/1251 ( 80%)]  Loss: 3.884 (3.73)  Time: 0.814s, 1257.65/s  (0.786s, 1302.11/s)  LR: 5.670e-04  Data: 0.011 (0.013)
Train: 138 [1050/1251 ( 84%)]  Loss: 4.000 (3.75)  Time: 0.777s, 1317.13/s  (0.786s, 1302.72/s)  LR: 5.670e-04  Data: 0.011 (0.013)
Train: 138 [1100/1251 ( 88%)]  Loss: 3.819 (3.75)  Time: 0.779s, 1314.06/s  (0.786s, 1303.25/s)  LR: 5.670e-04  Data: 0.011 (0.013)
Train: 138 [1150/1251 ( 92%)]  Loss: 3.902 (3.75)  Time: 0.779s, 1314.36/s  (0.785s, 1303.73/s)  LR: 5.670e-04  Data: 0.011 (0.013)
Train: 138 [1200/1251 ( 96%)]  Loss: 3.880 (3.76)  Time: 0.775s, 1321.84/s  (0.785s, 1304.05/s)  LR: 5.670e-04  Data: 0.011 (0.013)
Train: 138 [1250/1251 (100%)]  Loss: 3.933 (3.77)  Time: 0.766s, 1335.98/s  (0.785s, 1304.57/s)  LR: 5.670e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.481 (1.481)  Loss:  0.8926 (0.8926)  Acc@1: 86.7188 (86.7188)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.172 (0.560)  Loss:  0.9245 (1.5260)  Acc@1: 84.6698 (71.5060)  Acc@5: 96.5802 (90.3340)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-132.pth.tar', 71.79800006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-128.pth.tar', 71.71800006347657)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-137.pth.tar', 71.70800007080078)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-138.pth.tar', 71.50600006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-131.pth.tar', 71.48200009277343)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-136.pth.tar', 71.47200004638673)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-129.pth.tar', 71.47000016845703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-126.pth.tar', 71.38999991210937)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-124.pth.tar', 71.3380001147461)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-133.pth.tar', 71.23800001708985)

Train: 139 [   0/1251 (  0%)]  Loss: 3.721 (3.72)  Time: 2.295s,  446.13/s  (2.295s,  446.13/s)  LR: 5.619e-04  Data: 1.560 (1.560)
Train: 139 [  50/1251 (  4%)]  Loss: 3.683 (3.70)  Time: 0.824s, 1243.12/s  (0.848s, 1207.34/s)  LR: 5.619e-04  Data: 0.011 (0.046)
Train: 139 [ 100/1251 (  8%)]  Loss: 3.716 (3.71)  Time: 0.821s, 1247.85/s  (0.834s, 1228.23/s)  LR: 5.619e-04  Data: 0.011 (0.029)
Train: 139 [ 150/1251 ( 12%)]  Loss: 3.695 (3.70)  Time: 0.816s, 1254.81/s  (0.817s, 1253.51/s)  LR: 5.619e-04  Data: 0.011 (0.023)
Train: 139 [ 200/1251 ( 16%)]  Loss: 3.538 (3.67)  Time: 0.776s, 1319.02/s  (0.811s, 1263.08/s)  LR: 5.619e-04  Data: 0.011 (0.020)
Train: 139 [ 250/1251 ( 20%)]  Loss: 3.600 (3.66)  Time: 0.777s, 1317.98/s  (0.805s, 1272.75/s)  LR: 5.619e-04  Data: 0.011 (0.018)
Train: 139 [ 300/1251 ( 24%)]  Loss: 3.727 (3.67)  Time: 0.777s, 1317.93/s  (0.801s, 1279.18/s)  LR: 5.619e-04  Data: 0.011 (0.017)
Train: 139 [ 350/1251 ( 28%)]  Loss: 3.350 (3.63)  Time: 0.776s, 1320.15/s  (0.797s, 1284.45/s)  LR: 5.619e-04  Data: 0.011 (0.016)
Train: 139 [ 400/1251 ( 32%)]  Loss: 3.382 (3.60)  Time: 0.777s, 1318.64/s  (0.795s, 1287.72/s)  LR: 5.619e-04  Data: 0.011 (0.015)
Train: 139 [ 450/1251 ( 36%)]  Loss: 3.864 (3.63)  Time: 0.778s, 1315.43/s  (0.793s, 1290.54/s)  LR: 5.619e-04  Data: 0.011 (0.015)
Train: 139 [ 500/1251 ( 40%)]  Loss: 3.504 (3.62)  Time: 0.826s, 1240.39/s  (0.792s, 1292.97/s)  LR: 5.619e-04  Data: 0.011 (0.015)
Train: 139 [ 550/1251 ( 44%)]  Loss: 3.932 (3.64)  Time: 0.812s, 1261.63/s  (0.793s, 1291.63/s)  LR: 5.619e-04  Data: 0.011 (0.014)
Train: 139 [ 600/1251 ( 48%)]  Loss: 3.710 (3.65)  Time: 0.775s, 1320.54/s  (0.792s, 1293.71/s)  LR: 5.619e-04  Data: 0.011 (0.014)
Train: 139 [ 650/1251 ( 52%)]  Loss: 3.803 (3.66)  Time: 0.813s, 1260.01/s  (0.792s, 1292.86/s)  LR: 5.619e-04  Data: 0.011 (0.014)
Train: 139 [ 700/1251 ( 56%)]  Loss: 4.089 (3.69)  Time: 0.776s, 1319.45/s  (0.791s, 1294.54/s)  LR: 5.619e-04  Data: 0.011 (0.014)
Train: 139 [ 750/1251 ( 60%)]  Loss: 3.695 (3.69)  Time: 0.778s, 1316.55/s  (0.790s, 1295.84/s)  LR: 5.619e-04  Data: 0.011 (0.013)
Train: 139 [ 800/1251 ( 64%)]  Loss: 3.541 (3.68)  Time: 0.775s, 1321.09/s  (0.790s, 1296.99/s)  LR: 5.619e-04  Data: 0.011 (0.013)
Train: 139 [ 850/1251 ( 68%)]  Loss: 3.657 (3.68)  Time: 0.818s, 1252.55/s  (0.789s, 1297.76/s)  LR: 5.619e-04  Data: 0.011 (0.013)
Train: 139 [ 900/1251 ( 72%)]  Loss: 3.505 (3.67)  Time: 0.816s, 1254.25/s  (0.789s, 1297.53/s)  LR: 5.619e-04  Data: 0.011 (0.013)
Train: 139 [ 950/1251 ( 76%)]  Loss: 4.017 (3.69)  Time: 0.779s, 1315.18/s  (0.790s, 1296.08/s)  LR: 5.619e-04  Data: 0.011 (0.013)
Train: 139 [1000/1251 ( 80%)]  Loss: 3.547 (3.68)  Time: 0.810s, 1263.89/s  (0.790s, 1296.72/s)  LR: 5.619e-04  Data: 0.011 (0.013)
Train: 139 [1050/1251 ( 84%)]  Loss: 3.780 (3.68)  Time: 0.820s, 1249.11/s  (0.791s, 1295.30/s)  LR: 5.619e-04  Data: 0.011 (0.013)
Train: 139 [1100/1251 ( 88%)]  Loss: 3.806 (3.69)  Time: 0.778s, 1315.98/s  (0.790s, 1295.99/s)  LR: 5.619e-04  Data: 0.011 (0.013)
Train: 139 [1150/1251 ( 92%)]  Loss: 3.963 (3.70)  Time: 0.776s, 1320.24/s  (0.791s, 1294.70/s)  LR: 5.619e-04  Data: 0.011 (0.013)
Train: 139 [1200/1251 ( 96%)]  Loss: 4.036 (3.71)  Time: 0.780s, 1313.28/s  (0.790s, 1295.52/s)  LR: 5.619e-04  Data: 0.011 (0.013)
Train: 139 [1250/1251 (100%)]  Loss: 3.806 (3.72)  Time: 0.762s, 1343.71/s  (0.790s, 1296.25/s)  LR: 5.619e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.537 (1.537)  Loss:  0.9631 (0.9631)  Acc@1: 87.7930 (87.7930)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.171 (0.551)  Loss:  1.1628 (1.5432)  Acc@1: 81.9576 (71.3920)  Acc@5: 94.6934 (90.4940)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-132.pth.tar', 71.79800006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-128.pth.tar', 71.71800006347657)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-137.pth.tar', 71.70800007080078)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-138.pth.tar', 71.50600006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-131.pth.tar', 71.48200009277343)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-136.pth.tar', 71.47200004638673)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-129.pth.tar', 71.47000016845703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-139.pth.tar', 71.39200004882812)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-126.pth.tar', 71.38999991210937)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-124.pth.tar', 71.3380001147461)

Train: 140 [   0/1251 (  0%)]  Loss: 3.467 (3.47)  Time: 2.374s,  431.42/s  (2.374s,  431.42/s)  LR: 5.567e-04  Data: 1.638 (1.638)
Train: 140 [  50/1251 (  4%)]  Loss: 3.892 (3.68)  Time: 0.831s, 1231.88/s  (0.814s, 1258.30/s)  LR: 5.567e-04  Data: 0.011 (0.047)
Train: 140 [ 100/1251 (  8%)]  Loss: 3.819 (3.73)  Time: 0.777s, 1318.53/s  (0.797s, 1285.06/s)  LR: 5.567e-04  Data: 0.011 (0.029)
Train: 140 [ 150/1251 ( 12%)]  Loss: 3.575 (3.69)  Time: 0.776s, 1318.91/s  (0.791s, 1294.87/s)  LR: 5.567e-04  Data: 0.011 (0.023)
Train: 140 [ 200/1251 ( 16%)]  Loss: 3.723 (3.70)  Time: 0.811s, 1262.94/s  (0.796s, 1286.20/s)  LR: 5.567e-04  Data: 0.011 (0.020)
Train: 140 [ 250/1251 ( 20%)]  Loss: 3.743 (3.70)  Time: 0.777s, 1318.69/s  (0.797s, 1285.28/s)  LR: 5.567e-04  Data: 0.011 (0.018)
Train: 140 [ 300/1251 ( 24%)]  Loss: 3.380 (3.66)  Time: 0.776s, 1320.09/s  (0.794s, 1290.43/s)  LR: 5.567e-04  Data: 0.011 (0.017)
Train: 140 [ 350/1251 ( 28%)]  Loss: 4.170 (3.72)  Time: 0.811s, 1262.00/s  (0.792s, 1292.80/s)  LR: 5.567e-04  Data: 0.012 (0.016)
Train: 140 [ 400/1251 ( 32%)]  Loss: 3.657 (3.71)  Time: 0.813s, 1259.20/s  (0.794s, 1288.96/s)  LR: 5.567e-04  Data: 0.011 (0.016)
Train: 140 [ 450/1251 ( 36%)]  Loss: 3.681 (3.71)  Time: 0.777s, 1317.38/s  (0.794s, 1289.85/s)  LR: 5.567e-04  Data: 0.011 (0.015)
Train: 140 [ 500/1251 ( 40%)]  Loss: 3.526 (3.69)  Time: 0.778s, 1316.78/s  (0.792s, 1292.42/s)  LR: 5.567e-04  Data: 0.011 (0.015)
Train: 140 [ 550/1251 ( 44%)]  Loss: 3.480 (3.68)  Time: 0.778s, 1316.39/s  (0.791s, 1294.49/s)  LR: 5.567e-04  Data: 0.011 (0.014)
Train: 140 [ 600/1251 ( 48%)]  Loss: 3.898 (3.69)  Time: 0.778s, 1315.50/s  (0.790s, 1296.21/s)  LR: 5.567e-04  Data: 0.011 (0.014)
Train: 140 [ 650/1251 ( 52%)]  Loss: 3.914 (3.71)  Time: 0.778s, 1316.69/s  (0.789s, 1297.77/s)  LR: 5.567e-04  Data: 0.011 (0.014)
Train: 140 [ 700/1251 ( 56%)]  Loss: 3.748 (3.71)  Time: 0.777s, 1318.45/s  (0.788s, 1298.79/s)  LR: 5.567e-04  Data: 0.011 (0.014)
Train: 140 [ 750/1251 ( 60%)]  Loss: 4.076 (3.73)  Time: 0.778s, 1315.64/s  (0.788s, 1300.00/s)  LR: 5.567e-04  Data: 0.011 (0.014)
Train: 140 [ 800/1251 ( 64%)]  Loss: 3.956 (3.75)  Time: 0.776s, 1318.83/s  (0.787s, 1300.73/s)  LR: 5.567e-04  Data: 0.011 (0.013)
Train: 140 [ 850/1251 ( 68%)]  Loss: 3.485 (3.73)  Time: 0.778s, 1316.66/s  (0.787s, 1301.59/s)  LR: 5.567e-04  Data: 0.011 (0.013)
Train: 140 [ 900/1251 ( 72%)]  Loss: 3.719 (3.73)  Time: 0.779s, 1314.17/s  (0.786s, 1302.44/s)  LR: 5.567e-04  Data: 0.011 (0.013)
Train: 140 [ 950/1251 ( 76%)]  Loss: 3.645 (3.73)  Time: 0.777s, 1317.26/s  (0.787s, 1301.41/s)  LR: 5.567e-04  Data: 0.011 (0.013)
Train: 140 [1000/1251 ( 80%)]  Loss: 4.001 (3.74)  Time: 0.777s, 1317.70/s  (0.786s, 1302.18/s)  LR: 5.567e-04  Data: 0.011 (0.013)
Train: 140 [1050/1251 ( 84%)]  Loss: 3.397 (3.72)  Time: 0.777s, 1317.67/s  (0.786s, 1302.81/s)  LR: 5.567e-04  Data: 0.011 (0.013)
Train: 140 [1100/1251 ( 88%)]  Loss: 3.602 (3.72)  Time: 0.779s, 1315.26/s  (0.786s, 1303.46/s)  LR: 5.567e-04  Data: 0.011 (0.013)
Train: 140 [1150/1251 ( 92%)]  Loss: 3.657 (3.72)  Time: 0.779s, 1315.33/s  (0.785s, 1303.87/s)  LR: 5.567e-04  Data: 0.011 (0.013)
Train: 140 [1200/1251 ( 96%)]  Loss: 3.425 (3.71)  Time: 0.784s, 1305.83/s  (0.785s, 1304.27/s)  LR: 5.567e-04  Data: 0.011 (0.013)
Train: 140 [1250/1251 (100%)]  Loss: 3.245 (3.69)  Time: 0.765s, 1338.38/s  (0.785s, 1304.80/s)  LR: 5.567e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.510 (1.510)  Loss:  0.8968 (0.8968)  Acc@1: 88.0859 (88.0859)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.172 (0.555)  Loss:  1.0056 (1.5310)  Acc@1: 81.7217 (70.9300)  Acc@5: 95.4009 (90.4100)
Train: 141 [   0/1251 (  0%)]  Loss: 3.866 (3.87)  Time: 2.164s,  473.14/s  (2.164s,  473.14/s)  LR: 5.516e-04  Data: 1.429 (1.429)
Train: 141 [  50/1251 (  4%)]  Loss: 3.533 (3.70)  Time: 0.779s, 1314.20/s  (0.824s, 1242.80/s)  LR: 5.516e-04  Data: 0.011 (0.044)
Train: 141 [ 100/1251 (  8%)]  Loss: 3.744 (3.71)  Time: 0.784s, 1305.91/s  (0.802s, 1277.55/s)  LR: 5.516e-04  Data: 0.011 (0.028)
Train: 141 [ 150/1251 ( 12%)]  Loss: 3.550 (3.67)  Time: 0.776s, 1319.55/s  (0.806s, 1270.19/s)  LR: 5.516e-04  Data: 0.011 (0.022)
Train: 141 [ 200/1251 ( 16%)]  Loss: 3.749 (3.69)  Time: 0.777s, 1318.67/s  (0.804s, 1274.21/s)  LR: 5.516e-04  Data: 0.011 (0.019)
Train: 141 [ 250/1251 ( 20%)]  Loss: 3.856 (3.72)  Time: 0.778s, 1316.50/s  (0.799s, 1282.24/s)  LR: 5.516e-04  Data: 0.011 (0.018)
Train: 141 [ 300/1251 ( 24%)]  Loss: 3.696 (3.71)  Time: 0.779s, 1314.56/s  (0.798s, 1283.79/s)  LR: 5.516e-04  Data: 0.011 (0.017)
Train: 141 [ 350/1251 ( 28%)]  Loss: 3.908 (3.74)  Time: 0.779s, 1315.29/s  (0.795s, 1288.47/s)  LR: 5.516e-04  Data: 0.011 (0.016)
Train: 141 [ 400/1251 ( 32%)]  Loss: 3.991 (3.77)  Time: 0.776s, 1319.63/s  (0.793s, 1291.58/s)  LR: 5.516e-04  Data: 0.011 (0.015)
Train: 141 [ 450/1251 ( 36%)]  Loss: 3.537 (3.74)  Time: 0.777s, 1317.38/s  (0.791s, 1294.08/s)  LR: 5.516e-04  Data: 0.011 (0.015)
Train: 141 [ 500/1251 ( 40%)]  Loss: 3.855 (3.75)  Time: 0.777s, 1318.24/s  (0.790s, 1296.17/s)  LR: 5.516e-04  Data: 0.011 (0.014)
Train: 141 [ 550/1251 ( 44%)]  Loss: 4.211 (3.79)  Time: 0.777s, 1318.71/s  (0.789s, 1297.73/s)  LR: 5.516e-04  Data: 0.011 (0.014)
Train: 141 [ 600/1251 ( 48%)]  Loss: 3.984 (3.81)  Time: 0.778s, 1315.89/s  (0.788s, 1299.02/s)  LR: 5.516e-04  Data: 0.011 (0.014)
Train: 141 [ 650/1251 ( 52%)]  Loss: 3.693 (3.80)  Time: 0.867s, 1180.61/s  (0.790s, 1296.86/s)  LR: 5.516e-04  Data: 0.011 (0.014)
Train: 141 [ 700/1251 ( 56%)]  Loss: 3.960 (3.81)  Time: 0.777s, 1317.77/s  (0.789s, 1298.28/s)  LR: 5.516e-04  Data: 0.011 (0.013)
Train: 141 [ 750/1251 ( 60%)]  Loss: 3.733 (3.80)  Time: 0.778s, 1316.14/s  (0.789s, 1297.96/s)  LR: 5.516e-04  Data: 0.011 (0.013)
Train: 141 [ 800/1251 ( 64%)]  Loss: 3.732 (3.80)  Time: 0.779s, 1314.89/s  (0.788s, 1298.94/s)  LR: 5.516e-04  Data: 0.011 (0.013)
Train: 141 [ 850/1251 ( 68%)]  Loss: 3.537 (3.79)  Time: 0.777s, 1317.48/s  (0.788s, 1299.99/s)  LR: 5.516e-04  Data: 0.011 (0.013)
Train: 141 [ 900/1251 ( 72%)]  Loss: 3.801 (3.79)  Time: 0.781s, 1310.47/s  (0.788s, 1299.18/s)  LR: 5.516e-04  Data: 0.011 (0.013)
Train: 141 [ 950/1251 ( 76%)]  Loss: 3.801 (3.79)  Time: 0.778s, 1316.28/s  (0.788s, 1300.09/s)  LR: 5.516e-04  Data: 0.011 (0.013)
Train: 141 [1000/1251 ( 80%)]  Loss: 3.830 (3.79)  Time: 0.815s, 1255.81/s  (0.788s, 1300.10/s)  LR: 5.516e-04  Data: 0.011 (0.013)
Train: 141 [1050/1251 ( 84%)]  Loss: 4.110 (3.80)  Time: 0.811s, 1263.17/s  (0.788s, 1299.96/s)  LR: 5.516e-04  Data: 0.011 (0.013)
Train: 141 [1100/1251 ( 88%)]  Loss: 3.933 (3.81)  Time: 0.777s, 1317.65/s  (0.788s, 1300.18/s)  LR: 5.516e-04  Data: 0.011 (0.012)
Train: 141 [1150/1251 ( 92%)]  Loss: 3.517 (3.80)  Time: 0.777s, 1318.33/s  (0.787s, 1300.78/s)  LR: 5.516e-04  Data: 0.011 (0.012)
Train: 141 [1200/1251 ( 96%)]  Loss: 3.660 (3.79)  Time: 0.777s, 1318.65/s  (0.787s, 1301.45/s)  LR: 5.516e-04  Data: 0.011 (0.012)
Train: 141 [1250/1251 (100%)]  Loss: 3.493 (3.78)  Time: 0.797s, 1285.54/s  (0.787s, 1301.14/s)  LR: 5.516e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.576 (1.576)  Loss:  1.0182 (1.0182)  Acc@1: 86.8164 (86.8164)  Acc@5: 96.2891 (96.2891)
Test: [  48/48]  Time: 0.172 (0.551)  Loss:  0.8407 (1.5252)  Acc@1: 84.9057 (71.2700)  Acc@5: 96.9340 (90.5280)
Train: 142 [   0/1251 (  0%)]  Loss: 3.798 (3.80)  Time: 2.161s,  473.87/s  (2.161s,  473.87/s)  LR: 5.464e-04  Data: 1.425 (1.425)
Train: 142 [  50/1251 (  4%)]  Loss: 3.769 (3.78)  Time: 0.780s, 1312.30/s  (0.813s, 1258.77/s)  LR: 5.464e-04  Data: 0.012 (0.047)
Train: 142 [ 100/1251 (  8%)]  Loss: 3.445 (3.67)  Time: 0.809s, 1265.56/s  (0.798s, 1283.01/s)  LR: 5.464e-04  Data: 0.011 (0.029)
Train: 142 [ 150/1251 ( 12%)]  Loss: 4.185 (3.80)  Time: 0.779s, 1314.97/s  (0.800s, 1280.47/s)  LR: 5.464e-04  Data: 0.011 (0.023)
Train: 142 [ 200/1251 ( 16%)]  Loss: 3.881 (3.82)  Time: 0.778s, 1316.51/s  (0.794s, 1288.87/s)  LR: 5.464e-04  Data: 0.011 (0.020)
Train: 142 [ 250/1251 ( 20%)]  Loss: 3.781 (3.81)  Time: 0.779s, 1315.03/s  (0.791s, 1294.05/s)  LR: 5.464e-04  Data: 0.011 (0.018)
Train: 142 [ 300/1251 ( 24%)]  Loss: 3.616 (3.78)  Time: 0.778s, 1317.01/s  (0.789s, 1297.78/s)  LR: 5.464e-04  Data: 0.011 (0.017)
Train: 142 [ 350/1251 ( 28%)]  Loss: 4.303 (3.85)  Time: 0.777s, 1318.58/s  (0.788s, 1300.32/s)  LR: 5.464e-04  Data: 0.011 (0.016)
Train: 142 [ 400/1251 ( 32%)]  Loss: 3.420 (3.80)  Time: 0.778s, 1316.14/s  (0.786s, 1302.38/s)  LR: 5.464e-04  Data: 0.011 (0.016)
Train: 142 [ 450/1251 ( 36%)]  Loss: 3.797 (3.80)  Time: 0.778s, 1316.60/s  (0.786s, 1303.37/s)  LR: 5.464e-04  Data: 0.011 (0.015)
Train: 142 [ 500/1251 ( 40%)]  Loss: 3.705 (3.79)  Time: 0.777s, 1317.81/s  (0.785s, 1304.28/s)  LR: 5.464e-04  Data: 0.011 (0.015)
Train: 142 [ 550/1251 ( 44%)]  Loss: 3.677 (3.78)  Time: 0.778s, 1315.69/s  (0.784s, 1305.46/s)  LR: 5.464e-04  Data: 0.011 (0.014)
Train: 142 [ 600/1251 ( 48%)]  Loss: 3.971 (3.80)  Time: 0.778s, 1315.50/s  (0.784s, 1306.17/s)  LR: 5.464e-04  Data: 0.011 (0.014)
Train: 142 [ 650/1251 ( 52%)]  Loss: 3.745 (3.79)  Time: 0.778s, 1315.53/s  (0.784s, 1306.08/s)  LR: 5.464e-04  Data: 0.011 (0.014)
Train: 142 [ 700/1251 ( 56%)]  Loss: 3.678 (3.78)  Time: 0.776s, 1318.82/s  (0.784s, 1306.64/s)  LR: 5.464e-04  Data: 0.012 (0.014)
Train: 142 [ 750/1251 ( 60%)]  Loss: 3.742 (3.78)  Time: 0.778s, 1316.28/s  (0.784s, 1306.90/s)  LR: 5.464e-04  Data: 0.011 (0.014)
Train: 142 [ 800/1251 ( 64%)]  Loss: 3.699 (3.78)  Time: 0.777s, 1318.07/s  (0.783s, 1307.10/s)  LR: 5.464e-04  Data: 0.011 (0.013)
Train: 142 [ 850/1251 ( 68%)]  Loss: 3.613 (3.77)  Time: 0.779s, 1314.03/s  (0.784s, 1306.61/s)  LR: 5.464e-04  Data: 0.011 (0.013)
Train: 142 [ 900/1251 ( 72%)]  Loss: 3.771 (3.77)  Time: 0.775s, 1320.82/s  (0.783s, 1307.10/s)  LR: 5.464e-04  Data: 0.011 (0.013)
Train: 142 [ 950/1251 ( 76%)]  Loss: 3.849 (3.77)  Time: 0.814s, 1257.78/s  (0.783s, 1307.07/s)  LR: 5.464e-04  Data: 0.011 (0.013)
Train: 142 [1000/1251 ( 80%)]  Loss: 3.490 (3.76)  Time: 0.777s, 1317.45/s  (0.784s, 1306.06/s)  LR: 5.464e-04  Data: 0.011 (0.013)
Train: 142 [1050/1251 ( 84%)]  Loss: 4.057 (3.77)  Time: 0.778s, 1316.81/s  (0.784s, 1306.50/s)  LR: 5.464e-04  Data: 0.011 (0.013)
Train: 142 [1100/1251 ( 88%)]  Loss: 4.061 (3.78)  Time: 0.777s, 1318.32/s  (0.783s, 1306.99/s)  LR: 5.464e-04  Data: 0.011 (0.013)
Train: 142 [1150/1251 ( 92%)]  Loss: 4.075 (3.80)  Time: 0.777s, 1317.19/s  (0.783s, 1307.35/s)  LR: 5.464e-04  Data: 0.011 (0.013)
Train: 142 [1200/1251 ( 96%)]  Loss: 4.255 (3.82)  Time: 0.775s, 1320.90/s  (0.783s, 1307.52/s)  LR: 5.464e-04  Data: 0.011 (0.013)
Train: 142 [1250/1251 (100%)]  Loss: 3.799 (3.81)  Time: 0.764s, 1340.27/s  (0.783s, 1307.91/s)  LR: 5.464e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.584 (1.584)  Loss:  0.9562 (0.9562)  Acc@1: 88.7695 (88.7695)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.172 (0.554)  Loss:  0.9087 (1.5328)  Acc@1: 84.1981 (71.6920)  Acc@5: 96.1085 (90.7080)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-132.pth.tar', 71.79800006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-128.pth.tar', 71.71800006347657)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-137.pth.tar', 71.70800007080078)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-142.pth.tar', 71.6920001171875)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-138.pth.tar', 71.50600006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-131.pth.tar', 71.48200009277343)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-136.pth.tar', 71.47200004638673)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-129.pth.tar', 71.47000016845703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-139.pth.tar', 71.39200004882812)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-126.pth.tar', 71.38999991210937)

Train: 143 [   0/1251 (  0%)]  Loss: 3.563 (3.56)  Time: 2.159s,  474.26/s  (2.159s,  474.26/s)  LR: 5.413e-04  Data: 1.425 (1.425)
Train: 143 [  50/1251 (  4%)]  Loss: 3.974 (3.77)  Time: 0.779s, 1314.97/s  (0.825s, 1240.81/s)  LR: 5.413e-04  Data: 0.011 (0.043)
Train: 143 [ 100/1251 (  8%)]  Loss: 3.641 (3.73)  Time: 0.779s, 1313.80/s  (0.802s, 1276.37/s)  LR: 5.413e-04  Data: 0.011 (0.027)
Train: 143 [ 150/1251 ( 12%)]  Loss: 3.784 (3.74)  Time: 0.778s, 1315.35/s  (0.794s, 1288.92/s)  LR: 5.413e-04  Data: 0.011 (0.022)
Train: 143 [ 200/1251 ( 16%)]  Loss: 3.639 (3.72)  Time: 0.777s, 1318.05/s  (0.800s, 1280.62/s)  LR: 5.413e-04  Data: 0.011 (0.019)
Train: 143 [ 250/1251 ( 20%)]  Loss: 3.870 (3.75)  Time: 0.780s, 1312.49/s  (0.796s, 1286.83/s)  LR: 5.413e-04  Data: 0.011 (0.018)
Train: 143 [ 300/1251 ( 24%)]  Loss: 4.153 (3.80)  Time: 0.776s, 1319.24/s  (0.793s, 1290.85/s)  LR: 5.413e-04  Data: 0.011 (0.017)
Train: 143 [ 350/1251 ( 28%)]  Loss: 3.536 (3.77)  Time: 0.779s, 1313.99/s  (0.791s, 1294.43/s)  LR: 5.413e-04  Data: 0.011 (0.016)
Train: 143 [ 400/1251 ( 32%)]  Loss: 3.672 (3.76)  Time: 0.779s, 1314.23/s  (0.790s, 1296.99/s)  LR: 5.413e-04  Data: 0.011 (0.015)
Train: 143 [ 450/1251 ( 36%)]  Loss: 3.938 (3.78)  Time: 0.775s, 1321.90/s  (0.788s, 1298.84/s)  LR: 5.413e-04  Data: 0.011 (0.015)
Train: 143 [ 500/1251 ( 40%)]  Loss: 3.988 (3.80)  Time: 0.827s, 1237.47/s  (0.789s, 1297.97/s)  LR: 5.413e-04  Data: 0.011 (0.014)
Train: 143 [ 550/1251 ( 44%)]  Loss: 3.500 (3.77)  Time: 0.812s, 1261.52/s  (0.791s, 1295.37/s)  LR: 5.413e-04  Data: 0.011 (0.014)
Train: 143 [ 600/1251 ( 48%)]  Loss: 3.784 (3.77)  Time: 0.784s, 1306.40/s  (0.790s, 1296.48/s)  LR: 5.413e-04  Data: 0.014 (0.014)
Train: 143 [ 650/1251 ( 52%)]  Loss: 3.544 (3.76)  Time: 0.777s, 1317.07/s  (0.789s, 1297.42/s)  LR: 5.413e-04  Data: 0.011 (0.014)
Train: 143 [ 700/1251 ( 56%)]  Loss: 3.816 (3.76)  Time: 0.777s, 1317.41/s  (0.788s, 1298.74/s)  LR: 5.413e-04  Data: 0.011 (0.013)
Train: 143 [ 750/1251 ( 60%)]  Loss: 3.315 (3.73)  Time: 0.778s, 1317.01/s  (0.788s, 1299.55/s)  LR: 5.413e-04  Data: 0.011 (0.013)
Train: 143 [ 800/1251 ( 64%)]  Loss: 3.413 (3.71)  Time: 0.778s, 1315.75/s  (0.787s, 1300.50/s)  LR: 5.413e-04  Data: 0.011 (0.013)
Train: 143 [ 850/1251 ( 68%)]  Loss: 3.593 (3.71)  Time: 0.778s, 1315.81/s  (0.787s, 1301.34/s)  LR: 5.413e-04  Data: 0.011 (0.013)
Train: 143 [ 900/1251 ( 72%)]  Loss: 3.919 (3.72)  Time: 0.778s, 1316.34/s  (0.787s, 1301.86/s)  LR: 5.413e-04  Data: 0.011 (0.013)
Train: 143 [ 950/1251 ( 76%)]  Loss: 3.952 (3.73)  Time: 0.812s, 1261.04/s  (0.787s, 1301.93/s)  LR: 5.413e-04  Data: 0.011 (0.013)
Train: 143 [1000/1251 ( 80%)]  Loss: 3.888 (3.74)  Time: 0.779s, 1314.20/s  (0.788s, 1299.76/s)  LR: 5.413e-04  Data: 0.011 (0.013)
Train: 143 [1050/1251 ( 84%)]  Loss: 3.825 (3.74)  Time: 0.815s, 1256.64/s  (0.787s, 1300.36/s)  LR: 5.413e-04  Data: 0.011 (0.013)
Train: 143 [1100/1251 ( 88%)]  Loss: 3.783 (3.74)  Time: 0.782s, 1309.73/s  (0.788s, 1299.49/s)  LR: 5.413e-04  Data: 0.011 (0.013)
Train: 143 [1150/1251 ( 92%)]  Loss: 4.030 (3.76)  Time: 0.777s, 1317.34/s  (0.788s, 1300.15/s)  LR: 5.413e-04  Data: 0.011 (0.013)
Train: 143 [1200/1251 ( 96%)]  Loss: 4.056 (3.77)  Time: 0.777s, 1318.45/s  (0.787s, 1300.79/s)  LR: 5.413e-04  Data: 0.011 (0.012)
Train: 143 [1250/1251 (100%)]  Loss: 3.630 (3.76)  Time: 0.766s, 1337.50/s  (0.787s, 1301.38/s)  LR: 5.413e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.514 (1.514)  Loss:  0.9078 (0.9078)  Acc@1: 88.0859 (88.0859)  Acc@5: 96.2891 (96.2891)
Test: [  48/48]  Time: 0.172 (0.550)  Loss:  1.0643 (1.4864)  Acc@1: 83.9623 (71.4640)  Acc@5: 95.9906 (90.5460)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-132.pth.tar', 71.79800006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-128.pth.tar', 71.71800006347657)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-137.pth.tar', 71.70800007080078)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-142.pth.tar', 71.6920001171875)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-138.pth.tar', 71.50600006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-131.pth.tar', 71.48200009277343)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-136.pth.tar', 71.47200004638673)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-129.pth.tar', 71.47000016845703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-143.pth.tar', 71.46400014404297)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-139.pth.tar', 71.39200004882812)

Train: 144 [   0/1251 (  0%)]  Loss: 3.624 (3.62)  Time: 2.394s,  427.67/s  (2.394s,  427.67/s)  LR: 5.361e-04  Data: 1.660 (1.660)
Train: 144 [  50/1251 (  4%)]  Loss: 3.607 (3.62)  Time: 0.815s, 1255.76/s  (0.843s, 1215.21/s)  LR: 5.361e-04  Data: 0.011 (0.044)
Train: 144 [ 100/1251 (  8%)]  Loss: 3.664 (3.63)  Time: 0.777s, 1318.37/s  (0.828s, 1237.23/s)  LR: 5.361e-04  Data: 0.011 (0.028)
Train: 144 [ 150/1251 ( 12%)]  Loss: 3.556 (3.61)  Time: 0.778s, 1316.50/s  (0.812s, 1261.40/s)  LR: 5.361e-04  Data: 0.011 (0.022)
Train: 144 [ 200/1251 ( 16%)]  Loss: 3.931 (3.68)  Time: 0.777s, 1318.48/s  (0.805s, 1272.12/s)  LR: 5.361e-04  Data: 0.011 (0.020)
Train: 144 [ 250/1251 ( 20%)]  Loss: 3.729 (3.69)  Time: 0.779s, 1314.84/s  (0.800s, 1280.40/s)  LR: 5.361e-04  Data: 0.011 (0.018)
Train: 144 [ 300/1251 ( 24%)]  Loss: 3.695 (3.69)  Time: 0.779s, 1314.95/s  (0.796s, 1286.29/s)  LR: 5.361e-04  Data: 0.011 (0.017)
Train: 144 [ 350/1251 ( 28%)]  Loss: 3.987 (3.72)  Time: 0.778s, 1316.14/s  (0.794s, 1290.30/s)  LR: 5.361e-04  Data: 0.011 (0.016)
Train: 144 [ 400/1251 ( 32%)]  Loss: 3.947 (3.75)  Time: 0.777s, 1317.70/s  (0.792s, 1293.51/s)  LR: 5.361e-04  Data: 0.011 (0.015)
Train: 144 [ 450/1251 ( 36%)]  Loss: 3.767 (3.75)  Time: 0.777s, 1317.18/s  (0.790s, 1295.58/s)  LR: 5.361e-04  Data: 0.011 (0.015)
Train: 144 [ 500/1251 ( 40%)]  Loss: 3.854 (3.76)  Time: 0.777s, 1317.95/s  (0.789s, 1297.35/s)  LR: 5.361e-04  Data: 0.011 (0.014)
Train: 144 [ 550/1251 ( 44%)]  Loss: 3.973 (3.78)  Time: 0.778s, 1315.73/s  (0.788s, 1298.93/s)  LR: 5.361e-04  Data: 0.011 (0.014)
Train: 144 [ 600/1251 ( 48%)]  Loss: 3.702 (3.77)  Time: 0.844s, 1213.67/s  (0.788s, 1299.09/s)  LR: 5.361e-04  Data: 0.011 (0.014)
Train: 144 [ 650/1251 ( 52%)]  Loss: 3.497 (3.75)  Time: 0.779s, 1313.72/s  (0.788s, 1299.41/s)  LR: 5.361e-04  Data: 0.011 (0.014)
Train: 144 [ 700/1251 ( 56%)]  Loss: 3.627 (3.74)  Time: 0.779s, 1314.73/s  (0.787s, 1300.50/s)  LR: 5.361e-04  Data: 0.011 (0.013)
Train: 144 [ 750/1251 ( 60%)]  Loss: 3.796 (3.75)  Time: 0.776s, 1320.05/s  (0.787s, 1301.56/s)  LR: 5.361e-04  Data: 0.011 (0.013)
Train: 144 [ 800/1251 ( 64%)]  Loss: 3.868 (3.75)  Time: 0.816s, 1254.81/s  (0.787s, 1300.62/s)  LR: 5.361e-04  Data: 0.011 (0.013)
Train: 144 [ 850/1251 ( 68%)]  Loss: 3.685 (3.75)  Time: 0.812s, 1260.45/s  (0.788s, 1299.78/s)  LR: 5.361e-04  Data: 0.011 (0.013)
Train: 144 [ 900/1251 ( 72%)]  Loss: 3.929 (3.76)  Time: 0.778s, 1315.37/s  (0.789s, 1298.60/s)  LR: 5.361e-04  Data: 0.011 (0.013)
Train: 144 [ 950/1251 ( 76%)]  Loss: 4.036 (3.77)  Time: 0.778s, 1316.90/s  (0.789s, 1297.83/s)  LR: 5.361e-04  Data: 0.011 (0.013)
Train: 144 [1000/1251 ( 80%)]  Loss: 3.917 (3.78)  Time: 0.778s, 1316.98/s  (0.788s, 1298.72/s)  LR: 5.361e-04  Data: 0.011 (0.013)
Train: 144 [1050/1251 ( 84%)]  Loss: 3.765 (3.78)  Time: 0.778s, 1315.72/s  (0.788s, 1298.82/s)  LR: 5.361e-04  Data: 0.011 (0.013)
Train: 144 [1100/1251 ( 88%)]  Loss: 3.728 (3.78)  Time: 0.777s, 1317.52/s  (0.788s, 1298.73/s)  LR: 5.361e-04  Data: 0.011 (0.013)
Train: 144 [1150/1251 ( 92%)]  Loss: 3.748 (3.78)  Time: 0.778s, 1316.79/s  (0.789s, 1298.42/s)  LR: 5.361e-04  Data: 0.011 (0.012)
Train: 144 [1200/1251 ( 96%)]  Loss: 3.731 (3.77)  Time: 0.779s, 1314.96/s  (0.789s, 1297.06/s)  LR: 5.361e-04  Data: 0.011 (0.012)
Train: 144 [1250/1251 (100%)]  Loss: 4.352 (3.80)  Time: 0.767s, 1334.99/s  (0.789s, 1297.87/s)  LR: 5.361e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.525 (1.525)  Loss:  1.0207 (1.0207)  Acc@1: 87.4023 (87.4023)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.172 (0.556)  Loss:  1.1021 (1.5182)  Acc@1: 82.5472 (71.9140)  Acc@5: 95.4009 (90.8200)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-144.pth.tar', 71.91400004638672)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-132.pth.tar', 71.79800006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-128.pth.tar', 71.71800006347657)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-137.pth.tar', 71.70800007080078)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-142.pth.tar', 71.6920001171875)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-138.pth.tar', 71.50600006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-131.pth.tar', 71.48200009277343)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-136.pth.tar', 71.47200004638673)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-129.pth.tar', 71.47000016845703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-143.pth.tar', 71.46400014404297)

Train: 145 [   0/1251 (  0%)]  Loss: 3.307 (3.31)  Time: 2.472s,  414.30/s  (2.472s,  414.30/s)  LR: 5.309e-04  Data: 1.737 (1.737)
Train: 145 [  50/1251 (  4%)]  Loss: 3.274 (3.29)  Time: 0.778s, 1316.31/s  (0.819s, 1251.06/s)  LR: 5.309e-04  Data: 0.011 (0.051)
Train: 145 [ 100/1251 (  8%)]  Loss: 3.710 (3.43)  Time: 0.777s, 1317.51/s  (0.798s, 1282.80/s)  LR: 5.309e-04  Data: 0.011 (0.031)
Train: 145 [ 150/1251 ( 12%)]  Loss: 3.939 (3.56)  Time: 0.817s, 1253.42/s  (0.794s, 1289.29/s)  LR: 5.309e-04  Data: 0.011 (0.025)
Train: 145 [ 200/1251 ( 16%)]  Loss: 3.745 (3.60)  Time: 0.817s, 1253.59/s  (0.795s, 1288.44/s)  LR: 5.309e-04  Data: 0.011 (0.021)
Train: 145 [ 250/1251 ( 20%)]  Loss: 3.520 (3.58)  Time: 0.812s, 1260.36/s  (0.798s, 1283.95/s)  LR: 5.309e-04  Data: 0.011 (0.019)
Train: 145 [ 300/1251 ( 24%)]  Loss: 3.838 (3.62)  Time: 0.777s, 1318.70/s  (0.798s, 1283.60/s)  LR: 5.309e-04  Data: 0.011 (0.018)
Train: 145 [ 350/1251 ( 28%)]  Loss: 4.125 (3.68)  Time: 0.777s, 1317.81/s  (0.795s, 1288.28/s)  LR: 5.309e-04  Data: 0.011 (0.017)
Train: 145 [ 400/1251 ( 32%)]  Loss: 3.731 (3.69)  Time: 0.817s, 1253.26/s  (0.796s, 1286.06/s)  LR: 5.309e-04  Data: 0.011 (0.016)
Train: 145 [ 450/1251 ( 36%)]  Loss: 3.807 (3.70)  Time: 0.778s, 1316.56/s  (0.794s, 1289.43/s)  LR: 5.309e-04  Data: 0.011 (0.016)
Train: 145 [ 500/1251 ( 40%)]  Loss: 3.686 (3.70)  Time: 0.780s, 1313.28/s  (0.793s, 1290.88/s)  LR: 5.309e-04  Data: 0.011 (0.015)
Train: 145 [ 550/1251 ( 44%)]  Loss: 3.764 (3.70)  Time: 0.777s, 1317.27/s  (0.792s, 1293.18/s)  LR: 5.309e-04  Data: 0.011 (0.015)
Train: 145 [ 600/1251 ( 48%)]  Loss: 3.721 (3.71)  Time: 0.777s, 1317.19/s  (0.791s, 1294.24/s)  LR: 5.309e-04  Data: 0.011 (0.015)
Train: 145 [ 650/1251 ( 52%)]  Loss: 3.942 (3.72)  Time: 0.811s, 1262.94/s  (0.793s, 1292.03/s)  LR: 5.309e-04  Data: 0.014 (0.014)
Train: 145 [ 700/1251 ( 56%)]  Loss: 3.709 (3.72)  Time: 0.779s, 1315.21/s  (0.793s, 1290.94/s)  LR: 5.309e-04  Data: 0.011 (0.014)
Train: 145 [ 750/1251 ( 60%)]  Loss: 3.945 (3.74)  Time: 0.778s, 1316.63/s  (0.792s, 1292.50/s)  LR: 5.309e-04  Data: 0.012 (0.014)
Train: 145 [ 800/1251 ( 64%)]  Loss: 3.870 (3.74)  Time: 0.777s, 1318.24/s  (0.791s, 1293.91/s)  LR: 5.309e-04  Data: 0.011 (0.014)
Train: 145 [ 850/1251 ( 68%)]  Loss: 3.610 (3.74)  Time: 0.777s, 1317.83/s  (0.791s, 1295.16/s)  LR: 5.309e-04  Data: 0.011 (0.014)
Train: 145 [ 900/1251 ( 72%)]  Loss: 3.791 (3.74)  Time: 0.780s, 1312.85/s  (0.790s, 1296.24/s)  LR: 5.309e-04  Data: 0.011 (0.013)
Train: 145 [ 950/1251 ( 76%)]  Loss: 3.491 (3.73)  Time: 0.830s, 1233.22/s  (0.789s, 1297.19/s)  LR: 5.309e-04  Data: 0.011 (0.013)
Train: 145 [1000/1251 ( 80%)]  Loss: 3.990 (3.74)  Time: 0.780s, 1312.80/s  (0.789s, 1297.49/s)  LR: 5.309e-04  Data: 0.011 (0.013)
Train: 145 [1050/1251 ( 84%)]  Loss: 3.754 (3.74)  Time: 0.779s, 1315.31/s  (0.789s, 1298.38/s)  LR: 5.309e-04  Data: 0.011 (0.013)
Train: 145 [1100/1251 ( 88%)]  Loss: 3.456 (3.73)  Time: 0.814s, 1258.72/s  (0.789s, 1298.15/s)  LR: 5.309e-04  Data: 0.011 (0.013)
Train: 145 [1150/1251 ( 92%)]  Loss: 3.472 (3.72)  Time: 0.777s, 1317.47/s  (0.789s, 1298.10/s)  LR: 5.309e-04  Data: 0.012 (0.013)
Train: 145 [1200/1251 ( 96%)]  Loss: 3.522 (3.71)  Time: 0.813s, 1259.72/s  (0.789s, 1297.25/s)  LR: 5.309e-04  Data: 0.011 (0.013)
Train: 145 [1250/1251 (100%)]  Loss: 3.421 (3.70)  Time: 0.763s, 1342.57/s  (0.790s, 1296.90/s)  LR: 5.309e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.518 (1.518)  Loss:  0.8877 (0.8877)  Acc@1: 87.4023 (87.4023)  Acc@5: 95.8008 (95.8008)
Test: [  48/48]  Time: 0.171 (0.551)  Loss:  0.9603 (1.4586)  Acc@1: 84.0802 (72.3080)  Acc@5: 95.7547 (90.8920)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-145.pth.tar', 72.30800006591797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-144.pth.tar', 71.91400004638672)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-132.pth.tar', 71.79800006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-128.pth.tar', 71.71800006347657)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-137.pth.tar', 71.70800007080078)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-142.pth.tar', 71.6920001171875)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-138.pth.tar', 71.50600006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-131.pth.tar', 71.48200009277343)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-136.pth.tar', 71.47200004638673)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-129.pth.tar', 71.47000016845703)

Train: 146 [   0/1251 (  0%)]  Loss: 3.654 (3.65)  Time: 2.244s,  456.39/s  (2.244s,  456.39/s)  LR: 5.257e-04  Data: 1.510 (1.510)
Train: 146 [  50/1251 (  4%)]  Loss: 3.860 (3.76)  Time: 0.812s, 1261.09/s  (0.830s, 1233.93/s)  LR: 5.257e-04  Data: 0.011 (0.044)
Train: 146 [ 100/1251 (  8%)]  Loss: 3.884 (3.80)  Time: 0.777s, 1318.12/s  (0.809s, 1266.30/s)  LR: 5.257e-04  Data: 0.011 (0.027)
Train: 146 [ 150/1251 ( 12%)]  Loss: 3.848 (3.81)  Time: 0.779s, 1314.07/s  (0.800s, 1280.65/s)  LR: 5.257e-04  Data: 0.011 (0.022)
Train: 146 [ 200/1251 ( 16%)]  Loss: 3.255 (3.70)  Time: 0.779s, 1315.34/s  (0.794s, 1289.02/s)  LR: 5.257e-04  Data: 0.011 (0.019)
Train: 146 [ 250/1251 ( 20%)]  Loss: 3.696 (3.70)  Time: 0.777s, 1318.16/s  (0.791s, 1294.11/s)  LR: 5.257e-04  Data: 0.011 (0.018)
Train: 146 [ 300/1251 ( 24%)]  Loss: 3.871 (3.72)  Time: 0.806s, 1270.31/s  (0.789s, 1297.69/s)  LR: 5.257e-04  Data: 0.011 (0.017)
Train: 146 [ 350/1251 ( 28%)]  Loss: 3.764 (3.73)  Time: 0.780s, 1311.99/s  (0.791s, 1294.30/s)  LR: 5.257e-04  Data: 0.011 (0.016)
Train: 146 [ 400/1251 ( 32%)]  Loss: 3.720 (3.73)  Time: 0.778s, 1315.44/s  (0.791s, 1294.47/s)  LR: 5.257e-04  Data: 0.010 (0.015)
Train: 146 [ 450/1251 ( 36%)]  Loss: 3.531 (3.71)  Time: 0.780s, 1312.49/s  (0.791s, 1294.34/s)  LR: 5.257e-04  Data: 0.010 (0.015)
Train: 146 [ 500/1251 ( 40%)]  Loss: 3.704 (3.71)  Time: 0.838s, 1222.63/s  (0.791s, 1294.45/s)  LR: 5.257e-04  Data: 0.011 (0.015)
Train: 146 [ 550/1251 ( 44%)]  Loss: 3.724 (3.71)  Time: 0.780s, 1313.03/s  (0.791s, 1295.38/s)  LR: 5.257e-04  Data: 0.011 (0.014)
Train: 146 [ 600/1251 ( 48%)]  Loss: 4.042 (3.73)  Time: 0.823s, 1244.17/s  (0.790s, 1295.65/s)  LR: 5.257e-04  Data: 0.011 (0.014)
Train: 146 [ 650/1251 ( 52%)]  Loss: 3.876 (3.74)  Time: 0.781s, 1311.82/s  (0.790s, 1296.22/s)  LR: 5.257e-04  Data: 0.011 (0.014)
Train: 146 [ 700/1251 ( 56%)]  Loss: 3.936 (3.76)  Time: 0.831s, 1232.00/s  (0.790s, 1296.14/s)  LR: 5.257e-04  Data: 0.011 (0.014)
Train: 146 [ 750/1251 ( 60%)]  Loss: 4.091 (3.78)  Time: 0.780s, 1312.56/s  (0.790s, 1296.05/s)  LR: 5.257e-04  Data: 0.011 (0.013)
Train: 146 [ 800/1251 ( 64%)]  Loss: 3.491 (3.76)  Time: 0.784s, 1305.88/s  (0.790s, 1296.22/s)  LR: 5.257e-04  Data: 0.011 (0.013)
Train: 146 [ 850/1251 ( 68%)]  Loss: 3.748 (3.76)  Time: 0.818s, 1251.72/s  (0.790s, 1296.66/s)  LR: 5.257e-04  Data: 0.012 (0.013)
Train: 146 [ 900/1251 ( 72%)]  Loss: 3.860 (3.77)  Time: 0.780s, 1312.85/s  (0.791s, 1294.49/s)  LR: 5.257e-04  Data: 0.011 (0.013)
Train: 146 [ 950/1251 ( 76%)]  Loss: 3.837 (3.77)  Time: 0.777s, 1318.26/s  (0.792s, 1293.35/s)  LR: 5.257e-04  Data: 0.011 (0.013)
Train: 146 [1000/1251 ( 80%)]  Loss: 4.088 (3.78)  Time: 0.780s, 1313.22/s  (0.792s, 1293.03/s)  LR: 5.257e-04  Data: 0.011 (0.013)
Train: 146 [1050/1251 ( 84%)]  Loss: 3.867 (3.79)  Time: 0.816s, 1255.36/s  (0.792s, 1292.79/s)  LR: 5.257e-04  Data: 0.010 (0.013)
Train: 146 [1100/1251 ( 88%)]  Loss: 3.772 (3.79)  Time: 0.787s, 1301.83/s  (0.792s, 1293.66/s)  LR: 5.257e-04  Data: 0.010 (0.013)
Train: 146 [1150/1251 ( 92%)]  Loss: 3.452 (3.77)  Time: 0.830s, 1233.86/s  (0.791s, 1294.01/s)  LR: 5.257e-04  Data: 0.010 (0.013)
Train: 146 [1200/1251 ( 96%)]  Loss: 3.547 (3.76)  Time: 0.817s, 1253.34/s  (0.792s, 1292.17/s)  LR: 5.257e-04  Data: 0.011 (0.012)
Train: 146 [1250/1251 (100%)]  Loss: 4.099 (3.78)  Time: 0.768s, 1333.12/s  (0.792s, 1292.15/s)  LR: 5.257e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.565 (1.565)  Loss:  0.9587 (0.9587)  Acc@1: 87.8906 (87.8906)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.172 (0.561)  Loss:  1.0259 (1.5021)  Acc@1: 82.3113 (72.0040)  Acc@5: 95.8726 (90.6500)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-145.pth.tar', 72.30800006591797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-146.pth.tar', 72.00400007324218)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-144.pth.tar', 71.91400004638672)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-132.pth.tar', 71.79800006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-128.pth.tar', 71.71800006347657)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-137.pth.tar', 71.70800007080078)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-142.pth.tar', 71.6920001171875)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-138.pth.tar', 71.50600006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-131.pth.tar', 71.48200009277343)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-136.pth.tar', 71.47200004638673)

Train: 147 [   0/1251 (  0%)]  Loss: 3.561 (3.56)  Time: 2.313s,  442.75/s  (2.313s,  442.75/s)  LR: 5.205e-04  Data: 1.579 (1.579)
Train: 147 [  50/1251 (  4%)]  Loss: 3.791 (3.68)  Time: 0.822s, 1246.35/s  (0.853s, 1200.05/s)  LR: 5.205e-04  Data: 0.011 (0.045)
Train: 147 [ 100/1251 (  8%)]  Loss: 3.638 (3.66)  Time: 0.809s, 1266.15/s  (0.840s, 1219.03/s)  LR: 5.205e-04  Data: 0.011 (0.028)
Train: 147 [ 150/1251 ( 12%)]  Loss: 3.597 (3.65)  Time: 0.797s, 1284.01/s  (0.825s, 1241.47/s)  LR: 5.205e-04  Data: 0.011 (0.023)
Train: 147 [ 200/1251 ( 16%)]  Loss: 4.012 (3.72)  Time: 0.843s, 1214.98/s  (0.818s, 1251.42/s)  LR: 5.205e-04  Data: 0.011 (0.020)
Train: 147 [ 250/1251 ( 20%)]  Loss: 3.285 (3.65)  Time: 0.778s, 1315.58/s  (0.814s, 1258.22/s)  LR: 5.205e-04  Data: 0.011 (0.018)
Train: 147 [ 300/1251 ( 24%)]  Loss: 3.427 (3.62)  Time: 0.790s, 1296.43/s  (0.808s, 1266.68/s)  LR: 5.205e-04  Data: 0.012 (0.017)
Train: 147 [ 350/1251 ( 28%)]  Loss: 3.915 (3.65)  Time: 0.815s, 1256.79/s  (0.807s, 1268.27/s)  LR: 5.205e-04  Data: 0.012 (0.016)
Train: 147 [ 400/1251 ( 32%)]  Loss: 3.372 (3.62)  Time: 0.779s, 1315.13/s  (0.805s, 1271.96/s)  LR: 5.205e-04  Data: 0.011 (0.015)
Train: 147 [ 450/1251 ( 36%)]  Loss: 3.921 (3.65)  Time: 0.780s, 1313.02/s  (0.804s, 1274.38/s)  LR: 5.205e-04  Data: 0.011 (0.015)
Train: 147 [ 500/1251 ( 40%)]  Loss: 3.732 (3.66)  Time: 0.777s, 1317.36/s  (0.802s, 1277.07/s)  LR: 5.205e-04  Data: 0.011 (0.014)
Train: 147 [ 550/1251 ( 44%)]  Loss: 4.032 (3.69)  Time: 0.812s, 1261.23/s  (0.802s, 1276.80/s)  LR: 5.205e-04  Data: 0.010 (0.014)
Train: 147 [ 600/1251 ( 48%)]  Loss: 3.477 (3.67)  Time: 0.779s, 1315.01/s  (0.802s, 1276.04/s)  LR: 5.205e-04  Data: 0.011 (0.014)
Train: 147 [ 650/1251 ( 52%)]  Loss: 3.780 (3.68)  Time: 0.779s, 1315.17/s  (0.801s, 1278.24/s)  LR: 5.205e-04  Data: 0.011 (0.014)
Train: 147 [ 700/1251 ( 56%)]  Loss: 3.790 (3.69)  Time: 0.778s, 1316.65/s  (0.800s, 1279.92/s)  LR: 5.205e-04  Data: 0.012 (0.013)
Train: 147 [ 750/1251 ( 60%)]  Loss: 3.403 (3.67)  Time: 0.820s, 1249.36/s  (0.800s, 1280.40/s)  LR: 5.205e-04  Data: 0.011 (0.013)
Train: 147 [ 800/1251 ( 64%)]  Loss: 3.620 (3.67)  Time: 0.830s, 1233.36/s  (0.799s, 1281.77/s)  LR: 5.205e-04  Data: 0.011 (0.013)
Train: 147 [ 850/1251 ( 68%)]  Loss: 3.518 (3.66)  Time: 0.790s, 1296.01/s  (0.798s, 1282.69/s)  LR: 5.205e-04  Data: 0.011 (0.013)
Train: 147 [ 900/1251 ( 72%)]  Loss: 3.902 (3.67)  Time: 0.816s, 1254.48/s  (0.798s, 1282.84/s)  LR: 5.205e-04  Data: 0.011 (0.013)
Train: 147 [ 950/1251 ( 76%)]  Loss: 3.649 (3.67)  Time: 0.780s, 1313.42/s  (0.798s, 1283.93/s)  LR: 5.205e-04  Data: 0.011 (0.013)
Train: 147 [1000/1251 ( 80%)]  Loss: 3.721 (3.67)  Time: 0.825s, 1240.94/s  (0.797s, 1284.13/s)  LR: 5.205e-04  Data: 0.012 (0.013)
Train: 147 [1050/1251 ( 84%)]  Loss: 3.618 (3.67)  Time: 0.780s, 1313.35/s  (0.797s, 1284.16/s)  LR: 5.205e-04  Data: 0.011 (0.013)
Train: 147 [1100/1251 ( 88%)]  Loss: 3.479 (3.66)  Time: 0.823s, 1244.93/s  (0.797s, 1285.29/s)  LR: 5.205e-04  Data: 0.011 (0.013)
Train: 147 [1150/1251 ( 92%)]  Loss: 3.731 (3.67)  Time: 0.781s, 1311.88/s  (0.797s, 1284.78/s)  LR: 5.205e-04  Data: 0.011 (0.013)
Train: 147 [1200/1251 ( 96%)]  Loss: 3.447 (3.66)  Time: 0.778s, 1315.64/s  (0.797s, 1284.16/s)  LR: 5.205e-04  Data: 0.011 (0.012)
Train: 147 [1250/1251 (100%)]  Loss: 3.747 (3.66)  Time: 0.768s, 1333.03/s  (0.797s, 1284.22/s)  LR: 5.205e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.524 (1.524)  Loss:  1.0315 (1.0315)  Acc@1: 87.7930 (87.7930)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.172 (0.564)  Loss:  1.0755 (1.5561)  Acc@1: 83.6085 (72.1840)  Acc@5: 96.2264 (90.9140)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-145.pth.tar', 72.30800006591797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-147.pth.tar', 72.18399999023437)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-146.pth.tar', 72.00400007324218)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-144.pth.tar', 71.91400004638672)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-132.pth.tar', 71.79800006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-128.pth.tar', 71.71800006347657)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-137.pth.tar', 71.70800007080078)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-142.pth.tar', 71.6920001171875)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-138.pth.tar', 71.50600006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-131.pth.tar', 71.48200009277343)

Train: 148 [   0/1251 (  0%)]  Loss: 3.737 (3.74)  Time: 2.329s,  439.62/s  (2.329s,  439.62/s)  LR: 5.154e-04  Data: 1.569 (1.569)
Train: 148 [  50/1251 (  4%)]  Loss: 3.819 (3.78)  Time: 0.863s, 1186.51/s  (0.841s, 1217.72/s)  LR: 5.154e-04  Data: 0.011 (0.049)
Train: 148 [ 100/1251 (  8%)]  Loss: 3.679 (3.74)  Time: 0.778s, 1316.48/s  (0.821s, 1246.93/s)  LR: 5.154e-04  Data: 0.011 (0.030)
Train: 148 [ 150/1251 ( 12%)]  Loss: 3.870 (3.78)  Time: 0.780s, 1312.39/s  (0.811s, 1262.20/s)  LR: 5.154e-04  Data: 0.011 (0.024)
Train: 148 [ 200/1251 ( 16%)]  Loss: 3.737 (3.77)  Time: 0.780s, 1313.42/s  (0.806s, 1269.89/s)  LR: 5.154e-04  Data: 0.011 (0.021)
Train: 148 [ 250/1251 ( 20%)]  Loss: 3.556 (3.73)  Time: 0.782s, 1309.76/s  (0.804s, 1274.34/s)  LR: 5.154e-04  Data: 0.011 (0.019)
Train: 148 [ 300/1251 ( 24%)]  Loss: 4.169 (3.80)  Time: 0.780s, 1313.10/s  (0.801s, 1279.01/s)  LR: 5.154e-04  Data: 0.011 (0.018)
Train: 148 [ 350/1251 ( 28%)]  Loss: 3.551 (3.76)  Time: 0.779s, 1314.47/s  (0.800s, 1280.75/s)  LR: 5.154e-04  Data: 0.011 (0.017)
Train: 148 [ 400/1251 ( 32%)]  Loss: 4.031 (3.79)  Time: 0.779s, 1314.47/s  (0.798s, 1282.54/s)  LR: 5.154e-04  Data: 0.011 (0.016)
Train: 148 [ 450/1251 ( 36%)]  Loss: 4.008 (3.82)  Time: 0.778s, 1316.50/s  (0.797s, 1285.25/s)  LR: 5.154e-04  Data: 0.011 (0.016)
Train: 148 [ 500/1251 ( 40%)]  Loss: 3.879 (3.82)  Time: 0.778s, 1316.17/s  (0.796s, 1285.69/s)  LR: 5.154e-04  Data: 0.011 (0.015)
Train: 148 [ 550/1251 ( 44%)]  Loss: 3.632 (3.81)  Time: 0.779s, 1315.24/s  (0.796s, 1286.03/s)  LR: 5.154e-04  Data: 0.011 (0.015)
Train: 148 [ 600/1251 ( 48%)]  Loss: 3.718 (3.80)  Time: 0.780s, 1313.48/s  (0.796s, 1286.34/s)  LR: 5.154e-04  Data: 0.011 (0.014)
Train: 148 [ 650/1251 ( 52%)]  Loss: 3.478 (3.78)  Time: 0.779s, 1313.81/s  (0.795s, 1288.14/s)  LR: 5.154e-04  Data: 0.011 (0.014)
Train: 148 [ 700/1251 ( 56%)]  Loss: 3.507 (3.76)  Time: 0.779s, 1314.27/s  (0.794s, 1289.45/s)  LR: 5.154e-04  Data: 0.011 (0.014)
Train: 148 [ 750/1251 ( 60%)]  Loss: 3.954 (3.77)  Time: 0.827s, 1237.56/s  (0.794s, 1289.84/s)  LR: 5.154e-04  Data: 0.011 (0.014)
Train: 148 [ 800/1251 ( 64%)]  Loss: 3.778 (3.77)  Time: 0.779s, 1315.04/s  (0.794s, 1290.48/s)  LR: 5.154e-04  Data: 0.011 (0.014)
Train: 148 [ 850/1251 ( 68%)]  Loss: 4.173 (3.79)  Time: 0.780s, 1312.76/s  (0.793s, 1291.62/s)  LR: 5.154e-04  Data: 0.011 (0.013)
Train: 148 [ 900/1251 ( 72%)]  Loss: 3.837 (3.80)  Time: 0.819s, 1250.42/s  (0.793s, 1291.76/s)  LR: 5.154e-04  Data: 0.011 (0.013)
Train: 148 [ 950/1251 ( 76%)]  Loss: 3.469 (3.78)  Time: 0.823s, 1243.80/s  (0.793s, 1291.78/s)  LR: 5.154e-04  Data: 0.011 (0.013)
Train: 148 [1000/1251 ( 80%)]  Loss: 3.430 (3.76)  Time: 0.776s, 1319.11/s  (0.792s, 1292.51/s)  LR: 5.154e-04  Data: 0.011 (0.013)
Train: 148 [1050/1251 ( 84%)]  Loss: 4.030 (3.77)  Time: 0.779s, 1313.98/s  (0.792s, 1292.59/s)  LR: 5.154e-04  Data: 0.011 (0.013)
Train: 148 [1100/1251 ( 88%)]  Loss: 3.508 (3.76)  Time: 0.817s, 1253.99/s  (0.793s, 1291.75/s)  LR: 5.154e-04  Data: 0.011 (0.013)
Train: 148 [1150/1251 ( 92%)]  Loss: 3.685 (3.76)  Time: 0.793s, 1292.05/s  (0.793s, 1291.84/s)  LR: 5.154e-04  Data: 0.010 (0.013)
Train: 148 [1200/1251 ( 96%)]  Loss: 3.754 (3.76)  Time: 0.782s, 1310.29/s  (0.792s, 1292.17/s)  LR: 5.154e-04  Data: 0.011 (0.013)
Train: 148 [1250/1251 (100%)]  Loss: 3.292 (3.74)  Time: 0.770s, 1329.02/s  (0.792s, 1292.19/s)  LR: 5.154e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.573 (1.573)  Loss:  0.9071 (0.9071)  Acc@1: 88.5742 (88.5742)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.172 (0.560)  Loss:  0.8783 (1.4517)  Acc@1: 83.7264 (72.3980)  Acc@5: 96.1085 (90.9560)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-148.pth.tar', 72.3980000415039)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-145.pth.tar', 72.30800006591797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-147.pth.tar', 72.18399999023437)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-146.pth.tar', 72.00400007324218)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-144.pth.tar', 71.91400004638672)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-132.pth.tar', 71.79800006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-128.pth.tar', 71.71800006347657)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-137.pth.tar', 71.70800007080078)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-142.pth.tar', 71.6920001171875)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-138.pth.tar', 71.50600006347656)

Train: 149 [   0/1251 (  0%)]  Loss: 4.275 (4.27)  Time: 2.294s,  446.43/s  (2.294s,  446.43/s)  LR: 5.102e-04  Data: 1.556 (1.556)
Train: 149 [  50/1251 (  4%)]  Loss: 3.508 (3.89)  Time: 0.876s, 1168.92/s  (0.842s, 1215.89/s)  LR: 5.102e-04  Data: 0.012 (0.046)
Train: 149 [ 100/1251 (  8%)]  Loss: 3.581 (3.79)  Time: 0.779s, 1314.09/s  (0.820s, 1248.16/s)  LR: 5.102e-04  Data: 0.011 (0.029)
Train: 149 [ 150/1251 ( 12%)]  Loss: 3.223 (3.65)  Time: 0.779s, 1314.34/s  (0.810s, 1264.83/s)  LR: 5.102e-04  Data: 0.011 (0.023)
Train: 149 [ 200/1251 ( 16%)]  Loss: 3.669 (3.65)  Time: 0.819s, 1250.55/s  (0.808s, 1266.62/s)  LR: 5.102e-04  Data: 0.012 (0.020)
Train: 149 [ 250/1251 ( 20%)]  Loss: 3.589 (3.64)  Time: 0.778s, 1316.71/s  (0.804s, 1273.52/s)  LR: 5.102e-04  Data: 0.011 (0.018)
Train: 149 [ 300/1251 ( 24%)]  Loss: 3.948 (3.68)  Time: 0.778s, 1316.28/s  (0.801s, 1278.97/s)  LR: 5.102e-04  Data: 0.011 (0.017)
Train: 149 [ 350/1251 ( 28%)]  Loss: 3.731 (3.69)  Time: 0.777s, 1317.36/s  (0.799s, 1280.84/s)  LR: 5.102e-04  Data: 0.012 (0.016)
Train: 149 [ 400/1251 ( 32%)]  Loss: 4.175 (3.74)  Time: 0.778s, 1316.49/s  (0.798s, 1283.13/s)  LR: 5.102e-04  Data: 0.011 (0.016)
Train: 149 [ 450/1251 ( 36%)]  Loss: 3.657 (3.74)  Time: 0.792s, 1292.23/s  (0.800s, 1280.44/s)  LR: 5.102e-04  Data: 0.011 (0.015)
Train: 149 [ 500/1251 ( 40%)]  Loss: 3.817 (3.74)  Time: 0.840s, 1219.10/s  (0.799s, 1281.55/s)  LR: 5.102e-04  Data: 0.011 (0.015)
Train: 149 [ 550/1251 ( 44%)]  Loss: 3.877 (3.75)  Time: 0.789s, 1298.32/s  (0.798s, 1282.98/s)  LR: 5.102e-04  Data: 0.011 (0.014)
Train: 149 [ 600/1251 ( 48%)]  Loss: 3.484 (3.73)  Time: 0.779s, 1315.27/s  (0.800s, 1280.77/s)  LR: 5.102e-04  Data: 0.011 (0.014)
Train: 149 [ 650/1251 ( 52%)]  Loss: 3.938 (3.75)  Time: 0.818s, 1252.16/s  (0.799s, 1281.81/s)  LR: 5.102e-04  Data: 0.011 (0.014)
Train: 149 [ 700/1251 ( 56%)]  Loss: 4.027 (3.77)  Time: 0.781s, 1310.48/s  (0.798s, 1282.95/s)  LR: 5.102e-04  Data: 0.011 (0.014)
Train: 149 [ 750/1251 ( 60%)]  Loss: 3.998 (3.78)  Time: 0.831s, 1231.56/s  (0.797s, 1284.42/s)  LR: 5.102e-04  Data: 0.012 (0.014)
Train: 149 [ 800/1251 ( 64%)]  Loss: 3.665 (3.77)  Time: 0.814s, 1257.42/s  (0.798s, 1283.72/s)  LR: 5.102e-04  Data: 0.012 (0.013)
Train: 149 [ 850/1251 ( 68%)]  Loss: 3.514 (3.76)  Time: 0.789s, 1297.42/s  (0.798s, 1283.49/s)  LR: 5.102e-04  Data: 0.011 (0.013)
Train: 149 [ 900/1251 ( 72%)]  Loss: 3.540 (3.75)  Time: 0.781s, 1310.99/s  (0.798s, 1283.90/s)  LR: 5.102e-04  Data: 0.011 (0.013)
Train: 149 [ 950/1251 ( 76%)]  Loss: 3.812 (3.75)  Time: 0.825s, 1241.50/s  (0.797s, 1284.82/s)  LR: 5.102e-04  Data: 0.011 (0.013)
Train: 149 [1000/1251 ( 80%)]  Loss: 3.685 (3.75)  Time: 0.814s, 1258.14/s  (0.798s, 1283.70/s)  LR: 5.102e-04  Data: 0.011 (0.013)
Train: 149 [1050/1251 ( 84%)]  Loss: 4.007 (3.76)  Time: 0.802s, 1277.20/s  (0.798s, 1283.51/s)  LR: 5.102e-04  Data: 0.011 (0.013)
Train: 149 [1100/1251 ( 88%)]  Loss: 3.836 (3.76)  Time: 0.779s, 1314.27/s  (0.797s, 1284.05/s)  LR: 5.102e-04  Data: 0.011 (0.013)
Train: 149 [1150/1251 ( 92%)]  Loss: 3.629 (3.76)  Time: 0.793s, 1291.83/s  (0.797s, 1285.12/s)  LR: 5.102e-04  Data: 0.012 (0.013)
Train: 149 [1200/1251 ( 96%)]  Loss: 3.459 (3.75)  Time: 0.781s, 1311.90/s  (0.797s, 1285.06/s)  LR: 5.102e-04  Data: 0.011 (0.013)
Train: 149 [1250/1251 (100%)]  Loss: 3.881 (3.75)  Time: 0.768s, 1333.34/s  (0.796s, 1285.65/s)  LR: 5.102e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.572 (1.572)  Loss:  0.9798 (0.9798)  Acc@1: 86.5234 (86.5234)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.172 (0.567)  Loss:  1.0492 (1.5504)  Acc@1: 82.3113 (71.2500)  Acc@5: 95.5189 (90.4120)
Train: 150 [   0/1251 (  0%)]  Loss: 4.055 (4.05)  Time: 2.314s,  442.47/s  (2.314s,  442.47/s)  LR: 5.050e-04  Data: 1.463 (1.463)
Train: 150 [  50/1251 (  4%)]  Loss: 3.773 (3.91)  Time: 0.780s, 1312.25/s  (0.822s, 1245.44/s)  LR: 5.050e-04  Data: 0.011 (0.044)
Train: 150 [ 100/1251 (  8%)]  Loss: 3.795 (3.87)  Time: 0.836s, 1225.10/s  (0.804s, 1273.61/s)  LR: 5.050e-04  Data: 0.011 (0.027)
Train: 150 [ 150/1251 ( 12%)]  Loss: 3.741 (3.84)  Time: 0.779s, 1314.25/s  (0.800s, 1280.56/s)  LR: 5.050e-04  Data: 0.011 (0.022)
Train: 150 [ 200/1251 ( 16%)]  Loss: 3.690 (3.81)  Time: 0.816s, 1254.73/s  (0.803s, 1275.31/s)  LR: 5.050e-04  Data: 0.012 (0.019)
Train: 150 [ 250/1251 ( 20%)]  Loss: 3.729 (3.80)  Time: 0.783s, 1308.50/s  (0.800s, 1279.49/s)  LR: 5.050e-04  Data: 0.011 (0.018)
Train: 150 [ 300/1251 ( 24%)]  Loss: 3.841 (3.80)  Time: 0.777s, 1317.77/s  (0.799s, 1281.89/s)  LR: 5.050e-04  Data: 0.011 (0.017)
Train: 150 [ 350/1251 ( 28%)]  Loss: 3.791 (3.80)  Time: 0.785s, 1304.66/s  (0.798s, 1283.09/s)  LR: 5.050e-04  Data: 0.014 (0.016)
Train: 150 [ 400/1251 ( 32%)]  Loss: 3.442 (3.76)  Time: 0.776s, 1320.34/s  (0.798s, 1283.96/s)  LR: 5.050e-04  Data: 0.011 (0.015)
Train: 150 [ 450/1251 ( 36%)]  Loss: 3.956 (3.78)  Time: 0.779s, 1315.03/s  (0.796s, 1285.76/s)  LR: 5.050e-04  Data: 0.011 (0.015)
Train: 150 [ 500/1251 ( 40%)]  Loss: 3.722 (3.78)  Time: 0.777s, 1318.49/s  (0.795s, 1287.57/s)  LR: 5.050e-04  Data: 0.011 (0.014)
Train: 150 [ 550/1251 ( 44%)]  Loss: 4.121 (3.80)  Time: 0.784s, 1305.37/s  (0.794s, 1288.87/s)  LR: 5.050e-04  Data: 0.011 (0.014)
Train: 150 [ 600/1251 ( 48%)]  Loss: 3.872 (3.81)  Time: 0.775s, 1321.88/s  (0.794s, 1289.99/s)  LR: 5.050e-04  Data: 0.011 (0.014)
Train: 150 [ 650/1251 ( 52%)]  Loss: 3.855 (3.81)  Time: 0.778s, 1315.45/s  (0.795s, 1288.67/s)  LR: 5.050e-04  Data: 0.012 (0.014)
Train: 150 [ 700/1251 ( 56%)]  Loss: 3.972 (3.82)  Time: 0.777s, 1318.21/s  (0.795s, 1288.16/s)  LR: 5.050e-04  Data: 0.011 (0.014)
Train: 150 [ 750/1251 ( 60%)]  Loss: 4.137 (3.84)  Time: 0.783s, 1307.86/s  (0.794s, 1289.22/s)  LR: 5.050e-04  Data: 0.011 (0.013)
Train: 150 [ 800/1251 ( 64%)]  Loss: 3.706 (3.84)  Time: 0.779s, 1313.93/s  (0.795s, 1288.43/s)  LR: 5.050e-04  Data: 0.011 (0.013)
Train: 150 [ 850/1251 ( 68%)]  Loss: 3.787 (3.83)  Time: 0.836s, 1224.97/s  (0.795s, 1288.59/s)  LR: 5.050e-04  Data: 0.012 (0.013)
Train: 150 [ 900/1251 ( 72%)]  Loss: 3.545 (3.82)  Time: 0.793s, 1291.29/s  (0.794s, 1289.21/s)  LR: 5.050e-04  Data: 0.013 (0.013)
Train: 150 [ 950/1251 ( 76%)]  Loss: 3.497 (3.80)  Time: 0.778s, 1316.40/s  (0.794s, 1289.02/s)  LR: 5.050e-04  Data: 0.011 (0.013)
Train: 150 [1000/1251 ( 80%)]  Loss: 3.789 (3.80)  Time: 0.778s, 1315.65/s  (0.794s, 1289.85/s)  LR: 5.050e-04  Data: 0.011 (0.013)
Train: 150 [1050/1251 ( 84%)]  Loss: 3.422 (3.78)  Time: 0.791s, 1293.87/s  (0.794s, 1289.58/s)  LR: 5.050e-04  Data: 0.014 (0.013)
Train: 150 [1100/1251 ( 88%)]  Loss: 4.023 (3.79)  Time: 0.785s, 1305.25/s  (0.794s, 1289.97/s)  LR: 5.050e-04  Data: 0.011 (0.013)
Train: 150 [1150/1251 ( 92%)]  Loss: 3.786 (3.79)  Time: 0.777s, 1317.10/s  (0.794s, 1289.78/s)  LR: 5.050e-04  Data: 0.011 (0.013)
Train: 150 [1200/1251 ( 96%)]  Loss: 3.747 (3.79)  Time: 0.836s, 1224.19/s  (0.794s, 1290.10/s)  LR: 5.050e-04  Data: 0.011 (0.013)
Train: 150 [1250/1251 (100%)]  Loss: 3.653 (3.79)  Time: 0.817s, 1253.43/s  (0.794s, 1290.45/s)  LR: 5.050e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.513 (1.513)  Loss:  0.7471 (0.7471)  Acc@1: 88.2812 (88.2812)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.172 (0.579)  Loss:  0.8573 (1.3572)  Acc@1: 83.1368 (72.3280)  Acc@5: 96.3444 (91.0580)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-148.pth.tar', 72.3980000415039)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-150.pth.tar', 72.32800004394531)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-145.pth.tar', 72.30800006591797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-147.pth.tar', 72.18399999023437)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-146.pth.tar', 72.00400007324218)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-144.pth.tar', 71.91400004638672)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-132.pth.tar', 71.79800006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-128.pth.tar', 71.71800006347657)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-137.pth.tar', 71.70800007080078)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-142.pth.tar', 71.6920001171875)

Train: 151 [   0/1251 (  0%)]  Loss: 3.684 (3.68)  Time: 2.234s,  458.33/s  (2.234s,  458.33/s)  LR: 4.998e-04  Data: 1.501 (1.501)
Train: 151 [  50/1251 (  4%)]  Loss: 3.463 (3.57)  Time: 0.781s, 1311.34/s  (0.819s, 1250.95/s)  LR: 4.998e-04  Data: 0.012 (0.047)
Train: 151 [ 100/1251 (  8%)]  Loss: 3.578 (3.58)  Time: 0.820s, 1248.64/s  (0.804s, 1273.42/s)  LR: 4.998e-04  Data: 0.011 (0.030)
Train: 151 [ 150/1251 ( 12%)]  Loss: 3.731 (3.61)  Time: 0.778s, 1315.60/s  (0.800s, 1279.79/s)  LR: 4.998e-04  Data: 0.012 (0.023)
Train: 151 [ 200/1251 ( 16%)]  Loss: 3.459 (3.58)  Time: 0.788s, 1300.32/s  (0.798s, 1283.58/s)  LR: 4.998e-04  Data: 0.011 (0.020)
Train: 151 [ 250/1251 ( 20%)]  Loss: 3.480 (3.57)  Time: 0.780s, 1312.76/s  (0.795s, 1287.48/s)  LR: 4.998e-04  Data: 0.012 (0.019)
Train: 151 [ 300/1251 ( 24%)]  Loss: 3.638 (3.58)  Time: 0.777s, 1317.49/s  (0.793s, 1291.38/s)  LR: 4.998e-04  Data: 0.011 (0.017)
Train: 151 [ 350/1251 ( 28%)]  Loss: 3.699 (3.59)  Time: 0.802s, 1276.40/s  (0.792s, 1292.55/s)  LR: 4.998e-04  Data: 0.010 (0.016)
Train: 151 [ 400/1251 ( 32%)]  Loss: 3.446 (3.58)  Time: 0.779s, 1314.47/s  (0.792s, 1293.59/s)  LR: 4.998e-04  Data: 0.011 (0.016)
Train: 151 [ 450/1251 ( 36%)]  Loss: 3.819 (3.60)  Time: 0.777s, 1317.64/s  (0.791s, 1294.52/s)  LR: 4.998e-04  Data: 0.010 (0.015)
Train: 151 [ 500/1251 ( 40%)]  Loss: 3.502 (3.59)  Time: 0.787s, 1300.62/s  (0.791s, 1294.42/s)  LR: 4.998e-04  Data: 0.012 (0.015)
Train: 151 [ 550/1251 ( 44%)]  Loss: 3.895 (3.62)  Time: 0.778s, 1316.66/s  (0.791s, 1294.72/s)  LR: 4.998e-04  Data: 0.011 (0.015)
Train: 151 [ 600/1251 ( 48%)]  Loss: 3.897 (3.64)  Time: 0.784s, 1306.39/s  (0.790s, 1295.71/s)  LR: 4.998e-04  Data: 0.011 (0.014)
Train: 151 [ 650/1251 ( 52%)]  Loss: 3.768 (3.65)  Time: 0.780s, 1312.47/s  (0.790s, 1296.03/s)  LR: 4.998e-04  Data: 0.011 (0.014)
Train: 151 [ 700/1251 ( 56%)]  Loss: 3.718 (3.65)  Time: 0.801s, 1278.90/s  (0.790s, 1296.33/s)  LR: 4.998e-04  Data: 0.012 (0.014)
Train: 151 [ 750/1251 ( 60%)]  Loss: 3.909 (3.67)  Time: 0.781s, 1311.62/s  (0.790s, 1296.88/s)  LR: 4.998e-04  Data: 0.011 (0.014)
Train: 151 [ 800/1251 ( 64%)]  Loss: 3.825 (3.68)  Time: 0.778s, 1316.71/s  (0.789s, 1297.60/s)  LR: 4.998e-04  Data: 0.011 (0.014)
Train: 151 [ 850/1251 ( 68%)]  Loss: 3.486 (3.67)  Time: 0.782s, 1309.84/s  (0.789s, 1297.13/s)  LR: 4.998e-04  Data: 0.011 (0.013)
Train: 151 [ 900/1251 ( 72%)]  Loss: 3.549 (3.66)  Time: 0.777s, 1317.89/s  (0.789s, 1297.91/s)  LR: 4.998e-04  Data: 0.011 (0.013)
Train: 151 [ 950/1251 ( 76%)]  Loss: 3.710 (3.66)  Time: 0.780s, 1313.62/s  (0.789s, 1298.27/s)  LR: 4.998e-04  Data: 0.011 (0.013)
Train: 151 [1000/1251 ( 80%)]  Loss: 3.831 (3.67)  Time: 0.780s, 1313.47/s  (0.789s, 1298.22/s)  LR: 4.998e-04  Data: 0.012 (0.013)
Train: 151 [1050/1251 ( 84%)]  Loss: 3.979 (3.68)  Time: 0.781s, 1311.95/s  (0.789s, 1298.26/s)  LR: 4.998e-04  Data: 0.011 (0.013)
Train: 151 [1100/1251 ( 88%)]  Loss: 3.111 (3.66)  Time: 0.780s, 1312.06/s  (0.789s, 1298.61/s)  LR: 4.998e-04  Data: 0.011 (0.013)
Train: 151 [1150/1251 ( 92%)]  Loss: 3.959 (3.67)  Time: 0.780s, 1313.31/s  (0.789s, 1298.66/s)  LR: 4.998e-04  Data: 0.011 (0.013)
Train: 151 [1200/1251 ( 96%)]  Loss: 3.897 (3.68)  Time: 0.778s, 1316.61/s  (0.789s, 1297.81/s)  LR: 4.998e-04  Data: 0.011 (0.013)
Train: 151 [1250/1251 (100%)]  Loss: 3.631 (3.68)  Time: 0.771s, 1327.96/s  (0.789s, 1297.47/s)  LR: 4.998e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.515 (1.515)  Loss:  0.7875 (0.7875)  Acc@1: 88.4766 (88.4766)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.172 (0.579)  Loss:  0.9413 (1.4750)  Acc@1: 83.3726 (72.6280)  Acc@5: 95.8726 (91.2000)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-151.pth.tar', 72.62800001708985)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-148.pth.tar', 72.3980000415039)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-150.pth.tar', 72.32800004394531)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-145.pth.tar', 72.30800006591797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-147.pth.tar', 72.18399999023437)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-146.pth.tar', 72.00400007324218)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-144.pth.tar', 71.91400004638672)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-132.pth.tar', 71.79800006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-128.pth.tar', 71.71800006347657)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-137.pth.tar', 71.70800007080078)

Train: 152 [   0/1251 (  0%)]  Loss: 3.730 (3.73)  Time: 2.374s,  431.28/s  (2.374s,  431.28/s)  LR: 4.946e-04  Data: 1.638 (1.638)
Train: 152 [  50/1251 (  4%)]  Loss: 3.704 (3.72)  Time: 0.781s, 1311.37/s  (0.826s, 1240.41/s)  LR: 4.946e-04  Data: 0.011 (0.048)
Train: 152 [ 100/1251 (  8%)]  Loss: 4.121 (3.85)  Time: 0.778s, 1315.45/s  (0.814s, 1258.61/s)  LR: 4.946e-04  Data: 0.011 (0.030)
Train: 152 [ 150/1251 ( 12%)]  Loss: 3.773 (3.83)  Time: 0.805s, 1272.24/s  (0.807s, 1269.26/s)  LR: 4.946e-04  Data: 0.011 (0.024)
Train: 152 [ 200/1251 ( 16%)]  Loss: 3.460 (3.76)  Time: 0.825s, 1241.75/s  (0.805s, 1272.24/s)  LR: 4.946e-04  Data: 0.012 (0.021)
Train: 152 [ 250/1251 ( 20%)]  Loss: 3.467 (3.71)  Time: 0.785s, 1303.77/s  (0.801s, 1278.42/s)  LR: 4.946e-04  Data: 0.011 (0.019)
Train: 152 [ 300/1251 ( 24%)]  Loss: 3.731 (3.71)  Time: 0.778s, 1315.56/s  (0.800s, 1279.88/s)  LR: 4.946e-04  Data: 0.011 (0.018)
Train: 152 [ 350/1251 ( 28%)]  Loss: 3.514 (3.69)  Time: 0.776s, 1319.41/s  (0.799s, 1282.14/s)  LR: 4.946e-04  Data: 0.011 (0.017)
Train: 152 [ 400/1251 ( 32%)]  Loss: 3.572 (3.67)  Time: 0.778s, 1315.87/s  (0.797s, 1284.91/s)  LR: 4.946e-04  Data: 0.012 (0.016)
Train: 152 [ 450/1251 ( 36%)]  Loss: 3.625 (3.67)  Time: 0.840s, 1218.79/s  (0.797s, 1284.99/s)  LR: 4.946e-04  Data: 0.011 (0.015)
Train: 152 [ 500/1251 ( 40%)]  Loss: 3.506 (3.65)  Time: 0.808s, 1267.19/s  (0.798s, 1282.83/s)  LR: 4.946e-04  Data: 0.010 (0.015)
Train: 152 [ 550/1251 ( 44%)]  Loss: 3.656 (3.65)  Time: 0.778s, 1315.74/s  (0.799s, 1281.17/s)  LR: 4.946e-04  Data: 0.012 (0.015)
Train: 152 [ 600/1251 ( 48%)]  Loss: 3.338 (3.63)  Time: 0.788s, 1299.22/s  (0.798s, 1282.75/s)  LR: 4.946e-04  Data: 0.011 (0.014)
Train: 152 [ 650/1251 ( 52%)]  Loss: 4.024 (3.66)  Time: 0.779s, 1314.49/s  (0.797s, 1284.56/s)  LR: 4.946e-04  Data: 0.012 (0.014)
Train: 152 [ 700/1251 ( 56%)]  Loss: 3.398 (3.64)  Time: 0.816s, 1255.07/s  (0.797s, 1285.31/s)  LR: 4.946e-04  Data: 0.011 (0.014)
Train: 152 [ 750/1251 ( 60%)]  Loss: 3.382 (3.63)  Time: 0.815s, 1256.21/s  (0.798s, 1283.43/s)  LR: 4.946e-04  Data: 0.011 (0.014)
Train: 152 [ 800/1251 ( 64%)]  Loss: 3.634 (3.63)  Time: 0.777s, 1317.95/s  (0.798s, 1283.54/s)  LR: 4.946e-04  Data: 0.010 (0.014)
Train: 152 [ 850/1251 ( 68%)]  Loss: 3.944 (3.64)  Time: 0.788s, 1299.59/s  (0.798s, 1283.12/s)  LR: 4.946e-04  Data: 0.010 (0.013)
Train: 152 [ 900/1251 ( 72%)]  Loss: 3.863 (3.65)  Time: 0.788s, 1299.01/s  (0.797s, 1284.64/s)  LR: 4.946e-04  Data: 0.011 (0.013)
Train: 152 [ 950/1251 ( 76%)]  Loss: 3.572 (3.65)  Time: 0.780s, 1312.87/s  (0.797s, 1285.22/s)  LR: 4.946e-04  Data: 0.011 (0.013)
Train: 152 [1000/1251 ( 80%)]  Loss: 3.931 (3.66)  Time: 0.778s, 1315.47/s  (0.797s, 1285.11/s)  LR: 4.946e-04  Data: 0.010 (0.013)
Train: 152 [1050/1251 ( 84%)]  Loss: 4.027 (3.68)  Time: 0.815s, 1256.45/s  (0.797s, 1284.58/s)  LR: 4.946e-04  Data: 0.011 (0.013)
Train: 152 [1100/1251 ( 88%)]  Loss: 3.830 (3.69)  Time: 0.785s, 1304.87/s  (0.797s, 1284.71/s)  LR: 4.946e-04  Data: 0.011 (0.013)
Train: 152 [1150/1251 ( 92%)]  Loss: 3.770 (3.69)  Time: 0.782s, 1309.96/s  (0.797s, 1285.51/s)  LR: 4.946e-04  Data: 0.011 (0.013)
Train: 152 [1200/1251 ( 96%)]  Loss: 3.805 (3.70)  Time: 0.777s, 1317.96/s  (0.797s, 1284.99/s)  LR: 4.946e-04  Data: 0.011 (0.013)
Train: 152 [1250/1251 (100%)]  Loss: 3.463 (3.69)  Time: 0.828s, 1236.12/s  (0.797s, 1285.12/s)  LR: 4.946e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.545 (1.545)  Loss:  0.8629 (0.8629)  Acc@1: 88.1836 (88.1836)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.172 (0.569)  Loss:  0.9607 (1.4589)  Acc@1: 82.9009 (72.2420)  Acc@5: 96.1085 (91.0160)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-151.pth.tar', 72.62800001708985)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-148.pth.tar', 72.3980000415039)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-150.pth.tar', 72.32800004394531)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-145.pth.tar', 72.30800006591797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-152.pth.tar', 72.24199994140625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-147.pth.tar', 72.18399999023437)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-146.pth.tar', 72.00400007324218)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-144.pth.tar', 71.91400004638672)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-132.pth.tar', 71.79800006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-128.pth.tar', 71.71800006347657)

Train: 153 [   0/1251 (  0%)]  Loss: 3.741 (3.74)  Time: 2.365s,  432.92/s  (2.365s,  432.92/s)  LR: 4.895e-04  Data: 1.629 (1.629)
Train: 153 [  50/1251 (  4%)]  Loss: 4.216 (3.98)  Time: 0.879s, 1165.43/s  (0.825s, 1241.50/s)  LR: 4.895e-04  Data: 0.011 (0.045)
Train: 153 [ 100/1251 (  8%)]  Loss: 3.511 (3.82)  Time: 0.814s, 1257.91/s  (0.815s, 1256.96/s)  LR: 4.895e-04  Data: 0.011 (0.028)
Train: 153 [ 150/1251 ( 12%)]  Loss: 3.521 (3.75)  Time: 0.842s, 1215.99/s  (0.809s, 1265.34/s)  LR: 4.895e-04  Data: 0.011 (0.023)
Train: 153 [ 200/1251 ( 16%)]  Loss: 3.087 (3.62)  Time: 0.782s, 1309.97/s  (0.807s, 1269.09/s)  LR: 4.895e-04  Data: 0.011 (0.020)
Train: 153 [ 250/1251 ( 20%)]  Loss: 3.720 (3.63)  Time: 0.779s, 1314.19/s  (0.803s, 1275.28/s)  LR: 4.895e-04  Data: 0.011 (0.018)
Train: 153 [ 300/1251 ( 24%)]  Loss: 4.092 (3.70)  Time: 0.781s, 1311.30/s  (0.800s, 1279.40/s)  LR: 4.895e-04  Data: 0.011 (0.017)
Train: 153 [ 350/1251 ( 28%)]  Loss: 3.633 (3.69)  Time: 0.780s, 1313.46/s  (0.797s, 1284.13/s)  LR: 4.895e-04  Data: 0.011 (0.016)
Train: 153 [ 400/1251 ( 32%)]  Loss: 3.522 (3.67)  Time: 0.780s, 1312.91/s  (0.797s, 1285.07/s)  LR: 4.895e-04  Data: 0.011 (0.015)
Train: 153 [ 450/1251 ( 36%)]  Loss: 3.969 (3.70)  Time: 0.819s, 1251.05/s  (0.797s, 1284.05/s)  LR: 4.895e-04  Data: 0.011 (0.015)
Train: 153 [ 500/1251 ( 40%)]  Loss: 3.415 (3.68)  Time: 0.779s, 1313.78/s  (0.799s, 1281.33/s)  LR: 4.895e-04  Data: 0.012 (0.014)
Train: 153 [ 550/1251 ( 44%)]  Loss: 3.839 (3.69)  Time: 0.811s, 1263.38/s  (0.799s, 1281.57/s)  LR: 4.895e-04  Data: 0.011 (0.014)
Train: 153 [ 600/1251 ( 48%)]  Loss: 4.149 (3.72)  Time: 0.781s, 1311.90/s  (0.799s, 1281.83/s)  LR: 4.895e-04  Data: 0.011 (0.014)
Train: 153 [ 650/1251 ( 52%)]  Loss: 3.874 (3.73)  Time: 0.814s, 1257.50/s  (0.798s, 1283.04/s)  LR: 4.895e-04  Data: 0.011 (0.014)
Train: 153 [ 700/1251 ( 56%)]  Loss: 4.028 (3.75)  Time: 0.786s, 1302.12/s  (0.798s, 1282.91/s)  LR: 4.895e-04  Data: 0.010 (0.014)
Train: 153 [ 750/1251 ( 60%)]  Loss: 3.668 (3.75)  Time: 0.776s, 1320.03/s  (0.797s, 1284.56/s)  LR: 4.895e-04  Data: 0.011 (0.013)
Train: 153 [ 800/1251 ( 64%)]  Loss: 3.862 (3.76)  Time: 0.820s, 1248.60/s  (0.798s, 1282.47/s)  LR: 4.895e-04  Data: 0.010 (0.013)
Train: 153 [ 850/1251 ( 68%)]  Loss: 3.739 (3.75)  Time: 0.849s, 1206.30/s  (0.798s, 1283.51/s)  LR: 4.895e-04  Data: 0.011 (0.013)
Train: 153 [ 900/1251 ( 72%)]  Loss: 3.717 (3.75)  Time: 0.778s, 1315.99/s  (0.797s, 1284.26/s)  LR: 4.895e-04  Data: 0.011 (0.013)
Train: 153 [ 950/1251 ( 76%)]  Loss: 3.775 (3.75)  Time: 0.779s, 1314.53/s  (0.797s, 1284.60/s)  LR: 4.895e-04  Data: 0.011 (0.013)
Train: 153 [1000/1251 ( 80%)]  Loss: 3.762 (3.75)  Time: 0.813s, 1260.21/s  (0.798s, 1283.22/s)  LR: 4.895e-04  Data: 0.011 (0.013)
Train: 153 [1050/1251 ( 84%)]  Loss: 3.990 (3.77)  Time: 0.776s, 1319.87/s  (0.798s, 1283.09/s)  LR: 4.895e-04  Data: 0.011 (0.013)
Train: 153 [1100/1251 ( 88%)]  Loss: 3.885 (3.77)  Time: 0.779s, 1315.25/s  (0.798s, 1283.70/s)  LR: 4.895e-04  Data: 0.011 (0.013)
Train: 153 [1150/1251 ( 92%)]  Loss: 4.018 (3.78)  Time: 0.819s, 1250.38/s  (0.798s, 1283.06/s)  LR: 4.895e-04  Data: 0.011 (0.013)
Train: 153 [1200/1251 ( 96%)]  Loss: 3.512 (3.77)  Time: 0.778s, 1315.95/s  (0.798s, 1283.03/s)  LR: 4.895e-04  Data: 0.011 (0.012)
Train: 153 [1250/1251 (100%)]  Loss: 3.931 (3.78)  Time: 0.800s, 1280.54/s  (0.798s, 1283.53/s)  LR: 4.895e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.577 (1.577)  Loss:  1.0769 (1.0769)  Acc@1: 87.5977 (87.5977)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.172 (0.561)  Loss:  0.9985 (1.5185)  Acc@1: 82.4293 (72.1260)  Acc@5: 95.5189 (91.0560)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-151.pth.tar', 72.62800001708985)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-148.pth.tar', 72.3980000415039)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-150.pth.tar', 72.32800004394531)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-145.pth.tar', 72.30800006591797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-152.pth.tar', 72.24199994140625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-147.pth.tar', 72.18399999023437)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-153.pth.tar', 72.12600012451172)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-146.pth.tar', 72.00400007324218)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-144.pth.tar', 71.91400004638672)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-132.pth.tar', 71.79800006347656)

Train: 154 [   0/1251 (  0%)]  Loss: 3.702 (3.70)  Time: 2.391s,  428.33/s  (2.391s,  428.33/s)  LR: 4.843e-04  Data: 1.656 (1.656)
Train: 154 [  50/1251 (  4%)]  Loss: 3.737 (3.72)  Time: 0.778s, 1316.53/s  (0.828s, 1236.18/s)  LR: 4.843e-04  Data: 0.012 (0.044)
Train: 154 [ 100/1251 (  8%)]  Loss: 3.628 (3.69)  Time: 0.809s, 1265.99/s  (0.814s, 1257.59/s)  LR: 4.843e-04  Data: 0.011 (0.028)
Train: 154 [ 150/1251 ( 12%)]  Loss: 3.831 (3.72)  Time: 0.807s, 1268.15/s  (0.804s, 1273.70/s)  LR: 4.843e-04  Data: 0.011 (0.022)
Train: 154 [ 200/1251 ( 16%)]  Loss: 3.563 (3.69)  Time: 0.778s, 1316.71/s  (0.806s, 1270.87/s)  LR: 4.843e-04  Data: 0.011 (0.019)
Train: 154 [ 250/1251 ( 20%)]  Loss: 3.868 (3.72)  Time: 0.780s, 1313.34/s  (0.802s, 1277.27/s)  LR: 4.843e-04  Data: 0.011 (0.018)
Train: 154 [ 300/1251 ( 24%)]  Loss: 3.701 (3.72)  Time: 0.789s, 1297.90/s  (0.798s, 1282.58/s)  LR: 4.843e-04  Data: 0.011 (0.017)
Train: 154 [ 350/1251 ( 28%)]  Loss: 3.611 (3.71)  Time: 0.792s, 1292.79/s  (0.796s, 1285.96/s)  LR: 4.843e-04  Data: 0.010 (0.016)
Train: 154 [ 400/1251 ( 32%)]  Loss: 3.307 (3.66)  Time: 0.782s, 1310.25/s  (0.795s, 1287.57/s)  LR: 4.843e-04  Data: 0.011 (0.015)
Train: 154 [ 450/1251 ( 36%)]  Loss: 3.339 (3.63)  Time: 0.776s, 1319.48/s  (0.794s, 1289.37/s)  LR: 4.843e-04  Data: 0.011 (0.015)
Train: 154 [ 500/1251 ( 40%)]  Loss: 3.415 (3.61)  Time: 0.822s, 1245.91/s  (0.795s, 1288.50/s)  LR: 4.843e-04  Data: 0.011 (0.014)
Train: 154 [ 550/1251 ( 44%)]  Loss: 3.648 (3.61)  Time: 0.776s, 1318.99/s  (0.794s, 1289.98/s)  LR: 4.843e-04  Data: 0.011 (0.014)
Train: 154 [ 600/1251 ( 48%)]  Loss: 3.698 (3.62)  Time: 0.775s, 1320.82/s  (0.793s, 1291.35/s)  LR: 4.843e-04  Data: 0.011 (0.014)
Train: 154 [ 650/1251 ( 52%)]  Loss: 3.587 (3.62)  Time: 0.779s, 1313.89/s  (0.792s, 1292.91/s)  LR: 4.843e-04  Data: 0.010 (0.014)
Train: 154 [ 700/1251 ( 56%)]  Loss: 3.695 (3.62)  Time: 0.778s, 1316.32/s  (0.791s, 1294.19/s)  LR: 4.843e-04  Data: 0.011 (0.013)
Train: 154 [ 750/1251 ( 60%)]  Loss: 3.588 (3.62)  Time: 0.777s, 1317.51/s  (0.791s, 1295.22/s)  LR: 4.843e-04  Data: 0.011 (0.013)
Train: 154 [ 800/1251 ( 64%)]  Loss: 3.603 (3.62)  Time: 0.862s, 1188.50/s  (0.790s, 1295.53/s)  LR: 4.843e-04  Data: 0.011 (0.013)
Train: 154 [ 850/1251 ( 68%)]  Loss: 3.590 (3.62)  Time: 0.783s, 1307.57/s  (0.790s, 1295.99/s)  LR: 4.843e-04  Data: 0.015 (0.013)
Train: 154 [ 900/1251 ( 72%)]  Loss: 3.656 (3.62)  Time: 0.779s, 1315.20/s  (0.790s, 1295.41/s)  LR: 4.843e-04  Data: 0.011 (0.013)
Train: 154 [ 950/1251 ( 76%)]  Loss: 3.689 (3.62)  Time: 0.822s, 1245.56/s  (0.791s, 1294.64/s)  LR: 4.843e-04  Data: 0.012 (0.013)
Train: 154 [1000/1251 ( 80%)]  Loss: 3.577 (3.62)  Time: 0.776s, 1319.76/s  (0.791s, 1294.47/s)  LR: 4.843e-04  Data: 0.011 (0.013)
Train: 154 [1050/1251 ( 84%)]  Loss: 3.466 (3.61)  Time: 0.819s, 1249.57/s  (0.791s, 1294.59/s)  LR: 4.843e-04  Data: 0.011 (0.013)
Train: 154 [1100/1251 ( 88%)]  Loss: 3.557 (3.61)  Time: 0.789s, 1297.77/s  (0.791s, 1293.97/s)  LR: 4.843e-04  Data: 0.011 (0.013)
Train: 154 [1150/1251 ( 92%)]  Loss: 4.082 (3.63)  Time: 0.776s, 1319.53/s  (0.791s, 1294.27/s)  LR: 4.843e-04  Data: 0.011 (0.012)
Train: 154 [1200/1251 ( 96%)]  Loss: 3.469 (3.62)  Time: 0.824s, 1242.13/s  (0.791s, 1294.26/s)  LR: 4.843e-04  Data: 0.011 (0.012)
Train: 154 [1250/1251 (100%)]  Loss: 3.772 (3.63)  Time: 0.769s, 1330.78/s  (0.791s, 1294.34/s)  LR: 4.843e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.564 (1.564)  Loss:  1.0242 (1.0242)  Acc@1: 88.3789 (88.3789)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.172 (0.572)  Loss:  1.0197 (1.6279)  Acc@1: 83.4906 (71.7180)  Acc@5: 94.9293 (90.6360)
Train: 155 [   0/1251 (  0%)]  Loss: 3.873 (3.87)  Time: 2.307s,  443.78/s  (2.307s,  443.78/s)  LR: 4.791e-04  Data: 1.572 (1.572)
Train: 155 [  50/1251 (  4%)]  Loss: 3.009 (3.44)  Time: 0.779s, 1314.63/s  (0.825s, 1241.56/s)  LR: 4.791e-04  Data: 0.011 (0.050)
Train: 155 [ 100/1251 (  8%)]  Loss: 3.611 (3.50)  Time: 0.780s, 1312.12/s  (0.804s, 1274.29/s)  LR: 4.791e-04  Data: 0.011 (0.030)
Train: 155 [ 150/1251 ( 12%)]  Loss: 3.790 (3.57)  Time: 0.779s, 1315.06/s  (0.798s, 1283.62/s)  LR: 4.791e-04  Data: 0.011 (0.024)
Train: 155 [ 200/1251 ( 16%)]  Loss: 3.872 (3.63)  Time: 0.804s, 1273.71/s  (0.803s, 1275.80/s)  LR: 4.791e-04  Data: 0.011 (0.021)
Train: 155 [ 250/1251 ( 20%)]  Loss: 3.983 (3.69)  Time: 0.781s, 1311.48/s  (0.800s, 1280.34/s)  LR: 4.791e-04  Data: 0.011 (0.019)
Train: 155 [ 300/1251 ( 24%)]  Loss: 3.529 (3.67)  Time: 0.818s, 1251.40/s  (0.798s, 1283.01/s)  LR: 4.791e-04  Data: 0.011 (0.017)
Train: 155 [ 350/1251 ( 28%)]  Loss: 3.806 (3.68)  Time: 0.781s, 1311.73/s  (0.798s, 1283.56/s)  LR: 4.791e-04  Data: 0.011 (0.017)
Train: 155 [ 400/1251 ( 32%)]  Loss: 3.459 (3.66)  Time: 0.816s, 1254.18/s  (0.797s, 1285.09/s)  LR: 4.791e-04  Data: 0.011 (0.016)
Train: 155 [ 450/1251 ( 36%)]  Loss: 3.446 (3.64)  Time: 0.778s, 1316.60/s  (0.796s, 1285.77/s)  LR: 4.791e-04  Data: 0.011 (0.015)
Train: 155 [ 500/1251 ( 40%)]  Loss: 3.521 (3.63)  Time: 0.828s, 1236.57/s  (0.797s, 1284.42/s)  LR: 4.791e-04  Data: 0.011 (0.015)
Train: 155 [ 550/1251 ( 44%)]  Loss: 4.252 (3.68)  Time: 0.775s, 1321.50/s  (0.796s, 1285.95/s)  LR: 4.791e-04  Data: 0.010 (0.015)
Train: 155 [ 600/1251 ( 48%)]  Loss: 3.867 (3.69)  Time: 0.780s, 1312.80/s  (0.796s, 1286.05/s)  LR: 4.791e-04  Data: 0.011 (0.014)
Train: 155 [ 650/1251 ( 52%)]  Loss: 3.992 (3.71)  Time: 0.778s, 1315.54/s  (0.796s, 1286.76/s)  LR: 4.791e-04  Data: 0.011 (0.014)
Train: 155 [ 700/1251 ( 56%)]  Loss: 3.808 (3.72)  Time: 0.779s, 1314.35/s  (0.795s, 1288.09/s)  LR: 4.791e-04  Data: 0.011 (0.014)
Train: 155 [ 750/1251 ( 60%)]  Loss: 3.817 (3.73)  Time: 0.779s, 1314.30/s  (0.794s, 1289.38/s)  LR: 4.791e-04  Data: 0.011 (0.014)
Train: 155 [ 800/1251 ( 64%)]  Loss: 3.877 (3.74)  Time: 0.831s, 1232.87/s  (0.795s, 1288.72/s)  LR: 4.791e-04  Data: 0.011 (0.013)
Train: 155 [ 850/1251 ( 68%)]  Loss: 3.559 (3.73)  Time: 0.809s, 1265.65/s  (0.795s, 1287.49/s)  LR: 4.791e-04  Data: 0.010 (0.013)
Train: 155 [ 900/1251 ( 72%)]  Loss: 3.915 (3.74)  Time: 0.828s, 1236.61/s  (0.796s, 1286.91/s)  LR: 4.791e-04  Data: 0.011 (0.013)
Train: 155 [ 950/1251 ( 76%)]  Loss: 3.769 (3.74)  Time: 0.780s, 1312.26/s  (0.795s, 1287.74/s)  LR: 4.791e-04  Data: 0.011 (0.013)
Train: 155 [1000/1251 ( 80%)]  Loss: 3.514 (3.73)  Time: 0.779s, 1315.06/s  (0.794s, 1288.96/s)  LR: 4.791e-04  Data: 0.010 (0.013)
Train: 155 [1050/1251 ( 84%)]  Loss: 3.848 (3.73)  Time: 0.777s, 1317.15/s  (0.794s, 1289.67/s)  LR: 4.791e-04  Data: 0.011 (0.013)
Train: 155 [1100/1251 ( 88%)]  Loss: 3.816 (3.74)  Time: 0.779s, 1314.61/s  (0.794s, 1290.28/s)  LR: 4.791e-04  Data: 0.011 (0.013)
Train: 155 [1150/1251 ( 92%)]  Loss: 3.627 (3.73)  Time: 0.780s, 1313.52/s  (0.793s, 1291.22/s)  LR: 4.791e-04  Data: 0.011 (0.013)
Train: 155 [1200/1251 ( 96%)]  Loss: 3.664 (3.73)  Time: 0.779s, 1313.92/s  (0.793s, 1291.89/s)  LR: 4.791e-04  Data: 0.012 (0.013)
Train: 155 [1250/1251 (100%)]  Loss: 3.634 (3.73)  Time: 0.769s, 1331.58/s  (0.793s, 1291.96/s)  LR: 4.791e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.513 (1.513)  Loss:  0.9329 (0.9329)  Acc@1: 88.5742 (88.5742)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.172 (0.560)  Loss:  1.0045 (1.5075)  Acc@1: 84.3160 (72.5440)  Acc@5: 96.1085 (91.0920)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-151.pth.tar', 72.62800001708985)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-155.pth.tar', 72.5440000390625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-148.pth.tar', 72.3980000415039)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-150.pth.tar', 72.32800004394531)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-145.pth.tar', 72.30800006591797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-152.pth.tar', 72.24199994140625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-147.pth.tar', 72.18399999023437)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-153.pth.tar', 72.12600012451172)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-146.pth.tar', 72.00400007324218)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-144.pth.tar', 71.91400004638672)

Train: 156 [   0/1251 (  0%)]  Loss: 3.812 (3.81)  Time: 2.294s,  446.33/s  (2.294s,  446.33/s)  LR: 4.739e-04  Data: 1.559 (1.559)
Train: 156 [  50/1251 (  4%)]  Loss: 3.932 (3.87)  Time: 0.778s, 1315.94/s  (0.835s, 1226.79/s)  LR: 4.739e-04  Data: 0.011 (0.046)
Train: 156 [ 100/1251 (  8%)]  Loss: 3.501 (3.75)  Time: 0.814s, 1257.25/s  (0.821s, 1246.88/s)  LR: 4.739e-04  Data: 0.011 (0.029)
Train: 156 [ 150/1251 ( 12%)]  Loss: 3.071 (3.58)  Time: 0.778s, 1316.26/s  (0.816s, 1255.20/s)  LR: 4.739e-04  Data: 0.011 (0.023)
Train: 156 [ 200/1251 ( 16%)]  Loss: 3.676 (3.60)  Time: 0.777s, 1317.25/s  (0.809s, 1265.13/s)  LR: 4.739e-04  Data: 0.011 (0.020)
Train: 156 [ 250/1251 ( 20%)]  Loss: 3.963 (3.66)  Time: 0.779s, 1315.26/s  (0.804s, 1273.82/s)  LR: 4.739e-04  Data: 0.011 (0.018)
Train: 156 [ 300/1251 ( 24%)]  Loss: 3.526 (3.64)  Time: 0.830s, 1233.60/s  (0.801s, 1278.16/s)  LR: 4.739e-04  Data: 0.011 (0.017)
Train: 156 [ 350/1251 ( 28%)]  Loss: 3.614 (3.64)  Time: 0.832s, 1231.41/s  (0.798s, 1282.55/s)  LR: 4.739e-04  Data: 0.011 (0.016)
Train: 156 [ 400/1251 ( 32%)]  Loss: 3.868 (3.66)  Time: 0.773s, 1323.92/s  (0.797s, 1285.26/s)  LR: 4.739e-04  Data: 0.010 (0.016)
Train: 156 [ 450/1251 ( 36%)]  Loss: 3.486 (3.64)  Time: 0.810s, 1263.48/s  (0.797s, 1284.61/s)  LR: 4.739e-04  Data: 0.011 (0.015)
Train: 156 [ 500/1251 ( 40%)]  Loss: 3.988 (3.68)  Time: 0.817s, 1253.92/s  (0.797s, 1284.08/s)  LR: 4.739e-04  Data: 0.011 (0.015)
Train: 156 [ 550/1251 ( 44%)]  Loss: 3.524 (3.66)  Time: 0.776s, 1319.44/s  (0.797s, 1285.09/s)  LR: 4.739e-04  Data: 0.011 (0.014)
Train: 156 [ 600/1251 ( 48%)]  Loss: 3.857 (3.68)  Time: 0.775s, 1320.98/s  (0.796s, 1286.79/s)  LR: 4.739e-04  Data: 0.012 (0.014)
Train: 156 [ 650/1251 ( 52%)]  Loss: 3.692 (3.68)  Time: 0.777s, 1317.78/s  (0.795s, 1287.57/s)  LR: 4.739e-04  Data: 0.010 (0.014)
Train: 156 [ 700/1251 ( 56%)]  Loss: 3.414 (3.66)  Time: 0.778s, 1316.67/s  (0.795s, 1287.89/s)  LR: 4.739e-04  Data: 0.011 (0.014)
Train: 156 [ 750/1251 ( 60%)]  Loss: 3.792 (3.67)  Time: 0.810s, 1263.61/s  (0.795s, 1288.00/s)  LR: 4.739e-04  Data: 0.011 (0.013)
Train: 156 [ 800/1251 ( 64%)]  Loss: 3.394 (3.65)  Time: 0.794s, 1289.86/s  (0.794s, 1289.30/s)  LR: 4.739e-04  Data: 0.011 (0.013)
Train: 156 [ 850/1251 ( 68%)]  Loss: 3.729 (3.66)  Time: 0.778s, 1315.71/s  (0.794s, 1289.75/s)  LR: 4.739e-04  Data: 0.011 (0.013)
Train: 156 [ 900/1251 ( 72%)]  Loss: 3.663 (3.66)  Time: 0.778s, 1315.89/s  (0.794s, 1289.87/s)  LR: 4.739e-04  Data: 0.011 (0.013)
Train: 156 [ 950/1251 ( 76%)]  Loss: 3.518 (3.65)  Time: 0.779s, 1313.79/s  (0.793s, 1290.95/s)  LR: 4.739e-04  Data: 0.011 (0.013)
Train: 156 [1000/1251 ( 80%)]  Loss: 3.485 (3.64)  Time: 0.811s, 1263.21/s  (0.794s, 1289.51/s)  LR: 4.739e-04  Data: 0.011 (0.013)
Train: 156 [1050/1251 ( 84%)]  Loss: 3.777 (3.65)  Time: 0.778s, 1315.87/s  (0.794s, 1290.35/s)  LR: 4.739e-04  Data: 0.010 (0.013)
Train: 156 [1100/1251 ( 88%)]  Loss: 3.988 (3.66)  Time: 0.778s, 1315.42/s  (0.793s, 1291.24/s)  LR: 4.739e-04  Data: 0.011 (0.013)
Train: 156 [1150/1251 ( 92%)]  Loss: 4.091 (3.68)  Time: 0.785s, 1303.65/s  (0.793s, 1291.82/s)  LR: 4.739e-04  Data: 0.010 (0.013)
Train: 156 [1200/1251 ( 96%)]  Loss: 3.949 (3.69)  Time: 0.776s, 1318.80/s  (0.793s, 1291.94/s)  LR: 4.739e-04  Data: 0.011 (0.013)
Train: 156 [1250/1251 (100%)]  Loss: 3.402 (3.68)  Time: 0.769s, 1331.75/s  (0.792s, 1292.25/s)  LR: 4.739e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.513 (1.513)  Loss:  0.7923 (0.7923)  Acc@1: 89.1602 (89.1602)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.172 (0.563)  Loss:  0.9442 (1.4092)  Acc@1: 83.7264 (72.3800)  Acc@5: 96.1085 (91.1020)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-151.pth.tar', 72.62800001708985)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-155.pth.tar', 72.5440000390625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-148.pth.tar', 72.3980000415039)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-156.pth.tar', 72.37999991210937)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-150.pth.tar', 72.32800004394531)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-145.pth.tar', 72.30800006591797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-152.pth.tar', 72.24199994140625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-147.pth.tar', 72.18399999023437)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-153.pth.tar', 72.12600012451172)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-146.pth.tar', 72.00400007324218)

Train: 157 [   0/1251 (  0%)]  Loss: 3.461 (3.46)  Time: 2.529s,  404.96/s  (2.529s,  404.96/s)  LR: 4.687e-04  Data: 1.720 (1.720)
Train: 157 [  50/1251 (  4%)]  Loss: 3.969 (3.71)  Time: 0.777s, 1317.37/s  (0.829s, 1235.95/s)  LR: 4.687e-04  Data: 0.011 (0.048)
Train: 157 [ 100/1251 (  8%)]  Loss: 3.650 (3.69)  Time: 0.783s, 1307.73/s  (0.807s, 1268.34/s)  LR: 4.687e-04  Data: 0.011 (0.030)
Train: 157 [ 150/1251 ( 12%)]  Loss: 4.119 (3.80)  Time: 0.776s, 1318.96/s  (0.800s, 1280.78/s)  LR: 4.687e-04  Data: 0.011 (0.024)
Train: 157 [ 200/1251 ( 16%)]  Loss: 3.759 (3.79)  Time: 0.777s, 1317.35/s  (0.801s, 1278.71/s)  LR: 4.687e-04  Data: 0.011 (0.021)
Train: 157 [ 250/1251 ( 20%)]  Loss: 3.482 (3.74)  Time: 0.814s, 1258.72/s  (0.801s, 1278.89/s)  LR: 4.687e-04  Data: 0.012 (0.019)
Train: 157 [ 300/1251 ( 24%)]  Loss: 3.733 (3.74)  Time: 0.832s, 1230.14/s  (0.799s, 1281.06/s)  LR: 4.687e-04  Data: 0.011 (0.017)
Train: 157 [ 350/1251 ( 28%)]  Loss: 3.695 (3.73)  Time: 0.778s, 1316.98/s  (0.797s, 1284.87/s)  LR: 4.687e-04  Data: 0.011 (0.017)
Train: 157 [ 400/1251 ( 32%)]  Loss: 3.428 (3.70)  Time: 0.785s, 1304.18/s  (0.797s, 1284.01/s)  LR: 4.687e-04  Data: 0.011 (0.016)
Train: 157 [ 450/1251 ( 36%)]  Loss: 4.112 (3.74)  Time: 0.779s, 1314.45/s  (0.797s, 1285.19/s)  LR: 4.687e-04  Data: 0.010 (0.015)
Train: 157 [ 500/1251 ( 40%)]  Loss: 3.528 (3.72)  Time: 0.778s, 1315.53/s  (0.796s, 1286.24/s)  LR: 4.687e-04  Data: 0.011 (0.015)
Train: 157 [ 550/1251 ( 44%)]  Loss: 3.462 (3.70)  Time: 0.777s, 1317.09/s  (0.796s, 1285.95/s)  LR: 4.687e-04  Data: 0.011 (0.015)
Train: 157 [ 600/1251 ( 48%)]  Loss: 4.065 (3.73)  Time: 0.806s, 1270.27/s  (0.796s, 1286.52/s)  LR: 4.687e-04  Data: 0.011 (0.014)
Train: 157 [ 650/1251 ( 52%)]  Loss: 3.853 (3.74)  Time: 0.779s, 1315.18/s  (0.797s, 1285.17/s)  LR: 4.687e-04  Data: 0.011 (0.014)
Train: 157 [ 700/1251 ( 56%)]  Loss: 3.609 (3.73)  Time: 0.817s, 1253.25/s  (0.796s, 1286.55/s)  LR: 4.687e-04  Data: 0.010 (0.014)
Train: 157 [ 750/1251 ( 60%)]  Loss: 3.575 (3.72)  Time: 0.777s, 1318.70/s  (0.795s, 1287.98/s)  LR: 4.687e-04  Data: 0.010 (0.014)
Train: 157 [ 800/1251 ( 64%)]  Loss: 3.961 (3.73)  Time: 0.778s, 1315.83/s  (0.795s, 1287.24/s)  LR: 4.687e-04  Data: 0.011 (0.013)
Train: 157 [ 850/1251 ( 68%)]  Loss: 3.632 (3.73)  Time: 0.781s, 1311.81/s  (0.796s, 1285.67/s)  LR: 4.687e-04  Data: 0.011 (0.013)
Train: 157 [ 900/1251 ( 72%)]  Loss: 3.901 (3.74)  Time: 0.782s, 1308.73/s  (0.796s, 1287.13/s)  LR: 4.687e-04  Data: 0.011 (0.013)
Train: 157 [ 950/1251 ( 76%)]  Loss: 3.431 (3.72)  Time: 0.779s, 1315.11/s  (0.795s, 1288.26/s)  LR: 4.687e-04  Data: 0.011 (0.013)
Train: 157 [1000/1251 ( 80%)]  Loss: 3.991 (3.73)  Time: 0.778s, 1315.48/s  (0.794s, 1289.26/s)  LR: 4.687e-04  Data: 0.010 (0.013)
Train: 157 [1050/1251 ( 84%)]  Loss: 3.964 (3.74)  Time: 0.835s, 1226.74/s  (0.794s, 1289.95/s)  LR: 4.687e-04  Data: 0.011 (0.013)
Train: 157 [1100/1251 ( 88%)]  Loss: 3.888 (3.75)  Time: 0.777s, 1317.65/s  (0.793s, 1290.65/s)  LR: 4.687e-04  Data: 0.012 (0.013)
Train: 157 [1150/1251 ( 92%)]  Loss: 3.962 (3.76)  Time: 0.779s, 1314.87/s  (0.793s, 1291.54/s)  LR: 4.687e-04  Data: 0.011 (0.013)
Train: 157 [1200/1251 ( 96%)]  Loss: 3.699 (3.76)  Time: 0.778s, 1316.19/s  (0.793s, 1291.05/s)  LR: 4.687e-04  Data: 0.011 (0.013)
Train: 157 [1250/1251 (100%)]  Loss: 3.799 (3.76)  Time: 0.770s, 1329.72/s  (0.793s, 1291.79/s)  LR: 4.687e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.501 (1.501)  Loss:  0.8519 (0.8519)  Acc@1: 88.0859 (88.0859)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.172 (0.563)  Loss:  0.9423 (1.4999)  Acc@1: 83.9623 (72.9060)  Acc@5: 95.9906 (91.2720)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-157.pth.tar', 72.90600001464844)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-151.pth.tar', 72.62800001708985)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-155.pth.tar', 72.5440000390625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-148.pth.tar', 72.3980000415039)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-156.pth.tar', 72.37999991210937)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-150.pth.tar', 72.32800004394531)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-145.pth.tar', 72.30800006591797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-152.pth.tar', 72.24199994140625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-147.pth.tar', 72.18399999023437)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-153.pth.tar', 72.12600012451172)

Train: 158 [   0/1251 (  0%)]  Loss: 3.798 (3.80)  Time: 2.465s,  415.39/s  (2.465s,  415.39/s)  LR: 4.636e-04  Data: 1.732 (1.732)
Train: 158 [  50/1251 (  4%)]  Loss: 3.605 (3.70)  Time: 0.814s, 1257.29/s  (0.834s, 1227.58/s)  LR: 4.636e-04  Data: 0.011 (0.046)
Train: 158 [ 100/1251 (  8%)]  Loss: 3.395 (3.60)  Time: 0.779s, 1314.54/s  (0.815s, 1256.69/s)  LR: 4.636e-04  Data: 0.011 (0.029)
Train: 158 [ 150/1251 ( 12%)]  Loss: 3.605 (3.60)  Time: 0.791s, 1294.80/s  (0.804s, 1273.62/s)  LR: 4.636e-04  Data: 0.011 (0.023)
Train: 158 [ 200/1251 ( 16%)]  Loss: 3.883 (3.66)  Time: 0.784s, 1305.41/s  (0.803s, 1275.55/s)  LR: 4.636e-04  Data: 0.012 (0.020)
Train: 158 [ 250/1251 ( 20%)]  Loss: 3.967 (3.71)  Time: 0.780s, 1312.10/s  (0.803s, 1275.57/s)  LR: 4.636e-04  Data: 0.012 (0.018)
Train: 158 [ 300/1251 ( 24%)]  Loss: 3.794 (3.72)  Time: 0.818s, 1251.92/s  (0.801s, 1278.28/s)  LR: 4.636e-04  Data: 0.011 (0.017)
Train: 158 [ 350/1251 ( 28%)]  Loss: 3.552 (3.70)  Time: 0.784s, 1306.85/s  (0.799s, 1281.53/s)  LR: 4.636e-04  Data: 0.010 (0.016)
Train: 158 [ 400/1251 ( 32%)]  Loss: 3.548 (3.68)  Time: 0.785s, 1304.47/s  (0.798s, 1283.02/s)  LR: 4.636e-04  Data: 0.011 (0.015)
Train: 158 [ 450/1251 ( 36%)]  Loss: 3.701 (3.68)  Time: 0.811s, 1261.93/s  (0.798s, 1283.75/s)  LR: 4.636e-04  Data: 0.011 (0.015)
Train: 158 [ 500/1251 ( 40%)]  Loss: 4.144 (3.73)  Time: 0.778s, 1315.84/s  (0.799s, 1281.95/s)  LR: 4.636e-04  Data: 0.011 (0.015)
Train: 158 [ 550/1251 ( 44%)]  Loss: 3.795 (3.73)  Time: 0.811s, 1263.02/s  (0.800s, 1279.90/s)  LR: 4.636e-04  Data: 0.011 (0.014)
Train: 158 [ 600/1251 ( 48%)]  Loss: 3.734 (3.73)  Time: 0.778s, 1316.47/s  (0.800s, 1279.35/s)  LR: 4.636e-04  Data: 0.011 (0.014)
Train: 158 [ 650/1251 ( 52%)]  Loss: 3.640 (3.73)  Time: 0.778s, 1315.59/s  (0.799s, 1280.90/s)  LR: 4.636e-04  Data: 0.011 (0.014)
Train: 158 [ 700/1251 ( 56%)]  Loss: 3.786 (3.73)  Time: 0.779s, 1314.06/s  (0.799s, 1281.35/s)  LR: 4.636e-04  Data: 0.011 (0.014)
Train: 158 [ 750/1251 ( 60%)]  Loss: 3.532 (3.72)  Time: 0.817s, 1253.27/s  (0.799s, 1282.10/s)  LR: 4.636e-04  Data: 0.025 (0.013)
Train: 158 [ 800/1251 ( 64%)]  Loss: 3.312 (3.69)  Time: 0.777s, 1317.33/s  (0.798s, 1283.58/s)  LR: 4.636e-04  Data: 0.011 (0.013)
Train: 158 [ 850/1251 ( 68%)]  Loss: 3.527 (3.68)  Time: 0.779s, 1313.76/s  (0.797s, 1284.17/s)  LR: 4.636e-04  Data: 0.011 (0.013)
Train: 158 [ 900/1251 ( 72%)]  Loss: 3.837 (3.69)  Time: 0.822s, 1245.21/s  (0.797s, 1284.94/s)  LR: 4.636e-04  Data: 0.011 (0.013)
Train: 158 [ 950/1251 ( 76%)]  Loss: 3.564 (3.69)  Time: 0.782s, 1309.07/s  (0.797s, 1285.11/s)  LR: 4.636e-04  Data: 0.011 (0.013)
Train: 158 [1000/1251 ( 80%)]  Loss: 3.954 (3.70)  Time: 0.841s, 1217.79/s  (0.797s, 1284.85/s)  LR: 4.636e-04  Data: 0.011 (0.013)
Train: 158 [1050/1251 ( 84%)]  Loss: 3.646 (3.70)  Time: 0.779s, 1315.30/s  (0.797s, 1284.85/s)  LR: 4.636e-04  Data: 0.011 (0.013)
Train: 158 [1100/1251 ( 88%)]  Loss: 3.519 (3.69)  Time: 0.793s, 1291.76/s  (0.797s, 1284.49/s)  LR: 4.636e-04  Data: 0.012 (0.013)
Train: 158 [1150/1251 ( 92%)]  Loss: 3.829 (3.69)  Time: 0.797s, 1284.55/s  (0.797s, 1284.99/s)  LR: 4.636e-04  Data: 0.011 (0.013)
Train: 158 [1200/1251 ( 96%)]  Loss: 3.515 (3.69)  Time: 0.798s, 1283.65/s  (0.797s, 1285.26/s)  LR: 4.636e-04  Data: 0.015 (0.013)
Train: 158 [1250/1251 (100%)]  Loss: 4.207 (3.71)  Time: 0.768s, 1334.09/s  (0.796s, 1285.85/s)  LR: 4.636e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.513 (1.513)  Loss:  0.9470 (0.9470)  Acc@1: 88.4766 (88.4766)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.172 (0.587)  Loss:  1.1167 (1.5481)  Acc@1: 84.4340 (72.4620)  Acc@5: 95.9906 (90.9100)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-157.pth.tar', 72.90600001464844)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-151.pth.tar', 72.62800001708985)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-155.pth.tar', 72.5440000390625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-158.pth.tar', 72.46200009033203)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-148.pth.tar', 72.3980000415039)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-156.pth.tar', 72.37999991210937)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-150.pth.tar', 72.32800004394531)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-145.pth.tar', 72.30800006591797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-152.pth.tar', 72.24199994140625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-147.pth.tar', 72.18399999023437)

Train: 159 [   0/1251 (  0%)]  Loss: 3.813 (3.81)  Time: 2.393s,  427.86/s  (2.393s,  427.86/s)  LR: 4.584e-04  Data: 1.654 (1.654)
Train: 159 [  50/1251 (  4%)]  Loss: 3.812 (3.81)  Time: 0.782s, 1309.81/s  (0.833s, 1228.80/s)  LR: 4.584e-04  Data: 0.011 (0.048)
Train: 159 [ 100/1251 (  8%)]  Loss: 3.951 (3.86)  Time: 0.782s, 1308.92/s  (0.816s, 1254.97/s)  LR: 4.584e-04  Data: 0.011 (0.030)
Train: 159 [ 150/1251 ( 12%)]  Loss: 3.585 (3.79)  Time: 0.777s, 1318.43/s  (0.809s, 1265.62/s)  LR: 4.584e-04  Data: 0.010 (0.024)
Train: 159 [ 200/1251 ( 16%)]  Loss: 3.664 (3.77)  Time: 0.833s, 1229.32/s  (0.805s, 1271.65/s)  LR: 4.584e-04  Data: 0.011 (0.020)
Train: 159 [ 250/1251 ( 20%)]  Loss: 3.701 (3.75)  Time: 0.783s, 1308.01/s  (0.802s, 1276.72/s)  LR: 4.584e-04  Data: 0.011 (0.019)
Train: 159 [ 300/1251 ( 24%)]  Loss: 3.825 (3.76)  Time: 0.818s, 1251.15/s  (0.800s, 1280.70/s)  LR: 4.584e-04  Data: 0.012 (0.017)
Train: 159 [ 350/1251 ( 28%)]  Loss: 3.804 (3.77)  Time: 0.780s, 1313.15/s  (0.800s, 1279.33/s)  LR: 4.584e-04  Data: 0.011 (0.016)
Train: 159 [ 400/1251 ( 32%)]  Loss: 3.351 (3.72)  Time: 0.776s, 1320.36/s  (0.800s, 1279.23/s)  LR: 4.584e-04  Data: 0.011 (0.016)
Train: 159 [ 450/1251 ( 36%)]  Loss: 4.000 (3.75)  Time: 0.779s, 1313.76/s  (0.799s, 1282.13/s)  LR: 4.584e-04  Data: 0.012 (0.015)
Train: 159 [ 500/1251 ( 40%)]  Loss: 3.509 (3.73)  Time: 0.842s, 1216.51/s  (0.798s, 1282.68/s)  LR: 4.584e-04  Data: 0.011 (0.015)
Train: 159 [ 550/1251 ( 44%)]  Loss: 3.680 (3.72)  Time: 0.784s, 1305.81/s  (0.798s, 1283.18/s)  LR: 4.584e-04  Data: 0.011 (0.015)
Train: 159 [ 600/1251 ( 48%)]  Loss: 3.820 (3.73)  Time: 0.779s, 1315.00/s  (0.797s, 1284.56/s)  LR: 4.584e-04  Data: 0.010 (0.014)
Train: 159 [ 650/1251 ( 52%)]  Loss: 4.047 (3.75)  Time: 0.777s, 1317.65/s  (0.796s, 1286.73/s)  LR: 4.584e-04  Data: 0.011 (0.014)
Train: 159 [ 700/1251 ( 56%)]  Loss: 3.690 (3.75)  Time: 0.780s, 1312.34/s  (0.795s, 1288.37/s)  LR: 4.584e-04  Data: 0.011 (0.014)
Train: 159 [ 750/1251 ( 60%)]  Loss: 3.677 (3.75)  Time: 0.820s, 1248.34/s  (0.794s, 1289.17/s)  LR: 4.584e-04  Data: 0.011 (0.014)
Train: 159 [ 800/1251 ( 64%)]  Loss: 3.665 (3.74)  Time: 0.781s, 1311.22/s  (0.794s, 1289.83/s)  LR: 4.584e-04  Data: 0.010 (0.013)
Train: 159 [ 850/1251 ( 68%)]  Loss: 3.740 (3.74)  Time: 0.779s, 1313.98/s  (0.793s, 1290.98/s)  LR: 4.584e-04  Data: 0.011 (0.013)
Train: 159 [ 900/1251 ( 72%)]  Loss: 3.699 (3.74)  Time: 0.780s, 1313.46/s  (0.793s, 1291.78/s)  LR: 4.584e-04  Data: 0.011 (0.013)
Train: 159 [ 950/1251 ( 76%)]  Loss: 3.882 (3.75)  Time: 0.820s, 1248.40/s  (0.793s, 1292.08/s)  LR: 4.584e-04  Data: 0.011 (0.013)
Train: 159 [1000/1251 ( 80%)]  Loss: 3.600 (3.74)  Time: 0.819s, 1250.77/s  (0.794s, 1290.47/s)  LR: 4.584e-04  Data: 0.011 (0.013)
Train: 159 [1050/1251 ( 84%)]  Loss: 4.014 (3.75)  Time: 0.860s, 1190.78/s  (0.794s, 1289.28/s)  LR: 4.584e-04  Data: 0.011 (0.013)
Train: 159 [1100/1251 ( 88%)]  Loss: 3.775 (3.75)  Time: 0.799s, 1281.66/s  (0.795s, 1288.26/s)  LR: 4.584e-04  Data: 0.011 (0.013)
Train: 159 [1150/1251 ( 92%)]  Loss: 3.700 (3.75)  Time: 0.784s, 1306.93/s  (0.795s, 1288.81/s)  LR: 4.584e-04  Data: 0.011 (0.013)
Train: 159 [1200/1251 ( 96%)]  Loss: 3.869 (3.75)  Time: 0.781s, 1311.92/s  (0.794s, 1289.64/s)  LR: 4.584e-04  Data: 0.011 (0.013)
Train: 159 [1250/1251 (100%)]  Loss: 3.805 (3.76)  Time: 0.767s, 1335.08/s  (0.794s, 1290.12/s)  LR: 4.584e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.497 (1.497)  Loss:  0.7332 (0.7332)  Acc@1: 88.4766 (88.4766)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.172 (0.560)  Loss:  0.9282 (1.3470)  Acc@1: 83.7264 (72.9660)  Acc@5: 95.6368 (91.2740)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-159.pth.tar', 72.9660000415039)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-157.pth.tar', 72.90600001464844)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-151.pth.tar', 72.62800001708985)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-155.pth.tar', 72.5440000390625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-158.pth.tar', 72.46200009033203)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-148.pth.tar', 72.3980000415039)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-156.pth.tar', 72.37999991210937)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-150.pth.tar', 72.32800004394531)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-145.pth.tar', 72.30800006591797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-152.pth.tar', 72.24199994140625)

Train: 160 [   0/1251 (  0%)]  Loss: 3.643 (3.64)  Time: 2.216s,  462.03/s  (2.216s,  462.03/s)  LR: 4.533e-04  Data: 1.479 (1.479)
Train: 160 [  50/1251 (  4%)]  Loss: 3.804 (3.72)  Time: 0.776s, 1319.23/s  (0.815s, 1256.71/s)  LR: 4.533e-04  Data: 0.010 (0.045)
Train: 160 [ 100/1251 (  8%)]  Loss: 3.426 (3.62)  Time: 0.777s, 1318.47/s  (0.812s, 1260.37/s)  LR: 4.533e-04  Data: 0.011 (0.028)
Train: 160 [ 150/1251 ( 12%)]  Loss: 4.001 (3.72)  Time: 0.781s, 1310.74/s  (0.803s, 1274.44/s)  LR: 4.533e-04  Data: 0.012 (0.023)
Train: 160 [ 200/1251 ( 16%)]  Loss: 3.891 (3.75)  Time: 0.779s, 1313.86/s  (0.798s, 1282.91/s)  LR: 4.533e-04  Data: 0.011 (0.020)
Train: 160 [ 250/1251 ( 20%)]  Loss: 4.019 (3.80)  Time: 0.803s, 1275.05/s  (0.796s, 1287.09/s)  LR: 4.533e-04  Data: 0.011 (0.018)
Train: 160 [ 300/1251 ( 24%)]  Loss: 3.324 (3.73)  Time: 0.777s, 1317.49/s  (0.795s, 1288.82/s)  LR: 4.533e-04  Data: 0.011 (0.017)
Train: 160 [ 350/1251 ( 28%)]  Loss: 4.026 (3.77)  Time: 0.833s, 1228.90/s  (0.794s, 1289.98/s)  LR: 4.533e-04  Data: 0.011 (0.016)
Train: 160 [ 400/1251 ( 32%)]  Loss: 3.730 (3.76)  Time: 0.782s, 1309.75/s  (0.793s, 1291.83/s)  LR: 4.533e-04  Data: 0.012 (0.015)
Train: 160 [ 450/1251 ( 36%)]  Loss: 3.911 (3.78)  Time: 0.779s, 1315.13/s  (0.792s, 1293.39/s)  LR: 4.533e-04  Data: 0.011 (0.015)
Train: 160 [ 500/1251 ( 40%)]  Loss: 3.470 (3.75)  Time: 0.815s, 1256.42/s  (0.791s, 1294.22/s)  LR: 4.533e-04  Data: 0.011 (0.014)
Train: 160 [ 550/1251 ( 44%)]  Loss: 3.752 (3.75)  Time: 0.779s, 1314.88/s  (0.791s, 1294.74/s)  LR: 4.533e-04  Data: 0.015 (0.014)
Train: 160 [ 600/1251 ( 48%)]  Loss: 3.888 (3.76)  Time: 0.815s, 1256.87/s  (0.791s, 1295.29/s)  LR: 4.533e-04  Data: 0.011 (0.014)
Train: 160 [ 650/1251 ( 52%)]  Loss: 3.469 (3.74)  Time: 0.779s, 1314.60/s  (0.790s, 1295.79/s)  LR: 4.533e-04  Data: 0.011 (0.014)
Train: 160 [ 700/1251 ( 56%)]  Loss: 4.157 (3.77)  Time: 0.783s, 1308.50/s  (0.790s, 1295.41/s)  LR: 4.533e-04  Data: 0.011 (0.014)
Train: 160 [ 750/1251 ( 60%)]  Loss: 3.564 (3.75)  Time: 0.785s, 1304.28/s  (0.791s, 1295.09/s)  LR: 4.533e-04  Data: 0.011 (0.013)
Train: 160 [ 800/1251 ( 64%)]  Loss: 3.819 (3.76)  Time: 0.818s, 1251.17/s  (0.790s, 1295.61/s)  LR: 4.533e-04  Data: 0.011 (0.013)
Train: 160 [ 850/1251 ( 68%)]  Loss: 3.561 (3.75)  Time: 0.780s, 1312.08/s  (0.791s, 1294.33/s)  LR: 4.533e-04  Data: 0.011 (0.013)
Train: 160 [ 900/1251 ( 72%)]  Loss: 3.808 (3.75)  Time: 0.780s, 1312.74/s  (0.792s, 1293.55/s)  LR: 4.533e-04  Data: 0.011 (0.013)
Train: 160 [ 950/1251 ( 76%)]  Loss: 3.841 (3.76)  Time: 0.786s, 1302.20/s  (0.791s, 1294.45/s)  LR: 4.533e-04  Data: 0.011 (0.013)
Train: 160 [1000/1251 ( 80%)]  Loss: 3.439 (3.74)  Time: 0.847s, 1209.29/s  (0.791s, 1294.59/s)  LR: 4.533e-04  Data: 0.011 (0.013)
Train: 160 [1050/1251 ( 84%)]  Loss: 3.569 (3.73)  Time: 0.811s, 1262.05/s  (0.791s, 1293.97/s)  LR: 4.533e-04  Data: 0.011 (0.013)
Train: 160 [1100/1251 ( 88%)]  Loss: 3.689 (3.73)  Time: 0.778s, 1315.41/s  (0.792s, 1293.72/s)  LR: 4.533e-04  Data: 0.012 (0.013)
Train: 160 [1150/1251 ( 92%)]  Loss: 3.679 (3.73)  Time: 0.816s, 1254.17/s  (0.792s, 1293.15/s)  LR: 4.533e-04  Data: 0.011 (0.013)
Train: 160 [1200/1251 ( 96%)]  Loss: 4.091 (3.74)  Time: 0.784s, 1305.71/s  (0.792s, 1292.67/s)  LR: 4.533e-04  Data: 0.011 (0.013)
Train: 160 [1250/1251 (100%)]  Loss: 3.724 (3.74)  Time: 0.765s, 1338.00/s  (0.792s, 1293.09/s)  LR: 4.533e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.516 (1.516)  Loss:  0.8149 (0.8149)  Acc@1: 88.1836 (88.1836)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.172 (0.562)  Loss:  0.9526 (1.4353)  Acc@1: 84.0802 (72.9500)  Acc@5: 95.8727 (91.2680)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-159.pth.tar', 72.9660000415039)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-160.pth.tar', 72.95000006591796)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-157.pth.tar', 72.90600001464844)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-151.pth.tar', 72.62800001708985)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-155.pth.tar', 72.5440000390625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-158.pth.tar', 72.46200009033203)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-148.pth.tar', 72.3980000415039)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-156.pth.tar', 72.37999991210937)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-150.pth.tar', 72.32800004394531)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-145.pth.tar', 72.30800006591797)

Train: 161 [   0/1251 (  0%)]  Loss: 4.064 (4.06)  Time: 2.337s,  438.17/s  (2.337s,  438.17/s)  LR: 4.481e-04  Data: 1.601 (1.601)
Train: 161 [  50/1251 (  4%)]  Loss: 3.699 (3.88)  Time: 0.777s, 1317.77/s  (0.830s, 1234.03/s)  LR: 4.481e-04  Data: 0.011 (0.046)
Train: 161 [ 100/1251 (  8%)]  Loss: 3.474 (3.75)  Time: 0.775s, 1320.44/s  (0.809s, 1265.72/s)  LR: 4.481e-04  Data: 0.011 (0.028)
Train: 161 [ 150/1251 ( 12%)]  Loss: 3.696 (3.73)  Time: 0.782s, 1308.82/s  (0.803s, 1275.50/s)  LR: 4.481e-04  Data: 0.011 (0.023)
Train: 161 [ 200/1251 ( 16%)]  Loss: 3.635 (3.71)  Time: 0.819s, 1250.09/s  (0.802s, 1276.30/s)  LR: 4.481e-04  Data: 0.011 (0.020)
Train: 161 [ 250/1251 ( 20%)]  Loss: 4.044 (3.77)  Time: 0.785s, 1303.93/s  (0.800s, 1279.39/s)  LR: 4.481e-04  Data: 0.011 (0.018)
Train: 161 [ 300/1251 ( 24%)]  Loss: 3.721 (3.76)  Time: 0.780s, 1311.99/s  (0.801s, 1278.75/s)  LR: 4.481e-04  Data: 0.011 (0.017)
Train: 161 [ 350/1251 ( 28%)]  Loss: 4.010 (3.79)  Time: 0.780s, 1313.44/s  (0.800s, 1279.68/s)  LR: 4.481e-04  Data: 0.011 (0.016)
Train: 161 [ 400/1251 ( 32%)]  Loss: 3.811 (3.79)  Time: 0.814s, 1258.56/s  (0.799s, 1281.37/s)  LR: 4.481e-04  Data: 0.011 (0.015)
Train: 161 [ 450/1251 ( 36%)]  Loss: 3.990 (3.81)  Time: 0.781s, 1310.84/s  (0.798s, 1283.51/s)  LR: 4.481e-04  Data: 0.011 (0.015)
Train: 161 [ 500/1251 ( 40%)]  Loss: 3.507 (3.79)  Time: 0.780s, 1312.59/s  (0.798s, 1283.25/s)  LR: 4.481e-04  Data: 0.011 (0.015)
Train: 161 [ 550/1251 ( 44%)]  Loss: 3.547 (3.77)  Time: 0.779s, 1314.58/s  (0.797s, 1284.59/s)  LR: 4.481e-04  Data: 0.011 (0.014)
Train: 161 [ 600/1251 ( 48%)]  Loss: 3.476 (3.74)  Time: 0.777s, 1318.64/s  (0.796s, 1286.46/s)  LR: 4.481e-04  Data: 0.011 (0.014)
Train: 161 [ 650/1251 ( 52%)]  Loss: 3.940 (3.76)  Time: 0.778s, 1315.93/s  (0.795s, 1288.17/s)  LR: 4.481e-04  Data: 0.011 (0.014)
Train: 161 [ 700/1251 ( 56%)]  Loss: 3.706 (3.75)  Time: 0.813s, 1258.97/s  (0.796s, 1286.83/s)  LR: 4.481e-04  Data: 0.011 (0.014)
Train: 161 [ 750/1251 ( 60%)]  Loss: 3.608 (3.75)  Time: 0.809s, 1265.56/s  (0.796s, 1285.91/s)  LR: 4.481e-04  Data: 0.010 (0.013)
Train: 161 [ 800/1251 ( 64%)]  Loss: 3.635 (3.74)  Time: 0.830s, 1234.11/s  (0.796s, 1286.34/s)  LR: 4.481e-04  Data: 0.011 (0.013)
Train: 161 [ 850/1251 ( 68%)]  Loss: 3.563 (3.73)  Time: 0.778s, 1316.20/s  (0.796s, 1287.07/s)  LR: 4.481e-04  Data: 0.011 (0.013)
Train: 161 [ 900/1251 ( 72%)]  Loss: 3.760 (3.73)  Time: 0.812s, 1261.78/s  (0.796s, 1286.97/s)  LR: 4.481e-04  Data: 0.011 (0.013)
Train: 161 [ 950/1251 ( 76%)]  Loss: 3.251 (3.71)  Time: 0.817s, 1253.84/s  (0.795s, 1287.90/s)  LR: 4.481e-04  Data: 0.011 (0.013)
Train: 161 [1000/1251 ( 80%)]  Loss: 3.592 (3.70)  Time: 0.814s, 1258.00/s  (0.796s, 1286.97/s)  LR: 4.481e-04  Data: 0.012 (0.013)
Train: 161 [1050/1251 ( 84%)]  Loss: 3.830 (3.71)  Time: 0.813s, 1259.36/s  (0.797s, 1285.52/s)  LR: 4.481e-04  Data: 0.011 (0.013)
Train: 161 [1100/1251 ( 88%)]  Loss: 3.566 (3.70)  Time: 0.777s, 1318.46/s  (0.796s, 1285.98/s)  LR: 4.481e-04  Data: 0.011 (0.013)
Train: 161 [1150/1251 ( 92%)]  Loss: 3.948 (3.71)  Time: 0.781s, 1310.59/s  (0.796s, 1286.95/s)  LR: 4.481e-04  Data: 0.011 (0.013)
Train: 161 [1200/1251 ( 96%)]  Loss: 3.524 (3.70)  Time: 0.779s, 1314.85/s  (0.795s, 1287.75/s)  LR: 4.481e-04  Data: 0.013 (0.013)
Train: 161 [1250/1251 (100%)]  Loss: 4.208 (3.72)  Time: 0.773s, 1324.75/s  (0.795s, 1287.66/s)  LR: 4.481e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.523 (1.523)  Loss:  0.8491 (0.8491)  Acc@1: 88.5742 (88.5742)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.172 (0.566)  Loss:  0.9488 (1.4275)  Acc@1: 83.0189 (72.7160)  Acc@5: 95.9906 (91.1360)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-159.pth.tar', 72.9660000415039)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-160.pth.tar', 72.95000006591796)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-157.pth.tar', 72.90600001464844)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-161.pth.tar', 72.71599986328125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-151.pth.tar', 72.62800001708985)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-155.pth.tar', 72.5440000390625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-158.pth.tar', 72.46200009033203)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-148.pth.tar', 72.3980000415039)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-156.pth.tar', 72.37999991210937)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-150.pth.tar', 72.32800004394531)

Train: 162 [   0/1251 (  0%)]  Loss: 3.465 (3.47)  Time: 2.448s,  418.37/s  (2.448s,  418.37/s)  LR: 4.430e-04  Data: 1.649 (1.649)
Train: 162 [  50/1251 (  4%)]  Loss: 3.315 (3.39)  Time: 0.779s, 1314.75/s  (0.834s, 1227.79/s)  LR: 4.430e-04  Data: 0.012 (0.048)
Train: 162 [ 100/1251 (  8%)]  Loss: 3.699 (3.49)  Time: 0.781s, 1310.98/s  (0.808s, 1266.91/s)  LR: 4.430e-04  Data: 0.011 (0.030)
Train: 162 [ 150/1251 ( 12%)]  Loss: 3.604 (3.52)  Time: 0.778s, 1316.66/s  (0.799s, 1281.23/s)  LR: 4.430e-04  Data: 0.011 (0.024)
Train: 162 [ 200/1251 ( 16%)]  Loss: 3.673 (3.55)  Time: 0.812s, 1260.76/s  (0.798s, 1283.50/s)  LR: 4.430e-04  Data: 0.012 (0.021)
Train: 162 [ 250/1251 ( 20%)]  Loss: 3.913 (3.61)  Time: 0.831s, 1232.34/s  (0.800s, 1280.19/s)  LR: 4.430e-04  Data: 0.012 (0.019)
Train: 162 [ 300/1251 ( 24%)]  Loss: 3.343 (3.57)  Time: 0.782s, 1310.20/s  (0.800s, 1280.38/s)  LR: 4.430e-04  Data: 0.012 (0.018)
Train: 162 [ 350/1251 ( 28%)]  Loss: 4.099 (3.64)  Time: 0.776s, 1320.15/s  (0.798s, 1282.82/s)  LR: 4.430e-04  Data: 0.011 (0.017)
Train: 162 [ 400/1251 ( 32%)]  Loss: 3.607 (3.64)  Time: 0.781s, 1311.31/s  (0.796s, 1286.43/s)  LR: 4.430e-04  Data: 0.011 (0.016)
Train: 162 [ 450/1251 ( 36%)]  Loss: 3.308 (3.60)  Time: 0.781s, 1311.68/s  (0.795s, 1288.35/s)  LR: 4.430e-04  Data: 0.011 (0.015)
Train: 162 [ 500/1251 ( 40%)]  Loss: 3.806 (3.62)  Time: 0.777s, 1317.12/s  (0.794s, 1290.41/s)  LR: 4.430e-04  Data: 0.011 (0.015)
Train: 162 [ 550/1251 ( 44%)]  Loss: 3.360 (3.60)  Time: 0.787s, 1301.22/s  (0.794s, 1288.95/s)  LR: 4.430e-04  Data: 0.012 (0.015)
Train: 162 [ 600/1251 ( 48%)]  Loss: 3.876 (3.62)  Time: 0.779s, 1314.43/s  (0.793s, 1290.56/s)  LR: 4.430e-04  Data: 0.011 (0.014)
Train: 162 [ 650/1251 ( 52%)]  Loss: 3.584 (3.62)  Time: 0.820s, 1249.51/s  (0.794s, 1290.28/s)  LR: 4.430e-04  Data: 0.011 (0.014)
Train: 162 [ 700/1251 ( 56%)]  Loss: 3.823 (3.63)  Time: 0.794s, 1289.40/s  (0.794s, 1290.08/s)  LR: 4.430e-04  Data: 0.011 (0.014)
Train: 162 [ 750/1251 ( 60%)]  Loss: 3.666 (3.63)  Time: 0.816s, 1255.54/s  (0.793s, 1291.02/s)  LR: 4.430e-04  Data: 0.012 (0.014)
Train: 162 [ 800/1251 ( 64%)]  Loss: 3.593 (3.63)  Time: 0.778s, 1315.75/s  (0.793s, 1291.76/s)  LR: 4.430e-04  Data: 0.011 (0.014)
Train: 162 [ 850/1251 ( 68%)]  Loss: 3.024 (3.60)  Time: 0.789s, 1297.76/s  (0.792s, 1292.62/s)  LR: 4.430e-04  Data: 0.011 (0.013)
Train: 162 [ 900/1251 ( 72%)]  Loss: 3.629 (3.60)  Time: 0.779s, 1314.85/s  (0.792s, 1293.23/s)  LR: 4.430e-04  Data: 0.011 (0.013)
Train: 162 [ 950/1251 ( 76%)]  Loss: 3.996 (3.62)  Time: 0.779s, 1315.34/s  (0.791s, 1293.89/s)  LR: 4.430e-04  Data: 0.011 (0.013)
Train: 162 [1000/1251 ( 80%)]  Loss: 3.262 (3.60)  Time: 0.779s, 1314.78/s  (0.791s, 1294.40/s)  LR: 4.430e-04  Data: 0.011 (0.013)
Train: 162 [1050/1251 ( 84%)]  Loss: 3.786 (3.61)  Time: 0.781s, 1311.33/s  (0.791s, 1294.53/s)  LR: 4.430e-04  Data: 0.010 (0.013)
Train: 162 [1100/1251 ( 88%)]  Loss: 3.594 (3.61)  Time: 0.829s, 1235.47/s  (0.791s, 1295.19/s)  LR: 4.430e-04  Data: 0.010 (0.013)
Train: 162 [1150/1251 ( 92%)]  Loss: 4.050 (3.63)  Time: 0.811s, 1262.83/s  (0.791s, 1294.94/s)  LR: 4.430e-04  Data: 0.011 (0.013)
Train: 162 [1200/1251 ( 96%)]  Loss: 4.057 (3.65)  Time: 0.812s, 1261.28/s  (0.791s, 1294.20/s)  LR: 4.430e-04  Data: 0.012 (0.013)
Train: 162 [1250/1251 (100%)]  Loss: 3.807 (3.65)  Time: 0.766s, 1336.08/s  (0.791s, 1294.71/s)  LR: 4.430e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.554 (1.554)  Loss:  0.8478 (0.8478)  Acc@1: 88.1836 (88.1836)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.172 (0.566)  Loss:  1.0051 (1.5186)  Acc@1: 85.1415 (72.5520)  Acc@5: 96.8160 (91.2120)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-159.pth.tar', 72.9660000415039)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-160.pth.tar', 72.95000006591796)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-157.pth.tar', 72.90600001464844)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-161.pth.tar', 72.71599986328125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-151.pth.tar', 72.62800001708985)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-162.pth.tar', 72.55200013916016)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-155.pth.tar', 72.5440000390625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-158.pth.tar', 72.46200009033203)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-148.pth.tar', 72.3980000415039)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-156.pth.tar', 72.37999991210937)

Train: 163 [   0/1251 (  0%)]  Loss: 3.699 (3.70)  Time: 2.190s,  467.52/s  (2.190s,  467.52/s)  LR: 4.378e-04  Data: 1.456 (1.456)
Train: 163 [  50/1251 (  4%)]  Loss: 3.317 (3.51)  Time: 0.777s, 1318.69/s  (0.818s, 1251.57/s)  LR: 4.378e-04  Data: 0.011 (0.044)
Train: 163 [ 100/1251 (  8%)]  Loss: 3.883 (3.63)  Time: 0.777s, 1317.21/s  (0.804s, 1273.54/s)  LR: 4.378e-04  Data: 0.011 (0.028)
Train: 163 [ 150/1251 ( 12%)]  Loss: 3.423 (3.58)  Time: 0.780s, 1313.58/s  (0.804s, 1272.97/s)  LR: 4.378e-04  Data: 0.012 (0.022)
Train: 163 [ 200/1251 ( 16%)]  Loss: 4.012 (3.67)  Time: 0.778s, 1315.98/s  (0.802s, 1277.37/s)  LR: 4.378e-04  Data: 0.011 (0.019)
Train: 163 [ 250/1251 ( 20%)]  Loss: 3.457 (3.63)  Time: 0.817s, 1252.78/s  (0.798s, 1282.84/s)  LR: 4.378e-04  Data: 0.010 (0.018)
Train: 163 [ 300/1251 ( 24%)]  Loss: 3.724 (3.64)  Time: 0.779s, 1314.62/s  (0.796s, 1287.01/s)  LR: 4.378e-04  Data: 0.011 (0.017)
Train: 163 [ 350/1251 ( 28%)]  Loss: 3.830 (3.67)  Time: 0.777s, 1317.21/s  (0.797s, 1285.40/s)  LR: 4.378e-04  Data: 0.011 (0.016)
Train: 163 [ 400/1251 ( 32%)]  Loss: 3.512 (3.65)  Time: 0.812s, 1260.94/s  (0.798s, 1282.71/s)  LR: 4.378e-04  Data: 0.011 (0.015)
Train: 163 [ 450/1251 ( 36%)]  Loss: 4.064 (3.69)  Time: 0.783s, 1307.57/s  (0.798s, 1283.88/s)  LR: 4.378e-04  Data: 0.017 (0.015)
Train: 163 [ 500/1251 ( 40%)]  Loss: 3.561 (3.68)  Time: 0.799s, 1280.82/s  (0.797s, 1284.24/s)  LR: 4.378e-04  Data: 0.011 (0.014)
Train: 163 [ 550/1251 ( 44%)]  Loss: 2.995 (3.62)  Time: 0.812s, 1260.64/s  (0.798s, 1283.49/s)  LR: 4.378e-04  Data: 0.011 (0.014)
Train: 163 [ 600/1251 ( 48%)]  Loss: 4.071 (3.66)  Time: 0.788s, 1299.55/s  (0.797s, 1284.28/s)  LR: 4.378e-04  Data: 0.011 (0.014)
Train: 163 [ 650/1251 ( 52%)]  Loss: 3.859 (3.67)  Time: 0.784s, 1306.75/s  (0.797s, 1285.28/s)  LR: 4.378e-04  Data: 0.011 (0.014)
Train: 163 [ 700/1251 ( 56%)]  Loss: 3.865 (3.68)  Time: 0.811s, 1262.53/s  (0.796s, 1286.70/s)  LR: 4.378e-04  Data: 0.011 (0.014)
Train: 163 [ 750/1251 ( 60%)]  Loss: 3.739 (3.69)  Time: 0.780s, 1312.39/s  (0.797s, 1284.39/s)  LR: 4.378e-04  Data: 0.011 (0.013)
Train: 163 [ 800/1251 ( 64%)]  Loss: 3.851 (3.70)  Time: 0.819s, 1250.68/s  (0.797s, 1284.49/s)  LR: 4.378e-04  Data: 0.011 (0.013)
Train: 163 [ 850/1251 ( 68%)]  Loss: 3.958 (3.71)  Time: 0.782s, 1309.60/s  (0.797s, 1284.46/s)  LR: 4.378e-04  Data: 0.011 (0.013)
Train: 163 [ 900/1251 ( 72%)]  Loss: 3.482 (3.70)  Time: 0.821s, 1247.52/s  (0.797s, 1285.14/s)  LR: 4.378e-04  Data: 0.011 (0.013)
Train: 163 [ 950/1251 ( 76%)]  Loss: 4.005 (3.72)  Time: 0.783s, 1308.27/s  (0.796s, 1285.68/s)  LR: 4.378e-04  Data: 0.011 (0.013)
Train: 163 [1000/1251 ( 80%)]  Loss: 3.548 (3.71)  Time: 0.779s, 1314.88/s  (0.796s, 1286.67/s)  LR: 4.378e-04  Data: 0.011 (0.013)
Train: 163 [1050/1251 ( 84%)]  Loss: 3.889 (3.72)  Time: 0.789s, 1297.36/s  (0.795s, 1287.57/s)  LR: 4.378e-04  Data: 0.011 (0.013)
Train: 163 [1100/1251 ( 88%)]  Loss: 4.057 (3.73)  Time: 0.779s, 1314.71/s  (0.795s, 1288.50/s)  LR: 4.378e-04  Data: 0.011 (0.013)
Train: 163 [1150/1251 ( 92%)]  Loss: 3.663 (3.73)  Time: 0.814s, 1258.12/s  (0.795s, 1288.43/s)  LR: 4.378e-04  Data: 0.011 (0.013)
Train: 163 [1200/1251 ( 96%)]  Loss: 3.528 (3.72)  Time: 0.776s, 1320.12/s  (0.795s, 1287.59/s)  LR: 4.378e-04  Data: 0.011 (0.012)
Train: 163 [1250/1251 (100%)]  Loss: 3.429 (3.71)  Time: 0.765s, 1338.03/s  (0.795s, 1288.34/s)  LR: 4.378e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.565 (1.565)  Loss:  0.8068 (0.8068)  Acc@1: 89.3555 (89.3555)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.171 (0.566)  Loss:  0.9419 (1.4416)  Acc@1: 84.5519 (73.3500)  Acc@5: 95.9906 (91.5080)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-163.pth.tar', 73.35000001220703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-159.pth.tar', 72.9660000415039)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-160.pth.tar', 72.95000006591796)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-157.pth.tar', 72.90600001464844)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-161.pth.tar', 72.71599986328125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-151.pth.tar', 72.62800001708985)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-162.pth.tar', 72.55200013916016)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-155.pth.tar', 72.5440000390625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-158.pth.tar', 72.46200009033203)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-148.pth.tar', 72.3980000415039)

Train: 164 [   0/1251 (  0%)]  Loss: 4.122 (4.12)  Time: 2.477s,  413.39/s  (2.477s,  413.39/s)  LR: 4.327e-04  Data: 1.742 (1.742)
Train: 164 [  50/1251 (  4%)]  Loss: 3.681 (3.90)  Time: 0.809s, 1265.14/s  (0.861s, 1189.41/s)  LR: 4.327e-04  Data: 0.011 (0.054)
Train: 164 [ 100/1251 (  8%)]  Loss: 3.564 (3.79)  Time: 0.815s, 1255.84/s  (0.826s, 1239.20/s)  LR: 4.327e-04  Data: 0.012 (0.033)
Train: 164 [ 150/1251 ( 12%)]  Loss: 3.906 (3.82)  Time: 0.779s, 1314.70/s  (0.818s, 1251.73/s)  LR: 4.327e-04  Data: 0.012 (0.026)
Train: 164 [ 200/1251 ( 16%)]  Loss: 4.202 (3.89)  Time: 0.787s, 1301.61/s  (0.809s, 1265.07/s)  LR: 4.327e-04  Data: 0.011 (0.022)
Train: 164 [ 250/1251 ( 20%)]  Loss: 3.490 (3.83)  Time: 0.779s, 1314.65/s  (0.806s, 1271.09/s)  LR: 4.327e-04  Data: 0.011 (0.020)
Train: 164 [ 300/1251 ( 24%)]  Loss: 3.448 (3.77)  Time: 0.805s, 1272.16/s  (0.805s, 1271.41/s)  LR: 4.327e-04  Data: 0.010 (0.018)
Train: 164 [ 350/1251 ( 28%)]  Loss: 3.739 (3.77)  Time: 0.815s, 1255.79/s  (0.805s, 1272.04/s)  LR: 4.327e-04  Data: 0.011 (0.017)
Train: 164 [ 400/1251 ( 32%)]  Loss: 3.439 (3.73)  Time: 0.799s, 1282.20/s  (0.805s, 1272.15/s)  LR: 4.327e-04  Data: 0.011 (0.016)
Train: 164 [ 450/1251 ( 36%)]  Loss: 3.692 (3.73)  Time: 0.778s, 1315.94/s  (0.805s, 1271.49/s)  LR: 4.327e-04  Data: 0.011 (0.016)
Train: 164 [ 500/1251 ( 40%)]  Loss: 3.703 (3.73)  Time: 0.779s, 1314.55/s  (0.804s, 1274.35/s)  LR: 4.327e-04  Data: 0.011 (0.015)
Train: 164 [ 550/1251 ( 44%)]  Loss: 3.670 (3.72)  Time: 0.777s, 1317.36/s  (0.803s, 1275.89/s)  LR: 4.327e-04  Data: 0.011 (0.015)
Train: 164 [ 600/1251 ( 48%)]  Loss: 3.761 (3.72)  Time: 0.814s, 1258.47/s  (0.803s, 1275.91/s)  LR: 4.327e-04  Data: 0.011 (0.015)
Train: 164 [ 650/1251 ( 52%)]  Loss: 3.858 (3.73)  Time: 0.815s, 1257.10/s  (0.803s, 1275.25/s)  LR: 4.327e-04  Data: 0.011 (0.014)
Train: 164 [ 700/1251 ( 56%)]  Loss: 3.418 (3.71)  Time: 0.778s, 1315.72/s  (0.802s, 1277.56/s)  LR: 4.327e-04  Data: 0.011 (0.014)
Train: 164 [ 750/1251 ( 60%)]  Loss: 3.596 (3.71)  Time: 0.780s, 1312.25/s  (0.800s, 1279.22/s)  LR: 4.327e-04  Data: 0.011 (0.014)
Train: 164 [ 800/1251 ( 64%)]  Loss: 3.619 (3.70)  Time: 0.778s, 1316.99/s  (0.799s, 1281.06/s)  LR: 4.327e-04  Data: 0.011 (0.014)
Train: 164 [ 850/1251 ( 68%)]  Loss: 3.780 (3.70)  Time: 0.787s, 1301.01/s  (0.799s, 1281.63/s)  LR: 4.327e-04  Data: 0.011 (0.014)
Train: 164 [ 900/1251 ( 72%)]  Loss: 3.579 (3.70)  Time: 0.780s, 1313.03/s  (0.799s, 1282.02/s)  LR: 4.327e-04  Data: 0.011 (0.013)
Train: 164 [ 950/1251 ( 76%)]  Loss: 3.592 (3.69)  Time: 0.775s, 1321.21/s  (0.798s, 1282.79/s)  LR: 4.327e-04  Data: 0.011 (0.013)
Train: 164 [1000/1251 ( 80%)]  Loss: 4.158 (3.72)  Time: 0.786s, 1303.60/s  (0.797s, 1284.06/s)  LR: 4.327e-04  Data: 0.013 (0.013)
Train: 164 [1050/1251 ( 84%)]  Loss: 3.735 (3.72)  Time: 0.777s, 1317.27/s  (0.797s, 1285.35/s)  LR: 4.327e-04  Data: 0.012 (0.013)
Train: 164 [1100/1251 ( 88%)]  Loss: 3.872 (3.72)  Time: 0.777s, 1318.04/s  (0.796s, 1286.01/s)  LR: 4.327e-04  Data: 0.011 (0.013)
Train: 164 [1150/1251 ( 92%)]  Loss: 3.879 (3.73)  Time: 0.811s, 1262.83/s  (0.796s, 1286.47/s)  LR: 4.327e-04  Data: 0.012 (0.013)
Train: 164 [1200/1251 ( 96%)]  Loss: 3.363 (3.71)  Time: 0.780s, 1312.26/s  (0.796s, 1286.43/s)  LR: 4.327e-04  Data: 0.011 (0.013)
Train: 164 [1250/1251 (100%)]  Loss: 3.603 (3.71)  Time: 0.808s, 1266.87/s  (0.796s, 1286.02/s)  LR: 4.327e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.580 (1.580)  Loss:  0.8417 (0.8417)  Acc@1: 88.3789 (88.3789)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.172 (0.579)  Loss:  0.9313 (1.3924)  Acc@1: 84.5519 (73.3020)  Acc@5: 96.2264 (91.5920)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-163.pth.tar', 73.35000001220703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-164.pth.tar', 73.30200014160157)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-159.pth.tar', 72.9660000415039)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-160.pth.tar', 72.95000006591796)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-157.pth.tar', 72.90600001464844)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-161.pth.tar', 72.71599986328125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-151.pth.tar', 72.62800001708985)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-162.pth.tar', 72.55200013916016)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-155.pth.tar', 72.5440000390625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-158.pth.tar', 72.46200009033203)

Train: 165 [   0/1251 (  0%)]  Loss: 3.552 (3.55)  Time: 2.318s,  441.81/s  (2.318s,  441.81/s)  LR: 4.276e-04  Data: 1.581 (1.581)
Train: 165 [  50/1251 (  4%)]  Loss: 3.581 (3.57)  Time: 0.780s, 1313.13/s  (0.818s, 1251.37/s)  LR: 4.276e-04  Data: 0.011 (0.044)
Train: 165 [ 100/1251 (  8%)]  Loss: 3.633 (3.59)  Time: 0.777s, 1317.13/s  (0.807s, 1269.10/s)  LR: 4.276e-04  Data: 0.011 (0.028)
Train: 165 [ 150/1251 ( 12%)]  Loss: 4.206 (3.74)  Time: 0.817s, 1253.53/s  (0.801s, 1278.33/s)  LR: 4.276e-04  Data: 0.011 (0.022)
Train: 165 [ 200/1251 ( 16%)]  Loss: 3.502 (3.69)  Time: 0.775s, 1320.73/s  (0.802s, 1277.55/s)  LR: 4.276e-04  Data: 0.012 (0.019)
Train: 165 [ 250/1251 ( 20%)]  Loss: 3.980 (3.74)  Time: 0.781s, 1311.91/s  (0.798s, 1283.53/s)  LR: 4.276e-04  Data: 0.011 (0.018)
Train: 165 [ 300/1251 ( 24%)]  Loss: 3.615 (3.72)  Time: 0.815s, 1255.87/s  (0.797s, 1284.33/s)  LR: 4.276e-04  Data: 0.011 (0.017)
Train: 165 [ 350/1251 ( 28%)]  Loss: 3.489 (3.69)  Time: 0.782s, 1309.14/s  (0.800s, 1280.77/s)  LR: 4.276e-04  Data: 0.011 (0.016)
Train: 165 [ 400/1251 ( 32%)]  Loss: 3.498 (3.67)  Time: 0.779s, 1314.18/s  (0.798s, 1283.37/s)  LR: 4.276e-04  Data: 0.011 (0.015)
Train: 165 [ 450/1251 ( 36%)]  Loss: 4.204 (3.73)  Time: 0.826s, 1239.23/s  (0.797s, 1284.89/s)  LR: 4.276e-04  Data: 0.012 (0.015)
Train: 165 [ 500/1251 ( 40%)]  Loss: 3.711 (3.72)  Time: 0.780s, 1312.60/s  (0.797s, 1284.60/s)  LR: 4.276e-04  Data: 0.011 (0.015)
Train: 165 [ 550/1251 ( 44%)]  Loss: 3.446 (3.70)  Time: 0.833s, 1229.59/s  (0.797s, 1284.26/s)  LR: 4.276e-04  Data: 0.011 (0.014)
Train: 165 [ 600/1251 ( 48%)]  Loss: 3.827 (3.71)  Time: 0.779s, 1315.09/s  (0.797s, 1284.84/s)  LR: 4.276e-04  Data: 0.011 (0.014)
Train: 165 [ 650/1251 ( 52%)]  Loss: 3.284 (3.68)  Time: 0.778s, 1315.40/s  (0.797s, 1285.45/s)  LR: 4.276e-04  Data: 0.011 (0.014)
Train: 165 [ 700/1251 ( 56%)]  Loss: 3.604 (3.68)  Time: 0.775s, 1321.18/s  (0.796s, 1286.70/s)  LR: 4.276e-04  Data: 0.011 (0.014)
Train: 165 [ 750/1251 ( 60%)]  Loss: 3.298 (3.65)  Time: 0.779s, 1313.90/s  (0.796s, 1286.66/s)  LR: 4.276e-04  Data: 0.012 (0.014)
Train: 165 [ 800/1251 ( 64%)]  Loss: 3.956 (3.67)  Time: 0.818s, 1251.61/s  (0.796s, 1286.98/s)  LR: 4.276e-04  Data: 0.012 (0.013)
Train: 165 [ 850/1251 ( 68%)]  Loss: 3.577 (3.66)  Time: 0.779s, 1313.81/s  (0.796s, 1285.89/s)  LR: 4.276e-04  Data: 0.011 (0.013)
Train: 165 [ 900/1251 ( 72%)]  Loss: 3.603 (3.66)  Time: 0.812s, 1260.77/s  (0.797s, 1284.94/s)  LR: 4.276e-04  Data: 0.011 (0.013)
Train: 165 [ 950/1251 ( 76%)]  Loss: 3.744 (3.67)  Time: 0.780s, 1312.12/s  (0.796s, 1285.79/s)  LR: 4.276e-04  Data: 0.011 (0.013)
Train: 165 [1000/1251 ( 80%)]  Loss: 3.786 (3.67)  Time: 0.790s, 1295.54/s  (0.796s, 1286.97/s)  LR: 4.276e-04  Data: 0.013 (0.013)
Train: 165 [1050/1251 ( 84%)]  Loss: 3.685 (3.67)  Time: 0.777s, 1317.41/s  (0.796s, 1286.55/s)  LR: 4.276e-04  Data: 0.011 (0.013)
Train: 165 [1100/1251 ( 88%)]  Loss: 3.566 (3.67)  Time: 0.827s, 1238.69/s  (0.796s, 1286.52/s)  LR: 4.276e-04  Data: 0.011 (0.013)
Train: 165 [1150/1251 ( 92%)]  Loss: 3.885 (3.68)  Time: 0.778s, 1316.87/s  (0.796s, 1286.73/s)  LR: 4.276e-04  Data: 0.011 (0.013)
Train: 165 [1200/1251 ( 96%)]  Loss: 3.725 (3.68)  Time: 0.781s, 1311.51/s  (0.796s, 1286.49/s)  LR: 4.276e-04  Data: 0.011 (0.013)
Train: 165 [1250/1251 (100%)]  Loss: 3.600 (3.68)  Time: 0.766s, 1335.99/s  (0.795s, 1287.38/s)  LR: 4.276e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.508 (1.508)  Loss:  0.8835 (0.8835)  Acc@1: 88.2812 (88.2812)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.172 (0.560)  Loss:  0.9430 (1.3914)  Acc@1: 84.7877 (73.4940)  Acc@5: 97.1698 (91.5560)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-165.pth.tar', 73.49399998535156)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-163.pth.tar', 73.35000001220703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-164.pth.tar', 73.30200014160157)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-159.pth.tar', 72.9660000415039)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-160.pth.tar', 72.95000006591796)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-157.pth.tar', 72.90600001464844)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-161.pth.tar', 72.71599986328125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-151.pth.tar', 72.62800001708985)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-162.pth.tar', 72.55200013916016)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-155.pth.tar', 72.5440000390625)

Train: 166 [   0/1251 (  0%)]  Loss: 3.523 (3.52)  Time: 2.225s,  460.14/s  (2.225s,  460.14/s)  LR: 4.224e-04  Data: 1.491 (1.491)
Train: 166 [  50/1251 (  4%)]  Loss: 3.334 (3.43)  Time: 0.781s, 1311.71/s  (0.821s, 1247.93/s)  LR: 4.224e-04  Data: 0.011 (0.046)
Train: 166 [ 100/1251 (  8%)]  Loss: 3.643 (3.50)  Time: 0.777s, 1318.60/s  (0.801s, 1278.70/s)  LR: 4.224e-04  Data: 0.011 (0.029)
Train: 166 [ 150/1251 ( 12%)]  Loss: 3.559 (3.51)  Time: 0.810s, 1263.89/s  (0.797s, 1284.94/s)  LR: 4.224e-04  Data: 0.011 (0.023)
Train: 166 [ 200/1251 ( 16%)]  Loss: 3.251 (3.46)  Time: 0.778s, 1316.28/s  (0.794s, 1288.98/s)  LR: 4.224e-04  Data: 0.011 (0.020)
Train: 166 [ 250/1251 ( 20%)]  Loss: 3.695 (3.50)  Time: 0.780s, 1312.43/s  (0.794s, 1289.63/s)  LR: 4.224e-04  Data: 0.011 (0.018)
Train: 166 [ 300/1251 ( 24%)]  Loss: 4.001 (3.57)  Time: 0.815s, 1256.58/s  (0.792s, 1292.85/s)  LR: 4.224e-04  Data: 0.011 (0.017)
Train: 166 [ 350/1251 ( 28%)]  Loss: 3.798 (3.60)  Time: 0.779s, 1314.95/s  (0.795s, 1287.27/s)  LR: 4.224e-04  Data: 0.011 (0.016)
Train: 166 [ 400/1251 ( 32%)]  Loss: 3.526 (3.59)  Time: 0.778s, 1316.16/s  (0.794s, 1289.71/s)  LR: 4.224e-04  Data: 0.012 (0.016)
Train: 166 [ 450/1251 ( 36%)]  Loss: 3.565 (3.59)  Time: 0.778s, 1316.07/s  (0.793s, 1291.17/s)  LR: 4.224e-04  Data: 0.011 (0.015)
Train: 166 [ 500/1251 ( 40%)]  Loss: 3.751 (3.60)  Time: 0.778s, 1315.53/s  (0.792s, 1292.72/s)  LR: 4.224e-04  Data: 0.011 (0.015)
Train: 166 [ 550/1251 ( 44%)]  Loss: 3.594 (3.60)  Time: 0.781s, 1311.45/s  (0.792s, 1292.53/s)  LR: 4.224e-04  Data: 0.011 (0.014)
Train: 166 [ 600/1251 ( 48%)]  Loss: 3.571 (3.60)  Time: 0.792s, 1292.19/s  (0.792s, 1293.41/s)  LR: 4.224e-04  Data: 0.011 (0.014)
Train: 166 [ 650/1251 ( 52%)]  Loss: 3.935 (3.62)  Time: 0.780s, 1312.09/s  (0.792s, 1293.47/s)  LR: 4.224e-04  Data: 0.011 (0.014)
Train: 166 [ 700/1251 ( 56%)]  Loss: 3.715 (3.63)  Time: 0.777s, 1318.31/s  (0.792s, 1292.35/s)  LR: 4.224e-04  Data: 0.011 (0.014)
Train: 166 [ 750/1251 ( 60%)]  Loss: 3.630 (3.63)  Time: 0.781s, 1310.85/s  (0.792s, 1293.19/s)  LR: 4.224e-04  Data: 0.011 (0.013)
Train: 166 [ 800/1251 ( 64%)]  Loss: 3.600 (3.63)  Time: 0.788s, 1299.29/s  (0.792s, 1293.17/s)  LR: 4.224e-04  Data: 0.012 (0.013)
Train: 166 [ 850/1251 ( 68%)]  Loss: 3.829 (3.64)  Time: 0.783s, 1308.30/s  (0.793s, 1291.07/s)  LR: 4.224e-04  Data: 0.011 (0.013)
Train: 166 [ 900/1251 ( 72%)]  Loss: 3.761 (3.65)  Time: 0.778s, 1315.52/s  (0.793s, 1291.31/s)  LR: 4.224e-04  Data: 0.011 (0.013)
Train: 166 [ 950/1251 ( 76%)]  Loss: 3.988 (3.66)  Time: 0.780s, 1312.65/s  (0.793s, 1291.95/s)  LR: 4.224e-04  Data: 0.013 (0.013)
Train: 166 [1000/1251 ( 80%)]  Loss: 3.615 (3.66)  Time: 0.814s, 1257.30/s  (0.793s, 1291.25/s)  LR: 4.224e-04  Data: 0.011 (0.013)
Train: 166 [1050/1251 ( 84%)]  Loss: 3.638 (3.66)  Time: 0.777s, 1317.63/s  (0.793s, 1291.15/s)  LR: 4.224e-04  Data: 0.010 (0.013)
Train: 166 [1100/1251 ( 88%)]  Loss: 3.767 (3.66)  Time: 0.789s, 1297.61/s  (0.793s, 1291.71/s)  LR: 4.224e-04  Data: 0.011 (0.013)
Train: 166 [1150/1251 ( 92%)]  Loss: 3.997 (3.68)  Time: 0.786s, 1302.67/s  (0.793s, 1291.98/s)  LR: 4.224e-04  Data: 0.011 (0.013)
Train: 166 [1200/1251 ( 96%)]  Loss: 3.949 (3.69)  Time: 0.780s, 1312.12/s  (0.793s, 1292.06/s)  LR: 4.224e-04  Data: 0.011 (0.013)
Train: 166 [1250/1251 (100%)]  Loss: 3.990 (3.70)  Time: 0.768s, 1333.05/s  (0.792s, 1292.42/s)  LR: 4.224e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.517 (1.517)  Loss:  0.9041 (0.9041)  Acc@1: 88.0859 (88.0859)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.172 (0.565)  Loss:  0.9872 (1.4592)  Acc@1: 84.3160 (73.2440)  Acc@5: 96.5802 (91.5320)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-165.pth.tar', 73.49399998535156)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-163.pth.tar', 73.35000001220703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-164.pth.tar', 73.30200014160157)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-166.pth.tar', 73.2440000390625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-159.pth.tar', 72.9660000415039)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-160.pth.tar', 72.95000006591796)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-157.pth.tar', 72.90600001464844)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-161.pth.tar', 72.71599986328125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-151.pth.tar', 72.62800001708985)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-162.pth.tar', 72.55200013916016)

Train: 167 [   0/1251 (  0%)]  Loss: 3.178 (3.18)  Time: 2.529s,  404.96/s  (2.529s,  404.96/s)  LR: 4.173e-04  Data: 1.793 (1.793)
Train: 167 [  50/1251 (  4%)]  Loss: 3.816 (3.50)  Time: 0.777s, 1318.52/s  (0.841s, 1217.82/s)  LR: 4.173e-04  Data: 0.011 (0.051)
Train: 167 [ 100/1251 (  8%)]  Loss: 3.708 (3.57)  Time: 0.778s, 1315.67/s  (0.814s, 1257.33/s)  LR: 4.173e-04  Data: 0.011 (0.031)
Train: 167 [ 150/1251 ( 12%)]  Loss: 3.694 (3.60)  Time: 0.778s, 1316.33/s  (0.808s, 1267.86/s)  LR: 4.173e-04  Data: 0.011 (0.024)
Train: 167 [ 200/1251 ( 16%)]  Loss: 3.923 (3.66)  Time: 0.790s, 1296.87/s  (0.806s, 1269.79/s)  LR: 4.173e-04  Data: 0.011 (0.021)
Train: 167 [ 250/1251 ( 20%)]  Loss: 3.852 (3.69)  Time: 0.780s, 1313.58/s  (0.803s, 1274.97/s)  LR: 4.173e-04  Data: 0.011 (0.019)
Train: 167 [ 300/1251 ( 24%)]  Loss: 3.627 (3.69)  Time: 0.783s, 1307.77/s  (0.802s, 1277.37/s)  LR: 4.173e-04  Data: 0.011 (0.018)
Train: 167 [ 350/1251 ( 28%)]  Loss: 3.652 (3.68)  Time: 0.834s, 1227.32/s  (0.801s, 1278.98/s)  LR: 4.173e-04  Data: 0.011 (0.017)
Train: 167 [ 400/1251 ( 32%)]  Loss: 3.579 (3.67)  Time: 0.777s, 1317.31/s  (0.800s, 1280.63/s)  LR: 4.173e-04  Data: 0.011 (0.016)
Train: 167 [ 450/1251 ( 36%)]  Loss: 3.683 (3.67)  Time: 0.816s, 1255.59/s  (0.799s, 1282.27/s)  LR: 4.173e-04  Data: 0.011 (0.016)
Train: 167 [ 500/1251 ( 40%)]  Loss: 3.815 (3.68)  Time: 0.777s, 1318.07/s  (0.798s, 1283.27/s)  LR: 4.173e-04  Data: 0.011 (0.015)
Train: 167 [ 550/1251 ( 44%)]  Loss: 4.003 (3.71)  Time: 0.779s, 1314.50/s  (0.797s, 1285.45/s)  LR: 4.173e-04  Data: 0.011 (0.015)
Train: 167 [ 600/1251 ( 48%)]  Loss: 3.770 (3.72)  Time: 0.804s, 1273.89/s  (0.797s, 1284.87/s)  LR: 4.173e-04  Data: 0.012 (0.014)
Train: 167 [ 650/1251 ( 52%)]  Loss: 3.977 (3.73)  Time: 0.811s, 1262.82/s  (0.797s, 1284.08/s)  LR: 4.173e-04  Data: 0.011 (0.014)
Train: 167 [ 700/1251 ( 56%)]  Loss: 3.338 (3.71)  Time: 0.781s, 1311.68/s  (0.797s, 1284.69/s)  LR: 4.173e-04  Data: 0.011 (0.014)
Train: 167 [ 750/1251 ( 60%)]  Loss: 3.253 (3.68)  Time: 0.812s, 1260.91/s  (0.798s, 1283.92/s)  LR: 4.173e-04  Data: 0.011 (0.014)
Train: 167 [ 800/1251 ( 64%)]  Loss: 3.509 (3.67)  Time: 0.831s, 1232.73/s  (0.798s, 1282.69/s)  LR: 4.173e-04  Data: 0.011 (0.014)
Train: 167 [ 850/1251 ( 68%)]  Loss: 3.629 (3.67)  Time: 0.780s, 1313.55/s  (0.798s, 1282.68/s)  LR: 4.173e-04  Data: 0.011 (0.013)
Train: 167 [ 900/1251 ( 72%)]  Loss: 2.996 (3.63)  Time: 0.809s, 1265.06/s  (0.798s, 1283.61/s)  LR: 4.173e-04  Data: 0.011 (0.013)
Train: 167 [ 950/1251 ( 76%)]  Loss: 3.814 (3.64)  Time: 0.837s, 1223.82/s  (0.798s, 1282.70/s)  LR: 4.173e-04  Data: 0.011 (0.013)
Train: 167 [1000/1251 ( 80%)]  Loss: 3.319 (3.63)  Time: 0.778s, 1316.49/s  (0.798s, 1283.64/s)  LR: 4.173e-04  Data: 0.011 (0.013)
Train: 167 [1050/1251 ( 84%)]  Loss: 3.835 (3.63)  Time: 0.791s, 1293.94/s  (0.797s, 1284.49/s)  LR: 4.173e-04  Data: 0.011 (0.013)
Train: 167 [1100/1251 ( 88%)]  Loss: 3.499 (3.63)  Time: 0.833s, 1229.99/s  (0.797s, 1284.76/s)  LR: 4.173e-04  Data: 0.011 (0.013)
Train: 167 [1150/1251 ( 92%)]  Loss: 4.161 (3.65)  Time: 0.825s, 1240.83/s  (0.797s, 1285.50/s)  LR: 4.173e-04  Data: 0.012 (0.013)
Train: 167 [1200/1251 ( 96%)]  Loss: 3.859 (3.66)  Time: 0.778s, 1315.74/s  (0.796s, 1286.13/s)  LR: 4.173e-04  Data: 0.011 (0.013)
Train: 167 [1250/1251 (100%)]  Loss: 3.739 (3.66)  Time: 0.802s, 1276.90/s  (0.797s, 1285.37/s)  LR: 4.173e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.506 (1.506)  Loss:  0.8409 (0.8409)  Acc@1: 88.2812 (88.2812)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.172 (0.562)  Loss:  0.8557 (1.4063)  Acc@1: 84.1981 (73.0500)  Acc@5: 95.7547 (91.1160)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-165.pth.tar', 73.49399998535156)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-163.pth.tar', 73.35000001220703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-164.pth.tar', 73.30200014160157)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-166.pth.tar', 73.2440000390625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-167.pth.tar', 73.04999998779297)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-159.pth.tar', 72.9660000415039)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-160.pth.tar', 72.95000006591796)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-157.pth.tar', 72.90600001464844)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-161.pth.tar', 72.71599986328125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-151.pth.tar', 72.62800001708985)

Train: 168 [   0/1251 (  0%)]  Loss: 3.569 (3.57)  Time: 2.335s,  438.49/s  (2.335s,  438.49/s)  LR: 4.122e-04  Data: 1.600 (1.600)
Train: 168 [  50/1251 (  4%)]  Loss: 3.389 (3.48)  Time: 0.778s, 1316.42/s  (0.828s, 1236.19/s)  LR: 4.122e-04  Data: 0.011 (0.047)
Train: 168 [ 100/1251 (  8%)]  Loss: 3.428 (3.46)  Time: 0.777s, 1317.17/s  (0.806s, 1271.15/s)  LR: 4.122e-04  Data: 0.011 (0.029)
Train: 168 [ 150/1251 ( 12%)]  Loss: 3.133 (3.38)  Time: 0.776s, 1320.09/s  (0.804s, 1273.19/s)  LR: 4.122e-04  Data: 0.011 (0.023)
Train: 168 [ 200/1251 ( 16%)]  Loss: 3.957 (3.50)  Time: 0.785s, 1305.00/s  (0.799s, 1280.86/s)  LR: 4.122e-04  Data: 0.010 (0.020)
Train: 168 [ 250/1251 ( 20%)]  Loss: 3.624 (3.52)  Time: 0.838s, 1222.32/s  (0.798s, 1282.69/s)  LR: 4.122e-04  Data: 0.011 (0.018)
Train: 168 [ 300/1251 ( 24%)]  Loss: 3.462 (3.51)  Time: 0.780s, 1312.51/s  (0.797s, 1284.93/s)  LR: 4.122e-04  Data: 0.011 (0.017)
Train: 168 [ 350/1251 ( 28%)]  Loss: 3.958 (3.57)  Time: 0.779s, 1313.82/s  (0.795s, 1287.99/s)  LR: 4.122e-04  Data: 0.011 (0.016)
Train: 168 [ 400/1251 ( 32%)]  Loss: 3.710 (3.58)  Time: 0.779s, 1314.62/s  (0.794s, 1290.32/s)  LR: 4.122e-04  Data: 0.011 (0.016)
Train: 168 [ 450/1251 ( 36%)]  Loss: 3.345 (3.56)  Time: 0.779s, 1314.53/s  (0.792s, 1292.12/s)  LR: 4.122e-04  Data: 0.011 (0.015)
Train: 168 [ 500/1251 ( 40%)]  Loss: 3.757 (3.58)  Time: 0.781s, 1311.02/s  (0.792s, 1292.99/s)  LR: 4.122e-04  Data: 0.011 (0.015)
Train: 168 [ 550/1251 ( 44%)]  Loss: 3.583 (3.58)  Time: 0.779s, 1314.26/s  (0.791s, 1294.90/s)  LR: 4.122e-04  Data: 0.010 (0.014)
Train: 168 [ 600/1251 ( 48%)]  Loss: 3.556 (3.57)  Time: 0.777s, 1318.57/s  (0.790s, 1296.08/s)  LR: 4.122e-04  Data: 0.011 (0.014)
Train: 168 [ 650/1251 ( 52%)]  Loss: 3.779 (3.59)  Time: 0.793s, 1290.73/s  (0.789s, 1297.06/s)  LR: 4.122e-04  Data: 0.011 (0.014)
Train: 168 [ 700/1251 ( 56%)]  Loss: 3.188 (3.56)  Time: 0.777s, 1317.32/s  (0.789s, 1297.78/s)  LR: 4.122e-04  Data: 0.011 (0.014)
Train: 168 [ 750/1251 ( 60%)]  Loss: 3.305 (3.55)  Time: 0.777s, 1318.33/s  (0.789s, 1298.45/s)  LR: 4.122e-04  Data: 0.011 (0.014)
Train: 168 [ 800/1251 ( 64%)]  Loss: 3.510 (3.54)  Time: 0.815s, 1256.59/s  (0.789s, 1298.10/s)  LR: 4.122e-04  Data: 0.011 (0.013)
Train: 168 [ 850/1251 ( 68%)]  Loss: 3.684 (3.55)  Time: 0.773s, 1324.28/s  (0.789s, 1297.90/s)  LR: 4.122e-04  Data: 0.011 (0.013)
Train: 168 [ 900/1251 ( 72%)]  Loss: 3.991 (3.58)  Time: 0.817s, 1253.17/s  (0.789s, 1297.85/s)  LR: 4.122e-04  Data: 0.013 (0.013)
Train: 168 [ 950/1251 ( 76%)]  Loss: 3.550 (3.57)  Time: 0.826s, 1239.17/s  (0.789s, 1297.52/s)  LR: 4.122e-04  Data: 0.012 (0.013)
Train: 168 [1000/1251 ( 80%)]  Loss: 3.754 (3.58)  Time: 0.804s, 1273.12/s  (0.790s, 1296.34/s)  LR: 4.122e-04  Data: 0.011 (0.013)
Train: 168 [1050/1251 ( 84%)]  Loss: 3.654 (3.59)  Time: 0.783s, 1307.81/s  (0.790s, 1295.62/s)  LR: 4.122e-04  Data: 0.011 (0.013)
Train: 168 [1100/1251 ( 88%)]  Loss: 3.209 (3.57)  Time: 0.781s, 1311.70/s  (0.790s, 1295.47/s)  LR: 4.122e-04  Data: 0.011 (0.013)
Train: 168 [1150/1251 ( 92%)]  Loss: 3.879 (3.58)  Time: 0.784s, 1305.66/s  (0.790s, 1295.85/s)  LR: 4.122e-04  Data: 0.010 (0.013)
Train: 168 [1200/1251 ( 96%)]  Loss: 3.764 (3.59)  Time: 0.818s, 1252.24/s  (0.790s, 1295.39/s)  LR: 4.122e-04  Data: 0.012 (0.013)
Train: 168 [1250/1251 (100%)]  Loss: 3.535 (3.59)  Time: 0.767s, 1334.66/s  (0.790s, 1295.70/s)  LR: 4.122e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.628 (1.628)  Loss:  0.9150 (0.9150)  Acc@1: 89.3555 (89.3555)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.172 (0.565)  Loss:  0.9756 (1.4587)  Acc@1: 84.3160 (73.2400)  Acc@5: 96.6981 (91.5400)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-165.pth.tar', 73.49399998535156)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-163.pth.tar', 73.35000001220703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-164.pth.tar', 73.30200014160157)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-166.pth.tar', 73.2440000390625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-168.pth.tar', 73.23999990966797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-167.pth.tar', 73.04999998779297)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-159.pth.tar', 72.9660000415039)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-160.pth.tar', 72.95000006591796)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-157.pth.tar', 72.90600001464844)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-161.pth.tar', 72.71599986328125)

Train: 169 [   0/1251 (  0%)]  Loss: 3.613 (3.61)  Time: 2.424s,  422.47/s  (2.424s,  422.47/s)  LR: 4.072e-04  Data: 1.690 (1.690)
Train: 169 [  50/1251 (  4%)]  Loss: 3.593 (3.60)  Time: 0.814s, 1257.83/s  (0.851s, 1202.80/s)  LR: 4.072e-04  Data: 0.012 (0.049)
Train: 169 [ 100/1251 (  8%)]  Loss: 3.936 (3.71)  Time: 0.814s, 1257.64/s  (0.833s, 1228.98/s)  LR: 4.072e-04  Data: 0.012 (0.030)
Train: 169 [ 150/1251 ( 12%)]  Loss: 3.999 (3.79)  Time: 0.780s, 1313.24/s  (0.821s, 1247.20/s)  LR: 4.072e-04  Data: 0.011 (0.024)
Train: 169 [ 200/1251 ( 16%)]  Loss: 3.339 (3.70)  Time: 0.812s, 1261.41/s  (0.815s, 1256.95/s)  LR: 4.072e-04  Data: 0.011 (0.021)
Train: 169 [ 250/1251 ( 20%)]  Loss: 3.738 (3.70)  Time: 0.782s, 1309.49/s  (0.811s, 1263.36/s)  LR: 4.072e-04  Data: 0.015 (0.019)
Train: 169 [ 300/1251 ( 24%)]  Loss: 3.729 (3.71)  Time: 0.779s, 1314.27/s  (0.808s, 1267.88/s)  LR: 4.072e-04  Data: 0.011 (0.018)
Train: 169 [ 350/1251 ( 28%)]  Loss: 3.682 (3.70)  Time: 0.778s, 1316.11/s  (0.804s, 1273.77/s)  LR: 4.072e-04  Data: 0.011 (0.017)
Train: 169 [ 400/1251 ( 32%)]  Loss: 3.287 (3.66)  Time: 0.780s, 1313.27/s  (0.802s, 1276.34/s)  LR: 4.072e-04  Data: 0.011 (0.016)
Train: 169 [ 450/1251 ( 36%)]  Loss: 3.762 (3.67)  Time: 0.813s, 1259.58/s  (0.802s, 1276.76/s)  LR: 4.072e-04  Data: 0.011 (0.015)
Train: 169 [ 500/1251 ( 40%)]  Loss: 3.629 (3.66)  Time: 0.776s, 1319.71/s  (0.802s, 1276.27/s)  LR: 4.072e-04  Data: 0.011 (0.015)
Train: 169 [ 550/1251 ( 44%)]  Loss: 3.806 (3.68)  Time: 0.790s, 1295.99/s  (0.801s, 1278.99/s)  LR: 4.072e-04  Data: 0.011 (0.015)
Train: 169 [ 600/1251 ( 48%)]  Loss: 3.285 (3.65)  Time: 0.778s, 1315.60/s  (0.799s, 1280.89/s)  LR: 4.072e-04  Data: 0.011 (0.014)
Train: 169 [ 650/1251 ( 52%)]  Loss: 3.771 (3.65)  Time: 0.774s, 1322.75/s  (0.798s, 1282.51/s)  LR: 4.072e-04  Data: 0.011 (0.014)
Train: 169 [ 700/1251 ( 56%)]  Loss: 3.805 (3.66)  Time: 0.782s, 1309.90/s  (0.797s, 1284.18/s)  LR: 4.072e-04  Data: 0.011 (0.014)
Train: 169 [ 750/1251 ( 60%)]  Loss: 3.394 (3.65)  Time: 0.811s, 1262.24/s  (0.797s, 1284.90/s)  LR: 4.072e-04  Data: 0.011 (0.014)
Train: 169 [ 800/1251 ( 64%)]  Loss: 3.440 (3.64)  Time: 0.779s, 1313.78/s  (0.797s, 1284.05/s)  LR: 4.072e-04  Data: 0.011 (0.014)
Train: 169 [ 850/1251 ( 68%)]  Loss: 3.197 (3.61)  Time: 0.777s, 1318.60/s  (0.797s, 1284.57/s)  LR: 4.072e-04  Data: 0.011 (0.013)
Train: 169 [ 900/1251 ( 72%)]  Loss: 3.977 (3.63)  Time: 0.781s, 1311.51/s  (0.796s, 1285.82/s)  LR: 4.072e-04  Data: 0.012 (0.013)
Train: 169 [ 950/1251 ( 76%)]  Loss: 3.515 (3.62)  Time: 0.837s, 1224.11/s  (0.796s, 1286.01/s)  LR: 4.072e-04  Data: 0.012 (0.013)
Train: 169 [1000/1251 ( 80%)]  Loss: 3.542 (3.62)  Time: 0.780s, 1313.64/s  (0.796s, 1286.52/s)  LR: 4.072e-04  Data: 0.011 (0.013)
Train: 169 [1050/1251 ( 84%)]  Loss: 3.609 (3.62)  Time: 0.793s, 1291.79/s  (0.795s, 1287.38/s)  LR: 4.072e-04  Data: 0.011 (0.013)
Train: 169 [1100/1251 ( 88%)]  Loss: 3.613 (3.62)  Time: 0.803s, 1275.22/s  (0.795s, 1287.45/s)  LR: 4.072e-04  Data: 0.011 (0.013)
Train: 169 [1150/1251 ( 92%)]  Loss: 3.758 (3.63)  Time: 0.777s, 1317.40/s  (0.795s, 1287.46/s)  LR: 4.072e-04  Data: 0.010 (0.013)
Train: 169 [1200/1251 ( 96%)]  Loss: 3.661 (3.63)  Time: 0.782s, 1310.05/s  (0.795s, 1287.80/s)  LR: 4.072e-04  Data: 0.011 (0.013)
Train: 169 [1250/1251 (100%)]  Loss: 3.766 (3.63)  Time: 0.769s, 1330.99/s  (0.795s, 1288.62/s)  LR: 4.072e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.585 (1.585)  Loss:  0.9467 (0.9467)  Acc@1: 88.5742 (88.5742)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.172 (0.555)  Loss:  1.0035 (1.4848)  Acc@1: 82.0755 (72.9980)  Acc@5: 95.7547 (91.4160)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-165.pth.tar', 73.49399998535156)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-163.pth.tar', 73.35000001220703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-164.pth.tar', 73.30200014160157)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-166.pth.tar', 73.2440000390625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-168.pth.tar', 73.23999990966797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-167.pth.tar', 73.04999998779297)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-169.pth.tar', 72.99800010009766)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-159.pth.tar', 72.9660000415039)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-160.pth.tar', 72.95000006591796)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-157.pth.tar', 72.90600001464844)

Train: 170 [   0/1251 (  0%)]  Loss: 4.035 (4.03)  Time: 2.150s,  476.28/s  (2.150s,  476.28/s)  LR: 4.021e-04  Data: 1.414 (1.414)
Train: 170 [  50/1251 (  4%)]  Loss: 3.755 (3.89)  Time: 0.813s, 1259.46/s  (0.821s, 1247.27/s)  LR: 4.021e-04  Data: 0.011 (0.044)
Train: 170 [ 100/1251 (  8%)]  Loss: 3.457 (3.75)  Time: 0.796s, 1286.29/s  (0.807s, 1269.51/s)  LR: 4.021e-04  Data: 0.011 (0.027)
Train: 170 [ 150/1251 ( 12%)]  Loss: 4.036 (3.82)  Time: 0.784s, 1306.54/s  (0.801s, 1278.04/s)  LR: 4.021e-04  Data: 0.010 (0.022)
Train: 170 [ 200/1251 ( 16%)]  Loss: 3.679 (3.79)  Time: 0.778s, 1316.15/s  (0.796s, 1286.08/s)  LR: 4.021e-04  Data: 0.011 (0.019)
Train: 170 [ 250/1251 ( 20%)]  Loss: 3.814 (3.80)  Time: 0.776s, 1318.92/s  (0.794s, 1289.18/s)  LR: 4.021e-04  Data: 0.011 (0.018)
Train: 170 [ 300/1251 ( 24%)]  Loss: 3.934 (3.82)  Time: 0.784s, 1305.95/s  (0.792s, 1292.73/s)  LR: 4.021e-04  Data: 0.010 (0.017)
Train: 170 [ 350/1251 ( 28%)]  Loss: 3.903 (3.83)  Time: 0.778s, 1315.49/s  (0.791s, 1294.49/s)  LR: 4.021e-04  Data: 0.011 (0.016)
Train: 170 [ 400/1251 ( 32%)]  Loss: 3.501 (3.79)  Time: 0.780s, 1313.44/s  (0.790s, 1295.78/s)  LR: 4.021e-04  Data: 0.011 (0.015)
Train: 170 [ 450/1251 ( 36%)]  Loss: 3.722 (3.78)  Time: 0.778s, 1316.95/s  (0.790s, 1295.62/s)  LR: 4.021e-04  Data: 0.011 (0.015)
Train: 170 [ 500/1251 ( 40%)]  Loss: 4.079 (3.81)  Time: 0.779s, 1313.89/s  (0.789s, 1297.40/s)  LR: 4.021e-04  Data: 0.011 (0.014)
Train: 170 [ 550/1251 ( 44%)]  Loss: 3.309 (3.77)  Time: 0.777s, 1317.32/s  (0.789s, 1297.49/s)  LR: 4.021e-04  Data: 0.011 (0.014)
Train: 170 [ 600/1251 ( 48%)]  Loss: 3.245 (3.73)  Time: 0.782s, 1309.60/s  (0.790s, 1296.46/s)  LR: 4.021e-04  Data: 0.011 (0.014)
Train: 170 [ 650/1251 ( 52%)]  Loss: 3.910 (3.74)  Time: 0.812s, 1260.60/s  (0.790s, 1295.39/s)  LR: 4.021e-04  Data: 0.011 (0.014)
Train: 170 [ 700/1251 ( 56%)]  Loss: 3.941 (3.75)  Time: 0.780s, 1313.39/s  (0.791s, 1294.45/s)  LR: 4.021e-04  Data: 0.011 (0.013)
Train: 170 [ 750/1251 ( 60%)]  Loss: 3.508 (3.74)  Time: 0.818s, 1251.42/s  (0.791s, 1294.28/s)  LR: 4.021e-04  Data: 0.011 (0.013)
Train: 170 [ 800/1251 ( 64%)]  Loss: 3.955 (3.75)  Time: 0.776s, 1319.60/s  (0.791s, 1294.44/s)  LR: 4.021e-04  Data: 0.011 (0.013)
Train: 170 [ 850/1251 ( 68%)]  Loss: 3.696 (3.75)  Time: 0.779s, 1314.42/s  (0.790s, 1295.41/s)  LR: 4.021e-04  Data: 0.011 (0.013)
Train: 170 [ 900/1251 ( 72%)]  Loss: 3.704 (3.75)  Time: 0.777s, 1318.19/s  (0.791s, 1294.66/s)  LR: 4.021e-04  Data: 0.012 (0.013)
Train: 170 [ 950/1251 ( 76%)]  Loss: 3.790 (3.75)  Time: 0.785s, 1304.07/s  (0.791s, 1295.27/s)  LR: 4.021e-04  Data: 0.011 (0.013)
Train: 170 [1000/1251 ( 80%)]  Loss: 3.606 (3.74)  Time: 0.786s, 1302.02/s  (0.790s, 1295.93/s)  LR: 4.021e-04  Data: 0.011 (0.013)
Train: 170 [1050/1251 ( 84%)]  Loss: 3.949 (3.75)  Time: 0.778s, 1315.88/s  (0.790s, 1296.58/s)  LR: 4.021e-04  Data: 0.011 (0.013)
Train: 170 [1100/1251 ( 88%)]  Loss: 3.816 (3.75)  Time: 0.779s, 1314.59/s  (0.790s, 1296.75/s)  LR: 4.021e-04  Data: 0.011 (0.013)
Train: 170 [1150/1251 ( 92%)]  Loss: 3.928 (3.76)  Time: 0.780s, 1313.06/s  (0.789s, 1297.31/s)  LR: 4.021e-04  Data: 0.011 (0.012)
Train: 170 [1200/1251 ( 96%)]  Loss: 3.583 (3.75)  Time: 0.776s, 1320.15/s  (0.789s, 1297.38/s)  LR: 4.021e-04  Data: 0.011 (0.012)
Train: 170 [1250/1251 (100%)]  Loss: 3.786 (3.76)  Time: 0.765s, 1338.94/s  (0.790s, 1296.65/s)  LR: 4.021e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.536 (1.536)  Loss:  0.7006 (0.7006)  Acc@1: 89.3555 (89.3555)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.172 (0.564)  Loss:  0.8376 (1.3211)  Acc@1: 84.0802 (73.0200)  Acc@5: 97.0519 (91.4260)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-165.pth.tar', 73.49399998535156)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-163.pth.tar', 73.35000001220703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-164.pth.tar', 73.30200014160157)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-166.pth.tar', 73.2440000390625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-168.pth.tar', 73.23999990966797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-167.pth.tar', 73.04999998779297)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-170.pth.tar', 73.02000006591797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-169.pth.tar', 72.99800010009766)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-159.pth.tar', 72.9660000415039)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-160.pth.tar', 72.95000006591796)

Train: 171 [   0/1251 (  0%)]  Loss: 3.709 (3.71)  Time: 2.373s,  431.47/s  (2.373s,  431.47/s)  LR: 3.970e-04  Data: 1.637 (1.637)
Train: 171 [  50/1251 (  4%)]  Loss: 3.882 (3.80)  Time: 0.809s, 1266.35/s  (0.838s, 1222.49/s)  LR: 3.970e-04  Data: 0.011 (0.045)
Train: 171 [ 100/1251 (  8%)]  Loss: 3.789 (3.79)  Time: 0.789s, 1298.12/s  (0.815s, 1256.36/s)  LR: 3.970e-04  Data: 0.011 (0.028)
Train: 171 [ 150/1251 ( 12%)]  Loss: 3.658 (3.76)  Time: 0.779s, 1314.84/s  (0.806s, 1270.48/s)  LR: 3.970e-04  Data: 0.011 (0.023)
Train: 171 [ 200/1251 ( 16%)]  Loss: 3.563 (3.72)  Time: 0.815s, 1256.40/s  (0.804s, 1273.83/s)  LR: 3.970e-04  Data: 0.011 (0.020)
Train: 171 [ 250/1251 ( 20%)]  Loss: 3.859 (3.74)  Time: 0.782s, 1310.00/s  (0.805s, 1271.84/s)  LR: 3.970e-04  Data: 0.011 (0.018)
Train: 171 [ 300/1251 ( 24%)]  Loss: 3.639 (3.73)  Time: 0.778s, 1316.55/s  (0.802s, 1277.55/s)  LR: 3.970e-04  Data: 0.011 (0.017)
Train: 171 [ 350/1251 ( 28%)]  Loss: 4.050 (3.77)  Time: 0.784s, 1306.06/s  (0.799s, 1281.51/s)  LR: 3.970e-04  Data: 0.011 (0.016)
Train: 171 [ 400/1251 ( 32%)]  Loss: 3.692 (3.76)  Time: 0.777s, 1318.44/s  (0.799s, 1282.31/s)  LR: 3.970e-04  Data: 0.011 (0.015)
Train: 171 [ 450/1251 ( 36%)]  Loss: 3.877 (3.77)  Time: 0.779s, 1315.32/s  (0.797s, 1284.54/s)  LR: 3.970e-04  Data: 0.011 (0.015)
Train: 171 [ 500/1251 ( 40%)]  Loss: 3.300 (3.73)  Time: 0.783s, 1308.46/s  (0.796s, 1286.84/s)  LR: 3.970e-04  Data: 0.011 (0.015)
Train: 171 [ 550/1251 ( 44%)]  Loss: 3.449 (3.71)  Time: 0.810s, 1263.88/s  (0.796s, 1285.90/s)  LR: 3.970e-04  Data: 0.011 (0.014)
Train: 171 [ 600/1251 ( 48%)]  Loss: 3.570 (3.70)  Time: 0.782s, 1309.92/s  (0.796s, 1286.68/s)  LR: 3.970e-04  Data: 0.011 (0.014)
Train: 171 [ 650/1251 ( 52%)]  Loss: 3.650 (3.69)  Time: 0.781s, 1311.81/s  (0.795s, 1287.85/s)  LR: 3.970e-04  Data: 0.011 (0.014)
Train: 171 [ 700/1251 ( 56%)]  Loss: 3.755 (3.70)  Time: 0.779s, 1315.13/s  (0.795s, 1288.05/s)  LR: 3.970e-04  Data: 0.011 (0.014)
Train: 171 [ 750/1251 ( 60%)]  Loss: 3.606 (3.69)  Time: 0.779s, 1314.62/s  (0.794s, 1289.09/s)  LR: 3.970e-04  Data: 0.010 (0.013)
Train: 171 [ 800/1251 ( 64%)]  Loss: 3.704 (3.69)  Time: 0.779s, 1314.15/s  (0.794s, 1289.28/s)  LR: 3.970e-04  Data: 0.011 (0.013)
Train: 171 [ 850/1251 ( 68%)]  Loss: 3.635 (3.69)  Time: 0.812s, 1260.54/s  (0.794s, 1289.56/s)  LR: 3.970e-04  Data: 0.011 (0.013)
Train: 171 [ 900/1251 ( 72%)]  Loss: 3.637 (3.69)  Time: 0.828s, 1237.36/s  (0.795s, 1287.81/s)  LR: 3.970e-04  Data: 0.012 (0.013)
Train: 171 [ 950/1251 ( 76%)]  Loss: 3.498 (3.68)  Time: 0.787s, 1301.53/s  (0.795s, 1288.59/s)  LR: 3.970e-04  Data: 0.011 (0.013)
Train: 171 [1000/1251 ( 80%)]  Loss: 3.739 (3.68)  Time: 0.778s, 1315.39/s  (0.794s, 1289.46/s)  LR: 3.970e-04  Data: 0.011 (0.013)
Train: 171 [1050/1251 ( 84%)]  Loss: 3.755 (3.68)  Time: 0.779s, 1314.99/s  (0.794s, 1289.84/s)  LR: 3.970e-04  Data: 0.011 (0.013)
Train: 171 [1100/1251 ( 88%)]  Loss: 3.978 (3.70)  Time: 0.814s, 1258.72/s  (0.794s, 1289.98/s)  LR: 3.970e-04  Data: 0.011 (0.013)
Train: 171 [1150/1251 ( 92%)]  Loss: 3.824 (3.70)  Time: 0.815s, 1255.75/s  (0.795s, 1288.20/s)  LR: 3.970e-04  Data: 0.011 (0.013)
Train: 171 [1200/1251 ( 96%)]  Loss: 3.576 (3.70)  Time: 0.779s, 1314.36/s  (0.795s, 1288.43/s)  LR: 3.970e-04  Data: 0.012 (0.013)
Train: 171 [1250/1251 (100%)]  Loss: 3.626 (3.69)  Time: 0.767s, 1334.61/s  (0.795s, 1288.78/s)  LR: 3.970e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.515 (1.515)  Loss:  0.9883 (0.9883)  Acc@1: 89.0625 (89.0625)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.172 (0.562)  Loss:  1.1368 (1.5155)  Acc@1: 83.1368 (73.5460)  Acc@5: 95.9906 (91.5200)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-171.pth.tar', 73.54600004394531)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-165.pth.tar', 73.49399998535156)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-163.pth.tar', 73.35000001220703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-164.pth.tar', 73.30200014160157)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-166.pth.tar', 73.2440000390625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-168.pth.tar', 73.23999990966797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-167.pth.tar', 73.04999998779297)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-170.pth.tar', 73.02000006591797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-169.pth.tar', 72.99800010009766)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-159.pth.tar', 72.9660000415039)

Train: 172 [   0/1251 (  0%)]  Loss: 3.566 (3.57)  Time: 2.243s,  456.59/s  (2.243s,  456.59/s)  LR: 3.920e-04  Data: 1.507 (1.507)
Train: 172 [  50/1251 (  4%)]  Loss: 3.351 (3.46)  Time: 0.783s, 1307.30/s  (0.823s, 1244.46/s)  LR: 3.920e-04  Data: 0.014 (0.042)
Train: 172 [ 100/1251 (  8%)]  Loss: 3.695 (3.54)  Time: 0.780s, 1313.54/s  (0.803s, 1275.77/s)  LR: 3.920e-04  Data: 0.011 (0.027)
Train: 172 [ 150/1251 ( 12%)]  Loss: 3.724 (3.58)  Time: 0.779s, 1314.99/s  (0.795s, 1287.28/s)  LR: 3.920e-04  Data: 0.011 (0.022)
Train: 172 [ 200/1251 ( 16%)]  Loss: 3.961 (3.66)  Time: 0.814s, 1258.24/s  (0.800s, 1279.54/s)  LR: 3.920e-04  Data: 0.011 (0.019)
Train: 172 [ 250/1251 ( 20%)]  Loss: 3.973 (3.71)  Time: 0.779s, 1313.78/s  (0.800s, 1280.65/s)  LR: 3.920e-04  Data: 0.011 (0.018)
Train: 172 [ 300/1251 ( 24%)]  Loss: 3.505 (3.68)  Time: 0.820s, 1248.08/s  (0.798s, 1283.28/s)  LR: 3.920e-04  Data: 0.011 (0.017)
Train: 172 [ 350/1251 ( 28%)]  Loss: 3.796 (3.70)  Time: 0.822s, 1245.30/s  (0.799s, 1281.93/s)  LR: 3.920e-04  Data: 0.011 (0.016)
Train: 172 [ 400/1251 ( 32%)]  Loss: 3.684 (3.69)  Time: 0.776s, 1319.94/s  (0.799s, 1282.31/s)  LR: 3.920e-04  Data: 0.011 (0.015)
Train: 172 [ 450/1251 ( 36%)]  Loss: 4.135 (3.74)  Time: 0.778s, 1316.60/s  (0.797s, 1285.29/s)  LR: 3.920e-04  Data: 0.010 (0.015)
Train: 172 [ 500/1251 ( 40%)]  Loss: 3.341 (3.70)  Time: 0.780s, 1312.92/s  (0.797s, 1284.90/s)  LR: 3.920e-04  Data: 0.011 (0.014)
Train: 172 [ 550/1251 ( 44%)]  Loss: 3.530 (3.69)  Time: 0.780s, 1312.51/s  (0.796s, 1286.16/s)  LR: 3.920e-04  Data: 0.011 (0.014)
Train: 172 [ 600/1251 ( 48%)]  Loss: 4.033 (3.71)  Time: 0.779s, 1313.91/s  (0.795s, 1287.80/s)  LR: 3.920e-04  Data: 0.011 (0.014)
Train: 172 [ 650/1251 ( 52%)]  Loss: 3.657 (3.71)  Time: 0.776s, 1320.04/s  (0.794s, 1289.33/s)  LR: 3.920e-04  Data: 0.010 (0.014)
Train: 172 [ 700/1251 ( 56%)]  Loss: 3.884 (3.72)  Time: 0.780s, 1313.21/s  (0.795s, 1287.61/s)  LR: 3.920e-04  Data: 0.010 (0.013)
Train: 172 [ 750/1251 ( 60%)]  Loss: 3.810 (3.73)  Time: 0.779s, 1313.70/s  (0.795s, 1288.79/s)  LR: 3.920e-04  Data: 0.011 (0.013)
Train: 172 [ 800/1251 ( 64%)]  Loss: 3.631 (3.72)  Time: 0.778s, 1315.69/s  (0.794s, 1289.98/s)  LR: 3.920e-04  Data: 0.011 (0.013)
Train: 172 [ 850/1251 ( 68%)]  Loss: 3.479 (3.71)  Time: 0.786s, 1303.28/s  (0.794s, 1289.44/s)  LR: 3.920e-04  Data: 0.011 (0.013)
Train: 172 [ 900/1251 ( 72%)]  Loss: 3.533 (3.70)  Time: 0.777s, 1317.26/s  (0.794s, 1289.73/s)  LR: 3.920e-04  Data: 0.011 (0.013)
Train: 172 [ 950/1251 ( 76%)]  Loss: 3.671 (3.70)  Time: 0.778s, 1316.39/s  (0.794s, 1289.16/s)  LR: 3.920e-04  Data: 0.011 (0.013)
Train: 172 [1000/1251 ( 80%)]  Loss: 3.898 (3.71)  Time: 0.779s, 1314.10/s  (0.794s, 1289.84/s)  LR: 3.920e-04  Data: 0.011 (0.013)
Train: 172 [1050/1251 ( 84%)]  Loss: 3.703 (3.71)  Time: 0.821s, 1247.19/s  (0.794s, 1288.93/s)  LR: 3.920e-04  Data: 0.011 (0.013)
Train: 172 [1100/1251 ( 88%)]  Loss: 3.911 (3.72)  Time: 0.779s, 1314.00/s  (0.795s, 1288.84/s)  LR: 3.920e-04  Data: 0.011 (0.013)
Train: 172 [1150/1251 ( 92%)]  Loss: 3.132 (3.69)  Time: 0.779s, 1314.08/s  (0.794s, 1289.62/s)  LR: 3.920e-04  Data: 0.012 (0.013)
Train: 172 [1200/1251 ( 96%)]  Loss: 3.608 (3.69)  Time: 0.802s, 1276.96/s  (0.794s, 1290.01/s)  LR: 3.920e-04  Data: 0.010 (0.012)
Train: 172 [1250/1251 (100%)]  Loss: 3.383 (3.68)  Time: 0.803s, 1275.54/s  (0.794s, 1288.95/s)  LR: 3.920e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.510 (1.510)  Loss:  0.8468 (0.8468)  Acc@1: 87.7930 (87.7930)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.172 (0.569)  Loss:  1.0733 (1.4535)  Acc@1: 82.4292 (73.3520)  Acc@5: 94.9292 (91.5320)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-171.pth.tar', 73.54600004394531)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-165.pth.tar', 73.49399998535156)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-172.pth.tar', 73.35199999511718)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-163.pth.tar', 73.35000001220703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-164.pth.tar', 73.30200014160157)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-166.pth.tar', 73.2440000390625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-168.pth.tar', 73.23999990966797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-167.pth.tar', 73.04999998779297)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-170.pth.tar', 73.02000006591797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-169.pth.tar', 72.99800010009766)

Train: 173 [   0/1251 (  0%)]  Loss: 3.596 (3.60)  Time: 2.270s,  451.13/s  (2.270s,  451.13/s)  LR: 3.869e-04  Data: 1.534 (1.534)
Train: 173 [  50/1251 (  4%)]  Loss: 3.967 (3.78)  Time: 0.778s, 1317.04/s  (0.835s, 1225.63/s)  LR: 3.869e-04  Data: 0.010 (0.047)
Train: 173 [ 100/1251 (  8%)]  Loss: 3.840 (3.80)  Time: 0.813s, 1260.17/s  (0.821s, 1246.51/s)  LR: 3.869e-04  Data: 0.011 (0.029)
Train: 173 [ 150/1251 ( 12%)]  Loss: 3.608 (3.75)  Time: 0.788s, 1298.71/s  (0.814s, 1257.46/s)  LR: 3.869e-04  Data: 0.011 (0.023)
Train: 173 [ 200/1251 ( 16%)]  Loss: 3.549 (3.71)  Time: 0.779s, 1314.72/s  (0.809s, 1265.49/s)  LR: 3.869e-04  Data: 0.011 (0.020)
Train: 173 [ 250/1251 ( 20%)]  Loss: 3.934 (3.75)  Time: 0.777s, 1317.95/s  (0.805s, 1272.81/s)  LR: 3.869e-04  Data: 0.011 (0.018)
Train: 173 [ 300/1251 ( 24%)]  Loss: 3.163 (3.67)  Time: 0.781s, 1311.49/s  (0.801s, 1278.72/s)  LR: 3.869e-04  Data: 0.011 (0.017)
Train: 173 [ 350/1251 ( 28%)]  Loss: 3.550 (3.65)  Time: 0.851s, 1202.90/s  (0.800s, 1279.99/s)  LR: 3.869e-04  Data: 0.011 (0.016)
Train: 173 [ 400/1251 ( 32%)]  Loss: 3.459 (3.63)  Time: 0.778s, 1315.83/s  (0.799s, 1281.91/s)  LR: 3.869e-04  Data: 0.013 (0.016)
Train: 173 [ 450/1251 ( 36%)]  Loss: 4.113 (3.68)  Time: 0.778s, 1315.39/s  (0.797s, 1285.14/s)  LR: 3.869e-04  Data: 0.011 (0.015)
Train: 173 [ 500/1251 ( 40%)]  Loss: 3.663 (3.68)  Time: 0.819s, 1250.70/s  (0.798s, 1283.76/s)  LR: 3.869e-04  Data: 0.011 (0.015)
Train: 173 [ 550/1251 ( 44%)]  Loss: 3.788 (3.69)  Time: 0.779s, 1315.04/s  (0.797s, 1284.09/s)  LR: 3.869e-04  Data: 0.011 (0.014)
Train: 173 [ 600/1251 ( 48%)]  Loss: 3.552 (3.68)  Time: 0.777s, 1317.27/s  (0.798s, 1282.60/s)  LR: 3.869e-04  Data: 0.011 (0.014)
Train: 173 [ 650/1251 ( 52%)]  Loss: 3.805 (3.68)  Time: 0.828s, 1236.10/s  (0.797s, 1284.06/s)  LR: 3.869e-04  Data: 0.011 (0.014)
Train: 173 [ 700/1251 ( 56%)]  Loss: 3.732 (3.69)  Time: 0.781s, 1310.69/s  (0.797s, 1285.07/s)  LR: 3.869e-04  Data: 0.012 (0.014)
Train: 173 [ 750/1251 ( 60%)]  Loss: 3.718 (3.69)  Time: 0.782s, 1309.52/s  (0.796s, 1286.27/s)  LR: 3.869e-04  Data: 0.011 (0.014)
Train: 173 [ 800/1251 ( 64%)]  Loss: 3.893 (3.70)  Time: 0.809s, 1265.24/s  (0.796s, 1286.97/s)  LR: 3.869e-04  Data: 0.011 (0.013)
Train: 173 [ 850/1251 ( 68%)]  Loss: 3.988 (3.72)  Time: 0.812s, 1260.39/s  (0.796s, 1286.69/s)  LR: 3.869e-04  Data: 0.011 (0.013)
Train: 173 [ 900/1251 ( 72%)]  Loss: 4.027 (3.73)  Time: 0.815s, 1256.00/s  (0.796s, 1286.37/s)  LR: 3.869e-04  Data: 0.011 (0.013)
Train: 173 [ 950/1251 ( 76%)]  Loss: 3.585 (3.73)  Time: 0.799s, 1281.83/s  (0.796s, 1286.65/s)  LR: 3.869e-04  Data: 0.011 (0.013)
Train: 173 [1000/1251 ( 80%)]  Loss: 3.757 (3.73)  Time: 0.778s, 1316.12/s  (0.795s, 1287.62/s)  LR: 3.869e-04  Data: 0.011 (0.013)
Train: 173 [1050/1251 ( 84%)]  Loss: 3.936 (3.74)  Time: 0.779s, 1314.27/s  (0.795s, 1287.61/s)  LR: 3.869e-04  Data: 0.011 (0.013)
Train: 173 [1100/1251 ( 88%)]  Loss: 3.576 (3.73)  Time: 0.815s, 1256.94/s  (0.795s, 1288.02/s)  LR: 3.869e-04  Data: 0.011 (0.013)
Train: 173 [1150/1251 ( 92%)]  Loss: 3.880 (3.74)  Time: 0.813s, 1260.14/s  (0.795s, 1287.50/s)  LR: 3.869e-04  Data: 0.011 (0.013)
Train: 173 [1200/1251 ( 96%)]  Loss: 3.460 (3.73)  Time: 0.785s, 1305.25/s  (0.795s, 1288.11/s)  LR: 3.869e-04  Data: 0.011 (0.013)
Train: 173 [1250/1251 (100%)]  Loss: 3.524 (3.72)  Time: 0.769s, 1331.31/s  (0.795s, 1287.43/s)  LR: 3.869e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.497 (1.497)  Loss:  0.7562 (0.7562)  Acc@1: 88.2812 (88.2812)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.172 (0.560)  Loss:  0.7868 (1.3420)  Acc@1: 84.5519 (73.3780)  Acc@5: 96.4623 (91.5280)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-171.pth.tar', 73.54600004394531)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-165.pth.tar', 73.49399998535156)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-173.pth.tar', 73.37800001220702)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-172.pth.tar', 73.35199999511718)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-163.pth.tar', 73.35000001220703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-164.pth.tar', 73.30200014160157)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-166.pth.tar', 73.2440000390625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-168.pth.tar', 73.23999990966797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-167.pth.tar', 73.04999998779297)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-170.pth.tar', 73.02000006591797)

Train: 174 [   0/1251 (  0%)]  Loss: 3.494 (3.49)  Time: 2.361s,  433.80/s  (2.361s,  433.80/s)  LR: 3.819e-04  Data: 1.508 (1.508)
Train: 174 [  50/1251 (  4%)]  Loss: 3.802 (3.65)  Time: 0.777s, 1317.51/s  (0.832s, 1230.94/s)  LR: 3.819e-04  Data: 0.012 (0.045)
Train: 174 [ 100/1251 (  8%)]  Loss: 3.575 (3.62)  Time: 0.779s, 1313.91/s  (0.813s, 1258.78/s)  LR: 3.819e-04  Data: 0.011 (0.028)
Train: 174 [ 150/1251 ( 12%)]  Loss: 3.361 (3.56)  Time: 0.786s, 1302.86/s  (0.804s, 1273.80/s)  LR: 3.819e-04  Data: 0.012 (0.023)
Train: 174 [ 200/1251 ( 16%)]  Loss: 3.607 (3.57)  Time: 0.788s, 1299.91/s  (0.799s, 1281.92/s)  LR: 3.819e-04  Data: 0.011 (0.020)
Train: 174 [ 250/1251 ( 20%)]  Loss: 3.834 (3.61)  Time: 0.779s, 1313.98/s  (0.796s, 1286.47/s)  LR: 3.819e-04  Data: 0.010 (0.018)
Train: 174 [ 300/1251 ( 24%)]  Loss: 3.965 (3.66)  Time: 0.788s, 1299.97/s  (0.796s, 1286.82/s)  LR: 3.819e-04  Data: 0.010 (0.017)
Train: 174 [ 350/1251 ( 28%)]  Loss: 3.518 (3.64)  Time: 0.777s, 1318.22/s  (0.794s, 1288.94/s)  LR: 3.819e-04  Data: 0.011 (0.016)
Train: 174 [ 400/1251 ( 32%)]  Loss: 3.823 (3.66)  Time: 0.780s, 1313.64/s  (0.794s, 1290.43/s)  LR: 3.819e-04  Data: 0.011 (0.015)
Train: 174 [ 450/1251 ( 36%)]  Loss: 3.820 (3.68)  Time: 0.780s, 1313.58/s  (0.793s, 1291.56/s)  LR: 3.819e-04  Data: 0.011 (0.015)
Train: 174 [ 500/1251 ( 40%)]  Loss: 3.595 (3.67)  Time: 0.778s, 1316.13/s  (0.792s, 1292.34/s)  LR: 3.819e-04  Data: 0.011 (0.014)
Train: 174 [ 550/1251 ( 44%)]  Loss: 3.858 (3.69)  Time: 0.819s, 1250.87/s  (0.792s, 1292.84/s)  LR: 3.819e-04  Data: 0.012 (0.014)
Train: 174 [ 600/1251 ( 48%)]  Loss: 3.777 (3.69)  Time: 0.781s, 1311.17/s  (0.792s, 1293.18/s)  LR: 3.819e-04  Data: 0.011 (0.014)
Train: 174 [ 650/1251 ( 52%)]  Loss: 4.003 (3.72)  Time: 0.780s, 1312.32/s  (0.791s, 1294.36/s)  LR: 3.819e-04  Data: 0.011 (0.014)
Train: 174 [ 700/1251 ( 56%)]  Loss: 3.622 (3.71)  Time: 0.819s, 1249.95/s  (0.792s, 1293.21/s)  LR: 3.819e-04  Data: 0.011 (0.014)
Train: 174 [ 750/1251 ( 60%)]  Loss: 3.358 (3.69)  Time: 0.820s, 1249.01/s  (0.791s, 1293.89/s)  LR: 3.819e-04  Data: 0.011 (0.013)
Train: 174 [ 800/1251 ( 64%)]  Loss: 3.739 (3.69)  Time: 0.778s, 1316.44/s  (0.792s, 1293.59/s)  LR: 3.819e-04  Data: 0.012 (0.013)
Train: 174 [ 850/1251 ( 68%)]  Loss: 3.659 (3.69)  Time: 0.776s, 1319.00/s  (0.792s, 1293.41/s)  LR: 3.819e-04  Data: 0.011 (0.013)
Train: 174 [ 900/1251 ( 72%)]  Loss: 3.528 (3.68)  Time: 0.779s, 1314.24/s  (0.791s, 1293.81/s)  LR: 3.819e-04  Data: 0.011 (0.013)
Train: 174 [ 950/1251 ( 76%)]  Loss: 3.647 (3.68)  Time: 0.780s, 1312.06/s  (0.791s, 1293.99/s)  LR: 3.819e-04  Data: 0.011 (0.013)
Train: 174 [1000/1251 ( 80%)]  Loss: 3.406 (3.67)  Time: 0.820s, 1248.53/s  (0.792s, 1293.05/s)  LR: 3.819e-04  Data: 0.011 (0.013)
Train: 174 [1050/1251 ( 84%)]  Loss: 3.423 (3.66)  Time: 0.781s, 1310.40/s  (0.792s, 1293.59/s)  LR: 3.819e-04  Data: 0.011 (0.013)
Train: 174 [1100/1251 ( 88%)]  Loss: 3.835 (3.66)  Time: 0.835s, 1226.50/s  (0.792s, 1292.67/s)  LR: 3.819e-04  Data: 0.011 (0.013)
Train: 174 [1150/1251 ( 92%)]  Loss: 3.522 (3.66)  Time: 0.775s, 1320.88/s  (0.792s, 1292.94/s)  LR: 3.819e-04  Data: 0.010 (0.013)
Train: 174 [1200/1251 ( 96%)]  Loss: 3.798 (3.66)  Time: 0.815s, 1256.40/s  (0.792s, 1293.45/s)  LR: 3.819e-04  Data: 0.011 (0.012)
Train: 174 [1250/1251 (100%)]  Loss: 3.549 (3.66)  Time: 0.801s, 1278.57/s  (0.791s, 1293.95/s)  LR: 3.819e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.530 (1.530)  Loss:  0.8604 (0.8604)  Acc@1: 89.0625 (89.0625)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.172 (0.563)  Loss:  0.9269 (1.4224)  Acc@1: 84.5519 (73.7020)  Acc@5: 95.2830 (91.6940)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-174.pth.tar', 73.70200001220704)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-171.pth.tar', 73.54600004394531)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-165.pth.tar', 73.49399998535156)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-173.pth.tar', 73.37800001220702)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-172.pth.tar', 73.35199999511718)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-163.pth.tar', 73.35000001220703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-164.pth.tar', 73.30200014160157)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-166.pth.tar', 73.2440000390625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-168.pth.tar', 73.23999990966797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-167.pth.tar', 73.04999998779297)

Train: 175 [   0/1251 (  0%)]  Loss: 3.434 (3.43)  Time: 2.305s,  444.18/s  (2.305s,  444.18/s)  LR: 3.769e-04  Data: 1.568 (1.568)
Train: 175 [  50/1251 (  4%)]  Loss: 3.726 (3.58)  Time: 0.780s, 1312.82/s  (0.819s, 1251.06/s)  LR: 3.769e-04  Data: 0.011 (0.043)
Train: 175 [ 100/1251 (  8%)]  Loss: 3.808 (3.66)  Time: 0.778s, 1316.86/s  (0.806s, 1270.44/s)  LR: 3.769e-04  Data: 0.011 (0.027)
Train: 175 [ 150/1251 ( 12%)]  Loss: 4.025 (3.75)  Time: 0.820s, 1248.93/s  (0.800s, 1279.37/s)  LR: 3.769e-04  Data: 0.011 (0.022)
Train: 175 [ 200/1251 ( 16%)]  Loss: 3.803 (3.76)  Time: 0.831s, 1231.60/s  (0.797s, 1284.37/s)  LR: 3.769e-04  Data: 0.014 (0.019)
Train: 175 [ 250/1251 ( 20%)]  Loss: 3.685 (3.75)  Time: 0.780s, 1312.54/s  (0.799s, 1282.04/s)  LR: 3.769e-04  Data: 0.011 (0.018)
Train: 175 [ 300/1251 ( 24%)]  Loss: 3.614 (3.73)  Time: 0.822s, 1245.98/s  (0.800s, 1280.41/s)  LR: 3.769e-04  Data: 0.011 (0.016)
Train: 175 [ 350/1251 ( 28%)]  Loss: 3.304 (3.67)  Time: 0.777s, 1318.30/s  (0.800s, 1279.72/s)  LR: 3.769e-04  Data: 0.010 (0.016)
Train: 175 [ 400/1251 ( 32%)]  Loss: 3.829 (3.69)  Time: 0.782s, 1309.32/s  (0.800s, 1279.46/s)  LR: 3.769e-04  Data: 0.014 (0.015)
Train: 175 [ 450/1251 ( 36%)]  Loss: 3.912 (3.71)  Time: 0.776s, 1320.25/s  (0.800s, 1279.62/s)  LR: 3.769e-04  Data: 0.011 (0.015)
Train: 175 [ 500/1251 ( 40%)]  Loss: 3.462 (3.69)  Time: 0.778s, 1316.12/s  (0.799s, 1281.61/s)  LR: 3.769e-04  Data: 0.011 (0.014)
Train: 175 [ 550/1251 ( 44%)]  Loss: 3.628 (3.69)  Time: 0.826s, 1240.37/s  (0.798s, 1283.68/s)  LR: 3.769e-04  Data: 0.011 (0.014)
Train: 175 [ 600/1251 ( 48%)]  Loss: 3.518 (3.67)  Time: 0.793s, 1291.49/s  (0.799s, 1281.87/s)  LR: 3.769e-04  Data: 0.012 (0.014)
Train: 175 [ 650/1251 ( 52%)]  Loss: 3.713 (3.68)  Time: 0.780s, 1313.36/s  (0.798s, 1283.20/s)  LR: 3.769e-04  Data: 0.011 (0.014)
Train: 175 [ 700/1251 ( 56%)]  Loss: 3.120 (3.64)  Time: 0.814s, 1257.90/s  (0.797s, 1284.53/s)  LR: 3.769e-04  Data: 0.012 (0.013)
Train: 175 [ 750/1251 ( 60%)]  Loss: 3.712 (3.64)  Time: 0.780s, 1312.81/s  (0.797s, 1284.47/s)  LR: 3.769e-04  Data: 0.011 (0.013)
Train: 175 [ 800/1251 ( 64%)]  Loss: 3.523 (3.64)  Time: 0.875s, 1170.65/s  (0.798s, 1283.75/s)  LR: 3.769e-04  Data: 0.011 (0.013)
Train: 175 [ 850/1251 ( 68%)]  Loss: 3.613 (3.63)  Time: 0.779s, 1314.39/s  (0.798s, 1283.92/s)  LR: 3.769e-04  Data: 0.011 (0.013)
Train: 175 [ 900/1251 ( 72%)]  Loss: 3.663 (3.64)  Time: 0.804s, 1272.88/s  (0.798s, 1283.80/s)  LR: 3.769e-04  Data: 0.011 (0.013)
Train: 175 [ 950/1251 ( 76%)]  Loss: 3.640 (3.64)  Time: 0.779s, 1313.72/s  (0.797s, 1284.41/s)  LR: 3.769e-04  Data: 0.011 (0.013)
Train: 175 [1000/1251 ( 80%)]  Loss: 3.729 (3.64)  Time: 0.820s, 1248.38/s  (0.797s, 1284.61/s)  LR: 3.769e-04  Data: 0.011 (0.013)
Train: 175 [1050/1251 ( 84%)]  Loss: 3.462 (3.63)  Time: 0.778s, 1315.72/s  (0.797s, 1284.22/s)  LR: 3.769e-04  Data: 0.011 (0.013)
Train: 175 [1100/1251 ( 88%)]  Loss: 3.816 (3.64)  Time: 0.878s, 1166.45/s  (0.797s, 1284.08/s)  LR: 3.769e-04  Data: 0.011 (0.013)
Train: 175 [1150/1251 ( 92%)]  Loss: 3.830 (3.65)  Time: 0.778s, 1316.77/s  (0.797s, 1284.02/s)  LR: 3.769e-04  Data: 0.011 (0.013)
Train: 175 [1200/1251 ( 96%)]  Loss: 3.377 (3.64)  Time: 0.778s, 1315.54/s  (0.797s, 1284.64/s)  LR: 3.769e-04  Data: 0.011 (0.012)
Train: 175 [1250/1251 (100%)]  Loss: 3.767 (3.64)  Time: 0.807s, 1269.34/s  (0.797s, 1284.60/s)  LR: 3.769e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.562 (1.562)  Loss:  0.9361 (0.9361)  Acc@1: 87.2070 (87.2070)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.172 (0.563)  Loss:  1.0983 (1.5164)  Acc@1: 83.9623 (73.4040)  Acc@5: 96.8160 (91.5900)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-174.pth.tar', 73.70200001220704)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-171.pth.tar', 73.54600004394531)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-165.pth.tar', 73.49399998535156)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-175.pth.tar', 73.40400001464843)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-173.pth.tar', 73.37800001220702)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-172.pth.tar', 73.35199999511718)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-163.pth.tar', 73.35000001220703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-164.pth.tar', 73.30200014160157)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-166.pth.tar', 73.2440000390625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-168.pth.tar', 73.23999990966797)

Train: 176 [   0/1251 (  0%)]  Loss: 3.466 (3.47)  Time: 2.437s,  420.15/s  (2.437s,  420.15/s)  LR: 3.719e-04  Data: 1.700 (1.700)
Train: 176 [  50/1251 (  4%)]  Loss: 3.738 (3.60)  Time: 0.778s, 1315.55/s  (0.834s, 1227.67/s)  LR: 3.719e-04  Data: 0.011 (0.052)
Train: 176 [ 100/1251 (  8%)]  Loss: 3.369 (3.52)  Time: 0.782s, 1309.30/s  (0.810s, 1263.54/s)  LR: 3.719e-04  Data: 0.011 (0.031)
Train: 176 [ 150/1251 ( 12%)]  Loss: 3.502 (3.52)  Time: 0.777s, 1317.38/s  (0.811s, 1263.10/s)  LR: 3.719e-04  Data: 0.011 (0.025)
Train: 176 [ 200/1251 ( 16%)]  Loss: 4.015 (3.62)  Time: 0.821s, 1247.93/s  (0.806s, 1269.79/s)  LR: 3.719e-04  Data: 0.011 (0.021)
Train: 176 [ 250/1251 ( 20%)]  Loss: 3.933 (3.67)  Time: 0.816s, 1255.25/s  (0.807s, 1269.46/s)  LR: 3.719e-04  Data: 0.011 (0.019)
Train: 176 [ 300/1251 ( 24%)]  Loss: 3.745 (3.68)  Time: 0.814s, 1258.29/s  (0.805s, 1272.67/s)  LR: 3.719e-04  Data: 0.011 (0.018)
Train: 176 [ 350/1251 ( 28%)]  Loss: 3.425 (3.65)  Time: 0.788s, 1299.86/s  (0.801s, 1278.04/s)  LR: 3.719e-04  Data: 0.011 (0.017)
Train: 176 [ 400/1251 ( 32%)]  Loss: 3.586 (3.64)  Time: 0.779s, 1315.30/s  (0.800s, 1280.57/s)  LR: 3.719e-04  Data: 0.011 (0.016)
Train: 176 [ 450/1251 ( 36%)]  Loss: 3.595 (3.64)  Time: 0.779s, 1315.14/s  (0.798s, 1283.73/s)  LR: 3.719e-04  Data: 0.011 (0.016)
Train: 176 [ 500/1251 ( 40%)]  Loss: 3.806 (3.65)  Time: 0.779s, 1314.69/s  (0.796s, 1285.93/s)  LR: 3.719e-04  Data: 0.011 (0.015)
Train: 176 [ 550/1251 ( 44%)]  Loss: 3.508 (3.64)  Time: 0.781s, 1310.84/s  (0.796s, 1286.88/s)  LR: 3.719e-04  Data: 0.011 (0.015)
Train: 176 [ 600/1251 ( 48%)]  Loss: 3.834 (3.66)  Time: 0.780s, 1312.52/s  (0.796s, 1286.03/s)  LR: 3.719e-04  Data: 0.011 (0.015)
Train: 176 [ 650/1251 ( 52%)]  Loss: 3.336 (3.63)  Time: 0.782s, 1310.09/s  (0.795s, 1287.96/s)  LR: 3.719e-04  Data: 0.011 (0.014)
Train: 176 [ 700/1251 ( 56%)]  Loss: 3.591 (3.63)  Time: 0.777s, 1317.47/s  (0.794s, 1288.88/s)  LR: 3.719e-04  Data: 0.011 (0.014)
Train: 176 [ 750/1251 ( 60%)]  Loss: 3.574 (3.63)  Time: 0.778s, 1317.00/s  (0.795s, 1288.52/s)  LR: 3.719e-04  Data: 0.011 (0.014)
Train: 176 [ 800/1251 ( 64%)]  Loss: 3.852 (3.64)  Time: 0.778s, 1315.76/s  (0.795s, 1288.05/s)  LR: 3.719e-04  Data: 0.011 (0.014)
Train: 176 [ 850/1251 ( 68%)]  Loss: 3.672 (3.64)  Time: 0.777s, 1318.27/s  (0.794s, 1289.46/s)  LR: 3.719e-04  Data: 0.011 (0.014)
Train: 176 [ 900/1251 ( 72%)]  Loss: 3.797 (3.65)  Time: 0.826s, 1239.75/s  (0.794s, 1289.26/s)  LR: 3.719e-04  Data: 0.011 (0.013)
Train: 176 [ 950/1251 ( 76%)]  Loss: 3.726 (3.65)  Time: 0.778s, 1315.59/s  (0.795s, 1288.55/s)  LR: 3.719e-04  Data: 0.011 (0.013)
Train: 176 [1000/1251 ( 80%)]  Loss: 3.772 (3.66)  Time: 0.779s, 1314.56/s  (0.794s, 1289.59/s)  LR: 3.719e-04  Data: 0.011 (0.013)
Train: 176 [1050/1251 ( 84%)]  Loss: 3.656 (3.66)  Time: 0.779s, 1314.61/s  (0.793s, 1290.64/s)  LR: 3.719e-04  Data: 0.012 (0.013)
Train: 176 [1100/1251 ( 88%)]  Loss: 3.908 (3.67)  Time: 0.777s, 1317.71/s  (0.793s, 1291.44/s)  LR: 3.719e-04  Data: 0.011 (0.013)
Train: 176 [1150/1251 ( 92%)]  Loss: 3.819 (3.68)  Time: 0.846s, 1210.64/s  (0.793s, 1291.24/s)  LR: 3.719e-04  Data: 0.011 (0.013)
Train: 176 [1200/1251 ( 96%)]  Loss: 3.682 (3.68)  Time: 0.778s, 1315.63/s  (0.793s, 1291.06/s)  LR: 3.719e-04  Data: 0.011 (0.013)
Train: 176 [1250/1251 (100%)]  Loss: 3.578 (3.67)  Time: 0.768s, 1333.82/s  (0.793s, 1291.77/s)  LR: 3.719e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.555 (1.555)  Loss:  0.8991 (0.8991)  Acc@1: 87.6953 (87.6953)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.172 (0.565)  Loss:  0.9004 (1.4166)  Acc@1: 85.4953 (73.5560)  Acc@5: 96.4623 (91.7540)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-174.pth.tar', 73.70200001220704)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-176.pth.tar', 73.55600003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-171.pth.tar', 73.54600004394531)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-165.pth.tar', 73.49399998535156)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-175.pth.tar', 73.40400001464843)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-173.pth.tar', 73.37800001220702)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-172.pth.tar', 73.35199999511718)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-163.pth.tar', 73.35000001220703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-164.pth.tar', 73.30200014160157)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-166.pth.tar', 73.2440000390625)

Train: 177 [   0/1251 (  0%)]  Loss: 3.576 (3.58)  Time: 2.345s,  436.75/s  (2.345s,  436.75/s)  LR: 3.669e-04  Data: 1.611 (1.611)
Train: 177 [  50/1251 (  4%)]  Loss: 3.642 (3.61)  Time: 0.781s, 1311.64/s  (0.833s, 1229.30/s)  LR: 3.669e-04  Data: 0.011 (0.048)
Train: 177 [ 100/1251 (  8%)]  Loss: 3.698 (3.64)  Time: 0.792s, 1293.55/s  (0.811s, 1261.93/s)  LR: 3.669e-04  Data: 0.011 (0.030)
Train: 177 [ 150/1251 ( 12%)]  Loss: 3.667 (3.65)  Time: 0.779s, 1313.90/s  (0.803s, 1275.72/s)  LR: 3.669e-04  Data: 0.011 (0.024)
Train: 177 [ 200/1251 ( 16%)]  Loss: 3.407 (3.60)  Time: 0.829s, 1235.55/s  (0.800s, 1280.08/s)  LR: 3.669e-04  Data: 0.014 (0.020)
Train: 177 [ 250/1251 ( 20%)]  Loss: 3.589 (3.60)  Time: 0.778s, 1315.55/s  (0.798s, 1283.80/s)  LR: 3.669e-04  Data: 0.011 (0.019)
Train: 177 [ 300/1251 ( 24%)]  Loss: 3.470 (3.58)  Time: 0.815s, 1255.93/s  (0.797s, 1285.01/s)  LR: 3.669e-04  Data: 0.011 (0.017)
Train: 177 [ 350/1251 ( 28%)]  Loss: 3.404 (3.56)  Time: 0.868s, 1179.36/s  (0.797s, 1284.96/s)  LR: 3.669e-04  Data: 0.011 (0.016)
Train: 177 [ 400/1251 ( 32%)]  Loss: 3.449 (3.54)  Time: 0.802s, 1276.41/s  (0.796s, 1285.79/s)  LR: 3.669e-04  Data: 0.011 (0.016)
Train: 177 [ 450/1251 ( 36%)]  Loss: 3.565 (3.55)  Time: 0.775s, 1320.67/s  (0.796s, 1286.96/s)  LR: 3.669e-04  Data: 0.011 (0.015)
Train: 177 [ 500/1251 ( 40%)]  Loss: 4.013 (3.59)  Time: 0.779s, 1314.15/s  (0.795s, 1288.52/s)  LR: 3.669e-04  Data: 0.011 (0.015)
Train: 177 [ 550/1251 ( 44%)]  Loss: 3.842 (3.61)  Time: 0.779s, 1314.29/s  (0.794s, 1289.72/s)  LR: 3.669e-04  Data: 0.011 (0.014)
Train: 177 [ 600/1251 ( 48%)]  Loss: 3.652 (3.61)  Time: 0.781s, 1311.39/s  (0.794s, 1290.41/s)  LR: 3.669e-04  Data: 0.011 (0.014)
Train: 177 [ 650/1251 ( 52%)]  Loss: 3.467 (3.60)  Time: 0.779s, 1314.92/s  (0.793s, 1290.95/s)  LR: 3.669e-04  Data: 0.011 (0.014)
Train: 177 [ 700/1251 ( 56%)]  Loss: 3.516 (3.60)  Time: 0.776s, 1319.85/s  (0.793s, 1291.31/s)  LR: 3.669e-04  Data: 0.011 (0.014)
Train: 177 [ 750/1251 ( 60%)]  Loss: 4.037 (3.62)  Time: 0.777s, 1317.43/s  (0.793s, 1290.51/s)  LR: 3.669e-04  Data: 0.011 (0.014)
Train: 177 [ 800/1251 ( 64%)]  Loss: 3.136 (3.60)  Time: 0.779s, 1313.90/s  (0.793s, 1291.39/s)  LR: 3.669e-04  Data: 0.012 (0.013)
Train: 177 [ 850/1251 ( 68%)]  Loss: 3.768 (3.61)  Time: 0.785s, 1305.09/s  (0.793s, 1290.75/s)  LR: 3.669e-04  Data: 0.011 (0.013)
Train: 177 [ 900/1251 ( 72%)]  Loss: 3.590 (3.60)  Time: 0.816s, 1255.13/s  (0.794s, 1289.73/s)  LR: 3.669e-04  Data: 0.012 (0.013)
Train: 177 [ 950/1251 ( 76%)]  Loss: 3.619 (3.61)  Time: 0.779s, 1314.30/s  (0.794s, 1290.02/s)  LR: 3.669e-04  Data: 0.011 (0.013)
Train: 177 [1000/1251 ( 80%)]  Loss: 3.806 (3.61)  Time: 0.783s, 1308.58/s  (0.794s, 1289.54/s)  LR: 3.669e-04  Data: 0.011 (0.013)
Train: 177 [1050/1251 ( 84%)]  Loss: 3.135 (3.59)  Time: 0.780s, 1313.09/s  (0.794s, 1290.16/s)  LR: 3.669e-04  Data: 0.011 (0.013)
Train: 177 [1100/1251 ( 88%)]  Loss: 3.434 (3.59)  Time: 0.812s, 1260.66/s  (0.794s, 1288.94/s)  LR: 3.669e-04  Data: 0.011 (0.013)
Train: 177 [1150/1251 ( 92%)]  Loss: 3.668 (3.59)  Time: 0.779s, 1314.55/s  (0.794s, 1289.53/s)  LR: 3.669e-04  Data: 0.011 (0.013)
Train: 177 [1200/1251 ( 96%)]  Loss: 3.231 (3.58)  Time: 0.779s, 1314.06/s  (0.794s, 1290.04/s)  LR: 3.669e-04  Data: 0.011 (0.013)
Train: 177 [1250/1251 (100%)]  Loss: 4.096 (3.60)  Time: 0.768s, 1333.83/s  (0.794s, 1290.31/s)  LR: 3.669e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.493 (1.493)  Loss:  0.8502 (0.8502)  Acc@1: 88.7695 (88.7695)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.172 (0.580)  Loss:  0.9912 (1.4284)  Acc@1: 85.8491 (73.9620)  Acc@5: 96.3443 (91.9280)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-177.pth.tar', 73.96200005859374)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-174.pth.tar', 73.70200001220704)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-176.pth.tar', 73.55600003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-171.pth.tar', 73.54600004394531)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-165.pth.tar', 73.49399998535156)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-175.pth.tar', 73.40400001464843)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-173.pth.tar', 73.37800001220702)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-172.pth.tar', 73.35199999511718)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-163.pth.tar', 73.35000001220703)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-164.pth.tar', 73.30200014160157)

Train: 178 [   0/1251 (  0%)]  Loss: 3.271 (3.27)  Time: 2.384s,  429.53/s  (2.384s,  429.53/s)  LR: 3.619e-04  Data: 1.650 (1.650)
Train: 178 [  50/1251 (  4%)]  Loss: 3.627 (3.45)  Time: 0.777s, 1317.77/s  (0.820s, 1249.12/s)  LR: 3.619e-04  Data: 0.012 (0.043)
Train: 178 [ 100/1251 (  8%)]  Loss: 3.901 (3.60)  Time: 0.810s, 1264.76/s  (0.803s, 1274.52/s)  LR: 3.619e-04  Data: 0.011 (0.027)
Train: 178 [ 150/1251 ( 12%)]  Loss: 3.270 (3.52)  Time: 0.842s, 1215.80/s  (0.799s, 1281.62/s)  LR: 3.619e-04  Data: 0.012 (0.022)
Train: 178 [ 200/1251 ( 16%)]  Loss: 3.709 (3.56)  Time: 0.784s, 1306.55/s  (0.803s, 1275.68/s)  LR: 3.619e-04  Data: 0.011 (0.019)
Train: 178 [ 250/1251 ( 20%)]  Loss: 3.918 (3.62)  Time: 0.792s, 1293.27/s  (0.800s, 1279.24/s)  LR: 3.619e-04  Data: 0.012 (0.018)
Train: 178 [ 300/1251 ( 24%)]  Loss: 3.931 (3.66)  Time: 0.791s, 1295.08/s  (0.800s, 1280.28/s)  LR: 3.619e-04  Data: 0.014 (0.017)
Train: 178 [ 350/1251 ( 28%)]  Loss: 3.651 (3.66)  Time: 0.778s, 1316.62/s  (0.800s, 1279.45/s)  LR: 3.619e-04  Data: 0.011 (0.016)
Train: 178 [ 400/1251 ( 32%)]  Loss: 3.911 (3.69)  Time: 0.781s, 1310.47/s  (0.799s, 1282.21/s)  LR: 3.619e-04  Data: 0.012 (0.015)
Train: 178 [ 450/1251 ( 36%)]  Loss: 3.183 (3.64)  Time: 0.779s, 1314.55/s  (0.797s, 1285.38/s)  LR: 3.619e-04  Data: 0.011 (0.015)
Train: 178 [ 500/1251 ( 40%)]  Loss: 3.420 (3.62)  Time: 0.779s, 1314.64/s  (0.798s, 1283.12/s)  LR: 3.619e-04  Data: 0.011 (0.015)
Train: 178 [ 550/1251 ( 44%)]  Loss: 3.675 (3.62)  Time: 0.782s, 1309.51/s  (0.797s, 1285.09/s)  LR: 3.619e-04  Data: 0.011 (0.014)
Train: 178 [ 600/1251 ( 48%)]  Loss: 3.651 (3.62)  Time: 0.778s, 1316.22/s  (0.796s, 1287.10/s)  LR: 3.619e-04  Data: 0.011 (0.014)
Train: 178 [ 650/1251 ( 52%)]  Loss: 3.068 (3.58)  Time: 0.815s, 1256.34/s  (0.795s, 1287.78/s)  LR: 3.619e-04  Data: 0.012 (0.014)
Train: 178 [ 700/1251 ( 56%)]  Loss: 3.287 (3.56)  Time: 0.777s, 1317.50/s  (0.795s, 1288.31/s)  LR: 3.619e-04  Data: 0.012 (0.014)
Train: 178 [ 750/1251 ( 60%)]  Loss: 3.478 (3.56)  Time: 0.777s, 1318.04/s  (0.795s, 1288.57/s)  LR: 3.619e-04  Data: 0.011 (0.014)
Train: 178 [ 800/1251 ( 64%)]  Loss: 3.692 (3.57)  Time: 0.773s, 1323.97/s  (0.794s, 1289.11/s)  LR: 3.619e-04  Data: 0.011 (0.013)
Train: 178 [ 850/1251 ( 68%)]  Loss: 4.101 (3.60)  Time: 0.780s, 1313.32/s  (0.794s, 1289.87/s)  LR: 3.619e-04  Data: 0.011 (0.013)
Train: 178 [ 900/1251 ( 72%)]  Loss: 3.502 (3.59)  Time: 0.781s, 1311.35/s  (0.794s, 1289.39/s)  LR: 3.619e-04  Data: 0.011 (0.013)
Train: 178 [ 950/1251 ( 76%)]  Loss: 3.391 (3.58)  Time: 0.778s, 1315.41/s  (0.794s, 1289.27/s)  LR: 3.619e-04  Data: 0.011 (0.013)
Train: 178 [1000/1251 ( 80%)]  Loss: 3.525 (3.58)  Time: 0.781s, 1311.68/s  (0.794s, 1290.09/s)  LR: 3.619e-04  Data: 0.011 (0.013)
Train: 178 [1050/1251 ( 84%)]  Loss: 3.904 (3.59)  Time: 0.817s, 1252.70/s  (0.794s, 1289.42/s)  LR: 3.619e-04  Data: 0.011 (0.013)
Train: 178 [1100/1251 ( 88%)]  Loss: 3.462 (3.59)  Time: 0.777s, 1317.59/s  (0.795s, 1288.83/s)  LR: 3.619e-04  Data: 0.011 (0.013)
Train: 178 [1150/1251 ( 92%)]  Loss: 3.301 (3.58)  Time: 0.778s, 1316.29/s  (0.795s, 1288.74/s)  LR: 3.619e-04  Data: 0.011 (0.013)
Train: 178 [1200/1251 ( 96%)]  Loss: 3.738 (3.58)  Time: 0.786s, 1303.18/s  (0.794s, 1289.51/s)  LR: 3.619e-04  Data: 0.011 (0.013)
Train: 178 [1250/1251 (100%)]  Loss: 3.681 (3.59)  Time: 0.770s, 1329.46/s  (0.794s, 1289.61/s)  LR: 3.619e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.608 (1.608)  Loss:  0.8381 (0.8381)  Acc@1: 89.4531 (89.4531)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.172 (0.561)  Loss:  0.8817 (1.4403)  Acc@1: 86.7924 (74.0220)  Acc@5: 96.4623 (91.9020)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-178.pth.tar', 74.02199995117188)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-177.pth.tar', 73.96200005859374)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-174.pth.tar', 73.70200001220704)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-176.pth.tar', 73.55600003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-171.pth.tar', 73.54600004394531)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-165.pth.tar', 73.49399998535156)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-175.pth.tar', 73.40400001464843)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-173.pth.tar', 73.37800001220702)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-172.pth.tar', 73.35199999511718)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-163.pth.tar', 73.35000001220703)

Train: 179 [   0/1251 (  0%)]  Loss: 3.246 (3.25)  Time: 2.215s,  462.26/s  (2.215s,  462.26/s)  LR: 3.570e-04  Data: 1.476 (1.476)
Train: 179 [  50/1251 (  4%)]  Loss: 3.714 (3.48)  Time: 0.812s, 1261.08/s  (0.821s, 1246.91/s)  LR: 3.570e-04  Data: 0.011 (0.044)
Train: 179 [ 100/1251 (  8%)]  Loss: 3.725 (3.56)  Time: 0.779s, 1315.04/s  (0.808s, 1266.78/s)  LR: 3.570e-04  Data: 0.011 (0.028)
Train: 179 [ 150/1251 ( 12%)]  Loss: 3.582 (3.57)  Time: 0.777s, 1317.67/s  (0.805s, 1271.64/s)  LR: 3.570e-04  Data: 0.011 (0.022)
Train: 179 [ 200/1251 ( 16%)]  Loss: 3.695 (3.59)  Time: 0.777s, 1318.28/s  (0.804s, 1272.91/s)  LR: 3.570e-04  Data: 0.010 (0.019)
Train: 179 [ 250/1251 ( 20%)]  Loss: 3.907 (3.64)  Time: 0.813s, 1258.92/s  (0.805s, 1271.41/s)  LR: 3.570e-04  Data: 0.011 (0.018)
Train: 179 [ 300/1251 ( 24%)]  Loss: 3.778 (3.66)  Time: 0.780s, 1313.56/s  (0.804s, 1273.43/s)  LR: 3.570e-04  Data: 0.011 (0.017)
Train: 179 [ 350/1251 ( 28%)]  Loss: 3.410 (3.63)  Time: 0.781s, 1311.77/s  (0.802s, 1276.26/s)  LR: 3.570e-04  Data: 0.011 (0.016)
Train: 179 [ 400/1251 ( 32%)]  Loss: 2.971 (3.56)  Time: 0.783s, 1308.20/s  (0.800s, 1280.47/s)  LR: 3.570e-04  Data: 0.011 (0.015)
Train: 179 [ 450/1251 ( 36%)]  Loss: 3.205 (3.52)  Time: 0.811s, 1261.91/s  (0.798s, 1283.75/s)  LR: 3.570e-04  Data: 0.011 (0.015)
Train: 179 [ 500/1251 ( 40%)]  Loss: 3.769 (3.55)  Time: 0.814s, 1257.74/s  (0.798s, 1282.57/s)  LR: 3.570e-04  Data: 0.011 (0.014)
Train: 179 [ 550/1251 ( 44%)]  Loss: 3.683 (3.56)  Time: 0.800s, 1280.18/s  (0.799s, 1282.06/s)  LR: 3.570e-04  Data: 0.011 (0.014)
Train: 179 [ 600/1251 ( 48%)]  Loss: 3.736 (3.57)  Time: 0.783s, 1307.66/s  (0.797s, 1284.06/s)  LR: 3.570e-04  Data: 0.011 (0.014)
Train: 179 [ 650/1251 ( 52%)]  Loss: 3.794 (3.59)  Time: 0.814s, 1257.85/s  (0.798s, 1283.70/s)  LR: 3.570e-04  Data: 0.011 (0.014)
Train: 179 [ 700/1251 ( 56%)]  Loss: 3.635 (3.59)  Time: 0.839s, 1221.18/s  (0.798s, 1283.61/s)  LR: 3.570e-04  Data: 0.011 (0.013)
Train: 179 [ 750/1251 ( 60%)]  Loss: 3.947 (3.61)  Time: 0.779s, 1314.18/s  (0.797s, 1284.02/s)  LR: 3.570e-04  Data: 0.011 (0.013)
Train: 179 [ 800/1251 ( 64%)]  Loss: 4.014 (3.64)  Time: 0.838s, 1222.03/s  (0.796s, 1285.71/s)  LR: 3.570e-04  Data: 0.011 (0.013)
Train: 179 [ 850/1251 ( 68%)]  Loss: 3.691 (3.64)  Time: 0.815s, 1256.98/s  (0.796s, 1286.44/s)  LR: 3.570e-04  Data: 0.010 (0.013)
Train: 179 [ 900/1251 ( 72%)]  Loss: 3.957 (3.66)  Time: 0.779s, 1314.26/s  (0.797s, 1285.60/s)  LR: 3.570e-04  Data: 0.011 (0.013)
Train: 179 [ 950/1251 ( 76%)]  Loss: 3.755 (3.66)  Time: 0.777s, 1317.59/s  (0.796s, 1286.57/s)  LR: 3.570e-04  Data: 0.011 (0.013)
Train: 179 [1000/1251 ( 80%)]  Loss: 3.503 (3.65)  Time: 0.778s, 1315.92/s  (0.795s, 1287.53/s)  LR: 3.570e-04  Data: 0.010 (0.013)
Train: 179 [1050/1251 ( 84%)]  Loss: 4.006 (3.67)  Time: 0.778s, 1316.04/s  (0.795s, 1287.45/s)  LR: 3.570e-04  Data: 0.011 (0.013)
Train: 179 [1100/1251 ( 88%)]  Loss: 3.494 (3.66)  Time: 0.780s, 1313.10/s  (0.795s, 1288.36/s)  LR: 3.570e-04  Data: 0.010 (0.013)
Train: 179 [1150/1251 ( 92%)]  Loss: 3.495 (3.65)  Time: 0.787s, 1301.69/s  (0.794s, 1289.09/s)  LR: 3.570e-04  Data: 0.011 (0.012)
Train: 179 [1200/1251 ( 96%)]  Loss: 3.946 (3.67)  Time: 0.778s, 1316.26/s  (0.794s, 1289.94/s)  LR: 3.570e-04  Data: 0.011 (0.012)
Train: 179 [1250/1251 (100%)]  Loss: 3.304 (3.65)  Time: 0.767s, 1334.36/s  (0.794s, 1290.38/s)  LR: 3.570e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.543 (1.543)  Loss:  0.7863 (0.7863)  Acc@1: 88.1836 (88.1836)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.172 (0.572)  Loss:  0.8338 (1.3559)  Acc@1: 84.6698 (73.7040)  Acc@5: 96.1085 (91.7240)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-178.pth.tar', 74.02199995117188)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-177.pth.tar', 73.96200005859374)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-179.pth.tar', 73.70400006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-174.pth.tar', 73.70200001220704)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-176.pth.tar', 73.55600003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-171.pth.tar', 73.54600004394531)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-165.pth.tar', 73.49399998535156)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-175.pth.tar', 73.40400001464843)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-173.pth.tar', 73.37800001220702)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-172.pth.tar', 73.35199999511718)

Train: 180 [   0/1251 (  0%)]  Loss: 3.379 (3.38)  Time: 2.221s,  460.98/s  (2.221s,  460.98/s)  LR: 3.520e-04  Data: 1.486 (1.486)
Train: 180 [  50/1251 (  4%)]  Loss: 3.441 (3.41)  Time: 0.814s, 1258.26/s  (0.829s, 1235.87/s)  LR: 3.520e-04  Data: 0.011 (0.044)
Train: 180 [ 100/1251 (  8%)]  Loss: 3.845 (3.55)  Time: 0.776s, 1318.95/s  (0.810s, 1264.12/s)  LR: 3.520e-04  Data: 0.011 (0.028)
Train: 180 [ 150/1251 ( 12%)]  Loss: 3.895 (3.64)  Time: 0.820s, 1248.69/s  (0.804s, 1273.63/s)  LR: 3.520e-04  Data: 0.012 (0.022)
Train: 180 [ 200/1251 ( 16%)]  Loss: 3.539 (3.62)  Time: 0.814s, 1257.29/s  (0.800s, 1279.88/s)  LR: 3.520e-04  Data: 0.011 (0.019)
Train: 180 [ 250/1251 ( 20%)]  Loss: 3.463 (3.59)  Time: 0.780s, 1313.42/s  (0.800s, 1279.66/s)  LR: 3.520e-04  Data: 0.011 (0.018)
Train: 180 [ 300/1251 ( 24%)]  Loss: 3.711 (3.61)  Time: 0.776s, 1319.15/s  (0.799s, 1281.78/s)  LR: 3.520e-04  Data: 0.011 (0.017)
Train: 180 [ 350/1251 ( 28%)]  Loss: 3.741 (3.63)  Time: 0.789s, 1298.08/s  (0.798s, 1283.83/s)  LR: 3.520e-04  Data: 0.011 (0.016)
Train: 180 [ 400/1251 ( 32%)]  Loss: 3.686 (3.63)  Time: 0.816s, 1254.42/s  (0.797s, 1285.61/s)  LR: 3.520e-04  Data: 0.011 (0.015)
Train: 180 [ 450/1251 ( 36%)]  Loss: 3.831 (3.65)  Time: 0.777s, 1317.34/s  (0.797s, 1284.52/s)  LR: 3.520e-04  Data: 0.011 (0.015)
Train: 180 [ 500/1251 ( 40%)]  Loss: 3.973 (3.68)  Time: 0.776s, 1319.94/s  (0.796s, 1285.88/s)  LR: 3.520e-04  Data: 0.011 (0.014)
Train: 180 [ 550/1251 ( 44%)]  Loss: 3.456 (3.66)  Time: 0.779s, 1314.66/s  (0.795s, 1287.39/s)  LR: 3.520e-04  Data: 0.011 (0.014)
Train: 180 [ 600/1251 ( 48%)]  Loss: 3.724 (3.67)  Time: 0.778s, 1315.60/s  (0.795s, 1287.78/s)  LR: 3.520e-04  Data: 0.011 (0.014)
Train: 180 [ 650/1251 ( 52%)]  Loss: 3.603 (3.66)  Time: 0.780s, 1312.45/s  (0.795s, 1288.63/s)  LR: 3.520e-04  Data: 0.011 (0.014)
Train: 180 [ 700/1251 ( 56%)]  Loss: 3.638 (3.66)  Time: 0.824s, 1242.62/s  (0.795s, 1288.71/s)  LR: 3.520e-04  Data: 0.010 (0.014)
Train: 180 [ 750/1251 ( 60%)]  Loss: 3.759 (3.67)  Time: 0.787s, 1301.78/s  (0.794s, 1289.78/s)  LR: 3.520e-04  Data: 0.017 (0.013)
Train: 180 [ 800/1251 ( 64%)]  Loss: 3.570 (3.66)  Time: 0.794s, 1290.42/s  (0.794s, 1289.47/s)  LR: 3.520e-04  Data: 0.011 (0.013)
Train: 180 [ 850/1251 ( 68%)]  Loss: 3.135 (3.63)  Time: 0.815s, 1256.06/s  (0.795s, 1288.28/s)  LR: 3.520e-04  Data: 0.010 (0.013)
Train: 180 [ 900/1251 ( 72%)]  Loss: 3.707 (3.64)  Time: 0.779s, 1314.59/s  (0.795s, 1287.36/s)  LR: 3.520e-04  Data: 0.011 (0.013)
Train: 180 [ 950/1251 ( 76%)]  Loss: 3.623 (3.64)  Time: 0.810s, 1263.89/s  (0.795s, 1287.53/s)  LR: 3.520e-04  Data: 0.011 (0.013)
Train: 180 [1000/1251 ( 80%)]  Loss: 3.896 (3.65)  Time: 0.778s, 1316.28/s  (0.795s, 1288.32/s)  LR: 3.520e-04  Data: 0.011 (0.013)
Train: 180 [1050/1251 ( 84%)]  Loss: 3.686 (3.65)  Time: 0.778s, 1316.07/s  (0.794s, 1289.14/s)  LR: 3.520e-04  Data: 0.011 (0.013)
Train: 180 [1100/1251 ( 88%)]  Loss: 3.526 (3.64)  Time: 0.777s, 1318.54/s  (0.794s, 1290.04/s)  LR: 3.520e-04  Data: 0.011 (0.013)
Train: 180 [1150/1251 ( 92%)]  Loss: 3.534 (3.64)  Time: 0.778s, 1316.46/s  (0.794s, 1290.42/s)  LR: 3.520e-04  Data: 0.011 (0.013)
Train: 180 [1200/1251 ( 96%)]  Loss: 3.512 (3.63)  Time: 0.778s, 1316.16/s  (0.793s, 1290.50/s)  LR: 3.520e-04  Data: 0.011 (0.013)
Train: 180 [1250/1251 (100%)]  Loss: 3.458 (3.63)  Time: 0.768s, 1333.02/s  (0.793s, 1291.19/s)  LR: 3.520e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.501 (1.501)  Loss:  0.9217 (0.9217)  Acc@1: 89.8438 (89.8438)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.172 (0.561)  Loss:  1.0198 (1.4677)  Acc@1: 85.2594 (73.9860)  Acc@5: 95.9906 (91.9080)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-178.pth.tar', 74.02199995117188)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-180.pth.tar', 73.98600006103516)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-177.pth.tar', 73.96200005859374)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-179.pth.tar', 73.70400006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-174.pth.tar', 73.70200001220704)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-176.pth.tar', 73.55600003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-171.pth.tar', 73.54600004394531)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-165.pth.tar', 73.49399998535156)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-175.pth.tar', 73.40400001464843)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-173.pth.tar', 73.37800001220702)

Train: 181 [   0/1251 (  0%)]  Loss: 3.364 (3.36)  Time: 2.234s,  458.34/s  (2.234s,  458.34/s)  LR: 3.471e-04  Data: 1.498 (1.498)
Train: 181 [  50/1251 (  4%)]  Loss: 3.853 (3.61)  Time: 0.816s, 1255.13/s  (0.829s, 1234.70/s)  LR: 3.471e-04  Data: 0.011 (0.042)
Train: 181 [ 100/1251 (  8%)]  Loss: 3.668 (3.63)  Time: 0.780s, 1312.32/s  (0.809s, 1265.04/s)  LR: 3.471e-04  Data: 0.011 (0.027)
Train: 181 [ 150/1251 ( 12%)]  Loss: 3.477 (3.59)  Time: 0.818s, 1251.89/s  (0.804s, 1273.62/s)  LR: 3.471e-04  Data: 0.011 (0.022)
Train: 181 [ 200/1251 ( 16%)]  Loss: 3.474 (3.57)  Time: 0.781s, 1311.42/s  (0.803s, 1275.41/s)  LR: 3.471e-04  Data: 0.010 (0.019)
Train: 181 [ 250/1251 ( 20%)]  Loss: 3.717 (3.59)  Time: 0.809s, 1265.10/s  (0.802s, 1277.46/s)  LR: 3.471e-04  Data: 0.012 (0.018)
Train: 181 [ 300/1251 ( 24%)]  Loss: 3.713 (3.61)  Time: 0.787s, 1301.69/s  (0.799s, 1280.99/s)  LR: 3.471e-04  Data: 0.012 (0.016)
Train: 181 [ 350/1251 ( 28%)]  Loss: 3.816 (3.64)  Time: 0.781s, 1311.37/s  (0.797s, 1284.43/s)  LR: 3.471e-04  Data: 0.010 (0.016)
Train: 181 [ 400/1251 ( 32%)]  Loss: 3.581 (3.63)  Time: 0.779s, 1314.62/s  (0.797s, 1285.35/s)  LR: 3.471e-04  Data: 0.011 (0.015)
Train: 181 [ 450/1251 ( 36%)]  Loss: 3.528 (3.62)  Time: 0.778s, 1315.91/s  (0.796s, 1286.61/s)  LR: 3.471e-04  Data: 0.011 (0.015)
Train: 181 [ 500/1251 ( 40%)]  Loss: 3.576 (3.62)  Time: 0.779s, 1314.66/s  (0.795s, 1287.95/s)  LR: 3.471e-04  Data: 0.011 (0.014)
Train: 181 [ 550/1251 ( 44%)]  Loss: 4.007 (3.65)  Time: 0.786s, 1302.52/s  (0.794s, 1290.02/s)  LR: 3.471e-04  Data: 0.012 (0.014)
Train: 181 [ 600/1251 ( 48%)]  Loss: 3.656 (3.65)  Time: 0.816s, 1254.49/s  (0.793s, 1290.98/s)  LR: 3.471e-04  Data: 0.011 (0.014)
Train: 181 [ 650/1251 ( 52%)]  Loss: 3.427 (3.63)  Time: 0.844s, 1212.92/s  (0.793s, 1290.97/s)  LR: 3.471e-04  Data: 0.011 (0.014)
Train: 181 [ 700/1251 ( 56%)]  Loss: 3.731 (3.64)  Time: 0.807s, 1268.21/s  (0.793s, 1291.64/s)  LR: 3.471e-04  Data: 0.012 (0.013)
Train: 181 [ 750/1251 ( 60%)]  Loss: 3.596 (3.64)  Time: 0.776s, 1319.55/s  (0.793s, 1291.99/s)  LR: 3.471e-04  Data: 0.011 (0.013)
Train: 181 [ 800/1251 ( 64%)]  Loss: 3.221 (3.61)  Time: 0.813s, 1259.76/s  (0.792s, 1292.75/s)  LR: 3.471e-04  Data: 0.012 (0.013)
Train: 181 [ 850/1251 ( 68%)]  Loss: 4.103 (3.64)  Time: 0.792s, 1292.28/s  (0.793s, 1291.31/s)  LR: 3.471e-04  Data: 0.011 (0.013)
Train: 181 [ 900/1251 ( 72%)]  Loss: 3.813 (3.65)  Time: 0.778s, 1315.72/s  (0.794s, 1290.15/s)  LR: 3.471e-04  Data: 0.011 (0.013)
Train: 181 [ 950/1251 ( 76%)]  Loss: 3.831 (3.66)  Time: 0.836s, 1225.24/s  (0.794s, 1289.71/s)  LR: 3.471e-04  Data: 0.011 (0.013)
Train: 181 [1000/1251 ( 80%)]  Loss: 3.787 (3.66)  Time: 0.830s, 1233.38/s  (0.794s, 1289.50/s)  LR: 3.471e-04  Data: 0.011 (0.013)
Train: 181 [1050/1251 ( 84%)]  Loss: 3.443 (3.65)  Time: 0.822s, 1245.35/s  (0.794s, 1289.70/s)  LR: 3.471e-04  Data: 0.010 (0.013)
Train: 181 [1100/1251 ( 88%)]  Loss: 3.845 (3.66)  Time: 0.781s, 1311.52/s  (0.794s, 1290.38/s)  LR: 3.471e-04  Data: 0.015 (0.013)
Train: 181 [1150/1251 ( 92%)]  Loss: 3.736 (3.67)  Time: 0.782s, 1310.27/s  (0.793s, 1290.98/s)  LR: 3.471e-04  Data: 0.011 (0.012)
Train: 181 [1200/1251 ( 96%)]  Loss: 3.692 (3.67)  Time: 0.777s, 1318.32/s  (0.793s, 1290.93/s)  LR: 3.471e-04  Data: 0.011 (0.012)
Train: 181 [1250/1251 (100%)]  Loss: 3.566 (3.66)  Time: 0.771s, 1327.90/s  (0.793s, 1291.83/s)  LR: 3.471e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.554 (1.554)  Loss:  0.9328 (0.9328)  Acc@1: 87.5000 (87.5000)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.172 (0.561)  Loss:  0.9872 (1.5084)  Acc@1: 83.9623 (73.4780)  Acc@5: 96.4623 (91.6960)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-178.pth.tar', 74.02199995117188)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-180.pth.tar', 73.98600006103516)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-177.pth.tar', 73.96200005859374)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-179.pth.tar', 73.70400006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-174.pth.tar', 73.70200001220704)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-176.pth.tar', 73.55600003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-171.pth.tar', 73.54600004394531)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-165.pth.tar', 73.49399998535156)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-181.pth.tar', 73.47800001464844)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-175.pth.tar', 73.40400001464843)

Train: 182 [   0/1251 (  0%)]  Loss: 3.734 (3.73)  Time: 2.266s,  451.95/s  (2.266s,  451.95/s)  LR: 3.422e-04  Data: 1.532 (1.532)
Train: 182 [  50/1251 (  4%)]  Loss: 3.969 (3.85)  Time: 0.779s, 1314.74/s  (0.817s, 1253.12/s)  LR: 3.422e-04  Data: 0.011 (0.045)
Train: 182 [ 100/1251 (  8%)]  Loss: 3.581 (3.76)  Time: 0.777s, 1317.12/s  (0.801s, 1278.84/s)  LR: 3.422e-04  Data: 0.011 (0.028)
Train: 182 [ 150/1251 ( 12%)]  Loss: 3.582 (3.72)  Time: 0.779s, 1314.94/s  (0.798s, 1283.70/s)  LR: 3.422e-04  Data: 0.011 (0.022)
Train: 182 [ 200/1251 ( 16%)]  Loss: 3.529 (3.68)  Time: 0.776s, 1319.05/s  (0.795s, 1287.96/s)  LR: 3.422e-04  Data: 0.011 (0.020)
Train: 182 [ 250/1251 ( 20%)]  Loss: 3.285 (3.61)  Time: 0.779s, 1315.27/s  (0.794s, 1290.03/s)  LR: 3.422e-04  Data: 0.012 (0.018)
Train: 182 [ 300/1251 ( 24%)]  Loss: 3.872 (3.65)  Time: 0.777s, 1317.36/s  (0.793s, 1290.49/s)  LR: 3.422e-04  Data: 0.011 (0.017)
Train: 182 [ 350/1251 ( 28%)]  Loss: 3.643 (3.65)  Time: 0.780s, 1313.19/s  (0.793s, 1290.79/s)  LR: 3.422e-04  Data: 0.010 (0.016)
Train: 182 [ 400/1251 ( 32%)]  Loss: 3.589 (3.64)  Time: 0.828s, 1235.98/s  (0.792s, 1292.36/s)  LR: 3.422e-04  Data: 0.012 (0.015)
Train: 182 [ 450/1251 ( 36%)]  Loss: 3.639 (3.64)  Time: 0.780s, 1312.10/s  (0.794s, 1290.43/s)  LR: 3.422e-04  Data: 0.011 (0.015)
Train: 182 [ 500/1251 ( 40%)]  Loss: 3.541 (3.63)  Time: 0.842s, 1215.99/s  (0.794s, 1289.52/s)  LR: 3.422e-04  Data: 0.012 (0.015)
Train: 182 [ 550/1251 ( 44%)]  Loss: 3.754 (3.64)  Time: 0.779s, 1314.82/s  (0.794s, 1289.31/s)  LR: 3.422e-04  Data: 0.011 (0.014)
Train: 182 [ 600/1251 ( 48%)]  Loss: 3.712 (3.65)  Time: 0.819s, 1250.45/s  (0.795s, 1287.39/s)  LR: 3.422e-04  Data: 0.011 (0.014)
Train: 182 [ 650/1251 ( 52%)]  Loss: 3.752 (3.66)  Time: 0.780s, 1312.23/s  (0.795s, 1288.72/s)  LR: 3.422e-04  Data: 0.011 (0.014)
Train: 182 [ 700/1251 ( 56%)]  Loss: 3.962 (3.68)  Time: 0.817s, 1253.38/s  (0.796s, 1286.97/s)  LR: 3.422e-04  Data: 0.012 (0.014)
Train: 182 [ 750/1251 ( 60%)]  Loss: 3.791 (3.68)  Time: 0.801s, 1277.79/s  (0.796s, 1286.57/s)  LR: 3.422e-04  Data: 0.011 (0.013)
Train: 182 [ 800/1251 ( 64%)]  Loss: 3.717 (3.69)  Time: 0.835s, 1226.82/s  (0.797s, 1285.17/s)  LR: 3.422e-04  Data: 0.012 (0.013)
Train: 182 [ 850/1251 ( 68%)]  Loss: 3.515 (3.68)  Time: 0.780s, 1312.52/s  (0.796s, 1285.77/s)  LR: 3.422e-04  Data: 0.011 (0.013)
Train: 182 [ 900/1251 ( 72%)]  Loss: 3.566 (3.67)  Time: 0.779s, 1315.09/s  (0.796s, 1286.99/s)  LR: 3.422e-04  Data: 0.011 (0.013)
Train: 182 [ 950/1251 ( 76%)]  Loss: 3.643 (3.67)  Time: 0.777s, 1318.34/s  (0.795s, 1288.16/s)  LR: 3.422e-04  Data: 0.012 (0.013)
Train: 182 [1000/1251 ( 80%)]  Loss: 3.599 (3.67)  Time: 0.780s, 1312.71/s  (0.794s, 1289.39/s)  LR: 3.422e-04  Data: 0.011 (0.013)
Train: 182 [1050/1251 ( 84%)]  Loss: 3.859 (3.67)  Time: 0.779s, 1313.68/s  (0.794s, 1289.74/s)  LR: 3.422e-04  Data: 0.011 (0.013)
Train: 182 [1100/1251 ( 88%)]  Loss: 3.576 (3.67)  Time: 0.784s, 1305.46/s  (0.794s, 1289.86/s)  LR: 3.422e-04  Data: 0.014 (0.013)
Train: 182 [1150/1251 ( 92%)]  Loss: 3.617 (3.67)  Time: 0.812s, 1261.74/s  (0.794s, 1289.52/s)  LR: 3.422e-04  Data: 0.011 (0.013)
Train: 182 [1200/1251 ( 96%)]  Loss: 3.800 (3.67)  Time: 0.826s, 1240.16/s  (0.794s, 1289.22/s)  LR: 3.422e-04  Data: 0.010 (0.013)
Train: 182 [1250/1251 (100%)]  Loss: 3.768 (3.68)  Time: 0.809s, 1265.48/s  (0.795s, 1288.56/s)  LR: 3.422e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.549 (1.549)  Loss:  0.6764 (0.6764)  Acc@1: 89.7461 (89.7461)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.172 (0.565)  Loss:  0.8317 (1.3240)  Acc@1: 85.3774 (73.5320)  Acc@5: 96.5802 (91.9080)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-178.pth.tar', 74.02199995117188)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-180.pth.tar', 73.98600006103516)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-177.pth.tar', 73.96200005859374)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-179.pth.tar', 73.70400006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-174.pth.tar', 73.70200001220704)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-176.pth.tar', 73.55600003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-171.pth.tar', 73.54600004394531)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-182.pth.tar', 73.53199998291015)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-165.pth.tar', 73.49399998535156)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-181.pth.tar', 73.47800001464844)

Train: 183 [   0/1251 (  0%)]  Loss: 3.538 (3.54)  Time: 2.857s,  358.36/s  (2.857s,  358.36/s)  LR: 3.373e-04  Data: 2.109 (2.109)
Train: 183 [  50/1251 (  4%)]  Loss: 3.414 (3.48)  Time: 0.780s, 1312.75/s  (0.830s, 1233.09/s)  LR: 3.373e-04  Data: 0.011 (0.056)
Train: 183 [ 100/1251 (  8%)]  Loss: 3.600 (3.52)  Time: 0.813s, 1258.79/s  (0.813s, 1259.60/s)  LR: 3.373e-04  Data: 0.011 (0.034)
Train: 183 [ 150/1251 ( 12%)]  Loss: 3.606 (3.54)  Time: 0.778s, 1315.90/s  (0.808s, 1266.90/s)  LR: 3.373e-04  Data: 0.011 (0.026)
Train: 183 [ 200/1251 ( 16%)]  Loss: 3.747 (3.58)  Time: 0.853s, 1199.87/s  (0.803s, 1275.96/s)  LR: 3.373e-04  Data: 0.011 (0.022)
Train: 183 [ 250/1251 ( 20%)]  Loss: 3.419 (3.55)  Time: 0.779s, 1314.29/s  (0.803s, 1275.16/s)  LR: 3.373e-04  Data: 0.011 (0.020)
Train: 183 [ 300/1251 ( 24%)]  Loss: 3.800 (3.59)  Time: 0.823s, 1244.48/s  (0.801s, 1279.05/s)  LR: 3.373e-04  Data: 0.011 (0.019)
Train: 183 [ 350/1251 ( 28%)]  Loss: 3.931 (3.63)  Time: 0.777s, 1317.26/s  (0.800s, 1279.56/s)  LR: 3.373e-04  Data: 0.011 (0.018)
Train: 183 [ 400/1251 ( 32%)]  Loss: 3.384 (3.60)  Time: 0.777s, 1317.22/s  (0.799s, 1282.07/s)  LR: 3.373e-04  Data: 0.010 (0.017)
Train: 183 [ 450/1251 ( 36%)]  Loss: 3.270 (3.57)  Time: 0.812s, 1261.18/s  (0.799s, 1280.99/s)  LR: 3.373e-04  Data: 0.010 (0.016)
Train: 183 [ 500/1251 ( 40%)]  Loss: 3.053 (3.52)  Time: 0.778s, 1316.52/s  (0.798s, 1283.71/s)  LR: 3.373e-04  Data: 0.011 (0.016)
Train: 183 [ 550/1251 ( 44%)]  Loss: 3.793 (3.55)  Time: 0.794s, 1289.23/s  (0.797s, 1284.42/s)  LR: 3.373e-04  Data: 0.011 (0.015)
Train: 183 [ 600/1251 ( 48%)]  Loss: 3.337 (3.53)  Time: 0.780s, 1312.29/s  (0.797s, 1285.57/s)  LR: 3.373e-04  Data: 0.011 (0.015)
Train: 183 [ 650/1251 ( 52%)]  Loss: 3.930 (3.56)  Time: 0.778s, 1316.18/s  (0.797s, 1285.15/s)  LR: 3.373e-04  Data: 0.011 (0.015)
Train: 183 [ 700/1251 ( 56%)]  Loss: 3.675 (3.57)  Time: 0.776s, 1318.88/s  (0.797s, 1285.36/s)  LR: 3.373e-04  Data: 0.011 (0.014)
Train: 183 [ 750/1251 ( 60%)]  Loss: 3.643 (3.57)  Time: 0.819s, 1250.39/s  (0.796s, 1286.81/s)  LR: 3.373e-04  Data: 0.011 (0.014)
Train: 183 [ 800/1251 ( 64%)]  Loss: 3.359 (3.56)  Time: 0.814s, 1258.41/s  (0.796s, 1286.67/s)  LR: 3.373e-04  Data: 0.011 (0.014)
Train: 183 [ 850/1251 ( 68%)]  Loss: 3.664 (3.56)  Time: 0.817s, 1253.52/s  (0.796s, 1285.93/s)  LR: 3.373e-04  Data: 0.011 (0.014)
Train: 183 [ 900/1251 ( 72%)]  Loss: 3.595 (3.57)  Time: 0.778s, 1316.54/s  (0.796s, 1285.93/s)  LR: 3.373e-04  Data: 0.011 (0.014)
Train: 183 [ 950/1251 ( 76%)]  Loss: 3.855 (3.58)  Time: 0.777s, 1317.77/s  (0.796s, 1286.30/s)  LR: 3.373e-04  Data: 0.011 (0.013)
Train: 183 [1000/1251 ( 80%)]  Loss: 3.741 (3.59)  Time: 0.779s, 1314.12/s  (0.796s, 1287.04/s)  LR: 3.373e-04  Data: 0.011 (0.013)
Train: 183 [1050/1251 ( 84%)]  Loss: 3.764 (3.60)  Time: 0.865s, 1183.68/s  (0.796s, 1287.06/s)  LR: 3.373e-04  Data: 0.011 (0.013)
Train: 183 [1100/1251 ( 88%)]  Loss: 3.567 (3.59)  Time: 0.779s, 1314.67/s  (0.795s, 1287.77/s)  LR: 3.373e-04  Data: 0.011 (0.013)
Train: 183 [1150/1251 ( 92%)]  Loss: 3.732 (3.60)  Time: 0.777s, 1318.13/s  (0.795s, 1288.22/s)  LR: 3.373e-04  Data: 0.011 (0.013)
Train: 183 [1200/1251 ( 96%)]  Loss: 3.850 (3.61)  Time: 0.777s, 1317.16/s  (0.794s, 1288.92/s)  LR: 3.373e-04  Data: 0.011 (0.013)
Train: 183 [1250/1251 (100%)]  Loss: 3.893 (3.62)  Time: 0.765s, 1339.24/s  (0.795s, 1287.63/s)  LR: 3.373e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.536 (1.536)  Loss:  0.8387 (0.8387)  Acc@1: 89.5508 (89.5508)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.172 (0.570)  Loss:  0.9488 (1.4473)  Acc@1: 84.6698 (73.9900)  Acc@5: 95.5189 (91.8160)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-178.pth.tar', 74.02199995117188)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-183.pth.tar', 73.99000006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-180.pth.tar', 73.98600006103516)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-177.pth.tar', 73.96200005859374)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-179.pth.tar', 73.70400006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-174.pth.tar', 73.70200001220704)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-176.pth.tar', 73.55600003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-171.pth.tar', 73.54600004394531)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-182.pth.tar', 73.53199998291015)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-165.pth.tar', 73.49399998535156)

Train: 184 [   0/1251 (  0%)]  Loss: 3.724 (3.72)  Time: 2.213s,  462.72/s  (2.213s,  462.72/s)  LR: 3.325e-04  Data: 1.476 (1.476)
Train: 184 [  50/1251 (  4%)]  Loss: 3.601 (3.66)  Time: 0.777s, 1317.35/s  (0.832s, 1230.19/s)  LR: 3.325e-04  Data: 0.011 (0.044)
Train: 184 [ 100/1251 (  8%)]  Loss: 3.718 (3.68)  Time: 0.790s, 1296.47/s  (0.811s, 1263.19/s)  LR: 3.325e-04  Data: 0.011 (0.027)
Train: 184 [ 150/1251 ( 12%)]  Loss: 3.763 (3.70)  Time: 0.780s, 1313.35/s  (0.802s, 1277.20/s)  LR: 3.325e-04  Data: 0.011 (0.022)
Train: 184 [ 200/1251 ( 16%)]  Loss: 3.350 (3.63)  Time: 0.779s, 1315.31/s  (0.798s, 1283.49/s)  LR: 3.325e-04  Data: 0.011 (0.019)
Train: 184 [ 250/1251 ( 20%)]  Loss: 3.779 (3.66)  Time: 0.802s, 1277.16/s  (0.794s, 1288.93/s)  LR: 3.325e-04  Data: 0.011 (0.018)
Train: 184 [ 300/1251 ( 24%)]  Loss: 3.385 (3.62)  Time: 0.779s, 1315.19/s  (0.795s, 1288.76/s)  LR: 3.325e-04  Data: 0.011 (0.017)
Train: 184 [ 350/1251 ( 28%)]  Loss: 3.485 (3.60)  Time: 0.813s, 1259.19/s  (0.796s, 1286.45/s)  LR: 3.325e-04  Data: 0.011 (0.016)
Train: 184 [ 400/1251 ( 32%)]  Loss: 3.776 (3.62)  Time: 0.850s, 1204.78/s  (0.796s, 1287.14/s)  LR: 3.325e-04  Data: 0.011 (0.015)
Train: 184 [ 450/1251 ( 36%)]  Loss: 3.688 (3.63)  Time: 0.782s, 1309.88/s  (0.795s, 1288.41/s)  LR: 3.325e-04  Data: 0.012 (0.015)
Train: 184 [ 500/1251 ( 40%)]  Loss: 3.709 (3.63)  Time: 0.780s, 1313.65/s  (0.795s, 1288.71/s)  LR: 3.325e-04  Data: 0.011 (0.014)
Train: 184 [ 550/1251 ( 44%)]  Loss: 3.377 (3.61)  Time: 0.811s, 1262.00/s  (0.794s, 1289.95/s)  LR: 3.325e-04  Data: 0.011 (0.014)
Train: 184 [ 600/1251 ( 48%)]  Loss: 3.543 (3.61)  Time: 0.851s, 1203.52/s  (0.794s, 1290.31/s)  LR: 3.325e-04  Data: 0.011 (0.014)
Train: 184 [ 650/1251 ( 52%)]  Loss: 3.711 (3.61)  Time: 0.784s, 1306.57/s  (0.793s, 1291.36/s)  LR: 3.325e-04  Data: 0.011 (0.014)
Train: 184 [ 700/1251 ( 56%)]  Loss: 3.784 (3.63)  Time: 0.778s, 1316.55/s  (0.793s, 1291.87/s)  LR: 3.325e-04  Data: 0.011 (0.013)
Train: 184 [ 750/1251 ( 60%)]  Loss: 3.481 (3.62)  Time: 0.780s, 1312.39/s  (0.792s, 1292.89/s)  LR: 3.325e-04  Data: 0.011 (0.013)
Train: 184 [ 800/1251 ( 64%)]  Loss: 3.387 (3.60)  Time: 0.777s, 1317.94/s  (0.791s, 1293.98/s)  LR: 3.325e-04  Data: 0.011 (0.013)
Train: 184 [ 850/1251 ( 68%)]  Loss: 3.644 (3.61)  Time: 0.817s, 1253.49/s  (0.792s, 1292.93/s)  LR: 3.325e-04  Data: 0.011 (0.013)
Train: 184 [ 900/1251 ( 72%)]  Loss: 3.703 (3.61)  Time: 0.818s, 1251.99/s  (0.792s, 1293.31/s)  LR: 3.325e-04  Data: 0.011 (0.013)
Train: 184 [ 950/1251 ( 76%)]  Loss: 3.444 (3.60)  Time: 0.822s, 1245.32/s  (0.792s, 1292.96/s)  LR: 3.325e-04  Data: 0.012 (0.013)
Train: 184 [1000/1251 ( 80%)]  Loss: 3.669 (3.61)  Time: 0.784s, 1306.17/s  (0.793s, 1291.27/s)  LR: 3.325e-04  Data: 0.011 (0.013)
Train: 184 [1050/1251 ( 84%)]  Loss: 3.724 (3.61)  Time: 0.779s, 1314.21/s  (0.793s, 1291.72/s)  LR: 3.325e-04  Data: 0.011 (0.013)
Train: 184 [1100/1251 ( 88%)]  Loss: 3.484 (3.61)  Time: 0.782s, 1309.04/s  (0.793s, 1291.87/s)  LR: 3.325e-04  Data: 0.012 (0.013)
Train: 184 [1150/1251 ( 92%)]  Loss: 3.454 (3.60)  Time: 0.785s, 1304.10/s  (0.793s, 1292.03/s)  LR: 3.325e-04  Data: 0.012 (0.013)
Train: 184 [1200/1251 ( 96%)]  Loss: 3.833 (3.61)  Time: 0.778s, 1316.26/s  (0.792s, 1292.46/s)  LR: 3.325e-04  Data: 0.011 (0.013)
Train: 184 [1250/1251 (100%)]  Loss: 3.498 (3.60)  Time: 0.792s, 1292.52/s  (0.792s, 1293.04/s)  LR: 3.325e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.588 (1.588)  Loss:  0.8810 (0.8810)  Acc@1: 88.3789 (88.3789)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.172 (0.569)  Loss:  0.9371 (1.4423)  Acc@1: 85.3774 (73.6040)  Acc@5: 96.9340 (91.5880)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-178.pth.tar', 74.02199995117188)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-183.pth.tar', 73.99000006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-180.pth.tar', 73.98600006103516)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-177.pth.tar', 73.96200005859374)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-179.pth.tar', 73.70400006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-174.pth.tar', 73.70200001220704)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-184.pth.tar', 73.60399998291015)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-176.pth.tar', 73.55600003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-171.pth.tar', 73.54600004394531)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-182.pth.tar', 73.53199998291015)

Train: 185 [   0/1251 (  0%)]  Loss: 3.828 (3.83)  Time: 2.250s,  455.18/s  (2.250s,  455.18/s)  LR: 3.276e-04  Data: 1.498 (1.498)
Train: 185 [  50/1251 (  4%)]  Loss: 3.637 (3.73)  Time: 0.779s, 1314.67/s  (0.840s, 1219.39/s)  LR: 3.276e-04  Data: 0.011 (0.044)
Train: 185 [ 100/1251 (  8%)]  Loss: 3.576 (3.68)  Time: 0.776s, 1318.87/s  (0.814s, 1258.67/s)  LR: 3.276e-04  Data: 0.011 (0.027)
Train: 185 [ 150/1251 ( 12%)]  Loss: 3.717 (3.69)  Time: 0.822s, 1245.78/s  (0.805s, 1271.75/s)  LR: 3.276e-04  Data: 0.012 (0.022)
Train: 185 [ 200/1251 ( 16%)]  Loss: 3.631 (3.68)  Time: 0.788s, 1300.10/s  (0.801s, 1277.67/s)  LR: 3.276e-04  Data: 0.011 (0.019)
Train: 185 [ 250/1251 ( 20%)]  Loss: 3.893 (3.71)  Time: 0.779s, 1314.37/s  (0.800s, 1280.68/s)  LR: 3.276e-04  Data: 0.011 (0.018)
Train: 185 [ 300/1251 ( 24%)]  Loss: 3.671 (3.71)  Time: 0.808s, 1267.19/s  (0.798s, 1282.79/s)  LR: 3.276e-04  Data: 0.011 (0.017)
Train: 185 [ 350/1251 ( 28%)]  Loss: 3.794 (3.72)  Time: 0.831s, 1231.93/s  (0.799s, 1282.16/s)  LR: 3.276e-04  Data: 0.011 (0.016)
Train: 185 [ 400/1251 ( 32%)]  Loss: 3.585 (3.70)  Time: 0.805s, 1271.94/s  (0.800s, 1279.97/s)  LR: 3.276e-04  Data: 0.011 (0.015)
Train: 185 [ 450/1251 ( 36%)]  Loss: 3.410 (3.67)  Time: 0.778s, 1315.62/s  (0.798s, 1282.64/s)  LR: 3.276e-04  Data: 0.011 (0.015)
Train: 185 [ 500/1251 ( 40%)]  Loss: 3.417 (3.65)  Time: 0.827s, 1237.83/s  (0.799s, 1282.29/s)  LR: 3.276e-04  Data: 0.011 (0.014)
Train: 185 [ 550/1251 ( 44%)]  Loss: 3.482 (3.64)  Time: 0.821s, 1247.70/s  (0.798s, 1283.92/s)  LR: 3.276e-04  Data: 0.011 (0.014)
Train: 185 [ 600/1251 ( 48%)]  Loss: 3.615 (3.64)  Time: 0.788s, 1298.86/s  (0.797s, 1284.08/s)  LR: 3.276e-04  Data: 0.011 (0.014)
Train: 185 [ 650/1251 ( 52%)]  Loss: 3.282 (3.61)  Time: 0.779s, 1314.89/s  (0.797s, 1285.13/s)  LR: 3.276e-04  Data: 0.011 (0.014)
Train: 185 [ 700/1251 ( 56%)]  Loss: 3.750 (3.62)  Time: 0.787s, 1300.87/s  (0.797s, 1285.33/s)  LR: 3.276e-04  Data: 0.012 (0.013)
Train: 185 [ 750/1251 ( 60%)]  Loss: 3.369 (3.60)  Time: 0.780s, 1313.33/s  (0.797s, 1285.03/s)  LR: 3.276e-04  Data: 0.011 (0.013)
Train: 185 [ 800/1251 ( 64%)]  Loss: 3.651 (3.61)  Time: 0.840s, 1218.33/s  (0.796s, 1286.15/s)  LR: 3.276e-04  Data: 0.011 (0.013)
Train: 185 [ 850/1251 ( 68%)]  Loss: 3.542 (3.60)  Time: 0.778s, 1315.47/s  (0.796s, 1286.85/s)  LR: 3.276e-04  Data: 0.011 (0.013)
Train: 185 [ 900/1251 ( 72%)]  Loss: 3.498 (3.60)  Time: 0.829s, 1235.30/s  (0.795s, 1287.60/s)  LR: 3.276e-04  Data: 0.011 (0.013)
Train: 185 [ 950/1251 ( 76%)]  Loss: 3.242 (3.58)  Time: 0.779s, 1313.72/s  (0.795s, 1288.40/s)  LR: 3.276e-04  Data: 0.011 (0.013)
Train: 185 [1000/1251 ( 80%)]  Loss: 3.737 (3.59)  Time: 0.777s, 1317.85/s  (0.794s, 1289.41/s)  LR: 3.276e-04  Data: 0.011 (0.013)
Train: 185 [1050/1251 ( 84%)]  Loss: 3.862 (3.60)  Time: 0.808s, 1267.45/s  (0.794s, 1290.23/s)  LR: 3.276e-04  Data: 0.011 (0.013)
Train: 185 [1100/1251 ( 88%)]  Loss: 3.713 (3.60)  Time: 0.775s, 1321.85/s  (0.794s, 1290.05/s)  LR: 3.276e-04  Data: 0.011 (0.013)
Train: 185 [1150/1251 ( 92%)]  Loss: 3.368 (3.59)  Time: 0.778s, 1316.74/s  (0.793s, 1290.63/s)  LR: 3.276e-04  Data: 0.011 (0.013)
Train: 185 [1200/1251 ( 96%)]  Loss: 3.851 (3.60)  Time: 0.778s, 1316.74/s  (0.793s, 1290.97/s)  LR: 3.276e-04  Data: 0.012 (0.012)
Train: 185 [1250/1251 (100%)]  Loss: 3.817 (3.61)  Time: 0.767s, 1334.84/s  (0.793s, 1291.65/s)  LR: 3.276e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.562 (1.562)  Loss:  0.7435 (0.7435)  Acc@1: 88.5742 (88.5742)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.172 (0.566)  Loss:  0.9148 (1.3807)  Acc@1: 84.0802 (73.8000)  Acc@5: 96.1085 (91.9040)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-178.pth.tar', 74.02199995117188)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-183.pth.tar', 73.99000006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-180.pth.tar', 73.98600006103516)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-177.pth.tar', 73.96200005859374)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-185.pth.tar', 73.80000006591797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-179.pth.tar', 73.70400006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-174.pth.tar', 73.70200001220704)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-184.pth.tar', 73.60399998291015)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-176.pth.tar', 73.55600003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-171.pth.tar', 73.54600004394531)

Train: 186 [   0/1251 (  0%)]  Loss: 3.550 (3.55)  Time: 2.428s,  421.70/s  (2.428s,  421.70/s)  LR: 3.228e-04  Data: 1.632 (1.632)
Train: 186 [  50/1251 (  4%)]  Loss: 3.414 (3.48)  Time: 0.779s, 1313.83/s  (0.821s, 1247.55/s)  LR: 3.228e-04  Data: 0.011 (0.045)
Train: 186 [ 100/1251 (  8%)]  Loss: 3.647 (3.54)  Time: 0.776s, 1319.32/s  (0.807s, 1268.73/s)  LR: 3.228e-04  Data: 0.011 (0.028)
Train: 186 [ 150/1251 ( 12%)]  Loss: 3.694 (3.58)  Time: 0.810s, 1264.57/s  (0.802s, 1276.74/s)  LR: 3.228e-04  Data: 0.011 (0.022)
Train: 186 [ 200/1251 ( 16%)]  Loss: 3.948 (3.65)  Time: 0.779s, 1313.80/s  (0.798s, 1282.77/s)  LR: 3.228e-04  Data: 0.011 (0.020)
Train: 186 [ 250/1251 ( 20%)]  Loss: 3.774 (3.67)  Time: 0.778s, 1316.97/s  (0.798s, 1283.77/s)  LR: 3.228e-04  Data: 0.011 (0.018)
Train: 186 [ 300/1251 ( 24%)]  Loss: 3.847 (3.70)  Time: 0.779s, 1314.60/s  (0.796s, 1286.60/s)  LR: 3.228e-04  Data: 0.011 (0.017)
Train: 186 [ 350/1251 ( 28%)]  Loss: 3.823 (3.71)  Time: 0.778s, 1316.70/s  (0.795s, 1287.78/s)  LR: 3.228e-04  Data: 0.010 (0.016)
Train: 186 [ 400/1251 ( 32%)]  Loss: 4.089 (3.75)  Time: 0.780s, 1313.16/s  (0.795s, 1287.92/s)  LR: 3.228e-04  Data: 0.011 (0.015)
Train: 186 [ 450/1251 ( 36%)]  Loss: 3.396 (3.72)  Time: 0.779s, 1314.41/s  (0.795s, 1287.50/s)  LR: 3.228e-04  Data: 0.011 (0.015)
Train: 186 [ 500/1251 ( 40%)]  Loss: 3.857 (3.73)  Time: 0.814s, 1258.01/s  (0.796s, 1287.08/s)  LR: 3.228e-04  Data: 0.010 (0.015)
Train: 186 [ 550/1251 ( 44%)]  Loss: 3.653 (3.72)  Time: 0.784s, 1305.54/s  (0.795s, 1288.07/s)  LR: 3.228e-04  Data: 0.011 (0.014)
Train: 186 [ 600/1251 ( 48%)]  Loss: 3.618 (3.72)  Time: 0.811s, 1262.12/s  (0.795s, 1287.25/s)  LR: 3.228e-04  Data: 0.011 (0.014)
Train: 186 [ 650/1251 ( 52%)]  Loss: 3.690 (3.71)  Time: 0.776s, 1318.90/s  (0.796s, 1285.89/s)  LR: 3.228e-04  Data: 0.012 (0.014)
Train: 186 [ 700/1251 ( 56%)]  Loss: 3.727 (3.72)  Time: 0.779s, 1315.27/s  (0.796s, 1287.11/s)  LR: 3.228e-04  Data: 0.011 (0.014)
Train: 186 [ 750/1251 ( 60%)]  Loss: 3.370 (3.69)  Time: 0.781s, 1310.36/s  (0.795s, 1288.25/s)  LR: 3.228e-04  Data: 0.011 (0.013)
Train: 186 [ 800/1251 ( 64%)]  Loss: 3.668 (3.69)  Time: 0.817s, 1253.19/s  (0.796s, 1287.11/s)  LR: 3.228e-04  Data: 0.011 (0.013)
Train: 186 [ 850/1251 ( 68%)]  Loss: 3.761 (3.70)  Time: 0.780s, 1312.57/s  (0.796s, 1287.12/s)  LR: 3.228e-04  Data: 0.011 (0.013)
Train: 186 [ 900/1251 ( 72%)]  Loss: 3.649 (3.69)  Time: 0.779s, 1314.56/s  (0.795s, 1287.99/s)  LR: 3.228e-04  Data: 0.011 (0.013)
Train: 186 [ 950/1251 ( 76%)]  Loss: 3.571 (3.69)  Time: 0.804s, 1273.88/s  (0.795s, 1288.31/s)  LR: 3.228e-04  Data: 0.011 (0.013)
Train: 186 [1000/1251 ( 80%)]  Loss: 3.486 (3.68)  Time: 0.812s, 1261.59/s  (0.795s, 1288.22/s)  LR: 3.228e-04  Data: 0.011 (0.013)
Train: 186 [1050/1251 ( 84%)]  Loss: 4.030 (3.69)  Time: 0.778s, 1315.35/s  (0.795s, 1288.33/s)  LR: 3.228e-04  Data: 0.011 (0.013)
Train: 186 [1100/1251 ( 88%)]  Loss: 3.773 (3.70)  Time: 0.820s, 1249.34/s  (0.794s, 1288.97/s)  LR: 3.228e-04  Data: 0.010 (0.013)
Train: 186 [1150/1251 ( 92%)]  Loss: 3.985 (3.71)  Time: 0.816s, 1255.33/s  (0.794s, 1289.06/s)  LR: 3.228e-04  Data: 0.011 (0.013)
Train: 186 [1200/1251 ( 96%)]  Loss: 3.739 (3.71)  Time: 0.777s, 1318.69/s  (0.795s, 1288.13/s)  LR: 3.228e-04  Data: 0.011 (0.013)
Train: 186 [1250/1251 (100%)]  Loss: 3.819 (3.71)  Time: 0.768s, 1333.07/s  (0.795s, 1288.82/s)  LR: 3.228e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.594 (1.594)  Loss:  0.8135 (0.8135)  Acc@1: 88.9648 (88.9648)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.172 (0.557)  Loss:  0.8940 (1.4021)  Acc@1: 85.4953 (74.4220)  Acc@5: 97.0519 (92.0240)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-186.pth.tar', 74.42200003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-178.pth.tar', 74.02199995117188)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-183.pth.tar', 73.99000006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-180.pth.tar', 73.98600006103516)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-177.pth.tar', 73.96200005859374)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-185.pth.tar', 73.80000006591797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-179.pth.tar', 73.70400006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-174.pth.tar', 73.70200001220704)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-184.pth.tar', 73.60399998291015)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-176.pth.tar', 73.55600003417969)

Train: 187 [   0/1251 (  0%)]  Loss: 3.602 (3.60)  Time: 2.379s,  430.36/s  (2.379s,  430.36/s)  LR: 3.180e-04  Data: 1.586 (1.586)
Train: 187 [  50/1251 (  4%)]  Loss: 3.820 (3.71)  Time: 0.782s, 1309.29/s  (0.846s, 1210.89/s)  LR: 3.180e-04  Data: 0.011 (0.047)
Train: 187 [ 100/1251 (  8%)]  Loss: 3.062 (3.49)  Time: 0.779s, 1313.70/s  (0.820s, 1248.82/s)  LR: 3.180e-04  Data: 0.011 (0.029)
Train: 187 [ 150/1251 ( 12%)]  Loss: 3.530 (3.50)  Time: 0.824s, 1242.98/s  (0.811s, 1262.88/s)  LR: 3.180e-04  Data: 0.012 (0.023)
Train: 187 [ 200/1251 ( 16%)]  Loss: 3.561 (3.52)  Time: 0.831s, 1231.58/s  (0.804s, 1273.19/s)  LR: 3.180e-04  Data: 0.011 (0.020)
Train: 187 [ 250/1251 ( 20%)]  Loss: 3.569 (3.52)  Time: 0.779s, 1313.82/s  (0.804s, 1273.99/s)  LR: 3.180e-04  Data: 0.011 (0.019)
Train: 187 [ 300/1251 ( 24%)]  Loss: 3.877 (3.57)  Time: 0.779s, 1314.49/s  (0.801s, 1278.46/s)  LR: 3.180e-04  Data: 0.011 (0.017)
Train: 187 [ 350/1251 ( 28%)]  Loss: 3.566 (3.57)  Time: 0.778s, 1316.69/s  (0.798s, 1283.16/s)  LR: 3.180e-04  Data: 0.011 (0.016)
Train: 187 [ 400/1251 ( 32%)]  Loss: 3.629 (3.58)  Time: 0.781s, 1310.34/s  (0.797s, 1284.76/s)  LR: 3.180e-04  Data: 0.011 (0.016)
Train: 187 [ 450/1251 ( 36%)]  Loss: 3.518 (3.57)  Time: 0.778s, 1316.75/s  (0.796s, 1286.01/s)  LR: 3.180e-04  Data: 0.011 (0.015)
Train: 187 [ 500/1251 ( 40%)]  Loss: 2.988 (3.52)  Time: 0.819s, 1249.85/s  (0.796s, 1286.68/s)  LR: 3.180e-04  Data: 0.011 (0.015)
Train: 187 [ 550/1251 ( 44%)]  Loss: 3.452 (3.51)  Time: 0.776s, 1318.95/s  (0.795s, 1287.33/s)  LR: 3.180e-04  Data: 0.011 (0.015)
Train: 187 [ 600/1251 ( 48%)]  Loss: 3.594 (3.52)  Time: 0.781s, 1310.94/s  (0.795s, 1288.66/s)  LR: 3.180e-04  Data: 0.011 (0.014)
Train: 187 [ 650/1251 ( 52%)]  Loss: 3.462 (3.52)  Time: 0.777s, 1318.68/s  (0.795s, 1288.52/s)  LR: 3.180e-04  Data: 0.011 (0.014)
Train: 187 [ 700/1251 ( 56%)]  Loss: 3.593 (3.52)  Time: 0.793s, 1290.66/s  (0.794s, 1289.12/s)  LR: 3.180e-04  Data: 0.011 (0.014)
Train: 187 [ 750/1251 ( 60%)]  Loss: 3.784 (3.54)  Time: 0.779s, 1315.33/s  (0.794s, 1289.67/s)  LR: 3.180e-04  Data: 0.011 (0.014)
Train: 187 [ 800/1251 ( 64%)]  Loss: 3.604 (3.54)  Time: 0.783s, 1308.58/s  (0.794s, 1289.49/s)  LR: 3.180e-04  Data: 0.011 (0.014)
Train: 187 [ 850/1251 ( 68%)]  Loss: 3.146 (3.52)  Time: 0.785s, 1303.66/s  (0.794s, 1288.95/s)  LR: 3.180e-04  Data: 0.011 (0.013)
Train: 187 [ 900/1251 ( 72%)]  Loss: 3.397 (3.51)  Time: 0.818s, 1252.15/s  (0.794s, 1289.30/s)  LR: 3.180e-04  Data: 0.011 (0.013)
Train: 187 [ 950/1251 ( 76%)]  Loss: 3.790 (3.53)  Time: 0.785s, 1304.91/s  (0.794s, 1289.75/s)  LR: 3.180e-04  Data: 0.011 (0.013)
Train: 187 [1000/1251 ( 80%)]  Loss: 3.433 (3.52)  Time: 0.779s, 1315.29/s  (0.794s, 1290.40/s)  LR: 3.180e-04  Data: 0.014 (0.013)
Train: 187 [1050/1251 ( 84%)]  Loss: 3.759 (3.53)  Time: 0.778s, 1315.56/s  (0.794s, 1290.45/s)  LR: 3.180e-04  Data: 0.011 (0.013)
Train: 187 [1100/1251 ( 88%)]  Loss: 3.356 (3.53)  Time: 0.780s, 1312.28/s  (0.794s, 1289.87/s)  LR: 3.180e-04  Data: 0.012 (0.013)
Train: 187 [1150/1251 ( 92%)]  Loss: 3.572 (3.53)  Time: 0.782s, 1309.77/s  (0.794s, 1290.21/s)  LR: 3.180e-04  Data: 0.011 (0.013)
Train: 187 [1200/1251 ( 96%)]  Loss: 3.424 (3.52)  Time: 0.779s, 1314.77/s  (0.793s, 1291.09/s)  LR: 3.180e-04  Data: 0.011 (0.013)
Train: 187 [1250/1251 (100%)]  Loss: 3.825 (3.54)  Time: 0.768s, 1333.55/s  (0.794s, 1290.12/s)  LR: 3.180e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.544 (1.544)  Loss:  0.8057 (0.8057)  Acc@1: 89.1602 (89.1602)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.172 (0.558)  Loss:  0.8637 (1.3473)  Acc@1: 85.3774 (74.4520)  Acc@5: 95.9906 (92.1660)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-187.pth.tar', 74.45200011230469)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-186.pth.tar', 74.42200003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-178.pth.tar', 74.02199995117188)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-183.pth.tar', 73.99000006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-180.pth.tar', 73.98600006103516)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-177.pth.tar', 73.96200005859374)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-185.pth.tar', 73.80000006591797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-179.pth.tar', 73.70400006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-174.pth.tar', 73.70200001220704)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-184.pth.tar', 73.60399998291015)

Train: 188 [   0/1251 (  0%)]  Loss: 3.543 (3.54)  Time: 2.319s,  441.49/s  (2.319s,  441.49/s)  LR: 3.132e-04  Data: 1.524 (1.524)
Train: 188 [  50/1251 (  4%)]  Loss: 3.490 (3.52)  Time: 0.777s, 1317.99/s  (0.827s, 1237.64/s)  LR: 3.132e-04  Data: 0.011 (0.045)
Train: 188 [ 100/1251 (  8%)]  Loss: 3.502 (3.51)  Time: 0.777s, 1317.51/s  (0.819s, 1249.58/s)  LR: 3.132e-04  Data: 0.011 (0.028)
Train: 188 [ 150/1251 ( 12%)]  Loss: 3.438 (3.49)  Time: 0.776s, 1319.29/s  (0.811s, 1262.72/s)  LR: 3.132e-04  Data: 0.011 (0.022)
Train: 188 [ 200/1251 ( 16%)]  Loss: 3.780 (3.55)  Time: 0.780s, 1313.57/s  (0.806s, 1270.78/s)  LR: 3.132e-04  Data: 0.011 (0.020)
Train: 188 [ 250/1251 ( 20%)]  Loss: 3.815 (3.59)  Time: 0.781s, 1310.73/s  (0.804s, 1274.08/s)  LR: 3.132e-04  Data: 0.011 (0.018)
Train: 188 [ 300/1251 ( 24%)]  Loss: 3.727 (3.61)  Time: 0.778s, 1315.41/s  (0.803s, 1275.02/s)  LR: 3.132e-04  Data: 0.011 (0.017)
Train: 188 [ 350/1251 ( 28%)]  Loss: 3.597 (3.61)  Time: 0.776s, 1318.90/s  (0.801s, 1278.57/s)  LR: 3.132e-04  Data: 0.010 (0.016)
Train: 188 [ 400/1251 ( 32%)]  Loss: 3.430 (3.59)  Time: 0.778s, 1316.83/s  (0.802s, 1276.98/s)  LR: 3.132e-04  Data: 0.011 (0.015)
Train: 188 [ 450/1251 ( 36%)]  Loss: 3.346 (3.57)  Time: 0.784s, 1305.78/s  (0.800s, 1279.51/s)  LR: 3.132e-04  Data: 0.011 (0.015)
Train: 188 [ 500/1251 ( 40%)]  Loss: 3.626 (3.57)  Time: 0.776s, 1319.63/s  (0.799s, 1281.28/s)  LR: 3.132e-04  Data: 0.011 (0.015)
Train: 188 [ 550/1251 ( 44%)]  Loss: 3.567 (3.57)  Time: 0.779s, 1314.89/s  (0.798s, 1283.81/s)  LR: 3.132e-04  Data: 0.011 (0.014)
Train: 188 [ 600/1251 ( 48%)]  Loss: 3.626 (3.58)  Time: 0.820s, 1249.29/s  (0.797s, 1284.13/s)  LR: 3.132e-04  Data: 0.012 (0.014)
Train: 188 [ 650/1251 ( 52%)]  Loss: 3.348 (3.56)  Time: 0.808s, 1267.99/s  (0.798s, 1283.79/s)  LR: 3.132e-04  Data: 0.012 (0.014)
Train: 188 [ 700/1251 ( 56%)]  Loss: 3.596 (3.56)  Time: 0.780s, 1312.25/s  (0.798s, 1283.59/s)  LR: 3.132e-04  Data: 0.011 (0.014)
Train: 188 [ 750/1251 ( 60%)]  Loss: 3.645 (3.57)  Time: 0.780s, 1312.63/s  (0.797s, 1285.05/s)  LR: 3.132e-04  Data: 0.011 (0.013)
Train: 188 [ 800/1251 ( 64%)]  Loss: 3.723 (3.58)  Time: 0.779s, 1314.19/s  (0.797s, 1285.15/s)  LR: 3.132e-04  Data: 0.011 (0.013)
Train: 188 [ 850/1251 ( 68%)]  Loss: 3.406 (3.57)  Time: 0.786s, 1303.16/s  (0.796s, 1286.16/s)  LR: 3.132e-04  Data: 0.010 (0.013)
Train: 188 [ 900/1251 ( 72%)]  Loss: 3.503 (3.56)  Time: 0.844s, 1212.81/s  (0.796s, 1287.01/s)  LR: 3.132e-04  Data: 0.011 (0.013)
Train: 188 [ 950/1251 ( 76%)]  Loss: 3.363 (3.55)  Time: 0.778s, 1315.50/s  (0.795s, 1287.75/s)  LR: 3.132e-04  Data: 0.011 (0.013)
Train: 188 [1000/1251 ( 80%)]  Loss: 3.494 (3.55)  Time: 0.777s, 1317.87/s  (0.795s, 1288.08/s)  LR: 3.132e-04  Data: 0.011 (0.013)
Train: 188 [1050/1251 ( 84%)]  Loss: 3.631 (3.55)  Time: 0.840s, 1219.37/s  (0.795s, 1287.95/s)  LR: 3.132e-04  Data: 0.012 (0.013)
Train: 188 [1100/1251 ( 88%)]  Loss: 3.689 (3.56)  Time: 0.835s, 1226.21/s  (0.796s, 1286.82/s)  LR: 3.132e-04  Data: 0.011 (0.013)
Train: 188 [1150/1251 ( 92%)]  Loss: 3.503 (3.56)  Time: 0.812s, 1260.96/s  (0.796s, 1285.85/s)  LR: 3.132e-04  Data: 0.011 (0.013)
Train: 188 [1200/1251 ( 96%)]  Loss: 3.258 (3.55)  Time: 0.777s, 1318.51/s  (0.797s, 1285.38/s)  LR: 3.132e-04  Data: 0.011 (0.013)
Train: 188 [1250/1251 (100%)]  Loss: 3.778 (3.55)  Time: 0.849s, 1205.43/s  (0.797s, 1285.49/s)  LR: 3.132e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.542 (1.542)  Loss:  0.8719 (0.8719)  Acc@1: 89.7461 (89.7461)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.172 (0.568)  Loss:  0.9702 (1.4308)  Acc@1: 85.0236 (74.1920)  Acc@5: 96.6981 (92.0380)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-187.pth.tar', 74.45200011230469)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-186.pth.tar', 74.42200003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-188.pth.tar', 74.19200008789062)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-178.pth.tar', 74.02199995117188)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-183.pth.tar', 73.99000006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-180.pth.tar', 73.98600006103516)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-177.pth.tar', 73.96200005859374)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-185.pth.tar', 73.80000006591797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-179.pth.tar', 73.70400006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-174.pth.tar', 73.70200001220704)

Train: 189 [   0/1251 (  0%)]  Loss: 3.554 (3.55)  Time: 2.458s,  416.54/s  (2.458s,  416.54/s)  LR: 3.084e-04  Data: 1.663 (1.663)
Train: 189 [  50/1251 (  4%)]  Loss: 3.848 (3.70)  Time: 0.794s, 1288.87/s  (0.842s, 1215.77/s)  LR: 3.084e-04  Data: 0.011 (0.048)
Train: 189 [ 100/1251 (  8%)]  Loss: 3.182 (3.53)  Time: 0.778s, 1316.34/s  (0.815s, 1255.80/s)  LR: 3.084e-04  Data: 0.011 (0.030)
Train: 189 [ 150/1251 ( 12%)]  Loss: 3.689 (3.57)  Time: 0.810s, 1263.81/s  (0.809s, 1265.79/s)  LR: 3.084e-04  Data: 0.011 (0.024)
Train: 189 [ 200/1251 ( 16%)]  Loss: 3.517 (3.56)  Time: 0.779s, 1314.72/s  (0.804s, 1273.54/s)  LR: 3.084e-04  Data: 0.011 (0.021)
Train: 189 [ 250/1251 ( 20%)]  Loss: 3.480 (3.54)  Time: 0.812s, 1260.95/s  (0.800s, 1279.22/s)  LR: 3.084e-04  Data: 0.011 (0.019)
Train: 189 [ 300/1251 ( 24%)]  Loss: 3.165 (3.49)  Time: 0.779s, 1315.13/s  (0.800s, 1280.22/s)  LR: 3.084e-04  Data: 0.011 (0.017)
Train: 189 [ 350/1251 ( 28%)]  Loss: 3.518 (3.49)  Time: 0.841s, 1217.33/s  (0.798s, 1282.99/s)  LR: 3.084e-04  Data: 0.011 (0.017)
Train: 189 [ 400/1251 ( 32%)]  Loss: 3.218 (3.46)  Time: 0.780s, 1311.99/s  (0.796s, 1286.06/s)  LR: 3.084e-04  Data: 0.011 (0.016)
Train: 189 [ 450/1251 ( 36%)]  Loss: 3.701 (3.49)  Time: 0.780s, 1313.33/s  (0.795s, 1287.73/s)  LR: 3.084e-04  Data: 0.011 (0.015)
Train: 189 [ 500/1251 ( 40%)]  Loss: 3.873 (3.52)  Time: 0.818s, 1251.75/s  (0.794s, 1289.35/s)  LR: 3.084e-04  Data: 0.012 (0.015)
Train: 189 [ 550/1251 ( 44%)]  Loss: 3.410 (3.51)  Time: 0.810s, 1264.93/s  (0.796s, 1286.42/s)  LR: 3.084e-04  Data: 0.011 (0.015)
Train: 189 [ 600/1251 ( 48%)]  Loss: 3.614 (3.52)  Time: 0.783s, 1308.10/s  (0.796s, 1286.33/s)  LR: 3.084e-04  Data: 0.011 (0.014)
Train: 189 [ 650/1251 ( 52%)]  Loss: 3.911 (3.55)  Time: 0.844s, 1212.77/s  (0.796s, 1286.43/s)  LR: 3.084e-04  Data: 0.012 (0.014)
Train: 189 [ 700/1251 ( 56%)]  Loss: 3.403 (3.54)  Time: 0.774s, 1323.68/s  (0.797s, 1284.77/s)  LR: 3.084e-04  Data: 0.011 (0.014)
Train: 189 [ 750/1251 ( 60%)]  Loss: 3.493 (3.54)  Time: 0.779s, 1314.34/s  (0.798s, 1283.12/s)  LR: 3.084e-04  Data: 0.011 (0.014)
Train: 189 [ 800/1251 ( 64%)]  Loss: 3.237 (3.52)  Time: 0.790s, 1296.14/s  (0.797s, 1284.17/s)  LR: 3.084e-04  Data: 0.011 (0.014)
Train: 189 [ 850/1251 ( 68%)]  Loss: 3.788 (3.53)  Time: 0.779s, 1314.99/s  (0.797s, 1285.05/s)  LR: 3.084e-04  Data: 0.011 (0.013)
Train: 189 [ 900/1251 ( 72%)]  Loss: 3.572 (3.54)  Time: 0.779s, 1314.10/s  (0.796s, 1286.17/s)  LR: 3.084e-04  Data: 0.011 (0.013)
Train: 189 [ 950/1251 ( 76%)]  Loss: 3.551 (3.54)  Time: 0.817s, 1253.80/s  (0.796s, 1286.79/s)  LR: 3.084e-04  Data: 0.012 (0.013)
Train: 189 [1000/1251 ( 80%)]  Loss: 3.587 (3.54)  Time: 0.779s, 1314.06/s  (0.796s, 1287.11/s)  LR: 3.084e-04  Data: 0.011 (0.013)
Train: 189 [1050/1251 ( 84%)]  Loss: 3.322 (3.53)  Time: 0.782s, 1308.74/s  (0.796s, 1286.91/s)  LR: 3.084e-04  Data: 0.012 (0.013)
Train: 189 [1100/1251 ( 88%)]  Loss: 3.517 (3.53)  Time: 0.778s, 1316.28/s  (0.796s, 1287.02/s)  LR: 3.084e-04  Data: 0.011 (0.013)
Train: 189 [1150/1251 ( 92%)]  Loss: 3.894 (3.54)  Time: 0.780s, 1312.05/s  (0.795s, 1287.42/s)  LR: 3.084e-04  Data: 0.011 (0.013)
Train: 189 [1200/1251 ( 96%)]  Loss: 3.594 (3.55)  Time: 0.783s, 1307.64/s  (0.795s, 1287.64/s)  LR: 3.084e-04  Data: 0.011 (0.013)
Train: 189 [1250/1251 (100%)]  Loss: 3.758 (3.55)  Time: 0.768s, 1332.67/s  (0.795s, 1288.29/s)  LR: 3.084e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.552 (1.552)  Loss:  0.8262 (0.8262)  Acc@1: 88.3789 (88.3789)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.172 (0.560)  Loss:  0.8581 (1.3863)  Acc@1: 84.0802 (74.1440)  Acc@5: 96.2264 (91.9180)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-187.pth.tar', 74.45200011230469)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-186.pth.tar', 74.42200003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-188.pth.tar', 74.19200008789062)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-189.pth.tar', 74.14399993652344)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-178.pth.tar', 74.02199995117188)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-183.pth.tar', 73.99000006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-180.pth.tar', 73.98600006103516)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-177.pth.tar', 73.96200005859374)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-185.pth.tar', 73.80000006591797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-179.pth.tar', 73.70400006347656)

Train: 190 [   0/1251 (  0%)]  Loss: 3.689 (3.69)  Time: 2.426s,  422.12/s  (2.426s,  422.12/s)  LR: 3.037e-04  Data: 1.692 (1.692)
Train: 190 [  50/1251 (  4%)]  Loss: 3.569 (3.63)  Time: 0.780s, 1312.50/s  (0.832s, 1231.02/s)  LR: 3.037e-04  Data: 0.011 (0.048)
Train: 190 [ 100/1251 (  8%)]  Loss: 3.408 (3.56)  Time: 0.825s, 1240.88/s  (0.821s, 1247.35/s)  LR: 3.037e-04  Data: 0.011 (0.030)
Train: 190 [ 150/1251 ( 12%)]  Loss: 3.628 (3.57)  Time: 0.808s, 1268.09/s  (0.813s, 1260.19/s)  LR: 3.037e-04  Data: 0.011 (0.024)
Train: 190 [ 200/1251 ( 16%)]  Loss: 3.590 (3.58)  Time: 0.778s, 1315.76/s  (0.809s, 1265.67/s)  LR: 3.037e-04  Data: 0.011 (0.021)
Train: 190 [ 250/1251 ( 20%)]  Loss: 3.371 (3.54)  Time: 0.829s, 1235.85/s  (0.805s, 1271.41/s)  LR: 3.037e-04  Data: 0.012 (0.019)
Train: 190 [ 300/1251 ( 24%)]  Loss: 3.278 (3.50)  Time: 0.788s, 1300.13/s  (0.805s, 1272.66/s)  LR: 3.037e-04  Data: 0.013 (0.018)
Train: 190 [ 350/1251 ( 28%)]  Loss: 3.535 (3.51)  Time: 0.780s, 1313.48/s  (0.801s, 1278.03/s)  LR: 3.037e-04  Data: 0.011 (0.017)
Train: 190 [ 400/1251 ( 32%)]  Loss: 3.659 (3.53)  Time: 0.810s, 1264.53/s  (0.801s, 1278.00/s)  LR: 3.037e-04  Data: 0.011 (0.016)
Train: 190 [ 450/1251 ( 36%)]  Loss: 3.435 (3.52)  Time: 0.778s, 1315.36/s  (0.800s, 1279.40/s)  LR: 3.037e-04  Data: 0.011 (0.015)
Train: 190 [ 500/1251 ( 40%)]  Loss: 3.573 (3.52)  Time: 0.839s, 1220.41/s  (0.800s, 1279.20/s)  LR: 3.037e-04  Data: 0.010 (0.015)
Train: 190 [ 550/1251 ( 44%)]  Loss: 3.179 (3.49)  Time: 0.815s, 1256.96/s  (0.799s, 1280.81/s)  LR: 3.037e-04  Data: 0.010 (0.015)
Train: 190 [ 600/1251 ( 48%)]  Loss: 3.529 (3.50)  Time: 0.778s, 1316.73/s  (0.800s, 1280.51/s)  LR: 3.037e-04  Data: 0.011 (0.014)
Train: 190 [ 650/1251 ( 52%)]  Loss: 3.276 (3.48)  Time: 0.780s, 1313.12/s  (0.798s, 1282.55/s)  LR: 3.037e-04  Data: 0.012 (0.014)
Train: 190 [ 700/1251 ( 56%)]  Loss: 3.625 (3.49)  Time: 0.777s, 1317.12/s  (0.798s, 1282.69/s)  LR: 3.037e-04  Data: 0.011 (0.014)
Train: 190 [ 750/1251 ( 60%)]  Loss: 3.889 (3.51)  Time: 0.826s, 1239.68/s  (0.798s, 1283.66/s)  LR: 3.037e-04  Data: 0.011 (0.014)
Train: 190 [ 800/1251 ( 64%)]  Loss: 3.577 (3.52)  Time: 0.779s, 1314.39/s  (0.798s, 1283.88/s)  LR: 3.037e-04  Data: 0.012 (0.014)
Train: 190 [ 850/1251 ( 68%)]  Loss: 3.643 (3.53)  Time: 0.781s, 1311.84/s  (0.798s, 1283.98/s)  LR: 3.037e-04  Data: 0.011 (0.013)
Train: 190 [ 900/1251 ( 72%)]  Loss: 3.256 (3.51)  Time: 0.786s, 1302.34/s  (0.797s, 1284.27/s)  LR: 3.037e-04  Data: 0.011 (0.013)
Train: 190 [ 950/1251 ( 76%)]  Loss: 3.691 (3.52)  Time: 0.807s, 1268.31/s  (0.797s, 1284.14/s)  LR: 3.037e-04  Data: 0.011 (0.013)
Train: 190 [1000/1251 ( 80%)]  Loss: 3.975 (3.54)  Time: 0.776s, 1318.84/s  (0.797s, 1284.69/s)  LR: 3.037e-04  Data: 0.010 (0.013)
Train: 190 [1050/1251 ( 84%)]  Loss: 3.982 (3.56)  Time: 0.776s, 1319.35/s  (0.797s, 1285.06/s)  LR: 3.037e-04  Data: 0.011 (0.013)
Train: 190 [1100/1251 ( 88%)]  Loss: 3.420 (3.56)  Time: 0.778s, 1315.66/s  (0.796s, 1285.69/s)  LR: 3.037e-04  Data: 0.011 (0.013)
Train: 190 [1150/1251 ( 92%)]  Loss: 3.381 (3.55)  Time: 0.778s, 1316.45/s  (0.796s, 1286.28/s)  LR: 3.037e-04  Data: 0.011 (0.013)
Train: 190 [1200/1251 ( 96%)]  Loss: 3.897 (3.56)  Time: 0.782s, 1308.95/s  (0.796s, 1286.79/s)  LR: 3.037e-04  Data: 0.011 (0.013)
Train: 190 [1250/1251 (100%)]  Loss: 3.628 (3.56)  Time: 0.771s, 1328.40/s  (0.796s, 1287.02/s)  LR: 3.037e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.571 (1.571)  Loss:  0.7617 (0.7617)  Acc@1: 89.2578 (89.2578)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.172 (0.570)  Loss:  0.8908 (1.3552)  Acc@1: 83.7264 (74.2300)  Acc@5: 96.4623 (91.9660)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-187.pth.tar', 74.45200011230469)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-186.pth.tar', 74.42200003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-190.pth.tar', 74.2300000415039)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-188.pth.tar', 74.19200008789062)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-189.pth.tar', 74.14399993652344)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-178.pth.tar', 74.02199995117188)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-183.pth.tar', 73.99000006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-180.pth.tar', 73.98600006103516)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-177.pth.tar', 73.96200005859374)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-185.pth.tar', 73.80000006591797)

Train: 191 [   0/1251 (  0%)]  Loss: 3.845 (3.84)  Time: 2.279s,  449.31/s  (2.279s,  449.31/s)  LR: 2.989e-04  Data: 1.543 (1.543)
Train: 191 [  50/1251 (  4%)]  Loss: 3.675 (3.76)  Time: 0.813s, 1259.59/s  (0.833s, 1229.35/s)  LR: 2.989e-04  Data: 0.011 (0.045)
Train: 191 [ 100/1251 (  8%)]  Loss: 3.418 (3.65)  Time: 0.777s, 1318.42/s  (0.820s, 1248.80/s)  LR: 2.989e-04  Data: 0.012 (0.028)
Train: 191 [ 150/1251 ( 12%)]  Loss: 3.407 (3.59)  Time: 0.780s, 1312.50/s  (0.809s, 1265.12/s)  LR: 2.989e-04  Data: 0.011 (0.023)
Train: 191 [ 200/1251 ( 16%)]  Loss: 3.358 (3.54)  Time: 0.784s, 1305.55/s  (0.806s, 1271.14/s)  LR: 2.989e-04  Data: 0.011 (0.020)
Train: 191 [ 250/1251 ( 20%)]  Loss: 3.496 (3.53)  Time: 0.776s, 1319.02/s  (0.804s, 1273.59/s)  LR: 2.989e-04  Data: 0.011 (0.018)
Train: 191 [ 300/1251 ( 24%)]  Loss: 3.486 (3.53)  Time: 0.794s, 1289.85/s  (0.802s, 1277.49/s)  LR: 2.989e-04  Data: 0.011 (0.017)
Train: 191 [ 350/1251 ( 28%)]  Loss: 3.413 (3.51)  Time: 0.786s, 1302.12/s  (0.799s, 1281.96/s)  LR: 2.989e-04  Data: 0.012 (0.016)
Train: 191 [ 400/1251 ( 32%)]  Loss: 3.684 (3.53)  Time: 0.781s, 1310.66/s  (0.797s, 1284.33/s)  LR: 2.989e-04  Data: 0.011 (0.016)
Train: 191 [ 450/1251 ( 36%)]  Loss: 3.837 (3.56)  Time: 0.776s, 1320.03/s  (0.796s, 1286.03/s)  LR: 2.989e-04  Data: 0.011 (0.015)
Train: 191 [ 500/1251 ( 40%)]  Loss: 3.461 (3.55)  Time: 0.818s, 1252.06/s  (0.798s, 1283.12/s)  LR: 2.989e-04  Data: 0.011 (0.015)
Train: 191 [ 550/1251 ( 44%)]  Loss: 4.023 (3.59)  Time: 0.820s, 1249.04/s  (0.800s, 1280.17/s)  LR: 2.989e-04  Data: 0.011 (0.014)
Train: 191 [ 600/1251 ( 48%)]  Loss: 3.733 (3.60)  Time: 0.809s, 1265.06/s  (0.800s, 1280.40/s)  LR: 2.989e-04  Data: 0.011 (0.014)
Train: 191 [ 650/1251 ( 52%)]  Loss: 3.331 (3.58)  Time: 0.822s, 1246.14/s  (0.800s, 1280.05/s)  LR: 2.989e-04  Data: 0.011 (0.014)
Train: 191 [ 700/1251 ( 56%)]  Loss: 3.714 (3.59)  Time: 0.780s, 1313.53/s  (0.800s, 1280.69/s)  LR: 2.989e-04  Data: 0.011 (0.014)
Train: 191 [ 750/1251 ( 60%)]  Loss: 3.723 (3.60)  Time: 0.782s, 1309.39/s  (0.799s, 1282.22/s)  LR: 2.989e-04  Data: 0.012 (0.014)
Train: 191 [ 800/1251 ( 64%)]  Loss: 3.852 (3.62)  Time: 0.822s, 1246.44/s  (0.798s, 1282.91/s)  LR: 2.989e-04  Data: 0.011 (0.013)
Train: 191 [ 850/1251 ( 68%)]  Loss: 3.879 (3.63)  Time: 0.821s, 1247.54/s  (0.799s, 1281.67/s)  LR: 2.989e-04  Data: 0.011 (0.013)
Train: 191 [ 900/1251 ( 72%)]  Loss: 3.365 (3.62)  Time: 0.777s, 1317.29/s  (0.800s, 1280.36/s)  LR: 2.989e-04  Data: 0.011 (0.013)
Train: 191 [ 950/1251 ( 76%)]  Loss: 3.862 (3.63)  Time: 0.778s, 1315.57/s  (0.799s, 1281.64/s)  LR: 2.989e-04  Data: 0.012 (0.013)
Train: 191 [1000/1251 ( 80%)]  Loss: 3.529 (3.62)  Time: 0.779s, 1314.39/s  (0.798s, 1282.87/s)  LR: 2.989e-04  Data: 0.011 (0.013)
Train: 191 [1050/1251 ( 84%)]  Loss: 3.108 (3.60)  Time: 0.779s, 1314.76/s  (0.798s, 1283.57/s)  LR: 2.989e-04  Data: 0.011 (0.013)
Train: 191 [1100/1251 ( 88%)]  Loss: 3.729 (3.61)  Time: 0.831s, 1231.64/s  (0.797s, 1284.35/s)  LR: 2.989e-04  Data: 0.011 (0.013)
Train: 191 [1150/1251 ( 92%)]  Loss: 3.862 (3.62)  Time: 0.780s, 1312.11/s  (0.797s, 1284.85/s)  LR: 2.989e-04  Data: 0.011 (0.013)
Train: 191 [1200/1251 ( 96%)]  Loss: 3.396 (3.61)  Time: 0.785s, 1304.21/s  (0.797s, 1285.21/s)  LR: 2.989e-04  Data: 0.011 (0.013)
Train: 191 [1250/1251 (100%)]  Loss: 3.910 (3.62)  Time: 0.770s, 1330.01/s  (0.797s, 1285.32/s)  LR: 2.989e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.540 (1.540)  Loss:  0.8693 (0.8693)  Acc@1: 89.4531 (89.4531)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.172 (0.562)  Loss:  0.8969 (1.3959)  Acc@1: 84.9057 (74.0200)  Acc@5: 96.6981 (92.0340)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-187.pth.tar', 74.45200011230469)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-186.pth.tar', 74.42200003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-190.pth.tar', 74.2300000415039)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-188.pth.tar', 74.19200008789062)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-189.pth.tar', 74.14399993652344)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-178.pth.tar', 74.02199995117188)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-191.pth.tar', 74.0200000366211)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-183.pth.tar', 73.99000006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-180.pth.tar', 73.98600006103516)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-177.pth.tar', 73.96200005859374)

Train: 192 [   0/1251 (  0%)]  Loss: 3.725 (3.73)  Time: 2.694s,  380.16/s  (2.694s,  380.16/s)  LR: 2.942e-04  Data: 1.957 (1.957)
Train: 192 [  50/1251 (  4%)]  Loss: 3.504 (3.61)  Time: 0.777s, 1317.46/s  (0.830s, 1233.06/s)  LR: 2.942e-04  Data: 0.011 (0.057)
Train: 192 [ 100/1251 (  8%)]  Loss: 3.965 (3.73)  Time: 0.780s, 1313.23/s  (0.810s, 1264.14/s)  LR: 2.942e-04  Data: 0.011 (0.034)
Train: 192 [ 150/1251 ( 12%)]  Loss: 3.141 (3.58)  Time: 0.781s, 1310.78/s  (0.807s, 1269.24/s)  LR: 2.942e-04  Data: 0.011 (0.027)
Train: 192 [ 200/1251 ( 16%)]  Loss: 3.758 (3.62)  Time: 0.786s, 1303.61/s  (0.801s, 1278.94/s)  LR: 2.942e-04  Data: 0.011 (0.023)
Train: 192 [ 250/1251 ( 20%)]  Loss: 3.416 (3.58)  Time: 0.779s, 1314.28/s  (0.802s, 1277.23/s)  LR: 2.942e-04  Data: 0.011 (0.020)
Train: 192 [ 300/1251 ( 24%)]  Loss: 3.549 (3.58)  Time: 0.779s, 1315.15/s  (0.801s, 1278.90/s)  LR: 2.942e-04  Data: 0.011 (0.019)
Train: 192 [ 350/1251 ( 28%)]  Loss: 3.497 (3.57)  Time: 0.780s, 1312.49/s  (0.800s, 1280.47/s)  LR: 2.942e-04  Data: 0.011 (0.018)
Train: 192 [ 400/1251 ( 32%)]  Loss: 3.653 (3.58)  Time: 0.779s, 1314.99/s  (0.798s, 1283.75/s)  LR: 2.942e-04  Data: 0.011 (0.017)
Train: 192 [ 450/1251 ( 36%)]  Loss: 2.892 (3.51)  Time: 0.794s, 1289.77/s  (0.797s, 1285.51/s)  LR: 2.942e-04  Data: 0.011 (0.016)
Train: 192 [ 500/1251 ( 40%)]  Loss: 3.747 (3.53)  Time: 0.824s, 1242.76/s  (0.796s, 1286.31/s)  LR: 2.942e-04  Data: 0.011 (0.016)
Train: 192 [ 550/1251 ( 44%)]  Loss: 3.504 (3.53)  Time: 0.775s, 1320.71/s  (0.796s, 1286.04/s)  LR: 2.942e-04  Data: 0.011 (0.015)
Train: 192 [ 600/1251 ( 48%)]  Loss: 3.503 (3.53)  Time: 0.781s, 1311.02/s  (0.796s, 1286.97/s)  LR: 2.942e-04  Data: 0.011 (0.015)
Train: 192 [ 650/1251 ( 52%)]  Loss: 3.560 (3.53)  Time: 0.779s, 1314.82/s  (0.795s, 1288.19/s)  LR: 2.942e-04  Data: 0.011 (0.015)
Train: 192 [ 700/1251 ( 56%)]  Loss: 3.256 (3.51)  Time: 0.815s, 1257.17/s  (0.795s, 1287.84/s)  LR: 2.942e-04  Data: 0.011 (0.014)
Train: 192 [ 750/1251 ( 60%)]  Loss: 3.625 (3.52)  Time: 0.780s, 1313.13/s  (0.794s, 1289.18/s)  LR: 2.942e-04  Data: 0.011 (0.014)
Train: 192 [ 800/1251 ( 64%)]  Loss: 3.071 (3.49)  Time: 0.777s, 1317.28/s  (0.794s, 1289.54/s)  LR: 2.942e-04  Data: 0.011 (0.014)
Train: 192 [ 850/1251 ( 68%)]  Loss: 3.541 (3.49)  Time: 0.778s, 1316.76/s  (0.793s, 1290.55/s)  LR: 2.942e-04  Data: 0.011 (0.014)
Train: 192 [ 900/1251 ( 72%)]  Loss: 3.983 (3.52)  Time: 0.778s, 1316.31/s  (0.794s, 1290.28/s)  LR: 2.942e-04  Data: 0.012 (0.014)
Train: 192 [ 950/1251 ( 76%)]  Loss: 3.372 (3.51)  Time: 0.776s, 1320.36/s  (0.794s, 1290.07/s)  LR: 2.942e-04  Data: 0.011 (0.014)
Train: 192 [1000/1251 ( 80%)]  Loss: 3.708 (3.52)  Time: 0.819s, 1249.71/s  (0.794s, 1289.69/s)  LR: 2.942e-04  Data: 0.011 (0.013)
Train: 192 [1050/1251 ( 84%)]  Loss: 3.877 (3.54)  Time: 0.837s, 1223.30/s  (0.794s, 1290.06/s)  LR: 2.942e-04  Data: 0.011 (0.013)
Train: 192 [1100/1251 ( 88%)]  Loss: 3.520 (3.54)  Time: 0.839s, 1220.08/s  (0.794s, 1289.27/s)  LR: 2.942e-04  Data: 0.011 (0.013)
Train: 192 [1150/1251 ( 92%)]  Loss: 3.998 (3.56)  Time: 0.778s, 1315.39/s  (0.794s, 1289.72/s)  LR: 2.942e-04  Data: 0.011 (0.013)
Train: 192 [1200/1251 ( 96%)]  Loss: 3.596 (3.56)  Time: 0.779s, 1314.55/s  (0.794s, 1289.82/s)  LR: 2.942e-04  Data: 0.011 (0.013)
Train: 192 [1250/1251 (100%)]  Loss: 3.712 (3.56)  Time: 0.817s, 1253.96/s  (0.794s, 1290.02/s)  LR: 2.942e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.506 (1.506)  Loss:  0.8452 (0.8452)  Acc@1: 89.3555 (89.3555)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.172 (0.566)  Loss:  0.8906 (1.3984)  Acc@1: 84.9057 (74.6580)  Acc@5: 96.8160 (92.1280)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-192.pth.tar', 74.65800003662109)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-187.pth.tar', 74.45200011230469)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-186.pth.tar', 74.42200003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-190.pth.tar', 74.2300000415039)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-188.pth.tar', 74.19200008789062)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-189.pth.tar', 74.14399993652344)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-178.pth.tar', 74.02199995117188)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-191.pth.tar', 74.0200000366211)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-183.pth.tar', 73.99000006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-180.pth.tar', 73.98600006103516)

Train: 193 [   0/1251 (  0%)]  Loss: 3.533 (3.53)  Time: 2.332s,  439.19/s  (2.332s,  439.19/s)  LR: 2.896e-04  Data: 1.598 (1.598)
Train: 193 [  50/1251 (  4%)]  Loss: 3.779 (3.66)  Time: 0.805s, 1271.95/s  (0.829s, 1235.55/s)  LR: 2.896e-04  Data: 0.011 (0.044)
Train: 193 [ 100/1251 (  8%)]  Loss: 4.077 (3.80)  Time: 0.779s, 1314.46/s  (0.813s, 1260.02/s)  LR: 2.896e-04  Data: 0.011 (0.028)
Train: 193 [ 150/1251 ( 12%)]  Loss: 3.475 (3.72)  Time: 0.816s, 1255.03/s  (0.810s, 1264.86/s)  LR: 2.896e-04  Data: 0.011 (0.022)
Train: 193 [ 200/1251 ( 16%)]  Loss: 3.579 (3.69)  Time: 0.813s, 1259.74/s  (0.807s, 1269.12/s)  LR: 2.896e-04  Data: 0.012 (0.020)
Train: 193 [ 250/1251 ( 20%)]  Loss: 3.954 (3.73)  Time: 0.779s, 1315.03/s  (0.804s, 1272.85/s)  LR: 2.896e-04  Data: 0.011 (0.018)
Train: 193 [ 300/1251 ( 24%)]  Loss: 3.609 (3.71)  Time: 0.791s, 1293.90/s  (0.803s, 1275.55/s)  LR: 2.896e-04  Data: 0.011 (0.017)
Train: 193 [ 350/1251 ( 28%)]  Loss: 3.479 (3.69)  Time: 0.780s, 1313.08/s  (0.803s, 1274.44/s)  LR: 2.896e-04  Data: 0.011 (0.016)
Train: 193 [ 400/1251 ( 32%)]  Loss: 3.842 (3.70)  Time: 0.814s, 1257.67/s  (0.803s, 1274.78/s)  LR: 2.896e-04  Data: 0.012 (0.015)
Train: 193 [ 450/1251 ( 36%)]  Loss: 3.406 (3.67)  Time: 0.777s, 1318.15/s  (0.803s, 1274.60/s)  LR: 2.896e-04  Data: 0.011 (0.015)
Train: 193 [ 500/1251 ( 40%)]  Loss: 3.741 (3.68)  Time: 0.778s, 1316.09/s  (0.801s, 1277.67/s)  LR: 2.896e-04  Data: 0.011 (0.015)
Train: 193 [ 550/1251 ( 44%)]  Loss: 3.708 (3.68)  Time: 0.813s, 1258.87/s  (0.800s, 1279.31/s)  LR: 2.896e-04  Data: 0.011 (0.014)
Train: 193 [ 600/1251 ( 48%)]  Loss: 3.461 (3.66)  Time: 0.778s, 1315.82/s  (0.800s, 1280.61/s)  LR: 2.896e-04  Data: 0.011 (0.014)
Train: 193 [ 650/1251 ( 52%)]  Loss: 3.516 (3.65)  Time: 0.779s, 1314.95/s  (0.799s, 1282.23/s)  LR: 2.896e-04  Data: 0.011 (0.014)
Train: 193 [ 700/1251 ( 56%)]  Loss: 3.202 (3.62)  Time: 0.811s, 1262.61/s  (0.798s, 1283.96/s)  LR: 2.896e-04  Data: 0.011 (0.014)
Train: 193 [ 750/1251 ( 60%)]  Loss: 3.637 (3.62)  Time: 0.778s, 1316.33/s  (0.799s, 1282.27/s)  LR: 2.896e-04  Data: 0.011 (0.013)
Train: 193 [ 800/1251 ( 64%)]  Loss: 3.578 (3.62)  Time: 0.776s, 1320.10/s  (0.798s, 1283.36/s)  LR: 2.896e-04  Data: 0.010 (0.013)
Train: 193 [ 850/1251 ( 68%)]  Loss: 3.875 (3.64)  Time: 0.776s, 1319.65/s  (0.798s, 1283.71/s)  LR: 2.896e-04  Data: 0.011 (0.013)
Train: 193 [ 900/1251 ( 72%)]  Loss: 3.665 (3.64)  Time: 0.788s, 1300.11/s  (0.797s, 1284.13/s)  LR: 2.896e-04  Data: 0.010 (0.013)
Train: 193 [ 950/1251 ( 76%)]  Loss: 3.643 (3.64)  Time: 0.781s, 1311.90/s  (0.797s, 1284.90/s)  LR: 2.896e-04  Data: 0.011 (0.013)
Train: 193 [1000/1251 ( 80%)]  Loss: 3.490 (3.63)  Time: 0.781s, 1310.55/s  (0.796s, 1286.13/s)  LR: 2.896e-04  Data: 0.011 (0.013)
Train: 193 [1050/1251 ( 84%)]  Loss: 3.499 (3.62)  Time: 0.780s, 1313.12/s  (0.796s, 1286.28/s)  LR: 2.896e-04  Data: 0.011 (0.013)
Train: 193 [1100/1251 ( 88%)]  Loss: 3.449 (3.62)  Time: 0.823s, 1243.71/s  (0.796s, 1286.56/s)  LR: 2.896e-04  Data: 0.011 (0.013)
Train: 193 [1150/1251 ( 92%)]  Loss: 3.339 (3.61)  Time: 0.823s, 1244.18/s  (0.796s, 1286.73/s)  LR: 2.896e-04  Data: 0.010 (0.013)
Train: 193 [1200/1251 ( 96%)]  Loss: 3.450 (3.60)  Time: 0.779s, 1315.12/s  (0.796s, 1286.62/s)  LR: 2.896e-04  Data: 0.011 (0.012)
Train: 193 [1250/1251 (100%)]  Loss: 3.374 (3.59)  Time: 0.769s, 1331.02/s  (0.795s, 1287.32/s)  LR: 2.896e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.573 (1.573)  Loss:  0.7671 (0.7671)  Acc@1: 89.0625 (89.0625)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.172 (0.562)  Loss:  0.8167 (1.3198)  Acc@1: 85.1415 (74.5140)  Acc@5: 96.6981 (92.1120)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-192.pth.tar', 74.65800003662109)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-193.pth.tar', 74.51400000976562)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-187.pth.tar', 74.45200011230469)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-186.pth.tar', 74.42200003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-190.pth.tar', 74.2300000415039)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-188.pth.tar', 74.19200008789062)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-189.pth.tar', 74.14399993652344)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-178.pth.tar', 74.02199995117188)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-191.pth.tar', 74.0200000366211)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-183.pth.tar', 73.99000006347656)

Train: 194 [   0/1251 (  0%)]  Loss: 3.615 (3.62)  Time: 2.292s,  446.70/s  (2.292s,  446.70/s)  LR: 2.849e-04  Data: 1.557 (1.557)
Train: 194 [  50/1251 (  4%)]  Loss: 3.357 (3.49)  Time: 0.777s, 1318.43/s  (0.833s, 1228.78/s)  LR: 2.849e-04  Data: 0.011 (0.047)
Train: 194 [ 100/1251 (  8%)]  Loss: 3.532 (3.50)  Time: 0.779s, 1314.98/s  (0.813s, 1258.86/s)  LR: 2.849e-04  Data: 0.011 (0.029)
Train: 194 [ 150/1251 ( 12%)]  Loss: 3.683 (3.55)  Time: 0.779s, 1314.81/s  (0.804s, 1274.25/s)  LR: 2.849e-04  Data: 0.011 (0.023)
Train: 194 [ 200/1251 ( 16%)]  Loss: 3.570 (3.55)  Time: 0.820s, 1249.04/s  (0.799s, 1280.98/s)  LR: 2.849e-04  Data: 0.011 (0.020)
Train: 194 [ 250/1251 ( 20%)]  Loss: 3.886 (3.61)  Time: 0.777s, 1318.56/s  (0.801s, 1278.85/s)  LR: 2.849e-04  Data: 0.011 (0.018)
Train: 194 [ 300/1251 ( 24%)]  Loss: 3.265 (3.56)  Time: 0.779s, 1314.88/s  (0.799s, 1281.06/s)  LR: 2.849e-04  Data: 0.011 (0.017)
Train: 194 [ 350/1251 ( 28%)]  Loss: 3.220 (3.52)  Time: 0.816s, 1254.41/s  (0.798s, 1283.02/s)  LR: 2.849e-04  Data: 0.011 (0.016)
Train: 194 [ 400/1251 ( 32%)]  Loss: 3.425 (3.51)  Time: 0.780s, 1312.75/s  (0.798s, 1283.74/s)  LR: 2.849e-04  Data: 0.011 (0.016)
Train: 194 [ 450/1251 ( 36%)]  Loss: 3.947 (3.55)  Time: 0.840s, 1218.42/s  (0.798s, 1283.76/s)  LR: 2.849e-04  Data: 0.011 (0.015)
Train: 194 [ 500/1251 ( 40%)]  Loss: 3.663 (3.56)  Time: 0.794s, 1289.83/s  (0.797s, 1284.80/s)  LR: 2.849e-04  Data: 0.015 (0.015)
Train: 194 [ 550/1251 ( 44%)]  Loss: 3.408 (3.55)  Time: 0.786s, 1303.45/s  (0.797s, 1284.53/s)  LR: 2.849e-04  Data: 0.012 (0.014)
Train: 194 [ 600/1251 ( 48%)]  Loss: 3.797 (3.57)  Time: 0.788s, 1299.39/s  (0.797s, 1285.55/s)  LR: 2.849e-04  Data: 0.011 (0.014)
Train: 194 [ 650/1251 ( 52%)]  Loss: 3.371 (3.55)  Time: 0.821s, 1247.91/s  (0.796s, 1286.71/s)  LR: 2.849e-04  Data: 0.011 (0.014)
Train: 194 [ 700/1251 ( 56%)]  Loss: 4.002 (3.58)  Time: 0.837s, 1223.50/s  (0.796s, 1286.59/s)  LR: 2.849e-04  Data: 0.011 (0.014)
Train: 194 [ 750/1251 ( 60%)]  Loss: 3.312 (3.57)  Time: 0.785s, 1305.28/s  (0.796s, 1286.47/s)  LR: 2.849e-04  Data: 0.011 (0.013)
Train: 194 [ 800/1251 ( 64%)]  Loss: 3.595 (3.57)  Time: 0.777s, 1317.38/s  (0.795s, 1287.30/s)  LR: 2.849e-04  Data: 0.011 (0.013)
Train: 194 [ 850/1251 ( 68%)]  Loss: 3.828 (3.58)  Time: 0.781s, 1311.48/s  (0.796s, 1286.77/s)  LR: 2.849e-04  Data: 0.011 (0.013)
Train: 194 [ 900/1251 ( 72%)]  Loss: 3.989 (3.60)  Time: 0.813s, 1258.91/s  (0.796s, 1285.90/s)  LR: 2.849e-04  Data: 0.011 (0.013)
Train: 194 [ 950/1251 ( 76%)]  Loss: 3.125 (3.58)  Time: 0.777s, 1317.67/s  (0.796s, 1286.93/s)  LR: 2.849e-04  Data: 0.011 (0.013)
Train: 194 [1000/1251 ( 80%)]  Loss: 3.837 (3.59)  Time: 0.795s, 1288.11/s  (0.795s, 1287.85/s)  LR: 2.849e-04  Data: 0.011 (0.013)
Train: 194 [1050/1251 ( 84%)]  Loss: 3.808 (3.60)  Time: 0.781s, 1311.32/s  (0.795s, 1288.74/s)  LR: 2.849e-04  Data: 0.011 (0.013)
Train: 194 [1100/1251 ( 88%)]  Loss: 3.653 (3.60)  Time: 0.825s, 1241.12/s  (0.794s, 1289.23/s)  LR: 2.849e-04  Data: 0.011 (0.013)
Train: 194 [1150/1251 ( 92%)]  Loss: 3.631 (3.61)  Time: 0.778s, 1316.53/s  (0.794s, 1289.56/s)  LR: 2.849e-04  Data: 0.011 (0.013)
Train: 194 [1200/1251 ( 96%)]  Loss: 3.586 (3.60)  Time: 0.779s, 1314.95/s  (0.794s, 1289.39/s)  LR: 2.849e-04  Data: 0.011 (0.013)
Train: 194 [1250/1251 (100%)]  Loss: 3.662 (3.61)  Time: 0.763s, 1341.99/s  (0.794s, 1289.74/s)  LR: 2.849e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.498 (1.498)  Loss:  0.8552 (0.8552)  Acc@1: 88.7695 (88.7695)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.172 (0.572)  Loss:  0.9345 (1.3420)  Acc@1: 83.4906 (74.3140)  Acc@5: 96.3443 (92.2500)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-192.pth.tar', 74.65800003662109)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-193.pth.tar', 74.51400000976562)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-187.pth.tar', 74.45200011230469)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-186.pth.tar', 74.42200003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-194.pth.tar', 74.31399993896484)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-190.pth.tar', 74.2300000415039)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-188.pth.tar', 74.19200008789062)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-189.pth.tar', 74.14399993652344)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-178.pth.tar', 74.02199995117188)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-191.pth.tar', 74.0200000366211)

Train: 195 [   0/1251 (  0%)]  Loss: 3.524 (3.52)  Time: 2.256s,  453.99/s  (2.256s,  453.99/s)  LR: 2.803e-04  Data: 1.519 (1.519)
Train: 195 [  50/1251 (  4%)]  Loss: 3.807 (3.67)  Time: 0.819s, 1250.13/s  (0.833s, 1229.24/s)  LR: 2.803e-04  Data: 0.012 (0.045)
Train: 195 [ 100/1251 (  8%)]  Loss: 3.664 (3.66)  Time: 0.778s, 1316.40/s  (0.823s, 1244.19/s)  LR: 2.803e-04  Data: 0.011 (0.028)
Train: 195 [ 150/1251 ( 12%)]  Loss: 3.258 (3.56)  Time: 0.857s, 1194.31/s  (0.812s, 1260.33/s)  LR: 2.803e-04  Data: 0.011 (0.022)
Train: 195 [ 200/1251 ( 16%)]  Loss: 3.753 (3.60)  Time: 0.826s, 1239.32/s  (0.811s, 1262.87/s)  LR: 2.803e-04  Data: 0.011 (0.020)
Train: 195 [ 250/1251 ( 20%)]  Loss: 3.659 (3.61)  Time: 0.786s, 1302.97/s  (0.805s, 1272.35/s)  LR: 2.803e-04  Data: 0.012 (0.018)
Train: 195 [ 300/1251 ( 24%)]  Loss: 3.287 (3.56)  Time: 0.779s, 1314.77/s  (0.801s, 1278.79/s)  LR: 2.803e-04  Data: 0.012 (0.017)
Train: 195 [ 350/1251 ( 28%)]  Loss: 3.619 (3.57)  Time: 0.778s, 1316.55/s  (0.798s, 1282.52/s)  LR: 2.803e-04  Data: 0.011 (0.016)
Train: 195 [ 400/1251 ( 32%)]  Loss: 3.469 (3.56)  Time: 0.813s, 1259.24/s  (0.797s, 1284.28/s)  LR: 2.803e-04  Data: 0.011 (0.015)
Train: 195 [ 450/1251 ( 36%)]  Loss: 3.574 (3.56)  Time: 0.779s, 1313.95/s  (0.796s, 1286.44/s)  LR: 2.803e-04  Data: 0.011 (0.015)
Train: 195 [ 500/1251 ( 40%)]  Loss: 3.624 (3.57)  Time: 0.833s, 1229.27/s  (0.795s, 1288.58/s)  LR: 2.803e-04  Data: 0.011 (0.015)
Train: 195 [ 550/1251 ( 44%)]  Loss: 3.531 (3.56)  Time: 0.810s, 1263.71/s  (0.795s, 1288.77/s)  LR: 2.803e-04  Data: 0.011 (0.014)
Train: 195 [ 600/1251 ( 48%)]  Loss: 3.455 (3.56)  Time: 0.779s, 1314.76/s  (0.794s, 1289.41/s)  LR: 2.803e-04  Data: 0.012 (0.014)
Train: 195 [ 650/1251 ( 52%)]  Loss: 3.713 (3.57)  Time: 0.781s, 1311.70/s  (0.794s, 1288.90/s)  LR: 2.803e-04  Data: 0.011 (0.014)
Train: 195 [ 700/1251 ( 56%)]  Loss: 3.543 (3.57)  Time: 0.814s, 1258.43/s  (0.794s, 1289.38/s)  LR: 2.803e-04  Data: 0.011 (0.014)
Train: 195 [ 750/1251 ( 60%)]  Loss: 3.798 (3.58)  Time: 0.777s, 1318.42/s  (0.795s, 1288.52/s)  LR: 2.803e-04  Data: 0.011 (0.013)
Train: 195 [ 800/1251 ( 64%)]  Loss: 3.212 (3.56)  Time: 0.778s, 1315.79/s  (0.794s, 1289.56/s)  LR: 2.803e-04  Data: 0.011 (0.013)
Train: 195 [ 850/1251 ( 68%)]  Loss: 3.727 (3.57)  Time: 0.797s, 1285.44/s  (0.794s, 1290.11/s)  LR: 2.803e-04  Data: 0.011 (0.013)
Train: 195 [ 900/1251 ( 72%)]  Loss: 3.524 (3.57)  Time: 0.777s, 1317.18/s  (0.793s, 1290.68/s)  LR: 2.803e-04  Data: 0.011 (0.013)
Train: 195 [ 950/1251 ( 76%)]  Loss: 3.489 (3.56)  Time: 0.781s, 1310.74/s  (0.794s, 1290.14/s)  LR: 2.803e-04  Data: 0.012 (0.013)
Train: 195 [1000/1251 ( 80%)]  Loss: 3.418 (3.55)  Time: 0.778s, 1315.70/s  (0.793s, 1290.84/s)  LR: 2.803e-04  Data: 0.011 (0.013)
Train: 195 [1050/1251 ( 84%)]  Loss: 4.033 (3.58)  Time: 0.780s, 1312.99/s  (0.793s, 1291.29/s)  LR: 2.803e-04  Data: 0.011 (0.013)
Train: 195 [1100/1251 ( 88%)]  Loss: 3.816 (3.59)  Time: 0.886s, 1155.86/s  (0.793s, 1291.25/s)  LR: 2.803e-04  Data: 0.011 (0.013)
Train: 195 [1150/1251 ( 92%)]  Loss: 3.245 (3.57)  Time: 0.779s, 1314.38/s  (0.793s, 1291.60/s)  LR: 2.803e-04  Data: 0.011 (0.013)
Train: 195 [1200/1251 ( 96%)]  Loss: 3.636 (3.58)  Time: 0.787s, 1301.07/s  (0.794s, 1290.39/s)  LR: 2.803e-04  Data: 0.012 (0.013)
Train: 195 [1250/1251 (100%)]  Loss: 3.777 (3.58)  Time: 0.764s, 1340.08/s  (0.793s, 1290.95/s)  LR: 2.803e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.528 (1.528)  Loss:  0.8986 (0.8986)  Acc@1: 88.7695 (88.7695)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.172 (0.561)  Loss:  1.0230 (1.4568)  Acc@1: 85.3774 (74.2140)  Acc@5: 96.1085 (92.0800)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-192.pth.tar', 74.65800003662109)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-193.pth.tar', 74.51400000976562)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-187.pth.tar', 74.45200011230469)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-186.pth.tar', 74.42200003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-194.pth.tar', 74.31399993896484)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-190.pth.tar', 74.2300000415039)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-195.pth.tar', 74.21400011230469)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-188.pth.tar', 74.19200008789062)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-189.pth.tar', 74.14399993652344)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-178.pth.tar', 74.02199995117188)

Train: 196 [   0/1251 (  0%)]  Loss: 3.911 (3.91)  Time: 2.381s,  430.08/s  (2.381s,  430.08/s)  LR: 2.757e-04  Data: 1.646 (1.646)
Train: 196 [  50/1251 (  4%)]  Loss: 3.801 (3.86)  Time: 0.778s, 1315.86/s  (0.822s, 1246.20/s)  LR: 2.757e-04  Data: 0.011 (0.047)
Train: 196 [ 100/1251 (  8%)]  Loss: 3.516 (3.74)  Time: 0.778s, 1316.85/s  (0.816s, 1255.08/s)  LR: 2.757e-04  Data: 0.011 (0.029)
Train: 196 [ 150/1251 ( 12%)]  Loss: 3.796 (3.76)  Time: 0.845s, 1211.60/s  (0.807s, 1269.15/s)  LR: 2.757e-04  Data: 0.011 (0.023)
Train: 196 [ 200/1251 ( 16%)]  Loss: 3.537 (3.71)  Time: 0.780s, 1312.13/s  (0.805s, 1271.73/s)  LR: 2.757e-04  Data: 0.012 (0.020)
Train: 196 [ 250/1251 ( 20%)]  Loss: 3.614 (3.70)  Time: 0.778s, 1316.41/s  (0.800s, 1279.58/s)  LR: 2.757e-04  Data: 0.011 (0.019)
Train: 196 [ 300/1251 ( 24%)]  Loss: 3.398 (3.65)  Time: 0.781s, 1311.64/s  (0.800s, 1280.39/s)  LR: 2.757e-04  Data: 0.011 (0.017)
Train: 196 [ 350/1251 ( 28%)]  Loss: 3.853 (3.68)  Time: 0.779s, 1314.85/s  (0.799s, 1282.27/s)  LR: 2.757e-04  Data: 0.012 (0.016)
Train: 196 [ 400/1251 ( 32%)]  Loss: 3.374 (3.64)  Time: 0.778s, 1315.36/s  (0.797s, 1284.77/s)  LR: 2.757e-04  Data: 0.011 (0.016)
Train: 196 [ 450/1251 ( 36%)]  Loss: 3.482 (3.63)  Time: 0.807s, 1268.95/s  (0.796s, 1286.25/s)  LR: 2.757e-04  Data: 0.012 (0.015)
Train: 196 [ 500/1251 ( 40%)]  Loss: 3.641 (3.63)  Time: 0.776s, 1319.07/s  (0.796s, 1286.91/s)  LR: 2.757e-04  Data: 0.011 (0.015)
Train: 196 [ 550/1251 ( 44%)]  Loss: 3.603 (3.63)  Time: 0.779s, 1313.68/s  (0.795s, 1288.43/s)  LR: 2.757e-04  Data: 0.011 (0.014)
Train: 196 [ 600/1251 ( 48%)]  Loss: 3.409 (3.61)  Time: 0.778s, 1316.81/s  (0.794s, 1289.73/s)  LR: 2.757e-04  Data: 0.011 (0.014)
Train: 196 [ 650/1251 ( 52%)]  Loss: 3.686 (3.62)  Time: 0.785s, 1303.70/s  (0.793s, 1291.08/s)  LR: 2.757e-04  Data: 0.011 (0.014)
Train: 196 [ 700/1251 ( 56%)]  Loss: 3.431 (3.60)  Time: 0.780s, 1312.10/s  (0.792s, 1292.24/s)  LR: 2.757e-04  Data: 0.011 (0.014)
Train: 196 [ 750/1251 ( 60%)]  Loss: 3.496 (3.60)  Time: 0.785s, 1304.27/s  (0.792s, 1292.28/s)  LR: 2.757e-04  Data: 0.011 (0.014)
Train: 196 [ 800/1251 ( 64%)]  Loss: 3.901 (3.61)  Time: 0.819s, 1250.60/s  (0.793s, 1290.56/s)  LR: 2.757e-04  Data: 0.010 (0.013)
Train: 196 [ 850/1251 ( 68%)]  Loss: 3.708 (3.62)  Time: 0.776s, 1319.69/s  (0.794s, 1290.35/s)  LR: 2.757e-04  Data: 0.011 (0.013)
Train: 196 [ 900/1251 ( 72%)]  Loss: 3.764 (3.63)  Time: 0.781s, 1310.92/s  (0.794s, 1289.46/s)  LR: 2.757e-04  Data: 0.010 (0.013)
Train: 196 [ 950/1251 ( 76%)]  Loss: 3.719 (3.63)  Time: 0.788s, 1299.28/s  (0.794s, 1290.32/s)  LR: 2.757e-04  Data: 0.011 (0.013)
Train: 196 [1000/1251 ( 80%)]  Loss: 3.688 (3.63)  Time: 0.813s, 1260.28/s  (0.793s, 1290.69/s)  LR: 2.757e-04  Data: 0.011 (0.013)
Train: 196 [1050/1251 ( 84%)]  Loss: 3.701 (3.64)  Time: 0.775s, 1320.64/s  (0.794s, 1290.43/s)  LR: 2.757e-04  Data: 0.011 (0.013)
Train: 196 [1100/1251 ( 88%)]  Loss: 3.538 (3.63)  Time: 0.778s, 1316.52/s  (0.793s, 1290.50/s)  LR: 2.757e-04  Data: 0.011 (0.013)
Train: 196 [1150/1251 ( 92%)]  Loss: 3.308 (3.62)  Time: 0.821s, 1246.64/s  (0.794s, 1289.62/s)  LR: 2.757e-04  Data: 0.012 (0.013)
Train: 196 [1200/1251 ( 96%)]  Loss: 3.834 (3.63)  Time: 0.778s, 1316.08/s  (0.794s, 1289.56/s)  LR: 2.757e-04  Data: 0.011 (0.013)
Train: 196 [1250/1251 (100%)]  Loss: 3.873 (3.64)  Time: 0.766s, 1336.50/s  (0.794s, 1290.24/s)  LR: 2.757e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.524 (1.524)  Loss:  0.8313 (0.8313)  Acc@1: 89.3555 (89.3555)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.172 (0.559)  Loss:  0.9026 (1.3561)  Acc@1: 84.1981 (74.4880)  Acc@5: 95.7547 (92.1600)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-192.pth.tar', 74.65800003662109)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-193.pth.tar', 74.51400000976562)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-196.pth.tar', 74.48799998779297)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-187.pth.tar', 74.45200011230469)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-186.pth.tar', 74.42200003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-194.pth.tar', 74.31399993896484)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-190.pth.tar', 74.2300000415039)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-195.pth.tar', 74.21400011230469)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-188.pth.tar', 74.19200008789062)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-189.pth.tar', 74.14399993652344)

Train: 197 [   0/1251 (  0%)]  Loss: 3.896 (3.90)  Time: 2.216s,  462.11/s  (2.216s,  462.11/s)  LR: 2.711e-04  Data: 1.483 (1.483)
Train: 197 [  50/1251 (  4%)]  Loss: 3.515 (3.71)  Time: 0.824s, 1242.48/s  (0.827s, 1237.96/s)  LR: 2.711e-04  Data: 0.011 (0.043)
Train: 197 [ 100/1251 (  8%)]  Loss: 3.614 (3.68)  Time: 0.778s, 1316.13/s  (0.809s, 1266.40/s)  LR: 2.711e-04  Data: 0.011 (0.027)
Train: 197 [ 150/1251 ( 12%)]  Loss: 3.078 (3.53)  Time: 0.816s, 1254.43/s  (0.807s, 1269.06/s)  LR: 2.711e-04  Data: 0.011 (0.022)
Train: 197 [ 200/1251 ( 16%)]  Loss: 3.326 (3.49)  Time: 0.788s, 1300.27/s  (0.801s, 1278.04/s)  LR: 2.711e-04  Data: 0.011 (0.019)
Train: 197 [ 250/1251 ( 20%)]  Loss: 3.732 (3.53)  Time: 0.785s, 1304.72/s  (0.800s, 1280.71/s)  LR: 2.711e-04  Data: 0.011 (0.018)
Train: 197 [ 300/1251 ( 24%)]  Loss: 3.526 (3.53)  Time: 0.778s, 1316.69/s  (0.798s, 1283.86/s)  LR: 2.711e-04  Data: 0.011 (0.017)
Train: 197 [ 350/1251 ( 28%)]  Loss: 3.455 (3.52)  Time: 0.780s, 1313.45/s  (0.796s, 1286.41/s)  LR: 2.711e-04  Data: 0.011 (0.016)
Train: 197 [ 400/1251 ( 32%)]  Loss: 3.643 (3.53)  Time: 0.789s, 1297.19/s  (0.795s, 1288.55/s)  LR: 2.711e-04  Data: 0.011 (0.015)
Train: 197 [ 450/1251 ( 36%)]  Loss: 3.684 (3.55)  Time: 0.774s, 1323.22/s  (0.795s, 1288.35/s)  LR: 2.711e-04  Data: 0.010 (0.015)
Train: 197 [ 500/1251 ( 40%)]  Loss: 3.435 (3.54)  Time: 0.780s, 1313.52/s  (0.796s, 1286.75/s)  LR: 2.711e-04  Data: 0.011 (0.014)
Train: 197 [ 550/1251 ( 44%)]  Loss: 3.839 (3.56)  Time: 0.781s, 1310.31/s  (0.795s, 1288.68/s)  LR: 2.711e-04  Data: 0.012 (0.014)
Train: 197 [ 600/1251 ( 48%)]  Loss: 3.701 (3.57)  Time: 0.815s, 1255.96/s  (0.794s, 1289.49/s)  LR: 2.711e-04  Data: 0.011 (0.014)
Train: 197 [ 650/1251 ( 52%)]  Loss: 3.356 (3.56)  Time: 0.839s, 1220.80/s  (0.795s, 1287.88/s)  LR: 2.711e-04  Data: 0.012 (0.014)
Train: 197 [ 700/1251 ( 56%)]  Loss: 3.319 (3.54)  Time: 0.816s, 1254.81/s  (0.795s, 1287.50/s)  LR: 2.711e-04  Data: 0.011 (0.013)
Train: 197 [ 750/1251 ( 60%)]  Loss: 3.596 (3.54)  Time: 0.782s, 1308.92/s  (0.796s, 1285.80/s)  LR: 2.711e-04  Data: 0.011 (0.013)
Train: 197 [ 800/1251 ( 64%)]  Loss: 3.129 (3.52)  Time: 0.778s, 1315.66/s  (0.796s, 1286.37/s)  LR: 2.711e-04  Data: 0.011 (0.013)
Train: 197 [ 850/1251 ( 68%)]  Loss: 3.865 (3.54)  Time: 0.812s, 1260.78/s  (0.796s, 1286.95/s)  LR: 2.711e-04  Data: 0.012 (0.013)
Train: 197 [ 900/1251 ( 72%)]  Loss: 3.369 (3.53)  Time: 0.774s, 1322.52/s  (0.795s, 1287.46/s)  LR: 2.711e-04  Data: 0.011 (0.013)
Train: 197 [ 950/1251 ( 76%)]  Loss: 3.446 (3.53)  Time: 0.782s, 1309.27/s  (0.795s, 1288.30/s)  LR: 2.711e-04  Data: 0.011 (0.013)
Train: 197 [1000/1251 ( 80%)]  Loss: 3.821 (3.54)  Time: 0.781s, 1311.73/s  (0.795s, 1288.65/s)  LR: 2.711e-04  Data: 0.011 (0.013)
Train: 197 [1050/1251 ( 84%)]  Loss: 3.482 (3.54)  Time: 0.779s, 1314.88/s  (0.795s, 1288.39/s)  LR: 2.711e-04  Data: 0.011 (0.013)
Train: 197 [1100/1251 ( 88%)]  Loss: 3.814 (3.55)  Time: 0.813s, 1259.32/s  (0.795s, 1288.61/s)  LR: 2.711e-04  Data: 0.011 (0.013)
Train: 197 [1150/1251 ( 92%)]  Loss: 3.657 (3.55)  Time: 0.776s, 1319.55/s  (0.794s, 1289.38/s)  LR: 2.711e-04  Data: 0.012 (0.013)
Train: 197 [1200/1251 ( 96%)]  Loss: 3.409 (3.55)  Time: 0.822s, 1246.01/s  (0.794s, 1290.00/s)  LR: 2.711e-04  Data: 0.011 (0.013)
Train: 197 [1250/1251 (100%)]  Loss: 3.614 (3.55)  Time: 0.771s, 1327.54/s  (0.794s, 1289.22/s)  LR: 2.711e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.566 (1.566)  Loss:  0.8241 (0.8241)  Acc@1: 89.5508 (89.5508)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.172 (0.561)  Loss:  0.8912 (1.3870)  Acc@1: 84.5519 (74.6960)  Acc@5: 96.4623 (92.3460)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-197.pth.tar', 74.69600001220704)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-192.pth.tar', 74.65800003662109)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-193.pth.tar', 74.51400000976562)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-196.pth.tar', 74.48799998779297)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-187.pth.tar', 74.45200011230469)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-186.pth.tar', 74.42200003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-194.pth.tar', 74.31399993896484)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-190.pth.tar', 74.2300000415039)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-195.pth.tar', 74.21400011230469)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-188.pth.tar', 74.19200008789062)

Train: 198 [   0/1251 (  0%)]  Loss: 3.031 (3.03)  Time: 2.402s,  426.26/s  (2.402s,  426.26/s)  LR: 2.665e-04  Data: 1.610 (1.610)
Train: 198 [  50/1251 (  4%)]  Loss: 3.330 (3.18)  Time: 0.779s, 1314.29/s  (0.828s, 1236.49/s)  LR: 2.665e-04  Data: 0.011 (0.043)
Train: 198 [ 100/1251 (  8%)]  Loss: 3.674 (3.34)  Time: 0.780s, 1312.10/s  (0.813s, 1260.20/s)  LR: 2.665e-04  Data: 0.011 (0.027)
Train: 198 [ 150/1251 ( 12%)]  Loss: 3.288 (3.33)  Time: 0.817s, 1253.36/s  (0.804s, 1274.32/s)  LR: 2.665e-04  Data: 0.012 (0.022)
Train: 198 [ 200/1251 ( 16%)]  Loss: 3.548 (3.37)  Time: 0.778s, 1316.69/s  (0.803s, 1275.15/s)  LR: 2.665e-04  Data: 0.011 (0.019)
Train: 198 [ 250/1251 ( 20%)]  Loss: 3.443 (3.39)  Time: 0.779s, 1314.08/s  (0.803s, 1275.45/s)  LR: 2.665e-04  Data: 0.010 (0.018)
Train: 198 [ 300/1251 ( 24%)]  Loss: 3.787 (3.44)  Time: 0.787s, 1300.86/s  (0.802s, 1277.03/s)  LR: 2.665e-04  Data: 0.011 (0.017)
Train: 198 [ 350/1251 ( 28%)]  Loss: 3.581 (3.46)  Time: 0.803s, 1275.69/s  (0.801s, 1279.18/s)  LR: 2.665e-04  Data: 0.011 (0.016)
Train: 198 [ 400/1251 ( 32%)]  Loss: 3.661 (3.48)  Time: 0.781s, 1311.85/s  (0.800s, 1280.27/s)  LR: 2.665e-04  Data: 0.011 (0.015)
Train: 198 [ 450/1251 ( 36%)]  Loss: 3.576 (3.49)  Time: 0.778s, 1315.97/s  (0.801s, 1279.06/s)  LR: 2.665e-04  Data: 0.011 (0.015)
Train: 198 [ 500/1251 ( 40%)]  Loss: 3.750 (3.52)  Time: 0.778s, 1316.56/s  (0.799s, 1281.95/s)  LR: 2.665e-04  Data: 0.011 (0.014)
Train: 198 [ 550/1251 ( 44%)]  Loss: 3.415 (3.51)  Time: 0.785s, 1304.90/s  (0.797s, 1284.55/s)  LR: 2.665e-04  Data: 0.011 (0.014)
Train: 198 [ 600/1251 ( 48%)]  Loss: 3.684 (3.52)  Time: 0.780s, 1312.10/s  (0.796s, 1286.36/s)  LR: 2.665e-04  Data: 0.011 (0.014)
Train: 198 [ 650/1251 ( 52%)]  Loss: 3.585 (3.53)  Time: 0.781s, 1311.45/s  (0.796s, 1286.91/s)  LR: 2.665e-04  Data: 0.011 (0.014)
Train: 198 [ 700/1251 ( 56%)]  Loss: 3.714 (3.54)  Time: 0.778s, 1315.90/s  (0.795s, 1287.99/s)  LR: 2.665e-04  Data: 0.011 (0.013)
Train: 198 [ 750/1251 ( 60%)]  Loss: 3.454 (3.53)  Time: 0.815s, 1255.72/s  (0.795s, 1288.59/s)  LR: 2.665e-04  Data: 0.011 (0.013)
Train: 198 [ 800/1251 ( 64%)]  Loss: 3.119 (3.51)  Time: 0.789s, 1298.62/s  (0.794s, 1289.33/s)  LR: 2.665e-04  Data: 0.011 (0.013)
Train: 198 [ 850/1251 ( 68%)]  Loss: 3.512 (3.51)  Time: 0.779s, 1315.14/s  (0.794s, 1289.09/s)  LR: 2.665e-04  Data: 0.011 (0.013)
Train: 198 [ 900/1251 ( 72%)]  Loss: 3.979 (3.53)  Time: 0.788s, 1299.68/s  (0.794s, 1289.55/s)  LR: 2.665e-04  Data: 0.011 (0.013)
Train: 198 [ 950/1251 ( 76%)]  Loss: 3.434 (3.53)  Time: 0.779s, 1314.66/s  (0.793s, 1290.50/s)  LR: 2.665e-04  Data: 0.011 (0.013)
Train: 198 [1000/1251 ( 80%)]  Loss: 3.298 (3.52)  Time: 0.818s, 1251.43/s  (0.793s, 1290.70/s)  LR: 2.665e-04  Data: 0.011 (0.013)
Train: 198 [1050/1251 ( 84%)]  Loss: 3.434 (3.51)  Time: 0.779s, 1314.72/s  (0.793s, 1291.37/s)  LR: 2.665e-04  Data: 0.011 (0.013)
Train: 198 [1100/1251 ( 88%)]  Loss: 3.556 (3.52)  Time: 0.777s, 1317.85/s  (0.793s, 1291.11/s)  LR: 2.665e-04  Data: 0.011 (0.013)
Train: 198 [1150/1251 ( 92%)]  Loss: 3.500 (3.51)  Time: 0.779s, 1313.92/s  (0.793s, 1291.80/s)  LR: 2.665e-04  Data: 0.011 (0.013)
Train: 198 [1200/1251 ( 96%)]  Loss: 3.746 (3.52)  Time: 0.777s, 1318.73/s  (0.792s, 1292.28/s)  LR: 2.665e-04  Data: 0.011 (0.012)
Train: 198 [1250/1251 (100%)]  Loss: 3.438 (3.52)  Time: 0.766s, 1336.35/s  (0.793s, 1291.03/s)  LR: 2.665e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.517 (1.517)  Loss:  0.7952 (0.7952)  Acc@1: 89.4531 (89.4531)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.172 (0.565)  Loss:  0.9008 (1.3396)  Acc@1: 84.6698 (74.7680)  Acc@5: 96.6981 (92.1840)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-198.pth.tar', 74.76800006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-197.pth.tar', 74.69600001220704)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-192.pth.tar', 74.65800003662109)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-193.pth.tar', 74.51400000976562)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-196.pth.tar', 74.48799998779297)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-187.pth.tar', 74.45200011230469)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-186.pth.tar', 74.42200003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-194.pth.tar', 74.31399993896484)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-190.pth.tar', 74.2300000415039)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-195.pth.tar', 74.21400011230469)

Train: 199 [   0/1251 (  0%)]  Loss: 3.408 (3.41)  Time: 2.378s,  430.53/s  (2.378s,  430.53/s)  LR: 2.620e-04  Data: 1.635 (1.635)
Train: 199 [  50/1251 (  4%)]  Loss: 3.554 (3.48)  Time: 0.777s, 1317.93/s  (0.833s, 1229.08/s)  LR: 2.620e-04  Data: 0.011 (0.044)
Train: 199 [ 100/1251 (  8%)]  Loss: 3.595 (3.52)  Time: 0.776s, 1318.84/s  (0.811s, 1261.92/s)  LR: 2.620e-04  Data: 0.011 (0.028)
Train: 199 [ 150/1251 ( 12%)]  Loss: 3.453 (3.50)  Time: 0.811s, 1263.22/s  (0.805s, 1272.01/s)  LR: 2.620e-04  Data: 0.012 (0.022)
Train: 199 [ 200/1251 ( 16%)]  Loss: 3.517 (3.51)  Time: 0.815s, 1255.87/s  (0.808s, 1267.67/s)  LR: 2.620e-04  Data: 0.011 (0.020)
Train: 199 [ 250/1251 ( 20%)]  Loss: 3.861 (3.56)  Time: 0.777s, 1318.34/s  (0.807s, 1269.51/s)  LR: 2.620e-04  Data: 0.012 (0.018)
Train: 199 [ 300/1251 ( 24%)]  Loss: 3.643 (3.58)  Time: 0.777s, 1317.50/s  (0.805s, 1271.75/s)  LR: 2.620e-04  Data: 0.011 (0.017)
Train: 199 [ 350/1251 ( 28%)]  Loss: 3.412 (3.56)  Time: 0.776s, 1319.06/s  (0.802s, 1277.06/s)  LR: 2.620e-04  Data: 0.011 (0.016)
Train: 199 [ 400/1251 ( 32%)]  Loss: 3.471 (3.55)  Time: 0.811s, 1262.90/s  (0.800s, 1279.63/s)  LR: 2.620e-04  Data: 0.011 (0.015)
Train: 199 [ 450/1251 ( 36%)]  Loss: 3.327 (3.52)  Time: 0.778s, 1315.51/s  (0.801s, 1278.43/s)  LR: 2.620e-04  Data: 0.011 (0.015)
Train: 199 [ 500/1251 ( 40%)]  Loss: 3.328 (3.51)  Time: 0.781s, 1311.78/s  (0.800s, 1279.80/s)  LR: 2.620e-04  Data: 0.011 (0.014)
Train: 199 [ 550/1251 ( 44%)]  Loss: 3.765 (3.53)  Time: 0.821s, 1247.80/s  (0.800s, 1280.23/s)  LR: 2.620e-04  Data: 0.011 (0.014)
Train: 199 [ 600/1251 ( 48%)]  Loss: 3.538 (3.53)  Time: 0.780s, 1312.48/s  (0.799s, 1280.86/s)  LR: 2.620e-04  Data: 0.011 (0.014)
Train: 199 [ 650/1251 ( 52%)]  Loss: 3.760 (3.55)  Time: 0.777s, 1318.29/s  (0.798s, 1282.69/s)  LR: 2.620e-04  Data: 0.011 (0.014)
Train: 199 [ 700/1251 ( 56%)]  Loss: 3.755 (3.56)  Time: 0.778s, 1316.57/s  (0.797s, 1284.22/s)  LR: 2.620e-04  Data: 0.010 (0.014)
Train: 199 [ 750/1251 ( 60%)]  Loss: 3.558 (3.56)  Time: 0.786s, 1303.01/s  (0.797s, 1284.51/s)  LR: 2.620e-04  Data: 0.011 (0.013)
Train: 199 [ 800/1251 ( 64%)]  Loss: 3.404 (3.55)  Time: 0.833s, 1228.75/s  (0.796s, 1285.85/s)  LR: 2.620e-04  Data: 0.011 (0.013)
Train: 199 [ 850/1251 ( 68%)]  Loss: 3.347 (3.54)  Time: 0.779s, 1314.38/s  (0.796s, 1286.61/s)  LR: 2.620e-04  Data: 0.011 (0.013)
Train: 199 [ 900/1251 ( 72%)]  Loss: 3.761 (3.55)  Time: 0.778s, 1316.79/s  (0.797s, 1285.06/s)  LR: 2.620e-04  Data: 0.011 (0.013)
Train: 199 [ 950/1251 ( 76%)]  Loss: 3.496 (3.55)  Time: 0.778s, 1316.93/s  (0.797s, 1284.12/s)  LR: 2.620e-04  Data: 0.011 (0.013)
Train: 199 [1000/1251 ( 80%)]  Loss: 3.675 (3.55)  Time: 0.780s, 1313.49/s  (0.797s, 1285.11/s)  LR: 2.620e-04  Data: 0.011 (0.013)
Train: 199 [1050/1251 ( 84%)]  Loss: 3.664 (3.56)  Time: 0.812s, 1261.03/s  (0.796s, 1285.96/s)  LR: 2.620e-04  Data: 0.011 (0.013)
Train: 199 [1100/1251 ( 88%)]  Loss: 3.612 (3.56)  Time: 0.819s, 1250.99/s  (0.797s, 1285.39/s)  LR: 2.620e-04  Data: 0.011 (0.013)
Train: 199 [1150/1251 ( 92%)]  Loss: 3.376 (3.55)  Time: 0.781s, 1311.44/s  (0.797s, 1285.35/s)  LR: 2.620e-04  Data: 0.010 (0.013)
Train: 199 [1200/1251 ( 96%)]  Loss: 3.215 (3.54)  Time: 0.779s, 1314.76/s  (0.796s, 1286.31/s)  LR: 2.620e-04  Data: 0.011 (0.013)
Train: 199 [1250/1251 (100%)]  Loss: 3.696 (3.55)  Time: 0.770s, 1329.76/s  (0.796s, 1287.23/s)  LR: 2.620e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.664 (1.664)  Loss:  0.8274 (0.8274)  Acc@1: 89.1602 (89.1602)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.171 (0.564)  Loss:  0.8616 (1.3870)  Acc@1: 84.3160 (74.3780)  Acc@5: 96.6981 (92.1860)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-198.pth.tar', 74.76800006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-197.pth.tar', 74.69600001220704)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-192.pth.tar', 74.65800003662109)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-193.pth.tar', 74.51400000976562)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-196.pth.tar', 74.48799998779297)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-187.pth.tar', 74.45200011230469)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-186.pth.tar', 74.42200003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-199.pth.tar', 74.3780000390625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-194.pth.tar', 74.31399993896484)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-190.pth.tar', 74.2300000415039)

Train: 200 [   0/1251 (  0%)]  Loss: 3.764 (3.76)  Time: 2.592s,  395.00/s  (2.592s,  395.00/s)  LR: 2.575e-04  Data: 1.841 (1.841)
Train: 200 [  50/1251 (  4%)]  Loss: 3.541 (3.65)  Time: 0.779s, 1313.98/s  (0.825s, 1241.96/s)  LR: 2.575e-04  Data: 0.011 (0.047)
Train: 200 [ 100/1251 (  8%)]  Loss: 3.445 (3.58)  Time: 0.778s, 1316.43/s  (0.821s, 1246.70/s)  LR: 2.575e-04  Data: 0.011 (0.029)
Train: 200 [ 150/1251 ( 12%)]  Loss: 4.086 (3.71)  Time: 0.778s, 1317.04/s  (0.808s, 1267.29/s)  LR: 2.575e-04  Data: 0.011 (0.023)
Train: 200 [ 200/1251 ( 16%)]  Loss: 3.399 (3.65)  Time: 0.780s, 1312.18/s  (0.802s, 1277.04/s)  LR: 2.575e-04  Data: 0.012 (0.020)
Train: 200 [ 250/1251 ( 20%)]  Loss: 3.195 (3.57)  Time: 0.779s, 1314.79/s  (0.798s, 1283.65/s)  LR: 2.575e-04  Data: 0.012 (0.018)
Train: 200 [ 300/1251 ( 24%)]  Loss: 3.514 (3.56)  Time: 0.778s, 1315.75/s  (0.797s, 1284.83/s)  LR: 2.575e-04  Data: 0.011 (0.017)
Train: 200 [ 350/1251 ( 28%)]  Loss: 3.398 (3.54)  Time: 0.814s, 1258.65/s  (0.799s, 1282.26/s)  LR: 2.575e-04  Data: 0.011 (0.016)
Train: 200 [ 400/1251 ( 32%)]  Loss: 3.602 (3.55)  Time: 0.815s, 1257.13/s  (0.800s, 1279.27/s)  LR: 2.575e-04  Data: 0.011 (0.016)
Train: 200 [ 450/1251 ( 36%)]  Loss: 3.158 (3.51)  Time: 0.778s, 1316.47/s  (0.801s, 1278.22/s)  LR: 2.575e-04  Data: 0.011 (0.015)
Train: 200 [ 500/1251 ( 40%)]  Loss: 3.528 (3.51)  Time: 0.783s, 1307.06/s  (0.800s, 1279.26/s)  LR: 2.575e-04  Data: 0.011 (0.015)
Train: 200 [ 550/1251 ( 44%)]  Loss: 3.534 (3.51)  Time: 0.779s, 1313.83/s  (0.799s, 1281.95/s)  LR: 2.575e-04  Data: 0.012 (0.014)
Train: 200 [ 600/1251 ( 48%)]  Loss: 3.513 (3.51)  Time: 0.778s, 1315.97/s  (0.798s, 1284.00/s)  LR: 2.575e-04  Data: 0.011 (0.014)
Train: 200 [ 650/1251 ( 52%)]  Loss: 3.209 (3.49)  Time: 0.820s, 1248.69/s  (0.797s, 1284.93/s)  LR: 2.575e-04  Data: 0.012 (0.014)
Train: 200 [ 700/1251 ( 56%)]  Loss: 3.281 (3.48)  Time: 0.779s, 1314.96/s  (0.798s, 1282.59/s)  LR: 2.575e-04  Data: 0.011 (0.014)
Train: 200 [ 750/1251 ( 60%)]  Loss: 3.488 (3.48)  Time: 0.836s, 1224.95/s  (0.798s, 1283.29/s)  LR: 2.575e-04  Data: 0.010 (0.014)
Train: 200 [ 800/1251 ( 64%)]  Loss: 3.070 (3.45)  Time: 0.779s, 1314.87/s  (0.798s, 1283.60/s)  LR: 2.575e-04  Data: 0.011 (0.013)
Train: 200 [ 850/1251 ( 68%)]  Loss: 3.731 (3.47)  Time: 0.785s, 1304.52/s  (0.797s, 1284.27/s)  LR: 2.575e-04  Data: 0.011 (0.013)
Train: 200 [ 900/1251 ( 72%)]  Loss: 3.725 (3.48)  Time: 0.812s, 1260.58/s  (0.797s, 1284.30/s)  LR: 2.575e-04  Data: 0.011 (0.013)
Train: 200 [ 950/1251 ( 76%)]  Loss: 3.736 (3.50)  Time: 0.775s, 1321.36/s  (0.797s, 1284.49/s)  LR: 2.575e-04  Data: 0.011 (0.013)
Train: 200 [1000/1251 ( 80%)]  Loss: 3.333 (3.49)  Time: 0.850s, 1205.32/s  (0.798s, 1283.23/s)  LR: 2.575e-04  Data: 0.012 (0.013)
Train: 200 [1050/1251 ( 84%)]  Loss: 3.171 (3.47)  Time: 0.779s, 1314.85/s  (0.798s, 1283.00/s)  LR: 2.575e-04  Data: 0.011 (0.013)
Train: 200 [1100/1251 ( 88%)]  Loss: 3.432 (3.47)  Time: 0.822s, 1245.88/s  (0.799s, 1281.90/s)  LR: 2.575e-04  Data: 0.011 (0.013)
Train: 200 [1150/1251 ( 92%)]  Loss: 3.843 (3.49)  Time: 0.789s, 1298.03/s  (0.799s, 1281.73/s)  LR: 2.575e-04  Data: 0.014 (0.013)
Train: 200 [1200/1251 ( 96%)]  Loss: 3.400 (3.48)  Time: 0.820s, 1248.90/s  (0.798s, 1282.82/s)  LR: 2.575e-04  Data: 0.011 (0.013)
Train: 200 [1250/1251 (100%)]  Loss: 3.579 (3.49)  Time: 0.766s, 1336.30/s  (0.798s, 1283.27/s)  LR: 2.575e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.495 (1.495)  Loss:  0.8295 (0.8295)  Acc@1: 88.4766 (88.4766)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.172 (0.561)  Loss:  0.8648 (1.3117)  Acc@1: 84.1981 (75.1320)  Acc@5: 96.8160 (92.3760)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-200.pth.tar', 75.13199998779297)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-198.pth.tar', 74.76800006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-197.pth.tar', 74.69600001220704)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-192.pth.tar', 74.65800003662109)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-193.pth.tar', 74.51400000976562)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-196.pth.tar', 74.48799998779297)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-187.pth.tar', 74.45200011230469)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-186.pth.tar', 74.42200003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-199.pth.tar', 74.3780000390625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-194.pth.tar', 74.31399993896484)

Train: 201 [   0/1251 (  0%)]  Loss: 3.523 (3.52)  Time: 2.267s,  451.67/s  (2.267s,  451.67/s)  LR: 2.530e-04  Data: 1.532 (1.532)
Train: 201 [  50/1251 (  4%)]  Loss: 3.254 (3.39)  Time: 0.779s, 1314.44/s  (0.827s, 1238.38/s)  LR: 2.530e-04  Data: 0.011 (0.049)
Train: 201 [ 100/1251 (  8%)]  Loss: 3.928 (3.57)  Time: 0.785s, 1304.90/s  (0.807s, 1268.50/s)  LR: 2.530e-04  Data: 0.011 (0.030)
Train: 201 [ 150/1251 ( 12%)]  Loss: 3.446 (3.54)  Time: 0.778s, 1315.73/s  (0.800s, 1280.57/s)  LR: 2.530e-04  Data: 0.011 (0.024)
Train: 201 [ 200/1251 ( 16%)]  Loss: 3.409 (3.51)  Time: 0.779s, 1314.52/s  (0.796s, 1287.24/s)  LR: 2.530e-04  Data: 0.011 (0.021)
Train: 201 [ 250/1251 ( 20%)]  Loss: 3.807 (3.56)  Time: 0.780s, 1312.96/s  (0.793s, 1290.93/s)  LR: 2.530e-04  Data: 0.011 (0.019)
Train: 201 [ 300/1251 ( 24%)]  Loss: 3.208 (3.51)  Time: 0.778s, 1316.16/s  (0.792s, 1293.21/s)  LR: 2.530e-04  Data: 0.011 (0.017)
Train: 201 [ 350/1251 ( 28%)]  Loss: 3.885 (3.56)  Time: 0.833s, 1229.45/s  (0.793s, 1290.61/s)  LR: 2.530e-04  Data: 0.011 (0.017)
Train: 201 [ 400/1251 ( 32%)]  Loss: 3.560 (3.56)  Time: 0.778s, 1315.74/s  (0.793s, 1291.79/s)  LR: 2.530e-04  Data: 0.011 (0.016)
Train: 201 [ 450/1251 ( 36%)]  Loss: 3.692 (3.57)  Time: 0.776s, 1318.83/s  (0.793s, 1290.88/s)  LR: 2.530e-04  Data: 0.011 (0.015)
Train: 201 [ 500/1251 ( 40%)]  Loss: 3.798 (3.59)  Time: 0.815s, 1256.29/s  (0.794s, 1290.34/s)  LR: 2.530e-04  Data: 0.011 (0.015)
Train: 201 [ 550/1251 ( 44%)]  Loss: 3.504 (3.58)  Time: 0.777s, 1317.14/s  (0.796s, 1287.18/s)  LR: 2.530e-04  Data: 0.011 (0.015)
Train: 201 [ 600/1251 ( 48%)]  Loss: 3.358 (3.57)  Time: 0.810s, 1264.65/s  (0.795s, 1287.34/s)  LR: 2.530e-04  Data: 0.011 (0.014)
Train: 201 [ 650/1251 ( 52%)]  Loss: 3.959 (3.60)  Time: 0.815s, 1256.81/s  (0.796s, 1287.24/s)  LR: 2.530e-04  Data: 0.012 (0.014)
Train: 201 [ 700/1251 ( 56%)]  Loss: 3.763 (3.61)  Time: 0.822s, 1245.56/s  (0.796s, 1285.77/s)  LR: 2.530e-04  Data: 0.012 (0.014)
Train: 201 [ 750/1251 ( 60%)]  Loss: 3.524 (3.60)  Time: 0.780s, 1313.56/s  (0.796s, 1285.75/s)  LR: 2.530e-04  Data: 0.011 (0.014)
Train: 201 [ 800/1251 ( 64%)]  Loss: 3.764 (3.61)  Time: 0.818s, 1251.13/s  (0.796s, 1286.66/s)  LR: 2.530e-04  Data: 0.012 (0.014)
Train: 201 [ 850/1251 ( 68%)]  Loss: 3.328 (3.59)  Time: 0.817s, 1253.00/s  (0.797s, 1285.23/s)  LR: 2.530e-04  Data: 0.011 (0.013)
Train: 201 [ 900/1251 ( 72%)]  Loss: 3.738 (3.60)  Time: 0.779s, 1314.26/s  (0.798s, 1283.60/s)  LR: 2.530e-04  Data: 0.011 (0.013)
Train: 201 [ 950/1251 ( 76%)]  Loss: 3.482 (3.60)  Time: 0.776s, 1319.03/s  (0.798s, 1283.28/s)  LR: 2.530e-04  Data: 0.010 (0.013)
Train: 201 [1000/1251 ( 80%)]  Loss: 3.707 (3.60)  Time: 0.778s, 1316.31/s  (0.797s, 1284.13/s)  LR: 2.530e-04  Data: 0.011 (0.013)
Train: 201 [1050/1251 ( 84%)]  Loss: 3.534 (3.60)  Time: 0.779s, 1313.80/s  (0.797s, 1284.71/s)  LR: 2.530e-04  Data: 0.011 (0.013)
Train: 201 [1100/1251 ( 88%)]  Loss: 3.274 (3.58)  Time: 0.777s, 1318.00/s  (0.798s, 1283.55/s)  LR: 2.530e-04  Data: 0.012 (0.013)
Train: 201 [1150/1251 ( 92%)]  Loss: 3.764 (3.59)  Time: 0.830s, 1233.99/s  (0.798s, 1283.52/s)  LR: 2.530e-04  Data: 0.011 (0.013)
Train: 201 [1200/1251 ( 96%)]  Loss: 3.789 (3.60)  Time: 0.777s, 1317.56/s  (0.797s, 1284.05/s)  LR: 2.530e-04  Data: 0.011 (0.013)
Train: 201 [1250/1251 (100%)]  Loss: 3.403 (3.59)  Time: 0.769s, 1331.80/s  (0.797s, 1285.14/s)  LR: 2.530e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.597 (1.597)  Loss:  0.7634 (0.7634)  Acc@1: 89.4531 (89.4531)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.172 (0.569)  Loss:  0.9437 (1.3898)  Acc@1: 85.6132 (74.8020)  Acc@5: 96.6981 (92.4300)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-200.pth.tar', 75.13199998779297)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-201.pth.tar', 74.80199995605469)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-198.pth.tar', 74.76800006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-197.pth.tar', 74.69600001220704)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-192.pth.tar', 74.65800003662109)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-193.pth.tar', 74.51400000976562)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-196.pth.tar', 74.48799998779297)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-187.pth.tar', 74.45200011230469)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-186.pth.tar', 74.42200003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-199.pth.tar', 74.3780000390625)

Train: 202 [   0/1251 (  0%)]  Loss: 3.307 (3.31)  Time: 2.331s,  439.39/s  (2.331s,  439.39/s)  LR: 2.486e-04  Data: 1.594 (1.594)
Train: 202 [  50/1251 (  4%)]  Loss: 3.193 (3.25)  Time: 0.781s, 1311.32/s  (0.854s, 1198.61/s)  LR: 2.486e-04  Data: 0.011 (0.050)
Train: 202 [ 100/1251 (  8%)]  Loss: 3.272 (3.26)  Time: 0.778s, 1315.84/s  (0.824s, 1242.36/s)  LR: 2.486e-04  Data: 0.011 (0.031)
Train: 202 [ 150/1251 ( 12%)]  Loss: 3.619 (3.35)  Time: 0.780s, 1312.61/s  (0.812s, 1260.76/s)  LR: 2.486e-04  Data: 0.012 (0.024)
Train: 202 [ 200/1251 ( 16%)]  Loss: 3.491 (3.38)  Time: 0.777s, 1317.52/s  (0.805s, 1272.67/s)  LR: 2.486e-04  Data: 0.011 (0.021)
Train: 202 [ 250/1251 ( 20%)]  Loss: 3.412 (3.38)  Time: 0.811s, 1263.18/s  (0.806s, 1271.17/s)  LR: 2.486e-04  Data: 0.011 (0.019)
Train: 202 [ 300/1251 ( 24%)]  Loss: 3.403 (3.39)  Time: 0.777s, 1317.72/s  (0.805s, 1271.46/s)  LR: 2.486e-04  Data: 0.012 (0.018)
Train: 202 [ 350/1251 ( 28%)]  Loss: 3.445 (3.39)  Time: 0.781s, 1310.36/s  (0.802s, 1276.46/s)  LR: 2.486e-04  Data: 0.011 (0.017)
Train: 202 [ 400/1251 ( 32%)]  Loss: 3.279 (3.38)  Time: 0.785s, 1305.21/s  (0.800s, 1279.85/s)  LR: 2.486e-04  Data: 0.011 (0.016)
Train: 202 [ 450/1251 ( 36%)]  Loss: 3.267 (3.37)  Time: 0.795s, 1288.82/s  (0.798s, 1283.20/s)  LR: 2.486e-04  Data: 0.011 (0.015)
Train: 202 [ 500/1251 ( 40%)]  Loss: 3.518 (3.38)  Time: 0.782s, 1309.59/s  (0.797s, 1285.56/s)  LR: 2.486e-04  Data: 0.011 (0.015)
Train: 202 [ 550/1251 ( 44%)]  Loss: 3.399 (3.38)  Time: 0.810s, 1264.01/s  (0.796s, 1286.90/s)  LR: 2.486e-04  Data: 0.011 (0.015)
Train: 202 [ 600/1251 ( 48%)]  Loss: 3.257 (3.37)  Time: 0.778s, 1315.45/s  (0.795s, 1288.59/s)  LR: 2.486e-04  Data: 0.011 (0.014)
Train: 202 [ 650/1251 ( 52%)]  Loss: 3.378 (3.37)  Time: 0.779s, 1314.99/s  (0.795s, 1288.70/s)  LR: 2.486e-04  Data: 0.011 (0.014)
Train: 202 [ 700/1251 ( 56%)]  Loss: 3.585 (3.39)  Time: 0.854s, 1199.56/s  (0.794s, 1289.05/s)  LR: 2.486e-04  Data: 0.011 (0.014)
Train: 202 [ 750/1251 ( 60%)]  Loss: 3.313 (3.38)  Time: 0.822s, 1246.15/s  (0.795s, 1288.19/s)  LR: 2.486e-04  Data: 0.012 (0.014)
Train: 202 [ 800/1251 ( 64%)]  Loss: 3.642 (3.40)  Time: 0.792s, 1292.36/s  (0.796s, 1286.76/s)  LR: 2.486e-04  Data: 0.011 (0.014)
Train: 202 [ 850/1251 ( 68%)]  Loss: 3.628 (3.41)  Time: 0.780s, 1312.54/s  (0.796s, 1286.90/s)  LR: 2.486e-04  Data: 0.010 (0.013)
Train: 202 [ 900/1251 ( 72%)]  Loss: 3.522 (3.42)  Time: 0.778s, 1316.66/s  (0.795s, 1287.59/s)  LR: 2.486e-04  Data: 0.011 (0.013)
Train: 202 [ 950/1251 ( 76%)]  Loss: 3.513 (3.42)  Time: 0.786s, 1301.98/s  (0.795s, 1287.29/s)  LR: 2.486e-04  Data: 0.011 (0.013)
Train: 202 [1000/1251 ( 80%)]  Loss: 3.358 (3.42)  Time: 0.814s, 1258.07/s  (0.795s, 1287.33/s)  LR: 2.486e-04  Data: 0.011 (0.013)
Train: 202 [1050/1251 ( 84%)]  Loss: 3.431 (3.42)  Time: 0.824s, 1243.33/s  (0.796s, 1286.39/s)  LR: 2.486e-04  Data: 0.011 (0.013)
Train: 202 [1100/1251 ( 88%)]  Loss: 3.364 (3.42)  Time: 0.781s, 1310.59/s  (0.795s, 1287.39/s)  LR: 2.486e-04  Data: 0.011 (0.013)
Train: 202 [1150/1251 ( 92%)]  Loss: 3.690 (3.43)  Time: 0.812s, 1260.97/s  (0.795s, 1287.60/s)  LR: 2.486e-04  Data: 0.011 (0.013)
Train: 202 [1200/1251 ( 96%)]  Loss: 3.368 (3.43)  Time: 0.782s, 1309.61/s  (0.796s, 1286.68/s)  LR: 2.486e-04  Data: 0.011 (0.013)
Train: 202 [1250/1251 (100%)]  Loss: 3.707 (3.44)  Time: 0.767s, 1335.80/s  (0.796s, 1286.93/s)  LR: 2.486e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.499 (1.499)  Loss:  0.7377 (0.7377)  Acc@1: 90.7227 (90.7227)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.172 (0.562)  Loss:  0.8375 (1.3431)  Acc@1: 85.4953 (74.9880)  Acc@5: 96.9340 (92.2620)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-200.pth.tar', 75.13199998779297)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-202.pth.tar', 74.98800003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-201.pth.tar', 74.80199995605469)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-198.pth.tar', 74.76800006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-197.pth.tar', 74.69600001220704)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-192.pth.tar', 74.65800003662109)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-193.pth.tar', 74.51400000976562)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-196.pth.tar', 74.48799998779297)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-187.pth.tar', 74.45200011230469)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-186.pth.tar', 74.42200003417969)

Train: 203 [   0/1251 (  0%)]  Loss: 3.249 (3.25)  Time: 2.591s,  395.17/s  (2.591s,  395.17/s)  LR: 2.442e-04  Data: 1.855 (1.855)
Train: 203 [  50/1251 (  4%)]  Loss: 3.429 (3.34)  Time: 0.782s, 1310.09/s  (0.848s, 1207.55/s)  LR: 2.442e-04  Data: 0.011 (0.054)
Train: 203 [ 100/1251 (  8%)]  Loss: 3.678 (3.45)  Time: 0.814s, 1257.96/s  (0.820s, 1248.82/s)  LR: 2.442e-04  Data: 0.011 (0.033)
Train: 203 [ 150/1251 ( 12%)]  Loss: 3.642 (3.50)  Time: 0.781s, 1311.57/s  (0.817s, 1253.09/s)  LR: 2.442e-04  Data: 0.011 (0.026)
Train: 203 [ 200/1251 ( 16%)]  Loss: 3.827 (3.57)  Time: 0.779s, 1314.85/s  (0.809s, 1266.48/s)  LR: 2.442e-04  Data: 0.011 (0.022)
Train: 203 [ 250/1251 ( 20%)]  Loss: 3.466 (3.55)  Time: 0.778s, 1316.35/s  (0.803s, 1275.51/s)  LR: 2.442e-04  Data: 0.011 (0.020)
Train: 203 [ 300/1251 ( 24%)]  Loss: 3.647 (3.56)  Time: 0.780s, 1312.00/s  (0.800s, 1280.00/s)  LR: 2.442e-04  Data: 0.011 (0.018)
Train: 203 [ 350/1251 ( 28%)]  Loss: 3.882 (3.60)  Time: 0.787s, 1300.79/s  (0.798s, 1282.72/s)  LR: 2.442e-04  Data: 0.011 (0.017)
Train: 203 [ 400/1251 ( 32%)]  Loss: 3.543 (3.60)  Time: 0.814s, 1257.32/s  (0.800s, 1279.67/s)  LR: 2.442e-04  Data: 0.011 (0.017)
Train: 203 [ 450/1251 ( 36%)]  Loss: 3.656 (3.60)  Time: 0.812s, 1261.85/s  (0.800s, 1279.33/s)  LR: 2.442e-04  Data: 0.011 (0.016)
Train: 203 [ 500/1251 ( 40%)]  Loss: 3.543 (3.60)  Time: 0.777s, 1317.35/s  (0.801s, 1279.08/s)  LR: 2.442e-04  Data: 0.011 (0.016)
Train: 203 [ 550/1251 ( 44%)]  Loss: 3.795 (3.61)  Time: 0.811s, 1262.46/s  (0.801s, 1279.01/s)  LR: 2.442e-04  Data: 0.011 (0.015)
Train: 203 [ 600/1251 ( 48%)]  Loss: 3.751 (3.62)  Time: 0.777s, 1317.41/s  (0.801s, 1278.79/s)  LR: 2.442e-04  Data: 0.011 (0.015)
Train: 203 [ 650/1251 ( 52%)]  Loss: 3.254 (3.60)  Time: 0.779s, 1314.08/s  (0.800s, 1279.96/s)  LR: 2.442e-04  Data: 0.011 (0.015)
Train: 203 [ 700/1251 ( 56%)]  Loss: 3.330 (3.58)  Time: 0.786s, 1302.78/s  (0.800s, 1280.45/s)  LR: 2.442e-04  Data: 0.011 (0.014)
Train: 203 [ 750/1251 ( 60%)]  Loss: 3.296 (3.56)  Time: 0.778s, 1316.61/s  (0.799s, 1281.93/s)  LR: 2.442e-04  Data: 0.012 (0.014)
Train: 203 [ 800/1251 ( 64%)]  Loss: 3.543 (3.56)  Time: 0.780s, 1312.73/s  (0.798s, 1283.22/s)  LR: 2.442e-04  Data: 0.011 (0.014)
Train: 203 [ 850/1251 ( 68%)]  Loss: 3.766 (3.57)  Time: 0.826s, 1240.07/s  (0.798s, 1283.59/s)  LR: 2.442e-04  Data: 0.011 (0.014)
Train: 203 [ 900/1251 ( 72%)]  Loss: 3.458 (3.57)  Time: 0.782s, 1309.58/s  (0.798s, 1283.58/s)  LR: 2.442e-04  Data: 0.013 (0.014)
Train: 203 [ 950/1251 ( 76%)]  Loss: 3.286 (3.55)  Time: 0.778s, 1316.38/s  (0.797s, 1284.89/s)  LR: 2.442e-04  Data: 0.010 (0.013)
Train: 203 [1000/1251 ( 80%)]  Loss: 3.213 (3.54)  Time: 0.806s, 1270.55/s  (0.797s, 1285.09/s)  LR: 2.442e-04  Data: 0.011 (0.013)
Train: 203 [1050/1251 ( 84%)]  Loss: 3.702 (3.54)  Time: 0.780s, 1312.67/s  (0.797s, 1285.43/s)  LR: 2.442e-04  Data: 0.011 (0.013)
Train: 203 [1100/1251 ( 88%)]  Loss: 3.219 (3.53)  Time: 0.832s, 1231.23/s  (0.797s, 1285.13/s)  LR: 2.442e-04  Data: 0.011 (0.013)
Train: 203 [1150/1251 ( 92%)]  Loss: 3.840 (3.54)  Time: 0.826s, 1240.17/s  (0.797s, 1285.53/s)  LR: 2.442e-04  Data: 0.011 (0.013)
Train: 203 [1200/1251 ( 96%)]  Loss: 3.270 (3.53)  Time: 0.780s, 1312.13/s  (0.796s, 1285.99/s)  LR: 2.442e-04  Data: 0.011 (0.013)
Train: 203 [1250/1251 (100%)]  Loss: 3.274 (3.52)  Time: 0.767s, 1334.33/s  (0.796s, 1285.75/s)  LR: 2.442e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.501 (1.501)  Loss:  0.7912 (0.7912)  Acc@1: 90.4297 (90.4297)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.172 (0.559)  Loss:  0.8679 (1.3642)  Acc@1: 86.2028 (75.1540)  Acc@5: 97.4057 (92.4480)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-203.pth.tar', 75.15399995361328)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-200.pth.tar', 75.13199998779297)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-202.pth.tar', 74.98800003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-201.pth.tar', 74.80199995605469)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-198.pth.tar', 74.76800006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-197.pth.tar', 74.69600001220704)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-192.pth.tar', 74.65800003662109)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-193.pth.tar', 74.51400000976562)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-196.pth.tar', 74.48799998779297)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-187.pth.tar', 74.45200011230469)

Train: 204 [   0/1251 (  0%)]  Loss: 3.045 (3.05)  Time: 2.328s,  439.86/s  (2.328s,  439.86/s)  LR: 2.398e-04  Data: 1.592 (1.592)
Train: 204 [  50/1251 (  4%)]  Loss: 3.626 (3.34)  Time: 0.834s, 1227.70/s  (0.822s, 1245.53/s)  LR: 2.398e-04  Data: 0.011 (0.050)
Train: 204 [ 100/1251 (  8%)]  Loss: 3.467 (3.38)  Time: 0.779s, 1314.36/s  (0.805s, 1272.21/s)  LR: 2.398e-04  Data: 0.011 (0.031)
Train: 204 [ 150/1251 ( 12%)]  Loss: 3.511 (3.41)  Time: 0.779s, 1314.56/s  (0.801s, 1278.47/s)  LR: 2.398e-04  Data: 0.011 (0.024)
Train: 204 [ 200/1251 ( 16%)]  Loss: 3.744 (3.48)  Time: 0.780s, 1313.33/s  (0.796s, 1285.95/s)  LR: 2.398e-04  Data: 0.011 (0.021)
Train: 204 [ 250/1251 ( 20%)]  Loss: 3.447 (3.47)  Time: 0.780s, 1313.12/s  (0.793s, 1291.00/s)  LR: 2.398e-04  Data: 0.011 (0.019)
Train: 204 [ 300/1251 ( 24%)]  Loss: 3.543 (3.48)  Time: 0.812s, 1261.33/s  (0.792s, 1293.05/s)  LR: 2.398e-04  Data: 0.018 (0.018)
Train: 204 [ 350/1251 ( 28%)]  Loss: 3.218 (3.45)  Time: 0.778s, 1316.52/s  (0.793s, 1290.53/s)  LR: 2.398e-04  Data: 0.011 (0.017)
Train: 204 [ 400/1251 ( 32%)]  Loss: 3.519 (3.46)  Time: 0.781s, 1310.76/s  (0.793s, 1291.25/s)  LR: 2.398e-04  Data: 0.011 (0.016)
Train: 204 [ 450/1251 ( 36%)]  Loss: 3.774 (3.49)  Time: 0.775s, 1321.29/s  (0.794s, 1289.22/s)  LR: 2.398e-04  Data: 0.012 (0.016)
Train: 204 [ 500/1251 ( 40%)]  Loss: 3.520 (3.49)  Time: 0.780s, 1313.36/s  (0.793s, 1290.66/s)  LR: 2.398e-04  Data: 0.011 (0.015)
Train: 204 [ 550/1251 ( 44%)]  Loss: 3.310 (3.48)  Time: 0.817s, 1253.77/s  (0.793s, 1291.21/s)  LR: 2.398e-04  Data: 0.011 (0.015)
Train: 204 [ 600/1251 ( 48%)]  Loss: 3.325 (3.47)  Time: 0.787s, 1301.62/s  (0.793s, 1291.97/s)  LR: 2.398e-04  Data: 0.014 (0.015)
Train: 204 [ 650/1251 ( 52%)]  Loss: 3.376 (3.46)  Time: 0.778s, 1315.37/s  (0.792s, 1292.70/s)  LR: 2.398e-04  Data: 0.011 (0.014)
Train: 204 [ 700/1251 ( 56%)]  Loss: 3.403 (3.46)  Time: 0.817s, 1253.66/s  (0.792s, 1293.16/s)  LR: 2.398e-04  Data: 0.011 (0.014)
Train: 204 [ 750/1251 ( 60%)]  Loss: 3.867 (3.48)  Time: 0.778s, 1316.44/s  (0.792s, 1293.47/s)  LR: 2.398e-04  Data: 0.011 (0.014)
Train: 204 [ 800/1251 ( 64%)]  Loss: 3.778 (3.50)  Time: 0.845s, 1211.58/s  (0.793s, 1292.08/s)  LR: 2.398e-04  Data: 0.011 (0.014)
Train: 204 [ 850/1251 ( 68%)]  Loss: 3.621 (3.51)  Time: 0.788s, 1298.78/s  (0.792s, 1292.54/s)  LR: 2.398e-04  Data: 0.011 (0.014)
Train: 204 [ 900/1251 ( 72%)]  Loss: 3.731 (3.52)  Time: 0.807s, 1269.23/s  (0.793s, 1291.37/s)  LR: 2.398e-04  Data: 0.011 (0.013)
Train: 204 [ 950/1251 ( 76%)]  Loss: 3.556 (3.52)  Time: 0.779s, 1314.82/s  (0.793s, 1291.27/s)  LR: 2.398e-04  Data: 0.011 (0.013)
Train: 204 [1000/1251 ( 80%)]  Loss: 3.472 (3.52)  Time: 0.776s, 1319.06/s  (0.793s, 1290.67/s)  LR: 2.398e-04  Data: 0.011 (0.013)
Train: 204 [1050/1251 ( 84%)]  Loss: 3.741 (3.53)  Time: 0.780s, 1312.40/s  (0.793s, 1291.11/s)  LR: 2.398e-04  Data: 0.011 (0.013)
Train: 204 [1100/1251 ( 88%)]  Loss: 3.364 (3.52)  Time: 0.820s, 1249.53/s  (0.793s, 1291.05/s)  LR: 2.398e-04  Data: 0.012 (0.013)
Train: 204 [1150/1251 ( 92%)]  Loss: 3.683 (3.53)  Time: 0.778s, 1316.13/s  (0.794s, 1289.90/s)  LR: 2.398e-04  Data: 0.011 (0.013)
Train: 204 [1200/1251 ( 96%)]  Loss: 3.893 (3.54)  Time: 0.815s, 1256.79/s  (0.794s, 1290.07/s)  LR: 2.398e-04  Data: 0.011 (0.013)
Train: 204 [1250/1251 (100%)]  Loss: 3.343 (3.53)  Time: 0.768s, 1332.84/s  (0.794s, 1290.01/s)  LR: 2.398e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.515 (1.515)  Loss:  0.8418 (0.8418)  Acc@1: 89.4531 (89.4531)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.172 (0.564)  Loss:  0.9667 (1.4043)  Acc@1: 85.2594 (75.1480)  Acc@5: 97.4057 (92.4360)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-203.pth.tar', 75.15399995361328)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-204.pth.tar', 75.14800006103516)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-200.pth.tar', 75.13199998779297)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-202.pth.tar', 74.98800003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-201.pth.tar', 74.80199995605469)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-198.pth.tar', 74.76800006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-197.pth.tar', 74.69600001220704)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-192.pth.tar', 74.65800003662109)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-193.pth.tar', 74.51400000976562)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-196.pth.tar', 74.48799998779297)

Train: 205 [   0/1251 (  0%)]  Loss: 3.536 (3.54)  Time: 2.324s,  440.60/s  (2.324s,  440.60/s)  LR: 2.354e-04  Data: 1.585 (1.585)
Train: 205 [  50/1251 (  4%)]  Loss: 3.239 (3.39)  Time: 0.790s, 1296.61/s  (0.844s, 1212.85/s)  LR: 2.354e-04  Data: 0.011 (0.045)
Train: 205 [ 100/1251 (  8%)]  Loss: 3.563 (3.45)  Time: 0.817s, 1252.61/s  (0.817s, 1253.78/s)  LR: 2.354e-04  Data: 0.012 (0.028)
Train: 205 [ 150/1251 ( 12%)]  Loss: 3.442 (3.45)  Time: 0.819s, 1250.99/s  (0.810s, 1263.45/s)  LR: 2.354e-04  Data: 0.011 (0.023)
Train: 205 [ 200/1251 ( 16%)]  Loss: 3.515 (3.46)  Time: 0.826s, 1239.00/s  (0.808s, 1266.72/s)  LR: 2.354e-04  Data: 0.010 (0.020)
Train: 205 [ 250/1251 ( 20%)]  Loss: 3.862 (3.53)  Time: 0.825s, 1241.12/s  (0.806s, 1269.78/s)  LR: 2.354e-04  Data: 0.011 (0.018)
Train: 205 [ 300/1251 ( 24%)]  Loss: 3.161 (3.47)  Time: 0.778s, 1315.42/s  (0.803s, 1274.96/s)  LR: 2.354e-04  Data: 0.011 (0.017)
Train: 205 [ 350/1251 ( 28%)]  Loss: 3.633 (3.49)  Time: 0.828s, 1237.10/s  (0.801s, 1278.50/s)  LR: 2.354e-04  Data: 0.011 (0.016)
Train: 205 [ 400/1251 ( 32%)]  Loss: 3.478 (3.49)  Time: 0.778s, 1316.07/s  (0.799s, 1280.81/s)  LR: 2.354e-04  Data: 0.011 (0.015)
Train: 205 [ 450/1251 ( 36%)]  Loss: 3.631 (3.51)  Time: 0.834s, 1227.20/s  (0.798s, 1282.62/s)  LR: 2.354e-04  Data: 0.011 (0.015)
Train: 205 [ 500/1251 ( 40%)]  Loss: 3.462 (3.50)  Time: 0.778s, 1316.40/s  (0.798s, 1283.23/s)  LR: 2.354e-04  Data: 0.011 (0.015)
Train: 205 [ 550/1251 ( 44%)]  Loss: 3.942 (3.54)  Time: 0.834s, 1227.37/s  (0.798s, 1283.09/s)  LR: 2.354e-04  Data: 0.011 (0.014)
Train: 205 [ 600/1251 ( 48%)]  Loss: 3.665 (3.55)  Time: 0.812s, 1260.73/s  (0.799s, 1282.00/s)  LR: 2.354e-04  Data: 0.011 (0.014)
Train: 205 [ 650/1251 ( 52%)]  Loss: 3.928 (3.58)  Time: 0.795s, 1287.72/s  (0.798s, 1282.76/s)  LR: 2.354e-04  Data: 0.011 (0.014)
Train: 205 [ 700/1251 ( 56%)]  Loss: 3.771 (3.59)  Time: 0.778s, 1316.78/s  (0.798s, 1282.45/s)  LR: 2.354e-04  Data: 0.012 (0.014)
Train: 205 [ 750/1251 ( 60%)]  Loss: 3.380 (3.58)  Time: 0.775s, 1320.93/s  (0.799s, 1281.83/s)  LR: 2.354e-04  Data: 0.011 (0.013)
Train: 205 [ 800/1251 ( 64%)]  Loss: 3.534 (3.57)  Time: 0.787s, 1300.92/s  (0.798s, 1282.97/s)  LR: 2.354e-04  Data: 0.011 (0.013)
Train: 205 [ 850/1251 ( 68%)]  Loss: 3.845 (3.59)  Time: 0.799s, 1281.54/s  (0.798s, 1283.49/s)  LR: 2.354e-04  Data: 0.011 (0.013)
Train: 205 [ 900/1251 ( 72%)]  Loss: 3.405 (3.58)  Time: 0.788s, 1298.71/s  (0.797s, 1284.87/s)  LR: 2.354e-04  Data: 0.011 (0.013)
Train: 205 [ 950/1251 ( 76%)]  Loss: 3.890 (3.59)  Time: 0.778s, 1315.73/s  (0.796s, 1286.17/s)  LR: 2.354e-04  Data: 0.011 (0.013)
Train: 205 [1000/1251 ( 80%)]  Loss: 3.533 (3.59)  Time: 0.816s, 1254.49/s  (0.796s, 1286.07/s)  LR: 2.354e-04  Data: 0.012 (0.013)
Train: 205 [1050/1251 ( 84%)]  Loss: 3.397 (3.58)  Time: 0.778s, 1315.85/s  (0.796s, 1285.96/s)  LR: 2.354e-04  Data: 0.011 (0.013)
Train: 205 [1100/1251 ( 88%)]  Loss: 3.383 (3.57)  Time: 0.778s, 1315.89/s  (0.796s, 1286.44/s)  LR: 2.354e-04  Data: 0.010 (0.013)
Train: 205 [1150/1251 ( 92%)]  Loss: 3.376 (3.57)  Time: 0.780s, 1313.18/s  (0.795s, 1287.27/s)  LR: 2.354e-04  Data: 0.011 (0.013)
Train: 205 [1200/1251 ( 96%)]  Loss: 3.462 (3.56)  Time: 0.779s, 1314.31/s  (0.795s, 1288.23/s)  LR: 2.354e-04  Data: 0.010 (0.013)
Train: 205 [1250/1251 (100%)]  Loss: 3.334 (3.55)  Time: 0.811s, 1262.47/s  (0.795s, 1287.86/s)  LR: 2.354e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.557 (1.557)  Loss:  0.7888 (0.7888)  Acc@1: 89.5508 (89.5508)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.172 (0.560)  Loss:  0.9060 (1.3499)  Acc@1: 84.4340 (74.7300)  Acc@5: 96.2264 (92.3160)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-203.pth.tar', 75.15399995361328)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-204.pth.tar', 75.14800006103516)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-200.pth.tar', 75.13199998779297)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-202.pth.tar', 74.98800003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-201.pth.tar', 74.80199995605469)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-198.pth.tar', 74.76800006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-205.pth.tar', 74.7299999609375)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-197.pth.tar', 74.69600001220704)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-192.pth.tar', 74.65800003662109)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-193.pth.tar', 74.51400000976562)

Train: 206 [   0/1251 (  0%)]  Loss: 3.336 (3.34)  Time: 2.328s,  439.85/s  (2.328s,  439.85/s)  LR: 2.311e-04  Data: 1.595 (1.595)
Train: 206 [  50/1251 (  4%)]  Loss: 3.829 (3.58)  Time: 0.778s, 1316.26/s  (0.828s, 1237.02/s)  LR: 2.311e-04  Data: 0.012 (0.047)
Train: 206 [ 100/1251 (  8%)]  Loss: 3.355 (3.51)  Time: 0.790s, 1296.95/s  (0.807s, 1268.57/s)  LR: 2.311e-04  Data: 0.011 (0.029)
Train: 206 [ 150/1251 ( 12%)]  Loss: 3.809 (3.58)  Time: 0.778s, 1316.38/s  (0.803s, 1275.30/s)  LR: 2.311e-04  Data: 0.011 (0.023)
Train: 206 [ 200/1251 ( 16%)]  Loss: 3.602 (3.59)  Time: 0.779s, 1314.90/s  (0.798s, 1283.57/s)  LR: 2.311e-04  Data: 0.011 (0.020)
Train: 206 [ 250/1251 ( 20%)]  Loss: 3.511 (3.57)  Time: 0.778s, 1315.93/s  (0.796s, 1286.80/s)  LR: 2.311e-04  Data: 0.011 (0.018)
Train: 206 [ 300/1251 ( 24%)]  Loss: 3.583 (3.57)  Time: 0.779s, 1314.50/s  (0.794s, 1289.68/s)  LR: 2.311e-04  Data: 0.011 (0.017)
Train: 206 [ 350/1251 ( 28%)]  Loss: 3.604 (3.58)  Time: 0.824s, 1241.99/s  (0.795s, 1287.31/s)  LR: 2.311e-04  Data: 0.011 (0.016)
Train: 206 [ 400/1251 ( 32%)]  Loss: 3.590 (3.58)  Time: 0.781s, 1310.93/s  (0.795s, 1288.49/s)  LR: 2.311e-04  Data: 0.011 (0.016)
Train: 206 [ 450/1251 ( 36%)]  Loss: 3.694 (3.59)  Time: 0.777s, 1318.62/s  (0.794s, 1290.00/s)  LR: 2.311e-04  Data: 0.011 (0.015)
Train: 206 [ 500/1251 ( 40%)]  Loss: 3.720 (3.60)  Time: 0.780s, 1313.57/s  (0.793s, 1291.14/s)  LR: 2.311e-04  Data: 0.011 (0.015)
Train: 206 [ 550/1251 ( 44%)]  Loss: 3.401 (3.59)  Time: 0.815s, 1256.79/s  (0.794s, 1290.40/s)  LR: 2.311e-04  Data: 0.012 (0.014)
Train: 206 [ 600/1251 ( 48%)]  Loss: 3.641 (3.59)  Time: 0.817s, 1253.43/s  (0.794s, 1289.09/s)  LR: 2.311e-04  Data: 0.011 (0.014)
Train: 206 [ 650/1251 ( 52%)]  Loss: 3.665 (3.60)  Time: 0.819s, 1249.80/s  (0.795s, 1287.32/s)  LR: 2.311e-04  Data: 0.011 (0.014)
Train: 206 [ 700/1251 ( 56%)]  Loss: 3.730 (3.60)  Time: 0.785s, 1305.25/s  (0.796s, 1286.08/s)  LR: 2.311e-04  Data: 0.011 (0.014)
Train: 206 [ 750/1251 ( 60%)]  Loss: 3.736 (3.61)  Time: 0.841s, 1218.00/s  (0.796s, 1286.49/s)  LR: 2.311e-04  Data: 0.011 (0.014)
Train: 206 [ 800/1251 ( 64%)]  Loss: 3.302 (3.59)  Time: 0.809s, 1265.91/s  (0.796s, 1286.72/s)  LR: 2.311e-04  Data: 0.011 (0.013)
Train: 206 [ 850/1251 ( 68%)]  Loss: 3.478 (3.59)  Time: 0.814s, 1258.32/s  (0.796s, 1286.51/s)  LR: 2.311e-04  Data: 0.012 (0.013)
Train: 206 [ 900/1251 ( 72%)]  Loss: 3.623 (3.59)  Time: 0.782s, 1309.66/s  (0.796s, 1286.58/s)  LR: 2.311e-04  Data: 0.011 (0.013)
Train: 206 [ 950/1251 ( 76%)]  Loss: 3.812 (3.60)  Time: 0.783s, 1308.11/s  (0.796s, 1286.15/s)  LR: 2.311e-04  Data: 0.011 (0.013)
Train: 206 [1000/1251 ( 80%)]  Loss: 3.552 (3.60)  Time: 0.779s, 1315.13/s  (0.796s, 1286.71/s)  LR: 2.311e-04  Data: 0.011 (0.013)
Train: 206 [1050/1251 ( 84%)]  Loss: 3.304 (3.59)  Time: 0.819s, 1251.05/s  (0.795s, 1287.38/s)  LR: 2.311e-04  Data: 0.011 (0.013)
Train: 206 [1100/1251 ( 88%)]  Loss: 3.330 (3.57)  Time: 0.806s, 1270.01/s  (0.796s, 1286.79/s)  LR: 2.311e-04  Data: 0.011 (0.013)
Train: 206 [1150/1251 ( 92%)]  Loss: 3.501 (3.57)  Time: 0.780s, 1312.81/s  (0.795s, 1287.54/s)  LR: 2.311e-04  Data: 0.011 (0.013)
Train: 206 [1200/1251 ( 96%)]  Loss: 3.213 (3.56)  Time: 0.779s, 1315.12/s  (0.795s, 1288.50/s)  LR: 2.311e-04  Data: 0.012 (0.013)
Train: 206 [1250/1251 (100%)]  Loss: 3.448 (3.55)  Time: 0.851s, 1203.62/s  (0.794s, 1289.02/s)  LR: 2.311e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.511 (1.511)  Loss:  0.7912 (0.7912)  Acc@1: 89.3555 (89.3555)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.172 (0.561)  Loss:  0.8481 (1.3616)  Acc@1: 85.4953 (74.8820)  Acc@5: 96.8160 (92.3480)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-203.pth.tar', 75.15399995361328)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-204.pth.tar', 75.14800006103516)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-200.pth.tar', 75.13199998779297)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-202.pth.tar', 74.98800003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-206.pth.tar', 74.88200003417968)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-201.pth.tar', 74.80199995605469)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-198.pth.tar', 74.76800006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-205.pth.tar', 74.7299999609375)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-197.pth.tar', 74.69600001220704)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-192.pth.tar', 74.65800003662109)

Train: 207 [   0/1251 (  0%)]  Loss: 3.576 (3.58)  Time: 2.303s,  444.66/s  (2.303s,  444.66/s)  LR: 2.268e-04  Data: 1.568 (1.568)
Train: 207 [  50/1251 (  4%)]  Loss: 3.618 (3.60)  Time: 0.792s, 1292.97/s  (0.825s, 1241.09/s)  LR: 2.268e-04  Data: 0.011 (0.046)
Train: 207 [ 100/1251 (  8%)]  Loss: 3.681 (3.62)  Time: 0.778s, 1315.54/s  (0.803s, 1275.59/s)  LR: 2.268e-04  Data: 0.011 (0.029)
Train: 207 [ 150/1251 ( 12%)]  Loss: 3.713 (3.65)  Time: 0.779s, 1314.25/s  (0.800s, 1280.35/s)  LR: 2.268e-04  Data: 0.011 (0.023)
Train: 207 [ 200/1251 ( 16%)]  Loss: 3.778 (3.67)  Time: 0.815s, 1256.93/s  (0.802s, 1277.50/s)  LR: 2.268e-04  Data: 0.011 (0.020)
Train: 207 [ 250/1251 ( 20%)]  Loss: 3.803 (3.69)  Time: 0.781s, 1310.96/s  (0.799s, 1281.41/s)  LR: 2.268e-04  Data: 0.010 (0.018)
Train: 207 [ 300/1251 ( 24%)]  Loss: 3.730 (3.70)  Time: 0.778s, 1316.98/s  (0.798s, 1283.05/s)  LR: 2.268e-04  Data: 0.012 (0.017)
Train: 207 [ 350/1251 ( 28%)]  Loss: 3.744 (3.71)  Time: 0.778s, 1315.66/s  (0.796s, 1286.28/s)  LR: 2.268e-04  Data: 0.011 (0.016)
Train: 207 [ 400/1251 ( 32%)]  Loss: 3.664 (3.70)  Time: 0.778s, 1316.49/s  (0.796s, 1287.09/s)  LR: 2.268e-04  Data: 0.011 (0.016)
Train: 207 [ 450/1251 ( 36%)]  Loss: 3.545 (3.69)  Time: 0.812s, 1261.41/s  (0.795s, 1288.45/s)  LR: 2.268e-04  Data: 0.011 (0.015)
Train: 207 [ 500/1251 ( 40%)]  Loss: 3.330 (3.65)  Time: 0.837s, 1223.92/s  (0.794s, 1289.90/s)  LR: 2.268e-04  Data: 0.011 (0.015)
Train: 207 [ 550/1251 ( 44%)]  Loss: 3.603 (3.65)  Time: 0.777s, 1318.40/s  (0.793s, 1291.59/s)  LR: 2.268e-04  Data: 0.011 (0.014)
Train: 207 [ 600/1251 ( 48%)]  Loss: 3.458 (3.63)  Time: 0.807s, 1269.55/s  (0.793s, 1290.67/s)  LR: 2.268e-04  Data: 0.011 (0.014)
Train: 207 [ 650/1251 ( 52%)]  Loss: 3.517 (3.63)  Time: 0.859s, 1191.73/s  (0.794s, 1289.93/s)  LR: 2.268e-04  Data: 0.011 (0.014)
Train: 207 [ 700/1251 ( 56%)]  Loss: 3.532 (3.62)  Time: 0.778s, 1317.02/s  (0.794s, 1290.08/s)  LR: 2.268e-04  Data: 0.011 (0.014)
Train: 207 [ 750/1251 ( 60%)]  Loss: 3.512 (3.61)  Time: 0.778s, 1316.13/s  (0.794s, 1289.41/s)  LR: 2.268e-04  Data: 0.011 (0.014)
Train: 207 [ 800/1251 ( 64%)]  Loss: 3.714 (3.62)  Time: 0.786s, 1302.44/s  (0.794s, 1289.51/s)  LR: 2.268e-04  Data: 0.011 (0.013)
Train: 207 [ 850/1251 ( 68%)]  Loss: 3.499 (3.61)  Time: 0.778s, 1316.85/s  (0.794s, 1289.87/s)  LR: 2.268e-04  Data: 0.011 (0.013)
Train: 207 [ 900/1251 ( 72%)]  Loss: 3.768 (3.62)  Time: 0.777s, 1318.08/s  (0.794s, 1290.21/s)  LR: 2.268e-04  Data: 0.011 (0.013)
Train: 207 [ 950/1251 ( 76%)]  Loss: 3.621 (3.62)  Time: 0.778s, 1316.78/s  (0.793s, 1291.09/s)  LR: 2.268e-04  Data: 0.011 (0.013)
Train: 207 [1000/1251 ( 80%)]  Loss: 3.803 (3.63)  Time: 0.811s, 1261.88/s  (0.793s, 1291.15/s)  LR: 2.268e-04  Data: 0.011 (0.013)
Train: 207 [1050/1251 ( 84%)]  Loss: 3.531 (3.62)  Time: 0.781s, 1311.08/s  (0.793s, 1290.71/s)  LR: 2.268e-04  Data: 0.011 (0.013)
Train: 207 [1100/1251 ( 88%)]  Loss: 3.362 (3.61)  Time: 0.784s, 1305.47/s  (0.793s, 1291.15/s)  LR: 2.268e-04  Data: 0.011 (0.013)
Train: 207 [1150/1251 ( 92%)]  Loss: 3.552 (3.61)  Time: 0.781s, 1311.80/s  (0.793s, 1291.55/s)  LR: 2.268e-04  Data: 0.012 (0.013)
Train: 207 [1200/1251 ( 96%)]  Loss: 3.321 (3.60)  Time: 0.795s, 1287.98/s  (0.793s, 1292.11/s)  LR: 2.268e-04  Data: 0.011 (0.013)
Train: 207 [1250/1251 (100%)]  Loss: 3.672 (3.60)  Time: 0.766s, 1336.89/s  (0.792s, 1292.25/s)  LR: 2.268e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.574 (1.574)  Loss:  0.8413 (0.8413)  Acc@1: 89.9414 (89.9414)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.172 (0.570)  Loss:  0.9077 (1.3792)  Acc@1: 84.3160 (75.1840)  Acc@5: 96.9340 (92.4600)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-207.pth.tar', 75.1840000390625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-203.pth.tar', 75.15399995361328)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-204.pth.tar', 75.14800006103516)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-200.pth.tar', 75.13199998779297)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-202.pth.tar', 74.98800003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-206.pth.tar', 74.88200003417968)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-201.pth.tar', 74.80199995605469)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-198.pth.tar', 74.76800006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-205.pth.tar', 74.7299999609375)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-197.pth.tar', 74.69600001220704)

Train: 208 [   0/1251 (  0%)]  Loss: 3.702 (3.70)  Time: 2.233s,  458.61/s  (2.233s,  458.61/s)  LR: 2.225e-04  Data: 1.497 (1.497)
Train: 208 [  50/1251 (  4%)]  Loss: 3.525 (3.61)  Time: 0.779s, 1314.91/s  (0.816s, 1254.29/s)  LR: 2.225e-04  Data: 0.011 (0.045)
Train: 208 [ 100/1251 (  8%)]  Loss: 3.553 (3.59)  Time: 0.781s, 1310.84/s  (0.805s, 1271.94/s)  LR: 2.225e-04  Data: 0.011 (0.028)
Train: 208 [ 150/1251 ( 12%)]  Loss: 3.539 (3.58)  Time: 0.780s, 1313.19/s  (0.800s, 1280.20/s)  LR: 2.225e-04  Data: 0.011 (0.023)
Train: 208 [ 200/1251 ( 16%)]  Loss: 3.452 (3.55)  Time: 0.784s, 1306.11/s  (0.798s, 1283.38/s)  LR: 2.225e-04  Data: 0.011 (0.020)
Train: 208 [ 250/1251 ( 20%)]  Loss: 3.103 (3.48)  Time: 0.778s, 1316.08/s  (0.794s, 1289.19/s)  LR: 2.225e-04  Data: 0.011 (0.018)
Train: 208 [ 300/1251 ( 24%)]  Loss: 4.058 (3.56)  Time: 0.781s, 1311.48/s  (0.793s, 1291.23/s)  LR: 2.225e-04  Data: 0.011 (0.017)
Train: 208 [ 350/1251 ( 28%)]  Loss: 3.571 (3.56)  Time: 0.778s, 1316.41/s  (0.791s, 1294.09/s)  LR: 2.225e-04  Data: 0.011 (0.016)
Train: 208 [ 400/1251 ( 32%)]  Loss: 3.607 (3.57)  Time: 0.813s, 1259.41/s  (0.791s, 1294.69/s)  LR: 2.225e-04  Data: 0.012 (0.015)
Train: 208 [ 450/1251 ( 36%)]  Loss: 3.366 (3.55)  Time: 0.799s, 1281.05/s  (0.792s, 1293.13/s)  LR: 2.225e-04  Data: 0.011 (0.015)
Train: 208 [ 500/1251 ( 40%)]  Loss: 3.554 (3.55)  Time: 0.780s, 1313.03/s  (0.791s, 1294.73/s)  LR: 2.225e-04  Data: 0.011 (0.015)
Train: 208 [ 550/1251 ( 44%)]  Loss: 3.310 (3.53)  Time: 0.817s, 1252.89/s  (0.791s, 1294.60/s)  LR: 2.225e-04  Data: 0.011 (0.014)
Train: 208 [ 600/1251 ( 48%)]  Loss: 3.798 (3.55)  Time: 0.779s, 1313.94/s  (0.791s, 1294.67/s)  LR: 2.225e-04  Data: 0.011 (0.014)
Train: 208 [ 650/1251 ( 52%)]  Loss: 3.405 (3.54)  Time: 0.820s, 1248.76/s  (0.791s, 1294.81/s)  LR: 2.225e-04  Data: 0.011 (0.014)
Train: 208 [ 700/1251 ( 56%)]  Loss: 3.310 (3.52)  Time: 0.780s, 1312.08/s  (0.792s, 1293.45/s)  LR: 2.225e-04  Data: 0.011 (0.014)
Train: 208 [ 750/1251 ( 60%)]  Loss: 3.321 (3.51)  Time: 0.801s, 1277.88/s  (0.792s, 1293.50/s)  LR: 2.225e-04  Data: 0.011 (0.013)
Train: 208 [ 800/1251 ( 64%)]  Loss: 3.185 (3.49)  Time: 0.786s, 1302.05/s  (0.791s, 1294.12/s)  LR: 2.225e-04  Data: 0.011 (0.013)
Train: 208 [ 850/1251 ( 68%)]  Loss: 3.623 (3.50)  Time: 0.776s, 1319.84/s  (0.792s, 1293.53/s)  LR: 2.225e-04  Data: 0.011 (0.013)
Train: 208 [ 900/1251 ( 72%)]  Loss: 3.468 (3.50)  Time: 0.801s, 1278.83/s  (0.792s, 1293.37/s)  LR: 2.225e-04  Data: 0.011 (0.013)
Train: 208 [ 950/1251 ( 76%)]  Loss: 3.758 (3.51)  Time: 0.777s, 1318.36/s  (0.792s, 1293.26/s)  LR: 2.225e-04  Data: 0.011 (0.013)
Train: 208 [1000/1251 ( 80%)]  Loss: 3.783 (3.52)  Time: 0.779s, 1314.87/s  (0.792s, 1292.81/s)  LR: 2.225e-04  Data: 0.011 (0.013)
Train: 208 [1050/1251 ( 84%)]  Loss: 3.603 (3.53)  Time: 0.777s, 1317.92/s  (0.792s, 1292.79/s)  LR: 2.225e-04  Data: 0.011 (0.013)
Train: 208 [1100/1251 ( 88%)]  Loss: 3.548 (3.53)  Time: 0.814s, 1257.89/s  (0.792s, 1292.52/s)  LR: 2.225e-04  Data: 0.012 (0.013)
Train: 208 [1150/1251 ( 92%)]  Loss: 3.542 (3.53)  Time: 0.780s, 1313.46/s  (0.792s, 1292.48/s)  LR: 2.225e-04  Data: 0.011 (0.013)
Train: 208 [1200/1251 ( 96%)]  Loss: 3.623 (3.53)  Time: 0.782s, 1309.40/s  (0.793s, 1291.73/s)  LR: 2.225e-04  Data: 0.011 (0.013)
Train: 208 [1250/1251 (100%)]  Loss: 3.331 (3.52)  Time: 0.765s, 1338.28/s  (0.793s, 1291.45/s)  LR: 2.225e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.505 (1.505)  Loss:  0.7705 (0.7705)  Acc@1: 89.6484 (89.6484)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.172 (0.571)  Loss:  0.7800 (1.2640)  Acc@1: 84.6698 (75.3400)  Acc@5: 96.5802 (92.5460)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-208.pth.tar', 75.34000006347657)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-207.pth.tar', 75.1840000390625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-203.pth.tar', 75.15399995361328)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-204.pth.tar', 75.14800006103516)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-200.pth.tar', 75.13199998779297)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-202.pth.tar', 74.98800003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-206.pth.tar', 74.88200003417968)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-201.pth.tar', 74.80199995605469)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-198.pth.tar', 74.76800006347656)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-205.pth.tar', 74.7299999609375)

Train: 209 [   0/1251 (  0%)]  Loss: 3.154 (3.15)  Time: 2.338s,  437.92/s  (2.338s,  437.92/s)  LR: 2.183e-04  Data: 1.604 (1.604)
Train: 209 [  50/1251 (  4%)]  Loss: 3.677 (3.42)  Time: 0.774s, 1322.36/s  (0.820s, 1248.46/s)  LR: 2.183e-04  Data: 0.011 (0.044)
Train: 209 [ 100/1251 (  8%)]  Loss: 3.765 (3.53)  Time: 0.812s, 1261.10/s  (0.808s, 1267.34/s)  LR: 2.183e-04  Data: 0.011 (0.028)
Train: 209 [ 150/1251 ( 12%)]  Loss: 3.542 (3.53)  Time: 0.778s, 1316.17/s  (0.802s, 1277.07/s)  LR: 2.183e-04  Data: 0.011 (0.022)
Train: 209 [ 200/1251 ( 16%)]  Loss: 3.483 (3.52)  Time: 0.812s, 1261.09/s  (0.798s, 1283.78/s)  LR: 2.183e-04  Data: 0.011 (0.020)
Train: 209 [ 250/1251 ( 20%)]  Loss: 3.569 (3.53)  Time: 0.779s, 1315.23/s  (0.796s, 1286.14/s)  LR: 2.183e-04  Data: 0.012 (0.018)
Train: 209 [ 300/1251 ( 24%)]  Loss: 3.562 (3.54)  Time: 0.793s, 1291.96/s  (0.794s, 1290.22/s)  LR: 2.183e-04  Data: 0.011 (0.017)
Train: 209 [ 350/1251 ( 28%)]  Loss: 3.206 (3.49)  Time: 0.820s, 1248.81/s  (0.793s, 1291.53/s)  LR: 2.183e-04  Data: 0.012 (0.016)
Train: 209 [ 400/1251 ( 32%)]  Loss: 3.525 (3.50)  Time: 0.781s, 1311.10/s  (0.793s, 1291.90/s)  LR: 2.183e-04  Data: 0.012 (0.016)
Train: 209 [ 450/1251 ( 36%)]  Loss: 3.588 (3.51)  Time: 0.779s, 1314.33/s  (0.792s, 1292.14/s)  LR: 2.183e-04  Data: 0.011 (0.015)
Train: 209 [ 500/1251 ( 40%)]  Loss: 3.755 (3.53)  Time: 0.777s, 1317.69/s  (0.792s, 1292.66/s)  LR: 2.183e-04  Data: 0.010 (0.015)
Train: 209 [ 550/1251 ( 44%)]  Loss: 3.762 (3.55)  Time: 0.780s, 1313.35/s  (0.791s, 1293.97/s)  LR: 2.183e-04  Data: 0.011 (0.014)
Train: 209 [ 600/1251 ( 48%)]  Loss: 3.302 (3.53)  Time: 0.790s, 1296.24/s  (0.791s, 1293.91/s)  LR: 2.183e-04  Data: 0.011 (0.014)
Train: 209 [ 650/1251 ( 52%)]  Loss: 3.600 (3.53)  Time: 0.788s, 1300.18/s  (0.791s, 1293.82/s)  LR: 2.183e-04  Data: 0.013 (0.014)
Train: 209 [ 700/1251 ( 56%)]  Loss: 3.504 (3.53)  Time: 0.777s, 1318.03/s  (0.791s, 1294.59/s)  LR: 2.183e-04  Data: 0.012 (0.014)
Train: 209 [ 750/1251 ( 60%)]  Loss: 3.316 (3.52)  Time: 0.780s, 1313.35/s  (0.791s, 1295.36/s)  LR: 2.183e-04  Data: 0.011 (0.013)
Train: 209 [ 800/1251 ( 64%)]  Loss: 3.245 (3.50)  Time: 0.816s, 1254.14/s  (0.791s, 1295.29/s)  LR: 2.183e-04  Data: 0.011 (0.013)
Train: 209 [ 850/1251 ( 68%)]  Loss: 3.583 (3.51)  Time: 0.789s, 1297.80/s  (0.790s, 1295.66/s)  LR: 2.183e-04  Data: 0.011 (0.013)
Train: 209 [ 900/1251 ( 72%)]  Loss: 3.830 (3.52)  Time: 0.779s, 1314.97/s  (0.791s, 1294.07/s)  LR: 2.183e-04  Data: 0.011 (0.013)
Train: 209 [ 950/1251 ( 76%)]  Loss: 3.570 (3.53)  Time: 0.835s, 1226.80/s  (0.791s, 1294.75/s)  LR: 2.183e-04  Data: 0.011 (0.013)
Train: 209 [1000/1251 ( 80%)]  Loss: 3.819 (3.54)  Time: 0.814s, 1258.11/s  (0.792s, 1293.56/s)  LR: 2.183e-04  Data: 0.011 (0.013)
Train: 209 [1050/1251 ( 84%)]  Loss: 3.273 (3.53)  Time: 0.778s, 1315.66/s  (0.791s, 1294.13/s)  LR: 2.183e-04  Data: 0.012 (0.013)
Train: 209 [1100/1251 ( 88%)]  Loss: 3.537 (3.53)  Time: 0.821s, 1247.32/s  (0.791s, 1294.24/s)  LR: 2.183e-04  Data: 0.011 (0.013)
Train: 209 [1150/1251 ( 92%)]  Loss: 3.801 (3.54)  Time: 0.778s, 1316.55/s  (0.792s, 1293.49/s)  LR: 2.183e-04  Data: 0.011 (0.013)
Train: 209 [1200/1251 ( 96%)]  Loss: 3.497 (3.54)  Time: 0.779s, 1314.40/s  (0.792s, 1293.62/s)  LR: 2.183e-04  Data: 0.011 (0.013)
Train: 209 [1250/1251 (100%)]  Loss: 3.523 (3.54)  Time: 0.780s, 1313.45/s  (0.792s, 1293.60/s)  LR: 2.183e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.551 (1.551)  Loss:  0.8251 (0.8251)  Acc@1: 90.3320 (90.3320)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.172 (0.566)  Loss:  0.9321 (1.3566)  Acc@1: 84.7877 (75.2640)  Acc@5: 97.2877 (92.5920)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-208.pth.tar', 75.34000006347657)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-209.pth.tar', 75.26399998535156)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-207.pth.tar', 75.1840000390625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-203.pth.tar', 75.15399995361328)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-204.pth.tar', 75.14800006103516)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-200.pth.tar', 75.13199998779297)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-202.pth.tar', 74.98800003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-206.pth.tar', 74.88200003417968)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-201.pth.tar', 74.80199995605469)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-198.pth.tar', 74.76800006347656)

Train: 210 [   0/1251 (  0%)]  Loss: 3.233 (3.23)  Time: 2.448s,  418.28/s  (2.448s,  418.28/s)  LR: 2.140e-04  Data: 1.648 (1.648)
Train: 210 [  50/1251 (  4%)]  Loss: 3.860 (3.55)  Time: 0.790s, 1295.79/s  (0.829s, 1234.57/s)  LR: 2.140e-04  Data: 0.011 (0.048)
Train: 210 [ 100/1251 (  8%)]  Loss: 3.614 (3.57)  Time: 0.782s, 1310.13/s  (0.814s, 1257.54/s)  LR: 2.140e-04  Data: 0.012 (0.030)
Train: 210 [ 150/1251 ( 12%)]  Loss: 3.407 (3.53)  Time: 0.778s, 1316.98/s  (0.805s, 1272.03/s)  LR: 2.140e-04  Data: 0.011 (0.024)
Train: 210 [ 200/1251 ( 16%)]  Loss: 3.485 (3.52)  Time: 0.778s, 1316.88/s  (0.799s, 1282.09/s)  LR: 2.140e-04  Data: 0.011 (0.021)
Train: 210 [ 250/1251 ( 20%)]  Loss: 3.097 (3.45)  Time: 0.790s, 1296.07/s  (0.798s, 1283.77/s)  LR: 2.140e-04  Data: 0.011 (0.019)
Train: 210 [ 300/1251 ( 24%)]  Loss: 3.345 (3.43)  Time: 0.816s, 1255.67/s  (0.797s, 1284.45/s)  LR: 2.140e-04  Data: 0.011 (0.018)
Train: 210 [ 350/1251 ( 28%)]  Loss: 3.413 (3.43)  Time: 0.779s, 1314.28/s  (0.795s, 1287.64/s)  LR: 2.140e-04  Data: 0.010 (0.017)
Train: 210 [ 400/1251 ( 32%)]  Loss: 3.112 (3.40)  Time: 0.849s, 1206.48/s  (0.794s, 1290.27/s)  LR: 2.140e-04  Data: 0.010 (0.016)
Train: 210 [ 450/1251 ( 36%)]  Loss: 3.600 (3.42)  Time: 0.812s, 1261.79/s  (0.793s, 1291.68/s)  LR: 2.140e-04  Data: 0.011 (0.015)
Train: 210 [ 500/1251 ( 40%)]  Loss: 3.488 (3.42)  Time: 0.833s, 1229.86/s  (0.793s, 1291.73/s)  LR: 2.140e-04  Data: 0.010 (0.015)
Train: 210 [ 550/1251 ( 44%)]  Loss: 3.312 (3.41)  Time: 0.780s, 1312.98/s  (0.792s, 1292.34/s)  LR: 2.140e-04  Data: 0.011 (0.015)
Train: 210 [ 600/1251 ( 48%)]  Loss: 3.153 (3.39)  Time: 0.807s, 1269.40/s  (0.792s, 1292.67/s)  LR: 2.140e-04  Data: 0.011 (0.014)
Train: 210 [ 650/1251 ( 52%)]  Loss: 3.507 (3.40)  Time: 0.789s, 1297.28/s  (0.792s, 1292.23/s)  LR: 2.140e-04  Data: 0.011 (0.014)
Train: 210 [ 700/1251 ( 56%)]  Loss: 3.257 (3.39)  Time: 0.780s, 1312.71/s  (0.793s, 1291.05/s)  LR: 2.140e-04  Data: 0.011 (0.014)
Train: 210 [ 750/1251 ( 60%)]  Loss: 3.302 (3.39)  Time: 0.819s, 1249.97/s  (0.793s, 1290.72/s)  LR: 2.140e-04  Data: 0.011 (0.014)
Train: 210 [ 800/1251 ( 64%)]  Loss: 3.245 (3.38)  Time: 0.820s, 1249.47/s  (0.795s, 1288.52/s)  LR: 2.140e-04  Data: 0.012 (0.013)
Train: 210 [ 850/1251 ( 68%)]  Loss: 3.393 (3.38)  Time: 0.779s, 1313.95/s  (0.794s, 1288.96/s)  LR: 2.140e-04  Data: 0.011 (0.013)
Train: 210 [ 900/1251 ( 72%)]  Loss: 3.604 (3.39)  Time: 0.778s, 1315.76/s  (0.795s, 1288.81/s)  LR: 2.140e-04  Data: 0.011 (0.013)
Train: 210 [ 950/1251 ( 76%)]  Loss: 3.411 (3.39)  Time: 0.777s, 1317.09/s  (0.794s, 1289.05/s)  LR: 2.140e-04  Data: 0.012 (0.013)
Train: 210 [1000/1251 ( 80%)]  Loss: 3.433 (3.39)  Time: 0.781s, 1311.16/s  (0.795s, 1288.27/s)  LR: 2.140e-04  Data: 0.011 (0.013)
Train: 210 [1050/1251 ( 84%)]  Loss: 3.109 (3.38)  Time: 0.808s, 1267.05/s  (0.795s, 1287.95/s)  LR: 2.140e-04  Data: 0.011 (0.013)
Train: 210 [1100/1251 ( 88%)]  Loss: 3.723 (3.40)  Time: 0.779s, 1314.68/s  (0.795s, 1287.81/s)  LR: 2.140e-04  Data: 0.011 (0.013)
Train: 210 [1150/1251 ( 92%)]  Loss: 3.470 (3.40)  Time: 0.780s, 1312.49/s  (0.795s, 1287.89/s)  LR: 2.140e-04  Data: 0.011 (0.013)
Train: 210 [1200/1251 ( 96%)]  Loss: 3.352 (3.40)  Time: 0.778s, 1316.25/s  (0.795s, 1288.68/s)  LR: 2.140e-04  Data: 0.011 (0.013)
Train: 210 [1250/1251 (100%)]  Loss: 3.538 (3.40)  Time: 0.768s, 1333.24/s  (0.795s, 1288.21/s)  LR: 2.140e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.571 (1.571)  Loss:  0.7649 (0.7649)  Acc@1: 90.9180 (90.9180)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.172 (0.561)  Loss:  0.8721 (1.3157)  Acc@1: 85.4953 (75.2840)  Acc@5: 96.8160 (92.6100)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-208.pth.tar', 75.34000006347657)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-210.pth.tar', 75.28400003417968)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-209.pth.tar', 75.26399998535156)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-207.pth.tar', 75.1840000390625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-203.pth.tar', 75.15399995361328)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-204.pth.tar', 75.14800006103516)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-200.pth.tar', 75.13199998779297)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-202.pth.tar', 74.98800003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-206.pth.tar', 74.88200003417968)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-201.pth.tar', 74.80199995605469)

Train: 211 [   0/1251 (  0%)]  Loss: 3.646 (3.65)  Time: 2.602s,  393.47/s  (2.602s,  393.47/s)  LR: 2.099e-04  Data: 1.846 (1.846)
Train: 211 [  50/1251 (  4%)]  Loss: 3.717 (3.68)  Time: 0.792s, 1293.70/s  (0.840s, 1218.86/s)  LR: 2.099e-04  Data: 0.012 (0.055)
Train: 211 [ 100/1251 (  8%)]  Loss: 3.271 (3.54)  Time: 0.778s, 1316.91/s  (0.814s, 1257.74/s)  LR: 2.099e-04  Data: 0.012 (0.033)
Train: 211 [ 150/1251 ( 12%)]  Loss: 3.324 (3.49)  Time: 0.779s, 1315.26/s  (0.807s, 1269.20/s)  LR: 2.099e-04  Data: 0.011 (0.026)
Train: 211 [ 200/1251 ( 16%)]  Loss: 3.392 (3.47)  Time: 0.805s, 1271.36/s  (0.801s, 1277.87/s)  LR: 2.099e-04  Data: 0.011 (0.022)
Train: 211 [ 250/1251 ( 20%)]  Loss: 3.707 (3.51)  Time: 0.812s, 1260.68/s  (0.801s, 1278.14/s)  LR: 2.099e-04  Data: 0.012 (0.020)
Train: 211 [ 300/1251 ( 24%)]  Loss: 3.527 (3.51)  Time: 0.821s, 1247.05/s  (0.803s, 1274.70/s)  LR: 2.099e-04  Data: 0.012 (0.019)
Train: 211 [ 350/1251 ( 28%)]  Loss: 3.823 (3.55)  Time: 0.801s, 1279.13/s  (0.803s, 1275.78/s)  LR: 2.099e-04  Data: 0.011 (0.018)
Train: 211 [ 400/1251 ( 32%)]  Loss: 3.595 (3.56)  Time: 0.779s, 1313.93/s  (0.802s, 1276.61/s)  LR: 2.099e-04  Data: 0.011 (0.017)
Train: 211 [ 450/1251 ( 36%)]  Loss: 3.487 (3.55)  Time: 0.820s, 1248.59/s  (0.803s, 1275.30/s)  LR: 2.099e-04  Data: 0.011 (0.016)
Train: 211 [ 500/1251 ( 40%)]  Loss: 3.552 (3.55)  Time: 0.865s, 1183.16/s  (0.803s, 1275.14/s)  LR: 2.099e-04  Data: 0.012 (0.016)
Train: 211 [ 550/1251 ( 44%)]  Loss: 3.650 (3.56)  Time: 0.779s, 1315.26/s  (0.802s, 1277.31/s)  LR: 2.099e-04  Data: 0.011 (0.015)
Train: 211 [ 600/1251 ( 48%)]  Loss: 3.377 (3.54)  Time: 0.782s, 1309.90/s  (0.802s, 1277.46/s)  LR: 2.099e-04  Data: 0.011 (0.015)
Train: 211 [ 650/1251 ( 52%)]  Loss: 3.489 (3.54)  Time: 0.780s, 1313.11/s  (0.801s, 1279.10/s)  LR: 2.099e-04  Data: 0.011 (0.015)
Train: 211 [ 700/1251 ( 56%)]  Loss: 3.632 (3.55)  Time: 0.784s, 1305.72/s  (0.800s, 1279.95/s)  LR: 2.099e-04  Data: 0.011 (0.014)
Train: 211 [ 750/1251 ( 60%)]  Loss: 3.373 (3.54)  Time: 0.778s, 1316.63/s  (0.800s, 1280.47/s)  LR: 2.099e-04  Data: 0.011 (0.014)
Train: 211 [ 800/1251 ( 64%)]  Loss: 3.087 (3.51)  Time: 0.838s, 1222.13/s  (0.799s, 1281.84/s)  LR: 2.099e-04  Data: 0.012 (0.014)
Train: 211 [ 850/1251 ( 68%)]  Loss: 3.234 (3.49)  Time: 0.780s, 1313.34/s  (0.799s, 1282.28/s)  LR: 2.099e-04  Data: 0.011 (0.014)
Train: 211 [ 900/1251 ( 72%)]  Loss: 3.592 (3.50)  Time: 0.781s, 1310.66/s  (0.798s, 1283.59/s)  LR: 2.099e-04  Data: 0.012 (0.014)
Train: 211 [ 950/1251 ( 76%)]  Loss: 3.653 (3.51)  Time: 0.777s, 1318.08/s  (0.798s, 1283.19/s)  LR: 2.099e-04  Data: 0.011 (0.013)
Train: 211 [1000/1251 ( 80%)]  Loss: 3.351 (3.50)  Time: 0.779s, 1315.04/s  (0.797s, 1284.15/s)  LR: 2.099e-04  Data: 0.011 (0.013)
Train: 211 [1050/1251 ( 84%)]  Loss: 3.521 (3.50)  Time: 0.778s, 1315.69/s  (0.797s, 1284.60/s)  LR: 2.099e-04  Data: 0.011 (0.013)
Train: 211 [1100/1251 ( 88%)]  Loss: 3.735 (3.51)  Time: 0.840s, 1218.36/s  (0.798s, 1284.01/s)  LR: 2.099e-04  Data: 0.011 (0.013)
Train: 211 [1150/1251 ( 92%)]  Loss: 3.473 (3.51)  Time: 0.778s, 1316.44/s  (0.797s, 1285.02/s)  LR: 2.099e-04  Data: 0.011 (0.013)
Train: 211 [1200/1251 ( 96%)]  Loss: 3.662 (3.51)  Time: 0.778s, 1315.71/s  (0.796s, 1286.01/s)  LR: 2.099e-04  Data: 0.011 (0.013)
Train: 211 [1250/1251 (100%)]  Loss: 3.747 (3.52)  Time: 0.802s, 1276.24/s  (0.796s, 1285.87/s)  LR: 2.099e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.546 (1.546)  Loss:  0.7524 (0.7524)  Acc@1: 90.5273 (90.5273)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.172 (0.564)  Loss:  0.8131 (1.2794)  Acc@1: 85.8491 (75.4360)  Acc@5: 96.2264 (92.7000)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-211.pth.tar', 75.43600005859375)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-208.pth.tar', 75.34000006347657)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-210.pth.tar', 75.28400003417968)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-209.pth.tar', 75.26399998535156)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-207.pth.tar', 75.1840000390625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-203.pth.tar', 75.15399995361328)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-204.pth.tar', 75.14800006103516)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-200.pth.tar', 75.13199998779297)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-202.pth.tar', 74.98800003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-206.pth.tar', 74.88200003417968)

Train: 212 [   0/1251 (  0%)]  Loss: 3.639 (3.64)  Time: 2.250s,  455.02/s  (2.250s,  455.02/s)  LR: 2.057e-04  Data: 1.515 (1.515)
Train: 212 [  50/1251 (  4%)]  Loss: 3.278 (3.46)  Time: 0.808s, 1267.15/s  (0.826s, 1239.89/s)  LR: 2.057e-04  Data: 0.011 (0.046)
Train: 212 [ 100/1251 (  8%)]  Loss: 3.289 (3.40)  Time: 0.777s, 1318.59/s  (0.816s, 1254.40/s)  LR: 2.057e-04  Data: 0.011 (0.029)
Train: 212 [ 150/1251 ( 12%)]  Loss: 3.686 (3.47)  Time: 0.776s, 1319.41/s  (0.809s, 1265.27/s)  LR: 2.057e-04  Data: 0.012 (0.023)
Train: 212 [ 200/1251 ( 16%)]  Loss: 3.274 (3.43)  Time: 0.778s, 1315.85/s  (0.806s, 1270.02/s)  LR: 2.057e-04  Data: 0.012 (0.020)
Train: 212 [ 250/1251 ( 20%)]  Loss: 3.755 (3.49)  Time: 0.780s, 1313.46/s  (0.803s, 1275.85/s)  LR: 2.057e-04  Data: 0.011 (0.018)
Train: 212 [ 300/1251 ( 24%)]  Loss: 3.247 (3.45)  Time: 0.778s, 1316.62/s  (0.803s, 1274.83/s)  LR: 2.057e-04  Data: 0.011 (0.017)
Train: 212 [ 350/1251 ( 28%)]  Loss: 3.506 (3.46)  Time: 0.778s, 1316.79/s  (0.800s, 1279.25/s)  LR: 2.057e-04  Data: 0.011 (0.016)
Train: 212 [ 400/1251 ( 32%)]  Loss: 3.152 (3.43)  Time: 0.778s, 1315.80/s  (0.799s, 1281.98/s)  LR: 2.057e-04  Data: 0.011 (0.016)
Train: 212 [ 450/1251 ( 36%)]  Loss: 3.615 (3.44)  Time: 0.779s, 1314.41/s  (0.798s, 1283.55/s)  LR: 2.057e-04  Data: 0.011 (0.015)
Train: 212 [ 500/1251 ( 40%)]  Loss: 3.612 (3.46)  Time: 0.779s, 1315.20/s  (0.797s, 1284.28/s)  LR: 2.057e-04  Data: 0.011 (0.015)
Train: 212 [ 550/1251 ( 44%)]  Loss: 3.790 (3.49)  Time: 0.778s, 1316.42/s  (0.797s, 1284.42/s)  LR: 2.057e-04  Data: 0.011 (0.014)
Train: 212 [ 600/1251 ( 48%)]  Loss: 3.294 (3.47)  Time: 0.812s, 1261.06/s  (0.798s, 1283.08/s)  LR: 2.057e-04  Data: 0.011 (0.014)
Train: 212 [ 650/1251 ( 52%)]  Loss: 3.456 (3.47)  Time: 0.779s, 1315.19/s  (0.798s, 1283.14/s)  LR: 2.057e-04  Data: 0.012 (0.014)
Train: 212 [ 700/1251 ( 56%)]  Loss: 3.676 (3.48)  Time: 0.809s, 1265.63/s  (0.798s, 1282.79/s)  LR: 2.057e-04  Data: 0.011 (0.014)
Train: 212 [ 750/1251 ( 60%)]  Loss: 3.292 (3.47)  Time: 0.779s, 1314.42/s  (0.799s, 1281.19/s)  LR: 2.057e-04  Data: 0.011 (0.013)
Train: 212 [ 800/1251 ( 64%)]  Loss: 3.454 (3.47)  Time: 0.778s, 1315.55/s  (0.798s, 1282.66/s)  LR: 2.057e-04  Data: 0.011 (0.013)
Train: 212 [ 850/1251 ( 68%)]  Loss: 3.402 (3.47)  Time: 0.813s, 1260.29/s  (0.798s, 1283.07/s)  LR: 2.057e-04  Data: 0.011 (0.013)
Train: 212 [ 900/1251 ( 72%)]  Loss: 3.103 (3.45)  Time: 0.788s, 1299.95/s  (0.797s, 1284.37/s)  LR: 2.057e-04  Data: 0.011 (0.013)
Train: 212 [ 950/1251 ( 76%)]  Loss: 3.645 (3.46)  Time: 0.810s, 1264.06/s  (0.797s, 1284.32/s)  LR: 2.057e-04  Data: 0.011 (0.013)
Train: 212 [1000/1251 ( 80%)]  Loss: 3.830 (3.48)  Time: 0.779s, 1314.03/s  (0.797s, 1284.80/s)  LR: 2.057e-04  Data: 0.011 (0.013)
Train: 212 [1050/1251 ( 84%)]  Loss: 3.800 (3.49)  Time: 0.777s, 1317.82/s  (0.797s, 1285.04/s)  LR: 2.057e-04  Data: 0.011 (0.013)
Train: 212 [1100/1251 ( 88%)]  Loss: 3.563 (3.49)  Time: 0.872s, 1174.42/s  (0.797s, 1284.85/s)  LR: 2.057e-04  Data: 0.011 (0.013)
Train: 212 [1150/1251 ( 92%)]  Loss: 3.348 (3.49)  Time: 0.779s, 1313.77/s  (0.796s, 1285.83/s)  LR: 2.057e-04  Data: 0.011 (0.013)
Train: 212 [1200/1251 ( 96%)]  Loss: 3.628 (3.49)  Time: 0.782s, 1310.23/s  (0.797s, 1285.61/s)  LR: 2.057e-04  Data: 0.011 (0.013)
Train: 212 [1250/1251 (100%)]  Loss: 3.464 (3.49)  Time: 0.799s, 1281.98/s  (0.797s, 1285.29/s)  LR: 2.057e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.607 (1.607)  Loss:  0.8373 (0.8373)  Acc@1: 90.2344 (90.2344)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.172 (0.568)  Loss:  0.8671 (1.3620)  Acc@1: 85.8491 (75.3440)  Acc@5: 96.9340 (92.5100)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-211.pth.tar', 75.43600005859375)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-212.pth.tar', 75.34400005859375)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-208.pth.tar', 75.34000006347657)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-210.pth.tar', 75.28400003417968)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-209.pth.tar', 75.26399998535156)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-207.pth.tar', 75.1840000390625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-203.pth.tar', 75.15399995361328)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-204.pth.tar', 75.14800006103516)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-200.pth.tar', 75.13199998779297)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-202.pth.tar', 74.98800003417969)

Train: 213 [   0/1251 (  0%)]  Loss: 3.248 (3.25)  Time: 2.267s,  451.79/s  (2.267s,  451.79/s)  LR: 2.016e-04  Data: 1.530 (1.530)
Train: 213 [  50/1251 (  4%)]  Loss: 3.431 (3.34)  Time: 0.779s, 1314.93/s  (0.821s, 1247.72/s)  LR: 2.016e-04  Data: 0.011 (0.045)
Train: 213 [ 100/1251 (  8%)]  Loss: 3.531 (3.40)  Time: 0.809s, 1265.02/s  (0.805s, 1271.79/s)  LR: 2.016e-04  Data: 0.011 (0.028)
Train: 213 [ 150/1251 ( 12%)]  Loss: 3.685 (3.47)  Time: 0.835s, 1226.32/s  (0.803s, 1275.20/s)  LR: 2.016e-04  Data: 0.011 (0.023)
Train: 213 [ 200/1251 ( 16%)]  Loss: 3.903 (3.56)  Time: 0.815s, 1255.75/s  (0.802s, 1276.37/s)  LR: 2.016e-04  Data: 0.011 (0.020)
Train: 213 [ 250/1251 ( 20%)]  Loss: 3.383 (3.53)  Time: 0.778s, 1316.36/s  (0.800s, 1279.98/s)  LR: 2.016e-04  Data: 0.011 (0.018)
Train: 213 [ 300/1251 ( 24%)]  Loss: 3.491 (3.52)  Time: 0.775s, 1320.65/s  (0.799s, 1282.17/s)  LR: 2.016e-04  Data: 0.012 (0.017)
Train: 213 [ 350/1251 ( 28%)]  Loss: 3.357 (3.50)  Time: 0.779s, 1313.74/s  (0.798s, 1282.76/s)  LR: 2.016e-04  Data: 0.011 (0.016)
Train: 213 [ 400/1251 ( 32%)]  Loss: 3.405 (3.49)  Time: 0.776s, 1319.72/s  (0.797s, 1284.47/s)  LR: 2.016e-04  Data: 0.011 (0.015)
Train: 213 [ 450/1251 ( 36%)]  Loss: 3.607 (3.50)  Time: 0.815s, 1257.00/s  (0.798s, 1283.70/s)  LR: 2.016e-04  Data: 0.011 (0.015)
Train: 213 [ 500/1251 ( 40%)]  Loss: 3.536 (3.51)  Time: 0.780s, 1313.45/s  (0.796s, 1285.64/s)  LR: 2.016e-04  Data: 0.011 (0.015)
Train: 213 [ 550/1251 ( 44%)]  Loss: 3.335 (3.49)  Time: 0.870s, 1176.63/s  (0.797s, 1284.03/s)  LR: 2.016e-04  Data: 0.011 (0.014)
Train: 213 [ 600/1251 ( 48%)]  Loss: 3.704 (3.51)  Time: 0.778s, 1315.80/s  (0.797s, 1284.38/s)  LR: 2.016e-04  Data: 0.011 (0.014)
Train: 213 [ 650/1251 ( 52%)]  Loss: 3.311 (3.49)  Time: 0.820s, 1249.08/s  (0.797s, 1284.02/s)  LR: 2.016e-04  Data: 0.012 (0.014)
Train: 213 [ 700/1251 ( 56%)]  Loss: 3.313 (3.48)  Time: 0.783s, 1307.96/s  (0.798s, 1283.08/s)  LR: 2.016e-04  Data: 0.011 (0.014)
Train: 213 [ 750/1251 ( 60%)]  Loss: 3.574 (3.49)  Time: 0.811s, 1263.10/s  (0.798s, 1283.99/s)  LR: 2.016e-04  Data: 0.011 (0.013)
Train: 213 [ 800/1251 ( 64%)]  Loss: 3.849 (3.51)  Time: 0.813s, 1259.59/s  (0.798s, 1283.66/s)  LR: 2.016e-04  Data: 0.012 (0.013)
Train: 213 [ 850/1251 ( 68%)]  Loss: 3.405 (3.50)  Time: 0.832s, 1230.60/s  (0.797s, 1284.12/s)  LR: 2.016e-04  Data: 0.011 (0.013)
Train: 213 [ 900/1251 ( 72%)]  Loss: 3.582 (3.51)  Time: 0.818s, 1252.00/s  (0.797s, 1284.92/s)  LR: 2.016e-04  Data: 0.011 (0.013)
Train: 213 [ 950/1251 ( 76%)]  Loss: 3.876 (3.53)  Time: 0.783s, 1307.73/s  (0.796s, 1285.79/s)  LR: 2.016e-04  Data: 0.011 (0.013)
Train: 213 [1000/1251 ( 80%)]  Loss: 3.384 (3.52)  Time: 0.818s, 1252.41/s  (0.796s, 1286.21/s)  LR: 2.016e-04  Data: 0.011 (0.013)
Train: 213 [1050/1251 ( 84%)]  Loss: 3.502 (3.52)  Time: 0.789s, 1297.47/s  (0.797s, 1284.82/s)  LR: 2.016e-04  Data: 0.013 (0.013)
Train: 213 [1100/1251 ( 88%)]  Loss: 3.637 (3.52)  Time: 0.775s, 1320.64/s  (0.797s, 1285.01/s)  LR: 2.016e-04  Data: 0.011 (0.013)
Train: 213 [1150/1251 ( 92%)]  Loss: 3.207 (3.51)  Time: 0.784s, 1306.15/s  (0.796s, 1285.72/s)  LR: 2.016e-04  Data: 0.011 (0.013)
Train: 213 [1200/1251 ( 96%)]  Loss: 3.634 (3.52)  Time: 0.801s, 1278.38/s  (0.796s, 1286.31/s)  LR: 2.016e-04  Data: 0.011 (0.013)
Train: 213 [1250/1251 (100%)]  Loss: 3.671 (3.52)  Time: 0.771s, 1328.26/s  (0.797s, 1285.59/s)  LR: 2.016e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.531 (1.531)  Loss:  0.7823 (0.7823)  Acc@1: 90.2344 (90.2344)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.172 (0.566)  Loss:  0.9087 (1.3800)  Acc@1: 85.3774 (75.6380)  Acc@5: 96.9340 (92.7420)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-213.pth.tar', 75.63799998291016)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-211.pth.tar', 75.43600005859375)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-212.pth.tar', 75.34400005859375)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-208.pth.tar', 75.34000006347657)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-210.pth.tar', 75.28400003417968)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-209.pth.tar', 75.26399998535156)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-207.pth.tar', 75.1840000390625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-203.pth.tar', 75.15399995361328)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-204.pth.tar', 75.14800006103516)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-200.pth.tar', 75.13199998779297)

Train: 214 [   0/1251 (  0%)]  Loss: 3.341 (3.34)  Time: 2.333s,  438.89/s  (2.333s,  438.89/s)  LR: 1.975e-04  Data: 1.599 (1.599)
Train: 214 [  50/1251 (  4%)]  Loss: 2.995 (3.17)  Time: 0.778s, 1316.59/s  (0.819s, 1250.94/s)  LR: 1.975e-04  Data: 0.011 (0.046)
Train: 214 [ 100/1251 (  8%)]  Loss: 3.396 (3.24)  Time: 0.778s, 1316.62/s  (0.806s, 1270.07/s)  LR: 1.975e-04  Data: 0.011 (0.029)
Train: 214 [ 150/1251 ( 12%)]  Loss: 3.387 (3.28)  Time: 0.814s, 1257.67/s  (0.805s, 1272.63/s)  LR: 1.975e-04  Data: 0.012 (0.023)
Train: 214 [ 200/1251 ( 16%)]  Loss: 3.101 (3.24)  Time: 0.778s, 1316.37/s  (0.801s, 1277.73/s)  LR: 1.975e-04  Data: 0.011 (0.020)
Train: 214 [ 250/1251 ( 20%)]  Loss: 3.179 (3.23)  Time: 0.814s, 1258.44/s  (0.799s, 1281.89/s)  LR: 1.975e-04  Data: 0.011 (0.018)
Train: 214 [ 300/1251 ( 24%)]  Loss: 3.457 (3.27)  Time: 0.778s, 1316.50/s  (0.798s, 1282.49/s)  LR: 1.975e-04  Data: 0.010 (0.017)
Train: 214 [ 350/1251 ( 28%)]  Loss: 3.369 (3.28)  Time: 0.828s, 1236.98/s  (0.797s, 1285.10/s)  LR: 1.975e-04  Data: 0.010 (0.016)
Train: 214 [ 400/1251 ( 32%)]  Loss: 3.606 (3.31)  Time: 0.821s, 1247.77/s  (0.796s, 1286.15/s)  LR: 1.975e-04  Data: 0.011 (0.016)
Train: 214 [ 450/1251 ( 36%)]  Loss: 3.365 (3.32)  Time: 0.780s, 1313.20/s  (0.798s, 1282.76/s)  LR: 1.975e-04  Data: 0.011 (0.015)
Train: 214 [ 500/1251 ( 40%)]  Loss: 3.629 (3.35)  Time: 0.777s, 1317.86/s  (0.798s, 1283.68/s)  LR: 1.975e-04  Data: 0.011 (0.015)
Train: 214 [ 550/1251 ( 44%)]  Loss: 3.212 (3.34)  Time: 0.779s, 1313.90/s  (0.797s, 1284.17/s)  LR: 1.975e-04  Data: 0.011 (0.014)
Train: 214 [ 600/1251 ( 48%)]  Loss: 3.340 (3.34)  Time: 0.779s, 1313.78/s  (0.797s, 1285.52/s)  LR: 1.975e-04  Data: 0.011 (0.014)
Train: 214 [ 650/1251 ( 52%)]  Loss: 3.415 (3.34)  Time: 0.837s, 1222.70/s  (0.796s, 1286.48/s)  LR: 1.975e-04  Data: 0.011 (0.014)
Train: 214 [ 700/1251 ( 56%)]  Loss: 3.650 (3.36)  Time: 0.812s, 1261.73/s  (0.796s, 1286.37/s)  LR: 1.975e-04  Data: 0.011 (0.014)
Train: 214 [ 750/1251 ( 60%)]  Loss: 3.211 (3.35)  Time: 0.826s, 1240.22/s  (0.796s, 1285.88/s)  LR: 1.975e-04  Data: 0.011 (0.013)
Train: 214 [ 800/1251 ( 64%)]  Loss: 3.599 (3.37)  Time: 0.778s, 1316.48/s  (0.797s, 1285.61/s)  LR: 1.975e-04  Data: 0.011 (0.013)
Train: 214 [ 850/1251 ( 68%)]  Loss: 3.356 (3.37)  Time: 0.792s, 1292.77/s  (0.796s, 1286.51/s)  LR: 1.975e-04  Data: 0.011 (0.013)
Train: 214 [ 900/1251 ( 72%)]  Loss: 3.262 (3.36)  Time: 0.820s, 1248.04/s  (0.797s, 1285.53/s)  LR: 1.975e-04  Data: 0.011 (0.013)
Train: 214 [ 950/1251 ( 76%)]  Loss: 3.734 (3.38)  Time: 0.790s, 1296.35/s  (0.796s, 1286.25/s)  LR: 1.975e-04  Data: 0.012 (0.013)
Train: 214 [1000/1251 ( 80%)]  Loss: 3.670 (3.39)  Time: 0.777s, 1317.70/s  (0.796s, 1286.73/s)  LR: 1.975e-04  Data: 0.012 (0.013)
Train: 214 [1050/1251 ( 84%)]  Loss: 3.644 (3.41)  Time: 0.779s, 1313.75/s  (0.795s, 1287.89/s)  LR: 1.975e-04  Data: 0.011 (0.013)
Train: 214 [1100/1251 ( 88%)]  Loss: 3.262 (3.40)  Time: 0.823s, 1244.25/s  (0.795s, 1287.91/s)  LR: 1.975e-04  Data: 0.011 (0.013)
Train: 214 [1150/1251 ( 92%)]  Loss: 3.415 (3.40)  Time: 0.778s, 1316.36/s  (0.795s, 1288.45/s)  LR: 1.975e-04  Data: 0.011 (0.013)
Train: 214 [1200/1251 ( 96%)]  Loss: 3.803 (3.42)  Time: 0.778s, 1316.19/s  (0.795s, 1287.63/s)  LR: 1.975e-04  Data: 0.011 (0.013)
Train: 214 [1250/1251 (100%)]  Loss: 3.430 (3.42)  Time: 0.767s, 1335.69/s  (0.796s, 1287.09/s)  LR: 1.975e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.540 (1.540)  Loss:  0.8107 (0.8107)  Acc@1: 90.5273 (90.5273)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.172 (0.578)  Loss:  0.8647 (1.3646)  Acc@1: 85.9670 (75.8600)  Acc@5: 97.2877 (92.9860)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-214.pth.tar', 75.86000010986328)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-213.pth.tar', 75.63799998291016)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-211.pth.tar', 75.43600005859375)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-212.pth.tar', 75.34400005859375)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-208.pth.tar', 75.34000006347657)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-210.pth.tar', 75.28400003417968)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-209.pth.tar', 75.26399998535156)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-207.pth.tar', 75.1840000390625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-203.pth.tar', 75.15399995361328)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-204.pth.tar', 75.14800006103516)

Train: 215 [   0/1251 (  0%)]  Loss: 3.376 (3.38)  Time: 2.335s,  438.57/s  (2.335s,  438.57/s)  LR: 1.935e-04  Data: 1.599 (1.599)
Train: 215 [  50/1251 (  4%)]  Loss: 3.751 (3.56)  Time: 0.778s, 1316.40/s  (0.824s, 1242.54/s)  LR: 1.935e-04  Data: 0.011 (0.042)
Train: 215 [ 100/1251 (  8%)]  Loss: 3.820 (3.65)  Time: 0.812s, 1260.43/s  (0.812s, 1261.31/s)  LR: 1.935e-04  Data: 0.011 (0.027)
Train: 215 [ 150/1251 ( 12%)]  Loss: 3.289 (3.56)  Time: 0.780s, 1313.28/s  (0.807s, 1269.53/s)  LR: 1.935e-04  Data: 0.011 (0.022)
Train: 215 [ 200/1251 ( 16%)]  Loss: 3.556 (3.56)  Time: 0.797s, 1284.27/s  (0.803s, 1275.42/s)  LR: 1.935e-04  Data: 0.011 (0.019)
Train: 215 [ 250/1251 ( 20%)]  Loss: 3.184 (3.50)  Time: 0.779s, 1314.28/s  (0.801s, 1277.70/s)  LR: 1.935e-04  Data: 0.011 (0.017)
Train: 215 [ 300/1251 ( 24%)]  Loss: 3.809 (3.54)  Time: 0.813s, 1259.74/s  (0.800s, 1280.69/s)  LR: 1.935e-04  Data: 0.010 (0.016)
Train: 215 [ 350/1251 ( 28%)]  Loss: 3.461 (3.53)  Time: 0.776s, 1319.61/s  (0.801s, 1278.75/s)  LR: 1.935e-04  Data: 0.011 (0.016)
Train: 215 [ 400/1251 ( 32%)]  Loss: 3.558 (3.53)  Time: 0.815s, 1256.80/s  (0.801s, 1277.79/s)  LR: 1.935e-04  Data: 0.011 (0.015)
Train: 215 [ 450/1251 ( 36%)]  Loss: 3.897 (3.57)  Time: 0.834s, 1227.73/s  (0.801s, 1278.59/s)  LR: 1.935e-04  Data: 0.011 (0.015)
Train: 215 [ 500/1251 ( 40%)]  Loss: 3.638 (3.58)  Time: 0.852s, 1201.68/s  (0.799s, 1281.42/s)  LR: 1.935e-04  Data: 0.011 (0.014)
Train: 215 [ 550/1251 ( 44%)]  Loss: 3.326 (3.56)  Time: 0.776s, 1319.29/s  (0.799s, 1281.70/s)  LR: 1.935e-04  Data: 0.012 (0.014)
Train: 215 [ 600/1251 ( 48%)]  Loss: 3.167 (3.53)  Time: 0.776s, 1319.12/s  (0.800s, 1280.60/s)  LR: 1.935e-04  Data: 0.011 (0.014)
Train: 215 [ 650/1251 ( 52%)]  Loss: 3.857 (3.55)  Time: 0.778s, 1316.89/s  (0.799s, 1282.18/s)  LR: 1.935e-04  Data: 0.011 (0.013)
Train: 215 [ 700/1251 ( 56%)]  Loss: 3.470 (3.54)  Time: 0.833s, 1229.44/s  (0.799s, 1282.30/s)  LR: 1.935e-04  Data: 0.011 (0.013)
Train: 215 [ 750/1251 ( 60%)]  Loss: 3.654 (3.55)  Time: 0.779s, 1313.82/s  (0.799s, 1281.86/s)  LR: 1.935e-04  Data: 0.011 (0.013)
Train: 215 [ 800/1251 ( 64%)]  Loss: 3.495 (3.55)  Time: 0.782s, 1309.44/s  (0.798s, 1282.73/s)  LR: 1.935e-04  Data: 0.011 (0.013)
Train: 215 [ 850/1251 ( 68%)]  Loss: 3.077 (3.52)  Time: 0.812s, 1260.47/s  (0.798s, 1283.27/s)  LR: 1.935e-04  Data: 0.011 (0.013)
Train: 215 [ 900/1251 ( 72%)]  Loss: 3.408 (3.52)  Time: 0.858s, 1193.86/s  (0.798s, 1283.04/s)  LR: 1.935e-04  Data: 0.011 (0.013)
Train: 215 [ 950/1251 ( 76%)]  Loss: 3.693 (3.52)  Time: 0.780s, 1312.84/s  (0.798s, 1282.84/s)  LR: 1.935e-04  Data: 0.011 (0.013)
Train: 215 [1000/1251 ( 80%)]  Loss: 3.616 (3.53)  Time: 0.779s, 1314.93/s  (0.798s, 1283.10/s)  LR: 1.935e-04  Data: 0.011 (0.013)
Train: 215 [1050/1251 ( 84%)]  Loss: 3.427 (3.52)  Time: 0.856s, 1196.31/s  (0.798s, 1283.97/s)  LR: 1.935e-04  Data: 0.010 (0.013)
Train: 215 [1100/1251 ( 88%)]  Loss: 3.286 (3.51)  Time: 0.779s, 1315.05/s  (0.797s, 1284.16/s)  LR: 1.935e-04  Data: 0.011 (0.013)
Train: 215 [1150/1251 ( 92%)]  Loss: 3.400 (3.51)  Time: 0.811s, 1262.37/s  (0.797s, 1284.75/s)  LR: 1.935e-04  Data: 0.011 (0.012)
Train: 215 [1200/1251 ( 96%)]  Loss: 3.489 (3.51)  Time: 0.780s, 1312.87/s  (0.797s, 1284.23/s)  LR: 1.935e-04  Data: 0.011 (0.012)
Train: 215 [1250/1251 (100%)]  Loss: 3.393 (3.50)  Time: 0.766s, 1336.12/s  (0.797s, 1284.26/s)  LR: 1.935e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.505 (1.505)  Loss:  0.7551 (0.7551)  Acc@1: 90.0391 (90.0391)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.172 (0.562)  Loss:  0.8357 (1.3282)  Acc@1: 85.1415 (75.6640)  Acc@5: 97.4057 (92.7000)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-214.pth.tar', 75.86000010986328)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-215.pth.tar', 75.66400013916015)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-213.pth.tar', 75.63799998291016)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-211.pth.tar', 75.43600005859375)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-212.pth.tar', 75.34400005859375)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-208.pth.tar', 75.34000006347657)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-210.pth.tar', 75.28400003417968)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-209.pth.tar', 75.26399998535156)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-207.pth.tar', 75.1840000390625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-203.pth.tar', 75.15399995361328)

Train: 216 [   0/1251 (  0%)]  Loss: 3.297 (3.30)  Time: 2.350s,  435.79/s  (2.350s,  435.79/s)  LR: 1.895e-04  Data: 1.614 (1.614)
Train: 216 [  50/1251 (  4%)]  Loss: 3.707 (3.50)  Time: 0.780s, 1312.27/s  (0.833s, 1228.97/s)  LR: 1.895e-04  Data: 0.011 (0.052)
Train: 216 [ 100/1251 (  8%)]  Loss: 3.452 (3.49)  Time: 0.814s, 1257.95/s  (0.820s, 1248.86/s)  LR: 1.895e-04  Data: 0.011 (0.032)
Train: 216 [ 150/1251 ( 12%)]  Loss: 3.729 (3.55)  Time: 0.813s, 1258.92/s  (0.812s, 1261.13/s)  LR: 1.895e-04  Data: 0.011 (0.025)
Train: 216 [ 200/1251 ( 16%)]  Loss: 3.451 (3.53)  Time: 0.786s, 1301.98/s  (0.805s, 1272.13/s)  LR: 1.895e-04  Data: 0.012 (0.022)
Train: 216 [ 250/1251 ( 20%)]  Loss: 3.919 (3.59)  Time: 0.816s, 1254.80/s  (0.805s, 1272.11/s)  LR: 1.895e-04  Data: 0.011 (0.020)
Train: 216 [ 300/1251 ( 24%)]  Loss: 3.855 (3.63)  Time: 0.776s, 1319.02/s  (0.804s, 1273.70/s)  LR: 1.895e-04  Data: 0.011 (0.018)
Train: 216 [ 350/1251 ( 28%)]  Loss: 3.795 (3.65)  Time: 0.821s, 1247.11/s  (0.805s, 1272.67/s)  LR: 1.895e-04  Data: 0.011 (0.017)
Train: 216 [ 400/1251 ( 32%)]  Loss: 3.395 (3.62)  Time: 0.778s, 1315.94/s  (0.805s, 1272.64/s)  LR: 1.895e-04  Data: 0.011 (0.016)
Train: 216 [ 450/1251 ( 36%)]  Loss: 3.397 (3.60)  Time: 0.817s, 1253.54/s  (0.804s, 1273.69/s)  LR: 1.895e-04  Data: 0.012 (0.016)
Train: 216 [ 500/1251 ( 40%)]  Loss: 3.150 (3.56)  Time: 0.779s, 1314.26/s  (0.805s, 1272.10/s)  LR: 1.895e-04  Data: 0.012 (0.015)
Train: 216 [ 550/1251 ( 44%)]  Loss: 3.736 (3.57)  Time: 0.777s, 1318.15/s  (0.803s, 1275.58/s)  LR: 1.895e-04  Data: 0.011 (0.015)
Train: 216 [ 600/1251 ( 48%)]  Loss: 3.718 (3.58)  Time: 0.779s, 1313.93/s  (0.802s, 1276.85/s)  LR: 1.895e-04  Data: 0.012 (0.015)
Train: 216 [ 650/1251 ( 52%)]  Loss: 3.255 (3.56)  Time: 0.823s, 1244.23/s  (0.802s, 1277.17/s)  LR: 1.895e-04  Data: 0.011 (0.014)
Train: 216 [ 700/1251 ( 56%)]  Loss: 3.593 (3.56)  Time: 0.817s, 1253.70/s  (0.803s, 1275.64/s)  LR: 1.895e-04  Data: 0.011 (0.014)
Train: 216 [ 750/1251 ( 60%)]  Loss: 3.538 (3.56)  Time: 0.777s, 1317.86/s  (0.803s, 1276.00/s)  LR: 1.895e-04  Data: 0.011 (0.014)
Train: 216 [ 800/1251 ( 64%)]  Loss: 3.670 (3.57)  Time: 0.778s, 1315.50/s  (0.802s, 1276.56/s)  LR: 1.895e-04  Data: 0.011 (0.014)
Train: 216 [ 850/1251 ( 68%)]  Loss: 3.300 (3.55)  Time: 0.779s, 1315.35/s  (0.801s, 1278.21/s)  LR: 1.895e-04  Data: 0.012 (0.014)
Train: 216 [ 900/1251 ( 72%)]  Loss: 3.458 (3.55)  Time: 0.821s, 1247.76/s  (0.801s, 1278.65/s)  LR: 1.895e-04  Data: 0.011 (0.014)
Train: 216 [ 950/1251 ( 76%)]  Loss: 3.440 (3.54)  Time: 0.784s, 1305.62/s  (0.801s, 1278.17/s)  LR: 1.895e-04  Data: 0.011 (0.013)
Train: 216 [1000/1251 ( 80%)]  Loss: 3.093 (3.52)  Time: 0.821s, 1246.61/s  (0.801s, 1277.93/s)  LR: 1.895e-04  Data: 0.012 (0.013)
Train: 216 [1050/1251 ( 84%)]  Loss: 3.658 (3.53)  Time: 0.813s, 1259.47/s  (0.801s, 1278.53/s)  LR: 1.895e-04  Data: 0.011 (0.013)
Train: 216 [1100/1251 ( 88%)]  Loss: 3.178 (3.51)  Time: 0.779s, 1315.31/s  (0.800s, 1279.34/s)  LR: 1.895e-04  Data: 0.011 (0.013)
Train: 216 [1150/1251 ( 92%)]  Loss: 3.564 (3.51)  Time: 0.778s, 1316.76/s  (0.800s, 1279.87/s)  LR: 1.895e-04  Data: 0.011 (0.013)
Train: 216 [1200/1251 ( 96%)]  Loss: 3.322 (3.51)  Time: 0.812s, 1260.51/s  (0.800s, 1280.06/s)  LR: 1.895e-04  Data: 0.010 (0.013)
Train: 216 [1250/1251 (100%)]  Loss: 3.439 (3.50)  Time: 0.803s, 1275.30/s  (0.800s, 1280.73/s)  LR: 1.895e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.541 (1.541)  Loss:  0.8215 (0.8215)  Acc@1: 90.1367 (90.1367)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.172 (0.559)  Loss:  0.9181 (1.3633)  Acc@1: 85.3774 (75.4820)  Acc@5: 96.6981 (92.7680)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-214.pth.tar', 75.86000010986328)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-215.pth.tar', 75.66400013916015)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-213.pth.tar', 75.63799998291016)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-216.pth.tar', 75.48200011230469)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-211.pth.tar', 75.43600005859375)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-212.pth.tar', 75.34400005859375)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-208.pth.tar', 75.34000006347657)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-210.pth.tar', 75.28400003417968)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-209.pth.tar', 75.26399998535156)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-207.pth.tar', 75.1840000390625)

Train: 217 [   0/1251 (  0%)]  Loss: 3.675 (3.67)  Time: 2.294s,  446.38/s  (2.294s,  446.38/s)  LR: 1.855e-04  Data: 1.559 (1.559)
Train: 217 [  50/1251 (  4%)]  Loss: 3.380 (3.53)  Time: 0.784s, 1305.61/s  (0.826s, 1239.43/s)  LR: 1.855e-04  Data: 0.012 (0.047)
Train: 217 [ 100/1251 (  8%)]  Loss: 3.381 (3.48)  Time: 0.779s, 1314.83/s  (0.808s, 1267.48/s)  LR: 1.855e-04  Data: 0.011 (0.029)
Train: 217 [ 150/1251 ( 12%)]  Loss: 3.039 (3.37)  Time: 0.809s, 1266.23/s  (0.801s, 1278.01/s)  LR: 1.855e-04  Data: 0.011 (0.023)
Train: 217 [ 200/1251 ( 16%)]  Loss: 3.603 (3.42)  Time: 0.778s, 1316.82/s  (0.800s, 1280.70/s)  LR: 1.855e-04  Data: 0.011 (0.020)
Train: 217 [ 250/1251 ( 20%)]  Loss: 3.671 (3.46)  Time: 0.779s, 1315.31/s  (0.800s, 1280.73/s)  LR: 1.855e-04  Data: 0.011 (0.018)
Train: 217 [ 300/1251 ( 24%)]  Loss: 3.389 (3.45)  Time: 0.787s, 1300.75/s  (0.798s, 1282.95/s)  LR: 1.855e-04  Data: 0.011 (0.017)
Train: 217 [ 350/1251 ( 28%)]  Loss: 3.121 (3.41)  Time: 0.800s, 1280.25/s  (0.798s, 1283.11/s)  LR: 1.855e-04  Data: 0.012 (0.016)
Train: 217 [ 400/1251 ( 32%)]  Loss: 3.334 (3.40)  Time: 0.777s, 1317.72/s  (0.797s, 1285.26/s)  LR: 1.855e-04  Data: 0.010 (0.016)
Train: 217 [ 450/1251 ( 36%)]  Loss: 3.551 (3.41)  Time: 0.780s, 1312.38/s  (0.795s, 1287.86/s)  LR: 1.855e-04  Data: 0.012 (0.015)
Train: 217 [ 500/1251 ( 40%)]  Loss: 3.731 (3.44)  Time: 0.818s, 1251.74/s  (0.796s, 1286.34/s)  LR: 1.855e-04  Data: 0.012 (0.015)
Train: 217 [ 550/1251 ( 44%)]  Loss: 3.586 (3.45)  Time: 0.823s, 1244.23/s  (0.799s, 1281.15/s)  LR: 1.855e-04  Data: 0.012 (0.015)
Train: 217 [ 600/1251 ( 48%)]  Loss: 3.820 (3.48)  Time: 0.779s, 1315.28/s  (0.798s, 1282.92/s)  LR: 1.855e-04  Data: 0.011 (0.014)
Train: 217 [ 650/1251 ( 52%)]  Loss: 3.230 (3.46)  Time: 0.829s, 1235.58/s  (0.798s, 1283.14/s)  LR: 1.855e-04  Data: 0.011 (0.014)
Train: 217 [ 700/1251 ( 56%)]  Loss: 3.485 (3.47)  Time: 0.781s, 1310.43/s  (0.797s, 1284.81/s)  LR: 1.855e-04  Data: 0.011 (0.014)
Train: 217 [ 750/1251 ( 60%)]  Loss: 3.696 (3.48)  Time: 0.781s, 1311.93/s  (0.796s, 1286.12/s)  LR: 1.855e-04  Data: 0.011 (0.014)
Train: 217 [ 800/1251 ( 64%)]  Loss: 3.286 (3.47)  Time: 0.780s, 1313.21/s  (0.795s, 1287.39/s)  LR: 1.855e-04  Data: 0.011 (0.014)
Train: 217 [ 850/1251 ( 68%)]  Loss: 3.388 (3.46)  Time: 0.812s, 1261.46/s  (0.795s, 1288.19/s)  LR: 1.855e-04  Data: 0.011 (0.013)
Train: 217 [ 900/1251 ( 72%)]  Loss: 3.156 (3.45)  Time: 0.824s, 1242.12/s  (0.796s, 1286.63/s)  LR: 1.855e-04  Data: 0.012 (0.013)
Train: 217 [ 950/1251 ( 76%)]  Loss: 3.630 (3.46)  Time: 0.870s, 1177.32/s  (0.797s, 1285.53/s)  LR: 1.855e-04  Data: 0.010 (0.013)
Train: 217 [1000/1251 ( 80%)]  Loss: 3.603 (3.46)  Time: 0.780s, 1312.24/s  (0.797s, 1284.95/s)  LR: 1.855e-04  Data: 0.011 (0.013)
Train: 217 [1050/1251 ( 84%)]  Loss: 3.720 (3.48)  Time: 0.843s, 1214.16/s  (0.797s, 1284.72/s)  LR: 1.855e-04  Data: 0.011 (0.013)
Train: 217 [1100/1251 ( 88%)]  Loss: 3.399 (3.47)  Time: 0.777s, 1318.66/s  (0.797s, 1285.17/s)  LR: 1.855e-04  Data: 0.011 (0.013)
Train: 217 [1150/1251 ( 92%)]  Loss: 3.417 (3.47)  Time: 0.778s, 1316.31/s  (0.796s, 1285.83/s)  LR: 1.855e-04  Data: 0.011 (0.013)
Train: 217 [1200/1251 ( 96%)]  Loss: 3.479 (3.47)  Time: 0.777s, 1317.65/s  (0.796s, 1286.71/s)  LR: 1.855e-04  Data: 0.011 (0.013)
Train: 217 [1250/1251 (100%)]  Loss: 3.752 (3.48)  Time: 0.768s, 1333.65/s  (0.795s, 1287.47/s)  LR: 1.855e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.517 (1.517)  Loss:  0.8040 (0.8040)  Acc@1: 90.0391 (90.0391)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.172 (0.563)  Loss:  0.8225 (1.3198)  Acc@1: 86.6745 (75.7660)  Acc@5: 96.9340 (92.8300)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-214.pth.tar', 75.86000010986328)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-217.pth.tar', 75.76600002929688)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-215.pth.tar', 75.66400013916015)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-213.pth.tar', 75.63799998291016)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-216.pth.tar', 75.48200011230469)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-211.pth.tar', 75.43600005859375)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-212.pth.tar', 75.34400005859375)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-208.pth.tar', 75.34000006347657)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-210.pth.tar', 75.28400003417968)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-209.pth.tar', 75.26399998535156)

Train: 218 [   0/1251 (  0%)]  Loss: 3.382 (3.38)  Time: 2.257s,  453.67/s  (2.257s,  453.67/s)  LR: 1.816e-04  Data: 1.500 (1.500)
Train: 218 [  50/1251 (  4%)]  Loss: 3.346 (3.36)  Time: 0.812s, 1260.70/s  (0.822s, 1246.11/s)  LR: 1.816e-04  Data: 0.011 (0.044)
Train: 218 [ 100/1251 (  8%)]  Loss: 3.080 (3.27)  Time: 0.813s, 1259.09/s  (0.812s, 1260.32/s)  LR: 1.816e-04  Data: 0.012 (0.028)
Train: 218 [ 150/1251 ( 12%)]  Loss: 3.493 (3.33)  Time: 0.777s, 1317.74/s  (0.808s, 1267.54/s)  LR: 1.816e-04  Data: 0.011 (0.022)
Train: 218 [ 200/1251 ( 16%)]  Loss: 3.458 (3.35)  Time: 0.781s, 1310.54/s  (0.806s, 1269.94/s)  LR: 1.816e-04  Data: 0.011 (0.019)
Train: 218 [ 250/1251 ( 20%)]  Loss: 3.585 (3.39)  Time: 0.780s, 1313.66/s  (0.805s, 1272.49/s)  LR: 1.816e-04  Data: 0.011 (0.018)
Train: 218 [ 300/1251 ( 24%)]  Loss: 3.741 (3.44)  Time: 0.778s, 1316.69/s  (0.802s, 1277.28/s)  LR: 1.816e-04  Data: 0.011 (0.017)
Train: 218 [ 350/1251 ( 28%)]  Loss: 3.315 (3.43)  Time: 0.783s, 1308.49/s  (0.799s, 1281.25/s)  LR: 1.816e-04  Data: 0.011 (0.016)
Train: 218 [ 400/1251 ( 32%)]  Loss: 3.139 (3.39)  Time: 0.813s, 1259.35/s  (0.799s, 1281.40/s)  LR: 1.816e-04  Data: 0.011 (0.015)
Train: 218 [ 450/1251 ( 36%)]  Loss: 3.501 (3.40)  Time: 0.788s, 1300.05/s  (0.799s, 1281.38/s)  LR: 1.816e-04  Data: 0.011 (0.015)
Train: 218 [ 500/1251 ( 40%)]  Loss: 3.449 (3.41)  Time: 0.779s, 1314.38/s  (0.798s, 1283.22/s)  LR: 1.816e-04  Data: 0.011 (0.014)
Train: 218 [ 550/1251 ( 44%)]  Loss: 3.468 (3.41)  Time: 0.828s, 1236.81/s  (0.798s, 1283.12/s)  LR: 1.816e-04  Data: 0.011 (0.014)
Train: 218 [ 600/1251 ( 48%)]  Loss: 3.716 (3.44)  Time: 0.787s, 1300.95/s  (0.798s, 1283.81/s)  LR: 1.816e-04  Data: 0.011 (0.014)
Train: 218 [ 650/1251 ( 52%)]  Loss: 3.769 (3.46)  Time: 0.778s, 1316.52/s  (0.797s, 1284.38/s)  LR: 1.816e-04  Data: 0.011 (0.014)
Train: 218 [ 700/1251 ( 56%)]  Loss: 3.673 (3.47)  Time: 0.817s, 1252.61/s  (0.797s, 1284.89/s)  LR: 1.816e-04  Data: 0.011 (0.013)
Train: 218 [ 750/1251 ( 60%)]  Loss: 3.617 (3.48)  Time: 0.776s, 1320.08/s  (0.797s, 1284.85/s)  LR: 1.816e-04  Data: 0.012 (0.013)
Train: 218 [ 800/1251 ( 64%)]  Loss: 3.637 (3.49)  Time: 0.776s, 1319.42/s  (0.797s, 1284.84/s)  LR: 1.816e-04  Data: 0.011 (0.013)
Train: 218 [ 850/1251 ( 68%)]  Loss: 3.322 (3.48)  Time: 0.779s, 1314.71/s  (0.796s, 1285.76/s)  LR: 1.816e-04  Data: 0.011 (0.013)
Train: 218 [ 900/1251 ( 72%)]  Loss: 3.185 (3.47)  Time: 0.779s, 1313.69/s  (0.796s, 1286.80/s)  LR: 1.816e-04  Data: 0.011 (0.013)
Train: 218 [ 950/1251 ( 76%)]  Loss: 3.471 (3.47)  Time: 0.780s, 1313.44/s  (0.796s, 1286.94/s)  LR: 1.816e-04  Data: 0.010 (0.013)
Train: 218 [1000/1251 ( 80%)]  Loss: 3.645 (3.48)  Time: 0.778s, 1316.65/s  (0.796s, 1287.05/s)  LR: 1.816e-04  Data: 0.012 (0.013)
Train: 218 [1050/1251 ( 84%)]  Loss: 2.961 (3.45)  Time: 0.807s, 1268.73/s  (0.796s, 1286.41/s)  LR: 1.816e-04  Data: 0.011 (0.013)
Train: 218 [1100/1251 ( 88%)]  Loss: 3.133 (3.44)  Time: 0.862s, 1187.83/s  (0.796s, 1286.73/s)  LR: 1.816e-04  Data: 0.011 (0.013)
Train: 218 [1150/1251 ( 92%)]  Loss: 2.984 (3.42)  Time: 0.778s, 1316.33/s  (0.796s, 1286.99/s)  LR: 1.816e-04  Data: 0.011 (0.013)
Train: 218 [1200/1251 ( 96%)]  Loss: 3.666 (3.43)  Time: 0.778s, 1315.48/s  (0.795s, 1287.61/s)  LR: 1.816e-04  Data: 0.011 (0.012)
Train: 218 [1250/1251 (100%)]  Loss: 3.316 (3.43)  Time: 0.765s, 1339.05/s  (0.795s, 1288.47/s)  LR: 1.816e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.522 (1.522)  Loss:  0.8952 (0.8952)  Acc@1: 90.1367 (90.1367)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.172 (0.565)  Loss:  0.9181 (1.4238)  Acc@1: 86.4387 (75.7400)  Acc@5: 97.2877 (92.7900)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-214.pth.tar', 75.86000010986328)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-217.pth.tar', 75.76600002929688)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-218.pth.tar', 75.74000005615234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-215.pth.tar', 75.66400013916015)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-213.pth.tar', 75.63799998291016)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-216.pth.tar', 75.48200011230469)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-211.pth.tar', 75.43600005859375)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-212.pth.tar', 75.34400005859375)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-208.pth.tar', 75.34000006347657)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-210.pth.tar', 75.28400003417968)

Train: 219 [   0/1251 (  0%)]  Loss: 3.379 (3.38)  Time: 2.319s,  441.63/s  (2.319s,  441.63/s)  LR: 1.777e-04  Data: 1.583 (1.583)
Train: 219 [  50/1251 (  4%)]  Loss: 3.087 (3.23)  Time: 0.778s, 1316.07/s  (0.818s, 1252.57/s)  LR: 1.777e-04  Data: 0.011 (0.043)
Train: 219 [ 100/1251 (  8%)]  Loss: 3.354 (3.27)  Time: 0.775s, 1322.01/s  (0.808s, 1267.93/s)  LR: 1.777e-04  Data: 0.011 (0.027)
Train: 219 [ 150/1251 ( 12%)]  Loss: 3.592 (3.35)  Time: 0.782s, 1309.42/s  (0.801s, 1277.69/s)  LR: 1.777e-04  Data: 0.011 (0.022)
Train: 219 [ 200/1251 ( 16%)]  Loss: 3.270 (3.34)  Time: 0.785s, 1303.84/s  (0.797s, 1285.30/s)  LR: 1.777e-04  Data: 0.011 (0.019)
Train: 219 [ 250/1251 ( 20%)]  Loss: 3.232 (3.32)  Time: 0.776s, 1318.75/s  (0.797s, 1284.32/s)  LR: 1.777e-04  Data: 0.011 (0.018)
Train: 219 [ 300/1251 ( 24%)]  Loss: 3.584 (3.36)  Time: 0.778s, 1316.33/s  (0.798s, 1283.58/s)  LR: 1.777e-04  Data: 0.011 (0.017)
Train: 219 [ 350/1251 ( 28%)]  Loss: 3.257 (3.34)  Time: 0.779s, 1314.27/s  (0.795s, 1287.27/s)  LR: 1.777e-04  Data: 0.011 (0.016)
Train: 219 [ 400/1251 ( 32%)]  Loss: 3.453 (3.36)  Time: 0.778s, 1315.61/s  (0.794s, 1289.82/s)  LR: 1.777e-04  Data: 0.011 (0.015)
Train: 219 [ 450/1251 ( 36%)]  Loss: 3.836 (3.40)  Time: 0.778s, 1316.10/s  (0.793s, 1292.00/s)  LR: 1.777e-04  Data: 0.011 (0.015)
Train: 219 [ 500/1251 ( 40%)]  Loss: 3.176 (3.38)  Time: 0.835s, 1226.40/s  (0.792s, 1292.23/s)  LR: 1.777e-04  Data: 0.011 (0.014)
Train: 219 [ 550/1251 ( 44%)]  Loss: 3.486 (3.39)  Time: 0.787s, 1300.43/s  (0.792s, 1293.37/s)  LR: 1.777e-04  Data: 0.011 (0.014)
Train: 219 [ 600/1251 ( 48%)]  Loss: 3.323 (3.39)  Time: 0.812s, 1261.43/s  (0.792s, 1293.04/s)  LR: 1.777e-04  Data: 0.011 (0.014)
Train: 219 [ 650/1251 ( 52%)]  Loss: 3.050 (3.36)  Time: 0.779s, 1314.79/s  (0.792s, 1293.30/s)  LR: 1.777e-04  Data: 0.011 (0.014)
Train: 219 [ 700/1251 ( 56%)]  Loss: 3.249 (3.36)  Time: 0.818s, 1251.36/s  (0.792s, 1292.35/s)  LR: 1.777e-04  Data: 0.011 (0.013)
Train: 219 [ 750/1251 ( 60%)]  Loss: 3.604 (3.37)  Time: 0.781s, 1310.83/s  (0.792s, 1293.36/s)  LR: 1.777e-04  Data: 0.011 (0.013)
Train: 219 [ 800/1251 ( 64%)]  Loss: 3.634 (3.39)  Time: 0.776s, 1320.19/s  (0.791s, 1294.39/s)  LR: 1.777e-04  Data: 0.011 (0.013)
Train: 219 [ 850/1251 ( 68%)]  Loss: 3.843 (3.41)  Time: 0.775s, 1321.02/s  (0.791s, 1294.43/s)  LR: 1.777e-04  Data: 0.011 (0.013)
Train: 219 [ 900/1251 ( 72%)]  Loss: 3.813 (3.43)  Time: 0.785s, 1305.29/s  (0.791s, 1295.00/s)  LR: 1.777e-04  Data: 0.011 (0.013)
Train: 219 [ 950/1251 ( 76%)]  Loss: 3.718 (3.45)  Time: 0.778s, 1315.60/s  (0.790s, 1295.69/s)  LR: 1.777e-04  Data: 0.011 (0.013)
Train: 219 [1000/1251 ( 80%)]  Loss: 3.573 (3.45)  Time: 0.777s, 1318.34/s  (0.791s, 1295.30/s)  LR: 1.777e-04  Data: 0.011 (0.013)
Train: 219 [1050/1251 ( 84%)]  Loss: 3.804 (3.47)  Time: 0.778s, 1315.62/s  (0.790s, 1295.53/s)  LR: 1.777e-04  Data: 0.011 (0.013)
Train: 219 [1100/1251 ( 88%)]  Loss: 3.398 (3.47)  Time: 0.850s, 1205.34/s  (0.790s, 1296.06/s)  LR: 1.777e-04  Data: 0.011 (0.013)
Train: 219 [1150/1251 ( 92%)]  Loss: 3.566 (3.47)  Time: 0.809s, 1265.56/s  (0.790s, 1296.65/s)  LR: 1.777e-04  Data: 0.011 (0.012)
Train: 219 [1200/1251 ( 96%)]  Loss: 3.625 (3.48)  Time: 0.777s, 1318.30/s  (0.790s, 1295.55/s)  LR: 1.777e-04  Data: 0.012 (0.012)
Train: 219 [1250/1251 (100%)]  Loss: 3.594 (3.48)  Time: 0.768s, 1332.80/s  (0.790s, 1296.02/s)  LR: 1.777e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.570 (1.570)  Loss:  0.7147 (0.7147)  Acc@1: 90.8203 (90.8203)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.172 (0.563)  Loss:  0.9075 (1.2908)  Acc@1: 85.0236 (75.8400)  Acc@5: 96.6981 (92.8520)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-214.pth.tar', 75.86000010986328)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-219.pth.tar', 75.8399999584961)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-217.pth.tar', 75.76600002929688)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-218.pth.tar', 75.74000005615234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-215.pth.tar', 75.66400013916015)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-213.pth.tar', 75.63799998291016)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-216.pth.tar', 75.48200011230469)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-211.pth.tar', 75.43600005859375)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-212.pth.tar', 75.34400005859375)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-208.pth.tar', 75.34000006347657)

Train: 220 [   0/1251 (  0%)]  Loss: 3.400 (3.40)  Time: 2.374s,  431.31/s  (2.374s,  431.31/s)  LR: 1.738e-04  Data: 1.640 (1.640)
Train: 220 [  50/1251 (  4%)]  Loss: 3.814 (3.61)  Time: 0.813s, 1260.06/s  (0.828s, 1236.10/s)  LR: 1.738e-04  Data: 0.011 (0.046)
Train: 220 [ 100/1251 (  8%)]  Loss: 3.665 (3.63)  Time: 0.779s, 1313.91/s  (0.809s, 1265.42/s)  LR: 1.738e-04  Data: 0.011 (0.029)
Train: 220 [ 150/1251 ( 12%)]  Loss: 3.624 (3.63)  Time: 0.784s, 1306.82/s  (0.804s, 1273.31/s)  LR: 1.738e-04  Data: 0.012 (0.023)
Train: 220 [ 200/1251 ( 16%)]  Loss: 3.346 (3.57)  Time: 0.815s, 1256.61/s  (0.800s, 1279.87/s)  LR: 1.738e-04  Data: 0.011 (0.020)
Train: 220 [ 250/1251 ( 20%)]  Loss: 3.663 (3.59)  Time: 0.820s, 1249.08/s  (0.803s, 1275.84/s)  LR: 1.738e-04  Data: 0.011 (0.018)
Train: 220 [ 300/1251 ( 24%)]  Loss: 3.221 (3.53)  Time: 0.785s, 1304.46/s  (0.803s, 1275.52/s)  LR: 1.738e-04  Data: 0.011 (0.017)
Train: 220 [ 350/1251 ( 28%)]  Loss: 3.719 (3.56)  Time: 0.776s, 1318.99/s  (0.801s, 1278.48/s)  LR: 1.738e-04  Data: 0.011 (0.016)
Train: 220 [ 400/1251 ( 32%)]  Loss: 3.613 (3.56)  Time: 0.779s, 1314.30/s  (0.799s, 1281.38/s)  LR: 1.738e-04  Data: 0.012 (0.016)
Train: 220 [ 450/1251 ( 36%)]  Loss: 3.092 (3.52)  Time: 0.842s, 1216.69/s  (0.799s, 1281.87/s)  LR: 1.738e-04  Data: 0.011 (0.015)
Train: 220 [ 500/1251 ( 40%)]  Loss: 3.804 (3.54)  Time: 0.778s, 1316.11/s  (0.797s, 1284.47/s)  LR: 1.738e-04  Data: 0.012 (0.015)
Train: 220 [ 550/1251 ( 44%)]  Loss: 3.341 (3.53)  Time: 0.820s, 1249.38/s  (0.797s, 1285.61/s)  LR: 1.738e-04  Data: 0.012 (0.014)
Train: 220 [ 600/1251 ( 48%)]  Loss: 3.567 (3.53)  Time: 0.781s, 1311.59/s  (0.795s, 1287.39/s)  LR: 1.738e-04  Data: 0.011 (0.014)
Train: 220 [ 650/1251 ( 52%)]  Loss: 3.428 (3.52)  Time: 0.779s, 1315.28/s  (0.795s, 1288.64/s)  LR: 1.738e-04  Data: 0.012 (0.014)
Train: 220 [ 700/1251 ( 56%)]  Loss: 3.581 (3.53)  Time: 0.816s, 1255.60/s  (0.795s, 1288.62/s)  LR: 1.738e-04  Data: 0.012 (0.014)
Train: 220 [ 750/1251 ( 60%)]  Loss: 3.451 (3.52)  Time: 0.783s, 1308.17/s  (0.795s, 1288.50/s)  LR: 1.738e-04  Data: 0.012 (0.014)
Train: 220 [ 800/1251 ( 64%)]  Loss: 3.439 (3.52)  Time: 0.789s, 1298.12/s  (0.795s, 1288.55/s)  LR: 1.738e-04  Data: 0.011 (0.013)
Train: 220 [ 850/1251 ( 68%)]  Loss: 3.422 (3.51)  Time: 0.800s, 1279.36/s  (0.794s, 1289.27/s)  LR: 1.738e-04  Data: 0.011 (0.013)
Train: 220 [ 900/1251 ( 72%)]  Loss: 3.429 (3.51)  Time: 0.784s, 1305.74/s  (0.794s, 1289.95/s)  LR: 1.738e-04  Data: 0.011 (0.013)
Train: 220 [ 950/1251 ( 76%)]  Loss: 3.257 (3.49)  Time: 0.779s, 1314.25/s  (0.793s, 1290.58/s)  LR: 1.738e-04  Data: 0.011 (0.013)
Train: 220 [1000/1251 ( 80%)]  Loss: 3.688 (3.50)  Time: 0.779s, 1314.81/s  (0.793s, 1290.50/s)  LR: 1.738e-04  Data: 0.011 (0.013)
Train: 220 [1050/1251 ( 84%)]  Loss: 3.522 (3.50)  Time: 0.778s, 1315.56/s  (0.794s, 1289.89/s)  LR: 1.738e-04  Data: 0.011 (0.013)
Train: 220 [1100/1251 ( 88%)]  Loss: 3.647 (3.51)  Time: 0.814s, 1258.34/s  (0.795s, 1288.41/s)  LR: 1.738e-04  Data: 0.012 (0.013)
Train: 220 [1150/1251 ( 92%)]  Loss: 3.345 (3.50)  Time: 0.818s, 1251.36/s  (0.795s, 1288.22/s)  LR: 1.738e-04  Data: 0.012 (0.013)
Train: 220 [1200/1251 ( 96%)]  Loss: 3.504 (3.50)  Time: 0.781s, 1311.89/s  (0.795s, 1288.50/s)  LR: 1.738e-04  Data: 0.011 (0.013)
Train: 220 [1250/1251 (100%)]  Loss: 3.690 (3.51)  Time: 0.768s, 1332.77/s  (0.794s, 1288.91/s)  LR: 1.738e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.551 (1.551)  Loss:  0.8627 (0.8627)  Acc@1: 89.7461 (89.7461)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.172 (0.559)  Loss:  0.9014 (1.3539)  Acc@1: 86.0849 (75.7920)  Acc@5: 97.1698 (92.8900)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-214.pth.tar', 75.86000010986328)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-219.pth.tar', 75.8399999584961)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-220.pth.tar', 75.79200003173828)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-217.pth.tar', 75.76600002929688)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-218.pth.tar', 75.74000005615234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-215.pth.tar', 75.66400013916015)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-213.pth.tar', 75.63799998291016)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-216.pth.tar', 75.48200011230469)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-211.pth.tar', 75.43600005859375)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-212.pth.tar', 75.34400005859375)

Train: 221 [   0/1251 (  0%)]  Loss: 3.211 (3.21)  Time: 2.400s,  426.72/s  (2.400s,  426.72/s)  LR: 1.699e-04  Data: 1.665 (1.665)
Train: 221 [  50/1251 (  4%)]  Loss: 3.271 (3.24)  Time: 0.818s, 1251.89/s  (0.826s, 1240.19/s)  LR: 1.699e-04  Data: 0.012 (0.045)
Train: 221 [ 100/1251 (  8%)]  Loss: 3.490 (3.32)  Time: 0.788s, 1299.92/s  (0.815s, 1255.71/s)  LR: 1.699e-04  Data: 0.012 (0.028)
Train: 221 [ 150/1251 ( 12%)]  Loss: 3.509 (3.37)  Time: 0.779s, 1314.23/s  (0.806s, 1269.76/s)  LR: 1.699e-04  Data: 0.011 (0.023)
Train: 221 [ 200/1251 ( 16%)]  Loss: 3.060 (3.31)  Time: 0.820s, 1248.21/s  (0.802s, 1277.18/s)  LR: 1.699e-04  Data: 0.011 (0.020)
Train: 221 [ 250/1251 ( 20%)]  Loss: 3.759 (3.38)  Time: 0.780s, 1312.97/s  (0.799s, 1281.65/s)  LR: 1.699e-04  Data: 0.012 (0.018)
Train: 221 [ 300/1251 ( 24%)]  Loss: 3.157 (3.35)  Time: 0.778s, 1316.23/s  (0.797s, 1284.87/s)  LR: 1.699e-04  Data: 0.011 (0.017)
Train: 221 [ 350/1251 ( 28%)]  Loss: 3.503 (3.37)  Time: 0.780s, 1312.86/s  (0.798s, 1283.41/s)  LR: 1.699e-04  Data: 0.011 (0.016)
Train: 221 [ 400/1251 ( 32%)]  Loss: 3.646 (3.40)  Time: 0.812s, 1260.84/s  (0.798s, 1283.12/s)  LR: 1.699e-04  Data: 0.011 (0.015)
Train: 221 [ 450/1251 ( 36%)]  Loss: 3.248 (3.39)  Time: 0.811s, 1262.56/s  (0.799s, 1282.36/s)  LR: 1.699e-04  Data: 0.011 (0.015)
Train: 221 [ 500/1251 ( 40%)]  Loss: 3.319 (3.38)  Time: 0.776s, 1319.40/s  (0.798s, 1282.66/s)  LR: 1.699e-04  Data: 0.011 (0.015)
Train: 221 [ 550/1251 ( 44%)]  Loss: 3.276 (3.37)  Time: 0.777s, 1317.56/s  (0.797s, 1285.13/s)  LR: 1.699e-04  Data: 0.011 (0.014)
Train: 221 [ 600/1251 ( 48%)]  Loss: 3.568 (3.39)  Time: 0.779s, 1314.09/s  (0.796s, 1286.36/s)  LR: 1.699e-04  Data: 0.011 (0.014)
Train: 221 [ 650/1251 ( 52%)]  Loss: 3.480 (3.39)  Time: 0.779s, 1314.92/s  (0.795s, 1288.23/s)  LR: 1.699e-04  Data: 0.011 (0.014)
Train: 221 [ 700/1251 ( 56%)]  Loss: 3.491 (3.40)  Time: 0.781s, 1310.58/s  (0.794s, 1289.69/s)  LR: 1.699e-04  Data: 0.011 (0.014)
Train: 221 [ 750/1251 ( 60%)]  Loss: 3.667 (3.42)  Time: 0.815s, 1256.61/s  (0.795s, 1288.24/s)  LR: 1.699e-04  Data: 0.011 (0.013)
Train: 221 [ 800/1251 ( 64%)]  Loss: 3.840 (3.44)  Time: 0.777s, 1317.39/s  (0.795s, 1287.77/s)  LR: 1.699e-04  Data: 0.011 (0.013)
Train: 221 [ 850/1251 ( 68%)]  Loss: 3.283 (3.43)  Time: 0.777s, 1318.28/s  (0.794s, 1288.94/s)  LR: 1.699e-04  Data: 0.011 (0.013)
Train: 221 [ 900/1251 ( 72%)]  Loss: 3.132 (3.42)  Time: 0.778s, 1315.74/s  (0.794s, 1290.00/s)  LR: 1.699e-04  Data: 0.011 (0.013)
Train: 221 [ 950/1251 ( 76%)]  Loss: 3.196 (3.41)  Time: 0.780s, 1313.11/s  (0.794s, 1290.34/s)  LR: 1.699e-04  Data: 0.011 (0.013)
Train: 221 [1000/1251 ( 80%)]  Loss: 3.344 (3.40)  Time: 0.809s, 1266.04/s  (0.793s, 1291.06/s)  LR: 1.699e-04  Data: 0.011 (0.013)
Train: 221 [1050/1251 ( 84%)]  Loss: 3.620 (3.41)  Time: 0.780s, 1312.39/s  (0.794s, 1289.49/s)  LR: 1.699e-04  Data: 0.011 (0.013)
Train: 221 [1100/1251 ( 88%)]  Loss: 3.643 (3.42)  Time: 0.781s, 1310.54/s  (0.795s, 1288.61/s)  LR: 1.699e-04  Data: 0.012 (0.013)
Train: 221 [1150/1251 ( 92%)]  Loss: 3.692 (3.43)  Time: 0.795s, 1287.39/s  (0.795s, 1287.56/s)  LR: 1.699e-04  Data: 0.011 (0.013)
Train: 221 [1200/1251 ( 96%)]  Loss: 3.639 (3.44)  Time: 0.778s, 1317.04/s  (0.795s, 1287.95/s)  LR: 1.699e-04  Data: 0.011 (0.013)
Train: 221 [1250/1251 (100%)]  Loss: 3.535 (3.45)  Time: 0.769s, 1332.42/s  (0.795s, 1288.69/s)  LR: 1.699e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.496 (1.496)  Loss:  0.7845 (0.7845)  Acc@1: 89.9414 (89.9414)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.172 (0.566)  Loss:  0.8071 (1.2643)  Acc@1: 85.7311 (75.9880)  Acc@5: 96.9340 (92.8700)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-221.pth.tar', 75.98800000732422)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-214.pth.tar', 75.86000010986328)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-219.pth.tar', 75.8399999584961)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-220.pth.tar', 75.79200003173828)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-217.pth.tar', 75.76600002929688)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-218.pth.tar', 75.74000005615234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-215.pth.tar', 75.66400013916015)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-213.pth.tar', 75.63799998291016)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-216.pth.tar', 75.48200011230469)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-211.pth.tar', 75.43600005859375)

Train: 222 [   0/1251 (  0%)]  Loss: 3.304 (3.30)  Time: 2.190s,  467.68/s  (2.190s,  467.68/s)  LR: 1.661e-04  Data: 1.453 (1.453)
Train: 222 [  50/1251 (  4%)]  Loss: 3.555 (3.43)  Time: 0.834s, 1227.24/s  (0.819s, 1250.74/s)  LR: 1.661e-04  Data: 0.011 (0.047)
Train: 222 [ 100/1251 (  8%)]  Loss: 2.870 (3.24)  Time: 0.828s, 1236.35/s  (0.810s, 1264.20/s)  LR: 1.661e-04  Data: 0.011 (0.029)
Train: 222 [ 150/1251 ( 12%)]  Loss: 3.484 (3.30)  Time: 0.777s, 1318.02/s  (0.805s, 1272.48/s)  LR: 1.661e-04  Data: 0.012 (0.023)
Train: 222 [ 200/1251 ( 16%)]  Loss: 3.590 (3.36)  Time: 0.804s, 1274.30/s  (0.802s, 1276.41/s)  LR: 1.661e-04  Data: 0.011 (0.020)
Train: 222 [ 250/1251 ( 20%)]  Loss: 3.408 (3.37)  Time: 0.819s, 1250.92/s  (0.800s, 1279.29/s)  LR: 1.661e-04  Data: 0.012 (0.018)
Train: 222 [ 300/1251 ( 24%)]  Loss: 3.462 (3.38)  Time: 0.817s, 1253.02/s  (0.803s, 1274.99/s)  LR: 1.661e-04  Data: 0.011 (0.017)
Train: 222 [ 350/1251 ( 28%)]  Loss: 3.345 (3.38)  Time: 0.813s, 1259.70/s  (0.803s, 1274.43/s)  LR: 1.661e-04  Data: 0.011 (0.016)
Train: 222 [ 400/1251 ( 32%)]  Loss: 3.996 (3.45)  Time: 0.779s, 1313.98/s  (0.804s, 1273.64/s)  LR: 1.661e-04  Data: 0.010 (0.016)
Train: 222 [ 450/1251 ( 36%)]  Loss: 3.437 (3.45)  Time: 0.819s, 1250.42/s  (0.802s, 1276.95/s)  LR: 1.661e-04  Data: 0.011 (0.015)
Train: 222 [ 500/1251 ( 40%)]  Loss: 3.618 (3.46)  Time: 0.778s, 1316.44/s  (0.801s, 1278.95/s)  LR: 1.661e-04  Data: 0.011 (0.015)
Train: 222 [ 550/1251 ( 44%)]  Loss: 3.667 (3.48)  Time: 0.777s, 1318.14/s  (0.800s, 1280.36/s)  LR: 1.661e-04  Data: 0.011 (0.014)
Train: 222 [ 600/1251 ( 48%)]  Loss: 3.636 (3.49)  Time: 0.776s, 1319.80/s  (0.799s, 1282.07/s)  LR: 1.661e-04  Data: 0.011 (0.014)
Train: 222 [ 650/1251 ( 52%)]  Loss: 3.762 (3.51)  Time: 0.780s, 1312.81/s  (0.800s, 1280.39/s)  LR: 1.661e-04  Data: 0.011 (0.014)
Train: 222 [ 700/1251 ( 56%)]  Loss: 3.231 (3.49)  Time: 0.777s, 1317.30/s  (0.799s, 1281.32/s)  LR: 1.661e-04  Data: 0.011 (0.014)
Train: 222 [ 750/1251 ( 60%)]  Loss: 3.348 (3.48)  Time: 0.782s, 1310.04/s  (0.799s, 1282.13/s)  LR: 1.661e-04  Data: 0.012 (0.014)
Train: 222 [ 800/1251 ( 64%)]  Loss: 3.324 (3.47)  Time: 0.781s, 1310.45/s  (0.798s, 1283.14/s)  LR: 1.661e-04  Data: 0.011 (0.013)
Train: 222 [ 850/1251 ( 68%)]  Loss: 3.431 (3.47)  Time: 0.777s, 1317.50/s  (0.797s, 1284.35/s)  LR: 1.661e-04  Data: 0.011 (0.013)
Train: 222 [ 900/1251 ( 72%)]  Loss: 3.769 (3.49)  Time: 0.778s, 1316.64/s  (0.796s, 1285.87/s)  LR: 1.661e-04  Data: 0.011 (0.013)
Train: 222 [ 950/1251 ( 76%)]  Loss: 3.448 (3.48)  Time: 0.780s, 1312.96/s  (0.796s, 1286.53/s)  LR: 1.661e-04  Data: 0.011 (0.013)
Train: 222 [1000/1251 ( 80%)]  Loss: 3.267 (3.47)  Time: 0.777s, 1317.71/s  (0.795s, 1287.57/s)  LR: 1.661e-04  Data: 0.010 (0.013)
Train: 222 [1050/1251 ( 84%)]  Loss: 3.082 (3.46)  Time: 0.780s, 1312.26/s  (0.795s, 1288.35/s)  LR: 1.661e-04  Data: 0.011 (0.013)
Train: 222 [1100/1251 ( 88%)]  Loss: 3.370 (3.45)  Time: 0.777s, 1317.69/s  (0.795s, 1288.80/s)  LR: 1.661e-04  Data: 0.011 (0.013)
Train: 222 [1150/1251 ( 92%)]  Loss: 3.337 (3.45)  Time: 0.780s, 1312.16/s  (0.794s, 1289.38/s)  LR: 1.661e-04  Data: 0.011 (0.013)
Train: 222 [1200/1251 ( 96%)]  Loss: 3.519 (3.45)  Time: 0.774s, 1322.45/s  (0.794s, 1289.86/s)  LR: 1.661e-04  Data: 0.011 (0.013)
Train: 222 [1250/1251 (100%)]  Loss: 3.354 (3.45)  Time: 0.777s, 1317.88/s  (0.794s, 1289.23/s)  LR: 1.661e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.573 (1.573)  Loss:  0.7890 (0.7890)  Acc@1: 90.8203 (90.8203)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.172 (0.566)  Loss:  0.9997 (1.4052)  Acc@1: 86.4387 (75.8840)  Acc@5: 96.6981 (92.9420)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-221.pth.tar', 75.98800000732422)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-222.pth.tar', 75.88400018554688)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-214.pth.tar', 75.86000010986328)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-219.pth.tar', 75.8399999584961)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-220.pth.tar', 75.79200003173828)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-217.pth.tar', 75.76600002929688)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-218.pth.tar', 75.74000005615234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-215.pth.tar', 75.66400013916015)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-213.pth.tar', 75.63799998291016)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-216.pth.tar', 75.48200011230469)

Train: 223 [   0/1251 (  0%)]  Loss: 3.858 (3.86)  Time: 2.450s,  418.01/s  (2.450s,  418.01/s)  LR: 1.624e-04  Data: 1.658 (1.658)
Train: 223 [  50/1251 (  4%)]  Loss: 3.181 (3.52)  Time: 0.777s, 1317.85/s  (0.825s, 1241.50/s)  LR: 1.624e-04  Data: 0.011 (0.048)
Train: 223 [ 100/1251 (  8%)]  Loss: 3.760 (3.60)  Time: 0.863s, 1187.00/s  (0.811s, 1262.02/s)  LR: 1.624e-04  Data: 0.014 (0.030)
Train: 223 [ 150/1251 ( 12%)]  Loss: 3.510 (3.58)  Time: 0.814s, 1257.83/s  (0.805s, 1272.45/s)  LR: 1.624e-04  Data: 0.012 (0.024)
Train: 223 [ 200/1251 ( 16%)]  Loss: 3.553 (3.57)  Time: 0.878s, 1166.32/s  (0.801s, 1278.15/s)  LR: 1.624e-04  Data: 0.011 (0.020)
Train: 223 [ 250/1251 ( 20%)]  Loss: 3.360 (3.54)  Time: 0.782s, 1310.06/s  (0.797s, 1284.33/s)  LR: 1.624e-04  Data: 0.011 (0.019)
Train: 223 [ 300/1251 ( 24%)]  Loss: 3.289 (3.50)  Time: 0.813s, 1259.27/s  (0.797s, 1285.13/s)  LR: 1.624e-04  Data: 0.012 (0.017)
Train: 223 [ 350/1251 ( 28%)]  Loss: 3.351 (3.48)  Time: 0.781s, 1311.11/s  (0.798s, 1282.75/s)  LR: 1.624e-04  Data: 0.010 (0.016)
Train: 223 [ 400/1251 ( 32%)]  Loss: 3.384 (3.47)  Time: 0.778s, 1315.70/s  (0.798s, 1283.90/s)  LR: 1.624e-04  Data: 0.011 (0.016)
Train: 223 [ 450/1251 ( 36%)]  Loss: 3.227 (3.45)  Time: 0.778s, 1316.71/s  (0.796s, 1286.18/s)  LR: 1.624e-04  Data: 0.012 (0.015)
Train: 223 [ 500/1251 ( 40%)]  Loss: 3.707 (3.47)  Time: 0.777s, 1317.56/s  (0.795s, 1287.74/s)  LR: 1.624e-04  Data: 0.011 (0.015)
Train: 223 [ 550/1251 ( 44%)]  Loss: 3.328 (3.46)  Time: 0.777s, 1318.16/s  (0.795s, 1288.44/s)  LR: 1.624e-04  Data: 0.011 (0.014)
Train: 223 [ 600/1251 ( 48%)]  Loss: 3.514 (3.46)  Time: 0.785s, 1303.96/s  (0.794s, 1289.01/s)  LR: 1.624e-04  Data: 0.011 (0.014)
Train: 223 [ 650/1251 ( 52%)]  Loss: 3.456 (3.46)  Time: 0.779s, 1314.31/s  (0.794s, 1289.63/s)  LR: 1.624e-04  Data: 0.011 (0.014)
Train: 223 [ 700/1251 ( 56%)]  Loss: 3.251 (3.45)  Time: 0.796s, 1286.28/s  (0.793s, 1291.06/s)  LR: 1.624e-04  Data: 0.011 (0.014)
Train: 223 [ 750/1251 ( 60%)]  Loss: 3.728 (3.47)  Time: 0.777s, 1317.22/s  (0.793s, 1291.35/s)  LR: 1.624e-04  Data: 0.011 (0.014)
Train: 223 [ 800/1251 ( 64%)]  Loss: 3.568 (3.47)  Time: 0.778s, 1316.80/s  (0.793s, 1290.53/s)  LR: 1.624e-04  Data: 0.011 (0.013)
Train: 223 [ 850/1251 ( 68%)]  Loss: 3.573 (3.48)  Time: 0.779s, 1314.40/s  (0.793s, 1290.70/s)  LR: 1.624e-04  Data: 0.011 (0.013)
Train: 223 [ 900/1251 ( 72%)]  Loss: 3.377 (3.47)  Time: 0.787s, 1300.59/s  (0.793s, 1291.61/s)  LR: 1.624e-04  Data: 0.014 (0.013)
Train: 223 [ 950/1251 ( 76%)]  Loss: 3.127 (3.46)  Time: 0.778s, 1316.44/s  (0.793s, 1292.08/s)  LR: 1.624e-04  Data: 0.011 (0.013)
Train: 223 [1000/1251 ( 80%)]  Loss: 3.479 (3.46)  Time: 0.793s, 1291.89/s  (0.792s, 1292.57/s)  LR: 1.624e-04  Data: 0.011 (0.013)
Train: 223 [1050/1251 ( 84%)]  Loss: 3.347 (3.45)  Time: 0.834s, 1227.36/s  (0.792s, 1292.93/s)  LR: 1.624e-04  Data: 0.011 (0.013)
Train: 223 [1100/1251 ( 88%)]  Loss: 3.517 (3.45)  Time: 0.827s, 1238.42/s  (0.792s, 1293.13/s)  LR: 1.624e-04  Data: 0.011 (0.013)
Train: 223 [1150/1251 ( 92%)]  Loss: 3.069 (3.44)  Time: 0.817s, 1253.83/s  (0.792s, 1293.73/s)  LR: 1.624e-04  Data: 0.011 (0.013)
Train: 223 [1200/1251 ( 96%)]  Loss: 3.711 (3.45)  Time: 0.777s, 1317.08/s  (0.792s, 1292.63/s)  LR: 1.624e-04  Data: 0.010 (0.013)
Train: 223 [1250/1251 (100%)]  Loss: 3.277 (3.44)  Time: 0.805s, 1272.39/s  (0.793s, 1291.53/s)  LR: 1.624e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.564 (1.564)  Loss:  0.8883 (0.8883)  Acc@1: 90.5273 (90.5273)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.172 (0.569)  Loss:  0.9777 (1.4060)  Acc@1: 86.0849 (75.9620)  Acc@5: 97.0519 (92.8520)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-221.pth.tar', 75.98800000732422)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-223.pth.tar', 75.9620001611328)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-222.pth.tar', 75.88400018554688)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-214.pth.tar', 75.86000010986328)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-219.pth.tar', 75.8399999584961)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-220.pth.tar', 75.79200003173828)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-217.pth.tar', 75.76600002929688)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-218.pth.tar', 75.74000005615234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-215.pth.tar', 75.66400013916015)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-213.pth.tar', 75.63799998291016)

Train: 224 [   0/1251 (  0%)]  Loss: 3.330 (3.33)  Time: 2.363s,  433.31/s  (2.363s,  433.31/s)  LR: 1.587e-04  Data: 1.627 (1.627)
Train: 224 [  50/1251 (  4%)]  Loss: 3.594 (3.46)  Time: 0.777s, 1318.02/s  (0.832s, 1230.41/s)  LR: 1.587e-04  Data: 0.011 (0.050)
Train: 224 [ 100/1251 (  8%)]  Loss: 3.021 (3.31)  Time: 0.782s, 1309.87/s  (0.811s, 1262.06/s)  LR: 1.587e-04  Data: 0.011 (0.031)
Train: 224 [ 150/1251 ( 12%)]  Loss: 3.120 (3.27)  Time: 0.817s, 1253.76/s  (0.804s, 1273.42/s)  LR: 1.587e-04  Data: 0.011 (0.024)
Train: 224 [ 200/1251 ( 16%)]  Loss: 3.442 (3.30)  Time: 0.779s, 1315.05/s  (0.800s, 1280.10/s)  LR: 1.587e-04  Data: 0.011 (0.021)
Train: 224 [ 250/1251 ( 20%)]  Loss: 3.198 (3.28)  Time: 0.810s, 1264.09/s  (0.798s, 1283.32/s)  LR: 1.587e-04  Data: 0.011 (0.019)
Train: 224 [ 300/1251 ( 24%)]  Loss: 3.295 (3.29)  Time: 0.781s, 1311.80/s  (0.796s, 1285.95/s)  LR: 1.587e-04  Data: 0.011 (0.018)
Train: 224 [ 350/1251 ( 28%)]  Loss: 3.678 (3.33)  Time: 0.777s, 1317.39/s  (0.796s, 1285.91/s)  LR: 1.587e-04  Data: 0.011 (0.017)
Train: 224 [ 400/1251 ( 32%)]  Loss: 3.654 (3.37)  Time: 0.780s, 1312.83/s  (0.797s, 1285.49/s)  LR: 1.587e-04  Data: 0.011 (0.016)
Train: 224 [ 450/1251 ( 36%)]  Loss: 3.623 (3.40)  Time: 0.831s, 1231.95/s  (0.795s, 1287.34/s)  LR: 1.587e-04  Data: 0.011 (0.015)
Train: 224 [ 500/1251 ( 40%)]  Loss: 3.242 (3.38)  Time: 0.777s, 1318.27/s  (0.795s, 1287.48/s)  LR: 1.587e-04  Data: 0.011 (0.015)
Train: 224 [ 550/1251 ( 44%)]  Loss: 3.397 (3.38)  Time: 0.821s, 1246.66/s  (0.795s, 1287.52/s)  LR: 1.587e-04  Data: 0.011 (0.015)
Train: 224 [ 600/1251 ( 48%)]  Loss: 3.459 (3.39)  Time: 0.776s, 1318.79/s  (0.795s, 1287.91/s)  LR: 1.587e-04  Data: 0.011 (0.014)
Train: 224 [ 650/1251 ( 52%)]  Loss: 3.202 (3.38)  Time: 0.779s, 1313.95/s  (0.794s, 1289.53/s)  LR: 1.587e-04  Data: 0.011 (0.014)
Train: 224 [ 700/1251 ( 56%)]  Loss: 3.482 (3.38)  Time: 0.776s, 1319.14/s  (0.793s, 1290.79/s)  LR: 1.587e-04  Data: 0.011 (0.014)
Train: 224 [ 750/1251 ( 60%)]  Loss: 3.853 (3.41)  Time: 0.813s, 1259.84/s  (0.793s, 1291.57/s)  LR: 1.587e-04  Data: 0.011 (0.014)
Train: 224 [ 800/1251 ( 64%)]  Loss: 3.257 (3.40)  Time: 0.781s, 1311.51/s  (0.792s, 1292.49/s)  LR: 1.587e-04  Data: 0.011 (0.014)
Train: 224 [ 850/1251 ( 68%)]  Loss: 3.319 (3.40)  Time: 0.814s, 1258.14/s  (0.792s, 1292.71/s)  LR: 1.587e-04  Data: 0.011 (0.013)
Train: 224 [ 900/1251 ( 72%)]  Loss: 3.191 (3.39)  Time: 0.824s, 1243.06/s  (0.793s, 1291.85/s)  LR: 1.587e-04  Data: 0.012 (0.013)
Train: 224 [ 950/1251 ( 76%)]  Loss: 3.868 (3.41)  Time: 0.817s, 1254.11/s  (0.793s, 1291.15/s)  LR: 1.587e-04  Data: 0.011 (0.013)
Train: 224 [1000/1251 ( 80%)]  Loss: 3.329 (3.41)  Time: 0.778s, 1315.43/s  (0.793s, 1291.58/s)  LR: 1.587e-04  Data: 0.011 (0.013)
Train: 224 [1050/1251 ( 84%)]  Loss: 3.529 (3.41)  Time: 0.775s, 1321.65/s  (0.793s, 1291.73/s)  LR: 1.587e-04  Data: 0.010 (0.013)
Train: 224 [1100/1251 ( 88%)]  Loss: 3.779 (3.43)  Time: 0.781s, 1311.38/s  (0.793s, 1291.78/s)  LR: 1.587e-04  Data: 0.011 (0.013)
Train: 224 [1150/1251 ( 92%)]  Loss: 3.309 (3.42)  Time: 0.781s, 1311.42/s  (0.792s, 1292.50/s)  LR: 1.587e-04  Data: 0.010 (0.013)
Train: 224 [1200/1251 ( 96%)]  Loss: 3.805 (3.44)  Time: 0.785s, 1303.80/s  (0.793s, 1291.75/s)  LR: 1.587e-04  Data: 0.010 (0.013)
Train: 224 [1250/1251 (100%)]  Loss: 3.683 (3.45)  Time: 0.767s, 1335.31/s  (0.792s, 1292.22/s)  LR: 1.587e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.598 (1.598)  Loss:  0.7682 (0.7682)  Acc@1: 89.8438 (89.8438)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.172 (0.565)  Loss:  0.8234 (1.3251)  Acc@1: 86.3208 (76.1000)  Acc@5: 97.1698 (93.0340)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-224.pth.tar', 76.10000000488282)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-221.pth.tar', 75.98800000732422)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-223.pth.tar', 75.9620001611328)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-222.pth.tar', 75.88400018554688)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-214.pth.tar', 75.86000010986328)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-219.pth.tar', 75.8399999584961)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-220.pth.tar', 75.79200003173828)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-217.pth.tar', 75.76600002929688)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-218.pth.tar', 75.74000005615234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-215.pth.tar', 75.66400013916015)

Train: 225 [   0/1251 (  0%)]  Loss: 3.492 (3.49)  Time: 2.513s,  407.53/s  (2.513s,  407.53/s)  LR: 1.550e-04  Data: 1.780 (1.780)
Train: 225 [  50/1251 (  4%)]  Loss: 3.251 (3.37)  Time: 0.779s, 1313.95/s  (0.818s, 1251.74/s)  LR: 1.550e-04  Data: 0.011 (0.048)
Train: 225 [ 100/1251 (  8%)]  Loss: 3.485 (3.41)  Time: 0.777s, 1317.26/s  (0.801s, 1278.95/s)  LR: 1.550e-04  Data: 0.011 (0.030)
Train: 225 [ 150/1251 ( 12%)]  Loss: 3.565 (3.45)  Time: 0.782s, 1309.66/s  (0.800s, 1279.88/s)  LR: 1.550e-04  Data: 0.011 (0.023)
Train: 225 [ 200/1251 ( 16%)]  Loss: 3.160 (3.39)  Time: 0.780s, 1312.65/s  (0.795s, 1288.12/s)  LR: 1.550e-04  Data: 0.011 (0.020)
Train: 225 [ 250/1251 ( 20%)]  Loss: 3.348 (3.38)  Time: 0.785s, 1304.40/s  (0.793s, 1291.05/s)  LR: 1.550e-04  Data: 0.011 (0.018)
Train: 225 [ 300/1251 ( 24%)]  Loss: 3.652 (3.42)  Time: 0.789s, 1298.15/s  (0.792s, 1293.50/s)  LR: 1.550e-04  Data: 0.016 (0.017)
Train: 225 [ 350/1251 ( 28%)]  Loss: 3.413 (3.42)  Time: 0.840s, 1219.52/s  (0.791s, 1295.28/s)  LR: 1.550e-04  Data: 0.011 (0.016)
Train: 225 [ 400/1251 ( 32%)]  Loss: 3.862 (3.47)  Time: 0.778s, 1315.81/s  (0.792s, 1293.61/s)  LR: 1.550e-04  Data: 0.011 (0.016)
Train: 225 [ 450/1251 ( 36%)]  Loss: 3.540 (3.48)  Time: 0.787s, 1301.48/s  (0.793s, 1290.63/s)  LR: 1.550e-04  Data: 0.012 (0.015)
Train: 225 [ 500/1251 ( 40%)]  Loss: 3.164 (3.45)  Time: 0.782s, 1310.16/s  (0.793s, 1291.75/s)  LR: 1.550e-04  Data: 0.011 (0.015)
Train: 225 [ 550/1251 ( 44%)]  Loss: 3.377 (3.44)  Time: 0.825s, 1241.64/s  (0.793s, 1291.92/s)  LR: 1.550e-04  Data: 0.011 (0.014)
Train: 225 [ 600/1251 ( 48%)]  Loss: 3.516 (3.45)  Time: 0.811s, 1262.72/s  (0.793s, 1291.80/s)  LR: 1.550e-04  Data: 0.010 (0.014)
Train: 225 [ 650/1251 ( 52%)]  Loss: 3.700 (3.47)  Time: 0.831s, 1232.06/s  (0.793s, 1291.71/s)  LR: 1.550e-04  Data: 0.011 (0.014)
Train: 225 [ 700/1251 ( 56%)]  Loss: 3.341 (3.46)  Time: 0.825s, 1240.82/s  (0.794s, 1290.31/s)  LR: 1.550e-04  Data: 0.011 (0.014)
Train: 225 [ 750/1251 ( 60%)]  Loss: 3.554 (3.46)  Time: 0.780s, 1313.06/s  (0.793s, 1291.22/s)  LR: 1.550e-04  Data: 0.011 (0.014)
Train: 225 [ 800/1251 ( 64%)]  Loss: 3.526 (3.47)  Time: 0.779s, 1314.72/s  (0.794s, 1290.04/s)  LR: 1.550e-04  Data: 0.011 (0.013)
Train: 225 [ 850/1251 ( 68%)]  Loss: 3.610 (3.48)  Time: 0.817s, 1253.80/s  (0.794s, 1289.71/s)  LR: 1.550e-04  Data: 0.011 (0.013)
Train: 225 [ 900/1251 ( 72%)]  Loss: 3.694 (3.49)  Time: 0.821s, 1246.69/s  (0.795s, 1288.35/s)  LR: 1.550e-04  Data: 0.011 (0.013)
Train: 225 [ 950/1251 ( 76%)]  Loss: 3.605 (3.49)  Time: 0.778s, 1316.66/s  (0.795s, 1288.55/s)  LR: 1.550e-04  Data: 0.011 (0.013)
Train: 225 [1000/1251 ( 80%)]  Loss: 3.532 (3.49)  Time: 0.779s, 1314.35/s  (0.794s, 1289.73/s)  LR: 1.550e-04  Data: 0.011 (0.013)
Train: 225 [1050/1251 ( 84%)]  Loss: 3.467 (3.49)  Time: 0.781s, 1310.89/s  (0.793s, 1290.58/s)  LR: 1.550e-04  Data: 0.012 (0.013)
Train: 225 [1100/1251 ( 88%)]  Loss: 3.349 (3.49)  Time: 0.797s, 1284.07/s  (0.793s, 1291.17/s)  LR: 1.550e-04  Data: 0.012 (0.013)
Train: 225 [1150/1251 ( 92%)]  Loss: 3.407 (3.48)  Time: 0.781s, 1311.04/s  (0.793s, 1291.84/s)  LR: 1.550e-04  Data: 0.012 (0.013)
Train: 225 [1200/1251 ( 96%)]  Loss: 3.583 (3.49)  Time: 0.780s, 1312.00/s  (0.793s, 1292.07/s)  LR: 1.550e-04  Data: 0.011 (0.013)
Train: 225 [1250/1251 (100%)]  Loss: 3.559 (3.49)  Time: 0.793s, 1292.10/s  (0.792s, 1292.56/s)  LR: 1.550e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.508 (1.508)  Loss:  0.7898 (0.7898)  Acc@1: 90.2344 (90.2344)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.172 (0.577)  Loss:  0.8589 (1.3227)  Acc@1: 85.7311 (76.1380)  Acc@5: 96.8160 (92.9380)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-225.pth.tar', 76.13800000732422)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-224.pth.tar', 76.10000000488282)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-221.pth.tar', 75.98800000732422)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-223.pth.tar', 75.9620001611328)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-222.pth.tar', 75.88400018554688)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-214.pth.tar', 75.86000010986328)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-219.pth.tar', 75.8399999584961)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-220.pth.tar', 75.79200003173828)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-217.pth.tar', 75.76600002929688)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-218.pth.tar', 75.74000005615234)

Train: 226 [   0/1251 (  0%)]  Loss: 3.610 (3.61)  Time: 2.449s,  418.07/s  (2.449s,  418.07/s)  LR: 1.513e-04  Data: 1.714 (1.714)
Train: 226 [  50/1251 (  4%)]  Loss: 3.731 (3.67)  Time: 0.778s, 1315.85/s  (0.831s, 1232.71/s)  LR: 1.513e-04  Data: 0.011 (0.050)
Train: 226 [ 100/1251 (  8%)]  Loss: 3.314 (3.55)  Time: 0.785s, 1304.30/s  (0.812s, 1260.52/s)  LR: 1.513e-04  Data: 0.012 (0.031)
Train: 226 [ 150/1251 ( 12%)]  Loss: 3.603 (3.56)  Time: 0.826s, 1239.34/s  (0.809s, 1265.60/s)  LR: 1.513e-04  Data: 0.014 (0.024)
Train: 226 [ 200/1251 ( 16%)]  Loss: 3.290 (3.51)  Time: 0.781s, 1311.28/s  (0.808s, 1266.84/s)  LR: 1.513e-04  Data: 0.012 (0.021)
Train: 226 [ 250/1251 ( 20%)]  Loss: 3.569 (3.52)  Time: 0.819s, 1250.01/s  (0.804s, 1274.12/s)  LR: 1.513e-04  Data: 0.011 (0.019)
Train: 226 [ 300/1251 ( 24%)]  Loss: 3.245 (3.48)  Time: 0.779s, 1314.32/s  (0.802s, 1277.36/s)  LR: 1.513e-04  Data: 0.011 (0.018)
Train: 226 [ 350/1251 ( 28%)]  Loss: 3.411 (3.47)  Time: 0.780s, 1312.34/s  (0.800s, 1279.37/s)  LR: 1.513e-04  Data: 0.011 (0.017)
Train: 226 [ 400/1251 ( 32%)]  Loss: 3.606 (3.49)  Time: 0.792s, 1293.31/s  (0.799s, 1281.49/s)  LR: 1.513e-04  Data: 0.012 (0.016)
Train: 226 [ 450/1251 ( 36%)]  Loss: 3.193 (3.46)  Time: 0.780s, 1312.26/s  (0.799s, 1281.39/s)  LR: 1.513e-04  Data: 0.011 (0.016)
Train: 226 [ 500/1251 ( 40%)]  Loss: 3.584 (3.47)  Time: 0.779s, 1314.22/s  (0.798s, 1283.47/s)  LR: 1.513e-04  Data: 0.011 (0.015)
Train: 226 [ 550/1251 ( 44%)]  Loss: 3.798 (3.50)  Time: 0.877s, 1167.82/s  (0.797s, 1285.06/s)  LR: 1.513e-04  Data: 0.011 (0.015)
Train: 226 [ 600/1251 ( 48%)]  Loss: 3.304 (3.48)  Time: 0.777s, 1317.51/s  (0.796s, 1286.41/s)  LR: 1.513e-04  Data: 0.010 (0.014)
Train: 226 [ 650/1251 ( 52%)]  Loss: 3.462 (3.48)  Time: 0.812s, 1261.58/s  (0.795s, 1288.21/s)  LR: 1.513e-04  Data: 0.011 (0.014)
Train: 226 [ 700/1251 ( 56%)]  Loss: 3.245 (3.46)  Time: 0.779s, 1314.53/s  (0.795s, 1288.56/s)  LR: 1.513e-04  Data: 0.011 (0.014)
Train: 226 [ 750/1251 ( 60%)]  Loss: 3.415 (3.46)  Time: 0.776s, 1319.49/s  (0.794s, 1289.59/s)  LR: 1.513e-04  Data: 0.011 (0.014)
Train: 226 [ 800/1251 ( 64%)]  Loss: 3.389 (3.46)  Time: 0.781s, 1310.70/s  (0.794s, 1290.08/s)  LR: 1.513e-04  Data: 0.011 (0.014)
Train: 226 [ 850/1251 ( 68%)]  Loss: 3.292 (3.45)  Time: 0.779s, 1315.03/s  (0.793s, 1291.11/s)  LR: 1.513e-04  Data: 0.012 (0.013)
Train: 226 [ 900/1251 ( 72%)]  Loss: 3.348 (3.44)  Time: 0.831s, 1231.88/s  (0.793s, 1291.58/s)  LR: 1.513e-04  Data: 0.012 (0.013)
Train: 226 [ 950/1251 ( 76%)]  Loss: 3.674 (3.45)  Time: 0.777s, 1317.41/s  (0.793s, 1290.80/s)  LR: 1.513e-04  Data: 0.011 (0.013)
Train: 226 [1000/1251 ( 80%)]  Loss: 3.544 (3.46)  Time: 0.779s, 1314.70/s  (0.793s, 1291.68/s)  LR: 1.513e-04  Data: 0.011 (0.013)
Train: 226 [1050/1251 ( 84%)]  Loss: 3.641 (3.47)  Time: 0.808s, 1266.83/s  (0.792s, 1292.43/s)  LR: 1.513e-04  Data: 0.011 (0.013)
Train: 226 [1100/1251 ( 88%)]  Loss: 3.424 (3.46)  Time: 0.780s, 1312.52/s  (0.792s, 1292.46/s)  LR: 1.513e-04  Data: 0.011 (0.013)
Train: 226 [1150/1251 ( 92%)]  Loss: 3.322 (3.46)  Time: 0.814s, 1258.39/s  (0.792s, 1292.71/s)  LR: 1.513e-04  Data: 0.011 (0.013)
Train: 226 [1200/1251 ( 96%)]  Loss: 3.712 (3.47)  Time: 0.781s, 1311.36/s  (0.792s, 1293.14/s)  LR: 1.513e-04  Data: 0.011 (0.013)
Train: 226 [1250/1251 (100%)]  Loss: 3.724 (3.48)  Time: 0.795s, 1288.15/s  (0.792s, 1293.65/s)  LR: 1.513e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.536 (1.536)  Loss:  0.7516 (0.7516)  Acc@1: 90.8203 (90.8203)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.172 (0.572)  Loss:  0.8064 (1.3160)  Acc@1: 86.6745 (76.3500)  Acc@5: 97.9953 (93.0680)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-226.pth.tar', 76.35000002929688)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-225.pth.tar', 76.13800000732422)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-224.pth.tar', 76.10000000488282)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-221.pth.tar', 75.98800000732422)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-223.pth.tar', 75.9620001611328)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-222.pth.tar', 75.88400018554688)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-214.pth.tar', 75.86000010986328)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-219.pth.tar', 75.8399999584961)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-220.pth.tar', 75.79200003173828)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-217.pth.tar', 75.76600002929688)

Train: 227 [   0/1251 (  0%)]  Loss: 3.542 (3.54)  Time: 2.349s,  435.91/s  (2.349s,  435.91/s)  LR: 1.477e-04  Data: 1.589 (1.589)
Train: 227 [  50/1251 (  4%)]  Loss: 3.570 (3.56)  Time: 0.779s, 1314.06/s  (0.829s, 1235.37/s)  LR: 1.477e-04  Data: 0.011 (0.046)
Train: 227 [ 100/1251 (  8%)]  Loss: 3.697 (3.60)  Time: 0.781s, 1311.49/s  (0.807s, 1268.22/s)  LR: 1.477e-04  Data: 0.011 (0.029)
Train: 227 [ 150/1251 ( 12%)]  Loss: 3.653 (3.62)  Time: 0.779s, 1313.94/s  (0.798s, 1282.56/s)  LR: 1.477e-04  Data: 0.011 (0.023)
Train: 227 [ 200/1251 ( 16%)]  Loss: 3.383 (3.57)  Time: 0.784s, 1305.34/s  (0.801s, 1278.48/s)  LR: 1.477e-04  Data: 0.011 (0.020)
Train: 227 [ 250/1251 ( 20%)]  Loss: 3.297 (3.52)  Time: 0.779s, 1314.01/s  (0.798s, 1283.47/s)  LR: 1.477e-04  Data: 0.011 (0.018)
Train: 227 [ 300/1251 ( 24%)]  Loss: 3.416 (3.51)  Time: 0.777s, 1317.28/s  (0.795s, 1287.41/s)  LR: 1.477e-04  Data: 0.011 (0.017)
Train: 227 [ 350/1251 ( 28%)]  Loss: 3.823 (3.55)  Time: 0.777s, 1317.79/s  (0.794s, 1290.10/s)  LR: 1.477e-04  Data: 0.011 (0.016)
Train: 227 [ 400/1251 ( 32%)]  Loss: 3.691 (3.56)  Time: 0.816s, 1254.55/s  (0.793s, 1291.44/s)  LR: 1.477e-04  Data: 0.011 (0.016)
Train: 227 [ 450/1251 ( 36%)]  Loss: 3.129 (3.52)  Time: 0.777s, 1317.74/s  (0.792s, 1292.97/s)  LR: 1.477e-04  Data: 0.011 (0.015)
Train: 227 [ 500/1251 ( 40%)]  Loss: 3.089 (3.48)  Time: 0.778s, 1315.95/s  (0.791s, 1294.22/s)  LR: 1.477e-04  Data: 0.011 (0.015)
Train: 227 [ 550/1251 ( 44%)]  Loss: 3.820 (3.51)  Time: 0.779s, 1314.99/s  (0.791s, 1295.24/s)  LR: 1.477e-04  Data: 0.011 (0.014)
Train: 227 [ 600/1251 ( 48%)]  Loss: 3.464 (3.51)  Time: 0.778s, 1316.78/s  (0.790s, 1296.11/s)  LR: 1.477e-04  Data: 0.011 (0.014)
Train: 227 [ 650/1251 ( 52%)]  Loss: 3.385 (3.50)  Time: 0.781s, 1311.68/s  (0.789s, 1297.18/s)  LR: 1.477e-04  Data: 0.011 (0.014)
Train: 227 [ 700/1251 ( 56%)]  Loss: 3.332 (3.49)  Time: 0.777s, 1317.28/s  (0.789s, 1298.18/s)  LR: 1.477e-04  Data: 0.011 (0.014)
Train: 227 [ 750/1251 ( 60%)]  Loss: 3.120 (3.46)  Time: 0.782s, 1309.30/s  (0.789s, 1298.38/s)  LR: 1.477e-04  Data: 0.011 (0.014)
Train: 227 [ 800/1251 ( 64%)]  Loss: 3.885 (3.49)  Time: 0.780s, 1313.28/s  (0.788s, 1298.97/s)  LR: 1.477e-04  Data: 0.012 (0.013)
Train: 227 [ 850/1251 ( 68%)]  Loss: 3.594 (3.49)  Time: 0.780s, 1313.36/s  (0.788s, 1299.42/s)  LR: 1.477e-04  Data: 0.011 (0.013)
Train: 227 [ 900/1251 ( 72%)]  Loss: 3.317 (3.48)  Time: 0.778s, 1316.76/s  (0.788s, 1299.57/s)  LR: 1.477e-04  Data: 0.011 (0.013)
Train: 227 [ 950/1251 ( 76%)]  Loss: 3.368 (3.48)  Time: 0.783s, 1307.37/s  (0.788s, 1299.80/s)  LR: 1.477e-04  Data: 0.011 (0.013)
Train: 227 [1000/1251 ( 80%)]  Loss: 3.614 (3.49)  Time: 0.777s, 1318.24/s  (0.788s, 1299.19/s)  LR: 1.477e-04  Data: 0.011 (0.013)
Train: 227 [1050/1251 ( 84%)]  Loss: 3.709 (3.50)  Time: 0.778s, 1315.43/s  (0.788s, 1299.32/s)  LR: 1.477e-04  Data: 0.011 (0.013)
Train: 227 [1100/1251 ( 88%)]  Loss: 3.651 (3.50)  Time: 0.779s, 1314.27/s  (0.788s, 1299.67/s)  LR: 1.477e-04  Data: 0.011 (0.013)
Train: 227 [1150/1251 ( 92%)]  Loss: 3.453 (3.50)  Time: 0.778s, 1316.07/s  (0.788s, 1300.08/s)  LR: 1.477e-04  Data: 0.011 (0.013)
Train: 227 [1200/1251 ( 96%)]  Loss: 3.411 (3.50)  Time: 0.779s, 1314.95/s  (0.787s, 1300.58/s)  LR: 1.477e-04  Data: 0.011 (0.013)
Train: 227 [1250/1251 (100%)]  Loss: 3.467 (3.50)  Time: 0.769s, 1331.93/s  (0.787s, 1300.83/s)  LR: 1.477e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.600 (1.600)  Loss:  0.7272 (0.7272)  Acc@1: 90.8203 (90.8203)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.172 (0.558)  Loss:  0.8556 (1.2727)  Acc@1: 86.3208 (76.2560)  Acc@5: 97.6415 (93.2280)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-226.pth.tar', 76.35000002929688)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-227.pth.tar', 76.25600013427734)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-225.pth.tar', 76.13800000732422)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-224.pth.tar', 76.10000000488282)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-221.pth.tar', 75.98800000732422)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-223.pth.tar', 75.9620001611328)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-222.pth.tar', 75.88400018554688)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-214.pth.tar', 75.86000010986328)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-219.pth.tar', 75.8399999584961)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-220.pth.tar', 75.79200003173828)

Train: 228 [   0/1251 (  0%)]  Loss: 3.560 (3.56)  Time: 2.331s,  439.28/s  (2.331s,  439.28/s)  LR: 1.442e-04  Data: 1.595 (1.595)
Train: 228 [  50/1251 (  4%)]  Loss: 3.718 (3.64)  Time: 0.781s, 1311.64/s  (0.816s, 1255.33/s)  LR: 1.442e-04  Data: 0.011 (0.045)
Train: 228 [ 100/1251 (  8%)]  Loss: 3.595 (3.62)  Time: 0.778s, 1315.98/s  (0.808s, 1267.53/s)  LR: 1.442e-04  Data: 0.011 (0.028)
Train: 228 [ 150/1251 ( 12%)]  Loss: 3.520 (3.60)  Time: 0.775s, 1321.87/s  (0.803s, 1275.74/s)  LR: 1.442e-04  Data: 0.011 (0.023)
Train: 228 [ 200/1251 ( 16%)]  Loss: 3.392 (3.56)  Time: 0.814s, 1257.84/s  (0.806s, 1270.63/s)  LR: 1.442e-04  Data: 0.011 (0.020)
Train: 228 [ 250/1251 ( 20%)]  Loss: 3.298 (3.51)  Time: 0.779s, 1314.17/s  (0.804s, 1274.33/s)  LR: 1.442e-04  Data: 0.011 (0.018)
Train: 228 [ 300/1251 ( 24%)]  Loss: 3.368 (3.49)  Time: 0.780s, 1312.03/s  (0.800s, 1279.94/s)  LR: 1.442e-04  Data: 0.011 (0.017)
Train: 228 [ 350/1251 ( 28%)]  Loss: 3.296 (3.47)  Time: 0.820s, 1249.00/s  (0.799s, 1282.02/s)  LR: 1.442e-04  Data: 0.012 (0.016)
Train: 228 [ 400/1251 ( 32%)]  Loss: 3.386 (3.46)  Time: 0.783s, 1308.39/s  (0.797s, 1284.58/s)  LR: 1.442e-04  Data: 0.011 (0.015)
Train: 228 [ 450/1251 ( 36%)]  Loss: 3.449 (3.46)  Time: 0.814s, 1257.51/s  (0.797s, 1284.36/s)  LR: 1.442e-04  Data: 0.011 (0.015)
Train: 228 [ 500/1251 ( 40%)]  Loss: 3.326 (3.45)  Time: 0.843s, 1214.60/s  (0.798s, 1282.66/s)  LR: 1.442e-04  Data: 0.011 (0.015)
Train: 228 [ 550/1251 ( 44%)]  Loss: 3.564 (3.46)  Time: 0.777s, 1317.37/s  (0.797s, 1284.22/s)  LR: 1.442e-04  Data: 0.011 (0.014)
Train: 228 [ 600/1251 ( 48%)]  Loss: 3.421 (3.45)  Time: 0.789s, 1298.66/s  (0.798s, 1283.47/s)  LR: 1.442e-04  Data: 0.011 (0.014)
Train: 228 [ 650/1251 ( 52%)]  Loss: 3.678 (3.47)  Time: 0.778s, 1316.19/s  (0.797s, 1285.29/s)  LR: 1.442e-04  Data: 0.011 (0.014)
Train: 228 [ 700/1251 ( 56%)]  Loss: 3.144 (3.45)  Time: 0.814s, 1257.22/s  (0.797s, 1285.28/s)  LR: 1.442e-04  Data: 0.012 (0.014)
Train: 228 [ 750/1251 ( 60%)]  Loss: 3.496 (3.45)  Time: 0.778s, 1316.91/s  (0.797s, 1284.86/s)  LR: 1.442e-04  Data: 0.011 (0.013)
Train: 228 [ 800/1251 ( 64%)]  Loss: 3.575 (3.46)  Time: 0.814s, 1258.67/s  (0.796s, 1285.63/s)  LR: 1.442e-04  Data: 0.011 (0.013)
Train: 228 [ 850/1251 ( 68%)]  Loss: 3.514 (3.46)  Time: 0.779s, 1314.15/s  (0.796s, 1286.00/s)  LR: 1.442e-04  Data: 0.011 (0.013)
Train: 228 [ 900/1251 ( 72%)]  Loss: 3.500 (3.46)  Time: 0.777s, 1318.23/s  (0.795s, 1287.29/s)  LR: 1.442e-04  Data: 0.011 (0.013)
Train: 228 [ 950/1251 ( 76%)]  Loss: 3.470 (3.46)  Time: 0.780s, 1313.30/s  (0.795s, 1288.02/s)  LR: 1.442e-04  Data: 0.011 (0.013)
Train: 228 [1000/1251 ( 80%)]  Loss: 3.247 (3.45)  Time: 0.818s, 1251.40/s  (0.795s, 1288.05/s)  LR: 1.442e-04  Data: 0.011 (0.013)
Train: 228 [1050/1251 ( 84%)]  Loss: 3.311 (3.45)  Time: 0.782s, 1309.92/s  (0.795s, 1288.05/s)  LR: 1.442e-04  Data: 0.010 (0.013)
Train: 228 [1100/1251 ( 88%)]  Loss: 3.768 (3.46)  Time: 0.807s, 1268.54/s  (0.795s, 1288.31/s)  LR: 1.442e-04  Data: 0.011 (0.013)
Train: 228 [1150/1251 ( 92%)]  Loss: 3.797 (3.47)  Time: 0.813s, 1259.26/s  (0.795s, 1288.28/s)  LR: 1.442e-04  Data: 0.011 (0.013)
Train: 228 [1200/1251 ( 96%)]  Loss: 3.436 (3.47)  Time: 0.777s, 1318.21/s  (0.795s, 1288.04/s)  LR: 1.442e-04  Data: 0.011 (0.013)
Train: 228 [1250/1251 (100%)]  Loss: 3.470 (3.47)  Time: 0.767s, 1335.73/s  (0.796s, 1287.01/s)  LR: 1.442e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.612 (1.612)  Loss:  0.7138 (0.7138)  Acc@1: 90.5273 (90.5273)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.172 (0.565)  Loss:  0.8191 (1.2445)  Acc@1: 87.2641 (76.6260)  Acc@5: 97.7594 (93.3300)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-228.pth.tar', 76.62599989746094)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-226.pth.tar', 76.35000002929688)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-227.pth.tar', 76.25600013427734)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-225.pth.tar', 76.13800000732422)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-224.pth.tar', 76.10000000488282)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-221.pth.tar', 75.98800000732422)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-223.pth.tar', 75.9620001611328)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-222.pth.tar', 75.88400018554688)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-214.pth.tar', 75.86000010986328)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-219.pth.tar', 75.8399999584961)

Train: 229 [   0/1251 (  0%)]  Loss: 3.359 (3.36)  Time: 2.289s,  447.29/s  (2.289s,  447.29/s)  LR: 1.406e-04  Data: 1.553 (1.553)
Train: 229 [  50/1251 (  4%)]  Loss: 3.438 (3.40)  Time: 0.779s, 1313.75/s  (0.816s, 1255.12/s)  LR: 1.406e-04  Data: 0.010 (0.043)
Train: 229 [ 100/1251 (  8%)]  Loss: 3.275 (3.36)  Time: 0.777s, 1318.71/s  (0.801s, 1278.10/s)  LR: 1.406e-04  Data: 0.011 (0.027)
Train: 229 [ 150/1251 ( 12%)]  Loss: 3.485 (3.39)  Time: 0.779s, 1315.31/s  (0.794s, 1289.39/s)  LR: 1.406e-04  Data: 0.011 (0.022)
Train: 229 [ 200/1251 ( 16%)]  Loss: 3.435 (3.40)  Time: 0.778s, 1316.53/s  (0.791s, 1294.28/s)  LR: 1.406e-04  Data: 0.011 (0.019)
Train: 229 [ 250/1251 ( 20%)]  Loss: 3.348 (3.39)  Time: 0.777s, 1318.00/s  (0.792s, 1292.82/s)  LR: 1.406e-04  Data: 0.011 (0.018)
Train: 229 [ 300/1251 ( 24%)]  Loss: 3.556 (3.41)  Time: 0.779s, 1314.06/s  (0.790s, 1296.28/s)  LR: 1.406e-04  Data: 0.012 (0.016)
Train: 229 [ 350/1251 ( 28%)]  Loss: 3.609 (3.44)  Time: 0.784s, 1306.35/s  (0.789s, 1297.86/s)  LR: 1.406e-04  Data: 0.011 (0.016)
Train: 229 [ 400/1251 ( 32%)]  Loss: 3.627 (3.46)  Time: 0.779s, 1315.27/s  (0.788s, 1299.01/s)  LR: 1.406e-04  Data: 0.011 (0.015)
Train: 229 [ 450/1251 ( 36%)]  Loss: 3.518 (3.47)  Time: 0.778s, 1316.71/s  (0.788s, 1300.18/s)  LR: 1.406e-04  Data: 0.011 (0.015)
Train: 229 [ 500/1251 ( 40%)]  Loss: 3.204 (3.44)  Time: 0.775s, 1321.80/s  (0.787s, 1300.84/s)  LR: 1.406e-04  Data: 0.011 (0.014)
Train: 229 [ 550/1251 ( 44%)]  Loss: 3.876 (3.48)  Time: 0.817s, 1253.26/s  (0.788s, 1299.23/s)  LR: 1.406e-04  Data: 0.011 (0.014)
Train: 229 [ 600/1251 ( 48%)]  Loss: 3.817 (3.50)  Time: 0.776s, 1320.19/s  (0.789s, 1297.67/s)  LR: 1.406e-04  Data: 0.010 (0.014)
Train: 229 [ 650/1251 ( 52%)]  Loss: 3.323 (3.49)  Time: 0.782s, 1309.12/s  (0.789s, 1298.01/s)  LR: 1.406e-04  Data: 0.010 (0.014)
Train: 229 [ 700/1251 ( 56%)]  Loss: 3.729 (3.51)  Time: 0.778s, 1315.71/s  (0.790s, 1295.87/s)  LR: 1.406e-04  Data: 0.011 (0.013)
Train: 229 [ 750/1251 ( 60%)]  Loss: 3.085 (3.48)  Time: 0.813s, 1258.77/s  (0.790s, 1295.42/s)  LR: 1.406e-04  Data: 0.011 (0.013)
Train: 229 [ 800/1251 ( 64%)]  Loss: 3.387 (3.47)  Time: 0.786s, 1302.08/s  (0.790s, 1295.40/s)  LR: 1.406e-04  Data: 0.011 (0.013)
Train: 229 [ 850/1251 ( 68%)]  Loss: 3.428 (3.47)  Time: 0.776s, 1319.04/s  (0.790s, 1296.18/s)  LR: 1.406e-04  Data: 0.011 (0.013)
Train: 229 [ 900/1251 ( 72%)]  Loss: 3.806 (3.49)  Time: 0.780s, 1312.62/s  (0.790s, 1295.66/s)  LR: 1.406e-04  Data: 0.011 (0.013)
Train: 229 [ 950/1251 ( 76%)]  Loss: 3.671 (3.50)  Time: 0.818s, 1252.43/s  (0.790s, 1295.61/s)  LR: 1.406e-04  Data: 0.011 (0.013)
Train: 229 [1000/1251 ( 80%)]  Loss: 3.650 (3.51)  Time: 0.774s, 1322.77/s  (0.791s, 1294.58/s)  LR: 1.406e-04  Data: 0.011 (0.013)
Train: 229 [1050/1251 ( 84%)]  Loss: 3.337 (3.50)  Time: 0.775s, 1321.37/s  (0.791s, 1295.08/s)  LR: 1.406e-04  Data: 0.011 (0.013)
Train: 229 [1100/1251 ( 88%)]  Loss: 3.747 (3.51)  Time: 0.779s, 1314.77/s  (0.790s, 1295.55/s)  LR: 1.406e-04  Data: 0.011 (0.013)
Train: 229 [1150/1251 ( 92%)]  Loss: 3.731 (3.52)  Time: 0.820s, 1248.34/s  (0.790s, 1295.80/s)  LR: 1.406e-04  Data: 0.011 (0.012)
Train: 229 [1200/1251 ( 96%)]  Loss: 3.731 (3.53)  Time: 0.820s, 1248.80/s  (0.791s, 1295.06/s)  LR: 1.406e-04  Data: 0.012 (0.012)
Train: 229 [1250/1251 (100%)]  Loss: 3.677 (3.53)  Time: 0.772s, 1326.75/s  (0.791s, 1295.03/s)  LR: 1.406e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.573 (1.573)  Loss:  0.7553 (0.7553)  Acc@1: 90.3320 (90.3320)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.172 (0.564)  Loss:  0.8175 (1.3088)  Acc@1: 86.4387 (76.1360)  Acc@5: 97.1698 (92.8980)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-228.pth.tar', 76.62599989746094)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-226.pth.tar', 76.35000002929688)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-227.pth.tar', 76.25600013427734)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-225.pth.tar', 76.13800000732422)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-229.pth.tar', 76.13599992675782)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-224.pth.tar', 76.10000000488282)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-221.pth.tar', 75.98800000732422)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-223.pth.tar', 75.9620001611328)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-222.pth.tar', 75.88400018554688)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-214.pth.tar', 75.86000010986328)

Train: 230 [   0/1251 (  0%)]  Loss: 3.587 (3.59)  Time: 2.190s,  467.64/s  (2.190s,  467.64/s)  LR: 1.371e-04  Data: 1.455 (1.455)
Train: 230 [  50/1251 (  4%)]  Loss: 3.628 (3.61)  Time: 0.780s, 1312.68/s  (0.816s, 1254.22/s)  LR: 1.371e-04  Data: 0.011 (0.044)
Train: 230 [ 100/1251 (  8%)]  Loss: 3.329 (3.51)  Time: 0.836s, 1224.93/s  (0.803s, 1275.03/s)  LR: 1.371e-04  Data: 0.011 (0.028)
Train: 230 [ 150/1251 ( 12%)]  Loss: 3.666 (3.55)  Time: 0.835s, 1225.64/s  (0.797s, 1284.47/s)  LR: 1.371e-04  Data: 0.010 (0.022)
Train: 230 [ 200/1251 ( 16%)]  Loss: 3.496 (3.54)  Time: 0.817s, 1254.09/s  (0.802s, 1277.55/s)  LR: 1.371e-04  Data: 0.012 (0.020)
Train: 230 [ 250/1251 ( 20%)]  Loss: 3.469 (3.53)  Time: 0.778s, 1315.69/s  (0.798s, 1283.70/s)  LR: 1.371e-04  Data: 0.011 (0.018)
Train: 230 [ 300/1251 ( 24%)]  Loss: 3.437 (3.52)  Time: 0.816s, 1255.28/s  (0.799s, 1281.79/s)  LR: 1.371e-04  Data: 0.011 (0.017)
Train: 230 [ 350/1251 ( 28%)]  Loss: 3.416 (3.50)  Time: 0.782s, 1308.91/s  (0.798s, 1283.44/s)  LR: 1.371e-04  Data: 0.011 (0.016)
Train: 230 [ 400/1251 ( 32%)]  Loss: 3.275 (3.48)  Time: 0.778s, 1315.61/s  (0.796s, 1286.45/s)  LR: 1.371e-04  Data: 0.011 (0.015)
Train: 230 [ 450/1251 ( 36%)]  Loss: 3.367 (3.47)  Time: 0.785s, 1305.21/s  (0.795s, 1288.49/s)  LR: 1.371e-04  Data: 0.011 (0.015)
Train: 230 [ 500/1251 ( 40%)]  Loss: 3.077 (3.43)  Time: 0.817s, 1253.33/s  (0.794s, 1289.50/s)  LR: 1.371e-04  Data: 0.014 (0.015)
Train: 230 [ 550/1251 ( 44%)]  Loss: 3.306 (3.42)  Time: 0.816s, 1255.16/s  (0.795s, 1287.35/s)  LR: 1.371e-04  Data: 0.012 (0.014)
Train: 230 [ 600/1251 ( 48%)]  Loss: 3.808 (3.45)  Time: 0.786s, 1303.13/s  (0.795s, 1287.29/s)  LR: 1.371e-04  Data: 0.018 (0.014)
Train: 230 [ 650/1251 ( 52%)]  Loss: 3.279 (3.44)  Time: 0.777s, 1317.53/s  (0.795s, 1288.60/s)  LR: 1.371e-04  Data: 0.011 (0.014)
Train: 230 [ 700/1251 ( 56%)]  Loss: 3.446 (3.44)  Time: 0.779s, 1314.04/s  (0.794s, 1289.76/s)  LR: 1.371e-04  Data: 0.011 (0.014)
Train: 230 [ 750/1251 ( 60%)]  Loss: 3.740 (3.46)  Time: 0.811s, 1262.09/s  (0.795s, 1288.78/s)  LR: 1.371e-04  Data: 0.011 (0.014)
Train: 230 [ 800/1251 ( 64%)]  Loss: 3.465 (3.46)  Time: 0.811s, 1262.08/s  (0.795s, 1287.38/s)  LR: 1.371e-04  Data: 0.011 (0.013)
Train: 230 [ 850/1251 ( 68%)]  Loss: 3.236 (3.45)  Time: 0.777s, 1318.02/s  (0.795s, 1288.23/s)  LR: 1.371e-04  Data: 0.011 (0.013)
Train: 230 [ 900/1251 ( 72%)]  Loss: 3.158 (3.43)  Time: 0.820s, 1248.59/s  (0.795s, 1287.53/s)  LR: 1.371e-04  Data: 0.011 (0.013)
Train: 230 [ 950/1251 ( 76%)]  Loss: 3.623 (3.44)  Time: 0.777s, 1318.74/s  (0.795s, 1287.38/s)  LR: 1.371e-04  Data: 0.010 (0.013)
Train: 230 [1000/1251 ( 80%)]  Loss: 3.424 (3.44)  Time: 0.811s, 1262.30/s  (0.795s, 1287.44/s)  LR: 1.371e-04  Data: 0.011 (0.013)
Train: 230 [1050/1251 ( 84%)]  Loss: 3.727 (3.45)  Time: 0.814s, 1258.39/s  (0.796s, 1287.01/s)  LR: 1.371e-04  Data: 0.011 (0.013)
Train: 230 [1100/1251 ( 88%)]  Loss: 3.341 (3.45)  Time: 0.838s, 1222.18/s  (0.796s, 1287.06/s)  LR: 1.371e-04  Data: 0.011 (0.013)
Train: 230 [1150/1251 ( 92%)]  Loss: 3.409 (3.45)  Time: 0.789s, 1297.92/s  (0.795s, 1287.39/s)  LR: 1.371e-04  Data: 0.011 (0.013)
Train: 230 [1200/1251 ( 96%)]  Loss: 3.502 (3.45)  Time: 0.778s, 1315.38/s  (0.795s, 1288.18/s)  LR: 1.371e-04  Data: 0.011 (0.013)
Train: 230 [1250/1251 (100%)]  Loss: 3.583 (3.45)  Time: 0.802s, 1277.49/s  (0.795s, 1288.06/s)  LR: 1.371e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.591 (1.591)  Loss:  0.6935 (0.6935)  Acc@1: 90.7227 (90.7227)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.172 (0.561)  Loss:  0.8449 (1.2934)  Acc@1: 85.9670 (76.5820)  Acc@5: 96.8160 (93.2540)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-228.pth.tar', 76.62599989746094)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-230.pth.tar', 76.58199998046875)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-226.pth.tar', 76.35000002929688)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-227.pth.tar', 76.25600013427734)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-225.pth.tar', 76.13800000732422)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-229.pth.tar', 76.13599992675782)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-224.pth.tar', 76.10000000488282)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-221.pth.tar', 75.98800000732422)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-223.pth.tar', 75.9620001611328)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-222.pth.tar', 75.88400018554688)

Train: 231 [   0/1251 (  0%)]  Loss: 3.225 (3.23)  Time: 2.215s,  462.38/s  (2.215s,  462.38/s)  LR: 1.337e-04  Data: 1.480 (1.480)
Train: 231 [  50/1251 (  4%)]  Loss: 3.529 (3.38)  Time: 0.777s, 1317.82/s  (0.825s, 1240.78/s)  LR: 1.337e-04  Data: 0.011 (0.046)
Train: 231 [ 100/1251 (  8%)]  Loss: 3.364 (3.37)  Time: 0.781s, 1310.60/s  (0.811s, 1262.32/s)  LR: 1.337e-04  Data: 0.011 (0.029)
Train: 231 [ 150/1251 ( 12%)]  Loss: 3.798 (3.48)  Time: 0.778s, 1315.46/s  (0.804s, 1273.05/s)  LR: 1.337e-04  Data: 0.010 (0.023)
Train: 231 [ 200/1251 ( 16%)]  Loss: 3.313 (3.45)  Time: 0.778s, 1315.40/s  (0.799s, 1281.71/s)  LR: 1.337e-04  Data: 0.011 (0.020)
Train: 231 [ 250/1251 ( 20%)]  Loss: 3.693 (3.49)  Time: 0.776s, 1319.27/s  (0.797s, 1285.11/s)  LR: 1.337e-04  Data: 0.011 (0.018)
Train: 231 [ 300/1251 ( 24%)]  Loss: 3.357 (3.47)  Time: 0.802s, 1276.17/s  (0.800s, 1280.17/s)  LR: 1.337e-04  Data: 0.010 (0.017)
Train: 231 [ 350/1251 ( 28%)]  Loss: 3.384 (3.46)  Time: 0.780s, 1313.40/s  (0.798s, 1283.11/s)  LR: 1.337e-04  Data: 0.011 (0.016)
Train: 231 [ 400/1251 ( 32%)]  Loss: 3.555 (3.47)  Time: 0.785s, 1304.06/s  (0.797s, 1285.60/s)  LR: 1.337e-04  Data: 0.011 (0.016)
Train: 231 [ 450/1251 ( 36%)]  Loss: 3.572 (3.48)  Time: 0.828s, 1236.24/s  (0.796s, 1287.07/s)  LR: 1.337e-04  Data: 0.011 (0.015)
Train: 231 [ 500/1251 ( 40%)]  Loss: 3.147 (3.45)  Time: 0.817s, 1253.42/s  (0.795s, 1288.58/s)  LR: 1.337e-04  Data: 0.011 (0.015)
Train: 231 [ 550/1251 ( 44%)]  Loss: 3.814 (3.48)  Time: 0.818s, 1251.33/s  (0.797s, 1285.31/s)  LR: 1.337e-04  Data: 0.011 (0.014)
Train: 231 [ 600/1251 ( 48%)]  Loss: 3.158 (3.45)  Time: 0.780s, 1313.48/s  (0.798s, 1283.74/s)  LR: 1.337e-04  Data: 0.011 (0.014)
Train: 231 [ 650/1251 ( 52%)]  Loss: 3.683 (3.47)  Time: 0.779s, 1314.88/s  (0.797s, 1285.41/s)  LR: 1.337e-04  Data: 0.012 (0.014)
Train: 231 [ 700/1251 ( 56%)]  Loss: 3.628 (3.48)  Time: 0.823s, 1243.77/s  (0.796s, 1286.73/s)  LR: 1.337e-04  Data: 0.010 (0.014)
Train: 231 [ 750/1251 ( 60%)]  Loss: 3.346 (3.47)  Time: 0.781s, 1311.80/s  (0.796s, 1286.03/s)  LR: 1.337e-04  Data: 0.011 (0.014)
Train: 231 [ 800/1251 ( 64%)]  Loss: 3.454 (3.47)  Time: 0.778s, 1316.30/s  (0.795s, 1287.42/s)  LR: 1.337e-04  Data: 0.012 (0.013)
Train: 231 [ 850/1251 ( 68%)]  Loss: 3.392 (3.47)  Time: 0.781s, 1310.46/s  (0.795s, 1288.63/s)  LR: 1.337e-04  Data: 0.011 (0.013)
Train: 231 [ 900/1251 ( 72%)]  Loss: 3.515 (3.47)  Time: 0.811s, 1262.31/s  (0.794s, 1289.00/s)  LR: 1.337e-04  Data: 0.011 (0.013)
Train: 231 [ 950/1251 ( 76%)]  Loss: 3.626 (3.48)  Time: 0.817s, 1253.01/s  (0.795s, 1287.69/s)  LR: 1.337e-04  Data: 0.011 (0.013)
Train: 231 [1000/1251 ( 80%)]  Loss: 3.384 (3.47)  Time: 0.779s, 1314.71/s  (0.795s, 1288.50/s)  LR: 1.337e-04  Data: 0.011 (0.013)
Train: 231 [1050/1251 ( 84%)]  Loss: 3.123 (3.46)  Time: 0.779s, 1314.92/s  (0.795s, 1288.01/s)  LR: 1.337e-04  Data: 0.012 (0.013)
Train: 231 [1100/1251 ( 88%)]  Loss: 3.269 (3.45)  Time: 0.818s, 1251.14/s  (0.794s, 1288.90/s)  LR: 1.337e-04  Data: 0.011 (0.013)
Train: 231 [1150/1251 ( 92%)]  Loss: 3.419 (3.45)  Time: 0.781s, 1310.90/s  (0.795s, 1288.25/s)  LR: 1.337e-04  Data: 0.011 (0.013)
Train: 231 [1200/1251 ( 96%)]  Loss: 3.433 (3.45)  Time: 0.780s, 1312.85/s  (0.795s, 1287.59/s)  LR: 1.337e-04  Data: 0.011 (0.013)
Train: 231 [1250/1251 (100%)]  Loss: 3.711 (3.46)  Time: 0.767s, 1334.63/s  (0.795s, 1288.01/s)  LR: 1.337e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.498 (1.498)  Loss:  0.7962 (0.7962)  Acc@1: 90.9180 (90.9180)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.172 (0.560)  Loss:  0.8185 (1.3193)  Acc@1: 85.6132 (76.7100)  Acc@5: 97.9953 (93.3580)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-231.pth.tar', 76.71000008544922)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-228.pth.tar', 76.62599989746094)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-230.pth.tar', 76.58199998046875)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-226.pth.tar', 76.35000002929688)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-227.pth.tar', 76.25600013427734)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-225.pth.tar', 76.13800000732422)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-229.pth.tar', 76.13599992675782)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-224.pth.tar', 76.10000000488282)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-221.pth.tar', 75.98800000732422)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-223.pth.tar', 75.9620001611328)

Train: 232 [   0/1251 (  0%)]  Loss: 3.404 (3.40)  Time: 2.236s,  458.02/s  (2.236s,  458.02/s)  LR: 1.303e-04  Data: 1.500 (1.500)
Train: 232 [  50/1251 (  4%)]  Loss: 3.201 (3.30)  Time: 0.787s, 1301.18/s  (0.837s, 1223.45/s)  LR: 1.303e-04  Data: 0.011 (0.043)
Train: 232 [ 100/1251 (  8%)]  Loss: 3.682 (3.43)  Time: 0.778s, 1316.30/s  (0.811s, 1262.44/s)  LR: 1.303e-04  Data: 0.011 (0.027)
Train: 232 [ 150/1251 ( 12%)]  Loss: 3.053 (3.33)  Time: 0.780s, 1312.58/s  (0.803s, 1275.59/s)  LR: 1.303e-04  Data: 0.011 (0.022)
Train: 232 [ 200/1251 ( 16%)]  Loss: 3.369 (3.34)  Time: 0.819s, 1250.18/s  (0.800s, 1280.03/s)  LR: 1.303e-04  Data: 0.011 (0.019)
Train: 232 [ 250/1251 ( 20%)]  Loss: 3.363 (3.35)  Time: 0.778s, 1316.85/s  (0.797s, 1284.50/s)  LR: 1.303e-04  Data: 0.011 (0.018)
Train: 232 [ 300/1251 ( 24%)]  Loss: 3.390 (3.35)  Time: 0.813s, 1259.35/s  (0.796s, 1285.95/s)  LR: 1.303e-04  Data: 0.011 (0.017)
Train: 232 [ 350/1251 ( 28%)]  Loss: 3.276 (3.34)  Time: 0.776s, 1319.19/s  (0.796s, 1286.80/s)  LR: 1.303e-04  Data: 0.011 (0.016)
Train: 232 [ 400/1251 ( 32%)]  Loss: 3.445 (3.35)  Time: 0.782s, 1308.70/s  (0.795s, 1287.48/s)  LR: 1.303e-04  Data: 0.011 (0.015)
Train: 232 [ 450/1251 ( 36%)]  Loss: 3.691 (3.39)  Time: 0.848s, 1208.09/s  (0.796s, 1286.82/s)  LR: 1.303e-04  Data: 0.011 (0.015)
Train: 232 [ 500/1251 ( 40%)]  Loss: 2.853 (3.34)  Time: 0.779s, 1315.25/s  (0.795s, 1287.99/s)  LR: 1.303e-04  Data: 0.011 (0.014)
Train: 232 [ 550/1251 ( 44%)]  Loss: 3.528 (3.35)  Time: 0.815s, 1255.71/s  (0.795s, 1288.13/s)  LR: 1.303e-04  Data: 0.012 (0.014)
Train: 232 [ 600/1251 ( 48%)]  Loss: 3.469 (3.36)  Time: 0.815s, 1256.58/s  (0.796s, 1285.85/s)  LR: 1.303e-04  Data: 0.010 (0.014)
Train: 232 [ 650/1251 ( 52%)]  Loss: 3.591 (3.38)  Time: 0.778s, 1316.44/s  (0.796s, 1286.12/s)  LR: 1.303e-04  Data: 0.011 (0.014)
Train: 232 [ 700/1251 ( 56%)]  Loss: 3.675 (3.40)  Time: 0.780s, 1312.19/s  (0.795s, 1287.83/s)  LR: 1.303e-04  Data: 0.011 (0.013)
Train: 232 [ 750/1251 ( 60%)]  Loss: 3.497 (3.41)  Time: 0.787s, 1301.31/s  (0.795s, 1288.50/s)  LR: 1.303e-04  Data: 0.011 (0.013)
Train: 232 [ 800/1251 ( 64%)]  Loss: 3.295 (3.40)  Time: 0.826s, 1239.31/s  (0.795s, 1288.42/s)  LR: 1.303e-04  Data: 0.012 (0.013)
Train: 232 [ 850/1251 ( 68%)]  Loss: 3.693 (3.42)  Time: 0.815s, 1256.52/s  (0.794s, 1289.12/s)  LR: 1.303e-04  Data: 0.012 (0.013)
Train: 232 [ 900/1251 ( 72%)]  Loss: 3.267 (3.41)  Time: 0.779s, 1314.47/s  (0.794s, 1289.99/s)  LR: 1.303e-04  Data: 0.011 (0.013)
Train: 232 [ 950/1251 ( 76%)]  Loss: 3.601 (3.42)  Time: 0.807s, 1269.39/s  (0.793s, 1291.00/s)  LR: 1.303e-04  Data: 0.011 (0.013)
Train: 232 [1000/1251 ( 80%)]  Loss: 3.437 (3.42)  Time: 0.779s, 1315.05/s  (0.793s, 1290.92/s)  LR: 1.303e-04  Data: 0.011 (0.013)
Train: 232 [1050/1251 ( 84%)]  Loss: 3.052 (3.40)  Time: 0.776s, 1320.28/s  (0.794s, 1290.19/s)  LR: 1.303e-04  Data: 0.011 (0.013)
Train: 232 [1100/1251 ( 88%)]  Loss: 3.595 (3.41)  Time: 0.800s, 1279.46/s  (0.794s, 1289.84/s)  LR: 1.303e-04  Data: 0.011 (0.013)
Train: 232 [1150/1251 ( 92%)]  Loss: 3.704 (3.42)  Time: 0.779s, 1315.33/s  (0.793s, 1290.81/s)  LR: 1.303e-04  Data: 0.011 (0.013)
Train: 232 [1200/1251 ( 96%)]  Loss: 3.435 (3.42)  Time: 0.777s, 1317.28/s  (0.793s, 1291.19/s)  LR: 1.303e-04  Data: 0.011 (0.012)
Train: 232 [1250/1251 (100%)]  Loss: 3.026 (3.41)  Time: 0.796s, 1285.69/s  (0.793s, 1290.88/s)  LR: 1.303e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.509 (1.509)  Loss:  0.7047 (0.7047)  Acc@1: 90.8203 (90.8203)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.172 (0.558)  Loss:  0.8602 (1.2606)  Acc@1: 85.0236 (76.4980)  Acc@5: 97.0519 (93.1760)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-231.pth.tar', 76.71000008544922)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-228.pth.tar', 76.62599989746094)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-230.pth.tar', 76.58199998046875)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-232.pth.tar', 76.49800008789063)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-226.pth.tar', 76.35000002929688)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-227.pth.tar', 76.25600013427734)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-225.pth.tar', 76.13800000732422)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-229.pth.tar', 76.13599992675782)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-224.pth.tar', 76.10000000488282)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-221.pth.tar', 75.98800000732422)

Train: 233 [   0/1251 (  0%)]  Loss: 3.318 (3.32)  Time: 2.164s,  473.13/s  (2.164s,  473.13/s)  LR: 1.269e-04  Data: 1.427 (1.427)
Train: 233 [  50/1251 (  4%)]  Loss: 3.269 (3.29)  Time: 0.777s, 1317.21/s  (0.819s, 1250.65/s)  LR: 1.269e-04  Data: 0.011 (0.046)
Train: 233 [ 100/1251 (  8%)]  Loss: 3.365 (3.32)  Time: 0.791s, 1293.80/s  (0.807s, 1268.39/s)  LR: 1.269e-04  Data: 0.011 (0.029)
Train: 233 [ 150/1251 ( 12%)]  Loss: 3.582 (3.38)  Time: 0.779s, 1315.09/s  (0.806s, 1269.93/s)  LR: 1.269e-04  Data: 0.011 (0.023)
Train: 233 [ 200/1251 ( 16%)]  Loss: 3.249 (3.36)  Time: 0.779s, 1315.22/s  (0.800s, 1279.43/s)  LR: 1.269e-04  Data: 0.011 (0.020)
Train: 233 [ 250/1251 ( 20%)]  Loss: 3.459 (3.37)  Time: 0.814s, 1257.60/s  (0.798s, 1282.61/s)  LR: 1.269e-04  Data: 0.011 (0.018)
Train: 233 [ 300/1251 ( 24%)]  Loss: 3.071 (3.33)  Time: 0.834s, 1228.30/s  (0.802s, 1277.54/s)  LR: 1.269e-04  Data: 0.010 (0.017)
Train: 233 [ 350/1251 ( 28%)]  Loss: 3.344 (3.33)  Time: 0.819s, 1250.60/s  (0.800s, 1280.15/s)  LR: 1.269e-04  Data: 0.011 (0.016)
Train: 233 [ 400/1251 ( 32%)]  Loss: 3.900 (3.40)  Time: 0.784s, 1305.83/s  (0.799s, 1281.26/s)  LR: 1.269e-04  Data: 0.011 (0.016)
Train: 233 [ 450/1251 ( 36%)]  Loss: 3.267 (3.38)  Time: 0.778s, 1316.84/s  (0.801s, 1279.00/s)  LR: 1.269e-04  Data: 0.012 (0.015)
Train: 233 [ 500/1251 ( 40%)]  Loss: 3.310 (3.38)  Time: 0.775s, 1321.01/s  (0.799s, 1281.77/s)  LR: 1.269e-04  Data: 0.011 (0.015)
Train: 233 [ 550/1251 ( 44%)]  Loss: 3.434 (3.38)  Time: 0.778s, 1315.53/s  (0.797s, 1284.38/s)  LR: 1.269e-04  Data: 0.011 (0.014)
Train: 233 [ 600/1251 ( 48%)]  Loss: 3.611 (3.40)  Time: 0.778s, 1316.23/s  (0.796s, 1286.08/s)  LR: 1.269e-04  Data: 0.011 (0.014)
Train: 233 [ 650/1251 ( 52%)]  Loss: 3.307 (3.39)  Time: 0.779s, 1314.91/s  (0.796s, 1287.17/s)  LR: 1.269e-04  Data: 0.011 (0.014)
Train: 233 [ 700/1251 ( 56%)]  Loss: 3.328 (3.39)  Time: 0.779s, 1314.78/s  (0.795s, 1288.32/s)  LR: 1.269e-04  Data: 0.011 (0.014)
Train: 233 [ 750/1251 ( 60%)]  Loss: 3.083 (3.37)  Time: 0.821s, 1247.61/s  (0.795s, 1288.66/s)  LR: 1.269e-04  Data: 0.012 (0.014)
Train: 233 [ 800/1251 ( 64%)]  Loss: 3.410 (3.37)  Time: 0.820s, 1248.43/s  (0.796s, 1285.94/s)  LR: 1.269e-04  Data: 0.011 (0.013)
Train: 233 [ 850/1251 ( 68%)]  Loss: 3.373 (3.37)  Time: 0.821s, 1246.64/s  (0.797s, 1284.36/s)  LR: 1.269e-04  Data: 0.014 (0.013)
Train: 233 [ 900/1251 ( 72%)]  Loss: 3.142 (3.36)  Time: 0.781s, 1310.95/s  (0.797s, 1285.33/s)  LR: 1.269e-04  Data: 0.011 (0.013)
Train: 233 [ 950/1251 ( 76%)]  Loss: 3.854 (3.38)  Time: 0.779s, 1314.77/s  (0.796s, 1286.46/s)  LR: 1.269e-04  Data: 0.011 (0.013)
Train: 233 [1000/1251 ( 80%)]  Loss: 3.779 (3.40)  Time: 0.778s, 1315.45/s  (0.796s, 1286.64/s)  LR: 1.269e-04  Data: 0.011 (0.013)
Train: 233 [1050/1251 ( 84%)]  Loss: 2.865 (3.38)  Time: 0.780s, 1312.65/s  (0.795s, 1287.84/s)  LR: 1.269e-04  Data: 0.011 (0.013)
Train: 233 [1100/1251 ( 88%)]  Loss: 3.136 (3.37)  Time: 0.808s, 1268.02/s  (0.795s, 1287.99/s)  LR: 1.269e-04  Data: 0.011 (0.013)
Train: 233 [1150/1251 ( 92%)]  Loss: 3.403 (3.37)  Time: 0.781s, 1311.25/s  (0.795s, 1288.22/s)  LR: 1.269e-04  Data: 0.011 (0.013)
Train: 233 [1200/1251 ( 96%)]  Loss: 3.795 (3.39)  Time: 0.779s, 1314.76/s  (0.794s, 1289.11/s)  LR: 1.269e-04  Data: 0.011 (0.013)
Train: 233 [1250/1251 (100%)]  Loss: 3.345 (3.38)  Time: 0.768s, 1333.13/s  (0.794s, 1289.94/s)  LR: 1.269e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.529 (1.529)  Loss:  0.7035 (0.7035)  Acc@1: 90.7227 (90.7227)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.172 (0.560)  Loss:  0.8165 (1.2525)  Acc@1: 85.3774 (76.4900)  Acc@5: 96.3443 (93.0640)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-231.pth.tar', 76.71000008544922)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-228.pth.tar', 76.62599989746094)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-230.pth.tar', 76.58199998046875)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-232.pth.tar', 76.49800008789063)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-233.pth.tar', 76.48999998291016)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-226.pth.tar', 76.35000002929688)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-227.pth.tar', 76.25600013427734)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-225.pth.tar', 76.13800000732422)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-229.pth.tar', 76.13599992675782)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-224.pth.tar', 76.10000000488282)

Train: 234 [   0/1251 (  0%)]  Loss: 3.556 (3.56)  Time: 2.217s,  461.89/s  (2.217s,  461.89/s)  LR: 1.236e-04  Data: 1.480 (1.480)
Train: 234 [  50/1251 (  4%)]  Loss: 3.001 (3.28)  Time: 0.787s, 1301.26/s  (0.826s, 1239.71/s)  LR: 1.236e-04  Data: 0.013 (0.045)
Train: 234 [ 100/1251 (  8%)]  Loss: 3.159 (3.24)  Time: 0.778s, 1316.57/s  (0.807s, 1268.89/s)  LR: 1.236e-04  Data: 0.011 (0.028)
Train: 234 [ 150/1251 ( 12%)]  Loss: 3.451 (3.29)  Time: 0.777s, 1317.74/s  (0.800s, 1280.42/s)  LR: 1.236e-04  Data: 0.011 (0.023)
Train: 234 [ 200/1251 ( 16%)]  Loss: 3.328 (3.30)  Time: 0.785s, 1304.73/s  (0.799s, 1281.57/s)  LR: 1.236e-04  Data: 0.013 (0.020)
Train: 234 [ 250/1251 ( 20%)]  Loss: 3.631 (3.35)  Time: 0.779s, 1314.50/s  (0.797s, 1285.15/s)  LR: 1.236e-04  Data: 0.011 (0.018)
Train: 234 [ 300/1251 ( 24%)]  Loss: 3.390 (3.36)  Time: 0.782s, 1309.63/s  (0.797s, 1284.07/s)  LR: 1.236e-04  Data: 0.010 (0.017)
Train: 234 [ 350/1251 ( 28%)]  Loss: 3.594 (3.39)  Time: 0.780s, 1312.20/s  (0.796s, 1286.81/s)  LR: 1.236e-04  Data: 0.011 (0.016)
Train: 234 [ 400/1251 ( 32%)]  Loss: 3.328 (3.38)  Time: 0.822s, 1245.51/s  (0.796s, 1287.01/s)  LR: 1.236e-04  Data: 0.011 (0.015)
Train: 234 [ 450/1251 ( 36%)]  Loss: 3.051 (3.35)  Time: 0.777s, 1317.81/s  (0.796s, 1286.88/s)  LR: 1.236e-04  Data: 0.011 (0.015)
Train: 234 [ 500/1251 ( 40%)]  Loss: 3.483 (3.36)  Time: 0.839s, 1221.06/s  (0.795s, 1288.78/s)  LR: 1.236e-04  Data: 0.010 (0.015)
Train: 234 [ 550/1251 ( 44%)]  Loss: 3.481 (3.37)  Time: 0.814s, 1258.28/s  (0.794s, 1289.00/s)  LR: 1.236e-04  Data: 0.011 (0.014)
Train: 234 [ 600/1251 ( 48%)]  Loss: 3.543 (3.38)  Time: 0.780s, 1312.24/s  (0.794s, 1289.85/s)  LR: 1.236e-04  Data: 0.011 (0.014)
Train: 234 [ 650/1251 ( 52%)]  Loss: 3.764 (3.41)  Time: 0.778s, 1316.85/s  (0.794s, 1290.25/s)  LR: 1.236e-04  Data: 0.011 (0.014)
Train: 234 [ 700/1251 ( 56%)]  Loss: 3.199 (3.40)  Time: 0.777s, 1317.55/s  (0.795s, 1287.63/s)  LR: 1.236e-04  Data: 0.011 (0.014)
Train: 234 [ 750/1251 ( 60%)]  Loss: 3.165 (3.38)  Time: 0.781s, 1311.65/s  (0.795s, 1287.71/s)  LR: 1.236e-04  Data: 0.013 (0.013)
Train: 234 [ 800/1251 ( 64%)]  Loss: 3.175 (3.37)  Time: 0.790s, 1295.82/s  (0.796s, 1286.93/s)  LR: 1.236e-04  Data: 0.010 (0.013)
Train: 234 [ 850/1251 ( 68%)]  Loss: 3.687 (3.39)  Time: 0.776s, 1319.13/s  (0.795s, 1287.60/s)  LR: 1.236e-04  Data: 0.011 (0.013)
Train: 234 [ 900/1251 ( 72%)]  Loss: 3.127 (3.37)  Time: 0.781s, 1311.82/s  (0.795s, 1288.14/s)  LR: 1.236e-04  Data: 0.011 (0.013)
Train: 234 [ 950/1251 ( 76%)]  Loss: 3.262 (3.37)  Time: 0.777s, 1318.56/s  (0.796s, 1286.97/s)  LR: 1.236e-04  Data: 0.011 (0.013)
Train: 234 [1000/1251 ( 80%)]  Loss: 2.973 (3.35)  Time: 0.777s, 1317.65/s  (0.795s, 1288.14/s)  LR: 1.236e-04  Data: 0.011 (0.013)
Train: 234 [1050/1251 ( 84%)]  Loss: 3.419 (3.35)  Time: 0.778s, 1316.21/s  (0.794s, 1289.17/s)  LR: 1.236e-04  Data: 0.011 (0.013)
Train: 234 [1100/1251 ( 88%)]  Loss: 3.163 (3.34)  Time: 0.779s, 1314.02/s  (0.794s, 1288.91/s)  LR: 1.236e-04  Data: 0.011 (0.013)
Train: 234 [1150/1251 ( 92%)]  Loss: 3.276 (3.34)  Time: 0.809s, 1266.24/s  (0.795s, 1288.86/s)  LR: 1.236e-04  Data: 0.011 (0.013)
Train: 234 [1200/1251 ( 96%)]  Loss: 3.546 (3.35)  Time: 0.798s, 1283.96/s  (0.794s, 1288.99/s)  LR: 1.236e-04  Data: 0.011 (0.013)
Train: 234 [1250/1251 (100%)]  Loss: 3.565 (3.36)  Time: 0.768s, 1334.19/s  (0.794s, 1289.21/s)  LR: 1.236e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.596 (1.596)  Loss:  0.8272 (0.8272)  Acc@1: 90.9180 (90.9180)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.172 (0.573)  Loss:  0.8730 (1.3455)  Acc@1: 85.8491 (76.3500)  Acc@5: 97.2877 (93.1520)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-231.pth.tar', 76.71000008544922)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-228.pth.tar', 76.62599989746094)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-230.pth.tar', 76.58199998046875)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-232.pth.tar', 76.49800008789063)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-233.pth.tar', 76.48999998291016)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-226.pth.tar', 76.35000002929688)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-234.pth.tar', 76.34999992919921)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-227.pth.tar', 76.25600013427734)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-225.pth.tar', 76.13800000732422)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-229.pth.tar', 76.13599992675782)

Train: 235 [   0/1251 (  0%)]  Loss: 3.403 (3.40)  Time: 2.264s,  452.31/s  (2.264s,  452.31/s)  LR: 1.203e-04  Data: 1.527 (1.527)
Train: 235 [  50/1251 (  4%)]  Loss: 3.311 (3.36)  Time: 0.777s, 1317.41/s  (0.826s, 1239.03/s)  LR: 1.203e-04  Data: 0.012 (0.045)
Train: 235 [ 100/1251 (  8%)]  Loss: 3.641 (3.45)  Time: 0.779s, 1315.04/s  (0.805s, 1272.26/s)  LR: 1.203e-04  Data: 0.011 (0.028)
Train: 235 [ 150/1251 ( 12%)]  Loss: 3.650 (3.50)  Time: 0.816s, 1254.76/s  (0.801s, 1278.12/s)  LR: 1.203e-04  Data: 0.012 (0.023)
Train: 235 [ 200/1251 ( 16%)]  Loss: 3.791 (3.56)  Time: 0.780s, 1312.73/s  (0.800s, 1280.47/s)  LR: 1.203e-04  Data: 0.011 (0.020)
Train: 235 [ 250/1251 ( 20%)]  Loss: 3.503 (3.55)  Time: 0.789s, 1297.86/s  (0.797s, 1284.79/s)  LR: 1.203e-04  Data: 0.011 (0.018)
Train: 235 [ 300/1251 ( 24%)]  Loss: 3.015 (3.47)  Time: 0.811s, 1262.83/s  (0.796s, 1285.88/s)  LR: 1.203e-04  Data: 0.011 (0.017)
Train: 235 [ 350/1251 ( 28%)]  Loss: 3.255 (3.45)  Time: 0.776s, 1319.18/s  (0.796s, 1286.60/s)  LR: 1.203e-04  Data: 0.011 (0.016)
Train: 235 [ 400/1251 ( 32%)]  Loss: 3.410 (3.44)  Time: 0.788s, 1299.75/s  (0.794s, 1289.15/s)  LR: 1.203e-04  Data: 0.011 (0.015)
Train: 235 [ 450/1251 ( 36%)]  Loss: 3.428 (3.44)  Time: 0.776s, 1320.24/s  (0.793s, 1290.73/s)  LR: 1.203e-04  Data: 0.011 (0.015)
Train: 235 [ 500/1251 ( 40%)]  Loss: 3.181 (3.42)  Time: 0.778s, 1316.86/s  (0.794s, 1289.53/s)  LR: 1.203e-04  Data: 0.011 (0.015)
Train: 235 [ 550/1251 ( 44%)]  Loss: 3.581 (3.43)  Time: 0.778s, 1316.17/s  (0.794s, 1289.61/s)  LR: 1.203e-04  Data: 0.012 (0.014)
Train: 235 [ 600/1251 ( 48%)]  Loss: 3.386 (3.43)  Time: 0.778s, 1316.90/s  (0.793s, 1291.10/s)  LR: 1.203e-04  Data: 0.011 (0.014)
Train: 235 [ 650/1251 ( 52%)]  Loss: 3.279 (3.42)  Time: 0.781s, 1311.85/s  (0.792s, 1292.70/s)  LR: 1.203e-04  Data: 0.011 (0.014)
Train: 235 [ 700/1251 ( 56%)]  Loss: 3.083 (3.39)  Time: 0.778s, 1316.24/s  (0.791s, 1293.96/s)  LR: 1.203e-04  Data: 0.011 (0.014)
Train: 235 [ 750/1251 ( 60%)]  Loss: 3.665 (3.41)  Time: 0.779s, 1314.64/s  (0.791s, 1294.83/s)  LR: 1.203e-04  Data: 0.010 (0.013)
Train: 235 [ 800/1251 ( 64%)]  Loss: 3.570 (3.42)  Time: 0.779s, 1314.39/s  (0.790s, 1295.68/s)  LR: 1.203e-04  Data: 0.011 (0.013)
Train: 235 [ 850/1251 ( 68%)]  Loss: 3.464 (3.42)  Time: 0.816s, 1254.85/s  (0.790s, 1295.59/s)  LR: 1.203e-04  Data: 0.011 (0.013)
Train: 235 [ 900/1251 ( 72%)]  Loss: 3.267 (3.41)  Time: 0.820s, 1248.83/s  (0.790s, 1295.46/s)  LR: 1.203e-04  Data: 0.011 (0.013)
Train: 235 [ 950/1251 ( 76%)]  Loss: 3.060 (3.40)  Time: 0.780s, 1312.87/s  (0.792s, 1293.74/s)  LR: 1.203e-04  Data: 0.012 (0.013)
Train: 235 [1000/1251 ( 80%)]  Loss: 2.938 (3.38)  Time: 0.783s, 1307.72/s  (0.792s, 1293.60/s)  LR: 1.203e-04  Data: 0.011 (0.013)
Train: 235 [1050/1251 ( 84%)]  Loss: 3.494 (3.38)  Time: 0.778s, 1317.04/s  (0.791s, 1294.36/s)  LR: 1.203e-04  Data: 0.011 (0.013)
Train: 235 [1100/1251 ( 88%)]  Loss: 3.400 (3.38)  Time: 0.779s, 1315.22/s  (0.791s, 1294.70/s)  LR: 1.203e-04  Data: 0.011 (0.013)
Train: 235 [1150/1251 ( 92%)]  Loss: 3.546 (3.39)  Time: 0.814s, 1258.28/s  (0.791s, 1294.95/s)  LR: 1.203e-04  Data: 0.011 (0.013)
Train: 235 [1200/1251 ( 96%)]  Loss: 3.733 (3.40)  Time: 0.780s, 1312.09/s  (0.792s, 1293.58/s)  LR: 1.203e-04  Data: 0.010 (0.013)
Train: 235 [1250/1251 (100%)]  Loss: 3.341 (3.40)  Time: 0.827s, 1237.61/s  (0.792s, 1293.26/s)  LR: 1.203e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.510 (1.510)  Loss:  0.7735 (0.7735)  Acc@1: 90.6250 (90.6250)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.172 (0.567)  Loss:  0.8465 (1.3448)  Acc@1: 85.7311 (76.7180)  Acc@5: 97.4057 (93.3060)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-235.pth.tar', 76.71800000732422)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-231.pth.tar', 76.71000008544922)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-228.pth.tar', 76.62599989746094)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-230.pth.tar', 76.58199998046875)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-232.pth.tar', 76.49800008789063)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-233.pth.tar', 76.48999998291016)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-226.pth.tar', 76.35000002929688)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-234.pth.tar', 76.34999992919921)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-227.pth.tar', 76.25600013427734)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-225.pth.tar', 76.13800000732422)

Train: 236 [   0/1251 (  0%)]  Loss: 3.684 (3.68)  Time: 2.358s,  434.19/s  (2.358s,  434.19/s)  LR: 1.171e-04  Data: 1.569 (1.569)
Train: 236 [  50/1251 (  4%)]  Loss: 3.309 (3.50)  Time: 0.786s, 1302.42/s  (0.833s, 1229.98/s)  LR: 1.171e-04  Data: 0.014 (0.045)
Train: 236 [ 100/1251 (  8%)]  Loss: 3.153 (3.38)  Time: 0.776s, 1319.19/s  (0.813s, 1260.04/s)  LR: 1.171e-04  Data: 0.011 (0.028)
Train: 236 [ 150/1251 ( 12%)]  Loss: 3.393 (3.38)  Time: 0.781s, 1310.55/s  (0.810s, 1263.84/s)  LR: 1.171e-04  Data: 0.011 (0.023)
Train: 236 [ 200/1251 ( 16%)]  Loss: 2.850 (3.28)  Time: 0.776s, 1318.97/s  (0.809s, 1266.22/s)  LR: 1.171e-04  Data: 0.011 (0.020)
Train: 236 [ 250/1251 ( 20%)]  Loss: 3.466 (3.31)  Time: 0.779s, 1314.28/s  (0.804s, 1273.57/s)  LR: 1.171e-04  Data: 0.011 (0.018)
Train: 236 [ 300/1251 ( 24%)]  Loss: 3.246 (3.30)  Time: 0.780s, 1313.64/s  (0.802s, 1277.14/s)  LR: 1.171e-04  Data: 0.011 (0.017)
Train: 236 [ 350/1251 ( 28%)]  Loss: 3.501 (3.33)  Time: 0.780s, 1313.00/s  (0.799s, 1280.95/s)  LR: 1.171e-04  Data: 0.012 (0.016)
Train: 236 [ 400/1251 ( 32%)]  Loss: 3.527 (3.35)  Time: 0.779s, 1314.03/s  (0.799s, 1282.15/s)  LR: 1.171e-04  Data: 0.011 (0.015)
Train: 236 [ 450/1251 ( 36%)]  Loss: 3.362 (3.35)  Time: 0.829s, 1235.30/s  (0.799s, 1281.04/s)  LR: 1.171e-04  Data: 0.011 (0.015)
Train: 236 [ 500/1251 ( 40%)]  Loss: 3.681 (3.38)  Time: 0.779s, 1313.91/s  (0.799s, 1281.25/s)  LR: 1.171e-04  Data: 0.012 (0.015)
Train: 236 [ 550/1251 ( 44%)]  Loss: 3.599 (3.40)  Time: 0.779s, 1313.98/s  (0.798s, 1282.83/s)  LR: 1.171e-04  Data: 0.011 (0.014)
Train: 236 [ 600/1251 ( 48%)]  Loss: 3.262 (3.39)  Time: 0.778s, 1316.69/s  (0.798s, 1283.44/s)  LR: 1.171e-04  Data: 0.012 (0.014)
Train: 236 [ 650/1251 ( 52%)]  Loss: 3.487 (3.39)  Time: 0.777s, 1318.32/s  (0.797s, 1285.09/s)  LR: 1.171e-04  Data: 0.012 (0.014)
Train: 236 [ 700/1251 ( 56%)]  Loss: 3.181 (3.38)  Time: 0.777s, 1317.49/s  (0.797s, 1285.59/s)  LR: 1.171e-04  Data: 0.011 (0.014)
Train: 236 [ 750/1251 ( 60%)]  Loss: 3.298 (3.37)  Time: 0.778s, 1316.19/s  (0.797s, 1285.06/s)  LR: 1.171e-04  Data: 0.011 (0.013)
Train: 236 [ 800/1251 ( 64%)]  Loss: 3.291 (3.37)  Time: 0.783s, 1308.20/s  (0.797s, 1284.57/s)  LR: 1.171e-04  Data: 0.011 (0.013)
Train: 236 [ 850/1251 ( 68%)]  Loss: 3.357 (3.37)  Time: 0.777s, 1317.93/s  (0.797s, 1284.70/s)  LR: 1.171e-04  Data: 0.010 (0.013)
Train: 236 [ 900/1251 ( 72%)]  Loss: 3.395 (3.37)  Time: 0.789s, 1297.88/s  (0.797s, 1285.50/s)  LR: 1.171e-04  Data: 0.011 (0.013)
Train: 236 [ 950/1251 ( 76%)]  Loss: 3.277 (3.37)  Time: 0.779s, 1314.04/s  (0.796s, 1286.61/s)  LR: 1.171e-04  Data: 0.011 (0.013)
Train: 236 [1000/1251 ( 80%)]  Loss: 2.913 (3.34)  Time: 0.781s, 1310.98/s  (0.795s, 1287.86/s)  LR: 1.171e-04  Data: 0.012 (0.013)
Train: 236 [1050/1251 ( 84%)]  Loss: 3.525 (3.35)  Time: 0.796s, 1286.89/s  (0.795s, 1288.60/s)  LR: 1.171e-04  Data: 0.012 (0.013)
Train: 236 [1100/1251 ( 88%)]  Loss: 3.221 (3.35)  Time: 0.779s, 1314.07/s  (0.794s, 1289.08/s)  LR: 1.171e-04  Data: 0.011 (0.013)
Train: 236 [1150/1251 ( 92%)]  Loss: 3.372 (3.35)  Time: 0.784s, 1306.14/s  (0.794s, 1289.95/s)  LR: 1.171e-04  Data: 0.011 (0.013)
Train: 236 [1200/1251 ( 96%)]  Loss: 3.356 (3.35)  Time: 0.782s, 1309.86/s  (0.793s, 1290.76/s)  LR: 1.171e-04  Data: 0.011 (0.013)
Train: 236 [1250/1251 (100%)]  Loss: 3.426 (3.35)  Time: 0.774s, 1323.21/s  (0.793s, 1291.23/s)  LR: 1.171e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.544 (1.544)  Loss:  0.7479 (0.7479)  Acc@1: 90.7227 (90.7227)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.172 (0.585)  Loss:  0.8552 (1.2683)  Acc@1: 85.6132 (76.8760)  Acc@5: 96.6981 (93.3160)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-236.pth.tar', 76.87599995605468)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-235.pth.tar', 76.71800000732422)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-231.pth.tar', 76.71000008544922)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-228.pth.tar', 76.62599989746094)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-230.pth.tar', 76.58199998046875)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-232.pth.tar', 76.49800008789063)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-233.pth.tar', 76.48999998291016)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-226.pth.tar', 76.35000002929688)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-234.pth.tar', 76.34999992919921)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-227.pth.tar', 76.25600013427734)

Train: 237 [   0/1251 (  0%)]  Loss: 3.117 (3.12)  Time: 2.317s,  441.96/s  (2.317s,  441.96/s)  LR: 1.139e-04  Data: 1.582 (1.582)
Train: 237 [  50/1251 (  4%)]  Loss: 3.275 (3.20)  Time: 0.823s, 1243.67/s  (0.820s, 1248.77/s)  LR: 1.139e-04  Data: 0.011 (0.044)
Train: 237 [ 100/1251 (  8%)]  Loss: 3.454 (3.28)  Time: 0.781s, 1310.74/s  (0.807s, 1268.39/s)  LR: 1.139e-04  Data: 0.011 (0.028)
Train: 237 [ 150/1251 ( 12%)]  Loss: 3.531 (3.34)  Time: 0.782s, 1310.01/s  (0.803s, 1275.57/s)  LR: 1.139e-04  Data: 0.011 (0.022)
Train: 237 [ 200/1251 ( 16%)]  Loss: 3.721 (3.42)  Time: 0.813s, 1258.93/s  (0.799s, 1282.29/s)  LR: 1.139e-04  Data: 0.011 (0.019)
Train: 237 [ 250/1251 ( 20%)]  Loss: 3.093 (3.37)  Time: 0.833s, 1229.59/s  (0.803s, 1275.86/s)  LR: 1.139e-04  Data: 0.011 (0.018)
Train: 237 [ 300/1251 ( 24%)]  Loss: 3.600 (3.40)  Time: 0.778s, 1316.44/s  (0.801s, 1278.80/s)  LR: 1.139e-04  Data: 0.011 (0.017)
Train: 237 [ 350/1251 ( 28%)]  Loss: 3.678 (3.43)  Time: 0.811s, 1262.23/s  (0.800s, 1280.12/s)  LR: 1.139e-04  Data: 0.011 (0.016)
Train: 237 [ 400/1251 ( 32%)]  Loss: 3.262 (3.41)  Time: 0.779s, 1314.46/s  (0.799s, 1281.80/s)  LR: 1.139e-04  Data: 0.012 (0.015)
Train: 237 [ 450/1251 ( 36%)]  Loss: 3.546 (3.43)  Time: 0.779s, 1314.84/s  (0.797s, 1284.77/s)  LR: 1.139e-04  Data: 0.011 (0.015)
Train: 237 [ 500/1251 ( 40%)]  Loss: 2.993 (3.39)  Time: 0.779s, 1314.84/s  (0.796s, 1287.20/s)  LR: 1.139e-04  Data: 0.011 (0.014)
Train: 237 [ 550/1251 ( 44%)]  Loss: 3.203 (3.37)  Time: 0.779s, 1314.16/s  (0.795s, 1287.41/s)  LR: 1.139e-04  Data: 0.011 (0.014)
Train: 237 [ 600/1251 ( 48%)]  Loss: 3.351 (3.37)  Time: 0.780s, 1312.31/s  (0.795s, 1288.29/s)  LR: 1.139e-04  Data: 0.011 (0.014)
Train: 237 [ 650/1251 ( 52%)]  Loss: 3.193 (3.36)  Time: 0.845s, 1211.85/s  (0.795s, 1288.21/s)  LR: 1.139e-04  Data: 0.011 (0.014)
Train: 237 [ 700/1251 ( 56%)]  Loss: 3.286 (3.35)  Time: 0.778s, 1316.23/s  (0.794s, 1289.55/s)  LR: 1.139e-04  Data: 0.011 (0.013)
Train: 237 [ 750/1251 ( 60%)]  Loss: 3.460 (3.36)  Time: 0.777s, 1317.06/s  (0.794s, 1289.33/s)  LR: 1.139e-04  Data: 0.011 (0.013)
Train: 237 [ 800/1251 ( 64%)]  Loss: 3.507 (3.37)  Time: 0.777s, 1317.75/s  (0.793s, 1290.55/s)  LR: 1.139e-04  Data: 0.011 (0.013)
Train: 237 [ 850/1251 ( 68%)]  Loss: 3.374 (3.37)  Time: 0.783s, 1307.85/s  (0.793s, 1291.63/s)  LR: 1.139e-04  Data: 0.011 (0.013)
Train: 237 [ 900/1251 ( 72%)]  Loss: 3.758 (3.39)  Time: 0.780s, 1312.37/s  (0.792s, 1292.40/s)  LR: 1.139e-04  Data: 0.011 (0.013)
Train: 237 [ 950/1251 ( 76%)]  Loss: 3.490 (3.39)  Time: 0.780s, 1313.39/s  (0.792s, 1292.19/s)  LR: 1.139e-04  Data: 0.011 (0.013)
Train: 237 [1000/1251 ( 80%)]  Loss: 3.472 (3.40)  Time: 0.778s, 1315.39/s  (0.792s, 1292.93/s)  LR: 1.139e-04  Data: 0.011 (0.013)
Train: 237 [1050/1251 ( 84%)]  Loss: 3.234 (3.39)  Time: 0.779s, 1315.15/s  (0.792s, 1292.58/s)  LR: 1.139e-04  Data: 0.012 (0.013)
Train: 237 [1100/1251 ( 88%)]  Loss: 3.127 (3.38)  Time: 0.778s, 1316.35/s  (0.792s, 1292.59/s)  LR: 1.139e-04  Data: 0.011 (0.013)
Train: 237 [1150/1251 ( 92%)]  Loss: 2.830 (3.36)  Time: 0.813s, 1259.44/s  (0.792s, 1292.14/s)  LR: 1.139e-04  Data: 0.010 (0.013)
Train: 237 [1200/1251 ( 96%)]  Loss: 3.695 (3.37)  Time: 0.817s, 1252.92/s  (0.793s, 1290.91/s)  LR: 1.139e-04  Data: 0.012 (0.012)
Train: 237 [1250/1251 (100%)]  Loss: 3.295 (3.37)  Time: 0.772s, 1326.00/s  (0.793s, 1290.50/s)  LR: 1.139e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.524 (1.524)  Loss:  0.8795 (0.8795)  Acc@1: 91.5039 (91.5039)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.172 (0.563)  Loss:  0.9982 (1.3671)  Acc@1: 85.4953 (76.9320)  Acc@5: 97.0519 (93.4740)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-237.pth.tar', 76.9320000341797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-236.pth.tar', 76.87599995605468)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-235.pth.tar', 76.71800000732422)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-231.pth.tar', 76.71000008544922)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-228.pth.tar', 76.62599989746094)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-230.pth.tar', 76.58199998046875)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-232.pth.tar', 76.49800008789063)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-233.pth.tar', 76.48999998291016)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-226.pth.tar', 76.35000002929688)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-234.pth.tar', 76.34999992919921)

Train: 238 [   0/1251 (  0%)]  Loss: 3.199 (3.20)  Time: 2.491s,  411.12/s  (2.491s,  411.12/s)  LR: 1.107e-04  Data: 1.756 (1.756)
Train: 238 [  50/1251 (  4%)]  Loss: 3.285 (3.24)  Time: 0.814s, 1258.06/s  (0.834s, 1228.02/s)  LR: 1.107e-04  Data: 0.013 (0.051)
Train: 238 [ 100/1251 (  8%)]  Loss: 3.559 (3.35)  Time: 0.821s, 1247.62/s  (0.817s, 1252.64/s)  LR: 1.107e-04  Data: 0.011 (0.032)
Train: 238 [ 150/1251 ( 12%)]  Loss: 3.520 (3.39)  Time: 0.813s, 1260.17/s  (0.814s, 1258.10/s)  LR: 1.107e-04  Data: 0.011 (0.025)
Train: 238 [ 200/1251 ( 16%)]  Loss: 3.379 (3.39)  Time: 0.776s, 1319.50/s  (0.811s, 1262.51/s)  LR: 1.107e-04  Data: 0.011 (0.021)
Train: 238 [ 250/1251 ( 20%)]  Loss: 3.335 (3.38)  Time: 0.777s, 1317.05/s  (0.807s, 1269.61/s)  LR: 1.107e-04  Data: 0.011 (0.019)
Train: 238 [ 300/1251 ( 24%)]  Loss: 3.304 (3.37)  Time: 0.778s, 1316.69/s  (0.803s, 1275.75/s)  LR: 1.107e-04  Data: 0.011 (0.018)
Train: 238 [ 350/1251 ( 28%)]  Loss: 3.516 (3.39)  Time: 0.786s, 1302.43/s  (0.803s, 1275.56/s)  LR: 1.107e-04  Data: 0.011 (0.017)
Train: 238 [ 400/1251 ( 32%)]  Loss: 3.688 (3.42)  Time: 0.778s, 1316.38/s  (0.801s, 1279.05/s)  LR: 1.107e-04  Data: 0.011 (0.016)
Train: 238 [ 450/1251 ( 36%)]  Loss: 3.943 (3.47)  Time: 0.797s, 1284.70/s  (0.800s, 1279.26/s)  LR: 1.107e-04  Data: 0.011 (0.016)
Train: 238 [ 500/1251 ( 40%)]  Loss: 3.381 (3.46)  Time: 0.779s, 1314.34/s  (0.799s, 1281.38/s)  LR: 1.107e-04  Data: 0.011 (0.015)
Train: 238 [ 550/1251 ( 44%)]  Loss: 3.045 (3.43)  Time: 0.780s, 1312.62/s  (0.798s, 1282.51/s)  LR: 1.107e-04  Data: 0.011 (0.015)
Train: 238 [ 600/1251 ( 48%)]  Loss: 3.476 (3.43)  Time: 0.813s, 1260.26/s  (0.798s, 1282.61/s)  LR: 1.107e-04  Data: 0.011 (0.015)
Train: 238 [ 650/1251 ( 52%)]  Loss: 3.615 (3.45)  Time: 0.777s, 1318.45/s  (0.798s, 1283.80/s)  LR: 1.107e-04  Data: 0.011 (0.014)
Train: 238 [ 700/1251 ( 56%)]  Loss: 3.612 (3.46)  Time: 0.779s, 1313.78/s  (0.797s, 1285.54/s)  LR: 1.107e-04  Data: 0.010 (0.014)
Train: 238 [ 750/1251 ( 60%)]  Loss: 3.619 (3.47)  Time: 0.777s, 1318.12/s  (0.796s, 1287.03/s)  LR: 1.107e-04  Data: 0.011 (0.014)
Train: 238 [ 800/1251 ( 64%)]  Loss: 3.743 (3.48)  Time: 0.779s, 1314.52/s  (0.796s, 1287.00/s)  LR: 1.107e-04  Data: 0.011 (0.014)
Train: 238 [ 850/1251 ( 68%)]  Loss: 3.466 (3.48)  Time: 0.783s, 1307.54/s  (0.796s, 1286.74/s)  LR: 1.107e-04  Data: 0.011 (0.014)
Train: 238 [ 900/1251 ( 72%)]  Loss: 3.695 (3.49)  Time: 0.777s, 1317.35/s  (0.795s, 1287.67/s)  LR: 1.107e-04  Data: 0.011 (0.013)
Train: 238 [ 950/1251 ( 76%)]  Loss: 3.363 (3.49)  Time: 0.776s, 1318.79/s  (0.795s, 1288.65/s)  LR: 1.107e-04  Data: 0.011 (0.013)
Train: 238 [1000/1251 ( 80%)]  Loss: 3.391 (3.48)  Time: 0.778s, 1316.01/s  (0.794s, 1289.69/s)  LR: 1.107e-04  Data: 0.012 (0.013)
Train: 238 [1050/1251 ( 84%)]  Loss: 3.171 (3.47)  Time: 0.777s, 1317.97/s  (0.793s, 1290.57/s)  LR: 1.107e-04  Data: 0.011 (0.013)
Train: 238 [1100/1251 ( 88%)]  Loss: 3.714 (3.48)  Time: 0.812s, 1261.54/s  (0.793s, 1290.75/s)  LR: 1.107e-04  Data: 0.011 (0.013)
Train: 238 [1150/1251 ( 92%)]  Loss: 3.155 (3.47)  Time: 0.779s, 1313.95/s  (0.793s, 1290.59/s)  LR: 1.107e-04  Data: 0.011 (0.013)
Train: 238 [1200/1251 ( 96%)]  Loss: 3.312 (3.46)  Time: 0.778s, 1316.80/s  (0.793s, 1290.98/s)  LR: 1.107e-04  Data: 0.011 (0.013)
Train: 238 [1250/1251 (100%)]  Loss: 3.318 (3.45)  Time: 0.803s, 1274.98/s  (0.794s, 1289.76/s)  LR: 1.107e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.542 (1.542)  Loss:  0.7133 (0.7133)  Acc@1: 91.6992 (91.6992)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.172 (0.565)  Loss:  0.8352 (1.2620)  Acc@1: 86.0849 (76.9260)  Acc@5: 97.0519 (93.4460)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-237.pth.tar', 76.9320000341797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-238.pth.tar', 76.92600016113282)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-236.pth.tar', 76.87599995605468)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-235.pth.tar', 76.71800000732422)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-231.pth.tar', 76.71000008544922)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-228.pth.tar', 76.62599989746094)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-230.pth.tar', 76.58199998046875)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-232.pth.tar', 76.49800008789063)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-233.pth.tar', 76.48999998291016)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-226.pth.tar', 76.35000002929688)

Train: 239 [   0/1251 (  0%)]  Loss: 3.625 (3.62)  Time: 2.316s,  442.11/s  (2.316s,  442.11/s)  LR: 1.076e-04  Data: 1.582 (1.582)
Train: 239 [  50/1251 (  4%)]  Loss: 3.455 (3.54)  Time: 0.813s, 1258.95/s  (0.829s, 1235.32/s)  LR: 1.076e-04  Data: 0.012 (0.045)
Train: 239 [ 100/1251 (  8%)]  Loss: 3.354 (3.48)  Time: 0.813s, 1259.38/s  (0.814s, 1257.33/s)  LR: 1.076e-04  Data: 0.011 (0.028)
Train: 239 [ 150/1251 ( 12%)]  Loss: 3.158 (3.40)  Time: 0.815s, 1256.10/s  (0.815s, 1255.69/s)  LR: 1.076e-04  Data: 0.011 (0.023)
Train: 239 [ 200/1251 ( 16%)]  Loss: 3.370 (3.39)  Time: 0.835s, 1225.79/s  (0.815s, 1257.12/s)  LR: 1.076e-04  Data: 0.012 (0.020)
Train: 239 [ 250/1251 ( 20%)]  Loss: 3.069 (3.34)  Time: 0.775s, 1321.88/s  (0.808s, 1266.59/s)  LR: 1.076e-04  Data: 0.011 (0.018)
Train: 239 [ 300/1251 ( 24%)]  Loss: 3.633 (3.38)  Time: 0.776s, 1319.56/s  (0.804s, 1273.01/s)  LR: 1.076e-04  Data: 0.011 (0.017)
Train: 239 [ 350/1251 ( 28%)]  Loss: 3.214 (3.36)  Time: 0.799s, 1281.92/s  (0.804s, 1273.31/s)  LR: 1.076e-04  Data: 0.011 (0.016)
Train: 239 [ 400/1251 ( 32%)]  Loss: 3.461 (3.37)  Time: 0.781s, 1311.68/s  (0.802s, 1276.07/s)  LR: 1.076e-04  Data: 0.011 (0.015)
Train: 239 [ 450/1251 ( 36%)]  Loss: 3.437 (3.38)  Time: 0.786s, 1302.64/s  (0.801s, 1278.91/s)  LR: 1.076e-04  Data: 0.011 (0.015)
Train: 239 [ 500/1251 ( 40%)]  Loss: 3.574 (3.40)  Time: 0.778s, 1315.44/s  (0.800s, 1280.78/s)  LR: 1.076e-04  Data: 0.012 (0.015)
Train: 239 [ 550/1251 ( 44%)]  Loss: 3.340 (3.39)  Time: 0.810s, 1263.53/s  (0.799s, 1281.54/s)  LR: 1.076e-04  Data: 0.012 (0.014)
Train: 239 [ 600/1251 ( 48%)]  Loss: 3.501 (3.40)  Time: 0.777s, 1317.63/s  (0.799s, 1282.16/s)  LR: 1.076e-04  Data: 0.011 (0.014)
Train: 239 [ 650/1251 ( 52%)]  Loss: 3.406 (3.40)  Time: 0.776s, 1319.42/s  (0.798s, 1283.37/s)  LR: 1.076e-04  Data: 0.011 (0.014)
Train: 239 [ 700/1251 ( 56%)]  Loss: 3.410 (3.40)  Time: 0.820s, 1248.58/s  (0.797s, 1285.24/s)  LR: 1.076e-04  Data: 0.011 (0.014)
Train: 239 [ 750/1251 ( 60%)]  Loss: 3.343 (3.40)  Time: 0.782s, 1310.03/s  (0.796s, 1286.75/s)  LR: 1.076e-04  Data: 0.011 (0.013)
Train: 239 [ 800/1251 ( 64%)]  Loss: 3.055 (3.38)  Time: 0.775s, 1321.88/s  (0.795s, 1288.21/s)  LR: 1.076e-04  Data: 0.011 (0.013)
Train: 239 [ 850/1251 ( 68%)]  Loss: 3.283 (3.37)  Time: 0.816s, 1255.57/s  (0.795s, 1288.84/s)  LR: 1.076e-04  Data: 0.011 (0.013)
Train: 239 [ 900/1251 ( 72%)]  Loss: 3.545 (3.38)  Time: 0.777s, 1317.81/s  (0.795s, 1288.44/s)  LR: 1.076e-04  Data: 0.011 (0.013)
Train: 239 [ 950/1251 ( 76%)]  Loss: 3.411 (3.38)  Time: 0.781s, 1311.59/s  (0.795s, 1288.59/s)  LR: 1.076e-04  Data: 0.011 (0.013)
Train: 239 [1000/1251 ( 80%)]  Loss: 3.425 (3.38)  Time: 0.781s, 1311.68/s  (0.794s, 1288.96/s)  LR: 1.076e-04  Data: 0.011 (0.013)
Train: 239 [1050/1251 ( 84%)]  Loss: 3.260 (3.38)  Time: 0.822s, 1245.39/s  (0.795s, 1287.86/s)  LR: 1.076e-04  Data: 0.012 (0.013)
Train: 239 [1100/1251 ( 88%)]  Loss: 3.332 (3.38)  Time: 0.782s, 1309.92/s  (0.795s, 1287.96/s)  LR: 1.076e-04  Data: 0.011 (0.013)
Train: 239 [1150/1251 ( 92%)]  Loss: 3.329 (3.37)  Time: 0.840s, 1219.58/s  (0.795s, 1287.91/s)  LR: 1.076e-04  Data: 0.012 (0.013)
Train: 239 [1200/1251 ( 96%)]  Loss: 3.298 (3.37)  Time: 0.782s, 1308.79/s  (0.795s, 1288.57/s)  LR: 1.076e-04  Data: 0.011 (0.013)
Train: 239 [1250/1251 (100%)]  Loss: 3.377 (3.37)  Time: 0.769s, 1332.30/s  (0.794s, 1289.17/s)  LR: 1.076e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.513 (1.513)  Loss:  0.8127 (0.8127)  Acc@1: 90.7227 (90.7227)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.172 (0.568)  Loss:  0.8928 (1.3185)  Acc@1: 85.7311 (76.9480)  Acc@5: 96.9340 (93.5460)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-239.pth.tar', 76.94799987792969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-237.pth.tar', 76.9320000341797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-238.pth.tar', 76.92600016113282)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-236.pth.tar', 76.87599995605468)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-235.pth.tar', 76.71800000732422)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-231.pth.tar', 76.71000008544922)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-228.pth.tar', 76.62599989746094)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-230.pth.tar', 76.58199998046875)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-232.pth.tar', 76.49800008789063)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-233.pth.tar', 76.48999998291016)

Train: 240 [   0/1251 (  0%)]  Loss: 3.339 (3.34)  Time: 2.441s,  419.57/s  (2.441s,  419.57/s)  LR: 1.045e-04  Data: 1.706 (1.706)
Train: 240 [  50/1251 (  4%)]  Loss: 3.363 (3.35)  Time: 0.838s, 1221.29/s  (0.838s, 1222.52/s)  LR: 1.045e-04  Data: 0.011 (0.048)
Train: 240 [ 100/1251 (  8%)]  Loss: 3.180 (3.29)  Time: 0.779s, 1314.80/s  (0.815s, 1255.90/s)  LR: 1.045e-04  Data: 0.011 (0.030)
Train: 240 [ 150/1251 ( 12%)]  Loss: 3.436 (3.33)  Time: 0.787s, 1300.86/s  (0.813s, 1259.95/s)  LR: 1.045e-04  Data: 0.011 (0.024)
Train: 240 [ 200/1251 ( 16%)]  Loss: 2.690 (3.20)  Time: 0.780s, 1313.33/s  (0.806s, 1269.83/s)  LR: 1.045e-04  Data: 0.011 (0.020)
Train: 240 [ 250/1251 ( 20%)]  Loss: 3.465 (3.25)  Time: 0.781s, 1311.77/s  (0.802s, 1276.66/s)  LR: 1.045e-04  Data: 0.011 (0.019)
Train: 240 [ 300/1251 ( 24%)]  Loss: 3.445 (3.27)  Time: 0.830s, 1233.59/s  (0.801s, 1277.74/s)  LR: 1.045e-04  Data: 0.012 (0.017)
Train: 240 [ 350/1251 ( 28%)]  Loss: 3.434 (3.29)  Time: 0.836s, 1224.72/s  (0.802s, 1276.89/s)  LR: 1.045e-04  Data: 0.011 (0.016)
Train: 240 [ 400/1251 ( 32%)]  Loss: 3.230 (3.29)  Time: 0.778s, 1316.71/s  (0.800s, 1279.23/s)  LR: 1.045e-04  Data: 0.011 (0.016)
Train: 240 [ 450/1251 ( 36%)]  Loss: 3.585 (3.32)  Time: 0.778s, 1316.33/s  (0.799s, 1281.75/s)  LR: 1.045e-04  Data: 0.011 (0.015)
Train: 240 [ 500/1251 ( 40%)]  Loss: 3.340 (3.32)  Time: 0.779s, 1315.21/s  (0.797s, 1284.51/s)  LR: 1.045e-04  Data: 0.011 (0.015)
Train: 240 [ 550/1251 ( 44%)]  Loss: 2.929 (3.29)  Time: 0.811s, 1262.82/s  (0.798s, 1283.86/s)  LR: 1.045e-04  Data: 0.011 (0.014)
Train: 240 [ 600/1251 ( 48%)]  Loss: 3.343 (3.29)  Time: 0.778s, 1316.30/s  (0.799s, 1281.63/s)  LR: 1.045e-04  Data: 0.011 (0.014)
Train: 240 [ 650/1251 ( 52%)]  Loss: 3.441 (3.30)  Time: 0.779s, 1313.98/s  (0.798s, 1283.54/s)  LR: 1.045e-04  Data: 0.011 (0.014)
Train: 240 [ 700/1251 ( 56%)]  Loss: 3.646 (3.32)  Time: 0.780s, 1313.33/s  (0.797s, 1284.77/s)  LR: 1.045e-04  Data: 0.011 (0.014)
Train: 240 [ 750/1251 ( 60%)]  Loss: 3.596 (3.34)  Time: 0.781s, 1310.41/s  (0.797s, 1284.95/s)  LR: 1.045e-04  Data: 0.011 (0.014)
Train: 240 [ 800/1251 ( 64%)]  Loss: 3.326 (3.34)  Time: 0.782s, 1309.28/s  (0.797s, 1284.68/s)  LR: 1.045e-04  Data: 0.011 (0.013)
Train: 240 [ 850/1251 ( 68%)]  Loss: 3.368 (3.34)  Time: 0.777s, 1317.65/s  (0.797s, 1285.02/s)  LR: 1.045e-04  Data: 0.011 (0.013)
Train: 240 [ 900/1251 ( 72%)]  Loss: 3.334 (3.34)  Time: 0.782s, 1308.90/s  (0.797s, 1285.39/s)  LR: 1.045e-04  Data: 0.011 (0.013)
Train: 240 [ 950/1251 ( 76%)]  Loss: 3.295 (3.34)  Time: 0.779s, 1313.89/s  (0.796s, 1285.76/s)  LR: 1.045e-04  Data: 0.011 (0.013)
Train: 240 [1000/1251 ( 80%)]  Loss: 3.409 (3.34)  Time: 0.826s, 1240.08/s  (0.796s, 1286.91/s)  LR: 1.045e-04  Data: 0.011 (0.013)
Train: 240 [1050/1251 ( 84%)]  Loss: 3.291 (3.34)  Time: 0.791s, 1295.00/s  (0.796s, 1286.94/s)  LR: 1.045e-04  Data: 0.012 (0.013)
Train: 240 [1100/1251 ( 88%)]  Loss: 3.421 (3.34)  Time: 0.778s, 1315.38/s  (0.795s, 1287.50/s)  LR: 1.045e-04  Data: 0.011 (0.013)
Train: 240 [1150/1251 ( 92%)]  Loss: 3.711 (3.36)  Time: 0.842s, 1215.56/s  (0.796s, 1286.54/s)  LR: 1.045e-04  Data: 0.011 (0.013)
Train: 240 [1200/1251 ( 96%)]  Loss: 3.528 (3.37)  Time: 0.777s, 1318.24/s  (0.796s, 1285.65/s)  LR: 1.045e-04  Data: 0.011 (0.013)
Train: 240 [1250/1251 (100%)]  Loss: 3.637 (3.38)  Time: 0.768s, 1332.95/s  (0.796s, 1286.35/s)  LR: 1.045e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.531 (1.531)  Loss:  0.6815 (0.6815)  Acc@1: 91.2109 (91.2109)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.172 (0.563)  Loss:  0.7314 (1.1954)  Acc@1: 86.3208 (76.9920)  Acc@5: 97.6415 (93.5440)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-240.pth.tar', 76.99200013427735)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-239.pth.tar', 76.94799987792969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-237.pth.tar', 76.9320000341797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-238.pth.tar', 76.92600016113282)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-236.pth.tar', 76.87599995605468)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-235.pth.tar', 76.71800000732422)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-231.pth.tar', 76.71000008544922)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-228.pth.tar', 76.62599989746094)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-230.pth.tar', 76.58199998046875)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-232.pth.tar', 76.49800008789063)

Train: 241 [   0/1251 (  0%)]  Loss: 3.239 (3.24)  Time: 2.648s,  386.68/s  (2.648s,  386.68/s)  LR: 1.015e-04  Data: 1.904 (1.904)
Train: 241 [  50/1251 (  4%)]  Loss: 3.588 (3.41)  Time: 0.831s, 1232.85/s  (0.833s, 1228.97/s)  LR: 1.015e-04  Data: 0.011 (0.048)
Train: 241 [ 100/1251 (  8%)]  Loss: 3.162 (3.33)  Time: 0.777s, 1317.06/s  (0.809s, 1266.28/s)  LR: 1.015e-04  Data: 0.011 (0.030)
Train: 241 [ 150/1251 ( 12%)]  Loss: 3.254 (3.31)  Time: 0.798s, 1282.87/s  (0.801s, 1279.16/s)  LR: 1.015e-04  Data: 0.011 (0.024)
Train: 241 [ 200/1251 ( 16%)]  Loss: 3.550 (3.36)  Time: 0.786s, 1303.50/s  (0.796s, 1286.92/s)  LR: 1.015e-04  Data: 0.011 (0.020)
Train: 241 [ 250/1251 ( 20%)]  Loss: 3.749 (3.42)  Time: 0.812s, 1261.39/s  (0.794s, 1289.15/s)  LR: 1.015e-04  Data: 0.010 (0.019)
Train: 241 [ 300/1251 ( 24%)]  Loss: 3.645 (3.46)  Time: 0.779s, 1315.00/s  (0.792s, 1292.93/s)  LR: 1.015e-04  Data: 0.011 (0.017)
Train: 241 [ 350/1251 ( 28%)]  Loss: 3.439 (3.45)  Time: 0.777s, 1318.02/s  (0.790s, 1295.64/s)  LR: 1.015e-04  Data: 0.011 (0.016)
Train: 241 [ 400/1251 ( 32%)]  Loss: 3.693 (3.48)  Time: 0.825s, 1241.89/s  (0.790s, 1295.83/s)  LR: 1.015e-04  Data: 0.011 (0.016)
Train: 241 [ 450/1251 ( 36%)]  Loss: 3.514 (3.48)  Time: 0.791s, 1294.32/s  (0.791s, 1293.92/s)  LR: 1.015e-04  Data: 0.011 (0.015)
Train: 241 [ 500/1251 ( 40%)]  Loss: 3.703 (3.50)  Time: 0.779s, 1314.64/s  (0.793s, 1291.47/s)  LR: 1.015e-04  Data: 0.012 (0.015)
Train: 241 [ 550/1251 ( 44%)]  Loss: 3.339 (3.49)  Time: 0.778s, 1317.04/s  (0.792s, 1292.67/s)  LR: 1.015e-04  Data: 0.011 (0.015)
Train: 241 [ 600/1251 ( 48%)]  Loss: 3.696 (3.51)  Time: 0.815s, 1256.56/s  (0.792s, 1293.13/s)  LR: 1.015e-04  Data: 0.012 (0.014)
Train: 241 [ 650/1251 ( 52%)]  Loss: 3.204 (3.48)  Time: 0.779s, 1313.89/s  (0.792s, 1293.35/s)  LR: 1.015e-04  Data: 0.011 (0.014)
Train: 241 [ 700/1251 ( 56%)]  Loss: 3.420 (3.48)  Time: 0.820s, 1248.69/s  (0.792s, 1293.50/s)  LR: 1.015e-04  Data: 0.012 (0.014)
Train: 241 [ 750/1251 ( 60%)]  Loss: 3.389 (3.47)  Time: 0.791s, 1294.44/s  (0.791s, 1294.11/s)  LR: 1.015e-04  Data: 0.011 (0.014)
Train: 241 [ 800/1251 ( 64%)]  Loss: 3.319 (3.46)  Time: 0.812s, 1260.59/s  (0.792s, 1293.60/s)  LR: 1.015e-04  Data: 0.011 (0.013)
Train: 241 [ 850/1251 ( 68%)]  Loss: 3.622 (3.47)  Time: 0.788s, 1299.04/s  (0.791s, 1293.84/s)  LR: 1.015e-04  Data: 0.011 (0.013)
Train: 241 [ 900/1251 ( 72%)]  Loss: 3.392 (3.47)  Time: 0.778s, 1316.56/s  (0.791s, 1294.20/s)  LR: 1.015e-04  Data: 0.010 (0.013)
Train: 241 [ 950/1251 ( 76%)]  Loss: 3.455 (3.47)  Time: 0.778s, 1316.54/s  (0.792s, 1293.36/s)  LR: 1.015e-04  Data: 0.011 (0.013)
Train: 241 [1000/1251 ( 80%)]  Loss: 3.521 (3.47)  Time: 0.818s, 1251.33/s  (0.792s, 1293.26/s)  LR: 1.015e-04  Data: 0.012 (0.013)
Train: 241 [1050/1251 ( 84%)]  Loss: 3.639 (3.48)  Time: 0.780s, 1313.39/s  (0.792s, 1292.24/s)  LR: 1.015e-04  Data: 0.011 (0.013)
Train: 241 [1100/1251 ( 88%)]  Loss: 3.138 (3.46)  Time: 0.795s, 1288.66/s  (0.792s, 1292.66/s)  LR: 1.015e-04  Data: 0.010 (0.013)
Train: 241 [1150/1251 ( 92%)]  Loss: 3.532 (3.47)  Time: 0.813s, 1259.21/s  (0.793s, 1291.62/s)  LR: 1.015e-04  Data: 0.011 (0.013)
Train: 241 [1200/1251 ( 96%)]  Loss: 3.787 (3.48)  Time: 0.814s, 1258.29/s  (0.793s, 1291.80/s)  LR: 1.015e-04  Data: 0.011 (0.013)
Train: 241 [1250/1251 (100%)]  Loss: 3.091 (3.46)  Time: 0.800s, 1279.68/s  (0.792s, 1292.18/s)  LR: 1.015e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.515 (1.515)  Loss:  0.7794 (0.7794)  Acc@1: 90.7227 (90.7227)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.172 (0.575)  Loss:  0.8228 (1.2879)  Acc@1: 85.9670 (76.8020)  Acc@5: 97.1698 (93.5280)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-240.pth.tar', 76.99200013427735)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-239.pth.tar', 76.94799987792969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-237.pth.tar', 76.9320000341797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-238.pth.tar', 76.92600016113282)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-236.pth.tar', 76.87599995605468)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-241.pth.tar', 76.80200010986329)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-235.pth.tar', 76.71800000732422)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-231.pth.tar', 76.71000008544922)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-228.pth.tar', 76.62599989746094)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-230.pth.tar', 76.58199998046875)

Train: 242 [   0/1251 (  0%)]  Loss: 3.603 (3.60)  Time: 2.339s,  437.88/s  (2.339s,  437.88/s)  LR: 9.853e-05  Data: 1.603 (1.603)
Train: 242 [  50/1251 (  4%)]  Loss: 3.180 (3.39)  Time: 0.784s, 1306.19/s  (0.837s, 1222.82/s)  LR: 9.853e-05  Data: 0.011 (0.042)
Train: 242 [ 100/1251 (  8%)]  Loss: 3.676 (3.49)  Time: 0.775s, 1320.69/s  (0.811s, 1262.81/s)  LR: 9.853e-05  Data: 0.011 (0.027)
Train: 242 [ 150/1251 ( 12%)]  Loss: 3.219 (3.42)  Time: 0.793s, 1291.19/s  (0.803s, 1274.54/s)  LR: 9.853e-05  Data: 0.012 (0.022)
Train: 242 [ 200/1251 ( 16%)]  Loss: 3.289 (3.39)  Time: 0.781s, 1311.03/s  (0.804s, 1273.90/s)  LR: 9.853e-05  Data: 0.011 (0.019)
Train: 242 [ 250/1251 ( 20%)]  Loss: 3.265 (3.37)  Time: 0.789s, 1297.37/s  (0.801s, 1277.78/s)  LR: 9.853e-05  Data: 0.011 (0.017)
Train: 242 [ 300/1251 ( 24%)]  Loss: 3.214 (3.35)  Time: 0.784s, 1306.13/s  (0.798s, 1283.09/s)  LR: 9.853e-05  Data: 0.011 (0.016)
Train: 242 [ 350/1251 ( 28%)]  Loss: 3.010 (3.31)  Time: 0.829s, 1235.40/s  (0.797s, 1284.01/s)  LR: 9.853e-05  Data: 0.011 (0.016)
Train: 242 [ 400/1251 ( 32%)]  Loss: 3.218 (3.30)  Time: 0.778s, 1315.87/s  (0.798s, 1283.20/s)  LR: 9.853e-05  Data: 0.011 (0.015)
Train: 242 [ 450/1251 ( 36%)]  Loss: 3.131 (3.28)  Time: 0.812s, 1261.54/s  (0.798s, 1283.04/s)  LR: 9.853e-05  Data: 0.011 (0.015)
Train: 242 [ 500/1251 ( 40%)]  Loss: 3.519 (3.30)  Time: 0.818s, 1251.72/s  (0.800s, 1280.34/s)  LR: 9.853e-05  Data: 0.011 (0.014)
Train: 242 [ 550/1251 ( 44%)]  Loss: 3.281 (3.30)  Time: 0.777s, 1317.76/s  (0.799s, 1281.65/s)  LR: 9.853e-05  Data: 0.011 (0.014)
Train: 242 [ 600/1251 ( 48%)]  Loss: 3.525 (3.32)  Time: 0.783s, 1306.96/s  (0.798s, 1283.50/s)  LR: 9.853e-05  Data: 0.011 (0.014)
Train: 242 [ 650/1251 ( 52%)]  Loss: 3.474 (3.33)  Time: 0.819s, 1250.47/s  (0.797s, 1284.71/s)  LR: 9.853e-05  Data: 0.010 (0.013)
Train: 242 [ 700/1251 ( 56%)]  Loss: 3.057 (3.31)  Time: 0.790s, 1296.11/s  (0.796s, 1286.01/s)  LR: 9.853e-05  Data: 0.011 (0.013)
Train: 242 [ 750/1251 ( 60%)]  Loss: 3.180 (3.30)  Time: 0.779s, 1315.21/s  (0.795s, 1287.28/s)  LR: 9.853e-05  Data: 0.011 (0.013)
Train: 242 [ 800/1251 ( 64%)]  Loss: 3.466 (3.31)  Time: 0.782s, 1309.10/s  (0.795s, 1288.48/s)  LR: 9.853e-05  Data: 0.010 (0.013)
Train: 242 [ 850/1251 ( 68%)]  Loss: 3.555 (3.33)  Time: 0.791s, 1293.89/s  (0.795s, 1288.17/s)  LR: 9.853e-05  Data: 0.011 (0.013)
Train: 242 [ 900/1251 ( 72%)]  Loss: 3.742 (3.35)  Time: 0.777s, 1318.01/s  (0.794s, 1289.13/s)  LR: 9.853e-05  Data: 0.010 (0.013)
Train: 242 [ 950/1251 ( 76%)]  Loss: 3.501 (3.36)  Time: 0.779s, 1314.67/s  (0.794s, 1290.10/s)  LR: 9.853e-05  Data: 0.011 (0.013)
Train: 242 [1000/1251 ( 80%)]  Loss: 3.088 (3.34)  Time: 0.777s, 1317.88/s  (0.793s, 1290.84/s)  LR: 9.853e-05  Data: 0.010 (0.013)
Train: 242 [1050/1251 ( 84%)]  Loss: 3.418 (3.35)  Time: 0.777s, 1317.77/s  (0.794s, 1290.22/s)  LR: 9.853e-05  Data: 0.011 (0.013)
Train: 242 [1100/1251 ( 88%)]  Loss: 3.165 (3.34)  Time: 0.780s, 1313.39/s  (0.793s, 1290.59/s)  LR: 9.853e-05  Data: 0.011 (0.012)
Train: 242 [1150/1251 ( 92%)]  Loss: 3.202 (3.33)  Time: 0.777s, 1317.50/s  (0.793s, 1291.19/s)  LR: 9.853e-05  Data: 0.011 (0.012)
Train: 242 [1200/1251 ( 96%)]  Loss: 3.481 (3.34)  Time: 0.818s, 1251.42/s  (0.794s, 1289.71/s)  LR: 9.853e-05  Data: 0.011 (0.012)
Train: 242 [1250/1251 (100%)]  Loss: 3.181 (3.33)  Time: 0.766s, 1337.51/s  (0.794s, 1289.25/s)  LR: 9.853e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.501 (1.501)  Loss:  0.7784 (0.7784)  Acc@1: 91.3086 (91.3086)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.172 (0.560)  Loss:  0.8066 (1.3032)  Acc@1: 85.4953 (76.9940)  Acc@5: 97.2877 (93.5280)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-242.pth.tar', 76.99400003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-240.pth.tar', 76.99200013427735)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-239.pth.tar', 76.94799987792969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-237.pth.tar', 76.9320000341797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-238.pth.tar', 76.92600016113282)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-236.pth.tar', 76.87599995605468)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-241.pth.tar', 76.80200010986329)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-235.pth.tar', 76.71800000732422)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-231.pth.tar', 76.71000008544922)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-228.pth.tar', 76.62599989746094)

Train: 243 [   0/1251 (  0%)]  Loss: 3.663 (3.66)  Time: 2.303s,  444.73/s  (2.303s,  444.73/s)  LR: 9.560e-05  Data: 1.514 (1.514)
Train: 243 [  50/1251 (  4%)]  Loss: 3.134 (3.40)  Time: 0.783s, 1308.51/s  (0.817s, 1253.00/s)  LR: 9.560e-05  Data: 0.011 (0.046)
Train: 243 [ 100/1251 (  8%)]  Loss: 3.799 (3.53)  Time: 0.809s, 1266.13/s  (0.806s, 1270.55/s)  LR: 9.560e-05  Data: 0.010 (0.029)
Train: 243 [ 150/1251 ( 12%)]  Loss: 3.303 (3.47)  Time: 0.779s, 1314.40/s  (0.803s, 1274.98/s)  LR: 9.560e-05  Data: 0.011 (0.023)
Train: 243 [ 200/1251 ( 16%)]  Loss: 3.094 (3.40)  Time: 0.834s, 1227.87/s  (0.798s, 1282.77/s)  LR: 9.560e-05  Data: 0.010 (0.020)
Train: 243 [ 250/1251 ( 20%)]  Loss: 3.030 (3.34)  Time: 0.777s, 1318.24/s  (0.795s, 1287.27/s)  LR: 9.560e-05  Data: 0.011 (0.018)
Train: 243 [ 300/1251 ( 24%)]  Loss: 3.500 (3.36)  Time: 0.780s, 1313.06/s  (0.793s, 1290.69/s)  LR: 9.560e-05  Data: 0.011 (0.017)
Train: 243 [ 350/1251 ( 28%)]  Loss: 3.453 (3.37)  Time: 0.833s, 1228.71/s  (0.793s, 1291.24/s)  LR: 9.560e-05  Data: 0.011 (0.016)
Train: 243 [ 400/1251 ( 32%)]  Loss: 3.355 (3.37)  Time: 0.781s, 1311.31/s  (0.792s, 1293.16/s)  LR: 9.560e-05  Data: 0.018 (0.016)
Train: 243 [ 450/1251 ( 36%)]  Loss: 3.396 (3.37)  Time: 0.787s, 1301.59/s  (0.794s, 1289.45/s)  LR: 9.560e-05  Data: 0.011 (0.015)
Train: 243 [ 500/1251 ( 40%)]  Loss: 3.536 (3.39)  Time: 0.909s, 1127.00/s  (0.794s, 1289.36/s)  LR: 9.560e-05  Data: 0.012 (0.015)
Train: 243 [ 550/1251 ( 44%)]  Loss: 3.494 (3.40)  Time: 0.779s, 1314.91/s  (0.794s, 1290.36/s)  LR: 9.560e-05  Data: 0.011 (0.014)
Train: 243 [ 600/1251 ( 48%)]  Loss: 3.812 (3.43)  Time: 0.779s, 1313.85/s  (0.793s, 1291.97/s)  LR: 9.560e-05  Data: 0.012 (0.014)
Train: 243 [ 650/1251 ( 52%)]  Loss: 3.491 (3.43)  Time: 0.782s, 1308.86/s  (0.792s, 1293.06/s)  LR: 9.560e-05  Data: 0.012 (0.014)
Train: 243 [ 700/1251 ( 56%)]  Loss: 3.365 (3.43)  Time: 0.776s, 1320.34/s  (0.791s, 1293.77/s)  LR: 9.560e-05  Data: 0.011 (0.014)
Train: 243 [ 750/1251 ( 60%)]  Loss: 2.930 (3.40)  Time: 0.811s, 1262.66/s  (0.792s, 1293.66/s)  LR: 9.560e-05  Data: 0.012 (0.014)
Train: 243 [ 800/1251 ( 64%)]  Loss: 3.295 (3.39)  Time: 0.825s, 1241.45/s  (0.792s, 1293.69/s)  LR: 9.560e-05  Data: 0.011 (0.013)
Train: 243 [ 850/1251 ( 68%)]  Loss: 3.226 (3.38)  Time: 0.780s, 1313.09/s  (0.791s, 1294.12/s)  LR: 9.560e-05  Data: 0.011 (0.013)
Train: 243 [ 900/1251 ( 72%)]  Loss: 3.274 (3.38)  Time: 0.828s, 1236.81/s  (0.791s, 1294.62/s)  LR: 9.560e-05  Data: 0.011 (0.013)
Train: 243 [ 950/1251 ( 76%)]  Loss: 3.412 (3.38)  Time: 0.845s, 1211.19/s  (0.791s, 1294.20/s)  LR: 9.560e-05  Data: 0.011 (0.013)
Train: 243 [1000/1251 ( 80%)]  Loss: 3.432 (3.38)  Time: 0.815s, 1256.40/s  (0.792s, 1293.73/s)  LR: 9.560e-05  Data: 0.010 (0.013)
Train: 243 [1050/1251 ( 84%)]  Loss: 3.265 (3.38)  Time: 0.778s, 1316.24/s  (0.791s, 1293.89/s)  LR: 9.560e-05  Data: 0.011 (0.013)
Train: 243 [1100/1251 ( 88%)]  Loss: 3.381 (3.38)  Time: 0.779s, 1315.11/s  (0.792s, 1293.68/s)  LR: 9.560e-05  Data: 0.011 (0.013)
Train: 243 [1150/1251 ( 92%)]  Loss: 3.319 (3.37)  Time: 0.819s, 1250.53/s  (0.791s, 1294.21/s)  LR: 9.560e-05  Data: 0.011 (0.013)
Train: 243 [1200/1251 ( 96%)]  Loss: 2.978 (3.36)  Time: 0.780s, 1312.95/s  (0.791s, 1294.46/s)  LR: 9.560e-05  Data: 0.011 (0.013)
Train: 243 [1250/1251 (100%)]  Loss: 3.566 (3.37)  Time: 0.767s, 1334.57/s  (0.791s, 1294.29/s)  LR: 9.560e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.522 (1.522)  Loss:  0.7128 (0.7128)  Acc@1: 91.4062 (91.4062)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.172 (0.560)  Loss:  0.7783 (1.2241)  Acc@1: 85.2594 (77.2540)  Acc@5: 96.8160 (93.5100)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-243.pth.tar', 77.25399993164062)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-242.pth.tar', 76.99400003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-240.pth.tar', 76.99200013427735)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-239.pth.tar', 76.94799987792969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-237.pth.tar', 76.9320000341797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-238.pth.tar', 76.92600016113282)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-236.pth.tar', 76.87599995605468)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-241.pth.tar', 76.80200010986329)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-235.pth.tar', 76.71800000732422)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-231.pth.tar', 76.71000008544922)

Train: 244 [   0/1251 (  0%)]  Loss: 3.583 (3.58)  Time: 2.271s,  450.94/s  (2.271s,  450.94/s)  LR: 9.270e-05  Data: 1.536 (1.536)
Train: 244 [  50/1251 (  4%)]  Loss: 3.181 (3.38)  Time: 0.778s, 1316.01/s  (0.823s, 1244.95/s)  LR: 9.270e-05  Data: 0.011 (0.047)
Train: 244 [ 100/1251 (  8%)]  Loss: 3.422 (3.40)  Time: 0.777s, 1317.11/s  (0.805s, 1271.77/s)  LR: 9.270e-05  Data: 0.010 (0.029)
Train: 244 [ 150/1251 ( 12%)]  Loss: 3.548 (3.43)  Time: 0.777s, 1317.14/s  (0.798s, 1283.68/s)  LR: 9.270e-05  Data: 0.011 (0.023)
Train: 244 [ 200/1251 ( 16%)]  Loss: 3.160 (3.38)  Time: 0.779s, 1315.14/s  (0.796s, 1286.09/s)  LR: 9.270e-05  Data: 0.011 (0.020)
Train: 244 [ 250/1251 ( 20%)]  Loss: 3.634 (3.42)  Time: 0.808s, 1267.99/s  (0.798s, 1283.32/s)  LR: 9.270e-05  Data: 0.012 (0.018)
Train: 244 [ 300/1251 ( 24%)]  Loss: 3.330 (3.41)  Time: 0.781s, 1311.68/s  (0.796s, 1286.99/s)  LR: 9.270e-05  Data: 0.010 (0.017)
Train: 244 [ 350/1251 ( 28%)]  Loss: 3.244 (3.39)  Time: 0.814s, 1257.93/s  (0.795s, 1288.53/s)  LR: 9.270e-05  Data: 0.011 (0.016)
Train: 244 [ 400/1251 ( 32%)]  Loss: 3.508 (3.40)  Time: 0.778s, 1316.81/s  (0.797s, 1285.43/s)  LR: 9.270e-05  Data: 0.011 (0.016)
Train: 244 [ 450/1251 ( 36%)]  Loss: 3.022 (3.36)  Time: 0.780s, 1313.23/s  (0.797s, 1285.62/s)  LR: 9.270e-05  Data: 0.011 (0.015)
Train: 244 [ 500/1251 ( 40%)]  Loss: 3.202 (3.35)  Time: 0.776s, 1320.31/s  (0.797s, 1284.20/s)  LR: 9.270e-05  Data: 0.011 (0.015)
Train: 244 [ 550/1251 ( 44%)]  Loss: 2.887 (3.31)  Time: 0.787s, 1301.00/s  (0.796s, 1285.85/s)  LR: 9.270e-05  Data: 0.014 (0.014)
Train: 244 [ 600/1251 ( 48%)]  Loss: 3.229 (3.30)  Time: 0.812s, 1261.35/s  (0.796s, 1285.66/s)  LR: 9.270e-05  Data: 0.012 (0.014)
Train: 244 [ 650/1251 ( 52%)]  Loss: 2.869 (3.27)  Time: 0.783s, 1307.34/s  (0.796s, 1286.61/s)  LR: 9.270e-05  Data: 0.011 (0.014)
Train: 244 [ 700/1251 ( 56%)]  Loss: 3.454 (3.28)  Time: 0.779s, 1313.69/s  (0.795s, 1288.24/s)  LR: 9.270e-05  Data: 0.011 (0.014)
Train: 244 [ 750/1251 ( 60%)]  Loss: 3.047 (3.27)  Time: 0.780s, 1312.22/s  (0.795s, 1288.82/s)  LR: 9.270e-05  Data: 0.011 (0.014)
Train: 244 [ 800/1251 ( 64%)]  Loss: 3.323 (3.27)  Time: 0.779s, 1314.67/s  (0.794s, 1289.50/s)  LR: 9.270e-05  Data: 0.011 (0.013)
Train: 244 [ 850/1251 ( 68%)]  Loss: 3.415 (3.28)  Time: 0.777s, 1318.67/s  (0.794s, 1290.00/s)  LR: 9.270e-05  Data: 0.011 (0.013)
Train: 244 [ 900/1251 ( 72%)]  Loss: 3.399 (3.29)  Time: 0.779s, 1315.32/s  (0.793s, 1290.97/s)  LR: 9.270e-05  Data: 0.011 (0.013)
Train: 244 [ 950/1251 ( 76%)]  Loss: 3.605 (3.30)  Time: 0.779s, 1315.19/s  (0.794s, 1290.27/s)  LR: 9.270e-05  Data: 0.011 (0.013)
Train: 244 [1000/1251 ( 80%)]  Loss: 3.375 (3.31)  Time: 0.777s, 1318.24/s  (0.794s, 1290.06/s)  LR: 9.270e-05  Data: 0.011 (0.013)
Train: 244 [1050/1251 ( 84%)]  Loss: 3.083 (3.30)  Time: 0.778s, 1315.53/s  (0.794s, 1290.25/s)  LR: 9.270e-05  Data: 0.011 (0.013)
Train: 244 [1100/1251 ( 88%)]  Loss: 3.347 (3.30)  Time: 0.812s, 1261.20/s  (0.794s, 1290.16/s)  LR: 9.270e-05  Data: 0.010 (0.013)
Train: 244 [1150/1251 ( 92%)]  Loss: 2.862 (3.28)  Time: 0.780s, 1312.71/s  (0.793s, 1290.65/s)  LR: 9.270e-05  Data: 0.011 (0.013)
Train: 244 [1200/1251 ( 96%)]  Loss: 3.239 (3.28)  Time: 0.778s, 1316.18/s  (0.793s, 1291.35/s)  LR: 9.270e-05  Data: 0.011 (0.013)
Train: 244 [1250/1251 (100%)]  Loss: 3.440 (3.28)  Time: 0.770s, 1329.58/s  (0.793s, 1291.81/s)  LR: 9.270e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.521 (1.521)  Loss:  0.7394 (0.7394)  Acc@1: 90.4297 (90.4297)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.172 (0.569)  Loss:  0.8869 (1.3124)  Acc@1: 86.3208 (76.8720)  Acc@5: 97.1698 (93.3580)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-243.pth.tar', 77.25399993164062)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-242.pth.tar', 76.99400003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-240.pth.tar', 76.99200013427735)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-239.pth.tar', 76.94799987792969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-237.pth.tar', 76.9320000341797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-238.pth.tar', 76.92600016113282)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-236.pth.tar', 76.87599995605468)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-244.pth.tar', 76.8720000048828)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-241.pth.tar', 76.80200010986329)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-235.pth.tar', 76.71800000732422)

Train: 245 [   0/1251 (  0%)]  Loss: 3.526 (3.53)  Time: 2.298s,  445.61/s  (2.298s,  445.61/s)  LR: 8.986e-05  Data: 1.564 (1.564)
Train: 245 [  50/1251 (  4%)]  Loss: 3.433 (3.48)  Time: 0.780s, 1312.40/s  (0.822s, 1246.03/s)  LR: 8.986e-05  Data: 0.011 (0.046)
Train: 245 [ 100/1251 (  8%)]  Loss: 3.488 (3.48)  Time: 0.811s, 1262.67/s  (0.817s, 1252.96/s)  LR: 8.986e-05  Data: 0.011 (0.029)
Train: 245 [ 150/1251 ( 12%)]  Loss: 3.294 (3.44)  Time: 0.779s, 1314.66/s  (0.810s, 1263.60/s)  LR: 8.986e-05  Data: 0.011 (0.023)
Train: 245 [ 200/1251 ( 16%)]  Loss: 3.361 (3.42)  Time: 0.781s, 1311.21/s  (0.803s, 1275.15/s)  LR: 8.986e-05  Data: 0.011 (0.020)
Train: 245 [ 250/1251 ( 20%)]  Loss: 3.218 (3.39)  Time: 0.777s, 1317.08/s  (0.799s, 1281.17/s)  LR: 8.986e-05  Data: 0.012 (0.018)
Train: 245 [ 300/1251 ( 24%)]  Loss: 3.506 (3.40)  Time: 0.804s, 1273.24/s  (0.796s, 1286.09/s)  LR: 8.986e-05  Data: 0.011 (0.017)
Train: 245 [ 350/1251 ( 28%)]  Loss: 3.440 (3.41)  Time: 0.781s, 1311.60/s  (0.796s, 1286.98/s)  LR: 8.986e-05  Data: 0.011 (0.016)
Train: 245 [ 400/1251 ( 32%)]  Loss: 3.303 (3.40)  Time: 0.779s, 1313.85/s  (0.796s, 1286.50/s)  LR: 8.986e-05  Data: 0.012 (0.015)
Train: 245 [ 450/1251 ( 36%)]  Loss: 3.541 (3.41)  Time: 0.779s, 1314.93/s  (0.795s, 1287.94/s)  LR: 8.986e-05  Data: 0.011 (0.015)
Train: 245 [ 500/1251 ( 40%)]  Loss: 3.337 (3.40)  Time: 0.779s, 1313.72/s  (0.795s, 1287.96/s)  LR: 8.986e-05  Data: 0.012 (0.015)
Train: 245 [ 550/1251 ( 44%)]  Loss: 3.381 (3.40)  Time: 0.778s, 1316.70/s  (0.795s, 1288.19/s)  LR: 8.986e-05  Data: 0.011 (0.014)
Train: 245 [ 600/1251 ( 48%)]  Loss: 3.455 (3.41)  Time: 0.840s, 1218.63/s  (0.795s, 1288.34/s)  LR: 8.986e-05  Data: 0.011 (0.014)
Train: 245 [ 650/1251 ( 52%)]  Loss: 3.296 (3.40)  Time: 0.812s, 1261.29/s  (0.794s, 1289.16/s)  LR: 8.986e-05  Data: 0.010 (0.014)
Train: 245 [ 700/1251 ( 56%)]  Loss: 3.652 (3.42)  Time: 0.779s, 1314.76/s  (0.795s, 1288.55/s)  LR: 8.986e-05  Data: 0.012 (0.014)
Train: 245 [ 750/1251 ( 60%)]  Loss: 3.132 (3.40)  Time: 0.782s, 1310.25/s  (0.795s, 1288.78/s)  LR: 8.986e-05  Data: 0.011 (0.013)
Train: 245 [ 800/1251 ( 64%)]  Loss: 3.295 (3.39)  Time: 0.778s, 1316.28/s  (0.795s, 1288.83/s)  LR: 8.986e-05  Data: 0.011 (0.013)
Train: 245 [ 850/1251 ( 68%)]  Loss: 3.184 (3.38)  Time: 0.780s, 1312.82/s  (0.794s, 1290.14/s)  LR: 8.986e-05  Data: 0.011 (0.013)
Train: 245 [ 900/1251 ( 72%)]  Loss: 3.380 (3.38)  Time: 0.780s, 1312.78/s  (0.793s, 1290.86/s)  LR: 8.986e-05  Data: 0.012 (0.013)
Train: 245 [ 950/1251 ( 76%)]  Loss: 3.286 (3.38)  Time: 0.777s, 1317.72/s  (0.793s, 1291.90/s)  LR: 8.986e-05  Data: 0.011 (0.013)
Train: 245 [1000/1251 ( 80%)]  Loss: 3.703 (3.39)  Time: 0.811s, 1263.21/s  (0.792s, 1292.38/s)  LR: 8.986e-05  Data: 0.011 (0.013)
Train: 245 [1050/1251 ( 84%)]  Loss: 3.384 (3.39)  Time: 0.777s, 1318.51/s  (0.792s, 1292.63/s)  LR: 8.986e-05  Data: 0.011 (0.013)
Train: 245 [1100/1251 ( 88%)]  Loss: 3.433 (3.39)  Time: 0.821s, 1247.12/s  (0.793s, 1291.13/s)  LR: 8.986e-05  Data: 0.012 (0.013)
Train: 245 [1150/1251 ( 92%)]  Loss: 3.347 (3.39)  Time: 0.778s, 1315.86/s  (0.793s, 1291.45/s)  LR: 8.986e-05  Data: 0.011 (0.013)
Train: 245 [1200/1251 ( 96%)]  Loss: 3.021 (3.38)  Time: 0.812s, 1261.04/s  (0.793s, 1290.80/s)  LR: 8.986e-05  Data: 0.011 (0.013)
Train: 245 [1250/1251 (100%)]  Loss: 3.822 (3.39)  Time: 0.768s, 1332.48/s  (0.793s, 1290.70/s)  LR: 8.986e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.504 (1.504)  Loss:  0.7869 (0.7869)  Acc@1: 91.0156 (91.0156)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.172 (0.561)  Loss:  0.8270 (1.3262)  Acc@1: 86.5566 (76.9260)  Acc@5: 97.6415 (93.4340)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-243.pth.tar', 77.25399993164062)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-242.pth.tar', 76.99400003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-240.pth.tar', 76.99200013427735)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-239.pth.tar', 76.94799987792969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-237.pth.tar', 76.9320000341797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-238.pth.tar', 76.92600016113282)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-245.pth.tar', 76.92600010742187)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-236.pth.tar', 76.87599995605468)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-244.pth.tar', 76.8720000048828)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-241.pth.tar', 76.80200010986329)

Train: 246 [   0/1251 (  0%)]  Loss: 3.246 (3.25)  Time: 2.392s,  428.13/s  (2.392s,  428.13/s)  LR: 8.706e-05  Data: 1.577 (1.577)
Train: 246 [  50/1251 (  4%)]  Loss: 3.233 (3.24)  Time: 0.778s, 1315.64/s  (0.819s, 1249.80/s)  LR: 8.706e-05  Data: 0.011 (0.044)
Train: 246 [ 100/1251 (  8%)]  Loss: 3.267 (3.25)  Time: 0.792s, 1293.26/s  (0.803s, 1275.03/s)  LR: 8.706e-05  Data: 0.011 (0.028)
Train: 246 [ 150/1251 ( 12%)]  Loss: 3.602 (3.34)  Time: 0.778s, 1316.05/s  (0.799s, 1281.73/s)  LR: 8.706e-05  Data: 0.011 (0.022)
Train: 246 [ 200/1251 ( 16%)]  Loss: 3.767 (3.42)  Time: 0.780s, 1313.32/s  (0.795s, 1287.47/s)  LR: 8.706e-05  Data: 0.011 (0.019)
Train: 246 [ 250/1251 ( 20%)]  Loss: 3.173 (3.38)  Time: 0.779s, 1313.85/s  (0.795s, 1288.79/s)  LR: 8.706e-05  Data: 0.011 (0.018)
Train: 246 [ 300/1251 ( 24%)]  Loss: 3.490 (3.40)  Time: 0.778s, 1316.92/s  (0.793s, 1291.96/s)  LR: 8.706e-05  Data: 0.011 (0.017)
Train: 246 [ 350/1251 ( 28%)]  Loss: 3.225 (3.38)  Time: 0.776s, 1319.16/s  (0.792s, 1292.96/s)  LR: 8.706e-05  Data: 0.011 (0.016)
Train: 246 [ 400/1251 ( 32%)]  Loss: 3.806 (3.42)  Time: 0.781s, 1311.52/s  (0.792s, 1293.38/s)  LR: 8.706e-05  Data: 0.011 (0.015)
Train: 246 [ 450/1251 ( 36%)]  Loss: 3.440 (3.42)  Time: 0.776s, 1319.79/s  (0.791s, 1294.98/s)  LR: 8.706e-05  Data: 0.010 (0.015)
Train: 246 [ 500/1251 ( 40%)]  Loss: 3.575 (3.44)  Time: 0.780s, 1313.13/s  (0.791s, 1294.32/s)  LR: 8.706e-05  Data: 0.011 (0.014)
Train: 246 [ 550/1251 ( 44%)]  Loss: 3.373 (3.43)  Time: 0.779s, 1314.98/s  (0.791s, 1294.96/s)  LR: 8.706e-05  Data: 0.011 (0.014)
Train: 246 [ 600/1251 ( 48%)]  Loss: 3.490 (3.44)  Time: 0.813s, 1259.48/s  (0.792s, 1292.56/s)  LR: 8.706e-05  Data: 0.011 (0.014)
Train: 246 [ 650/1251 ( 52%)]  Loss: 3.378 (3.43)  Time: 0.781s, 1310.96/s  (0.793s, 1291.13/s)  LR: 8.706e-05  Data: 0.011 (0.014)
Train: 246 [ 700/1251 ( 56%)]  Loss: 3.543 (3.44)  Time: 0.779s, 1315.21/s  (0.793s, 1291.45/s)  LR: 8.706e-05  Data: 0.012 (0.013)
Train: 246 [ 750/1251 ( 60%)]  Loss: 3.045 (3.42)  Time: 0.781s, 1310.91/s  (0.792s, 1292.51/s)  LR: 8.706e-05  Data: 0.011 (0.013)
Train: 246 [ 800/1251 ( 64%)]  Loss: 3.087 (3.40)  Time: 0.779s, 1315.26/s  (0.792s, 1293.31/s)  LR: 8.706e-05  Data: 0.011 (0.013)
Train: 246 [ 850/1251 ( 68%)]  Loss: 3.551 (3.40)  Time: 0.780s, 1312.58/s  (0.791s, 1294.19/s)  LR: 8.706e-05  Data: 0.011 (0.013)
Train: 246 [ 900/1251 ( 72%)]  Loss: 3.425 (3.41)  Time: 0.797s, 1284.78/s  (0.791s, 1294.23/s)  LR: 8.706e-05  Data: 0.010 (0.013)
Train: 246 [ 950/1251 ( 76%)]  Loss: 3.382 (3.40)  Time: 0.816s, 1254.48/s  (0.791s, 1293.77/s)  LR: 8.706e-05  Data: 0.011 (0.013)
Train: 246 [1000/1251 ( 80%)]  Loss: 3.658 (3.42)  Time: 0.779s, 1314.21/s  (0.791s, 1294.63/s)  LR: 8.706e-05  Data: 0.010 (0.013)
Train: 246 [1050/1251 ( 84%)]  Loss: 3.477 (3.42)  Time: 0.781s, 1310.95/s  (0.791s, 1294.92/s)  LR: 8.706e-05  Data: 0.010 (0.013)
Train: 246 [1100/1251 ( 88%)]  Loss: 3.494 (3.42)  Time: 0.777s, 1318.50/s  (0.791s, 1294.45/s)  LR: 8.706e-05  Data: 0.011 (0.013)
Train: 246 [1150/1251 ( 92%)]  Loss: 3.295 (3.42)  Time: 0.779s, 1314.46/s  (0.791s, 1294.43/s)  LR: 8.706e-05  Data: 0.011 (0.012)
Train: 246 [1200/1251 ( 96%)]  Loss: 3.645 (3.43)  Time: 0.811s, 1261.94/s  (0.791s, 1293.80/s)  LR: 8.706e-05  Data: 0.011 (0.012)
Train: 246 [1250/1251 (100%)]  Loss: 3.408 (3.43)  Time: 0.767s, 1335.34/s  (0.791s, 1294.24/s)  LR: 8.706e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.528 (1.528)  Loss:  0.6796 (0.6796)  Acc@1: 90.6250 (90.6250)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.172 (0.566)  Loss:  0.7787 (1.2000)  Acc@1: 85.4953 (77.3240)  Acc@5: 97.1698 (93.7480)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-246.pth.tar', 77.32400003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-243.pth.tar', 77.25399993164062)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-242.pth.tar', 76.99400003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-240.pth.tar', 76.99200013427735)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-239.pth.tar', 76.94799987792969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-237.pth.tar', 76.9320000341797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-238.pth.tar', 76.92600016113282)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-245.pth.tar', 76.92600010742187)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-236.pth.tar', 76.87599995605468)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-244.pth.tar', 76.8720000048828)

Train: 247 [   0/1251 (  0%)]  Loss: 3.279 (3.28)  Time: 2.229s,  459.44/s  (2.229s,  459.44/s)  LR: 8.430e-05  Data: 1.493 (1.493)
Train: 247 [  50/1251 (  4%)]  Loss: 3.452 (3.37)  Time: 0.775s, 1321.06/s  (0.826s, 1239.48/s)  LR: 8.430e-05  Data: 0.011 (0.043)
Train: 247 [ 100/1251 (  8%)]  Loss: 3.521 (3.42)  Time: 0.778s, 1316.49/s  (0.806s, 1270.94/s)  LR: 8.430e-05  Data: 0.011 (0.027)
Train: 247 [ 150/1251 ( 12%)]  Loss: 3.411 (3.42)  Time: 0.778s, 1316.52/s  (0.801s, 1278.29/s)  LR: 8.430e-05  Data: 0.011 (0.022)
Train: 247 [ 200/1251 ( 16%)]  Loss: 3.618 (3.46)  Time: 0.811s, 1262.05/s  (0.801s, 1278.99/s)  LR: 8.430e-05  Data: 0.010 (0.019)
Train: 247 [ 250/1251 ( 20%)]  Loss: 3.329 (3.44)  Time: 0.777s, 1318.54/s  (0.798s, 1282.74/s)  LR: 8.430e-05  Data: 0.011 (0.018)
Train: 247 [ 300/1251 ( 24%)]  Loss: 3.266 (3.41)  Time: 0.835s, 1225.87/s  (0.796s, 1285.64/s)  LR: 8.430e-05  Data: 0.011 (0.016)
Train: 247 [ 350/1251 ( 28%)]  Loss: 3.653 (3.44)  Time: 0.778s, 1315.65/s  (0.795s, 1288.44/s)  LR: 8.430e-05  Data: 0.012 (0.016)
Train: 247 [ 400/1251 ( 32%)]  Loss: 3.494 (3.45)  Time: 0.820s, 1248.96/s  (0.795s, 1288.07/s)  LR: 8.430e-05  Data: 0.011 (0.015)
Train: 247 [ 450/1251 ( 36%)]  Loss: 3.286 (3.43)  Time: 0.799s, 1281.31/s  (0.796s, 1286.95/s)  LR: 8.430e-05  Data: 0.011 (0.015)
Train: 247 [ 500/1251 ( 40%)]  Loss: 3.204 (3.41)  Time: 0.775s, 1320.57/s  (0.794s, 1288.89/s)  LR: 8.430e-05  Data: 0.011 (0.014)
Train: 247 [ 550/1251 ( 44%)]  Loss: 3.267 (3.40)  Time: 0.815s, 1255.78/s  (0.794s, 1288.90/s)  LR: 8.430e-05  Data: 0.011 (0.014)
Train: 247 [ 600/1251 ( 48%)]  Loss: 3.246 (3.39)  Time: 0.790s, 1296.59/s  (0.795s, 1288.29/s)  LR: 8.430e-05  Data: 0.010 (0.014)
Train: 247 [ 650/1251 ( 52%)]  Loss: 3.291 (3.38)  Time: 0.814s, 1257.93/s  (0.795s, 1288.22/s)  LR: 8.430e-05  Data: 0.011 (0.014)
Train: 247 [ 700/1251 ( 56%)]  Loss: 3.263 (3.37)  Time: 0.785s, 1304.23/s  (0.796s, 1287.15/s)  LR: 8.430e-05  Data: 0.011 (0.013)
Train: 247 [ 750/1251 ( 60%)]  Loss: 3.457 (3.38)  Time: 0.781s, 1311.60/s  (0.795s, 1287.44/s)  LR: 8.430e-05  Data: 0.011 (0.013)
Train: 247 [ 800/1251 ( 64%)]  Loss: 3.726 (3.40)  Time: 0.839s, 1221.05/s  (0.796s, 1287.10/s)  LR: 8.430e-05  Data: 0.011 (0.013)
Train: 247 [ 850/1251 ( 68%)]  Loss: 3.157 (3.38)  Time: 0.808s, 1267.22/s  (0.795s, 1288.28/s)  LR: 8.430e-05  Data: 0.010 (0.013)
Train: 247 [ 900/1251 ( 72%)]  Loss: 3.254 (3.38)  Time: 0.791s, 1294.49/s  (0.796s, 1286.84/s)  LR: 8.430e-05  Data: 0.011 (0.013)
Train: 247 [ 950/1251 ( 76%)]  Loss: 3.363 (3.38)  Time: 0.777s, 1317.44/s  (0.795s, 1287.79/s)  LR: 8.430e-05  Data: 0.011 (0.013)
Train: 247 [1000/1251 ( 80%)]  Loss: 3.298 (3.37)  Time: 0.778s, 1315.43/s  (0.795s, 1288.16/s)  LR: 8.430e-05  Data: 0.011 (0.013)
Train: 247 [1050/1251 ( 84%)]  Loss: 3.432 (3.38)  Time: 0.779s, 1314.21/s  (0.795s, 1288.68/s)  LR: 8.430e-05  Data: 0.011 (0.013)
Train: 247 [1100/1251 ( 88%)]  Loss: 3.644 (3.39)  Time: 0.780s, 1312.23/s  (0.795s, 1288.16/s)  LR: 8.430e-05  Data: 0.011 (0.013)
Train: 247 [1150/1251 ( 92%)]  Loss: 3.093 (3.38)  Time: 0.831s, 1232.09/s  (0.794s, 1288.88/s)  LR: 8.430e-05  Data: 0.011 (0.012)
Train: 247 [1200/1251 ( 96%)]  Loss: 3.282 (3.37)  Time: 0.778s, 1316.77/s  (0.794s, 1289.64/s)  LR: 8.430e-05  Data: 0.011 (0.012)
Train: 247 [1250/1251 (100%)]  Loss: 3.356 (3.37)  Time: 0.767s, 1335.07/s  (0.794s, 1289.98/s)  LR: 8.430e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.556 (1.556)  Loss:  0.6874 (0.6874)  Acc@1: 91.4062 (91.4062)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.172 (0.569)  Loss:  0.8263 (1.2301)  Acc@1: 85.8491 (77.3720)  Acc@5: 97.0519 (93.6220)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-247.pth.tar', 77.37199992919922)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-246.pth.tar', 77.32400003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-243.pth.tar', 77.25399993164062)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-242.pth.tar', 76.99400003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-240.pth.tar', 76.99200013427735)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-239.pth.tar', 76.94799987792969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-237.pth.tar', 76.9320000341797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-238.pth.tar', 76.92600016113282)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-245.pth.tar', 76.92600010742187)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-236.pth.tar', 76.87599995605468)

Train: 248 [   0/1251 (  0%)]  Loss: 3.275 (3.27)  Time: 2.355s,  434.87/s  (2.355s,  434.87/s)  LR: 8.159e-05  Data: 1.619 (1.619)
Train: 248 [  50/1251 (  4%)]  Loss: 3.348 (3.31)  Time: 0.788s, 1300.22/s  (0.829s, 1235.77/s)  LR: 8.159e-05  Data: 0.011 (0.049)
Train: 248 [ 100/1251 (  8%)]  Loss: 3.181 (3.27)  Time: 0.784s, 1306.91/s  (0.808s, 1267.74/s)  LR: 8.159e-05  Data: 0.011 (0.030)
Train: 248 [ 150/1251 ( 12%)]  Loss: 3.203 (3.25)  Time: 0.812s, 1261.86/s  (0.802s, 1277.06/s)  LR: 8.159e-05  Data: 0.011 (0.024)
Train: 248 [ 200/1251 ( 16%)]  Loss: 3.545 (3.31)  Time: 0.778s, 1317.00/s  (0.799s, 1281.96/s)  LR: 8.159e-05  Data: 0.011 (0.021)
Train: 248 [ 250/1251 ( 20%)]  Loss: 3.381 (3.32)  Time: 0.787s, 1300.33/s  (0.797s, 1284.74/s)  LR: 8.159e-05  Data: 0.011 (0.019)
Train: 248 [ 300/1251 ( 24%)]  Loss: 3.356 (3.33)  Time: 0.779s, 1314.97/s  (0.795s, 1287.92/s)  LR: 8.159e-05  Data: 0.011 (0.017)
Train: 248 [ 350/1251 ( 28%)]  Loss: 3.308 (3.32)  Time: 0.778s, 1316.68/s  (0.795s, 1288.19/s)  LR: 8.159e-05  Data: 0.011 (0.016)
Train: 248 [ 400/1251 ( 32%)]  Loss: 3.358 (3.33)  Time: 0.778s, 1315.83/s  (0.795s, 1288.81/s)  LR: 8.159e-05  Data: 0.011 (0.016)
Train: 248 [ 450/1251 ( 36%)]  Loss: 3.187 (3.31)  Time: 0.780s, 1312.65/s  (0.794s, 1289.46/s)  LR: 8.159e-05  Data: 0.011 (0.015)
Train: 248 [ 500/1251 ( 40%)]  Loss: 3.261 (3.31)  Time: 0.777s, 1317.07/s  (0.794s, 1290.41/s)  LR: 8.159e-05  Data: 0.011 (0.015)
Train: 248 [ 550/1251 ( 44%)]  Loss: 3.274 (3.31)  Time: 0.813s, 1259.39/s  (0.794s, 1290.16/s)  LR: 8.159e-05  Data: 0.010 (0.014)
Train: 248 [ 600/1251 ( 48%)]  Loss: 3.432 (3.32)  Time: 0.778s, 1315.63/s  (0.794s, 1290.36/s)  LR: 8.159e-05  Data: 0.011 (0.014)
Train: 248 [ 650/1251 ( 52%)]  Loss: 3.787 (3.35)  Time: 0.813s, 1259.43/s  (0.793s, 1291.79/s)  LR: 8.159e-05  Data: 0.011 (0.014)
Train: 248 [ 700/1251 ( 56%)]  Loss: 3.130 (3.34)  Time: 0.791s, 1295.35/s  (0.792s, 1292.57/s)  LR: 8.159e-05  Data: 0.013 (0.014)
Train: 248 [ 750/1251 ( 60%)]  Loss: 3.363 (3.34)  Time: 0.776s, 1319.00/s  (0.793s, 1291.83/s)  LR: 8.159e-05  Data: 0.011 (0.014)
Train: 248 [ 800/1251 ( 64%)]  Loss: 3.478 (3.35)  Time: 0.794s, 1289.61/s  (0.792s, 1292.69/s)  LR: 8.159e-05  Data: 0.011 (0.013)
Train: 248 [ 850/1251 ( 68%)]  Loss: 3.464 (3.35)  Time: 0.778s, 1316.52/s  (0.792s, 1292.54/s)  LR: 8.159e-05  Data: 0.011 (0.013)
Train: 248 [ 900/1251 ( 72%)]  Loss: 3.065 (3.34)  Time: 0.780s, 1313.38/s  (0.792s, 1292.66/s)  LR: 8.159e-05  Data: 0.011 (0.013)
Train: 248 [ 950/1251 ( 76%)]  Loss: 3.539 (3.35)  Time: 0.776s, 1319.16/s  (0.792s, 1292.72/s)  LR: 8.159e-05  Data: 0.011 (0.013)
Train: 248 [1000/1251 ( 80%)]  Loss: 3.477 (3.35)  Time: 0.779s, 1314.97/s  (0.792s, 1293.23/s)  LR: 8.159e-05  Data: 0.011 (0.013)
Train: 248 [1050/1251 ( 84%)]  Loss: 3.299 (3.35)  Time: 0.821s, 1247.46/s  (0.791s, 1293.75/s)  LR: 8.159e-05  Data: 0.012 (0.013)
Train: 248 [1100/1251 ( 88%)]  Loss: 3.040 (3.34)  Time: 0.779s, 1314.38/s  (0.792s, 1293.35/s)  LR: 8.159e-05  Data: 0.012 (0.013)
Train: 248 [1150/1251 ( 92%)]  Loss: 3.280 (3.33)  Time: 0.788s, 1299.71/s  (0.792s, 1293.59/s)  LR: 8.159e-05  Data: 0.010 (0.013)
Train: 248 [1200/1251 ( 96%)]  Loss: 3.172 (3.33)  Time: 0.779s, 1314.44/s  (0.792s, 1293.58/s)  LR: 8.159e-05  Data: 0.011 (0.013)
Train: 248 [1250/1251 (100%)]  Loss: 3.213 (3.32)  Time: 0.769s, 1331.34/s  (0.791s, 1294.02/s)  LR: 8.159e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.560 (1.560)  Loss:  0.7103 (0.7103)  Acc@1: 91.1133 (91.1133)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.172 (0.567)  Loss:  0.7622 (1.2169)  Acc@1: 87.0283 (77.3440)  Acc@5: 97.9953 (93.6700)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-247.pth.tar', 77.37199992919922)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-248.pth.tar', 77.3439999243164)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-246.pth.tar', 77.32400003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-243.pth.tar', 77.25399993164062)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-242.pth.tar', 76.99400003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-240.pth.tar', 76.99200013427735)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-239.pth.tar', 76.94799987792969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-237.pth.tar', 76.9320000341797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-238.pth.tar', 76.92600016113282)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-245.pth.tar', 76.92600010742187)

Train: 249 [   0/1251 (  0%)]  Loss: 3.133 (3.13)  Time: 2.254s,  454.23/s  (2.254s,  454.23/s)  LR: 7.893e-05  Data: 1.520 (1.520)
Train: 249 [  50/1251 (  4%)]  Loss: 3.514 (3.32)  Time: 0.819s, 1249.78/s  (0.827s, 1237.58/s)  LR: 7.893e-05  Data: 0.012 (0.047)
Train: 249 [ 100/1251 (  8%)]  Loss: 3.255 (3.30)  Time: 0.803s, 1275.48/s  (0.820s, 1248.77/s)  LR: 7.893e-05  Data: 0.011 (0.029)
Train: 249 [ 150/1251 ( 12%)]  Loss: 3.351 (3.31)  Time: 0.780s, 1312.57/s  (0.810s, 1263.45/s)  LR: 7.893e-05  Data: 0.010 (0.023)
Train: 249 [ 200/1251 ( 16%)]  Loss: 3.251 (3.30)  Time: 0.829s, 1234.79/s  (0.808s, 1267.70/s)  LR: 7.893e-05  Data: 0.012 (0.020)
Train: 249 [ 250/1251 ( 20%)]  Loss: 3.302 (3.30)  Time: 0.830s, 1233.05/s  (0.805s, 1272.15/s)  LR: 7.893e-05  Data: 0.011 (0.019)
Train: 249 [ 300/1251 ( 24%)]  Loss: 3.445 (3.32)  Time: 0.817s, 1253.45/s  (0.803s, 1275.41/s)  LR: 7.893e-05  Data: 0.011 (0.017)
Train: 249 [ 350/1251 ( 28%)]  Loss: 3.342 (3.32)  Time: 0.812s, 1261.62/s  (0.802s, 1277.14/s)  LR: 7.893e-05  Data: 0.011 (0.017)
Train: 249 [ 400/1251 ( 32%)]  Loss: 3.316 (3.32)  Time: 0.811s, 1262.09/s  (0.801s, 1278.76/s)  LR: 7.893e-05  Data: 0.011 (0.016)
Train: 249 [ 450/1251 ( 36%)]  Loss: 3.063 (3.30)  Time: 0.816s, 1254.64/s  (0.799s, 1281.03/s)  LR: 7.893e-05  Data: 0.011 (0.015)
Train: 249 [ 500/1251 ( 40%)]  Loss: 3.150 (3.28)  Time: 0.780s, 1313.04/s  (0.799s, 1281.88/s)  LR: 7.893e-05  Data: 0.010 (0.015)
Train: 249 [ 550/1251 ( 44%)]  Loss: 3.697 (3.32)  Time: 0.774s, 1323.38/s  (0.798s, 1283.94/s)  LR: 7.893e-05  Data: 0.010 (0.015)
Train: 249 [ 600/1251 ( 48%)]  Loss: 3.192 (3.31)  Time: 0.778s, 1316.70/s  (0.796s, 1285.98/s)  LR: 7.893e-05  Data: 0.012 (0.014)
Train: 249 [ 650/1251 ( 52%)]  Loss: 3.496 (3.32)  Time: 0.778s, 1316.34/s  (0.796s, 1286.87/s)  LR: 7.893e-05  Data: 0.011 (0.014)
Train: 249 [ 700/1251 ( 56%)]  Loss: 2.990 (3.30)  Time: 0.779s, 1313.81/s  (0.795s, 1287.27/s)  LR: 7.893e-05  Data: 0.012 (0.014)
Train: 249 [ 750/1251 ( 60%)]  Loss: 2.960 (3.28)  Time: 0.801s, 1279.10/s  (0.796s, 1287.16/s)  LR: 7.893e-05  Data: 0.011 (0.014)
Train: 249 [ 800/1251 ( 64%)]  Loss: 3.378 (3.28)  Time: 0.778s, 1316.60/s  (0.795s, 1288.15/s)  LR: 7.893e-05  Data: 0.011 (0.014)
Train: 249 [ 850/1251 ( 68%)]  Loss: 3.037 (3.27)  Time: 0.822s, 1246.48/s  (0.796s, 1286.88/s)  LR: 7.893e-05  Data: 0.012 (0.013)
Train: 249 [ 900/1251 ( 72%)]  Loss: 3.459 (3.28)  Time: 0.782s, 1310.28/s  (0.796s, 1286.38/s)  LR: 7.893e-05  Data: 0.012 (0.013)
Train: 249 [ 950/1251 ( 76%)]  Loss: 3.181 (3.28)  Time: 0.815s, 1255.73/s  (0.796s, 1286.54/s)  LR: 7.893e-05  Data: 0.011 (0.013)
Train: 249 [1000/1251 ( 80%)]  Loss: 3.537 (3.29)  Time: 0.779s, 1314.60/s  (0.796s, 1286.97/s)  LR: 7.893e-05  Data: 0.011 (0.013)
Train: 249 [1050/1251 ( 84%)]  Loss: 3.505 (3.30)  Time: 0.775s, 1320.82/s  (0.795s, 1287.43/s)  LR: 7.893e-05  Data: 0.011 (0.013)
Train: 249 [1100/1251 ( 88%)]  Loss: 3.769 (3.32)  Time: 0.778s, 1315.79/s  (0.795s, 1288.23/s)  LR: 7.893e-05  Data: 0.011 (0.013)
Train: 249 [1150/1251 ( 92%)]  Loss: 3.499 (3.33)  Time: 0.780s, 1313.31/s  (0.794s, 1289.12/s)  LR: 7.893e-05  Data: 0.012 (0.013)
Train: 249 [1200/1251 ( 96%)]  Loss: 3.345 (3.33)  Time: 0.790s, 1296.76/s  (0.794s, 1289.57/s)  LR: 7.893e-05  Data: 0.012 (0.013)
Train: 249 [1250/1251 (100%)]  Loss: 3.481 (3.33)  Time: 0.811s, 1262.47/s  (0.794s, 1289.02/s)  LR: 7.893e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.498 (1.498)  Loss:  0.6983 (0.6983)  Acc@1: 91.2109 (91.2109)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.172 (0.563)  Loss:  0.7950 (1.2501)  Acc@1: 87.2642 (77.3240)  Acc@5: 97.2877 (93.5880)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-247.pth.tar', 77.37199992919922)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-248.pth.tar', 77.3439999243164)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-246.pth.tar', 77.32400003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-249.pth.tar', 77.32400002685547)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-243.pth.tar', 77.25399993164062)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-242.pth.tar', 76.99400003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-240.pth.tar', 76.99200013427735)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-239.pth.tar', 76.94799987792969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-237.pth.tar', 76.9320000341797)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-238.pth.tar', 76.92600016113282)

Train: 250 [   0/1251 (  0%)]  Loss: 3.248 (3.25)  Time: 2.442s,  419.36/s  (2.442s,  419.36/s)  LR: 7.632e-05  Data: 1.707 (1.707)
Train: 250 [  50/1251 (  4%)]  Loss: 3.308 (3.28)  Time: 0.795s, 1288.56/s  (0.818s, 1252.50/s)  LR: 7.632e-05  Data: 0.011 (0.045)
Train: 250 [ 100/1251 (  8%)]  Loss: 3.476 (3.34)  Time: 0.781s, 1310.32/s  (0.800s, 1279.58/s)  LR: 7.632e-05  Data: 0.011 (0.028)
Train: 250 [ 150/1251 ( 12%)]  Loss: 3.505 (3.38)  Time: 0.790s, 1295.61/s  (0.795s, 1287.96/s)  LR: 7.632e-05  Data: 0.011 (0.022)
Train: 250 [ 200/1251 ( 16%)]  Loss: 3.061 (3.32)  Time: 0.778s, 1316.24/s  (0.792s, 1293.22/s)  LR: 7.632e-05  Data: 0.011 (0.020)
Train: 250 [ 250/1251 ( 20%)]  Loss: 3.432 (3.34)  Time: 0.802s, 1276.32/s  (0.792s, 1292.69/s)  LR: 7.632e-05  Data: 0.011 (0.018)
Train: 250 [ 300/1251 ( 24%)]  Loss: 3.336 (3.34)  Time: 0.813s, 1259.65/s  (0.792s, 1293.64/s)  LR: 7.632e-05  Data: 0.011 (0.017)
Train: 250 [ 350/1251 ( 28%)]  Loss: 3.646 (3.38)  Time: 0.782s, 1310.20/s  (0.791s, 1294.75/s)  LR: 7.632e-05  Data: 0.011 (0.016)
Train: 250 [ 400/1251 ( 32%)]  Loss: 3.438 (3.38)  Time: 0.811s, 1263.18/s  (0.791s, 1294.82/s)  LR: 7.632e-05  Data: 0.011 (0.015)
Train: 250 [ 450/1251 ( 36%)]  Loss: 3.207 (3.37)  Time: 0.781s, 1310.89/s  (0.791s, 1294.26/s)  LR: 7.632e-05  Data: 0.011 (0.015)
Train: 250 [ 500/1251 ( 40%)]  Loss: 3.535 (3.38)  Time: 0.812s, 1261.63/s  (0.791s, 1295.36/s)  LR: 7.632e-05  Data: 0.011 (0.015)
Train: 250 [ 550/1251 ( 44%)]  Loss: 3.688 (3.41)  Time: 0.779s, 1315.09/s  (0.791s, 1295.11/s)  LR: 7.632e-05  Data: 0.011 (0.014)
Train: 250 [ 600/1251 ( 48%)]  Loss: 3.210 (3.39)  Time: 0.777s, 1318.52/s  (0.790s, 1296.63/s)  LR: 7.632e-05  Data: 0.011 (0.014)
Train: 250 [ 650/1251 ( 52%)]  Loss: 3.274 (3.38)  Time: 0.775s, 1321.05/s  (0.790s, 1296.81/s)  LR: 7.632e-05  Data: 0.011 (0.014)
Train: 250 [ 700/1251 ( 56%)]  Loss: 3.385 (3.38)  Time: 0.779s, 1315.33/s  (0.789s, 1297.95/s)  LR: 7.632e-05  Data: 0.011 (0.014)
Train: 250 [ 750/1251 ( 60%)]  Loss: 3.017 (3.36)  Time: 0.782s, 1308.98/s  (0.789s, 1297.38/s)  LR: 7.632e-05  Data: 0.011 (0.013)
Train: 250 [ 800/1251 ( 64%)]  Loss: 3.249 (3.35)  Time: 0.821s, 1247.78/s  (0.789s, 1297.48/s)  LR: 7.632e-05  Data: 0.011 (0.013)
Train: 250 [ 850/1251 ( 68%)]  Loss: 3.408 (3.36)  Time: 0.826s, 1240.27/s  (0.790s, 1296.26/s)  LR: 7.632e-05  Data: 0.011 (0.013)
Train: 250 [ 900/1251 ( 72%)]  Loss: 3.419 (3.36)  Time: 0.778s, 1315.79/s  (0.789s, 1297.03/s)  LR: 7.632e-05  Data: 0.011 (0.013)
Train: 250 [ 950/1251 ( 76%)]  Loss: 3.293 (3.36)  Time: 0.779s, 1315.26/s  (0.790s, 1296.56/s)  LR: 7.632e-05  Data: 0.011 (0.013)
Train: 250 [1000/1251 ( 80%)]  Loss: 3.354 (3.36)  Time: 0.778s, 1315.57/s  (0.789s, 1297.28/s)  LR: 7.632e-05  Data: 0.011 (0.013)
Train: 250 [1050/1251 ( 84%)]  Loss: 3.408 (3.36)  Time: 0.780s, 1313.25/s  (0.789s, 1297.54/s)  LR: 7.632e-05  Data: 0.011 (0.013)
Train: 250 [1100/1251 ( 88%)]  Loss: 3.127 (3.35)  Time: 0.778s, 1316.42/s  (0.789s, 1297.56/s)  LR: 7.632e-05  Data: 0.011 (0.013)
Train: 250 [1150/1251 ( 92%)]  Loss: 3.257 (3.35)  Time: 0.811s, 1263.09/s  (0.789s, 1297.49/s)  LR: 7.632e-05  Data: 0.010 (0.013)
Train: 250 [1200/1251 ( 96%)]  Loss: 3.403 (3.35)  Time: 0.812s, 1261.78/s  (0.790s, 1296.52/s)  LR: 7.632e-05  Data: 0.011 (0.012)
Train: 250 [1250/1251 (100%)]  Loss: 3.344 (3.35)  Time: 0.785s, 1305.16/s  (0.790s, 1295.79/s)  LR: 7.632e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.561 (1.561)  Loss:  0.7624 (0.7624)  Acc@1: 91.5039 (91.5039)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.172 (0.565)  Loss:  0.8340 (1.2787)  Acc@1: 87.7358 (77.3700)  Acc@5: 97.6415 (93.6760)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-247.pth.tar', 77.37199992919922)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-250.pth.tar', 77.36999997314453)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-248.pth.tar', 77.3439999243164)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-246.pth.tar', 77.32400003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-249.pth.tar', 77.32400002685547)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-243.pth.tar', 77.25399993164062)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-242.pth.tar', 76.99400003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-240.pth.tar', 76.99200013427735)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-239.pth.tar', 76.94799987792969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-237.pth.tar', 76.9320000341797)

Train: 251 [   0/1251 (  0%)]  Loss: 3.068 (3.07)  Time: 2.205s,  464.29/s  (2.205s,  464.29/s)  LR: 7.375e-05  Data: 1.472 (1.472)
Train: 251 [  50/1251 (  4%)]  Loss: 3.324 (3.20)  Time: 0.781s, 1310.91/s  (0.816s, 1255.10/s)  LR: 7.375e-05  Data: 0.011 (0.045)
Train: 251 [ 100/1251 (  8%)]  Loss: 3.283 (3.22)  Time: 0.830s, 1233.87/s  (0.803s, 1275.44/s)  LR: 7.375e-05  Data: 0.011 (0.028)
Train: 251 [ 150/1251 ( 12%)]  Loss: 3.086 (3.19)  Time: 0.777s, 1317.47/s  (0.800s, 1280.47/s)  LR: 7.375e-05  Data: 0.011 (0.023)
Train: 251 [ 200/1251 ( 16%)]  Loss: 3.322 (3.22)  Time: 0.819s, 1250.42/s  (0.798s, 1282.72/s)  LR: 7.375e-05  Data: 0.011 (0.020)
Train: 251 [ 250/1251 ( 20%)]  Loss: 3.359 (3.24)  Time: 0.819s, 1250.69/s  (0.801s, 1278.17/s)  LR: 7.375e-05  Data: 0.011 (0.018)
Train: 251 [ 300/1251 ( 24%)]  Loss: 3.163 (3.23)  Time: 0.817s, 1253.05/s  (0.800s, 1280.50/s)  LR: 7.375e-05  Data: 0.011 (0.017)
Train: 251 [ 350/1251 ( 28%)]  Loss: 3.407 (3.25)  Time: 0.777s, 1317.58/s  (0.800s, 1280.20/s)  LR: 7.375e-05  Data: 0.011 (0.016)
Train: 251 [ 400/1251 ( 32%)]  Loss: 3.430 (3.27)  Time: 0.812s, 1261.83/s  (0.799s, 1282.35/s)  LR: 7.375e-05  Data: 0.011 (0.015)
Train: 251 [ 450/1251 ( 36%)]  Loss: 3.046 (3.25)  Time: 0.817s, 1254.09/s  (0.799s, 1280.84/s)  LR: 7.375e-05  Data: 0.011 (0.015)
Train: 251 [ 500/1251 ( 40%)]  Loss: 3.529 (3.27)  Time: 0.832s, 1231.34/s  (0.799s, 1281.30/s)  LR: 7.375e-05  Data: 0.011 (0.015)
Train: 251 [ 550/1251 ( 44%)]  Loss: 3.353 (3.28)  Time: 0.789s, 1298.48/s  (0.798s, 1283.44/s)  LR: 7.375e-05  Data: 0.012 (0.014)
Train: 251 [ 600/1251 ( 48%)]  Loss: 3.236 (3.28)  Time: 0.775s, 1321.41/s  (0.797s, 1284.61/s)  LR: 7.375e-05  Data: 0.011 (0.014)
Train: 251 [ 650/1251 ( 52%)]  Loss: 3.463 (3.29)  Time: 0.779s, 1315.27/s  (0.797s, 1285.49/s)  LR: 7.375e-05  Data: 0.011 (0.014)
Train: 251 [ 700/1251 ( 56%)]  Loss: 3.379 (3.30)  Time: 0.779s, 1314.44/s  (0.796s, 1286.66/s)  LR: 7.375e-05  Data: 0.012 (0.014)
Train: 251 [ 750/1251 ( 60%)]  Loss: 3.513 (3.31)  Time: 0.780s, 1313.57/s  (0.796s, 1286.74/s)  LR: 7.375e-05  Data: 0.011 (0.013)
Train: 251 [ 800/1251 ( 64%)]  Loss: 3.108 (3.30)  Time: 0.779s, 1315.27/s  (0.795s, 1287.92/s)  LR: 7.375e-05  Data: 0.011 (0.013)
Train: 251 [ 850/1251 ( 68%)]  Loss: 3.711 (3.32)  Time: 0.784s, 1305.41/s  (0.795s, 1288.12/s)  LR: 7.375e-05  Data: 0.010 (0.013)
Train: 251 [ 900/1251 ( 72%)]  Loss: 3.542 (3.33)  Time: 0.826s, 1240.00/s  (0.795s, 1288.43/s)  LR: 7.375e-05  Data: 0.012 (0.013)
Train: 251 [ 950/1251 ( 76%)]  Loss: 3.127 (3.32)  Time: 0.779s, 1314.90/s  (0.794s, 1289.02/s)  LR: 7.375e-05  Data: 0.011 (0.013)
Train: 251 [1000/1251 ( 80%)]  Loss: 3.323 (3.32)  Time: 0.791s, 1293.92/s  (0.794s, 1289.36/s)  LR: 7.375e-05  Data: 0.013 (0.013)
Train: 251 [1050/1251 ( 84%)]  Loss: 3.473 (3.33)  Time: 0.779s, 1313.96/s  (0.794s, 1289.11/s)  LR: 7.375e-05  Data: 0.011 (0.013)
Train: 251 [1100/1251 ( 88%)]  Loss: 3.484 (3.34)  Time: 0.828s, 1237.34/s  (0.794s, 1289.70/s)  LR: 7.375e-05  Data: 0.011 (0.013)
Train: 251 [1150/1251 ( 92%)]  Loss: 3.387 (3.34)  Time: 0.782s, 1309.65/s  (0.794s, 1290.33/s)  LR: 7.375e-05  Data: 0.011 (0.013)
Train: 251 [1200/1251 ( 96%)]  Loss: 3.170 (3.33)  Time: 0.779s, 1314.74/s  (0.793s, 1290.64/s)  LR: 7.375e-05  Data: 0.011 (0.013)
Train: 251 [1250/1251 (100%)]  Loss: 3.575 (3.34)  Time: 0.772s, 1325.88/s  (0.793s, 1290.71/s)  LR: 7.375e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.501 (1.501)  Loss:  0.7985 (0.7985)  Acc@1: 90.9180 (90.9180)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.172 (0.567)  Loss:  0.8667 (1.3324)  Acc@1: 86.4387 (77.0940)  Acc@5: 97.1698 (93.5440)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-247.pth.tar', 77.37199992919922)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-250.pth.tar', 77.36999997314453)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-248.pth.tar', 77.3439999243164)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-246.pth.tar', 77.32400003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-249.pth.tar', 77.32400002685547)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-243.pth.tar', 77.25399993164062)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-251.pth.tar', 77.09400005615234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-242.pth.tar', 76.99400003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-240.pth.tar', 76.99200013427735)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-239.pth.tar', 76.94799987792969)

Train: 252 [   0/1251 (  0%)]  Loss: 3.394 (3.39)  Time: 2.375s,  431.22/s  (2.375s,  431.22/s)  LR: 7.123e-05  Data: 1.581 (1.581)
Train: 252 [  50/1251 (  4%)]  Loss: 3.419 (3.41)  Time: 0.781s, 1311.52/s  (0.841s, 1217.80/s)  LR: 7.123e-05  Data: 0.011 (0.047)
Train: 252 [ 100/1251 (  8%)]  Loss: 3.117 (3.31)  Time: 0.838s, 1222.27/s  (0.814s, 1258.33/s)  LR: 7.123e-05  Data: 0.011 (0.029)
Train: 252 [ 150/1251 ( 12%)]  Loss: 3.216 (3.29)  Time: 0.816s, 1254.79/s  (0.809s, 1265.32/s)  LR: 7.123e-05  Data: 0.011 (0.023)
Train: 252 [ 200/1251 ( 16%)]  Loss: 3.443 (3.32)  Time: 0.781s, 1311.65/s  (0.806s, 1269.87/s)  LR: 7.123e-05  Data: 0.011 (0.020)
Train: 252 [ 250/1251 ( 20%)]  Loss: 3.780 (3.39)  Time: 0.779s, 1314.91/s  (0.805s, 1272.37/s)  LR: 7.123e-05  Data: 0.011 (0.019)
Train: 252 [ 300/1251 ( 24%)]  Loss: 3.183 (3.36)  Time: 0.777s, 1318.16/s  (0.803s, 1274.93/s)  LR: 7.123e-05  Data: 0.011 (0.017)
Train: 252 [ 350/1251 ( 28%)]  Loss: 3.274 (3.35)  Time: 0.819s, 1250.03/s  (0.802s, 1277.08/s)  LR: 7.123e-05  Data: 0.011 (0.016)
Train: 252 [ 400/1251 ( 32%)]  Loss: 3.414 (3.36)  Time: 0.820s, 1249.48/s  (0.801s, 1278.74/s)  LR: 7.123e-05  Data: 0.011 (0.016)
Train: 252 [ 450/1251 ( 36%)]  Loss: 3.147 (3.34)  Time: 0.781s, 1310.34/s  (0.800s, 1279.94/s)  LR: 7.123e-05  Data: 0.011 (0.015)
Train: 252 [ 500/1251 ( 40%)]  Loss: 3.340 (3.34)  Time: 0.779s, 1314.35/s  (0.799s, 1282.39/s)  LR: 7.123e-05  Data: 0.011 (0.015)
Train: 252 [ 550/1251 ( 44%)]  Loss: 3.528 (3.35)  Time: 0.777s, 1318.03/s  (0.797s, 1284.94/s)  LR: 7.123e-05  Data: 0.011 (0.014)
Train: 252 [ 600/1251 ( 48%)]  Loss: 3.249 (3.35)  Time: 0.780s, 1313.13/s  (0.796s, 1286.99/s)  LR: 7.123e-05  Data: 0.012 (0.014)
Train: 252 [ 650/1251 ( 52%)]  Loss: 3.047 (3.33)  Time: 0.812s, 1261.80/s  (0.795s, 1287.40/s)  LR: 7.123e-05  Data: 0.011 (0.014)
Train: 252 [ 700/1251 ( 56%)]  Loss: 3.247 (3.32)  Time: 0.784s, 1305.52/s  (0.794s, 1288.92/s)  LR: 7.123e-05  Data: 0.011 (0.014)
Train: 252 [ 750/1251 ( 60%)]  Loss: 3.507 (3.33)  Time: 0.780s, 1312.03/s  (0.794s, 1289.41/s)  LR: 7.123e-05  Data: 0.011 (0.014)
Train: 252 [ 800/1251 ( 64%)]  Loss: 3.129 (3.32)  Time: 0.778s, 1315.96/s  (0.794s, 1289.63/s)  LR: 7.123e-05  Data: 0.011 (0.013)
Train: 252 [ 850/1251 ( 68%)]  Loss: 3.191 (3.31)  Time: 0.780s, 1313.32/s  (0.794s, 1290.47/s)  LR: 7.123e-05  Data: 0.011 (0.013)
Train: 252 [ 900/1251 ( 72%)]  Loss: 3.638 (3.33)  Time: 0.778s, 1315.53/s  (0.793s, 1291.61/s)  LR: 7.123e-05  Data: 0.011 (0.013)
Train: 252 [ 950/1251 ( 76%)]  Loss: 3.684 (3.35)  Time: 0.780s, 1312.63/s  (0.792s, 1292.60/s)  LR: 7.123e-05  Data: 0.011 (0.013)
Train: 252 [1000/1251 ( 80%)]  Loss: 3.263 (3.34)  Time: 0.784s, 1306.45/s  (0.792s, 1292.32/s)  LR: 7.123e-05  Data: 0.011 (0.013)
Train: 252 [1050/1251 ( 84%)]  Loss: 3.562 (3.35)  Time: 0.777s, 1317.88/s  (0.792s, 1292.34/s)  LR: 7.123e-05  Data: 0.011 (0.013)
Train: 252 [1100/1251 ( 88%)]  Loss: 2.805 (3.33)  Time: 0.777s, 1317.06/s  (0.792s, 1292.96/s)  LR: 7.123e-05  Data: 0.011 (0.013)
Train: 252 [1150/1251 ( 92%)]  Loss: 3.621 (3.34)  Time: 0.777s, 1317.68/s  (0.791s, 1293.81/s)  LR: 7.123e-05  Data: 0.011 (0.013)
Train: 252 [1200/1251 ( 96%)]  Loss: 3.320 (3.34)  Time: 0.782s, 1308.87/s  (0.791s, 1294.48/s)  LR: 7.123e-05  Data: 0.011 (0.013)
Train: 252 [1250/1251 (100%)]  Loss: 3.602 (3.35)  Time: 0.826s, 1239.45/s  (0.791s, 1293.80/s)  LR: 7.123e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.525 (1.525)  Loss:  0.7214 (0.7214)  Acc@1: 91.1133 (91.1133)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.172 (0.566)  Loss:  0.8082 (1.2256)  Acc@1: 86.7924 (77.5440)  Acc@5: 97.2877 (93.8300)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-252.pth.tar', 77.54399995117187)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-247.pth.tar', 77.37199992919922)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-250.pth.tar', 77.36999997314453)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-248.pth.tar', 77.3439999243164)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-246.pth.tar', 77.32400003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-249.pth.tar', 77.32400002685547)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-243.pth.tar', 77.25399993164062)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-251.pth.tar', 77.09400005615234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-242.pth.tar', 76.99400003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-240.pth.tar', 76.99200013427735)

Train: 253 [   0/1251 (  0%)]  Loss: 3.438 (3.44)  Time: 2.366s,  432.84/s  (2.366s,  432.84/s)  LR: 6.875e-05  Data: 1.585 (1.585)
Train: 253 [  50/1251 (  4%)]  Loss: 3.331 (3.38)  Time: 0.779s, 1314.18/s  (0.820s, 1249.13/s)  LR: 6.875e-05  Data: 0.011 (0.045)
Train: 253 [ 100/1251 (  8%)]  Loss: 3.322 (3.36)  Time: 0.838s, 1222.27/s  (0.809s, 1265.38/s)  LR: 6.875e-05  Data: 0.011 (0.028)
Train: 253 [ 150/1251 ( 12%)]  Loss: 3.383 (3.37)  Time: 0.778s, 1316.21/s  (0.801s, 1278.67/s)  LR: 6.875e-05  Data: 0.011 (0.023)
Train: 253 [ 200/1251 ( 16%)]  Loss: 3.259 (3.35)  Time: 0.792s, 1293.18/s  (0.797s, 1284.10/s)  LR: 6.875e-05  Data: 0.011 (0.020)
Train: 253 [ 250/1251 ( 20%)]  Loss: 3.567 (3.38)  Time: 0.809s, 1265.45/s  (0.798s, 1283.80/s)  LR: 6.875e-05  Data: 0.012 (0.018)
Train: 253 [ 300/1251 ( 24%)]  Loss: 3.155 (3.35)  Time: 0.777s, 1318.01/s  (0.796s, 1285.97/s)  LR: 6.875e-05  Data: 0.011 (0.017)
Train: 253 [ 350/1251 ( 28%)]  Loss: 3.526 (3.37)  Time: 0.779s, 1315.34/s  (0.796s, 1285.97/s)  LR: 6.875e-05  Data: 0.011 (0.016)
Train: 253 [ 400/1251 ( 32%)]  Loss: 3.559 (3.39)  Time: 0.783s, 1307.47/s  (0.795s, 1287.31/s)  LR: 6.875e-05  Data: 0.012 (0.015)
Train: 253 [ 450/1251 ( 36%)]  Loss: 3.301 (3.38)  Time: 0.778s, 1315.48/s  (0.794s, 1289.82/s)  LR: 6.875e-05  Data: 0.012 (0.015)
Train: 253 [ 500/1251 ( 40%)]  Loss: 3.340 (3.38)  Time: 0.777s, 1317.32/s  (0.793s, 1291.40/s)  LR: 6.875e-05  Data: 0.011 (0.015)
Train: 253 [ 550/1251 ( 44%)]  Loss: 3.408 (3.38)  Time: 0.782s, 1309.61/s  (0.793s, 1291.45/s)  LR: 6.875e-05  Data: 0.011 (0.014)
Train: 253 [ 600/1251 ( 48%)]  Loss: 3.364 (3.38)  Time: 0.779s, 1314.54/s  (0.793s, 1290.88/s)  LR: 6.875e-05  Data: 0.011 (0.014)
Train: 253 [ 650/1251 ( 52%)]  Loss: 3.208 (3.37)  Time: 0.779s, 1314.29/s  (0.793s, 1291.51/s)  LR: 6.875e-05  Data: 0.012 (0.014)
Train: 253 [ 700/1251 ( 56%)]  Loss: 3.448 (3.37)  Time: 0.783s, 1307.09/s  (0.793s, 1291.18/s)  LR: 6.875e-05  Data: 0.013 (0.014)
Train: 253 [ 750/1251 ( 60%)]  Loss: 3.516 (3.38)  Time: 0.778s, 1316.29/s  (0.793s, 1290.72/s)  LR: 6.875e-05  Data: 0.011 (0.013)
Train: 253 [ 800/1251 ( 64%)]  Loss: 3.551 (3.39)  Time: 0.817s, 1253.89/s  (0.795s, 1287.85/s)  LR: 6.875e-05  Data: 0.011 (0.013)
Train: 253 [ 850/1251 ( 68%)]  Loss: 3.175 (3.38)  Time: 0.786s, 1303.59/s  (0.795s, 1287.93/s)  LR: 6.875e-05  Data: 0.011 (0.013)
Train: 253 [ 900/1251 ( 72%)]  Loss: 3.469 (3.39)  Time: 0.793s, 1291.48/s  (0.795s, 1288.64/s)  LR: 6.875e-05  Data: 0.011 (0.013)
Train: 253 [ 950/1251 ( 76%)]  Loss: 3.256 (3.38)  Time: 0.777s, 1317.19/s  (0.795s, 1288.43/s)  LR: 6.875e-05  Data: 0.011 (0.013)
Train: 253 [1000/1251 ( 80%)]  Loss: 3.425 (3.38)  Time: 0.816s, 1255.16/s  (0.795s, 1288.70/s)  LR: 6.875e-05  Data: 0.011 (0.013)
Train: 253 [1050/1251 ( 84%)]  Loss: 3.189 (3.37)  Time: 0.792s, 1293.72/s  (0.795s, 1287.35/s)  LR: 6.875e-05  Data: 0.010 (0.013)
Train: 253 [1100/1251 ( 88%)]  Loss: 3.520 (3.38)  Time: 0.818s, 1251.16/s  (0.795s, 1287.36/s)  LR: 6.875e-05  Data: 0.011 (0.013)
Train: 253 [1150/1251 ( 92%)]  Loss: 3.521 (3.38)  Time: 0.821s, 1246.81/s  (0.795s, 1288.14/s)  LR: 6.875e-05  Data: 0.011 (0.013)
Train: 253 [1200/1251 ( 96%)]  Loss: 3.508 (3.39)  Time: 0.778s, 1316.36/s  (0.795s, 1288.07/s)  LR: 6.875e-05  Data: 0.012 (0.013)
Train: 253 [1250/1251 (100%)]  Loss: 3.455 (3.39)  Time: 0.766s, 1337.63/s  (0.794s, 1289.03/s)  LR: 6.875e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.541 (1.541)  Loss:  0.7334 (0.7334)  Acc@1: 91.4062 (91.4062)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.172 (0.561)  Loss:  0.7951 (1.2445)  Acc@1: 87.1462 (77.4660)  Acc@5: 97.7594 (93.8940)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-252.pth.tar', 77.54399995117187)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-253.pth.tar', 77.46599997558594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-247.pth.tar', 77.37199992919922)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-250.pth.tar', 77.36999997314453)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-248.pth.tar', 77.3439999243164)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-246.pth.tar', 77.32400003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-249.pth.tar', 77.32400002685547)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-243.pth.tar', 77.25399993164062)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-251.pth.tar', 77.09400005615234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-242.pth.tar', 76.99400003417969)

Train: 254 [   0/1251 (  0%)]  Loss: 3.291 (3.29)  Time: 2.408s,  425.16/s  (2.408s,  425.16/s)  LR: 6.633e-05  Data: 1.672 (1.672)
Train: 254 [  50/1251 (  4%)]  Loss: 3.563 (3.43)  Time: 0.812s, 1261.37/s  (0.834s, 1227.85/s)  LR: 6.633e-05  Data: 0.011 (0.049)
Train: 254 [ 100/1251 (  8%)]  Loss: 3.367 (3.41)  Time: 0.781s, 1311.16/s  (0.812s, 1261.16/s)  LR: 6.633e-05  Data: 0.012 (0.030)
Train: 254 [ 150/1251 ( 12%)]  Loss: 3.093 (3.33)  Time: 0.794s, 1289.44/s  (0.806s, 1270.93/s)  LR: 6.633e-05  Data: 0.011 (0.024)
Train: 254 [ 200/1251 ( 16%)]  Loss: 3.128 (3.29)  Time: 0.809s, 1265.46/s  (0.801s, 1278.13/s)  LR: 6.633e-05  Data: 0.011 (0.021)
Train: 254 [ 250/1251 ( 20%)]  Loss: 3.521 (3.33)  Time: 0.809s, 1265.04/s  (0.799s, 1281.81/s)  LR: 6.633e-05  Data: 0.011 (0.019)
Train: 254 [ 300/1251 ( 24%)]  Loss: 3.196 (3.31)  Time: 0.793s, 1291.57/s  (0.800s, 1279.89/s)  LR: 6.633e-05  Data: 0.011 (0.017)
Train: 254 [ 350/1251 ( 28%)]  Loss: 3.665 (3.35)  Time: 0.837s, 1222.99/s  (0.797s, 1284.44/s)  LR: 6.633e-05  Data: 0.011 (0.017)
Train: 254 [ 400/1251 ( 32%)]  Loss: 3.244 (3.34)  Time: 0.863s, 1186.32/s  (0.798s, 1282.82/s)  LR: 6.633e-05  Data: 0.012 (0.016)
Train: 254 [ 450/1251 ( 36%)]  Loss: 3.426 (3.35)  Time: 0.786s, 1302.57/s  (0.797s, 1284.60/s)  LR: 6.633e-05  Data: 0.011 (0.015)
Train: 254 [ 500/1251 ( 40%)]  Loss: 3.163 (3.33)  Time: 0.790s, 1295.72/s  (0.798s, 1283.59/s)  LR: 6.633e-05  Data: 0.011 (0.015)
Train: 254 [ 550/1251 ( 44%)]  Loss: 3.321 (3.33)  Time: 0.788s, 1299.79/s  (0.797s, 1285.21/s)  LR: 6.633e-05  Data: 0.011 (0.015)
Train: 254 [ 600/1251 ( 48%)]  Loss: 3.340 (3.33)  Time: 0.775s, 1321.74/s  (0.796s, 1286.37/s)  LR: 6.633e-05  Data: 0.011 (0.014)
Train: 254 [ 650/1251 ( 52%)]  Loss: 3.412 (3.34)  Time: 0.779s, 1314.77/s  (0.796s, 1287.08/s)  LR: 6.633e-05  Data: 0.011 (0.014)
Train: 254 [ 700/1251 ( 56%)]  Loss: 3.424 (3.34)  Time: 0.814s, 1257.35/s  (0.796s, 1286.92/s)  LR: 6.633e-05  Data: 0.011 (0.014)
Train: 254 [ 750/1251 ( 60%)]  Loss: 3.315 (3.34)  Time: 0.810s, 1264.03/s  (0.796s, 1286.74/s)  LR: 6.633e-05  Data: 0.011 (0.014)
Train: 254 [ 800/1251 ( 64%)]  Loss: 3.565 (3.36)  Time: 0.778s, 1315.99/s  (0.795s, 1287.49/s)  LR: 6.633e-05  Data: 0.011 (0.013)
Train: 254 [ 850/1251 ( 68%)]  Loss: 3.599 (3.37)  Time: 0.778s, 1316.66/s  (0.796s, 1286.52/s)  LR: 6.633e-05  Data: 0.010 (0.013)
Train: 254 [ 900/1251 ( 72%)]  Loss: 3.443 (3.37)  Time: 0.779s, 1314.47/s  (0.796s, 1287.18/s)  LR: 6.633e-05  Data: 0.011 (0.013)
Train: 254 [ 950/1251 ( 76%)]  Loss: 3.305 (3.37)  Time: 0.810s, 1264.65/s  (0.796s, 1286.55/s)  LR: 6.633e-05  Data: 0.011 (0.013)
Train: 254 [1000/1251 ( 80%)]  Loss: 3.449 (3.37)  Time: 0.818s, 1251.47/s  (0.796s, 1286.50/s)  LR: 6.633e-05  Data: 0.012 (0.013)
Train: 254 [1050/1251 ( 84%)]  Loss: 3.371 (3.37)  Time: 0.781s, 1310.61/s  (0.796s, 1286.97/s)  LR: 6.633e-05  Data: 0.011 (0.013)
Train: 254 [1100/1251 ( 88%)]  Loss: 3.324 (3.37)  Time: 0.780s, 1312.95/s  (0.795s, 1287.32/s)  LR: 6.633e-05  Data: 0.011 (0.013)
Train: 254 [1150/1251 ( 92%)]  Loss: 3.433 (3.37)  Time: 0.815s, 1256.53/s  (0.795s, 1287.99/s)  LR: 6.633e-05  Data: 0.010 (0.013)
Train: 254 [1200/1251 ( 96%)]  Loss: 3.143 (3.36)  Time: 0.776s, 1318.98/s  (0.796s, 1287.12/s)  LR: 6.633e-05  Data: 0.011 (0.013)
Train: 254 [1250/1251 (100%)]  Loss: 3.301 (3.36)  Time: 0.765s, 1338.15/s  (0.796s, 1287.20/s)  LR: 6.633e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.517 (1.517)  Loss:  0.7179 (0.7179)  Acc@1: 91.2109 (91.2109)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.172 (0.560)  Loss:  0.7932 (1.2570)  Acc@1: 87.1462 (77.5360)  Acc@5: 97.5236 (93.7080)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-252.pth.tar', 77.54399995117187)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-254.pth.tar', 77.53599997558594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-253.pth.tar', 77.46599997558594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-247.pth.tar', 77.37199992919922)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-250.pth.tar', 77.36999997314453)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-248.pth.tar', 77.3439999243164)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-246.pth.tar', 77.32400003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-249.pth.tar', 77.32400002685547)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-243.pth.tar', 77.25399993164062)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-251.pth.tar', 77.09400005615234)

Train: 255 [   0/1251 (  0%)]  Loss: 3.245 (3.24)  Time: 2.261s,  452.81/s  (2.261s,  452.81/s)  LR: 6.395e-05  Data: 1.525 (1.525)
Train: 255 [  50/1251 (  4%)]  Loss: 2.908 (3.08)  Time: 0.792s, 1292.39/s  (0.836s, 1224.27/s)  LR: 6.395e-05  Data: 0.011 (0.046)
Train: 255 [ 100/1251 (  8%)]  Loss: 3.101 (3.08)  Time: 0.786s, 1302.83/s  (0.821s, 1247.83/s)  LR: 6.395e-05  Data: 0.011 (0.029)
Train: 255 [ 150/1251 ( 12%)]  Loss: 3.163 (3.10)  Time: 0.782s, 1309.16/s  (0.809s, 1266.28/s)  LR: 6.395e-05  Data: 0.011 (0.023)
Train: 255 [ 200/1251 ( 16%)]  Loss: 3.475 (3.18)  Time: 0.780s, 1312.64/s  (0.802s, 1276.02/s)  LR: 6.395e-05  Data: 0.011 (0.020)
Train: 255 [ 250/1251 ( 20%)]  Loss: 3.609 (3.25)  Time: 0.820s, 1249.05/s  (0.801s, 1278.23/s)  LR: 6.395e-05  Data: 0.011 (0.018)
Train: 255 [ 300/1251 ( 24%)]  Loss: 3.215 (3.25)  Time: 0.777s, 1318.52/s  (0.801s, 1278.14/s)  LR: 6.395e-05  Data: 0.011 (0.017)
Train: 255 [ 350/1251 ( 28%)]  Loss: 3.353 (3.26)  Time: 0.776s, 1320.11/s  (0.799s, 1280.93/s)  LR: 6.395e-05  Data: 0.011 (0.016)
Train: 255 [ 400/1251 ( 32%)]  Loss: 3.418 (3.28)  Time: 0.778s, 1316.32/s  (0.799s, 1280.96/s)  LR: 6.395e-05  Data: 0.011 (0.016)
Train: 255 [ 450/1251 ( 36%)]  Loss: 3.403 (3.29)  Time: 0.779s, 1314.32/s  (0.799s, 1282.36/s)  LR: 6.395e-05  Data: 0.011 (0.015)
Train: 255 [ 500/1251 ( 40%)]  Loss: 3.421 (3.30)  Time: 0.777s, 1317.80/s  (0.797s, 1285.00/s)  LR: 6.395e-05  Data: 0.011 (0.015)
Train: 255 [ 550/1251 ( 44%)]  Loss: 3.303 (3.30)  Time: 0.782s, 1310.20/s  (0.796s, 1286.09/s)  LR: 6.395e-05  Data: 0.011 (0.014)
Train: 255 [ 600/1251 ( 48%)]  Loss: 3.469 (3.31)  Time: 0.819s, 1250.50/s  (0.796s, 1286.53/s)  LR: 6.395e-05  Data: 0.011 (0.014)
Train: 255 [ 650/1251 ( 52%)]  Loss: 3.414 (3.32)  Time: 0.791s, 1294.61/s  (0.795s, 1288.00/s)  LR: 6.395e-05  Data: 0.011 (0.014)
Train: 255 [ 700/1251 ( 56%)]  Loss: 3.055 (3.30)  Time: 0.778s, 1316.27/s  (0.795s, 1287.61/s)  LR: 6.395e-05  Data: 0.011 (0.014)
Train: 255 [ 750/1251 ( 60%)]  Loss: 3.261 (3.30)  Time: 0.777s, 1317.41/s  (0.795s, 1287.76/s)  LR: 6.395e-05  Data: 0.011 (0.014)
Train: 255 [ 800/1251 ( 64%)]  Loss: 3.474 (3.31)  Time: 0.812s, 1260.96/s  (0.795s, 1288.01/s)  LR: 6.395e-05  Data: 0.011 (0.013)
Train: 255 [ 850/1251 ( 68%)]  Loss: 3.354 (3.31)  Time: 0.817s, 1252.77/s  (0.796s, 1287.12/s)  LR: 6.395e-05  Data: 0.011 (0.013)
Train: 255 [ 900/1251 ( 72%)]  Loss: 3.180 (3.31)  Time: 0.781s, 1310.97/s  (0.795s, 1287.70/s)  LR: 6.395e-05  Data: 0.011 (0.013)
Train: 255 [ 950/1251 ( 76%)]  Loss: 3.428 (3.31)  Time: 0.776s, 1320.32/s  (0.795s, 1288.57/s)  LR: 6.395e-05  Data: 0.010 (0.013)
Train: 255 [1000/1251 ( 80%)]  Loss: 3.289 (3.31)  Time: 0.781s, 1310.88/s  (0.795s, 1288.32/s)  LR: 6.395e-05  Data: 0.011 (0.013)
Train: 255 [1050/1251 ( 84%)]  Loss: 3.507 (3.32)  Time: 0.815s, 1256.54/s  (0.796s, 1286.58/s)  LR: 6.395e-05  Data: 0.011 (0.013)
Train: 255 [1100/1251 ( 88%)]  Loss: 3.317 (3.32)  Time: 0.784s, 1306.08/s  (0.796s, 1287.00/s)  LR: 6.395e-05  Data: 0.011 (0.013)
Train: 255 [1150/1251 ( 92%)]  Loss: 2.994 (3.31)  Time: 0.779s, 1315.32/s  (0.796s, 1287.05/s)  LR: 6.395e-05  Data: 0.011 (0.013)
Train: 255 [1200/1251 ( 96%)]  Loss: 3.357 (3.31)  Time: 0.778s, 1315.84/s  (0.795s, 1287.46/s)  LR: 6.395e-05  Data: 0.011 (0.013)
Train: 255 [1250/1251 (100%)]  Loss: 3.486 (3.32)  Time: 0.766s, 1336.17/s  (0.795s, 1288.24/s)  LR: 6.395e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.519 (1.519)  Loss:  0.6684 (0.6684)  Acc@1: 90.9180 (90.9180)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.172 (0.566)  Loss:  0.7507 (1.1989)  Acc@1: 87.1462 (77.7320)  Acc@5: 97.2877 (93.7160)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-255.pth.tar', 77.73199997558594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-252.pth.tar', 77.54399995117187)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-254.pth.tar', 77.53599997558594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-253.pth.tar', 77.46599997558594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-247.pth.tar', 77.37199992919922)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-250.pth.tar', 77.36999997314453)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-248.pth.tar', 77.3439999243164)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-246.pth.tar', 77.32400003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-249.pth.tar', 77.32400002685547)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-243.pth.tar', 77.25399993164062)

Train: 256 [   0/1251 (  0%)]  Loss: 3.626 (3.63)  Time: 2.166s,  472.71/s  (2.166s,  472.71/s)  LR: 6.162e-05  Data: 1.431 (1.431)
Train: 256 [  50/1251 (  4%)]  Loss: 2.905 (3.27)  Time: 0.813s, 1258.89/s  (0.830s, 1233.53/s)  LR: 6.162e-05  Data: 0.012 (0.042)
Train: 256 [ 100/1251 (  8%)]  Loss: 3.362 (3.30)  Time: 0.817s, 1253.79/s  (0.818s, 1251.86/s)  LR: 6.162e-05  Data: 0.011 (0.027)
Train: 256 [ 150/1251 ( 12%)]  Loss: 3.522 (3.35)  Time: 0.778s, 1315.38/s  (0.812s, 1261.30/s)  LR: 6.162e-05  Data: 0.011 (0.022)
Train: 256 [ 200/1251 ( 16%)]  Loss: 3.339 (3.35)  Time: 0.779s, 1314.71/s  (0.807s, 1269.41/s)  LR: 6.162e-05  Data: 0.011 (0.019)
Train: 256 [ 250/1251 ( 20%)]  Loss: 3.504 (3.38)  Time: 0.780s, 1312.49/s  (0.803s, 1275.05/s)  LR: 6.162e-05  Data: 0.011 (0.017)
Train: 256 [ 300/1251 ( 24%)]  Loss: 3.170 (3.35)  Time: 0.861s, 1188.64/s  (0.804s, 1274.04/s)  LR: 6.162e-05  Data: 0.011 (0.016)
Train: 256 [ 350/1251 ( 28%)]  Loss: 3.391 (3.35)  Time: 0.779s, 1315.25/s  (0.803s, 1275.42/s)  LR: 6.162e-05  Data: 0.011 (0.016)
Train: 256 [ 400/1251 ( 32%)]  Loss: 3.407 (3.36)  Time: 0.779s, 1314.86/s  (0.802s, 1276.44/s)  LR: 6.162e-05  Data: 0.011 (0.015)
Train: 256 [ 450/1251 ( 36%)]  Loss: 3.493 (3.37)  Time: 0.787s, 1300.76/s  (0.801s, 1277.89/s)  LR: 6.162e-05  Data: 0.011 (0.015)
Train: 256 [ 500/1251 ( 40%)]  Loss: 3.270 (3.36)  Time: 0.811s, 1261.95/s  (0.801s, 1278.03/s)  LR: 6.162e-05  Data: 0.011 (0.014)
Train: 256 [ 550/1251 ( 44%)]  Loss: 3.526 (3.38)  Time: 0.780s, 1312.68/s  (0.800s, 1279.76/s)  LR: 6.162e-05  Data: 0.011 (0.014)
Train: 256 [ 600/1251 ( 48%)]  Loss: 3.410 (3.38)  Time: 0.794s, 1290.25/s  (0.799s, 1281.47/s)  LR: 6.162e-05  Data: 0.011 (0.014)
Train: 256 [ 650/1251 ( 52%)]  Loss: 3.104 (3.36)  Time: 0.781s, 1311.53/s  (0.798s, 1283.67/s)  LR: 6.162e-05  Data: 0.011 (0.014)
Train: 256 [ 700/1251 ( 56%)]  Loss: 3.291 (3.35)  Time: 0.777s, 1317.30/s  (0.797s, 1285.60/s)  LR: 6.162e-05  Data: 0.011 (0.014)
Train: 256 [ 750/1251 ( 60%)]  Loss: 3.175 (3.34)  Time: 0.791s, 1295.27/s  (0.796s, 1286.90/s)  LR: 6.162e-05  Data: 0.011 (0.013)
Train: 256 [ 800/1251 ( 64%)]  Loss: 3.209 (3.34)  Time: 0.815s, 1257.07/s  (0.797s, 1285.49/s)  LR: 6.162e-05  Data: 0.012 (0.013)
Train: 256 [ 850/1251 ( 68%)]  Loss: 3.363 (3.34)  Time: 0.832s, 1230.50/s  (0.798s, 1284.00/s)  LR: 6.162e-05  Data: 0.012 (0.013)
Train: 256 [ 900/1251 ( 72%)]  Loss: 3.422 (3.34)  Time: 0.834s, 1228.48/s  (0.797s, 1284.92/s)  LR: 6.162e-05  Data: 0.011 (0.013)
Train: 256 [ 950/1251 ( 76%)]  Loss: 3.050 (3.33)  Time: 0.781s, 1310.32/s  (0.796s, 1285.77/s)  LR: 6.162e-05  Data: 0.011 (0.013)
Train: 256 [1000/1251 ( 80%)]  Loss: 3.486 (3.33)  Time: 0.820s, 1249.20/s  (0.796s, 1285.97/s)  LR: 6.162e-05  Data: 0.011 (0.013)
Train: 256 [1050/1251 ( 84%)]  Loss: 3.729 (3.35)  Time: 0.783s, 1308.59/s  (0.797s, 1285.30/s)  LR: 6.162e-05  Data: 0.011 (0.013)
Train: 256 [1100/1251 ( 88%)]  Loss: 3.097 (3.34)  Time: 0.778s, 1315.60/s  (0.796s, 1286.12/s)  LR: 6.162e-05  Data: 0.011 (0.013)
Train: 256 [1150/1251 ( 92%)]  Loss: 3.152 (3.33)  Time: 0.785s, 1304.53/s  (0.796s, 1286.91/s)  LR: 6.162e-05  Data: 0.010 (0.013)
Train: 256 [1200/1251 ( 96%)]  Loss: 3.050 (3.32)  Time: 0.819s, 1249.71/s  (0.795s, 1287.25/s)  LR: 6.162e-05  Data: 0.011 (0.013)
Train: 256 [1250/1251 (100%)]  Loss: 3.222 (3.32)  Time: 0.805s, 1272.38/s  (0.796s, 1287.21/s)  LR: 6.162e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.550 (1.550)  Loss:  0.7637 (0.7637)  Acc@1: 91.5039 (91.5039)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.172 (0.562)  Loss:  0.8500 (1.2675)  Acc@1: 85.9670 (77.6500)  Acc@5: 97.0519 (93.7640)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-255.pth.tar', 77.73199997558594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-256.pth.tar', 77.65000010986329)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-252.pth.tar', 77.54399995117187)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-254.pth.tar', 77.53599997558594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-253.pth.tar', 77.46599997558594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-247.pth.tar', 77.37199992919922)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-250.pth.tar', 77.36999997314453)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-248.pth.tar', 77.3439999243164)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-246.pth.tar', 77.32400003417969)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-249.pth.tar', 77.32400002685547)

Train: 257 [   0/1251 (  0%)]  Loss: 3.289 (3.29)  Time: 2.271s,  450.89/s  (2.271s,  450.89/s)  LR: 5.934e-05  Data: 1.535 (1.535)
Train: 257 [  50/1251 (  4%)]  Loss: 3.500 (3.39)  Time: 0.839s, 1220.13/s  (0.818s, 1252.01/s)  LR: 5.934e-05  Data: 0.010 (0.044)
Train: 257 [ 100/1251 (  8%)]  Loss: 3.275 (3.35)  Time: 0.780s, 1313.30/s  (0.809s, 1265.47/s)  LR: 5.934e-05  Data: 0.011 (0.028)
Train: 257 [ 150/1251 ( 12%)]  Loss: 3.410 (3.37)  Time: 0.778s, 1316.53/s  (0.801s, 1279.01/s)  LR: 5.934e-05  Data: 0.010 (0.022)
Train: 257 [ 200/1251 ( 16%)]  Loss: 3.614 (3.42)  Time: 0.779s, 1314.19/s  (0.796s, 1287.21/s)  LR: 5.934e-05  Data: 0.011 (0.019)
Train: 257 [ 250/1251 ( 20%)]  Loss: 3.247 (3.39)  Time: 0.781s, 1311.68/s  (0.792s, 1292.61/s)  LR: 5.934e-05  Data: 0.011 (0.018)
Train: 257 [ 300/1251 ( 24%)]  Loss: 3.003 (3.33)  Time: 0.780s, 1313.60/s  (0.793s, 1290.69/s)  LR: 5.934e-05  Data: 0.011 (0.017)
Train: 257 [ 350/1251 ( 28%)]  Loss: 3.179 (3.31)  Time: 0.779s, 1314.78/s  (0.792s, 1293.18/s)  LR: 5.934e-05  Data: 0.011 (0.016)
Train: 257 [ 400/1251 ( 32%)]  Loss: 3.419 (3.33)  Time: 0.778s, 1315.72/s  (0.792s, 1293.13/s)  LR: 5.934e-05  Data: 0.011 (0.015)
Train: 257 [ 450/1251 ( 36%)]  Loss: 3.081 (3.30)  Time: 0.780s, 1312.81/s  (0.791s, 1295.28/s)  LR: 5.934e-05  Data: 0.011 (0.015)
Train: 257 [ 500/1251 ( 40%)]  Loss: 3.383 (3.31)  Time: 0.877s, 1168.14/s  (0.790s, 1296.34/s)  LR: 5.934e-05  Data: 0.011 (0.014)
Train: 257 [ 550/1251 ( 44%)]  Loss: 3.434 (3.32)  Time: 0.816s, 1255.02/s  (0.791s, 1294.73/s)  LR: 5.934e-05  Data: 0.011 (0.014)
Train: 257 [ 600/1251 ( 48%)]  Loss: 3.070 (3.30)  Time: 0.814s, 1258.37/s  (0.791s, 1294.23/s)  LR: 5.934e-05  Data: 0.011 (0.014)
Train: 257 [ 650/1251 ( 52%)]  Loss: 3.509 (3.32)  Time: 0.780s, 1312.40/s  (0.791s, 1295.32/s)  LR: 5.934e-05  Data: 0.011 (0.014)
Train: 257 [ 700/1251 ( 56%)]  Loss: 2.943 (3.29)  Time: 0.781s, 1311.41/s  (0.790s, 1295.74/s)  LR: 5.934e-05  Data: 0.011 (0.013)
Train: 257 [ 750/1251 ( 60%)]  Loss: 3.779 (3.32)  Time: 0.777s, 1317.15/s  (0.791s, 1294.90/s)  LR: 5.934e-05  Data: 0.011 (0.013)
Train: 257 [ 800/1251 ( 64%)]  Loss: 3.419 (3.33)  Time: 0.784s, 1306.73/s  (0.791s, 1295.10/s)  LR: 5.934e-05  Data: 0.011 (0.013)
Train: 257 [ 850/1251 ( 68%)]  Loss: 3.078 (3.31)  Time: 0.777s, 1318.31/s  (0.791s, 1295.02/s)  LR: 5.934e-05  Data: 0.011 (0.013)
Train: 257 [ 900/1251 ( 72%)]  Loss: 3.072 (3.30)  Time: 0.778s, 1315.65/s  (0.791s, 1294.48/s)  LR: 5.934e-05  Data: 0.011 (0.013)
Train: 257 [ 950/1251 ( 76%)]  Loss: 3.588 (3.31)  Time: 0.818s, 1251.86/s  (0.791s, 1294.58/s)  LR: 5.934e-05  Data: 0.011 (0.013)
Train: 257 [1000/1251 ( 80%)]  Loss: 3.308 (3.31)  Time: 0.793s, 1291.04/s  (0.791s, 1294.70/s)  LR: 5.934e-05  Data: 0.011 (0.013)
Train: 257 [1050/1251 ( 84%)]  Loss: 3.042 (3.30)  Time: 0.781s, 1310.88/s  (0.791s, 1294.96/s)  LR: 5.934e-05  Data: 0.011 (0.013)
Train: 257 [1100/1251 ( 88%)]  Loss: 3.254 (3.30)  Time: 0.785s, 1304.54/s  (0.791s, 1294.60/s)  LR: 5.934e-05  Data: 0.011 (0.013)
Train: 257 [1150/1251 ( 92%)]  Loss: 2.950 (3.29)  Time: 0.794s, 1289.65/s  (0.791s, 1294.95/s)  LR: 5.934e-05  Data: 0.011 (0.012)
Train: 257 [1200/1251 ( 96%)]  Loss: 3.449 (3.29)  Time: 0.778s, 1316.00/s  (0.790s, 1295.53/s)  LR: 5.934e-05  Data: 0.011 (0.012)
Train: 257 [1250/1251 (100%)]  Loss: 3.635 (3.30)  Time: 0.797s, 1284.39/s  (0.790s, 1295.85/s)  LR: 5.934e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.526 (1.526)  Loss:  0.7592 (0.7592)  Acc@1: 91.2109 (91.2109)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.172 (0.561)  Loss:  0.8639 (1.2809)  Acc@1: 86.6745 (77.8100)  Acc@5: 98.1132 (93.8220)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-257.pth.tar', 77.80999989990234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-255.pth.tar', 77.73199997558594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-256.pth.tar', 77.65000010986329)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-252.pth.tar', 77.54399995117187)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-254.pth.tar', 77.53599997558594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-253.pth.tar', 77.46599997558594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-247.pth.tar', 77.37199992919922)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-250.pth.tar', 77.36999997314453)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-248.pth.tar', 77.3439999243164)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-246.pth.tar', 77.32400003417969)

Train: 258 [   0/1251 (  0%)]  Loss: 3.436 (3.44)  Time: 2.367s,  432.57/s  (2.367s,  432.57/s)  LR: 5.711e-05  Data: 1.568 (1.568)
Train: 258 [  50/1251 (  4%)]  Loss: 3.557 (3.50)  Time: 0.806s, 1269.82/s  (0.828s, 1236.76/s)  LR: 5.711e-05  Data: 0.012 (0.043)
Train: 258 [ 100/1251 (  8%)]  Loss: 3.311 (3.43)  Time: 0.802s, 1276.20/s  (0.820s, 1249.41/s)  LR: 5.711e-05  Data: 0.012 (0.028)
Train: 258 [ 150/1251 ( 12%)]  Loss: 3.031 (3.33)  Time: 0.777s, 1317.50/s  (0.814s, 1257.75/s)  LR: 5.711e-05  Data: 0.011 (0.022)
Train: 258 [ 200/1251 ( 16%)]  Loss: 3.352 (3.34)  Time: 0.792s, 1292.41/s  (0.811s, 1263.29/s)  LR: 5.711e-05  Data: 0.010 (0.019)
Train: 258 [ 250/1251 ( 20%)]  Loss: 3.123 (3.30)  Time: 0.780s, 1312.54/s  (0.807s, 1269.47/s)  LR: 5.711e-05  Data: 0.011 (0.018)
Train: 258 [ 300/1251 ( 24%)]  Loss: 3.252 (3.29)  Time: 0.777s, 1317.51/s  (0.804s, 1273.85/s)  LR: 5.711e-05  Data: 0.011 (0.017)
Train: 258 [ 350/1251 ( 28%)]  Loss: 3.282 (3.29)  Time: 0.820s, 1248.07/s  (0.803s, 1275.08/s)  LR: 5.711e-05  Data: 0.011 (0.016)
Train: 258 [ 400/1251 ( 32%)]  Loss: 3.696 (3.34)  Time: 0.779s, 1315.12/s  (0.800s, 1279.44/s)  LR: 5.711e-05  Data: 0.011 (0.015)
Train: 258 [ 450/1251 ( 36%)]  Loss: 2.919 (3.30)  Time: 0.821s, 1247.83/s  (0.800s, 1279.35/s)  LR: 5.711e-05  Data: 0.011 (0.015)
Train: 258 [ 500/1251 ( 40%)]  Loss: 3.513 (3.32)  Time: 0.777s, 1318.73/s  (0.800s, 1279.36/s)  LR: 5.711e-05  Data: 0.011 (0.014)
Train: 258 [ 550/1251 ( 44%)]  Loss: 2.902 (3.28)  Time: 0.778s, 1316.82/s  (0.800s, 1279.56/s)  LR: 5.711e-05  Data: 0.011 (0.014)
Train: 258 [ 600/1251 ( 48%)]  Loss: 3.639 (3.31)  Time: 0.778s, 1316.67/s  (0.799s, 1281.09/s)  LR: 5.711e-05  Data: 0.011 (0.014)
Train: 258 [ 650/1251 ( 52%)]  Loss: 3.248 (3.30)  Time: 0.775s, 1320.67/s  (0.798s, 1282.84/s)  LR: 5.711e-05  Data: 0.010 (0.014)
Train: 258 [ 700/1251 ( 56%)]  Loss: 3.236 (3.30)  Time: 0.824s, 1242.58/s  (0.799s, 1282.38/s)  LR: 5.711e-05  Data: 0.011 (0.014)
Train: 258 [ 750/1251 ( 60%)]  Loss: 3.591 (3.32)  Time: 0.777s, 1317.21/s  (0.800s, 1280.25/s)  LR: 5.711e-05  Data: 0.011 (0.013)
Train: 258 [ 800/1251 ( 64%)]  Loss: 3.491 (3.33)  Time: 0.785s, 1303.83/s  (0.799s, 1281.98/s)  LR: 5.711e-05  Data: 0.011 (0.013)
Train: 258 [ 850/1251 ( 68%)]  Loss: 3.547 (3.34)  Time: 0.778s, 1315.48/s  (0.798s, 1283.59/s)  LR: 5.711e-05  Data: 0.011 (0.013)
Train: 258 [ 900/1251 ( 72%)]  Loss: 2.968 (3.32)  Time: 0.783s, 1308.30/s  (0.797s, 1284.82/s)  LR: 5.711e-05  Data: 0.012 (0.013)
Train: 258 [ 950/1251 ( 76%)]  Loss: 3.333 (3.32)  Time: 0.781s, 1311.56/s  (0.796s, 1286.08/s)  LR: 5.711e-05  Data: 0.010 (0.013)
Train: 258 [1000/1251 ( 80%)]  Loss: 3.573 (3.33)  Time: 0.783s, 1308.39/s  (0.796s, 1287.12/s)  LR: 5.711e-05  Data: 0.011 (0.013)
Train: 258 [1050/1251 ( 84%)]  Loss: 3.125 (3.32)  Time: 0.780s, 1311.98/s  (0.796s, 1287.11/s)  LR: 5.711e-05  Data: 0.012 (0.013)
Train: 258 [1100/1251 ( 88%)]  Loss: 3.508 (3.33)  Time: 0.787s, 1301.26/s  (0.795s, 1287.60/s)  LR: 5.711e-05  Data: 0.011 (0.013)
Train: 258 [1150/1251 ( 92%)]  Loss: 3.319 (3.33)  Time: 0.820s, 1249.47/s  (0.795s, 1287.84/s)  LR: 5.711e-05  Data: 0.011 (0.013)
Train: 258 [1200/1251 ( 96%)]  Loss: 3.399 (3.33)  Time: 0.782s, 1309.77/s  (0.795s, 1288.03/s)  LR: 5.711e-05  Data: 0.011 (0.013)
Train: 258 [1250/1251 (100%)]  Loss: 3.215 (3.33)  Time: 0.769s, 1332.10/s  (0.795s, 1287.83/s)  LR: 5.711e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.522 (1.522)  Loss:  0.7737 (0.7737)  Acc@1: 90.9180 (90.9180)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.172 (0.559)  Loss:  0.8816 (1.3045)  Acc@1: 85.8491 (77.6520)  Acc@5: 97.4057 (93.7840)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-257.pth.tar', 77.80999989990234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-255.pth.tar', 77.73199997558594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-258.pth.tar', 77.65200005859376)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-256.pth.tar', 77.65000010986329)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-252.pth.tar', 77.54399995117187)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-254.pth.tar', 77.53599997558594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-253.pth.tar', 77.46599997558594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-247.pth.tar', 77.37199992919922)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-250.pth.tar', 77.36999997314453)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-248.pth.tar', 77.3439999243164)

Train: 259 [   0/1251 (  0%)]  Loss: 3.261 (3.26)  Time: 2.341s,  437.39/s  (2.341s,  437.39/s)  LR: 5.493e-05  Data: 1.607 (1.607)
Train: 259 [  50/1251 (  4%)]  Loss: 3.388 (3.32)  Time: 0.779s, 1314.69/s  (0.833s, 1229.31/s)  LR: 5.493e-05  Data: 0.011 (0.043)
Train: 259 [ 100/1251 (  8%)]  Loss: 3.408 (3.35)  Time: 0.778s, 1315.78/s  (0.814s, 1257.99/s)  LR: 5.493e-05  Data: 0.012 (0.027)
Train: 259 [ 150/1251 ( 12%)]  Loss: 3.469 (3.38)  Time: 0.778s, 1316.79/s  (0.805s, 1271.87/s)  LR: 5.493e-05  Data: 0.011 (0.022)
Train: 259 [ 200/1251 ( 16%)]  Loss: 3.643 (3.43)  Time: 0.834s, 1227.36/s  (0.801s, 1278.41/s)  LR: 5.493e-05  Data: 0.011 (0.019)
Train: 259 [ 250/1251 ( 20%)]  Loss: 3.511 (3.45)  Time: 0.810s, 1263.80/s  (0.800s, 1280.10/s)  LR: 5.493e-05  Data: 0.011 (0.018)
Train: 259 [ 300/1251 ( 24%)]  Loss: 3.219 (3.41)  Time: 0.781s, 1311.85/s  (0.799s, 1281.15/s)  LR: 5.493e-05  Data: 0.011 (0.016)
Train: 259 [ 350/1251 ( 28%)]  Loss: 3.564 (3.43)  Time: 0.787s, 1301.69/s  (0.799s, 1282.24/s)  LR: 5.493e-05  Data: 0.010 (0.016)
Train: 259 [ 400/1251 ( 32%)]  Loss: 3.300 (3.42)  Time: 0.814s, 1258.65/s  (0.799s, 1281.61/s)  LR: 5.493e-05  Data: 0.011 (0.015)
Train: 259 [ 450/1251 ( 36%)]  Loss: 3.413 (3.42)  Time: 0.814s, 1258.45/s  (0.800s, 1279.39/s)  LR: 5.493e-05  Data: 0.011 (0.015)
Train: 259 [ 500/1251 ( 40%)]  Loss: 3.228 (3.40)  Time: 0.781s, 1311.11/s  (0.801s, 1277.82/s)  LR: 5.493e-05  Data: 0.011 (0.014)
Train: 259 [ 550/1251 ( 44%)]  Loss: 3.221 (3.39)  Time: 0.818s, 1251.13/s  (0.802s, 1277.05/s)  LR: 5.493e-05  Data: 0.012 (0.014)
Train: 259 [ 600/1251 ( 48%)]  Loss: 2.948 (3.35)  Time: 0.780s, 1312.49/s  (0.803s, 1275.91/s)  LR: 5.493e-05  Data: 0.011 (0.014)
Train: 259 [ 650/1251 ( 52%)]  Loss: 3.693 (3.38)  Time: 0.779s, 1314.61/s  (0.802s, 1276.24/s)  LR: 5.493e-05  Data: 0.012 (0.014)
Train: 259 [ 700/1251 ( 56%)]  Loss: 3.623 (3.39)  Time: 0.819s, 1251.01/s  (0.802s, 1277.49/s)  LR: 5.493e-05  Data: 0.011 (0.013)
Train: 259 [ 750/1251 ( 60%)]  Loss: 3.546 (3.40)  Time: 0.787s, 1300.82/s  (0.801s, 1277.92/s)  LR: 5.493e-05  Data: 0.011 (0.013)
Train: 259 [ 800/1251 ( 64%)]  Loss: 3.421 (3.40)  Time: 0.848s, 1207.71/s  (0.801s, 1277.61/s)  LR: 5.493e-05  Data: 0.011 (0.013)
Train: 259 [ 850/1251 ( 68%)]  Loss: 3.295 (3.40)  Time: 0.807s, 1269.10/s  (0.801s, 1278.35/s)  LR: 5.493e-05  Data: 0.011 (0.013)
Train: 259 [ 900/1251 ( 72%)]  Loss: 3.560 (3.41)  Time: 0.778s, 1316.60/s  (0.801s, 1278.29/s)  LR: 5.493e-05  Data: 0.011 (0.013)
Train: 259 [ 950/1251 ( 76%)]  Loss: 3.438 (3.41)  Time: 0.776s, 1319.86/s  (0.800s, 1279.76/s)  LR: 5.493e-05  Data: 0.012 (0.013)
Train: 259 [1000/1251 ( 80%)]  Loss: 3.377 (3.41)  Time: 0.779s, 1314.69/s  (0.800s, 1280.31/s)  LR: 5.493e-05  Data: 0.010 (0.013)
Train: 259 [1050/1251 ( 84%)]  Loss: 3.474 (3.41)  Time: 0.780s, 1313.02/s  (0.799s, 1281.27/s)  LR: 5.493e-05  Data: 0.011 (0.013)
Train: 259 [1100/1251 ( 88%)]  Loss: 3.249 (3.40)  Time: 0.835s, 1225.77/s  (0.799s, 1281.76/s)  LR: 5.493e-05  Data: 0.011 (0.013)
Train: 259 [1150/1251 ( 92%)]  Loss: 3.101 (3.39)  Time: 0.780s, 1312.05/s  (0.799s, 1281.25/s)  LR: 5.493e-05  Data: 0.011 (0.012)
Train: 259 [1200/1251 ( 96%)]  Loss: 3.125 (3.38)  Time: 0.820s, 1248.94/s  (0.799s, 1281.29/s)  LR: 5.493e-05  Data: 0.011 (0.012)
Train: 259 [1250/1251 (100%)]  Loss: 3.477 (3.38)  Time: 0.804s, 1274.17/s  (0.800s, 1280.52/s)  LR: 5.493e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.495 (1.495)  Loss:  0.7049 (0.7049)  Acc@1: 91.4062 (91.4062)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.172 (0.560)  Loss:  0.7896 (1.2255)  Acc@1: 87.2641 (77.7000)  Acc@5: 97.9953 (93.8680)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-257.pth.tar', 77.80999989990234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-255.pth.tar', 77.73199997558594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-259.pth.tar', 77.69999989746094)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-258.pth.tar', 77.65200005859376)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-256.pth.tar', 77.65000010986329)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-252.pth.tar', 77.54399995117187)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-254.pth.tar', 77.53599997558594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-253.pth.tar', 77.46599997558594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-247.pth.tar', 77.37199992919922)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-250.pth.tar', 77.36999997314453)

Train: 260 [   0/1251 (  0%)]  Loss: 2.922 (2.92)  Time: 2.205s,  464.46/s  (2.205s,  464.46/s)  LR: 5.279e-05  Data: 1.470 (1.470)
Train: 260 [  50/1251 (  4%)]  Loss: 3.665 (3.29)  Time: 0.837s, 1222.90/s  (0.826s, 1239.59/s)  LR: 5.279e-05  Data: 0.011 (0.043)
Train: 260 [ 100/1251 (  8%)]  Loss: 3.058 (3.21)  Time: 0.779s, 1314.06/s  (0.805s, 1271.55/s)  LR: 5.279e-05  Data: 0.011 (0.027)
Train: 260 [ 150/1251 ( 12%)]  Loss: 3.156 (3.20)  Time: 0.778s, 1316.05/s  (0.802s, 1277.18/s)  LR: 5.279e-05  Data: 0.012 (0.022)
Train: 260 [ 200/1251 ( 16%)]  Loss: 3.157 (3.19)  Time: 0.816s, 1254.60/s  (0.804s, 1273.78/s)  LR: 5.279e-05  Data: 0.011 (0.019)
Train: 260 [ 250/1251 ( 20%)]  Loss: 3.283 (3.21)  Time: 0.776s, 1319.24/s  (0.806s, 1270.54/s)  LR: 5.279e-05  Data: 0.011 (0.018)
Train: 260 [ 300/1251 ( 24%)]  Loss: 3.262 (3.21)  Time: 0.783s, 1307.74/s  (0.802s, 1277.29/s)  LR: 5.279e-05  Data: 0.011 (0.016)
Train: 260 [ 350/1251 ( 28%)]  Loss: 3.513 (3.25)  Time: 0.866s, 1182.16/s  (0.801s, 1278.03/s)  LR: 5.279e-05  Data: 0.011 (0.016)
Train: 260 [ 400/1251 ( 32%)]  Loss: 3.530 (3.28)  Time: 0.777s, 1317.73/s  (0.801s, 1278.80/s)  LR: 5.279e-05  Data: 0.011 (0.015)
Train: 260 [ 450/1251 ( 36%)]  Loss: 3.298 (3.28)  Time: 0.778s, 1316.89/s  (0.799s, 1281.42/s)  LR: 5.279e-05  Data: 0.011 (0.015)
Train: 260 [ 500/1251 ( 40%)]  Loss: 3.405 (3.30)  Time: 0.818s, 1252.25/s  (0.798s, 1283.59/s)  LR: 5.279e-05  Data: 0.011 (0.014)
Train: 260 [ 550/1251 ( 44%)]  Loss: 3.213 (3.29)  Time: 0.821s, 1247.95/s  (0.797s, 1285.08/s)  LR: 5.279e-05  Data: 0.011 (0.014)
Train: 260 [ 600/1251 ( 48%)]  Loss: 3.309 (3.29)  Time: 0.780s, 1313.54/s  (0.796s, 1286.59/s)  LR: 5.279e-05  Data: 0.011 (0.014)
Train: 260 [ 650/1251 ( 52%)]  Loss: 3.529 (3.31)  Time: 0.820s, 1249.01/s  (0.796s, 1286.77/s)  LR: 5.279e-05  Data: 0.012 (0.014)
Train: 260 [ 700/1251 ( 56%)]  Loss: 3.446 (3.32)  Time: 0.795s, 1287.74/s  (0.795s, 1287.68/s)  LR: 5.279e-05  Data: 0.011 (0.013)
Train: 260 [ 750/1251 ( 60%)]  Loss: 3.545 (3.33)  Time: 0.814s, 1257.87/s  (0.795s, 1288.29/s)  LR: 5.279e-05  Data: 0.011 (0.013)
Train: 260 [ 800/1251 ( 64%)]  Loss: 3.418 (3.34)  Time: 0.788s, 1298.89/s  (0.794s, 1289.65/s)  LR: 5.279e-05  Data: 0.010 (0.013)
Train: 260 [ 850/1251 ( 68%)]  Loss: 3.319 (3.33)  Time: 0.779s, 1314.36/s  (0.794s, 1289.96/s)  LR: 5.279e-05  Data: 0.011 (0.013)
Train: 260 [ 900/1251 ( 72%)]  Loss: 3.147 (3.32)  Time: 0.821s, 1247.85/s  (0.794s, 1290.23/s)  LR: 5.279e-05  Data: 0.011 (0.013)
Train: 260 [ 950/1251 ( 76%)]  Loss: 3.362 (3.33)  Time: 0.780s, 1313.43/s  (0.793s, 1290.81/s)  LR: 5.279e-05  Data: 0.011 (0.013)
Train: 260 [1000/1251 ( 80%)]  Loss: 3.180 (3.32)  Time: 0.782s, 1308.96/s  (0.793s, 1291.00/s)  LR: 5.279e-05  Data: 0.011 (0.013)
Train: 260 [1050/1251 ( 84%)]  Loss: 3.387 (3.32)  Time: 0.779s, 1315.18/s  (0.793s, 1290.90/s)  LR: 5.279e-05  Data: 0.011 (0.013)
Train: 260 [1100/1251 ( 88%)]  Loss: 3.169 (3.32)  Time: 0.779s, 1314.86/s  (0.793s, 1290.50/s)  LR: 5.279e-05  Data: 0.011 (0.012)
Train: 260 [1150/1251 ( 92%)]  Loss: 3.551 (3.33)  Time: 0.781s, 1311.63/s  (0.793s, 1291.21/s)  LR: 5.279e-05  Data: 0.011 (0.012)
Train: 260 [1200/1251 ( 96%)]  Loss: 3.398 (3.33)  Time: 0.786s, 1302.92/s  (0.793s, 1291.31/s)  LR: 5.279e-05  Data: 0.011 (0.012)
Train: 260 [1250/1251 (100%)]  Loss: 3.272 (3.33)  Time: 0.767s, 1334.52/s  (0.793s, 1291.97/s)  LR: 5.279e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.513 (1.513)  Loss:  0.7816 (0.7816)  Acc@1: 91.6016 (91.6016)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.172 (0.563)  Loss:  0.8314 (1.2681)  Acc@1: 86.0849 (77.7060)  Acc@5: 97.7594 (93.8580)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-257.pth.tar', 77.80999989990234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-255.pth.tar', 77.73199997558594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-260.pth.tar', 77.70600003173828)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-259.pth.tar', 77.69999989746094)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-258.pth.tar', 77.65200005859376)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-256.pth.tar', 77.65000010986329)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-252.pth.tar', 77.54399995117187)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-254.pth.tar', 77.53599997558594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-253.pth.tar', 77.46599997558594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-247.pth.tar', 77.37199992919922)

Train: 261 [   0/1251 (  0%)]  Loss: 2.907 (2.91)  Time: 2.264s,  452.34/s  (2.264s,  452.34/s)  LR: 5.071e-05  Data: 1.522 (1.522)
Train: 261 [  50/1251 (  4%)]  Loss: 3.299 (3.10)  Time: 0.782s, 1309.68/s  (0.826s, 1239.70/s)  LR: 5.071e-05  Data: 0.011 (0.048)
Train: 261 [ 100/1251 (  8%)]  Loss: 3.539 (3.25)  Time: 0.779s, 1314.24/s  (0.804s, 1273.21/s)  LR: 5.071e-05  Data: 0.010 (0.030)
Train: 261 [ 150/1251 ( 12%)]  Loss: 2.922 (3.17)  Time: 0.817s, 1253.89/s  (0.801s, 1279.16/s)  LR: 5.071e-05  Data: 0.011 (0.023)
Train: 261 [ 200/1251 ( 16%)]  Loss: 3.339 (3.20)  Time: 0.779s, 1314.47/s  (0.797s, 1284.78/s)  LR: 5.071e-05  Data: 0.011 (0.020)
Train: 261 [ 250/1251 ( 20%)]  Loss: 3.575 (3.26)  Time: 0.777s, 1317.14/s  (0.796s, 1286.12/s)  LR: 5.071e-05  Data: 0.011 (0.019)
Train: 261 [ 300/1251 ( 24%)]  Loss: 3.081 (3.24)  Time: 0.782s, 1309.17/s  (0.798s, 1283.49/s)  LR: 5.071e-05  Data: 0.011 (0.017)
Train: 261 [ 350/1251 ( 28%)]  Loss: 3.067 (3.22)  Time: 0.777s, 1318.28/s  (0.796s, 1287.07/s)  LR: 5.071e-05  Data: 0.011 (0.016)
Train: 261 [ 400/1251 ( 32%)]  Loss: 3.020 (3.19)  Time: 0.779s, 1313.94/s  (0.794s, 1289.61/s)  LR: 5.071e-05  Data: 0.011 (0.016)
Train: 261 [ 450/1251 ( 36%)]  Loss: 3.266 (3.20)  Time: 0.781s, 1310.96/s  (0.793s, 1291.18/s)  LR: 5.071e-05  Data: 0.011 (0.015)
Train: 261 [ 500/1251 ( 40%)]  Loss: 3.546 (3.23)  Time: 0.780s, 1313.64/s  (0.792s, 1293.24/s)  LR: 5.071e-05  Data: 0.011 (0.015)
Train: 261 [ 550/1251 ( 44%)]  Loss: 3.152 (3.23)  Time: 0.815s, 1256.14/s  (0.793s, 1291.21/s)  LR: 5.071e-05  Data: 0.011 (0.014)
Train: 261 [ 600/1251 ( 48%)]  Loss: 3.073 (3.21)  Time: 0.818s, 1252.10/s  (0.794s, 1289.33/s)  LR: 5.071e-05  Data: 0.011 (0.014)
Train: 261 [ 650/1251 ( 52%)]  Loss: 3.252 (3.22)  Time: 0.815s, 1256.31/s  (0.796s, 1287.15/s)  LR: 5.071e-05  Data: 0.011 (0.014)
Train: 261 [ 700/1251 ( 56%)]  Loss: 3.576 (3.24)  Time: 0.780s, 1312.71/s  (0.795s, 1287.33/s)  LR: 5.071e-05  Data: 0.011 (0.014)
Train: 261 [ 750/1251 ( 60%)]  Loss: 3.334 (3.25)  Time: 0.779s, 1314.73/s  (0.795s, 1288.57/s)  LR: 5.071e-05  Data: 0.011 (0.014)
Train: 261 [ 800/1251 ( 64%)]  Loss: 3.118 (3.24)  Time: 0.777s, 1317.29/s  (0.794s, 1288.87/s)  LR: 5.071e-05  Data: 0.011 (0.013)
Train: 261 [ 850/1251 ( 68%)]  Loss: 3.417 (3.25)  Time: 0.779s, 1314.37/s  (0.794s, 1289.43/s)  LR: 5.071e-05  Data: 0.011 (0.013)
Train: 261 [ 900/1251 ( 72%)]  Loss: 3.276 (3.25)  Time: 0.817s, 1253.71/s  (0.795s, 1288.50/s)  LR: 5.071e-05  Data: 0.011 (0.013)
Train: 261 [ 950/1251 ( 76%)]  Loss: 3.131 (3.24)  Time: 0.829s, 1234.49/s  (0.794s, 1289.12/s)  LR: 5.071e-05  Data: 0.011 (0.013)
Train: 261 [1000/1251 ( 80%)]  Loss: 3.455 (3.25)  Time: 0.783s, 1308.32/s  (0.794s, 1289.89/s)  LR: 5.071e-05  Data: 0.011 (0.013)
Train: 261 [1050/1251 ( 84%)]  Loss: 3.161 (3.25)  Time: 0.822s, 1245.59/s  (0.793s, 1290.62/s)  LR: 5.071e-05  Data: 0.011 (0.013)
Train: 261 [1100/1251 ( 88%)]  Loss: 3.034 (3.24)  Time: 0.780s, 1312.25/s  (0.793s, 1290.96/s)  LR: 5.071e-05  Data: 0.011 (0.013)
Train: 261 [1150/1251 ( 92%)]  Loss: 3.336 (3.24)  Time: 0.780s, 1312.90/s  (0.793s, 1291.63/s)  LR: 5.071e-05  Data: 0.011 (0.013)
Train: 261 [1200/1251 ( 96%)]  Loss: 3.307 (3.25)  Time: 0.776s, 1318.99/s  (0.792s, 1292.41/s)  LR: 5.071e-05  Data: 0.011 (0.013)
Train: 261 [1250/1251 (100%)]  Loss: 3.037 (3.24)  Time: 0.809s, 1265.91/s  (0.792s, 1292.73/s)  LR: 5.071e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.491 (1.491)  Loss:  0.6673 (0.6673)  Acc@1: 91.4062 (91.4062)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.172 (0.565)  Loss:  0.7522 (1.1988)  Acc@1: 87.1462 (77.8840)  Acc@5: 98.2311 (93.8940)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-261.pth.tar', 77.88399997558594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-257.pth.tar', 77.80999989990234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-255.pth.tar', 77.73199997558594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-260.pth.tar', 77.70600003173828)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-259.pth.tar', 77.69999989746094)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-258.pth.tar', 77.65200005859376)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-256.pth.tar', 77.65000010986329)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-252.pth.tar', 77.54399995117187)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-254.pth.tar', 77.53599997558594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-253.pth.tar', 77.46599997558594)

Train: 262 [   0/1251 (  0%)]  Loss: 3.412 (3.41)  Time: 2.280s,  449.16/s  (2.280s,  449.16/s)  LR: 4.868e-05  Data: 1.546 (1.546)
Train: 262 [  50/1251 (  4%)]  Loss: 3.308 (3.36)  Time: 0.777s, 1318.09/s  (0.822s, 1245.14/s)  LR: 4.868e-05  Data: 0.010 (0.044)
Train: 262 [ 100/1251 (  8%)]  Loss: 3.297 (3.34)  Time: 0.779s, 1314.36/s  (0.808s, 1268.04/s)  LR: 4.868e-05  Data: 0.011 (0.027)
Train: 262 [ 150/1251 ( 12%)]  Loss: 3.036 (3.26)  Time: 0.777s, 1318.74/s  (0.800s, 1279.96/s)  LR: 4.868e-05  Data: 0.010 (0.022)
Train: 262 [ 200/1251 ( 16%)]  Loss: 3.590 (3.33)  Time: 0.787s, 1301.61/s  (0.797s, 1284.70/s)  LR: 4.868e-05  Data: 0.011 (0.019)
Train: 262 [ 250/1251 ( 20%)]  Loss: 3.238 (3.31)  Time: 0.821s, 1246.94/s  (0.795s, 1287.93/s)  LR: 4.868e-05  Data: 0.011 (0.018)
Train: 262 [ 300/1251 ( 24%)]  Loss: 3.301 (3.31)  Time: 0.778s, 1316.92/s  (0.796s, 1285.93/s)  LR: 4.868e-05  Data: 0.011 (0.016)
Train: 262 [ 350/1251 ( 28%)]  Loss: 2.959 (3.27)  Time: 0.779s, 1314.92/s  (0.794s, 1288.97/s)  LR: 4.868e-05  Data: 0.011 (0.016)
Train: 262 [ 400/1251 ( 32%)]  Loss: 3.120 (3.25)  Time: 0.779s, 1314.55/s  (0.793s, 1290.65/s)  LR: 4.868e-05  Data: 0.010 (0.015)
Train: 262 [ 450/1251 ( 36%)]  Loss: 3.328 (3.26)  Time: 0.779s, 1315.06/s  (0.793s, 1291.61/s)  LR: 4.868e-05  Data: 0.011 (0.015)
Train: 262 [ 500/1251 ( 40%)]  Loss: 3.242 (3.26)  Time: 0.822s, 1245.64/s  (0.793s, 1291.38/s)  LR: 4.868e-05  Data: 0.011 (0.014)
Train: 262 [ 550/1251 ( 44%)]  Loss: 3.068 (3.24)  Time: 0.818s, 1251.40/s  (0.795s, 1288.51/s)  LR: 4.868e-05  Data: 0.012 (0.014)
Train: 262 [ 600/1251 ( 48%)]  Loss: 3.667 (3.27)  Time: 0.778s, 1316.53/s  (0.795s, 1288.58/s)  LR: 4.868e-05  Data: 0.010 (0.014)
Train: 262 [ 650/1251 ( 52%)]  Loss: 3.278 (3.27)  Time: 0.779s, 1314.04/s  (0.795s, 1288.21/s)  LR: 4.868e-05  Data: 0.011 (0.014)
Train: 262 [ 700/1251 ( 56%)]  Loss: 3.223 (3.27)  Time: 0.779s, 1314.17/s  (0.794s, 1289.00/s)  LR: 4.868e-05  Data: 0.010 (0.013)
Train: 262 [ 750/1251 ( 60%)]  Loss: 3.271 (3.27)  Time: 0.776s, 1319.26/s  (0.794s, 1288.90/s)  LR: 4.868e-05  Data: 0.011 (0.013)
Train: 262 [ 800/1251 ( 64%)]  Loss: 3.520 (3.29)  Time: 0.776s, 1318.94/s  (0.794s, 1289.96/s)  LR: 4.868e-05  Data: 0.011 (0.013)
Train: 262 [ 850/1251 ( 68%)]  Loss: 3.372 (3.29)  Time: 0.779s, 1314.82/s  (0.793s, 1290.71/s)  LR: 4.868e-05  Data: 0.011 (0.013)
Train: 262 [ 900/1251 ( 72%)]  Loss: 3.157 (3.28)  Time: 0.797s, 1284.72/s  (0.793s, 1291.38/s)  LR: 4.868e-05  Data: 0.010 (0.013)
Train: 262 [ 950/1251 ( 76%)]  Loss: 3.307 (3.28)  Time: 0.780s, 1313.58/s  (0.793s, 1291.25/s)  LR: 4.868e-05  Data: 0.011 (0.013)
Train: 262 [1000/1251 ( 80%)]  Loss: 3.175 (3.28)  Time: 0.783s, 1307.12/s  (0.794s, 1290.30/s)  LR: 4.868e-05  Data: 0.011 (0.013)
Train: 262 [1050/1251 ( 84%)]  Loss: 3.496 (3.29)  Time: 0.806s, 1269.70/s  (0.793s, 1290.49/s)  LR: 4.868e-05  Data: 0.011 (0.013)
Train: 262 [1100/1251 ( 88%)]  Loss: 3.019 (3.28)  Time: 0.777s, 1318.56/s  (0.793s, 1290.61/s)  LR: 4.868e-05  Data: 0.011 (0.012)
Train: 262 [1150/1251 ( 92%)]  Loss: 3.408 (3.28)  Time: 0.779s, 1314.39/s  (0.794s, 1290.32/s)  LR: 4.868e-05  Data: 0.011 (0.012)
Train: 262 [1200/1251 ( 96%)]  Loss: 3.386 (3.29)  Time: 0.781s, 1311.11/s  (0.794s, 1289.63/s)  LR: 4.868e-05  Data: 0.011 (0.012)
Train: 262 [1250/1251 (100%)]  Loss: 3.390 (3.29)  Time: 0.771s, 1327.46/s  (0.794s, 1290.42/s)  LR: 4.868e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.523 (1.523)  Loss:  0.7414 (0.7414)  Acc@1: 91.6992 (91.6992)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.172 (0.589)  Loss:  0.8220 (1.2787)  Acc@1: 86.9104 (77.8920)  Acc@5: 98.2311 (93.9000)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-262.pth.tar', 77.89199987304687)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-261.pth.tar', 77.88399997558594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-257.pth.tar', 77.80999989990234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-255.pth.tar', 77.73199997558594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-260.pth.tar', 77.70600003173828)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-259.pth.tar', 77.69999989746094)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-258.pth.tar', 77.65200005859376)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-256.pth.tar', 77.65000010986329)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-252.pth.tar', 77.54399995117187)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-254.pth.tar', 77.53599997558594)

Train: 263 [   0/1251 (  0%)]  Loss: 3.476 (3.48)  Time: 2.249s,  455.41/s  (2.249s,  455.41/s)  LR: 4.669e-05  Data: 1.515 (1.515)
Train: 263 [  50/1251 (  4%)]  Loss: 3.340 (3.41)  Time: 0.781s, 1311.67/s  (0.822s, 1245.35/s)  LR: 4.669e-05  Data: 0.012 (0.046)
Train: 263 [ 100/1251 (  8%)]  Loss: 3.329 (3.38)  Time: 0.821s, 1246.90/s  (0.812s, 1260.76/s)  LR: 4.669e-05  Data: 0.011 (0.029)
Train: 263 [ 150/1251 ( 12%)]  Loss: 3.226 (3.34)  Time: 0.778s, 1316.28/s  (0.804s, 1273.00/s)  LR: 4.669e-05  Data: 0.012 (0.023)
Train: 263 [ 200/1251 ( 16%)]  Loss: 3.547 (3.38)  Time: 0.830s, 1233.04/s  (0.801s, 1278.10/s)  LR: 4.669e-05  Data: 0.012 (0.020)
Train: 263 [ 250/1251 ( 20%)]  Loss: 3.262 (3.36)  Time: 0.789s, 1297.67/s  (0.799s, 1282.06/s)  LR: 4.669e-05  Data: 0.011 (0.018)
Train: 263 [ 300/1251 ( 24%)]  Loss: 3.225 (3.34)  Time: 0.816s, 1255.55/s  (0.799s, 1282.12/s)  LR: 4.669e-05  Data: 0.012 (0.017)
Train: 263 [ 350/1251 ( 28%)]  Loss: 3.472 (3.36)  Time: 0.784s, 1306.53/s  (0.797s, 1284.52/s)  LR: 4.669e-05  Data: 0.013 (0.016)
Train: 263 [ 400/1251 ( 32%)]  Loss: 3.053 (3.33)  Time: 0.808s, 1267.18/s  (0.797s, 1285.49/s)  LR: 4.669e-05  Data: 0.010 (0.016)
Train: 263 [ 450/1251 ( 36%)]  Loss: 3.273 (3.32)  Time: 0.784s, 1306.83/s  (0.796s, 1286.30/s)  LR: 4.669e-05  Data: 0.011 (0.015)
Train: 263 [ 500/1251 ( 40%)]  Loss: 3.369 (3.32)  Time: 0.782s, 1308.92/s  (0.795s, 1287.65/s)  LR: 4.669e-05  Data: 0.011 (0.015)
Train: 263 [ 550/1251 ( 44%)]  Loss: 2.889 (3.29)  Time: 0.779s, 1313.98/s  (0.795s, 1288.09/s)  LR: 4.669e-05  Data: 0.011 (0.014)
Train: 263 [ 600/1251 ( 48%)]  Loss: 3.177 (3.28)  Time: 0.779s, 1315.26/s  (0.796s, 1286.99/s)  LR: 4.669e-05  Data: 0.011 (0.014)
Train: 263 [ 650/1251 ( 52%)]  Loss: 3.437 (3.29)  Time: 0.779s, 1314.87/s  (0.795s, 1288.16/s)  LR: 4.669e-05  Data: 0.011 (0.014)
Train: 263 [ 700/1251 ( 56%)]  Loss: 3.501 (3.31)  Time: 0.775s, 1320.55/s  (0.795s, 1288.48/s)  LR: 4.669e-05  Data: 0.011 (0.014)
Train: 263 [ 750/1251 ( 60%)]  Loss: 3.277 (3.30)  Time: 0.789s, 1297.82/s  (0.794s, 1289.46/s)  LR: 4.669e-05  Data: 0.011 (0.014)
Train: 263 [ 800/1251 ( 64%)]  Loss: 3.498 (3.31)  Time: 0.915s, 1119.37/s  (0.794s, 1289.78/s)  LR: 4.669e-05  Data: 0.012 (0.013)
Train: 263 [ 850/1251 ( 68%)]  Loss: 3.373 (3.32)  Time: 0.816s, 1254.84/s  (0.795s, 1287.83/s)  LR: 4.669e-05  Data: 0.011 (0.013)
Train: 263 [ 900/1251 ( 72%)]  Loss: 3.133 (3.31)  Time: 0.779s, 1313.78/s  (0.795s, 1287.26/s)  LR: 4.669e-05  Data: 0.011 (0.013)
Train: 263 [ 950/1251 ( 76%)]  Loss: 3.381 (3.31)  Time: 0.781s, 1311.93/s  (0.795s, 1287.47/s)  LR: 4.669e-05  Data: 0.011 (0.013)
Train: 263 [1000/1251 ( 80%)]  Loss: 3.428 (3.32)  Time: 0.811s, 1262.62/s  (0.795s, 1287.40/s)  LR: 4.669e-05  Data: 0.011 (0.013)
Train: 263 [1050/1251 ( 84%)]  Loss: 3.858 (3.34)  Time: 0.845s, 1211.92/s  (0.795s, 1287.68/s)  LR: 4.669e-05  Data: 0.011 (0.013)
Train: 263 [1100/1251 ( 88%)]  Loss: 3.394 (3.34)  Time: 0.777s, 1318.01/s  (0.795s, 1288.02/s)  LR: 4.669e-05  Data: 0.011 (0.013)
Train: 263 [1150/1251 ( 92%)]  Loss: 3.320 (3.34)  Time: 0.778s, 1316.16/s  (0.795s, 1288.37/s)  LR: 4.669e-05  Data: 0.011 (0.013)
Train: 263 [1200/1251 ( 96%)]  Loss: 3.340 (3.34)  Time: 0.779s, 1314.48/s  (0.794s, 1289.30/s)  LR: 4.669e-05  Data: 0.011 (0.013)
Train: 263 [1250/1251 (100%)]  Loss: 3.065 (3.33)  Time: 0.768s, 1333.28/s  (0.794s, 1289.08/s)  LR: 4.669e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.525 (1.525)  Loss:  0.6762 (0.6762)  Acc@1: 91.1133 (91.1133)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.172 (0.560)  Loss:  0.7289 (1.1693)  Acc@1: 86.4387 (77.8840)  Acc@5: 97.9953 (93.9100)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-262.pth.tar', 77.89199987304687)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-261.pth.tar', 77.88399997558594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-263.pth.tar', 77.88399992675781)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-257.pth.tar', 77.80999989990234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-255.pth.tar', 77.73199997558594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-260.pth.tar', 77.70600003173828)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-259.pth.tar', 77.69999989746094)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-258.pth.tar', 77.65200005859376)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-256.pth.tar', 77.65000010986329)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-252.pth.tar', 77.54399995117187)

Train: 264 [   0/1251 (  0%)]  Loss: 3.451 (3.45)  Time: 2.295s,  446.27/s  (2.295s,  446.27/s)  LR: 4.476e-05  Data: 1.560 (1.560)
Train: 264 [  50/1251 (  4%)]  Loss: 3.629 (3.54)  Time: 0.811s, 1261.97/s  (0.837s, 1223.90/s)  LR: 4.476e-05  Data: 0.011 (0.046)
Train: 264 [ 100/1251 (  8%)]  Loss: 3.316 (3.47)  Time: 0.794s, 1288.97/s  (0.818s, 1252.36/s)  LR: 4.476e-05  Data: 0.016 (0.029)
Train: 264 [ 150/1251 ( 12%)]  Loss: 3.412 (3.45)  Time: 0.781s, 1311.86/s  (0.810s, 1264.66/s)  LR: 4.476e-05  Data: 0.011 (0.023)
Train: 264 [ 200/1251 ( 16%)]  Loss: 3.473 (3.46)  Time: 0.780s, 1312.02/s  (0.805s, 1271.76/s)  LR: 4.476e-05  Data: 0.011 (0.020)
Train: 264 [ 250/1251 ( 20%)]  Loss: 3.387 (3.44)  Time: 0.813s, 1259.49/s  (0.804s, 1274.30/s)  LR: 4.476e-05  Data: 0.012 (0.018)
Train: 264 [ 300/1251 ( 24%)]  Loss: 3.467 (3.45)  Time: 0.778s, 1316.80/s  (0.804s, 1273.75/s)  LR: 4.476e-05  Data: 0.011 (0.017)
Train: 264 [ 350/1251 ( 28%)]  Loss: 3.160 (3.41)  Time: 0.779s, 1314.22/s  (0.801s, 1278.54/s)  LR: 4.476e-05  Data: 0.011 (0.016)
Train: 264 [ 400/1251 ( 32%)]  Loss: 3.074 (3.37)  Time: 0.815s, 1256.75/s  (0.801s, 1277.94/s)  LR: 4.476e-05  Data: 0.011 (0.016)
Train: 264 [ 450/1251 ( 36%)]  Loss: 3.387 (3.38)  Time: 0.779s, 1313.81/s  (0.801s, 1278.39/s)  LR: 4.476e-05  Data: 0.011 (0.015)
Train: 264 [ 500/1251 ( 40%)]  Loss: 3.403 (3.38)  Time: 0.780s, 1312.44/s  (0.800s, 1279.30/s)  LR: 4.476e-05  Data: 0.011 (0.015)
Train: 264 [ 550/1251 ( 44%)]  Loss: 3.435 (3.38)  Time: 0.779s, 1314.89/s  (0.800s, 1279.84/s)  LR: 4.476e-05  Data: 0.011 (0.014)
Train: 264 [ 600/1251 ( 48%)]  Loss: 3.343 (3.38)  Time: 0.778s, 1316.21/s  (0.799s, 1281.14/s)  LR: 4.476e-05  Data: 0.011 (0.014)
Train: 264 [ 650/1251 ( 52%)]  Loss: 3.498 (3.39)  Time: 0.778s, 1315.66/s  (0.799s, 1282.38/s)  LR: 4.476e-05  Data: 0.011 (0.014)
Train: 264 [ 700/1251 ( 56%)]  Loss: 3.299 (3.38)  Time: 0.780s, 1312.63/s  (0.798s, 1283.10/s)  LR: 4.476e-05  Data: 0.011 (0.014)
Train: 264 [ 750/1251 ( 60%)]  Loss: 3.254 (3.37)  Time: 0.776s, 1320.33/s  (0.797s, 1284.52/s)  LR: 4.476e-05  Data: 0.011 (0.014)
Train: 264 [ 800/1251 ( 64%)]  Loss: 3.159 (3.36)  Time: 0.779s, 1314.71/s  (0.797s, 1284.32/s)  LR: 4.476e-05  Data: 0.011 (0.013)
Train: 264 [ 850/1251 ( 68%)]  Loss: 3.371 (3.36)  Time: 0.780s, 1312.34/s  (0.796s, 1286.01/s)  LR: 4.476e-05  Data: 0.012 (0.013)
Train: 264 [ 900/1251 ( 72%)]  Loss: 3.182 (3.35)  Time: 0.779s, 1314.14/s  (0.796s, 1286.13/s)  LR: 4.476e-05  Data: 0.011 (0.013)
Train: 264 [ 950/1251 ( 76%)]  Loss: 3.365 (3.35)  Time: 0.777s, 1317.52/s  (0.796s, 1286.21/s)  LR: 4.476e-05  Data: 0.011 (0.013)
Train: 264 [1000/1251 ( 80%)]  Loss: 3.427 (3.36)  Time: 0.826s, 1239.81/s  (0.796s, 1287.05/s)  LR: 4.476e-05  Data: 0.010 (0.013)
Train: 264 [1050/1251 ( 84%)]  Loss: 3.537 (3.37)  Time: 0.837s, 1223.15/s  (0.796s, 1286.55/s)  LR: 4.476e-05  Data: 0.011 (0.013)
Train: 264 [1100/1251 ( 88%)]  Loss: 3.158 (3.36)  Time: 0.779s, 1314.54/s  (0.796s, 1286.38/s)  LR: 4.476e-05  Data: 0.011 (0.013)
Train: 264 [1150/1251 ( 92%)]  Loss: 3.474 (3.36)  Time: 0.801s, 1279.10/s  (0.796s, 1285.75/s)  LR: 4.476e-05  Data: 0.011 (0.013)
Train: 264 [1200/1251 ( 96%)]  Loss: 3.667 (3.37)  Time: 0.817s, 1253.53/s  (0.796s, 1286.75/s)  LR: 4.476e-05  Data: 0.012 (0.013)
Train: 264 [1250/1251 (100%)]  Loss: 3.304 (3.37)  Time: 0.775s, 1320.85/s  (0.796s, 1287.16/s)  LR: 4.476e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.498 (1.498)  Loss:  0.7586 (0.7586)  Acc@1: 91.4062 (91.4062)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.171 (0.554)  Loss:  0.8529 (1.2798)  Acc@1: 86.6745 (77.7380)  Acc@5: 97.7594 (93.8660)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-262.pth.tar', 77.89199987304687)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-261.pth.tar', 77.88399997558594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-263.pth.tar', 77.88399992675781)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-257.pth.tar', 77.80999989990234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-264.pth.tar', 77.73800002929687)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-255.pth.tar', 77.73199997558594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-260.pth.tar', 77.70600003173828)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-259.pth.tar', 77.69999989746094)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-258.pth.tar', 77.65200005859376)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-256.pth.tar', 77.65000010986329)

Train: 265 [   0/1251 (  0%)]  Loss: 3.560 (3.56)  Time: 2.354s,  434.95/s  (2.354s,  434.95/s)  LR: 4.288e-05  Data: 1.621 (1.621)
Train: 265 [  50/1251 (  4%)]  Loss: 3.792 (3.68)  Time: 0.819s, 1250.00/s  (0.837s, 1223.25/s)  LR: 4.288e-05  Data: 0.011 (0.046)
Train: 265 [ 100/1251 (  8%)]  Loss: 3.451 (3.60)  Time: 0.837s, 1223.85/s  (0.818s, 1251.80/s)  LR: 4.288e-05  Data: 0.011 (0.029)
Train: 265 [ 150/1251 ( 12%)]  Loss: 3.315 (3.53)  Time: 0.814s, 1257.85/s  (0.817s, 1252.66/s)  LR: 4.288e-05  Data: 0.012 (0.023)
Train: 265 [ 200/1251 ( 16%)]  Loss: 3.234 (3.47)  Time: 0.808s, 1266.82/s  (0.812s, 1260.91/s)  LR: 4.288e-05  Data: 0.011 (0.020)
Train: 265 [ 250/1251 ( 20%)]  Loss: 3.345 (3.45)  Time: 0.779s, 1314.57/s  (0.808s, 1267.19/s)  LR: 4.288e-05  Data: 0.011 (0.018)
Train: 265 [ 300/1251 ( 24%)]  Loss: 3.502 (3.46)  Time: 0.777s, 1318.14/s  (0.803s, 1274.43/s)  LR: 4.288e-05  Data: 0.011 (0.017)
Train: 265 [ 350/1251 ( 28%)]  Loss: 3.313 (3.44)  Time: 0.778s, 1315.45/s  (0.801s, 1279.11/s)  LR: 4.288e-05  Data: 0.010 (0.016)
Train: 265 [ 400/1251 ( 32%)]  Loss: 3.463 (3.44)  Time: 0.780s, 1313.32/s  (0.798s, 1282.94/s)  LR: 4.288e-05  Data: 0.011 (0.016)
Train: 265 [ 450/1251 ( 36%)]  Loss: 3.331 (3.43)  Time: 0.819s, 1250.33/s  (0.798s, 1283.98/s)  LR: 4.288e-05  Data: 0.010 (0.015)
Train: 265 [ 500/1251 ( 40%)]  Loss: 3.220 (3.41)  Time: 0.781s, 1310.82/s  (0.797s, 1285.23/s)  LR: 4.288e-05  Data: 0.010 (0.015)
Train: 265 [ 550/1251 ( 44%)]  Loss: 3.084 (3.38)  Time: 0.777s, 1317.90/s  (0.796s, 1287.11/s)  LR: 4.288e-05  Data: 0.011 (0.014)
Train: 265 [ 600/1251 ( 48%)]  Loss: 3.266 (3.38)  Time: 0.814s, 1258.35/s  (0.796s, 1285.73/s)  LR: 4.288e-05  Data: 0.010 (0.014)
Train: 265 [ 650/1251 ( 52%)]  Loss: 3.394 (3.38)  Time: 0.780s, 1313.43/s  (0.796s, 1286.00/s)  LR: 4.288e-05  Data: 0.011 (0.014)
Train: 265 [ 700/1251 ( 56%)]  Loss: 3.039 (3.35)  Time: 0.814s, 1257.35/s  (0.797s, 1285.15/s)  LR: 4.288e-05  Data: 0.011 (0.014)
Train: 265 [ 750/1251 ( 60%)]  Loss: 3.241 (3.35)  Time: 0.781s, 1311.77/s  (0.797s, 1284.99/s)  LR: 4.288e-05  Data: 0.011 (0.014)
Train: 265 [ 800/1251 ( 64%)]  Loss: 3.311 (3.34)  Time: 0.780s, 1312.65/s  (0.797s, 1285.35/s)  LR: 4.288e-05  Data: 0.011 (0.013)
Train: 265 [ 850/1251 ( 68%)]  Loss: 3.260 (3.34)  Time: 0.826s, 1239.34/s  (0.797s, 1284.59/s)  LR: 4.288e-05  Data: 0.011 (0.013)
Train: 265 [ 900/1251 ( 72%)]  Loss: 3.460 (3.35)  Time: 0.778s, 1315.36/s  (0.798s, 1283.53/s)  LR: 4.288e-05  Data: 0.011 (0.013)
Train: 265 [ 950/1251 ( 76%)]  Loss: 3.153 (3.34)  Time: 0.818s, 1252.54/s  (0.797s, 1284.04/s)  LR: 4.288e-05  Data: 0.010 (0.013)
Train: 265 [1000/1251 ( 80%)]  Loss: 3.295 (3.33)  Time: 0.776s, 1318.96/s  (0.797s, 1284.02/s)  LR: 4.288e-05  Data: 0.012 (0.013)
Train: 265 [1050/1251 ( 84%)]  Loss: 3.159 (3.33)  Time: 0.788s, 1299.09/s  (0.797s, 1285.29/s)  LR: 4.288e-05  Data: 0.012 (0.013)
Train: 265 [1100/1251 ( 88%)]  Loss: 3.121 (3.32)  Time: 0.777s, 1318.16/s  (0.796s, 1286.16/s)  LR: 4.288e-05  Data: 0.011 (0.013)
Train: 265 [1150/1251 ( 92%)]  Loss: 3.433 (3.32)  Time: 0.795s, 1288.60/s  (0.796s, 1286.57/s)  LR: 4.288e-05  Data: 0.011 (0.013)
Train: 265 [1200/1251 ( 96%)]  Loss: 3.716 (3.34)  Time: 0.783s, 1307.87/s  (0.796s, 1286.11/s)  LR: 4.288e-05  Data: 0.011 (0.013)
Train: 265 [1250/1251 (100%)]  Loss: 3.531 (3.35)  Time: 0.806s, 1269.90/s  (0.797s, 1285.09/s)  LR: 4.288e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.507 (1.507)  Loss:  0.7082 (0.7082)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.172 (0.560)  Loss:  0.8050 (1.2355)  Acc@1: 85.9670 (77.9080)  Acc@5: 97.6415 (93.9240)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-265.pth.tar', 77.90799998046874)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-262.pth.tar', 77.89199987304687)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-261.pth.tar', 77.88399997558594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-263.pth.tar', 77.88399992675781)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-257.pth.tar', 77.80999989990234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-264.pth.tar', 77.73800002929687)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-255.pth.tar', 77.73199997558594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-260.pth.tar', 77.70600003173828)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-259.pth.tar', 77.69999989746094)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-258.pth.tar', 77.65200005859376)

Train: 266 [   0/1251 (  0%)]  Loss: 3.426 (3.43)  Time: 2.262s,  452.73/s  (2.262s,  452.73/s)  LR: 4.105e-05  Data: 1.528 (1.528)
Train: 266 [  50/1251 (  4%)]  Loss: 2.945 (3.19)  Time: 0.811s, 1262.98/s  (0.834s, 1227.18/s)  LR: 4.105e-05  Data: 0.011 (0.047)
Train: 266 [ 100/1251 (  8%)]  Loss: 3.530 (3.30)  Time: 0.854s, 1199.76/s  (0.814s, 1258.06/s)  LR: 4.105e-05  Data: 0.011 (0.029)
Train: 266 [ 150/1251 ( 12%)]  Loss: 3.462 (3.34)  Time: 0.777s, 1318.62/s  (0.806s, 1270.96/s)  LR: 4.105e-05  Data: 0.011 (0.023)
Train: 266 [ 200/1251 ( 16%)]  Loss: 3.062 (3.29)  Time: 0.777s, 1317.10/s  (0.804s, 1274.01/s)  LR: 4.105e-05  Data: 0.011 (0.020)
Train: 266 [ 250/1251 ( 20%)]  Loss: 3.189 (3.27)  Time: 0.778s, 1315.38/s  (0.802s, 1276.59/s)  LR: 4.105e-05  Data: 0.011 (0.018)
Train: 266 [ 300/1251 ( 24%)]  Loss: 3.045 (3.24)  Time: 0.780s, 1313.12/s  (0.801s, 1277.61/s)  LR: 4.105e-05  Data: 0.011 (0.017)
Train: 266 [ 350/1251 ( 28%)]  Loss: 3.327 (3.25)  Time: 0.813s, 1259.43/s  (0.801s, 1277.65/s)  LR: 4.105e-05  Data: 0.011 (0.016)
Train: 266 [ 400/1251 ( 32%)]  Loss: 3.598 (3.29)  Time: 0.814s, 1258.34/s  (0.800s, 1279.84/s)  LR: 4.105e-05  Data: 0.011 (0.016)
Train: 266 [ 450/1251 ( 36%)]  Loss: 3.631 (3.32)  Time: 0.779s, 1314.78/s  (0.800s, 1279.69/s)  LR: 4.105e-05  Data: 0.011 (0.015)
Train: 266 [ 500/1251 ( 40%)]  Loss: 3.229 (3.31)  Time: 0.777s, 1317.16/s  (0.800s, 1280.80/s)  LR: 4.105e-05  Data: 0.011 (0.015)
Train: 266 [ 550/1251 ( 44%)]  Loss: 3.331 (3.31)  Time: 0.795s, 1288.37/s  (0.798s, 1283.21/s)  LR: 4.105e-05  Data: 0.011 (0.014)
Train: 266 [ 600/1251 ( 48%)]  Loss: 3.432 (3.32)  Time: 0.780s, 1312.60/s  (0.797s, 1284.16/s)  LR: 4.105e-05  Data: 0.011 (0.014)
Train: 266 [ 650/1251 ( 52%)]  Loss: 3.488 (3.34)  Time: 0.818s, 1251.75/s  (0.797s, 1285.03/s)  LR: 4.105e-05  Data: 0.012 (0.014)
Train: 266 [ 700/1251 ( 56%)]  Loss: 3.577 (3.35)  Time: 0.813s, 1260.01/s  (0.797s, 1284.61/s)  LR: 4.105e-05  Data: 0.012 (0.014)
Train: 266 [ 750/1251 ( 60%)]  Loss: 3.447 (3.36)  Time: 0.795s, 1288.84/s  (0.798s, 1283.78/s)  LR: 4.105e-05  Data: 0.011 (0.014)
Train: 266 [ 800/1251 ( 64%)]  Loss: 2.954 (3.33)  Time: 0.807s, 1268.40/s  (0.797s, 1284.90/s)  LR: 4.105e-05  Data: 0.011 (0.013)
Train: 266 [ 850/1251 ( 68%)]  Loss: 3.205 (3.33)  Time: 0.779s, 1314.03/s  (0.796s, 1286.24/s)  LR: 4.105e-05  Data: 0.011 (0.013)
Train: 266 [ 900/1251 ( 72%)]  Loss: 3.650 (3.34)  Time: 0.778s, 1316.75/s  (0.796s, 1286.70/s)  LR: 4.105e-05  Data: 0.011 (0.013)
Train: 266 [ 950/1251 ( 76%)]  Loss: 3.116 (3.33)  Time: 0.781s, 1311.55/s  (0.795s, 1287.46/s)  LR: 4.105e-05  Data: 0.011 (0.013)
Train: 266 [1000/1251 ( 80%)]  Loss: 3.444 (3.34)  Time: 0.776s, 1319.42/s  (0.795s, 1288.00/s)  LR: 4.105e-05  Data: 0.011 (0.013)
Train: 266 [1050/1251 ( 84%)]  Loss: 2.944 (3.32)  Time: 0.781s, 1310.76/s  (0.795s, 1288.57/s)  LR: 4.105e-05  Data: 0.012 (0.013)
Train: 266 [1100/1251 ( 88%)]  Loss: 3.326 (3.32)  Time: 0.779s, 1314.49/s  (0.794s, 1289.02/s)  LR: 4.105e-05  Data: 0.011 (0.013)
Train: 266 [1150/1251 ( 92%)]  Loss: 3.559 (3.33)  Time: 0.779s, 1314.55/s  (0.794s, 1290.00/s)  LR: 4.105e-05  Data: 0.012 (0.013)
Train: 266 [1200/1251 ( 96%)]  Loss: 3.467 (3.34)  Time: 0.776s, 1318.79/s  (0.794s, 1290.43/s)  LR: 4.105e-05  Data: 0.011 (0.013)
Train: 266 [1250/1251 (100%)]  Loss: 3.611 (3.35)  Time: 0.769s, 1331.00/s  (0.793s, 1290.88/s)  LR: 4.105e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.497 (1.497)  Loss:  0.8015 (0.8015)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.172 (0.564)  Loss:  0.8519 (1.3169)  Acc@1: 86.9104 (77.6000)  Acc@5: 97.2877 (93.8460)
Train: 267 [   0/1251 (  0%)]  Loss: 3.392 (3.39)  Time: 2.304s,  444.46/s  (2.304s,  444.46/s)  LR: 3.926e-05  Data: 1.568 (1.568)
Train: 267 [  50/1251 (  4%)]  Loss: 3.306 (3.35)  Time: 0.777s, 1317.55/s  (0.829s, 1235.09/s)  LR: 3.926e-05  Data: 0.011 (0.048)
Train: 267 [ 100/1251 (  8%)]  Loss: 3.046 (3.25)  Time: 0.781s, 1311.41/s  (0.817s, 1252.91/s)  LR: 3.926e-05  Data: 0.011 (0.030)
Train: 267 [ 150/1251 ( 12%)]  Loss: 3.436 (3.29)  Time: 0.815s, 1256.69/s  (0.810s, 1264.48/s)  LR: 3.926e-05  Data: 0.011 (0.024)
Train: 267 [ 200/1251 ( 16%)]  Loss: 3.233 (3.28)  Time: 0.779s, 1314.74/s  (0.804s, 1273.90/s)  LR: 3.926e-05  Data: 0.010 (0.020)
Train: 267 [ 250/1251 ( 20%)]  Loss: 3.611 (3.34)  Time: 0.784s, 1306.45/s  (0.799s, 1281.72/s)  LR: 3.926e-05  Data: 0.011 (0.019)
Train: 267 [ 300/1251 ( 24%)]  Loss: 3.338 (3.34)  Time: 0.814s, 1257.32/s  (0.797s, 1284.07/s)  LR: 3.926e-05  Data: 0.013 (0.017)
Train: 267 [ 350/1251 ( 28%)]  Loss: 3.085 (3.31)  Time: 0.780s, 1312.12/s  (0.796s, 1286.77/s)  LR: 3.926e-05  Data: 0.011 (0.016)
Train: 267 [ 400/1251 ( 32%)]  Loss: 3.533 (3.33)  Time: 0.782s, 1309.42/s  (0.794s, 1288.95/s)  LR: 3.926e-05  Data: 0.010 (0.016)
Train: 267 [ 450/1251 ( 36%)]  Loss: 3.485 (3.35)  Time: 0.818s, 1252.18/s  (0.794s, 1289.71/s)  LR: 3.926e-05  Data: 0.011 (0.015)
Train: 267 [ 500/1251 ( 40%)]  Loss: 3.406 (3.35)  Time: 0.836s, 1224.99/s  (0.795s, 1288.41/s)  LR: 3.926e-05  Data: 0.011 (0.015)
Train: 267 [ 550/1251 ( 44%)]  Loss: 3.107 (3.33)  Time: 0.778s, 1316.73/s  (0.795s, 1287.72/s)  LR: 3.926e-05  Data: 0.011 (0.014)
Train: 267 [ 600/1251 ( 48%)]  Loss: 3.088 (3.31)  Time: 0.778s, 1315.61/s  (0.796s, 1286.78/s)  LR: 3.926e-05  Data: 0.011 (0.014)
Train: 267 [ 650/1251 ( 52%)]  Loss: 3.597 (3.33)  Time: 0.778s, 1315.74/s  (0.796s, 1287.08/s)  LR: 3.926e-05  Data: 0.011 (0.014)
Train: 267 [ 700/1251 ( 56%)]  Loss: 3.310 (3.33)  Time: 0.785s, 1304.15/s  (0.795s, 1288.52/s)  LR: 3.926e-05  Data: 0.011 (0.014)
Train: 267 [ 750/1251 ( 60%)]  Loss: 3.348 (3.33)  Time: 0.811s, 1262.77/s  (0.794s, 1289.57/s)  LR: 3.926e-05  Data: 0.010 (0.014)
Train: 267 [ 800/1251 ( 64%)]  Loss: 3.182 (3.32)  Time: 0.834s, 1227.40/s  (0.795s, 1288.71/s)  LR: 3.926e-05  Data: 0.017 (0.013)
Train: 267 [ 850/1251 ( 68%)]  Loss: 3.231 (3.32)  Time: 0.820s, 1249.53/s  (0.794s, 1289.23/s)  LR: 3.926e-05  Data: 0.011 (0.013)
Train: 267 [ 900/1251 ( 72%)]  Loss: 3.241 (3.31)  Time: 0.789s, 1297.38/s  (0.794s, 1289.53/s)  LR: 3.926e-05  Data: 0.011 (0.013)
Train: 267 [ 950/1251 ( 76%)]  Loss: 3.092 (3.30)  Time: 0.778s, 1316.55/s  (0.794s, 1289.20/s)  LR: 3.926e-05  Data: 0.011 (0.013)
Train: 267 [1000/1251 ( 80%)]  Loss: 3.305 (3.30)  Time: 0.804s, 1273.67/s  (0.794s, 1290.15/s)  LR: 3.926e-05  Data: 0.010 (0.013)
Train: 267 [1050/1251 ( 84%)]  Loss: 3.329 (3.30)  Time: 0.779s, 1314.68/s  (0.794s, 1290.14/s)  LR: 3.926e-05  Data: 0.011 (0.013)
Train: 267 [1100/1251 ( 88%)]  Loss: 3.523 (3.31)  Time: 0.827s, 1237.81/s  (0.793s, 1290.86/s)  LR: 3.926e-05  Data: 0.011 (0.013)
Train: 267 [1150/1251 ( 92%)]  Loss: 3.183 (3.31)  Time: 0.813s, 1259.93/s  (0.794s, 1290.15/s)  LR: 3.926e-05  Data: 0.011 (0.013)
Train: 267 [1200/1251 ( 96%)]  Loss: 3.321 (3.31)  Time: 0.788s, 1299.90/s  (0.794s, 1289.99/s)  LR: 3.926e-05  Data: 0.013 (0.013)
Train: 267 [1250/1251 (100%)]  Loss: 3.028 (3.30)  Time: 0.770s, 1329.50/s  (0.794s, 1289.77/s)  LR: 3.926e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.525 (1.525)  Loss:  0.6699 (0.6699)  Acc@1: 91.6016 (91.6016)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.172 (0.566)  Loss:  0.7665 (1.1938)  Acc@1: 87.2642 (78.0600)  Acc@5: 98.1132 (93.9480)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-267.pth.tar', 78.06000002685546)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-265.pth.tar', 77.90799998046874)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-262.pth.tar', 77.89199987304687)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-261.pth.tar', 77.88399997558594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-263.pth.tar', 77.88399992675781)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-257.pth.tar', 77.80999989990234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-264.pth.tar', 77.73800002929687)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-255.pth.tar', 77.73199997558594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-260.pth.tar', 77.70600003173828)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-259.pth.tar', 77.69999989746094)

Train: 268 [   0/1251 (  0%)]  Loss: 3.511 (3.51)  Time: 2.247s,  455.74/s  (2.247s,  455.74/s)  LR: 3.753e-05  Data: 1.512 (1.512)
Train: 268 [  50/1251 (  4%)]  Loss: 3.628 (3.57)  Time: 0.811s, 1262.18/s  (0.823s, 1244.16/s)  LR: 3.753e-05  Data: 0.011 (0.044)
Train: 268 [ 100/1251 (  8%)]  Loss: 3.293 (3.48)  Time: 0.786s, 1303.13/s  (0.803s, 1275.04/s)  LR: 3.753e-05  Data: 0.010 (0.027)
Train: 268 [ 150/1251 ( 12%)]  Loss: 3.172 (3.40)  Time: 0.780s, 1313.56/s  (0.799s, 1281.40/s)  LR: 3.753e-05  Data: 0.011 (0.022)
Train: 268 [ 200/1251 ( 16%)]  Loss: 3.224 (3.37)  Time: 0.779s, 1314.50/s  (0.798s, 1282.44/s)  LR: 3.753e-05  Data: 0.012 (0.019)
Train: 268 [ 250/1251 ( 20%)]  Loss: 3.315 (3.36)  Time: 0.777s, 1317.99/s  (0.800s, 1279.57/s)  LR: 3.753e-05  Data: 0.011 (0.018)
Train: 268 [ 300/1251 ( 24%)]  Loss: 3.118 (3.32)  Time: 0.779s, 1315.30/s  (0.797s, 1284.10/s)  LR: 3.753e-05  Data: 0.011 (0.017)
Train: 268 [ 350/1251 ( 28%)]  Loss: 3.526 (3.35)  Time: 0.778s, 1316.14/s  (0.796s, 1285.96/s)  LR: 3.753e-05  Data: 0.011 (0.016)
Train: 268 [ 400/1251 ( 32%)]  Loss: 3.690 (3.39)  Time: 0.780s, 1313.02/s  (0.795s, 1288.74/s)  LR: 3.753e-05  Data: 0.011 (0.015)
Train: 268 [ 450/1251 ( 36%)]  Loss: 2.990 (3.35)  Time: 0.777s, 1318.64/s  (0.793s, 1291.15/s)  LR: 3.753e-05  Data: 0.011 (0.015)
Train: 268 [ 500/1251 ( 40%)]  Loss: 3.315 (3.34)  Time: 0.778s, 1315.39/s  (0.792s, 1292.15/s)  LR: 3.753e-05  Data: 0.011 (0.014)
Train: 268 [ 550/1251 ( 44%)]  Loss: 3.357 (3.34)  Time: 0.779s, 1315.05/s  (0.791s, 1293.87/s)  LR: 3.753e-05  Data: 0.011 (0.014)
Train: 268 [ 600/1251 ( 48%)]  Loss: 3.246 (3.34)  Time: 0.822s, 1246.27/s  (0.791s, 1294.64/s)  LR: 3.753e-05  Data: 0.011 (0.014)
Train: 268 [ 650/1251 ( 52%)]  Loss: 3.187 (3.33)  Time: 0.777s, 1318.15/s  (0.792s, 1293.46/s)  LR: 3.753e-05  Data: 0.011 (0.014)
Train: 268 [ 700/1251 ( 56%)]  Loss: 3.587 (3.34)  Time: 0.779s, 1314.99/s  (0.792s, 1293.01/s)  LR: 3.753e-05  Data: 0.011 (0.013)
Train: 268 [ 750/1251 ( 60%)]  Loss: 3.007 (3.32)  Time: 0.780s, 1312.44/s  (0.791s, 1293.86/s)  LR: 3.753e-05  Data: 0.011 (0.013)
Train: 268 [ 800/1251 ( 64%)]  Loss: 3.611 (3.34)  Time: 0.778s, 1315.85/s  (0.791s, 1294.47/s)  LR: 3.753e-05  Data: 0.011 (0.013)
Train: 268 [ 850/1251 ( 68%)]  Loss: 2.955 (3.32)  Time: 0.785s, 1304.48/s  (0.791s, 1295.21/s)  LR: 3.753e-05  Data: 0.011 (0.013)
Train: 268 [ 900/1251 ( 72%)]  Loss: 3.139 (3.31)  Time: 0.781s, 1310.90/s  (0.791s, 1295.26/s)  LR: 3.753e-05  Data: 0.011 (0.013)
Train: 268 [ 950/1251 ( 76%)]  Loss: 3.169 (3.30)  Time: 0.827s, 1237.67/s  (0.790s, 1295.70/s)  LR: 3.753e-05  Data: 0.010 (0.013)
Train: 268 [1000/1251 ( 80%)]  Loss: 3.107 (3.29)  Time: 0.781s, 1310.54/s  (0.790s, 1295.68/s)  LR: 3.753e-05  Data: 0.011 (0.013)
Train: 268 [1050/1251 ( 84%)]  Loss: 3.347 (3.30)  Time: 0.786s, 1302.39/s  (0.791s, 1295.19/s)  LR: 3.753e-05  Data: 0.011 (0.013)
Train: 268 [1100/1251 ( 88%)]  Loss: 3.522 (3.30)  Time: 0.823s, 1244.07/s  (0.791s, 1294.56/s)  LR: 3.753e-05  Data: 0.011 (0.013)
Train: 268 [1150/1251 ( 92%)]  Loss: 3.050 (3.29)  Time: 0.819s, 1251.03/s  (0.791s, 1294.14/s)  LR: 3.753e-05  Data: 0.012 (0.013)
Train: 268 [1200/1251 ( 96%)]  Loss: 3.579 (3.31)  Time: 0.787s, 1301.32/s  (0.791s, 1293.98/s)  LR: 3.753e-05  Data: 0.015 (0.013)
Train: 268 [1250/1251 (100%)]  Loss: 3.144 (3.30)  Time: 0.767s, 1335.33/s  (0.792s, 1293.61/s)  LR: 3.753e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.575 (1.575)  Loss:  0.7496 (0.7496)  Acc@1: 91.8945 (91.8945)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.172 (0.571)  Loss:  0.8300 (1.2630)  Acc@1: 87.0283 (78.0360)  Acc@5: 97.9953 (93.9500)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-267.pth.tar', 78.06000002685546)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-268.pth.tar', 78.0359999243164)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-265.pth.tar', 77.90799998046874)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-262.pth.tar', 77.89199987304687)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-261.pth.tar', 77.88399997558594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-263.pth.tar', 77.88399992675781)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-257.pth.tar', 77.80999989990234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-264.pth.tar', 77.73800002929687)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-255.pth.tar', 77.73199997558594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-260.pth.tar', 77.70600003173828)

Train: 269 [   0/1251 (  0%)]  Loss: 3.338 (3.34)  Time: 2.303s,  444.72/s  (2.303s,  444.72/s)  LR: 3.585e-05  Data: 1.567 (1.567)
Train: 269 [  50/1251 (  4%)]  Loss: 3.122 (3.23)  Time: 0.780s, 1313.21/s  (0.824s, 1243.07/s)  LR: 3.585e-05  Data: 0.011 (0.046)
Train: 269 [ 100/1251 (  8%)]  Loss: 3.065 (3.17)  Time: 0.855s, 1197.53/s  (0.808s, 1267.94/s)  LR: 3.585e-05  Data: 0.010 (0.029)
Train: 269 [ 150/1251 ( 12%)]  Loss: 3.537 (3.27)  Time: 0.774s, 1322.35/s  (0.802s, 1276.58/s)  LR: 3.585e-05  Data: 0.011 (0.023)
Train: 269 [ 200/1251 ( 16%)]  Loss: 3.149 (3.24)  Time: 0.779s, 1313.99/s  (0.799s, 1281.27/s)  LR: 3.585e-05  Data: 0.010 (0.020)
Train: 269 [ 250/1251 ( 20%)]  Loss: 3.541 (3.29)  Time: 0.848s, 1207.14/s  (0.801s, 1277.71/s)  LR: 3.585e-05  Data: 0.012 (0.018)
Train: 269 [ 300/1251 ( 24%)]  Loss: 3.386 (3.31)  Time: 0.778s, 1315.58/s  (0.800s, 1280.13/s)  LR: 3.585e-05  Data: 0.011 (0.017)
Train: 269 [ 350/1251 ( 28%)]  Loss: 3.477 (3.33)  Time: 0.781s, 1311.67/s  (0.798s, 1282.86/s)  LR: 3.585e-05  Data: 0.011 (0.016)
Train: 269 [ 400/1251 ( 32%)]  Loss: 3.485 (3.34)  Time: 0.781s, 1310.30/s  (0.797s, 1285.50/s)  LR: 3.585e-05  Data: 0.011 (0.016)
Train: 269 [ 450/1251 ( 36%)]  Loss: 3.478 (3.36)  Time: 0.785s, 1304.62/s  (0.796s, 1286.54/s)  LR: 3.585e-05  Data: 0.011 (0.015)
Train: 269 [ 500/1251 ( 40%)]  Loss: 3.219 (3.35)  Time: 0.815s, 1256.88/s  (0.795s, 1287.63/s)  LR: 3.585e-05  Data: 0.012 (0.015)
Train: 269 [ 550/1251 ( 44%)]  Loss: 3.244 (3.34)  Time: 0.776s, 1318.88/s  (0.795s, 1288.22/s)  LR: 3.585e-05  Data: 0.011 (0.014)
Train: 269 [ 600/1251 ( 48%)]  Loss: 3.474 (3.35)  Time: 0.818s, 1252.17/s  (0.794s, 1289.23/s)  LR: 3.585e-05  Data: 0.011 (0.014)
Train: 269 [ 650/1251 ( 52%)]  Loss: 3.183 (3.34)  Time: 0.841s, 1217.67/s  (0.795s, 1288.10/s)  LR: 3.585e-05  Data: 0.011 (0.014)
Train: 269 [ 700/1251 ( 56%)]  Loss: 3.081 (3.32)  Time: 0.814s, 1257.78/s  (0.795s, 1287.94/s)  LR: 3.585e-05  Data: 0.011 (0.014)
Train: 269 [ 750/1251 ( 60%)]  Loss: 3.154 (3.31)  Time: 0.807s, 1269.59/s  (0.797s, 1285.22/s)  LR: 3.585e-05  Data: 0.011 (0.013)
Train: 269 [ 800/1251 ( 64%)]  Loss: 3.668 (3.33)  Time: 0.780s, 1312.55/s  (0.797s, 1284.65/s)  LR: 3.585e-05  Data: 0.011 (0.013)
Train: 269 [ 850/1251 ( 68%)]  Loss: 3.279 (3.33)  Time: 0.780s, 1312.44/s  (0.797s, 1285.42/s)  LR: 3.585e-05  Data: 0.011 (0.013)
Train: 269 [ 900/1251 ( 72%)]  Loss: 3.448 (3.33)  Time: 0.780s, 1313.33/s  (0.796s, 1286.50/s)  LR: 3.585e-05  Data: 0.011 (0.013)
Train: 269 [ 950/1251 ( 76%)]  Loss: 3.602 (3.35)  Time: 0.782s, 1308.92/s  (0.796s, 1287.21/s)  LR: 3.585e-05  Data: 0.011 (0.013)
Train: 269 [1000/1251 ( 80%)]  Loss: 3.183 (3.34)  Time: 0.815s, 1256.71/s  (0.795s, 1287.49/s)  LR: 3.585e-05  Data: 0.011 (0.013)
Train: 269 [1050/1251 ( 84%)]  Loss: 3.361 (3.34)  Time: 0.781s, 1311.29/s  (0.795s, 1288.31/s)  LR: 3.585e-05  Data: 0.011 (0.013)
Train: 269 [1100/1251 ( 88%)]  Loss: 3.212 (3.33)  Time: 0.794s, 1289.17/s  (0.795s, 1288.40/s)  LR: 3.585e-05  Data: 0.011 (0.013)
Train: 269 [1150/1251 ( 92%)]  Loss: 2.773 (3.31)  Time: 0.779s, 1314.56/s  (0.794s, 1289.29/s)  LR: 3.585e-05  Data: 0.011 (0.013)
Train: 269 [1200/1251 ( 96%)]  Loss: 2.970 (3.30)  Time: 0.791s, 1294.01/s  (0.794s, 1289.75/s)  LR: 3.585e-05  Data: 0.011 (0.013)
Train: 269 [1250/1251 (100%)]  Loss: 3.209 (3.29)  Time: 0.801s, 1278.74/s  (0.794s, 1290.22/s)  LR: 3.585e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.508 (1.508)  Loss:  0.6915 (0.6915)  Acc@1: 91.7969 (91.7969)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.172 (0.560)  Loss:  0.7872 (1.2203)  Acc@1: 86.9104 (78.1060)  Acc@5: 97.7594 (93.9800)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-269.pth.tar', 78.1060000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-267.pth.tar', 78.06000002685546)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-268.pth.tar', 78.0359999243164)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-265.pth.tar', 77.90799998046874)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-262.pth.tar', 77.89199987304687)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-261.pth.tar', 77.88399997558594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-263.pth.tar', 77.88399992675781)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-257.pth.tar', 77.80999989990234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-264.pth.tar', 77.73800002929687)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-255.pth.tar', 77.73199997558594)

Train: 270 [   0/1251 (  0%)]  Loss: 3.451 (3.45)  Time: 2.289s,  447.27/s  (2.289s,  447.27/s)  LR: 3.423e-05  Data: 1.490 (1.490)
Train: 270 [  50/1251 (  4%)]  Loss: 3.331 (3.39)  Time: 0.847s, 1209.23/s  (0.839s, 1221.11/s)  LR: 3.423e-05  Data: 0.011 (0.043)
Train: 270 [ 100/1251 (  8%)]  Loss: 3.301 (3.36)  Time: 0.777s, 1317.24/s  (0.816s, 1255.43/s)  LR: 3.423e-05  Data: 0.010 (0.027)
Train: 270 [ 150/1251 ( 12%)]  Loss: 3.106 (3.30)  Time: 0.791s, 1294.96/s  (0.809s, 1265.82/s)  LR: 3.423e-05  Data: 0.012 (0.022)
Train: 270 [ 200/1251 ( 16%)]  Loss: 3.547 (3.35)  Time: 0.834s, 1227.36/s  (0.804s, 1274.16/s)  LR: 3.423e-05  Data: 0.011 (0.019)
Train: 270 [ 250/1251 ( 20%)]  Loss: 3.308 (3.34)  Time: 0.780s, 1313.16/s  (0.802s, 1276.03/s)  LR: 3.423e-05  Data: 0.012 (0.018)
Train: 270 [ 300/1251 ( 24%)]  Loss: 3.209 (3.32)  Time: 0.826s, 1239.84/s  (0.802s, 1276.93/s)  LR: 3.423e-05  Data: 0.012 (0.016)
Train: 270 [ 350/1251 ( 28%)]  Loss: 3.457 (3.34)  Time: 0.818s, 1251.61/s  (0.801s, 1278.46/s)  LR: 3.423e-05  Data: 0.011 (0.016)
Train: 270 [ 400/1251 ( 32%)]  Loss: 3.304 (3.33)  Time: 0.783s, 1307.16/s  (0.800s, 1280.64/s)  LR: 3.423e-05  Data: 0.011 (0.015)
Train: 270 [ 450/1251 ( 36%)]  Loss: 2.942 (3.30)  Time: 0.781s, 1311.06/s  (0.798s, 1283.12/s)  LR: 3.423e-05  Data: 0.011 (0.015)
Train: 270 [ 500/1251 ( 40%)]  Loss: 3.187 (3.29)  Time: 0.819s, 1250.68/s  (0.799s, 1281.96/s)  LR: 3.423e-05  Data: 0.012 (0.014)
Train: 270 [ 550/1251 ( 44%)]  Loss: 3.461 (3.30)  Time: 0.819s, 1251.03/s  (0.798s, 1283.63/s)  LR: 3.423e-05  Data: 0.011 (0.014)
Train: 270 [ 600/1251 ( 48%)]  Loss: 3.438 (3.31)  Time: 0.810s, 1263.64/s  (0.797s, 1284.41/s)  LR: 3.423e-05  Data: 0.011 (0.014)
Train: 270 [ 650/1251 ( 52%)]  Loss: 3.605 (3.33)  Time: 0.819s, 1250.03/s  (0.796s, 1286.15/s)  LR: 3.423e-05  Data: 0.011 (0.014)
Train: 270 [ 700/1251 ( 56%)]  Loss: 3.328 (3.33)  Time: 0.789s, 1297.53/s  (0.796s, 1286.52/s)  LR: 3.423e-05  Data: 0.011 (0.013)
Train: 270 [ 750/1251 ( 60%)]  Loss: 3.216 (3.32)  Time: 0.785s, 1305.25/s  (0.795s, 1287.64/s)  LR: 3.423e-05  Data: 0.012 (0.013)
Train: 270 [ 800/1251 ( 64%)]  Loss: 3.461 (3.33)  Time: 0.778s, 1315.73/s  (0.794s, 1288.92/s)  LR: 3.423e-05  Data: 0.010 (0.013)
Train: 270 [ 850/1251 ( 68%)]  Loss: 3.502 (3.34)  Time: 0.801s, 1278.11/s  (0.794s, 1289.50/s)  LR: 3.423e-05  Data: 0.011 (0.013)
Train: 270 [ 900/1251 ( 72%)]  Loss: 3.419 (3.35)  Time: 0.778s, 1316.13/s  (0.795s, 1288.76/s)  LR: 3.423e-05  Data: 0.011 (0.013)
Train: 270 [ 950/1251 ( 76%)]  Loss: 3.496 (3.35)  Time: 0.776s, 1318.74/s  (0.794s, 1289.79/s)  LR: 3.423e-05  Data: 0.011 (0.013)
Train: 270 [1000/1251 ( 80%)]  Loss: 3.298 (3.35)  Time: 0.780s, 1312.97/s  (0.793s, 1290.72/s)  LR: 3.423e-05  Data: 0.010 (0.013)
Train: 270 [1050/1251 ( 84%)]  Loss: 2.840 (3.33)  Time: 0.834s, 1227.74/s  (0.793s, 1290.98/s)  LR: 3.423e-05  Data: 0.011 (0.013)
Train: 270 [1100/1251 ( 88%)]  Loss: 3.732 (3.35)  Time: 0.794s, 1289.41/s  (0.793s, 1291.44/s)  LR: 3.423e-05  Data: 0.011 (0.012)
Train: 270 [1150/1251 ( 92%)]  Loss: 3.161 (3.34)  Time: 0.779s, 1314.47/s  (0.793s, 1291.34/s)  LR: 3.423e-05  Data: 0.011 (0.012)
Train: 270 [1200/1251 ( 96%)]  Loss: 3.337 (3.34)  Time: 0.780s, 1313.65/s  (0.793s, 1291.99/s)  LR: 3.423e-05  Data: 0.011 (0.012)
Train: 270 [1250/1251 (100%)]  Loss: 3.585 (3.35)  Time: 0.773s, 1323.90/s  (0.792s, 1292.57/s)  LR: 3.423e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.554 (1.554)  Loss:  0.7931 (0.7931)  Acc@1: 91.4062 (91.4062)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.172 (0.568)  Loss:  0.8634 (1.3126)  Acc@1: 86.3208 (77.9020)  Acc@5: 98.2311 (93.9180)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-269.pth.tar', 78.1060000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-267.pth.tar', 78.06000002685546)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-268.pth.tar', 78.0359999243164)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-265.pth.tar', 77.90799998046874)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-270.pth.tar', 77.90200013427734)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-262.pth.tar', 77.89199987304687)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-261.pth.tar', 77.88399997558594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-263.pth.tar', 77.88399992675781)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-257.pth.tar', 77.80999989990234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-264.pth.tar', 77.73800002929687)

Train: 271 [   0/1251 (  0%)]  Loss: 3.186 (3.19)  Time: 2.353s,  435.15/s  (2.353s,  435.15/s)  LR: 3.265e-05  Data: 1.619 (1.619)
Train: 271 [  50/1251 (  4%)]  Loss: 3.410 (3.30)  Time: 0.777s, 1318.05/s  (0.822s, 1245.41/s)  LR: 3.265e-05  Data: 0.011 (0.048)
Train: 271 [ 100/1251 (  8%)]  Loss: 3.340 (3.31)  Time: 0.782s, 1309.89/s  (0.806s, 1270.00/s)  LR: 3.265e-05  Data: 0.011 (0.030)
Train: 271 [ 150/1251 ( 12%)]  Loss: 3.485 (3.36)  Time: 0.787s, 1300.78/s  (0.800s, 1279.38/s)  LR: 3.265e-05  Data: 0.011 (0.024)
Train: 271 [ 200/1251 ( 16%)]  Loss: 3.491 (3.38)  Time: 0.835s, 1226.99/s  (0.797s, 1284.58/s)  LR: 3.265e-05  Data: 0.011 (0.021)
Train: 271 [ 250/1251 ( 20%)]  Loss: 3.482 (3.40)  Time: 0.782s, 1309.71/s  (0.796s, 1285.94/s)  LR: 3.265e-05  Data: 0.011 (0.019)
Train: 271 [ 300/1251 ( 24%)]  Loss: 3.453 (3.41)  Time: 0.782s, 1309.10/s  (0.795s, 1287.94/s)  LR: 3.265e-05  Data: 0.011 (0.017)
Train: 271 [ 350/1251 ( 28%)]  Loss: 3.375 (3.40)  Time: 0.838s, 1222.18/s  (0.796s, 1286.72/s)  LR: 3.265e-05  Data: 0.010 (0.016)
Train: 271 [ 400/1251 ( 32%)]  Loss: 3.348 (3.40)  Time: 0.783s, 1307.28/s  (0.794s, 1289.33/s)  LR: 3.265e-05  Data: 0.011 (0.016)
Train: 271 [ 450/1251 ( 36%)]  Loss: 3.637 (3.42)  Time: 0.779s, 1313.97/s  (0.795s, 1288.65/s)  LR: 3.265e-05  Data: 0.011 (0.015)
Train: 271 [ 500/1251 ( 40%)]  Loss: 3.207 (3.40)  Time: 0.787s, 1300.54/s  (0.794s, 1288.98/s)  LR: 3.265e-05  Data: 0.011 (0.015)
Train: 271 [ 550/1251 ( 44%)]  Loss: 3.336 (3.40)  Time: 0.788s, 1299.00/s  (0.794s, 1289.71/s)  LR: 3.265e-05  Data: 0.014 (0.014)
Train: 271 [ 600/1251 ( 48%)]  Loss: 3.303 (3.39)  Time: 0.780s, 1312.15/s  (0.793s, 1291.30/s)  LR: 3.265e-05  Data: 0.011 (0.014)
Train: 271 [ 650/1251 ( 52%)]  Loss: 3.278 (3.38)  Time: 0.779s, 1315.23/s  (0.793s, 1291.87/s)  LR: 3.265e-05  Data: 0.011 (0.014)
Train: 271 [ 700/1251 ( 56%)]  Loss: 3.413 (3.38)  Time: 0.783s, 1307.03/s  (0.792s, 1293.08/s)  LR: 3.265e-05  Data: 0.011 (0.014)
Train: 271 [ 750/1251 ( 60%)]  Loss: 3.316 (3.38)  Time: 0.780s, 1312.64/s  (0.792s, 1293.29/s)  LR: 3.265e-05  Data: 0.011 (0.014)
Train: 271 [ 800/1251 ( 64%)]  Loss: 3.469 (3.38)  Time: 0.778s, 1316.42/s  (0.792s, 1293.41/s)  LR: 3.265e-05  Data: 0.011 (0.013)
Train: 271 [ 850/1251 ( 68%)]  Loss: 3.036 (3.36)  Time: 0.779s, 1314.32/s  (0.791s, 1294.45/s)  LR: 3.265e-05  Data: 0.011 (0.013)
Train: 271 [ 900/1251 ( 72%)]  Loss: 3.663 (3.38)  Time: 0.780s, 1313.35/s  (0.791s, 1295.17/s)  LR: 3.265e-05  Data: 0.011 (0.013)
Train: 271 [ 950/1251 ( 76%)]  Loss: 3.288 (3.38)  Time: 0.814s, 1258.33/s  (0.791s, 1294.68/s)  LR: 3.265e-05  Data: 0.011 (0.013)
Train: 271 [1000/1251 ( 80%)]  Loss: 3.157 (3.37)  Time: 0.812s, 1261.35/s  (0.792s, 1292.58/s)  LR: 3.265e-05  Data: 0.011 (0.013)
Train: 271 [1050/1251 ( 84%)]  Loss: 3.270 (3.36)  Time: 0.779s, 1314.44/s  (0.792s, 1292.54/s)  LR: 3.265e-05  Data: 0.011 (0.013)
Train: 271 [1100/1251 ( 88%)]  Loss: 3.531 (3.37)  Time: 0.823s, 1244.23/s  (0.792s, 1292.31/s)  LR: 3.265e-05  Data: 0.011 (0.013)
Train: 271 [1150/1251 ( 92%)]  Loss: 3.313 (3.37)  Time: 0.785s, 1304.52/s  (0.792s, 1292.39/s)  LR: 3.265e-05  Data: 0.014 (0.013)
Train: 271 [1200/1251 ( 96%)]  Loss: 3.580 (3.37)  Time: 0.781s, 1311.19/s  (0.792s, 1292.32/s)  LR: 3.265e-05  Data: 0.011 (0.013)
Train: 271 [1250/1251 (100%)]  Loss: 3.448 (3.38)  Time: 0.769s, 1332.44/s  (0.792s, 1292.32/s)  LR: 3.265e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.505 (1.505)  Loss:  0.7639 (0.7639)  Acc@1: 91.3086 (91.3086)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.172 (0.556)  Loss:  0.8190 (1.2635)  Acc@1: 86.5566 (77.9540)  Acc@5: 97.8774 (93.9580)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-269.pth.tar', 78.1060000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-267.pth.tar', 78.06000002685546)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-268.pth.tar', 78.0359999243164)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-271.pth.tar', 77.95399997802734)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-265.pth.tar', 77.90799998046874)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-270.pth.tar', 77.90200013427734)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-262.pth.tar', 77.89199987304687)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-261.pth.tar', 77.88399997558594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-263.pth.tar', 77.88399992675781)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-257.pth.tar', 77.80999989990234)

Train: 272 [   0/1251 (  0%)]  Loss: 3.231 (3.23)  Time: 2.260s,  453.12/s  (2.260s,  453.12/s)  LR: 3.113e-05  Data: 1.523 (1.523)
Train: 272 [  50/1251 (  4%)]  Loss: 3.487 (3.36)  Time: 0.816s, 1255.02/s  (0.833s, 1228.90/s)  LR: 3.113e-05  Data: 0.011 (0.045)
Train: 272 [ 100/1251 (  8%)]  Loss: 3.009 (3.24)  Time: 0.784s, 1306.77/s  (0.810s, 1264.66/s)  LR: 3.113e-05  Data: 0.011 (0.028)
Train: 272 [ 150/1251 ( 12%)]  Loss: 3.401 (3.28)  Time: 0.796s, 1285.95/s  (0.803s, 1275.43/s)  LR: 3.113e-05  Data: 0.011 (0.022)
Train: 272 [ 200/1251 ( 16%)]  Loss: 3.648 (3.36)  Time: 0.829s, 1235.80/s  (0.801s, 1278.85/s)  LR: 3.113e-05  Data: 0.011 (0.020)
Train: 272 [ 250/1251 ( 20%)]  Loss: 3.079 (3.31)  Time: 0.819s, 1250.19/s  (0.798s, 1283.96/s)  LR: 3.113e-05  Data: 0.011 (0.018)
Train: 272 [ 300/1251 ( 24%)]  Loss: 3.587 (3.35)  Time: 0.820s, 1249.06/s  (0.798s, 1283.37/s)  LR: 3.113e-05  Data: 0.010 (0.017)
Train: 272 [ 350/1251 ( 28%)]  Loss: 3.240 (3.34)  Time: 0.780s, 1312.27/s  (0.797s, 1285.18/s)  LR: 3.113e-05  Data: 0.011 (0.016)
Train: 272 [ 400/1251 ( 32%)]  Loss: 3.548 (3.36)  Time: 0.778s, 1315.80/s  (0.795s, 1287.73/s)  LR: 3.113e-05  Data: 0.011 (0.015)
Train: 272 [ 450/1251 ( 36%)]  Loss: 3.251 (3.35)  Time: 0.822s, 1245.50/s  (0.794s, 1289.83/s)  LR: 3.113e-05  Data: 0.011 (0.015)
Train: 272 [ 500/1251 ( 40%)]  Loss: 3.161 (3.33)  Time: 0.816s, 1254.82/s  (0.793s, 1291.12/s)  LR: 3.113e-05  Data: 0.011 (0.014)
Train: 272 [ 550/1251 ( 44%)]  Loss: 2.954 (3.30)  Time: 0.778s, 1316.72/s  (0.792s, 1292.34/s)  LR: 3.113e-05  Data: 0.011 (0.014)
Train: 272 [ 600/1251 ( 48%)]  Loss: 3.560 (3.32)  Time: 0.788s, 1299.56/s  (0.792s, 1293.27/s)  LR: 3.113e-05  Data: 0.010 (0.014)
Train: 272 [ 650/1251 ( 52%)]  Loss: 3.362 (3.32)  Time: 0.785s, 1304.91/s  (0.792s, 1292.15/s)  LR: 3.113e-05  Data: 0.012 (0.014)
Train: 272 [ 700/1251 ( 56%)]  Loss: 3.693 (3.35)  Time: 0.777s, 1317.31/s  (0.792s, 1293.36/s)  LR: 3.113e-05  Data: 0.012 (0.013)
Train: 272 [ 750/1251 ( 60%)]  Loss: 3.306 (3.34)  Time: 0.816s, 1255.63/s  (0.793s, 1291.70/s)  LR: 3.113e-05  Data: 0.012 (0.013)
Train: 272 [ 800/1251 ( 64%)]  Loss: 3.017 (3.33)  Time: 0.874s, 1171.65/s  (0.793s, 1291.69/s)  LR: 3.113e-05  Data: 0.010 (0.013)
Train: 272 [ 850/1251 ( 68%)]  Loss: 3.489 (3.33)  Time: 0.814s, 1257.25/s  (0.793s, 1290.85/s)  LR: 3.113e-05  Data: 0.011 (0.013)
Train: 272 [ 900/1251 ( 72%)]  Loss: 3.446 (3.34)  Time: 0.780s, 1313.41/s  (0.793s, 1291.75/s)  LR: 3.113e-05  Data: 0.011 (0.013)
Train: 272 [ 950/1251 ( 76%)]  Loss: 3.571 (3.35)  Time: 0.780s, 1312.13/s  (0.792s, 1292.29/s)  LR: 3.113e-05  Data: 0.011 (0.013)
Train: 272 [1000/1251 ( 80%)]  Loss: 3.193 (3.34)  Time: 0.805s, 1272.09/s  (0.792s, 1293.15/s)  LR: 3.113e-05  Data: 0.011 (0.013)
Train: 272 [1050/1251 ( 84%)]  Loss: 3.279 (3.34)  Time: 0.779s, 1313.76/s  (0.792s, 1293.68/s)  LR: 3.113e-05  Data: 0.011 (0.013)
Train: 272 [1100/1251 ( 88%)]  Loss: 3.524 (3.35)  Time: 0.778s, 1315.43/s  (0.792s, 1293.65/s)  LR: 3.113e-05  Data: 0.010 (0.013)
Train: 272 [1150/1251 ( 92%)]  Loss: 3.284 (3.35)  Time: 0.779s, 1314.82/s  (0.792s, 1293.42/s)  LR: 3.113e-05  Data: 0.010 (0.013)
Train: 272 [1200/1251 ( 96%)]  Loss: 3.280 (3.34)  Time: 0.822s, 1245.08/s  (0.792s, 1293.42/s)  LR: 3.113e-05  Data: 0.011 (0.012)
Train: 272 [1250/1251 (100%)]  Loss: 3.550 (3.35)  Time: 0.765s, 1338.56/s  (0.792s, 1293.38/s)  LR: 3.113e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.552 (1.552)  Loss:  0.7182 (0.7182)  Acc@1: 91.5039 (91.5039)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.172 (0.557)  Loss:  0.8172 (1.2271)  Acc@1: 86.6745 (78.1520)  Acc@5: 97.5236 (94.0680)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-272.pth.tar', 78.15200002929687)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-269.pth.tar', 78.1060000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-267.pth.tar', 78.06000002685546)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-268.pth.tar', 78.0359999243164)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-271.pth.tar', 77.95399997802734)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-265.pth.tar', 77.90799998046874)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-270.pth.tar', 77.90200013427734)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-262.pth.tar', 77.89199987304687)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-261.pth.tar', 77.88399997558594)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-263.pth.tar', 77.88399992675781)

Train: 273 [   0/1251 (  0%)]  Loss: 3.173 (3.17)  Time: 2.209s,  463.63/s  (2.209s,  463.63/s)  LR: 2.965e-05  Data: 1.473 (1.473)
Train: 273 [  50/1251 (  4%)]  Loss: 3.243 (3.21)  Time: 0.812s, 1261.16/s  (0.826s, 1239.42/s)  LR: 2.965e-05  Data: 0.011 (0.043)
Train: 273 [ 100/1251 (  8%)]  Loss: 3.391 (3.27)  Time: 0.785s, 1305.01/s  (0.809s, 1266.18/s)  LR: 2.965e-05  Data: 0.011 (0.027)
Train: 273 [ 150/1251 ( 12%)]  Loss: 2.862 (3.17)  Time: 0.835s, 1226.45/s  (0.801s, 1278.06/s)  LR: 2.965e-05  Data: 0.011 (0.022)
Train: 273 [ 200/1251 ( 16%)]  Loss: 3.235 (3.18)  Time: 0.780s, 1312.94/s  (0.799s, 1281.48/s)  LR: 2.965e-05  Data: 0.011 (0.019)
Train: 273 [ 250/1251 ( 20%)]  Loss: 3.369 (3.21)  Time: 0.816s, 1255.65/s  (0.797s, 1285.57/s)  LR: 2.965e-05  Data: 0.013 (0.018)
Train: 273 [ 300/1251 ( 24%)]  Loss: 3.127 (3.20)  Time: 0.781s, 1311.42/s  (0.796s, 1286.42/s)  LR: 2.965e-05  Data: 0.010 (0.017)
Train: 273 [ 350/1251 ( 28%)]  Loss: 3.476 (3.23)  Time: 0.782s, 1309.65/s  (0.797s, 1285.37/s)  LR: 2.965e-05  Data: 0.011 (0.016)
Train: 273 [ 400/1251 ( 32%)]  Loss: 2.966 (3.20)  Time: 0.780s, 1312.12/s  (0.796s, 1286.62/s)  LR: 2.965e-05  Data: 0.011 (0.015)
Train: 273 [ 450/1251 ( 36%)]  Loss: 3.387 (3.22)  Time: 0.778s, 1316.46/s  (0.794s, 1289.53/s)  LR: 2.965e-05  Data: 0.011 (0.015)
Train: 273 [ 500/1251 ( 40%)]  Loss: 3.108 (3.21)  Time: 0.784s, 1306.59/s  (0.793s, 1291.14/s)  LR: 2.965e-05  Data: 0.011 (0.014)
Train: 273 [ 550/1251 ( 44%)]  Loss: 2.992 (3.19)  Time: 0.818s, 1252.35/s  (0.792s, 1292.32/s)  LR: 2.965e-05  Data: 0.011 (0.014)
Train: 273 [ 600/1251 ( 48%)]  Loss: 3.546 (3.22)  Time: 0.780s, 1312.29/s  (0.792s, 1293.08/s)  LR: 2.965e-05  Data: 0.011 (0.014)
Train: 273 [ 650/1251 ( 52%)]  Loss: 3.108 (3.21)  Time: 0.822s, 1245.00/s  (0.792s, 1292.60/s)  LR: 2.965e-05  Data: 0.011 (0.014)
Train: 273 [ 700/1251 ( 56%)]  Loss: 3.361 (3.22)  Time: 0.822s, 1245.65/s  (0.793s, 1290.59/s)  LR: 2.965e-05  Data: 0.011 (0.013)
Train: 273 [ 750/1251 ( 60%)]  Loss: 3.432 (3.24)  Time: 0.832s, 1231.15/s  (0.794s, 1290.19/s)  LR: 2.965e-05  Data: 0.011 (0.013)
Train: 273 [ 800/1251 ( 64%)]  Loss: 3.280 (3.24)  Time: 0.778s, 1315.38/s  (0.793s, 1291.53/s)  LR: 2.965e-05  Data: 0.011 (0.013)
Train: 273 [ 850/1251 ( 68%)]  Loss: 3.220 (3.24)  Time: 0.814s, 1258.53/s  (0.793s, 1291.20/s)  LR: 2.965e-05  Data: 0.012 (0.013)
Train: 273 [ 900/1251 ( 72%)]  Loss: 3.225 (3.24)  Time: 0.779s, 1314.07/s  (0.794s, 1289.98/s)  LR: 2.965e-05  Data: 0.011 (0.013)
Train: 273 [ 950/1251 ( 76%)]  Loss: 2.984 (3.22)  Time: 0.777s, 1317.95/s  (0.793s, 1290.81/s)  LR: 2.965e-05  Data: 0.011 (0.013)
Train: 273 [1000/1251 ( 80%)]  Loss: 3.253 (3.23)  Time: 0.778s, 1316.05/s  (0.793s, 1291.73/s)  LR: 2.965e-05  Data: 0.011 (0.013)
Train: 273 [1050/1251 ( 84%)]  Loss: 3.055 (3.22)  Time: 0.779s, 1314.72/s  (0.793s, 1291.51/s)  LR: 2.965e-05  Data: 0.011 (0.013)
Train: 273 [1100/1251 ( 88%)]  Loss: 2.826 (3.20)  Time: 0.802s, 1276.06/s  (0.793s, 1291.50/s)  LR: 2.965e-05  Data: 0.011 (0.013)
Train: 273 [1150/1251 ( 92%)]  Loss: 3.300 (3.20)  Time: 0.837s, 1223.36/s  (0.793s, 1291.33/s)  LR: 2.965e-05  Data: 0.011 (0.012)
Train: 273 [1200/1251 ( 96%)]  Loss: 3.592 (3.22)  Time: 0.781s, 1310.42/s  (0.792s, 1292.18/s)  LR: 2.965e-05  Data: 0.011 (0.012)
Train: 273 [1250/1251 (100%)]  Loss: 3.431 (3.23)  Time: 0.806s, 1270.94/s  (0.793s, 1291.89/s)  LR: 2.965e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.552 (1.552)  Loss:  0.7109 (0.7109)  Acc@1: 91.3086 (91.3086)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.172 (0.566)  Loss:  0.8086 (1.2320)  Acc@1: 85.8491 (78.0960)  Acc@5: 97.8774 (94.0060)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-272.pth.tar', 78.15200002929687)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-269.pth.tar', 78.1060000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-273.pth.tar', 78.09600005859375)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-267.pth.tar', 78.06000002685546)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-268.pth.tar', 78.0359999243164)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-271.pth.tar', 77.95399997802734)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-265.pth.tar', 77.90799998046874)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-270.pth.tar', 77.90200013427734)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-262.pth.tar', 77.89199987304687)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-261.pth.tar', 77.88399997558594)

Train: 274 [   0/1251 (  0%)]  Loss: 3.108 (3.11)  Time: 2.241s,  456.90/s  (2.241s,  456.90/s)  LR: 2.823e-05  Data: 1.506 (1.506)
Train: 274 [  50/1251 (  4%)]  Loss: 3.334 (3.22)  Time: 0.814s, 1258.40/s  (0.825s, 1241.88/s)  LR: 2.823e-05  Data: 0.012 (0.046)
Train: 274 [ 100/1251 (  8%)]  Loss: 3.302 (3.25)  Time: 0.779s, 1313.68/s  (0.808s, 1266.88/s)  LR: 2.823e-05  Data: 0.012 (0.029)
Train: 274 [ 150/1251 ( 12%)]  Loss: 3.233 (3.24)  Time: 0.818s, 1251.23/s  (0.804s, 1273.17/s)  LR: 2.823e-05  Data: 0.011 (0.023)
Train: 274 [ 200/1251 ( 16%)]  Loss: 2.890 (3.17)  Time: 0.788s, 1299.31/s  (0.806s, 1269.80/s)  LR: 2.823e-05  Data: 0.011 (0.020)
Train: 274 [ 250/1251 ( 20%)]  Loss: 3.380 (3.21)  Time: 0.777s, 1317.73/s  (0.802s, 1276.90/s)  LR: 2.823e-05  Data: 0.011 (0.018)
Train: 274 [ 300/1251 ( 24%)]  Loss: 3.459 (3.24)  Time: 0.781s, 1311.68/s  (0.799s, 1282.11/s)  LR: 2.823e-05  Data: 0.011 (0.017)
Train: 274 [ 350/1251 ( 28%)]  Loss: 2.758 (3.18)  Time: 0.779s, 1314.49/s  (0.798s, 1283.10/s)  LR: 2.823e-05  Data: 0.012 (0.016)
Train: 274 [ 400/1251 ( 32%)]  Loss: 3.127 (3.18)  Time: 0.813s, 1259.02/s  (0.799s, 1281.32/s)  LR: 2.823e-05  Data: 0.011 (0.016)
Train: 274 [ 450/1251 ( 36%)]  Loss: 3.194 (3.18)  Time: 0.787s, 1300.33/s  (0.798s, 1283.72/s)  LR: 2.823e-05  Data: 0.011 (0.015)
Train: 274 [ 500/1251 ( 40%)]  Loss: 3.150 (3.18)  Time: 0.818s, 1251.67/s  (0.798s, 1283.00/s)  LR: 2.823e-05  Data: 0.011 (0.015)
Train: 274 [ 550/1251 ( 44%)]  Loss: 3.843 (3.23)  Time: 0.779s, 1314.89/s  (0.799s, 1281.74/s)  LR: 2.823e-05  Data: 0.012 (0.014)
Train: 274 [ 600/1251 ( 48%)]  Loss: 3.068 (3.22)  Time: 0.804s, 1274.39/s  (0.797s, 1284.13/s)  LR: 2.823e-05  Data: 0.011 (0.014)
Train: 274 [ 650/1251 ( 52%)]  Loss: 3.473 (3.24)  Time: 0.830s, 1233.64/s  (0.798s, 1283.94/s)  LR: 2.823e-05  Data: 0.011 (0.014)
Train: 274 [ 700/1251 ( 56%)]  Loss: 3.491 (3.25)  Time: 0.777s, 1317.28/s  (0.797s, 1285.16/s)  LR: 2.823e-05  Data: 0.010 (0.014)
Train: 274 [ 750/1251 ( 60%)]  Loss: 3.317 (3.26)  Time: 0.812s, 1260.91/s  (0.796s, 1285.88/s)  LR: 2.823e-05  Data: 0.011 (0.014)
Train: 274 [ 800/1251 ( 64%)]  Loss: 3.182 (3.25)  Time: 0.775s, 1320.45/s  (0.797s, 1284.20/s)  LR: 2.823e-05  Data: 0.011 (0.013)
Train: 274 [ 850/1251 ( 68%)]  Loss: 3.510 (3.27)  Time: 0.813s, 1258.94/s  (0.798s, 1283.92/s)  LR: 2.823e-05  Data: 0.011 (0.013)
Train: 274 [ 900/1251 ( 72%)]  Loss: 3.150 (3.26)  Time: 0.781s, 1311.12/s  (0.797s, 1285.17/s)  LR: 2.823e-05  Data: 0.011 (0.013)
Train: 274 [ 950/1251 ( 76%)]  Loss: 3.605 (3.28)  Time: 0.780s, 1313.08/s  (0.796s, 1285.88/s)  LR: 2.823e-05  Data: 0.011 (0.013)
Train: 274 [1000/1251 ( 80%)]  Loss: 3.381 (3.28)  Time: 0.779s, 1314.63/s  (0.796s, 1286.54/s)  LR: 2.823e-05  Data: 0.011 (0.013)
Train: 274 [1050/1251 ( 84%)]  Loss: 3.480 (3.29)  Time: 0.779s, 1314.69/s  (0.796s, 1286.82/s)  LR: 2.823e-05  Data: 0.012 (0.013)
Train: 274 [1100/1251 ( 88%)]  Loss: 3.048 (3.28)  Time: 0.778s, 1316.63/s  (0.795s, 1287.59/s)  LR: 2.823e-05  Data: 0.011 (0.013)
Train: 274 [1150/1251 ( 92%)]  Loss: 2.836 (3.26)  Time: 0.776s, 1319.04/s  (0.795s, 1288.13/s)  LR: 2.823e-05  Data: 0.011 (0.013)
Train: 274 [1200/1251 ( 96%)]  Loss: 3.449 (3.27)  Time: 0.776s, 1318.79/s  (0.795s, 1288.81/s)  LR: 2.823e-05  Data: 0.011 (0.013)
Train: 274 [1250/1251 (100%)]  Loss: 3.097 (3.26)  Time: 0.770s, 1330.18/s  (0.794s, 1289.80/s)  LR: 2.823e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.505 (1.505)  Loss:  0.6606 (0.6606)  Acc@1: 91.2109 (91.2109)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.172 (0.563)  Loss:  0.7614 (1.1890)  Acc@1: 86.0849 (78.1600)  Acc@5: 97.8774 (94.1000)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-274.pth.tar', 78.16000016113281)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-272.pth.tar', 78.15200002929687)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-269.pth.tar', 78.1060000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-273.pth.tar', 78.09600005859375)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-267.pth.tar', 78.06000002685546)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-268.pth.tar', 78.0359999243164)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-271.pth.tar', 77.95399997802734)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-265.pth.tar', 77.90799998046874)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-270.pth.tar', 77.90200013427734)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-262.pth.tar', 77.89199987304687)

Train: 275 [   0/1251 (  0%)]  Loss: 2.935 (2.93)  Time: 2.278s,  449.56/s  (2.278s,  449.56/s)  LR: 2.687e-05  Data: 1.542 (1.542)
Train: 275 [  50/1251 (  4%)]  Loss: 3.026 (2.98)  Time: 0.778s, 1315.86/s  (0.820s, 1249.05/s)  LR: 2.687e-05  Data: 0.011 (0.043)
Train: 275 [ 100/1251 (  8%)]  Loss: 2.893 (2.95)  Time: 0.817s, 1253.28/s  (0.810s, 1264.95/s)  LR: 2.687e-05  Data: 0.011 (0.027)
Train: 275 [ 150/1251 ( 12%)]  Loss: 3.100 (2.99)  Time: 0.787s, 1301.01/s  (0.803s, 1275.34/s)  LR: 2.687e-05  Data: 0.011 (0.022)
Train: 275 [ 200/1251 ( 16%)]  Loss: 3.393 (3.07)  Time: 0.778s, 1316.31/s  (0.798s, 1283.58/s)  LR: 2.687e-05  Data: 0.011 (0.019)
Train: 275 [ 250/1251 ( 20%)]  Loss: 3.377 (3.12)  Time: 0.785s, 1303.92/s  (0.796s, 1287.19/s)  LR: 2.687e-05  Data: 0.010 (0.018)
Train: 275 [ 300/1251 ( 24%)]  Loss: 3.155 (3.13)  Time: 0.778s, 1316.57/s  (0.793s, 1290.59/s)  LR: 2.687e-05  Data: 0.011 (0.017)
Train: 275 [ 350/1251 ( 28%)]  Loss: 3.636 (3.19)  Time: 0.778s, 1316.71/s  (0.793s, 1291.33/s)  LR: 2.687e-05  Data: 0.012 (0.016)
Train: 275 [ 400/1251 ( 32%)]  Loss: 3.175 (3.19)  Time: 0.835s, 1225.61/s  (0.792s, 1293.67/s)  LR: 2.687e-05  Data: 0.010 (0.015)
Train: 275 [ 450/1251 ( 36%)]  Loss: 3.303 (3.20)  Time: 0.815s, 1256.74/s  (0.792s, 1292.71/s)  LR: 2.687e-05  Data: 0.011 (0.015)
Train: 275 [ 500/1251 ( 40%)]  Loss: 3.400 (3.22)  Time: 0.779s, 1314.59/s  (0.792s, 1292.28/s)  LR: 2.687e-05  Data: 0.011 (0.014)
Train: 275 [ 550/1251 ( 44%)]  Loss: 2.956 (3.20)  Time: 0.792s, 1292.97/s  (0.793s, 1291.90/s)  LR: 2.687e-05  Data: 0.012 (0.014)
Train: 275 [ 600/1251 ( 48%)]  Loss: 3.442 (3.21)  Time: 0.778s, 1315.73/s  (0.792s, 1293.33/s)  LR: 2.687e-05  Data: 0.012 (0.014)
Train: 275 [ 650/1251 ( 52%)]  Loss: 3.195 (3.21)  Time: 0.829s, 1235.19/s  (0.792s, 1292.70/s)  LR: 2.687e-05  Data: 0.011 (0.014)
Train: 275 [ 700/1251 ( 56%)]  Loss: 3.421 (3.23)  Time: 0.780s, 1313.55/s  (0.792s, 1293.58/s)  LR: 2.687e-05  Data: 0.012 (0.013)
Train: 275 [ 750/1251 ( 60%)]  Loss: 3.302 (3.23)  Time: 0.824s, 1242.43/s  (0.791s, 1294.54/s)  LR: 2.687e-05  Data: 0.011 (0.013)
Train: 275 [ 800/1251 ( 64%)]  Loss: 3.731 (3.26)  Time: 0.777s, 1317.87/s  (0.791s, 1294.15/s)  LR: 2.687e-05  Data: 0.011 (0.013)
Train: 275 [ 850/1251 ( 68%)]  Loss: 3.507 (3.27)  Time: 0.806s, 1271.03/s  (0.791s, 1294.37/s)  LR: 2.687e-05  Data: 0.010 (0.013)
Train: 275 [ 900/1251 ( 72%)]  Loss: 3.102 (3.27)  Time: 0.776s, 1320.31/s  (0.791s, 1294.49/s)  LR: 2.687e-05  Data: 0.010 (0.013)
Train: 275 [ 950/1251 ( 76%)]  Loss: 3.056 (3.26)  Time: 0.847s, 1208.77/s  (0.791s, 1293.98/s)  LR: 2.687e-05  Data: 0.011 (0.013)
Train: 275 [1000/1251 ( 80%)]  Loss: 3.162 (3.25)  Time: 0.778s, 1316.36/s  (0.792s, 1293.08/s)  LR: 2.687e-05  Data: 0.012 (0.013)
Train: 275 [1050/1251 ( 84%)]  Loss: 3.171 (3.25)  Time: 0.785s, 1304.43/s  (0.793s, 1291.83/s)  LR: 2.687e-05  Data: 0.011 (0.013)
Train: 275 [1100/1251 ( 88%)]  Loss: 3.581 (3.26)  Time: 0.781s, 1310.71/s  (0.792s, 1292.33/s)  LR: 2.687e-05  Data: 0.011 (0.013)
Train: 275 [1150/1251 ( 92%)]  Loss: 3.313 (3.26)  Time: 0.781s, 1311.76/s  (0.792s, 1293.15/s)  LR: 2.687e-05  Data: 0.011 (0.013)
Train: 275 [1200/1251 ( 96%)]  Loss: 3.589 (3.28)  Time: 0.781s, 1311.82/s  (0.792s, 1293.70/s)  LR: 2.687e-05  Data: 0.011 (0.012)
Train: 275 [1250/1251 (100%)]  Loss: 2.832 (3.26)  Time: 0.772s, 1325.94/s  (0.791s, 1294.18/s)  LR: 2.687e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.553 (1.553)  Loss:  0.6690 (0.6690)  Acc@1: 91.4062 (91.4062)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.172 (0.557)  Loss:  0.7417 (1.1723)  Acc@1: 86.9104 (78.1740)  Acc@5: 98.1132 (94.0100)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-275.pth.tar', 78.1740000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-274.pth.tar', 78.16000016113281)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-272.pth.tar', 78.15200002929687)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-269.pth.tar', 78.1060000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-273.pth.tar', 78.09600005859375)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-267.pth.tar', 78.06000002685546)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-268.pth.tar', 78.0359999243164)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-271.pth.tar', 77.95399997802734)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-265.pth.tar', 77.90799998046874)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-270.pth.tar', 77.90200013427734)

Train: 276 [   0/1251 (  0%)]  Loss: 3.462 (3.46)  Time: 2.237s,  457.66/s  (2.237s,  457.66/s)  LR: 2.555e-05  Data: 1.501 (1.501)
Train: 276 [  50/1251 (  4%)]  Loss: 3.455 (3.46)  Time: 0.783s, 1307.52/s  (0.828s, 1236.47/s)  LR: 2.555e-05  Data: 0.015 (0.048)
Train: 276 [ 100/1251 (  8%)]  Loss: 3.156 (3.36)  Time: 0.815s, 1256.82/s  (0.808s, 1266.70/s)  LR: 2.555e-05  Data: 0.012 (0.030)
Train: 276 [ 150/1251 ( 12%)]  Loss: 3.251 (3.33)  Time: 0.793s, 1292.07/s  (0.803s, 1274.93/s)  LR: 2.555e-05  Data: 0.011 (0.024)
Train: 276 [ 200/1251 ( 16%)]  Loss: 3.140 (3.29)  Time: 0.779s, 1314.55/s  (0.802s, 1276.07/s)  LR: 2.555e-05  Data: 0.010 (0.021)
Train: 276 [ 250/1251 ( 20%)]  Loss: 2.987 (3.24)  Time: 0.782s, 1309.88/s  (0.799s, 1282.25/s)  LR: 2.555e-05  Data: 0.011 (0.019)
Train: 276 [ 300/1251 ( 24%)]  Loss: 3.190 (3.23)  Time: 0.777s, 1318.53/s  (0.798s, 1283.01/s)  LR: 2.555e-05  Data: 0.012 (0.018)
Train: 276 [ 350/1251 ( 28%)]  Loss: 3.422 (3.26)  Time: 0.790s, 1296.15/s  (0.797s, 1285.26/s)  LR: 2.555e-05  Data: 0.012 (0.017)
Train: 276 [ 400/1251 ( 32%)]  Loss: 3.422 (3.28)  Time: 0.776s, 1318.89/s  (0.795s, 1287.68/s)  LR: 2.555e-05  Data: 0.011 (0.016)
Train: 276 [ 450/1251 ( 36%)]  Loss: 3.185 (3.27)  Time: 0.814s, 1257.91/s  (0.796s, 1285.91/s)  LR: 2.555e-05  Data: 0.011 (0.016)
Train: 276 [ 500/1251 ( 40%)]  Loss: 3.313 (3.27)  Time: 0.779s, 1313.97/s  (0.796s, 1287.06/s)  LR: 2.555e-05  Data: 0.011 (0.015)
Train: 276 [ 550/1251 ( 44%)]  Loss: 3.040 (3.25)  Time: 0.781s, 1311.84/s  (0.796s, 1285.82/s)  LR: 2.555e-05  Data: 0.011 (0.015)
Train: 276 [ 600/1251 ( 48%)]  Loss: 3.082 (3.24)  Time: 0.788s, 1300.08/s  (0.796s, 1286.92/s)  LR: 2.555e-05  Data: 0.011 (0.014)
Train: 276 [ 650/1251 ( 52%)]  Loss: 3.316 (3.24)  Time: 0.784s, 1306.25/s  (0.796s, 1286.85/s)  LR: 2.555e-05  Data: 0.011 (0.014)
Train: 276 [ 700/1251 ( 56%)]  Loss: 3.187 (3.24)  Time: 0.777s, 1318.44/s  (0.795s, 1287.92/s)  LR: 2.555e-05  Data: 0.011 (0.014)
Train: 276 [ 750/1251 ( 60%)]  Loss: 3.634 (3.27)  Time: 0.785s, 1303.88/s  (0.795s, 1287.43/s)  LR: 2.555e-05  Data: 0.012 (0.014)
Train: 276 [ 800/1251 ( 64%)]  Loss: 3.478 (3.28)  Time: 0.777s, 1317.50/s  (0.796s, 1286.79/s)  LR: 2.555e-05  Data: 0.011 (0.014)
Train: 276 [ 850/1251 ( 68%)]  Loss: 3.118 (3.27)  Time: 0.779s, 1314.52/s  (0.795s, 1287.43/s)  LR: 2.555e-05  Data: 0.011 (0.013)
Train: 276 [ 900/1251 ( 72%)]  Loss: 3.195 (3.26)  Time: 0.801s, 1278.77/s  (0.796s, 1286.88/s)  LR: 2.555e-05  Data: 0.011 (0.013)
Train: 276 [ 950/1251 ( 76%)]  Loss: 3.613 (3.28)  Time: 0.783s, 1308.32/s  (0.796s, 1286.96/s)  LR: 2.555e-05  Data: 0.011 (0.013)
Train: 276 [1000/1251 ( 80%)]  Loss: 3.334 (3.28)  Time: 0.836s, 1224.64/s  (0.796s, 1287.22/s)  LR: 2.555e-05  Data: 0.011 (0.013)
Train: 276 [1050/1251 ( 84%)]  Loss: 3.560 (3.30)  Time: 0.815s, 1256.79/s  (0.795s, 1287.65/s)  LR: 2.555e-05  Data: 0.011 (0.013)
Train: 276 [1100/1251 ( 88%)]  Loss: 3.451 (3.30)  Time: 0.778s, 1316.57/s  (0.796s, 1286.38/s)  LR: 2.555e-05  Data: 0.011 (0.013)
Train: 276 [1150/1251 ( 92%)]  Loss: 3.176 (3.30)  Time: 0.781s, 1310.81/s  (0.796s, 1286.85/s)  LR: 2.555e-05  Data: 0.011 (0.013)
Train: 276 [1200/1251 ( 96%)]  Loss: 3.291 (3.30)  Time: 0.778s, 1316.39/s  (0.795s, 1287.85/s)  LR: 2.555e-05  Data: 0.011 (0.013)
Train: 276 [1250/1251 (100%)]  Loss: 3.319 (3.30)  Time: 0.771s, 1328.69/s  (0.795s, 1288.62/s)  LR: 2.555e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.557 (1.557)  Loss:  0.6947 (0.6947)  Acc@1: 91.7969 (91.7969)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.172 (0.582)  Loss:  0.8063 (1.2085)  Acc@1: 86.4387 (78.2940)  Acc@5: 97.7594 (94.0760)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-276.pth.tar', 78.29400005615234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-275.pth.tar', 78.1740000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-274.pth.tar', 78.16000016113281)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-272.pth.tar', 78.15200002929687)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-269.pth.tar', 78.1060000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-273.pth.tar', 78.09600005859375)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-267.pth.tar', 78.06000002685546)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-268.pth.tar', 78.0359999243164)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-271.pth.tar', 77.95399997802734)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-265.pth.tar', 77.90799998046874)

Train: 277 [   0/1251 (  0%)]  Loss: 3.311 (3.31)  Time: 2.293s,  446.60/s  (2.293s,  446.60/s)  LR: 2.429e-05  Data: 1.559 (1.559)
Train: 277 [  50/1251 (  4%)]  Loss: 3.418 (3.36)  Time: 0.779s, 1314.67/s  (0.814s, 1257.92/s)  LR: 2.429e-05  Data: 0.011 (0.043)
Train: 277 [ 100/1251 (  8%)]  Loss: 3.342 (3.36)  Time: 0.817s, 1252.80/s  (0.803s, 1275.05/s)  LR: 2.429e-05  Data: 0.011 (0.027)
Train: 277 [ 150/1251 ( 12%)]  Loss: 3.185 (3.31)  Time: 0.776s, 1318.75/s  (0.798s, 1283.28/s)  LR: 2.429e-05  Data: 0.011 (0.022)
Train: 277 [ 200/1251 ( 16%)]  Loss: 3.120 (3.27)  Time: 0.778s, 1315.94/s  (0.795s, 1287.24/s)  LR: 2.429e-05  Data: 0.011 (0.019)
Train: 277 [ 250/1251 ( 20%)]  Loss: 3.325 (3.28)  Time: 0.780s, 1313.22/s  (0.794s, 1289.95/s)  LR: 2.429e-05  Data: 0.011 (0.018)
Train: 277 [ 300/1251 ( 24%)]  Loss: 3.196 (3.27)  Time: 0.827s, 1238.38/s  (0.792s, 1292.81/s)  LR: 2.429e-05  Data: 0.011 (0.017)
Train: 277 [ 350/1251 ( 28%)]  Loss: 3.480 (3.30)  Time: 0.830s, 1233.63/s  (0.792s, 1292.97/s)  LR: 2.429e-05  Data: 0.011 (0.016)
Train: 277 [ 400/1251 ( 32%)]  Loss: 2.992 (3.26)  Time: 0.819s, 1250.79/s  (0.792s, 1293.07/s)  LR: 2.429e-05  Data: 0.012 (0.015)
Train: 277 [ 450/1251 ( 36%)]  Loss: 2.983 (3.23)  Time: 0.779s, 1315.00/s  (0.792s, 1292.43/s)  LR: 2.429e-05  Data: 0.011 (0.015)
Train: 277 [ 500/1251 ( 40%)]  Loss: 3.289 (3.24)  Time: 0.782s, 1309.02/s  (0.791s, 1293.93/s)  LR: 2.429e-05  Data: 0.011 (0.014)
Train: 277 [ 550/1251 ( 44%)]  Loss: 3.177 (3.23)  Time: 0.778s, 1315.95/s  (0.792s, 1293.47/s)  LR: 2.429e-05  Data: 0.011 (0.014)
Train: 277 [ 600/1251 ( 48%)]  Loss: 3.071 (3.22)  Time: 0.791s, 1294.44/s  (0.792s, 1293.62/s)  LR: 2.429e-05  Data: 0.011 (0.014)
Train: 277 [ 650/1251 ( 52%)]  Loss: 3.310 (3.23)  Time: 0.850s, 1204.82/s  (0.792s, 1292.51/s)  LR: 2.429e-05  Data: 0.012 (0.014)
Train: 277 [ 700/1251 ( 56%)]  Loss: 3.399 (3.24)  Time: 0.793s, 1291.63/s  (0.792s, 1292.89/s)  LR: 2.429e-05  Data: 0.010 (0.013)
Train: 277 [ 750/1251 ( 60%)]  Loss: 3.251 (3.24)  Time: 0.781s, 1311.39/s  (0.791s, 1293.79/s)  LR: 2.429e-05  Data: 0.011 (0.013)
Train: 277 [ 800/1251 ( 64%)]  Loss: 3.439 (3.25)  Time: 0.780s, 1312.58/s  (0.792s, 1292.88/s)  LR: 2.429e-05  Data: 0.011 (0.013)
Train: 277 [ 850/1251 ( 68%)]  Loss: 2.963 (3.24)  Time: 0.775s, 1320.60/s  (0.792s, 1292.72/s)  LR: 2.429e-05  Data: 0.011 (0.013)
Train: 277 [ 900/1251 ( 72%)]  Loss: 3.119 (3.23)  Time: 0.787s, 1301.80/s  (0.792s, 1292.86/s)  LR: 2.429e-05  Data: 0.011 (0.013)
Train: 277 [ 950/1251 ( 76%)]  Loss: 3.255 (3.23)  Time: 0.779s, 1314.54/s  (0.792s, 1293.32/s)  LR: 2.429e-05  Data: 0.011 (0.013)
Train: 277 [1000/1251 ( 80%)]  Loss: 3.513 (3.24)  Time: 0.778s, 1315.61/s  (0.791s, 1294.15/s)  LR: 2.429e-05  Data: 0.011 (0.013)
Train: 277 [1050/1251 ( 84%)]  Loss: 3.606 (3.26)  Time: 0.778s, 1316.74/s  (0.791s, 1294.86/s)  LR: 2.429e-05  Data: 0.011 (0.013)
Train: 277 [1100/1251 ( 88%)]  Loss: 3.416 (3.27)  Time: 0.773s, 1324.03/s  (0.791s, 1295.07/s)  LR: 2.429e-05  Data: 0.010 (0.013)
Train: 277 [1150/1251 ( 92%)]  Loss: 3.536 (3.28)  Time: 0.829s, 1235.26/s  (0.791s, 1294.84/s)  LR: 2.429e-05  Data: 0.012 (0.012)
Train: 277 [1200/1251 ( 96%)]  Loss: 2.955 (3.27)  Time: 0.778s, 1315.72/s  (0.791s, 1294.52/s)  LR: 2.429e-05  Data: 0.011 (0.012)
Train: 277 [1250/1251 (100%)]  Loss: 3.760 (3.28)  Time: 0.799s, 1281.33/s  (0.791s, 1294.75/s)  LR: 2.429e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.579 (1.579)  Loss:  0.7431 (0.7431)  Acc@1: 92.2852 (92.2852)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.172 (0.559)  Loss:  0.8096 (1.2621)  Acc@1: 87.1462 (78.3060)  Acc@5: 98.1132 (94.0440)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-277.pth.tar', 78.30599997558593)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-276.pth.tar', 78.29400005615234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-275.pth.tar', 78.1740000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-274.pth.tar', 78.16000016113281)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-272.pth.tar', 78.15200002929687)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-269.pth.tar', 78.1060000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-273.pth.tar', 78.09600005859375)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-267.pth.tar', 78.06000002685546)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-268.pth.tar', 78.0359999243164)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-271.pth.tar', 77.95399997802734)

Train: 278 [   0/1251 (  0%)]  Loss: 3.373 (3.37)  Time: 2.333s,  439.01/s  (2.333s,  439.01/s)  LR: 2.308e-05  Data: 1.597 (1.597)
Train: 278 [  50/1251 (  4%)]  Loss: 3.377 (3.37)  Time: 0.825s, 1241.12/s  (0.839s, 1220.64/s)  LR: 2.308e-05  Data: 0.011 (0.046)
Train: 278 [ 100/1251 (  8%)]  Loss: 3.351 (3.37)  Time: 0.777s, 1317.77/s  (0.812s, 1261.13/s)  LR: 2.308e-05  Data: 0.011 (0.028)
Train: 278 [ 150/1251 ( 12%)]  Loss: 3.326 (3.36)  Time: 0.778s, 1316.46/s  (0.804s, 1273.54/s)  LR: 2.308e-05  Data: 0.011 (0.023)
Train: 278 [ 200/1251 ( 16%)]  Loss: 3.550 (3.40)  Time: 0.779s, 1315.20/s  (0.799s, 1282.11/s)  LR: 2.308e-05  Data: 0.010 (0.020)
Train: 278 [ 250/1251 ( 20%)]  Loss: 3.045 (3.34)  Time: 0.778s, 1316.17/s  (0.798s, 1283.05/s)  LR: 2.308e-05  Data: 0.011 (0.018)
Train: 278 [ 300/1251 ( 24%)]  Loss: 3.539 (3.37)  Time: 0.812s, 1261.65/s  (0.796s, 1286.09/s)  LR: 2.308e-05  Data: 0.011 (0.017)
Train: 278 [ 350/1251 ( 28%)]  Loss: 3.203 (3.35)  Time: 0.815s, 1255.67/s  (0.796s, 1285.93/s)  LR: 2.308e-05  Data: 0.011 (0.016)
Train: 278 [ 400/1251 ( 32%)]  Loss: 3.227 (3.33)  Time: 0.865s, 1183.91/s  (0.797s, 1285.25/s)  LR: 2.308e-05  Data: 0.011 (0.015)
Train: 278 [ 450/1251 ( 36%)]  Loss: 3.502 (3.35)  Time: 0.816s, 1254.54/s  (0.799s, 1281.75/s)  LR: 2.308e-05  Data: 0.011 (0.015)
Train: 278 [ 500/1251 ( 40%)]  Loss: 3.358 (3.35)  Time: 0.780s, 1312.46/s  (0.799s, 1281.96/s)  LR: 2.308e-05  Data: 0.011 (0.014)
Train: 278 [ 550/1251 ( 44%)]  Loss: 3.680 (3.38)  Time: 0.777s, 1317.50/s  (0.798s, 1283.00/s)  LR: 2.308e-05  Data: 0.011 (0.014)
Train: 278 [ 600/1251 ( 48%)]  Loss: 3.047 (3.35)  Time: 0.815s, 1256.77/s  (0.799s, 1282.17/s)  LR: 2.308e-05  Data: 0.012 (0.014)
Train: 278 [ 650/1251 ( 52%)]  Loss: 3.358 (3.35)  Time: 0.865s, 1183.70/s  (0.800s, 1280.64/s)  LR: 2.308e-05  Data: 0.011 (0.014)
Train: 278 [ 700/1251 ( 56%)]  Loss: 3.211 (3.34)  Time: 0.777s, 1317.05/s  (0.799s, 1282.10/s)  LR: 2.308e-05  Data: 0.011 (0.013)
Train: 278 [ 750/1251 ( 60%)]  Loss: 3.001 (3.32)  Time: 0.815s, 1256.96/s  (0.799s, 1281.81/s)  LR: 2.308e-05  Data: 0.011 (0.013)
Train: 278 [ 800/1251 ( 64%)]  Loss: 3.060 (3.31)  Time: 0.775s, 1320.75/s  (0.799s, 1281.31/s)  LR: 2.308e-05  Data: 0.010 (0.013)
Train: 278 [ 850/1251 ( 68%)]  Loss: 3.164 (3.30)  Time: 0.778s, 1316.15/s  (0.798s, 1282.44/s)  LR: 2.308e-05  Data: 0.011 (0.013)
Train: 278 [ 900/1251 ( 72%)]  Loss: 3.340 (3.30)  Time: 0.779s, 1313.91/s  (0.797s, 1284.03/s)  LR: 2.308e-05  Data: 0.011 (0.013)
Train: 278 [ 950/1251 ( 76%)]  Loss: 3.116 (3.29)  Time: 0.779s, 1314.18/s  (0.797s, 1284.59/s)  LR: 2.308e-05  Data: 0.011 (0.013)
Train: 278 [1000/1251 ( 80%)]  Loss: 2.979 (3.28)  Time: 0.781s, 1311.55/s  (0.797s, 1284.82/s)  LR: 2.308e-05  Data: 0.011 (0.013)
Train: 278 [1050/1251 ( 84%)]  Loss: 3.495 (3.29)  Time: 0.780s, 1312.96/s  (0.796s, 1285.67/s)  LR: 2.308e-05  Data: 0.011 (0.013)
Train: 278 [1100/1251 ( 88%)]  Loss: 3.252 (3.28)  Time: 0.779s, 1314.74/s  (0.796s, 1286.43/s)  LR: 2.308e-05  Data: 0.011 (0.013)
Train: 278 [1150/1251 ( 92%)]  Loss: 2.986 (3.27)  Time: 0.835s, 1225.97/s  (0.796s, 1286.32/s)  LR: 2.308e-05  Data: 0.012 (0.012)
Train: 278 [1200/1251 ( 96%)]  Loss: 3.052 (3.26)  Time: 0.778s, 1315.98/s  (0.796s, 1286.52/s)  LR: 2.308e-05  Data: 0.011 (0.012)
Train: 278 [1250/1251 (100%)]  Loss: 3.197 (3.26)  Time: 0.769s, 1331.89/s  (0.795s, 1287.48/s)  LR: 2.308e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.504 (1.504)  Loss:  0.6570 (0.6570)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.172 (0.562)  Loss:  0.7489 (1.1714)  Acc@1: 86.7925 (78.3000)  Acc@5: 98.1132 (94.0880)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-277.pth.tar', 78.30599997558593)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-278.pth.tar', 78.3000000805664)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-276.pth.tar', 78.29400005615234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-275.pth.tar', 78.1740000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-274.pth.tar', 78.16000016113281)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-272.pth.tar', 78.15200002929687)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-269.pth.tar', 78.1060000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-273.pth.tar', 78.09600005859375)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-267.pth.tar', 78.06000002685546)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-268.pth.tar', 78.0359999243164)

Train: 279 [   0/1251 (  0%)]  Loss: 3.360 (3.36)  Time: 2.258s,  453.42/s  (2.258s,  453.42/s)  LR: 2.192e-05  Data: 1.522 (1.522)
Train: 279 [  50/1251 (  4%)]  Loss: 3.278 (3.32)  Time: 0.857s, 1194.70/s  (0.822s, 1246.28/s)  LR: 2.192e-05  Data: 0.011 (0.046)
Train: 279 [ 100/1251 (  8%)]  Loss: 3.326 (3.32)  Time: 0.777s, 1317.19/s  (0.806s, 1269.71/s)  LR: 2.192e-05  Data: 0.011 (0.028)
Train: 279 [ 150/1251 ( 12%)]  Loss: 3.067 (3.26)  Time: 0.787s, 1301.00/s  (0.799s, 1281.72/s)  LR: 2.192e-05  Data: 0.011 (0.023)
Train: 279 [ 200/1251 ( 16%)]  Loss: 3.248 (3.26)  Time: 0.776s, 1318.89/s  (0.796s, 1285.71/s)  LR: 2.192e-05  Data: 0.011 (0.020)
Train: 279 [ 250/1251 ( 20%)]  Loss: 3.222 (3.25)  Time: 0.777s, 1317.67/s  (0.796s, 1287.14/s)  LR: 2.192e-05  Data: 0.011 (0.018)
Train: 279 [ 300/1251 ( 24%)]  Loss: 3.722 (3.32)  Time: 0.789s, 1298.48/s  (0.796s, 1287.08/s)  LR: 2.192e-05  Data: 0.011 (0.017)
Train: 279 [ 350/1251 ( 28%)]  Loss: 3.380 (3.33)  Time: 0.777s, 1317.20/s  (0.794s, 1289.61/s)  LR: 2.192e-05  Data: 0.011 (0.016)
Train: 279 [ 400/1251 ( 32%)]  Loss: 3.421 (3.34)  Time: 0.778s, 1316.28/s  (0.793s, 1290.94/s)  LR: 2.192e-05  Data: 0.012 (0.015)
Train: 279 [ 450/1251 ( 36%)]  Loss: 3.303 (3.33)  Time: 0.779s, 1314.57/s  (0.792s, 1293.11/s)  LR: 2.192e-05  Data: 0.011 (0.015)
Train: 279 [ 500/1251 ( 40%)]  Loss: 3.441 (3.34)  Time: 0.780s, 1312.41/s  (0.791s, 1294.29/s)  LR: 2.192e-05  Data: 0.011 (0.015)
Train: 279 [ 550/1251 ( 44%)]  Loss: 3.515 (3.36)  Time: 0.779s, 1314.45/s  (0.790s, 1295.58/s)  LR: 2.192e-05  Data: 0.011 (0.014)
Train: 279 [ 600/1251 ( 48%)]  Loss: 3.119 (3.34)  Time: 0.800s, 1279.70/s  (0.790s, 1296.78/s)  LR: 2.192e-05  Data: 0.012 (0.014)
Train: 279 [ 650/1251 ( 52%)]  Loss: 3.321 (3.34)  Time: 0.813s, 1259.17/s  (0.792s, 1293.71/s)  LR: 2.192e-05  Data: 0.011 (0.014)
Train: 279 [ 700/1251 ( 56%)]  Loss: 3.492 (3.35)  Time: 0.779s, 1313.78/s  (0.791s, 1293.85/s)  LR: 2.192e-05  Data: 0.012 (0.014)
Train: 279 [ 750/1251 ( 60%)]  Loss: 3.420 (3.35)  Time: 0.810s, 1264.81/s  (0.793s, 1292.01/s)  LR: 2.192e-05  Data: 0.011 (0.013)
Train: 279 [ 800/1251 ( 64%)]  Loss: 3.267 (3.35)  Time: 0.778s, 1316.95/s  (0.793s, 1290.57/s)  LR: 2.192e-05  Data: 0.010 (0.013)
Train: 279 [ 850/1251 ( 68%)]  Loss: 3.460 (3.35)  Time: 0.830s, 1233.59/s  (0.793s, 1291.23/s)  LR: 2.192e-05  Data: 0.011 (0.013)
Train: 279 [ 900/1251 ( 72%)]  Loss: 3.408 (3.36)  Time: 0.778s, 1317.03/s  (0.793s, 1290.90/s)  LR: 2.192e-05  Data: 0.010 (0.013)
Train: 279 [ 950/1251 ( 76%)]  Loss: 3.348 (3.36)  Time: 0.813s, 1259.57/s  (0.793s, 1290.71/s)  LR: 2.192e-05  Data: 0.011 (0.013)
Train: 279 [1000/1251 ( 80%)]  Loss: 3.359 (3.36)  Time: 0.778s, 1316.64/s  (0.793s, 1291.38/s)  LR: 2.192e-05  Data: 0.011 (0.013)
Train: 279 [1050/1251 ( 84%)]  Loss: 3.098 (3.34)  Time: 0.778s, 1315.69/s  (0.792s, 1292.23/s)  LR: 2.192e-05  Data: 0.010 (0.013)
Train: 279 [1100/1251 ( 88%)]  Loss: 2.969 (3.33)  Time: 0.778s, 1316.06/s  (0.792s, 1292.83/s)  LR: 2.192e-05  Data: 0.012 (0.013)
Train: 279 [1150/1251 ( 92%)]  Loss: 3.210 (3.32)  Time: 0.778s, 1316.93/s  (0.792s, 1293.06/s)  LR: 2.192e-05  Data: 0.012 (0.013)
Train: 279 [1200/1251 ( 96%)]  Loss: 3.372 (3.33)  Time: 0.817s, 1253.59/s  (0.792s, 1292.12/s)  LR: 2.192e-05  Data: 0.011 (0.012)
Train: 279 [1250/1251 (100%)]  Loss: 3.016 (3.31)  Time: 0.770s, 1329.09/s  (0.793s, 1291.76/s)  LR: 2.192e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.575 (1.575)  Loss:  0.6988 (0.6988)  Acc@1: 91.6992 (91.6992)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.172 (0.555)  Loss:  0.7895 (1.2115)  Acc@1: 87.3821 (78.2500)  Acc@5: 97.7594 (94.0300)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-277.pth.tar', 78.30599997558593)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-278.pth.tar', 78.3000000805664)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-276.pth.tar', 78.29400005615234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-279.pth.tar', 78.250000078125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-275.pth.tar', 78.1740000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-274.pth.tar', 78.16000016113281)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-272.pth.tar', 78.15200002929687)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-269.pth.tar', 78.1060000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-273.pth.tar', 78.09600005859375)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-267.pth.tar', 78.06000002685546)

Train: 280 [   0/1251 (  0%)]  Loss: 3.445 (3.45)  Time: 2.208s,  463.76/s  (2.208s,  463.76/s)  LR: 2.082e-05  Data: 1.472 (1.472)
Train: 280 [  50/1251 (  4%)]  Loss: 2.848 (3.15)  Time: 0.778s, 1315.49/s  (0.845s, 1211.30/s)  LR: 2.082e-05  Data: 0.011 (0.045)
Train: 280 [ 100/1251 (  8%)]  Loss: 3.122 (3.14)  Time: 0.776s, 1319.59/s  (0.817s, 1253.80/s)  LR: 2.082e-05  Data: 0.010 (0.028)
Train: 280 [ 150/1251 ( 12%)]  Loss: 3.084 (3.12)  Time: 0.813s, 1259.69/s  (0.812s, 1260.71/s)  LR: 2.082e-05  Data: 0.011 (0.022)
Train: 280 [ 200/1251 ( 16%)]  Loss: 3.385 (3.18)  Time: 0.776s, 1319.06/s  (0.808s, 1268.08/s)  LR: 2.082e-05  Data: 0.010 (0.020)
Train: 280 [ 250/1251 ( 20%)]  Loss: 3.191 (3.18)  Time: 0.778s, 1316.47/s  (0.803s, 1275.44/s)  LR: 2.082e-05  Data: 0.011 (0.018)
Train: 280 [ 300/1251 ( 24%)]  Loss: 3.312 (3.20)  Time: 0.814s, 1257.46/s  (0.801s, 1278.14/s)  LR: 2.082e-05  Data: 0.011 (0.017)
Train: 280 [ 350/1251 ( 28%)]  Loss: 3.638 (3.25)  Time: 0.779s, 1313.75/s  (0.801s, 1278.29/s)  LR: 2.082e-05  Data: 0.011 (0.016)
Train: 280 [ 400/1251 ( 32%)]  Loss: 3.521 (3.28)  Time: 0.814s, 1257.81/s  (0.801s, 1278.09/s)  LR: 2.082e-05  Data: 0.012 (0.015)
Train: 280 [ 450/1251 ( 36%)]  Loss: 3.742 (3.33)  Time: 0.779s, 1314.61/s  (0.802s, 1277.27/s)  LR: 2.082e-05  Data: 0.011 (0.015)
Train: 280 [ 500/1251 ( 40%)]  Loss: 3.456 (3.34)  Time: 0.816s, 1254.87/s  (0.801s, 1278.69/s)  LR: 2.082e-05  Data: 0.012 (0.014)
Train: 280 [ 550/1251 ( 44%)]  Loss: 3.375 (3.34)  Time: 0.779s, 1313.85/s  (0.800s, 1280.69/s)  LR: 2.082e-05  Data: 0.011 (0.014)
Train: 280 [ 600/1251 ( 48%)]  Loss: 3.337 (3.34)  Time: 0.781s, 1310.31/s  (0.799s, 1281.50/s)  LR: 2.082e-05  Data: 0.010 (0.014)
Train: 280 [ 650/1251 ( 52%)]  Loss: 3.552 (3.36)  Time: 0.775s, 1320.57/s  (0.798s, 1283.23/s)  LR: 2.082e-05  Data: 0.011 (0.014)
Train: 280 [ 700/1251 ( 56%)]  Loss: 2.686 (3.31)  Time: 0.778s, 1316.20/s  (0.798s, 1283.33/s)  LR: 2.082e-05  Data: 0.011 (0.013)
Train: 280 [ 750/1251 ( 60%)]  Loss: 3.297 (3.31)  Time: 0.776s, 1319.83/s  (0.798s, 1283.95/s)  LR: 2.082e-05  Data: 0.011 (0.013)
Train: 280 [ 800/1251 ( 64%)]  Loss: 3.356 (3.31)  Time: 0.778s, 1315.75/s  (0.797s, 1285.36/s)  LR: 2.082e-05  Data: 0.011 (0.013)
Train: 280 [ 850/1251 ( 68%)]  Loss: 3.203 (3.31)  Time: 0.778s, 1315.47/s  (0.796s, 1286.04/s)  LR: 2.082e-05  Data: 0.010 (0.013)
Train: 280 [ 900/1251 ( 72%)]  Loss: 3.539 (3.32)  Time: 0.780s, 1312.47/s  (0.795s, 1287.27/s)  LR: 2.082e-05  Data: 0.011 (0.013)
Train: 280 [ 950/1251 ( 76%)]  Loss: 3.240 (3.32)  Time: 0.794s, 1289.26/s  (0.795s, 1287.75/s)  LR: 2.082e-05  Data: 0.011 (0.013)
Train: 280 [1000/1251 ( 80%)]  Loss: 3.497 (3.33)  Time: 0.811s, 1262.54/s  (0.795s, 1287.64/s)  LR: 2.082e-05  Data: 0.010 (0.013)
Train: 280 [1050/1251 ( 84%)]  Loss: 2.762 (3.30)  Time: 0.780s, 1312.67/s  (0.795s, 1287.34/s)  LR: 2.082e-05  Data: 0.010 (0.013)
Train: 280 [1100/1251 ( 88%)]  Loss: 3.437 (3.31)  Time: 0.781s, 1310.62/s  (0.795s, 1287.88/s)  LR: 2.082e-05  Data: 0.011 (0.013)
Train: 280 [1150/1251 ( 92%)]  Loss: 3.464 (3.31)  Time: 0.778s, 1315.42/s  (0.795s, 1288.80/s)  LR: 2.082e-05  Data: 0.011 (0.012)
Train: 280 [1200/1251 ( 96%)]  Loss: 3.854 (3.33)  Time: 0.778s, 1316.88/s  (0.794s, 1289.30/s)  LR: 2.082e-05  Data: 0.010 (0.012)
Train: 280 [1250/1251 (100%)]  Loss: 3.119 (3.33)  Time: 0.771s, 1327.93/s  (0.794s, 1290.03/s)  LR: 2.082e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.535 (1.535)  Loss:  0.6995 (0.6995)  Acc@1: 91.5039 (91.5039)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.172 (0.558)  Loss:  0.7712 (1.2056)  Acc@1: 86.9104 (78.2740)  Acc@5: 97.9953 (94.1280)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-277.pth.tar', 78.30599997558593)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-278.pth.tar', 78.3000000805664)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-276.pth.tar', 78.29400005615234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-280.pth.tar', 78.27400013183593)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-279.pth.tar', 78.250000078125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-275.pth.tar', 78.1740000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-274.pth.tar', 78.16000016113281)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-272.pth.tar', 78.15200002929687)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-269.pth.tar', 78.1060000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-273.pth.tar', 78.09600005859375)

Train: 281 [   0/1251 (  0%)]  Loss: 3.347 (3.35)  Time: 2.219s,  461.47/s  (2.219s,  461.47/s)  LR: 1.977e-05  Data: 1.483 (1.483)
Train: 281 [  50/1251 (  4%)]  Loss: 3.369 (3.36)  Time: 0.812s, 1260.83/s  (0.836s, 1224.79/s)  LR: 1.977e-05  Data: 0.010 (0.048)
Train: 281 [ 100/1251 (  8%)]  Loss: 3.492 (3.40)  Time: 0.777s, 1318.16/s  (0.821s, 1246.80/s)  LR: 1.977e-05  Data: 0.011 (0.030)
Train: 281 [ 150/1251 ( 12%)]  Loss: 3.313 (3.38)  Time: 0.813s, 1259.57/s  (0.812s, 1260.69/s)  LR: 1.977e-05  Data: 0.011 (0.024)
Train: 281 [ 200/1251 ( 16%)]  Loss: 3.383 (3.38)  Time: 0.776s, 1319.04/s  (0.810s, 1264.71/s)  LR: 1.977e-05  Data: 0.011 (0.021)
Train: 281 [ 250/1251 ( 20%)]  Loss: 3.441 (3.39)  Time: 0.802s, 1276.64/s  (0.805s, 1271.75/s)  LR: 1.977e-05  Data: 0.010 (0.019)
Train: 281 [ 300/1251 ( 24%)]  Loss: 3.279 (3.37)  Time: 0.784s, 1306.38/s  (0.803s, 1274.98/s)  LR: 1.977e-05  Data: 0.011 (0.017)
Train: 281 [ 350/1251 ( 28%)]  Loss: 3.246 (3.36)  Time: 0.830s, 1233.62/s  (0.802s, 1276.44/s)  LR: 1.977e-05  Data: 0.011 (0.016)
Train: 281 [ 400/1251 ( 32%)]  Loss: 3.121 (3.33)  Time: 0.814s, 1258.70/s  (0.801s, 1278.69/s)  LR: 1.977e-05  Data: 0.011 (0.016)
Train: 281 [ 450/1251 ( 36%)]  Loss: 3.049 (3.30)  Time: 0.779s, 1313.99/s  (0.799s, 1281.18/s)  LR: 1.977e-05  Data: 0.011 (0.015)
Train: 281 [ 500/1251 ( 40%)]  Loss: 3.282 (3.30)  Time: 0.777s, 1318.37/s  (0.798s, 1283.83/s)  LR: 1.977e-05  Data: 0.011 (0.015)
Train: 281 [ 550/1251 ( 44%)]  Loss: 3.416 (3.31)  Time: 0.775s, 1321.31/s  (0.796s, 1286.26/s)  LR: 1.977e-05  Data: 0.011 (0.014)
Train: 281 [ 600/1251 ( 48%)]  Loss: 3.364 (3.32)  Time: 0.776s, 1318.91/s  (0.795s, 1288.40/s)  LR: 1.977e-05  Data: 0.011 (0.014)
Train: 281 [ 650/1251 ( 52%)]  Loss: 3.157 (3.30)  Time: 0.897s, 1140.95/s  (0.794s, 1289.44/s)  LR: 1.977e-05  Data: 0.011 (0.014)
Train: 281 [ 700/1251 ( 56%)]  Loss: 3.568 (3.32)  Time: 0.779s, 1314.86/s  (0.794s, 1290.08/s)  LR: 1.977e-05  Data: 0.011 (0.014)
Train: 281 [ 750/1251 ( 60%)]  Loss: 3.278 (3.32)  Time: 0.781s, 1310.70/s  (0.794s, 1289.01/s)  LR: 1.977e-05  Data: 0.011 (0.014)
Train: 281 [ 800/1251 ( 64%)]  Loss: 3.508 (3.33)  Time: 0.812s, 1261.48/s  (0.795s, 1288.54/s)  LR: 1.977e-05  Data: 0.011 (0.013)
Train: 281 [ 850/1251 ( 68%)]  Loss: 3.214 (3.32)  Time: 0.814s, 1258.30/s  (0.794s, 1289.48/s)  LR: 1.977e-05  Data: 0.011 (0.013)
Train: 281 [ 900/1251 ( 72%)]  Loss: 3.279 (3.32)  Time: 0.778s, 1316.65/s  (0.795s, 1288.86/s)  LR: 1.977e-05  Data: 0.011 (0.013)
Train: 281 [ 950/1251 ( 76%)]  Loss: 3.351 (3.32)  Time: 0.778s, 1315.52/s  (0.795s, 1288.45/s)  LR: 1.977e-05  Data: 0.011 (0.013)
Train: 281 [1000/1251 ( 80%)]  Loss: 3.189 (3.32)  Time: 0.780s, 1313.15/s  (0.795s, 1288.36/s)  LR: 1.977e-05  Data: 0.011 (0.013)
Train: 281 [1050/1251 ( 84%)]  Loss: 3.179 (3.31)  Time: 0.781s, 1311.83/s  (0.795s, 1288.40/s)  LR: 1.977e-05  Data: 0.011 (0.013)
Train: 281 [1100/1251 ( 88%)]  Loss: 3.128 (3.30)  Time: 0.813s, 1258.94/s  (0.794s, 1289.34/s)  LR: 1.977e-05  Data: 0.011 (0.013)
Train: 281 [1150/1251 ( 92%)]  Loss: 3.488 (3.31)  Time: 0.777s, 1317.73/s  (0.794s, 1289.21/s)  LR: 1.977e-05  Data: 0.011 (0.013)
Train: 281 [1200/1251 ( 96%)]  Loss: 3.118 (3.30)  Time: 0.779s, 1314.67/s  (0.794s, 1289.03/s)  LR: 1.977e-05  Data: 0.011 (0.013)
Train: 281 [1250/1251 (100%)]  Loss: 3.548 (3.31)  Time: 0.765s, 1338.11/s  (0.794s, 1289.14/s)  LR: 1.977e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.565 (1.565)  Loss:  0.7738 (0.7738)  Acc@1: 91.8945 (91.8945)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.172 (0.565)  Loss:  0.8356 (1.2798)  Acc@1: 87.3821 (78.1960)  Acc@5: 98.2311 (93.9940)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-277.pth.tar', 78.30599997558593)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-278.pth.tar', 78.3000000805664)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-276.pth.tar', 78.29400005615234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-280.pth.tar', 78.27400013183593)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-279.pth.tar', 78.250000078125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-281.pth.tar', 78.19599994873047)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-275.pth.tar', 78.1740000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-274.pth.tar', 78.16000016113281)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-272.pth.tar', 78.15200002929687)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-269.pth.tar', 78.1060000024414)

Train: 282 [   0/1251 (  0%)]  Loss: 2.961 (2.96)  Time: 2.265s,  452.13/s  (2.265s,  452.13/s)  LR: 1.877e-05  Data: 1.530 (1.530)
Train: 282 [  50/1251 (  4%)]  Loss: 3.234 (3.10)  Time: 0.798s, 1283.97/s  (0.825s, 1240.95/s)  LR: 1.877e-05  Data: 0.011 (0.048)
Train: 282 [ 100/1251 (  8%)]  Loss: 3.053 (3.08)  Time: 0.789s, 1298.58/s  (0.808s, 1267.52/s)  LR: 1.877e-05  Data: 0.011 (0.029)
Train: 282 [ 150/1251 ( 12%)]  Loss: 3.315 (3.14)  Time: 0.807s, 1268.59/s  (0.804s, 1274.40/s)  LR: 1.877e-05  Data: 0.010 (0.023)
Train: 282 [ 200/1251 ( 16%)]  Loss: 3.316 (3.18)  Time: 0.812s, 1261.82/s  (0.801s, 1278.28/s)  LR: 1.877e-05  Data: 0.011 (0.020)
Train: 282 [ 250/1251 ( 20%)]  Loss: 3.489 (3.23)  Time: 0.777s, 1318.08/s  (0.799s, 1281.50/s)  LR: 1.877e-05  Data: 0.011 (0.019)
Train: 282 [ 300/1251 ( 24%)]  Loss: 3.535 (3.27)  Time: 0.789s, 1297.37/s  (0.797s, 1284.30/s)  LR: 1.877e-05  Data: 0.010 (0.017)
Train: 282 [ 350/1251 ( 28%)]  Loss: 2.968 (3.23)  Time: 0.830s, 1233.08/s  (0.798s, 1283.60/s)  LR: 1.877e-05  Data: 0.011 (0.016)
Train: 282 [ 400/1251 ( 32%)]  Loss: 3.292 (3.24)  Time: 0.778s, 1316.52/s  (0.796s, 1285.86/s)  LR: 1.877e-05  Data: 0.011 (0.016)
Train: 282 [ 450/1251 ( 36%)]  Loss: 2.997 (3.22)  Time: 0.779s, 1314.95/s  (0.796s, 1287.13/s)  LR: 1.877e-05  Data: 0.011 (0.015)
Train: 282 [ 500/1251 ( 40%)]  Loss: 3.154 (3.21)  Time: 0.779s, 1314.36/s  (0.794s, 1289.14/s)  LR: 1.877e-05  Data: 0.010 (0.015)
Train: 282 [ 550/1251 ( 44%)]  Loss: 3.298 (3.22)  Time: 0.779s, 1314.05/s  (0.794s, 1290.00/s)  LR: 1.877e-05  Data: 0.011 (0.014)
Train: 282 [ 600/1251 ( 48%)]  Loss: 3.384 (3.23)  Time: 0.815s, 1256.50/s  (0.793s, 1291.50/s)  LR: 1.877e-05  Data: 0.011 (0.014)
Train: 282 [ 650/1251 ( 52%)]  Loss: 3.303 (3.24)  Time: 0.779s, 1314.04/s  (0.793s, 1291.24/s)  LR: 1.877e-05  Data: 0.011 (0.014)
Train: 282 [ 700/1251 ( 56%)]  Loss: 3.347 (3.24)  Time: 0.777s, 1317.91/s  (0.793s, 1291.51/s)  LR: 1.877e-05  Data: 0.011 (0.014)
Train: 282 [ 750/1251 ( 60%)]  Loss: 3.680 (3.27)  Time: 0.779s, 1313.88/s  (0.793s, 1291.81/s)  LR: 1.877e-05  Data: 0.011 (0.014)
Train: 282 [ 800/1251 ( 64%)]  Loss: 3.369 (3.28)  Time: 0.777s, 1317.14/s  (0.792s, 1292.70/s)  LR: 1.877e-05  Data: 0.011 (0.013)
Train: 282 [ 850/1251 ( 68%)]  Loss: 2.928 (3.26)  Time: 0.779s, 1315.06/s  (0.792s, 1293.11/s)  LR: 1.877e-05  Data: 0.011 (0.013)
Train: 282 [ 900/1251 ( 72%)]  Loss: 3.268 (3.26)  Time: 0.808s, 1267.28/s  (0.792s, 1293.48/s)  LR: 1.877e-05  Data: 0.012 (0.013)
Train: 282 [ 950/1251 ( 76%)]  Loss: 3.294 (3.26)  Time: 0.787s, 1300.80/s  (0.792s, 1292.83/s)  LR: 1.877e-05  Data: 0.011 (0.013)
Train: 282 [1000/1251 ( 80%)]  Loss: 3.436 (3.27)  Time: 0.784s, 1306.64/s  (0.792s, 1292.91/s)  LR: 1.877e-05  Data: 0.012 (0.013)
Train: 282 [1050/1251 ( 84%)]  Loss: 3.448 (3.28)  Time: 0.790s, 1295.81/s  (0.792s, 1292.66/s)  LR: 1.877e-05  Data: 0.011 (0.013)
Train: 282 [1100/1251 ( 88%)]  Loss: 3.314 (3.28)  Time: 0.777s, 1318.39/s  (0.792s, 1292.98/s)  LR: 1.877e-05  Data: 0.011 (0.013)
Train: 282 [1150/1251 ( 92%)]  Loss: 3.298 (3.28)  Time: 0.776s, 1320.03/s  (0.793s, 1291.84/s)  LR: 1.877e-05  Data: 0.011 (0.013)
Train: 282 [1200/1251 ( 96%)]  Loss: 2.916 (3.26)  Time: 0.778s, 1316.26/s  (0.793s, 1291.67/s)  LR: 1.877e-05  Data: 0.012 (0.013)
Train: 282 [1250/1251 (100%)]  Loss: 3.195 (3.26)  Time: 0.771s, 1327.85/s  (0.793s, 1291.81/s)  LR: 1.877e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.511 (1.511)  Loss:  0.6701 (0.6701)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.172 (0.565)  Loss:  0.7629 (1.1955)  Acc@1: 87.6179 (78.2840)  Acc@5: 97.9953 (94.0640)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-277.pth.tar', 78.30599997558593)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-278.pth.tar', 78.3000000805664)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-276.pth.tar', 78.29400005615234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-282.pth.tar', 78.28400018066407)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-280.pth.tar', 78.27400013183593)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-279.pth.tar', 78.250000078125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-281.pth.tar', 78.19599994873047)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-275.pth.tar', 78.1740000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-274.pth.tar', 78.16000016113281)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-272.pth.tar', 78.15200002929687)

Train: 283 [   0/1251 (  0%)]  Loss: 3.199 (3.20)  Time: 2.349s,  436.01/s  (2.349s,  436.01/s)  LR: 1.782e-05  Data: 1.613 (1.613)
Train: 283 [  50/1251 (  4%)]  Loss: 3.579 (3.39)  Time: 0.778s, 1315.58/s  (0.829s, 1235.14/s)  LR: 1.782e-05  Data: 0.011 (0.047)
Train: 283 [ 100/1251 (  8%)]  Loss: 3.328 (3.37)  Time: 0.854s, 1199.03/s  (0.812s, 1261.43/s)  LR: 1.782e-05  Data: 0.012 (0.029)
Train: 283 [ 150/1251 ( 12%)]  Loss: 3.110 (3.30)  Time: 0.776s, 1319.86/s  (0.804s, 1273.14/s)  LR: 1.782e-05  Data: 0.010 (0.023)
Train: 283 [ 200/1251 ( 16%)]  Loss: 2.888 (3.22)  Time: 0.780s, 1312.07/s  (0.799s, 1281.05/s)  LR: 1.782e-05  Data: 0.011 (0.020)
Train: 283 [ 250/1251 ( 20%)]  Loss: 2.970 (3.18)  Time: 0.779s, 1313.95/s  (0.799s, 1281.26/s)  LR: 1.782e-05  Data: 0.011 (0.018)
Train: 283 [ 300/1251 ( 24%)]  Loss: 3.677 (3.25)  Time: 0.807s, 1268.49/s  (0.797s, 1284.52/s)  LR: 1.782e-05  Data: 0.011 (0.017)
Train: 283 [ 350/1251 ( 28%)]  Loss: 3.408 (3.27)  Time: 0.775s, 1321.03/s  (0.796s, 1287.04/s)  LR: 1.782e-05  Data: 0.010 (0.016)
Train: 283 [ 400/1251 ( 32%)]  Loss: 3.244 (3.27)  Time: 0.780s, 1313.08/s  (0.795s, 1288.33/s)  LR: 1.782e-05  Data: 0.011 (0.016)
Train: 283 [ 450/1251 ( 36%)]  Loss: 3.611 (3.30)  Time: 0.775s, 1321.20/s  (0.795s, 1288.31/s)  LR: 1.782e-05  Data: 0.011 (0.015)
Train: 283 [ 500/1251 ( 40%)]  Loss: 3.438 (3.31)  Time: 0.793s, 1292.05/s  (0.794s, 1289.99/s)  LR: 1.782e-05  Data: 0.011 (0.015)
Train: 283 [ 550/1251 ( 44%)]  Loss: 3.380 (3.32)  Time: 0.831s, 1232.31/s  (0.793s, 1291.18/s)  LR: 1.782e-05  Data: 0.011 (0.014)
Train: 283 [ 600/1251 ( 48%)]  Loss: 3.526 (3.34)  Time: 0.777s, 1317.77/s  (0.792s, 1292.18/s)  LR: 1.782e-05  Data: 0.011 (0.014)
Train: 283 [ 650/1251 ( 52%)]  Loss: 2.874 (3.30)  Time: 0.778s, 1316.15/s  (0.792s, 1292.98/s)  LR: 1.782e-05  Data: 0.011 (0.014)
Train: 283 [ 700/1251 ( 56%)]  Loss: 3.079 (3.29)  Time: 0.779s, 1314.95/s  (0.791s, 1294.21/s)  LR: 1.782e-05  Data: 0.011 (0.014)
Train: 283 [ 750/1251 ( 60%)]  Loss: 3.013 (3.27)  Time: 0.811s, 1263.28/s  (0.791s, 1293.89/s)  LR: 1.782e-05  Data: 0.011 (0.013)
Train: 283 [ 800/1251 ( 64%)]  Loss: 3.171 (3.26)  Time: 0.778s, 1316.14/s  (0.792s, 1293.45/s)  LR: 1.782e-05  Data: 0.011 (0.013)
Train: 283 [ 850/1251 ( 68%)]  Loss: 3.394 (3.27)  Time: 0.786s, 1302.49/s  (0.791s, 1294.17/s)  LR: 1.782e-05  Data: 0.011 (0.013)
Train: 283 [ 900/1251 ( 72%)]  Loss: 3.486 (3.28)  Time: 0.826s, 1240.23/s  (0.791s, 1294.36/s)  LR: 1.782e-05  Data: 0.012 (0.013)
Train: 283 [ 950/1251 ( 76%)]  Loss: 3.114 (3.27)  Time: 0.779s, 1314.35/s  (0.791s, 1294.41/s)  LR: 1.782e-05  Data: 0.011 (0.013)
Train: 283 [1000/1251 ( 80%)]  Loss: 3.537 (3.29)  Time: 0.776s, 1319.45/s  (0.791s, 1295.18/s)  LR: 1.782e-05  Data: 0.011 (0.013)
Train: 283 [1050/1251 ( 84%)]  Loss: 3.223 (3.28)  Time: 0.779s, 1313.86/s  (0.791s, 1295.25/s)  LR: 1.782e-05  Data: 0.011 (0.013)
Train: 283 [1100/1251 ( 88%)]  Loss: 3.353 (3.29)  Time: 0.828s, 1236.68/s  (0.791s, 1295.33/s)  LR: 1.782e-05  Data: 0.011 (0.013)
Train: 283 [1150/1251 ( 92%)]  Loss: 3.416 (3.29)  Time: 0.782s, 1309.93/s  (0.790s, 1295.69/s)  LR: 1.782e-05  Data: 0.012 (0.013)
Train: 283 [1200/1251 ( 96%)]  Loss: 2.971 (3.28)  Time: 0.779s, 1313.77/s  (0.790s, 1296.40/s)  LR: 1.782e-05  Data: 0.011 (0.013)
Train: 283 [1250/1251 (100%)]  Loss: 3.234 (3.28)  Time: 0.801s, 1277.82/s  (0.790s, 1295.49/s)  LR: 1.782e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.553 (1.553)  Loss:  0.7519 (0.7519)  Acc@1: 91.6992 (91.6992)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.172 (0.568)  Loss:  0.8191 (1.2502)  Acc@1: 86.9104 (78.2540)  Acc@5: 97.9953 (94.0280)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-277.pth.tar', 78.30599997558593)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-278.pth.tar', 78.3000000805664)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-276.pth.tar', 78.29400005615234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-282.pth.tar', 78.28400018066407)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-280.pth.tar', 78.27400013183593)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-283.pth.tar', 78.2540000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-279.pth.tar', 78.250000078125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-281.pth.tar', 78.19599994873047)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-275.pth.tar', 78.1740000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-274.pth.tar', 78.16000016113281)

Train: 284 [   0/1251 (  0%)]  Loss: 3.208 (3.21)  Time: 2.219s,  461.46/s  (2.219s,  461.46/s)  LR: 1.693e-05  Data: 1.485 (1.485)
Train: 284 [  50/1251 (  4%)]  Loss: 3.190 (3.20)  Time: 0.818s, 1251.42/s  (0.834s, 1227.46/s)  LR: 1.693e-05  Data: 0.012 (0.047)
Train: 284 [ 100/1251 (  8%)]  Loss: 3.188 (3.20)  Time: 0.779s, 1314.37/s  (0.819s, 1250.34/s)  LR: 1.693e-05  Data: 0.012 (0.029)
Train: 284 [ 150/1251 ( 12%)]  Loss: 3.584 (3.29)  Time: 0.779s, 1314.31/s  (0.807s, 1268.15/s)  LR: 1.693e-05  Data: 0.011 (0.023)
Train: 284 [ 200/1251 ( 16%)]  Loss: 3.225 (3.28)  Time: 0.821s, 1247.93/s  (0.803s, 1274.64/s)  LR: 1.693e-05  Data: 0.011 (0.020)
Train: 284 [ 250/1251 ( 20%)]  Loss: 3.771 (3.36)  Time: 0.816s, 1254.72/s  (0.802s, 1276.92/s)  LR: 1.693e-05  Data: 0.012 (0.018)
Train: 284 [ 300/1251 ( 24%)]  Loss: 3.399 (3.37)  Time: 0.776s, 1319.45/s  (0.800s, 1279.57/s)  LR: 1.693e-05  Data: 0.011 (0.017)
Train: 284 [ 350/1251 ( 28%)]  Loss: 3.269 (3.35)  Time: 0.784s, 1306.30/s  (0.799s, 1281.79/s)  LR: 1.693e-05  Data: 0.011 (0.016)
Train: 284 [ 400/1251 ( 32%)]  Loss: 3.099 (3.33)  Time: 0.814s, 1258.22/s  (0.800s, 1280.25/s)  LR: 1.693e-05  Data: 0.011 (0.016)
Train: 284 [ 450/1251 ( 36%)]  Loss: 3.315 (3.32)  Time: 0.779s, 1313.92/s  (0.799s, 1282.40/s)  LR: 1.693e-05  Data: 0.011 (0.015)
Train: 284 [ 500/1251 ( 40%)]  Loss: 3.020 (3.30)  Time: 0.778s, 1316.05/s  (0.798s, 1283.19/s)  LR: 1.693e-05  Data: 0.010 (0.015)
Train: 284 [ 550/1251 ( 44%)]  Loss: 3.090 (3.28)  Time: 0.816s, 1255.57/s  (0.797s, 1284.86/s)  LR: 1.693e-05  Data: 0.010 (0.014)
Train: 284 [ 600/1251 ( 48%)]  Loss: 3.295 (3.28)  Time: 0.814s, 1257.53/s  (0.798s, 1282.88/s)  LR: 1.693e-05  Data: 0.011 (0.014)
Train: 284 [ 650/1251 ( 52%)]  Loss: 3.572 (3.30)  Time: 0.777s, 1317.66/s  (0.799s, 1281.70/s)  LR: 1.693e-05  Data: 0.011 (0.014)
Train: 284 [ 700/1251 ( 56%)]  Loss: 3.634 (3.32)  Time: 0.779s, 1314.75/s  (0.799s, 1282.40/s)  LR: 1.693e-05  Data: 0.011 (0.014)
Train: 284 [ 750/1251 ( 60%)]  Loss: 3.101 (3.31)  Time: 0.778s, 1316.67/s  (0.798s, 1283.84/s)  LR: 1.693e-05  Data: 0.011 (0.014)
Train: 284 [ 800/1251 ( 64%)]  Loss: 3.320 (3.31)  Time: 0.811s, 1262.75/s  (0.797s, 1284.67/s)  LR: 1.693e-05  Data: 0.011 (0.013)
Train: 284 [ 850/1251 ( 68%)]  Loss: 3.198 (3.30)  Time: 0.780s, 1313.02/s  (0.797s, 1285.29/s)  LR: 1.693e-05  Data: 0.011 (0.013)
Train: 284 [ 900/1251 ( 72%)]  Loss: 3.300 (3.30)  Time: 0.779s, 1314.62/s  (0.796s, 1285.86/s)  LR: 1.693e-05  Data: 0.011 (0.013)
Train: 284 [ 950/1251 ( 76%)]  Loss: 3.418 (3.31)  Time: 0.780s, 1313.29/s  (0.796s, 1286.95/s)  LR: 1.693e-05  Data: 0.010 (0.013)
Train: 284 [1000/1251 ( 80%)]  Loss: 3.161 (3.30)  Time: 0.783s, 1307.56/s  (0.795s, 1287.95/s)  LR: 1.693e-05  Data: 0.011 (0.013)
Train: 284 [1050/1251 ( 84%)]  Loss: 3.022 (3.29)  Time: 0.776s, 1319.00/s  (0.795s, 1288.10/s)  LR: 1.693e-05  Data: 0.010 (0.013)
Train: 284 [1100/1251 ( 88%)]  Loss: 3.427 (3.30)  Time: 0.819s, 1249.68/s  (0.795s, 1288.56/s)  LR: 1.693e-05  Data: 0.012 (0.013)
Train: 284 [1150/1251 ( 92%)]  Loss: 3.288 (3.30)  Time: 0.779s, 1314.43/s  (0.795s, 1287.94/s)  LR: 1.693e-05  Data: 0.014 (0.013)
Train: 284 [1200/1251 ( 96%)]  Loss: 3.497 (3.30)  Time: 0.779s, 1313.77/s  (0.795s, 1288.83/s)  LR: 1.693e-05  Data: 0.011 (0.013)
Train: 284 [1250/1251 (100%)]  Loss: 3.260 (3.30)  Time: 0.764s, 1340.74/s  (0.794s, 1288.99/s)  LR: 1.693e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.538 (1.538)  Loss:  0.6920 (0.6920)  Acc@1: 91.6992 (91.6992)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.172 (0.559)  Loss:  0.7625 (1.2016)  Acc@1: 87.0283 (78.3120)  Acc@5: 98.4670 (94.1060)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-284.pth.tar', 78.31199992431641)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-277.pth.tar', 78.30599997558593)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-278.pth.tar', 78.3000000805664)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-276.pth.tar', 78.29400005615234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-282.pth.tar', 78.28400018066407)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-280.pth.tar', 78.27400013183593)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-283.pth.tar', 78.2540000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-279.pth.tar', 78.250000078125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-281.pth.tar', 78.19599994873047)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-275.pth.tar', 78.1740000024414)

Train: 285 [   0/1251 (  0%)]  Loss: 3.333 (3.33)  Time: 2.196s,  466.41/s  (2.196s,  466.41/s)  LR: 1.609e-05  Data: 1.459 (1.459)
Train: 285 [  50/1251 (  4%)]  Loss: 3.263 (3.30)  Time: 0.790s, 1296.37/s  (0.828s, 1237.09/s)  LR: 1.609e-05  Data: 0.011 (0.045)
Train: 285 [ 100/1251 (  8%)]  Loss: 3.216 (3.27)  Time: 0.813s, 1259.91/s  (0.812s, 1261.26/s)  LR: 1.609e-05  Data: 0.011 (0.028)
Train: 285 [ 150/1251 ( 12%)]  Loss: 3.282 (3.27)  Time: 0.781s, 1311.49/s  (0.807s, 1268.63/s)  LR: 1.609e-05  Data: 0.011 (0.022)
Train: 285 [ 200/1251 ( 16%)]  Loss: 3.094 (3.24)  Time: 0.781s, 1311.64/s  (0.802s, 1277.13/s)  LR: 1.609e-05  Data: 0.011 (0.020)
Train: 285 [ 250/1251 ( 20%)]  Loss: 3.149 (3.22)  Time: 0.789s, 1297.14/s  (0.800s, 1279.86/s)  LR: 1.609e-05  Data: 0.011 (0.018)
Train: 285 [ 300/1251 ( 24%)]  Loss: 3.473 (3.26)  Time: 0.777s, 1318.37/s  (0.798s, 1282.59/s)  LR: 1.609e-05  Data: 0.011 (0.017)
Train: 285 [ 350/1251 ( 28%)]  Loss: 3.139 (3.24)  Time: 0.779s, 1315.06/s  (0.798s, 1283.03/s)  LR: 1.609e-05  Data: 0.011 (0.016)
Train: 285 [ 400/1251 ( 32%)]  Loss: 3.603 (3.28)  Time: 0.779s, 1313.92/s  (0.797s, 1284.70/s)  LR: 1.609e-05  Data: 0.010 (0.015)
Train: 285 [ 450/1251 ( 36%)]  Loss: 3.410 (3.30)  Time: 0.778s, 1315.42/s  (0.798s, 1282.50/s)  LR: 1.609e-05  Data: 0.011 (0.015)
Train: 285 [ 500/1251 ( 40%)]  Loss: 3.414 (3.31)  Time: 0.822s, 1245.48/s  (0.799s, 1282.15/s)  LR: 1.609e-05  Data: 0.010 (0.015)
Train: 285 [ 550/1251 ( 44%)]  Loss: 3.171 (3.30)  Time: 0.785s, 1304.49/s  (0.799s, 1282.39/s)  LR: 1.609e-05  Data: 0.011 (0.014)
Train: 285 [ 600/1251 ( 48%)]  Loss: 3.263 (3.29)  Time: 0.792s, 1292.35/s  (0.798s, 1283.77/s)  LR: 1.609e-05  Data: 0.012 (0.014)
Train: 285 [ 650/1251 ( 52%)]  Loss: 3.191 (3.29)  Time: 0.833s, 1228.76/s  (0.798s, 1283.49/s)  LR: 1.609e-05  Data: 0.011 (0.014)
Train: 285 [ 700/1251 ( 56%)]  Loss: 3.238 (3.28)  Time: 0.793s, 1291.75/s  (0.797s, 1284.81/s)  LR: 1.609e-05  Data: 0.011 (0.014)
Train: 285 [ 750/1251 ( 60%)]  Loss: 3.315 (3.28)  Time: 0.810s, 1263.61/s  (0.797s, 1284.07/s)  LR: 1.609e-05  Data: 0.011 (0.013)
Train: 285 [ 800/1251 ( 64%)]  Loss: 3.348 (3.29)  Time: 0.778s, 1315.49/s  (0.797s, 1284.13/s)  LR: 1.609e-05  Data: 0.011 (0.013)
Train: 285 [ 850/1251 ( 68%)]  Loss: 3.266 (3.29)  Time: 0.779s, 1314.45/s  (0.797s, 1285.50/s)  LR: 1.609e-05  Data: 0.011 (0.013)
Train: 285 [ 900/1251 ( 72%)]  Loss: 3.101 (3.28)  Time: 0.778s, 1315.46/s  (0.796s, 1286.98/s)  LR: 1.609e-05  Data: 0.010 (0.013)
Train: 285 [ 950/1251 ( 76%)]  Loss: 3.307 (3.28)  Time: 0.811s, 1262.25/s  (0.795s, 1287.77/s)  LR: 1.609e-05  Data: 0.011 (0.013)
Train: 285 [1000/1251 ( 80%)]  Loss: 2.968 (3.26)  Time: 0.780s, 1313.61/s  (0.795s, 1288.45/s)  LR: 1.609e-05  Data: 0.011 (0.013)
Train: 285 [1050/1251 ( 84%)]  Loss: 3.375 (3.27)  Time: 0.781s, 1310.82/s  (0.794s, 1289.37/s)  LR: 1.609e-05  Data: 0.012 (0.013)
Train: 285 [1100/1251 ( 88%)]  Loss: 3.079 (3.26)  Time: 0.779s, 1315.29/s  (0.794s, 1289.58/s)  LR: 1.609e-05  Data: 0.011 (0.013)
Train: 285 [1150/1251 ( 92%)]  Loss: 3.138 (3.26)  Time: 0.778s, 1316.48/s  (0.794s, 1289.92/s)  LR: 1.609e-05  Data: 0.011 (0.013)
Train: 285 [1200/1251 ( 96%)]  Loss: 3.629 (3.27)  Time: 0.820s, 1248.31/s  (0.794s, 1290.19/s)  LR: 1.609e-05  Data: 0.011 (0.012)
Train: 285 [1250/1251 (100%)]  Loss: 3.409 (3.28)  Time: 0.768s, 1332.68/s  (0.793s, 1290.80/s)  LR: 1.609e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.517 (1.517)  Loss:  0.7097 (0.7097)  Acc@1: 91.8945 (91.8945)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.172 (0.556)  Loss:  0.7730 (1.2205)  Acc@1: 87.0283 (78.3760)  Acc@5: 98.2311 (94.1040)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-285.pth.tar', 78.3759999243164)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-284.pth.tar', 78.31199992431641)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-277.pth.tar', 78.30599997558593)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-278.pth.tar', 78.3000000805664)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-276.pth.tar', 78.29400005615234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-282.pth.tar', 78.28400018066407)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-280.pth.tar', 78.27400013183593)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-283.pth.tar', 78.2540000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-279.pth.tar', 78.250000078125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-281.pth.tar', 78.19599994873047)

Train: 286 [   0/1251 (  0%)]  Loss: 2.981 (2.98)  Time: 2.239s,  457.38/s  (2.239s,  457.38/s)  LR: 1.531e-05  Data: 1.497 (1.497)
Train: 286 [  50/1251 (  4%)]  Loss: 3.335 (3.16)  Time: 0.779s, 1315.26/s  (0.819s, 1249.67/s)  LR: 1.531e-05  Data: 0.011 (0.042)
Train: 286 [ 100/1251 (  8%)]  Loss: 3.153 (3.16)  Time: 0.790s, 1295.77/s  (0.801s, 1278.10/s)  LR: 1.531e-05  Data: 0.011 (0.027)
Train: 286 [ 150/1251 ( 12%)]  Loss: 2.984 (3.11)  Time: 0.817s, 1253.56/s  (0.800s, 1279.60/s)  LR: 1.531e-05  Data: 0.012 (0.022)
Train: 286 [ 200/1251 ( 16%)]  Loss: 3.194 (3.13)  Time: 0.779s, 1313.67/s  (0.798s, 1283.29/s)  LR: 1.531e-05  Data: 0.011 (0.019)
Train: 286 [ 250/1251 ( 20%)]  Loss: 3.185 (3.14)  Time: 0.813s, 1260.07/s  (0.800s, 1280.28/s)  LR: 1.531e-05  Data: 0.010 (0.017)
Train: 286 [ 300/1251 ( 24%)]  Loss: 3.064 (3.13)  Time: 0.819s, 1249.91/s  (0.797s, 1284.71/s)  LR: 1.531e-05  Data: 0.011 (0.016)
Train: 286 [ 350/1251 ( 28%)]  Loss: 3.545 (3.18)  Time: 0.777s, 1318.07/s  (0.797s, 1284.99/s)  LR: 1.531e-05  Data: 0.011 (0.016)
Train: 286 [ 400/1251 ( 32%)]  Loss: 3.218 (3.18)  Time: 0.776s, 1318.78/s  (0.798s, 1282.76/s)  LR: 1.531e-05  Data: 0.011 (0.015)
Train: 286 [ 450/1251 ( 36%)]  Loss: 3.366 (3.20)  Time: 0.778s, 1316.59/s  (0.797s, 1285.62/s)  LR: 1.531e-05  Data: 0.011 (0.015)
Train: 286 [ 500/1251 ( 40%)]  Loss: 3.180 (3.20)  Time: 0.776s, 1320.20/s  (0.796s, 1286.00/s)  LR: 1.531e-05  Data: 0.012 (0.014)
Train: 286 [ 550/1251 ( 44%)]  Loss: 3.177 (3.20)  Time: 0.778s, 1316.83/s  (0.795s, 1287.81/s)  LR: 1.531e-05  Data: 0.011 (0.014)
Train: 286 [ 600/1251 ( 48%)]  Loss: 3.156 (3.20)  Time: 0.777s, 1317.59/s  (0.794s, 1289.86/s)  LR: 1.531e-05  Data: 0.011 (0.014)
Train: 286 [ 650/1251 ( 52%)]  Loss: 3.260 (3.20)  Time: 0.781s, 1311.80/s  (0.793s, 1291.68/s)  LR: 1.531e-05  Data: 0.012 (0.013)
Train: 286 [ 700/1251 ( 56%)]  Loss: 3.153 (3.20)  Time: 0.842s, 1216.56/s  (0.793s, 1290.78/s)  LR: 1.531e-05  Data: 0.011 (0.013)
Train: 286 [ 750/1251 ( 60%)]  Loss: 3.272 (3.20)  Time: 0.776s, 1319.18/s  (0.793s, 1291.79/s)  LR: 1.531e-05  Data: 0.012 (0.013)
Train: 286 [ 800/1251 ( 64%)]  Loss: 3.465 (3.22)  Time: 0.817s, 1253.85/s  (0.792s, 1292.24/s)  LR: 1.531e-05  Data: 0.012 (0.013)
Train: 286 [ 850/1251 ( 68%)]  Loss: 3.303 (3.22)  Time: 0.777s, 1318.64/s  (0.792s, 1292.77/s)  LR: 1.531e-05  Data: 0.011 (0.013)
Train: 286 [ 900/1251 ( 72%)]  Loss: 3.178 (3.22)  Time: 0.779s, 1314.97/s  (0.791s, 1293.78/s)  LR: 1.531e-05  Data: 0.011 (0.013)
Train: 286 [ 950/1251 ( 76%)]  Loss: 3.043 (3.21)  Time: 0.779s, 1314.58/s  (0.791s, 1294.44/s)  LR: 1.531e-05  Data: 0.011 (0.013)
Train: 286 [1000/1251 ( 80%)]  Loss: 3.380 (3.22)  Time: 0.788s, 1298.96/s  (0.791s, 1295.32/s)  LR: 1.531e-05  Data: 0.011 (0.013)
Train: 286 [1050/1251 ( 84%)]  Loss: 3.110 (3.21)  Time: 0.778s, 1315.75/s  (0.790s, 1296.13/s)  LR: 1.531e-05  Data: 0.011 (0.013)
Train: 286 [1100/1251 ( 88%)]  Loss: 3.712 (3.24)  Time: 0.822s, 1246.14/s  (0.790s, 1295.45/s)  LR: 1.531e-05  Data: 0.011 (0.012)
Train: 286 [1150/1251 ( 92%)]  Loss: 3.189 (3.23)  Time: 0.778s, 1316.60/s  (0.791s, 1295.12/s)  LR: 1.531e-05  Data: 0.011 (0.012)
Train: 286 [1200/1251 ( 96%)]  Loss: 3.189 (3.23)  Time: 0.816s, 1254.24/s  (0.791s, 1294.11/s)  LR: 1.531e-05  Data: 0.011 (0.012)
Train: 286 [1250/1251 (100%)]  Loss: 3.388 (3.24)  Time: 0.766s, 1337.14/s  (0.791s, 1293.83/s)  LR: 1.531e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.498 (1.498)  Loss:  0.7089 (0.7089)  Acc@1: 91.4062 (91.4062)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.172 (0.560)  Loss:  0.7619 (1.1952)  Acc@1: 87.1462 (78.2380)  Acc@5: 98.2311 (94.0700)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-285.pth.tar', 78.3759999243164)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-284.pth.tar', 78.31199992431641)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-277.pth.tar', 78.30599997558593)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-278.pth.tar', 78.3000000805664)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-276.pth.tar', 78.29400005615234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-282.pth.tar', 78.28400018066407)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-280.pth.tar', 78.27400013183593)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-283.pth.tar', 78.2540000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-279.pth.tar', 78.250000078125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-286.pth.tar', 78.23799997558594)

Train: 287 [   0/1251 (  0%)]  Loss: 3.440 (3.44)  Time: 2.415s,  424.07/s  (2.415s,  424.07/s)  LR: 1.458e-05  Data: 1.618 (1.618)
Train: 287 [  50/1251 (  4%)]  Loss: 3.111 (3.28)  Time: 0.780s, 1313.17/s  (0.843s, 1214.85/s)  LR: 1.458e-05  Data: 0.011 (0.053)
Train: 287 [ 100/1251 (  8%)]  Loss: 3.311 (3.29)  Time: 0.780s, 1312.53/s  (0.817s, 1253.60/s)  LR: 1.458e-05  Data: 0.011 (0.032)
Train: 287 [ 150/1251 ( 12%)]  Loss: 3.261 (3.28)  Time: 0.778s, 1317.02/s  (0.807s, 1269.55/s)  LR: 1.458e-05  Data: 0.012 (0.025)
Train: 287 [ 200/1251 ( 16%)]  Loss: 3.060 (3.24)  Time: 0.778s, 1316.53/s  (0.801s, 1278.76/s)  LR: 1.458e-05  Data: 0.011 (0.022)
Train: 287 [ 250/1251 ( 20%)]  Loss: 3.553 (3.29)  Time: 0.789s, 1297.48/s  (0.797s, 1285.01/s)  LR: 1.458e-05  Data: 0.011 (0.020)
Train: 287 [ 300/1251 ( 24%)]  Loss: 3.459 (3.31)  Time: 0.776s, 1319.72/s  (0.799s, 1282.32/s)  LR: 1.458e-05  Data: 0.011 (0.018)
Train: 287 [ 350/1251 ( 28%)]  Loss: 3.319 (3.31)  Time: 0.779s, 1315.31/s  (0.797s, 1284.96/s)  LR: 1.458e-05  Data: 0.011 (0.017)
Train: 287 [ 400/1251 ( 32%)]  Loss: 3.486 (3.33)  Time: 0.778s, 1316.75/s  (0.795s, 1287.66/s)  LR: 1.458e-05  Data: 0.011 (0.016)
Train: 287 [ 450/1251 ( 36%)]  Loss: 3.194 (3.32)  Time: 0.813s, 1260.03/s  (0.795s, 1288.65/s)  LR: 1.458e-05  Data: 0.011 (0.016)
Train: 287 [ 500/1251 ( 40%)]  Loss: 3.132 (3.30)  Time: 0.787s, 1300.97/s  (0.793s, 1290.51/s)  LR: 1.458e-05  Data: 0.011 (0.015)
Train: 287 [ 550/1251 ( 44%)]  Loss: 3.357 (3.31)  Time: 0.779s, 1314.32/s  (0.793s, 1291.19/s)  LR: 1.458e-05  Data: 0.010 (0.015)
Train: 287 [ 600/1251 ( 48%)]  Loss: 3.556 (3.33)  Time: 0.782s, 1310.23/s  (0.793s, 1291.93/s)  LR: 1.458e-05  Data: 0.011 (0.015)
Train: 287 [ 650/1251 ( 52%)]  Loss: 3.285 (3.32)  Time: 0.824s, 1243.17/s  (0.792s, 1292.93/s)  LR: 1.458e-05  Data: 0.011 (0.014)
Train: 287 [ 700/1251 ( 56%)]  Loss: 3.246 (3.32)  Time: 0.781s, 1311.50/s  (0.793s, 1291.54/s)  LR: 1.458e-05  Data: 0.011 (0.014)
Train: 287 [ 750/1251 ( 60%)]  Loss: 3.376 (3.32)  Time: 0.781s, 1310.87/s  (0.792s, 1292.74/s)  LR: 1.458e-05  Data: 0.011 (0.014)
Train: 287 [ 800/1251 ( 64%)]  Loss: 3.433 (3.33)  Time: 0.778s, 1315.40/s  (0.792s, 1293.58/s)  LR: 1.458e-05  Data: 0.011 (0.014)
Train: 287 [ 850/1251 ( 68%)]  Loss: 3.641 (3.35)  Time: 0.780s, 1313.20/s  (0.791s, 1294.63/s)  LR: 1.458e-05  Data: 0.011 (0.014)
Train: 287 [ 900/1251 ( 72%)]  Loss: 3.433 (3.35)  Time: 0.786s, 1302.26/s  (0.791s, 1295.22/s)  LR: 1.458e-05  Data: 0.011 (0.013)
Train: 287 [ 950/1251 ( 76%)]  Loss: 3.321 (3.35)  Time: 0.813s, 1258.96/s  (0.790s, 1295.46/s)  LR: 1.458e-05  Data: 0.011 (0.013)
Train: 287 [1000/1251 ( 80%)]  Loss: 3.424 (3.35)  Time: 0.778s, 1315.49/s  (0.790s, 1295.97/s)  LR: 1.458e-05  Data: 0.011 (0.013)
Train: 287 [1050/1251 ( 84%)]  Loss: 3.206 (3.35)  Time: 0.791s, 1294.86/s  (0.790s, 1295.47/s)  LR: 1.458e-05  Data: 0.011 (0.013)
Train: 287 [1100/1251 ( 88%)]  Loss: 3.341 (3.35)  Time: 0.780s, 1312.85/s  (0.790s, 1295.63/s)  LR: 1.458e-05  Data: 0.011 (0.013)
Train: 287 [1150/1251 ( 92%)]  Loss: 3.166 (3.34)  Time: 0.815s, 1256.42/s  (0.791s, 1295.09/s)  LR: 1.458e-05  Data: 0.011 (0.013)
Train: 287 [1200/1251 ( 96%)]  Loss: 3.046 (3.33)  Time: 0.779s, 1315.00/s  (0.791s, 1294.90/s)  LR: 1.458e-05  Data: 0.012 (0.013)
Train: 287 [1250/1251 (100%)]  Loss: 3.566 (3.34)  Time: 0.764s, 1339.76/s  (0.791s, 1295.31/s)  LR: 1.458e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.499 (1.499)  Loss:  0.7784 (0.7784)  Acc@1: 91.6992 (91.6992)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.172 (0.563)  Loss:  0.8552 (1.2824)  Acc@1: 87.0283 (78.3240)  Acc@5: 98.1132 (94.0800)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-285.pth.tar', 78.3759999243164)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-287.pth.tar', 78.32400005371093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-284.pth.tar', 78.31199992431641)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-277.pth.tar', 78.30599997558593)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-278.pth.tar', 78.3000000805664)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-276.pth.tar', 78.29400005615234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-282.pth.tar', 78.28400018066407)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-280.pth.tar', 78.27400013183593)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-283.pth.tar', 78.2540000024414)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-279.pth.tar', 78.250000078125)

Train: 288 [   0/1251 (  0%)]  Loss: 3.505 (3.50)  Time: 2.298s,  445.64/s  (2.298s,  445.64/s)  LR: 1.390e-05  Data: 1.564 (1.564)
Train: 288 [  50/1251 (  4%)]  Loss: 3.127 (3.32)  Time: 0.819s, 1250.19/s  (0.824s, 1242.31/s)  LR: 1.390e-05  Data: 0.010 (0.049)
Train: 288 [ 100/1251 (  8%)]  Loss: 3.335 (3.32)  Time: 0.780s, 1313.08/s  (0.813s, 1259.59/s)  LR: 1.390e-05  Data: 0.011 (0.030)
Train: 288 [ 150/1251 ( 12%)]  Loss: 2.906 (3.22)  Time: 0.814s, 1257.28/s  (0.809s, 1266.03/s)  LR: 1.390e-05  Data: 0.011 (0.024)
Train: 288 [ 200/1251 ( 16%)]  Loss: 3.521 (3.28)  Time: 0.778s, 1316.10/s  (0.804s, 1273.26/s)  LR: 1.390e-05  Data: 0.011 (0.021)
Train: 288 [ 250/1251 ( 20%)]  Loss: 3.558 (3.33)  Time: 0.781s, 1311.11/s  (0.802s, 1277.33/s)  LR: 1.390e-05  Data: 0.011 (0.019)
Train: 288 [ 300/1251 ( 24%)]  Loss: 3.457 (3.34)  Time: 0.779s, 1314.25/s  (0.799s, 1281.20/s)  LR: 1.390e-05  Data: 0.010 (0.017)
Train: 288 [ 350/1251 ( 28%)]  Loss: 3.063 (3.31)  Time: 0.780s, 1312.29/s  (0.799s, 1282.39/s)  LR: 1.390e-05  Data: 0.011 (0.017)
Train: 288 [ 400/1251 ( 32%)]  Loss: 3.335 (3.31)  Time: 0.818s, 1251.64/s  (0.797s, 1284.80/s)  LR: 1.390e-05  Data: 0.011 (0.016)
Train: 288 [ 450/1251 ( 36%)]  Loss: 3.178 (3.30)  Time: 0.780s, 1313.04/s  (0.797s, 1284.79/s)  LR: 1.390e-05  Data: 0.011 (0.015)
Train: 288 [ 500/1251 ( 40%)]  Loss: 3.181 (3.29)  Time: 0.788s, 1299.41/s  (0.796s, 1286.87/s)  LR: 1.390e-05  Data: 0.011 (0.015)
Train: 288 [ 550/1251 ( 44%)]  Loss: 3.284 (3.29)  Time: 0.781s, 1311.78/s  (0.795s, 1288.74/s)  LR: 1.390e-05  Data: 0.012 (0.015)
Train: 288 [ 600/1251 ( 48%)]  Loss: 3.268 (3.29)  Time: 0.776s, 1319.78/s  (0.794s, 1290.08/s)  LR: 1.390e-05  Data: 0.011 (0.014)
Train: 288 [ 650/1251 ( 52%)]  Loss: 3.114 (3.27)  Time: 0.779s, 1313.79/s  (0.793s, 1290.91/s)  LR: 1.390e-05  Data: 0.011 (0.014)
Train: 288 [ 700/1251 ( 56%)]  Loss: 3.460 (3.29)  Time: 0.777s, 1317.42/s  (0.793s, 1291.48/s)  LR: 1.390e-05  Data: 0.011 (0.014)
Train: 288 [ 750/1251 ( 60%)]  Loss: 3.305 (3.29)  Time: 0.840s, 1219.57/s  (0.793s, 1291.37/s)  LR: 1.390e-05  Data: 0.011 (0.014)
Train: 288 [ 800/1251 ( 64%)]  Loss: 3.353 (3.29)  Time: 0.779s, 1314.00/s  (0.793s, 1291.82/s)  LR: 1.390e-05  Data: 0.012 (0.014)
Train: 288 [ 850/1251 ( 68%)]  Loss: 3.425 (3.30)  Time: 0.828s, 1237.13/s  (0.792s, 1292.73/s)  LR: 1.390e-05  Data: 0.012 (0.013)
Train: 288 [ 900/1251 ( 72%)]  Loss: 3.217 (3.29)  Time: 0.774s, 1323.33/s  (0.793s, 1291.00/s)  LR: 1.390e-05  Data: 0.010 (0.013)
Train: 288 [ 950/1251 ( 76%)]  Loss: 3.262 (3.29)  Time: 0.775s, 1321.85/s  (0.793s, 1291.50/s)  LR: 1.390e-05  Data: 0.011 (0.013)
Train: 288 [1000/1251 ( 80%)]  Loss: 3.296 (3.29)  Time: 0.815s, 1255.85/s  (0.794s, 1290.07/s)  LR: 1.390e-05  Data: 0.010 (0.013)
Train: 288 [1050/1251 ( 84%)]  Loss: 3.326 (3.29)  Time: 0.788s, 1299.31/s  (0.793s, 1290.49/s)  LR: 1.390e-05  Data: 0.011 (0.013)
Train: 288 [1100/1251 ( 88%)]  Loss: 3.259 (3.29)  Time: 0.777s, 1318.48/s  (0.793s, 1290.64/s)  LR: 1.390e-05  Data: 0.012 (0.013)
Train: 288 [1150/1251 ( 92%)]  Loss: 3.297 (3.29)  Time: 0.782s, 1309.55/s  (0.793s, 1290.89/s)  LR: 1.390e-05  Data: 0.011 (0.013)
Train: 288 [1200/1251 ( 96%)]  Loss: 3.485 (3.30)  Time: 0.778s, 1316.07/s  (0.793s, 1291.72/s)  LR: 1.390e-05  Data: 0.011 (0.013)
Train: 288 [1250/1251 (100%)]  Loss: 3.067 (3.29)  Time: 0.770s, 1330.51/s  (0.792s, 1292.22/s)  LR: 1.390e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.510 (1.510)  Loss:  0.7467 (0.7467)  Acc@1: 91.5039 (91.5039)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.172 (0.560)  Loss:  0.8167 (1.2467)  Acc@1: 86.7925 (78.3540)  Acc@5: 97.8774 (94.0880)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-285.pth.tar', 78.3759999243164)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-288.pth.tar', 78.35400008056641)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-287.pth.tar', 78.32400005371093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-284.pth.tar', 78.31199992431641)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-277.pth.tar', 78.30599997558593)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-278.pth.tar', 78.3000000805664)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-276.pth.tar', 78.29400005615234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-282.pth.tar', 78.28400018066407)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-280.pth.tar', 78.27400013183593)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-283.pth.tar', 78.2540000024414)

Train: 289 [   0/1251 (  0%)]  Loss: 3.074 (3.07)  Time: 2.259s,  453.28/s  (2.259s,  453.28/s)  LR: 1.328e-05  Data: 1.523 (1.523)
Train: 289 [  50/1251 (  4%)]  Loss: 3.243 (3.16)  Time: 0.830s, 1234.45/s  (0.819s, 1250.24/s)  LR: 1.328e-05  Data: 0.011 (0.046)
Train: 289 [ 100/1251 (  8%)]  Loss: 3.274 (3.20)  Time: 0.779s, 1313.87/s  (0.807s, 1268.84/s)  LR: 1.328e-05  Data: 0.011 (0.029)
Train: 289 [ 150/1251 ( 12%)]  Loss: 3.295 (3.22)  Time: 0.779s, 1313.80/s  (0.798s, 1282.73/s)  LR: 1.328e-05  Data: 0.011 (0.023)
Train: 289 [ 200/1251 ( 16%)]  Loss: 3.201 (3.22)  Time: 0.777s, 1317.20/s  (0.795s, 1287.70/s)  LR: 1.328e-05  Data: 0.011 (0.020)
Train: 289 [ 250/1251 ( 20%)]  Loss: 3.136 (3.20)  Time: 0.779s, 1315.25/s  (0.793s, 1292.03/s)  LR: 1.328e-05  Data: 0.012 (0.018)
Train: 289 [ 300/1251 ( 24%)]  Loss: 3.261 (3.21)  Time: 0.823s, 1243.71/s  (0.791s, 1293.94/s)  LR: 1.328e-05  Data: 0.011 (0.017)
Train: 289 [ 350/1251 ( 28%)]  Loss: 3.515 (3.25)  Time: 0.795s, 1287.78/s  (0.791s, 1294.40/s)  LR: 1.328e-05  Data: 0.012 (0.016)
Train: 289 [ 400/1251 ( 32%)]  Loss: 3.387 (3.27)  Time: 0.826s, 1239.38/s  (0.792s, 1293.03/s)  LR: 1.328e-05  Data: 0.011 (0.016)
Train: 289 [ 450/1251 ( 36%)]  Loss: 3.281 (3.27)  Time: 0.791s, 1295.34/s  (0.793s, 1290.51/s)  LR: 1.328e-05  Data: 0.012 (0.015)
Train: 289 [ 500/1251 ( 40%)]  Loss: 3.516 (3.29)  Time: 0.781s, 1311.47/s  (0.793s, 1290.49/s)  LR: 1.328e-05  Data: 0.011 (0.015)
Train: 289 [ 550/1251 ( 44%)]  Loss: 3.394 (3.30)  Time: 0.790s, 1296.75/s  (0.793s, 1291.82/s)  LR: 1.328e-05  Data: 0.012 (0.015)
Train: 289 [ 600/1251 ( 48%)]  Loss: 3.408 (3.31)  Time: 0.834s, 1228.01/s  (0.795s, 1287.76/s)  LR: 1.328e-05  Data: 0.012 (0.014)
Train: 289 [ 650/1251 ( 52%)]  Loss: 3.180 (3.30)  Time: 0.779s, 1315.08/s  (0.795s, 1288.82/s)  LR: 1.328e-05  Data: 0.012 (0.014)
Train: 289 [ 700/1251 ( 56%)]  Loss: 3.443 (3.31)  Time: 0.776s, 1318.85/s  (0.794s, 1289.69/s)  LR: 1.328e-05  Data: 0.011 (0.014)
Train: 289 [ 750/1251 ( 60%)]  Loss: 3.693 (3.33)  Time: 0.777s, 1317.24/s  (0.794s, 1289.07/s)  LR: 1.328e-05  Data: 0.011 (0.014)
Train: 289 [ 800/1251 ( 64%)]  Loss: 3.442 (3.34)  Time: 0.782s, 1308.91/s  (0.794s, 1289.28/s)  LR: 1.328e-05  Data: 0.011 (0.014)
Train: 289 [ 850/1251 ( 68%)]  Loss: 3.354 (3.34)  Time: 0.778s, 1316.10/s  (0.795s, 1288.03/s)  LR: 1.328e-05  Data: 0.010 (0.013)
Train: 289 [ 900/1251 ( 72%)]  Loss: 3.214 (3.33)  Time: 0.779s, 1315.31/s  (0.795s, 1288.81/s)  LR: 1.328e-05  Data: 0.010 (0.013)
Train: 289 [ 950/1251 ( 76%)]  Loss: 3.560 (3.34)  Time: 0.789s, 1297.79/s  (0.794s, 1289.41/s)  LR: 1.328e-05  Data: 0.010 (0.013)
Train: 289 [1000/1251 ( 80%)]  Loss: 3.103 (3.33)  Time: 0.780s, 1312.04/s  (0.794s, 1289.06/s)  LR: 1.328e-05  Data: 0.011 (0.013)
Train: 289 [1050/1251 ( 84%)]  Loss: 3.417 (3.34)  Time: 0.777s, 1318.15/s  (0.794s, 1289.84/s)  LR: 1.328e-05  Data: 0.011 (0.013)
Train: 289 [1100/1251 ( 88%)]  Loss: 3.414 (3.34)  Time: 0.777s, 1317.41/s  (0.794s, 1290.27/s)  LR: 1.328e-05  Data: 0.011 (0.013)
Train: 289 [1150/1251 ( 92%)]  Loss: 2.860 (3.32)  Time: 0.780s, 1312.53/s  (0.793s, 1290.84/s)  LR: 1.328e-05  Data: 0.011 (0.013)
Train: 289 [1200/1251 ( 96%)]  Loss: 3.702 (3.33)  Time: 0.862s, 1188.35/s  (0.793s, 1290.93/s)  LR: 1.328e-05  Data: 0.012 (0.013)
Train: 289 [1250/1251 (100%)]  Loss: 3.651 (3.35)  Time: 0.805s, 1272.31/s  (0.793s, 1291.09/s)  LR: 1.328e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.541 (1.541)  Loss:  0.7269 (0.7269)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.172 (0.560)  Loss:  0.8109 (1.2261)  Acc@1: 86.6745 (78.3600)  Acc@5: 97.8774 (94.1160)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-285.pth.tar', 78.3759999243164)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-289.pth.tar', 78.36000002929687)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-288.pth.tar', 78.35400008056641)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-287.pth.tar', 78.32400005371093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-284.pth.tar', 78.31199992431641)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-277.pth.tar', 78.30599997558593)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-278.pth.tar', 78.3000000805664)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-276.pth.tar', 78.29400005615234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-282.pth.tar', 78.28400018066407)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-280.pth.tar', 78.27400013183593)

Train: 290 [   0/1251 (  0%)]  Loss: 3.141 (3.14)  Time: 2.272s,  450.68/s  (2.272s,  450.68/s)  LR: 1.271e-05  Data: 1.503 (1.503)
Train: 290 [  50/1251 (  4%)]  Loss: 3.396 (3.27)  Time: 0.780s, 1312.66/s  (0.828s, 1237.33/s)  LR: 1.271e-05  Data: 0.011 (0.044)
Train: 290 [ 100/1251 (  8%)]  Loss: 3.659 (3.40)  Time: 0.778s, 1315.78/s  (0.806s, 1270.99/s)  LR: 1.271e-05  Data: 0.011 (0.028)
Train: 290 [ 150/1251 ( 12%)]  Loss: 2.894 (3.27)  Time: 0.781s, 1310.74/s  (0.800s, 1280.06/s)  LR: 1.271e-05  Data: 0.011 (0.022)
Train: 290 [ 200/1251 ( 16%)]  Loss: 3.343 (3.29)  Time: 0.780s, 1312.99/s  (0.797s, 1284.89/s)  LR: 1.271e-05  Data: 0.011 (0.020)
Train: 290 [ 250/1251 ( 20%)]  Loss: 3.035 (3.24)  Time: 0.780s, 1313.53/s  (0.794s, 1290.12/s)  LR: 1.271e-05  Data: 0.011 (0.018)
Train: 290 [ 300/1251 ( 24%)]  Loss: 3.172 (3.23)  Time: 0.779s, 1315.31/s  (0.794s, 1289.94/s)  LR: 1.271e-05  Data: 0.012 (0.017)
Train: 290 [ 350/1251 ( 28%)]  Loss: 3.219 (3.23)  Time: 0.778s, 1316.93/s  (0.795s, 1288.46/s)  LR: 1.271e-05  Data: 0.011 (0.016)
Train: 290 [ 400/1251 ( 32%)]  Loss: 3.314 (3.24)  Time: 0.815s, 1256.76/s  (0.794s, 1289.80/s)  LR: 1.271e-05  Data: 0.011 (0.015)
Train: 290 [ 450/1251 ( 36%)]  Loss: 3.433 (3.26)  Time: 0.778s, 1316.19/s  (0.794s, 1289.27/s)  LR: 1.271e-05  Data: 0.011 (0.015)
Train: 290 [ 500/1251 ( 40%)]  Loss: 3.353 (3.27)  Time: 0.844s, 1212.77/s  (0.794s, 1289.43/s)  LR: 1.271e-05  Data: 0.011 (0.014)
Train: 290 [ 550/1251 ( 44%)]  Loss: 3.219 (3.26)  Time: 0.780s, 1313.41/s  (0.794s, 1289.46/s)  LR: 1.271e-05  Data: 0.011 (0.014)
Train: 290 [ 600/1251 ( 48%)]  Loss: 3.108 (3.25)  Time: 0.815s, 1256.67/s  (0.795s, 1288.34/s)  LR: 1.271e-05  Data: 0.011 (0.014)
Train: 290 [ 650/1251 ( 52%)]  Loss: 3.290 (3.26)  Time: 0.816s, 1255.67/s  (0.796s, 1286.88/s)  LR: 1.271e-05  Data: 0.011 (0.014)
Train: 290 [ 700/1251 ( 56%)]  Loss: 3.532 (3.27)  Time: 0.781s, 1311.80/s  (0.797s, 1285.39/s)  LR: 1.271e-05  Data: 0.011 (0.014)
Train: 290 [ 750/1251 ( 60%)]  Loss: 3.478 (3.29)  Time: 0.779s, 1314.17/s  (0.796s, 1286.00/s)  LR: 1.271e-05  Data: 0.011 (0.013)
Train: 290 [ 800/1251 ( 64%)]  Loss: 3.237 (3.28)  Time: 0.807s, 1269.25/s  (0.796s, 1286.11/s)  LR: 1.271e-05  Data: 0.015 (0.013)
Train: 290 [ 850/1251 ( 68%)]  Loss: 3.322 (3.29)  Time: 0.782s, 1309.74/s  (0.797s, 1285.46/s)  LR: 1.271e-05  Data: 0.011 (0.013)
Train: 290 [ 900/1251 ( 72%)]  Loss: 3.516 (3.30)  Time: 0.779s, 1315.24/s  (0.796s, 1285.96/s)  LR: 1.271e-05  Data: 0.011 (0.013)
Train: 290 [ 950/1251 ( 76%)]  Loss: 3.386 (3.30)  Time: 0.816s, 1255.46/s  (0.797s, 1285.42/s)  LR: 1.271e-05  Data: 0.011 (0.013)
Train: 290 [1000/1251 ( 80%)]  Loss: 3.011 (3.29)  Time: 0.779s, 1314.93/s  (0.796s, 1286.48/s)  LR: 1.271e-05  Data: 0.011 (0.013)
Train: 290 [1050/1251 ( 84%)]  Loss: 3.095 (3.28)  Time: 0.787s, 1300.84/s  (0.796s, 1287.21/s)  LR: 1.271e-05  Data: 0.011 (0.013)
Train: 290 [1100/1251 ( 88%)]  Loss: 3.133 (3.27)  Time: 0.776s, 1319.03/s  (0.795s, 1287.99/s)  LR: 1.271e-05  Data: 0.011 (0.013)
Train: 290 [1150/1251 ( 92%)]  Loss: 3.440 (3.28)  Time: 0.839s, 1221.06/s  (0.795s, 1287.27/s)  LR: 1.271e-05  Data: 0.011 (0.013)
Train: 290 [1200/1251 ( 96%)]  Loss: 3.328 (3.28)  Time: 0.780s, 1312.76/s  (0.796s, 1287.08/s)  LR: 1.271e-05  Data: 0.011 (0.013)
Train: 290 [1250/1251 (100%)]  Loss: 3.026 (3.27)  Time: 0.768s, 1334.16/s  (0.795s, 1287.65/s)  LR: 1.271e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.574 (1.574)  Loss:  0.6617 (0.6617)  Acc@1: 91.5039 (91.5039)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.172 (0.571)  Loss:  0.7259 (1.1572)  Acc@1: 87.3821 (78.4760)  Acc@5: 98.1132 (94.1700)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-290.pth.tar', 78.476000078125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-285.pth.tar', 78.3759999243164)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-289.pth.tar', 78.36000002929687)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-288.pth.tar', 78.35400008056641)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-287.pth.tar', 78.32400005371093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-284.pth.tar', 78.31199992431641)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-277.pth.tar', 78.30599997558593)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-278.pth.tar', 78.3000000805664)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-276.pth.tar', 78.29400005615234)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-282.pth.tar', 78.28400018066407)

Train: 291 [   0/1251 (  0%)]  Loss: 3.356 (3.36)  Time: 2.265s,  452.15/s  (2.265s,  452.15/s)  LR: 1.220e-05  Data: 1.528 (1.528)
Train: 291 [  50/1251 (  4%)]  Loss: 3.115 (3.24)  Time: 0.812s, 1260.66/s  (0.832s, 1231.47/s)  LR: 1.220e-05  Data: 0.010 (0.048)
Train: 291 [ 100/1251 (  8%)]  Loss: 3.126 (3.20)  Time: 0.778s, 1316.31/s  (0.822s, 1245.35/s)  LR: 1.220e-05  Data: 0.011 (0.030)
Train: 291 [ 150/1251 ( 12%)]  Loss: 2.798 (3.10)  Time: 0.783s, 1308.53/s  (0.811s, 1262.35/s)  LR: 1.220e-05  Data: 0.011 (0.024)
Train: 291 [ 200/1251 ( 16%)]  Loss: 3.281 (3.14)  Time: 0.779s, 1314.61/s  (0.804s, 1273.35/s)  LR: 1.220e-05  Data: 0.011 (0.021)
Train: 291 [ 250/1251 ( 20%)]  Loss: 3.410 (3.18)  Time: 0.777s, 1317.54/s  (0.801s, 1278.18/s)  LR: 1.220e-05  Data: 0.011 (0.019)
Train: 291 [ 300/1251 ( 24%)]  Loss: 3.257 (3.19)  Time: 0.778s, 1315.83/s  (0.798s, 1283.58/s)  LR: 1.220e-05  Data: 0.011 (0.017)
Train: 291 [ 350/1251 ( 28%)]  Loss: 3.360 (3.21)  Time: 0.847s, 1209.65/s  (0.799s, 1282.21/s)  LR: 1.220e-05  Data: 0.011 (0.017)
Train: 291 [ 400/1251 ( 32%)]  Loss: 3.513 (3.25)  Time: 0.820s, 1249.14/s  (0.799s, 1281.90/s)  LR: 1.220e-05  Data: 0.011 (0.016)
Train: 291 [ 450/1251 ( 36%)]  Loss: 2.815 (3.20)  Time: 0.779s, 1314.94/s  (0.798s, 1283.23/s)  LR: 1.220e-05  Data: 0.012 (0.015)
Train: 291 [ 500/1251 ( 40%)]  Loss: 3.413 (3.22)  Time: 0.858s, 1193.21/s  (0.799s, 1280.99/s)  LR: 1.220e-05  Data: 0.011 (0.015)
Train: 291 [ 550/1251 ( 44%)]  Loss: 3.308 (3.23)  Time: 0.836s, 1225.20/s  (0.799s, 1281.69/s)  LR: 1.220e-05  Data: 0.011 (0.015)
Train: 291 [ 600/1251 ( 48%)]  Loss: 3.558 (3.25)  Time: 0.811s, 1262.48/s  (0.800s, 1280.69/s)  LR: 1.220e-05  Data: 0.011 (0.014)
Train: 291 [ 650/1251 ( 52%)]  Loss: 3.195 (3.25)  Time: 0.780s, 1313.20/s  (0.800s, 1280.09/s)  LR: 1.220e-05  Data: 0.011 (0.014)
Train: 291 [ 700/1251 ( 56%)]  Loss: 3.015 (3.23)  Time: 0.778s, 1316.43/s  (0.799s, 1281.00/s)  LR: 1.220e-05  Data: 0.011 (0.014)
Train: 291 [ 750/1251 ( 60%)]  Loss: 3.504 (3.25)  Time: 0.779s, 1313.71/s  (0.800s, 1280.51/s)  LR: 1.220e-05  Data: 0.011 (0.014)
Train: 291 [ 800/1251 ( 64%)]  Loss: 3.132 (3.24)  Time: 0.778s, 1315.48/s  (0.799s, 1281.47/s)  LR: 1.220e-05  Data: 0.011 (0.013)
Train: 291 [ 850/1251 ( 68%)]  Loss: 3.431 (3.25)  Time: 0.779s, 1314.80/s  (0.798s, 1282.43/s)  LR: 1.220e-05  Data: 0.011 (0.013)
Train: 291 [ 900/1251 ( 72%)]  Loss: 3.171 (3.25)  Time: 0.818s, 1251.13/s  (0.798s, 1283.26/s)  LR: 1.220e-05  Data: 0.011 (0.013)
Train: 291 [ 950/1251 ( 76%)]  Loss: 3.575 (3.27)  Time: 0.788s, 1298.97/s  (0.798s, 1282.91/s)  LR: 1.220e-05  Data: 0.012 (0.013)
Train: 291 [1000/1251 ( 80%)]  Loss: 3.095 (3.26)  Time: 0.779s, 1315.04/s  (0.798s, 1283.90/s)  LR: 1.220e-05  Data: 0.011 (0.013)
Train: 291 [1050/1251 ( 84%)]  Loss: 3.167 (3.25)  Time: 0.781s, 1310.55/s  (0.798s, 1283.88/s)  LR: 1.220e-05  Data: 0.011 (0.013)
Train: 291 [1100/1251 ( 88%)]  Loss: 3.264 (3.25)  Time: 0.841s, 1218.07/s  (0.798s, 1284.01/s)  LR: 1.220e-05  Data: 0.011 (0.013)
Train: 291 [1150/1251 ( 92%)]  Loss: 3.445 (3.26)  Time: 0.816s, 1254.24/s  (0.798s, 1283.85/s)  LR: 1.220e-05  Data: 0.016 (0.013)
Train: 291 [1200/1251 ( 96%)]  Loss: 3.359 (3.27)  Time: 0.811s, 1262.38/s  (0.797s, 1284.44/s)  LR: 1.220e-05  Data: 0.011 (0.013)
Train: 291 [1250/1251 (100%)]  Loss: 3.357 (3.27)  Time: 0.770s, 1330.09/s  (0.797s, 1284.43/s)  LR: 1.220e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.548 (1.548)  Loss:  0.7281 (0.7281)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.172 (0.560)  Loss:  0.8074 (1.2160)  Acc@1: 87.2641 (78.3920)  Acc@5: 97.6415 (94.0680)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-290.pth.tar', 78.476000078125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-291.pth.tar', 78.39199989746093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-285.pth.tar', 78.3759999243164)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-289.pth.tar', 78.36000002929687)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-288.pth.tar', 78.35400008056641)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-287.pth.tar', 78.32400005371093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-284.pth.tar', 78.31199992431641)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-277.pth.tar', 78.30599997558593)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-278.pth.tar', 78.3000000805664)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-276.pth.tar', 78.29400005615234)

Train: 292 [   0/1251 (  0%)]  Loss: 3.251 (3.25)  Time: 2.614s,  391.77/s  (2.614s,  391.77/s)  LR: 1.174e-05  Data: 1.860 (1.860)
Train: 292 [  50/1251 (  4%)]  Loss: 3.272 (3.26)  Time: 0.806s, 1270.10/s  (0.837s, 1223.90/s)  LR: 1.174e-05  Data: 0.011 (0.052)
Train: 292 [ 100/1251 (  8%)]  Loss: 3.575 (3.37)  Time: 0.776s, 1320.31/s  (0.812s, 1261.27/s)  LR: 1.174e-05  Data: 0.011 (0.032)
Train: 292 [ 150/1251 ( 12%)]  Loss: 3.020 (3.28)  Time: 0.844s, 1213.90/s  (0.807s, 1269.42/s)  LR: 1.174e-05  Data: 0.012 (0.025)
Train: 292 [ 200/1251 ( 16%)]  Loss: 3.393 (3.30)  Time: 0.780s, 1313.60/s  (0.800s, 1279.43/s)  LR: 1.174e-05  Data: 0.011 (0.022)
Train: 292 [ 250/1251 ( 20%)]  Loss: 3.583 (3.35)  Time: 0.814s, 1258.60/s  (0.802s, 1276.53/s)  LR: 1.174e-05  Data: 0.011 (0.020)
Train: 292 [ 300/1251 ( 24%)]  Loss: 2.965 (3.29)  Time: 0.779s, 1314.83/s  (0.803s, 1274.83/s)  LR: 1.174e-05  Data: 0.011 (0.018)
Train: 292 [ 350/1251 ( 28%)]  Loss: 3.269 (3.29)  Time: 0.777s, 1317.32/s  (0.802s, 1276.91/s)  LR: 1.174e-05  Data: 0.012 (0.017)
Train: 292 [ 400/1251 ( 32%)]  Loss: 3.334 (3.30)  Time: 0.786s, 1302.79/s  (0.801s, 1277.64/s)  LR: 1.174e-05  Data: 0.011 (0.016)
Train: 292 [ 450/1251 ( 36%)]  Loss: 3.475 (3.31)  Time: 0.782s, 1309.05/s  (0.800s, 1279.75/s)  LR: 1.174e-05  Data: 0.011 (0.016)
Train: 292 [ 500/1251 ( 40%)]  Loss: 3.591 (3.34)  Time: 0.864s, 1184.60/s  (0.800s, 1279.89/s)  LR: 1.174e-05  Data: 0.011 (0.015)
Train: 292 [ 550/1251 ( 44%)]  Loss: 3.437 (3.35)  Time: 0.776s, 1319.43/s  (0.800s, 1280.34/s)  LR: 1.174e-05  Data: 0.011 (0.015)
Train: 292 [ 600/1251 ( 48%)]  Loss: 3.017 (3.32)  Time: 0.801s, 1278.32/s  (0.799s, 1281.55/s)  LR: 1.174e-05  Data: 0.011 (0.015)
Train: 292 [ 650/1251 ( 52%)]  Loss: 3.111 (3.31)  Time: 0.810s, 1264.76/s  (0.798s, 1282.71/s)  LR: 1.174e-05  Data: 0.011 (0.014)
Train: 292 [ 700/1251 ( 56%)]  Loss: 3.168 (3.30)  Time: 0.776s, 1319.39/s  (0.798s, 1283.39/s)  LR: 1.174e-05  Data: 0.011 (0.014)
Train: 292 [ 750/1251 ( 60%)]  Loss: 3.300 (3.30)  Time: 0.777s, 1318.73/s  (0.798s, 1282.76/s)  LR: 1.174e-05  Data: 0.012 (0.014)
Train: 292 [ 800/1251 ( 64%)]  Loss: 3.048 (3.28)  Time: 0.779s, 1313.86/s  (0.798s, 1283.94/s)  LR: 1.174e-05  Data: 0.011 (0.014)
Train: 292 [ 850/1251 ( 68%)]  Loss: 3.114 (3.27)  Time: 0.821s, 1247.77/s  (0.798s, 1283.42/s)  LR: 1.174e-05  Data: 0.011 (0.014)
Train: 292 [ 900/1251 ( 72%)]  Loss: 3.123 (3.27)  Time: 0.780s, 1312.17/s  (0.797s, 1284.62/s)  LR: 1.174e-05  Data: 0.011 (0.013)
Train: 292 [ 950/1251 ( 76%)]  Loss: 3.369 (3.27)  Time: 0.779s, 1314.05/s  (0.797s, 1285.09/s)  LR: 1.174e-05  Data: 0.011 (0.013)
Train: 292 [1000/1251 ( 80%)]  Loss: 3.431 (3.28)  Time: 0.782s, 1310.14/s  (0.797s, 1285.62/s)  LR: 1.174e-05  Data: 0.011 (0.013)
Train: 292 [1050/1251 ( 84%)]  Loss: 3.375 (3.28)  Time: 0.777s, 1317.74/s  (0.796s, 1286.64/s)  LR: 1.174e-05  Data: 0.011 (0.013)
Train: 292 [1100/1251 ( 88%)]  Loss: 3.614 (3.30)  Time: 0.826s, 1239.64/s  (0.796s, 1287.14/s)  LR: 1.174e-05  Data: 0.011 (0.013)
Train: 292 [1150/1251 ( 92%)]  Loss: 3.355 (3.30)  Time: 0.778s, 1316.65/s  (0.795s, 1287.45/s)  LR: 1.174e-05  Data: 0.011 (0.013)
Train: 292 [1200/1251 ( 96%)]  Loss: 3.070 (3.29)  Time: 0.786s, 1302.70/s  (0.795s, 1287.41/s)  LR: 1.174e-05  Data: 0.011 (0.013)
Train: 292 [1250/1251 (100%)]  Loss: 3.575 (3.30)  Time: 0.800s, 1279.84/s  (0.795s, 1287.83/s)  LR: 1.174e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.524 (1.524)  Loss:  0.7488 (0.7488)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.172 (0.562)  Loss:  0.8279 (1.2487)  Acc@1: 87.1462 (78.2620)  Acc@5: 98.2311 (94.1260)
Train: 293 [   0/1251 (  0%)]  Loss: 3.015 (3.02)  Time: 2.289s,  447.36/s  (2.289s,  447.36/s)  LR: 1.133e-05  Data: 1.553 (1.553)
Train: 293 [  50/1251 (  4%)]  Loss: 3.702 (3.36)  Time: 0.780s, 1313.57/s  (0.816s, 1254.20/s)  LR: 1.133e-05  Data: 0.011 (0.047)
Train: 293 [ 100/1251 (  8%)]  Loss: 3.381 (3.37)  Time: 0.814s, 1258.10/s  (0.810s, 1264.01/s)  LR: 1.133e-05  Data: 0.011 (0.029)
Train: 293 [ 150/1251 ( 12%)]  Loss: 3.474 (3.39)  Time: 0.784s, 1306.88/s  (0.804s, 1274.37/s)  LR: 1.133e-05  Data: 0.011 (0.023)
Train: 293 [ 200/1251 ( 16%)]  Loss: 3.302 (3.37)  Time: 0.812s, 1260.60/s  (0.800s, 1279.24/s)  LR: 1.133e-05  Data: 0.010 (0.020)
Train: 293 [ 250/1251 ( 20%)]  Loss: 2.921 (3.30)  Time: 0.781s, 1311.96/s  (0.801s, 1278.22/s)  LR: 1.133e-05  Data: 0.011 (0.018)
Train: 293 [ 300/1251 ( 24%)]  Loss: 3.142 (3.28)  Time: 0.779s, 1313.98/s  (0.799s, 1281.14/s)  LR: 1.133e-05  Data: 0.011 (0.017)
Train: 293 [ 350/1251 ( 28%)]  Loss: 3.200 (3.27)  Time: 0.832s, 1231.35/s  (0.800s, 1279.86/s)  LR: 1.133e-05  Data: 0.011 (0.016)
Train: 293 [ 400/1251 ( 32%)]  Loss: 3.362 (3.28)  Time: 0.778s, 1316.74/s  (0.799s, 1282.15/s)  LR: 1.133e-05  Data: 0.011 (0.016)
Train: 293 [ 450/1251 ( 36%)]  Loss: 3.197 (3.27)  Time: 0.815s, 1256.90/s  (0.801s, 1279.01/s)  LR: 1.133e-05  Data: 0.011 (0.015)
Train: 293 [ 500/1251 ( 40%)]  Loss: 3.535 (3.29)  Time: 0.780s, 1313.27/s  (0.799s, 1281.53/s)  LR: 1.133e-05  Data: 0.011 (0.015)
Train: 293 [ 550/1251 ( 44%)]  Loss: 3.402 (3.30)  Time: 0.781s, 1311.08/s  (0.798s, 1283.27/s)  LR: 1.133e-05  Data: 0.011 (0.014)
Train: 293 [ 600/1251 ( 48%)]  Loss: 3.181 (3.29)  Time: 0.834s, 1227.51/s  (0.797s, 1284.38/s)  LR: 1.133e-05  Data: 0.011 (0.014)
Train: 293 [ 650/1251 ( 52%)]  Loss: 2.740 (3.25)  Time: 0.777s, 1317.31/s  (0.797s, 1284.53/s)  LR: 1.133e-05  Data: 0.011 (0.014)
Train: 293 [ 700/1251 ( 56%)]  Loss: 3.103 (3.24)  Time: 0.835s, 1226.09/s  (0.797s, 1285.38/s)  LR: 1.133e-05  Data: 0.011 (0.014)
Train: 293 [ 750/1251 ( 60%)]  Loss: 3.391 (3.25)  Time: 0.780s, 1313.20/s  (0.797s, 1285.12/s)  LR: 1.133e-05  Data: 0.011 (0.013)
Train: 293 [ 800/1251 ( 64%)]  Loss: 3.165 (3.25)  Time: 0.779s, 1314.40/s  (0.796s, 1285.65/s)  LR: 1.133e-05  Data: 0.011 (0.013)
Train: 293 [ 850/1251 ( 68%)]  Loss: 3.430 (3.26)  Time: 0.779s, 1314.18/s  (0.797s, 1285.40/s)  LR: 1.133e-05  Data: 0.011 (0.013)
Train: 293 [ 900/1251 ( 72%)]  Loss: 3.372 (3.26)  Time: 0.777s, 1317.38/s  (0.796s, 1286.47/s)  LR: 1.133e-05  Data: 0.011 (0.013)
Train: 293 [ 950/1251 ( 76%)]  Loss: 2.829 (3.24)  Time: 0.812s, 1260.57/s  (0.796s, 1287.14/s)  LR: 1.133e-05  Data: 0.011 (0.013)
Train: 293 [1000/1251 ( 80%)]  Loss: 3.245 (3.24)  Time: 0.778s, 1317.00/s  (0.796s, 1287.20/s)  LR: 1.133e-05  Data: 0.012 (0.013)
Train: 293 [1050/1251 ( 84%)]  Loss: 3.230 (3.24)  Time: 0.779s, 1314.58/s  (0.795s, 1288.18/s)  LR: 1.133e-05  Data: 0.011 (0.013)
Train: 293 [1100/1251 ( 88%)]  Loss: 3.659 (3.26)  Time: 0.823s, 1244.74/s  (0.795s, 1288.39/s)  LR: 1.133e-05  Data: 0.011 (0.013)
Train: 293 [1150/1251 ( 92%)]  Loss: 3.440 (3.27)  Time: 0.813s, 1258.85/s  (0.795s, 1288.14/s)  LR: 1.133e-05  Data: 0.011 (0.013)
Train: 293 [1200/1251 ( 96%)]  Loss: 3.319 (3.27)  Time: 0.784s, 1306.85/s  (0.795s, 1288.51/s)  LR: 1.133e-05  Data: 0.011 (0.013)
Train: 293 [1250/1251 (100%)]  Loss: 3.370 (3.27)  Time: 0.768s, 1334.04/s  (0.794s, 1289.05/s)  LR: 1.133e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.545 (1.545)  Loss:  0.6463 (0.6463)  Acc@1: 91.6992 (91.6992)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.172 (0.556)  Loss:  0.7285 (1.1341)  Acc@1: 87.2642 (78.4300)  Acc@5: 98.4670 (94.2100)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-290.pth.tar', 78.476000078125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-293.pth.tar', 78.43000015625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-291.pth.tar', 78.39199989746093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-285.pth.tar', 78.3759999243164)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-289.pth.tar', 78.36000002929687)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-288.pth.tar', 78.35400008056641)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-287.pth.tar', 78.32400005371093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-284.pth.tar', 78.31199992431641)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-277.pth.tar', 78.30599997558593)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-278.pth.tar', 78.3000000805664)

Train: 294 [   0/1251 (  0%)]  Loss: 3.285 (3.29)  Time: 2.397s,  427.20/s  (2.397s,  427.20/s)  LR: 1.098e-05  Data: 1.549 (1.549)
Train: 294 [  50/1251 (  4%)]  Loss: 2.940 (3.11)  Time: 0.781s, 1311.57/s  (0.842s, 1216.66/s)  LR: 1.098e-05  Data: 0.011 (0.044)
Train: 294 [ 100/1251 (  8%)]  Loss: 3.647 (3.29)  Time: 0.779s, 1315.07/s  (0.818s, 1251.73/s)  LR: 1.098e-05  Data: 0.011 (0.028)
Train: 294 [ 150/1251 ( 12%)]  Loss: 3.445 (3.33)  Time: 0.813s, 1259.82/s  (0.812s, 1261.53/s)  LR: 1.098e-05  Data: 0.011 (0.022)
Train: 294 [ 200/1251 ( 16%)]  Loss: 3.159 (3.30)  Time: 0.779s, 1314.86/s  (0.808s, 1267.53/s)  LR: 1.098e-05  Data: 0.011 (0.019)
Train: 294 [ 250/1251 ( 20%)]  Loss: 3.188 (3.28)  Time: 0.777s, 1317.34/s  (0.804s, 1273.05/s)  LR: 1.098e-05  Data: 0.011 (0.018)
Train: 294 [ 300/1251 ( 24%)]  Loss: 3.451 (3.30)  Time: 0.815s, 1257.05/s  (0.802s, 1277.17/s)  LR: 1.098e-05  Data: 0.012 (0.017)
Train: 294 [ 350/1251 ( 28%)]  Loss: 3.448 (3.32)  Time: 0.780s, 1312.71/s  (0.802s, 1277.11/s)  LR: 1.098e-05  Data: 0.011 (0.016)
Train: 294 [ 400/1251 ( 32%)]  Loss: 3.248 (3.31)  Time: 0.778s, 1315.82/s  (0.800s, 1279.88/s)  LR: 1.098e-05  Data: 0.011 (0.015)
Train: 294 [ 450/1251 ( 36%)]  Loss: 3.212 (3.30)  Time: 0.778s, 1315.81/s  (0.799s, 1281.90/s)  LR: 1.098e-05  Data: 0.011 (0.015)
Train: 294 [ 500/1251 ( 40%)]  Loss: 3.241 (3.30)  Time: 0.823s, 1244.54/s  (0.799s, 1282.20/s)  LR: 1.098e-05  Data: 0.011 (0.014)
Train: 294 [ 550/1251 ( 44%)]  Loss: 3.364 (3.30)  Time: 0.779s, 1314.19/s  (0.799s, 1281.13/s)  LR: 1.098e-05  Data: 0.011 (0.014)
Train: 294 [ 600/1251 ( 48%)]  Loss: 3.450 (3.31)  Time: 0.780s, 1312.77/s  (0.800s, 1280.28/s)  LR: 1.098e-05  Data: 0.011 (0.014)
Train: 294 [ 650/1251 ( 52%)]  Loss: 3.692 (3.34)  Time: 0.785s, 1304.02/s  (0.799s, 1281.55/s)  LR: 1.098e-05  Data: 0.011 (0.014)
Train: 294 [ 700/1251 ( 56%)]  Loss: 3.251 (3.33)  Time: 0.811s, 1263.23/s  (0.799s, 1282.06/s)  LR: 1.098e-05  Data: 0.011 (0.014)
Train: 294 [ 750/1251 ( 60%)]  Loss: 3.390 (3.34)  Time: 0.779s, 1315.28/s  (0.798s, 1282.94/s)  LR: 1.098e-05  Data: 0.011 (0.013)
Train: 294 [ 800/1251 ( 64%)]  Loss: 3.261 (3.33)  Time: 0.778s, 1316.06/s  (0.797s, 1284.46/s)  LR: 1.098e-05  Data: 0.011 (0.013)
Train: 294 [ 850/1251 ( 68%)]  Loss: 3.325 (3.33)  Time: 0.780s, 1313.56/s  (0.797s, 1284.74/s)  LR: 1.098e-05  Data: 0.011 (0.013)
Train: 294 [ 900/1251 ( 72%)]  Loss: 2.919 (3.31)  Time: 0.781s, 1311.14/s  (0.797s, 1284.70/s)  LR: 1.098e-05  Data: 0.012 (0.013)
Train: 294 [ 950/1251 ( 76%)]  Loss: 3.496 (3.32)  Time: 0.781s, 1311.15/s  (0.797s, 1285.12/s)  LR: 1.098e-05  Data: 0.011 (0.013)
Train: 294 [1000/1251 ( 80%)]  Loss: 3.532 (3.33)  Time: 0.823s, 1244.97/s  (0.796s, 1285.62/s)  LR: 1.098e-05  Data: 0.012 (0.013)
Train: 294 [1050/1251 ( 84%)]  Loss: 3.423 (3.33)  Time: 0.820s, 1248.45/s  (0.797s, 1284.85/s)  LR: 1.098e-05  Data: 0.011 (0.013)
Train: 294 [1100/1251 ( 88%)]  Loss: 3.417 (3.34)  Time: 0.778s, 1315.63/s  (0.796s, 1285.77/s)  LR: 1.098e-05  Data: 0.011 (0.013)
Train: 294 [1150/1251 ( 92%)]  Loss: 2.984 (3.32)  Time: 0.778s, 1316.76/s  (0.796s, 1286.60/s)  LR: 1.098e-05  Data: 0.011 (0.013)
Train: 294 [1200/1251 ( 96%)]  Loss: 3.416 (3.33)  Time: 0.781s, 1311.94/s  (0.796s, 1286.99/s)  LR: 1.098e-05  Data: 0.012 (0.013)
Train: 294 [1250/1251 (100%)]  Loss: 3.190 (3.32)  Time: 0.775s, 1320.80/s  (0.795s, 1287.28/s)  LR: 1.098e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.500 (1.500)  Loss:  0.7375 (0.7375)  Acc@1: 91.7969 (91.7969)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.172 (0.565)  Loss:  0.8058 (1.2425)  Acc@1: 86.9104 (78.2980)  Acc@5: 98.1132 (94.1060)
Train: 295 [   0/1251 (  0%)]  Loss: 3.131 (3.13)  Time: 2.270s,  451.05/s  (2.270s,  451.05/s)  LR: 1.068e-05  Data: 1.534 (1.534)
Train: 295 [  50/1251 (  4%)]  Loss: 3.382 (3.26)  Time: 0.777s, 1317.12/s  (0.830s, 1233.49/s)  LR: 1.068e-05  Data: 0.011 (0.047)
Train: 295 [ 100/1251 (  8%)]  Loss: 3.319 (3.28)  Time: 0.815s, 1256.44/s  (0.821s, 1247.28/s)  LR: 1.068e-05  Data: 0.011 (0.029)
Train: 295 [ 150/1251 ( 12%)]  Loss: 3.214 (3.26)  Time: 0.815s, 1256.82/s  (0.814s, 1257.86/s)  LR: 1.068e-05  Data: 0.012 (0.023)
Train: 295 [ 200/1251 ( 16%)]  Loss: 3.326 (3.27)  Time: 0.780s, 1312.65/s  (0.809s, 1266.49/s)  LR: 1.068e-05  Data: 0.011 (0.020)
Train: 295 [ 250/1251 ( 20%)]  Loss: 2.945 (3.22)  Time: 0.777s, 1317.34/s  (0.805s, 1271.37/s)  LR: 1.068e-05  Data: 0.011 (0.018)
Train: 295 [ 300/1251 ( 24%)]  Loss: 2.839 (3.17)  Time: 0.817s, 1253.36/s  (0.803s, 1275.22/s)  LR: 1.068e-05  Data: 0.011 (0.017)
Train: 295 [ 350/1251 ( 28%)]  Loss: 2.959 (3.14)  Time: 0.777s, 1317.29/s  (0.802s, 1277.08/s)  LR: 1.068e-05  Data: 0.011 (0.016)
Train: 295 [ 400/1251 ( 32%)]  Loss: 3.120 (3.14)  Time: 0.778s, 1316.27/s  (0.800s, 1279.55/s)  LR: 1.068e-05  Data: 0.011 (0.016)
Train: 295 [ 450/1251 ( 36%)]  Loss: 3.257 (3.15)  Time: 0.815s, 1255.77/s  (0.800s, 1279.67/s)  LR: 1.068e-05  Data: 0.012 (0.015)
Train: 295 [ 500/1251 ( 40%)]  Loss: 3.456 (3.18)  Time: 0.846s, 1210.08/s  (0.800s, 1280.36/s)  LR: 1.068e-05  Data: 0.012 (0.015)
Train: 295 [ 550/1251 ( 44%)]  Loss: 3.450 (3.20)  Time: 0.779s, 1315.19/s  (0.800s, 1280.72/s)  LR: 1.068e-05  Data: 0.011 (0.014)
Train: 295 [ 600/1251 ( 48%)]  Loss: 3.416 (3.22)  Time: 0.779s, 1314.35/s  (0.798s, 1282.67/s)  LR: 1.068e-05  Data: 0.011 (0.014)
Train: 295 [ 650/1251 ( 52%)]  Loss: 2.927 (3.20)  Time: 0.813s, 1259.94/s  (0.798s, 1283.15/s)  LR: 1.068e-05  Data: 0.011 (0.014)
Train: 295 [ 700/1251 ( 56%)]  Loss: 3.550 (3.22)  Time: 0.781s, 1311.79/s  (0.798s, 1283.69/s)  LR: 1.068e-05  Data: 0.012 (0.014)
Train: 295 [ 750/1251 ( 60%)]  Loss: 3.188 (3.22)  Time: 0.822s, 1245.03/s  (0.798s, 1283.56/s)  LR: 1.068e-05  Data: 0.011 (0.014)
Train: 295 [ 800/1251 ( 64%)]  Loss: 3.193 (3.22)  Time: 0.777s, 1317.06/s  (0.797s, 1284.71/s)  LR: 1.068e-05  Data: 0.011 (0.013)
Train: 295 [ 850/1251 ( 68%)]  Loss: 2.962 (3.20)  Time: 0.778s, 1315.69/s  (0.796s, 1286.15/s)  LR: 1.068e-05  Data: 0.011 (0.013)
Train: 295 [ 900/1251 ( 72%)]  Loss: 3.359 (3.21)  Time: 0.832s, 1230.35/s  (0.796s, 1286.78/s)  LR: 1.068e-05  Data: 0.011 (0.013)
Train: 295 [ 950/1251 ( 76%)]  Loss: 3.327 (3.22)  Time: 0.811s, 1263.04/s  (0.796s, 1286.09/s)  LR: 1.068e-05  Data: 0.011 (0.013)
Train: 295 [1000/1251 ( 80%)]  Loss: 3.228 (3.22)  Time: 0.777s, 1317.51/s  (0.797s, 1285.17/s)  LR: 1.068e-05  Data: 0.011 (0.013)
Train: 295 [1050/1251 ( 84%)]  Loss: 3.331 (3.22)  Time: 0.782s, 1309.02/s  (0.796s, 1285.93/s)  LR: 1.068e-05  Data: 0.011 (0.013)
Train: 295 [1100/1251 ( 88%)]  Loss: 3.203 (3.22)  Time: 0.820s, 1248.53/s  (0.797s, 1285.31/s)  LR: 1.068e-05  Data: 0.011 (0.013)
Train: 295 [1150/1251 ( 92%)]  Loss: 3.210 (3.22)  Time: 0.824s, 1243.01/s  (0.796s, 1286.29/s)  LR: 1.068e-05  Data: 0.012 (0.013)
Train: 295 [1200/1251 ( 96%)]  Loss: 3.266 (3.22)  Time: 0.780s, 1313.53/s  (0.796s, 1287.08/s)  LR: 1.068e-05  Data: 0.011 (0.013)
Train: 295 [1250/1251 (100%)]  Loss: 3.337 (3.23)  Time: 0.766s, 1337.08/s  (0.795s, 1287.89/s)  LR: 1.068e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.508 (1.508)  Loss:  0.7617 (0.7617)  Acc@1: 91.6992 (91.6992)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.172 (0.578)  Loss:  0.8610 (1.2735)  Acc@1: 86.7925 (78.2980)  Acc@5: 97.8774 (94.0540)
Train: 296 [   0/1251 (  0%)]  Loss: 3.333 (3.33)  Time: 2.228s,  459.65/s  (2.228s,  459.65/s)  LR: 1.043e-05  Data: 1.493 (1.493)
Train: 296 [  50/1251 (  4%)]  Loss: 3.141 (3.24)  Time: 0.825s, 1240.85/s  (0.831s, 1231.75/s)  LR: 1.043e-05  Data: 0.012 (0.047)
Train: 296 [ 100/1251 (  8%)]  Loss: 3.531 (3.33)  Time: 0.813s, 1258.79/s  (0.812s, 1260.98/s)  LR: 1.043e-05  Data: 0.011 (0.029)
Train: 296 [ 150/1251 ( 12%)]  Loss: 3.494 (3.37)  Time: 0.776s, 1318.87/s  (0.807s, 1269.25/s)  LR: 1.043e-05  Data: 0.011 (0.023)
Train: 296 [ 200/1251 ( 16%)]  Loss: 3.314 (3.36)  Time: 0.809s, 1265.56/s  (0.804s, 1272.95/s)  LR: 1.043e-05  Data: 0.011 (0.020)
Train: 296 [ 250/1251 ( 20%)]  Loss: 3.450 (3.38)  Time: 0.817s, 1253.39/s  (0.800s, 1280.43/s)  LR: 1.043e-05  Data: 0.012 (0.018)
Train: 296 [ 300/1251 ( 24%)]  Loss: 3.421 (3.38)  Time: 0.813s, 1260.11/s  (0.800s, 1279.53/s)  LR: 1.043e-05  Data: 0.010 (0.017)
Train: 296 [ 350/1251 ( 28%)]  Loss: 3.185 (3.36)  Time: 0.779s, 1314.89/s  (0.798s, 1283.69/s)  LR: 1.043e-05  Data: 0.011 (0.016)
Train: 296 [ 400/1251 ( 32%)]  Loss: 3.336 (3.36)  Time: 0.777s, 1317.36/s  (0.798s, 1283.30/s)  LR: 1.043e-05  Data: 0.012 (0.016)
Train: 296 [ 450/1251 ( 36%)]  Loss: 3.494 (3.37)  Time: 0.779s, 1314.13/s  (0.798s, 1283.49/s)  LR: 1.043e-05  Data: 0.011 (0.015)
Train: 296 [ 500/1251 ( 40%)]  Loss: 3.358 (3.37)  Time: 0.776s, 1319.93/s  (0.797s, 1284.09/s)  LR: 1.043e-05  Data: 0.011 (0.015)
Train: 296 [ 550/1251 ( 44%)]  Loss: 3.040 (3.34)  Time: 0.779s, 1314.73/s  (0.796s, 1286.37/s)  LR: 1.043e-05  Data: 0.011 (0.014)
Train: 296 [ 600/1251 ( 48%)]  Loss: 3.324 (3.34)  Time: 0.776s, 1319.81/s  (0.795s, 1288.58/s)  LR: 1.043e-05  Data: 0.010 (0.014)
Train: 296 [ 650/1251 ( 52%)]  Loss: 3.344 (3.34)  Time: 0.780s, 1312.34/s  (0.796s, 1286.57/s)  LR: 1.043e-05  Data: 0.012 (0.014)
Train: 296 [ 700/1251 ( 56%)]  Loss: 3.228 (3.33)  Time: 0.780s, 1312.96/s  (0.795s, 1288.32/s)  LR: 1.043e-05  Data: 0.011 (0.014)
Train: 296 [ 750/1251 ( 60%)]  Loss: 3.497 (3.34)  Time: 0.835s, 1225.73/s  (0.795s, 1287.75/s)  LR: 1.043e-05  Data: 0.011 (0.013)
Train: 296 [ 800/1251 ( 64%)]  Loss: 3.332 (3.34)  Time: 0.790s, 1296.03/s  (0.795s, 1288.69/s)  LR: 1.043e-05  Data: 0.011 (0.013)
Train: 296 [ 850/1251 ( 68%)]  Loss: 3.237 (3.34)  Time: 0.781s, 1310.76/s  (0.794s, 1289.50/s)  LR: 1.043e-05  Data: 0.011 (0.013)
Train: 296 [ 900/1251 ( 72%)]  Loss: 3.736 (3.36)  Time: 0.780s, 1312.58/s  (0.794s, 1289.32/s)  LR: 1.043e-05  Data: 0.011 (0.013)
Train: 296 [ 950/1251 ( 76%)]  Loss: 3.027 (3.34)  Time: 0.792s, 1292.79/s  (0.794s, 1289.07/s)  LR: 1.043e-05  Data: 0.011 (0.013)
Train: 296 [1000/1251 ( 80%)]  Loss: 3.325 (3.34)  Time: 0.835s, 1225.79/s  (0.794s, 1289.93/s)  LR: 1.043e-05  Data: 0.012 (0.013)
Train: 296 [1050/1251 ( 84%)]  Loss: 3.640 (3.35)  Time: 0.778s, 1316.30/s  (0.794s, 1289.35/s)  LR: 1.043e-05  Data: 0.010 (0.013)
Train: 296 [1100/1251 ( 88%)]  Loss: 3.037 (3.34)  Time: 0.785s, 1304.13/s  (0.794s, 1288.96/s)  LR: 1.043e-05  Data: 0.011 (0.013)
Train: 296 [1150/1251 ( 92%)]  Loss: 3.361 (3.34)  Time: 0.789s, 1298.44/s  (0.795s, 1288.83/s)  LR: 1.043e-05  Data: 0.011 (0.013)
Train: 296 [1200/1251 ( 96%)]  Loss: 3.414 (3.34)  Time: 0.786s, 1302.84/s  (0.794s, 1289.72/s)  LR: 1.043e-05  Data: 0.010 (0.013)
Train: 296 [1250/1251 (100%)]  Loss: 3.085 (3.33)  Time: 0.768s, 1333.83/s  (0.794s, 1290.47/s)  LR: 1.043e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.551 (1.551)  Loss:  0.6891 (0.6891)  Acc@1: 91.7969 (91.7969)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.172 (0.556)  Loss:  0.7807 (1.1938)  Acc@1: 87.2642 (78.4740)  Acc@5: 98.2311 (94.1620)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-290.pth.tar', 78.476000078125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-296.pth.tar', 78.47400002685546)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-293.pth.tar', 78.43000015625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-291.pth.tar', 78.39199989746093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-285.pth.tar', 78.3759999243164)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-289.pth.tar', 78.36000002929687)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-288.pth.tar', 78.35400008056641)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-287.pth.tar', 78.32400005371093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-284.pth.tar', 78.31199992431641)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-277.pth.tar', 78.30599997558593)

Train: 297 [   0/1251 (  0%)]  Loss: 3.257 (3.26)  Time: 2.249s,  455.37/s  (2.249s,  455.37/s)  LR: 1.024e-05  Data: 1.515 (1.515)
Train: 297 [  50/1251 (  4%)]  Loss: 2.857 (3.06)  Time: 0.831s, 1232.00/s  (0.820s, 1248.34/s)  LR: 1.024e-05  Data: 0.011 (0.045)
Train: 297 [ 100/1251 (  8%)]  Loss: 3.290 (3.13)  Time: 0.780s, 1312.78/s  (0.801s, 1277.62/s)  LR: 1.024e-05  Data: 0.012 (0.028)
Train: 297 [ 150/1251 ( 12%)]  Loss: 3.320 (3.18)  Time: 0.794s, 1289.01/s  (0.803s, 1274.96/s)  LR: 1.024e-05  Data: 0.010 (0.023)
Train: 297 [ 200/1251 ( 16%)]  Loss: 3.118 (3.17)  Time: 0.780s, 1313.34/s  (0.800s, 1280.23/s)  LR: 1.024e-05  Data: 0.011 (0.020)
Train: 297 [ 250/1251 ( 20%)]  Loss: 3.284 (3.19)  Time: 0.794s, 1289.78/s  (0.796s, 1285.90/s)  LR: 1.024e-05  Data: 0.011 (0.018)
Train: 297 [ 300/1251 ( 24%)]  Loss: 3.398 (3.22)  Time: 0.780s, 1313.28/s  (0.794s, 1288.92/s)  LR: 1.024e-05  Data: 0.011 (0.017)
Train: 297 [ 350/1251 ( 28%)]  Loss: 3.514 (3.25)  Time: 0.815s, 1256.18/s  (0.796s, 1287.21/s)  LR: 1.024e-05  Data: 0.012 (0.016)
Train: 297 [ 400/1251 ( 32%)]  Loss: 3.066 (3.23)  Time: 0.778s, 1316.52/s  (0.796s, 1286.90/s)  LR: 1.024e-05  Data: 0.011 (0.015)
Train: 297 [ 450/1251 ( 36%)]  Loss: 2.648 (3.18)  Time: 0.815s, 1256.48/s  (0.796s, 1286.23/s)  LR: 1.024e-05  Data: 0.012 (0.015)
Train: 297 [ 500/1251 ( 40%)]  Loss: 3.083 (3.17)  Time: 0.814s, 1257.66/s  (0.798s, 1283.34/s)  LR: 1.024e-05  Data: 0.011 (0.015)
Train: 297 [ 550/1251 ( 44%)]  Loss: 2.778 (3.13)  Time: 0.779s, 1315.12/s  (0.797s, 1284.44/s)  LR: 1.024e-05  Data: 0.011 (0.014)
Train: 297 [ 600/1251 ( 48%)]  Loss: 3.491 (3.16)  Time: 0.777s, 1318.19/s  (0.796s, 1285.85/s)  LR: 1.024e-05  Data: 0.011 (0.014)
Train: 297 [ 650/1251 ( 52%)]  Loss: 3.388 (3.18)  Time: 0.786s, 1303.46/s  (0.796s, 1285.93/s)  LR: 1.024e-05  Data: 0.011 (0.014)
Train: 297 [ 700/1251 ( 56%)]  Loss: 3.347 (3.19)  Time: 0.781s, 1311.13/s  (0.796s, 1286.75/s)  LR: 1.024e-05  Data: 0.011 (0.014)
Train: 297 [ 750/1251 ( 60%)]  Loss: 3.214 (3.19)  Time: 0.780s, 1312.34/s  (0.795s, 1287.25/s)  LR: 1.024e-05  Data: 0.011 (0.013)
Train: 297 [ 800/1251 ( 64%)]  Loss: 3.132 (3.19)  Time: 0.780s, 1312.04/s  (0.795s, 1287.73/s)  LR: 1.024e-05  Data: 0.011 (0.013)
Train: 297 [ 850/1251 ( 68%)]  Loss: 2.817 (3.17)  Time: 0.781s, 1310.40/s  (0.795s, 1288.25/s)  LR: 1.024e-05  Data: 0.011 (0.013)
Train: 297 [ 900/1251 ( 72%)]  Loss: 3.670 (3.19)  Time: 0.779s, 1315.16/s  (0.794s, 1288.96/s)  LR: 1.024e-05  Data: 0.013 (0.013)
Train: 297 [ 950/1251 ( 76%)]  Loss: 3.352 (3.20)  Time: 0.775s, 1320.48/s  (0.794s, 1289.66/s)  LR: 1.024e-05  Data: 0.011 (0.013)
Train: 297 [1000/1251 ( 80%)]  Loss: 3.203 (3.20)  Time: 0.815s, 1255.88/s  (0.794s, 1289.89/s)  LR: 1.024e-05  Data: 0.011 (0.013)
Train: 297 [1050/1251 ( 84%)]  Loss: 3.235 (3.20)  Time: 0.781s, 1311.93/s  (0.794s, 1289.85/s)  LR: 1.024e-05  Data: 0.011 (0.013)
Train: 297 [1100/1251 ( 88%)]  Loss: 3.169 (3.20)  Time: 0.816s, 1255.30/s  (0.794s, 1290.33/s)  LR: 1.024e-05  Data: 0.011 (0.013)
Train: 297 [1150/1251 ( 92%)]  Loss: 3.198 (3.20)  Time: 0.815s, 1256.29/s  (0.793s, 1290.89/s)  LR: 1.024e-05  Data: 0.010 (0.013)
Train: 297 [1200/1251 ( 96%)]  Loss: 3.481 (3.21)  Time: 0.814s, 1257.62/s  (0.794s, 1289.66/s)  LR: 1.024e-05  Data: 0.011 (0.013)
Train: 297 [1250/1251 (100%)]  Loss: 3.346 (3.22)  Time: 0.833s, 1229.41/s  (0.794s, 1289.81/s)  LR: 1.024e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.493 (1.493)  Loss:  0.7852 (0.7852)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.172 (0.562)  Loss:  0.8437 (1.2840)  Acc@1: 87.7358 (78.3220)  Acc@5: 98.1132 (94.0880)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-290.pth.tar', 78.476000078125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-296.pth.tar', 78.47400002685546)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-293.pth.tar', 78.43000015625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-291.pth.tar', 78.39199989746093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-285.pth.tar', 78.3759999243164)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-289.pth.tar', 78.36000002929687)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-288.pth.tar', 78.35400008056641)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-287.pth.tar', 78.32400005371093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-297.pth.tar', 78.32199997314453)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-284.pth.tar', 78.31199992431641)

Train: 298 [   0/1251 (  0%)]  Loss: 3.178 (3.18)  Time: 2.314s,  442.60/s  (2.314s,  442.60/s)  LR: 1.011e-05  Data: 1.579 (1.579)
Train: 298 [  50/1251 (  4%)]  Loss: 3.092 (3.14)  Time: 0.780s, 1312.41/s  (0.833s, 1229.03/s)  LR: 1.011e-05  Data: 0.011 (0.047)
Train: 298 [ 100/1251 (  8%)]  Loss: 3.394 (3.22)  Time: 0.778s, 1315.75/s  (0.812s, 1261.75/s)  LR: 1.011e-05  Data: 0.011 (0.029)
Train: 298 [ 150/1251 ( 12%)]  Loss: 3.449 (3.28)  Time: 0.780s, 1313.65/s  (0.805s, 1272.54/s)  LR: 1.011e-05  Data: 0.011 (0.023)
Train: 298 [ 200/1251 ( 16%)]  Loss: 3.364 (3.30)  Time: 0.814s, 1257.33/s  (0.801s, 1279.15/s)  LR: 1.011e-05  Data: 0.011 (0.020)
Train: 298 [ 250/1251 ( 20%)]  Loss: 3.393 (3.31)  Time: 0.779s, 1314.97/s  (0.798s, 1282.95/s)  LR: 1.011e-05  Data: 0.011 (0.018)
Train: 298 [ 300/1251 ( 24%)]  Loss: 3.755 (3.38)  Time: 0.815s, 1256.71/s  (0.798s, 1283.45/s)  LR: 1.011e-05  Data: 0.011 (0.017)
Train: 298 [ 350/1251 ( 28%)]  Loss: 3.614 (3.41)  Time: 0.777s, 1317.35/s  (0.797s, 1285.54/s)  LR: 1.011e-05  Data: 0.011 (0.016)
Train: 298 [ 400/1251 ( 32%)]  Loss: 3.401 (3.40)  Time: 0.777s, 1317.12/s  (0.797s, 1284.10/s)  LR: 1.011e-05  Data: 0.011 (0.016)
Train: 298 [ 450/1251 ( 36%)]  Loss: 3.027 (3.37)  Time: 0.826s, 1239.34/s  (0.796s, 1286.27/s)  LR: 1.011e-05  Data: 0.011 (0.015)
Train: 298 [ 500/1251 ( 40%)]  Loss: 3.509 (3.38)  Time: 0.840s, 1218.35/s  (0.795s, 1287.92/s)  LR: 1.011e-05  Data: 0.011 (0.015)
Train: 298 [ 550/1251 ( 44%)]  Loss: 3.398 (3.38)  Time: 0.817s, 1253.85/s  (0.795s, 1288.59/s)  LR: 1.011e-05  Data: 0.011 (0.014)
Train: 298 [ 600/1251 ( 48%)]  Loss: 3.258 (3.37)  Time: 0.820s, 1248.18/s  (0.795s, 1288.63/s)  LR: 1.011e-05  Data: 0.012 (0.014)
Train: 298 [ 650/1251 ( 52%)]  Loss: 3.113 (3.35)  Time: 0.786s, 1302.47/s  (0.794s, 1290.04/s)  LR: 1.011e-05  Data: 0.013 (0.014)
Train: 298 [ 700/1251 ( 56%)]  Loss: 3.302 (3.35)  Time: 0.837s, 1223.47/s  (0.794s, 1290.46/s)  LR: 1.011e-05  Data: 0.011 (0.014)
Train: 298 [ 750/1251 ( 60%)]  Loss: 3.298 (3.35)  Time: 0.797s, 1284.37/s  (0.793s, 1291.18/s)  LR: 1.011e-05  Data: 0.011 (0.014)
Train: 298 [ 800/1251 ( 64%)]  Loss: 3.326 (3.35)  Time: 0.780s, 1312.71/s  (0.794s, 1289.95/s)  LR: 1.011e-05  Data: 0.011 (0.013)
Train: 298 [ 850/1251 ( 68%)]  Loss: 2.964 (3.32)  Time: 0.779s, 1314.75/s  (0.794s, 1290.47/s)  LR: 1.011e-05  Data: 0.011 (0.013)
Train: 298 [ 900/1251 ( 72%)]  Loss: 3.413 (3.33)  Time: 0.776s, 1319.05/s  (0.793s, 1291.06/s)  LR: 1.011e-05  Data: 0.011 (0.013)
Train: 298 [ 950/1251 ( 76%)]  Loss: 3.293 (3.33)  Time: 0.818s, 1251.58/s  (0.793s, 1291.70/s)  LR: 1.011e-05  Data: 0.011 (0.013)
Train: 298 [1000/1251 ( 80%)]  Loss: 3.147 (3.32)  Time: 0.811s, 1262.55/s  (0.793s, 1291.72/s)  LR: 1.011e-05  Data: 0.011 (0.013)
Train: 298 [1050/1251 ( 84%)]  Loss: 3.152 (3.31)  Time: 0.833s, 1229.01/s  (0.793s, 1291.09/s)  LR: 1.011e-05  Data: 0.011 (0.013)
Train: 298 [1100/1251 ( 88%)]  Loss: 3.151 (3.30)  Time: 0.813s, 1259.34/s  (0.793s, 1291.72/s)  LR: 1.011e-05  Data: 0.011 (0.013)
Train: 298 [1150/1251 ( 92%)]  Loss: 3.183 (3.30)  Time: 0.783s, 1306.97/s  (0.794s, 1290.00/s)  LR: 1.011e-05  Data: 0.011 (0.013)
Train: 298 [1200/1251 ( 96%)]  Loss: 3.275 (3.30)  Time: 0.816s, 1255.63/s  (0.794s, 1289.79/s)  LR: 1.011e-05  Data: 0.012 (0.013)
Train: 298 [1250/1251 (100%)]  Loss: 2.962 (3.29)  Time: 0.811s, 1262.06/s  (0.795s, 1287.86/s)  LR: 1.011e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.508 (1.508)  Loss:  0.7574 (0.7574)  Acc@1: 91.8945 (91.8945)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.172 (0.581)  Loss:  0.8474 (1.2646)  Acc@1: 87.0283 (78.3980)  Acc@5: 98.1132 (94.1020)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-290.pth.tar', 78.476000078125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-296.pth.tar', 78.47400002685546)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-293.pth.tar', 78.43000015625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-298.pth.tar', 78.39800005371093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-291.pth.tar', 78.39199989746093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-285.pth.tar', 78.3759999243164)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-289.pth.tar', 78.36000002929687)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-288.pth.tar', 78.35400008056641)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-287.pth.tar', 78.32400005371093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-297.pth.tar', 78.32199997314453)

Train: 299 [   0/1251 (  0%)]  Loss: 2.939 (2.94)  Time: 2.333s,  438.85/s  (2.333s,  438.85/s)  LR: 1.003e-05  Data: 1.599 (1.599)
Train: 299 [  50/1251 (  4%)]  Loss: 3.168 (3.05)  Time: 0.779s, 1313.90/s  (0.838s, 1222.20/s)  LR: 1.003e-05  Data: 0.011 (0.048)
Train: 299 [ 100/1251 (  8%)]  Loss: 3.269 (3.13)  Time: 0.816s, 1255.06/s  (0.817s, 1253.16/s)  LR: 1.003e-05  Data: 0.010 (0.029)
Train: 299 [ 150/1251 ( 12%)]  Loss: 3.275 (3.16)  Time: 0.779s, 1314.92/s  (0.811s, 1262.63/s)  LR: 1.003e-05  Data: 0.011 (0.023)
Train: 299 [ 200/1251 ( 16%)]  Loss: 3.192 (3.17)  Time: 0.781s, 1311.88/s  (0.810s, 1264.16/s)  LR: 1.003e-05  Data: 0.011 (0.020)
Train: 299 [ 250/1251 ( 20%)]  Loss: 3.210 (3.18)  Time: 0.777s, 1318.48/s  (0.805s, 1272.42/s)  LR: 1.003e-05  Data: 0.011 (0.018)
Train: 299 [ 300/1251 ( 24%)]  Loss: 3.437 (3.21)  Time: 0.795s, 1287.36/s  (0.801s, 1278.13/s)  LR: 1.003e-05  Data: 0.011 (0.017)
Train: 299 [ 350/1251 ( 28%)]  Loss: 3.340 (3.23)  Time: 0.816s, 1255.42/s  (0.801s, 1278.10/s)  LR: 1.003e-05  Data: 0.011 (0.016)
Train: 299 [ 400/1251 ( 32%)]  Loss: 3.340 (3.24)  Time: 0.781s, 1310.81/s  (0.801s, 1277.89/s)  LR: 1.003e-05  Data: 0.016 (0.016)
Train: 299 [ 450/1251 ( 36%)]  Loss: 3.045 (3.22)  Time: 0.779s, 1314.90/s  (0.799s, 1280.83/s)  LR: 1.003e-05  Data: 0.011 (0.015)
Train: 299 [ 500/1251 ( 40%)]  Loss: 3.294 (3.23)  Time: 0.828s, 1236.60/s  (0.800s, 1280.73/s)  LR: 1.003e-05  Data: 0.011 (0.015)
Train: 299 [ 550/1251 ( 44%)]  Loss: 3.075 (3.22)  Time: 0.783s, 1308.59/s  (0.799s, 1280.81/s)  LR: 1.003e-05  Data: 0.011 (0.014)
Train: 299 [ 600/1251 ( 48%)]  Loss: 3.178 (3.21)  Time: 0.778s, 1316.58/s  (0.799s, 1282.13/s)  LR: 1.003e-05  Data: 0.011 (0.014)
Train: 299 [ 650/1251 ( 52%)]  Loss: 3.246 (3.21)  Time: 0.776s, 1319.88/s  (0.798s, 1283.42/s)  LR: 1.003e-05  Data: 0.011 (0.014)
Train: 299 [ 700/1251 ( 56%)]  Loss: 3.049 (3.20)  Time: 0.780s, 1313.55/s  (0.797s, 1285.08/s)  LR: 1.003e-05  Data: 0.011 (0.014)
Train: 299 [ 750/1251 ( 60%)]  Loss: 3.210 (3.20)  Time: 0.776s, 1320.13/s  (0.797s, 1285.61/s)  LR: 1.003e-05  Data: 0.011 (0.014)
Train: 299 [ 800/1251 ( 64%)]  Loss: 3.017 (3.19)  Time: 0.877s, 1167.31/s  (0.797s, 1284.90/s)  LR: 1.003e-05  Data: 0.011 (0.013)
Train: 299 [ 850/1251 ( 68%)]  Loss: 3.251 (3.20)  Time: 0.776s, 1319.63/s  (0.797s, 1284.43/s)  LR: 1.003e-05  Data: 0.011 (0.013)
Train: 299 [ 900/1251 ( 72%)]  Loss: 3.135 (3.19)  Time: 0.779s, 1315.09/s  (0.797s, 1284.94/s)  LR: 1.003e-05  Data: 0.011 (0.013)
Train: 299 [ 950/1251 ( 76%)]  Loss: 3.021 (3.18)  Time: 0.778s, 1317.02/s  (0.796s, 1286.10/s)  LR: 1.003e-05  Data: 0.011 (0.013)
Train: 299 [1000/1251 ( 80%)]  Loss: 3.383 (3.19)  Time: 0.778s, 1316.29/s  (0.795s, 1287.33/s)  LR: 1.003e-05  Data: 0.011 (0.013)
Train: 299 [1050/1251 ( 84%)]  Loss: 3.421 (3.20)  Time: 0.779s, 1314.81/s  (0.795s, 1288.48/s)  LR: 1.003e-05  Data: 0.011 (0.013)
Train: 299 [1100/1251 ( 88%)]  Loss: 3.667 (3.22)  Time: 0.775s, 1321.49/s  (0.794s, 1288.87/s)  LR: 1.003e-05  Data: 0.011 (0.013)
Train: 299 [1150/1251 ( 92%)]  Loss: 3.366 (3.23)  Time: 0.809s, 1265.56/s  (0.794s, 1289.12/s)  LR: 1.003e-05  Data: 0.011 (0.013)
Train: 299 [1200/1251 ( 96%)]  Loss: 3.273 (3.23)  Time: 0.819s, 1250.77/s  (0.795s, 1288.23/s)  LR: 1.003e-05  Data: 0.011 (0.013)
Train: 299 [1250/1251 (100%)]  Loss: 3.021 (3.22)  Time: 0.796s, 1287.01/s  (0.796s, 1287.02/s)  LR: 1.003e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.496 (1.496)  Loss:  0.7912 (0.7912)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.172 (0.556)  Loss:  0.8683 (1.2840)  Acc@1: 87.1462 (78.3860)  Acc@5: 98.2311 (94.1140)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-290.pth.tar', 78.476000078125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-296.pth.tar', 78.47400002685546)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-293.pth.tar', 78.43000015625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-298.pth.tar', 78.39800005371093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-291.pth.tar', 78.39199989746093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-299.pth.tar', 78.38599997558593)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-285.pth.tar', 78.3759999243164)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-289.pth.tar', 78.36000002929687)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-288.pth.tar', 78.35400008056641)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-287.pth.tar', 78.32400005371093)

Train: 300 [   0/1251 (  0%)]  Loss: 2.850 (2.85)  Time: 2.191s,  467.28/s  (2.191s,  467.28/s)  LR: 1.000e-05  Data: 1.454 (1.454)
Train: 300 [  50/1251 (  4%)]  Loss: 3.462 (3.16)  Time: 0.809s, 1266.19/s  (0.822s, 1245.48/s)  LR: 1.000e-05  Data: 0.011 (0.043)
Train: 300 [ 100/1251 (  8%)]  Loss: 3.452 (3.25)  Time: 0.778s, 1315.48/s  (0.804s, 1274.38/s)  LR: 1.000e-05  Data: 0.011 (0.027)
Train: 300 [ 150/1251 ( 12%)]  Loss: 3.160 (3.23)  Time: 0.810s, 1264.24/s  (0.801s, 1278.27/s)  LR: 1.000e-05  Data: 0.011 (0.022)
Train: 300 [ 200/1251 ( 16%)]  Loss: 3.025 (3.19)  Time: 0.811s, 1262.35/s  (0.799s, 1281.23/s)  LR: 1.000e-05  Data: 0.011 (0.019)
Train: 300 [ 250/1251 ( 20%)]  Loss: 3.195 (3.19)  Time: 0.793s, 1291.96/s  (0.797s, 1285.14/s)  LR: 1.000e-05  Data: 0.011 (0.018)
Train: 300 [ 300/1251 ( 24%)]  Loss: 3.270 (3.20)  Time: 0.777s, 1317.17/s  (0.795s, 1288.47/s)  LR: 1.000e-05  Data: 0.011 (0.017)
Train: 300 [ 350/1251 ( 28%)]  Loss: 3.283 (3.21)  Time: 0.778s, 1316.18/s  (0.794s, 1290.31/s)  LR: 1.000e-05  Data: 0.011 (0.016)
Train: 300 [ 400/1251 ( 32%)]  Loss: 3.113 (3.20)  Time: 0.810s, 1263.88/s  (0.794s, 1290.01/s)  LR: 1.000e-05  Data: 0.011 (0.015)
Train: 300 [ 450/1251 ( 36%)]  Loss: 3.113 (3.19)  Time: 0.779s, 1313.87/s  (0.794s, 1289.05/s)  LR: 1.000e-05  Data: 0.011 (0.015)
Train: 300 [ 500/1251 ( 40%)]  Loss: 3.188 (3.19)  Time: 0.776s, 1320.33/s  (0.794s, 1289.65/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 300 [ 550/1251 ( 44%)]  Loss: 2.672 (3.15)  Time: 0.811s, 1262.90/s  (0.793s, 1290.71/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 300 [ 600/1251 ( 48%)]  Loss: 3.385 (3.17)  Time: 0.823s, 1244.65/s  (0.794s, 1289.72/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 300 [ 650/1251 ( 52%)]  Loss: 2.749 (3.14)  Time: 0.779s, 1314.62/s  (0.794s, 1290.33/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 300 [ 700/1251 ( 56%)]  Loss: 3.383 (3.15)  Time: 0.778s, 1316.94/s  (0.793s, 1291.82/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 300 [ 750/1251 ( 60%)]  Loss: 3.173 (3.15)  Time: 0.815s, 1257.06/s  (0.793s, 1291.64/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 300 [ 800/1251 ( 64%)]  Loss: 3.608 (3.18)  Time: 0.779s, 1314.70/s  (0.792s, 1292.31/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 300 [ 850/1251 ( 68%)]  Loss: 3.690 (3.21)  Time: 0.778s, 1316.57/s  (0.792s, 1293.27/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 300 [ 900/1251 ( 72%)]  Loss: 3.099 (3.20)  Time: 0.781s, 1311.31/s  (0.791s, 1293.91/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 300 [ 950/1251 ( 76%)]  Loss: 3.176 (3.20)  Time: 0.811s, 1262.06/s  (0.791s, 1294.28/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 300 [1000/1251 ( 80%)]  Loss: 3.124 (3.20)  Time: 0.778s, 1316.66/s  (0.792s, 1293.05/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 300 [1050/1251 ( 84%)]  Loss: 3.141 (3.20)  Time: 0.779s, 1315.33/s  (0.792s, 1293.13/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 300 [1100/1251 ( 88%)]  Loss: 3.368 (3.20)  Time: 0.819s, 1250.00/s  (0.792s, 1293.25/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 300 [1150/1251 ( 92%)]  Loss: 3.056 (3.20)  Time: 0.816s, 1255.12/s  (0.792s, 1292.17/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 300 [1200/1251 ( 96%)]  Loss: 3.522 (3.21)  Time: 0.817s, 1253.82/s  (0.793s, 1291.19/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 300 [1250/1251 (100%)]  Loss: 3.490 (3.22)  Time: 0.769s, 1331.27/s  (0.793s, 1291.70/s)  LR: 1.000e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.499 (1.499)  Loss:  0.7210 (0.7210)  Acc@1: 91.6016 (91.6016)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.172 (0.566)  Loss:  0.7954 (1.2232)  Acc@1: 87.2642 (78.4620)  Acc@5: 98.2311 (94.1500)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-290.pth.tar', 78.476000078125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-296.pth.tar', 78.47400002685546)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-300.pth.tar', 78.46200002685546)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-293.pth.tar', 78.43000015625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-298.pth.tar', 78.39800005371093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-291.pth.tar', 78.39199989746093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-299.pth.tar', 78.38599997558593)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-285.pth.tar', 78.3759999243164)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-289.pth.tar', 78.36000002929687)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-288.pth.tar', 78.35400008056641)

Train: 301 [   0/1251 (  0%)]  Loss: 3.206 (3.21)  Time: 2.241s,  456.87/s  (2.241s,  456.87/s)  LR: 1.000e-05  Data: 1.507 (1.507)
Train: 301 [  50/1251 (  4%)]  Loss: 3.752 (3.48)  Time: 0.819s, 1250.17/s  (0.818s, 1252.01/s)  LR: 1.000e-05  Data: 0.010 (0.046)
Train: 301 [ 100/1251 (  8%)]  Loss: 3.094 (3.35)  Time: 0.814s, 1257.29/s  (0.811s, 1262.49/s)  LR: 1.000e-05  Data: 0.011 (0.028)
Train: 301 [ 150/1251 ( 12%)]  Loss: 2.828 (3.22)  Time: 0.818s, 1252.59/s  (0.810s, 1264.27/s)  LR: 1.000e-05  Data: 0.012 (0.023)
Train: 301 [ 200/1251 ( 16%)]  Loss: 3.308 (3.24)  Time: 0.851s, 1203.27/s  (0.810s, 1264.45/s)  LR: 1.000e-05  Data: 0.010 (0.020)
Train: 301 [ 250/1251 ( 20%)]  Loss: 2.982 (3.20)  Time: 0.844s, 1213.60/s  (0.808s, 1267.78/s)  LR: 1.000e-05  Data: 0.011 (0.018)
Train: 301 [ 300/1251 ( 24%)]  Loss: 3.211 (3.20)  Time: 0.786s, 1303.58/s  (0.808s, 1268.09/s)  LR: 1.000e-05  Data: 0.011 (0.017)
Train: 301 [ 350/1251 ( 28%)]  Loss: 3.323 (3.21)  Time: 0.811s, 1261.92/s  (0.807s, 1269.32/s)  LR: 1.000e-05  Data: 0.011 (0.016)
Train: 301 [ 400/1251 ( 32%)]  Loss: 3.367 (3.23)  Time: 0.780s, 1313.58/s  (0.804s, 1272.87/s)  LR: 1.000e-05  Data: 0.010 (0.015)
Train: 301 [ 450/1251 ( 36%)]  Loss: 3.426 (3.25)  Time: 0.780s, 1313.29/s  (0.803s, 1275.74/s)  LR: 1.000e-05  Data: 0.011 (0.015)
Train: 301 [ 500/1251 ( 40%)]  Loss: 3.195 (3.24)  Time: 0.779s, 1314.29/s  (0.801s, 1278.77/s)  LR: 1.000e-05  Data: 0.011 (0.015)
Train: 301 [ 550/1251 ( 44%)]  Loss: 3.352 (3.25)  Time: 0.779s, 1314.54/s  (0.800s, 1280.17/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 301 [ 600/1251 ( 48%)]  Loss: 3.051 (3.24)  Time: 0.777s, 1318.37/s  (0.800s, 1279.28/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 301 [ 650/1251 ( 52%)]  Loss: 3.359 (3.25)  Time: 0.779s, 1314.39/s  (0.800s, 1279.55/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 301 [ 700/1251 ( 56%)]  Loss: 3.013 (3.23)  Time: 0.823s, 1244.60/s  (0.800s, 1279.71/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 301 [ 750/1251 ( 60%)]  Loss: 3.240 (3.23)  Time: 0.817s, 1253.38/s  (0.800s, 1279.88/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 301 [ 800/1251 ( 64%)]  Loss: 2.914 (3.21)  Time: 0.777s, 1317.35/s  (0.799s, 1280.89/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 301 [ 850/1251 ( 68%)]  Loss: 3.201 (3.21)  Time: 0.778s, 1316.57/s  (0.798s, 1282.56/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 301 [ 900/1251 ( 72%)]  Loss: 3.352 (3.22)  Time: 0.779s, 1315.23/s  (0.798s, 1282.62/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 301 [ 950/1251 ( 76%)]  Loss: 3.061 (3.21)  Time: 0.781s, 1310.77/s  (0.798s, 1283.36/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 301 [1000/1251 ( 80%)]  Loss: 3.298 (3.22)  Time: 0.779s, 1314.66/s  (0.798s, 1283.68/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 301 [1050/1251 ( 84%)]  Loss: 3.580 (3.23)  Time: 0.779s, 1314.80/s  (0.797s, 1284.79/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 301 [1100/1251 ( 88%)]  Loss: 3.802 (3.26)  Time: 0.780s, 1313.43/s  (0.797s, 1285.23/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 301 [1150/1251 ( 92%)]  Loss: 3.182 (3.25)  Time: 0.778s, 1316.88/s  (0.797s, 1285.59/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 301 [1200/1251 ( 96%)]  Loss: 3.439 (3.26)  Time: 0.867s, 1181.42/s  (0.796s, 1285.92/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 301 [1250/1251 (100%)]  Loss: 3.232 (3.26)  Time: 0.767s, 1335.91/s  (0.796s, 1286.12/s)  LR: 1.000e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.516 (1.516)  Loss:  0.7896 (0.7896)  Acc@1: 91.8945 (91.8945)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.172 (0.558)  Loss:  0.8608 (1.2866)  Acc@1: 86.4387 (78.2760)  Acc@5: 97.7594 (94.1140)
Train: 302 [   0/1251 (  0%)]  Loss: 3.275 (3.28)  Time: 2.405s,  425.72/s  (2.405s,  425.72/s)  LR: 1.000e-05  Data: 1.668 (1.668)
Train: 302 [  50/1251 (  4%)]  Loss: 3.085 (3.18)  Time: 0.781s, 1311.53/s  (0.814s, 1258.43/s)  LR: 1.000e-05  Data: 0.011 (0.043)
Train: 302 [ 100/1251 (  8%)]  Loss: 3.086 (3.15)  Time: 0.777s, 1317.53/s  (0.812s, 1261.47/s)  LR: 1.000e-05  Data: 0.010 (0.027)
Train: 302 [ 150/1251 ( 12%)]  Loss: 3.293 (3.18)  Time: 0.811s, 1263.25/s  (0.811s, 1262.77/s)  LR: 1.000e-05  Data: 0.011 (0.022)
Train: 302 [ 200/1251 ( 16%)]  Loss: 3.255 (3.20)  Time: 0.777s, 1318.20/s  (0.804s, 1273.83/s)  LR: 1.000e-05  Data: 0.011 (0.019)
Train: 302 [ 250/1251 ( 20%)]  Loss: 3.307 (3.22)  Time: 0.821s, 1247.70/s  (0.802s, 1276.80/s)  LR: 1.000e-05  Data: 0.011 (0.018)
Train: 302 [ 300/1251 ( 24%)]  Loss: 3.448 (3.25)  Time: 0.782s, 1309.68/s  (0.801s, 1278.41/s)  LR: 1.000e-05  Data: 0.011 (0.017)
Train: 302 [ 350/1251 ( 28%)]  Loss: 3.200 (3.24)  Time: 0.781s, 1310.32/s  (0.799s, 1281.28/s)  LR: 1.000e-05  Data: 0.011 (0.016)
Train: 302 [ 400/1251 ( 32%)]  Loss: 3.393 (3.26)  Time: 0.780s, 1312.29/s  (0.797s, 1284.20/s)  LR: 1.000e-05  Data: 0.011 (0.015)
Train: 302 [ 450/1251 ( 36%)]  Loss: 3.002 (3.23)  Time: 0.778s, 1315.67/s  (0.796s, 1287.15/s)  LR: 1.000e-05  Data: 0.012 (0.015)
Train: 302 [ 500/1251 ( 40%)]  Loss: 3.021 (3.21)  Time: 0.826s, 1239.99/s  (0.797s, 1284.96/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 302 [ 550/1251 ( 44%)]  Loss: 3.071 (3.20)  Time: 0.784s, 1305.40/s  (0.797s, 1284.50/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 302 [ 600/1251 ( 48%)]  Loss: 3.669 (3.24)  Time: 0.778s, 1316.75/s  (0.796s, 1286.04/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 302 [ 650/1251 ( 52%)]  Loss: 3.390 (3.25)  Time: 0.819s, 1249.84/s  (0.795s, 1287.97/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 302 [ 700/1251 ( 56%)]  Loss: 3.421 (3.26)  Time: 0.779s, 1314.74/s  (0.794s, 1288.96/s)  LR: 1.000e-05  Data: 0.012 (0.014)
Train: 302 [ 750/1251 ( 60%)]  Loss: 3.431 (3.27)  Time: 0.784s, 1306.29/s  (0.796s, 1286.98/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 302 [ 800/1251 ( 64%)]  Loss: 3.064 (3.26)  Time: 0.780s, 1313.50/s  (0.795s, 1288.19/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 302 [ 850/1251 ( 68%)]  Loss: 3.440 (3.27)  Time: 0.792s, 1292.27/s  (0.796s, 1286.79/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 302 [ 900/1251 ( 72%)]  Loss: 3.040 (3.26)  Time: 0.815s, 1256.79/s  (0.796s, 1287.21/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 302 [ 950/1251 ( 76%)]  Loss: 3.452 (3.27)  Time: 0.779s, 1313.68/s  (0.796s, 1287.16/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 302 [1000/1251 ( 80%)]  Loss: 3.332 (3.27)  Time: 0.777s, 1318.72/s  (0.795s, 1287.50/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 302 [1050/1251 ( 84%)]  Loss: 3.365 (3.27)  Time: 0.777s, 1318.58/s  (0.795s, 1287.88/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 302 [1100/1251 ( 88%)]  Loss: 3.385 (3.28)  Time: 0.776s, 1318.77/s  (0.795s, 1288.67/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 302 [1150/1251 ( 92%)]  Loss: 3.573 (3.29)  Time: 0.806s, 1270.62/s  (0.795s, 1288.42/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 302 [1200/1251 ( 96%)]  Loss: 3.047 (3.28)  Time: 0.774s, 1322.96/s  (0.794s, 1289.14/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 302 [1250/1251 (100%)]  Loss: 3.346 (3.28)  Time: 0.801s, 1278.20/s  (0.794s, 1289.72/s)  LR: 1.000e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.543 (1.543)  Loss:  0.7702 (0.7702)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.172 (0.556)  Loss:  0.8449 (1.2782)  Acc@1: 87.3821 (78.3280)  Acc@5: 97.7594 (94.0620)
Train: 303 [   0/1251 (  0%)]  Loss: 3.282 (3.28)  Time: 2.298s,  445.69/s  (2.298s,  445.69/s)  LR: 1.000e-05  Data: 1.544 (1.544)
Train: 303 [  50/1251 (  4%)]  Loss: 3.308 (3.30)  Time: 0.779s, 1313.94/s  (0.826s, 1239.53/s)  LR: 1.000e-05  Data: 0.011 (0.044)
Train: 303 [ 100/1251 (  8%)]  Loss: 3.074 (3.22)  Time: 0.792s, 1292.88/s  (0.806s, 1270.10/s)  LR: 1.000e-05  Data: 0.011 (0.028)
Train: 303 [ 150/1251 ( 12%)]  Loss: 3.208 (3.22)  Time: 0.807s, 1269.11/s  (0.806s, 1270.84/s)  LR: 1.000e-05  Data: 0.011 (0.022)
Train: 303 [ 200/1251 ( 16%)]  Loss: 3.455 (3.27)  Time: 0.778s, 1315.46/s  (0.801s, 1278.04/s)  LR: 1.000e-05  Data: 0.010 (0.019)
Train: 303 [ 250/1251 ( 20%)]  Loss: 3.258 (3.26)  Time: 0.815s, 1256.97/s  (0.801s, 1278.77/s)  LR: 1.000e-05  Data: 0.011 (0.018)
Train: 303 [ 300/1251 ( 24%)]  Loss: 3.173 (3.25)  Time: 0.791s, 1294.17/s  (0.798s, 1282.50/s)  LR: 1.000e-05  Data: 0.011 (0.017)
Train: 303 [ 350/1251 ( 28%)]  Loss: 3.258 (3.25)  Time: 0.779s, 1315.31/s  (0.798s, 1283.72/s)  LR: 1.000e-05  Data: 0.011 (0.016)
Train: 303 [ 400/1251 ( 32%)]  Loss: 3.516 (3.28)  Time: 0.781s, 1311.40/s  (0.797s, 1284.56/s)  LR: 1.000e-05  Data: 0.012 (0.015)
Train: 303 [ 450/1251 ( 36%)]  Loss: 3.622 (3.32)  Time: 0.813s, 1258.79/s  (0.796s, 1286.44/s)  LR: 1.000e-05  Data: 0.010 (0.015)
Train: 303 [ 500/1251 ( 40%)]  Loss: 3.211 (3.31)  Time: 0.815s, 1256.30/s  (0.797s, 1284.03/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 303 [ 550/1251 ( 44%)]  Loss: 3.623 (3.33)  Time: 0.792s, 1292.32/s  (0.797s, 1284.44/s)  LR: 1.000e-05  Data: 0.013 (0.014)
Train: 303 [ 600/1251 ( 48%)]  Loss: 3.261 (3.33)  Time: 0.835s, 1226.43/s  (0.797s, 1285.56/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 303 [ 650/1251 ( 52%)]  Loss: 2.802 (3.29)  Time: 0.778s, 1316.59/s  (0.796s, 1286.99/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 303 [ 700/1251 ( 56%)]  Loss: 3.033 (3.27)  Time: 0.816s, 1254.35/s  (0.795s, 1287.39/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 303 [ 750/1251 ( 60%)]  Loss: 3.398 (3.28)  Time: 0.813s, 1259.05/s  (0.796s, 1286.21/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 303 [ 800/1251 ( 64%)]  Loss: 3.327 (3.28)  Time: 0.782s, 1309.24/s  (0.796s, 1286.88/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 303 [ 850/1251 ( 68%)]  Loss: 3.265 (3.28)  Time: 0.775s, 1321.50/s  (0.796s, 1286.71/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 303 [ 900/1251 ( 72%)]  Loss: 3.241 (3.28)  Time: 0.782s, 1308.66/s  (0.795s, 1287.46/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 303 [ 950/1251 ( 76%)]  Loss: 3.354 (3.28)  Time: 0.779s, 1314.72/s  (0.796s, 1287.19/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 303 [1000/1251 ( 80%)]  Loss: 3.275 (3.28)  Time: 0.781s, 1311.88/s  (0.795s, 1287.43/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 303 [1050/1251 ( 84%)]  Loss: 3.458 (3.29)  Time: 0.779s, 1314.59/s  (0.795s, 1288.10/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 303 [1100/1251 ( 88%)]  Loss: 3.487 (3.30)  Time: 0.807s, 1268.20/s  (0.795s, 1288.08/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 303 [1150/1251 ( 92%)]  Loss: 3.327 (3.30)  Time: 0.805s, 1271.39/s  (0.795s, 1287.33/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 303 [1200/1251 ( 96%)]  Loss: 3.101 (3.29)  Time: 0.792s, 1292.36/s  (0.795s, 1288.08/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 303 [1250/1251 (100%)]  Loss: 3.309 (3.29)  Time: 0.811s, 1262.10/s  (0.795s, 1288.64/s)  LR: 1.000e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.558 (1.558)  Loss:  0.6531 (0.6531)  Acc@1: 91.8945 (91.8945)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.172 (0.561)  Loss:  0.7410 (1.1604)  Acc@1: 86.7924 (78.4720)  Acc@5: 97.7594 (94.1920)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-290.pth.tar', 78.476000078125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-296.pth.tar', 78.47400002685546)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-303.pth.tar', 78.47199995117188)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-300.pth.tar', 78.46200002685546)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-293.pth.tar', 78.43000015625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-298.pth.tar', 78.39800005371093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-291.pth.tar', 78.39199989746093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-299.pth.tar', 78.38599997558593)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-285.pth.tar', 78.3759999243164)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-289.pth.tar', 78.36000002929687)

Train: 304 [   0/1251 (  0%)]  Loss: 3.163 (3.16)  Time: 2.294s,  446.36/s  (2.294s,  446.36/s)  LR: 1.000e-05  Data: 1.560 (1.560)
Train: 304 [  50/1251 (  4%)]  Loss: 3.657 (3.41)  Time: 0.779s, 1314.75/s  (0.836s, 1224.16/s)  LR: 1.000e-05  Data: 0.011 (0.047)
Train: 304 [ 100/1251 (  8%)]  Loss: 3.649 (3.49)  Time: 0.785s, 1303.96/s  (0.814s, 1257.99/s)  LR: 1.000e-05  Data: 0.011 (0.029)
Train: 304 [ 150/1251 ( 12%)]  Loss: 3.069 (3.38)  Time: 0.781s, 1311.82/s  (0.807s, 1268.23/s)  LR: 1.000e-05  Data: 0.011 (0.023)
Train: 304 [ 200/1251 ( 16%)]  Loss: 3.720 (3.45)  Time: 0.822s, 1245.83/s  (0.805s, 1272.51/s)  LR: 1.000e-05  Data: 0.011 (0.020)
Train: 304 [ 250/1251 ( 20%)]  Loss: 3.386 (3.44)  Time: 0.786s, 1302.38/s  (0.802s, 1277.57/s)  LR: 1.000e-05  Data: 0.012 (0.018)
Train: 304 [ 300/1251 ( 24%)]  Loss: 3.392 (3.43)  Time: 0.786s, 1302.14/s  (0.801s, 1278.44/s)  LR: 1.000e-05  Data: 0.011 (0.017)
Train: 304 [ 350/1251 ( 28%)]  Loss: 2.943 (3.37)  Time: 0.779s, 1314.53/s  (0.799s, 1281.72/s)  LR: 1.000e-05  Data: 0.011 (0.016)
Train: 304 [ 400/1251 ( 32%)]  Loss: 3.295 (3.36)  Time: 0.778s, 1315.70/s  (0.797s, 1284.06/s)  LR: 1.000e-05  Data: 0.011 (0.016)
Train: 304 [ 450/1251 ( 36%)]  Loss: 3.552 (3.38)  Time: 0.787s, 1301.09/s  (0.797s, 1285.62/s)  LR: 1.000e-05  Data: 0.011 (0.015)
Train: 304 [ 500/1251 ( 40%)]  Loss: 2.947 (3.34)  Time: 0.812s, 1261.79/s  (0.796s, 1286.90/s)  LR: 1.000e-05  Data: 0.011 (0.015)
Train: 304 [ 550/1251 ( 44%)]  Loss: 3.006 (3.31)  Time: 0.824s, 1242.09/s  (0.795s, 1287.35/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 304 [ 600/1251 ( 48%)]  Loss: 3.271 (3.31)  Time: 0.844s, 1212.77/s  (0.795s, 1287.84/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 304 [ 650/1251 ( 52%)]  Loss: 3.277 (3.31)  Time: 0.784s, 1305.47/s  (0.795s, 1288.07/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 304 [ 700/1251 ( 56%)]  Loss: 2.994 (3.29)  Time: 0.819s, 1249.56/s  (0.795s, 1288.85/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 304 [ 750/1251 ( 60%)]  Loss: 2.953 (3.27)  Time: 0.810s, 1264.73/s  (0.795s, 1287.87/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 304 [ 800/1251 ( 64%)]  Loss: 3.364 (3.27)  Time: 0.779s, 1314.40/s  (0.795s, 1288.18/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 304 [ 850/1251 ( 68%)]  Loss: 3.346 (3.28)  Time: 0.813s, 1258.92/s  (0.795s, 1288.62/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 304 [ 900/1251 ( 72%)]  Loss: 3.544 (3.29)  Time: 0.786s, 1302.76/s  (0.795s, 1288.34/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 304 [ 950/1251 ( 76%)]  Loss: 2.822 (3.27)  Time: 0.778s, 1317.03/s  (0.794s, 1289.05/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 304 [1000/1251 ( 80%)]  Loss: 3.348 (3.27)  Time: 0.778s, 1315.83/s  (0.794s, 1289.31/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 304 [1050/1251 ( 84%)]  Loss: 3.170 (3.27)  Time: 0.779s, 1314.88/s  (0.794s, 1289.29/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 304 [1100/1251 ( 88%)]  Loss: 2.952 (3.25)  Time: 0.826s, 1239.24/s  (0.794s, 1289.87/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 304 [1150/1251 ( 92%)]  Loss: 3.312 (3.26)  Time: 0.778s, 1315.60/s  (0.794s, 1289.03/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 304 [1200/1251 ( 96%)]  Loss: 3.056 (3.25)  Time: 0.829s, 1235.16/s  (0.794s, 1289.75/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 304 [1250/1251 (100%)]  Loss: 3.223 (3.25)  Time: 0.806s, 1270.22/s  (0.794s, 1289.03/s)  LR: 1.000e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.533 (1.533)  Loss:  0.7987 (0.7987)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.172 (0.555)  Loss:  0.8746 (1.3151)  Acc@1: 86.7925 (78.3440)  Acc@5: 98.1132 (94.1000)
Train: 305 [   0/1251 (  0%)]  Loss: 3.423 (3.42)  Time: 2.309s,  443.48/s  (2.309s,  443.48/s)  LR: 1.000e-05  Data: 1.574 (1.574)
Train: 305 [  50/1251 (  4%)]  Loss: 3.373 (3.40)  Time: 0.835s, 1226.05/s  (0.835s, 1226.87/s)  LR: 1.000e-05  Data: 0.012 (0.047)
Train: 305 [ 100/1251 (  8%)]  Loss: 3.669 (3.49)  Time: 0.815s, 1257.08/s  (0.818s, 1251.59/s)  LR: 1.000e-05  Data: 0.011 (0.029)
Train: 305 [ 150/1251 ( 12%)]  Loss: 3.520 (3.50)  Time: 0.812s, 1260.71/s  (0.814s, 1257.59/s)  LR: 1.000e-05  Data: 0.011 (0.023)
Train: 305 [ 200/1251 ( 16%)]  Loss: 3.367 (3.47)  Time: 0.779s, 1314.22/s  (0.809s, 1265.56/s)  LR: 1.000e-05  Data: 0.011 (0.020)
Train: 305 [ 250/1251 ( 20%)]  Loss: 3.059 (3.40)  Time: 0.780s, 1312.66/s  (0.807s, 1269.33/s)  LR: 1.000e-05  Data: 0.010 (0.018)
Train: 305 [ 300/1251 ( 24%)]  Loss: 3.254 (3.38)  Time: 0.777s, 1317.62/s  (0.804s, 1274.13/s)  LR: 1.000e-05  Data: 0.011 (0.017)
Train: 305 [ 350/1251 ( 28%)]  Loss: 3.354 (3.38)  Time: 0.812s, 1261.83/s  (0.804s, 1273.35/s)  LR: 1.000e-05  Data: 0.012 (0.016)
Train: 305 [ 400/1251 ( 32%)]  Loss: 3.356 (3.37)  Time: 0.777s, 1317.21/s  (0.803s, 1275.30/s)  LR: 1.000e-05  Data: 0.011 (0.016)
Train: 305 [ 450/1251 ( 36%)]  Loss: 3.422 (3.38)  Time: 0.780s, 1312.30/s  (0.801s, 1277.75/s)  LR: 1.000e-05  Data: 0.011 (0.015)
Train: 305 [ 500/1251 ( 40%)]  Loss: 3.246 (3.37)  Time: 0.778s, 1315.99/s  (0.801s, 1278.63/s)  LR: 1.000e-05  Data: 0.011 (0.015)
Train: 305 [ 550/1251 ( 44%)]  Loss: 3.146 (3.35)  Time: 0.779s, 1314.92/s  (0.800s, 1280.37/s)  LR: 1.000e-05  Data: 0.012 (0.014)
Train: 305 [ 600/1251 ( 48%)]  Loss: 3.377 (3.35)  Time: 0.773s, 1323.85/s  (0.798s, 1282.63/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 305 [ 650/1251 ( 52%)]  Loss: 3.291 (3.35)  Time: 0.820s, 1248.69/s  (0.798s, 1282.48/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 305 [ 700/1251 ( 56%)]  Loss: 3.305 (3.34)  Time: 0.815s, 1256.55/s  (0.799s, 1281.54/s)  LR: 1.000e-05  Data: 0.012 (0.014)
Train: 305 [ 750/1251 ( 60%)]  Loss: 3.232 (3.34)  Time: 0.822s, 1246.05/s  (0.799s, 1280.99/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 305 [ 800/1251 ( 64%)]  Loss: 3.467 (3.34)  Time: 0.778s, 1315.51/s  (0.799s, 1281.80/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 305 [ 850/1251 ( 68%)]  Loss: 3.638 (3.36)  Time: 0.814s, 1257.80/s  (0.799s, 1280.90/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 305 [ 900/1251 ( 72%)]  Loss: 3.218 (3.35)  Time: 0.777s, 1318.61/s  (0.799s, 1281.34/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 305 [ 950/1251 ( 76%)]  Loss: 3.441 (3.36)  Time: 0.837s, 1223.84/s  (0.798s, 1282.81/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 305 [1000/1251 ( 80%)]  Loss: 3.343 (3.36)  Time: 0.778s, 1316.06/s  (0.798s, 1283.77/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 305 [1050/1251 ( 84%)]  Loss: 2.940 (3.34)  Time: 0.799s, 1281.25/s  (0.797s, 1284.55/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 305 [1100/1251 ( 88%)]  Loss: 3.238 (3.33)  Time: 0.785s, 1303.75/s  (0.797s, 1285.38/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 305 [1150/1251 ( 92%)]  Loss: 3.550 (3.34)  Time: 0.779s, 1314.64/s  (0.796s, 1286.11/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 305 [1200/1251 ( 96%)]  Loss: 3.324 (3.34)  Time: 0.789s, 1297.94/s  (0.796s, 1286.88/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 305 [1250/1251 (100%)]  Loss: 3.030 (3.33)  Time: 0.820s, 1248.78/s  (0.795s, 1287.47/s)  LR: 1.000e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.541 (1.541)  Loss:  0.6689 (0.6689)  Acc@1: 92.3828 (92.3828)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.172 (0.555)  Loss:  0.7578 (1.1897)  Acc@1: 87.2641 (78.5020)  Acc@5: 98.1132 (94.2380)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-305.pth.tar', 78.50199989746093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-290.pth.tar', 78.476000078125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-296.pth.tar', 78.47400002685546)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-303.pth.tar', 78.47199995117188)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-300.pth.tar', 78.46200002685546)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-293.pth.tar', 78.43000015625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-298.pth.tar', 78.39800005371093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-291.pth.tar', 78.39199989746093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-299.pth.tar', 78.38599997558593)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-285.pth.tar', 78.3759999243164)

Train: 306 [   0/1251 (  0%)]  Loss: 3.094 (3.09)  Time: 2.340s,  437.59/s  (2.340s,  437.59/s)  LR: 1.000e-05  Data: 1.545 (1.545)
Train: 306 [  50/1251 (  4%)]  Loss: 2.869 (2.98)  Time: 0.812s, 1260.85/s  (0.830s, 1233.35/s)  LR: 1.000e-05  Data: 0.011 (0.046)
Train: 306 [ 100/1251 (  8%)]  Loss: 3.235 (3.07)  Time: 0.777s, 1318.18/s  (0.807s, 1269.15/s)  LR: 1.000e-05  Data: 0.011 (0.029)
Train: 306 [ 150/1251 ( 12%)]  Loss: 3.120 (3.08)  Time: 0.779s, 1314.82/s  (0.799s, 1281.83/s)  LR: 1.000e-05  Data: 0.011 (0.023)
Train: 306 [ 200/1251 ( 16%)]  Loss: 3.523 (3.17)  Time: 0.780s, 1312.36/s  (0.796s, 1286.20/s)  LR: 1.000e-05  Data: 0.011 (0.020)
Train: 306 [ 250/1251 ( 20%)]  Loss: 3.141 (3.16)  Time: 0.780s, 1312.12/s  (0.794s, 1289.21/s)  LR: 1.000e-05  Data: 0.011 (0.018)
Train: 306 [ 300/1251 ( 24%)]  Loss: 3.282 (3.18)  Time: 0.780s, 1312.29/s  (0.794s, 1290.41/s)  LR: 1.000e-05  Data: 0.011 (0.017)
Train: 306 [ 350/1251 ( 28%)]  Loss: 2.754 (3.13)  Time: 0.775s, 1321.49/s  (0.793s, 1291.56/s)  LR: 1.000e-05  Data: 0.011 (0.016)
Train: 306 [ 400/1251 ( 32%)]  Loss: 3.680 (3.19)  Time: 0.814s, 1257.64/s  (0.792s, 1292.31/s)  LR: 1.000e-05  Data: 0.011 (0.015)
Train: 306 [ 450/1251 ( 36%)]  Loss: 3.481 (3.22)  Time: 0.788s, 1300.00/s  (0.793s, 1291.19/s)  LR: 1.000e-05  Data: 0.011 (0.015)
Train: 306 [ 500/1251 ( 40%)]  Loss: 3.501 (3.24)  Time: 0.775s, 1320.51/s  (0.792s, 1292.52/s)  LR: 1.000e-05  Data: 0.011 (0.015)
Train: 306 [ 550/1251 ( 44%)]  Loss: 3.478 (3.26)  Time: 0.777s, 1318.61/s  (0.792s, 1293.42/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 306 [ 600/1251 ( 48%)]  Loss: 3.321 (3.27)  Time: 0.791s, 1293.86/s  (0.792s, 1292.87/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 306 [ 650/1251 ( 52%)]  Loss: 3.501 (3.28)  Time: 0.815s, 1256.33/s  (0.794s, 1289.98/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 306 [ 700/1251 ( 56%)]  Loss: 3.035 (3.27)  Time: 0.779s, 1314.78/s  (0.793s, 1290.94/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 306 [ 750/1251 ( 60%)]  Loss: 3.413 (3.28)  Time: 0.812s, 1260.84/s  (0.793s, 1290.76/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 306 [ 800/1251 ( 64%)]  Loss: 3.413 (3.28)  Time: 0.779s, 1314.48/s  (0.793s, 1291.02/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 306 [ 850/1251 ( 68%)]  Loss: 3.572 (3.30)  Time: 0.818s, 1252.33/s  (0.793s, 1290.69/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 306 [ 900/1251 ( 72%)]  Loss: 3.641 (3.32)  Time: 0.779s, 1314.86/s  (0.794s, 1289.48/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 306 [ 950/1251 ( 76%)]  Loss: 3.463 (3.33)  Time: 0.775s, 1320.53/s  (0.795s, 1288.46/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 306 [1000/1251 ( 80%)]  Loss: 3.197 (3.32)  Time: 0.778s, 1315.75/s  (0.794s, 1289.61/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 306 [1050/1251 ( 84%)]  Loss: 3.336 (3.32)  Time: 0.780s, 1312.57/s  (0.793s, 1290.59/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 306 [1100/1251 ( 88%)]  Loss: 2.776 (3.30)  Time: 0.803s, 1275.03/s  (0.794s, 1290.20/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 306 [1150/1251 ( 92%)]  Loss: 3.320 (3.30)  Time: 0.780s, 1313.10/s  (0.793s, 1290.74/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 306 [1200/1251 ( 96%)]  Loss: 3.236 (3.30)  Time: 0.823s, 1244.09/s  (0.793s, 1290.65/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 306 [1250/1251 (100%)]  Loss: 3.404 (3.30)  Time: 0.767s, 1334.24/s  (0.793s, 1291.41/s)  LR: 1.000e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.553 (1.553)  Loss:  0.6662 (0.6662)  Acc@1: 91.6992 (91.6992)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.172 (0.573)  Loss:  0.7541 (1.1776)  Acc@1: 87.1462 (78.4580)  Acc@5: 98.2311 (94.1980)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-305.pth.tar', 78.50199989746093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-290.pth.tar', 78.476000078125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-296.pth.tar', 78.47400002685546)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-303.pth.tar', 78.47199995117188)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-300.pth.tar', 78.46200002685546)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-306.pth.tar', 78.45799984619141)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-293.pth.tar', 78.43000015625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-298.pth.tar', 78.39800005371093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-291.pth.tar', 78.39199989746093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-299.pth.tar', 78.38599997558593)

Train: 307 [   0/1251 (  0%)]  Loss: 3.421 (3.42)  Time: 2.285s,  448.06/s  (2.285s,  448.06/s)  LR: 1.000e-05  Data: 1.548 (1.548)
Train: 307 [  50/1251 (  4%)]  Loss: 3.424 (3.42)  Time: 0.779s, 1313.82/s  (0.837s, 1223.59/s)  LR: 1.000e-05  Data: 0.011 (0.046)
Train: 307 [ 100/1251 (  8%)]  Loss: 3.094 (3.31)  Time: 0.777s, 1318.02/s  (0.810s, 1264.24/s)  LR: 1.000e-05  Data: 0.011 (0.029)
Train: 307 [ 150/1251 ( 12%)]  Loss: 3.056 (3.25)  Time: 0.779s, 1314.14/s  (0.803s, 1275.19/s)  LR: 1.000e-05  Data: 0.011 (0.023)
Train: 307 [ 200/1251 ( 16%)]  Loss: 3.368 (3.27)  Time: 0.833s, 1229.20/s  (0.800s, 1279.39/s)  LR: 1.000e-05  Data: 0.011 (0.020)
Train: 307 [ 250/1251 ( 20%)]  Loss: 2.793 (3.19)  Time: 0.815s, 1256.78/s  (0.797s, 1284.40/s)  LR: 1.000e-05  Data: 0.011 (0.018)
Train: 307 [ 300/1251 ( 24%)]  Loss: 3.245 (3.20)  Time: 0.778s, 1315.81/s  (0.801s, 1278.25/s)  LR: 1.000e-05  Data: 0.011 (0.017)
Train: 307 [ 350/1251 ( 28%)]  Loss: 3.243 (3.21)  Time: 0.778s, 1316.17/s  (0.801s, 1277.97/s)  LR: 1.000e-05  Data: 0.011 (0.016)
Train: 307 [ 400/1251 ( 32%)]  Loss: 3.222 (3.21)  Time: 0.778s, 1316.05/s  (0.800s, 1279.95/s)  LR: 1.000e-05  Data: 0.011 (0.015)
Train: 307 [ 450/1251 ( 36%)]  Loss: 3.177 (3.20)  Time: 0.777s, 1318.07/s  (0.799s, 1281.71/s)  LR: 1.000e-05  Data: 0.011 (0.015)
Train: 307 [ 500/1251 ( 40%)]  Loss: 3.547 (3.24)  Time: 0.785s, 1304.31/s  (0.797s, 1284.43/s)  LR: 1.000e-05  Data: 0.011 (0.015)
Train: 307 [ 550/1251 ( 44%)]  Loss: 3.494 (3.26)  Time: 0.778s, 1316.35/s  (0.797s, 1285.21/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 307 [ 600/1251 ( 48%)]  Loss: 3.278 (3.26)  Time: 0.783s, 1307.54/s  (0.796s, 1287.07/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 307 [ 650/1251 ( 52%)]  Loss: 3.376 (3.27)  Time: 0.815s, 1256.02/s  (0.795s, 1287.32/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 307 [ 700/1251 ( 56%)]  Loss: 3.027 (3.25)  Time: 0.782s, 1309.56/s  (0.795s, 1287.48/s)  LR: 1.000e-05  Data: 0.016 (0.014)
Train: 307 [ 750/1251 ( 60%)]  Loss: 2.919 (3.23)  Time: 0.777s, 1318.48/s  (0.796s, 1286.07/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 307 [ 800/1251 ( 64%)]  Loss: 3.307 (3.23)  Time: 0.813s, 1260.14/s  (0.797s, 1284.55/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 307 [ 850/1251 ( 68%)]  Loss: 3.238 (3.23)  Time: 0.818s, 1251.83/s  (0.798s, 1282.54/s)  LR: 1.000e-05  Data: 0.014 (0.013)
Train: 307 [ 900/1251 ( 72%)]  Loss: 2.814 (3.21)  Time: 0.777s, 1318.11/s  (0.799s, 1282.21/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 307 [ 950/1251 ( 76%)]  Loss: 3.342 (3.22)  Time: 0.793s, 1292.11/s  (0.798s, 1283.67/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 307 [1000/1251 ( 80%)]  Loss: 3.277 (3.22)  Time: 0.813s, 1260.26/s  (0.798s, 1283.29/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 307 [1050/1251 ( 84%)]  Loss: 2.987 (3.21)  Time: 0.783s, 1307.46/s  (0.798s, 1283.11/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 307 [1100/1251 ( 88%)]  Loss: 2.813 (3.19)  Time: 0.777s, 1317.98/s  (0.798s, 1282.83/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 307 [1150/1251 ( 92%)]  Loss: 3.416 (3.20)  Time: 0.784s, 1306.77/s  (0.798s, 1282.83/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 307 [1200/1251 ( 96%)]  Loss: 3.341 (3.21)  Time: 0.779s, 1314.31/s  (0.798s, 1283.86/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 307 [1250/1251 (100%)]  Loss: 3.243 (3.21)  Time: 0.827s, 1237.65/s  (0.797s, 1284.44/s)  LR: 1.000e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.516 (1.516)  Loss:  0.6629 (0.6629)  Acc@1: 91.6016 (91.6016)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.172 (0.562)  Loss:  0.7530 (1.1702)  Acc@1: 87.5000 (78.5300)  Acc@5: 97.7594 (94.1920)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-307.pth.tar', 78.53)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-305.pth.tar', 78.50199989746093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-290.pth.tar', 78.476000078125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-296.pth.tar', 78.47400002685546)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-303.pth.tar', 78.47199995117188)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-300.pth.tar', 78.46200002685546)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-306.pth.tar', 78.45799984619141)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-293.pth.tar', 78.43000015625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-298.pth.tar', 78.39800005371093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-291.pth.tar', 78.39199989746093)

Train: 308 [   0/1251 (  0%)]  Loss: 3.369 (3.37)  Time: 2.262s,  452.76/s  (2.262s,  452.76/s)  LR: 1.000e-05  Data: 1.527 (1.527)
Train: 308 [  50/1251 (  4%)]  Loss: 2.834 (3.10)  Time: 0.820s, 1249.01/s  (0.830s, 1234.38/s)  LR: 1.000e-05  Data: 0.012 (0.044)
Train: 308 [ 100/1251 (  8%)]  Loss: 3.409 (3.20)  Time: 0.779s, 1314.10/s  (0.811s, 1261.88/s)  LR: 1.000e-05  Data: 0.011 (0.028)
Train: 308 [ 150/1251 ( 12%)]  Loss: 3.116 (3.18)  Time: 0.778s, 1316.72/s  (0.803s, 1275.50/s)  LR: 1.000e-05  Data: 0.010 (0.022)
Train: 308 [ 200/1251 ( 16%)]  Loss: 3.339 (3.21)  Time: 0.780s, 1312.37/s  (0.798s, 1282.77/s)  LR: 1.000e-05  Data: 0.011 (0.020)
Train: 308 [ 250/1251 ( 20%)]  Loss: 2.860 (3.15)  Time: 0.777s, 1318.67/s  (0.795s, 1287.62/s)  LR: 1.000e-05  Data: 0.011 (0.018)
Train: 308 [ 300/1251 ( 24%)]  Loss: 3.257 (3.17)  Time: 0.780s, 1313.13/s  (0.793s, 1291.58/s)  LR: 1.000e-05  Data: 0.011 (0.017)
Train: 308 [ 350/1251 ( 28%)]  Loss: 3.231 (3.18)  Time: 0.781s, 1311.73/s  (0.792s, 1293.73/s)  LR: 1.000e-05  Data: 0.011 (0.016)
Train: 308 [ 400/1251 ( 32%)]  Loss: 3.184 (3.18)  Time: 0.776s, 1319.73/s  (0.790s, 1295.63/s)  LR: 1.000e-05  Data: 0.011 (0.015)
Train: 308 [ 450/1251 ( 36%)]  Loss: 3.238 (3.18)  Time: 0.775s, 1321.67/s  (0.791s, 1295.31/s)  LR: 1.000e-05  Data: 0.011 (0.015)
Train: 308 [ 500/1251 ( 40%)]  Loss: 3.214 (3.19)  Time: 0.779s, 1314.42/s  (0.791s, 1295.17/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 308 [ 550/1251 ( 44%)]  Loss: 3.435 (3.21)  Time: 0.779s, 1314.01/s  (0.791s, 1294.01/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 308 [ 600/1251 ( 48%)]  Loss: 3.405 (3.22)  Time: 0.776s, 1319.58/s  (0.790s, 1295.59/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 308 [ 650/1251 ( 52%)]  Loss: 3.535 (3.24)  Time: 0.779s, 1313.86/s  (0.790s, 1295.70/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 308 [ 700/1251 ( 56%)]  Loss: 3.323 (3.25)  Time: 0.780s, 1312.88/s  (0.790s, 1296.38/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 308 [ 750/1251 ( 60%)]  Loss: 3.066 (3.24)  Time: 0.777s, 1318.64/s  (0.790s, 1295.85/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 308 [ 800/1251 ( 64%)]  Loss: 2.988 (3.22)  Time: 0.814s, 1257.94/s  (0.791s, 1295.09/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 308 [ 850/1251 ( 68%)]  Loss: 3.259 (3.23)  Time: 0.809s, 1265.12/s  (0.791s, 1294.12/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 308 [ 900/1251 ( 72%)]  Loss: 3.160 (3.22)  Time: 0.814s, 1258.22/s  (0.791s, 1293.85/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 308 [ 950/1251 ( 76%)]  Loss: 3.168 (3.22)  Time: 0.777s, 1317.65/s  (0.792s, 1292.47/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 308 [1000/1251 ( 80%)]  Loss: 3.381 (3.23)  Time: 0.778s, 1315.92/s  (0.792s, 1292.96/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 308 [1050/1251 ( 84%)]  Loss: 3.209 (3.23)  Time: 0.827s, 1238.19/s  (0.792s, 1292.44/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 308 [1100/1251 ( 88%)]  Loss: 3.296 (3.23)  Time: 0.801s, 1277.71/s  (0.792s, 1292.22/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 308 [1150/1251 ( 92%)]  Loss: 3.234 (3.23)  Time: 0.780s, 1312.99/s  (0.793s, 1291.76/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 308 [1200/1251 ( 96%)]  Loss: 3.506 (3.24)  Time: 0.781s, 1311.89/s  (0.792s, 1292.41/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 308 [1250/1251 (100%)]  Loss: 3.417 (3.25)  Time: 0.806s, 1271.03/s  (0.793s, 1291.35/s)  LR: 1.000e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.600 (1.600)  Loss:  0.7315 (0.7315)  Acc@1: 91.7969 (91.7969)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.172 (0.570)  Loss:  0.8079 (1.2309)  Acc@1: 86.7925 (78.3540)  Acc@5: 97.9953 (94.1760)
Train: 309 [   0/1251 (  0%)]  Loss: 2.861 (2.86)  Time: 2.227s,  459.82/s  (2.227s,  459.82/s)  LR: 1.000e-05  Data: 1.492 (1.492)
Train: 309 [  50/1251 (  4%)]  Loss: 3.291 (3.08)  Time: 0.835s, 1226.67/s  (0.839s, 1220.03/s)  LR: 1.000e-05  Data: 0.011 (0.046)
Train: 309 [ 100/1251 (  8%)]  Loss: 3.427 (3.19)  Time: 0.779s, 1314.43/s  (0.815s, 1256.86/s)  LR: 1.000e-05  Data: 0.012 (0.029)
Train: 309 [ 150/1251 ( 12%)]  Loss: 3.369 (3.24)  Time: 0.813s, 1259.09/s  (0.808s, 1268.06/s)  LR: 1.000e-05  Data: 0.011 (0.023)
Train: 309 [ 200/1251 ( 16%)]  Loss: 2.883 (3.17)  Time: 0.785s, 1304.12/s  (0.803s, 1275.95/s)  LR: 1.000e-05  Data: 0.010 (0.020)
Train: 309 [ 250/1251 ( 20%)]  Loss: 3.587 (3.24)  Time: 0.778s, 1315.83/s  (0.800s, 1279.95/s)  LR: 1.000e-05  Data: 0.011 (0.018)
Train: 309 [ 300/1251 ( 24%)]  Loss: 3.497 (3.27)  Time: 0.786s, 1303.47/s  (0.798s, 1283.86/s)  LR: 1.000e-05  Data: 0.011 (0.017)
Train: 309 [ 350/1251 ( 28%)]  Loss: 3.827 (3.34)  Time: 0.779s, 1314.70/s  (0.797s, 1285.55/s)  LR: 1.000e-05  Data: 0.011 (0.016)
Train: 309 [ 400/1251 ( 32%)]  Loss: 2.614 (3.26)  Time: 0.780s, 1312.35/s  (0.795s, 1288.34/s)  LR: 1.000e-05  Data: 0.011 (0.016)
Train: 309 [ 450/1251 ( 36%)]  Loss: 3.021 (3.24)  Time: 0.824s, 1242.76/s  (0.795s, 1287.56/s)  LR: 1.000e-05  Data: 0.011 (0.015)
Train: 309 [ 500/1251 ( 40%)]  Loss: 3.062 (3.22)  Time: 0.779s, 1314.86/s  (0.795s, 1288.01/s)  LR: 1.000e-05  Data: 0.011 (0.015)
Train: 309 [ 550/1251 ( 44%)]  Loss: 3.412 (3.24)  Time: 0.779s, 1314.43/s  (0.795s, 1287.65/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 309 [ 600/1251 ( 48%)]  Loss: 3.360 (3.25)  Time: 0.778s, 1316.43/s  (0.795s, 1288.37/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 309 [ 650/1251 ( 52%)]  Loss: 3.504 (3.27)  Time: 0.823s, 1244.19/s  (0.796s, 1286.91/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 309 [ 700/1251 ( 56%)]  Loss: 3.243 (3.26)  Time: 0.817s, 1253.83/s  (0.796s, 1286.21/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 309 [ 750/1251 ( 60%)]  Loss: 3.341 (3.27)  Time: 0.779s, 1314.18/s  (0.796s, 1285.68/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 309 [ 800/1251 ( 64%)]  Loss: 3.290 (3.27)  Time: 0.778s, 1316.49/s  (0.796s, 1286.73/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 309 [ 850/1251 ( 68%)]  Loss: 3.249 (3.27)  Time: 0.818s, 1251.09/s  (0.796s, 1286.13/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 309 [ 900/1251 ( 72%)]  Loss: 3.815 (3.30)  Time: 0.779s, 1315.34/s  (0.796s, 1285.84/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 309 [ 950/1251 ( 76%)]  Loss: 3.309 (3.30)  Time: 0.777s, 1318.05/s  (0.796s, 1287.07/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 309 [1000/1251 ( 80%)]  Loss: 3.213 (3.29)  Time: 0.777s, 1317.71/s  (0.795s, 1287.35/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 309 [1050/1251 ( 84%)]  Loss: 3.393 (3.30)  Time: 0.812s, 1260.54/s  (0.796s, 1286.83/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 309 [1100/1251 ( 88%)]  Loss: 3.593 (3.31)  Time: 0.777s, 1318.17/s  (0.796s, 1286.90/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 309 [1150/1251 ( 92%)]  Loss: 3.222 (3.31)  Time: 0.779s, 1314.18/s  (0.796s, 1287.09/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 309 [1200/1251 ( 96%)]  Loss: 3.561 (3.32)  Time: 0.779s, 1313.73/s  (0.795s, 1287.71/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 309 [1250/1251 (100%)]  Loss: 3.469 (3.32)  Time: 0.800s, 1279.88/s  (0.795s, 1288.60/s)  LR: 1.000e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.528 (1.528)  Loss:  0.6164 (0.6164)  Acc@1: 91.6016 (91.6016)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.172 (0.558)  Loss:  0.7029 (1.1146)  Acc@1: 86.9104 (78.5560)  Acc@5: 98.2311 (94.2800)
Current checkpoints:
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-309.pth.tar', 78.55600000244141)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-307.pth.tar', 78.53)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-305.pth.tar', 78.50199989746093)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-290.pth.tar', 78.476000078125)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-296.pth.tar', 78.47400002685546)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-303.pth.tar', 78.47199995117188)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-300.pth.tar', 78.46200002685546)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-306.pth.tar', 78.45799984619141)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-293.pth.tar', 78.43000015625)
 ('./output/train/20220201-234832-hrnet18-224/checkpoint-298.pth.tar', 78.39800005371093)

*** Best metric: 78.55600000244141 (epoch 309)
