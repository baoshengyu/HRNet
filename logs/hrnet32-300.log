/home/baoshengyu/anaconda3/lib/python3.7/site-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 6
Added key: store_based_barrier_key:1 to store for rank: 5
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 7
Added key: store_based_barrier_key:1 to store for rank: 4
Added key: store_based_barrier_key:1 to store for rank: 3
Added key: store_based_barrier_key:1 to store for rank: 1
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 7, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 4, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 5, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 6, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 8.
Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 8.
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 8.
Model hrnet32 created, param count:41237832
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.875
AMP not enabled. Training in float32.
Using native Torch DistributedDataParallel.
Scheduled epochs: 310
Train: 0 [   0/1251 (  0%)]  Loss: 6.934 (6.93)  Time: 9.639s,  106.23/s  (9.639s,  106.23/s)  LR: 1.000e-06  Data: 1.875 (1.875)
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Train: 0 [  50/1251 (  4%)]  Loss: 6.939 (6.94)  Time: 0.995s, 1028.92/s  (1.211s,  845.35/s)  LR: 1.000e-06  Data: 0.011 (0.048)
Train: 0 [ 100/1251 (  8%)]  Loss: 6.948 (6.94)  Time: 0.993s, 1030.75/s  (1.114s,  919.43/s)  LR: 1.000e-06  Data: 0.011 (0.030)
Train: 0 [ 150/1251 ( 12%)]  Loss: 6.930 (6.94)  Time: 0.996s, 1027.99/s  (1.081s,  947.68/s)  LR: 1.000e-06  Data: 0.011 (0.024)
Train: 0 [ 200/1251 ( 16%)]  Loss: 6.931 (6.94)  Time: 0.999s, 1025.45/s  (1.063s,  963.51/s)  LR: 1.000e-06  Data: 0.012 (0.021)
Train: 0 [ 250/1251 ( 20%)]  Loss: 6.910 (6.93)  Time: 1.039s,  985.31/s  (1.052s,  973.47/s)  LR: 1.000e-06  Data: 0.012 (0.019)
Train: 0 [ 300/1251 ( 24%)]  Loss: 6.940 (6.93)  Time: 1.001s, 1022.49/s  (1.045s,  980.16/s)  LR: 1.000e-06  Data: 0.011 (0.018)
Train: 0 [ 350/1251 ( 28%)]  Loss: 6.928 (6.93)  Time: 1.044s,  980.71/s  (1.039s,  985.67/s)  LR: 1.000e-06  Data: 0.011 (0.017)
Train: 0 [ 400/1251 ( 32%)]  Loss: 6.935 (6.93)  Time: 0.999s, 1025.18/s  (1.038s,  986.34/s)  LR: 1.000e-06  Data: 0.013 (0.016)
Train: 0 [ 450/1251 ( 36%)]  Loss: 6.919 (6.93)  Time: 1.031s,  993.41/s  (1.035s,  989.81/s)  LR: 1.000e-06  Data: 0.011 (0.016)
Train: 0 [ 500/1251 ( 40%)]  Loss: 6.917 (6.93)  Time: 0.996s, 1028.09/s  (1.031s,  992.97/s)  LR: 1.000e-06  Data: 0.011 (0.015)
Train: 0 [ 550/1251 ( 44%)]  Loss: 6.907 (6.93)  Time: 0.996s, 1028.16/s  (1.028s,  995.83/s)  LR: 1.000e-06  Data: 0.012 (0.015)
Train: 0 [ 600/1251 ( 48%)]  Loss: 6.911 (6.93)  Time: 0.998s, 1026.22/s  (1.027s,  996.98/s)  LR: 1.000e-06  Data: 0.011 (0.015)
Train: 0 [ 650/1251 ( 52%)]  Loss: 6.913 (6.93)  Time: 0.997s, 1026.65/s  (1.026s,  998.28/s)  LR: 1.000e-06  Data: 0.011 (0.014)
Train: 0 [ 700/1251 ( 56%)]  Loss: 6.914 (6.93)  Time: 0.995s, 1029.04/s  (1.025s,  998.84/s)  LR: 1.000e-06  Data: 0.011 (0.014)
Train: 0 [ 750/1251 ( 60%)]  Loss: 6.911 (6.92)  Time: 0.997s, 1027.04/s  (1.024s, 1000.01/s)  LR: 1.000e-06  Data: 0.011 (0.014)
Train: 0 [ 800/1251 ( 64%)]  Loss: 6.916 (6.92)  Time: 0.998s, 1026.27/s  (1.023s, 1001.11/s)  LR: 1.000e-06  Data: 0.010 (0.014)
Train: 0 [ 850/1251 ( 68%)]  Loss: 6.905 (6.92)  Time: 0.995s, 1029.63/s  (1.022s, 1002.09/s)  LR: 1.000e-06  Data: 0.010 (0.014)
Train: 0 [ 900/1251 ( 72%)]  Loss: 6.912 (6.92)  Time: 0.996s, 1028.01/s  (1.021s, 1003.22/s)  LR: 1.000e-06  Data: 0.012 (0.013)
Train: 0 [ 950/1251 ( 76%)]  Loss: 6.922 (6.92)  Time: 0.995s, 1029.10/s  (1.021s, 1003.07/s)  LR: 1.000e-06  Data: 0.012 (0.013)
Train: 0 [1000/1251 ( 80%)]  Loss: 6.918 (6.92)  Time: 1.020s, 1003.78/s  (1.020s, 1003.67/s)  LR: 1.000e-06  Data: 0.012 (0.013)
Train: 0 [1050/1251 ( 84%)]  Loss: 6.917 (6.92)  Time: 1.053s,  972.10/s  (1.020s, 1003.77/s)  LR: 1.000e-06  Data: 0.012 (0.013)
Train: 0 [1100/1251 ( 88%)]  Loss: 6.924 (6.92)  Time: 0.994s, 1030.00/s  (1.020s, 1004.07/s)  LR: 1.000e-06  Data: 0.011 (0.013)
Train: 0 [1150/1251 ( 92%)]  Loss: 6.916 (6.92)  Time: 0.999s, 1024.58/s  (1.019s, 1004.74/s)  LR: 1.000e-06  Data: 0.012 (0.013)
Train: 0 [1200/1251 ( 96%)]  Loss: 6.910 (6.92)  Time: 1.034s,  989.98/s  (1.019s, 1005.38/s)  LR: 1.000e-06  Data: 0.011 (0.013)
Train: 0 [1250/1251 (100%)]  Loss: 6.911 (6.92)  Time: 0.984s, 1040.62/s  (1.018s, 1005.94/s)  LR: 1.000e-06  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.018 (2.018)  Loss:  6.8192 (6.8192)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  3.1250 ( 3.1250)
Test: [  48/48]  Time: 2.794 (0.634)  Loss:  6.8444 (6.9015)  Acc@1:  0.9434 ( 0.1680)  Acc@5:  3.0660 ( 0.7680)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-0.pth.tar', 0.1679999987220764)

Train: 1 [   0/1251 (  0%)]  Loss: 6.920 (6.92)  Time: 2.574s,  397.81/s  (2.574s,  397.81/s)  LR: 2.008e-04  Data: 1.584 (1.584)
Train: 1 [  50/1251 (  4%)]  Loss: 6.915 (6.92)  Time: 0.996s, 1028.28/s  (1.050s,  975.08/s)  LR: 2.008e-04  Data: 0.012 (0.045)
Train: 1 [ 100/1251 (  8%)]  Loss: 6.917 (6.92)  Time: 0.995s, 1029.13/s  (1.037s,  987.51/s)  LR: 2.008e-04  Data: 0.011 (0.028)
Train: 1 [ 150/1251 ( 12%)]  Loss: 6.881 (6.91)  Time: 1.044s,  981.01/s  (1.035s,  989.27/s)  LR: 2.008e-04  Data: 0.011 (0.022)
Train: 1 [ 200/1251 ( 16%)]  Loss: 6.884 (6.90)  Time: 1.042s,  982.74/s  (1.030s,  993.90/s)  LR: 2.008e-04  Data: 0.012 (0.020)
Train: 1 [ 250/1251 ( 20%)]  Loss: 6.861 (6.90)  Time: 1.033s,  990.82/s  (1.028s,  995.68/s)  LR: 2.008e-04  Data: 0.011 (0.018)
Train: 1 [ 300/1251 ( 24%)]  Loss: 6.820 (6.89)  Time: 1.037s,  987.78/s  (1.027s,  997.16/s)  LR: 2.008e-04  Data: 0.011 (0.017)
Train: 1 [ 350/1251 ( 28%)]  Loss: 6.755 (6.87)  Time: 1.059s,  966.97/s  (1.025s,  998.79/s)  LR: 2.008e-04  Data: 0.011 (0.016)
Train: 1 [ 400/1251 ( 32%)]  Loss: 6.766 (6.86)  Time: 0.996s, 1028.22/s  (1.024s,  999.71/s)  LR: 2.008e-04  Data: 0.011 (0.015)
Train: 1 [ 450/1251 ( 36%)]  Loss: 6.731 (6.84)  Time: 0.997s, 1027.28/s  (1.022s, 1001.61/s)  LR: 2.008e-04  Data: 0.011 (0.015)
Train: 1 [ 500/1251 ( 40%)]  Loss: 6.698 (6.83)  Time: 0.995s, 1028.89/s  (1.021s, 1002.71/s)  LR: 2.008e-04  Data: 0.012 (0.015)
Train: 1 [ 550/1251 ( 44%)]  Loss: 6.768 (6.83)  Time: 1.039s,  985.62/s  (1.021s, 1002.71/s)  LR: 2.008e-04  Data: 0.011 (0.014)
Train: 1 [ 600/1251 ( 48%)]  Loss: 6.684 (6.82)  Time: 1.038s,  986.14/s  (1.021s, 1003.34/s)  LR: 2.008e-04  Data: 0.011 (0.014)
Train: 1 [ 650/1251 ( 52%)]  Loss: 6.730 (6.81)  Time: 1.060s,  965.88/s  (1.021s, 1003.07/s)  LR: 2.008e-04  Data: 0.011 (0.014)
Train: 1 [ 700/1251 ( 56%)]  Loss: 6.673 (6.80)  Time: 1.003s, 1020.68/s  (1.020s, 1003.55/s)  LR: 2.008e-04  Data: 0.010 (0.014)
Train: 1 [ 750/1251 ( 60%)]  Loss: 6.665 (6.79)  Time: 1.043s,  981.54/s  (1.020s, 1004.27/s)  LR: 2.008e-04  Data: 0.011 (0.013)
Train: 1 [ 800/1251 ( 64%)]  Loss: 6.663 (6.78)  Time: 1.059s,  966.58/s  (1.020s, 1004.29/s)  LR: 2.008e-04  Data: 0.011 (0.013)
Train: 1 [ 850/1251 ( 68%)]  Loss: 6.602 (6.77)  Time: 1.002s, 1022.24/s  (1.019s, 1004.60/s)  LR: 2.008e-04  Data: 0.011 (0.013)
Train: 1 [ 900/1251 ( 72%)]  Loss: 6.561 (6.76)  Time: 0.998s, 1025.84/s  (1.019s, 1004.92/s)  LR: 2.008e-04  Data: 0.011 (0.013)
Train: 1 [ 950/1251 ( 76%)]  Loss: 6.620 (6.76)  Time: 0.999s, 1024.83/s  (1.019s, 1004.93/s)  LR: 2.008e-04  Data: 0.010 (0.013)
Train: 1 [1000/1251 ( 80%)]  Loss: 6.576 (6.75)  Time: 1.000s, 1024.19/s  (1.019s, 1005.20/s)  LR: 2.008e-04  Data: 0.014 (0.013)
Train: 1 [1050/1251 ( 84%)]  Loss: 6.652 (6.74)  Time: 1.086s,  942.72/s  (1.018s, 1005.68/s)  LR: 2.008e-04  Data: 0.012 (0.013)
Train: 1 [1100/1251 ( 88%)]  Loss: 6.450 (6.73)  Time: 1.029s,  995.45/s  (1.018s, 1006.05/s)  LR: 2.008e-04  Data: 0.013 (0.013)
Train: 1 [1150/1251 ( 92%)]  Loss: 6.580 (6.72)  Time: 1.044s,  981.06/s  (1.018s, 1006.25/s)  LR: 2.008e-04  Data: 0.011 (0.013)
Train: 1 [1200/1251 ( 96%)]  Loss: 6.531 (6.72)  Time: 1.043s,  982.02/s  (1.017s, 1006.57/s)  LR: 2.008e-04  Data: 0.011 (0.013)
Train: 1 [1250/1251 (100%)]  Loss: 6.478 (6.71)  Time: 0.983s, 1041.52/s  (1.017s, 1006.86/s)  LR: 2.008e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.644 (1.644)  Loss:  5.6848 (5.6848)  Acc@1:  0.7812 ( 0.7812)  Acc@5: 14.7461 (14.7461)
Test: [  48/48]  Time: 0.245 (0.565)  Loss:  5.3018 (5.8356)  Acc@1: 13.9151 ( 2.8140)  Acc@5: 26.1792 (10.0280)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-1.pth.tar', 2.8140000006103514)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-0.pth.tar', 0.1679999987220764)

Train: 2 [   0/1251 (  0%)]  Loss: 6.479 (6.48)  Time: 2.493s,  410.78/s  (2.493s,  410.78/s)  LR: 4.006e-04  Data: 1.530 (1.530)
Train: 2 [  50/1251 (  4%)]  Loss: 6.634 (6.56)  Time: 0.995s, 1028.68/s  (1.039s,  985.51/s)  LR: 4.006e-04  Data: 0.012 (0.044)
Train: 2 [ 100/1251 (  8%)]  Loss: 6.407 (6.51)  Time: 1.060s,  965.62/s  (1.027s,  997.50/s)  LR: 4.006e-04  Data: 0.010 (0.028)
Train: 2 [ 150/1251 ( 12%)]  Loss: 6.493 (6.50)  Time: 0.995s, 1028.65/s  (1.025s,  999.19/s)  LR: 4.006e-04  Data: 0.011 (0.022)
Train: 2 [ 200/1251 ( 16%)]  Loss: 6.514 (6.51)  Time: 0.994s, 1030.48/s  (1.025s,  998.63/s)  LR: 4.006e-04  Data: 0.011 (0.019)
Train: 2 [ 250/1251 ( 20%)]  Loss: 6.514 (6.51)  Time: 0.997s, 1027.22/s  (1.021s, 1002.84/s)  LR: 4.006e-04  Data: 0.011 (0.018)
Train: 2 [ 300/1251 ( 24%)]  Loss: 6.418 (6.49)  Time: 0.996s, 1028.39/s  (1.019s, 1005.31/s)  LR: 4.006e-04  Data: 0.011 (0.017)
Train: 2 [ 350/1251 ( 28%)]  Loss: 6.368 (6.48)  Time: 0.998s, 1026.33/s  (1.016s, 1007.56/s)  LR: 4.006e-04  Data: 0.011 (0.016)
Train: 2 [ 400/1251 ( 32%)]  Loss: 6.483 (6.48)  Time: 1.049s,  976.39/s  (1.016s, 1007.61/s)  LR: 4.006e-04  Data: 0.012 (0.015)
Train: 2 [ 450/1251 ( 36%)]  Loss: 6.358 (6.47)  Time: 1.002s, 1022.12/s  (1.015s, 1009.20/s)  LR: 4.006e-04  Data: 0.010 (0.015)
Train: 2 [ 500/1251 ( 40%)]  Loss: 6.467 (6.47)  Time: 1.023s, 1000.83/s  (1.016s, 1007.80/s)  LR: 4.006e-04  Data: 0.010 (0.014)
Train: 2 [ 550/1251 ( 44%)]  Loss: 6.287 (6.45)  Time: 0.998s, 1026.56/s  (1.015s, 1009.22/s)  LR: 4.006e-04  Data: 0.011 (0.014)
Train: 2 [ 600/1251 ( 48%)]  Loss: 6.265 (6.44)  Time: 0.995s, 1029.51/s  (1.014s, 1009.96/s)  LR: 4.006e-04  Data: 0.010 (0.014)
Train: 2 [ 650/1251 ( 52%)]  Loss: 6.306 (6.43)  Time: 0.997s, 1026.69/s  (1.014s, 1010.27/s)  LR: 4.006e-04  Data: 0.011 (0.014)
Train: 2 [ 700/1251 ( 56%)]  Loss: 6.398 (6.43)  Time: 0.996s, 1027.77/s  (1.015s, 1009.03/s)  LR: 4.006e-04  Data: 0.011 (0.013)
Train: 2 [ 750/1251 ( 60%)]  Loss: 6.304 (6.42)  Time: 0.995s, 1028.80/s  (1.015s, 1009.30/s)  LR: 4.006e-04  Data: 0.010 (0.013)
Train: 2 [ 800/1251 ( 64%)]  Loss: 6.262 (6.41)  Time: 0.996s, 1028.23/s  (1.015s, 1008.38/s)  LR: 4.006e-04  Data: 0.011 (0.013)
Train: 2 [ 850/1251 ( 68%)]  Loss: 6.124 (6.39)  Time: 1.071s,  956.09/s  (1.015s, 1008.40/s)  LR: 4.006e-04  Data: 0.011 (0.013)
Train: 2 [ 900/1251 ( 72%)]  Loss: 6.261 (6.39)  Time: 1.001s, 1023.33/s  (1.015s, 1009.25/s)  LR: 4.006e-04  Data: 0.011 (0.013)
Train: 2 [ 950/1251 ( 76%)]  Loss: 6.263 (6.38)  Time: 0.995s, 1028.89/s  (1.015s, 1009.05/s)  LR: 4.006e-04  Data: 0.010 (0.013)
Train: 2 [1000/1251 ( 80%)]  Loss: 6.109 (6.37)  Time: 0.996s, 1027.91/s  (1.014s, 1009.73/s)  LR: 4.006e-04  Data: 0.012 (0.013)
Train: 2 [1050/1251 ( 84%)]  Loss: 6.297 (6.36)  Time: 0.995s, 1028.95/s  (1.013s, 1010.53/s)  LR: 4.006e-04  Data: 0.011 (0.013)
Train: 2 [1100/1251 ( 88%)]  Loss: 6.238 (6.36)  Time: 0.996s, 1028.21/s  (1.013s, 1010.51/s)  LR: 4.006e-04  Data: 0.011 (0.013)
Train: 2 [1150/1251 ( 92%)]  Loss: 6.191 (6.35)  Time: 0.997s, 1026.97/s  (1.013s, 1010.55/s)  LR: 4.006e-04  Data: 0.012 (0.013)
Train: 2 [1200/1251 ( 96%)]  Loss: 6.081 (6.34)  Time: 0.996s, 1027.67/s  (1.013s, 1011.03/s)  LR: 4.006e-04  Data: 0.012 (0.013)
Train: 2 [1250/1251 (100%)]  Loss: 6.144 (6.33)  Time: 0.985s, 1039.71/s  (1.012s, 1011.62/s)  LR: 4.006e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.591 (1.591)  Loss:  4.9670 (4.9670)  Acc@1:  8.4961 ( 8.4961)  Acc@5: 21.1914 (21.1914)
Test: [  48/48]  Time: 0.245 (0.578)  Loss:  4.8472 (5.3352)  Acc@1: 13.5613 ( 6.2160)  Acc@5: 27.4764 (18.0780)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-2.pth.tar', 6.216000008544922)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-1.pth.tar', 2.8140000006103514)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-0.pth.tar', 0.1679999987220764)

Train: 3 [   0/1251 (  0%)]  Loss: 6.216 (6.22)  Time: 2.453s,  417.36/s  (2.453s,  417.36/s)  LR: 6.004e-04  Data: 1.500 (1.500)
Train: 3 [  50/1251 (  4%)]  Loss: 6.211 (6.21)  Time: 0.996s, 1027.95/s  (1.032s,  992.28/s)  LR: 6.004e-04  Data: 0.011 (0.040)
Train: 3 [ 100/1251 (  8%)]  Loss: 6.198 (6.21)  Time: 0.996s, 1028.13/s  (1.016s, 1007.61/s)  LR: 6.004e-04  Data: 0.011 (0.026)
Train: 3 [ 150/1251 ( 12%)]  Loss: 6.148 (6.19)  Time: 0.998s, 1026.08/s  (1.011s, 1012.71/s)  LR: 6.004e-04  Data: 0.011 (0.021)
Train: 3 [ 200/1251 ( 16%)]  Loss: 6.275 (6.21)  Time: 0.999s, 1025.00/s  (1.009s, 1014.88/s)  LR: 6.004e-04  Data: 0.012 (0.018)
Train: 3 [ 250/1251 ( 20%)]  Loss: 6.061 (6.18)  Time: 1.049s,  975.78/s  (1.012s, 1012.10/s)  LR: 6.004e-04  Data: 0.010 (0.017)
Train: 3 [ 300/1251 ( 24%)]  Loss: 6.018 (6.16)  Time: 1.006s, 1018.00/s  (1.010s, 1013.66/s)  LR: 6.004e-04  Data: 0.012 (0.016)
Train: 3 [ 350/1251 ( 28%)]  Loss: 6.202 (6.17)  Time: 1.028s,  996.15/s  (1.009s, 1014.81/s)  LR: 6.004e-04  Data: 0.010 (0.015)
Train: 3 [ 400/1251 ( 32%)]  Loss: 6.216 (6.17)  Time: 0.998s, 1026.23/s  (1.011s, 1012.70/s)  LR: 6.004e-04  Data: 0.010 (0.015)
Train: 3 [ 450/1251 ( 36%)]  Loss: 5.899 (6.14)  Time: 0.997s, 1027.14/s  (1.011s, 1013.14/s)  LR: 6.004e-04  Data: 0.010 (0.014)
Train: 3 [ 500/1251 ( 40%)]  Loss: 6.083 (6.14)  Time: 0.994s, 1030.46/s  (1.010s, 1013.47/s)  LR: 6.004e-04  Data: 0.010 (0.014)
Train: 3 [ 550/1251 ( 44%)]  Loss: 6.073 (6.13)  Time: 1.033s,  990.89/s  (1.010s, 1013.74/s)  LR: 6.004e-04  Data: 0.011 (0.014)
Train: 3 [ 600/1251 ( 48%)]  Loss: 5.916 (6.12)  Time: 1.000s, 1023.81/s  (1.011s, 1012.75/s)  LR: 6.004e-04  Data: 0.011 (0.014)
Train: 3 [ 650/1251 ( 52%)]  Loss: 6.163 (6.12)  Time: 0.990s, 1034.48/s  (1.011s, 1012.61/s)  LR: 6.004e-04  Data: 0.010 (0.013)
Train: 3 [ 700/1251 ( 56%)]  Loss: 5.873 (6.10)  Time: 0.995s, 1029.41/s  (1.010s, 1013.42/s)  LR: 6.004e-04  Data: 0.011 (0.013)
Train: 3 [ 750/1251 ( 60%)]  Loss: 6.021 (6.10)  Time: 1.004s, 1019.81/s  (1.010s, 1013.97/s)  LR: 6.004e-04  Data: 0.011 (0.013)
Train: 3 [ 800/1251 ( 64%)]  Loss: 5.936 (6.09)  Time: 0.996s, 1028.57/s  (1.010s, 1014.19/s)  LR: 6.004e-04  Data: 0.011 (0.013)
Train: 3 [ 850/1251 ( 68%)]  Loss: 5.838 (6.07)  Time: 1.008s, 1016.12/s  (1.009s, 1014.78/s)  LR: 6.004e-04  Data: 0.012 (0.013)
Train: 3 [ 900/1251 ( 72%)]  Loss: 5.767 (6.06)  Time: 0.999s, 1025.44/s  (1.009s, 1015.16/s)  LR: 6.004e-04  Data: 0.011 (0.013)
Train: 3 [ 950/1251 ( 76%)]  Loss: 5.865 (6.05)  Time: 0.989s, 1034.89/s  (1.008s, 1015.62/s)  LR: 6.004e-04  Data: 0.011 (0.013)
Train: 3 [1000/1251 ( 80%)]  Loss: 6.026 (6.05)  Time: 1.043s,  981.40/s  (1.008s, 1015.37/s)  LR: 6.004e-04  Data: 0.010 (0.013)
Train: 3 [1050/1251 ( 84%)]  Loss: 6.125 (6.05)  Time: 0.996s, 1027.60/s  (1.009s, 1014.95/s)  LR: 6.004e-04  Data: 0.012 (0.013)
Train: 3 [1100/1251 ( 88%)]  Loss: 5.878 (6.04)  Time: 0.995s, 1029.56/s  (1.009s, 1015.27/s)  LR: 6.004e-04  Data: 0.012 (0.013)
Train: 3 [1150/1251 ( 92%)]  Loss: 5.634 (6.03)  Time: 0.998s, 1026.27/s  (1.009s, 1015.14/s)  LR: 6.004e-04  Data: 0.012 (0.012)
Train: 3 [1200/1251 ( 96%)]  Loss: 5.882 (6.02)  Time: 1.003s, 1020.91/s  (1.008s, 1015.59/s)  LR: 6.004e-04  Data: 0.012 (0.012)
Train: 3 [1250/1251 (100%)]  Loss: 5.785 (6.01)  Time: 0.984s, 1041.08/s  (1.008s, 1015.94/s)  LR: 6.004e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.608 (1.608)  Loss:  4.3418 (4.3418)  Acc@1: 17.4805 (17.4805)  Acc@5: 41.3086 (41.3086)
Test: [  48/48]  Time: 0.245 (0.568)  Loss:  3.6525 (4.4249)  Acc@1: 36.7925 (16.1460)  Acc@5: 57.1934 (37.4920)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-3.pth.tar', 16.146000080566406)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-2.pth.tar', 6.216000008544922)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-1.pth.tar', 2.8140000006103514)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-0.pth.tar', 0.1679999987220764)

Train: 4 [   0/1251 (  0%)]  Loss: 6.063 (6.06)  Time: 2.366s,  432.74/s  (2.366s,  432.74/s)  LR: 8.002e-04  Data: 1.418 (1.418)
Train: 4 [  50/1251 (  4%)]  Loss: 5.741 (5.90)  Time: 0.995s, 1028.87/s  (1.045s,  979.84/s)  LR: 8.002e-04  Data: 0.011 (0.039)
Train: 4 [ 100/1251 (  8%)]  Loss: 5.680 (5.83)  Time: 1.034s,  990.18/s  (1.024s, 1000.27/s)  LR: 8.002e-04  Data: 0.010 (0.026)
Train: 4 [ 150/1251 ( 12%)]  Loss: 5.812 (5.82)  Time: 0.999s, 1024.72/s  (1.019s, 1004.75/s)  LR: 8.002e-04  Data: 0.011 (0.021)
Train: 4 [ 200/1251 ( 16%)]  Loss: 5.649 (5.79)  Time: 0.993s, 1031.28/s  (1.015s, 1008.41/s)  LR: 8.002e-04  Data: 0.010 (0.018)
Train: 4 [ 250/1251 ( 20%)]  Loss: 5.531 (5.75)  Time: 1.007s, 1016.68/s  (1.012s, 1011.66/s)  LR: 8.002e-04  Data: 0.011 (0.017)
Train: 4 [ 300/1251 ( 24%)]  Loss: 5.994 (5.78)  Time: 1.006s, 1017.54/s  (1.011s, 1013.07/s)  LR: 8.002e-04  Data: 0.011 (0.016)
Train: 4 [ 350/1251 ( 28%)]  Loss: 5.757 (5.78)  Time: 0.995s, 1028.66/s  (1.010s, 1014.17/s)  LR: 8.002e-04  Data: 0.011 (0.015)
Train: 4 [ 400/1251 ( 32%)]  Loss: 5.832 (5.78)  Time: 0.996s, 1028.11/s  (1.008s, 1015.39/s)  LR: 8.002e-04  Data: 0.012 (0.015)
Train: 4 [ 450/1251 ( 36%)]  Loss: 5.802 (5.79)  Time: 1.008s, 1015.83/s  (1.008s, 1016.18/s)  LR: 8.002e-04  Data: 0.011 (0.014)
Train: 4 [ 500/1251 ( 40%)]  Loss: 5.899 (5.80)  Time: 0.994s, 1029.80/s  (1.007s, 1016.68/s)  LR: 8.002e-04  Data: 0.011 (0.014)
Train: 4 [ 550/1251 ( 44%)]  Loss: 5.551 (5.78)  Time: 1.062s,  964.35/s  (1.009s, 1014.67/s)  LR: 8.002e-04  Data: 0.011 (0.014)
Train: 4 [ 600/1251 ( 48%)]  Loss: 5.779 (5.78)  Time: 0.997s, 1027.40/s  (1.010s, 1014.24/s)  LR: 8.002e-04  Data: 0.012 (0.014)
Train: 4 [ 650/1251 ( 52%)]  Loss: 6.037 (5.79)  Time: 0.997s, 1027.57/s  (1.009s, 1014.96/s)  LR: 8.002e-04  Data: 0.012 (0.013)
Train: 4 [ 700/1251 ( 56%)]  Loss: 5.471 (5.77)  Time: 1.008s, 1015.86/s  (1.010s, 1013.42/s)  LR: 8.002e-04  Data: 0.011 (0.013)
Train: 4 [ 750/1251 ( 60%)]  Loss: 5.804 (5.77)  Time: 0.997s, 1026.97/s  (1.010s, 1013.80/s)  LR: 8.002e-04  Data: 0.011 (0.013)
Train: 4 [ 800/1251 ( 64%)]  Loss: 5.675 (5.77)  Time: 1.032s,  992.10/s  (1.010s, 1014.32/s)  LR: 8.002e-04  Data: 0.011 (0.013)
Train: 4 [ 850/1251 ( 68%)]  Loss: 5.808 (5.77)  Time: 0.998s, 1025.97/s  (1.010s, 1014.22/s)  LR: 8.002e-04  Data: 0.011 (0.013)
Train: 4 [ 900/1251 ( 72%)]  Loss: 5.838 (5.77)  Time: 1.011s, 1013.32/s  (1.009s, 1014.79/s)  LR: 8.002e-04  Data: 0.010 (0.013)
Train: 4 [ 950/1251 ( 76%)]  Loss: 5.748 (5.77)  Time: 0.998s, 1026.50/s  (1.009s, 1015.32/s)  LR: 8.002e-04  Data: 0.011 (0.013)
Train: 4 [1000/1251 ( 80%)]  Loss: 5.230 (5.75)  Time: 0.994s, 1029.93/s  (1.008s, 1015.83/s)  LR: 8.002e-04  Data: 0.011 (0.013)
Train: 4 [1050/1251 ( 84%)]  Loss: 5.818 (5.75)  Time: 0.998s, 1026.54/s  (1.008s, 1016.16/s)  LR: 8.002e-04  Data: 0.011 (0.012)
Train: 4 [1100/1251 ( 88%)]  Loss: 5.692 (5.75)  Time: 0.996s, 1028.12/s  (1.008s, 1016.23/s)  LR: 8.002e-04  Data: 0.011 (0.012)
Train: 4 [1150/1251 ( 92%)]  Loss: 5.432 (5.74)  Time: 0.997s, 1027.19/s  (1.007s, 1016.58/s)  LR: 8.002e-04  Data: 0.011 (0.012)
Train: 4 [1200/1251 ( 96%)]  Loss: 5.652 (5.73)  Time: 1.001s, 1022.48/s  (1.007s, 1016.84/s)  LR: 8.002e-04  Data: 0.012 (0.012)
Train: 4 [1250/1251 (100%)]  Loss: 5.642 (5.73)  Time: 1.022s, 1002.22/s  (1.008s, 1016.35/s)  LR: 8.002e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.603 (1.603)  Loss:  3.4826 (3.4826)  Acc@1: 28.0273 (28.0273)  Acc@5: 59.9609 (59.9609)
Test: [  48/48]  Time: 0.245 (0.567)  Loss:  2.5829 (3.8840)  Acc@1: 53.4198 (23.8680)  Acc@5: 73.4670 (48.7740)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-4.pth.tar', 23.86800006347656)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-3.pth.tar', 16.146000080566406)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-2.pth.tar', 6.216000008544922)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-1.pth.tar', 2.8140000006103514)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-0.pth.tar', 0.1679999987220764)

Train: 5 [   0/1251 (  0%)]  Loss: 5.509 (5.51)  Time: 2.526s,  405.41/s  (2.526s,  405.41/s)  LR: 9.993e-04  Data: 1.572 (1.572)
Train: 5 [  50/1251 (  4%)]  Loss: 5.601 (5.56)  Time: 0.997s, 1026.76/s  (1.060s,  965.63/s)  LR: 9.993e-04  Data: 0.011 (0.042)
Train: 5 [ 100/1251 (  8%)]  Loss: 5.516 (5.54)  Time: 1.005s, 1018.91/s  (1.033s,  991.36/s)  LR: 9.993e-04  Data: 0.011 (0.027)
Train: 5 [ 150/1251 ( 12%)]  Loss: 5.121 (5.44)  Time: 1.012s, 1011.94/s  (1.022s, 1001.48/s)  LR: 9.993e-04  Data: 0.011 (0.021)
Train: 5 [ 200/1251 ( 16%)]  Loss: 5.683 (5.49)  Time: 0.998s, 1025.92/s  (1.021s, 1003.42/s)  LR: 9.993e-04  Data: 0.010 (0.019)
Train: 5 [ 250/1251 ( 20%)]  Loss: 5.648 (5.51)  Time: 1.001s, 1022.86/s  (1.020s, 1004.40/s)  LR: 9.993e-04  Data: 0.010 (0.017)
Train: 5 [ 300/1251 ( 24%)]  Loss: 5.636 (5.53)  Time: 0.997s, 1027.04/s  (1.016s, 1007.79/s)  LR: 9.993e-04  Data: 0.011 (0.016)
Train: 5 [ 350/1251 ( 28%)]  Loss: 5.543 (5.53)  Time: 0.996s, 1028.27/s  (1.014s, 1009.64/s)  LR: 9.993e-04  Data: 0.012 (0.015)
Train: 5 [ 400/1251 ( 32%)]  Loss: 5.475 (5.53)  Time: 1.000s, 1023.73/s  (1.013s, 1011.07/s)  LR: 9.993e-04  Data: 0.012 (0.015)
Train: 5 [ 450/1251 ( 36%)]  Loss: 5.672 (5.54)  Time: 0.998s, 1026.26/s  (1.013s, 1011.31/s)  LR: 9.993e-04  Data: 0.011 (0.015)
Train: 5 [ 500/1251 ( 40%)]  Loss: 5.696 (5.55)  Time: 1.036s,  988.62/s  (1.011s, 1012.69/s)  LR: 9.993e-04  Data: 0.011 (0.014)
Train: 5 [ 550/1251 ( 44%)]  Loss: 5.593 (5.56)  Time: 1.068s,  958.52/s  (1.011s, 1012.54/s)  LR: 9.993e-04  Data: 0.012 (0.014)
Train: 5 [ 600/1251 ( 48%)]  Loss: 5.449 (5.55)  Time: 0.999s, 1025.38/s  (1.012s, 1011.52/s)  LR: 9.993e-04  Data: 0.012 (0.014)
Train: 5 [ 650/1251 ( 52%)]  Loss: 5.692 (5.56)  Time: 0.999s, 1024.64/s  (1.012s, 1012.27/s)  LR: 9.993e-04  Data: 0.011 (0.014)
Train: 5 [ 700/1251 ( 56%)]  Loss: 5.409 (5.55)  Time: 1.006s, 1017.63/s  (1.011s, 1013.18/s)  LR: 9.993e-04  Data: 0.011 (0.014)
Train: 5 [ 750/1251 ( 60%)]  Loss: 5.555 (5.55)  Time: 1.000s, 1023.49/s  (1.010s, 1013.98/s)  LR: 9.993e-04  Data: 0.014 (0.013)
Train: 5 [ 800/1251 ( 64%)]  Loss: 5.312 (5.54)  Time: 0.994s, 1030.12/s  (1.009s, 1014.53/s)  LR: 9.993e-04  Data: 0.011 (0.013)
Train: 5 [ 850/1251 ( 68%)]  Loss: 5.587 (5.54)  Time: 0.997s, 1027.51/s  (1.009s, 1015.19/s)  LR: 9.993e-04  Data: 0.011 (0.013)
Train: 5 [ 900/1251 ( 72%)]  Loss: 5.638 (5.54)  Time: 0.995s, 1029.33/s  (1.009s, 1014.91/s)  LR: 9.993e-04  Data: 0.010 (0.013)
Train: 5 [ 950/1251 ( 76%)]  Loss: 5.408 (5.54)  Time: 0.995s, 1029.57/s  (1.008s, 1015.49/s)  LR: 9.993e-04  Data: 0.011 (0.013)
Train: 5 [1000/1251 ( 80%)]  Loss: 5.495 (5.54)  Time: 1.001s, 1023.00/s  (1.008s, 1015.64/s)  LR: 9.993e-04  Data: 0.010 (0.013)
Train: 5 [1050/1251 ( 84%)]  Loss: 5.603 (5.54)  Time: 1.036s,  988.41/s  (1.009s, 1015.15/s)  LR: 9.993e-04  Data: 0.011 (0.013)
Train: 5 [1100/1251 ( 88%)]  Loss: 5.289 (5.53)  Time: 1.027s,  997.55/s  (1.009s, 1015.34/s)  LR: 9.993e-04  Data: 0.011 (0.013)
Train: 5 [1150/1251 ( 92%)]  Loss: 5.149 (5.51)  Time: 0.998s, 1025.57/s  (1.008s, 1015.56/s)  LR: 9.993e-04  Data: 0.011 (0.013)
Train: 5 [1200/1251 ( 96%)]  Loss: 5.362 (5.51)  Time: 1.010s, 1013.56/s  (1.009s, 1015.21/s)  LR: 9.993e-04  Data: 0.011 (0.012)
Train: 5 [1250/1251 (100%)]  Loss: 5.387 (5.50)  Time: 0.985s, 1039.82/s  (1.008s, 1015.62/s)  LR: 9.993e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.760 (1.760)  Loss:  2.5726 (2.5726)  Acc@1: 47.4609 (47.4609)  Acc@5: 75.3906 (75.3906)
Test: [  48/48]  Time: 0.245 (0.583)  Loss:  2.3334 (3.3252)  Acc@1: 52.0047 (31.3320)  Acc@5: 73.9387 (58.4780)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-5.pth.tar', 31.331999965820312)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-4.pth.tar', 23.86800006347656)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-3.pth.tar', 16.146000080566406)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-2.pth.tar', 6.216000008544922)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-1.pth.tar', 2.8140000006103514)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-0.pth.tar', 0.1679999987220764)

Train: 6 [   0/1251 (  0%)]  Loss: 5.594 (5.59)  Time: 2.403s,  426.14/s  (2.403s,  426.14/s)  LR: 9.990e-04  Data: 1.443 (1.443)
Train: 6 [  50/1251 (  4%)]  Loss: 5.274 (5.43)  Time: 0.997s, 1027.56/s  (1.051s,  973.85/s)  LR: 9.990e-04  Data: 0.011 (0.040)
Train: 6 [ 100/1251 (  8%)]  Loss: 5.113 (5.33)  Time: 1.069s,  957.77/s  (1.028s,  996.19/s)  LR: 9.990e-04  Data: 0.012 (0.026)
Train: 6 [ 150/1251 ( 12%)]  Loss: 5.615 (5.40)  Time: 0.999s, 1025.14/s  (1.025s,  999.10/s)  LR: 9.990e-04  Data: 0.010 (0.021)
Train: 6 [ 200/1251 ( 16%)]  Loss: 5.227 (5.36)  Time: 0.995s, 1028.76/s  (1.018s, 1005.65/s)  LR: 9.990e-04  Data: 0.011 (0.018)
Train: 6 [ 250/1251 ( 20%)]  Loss: 5.336 (5.36)  Time: 1.007s, 1017.12/s  (1.015s, 1009.33/s)  LR: 9.990e-04  Data: 0.012 (0.017)
Train: 6 [ 300/1251 ( 24%)]  Loss: 5.414 (5.37)  Time: 0.997s, 1026.61/s  (1.013s, 1011.06/s)  LR: 9.990e-04  Data: 0.011 (0.016)
Train: 6 [ 350/1251 ( 28%)]  Loss: 5.013 (5.32)  Time: 1.051s,  973.97/s  (1.014s, 1010.32/s)  LR: 9.990e-04  Data: 0.011 (0.015)
Train: 6 [ 400/1251 ( 32%)]  Loss: 5.129 (5.30)  Time: 0.994s, 1030.09/s  (1.012s, 1011.90/s)  LR: 9.990e-04  Data: 0.011 (0.015)
Train: 6 [ 450/1251 ( 36%)]  Loss: 5.367 (5.31)  Time: 1.049s,  976.44/s  (1.011s, 1012.46/s)  LR: 9.990e-04  Data: 0.011 (0.014)
Train: 6 [ 500/1251 ( 40%)]  Loss: 4.820 (5.26)  Time: 0.996s, 1027.98/s  (1.010s, 1013.68/s)  LR: 9.990e-04  Data: 0.011 (0.014)
Train: 6 [ 550/1251 ( 44%)]  Loss: 5.448 (5.28)  Time: 0.995s, 1028.94/s  (1.009s, 1014.83/s)  LR: 9.990e-04  Data: 0.011 (0.014)
Train: 6 [ 600/1251 ( 48%)]  Loss: 5.099 (5.27)  Time: 0.997s, 1027.20/s  (1.008s, 1015.54/s)  LR: 9.990e-04  Data: 0.011 (0.014)
Train: 6 [ 650/1251 ( 52%)]  Loss: 5.308 (5.27)  Time: 0.997s, 1027.31/s  (1.009s, 1015.24/s)  LR: 9.990e-04  Data: 0.011 (0.013)
Train: 6 [ 700/1251 ( 56%)]  Loss: 5.405 (5.28)  Time: 1.052s,  973.46/s  (1.008s, 1015.86/s)  LR: 9.990e-04  Data: 0.011 (0.013)
Train: 6 [ 750/1251 ( 60%)]  Loss: 5.195 (5.27)  Time: 0.995s, 1029.12/s  (1.007s, 1016.47/s)  LR: 9.990e-04  Data: 0.011 (0.013)
Train: 6 [ 800/1251 ( 64%)]  Loss: 5.156 (5.27)  Time: 1.002s, 1022.29/s  (1.007s, 1017.12/s)  LR: 9.990e-04  Data: 0.013 (0.013)
Train: 6 [ 850/1251 ( 68%)]  Loss: 5.464 (5.28)  Time: 1.054s,  971.31/s  (1.008s, 1016.15/s)  LR: 9.990e-04  Data: 0.011 (0.013)
Train: 6 [ 900/1251 ( 72%)]  Loss: 4.898 (5.26)  Time: 0.993s, 1031.24/s  (1.008s, 1016.28/s)  LR: 9.990e-04  Data: 0.010 (0.013)
Train: 6 [ 950/1251 ( 76%)]  Loss: 5.159 (5.25)  Time: 0.997s, 1026.85/s  (1.008s, 1016.06/s)  LR: 9.990e-04  Data: 0.011 (0.013)
Train: 6 [1000/1251 ( 80%)]  Loss: 5.191 (5.25)  Time: 0.995s, 1029.20/s  (1.008s, 1015.93/s)  LR: 9.990e-04  Data: 0.010 (0.013)
Train: 6 [1050/1251 ( 84%)]  Loss: 5.367 (5.25)  Time: 0.994s, 1029.89/s  (1.007s, 1016.43/s)  LR: 9.990e-04  Data: 0.011 (0.012)
Train: 6 [1100/1251 ( 88%)]  Loss: 5.048 (5.25)  Time: 0.995s, 1029.57/s  (1.007s, 1016.94/s)  LR: 9.990e-04  Data: 0.010 (0.012)
Train: 6 [1150/1251 ( 92%)]  Loss: 5.285 (5.25)  Time: 0.995s, 1029.36/s  (1.007s, 1017.29/s)  LR: 9.990e-04  Data: 0.011 (0.012)
Train: 6 [1200/1251 ( 96%)]  Loss: 5.069 (5.24)  Time: 1.061s,  965.40/s  (1.007s, 1016.54/s)  LR: 9.990e-04  Data: 0.011 (0.012)
Train: 6 [1250/1251 (100%)]  Loss: 5.141 (5.24)  Time: 1.024s,  999.65/s  (1.008s, 1016.13/s)  LR: 9.990e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.595 (1.595)  Loss:  2.4102 (2.4102)  Acc@1: 52.3438 (52.3438)  Acc@5: 79.8828 (79.8828)
Test: [  48/48]  Time: 0.246 (0.565)  Loss:  2.6078 (3.1553)  Acc@1: 56.0142 (36.4740)  Acc@5: 73.9387 (63.6420)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-6.pth.tar', 36.47400002685547)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-5.pth.tar', 31.331999965820312)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-4.pth.tar', 23.86800006347656)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-3.pth.tar', 16.146000080566406)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-2.pth.tar', 6.216000008544922)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-1.pth.tar', 2.8140000006103514)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-0.pth.tar', 0.1679999987220764)

Train: 7 [   0/1251 (  0%)]  Loss: 5.275 (5.27)  Time: 2.463s,  415.80/s  (2.463s,  415.80/s)  LR: 9.987e-04  Data: 1.504 (1.504)
Train: 7 [  50/1251 (  4%)]  Loss: 5.066 (5.17)  Time: 1.007s, 1017.07/s  (1.038s,  986.39/s)  LR: 9.987e-04  Data: 0.012 (0.041)
Train: 7 [ 100/1251 (  8%)]  Loss: 5.058 (5.13)  Time: 0.994s, 1030.02/s  (1.021s, 1003.41/s)  LR: 9.987e-04  Data: 0.011 (0.026)
Train: 7 [ 150/1251 ( 12%)]  Loss: 5.370 (5.19)  Time: 0.996s, 1027.62/s  (1.018s, 1006.00/s)  LR: 9.987e-04  Data: 0.010 (0.021)
Train: 7 [ 200/1251 ( 16%)]  Loss: 5.444 (5.24)  Time: 1.065s,  961.61/s  (1.014s, 1009.72/s)  LR: 9.987e-04  Data: 0.012 (0.019)
Train: 7 [ 250/1251 ( 20%)]  Loss: 5.378 (5.27)  Time: 0.995s, 1029.05/s  (1.013s, 1011.16/s)  LR: 9.987e-04  Data: 0.012 (0.017)
Train: 7 [ 300/1251 ( 24%)]  Loss: 5.058 (5.24)  Time: 1.034s,  990.77/s  (1.015s, 1009.32/s)  LR: 9.987e-04  Data: 0.011 (0.016)
Train: 7 [ 350/1251 ( 28%)]  Loss: 5.191 (5.23)  Time: 0.994s, 1030.52/s  (1.015s, 1008.57/s)  LR: 9.987e-04  Data: 0.011 (0.015)
Train: 7 [ 400/1251 ( 32%)]  Loss: 5.242 (5.23)  Time: 0.996s, 1028.07/s  (1.015s, 1008.95/s)  LR: 9.987e-04  Data: 0.012 (0.015)
Train: 7 [ 450/1251 ( 36%)]  Loss: 4.896 (5.20)  Time: 0.995s, 1028.84/s  (1.013s, 1010.80/s)  LR: 9.987e-04  Data: 0.011 (0.015)
Train: 7 [ 500/1251 ( 40%)]  Loss: 5.129 (5.19)  Time: 0.995s, 1029.62/s  (1.012s, 1012.29/s)  LR: 9.987e-04  Data: 0.011 (0.014)
Train: 7 [ 550/1251 ( 44%)]  Loss: 4.754 (5.15)  Time: 1.061s,  965.09/s  (1.012s, 1011.41/s)  LR: 9.987e-04  Data: 0.010 (0.014)
Train: 7 [ 600/1251 ( 48%)]  Loss: 5.103 (5.15)  Time: 0.996s, 1027.85/s  (1.013s, 1011.19/s)  LR: 9.987e-04  Data: 0.010 (0.014)
Train: 7 [ 650/1251 ( 52%)]  Loss: 4.836 (5.13)  Time: 0.996s, 1028.55/s  (1.013s, 1010.92/s)  LR: 9.987e-04  Data: 0.011 (0.014)
Train: 7 [ 700/1251 ( 56%)]  Loss: 4.989 (5.12)  Time: 0.998s, 1026.03/s  (1.012s, 1012.07/s)  LR: 9.987e-04  Data: 0.012 (0.013)
Train: 7 [ 750/1251 ( 60%)]  Loss: 4.835 (5.10)  Time: 0.995s, 1029.44/s  (1.011s, 1012.87/s)  LR: 9.987e-04  Data: 0.011 (0.013)
Train: 7 [ 800/1251 ( 64%)]  Loss: 5.120 (5.10)  Time: 0.996s, 1027.85/s  (1.010s, 1013.47/s)  LR: 9.987e-04  Data: 0.011 (0.013)
Train: 7 [ 850/1251 ( 68%)]  Loss: 5.058 (5.10)  Time: 0.997s, 1027.50/s  (1.010s, 1014.06/s)  LR: 9.987e-04  Data: 0.011 (0.013)
Train: 7 [ 900/1251 ( 72%)]  Loss: 5.144 (5.10)  Time: 0.997s, 1027.17/s  (1.009s, 1014.40/s)  LR: 9.987e-04  Data: 0.010 (0.013)
Train: 7 [ 950/1251 ( 76%)]  Loss: 4.923 (5.09)  Time: 1.005s, 1019.24/s  (1.009s, 1014.50/s)  LR: 9.987e-04  Data: 0.010 (0.013)
Train: 7 [1000/1251 ( 80%)]  Loss: 5.083 (5.09)  Time: 0.998s, 1026.56/s  (1.010s, 1013.61/s)  LR: 9.987e-04  Data: 0.011 (0.013)
Train: 7 [1050/1251 ( 84%)]  Loss: 5.070 (5.09)  Time: 0.999s, 1025.25/s  (1.010s, 1013.83/s)  LR: 9.987e-04  Data: 0.012 (0.013)
Train: 7 [1100/1251 ( 88%)]  Loss: 4.901 (5.08)  Time: 0.997s, 1027.30/s  (1.010s, 1014.36/s)  LR: 9.987e-04  Data: 0.011 (0.013)
Train: 7 [1150/1251 ( 92%)]  Loss: 4.912 (5.08)  Time: 0.996s, 1028.09/s  (1.009s, 1014.67/s)  LR: 9.987e-04  Data: 0.011 (0.012)
Train: 7 [1200/1251 ( 96%)]  Loss: 4.945 (5.07)  Time: 1.018s, 1006.32/s  (1.009s, 1014.99/s)  LR: 9.987e-04  Data: 0.012 (0.012)
Train: 7 [1250/1251 (100%)]  Loss: 4.941 (5.07)  Time: 0.985s, 1039.83/s  (1.008s, 1015.44/s)  LR: 9.987e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.649 (1.649)  Loss:  1.8622 (1.8622)  Acc@1: 62.4023 (62.4023)  Acc@5: 85.5469 (85.5469)
Test: [  48/48]  Time: 0.245 (0.579)  Loss:  1.8877 (2.8307)  Acc@1: 64.7406 (40.3700)  Acc@5: 83.0189 (68.0040)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-7.pth.tar', 40.370000068359374)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-6.pth.tar', 36.47400002685547)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-5.pth.tar', 31.331999965820312)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-4.pth.tar', 23.86800006347656)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-3.pth.tar', 16.146000080566406)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-2.pth.tar', 6.216000008544922)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-1.pth.tar', 2.8140000006103514)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-0.pth.tar', 0.1679999987220764)

Train: 8 [   0/1251 (  0%)]  Loss: 5.154 (5.15)  Time: 2.618s,  391.17/s  (2.618s,  391.17/s)  LR: 9.983e-04  Data: 1.601 (1.601)
Train: 8 [  50/1251 (  4%)]  Loss: 4.987 (5.07)  Time: 0.997s, 1026.97/s  (1.037s,  987.55/s)  LR: 9.983e-04  Data: 0.011 (0.044)
Train: 8 [ 100/1251 (  8%)]  Loss: 4.674 (4.94)  Time: 1.012s, 1011.97/s  (1.018s, 1005.65/s)  LR: 9.983e-04  Data: 0.010 (0.028)
Train: 8 [ 150/1251 ( 12%)]  Loss: 4.852 (4.92)  Time: 1.003s, 1020.51/s  (1.013s, 1011.13/s)  LR: 9.983e-04  Data: 0.010 (0.022)
Train: 8 [ 200/1251 ( 16%)]  Loss: 4.800 (4.89)  Time: 0.997s, 1026.88/s  (1.010s, 1014.19/s)  LR: 9.983e-04  Data: 0.011 (0.019)
Train: 8 [ 250/1251 ( 20%)]  Loss: 4.652 (4.85)  Time: 0.994s, 1029.91/s  (1.007s, 1016.59/s)  LR: 9.983e-04  Data: 0.010 (0.018)
Train: 8 [ 300/1251 ( 24%)]  Loss: 4.976 (4.87)  Time: 1.063s,  963.52/s  (1.008s, 1015.94/s)  LR: 9.983e-04  Data: 0.013 (0.017)
Train: 8 [ 350/1251 ( 28%)]  Loss: 4.789 (4.86)  Time: 0.991s, 1033.49/s  (1.007s, 1017.04/s)  LR: 9.983e-04  Data: 0.010 (0.016)
Train: 8 [ 400/1251 ( 32%)]  Loss: 4.857 (4.86)  Time: 0.994s, 1030.46/s  (1.011s, 1013.30/s)  LR: 9.983e-04  Data: 0.011 (0.015)
Train: 8 [ 450/1251 ( 36%)]  Loss: 4.833 (4.86)  Time: 0.996s, 1028.10/s  (1.013s, 1011.30/s)  LR: 9.983e-04  Data: 0.011 (0.015)
Train: 8 [ 500/1251 ( 40%)]  Loss: 5.021 (4.87)  Time: 1.044s,  981.22/s  (1.014s, 1010.34/s)  LR: 9.983e-04  Data: 0.011 (0.014)
Train: 8 [ 550/1251 ( 44%)]  Loss: 4.824 (4.87)  Time: 1.085s,  943.51/s  (1.015s, 1009.31/s)  LR: 9.983e-04  Data: 0.010 (0.014)
Train: 8 [ 600/1251 ( 48%)]  Loss: 4.810 (4.86)  Time: 0.996s, 1028.19/s  (1.016s, 1007.53/s)  LR: 9.983e-04  Data: 0.011 (0.014)
Train: 8 [ 650/1251 ( 52%)]  Loss: 5.237 (4.89)  Time: 1.036s,  988.04/s  (1.016s, 1008.10/s)  LR: 9.983e-04  Data: 0.011 (0.014)
Train: 8 [ 700/1251 ( 56%)]  Loss: 4.646 (4.87)  Time: 1.000s, 1024.36/s  (1.015s, 1009.01/s)  LR: 9.983e-04  Data: 0.010 (0.013)
Train: 8 [ 750/1251 ( 60%)]  Loss: 4.898 (4.88)  Time: 1.000s, 1023.96/s  (1.014s, 1009.60/s)  LR: 9.983e-04  Data: 0.011 (0.013)
Train: 8 [ 800/1251 ( 64%)]  Loss: 4.648 (4.86)  Time: 0.996s, 1028.07/s  (1.013s, 1010.40/s)  LR: 9.983e-04  Data: 0.011 (0.013)
Train: 8 [ 850/1251 ( 68%)]  Loss: 4.965 (4.87)  Time: 0.999s, 1024.56/s  (1.013s, 1011.29/s)  LR: 9.983e-04  Data: 0.012 (0.013)
Train: 8 [ 900/1251 ( 72%)]  Loss: 4.565 (4.85)  Time: 1.034s,  989.96/s  (1.012s, 1011.94/s)  LR: 9.983e-04  Data: 0.011 (0.013)
Train: 8 [ 950/1251 ( 76%)]  Loss: 4.197 (4.82)  Time: 0.993s, 1031.45/s  (1.012s, 1011.38/s)  LR: 9.983e-04  Data: 0.010 (0.013)
Train: 8 [1000/1251 ( 80%)]  Loss: 5.199 (4.84)  Time: 1.000s, 1023.57/s  (1.013s, 1011.13/s)  LR: 9.983e-04  Data: 0.010 (0.013)
Train: 8 [1050/1251 ( 84%)]  Loss: 4.617 (4.83)  Time: 0.996s, 1028.44/s  (1.013s, 1011.21/s)  LR: 9.983e-04  Data: 0.011 (0.013)
Train: 8 [1100/1251 ( 88%)]  Loss: 5.186 (4.84)  Time: 1.001s, 1023.47/s  (1.012s, 1011.72/s)  LR: 9.983e-04  Data: 0.010 (0.013)
Train: 8 [1150/1251 ( 92%)]  Loss: 5.032 (4.85)  Time: 0.996s, 1027.81/s  (1.012s, 1011.64/s)  LR: 9.983e-04  Data: 0.011 (0.013)
Train: 8 [1200/1251 ( 96%)]  Loss: 5.097 (4.86)  Time: 0.998s, 1026.42/s  (1.012s, 1012.05/s)  LR: 9.983e-04  Data: 0.011 (0.012)
Train: 8 [1250/1251 (100%)]  Loss: 4.752 (4.86)  Time: 0.984s, 1041.06/s  (1.011s, 1012.66/s)  LR: 9.983e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.654 (1.654)  Loss:  1.7978 (1.7978)  Acc@1: 62.5977 (62.5977)  Acc@5: 86.1328 (86.1328)
Test: [  48/48]  Time: 0.245 (0.576)  Loss:  1.8028 (2.5298)  Acc@1: 67.0991 (46.2120)  Acc@5: 83.9623 (73.3280)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-8.pth.tar', 46.21200005859375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-7.pth.tar', 40.370000068359374)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-6.pth.tar', 36.47400002685547)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-5.pth.tar', 31.331999965820312)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-4.pth.tar', 23.86800006347656)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-3.pth.tar', 16.146000080566406)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-2.pth.tar', 6.216000008544922)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-1.pth.tar', 2.8140000006103514)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-0.pth.tar', 0.1679999987220764)

Train: 9 [   0/1251 (  0%)]  Loss: 4.852 (4.85)  Time: 2.616s,  391.48/s  (2.616s,  391.48/s)  LR: 9.978e-04  Data: 1.656 (1.656)
Train: 9 [  50/1251 (  4%)]  Loss: 4.634 (4.74)  Time: 0.999s, 1024.78/s  (1.035s,  988.97/s)  LR: 9.978e-04  Data: 0.011 (0.046)
Train: 9 [ 100/1251 (  8%)]  Loss: 4.834 (4.77)  Time: 1.037s,  987.37/s  (1.024s, 1000.04/s)  LR: 9.978e-04  Data: 0.012 (0.029)
Train: 9 [ 150/1251 ( 12%)]  Loss: 4.742 (4.77)  Time: 0.997s, 1027.36/s  (1.019s, 1005.02/s)  LR: 9.978e-04  Data: 0.011 (0.023)
Train: 9 [ 200/1251 ( 16%)]  Loss: 4.588 (4.73)  Time: 0.997s, 1027.24/s  (1.017s, 1007.24/s)  LR: 9.978e-04  Data: 0.011 (0.020)
Train: 9 [ 250/1251 ( 20%)]  Loss: 4.656 (4.72)  Time: 0.998s, 1025.85/s  (1.016s, 1007.83/s)  LR: 9.978e-04  Data: 0.012 (0.018)
Train: 9 [ 300/1251 ( 24%)]  Loss: 4.616 (4.70)  Time: 0.994s, 1030.14/s  (1.014s, 1010.13/s)  LR: 9.978e-04  Data: 0.010 (0.017)
Train: 9 [ 350/1251 ( 28%)]  Loss: 5.000 (4.74)  Time: 0.990s, 1033.96/s  (1.014s, 1009.59/s)  LR: 9.978e-04  Data: 0.011 (0.016)
Train: 9 [ 400/1251 ( 32%)]  Loss: 5.008 (4.77)  Time: 1.033s,  991.37/s  (1.013s, 1010.46/s)  LR: 9.978e-04  Data: 0.010 (0.015)
Train: 9 [ 450/1251 ( 36%)]  Loss: 4.925 (4.79)  Time: 0.993s, 1031.32/s  (1.014s, 1010.03/s)  LR: 9.978e-04  Data: 0.010 (0.015)
Train: 9 [ 500/1251 ( 40%)]  Loss: 4.469 (4.76)  Time: 0.997s, 1027.32/s  (1.012s, 1011.52/s)  LR: 9.978e-04  Data: 0.010 (0.015)
Train: 9 [ 550/1251 ( 44%)]  Loss: 4.804 (4.76)  Time: 0.996s, 1027.94/s  (1.011s, 1012.59/s)  LR: 9.978e-04  Data: 0.011 (0.014)
Train: 9 [ 600/1251 ( 48%)]  Loss: 4.866 (4.77)  Time: 0.995s, 1028.67/s  (1.011s, 1012.41/s)  LR: 9.978e-04  Data: 0.010 (0.014)
Train: 9 [ 650/1251 ( 52%)]  Loss: 4.358 (4.74)  Time: 0.996s, 1028.52/s  (1.011s, 1013.22/s)  LR: 9.978e-04  Data: 0.011 (0.014)
Train: 9 [ 700/1251 ( 56%)]  Loss: 4.390 (4.72)  Time: 0.999s, 1025.53/s  (1.010s, 1013.73/s)  LR: 9.978e-04  Data: 0.011 (0.014)
Train: 9 [ 750/1251 ( 60%)]  Loss: 5.396 (4.76)  Time: 1.003s, 1021.23/s  (1.009s, 1014.42/s)  LR: 9.978e-04  Data: 0.010 (0.013)
Train: 9 [ 800/1251 ( 64%)]  Loss: 4.565 (4.75)  Time: 0.996s, 1027.74/s  (1.010s, 1014.28/s)  LR: 9.978e-04  Data: 0.011 (0.013)
Train: 9 [ 850/1251 ( 68%)]  Loss: 5.020 (4.76)  Time: 0.999s, 1024.55/s  (1.009s, 1014.76/s)  LR: 9.978e-04  Data: 0.012 (0.013)
Train: 9 [ 900/1251 ( 72%)]  Loss: 4.499 (4.75)  Time: 0.994s, 1029.74/s  (1.009s, 1015.01/s)  LR: 9.978e-04  Data: 0.011 (0.013)
Train: 9 [ 950/1251 ( 76%)]  Loss: 4.844 (4.75)  Time: 0.991s, 1033.68/s  (1.009s, 1015.10/s)  LR: 9.978e-04  Data: 0.010 (0.013)
Train: 9 [1000/1251 ( 80%)]  Loss: 4.778 (4.75)  Time: 1.000s, 1024.32/s  (1.009s, 1015.08/s)  LR: 9.978e-04  Data: 0.011 (0.013)
Train: 9 [1050/1251 ( 84%)]  Loss: 4.760 (4.75)  Time: 0.997s, 1026.66/s  (1.010s, 1014.15/s)  LR: 9.978e-04  Data: 0.011 (0.013)
Train: 9 [1100/1251 ( 88%)]  Loss: 4.565 (4.75)  Time: 0.995s, 1028.98/s  (1.010s, 1013.60/s)  LR: 9.978e-04  Data: 0.012 (0.013)
Train: 9 [1150/1251 ( 92%)]  Loss: 4.836 (4.75)  Time: 0.997s, 1026.80/s  (1.010s, 1014.08/s)  LR: 9.978e-04  Data: 0.012 (0.013)
Train: 9 [1200/1251 ( 96%)]  Loss: 4.880 (4.76)  Time: 0.995s, 1029.51/s  (1.009s, 1014.44/s)  LR: 9.978e-04  Data: 0.011 (0.013)
Train: 9 [1250/1251 (100%)]  Loss: 4.692 (4.75)  Time: 0.984s, 1040.25/s  (1.009s, 1014.91/s)  LR: 9.978e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.683 (1.683)  Loss:  1.7741 (1.7741)  Acc@1: 69.5312 (69.5312)  Acc@5: 90.0391 (90.0391)
Test: [  48/48]  Time: 0.245 (0.567)  Loss:  1.9005 (2.5433)  Acc@1: 68.8679 (48.0300)  Acc@5: 84.5519 (74.9280)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-9.pth.tar', 48.03000005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-8.pth.tar', 46.21200005859375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-7.pth.tar', 40.370000068359374)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-6.pth.tar', 36.47400002685547)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-5.pth.tar', 31.331999965820312)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-4.pth.tar', 23.86800006347656)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-3.pth.tar', 16.146000080566406)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-2.pth.tar', 6.216000008544922)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-1.pth.tar', 2.8140000006103514)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-0.pth.tar', 0.1679999987220764)

Train: 10 [   0/1251 (  0%)]  Loss: 4.357 (4.36)  Time: 2.457s,  416.75/s  (2.457s,  416.75/s)  LR: 9.973e-04  Data: 1.492 (1.492)
Train: 10 [  50/1251 (  4%)]  Loss: 4.974 (4.67)  Time: 1.005s, 1019.37/s  (1.030s,  993.70/s)  LR: 9.973e-04  Data: 0.010 (0.040)
Train: 10 [ 100/1251 (  8%)]  Loss: 4.519 (4.62)  Time: 0.992s, 1032.77/s  (1.016s, 1008.00/s)  LR: 9.973e-04  Data: 0.010 (0.026)
Train: 10 [ 150/1251 ( 12%)]  Loss: 4.228 (4.52)  Time: 0.996s, 1028.22/s  (1.010s, 1013.60/s)  LR: 9.973e-04  Data: 0.010 (0.021)
Train: 10 [ 200/1251 ( 16%)]  Loss: 4.496 (4.51)  Time: 1.052s,  973.46/s  (1.008s, 1015.67/s)  LR: 9.973e-04  Data: 0.011 (0.018)
Train: 10 [ 250/1251 ( 20%)]  Loss: 4.492 (4.51)  Time: 1.061s,  964.71/s  (1.013s, 1010.45/s)  LR: 9.973e-04  Data: 0.011 (0.017)
Train: 10 [ 300/1251 ( 24%)]  Loss: 4.672 (4.53)  Time: 1.013s, 1010.53/s  (1.014s, 1009.82/s)  LR: 9.973e-04  Data: 0.019 (0.016)
Train: 10 [ 350/1251 ( 28%)]  Loss: 4.675 (4.55)  Time: 1.000s, 1023.80/s  (1.014s, 1010.12/s)  LR: 9.973e-04  Data: 0.010 (0.015)
Train: 10 [ 400/1251 ( 32%)]  Loss: 4.773 (4.58)  Time: 1.035s,  989.66/s  (1.012s, 1011.42/s)  LR: 9.973e-04  Data: 0.010 (0.015)
Train: 10 [ 450/1251 ( 36%)]  Loss: 4.542 (4.57)  Time: 0.994s, 1030.36/s  (1.013s, 1011.13/s)  LR: 9.973e-04  Data: 0.010 (0.014)
Train: 10 [ 500/1251 ( 40%)]  Loss: 4.830 (4.60)  Time: 1.055s,  970.88/s  (1.012s, 1011.76/s)  LR: 9.973e-04  Data: 0.011 (0.014)
Train: 10 [ 550/1251 ( 44%)]  Loss: 4.838 (4.62)  Time: 1.057s,  969.02/s  (1.011s, 1012.95/s)  LR: 9.973e-04  Data: 0.011 (0.014)
Train: 10 [ 600/1251 ( 48%)]  Loss: 4.771 (4.63)  Time: 0.995s, 1028.84/s  (1.012s, 1012.06/s)  LR: 9.973e-04  Data: 0.012 (0.013)
Train: 10 [ 650/1251 ( 52%)]  Loss: 4.846 (4.64)  Time: 0.997s, 1027.17/s  (1.011s, 1012.92/s)  LR: 9.973e-04  Data: 0.012 (0.013)
Train: 10 [ 700/1251 ( 56%)]  Loss: 4.531 (4.64)  Time: 0.998s, 1026.28/s  (1.011s, 1013.09/s)  LR: 9.973e-04  Data: 0.012 (0.013)
Train: 10 [ 750/1251 ( 60%)]  Loss: 4.525 (4.63)  Time: 0.995s, 1029.18/s  (1.011s, 1012.97/s)  LR: 9.973e-04  Data: 0.011 (0.013)
Train: 10 [ 800/1251 ( 64%)]  Loss: 4.861 (4.64)  Time: 0.997s, 1026.66/s  (1.010s, 1013.66/s)  LR: 9.973e-04  Data: 0.012 (0.013)
Train: 10 [ 850/1251 ( 68%)]  Loss: 4.708 (4.65)  Time: 1.001s, 1022.94/s  (1.010s, 1014.33/s)  LR: 9.973e-04  Data: 0.011 (0.013)
Train: 10 [ 900/1251 ( 72%)]  Loss: 4.523 (4.64)  Time: 0.997s, 1026.99/s  (1.010s, 1014.05/s)  LR: 9.973e-04  Data: 0.012 (0.013)
Train: 10 [ 950/1251 ( 76%)]  Loss: 4.509 (4.63)  Time: 1.000s, 1024.45/s  (1.009s, 1014.68/s)  LR: 9.973e-04  Data: 0.011 (0.013)
Train: 10 [1000/1251 ( 80%)]  Loss: 4.628 (4.63)  Time: 0.994s, 1029.68/s  (1.010s, 1013.93/s)  LR: 9.973e-04  Data: 0.010 (0.013)
Train: 10 [1050/1251 ( 84%)]  Loss: 4.985 (4.65)  Time: 1.036s,  988.29/s  (1.010s, 1013.36/s)  LR: 9.973e-04  Data: 0.011 (0.012)
Train: 10 [1100/1251 ( 88%)]  Loss: 4.818 (4.66)  Time: 1.003s, 1021.29/s  (1.011s, 1013.35/s)  LR: 9.973e-04  Data: 0.011 (0.012)
Train: 10 [1150/1251 ( 92%)]  Loss: 4.937 (4.67)  Time: 1.047s,  977.59/s  (1.012s, 1012.20/s)  LR: 9.973e-04  Data: 0.011 (0.012)
Train: 10 [1200/1251 ( 96%)]  Loss: 4.802 (4.67)  Time: 0.995s, 1028.87/s  (1.013s, 1010.96/s)  LR: 9.973e-04  Data: 0.011 (0.012)
Train: 10 [1250/1251 (100%)]  Loss: 4.683 (4.67)  Time: 0.982s, 1042.96/s  (1.012s, 1011.42/s)  LR: 9.973e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.709 (1.709)  Loss:  1.7206 (1.7206)  Acc@1: 68.1641 (68.1641)  Acc@5: 86.6211 (86.6211)
Test: [  48/48]  Time: 0.245 (0.569)  Loss:  1.5666 (2.3726)  Acc@1: 70.9906 (50.1360)  Acc@5: 87.8538 (76.1580)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-10.pth.tar', 50.13600006835937)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-9.pth.tar', 48.03000005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-8.pth.tar', 46.21200005859375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-7.pth.tar', 40.370000068359374)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-6.pth.tar', 36.47400002685547)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-5.pth.tar', 31.331999965820312)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-4.pth.tar', 23.86800006347656)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-3.pth.tar', 16.146000080566406)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-2.pth.tar', 6.216000008544922)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-1.pth.tar', 2.8140000006103514)

Train: 11 [   0/1251 (  0%)]  Loss: 4.652 (4.65)  Time: 2.717s,  376.82/s  (2.717s,  376.82/s)  LR: 9.967e-04  Data: 1.758 (1.758)
Train: 11 [  50/1251 (  4%)]  Loss: 4.778 (4.72)  Time: 0.998s, 1026.37/s  (1.032s,  992.44/s)  LR: 9.967e-04  Data: 0.010 (0.046)
Train: 11 [ 100/1251 (  8%)]  Loss: 4.742 (4.72)  Time: 0.999s, 1025.32/s  (1.015s, 1008.90/s)  LR: 9.967e-04  Data: 0.011 (0.029)
Train: 11 [ 150/1251 ( 12%)]  Loss: 4.476 (4.66)  Time: 1.006s, 1018.25/s  (1.010s, 1013.84/s)  LR: 9.967e-04  Data: 0.011 (0.023)
Train: 11 [ 200/1251 ( 16%)]  Loss: 4.379 (4.61)  Time: 0.999s, 1025.23/s  (1.010s, 1014.23/s)  LR: 9.967e-04  Data: 0.012 (0.020)
Train: 11 [ 250/1251 ( 20%)]  Loss: 4.600 (4.60)  Time: 0.996s, 1028.34/s  (1.014s, 1009.87/s)  LR: 9.967e-04  Data: 0.011 (0.018)
Train: 11 [ 300/1251 ( 24%)]  Loss: 4.785 (4.63)  Time: 0.998s, 1026.14/s  (1.013s, 1010.58/s)  LR: 9.967e-04  Data: 0.012 (0.017)
Train: 11 [ 350/1251 ( 28%)]  Loss: 4.732 (4.64)  Time: 1.031s,  993.31/s  (1.014s, 1009.94/s)  LR: 9.967e-04  Data: 0.011 (0.016)
Train: 11 [ 400/1251 ( 32%)]  Loss: 4.471 (4.62)  Time: 1.052s,  973.80/s  (1.018s, 1005.55/s)  LR: 9.967e-04  Data: 0.011 (0.016)
Train: 11 [ 450/1251 ( 36%)]  Loss: 5.003 (4.66)  Time: 0.995s, 1029.41/s  (1.020s, 1004.14/s)  LR: 9.967e-04  Data: 0.011 (0.015)
Train: 11 [ 500/1251 ( 40%)]  Loss: 4.309 (4.63)  Time: 0.993s, 1031.03/s  (1.018s, 1006.17/s)  LR: 9.967e-04  Data: 0.011 (0.015)
Train: 11 [ 550/1251 ( 44%)]  Loss: 4.574 (4.63)  Time: 1.081s,  947.26/s  (1.018s, 1006.10/s)  LR: 9.967e-04  Data: 0.011 (0.015)
Train: 11 [ 600/1251 ( 48%)]  Loss: 4.494 (4.61)  Time: 0.998s, 1026.13/s  (1.016s, 1007.65/s)  LR: 9.967e-04  Data: 0.012 (0.014)
Train: 11 [ 650/1251 ( 52%)]  Loss: 4.954 (4.64)  Time: 0.996s, 1027.71/s  (1.016s, 1008.04/s)  LR: 9.967e-04  Data: 0.012 (0.014)
Train: 11 [ 700/1251 ( 56%)]  Loss: 4.244 (4.61)  Time: 0.997s, 1026.77/s  (1.016s, 1008.25/s)  LR: 9.967e-04  Data: 0.011 (0.014)
Train: 11 [ 750/1251 ( 60%)]  Loss: 4.998 (4.64)  Time: 0.998s, 1026.07/s  (1.015s, 1009.12/s)  LR: 9.967e-04  Data: 0.011 (0.014)
Train: 11 [ 800/1251 ( 64%)]  Loss: 4.477 (4.63)  Time: 1.001s, 1022.88/s  (1.014s, 1009.66/s)  LR: 9.967e-04  Data: 0.011 (0.014)
Train: 11 [ 850/1251 ( 68%)]  Loss: 4.463 (4.62)  Time: 1.031s,  992.80/s  (1.015s, 1009.19/s)  LR: 9.967e-04  Data: 0.011 (0.013)
Train: 11 [ 900/1251 ( 72%)]  Loss: 4.535 (4.61)  Time: 0.994s, 1029.76/s  (1.014s, 1009.96/s)  LR: 9.967e-04  Data: 0.011 (0.013)
Train: 11 [ 950/1251 ( 76%)]  Loss: 4.577 (4.61)  Time: 0.996s, 1027.60/s  (1.014s, 1010.13/s)  LR: 9.967e-04  Data: 0.012 (0.013)
Train: 11 [1000/1251 ( 80%)]  Loss: 4.594 (4.61)  Time: 0.997s, 1027.27/s  (1.013s, 1010.82/s)  LR: 9.967e-04  Data: 0.011 (0.013)
Train: 11 [1050/1251 ( 84%)]  Loss: 4.441 (4.60)  Time: 0.997s, 1027.56/s  (1.012s, 1011.42/s)  LR: 9.967e-04  Data: 0.012 (0.013)
Train: 11 [1100/1251 ( 88%)]  Loss: 4.242 (4.59)  Time: 1.034s,  990.80/s  (1.012s, 1011.88/s)  LR: 9.967e-04  Data: 0.012 (0.013)
Train: 11 [1150/1251 ( 92%)]  Loss: 4.690 (4.59)  Time: 1.061s,  965.50/s  (1.012s, 1011.42/s)  LR: 9.967e-04  Data: 0.011 (0.013)
Train: 11 [1200/1251 ( 96%)]  Loss: 4.574 (4.59)  Time: 0.996s, 1027.60/s  (1.013s, 1011.17/s)  LR: 9.967e-04  Data: 0.012 (0.013)
Train: 11 [1250/1251 (100%)]  Loss: 4.260 (4.58)  Time: 0.982s, 1042.33/s  (1.013s, 1010.91/s)  LR: 9.967e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.632 (1.632)  Loss:  1.2751 (1.2751)  Acc@1: 75.4883 (75.4883)  Acc@5: 92.5781 (92.5781)
Test: [  48/48]  Time: 0.245 (0.564)  Loss:  1.5515 (2.2733)  Acc@1: 72.6415 (53.6240)  Acc@5: 88.2076 (79.0340)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-11.pth.tar', 53.62400000976562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-10.pth.tar', 50.13600006835937)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-9.pth.tar', 48.03000005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-8.pth.tar', 46.21200005859375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-7.pth.tar', 40.370000068359374)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-6.pth.tar', 36.47400002685547)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-5.pth.tar', 31.331999965820312)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-4.pth.tar', 23.86800006347656)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-3.pth.tar', 16.146000080566406)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-2.pth.tar', 6.216000008544922)

Train: 12 [   0/1251 (  0%)]  Loss: 4.577 (4.58)  Time: 4.399s,  232.80/s  (4.399s,  232.80/s)  LR: 9.961e-04  Data: 3.088 (3.088)
Train: 12 [  50/1251 (  4%)]  Loss: 4.308 (4.44)  Time: 1.002s, 1021.75/s  (1.078s,  949.59/s)  LR: 9.961e-04  Data: 0.012 (0.072)
Train: 12 [ 100/1251 (  8%)]  Loss: 4.623 (4.50)  Time: 1.004s, 1019.57/s  (1.046s,  979.42/s)  LR: 9.961e-04  Data: 0.011 (0.042)
Train: 12 [ 150/1251 ( 12%)]  Loss: 4.465 (4.49)  Time: 1.038s,  986.80/s  (1.034s,  990.27/s)  LR: 9.961e-04  Data: 0.012 (0.032)
Train: 12 [ 200/1251 ( 16%)]  Loss: 4.777 (4.55)  Time: 0.996s, 1027.67/s  (1.026s,  997.81/s)  LR: 9.961e-04  Data: 0.012 (0.027)
Train: 12 [ 250/1251 ( 20%)]  Loss: 4.687 (4.57)  Time: 0.994s, 1029.90/s  (1.021s, 1002.72/s)  LR: 9.961e-04  Data: 0.011 (0.024)
Train: 12 [ 300/1251 ( 24%)]  Loss: 4.298 (4.53)  Time: 0.996s, 1027.77/s  (1.017s, 1006.43/s)  LR: 9.961e-04  Data: 0.011 (0.022)
Train: 12 [ 350/1251 ( 28%)]  Loss: 4.619 (4.54)  Time: 0.994s, 1030.54/s  (1.015s, 1009.15/s)  LR: 9.961e-04  Data: 0.010 (0.020)
Train: 12 [ 400/1251 ( 32%)]  Loss: 4.510 (4.54)  Time: 0.998s, 1026.01/s  (1.013s, 1011.07/s)  LR: 9.961e-04  Data: 0.011 (0.019)
Train: 12 [ 450/1251 ( 36%)]  Loss: 4.598 (4.55)  Time: 0.996s, 1028.23/s  (1.011s, 1012.85/s)  LR: 9.961e-04  Data: 0.011 (0.018)
Train: 12 [ 500/1251 ( 40%)]  Loss: 4.380 (4.53)  Time: 1.004s, 1020.05/s  (1.010s, 1013.94/s)  LR: 9.961e-04  Data: 0.011 (0.017)
Train: 12 [ 550/1251 ( 44%)]  Loss: 4.553 (4.53)  Time: 1.011s, 1012.89/s  (1.010s, 1014.19/s)  LR: 9.961e-04  Data: 0.011 (0.017)
Train: 12 [ 600/1251 ( 48%)]  Loss: 4.407 (4.52)  Time: 1.034s,  990.66/s  (1.009s, 1014.96/s)  LR: 9.961e-04  Data: 0.012 (0.016)
Train: 12 [ 650/1251 ( 52%)]  Loss: 4.356 (4.51)  Time: 1.059s,  967.07/s  (1.008s, 1015.42/s)  LR: 9.961e-04  Data: 0.012 (0.016)
Train: 12 [ 700/1251 ( 56%)]  Loss: 4.102 (4.48)  Time: 1.047s,  977.96/s  (1.008s, 1015.74/s)  LR: 9.961e-04  Data: 0.012 (0.016)
Train: 12 [ 750/1251 ( 60%)]  Loss: 4.548 (4.49)  Time: 0.995s, 1029.15/s  (1.008s, 1016.31/s)  LR: 9.961e-04  Data: 0.012 (0.015)
Train: 12 [ 800/1251 ( 64%)]  Loss: 4.544 (4.49)  Time: 0.996s, 1028.02/s  (1.008s, 1016.00/s)  LR: 9.961e-04  Data: 0.012 (0.015)
Train: 12 [ 850/1251 ( 68%)]  Loss: 4.776 (4.51)  Time: 0.995s, 1028.66/s  (1.008s, 1015.98/s)  LR: 9.961e-04  Data: 0.011 (0.015)
Train: 12 [ 900/1251 ( 72%)]  Loss: 4.444 (4.50)  Time: 0.996s, 1028.43/s  (1.008s, 1016.37/s)  LR: 9.961e-04  Data: 0.011 (0.015)
Train: 12 [ 950/1251 ( 76%)]  Loss: 4.487 (4.50)  Time: 1.028s,  995.90/s  (1.008s, 1016.27/s)  LR: 9.961e-04  Data: 0.011 (0.015)
Train: 12 [1000/1251 ( 80%)]  Loss: 4.536 (4.50)  Time: 0.997s, 1027.10/s  (1.007s, 1016.63/s)  LR: 9.961e-04  Data: 0.011 (0.014)
Train: 12 [1050/1251 ( 84%)]  Loss: 4.553 (4.51)  Time: 0.995s, 1029.42/s  (1.007s, 1016.93/s)  LR: 9.961e-04  Data: 0.011 (0.014)
Train: 12 [1100/1251 ( 88%)]  Loss: 4.413 (4.50)  Time: 0.997s, 1027.08/s  (1.007s, 1017.15/s)  LR: 9.961e-04  Data: 0.011 (0.014)
Train: 12 [1150/1251 ( 92%)]  Loss: 4.349 (4.50)  Time: 1.056s,  969.27/s  (1.007s, 1017.15/s)  LR: 9.961e-04  Data: 0.012 (0.014)
Train: 12 [1200/1251 ( 96%)]  Loss: 4.566 (4.50)  Time: 0.996s, 1027.70/s  (1.007s, 1017.30/s)  LR: 9.961e-04  Data: 0.012 (0.014)
Train: 12 [1250/1251 (100%)]  Loss: 4.654 (4.50)  Time: 0.986s, 1038.97/s  (1.006s, 1017.39/s)  LR: 9.961e-04  Data: 0.000 (0.014)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.720 (1.720)  Loss:  1.3598 (1.3598)  Acc@1: 75.1953 (75.1953)  Acc@5: 91.5039 (91.5039)
Test: [  48/48]  Time: 0.245 (0.578)  Loss:  1.3878 (2.1273)  Acc@1: 72.5236 (55.6700)  Acc@5: 89.7406 (80.7120)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-12.pth.tar', 55.66999995849609)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-11.pth.tar', 53.62400000976562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-10.pth.tar', 50.13600006835937)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-9.pth.tar', 48.03000005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-8.pth.tar', 46.21200005859375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-7.pth.tar', 40.370000068359374)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-6.pth.tar', 36.47400002685547)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-5.pth.tar', 31.331999965820312)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-4.pth.tar', 23.86800006347656)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-3.pth.tar', 16.146000080566406)

Train: 13 [   0/1251 (  0%)]  Loss: 4.589 (4.59)  Time: 2.476s,  413.60/s  (2.476s,  413.60/s)  LR: 9.954e-04  Data: 1.512 (1.512)
Train: 13 [  50/1251 (  4%)]  Loss: 4.191 (4.39)  Time: 0.997s, 1027.41/s  (1.035s,  989.50/s)  LR: 9.954e-04  Data: 0.011 (0.041)
Train: 13 [ 100/1251 (  8%)]  Loss: 4.309 (4.36)  Time: 1.036s,  988.45/s  (1.021s, 1003.02/s)  LR: 9.954e-04  Data: 0.011 (0.027)
Train: 13 [ 150/1251 ( 12%)]  Loss: 4.386 (4.37)  Time: 0.996s, 1027.79/s  (1.015s, 1009.03/s)  LR: 9.954e-04  Data: 0.012 (0.022)
Train: 13 [ 200/1251 ( 16%)]  Loss: 4.207 (4.34)  Time: 0.997s, 1027.32/s  (1.011s, 1013.14/s)  LR: 9.954e-04  Data: 0.011 (0.019)
Train: 13 [ 250/1251 ( 20%)]  Loss: 4.744 (4.40)  Time: 0.995s, 1028.92/s  (1.009s, 1015.09/s)  LR: 9.954e-04  Data: 0.011 (0.017)
Train: 13 [ 300/1251 ( 24%)]  Loss: 4.501 (4.42)  Time: 0.996s, 1027.63/s  (1.009s, 1015.33/s)  LR: 9.954e-04  Data: 0.012 (0.016)
Train: 13 [ 350/1251 ( 28%)]  Loss: 4.719 (4.46)  Time: 0.996s, 1027.78/s  (1.009s, 1015.09/s)  LR: 9.954e-04  Data: 0.012 (0.016)
Train: 13 [ 400/1251 ( 32%)]  Loss: 4.157 (4.42)  Time: 0.996s, 1028.11/s  (1.007s, 1016.49/s)  LR: 9.954e-04  Data: 0.011 (0.015)
Train: 13 [ 450/1251 ( 36%)]  Loss: 4.561 (4.44)  Time: 1.003s, 1021.07/s  (1.008s, 1016.02/s)  LR: 9.954e-04  Data: 0.012 (0.015)
Train: 13 [ 500/1251 ( 40%)]  Loss: 4.665 (4.46)  Time: 1.005s, 1018.83/s  (1.007s, 1016.83/s)  LR: 9.954e-04  Data: 0.012 (0.014)
Train: 13 [ 550/1251 ( 44%)]  Loss: 4.224 (4.44)  Time: 0.997s, 1027.37/s  (1.006s, 1017.60/s)  LR: 9.954e-04  Data: 0.012 (0.014)
Train: 13 [ 600/1251 ( 48%)]  Loss: 4.254 (4.42)  Time: 0.995s, 1028.84/s  (1.006s, 1018.17/s)  LR: 9.954e-04  Data: 0.011 (0.014)
Train: 13 [ 650/1251 ( 52%)]  Loss: 4.435 (4.42)  Time: 0.998s, 1026.56/s  (1.006s, 1018.24/s)  LR: 9.954e-04  Data: 0.011 (0.014)
Train: 13 [ 700/1251 ( 56%)]  Loss: 4.613 (4.44)  Time: 0.994s, 1030.51/s  (1.006s, 1018.29/s)  LR: 9.954e-04  Data: 0.012 (0.013)
Train: 13 [ 750/1251 ( 60%)]  Loss: 4.375 (4.43)  Time: 0.994s, 1030.68/s  (1.005s, 1018.69/s)  LR: 9.954e-04  Data: 0.010 (0.013)
Train: 13 [ 800/1251 ( 64%)]  Loss: 4.398 (4.43)  Time: 1.043s,  981.50/s  (1.006s, 1018.15/s)  LR: 9.954e-04  Data: 0.012 (0.013)
Train: 13 [ 850/1251 ( 68%)]  Loss: 3.652 (4.39)  Time: 0.996s, 1028.59/s  (1.006s, 1017.71/s)  LR: 9.954e-04  Data: 0.011 (0.013)
Train: 13 [ 900/1251 ( 72%)]  Loss: 4.299 (4.38)  Time: 0.995s, 1028.76/s  (1.007s, 1017.19/s)  LR: 9.954e-04  Data: 0.012 (0.013)
Train: 13 [ 950/1251 ( 76%)]  Loss: 4.305 (4.38)  Time: 1.031s,  993.62/s  (1.006s, 1017.54/s)  LR: 9.954e-04  Data: 0.011 (0.013)
Train: 13 [1000/1251 ( 80%)]  Loss: 4.406 (4.38)  Time: 0.994s, 1029.74/s  (1.006s, 1017.42/s)  LR: 9.954e-04  Data: 0.011 (0.013)
Train: 13 [1050/1251 ( 84%)]  Loss: 4.469 (4.38)  Time: 0.995s, 1028.91/s  (1.006s, 1017.87/s)  LR: 9.954e-04  Data: 0.011 (0.013)
Train: 13 [1100/1251 ( 88%)]  Loss: 4.372 (4.38)  Time: 0.999s, 1025.31/s  (1.006s, 1018.18/s)  LR: 9.954e-04  Data: 0.012 (0.013)
Train: 13 [1150/1251 ( 92%)]  Loss: 4.299 (4.38)  Time: 1.023s, 1000.90/s  (1.005s, 1018.49/s)  LR: 9.954e-04  Data: 0.012 (0.013)
Train: 13 [1200/1251 ( 96%)]  Loss: 4.467 (4.38)  Time: 1.044s,  981.23/s  (1.006s, 1018.40/s)  LR: 9.954e-04  Data: 0.011 (0.013)
Train: 13 [1250/1251 (100%)]  Loss: 4.177 (4.38)  Time: 0.984s, 1040.62/s  (1.005s, 1018.75/s)  LR: 9.954e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.655 (1.655)  Loss:  1.3436 (1.3436)  Acc@1: 75.7812 (75.7812)  Acc@5: 91.6992 (91.6992)
Test: [  48/48]  Time: 0.245 (0.564)  Loss:  1.3322 (2.1132)  Acc@1: 75.2359 (55.9440)  Acc@5: 90.5660 (80.8100)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-13.pth.tar', 55.944000102539064)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-12.pth.tar', 55.66999995849609)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-11.pth.tar', 53.62400000976562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-10.pth.tar', 50.13600006835937)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-9.pth.tar', 48.03000005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-8.pth.tar', 46.21200005859375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-7.pth.tar', 40.370000068359374)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-6.pth.tar', 36.47400002685547)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-5.pth.tar', 31.331999965820312)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-4.pth.tar', 23.86800006347656)

Train: 14 [   0/1251 (  0%)]  Loss: 4.766 (4.77)  Time: 2.447s,  418.40/s  (2.447s,  418.40/s)  LR: 9.947e-04  Data: 1.497 (1.497)
Train: 14 [  50/1251 (  4%)]  Loss: 4.523 (4.64)  Time: 0.999s, 1025.10/s  (1.046s,  979.36/s)  LR: 9.947e-04  Data: 0.011 (0.047)
Train: 14 [ 100/1251 (  8%)]  Loss: 3.888 (4.39)  Time: 1.035s,  989.77/s  (1.028s,  995.96/s)  LR: 9.947e-04  Data: 0.011 (0.029)
Train: 14 [ 150/1251 ( 12%)]  Loss: 4.495 (4.42)  Time: 0.997s, 1026.98/s  (1.020s, 1003.59/s)  LR: 9.947e-04  Data: 0.011 (0.023)
Train: 14 [ 200/1251 ( 16%)]  Loss: 4.568 (4.45)  Time: 1.067s,  959.75/s  (1.021s, 1003.13/s)  LR: 9.947e-04  Data: 0.011 (0.020)
Train: 14 [ 250/1251 ( 20%)]  Loss: 4.705 (4.49)  Time: 0.999s, 1024.62/s  (1.020s, 1004.14/s)  LR: 9.947e-04  Data: 0.011 (0.019)
Train: 14 [ 300/1251 ( 24%)]  Loss: 4.409 (4.48)  Time: 0.994s, 1029.69/s  (1.016s, 1007.74/s)  LR: 9.947e-04  Data: 0.010 (0.017)
Train: 14 [ 350/1251 ( 28%)]  Loss: 4.372 (4.47)  Time: 0.998s, 1026.29/s  (1.013s, 1010.39/s)  LR: 9.947e-04  Data: 0.012 (0.017)
Train: 14 [ 400/1251 ( 32%)]  Loss: 4.509 (4.47)  Time: 1.001s, 1023.07/s  (1.012s, 1011.62/s)  LR: 9.947e-04  Data: 0.010 (0.016)
Train: 14 [ 450/1251 ( 36%)]  Loss: 4.594 (4.48)  Time: 0.994s, 1030.05/s  (1.012s, 1012.33/s)  LR: 9.947e-04  Data: 0.011 (0.015)
Train: 14 [ 500/1251 ( 40%)]  Loss: 4.188 (4.46)  Time: 0.998s, 1026.23/s  (1.010s, 1013.38/s)  LR: 9.947e-04  Data: 0.011 (0.015)
Train: 14 [ 550/1251 ( 44%)]  Loss: 4.357 (4.45)  Time: 0.995s, 1029.28/s  (1.009s, 1014.38/s)  LR: 9.947e-04  Data: 0.011 (0.015)
Train: 14 [ 600/1251 ( 48%)]  Loss: 4.577 (4.46)  Time: 0.995s, 1028.86/s  (1.009s, 1014.64/s)  LR: 9.947e-04  Data: 0.011 (0.014)
Train: 14 [ 650/1251 ( 52%)]  Loss: 4.507 (4.46)  Time: 1.013s, 1010.66/s  (1.008s, 1015.45/s)  LR: 9.947e-04  Data: 0.010 (0.014)
Train: 14 [ 700/1251 ( 56%)]  Loss: 4.483 (4.46)  Time: 0.995s, 1029.59/s  (1.008s, 1016.20/s)  LR: 9.947e-04  Data: 0.011 (0.014)
Train: 14 [ 750/1251 ( 60%)]  Loss: 4.519 (4.47)  Time: 0.994s, 1030.29/s  (1.007s, 1016.89/s)  LR: 9.947e-04  Data: 0.011 (0.014)
Train: 14 [ 800/1251 ( 64%)]  Loss: 4.117 (4.45)  Time: 0.996s, 1027.83/s  (1.006s, 1017.42/s)  LR: 9.947e-04  Data: 0.011 (0.014)
Train: 14 [ 850/1251 ( 68%)]  Loss: 3.902 (4.42)  Time: 1.005s, 1018.97/s  (1.006s, 1017.86/s)  LR: 9.947e-04  Data: 0.011 (0.013)
Train: 14 [ 900/1251 ( 72%)]  Loss: 4.131 (4.40)  Time: 0.996s, 1028.30/s  (1.006s, 1018.04/s)  LR: 9.947e-04  Data: 0.012 (0.013)
Train: 14 [ 950/1251 ( 76%)]  Loss: 4.490 (4.41)  Time: 0.994s, 1030.28/s  (1.006s, 1018.15/s)  LR: 9.947e-04  Data: 0.010 (0.013)
Train: 14 [1000/1251 ( 80%)]  Loss: 4.087 (4.39)  Time: 0.996s, 1027.70/s  (1.005s, 1018.60/s)  LR: 9.947e-04  Data: 0.012 (0.013)
Train: 14 [1050/1251 ( 84%)]  Loss: 4.157 (4.38)  Time: 1.011s, 1012.93/s  (1.005s, 1018.70/s)  LR: 9.947e-04  Data: 0.012 (0.013)
Train: 14 [1100/1251 ( 88%)]  Loss: 4.111 (4.37)  Time: 1.037s,  987.05/s  (1.006s, 1017.58/s)  LR: 9.947e-04  Data: 0.011 (0.013)
Train: 14 [1150/1251 ( 92%)]  Loss: 4.151 (4.36)  Time: 1.011s, 1013.32/s  (1.006s, 1017.83/s)  LR: 9.947e-04  Data: 0.010 (0.013)
Train: 14 [1200/1251 ( 96%)]  Loss: 4.579 (4.37)  Time: 1.038s,  986.08/s  (1.007s, 1017.04/s)  LR: 9.947e-04  Data: 0.011 (0.013)
Train: 14 [1250/1251 (100%)]  Loss: 4.736 (4.38)  Time: 0.983s, 1041.69/s  (1.007s, 1017.19/s)  LR: 9.947e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.633 (1.633)  Loss:  1.2803 (1.2803)  Acc@1: 75.1953 (75.1953)  Acc@5: 91.6992 (91.6992)
Test: [  48/48]  Time: 0.245 (0.569)  Loss:  1.4680 (2.0999)  Acc@1: 73.8208 (57.7360)  Acc@5: 89.1509 (82.0840)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-14.pth.tar', 57.73600013427735)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-13.pth.tar', 55.944000102539064)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-12.pth.tar', 55.66999995849609)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-11.pth.tar', 53.62400000976562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-10.pth.tar', 50.13600006835937)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-9.pth.tar', 48.03000005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-8.pth.tar', 46.21200005859375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-7.pth.tar', 40.370000068359374)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-6.pth.tar', 36.47400002685547)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-5.pth.tar', 31.331999965820312)

Train: 15 [   0/1251 (  0%)]  Loss: 4.593 (4.59)  Time: 2.686s,  381.21/s  (2.686s,  381.21/s)  LR: 9.939e-04  Data: 1.723 (1.723)
Train: 15 [  50/1251 (  4%)]  Loss: 4.242 (4.42)  Time: 0.993s, 1030.88/s  (1.040s,  984.45/s)  LR: 9.939e-04  Data: 0.011 (0.045)
Train: 15 [ 100/1251 (  8%)]  Loss: 4.282 (4.37)  Time: 0.997s, 1026.83/s  (1.021s, 1002.49/s)  LR: 9.939e-04  Data: 0.012 (0.028)
Train: 15 [ 150/1251 ( 12%)]  Loss: 4.074 (4.30)  Time: 0.996s, 1027.90/s  (1.018s, 1005.58/s)  LR: 9.939e-04  Data: 0.011 (0.023)
Train: 15 [ 200/1251 ( 16%)]  Loss: 4.409 (4.32)  Time: 1.030s,  994.62/s  (1.014s, 1009.66/s)  LR: 9.939e-04  Data: 0.012 (0.020)
Train: 15 [ 250/1251 ( 20%)]  Loss: 3.988 (4.26)  Time: 0.997s, 1026.74/s  (1.012s, 1011.74/s)  LR: 9.939e-04  Data: 0.012 (0.018)
Train: 15 [ 300/1251 ( 24%)]  Loss: 4.519 (4.30)  Time: 1.032s,  992.69/s  (1.013s, 1010.57/s)  LR: 9.939e-04  Data: 0.011 (0.017)
Train: 15 [ 350/1251 ( 28%)]  Loss: 4.439 (4.32)  Time: 1.000s, 1024.08/s  (1.013s, 1010.81/s)  LR: 9.939e-04  Data: 0.011 (0.016)
Train: 15 [ 400/1251 ( 32%)]  Loss: 4.746 (4.37)  Time: 0.997s, 1027.09/s  (1.012s, 1012.25/s)  LR: 9.939e-04  Data: 0.011 (0.015)
Train: 15 [ 450/1251 ( 36%)]  Loss: 4.741 (4.40)  Time: 0.995s, 1029.55/s  (1.010s, 1013.70/s)  LR: 9.939e-04  Data: 0.011 (0.015)
Train: 15 [ 500/1251 ( 40%)]  Loss: 4.532 (4.41)  Time: 1.005s, 1019.17/s  (1.009s, 1014.94/s)  LR: 9.939e-04  Data: 0.011 (0.015)
Train: 15 [ 550/1251 ( 44%)]  Loss: 4.705 (4.44)  Time: 0.991s, 1032.97/s  (1.008s, 1015.41/s)  LR: 9.939e-04  Data: 0.010 (0.014)
Train: 15 [ 600/1251 ( 48%)]  Loss: 4.424 (4.44)  Time: 0.995s, 1029.19/s  (1.008s, 1015.81/s)  LR: 9.939e-04  Data: 0.010 (0.014)
Train: 15 [ 650/1251 ( 52%)]  Loss: 4.650 (4.45)  Time: 1.002s, 1022.19/s  (1.007s, 1016.40/s)  LR: 9.939e-04  Data: 0.011 (0.014)
Train: 15 [ 700/1251 ( 56%)]  Loss: 4.383 (4.45)  Time: 1.004s, 1020.04/s  (1.007s, 1016.75/s)  LR: 9.939e-04  Data: 0.010 (0.014)
Train: 15 [ 750/1251 ( 60%)]  Loss: 4.135 (4.43)  Time: 1.007s, 1016.98/s  (1.007s, 1017.17/s)  LR: 9.939e-04  Data: 0.011 (0.013)
Train: 15 [ 800/1251 ( 64%)]  Loss: 4.365 (4.43)  Time: 0.995s, 1028.65/s  (1.006s, 1017.82/s)  LR: 9.939e-04  Data: 0.011 (0.013)
Train: 15 [ 850/1251 ( 68%)]  Loss: 4.359 (4.42)  Time: 0.993s, 1031.27/s  (1.006s, 1017.63/s)  LR: 9.939e-04  Data: 0.011 (0.013)
Train: 15 [ 900/1251 ( 72%)]  Loss: 4.677 (4.43)  Time: 1.023s, 1000.79/s  (1.006s, 1017.71/s)  LR: 9.939e-04  Data: 0.011 (0.013)
Train: 15 [ 950/1251 ( 76%)]  Loss: 4.388 (4.43)  Time: 1.041s,  983.62/s  (1.006s, 1017.79/s)  LR: 9.939e-04  Data: 0.011 (0.013)
Train: 15 [1000/1251 ( 80%)]  Loss: 4.004 (4.41)  Time: 1.005s, 1018.54/s  (1.006s, 1017.66/s)  LR: 9.939e-04  Data: 0.011 (0.013)
Train: 15 [1050/1251 ( 84%)]  Loss: 4.399 (4.41)  Time: 1.000s, 1023.81/s  (1.006s, 1017.87/s)  LR: 9.939e-04  Data: 0.011 (0.013)
Train: 15 [1100/1251 ( 88%)]  Loss: 4.117 (4.40)  Time: 0.998s, 1026.39/s  (1.006s, 1018.06/s)  LR: 9.939e-04  Data: 0.011 (0.013)
Train: 15 [1150/1251 ( 92%)]  Loss: 4.192 (4.39)  Time: 0.994s, 1030.48/s  (1.006s, 1018.30/s)  LR: 9.939e-04  Data: 0.010 (0.013)
Train: 15 [1200/1251 ( 96%)]  Loss: 4.391 (4.39)  Time: 1.001s, 1022.67/s  (1.005s, 1018.49/s)  LR: 9.939e-04  Data: 0.011 (0.013)
Train: 15 [1250/1251 (100%)]  Loss: 4.591 (4.40)  Time: 1.023s, 1000.51/s  (1.006s, 1017.76/s)  LR: 9.939e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.693 (1.693)  Loss:  1.3362 (1.3362)  Acc@1: 76.3672 (76.3672)  Acc@5: 92.4805 (92.4805)
Test: [  48/48]  Time: 0.245 (0.578)  Loss:  1.4162 (2.0583)  Acc@1: 74.8821 (59.0240)  Acc@5: 91.5094 (82.9540)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-15.pth.tar', 59.024000078125)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-14.pth.tar', 57.73600013427735)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-13.pth.tar', 55.944000102539064)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-12.pth.tar', 55.66999995849609)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-11.pth.tar', 53.62400000976562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-10.pth.tar', 50.13600006835937)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-9.pth.tar', 48.03000005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-8.pth.tar', 46.21200005859375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-7.pth.tar', 40.370000068359374)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-6.pth.tar', 36.47400002685547)

Train: 16 [   0/1251 (  0%)]  Loss: 4.135 (4.14)  Time: 2.513s,  407.48/s  (2.513s,  407.48/s)  LR: 9.931e-04  Data: 1.549 (1.549)
Train: 16 [  50/1251 (  4%)]  Loss: 4.567 (4.35)  Time: 1.055s,  970.26/s  (1.046s,  979.20/s)  LR: 9.931e-04  Data: 0.011 (0.041)
Train: 16 [ 100/1251 (  8%)]  Loss: 3.863 (4.19)  Time: 1.042s,  982.71/s  (1.026s,  997.57/s)  LR: 9.931e-04  Data: 0.012 (0.026)
Train: 16 [ 150/1251 ( 12%)]  Loss: 3.984 (4.14)  Time: 0.996s, 1028.41/s  (1.022s, 1002.06/s)  LR: 9.931e-04  Data: 0.010 (0.021)
Train: 16 [ 200/1251 ( 16%)]  Loss: 3.950 (4.10)  Time: 0.995s, 1029.29/s  (1.016s, 1007.46/s)  LR: 9.931e-04  Data: 0.011 (0.019)
Train: 16 [ 250/1251 ( 20%)]  Loss: 4.324 (4.14)  Time: 0.994s, 1030.69/s  (1.013s, 1010.59/s)  LR: 9.931e-04  Data: 0.011 (0.017)
Train: 16 [ 300/1251 ( 24%)]  Loss: 4.482 (4.19)  Time: 0.994s, 1030.28/s  (1.013s, 1010.39/s)  LR: 9.931e-04  Data: 0.011 (0.016)
Train: 16 [ 350/1251 ( 28%)]  Loss: 4.511 (4.23)  Time: 0.994s, 1030.54/s  (1.011s, 1012.55/s)  LR: 9.931e-04  Data: 0.011 (0.016)
Train: 16 [ 400/1251 ( 32%)]  Loss: 4.223 (4.23)  Time: 0.995s, 1028.81/s  (1.010s, 1013.43/s)  LR: 9.931e-04  Data: 0.011 (0.015)
Train: 16 [ 450/1251 ( 36%)]  Loss: 4.885 (4.29)  Time: 0.998s, 1026.02/s  (1.010s, 1014.27/s)  LR: 9.931e-04  Data: 0.012 (0.015)
Train: 16 [ 500/1251 ( 40%)]  Loss: 4.236 (4.29)  Time: 1.053s,  972.54/s  (1.009s, 1014.84/s)  LR: 9.931e-04  Data: 0.011 (0.014)
Train: 16 [ 550/1251 ( 44%)]  Loss: 4.088 (4.27)  Time: 0.997s, 1027.58/s  (1.009s, 1014.39/s)  LR: 9.931e-04  Data: 0.011 (0.014)
Train: 16 [ 600/1251 ( 48%)]  Loss: 4.697 (4.30)  Time: 1.002s, 1022.30/s  (1.010s, 1013.50/s)  LR: 9.931e-04  Data: 0.012 (0.014)
Train: 16 [ 650/1251 ( 52%)]  Loss: 4.587 (4.32)  Time: 1.032s,  992.35/s  (1.010s, 1014.11/s)  LR: 9.931e-04  Data: 0.010 (0.013)
Train: 16 [ 700/1251 ( 56%)]  Loss: 4.079 (4.31)  Time: 1.052s,  973.80/s  (1.010s, 1013.74/s)  LR: 9.931e-04  Data: 0.011 (0.013)
Train: 16 [ 750/1251 ( 60%)]  Loss: 4.079 (4.29)  Time: 1.053s,  972.07/s  (1.011s, 1013.31/s)  LR: 9.931e-04  Data: 0.011 (0.013)
Train: 16 [ 800/1251 ( 64%)]  Loss: 4.520 (4.31)  Time: 1.048s,  976.94/s  (1.011s, 1013.24/s)  LR: 9.931e-04  Data: 0.011 (0.013)
Train: 16 [ 850/1251 ( 68%)]  Loss: 4.326 (4.31)  Time: 0.997s, 1026.90/s  (1.010s, 1013.93/s)  LR: 9.931e-04  Data: 0.011 (0.013)
Train: 16 [ 900/1251 ( 72%)]  Loss: 3.955 (4.29)  Time: 0.999s, 1024.68/s  (1.009s, 1014.56/s)  LR: 9.931e-04  Data: 0.012 (0.013)
Train: 16 [ 950/1251 ( 76%)]  Loss: 4.513 (4.30)  Time: 0.997s, 1026.95/s  (1.009s, 1015.12/s)  LR: 9.931e-04  Data: 0.012 (0.013)
Train: 16 [1000/1251 ( 80%)]  Loss: 4.432 (4.31)  Time: 1.026s,  997.98/s  (1.008s, 1015.53/s)  LR: 9.931e-04  Data: 0.011 (0.013)
Train: 16 [1050/1251 ( 84%)]  Loss: 4.263 (4.30)  Time: 0.996s, 1027.92/s  (1.008s, 1016.03/s)  LR: 9.931e-04  Data: 0.011 (0.013)
Train: 16 [1100/1251 ( 88%)]  Loss: 4.605 (4.32)  Time: 0.995s, 1029.00/s  (1.007s, 1016.40/s)  LR: 9.931e-04  Data: 0.010 (0.012)
Train: 16 [1150/1251 ( 92%)]  Loss: 4.537 (4.33)  Time: 1.037s,  987.77/s  (1.007s, 1016.78/s)  LR: 9.931e-04  Data: 0.011 (0.012)
Train: 16 [1200/1251 ( 96%)]  Loss: 4.118 (4.32)  Time: 1.025s,  998.59/s  (1.007s, 1016.40/s)  LR: 9.931e-04  Data: 0.010 (0.012)
Train: 16 [1250/1251 (100%)]  Loss: 4.488 (4.32)  Time: 1.018s, 1005.41/s  (1.008s, 1016.32/s)  LR: 9.931e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.627 (1.627)  Loss:  1.3246 (1.3246)  Acc@1: 75.4883 (75.4883)  Acc@5: 91.9922 (91.9922)
Test: [  48/48]  Time: 0.245 (0.571)  Loss:  1.2252 (1.9754)  Acc@1: 77.5943 (59.4700)  Acc@5: 92.0991 (83.3260)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-16.pth.tar', 59.4699999633789)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-15.pth.tar', 59.024000078125)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-14.pth.tar', 57.73600013427735)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-13.pth.tar', 55.944000102539064)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-12.pth.tar', 55.66999995849609)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-11.pth.tar', 53.62400000976562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-10.pth.tar', 50.13600006835937)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-9.pth.tar', 48.03000005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-8.pth.tar', 46.21200005859375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-7.pth.tar', 40.370000068359374)

Train: 17 [   0/1251 (  0%)]  Loss: 4.353 (4.35)  Time: 2.485s,  412.10/s  (2.485s,  412.10/s)  LR: 9.922e-04  Data: 1.527 (1.527)
Train: 17 [  50/1251 (  4%)]  Loss: 4.399 (4.38)  Time: 0.995s, 1029.39/s  (1.035s,  989.61/s)  LR: 9.922e-04  Data: 0.011 (0.041)
Train: 17 [ 100/1251 (  8%)]  Loss: 3.929 (4.23)  Time: 0.995s, 1029.41/s  (1.017s, 1006.60/s)  LR: 9.922e-04  Data: 0.011 (0.026)
Train: 17 [ 150/1251 ( 12%)]  Loss: 4.486 (4.29)  Time: 0.991s, 1033.27/s  (1.018s, 1005.60/s)  LR: 9.922e-04  Data: 0.010 (0.021)
Train: 17 [ 200/1251 ( 16%)]  Loss: 4.477 (4.33)  Time: 1.000s, 1024.43/s  (1.017s, 1006.88/s)  LR: 9.922e-04  Data: 0.012 (0.019)
Train: 17 [ 250/1251 ( 20%)]  Loss: 4.445 (4.35)  Time: 1.033s,  991.45/s  (1.017s, 1006.59/s)  LR: 9.922e-04  Data: 0.015 (0.018)
Train: 17 [ 300/1251 ( 24%)]  Loss: 4.415 (4.36)  Time: 0.996s, 1027.74/s  (1.018s, 1006.21/s)  LR: 9.922e-04  Data: 0.012 (0.017)
Train: 17 [ 350/1251 ( 28%)]  Loss: 4.155 (4.33)  Time: 1.001s, 1023.32/s  (1.015s, 1008.59/s)  LR: 9.922e-04  Data: 0.011 (0.016)
Train: 17 [ 400/1251 ( 32%)]  Loss: 3.956 (4.29)  Time: 0.993s, 1031.25/s  (1.014s, 1009.58/s)  LR: 9.922e-04  Data: 0.010 (0.015)
Train: 17 [ 450/1251 ( 36%)]  Loss: 4.238 (4.29)  Time: 0.998s, 1026.48/s  (1.013s, 1011.19/s)  LR: 9.922e-04  Data: 0.012 (0.015)
Train: 17 [ 500/1251 ( 40%)]  Loss: 4.096 (4.27)  Time: 0.996s, 1028.51/s  (1.011s, 1012.53/s)  LR: 9.922e-04  Data: 0.011 (0.014)
Train: 17 [ 550/1251 ( 44%)]  Loss: 4.440 (4.28)  Time: 0.996s, 1028.40/s  (1.010s, 1013.76/s)  LR: 9.922e-04  Data: 0.011 (0.014)
Train: 17 [ 600/1251 ( 48%)]  Loss: 4.553 (4.30)  Time: 0.997s, 1026.91/s  (1.011s, 1012.96/s)  LR: 9.922e-04  Data: 0.012 (0.014)
Train: 17 [ 650/1251 ( 52%)]  Loss: 4.164 (4.29)  Time: 0.995s, 1029.20/s  (1.010s, 1013.47/s)  LR: 9.922e-04  Data: 0.011 (0.014)
Train: 17 [ 700/1251 ( 56%)]  Loss: 4.238 (4.29)  Time: 1.078s,  950.09/s  (1.011s, 1012.84/s)  LR: 9.922e-04  Data: 0.010 (0.013)
Train: 17 [ 750/1251 ( 60%)]  Loss: 5.082 (4.34)  Time: 0.997s, 1027.56/s  (1.010s, 1013.67/s)  LR: 9.922e-04  Data: 0.011 (0.013)
Train: 17 [ 800/1251 ( 64%)]  Loss: 4.437 (4.34)  Time: 0.997s, 1026.90/s  (1.009s, 1014.44/s)  LR: 9.922e-04  Data: 0.011 (0.013)
Train: 17 [ 850/1251 ( 68%)]  Loss: 4.014 (4.33)  Time: 0.999s, 1024.62/s  (1.009s, 1014.42/s)  LR: 9.922e-04  Data: 0.010 (0.013)
Train: 17 [ 900/1251 ( 72%)]  Loss: 4.236 (4.32)  Time: 0.999s, 1025.41/s  (1.010s, 1014.31/s)  LR: 9.922e-04  Data: 0.011 (0.013)
Train: 17 [ 950/1251 ( 76%)]  Loss: 4.058 (4.31)  Time: 0.994s, 1030.69/s  (1.009s, 1014.76/s)  LR: 9.922e-04  Data: 0.011 (0.013)
Train: 17 [1000/1251 ( 80%)]  Loss: 4.040 (4.30)  Time: 1.027s,  997.09/s  (1.009s, 1014.59/s)  LR: 9.922e-04  Data: 0.011 (0.013)
Train: 17 [1050/1251 ( 84%)]  Loss: 4.099 (4.29)  Time: 0.992s, 1032.47/s  (1.009s, 1014.63/s)  LR: 9.922e-04  Data: 0.010 (0.013)
Train: 17 [1100/1251 ( 88%)]  Loss: 4.301 (4.29)  Time: 1.062s,  964.42/s  (1.010s, 1013.88/s)  LR: 9.922e-04  Data: 0.010 (0.012)
Train: 17 [1150/1251 ( 92%)]  Loss: 4.533 (4.30)  Time: 0.996s, 1027.76/s  (1.010s, 1013.86/s)  LR: 9.922e-04  Data: 0.012 (0.012)
Train: 17 [1200/1251 ( 96%)]  Loss: 4.233 (4.30)  Time: 1.063s,  963.45/s  (1.010s, 1014.14/s)  LR: 9.922e-04  Data: 0.012 (0.012)
Train: 17 [1250/1251 (100%)]  Loss: 4.114 (4.29)  Time: 0.982s, 1042.71/s  (1.010s, 1014.21/s)  LR: 9.922e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.678 (1.678)  Loss:  1.3985 (1.3985)  Acc@1: 77.5391 (77.5391)  Acc@5: 93.5547 (93.5547)
Test: [  48/48]  Time: 0.245 (0.573)  Loss:  1.3251 (1.9567)  Acc@1: 75.1179 (61.2020)  Acc@5: 90.9198 (84.5560)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-17.pth.tar', 61.20200005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-16.pth.tar', 59.4699999633789)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-15.pth.tar', 59.024000078125)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-14.pth.tar', 57.73600013427735)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-13.pth.tar', 55.944000102539064)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-12.pth.tar', 55.66999995849609)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-11.pth.tar', 53.62400000976562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-10.pth.tar', 50.13600006835937)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-9.pth.tar', 48.03000005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-8.pth.tar', 46.21200005859375)

Train: 18 [   0/1251 (  0%)]  Loss: 4.472 (4.47)  Time: 2.587s,  395.86/s  (2.587s,  395.86/s)  LR: 9.912e-04  Data: 1.634 (1.634)
Train: 18 [  50/1251 (  4%)]  Loss: 4.513 (4.49)  Time: 0.996s, 1028.05/s  (1.038s,  986.49/s)  LR: 9.912e-04  Data: 0.011 (0.043)
Train: 18 [ 100/1251 (  8%)]  Loss: 4.573 (4.52)  Time: 0.996s, 1027.62/s  (1.023s, 1000.50/s)  LR: 9.912e-04  Data: 0.010 (0.027)
Train: 18 [ 150/1251 ( 12%)]  Loss: 4.521 (4.52)  Time: 0.997s, 1026.92/s  (1.019s, 1004.84/s)  LR: 9.912e-04  Data: 0.011 (0.022)
Train: 18 [ 200/1251 ( 16%)]  Loss: 4.523 (4.52)  Time: 1.051s,  974.58/s  (1.017s, 1007.33/s)  LR: 9.912e-04  Data: 0.011 (0.019)
Train: 18 [ 250/1251 ( 20%)]  Loss: 3.906 (4.42)  Time: 1.052s,  973.15/s  (1.021s, 1002.98/s)  LR: 9.912e-04  Data: 0.011 (0.017)
Train: 18 [ 300/1251 ( 24%)]  Loss: 4.278 (4.40)  Time: 1.010s, 1013.47/s  (1.018s, 1006.34/s)  LR: 9.912e-04  Data: 0.012 (0.016)
Train: 18 [ 350/1251 ( 28%)]  Loss: 4.170 (4.37)  Time: 1.036s,  988.78/s  (1.016s, 1007.98/s)  LR: 9.912e-04  Data: 0.012 (0.016)
Train: 18 [ 400/1251 ( 32%)]  Loss: 4.271 (4.36)  Time: 0.997s, 1026.83/s  (1.015s, 1009.22/s)  LR: 9.912e-04  Data: 0.012 (0.015)
Train: 18 [ 450/1251 ( 36%)]  Loss: 3.818 (4.30)  Time: 1.028s,  996.42/s  (1.013s, 1010.66/s)  LR: 9.912e-04  Data: 0.010 (0.015)
Train: 18 [ 500/1251 ( 40%)]  Loss: 4.390 (4.31)  Time: 1.057s,  968.38/s  (1.013s, 1010.87/s)  LR: 9.912e-04  Data: 0.011 (0.014)
Train: 18 [ 550/1251 ( 44%)]  Loss: 3.500 (4.24)  Time: 1.023s, 1000.74/s  (1.012s, 1011.91/s)  LR: 9.912e-04  Data: 0.010 (0.014)
Train: 18 [ 600/1251 ( 48%)]  Loss: 4.218 (4.24)  Time: 0.998s, 1026.21/s  (1.012s, 1012.02/s)  LR: 9.912e-04  Data: 0.012 (0.014)
Train: 18 [ 650/1251 ( 52%)]  Loss: 4.089 (4.23)  Time: 0.993s, 1031.26/s  (1.011s, 1012.83/s)  LR: 9.912e-04  Data: 0.011 (0.014)
Train: 18 [ 700/1251 ( 56%)]  Loss: 4.112 (4.22)  Time: 1.013s, 1010.60/s  (1.011s, 1013.34/s)  LR: 9.912e-04  Data: 0.011 (0.013)
Train: 18 [ 750/1251 ( 60%)]  Loss: 4.039 (4.21)  Time: 0.997s, 1027.51/s  (1.010s, 1014.18/s)  LR: 9.912e-04  Data: 0.011 (0.013)
Train: 18 [ 800/1251 ( 64%)]  Loss: 4.240 (4.21)  Time: 0.997s, 1027.49/s  (1.010s, 1014.29/s)  LR: 9.912e-04  Data: 0.011 (0.013)
Train: 18 [ 850/1251 ( 68%)]  Loss: 4.549 (4.23)  Time: 0.997s, 1026.81/s  (1.009s, 1014.87/s)  LR: 9.912e-04  Data: 0.012 (0.013)
Train: 18 [ 900/1251 ( 72%)]  Loss: 4.211 (4.23)  Time: 0.995s, 1029.39/s  (1.009s, 1015.21/s)  LR: 9.912e-04  Data: 0.012 (0.013)
Train: 18 [ 950/1251 ( 76%)]  Loss: 4.225 (4.23)  Time: 0.996s, 1028.15/s  (1.009s, 1014.95/s)  LR: 9.912e-04  Data: 0.011 (0.013)
Train: 18 [1000/1251 ( 80%)]  Loss: 4.563 (4.25)  Time: 1.003s, 1020.48/s  (1.010s, 1014.22/s)  LR: 9.912e-04  Data: 0.012 (0.013)
Train: 18 [1050/1251 ( 84%)]  Loss: 4.333 (4.25)  Time: 1.002s, 1022.20/s  (1.009s, 1014.52/s)  LR: 9.912e-04  Data: 0.011 (0.013)
Train: 18 [1100/1251 ( 88%)]  Loss: 4.135 (4.25)  Time: 0.992s, 1032.51/s  (1.010s, 1013.85/s)  LR: 9.912e-04  Data: 0.011 (0.013)
Train: 18 [1150/1251 ( 92%)]  Loss: 4.108 (4.24)  Time: 0.994s, 1030.54/s  (1.010s, 1014.27/s)  LR: 9.912e-04  Data: 0.010 (0.013)
Train: 18 [1200/1251 ( 96%)]  Loss: 4.489 (4.25)  Time: 1.007s, 1017.06/s  (1.009s, 1014.63/s)  LR: 9.912e-04  Data: 0.012 (0.012)
Train: 18 [1250/1251 (100%)]  Loss: 4.179 (4.25)  Time: 1.024s, 1000.30/s  (1.009s, 1014.83/s)  LR: 9.912e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.590 (1.590)  Loss:  1.3710 (1.3710)  Acc@1: 77.6367 (77.6367)  Acc@5: 93.8477 (93.8477)
Test: [  48/48]  Time: 0.246 (0.568)  Loss:  1.3672 (2.0333)  Acc@1: 77.8302 (60.5580)  Acc@5: 92.4528 (84.3040)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-17.pth.tar', 61.20200005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-18.pth.tar', 60.55799993652344)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-16.pth.tar', 59.4699999633789)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-15.pth.tar', 59.024000078125)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-14.pth.tar', 57.73600013427735)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-13.pth.tar', 55.944000102539064)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-12.pth.tar', 55.66999995849609)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-11.pth.tar', 53.62400000976562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-10.pth.tar', 50.13600006835937)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-9.pth.tar', 48.03000005126953)

Train: 19 [   0/1251 (  0%)]  Loss: 4.049 (4.05)  Time: 2.434s,  420.69/s  (2.434s,  420.69/s)  LR: 9.902e-04  Data: 1.472 (1.472)
Train: 19 [  50/1251 (  4%)]  Loss: 4.401 (4.22)  Time: 1.040s,  984.62/s  (1.038s,  986.32/s)  LR: 9.902e-04  Data: 0.010 (0.041)
Train: 19 [ 100/1251 (  8%)]  Loss: 4.394 (4.28)  Time: 0.996s, 1027.71/s  (1.019s, 1005.20/s)  LR: 9.902e-04  Data: 0.011 (0.026)
Train: 19 [ 150/1251 ( 12%)]  Loss: 3.939 (4.20)  Time: 0.994s, 1030.27/s  (1.016s, 1008.28/s)  LR: 9.902e-04  Data: 0.010 (0.021)
Train: 19 [ 200/1251 ( 16%)]  Loss: 4.263 (4.21)  Time: 1.036s,  987.99/s  (1.013s, 1011.18/s)  LR: 9.902e-04  Data: 0.012 (0.019)
Train: 19 [ 250/1251 ( 20%)]  Loss: 4.711 (4.29)  Time: 1.062s,  964.09/s  (1.012s, 1012.11/s)  LR: 9.902e-04  Data: 0.011 (0.017)
Train: 19 [ 300/1251 ( 24%)]  Loss: 4.439 (4.31)  Time: 0.998s, 1026.04/s  (1.011s, 1012.72/s)  LR: 9.902e-04  Data: 0.012 (0.016)
Train: 19 [ 350/1251 ( 28%)]  Loss: 4.340 (4.32)  Time: 1.033s,  991.36/s  (1.010s, 1013.89/s)  LR: 9.902e-04  Data: 0.012 (0.016)
Train: 19 [ 400/1251 ( 32%)]  Loss: 4.278 (4.31)  Time: 1.044s,  981.09/s  (1.012s, 1011.61/s)  LR: 9.902e-04  Data: 0.011 (0.015)
Train: 19 [ 450/1251 ( 36%)]  Loss: 4.302 (4.31)  Time: 1.055s,  970.74/s  (1.013s, 1011.31/s)  LR: 9.902e-04  Data: 0.011 (0.015)
Train: 19 [ 500/1251 ( 40%)]  Loss: 4.469 (4.33)  Time: 0.994s, 1030.26/s  (1.012s, 1012.09/s)  LR: 9.902e-04  Data: 0.011 (0.014)
Train: 19 [ 550/1251 ( 44%)]  Loss: 4.017 (4.30)  Time: 1.004s, 1020.33/s  (1.012s, 1012.14/s)  LR: 9.902e-04  Data: 0.011 (0.014)
Train: 19 [ 600/1251 ( 48%)]  Loss: 3.844 (4.27)  Time: 0.995s, 1029.35/s  (1.011s, 1012.82/s)  LR: 9.902e-04  Data: 0.011 (0.014)
Train: 19 [ 650/1251 ( 52%)]  Loss: 4.355 (4.27)  Time: 1.004s, 1020.01/s  (1.011s, 1013.30/s)  LR: 9.902e-04  Data: 0.011 (0.013)
Train: 19 [ 700/1251 ( 56%)]  Loss: 4.014 (4.25)  Time: 0.996s, 1028.05/s  (1.010s, 1014.25/s)  LR: 9.902e-04  Data: 0.012 (0.013)
Train: 19 [ 750/1251 ( 60%)]  Loss: 4.022 (4.24)  Time: 0.994s, 1030.27/s  (1.009s, 1015.08/s)  LR: 9.902e-04  Data: 0.012 (0.013)
Train: 19 [ 800/1251 ( 64%)]  Loss: 4.221 (4.24)  Time: 0.997s, 1027.52/s  (1.008s, 1015.70/s)  LR: 9.902e-04  Data: 0.012 (0.013)
Train: 19 [ 850/1251 ( 68%)]  Loss: 4.252 (4.24)  Time: 0.999s, 1025.25/s  (1.008s, 1016.32/s)  LR: 9.902e-04  Data: 0.011 (0.013)
Train: 19 [ 900/1251 ( 72%)]  Loss: 4.236 (4.24)  Time: 0.996s, 1027.95/s  (1.007s, 1016.84/s)  LR: 9.902e-04  Data: 0.012 (0.013)
Train: 19 [ 950/1251 ( 76%)]  Loss: 4.434 (4.25)  Time: 0.992s, 1032.46/s  (1.007s, 1017.33/s)  LR: 9.902e-04  Data: 0.011 (0.013)
Train: 19 [1000/1251 ( 80%)]  Loss: 4.217 (4.25)  Time: 0.995s, 1028.72/s  (1.006s, 1017.79/s)  LR: 9.902e-04  Data: 0.011 (0.013)
Train: 19 [1050/1251 ( 84%)]  Loss: 4.385 (4.25)  Time: 0.994s, 1029.82/s  (1.006s, 1018.15/s)  LR: 9.902e-04  Data: 0.011 (0.013)
Train: 19 [1100/1251 ( 88%)]  Loss: 4.466 (4.26)  Time: 0.996s, 1028.47/s  (1.005s, 1018.43/s)  LR: 9.902e-04  Data: 0.011 (0.013)
Train: 19 [1150/1251 ( 92%)]  Loss: 4.243 (4.26)  Time: 0.996s, 1027.98/s  (1.005s, 1018.68/s)  LR: 9.902e-04  Data: 0.011 (0.012)
Train: 19 [1200/1251 ( 96%)]  Loss: 4.110 (4.26)  Time: 1.009s, 1014.53/s  (1.005s, 1018.99/s)  LR: 9.902e-04  Data: 0.011 (0.012)
Train: 19 [1250/1251 (100%)]  Loss: 4.258 (4.26)  Time: 1.027s,  996.73/s  (1.005s, 1018.42/s)  LR: 9.902e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.690 (1.690)  Loss:  1.0196 (1.0196)  Acc@1: 81.6406 (81.6406)  Acc@5: 95.3125 (95.3125)
Test: [  48/48]  Time: 0.245 (0.572)  Loss:  1.4319 (1.8919)  Acc@1: 75.9434 (62.3340)  Acc@5: 91.3915 (85.0500)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-19.pth.tar', 62.33399989257813)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-17.pth.tar', 61.20200005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-18.pth.tar', 60.55799993652344)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-16.pth.tar', 59.4699999633789)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-15.pth.tar', 59.024000078125)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-14.pth.tar', 57.73600013427735)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-13.pth.tar', 55.944000102539064)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-12.pth.tar', 55.66999995849609)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-11.pth.tar', 53.62400000976562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-10.pth.tar', 50.13600006835937)

Train: 20 [   0/1251 (  0%)]  Loss: 4.122 (4.12)  Time: 2.707s,  378.26/s  (2.707s,  378.26/s)  LR: 9.892e-04  Data: 1.687 (1.687)
Train: 20 [  50/1251 (  4%)]  Loss: 4.233 (4.18)  Time: 0.996s, 1028.02/s  (1.042s,  982.54/s)  LR: 9.892e-04  Data: 0.012 (0.044)
Train: 20 [ 100/1251 (  8%)]  Loss: 4.208 (4.19)  Time: 1.005s, 1019.18/s  (1.021s, 1003.34/s)  LR: 9.892e-04  Data: 0.012 (0.028)
Train: 20 [ 150/1251 ( 12%)]  Loss: 4.104 (4.17)  Time: 1.002s, 1021.66/s  (1.018s, 1005.48/s)  LR: 9.892e-04  Data: 0.011 (0.022)
Train: 20 [ 200/1251 ( 16%)]  Loss: 4.553 (4.24)  Time: 0.995s, 1029.40/s  (1.018s, 1006.04/s)  LR: 9.892e-04  Data: 0.011 (0.019)
Train: 20 [ 250/1251 ( 20%)]  Loss: 4.671 (4.32)  Time: 1.050s,  975.23/s  (1.014s, 1009.91/s)  LR: 9.892e-04  Data: 0.011 (0.018)
Train: 20 [ 300/1251 ( 24%)]  Loss: 4.009 (4.27)  Time: 0.994s, 1030.04/s  (1.013s, 1010.44/s)  LR: 9.892e-04  Data: 0.011 (0.017)
Train: 20 [ 350/1251 ( 28%)]  Loss: 3.979 (4.23)  Time: 0.992s, 1032.58/s  (1.011s, 1012.90/s)  LR: 9.892e-04  Data: 0.010 (0.016)
Train: 20 [ 400/1251 ( 32%)]  Loss: 4.414 (4.25)  Time: 0.995s, 1028.90/s  (1.010s, 1013.37/s)  LR: 9.892e-04  Data: 0.011 (0.015)
Train: 20 [ 450/1251 ( 36%)]  Loss: 3.990 (4.23)  Time: 0.997s, 1027.00/s  (1.009s, 1014.98/s)  LR: 9.892e-04  Data: 0.012 (0.015)
Train: 20 [ 500/1251 ( 40%)]  Loss: 4.313 (4.24)  Time: 0.997s, 1027.13/s  (1.008s, 1015.92/s)  LR: 9.892e-04  Data: 0.010 (0.014)
Train: 20 [ 550/1251 ( 44%)]  Loss: 4.066 (4.22)  Time: 0.996s, 1027.76/s  (1.007s, 1016.74/s)  LR: 9.892e-04  Data: 0.011 (0.014)
Train: 20 [ 600/1251 ( 48%)]  Loss: 4.121 (4.21)  Time: 1.000s, 1024.35/s  (1.006s, 1017.41/s)  LR: 9.892e-04  Data: 0.011 (0.014)
Train: 20 [ 650/1251 ( 52%)]  Loss: 4.307 (4.22)  Time: 0.998s, 1026.52/s  (1.006s, 1018.10/s)  LR: 9.892e-04  Data: 0.010 (0.014)
Train: 20 [ 700/1251 ( 56%)]  Loss: 3.982 (4.20)  Time: 1.050s,  975.15/s  (1.006s, 1017.50/s)  LR: 9.892e-04  Data: 0.011 (0.013)
Train: 20 [ 750/1251 ( 60%)]  Loss: 3.806 (4.18)  Time: 0.993s, 1031.00/s  (1.006s, 1017.87/s)  LR: 9.892e-04  Data: 0.010 (0.013)
Train: 20 [ 800/1251 ( 64%)]  Loss: 3.874 (4.16)  Time: 0.999s, 1024.67/s  (1.005s, 1018.47/s)  LR: 9.892e-04  Data: 0.011 (0.013)
Train: 20 [ 850/1251 ( 68%)]  Loss: 4.136 (4.16)  Time: 0.998s, 1025.65/s  (1.005s, 1018.91/s)  LR: 9.892e-04  Data: 0.011 (0.013)
Train: 20 [ 900/1251 ( 72%)]  Loss: 4.131 (4.16)  Time: 0.999s, 1024.72/s  (1.005s, 1019.34/s)  LR: 9.892e-04  Data: 0.011 (0.013)
Train: 20 [ 950/1251 ( 76%)]  Loss: 4.430 (4.17)  Time: 0.998s, 1026.33/s  (1.004s, 1019.58/s)  LR: 9.892e-04  Data: 0.011 (0.013)
Train: 20 [1000/1251 ( 80%)]  Loss: 4.119 (4.17)  Time: 1.014s, 1009.68/s  (1.004s, 1019.80/s)  LR: 9.892e-04  Data: 0.011 (0.013)
Train: 20 [1050/1251 ( 84%)]  Loss: 4.620 (4.19)  Time: 1.064s,  962.59/s  (1.004s, 1019.50/s)  LR: 9.892e-04  Data: 0.011 (0.013)
Train: 20 [1100/1251 ( 88%)]  Loss: 4.540 (4.21)  Time: 0.995s, 1029.25/s  (1.005s, 1019.15/s)  LR: 9.892e-04  Data: 0.011 (0.013)
Train: 20 [1150/1251 ( 92%)]  Loss: 4.729 (4.23)  Time: 1.008s, 1016.26/s  (1.005s, 1018.92/s)  LR: 9.892e-04  Data: 0.014 (0.012)
Train: 20 [1200/1251 ( 96%)]  Loss: 4.397 (4.23)  Time: 1.044s,  981.30/s  (1.005s, 1019.10/s)  LR: 9.892e-04  Data: 0.011 (0.012)
Train: 20 [1250/1251 (100%)]  Loss: 4.126 (4.23)  Time: 0.983s, 1041.86/s  (1.005s, 1019.29/s)  LR: 9.892e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.608 (1.608)  Loss:  1.2913 (1.2913)  Acc@1: 80.2734 (80.2734)  Acc@5: 94.6289 (94.6289)
Test: [  48/48]  Time: 0.245 (0.584)  Loss:  1.3285 (2.0215)  Acc@1: 78.7736 (61.6700)  Acc@5: 92.8066 (84.9440)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-19.pth.tar', 62.33399989257813)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-20.pth.tar', 61.66999995849609)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-17.pth.tar', 61.20200005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-18.pth.tar', 60.55799993652344)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-16.pth.tar', 59.4699999633789)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-15.pth.tar', 59.024000078125)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-14.pth.tar', 57.73600013427735)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-13.pth.tar', 55.944000102539064)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-12.pth.tar', 55.66999995849609)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-11.pth.tar', 53.62400000976562)

Train: 21 [   0/1251 (  0%)]  Loss: 3.639 (3.64)  Time: 4.324s,  236.84/s  (4.324s,  236.84/s)  LR: 9.881e-04  Data: 3.054 (3.054)
Train: 21 [  50/1251 (  4%)]  Loss: 4.338 (3.99)  Time: 1.037s,  987.68/s  (1.091s,  938.58/s)  LR: 9.881e-04  Data: 0.011 (0.071)
Train: 21 [ 100/1251 (  8%)]  Loss: 4.301 (4.09)  Time: 0.995s, 1029.57/s  (1.046s,  979.02/s)  LR: 9.881e-04  Data: 0.011 (0.041)
Train: 21 [ 150/1251 ( 12%)]  Loss: 4.397 (4.17)  Time: 0.997s, 1027.57/s  (1.032s,  992.20/s)  LR: 9.881e-04  Data: 0.011 (0.031)
Train: 21 [ 200/1251 ( 16%)]  Loss: 3.790 (4.09)  Time: 0.996s, 1028.27/s  (1.024s, 1000.44/s)  LR: 9.881e-04  Data: 0.011 (0.026)
Train: 21 [ 250/1251 ( 20%)]  Loss: 4.295 (4.13)  Time: 1.033s,  991.06/s  (1.025s,  999.38/s)  LR: 9.881e-04  Data: 0.010 (0.023)
Train: 21 [ 300/1251 ( 24%)]  Loss: 3.952 (4.10)  Time: 0.994s, 1029.67/s  (1.021s, 1003.12/s)  LR: 9.881e-04  Data: 0.012 (0.021)
Train: 21 [ 350/1251 ( 28%)]  Loss: 4.180 (4.11)  Time: 0.996s, 1028.01/s  (1.018s, 1005.85/s)  LR: 9.881e-04  Data: 0.011 (0.020)
Train: 21 [ 400/1251 ( 32%)]  Loss: 4.051 (4.10)  Time: 0.997s, 1027.44/s  (1.016s, 1007.72/s)  LR: 9.881e-04  Data: 0.011 (0.019)
Train: 21 [ 450/1251 ( 36%)]  Loss: 4.231 (4.12)  Time: 0.996s, 1027.63/s  (1.014s, 1009.63/s)  LR: 9.881e-04  Data: 0.012 (0.018)
Train: 21 [ 500/1251 ( 40%)]  Loss: 4.127 (4.12)  Time: 0.996s, 1027.80/s  (1.013s, 1011.24/s)  LR: 9.881e-04  Data: 0.011 (0.017)
Train: 21 [ 550/1251 ( 44%)]  Loss: 4.096 (4.12)  Time: 1.002s, 1021.83/s  (1.012s, 1012.20/s)  LR: 9.881e-04  Data: 0.015 (0.017)
Train: 21 [ 600/1251 ( 48%)]  Loss: 4.182 (4.12)  Time: 0.994s, 1030.59/s  (1.011s, 1013.03/s)  LR: 9.881e-04  Data: 0.012 (0.016)
Train: 21 [ 650/1251 ( 52%)]  Loss: 4.094 (4.12)  Time: 0.998s, 1025.56/s  (1.010s, 1013.98/s)  LR: 9.881e-04  Data: 0.013 (0.016)
Train: 21 [ 700/1251 ( 56%)]  Loss: 4.279 (4.13)  Time: 0.996s, 1028.15/s  (1.009s, 1014.83/s)  LR: 9.881e-04  Data: 0.012 (0.015)
Train: 21 [ 750/1251 ( 60%)]  Loss: 4.674 (4.16)  Time: 0.996s, 1028.42/s  (1.010s, 1014.27/s)  LR: 9.881e-04  Data: 0.011 (0.015)
Train: 21 [ 800/1251 ( 64%)]  Loss: 4.392 (4.18)  Time: 0.996s, 1028.20/s  (1.009s, 1015.01/s)  LR: 9.881e-04  Data: 0.011 (0.015)
Train: 21 [ 850/1251 ( 68%)]  Loss: 4.366 (4.19)  Time: 0.994s, 1030.20/s  (1.008s, 1015.64/s)  LR: 9.881e-04  Data: 0.011 (0.015)
Train: 21 [ 900/1251 ( 72%)]  Loss: 4.469 (4.20)  Time: 1.006s, 1018.03/s  (1.008s, 1016.25/s)  LR: 9.881e-04  Data: 0.010 (0.015)
Train: 21 [ 950/1251 ( 76%)]  Loss: 4.509 (4.22)  Time: 0.995s, 1029.62/s  (1.007s, 1016.52/s)  LR: 9.881e-04  Data: 0.011 (0.014)
Train: 21 [1000/1251 ( 80%)]  Loss: 3.917 (4.20)  Time: 0.997s, 1027.37/s  (1.007s, 1016.56/s)  LR: 9.881e-04  Data: 0.012 (0.014)
Train: 21 [1050/1251 ( 84%)]  Loss: 4.248 (4.21)  Time: 0.996s, 1028.36/s  (1.007s, 1016.89/s)  LR: 9.881e-04  Data: 0.011 (0.014)
Train: 21 [1100/1251 ( 88%)]  Loss: 4.180 (4.20)  Time: 0.996s, 1027.76/s  (1.007s, 1016.58/s)  LR: 9.881e-04  Data: 0.012 (0.014)
Train: 21 [1150/1251 ( 92%)]  Loss: 4.530 (4.22)  Time: 0.998s, 1026.24/s  (1.007s, 1017.02/s)  LR: 9.881e-04  Data: 0.012 (0.014)
Train: 21 [1200/1251 ( 96%)]  Loss: 4.379 (4.22)  Time: 0.997s, 1026.77/s  (1.006s, 1017.44/s)  LR: 9.881e-04  Data: 0.012 (0.014)
Train: 21 [1250/1251 (100%)]  Loss: 4.239 (4.23)  Time: 0.980s, 1044.64/s  (1.006s, 1017.66/s)  LR: 9.881e-04  Data: 0.000 (0.014)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.674 (1.674)  Loss:  1.0802 (1.0802)  Acc@1: 82.5195 (82.5195)  Acc@5: 95.3125 (95.3125)
Test: [  48/48]  Time: 0.245 (0.569)  Loss:  1.3056 (1.8389)  Acc@1: 77.0047 (62.6920)  Acc@5: 91.7453 (85.6860)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-21.pth.tar', 62.691999965820315)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-19.pth.tar', 62.33399989257813)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-20.pth.tar', 61.66999995849609)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-17.pth.tar', 61.20200005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-18.pth.tar', 60.55799993652344)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-16.pth.tar', 59.4699999633789)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-15.pth.tar', 59.024000078125)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-14.pth.tar', 57.73600013427735)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-13.pth.tar', 55.944000102539064)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-12.pth.tar', 55.66999995849609)

Train: 22 [   0/1251 (  0%)]  Loss: 4.432 (4.43)  Time: 2.799s,  365.87/s  (2.799s,  365.87/s)  LR: 9.869e-04  Data: 1.837 (1.837)
Train: 22 [  50/1251 (  4%)]  Loss: 4.200 (4.32)  Time: 1.037s,  987.80/s  (1.040s,  984.33/s)  LR: 9.869e-04  Data: 0.012 (0.047)
Train: 22 [ 100/1251 (  8%)]  Loss: 4.144 (4.26)  Time: 1.036s,  987.96/s  (1.025s,  999.42/s)  LR: 9.869e-04  Data: 0.011 (0.030)
Train: 22 [ 150/1251 ( 12%)]  Loss: 4.054 (4.21)  Time: 1.001s, 1022.63/s  (1.019s, 1004.72/s)  LR: 9.869e-04  Data: 0.011 (0.024)
Train: 22 [ 200/1251 ( 16%)]  Loss: 4.159 (4.20)  Time: 0.994s, 1029.81/s  (1.014s, 1010.15/s)  LR: 9.869e-04  Data: 0.011 (0.021)
Train: 22 [ 250/1251 ( 20%)]  Loss: 4.366 (4.23)  Time: 0.994s, 1030.41/s  (1.011s, 1012.82/s)  LR: 9.869e-04  Data: 0.011 (0.019)
Train: 22 [ 300/1251 ( 24%)]  Loss: 4.297 (4.24)  Time: 1.034s,  990.29/s  (1.013s, 1011.13/s)  LR: 9.869e-04  Data: 0.010 (0.017)
Train: 22 [ 350/1251 ( 28%)]  Loss: 4.319 (4.25)  Time: 1.011s, 1013.21/s  (1.013s, 1010.37/s)  LR: 9.869e-04  Data: 0.010 (0.017)
Train: 22 [ 400/1251 ( 32%)]  Loss: 4.005 (4.22)  Time: 1.004s, 1019.91/s  (1.013s, 1010.64/s)  LR: 9.869e-04  Data: 0.011 (0.016)
Train: 22 [ 450/1251 ( 36%)]  Loss: 4.240 (4.22)  Time: 0.996s, 1028.04/s  (1.011s, 1012.37/s)  LR: 9.869e-04  Data: 0.012 (0.015)
Train: 22 [ 500/1251 ( 40%)]  Loss: 3.929 (4.20)  Time: 0.999s, 1024.82/s  (1.010s, 1013.81/s)  LR: 9.869e-04  Data: 0.010 (0.015)
Train: 22 [ 550/1251 ( 44%)]  Loss: 4.500 (4.22)  Time: 1.001s, 1022.57/s  (1.010s, 1014.19/s)  LR: 9.869e-04  Data: 0.011 (0.015)
Train: 22 [ 600/1251 ( 48%)]  Loss: 4.377 (4.23)  Time: 0.994s, 1030.47/s  (1.009s, 1015.29/s)  LR: 9.869e-04  Data: 0.012 (0.014)
Train: 22 [ 650/1251 ( 52%)]  Loss: 4.132 (4.23)  Time: 1.058s,  968.02/s  (1.008s, 1015.49/s)  LR: 9.869e-04  Data: 0.014 (0.014)
Train: 22 [ 700/1251 ( 56%)]  Loss: 4.191 (4.22)  Time: 0.996s, 1028.51/s  (1.008s, 1015.60/s)  LR: 9.869e-04  Data: 0.010 (0.014)
Train: 22 [ 750/1251 ( 60%)]  Loss: 4.500 (4.24)  Time: 0.998s, 1026.40/s  (1.008s, 1016.21/s)  LR: 9.869e-04  Data: 0.011 (0.014)
Train: 22 [ 800/1251 ( 64%)]  Loss: 4.144 (4.23)  Time: 0.996s, 1027.65/s  (1.007s, 1016.81/s)  LR: 9.869e-04  Data: 0.011 (0.014)
Train: 22 [ 850/1251 ( 68%)]  Loss: 3.945 (4.22)  Time: 0.994s, 1029.83/s  (1.007s, 1017.01/s)  LR: 9.869e-04  Data: 0.012 (0.013)
Train: 22 [ 900/1251 ( 72%)]  Loss: 4.237 (4.22)  Time: 0.994s, 1030.41/s  (1.007s, 1016.56/s)  LR: 9.869e-04  Data: 0.010 (0.013)
Train: 22 [ 950/1251 ( 76%)]  Loss: 4.427 (4.23)  Time: 1.018s, 1005.61/s  (1.007s, 1017.02/s)  LR: 9.869e-04  Data: 0.010 (0.013)
Train: 22 [1000/1251 ( 80%)]  Loss: 4.008 (4.22)  Time: 0.995s, 1028.74/s  (1.006s, 1017.40/s)  LR: 9.869e-04  Data: 0.011 (0.013)
Train: 22 [1050/1251 ( 84%)]  Loss: 4.578 (4.24)  Time: 0.998s, 1025.71/s  (1.007s, 1017.38/s)  LR: 9.869e-04  Data: 0.011 (0.013)
Train: 22 [1100/1251 ( 88%)]  Loss: 4.453 (4.25)  Time: 0.997s, 1026.98/s  (1.006s, 1017.41/s)  LR: 9.869e-04  Data: 0.012 (0.013)
Train: 22 [1150/1251 ( 92%)]  Loss: 4.145 (4.24)  Time: 1.058s,  967.71/s  (1.006s, 1017.55/s)  LR: 9.869e-04  Data: 0.010 (0.013)
Train: 22 [1200/1251 ( 96%)]  Loss: 4.434 (4.25)  Time: 1.012s, 1012.16/s  (1.006s, 1017.59/s)  LR: 9.869e-04  Data: 0.012 (0.013)
Train: 22 [1250/1251 (100%)]  Loss: 3.815 (4.23)  Time: 1.015s, 1008.50/s  (1.006s, 1017.65/s)  LR: 9.869e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.671 (1.671)  Loss:  1.1759 (1.1759)  Acc@1: 78.3203 (78.3203)  Acc@5: 93.2617 (93.2617)
Test: [  48/48]  Time: 0.245 (0.569)  Loss:  1.1295 (1.7908)  Acc@1: 78.6557 (63.0040)  Acc@5: 92.6887 (85.8720)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-22.pth.tar', 63.00400003662109)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-21.pth.tar', 62.691999965820315)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-19.pth.tar', 62.33399989257813)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-20.pth.tar', 61.66999995849609)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-17.pth.tar', 61.20200005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-18.pth.tar', 60.55799993652344)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-16.pth.tar', 59.4699999633789)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-15.pth.tar', 59.024000078125)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-14.pth.tar', 57.73600013427735)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-13.pth.tar', 55.944000102539064)

Train: 23 [   0/1251 (  0%)]  Loss: 4.312 (4.31)  Time: 2.356s,  434.59/s  (2.356s,  434.59/s)  LR: 9.857e-04  Data: 1.391 (1.391)
Train: 23 [  50/1251 (  4%)]  Loss: 4.197 (4.25)  Time: 1.035s,  989.16/s  (1.035s,  989.78/s)  LR: 9.857e-04  Data: 0.011 (0.041)
Train: 23 [ 100/1251 (  8%)]  Loss: 4.339 (4.28)  Time: 1.001s, 1022.59/s  (1.018s, 1006.17/s)  LR: 9.857e-04  Data: 0.011 (0.026)
Train: 23 [ 150/1251 ( 12%)]  Loss: 3.991 (4.21)  Time: 0.998s, 1025.82/s  (1.012s, 1012.04/s)  LR: 9.857e-04  Data: 0.013 (0.021)
Train: 23 [ 200/1251 ( 16%)]  Loss: 4.535 (4.27)  Time: 1.001s, 1022.74/s  (1.009s, 1014.74/s)  LR: 9.857e-04  Data: 0.012 (0.019)
Train: 23 [ 250/1251 ( 20%)]  Loss: 4.267 (4.27)  Time: 1.004s, 1019.68/s  (1.009s, 1014.87/s)  LR: 9.857e-04  Data: 0.015 (0.017)
Train: 23 [ 300/1251 ( 24%)]  Loss: 4.214 (4.27)  Time: 1.062s,  964.12/s  (1.009s, 1015.19/s)  LR: 9.857e-04  Data: 0.011 (0.016)
Train: 23 [ 350/1251 ( 28%)]  Loss: 4.188 (4.26)  Time: 0.995s, 1028.65/s  (1.008s, 1016.15/s)  LR: 9.857e-04  Data: 0.012 (0.016)
Train: 23 [ 400/1251 ( 32%)]  Loss: 3.968 (4.22)  Time: 0.995s, 1029.52/s  (1.008s, 1016.19/s)  LR: 9.857e-04  Data: 0.011 (0.015)
Train: 23 [ 450/1251 ( 36%)]  Loss: 4.167 (4.22)  Time: 1.031s,  993.57/s  (1.008s, 1015.64/s)  LR: 9.857e-04  Data: 0.011 (0.015)
Train: 23 [ 500/1251 ( 40%)]  Loss: 4.122 (4.21)  Time: 1.103s,  928.58/s  (1.009s, 1014.70/s)  LR: 9.857e-04  Data: 0.015 (0.014)
Train: 23 [ 550/1251 ( 44%)]  Loss: 4.264 (4.21)  Time: 0.997s, 1026.78/s  (1.008s, 1015.72/s)  LR: 9.857e-04  Data: 0.012 (0.014)
Train: 23 [ 600/1251 ( 48%)]  Loss: 4.408 (4.23)  Time: 0.996s, 1027.74/s  (1.007s, 1016.51/s)  LR: 9.857e-04  Data: 0.012 (0.014)
Train: 23 [ 650/1251 ( 52%)]  Loss: 4.064 (4.22)  Time: 1.041s,  983.86/s  (1.007s, 1016.60/s)  LR: 9.857e-04  Data: 0.010 (0.014)
Train: 23 [ 700/1251 ( 56%)]  Loss: 3.877 (4.19)  Time: 1.002s, 1022.24/s  (1.007s, 1016.95/s)  LR: 9.857e-04  Data: 0.011 (0.013)
Train: 23 [ 750/1251 ( 60%)]  Loss: 3.847 (4.17)  Time: 0.997s, 1026.94/s  (1.006s, 1017.51/s)  LR: 9.857e-04  Data: 0.012 (0.013)
Train: 23 [ 800/1251 ( 64%)]  Loss: 4.341 (4.18)  Time: 0.997s, 1026.61/s  (1.006s, 1017.84/s)  LR: 9.857e-04  Data: 0.012 (0.013)
Train: 23 [ 850/1251 ( 68%)]  Loss: 4.363 (4.19)  Time: 1.002s, 1021.50/s  (1.006s, 1017.91/s)  LR: 9.857e-04  Data: 0.010 (0.013)
Train: 23 [ 900/1251 ( 72%)]  Loss: 3.675 (4.17)  Time: 0.996s, 1027.83/s  (1.006s, 1018.22/s)  LR: 9.857e-04  Data: 0.012 (0.013)
Train: 23 [ 950/1251 ( 76%)]  Loss: 4.193 (4.17)  Time: 0.996s, 1027.60/s  (1.005s, 1018.42/s)  LR: 9.857e-04  Data: 0.012 (0.013)
Train: 23 [1000/1251 ( 80%)]  Loss: 4.020 (4.16)  Time: 0.996s, 1028.20/s  (1.005s, 1018.89/s)  LR: 9.857e-04  Data: 0.011 (0.013)
Train: 23 [1050/1251 ( 84%)]  Loss: 4.056 (4.15)  Time: 0.998s, 1026.14/s  (1.005s, 1018.93/s)  LR: 9.857e-04  Data: 0.012 (0.013)
Train: 23 [1100/1251 ( 88%)]  Loss: 4.211 (4.16)  Time: 0.996s, 1027.91/s  (1.005s, 1019.18/s)  LR: 9.857e-04  Data: 0.012 (0.013)
Train: 23 [1150/1251 ( 92%)]  Loss: 4.038 (4.15)  Time: 0.995s, 1029.34/s  (1.004s, 1019.46/s)  LR: 9.857e-04  Data: 0.012 (0.013)
Train: 23 [1200/1251 ( 96%)]  Loss: 4.132 (4.15)  Time: 1.003s, 1021.36/s  (1.004s, 1019.50/s)  LR: 9.857e-04  Data: 0.011 (0.013)
Train: 23 [1250/1251 (100%)]  Loss: 4.184 (4.15)  Time: 0.983s, 1041.88/s  (1.004s, 1019.65/s)  LR: 9.857e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.603 (1.603)  Loss:  1.1117 (1.1117)  Acc@1: 82.7148 (82.7148)  Acc@5: 95.0195 (95.0195)
Test: [  48/48]  Time: 0.245 (0.565)  Loss:  1.1224 (1.8033)  Acc@1: 79.9528 (64.0100)  Acc@5: 93.6321 (86.1060)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-23.pth.tar', 64.00999995361327)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-22.pth.tar', 63.00400003662109)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-21.pth.tar', 62.691999965820315)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-19.pth.tar', 62.33399989257813)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-20.pth.tar', 61.66999995849609)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-17.pth.tar', 61.20200005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-18.pth.tar', 60.55799993652344)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-16.pth.tar', 59.4699999633789)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-15.pth.tar', 59.024000078125)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-14.pth.tar', 57.73600013427735)

Train: 24 [   0/1251 (  0%)]  Loss: 4.327 (4.33)  Time: 2.412s,  424.47/s  (2.412s,  424.47/s)  LR: 9.844e-04  Data: 1.446 (1.446)
Train: 24 [  50/1251 (  4%)]  Loss: 3.928 (4.13)  Time: 1.001s, 1023.18/s  (1.032s,  992.26/s)  LR: 9.844e-04  Data: 0.010 (0.040)
Train: 24 [ 100/1251 (  8%)]  Loss: 4.513 (4.26)  Time: 0.993s, 1031.61/s  (1.015s, 1008.61/s)  LR: 9.844e-04  Data: 0.010 (0.026)
Train: 24 [ 150/1251 ( 12%)]  Loss: 3.778 (4.14)  Time: 0.997s, 1027.51/s  (1.010s, 1014.15/s)  LR: 9.844e-04  Data: 0.011 (0.021)
Train: 24 [ 200/1251 ( 16%)]  Loss: 4.292 (4.17)  Time: 0.994s, 1029.90/s  (1.007s, 1017.14/s)  LR: 9.844e-04  Data: 0.011 (0.019)
Train: 24 [ 250/1251 ( 20%)]  Loss: 3.860 (4.12)  Time: 0.998s, 1026.22/s  (1.007s, 1016.72/s)  LR: 9.844e-04  Data: 0.011 (0.017)
Train: 24 [ 300/1251 ( 24%)]  Loss: 3.816 (4.07)  Time: 0.995s, 1029.16/s  (1.009s, 1014.68/s)  LR: 9.844e-04  Data: 0.011 (0.016)
Train: 24 [ 350/1251 ( 28%)]  Loss: 4.388 (4.11)  Time: 0.997s, 1026.81/s  (1.009s, 1015.21/s)  LR: 9.844e-04  Data: 0.012 (0.015)
Train: 24 [ 400/1251 ( 32%)]  Loss: 3.725 (4.07)  Time: 1.004s, 1019.65/s  (1.010s, 1013.69/s)  LR: 9.844e-04  Data: 0.011 (0.015)
Train: 24 [ 450/1251 ( 36%)]  Loss: 3.924 (4.06)  Time: 0.997s, 1026.86/s  (1.010s, 1013.84/s)  LR: 9.844e-04  Data: 0.011 (0.014)
Train: 24 [ 500/1251 ( 40%)]  Loss: 3.609 (4.01)  Time: 0.995s, 1028.71/s  (1.010s, 1013.74/s)  LR: 9.844e-04  Data: 0.011 (0.014)
Train: 24 [ 550/1251 ( 44%)]  Loss: 3.615 (3.98)  Time: 0.998s, 1025.63/s  (1.009s, 1014.65/s)  LR: 9.844e-04  Data: 0.011 (0.014)
Train: 24 [ 600/1251 ( 48%)]  Loss: 4.272 (4.00)  Time: 0.993s, 1031.09/s  (1.008s, 1015.62/s)  LR: 9.844e-04  Data: 0.011 (0.014)
Train: 24 [ 650/1251 ( 52%)]  Loss: 3.972 (4.00)  Time: 0.996s, 1027.69/s  (1.008s, 1016.37/s)  LR: 9.844e-04  Data: 0.011 (0.013)
Train: 24 [ 700/1251 ( 56%)]  Loss: 4.259 (4.02)  Time: 0.997s, 1026.99/s  (1.007s, 1016.40/s)  LR: 9.844e-04  Data: 0.011 (0.013)
Train: 24 [ 750/1251 ( 60%)]  Loss: 4.241 (4.03)  Time: 0.992s, 1031.79/s  (1.007s, 1016.91/s)  LR: 9.844e-04  Data: 0.011 (0.013)
Train: 24 [ 800/1251 ( 64%)]  Loss: 4.064 (4.03)  Time: 0.992s, 1032.08/s  (1.007s, 1017.30/s)  LR: 9.844e-04  Data: 0.010 (0.013)
Train: 24 [ 850/1251 ( 68%)]  Loss: 3.927 (4.03)  Time: 1.003s, 1020.62/s  (1.006s, 1017.57/s)  LR: 9.844e-04  Data: 0.012 (0.013)
Train: 24 [ 900/1251 ( 72%)]  Loss: 4.328 (4.04)  Time: 0.995s, 1029.31/s  (1.006s, 1018.06/s)  LR: 9.844e-04  Data: 0.011 (0.013)
Train: 24 [ 950/1251 ( 76%)]  Loss: 4.615 (4.07)  Time: 0.992s, 1032.67/s  (1.006s, 1018.24/s)  LR: 9.844e-04  Data: 0.011 (0.013)
Train: 24 [1000/1251 ( 80%)]  Loss: 4.019 (4.07)  Time: 0.996s, 1027.74/s  (1.005s, 1018.66/s)  LR: 9.844e-04  Data: 0.012 (0.013)
Train: 24 [1050/1251 ( 84%)]  Loss: 4.048 (4.07)  Time: 1.032s,  991.84/s  (1.005s, 1018.49/s)  LR: 9.844e-04  Data: 0.011 (0.013)
Train: 24 [1100/1251 ( 88%)]  Loss: 3.775 (4.06)  Time: 1.031s,  993.67/s  (1.005s, 1018.44/s)  LR: 9.844e-04  Data: 0.011 (0.013)
Train: 24 [1150/1251 ( 92%)]  Loss: 3.853 (4.05)  Time: 0.995s, 1029.47/s  (1.006s, 1017.87/s)  LR: 9.844e-04  Data: 0.011 (0.012)
Train: 24 [1200/1251 ( 96%)]  Loss: 4.165 (4.05)  Time: 1.065s,  961.51/s  (1.006s, 1017.47/s)  LR: 9.844e-04  Data: 0.011 (0.012)
Train: 24 [1250/1251 (100%)]  Loss: 4.152 (4.06)  Time: 0.983s, 1041.49/s  (1.006s, 1017.40/s)  LR: 9.844e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.606 (1.606)  Loss:  0.9136 (0.9136)  Acc@1: 83.2031 (83.2031)  Acc@5: 95.6055 (95.6055)
Test: [  48/48]  Time: 0.245 (0.568)  Loss:  1.0910 (1.7656)  Acc@1: 79.3632 (64.2400)  Acc@5: 92.8066 (86.2980)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-24.pth.tar', 64.23999995605469)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-23.pth.tar', 64.00999995361327)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-22.pth.tar', 63.00400003662109)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-21.pth.tar', 62.691999965820315)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-19.pth.tar', 62.33399989257813)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-20.pth.tar', 61.66999995849609)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-17.pth.tar', 61.20200005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-18.pth.tar', 60.55799993652344)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-16.pth.tar', 59.4699999633789)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-15.pth.tar', 59.024000078125)

Train: 25 [   0/1251 (  0%)]  Loss: 4.012 (4.01)  Time: 2.529s,  404.87/s  (2.529s,  404.87/s)  LR: 9.831e-04  Data: 1.568 (1.568)
Train: 25 [  50/1251 (  4%)]  Loss: 4.196 (4.10)  Time: 0.998s, 1025.80/s  (1.043s,  981.95/s)  LR: 9.831e-04  Data: 0.012 (0.042)
Train: 25 [ 100/1251 (  8%)]  Loss: 4.156 (4.12)  Time: 0.995s, 1029.32/s  (1.028s,  995.97/s)  LR: 9.831e-04  Data: 0.011 (0.027)
Train: 25 [ 150/1251 ( 12%)]  Loss: 4.153 (4.13)  Time: 0.991s, 1033.07/s  (1.022s, 1002.28/s)  LR: 9.831e-04  Data: 0.011 (0.022)
Train: 25 [ 200/1251 ( 16%)]  Loss: 4.122 (4.13)  Time: 0.995s, 1029.25/s  (1.020s, 1004.03/s)  LR: 9.831e-04  Data: 0.011 (0.019)
Train: 25 [ 250/1251 ( 20%)]  Loss: 3.785 (4.07)  Time: 0.994s, 1029.94/s  (1.017s, 1006.67/s)  LR: 9.831e-04  Data: 0.011 (0.018)
Train: 25 [ 300/1251 ( 24%)]  Loss: 3.769 (4.03)  Time: 0.996s, 1027.94/s  (1.014s, 1009.79/s)  LR: 9.831e-04  Data: 0.011 (0.017)
Train: 25 [ 350/1251 ( 28%)]  Loss: 4.022 (4.03)  Time: 0.996s, 1028.42/s  (1.012s, 1011.94/s)  LR: 9.831e-04  Data: 0.011 (0.016)
Train: 25 [ 400/1251 ( 32%)]  Loss: 4.208 (4.05)  Time: 0.996s, 1028.35/s  (1.012s, 1011.48/s)  LR: 9.831e-04  Data: 0.012 (0.015)
Train: 25 [ 450/1251 ( 36%)]  Loss: 4.220 (4.06)  Time: 0.995s, 1028.63/s  (1.011s, 1012.50/s)  LR: 9.831e-04  Data: 0.011 (0.015)
Train: 25 [ 500/1251 ( 40%)]  Loss: 4.255 (4.08)  Time: 0.996s, 1027.62/s  (1.010s, 1013.47/s)  LR: 9.831e-04  Data: 0.011 (0.014)
Train: 25 [ 550/1251 ( 44%)]  Loss: 4.276 (4.10)  Time: 1.044s,  980.46/s  (1.011s, 1013.31/s)  LR: 9.831e-04  Data: 0.011 (0.014)
Train: 25 [ 600/1251 ( 48%)]  Loss: 4.046 (4.09)  Time: 0.995s, 1029.19/s  (1.011s, 1012.63/s)  LR: 9.831e-04  Data: 0.011 (0.014)
Train: 25 [ 650/1251 ( 52%)]  Loss: 3.864 (4.08)  Time: 0.996s, 1028.16/s  (1.011s, 1013.14/s)  LR: 9.831e-04  Data: 0.012 (0.014)
Train: 25 [ 700/1251 ( 56%)]  Loss: 3.996 (4.07)  Time: 0.994s, 1030.37/s  (1.010s, 1013.52/s)  LR: 9.831e-04  Data: 0.010 (0.013)
Train: 25 [ 750/1251 ( 60%)]  Loss: 4.171 (4.08)  Time: 1.065s,  961.19/s  (1.010s, 1013.68/s)  LR: 9.831e-04  Data: 0.011 (0.013)
Train: 25 [ 800/1251 ( 64%)]  Loss: 4.216 (4.09)  Time: 0.996s, 1028.23/s  (1.010s, 1013.90/s)  LR: 9.831e-04  Data: 0.011 (0.013)
Train: 25 [ 850/1251 ( 68%)]  Loss: 3.861 (4.07)  Time: 0.995s, 1029.06/s  (1.010s, 1013.38/s)  LR: 9.831e-04  Data: 0.011 (0.013)
Train: 25 [ 900/1251 ( 72%)]  Loss: 4.207 (4.08)  Time: 1.063s,  962.94/s  (1.011s, 1012.39/s)  LR: 9.831e-04  Data: 0.011 (0.013)
Train: 25 [ 950/1251 ( 76%)]  Loss: 3.932 (4.07)  Time: 0.996s, 1028.48/s  (1.011s, 1012.73/s)  LR: 9.831e-04  Data: 0.011 (0.013)
Train: 25 [1000/1251 ( 80%)]  Loss: 4.282 (4.08)  Time: 0.998s, 1026.39/s  (1.011s, 1012.45/s)  LR: 9.831e-04  Data: 0.010 (0.013)
Train: 25 [1050/1251 ( 84%)]  Loss: 4.411 (4.10)  Time: 1.035s,  989.78/s  (1.012s, 1012.13/s)  LR: 9.831e-04  Data: 0.011 (0.013)
Train: 25 [1100/1251 ( 88%)]  Loss: 4.279 (4.11)  Time: 0.998s, 1025.62/s  (1.011s, 1012.45/s)  LR: 9.831e-04  Data: 0.011 (0.013)
Train: 25 [1150/1251 ( 92%)]  Loss: 4.519 (4.12)  Time: 1.008s, 1015.56/s  (1.011s, 1012.91/s)  LR: 9.831e-04  Data: 0.011 (0.013)
Train: 25 [1200/1251 ( 96%)]  Loss: 4.122 (4.12)  Time: 0.996s, 1028.60/s  (1.010s, 1013.36/s)  LR: 9.831e-04  Data: 0.012 (0.013)
Train: 25 [1250/1251 (100%)]  Loss: 4.117 (4.12)  Time: 0.986s, 1038.82/s  (1.010s, 1013.76/s)  LR: 9.831e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.636 (1.636)  Loss:  1.1486 (1.1486)  Acc@1: 83.3984 (83.3984)  Acc@5: 95.5078 (95.5078)
Test: [  48/48]  Time: 0.245 (0.564)  Loss:  1.0966 (1.7620)  Acc@1: 81.1321 (65.2040)  Acc@5: 93.9858 (86.9900)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-25.pth.tar', 65.20399994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-24.pth.tar', 64.23999995605469)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-23.pth.tar', 64.00999995361327)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-22.pth.tar', 63.00400003662109)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-21.pth.tar', 62.691999965820315)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-19.pth.tar', 62.33399989257813)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-20.pth.tar', 61.66999995849609)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-17.pth.tar', 61.20200005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-18.pth.tar', 60.55799993652344)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-16.pth.tar', 59.4699999633789)

Train: 26 [   0/1251 (  0%)]  Loss: 4.210 (4.21)  Time: 2.391s,  428.19/s  (2.391s,  428.19/s)  LR: 9.818e-04  Data: 1.431 (1.431)
Train: 26 [  50/1251 (  4%)]  Loss: 3.544 (3.88)  Time: 0.997s, 1026.98/s  (1.035s,  989.42/s)  LR: 9.818e-04  Data: 0.012 (0.039)
Train: 26 [ 100/1251 (  8%)]  Loss: 4.391 (4.05)  Time: 1.064s,  962.83/s  (1.018s, 1005.86/s)  LR: 9.818e-04  Data: 0.011 (0.025)
Train: 26 [ 150/1251 ( 12%)]  Loss: 4.327 (4.12)  Time: 1.071s,  956.47/s  (1.015s, 1008.69/s)  LR: 9.818e-04  Data: 0.010 (0.021)
Train: 26 [ 200/1251 ( 16%)]  Loss: 3.850 (4.06)  Time: 0.997s, 1027.12/s  (1.013s, 1010.68/s)  LR: 9.818e-04  Data: 0.011 (0.018)
Train: 26 [ 250/1251 ( 20%)]  Loss: 3.852 (4.03)  Time: 0.997s, 1026.62/s  (1.011s, 1012.41/s)  LR: 9.818e-04  Data: 0.011 (0.017)
Train: 26 [ 300/1251 ( 24%)]  Loss: 4.270 (4.06)  Time: 0.998s, 1026.30/s  (1.009s, 1014.54/s)  LR: 9.818e-04  Data: 0.011 (0.016)
Train: 26 [ 350/1251 ( 28%)]  Loss: 4.290 (4.09)  Time: 0.997s, 1027.08/s  (1.009s, 1014.37/s)  LR: 9.818e-04  Data: 0.011 (0.015)
Train: 26 [ 400/1251 ( 32%)]  Loss: 4.484 (4.14)  Time: 0.994s, 1030.09/s  (1.009s, 1014.68/s)  LR: 9.818e-04  Data: 0.011 (0.015)
Train: 26 [ 450/1251 ( 36%)]  Loss: 4.354 (4.16)  Time: 1.002s, 1022.10/s  (1.011s, 1013.24/s)  LR: 9.818e-04  Data: 0.012 (0.014)
Train: 26 [ 500/1251 ( 40%)]  Loss: 4.108 (4.15)  Time: 0.997s, 1027.39/s  (1.010s, 1013.58/s)  LR: 9.818e-04  Data: 0.011 (0.014)
Train: 26 [ 550/1251 ( 44%)]  Loss: 3.931 (4.13)  Time: 0.997s, 1027.21/s  (1.010s, 1014.33/s)  LR: 9.818e-04  Data: 0.012 (0.014)
Train: 26 [ 600/1251 ( 48%)]  Loss: 4.241 (4.14)  Time: 1.000s, 1024.40/s  (1.009s, 1015.36/s)  LR: 9.818e-04  Data: 0.010 (0.014)
Train: 26 [ 650/1251 ( 52%)]  Loss: 4.220 (4.15)  Time: 1.062s,  963.79/s  (1.009s, 1014.55/s)  LR: 9.818e-04  Data: 0.011 (0.013)
Train: 26 [ 700/1251 ( 56%)]  Loss: 4.505 (4.17)  Time: 0.999s, 1024.99/s  (1.009s, 1014.54/s)  LR: 9.818e-04  Data: 0.011 (0.013)
Train: 26 [ 750/1251 ( 60%)]  Loss: 4.049 (4.16)  Time: 1.035s,  989.09/s  (1.009s, 1014.79/s)  LR: 9.818e-04  Data: 0.011 (0.013)
Train: 26 [ 800/1251 ( 64%)]  Loss: 4.475 (4.18)  Time: 0.999s, 1025.45/s  (1.009s, 1014.87/s)  LR: 9.818e-04  Data: 0.011 (0.013)
Train: 26 [ 850/1251 ( 68%)]  Loss: 4.530 (4.20)  Time: 1.005s, 1019.34/s  (1.009s, 1015.12/s)  LR: 9.818e-04  Data: 0.011 (0.013)
Train: 26 [ 900/1251 ( 72%)]  Loss: 4.048 (4.19)  Time: 1.000s, 1023.57/s  (1.008s, 1015.53/s)  LR: 9.818e-04  Data: 0.011 (0.013)
Train: 26 [ 950/1251 ( 76%)]  Loss: 3.460 (4.16)  Time: 0.998s, 1025.58/s  (1.008s, 1015.92/s)  LR: 9.818e-04  Data: 0.011 (0.013)
Train: 26 [1000/1251 ( 80%)]  Loss: 4.287 (4.16)  Time: 0.991s, 1033.10/s  (1.008s, 1016.03/s)  LR: 9.818e-04  Data: 0.011 (0.013)
Train: 26 [1050/1251 ( 84%)]  Loss: 4.109 (4.16)  Time: 1.050s,  975.40/s  (1.008s, 1016.20/s)  LR: 9.818e-04  Data: 0.011 (0.012)
Train: 26 [1100/1251 ( 88%)]  Loss: 4.032 (4.16)  Time: 1.067s,  959.92/s  (1.008s, 1015.93/s)  LR: 9.818e-04  Data: 0.015 (0.012)
Train: 26 [1150/1251 ( 92%)]  Loss: 4.140 (4.15)  Time: 0.995s, 1028.85/s  (1.009s, 1015.20/s)  LR: 9.818e-04  Data: 0.011 (0.012)
Train: 26 [1200/1251 ( 96%)]  Loss: 4.116 (4.15)  Time: 1.001s, 1023.36/s  (1.009s, 1014.87/s)  LR: 9.818e-04  Data: 0.011 (0.012)
Train: 26 [1250/1251 (100%)]  Loss: 3.481 (4.13)  Time: 0.985s, 1039.98/s  (1.009s, 1015.05/s)  LR: 9.818e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.623 (1.623)  Loss:  1.1396 (1.1396)  Acc@1: 79.6875 (79.6875)  Acc@5: 93.0664 (93.0664)
Test: [  48/48]  Time: 0.245 (0.571)  Loss:  1.0723 (1.7639)  Acc@1: 80.4245 (63.2460)  Acc@5: 93.0424 (85.6100)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-25.pth.tar', 65.20399994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-24.pth.tar', 64.23999995605469)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-23.pth.tar', 64.00999995361327)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-26.pth.tar', 63.24600002929687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-22.pth.tar', 63.00400003662109)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-21.pth.tar', 62.691999965820315)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-19.pth.tar', 62.33399989257813)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-20.pth.tar', 61.66999995849609)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-17.pth.tar', 61.20200005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-18.pth.tar', 60.55799993652344)

Train: 27 [   0/1251 (  0%)]  Loss: 4.478 (4.48)  Time: 2.579s,  397.07/s  (2.579s,  397.07/s)  LR: 9.803e-04  Data: 1.563 (1.563)
Train: 27 [  50/1251 (  4%)]  Loss: 4.511 (4.49)  Time: 0.997s, 1027.29/s  (1.044s,  980.94/s)  LR: 9.803e-04  Data: 0.011 (0.042)
Train: 27 [ 100/1251 (  8%)]  Loss: 4.385 (4.46)  Time: 0.992s, 1031.97/s  (1.021s, 1002.48/s)  LR: 9.803e-04  Data: 0.011 (0.027)
Train: 27 [ 150/1251 ( 12%)]  Loss: 3.901 (4.32)  Time: 0.993s, 1030.77/s  (1.018s, 1006.31/s)  LR: 9.803e-04  Data: 0.011 (0.021)
Train: 27 [ 200/1251 ( 16%)]  Loss: 4.212 (4.30)  Time: 0.997s, 1026.95/s  (1.016s, 1007.38/s)  LR: 9.803e-04  Data: 0.011 (0.019)
Train: 27 [ 250/1251 ( 20%)]  Loss: 4.084 (4.26)  Time: 0.998s, 1025.97/s  (1.015s, 1008.57/s)  LR: 9.803e-04  Data: 0.011 (0.017)
Train: 27 [ 300/1251 ( 24%)]  Loss: 4.369 (4.28)  Time: 0.996s, 1027.96/s  (1.013s, 1010.64/s)  LR: 9.803e-04  Data: 0.011 (0.016)
Train: 27 [ 350/1251 ( 28%)]  Loss: 4.624 (4.32)  Time: 0.999s, 1025.03/s  (1.014s, 1009.78/s)  LR: 9.803e-04  Data: 0.010 (0.016)
Train: 27 [ 400/1251 ( 32%)]  Loss: 3.400 (4.22)  Time: 1.003s, 1020.54/s  (1.012s, 1011.46/s)  LR: 9.803e-04  Data: 0.011 (0.015)
Train: 27 [ 450/1251 ( 36%)]  Loss: 3.660 (4.16)  Time: 0.994s, 1030.50/s  (1.011s, 1012.66/s)  LR: 9.803e-04  Data: 0.011 (0.015)
Train: 27 [ 500/1251 ( 40%)]  Loss: 3.691 (4.12)  Time: 0.996s, 1028.32/s  (1.010s, 1013.47/s)  LR: 9.803e-04  Data: 0.012 (0.014)
Train: 27 [ 550/1251 ( 44%)]  Loss: 4.232 (4.13)  Time: 1.000s, 1023.78/s  (1.011s, 1013.09/s)  LR: 9.803e-04  Data: 0.010 (0.014)
Train: 27 [ 600/1251 ( 48%)]  Loss: 4.174 (4.13)  Time: 1.061s,  965.56/s  (1.012s, 1011.94/s)  LR: 9.803e-04  Data: 0.011 (0.014)
Train: 27 [ 650/1251 ( 52%)]  Loss: 4.193 (4.14)  Time: 1.017s, 1006.73/s  (1.012s, 1011.72/s)  LR: 9.803e-04  Data: 0.010 (0.014)
Train: 27 [ 700/1251 ( 56%)]  Loss: 4.423 (4.16)  Time: 0.997s, 1026.94/s  (1.012s, 1012.23/s)  LR: 9.803e-04  Data: 0.011 (0.013)
Train: 27 [ 750/1251 ( 60%)]  Loss: 4.150 (4.16)  Time: 0.997s, 1027.42/s  (1.011s, 1013.07/s)  LR: 9.803e-04  Data: 0.011 (0.013)
Train: 27 [ 800/1251 ( 64%)]  Loss: 4.048 (4.15)  Time: 0.993s, 1031.22/s  (1.010s, 1013.90/s)  LR: 9.803e-04  Data: 0.011 (0.013)
Train: 27 [ 850/1251 ( 68%)]  Loss: 4.569 (4.17)  Time: 0.996s, 1027.97/s  (1.010s, 1013.58/s)  LR: 9.803e-04  Data: 0.012 (0.013)
Train: 27 [ 900/1251 ( 72%)]  Loss: 4.212 (4.17)  Time: 1.065s,  961.21/s  (1.010s, 1013.41/s)  LR: 9.803e-04  Data: 0.011 (0.013)
Train: 27 [ 950/1251 ( 76%)]  Loss: 4.396 (4.19)  Time: 0.993s, 1031.49/s  (1.010s, 1013.66/s)  LR: 9.803e-04  Data: 0.010 (0.013)
Train: 27 [1000/1251 ( 80%)]  Loss: 4.218 (4.19)  Time: 0.995s, 1029.42/s  (1.011s, 1013.13/s)  LR: 9.803e-04  Data: 0.010 (0.013)
Train: 27 [1050/1251 ( 84%)]  Loss: 4.249 (4.19)  Time: 0.994s, 1029.94/s  (1.010s, 1013.39/s)  LR: 9.803e-04  Data: 0.011 (0.013)
Train: 27 [1100/1251 ( 88%)]  Loss: 4.128 (4.19)  Time: 0.996s, 1028.40/s  (1.010s, 1013.58/s)  LR: 9.803e-04  Data: 0.011 (0.013)
Train: 27 [1150/1251 ( 92%)]  Loss: 4.373 (4.20)  Time: 1.033s,  990.93/s  (1.011s, 1012.72/s)  LR: 9.803e-04  Data: 0.012 (0.013)
Train: 27 [1200/1251 ( 96%)]  Loss: 4.033 (4.19)  Time: 1.003s, 1021.07/s  (1.011s, 1012.75/s)  LR: 9.803e-04  Data: 0.011 (0.013)
Train: 27 [1250/1251 (100%)]  Loss: 4.114 (4.19)  Time: 0.984s, 1040.53/s  (1.011s, 1013.26/s)  LR: 9.803e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.641 (1.641)  Loss:  1.1746 (1.1746)  Acc@1: 81.3477 (81.3477)  Acc@5: 95.2148 (95.2148)
Test: [  48/48]  Time: 0.245 (0.569)  Loss:  1.2745 (1.7848)  Acc@1: 76.7689 (65.5340)  Acc@5: 91.6274 (87.3660)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-27.pth.tar', 65.53399999267579)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-25.pth.tar', 65.20399994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-24.pth.tar', 64.23999995605469)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-23.pth.tar', 64.00999995361327)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-26.pth.tar', 63.24600002929687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-22.pth.tar', 63.00400003662109)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-21.pth.tar', 62.691999965820315)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-19.pth.tar', 62.33399989257813)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-20.pth.tar', 61.66999995849609)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-17.pth.tar', 61.20200005126953)

Train: 28 [   0/1251 (  0%)]  Loss: 4.103 (4.10)  Time: 2.490s,  411.32/s  (2.490s,  411.32/s)  LR: 9.789e-04  Data: 1.525 (1.525)
Train: 28 [  50/1251 (  4%)]  Loss: 4.191 (4.15)  Time: 0.997s, 1027.02/s  (1.030s,  993.73/s)  LR: 9.789e-04  Data: 0.011 (0.042)
Train: 28 [ 100/1251 (  8%)]  Loss: 4.016 (4.10)  Time: 1.021s, 1003.08/s  (1.022s, 1002.14/s)  LR: 9.789e-04  Data: 0.012 (0.027)
Train: 28 [ 150/1251 ( 12%)]  Loss: 4.096 (4.10)  Time: 1.062s,  964.30/s  (1.022s, 1001.78/s)  LR: 9.789e-04  Data: 0.011 (0.022)
Train: 28 [ 200/1251 ( 16%)]  Loss: 4.041 (4.09)  Time: 0.997s, 1027.38/s  (1.021s, 1003.37/s)  LR: 9.789e-04  Data: 0.012 (0.019)
Train: 28 [ 250/1251 ( 20%)]  Loss: 3.867 (4.05)  Time: 0.997s, 1026.94/s  (1.020s, 1004.15/s)  LR: 9.789e-04  Data: 0.011 (0.017)
Train: 28 [ 300/1251 ( 24%)]  Loss: 3.949 (4.04)  Time: 0.992s, 1032.35/s  (1.016s, 1007.47/s)  LR: 9.789e-04  Data: 0.010 (0.016)
Train: 28 [ 350/1251 ( 28%)]  Loss: 3.860 (4.02)  Time: 1.007s, 1016.79/s  (1.014s, 1009.53/s)  LR: 9.789e-04  Data: 0.011 (0.016)
Train: 28 [ 400/1251 ( 32%)]  Loss: 4.228 (4.04)  Time: 0.996s, 1028.22/s  (1.013s, 1010.67/s)  LR: 9.789e-04  Data: 0.011 (0.015)
Train: 28 [ 450/1251 ( 36%)]  Loss: 4.022 (4.04)  Time: 0.995s, 1029.40/s  (1.012s, 1012.25/s)  LR: 9.789e-04  Data: 0.011 (0.015)
Train: 28 [ 500/1251 ( 40%)]  Loss: 3.799 (4.02)  Time: 1.063s,  963.57/s  (1.013s, 1010.92/s)  LR: 9.789e-04  Data: 0.010 (0.014)
Train: 28 [ 550/1251 ( 44%)]  Loss: 3.918 (4.01)  Time: 0.997s, 1027.06/s  (1.012s, 1012.14/s)  LR: 9.789e-04  Data: 0.012 (0.014)
Train: 28 [ 600/1251 ( 48%)]  Loss: 4.096 (4.01)  Time: 0.998s, 1025.93/s  (1.011s, 1012.94/s)  LR: 9.789e-04  Data: 0.012 (0.014)
Train: 28 [ 650/1251 ( 52%)]  Loss: 3.848 (4.00)  Time: 1.000s, 1024.26/s  (1.010s, 1013.72/s)  LR: 9.789e-04  Data: 0.011 (0.014)
Train: 28 [ 700/1251 ( 56%)]  Loss: 4.170 (4.01)  Time: 0.994s, 1029.73/s  (1.009s, 1014.44/s)  LR: 9.789e-04  Data: 0.011 (0.013)
Train: 28 [ 750/1251 ( 60%)]  Loss: 3.657 (3.99)  Time: 1.037s,  986.99/s  (1.009s, 1015.08/s)  LR: 9.789e-04  Data: 0.012 (0.013)
Train: 28 [ 800/1251 ( 64%)]  Loss: 3.904 (3.99)  Time: 1.053s,  972.56/s  (1.009s, 1014.55/s)  LR: 9.789e-04  Data: 0.012 (0.013)
Train: 28 [ 850/1251 ( 68%)]  Loss: 3.837 (3.98)  Time: 0.996s, 1027.63/s  (1.009s, 1015.07/s)  LR: 9.789e-04  Data: 0.012 (0.013)
Train: 28 [ 900/1251 ( 72%)]  Loss: 4.101 (3.98)  Time: 1.045s,  979.53/s  (1.009s, 1014.84/s)  LR: 9.789e-04  Data: 0.012 (0.013)
Train: 28 [ 950/1251 ( 76%)]  Loss: 4.260 (4.00)  Time: 0.998s, 1026.00/s  (1.009s, 1015.18/s)  LR: 9.789e-04  Data: 0.012 (0.013)
Train: 28 [1000/1251 ( 80%)]  Loss: 3.991 (4.00)  Time: 0.997s, 1027.07/s  (1.008s, 1015.49/s)  LR: 9.789e-04  Data: 0.011 (0.013)
Train: 28 [1050/1251 ( 84%)]  Loss: 3.753 (3.99)  Time: 1.032s,  991.91/s  (1.008s, 1015.64/s)  LR: 9.789e-04  Data: 0.011 (0.013)
Train: 28 [1100/1251 ( 88%)]  Loss: 3.891 (3.98)  Time: 0.997s, 1027.36/s  (1.008s, 1015.94/s)  LR: 9.789e-04  Data: 0.011 (0.013)
Train: 28 [1150/1251 ( 92%)]  Loss: 4.000 (3.98)  Time: 1.067s,  959.43/s  (1.008s, 1015.42/s)  LR: 9.789e-04  Data: 0.011 (0.013)
Train: 28 [1200/1251 ( 96%)]  Loss: 4.138 (3.99)  Time: 1.010s, 1014.15/s  (1.009s, 1014.90/s)  LR: 9.789e-04  Data: 0.011 (0.013)
Train: 28 [1250/1251 (100%)]  Loss: 4.096 (3.99)  Time: 1.024s,  999.60/s  (1.009s, 1014.95/s)  LR: 9.789e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.595 (1.595)  Loss:  1.1900 (1.1900)  Acc@1: 81.9336 (81.9336)  Acc@5: 94.2383 (94.2383)
Test: [  48/48]  Time: 0.245 (0.578)  Loss:  1.1781 (1.7073)  Acc@1: 78.4198 (65.4920)  Acc@5: 92.6887 (87.1480)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-27.pth.tar', 65.53399999267579)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-28.pth.tar', 65.49200006347657)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-25.pth.tar', 65.20399994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-24.pth.tar', 64.23999995605469)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-23.pth.tar', 64.00999995361327)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-26.pth.tar', 63.24600002929687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-22.pth.tar', 63.00400003662109)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-21.pth.tar', 62.691999965820315)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-19.pth.tar', 62.33399989257813)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-20.pth.tar', 61.66999995849609)

Train: 29 [   0/1251 (  0%)]  Loss: 4.175 (4.17)  Time: 2.614s,  391.78/s  (2.614s,  391.78/s)  LR: 9.773e-04  Data: 1.649 (1.649)
Train: 29 [  50/1251 (  4%)]  Loss: 4.279 (4.23)  Time: 1.034s,  990.23/s  (1.068s,  958.99/s)  LR: 9.773e-04  Data: 0.012 (0.045)
Train: 29 [ 100/1251 (  8%)]  Loss: 4.331 (4.26)  Time: 0.999s, 1024.67/s  (1.044s,  980.61/s)  LR: 9.773e-04  Data: 0.012 (0.028)
Train: 29 [ 150/1251 ( 12%)]  Loss: 4.094 (4.22)  Time: 1.034s,  990.03/s  (1.030s,  994.24/s)  LR: 9.773e-04  Data: 0.011 (0.023)
Train: 29 [ 200/1251 ( 16%)]  Loss: 4.459 (4.27)  Time: 0.996s, 1027.68/s  (1.022s, 1001.48/s)  LR: 9.773e-04  Data: 0.011 (0.020)
Train: 29 [ 250/1251 ( 20%)]  Loss: 3.772 (4.18)  Time: 1.005s, 1018.80/s  (1.018s, 1006.09/s)  LR: 9.773e-04  Data: 0.012 (0.018)
Train: 29 [ 300/1251 ( 24%)]  Loss: 4.055 (4.17)  Time: 0.997s, 1027.41/s  (1.018s, 1005.74/s)  LR: 9.773e-04  Data: 0.011 (0.017)
Train: 29 [ 350/1251 ( 28%)]  Loss: 3.982 (4.14)  Time: 0.996s, 1027.99/s  (1.015s, 1008.53/s)  LR: 9.773e-04  Data: 0.011 (0.016)
Train: 29 [ 400/1251 ( 32%)]  Loss: 4.219 (4.15)  Time: 0.997s, 1026.83/s  (1.013s, 1010.42/s)  LR: 9.773e-04  Data: 0.012 (0.015)
Train: 29 [ 450/1251 ( 36%)]  Loss: 4.380 (4.17)  Time: 0.995s, 1028.67/s  (1.012s, 1012.14/s)  LR: 9.773e-04  Data: 0.011 (0.015)
Train: 29 [ 500/1251 ( 40%)]  Loss: 4.189 (4.18)  Time: 1.005s, 1018.67/s  (1.013s, 1010.71/s)  LR: 9.773e-04  Data: 0.012 (0.015)
Train: 29 [ 550/1251 ( 44%)]  Loss: 4.072 (4.17)  Time: 1.045s,  980.11/s  (1.014s, 1010.20/s)  LR: 9.773e-04  Data: 0.011 (0.014)
Train: 29 [ 600/1251 ( 48%)]  Loss: 4.182 (4.17)  Time: 0.997s, 1026.88/s  (1.014s, 1010.25/s)  LR: 9.773e-04  Data: 0.010 (0.014)
Train: 29 [ 650/1251 ( 52%)]  Loss: 4.110 (4.16)  Time: 1.063s,  962.97/s  (1.013s, 1011.14/s)  LR: 9.773e-04  Data: 0.010 (0.014)
Train: 29 [ 700/1251 ( 56%)]  Loss: 4.041 (4.16)  Time: 1.054s,  971.29/s  (1.012s, 1011.41/s)  LR: 9.773e-04  Data: 0.011 (0.014)
Train: 29 [ 750/1251 ( 60%)]  Loss: 3.488 (4.11)  Time: 0.995s, 1029.40/s  (1.012s, 1012.04/s)  LR: 9.773e-04  Data: 0.011 (0.013)
Train: 29 [ 800/1251 ( 64%)]  Loss: 3.803 (4.10)  Time: 0.996s, 1028.36/s  (1.012s, 1012.23/s)  LR: 9.773e-04  Data: 0.010 (0.013)
Train: 29 [ 850/1251 ( 68%)]  Loss: 4.132 (4.10)  Time: 0.998s, 1026.32/s  (1.012s, 1012.08/s)  LR: 9.773e-04  Data: 0.011 (0.013)
Train: 29 [ 900/1251 ( 72%)]  Loss: 3.778 (4.08)  Time: 1.003s, 1021.38/s  (1.012s, 1011.89/s)  LR: 9.773e-04  Data: 0.011 (0.013)
Train: 29 [ 950/1251 ( 76%)]  Loss: 3.899 (4.07)  Time: 1.003s, 1020.72/s  (1.011s, 1012.54/s)  LR: 9.773e-04  Data: 0.010 (0.013)
Train: 29 [1000/1251 ( 80%)]  Loss: 4.040 (4.07)  Time: 1.002s, 1022.26/s  (1.011s, 1013.12/s)  LR: 9.773e-04  Data: 0.010 (0.013)
Train: 29 [1050/1251 ( 84%)]  Loss: 4.360 (4.08)  Time: 1.066s,  960.92/s  (1.011s, 1012.98/s)  LR: 9.773e-04  Data: 0.011 (0.013)
Train: 29 [1100/1251 ( 88%)]  Loss: 3.816 (4.07)  Time: 0.994s, 1029.93/s  (1.010s, 1013.62/s)  LR: 9.773e-04  Data: 0.013 (0.013)
Train: 29 [1150/1251 ( 92%)]  Loss: 4.131 (4.07)  Time: 1.004s, 1020.17/s  (1.010s, 1013.89/s)  LR: 9.773e-04  Data: 0.011 (0.013)
Train: 29 [1200/1251 ( 96%)]  Loss: 3.978 (4.07)  Time: 0.991s, 1033.39/s  (1.009s, 1014.45/s)  LR: 9.773e-04  Data: 0.011 (0.013)
Train: 29 [1250/1251 (100%)]  Loss: 4.325 (4.08)  Time: 0.983s, 1041.39/s  (1.009s, 1014.81/s)  LR: 9.773e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.658 (1.658)  Loss:  0.9576 (0.9576)  Acc@1: 83.4961 (83.4961)  Acc@5: 95.6055 (95.6055)
Test: [  48/48]  Time: 0.245 (0.573)  Loss:  1.1156 (1.7153)  Acc@1: 79.8349 (65.9100)  Acc@5: 95.0472 (87.6700)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-29.pth.tar', 65.91000003173828)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-27.pth.tar', 65.53399999267579)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-28.pth.tar', 65.49200006347657)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-25.pth.tar', 65.20399994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-24.pth.tar', 64.23999995605469)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-23.pth.tar', 64.00999995361327)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-26.pth.tar', 63.24600002929687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-22.pth.tar', 63.00400003662109)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-21.pth.tar', 62.691999965820315)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-19.pth.tar', 62.33399989257813)

Train: 30 [   0/1251 (  0%)]  Loss: 4.295 (4.29)  Time: 4.352s,  235.31/s  (4.352s,  235.31/s)  LR: 9.758e-04  Data: 3.090 (3.090)
Train: 30 [  50/1251 (  4%)]  Loss: 4.180 (4.24)  Time: 0.997s, 1027.00/s  (1.089s,  939.89/s)  LR: 9.758e-04  Data: 0.012 (0.072)
Train: 30 [ 100/1251 (  8%)]  Loss: 3.838 (4.10)  Time: 0.996s, 1028.12/s  (1.052s,  973.00/s)  LR: 9.758e-04  Data: 0.011 (0.042)
Train: 30 [ 150/1251 ( 12%)]  Loss: 3.798 (4.03)  Time: 0.998s, 1025.54/s  (1.038s,  986.46/s)  LR: 9.758e-04  Data: 0.012 (0.032)
Train: 30 [ 200/1251 ( 16%)]  Loss: 4.073 (4.04)  Time: 0.998s, 1025.82/s  (1.029s,  995.48/s)  LR: 9.758e-04  Data: 0.011 (0.027)
Train: 30 [ 250/1251 ( 20%)]  Loss: 4.316 (4.08)  Time: 0.995s, 1029.30/s  (1.022s, 1001.49/s)  LR: 9.758e-04  Data: 0.011 (0.023)
Train: 30 [ 300/1251 ( 24%)]  Loss: 4.058 (4.08)  Time: 0.997s, 1026.72/s  (1.018s, 1005.67/s)  LR: 9.758e-04  Data: 0.011 (0.021)
Train: 30 [ 350/1251 ( 28%)]  Loss: 3.778 (4.04)  Time: 0.995s, 1028.93/s  (1.016s, 1008.27/s)  LR: 9.758e-04  Data: 0.011 (0.020)
Train: 30 [ 400/1251 ( 32%)]  Loss: 4.316 (4.07)  Time: 1.000s, 1024.14/s  (1.015s, 1008.88/s)  LR: 9.758e-04  Data: 0.010 (0.019)
Train: 30 [ 450/1251 ( 36%)]  Loss: 4.079 (4.07)  Time: 1.001s, 1023.25/s  (1.014s, 1010.27/s)  LR: 9.758e-04  Data: 0.012 (0.018)
Train: 30 [ 500/1251 ( 40%)]  Loss: 4.263 (4.09)  Time: 0.998s, 1025.55/s  (1.013s, 1011.00/s)  LR: 9.758e-04  Data: 0.011 (0.017)
Train: 30 [ 550/1251 ( 44%)]  Loss: 3.912 (4.08)  Time: 0.995s, 1028.72/s  (1.012s, 1012.28/s)  LR: 9.758e-04  Data: 0.011 (0.017)
Train: 30 [ 600/1251 ( 48%)]  Loss: 4.000 (4.07)  Time: 0.996s, 1027.77/s  (1.011s, 1013.16/s)  LR: 9.758e-04  Data: 0.011 (0.016)
Train: 30 [ 650/1251 ( 52%)]  Loss: 4.239 (4.08)  Time: 0.996s, 1027.75/s  (1.012s, 1012.34/s)  LR: 9.758e-04  Data: 0.011 (0.016)
Train: 30 [ 700/1251 ( 56%)]  Loss: 3.977 (4.07)  Time: 1.004s, 1020.38/s  (1.011s, 1013.33/s)  LR: 9.758e-04  Data: 0.012 (0.016)
Train: 30 [ 750/1251 ( 60%)]  Loss: 4.187 (4.08)  Time: 1.032s,  992.66/s  (1.010s, 1014.12/s)  LR: 9.758e-04  Data: 0.011 (0.015)
Train: 30 [ 800/1251 ( 64%)]  Loss: 4.357 (4.10)  Time: 1.001s, 1023.36/s  (1.010s, 1013.59/s)  LR: 9.758e-04  Data: 0.011 (0.015)
Train: 30 [ 850/1251 ( 68%)]  Loss: 4.124 (4.10)  Time: 1.042s,  982.85/s  (1.010s, 1013.74/s)  LR: 9.758e-04  Data: 0.010 (0.015)
Train: 30 [ 900/1251 ( 72%)]  Loss: 4.303 (4.11)  Time: 0.994s, 1030.54/s  (1.010s, 1013.37/s)  LR: 9.758e-04  Data: 0.011 (0.015)
Train: 30 [ 950/1251 ( 76%)]  Loss: 4.478 (4.13)  Time: 0.995s, 1029.63/s  (1.010s, 1013.64/s)  LR: 9.758e-04  Data: 0.011 (0.014)
Train: 30 [1000/1251 ( 80%)]  Loss: 4.093 (4.13)  Time: 0.997s, 1027.00/s  (1.010s, 1014.08/s)  LR: 9.758e-04  Data: 0.011 (0.014)
Train: 30 [1050/1251 ( 84%)]  Loss: 3.723 (4.11)  Time: 0.996s, 1028.33/s  (1.009s, 1014.61/s)  LR: 9.758e-04  Data: 0.012 (0.014)
Train: 30 [1100/1251 ( 88%)]  Loss: 4.544 (4.13)  Time: 0.993s, 1030.71/s  (1.009s, 1014.54/s)  LR: 9.758e-04  Data: 0.011 (0.014)
Train: 30 [1150/1251 ( 92%)]  Loss: 4.141 (4.13)  Time: 0.998s, 1025.78/s  (1.009s, 1014.47/s)  LR: 9.758e-04  Data: 0.012 (0.014)
Train: 30 [1200/1251 ( 96%)]  Loss: 3.676 (4.11)  Time: 0.995s, 1028.77/s  (1.010s, 1014.01/s)  LR: 9.758e-04  Data: 0.011 (0.014)
Train: 30 [1250/1251 (100%)]  Loss: 3.823 (4.10)  Time: 0.979s, 1045.49/s  (1.009s, 1014.49/s)  LR: 9.758e-04  Data: 0.000 (0.014)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.586 (1.586)  Loss:  1.0221 (1.0221)  Acc@1: 82.3242 (82.3242)  Acc@5: 95.3125 (95.3125)
Test: [  48/48]  Time: 0.245 (0.575)  Loss:  1.0356 (1.6542)  Acc@1: 80.1887 (65.9720)  Acc@5: 93.5141 (87.4800)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-30.pth.tar', 65.97199992675782)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-29.pth.tar', 65.91000003173828)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-27.pth.tar', 65.53399999267579)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-28.pth.tar', 65.49200006347657)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-25.pth.tar', 65.20399994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-24.pth.tar', 64.23999995605469)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-23.pth.tar', 64.00999995361327)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-26.pth.tar', 63.24600002929687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-22.pth.tar', 63.00400003662109)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-21.pth.tar', 62.691999965820315)

Train: 31 [   0/1251 (  0%)]  Loss: 4.344 (4.34)  Time: 2.503s,  409.05/s  (2.503s,  409.05/s)  LR: 9.741e-04  Data: 1.497 (1.497)
Train: 31 [  50/1251 (  4%)]  Loss: 4.022 (4.18)  Time: 0.996s, 1028.45/s  (1.040s,  984.96/s)  LR: 9.741e-04  Data: 0.011 (0.043)
Train: 31 [ 100/1251 (  8%)]  Loss: 4.176 (4.18)  Time: 0.997s, 1026.94/s  (1.019s, 1005.17/s)  LR: 9.741e-04  Data: 0.011 (0.027)
Train: 31 [ 150/1251 ( 12%)]  Loss: 4.025 (4.14)  Time: 1.004s, 1020.15/s  (1.018s, 1006.26/s)  LR: 9.741e-04  Data: 0.010 (0.022)
Train: 31 [ 200/1251 ( 16%)]  Loss: 4.120 (4.14)  Time: 1.004s, 1019.48/s  (1.013s, 1010.69/s)  LR: 9.741e-04  Data: 0.011 (0.019)
Train: 31 [ 250/1251 ( 20%)]  Loss: 4.261 (4.16)  Time: 0.996s, 1027.61/s  (1.010s, 1014.17/s)  LR: 9.741e-04  Data: 0.010 (0.018)
Train: 31 [ 300/1251 ( 24%)]  Loss: 3.929 (4.13)  Time: 1.047s,  978.31/s  (1.010s, 1013.76/s)  LR: 9.741e-04  Data: 0.011 (0.017)
Train: 31 [ 350/1251 ( 28%)]  Loss: 3.596 (4.06)  Time: 0.995s, 1029.60/s  (1.009s, 1015.32/s)  LR: 9.741e-04  Data: 0.011 (0.016)
Train: 31 [ 400/1251 ( 32%)]  Loss: 4.161 (4.07)  Time: 1.021s, 1003.29/s  (1.008s, 1015.57/s)  LR: 9.741e-04  Data: 0.012 (0.015)
Train: 31 [ 450/1251 ( 36%)]  Loss: 4.160 (4.08)  Time: 1.001s, 1022.65/s  (1.007s, 1016.54/s)  LR: 9.741e-04  Data: 0.011 (0.015)
Train: 31 [ 500/1251 ( 40%)]  Loss: 4.026 (4.07)  Time: 0.996s, 1028.49/s  (1.006s, 1017.43/s)  LR: 9.741e-04  Data: 0.014 (0.014)
Train: 31 [ 550/1251 ( 44%)]  Loss: 3.926 (4.06)  Time: 1.004s, 1019.81/s  (1.009s, 1014.95/s)  LR: 9.741e-04  Data: 0.011 (0.014)
Train: 31 [ 600/1251 ( 48%)]  Loss: 3.560 (4.02)  Time: 0.993s, 1031.50/s  (1.008s, 1015.62/s)  LR: 9.741e-04  Data: 0.011 (0.014)
Train: 31 [ 650/1251 ( 52%)]  Loss: 4.027 (4.02)  Time: 0.997s, 1027.28/s  (1.007s, 1016.52/s)  LR: 9.741e-04  Data: 0.011 (0.014)
Train: 31 [ 700/1251 ( 56%)]  Loss: 3.834 (4.01)  Time: 0.991s, 1033.62/s  (1.007s, 1016.89/s)  LR: 9.741e-04  Data: 0.010 (0.013)
Train: 31 [ 750/1251 ( 60%)]  Loss: 3.995 (4.01)  Time: 0.999s, 1025.27/s  (1.007s, 1016.74/s)  LR: 9.741e-04  Data: 0.011 (0.013)
Train: 31 [ 800/1251 ( 64%)]  Loss: 4.003 (4.01)  Time: 0.996s, 1028.15/s  (1.007s, 1017.28/s)  LR: 9.741e-04  Data: 0.012 (0.013)
Train: 31 [ 850/1251 ( 68%)]  Loss: 3.590 (3.99)  Time: 1.024s, 1000.14/s  (1.007s, 1017.34/s)  LR: 9.741e-04  Data: 0.011 (0.013)
Train: 31 [ 900/1251 ( 72%)]  Loss: 4.008 (3.99)  Time: 1.001s, 1022.73/s  (1.006s, 1017.79/s)  LR: 9.741e-04  Data: 0.014 (0.013)
Train: 31 [ 950/1251 ( 76%)]  Loss: 3.971 (3.99)  Time: 0.996s, 1027.78/s  (1.006s, 1017.60/s)  LR: 9.741e-04  Data: 0.012 (0.013)
Train: 31 [1000/1251 ( 80%)]  Loss: 4.253 (4.00)  Time: 0.997s, 1026.76/s  (1.006s, 1018.01/s)  LR: 9.741e-04  Data: 0.011 (0.013)
Train: 31 [1050/1251 ( 84%)]  Loss: 3.830 (3.99)  Time: 1.000s, 1023.65/s  (1.006s, 1017.84/s)  LR: 9.741e-04  Data: 0.011 (0.013)
Train: 31 [1100/1251 ( 88%)]  Loss: 4.035 (3.99)  Time: 1.033s,  991.71/s  (1.006s, 1018.00/s)  LR: 9.741e-04  Data: 0.012 (0.013)
Train: 31 [1150/1251 ( 92%)]  Loss: 3.952 (3.99)  Time: 0.998s, 1025.84/s  (1.006s, 1017.39/s)  LR: 9.741e-04  Data: 0.011 (0.013)
Train: 31 [1200/1251 ( 96%)]  Loss: 4.351 (4.01)  Time: 0.996s, 1028.48/s  (1.007s, 1017.14/s)  LR: 9.741e-04  Data: 0.011 (0.013)
Train: 31 [1250/1251 (100%)]  Loss: 4.381 (4.02)  Time: 0.984s, 1041.00/s  (1.006s, 1017.54/s)  LR: 9.741e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.707 (1.707)  Loss:  1.0357 (1.0357)  Acc@1: 84.2773 (84.2773)  Acc@5: 95.8984 (95.8984)
Test: [  48/48]  Time: 0.245 (0.565)  Loss:  1.1122 (1.6848)  Acc@1: 81.1321 (66.1660)  Acc@5: 93.1604 (87.6020)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-31.pth.tar', 66.166000078125)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-30.pth.tar', 65.97199992675782)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-29.pth.tar', 65.91000003173828)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-27.pth.tar', 65.53399999267579)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-28.pth.tar', 65.49200006347657)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-25.pth.tar', 65.20399994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-24.pth.tar', 64.23999995605469)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-23.pth.tar', 64.00999995361327)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-26.pth.tar', 63.24600002929687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-22.pth.tar', 63.00400003662109)

Train: 32 [   0/1251 (  0%)]  Loss: 4.274 (4.27)  Time: 2.478s,  413.26/s  (2.478s,  413.26/s)  LR: 9.725e-04  Data: 1.510 (1.510)
Train: 32 [  50/1251 (  4%)]  Loss: 3.982 (4.13)  Time: 1.020s, 1003.93/s  (1.042s,  982.71/s)  LR: 9.725e-04  Data: 0.010 (0.041)
Train: 32 [ 100/1251 (  8%)]  Loss: 3.802 (4.02)  Time: 1.020s, 1003.63/s  (1.027s,  996.73/s)  LR: 9.725e-04  Data: 0.012 (0.026)
Train: 32 [ 150/1251 ( 12%)]  Loss: 4.139 (4.05)  Time: 0.999s, 1024.73/s  (1.018s, 1006.03/s)  LR: 9.725e-04  Data: 0.011 (0.021)
Train: 32 [ 200/1251 ( 16%)]  Loss: 3.710 (3.98)  Time: 0.996s, 1027.88/s  (1.014s, 1010.11/s)  LR: 9.725e-04  Data: 0.012 (0.019)
Train: 32 [ 250/1251 ( 20%)]  Loss: 3.989 (3.98)  Time: 0.994s, 1030.02/s  (1.010s, 1013.71/s)  LR: 9.725e-04  Data: 0.010 (0.017)
Train: 32 [ 300/1251 ( 24%)]  Loss: 3.777 (3.95)  Time: 0.994s, 1030.05/s  (1.008s, 1015.39/s)  LR: 9.725e-04  Data: 0.011 (0.016)
Train: 32 [ 350/1251 ( 28%)]  Loss: 4.281 (3.99)  Time: 0.994s, 1030.62/s  (1.009s, 1014.68/s)  LR: 9.725e-04  Data: 0.010 (0.016)
Train: 32 [ 400/1251 ( 32%)]  Loss: 3.932 (3.99)  Time: 0.998s, 1026.37/s  (1.009s, 1014.71/s)  LR: 9.725e-04  Data: 0.010 (0.015)
Train: 32 [ 450/1251 ( 36%)]  Loss: 3.320 (3.92)  Time: 0.994s, 1030.60/s  (1.010s, 1014.12/s)  LR: 9.725e-04  Data: 0.011 (0.015)
Train: 32 [ 500/1251 ( 40%)]  Loss: 4.386 (3.96)  Time: 0.998s, 1026.42/s  (1.009s, 1015.00/s)  LR: 9.725e-04  Data: 0.011 (0.014)
Train: 32 [ 550/1251 ( 44%)]  Loss: 4.178 (3.98)  Time: 0.996s, 1027.89/s  (1.008s, 1015.94/s)  LR: 9.725e-04  Data: 0.011 (0.014)
Train: 32 [ 600/1251 ( 48%)]  Loss: 3.876 (3.97)  Time: 0.994s, 1029.99/s  (1.009s, 1015.11/s)  LR: 9.725e-04  Data: 0.011 (0.014)
Train: 32 [ 650/1251 ( 52%)]  Loss: 3.975 (3.97)  Time: 1.047s,  977.59/s  (1.009s, 1014.87/s)  LR: 9.725e-04  Data: 0.011 (0.014)
Train: 32 [ 700/1251 ( 56%)]  Loss: 3.944 (3.97)  Time: 0.996s, 1027.69/s  (1.009s, 1014.97/s)  LR: 9.725e-04  Data: 0.011 (0.013)
Train: 32 [ 750/1251 ( 60%)]  Loss: 3.810 (3.96)  Time: 0.996s, 1028.56/s  (1.008s, 1015.46/s)  LR: 9.725e-04  Data: 0.012 (0.013)
Train: 32 [ 800/1251 ( 64%)]  Loss: 4.065 (3.97)  Time: 0.996s, 1028.49/s  (1.009s, 1015.11/s)  LR: 9.725e-04  Data: 0.011 (0.013)
Train: 32 [ 850/1251 ( 68%)]  Loss: 4.032 (3.97)  Time: 0.994s, 1029.79/s  (1.009s, 1015.34/s)  LR: 9.725e-04  Data: 0.010 (0.013)
Train: 32 [ 900/1251 ( 72%)]  Loss: 3.979 (3.97)  Time: 0.997s, 1027.28/s  (1.010s, 1014.28/s)  LR: 9.725e-04  Data: 0.012 (0.013)
Train: 32 [ 950/1251 ( 76%)]  Loss: 4.017 (3.97)  Time: 1.000s, 1023.86/s  (1.010s, 1014.00/s)  LR: 9.725e-04  Data: 0.011 (0.013)
Train: 32 [1000/1251 ( 80%)]  Loss: 4.026 (3.98)  Time: 0.996s, 1028.40/s  (1.009s, 1014.73/s)  LR: 9.725e-04  Data: 0.011 (0.013)
Train: 32 [1050/1251 ( 84%)]  Loss: 4.044 (3.98)  Time: 1.057s,  968.91/s  (1.009s, 1014.69/s)  LR: 9.725e-04  Data: 0.011 (0.013)
Train: 32 [1100/1251 ( 88%)]  Loss: 3.651 (3.96)  Time: 1.034s,  990.80/s  (1.009s, 1014.55/s)  LR: 9.725e-04  Data: 0.010 (0.013)
Train: 32 [1150/1251 ( 92%)]  Loss: 4.014 (3.97)  Time: 1.035s,  989.51/s  (1.011s, 1013.23/s)  LR: 9.725e-04  Data: 0.012 (0.013)
Train: 32 [1200/1251 ( 96%)]  Loss: 4.179 (3.98)  Time: 0.996s, 1028.45/s  (1.011s, 1013.30/s)  LR: 9.725e-04  Data: 0.011 (0.013)
Train: 32 [1250/1251 (100%)]  Loss: 4.183 (3.98)  Time: 0.987s, 1037.74/s  (1.010s, 1013.75/s)  LR: 9.725e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.655 (1.655)  Loss:  1.0949 (1.0949)  Acc@1: 83.1055 (83.1055)  Acc@5: 95.8008 (95.8008)
Test: [  48/48]  Time: 0.245 (0.567)  Loss:  1.1396 (1.8274)  Acc@1: 82.0755 (66.0640)  Acc@5: 94.9292 (87.6540)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-31.pth.tar', 66.166000078125)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-32.pth.tar', 66.06399997070312)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-30.pth.tar', 65.97199992675782)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-29.pth.tar', 65.91000003173828)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-27.pth.tar', 65.53399999267579)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-28.pth.tar', 65.49200006347657)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-25.pth.tar', 65.20399994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-24.pth.tar', 64.23999995605469)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-23.pth.tar', 64.00999995361327)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-26.pth.tar', 63.24600002929687)

Train: 33 [   0/1251 (  0%)]  Loss: 4.083 (4.08)  Time: 2.423s,  422.65/s  (2.423s,  422.65/s)  LR: 9.707e-04  Data: 1.463 (1.463)
Train: 33 [  50/1251 (  4%)]  Loss: 3.454 (3.77)  Time: 1.035s,  989.54/s  (1.033s,  991.16/s)  LR: 9.707e-04  Data: 0.011 (0.040)
Train: 33 [ 100/1251 (  8%)]  Loss: 4.138 (3.89)  Time: 0.997s, 1027.56/s  (1.024s, 1000.49/s)  LR: 9.707e-04  Data: 0.012 (0.026)
Train: 33 [ 150/1251 ( 12%)]  Loss: 3.718 (3.85)  Time: 0.993s, 1031.31/s  (1.016s, 1008.12/s)  LR: 9.707e-04  Data: 0.010 (0.021)
Train: 33 [ 200/1251 ( 16%)]  Loss: 3.742 (3.83)  Time: 0.994s, 1030.18/s  (1.011s, 1012.48/s)  LR: 9.707e-04  Data: 0.011 (0.019)
Train: 33 [ 250/1251 ( 20%)]  Loss: 4.055 (3.86)  Time: 0.994s, 1030.59/s  (1.009s, 1014.43/s)  LR: 9.707e-04  Data: 0.011 (0.017)
Train: 33 [ 300/1251 ( 24%)]  Loss: 4.049 (3.89)  Time: 1.008s, 1016.04/s  (1.008s, 1016.18/s)  LR: 9.707e-04  Data: 0.011 (0.016)
Train: 33 [ 350/1251 ( 28%)]  Loss: 3.934 (3.90)  Time: 1.000s, 1023.84/s  (1.007s, 1016.90/s)  LR: 9.707e-04  Data: 0.011 (0.015)
Train: 33 [ 400/1251 ( 32%)]  Loss: 4.029 (3.91)  Time: 0.994s, 1030.66/s  (1.006s, 1018.14/s)  LR: 9.707e-04  Data: 0.010 (0.015)
Train: 33 [ 450/1251 ( 36%)]  Loss: 4.035 (3.92)  Time: 1.038s,  986.54/s  (1.006s, 1018.14/s)  LR: 9.707e-04  Data: 0.012 (0.015)
Train: 33 [ 500/1251 ( 40%)]  Loss: 3.903 (3.92)  Time: 0.996s, 1028.22/s  (1.005s, 1018.79/s)  LR: 9.707e-04  Data: 0.010 (0.014)
Train: 33 [ 550/1251 ( 44%)]  Loss: 4.029 (3.93)  Time: 0.997s, 1027.49/s  (1.006s, 1018.17/s)  LR: 9.707e-04  Data: 0.011 (0.014)
Train: 33 [ 600/1251 ( 48%)]  Loss: 3.938 (3.93)  Time: 0.996s, 1027.79/s  (1.005s, 1018.96/s)  LR: 9.707e-04  Data: 0.012 (0.014)
Train: 33 [ 650/1251 ( 52%)]  Loss: 3.865 (3.93)  Time: 1.028s,  995.86/s  (1.005s, 1019.37/s)  LR: 9.707e-04  Data: 0.011 (0.014)
Train: 33 [ 700/1251 ( 56%)]  Loss: 3.929 (3.93)  Time: 1.046s,  978.55/s  (1.005s, 1019.39/s)  LR: 9.707e-04  Data: 0.010 (0.013)
Train: 33 [ 750/1251 ( 60%)]  Loss: 4.434 (3.96)  Time: 1.025s,  998.90/s  (1.005s, 1018.80/s)  LR: 9.707e-04  Data: 0.011 (0.013)
Train: 33 [ 800/1251 ( 64%)]  Loss: 4.150 (3.97)  Time: 0.997s, 1027.58/s  (1.005s, 1018.81/s)  LR: 9.707e-04  Data: 0.012 (0.013)
Train: 33 [ 850/1251 ( 68%)]  Loss: 4.025 (3.97)  Time: 1.060s,  965.76/s  (1.005s, 1019.05/s)  LR: 9.707e-04  Data: 0.012 (0.013)
Train: 33 [ 900/1251 ( 72%)]  Loss: 3.778 (3.96)  Time: 1.002s, 1022.25/s  (1.006s, 1017.43/s)  LR: 9.707e-04  Data: 0.011 (0.013)
Train: 33 [ 950/1251 ( 76%)]  Loss: 4.008 (3.96)  Time: 1.055s,  970.26/s  (1.007s, 1016.84/s)  LR: 9.707e-04  Data: 0.011 (0.013)
Train: 33 [1000/1251 ( 80%)]  Loss: 4.070 (3.97)  Time: 0.993s, 1031.29/s  (1.007s, 1016.56/s)  LR: 9.707e-04  Data: 0.011 (0.013)
Train: 33 [1050/1251 ( 84%)]  Loss: 3.909 (3.97)  Time: 0.997s, 1026.60/s  (1.007s, 1016.77/s)  LR: 9.707e-04  Data: 0.011 (0.013)
Train: 33 [1100/1251 ( 88%)]  Loss: 4.165 (3.98)  Time: 0.998s, 1025.96/s  (1.007s, 1016.83/s)  LR: 9.707e-04  Data: 0.010 (0.013)
Train: 33 [1150/1251 ( 92%)]  Loss: 3.712 (3.96)  Time: 0.999s, 1025.47/s  (1.008s, 1015.86/s)  LR: 9.707e-04  Data: 0.012 (0.013)
Train: 33 [1200/1251 ( 96%)]  Loss: 4.082 (3.97)  Time: 0.998s, 1026.34/s  (1.008s, 1015.52/s)  LR: 9.707e-04  Data: 0.012 (0.012)
Train: 33 [1250/1251 (100%)]  Loss: 3.981 (3.97)  Time: 0.984s, 1040.65/s  (1.008s, 1015.92/s)  LR: 9.707e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.694 (1.694)  Loss:  0.9535 (0.9535)  Acc@1: 86.2305 (86.2305)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.245 (0.563)  Loss:  1.0821 (1.6838)  Acc@1: 82.3113 (67.2460)  Acc@5: 93.8679 (88.2380)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-33.pth.tar', 67.24600007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-31.pth.tar', 66.166000078125)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-32.pth.tar', 66.06399997070312)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-30.pth.tar', 65.97199992675782)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-29.pth.tar', 65.91000003173828)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-27.pth.tar', 65.53399999267579)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-28.pth.tar', 65.49200006347657)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-25.pth.tar', 65.20399994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-24.pth.tar', 64.23999995605469)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-23.pth.tar', 64.00999995361327)

Train: 34 [   0/1251 (  0%)]  Loss: 4.072 (4.07)  Time: 2.496s,  410.24/s  (2.496s,  410.24/s)  LR: 9.690e-04  Data: 1.515 (1.515)
Train: 34 [  50/1251 (  4%)]  Loss: 4.225 (4.15)  Time: 0.995s, 1028.76/s  (1.043s,  982.04/s)  LR: 9.690e-04  Data: 0.011 (0.041)
Train: 34 [ 100/1251 (  8%)]  Loss: 4.111 (4.14)  Time: 0.997s, 1026.74/s  (1.022s, 1002.14/s)  LR: 9.690e-04  Data: 0.011 (0.026)
Train: 34 [ 150/1251 ( 12%)]  Loss: 3.870 (4.07)  Time: 0.999s, 1024.78/s  (1.014s, 1009.70/s)  LR: 9.690e-04  Data: 0.011 (0.021)
Train: 34 [ 200/1251 ( 16%)]  Loss: 4.255 (4.11)  Time: 0.996s, 1028.28/s  (1.012s, 1011.67/s)  LR: 9.690e-04  Data: 0.011 (0.019)
Train: 34 [ 250/1251 ( 20%)]  Loss: 4.377 (4.15)  Time: 0.995s, 1028.79/s  (1.010s, 1014.23/s)  LR: 9.690e-04  Data: 0.012 (0.017)
Train: 34 [ 300/1251 ( 24%)]  Loss: 4.010 (4.13)  Time: 0.996s, 1028.43/s  (1.009s, 1015.35/s)  LR: 9.690e-04  Data: 0.012 (0.016)
Train: 34 [ 350/1251 ( 28%)]  Loss: 3.846 (4.10)  Time: 1.000s, 1024.10/s  (1.007s, 1016.92/s)  LR: 9.690e-04  Data: 0.011 (0.015)
Train: 34 [ 400/1251 ( 32%)]  Loss: 4.583 (4.15)  Time: 0.994s, 1030.28/s  (1.008s, 1015.39/s)  LR: 9.690e-04  Data: 0.011 (0.015)
Train: 34 [ 450/1251 ( 36%)]  Loss: 4.237 (4.16)  Time: 0.993s, 1031.03/s  (1.008s, 1015.65/s)  LR: 9.690e-04  Data: 0.012 (0.014)
Train: 34 [ 500/1251 ( 40%)]  Loss: 4.176 (4.16)  Time: 1.033s,  991.71/s  (1.008s, 1015.46/s)  LR: 9.690e-04  Data: 0.011 (0.014)
Train: 34 [ 550/1251 ( 44%)]  Loss: 3.999 (4.15)  Time: 1.061s,  965.08/s  (1.010s, 1014.32/s)  LR: 9.690e-04  Data: 0.011 (0.014)
Train: 34 [ 600/1251 ( 48%)]  Loss: 3.918 (4.13)  Time: 0.995s, 1028.70/s  (1.009s, 1015.28/s)  LR: 9.690e-04  Data: 0.011 (0.014)
Train: 34 [ 650/1251 ( 52%)]  Loss: 4.266 (4.14)  Time: 0.995s, 1029.23/s  (1.008s, 1015.79/s)  LR: 9.690e-04  Data: 0.011 (0.013)
Train: 34 [ 700/1251 ( 56%)]  Loss: 3.872 (4.12)  Time: 0.995s, 1029.17/s  (1.007s, 1016.61/s)  LR: 9.690e-04  Data: 0.011 (0.013)
Train: 34 [ 750/1251 ( 60%)]  Loss: 3.732 (4.10)  Time: 0.995s, 1029.57/s  (1.007s, 1017.35/s)  LR: 9.690e-04  Data: 0.011 (0.013)
Train: 34 [ 800/1251 ( 64%)]  Loss: 3.794 (4.08)  Time: 0.996s, 1027.67/s  (1.006s, 1017.74/s)  LR: 9.690e-04  Data: 0.011 (0.013)
Train: 34 [ 850/1251 ( 68%)]  Loss: 4.288 (4.09)  Time: 0.997s, 1026.99/s  (1.006s, 1018.22/s)  LR: 9.690e-04  Data: 0.011 (0.013)
Train: 34 [ 900/1251 ( 72%)]  Loss: 3.833 (4.08)  Time: 1.058s,  968.24/s  (1.006s, 1017.55/s)  LR: 9.690e-04  Data: 0.011 (0.013)
Train: 34 [ 950/1251 ( 76%)]  Loss: 4.030 (4.07)  Time: 1.016s, 1007.84/s  (1.007s, 1017.08/s)  LR: 9.690e-04  Data: 0.011 (0.013)
Train: 34 [1000/1251 ( 80%)]  Loss: 3.858 (4.06)  Time: 1.007s, 1016.81/s  (1.007s, 1017.38/s)  LR: 9.690e-04  Data: 0.011 (0.013)
Train: 34 [1050/1251 ( 84%)]  Loss: 3.934 (4.06)  Time: 0.997s, 1027.46/s  (1.007s, 1016.99/s)  LR: 9.690e-04  Data: 0.012 (0.013)
Train: 34 [1100/1251 ( 88%)]  Loss: 4.004 (4.06)  Time: 0.997s, 1027.03/s  (1.008s, 1016.01/s)  LR: 9.690e-04  Data: 0.012 (0.013)
Train: 34 [1150/1251 ( 92%)]  Loss: 4.087 (4.06)  Time: 1.009s, 1014.78/s  (1.008s, 1016.35/s)  LR: 9.690e-04  Data: 0.012 (0.012)
Train: 34 [1200/1251 ( 96%)]  Loss: 3.441 (4.03)  Time: 0.998s, 1025.67/s  (1.007s, 1016.60/s)  LR: 9.690e-04  Data: 0.011 (0.012)
Train: 34 [1250/1251 (100%)]  Loss: 3.947 (4.03)  Time: 0.982s, 1042.31/s  (1.007s, 1016.70/s)  LR: 9.690e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.594 (1.594)  Loss:  0.8991 (0.8991)  Acc@1: 84.3750 (84.3750)  Acc@5: 95.9961 (95.9961)
Test: [  48/48]  Time: 0.245 (0.570)  Loss:  1.1874 (1.6001)  Acc@1: 79.4811 (66.9460)  Acc@5: 93.3962 (88.2100)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-33.pth.tar', 67.24600007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-34.pth.tar', 66.9459998779297)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-31.pth.tar', 66.166000078125)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-32.pth.tar', 66.06399997070312)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-30.pth.tar', 65.97199992675782)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-29.pth.tar', 65.91000003173828)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-27.pth.tar', 65.53399999267579)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-28.pth.tar', 65.49200006347657)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-25.pth.tar', 65.20399994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-24.pth.tar', 64.23999995605469)

Train: 35 [   0/1251 (  0%)]  Loss: 4.140 (4.14)  Time: 2.501s,  409.46/s  (2.501s,  409.46/s)  LR: 9.671e-04  Data: 1.542 (1.542)
Train: 35 [  50/1251 (  4%)]  Loss: 3.857 (4.00)  Time: 0.998s, 1026.34/s  (1.029s,  994.86/s)  LR: 9.671e-04  Data: 0.010 (0.041)
Train: 35 [ 100/1251 (  8%)]  Loss: 4.036 (4.01)  Time: 0.997s, 1027.19/s  (1.017s, 1006.56/s)  LR: 9.671e-04  Data: 0.011 (0.026)
Train: 35 [ 150/1251 ( 12%)]  Loss: 4.174 (4.05)  Time: 0.997s, 1027.40/s  (1.012s, 1011.38/s)  LR: 9.671e-04  Data: 0.011 (0.021)
Train: 35 [ 200/1251 ( 16%)]  Loss: 3.497 (3.94)  Time: 1.064s,  962.36/s  (1.011s, 1012.76/s)  LR: 9.671e-04  Data: 0.011 (0.019)
Train: 35 [ 250/1251 ( 20%)]  Loss: 4.011 (3.95)  Time: 0.994s, 1029.95/s  (1.012s, 1011.93/s)  LR: 9.671e-04  Data: 0.012 (0.017)
Train: 35 [ 300/1251 ( 24%)]  Loss: 4.019 (3.96)  Time: 1.028s,  996.25/s  (1.011s, 1012.62/s)  LR: 9.671e-04  Data: 0.011 (0.016)
Train: 35 [ 350/1251 ( 28%)]  Loss: 3.901 (3.95)  Time: 0.995s, 1029.57/s  (1.011s, 1012.94/s)  LR: 9.671e-04  Data: 0.011 (0.016)
Train: 35 [ 400/1251 ( 32%)]  Loss: 3.915 (3.95)  Time: 0.995s, 1029.04/s  (1.009s, 1014.42/s)  LR: 9.671e-04  Data: 0.011 (0.015)
Train: 35 [ 450/1251 ( 36%)]  Loss: 4.112 (3.97)  Time: 1.037s,  987.87/s  (1.009s, 1014.50/s)  LR: 9.671e-04  Data: 0.011 (0.015)
Train: 35 [ 500/1251 ( 40%)]  Loss: 4.005 (3.97)  Time: 0.995s, 1029.07/s  (1.009s, 1015.13/s)  LR: 9.671e-04  Data: 0.011 (0.014)
Train: 35 [ 550/1251 ( 44%)]  Loss: 4.105 (3.98)  Time: 1.033s,  991.23/s  (1.008s, 1015.52/s)  LR: 9.671e-04  Data: 0.011 (0.014)
Train: 35 [ 600/1251 ( 48%)]  Loss: 3.926 (3.98)  Time: 1.000s, 1023.80/s  (1.009s, 1014.73/s)  LR: 9.671e-04  Data: 0.011 (0.014)
Train: 35 [ 650/1251 ( 52%)]  Loss: 4.168 (3.99)  Time: 0.994s, 1030.55/s  (1.008s, 1015.64/s)  LR: 9.671e-04  Data: 0.010 (0.014)
Train: 35 [ 700/1251 ( 56%)]  Loss: 3.975 (3.99)  Time: 0.994s, 1030.11/s  (1.007s, 1016.44/s)  LR: 9.671e-04  Data: 0.011 (0.013)
Train: 35 [ 750/1251 ( 60%)]  Loss: 3.983 (3.99)  Time: 0.997s, 1026.65/s  (1.007s, 1017.16/s)  LR: 9.671e-04  Data: 0.012 (0.013)
Train: 35 [ 800/1251 ( 64%)]  Loss: 4.197 (4.00)  Time: 1.049s,  976.61/s  (1.007s, 1017.17/s)  LR: 9.671e-04  Data: 0.011 (0.013)
Train: 35 [ 850/1251 ( 68%)]  Loss: 4.002 (4.00)  Time: 0.994s, 1029.89/s  (1.006s, 1017.69/s)  LR: 9.671e-04  Data: 0.011 (0.013)
Train: 35 [ 900/1251 ( 72%)]  Loss: 4.128 (4.01)  Time: 0.998s, 1025.72/s  (1.006s, 1018.10/s)  LR: 9.671e-04  Data: 0.011 (0.013)
Train: 35 [ 950/1251 ( 76%)]  Loss: 3.942 (4.00)  Time: 0.996s, 1027.67/s  (1.005s, 1018.55/s)  LR: 9.671e-04  Data: 0.011 (0.013)
Train: 35 [1000/1251 ( 80%)]  Loss: 4.219 (4.01)  Time: 0.996s, 1028.21/s  (1.005s, 1018.89/s)  LR: 9.671e-04  Data: 0.011 (0.013)
Train: 35 [1050/1251 ( 84%)]  Loss: 3.955 (4.01)  Time: 0.994s, 1030.49/s  (1.005s, 1018.84/s)  LR: 9.671e-04  Data: 0.011 (0.013)
Train: 35 [1100/1251 ( 88%)]  Loss: 4.055 (4.01)  Time: 0.994s, 1029.85/s  (1.006s, 1018.39/s)  LR: 9.671e-04  Data: 0.011 (0.012)
Train: 35 [1150/1251 ( 92%)]  Loss: 4.188 (4.02)  Time: 0.998s, 1025.85/s  (1.005s, 1018.56/s)  LR: 9.671e-04  Data: 0.012 (0.012)
Train: 35 [1200/1251 ( 96%)]  Loss: 4.272 (4.03)  Time: 1.037s,  987.57/s  (1.006s, 1018.22/s)  LR: 9.671e-04  Data: 0.012 (0.012)
Train: 35 [1250/1251 (100%)]  Loss: 3.808 (4.02)  Time: 0.982s, 1042.92/s  (1.006s, 1018.18/s)  LR: 9.671e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.623 (1.623)  Loss:  1.0282 (1.0282)  Acc@1: 85.7422 (85.7422)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.245 (0.568)  Loss:  1.1501 (1.6371)  Acc@1: 80.4245 (67.3720)  Acc@5: 94.3396 (88.3560)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-35.pth.tar', 67.37200002929687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-33.pth.tar', 67.24600007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-34.pth.tar', 66.9459998779297)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-31.pth.tar', 66.166000078125)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-32.pth.tar', 66.06399997070312)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-30.pth.tar', 65.97199992675782)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-29.pth.tar', 65.91000003173828)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-27.pth.tar', 65.53399999267579)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-28.pth.tar', 65.49200006347657)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-25.pth.tar', 65.20399994873047)

Train: 36 [   0/1251 (  0%)]  Loss: 3.929 (3.93)  Time: 2.413s,  424.37/s  (2.413s,  424.37/s)  LR: 9.652e-04  Data: 1.449 (1.449)
Train: 36 [  50/1251 (  4%)]  Loss: 4.194 (4.06)  Time: 0.996s, 1027.98/s  (1.033s,  990.81/s)  LR: 9.652e-04  Data: 0.012 (0.040)
Train: 36 [ 100/1251 (  8%)]  Loss: 4.136 (4.09)  Time: 1.023s, 1000.69/s  (1.024s, 1000.47/s)  LR: 9.652e-04  Data: 0.011 (0.026)
Train: 36 [ 150/1251 ( 12%)]  Loss: 4.388 (4.16)  Time: 1.055s,  970.26/s  (1.019s, 1005.33/s)  LR: 9.652e-04  Data: 0.011 (0.021)
Train: 36 [ 200/1251 ( 16%)]  Loss: 3.796 (4.09)  Time: 0.997s, 1027.08/s  (1.015s, 1008.67/s)  LR: 9.652e-04  Data: 0.011 (0.018)
Train: 36 [ 250/1251 ( 20%)]  Loss: 4.134 (4.10)  Time: 0.995s, 1029.15/s  (1.012s, 1012.03/s)  LR: 9.652e-04  Data: 0.011 (0.017)
Train: 36 [ 300/1251 ( 24%)]  Loss: 3.893 (4.07)  Time: 0.996s, 1027.65/s  (1.012s, 1012.27/s)  LR: 9.652e-04  Data: 0.011 (0.016)
Train: 36 [ 350/1251 ( 28%)]  Loss: 3.976 (4.06)  Time: 0.997s, 1027.55/s  (1.010s, 1014.25/s)  LR: 9.652e-04  Data: 0.012 (0.015)
Train: 36 [ 400/1251 ( 32%)]  Loss: 4.003 (4.05)  Time: 1.004s, 1020.17/s  (1.008s, 1015.68/s)  LR: 9.652e-04  Data: 0.011 (0.015)
Train: 36 [ 450/1251 ( 36%)]  Loss: 3.816 (4.03)  Time: 0.998s, 1026.15/s  (1.007s, 1016.80/s)  LR: 9.652e-04  Data: 0.012 (0.014)
Train: 36 [ 500/1251 ( 40%)]  Loss: 3.736 (4.00)  Time: 1.035s,  989.35/s  (1.009s, 1015.32/s)  LR: 9.652e-04  Data: 0.012 (0.014)
Train: 36 [ 550/1251 ( 44%)]  Loss: 3.730 (3.98)  Time: 1.004s, 1020.42/s  (1.009s, 1014.98/s)  LR: 9.652e-04  Data: 0.011 (0.014)
Train: 36 [ 600/1251 ( 48%)]  Loss: 3.776 (3.96)  Time: 1.026s,  998.10/s  (1.010s, 1014.15/s)  LR: 9.652e-04  Data: 0.011 (0.014)
Train: 36 [ 650/1251 ( 52%)]  Loss: 4.253 (3.98)  Time: 0.995s, 1029.33/s  (1.010s, 1014.34/s)  LR: 9.652e-04  Data: 0.011 (0.013)
Train: 36 [ 700/1251 ( 56%)]  Loss: 3.971 (3.98)  Time: 1.008s, 1016.15/s  (1.009s, 1014.42/s)  LR: 9.652e-04  Data: 0.011 (0.013)
Train: 36 [ 750/1251 ( 60%)]  Loss: 4.135 (3.99)  Time: 0.994s, 1029.96/s  (1.009s, 1015.07/s)  LR: 9.652e-04  Data: 0.011 (0.013)
Train: 36 [ 800/1251 ( 64%)]  Loss: 3.812 (3.98)  Time: 0.996s, 1027.61/s  (1.008s, 1015.56/s)  LR: 9.652e-04  Data: 0.011 (0.013)
Train: 36 [ 850/1251 ( 68%)]  Loss: 4.190 (3.99)  Time: 0.995s, 1028.65/s  (1.008s, 1016.00/s)  LR: 9.652e-04  Data: 0.011 (0.013)
Train: 36 [ 900/1251 ( 72%)]  Loss: 4.283 (4.01)  Time: 1.037s,  987.74/s  (1.008s, 1015.66/s)  LR: 9.652e-04  Data: 0.011 (0.013)
Train: 36 [ 950/1251 ( 76%)]  Loss: 4.021 (4.01)  Time: 0.997s, 1027.25/s  (1.008s, 1015.79/s)  LR: 9.652e-04  Data: 0.012 (0.013)
Train: 36 [1000/1251 ( 80%)]  Loss: 3.825 (4.00)  Time: 1.005s, 1018.86/s  (1.009s, 1015.26/s)  LR: 9.652e-04  Data: 0.016 (0.013)
Train: 36 [1050/1251 ( 84%)]  Loss: 3.686 (3.99)  Time: 1.005s, 1019.09/s  (1.009s, 1015.20/s)  LR: 9.652e-04  Data: 0.012 (0.013)
Train: 36 [1100/1251 ( 88%)]  Loss: 3.867 (3.98)  Time: 1.002s, 1021.58/s  (1.008s, 1015.60/s)  LR: 9.652e-04  Data: 0.010 (0.013)
Train: 36 [1150/1251 ( 92%)]  Loss: 3.767 (3.97)  Time: 0.997s, 1027.56/s  (1.008s, 1015.41/s)  LR: 9.652e-04  Data: 0.012 (0.012)
Train: 36 [1200/1251 ( 96%)]  Loss: 3.678 (3.96)  Time: 1.000s, 1023.50/s  (1.008s, 1015.74/s)  LR: 9.652e-04  Data: 0.011 (0.012)
Train: 36 [1250/1251 (100%)]  Loss: 4.027 (3.96)  Time: 0.983s, 1041.91/s  (1.008s, 1016.21/s)  LR: 9.652e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.637 (1.637)  Loss:  1.0278 (1.0278)  Acc@1: 85.1562 (85.1562)  Acc@5: 95.8984 (95.8984)
Test: [  48/48]  Time: 0.245 (0.572)  Loss:  0.9618 (1.6628)  Acc@1: 82.1934 (67.1440)  Acc@5: 95.6368 (88.3940)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-35.pth.tar', 67.37200002929687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-33.pth.tar', 67.24600007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-36.pth.tar', 67.14400002197266)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-34.pth.tar', 66.9459998779297)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-31.pth.tar', 66.166000078125)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-32.pth.tar', 66.06399997070312)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-30.pth.tar', 65.97199992675782)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-29.pth.tar', 65.91000003173828)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-27.pth.tar', 65.53399999267579)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-28.pth.tar', 65.49200006347657)

Train: 37 [   0/1251 (  0%)]  Loss: 3.885 (3.88)  Time: 2.483s,  412.48/s  (2.483s,  412.48/s)  LR: 9.633e-04  Data: 1.523 (1.523)
Train: 37 [  50/1251 (  4%)]  Loss: 4.020 (3.95)  Time: 0.996s, 1027.92/s  (1.040s,  984.85/s)  LR: 9.633e-04  Data: 0.011 (0.041)
Train: 37 [ 100/1251 (  8%)]  Loss: 4.089 (4.00)  Time: 0.995s, 1028.73/s  (1.021s, 1002.70/s)  LR: 9.633e-04  Data: 0.011 (0.026)
Train: 37 [ 150/1251 ( 12%)]  Loss: 3.567 (3.89)  Time: 0.995s, 1028.89/s  (1.015s, 1008.91/s)  LR: 9.633e-04  Data: 0.011 (0.021)
Train: 37 [ 200/1251 ( 16%)]  Loss: 3.788 (3.87)  Time: 0.992s, 1032.01/s  (1.011s, 1012.68/s)  LR: 9.633e-04  Data: 0.010 (0.019)
Train: 37 [ 250/1251 ( 20%)]  Loss: 4.037 (3.90)  Time: 0.998s, 1026.04/s  (1.010s, 1013.91/s)  LR: 9.633e-04  Data: 0.012 (0.017)
Train: 37 [ 300/1251 ( 24%)]  Loss: 3.859 (3.89)  Time: 0.995s, 1028.77/s  (1.009s, 1014.61/s)  LR: 9.633e-04  Data: 0.011 (0.016)
Train: 37 [ 350/1251 ( 28%)]  Loss: 4.171 (3.93)  Time: 1.007s, 1016.80/s  (1.010s, 1013.74/s)  LR: 9.633e-04  Data: 0.011 (0.015)
Train: 37 [ 400/1251 ( 32%)]  Loss: 4.142 (3.95)  Time: 0.996s, 1028.15/s  (1.011s, 1012.55/s)  LR: 9.633e-04  Data: 0.012 (0.015)
Train: 37 [ 450/1251 ( 36%)]  Loss: 3.465 (3.90)  Time: 0.996s, 1028.09/s  (1.012s, 1011.88/s)  LR: 9.633e-04  Data: 0.011 (0.015)
Train: 37 [ 500/1251 ( 40%)]  Loss: 4.228 (3.93)  Time: 0.997s, 1027.07/s  (1.011s, 1012.68/s)  LR: 9.633e-04  Data: 0.010 (0.014)
Train: 37 [ 550/1251 ( 44%)]  Loss: 3.974 (3.94)  Time: 1.008s, 1016.32/s  (1.010s, 1013.67/s)  LR: 9.633e-04  Data: 0.011 (0.014)
Train: 37 [ 600/1251 ( 48%)]  Loss: 4.392 (3.97)  Time: 0.994s, 1030.29/s  (1.011s, 1012.43/s)  LR: 9.633e-04  Data: 0.010 (0.014)
Train: 37 [ 650/1251 ( 52%)]  Loss: 4.253 (3.99)  Time: 0.996s, 1027.96/s  (1.011s, 1012.88/s)  LR: 9.633e-04  Data: 0.011 (0.013)
Train: 37 [ 700/1251 ( 56%)]  Loss: 3.984 (3.99)  Time: 1.000s, 1023.82/s  (1.011s, 1013.09/s)  LR: 9.633e-04  Data: 0.011 (0.013)
Train: 37 [ 750/1251 ( 60%)]  Loss: 3.509 (3.96)  Time: 1.037s,  987.33/s  (1.011s, 1013.31/s)  LR: 9.633e-04  Data: 0.012 (0.013)
Train: 37 [ 800/1251 ( 64%)]  Loss: 4.063 (3.97)  Time: 0.996s, 1028.54/s  (1.011s, 1013.28/s)  LR: 9.633e-04  Data: 0.011 (0.013)
Train: 37 [ 850/1251 ( 68%)]  Loss: 3.722 (3.95)  Time: 0.996s, 1028.45/s  (1.010s, 1013.77/s)  LR: 9.633e-04  Data: 0.012 (0.013)
Train: 37 [ 900/1251 ( 72%)]  Loss: 3.853 (3.95)  Time: 0.995s, 1028.76/s  (1.009s, 1014.39/s)  LR: 9.633e-04  Data: 0.011 (0.013)
Train: 37 [ 950/1251 ( 76%)]  Loss: 4.373 (3.97)  Time: 1.003s, 1020.55/s  (1.009s, 1014.89/s)  LR: 9.633e-04  Data: 0.011 (0.013)
Train: 37 [1000/1251 ( 80%)]  Loss: 4.116 (3.98)  Time: 1.003s, 1021.08/s  (1.009s, 1014.81/s)  LR: 9.633e-04  Data: 0.011 (0.013)
Train: 37 [1050/1251 ( 84%)]  Loss: 4.192 (3.99)  Time: 1.003s, 1020.82/s  (1.009s, 1014.55/s)  LR: 9.633e-04  Data: 0.011 (0.013)
Train: 37 [1100/1251 ( 88%)]  Loss: 3.985 (3.99)  Time: 1.062s,  963.97/s  (1.009s, 1014.53/s)  LR: 9.633e-04  Data: 0.012 (0.012)
Train: 37 [1150/1251 ( 92%)]  Loss: 4.039 (3.99)  Time: 1.001s, 1023.45/s  (1.010s, 1014.29/s)  LR: 9.633e-04  Data: 0.012 (0.012)
Train: 37 [1200/1251 ( 96%)]  Loss: 3.617 (3.97)  Time: 0.994s, 1030.29/s  (1.009s, 1014.61/s)  LR: 9.633e-04  Data: 0.011 (0.012)
Train: 37 [1250/1251 (100%)]  Loss: 4.353 (3.99)  Time: 0.984s, 1041.18/s  (1.009s, 1015.04/s)  LR: 9.633e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.682 (1.682)  Loss:  1.0999 (1.0999)  Acc@1: 84.0820 (84.0820)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.245 (0.567)  Loss:  1.1890 (1.6976)  Acc@1: 81.2500 (67.4620)  Acc@5: 94.1038 (88.4900)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-37.pth.tar', 67.46200012939453)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-35.pth.tar', 67.37200002929687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-33.pth.tar', 67.24600007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-36.pth.tar', 67.14400002197266)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-34.pth.tar', 66.9459998779297)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-31.pth.tar', 66.166000078125)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-32.pth.tar', 66.06399997070312)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-30.pth.tar', 65.97199992675782)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-29.pth.tar', 65.91000003173828)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-27.pth.tar', 65.53399999267579)

Train: 38 [   0/1251 (  0%)]  Loss: 3.930 (3.93)  Time: 2.445s,  418.84/s  (2.445s,  418.84/s)  LR: 9.613e-04  Data: 1.482 (1.482)
Train: 38 [  50/1251 (  4%)]  Loss: 3.916 (3.92)  Time: 0.999s, 1025.48/s  (1.039s,  985.76/s)  LR: 9.613e-04  Data: 0.011 (0.040)
Train: 38 [ 100/1251 (  8%)]  Loss: 3.894 (3.91)  Time: 0.994s, 1030.10/s  (1.023s, 1001.10/s)  LR: 9.613e-04  Data: 0.011 (0.026)
Train: 38 [ 150/1251 ( 12%)]  Loss: 4.009 (3.94)  Time: 0.996s, 1028.35/s  (1.015s, 1008.61/s)  LR: 9.613e-04  Data: 0.011 (0.021)
Train: 38 [ 200/1251 ( 16%)]  Loss: 3.819 (3.91)  Time: 1.032s,  992.18/s  (1.012s, 1012.14/s)  LR: 9.613e-04  Data: 0.012 (0.019)
Train: 38 [ 250/1251 ( 20%)]  Loss: 3.760 (3.89)  Time: 1.003s, 1020.92/s  (1.010s, 1014.08/s)  LR: 9.613e-04  Data: 0.011 (0.017)
Train: 38 [ 300/1251 ( 24%)]  Loss: 4.125 (3.92)  Time: 1.066s,  961.00/s  (1.010s, 1013.60/s)  LR: 9.613e-04  Data: 0.011 (0.016)
Train: 38 [ 350/1251 ( 28%)]  Loss: 4.367 (3.98)  Time: 0.995s, 1028.74/s  (1.011s, 1012.64/s)  LR: 9.613e-04  Data: 0.011 (0.016)
Train: 38 [ 400/1251 ( 32%)]  Loss: 4.191 (4.00)  Time: 0.995s, 1028.70/s  (1.012s, 1011.91/s)  LR: 9.613e-04  Data: 0.011 (0.015)
Train: 38 [ 450/1251 ( 36%)]  Loss: 3.916 (3.99)  Time: 0.995s, 1029.11/s  (1.011s, 1013.20/s)  LR: 9.613e-04  Data: 0.012 (0.015)
Train: 38 [ 500/1251 ( 40%)]  Loss: 3.940 (3.99)  Time: 1.002s, 1022.46/s  (1.009s, 1014.42/s)  LR: 9.613e-04  Data: 0.011 (0.014)
Train: 38 [ 550/1251 ( 44%)]  Loss: 4.002 (3.99)  Time: 0.995s, 1029.34/s  (1.009s, 1015.06/s)  LR: 9.613e-04  Data: 0.011 (0.014)
Train: 38 [ 600/1251 ( 48%)]  Loss: 4.113 (4.00)  Time: 1.002s, 1021.67/s  (1.009s, 1015.26/s)  LR: 9.613e-04  Data: 0.011 (0.014)
Train: 38 [ 650/1251 ( 52%)]  Loss: 3.909 (3.99)  Time: 0.995s, 1028.63/s  (1.008s, 1015.72/s)  LR: 9.613e-04  Data: 0.011 (0.014)
Train: 38 [ 700/1251 ( 56%)]  Loss: 4.338 (4.02)  Time: 0.996s, 1028.37/s  (1.009s, 1015.31/s)  LR: 9.613e-04  Data: 0.010 (0.013)
Train: 38 [ 750/1251 ( 60%)]  Loss: 4.009 (4.01)  Time: 0.996s, 1028.16/s  (1.008s, 1015.98/s)  LR: 9.613e-04  Data: 0.011 (0.013)
Train: 38 [ 800/1251 ( 64%)]  Loss: 4.313 (4.03)  Time: 0.994s, 1029.99/s  (1.008s, 1016.23/s)  LR: 9.613e-04  Data: 0.011 (0.013)
Train: 38 [ 850/1251 ( 68%)]  Loss: 3.985 (4.03)  Time: 1.031s,  993.59/s  (1.007s, 1016.51/s)  LR: 9.613e-04  Data: 0.011 (0.013)
Train: 38 [ 900/1251 ( 72%)]  Loss: 3.652 (4.01)  Time: 1.006s, 1018.10/s  (1.008s, 1016.07/s)  LR: 9.613e-04  Data: 0.011 (0.013)
Train: 38 [ 950/1251 ( 76%)]  Loss: 4.266 (4.02)  Time: 0.996s, 1027.82/s  (1.007s, 1016.56/s)  LR: 9.613e-04  Data: 0.012 (0.013)
Train: 38 [1000/1251 ( 80%)]  Loss: 3.928 (4.02)  Time: 0.997s, 1027.37/s  (1.007s, 1016.89/s)  LR: 9.613e-04  Data: 0.011 (0.013)
Train: 38 [1050/1251 ( 84%)]  Loss: 4.291 (4.03)  Time: 0.996s, 1028.18/s  (1.007s, 1017.24/s)  LR: 9.613e-04  Data: 0.012 (0.013)
Train: 38 [1100/1251 ( 88%)]  Loss: 3.905 (4.03)  Time: 1.000s, 1024.29/s  (1.007s, 1017.14/s)  LR: 9.613e-04  Data: 0.011 (0.013)
Train: 38 [1150/1251 ( 92%)]  Loss: 4.316 (4.04)  Time: 0.995s, 1029.07/s  (1.006s, 1017.47/s)  LR: 9.613e-04  Data: 0.012 (0.013)
Train: 38 [1200/1251 ( 96%)]  Loss: 4.229 (4.04)  Time: 1.006s, 1017.98/s  (1.006s, 1017.80/s)  LR: 9.613e-04  Data: 0.011 (0.012)
Train: 38 [1250/1251 (100%)]  Loss: 4.115 (4.05)  Time: 0.985s, 1040.08/s  (1.006s, 1017.85/s)  LR: 9.613e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.609 (1.609)  Loss:  1.1360 (1.1360)  Acc@1: 85.2539 (85.2539)  Acc@5: 95.4102 (95.4102)
Test: [  48/48]  Time: 0.245 (0.563)  Loss:  1.1152 (1.6328)  Acc@1: 81.4858 (68.3260)  Acc@5: 94.1038 (88.8480)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-38.pth.tar', 68.32599997314453)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-37.pth.tar', 67.46200012939453)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-35.pth.tar', 67.37200002929687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-33.pth.tar', 67.24600007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-36.pth.tar', 67.14400002197266)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-34.pth.tar', 66.9459998779297)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-31.pth.tar', 66.166000078125)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-32.pth.tar', 66.06399997070312)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-30.pth.tar', 65.97199992675782)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-29.pth.tar', 65.91000003173828)

Train: 39 [   0/1251 (  0%)]  Loss: 4.004 (4.00)  Time: 4.225s,  242.39/s  (4.225s,  242.39/s)  LR: 9.593e-04  Data: 2.977 (2.977)
Train: 39 [  50/1251 (  4%)]  Loss: 4.035 (4.02)  Time: 0.997s, 1026.82/s  (1.073s,  954.40/s)  LR: 9.593e-04  Data: 0.012 (0.069)
Train: 39 [ 100/1251 (  8%)]  Loss: 4.158 (4.07)  Time: 0.996s, 1027.69/s  (1.036s,  988.45/s)  LR: 9.593e-04  Data: 0.011 (0.041)
Train: 39 [ 150/1251 ( 12%)]  Loss: 3.692 (3.97)  Time: 1.040s,  984.70/s  (1.025s,  999.18/s)  LR: 9.593e-04  Data: 0.011 (0.031)
Train: 39 [ 200/1251 ( 16%)]  Loss: 3.830 (3.94)  Time: 1.000s, 1023.60/s  (1.018s, 1005.47/s)  LR: 9.593e-04  Data: 0.011 (0.026)
Train: 39 [ 250/1251 ( 20%)]  Loss: 3.994 (3.95)  Time: 0.997s, 1027.40/s  (1.019s, 1004.93/s)  LR: 9.593e-04  Data: 0.012 (0.023)
Train: 39 [ 300/1251 ( 24%)]  Loss: 4.064 (3.97)  Time: 0.996s, 1028.22/s  (1.015s, 1008.42/s)  LR: 9.593e-04  Data: 0.012 (0.021)
Train: 39 [ 350/1251 ( 28%)]  Loss: 3.875 (3.96)  Time: 1.000s, 1023.78/s  (1.013s, 1010.45/s)  LR: 9.593e-04  Data: 0.012 (0.020)
Train: 39 [ 400/1251 ( 32%)]  Loss: 3.919 (3.95)  Time: 0.997s, 1027.07/s  (1.012s, 1012.18/s)  LR: 9.593e-04  Data: 0.012 (0.019)
Train: 39 [ 450/1251 ( 36%)]  Loss: 3.899 (3.95)  Time: 0.998s, 1025.55/s  (1.011s, 1012.38/s)  LR: 9.593e-04  Data: 0.011 (0.018)
Train: 39 [ 500/1251 ( 40%)]  Loss: 3.883 (3.94)  Time: 0.994s, 1030.05/s  (1.010s, 1013.61/s)  LR: 9.593e-04  Data: 0.011 (0.017)
Train: 39 [ 550/1251 ( 44%)]  Loss: 4.176 (3.96)  Time: 1.049s,  976.50/s  (1.010s, 1013.91/s)  LR: 9.593e-04  Data: 0.012 (0.017)
Train: 39 [ 600/1251 ( 48%)]  Loss: 4.092 (3.97)  Time: 0.998s, 1026.48/s  (1.010s, 1013.71/s)  LR: 9.593e-04  Data: 0.012 (0.016)
Train: 39 [ 650/1251 ( 52%)]  Loss: 4.431 (4.00)  Time: 0.995s, 1029.17/s  (1.009s, 1014.71/s)  LR: 9.593e-04  Data: 0.011 (0.016)
Train: 39 [ 700/1251 ( 56%)]  Loss: 4.082 (4.01)  Time: 1.058s,  967.70/s  (1.008s, 1015.37/s)  LR: 9.593e-04  Data: 0.011 (0.016)
Train: 39 [ 750/1251 ( 60%)]  Loss: 4.134 (4.02)  Time: 0.994s, 1030.57/s  (1.009s, 1015.31/s)  LR: 9.593e-04  Data: 0.011 (0.015)
Train: 39 [ 800/1251 ( 64%)]  Loss: 4.090 (4.02)  Time: 0.994s, 1029.78/s  (1.008s, 1015.98/s)  LR: 9.593e-04  Data: 0.011 (0.015)
Train: 39 [ 850/1251 ( 68%)]  Loss: 4.281 (4.04)  Time: 0.997s, 1027.07/s  (1.008s, 1015.80/s)  LR: 9.593e-04  Data: 0.012 (0.015)
Train: 39 [ 900/1251 ( 72%)]  Loss: 4.211 (4.04)  Time: 0.999s, 1025.14/s  (1.008s, 1016.21/s)  LR: 9.593e-04  Data: 0.011 (0.015)
Train: 39 [ 950/1251 ( 76%)]  Loss: 3.673 (4.03)  Time: 0.997s, 1027.49/s  (1.008s, 1016.35/s)  LR: 9.593e-04  Data: 0.011 (0.014)
Train: 39 [1000/1251 ( 80%)]  Loss: 3.750 (4.01)  Time: 1.022s, 1001.73/s  (1.007s, 1016.48/s)  LR: 9.593e-04  Data: 0.011 (0.014)
Train: 39 [1050/1251 ( 84%)]  Loss: 4.052 (4.01)  Time: 0.998s, 1026.55/s  (1.007s, 1016.80/s)  LR: 9.593e-04  Data: 0.012 (0.014)
Train: 39 [1100/1251 ( 88%)]  Loss: 4.531 (4.04)  Time: 1.082s,  946.02/s  (1.007s, 1016.93/s)  LR: 9.593e-04  Data: 0.012 (0.014)
Train: 39 [1150/1251 ( 92%)]  Loss: 4.182 (4.04)  Time: 1.006s, 1018.05/s  (1.007s, 1017.15/s)  LR: 9.593e-04  Data: 0.011 (0.014)
Train: 39 [1200/1251 ( 96%)]  Loss: 4.257 (4.05)  Time: 1.065s,  961.27/s  (1.007s, 1016.94/s)  LR: 9.593e-04  Data: 0.012 (0.014)
Train: 39 [1250/1251 (100%)]  Loss: 4.119 (4.05)  Time: 0.983s, 1041.86/s  (1.007s, 1017.24/s)  LR: 9.593e-04  Data: 0.000 (0.014)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.636 (1.636)  Loss:  1.0350 (1.0350)  Acc@1: 87.4023 (87.4023)  Acc@5: 96.0938 (96.0938)
Test: [  48/48]  Time: 0.245 (0.568)  Loss:  1.1034 (1.6641)  Acc@1: 82.0755 (67.8840)  Acc@5: 94.3396 (88.6220)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-38.pth.tar', 68.32599997314453)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-39.pth.tar', 67.88400010009765)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-37.pth.tar', 67.46200012939453)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-35.pth.tar', 67.37200002929687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-33.pth.tar', 67.24600007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-36.pth.tar', 67.14400002197266)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-34.pth.tar', 66.9459998779297)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-31.pth.tar', 66.166000078125)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-32.pth.tar', 66.06399997070312)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-30.pth.tar', 65.97199992675782)

Train: 40 [   0/1251 (  0%)]  Loss: 3.905 (3.91)  Time: 2.420s,  423.19/s  (2.420s,  423.19/s)  LR: 9.572e-04  Data: 1.456 (1.456)
Train: 40 [  50/1251 (  4%)]  Loss: 4.393 (4.15)  Time: 1.004s, 1020.30/s  (1.040s,  984.78/s)  LR: 9.572e-04  Data: 0.011 (0.039)
Train: 40 [ 100/1251 (  8%)]  Loss: 4.313 (4.20)  Time: 0.999s, 1025.26/s  (1.021s, 1003.29/s)  LR: 9.572e-04  Data: 0.011 (0.025)
Train: 40 [ 150/1251 ( 12%)]  Loss: 4.220 (4.21)  Time: 1.044s,  980.61/s  (1.019s, 1005.14/s)  LR: 9.572e-04  Data: 0.011 (0.021)
Train: 40 [ 200/1251 ( 16%)]  Loss: 3.935 (4.15)  Time: 0.996s, 1027.76/s  (1.014s, 1010.16/s)  LR: 9.572e-04  Data: 0.012 (0.018)
Train: 40 [ 250/1251 ( 20%)]  Loss: 3.680 (4.07)  Time: 0.994s, 1029.95/s  (1.014s, 1010.30/s)  LR: 9.572e-04  Data: 0.011 (0.017)
Train: 40 [ 300/1251 ( 24%)]  Loss: 4.233 (4.10)  Time: 1.021s, 1002.73/s  (1.012s, 1012.22/s)  LR: 9.572e-04  Data: 0.011 (0.016)
Train: 40 [ 350/1251 ( 28%)]  Loss: 3.922 (4.08)  Time: 0.994s, 1030.16/s  (1.010s, 1013.40/s)  LR: 9.572e-04  Data: 0.011 (0.015)
Train: 40 [ 400/1251 ( 32%)]  Loss: 4.124 (4.08)  Time: 0.996s, 1028.53/s  (1.009s, 1014.45/s)  LR: 9.572e-04  Data: 0.012 (0.015)
Train: 40 [ 450/1251 ( 36%)]  Loss: 3.907 (4.06)  Time: 0.995s, 1029.55/s  (1.008s, 1015.76/s)  LR: 9.572e-04  Data: 0.011 (0.014)
Train: 40 [ 500/1251 ( 40%)]  Loss: 4.115 (4.07)  Time: 1.047s,  977.81/s  (1.008s, 1016.28/s)  LR: 9.572e-04  Data: 0.011 (0.014)
Train: 40 [ 550/1251 ( 44%)]  Loss: 3.717 (4.04)  Time: 0.996s, 1028.06/s  (1.008s, 1015.59/s)  LR: 9.572e-04  Data: 0.011 (0.014)
Train: 40 [ 600/1251 ( 48%)]  Loss: 3.781 (4.02)  Time: 1.032s,  992.72/s  (1.008s, 1015.68/s)  LR: 9.572e-04  Data: 0.010 (0.014)
Train: 40 [ 650/1251 ( 52%)]  Loss: 4.098 (4.02)  Time: 1.007s, 1017.03/s  (1.008s, 1015.96/s)  LR: 9.572e-04  Data: 0.011 (0.013)
Train: 40 [ 700/1251 ( 56%)]  Loss: 3.969 (4.02)  Time: 1.034s,  990.19/s  (1.008s, 1015.98/s)  LR: 9.572e-04  Data: 0.011 (0.013)
Train: 40 [ 750/1251 ( 60%)]  Loss: 4.052 (4.02)  Time: 0.996s, 1027.99/s  (1.008s, 1015.90/s)  LR: 9.572e-04  Data: 0.011 (0.013)
Train: 40 [ 800/1251 ( 64%)]  Loss: 3.743 (4.01)  Time: 0.996s, 1028.13/s  (1.008s, 1015.78/s)  LR: 9.572e-04  Data: 0.011 (0.013)
Train: 40 [ 850/1251 ( 68%)]  Loss: 4.086 (4.01)  Time: 0.996s, 1028.29/s  (1.008s, 1015.39/s)  LR: 9.572e-04  Data: 0.010 (0.013)
Train: 40 [ 900/1251 ( 72%)]  Loss: 3.704 (3.99)  Time: 0.997s, 1027.54/s  (1.008s, 1015.79/s)  LR: 9.572e-04  Data: 0.011 (0.013)
Train: 40 [ 950/1251 ( 76%)]  Loss: 3.973 (3.99)  Time: 1.002s, 1021.45/s  (1.009s, 1014.72/s)  LR: 9.572e-04  Data: 0.011 (0.013)
Train: 40 [1000/1251 ( 80%)]  Loss: 3.688 (3.98)  Time: 0.998s, 1026.23/s  (1.009s, 1014.64/s)  LR: 9.572e-04  Data: 0.011 (0.013)
Train: 40 [1050/1251 ( 84%)]  Loss: 3.706 (3.97)  Time: 1.074s,  953.53/s  (1.009s, 1014.82/s)  LR: 9.572e-04  Data: 0.012 (0.013)
Train: 40 [1100/1251 ( 88%)]  Loss: 3.864 (3.96)  Time: 0.997s, 1027.37/s  (1.009s, 1015.15/s)  LR: 9.572e-04  Data: 0.011 (0.013)
Train: 40 [1150/1251 ( 92%)]  Loss: 4.011 (3.96)  Time: 0.996s, 1027.67/s  (1.009s, 1014.88/s)  LR: 9.572e-04  Data: 0.012 (0.013)
Train: 40 [1200/1251 ( 96%)]  Loss: 3.890 (3.96)  Time: 1.050s,  975.35/s  (1.009s, 1014.54/s)  LR: 9.572e-04  Data: 0.011 (0.012)
Train: 40 [1250/1251 (100%)]  Loss: 3.919 (3.96)  Time: 0.986s, 1038.85/s  (1.010s, 1014.34/s)  LR: 9.572e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.617 (1.617)  Loss:  0.8861 (0.8861)  Acc@1: 84.3750 (84.3750)  Acc@5: 95.3125 (95.3125)
Test: [  48/48]  Time: 0.245 (0.564)  Loss:  1.0374 (1.5595)  Acc@1: 78.6557 (67.6300)  Acc@5: 93.8679 (88.6240)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-38.pth.tar', 68.32599997314453)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-39.pth.tar', 67.88400010009765)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-40.pth.tar', 67.62999990722656)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-37.pth.tar', 67.46200012939453)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-35.pth.tar', 67.37200002929687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-33.pth.tar', 67.24600007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-36.pth.tar', 67.14400002197266)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-34.pth.tar', 66.9459998779297)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-31.pth.tar', 66.166000078125)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-32.pth.tar', 66.06399997070312)

Train: 41 [   0/1251 (  0%)]  Loss: 3.785 (3.78)  Time: 2.488s,  411.52/s  (2.488s,  411.52/s)  LR: 9.551e-04  Data: 1.530 (1.530)
Train: 41 [  50/1251 (  4%)]  Loss: 4.065 (3.93)  Time: 1.009s, 1015.11/s  (1.033s,  991.28/s)  LR: 9.551e-04  Data: 0.012 (0.046)
Train: 41 [ 100/1251 (  8%)]  Loss: 3.799 (3.88)  Time: 0.996s, 1028.50/s  (1.016s, 1007.66/s)  LR: 9.551e-04  Data: 0.011 (0.029)
Train: 41 [ 150/1251 ( 12%)]  Loss: 4.390 (4.01)  Time: 1.047s,  978.08/s  (1.014s, 1010.04/s)  LR: 9.551e-04  Data: 0.012 (0.023)
Train: 41 [ 200/1251 ( 16%)]  Loss: 4.185 (4.04)  Time: 0.995s, 1028.79/s  (1.015s, 1008.39/s)  LR: 9.551e-04  Data: 0.012 (0.020)
Train: 41 [ 250/1251 ( 20%)]  Loss: 3.995 (4.04)  Time: 0.990s, 1033.90/s  (1.018s, 1005.55/s)  LR: 9.551e-04  Data: 0.010 (0.018)
Train: 41 [ 300/1251 ( 24%)]  Loss: 4.058 (4.04)  Time: 0.995s, 1029.13/s  (1.019s, 1004.82/s)  LR: 9.551e-04  Data: 0.012 (0.017)
Train: 41 [ 350/1251 ( 28%)]  Loss: 4.144 (4.05)  Time: 1.064s,  962.77/s  (1.020s, 1003.61/s)  LR: 9.551e-04  Data: 0.011 (0.016)
Train: 41 [ 400/1251 ( 32%)]  Loss: 3.427 (3.98)  Time: 0.995s, 1028.69/s  (1.020s, 1003.73/s)  LR: 9.551e-04  Data: 0.011 (0.015)
Train: 41 [ 450/1251 ( 36%)]  Loss: 4.160 (4.00)  Time: 0.995s, 1028.72/s  (1.019s, 1004.49/s)  LR: 9.551e-04  Data: 0.011 (0.015)
Train: 41 [ 500/1251 ( 40%)]  Loss: 3.911 (3.99)  Time: 0.997s, 1026.72/s  (1.018s, 1005.81/s)  LR: 9.551e-04  Data: 0.010 (0.015)
Train: 41 [ 550/1251 ( 44%)]  Loss: 3.822 (3.98)  Time: 0.995s, 1029.29/s  (1.016s, 1007.40/s)  LR: 9.551e-04  Data: 0.012 (0.014)
Train: 41 [ 600/1251 ( 48%)]  Loss: 3.820 (3.97)  Time: 1.004s, 1019.86/s  (1.015s, 1008.71/s)  LR: 9.551e-04  Data: 0.011 (0.014)
Train: 41 [ 650/1251 ( 52%)]  Loss: 3.851 (3.96)  Time: 0.996s, 1027.64/s  (1.014s, 1009.39/s)  LR: 9.551e-04  Data: 0.010 (0.014)
Train: 41 [ 700/1251 ( 56%)]  Loss: 4.120 (3.97)  Time: 0.992s, 1032.38/s  (1.013s, 1010.56/s)  LR: 9.551e-04  Data: 0.010 (0.014)
Train: 41 [ 750/1251 ( 60%)]  Loss: 3.907 (3.96)  Time: 0.994s, 1029.76/s  (1.014s, 1010.35/s)  LR: 9.551e-04  Data: 0.011 (0.013)
Train: 41 [ 800/1251 ( 64%)]  Loss: 4.067 (3.97)  Time: 1.003s, 1021.23/s  (1.013s, 1011.22/s)  LR: 9.551e-04  Data: 0.011 (0.013)
Train: 41 [ 850/1251 ( 68%)]  Loss: 3.683 (3.95)  Time: 1.033s,  991.73/s  (1.012s, 1011.88/s)  LR: 9.551e-04  Data: 0.011 (0.013)
Train: 41 [ 900/1251 ( 72%)]  Loss: 3.925 (3.95)  Time: 0.993s, 1030.75/s  (1.011s, 1012.39/s)  LR: 9.551e-04  Data: 0.010 (0.013)
Train: 41 [ 950/1251 ( 76%)]  Loss: 3.739 (3.94)  Time: 1.059s,  966.72/s  (1.012s, 1012.29/s)  LR: 9.551e-04  Data: 0.011 (0.013)
Train: 41 [1000/1251 ( 80%)]  Loss: 3.853 (3.94)  Time: 1.063s,  963.39/s  (1.013s, 1011.32/s)  LR: 9.551e-04  Data: 0.010 (0.013)
Train: 41 [1050/1251 ( 84%)]  Loss: 4.137 (3.95)  Time: 1.038s,  986.20/s  (1.013s, 1011.30/s)  LR: 9.551e-04  Data: 0.010 (0.013)
Train: 41 [1100/1251 ( 88%)]  Loss: 3.889 (3.94)  Time: 0.995s, 1029.48/s  (1.013s, 1011.00/s)  LR: 9.551e-04  Data: 0.010 (0.013)
Train: 41 [1150/1251 ( 92%)]  Loss: 3.736 (3.94)  Time: 1.038s,  986.26/s  (1.013s, 1011.10/s)  LR: 9.551e-04  Data: 0.011 (0.013)
Train: 41 [1200/1251 ( 96%)]  Loss: 4.077 (3.94)  Time: 0.996s, 1027.89/s  (1.012s, 1011.50/s)  LR: 9.551e-04  Data: 0.011 (0.013)
Train: 41 [1250/1251 (100%)]  Loss: 3.975 (3.94)  Time: 0.982s, 1042.38/s  (1.012s, 1012.09/s)  LR: 9.551e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.627 (1.627)  Loss:  0.9725 (0.9725)  Acc@1: 83.7891 (83.7891)  Acc@5: 95.8008 (95.8008)
Test: [  48/48]  Time: 0.245 (0.570)  Loss:  1.0200 (1.5460)  Acc@1: 81.6038 (67.8240)  Acc@5: 93.8679 (88.7200)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-38.pth.tar', 68.32599997314453)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-39.pth.tar', 67.88400010009765)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-41.pth.tar', 67.82400002441406)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-40.pth.tar', 67.62999990722656)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-37.pth.tar', 67.46200012939453)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-35.pth.tar', 67.37200002929687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-33.pth.tar', 67.24600007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-36.pth.tar', 67.14400002197266)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-34.pth.tar', 66.9459998779297)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-31.pth.tar', 66.166000078125)

Train: 42 [   0/1251 (  0%)]  Loss: 3.762 (3.76)  Time: 2.541s,  402.96/s  (2.541s,  402.96/s)  LR: 9.529e-04  Data: 1.577 (1.577)
Train: 42 [  50/1251 (  4%)]  Loss: 3.640 (3.70)  Time: 0.995s, 1029.20/s  (1.055s,  970.89/s)  LR: 9.529e-04  Data: 0.011 (0.045)
Train: 42 [ 100/1251 (  8%)]  Loss: 3.964 (3.79)  Time: 0.994s, 1029.84/s  (1.032s,  992.31/s)  LR: 9.529e-04  Data: 0.011 (0.028)
Train: 42 [ 150/1251 ( 12%)]  Loss: 4.128 (3.87)  Time: 0.998s, 1025.86/s  (1.025s,  999.13/s)  LR: 9.529e-04  Data: 0.011 (0.022)
Train: 42 [ 200/1251 ( 16%)]  Loss: 3.803 (3.86)  Time: 0.995s, 1028.86/s  (1.022s, 1002.45/s)  LR: 9.529e-04  Data: 0.010 (0.020)
Train: 42 [ 250/1251 ( 20%)]  Loss: 4.203 (3.92)  Time: 0.997s, 1027.22/s  (1.017s, 1006.50/s)  LR: 9.529e-04  Data: 0.011 (0.018)
Train: 42 [ 300/1251 ( 24%)]  Loss: 4.125 (3.95)  Time: 0.997s, 1027.33/s  (1.015s, 1008.75/s)  LR: 9.529e-04  Data: 0.012 (0.017)
Train: 42 [ 350/1251 ( 28%)]  Loss: 3.793 (3.93)  Time: 0.996s, 1028.30/s  (1.016s, 1007.94/s)  LR: 9.529e-04  Data: 0.011 (0.016)
Train: 42 [ 400/1251 ( 32%)]  Loss: 3.920 (3.93)  Time: 0.993s, 1030.92/s  (1.015s, 1008.60/s)  LR: 9.529e-04  Data: 0.011 (0.015)
Train: 42 [ 450/1251 ( 36%)]  Loss: 4.003 (3.93)  Time: 0.997s, 1027.47/s  (1.014s, 1010.36/s)  LR: 9.529e-04  Data: 0.011 (0.015)
Train: 42 [ 500/1251 ( 40%)]  Loss: 4.026 (3.94)  Time: 0.998s, 1026.25/s  (1.013s, 1011.16/s)  LR: 9.529e-04  Data: 0.012 (0.015)
Train: 42 [ 550/1251 ( 44%)]  Loss: 3.599 (3.91)  Time: 1.003s, 1021.40/s  (1.012s, 1012.21/s)  LR: 9.529e-04  Data: 0.011 (0.014)
Train: 42 [ 600/1251 ( 48%)]  Loss: 3.949 (3.92)  Time: 1.004s, 1019.49/s  (1.011s, 1012.64/s)  LR: 9.529e-04  Data: 0.011 (0.014)
Train: 42 [ 650/1251 ( 52%)]  Loss: 3.995 (3.92)  Time: 0.995s, 1029.47/s  (1.010s, 1013.38/s)  LR: 9.529e-04  Data: 0.011 (0.014)
Train: 42 [ 700/1251 ( 56%)]  Loss: 4.036 (3.93)  Time: 0.998s, 1026.17/s  (1.010s, 1013.91/s)  LR: 9.529e-04  Data: 0.011 (0.014)
Train: 42 [ 750/1251 ( 60%)]  Loss: 3.810 (3.92)  Time: 0.996s, 1028.12/s  (1.009s, 1014.69/s)  LR: 9.529e-04  Data: 0.012 (0.013)
Train: 42 [ 800/1251 ( 64%)]  Loss: 4.091 (3.93)  Time: 1.029s,  995.01/s  (1.009s, 1014.53/s)  LR: 9.529e-04  Data: 0.011 (0.013)
Train: 42 [ 850/1251 ( 68%)]  Loss: 4.110 (3.94)  Time: 1.003s, 1020.73/s  (1.010s, 1013.53/s)  LR: 9.529e-04  Data: 0.011 (0.013)
Train: 42 [ 900/1251 ( 72%)]  Loss: 4.247 (3.96)  Time: 0.993s, 1031.72/s  (1.010s, 1014.14/s)  LR: 9.529e-04  Data: 0.012 (0.013)
Train: 42 [ 950/1251 ( 76%)]  Loss: 4.183 (3.97)  Time: 0.997s, 1026.89/s  (1.010s, 1014.01/s)  LR: 9.529e-04  Data: 0.010 (0.013)
Train: 42 [1000/1251 ( 80%)]  Loss: 4.191 (3.98)  Time: 0.996s, 1028.00/s  (1.009s, 1014.53/s)  LR: 9.529e-04  Data: 0.012 (0.013)
Train: 42 [1050/1251 ( 84%)]  Loss: 3.803 (3.97)  Time: 1.000s, 1023.62/s  (1.009s, 1014.43/s)  LR: 9.529e-04  Data: 0.011 (0.013)
Train: 42 [1100/1251 ( 88%)]  Loss: 4.377 (3.99)  Time: 0.995s, 1029.09/s  (1.009s, 1014.85/s)  LR: 9.529e-04  Data: 0.011 (0.013)
Train: 42 [1150/1251 ( 92%)]  Loss: 4.028 (3.99)  Time: 0.995s, 1028.68/s  (1.009s, 1015.32/s)  LR: 9.529e-04  Data: 0.012 (0.013)
Train: 42 [1200/1251 ( 96%)]  Loss: 4.081 (3.99)  Time: 0.994s, 1030.23/s  (1.008s, 1015.66/s)  LR: 9.529e-04  Data: 0.010 (0.013)
Train: 42 [1250/1251 (100%)]  Loss: 4.050 (4.00)  Time: 0.984s, 1040.48/s  (1.008s, 1015.49/s)  LR: 9.529e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.670 (1.670)  Loss:  0.9148 (0.9148)  Acc@1: 84.6680 (84.6680)  Acc@5: 95.9961 (95.9961)
Test: [  48/48]  Time: 0.245 (0.570)  Loss:  1.0920 (1.5617)  Acc@1: 81.1321 (68.3400)  Acc@5: 93.5142 (89.1200)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-42.pth.tar', 68.33999994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-38.pth.tar', 68.32599997314453)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-39.pth.tar', 67.88400010009765)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-41.pth.tar', 67.82400002441406)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-40.pth.tar', 67.62999990722656)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-37.pth.tar', 67.46200012939453)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-35.pth.tar', 67.37200002929687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-33.pth.tar', 67.24600007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-36.pth.tar', 67.14400002197266)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-34.pth.tar', 66.9459998779297)

Train: 43 [   0/1251 (  0%)]  Loss: 4.258 (4.26)  Time: 2.641s,  387.69/s  (2.641s,  387.69/s)  LR: 9.507e-04  Data: 1.674 (1.674)
Train: 43 [  50/1251 (  4%)]  Loss: 4.135 (4.20)  Time: 0.997s, 1027.15/s  (1.035s,  989.53/s)  LR: 9.507e-04  Data: 0.011 (0.044)
Train: 43 [ 100/1251 (  8%)]  Loss: 3.544 (3.98)  Time: 0.995s, 1029.45/s  (1.020s, 1004.30/s)  LR: 9.507e-04  Data: 0.011 (0.028)
Train: 43 [ 150/1251 ( 12%)]  Loss: 4.052 (4.00)  Time: 1.000s, 1023.89/s  (1.017s, 1006.53/s)  LR: 9.507e-04  Data: 0.011 (0.022)
Train: 43 [ 200/1251 ( 16%)]  Loss: 3.659 (3.93)  Time: 1.000s, 1024.13/s  (1.015s, 1008.49/s)  LR: 9.507e-04  Data: 0.011 (0.019)
Train: 43 [ 250/1251 ( 20%)]  Loss: 3.976 (3.94)  Time: 1.004s, 1020.16/s  (1.013s, 1010.44/s)  LR: 9.507e-04  Data: 0.011 (0.018)
Train: 43 [ 300/1251 ( 24%)]  Loss: 4.015 (3.95)  Time: 1.048s,  976.91/s  (1.011s, 1012.50/s)  LR: 9.507e-04  Data: 0.011 (0.017)
Train: 43 [ 350/1251 ( 28%)]  Loss: 4.105 (3.97)  Time: 0.998s, 1025.60/s  (1.010s, 1014.31/s)  LR: 9.507e-04  Data: 0.011 (0.016)
Train: 43 [ 400/1251 ( 32%)]  Loss: 3.966 (3.97)  Time: 0.997s, 1026.86/s  (1.011s, 1013.31/s)  LR: 9.507e-04  Data: 0.010 (0.015)
Train: 43 [ 450/1251 ( 36%)]  Loss: 3.486 (3.92)  Time: 0.995s, 1029.07/s  (1.010s, 1014.14/s)  LR: 9.507e-04  Data: 0.011 (0.015)
Train: 43 [ 500/1251 ( 40%)]  Loss: 3.922 (3.92)  Time: 0.995s, 1029.50/s  (1.009s, 1015.07/s)  LR: 9.507e-04  Data: 0.010 (0.014)
Train: 43 [ 550/1251 ( 44%)]  Loss: 3.610 (3.89)  Time: 1.034s,  990.30/s  (1.009s, 1014.94/s)  LR: 9.507e-04  Data: 0.011 (0.014)
Train: 43 [ 600/1251 ( 48%)]  Loss: 3.942 (3.90)  Time: 1.051s,  973.87/s  (1.009s, 1014.77/s)  LR: 9.507e-04  Data: 0.011 (0.014)
Train: 43 [ 650/1251 ( 52%)]  Loss: 3.903 (3.90)  Time: 0.989s, 1034.99/s  (1.009s, 1015.03/s)  LR: 9.507e-04  Data: 0.010 (0.014)
Train: 43 [ 700/1251 ( 56%)]  Loss: 3.847 (3.89)  Time: 0.995s, 1028.67/s  (1.008s, 1015.37/s)  LR: 9.507e-04  Data: 0.011 (0.013)
Train: 43 [ 750/1251 ( 60%)]  Loss: 3.941 (3.90)  Time: 0.995s, 1029.29/s  (1.008s, 1015.90/s)  LR: 9.507e-04  Data: 0.010 (0.013)
Train: 43 [ 800/1251 ( 64%)]  Loss: 3.941 (3.90)  Time: 0.992s, 1031.93/s  (1.007s, 1016.49/s)  LR: 9.507e-04  Data: 0.010 (0.013)
Train: 43 [ 850/1251 ( 68%)]  Loss: 3.867 (3.90)  Time: 0.996s, 1028.63/s  (1.007s, 1017.05/s)  LR: 9.507e-04  Data: 0.011 (0.013)
Train: 43 [ 900/1251 ( 72%)]  Loss: 3.783 (3.89)  Time: 1.045s,  979.72/s  (1.007s, 1016.94/s)  LR: 9.507e-04  Data: 0.011 (0.013)
Train: 43 [ 950/1251 ( 76%)]  Loss: 3.708 (3.88)  Time: 0.996s, 1027.70/s  (1.006s, 1017.43/s)  LR: 9.507e-04  Data: 0.012 (0.013)
Train: 43 [1000/1251 ( 80%)]  Loss: 4.110 (3.89)  Time: 1.060s,  965.91/s  (1.007s, 1017.35/s)  LR: 9.507e-04  Data: 0.012 (0.013)
Train: 43 [1050/1251 ( 84%)]  Loss: 4.158 (3.91)  Time: 1.000s, 1023.57/s  (1.007s, 1016.95/s)  LR: 9.507e-04  Data: 0.011 (0.013)
Train: 43 [1100/1251 ( 88%)]  Loss: 3.846 (3.90)  Time: 0.998s, 1026.23/s  (1.007s, 1017.21/s)  LR: 9.507e-04  Data: 0.012 (0.013)
Train: 43 [1150/1251 ( 92%)]  Loss: 4.129 (3.91)  Time: 0.994s, 1029.95/s  (1.007s, 1017.31/s)  LR: 9.507e-04  Data: 0.010 (0.013)
Train: 43 [1200/1251 ( 96%)]  Loss: 4.425 (3.93)  Time: 1.003s, 1020.66/s  (1.006s, 1017.77/s)  LR: 9.507e-04  Data: 0.012 (0.013)
Train: 43 [1250/1251 (100%)]  Loss: 3.500 (3.92)  Time: 0.983s, 1041.38/s  (1.007s, 1017.32/s)  LR: 9.507e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.666 (1.666)  Loss:  1.0954 (1.0954)  Acc@1: 83.8867 (83.8867)  Acc@5: 95.4102 (95.4102)
Test: [  48/48]  Time: 0.245 (0.571)  Loss:  1.1222 (1.6420)  Acc@1: 82.3113 (68.3480)  Acc@5: 93.3962 (89.0080)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-43.pth.tar', 68.34800007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-42.pth.tar', 68.33999994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-38.pth.tar', 68.32599997314453)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-39.pth.tar', 67.88400010009765)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-41.pth.tar', 67.82400002441406)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-40.pth.tar', 67.62999990722656)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-37.pth.tar', 67.46200012939453)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-35.pth.tar', 67.37200002929687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-33.pth.tar', 67.24600007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-36.pth.tar', 67.14400002197266)

Train: 44 [   0/1251 (  0%)]  Loss: 3.773 (3.77)  Time: 2.433s,  420.81/s  (2.433s,  420.81/s)  LR: 9.484e-04  Data: 1.469 (1.469)
Train: 44 [  50/1251 (  4%)]  Loss: 4.200 (3.99)  Time: 1.062s,  964.36/s  (1.043s,  981.48/s)  LR: 9.484e-04  Data: 0.012 (0.041)
Train: 44 [ 100/1251 (  8%)]  Loss: 3.539 (3.84)  Time: 1.060s,  966.16/s  (1.036s,  988.59/s)  LR: 9.484e-04  Data: 0.011 (0.026)
Train: 44 [ 150/1251 ( 12%)]  Loss: 3.900 (3.85)  Time: 1.036s,  988.77/s  (1.029s,  994.89/s)  LR: 9.484e-04  Data: 0.012 (0.021)
Train: 44 [ 200/1251 ( 16%)]  Loss: 3.744 (3.83)  Time: 0.997s, 1027.58/s  (1.023s, 1001.18/s)  LR: 9.484e-04  Data: 0.010 (0.019)
Train: 44 [ 250/1251 ( 20%)]  Loss: 4.010 (3.86)  Time: 0.997s, 1026.92/s  (1.019s, 1005.01/s)  LR: 9.484e-04  Data: 0.012 (0.017)
Train: 44 [ 300/1251 ( 24%)]  Loss: 4.710 (3.98)  Time: 1.064s,  962.80/s  (1.017s, 1006.74/s)  LR: 9.484e-04  Data: 0.012 (0.016)
Train: 44 [ 350/1251 ( 28%)]  Loss: 3.337 (3.90)  Time: 0.993s, 1031.09/s  (1.017s, 1007.35/s)  LR: 9.484e-04  Data: 0.011 (0.016)
Train: 44 [ 400/1251 ( 32%)]  Loss: 4.377 (3.95)  Time: 0.995s, 1029.45/s  (1.015s, 1009.33/s)  LR: 9.484e-04  Data: 0.011 (0.015)
Train: 44 [ 450/1251 ( 36%)]  Loss: 3.774 (3.94)  Time: 0.997s, 1026.90/s  (1.013s, 1011.13/s)  LR: 9.484e-04  Data: 0.011 (0.015)
Train: 44 [ 500/1251 ( 40%)]  Loss: 3.521 (3.90)  Time: 0.998s, 1026.19/s  (1.011s, 1012.58/s)  LR: 9.484e-04  Data: 0.011 (0.014)
Train: 44 [ 550/1251 ( 44%)]  Loss: 3.876 (3.90)  Time: 1.013s, 1010.47/s  (1.010s, 1013.66/s)  LR: 9.484e-04  Data: 0.011 (0.014)
Train: 44 [ 600/1251 ( 48%)]  Loss: 3.790 (3.89)  Time: 0.996s, 1027.64/s  (1.009s, 1014.66/s)  LR: 9.484e-04  Data: 0.012 (0.014)
Train: 44 [ 650/1251 ( 52%)]  Loss: 3.850 (3.89)  Time: 0.998s, 1026.23/s  (1.009s, 1015.32/s)  LR: 9.484e-04  Data: 0.010 (0.014)
Train: 44 [ 700/1251 ( 56%)]  Loss: 3.858 (3.88)  Time: 1.062s,  964.48/s  (1.008s, 1015.38/s)  LR: 9.484e-04  Data: 0.012 (0.013)
Train: 44 [ 750/1251 ( 60%)]  Loss: 4.142 (3.90)  Time: 1.066s,  961.03/s  (1.008s, 1015.39/s)  LR: 9.484e-04  Data: 0.011 (0.013)
Train: 44 [ 800/1251 ( 64%)]  Loss: 4.077 (3.91)  Time: 1.001s, 1022.91/s  (1.008s, 1015.52/s)  LR: 9.484e-04  Data: 0.011 (0.013)
Train: 44 [ 850/1251 ( 68%)]  Loss: 3.921 (3.91)  Time: 1.033s,  990.88/s  (1.008s, 1016.03/s)  LR: 9.484e-04  Data: 0.010 (0.013)
Train: 44 [ 900/1251 ( 72%)]  Loss: 3.962 (3.91)  Time: 1.000s, 1024.26/s  (1.008s, 1016.05/s)  LR: 9.484e-04  Data: 0.013 (0.013)
Train: 44 [ 950/1251 ( 76%)]  Loss: 4.156 (3.93)  Time: 0.996s, 1027.61/s  (1.008s, 1016.27/s)  LR: 9.484e-04  Data: 0.011 (0.013)
Train: 44 [1000/1251 ( 80%)]  Loss: 4.046 (3.93)  Time: 0.998s, 1026.49/s  (1.007s, 1016.67/s)  LR: 9.484e-04  Data: 0.011 (0.013)
Train: 44 [1050/1251 ( 84%)]  Loss: 3.906 (3.93)  Time: 0.995s, 1028.91/s  (1.007s, 1016.59/s)  LR: 9.484e-04  Data: 0.011 (0.013)
Train: 44 [1100/1251 ( 88%)]  Loss: 3.968 (3.93)  Time: 1.034s,  990.45/s  (1.008s, 1016.26/s)  LR: 9.484e-04  Data: 0.012 (0.013)
Train: 44 [1150/1251 ( 92%)]  Loss: 3.738 (3.92)  Time: 0.996s, 1028.38/s  (1.008s, 1016.10/s)  LR: 9.484e-04  Data: 0.011 (0.013)
Train: 44 [1200/1251 ( 96%)]  Loss: 3.797 (3.92)  Time: 1.004s, 1020.38/s  (1.008s, 1016.29/s)  LR: 9.484e-04  Data: 0.012 (0.012)
Train: 44 [1250/1251 (100%)]  Loss: 4.399 (3.94)  Time: 0.984s, 1040.75/s  (1.007s, 1016.57/s)  LR: 9.484e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.619 (1.619)  Loss:  0.9724 (0.9724)  Acc@1: 84.8633 (84.8633)  Acc@5: 95.7031 (95.7031)
Test: [  48/48]  Time: 0.245 (0.568)  Loss:  1.0052 (1.5645)  Acc@1: 81.9576 (68.0380)  Acc@5: 94.9292 (88.7140)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-43.pth.tar', 68.34800007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-42.pth.tar', 68.33999994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-38.pth.tar', 68.32599997314453)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-44.pth.tar', 68.03800004882812)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-39.pth.tar', 67.88400010009765)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-41.pth.tar', 67.82400002441406)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-40.pth.tar', 67.62999990722656)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-37.pth.tar', 67.46200012939453)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-35.pth.tar', 67.37200002929687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-33.pth.tar', 67.24600007324219)

Train: 45 [   0/1251 (  0%)]  Loss: 3.935 (3.94)  Time: 2.488s,  411.55/s  (2.488s,  411.55/s)  LR: 9.460e-04  Data: 1.532 (1.532)
Train: 45 [  50/1251 (  4%)]  Loss: 3.831 (3.88)  Time: 1.032s,  992.24/s  (1.042s,  982.91/s)  LR: 9.460e-04  Data: 0.011 (0.041)
Train: 45 [ 100/1251 (  8%)]  Loss: 4.136 (3.97)  Time: 1.014s, 1009.54/s  (1.033s,  991.76/s)  LR: 9.460e-04  Data: 0.011 (0.026)
Train: 45 [ 150/1251 ( 12%)]  Loss: 4.287 (4.05)  Time: 1.000s, 1023.59/s  (1.021s, 1002.62/s)  LR: 9.460e-04  Data: 0.012 (0.021)
Train: 45 [ 200/1251 ( 16%)]  Loss: 4.048 (4.05)  Time: 1.050s,  975.07/s  (1.023s, 1000.64/s)  LR: 9.460e-04  Data: 0.011 (0.019)
Train: 45 [ 250/1251 ( 20%)]  Loss: 4.170 (4.07)  Time: 1.003s, 1020.91/s  (1.021s, 1002.93/s)  LR: 9.460e-04  Data: 0.012 (0.017)
Train: 45 [ 300/1251 ( 24%)]  Loss: 3.696 (4.01)  Time: 0.997s, 1027.00/s  (1.017s, 1006.78/s)  LR: 9.460e-04  Data: 0.011 (0.016)
Train: 45 [ 350/1251 ( 28%)]  Loss: 3.763 (3.98)  Time: 0.992s, 1031.84/s  (1.015s, 1009.23/s)  LR: 9.460e-04  Data: 0.011 (0.015)
Train: 45 [ 400/1251 ( 32%)]  Loss: 4.198 (4.01)  Time: 0.998s, 1026.57/s  (1.013s, 1011.26/s)  LR: 9.460e-04  Data: 0.011 (0.015)
Train: 45 [ 450/1251 ( 36%)]  Loss: 3.832 (3.99)  Time: 1.058s,  967.78/s  (1.011s, 1012.50/s)  LR: 9.460e-04  Data: 0.011 (0.014)
Train: 45 [ 500/1251 ( 40%)]  Loss: 3.558 (3.95)  Time: 1.037s,  987.42/s  (1.012s, 1011.97/s)  LR: 9.460e-04  Data: 0.011 (0.014)
Train: 45 [ 550/1251 ( 44%)]  Loss: 3.903 (3.95)  Time: 0.993s, 1031.02/s  (1.012s, 1012.27/s)  LR: 9.460e-04  Data: 0.010 (0.014)
Train: 45 [ 600/1251 ( 48%)]  Loss: 3.877 (3.94)  Time: 0.994s, 1029.88/s  (1.011s, 1012.96/s)  LR: 9.460e-04  Data: 0.011 (0.014)
Train: 45 [ 650/1251 ( 52%)]  Loss: 3.597 (3.92)  Time: 0.994s, 1029.94/s  (1.011s, 1013.20/s)  LR: 9.460e-04  Data: 0.011 (0.013)
Train: 45 [ 700/1251 ( 56%)]  Loss: 4.058 (3.93)  Time: 0.992s, 1032.34/s  (1.011s, 1012.79/s)  LR: 9.460e-04  Data: 0.010 (0.013)
Train: 45 [ 750/1251 ( 60%)]  Loss: 3.875 (3.92)  Time: 0.998s, 1025.71/s  (1.010s, 1013.68/s)  LR: 9.460e-04  Data: 0.012 (0.013)
Train: 45 [ 800/1251 ( 64%)]  Loss: 4.149 (3.94)  Time: 0.994s, 1030.36/s  (1.010s, 1014.06/s)  LR: 9.460e-04  Data: 0.011 (0.013)
Train: 45 [ 850/1251 ( 68%)]  Loss: 4.188 (3.95)  Time: 0.995s, 1029.47/s  (1.009s, 1014.67/s)  LR: 9.460e-04  Data: 0.010 (0.013)
Train: 45 [ 900/1251 ( 72%)]  Loss: 3.656 (3.93)  Time: 0.995s, 1029.00/s  (1.010s, 1013.52/s)  LR: 9.460e-04  Data: 0.011 (0.013)
Train: 45 [ 950/1251 ( 76%)]  Loss: 4.274 (3.95)  Time: 1.038s,  986.20/s  (1.011s, 1012.96/s)  LR: 9.460e-04  Data: 0.011 (0.013)
Train: 45 [1000/1251 ( 80%)]  Loss: 3.462 (3.93)  Time: 0.997s, 1026.86/s  (1.010s, 1013.66/s)  LR: 9.460e-04  Data: 0.010 (0.013)
Train: 45 [1050/1251 ( 84%)]  Loss: 3.898 (3.93)  Time: 1.039s,  985.97/s  (1.010s, 1014.14/s)  LR: 9.460e-04  Data: 0.011 (0.013)
Train: 45 [1100/1251 ( 88%)]  Loss: 4.016 (3.93)  Time: 1.000s, 1023.79/s  (1.010s, 1014.02/s)  LR: 9.460e-04  Data: 0.011 (0.012)
Train: 45 [1150/1251 ( 92%)]  Loss: 4.264 (3.94)  Time: 0.996s, 1028.23/s  (1.009s, 1014.49/s)  LR: 9.460e-04  Data: 0.011 (0.012)
Train: 45 [1200/1251 ( 96%)]  Loss: 4.184 (3.95)  Time: 0.998s, 1026.57/s  (1.009s, 1014.78/s)  LR: 9.460e-04  Data: 0.012 (0.012)
Train: 45 [1250/1251 (100%)]  Loss: 4.192 (3.96)  Time: 1.023s, 1001.05/s  (1.009s, 1014.76/s)  LR: 9.460e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.644 (1.644)  Loss:  1.0675 (1.0675)  Acc@1: 84.3750 (84.3750)  Acc@5: 96.2891 (96.2891)
Test: [  48/48]  Time: 0.245 (0.571)  Loss:  1.0218 (1.6093)  Acc@1: 82.5472 (68.1960)  Acc@5: 93.6321 (88.8200)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-43.pth.tar', 68.34800007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-42.pth.tar', 68.33999994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-38.pth.tar', 68.32599997314453)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-45.pth.tar', 68.19599991699219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-44.pth.tar', 68.03800004882812)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-39.pth.tar', 67.88400010009765)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-41.pth.tar', 67.82400002441406)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-40.pth.tar', 67.62999990722656)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-37.pth.tar', 67.46200012939453)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-35.pth.tar', 67.37200002929687)

Train: 46 [   0/1251 (  0%)]  Loss: 4.059 (4.06)  Time: 2.503s,  409.16/s  (2.503s,  409.16/s)  LR: 9.437e-04  Data: 1.550 (1.550)
Train: 46 [  50/1251 (  4%)]  Loss: 3.941 (4.00)  Time: 0.997s, 1027.50/s  (1.040s,  984.76/s)  LR: 9.437e-04  Data: 0.010 (0.041)
Train: 46 [ 100/1251 (  8%)]  Loss: 4.030 (4.01)  Time: 0.997s, 1027.33/s  (1.022s, 1001.71/s)  LR: 9.437e-04  Data: 0.011 (0.026)
Train: 46 [ 150/1251 ( 12%)]  Loss: 4.019 (4.01)  Time: 0.995s, 1029.06/s  (1.015s, 1008.87/s)  LR: 9.437e-04  Data: 0.011 (0.021)
Train: 46 [ 200/1251 ( 16%)]  Loss: 3.537 (3.92)  Time: 1.000s, 1024.05/s  (1.011s, 1012.86/s)  LR: 9.437e-04  Data: 0.011 (0.019)
Train: 46 [ 250/1251 ( 20%)]  Loss: 3.955 (3.92)  Time: 0.996s, 1027.93/s  (1.008s, 1015.39/s)  LR: 9.437e-04  Data: 0.011 (0.017)
Train: 46 [ 300/1251 ( 24%)]  Loss: 3.657 (3.89)  Time: 0.996s, 1028.36/s  (1.008s, 1016.11/s)  LR: 9.437e-04  Data: 0.012 (0.016)
Train: 46 [ 350/1251 ( 28%)]  Loss: 3.898 (3.89)  Time: 0.994s, 1030.31/s  (1.007s, 1017.01/s)  LR: 9.437e-04  Data: 0.010 (0.015)
Train: 46 [ 400/1251 ( 32%)]  Loss: 4.112 (3.91)  Time: 0.996s, 1028.58/s  (1.006s, 1017.48/s)  LR: 9.437e-04  Data: 0.011 (0.015)
Train: 46 [ 450/1251 ( 36%)]  Loss: 3.677 (3.89)  Time: 1.067s,  959.43/s  (1.007s, 1017.18/s)  LR: 9.437e-04  Data: 0.011 (0.014)
Train: 46 [ 500/1251 ( 40%)]  Loss: 4.045 (3.90)  Time: 0.995s, 1029.22/s  (1.008s, 1015.88/s)  LR: 9.437e-04  Data: 0.011 (0.014)
Train: 46 [ 550/1251 ( 44%)]  Loss: 3.775 (3.89)  Time: 0.994s, 1030.40/s  (1.007s, 1016.60/s)  LR: 9.437e-04  Data: 0.011 (0.014)
Train: 46 [ 600/1251 ( 48%)]  Loss: 4.228 (3.92)  Time: 1.035s,  989.36/s  (1.007s, 1016.80/s)  LR: 9.437e-04  Data: 0.012 (0.014)
Train: 46 [ 650/1251 ( 52%)]  Loss: 3.876 (3.91)  Time: 0.995s, 1029.01/s  (1.006s, 1017.43/s)  LR: 9.437e-04  Data: 0.011 (0.013)
Train: 46 [ 700/1251 ( 56%)]  Loss: 3.907 (3.91)  Time: 0.997s, 1027.00/s  (1.006s, 1017.99/s)  LR: 9.437e-04  Data: 0.012 (0.013)
Train: 46 [ 750/1251 ( 60%)]  Loss: 3.800 (3.91)  Time: 1.040s,  984.66/s  (1.007s, 1017.24/s)  LR: 9.437e-04  Data: 0.011 (0.013)
Train: 46 [ 800/1251 ( 64%)]  Loss: 4.434 (3.94)  Time: 0.990s, 1033.88/s  (1.007s, 1016.94/s)  LR: 9.437e-04  Data: 0.011 (0.013)
Train: 46 [ 850/1251 ( 68%)]  Loss: 3.785 (3.93)  Time: 1.063s,  963.64/s  (1.009s, 1015.23/s)  LR: 9.437e-04  Data: 0.013 (0.013)
Train: 46 [ 900/1251 ( 72%)]  Loss: 4.282 (3.95)  Time: 1.032s,  992.21/s  (1.009s, 1015.36/s)  LR: 9.437e-04  Data: 0.011 (0.013)
Train: 46 [ 950/1251 ( 76%)]  Loss: 3.606 (3.93)  Time: 1.051s,  974.15/s  (1.008s, 1015.47/s)  LR: 9.437e-04  Data: 0.012 (0.013)
Train: 46 [1000/1251 ( 80%)]  Loss: 3.842 (3.93)  Time: 0.997s, 1027.04/s  (1.010s, 1013.77/s)  LR: 9.437e-04  Data: 0.012 (0.013)
Train: 46 [1050/1251 ( 84%)]  Loss: 3.991 (3.93)  Time: 1.079s,  949.35/s  (1.010s, 1013.89/s)  LR: 9.437e-04  Data: 0.012 (0.013)
Train: 46 [1100/1251 ( 88%)]  Loss: 4.008 (3.93)  Time: 0.996s, 1028.17/s  (1.011s, 1012.90/s)  LR: 9.437e-04  Data: 0.010 (0.013)
Train: 46 [1150/1251 ( 92%)]  Loss: 3.831 (3.93)  Time: 0.994s, 1029.67/s  (1.011s, 1012.83/s)  LR: 9.437e-04  Data: 0.011 (0.013)
Train: 46 [1200/1251 ( 96%)]  Loss: 3.726 (3.92)  Time: 0.998s, 1025.87/s  (1.011s, 1013.13/s)  LR: 9.437e-04  Data: 0.011 (0.012)
Train: 46 [1250/1251 (100%)]  Loss: 3.645 (3.91)  Time: 1.021s, 1003.11/s  (1.011s, 1013.33/s)  LR: 9.437e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.609 (1.609)  Loss:  0.9112 (0.9112)  Acc@1: 87.1094 (87.1094)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.245 (0.567)  Loss:  1.0094 (1.5644)  Acc@1: 83.4906 (68.6700)  Acc@5: 95.1651 (89.1340)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-46.pth.tar', 68.66999993896485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-43.pth.tar', 68.34800007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-42.pth.tar', 68.33999994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-38.pth.tar', 68.32599997314453)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-45.pth.tar', 68.19599991699219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-44.pth.tar', 68.03800004882812)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-39.pth.tar', 67.88400010009765)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-41.pth.tar', 67.82400002441406)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-40.pth.tar', 67.62999990722656)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-37.pth.tar', 67.46200012939453)

Train: 47 [   0/1251 (  0%)]  Loss: 4.151 (4.15)  Time: 2.489s,  411.42/s  (2.489s,  411.42/s)  LR: 9.412e-04  Data: 1.529 (1.529)
Train: 47 [  50/1251 (  4%)]  Loss: 4.162 (4.16)  Time: 1.007s, 1016.68/s  (1.041s,  984.13/s)  LR: 9.412e-04  Data: 0.011 (0.041)
Train: 47 [ 100/1251 (  8%)]  Loss: 3.633 (3.98)  Time: 0.999s, 1024.73/s  (1.021s, 1003.41/s)  LR: 9.412e-04  Data: 0.011 (0.026)
Train: 47 [ 150/1251 ( 12%)]  Loss: 3.969 (3.98)  Time: 1.003s, 1020.99/s  (1.014s, 1009.96/s)  LR: 9.412e-04  Data: 0.011 (0.021)
Train: 47 [ 200/1251 ( 16%)]  Loss: 3.857 (3.95)  Time: 0.998s, 1026.00/s  (1.010s, 1013.60/s)  LR: 9.412e-04  Data: 0.012 (0.019)
Train: 47 [ 250/1251 ( 20%)]  Loss: 3.644 (3.90)  Time: 0.994s, 1030.55/s  (1.009s, 1014.64/s)  LR: 9.412e-04  Data: 0.011 (0.017)
Train: 47 [ 300/1251 ( 24%)]  Loss: 3.891 (3.90)  Time: 0.995s, 1029.32/s  (1.008s, 1016.19/s)  LR: 9.412e-04  Data: 0.011 (0.016)
Train: 47 [ 350/1251 ( 28%)]  Loss: 3.989 (3.91)  Time: 0.999s, 1025.03/s  (1.007s, 1017.36/s)  LR: 9.412e-04  Data: 0.011 (0.015)
Train: 47 [ 400/1251 ( 32%)]  Loss: 3.927 (3.91)  Time: 0.998s, 1026.30/s  (1.007s, 1017.19/s)  LR: 9.412e-04  Data: 0.012 (0.015)
Train: 47 [ 450/1251 ( 36%)]  Loss: 4.223 (3.94)  Time: 1.001s, 1023.05/s  (1.007s, 1016.52/s)  LR: 9.412e-04  Data: 0.011 (0.014)
Train: 47 [ 500/1251 ( 40%)]  Loss: 4.339 (3.98)  Time: 1.001s, 1022.80/s  (1.007s, 1016.99/s)  LR: 9.412e-04  Data: 0.012 (0.014)
Train: 47 [ 550/1251 ( 44%)]  Loss: 3.770 (3.96)  Time: 0.995s, 1029.13/s  (1.006s, 1017.39/s)  LR: 9.412e-04  Data: 0.010 (0.014)
Train: 47 [ 600/1251 ( 48%)]  Loss: 4.392 (4.00)  Time: 0.995s, 1029.35/s  (1.007s, 1017.32/s)  LR: 9.412e-04  Data: 0.011 (0.014)
Train: 47 [ 650/1251 ( 52%)]  Loss: 3.449 (3.96)  Time: 1.004s, 1020.38/s  (1.006s, 1017.86/s)  LR: 9.412e-04  Data: 0.011 (0.013)
Train: 47 [ 700/1251 ( 56%)]  Loss: 3.882 (3.95)  Time: 0.995s, 1028.80/s  (1.006s, 1018.14/s)  LR: 9.412e-04  Data: 0.010 (0.013)
Train: 47 [ 750/1251 ( 60%)]  Loss: 3.634 (3.93)  Time: 0.996s, 1028.28/s  (1.006s, 1018.29/s)  LR: 9.412e-04  Data: 0.012 (0.013)
Train: 47 [ 800/1251 ( 64%)]  Loss: 4.043 (3.94)  Time: 0.997s, 1027.06/s  (1.005s, 1018.73/s)  LR: 9.412e-04  Data: 0.011 (0.013)
Train: 47 [ 850/1251 ( 68%)]  Loss: 4.064 (3.95)  Time: 0.994s, 1030.69/s  (1.005s, 1018.59/s)  LR: 9.412e-04  Data: 0.011 (0.013)
Train: 47 [ 900/1251 ( 72%)]  Loss: 4.264 (3.96)  Time: 1.004s, 1020.22/s  (1.005s, 1019.02/s)  LR: 9.412e-04  Data: 0.013 (0.013)
Train: 47 [ 950/1251 ( 76%)]  Loss: 3.833 (3.96)  Time: 0.996s, 1028.42/s  (1.005s, 1019.19/s)  LR: 9.412e-04  Data: 0.011 (0.013)
Train: 47 [1000/1251 ( 80%)]  Loss: 3.886 (3.95)  Time: 1.032s,  991.96/s  (1.005s, 1018.72/s)  LR: 9.412e-04  Data: 0.012 (0.013)
Train: 47 [1050/1251 ( 84%)]  Loss: 4.070 (3.96)  Time: 0.997s, 1027.31/s  (1.005s, 1018.82/s)  LR: 9.412e-04  Data: 0.012 (0.013)
Train: 47 [1100/1251 ( 88%)]  Loss: 3.781 (3.95)  Time: 0.996s, 1028.46/s  (1.005s, 1019.04/s)  LR: 9.412e-04  Data: 0.012 (0.012)
Train: 47 [1150/1251 ( 92%)]  Loss: 3.748 (3.94)  Time: 0.997s, 1027.55/s  (1.005s, 1019.08/s)  LR: 9.412e-04  Data: 0.010 (0.012)
Train: 47 [1200/1251 ( 96%)]  Loss: 3.916 (3.94)  Time: 0.995s, 1028.85/s  (1.005s, 1018.51/s)  LR: 9.412e-04  Data: 0.010 (0.012)
Train: 47 [1250/1251 (100%)]  Loss: 3.843 (3.94)  Time: 0.982s, 1042.73/s  (1.005s, 1018.81/s)  LR: 9.412e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.598 (1.598)  Loss:  0.9011 (0.9011)  Acc@1: 85.5469 (85.5469)  Acc@5: 95.5078 (95.5078)
Test: [  48/48]  Time: 0.245 (0.567)  Loss:  1.0273 (1.5110)  Acc@1: 82.0755 (68.6320)  Acc@5: 93.3962 (88.8140)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-46.pth.tar', 68.66999993896485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-47.pth.tar', 68.63200010009766)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-43.pth.tar', 68.34800007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-42.pth.tar', 68.33999994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-38.pth.tar', 68.32599997314453)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-45.pth.tar', 68.19599991699219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-44.pth.tar', 68.03800004882812)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-39.pth.tar', 67.88400010009765)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-41.pth.tar', 67.82400002441406)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-40.pth.tar', 67.62999990722656)

Train: 48 [   0/1251 (  0%)]  Loss: 4.125 (4.12)  Time: 4.393s,  233.10/s  (4.393s,  233.10/s)  LR: 9.388e-04  Data: 3.159 (3.159)
Train: 48 [  50/1251 (  4%)]  Loss: 3.638 (3.88)  Time: 0.995s, 1029.46/s  (1.084s,  944.72/s)  LR: 9.388e-04  Data: 0.011 (0.073)
Train: 48 [ 100/1251 (  8%)]  Loss: 4.043 (3.94)  Time: 0.994s, 1030.60/s  (1.042s,  982.27/s)  LR: 9.388e-04  Data: 0.011 (0.042)
Train: 48 [ 150/1251 ( 12%)]  Loss: 4.270 (4.02)  Time: 0.995s, 1029.00/s  (1.034s,  990.70/s)  LR: 9.388e-04  Data: 0.010 (0.032)
Train: 48 [ 200/1251 ( 16%)]  Loss: 3.865 (3.99)  Time: 0.997s, 1027.50/s  (1.026s,  997.84/s)  LR: 9.388e-04  Data: 0.011 (0.027)
Train: 48 [ 250/1251 ( 20%)]  Loss: 4.197 (4.02)  Time: 0.996s, 1028.09/s  (1.021s, 1002.76/s)  LR: 9.388e-04  Data: 0.011 (0.024)
Train: 48 [ 300/1251 ( 24%)]  Loss: 4.071 (4.03)  Time: 0.994s, 1030.48/s  (1.018s, 1005.86/s)  LR: 9.388e-04  Data: 0.011 (0.022)
Train: 48 [ 350/1251 ( 28%)]  Loss: 3.891 (4.01)  Time: 1.000s, 1023.93/s  (1.016s, 1007.75/s)  LR: 9.388e-04  Data: 0.011 (0.020)
Train: 48 [ 400/1251 ( 32%)]  Loss: 4.388 (4.05)  Time: 1.042s,  982.69/s  (1.015s, 1009.10/s)  LR: 9.388e-04  Data: 0.012 (0.019)
Train: 48 [ 450/1251 ( 36%)]  Loss: 3.888 (4.04)  Time: 0.992s, 1031.87/s  (1.015s, 1009.26/s)  LR: 9.388e-04  Data: 0.010 (0.018)
Train: 48 [ 500/1251 ( 40%)]  Loss: 4.212 (4.05)  Time: 0.994s, 1030.54/s  (1.014s, 1009.89/s)  LR: 9.388e-04  Data: 0.011 (0.017)
Train: 48 [ 550/1251 ( 44%)]  Loss: 4.379 (4.08)  Time: 0.997s, 1027.47/s  (1.012s, 1011.47/s)  LR: 9.388e-04  Data: 0.010 (0.017)
Train: 48 [ 600/1251 ( 48%)]  Loss: 3.658 (4.05)  Time: 0.998s, 1026.53/s  (1.011s, 1012.61/s)  LR: 9.388e-04  Data: 0.011 (0.016)
Train: 48 [ 650/1251 ( 52%)]  Loss: 3.788 (4.03)  Time: 0.997s, 1026.83/s  (1.010s, 1013.37/s)  LR: 9.388e-04  Data: 0.011 (0.016)
Train: 48 [ 700/1251 ( 56%)]  Loss: 3.738 (4.01)  Time: 1.013s, 1011.31/s  (1.010s, 1013.94/s)  LR: 9.388e-04  Data: 0.011 (0.016)
Train: 48 [ 750/1251 ( 60%)]  Loss: 4.004 (4.01)  Time: 1.017s, 1007.37/s  (1.010s, 1013.82/s)  LR: 9.388e-04  Data: 0.011 (0.015)
Train: 48 [ 800/1251 ( 64%)]  Loss: 3.943 (4.01)  Time: 0.998s, 1025.60/s  (1.009s, 1014.47/s)  LR: 9.388e-04  Data: 0.011 (0.015)
Train: 48 [ 850/1251 ( 68%)]  Loss: 4.253 (4.02)  Time: 0.997s, 1026.85/s  (1.009s, 1015.28/s)  LR: 9.388e-04  Data: 0.012 (0.015)
Train: 48 [ 900/1251 ( 72%)]  Loss: 4.080 (4.02)  Time: 0.995s, 1028.73/s  (1.008s, 1015.77/s)  LR: 9.388e-04  Data: 0.011 (0.015)
Train: 48 [ 950/1251 ( 76%)]  Loss: 4.110 (4.03)  Time: 1.035s,  989.64/s  (1.008s, 1016.15/s)  LR: 9.388e-04  Data: 0.012 (0.014)
Train: 48 [1000/1251 ( 80%)]  Loss: 3.830 (4.02)  Time: 0.994s, 1030.12/s  (1.008s, 1015.91/s)  LR: 9.388e-04  Data: 0.011 (0.014)
Train: 48 [1050/1251 ( 84%)]  Loss: 3.942 (4.01)  Time: 0.992s, 1032.76/s  (1.009s, 1015.04/s)  LR: 9.388e-04  Data: 0.011 (0.014)
Train: 48 [1100/1251 ( 88%)]  Loss: 4.194 (4.02)  Time: 0.995s, 1028.83/s  (1.008s, 1015.53/s)  LR: 9.388e-04  Data: 0.011 (0.014)
Train: 48 [1150/1251 ( 92%)]  Loss: 3.710 (4.01)  Time: 0.997s, 1027.56/s  (1.008s, 1015.96/s)  LR: 9.388e-04  Data: 0.011 (0.014)
Train: 48 [1200/1251 ( 96%)]  Loss: 3.900 (4.00)  Time: 1.088s,  941.59/s  (1.008s, 1015.70/s)  LR: 9.388e-04  Data: 0.011 (0.014)
Train: 48 [1250/1251 (100%)]  Loss: 4.183 (4.01)  Time: 1.021s, 1002.59/s  (1.009s, 1015.26/s)  LR: 9.388e-04  Data: 0.000 (0.014)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.598 (1.598)  Loss:  1.0505 (1.0505)  Acc@1: 85.0586 (85.0586)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.245 (0.570)  Loss:  1.0662 (1.6572)  Acc@1: 81.8396 (67.9940)  Acc@5: 95.0472 (88.7120)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-46.pth.tar', 68.66999993896485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-47.pth.tar', 68.63200010009766)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-43.pth.tar', 68.34800007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-42.pth.tar', 68.33999994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-38.pth.tar', 68.32599997314453)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-45.pth.tar', 68.19599991699219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-44.pth.tar', 68.03800004882812)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-48.pth.tar', 67.99399986816407)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-39.pth.tar', 67.88400010009765)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-41.pth.tar', 67.82400002441406)

Train: 49 [   0/1251 (  0%)]  Loss: 3.597 (3.60)  Time: 2.442s,  419.35/s  (2.442s,  419.35/s)  LR: 9.363e-04  Data: 1.481 (1.481)
Train: 49 [  50/1251 (  4%)]  Loss: 3.742 (3.67)  Time: 0.996s, 1028.11/s  (1.027s,  996.84/s)  LR: 9.363e-04  Data: 0.011 (0.040)
Train: 49 [ 100/1251 (  8%)]  Loss: 4.189 (3.84)  Time: 0.996s, 1028.50/s  (1.014s, 1009.45/s)  LR: 9.363e-04  Data: 0.011 (0.026)
Train: 49 [ 150/1251 ( 12%)]  Loss: 3.594 (3.78)  Time: 0.997s, 1026.74/s  (1.014s, 1010.36/s)  LR: 9.363e-04  Data: 0.011 (0.021)
Train: 49 [ 200/1251 ( 16%)]  Loss: 4.028 (3.83)  Time: 0.996s, 1028.38/s  (1.010s, 1014.34/s)  LR: 9.363e-04  Data: 0.010 (0.018)
Train: 49 [ 250/1251 ( 20%)]  Loss: 4.336 (3.91)  Time: 0.997s, 1026.85/s  (1.010s, 1014.15/s)  LR: 9.363e-04  Data: 0.012 (0.017)
Train: 49 [ 300/1251 ( 24%)]  Loss: 4.169 (3.95)  Time: 0.996s, 1027.68/s  (1.008s, 1015.89/s)  LR: 9.363e-04  Data: 0.011 (0.016)
Train: 49 [ 350/1251 ( 28%)]  Loss: 4.286 (3.99)  Time: 0.997s, 1027.47/s  (1.007s, 1016.88/s)  LR: 9.363e-04  Data: 0.011 (0.015)
Train: 49 [ 400/1251 ( 32%)]  Loss: 3.859 (3.98)  Time: 1.015s, 1008.50/s  (1.006s, 1017.60/s)  LR: 9.363e-04  Data: 0.010 (0.015)
Train: 49 [ 450/1251 ( 36%)]  Loss: 3.848 (3.96)  Time: 1.022s, 1001.59/s  (1.007s, 1016.55/s)  LR: 9.363e-04  Data: 0.010 (0.014)
Train: 49 [ 500/1251 ( 40%)]  Loss: 3.795 (3.95)  Time: 0.995s, 1029.48/s  (1.007s, 1016.97/s)  LR: 9.363e-04  Data: 0.011 (0.014)
Train: 49 [ 550/1251 ( 44%)]  Loss: 4.397 (3.99)  Time: 0.994s, 1030.12/s  (1.006s, 1017.89/s)  LR: 9.363e-04  Data: 0.010 (0.014)
Train: 49 [ 600/1251 ( 48%)]  Loss: 3.585 (3.96)  Time: 0.994s, 1030.63/s  (1.006s, 1018.36/s)  LR: 9.363e-04  Data: 0.012 (0.013)
Train: 49 [ 650/1251 ( 52%)]  Loss: 4.016 (3.96)  Time: 1.031s,  993.02/s  (1.006s, 1018.30/s)  LR: 9.363e-04  Data: 0.011 (0.013)
Train: 49 [ 700/1251 ( 56%)]  Loss: 4.029 (3.96)  Time: 0.998s, 1026.41/s  (1.006s, 1018.04/s)  LR: 9.363e-04  Data: 0.011 (0.013)
Train: 49 [ 750/1251 ( 60%)]  Loss: 4.095 (3.97)  Time: 0.996s, 1028.28/s  (1.006s, 1018.21/s)  LR: 9.363e-04  Data: 0.011 (0.013)
Train: 49 [ 800/1251 ( 64%)]  Loss: 3.928 (3.97)  Time: 0.996s, 1027.83/s  (1.005s, 1018.67/s)  LR: 9.363e-04  Data: 0.010 (0.013)
Train: 49 [ 850/1251 ( 68%)]  Loss: 4.125 (3.98)  Time: 0.993s, 1031.10/s  (1.005s, 1018.95/s)  LR: 9.363e-04  Data: 0.011 (0.013)
Train: 49 [ 900/1251 ( 72%)]  Loss: 3.948 (3.98)  Time: 0.997s, 1027.55/s  (1.005s, 1019.01/s)  LR: 9.363e-04  Data: 0.011 (0.013)
Train: 49 [ 950/1251 ( 76%)]  Loss: 3.722 (3.96)  Time: 0.996s, 1027.83/s  (1.005s, 1019.36/s)  LR: 9.363e-04  Data: 0.011 (0.013)
Train: 49 [1000/1251 ( 80%)]  Loss: 3.975 (3.96)  Time: 0.994s, 1030.12/s  (1.004s, 1019.56/s)  LR: 9.363e-04  Data: 0.011 (0.013)
Train: 49 [1050/1251 ( 84%)]  Loss: 4.186 (3.97)  Time: 0.993s, 1031.73/s  (1.004s, 1019.69/s)  LR: 9.363e-04  Data: 0.010 (0.012)
Train: 49 [1100/1251 ( 88%)]  Loss: 3.939 (3.97)  Time: 1.035s,  988.96/s  (1.004s, 1019.71/s)  LR: 9.363e-04  Data: 0.011 (0.012)
Train: 49 [1150/1251 ( 92%)]  Loss: 3.731 (3.96)  Time: 0.996s, 1027.61/s  (1.004s, 1019.72/s)  LR: 9.363e-04  Data: 0.011 (0.012)
Train: 49 [1200/1251 ( 96%)]  Loss: 4.031 (3.97)  Time: 0.996s, 1028.44/s  (1.004s, 1019.87/s)  LR: 9.363e-04  Data: 0.011 (0.012)
Train: 49 [1250/1251 (100%)]  Loss: 3.785 (3.96)  Time: 1.033s,  991.32/s  (1.004s, 1019.56/s)  LR: 9.363e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.675 (1.675)  Loss:  1.0188 (1.0188)  Acc@1: 85.6445 (85.6445)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.245 (0.571)  Loss:  1.0281 (1.6036)  Acc@1: 80.3066 (68.6980)  Acc@5: 94.5755 (89.1220)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-49.pth.tar', 68.69800010742188)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-46.pth.tar', 68.66999993896485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-47.pth.tar', 68.63200010009766)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-43.pth.tar', 68.34800007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-42.pth.tar', 68.33999994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-38.pth.tar', 68.32599997314453)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-45.pth.tar', 68.19599991699219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-44.pth.tar', 68.03800004882812)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-48.pth.tar', 67.99399986816407)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-39.pth.tar', 67.88400010009765)

Train: 50 [   0/1251 (  0%)]  Loss: 3.924 (3.92)  Time: 2.560s,  399.92/s  (2.560s,  399.92/s)  LR: 9.337e-04  Data: 1.596 (1.596)
Train: 50 [  50/1251 (  4%)]  Loss: 3.857 (3.89)  Time: 1.007s, 1017.06/s  (1.043s,  981.73/s)  LR: 9.337e-04  Data: 0.012 (0.042)
Train: 50 [ 100/1251 (  8%)]  Loss: 4.320 (4.03)  Time: 1.004s, 1019.73/s  (1.022s, 1001.73/s)  LR: 9.337e-04  Data: 0.011 (0.027)
Train: 50 [ 150/1251 ( 12%)]  Loss: 4.138 (4.06)  Time: 0.994s, 1029.70/s  (1.014s, 1009.50/s)  LR: 9.337e-04  Data: 0.011 (0.022)
Train: 50 [ 200/1251 ( 16%)]  Loss: 3.912 (4.03)  Time: 0.999s, 1024.81/s  (1.011s, 1013.05/s)  LR: 9.337e-04  Data: 0.011 (0.019)
Train: 50 [ 250/1251 ( 20%)]  Loss: 3.546 (3.95)  Time: 0.998s, 1025.93/s  (1.009s, 1014.73/s)  LR: 9.337e-04  Data: 0.010 (0.017)
Train: 50 [ 300/1251 ( 24%)]  Loss: 4.239 (3.99)  Time: 1.001s, 1022.55/s  (1.008s, 1015.76/s)  LR: 9.337e-04  Data: 0.013 (0.016)
Train: 50 [ 350/1251 ( 28%)]  Loss: 3.811 (3.97)  Time: 0.992s, 1031.80/s  (1.006s, 1017.49/s)  LR: 9.337e-04  Data: 0.010 (0.016)
Train: 50 [ 400/1251 ( 32%)]  Loss: 3.862 (3.96)  Time: 1.026s,  997.71/s  (1.006s, 1017.41/s)  LR: 9.337e-04  Data: 0.010 (0.015)
Train: 50 [ 450/1251 ( 36%)]  Loss: 3.808 (3.94)  Time: 1.053s,  972.06/s  (1.007s, 1016.57/s)  LR: 9.337e-04  Data: 0.010 (0.015)
Train: 50 [ 500/1251 ( 40%)]  Loss: 4.191 (3.96)  Time: 1.034s,  990.62/s  (1.007s, 1016.89/s)  LR: 9.337e-04  Data: 0.011 (0.014)
Train: 50 [ 550/1251 ( 44%)]  Loss: 3.685 (3.94)  Time: 0.994s, 1030.57/s  (1.007s, 1017.30/s)  LR: 9.337e-04  Data: 0.011 (0.014)
Train: 50 [ 600/1251 ( 48%)]  Loss: 3.756 (3.93)  Time: 1.000s, 1023.82/s  (1.006s, 1017.99/s)  LR: 9.337e-04  Data: 0.012 (0.014)
Train: 50 [ 650/1251 ( 52%)]  Loss: 3.717 (3.91)  Time: 0.994s, 1029.93/s  (1.006s, 1018.39/s)  LR: 9.337e-04  Data: 0.011 (0.013)
Train: 50 [ 700/1251 ( 56%)]  Loss: 3.661 (3.90)  Time: 0.996s, 1028.42/s  (1.005s, 1018.81/s)  LR: 9.337e-04  Data: 0.011 (0.013)
Train: 50 [ 750/1251 ( 60%)]  Loss: 4.185 (3.91)  Time: 1.000s, 1024.22/s  (1.005s, 1018.69/s)  LR: 9.337e-04  Data: 0.011 (0.013)
Train: 50 [ 800/1251 ( 64%)]  Loss: 3.728 (3.90)  Time: 0.994s, 1030.35/s  (1.005s, 1018.91/s)  LR: 9.337e-04  Data: 0.011 (0.013)
Train: 50 [ 850/1251 ( 68%)]  Loss: 3.616 (3.89)  Time: 1.065s,  961.22/s  (1.006s, 1017.60/s)  LR: 9.337e-04  Data: 0.012 (0.013)
Train: 50 [ 900/1251 ( 72%)]  Loss: 4.258 (3.91)  Time: 0.996s, 1027.91/s  (1.006s, 1017.56/s)  LR: 9.337e-04  Data: 0.011 (0.013)
Train: 50 [ 950/1251 ( 76%)]  Loss: 3.975 (3.91)  Time: 0.995s, 1028.96/s  (1.006s, 1017.89/s)  LR: 9.337e-04  Data: 0.011 (0.013)
Train: 50 [1000/1251 ( 80%)]  Loss: 3.673 (3.90)  Time: 0.997s, 1027.44/s  (1.006s, 1018.37/s)  LR: 9.337e-04  Data: 0.012 (0.013)
Train: 50 [1050/1251 ( 84%)]  Loss: 3.781 (3.89)  Time: 0.997s, 1027.18/s  (1.005s, 1018.64/s)  LR: 9.337e-04  Data: 0.011 (0.013)
Train: 50 [1100/1251 ( 88%)]  Loss: 4.033 (3.90)  Time: 0.996s, 1027.66/s  (1.005s, 1018.91/s)  LR: 9.337e-04  Data: 0.011 (0.012)
Train: 50 [1150/1251 ( 92%)]  Loss: 3.800 (3.89)  Time: 0.999s, 1025.23/s  (1.005s, 1019.07/s)  LR: 9.337e-04  Data: 0.011 (0.012)
Train: 50 [1200/1251 ( 96%)]  Loss: 3.828 (3.89)  Time: 0.996s, 1028.58/s  (1.005s, 1019.19/s)  LR: 9.337e-04  Data: 0.012 (0.012)
Train: 50 [1250/1251 (100%)]  Loss: 3.904 (3.89)  Time: 0.981s, 1043.51/s  (1.005s, 1019.35/s)  LR: 9.337e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.755 (1.755)  Loss:  1.0793 (1.0793)  Acc@1: 86.4258 (86.4258)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.245 (0.570)  Loss:  1.1073 (1.6516)  Acc@1: 79.8349 (69.0340)  Acc@5: 94.5755 (89.5160)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-50.pth.tar', 69.03400003173829)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-49.pth.tar', 68.69800010742188)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-46.pth.tar', 68.66999993896485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-47.pth.tar', 68.63200010009766)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-43.pth.tar', 68.34800007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-42.pth.tar', 68.33999994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-38.pth.tar', 68.32599997314453)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-45.pth.tar', 68.19599991699219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-44.pth.tar', 68.03800004882812)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-48.pth.tar', 67.99399986816407)

Train: 51 [   0/1251 (  0%)]  Loss: 3.458 (3.46)  Time: 2.584s,  396.36/s  (2.584s,  396.36/s)  LR: 9.311e-04  Data: 1.624 (1.624)
Train: 51 [  50/1251 (  4%)]  Loss: 3.979 (3.72)  Time: 0.996s, 1028.28/s  (1.052s,  973.44/s)  LR: 9.311e-04  Data: 0.011 (0.046)
Train: 51 [ 100/1251 (  8%)]  Loss: 3.884 (3.77)  Time: 1.032s,  992.46/s  (1.044s,  981.18/s)  LR: 9.311e-04  Data: 0.012 (0.029)
Train: 51 [ 150/1251 ( 12%)]  Loss: 3.945 (3.82)  Time: 1.044s,  981.19/s  (1.034s,  990.43/s)  LR: 9.311e-04  Data: 0.012 (0.023)
Train: 51 [ 200/1251 ( 16%)]  Loss: 3.912 (3.84)  Time: 1.009s, 1014.77/s  (1.030s,  994.53/s)  LR: 9.311e-04  Data: 0.011 (0.020)
Train: 51 [ 250/1251 ( 20%)]  Loss: 4.251 (3.90)  Time: 0.998s, 1026.47/s  (1.024s, 1000.04/s)  LR: 9.311e-04  Data: 0.011 (0.018)
Train: 51 [ 300/1251 ( 24%)]  Loss: 3.791 (3.89)  Time: 0.995s, 1029.37/s  (1.020s, 1004.11/s)  LR: 9.311e-04  Data: 0.011 (0.017)
Train: 51 [ 350/1251 ( 28%)]  Loss: 4.092 (3.91)  Time: 0.996s, 1028.13/s  (1.019s, 1005.38/s)  LR: 9.311e-04  Data: 0.012 (0.016)
Train: 51 [ 400/1251 ( 32%)]  Loss: 4.125 (3.94)  Time: 0.997s, 1027.01/s  (1.017s, 1006.68/s)  LR: 9.311e-04  Data: 0.011 (0.016)
Train: 51 [ 450/1251 ( 36%)]  Loss: 3.957 (3.94)  Time: 1.000s, 1023.68/s  (1.015s, 1008.63/s)  LR: 9.311e-04  Data: 0.011 (0.015)
Train: 51 [ 500/1251 ( 40%)]  Loss: 3.896 (3.94)  Time: 0.997s, 1027.07/s  (1.015s, 1008.47/s)  LR: 9.311e-04  Data: 0.011 (0.015)
Train: 51 [ 550/1251 ( 44%)]  Loss: 4.123 (3.95)  Time: 0.995s, 1029.20/s  (1.014s, 1009.63/s)  LR: 9.311e-04  Data: 0.011 (0.014)
Train: 51 [ 600/1251 ( 48%)]  Loss: 3.773 (3.94)  Time: 1.001s, 1022.90/s  (1.015s, 1009.30/s)  LR: 9.311e-04  Data: 0.011 (0.014)
Train: 51 [ 650/1251 ( 52%)]  Loss: 4.074 (3.95)  Time: 1.001s, 1023.23/s  (1.013s, 1010.61/s)  LR: 9.311e-04  Data: 0.011 (0.014)
Train: 51 [ 700/1251 ( 56%)]  Loss: 4.270 (3.97)  Time: 1.000s, 1024.36/s  (1.013s, 1010.64/s)  LR: 9.311e-04  Data: 0.012 (0.014)
Train: 51 [ 750/1251 ( 60%)]  Loss: 3.954 (3.97)  Time: 0.995s, 1028.87/s  (1.012s, 1011.52/s)  LR: 9.311e-04  Data: 0.010 (0.013)
Train: 51 [ 800/1251 ( 64%)]  Loss: 3.837 (3.96)  Time: 0.993s, 1031.21/s  (1.012s, 1012.26/s)  LR: 9.311e-04  Data: 0.010 (0.013)
Train: 51 [ 850/1251 ( 68%)]  Loss: 4.076 (3.97)  Time: 1.000s, 1024.47/s  (1.012s, 1012.10/s)  LR: 9.311e-04  Data: 0.011 (0.013)
Train: 51 [ 900/1251 ( 72%)]  Loss: 3.694 (3.95)  Time: 1.016s, 1008.11/s  (1.012s, 1011.90/s)  LR: 9.311e-04  Data: 0.010 (0.013)
Train: 51 [ 950/1251 ( 76%)]  Loss: 3.594 (3.93)  Time: 0.996s, 1028.40/s  (1.013s, 1010.95/s)  LR: 9.311e-04  Data: 0.011 (0.013)
Train: 51 [1000/1251 ( 80%)]  Loss: 4.104 (3.94)  Time: 0.997s, 1026.66/s  (1.013s, 1010.48/s)  LR: 9.311e-04  Data: 0.012 (0.013)
Train: 51 [1050/1251 ( 84%)]  Loss: 3.683 (3.93)  Time: 1.049s,  976.00/s  (1.014s, 1009.94/s)  LR: 9.311e-04  Data: 0.010 (0.013)
Train: 51 [1100/1251 ( 88%)]  Loss: 3.921 (3.93)  Time: 0.997s, 1027.22/s  (1.015s, 1009.00/s)  LR: 9.311e-04  Data: 0.012 (0.013)
Train: 51 [1150/1251 ( 92%)]  Loss: 3.837 (3.93)  Time: 1.000s, 1024.03/s  (1.014s, 1009.38/s)  LR: 9.311e-04  Data: 0.010 (0.013)
Train: 51 [1200/1251 ( 96%)]  Loss: 3.559 (3.91)  Time: 0.994s, 1030.37/s  (1.014s, 1009.53/s)  LR: 9.311e-04  Data: 0.011 (0.013)
Train: 51 [1250/1251 (100%)]  Loss: 3.729 (3.90)  Time: 0.987s, 1037.21/s  (1.014s, 1010.05/s)  LR: 9.311e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.665 (1.665)  Loss:  0.9829 (0.9829)  Acc@1: 84.9609 (84.9609)  Acc@5: 95.6055 (95.6055)
Test: [  48/48]  Time: 0.245 (0.568)  Loss:  1.1793 (1.6087)  Acc@1: 81.9576 (68.8520)  Acc@5: 94.1038 (89.3380)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-50.pth.tar', 69.03400003173829)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-51.pth.tar', 68.85200004882813)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-49.pth.tar', 68.69800010742188)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-46.pth.tar', 68.66999993896485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-47.pth.tar', 68.63200010009766)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-43.pth.tar', 68.34800007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-42.pth.tar', 68.33999994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-38.pth.tar', 68.32599997314453)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-45.pth.tar', 68.19599991699219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-44.pth.tar', 68.03800004882812)

Train: 52 [   0/1251 (  0%)]  Loss: 3.818 (3.82)  Time: 2.515s,  407.18/s  (2.515s,  407.18/s)  LR: 9.284e-04  Data: 1.554 (1.554)
Train: 52 [  50/1251 (  4%)]  Loss: 3.724 (3.77)  Time: 1.000s, 1024.08/s  (1.038s,  986.24/s)  LR: 9.284e-04  Data: 0.010 (0.041)
Train: 52 [ 100/1251 (  8%)]  Loss: 3.905 (3.82)  Time: 0.995s, 1029.09/s  (1.019s, 1004.74/s)  LR: 9.284e-04  Data: 0.012 (0.026)
Train: 52 [ 150/1251 ( 12%)]  Loss: 4.179 (3.91)  Time: 0.998s, 1026.00/s  (1.014s, 1009.90/s)  LR: 9.284e-04  Data: 0.011 (0.021)
Train: 52 [ 200/1251 ( 16%)]  Loss: 3.715 (3.87)  Time: 1.001s, 1022.68/s  (1.011s, 1013.07/s)  LR: 9.284e-04  Data: 0.013 (0.019)
Train: 52 [ 250/1251 ( 20%)]  Loss: 3.841 (3.86)  Time: 0.997s, 1027.33/s  (1.008s, 1015.86/s)  LR: 9.284e-04  Data: 0.012 (0.017)
Train: 52 [ 300/1251 ( 24%)]  Loss: 3.591 (3.82)  Time: 1.000s, 1023.77/s  (1.007s, 1017.13/s)  LR: 9.284e-04  Data: 0.015 (0.016)
Train: 52 [ 350/1251 ( 28%)]  Loss: 3.583 (3.79)  Time: 0.995s, 1028.65/s  (1.006s, 1018.27/s)  LR: 9.284e-04  Data: 0.011 (0.015)
Train: 52 [ 400/1251 ( 32%)]  Loss: 4.043 (3.82)  Time: 0.997s, 1027.24/s  (1.005s, 1018.85/s)  LR: 9.284e-04  Data: 0.011 (0.015)
Train: 52 [ 450/1251 ( 36%)]  Loss: 3.637 (3.80)  Time: 0.998s, 1025.94/s  (1.005s, 1019.30/s)  LR: 9.284e-04  Data: 0.011 (0.014)
Train: 52 [ 500/1251 ( 40%)]  Loss: 4.208 (3.84)  Time: 1.050s,  974.93/s  (1.004s, 1019.75/s)  LR: 9.284e-04  Data: 0.011 (0.014)
Train: 52 [ 550/1251 ( 44%)]  Loss: 3.870 (3.84)  Time: 0.996s, 1027.88/s  (1.004s, 1019.90/s)  LR: 9.284e-04  Data: 0.011 (0.014)
Train: 52 [ 600/1251 ( 48%)]  Loss: 3.555 (3.82)  Time: 1.061s,  965.34/s  (1.005s, 1018.68/s)  LR: 9.284e-04  Data: 0.011 (0.014)
Train: 52 [ 650/1251 ( 52%)]  Loss: 3.911 (3.83)  Time: 1.058s,  967.89/s  (1.006s, 1017.96/s)  LR: 9.284e-04  Data: 0.012 (0.013)
Train: 52 [ 700/1251 ( 56%)]  Loss: 3.908 (3.83)  Time: 1.053s,  972.01/s  (1.010s, 1014.34/s)  LR: 9.284e-04  Data: 0.012 (0.013)
Train: 52 [ 750/1251 ( 60%)]  Loss: 3.971 (3.84)  Time: 0.997s, 1027.44/s  (1.009s, 1014.81/s)  LR: 9.284e-04  Data: 0.011 (0.013)
Train: 52 [ 800/1251 ( 64%)]  Loss: 3.865 (3.84)  Time: 1.033s,  991.68/s  (1.009s, 1014.96/s)  LR: 9.284e-04  Data: 0.011 (0.013)
Train: 52 [ 850/1251 ( 68%)]  Loss: 3.752 (3.84)  Time: 0.996s, 1028.38/s  (1.009s, 1014.86/s)  LR: 9.284e-04  Data: 0.010 (0.013)
Train: 52 [ 900/1251 ( 72%)]  Loss: 4.117 (3.85)  Time: 0.997s, 1027.39/s  (1.009s, 1015.18/s)  LR: 9.284e-04  Data: 0.012 (0.013)
Train: 52 [ 950/1251 ( 76%)]  Loss: 3.858 (3.85)  Time: 0.996s, 1027.94/s  (1.008s, 1015.42/s)  LR: 9.284e-04  Data: 0.012 (0.013)
Train: 52 [1000/1251 ( 80%)]  Loss: 3.831 (3.85)  Time: 0.994s, 1029.74/s  (1.008s, 1015.79/s)  LR: 9.284e-04  Data: 0.011 (0.013)
Train: 52 [1050/1251 ( 84%)]  Loss: 3.906 (3.85)  Time: 0.996s, 1027.72/s  (1.008s, 1015.96/s)  LR: 9.284e-04  Data: 0.011 (0.013)
Train: 52 [1100/1251 ( 88%)]  Loss: 3.945 (3.86)  Time: 0.995s, 1029.41/s  (1.008s, 1015.85/s)  LR: 9.284e-04  Data: 0.011 (0.013)
Train: 52 [1150/1251 ( 92%)]  Loss: 3.814 (3.86)  Time: 1.038s,  986.31/s  (1.008s, 1015.95/s)  LR: 9.284e-04  Data: 0.012 (0.013)
Train: 52 [1200/1251 ( 96%)]  Loss: 3.925 (3.86)  Time: 0.994s, 1030.04/s  (1.008s, 1016.05/s)  LR: 9.284e-04  Data: 0.010 (0.012)
Train: 52 [1250/1251 (100%)]  Loss: 3.390 (3.84)  Time: 0.983s, 1042.08/s  (1.007s, 1016.44/s)  LR: 9.284e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.602 (1.602)  Loss:  0.9129 (0.9129)  Acc@1: 85.3516 (85.3516)  Acc@5: 95.3125 (95.3125)
Test: [  48/48]  Time: 0.245 (0.574)  Loss:  0.9488 (1.5725)  Acc@1: 83.3726 (68.6920)  Acc@5: 95.9906 (89.1720)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-50.pth.tar', 69.03400003173829)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-51.pth.tar', 68.85200004882813)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-49.pth.tar', 68.69800010742188)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-52.pth.tar', 68.69200001708984)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-46.pth.tar', 68.66999993896485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-47.pth.tar', 68.63200010009766)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-43.pth.tar', 68.34800007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-42.pth.tar', 68.33999994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-38.pth.tar', 68.32599997314453)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-45.pth.tar', 68.19599991699219)

Train: 53 [   0/1251 (  0%)]  Loss: 3.860 (3.86)  Time: 2.744s,  373.14/s  (2.744s,  373.14/s)  LR: 9.257e-04  Data: 1.786 (1.786)
Train: 53 [  50/1251 (  4%)]  Loss: 3.644 (3.75)  Time: 0.995s, 1028.90/s  (1.038s,  986.53/s)  LR: 9.257e-04  Data: 0.012 (0.047)
Train: 53 [ 100/1251 (  8%)]  Loss: 3.724 (3.74)  Time: 0.998s, 1026.42/s  (1.021s, 1003.01/s)  LR: 9.257e-04  Data: 0.011 (0.029)
Train: 53 [ 150/1251 ( 12%)]  Loss: 4.004 (3.81)  Time: 0.998s, 1026.11/s  (1.013s, 1010.60/s)  LR: 9.257e-04  Data: 0.012 (0.023)
Train: 53 [ 200/1251 ( 16%)]  Loss: 3.917 (3.83)  Time: 0.995s, 1028.65/s  (1.016s, 1008.18/s)  LR: 9.257e-04  Data: 0.012 (0.020)
Train: 53 [ 250/1251 ( 20%)]  Loss: 3.442 (3.76)  Time: 0.993s, 1030.89/s  (1.013s, 1010.87/s)  LR: 9.257e-04  Data: 0.011 (0.019)
Train: 53 [ 300/1251 ( 24%)]  Loss: 3.755 (3.76)  Time: 1.053s,  972.29/s  (1.012s, 1011.96/s)  LR: 9.257e-04  Data: 0.012 (0.017)
Train: 53 [ 350/1251 ( 28%)]  Loss: 3.804 (3.77)  Time: 1.000s, 1023.97/s  (1.014s, 1010.08/s)  LR: 9.257e-04  Data: 0.011 (0.017)
Train: 53 [ 400/1251 ( 32%)]  Loss: 4.029 (3.80)  Time: 1.001s, 1022.80/s  (1.015s, 1008.85/s)  LR: 9.257e-04  Data: 0.011 (0.016)
Train: 53 [ 450/1251 ( 36%)]  Loss: 3.911 (3.81)  Time: 0.995s, 1029.11/s  (1.013s, 1010.79/s)  LR: 9.257e-04  Data: 0.011 (0.015)
Train: 53 [ 500/1251 ( 40%)]  Loss: 4.014 (3.83)  Time: 1.062s,  964.28/s  (1.012s, 1011.44/s)  LR: 9.257e-04  Data: 0.011 (0.015)
Train: 53 [ 550/1251 ( 44%)]  Loss: 4.088 (3.85)  Time: 0.996s, 1028.59/s  (1.011s, 1012.41/s)  LR: 9.257e-04  Data: 0.011 (0.015)
Train: 53 [ 600/1251 ( 48%)]  Loss: 3.648 (3.83)  Time: 0.996s, 1027.63/s  (1.012s, 1012.01/s)  LR: 9.257e-04  Data: 0.012 (0.014)
Train: 53 [ 650/1251 ( 52%)]  Loss: 3.964 (3.84)  Time: 1.066s,  960.78/s  (1.011s, 1012.49/s)  LR: 9.257e-04  Data: 0.012 (0.014)
Train: 53 [ 700/1251 ( 56%)]  Loss: 3.933 (3.85)  Time: 0.996s, 1028.53/s  (1.011s, 1012.52/s)  LR: 9.257e-04  Data: 0.012 (0.014)
Train: 53 [ 750/1251 ( 60%)]  Loss: 3.980 (3.86)  Time: 0.996s, 1028.52/s  (1.013s, 1011.21/s)  LR: 9.257e-04  Data: 0.012 (0.014)
Train: 53 [ 800/1251 ( 64%)]  Loss: 3.788 (3.85)  Time: 1.030s,  994.15/s  (1.012s, 1011.96/s)  LR: 9.257e-04  Data: 0.011 (0.013)
Train: 53 [ 850/1251 ( 68%)]  Loss: 3.989 (3.86)  Time: 1.059s,  966.67/s  (1.013s, 1011.04/s)  LR: 9.257e-04  Data: 0.010 (0.013)
Train: 53 [ 900/1251 ( 72%)]  Loss: 3.657 (3.85)  Time: 1.007s, 1016.85/s  (1.013s, 1010.77/s)  LR: 9.257e-04  Data: 0.011 (0.013)
Train: 53 [ 950/1251 ( 76%)]  Loss: 3.653 (3.84)  Time: 0.993s, 1030.95/s  (1.012s, 1011.50/s)  LR: 9.257e-04  Data: 0.010 (0.013)
Train: 53 [1000/1251 ( 80%)]  Loss: 3.998 (3.85)  Time: 1.023s, 1000.91/s  (1.012s, 1011.94/s)  LR: 9.257e-04  Data: 0.010 (0.013)
Train: 53 [1050/1251 ( 84%)]  Loss: 3.808 (3.85)  Time: 0.999s, 1024.66/s  (1.011s, 1012.48/s)  LR: 9.257e-04  Data: 0.011 (0.013)
Train: 53 [1100/1251 ( 88%)]  Loss: 3.545 (3.83)  Time: 0.996s, 1028.14/s  (1.011s, 1012.89/s)  LR: 9.257e-04  Data: 0.011 (0.013)
Train: 53 [1150/1251 ( 92%)]  Loss: 3.664 (3.83)  Time: 0.997s, 1027.12/s  (1.011s, 1012.99/s)  LR: 9.257e-04  Data: 0.011 (0.013)
Train: 53 [1200/1251 ( 96%)]  Loss: 3.803 (3.82)  Time: 1.046s,  978.86/s  (1.011s, 1013.14/s)  LR: 9.257e-04  Data: 0.011 (0.013)
Train: 53 [1250/1251 (100%)]  Loss: 3.951 (3.83)  Time: 0.985s, 1039.99/s  (1.011s, 1012.80/s)  LR: 9.257e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.636 (1.636)  Loss:  0.9815 (0.9815)  Acc@1: 85.0586 (85.0586)  Acc@5: 96.2891 (96.2891)
Test: [  48/48]  Time: 0.245 (0.569)  Loss:  0.9695 (1.6036)  Acc@1: 83.7264 (69.5940)  Acc@5: 96.1085 (89.5340)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-53.pth.tar', 69.5940000415039)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-50.pth.tar', 69.03400003173829)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-51.pth.tar', 68.85200004882813)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-49.pth.tar', 68.69800010742188)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-52.pth.tar', 68.69200001708984)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-46.pth.tar', 68.66999993896485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-47.pth.tar', 68.63200010009766)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-43.pth.tar', 68.34800007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-42.pth.tar', 68.33999994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-38.pth.tar', 68.32599997314453)

Train: 54 [   0/1251 (  0%)]  Loss: 4.076 (4.08)  Time: 2.492s,  410.93/s  (2.492s,  410.93/s)  LR: 9.229e-04  Data: 1.528 (1.528)
Train: 54 [  50/1251 (  4%)]  Loss: 3.857 (3.97)  Time: 1.049s,  976.56/s  (1.032s,  991.83/s)  LR: 9.229e-04  Data: 0.011 (0.041)
Train: 54 [ 100/1251 (  8%)]  Loss: 3.814 (3.92)  Time: 1.003s, 1021.14/s  (1.022s, 1001.97/s)  LR: 9.229e-04  Data: 0.012 (0.026)
Train: 54 [ 150/1251 ( 12%)]  Loss: 4.192 (3.99)  Time: 1.050s,  975.54/s  (1.028s,  996.29/s)  LR: 9.229e-04  Data: 0.011 (0.021)
Train: 54 [ 200/1251 ( 16%)]  Loss: 3.741 (3.94)  Time: 1.004s, 1020.00/s  (1.024s, 1000.13/s)  LR: 9.229e-04  Data: 0.011 (0.019)
Train: 54 [ 250/1251 ( 20%)]  Loss: 3.973 (3.94)  Time: 1.058s,  967.62/s  (1.022s, 1002.12/s)  LR: 9.229e-04  Data: 0.011 (0.017)
Train: 54 [ 300/1251 ( 24%)]  Loss: 4.191 (3.98)  Time: 1.040s,  984.85/s  (1.020s, 1003.52/s)  LR: 9.229e-04  Data: 0.012 (0.016)
Train: 54 [ 350/1251 ( 28%)]  Loss: 4.165 (4.00)  Time: 1.041s,  984.07/s  (1.021s, 1002.60/s)  LR: 9.229e-04  Data: 0.011 (0.015)
Train: 54 [ 400/1251 ( 32%)]  Loss: 4.170 (4.02)  Time: 0.993s, 1031.32/s  (1.019s, 1005.08/s)  LR: 9.229e-04  Data: 0.011 (0.015)
Train: 54 [ 450/1251 ( 36%)]  Loss: 4.016 (4.02)  Time: 0.995s, 1029.48/s  (1.017s, 1006.91/s)  LR: 9.229e-04  Data: 0.011 (0.014)
Train: 54 [ 500/1251 ( 40%)]  Loss: 4.058 (4.02)  Time: 0.996s, 1027.72/s  (1.015s, 1008.61/s)  LR: 9.229e-04  Data: 0.012 (0.014)
Train: 54 [ 550/1251 ( 44%)]  Loss: 3.742 (4.00)  Time: 0.996s, 1028.25/s  (1.014s, 1010.31/s)  LR: 9.229e-04  Data: 0.010 (0.014)
Train: 54 [ 600/1251 ( 48%)]  Loss: 3.883 (3.99)  Time: 1.004s, 1019.69/s  (1.015s, 1008.87/s)  LR: 9.229e-04  Data: 0.011 (0.014)
Train: 54 [ 650/1251 ( 52%)]  Loss: 4.026 (3.99)  Time: 1.002s, 1021.79/s  (1.015s, 1008.84/s)  LR: 9.229e-04  Data: 0.014 (0.013)
Train: 54 [ 700/1251 ( 56%)]  Loss: 3.770 (3.98)  Time: 0.997s, 1026.77/s  (1.014s, 1009.71/s)  LR: 9.229e-04  Data: 0.011 (0.013)
Train: 54 [ 750/1251 ( 60%)]  Loss: 3.717 (3.96)  Time: 0.994s, 1029.79/s  (1.013s, 1010.61/s)  LR: 9.229e-04  Data: 0.011 (0.013)
Train: 54 [ 800/1251 ( 64%)]  Loss: 4.184 (3.98)  Time: 0.996s, 1027.77/s  (1.013s, 1010.42/s)  LR: 9.229e-04  Data: 0.012 (0.013)
Train: 54 [ 850/1251 ( 68%)]  Loss: 3.828 (3.97)  Time: 1.042s,  983.16/s  (1.014s, 1010.02/s)  LR: 9.229e-04  Data: 0.011 (0.013)
Train: 54 [ 900/1251 ( 72%)]  Loss: 3.987 (3.97)  Time: 0.997s, 1027.23/s  (1.015s, 1009.33/s)  LR: 9.229e-04  Data: 0.011 (0.013)
Train: 54 [ 950/1251 ( 76%)]  Loss: 3.479 (3.94)  Time: 0.996s, 1027.88/s  (1.014s, 1009.94/s)  LR: 9.229e-04  Data: 0.012 (0.013)
Train: 54 [1000/1251 ( 80%)]  Loss: 3.726 (3.93)  Time: 0.998s, 1025.99/s  (1.014s, 1009.92/s)  LR: 9.229e-04  Data: 0.012 (0.013)
Train: 54 [1050/1251 ( 84%)]  Loss: 4.119 (3.94)  Time: 0.994s, 1030.64/s  (1.013s, 1010.66/s)  LR: 9.229e-04  Data: 0.011 (0.013)
Train: 54 [1100/1251 ( 88%)]  Loss: 3.799 (3.94)  Time: 0.996s, 1028.15/s  (1.013s, 1011.20/s)  LR: 9.229e-04  Data: 0.011 (0.013)
Train: 54 [1150/1251 ( 92%)]  Loss: 4.030 (3.94)  Time: 0.998s, 1026.24/s  (1.012s, 1011.75/s)  LR: 9.229e-04  Data: 0.011 (0.012)
Train: 54 [1200/1251 ( 96%)]  Loss: 3.989 (3.94)  Time: 0.995s, 1029.12/s  (1.012s, 1012.24/s)  LR: 9.229e-04  Data: 0.011 (0.012)
Train: 54 [1250/1251 (100%)]  Loss: 3.743 (3.93)  Time: 0.983s, 1041.21/s  (1.011s, 1012.83/s)  LR: 9.229e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.627 (1.627)  Loss:  0.9141 (0.9141)  Acc@1: 84.8633 (84.8633)  Acc@5: 95.7031 (95.7031)
Test: [  48/48]  Time: 0.245 (0.568)  Loss:  1.0188 (1.5776)  Acc@1: 83.1368 (69.6720)  Acc@5: 95.2830 (89.6040)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-54.pth.tar', 69.67200004394532)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-53.pth.tar', 69.5940000415039)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-50.pth.tar', 69.03400003173829)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-51.pth.tar', 68.85200004882813)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-49.pth.tar', 68.69800010742188)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-52.pth.tar', 68.69200001708984)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-46.pth.tar', 68.66999993896485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-47.pth.tar', 68.63200010009766)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-43.pth.tar', 68.34800007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-42.pth.tar', 68.33999994873047)

Train: 55 [   0/1251 (  0%)]  Loss: 4.103 (4.10)  Time: 2.502s,  409.35/s  (2.502s,  409.35/s)  LR: 9.201e-04  Data: 1.543 (1.543)
Train: 55 [  50/1251 (  4%)]  Loss: 3.835 (3.97)  Time: 1.060s,  965.62/s  (1.030s,  994.63/s)  LR: 9.201e-04  Data: 0.011 (0.041)
Train: 55 [ 100/1251 (  8%)]  Loss: 3.698 (3.88)  Time: 0.998s, 1026.00/s  (1.020s, 1004.18/s)  LR: 9.201e-04  Data: 0.011 (0.026)
Train: 55 [ 150/1251 ( 12%)]  Loss: 3.829 (3.87)  Time: 0.996s, 1028.60/s  (1.012s, 1011.44/s)  LR: 9.201e-04  Data: 0.011 (0.021)
Train: 55 [ 200/1251 ( 16%)]  Loss: 3.620 (3.82)  Time: 1.000s, 1024.49/s  (1.009s, 1015.02/s)  LR: 9.201e-04  Data: 0.010 (0.019)
Train: 55 [ 250/1251 ( 20%)]  Loss: 3.480 (3.76)  Time: 0.995s, 1029.35/s  (1.007s, 1017.00/s)  LR: 9.201e-04  Data: 0.011 (0.017)
Train: 55 [ 300/1251 ( 24%)]  Loss: 4.080 (3.81)  Time: 0.996s, 1028.58/s  (1.005s, 1018.50/s)  LR: 9.201e-04  Data: 0.012 (0.016)
Train: 55 [ 350/1251 ( 28%)]  Loss: 3.822 (3.81)  Time: 0.998s, 1026.53/s  (1.005s, 1019.26/s)  LR: 9.201e-04  Data: 0.011 (0.015)
Train: 55 [ 400/1251 ( 32%)]  Loss: 3.912 (3.82)  Time: 1.000s, 1024.26/s  (1.006s, 1018.05/s)  LR: 9.201e-04  Data: 0.011 (0.015)
Train: 55 [ 450/1251 ( 36%)]  Loss: 4.167 (3.85)  Time: 1.036s,  988.01/s  (1.006s, 1018.34/s)  LR: 9.201e-04  Data: 0.012 (0.014)
Train: 55 [ 500/1251 ( 40%)]  Loss: 3.835 (3.85)  Time: 0.995s, 1028.68/s  (1.005s, 1018.40/s)  LR: 9.201e-04  Data: 0.011 (0.014)
Train: 55 [ 550/1251 ( 44%)]  Loss: 3.609 (3.83)  Time: 0.991s, 1033.44/s  (1.005s, 1019.22/s)  LR: 9.201e-04  Data: 0.010 (0.014)
Train: 55 [ 600/1251 ( 48%)]  Loss: 3.659 (3.82)  Time: 0.996s, 1028.56/s  (1.004s, 1019.86/s)  LR: 9.201e-04  Data: 0.011 (0.014)
Train: 55 [ 650/1251 ( 52%)]  Loss: 3.604 (3.80)  Time: 0.996s, 1027.65/s  (1.005s, 1018.92/s)  LR: 9.201e-04  Data: 0.011 (0.013)
Train: 55 [ 700/1251 ( 56%)]  Loss: 4.031 (3.82)  Time: 0.996s, 1027.91/s  (1.005s, 1018.64/s)  LR: 9.201e-04  Data: 0.011 (0.013)
Train: 55 [ 750/1251 ( 60%)]  Loss: 3.802 (3.82)  Time: 1.027s,  996.95/s  (1.005s, 1018.75/s)  LR: 9.201e-04  Data: 0.012 (0.013)
Train: 55 [ 800/1251 ( 64%)]  Loss: 3.864 (3.82)  Time: 1.038s,  986.64/s  (1.006s, 1017.96/s)  LR: 9.201e-04  Data: 0.010 (0.013)
Train: 55 [ 850/1251 ( 68%)]  Loss: 3.521 (3.80)  Time: 0.997s, 1026.75/s  (1.006s, 1017.42/s)  LR: 9.201e-04  Data: 0.011 (0.013)
Train: 55 [ 900/1251 ( 72%)]  Loss: 3.471 (3.79)  Time: 1.024s, 1000.22/s  (1.006s, 1017.77/s)  LR: 9.201e-04  Data: 0.011 (0.013)
Train: 55 [ 950/1251 ( 76%)]  Loss: 3.433 (3.77)  Time: 1.001s, 1023.42/s  (1.006s, 1018.02/s)  LR: 9.201e-04  Data: 0.011 (0.013)
Train: 55 [1000/1251 ( 80%)]  Loss: 3.564 (3.76)  Time: 1.002s, 1021.49/s  (1.006s, 1018.00/s)  LR: 9.201e-04  Data: 0.012 (0.013)
Train: 55 [1050/1251 ( 84%)]  Loss: 3.696 (3.76)  Time: 0.994s, 1029.84/s  (1.006s, 1018.26/s)  LR: 9.201e-04  Data: 0.011 (0.013)
Train: 55 [1100/1251 ( 88%)]  Loss: 4.040 (3.77)  Time: 0.996s, 1027.74/s  (1.005s, 1018.59/s)  LR: 9.201e-04  Data: 0.011 (0.013)
Train: 55 [1150/1251 ( 92%)]  Loss: 3.758 (3.77)  Time: 0.995s, 1028.63/s  (1.006s, 1017.83/s)  LR: 9.201e-04  Data: 0.011 (0.012)
Train: 55 [1200/1251 ( 96%)]  Loss: 3.880 (3.77)  Time: 0.995s, 1029.40/s  (1.006s, 1017.55/s)  LR: 9.201e-04  Data: 0.011 (0.012)
Train: 55 [1250/1251 (100%)]  Loss: 3.397 (3.76)  Time: 1.023s, 1001.32/s  (1.007s, 1016.92/s)  LR: 9.201e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.629 (1.629)  Loss:  0.8683 (0.8683)  Acc@1: 87.0117 (87.0117)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.245 (0.567)  Loss:  1.0887 (1.5470)  Acc@1: 81.9576 (69.6380)  Acc@5: 93.7500 (89.8140)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-54.pth.tar', 69.67200004394532)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-55.pth.tar', 69.63800004882812)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-53.pth.tar', 69.5940000415039)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-50.pth.tar', 69.03400003173829)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-51.pth.tar', 68.85200004882813)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-49.pth.tar', 68.69800010742188)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-52.pth.tar', 68.69200001708984)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-46.pth.tar', 68.66999993896485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-47.pth.tar', 68.63200010009766)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-43.pth.tar', 68.34800007324219)

Train: 56 [   0/1251 (  0%)]  Loss: 3.659 (3.66)  Time: 2.506s,  408.58/s  (2.506s,  408.58/s)  LR: 9.173e-04  Data: 1.466 (1.466)
Train: 56 [  50/1251 (  4%)]  Loss: 3.989 (3.82)  Time: 0.996s, 1028.05/s  (1.047s,  978.12/s)  LR: 9.173e-04  Data: 0.010 (0.040)
Train: 56 [ 100/1251 (  8%)]  Loss: 3.536 (3.73)  Time: 0.998s, 1026.24/s  (1.024s,  999.61/s)  LR: 9.173e-04  Data: 0.010 (0.026)
Train: 56 [ 150/1251 ( 12%)]  Loss: 3.369 (3.64)  Time: 1.022s, 1002.21/s  (1.016s, 1007.85/s)  LR: 9.173e-04  Data: 0.011 (0.021)
Train: 56 [ 200/1251 ( 16%)]  Loss: 3.534 (3.62)  Time: 0.994s, 1030.25/s  (1.013s, 1010.99/s)  LR: 9.173e-04  Data: 0.010 (0.018)
Train: 56 [ 250/1251 ( 20%)]  Loss: 3.799 (3.65)  Time: 0.997s, 1026.99/s  (1.011s, 1012.96/s)  LR: 9.173e-04  Data: 0.011 (0.017)
Train: 56 [ 300/1251 ( 24%)]  Loss: 3.940 (3.69)  Time: 0.997s, 1026.62/s  (1.009s, 1015.18/s)  LR: 9.173e-04  Data: 0.011 (0.016)
Train: 56 [ 350/1251 ( 28%)]  Loss: 4.066 (3.74)  Time: 0.997s, 1026.76/s  (1.008s, 1016.18/s)  LR: 9.173e-04  Data: 0.012 (0.015)
Train: 56 [ 400/1251 ( 32%)]  Loss: 3.545 (3.72)  Time: 1.038s,  986.91/s  (1.009s, 1014.51/s)  LR: 9.173e-04  Data: 0.011 (0.015)
Train: 56 [ 450/1251 ( 36%)]  Loss: 3.424 (3.69)  Time: 0.998s, 1025.56/s  (1.011s, 1012.48/s)  LR: 9.173e-04  Data: 0.011 (0.014)
Train: 56 [ 500/1251 ( 40%)]  Loss: 4.111 (3.72)  Time: 0.996s, 1027.81/s  (1.010s, 1013.77/s)  LR: 9.173e-04  Data: 0.011 (0.014)
Train: 56 [ 550/1251 ( 44%)]  Loss: 3.874 (3.74)  Time: 0.997s, 1027.30/s  (1.009s, 1014.52/s)  LR: 9.173e-04  Data: 0.011 (0.014)
Train: 56 [ 600/1251 ( 48%)]  Loss: 3.540 (3.72)  Time: 1.030s,  994.49/s  (1.009s, 1015.37/s)  LR: 9.173e-04  Data: 0.010 (0.014)
Train: 56 [ 650/1251 ( 52%)]  Loss: 4.121 (3.75)  Time: 0.997s, 1026.90/s  (1.008s, 1016.26/s)  LR: 9.173e-04  Data: 0.011 (0.013)
Train: 56 [ 700/1251 ( 56%)]  Loss: 3.897 (3.76)  Time: 0.995s, 1029.05/s  (1.007s, 1016.89/s)  LR: 9.173e-04  Data: 0.011 (0.013)
Train: 56 [ 750/1251 ( 60%)]  Loss: 3.670 (3.75)  Time: 0.997s, 1027.08/s  (1.007s, 1017.23/s)  LR: 9.173e-04  Data: 0.011 (0.013)
Train: 56 [ 800/1251 ( 64%)]  Loss: 3.747 (3.75)  Time: 0.999s, 1024.63/s  (1.007s, 1017.34/s)  LR: 9.173e-04  Data: 0.010 (0.013)
Train: 56 [ 850/1251 ( 68%)]  Loss: 3.539 (3.74)  Time: 0.997s, 1027.37/s  (1.006s, 1017.43/s)  LR: 9.173e-04  Data: 0.010 (0.013)
Train: 56 [ 900/1251 ( 72%)]  Loss: 3.898 (3.75)  Time: 0.997s, 1026.80/s  (1.006s, 1018.00/s)  LR: 9.173e-04  Data: 0.010 (0.013)
Train: 56 [ 950/1251 ( 76%)]  Loss: 3.933 (3.76)  Time: 0.995s, 1029.54/s  (1.005s, 1018.50/s)  LR: 9.173e-04  Data: 0.011 (0.013)
Train: 56 [1000/1251 ( 80%)]  Loss: 3.947 (3.77)  Time: 1.035s,  989.69/s  (1.005s, 1018.80/s)  LR: 9.173e-04  Data: 0.011 (0.013)
Train: 56 [1050/1251 ( 84%)]  Loss: 3.552 (3.76)  Time: 0.995s, 1028.84/s  (1.005s, 1018.89/s)  LR: 9.173e-04  Data: 0.011 (0.012)
Train: 56 [1100/1251 ( 88%)]  Loss: 3.535 (3.75)  Time: 0.996s, 1028.02/s  (1.005s, 1019.15/s)  LR: 9.173e-04  Data: 0.011 (0.012)
Train: 56 [1150/1251 ( 92%)]  Loss: 4.075 (3.76)  Time: 0.997s, 1026.63/s  (1.005s, 1019.08/s)  LR: 9.173e-04  Data: 0.011 (0.012)
Train: 56 [1200/1251 ( 96%)]  Loss: 3.565 (3.75)  Time: 1.066s,  960.76/s  (1.005s, 1019.09/s)  LR: 9.173e-04  Data: 0.011 (0.012)
Train: 56 [1250/1251 (100%)]  Loss: 3.873 (3.76)  Time: 0.985s, 1039.53/s  (1.005s, 1019.34/s)  LR: 9.173e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.589 (1.589)  Loss:  0.8439 (0.8439)  Acc@1: 86.4258 (86.4258)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.245 (0.572)  Loss:  1.0312 (1.5579)  Acc@1: 83.0189 (69.5360)  Acc@5: 94.9292 (89.9120)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-54.pth.tar', 69.67200004394532)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-55.pth.tar', 69.63800004882812)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-53.pth.tar', 69.5940000415039)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-56.pth.tar', 69.53599999267578)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-50.pth.tar', 69.03400003173829)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-51.pth.tar', 68.85200004882813)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-49.pth.tar', 68.69800010742188)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-52.pth.tar', 68.69200001708984)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-46.pth.tar', 68.66999993896485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-47.pth.tar', 68.63200010009766)

Train: 57 [   0/1251 (  0%)]  Loss: 3.866 (3.87)  Time: 4.285s,  238.97/s  (4.285s,  238.97/s)  LR: 9.144e-04  Data: 3.033 (3.033)
Train: 57 [  50/1251 (  4%)]  Loss: 3.682 (3.77)  Time: 1.001s, 1022.91/s  (1.071s,  956.15/s)  LR: 9.144e-04  Data: 0.011 (0.070)
Train: 57 [ 100/1251 (  8%)]  Loss: 4.209 (3.92)  Time: 0.994s, 1030.23/s  (1.037s,  987.02/s)  LR: 9.144e-04  Data: 0.012 (0.041)
Train: 57 [ 150/1251 ( 12%)]  Loss: 4.217 (3.99)  Time: 1.005s, 1019.24/s  (1.027s,  997.14/s)  LR: 9.144e-04  Data: 0.010 (0.031)
Train: 57 [ 200/1251 ( 16%)]  Loss: 3.848 (3.96)  Time: 0.997s, 1027.03/s  (1.021s, 1003.19/s)  LR: 9.144e-04  Data: 0.011 (0.026)
Train: 57 [ 250/1251 ( 20%)]  Loss: 3.807 (3.94)  Time: 1.001s, 1022.72/s  (1.018s, 1006.04/s)  LR: 9.144e-04  Data: 0.011 (0.023)
Train: 57 [ 300/1251 ( 24%)]  Loss: 3.989 (3.95)  Time: 0.994s, 1029.76/s  (1.015s, 1009.18/s)  LR: 9.144e-04  Data: 0.011 (0.021)
Train: 57 [ 350/1251 ( 28%)]  Loss: 4.118 (3.97)  Time: 0.997s, 1026.74/s  (1.014s, 1009.37/s)  LR: 9.144e-04  Data: 0.011 (0.020)
Train: 57 [ 400/1251 ( 32%)]  Loss: 3.685 (3.94)  Time: 0.996s, 1027.75/s  (1.012s, 1011.51/s)  LR: 9.144e-04  Data: 0.012 (0.019)
Train: 57 [ 450/1251 ( 36%)]  Loss: 4.078 (3.95)  Time: 0.999s, 1024.92/s  (1.012s, 1011.88/s)  LR: 9.144e-04  Data: 0.011 (0.018)
Train: 57 [ 500/1251 ( 40%)]  Loss: 4.165 (3.97)  Time: 0.995s, 1029.18/s  (1.011s, 1013.07/s)  LR: 9.144e-04  Data: 0.012 (0.017)
Train: 57 [ 550/1251 ( 44%)]  Loss: 4.194 (3.99)  Time: 1.039s,  985.97/s  (1.011s, 1012.50/s)  LR: 9.144e-04  Data: 0.011 (0.016)
Train: 57 [ 600/1251 ( 48%)]  Loss: 3.819 (3.98)  Time: 1.007s, 1016.94/s  (1.013s, 1011.20/s)  LR: 9.144e-04  Data: 0.011 (0.016)
Train: 57 [ 650/1251 ( 52%)]  Loss: 3.932 (3.97)  Time: 0.993s, 1031.63/s  (1.012s, 1011.82/s)  LR: 9.144e-04  Data: 0.010 (0.016)
Train: 57 [ 700/1251 ( 56%)]  Loss: 3.792 (3.96)  Time: 0.997s, 1026.77/s  (1.012s, 1012.15/s)  LR: 9.144e-04  Data: 0.011 (0.015)
Train: 57 [ 750/1251 ( 60%)]  Loss: 3.654 (3.94)  Time: 0.996s, 1028.31/s  (1.011s, 1012.94/s)  LR: 9.144e-04  Data: 0.010 (0.015)
Train: 57 [ 800/1251 ( 64%)]  Loss: 4.127 (3.95)  Time: 0.997s, 1026.80/s  (1.010s, 1013.65/s)  LR: 9.144e-04  Data: 0.011 (0.015)
Train: 57 [ 850/1251 ( 68%)]  Loss: 3.750 (3.94)  Time: 0.999s, 1024.89/s  (1.010s, 1013.78/s)  LR: 9.144e-04  Data: 0.012 (0.015)
Train: 57 [ 900/1251 ( 72%)]  Loss: 4.080 (3.95)  Time: 0.998s, 1026.36/s  (1.010s, 1013.85/s)  LR: 9.144e-04  Data: 0.011 (0.014)
Train: 57 [ 950/1251 ( 76%)]  Loss: 4.076 (3.95)  Time: 1.035s,  988.92/s  (1.009s, 1014.41/s)  LR: 9.144e-04  Data: 0.010 (0.014)
Train: 57 [1000/1251 ( 80%)]  Loss: 3.867 (3.95)  Time: 1.033s,  991.67/s  (1.010s, 1014.03/s)  LR: 9.144e-04  Data: 0.012 (0.014)
Train: 57 [1050/1251 ( 84%)]  Loss: 3.690 (3.94)  Time: 0.994s, 1030.49/s  (1.010s, 1014.14/s)  LR: 9.144e-04  Data: 0.010 (0.014)
Train: 57 [1100/1251 ( 88%)]  Loss: 3.610 (3.92)  Time: 0.996s, 1028.25/s  (1.009s, 1014.59/s)  LR: 9.144e-04  Data: 0.011 (0.014)
Train: 57 [1150/1251 ( 92%)]  Loss: 3.931 (3.92)  Time: 0.995s, 1028.96/s  (1.009s, 1015.17/s)  LR: 9.144e-04  Data: 0.011 (0.014)
Train: 57 [1200/1251 ( 96%)]  Loss: 3.835 (3.92)  Time: 0.995s, 1029.16/s  (1.009s, 1014.51/s)  LR: 9.144e-04  Data: 0.011 (0.014)
Train: 57 [1250/1251 (100%)]  Loss: 3.340 (3.90)  Time: 0.982s, 1042.67/s  (1.009s, 1014.93/s)  LR: 9.144e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.646 (1.646)  Loss:  0.9228 (0.9228)  Acc@1: 85.5469 (85.5469)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.245 (0.565)  Loss:  0.9356 (1.4994)  Acc@1: 82.5472 (70.3120)  Acc@5: 94.8113 (89.8740)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-57.pth.tar', 70.31200004638671)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-54.pth.tar', 69.67200004394532)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-55.pth.tar', 69.63800004882812)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-53.pth.tar', 69.5940000415039)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-56.pth.tar', 69.53599999267578)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-50.pth.tar', 69.03400003173829)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-51.pth.tar', 68.85200004882813)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-49.pth.tar', 68.69800010742188)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-52.pth.tar', 68.69200001708984)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-46.pth.tar', 68.66999993896485)

Train: 58 [   0/1251 (  0%)]  Loss: 3.811 (3.81)  Time: 2.433s,  420.90/s  (2.433s,  420.90/s)  LR: 9.115e-04  Data: 1.471 (1.471)
Train: 58 [  50/1251 (  4%)]  Loss: 3.565 (3.69)  Time: 1.019s, 1005.13/s  (1.049s,  976.03/s)  LR: 9.115e-04  Data: 0.011 (0.040)
Train: 58 [ 100/1251 (  8%)]  Loss: 3.898 (3.76)  Time: 1.043s,  982.03/s  (1.031s,  993.61/s)  LR: 9.115e-04  Data: 0.011 (0.026)
Train: 58 [ 150/1251 ( 12%)]  Loss: 4.209 (3.87)  Time: 0.998s, 1026.30/s  (1.026s,  998.40/s)  LR: 9.115e-04  Data: 0.010 (0.021)
Train: 58 [ 200/1251 ( 16%)]  Loss: 3.891 (3.87)  Time: 0.996s, 1028.09/s  (1.019s, 1004.96/s)  LR: 9.115e-04  Data: 0.010 (0.018)
Train: 58 [ 250/1251 ( 20%)]  Loss: 3.638 (3.84)  Time: 0.999s, 1025.11/s  (1.015s, 1009.12/s)  LR: 9.115e-04  Data: 0.011 (0.017)
Train: 58 [ 300/1251 ( 24%)]  Loss: 4.185 (3.89)  Time: 0.997s, 1026.86/s  (1.015s, 1008.94/s)  LR: 9.115e-04  Data: 0.011 (0.016)
Train: 58 [ 350/1251 ( 28%)]  Loss: 4.082 (3.91)  Time: 0.996s, 1027.62/s  (1.014s, 1010.21/s)  LR: 9.115e-04  Data: 0.010 (0.015)
Train: 58 [ 400/1251 ( 32%)]  Loss: 3.692 (3.89)  Time: 1.017s, 1007.20/s  (1.014s, 1010.06/s)  LR: 9.115e-04  Data: 0.011 (0.015)
Train: 58 [ 450/1251 ( 36%)]  Loss: 3.760 (3.87)  Time: 0.994s, 1030.69/s  (1.014s, 1010.26/s)  LR: 9.115e-04  Data: 0.011 (0.014)
Train: 58 [ 500/1251 ( 40%)]  Loss: 3.821 (3.87)  Time: 1.044s,  981.12/s  (1.013s, 1010.75/s)  LR: 9.115e-04  Data: 0.011 (0.014)
Train: 58 [ 550/1251 ( 44%)]  Loss: 3.778 (3.86)  Time: 0.999s, 1025.01/s  (1.013s, 1010.94/s)  LR: 9.115e-04  Data: 0.011 (0.014)
Train: 58 [ 600/1251 ( 48%)]  Loss: 3.696 (3.85)  Time: 0.997s, 1027.40/s  (1.014s, 1009.92/s)  LR: 9.115e-04  Data: 0.011 (0.014)
Train: 58 [ 650/1251 ( 52%)]  Loss: 3.869 (3.85)  Time: 0.999s, 1024.89/s  (1.013s, 1010.69/s)  LR: 9.115e-04  Data: 0.011 (0.013)
Train: 58 [ 700/1251 ( 56%)]  Loss: 3.653 (3.84)  Time: 1.002s, 1021.52/s  (1.012s, 1011.76/s)  LR: 9.115e-04  Data: 0.014 (0.013)
Train: 58 [ 750/1251 ( 60%)]  Loss: 3.994 (3.85)  Time: 0.996s, 1028.18/s  (1.011s, 1012.44/s)  LR: 9.115e-04  Data: 0.011 (0.013)
Train: 58 [ 800/1251 ( 64%)]  Loss: 3.636 (3.83)  Time: 1.051s,  974.08/s  (1.011s, 1012.79/s)  LR: 9.115e-04  Data: 0.011 (0.013)
Train: 58 [ 850/1251 ( 68%)]  Loss: 3.917 (3.84)  Time: 1.039s,  985.20/s  (1.011s, 1012.78/s)  LR: 9.115e-04  Data: 0.011 (0.013)
Train: 58 [ 900/1251 ( 72%)]  Loss: 4.200 (3.86)  Time: 1.071s,  955.73/s  (1.011s, 1013.35/s)  LR: 9.115e-04  Data: 0.016 (0.013)
Train: 58 [ 950/1251 ( 76%)]  Loss: 3.490 (3.84)  Time: 0.997s, 1026.83/s  (1.011s, 1012.74/s)  LR: 9.115e-04  Data: 0.012 (0.013)
Train: 58 [1000/1251 ( 80%)]  Loss: 4.151 (3.85)  Time: 0.997s, 1027.37/s  (1.012s, 1012.19/s)  LR: 9.115e-04  Data: 0.012 (0.013)
Train: 58 [1050/1251 ( 84%)]  Loss: 4.152 (3.87)  Time: 0.995s, 1029.10/s  (1.012s, 1011.65/s)  LR: 9.115e-04  Data: 0.011 (0.013)
Train: 58 [1100/1251 ( 88%)]  Loss: 3.908 (3.87)  Time: 0.999s, 1025.45/s  (1.012s, 1012.17/s)  LR: 9.115e-04  Data: 0.011 (0.013)
Train: 58 [1150/1251 ( 92%)]  Loss: 3.708 (3.86)  Time: 0.997s, 1026.89/s  (1.011s, 1012.63/s)  LR: 9.115e-04  Data: 0.012 (0.013)
Train: 58 [1200/1251 ( 96%)]  Loss: 3.652 (3.85)  Time: 1.007s, 1016.55/s  (1.011s, 1012.69/s)  LR: 9.115e-04  Data: 0.012 (0.013)
Train: 58 [1250/1251 (100%)]  Loss: 4.024 (3.86)  Time: 0.982s, 1042.84/s  (1.011s, 1013.19/s)  LR: 9.115e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.667 (1.667)  Loss:  0.8980 (0.8980)  Acc@1: 83.2031 (83.2031)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.245 (0.568)  Loss:  0.9497 (1.5089)  Acc@1: 82.9009 (69.8340)  Acc@5: 95.7547 (89.8140)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-57.pth.tar', 70.31200004638671)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-58.pth.tar', 69.83400007080078)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-54.pth.tar', 69.67200004394532)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-55.pth.tar', 69.63800004882812)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-53.pth.tar', 69.5940000415039)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-56.pth.tar', 69.53599999267578)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-50.pth.tar', 69.03400003173829)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-51.pth.tar', 68.85200004882813)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-49.pth.tar', 68.69800010742188)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-52.pth.tar', 68.69200001708984)

Train: 59 [   0/1251 (  0%)]  Loss: 3.598 (3.60)  Time: 2.457s,  416.81/s  (2.457s,  416.81/s)  LR: 9.085e-04  Data: 1.492 (1.492)
Train: 59 [  50/1251 (  4%)]  Loss: 3.898 (3.75)  Time: 0.996s, 1028.00/s  (1.045s,  979.89/s)  LR: 9.085e-04  Data: 0.010 (0.040)
Train: 59 [ 100/1251 (  8%)]  Loss: 3.943 (3.81)  Time: 0.997s, 1026.97/s  (1.023s, 1001.41/s)  LR: 9.085e-04  Data: 0.012 (0.026)
Train: 59 [ 150/1251 ( 12%)]  Loss: 3.925 (3.84)  Time: 0.996s, 1027.98/s  (1.018s, 1005.97/s)  LR: 9.085e-04  Data: 0.011 (0.021)
Train: 59 [ 200/1251 ( 16%)]  Loss: 3.801 (3.83)  Time: 0.993s, 1031.70/s  (1.014s, 1009.85/s)  LR: 9.085e-04  Data: 0.011 (0.019)
Train: 59 [ 250/1251 ( 20%)]  Loss: 3.931 (3.85)  Time: 0.998s, 1026.05/s  (1.014s, 1010.20/s)  LR: 9.085e-04  Data: 0.011 (0.017)
Train: 59 [ 300/1251 ( 24%)]  Loss: 3.865 (3.85)  Time: 1.000s, 1024.51/s  (1.011s, 1012.67/s)  LR: 9.085e-04  Data: 0.011 (0.016)
Train: 59 [ 350/1251 ( 28%)]  Loss: 3.922 (3.86)  Time: 0.996s, 1028.39/s  (1.014s, 1009.60/s)  LR: 9.085e-04  Data: 0.012 (0.016)
Train: 59 [ 400/1251 ( 32%)]  Loss: 3.709 (3.84)  Time: 0.997s, 1026.85/s  (1.012s, 1011.58/s)  LR: 9.085e-04  Data: 0.012 (0.015)
Train: 59 [ 450/1251 ( 36%)]  Loss: 3.935 (3.85)  Time: 1.003s, 1021.30/s  (1.011s, 1013.13/s)  LR: 9.085e-04  Data: 0.011 (0.015)
Train: 59 [ 500/1251 ( 40%)]  Loss: 3.912 (3.86)  Time: 0.995s, 1029.40/s  (1.010s, 1013.89/s)  LR: 9.085e-04  Data: 0.011 (0.014)
Train: 59 [ 550/1251 ( 44%)]  Loss: 3.881 (3.86)  Time: 0.999s, 1025.15/s  (1.009s, 1015.03/s)  LR: 9.085e-04  Data: 0.012 (0.014)
Train: 59 [ 600/1251 ( 48%)]  Loss: 3.728 (3.85)  Time: 1.012s, 1011.79/s  (1.008s, 1015.44/s)  LR: 9.085e-04  Data: 0.011 (0.014)
Train: 59 [ 650/1251 ( 52%)]  Loss: 3.763 (3.84)  Time: 0.997s, 1027.44/s  (1.008s, 1015.94/s)  LR: 9.085e-04  Data: 0.012 (0.014)
Train: 59 [ 700/1251 ( 56%)]  Loss: 4.087 (3.86)  Time: 0.994s, 1029.76/s  (1.007s, 1016.63/s)  LR: 9.085e-04  Data: 0.011 (0.013)
Train: 59 [ 750/1251 ( 60%)]  Loss: 3.923 (3.86)  Time: 0.993s, 1031.16/s  (1.008s, 1016.22/s)  LR: 9.085e-04  Data: 0.011 (0.013)
Train: 59 [ 800/1251 ( 64%)]  Loss: 4.054 (3.87)  Time: 1.038s,  986.25/s  (1.008s, 1015.72/s)  LR: 9.085e-04  Data: 0.011 (0.013)
Train: 59 [ 850/1251 ( 68%)]  Loss: 3.879 (3.88)  Time: 0.994s, 1030.50/s  (1.009s, 1015.26/s)  LR: 9.085e-04  Data: 0.011 (0.013)
Train: 59 [ 900/1251 ( 72%)]  Loss: 3.919 (3.88)  Time: 0.998s, 1026.16/s  (1.008s, 1015.65/s)  LR: 9.085e-04  Data: 0.011 (0.013)
Train: 59 [ 950/1251 ( 76%)]  Loss: 4.037 (3.89)  Time: 0.999s, 1024.81/s  (1.008s, 1016.14/s)  LR: 9.085e-04  Data: 0.011 (0.013)
Train: 59 [1000/1251 ( 80%)]  Loss: 3.955 (3.89)  Time: 0.992s, 1032.48/s  (1.008s, 1016.29/s)  LR: 9.085e-04  Data: 0.010 (0.013)
Train: 59 [1050/1251 ( 84%)]  Loss: 4.125 (3.90)  Time: 1.000s, 1024.07/s  (1.007s, 1016.61/s)  LR: 9.085e-04  Data: 0.011 (0.013)
Train: 59 [1100/1251 ( 88%)]  Loss: 4.088 (3.91)  Time: 0.997s, 1027.59/s  (1.007s, 1016.74/s)  LR: 9.085e-04  Data: 0.011 (0.013)
Train: 59 [1150/1251 ( 92%)]  Loss: 3.331 (3.88)  Time: 0.999s, 1025.31/s  (1.007s, 1017.07/s)  LR: 9.085e-04  Data: 0.011 (0.013)
Train: 59 [1200/1251 ( 96%)]  Loss: 4.076 (3.89)  Time: 0.997s, 1027.05/s  (1.006s, 1017.41/s)  LR: 9.085e-04  Data: 0.011 (0.012)
Train: 59 [1250/1251 (100%)]  Loss: 3.880 (3.89)  Time: 0.985s, 1039.41/s  (1.006s, 1017.77/s)  LR: 9.085e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.657 (1.657)  Loss:  1.0389 (1.0389)  Acc@1: 86.4258 (86.4258)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.245 (0.569)  Loss:  1.1553 (1.5962)  Acc@1: 81.3679 (69.7860)  Acc@5: 94.5755 (89.7260)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-57.pth.tar', 70.31200004638671)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-58.pth.tar', 69.83400007080078)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-59.pth.tar', 69.78600005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-54.pth.tar', 69.67200004394532)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-55.pth.tar', 69.63800004882812)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-53.pth.tar', 69.5940000415039)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-56.pth.tar', 69.53599999267578)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-50.pth.tar', 69.03400003173829)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-51.pth.tar', 68.85200004882813)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-49.pth.tar', 68.69800010742188)

Train: 60 [   0/1251 (  0%)]  Loss: 3.783 (3.78)  Time: 2.558s,  400.30/s  (2.558s,  400.30/s)  LR: 9.055e-04  Data: 1.594 (1.594)
Train: 60 [  50/1251 (  4%)]  Loss: 3.774 (3.78)  Time: 0.995s, 1029.32/s  (1.055s,  970.78/s)  LR: 9.055e-04  Data: 0.012 (0.043)
Train: 60 [ 100/1251 (  8%)]  Loss: 4.190 (3.92)  Time: 1.010s, 1014.34/s  (1.029s,  995.58/s)  LR: 9.055e-04  Data: 0.012 (0.027)
Train: 60 [ 150/1251 ( 12%)]  Loss: 3.595 (3.84)  Time: 0.997s, 1027.40/s  (1.019s, 1004.85/s)  LR: 9.055e-04  Data: 0.011 (0.022)
Train: 60 [ 200/1251 ( 16%)]  Loss: 4.207 (3.91)  Time: 0.995s, 1029.32/s  (1.014s, 1010.21/s)  LR: 9.055e-04  Data: 0.011 (0.019)
Train: 60 [ 250/1251 ( 20%)]  Loss: 3.684 (3.87)  Time: 0.996s, 1028.38/s  (1.011s, 1012.87/s)  LR: 9.055e-04  Data: 0.011 (0.018)
Train: 60 [ 300/1251 ( 24%)]  Loss: 3.582 (3.83)  Time: 0.994s, 1030.56/s  (1.009s, 1014.56/s)  LR: 9.055e-04  Data: 0.011 (0.017)
Train: 60 [ 350/1251 ( 28%)]  Loss: 4.340 (3.89)  Time: 1.002s, 1021.83/s  (1.008s, 1016.29/s)  LR: 9.055e-04  Data: 0.015 (0.016)
Train: 60 [ 400/1251 ( 32%)]  Loss: 3.851 (3.89)  Time: 0.995s, 1028.76/s  (1.007s, 1017.25/s)  LR: 9.055e-04  Data: 0.011 (0.015)
Train: 60 [ 450/1251 ( 36%)]  Loss: 3.927 (3.89)  Time: 0.999s, 1024.60/s  (1.006s, 1017.83/s)  LR: 9.055e-04  Data: 0.012 (0.015)
Train: 60 [ 500/1251 ( 40%)]  Loss: 4.081 (3.91)  Time: 0.995s, 1028.82/s  (1.006s, 1018.30/s)  LR: 9.055e-04  Data: 0.012 (0.014)
Train: 60 [ 550/1251 ( 44%)]  Loss: 3.667 (3.89)  Time: 0.994s, 1029.93/s  (1.005s, 1018.95/s)  LR: 9.055e-04  Data: 0.011 (0.014)
Train: 60 [ 600/1251 ( 48%)]  Loss: 3.772 (3.88)  Time: 0.992s, 1032.40/s  (1.006s, 1017.51/s)  LR: 9.055e-04  Data: 0.010 (0.014)
Train: 60 [ 650/1251 ( 52%)]  Loss: 3.919 (3.88)  Time: 1.004s, 1019.67/s  (1.006s, 1017.53/s)  LR: 9.055e-04  Data: 0.014 (0.014)
Train: 60 [ 700/1251 ( 56%)]  Loss: 4.093 (3.90)  Time: 0.996s, 1028.20/s  (1.006s, 1017.95/s)  LR: 9.055e-04  Data: 0.011 (0.014)
Train: 60 [ 750/1251 ( 60%)]  Loss: 4.023 (3.91)  Time: 1.001s, 1023.48/s  (1.005s, 1018.58/s)  LR: 9.055e-04  Data: 0.012 (0.013)
Train: 60 [ 800/1251 ( 64%)]  Loss: 3.810 (3.90)  Time: 1.005s, 1018.90/s  (1.005s, 1018.94/s)  LR: 9.055e-04  Data: 0.011 (0.013)
Train: 60 [ 850/1251 ( 68%)]  Loss: 3.949 (3.90)  Time: 0.995s, 1028.84/s  (1.005s, 1018.90/s)  LR: 9.055e-04  Data: 0.011 (0.013)
Train: 60 [ 900/1251 ( 72%)]  Loss: 3.883 (3.90)  Time: 0.996s, 1028.41/s  (1.005s, 1018.76/s)  LR: 9.055e-04  Data: 0.012 (0.013)
Train: 60 [ 950/1251 ( 76%)]  Loss: 3.571 (3.89)  Time: 0.996s, 1027.90/s  (1.005s, 1018.79/s)  LR: 9.055e-04  Data: 0.011 (0.013)
Train: 60 [1000/1251 ( 80%)]  Loss: 3.553 (3.87)  Time: 1.048s,  976.84/s  (1.005s, 1018.68/s)  LR: 9.055e-04  Data: 0.011 (0.013)
Train: 60 [1050/1251 ( 84%)]  Loss: 3.884 (3.87)  Time: 0.999s, 1025.42/s  (1.005s, 1018.61/s)  LR: 9.055e-04  Data: 0.011 (0.013)
Train: 60 [1100/1251 ( 88%)]  Loss: 3.727 (3.86)  Time: 0.999s, 1024.79/s  (1.005s, 1018.92/s)  LR: 9.055e-04  Data: 0.011 (0.013)
Train: 60 [1150/1251 ( 92%)]  Loss: 3.959 (3.87)  Time: 0.997s, 1027.27/s  (1.005s, 1019.11/s)  LR: 9.055e-04  Data: 0.011 (0.013)
Train: 60 [1200/1251 ( 96%)]  Loss: 3.751 (3.86)  Time: 0.996s, 1027.84/s  (1.005s, 1018.93/s)  LR: 9.055e-04  Data: 0.011 (0.013)
Train: 60 [1250/1251 (100%)]  Loss: 3.556 (3.85)  Time: 0.981s, 1043.43/s  (1.005s, 1019.16/s)  LR: 9.055e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.603 (1.603)  Loss:  0.9217 (0.9217)  Acc@1: 85.7422 (85.7422)  Acc@5: 95.4102 (95.4102)
Test: [  48/48]  Time: 0.245 (0.567)  Loss:  1.0066 (1.5836)  Acc@1: 81.9576 (68.9140)  Acc@5: 94.4576 (89.0240)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-57.pth.tar', 70.31200004638671)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-58.pth.tar', 69.83400007080078)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-59.pth.tar', 69.78600005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-54.pth.tar', 69.67200004394532)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-55.pth.tar', 69.63800004882812)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-53.pth.tar', 69.5940000415039)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-56.pth.tar', 69.53599999267578)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-50.pth.tar', 69.03400003173829)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-60.pth.tar', 68.91400004882813)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-51.pth.tar', 68.85200004882813)

Train: 61 [   0/1251 (  0%)]  Loss: 4.254 (4.25)  Time: 2.492s,  410.86/s  (2.492s,  410.86/s)  LR: 9.024e-04  Data: 1.534 (1.534)
Train: 61 [  50/1251 (  4%)]  Loss: 3.899 (4.08)  Time: 1.001s, 1023.48/s  (1.033s,  990.92/s)  LR: 9.024e-04  Data: 0.011 (0.041)
Train: 61 [ 100/1251 (  8%)]  Loss: 3.986 (4.05)  Time: 0.995s, 1029.40/s  (1.022s, 1002.35/s)  LR: 9.024e-04  Data: 0.011 (0.026)
Train: 61 [ 150/1251 ( 12%)]  Loss: 4.100 (4.06)  Time: 0.997s, 1027.12/s  (1.016s, 1007.62/s)  LR: 9.024e-04  Data: 0.011 (0.021)
Train: 61 [ 200/1251 ( 16%)]  Loss: 3.964 (4.04)  Time: 0.995s, 1029.28/s  (1.018s, 1005.66/s)  LR: 9.024e-04  Data: 0.011 (0.019)
Train: 61 [ 250/1251 ( 20%)]  Loss: 3.693 (3.98)  Time: 0.993s, 1031.33/s  (1.015s, 1008.41/s)  LR: 9.024e-04  Data: 0.011 (0.017)
Train: 61 [ 300/1251 ( 24%)]  Loss: 3.705 (3.94)  Time: 0.995s, 1029.45/s  (1.014s, 1009.57/s)  LR: 9.024e-04  Data: 0.012 (0.016)
Train: 61 [ 350/1251 ( 28%)]  Loss: 3.826 (3.93)  Time: 0.995s, 1029.50/s  (1.013s, 1010.70/s)  LR: 9.024e-04  Data: 0.012 (0.016)
Train: 61 [ 400/1251 ( 32%)]  Loss: 3.377 (3.87)  Time: 1.036s,  988.44/s  (1.012s, 1011.83/s)  LR: 9.024e-04  Data: 0.011 (0.015)
Train: 61 [ 450/1251 ( 36%)]  Loss: 3.637 (3.84)  Time: 0.996s, 1028.41/s  (1.011s, 1013.26/s)  LR: 9.024e-04  Data: 0.012 (0.015)
Train: 61 [ 500/1251 ( 40%)]  Loss: 3.653 (3.83)  Time: 1.002s, 1021.74/s  (1.011s, 1012.84/s)  LR: 9.024e-04  Data: 0.012 (0.014)
Train: 61 [ 550/1251 ( 44%)]  Loss: 4.038 (3.84)  Time: 0.997s, 1026.71/s  (1.010s, 1014.17/s)  LR: 9.024e-04  Data: 0.011 (0.014)
Train: 61 [ 600/1251 ( 48%)]  Loss: 4.281 (3.88)  Time: 0.996s, 1027.87/s  (1.011s, 1012.99/s)  LR: 9.024e-04  Data: 0.011 (0.014)
Train: 61 [ 650/1251 ( 52%)]  Loss: 4.015 (3.89)  Time: 0.997s, 1026.65/s  (1.011s, 1012.83/s)  LR: 9.024e-04  Data: 0.012 (0.014)
Train: 61 [ 700/1251 ( 56%)]  Loss: 3.439 (3.86)  Time: 0.999s, 1025.19/s  (1.011s, 1012.92/s)  LR: 9.024e-04  Data: 0.011 (0.013)
Train: 61 [ 750/1251 ( 60%)]  Loss: 4.192 (3.88)  Time: 0.997s, 1026.59/s  (1.011s, 1013.32/s)  LR: 9.024e-04  Data: 0.012 (0.013)
Train: 61 [ 800/1251 ( 64%)]  Loss: 3.511 (3.86)  Time: 0.994s, 1030.59/s  (1.010s, 1014.03/s)  LR: 9.024e-04  Data: 0.011 (0.013)
Train: 61 [ 850/1251 ( 68%)]  Loss: 3.652 (3.85)  Time: 0.996s, 1027.62/s  (1.010s, 1014.22/s)  LR: 9.024e-04  Data: 0.011 (0.013)
Train: 61 [ 900/1251 ( 72%)]  Loss: 4.126 (3.86)  Time: 0.992s, 1032.76/s  (1.010s, 1013.72/s)  LR: 9.024e-04  Data: 0.010 (0.013)
Train: 61 [ 950/1251 ( 76%)]  Loss: 3.972 (3.87)  Time: 0.993s, 1031.44/s  (1.010s, 1014.24/s)  LR: 9.024e-04  Data: 0.011 (0.013)
Train: 61 [1000/1251 ( 80%)]  Loss: 4.111 (3.88)  Time: 1.001s, 1023.41/s  (1.009s, 1014.77/s)  LR: 9.024e-04  Data: 0.011 (0.013)
Train: 61 [1050/1251 ( 84%)]  Loss: 4.055 (3.89)  Time: 1.006s, 1017.73/s  (1.009s, 1015.14/s)  LR: 9.024e-04  Data: 0.011 (0.013)
Train: 61 [1100/1251 ( 88%)]  Loss: 3.822 (3.88)  Time: 1.036s,  988.32/s  (1.009s, 1014.93/s)  LR: 9.024e-04  Data: 0.011 (0.013)
Train: 61 [1150/1251 ( 92%)]  Loss: 3.571 (3.87)  Time: 1.031s,  993.61/s  (1.009s, 1014.59/s)  LR: 9.024e-04  Data: 0.011 (0.013)
Train: 61 [1200/1251 ( 96%)]  Loss: 4.243 (3.88)  Time: 0.994s, 1029.83/s  (1.009s, 1014.99/s)  LR: 9.024e-04  Data: 0.011 (0.012)
Train: 61 [1250/1251 (100%)]  Loss: 3.850 (3.88)  Time: 0.983s, 1041.75/s  (1.009s, 1014.85/s)  LR: 9.024e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.651 (1.651)  Loss:  0.8680 (0.8680)  Acc@1: 85.4492 (85.4492)  Acc@5: 96.0938 (96.0938)
Test: [  48/48]  Time: 0.245 (0.569)  Loss:  0.8972 (1.4532)  Acc@1: 81.2500 (69.2960)  Acc@5: 95.1651 (89.6060)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-57.pth.tar', 70.31200004638671)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-58.pth.tar', 69.83400007080078)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-59.pth.tar', 69.78600005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-54.pth.tar', 69.67200004394532)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-55.pth.tar', 69.63800004882812)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-53.pth.tar', 69.5940000415039)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-56.pth.tar', 69.53599999267578)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-61.pth.tar', 69.296)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-50.pth.tar', 69.03400003173829)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-60.pth.tar', 68.91400004882813)

Train: 62 [   0/1251 (  0%)]  Loss: 4.179 (4.18)  Time: 2.499s,  409.75/s  (2.499s,  409.75/s)  LR: 8.993e-04  Data: 1.529 (1.529)
Train: 62 [  50/1251 (  4%)]  Loss: 4.033 (4.11)  Time: 0.998s, 1026.26/s  (1.035s,  989.53/s)  LR: 8.993e-04  Data: 0.012 (0.041)
Train: 62 [ 100/1251 (  8%)]  Loss: 3.797 (4.00)  Time: 0.997s, 1026.70/s  (1.018s, 1006.09/s)  LR: 8.993e-04  Data: 0.012 (0.026)
Train: 62 [ 150/1251 ( 12%)]  Loss: 3.804 (3.95)  Time: 0.997s, 1026.90/s  (1.016s, 1007.56/s)  LR: 8.993e-04  Data: 0.011 (0.021)
Train: 62 [ 200/1251 ( 16%)]  Loss: 3.440 (3.85)  Time: 1.002s, 1021.91/s  (1.013s, 1010.67/s)  LR: 8.993e-04  Data: 0.011 (0.019)
Train: 62 [ 250/1251 ( 20%)]  Loss: 3.831 (3.85)  Time: 1.003s, 1020.70/s  (1.011s, 1012.87/s)  LR: 8.993e-04  Data: 0.011 (0.017)
Train: 62 [ 300/1251 ( 24%)]  Loss: 3.798 (3.84)  Time: 0.995s, 1028.82/s  (1.009s, 1014.62/s)  LR: 8.993e-04  Data: 0.011 (0.016)
Train: 62 [ 350/1251 ( 28%)]  Loss: 3.981 (3.86)  Time: 1.001s, 1023.00/s  (1.008s, 1015.50/s)  LR: 8.993e-04  Data: 0.011 (0.016)
Train: 62 [ 400/1251 ( 32%)]  Loss: 4.114 (3.89)  Time: 0.997s, 1026.78/s  (1.009s, 1015.11/s)  LR: 8.993e-04  Data: 0.011 (0.015)
Train: 62 [ 450/1251 ( 36%)]  Loss: 3.909 (3.89)  Time: 0.990s, 1034.44/s  (1.008s, 1015.74/s)  LR: 8.993e-04  Data: 0.010 (0.015)
Train: 62 [ 500/1251 ( 40%)]  Loss: 3.943 (3.89)  Time: 1.017s, 1006.50/s  (1.007s, 1016.39/s)  LR: 8.993e-04  Data: 0.011 (0.014)
Train: 62 [ 550/1251 ( 44%)]  Loss: 3.871 (3.89)  Time: 0.993s, 1031.53/s  (1.007s, 1016.90/s)  LR: 8.993e-04  Data: 0.011 (0.014)
Train: 62 [ 600/1251 ( 48%)]  Loss: 3.953 (3.90)  Time: 1.000s, 1023.91/s  (1.007s, 1017.15/s)  LR: 8.993e-04  Data: 0.015 (0.014)
Train: 62 [ 650/1251 ( 52%)]  Loss: 3.856 (3.89)  Time: 0.993s, 1030.79/s  (1.007s, 1017.15/s)  LR: 8.993e-04  Data: 0.011 (0.014)
Train: 62 [ 700/1251 ( 56%)]  Loss: 3.735 (3.88)  Time: 0.996s, 1028.62/s  (1.007s, 1017.06/s)  LR: 8.993e-04  Data: 0.011 (0.013)
Train: 62 [ 750/1251 ( 60%)]  Loss: 3.931 (3.89)  Time: 1.015s, 1009.11/s  (1.007s, 1016.91/s)  LR: 8.993e-04  Data: 0.011 (0.013)
Train: 62 [ 800/1251 ( 64%)]  Loss: 3.793 (3.88)  Time: 0.993s, 1030.76/s  (1.007s, 1017.04/s)  LR: 8.993e-04  Data: 0.011 (0.013)
Train: 62 [ 850/1251 ( 68%)]  Loss: 4.176 (3.90)  Time: 0.995s, 1029.26/s  (1.007s, 1017.31/s)  LR: 8.993e-04  Data: 0.011 (0.013)
Train: 62 [ 900/1251 ( 72%)]  Loss: 3.776 (3.89)  Time: 1.045s,  980.34/s  (1.007s, 1017.07/s)  LR: 8.993e-04  Data: 0.012 (0.013)
Train: 62 [ 950/1251 ( 76%)]  Loss: 4.027 (3.90)  Time: 0.995s, 1029.51/s  (1.007s, 1017.24/s)  LR: 8.993e-04  Data: 0.012 (0.013)
Train: 62 [1000/1251 ( 80%)]  Loss: 3.590 (3.88)  Time: 0.996s, 1028.32/s  (1.007s, 1017.26/s)  LR: 8.993e-04  Data: 0.011 (0.013)
Train: 62 [1050/1251 ( 84%)]  Loss: 4.176 (3.90)  Time: 0.995s, 1029.54/s  (1.006s, 1017.57/s)  LR: 8.993e-04  Data: 0.011 (0.013)
Train: 62 [1100/1251 ( 88%)]  Loss: 4.033 (3.90)  Time: 0.995s, 1029.44/s  (1.006s, 1017.40/s)  LR: 8.993e-04  Data: 0.012 (0.013)
Train: 62 [1150/1251 ( 92%)]  Loss: 3.475 (3.88)  Time: 0.993s, 1031.43/s  (1.006s, 1017.56/s)  LR: 8.993e-04  Data: 0.011 (0.013)
Train: 62 [1200/1251 ( 96%)]  Loss: 3.524 (3.87)  Time: 1.001s, 1023.33/s  (1.006s, 1017.82/s)  LR: 8.993e-04  Data: 0.012 (0.012)
Train: 62 [1250/1251 (100%)]  Loss: 4.090 (3.88)  Time: 0.984s, 1041.12/s  (1.006s, 1018.23/s)  LR: 8.993e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.659 (1.659)  Loss:  0.9331 (0.9331)  Acc@1: 84.6680 (84.6680)  Acc@5: 95.8008 (95.8008)
Test: [  48/48]  Time: 0.245 (0.570)  Loss:  1.0058 (1.5325)  Acc@1: 82.3113 (69.9260)  Acc@5: 94.2217 (89.9080)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-57.pth.tar', 70.31200004638671)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-62.pth.tar', 69.92599994384766)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-58.pth.tar', 69.83400007080078)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-59.pth.tar', 69.78600005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-54.pth.tar', 69.67200004394532)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-55.pth.tar', 69.63800004882812)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-53.pth.tar', 69.5940000415039)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-56.pth.tar', 69.53599999267578)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-61.pth.tar', 69.296)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-50.pth.tar', 69.03400003173829)

Train: 63 [   0/1251 (  0%)]  Loss: 3.594 (3.59)  Time: 2.434s,  420.68/s  (2.434s,  420.68/s)  LR: 8.961e-04  Data: 1.472 (1.472)
Train: 63 [  50/1251 (  4%)]  Loss: 4.117 (3.86)  Time: 0.995s, 1029.48/s  (1.055s,  970.39/s)  LR: 8.961e-04  Data: 0.012 (0.040)
Train: 63 [ 100/1251 (  8%)]  Loss: 4.224 (3.98)  Time: 0.995s, 1028.72/s  (1.029s,  995.59/s)  LR: 8.961e-04  Data: 0.011 (0.026)
Train: 63 [ 150/1251 ( 12%)]  Loss: 3.928 (3.97)  Time: 0.996s, 1028.25/s  (1.019s, 1005.32/s)  LR: 8.961e-04  Data: 0.012 (0.021)
Train: 63 [ 200/1251 ( 16%)]  Loss: 3.607 (3.89)  Time: 0.997s, 1026.96/s  (1.014s, 1009.64/s)  LR: 8.961e-04  Data: 0.011 (0.019)
Train: 63 [ 250/1251 ( 20%)]  Loss: 3.735 (3.87)  Time: 0.995s, 1028.73/s  (1.014s, 1009.84/s)  LR: 8.961e-04  Data: 0.011 (0.017)
Train: 63 [ 300/1251 ( 24%)]  Loss: 3.738 (3.85)  Time: 0.997s, 1027.38/s  (1.012s, 1012.34/s)  LR: 8.961e-04  Data: 0.011 (0.016)
Train: 63 [ 350/1251 ( 28%)]  Loss: 3.615 (3.82)  Time: 0.996s, 1028.41/s  (1.009s, 1014.39/s)  LR: 8.961e-04  Data: 0.011 (0.016)
Train: 63 [ 400/1251 ( 32%)]  Loss: 4.029 (3.84)  Time: 0.998s, 1026.25/s  (1.009s, 1015.03/s)  LR: 8.961e-04  Data: 0.011 (0.015)
Train: 63 [ 450/1251 ( 36%)]  Loss: 3.932 (3.85)  Time: 1.002s, 1021.99/s  (1.009s, 1014.54/s)  LR: 8.961e-04  Data: 0.011 (0.015)
Train: 63 [ 500/1251 ( 40%)]  Loss: 3.996 (3.87)  Time: 0.998s, 1026.28/s  (1.010s, 1013.91/s)  LR: 8.961e-04  Data: 0.011 (0.014)
Train: 63 [ 550/1251 ( 44%)]  Loss: 3.954 (3.87)  Time: 1.025s,  998.86/s  (1.010s, 1013.92/s)  LR: 8.961e-04  Data: 0.011 (0.014)
Train: 63 [ 600/1251 ( 48%)]  Loss: 3.516 (3.85)  Time: 0.996s, 1027.73/s  (1.011s, 1013.20/s)  LR: 8.961e-04  Data: 0.011 (0.014)
Train: 63 [ 650/1251 ( 52%)]  Loss: 3.809 (3.84)  Time: 1.080s,  948.38/s  (1.011s, 1012.81/s)  LR: 8.961e-04  Data: 0.011 (0.014)
Train: 63 [ 700/1251 ( 56%)]  Loss: 3.954 (3.85)  Time: 0.999s, 1025.17/s  (1.012s, 1011.69/s)  LR: 8.961e-04  Data: 0.011 (0.013)
Train: 63 [ 750/1251 ( 60%)]  Loss: 3.949 (3.86)  Time: 0.996s, 1028.13/s  (1.014s, 1010.10/s)  LR: 8.961e-04  Data: 0.012 (0.013)
Train: 63 [ 800/1251 ( 64%)]  Loss: 3.762 (3.85)  Time: 0.995s, 1029.59/s  (1.013s, 1011.00/s)  LR: 8.961e-04  Data: 0.011 (0.013)
Train: 63 [ 850/1251 ( 68%)]  Loss: 4.048 (3.86)  Time: 0.993s, 1031.19/s  (1.012s, 1011.87/s)  LR: 8.961e-04  Data: 0.011 (0.013)
Train: 63 [ 900/1251 ( 72%)]  Loss: 3.789 (3.86)  Time: 0.997s, 1026.70/s  (1.012s, 1012.25/s)  LR: 8.961e-04  Data: 0.012 (0.013)
Train: 63 [ 950/1251 ( 76%)]  Loss: 3.921 (3.86)  Time: 0.997s, 1027.37/s  (1.011s, 1013.03/s)  LR: 8.961e-04  Data: 0.012 (0.013)
Train: 63 [1000/1251 ( 80%)]  Loss: 3.824 (3.86)  Time: 1.044s,  980.84/s  (1.011s, 1012.36/s)  LR: 8.961e-04  Data: 0.011 (0.013)
Train: 63 [1050/1251 ( 84%)]  Loss: 4.117 (3.87)  Time: 0.996s, 1028.49/s  (1.011s, 1012.44/s)  LR: 8.961e-04  Data: 0.012 (0.013)
Train: 63 [1100/1251 ( 88%)]  Loss: 3.931 (3.87)  Time: 1.039s,  985.27/s  (1.012s, 1012.35/s)  LR: 8.961e-04  Data: 0.011 (0.013)
Train: 63 [1150/1251 ( 92%)]  Loss: 3.997 (3.88)  Time: 1.000s, 1024.48/s  (1.011s, 1012.41/s)  LR: 8.961e-04  Data: 0.011 (0.013)
Train: 63 [1200/1251 ( 96%)]  Loss: 4.088 (3.89)  Time: 0.995s, 1029.21/s  (1.011s, 1012.72/s)  LR: 8.961e-04  Data: 0.011 (0.012)
Train: 63 [1250/1251 (100%)]  Loss: 3.969 (3.89)  Time: 0.982s, 1042.59/s  (1.011s, 1013.25/s)  LR: 8.961e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.687 (1.687)  Loss:  0.9260 (0.9260)  Acc@1: 87.1094 (87.1094)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.245 (0.573)  Loss:  1.0055 (1.5411)  Acc@1: 81.3679 (70.3640)  Acc@5: 95.1651 (90.1300)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-63.pth.tar', 70.36400005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-57.pth.tar', 70.31200004638671)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-62.pth.tar', 69.92599994384766)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-58.pth.tar', 69.83400007080078)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-59.pth.tar', 69.78600005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-54.pth.tar', 69.67200004394532)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-55.pth.tar', 69.63800004882812)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-53.pth.tar', 69.5940000415039)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-56.pth.tar', 69.53599999267578)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-61.pth.tar', 69.296)

Train: 64 [   0/1251 (  0%)]  Loss: 3.837 (3.84)  Time: 2.416s,  423.91/s  (2.416s,  423.91/s)  LR: 8.929e-04  Data: 1.465 (1.465)
Train: 64 [  50/1251 (  4%)]  Loss: 3.661 (3.75)  Time: 0.996s, 1028.34/s  (1.036s,  988.45/s)  LR: 8.929e-04  Data: 0.011 (0.039)
Train: 64 [ 100/1251 (  8%)]  Loss: 3.849 (3.78)  Time: 1.036s,  988.35/s  (1.025s,  999.40/s)  LR: 8.929e-04  Data: 0.011 (0.026)
Train: 64 [ 150/1251 ( 12%)]  Loss: 3.947 (3.82)  Time: 0.997s, 1027.28/s  (1.017s, 1007.14/s)  LR: 8.929e-04  Data: 0.011 (0.021)
Train: 64 [ 200/1251 ( 16%)]  Loss: 3.678 (3.79)  Time: 0.994s, 1030.52/s  (1.012s, 1011.72/s)  LR: 8.929e-04  Data: 0.011 (0.018)
Train: 64 [ 250/1251 ( 20%)]  Loss: 4.330 (3.88)  Time: 0.992s, 1031.85/s  (1.010s, 1014.07/s)  LR: 8.929e-04  Data: 0.011 (0.017)
Train: 64 [ 300/1251 ( 24%)]  Loss: 3.663 (3.85)  Time: 0.996s, 1028.28/s  (1.008s, 1015.66/s)  LR: 8.929e-04  Data: 0.012 (0.016)
Train: 64 [ 350/1251 ( 28%)]  Loss: 3.597 (3.82)  Time: 1.003s, 1020.54/s  (1.008s, 1015.86/s)  LR: 8.929e-04  Data: 0.011 (0.015)
Train: 64 [ 400/1251 ( 32%)]  Loss: 3.808 (3.82)  Time: 0.993s, 1030.96/s  (1.007s, 1016.38/s)  LR: 8.929e-04  Data: 0.011 (0.015)
Train: 64 [ 450/1251 ( 36%)]  Loss: 3.579 (3.79)  Time: 0.996s, 1028.51/s  (1.008s, 1016.20/s)  LR: 8.929e-04  Data: 0.012 (0.014)
Train: 64 [ 500/1251 ( 40%)]  Loss: 3.837 (3.80)  Time: 0.994s, 1030.17/s  (1.010s, 1014.00/s)  LR: 8.929e-04  Data: 0.011 (0.014)
Train: 64 [ 550/1251 ( 44%)]  Loss: 3.960 (3.81)  Time: 0.996s, 1027.70/s  (1.009s, 1014.89/s)  LR: 8.929e-04  Data: 0.011 (0.014)
Train: 64 [ 600/1251 ( 48%)]  Loss: 3.800 (3.81)  Time: 1.000s, 1024.42/s  (1.008s, 1015.70/s)  LR: 8.929e-04  Data: 0.011 (0.014)
Train: 64 [ 650/1251 ( 52%)]  Loss: 4.115 (3.83)  Time: 0.995s, 1029.66/s  (1.007s, 1016.39/s)  LR: 8.929e-04  Data: 0.010 (0.014)
Train: 64 [ 700/1251 ( 56%)]  Loss: 3.981 (3.84)  Time: 0.994s, 1029.92/s  (1.007s, 1017.03/s)  LR: 8.929e-04  Data: 0.011 (0.013)
Train: 64 [ 750/1251 ( 60%)]  Loss: 4.119 (3.86)  Time: 0.996s, 1028.06/s  (1.007s, 1017.19/s)  LR: 8.929e-04  Data: 0.011 (0.013)
Train: 64 [ 800/1251 ( 64%)]  Loss: 4.008 (3.87)  Time: 0.999s, 1025.44/s  (1.007s, 1016.49/s)  LR: 8.929e-04  Data: 0.012 (0.013)
Train: 64 [ 850/1251 ( 68%)]  Loss: 4.174 (3.89)  Time: 0.997s, 1027.08/s  (1.007s, 1016.46/s)  LR: 8.929e-04  Data: 0.011 (0.013)
Train: 64 [ 900/1251 ( 72%)]  Loss: 3.778 (3.88)  Time: 0.998s, 1026.32/s  (1.007s, 1016.77/s)  LR: 8.929e-04  Data: 0.010 (0.013)
Train: 64 [ 950/1251 ( 76%)]  Loss: 3.480 (3.86)  Time: 0.999s, 1025.51/s  (1.007s, 1016.90/s)  LR: 8.929e-04  Data: 0.012 (0.013)
Train: 64 [1000/1251 ( 80%)]  Loss: 3.767 (3.86)  Time: 0.993s, 1030.73/s  (1.007s, 1017.04/s)  LR: 8.929e-04  Data: 0.011 (0.013)
Train: 64 [1050/1251 ( 84%)]  Loss: 3.715 (3.85)  Time: 0.997s, 1027.28/s  (1.006s, 1017.58/s)  LR: 8.929e-04  Data: 0.011 (0.013)
Train: 64 [1100/1251 ( 88%)]  Loss: 3.770 (3.85)  Time: 0.996s, 1027.76/s  (1.006s, 1017.90/s)  LR: 8.929e-04  Data: 0.010 (0.013)
Train: 64 [1150/1251 ( 92%)]  Loss: 4.056 (3.85)  Time: 1.002s, 1021.50/s  (1.007s, 1017.21/s)  LR: 8.929e-04  Data: 0.010 (0.012)
Train: 64 [1200/1251 ( 96%)]  Loss: 3.526 (3.84)  Time: 0.997s, 1026.79/s  (1.006s, 1017.47/s)  LR: 8.929e-04  Data: 0.011 (0.012)
Train: 64 [1250/1251 (100%)]  Loss: 4.114 (3.85)  Time: 0.982s, 1042.26/s  (1.006s, 1017.89/s)  LR: 8.929e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.670 (1.670)  Loss:  0.9160 (0.9160)  Acc@1: 86.8164 (86.8164)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.245 (0.570)  Loss:  1.0847 (1.5635)  Acc@1: 81.1321 (69.8240)  Acc@5: 95.5189 (89.9580)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-63.pth.tar', 70.36400005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-57.pth.tar', 70.31200004638671)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-62.pth.tar', 69.92599994384766)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-58.pth.tar', 69.83400007080078)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-64.pth.tar', 69.824000078125)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-59.pth.tar', 69.78600005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-54.pth.tar', 69.67200004394532)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-55.pth.tar', 69.63800004882812)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-53.pth.tar', 69.5940000415039)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-56.pth.tar', 69.53599999267578)

Train: 65 [   0/1251 (  0%)]  Loss: 4.090 (4.09)  Time: 2.405s,  425.72/s  (2.405s,  425.72/s)  LR: 8.897e-04  Data: 1.443 (1.443)
Train: 65 [  50/1251 (  4%)]  Loss: 3.816 (3.95)  Time: 1.005s, 1018.96/s  (1.030s,  994.41/s)  LR: 8.897e-04  Data: 0.011 (0.040)
Train: 65 [ 100/1251 (  8%)]  Loss: 3.697 (3.87)  Time: 0.999s, 1025.29/s  (1.016s, 1008.00/s)  LR: 8.897e-04  Data: 0.011 (0.026)
Train: 65 [ 150/1251 ( 12%)]  Loss: 3.866 (3.87)  Time: 0.995s, 1028.70/s  (1.010s, 1013.63/s)  LR: 8.897e-04  Data: 0.011 (0.021)
Train: 65 [ 200/1251 ( 16%)]  Loss: 4.112 (3.92)  Time: 0.997s, 1027.49/s  (1.008s, 1015.53/s)  LR: 8.897e-04  Data: 0.012 (0.018)
Train: 65 [ 250/1251 ( 20%)]  Loss: 3.954 (3.92)  Time: 0.996s, 1028.00/s  (1.007s, 1017.38/s)  LR: 8.897e-04  Data: 0.011 (0.017)
Train: 65 [ 300/1251 ( 24%)]  Loss: 3.722 (3.89)  Time: 1.039s,  985.66/s  (1.006s, 1018.03/s)  LR: 8.897e-04  Data: 0.011 (0.016)
Train: 65 [ 350/1251 ( 28%)]  Loss: 4.162 (3.93)  Time: 1.043s,  981.82/s  (1.006s, 1017.73/s)  LR: 8.897e-04  Data: 0.010 (0.015)
Train: 65 [ 400/1251 ( 32%)]  Loss: 4.090 (3.95)  Time: 0.995s, 1029.49/s  (1.006s, 1018.28/s)  LR: 8.897e-04  Data: 0.012 (0.015)
Train: 65 [ 450/1251 ( 36%)]  Loss: 3.976 (3.95)  Time: 0.999s, 1024.55/s  (1.006s, 1017.72/s)  LR: 8.897e-04  Data: 0.012 (0.014)
Train: 65 [ 500/1251 ( 40%)]  Loss: 3.880 (3.94)  Time: 0.997s, 1026.92/s  (1.007s, 1017.16/s)  LR: 8.897e-04  Data: 0.012 (0.014)
Train: 65 [ 550/1251 ( 44%)]  Loss: 3.669 (3.92)  Time: 0.997s, 1026.80/s  (1.006s, 1017.95/s)  LR: 8.897e-04  Data: 0.010 (0.014)
Train: 65 [ 600/1251 ( 48%)]  Loss: 3.909 (3.92)  Time: 0.997s, 1027.27/s  (1.005s, 1018.59/s)  LR: 8.897e-04  Data: 0.011 (0.014)
Train: 65 [ 650/1251 ( 52%)]  Loss: 3.579 (3.89)  Time: 0.996s, 1028.51/s  (1.006s, 1018.00/s)  LR: 8.897e-04  Data: 0.011 (0.013)
Train: 65 [ 700/1251 ( 56%)]  Loss: 3.781 (3.89)  Time: 1.004s, 1019.66/s  (1.006s, 1018.16/s)  LR: 8.897e-04  Data: 0.011 (0.013)
Train: 65 [ 750/1251 ( 60%)]  Loss: 3.687 (3.87)  Time: 1.000s, 1024.29/s  (1.006s, 1018.35/s)  LR: 8.897e-04  Data: 0.014 (0.013)
Train: 65 [ 800/1251 ( 64%)]  Loss: 3.554 (3.86)  Time: 0.997s, 1027.04/s  (1.006s, 1018.02/s)  LR: 8.897e-04  Data: 0.012 (0.013)
Train: 65 [ 850/1251 ( 68%)]  Loss: 4.035 (3.87)  Time: 1.030s,  993.80/s  (1.007s, 1017.14/s)  LR: 8.897e-04  Data: 0.011 (0.013)
Train: 65 [ 900/1251 ( 72%)]  Loss: 3.806 (3.86)  Time: 1.046s,  978.95/s  (1.007s, 1016.45/s)  LR: 8.897e-04  Data: 0.010 (0.013)
Train: 65 [ 950/1251 ( 76%)]  Loss: 3.990 (3.87)  Time: 1.059s,  966.57/s  (1.008s, 1015.69/s)  LR: 8.897e-04  Data: 0.011 (0.013)
Train: 65 [1000/1251 ( 80%)]  Loss: 4.002 (3.87)  Time: 0.995s, 1029.37/s  (1.009s, 1014.70/s)  LR: 8.897e-04  Data: 0.011 (0.013)
Train: 65 [1050/1251 ( 84%)]  Loss: 3.772 (3.87)  Time: 0.996s, 1027.96/s  (1.009s, 1014.87/s)  LR: 8.897e-04  Data: 0.011 (0.012)
Train: 65 [1100/1251 ( 88%)]  Loss: 3.874 (3.87)  Time: 0.996s, 1028.35/s  (1.009s, 1014.76/s)  LR: 8.897e-04  Data: 0.011 (0.012)
Train: 65 [1150/1251 ( 92%)]  Loss: 3.958 (3.87)  Time: 1.006s, 1017.71/s  (1.009s, 1014.80/s)  LR: 8.897e-04  Data: 0.012 (0.012)
Train: 65 [1200/1251 ( 96%)]  Loss: 3.578 (3.86)  Time: 1.061s,  965.12/s  (1.009s, 1014.78/s)  LR: 8.897e-04  Data: 0.011 (0.012)
Train: 65 [1250/1251 (100%)]  Loss: 4.070 (3.87)  Time: 0.998s, 1026.53/s  (1.009s, 1015.06/s)  LR: 8.897e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.677 (1.677)  Loss:  0.7335 (0.7335)  Acc@1: 86.3281 (86.3281)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.245 (0.565)  Loss:  0.9081 (1.4521)  Acc@1: 80.0708 (70.3220)  Acc@5: 95.5189 (90.1680)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-63.pth.tar', 70.36400005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-65.pth.tar', 70.32200000488281)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-57.pth.tar', 70.31200004638671)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-62.pth.tar', 69.92599994384766)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-58.pth.tar', 69.83400007080078)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-64.pth.tar', 69.824000078125)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-59.pth.tar', 69.78600005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-54.pth.tar', 69.67200004394532)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-55.pth.tar', 69.63800004882812)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-53.pth.tar', 69.5940000415039)

Train: 66 [   0/1251 (  0%)]  Loss: 3.561 (3.56)  Time: 4.242s,  241.41/s  (4.242s,  241.41/s)  LR: 8.864e-04  Data: 2.967 (2.967)
Train: 66 [  50/1251 (  4%)]  Loss: 4.218 (3.89)  Time: 0.996s, 1028.33/s  (1.065s,  961.29/s)  LR: 8.864e-04  Data: 0.011 (0.069)
Train: 66 [ 100/1251 (  8%)]  Loss: 4.173 (3.98)  Time: 1.001s, 1022.77/s  (1.032s,  992.55/s)  LR: 8.864e-04  Data: 0.011 (0.040)
Train: 66 [ 150/1251 ( 12%)]  Loss: 3.731 (3.92)  Time: 1.035s,  989.23/s  (1.022s, 1001.85/s)  LR: 8.864e-04  Data: 0.011 (0.031)
Train: 66 [ 200/1251 ( 16%)]  Loss: 3.640 (3.86)  Time: 1.005s, 1019.27/s  (1.018s, 1005.45/s)  LR: 8.864e-04  Data: 0.012 (0.026)
Train: 66 [ 250/1251 ( 20%)]  Loss: 3.944 (3.88)  Time: 1.005s, 1018.57/s  (1.016s, 1008.04/s)  LR: 8.864e-04  Data: 0.012 (0.023)
Train: 66 [ 300/1251 ( 24%)]  Loss: 3.926 (3.88)  Time: 0.994s, 1030.45/s  (1.013s, 1010.55/s)  LR: 8.864e-04  Data: 0.010 (0.021)
Train: 66 [ 350/1251 ( 28%)]  Loss: 3.616 (3.85)  Time: 0.997s, 1027.40/s  (1.013s, 1010.86/s)  LR: 8.864e-04  Data: 0.011 (0.019)
Train: 66 [ 400/1251 ( 32%)]  Loss: 3.585 (3.82)  Time: 1.058s,  967.57/s  (1.014s, 1010.01/s)  LR: 8.864e-04  Data: 0.011 (0.018)
Train: 66 [ 450/1251 ( 36%)]  Loss: 3.961 (3.84)  Time: 0.993s, 1031.05/s  (1.015s, 1008.82/s)  LR: 8.864e-04  Data: 0.010 (0.018)
Train: 66 [ 500/1251 ( 40%)]  Loss: 3.675 (3.82)  Time: 1.002s, 1021.87/s  (1.013s, 1010.69/s)  LR: 8.864e-04  Data: 0.011 (0.017)
Train: 66 [ 550/1251 ( 44%)]  Loss: 3.237 (3.77)  Time: 1.037s,  987.68/s  (1.013s, 1010.75/s)  LR: 8.864e-04  Data: 0.010 (0.016)
Train: 66 [ 600/1251 ( 48%)]  Loss: 3.717 (3.77)  Time: 0.997s, 1026.95/s  (1.014s, 1009.91/s)  LR: 8.864e-04  Data: 0.011 (0.016)
Train: 66 [ 650/1251 ( 52%)]  Loss: 3.538 (3.75)  Time: 0.997s, 1027.31/s  (1.014s, 1009.58/s)  LR: 8.864e-04  Data: 0.011 (0.016)
Train: 66 [ 700/1251 ( 56%)]  Loss: 4.056 (3.77)  Time: 0.995s, 1028.94/s  (1.013s, 1010.84/s)  LR: 8.864e-04  Data: 0.011 (0.015)
Train: 66 [ 750/1251 ( 60%)]  Loss: 3.857 (3.78)  Time: 1.001s, 1022.80/s  (1.012s, 1011.77/s)  LR: 8.864e-04  Data: 0.011 (0.015)
Train: 66 [ 800/1251 ( 64%)]  Loss: 4.105 (3.80)  Time: 0.997s, 1027.48/s  (1.011s, 1012.53/s)  LR: 8.864e-04  Data: 0.011 (0.015)
Train: 66 [ 850/1251 ( 68%)]  Loss: 3.819 (3.80)  Time: 0.995s, 1029.00/s  (1.011s, 1013.20/s)  LR: 8.864e-04  Data: 0.011 (0.015)
Train: 66 [ 900/1251 ( 72%)]  Loss: 3.512 (3.78)  Time: 1.004s, 1019.98/s  (1.010s, 1013.86/s)  LR: 8.864e-04  Data: 0.011 (0.014)
Train: 66 [ 950/1251 ( 76%)]  Loss: 3.655 (3.78)  Time: 0.997s, 1027.55/s  (1.009s, 1014.57/s)  LR: 8.864e-04  Data: 0.011 (0.014)
Train: 66 [1000/1251 ( 80%)]  Loss: 3.920 (3.78)  Time: 0.999s, 1024.58/s  (1.009s, 1014.83/s)  LR: 8.864e-04  Data: 0.012 (0.014)
Train: 66 [1050/1251 ( 84%)]  Loss: 4.032 (3.79)  Time: 0.995s, 1029.42/s  (1.008s, 1015.37/s)  LR: 8.864e-04  Data: 0.010 (0.014)
Train: 66 [1100/1251 ( 88%)]  Loss: 3.666 (3.79)  Time: 1.041s,  983.48/s  (1.008s, 1015.72/s)  LR: 8.864e-04  Data: 0.010 (0.014)
Train: 66 [1150/1251 ( 92%)]  Loss: 3.826 (3.79)  Time: 1.009s, 1014.53/s  (1.008s, 1015.86/s)  LR: 8.864e-04  Data: 0.011 (0.014)
Train: 66 [1200/1251 ( 96%)]  Loss: 4.087 (3.80)  Time: 0.996s, 1028.62/s  (1.008s, 1016.08/s)  LR: 8.864e-04  Data: 0.011 (0.014)
Train: 66 [1250/1251 (100%)]  Loss: 4.040 (3.81)  Time: 0.983s, 1041.59/s  (1.008s, 1016.18/s)  LR: 8.864e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.622 (1.622)  Loss:  0.9232 (0.9232)  Acc@1: 85.9375 (85.9375)  Acc@5: 95.6055 (95.6055)
Test: [  48/48]  Time: 0.245 (0.570)  Loss:  0.9518 (1.5062)  Acc@1: 82.4293 (69.8180)  Acc@5: 95.0472 (89.9620)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-63.pth.tar', 70.36400005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-65.pth.tar', 70.32200000488281)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-57.pth.tar', 70.31200004638671)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-62.pth.tar', 69.92599994384766)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-58.pth.tar', 69.83400007080078)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-64.pth.tar', 69.824000078125)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-66.pth.tar', 69.81800012451171)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-59.pth.tar', 69.78600005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-54.pth.tar', 69.67200004394532)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-55.pth.tar', 69.63800004882812)

Train: 67 [   0/1251 (  0%)]  Loss: 4.028 (4.03)  Time: 2.459s,  416.37/s  (2.459s,  416.37/s)  LR: 8.831e-04  Data: 1.493 (1.493)
Train: 67 [  50/1251 (  4%)]  Loss: 4.007 (4.02)  Time: 1.047s,  977.66/s  (1.046s,  979.25/s)  LR: 8.831e-04  Data: 0.011 (0.042)
Train: 67 [ 100/1251 (  8%)]  Loss: 3.882 (3.97)  Time: 1.036s,  988.42/s  (1.032s,  991.96/s)  LR: 8.831e-04  Data: 0.011 (0.027)
Train: 67 [ 150/1251 ( 12%)]  Loss: 3.886 (3.95)  Time: 1.034s,  990.29/s  (1.024s,  999.75/s)  LR: 8.831e-04  Data: 0.012 (0.022)
Train: 67 [ 200/1251 ( 16%)]  Loss: 3.530 (3.87)  Time: 0.998s, 1026.30/s  (1.022s, 1002.26/s)  LR: 8.831e-04  Data: 0.011 (0.019)
Train: 67 [ 250/1251 ( 20%)]  Loss: 4.248 (3.93)  Time: 0.991s, 1033.32/s  (1.019s, 1004.98/s)  LR: 8.831e-04  Data: 0.011 (0.017)
Train: 67 [ 300/1251 ( 24%)]  Loss: 3.792 (3.91)  Time: 1.022s, 1001.87/s  (1.016s, 1008.10/s)  LR: 8.831e-04  Data: 0.010 (0.016)
Train: 67 [ 350/1251 ( 28%)]  Loss: 3.474 (3.86)  Time: 1.033s,  991.49/s  (1.015s, 1008.66/s)  LR: 8.831e-04  Data: 0.011 (0.016)
Train: 67 [ 400/1251 ( 32%)]  Loss: 3.837 (3.85)  Time: 0.998s, 1026.04/s  (1.017s, 1006.98/s)  LR: 8.831e-04  Data: 0.011 (0.015)
Train: 67 [ 450/1251 ( 36%)]  Loss: 4.097 (3.88)  Time: 1.003s, 1021.23/s  (1.015s, 1009.07/s)  LR: 8.831e-04  Data: 0.011 (0.015)
Train: 67 [ 500/1251 ( 40%)]  Loss: 3.937 (3.88)  Time: 0.997s, 1027.18/s  (1.014s, 1010.17/s)  LR: 8.831e-04  Data: 0.011 (0.014)
Train: 67 [ 550/1251 ( 44%)]  Loss: 3.960 (3.89)  Time: 1.037s,  987.94/s  (1.014s, 1010.22/s)  LR: 8.831e-04  Data: 0.011 (0.014)
Train: 67 [ 600/1251 ( 48%)]  Loss: 3.718 (3.88)  Time: 1.043s,  982.22/s  (1.013s, 1010.45/s)  LR: 8.831e-04  Data: 0.010 (0.014)
Train: 67 [ 650/1251 ( 52%)]  Loss: 3.503 (3.85)  Time: 1.059s,  966.59/s  (1.014s, 1009.39/s)  LR: 8.831e-04  Data: 0.011 (0.014)
Train: 67 [ 700/1251 ( 56%)]  Loss: 3.774 (3.84)  Time: 0.994s, 1029.99/s  (1.015s, 1009.12/s)  LR: 8.831e-04  Data: 0.010 (0.013)
Train: 67 [ 750/1251 ( 60%)]  Loss: 3.343 (3.81)  Time: 0.996s, 1028.23/s  (1.014s, 1009.59/s)  LR: 8.831e-04  Data: 0.010 (0.013)
Train: 67 [ 800/1251 ( 64%)]  Loss: 3.995 (3.82)  Time: 0.998s, 1026.51/s  (1.013s, 1010.61/s)  LR: 8.831e-04  Data: 0.011 (0.013)
Train: 67 [ 850/1251 ( 68%)]  Loss: 4.109 (3.84)  Time: 1.005s, 1019.38/s  (1.013s, 1011.33/s)  LR: 8.831e-04  Data: 0.011 (0.013)
Train: 67 [ 900/1251 ( 72%)]  Loss: 4.019 (3.85)  Time: 0.995s, 1028.88/s  (1.013s, 1010.43/s)  LR: 8.831e-04  Data: 0.011 (0.013)
Train: 67 [ 950/1251 ( 76%)]  Loss: 3.970 (3.86)  Time: 1.002s, 1021.88/s  (1.013s, 1011.25/s)  LR: 8.831e-04  Data: 0.010 (0.013)
Train: 67 [1000/1251 ( 80%)]  Loss: 3.680 (3.85)  Time: 0.995s, 1029.13/s  (1.013s, 1011.13/s)  LR: 8.831e-04  Data: 0.011 (0.013)
Train: 67 [1050/1251 ( 84%)]  Loss: 4.143 (3.86)  Time: 1.043s,  982.00/s  (1.014s, 1010.34/s)  LR: 8.831e-04  Data: 0.011 (0.013)
Train: 67 [1100/1251 ( 88%)]  Loss: 3.605 (3.85)  Time: 0.996s, 1027.62/s  (1.013s, 1010.65/s)  LR: 8.831e-04  Data: 0.012 (0.013)
Train: 67 [1150/1251 ( 92%)]  Loss: 3.914 (3.85)  Time: 0.994s, 1029.94/s  (1.013s, 1011.16/s)  LR: 8.831e-04  Data: 0.010 (0.012)
Train: 67 [1200/1251 ( 96%)]  Loss: 3.906 (3.85)  Time: 0.993s, 1031.23/s  (1.012s, 1011.81/s)  LR: 8.831e-04  Data: 0.010 (0.012)
Train: 67 [1250/1251 (100%)]  Loss: 3.712 (3.85)  Time: 0.982s, 1042.73/s  (1.012s, 1011.93/s)  LR: 8.831e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.653 (1.653)  Loss:  0.9181 (0.9181)  Acc@1: 84.9609 (84.9609)  Acc@5: 95.9961 (95.9961)
Test: [  48/48]  Time: 0.245 (0.566)  Loss:  0.9929 (1.5056)  Acc@1: 83.1368 (70.6820)  Acc@5: 95.5189 (90.4200)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-67.pth.tar', 70.68200004394531)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-63.pth.tar', 70.36400005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-65.pth.tar', 70.32200000488281)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-57.pth.tar', 70.31200004638671)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-62.pth.tar', 69.92599994384766)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-58.pth.tar', 69.83400007080078)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-64.pth.tar', 69.824000078125)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-66.pth.tar', 69.81800012451171)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-59.pth.tar', 69.78600005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-54.pth.tar', 69.67200004394532)

Train: 68 [   0/1251 (  0%)]  Loss: 3.632 (3.63)  Time: 2.612s,  392.05/s  (2.612s,  392.05/s)  LR: 8.797e-04  Data: 1.608 (1.608)
Train: 68 [  50/1251 (  4%)]  Loss: 3.893 (3.76)  Time: 0.998s, 1025.67/s  (1.060s,  965.95/s)  LR: 8.797e-04  Data: 0.011 (0.042)
Train: 68 [ 100/1251 (  8%)]  Loss: 3.649 (3.72)  Time: 0.995s, 1029.20/s  (1.031s,  993.42/s)  LR: 8.797e-04  Data: 0.011 (0.027)
Train: 68 [ 150/1251 ( 12%)]  Loss: 3.792 (3.74)  Time: 0.997s, 1027.39/s  (1.021s, 1003.18/s)  LR: 8.797e-04  Data: 0.011 (0.022)
Train: 68 [ 200/1251 ( 16%)]  Loss: 2.984 (3.59)  Time: 0.998s, 1025.97/s  (1.016s, 1008.35/s)  LR: 8.797e-04  Data: 0.010 (0.019)
Train: 68 [ 250/1251 ( 20%)]  Loss: 3.318 (3.54)  Time: 1.034s,  990.41/s  (1.014s, 1009.44/s)  LR: 8.797e-04  Data: 0.011 (0.017)
Train: 68 [ 300/1251 ( 24%)]  Loss: 3.645 (3.56)  Time: 1.017s, 1006.93/s  (1.012s, 1011.46/s)  LR: 8.797e-04  Data: 0.010 (0.016)
Train: 68 [ 350/1251 ( 28%)]  Loss: 3.795 (3.59)  Time: 0.989s, 1035.28/s  (1.011s, 1013.31/s)  LR: 8.797e-04  Data: 0.010 (0.016)
Train: 68 [ 400/1251 ( 32%)]  Loss: 4.004 (3.63)  Time: 0.998s, 1025.65/s  (1.010s, 1014.28/s)  LR: 8.797e-04  Data: 0.011 (0.015)
Train: 68 [ 450/1251 ( 36%)]  Loss: 3.963 (3.67)  Time: 1.002s, 1021.47/s  (1.009s, 1014.61/s)  LR: 8.797e-04  Data: 0.011 (0.015)
Train: 68 [ 500/1251 ( 40%)]  Loss: 3.510 (3.65)  Time: 0.996s, 1027.85/s  (1.008s, 1015.73/s)  LR: 8.797e-04  Data: 0.011 (0.014)
Train: 68 [ 550/1251 ( 44%)]  Loss: 3.687 (3.66)  Time: 1.031s,  993.40/s  (1.007s, 1016.62/s)  LR: 8.797e-04  Data: 0.011 (0.014)
Train: 68 [ 600/1251 ( 48%)]  Loss: 3.915 (3.68)  Time: 0.995s, 1028.68/s  (1.007s, 1017.28/s)  LR: 8.797e-04  Data: 0.011 (0.014)
Train: 68 [ 650/1251 ( 52%)]  Loss: 3.888 (3.69)  Time: 1.051s,  973.94/s  (1.008s, 1016.16/s)  LR: 8.797e-04  Data: 0.011 (0.013)
Train: 68 [ 700/1251 ( 56%)]  Loss: 4.034 (3.71)  Time: 0.995s, 1029.49/s  (1.007s, 1016.92/s)  LR: 8.797e-04  Data: 0.011 (0.013)
Train: 68 [ 750/1251 ( 60%)]  Loss: 3.913 (3.73)  Time: 0.993s, 1031.04/s  (1.007s, 1017.34/s)  LR: 8.797e-04  Data: 0.010 (0.013)
Train: 68 [ 800/1251 ( 64%)]  Loss: 3.830 (3.73)  Time: 0.994s, 1030.08/s  (1.006s, 1017.45/s)  LR: 8.797e-04  Data: 0.011 (0.013)
Train: 68 [ 850/1251 ( 68%)]  Loss: 3.415 (3.71)  Time: 0.997s, 1027.20/s  (1.006s, 1017.76/s)  LR: 8.797e-04  Data: 0.011 (0.013)
Train: 68 [ 900/1251 ( 72%)]  Loss: 3.540 (3.71)  Time: 1.014s, 1009.73/s  (1.006s, 1018.07/s)  LR: 8.797e-04  Data: 0.010 (0.013)
Train: 68 [ 950/1251 ( 76%)]  Loss: 3.639 (3.70)  Time: 1.067s,  959.87/s  (1.005s, 1018.42/s)  LR: 8.797e-04  Data: 0.011 (0.013)
Train: 68 [1000/1251 ( 80%)]  Loss: 3.888 (3.71)  Time: 0.995s, 1029.18/s  (1.005s, 1018.42/s)  LR: 8.797e-04  Data: 0.011 (0.013)
Train: 68 [1050/1251 ( 84%)]  Loss: 3.558 (3.70)  Time: 0.997s, 1027.52/s  (1.005s, 1018.59/s)  LR: 8.797e-04  Data: 0.011 (0.013)
Train: 68 [1100/1251 ( 88%)]  Loss: 3.855 (3.71)  Time: 0.994s, 1030.23/s  (1.005s, 1018.76/s)  LR: 8.797e-04  Data: 0.011 (0.012)
Train: 68 [1150/1251 ( 92%)]  Loss: 3.683 (3.71)  Time: 0.995s, 1028.91/s  (1.005s, 1019.14/s)  LR: 8.797e-04  Data: 0.012 (0.012)
Train: 68 [1200/1251 ( 96%)]  Loss: 3.931 (3.72)  Time: 0.996s, 1028.03/s  (1.005s, 1019.39/s)  LR: 8.797e-04  Data: 0.011 (0.012)
Train: 68 [1250/1251 (100%)]  Loss: 3.874 (3.72)  Time: 0.983s, 1042.23/s  (1.004s, 1019.62/s)  LR: 8.797e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.619 (1.619)  Loss:  0.8559 (0.8559)  Acc@1: 86.0352 (86.0352)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.245 (0.568)  Loss:  1.0374 (1.5306)  Acc@1: 82.1934 (69.4880)  Acc@5: 94.4576 (89.9440)
Train: 69 [   0/1251 (  0%)]  Loss: 3.621 (3.62)  Time: 2.379s,  430.40/s  (2.379s,  430.40/s)  LR: 8.763e-04  Data: 1.415 (1.415)
Train: 69 [  50/1251 (  4%)]  Loss: 3.882 (3.75)  Time: 0.996s, 1027.80/s  (1.030s,  994.51/s)  LR: 8.763e-04  Data: 0.010 (0.041)
Train: 69 [ 100/1251 (  8%)]  Loss: 3.843 (3.78)  Time: 0.997s, 1027.33/s  (1.015s, 1009.26/s)  LR: 8.763e-04  Data: 0.012 (0.026)
Train: 69 [ 150/1251 ( 12%)]  Loss: 3.647 (3.75)  Time: 1.058s,  967.75/s  (1.015s, 1008.87/s)  LR: 8.763e-04  Data: 0.011 (0.021)
Train: 69 [ 200/1251 ( 16%)]  Loss: 4.052 (3.81)  Time: 1.036s,  988.67/s  (1.013s, 1011.27/s)  LR: 8.763e-04  Data: 0.011 (0.019)
Train: 69 [ 250/1251 ( 20%)]  Loss: 3.609 (3.78)  Time: 1.000s, 1024.26/s  (1.012s, 1011.74/s)  LR: 8.763e-04  Data: 0.011 (0.017)
Train: 69 [ 300/1251 ( 24%)]  Loss: 3.586 (3.75)  Time: 0.995s, 1028.98/s  (1.010s, 1013.88/s)  LR: 8.763e-04  Data: 0.011 (0.016)
Train: 69 [ 350/1251 ( 28%)]  Loss: 3.274 (3.69)  Time: 0.997s, 1026.85/s  (1.010s, 1013.77/s)  LR: 8.763e-04  Data: 0.011 (0.015)
Train: 69 [ 400/1251 ( 32%)]  Loss: 3.716 (3.69)  Time: 1.000s, 1024.34/s  (1.010s, 1014.31/s)  LR: 8.763e-04  Data: 0.012 (0.015)
Train: 69 [ 450/1251 ( 36%)]  Loss: 3.935 (3.72)  Time: 0.993s, 1030.89/s  (1.010s, 1013.64/s)  LR: 8.763e-04  Data: 0.011 (0.014)
Train: 69 [ 500/1251 ( 40%)]  Loss: 3.989 (3.74)  Time: 1.004s, 1020.42/s  (1.010s, 1014.12/s)  LR: 8.763e-04  Data: 0.012 (0.014)
Train: 69 [ 550/1251 ( 44%)]  Loss: 4.084 (3.77)  Time: 1.035s,  989.47/s  (1.010s, 1013.54/s)  LR: 8.763e-04  Data: 0.011 (0.014)
Train: 69 [ 600/1251 ( 48%)]  Loss: 3.685 (3.76)  Time: 0.996s, 1027.65/s  (1.010s, 1013.57/s)  LR: 8.763e-04  Data: 0.010 (0.014)
Train: 69 [ 650/1251 ( 52%)]  Loss: 3.916 (3.77)  Time: 1.038s,  986.14/s  (1.010s, 1013.73/s)  LR: 8.763e-04  Data: 0.010 (0.013)
Train: 69 [ 700/1251 ( 56%)]  Loss: 3.840 (3.78)  Time: 0.995s, 1028.66/s  (1.010s, 1014.27/s)  LR: 8.763e-04  Data: 0.011 (0.013)
Train: 69 [ 750/1251 ( 60%)]  Loss: 3.877 (3.78)  Time: 0.999s, 1025.06/s  (1.009s, 1014.91/s)  LR: 8.763e-04  Data: 0.012 (0.013)
Train: 69 [ 800/1251 ( 64%)]  Loss: 3.704 (3.78)  Time: 1.034s,  990.39/s  (1.009s, 1015.25/s)  LR: 8.763e-04  Data: 0.011 (0.013)
Train: 69 [ 850/1251 ( 68%)]  Loss: 4.014 (3.79)  Time: 0.997s, 1027.00/s  (1.008s, 1015.74/s)  LR: 8.763e-04  Data: 0.012 (0.013)
Train: 69 [ 900/1251 ( 72%)]  Loss: 3.197 (3.76)  Time: 0.996s, 1028.04/s  (1.009s, 1015.08/s)  LR: 8.763e-04  Data: 0.011 (0.013)
Train: 69 [ 950/1251 ( 76%)]  Loss: 3.689 (3.76)  Time: 0.996s, 1028.62/s  (1.008s, 1015.50/s)  LR: 8.763e-04  Data: 0.011 (0.013)
Train: 69 [1000/1251 ( 80%)]  Loss: 4.344 (3.79)  Time: 1.045s,  980.02/s  (1.008s, 1015.96/s)  LR: 8.763e-04  Data: 0.012 (0.013)
Train: 69 [1050/1251 ( 84%)]  Loss: 3.858 (3.79)  Time: 0.997s, 1026.94/s  (1.008s, 1015.59/s)  LR: 8.763e-04  Data: 0.012 (0.013)
Train: 69 [1100/1251 ( 88%)]  Loss: 3.916 (3.79)  Time: 0.997s, 1027.43/s  (1.008s, 1016.04/s)  LR: 8.763e-04  Data: 0.012 (0.012)
Train: 69 [1150/1251 ( 92%)]  Loss: 3.741 (3.79)  Time: 0.997s, 1026.82/s  (1.008s, 1016.34/s)  LR: 8.763e-04  Data: 0.012 (0.012)
Train: 69 [1200/1251 ( 96%)]  Loss: 3.593 (3.78)  Time: 1.005s, 1018.71/s  (1.008s, 1016.34/s)  LR: 8.763e-04  Data: 0.011 (0.012)
Train: 69 [1250/1251 (100%)]  Loss: 3.862 (3.79)  Time: 0.983s, 1042.15/s  (1.008s, 1015.39/s)  LR: 8.763e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.722 (1.722)  Loss:  0.8307 (0.8307)  Acc@1: 86.4258 (86.4258)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.246 (0.568)  Loss:  0.9715 (1.5013)  Acc@1: 82.4292 (69.6480)  Acc@5: 95.8727 (90.0580)
Train: 70 [   0/1251 (  0%)]  Loss: 3.703 (3.70)  Time: 2.466s,  415.20/s  (2.466s,  415.20/s)  LR: 8.729e-04  Data: 1.515 (1.515)
Train: 70 [  50/1251 (  4%)]  Loss: 3.708 (3.71)  Time: 1.001s, 1023.36/s  (1.041s,  983.92/s)  LR: 8.729e-04  Data: 0.011 (0.044)
Train: 70 [ 100/1251 (  8%)]  Loss: 3.745 (3.72)  Time: 0.997s, 1026.81/s  (1.023s, 1000.59/s)  LR: 8.729e-04  Data: 0.011 (0.027)
Train: 70 [ 150/1251 ( 12%)]  Loss: 3.904 (3.76)  Time: 0.994s, 1030.29/s  (1.016s, 1008.28/s)  LR: 8.729e-04  Data: 0.011 (0.022)
Train: 70 [ 200/1251 ( 16%)]  Loss: 3.695 (3.75)  Time: 0.997s, 1027.19/s  (1.017s, 1006.62/s)  LR: 8.729e-04  Data: 0.011 (0.019)
Train: 70 [ 250/1251 ( 20%)]  Loss: 3.731 (3.75)  Time: 1.005s, 1018.82/s  (1.014s, 1009.87/s)  LR: 8.729e-04  Data: 0.012 (0.018)
Train: 70 [ 300/1251 ( 24%)]  Loss: 3.974 (3.78)  Time: 1.006s, 1018.02/s  (1.012s, 1011.69/s)  LR: 8.729e-04  Data: 0.016 (0.017)
Train: 70 [ 350/1251 ( 28%)]  Loss: 3.807 (3.78)  Time: 1.005s, 1018.87/s  (1.011s, 1013.06/s)  LR: 8.729e-04  Data: 0.012 (0.016)
Train: 70 [ 400/1251 ( 32%)]  Loss: 4.121 (3.82)  Time: 0.995s, 1029.21/s  (1.010s, 1013.98/s)  LR: 8.729e-04  Data: 0.011 (0.015)
Train: 70 [ 450/1251 ( 36%)]  Loss: 4.085 (3.85)  Time: 1.047s,  978.49/s  (1.009s, 1014.95/s)  LR: 8.729e-04  Data: 0.011 (0.015)
Train: 70 [ 500/1251 ( 40%)]  Loss: 3.724 (3.84)  Time: 0.998s, 1025.86/s  (1.008s, 1015.47/s)  LR: 8.729e-04  Data: 0.015 (0.015)
Train: 70 [ 550/1251 ( 44%)]  Loss: 3.453 (3.80)  Time: 0.993s, 1030.73/s  (1.008s, 1016.22/s)  LR: 8.729e-04  Data: 0.011 (0.014)
Train: 70 [ 600/1251 ( 48%)]  Loss: 3.877 (3.81)  Time: 0.998s, 1025.94/s  (1.007s, 1016.64/s)  LR: 8.729e-04  Data: 0.012 (0.014)
Train: 70 [ 650/1251 ( 52%)]  Loss: 4.104 (3.83)  Time: 0.996s, 1027.76/s  (1.007s, 1016.70/s)  LR: 8.729e-04  Data: 0.012 (0.014)
Train: 70 [ 700/1251 ( 56%)]  Loss: 3.726 (3.82)  Time: 1.002s, 1021.85/s  (1.007s, 1016.83/s)  LR: 8.729e-04  Data: 0.012 (0.014)
Train: 70 [ 750/1251 ( 60%)]  Loss: 4.065 (3.84)  Time: 1.008s, 1015.57/s  (1.007s, 1016.84/s)  LR: 8.729e-04  Data: 0.012 (0.014)
Train: 70 [ 800/1251 ( 64%)]  Loss: 3.632 (3.83)  Time: 1.028s,  996.44/s  (1.008s, 1016.30/s)  LR: 8.729e-04  Data: 0.013 (0.013)
Train: 70 [ 850/1251 ( 68%)]  Loss: 3.600 (3.81)  Time: 0.998s, 1026.55/s  (1.008s, 1016.20/s)  LR: 8.729e-04  Data: 0.012 (0.013)
Train: 70 [ 900/1251 ( 72%)]  Loss: 3.887 (3.82)  Time: 0.996s, 1027.71/s  (1.007s, 1016.61/s)  LR: 8.729e-04  Data: 0.011 (0.013)
Train: 70 [ 950/1251 ( 76%)]  Loss: 3.671 (3.81)  Time: 0.998s, 1026.51/s  (1.007s, 1016.79/s)  LR: 8.729e-04  Data: 0.015 (0.013)
Train: 70 [1000/1251 ( 80%)]  Loss: 3.765 (3.81)  Time: 0.999s, 1025.39/s  (1.007s, 1017.23/s)  LR: 8.729e-04  Data: 0.012 (0.013)
Train: 70 [1050/1251 ( 84%)]  Loss: 3.699 (3.80)  Time: 0.997s, 1026.69/s  (1.007s, 1017.18/s)  LR: 8.729e-04  Data: 0.011 (0.013)
Train: 70 [1100/1251 ( 88%)]  Loss: 3.630 (3.80)  Time: 0.998s, 1025.66/s  (1.007s, 1017.22/s)  LR: 8.729e-04  Data: 0.011 (0.013)
Train: 70 [1150/1251 ( 92%)]  Loss: 3.779 (3.80)  Time: 0.991s, 1033.08/s  (1.006s, 1017.42/s)  LR: 8.729e-04  Data: 0.011 (0.013)
Train: 70 [1200/1251 ( 96%)]  Loss: 3.680 (3.79)  Time: 0.995s, 1029.58/s  (1.006s, 1017.73/s)  LR: 8.729e-04  Data: 0.011 (0.013)
Train: 70 [1250/1251 (100%)]  Loss: 3.315 (3.77)  Time: 0.984s, 1040.97/s  (1.007s, 1017.05/s)  LR: 8.729e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.604 (1.604)  Loss:  0.8339 (0.8339)  Acc@1: 86.5234 (86.5234)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.245 (0.573)  Loss:  0.9569 (1.4812)  Acc@1: 81.0142 (70.4880)  Acc@5: 95.4009 (90.3740)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-67.pth.tar', 70.68200004394531)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-70.pth.tar', 70.48800002685547)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-63.pth.tar', 70.36400005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-65.pth.tar', 70.32200000488281)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-57.pth.tar', 70.31200004638671)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-62.pth.tar', 69.92599994384766)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-58.pth.tar', 69.83400007080078)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-64.pth.tar', 69.824000078125)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-66.pth.tar', 69.81800012451171)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-59.pth.tar', 69.78600005126953)

Train: 71 [   0/1251 (  0%)]  Loss: 3.697 (3.70)  Time: 2.521s,  406.13/s  (2.521s,  406.13/s)  LR: 8.694e-04  Data: 1.558 (1.558)
Train: 71 [  50/1251 (  4%)]  Loss: 3.989 (3.84)  Time: 1.049s,  975.74/s  (1.046s,  979.27/s)  LR: 8.694e-04  Data: 0.011 (0.044)
Train: 71 [ 100/1251 (  8%)]  Loss: 3.630 (3.77)  Time: 1.065s,  961.59/s  (1.036s,  988.13/s)  LR: 8.694e-04  Data: 0.012 (0.028)
Train: 71 [ 150/1251 ( 12%)]  Loss: 3.408 (3.68)  Time: 0.997s, 1027.07/s  (1.027s,  997.12/s)  LR: 8.694e-04  Data: 0.011 (0.022)
Train: 71 [ 200/1251 ( 16%)]  Loss: 3.749 (3.69)  Time: 0.997s, 1026.97/s  (1.021s, 1002.70/s)  LR: 8.694e-04  Data: 0.011 (0.020)
Train: 71 [ 250/1251 ( 20%)]  Loss: 3.373 (3.64)  Time: 0.998s, 1026.31/s  (1.017s, 1007.13/s)  LR: 8.694e-04  Data: 0.010 (0.018)
Train: 71 [ 300/1251 ( 24%)]  Loss: 4.151 (3.71)  Time: 0.995s, 1029.57/s  (1.014s, 1010.02/s)  LR: 8.694e-04  Data: 0.011 (0.017)
Train: 71 [ 350/1251 ( 28%)]  Loss: 3.585 (3.70)  Time: 0.996s, 1028.39/s  (1.015s, 1009.14/s)  LR: 8.694e-04  Data: 0.011 (0.016)
Train: 71 [ 400/1251 ( 32%)]  Loss: 3.969 (3.73)  Time: 0.993s, 1031.05/s  (1.013s, 1011.11/s)  LR: 8.694e-04  Data: 0.011 (0.015)
Train: 71 [ 450/1251 ( 36%)]  Loss: 3.781 (3.73)  Time: 0.998s, 1026.01/s  (1.011s, 1012.43/s)  LR: 8.694e-04  Data: 0.011 (0.015)
Train: 71 [ 500/1251 ( 40%)]  Loss: 3.770 (3.74)  Time: 0.998s, 1025.75/s  (1.011s, 1013.23/s)  LR: 8.694e-04  Data: 0.012 (0.015)
Train: 71 [ 550/1251 ( 44%)]  Loss: 3.442 (3.71)  Time: 1.024s,  999.81/s  (1.010s, 1013.92/s)  LR: 8.694e-04  Data: 0.011 (0.014)
Train: 71 [ 600/1251 ( 48%)]  Loss: 3.640 (3.71)  Time: 0.998s, 1026.13/s  (1.009s, 1014.75/s)  LR: 8.694e-04  Data: 0.011 (0.014)
Train: 71 [ 650/1251 ( 52%)]  Loss: 3.796 (3.71)  Time: 0.998s, 1026.47/s  (1.010s, 1013.56/s)  LR: 8.694e-04  Data: 0.011 (0.014)
Train: 71 [ 700/1251 ( 56%)]  Loss: 3.771 (3.72)  Time: 0.996s, 1027.79/s  (1.010s, 1014.17/s)  LR: 8.694e-04  Data: 0.012 (0.014)
Train: 71 [ 750/1251 ( 60%)]  Loss: 3.849 (3.72)  Time: 1.043s,  982.05/s  (1.009s, 1014.59/s)  LR: 8.694e-04  Data: 0.011 (0.013)
Train: 71 [ 800/1251 ( 64%)]  Loss: 3.817 (3.73)  Time: 0.998s, 1026.07/s  (1.010s, 1014.03/s)  LR: 8.694e-04  Data: 0.011 (0.013)
Train: 71 [ 850/1251 ( 68%)]  Loss: 4.006 (3.75)  Time: 1.052s,  973.42/s  (1.010s, 1013.45/s)  LR: 8.694e-04  Data: 0.010 (0.013)
Train: 71 [ 900/1251 ( 72%)]  Loss: 3.976 (3.76)  Time: 0.998s, 1026.14/s  (1.010s, 1013.54/s)  LR: 8.694e-04  Data: 0.011 (0.013)
Train: 71 [ 950/1251 ( 76%)]  Loss: 3.568 (3.75)  Time: 0.996s, 1027.79/s  (1.010s, 1013.88/s)  LR: 8.694e-04  Data: 0.012 (0.013)
Train: 71 [1000/1251 ( 80%)]  Loss: 3.742 (3.75)  Time: 0.999s, 1025.33/s  (1.010s, 1014.04/s)  LR: 8.694e-04  Data: 0.012 (0.013)
Train: 71 [1050/1251 ( 84%)]  Loss: 4.026 (3.76)  Time: 1.060s,  965.78/s  (1.009s, 1014.56/s)  LR: 8.694e-04  Data: 0.012 (0.013)
Train: 71 [1100/1251 ( 88%)]  Loss: 4.154 (3.78)  Time: 1.004s, 1020.17/s  (1.010s, 1014.16/s)  LR: 8.694e-04  Data: 0.012 (0.013)
Train: 71 [1150/1251 ( 92%)]  Loss: 3.763 (3.78)  Time: 1.001s, 1023.18/s  (1.010s, 1014.29/s)  LR: 8.694e-04  Data: 0.011 (0.013)
Train: 71 [1200/1251 ( 96%)]  Loss: 4.004 (3.79)  Time: 0.995s, 1029.13/s  (1.009s, 1014.38/s)  LR: 8.694e-04  Data: 0.011 (0.013)
Train: 71 [1250/1251 (100%)]  Loss: 4.187 (3.80)  Time: 0.983s, 1042.22/s  (1.010s, 1013.58/s)  LR: 8.694e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.626 (1.626)  Loss:  0.8821 (0.8821)  Acc@1: 86.3281 (86.3281)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.245 (0.569)  Loss:  1.0385 (1.5096)  Acc@1: 82.0755 (69.8840)  Acc@5: 94.2217 (89.8440)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-67.pth.tar', 70.68200004394531)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-70.pth.tar', 70.48800002685547)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-63.pth.tar', 70.36400005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-65.pth.tar', 70.32200000488281)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-57.pth.tar', 70.31200004638671)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-62.pth.tar', 69.92599994384766)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-71.pth.tar', 69.88399997070313)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-58.pth.tar', 69.83400007080078)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-64.pth.tar', 69.824000078125)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-66.pth.tar', 69.81800012451171)

Train: 72 [   0/1251 (  0%)]  Loss: 3.866 (3.87)  Time: 2.493s,  410.69/s  (2.493s,  410.69/s)  LR: 8.658e-04  Data: 1.530 (1.530)
Train: 72 [  50/1251 (  4%)]  Loss: 3.731 (3.80)  Time: 1.037s,  987.83/s  (1.050s,  975.08/s)  LR: 8.658e-04  Data: 0.012 (0.041)
Train: 72 [ 100/1251 (  8%)]  Loss: 3.715 (3.77)  Time: 1.064s,  962.03/s  (1.029s,  995.39/s)  LR: 8.658e-04  Data: 0.011 (0.026)
Train: 72 [ 150/1251 ( 12%)]  Loss: 3.726 (3.76)  Time: 0.996s, 1027.65/s  (1.021s, 1002.88/s)  LR: 8.658e-04  Data: 0.012 (0.021)
Train: 72 [ 200/1251 ( 16%)]  Loss: 3.926 (3.79)  Time: 1.001s, 1023.29/s  (1.017s, 1007.02/s)  LR: 8.658e-04  Data: 0.013 (0.019)
Train: 72 [ 250/1251 ( 20%)]  Loss: 3.788 (3.79)  Time: 0.997s, 1027.43/s  (1.015s, 1008.85/s)  LR: 8.658e-04  Data: 0.012 (0.017)
Train: 72 [ 300/1251 ( 24%)]  Loss: 3.687 (3.78)  Time: 0.995s, 1029.38/s  (1.014s, 1010.30/s)  LR: 8.658e-04  Data: 0.012 (0.016)
Train: 72 [ 350/1251 ( 28%)]  Loss: 4.169 (3.83)  Time: 0.998s, 1026.07/s  (1.012s, 1011.51/s)  LR: 8.658e-04  Data: 0.010 (0.016)
Train: 72 [ 400/1251 ( 32%)]  Loss: 3.924 (3.84)  Time: 0.999s, 1025.12/s  (1.012s, 1011.60/s)  LR: 8.658e-04  Data: 0.010 (0.015)
Train: 72 [ 450/1251 ( 36%)]  Loss: 3.837 (3.84)  Time: 0.996s, 1028.57/s  (1.011s, 1013.07/s)  LR: 8.658e-04  Data: 0.011 (0.015)
Train: 72 [ 500/1251 ( 40%)]  Loss: 3.983 (3.85)  Time: 0.995s, 1029.20/s  (1.013s, 1011.24/s)  LR: 8.658e-04  Data: 0.011 (0.014)
Train: 72 [ 550/1251 ( 44%)]  Loss: 4.000 (3.86)  Time: 0.997s, 1027.08/s  (1.011s, 1012.63/s)  LR: 8.658e-04  Data: 0.012 (0.014)
Train: 72 [ 600/1251 ( 48%)]  Loss: 3.769 (3.86)  Time: 1.010s, 1013.55/s  (1.011s, 1012.68/s)  LR: 8.658e-04  Data: 0.011 (0.014)
Train: 72 [ 650/1251 ( 52%)]  Loss: 3.581 (3.84)  Time: 0.995s, 1028.64/s  (1.010s, 1013.37/s)  LR: 8.658e-04  Data: 0.011 (0.014)
Train: 72 [ 700/1251 ( 56%)]  Loss: 3.741 (3.83)  Time: 1.000s, 1024.10/s  (1.010s, 1014.25/s)  LR: 8.658e-04  Data: 0.012 (0.013)
Train: 72 [ 750/1251 ( 60%)]  Loss: 3.761 (3.83)  Time: 0.996s, 1028.31/s  (1.009s, 1015.12/s)  LR: 8.658e-04  Data: 0.011 (0.013)
Train: 72 [ 800/1251 ( 64%)]  Loss: 3.745 (3.82)  Time: 0.995s, 1028.65/s  (1.009s, 1014.90/s)  LR: 8.658e-04  Data: 0.012 (0.013)
Train: 72 [ 850/1251 ( 68%)]  Loss: 3.897 (3.82)  Time: 0.994s, 1029.99/s  (1.008s, 1015.60/s)  LR: 8.658e-04  Data: 0.011 (0.013)
Train: 72 [ 900/1251 ( 72%)]  Loss: 3.648 (3.82)  Time: 0.996s, 1027.64/s  (1.008s, 1015.91/s)  LR: 8.658e-04  Data: 0.011 (0.013)
Train: 72 [ 950/1251 ( 76%)]  Loss: 3.596 (3.80)  Time: 0.993s, 1031.21/s  (1.008s, 1015.72/s)  LR: 8.658e-04  Data: 0.011 (0.013)
Train: 72 [1000/1251 ( 80%)]  Loss: 3.707 (3.80)  Time: 0.994s, 1030.24/s  (1.008s, 1016.05/s)  LR: 8.658e-04  Data: 0.010 (0.013)
Train: 72 [1050/1251 ( 84%)]  Loss: 3.857 (3.80)  Time: 0.996s, 1028.61/s  (1.009s, 1015.14/s)  LR: 8.658e-04  Data: 0.011 (0.013)
Train: 72 [1100/1251 ( 88%)]  Loss: 3.507 (3.79)  Time: 0.995s, 1028.80/s  (1.008s, 1015.55/s)  LR: 8.658e-04  Data: 0.011 (0.013)
Train: 72 [1150/1251 ( 92%)]  Loss: 3.401 (3.77)  Time: 1.003s, 1020.83/s  (1.009s, 1015.19/s)  LR: 8.658e-04  Data: 0.012 (0.013)
Train: 72 [1200/1251 ( 96%)]  Loss: 3.785 (3.77)  Time: 0.996s, 1027.70/s  (1.009s, 1015.28/s)  LR: 8.658e-04  Data: 0.011 (0.013)
Train: 72 [1250/1251 (100%)]  Loss: 3.550 (3.77)  Time: 0.984s, 1040.98/s  (1.008s, 1015.40/s)  LR: 8.658e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.694 (1.694)  Loss:  0.8073 (0.8073)  Acc@1: 87.0117 (87.0117)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.245 (0.571)  Loss:  1.0098 (1.4500)  Acc@1: 82.4292 (70.5600)  Acc@5: 94.4576 (90.2640)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-67.pth.tar', 70.68200004394531)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-72.pth.tar', 70.55999999511718)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-70.pth.tar', 70.48800002685547)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-63.pth.tar', 70.36400005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-65.pth.tar', 70.32200000488281)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-57.pth.tar', 70.31200004638671)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-62.pth.tar', 69.92599994384766)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-71.pth.tar', 69.88399997070313)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-58.pth.tar', 69.83400007080078)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-64.pth.tar', 69.824000078125)

Train: 73 [   0/1251 (  0%)]  Loss: 3.997 (4.00)  Time: 2.448s,  418.26/s  (2.448s,  418.26/s)  LR: 8.623e-04  Data: 1.488 (1.488)
Train: 73 [  50/1251 (  4%)]  Loss: 3.732 (3.86)  Time: 0.992s, 1031.97/s  (1.047s,  978.42/s)  LR: 8.623e-04  Data: 0.011 (0.040)
Train: 73 [ 100/1251 (  8%)]  Loss: 3.144 (3.62)  Time: 0.998s, 1025.71/s  (1.025s,  999.33/s)  LR: 8.623e-04  Data: 0.012 (0.026)
Train: 73 [ 150/1251 ( 12%)]  Loss: 3.752 (3.66)  Time: 0.995s, 1029.56/s  (1.018s, 1006.15/s)  LR: 8.623e-04  Data: 0.011 (0.021)
Train: 73 [ 200/1251 ( 16%)]  Loss: 3.212 (3.57)  Time: 0.996s, 1028.00/s  (1.014s, 1010.09/s)  LR: 8.623e-04  Data: 0.012 (0.019)
Train: 73 [ 250/1251 ( 20%)]  Loss: 3.874 (3.62)  Time: 1.022s, 1001.74/s  (1.013s, 1011.15/s)  LR: 8.623e-04  Data: 0.011 (0.017)
Train: 73 [ 300/1251 ( 24%)]  Loss: 3.647 (3.62)  Time: 1.001s, 1023.06/s  (1.011s, 1012.94/s)  LR: 8.623e-04  Data: 0.013 (0.016)
Train: 73 [ 350/1251 ( 28%)]  Loss: 3.612 (3.62)  Time: 0.991s, 1032.81/s  (1.013s, 1010.82/s)  LR: 8.623e-04  Data: 0.011 (0.016)
Train: 73 [ 400/1251 ( 32%)]  Loss: 3.667 (3.63)  Time: 1.000s, 1024.33/s  (1.016s, 1008.28/s)  LR: 8.623e-04  Data: 0.012 (0.015)
Train: 73 [ 450/1251 ( 36%)]  Loss: 3.939 (3.66)  Time: 1.012s, 1011.45/s  (1.015s, 1009.17/s)  LR: 8.623e-04  Data: 0.012 (0.015)
Train: 73 [ 500/1251 ( 40%)]  Loss: 3.737 (3.66)  Time: 1.064s,  962.46/s  (1.015s, 1008.91/s)  LR: 8.623e-04  Data: 0.010 (0.014)
Train: 73 [ 550/1251 ( 44%)]  Loss: 3.943 (3.69)  Time: 0.996s, 1027.72/s  (1.016s, 1008.06/s)  LR: 8.623e-04  Data: 0.012 (0.014)
Train: 73 [ 600/1251 ( 48%)]  Loss: 3.466 (3.67)  Time: 1.005s, 1018.70/s  (1.017s, 1007.34/s)  LR: 8.623e-04  Data: 0.011 (0.014)
Train: 73 [ 650/1251 ( 52%)]  Loss: 4.190 (3.71)  Time: 1.000s, 1023.78/s  (1.016s, 1008.36/s)  LR: 8.623e-04  Data: 0.011 (0.014)
Train: 73 [ 700/1251 ( 56%)]  Loss: 4.272 (3.75)  Time: 0.996s, 1027.86/s  (1.014s, 1009.48/s)  LR: 8.623e-04  Data: 0.011 (0.013)
Train: 73 [ 750/1251 ( 60%)]  Loss: 3.877 (3.75)  Time: 1.046s,  979.37/s  (1.014s, 1009.77/s)  LR: 8.623e-04  Data: 0.011 (0.013)
Train: 73 [ 800/1251 ( 64%)]  Loss: 3.806 (3.76)  Time: 0.996s, 1028.37/s  (1.013s, 1010.60/s)  LR: 8.623e-04  Data: 0.011 (0.013)
Train: 73 [ 850/1251 ( 68%)]  Loss: 3.329 (3.73)  Time: 0.996s, 1027.83/s  (1.013s, 1010.91/s)  LR: 8.623e-04  Data: 0.011 (0.013)
Train: 73 [ 900/1251 ( 72%)]  Loss: 3.788 (3.74)  Time: 0.998s, 1026.02/s  (1.012s, 1011.73/s)  LR: 8.623e-04  Data: 0.011 (0.013)
Train: 73 [ 950/1251 ( 76%)]  Loss: 3.838 (3.74)  Time: 0.996s, 1027.96/s  (1.012s, 1012.26/s)  LR: 8.623e-04  Data: 0.011 (0.013)
Train: 73 [1000/1251 ( 80%)]  Loss: 3.999 (3.75)  Time: 0.995s, 1028.80/s  (1.012s, 1012.16/s)  LR: 8.623e-04  Data: 0.011 (0.013)
Train: 73 [1050/1251 ( 84%)]  Loss: 3.805 (3.76)  Time: 0.997s, 1026.76/s  (1.011s, 1012.80/s)  LR: 8.623e-04  Data: 0.011 (0.013)
Train: 73 [1100/1251 ( 88%)]  Loss: 3.580 (3.75)  Time: 1.000s, 1023.52/s  (1.011s, 1012.96/s)  LR: 8.623e-04  Data: 0.010 (0.013)
Train: 73 [1150/1251 ( 92%)]  Loss: 3.910 (3.75)  Time: 0.999s, 1024.81/s  (1.011s, 1013.25/s)  LR: 8.623e-04  Data: 0.011 (0.013)
Train: 73 [1200/1251 ( 96%)]  Loss: 3.531 (3.75)  Time: 0.993s, 1031.06/s  (1.011s, 1012.92/s)  LR: 8.623e-04  Data: 0.011 (0.012)
Train: 73 [1250/1251 (100%)]  Loss: 4.122 (3.76)  Time: 0.983s, 1041.88/s  (1.011s, 1013.23/s)  LR: 8.623e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.637 (1.637)  Loss:  0.8908 (0.8908)  Acc@1: 85.1562 (85.1562)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.245 (0.565)  Loss:  1.0423 (1.5063)  Acc@1: 81.7217 (70.7700)  Acc@5: 95.5189 (90.2280)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-73.pth.tar', 70.7700000756836)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-67.pth.tar', 70.68200004394531)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-72.pth.tar', 70.55999999511718)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-70.pth.tar', 70.48800002685547)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-63.pth.tar', 70.36400005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-65.pth.tar', 70.32200000488281)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-57.pth.tar', 70.31200004638671)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-62.pth.tar', 69.92599994384766)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-71.pth.tar', 69.88399997070313)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-58.pth.tar', 69.83400007080078)

Train: 74 [   0/1251 (  0%)]  Loss: 4.185 (4.19)  Time: 2.395s,  427.50/s  (2.395s,  427.50/s)  LR: 8.587e-04  Data: 1.445 (1.445)
Train: 74 [  50/1251 (  4%)]  Loss: 3.670 (3.93)  Time: 1.000s, 1024.03/s  (1.042s,  982.89/s)  LR: 8.587e-04  Data: 0.010 (0.039)
Train: 74 [ 100/1251 (  8%)]  Loss: 3.916 (3.92)  Time: 1.004s, 1019.92/s  (1.024s, 1000.44/s)  LR: 8.587e-04  Data: 0.011 (0.025)
Train: 74 [ 150/1251 ( 12%)]  Loss: 4.014 (3.95)  Time: 0.997s, 1027.30/s  (1.016s, 1007.41/s)  LR: 8.587e-04  Data: 0.011 (0.021)
Train: 74 [ 200/1251 ( 16%)]  Loss: 3.633 (3.88)  Time: 1.007s, 1016.92/s  (1.015s, 1009.28/s)  LR: 8.587e-04  Data: 0.015 (0.018)
Train: 74 [ 250/1251 ( 20%)]  Loss: 3.699 (3.85)  Time: 0.994s, 1030.42/s  (1.013s, 1011.14/s)  LR: 8.587e-04  Data: 0.011 (0.017)
Train: 74 [ 300/1251 ( 24%)]  Loss: 3.635 (3.82)  Time: 1.035s,  989.20/s  (1.011s, 1012.84/s)  LR: 8.587e-04  Data: 0.011 (0.016)
Train: 74 [ 350/1251 ( 28%)]  Loss: 3.971 (3.84)  Time: 1.004s, 1019.80/s  (1.010s, 1013.93/s)  LR: 8.587e-04  Data: 0.011 (0.015)
Train: 74 [ 400/1251 ( 32%)]  Loss: 3.878 (3.84)  Time: 1.018s, 1006.08/s  (1.009s, 1014.94/s)  LR: 8.587e-04  Data: 0.011 (0.015)
Train: 74 [ 450/1251 ( 36%)]  Loss: 3.455 (3.81)  Time: 0.999s, 1024.96/s  (1.008s, 1015.68/s)  LR: 8.587e-04  Data: 0.012 (0.014)
Train: 74 [ 500/1251 ( 40%)]  Loss: 3.764 (3.80)  Time: 1.016s, 1008.15/s  (1.008s, 1015.73/s)  LR: 8.587e-04  Data: 0.011 (0.014)
Train: 74 [ 550/1251 ( 44%)]  Loss: 3.395 (3.77)  Time: 0.999s, 1024.98/s  (1.008s, 1016.24/s)  LR: 8.587e-04  Data: 0.011 (0.014)
Train: 74 [ 600/1251 ( 48%)]  Loss: 3.609 (3.76)  Time: 0.997s, 1027.10/s  (1.007s, 1016.84/s)  LR: 8.587e-04  Data: 0.012 (0.014)
Train: 74 [ 650/1251 ( 52%)]  Loss: 3.884 (3.76)  Time: 1.031s,  993.29/s  (1.006s, 1017.49/s)  LR: 8.587e-04  Data: 0.011 (0.013)
Train: 74 [ 700/1251 ( 56%)]  Loss: 3.388 (3.74)  Time: 0.995s, 1029.40/s  (1.006s, 1017.55/s)  LR: 8.587e-04  Data: 0.011 (0.013)
Train: 74 [ 750/1251 ( 60%)]  Loss: 3.737 (3.74)  Time: 0.996s, 1028.00/s  (1.006s, 1017.60/s)  LR: 8.587e-04  Data: 0.012 (0.013)
Train: 74 [ 800/1251 ( 64%)]  Loss: 4.030 (3.76)  Time: 0.995s, 1028.69/s  (1.006s, 1018.04/s)  LR: 8.587e-04  Data: 0.011 (0.013)
Train: 74 [ 850/1251 ( 68%)]  Loss: 3.813 (3.76)  Time: 0.995s, 1028.66/s  (1.005s, 1018.51/s)  LR: 8.587e-04  Data: 0.011 (0.013)
Train: 74 [ 900/1251 ( 72%)]  Loss: 4.023 (3.77)  Time: 0.995s, 1029.09/s  (1.006s, 1018.39/s)  LR: 8.587e-04  Data: 0.011 (0.013)
Train: 74 [ 950/1251 ( 76%)]  Loss: 4.156 (3.79)  Time: 0.996s, 1027.90/s  (1.005s, 1018.57/s)  LR: 8.587e-04  Data: 0.012 (0.013)
Train: 74 [1000/1251 ( 80%)]  Loss: 4.002 (3.80)  Time: 0.998s, 1026.28/s  (1.005s, 1018.89/s)  LR: 8.587e-04  Data: 0.012 (0.013)
Train: 74 [1050/1251 ( 84%)]  Loss: 4.096 (3.82)  Time: 1.038s,  986.81/s  (1.006s, 1017.52/s)  LR: 8.587e-04  Data: 0.010 (0.013)
Train: 74 [1100/1251 ( 88%)]  Loss: 3.736 (3.81)  Time: 0.994s, 1030.65/s  (1.007s, 1016.62/s)  LR: 8.587e-04  Data: 0.011 (0.012)
Train: 74 [1150/1251 ( 92%)]  Loss: 3.926 (3.82)  Time: 1.005s, 1018.76/s  (1.007s, 1016.74/s)  LR: 8.587e-04  Data: 0.011 (0.012)
Train: 74 [1200/1251 ( 96%)]  Loss: 3.594 (3.81)  Time: 0.999s, 1024.69/s  (1.007s, 1016.80/s)  LR: 8.587e-04  Data: 0.012 (0.012)
Train: 74 [1250/1251 (100%)]  Loss: 3.519 (3.80)  Time: 1.021s, 1003.02/s  (1.007s, 1016.91/s)  LR: 8.587e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.594 (1.594)  Loss:  0.9399 (0.9399)  Acc@1: 87.1094 (87.1094)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.245 (0.569)  Loss:  0.9085 (1.4846)  Acc@1: 82.0755 (70.5340)  Acc@5: 96.1085 (90.0680)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-73.pth.tar', 70.7700000756836)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-67.pth.tar', 70.68200004394531)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-72.pth.tar', 70.55999999511718)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-74.pth.tar', 70.53399997070312)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-70.pth.tar', 70.48800002685547)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-63.pth.tar', 70.36400005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-65.pth.tar', 70.32200000488281)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-57.pth.tar', 70.31200004638671)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-62.pth.tar', 69.92599994384766)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-71.pth.tar', 69.88399997070313)

Train: 75 [   0/1251 (  0%)]  Loss: 3.942 (3.94)  Time: 4.285s,  238.97/s  (4.285s,  238.97/s)  LR: 8.550e-04  Data: 3.008 (3.008)
Train: 75 [  50/1251 (  4%)]  Loss: 4.130 (4.04)  Time: 0.998s, 1026.47/s  (1.079s,  948.84/s)  LR: 8.550e-04  Data: 0.012 (0.070)
Train: 75 [ 100/1251 (  8%)]  Loss: 4.163 (4.08)  Time: 0.997s, 1026.94/s  (1.045s,  980.11/s)  LR: 8.550e-04  Data: 0.012 (0.041)
Train: 75 [ 150/1251 ( 12%)]  Loss: 3.985 (4.06)  Time: 1.000s, 1023.59/s  (1.033s,  991.10/s)  LR: 8.550e-04  Data: 0.011 (0.031)
Train: 75 [ 200/1251 ( 16%)]  Loss: 3.799 (4.00)  Time: 0.992s, 1031.77/s  (1.027s,  997.05/s)  LR: 8.550e-04  Data: 0.011 (0.026)
Train: 75 [ 250/1251 ( 20%)]  Loss: 3.479 (3.92)  Time: 0.994s, 1030.03/s  (1.023s, 1000.58/s)  LR: 8.550e-04  Data: 0.012 (0.023)
Train: 75 [ 300/1251 ( 24%)]  Loss: 4.021 (3.93)  Time: 1.046s,  978.76/s  (1.023s, 1000.74/s)  LR: 8.550e-04  Data: 0.011 (0.021)
Train: 75 [ 350/1251 ( 28%)]  Loss: 3.662 (3.90)  Time: 1.036s,  988.77/s  (1.022s, 1001.89/s)  LR: 8.550e-04  Data: 0.011 (0.020)
Train: 75 [ 400/1251 ( 32%)]  Loss: 3.756 (3.88)  Time: 1.053s,  972.35/s  (1.022s, 1002.00/s)  LR: 8.550e-04  Data: 0.010 (0.019)
Train: 75 [ 450/1251 ( 36%)]  Loss: 3.850 (3.88)  Time: 1.052s,  973.07/s  (1.021s, 1003.01/s)  LR: 8.550e-04  Data: 0.011 (0.018)
Train: 75 [ 500/1251 ( 40%)]  Loss: 3.469 (3.84)  Time: 0.993s, 1031.12/s  (1.019s, 1004.84/s)  LR: 8.550e-04  Data: 0.011 (0.017)
Train: 75 [ 550/1251 ( 44%)]  Loss: 3.519 (3.81)  Time: 0.994s, 1029.75/s  (1.017s, 1006.46/s)  LR: 8.550e-04  Data: 0.011 (0.017)
Train: 75 [ 600/1251 ( 48%)]  Loss: 4.071 (3.83)  Time: 1.005s, 1019.02/s  (1.016s, 1007.83/s)  LR: 8.550e-04  Data: 0.011 (0.016)
Train: 75 [ 650/1251 ( 52%)]  Loss: 3.504 (3.81)  Time: 0.993s, 1031.08/s  (1.016s, 1008.30/s)  LR: 8.550e-04  Data: 0.011 (0.016)
Train: 75 [ 700/1251 ( 56%)]  Loss: 3.739 (3.81)  Time: 0.990s, 1034.37/s  (1.017s, 1007.16/s)  LR: 8.550e-04  Data: 0.010 (0.016)
Train: 75 [ 750/1251 ( 60%)]  Loss: 3.576 (3.79)  Time: 0.995s, 1029.53/s  (1.016s, 1008.22/s)  LR: 8.550e-04  Data: 0.011 (0.015)
Train: 75 [ 800/1251 ( 64%)]  Loss: 3.860 (3.80)  Time: 1.037s,  987.32/s  (1.015s, 1008.81/s)  LR: 8.550e-04  Data: 0.010 (0.015)
Train: 75 [ 850/1251 ( 68%)]  Loss: 3.501 (3.78)  Time: 1.002s, 1021.79/s  (1.014s, 1009.73/s)  LR: 8.550e-04  Data: 0.011 (0.015)
Train: 75 [ 900/1251 ( 72%)]  Loss: 3.836 (3.78)  Time: 1.000s, 1023.92/s  (1.014s, 1010.16/s)  LR: 8.550e-04  Data: 0.012 (0.015)
Train: 75 [ 950/1251 ( 76%)]  Loss: 3.540 (3.77)  Time: 1.060s,  966.26/s  (1.014s, 1010.25/s)  LR: 8.550e-04  Data: 0.012 (0.014)
Train: 75 [1000/1251 ( 80%)]  Loss: 4.018 (3.78)  Time: 0.994s, 1030.58/s  (1.013s, 1010.53/s)  LR: 8.550e-04  Data: 0.010 (0.014)
Train: 75 [1050/1251 ( 84%)]  Loss: 3.594 (3.77)  Time: 0.995s, 1029.32/s  (1.013s, 1011.08/s)  LR: 8.550e-04  Data: 0.012 (0.014)
Train: 75 [1100/1251 ( 88%)]  Loss: 3.960 (3.78)  Time: 0.992s, 1032.39/s  (1.012s, 1011.46/s)  LR: 8.550e-04  Data: 0.010 (0.014)
Train: 75 [1150/1251 ( 92%)]  Loss: 3.692 (3.78)  Time: 0.995s, 1028.96/s  (1.012s, 1011.66/s)  LR: 8.550e-04  Data: 0.011 (0.014)
Train: 75 [1200/1251 ( 96%)]  Loss: 3.624 (3.77)  Time: 0.995s, 1029.61/s  (1.012s, 1012.26/s)  LR: 8.550e-04  Data: 0.011 (0.014)
Train: 75 [1250/1251 (100%)]  Loss: 3.610 (3.77)  Time: 0.981s, 1044.08/s  (1.011s, 1012.38/s)  LR: 8.550e-04  Data: 0.000 (0.014)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.593 (1.593)  Loss:  0.7924 (0.7924)  Acc@1: 86.0352 (86.0352)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.245 (0.576)  Loss:  0.9134 (1.4251)  Acc@1: 82.9009 (70.5020)  Acc@5: 95.7547 (90.1220)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-73.pth.tar', 70.7700000756836)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-67.pth.tar', 70.68200004394531)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-72.pth.tar', 70.55999999511718)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-74.pth.tar', 70.53399997070312)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-75.pth.tar', 70.50200007080078)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-70.pth.tar', 70.48800002685547)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-63.pth.tar', 70.36400005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-65.pth.tar', 70.32200000488281)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-57.pth.tar', 70.31200004638671)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-62.pth.tar', 69.92599994384766)

Train: 76 [   0/1251 (  0%)]  Loss: 3.943 (3.94)  Time: 2.571s,  398.25/s  (2.571s,  398.25/s)  LR: 8.513e-04  Data: 1.608 (1.608)
Train: 76 [  50/1251 (  4%)]  Loss: 3.615 (3.78)  Time: 0.995s, 1028.96/s  (1.041s,  983.63/s)  LR: 8.513e-04  Data: 0.011 (0.043)
Train: 76 [ 100/1251 (  8%)]  Loss: 3.594 (3.72)  Time: 0.997s, 1027.55/s  (1.031s,  992.85/s)  LR: 8.513e-04  Data: 0.012 (0.027)
Train: 76 [ 150/1251 ( 12%)]  Loss: 3.355 (3.63)  Time: 0.995s, 1028.86/s  (1.022s, 1001.92/s)  LR: 8.513e-04  Data: 0.011 (0.022)
Train: 76 [ 200/1251 ( 16%)]  Loss: 3.782 (3.66)  Time: 0.995s, 1028.65/s  (1.021s, 1003.09/s)  LR: 8.513e-04  Data: 0.011 (0.019)
Train: 76 [ 250/1251 ( 20%)]  Loss: 3.659 (3.66)  Time: 0.999s, 1024.68/s  (1.021s, 1003.13/s)  LR: 8.513e-04  Data: 0.012 (0.018)
Train: 76 [ 300/1251 ( 24%)]  Loss: 3.811 (3.68)  Time: 0.998s, 1026.22/s  (1.022s, 1001.94/s)  LR: 8.513e-04  Data: 0.012 (0.017)
Train: 76 [ 350/1251 ( 28%)]  Loss: 3.853 (3.70)  Time: 0.999s, 1025.26/s  (1.019s, 1005.20/s)  LR: 8.513e-04  Data: 0.012 (0.016)
Train: 76 [ 400/1251 ( 32%)]  Loss: 3.717 (3.70)  Time: 0.999s, 1024.87/s  (1.019s, 1005.31/s)  LR: 8.513e-04  Data: 0.011 (0.015)
Train: 76 [ 450/1251 ( 36%)]  Loss: 3.792 (3.71)  Time: 0.997s, 1027.44/s  (1.017s, 1007.02/s)  LR: 8.513e-04  Data: 0.012 (0.015)
Train: 76 [ 500/1251 ( 40%)]  Loss: 3.623 (3.70)  Time: 0.996s, 1027.98/s  (1.015s, 1008.57/s)  LR: 8.513e-04  Data: 0.011 (0.015)
Train: 76 [ 550/1251 ( 44%)]  Loss: 4.130 (3.74)  Time: 1.009s, 1015.31/s  (1.014s, 1009.91/s)  LR: 8.513e-04  Data: 0.010 (0.014)
Train: 76 [ 600/1251 ( 48%)]  Loss: 4.056 (3.76)  Time: 0.994s, 1029.85/s  (1.013s, 1011.28/s)  LR: 8.513e-04  Data: 0.010 (0.014)
Train: 76 [ 650/1251 ( 52%)]  Loss: 3.393 (3.74)  Time: 0.997s, 1027.45/s  (1.011s, 1012.38/s)  LR: 8.513e-04  Data: 0.012 (0.014)
Train: 76 [ 700/1251 ( 56%)]  Loss: 3.734 (3.74)  Time: 0.997s, 1027.22/s  (1.011s, 1012.69/s)  LR: 8.513e-04  Data: 0.011 (0.014)
Train: 76 [ 750/1251 ( 60%)]  Loss: 3.622 (3.73)  Time: 1.064s,  962.07/s  (1.012s, 1011.69/s)  LR: 8.513e-04  Data: 0.011 (0.013)
Train: 76 [ 800/1251 ( 64%)]  Loss: 3.929 (3.74)  Time: 1.038s,  986.89/s  (1.012s, 1012.24/s)  LR: 8.513e-04  Data: 0.011 (0.013)
Train: 76 [ 850/1251 ( 68%)]  Loss: 3.552 (3.73)  Time: 0.994s, 1030.43/s  (1.011s, 1012.65/s)  LR: 8.513e-04  Data: 0.011 (0.013)
Train: 76 [ 900/1251 ( 72%)]  Loss: 3.652 (3.73)  Time: 0.994s, 1030.30/s  (1.011s, 1013.23/s)  LR: 8.513e-04  Data: 0.011 (0.013)
Train: 76 [ 950/1251 ( 76%)]  Loss: 3.420 (3.71)  Time: 0.998s, 1026.35/s  (1.010s, 1013.94/s)  LR: 8.513e-04  Data: 0.011 (0.013)
Train: 76 [1000/1251 ( 80%)]  Loss: 4.121 (3.73)  Time: 0.996s, 1027.69/s  (1.010s, 1014.20/s)  LR: 8.513e-04  Data: 0.011 (0.013)
Train: 76 [1050/1251 ( 84%)]  Loss: 3.468 (3.72)  Time: 0.995s, 1029.07/s  (1.010s, 1014.23/s)  LR: 8.513e-04  Data: 0.012 (0.013)
Train: 76 [1100/1251 ( 88%)]  Loss: 4.140 (3.74)  Time: 1.005s, 1018.83/s  (1.010s, 1014.24/s)  LR: 8.513e-04  Data: 0.011 (0.013)
Train: 76 [1150/1251 ( 92%)]  Loss: 3.564 (3.73)  Time: 0.994s, 1029.99/s  (1.009s, 1014.77/s)  LR: 8.513e-04  Data: 0.011 (0.013)
Train: 76 [1200/1251 ( 96%)]  Loss: 3.561 (3.72)  Time: 1.035s,  988.99/s  (1.009s, 1014.40/s)  LR: 8.513e-04  Data: 0.010 (0.013)
Train: 76 [1250/1251 (100%)]  Loss: 3.522 (3.72)  Time: 1.022s, 1001.54/s  (1.010s, 1014.01/s)  LR: 8.513e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.649 (1.649)  Loss:  0.9564 (0.9564)  Acc@1: 85.4492 (85.4492)  Acc@5: 95.5078 (95.5078)
Test: [  48/48]  Time: 0.245 (0.572)  Loss:  1.0439 (1.4725)  Acc@1: 80.8962 (70.6220)  Acc@5: 93.9858 (90.4020)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-73.pth.tar', 70.7700000756836)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-67.pth.tar', 70.68200004394531)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-76.pth.tar', 70.62199997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-72.pth.tar', 70.55999999511718)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-74.pth.tar', 70.53399997070312)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-75.pth.tar', 70.50200007080078)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-70.pth.tar', 70.48800002685547)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-63.pth.tar', 70.36400005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-65.pth.tar', 70.32200000488281)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-57.pth.tar', 70.31200004638671)

Train: 77 [   0/1251 (  0%)]  Loss: 3.817 (3.82)  Time: 2.548s,  401.83/s  (2.548s,  401.83/s)  LR: 8.476e-04  Data: 1.581 (1.581)
Train: 77 [  50/1251 (  4%)]  Loss: 3.812 (3.81)  Time: 1.012s, 1012.34/s  (1.040s,  984.62/s)  LR: 8.476e-04  Data: 0.013 (0.042)
Train: 77 [ 100/1251 (  8%)]  Loss: 3.760 (3.80)  Time: 0.997s, 1027.15/s  (1.020s, 1004.27/s)  LR: 8.476e-04  Data: 0.011 (0.027)
Train: 77 [ 150/1251 ( 12%)]  Loss: 4.230 (3.90)  Time: 0.996s, 1028.14/s  (1.013s, 1011.24/s)  LR: 8.476e-04  Data: 0.011 (0.021)
Train: 77 [ 200/1251 ( 16%)]  Loss: 3.662 (3.86)  Time: 1.050s,  974.83/s  (1.012s, 1011.59/s)  LR: 8.476e-04  Data: 0.010 (0.019)
Train: 77 [ 250/1251 ( 20%)]  Loss: 3.984 (3.88)  Time: 1.000s, 1024.04/s  (1.016s, 1008.10/s)  LR: 8.476e-04  Data: 0.011 (0.017)
Train: 77 [ 300/1251 ( 24%)]  Loss: 3.574 (3.83)  Time: 1.038s,  986.04/s  (1.018s, 1006.29/s)  LR: 8.476e-04  Data: 0.011 (0.016)
Train: 77 [ 350/1251 ( 28%)]  Loss: 3.969 (3.85)  Time: 1.007s, 1016.84/s  (1.020s, 1004.12/s)  LR: 8.476e-04  Data: 0.011 (0.015)
Train: 77 [ 400/1251 ( 32%)]  Loss: 3.807 (3.85)  Time: 0.995s, 1028.95/s  (1.018s, 1005.91/s)  LR: 8.476e-04  Data: 0.011 (0.015)
Train: 77 [ 450/1251 ( 36%)]  Loss: 4.338 (3.90)  Time: 1.049s,  975.87/s  (1.019s, 1004.54/s)  LR: 8.476e-04  Data: 0.011 (0.015)
Train: 77 [ 500/1251 ( 40%)]  Loss: 3.708 (3.88)  Time: 0.996s, 1027.70/s  (1.018s, 1005.96/s)  LR: 8.476e-04  Data: 0.010 (0.014)
Train: 77 [ 550/1251 ( 44%)]  Loss: 3.635 (3.86)  Time: 0.997s, 1027.13/s  (1.016s, 1007.57/s)  LR: 8.476e-04  Data: 0.012 (0.014)
Train: 77 [ 600/1251 ( 48%)]  Loss: 3.573 (3.84)  Time: 0.994s, 1030.60/s  (1.015s, 1008.96/s)  LR: 8.476e-04  Data: 0.011 (0.014)
Train: 77 [ 650/1251 ( 52%)]  Loss: 3.717 (3.83)  Time: 0.996s, 1028.00/s  (1.014s, 1010.35/s)  LR: 8.476e-04  Data: 0.011 (0.013)
Train: 77 [ 700/1251 ( 56%)]  Loss: 3.723 (3.82)  Time: 0.998s, 1026.23/s  (1.013s, 1011.33/s)  LR: 8.476e-04  Data: 0.011 (0.013)
Train: 77 [ 750/1251 ( 60%)]  Loss: 3.582 (3.81)  Time: 0.999s, 1025.47/s  (1.012s, 1012.27/s)  LR: 8.476e-04  Data: 0.011 (0.013)
Train: 77 [ 800/1251 ( 64%)]  Loss: 3.436 (3.78)  Time: 0.997s, 1027.33/s  (1.011s, 1012.37/s)  LR: 8.476e-04  Data: 0.011 (0.013)
Train: 77 [ 850/1251 ( 68%)]  Loss: 3.554 (3.77)  Time: 0.997s, 1026.58/s  (1.011s, 1012.77/s)  LR: 8.476e-04  Data: 0.012 (0.013)
Train: 77 [ 900/1251 ( 72%)]  Loss: 4.024 (3.78)  Time: 0.993s, 1031.05/s  (1.011s, 1013.31/s)  LR: 8.476e-04  Data: 0.010 (0.013)
Train: 77 [ 950/1251 ( 76%)]  Loss: 3.934 (3.79)  Time: 1.050s,  975.27/s  (1.011s, 1012.96/s)  LR: 8.476e-04  Data: 0.011 (0.013)
Train: 77 [1000/1251 ( 80%)]  Loss: 3.680 (3.79)  Time: 1.041s,  983.67/s  (1.010s, 1013.62/s)  LR: 8.476e-04  Data: 0.012 (0.013)
Train: 77 [1050/1251 ( 84%)]  Loss: 3.685 (3.78)  Time: 1.032s,  991.82/s  (1.010s, 1013.73/s)  LR: 8.476e-04  Data: 0.011 (0.013)
Train: 77 [1100/1251 ( 88%)]  Loss: 3.482 (3.77)  Time: 0.994s, 1029.78/s  (1.010s, 1014.03/s)  LR: 8.476e-04  Data: 0.011 (0.012)
Train: 77 [1150/1251 ( 92%)]  Loss: 3.839 (3.77)  Time: 1.005s, 1019.01/s  (1.009s, 1014.53/s)  LR: 8.476e-04  Data: 0.014 (0.012)
Train: 77 [1200/1251 ( 96%)]  Loss: 3.862 (3.78)  Time: 0.999s, 1025.05/s  (1.009s, 1014.52/s)  LR: 8.476e-04  Data: 0.011 (0.012)
Train: 77 [1250/1251 (100%)]  Loss: 3.879 (3.78)  Time: 0.984s, 1040.64/s  (1.009s, 1014.94/s)  LR: 8.476e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.764 (1.764)  Loss:  0.8868 (0.8868)  Acc@1: 86.9141 (86.9141)  Acc@5: 96.2891 (96.2891)
Test: [  48/48]  Time: 0.245 (0.572)  Loss:  0.8785 (1.4373)  Acc@1: 82.3113 (71.3840)  Acc@5: 95.5189 (90.6260)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-77.pth.tar', 71.38400007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-73.pth.tar', 70.7700000756836)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-67.pth.tar', 70.68200004394531)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-76.pth.tar', 70.62199997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-72.pth.tar', 70.55999999511718)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-74.pth.tar', 70.53399997070312)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-75.pth.tar', 70.50200007080078)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-70.pth.tar', 70.48800002685547)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-63.pth.tar', 70.36400005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-65.pth.tar', 70.32200000488281)

Train: 78 [   0/1251 (  0%)]  Loss: 3.469 (3.47)  Time: 2.436s,  420.37/s  (2.436s,  420.37/s)  LR: 8.439e-04  Data: 1.473 (1.473)
Train: 78 [  50/1251 (  4%)]  Loss: 4.029 (3.75)  Time: 0.998s, 1026.06/s  (1.035s,  989.59/s)  LR: 8.439e-04  Data: 0.014 (0.043)
Train: 78 [ 100/1251 (  8%)]  Loss: 4.184 (3.89)  Time: 1.050s,  975.20/s  (1.019s, 1004.61/s)  LR: 8.439e-04  Data: 0.011 (0.027)
Train: 78 [ 150/1251 ( 12%)]  Loss: 3.772 (3.86)  Time: 0.995s, 1029.17/s  (1.013s, 1010.50/s)  LR: 8.439e-04  Data: 0.012 (0.022)
Train: 78 [ 200/1251 ( 16%)]  Loss: 3.392 (3.77)  Time: 1.001s, 1022.64/s  (1.009s, 1014.40/s)  LR: 8.439e-04  Data: 0.011 (0.019)
Train: 78 [ 250/1251 ( 20%)]  Loss: 3.720 (3.76)  Time: 1.048s,  977.22/s  (1.009s, 1014.65/s)  LR: 8.439e-04  Data: 0.011 (0.018)
Train: 78 [ 300/1251 ( 24%)]  Loss: 4.032 (3.80)  Time: 0.999s, 1025.28/s  (1.010s, 1013.69/s)  LR: 8.439e-04  Data: 0.011 (0.016)
Train: 78 [ 350/1251 ( 28%)]  Loss: 3.633 (3.78)  Time: 1.015s, 1008.59/s  (1.009s, 1015.10/s)  LR: 8.439e-04  Data: 0.010 (0.016)
Train: 78 [ 400/1251 ( 32%)]  Loss: 3.644 (3.76)  Time: 0.996s, 1027.87/s  (1.008s, 1015.70/s)  LR: 8.439e-04  Data: 0.011 (0.015)
Train: 78 [ 450/1251 ( 36%)]  Loss: 3.731 (3.76)  Time: 0.996s, 1028.36/s  (1.007s, 1016.91/s)  LR: 8.439e-04  Data: 0.012 (0.015)
Train: 78 [ 500/1251 ( 40%)]  Loss: 3.954 (3.78)  Time: 0.995s, 1028.66/s  (1.006s, 1017.80/s)  LR: 8.439e-04  Data: 0.010 (0.014)
Train: 78 [ 550/1251 ( 44%)]  Loss: 3.702 (3.77)  Time: 0.997s, 1027.47/s  (1.005s, 1018.43/s)  LR: 8.439e-04  Data: 0.011 (0.014)
Train: 78 [ 600/1251 ( 48%)]  Loss: 3.420 (3.74)  Time: 0.997s, 1026.77/s  (1.005s, 1019.16/s)  LR: 8.439e-04  Data: 0.012 (0.014)
Train: 78 [ 650/1251 ( 52%)]  Loss: 3.794 (3.75)  Time: 0.994s, 1029.94/s  (1.006s, 1018.39/s)  LR: 8.439e-04  Data: 0.010 (0.014)
Train: 78 [ 700/1251 ( 56%)]  Loss: 3.659 (3.74)  Time: 0.996s, 1028.01/s  (1.005s, 1018.71/s)  LR: 8.439e-04  Data: 0.010 (0.013)
Train: 78 [ 750/1251 ( 60%)]  Loss: 3.759 (3.74)  Time: 1.057s,  969.15/s  (1.005s, 1018.54/s)  LR: 8.439e-04  Data: 0.010 (0.013)
Train: 78 [ 800/1251 ( 64%)]  Loss: 3.900 (3.75)  Time: 0.996s, 1027.66/s  (1.005s, 1018.60/s)  LR: 8.439e-04  Data: 0.011 (0.013)
Train: 78 [ 850/1251 ( 68%)]  Loss: 3.399 (3.73)  Time: 0.999s, 1025.37/s  (1.005s, 1019.12/s)  LR: 8.439e-04  Data: 0.011 (0.013)
Train: 78 [ 900/1251 ( 72%)]  Loss: 4.081 (3.75)  Time: 0.996s, 1028.34/s  (1.006s, 1018.40/s)  LR: 8.439e-04  Data: 0.011 (0.013)
Train: 78 [ 950/1251 ( 76%)]  Loss: 3.899 (3.76)  Time: 1.033s,  991.69/s  (1.006s, 1018.22/s)  LR: 8.439e-04  Data: 0.010 (0.013)
Train: 78 [1000/1251 ( 80%)]  Loss: 3.687 (3.76)  Time: 0.999s, 1025.40/s  (1.006s, 1017.61/s)  LR: 8.439e-04  Data: 0.011 (0.013)
Train: 78 [1050/1251 ( 84%)]  Loss: 3.734 (3.75)  Time: 1.002s, 1021.62/s  (1.006s, 1017.85/s)  LR: 8.439e-04  Data: 0.012 (0.013)
Train: 78 [1100/1251 ( 88%)]  Loss: 3.843 (3.76)  Time: 0.997s, 1027.02/s  (1.006s, 1018.13/s)  LR: 8.439e-04  Data: 0.012 (0.012)
Train: 78 [1150/1251 ( 92%)]  Loss: 3.695 (3.76)  Time: 1.000s, 1024.04/s  (1.007s, 1017.01/s)  LR: 8.439e-04  Data: 0.011 (0.012)
Train: 78 [1200/1251 ( 96%)]  Loss: 3.891 (3.76)  Time: 1.042s,  983.01/s  (1.007s, 1016.88/s)  LR: 8.439e-04  Data: 0.012 (0.012)
Train: 78 [1250/1251 (100%)]  Loss: 3.587 (3.75)  Time: 0.984s, 1041.03/s  (1.008s, 1016.18/s)  LR: 8.439e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.608 (1.608)  Loss:  0.8900 (0.8900)  Acc@1: 85.8398 (85.8398)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.245 (0.572)  Loss:  0.9777 (1.5084)  Acc@1: 82.3113 (70.5800)  Acc@5: 94.1038 (90.2900)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-77.pth.tar', 71.38400007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-73.pth.tar', 70.7700000756836)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-67.pth.tar', 70.68200004394531)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-76.pth.tar', 70.62199997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-78.pth.tar', 70.58000007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-72.pth.tar', 70.55999999511718)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-74.pth.tar', 70.53399997070312)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-75.pth.tar', 70.50200007080078)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-70.pth.tar', 70.48800002685547)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-63.pth.tar', 70.36400005126953)

Train: 79 [   0/1251 (  0%)]  Loss: 3.508 (3.51)  Time: 2.555s,  400.78/s  (2.555s,  400.78/s)  LR: 8.401e-04  Data: 1.589 (1.589)
Train: 79 [  50/1251 (  4%)]  Loss: 3.614 (3.56)  Time: 0.998s, 1025.58/s  (1.052s,  973.34/s)  LR: 8.401e-04  Data: 0.010 (0.042)
Train: 79 [ 100/1251 (  8%)]  Loss: 3.502 (3.54)  Time: 0.999s, 1024.87/s  (1.033s,  990.95/s)  LR: 8.401e-04  Data: 0.011 (0.027)
Train: 79 [ 150/1251 ( 12%)]  Loss: 3.719 (3.59)  Time: 0.997s, 1027.31/s  (1.023s, 1001.39/s)  LR: 8.401e-04  Data: 0.011 (0.022)
Train: 79 [ 200/1251 ( 16%)]  Loss: 4.218 (3.71)  Time: 0.996s, 1028.59/s  (1.016s, 1007.59/s)  LR: 8.401e-04  Data: 0.012 (0.019)
Train: 79 [ 250/1251 ( 20%)]  Loss: 3.851 (3.74)  Time: 1.049s,  975.99/s  (1.015s, 1008.51/s)  LR: 8.401e-04  Data: 0.010 (0.017)
Train: 79 [ 300/1251 ( 24%)]  Loss: 3.479 (3.70)  Time: 0.998s, 1026.50/s  (1.013s, 1010.91/s)  LR: 8.401e-04  Data: 0.012 (0.016)
Train: 79 [ 350/1251 ( 28%)]  Loss: 4.243 (3.77)  Time: 0.996s, 1028.40/s  (1.012s, 1012.22/s)  LR: 8.401e-04  Data: 0.011 (0.016)
Train: 79 [ 400/1251 ( 32%)]  Loss: 3.711 (3.76)  Time: 0.995s, 1029.66/s  (1.010s, 1013.73/s)  LR: 8.401e-04  Data: 0.010 (0.015)
Train: 79 [ 450/1251 ( 36%)]  Loss: 3.880 (3.77)  Time: 1.028s,  996.00/s  (1.009s, 1014.94/s)  LR: 8.401e-04  Data: 0.011 (0.015)
Train: 79 [ 500/1251 ( 40%)]  Loss: 3.812 (3.78)  Time: 0.997s, 1026.93/s  (1.009s, 1015.36/s)  LR: 8.401e-04  Data: 0.011 (0.014)
Train: 79 [ 550/1251 ( 44%)]  Loss: 4.024 (3.80)  Time: 1.028s,  995.69/s  (1.008s, 1015.85/s)  LR: 8.401e-04  Data: 0.011 (0.014)
Train: 79 [ 600/1251 ( 48%)]  Loss: 3.841 (3.80)  Time: 0.997s, 1026.95/s  (1.007s, 1016.63/s)  LR: 8.401e-04  Data: 0.012 (0.014)
Train: 79 [ 650/1251 ( 52%)]  Loss: 3.259 (3.76)  Time: 0.995s, 1029.33/s  (1.007s, 1016.46/s)  LR: 8.401e-04  Data: 0.011 (0.013)
Train: 79 [ 700/1251 ( 56%)]  Loss: 3.910 (3.77)  Time: 0.994s, 1030.55/s  (1.007s, 1016.71/s)  LR: 8.401e-04  Data: 0.010 (0.013)
Train: 79 [ 750/1251 ( 60%)]  Loss: 3.808 (3.77)  Time: 0.997s, 1027.56/s  (1.007s, 1017.38/s)  LR: 8.401e-04  Data: 0.011 (0.013)
Train: 79 [ 800/1251 ( 64%)]  Loss: 3.944 (3.78)  Time: 0.996s, 1028.57/s  (1.006s, 1017.95/s)  LR: 8.401e-04  Data: 0.011 (0.013)
Train: 79 [ 850/1251 ( 68%)]  Loss: 3.792 (3.78)  Time: 0.998s, 1026.48/s  (1.006s, 1018.25/s)  LR: 8.401e-04  Data: 0.012 (0.013)
Train: 79 [ 900/1251 ( 72%)]  Loss: 3.958 (3.79)  Time: 1.064s,  962.76/s  (1.007s, 1016.90/s)  LR: 8.401e-04  Data: 0.011 (0.013)
Train: 79 [ 950/1251 ( 76%)]  Loss: 3.641 (3.79)  Time: 0.995s, 1028.70/s  (1.008s, 1015.81/s)  LR: 8.401e-04  Data: 0.012 (0.013)
Train: 79 [1000/1251 ( 80%)]  Loss: 4.213 (3.81)  Time: 0.999s, 1024.58/s  (1.008s, 1016.27/s)  LR: 8.401e-04  Data: 0.012 (0.013)
Train: 79 [1050/1251 ( 84%)]  Loss: 3.889 (3.81)  Time: 1.099s,  931.93/s  (1.008s, 1015.82/s)  LR: 8.401e-04  Data: 0.011 (0.013)
Train: 79 [1100/1251 ( 88%)]  Loss: 3.811 (3.81)  Time: 0.993s, 1030.98/s  (1.008s, 1015.46/s)  LR: 8.401e-04  Data: 0.010 (0.013)
Train: 79 [1150/1251 ( 92%)]  Loss: 3.896 (3.81)  Time: 1.052s,  973.56/s  (1.008s, 1015.64/s)  LR: 8.401e-04  Data: 0.011 (0.012)
Train: 79 [1200/1251 ( 96%)]  Loss: 3.712 (3.81)  Time: 0.994s, 1030.05/s  (1.008s, 1015.46/s)  LR: 8.401e-04  Data: 0.011 (0.012)
Train: 79 [1250/1251 (100%)]  Loss: 4.275 (3.83)  Time: 0.982s, 1043.12/s  (1.008s, 1015.88/s)  LR: 8.401e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.691 (1.691)  Loss:  0.8790 (0.8790)  Acc@1: 86.6211 (86.6211)  Acc@5: 96.2891 (96.2891)
Test: [  48/48]  Time: 0.245 (0.567)  Loss:  0.9643 (1.4498)  Acc@1: 83.7264 (70.8860)  Acc@5: 95.4009 (90.5040)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-77.pth.tar', 71.38400007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-79.pth.tar', 70.88599991210937)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-73.pth.tar', 70.7700000756836)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-67.pth.tar', 70.68200004394531)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-76.pth.tar', 70.62199997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-78.pth.tar', 70.58000007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-72.pth.tar', 70.55999999511718)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-74.pth.tar', 70.53399997070312)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-75.pth.tar', 70.50200007080078)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-70.pth.tar', 70.48800002685547)

Train: 80 [   0/1251 (  0%)]  Loss: 3.749 (3.75)  Time: 2.655s,  385.67/s  (2.655s,  385.67/s)  LR: 8.362e-04  Data: 1.693 (1.693)
Train: 80 [  50/1251 (  4%)]  Loss: 3.705 (3.73)  Time: 0.994s, 1030.26/s  (1.031s,  993.65/s)  LR: 8.362e-04  Data: 0.012 (0.044)
Train: 80 [ 100/1251 (  8%)]  Loss: 3.890 (3.78)  Time: 0.997s, 1026.58/s  (1.014s, 1009.89/s)  LR: 8.362e-04  Data: 0.011 (0.028)
Train: 80 [ 150/1251 ( 12%)]  Loss: 3.852 (3.80)  Time: 0.993s, 1030.73/s  (1.012s, 1012.31/s)  LR: 8.362e-04  Data: 0.010 (0.022)
Train: 80 [ 200/1251 ( 16%)]  Loss: 3.942 (3.83)  Time: 0.995s, 1028.74/s  (1.009s, 1015.10/s)  LR: 8.362e-04  Data: 0.011 (0.020)
Train: 80 [ 250/1251 ( 20%)]  Loss: 3.965 (3.85)  Time: 0.997s, 1027.57/s  (1.007s, 1016.44/s)  LR: 8.362e-04  Data: 0.011 (0.018)
Train: 80 [ 300/1251 ( 24%)]  Loss: 3.655 (3.82)  Time: 0.993s, 1030.95/s  (1.007s, 1016.69/s)  LR: 8.362e-04  Data: 0.011 (0.017)
Train: 80 [ 350/1251 ( 28%)]  Loss: 3.934 (3.84)  Time: 0.997s, 1026.74/s  (1.006s, 1017.47/s)  LR: 8.362e-04  Data: 0.011 (0.016)
Train: 80 [ 400/1251 ( 32%)]  Loss: 4.072 (3.86)  Time: 1.006s, 1018.02/s  (1.007s, 1016.60/s)  LR: 8.362e-04  Data: 0.011 (0.015)
Train: 80 [ 450/1251 ( 36%)]  Loss: 3.830 (3.86)  Time: 1.006s, 1018.13/s  (1.006s, 1017.42/s)  LR: 8.362e-04  Data: 0.011 (0.015)
Train: 80 [ 500/1251 ( 40%)]  Loss: 3.746 (3.85)  Time: 1.007s, 1017.11/s  (1.009s, 1015.21/s)  LR: 8.362e-04  Data: 0.011 (0.014)
Train: 80 [ 550/1251 ( 44%)]  Loss: 3.676 (3.83)  Time: 0.997s, 1027.06/s  (1.008s, 1015.56/s)  LR: 8.362e-04  Data: 0.011 (0.014)
Train: 80 [ 600/1251 ( 48%)]  Loss: 3.970 (3.85)  Time: 0.994s, 1029.99/s  (1.008s, 1016.05/s)  LR: 8.362e-04  Data: 0.010 (0.014)
Train: 80 [ 650/1251 ( 52%)]  Loss: 3.630 (3.83)  Time: 0.995s, 1029.37/s  (1.007s, 1016.73/s)  LR: 8.362e-04  Data: 0.010 (0.014)
Train: 80 [ 700/1251 ( 56%)]  Loss: 3.343 (3.80)  Time: 0.998s, 1025.59/s  (1.006s, 1017.51/s)  LR: 8.362e-04  Data: 0.011 (0.013)
Train: 80 [ 750/1251 ( 60%)]  Loss: 3.946 (3.81)  Time: 0.995s, 1029.18/s  (1.007s, 1017.33/s)  LR: 8.362e-04  Data: 0.011 (0.013)
Train: 80 [ 800/1251 ( 64%)]  Loss: 3.489 (3.79)  Time: 0.996s, 1028.55/s  (1.006s, 1017.76/s)  LR: 8.362e-04  Data: 0.011 (0.013)
Train: 80 [ 850/1251 ( 68%)]  Loss: 3.789 (3.79)  Time: 0.998s, 1026.30/s  (1.006s, 1018.24/s)  LR: 8.362e-04  Data: 0.011 (0.013)
Train: 80 [ 900/1251 ( 72%)]  Loss: 4.017 (3.80)  Time: 0.996s, 1028.32/s  (1.005s, 1018.47/s)  LR: 8.362e-04  Data: 0.011 (0.013)
Train: 80 [ 950/1251 ( 76%)]  Loss: 3.794 (3.80)  Time: 0.995s, 1029.58/s  (1.005s, 1018.88/s)  LR: 8.362e-04  Data: 0.010 (0.013)
Train: 80 [1000/1251 ( 80%)]  Loss: 3.563 (3.79)  Time: 0.995s, 1028.86/s  (1.005s, 1019.30/s)  LR: 8.362e-04  Data: 0.012 (0.013)
Train: 80 [1050/1251 ( 84%)]  Loss: 3.883 (3.79)  Time: 0.997s, 1027.37/s  (1.004s, 1019.50/s)  LR: 8.362e-04  Data: 0.011 (0.013)
Train: 80 [1100/1251 ( 88%)]  Loss: 3.743 (3.79)  Time: 0.995s, 1029.46/s  (1.004s, 1019.71/s)  LR: 8.362e-04  Data: 0.010 (0.013)
Train: 80 [1150/1251 ( 92%)]  Loss: 3.351 (3.77)  Time: 0.997s, 1027.13/s  (1.004s, 1020.08/s)  LR: 8.362e-04  Data: 0.012 (0.013)
Train: 80 [1200/1251 ( 96%)]  Loss: 3.822 (3.77)  Time: 0.997s, 1026.77/s  (1.004s, 1020.06/s)  LR: 8.362e-04  Data: 0.011 (0.012)
Train: 80 [1250/1251 (100%)]  Loss: 3.645 (3.77)  Time: 0.984s, 1040.70/s  (1.004s, 1019.85/s)  LR: 8.362e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.639 (1.639)  Loss:  0.8489 (0.8489)  Acc@1: 87.1094 (87.1094)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.245 (0.579)  Loss:  1.0476 (1.4770)  Acc@1: 83.8443 (71.1800)  Acc@5: 95.5189 (90.5300)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-77.pth.tar', 71.38400007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-80.pth.tar', 71.1799999633789)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-79.pth.tar', 70.88599991210937)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-73.pth.tar', 70.7700000756836)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-67.pth.tar', 70.68200004394531)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-76.pth.tar', 70.62199997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-78.pth.tar', 70.58000007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-72.pth.tar', 70.55999999511718)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-74.pth.tar', 70.53399997070312)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-75.pth.tar', 70.50200007080078)

Train: 81 [   0/1251 (  0%)]  Loss: 3.702 (3.70)  Time: 2.513s,  407.56/s  (2.513s,  407.56/s)  LR: 8.323e-04  Data: 1.546 (1.546)
Train: 81 [  50/1251 (  4%)]  Loss: 3.722 (3.71)  Time: 0.997s, 1027.27/s  (1.038s,  986.13/s)  LR: 8.323e-04  Data: 0.012 (0.041)
Train: 81 [ 100/1251 (  8%)]  Loss: 3.267 (3.56)  Time: 0.999s, 1024.92/s  (1.020s, 1004.32/s)  LR: 8.323e-04  Data: 0.010 (0.026)
Train: 81 [ 150/1251 ( 12%)]  Loss: 3.584 (3.57)  Time: 0.998s, 1026.33/s  (1.017s, 1006.97/s)  LR: 8.323e-04  Data: 0.011 (0.021)
Train: 81 [ 200/1251 ( 16%)]  Loss: 3.508 (3.56)  Time: 1.011s, 1012.67/s  (1.015s, 1009.35/s)  LR: 8.323e-04  Data: 0.011 (0.019)
Train: 81 [ 250/1251 ( 20%)]  Loss: 3.693 (3.58)  Time: 0.995s, 1029.11/s  (1.011s, 1012.41/s)  LR: 8.323e-04  Data: 0.011 (0.017)
Train: 81 [ 300/1251 ( 24%)]  Loss: 3.594 (3.58)  Time: 0.990s, 1033.88/s  (1.010s, 1013.77/s)  LR: 8.323e-04  Data: 0.011 (0.016)
Train: 81 [ 350/1251 ( 28%)]  Loss: 3.921 (3.62)  Time: 0.990s, 1034.41/s  (1.009s, 1014.83/s)  LR: 8.323e-04  Data: 0.011 (0.015)
Train: 81 [ 400/1251 ( 32%)]  Loss: 4.227 (3.69)  Time: 0.995s, 1028.87/s  (1.008s, 1016.26/s)  LR: 8.323e-04  Data: 0.011 (0.015)
Train: 81 [ 450/1251 ( 36%)]  Loss: 3.554 (3.68)  Time: 1.062s,  964.28/s  (1.010s, 1014.07/s)  LR: 8.323e-04  Data: 0.011 (0.015)
Train: 81 [ 500/1251 ( 40%)]  Loss: 3.934 (3.70)  Time: 1.036s,  988.54/s  (1.011s, 1013.32/s)  LR: 8.323e-04  Data: 0.011 (0.014)
Train: 81 [ 550/1251 ( 44%)]  Loss: 3.838 (3.71)  Time: 0.997s, 1027.50/s  (1.010s, 1013.95/s)  LR: 8.323e-04  Data: 0.012 (0.014)
Train: 81 [ 600/1251 ( 48%)]  Loss: 3.628 (3.71)  Time: 0.996s, 1028.34/s  (1.010s, 1014.22/s)  LR: 8.323e-04  Data: 0.011 (0.014)
Train: 81 [ 650/1251 ( 52%)]  Loss: 3.338 (3.68)  Time: 0.996s, 1028.51/s  (1.009s, 1014.86/s)  LR: 8.323e-04  Data: 0.011 (0.014)
Train: 81 [ 700/1251 ( 56%)]  Loss: 3.757 (3.68)  Time: 0.995s, 1029.05/s  (1.009s, 1015.09/s)  LR: 8.323e-04  Data: 0.011 (0.013)
Train: 81 [ 750/1251 ( 60%)]  Loss: 4.124 (3.71)  Time: 0.998s, 1026.15/s  (1.008s, 1015.87/s)  LR: 8.323e-04  Data: 0.011 (0.013)
Train: 81 [ 800/1251 ( 64%)]  Loss: 3.625 (3.71)  Time: 1.058s,  967.88/s  (1.008s, 1015.48/s)  LR: 8.323e-04  Data: 0.011 (0.013)
Train: 81 [ 850/1251 ( 68%)]  Loss: 3.914 (3.72)  Time: 0.995s, 1028.89/s  (1.008s, 1016.03/s)  LR: 8.323e-04  Data: 0.011 (0.013)
Train: 81 [ 900/1251 ( 72%)]  Loss: 3.928 (3.73)  Time: 1.001s, 1022.60/s  (1.008s, 1015.55/s)  LR: 8.323e-04  Data: 0.010 (0.013)
Train: 81 [ 950/1251 ( 76%)]  Loss: 3.623 (3.72)  Time: 1.057s,  969.06/s  (1.008s, 1015.46/s)  LR: 8.323e-04  Data: 0.012 (0.013)
Train: 81 [1000/1251 ( 80%)]  Loss: 3.590 (3.72)  Time: 0.995s, 1029.26/s  (1.009s, 1015.07/s)  LR: 8.323e-04  Data: 0.011 (0.013)
Train: 81 [1050/1251 ( 84%)]  Loss: 3.793 (3.72)  Time: 0.998s, 1026.16/s  (1.009s, 1014.84/s)  LR: 8.323e-04  Data: 0.011 (0.013)
Train: 81 [1100/1251 ( 88%)]  Loss: 3.896 (3.73)  Time: 0.998s, 1025.54/s  (1.009s, 1015.30/s)  LR: 8.323e-04  Data: 0.012 (0.012)
Train: 81 [1150/1251 ( 92%)]  Loss: 3.594 (3.72)  Time: 0.995s, 1029.30/s  (1.008s, 1015.61/s)  LR: 8.323e-04  Data: 0.010 (0.012)
Train: 81 [1200/1251 ( 96%)]  Loss: 3.760 (3.72)  Time: 1.055s,  970.72/s  (1.008s, 1015.88/s)  LR: 8.323e-04  Data: 0.011 (0.012)
Train: 81 [1250/1251 (100%)]  Loss: 3.908 (3.73)  Time: 0.981s, 1044.27/s  (1.008s, 1016.00/s)  LR: 8.323e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.685 (1.685)  Loss:  0.8227 (0.8227)  Acc@1: 89.7461 (89.7461)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.245 (0.577)  Loss:  1.0088 (1.4516)  Acc@1: 81.7217 (71.2320)  Acc@5: 94.8113 (90.5840)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-77.pth.tar', 71.38400007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-81.pth.tar', 71.23199994628906)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-80.pth.tar', 71.1799999633789)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-79.pth.tar', 70.88599991210937)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-73.pth.tar', 70.7700000756836)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-67.pth.tar', 70.68200004394531)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-76.pth.tar', 70.62199997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-78.pth.tar', 70.58000007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-72.pth.tar', 70.55999999511718)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-74.pth.tar', 70.53399997070312)

Train: 82 [   0/1251 (  0%)]  Loss: 3.977 (3.98)  Time: 2.404s,  426.00/s  (2.404s,  426.00/s)  LR: 8.284e-04  Data: 1.447 (1.447)
Train: 82 [  50/1251 (  4%)]  Loss: 3.571 (3.77)  Time: 0.995s, 1028.72/s  (1.030s,  993.74/s)  LR: 8.284e-04  Data: 0.011 (0.040)
Train: 82 [ 100/1251 (  8%)]  Loss: 3.790 (3.78)  Time: 0.999s, 1025.17/s  (1.020s, 1004.19/s)  LR: 8.284e-04  Data: 0.011 (0.026)
Train: 82 [ 150/1251 ( 12%)]  Loss: 4.014 (3.84)  Time: 1.040s,  984.94/s  (1.017s, 1006.87/s)  LR: 8.284e-04  Data: 0.012 (0.021)
Train: 82 [ 200/1251 ( 16%)]  Loss: 3.880 (3.85)  Time: 1.035s,  989.48/s  (1.019s, 1005.04/s)  LR: 8.284e-04  Data: 0.012 (0.019)
Train: 82 [ 250/1251 ( 20%)]  Loss: 4.296 (3.92)  Time: 0.998s, 1026.09/s  (1.017s, 1007.17/s)  LR: 8.284e-04  Data: 0.012 (0.017)
Train: 82 [ 300/1251 ( 24%)]  Loss: 3.743 (3.90)  Time: 0.996s, 1028.29/s  (1.014s, 1009.69/s)  LR: 8.284e-04  Data: 0.011 (0.016)
Train: 82 [ 350/1251 ( 28%)]  Loss: 3.268 (3.82)  Time: 0.994s, 1029.71/s  (1.013s, 1011.29/s)  LR: 8.284e-04  Data: 0.011 (0.016)
Train: 82 [ 400/1251 ( 32%)]  Loss: 3.481 (3.78)  Time: 0.995s, 1029.43/s  (1.011s, 1012.80/s)  LR: 8.284e-04  Data: 0.011 (0.015)
Train: 82 [ 450/1251 ( 36%)]  Loss: 3.668 (3.77)  Time: 0.994s, 1030.58/s  (1.010s, 1013.93/s)  LR: 8.284e-04  Data: 0.010 (0.015)
Train: 82 [ 500/1251 ( 40%)]  Loss: 3.882 (3.78)  Time: 0.997s, 1026.91/s  (1.009s, 1014.97/s)  LR: 8.284e-04  Data: 0.012 (0.014)
Train: 82 [ 550/1251 ( 44%)]  Loss: 4.104 (3.81)  Time: 1.023s, 1001.45/s  (1.009s, 1015.28/s)  LR: 8.284e-04  Data: 0.011 (0.014)
Train: 82 [ 600/1251 ( 48%)]  Loss: 3.904 (3.81)  Time: 1.052s,  973.23/s  (1.008s, 1015.39/s)  LR: 8.284e-04  Data: 0.011 (0.014)
Train: 82 [ 650/1251 ( 52%)]  Loss: 3.973 (3.83)  Time: 0.998s, 1026.24/s  (1.009s, 1014.87/s)  LR: 8.284e-04  Data: 0.012 (0.014)
Train: 82 [ 700/1251 ( 56%)]  Loss: 3.820 (3.82)  Time: 0.995s, 1029.37/s  (1.008s, 1015.38/s)  LR: 8.284e-04  Data: 0.011 (0.013)
Train: 82 [ 750/1251 ( 60%)]  Loss: 3.672 (3.82)  Time: 0.996s, 1027.96/s  (1.009s, 1015.15/s)  LR: 8.284e-04  Data: 0.011 (0.013)
Train: 82 [ 800/1251 ( 64%)]  Loss: 3.603 (3.80)  Time: 0.997s, 1027.42/s  (1.008s, 1015.83/s)  LR: 8.284e-04  Data: 0.012 (0.013)
Train: 82 [ 850/1251 ( 68%)]  Loss: 3.685 (3.80)  Time: 1.035s,  989.39/s  (1.009s, 1015.27/s)  LR: 8.284e-04  Data: 0.012 (0.013)
Train: 82 [ 900/1251 ( 72%)]  Loss: 3.412 (3.78)  Time: 0.996s, 1027.98/s  (1.008s, 1015.80/s)  LR: 8.284e-04  Data: 0.011 (0.013)
Train: 82 [ 950/1251 ( 76%)]  Loss: 3.772 (3.78)  Time: 0.996s, 1028.17/s  (1.008s, 1016.01/s)  LR: 8.284e-04  Data: 0.010 (0.013)
Train: 82 [1000/1251 ( 80%)]  Loss: 3.512 (3.76)  Time: 1.067s,  959.73/s  (1.008s, 1015.75/s)  LR: 8.284e-04  Data: 0.012 (0.013)
Train: 82 [1050/1251 ( 84%)]  Loss: 3.905 (3.77)  Time: 0.998s, 1026.56/s  (1.008s, 1015.94/s)  LR: 8.284e-04  Data: 0.011 (0.013)
Train: 82 [1100/1251 ( 88%)]  Loss: 3.717 (3.77)  Time: 0.998s, 1026.39/s  (1.008s, 1016.06/s)  LR: 8.284e-04  Data: 0.011 (0.013)
Train: 82 [1150/1251 ( 92%)]  Loss: 3.454 (3.75)  Time: 0.995s, 1028.73/s  (1.007s, 1016.51/s)  LR: 8.284e-04  Data: 0.011 (0.012)
Train: 82 [1200/1251 ( 96%)]  Loss: 3.948 (3.76)  Time: 0.996s, 1027.62/s  (1.007s, 1017.00/s)  LR: 8.284e-04  Data: 0.011 (0.012)
Train: 82 [1250/1251 (100%)]  Loss: 4.112 (3.78)  Time: 0.982s, 1042.58/s  (1.006s, 1017.44/s)  LR: 8.284e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.614 (1.614)  Loss:  0.8971 (0.8971)  Acc@1: 86.7188 (86.7188)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.245 (0.574)  Loss:  0.9692 (1.4817)  Acc@1: 82.7830 (71.1900)  Acc@5: 95.7547 (90.7180)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-77.pth.tar', 71.38400007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-81.pth.tar', 71.23199994628906)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-82.pth.tar', 71.19000001953125)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-80.pth.tar', 71.1799999633789)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-79.pth.tar', 70.88599991210937)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-73.pth.tar', 70.7700000756836)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-67.pth.tar', 70.68200004394531)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-76.pth.tar', 70.62199997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-78.pth.tar', 70.58000007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-72.pth.tar', 70.55999999511718)

Train: 83 [   0/1251 (  0%)]  Loss: 3.847 (3.85)  Time: 2.641s,  387.78/s  (2.641s,  387.78/s)  LR: 8.245e-04  Data: 1.653 (1.653)
Train: 83 [  50/1251 (  4%)]  Loss: 3.564 (3.71)  Time: 1.063s,  963.39/s  (1.072s,  955.44/s)  LR: 8.245e-04  Data: 0.011 (0.044)
Train: 83 [ 100/1251 (  8%)]  Loss: 3.832 (3.75)  Time: 1.065s,  961.24/s  (1.039s,  985.32/s)  LR: 8.245e-04  Data: 0.012 (0.028)
Train: 83 [ 150/1251 ( 12%)]  Loss: 3.792 (3.76)  Time: 1.038s,  986.29/s  (1.030s,  994.21/s)  LR: 8.245e-04  Data: 0.011 (0.022)
Train: 83 [ 200/1251 ( 16%)]  Loss: 3.836 (3.77)  Time: 0.997s, 1027.53/s  (1.027s,  997.00/s)  LR: 8.245e-04  Data: 0.012 (0.019)
Train: 83 [ 250/1251 ( 20%)]  Loss: 3.762 (3.77)  Time: 0.996s, 1027.61/s  (1.024s,  999.96/s)  LR: 8.245e-04  Data: 0.011 (0.018)
Train: 83 [ 300/1251 ( 24%)]  Loss: 3.428 (3.72)  Time: 0.998s, 1026.26/s  (1.024s,  999.68/s)  LR: 8.245e-04  Data: 0.011 (0.017)
Train: 83 [ 350/1251 ( 28%)]  Loss: 3.670 (3.72)  Time: 0.998s, 1025.95/s  (1.021s, 1003.22/s)  LR: 8.245e-04  Data: 0.011 (0.016)
Train: 83 [ 400/1251 ( 32%)]  Loss: 3.691 (3.71)  Time: 0.996s, 1028.05/s  (1.020s, 1004.04/s)  LR: 8.245e-04  Data: 0.011 (0.015)
Train: 83 [ 450/1251 ( 36%)]  Loss: 3.697 (3.71)  Time: 1.005s, 1019.26/s  (1.018s, 1005.95/s)  LR: 8.245e-04  Data: 0.011 (0.015)
Train: 83 [ 500/1251 ( 40%)]  Loss: 3.535 (3.70)  Time: 1.046s,  978.68/s  (1.016s, 1007.77/s)  LR: 8.245e-04  Data: 0.012 (0.014)
Train: 83 [ 550/1251 ( 44%)]  Loss: 4.013 (3.72)  Time: 0.999s, 1024.92/s  (1.015s, 1008.79/s)  LR: 8.245e-04  Data: 0.011 (0.014)
Train: 83 [ 600/1251 ( 48%)]  Loss: 3.937 (3.74)  Time: 1.000s, 1024.05/s  (1.014s, 1009.54/s)  LR: 8.245e-04  Data: 0.012 (0.014)
Train: 83 [ 650/1251 ( 52%)]  Loss: 3.762 (3.74)  Time: 0.997s, 1027.31/s  (1.013s, 1010.47/s)  LR: 8.245e-04  Data: 0.011 (0.014)
Train: 83 [ 700/1251 ( 56%)]  Loss: 4.054 (3.76)  Time: 1.063s,  963.25/s  (1.014s, 1010.21/s)  LR: 8.245e-04  Data: 0.012 (0.014)
Train: 83 [ 750/1251 ( 60%)]  Loss: 3.957 (3.77)  Time: 1.004s, 1020.19/s  (1.013s, 1011.15/s)  LR: 8.245e-04  Data: 0.011 (0.013)
Train: 83 [ 800/1251 ( 64%)]  Loss: 3.763 (3.77)  Time: 1.059s,  966.70/s  (1.012s, 1011.50/s)  LR: 8.245e-04  Data: 0.012 (0.013)
Train: 83 [ 850/1251 ( 68%)]  Loss: 3.853 (3.78)  Time: 1.047s,  977.76/s  (1.012s, 1012.08/s)  LR: 8.245e-04  Data: 0.011 (0.013)
Train: 83 [ 900/1251 ( 72%)]  Loss: 3.643 (3.77)  Time: 0.996s, 1028.17/s  (1.012s, 1011.79/s)  LR: 8.245e-04  Data: 0.011 (0.013)
Train: 83 [ 950/1251 ( 76%)]  Loss: 3.864 (3.78)  Time: 0.994s, 1030.37/s  (1.011s, 1012.38/s)  LR: 8.245e-04  Data: 0.011 (0.013)
Train: 83 [1000/1251 ( 80%)]  Loss: 3.789 (3.78)  Time: 1.033s,  991.52/s  (1.012s, 1012.21/s)  LR: 8.245e-04  Data: 0.011 (0.013)
Train: 83 [1050/1251 ( 84%)]  Loss: 3.760 (3.77)  Time: 1.012s, 1011.88/s  (1.012s, 1012.28/s)  LR: 8.245e-04  Data: 0.012 (0.013)
Train: 83 [1100/1251 ( 88%)]  Loss: 3.585 (3.77)  Time: 0.995s, 1028.80/s  (1.011s, 1012.82/s)  LR: 8.245e-04  Data: 0.011 (0.013)
Train: 83 [1150/1251 ( 92%)]  Loss: 3.767 (3.77)  Time: 0.996s, 1028.43/s  (1.010s, 1013.46/s)  LR: 8.245e-04  Data: 0.011 (0.013)
Train: 83 [1200/1251 ( 96%)]  Loss: 4.047 (3.78)  Time: 0.997s, 1027.37/s  (1.010s, 1013.84/s)  LR: 8.245e-04  Data: 0.011 (0.012)
Train: 83 [1250/1251 (100%)]  Loss: 3.610 (3.77)  Time: 0.982s, 1043.07/s  (1.009s, 1014.40/s)  LR: 8.245e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.617 (1.617)  Loss:  0.8686 (0.8686)  Acc@1: 87.8906 (87.8906)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.245 (0.575)  Loss:  0.9737 (1.4775)  Acc@1: 82.1934 (71.8220)  Acc@5: 95.2830 (90.9580)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-83.pth.tar', 71.82200015136719)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-77.pth.tar', 71.38400007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-81.pth.tar', 71.23199994628906)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-82.pth.tar', 71.19000001953125)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-80.pth.tar', 71.1799999633789)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-79.pth.tar', 70.88599991210937)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-73.pth.tar', 70.7700000756836)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-67.pth.tar', 70.68200004394531)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-76.pth.tar', 70.62199997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-78.pth.tar', 70.58000007324219)

Train: 84 [   0/1251 (  0%)]  Loss: 4.075 (4.08)  Time: 4.187s,  244.58/s  (4.187s,  244.58/s)  LR: 8.205e-04  Data: 2.955 (2.955)
Train: 84 [  50/1251 (  4%)]  Loss: 3.939 (4.01)  Time: 1.043s,  981.87/s  (1.067s,  959.88/s)  LR: 8.205e-04  Data: 0.011 (0.069)
Train: 84 [ 100/1251 (  8%)]  Loss: 3.835 (3.95)  Time: 0.995s, 1029.39/s  (1.037s,  987.57/s)  LR: 8.205e-04  Data: 0.011 (0.040)
Train: 84 [ 150/1251 ( 12%)]  Loss: 3.688 (3.88)  Time: 1.063s,  963.20/s  (1.026s,  997.92/s)  LR: 8.205e-04  Data: 0.011 (0.031)
Train: 84 [ 200/1251 ( 16%)]  Loss: 3.687 (3.84)  Time: 0.995s, 1028.70/s  (1.022s, 1001.54/s)  LR: 8.205e-04  Data: 0.011 (0.026)
Train: 84 [ 250/1251 ( 20%)]  Loss: 3.778 (3.83)  Time: 1.001s, 1023.28/s  (1.019s, 1005.29/s)  LR: 8.205e-04  Data: 0.011 (0.023)
Train: 84 [ 300/1251 ( 24%)]  Loss: 3.724 (3.82)  Time: 0.996s, 1027.86/s  (1.018s, 1005.44/s)  LR: 8.205e-04  Data: 0.010 (0.021)
Train: 84 [ 350/1251 ( 28%)]  Loss: 3.719 (3.81)  Time: 1.025s,  999.42/s  (1.017s, 1007.24/s)  LR: 8.205e-04  Data: 0.010 (0.020)
Train: 84 [ 400/1251 ( 32%)]  Loss: 3.657 (3.79)  Time: 1.026s,  997.59/s  (1.015s, 1008.47/s)  LR: 8.205e-04  Data: 0.011 (0.018)
Train: 84 [ 450/1251 ( 36%)]  Loss: 3.715 (3.78)  Time: 0.996s, 1027.74/s  (1.014s, 1010.24/s)  LR: 8.205e-04  Data: 0.012 (0.018)
Train: 84 [ 500/1251 ( 40%)]  Loss: 3.961 (3.80)  Time: 1.055s,  970.79/s  (1.013s, 1010.96/s)  LR: 8.205e-04  Data: 0.010 (0.017)
Train: 84 [ 550/1251 ( 44%)]  Loss: 3.745 (3.79)  Time: 1.064s,  962.74/s  (1.012s, 1011.67/s)  LR: 8.205e-04  Data: 0.011 (0.016)
Train: 84 [ 600/1251 ( 48%)]  Loss: 3.792 (3.79)  Time: 0.999s, 1024.71/s  (1.011s, 1012.78/s)  LR: 8.205e-04  Data: 0.012 (0.016)
Train: 84 [ 650/1251 ( 52%)]  Loss: 3.647 (3.78)  Time: 0.997s, 1027.23/s  (1.010s, 1013.68/s)  LR: 8.205e-04  Data: 0.011 (0.016)
Train: 84 [ 700/1251 ( 56%)]  Loss: 3.675 (3.78)  Time: 1.090s,  939.38/s  (1.011s, 1012.90/s)  LR: 8.205e-04  Data: 0.012 (0.015)
Train: 84 [ 750/1251 ( 60%)]  Loss: 3.854 (3.78)  Time: 0.999s, 1025.37/s  (1.011s, 1013.21/s)  LR: 8.205e-04  Data: 0.011 (0.015)
Train: 84 [ 800/1251 ( 64%)]  Loss: 3.766 (3.78)  Time: 0.994s, 1030.32/s  (1.010s, 1013.83/s)  LR: 8.205e-04  Data: 0.010 (0.015)
Train: 84 [ 850/1251 ( 68%)]  Loss: 3.771 (3.78)  Time: 0.994s, 1030.38/s  (1.010s, 1013.87/s)  LR: 8.205e-04  Data: 0.011 (0.015)
Train: 84 [ 900/1251 ( 72%)]  Loss: 3.820 (3.78)  Time: 0.998s, 1026.29/s  (1.009s, 1014.36/s)  LR: 8.205e-04  Data: 0.011 (0.014)
Train: 84 [ 950/1251 ( 76%)]  Loss: 3.979 (3.79)  Time: 0.997s, 1027.37/s  (1.010s, 1014.29/s)  LR: 8.205e-04  Data: 0.011 (0.014)
Train: 84 [1000/1251 ( 80%)]  Loss: 3.794 (3.79)  Time: 0.996s, 1027.72/s  (1.009s, 1014.55/s)  LR: 8.205e-04  Data: 0.010 (0.014)
Train: 84 [1050/1251 ( 84%)]  Loss: 3.704 (3.79)  Time: 0.998s, 1025.99/s  (1.009s, 1014.86/s)  LR: 8.205e-04  Data: 0.012 (0.014)
Train: 84 [1100/1251 ( 88%)]  Loss: 3.810 (3.79)  Time: 0.994s, 1029.68/s  (1.009s, 1015.35/s)  LR: 8.205e-04  Data: 0.012 (0.014)
Train: 84 [1150/1251 ( 92%)]  Loss: 3.830 (3.79)  Time: 1.015s, 1008.78/s  (1.008s, 1015.51/s)  LR: 8.205e-04  Data: 0.011 (0.014)
Train: 84 [1200/1251 ( 96%)]  Loss: 4.051 (3.80)  Time: 0.998s, 1025.67/s  (1.008s, 1015.57/s)  LR: 8.205e-04  Data: 0.011 (0.014)
Train: 84 [1250/1251 (100%)]  Loss: 3.867 (3.80)  Time: 0.984s, 1040.26/s  (1.008s, 1015.58/s)  LR: 8.205e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.648 (1.648)  Loss:  0.8342 (0.8342)  Acc@1: 86.9141 (86.9141)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.245 (0.567)  Loss:  1.0604 (1.4728)  Acc@1: 81.4858 (71.4240)  Acc@5: 95.2830 (90.6180)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-83.pth.tar', 71.82200015136719)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-84.pth.tar', 71.42399997314453)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-77.pth.tar', 71.38400007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-81.pth.tar', 71.23199994628906)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-82.pth.tar', 71.19000001953125)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-80.pth.tar', 71.1799999633789)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-79.pth.tar', 70.88599991210937)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-73.pth.tar', 70.7700000756836)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-67.pth.tar', 70.68200004394531)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-76.pth.tar', 70.62199997558594)

Train: 85 [   0/1251 (  0%)]  Loss: 4.052 (4.05)  Time: 2.457s,  416.69/s  (2.457s,  416.69/s)  LR: 8.165e-04  Data: 1.487 (1.487)
Train: 85 [  50/1251 (  4%)]  Loss: 3.958 (4.01)  Time: 0.995s, 1028.78/s  (1.034s,  990.50/s)  LR: 8.165e-04  Data: 0.011 (0.046)
Train: 85 [ 100/1251 (  8%)]  Loss: 3.930 (3.98)  Time: 0.993s, 1031.25/s  (1.017s, 1007.10/s)  LR: 8.165e-04  Data: 0.011 (0.029)
Train: 85 [ 150/1251 ( 12%)]  Loss: 3.526 (3.87)  Time: 0.995s, 1029.36/s  (1.013s, 1010.53/s)  LR: 8.165e-04  Data: 0.011 (0.023)
Train: 85 [ 200/1251 ( 16%)]  Loss: 3.726 (3.84)  Time: 1.004s, 1019.79/s  (1.013s, 1011.17/s)  LR: 8.165e-04  Data: 0.011 (0.020)
Train: 85 [ 250/1251 ( 20%)]  Loss: 3.545 (3.79)  Time: 0.992s, 1031.83/s  (1.011s, 1013.35/s)  LR: 8.165e-04  Data: 0.010 (0.018)
Train: 85 [ 300/1251 ( 24%)]  Loss: 3.718 (3.78)  Time: 1.039s,  985.63/s  (1.016s, 1007.43/s)  LR: 8.165e-04  Data: 0.012 (0.017)
Train: 85 [ 350/1251 ( 28%)]  Loss: 3.989 (3.81)  Time: 0.995s, 1029.06/s  (1.015s, 1008.44/s)  LR: 8.165e-04  Data: 0.012 (0.016)
Train: 85 [ 400/1251 ( 32%)]  Loss: 3.375 (3.76)  Time: 0.994s, 1030.10/s  (1.014s, 1009.91/s)  LR: 8.165e-04  Data: 0.011 (0.016)
Train: 85 [ 450/1251 ( 36%)]  Loss: 3.360 (3.72)  Time: 1.042s,  982.87/s  (1.012s, 1011.45/s)  LR: 8.165e-04  Data: 0.010 (0.015)
Train: 85 [ 500/1251 ( 40%)]  Loss: 3.856 (3.73)  Time: 0.999s, 1025.53/s  (1.012s, 1012.34/s)  LR: 8.165e-04  Data: 0.012 (0.015)
Train: 85 [ 550/1251 ( 44%)]  Loss: 3.856 (3.74)  Time: 1.070s,  957.16/s  (1.011s, 1013.32/s)  LR: 8.165e-04  Data: 0.011 (0.014)
Train: 85 [ 600/1251 ( 48%)]  Loss: 3.887 (3.75)  Time: 1.022s, 1002.15/s  (1.011s, 1013.36/s)  LR: 8.165e-04  Data: 0.012 (0.014)
Train: 85 [ 650/1251 ( 52%)]  Loss: 3.814 (3.76)  Time: 1.077s,  950.93/s  (1.011s, 1013.07/s)  LR: 8.165e-04  Data: 0.011 (0.014)
Train: 85 [ 700/1251 ( 56%)]  Loss: 3.789 (3.76)  Time: 0.997s, 1027.56/s  (1.010s, 1013.36/s)  LR: 8.165e-04  Data: 0.011 (0.014)
Train: 85 [ 750/1251 ( 60%)]  Loss: 3.857 (3.76)  Time: 0.995s, 1028.95/s  (1.010s, 1013.38/s)  LR: 8.165e-04  Data: 0.012 (0.013)
Train: 85 [ 800/1251 ( 64%)]  Loss: 3.651 (3.76)  Time: 1.033s,  991.59/s  (1.011s, 1012.50/s)  LR: 8.165e-04  Data: 0.011 (0.013)
Train: 85 [ 850/1251 ( 68%)]  Loss: 3.563 (3.75)  Time: 1.044s,  980.41/s  (1.013s, 1010.93/s)  LR: 8.165e-04  Data: 0.011 (0.013)
Train: 85 [ 900/1251 ( 72%)]  Loss: 3.905 (3.76)  Time: 0.995s, 1029.33/s  (1.012s, 1011.58/s)  LR: 8.165e-04  Data: 0.011 (0.013)
Train: 85 [ 950/1251 ( 76%)]  Loss: 3.823 (3.76)  Time: 0.999s, 1025.03/s  (1.012s, 1012.24/s)  LR: 8.165e-04  Data: 0.012 (0.013)
Train: 85 [1000/1251 ( 80%)]  Loss: 3.721 (3.76)  Time: 0.993s, 1030.72/s  (1.011s, 1012.85/s)  LR: 8.165e-04  Data: 0.011 (0.013)
Train: 85 [1050/1251 ( 84%)]  Loss: 3.395 (3.74)  Time: 1.046s,  978.73/s  (1.011s, 1012.64/s)  LR: 8.165e-04  Data: 0.010 (0.013)
Train: 85 [1100/1251 ( 88%)]  Loss: 3.930 (3.75)  Time: 0.996s, 1027.80/s  (1.011s, 1013.16/s)  LR: 8.165e-04  Data: 0.011 (0.013)
Train: 85 [1150/1251 ( 92%)]  Loss: 3.630 (3.74)  Time: 1.033s,  991.48/s  (1.010s, 1013.50/s)  LR: 8.165e-04  Data: 0.010 (0.013)
Train: 85 [1200/1251 ( 96%)]  Loss: 4.006 (3.75)  Time: 0.995s, 1029.23/s  (1.011s, 1013.33/s)  LR: 8.165e-04  Data: 0.011 (0.013)
Train: 85 [1250/1251 (100%)]  Loss: 3.811 (3.76)  Time: 0.982s, 1042.76/s  (1.011s, 1013.13/s)  LR: 8.165e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.616 (1.616)  Loss:  0.8705 (0.8705)  Acc@1: 86.5234 (86.5234)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.245 (0.575)  Loss:  0.9548 (1.4351)  Acc@1: 82.5472 (70.9840)  Acc@5: 95.1651 (90.6920)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-83.pth.tar', 71.82200015136719)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-84.pth.tar', 71.42399997314453)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-77.pth.tar', 71.38400007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-81.pth.tar', 71.23199994628906)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-82.pth.tar', 71.19000001953125)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-80.pth.tar', 71.1799999633789)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-85.pth.tar', 70.98400004638673)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-79.pth.tar', 70.88599991210937)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-73.pth.tar', 70.7700000756836)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-67.pth.tar', 70.68200004394531)

Train: 86 [   0/1251 (  0%)]  Loss: 3.813 (3.81)  Time: 2.504s,  408.94/s  (2.504s,  408.94/s)  LR: 8.125e-04  Data: 1.511 (1.511)
Train: 86 [  50/1251 (  4%)]  Loss: 3.780 (3.80)  Time: 0.998s, 1026.28/s  (1.032s,  992.46/s)  LR: 8.125e-04  Data: 0.011 (0.041)
Train: 86 [ 100/1251 (  8%)]  Loss: 3.611 (3.73)  Time: 0.997s, 1026.97/s  (1.017s, 1006.87/s)  LR: 8.125e-04  Data: 0.011 (0.026)
Train: 86 [ 150/1251 ( 12%)]  Loss: 4.095 (3.82)  Time: 0.995s, 1029.30/s  (1.013s, 1011.31/s)  LR: 8.125e-04  Data: 0.010 (0.021)
Train: 86 [ 200/1251 ( 16%)]  Loss: 3.945 (3.85)  Time: 0.994s, 1029.69/s  (1.009s, 1014.43/s)  LR: 8.125e-04  Data: 0.011 (0.019)
Train: 86 [ 250/1251 ( 20%)]  Loss: 3.646 (3.82)  Time: 1.031s,  993.09/s  (1.010s, 1013.51/s)  LR: 8.125e-04  Data: 0.011 (0.017)
Train: 86 [ 300/1251 ( 24%)]  Loss: 3.556 (3.78)  Time: 0.996s, 1028.60/s  (1.009s, 1014.83/s)  LR: 8.125e-04  Data: 0.010 (0.016)
Train: 86 [ 350/1251 ( 28%)]  Loss: 3.484 (3.74)  Time: 0.997s, 1026.84/s  (1.007s, 1016.48/s)  LR: 8.125e-04  Data: 0.011 (0.015)
Train: 86 [ 400/1251 ( 32%)]  Loss: 3.605 (3.73)  Time: 1.057s,  968.56/s  (1.011s, 1013.23/s)  LR: 8.125e-04  Data: 0.011 (0.015)
Train: 86 [ 450/1251 ( 36%)]  Loss: 3.858 (3.74)  Time: 1.062s,  963.89/s  (1.012s, 1012.25/s)  LR: 8.125e-04  Data: 0.011 (0.014)
Train: 86 [ 500/1251 ( 40%)]  Loss: 3.985 (3.76)  Time: 0.999s, 1024.54/s  (1.010s, 1013.47/s)  LR: 8.125e-04  Data: 0.010 (0.014)
Train: 86 [ 550/1251 ( 44%)]  Loss: 3.868 (3.77)  Time: 1.007s, 1017.02/s  (1.010s, 1013.47/s)  LR: 8.125e-04  Data: 0.011 (0.014)
Train: 86 [ 600/1251 ( 48%)]  Loss: 3.718 (3.77)  Time: 0.995s, 1029.39/s  (1.009s, 1014.44/s)  LR: 8.125e-04  Data: 0.011 (0.014)
Train: 86 [ 650/1251 ( 52%)]  Loss: 3.779 (3.77)  Time: 0.994s, 1030.21/s  (1.010s, 1013.77/s)  LR: 8.125e-04  Data: 0.010 (0.013)
Train: 86 [ 700/1251 ( 56%)]  Loss: 3.787 (3.77)  Time: 0.997s, 1026.73/s  (1.009s, 1014.77/s)  LR: 8.125e-04  Data: 0.012 (0.013)
Train: 86 [ 750/1251 ( 60%)]  Loss: 3.815 (3.77)  Time: 1.025s,  998.82/s  (1.009s, 1014.93/s)  LR: 8.125e-04  Data: 0.011 (0.013)
Train: 86 [ 800/1251 ( 64%)]  Loss: 3.427 (3.75)  Time: 0.994s, 1030.50/s  (1.009s, 1015.30/s)  LR: 8.125e-04  Data: 0.010 (0.013)
Train: 86 [ 850/1251 ( 68%)]  Loss: 3.502 (3.74)  Time: 1.007s, 1016.43/s  (1.009s, 1015.24/s)  LR: 8.125e-04  Data: 0.012 (0.013)
Train: 86 [ 900/1251 ( 72%)]  Loss: 4.139 (3.76)  Time: 0.994s, 1029.99/s  (1.008s, 1015.67/s)  LR: 8.125e-04  Data: 0.011 (0.013)
Train: 86 [ 950/1251 ( 76%)]  Loss: 3.713 (3.76)  Time: 1.009s, 1015.34/s  (1.008s, 1015.50/s)  LR: 8.125e-04  Data: 0.012 (0.013)
Train: 86 [1000/1251 ( 80%)]  Loss: 3.957 (3.77)  Time: 0.995s, 1029.30/s  (1.008s, 1015.67/s)  LR: 8.125e-04  Data: 0.011 (0.013)
Train: 86 [1050/1251 ( 84%)]  Loss: 3.605 (3.76)  Time: 0.994s, 1030.09/s  (1.008s, 1015.92/s)  LR: 8.125e-04  Data: 0.011 (0.012)
Train: 86 [1100/1251 ( 88%)]  Loss: 3.826 (3.76)  Time: 1.034s,  989.86/s  (1.008s, 1015.98/s)  LR: 8.125e-04  Data: 0.011 (0.012)
Train: 86 [1150/1251 ( 92%)]  Loss: 3.630 (3.76)  Time: 0.995s, 1029.51/s  (1.009s, 1014.74/s)  LR: 8.125e-04  Data: 0.011 (0.012)
Train: 86 [1200/1251 ( 96%)]  Loss: 3.611 (3.75)  Time: 0.996s, 1028.26/s  (1.009s, 1015.21/s)  LR: 8.125e-04  Data: 0.011 (0.012)
Train: 86 [1250/1251 (100%)]  Loss: 4.095 (3.76)  Time: 0.993s, 1031.49/s  (1.008s, 1015.41/s)  LR: 8.125e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.607 (1.607)  Loss:  0.8868 (0.8868)  Acc@1: 87.1094 (87.1094)  Acc@5: 96.2891 (96.2891)
Test: [  48/48]  Time: 0.245 (0.568)  Loss:  0.9971 (1.4902)  Acc@1: 82.6651 (70.8360)  Acc@5: 95.4009 (90.5720)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-83.pth.tar', 71.82200015136719)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-84.pth.tar', 71.42399997314453)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-77.pth.tar', 71.38400007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-81.pth.tar', 71.23199994628906)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-82.pth.tar', 71.19000001953125)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-80.pth.tar', 71.1799999633789)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-85.pth.tar', 70.98400004638673)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-79.pth.tar', 70.88599991210937)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-86.pth.tar', 70.83600009765625)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-73.pth.tar', 70.7700000756836)

Train: 87 [   0/1251 (  0%)]  Loss: 3.551 (3.55)  Time: 2.413s,  424.42/s  (2.413s,  424.42/s)  LR: 8.084e-04  Data: 1.449 (1.449)
Train: 87 [  50/1251 (  4%)]  Loss: 3.382 (3.47)  Time: 1.055s,  970.96/s  (1.032s,  992.21/s)  LR: 8.084e-04  Data: 0.011 (0.039)
Train: 87 [ 100/1251 (  8%)]  Loss: 3.679 (3.54)  Time: 0.995s, 1029.33/s  (1.017s, 1007.05/s)  LR: 8.084e-04  Data: 0.012 (0.025)
Train: 87 [ 150/1251 ( 12%)]  Loss: 3.652 (3.57)  Time: 0.998s, 1025.92/s  (1.015s, 1009.13/s)  LR: 8.084e-04  Data: 0.011 (0.021)
Train: 87 [ 200/1251 ( 16%)]  Loss: 3.232 (3.50)  Time: 1.041s,  983.41/s  (1.011s, 1013.15/s)  LR: 8.084e-04  Data: 0.012 (0.018)
Train: 87 [ 250/1251 ( 20%)]  Loss: 3.718 (3.54)  Time: 0.996s, 1028.18/s  (1.009s, 1014.62/s)  LR: 8.084e-04  Data: 0.011 (0.017)
Train: 87 [ 300/1251 ( 24%)]  Loss: 3.377 (3.51)  Time: 0.995s, 1028.97/s  (1.010s, 1014.03/s)  LR: 8.084e-04  Data: 0.012 (0.016)
Train: 87 [ 350/1251 ( 28%)]  Loss: 3.948 (3.57)  Time: 1.037s,  987.57/s  (1.009s, 1014.43/s)  LR: 8.084e-04  Data: 0.011 (0.015)
Train: 87 [ 400/1251 ( 32%)]  Loss: 3.547 (3.56)  Time: 0.998s, 1026.29/s  (1.009s, 1015.33/s)  LR: 8.084e-04  Data: 0.011 (0.015)
Train: 87 [ 450/1251 ( 36%)]  Loss: 3.503 (3.56)  Time: 0.997s, 1026.81/s  (1.009s, 1014.78/s)  LR: 8.084e-04  Data: 0.012 (0.014)
Train: 87 [ 500/1251 ( 40%)]  Loss: 3.877 (3.59)  Time: 0.995s, 1028.86/s  (1.009s, 1014.86/s)  LR: 8.084e-04  Data: 0.011 (0.014)
Train: 87 [ 550/1251 ( 44%)]  Loss: 3.684 (3.60)  Time: 0.999s, 1025.41/s  (1.008s, 1015.84/s)  LR: 8.084e-04  Data: 0.011 (0.014)
Train: 87 [ 600/1251 ( 48%)]  Loss: 3.819 (3.61)  Time: 0.995s, 1028.73/s  (1.009s, 1014.53/s)  LR: 8.084e-04  Data: 0.012 (0.014)
Train: 87 [ 650/1251 ( 52%)]  Loss: 3.616 (3.61)  Time: 0.995s, 1029.43/s  (1.009s, 1015.34/s)  LR: 8.084e-04  Data: 0.011 (0.013)
Train: 87 [ 700/1251 ( 56%)]  Loss: 3.683 (3.62)  Time: 0.999s, 1025.32/s  (1.008s, 1015.98/s)  LR: 8.084e-04  Data: 0.012 (0.013)
Train: 87 [ 750/1251 ( 60%)]  Loss: 3.968 (3.64)  Time: 1.074s,  953.51/s  (1.008s, 1016.16/s)  LR: 8.084e-04  Data: 0.011 (0.013)
Train: 87 [ 800/1251 ( 64%)]  Loss: 4.036 (3.66)  Time: 0.997s, 1027.28/s  (1.007s, 1016.89/s)  LR: 8.084e-04  Data: 0.011 (0.013)
Train: 87 [ 850/1251 ( 68%)]  Loss: 3.683 (3.66)  Time: 0.998s, 1025.98/s  (1.007s, 1016.52/s)  LR: 8.084e-04  Data: 0.012 (0.013)
Train: 87 [ 900/1251 ( 72%)]  Loss: 3.842 (3.67)  Time: 0.997s, 1027.57/s  (1.007s, 1017.01/s)  LR: 8.084e-04  Data: 0.012 (0.013)
Train: 87 [ 950/1251 ( 76%)]  Loss: 3.574 (3.67)  Time: 0.997s, 1027.04/s  (1.007s, 1017.08/s)  LR: 8.084e-04  Data: 0.011 (0.013)
Train: 87 [1000/1251 ( 80%)]  Loss: 3.780 (3.67)  Time: 1.025s,  999.13/s  (1.007s, 1016.99/s)  LR: 8.084e-04  Data: 0.011 (0.013)
Train: 87 [1050/1251 ( 84%)]  Loss: 3.828 (3.68)  Time: 1.001s, 1023.22/s  (1.007s, 1016.72/s)  LR: 8.084e-04  Data: 0.011 (0.012)
Train: 87 [1100/1251 ( 88%)]  Loss: 4.127 (3.70)  Time: 0.997s, 1027.36/s  (1.007s, 1017.13/s)  LR: 8.084e-04  Data: 0.011 (0.012)
Train: 87 [1150/1251 ( 92%)]  Loss: 3.741 (3.70)  Time: 0.999s, 1025.19/s  (1.006s, 1017.53/s)  LR: 8.084e-04  Data: 0.010 (0.012)
Train: 87 [1200/1251 ( 96%)]  Loss: 4.021 (3.71)  Time: 1.006s, 1018.20/s  (1.006s, 1017.74/s)  LR: 8.084e-04  Data: 0.011 (0.012)
Train: 87 [1250/1251 (100%)]  Loss: 3.904 (3.72)  Time: 0.985s, 1039.54/s  (1.006s, 1018.05/s)  LR: 8.084e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.675 (1.675)  Loss:  0.9164 (0.9164)  Acc@1: 87.5000 (87.5000)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.245 (0.570)  Loss:  1.0295 (1.5352)  Acc@1: 83.4906 (71.2260)  Acc@5: 95.0472 (90.7080)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-83.pth.tar', 71.82200015136719)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-84.pth.tar', 71.42399997314453)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-77.pth.tar', 71.38400007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-81.pth.tar', 71.23199994628906)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-87.pth.tar', 71.22599993896485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-82.pth.tar', 71.19000001953125)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-80.pth.tar', 71.1799999633789)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-85.pth.tar', 70.98400004638673)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-79.pth.tar', 70.88599991210937)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-86.pth.tar', 70.83600009765625)

Train: 88 [   0/1251 (  0%)]  Loss: 3.772 (3.77)  Time: 2.691s,  380.58/s  (2.691s,  380.58/s)  LR: 8.043e-04  Data: 1.733 (1.733)
Train: 88 [  50/1251 (  4%)]  Loss: 3.530 (3.65)  Time: 0.999s, 1024.69/s  (1.034s,  990.31/s)  LR: 8.043e-04  Data: 0.011 (0.046)
Train: 88 [ 100/1251 (  8%)]  Loss: 3.907 (3.74)  Time: 0.997s, 1026.89/s  (1.017s, 1006.82/s)  LR: 8.043e-04  Data: 0.011 (0.029)
Train: 88 [ 150/1251 ( 12%)]  Loss: 3.569 (3.69)  Time: 1.002s, 1022.16/s  (1.011s, 1012.86/s)  LR: 8.043e-04  Data: 0.012 (0.023)
Train: 88 [ 200/1251 ( 16%)]  Loss: 3.854 (3.73)  Time: 1.000s, 1024.04/s  (1.009s, 1015.33/s)  LR: 8.043e-04  Data: 0.012 (0.020)
Train: 88 [ 250/1251 ( 20%)]  Loss: 3.942 (3.76)  Time: 0.995s, 1029.41/s  (1.007s, 1016.88/s)  LR: 8.043e-04  Data: 0.011 (0.018)
Train: 88 [ 300/1251 ( 24%)]  Loss: 3.726 (3.76)  Time: 0.994s, 1029.82/s  (1.006s, 1017.56/s)  LR: 8.043e-04  Data: 0.011 (0.017)
Train: 88 [ 350/1251 ( 28%)]  Loss: 3.859 (3.77)  Time: 0.991s, 1033.00/s  (1.005s, 1018.69/s)  LR: 8.043e-04  Data: 0.010 (0.016)
Train: 88 [ 400/1251 ( 32%)]  Loss: 3.925 (3.79)  Time: 0.999s, 1025.52/s  (1.006s, 1017.89/s)  LR: 8.043e-04  Data: 0.011 (0.016)
Train: 88 [ 450/1251 ( 36%)]  Loss: 3.885 (3.80)  Time: 0.993s, 1031.30/s  (1.006s, 1018.29/s)  LR: 8.043e-04  Data: 0.012 (0.015)
Train: 88 [ 500/1251 ( 40%)]  Loss: 3.731 (3.79)  Time: 0.994s, 1030.41/s  (1.005s, 1018.83/s)  LR: 8.043e-04  Data: 0.011 (0.015)
Train: 88 [ 550/1251 ( 44%)]  Loss: 3.753 (3.79)  Time: 0.989s, 1035.07/s  (1.005s, 1018.86/s)  LR: 8.043e-04  Data: 0.011 (0.014)
Train: 88 [ 600/1251 ( 48%)]  Loss: 4.179 (3.82)  Time: 1.072s,  955.47/s  (1.007s, 1016.59/s)  LR: 8.043e-04  Data: 0.012 (0.014)
Train: 88 [ 650/1251 ( 52%)]  Loss: 3.515 (3.80)  Time: 0.996s, 1028.45/s  (1.007s, 1016.98/s)  LR: 8.043e-04  Data: 0.012 (0.014)
Train: 88 [ 700/1251 ( 56%)]  Loss: 4.000 (3.81)  Time: 0.995s, 1028.85/s  (1.006s, 1017.63/s)  LR: 8.043e-04  Data: 0.011 (0.014)
Train: 88 [ 750/1251 ( 60%)]  Loss: 4.200 (3.83)  Time: 0.999s, 1024.72/s  (1.006s, 1017.50/s)  LR: 8.043e-04  Data: 0.011 (0.014)
Train: 88 [ 800/1251 ( 64%)]  Loss: 3.855 (3.84)  Time: 0.995s, 1028.87/s  (1.007s, 1016.98/s)  LR: 8.043e-04  Data: 0.011 (0.013)
Train: 88 [ 850/1251 ( 68%)]  Loss: 3.769 (3.83)  Time: 0.999s, 1024.84/s  (1.007s, 1016.49/s)  LR: 8.043e-04  Data: 0.010 (0.013)
Train: 88 [ 900/1251 ( 72%)]  Loss: 3.484 (3.81)  Time: 0.995s, 1029.30/s  (1.007s, 1016.89/s)  LR: 8.043e-04  Data: 0.010 (0.013)
Train: 88 [ 950/1251 ( 76%)]  Loss: 3.377 (3.79)  Time: 0.994s, 1030.38/s  (1.007s, 1016.89/s)  LR: 8.043e-04  Data: 0.011 (0.013)
Train: 88 [1000/1251 ( 80%)]  Loss: 3.846 (3.79)  Time: 0.996s, 1028.11/s  (1.007s, 1017.17/s)  LR: 8.043e-04  Data: 0.011 (0.013)
Train: 88 [1050/1251 ( 84%)]  Loss: 3.947 (3.80)  Time: 1.043s,  981.35/s  (1.008s, 1015.87/s)  LR: 8.043e-04  Data: 0.011 (0.013)
Train: 88 [1100/1251 ( 88%)]  Loss: 3.697 (3.80)  Time: 1.002s, 1022.17/s  (1.008s, 1015.39/s)  LR: 8.043e-04  Data: 0.011 (0.013)
Train: 88 [1150/1251 ( 92%)]  Loss: 3.879 (3.80)  Time: 0.995s, 1028.91/s  (1.008s, 1015.53/s)  LR: 8.043e-04  Data: 0.012 (0.013)
Train: 88 [1200/1251 ( 96%)]  Loss: 3.793 (3.80)  Time: 1.001s, 1023.41/s  (1.009s, 1014.68/s)  LR: 8.043e-04  Data: 0.012 (0.013)
Train: 88 [1250/1251 (100%)]  Loss: 3.769 (3.80)  Time: 0.988s, 1036.59/s  (1.009s, 1015.17/s)  LR: 8.043e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.727 (1.727)  Loss:  0.9115 (0.9115)  Acc@1: 85.8398 (85.8398)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.245 (0.565)  Loss:  0.9719 (1.4070)  Acc@1: 82.9009 (72.0240)  Acc@5: 95.8726 (91.3240)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-88.pth.tar', 72.02400007080078)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-83.pth.tar', 71.82200015136719)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-84.pth.tar', 71.42399997314453)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-77.pth.tar', 71.38400007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-81.pth.tar', 71.23199994628906)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-87.pth.tar', 71.22599993896485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-82.pth.tar', 71.19000001953125)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-80.pth.tar', 71.1799999633789)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-85.pth.tar', 70.98400004638673)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-79.pth.tar', 70.88599991210937)

Train: 89 [   0/1251 (  0%)]  Loss: 3.583 (3.58)  Time: 2.664s,  384.38/s  (2.664s,  384.38/s)  LR: 8.001e-04  Data: 1.704 (1.704)
Train: 89 [  50/1251 (  4%)]  Loss: 3.717 (3.65)  Time: 0.995s, 1028.65/s  (1.058s,  967.86/s)  LR: 8.001e-04  Data: 0.011 (0.045)
Train: 89 [ 100/1251 (  8%)]  Loss: 3.530 (3.61)  Time: 1.061s,  965.49/s  (1.029s,  994.78/s)  LR: 8.001e-04  Data: 0.011 (0.028)
Train: 89 [ 150/1251 ( 12%)]  Loss: 3.625 (3.61)  Time: 1.001s, 1023.32/s  (1.019s, 1004.44/s)  LR: 8.001e-04  Data: 0.011 (0.022)
Train: 89 [ 200/1251 ( 16%)]  Loss: 3.787 (3.65)  Time: 1.007s, 1017.03/s  (1.018s, 1005.83/s)  LR: 8.001e-04  Data: 0.011 (0.020)
Train: 89 [ 250/1251 ( 20%)]  Loss: 3.529 (3.63)  Time: 0.999s, 1025.34/s  (1.014s, 1009.54/s)  LR: 8.001e-04  Data: 0.011 (0.018)
Train: 89 [ 300/1251 ( 24%)]  Loss: 3.651 (3.63)  Time: 1.053s,  972.24/s  (1.012s, 1012.12/s)  LR: 8.001e-04  Data: 0.011 (0.017)
Train: 89 [ 350/1251 ( 28%)]  Loss: 3.731 (3.64)  Time: 0.998s, 1026.44/s  (1.012s, 1012.25/s)  LR: 8.001e-04  Data: 0.011 (0.016)
Train: 89 [ 400/1251 ( 32%)]  Loss: 3.621 (3.64)  Time: 0.994s, 1029.70/s  (1.010s, 1014.12/s)  LR: 8.001e-04  Data: 0.010 (0.015)
Train: 89 [ 450/1251 ( 36%)]  Loss: 3.734 (3.65)  Time: 0.994s, 1030.48/s  (1.008s, 1015.58/s)  LR: 8.001e-04  Data: 0.010 (0.015)
Train: 89 [ 500/1251 ( 40%)]  Loss: 3.872 (3.67)  Time: 0.993s, 1031.08/s  (1.007s, 1016.67/s)  LR: 8.001e-04  Data: 0.011 (0.014)
Train: 89 [ 550/1251 ( 44%)]  Loss: 3.570 (3.66)  Time: 0.998s, 1026.19/s  (1.006s, 1017.50/s)  LR: 8.001e-04  Data: 0.011 (0.014)
Train: 89 [ 600/1251 ( 48%)]  Loss: 3.834 (3.68)  Time: 0.995s, 1028.77/s  (1.006s, 1018.32/s)  LR: 8.001e-04  Data: 0.011 (0.014)
Train: 89 [ 650/1251 ( 52%)]  Loss: 3.778 (3.68)  Time: 0.997s, 1027.23/s  (1.005s, 1018.92/s)  LR: 8.001e-04  Data: 0.012 (0.014)
Train: 89 [ 700/1251 ( 56%)]  Loss: 3.610 (3.68)  Time: 0.996s, 1027.81/s  (1.004s, 1019.46/s)  LR: 8.001e-04  Data: 0.011 (0.013)
Train: 89 [ 750/1251 ( 60%)]  Loss: 3.683 (3.68)  Time: 0.994s, 1030.57/s  (1.004s, 1019.97/s)  LR: 8.001e-04  Data: 0.010 (0.013)
Train: 89 [ 800/1251 ( 64%)]  Loss: 3.784 (3.68)  Time: 1.061s,  965.45/s  (1.005s, 1019.15/s)  LR: 8.001e-04  Data: 0.012 (0.013)
Train: 89 [ 850/1251 ( 68%)]  Loss: 3.765 (3.69)  Time: 1.000s, 1024.48/s  (1.005s, 1019.25/s)  LR: 8.001e-04  Data: 0.010 (0.013)
Train: 89 [ 900/1251 ( 72%)]  Loss: 3.915 (3.70)  Time: 0.995s, 1029.45/s  (1.004s, 1019.70/s)  LR: 8.001e-04  Data: 0.011 (0.013)
Train: 89 [ 950/1251 ( 76%)]  Loss: 3.709 (3.70)  Time: 0.998s, 1025.80/s  (1.004s, 1019.90/s)  LR: 8.001e-04  Data: 0.011 (0.013)
Train: 89 [1000/1251 ( 80%)]  Loss: 4.088 (3.72)  Time: 1.024s, 1000.35/s  (1.004s, 1019.79/s)  LR: 8.001e-04  Data: 0.011 (0.013)
Train: 89 [1050/1251 ( 84%)]  Loss: 3.771 (3.72)  Time: 0.994s, 1030.49/s  (1.004s, 1019.90/s)  LR: 8.001e-04  Data: 0.011 (0.013)
Train: 89 [1100/1251 ( 88%)]  Loss: 3.600 (3.72)  Time: 0.996s, 1027.71/s  (1.004s, 1020.24/s)  LR: 8.001e-04  Data: 0.011 (0.013)
Train: 89 [1150/1251 ( 92%)]  Loss: 3.879 (3.72)  Time: 0.997s, 1027.42/s  (1.003s, 1020.45/s)  LR: 8.001e-04  Data: 0.011 (0.013)
Train: 89 [1200/1251 ( 96%)]  Loss: 3.348 (3.71)  Time: 0.995s, 1028.67/s  (1.003s, 1020.80/s)  LR: 8.001e-04  Data: 0.011 (0.012)
Train: 89 [1250/1251 (100%)]  Loss: 3.663 (3.71)  Time: 0.982s, 1042.52/s  (1.003s, 1021.06/s)  LR: 8.001e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.636 (1.636)  Loss:  0.8783 (0.8783)  Acc@1: 86.7188 (86.7188)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.245 (0.570)  Loss:  0.9898 (1.4554)  Acc@1: 83.4906 (71.5380)  Acc@5: 96.1085 (90.8060)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-88.pth.tar', 72.02400007080078)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-83.pth.tar', 71.82200015136719)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-89.pth.tar', 71.53800006835938)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-84.pth.tar', 71.42399997314453)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-77.pth.tar', 71.38400007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-81.pth.tar', 71.23199994628906)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-87.pth.tar', 71.22599993896485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-82.pth.tar', 71.19000001953125)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-80.pth.tar', 71.1799999633789)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-85.pth.tar', 70.98400004638673)

Train: 90 [   0/1251 (  0%)]  Loss: 3.714 (3.71)  Time: 2.459s,  416.36/s  (2.459s,  416.36/s)  LR: 7.960e-04  Data: 1.498 (1.498)
Train: 90 [  50/1251 (  4%)]  Loss: 3.674 (3.69)  Time: 0.998s, 1026.19/s  (1.039s,  985.52/s)  LR: 7.960e-04  Data: 0.011 (0.041)
Train: 90 [ 100/1251 (  8%)]  Loss: 3.928 (3.77)  Time: 0.996s, 1028.24/s  (1.020s, 1003.89/s)  LR: 7.960e-04  Data: 0.011 (0.026)
Train: 90 [ 150/1251 ( 12%)]  Loss: 3.348 (3.67)  Time: 0.991s, 1032.95/s  (1.013s, 1010.85/s)  LR: 7.960e-04  Data: 0.010 (0.021)
Train: 90 [ 200/1251 ( 16%)]  Loss: 3.607 (3.65)  Time: 1.043s,  982.08/s  (1.017s, 1007.35/s)  LR: 7.960e-04  Data: 0.010 (0.019)
Train: 90 [ 250/1251 ( 20%)]  Loss: 3.821 (3.68)  Time: 0.997s, 1027.13/s  (1.018s, 1005.73/s)  LR: 7.960e-04  Data: 0.011 (0.017)
Train: 90 [ 300/1251 ( 24%)]  Loss: 3.501 (3.66)  Time: 0.997s, 1027.15/s  (1.015s, 1008.88/s)  LR: 7.960e-04  Data: 0.011 (0.016)
Train: 90 [ 350/1251 ( 28%)]  Loss: 3.408 (3.63)  Time: 1.004s, 1019.59/s  (1.013s, 1011.14/s)  LR: 7.960e-04  Data: 0.011 (0.015)
Train: 90 [ 400/1251 ( 32%)]  Loss: 3.795 (3.64)  Time: 1.055s,  970.71/s  (1.014s, 1010.32/s)  LR: 7.960e-04  Data: 0.011 (0.015)
Train: 90 [ 450/1251 ( 36%)]  Loss: 3.702 (3.65)  Time: 0.994s, 1029.73/s  (1.015s, 1009.22/s)  LR: 7.960e-04  Data: 0.011 (0.014)
Train: 90 [ 500/1251 ( 40%)]  Loss: 3.679 (3.65)  Time: 1.036s,  988.82/s  (1.014s, 1009.76/s)  LR: 7.960e-04  Data: 0.011 (0.014)
Train: 90 [ 550/1251 ( 44%)]  Loss: 3.673 (3.65)  Time: 1.011s, 1012.37/s  (1.013s, 1010.46/s)  LR: 7.960e-04  Data: 0.010 (0.014)
Train: 90 [ 600/1251 ( 48%)]  Loss: 3.868 (3.67)  Time: 0.998s, 1025.88/s  (1.013s, 1010.66/s)  LR: 7.960e-04  Data: 0.011 (0.014)
Train: 90 [ 650/1251 ( 52%)]  Loss: 3.860 (3.68)  Time: 1.054s,  971.91/s  (1.012s, 1011.64/s)  LR: 7.960e-04  Data: 0.010 (0.013)
Train: 90 [ 700/1251 ( 56%)]  Loss: 3.933 (3.70)  Time: 1.001s, 1022.84/s  (1.012s, 1012.21/s)  LR: 7.960e-04  Data: 0.012 (0.013)
Train: 90 [ 750/1251 ( 60%)]  Loss: 3.674 (3.70)  Time: 0.999s, 1024.94/s  (1.011s, 1012.49/s)  LR: 7.960e-04  Data: 0.011 (0.013)
Train: 90 [ 800/1251 ( 64%)]  Loss: 4.043 (3.72)  Time: 0.994s, 1030.19/s  (1.011s, 1013.09/s)  LR: 7.960e-04  Data: 0.011 (0.013)
Train: 90 [ 850/1251 ( 68%)]  Loss: 3.274 (3.69)  Time: 0.995s, 1029.64/s  (1.010s, 1013.56/s)  LR: 7.960e-04  Data: 0.011 (0.013)
Train: 90 [ 900/1251 ( 72%)]  Loss: 3.524 (3.69)  Time: 0.995s, 1029.11/s  (1.010s, 1014.35/s)  LR: 7.960e-04  Data: 0.011 (0.013)
Train: 90 [ 950/1251 ( 76%)]  Loss: 3.206 (3.66)  Time: 0.998s, 1026.25/s  (1.009s, 1015.02/s)  LR: 7.960e-04  Data: 0.010 (0.013)
Train: 90 [1000/1251 ( 80%)]  Loss: 3.627 (3.66)  Time: 0.996s, 1027.78/s  (1.008s, 1015.58/s)  LR: 7.960e-04  Data: 0.011 (0.013)
Train: 90 [1050/1251 ( 84%)]  Loss: 3.761 (3.66)  Time: 0.995s, 1029.01/s  (1.008s, 1016.12/s)  LR: 7.960e-04  Data: 0.013 (0.012)
Train: 90 [1100/1251 ( 88%)]  Loss: 4.002 (3.68)  Time: 0.996s, 1027.96/s  (1.008s, 1015.43/s)  LR: 7.960e-04  Data: 0.012 (0.012)
Train: 90 [1150/1251 ( 92%)]  Loss: 3.608 (3.68)  Time: 0.998s, 1026.34/s  (1.008s, 1015.89/s)  LR: 7.960e-04  Data: 0.012 (0.012)
Train: 90 [1200/1251 ( 96%)]  Loss: 3.841 (3.68)  Time: 1.031s,  992.94/s  (1.008s, 1015.55/s)  LR: 7.960e-04  Data: 0.011 (0.012)
Train: 90 [1250/1251 (100%)]  Loss: 3.705 (3.68)  Time: 0.981s, 1043.33/s  (1.008s, 1015.87/s)  LR: 7.960e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.608 (1.608)  Loss:  0.8524 (0.8524)  Acc@1: 86.9141 (86.9141)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.245 (0.579)  Loss:  0.9296 (1.4653)  Acc@1: 84.1981 (71.9080)  Acc@5: 96.2264 (90.9100)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-88.pth.tar', 72.02400007080078)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-90.pth.tar', 71.9080001171875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-83.pth.tar', 71.82200015136719)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-89.pth.tar', 71.53800006835938)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-84.pth.tar', 71.42399997314453)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-77.pth.tar', 71.38400007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-81.pth.tar', 71.23199994628906)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-87.pth.tar', 71.22599993896485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-82.pth.tar', 71.19000001953125)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-80.pth.tar', 71.1799999633789)

Train: 91 [   0/1251 (  0%)]  Loss: 3.472 (3.47)  Time: 2.491s,  411.02/s  (2.491s,  411.02/s)  LR: 7.917e-04  Data: 1.528 (1.528)
Train: 91 [  50/1251 (  4%)]  Loss: 4.012 (3.74)  Time: 0.997s, 1027.28/s  (1.043s,  981.41/s)  LR: 7.917e-04  Data: 0.012 (0.041)
Train: 91 [ 100/1251 (  8%)]  Loss: 3.792 (3.76)  Time: 0.994s, 1029.98/s  (1.025s,  999.00/s)  LR: 7.917e-04  Data: 0.011 (0.026)
Train: 91 [ 150/1251 ( 12%)]  Loss: 3.703 (3.74)  Time: 1.004s, 1020.19/s  (1.016s, 1008.05/s)  LR: 7.917e-04  Data: 0.011 (0.021)
Train: 91 [ 200/1251 ( 16%)]  Loss: 3.742 (3.74)  Time: 1.008s, 1015.48/s  (1.012s, 1011.38/s)  LR: 7.917e-04  Data: 0.012 (0.019)
Train: 91 [ 250/1251 ( 20%)]  Loss: 3.713 (3.74)  Time: 0.999s, 1024.65/s  (1.010s, 1014.16/s)  LR: 7.917e-04  Data: 0.014 (0.017)
Train: 91 [ 300/1251 ( 24%)]  Loss: 3.761 (3.74)  Time: 1.002s, 1022.21/s  (1.008s, 1016.37/s)  LR: 7.917e-04  Data: 0.012 (0.016)
Train: 91 [ 350/1251 ( 28%)]  Loss: 3.828 (3.75)  Time: 0.996s, 1027.93/s  (1.006s, 1017.61/s)  LR: 7.917e-04  Data: 0.010 (0.015)
Train: 91 [ 400/1251 ( 32%)]  Loss: 3.740 (3.75)  Time: 0.998s, 1026.06/s  (1.005s, 1018.60/s)  LR: 7.917e-04  Data: 0.011 (0.015)
Train: 91 [ 450/1251 ( 36%)]  Loss: 3.660 (3.74)  Time: 1.002s, 1022.23/s  (1.008s, 1016.00/s)  LR: 7.917e-04  Data: 0.011 (0.014)
Train: 91 [ 500/1251 ( 40%)]  Loss: 3.211 (3.69)  Time: 0.993s, 1030.84/s  (1.007s, 1016.62/s)  LR: 7.917e-04  Data: 0.011 (0.014)
Train: 91 [ 550/1251 ( 44%)]  Loss: 3.691 (3.69)  Time: 1.063s,  963.15/s  (1.007s, 1017.10/s)  LR: 7.917e-04  Data: 0.012 (0.014)
Train: 91 [ 600/1251 ( 48%)]  Loss: 3.954 (3.71)  Time: 0.997s, 1026.76/s  (1.007s, 1017.38/s)  LR: 7.917e-04  Data: 0.012 (0.014)
Train: 91 [ 650/1251 ( 52%)]  Loss: 3.509 (3.70)  Time: 0.994s, 1030.00/s  (1.006s, 1017.96/s)  LR: 7.917e-04  Data: 0.011 (0.013)
Train: 91 [ 700/1251 ( 56%)]  Loss: 3.907 (3.71)  Time: 0.996s, 1027.99/s  (1.005s, 1018.42/s)  LR: 7.917e-04  Data: 0.011 (0.013)
Train: 91 [ 750/1251 ( 60%)]  Loss: 3.371 (3.69)  Time: 1.003s, 1020.91/s  (1.006s, 1017.81/s)  LR: 7.917e-04  Data: 0.011 (0.013)
Train: 91 [ 800/1251 ( 64%)]  Loss: 3.711 (3.69)  Time: 1.057s,  968.72/s  (1.006s, 1018.00/s)  LR: 7.917e-04  Data: 0.011 (0.013)
Train: 91 [ 850/1251 ( 68%)]  Loss: 3.543 (3.68)  Time: 0.994s, 1029.93/s  (1.006s, 1017.95/s)  LR: 7.917e-04  Data: 0.011 (0.013)
Train: 91 [ 900/1251 ( 72%)]  Loss: 3.951 (3.70)  Time: 0.995s, 1029.35/s  (1.006s, 1017.99/s)  LR: 7.917e-04  Data: 0.012 (0.013)
Train: 91 [ 950/1251 ( 76%)]  Loss: 3.842 (3.71)  Time: 1.035s,  989.32/s  (1.006s, 1018.04/s)  LR: 7.917e-04  Data: 0.010 (0.013)
Train: 91 [1000/1251 ( 80%)]  Loss: 4.005 (3.72)  Time: 0.996s, 1028.50/s  (1.006s, 1017.88/s)  LR: 7.917e-04  Data: 0.011 (0.013)
Train: 91 [1050/1251 ( 84%)]  Loss: 3.906 (3.73)  Time: 0.997s, 1027.44/s  (1.006s, 1018.21/s)  LR: 7.917e-04  Data: 0.012 (0.012)
Train: 91 [1100/1251 ( 88%)]  Loss: 3.900 (3.74)  Time: 0.997s, 1027.28/s  (1.005s, 1018.41/s)  LR: 7.917e-04  Data: 0.011 (0.012)
Train: 91 [1150/1251 ( 92%)]  Loss: 3.473 (3.72)  Time: 0.996s, 1028.54/s  (1.006s, 1018.22/s)  LR: 7.917e-04  Data: 0.011 (0.012)
Train: 91 [1200/1251 ( 96%)]  Loss: 3.586 (3.72)  Time: 0.995s, 1028.73/s  (1.005s, 1018.48/s)  LR: 7.917e-04  Data: 0.010 (0.012)
Train: 91 [1250/1251 (100%)]  Loss: 3.381 (3.71)  Time: 0.982s, 1043.05/s  (1.005s, 1018.73/s)  LR: 7.917e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.606 (1.606)  Loss:  0.9199 (0.9199)  Acc@1: 86.5234 (86.5234)  Acc@5: 95.7031 (95.7031)
Test: [  48/48]  Time: 0.245 (0.574)  Loss:  0.9126 (1.4995)  Acc@1: 83.6085 (70.6520)  Acc@5: 95.0472 (90.3260)
Train: 92 [   0/1251 (  0%)]  Loss: 3.958 (3.96)  Time: 2.646s,  386.94/s  (2.646s,  386.94/s)  LR: 7.875e-04  Data: 1.678 (1.678)
Train: 92 [  50/1251 (  4%)]  Loss: 3.922 (3.94)  Time: 0.995s, 1029.50/s  (1.041s,  983.99/s)  LR: 7.875e-04  Data: 0.011 (0.044)
Train: 92 [ 100/1251 (  8%)]  Loss: 3.540 (3.81)  Time: 1.059s,  967.25/s  (1.024s, 1000.13/s)  LR: 7.875e-04  Data: 0.010 (0.028)
Train: 92 [ 150/1251 ( 12%)]  Loss: 3.796 (3.80)  Time: 0.995s, 1029.65/s  (1.016s, 1008.09/s)  LR: 7.875e-04  Data: 0.011 (0.022)
Train: 92 [ 200/1251 ( 16%)]  Loss: 3.624 (3.77)  Time: 1.059s,  966.75/s  (1.015s, 1008.39/s)  LR: 7.875e-04  Data: 0.011 (0.019)
Train: 92 [ 250/1251 ( 20%)]  Loss: 3.679 (3.75)  Time: 0.996s, 1028.56/s  (1.016s, 1007.66/s)  LR: 7.875e-04  Data: 0.011 (0.018)
Train: 92 [ 300/1251 ( 24%)]  Loss: 3.832 (3.76)  Time: 0.997s, 1027.14/s  (1.016s, 1007.81/s)  LR: 7.875e-04  Data: 0.012 (0.017)
Train: 92 [ 350/1251 ( 28%)]  Loss: 3.602 (3.74)  Time: 0.993s, 1030.78/s  (1.013s, 1010.54/s)  LR: 7.875e-04  Data: 0.011 (0.016)
Train: 92 [ 400/1251 ( 32%)]  Loss: 3.695 (3.74)  Time: 1.064s,  962.65/s  (1.014s, 1009.59/s)  LR: 7.875e-04  Data: 0.011 (0.015)
Train: 92 [ 450/1251 ( 36%)]  Loss: 3.680 (3.73)  Time: 0.993s, 1030.72/s  (1.013s, 1011.06/s)  LR: 7.875e-04  Data: 0.010 (0.015)
Train: 92 [ 500/1251 ( 40%)]  Loss: 4.015 (3.76)  Time: 0.995s, 1028.81/s  (1.012s, 1011.37/s)  LR: 7.875e-04  Data: 0.011 (0.014)
Train: 92 [ 550/1251 ( 44%)]  Loss: 4.127 (3.79)  Time: 0.998s, 1026.29/s  (1.012s, 1012.33/s)  LR: 7.875e-04  Data: 0.011 (0.014)
Train: 92 [ 600/1251 ( 48%)]  Loss: 3.852 (3.79)  Time: 1.044s,  981.08/s  (1.012s, 1012.05/s)  LR: 7.875e-04  Data: 0.011 (0.014)
Train: 92 [ 650/1251 ( 52%)]  Loss: 3.606 (3.78)  Time: 1.058s,  967.99/s  (1.012s, 1011.69/s)  LR: 7.875e-04  Data: 0.011 (0.014)
Train: 92 [ 700/1251 ( 56%)]  Loss: 3.767 (3.78)  Time: 1.044s,  981.13/s  (1.013s, 1011.25/s)  LR: 7.875e-04  Data: 0.010 (0.013)
Train: 92 [ 750/1251 ( 60%)]  Loss: 3.848 (3.78)  Time: 0.998s, 1025.73/s  (1.012s, 1011.54/s)  LR: 7.875e-04  Data: 0.011 (0.013)
Train: 92 [ 800/1251 ( 64%)]  Loss: 3.489 (3.77)  Time: 1.001s, 1023.18/s  (1.011s, 1012.38/s)  LR: 7.875e-04  Data: 0.011 (0.013)
Train: 92 [ 850/1251 ( 68%)]  Loss: 3.805 (3.77)  Time: 0.997s, 1026.65/s  (1.011s, 1013.15/s)  LR: 7.875e-04  Data: 0.012 (0.013)
Train: 92 [ 900/1251 ( 72%)]  Loss: 3.785 (3.77)  Time: 0.997s, 1027.37/s  (1.010s, 1013.61/s)  LR: 7.875e-04  Data: 0.011 (0.013)
Train: 92 [ 950/1251 ( 76%)]  Loss: 3.830 (3.77)  Time: 0.997s, 1027.00/s  (1.010s, 1014.16/s)  LR: 7.875e-04  Data: 0.011 (0.013)
Train: 92 [1000/1251 ( 80%)]  Loss: 3.646 (3.77)  Time: 1.026s,  997.72/s  (1.009s, 1014.60/s)  LR: 7.875e-04  Data: 0.010 (0.013)
Train: 92 [1050/1251 ( 84%)]  Loss: 3.883 (3.77)  Time: 0.995s, 1028.87/s  (1.009s, 1015.16/s)  LR: 7.875e-04  Data: 0.011 (0.013)
Train: 92 [1100/1251 ( 88%)]  Loss: 4.031 (3.78)  Time: 1.000s, 1023.53/s  (1.009s, 1015.22/s)  LR: 7.875e-04  Data: 0.010 (0.013)
Train: 92 [1150/1251 ( 92%)]  Loss: 3.769 (3.78)  Time: 0.997s, 1027.17/s  (1.008s, 1015.58/s)  LR: 7.875e-04  Data: 0.011 (0.013)
Train: 92 [1200/1251 ( 96%)]  Loss: 3.532 (3.77)  Time: 0.999s, 1025.38/s  (1.008s, 1016.01/s)  LR: 7.875e-04  Data: 0.010 (0.012)
Train: 92 [1250/1251 (100%)]  Loss: 3.672 (3.77)  Time: 0.982s, 1042.62/s  (1.008s, 1015.94/s)  LR: 7.875e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.609 (1.609)  Loss:  0.7244 (0.7244)  Acc@1: 87.9883 (87.9883)  Acc@5: 96.2891 (96.2891)
Test: [  48/48]  Time: 0.245 (0.571)  Loss:  0.9229 (1.3630)  Acc@1: 80.8962 (72.1520)  Acc@5: 95.7547 (91.0580)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-92.pth.tar', 72.15199997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-88.pth.tar', 72.02400007080078)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-90.pth.tar', 71.9080001171875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-83.pth.tar', 71.82200015136719)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-89.pth.tar', 71.53800006835938)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-84.pth.tar', 71.42399997314453)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-77.pth.tar', 71.38400007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-81.pth.tar', 71.23199994628906)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-87.pth.tar', 71.22599993896485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-82.pth.tar', 71.19000001953125)

Train: 93 [   0/1251 (  0%)]  Loss: 4.203 (4.20)  Time: 4.220s,  242.64/s  (4.220s,  242.64/s)  LR: 7.832e-04  Data: 3.023 (3.023)
Train: 93 [  50/1251 (  4%)]  Loss: 3.514 (3.86)  Time: 1.051s,  973.91/s  (1.073s,  954.58/s)  LR: 7.832e-04  Data: 0.011 (0.070)
Train: 93 [ 100/1251 (  8%)]  Loss: 3.934 (3.88)  Time: 0.992s, 1032.61/s  (1.057s,  968.97/s)  LR: 7.832e-04  Data: 0.010 (0.041)
Train: 93 [ 150/1251 ( 12%)]  Loss: 3.751 (3.85)  Time: 1.035s,  989.61/s  (1.041s,  983.84/s)  LR: 7.832e-04  Data: 0.012 (0.031)
Train: 93 [ 200/1251 ( 16%)]  Loss: 3.473 (3.78)  Time: 0.994s, 1030.47/s  (1.030s,  993.79/s)  LR: 7.832e-04  Data: 0.011 (0.026)
Train: 93 [ 250/1251 ( 20%)]  Loss: 3.722 (3.77)  Time: 0.996s, 1027.80/s  (1.024s, 1000.20/s)  LR: 7.832e-04  Data: 0.012 (0.023)
Train: 93 [ 300/1251 ( 24%)]  Loss: 3.712 (3.76)  Time: 1.046s,  978.75/s  (1.023s, 1001.07/s)  LR: 7.832e-04  Data: 0.010 (0.021)
Train: 93 [ 350/1251 ( 28%)]  Loss: 3.468 (3.72)  Time: 1.003s, 1020.60/s  (1.020s, 1003.61/s)  LR: 7.832e-04  Data: 0.011 (0.020)
Train: 93 [ 400/1251 ( 32%)]  Loss: 3.541 (3.70)  Time: 1.003s, 1021.27/s  (1.018s, 1006.13/s)  LR: 7.832e-04  Data: 0.011 (0.019)
Train: 93 [ 450/1251 ( 36%)]  Loss: 3.933 (3.73)  Time: 0.997s, 1026.67/s  (1.016s, 1007.87/s)  LR: 7.832e-04  Data: 0.014 (0.018)
Train: 93 [ 500/1251 ( 40%)]  Loss: 3.576 (3.71)  Time: 0.994s, 1029.80/s  (1.014s, 1009.46/s)  LR: 7.832e-04  Data: 0.011 (0.017)
Train: 93 [ 550/1251 ( 44%)]  Loss: 3.932 (3.73)  Time: 1.019s, 1005.17/s  (1.013s, 1010.80/s)  LR: 7.832e-04  Data: 0.011 (0.017)
Train: 93 [ 600/1251 ( 48%)]  Loss: 3.817 (3.74)  Time: 0.995s, 1028.73/s  (1.012s, 1012.15/s)  LR: 7.832e-04  Data: 0.012 (0.016)
Train: 93 [ 650/1251 ( 52%)]  Loss: 3.719 (3.74)  Time: 0.997s, 1026.57/s  (1.012s, 1012.12/s)  LR: 7.832e-04  Data: 0.012 (0.016)
Train: 93 [ 700/1251 ( 56%)]  Loss: 3.863 (3.74)  Time: 0.998s, 1025.90/s  (1.011s, 1012.98/s)  LR: 7.832e-04  Data: 0.013 (0.015)
Train: 93 [ 750/1251 ( 60%)]  Loss: 3.503 (3.73)  Time: 0.996s, 1028.30/s  (1.010s, 1013.63/s)  LR: 7.832e-04  Data: 0.011 (0.015)
Train: 93 [ 800/1251 ( 64%)]  Loss: 3.923 (3.74)  Time: 1.001s, 1022.89/s  (1.011s, 1012.91/s)  LR: 7.832e-04  Data: 0.011 (0.015)
Train: 93 [ 850/1251 ( 68%)]  Loss: 3.689 (3.74)  Time: 0.995s, 1029.64/s  (1.010s, 1013.68/s)  LR: 7.832e-04  Data: 0.010 (0.015)
Train: 93 [ 900/1251 ( 72%)]  Loss: 3.819 (3.74)  Time: 1.047s,  978.00/s  (1.010s, 1013.56/s)  LR: 7.832e-04  Data: 0.012 (0.014)
Train: 93 [ 950/1251 ( 76%)]  Loss: 3.973 (3.75)  Time: 1.002s, 1022.30/s  (1.010s, 1013.57/s)  LR: 7.832e-04  Data: 0.011 (0.014)
Train: 93 [1000/1251 ( 80%)]  Loss: 3.445 (3.74)  Time: 0.994s, 1030.07/s  (1.010s, 1014.21/s)  LR: 7.832e-04  Data: 0.011 (0.014)
Train: 93 [1050/1251 ( 84%)]  Loss: 3.636 (3.73)  Time: 0.993s, 1031.06/s  (1.009s, 1014.79/s)  LR: 7.832e-04  Data: 0.011 (0.014)
Train: 93 [1100/1251 ( 88%)]  Loss: 3.768 (3.74)  Time: 1.004s, 1019.72/s  (1.009s, 1015.05/s)  LR: 7.832e-04  Data: 0.011 (0.014)
Train: 93 [1150/1251 ( 92%)]  Loss: 3.419 (3.72)  Time: 1.001s, 1022.78/s  (1.010s, 1014.26/s)  LR: 7.832e-04  Data: 0.010 (0.014)
Train: 93 [1200/1251 ( 96%)]  Loss: 3.534 (3.71)  Time: 1.041s,  984.00/s  (1.009s, 1014.42/s)  LR: 7.832e-04  Data: 0.011 (0.014)
Train: 93 [1250/1251 (100%)]  Loss: 4.162 (3.73)  Time: 1.025s,  998.71/s  (1.010s, 1013.72/s)  LR: 7.832e-04  Data: 0.000 (0.014)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.665 (1.665)  Loss:  0.8635 (0.8635)  Acc@1: 87.6953 (87.6953)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.245 (0.570)  Loss:  0.9059 (1.4463)  Acc@1: 82.5472 (71.9560)  Acc@5: 96.4623 (90.7960)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-92.pth.tar', 72.15199997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-88.pth.tar', 72.02400007080078)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-93.pth.tar', 71.95599991699218)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-90.pth.tar', 71.9080001171875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-83.pth.tar', 71.82200015136719)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-89.pth.tar', 71.53800006835938)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-84.pth.tar', 71.42399997314453)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-77.pth.tar', 71.38400007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-81.pth.tar', 71.23199994628906)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-87.pth.tar', 71.22599993896485)

Train: 94 [   0/1251 (  0%)]  Loss: 3.318 (3.32)  Time: 2.462s,  416.00/s  (2.462s,  416.00/s)  LR: 7.789e-04  Data: 1.499 (1.499)
Train: 94 [  50/1251 (  4%)]  Loss: 3.716 (3.52)  Time: 0.999s, 1024.56/s  (1.043s,  982.15/s)  LR: 7.789e-04  Data: 0.011 (0.043)
Train: 94 [ 100/1251 (  8%)]  Loss: 3.918 (3.65)  Time: 0.995s, 1029.56/s  (1.022s, 1001.64/s)  LR: 7.789e-04  Data: 0.011 (0.027)
Train: 94 [ 150/1251 ( 12%)]  Loss: 3.565 (3.63)  Time: 1.021s, 1003.28/s  (1.016s, 1007.92/s)  LR: 7.789e-04  Data: 0.012 (0.022)
Train: 94 [ 200/1251 ( 16%)]  Loss: 3.641 (3.63)  Time: 1.009s, 1014.75/s  (1.013s, 1010.90/s)  LR: 7.789e-04  Data: 0.010 (0.019)
Train: 94 [ 250/1251 ( 20%)]  Loss: 3.603 (3.63)  Time: 0.994s, 1030.13/s  (1.010s, 1013.54/s)  LR: 7.789e-04  Data: 0.011 (0.018)
Train: 94 [ 300/1251 ( 24%)]  Loss: 3.624 (3.63)  Time: 0.996s, 1027.98/s  (1.009s, 1014.65/s)  LR: 7.789e-04  Data: 0.011 (0.017)
Train: 94 [ 350/1251 ( 28%)]  Loss: 3.917 (3.66)  Time: 1.030s,  993.85/s  (1.008s, 1015.51/s)  LR: 7.789e-04  Data: 0.011 (0.016)
Train: 94 [ 400/1251 ( 32%)]  Loss: 3.753 (3.67)  Time: 0.996s, 1027.64/s  (1.007s, 1016.41/s)  LR: 7.789e-04  Data: 0.012 (0.015)
Train: 94 [ 450/1251 ( 36%)]  Loss: 3.748 (3.68)  Time: 0.995s, 1029.62/s  (1.008s, 1016.37/s)  LR: 7.789e-04  Data: 0.011 (0.015)
Train: 94 [ 500/1251 ( 40%)]  Loss: 4.419 (3.75)  Time: 1.029s,  995.25/s  (1.007s, 1016.82/s)  LR: 7.789e-04  Data: 0.011 (0.014)
Train: 94 [ 550/1251 ( 44%)]  Loss: 3.437 (3.72)  Time: 0.994s, 1030.60/s  (1.007s, 1016.77/s)  LR: 7.789e-04  Data: 0.010 (0.014)
Train: 94 [ 600/1251 ( 48%)]  Loss: 3.780 (3.73)  Time: 1.017s, 1006.43/s  (1.007s, 1017.32/s)  LR: 7.789e-04  Data: 0.010 (0.014)
Train: 94 [ 650/1251 ( 52%)]  Loss: 3.655 (3.72)  Time: 0.994s, 1029.86/s  (1.007s, 1017.27/s)  LR: 7.789e-04  Data: 0.011 (0.014)
Train: 94 [ 700/1251 ( 56%)]  Loss: 3.816 (3.73)  Time: 0.996s, 1027.69/s  (1.006s, 1017.77/s)  LR: 7.789e-04  Data: 0.011 (0.013)
Train: 94 [ 750/1251 ( 60%)]  Loss: 3.476 (3.71)  Time: 0.997s, 1027.26/s  (1.006s, 1017.57/s)  LR: 7.789e-04  Data: 0.010 (0.013)
Train: 94 [ 800/1251 ( 64%)]  Loss: 3.603 (3.71)  Time: 1.008s, 1015.40/s  (1.006s, 1017.70/s)  LR: 7.789e-04  Data: 0.011 (0.013)
Train: 94 [ 850/1251 ( 68%)]  Loss: 3.533 (3.70)  Time: 0.994s, 1029.84/s  (1.006s, 1018.19/s)  LR: 7.789e-04  Data: 0.012 (0.013)
Train: 94 [ 900/1251 ( 72%)]  Loss: 3.402 (3.68)  Time: 1.045s,  979.77/s  (1.006s, 1017.56/s)  LR: 7.789e-04  Data: 0.011 (0.013)
Train: 94 [ 950/1251 ( 76%)]  Loss: 4.192 (3.71)  Time: 0.997s, 1026.73/s  (1.006s, 1017.92/s)  LR: 7.789e-04  Data: 0.011 (0.013)
Train: 94 [1000/1251 ( 80%)]  Loss: 3.957 (3.72)  Time: 0.995s, 1028.69/s  (1.006s, 1017.68/s)  LR: 7.789e-04  Data: 0.011 (0.013)
Train: 94 [1050/1251 ( 84%)]  Loss: 4.004 (3.73)  Time: 1.057s,  968.94/s  (1.006s, 1017.41/s)  LR: 7.789e-04  Data: 0.012 (0.013)
Train: 94 [1100/1251 ( 88%)]  Loss: 3.649 (3.73)  Time: 1.000s, 1023.69/s  (1.008s, 1015.84/s)  LR: 7.789e-04  Data: 0.010 (0.013)
Train: 94 [1150/1251 ( 92%)]  Loss: 3.559 (3.72)  Time: 1.005s, 1018.75/s  (1.008s, 1016.09/s)  LR: 7.789e-04  Data: 0.011 (0.012)
Train: 94 [1200/1251 ( 96%)]  Loss: 4.075 (3.73)  Time: 0.993s, 1031.03/s  (1.008s, 1016.28/s)  LR: 7.789e-04  Data: 0.011 (0.012)
Train: 94 [1250/1251 (100%)]  Loss: 3.641 (3.73)  Time: 0.982s, 1042.91/s  (1.007s, 1016.54/s)  LR: 7.789e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.626 (1.626)  Loss:  0.8610 (0.8610)  Acc@1: 86.8164 (86.8164)  Acc@5: 95.6055 (95.6055)
Test: [  48/48]  Time: 0.245 (0.573)  Loss:  0.8785 (1.3630)  Acc@1: 83.1368 (72.1680)  Acc@5: 96.2264 (91.1780)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-94.pth.tar', 72.16800004394531)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-92.pth.tar', 72.15199997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-88.pth.tar', 72.02400007080078)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-93.pth.tar', 71.95599991699218)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-90.pth.tar', 71.9080001171875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-83.pth.tar', 71.82200015136719)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-89.pth.tar', 71.53800006835938)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-84.pth.tar', 71.42399997314453)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-77.pth.tar', 71.38400007324219)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-81.pth.tar', 71.23199994628906)

Train: 95 [   0/1251 (  0%)]  Loss: 3.584 (3.58)  Time: 2.743s,  373.25/s  (2.743s,  373.25/s)  LR: 7.746e-04  Data: 1.783 (1.783)
Train: 95 [  50/1251 (  4%)]  Loss: 3.814 (3.70)  Time: 0.996s, 1027.76/s  (1.039s,  985.75/s)  LR: 7.746e-04  Data: 0.014 (0.048)
Train: 95 [ 100/1251 (  8%)]  Loss: 3.251 (3.55)  Time: 1.045s,  979.58/s  (1.021s, 1002.96/s)  LR: 7.746e-04  Data: 0.011 (0.030)
Train: 95 [ 150/1251 ( 12%)]  Loss: 3.747 (3.60)  Time: 0.993s, 1031.12/s  (1.020s, 1004.33/s)  LR: 7.746e-04  Data: 0.010 (0.024)
Train: 95 [ 200/1251 ( 16%)]  Loss: 4.220 (3.72)  Time: 0.993s, 1030.84/s  (1.016s, 1007.81/s)  LR: 7.746e-04  Data: 0.011 (0.021)
Train: 95 [ 250/1251 ( 20%)]  Loss: 4.101 (3.79)  Time: 1.007s, 1016.66/s  (1.014s, 1010.20/s)  LR: 7.746e-04  Data: 0.011 (0.019)
Train: 95 [ 300/1251 ( 24%)]  Loss: 3.644 (3.77)  Time: 1.056s,  969.64/s  (1.014s, 1010.12/s)  LR: 7.746e-04  Data: 0.011 (0.017)
Train: 95 [ 350/1251 ( 28%)]  Loss: 3.830 (3.77)  Time: 0.994s, 1030.15/s  (1.017s, 1006.73/s)  LR: 7.746e-04  Data: 0.011 (0.017)
Train: 95 [ 400/1251 ( 32%)]  Loss: 3.582 (3.75)  Time: 0.996s, 1028.53/s  (1.016s, 1008.20/s)  LR: 7.746e-04  Data: 0.011 (0.016)
Train: 95 [ 450/1251 ( 36%)]  Loss: 3.911 (3.77)  Time: 0.996s, 1028.05/s  (1.015s, 1009.20/s)  LR: 7.746e-04  Data: 0.012 (0.015)
Train: 95 [ 500/1251 ( 40%)]  Loss: 3.818 (3.77)  Time: 0.998s, 1026.21/s  (1.013s, 1010.85/s)  LR: 7.746e-04  Data: 0.011 (0.015)
Train: 95 [ 550/1251 ( 44%)]  Loss: 3.626 (3.76)  Time: 0.996s, 1028.52/s  (1.013s, 1010.81/s)  LR: 7.746e-04  Data: 0.012 (0.015)
Train: 95 [ 600/1251 ( 48%)]  Loss: 3.796 (3.76)  Time: 1.063s,  962.97/s  (1.014s, 1010.03/s)  LR: 7.746e-04  Data: 0.011 (0.014)
Train: 95 [ 650/1251 ( 52%)]  Loss: 3.111 (3.72)  Time: 0.996s, 1028.46/s  (1.013s, 1011.19/s)  LR: 7.746e-04  Data: 0.011 (0.014)
Train: 95 [ 700/1251 ( 56%)]  Loss: 3.872 (3.73)  Time: 0.996s, 1027.85/s  (1.013s, 1011.13/s)  LR: 7.746e-04  Data: 0.011 (0.014)
Train: 95 [ 750/1251 ( 60%)]  Loss: 3.990 (3.74)  Time: 0.997s, 1027.04/s  (1.012s, 1011.98/s)  LR: 7.746e-04  Data: 0.011 (0.014)
Train: 95 [ 800/1251 ( 64%)]  Loss: 3.859 (3.75)  Time: 0.998s, 1025.74/s  (1.011s, 1012.84/s)  LR: 7.746e-04  Data: 0.012 (0.013)
Train: 95 [ 850/1251 ( 68%)]  Loss: 3.840 (3.76)  Time: 0.995s, 1028.80/s  (1.010s, 1013.62/s)  LR: 7.746e-04  Data: 0.010 (0.013)
Train: 95 [ 900/1251 ( 72%)]  Loss: 3.811 (3.76)  Time: 0.992s, 1031.92/s  (1.011s, 1012.98/s)  LR: 7.746e-04  Data: 0.010 (0.013)
Train: 95 [ 950/1251 ( 76%)]  Loss: 3.751 (3.76)  Time: 1.001s, 1023.05/s  (1.011s, 1013.05/s)  LR: 7.746e-04  Data: 0.011 (0.013)
Train: 95 [1000/1251 ( 80%)]  Loss: 3.847 (3.76)  Time: 0.994s, 1030.34/s  (1.010s, 1013.67/s)  LR: 7.746e-04  Data: 0.010 (0.013)
Train: 95 [1050/1251 ( 84%)]  Loss: 3.599 (3.75)  Time: 1.004s, 1019.91/s  (1.010s, 1014.00/s)  LR: 7.746e-04  Data: 0.011 (0.013)
Train: 95 [1100/1251 ( 88%)]  Loss: 3.560 (3.75)  Time: 0.997s, 1027.13/s  (1.009s, 1014.47/s)  LR: 7.746e-04  Data: 0.010 (0.013)
Train: 95 [1150/1251 ( 92%)]  Loss: 3.784 (3.75)  Time: 0.993s, 1030.87/s  (1.010s, 1013.69/s)  LR: 7.746e-04  Data: 0.010 (0.013)
Train: 95 [1200/1251 ( 96%)]  Loss: 3.169 (3.72)  Time: 0.996s, 1028.10/s  (1.010s, 1013.98/s)  LR: 7.746e-04  Data: 0.012 (0.013)
Train: 95 [1250/1251 (100%)]  Loss: 3.744 (3.73)  Time: 0.982s, 1042.63/s  (1.010s, 1014.20/s)  LR: 7.746e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.631 (1.631)  Loss:  0.7469 (0.7469)  Acc@1: 90.2344 (90.2344)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.245 (0.571)  Loss:  0.7979 (1.3788)  Acc@1: 84.5519 (72.1360)  Acc@5: 96.2264 (91.1240)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-94.pth.tar', 72.16800004394531)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-92.pth.tar', 72.15199997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-95.pth.tar', 72.13600001220703)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-88.pth.tar', 72.02400007080078)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-93.pth.tar', 71.95599991699218)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-90.pth.tar', 71.9080001171875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-83.pth.tar', 71.82200015136719)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-89.pth.tar', 71.53800006835938)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-84.pth.tar', 71.42399997314453)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-77.pth.tar', 71.38400007324219)

Train: 96 [   0/1251 (  0%)]  Loss: 3.693 (3.69)  Time: 2.496s,  410.18/s  (2.496s,  410.18/s)  LR: 7.702e-04  Data: 1.547 (1.547)
Train: 96 [  50/1251 (  4%)]  Loss: 4.096 (3.89)  Time: 1.007s, 1016.96/s  (1.053s,  972.32/s)  LR: 7.702e-04  Data: 0.011 (0.044)
Train: 96 [ 100/1251 (  8%)]  Loss: 3.723 (3.84)  Time: 0.998s, 1026.06/s  (1.027s,  996.88/s)  LR: 7.702e-04  Data: 0.011 (0.028)
Train: 96 [ 150/1251 ( 12%)]  Loss: 4.194 (3.93)  Time: 0.995s, 1028.87/s  (1.019s, 1004.65/s)  LR: 7.702e-04  Data: 0.011 (0.022)
Train: 96 [ 200/1251 ( 16%)]  Loss: 3.537 (3.85)  Time: 1.032s,  992.04/s  (1.018s, 1005.41/s)  LR: 7.702e-04  Data: 0.011 (0.019)
Train: 96 [ 250/1251 ( 20%)]  Loss: 3.339 (3.76)  Time: 1.002s, 1021.75/s  (1.018s, 1005.60/s)  LR: 7.702e-04  Data: 0.012 (0.018)
Train: 96 [ 300/1251 ( 24%)]  Loss: 3.869 (3.78)  Time: 0.995s, 1028.99/s  (1.016s, 1007.76/s)  LR: 7.702e-04  Data: 0.012 (0.017)
Train: 96 [ 350/1251 ( 28%)]  Loss: 3.648 (3.76)  Time: 0.996s, 1027.71/s  (1.014s, 1010.13/s)  LR: 7.702e-04  Data: 0.011 (0.016)
Train: 96 [ 400/1251 ( 32%)]  Loss: 3.627 (3.75)  Time: 1.037s,  987.54/s  (1.012s, 1011.85/s)  LR: 7.702e-04  Data: 0.011 (0.015)
Train: 96 [ 450/1251 ( 36%)]  Loss: 3.965 (3.77)  Time: 0.997s, 1026.63/s  (1.012s, 1011.92/s)  LR: 7.702e-04  Data: 0.011 (0.015)
Train: 96 [ 500/1251 ( 40%)]  Loss: 3.665 (3.76)  Time: 1.004s, 1020.39/s  (1.011s, 1013.17/s)  LR: 7.702e-04  Data: 0.011 (0.015)
Train: 96 [ 550/1251 ( 44%)]  Loss: 3.465 (3.73)  Time: 0.993s, 1031.14/s  (1.010s, 1014.26/s)  LR: 7.702e-04  Data: 0.011 (0.014)
Train: 96 [ 600/1251 ( 48%)]  Loss: 3.760 (3.74)  Time: 0.993s, 1030.76/s  (1.009s, 1014.89/s)  LR: 7.702e-04  Data: 0.011 (0.014)
Train: 96 [ 650/1251 ( 52%)]  Loss: 3.395 (3.71)  Time: 0.994s, 1029.82/s  (1.009s, 1015.10/s)  LR: 7.702e-04  Data: 0.011 (0.014)
Train: 96 [ 700/1251 ( 56%)]  Loss: 3.791 (3.72)  Time: 1.003s, 1020.77/s  (1.008s, 1015.62/s)  LR: 7.702e-04  Data: 0.011 (0.014)
Train: 96 [ 750/1251 ( 60%)]  Loss: 3.425 (3.70)  Time: 1.000s, 1023.68/s  (1.008s, 1015.77/s)  LR: 7.702e-04  Data: 0.012 (0.013)
Train: 96 [ 800/1251 ( 64%)]  Loss: 3.755 (3.70)  Time: 1.024s, 1000.13/s  (1.008s, 1016.21/s)  LR: 7.702e-04  Data: 0.011 (0.013)
Train: 96 [ 850/1251 ( 68%)]  Loss: 3.545 (3.69)  Time: 0.996s, 1028.29/s  (1.007s, 1016.82/s)  LR: 7.702e-04  Data: 0.012 (0.013)
Train: 96 [ 900/1251 ( 72%)]  Loss: 3.537 (3.69)  Time: 1.000s, 1024.19/s  (1.008s, 1015.94/s)  LR: 7.702e-04  Data: 0.010 (0.013)
Train: 96 [ 950/1251 ( 76%)]  Loss: 3.519 (3.68)  Time: 1.003s, 1021.41/s  (1.008s, 1015.99/s)  LR: 7.702e-04  Data: 0.011 (0.013)
Train: 96 [1000/1251 ( 80%)]  Loss: 3.885 (3.69)  Time: 1.064s,  962.50/s  (1.008s, 1016.36/s)  LR: 7.702e-04  Data: 0.012 (0.013)
Train: 96 [1050/1251 ( 84%)]  Loss: 3.497 (3.68)  Time: 0.991s, 1032.86/s  (1.007s, 1016.70/s)  LR: 7.702e-04  Data: 0.010 (0.013)
Train: 96 [1100/1251 ( 88%)]  Loss: 3.549 (3.67)  Time: 0.993s, 1031.40/s  (1.007s, 1016.93/s)  LR: 7.702e-04  Data: 0.010 (0.013)
Train: 96 [1150/1251 ( 92%)]  Loss: 3.682 (3.67)  Time: 0.998s, 1026.12/s  (1.007s, 1016.66/s)  LR: 7.702e-04  Data: 0.012 (0.013)
Train: 96 [1200/1251 ( 96%)]  Loss: 3.891 (3.68)  Time: 0.996s, 1027.65/s  (1.007s, 1016.81/s)  LR: 7.702e-04  Data: 0.011 (0.013)
Train: 96 [1250/1251 (100%)]  Loss: 3.326 (3.67)  Time: 0.985s, 1040.01/s  (1.007s, 1017.06/s)  LR: 7.702e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.695 (1.695)  Loss:  0.7588 (0.7588)  Acc@1: 88.6719 (88.6719)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.245 (0.577)  Loss:  0.9650 (1.4401)  Acc@1: 83.7264 (72.0860)  Acc@5: 94.5755 (91.0780)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-94.pth.tar', 72.16800004394531)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-92.pth.tar', 72.15199997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-95.pth.tar', 72.13600001220703)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-96.pth.tar', 72.08600017089844)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-88.pth.tar', 72.02400007080078)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-93.pth.tar', 71.95599991699218)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-90.pth.tar', 71.9080001171875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-83.pth.tar', 71.82200015136719)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-89.pth.tar', 71.53800006835938)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-84.pth.tar', 71.42399997314453)

Train: 97 [   0/1251 (  0%)]  Loss: 3.905 (3.91)  Time: 2.613s,  391.84/s  (2.613s,  391.84/s)  LR: 7.658e-04  Data: 1.634 (1.634)
Train: 97 [  50/1251 (  4%)]  Loss: 3.824 (3.86)  Time: 0.996s, 1028.27/s  (1.051s,  974.46/s)  LR: 7.658e-04  Data: 0.011 (0.043)
Train: 97 [ 100/1251 (  8%)]  Loss: 3.413 (3.71)  Time: 1.054s,  971.68/s  (1.033s,  991.64/s)  LR: 7.658e-04  Data: 0.011 (0.027)
Train: 97 [ 150/1251 ( 12%)]  Loss: 3.911 (3.76)  Time: 1.008s, 1015.88/s  (1.025s,  999.06/s)  LR: 7.658e-04  Data: 0.010 (0.022)
Train: 97 [ 200/1251 ( 16%)]  Loss: 3.799 (3.77)  Time: 0.999s, 1025.27/s  (1.019s, 1004.94/s)  LR: 7.658e-04  Data: 0.011 (0.019)
Train: 97 [ 250/1251 ( 20%)]  Loss: 3.637 (3.75)  Time: 0.999s, 1025.54/s  (1.015s, 1008.53/s)  LR: 7.658e-04  Data: 0.011 (0.018)
Train: 97 [ 300/1251 ( 24%)]  Loss: 4.086 (3.80)  Time: 0.991s, 1033.26/s  (1.012s, 1011.48/s)  LR: 7.658e-04  Data: 0.011 (0.017)
Train: 97 [ 350/1251 ( 28%)]  Loss: 3.594 (3.77)  Time: 1.008s, 1015.57/s  (1.012s, 1011.53/s)  LR: 7.658e-04  Data: 0.011 (0.016)
Train: 97 [ 400/1251 ( 32%)]  Loss: 3.656 (3.76)  Time: 0.996s, 1027.84/s  (1.012s, 1011.47/s)  LR: 7.658e-04  Data: 0.011 (0.015)
Train: 97 [ 450/1251 ( 36%)]  Loss: 3.468 (3.73)  Time: 1.002s, 1022.29/s  (1.012s, 1012.35/s)  LR: 7.658e-04  Data: 0.010 (0.015)
Train: 97 [ 500/1251 ( 40%)]  Loss: 3.597 (3.72)  Time: 0.995s, 1029.17/s  (1.011s, 1013.01/s)  LR: 7.658e-04  Data: 0.011 (0.014)
Train: 97 [ 550/1251 ( 44%)]  Loss: 3.525 (3.70)  Time: 0.996s, 1027.91/s  (1.010s, 1013.40/s)  LR: 7.658e-04  Data: 0.012 (0.014)
Train: 97 [ 600/1251 ( 48%)]  Loss: 3.697 (3.70)  Time: 0.997s, 1027.19/s  (1.010s, 1013.69/s)  LR: 7.658e-04  Data: 0.011 (0.014)
Train: 97 [ 650/1251 ( 52%)]  Loss: 3.735 (3.70)  Time: 1.035s,  989.45/s  (1.009s, 1014.47/s)  LR: 7.658e-04  Data: 0.011 (0.014)
Train: 97 [ 700/1251 ( 56%)]  Loss: 3.948 (3.72)  Time: 0.994s, 1030.41/s  (1.010s, 1014.33/s)  LR: 7.658e-04  Data: 0.012 (0.013)
Train: 97 [ 750/1251 ( 60%)]  Loss: 3.921 (3.73)  Time: 0.995s, 1028.95/s  (1.009s, 1014.81/s)  LR: 7.658e-04  Data: 0.010 (0.013)
Train: 97 [ 800/1251 ( 64%)]  Loss: 3.486 (3.72)  Time: 0.997s, 1026.72/s  (1.010s, 1013.78/s)  LR: 7.658e-04  Data: 0.011 (0.013)
Train: 97 [ 850/1251 ( 68%)]  Loss: 3.801 (3.72)  Time: 1.001s, 1023.20/s  (1.009s, 1014.53/s)  LR: 7.658e-04  Data: 0.011 (0.013)
Train: 97 [ 900/1251 ( 72%)]  Loss: 3.219 (3.70)  Time: 1.036s,  988.24/s  (1.011s, 1012.64/s)  LR: 7.658e-04  Data: 0.011 (0.013)
Train: 97 [ 950/1251 ( 76%)]  Loss: 3.710 (3.70)  Time: 0.995s, 1028.64/s  (1.011s, 1012.97/s)  LR: 7.658e-04  Data: 0.010 (0.013)
Train: 97 [1000/1251 ( 80%)]  Loss: 4.008 (3.71)  Time: 0.996s, 1028.05/s  (1.010s, 1013.57/s)  LR: 7.658e-04  Data: 0.011 (0.013)
Train: 97 [1050/1251 ( 84%)]  Loss: 3.811 (3.72)  Time: 0.998s, 1025.71/s  (1.010s, 1013.87/s)  LR: 7.658e-04  Data: 0.010 (0.013)
Train: 97 [1100/1251 ( 88%)]  Loss: 3.489 (3.71)  Time: 1.014s, 1009.87/s  (1.009s, 1014.37/s)  LR: 7.658e-04  Data: 0.012 (0.013)
Train: 97 [1150/1251 ( 92%)]  Loss: 3.743 (3.71)  Time: 0.997s, 1026.86/s  (1.009s, 1014.48/s)  LR: 7.658e-04  Data: 0.011 (0.013)
Train: 97 [1200/1251 ( 96%)]  Loss: 3.879 (3.71)  Time: 1.002s, 1022.46/s  (1.009s, 1015.02/s)  LR: 7.658e-04  Data: 0.011 (0.012)
Train: 97 [1250/1251 (100%)]  Loss: 4.063 (3.73)  Time: 0.985s, 1039.57/s  (1.008s, 1015.38/s)  LR: 7.658e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.638 (1.638)  Loss:  0.8920 (0.8920)  Acc@1: 88.2812 (88.2812)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.245 (0.564)  Loss:  1.1378 (1.4521)  Acc@1: 81.8396 (72.4740)  Acc@5: 94.9293 (91.4600)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-97.pth.tar', 72.47400012695313)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-94.pth.tar', 72.16800004394531)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-92.pth.tar', 72.15199997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-95.pth.tar', 72.13600001220703)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-96.pth.tar', 72.08600017089844)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-88.pth.tar', 72.02400007080078)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-93.pth.tar', 71.95599991699218)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-90.pth.tar', 71.9080001171875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-83.pth.tar', 71.82200015136719)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-89.pth.tar', 71.53800006835938)

Train: 98 [   0/1251 (  0%)]  Loss: 3.758 (3.76)  Time: 2.460s,  416.23/s  (2.460s,  416.23/s)  LR: 7.614e-04  Data: 1.486 (1.486)
Train: 98 [  50/1251 (  4%)]  Loss: 3.816 (3.79)  Time: 1.005s, 1019.07/s  (1.034s,  990.51/s)  LR: 7.614e-04  Data: 0.011 (0.040)
Train: 98 [ 100/1251 (  8%)]  Loss: 3.829 (3.80)  Time: 0.998s, 1026.10/s  (1.020s, 1004.16/s)  LR: 7.614e-04  Data: 0.010 (0.026)
Train: 98 [ 150/1251 ( 12%)]  Loss: 3.447 (3.71)  Time: 0.994s, 1029.88/s  (1.021s, 1002.97/s)  LR: 7.614e-04  Data: 0.011 (0.021)
Train: 98 [ 200/1251 ( 16%)]  Loss: 4.302 (3.83)  Time: 0.996s, 1028.01/s  (1.018s, 1005.49/s)  LR: 7.614e-04  Data: 0.012 (0.018)
Train: 98 [ 250/1251 ( 20%)]  Loss: 3.421 (3.76)  Time: 0.993s, 1031.11/s  (1.015s, 1008.85/s)  LR: 7.614e-04  Data: 0.010 (0.017)
Train: 98 [ 300/1251 ( 24%)]  Loss: 3.721 (3.76)  Time: 0.997s, 1026.94/s  (1.013s, 1011.26/s)  LR: 7.614e-04  Data: 0.011 (0.016)
Train: 98 [ 350/1251 ( 28%)]  Loss: 3.527 (3.73)  Time: 0.995s, 1028.73/s  (1.010s, 1013.50/s)  LR: 7.614e-04  Data: 0.012 (0.015)
Train: 98 [ 400/1251 ( 32%)]  Loss: 3.712 (3.73)  Time: 1.033s,  991.15/s  (1.010s, 1013.39/s)  LR: 7.614e-04  Data: 0.010 (0.015)
Train: 98 [ 450/1251 ( 36%)]  Loss: 3.955 (3.75)  Time: 1.061s,  964.85/s  (1.011s, 1012.58/s)  LR: 7.614e-04  Data: 0.011 (0.014)
Train: 98 [ 500/1251 ( 40%)]  Loss: 3.494 (3.73)  Time: 0.996s, 1028.01/s  (1.011s, 1012.61/s)  LR: 7.614e-04  Data: 0.011 (0.014)
Train: 98 [ 550/1251 ( 44%)]  Loss: 3.831 (3.73)  Time: 0.993s, 1030.85/s  (1.010s, 1013.39/s)  LR: 7.614e-04  Data: 0.010 (0.014)
Train: 98 [ 600/1251 ( 48%)]  Loss: 4.130 (3.76)  Time: 1.049s,  976.56/s  (1.010s, 1013.93/s)  LR: 7.614e-04  Data: 0.010 (0.014)
Train: 98 [ 650/1251 ( 52%)]  Loss: 3.760 (3.76)  Time: 0.996s, 1027.90/s  (1.009s, 1014.97/s)  LR: 7.614e-04  Data: 0.011 (0.013)
Train: 98 [ 700/1251 ( 56%)]  Loss: 3.765 (3.76)  Time: 0.998s, 1026.45/s  (1.009s, 1015.22/s)  LR: 7.614e-04  Data: 0.011 (0.013)
Train: 98 [ 750/1251 ( 60%)]  Loss: 3.817 (3.77)  Time: 0.998s, 1025.73/s  (1.008s, 1015.39/s)  LR: 7.614e-04  Data: 0.011 (0.013)
Train: 98 [ 800/1251 ( 64%)]  Loss: 3.798 (3.77)  Time: 0.996s, 1028.20/s  (1.010s, 1014.06/s)  LR: 7.614e-04  Data: 0.012 (0.013)
Train: 98 [ 850/1251 ( 68%)]  Loss: 3.387 (3.75)  Time: 0.998s, 1025.67/s  (1.010s, 1014.34/s)  LR: 7.614e-04  Data: 0.011 (0.013)
Train: 98 [ 900/1251 ( 72%)]  Loss: 3.606 (3.74)  Time: 0.998s, 1026.10/s  (1.010s, 1013.37/s)  LR: 7.614e-04  Data: 0.011 (0.013)
Train: 98 [ 950/1251 ( 76%)]  Loss: 3.647 (3.74)  Time: 0.996s, 1027.67/s  (1.010s, 1013.67/s)  LR: 7.614e-04  Data: 0.012 (0.013)
Train: 98 [1000/1251 ( 80%)]  Loss: 3.485 (3.72)  Time: 1.067s,  960.15/s  (1.011s, 1013.22/s)  LR: 7.614e-04  Data: 0.011 (0.013)
Train: 98 [1050/1251 ( 84%)]  Loss: 3.776 (3.73)  Time: 1.032s,  991.90/s  (1.011s, 1013.09/s)  LR: 7.614e-04  Data: 0.010 (0.012)
Train: 98 [1100/1251 ( 88%)]  Loss: 3.708 (3.73)  Time: 0.999s, 1024.60/s  (1.010s, 1013.51/s)  LR: 7.614e-04  Data: 0.011 (0.012)
Train: 98 [1150/1251 ( 92%)]  Loss: 4.143 (3.74)  Time: 0.997s, 1027.29/s  (1.010s, 1013.98/s)  LR: 7.614e-04  Data: 0.012 (0.012)
Train: 98 [1200/1251 ( 96%)]  Loss: 3.805 (3.75)  Time: 0.996s, 1028.43/s  (1.010s, 1014.14/s)  LR: 7.614e-04  Data: 0.012 (0.012)
Train: 98 [1250/1251 (100%)]  Loss: 3.926 (3.75)  Time: 0.983s, 1041.32/s  (1.010s, 1013.79/s)  LR: 7.614e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.601 (1.601)  Loss:  0.9458 (0.9458)  Acc@1: 88.4766 (88.4766)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.245 (0.569)  Loss:  1.0048 (1.4948)  Acc@1: 81.9576 (71.6620)  Acc@5: 95.6368 (90.8840)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-97.pth.tar', 72.47400012695313)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-94.pth.tar', 72.16800004394531)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-92.pth.tar', 72.15199997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-95.pth.tar', 72.13600001220703)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-96.pth.tar', 72.08600017089844)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-88.pth.tar', 72.02400007080078)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-93.pth.tar', 71.95599991699218)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-90.pth.tar', 71.9080001171875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-83.pth.tar', 71.82200015136719)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-98.pth.tar', 71.66200017822266)

Train: 99 [   0/1251 (  0%)]  Loss: 3.959 (3.96)  Time: 2.478s,  413.29/s  (2.478s,  413.29/s)  LR: 7.570e-04  Data: 1.516 (1.516)
Train: 99 [  50/1251 (  4%)]  Loss: 3.726 (3.84)  Time: 0.997s, 1027.14/s  (1.032s,  992.47/s)  LR: 7.570e-04  Data: 0.011 (0.041)
Train: 99 [ 100/1251 (  8%)]  Loss: 3.948 (3.88)  Time: 1.008s, 1015.95/s  (1.017s, 1007.16/s)  LR: 7.570e-04  Data: 0.011 (0.026)
Train: 99 [ 150/1251 ( 12%)]  Loss: 3.895 (3.88)  Time: 1.063s,  963.71/s  (1.012s, 1011.66/s)  LR: 7.570e-04  Data: 0.012 (0.021)
Train: 99 [ 200/1251 ( 16%)]  Loss: 3.579 (3.82)  Time: 0.995s, 1029.16/s  (1.009s, 1014.38/s)  LR: 7.570e-04  Data: 0.011 (0.019)
Train: 99 [ 250/1251 ( 20%)]  Loss: 3.403 (3.75)  Time: 1.033s,  990.85/s  (1.011s, 1012.74/s)  LR: 7.570e-04  Data: 0.011 (0.017)
Train: 99 [ 300/1251 ( 24%)]  Loss: 3.719 (3.75)  Time: 0.996s, 1027.73/s  (1.011s, 1013.07/s)  LR: 7.570e-04  Data: 0.012 (0.016)
Train: 99 [ 350/1251 ( 28%)]  Loss: 3.878 (3.76)  Time: 1.011s, 1012.95/s  (1.010s, 1014.29/s)  LR: 7.570e-04  Data: 0.011 (0.015)
Train: 99 [ 400/1251 ( 32%)]  Loss: 3.334 (3.72)  Time: 0.994s, 1029.84/s  (1.008s, 1015.41/s)  LR: 7.570e-04  Data: 0.011 (0.015)
Train: 99 [ 450/1251 ( 36%)]  Loss: 4.091 (3.75)  Time: 0.996s, 1028.49/s  (1.007s, 1016.41/s)  LR: 7.570e-04  Data: 0.012 (0.014)
Train: 99 [ 500/1251 ( 40%)]  Loss: 3.869 (3.76)  Time: 1.004s, 1019.69/s  (1.007s, 1016.68/s)  LR: 7.570e-04  Data: 0.011 (0.014)
Train: 99 [ 550/1251 ( 44%)]  Loss: 3.824 (3.77)  Time: 0.994s, 1030.18/s  (1.007s, 1017.22/s)  LR: 7.570e-04  Data: 0.011 (0.014)
Train: 99 [ 600/1251 ( 48%)]  Loss: 3.686 (3.76)  Time: 0.992s, 1032.38/s  (1.006s, 1017.49/s)  LR: 7.570e-04  Data: 0.010 (0.014)
Train: 99 [ 650/1251 ( 52%)]  Loss: 3.759 (3.76)  Time: 0.997s, 1026.68/s  (1.007s, 1017.23/s)  LR: 7.570e-04  Data: 0.012 (0.013)
Train: 99 [ 700/1251 ( 56%)]  Loss: 3.820 (3.77)  Time: 1.057s,  969.22/s  (1.006s, 1017.63/s)  LR: 7.570e-04  Data: 0.011 (0.013)
Train: 99 [ 750/1251 ( 60%)]  Loss: 4.082 (3.79)  Time: 0.999s, 1025.00/s  (1.009s, 1015.07/s)  LR: 7.570e-04  Data: 0.012 (0.013)
Train: 99 [ 800/1251 ( 64%)]  Loss: 3.678 (3.78)  Time: 0.999s, 1025.26/s  (1.008s, 1015.38/s)  LR: 7.570e-04  Data: 0.011 (0.013)
Train: 99 [ 850/1251 ( 68%)]  Loss: 3.927 (3.79)  Time: 0.997s, 1027.48/s  (1.008s, 1015.79/s)  LR: 7.570e-04  Data: 0.010 (0.013)
Train: 99 [ 900/1251 ( 72%)]  Loss: 3.526 (3.77)  Time: 0.997s, 1026.60/s  (1.008s, 1016.09/s)  LR: 7.570e-04  Data: 0.010 (0.013)
Train: 99 [ 950/1251 ( 76%)]  Loss: 3.401 (3.76)  Time: 0.996s, 1028.32/s  (1.008s, 1015.77/s)  LR: 7.570e-04  Data: 0.011 (0.013)
Train: 99 [1000/1251 ( 80%)]  Loss: 3.589 (3.75)  Time: 0.995s, 1029.66/s  (1.008s, 1015.58/s)  LR: 7.570e-04  Data: 0.011 (0.013)
Train: 99 [1050/1251 ( 84%)]  Loss: 3.724 (3.75)  Time: 0.996s, 1028.45/s  (1.008s, 1015.99/s)  LR: 7.570e-04  Data: 0.012 (0.013)
Train: 99 [1100/1251 ( 88%)]  Loss: 4.142 (3.76)  Time: 0.993s, 1031.67/s  (1.008s, 1016.11/s)  LR: 7.570e-04  Data: 0.011 (0.012)
Train: 99 [1150/1251 ( 92%)]  Loss: 3.688 (3.76)  Time: 1.001s, 1023.43/s  (1.008s, 1015.55/s)  LR: 7.570e-04  Data: 0.011 (0.012)
Train: 99 [1200/1251 ( 96%)]  Loss: 3.992 (3.77)  Time: 0.993s, 1030.73/s  (1.008s, 1015.88/s)  LR: 7.570e-04  Data: 0.011 (0.012)
Train: 99 [1250/1251 (100%)]  Loss: 3.964 (3.78)  Time: 1.044s,  981.26/s  (1.008s, 1015.69/s)  LR: 7.570e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.630 (1.630)  Loss:  0.9035 (0.9035)  Acc@1: 88.9648 (88.9648)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.245 (0.576)  Loss:  1.0297 (1.4927)  Acc@1: 83.2547 (71.9120)  Acc@5: 95.7547 (90.9640)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-97.pth.tar', 72.47400012695313)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-94.pth.tar', 72.16800004394531)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-92.pth.tar', 72.15199997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-95.pth.tar', 72.13600001220703)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-96.pth.tar', 72.08600017089844)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-88.pth.tar', 72.02400007080078)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-93.pth.tar', 71.95599991699218)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-99.pth.tar', 71.91200009521485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-90.pth.tar', 71.9080001171875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-83.pth.tar', 71.82200015136719)

Train: 100 [   0/1251 (  0%)]  Loss: 3.743 (3.74)  Time: 2.632s,  389.12/s  (2.632s,  389.12/s)  LR: 7.525e-04  Data: 1.655 (1.655)
Train: 100 [  50/1251 (  4%)]  Loss: 3.623 (3.68)  Time: 0.997s, 1027.43/s  (1.030s,  993.98/s)  LR: 7.525e-04  Data: 0.012 (0.043)
Train: 100 [ 100/1251 (  8%)]  Loss: 4.137 (3.83)  Time: 0.994s, 1029.73/s  (1.015s, 1008.72/s)  LR: 7.525e-04  Data: 0.011 (0.028)
Train: 100 [ 150/1251 ( 12%)]  Loss: 3.255 (3.69)  Time: 0.998s, 1026.26/s  (1.010s, 1013.62/s)  LR: 7.525e-04  Data: 0.011 (0.022)
Train: 100 [ 200/1251 ( 16%)]  Loss: 3.575 (3.67)  Time: 1.029s,  995.49/s  (1.012s, 1012.21/s)  LR: 7.525e-04  Data: 0.011 (0.020)
Train: 100 [ 250/1251 ( 20%)]  Loss: 3.826 (3.69)  Time: 0.997s, 1027.23/s  (1.012s, 1011.88/s)  LR: 7.525e-04  Data: 0.011 (0.018)
Train: 100 [ 300/1251 ( 24%)]  Loss: 3.843 (3.71)  Time: 0.996s, 1027.70/s  (1.010s, 1014.12/s)  LR: 7.525e-04  Data: 0.012 (0.017)
Train: 100 [ 350/1251 ( 28%)]  Loss: 3.874 (3.73)  Time: 1.003s, 1020.88/s  (1.010s, 1013.99/s)  LR: 7.525e-04  Data: 0.012 (0.016)
Train: 100 [ 400/1251 ( 32%)]  Loss: 3.529 (3.71)  Time: 0.993s, 1031.44/s  (1.010s, 1013.97/s)  LR: 7.525e-04  Data: 0.012 (0.015)
Train: 100 [ 450/1251 ( 36%)]  Loss: 3.780 (3.72)  Time: 0.996s, 1028.16/s  (1.010s, 1013.90/s)  LR: 7.525e-04  Data: 0.012 (0.015)
Train: 100 [ 500/1251 ( 40%)]  Loss: 3.495 (3.70)  Time: 1.046s,  979.40/s  (1.009s, 1014.60/s)  LR: 7.525e-04  Data: 0.012 (0.015)
Train: 100 [ 550/1251 ( 44%)]  Loss: 3.828 (3.71)  Time: 0.995s, 1028.97/s  (1.009s, 1014.61/s)  LR: 7.525e-04  Data: 0.012 (0.014)
Train: 100 [ 600/1251 ( 48%)]  Loss: 3.833 (3.72)  Time: 0.997s, 1027.06/s  (1.009s, 1015.26/s)  LR: 7.525e-04  Data: 0.012 (0.014)
Train: 100 [ 650/1251 ( 52%)]  Loss: 3.909 (3.73)  Time: 1.032s,  992.61/s  (1.009s, 1014.95/s)  LR: 7.525e-04  Data: 0.011 (0.014)
Train: 100 [ 700/1251 ( 56%)]  Loss: 3.438 (3.71)  Time: 0.993s, 1031.44/s  (1.010s, 1013.82/s)  LR: 7.525e-04  Data: 0.011 (0.014)
Train: 100 [ 750/1251 ( 60%)]  Loss: 3.750 (3.71)  Time: 1.006s, 1018.32/s  (1.009s, 1014.46/s)  LR: 7.525e-04  Data: 0.012 (0.013)
Train: 100 [ 800/1251 ( 64%)]  Loss: 3.632 (3.71)  Time: 0.996s, 1027.91/s  (1.009s, 1014.99/s)  LR: 7.525e-04  Data: 0.011 (0.013)
Train: 100 [ 850/1251 ( 68%)]  Loss: 3.516 (3.70)  Time: 0.995s, 1029.56/s  (1.009s, 1015.00/s)  LR: 7.525e-04  Data: 0.011 (0.013)
Train: 100 [ 900/1251 ( 72%)]  Loss: 3.541 (3.69)  Time: 1.003s, 1020.64/s  (1.009s, 1015.32/s)  LR: 7.525e-04  Data: 0.012 (0.013)
Train: 100 [ 950/1251 ( 76%)]  Loss: 4.153 (3.71)  Time: 1.001s, 1023.01/s  (1.008s, 1015.57/s)  LR: 7.525e-04  Data: 0.010 (0.013)
Train: 100 [1000/1251 ( 80%)]  Loss: 3.623 (3.71)  Time: 1.004s, 1020.23/s  (1.008s, 1015.91/s)  LR: 7.525e-04  Data: 0.012 (0.013)
Train: 100 [1050/1251 ( 84%)]  Loss: 3.674 (3.71)  Time: 1.001s, 1022.88/s  (1.008s, 1016.32/s)  LR: 7.525e-04  Data: 0.011 (0.013)
Train: 100 [1100/1251 ( 88%)]  Loss: 3.549 (3.70)  Time: 0.996s, 1028.28/s  (1.007s, 1016.71/s)  LR: 7.525e-04  Data: 0.011 (0.013)
Train: 100 [1150/1251 ( 92%)]  Loss: 3.803 (3.71)  Time: 0.995s, 1029.50/s  (1.007s, 1016.48/s)  LR: 7.525e-04  Data: 0.012 (0.013)
Train: 100 [1200/1251 ( 96%)]  Loss: 3.763 (3.71)  Time: 0.996s, 1028.00/s  (1.007s, 1016.56/s)  LR: 7.525e-04  Data: 0.012 (0.013)
Train: 100 [1250/1251 (100%)]  Loss: 3.615 (3.70)  Time: 0.984s, 1040.64/s  (1.007s, 1016.47/s)  LR: 7.525e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.626 (1.626)  Loss:  0.9279 (0.9279)  Acc@1: 87.4023 (87.4023)  Acc@5: 96.1914 (96.1914)
Test: [  48/48]  Time: 0.245 (0.579)  Loss:  1.0390 (1.4582)  Acc@1: 82.6651 (71.9140)  Acc@5: 95.1651 (90.8200)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-97.pth.tar', 72.47400012695313)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-94.pth.tar', 72.16800004394531)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-92.pth.tar', 72.15199997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-95.pth.tar', 72.13600001220703)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-96.pth.tar', 72.08600017089844)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-88.pth.tar', 72.02400007080078)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-93.pth.tar', 71.95599991699218)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-100.pth.tar', 71.91399996826172)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-99.pth.tar', 71.91200009521485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-90.pth.tar', 71.9080001171875)

Train: 101 [   0/1251 (  0%)]  Loss: 3.542 (3.54)  Time: 2.572s,  398.15/s  (2.572s,  398.15/s)  LR: 7.480e-04  Data: 1.560 (1.560)
Train: 101 [  50/1251 (  4%)]  Loss: 3.930 (3.74)  Time: 1.047s,  978.28/s  (1.031s,  992.97/s)  LR: 7.480e-04  Data: 0.011 (0.042)
Train: 101 [ 100/1251 (  8%)]  Loss: 3.521 (3.66)  Time: 0.995s, 1029.48/s  (1.023s, 1001.17/s)  LR: 7.480e-04  Data: 0.011 (0.027)
Train: 101 [ 150/1251 ( 12%)]  Loss: 3.749 (3.69)  Time: 0.995s, 1029.13/s  (1.015s, 1008.58/s)  LR: 7.480e-04  Data: 0.012 (0.022)
Train: 101 [ 200/1251 ( 16%)]  Loss: 3.385 (3.63)  Time: 0.992s, 1032.05/s  (1.014s, 1010.26/s)  LR: 7.480e-04  Data: 0.011 (0.019)
Train: 101 [ 250/1251 ( 20%)]  Loss: 3.682 (3.63)  Time: 0.998s, 1026.43/s  (1.011s, 1012.57/s)  LR: 7.480e-04  Data: 0.010 (0.017)
Train: 101 [ 300/1251 ( 24%)]  Loss: 3.606 (3.63)  Time: 0.996s, 1027.88/s  (1.010s, 1013.93/s)  LR: 7.480e-04  Data: 0.010 (0.016)
Train: 101 [ 350/1251 ( 28%)]  Loss: 3.820 (3.65)  Time: 0.992s, 1031.93/s  (1.008s, 1015.67/s)  LR: 7.480e-04  Data: 0.011 (0.015)
Train: 101 [ 400/1251 ( 32%)]  Loss: 3.568 (3.64)  Time: 1.037s,  987.57/s  (1.010s, 1014.29/s)  LR: 7.480e-04  Data: 0.010 (0.015)
Train: 101 [ 450/1251 ( 36%)]  Loss: 3.620 (3.64)  Time: 0.997s, 1027.43/s  (1.008s, 1015.81/s)  LR: 7.480e-04  Data: 0.011 (0.014)
Train: 101 [ 500/1251 ( 40%)]  Loss: 3.985 (3.67)  Time: 0.996s, 1028.42/s  (1.007s, 1016.92/s)  LR: 7.480e-04  Data: 0.010 (0.014)
Train: 101 [ 550/1251 ( 44%)]  Loss: 3.620 (3.67)  Time: 0.991s, 1033.80/s  (1.007s, 1017.00/s)  LR: 7.480e-04  Data: 0.010 (0.014)
Train: 101 [ 600/1251 ( 48%)]  Loss: 3.650 (3.67)  Time: 0.999s, 1025.41/s  (1.006s, 1017.64/s)  LR: 7.480e-04  Data: 0.012 (0.014)
Train: 101 [ 650/1251 ( 52%)]  Loss: 4.008 (3.69)  Time: 0.995s, 1028.94/s  (1.006s, 1017.47/s)  LR: 7.480e-04  Data: 0.011 (0.013)
Train: 101 [ 700/1251 ( 56%)]  Loss: 4.026 (3.71)  Time: 1.038s,  986.96/s  (1.006s, 1017.50/s)  LR: 7.480e-04  Data: 0.010 (0.013)
Train: 101 [ 750/1251 ( 60%)]  Loss: 3.721 (3.71)  Time: 0.998s, 1026.04/s  (1.006s, 1017.82/s)  LR: 7.480e-04  Data: 0.011 (0.013)
Train: 101 [ 800/1251 ( 64%)]  Loss: 3.619 (3.71)  Time: 0.993s, 1030.88/s  (1.006s, 1018.18/s)  LR: 7.480e-04  Data: 0.011 (0.013)
Train: 101 [ 850/1251 ( 68%)]  Loss: 3.904 (3.72)  Time: 1.039s,  985.17/s  (1.005s, 1018.65/s)  LR: 7.480e-04  Data: 0.012 (0.013)
Train: 101 [ 900/1251 ( 72%)]  Loss: 3.613 (3.71)  Time: 0.996s, 1028.49/s  (1.006s, 1017.66/s)  LR: 7.480e-04  Data: 0.012 (0.013)
Train: 101 [ 950/1251 ( 76%)]  Loss: 3.857 (3.72)  Time: 0.997s, 1027.20/s  (1.006s, 1017.71/s)  LR: 7.480e-04  Data: 0.011 (0.013)
Train: 101 [1000/1251 ( 80%)]  Loss: 3.961 (3.73)  Time: 0.995s, 1028.86/s  (1.006s, 1017.88/s)  LR: 7.480e-04  Data: 0.012 (0.013)
Train: 101 [1050/1251 ( 84%)]  Loss: 4.121 (3.75)  Time: 0.996s, 1027.74/s  (1.006s, 1017.94/s)  LR: 7.480e-04  Data: 0.011 (0.013)
Train: 101 [1100/1251 ( 88%)]  Loss: 3.429 (3.74)  Time: 0.996s, 1028.60/s  (1.006s, 1018.07/s)  LR: 7.480e-04  Data: 0.011 (0.012)
Train: 101 [1150/1251 ( 92%)]  Loss: 3.671 (3.73)  Time: 0.994s, 1029.75/s  (1.006s, 1018.11/s)  LR: 7.480e-04  Data: 0.011 (0.012)
Train: 101 [1200/1251 ( 96%)]  Loss: 3.109 (3.71)  Time: 0.995s, 1028.67/s  (1.005s, 1018.48/s)  LR: 7.480e-04  Data: 0.011 (0.012)
Train: 101 [1250/1251 (100%)]  Loss: 3.864 (3.71)  Time: 0.984s, 1040.22/s  (1.005s, 1018.82/s)  LR: 7.480e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.635 (1.635)  Loss:  0.9346 (0.9346)  Acc@1: 86.2305 (86.2305)  Acc@5: 96.1914 (96.1914)
Test: [  48/48]  Time: 0.245 (0.566)  Loss:  0.9884 (1.4250)  Acc@1: 82.4292 (71.8960)  Acc@5: 94.6934 (90.8500)
Train: 102 [   0/1251 (  0%)]  Loss: 3.862 (3.86)  Time: 4.158s,  246.27/s  (4.158s,  246.27/s)  LR: 7.435e-04  Data: 2.948 (2.948)
Train: 102 [  50/1251 (  4%)]  Loss: 3.801 (3.83)  Time: 0.999s, 1024.80/s  (1.091s,  938.64/s)  LR: 7.435e-04  Data: 0.011 (0.069)
Train: 102 [ 100/1251 (  8%)]  Loss: 4.104 (3.92)  Time: 1.039s,  985.69/s  (1.052s,  973.27/s)  LR: 7.435e-04  Data: 0.012 (0.040)
Train: 102 [ 150/1251 ( 12%)]  Loss: 3.727 (3.87)  Time: 0.999s, 1025.46/s  (1.037s,  987.21/s)  LR: 7.435e-04  Data: 0.010 (0.031)
Train: 102 [ 200/1251 ( 16%)]  Loss: 3.513 (3.80)  Time: 1.066s,  960.49/s  (1.032s,  992.47/s)  LR: 7.435e-04  Data: 0.012 (0.026)
Train: 102 [ 250/1251 ( 20%)]  Loss: 3.725 (3.79)  Time: 0.993s, 1031.70/s  (1.029s,  994.88/s)  LR: 7.435e-04  Data: 0.011 (0.023)
Train: 102 [ 300/1251 ( 24%)]  Loss: 3.575 (3.76)  Time: 1.001s, 1023.12/s  (1.026s,  997.89/s)  LR: 7.435e-04  Data: 0.011 (0.021)
Train: 102 [ 350/1251 ( 28%)]  Loss: 3.942 (3.78)  Time: 0.996s, 1028.29/s  (1.024s, 1000.15/s)  LR: 7.435e-04  Data: 0.012 (0.020)
Train: 102 [ 400/1251 ( 32%)]  Loss: 3.589 (3.76)  Time: 0.995s, 1029.00/s  (1.021s, 1002.85/s)  LR: 7.435e-04  Data: 0.011 (0.019)
Train: 102 [ 450/1251 ( 36%)]  Loss: 3.868 (3.77)  Time: 1.030s,  994.05/s  (1.019s, 1005.06/s)  LR: 7.435e-04  Data: 0.012 (0.018)
Train: 102 [ 500/1251 ( 40%)]  Loss: 4.101 (3.80)  Time: 0.994s, 1030.24/s  (1.017s, 1006.57/s)  LR: 7.435e-04  Data: 0.011 (0.017)
Train: 102 [ 550/1251 ( 44%)]  Loss: 3.593 (3.78)  Time: 0.997s, 1026.86/s  (1.017s, 1007.08/s)  LR: 7.435e-04  Data: 0.010 (0.016)
Train: 102 [ 600/1251 ( 48%)]  Loss: 3.777 (3.78)  Time: 0.993s, 1031.04/s  (1.016s, 1007.89/s)  LR: 7.435e-04  Data: 0.011 (0.016)
Train: 102 [ 650/1251 ( 52%)]  Loss: 3.501 (3.76)  Time: 1.038s,  986.17/s  (1.016s, 1008.06/s)  LR: 7.435e-04  Data: 0.012 (0.016)
Train: 102 [ 700/1251 ( 56%)]  Loss: 3.637 (3.75)  Time: 0.996s, 1028.34/s  (1.015s, 1009.05/s)  LR: 7.435e-04  Data: 0.011 (0.015)
Train: 102 [ 750/1251 ( 60%)]  Loss: 3.683 (3.75)  Time: 0.994s, 1030.37/s  (1.014s, 1009.84/s)  LR: 7.435e-04  Data: 0.011 (0.015)
Train: 102 [ 800/1251 ( 64%)]  Loss: 3.667 (3.75)  Time: 0.994s, 1030.04/s  (1.013s, 1010.95/s)  LR: 7.435e-04  Data: 0.010 (0.015)
Train: 102 [ 850/1251 ( 68%)]  Loss: 3.682 (3.74)  Time: 0.995s, 1028.75/s  (1.012s, 1011.57/s)  LR: 7.435e-04  Data: 0.011 (0.015)
Train: 102 [ 900/1251 ( 72%)]  Loss: 3.647 (3.74)  Time: 1.060s,  966.41/s  (1.012s, 1011.56/s)  LR: 7.435e-04  Data: 0.011 (0.014)
Train: 102 [ 950/1251 ( 76%)]  Loss: 3.561 (3.73)  Time: 0.997s, 1027.35/s  (1.013s, 1011.05/s)  LR: 7.435e-04  Data: 0.010 (0.014)
Train: 102 [1000/1251 ( 80%)]  Loss: 3.460 (3.71)  Time: 0.999s, 1024.69/s  (1.013s, 1010.94/s)  LR: 7.435e-04  Data: 0.011 (0.014)
Train: 102 [1050/1251 ( 84%)]  Loss: 4.091 (3.73)  Time: 0.994s, 1029.71/s  (1.012s, 1011.41/s)  LR: 7.435e-04  Data: 0.011 (0.014)
Train: 102 [1100/1251 ( 88%)]  Loss: 3.605 (3.73)  Time: 0.997s, 1027.14/s  (1.012s, 1011.64/s)  LR: 7.435e-04  Data: 0.011 (0.014)
Train: 102 [1150/1251 ( 92%)]  Loss: 3.687 (3.72)  Time: 1.007s, 1017.39/s  (1.012s, 1012.28/s)  LR: 7.435e-04  Data: 0.012 (0.014)
Train: 102 [1200/1251 ( 96%)]  Loss: 3.484 (3.72)  Time: 0.995s, 1028.88/s  (1.011s, 1012.79/s)  LR: 7.435e-04  Data: 0.012 (0.014)
Train: 102 [1250/1251 (100%)]  Loss: 3.851 (3.72)  Time: 0.986s, 1038.61/s  (1.011s, 1013.18/s)  LR: 7.435e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.611 (1.611)  Loss:  0.8846 (0.8846)  Acc@1: 87.9883 (87.9883)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.245 (0.571)  Loss:  0.9778 (1.4552)  Acc@1: 83.3726 (72.3540)  Acc@5: 95.4009 (91.3200)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-97.pth.tar', 72.47400012695313)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-102.pth.tar', 72.35400001708985)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-94.pth.tar', 72.16800004394531)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-92.pth.tar', 72.15199997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-95.pth.tar', 72.13600001220703)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-96.pth.tar', 72.08600017089844)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-88.pth.tar', 72.02400007080078)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-93.pth.tar', 71.95599991699218)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-100.pth.tar', 71.91399996826172)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-99.pth.tar', 71.91200009521485)

Train: 103 [   0/1251 (  0%)]  Loss: 3.680 (3.68)  Time: 2.632s,  389.08/s  (2.632s,  389.08/s)  LR: 7.389e-04  Data: 1.664 (1.664)
Train: 103 [  50/1251 (  4%)]  Loss: 3.625 (3.65)  Time: 0.994s, 1029.99/s  (1.034s,  990.13/s)  LR: 7.389e-04  Data: 0.011 (0.044)
Train: 103 [ 100/1251 (  8%)]  Loss: 3.451 (3.59)  Time: 0.994s, 1030.41/s  (1.019s, 1004.73/s)  LR: 7.389e-04  Data: 0.011 (0.028)
Train: 103 [ 150/1251 ( 12%)]  Loss: 3.703 (3.61)  Time: 0.996s, 1028.16/s  (1.015s, 1008.67/s)  LR: 7.389e-04  Data: 0.011 (0.022)
Train: 103 [ 200/1251 ( 16%)]  Loss: 3.737 (3.64)  Time: 0.996s, 1027.90/s  (1.015s, 1009.14/s)  LR: 7.389e-04  Data: 0.012 (0.019)
Train: 103 [ 250/1251 ( 20%)]  Loss: 3.819 (3.67)  Time: 0.996s, 1027.70/s  (1.012s, 1012.31/s)  LR: 7.389e-04  Data: 0.011 (0.018)
Train: 103 [ 300/1251 ( 24%)]  Loss: 3.888 (3.70)  Time: 0.997s, 1027.00/s  (1.009s, 1014.77/s)  LR: 7.389e-04  Data: 0.011 (0.017)
Train: 103 [ 350/1251 ( 28%)]  Loss: 3.609 (3.69)  Time: 1.034s,  990.52/s  (1.008s, 1015.52/s)  LR: 7.389e-04  Data: 0.011 (0.016)
Train: 103 [ 400/1251 ( 32%)]  Loss: 3.525 (3.67)  Time: 0.996s, 1027.94/s  (1.008s, 1016.13/s)  LR: 7.389e-04  Data: 0.011 (0.015)
Train: 103 [ 450/1251 ( 36%)]  Loss: 3.329 (3.64)  Time: 0.995s, 1029.60/s  (1.007s, 1016.97/s)  LR: 7.389e-04  Data: 0.011 (0.015)
Train: 103 [ 500/1251 ( 40%)]  Loss: 3.692 (3.64)  Time: 0.994s, 1030.43/s  (1.006s, 1017.75/s)  LR: 7.389e-04  Data: 0.011 (0.014)
Train: 103 [ 550/1251 ( 44%)]  Loss: 3.752 (3.65)  Time: 0.995s, 1029.51/s  (1.006s, 1018.23/s)  LR: 7.389e-04  Data: 0.011 (0.014)
Train: 103 [ 600/1251 ( 48%)]  Loss: 3.561 (3.64)  Time: 1.017s, 1006.75/s  (1.005s, 1018.87/s)  LR: 7.389e-04  Data: 0.014 (0.014)
Train: 103 [ 650/1251 ( 52%)]  Loss: 3.463 (3.63)  Time: 0.994s, 1029.80/s  (1.005s, 1019.37/s)  LR: 7.389e-04  Data: 0.011 (0.014)
Train: 103 [ 700/1251 ( 56%)]  Loss: 3.575 (3.63)  Time: 1.002s, 1021.77/s  (1.004s, 1019.79/s)  LR: 7.389e-04  Data: 0.010 (0.014)
Train: 103 [ 750/1251 ( 60%)]  Loss: 3.965 (3.65)  Time: 1.000s, 1024.03/s  (1.004s, 1019.72/s)  LR: 7.389e-04  Data: 0.010 (0.013)
Train: 103 [ 800/1251 ( 64%)]  Loss: 3.605 (3.65)  Time: 1.040s,  984.52/s  (1.004s, 1019.95/s)  LR: 7.389e-04  Data: 0.012 (0.013)
Train: 103 [ 850/1251 ( 68%)]  Loss: 3.624 (3.64)  Time: 0.997s, 1027.19/s  (1.004s, 1019.58/s)  LR: 7.389e-04  Data: 0.012 (0.013)
Train: 103 [ 900/1251 ( 72%)]  Loss: 3.481 (3.64)  Time: 1.007s, 1017.25/s  (1.005s, 1019.22/s)  LR: 7.389e-04  Data: 0.011 (0.013)
Train: 103 [ 950/1251 ( 76%)]  Loss: 4.125 (3.66)  Time: 0.996s, 1027.87/s  (1.005s, 1019.36/s)  LR: 7.389e-04  Data: 0.010 (0.013)
Train: 103 [1000/1251 ( 80%)]  Loss: 3.560 (3.66)  Time: 1.036s,  988.39/s  (1.006s, 1017.67/s)  LR: 7.389e-04  Data: 0.011 (0.013)
Train: 103 [1050/1251 ( 84%)]  Loss: 3.526 (3.65)  Time: 1.002s, 1021.87/s  (1.006s, 1017.74/s)  LR: 7.389e-04  Data: 0.011 (0.013)
Train: 103 [1100/1251 ( 88%)]  Loss: 3.766 (3.65)  Time: 0.994s, 1030.47/s  (1.007s, 1017.31/s)  LR: 7.389e-04  Data: 0.011 (0.013)
Train: 103 [1150/1251 ( 92%)]  Loss: 3.588 (3.65)  Time: 0.996s, 1028.34/s  (1.006s, 1017.54/s)  LR: 7.389e-04  Data: 0.011 (0.013)
Train: 103 [1200/1251 ( 96%)]  Loss: 3.722 (3.65)  Time: 1.005s, 1018.44/s  (1.006s, 1017.81/s)  LR: 7.389e-04  Data: 0.011 (0.013)
Train: 103 [1250/1251 (100%)]  Loss: 3.956 (3.67)  Time: 0.983s, 1041.40/s  (1.006s, 1017.55/s)  LR: 7.389e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.670 (1.670)  Loss:  0.8025 (0.8025)  Acc@1: 86.7188 (86.7188)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.245 (0.575)  Loss:  0.9583 (1.3798)  Acc@1: 82.6651 (72.6700)  Acc@5: 95.8726 (91.5420)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-103.pth.tar', 72.67000009765626)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-97.pth.tar', 72.47400012695313)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-102.pth.tar', 72.35400001708985)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-94.pth.tar', 72.16800004394531)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-92.pth.tar', 72.15199997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-95.pth.tar', 72.13600001220703)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-96.pth.tar', 72.08600017089844)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-88.pth.tar', 72.02400007080078)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-93.pth.tar', 71.95599991699218)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-100.pth.tar', 71.91399996826172)

Train: 104 [   0/1251 (  0%)]  Loss: 3.748 (3.75)  Time: 2.473s,  414.12/s  (2.473s,  414.12/s)  LR: 7.343e-04  Data: 1.506 (1.506)
Train: 104 [  50/1251 (  4%)]  Loss: 3.715 (3.73)  Time: 0.993s, 1030.83/s  (1.045s,  979.48/s)  LR: 7.343e-04  Data: 0.011 (0.041)
Train: 104 [ 100/1251 (  8%)]  Loss: 3.728 (3.73)  Time: 1.066s,  960.54/s  (1.031s,  993.53/s)  LR: 7.343e-04  Data: 0.012 (0.026)
Train: 104 [ 150/1251 ( 12%)]  Loss: 3.420 (3.65)  Time: 1.005s, 1019.24/s  (1.025s,  999.35/s)  LR: 7.343e-04  Data: 0.012 (0.021)
Train: 104 [ 200/1251 ( 16%)]  Loss: 3.510 (3.62)  Time: 1.000s, 1023.67/s  (1.022s, 1001.60/s)  LR: 7.343e-04  Data: 0.011 (0.019)
Train: 104 [ 250/1251 ( 20%)]  Loss: 3.430 (3.59)  Time: 0.995s, 1029.42/s  (1.018s, 1006.14/s)  LR: 7.343e-04  Data: 0.010 (0.017)
Train: 104 [ 300/1251 ( 24%)]  Loss: 3.537 (3.58)  Time: 0.997s, 1027.17/s  (1.017s, 1006.67/s)  LR: 7.343e-04  Data: 0.011 (0.016)
Train: 104 [ 350/1251 ( 28%)]  Loss: 3.824 (3.61)  Time: 0.994s, 1030.59/s  (1.015s, 1009.24/s)  LR: 7.343e-04  Data: 0.012 (0.016)
Train: 104 [ 400/1251 ( 32%)]  Loss: 3.690 (3.62)  Time: 0.996s, 1028.55/s  (1.015s, 1008.93/s)  LR: 7.343e-04  Data: 0.012 (0.015)
Train: 104 [ 450/1251 ( 36%)]  Loss: 3.987 (3.66)  Time: 0.996s, 1028.12/s  (1.014s, 1009.73/s)  LR: 7.343e-04  Data: 0.012 (0.015)
Train: 104 [ 500/1251 ( 40%)]  Loss: 3.341 (3.63)  Time: 1.005s, 1019.21/s  (1.013s, 1010.62/s)  LR: 7.343e-04  Data: 0.011 (0.014)
Train: 104 [ 550/1251 ( 44%)]  Loss: 3.293 (3.60)  Time: 0.995s, 1029.30/s  (1.013s, 1011.16/s)  LR: 7.343e-04  Data: 0.012 (0.014)
Train: 104 [ 600/1251 ( 48%)]  Loss: 3.784 (3.62)  Time: 1.004s, 1020.23/s  (1.015s, 1008.98/s)  LR: 7.343e-04  Data: 0.011 (0.014)
Train: 104 [ 650/1251 ( 52%)]  Loss: 3.685 (3.62)  Time: 1.000s, 1024.47/s  (1.014s, 1010.11/s)  LR: 7.343e-04  Data: 0.010 (0.014)
Train: 104 [ 700/1251 ( 56%)]  Loss: 3.768 (3.63)  Time: 1.001s, 1022.72/s  (1.013s, 1010.90/s)  LR: 7.343e-04  Data: 0.012 (0.013)
Train: 104 [ 750/1251 ( 60%)]  Loss: 3.735 (3.64)  Time: 0.997s, 1027.32/s  (1.012s, 1011.60/s)  LR: 7.343e-04  Data: 0.012 (0.013)
Train: 104 [ 800/1251 ( 64%)]  Loss: 3.863 (3.65)  Time: 0.996s, 1027.69/s  (1.012s, 1011.72/s)  LR: 7.343e-04  Data: 0.011 (0.013)
Train: 104 [ 850/1251 ( 68%)]  Loss: 3.411 (3.64)  Time: 1.002s, 1022.04/s  (1.011s, 1012.56/s)  LR: 7.343e-04  Data: 0.012 (0.013)
Train: 104 [ 900/1251 ( 72%)]  Loss: 3.542 (3.63)  Time: 0.996s, 1028.04/s  (1.011s, 1012.63/s)  LR: 7.343e-04  Data: 0.011 (0.013)
Train: 104 [ 950/1251 ( 76%)]  Loss: 3.531 (3.63)  Time: 1.049s,  975.96/s  (1.011s, 1012.55/s)  LR: 7.343e-04  Data: 0.010 (0.013)
Train: 104 [1000/1251 ( 80%)]  Loss: 3.899 (3.64)  Time: 1.005s, 1019.04/s  (1.011s, 1012.90/s)  LR: 7.343e-04  Data: 0.011 (0.013)
Train: 104 [1050/1251 ( 84%)]  Loss: 3.824 (3.65)  Time: 0.994s, 1030.09/s  (1.010s, 1013.42/s)  LR: 7.343e-04  Data: 0.011 (0.013)
Train: 104 [1100/1251 ( 88%)]  Loss: 3.651 (3.65)  Time: 1.001s, 1022.73/s  (1.010s, 1014.01/s)  LR: 7.343e-04  Data: 0.011 (0.013)
Train: 104 [1150/1251 ( 92%)]  Loss: 3.715 (3.65)  Time: 0.993s, 1030.79/s  (1.009s, 1014.42/s)  LR: 7.343e-04  Data: 0.011 (0.013)
Train: 104 [1200/1251 ( 96%)]  Loss: 3.585 (3.65)  Time: 0.994s, 1030.36/s  (1.009s, 1014.78/s)  LR: 7.343e-04  Data: 0.011 (0.012)
Train: 104 [1250/1251 (100%)]  Loss: 3.758 (3.65)  Time: 0.984s, 1040.71/s  (1.010s, 1014.35/s)  LR: 7.343e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.664 (1.664)  Loss:  0.8459 (0.8459)  Acc@1: 87.2070 (87.2070)  Acc@5: 96.1914 (96.1914)
Test: [  48/48]  Time: 0.245 (0.574)  Loss:  0.8986 (1.4196)  Acc@1: 83.7264 (72.2220)  Acc@5: 95.5189 (91.1940)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-103.pth.tar', 72.67000009765626)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-97.pth.tar', 72.47400012695313)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-102.pth.tar', 72.35400001708985)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-104.pth.tar', 72.2220000415039)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-94.pth.tar', 72.16800004394531)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-92.pth.tar', 72.15199997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-95.pth.tar', 72.13600001220703)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-96.pth.tar', 72.08600017089844)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-88.pth.tar', 72.02400007080078)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-93.pth.tar', 71.95599991699218)

Train: 105 [   0/1251 (  0%)]  Loss: 3.859 (3.86)  Time: 2.552s,  401.31/s  (2.552s,  401.31/s)  LR: 7.297e-04  Data: 1.544 (1.544)
Train: 105 [  50/1251 (  4%)]  Loss: 3.494 (3.68)  Time: 1.057s,  968.68/s  (1.050s,  975.11/s)  LR: 7.297e-04  Data: 0.011 (0.042)
Train: 105 [ 100/1251 (  8%)]  Loss: 3.837 (3.73)  Time: 0.996s, 1027.96/s  (1.030s,  994.09/s)  LR: 7.297e-04  Data: 0.011 (0.027)
Train: 105 [ 150/1251 ( 12%)]  Loss: 3.414 (3.65)  Time: 0.995s, 1028.80/s  (1.025s,  999.29/s)  LR: 7.297e-04  Data: 0.011 (0.021)
Train: 105 [ 200/1251 ( 16%)]  Loss: 3.831 (3.69)  Time: 0.994s, 1030.30/s  (1.018s, 1005.68/s)  LR: 7.297e-04  Data: 0.011 (0.019)
Train: 105 [ 250/1251 ( 20%)]  Loss: 3.450 (3.65)  Time: 1.050s,  974.79/s  (1.016s, 1008.37/s)  LR: 7.297e-04  Data: 0.012 (0.017)
Train: 105 [ 300/1251 ( 24%)]  Loss: 3.635 (3.65)  Time: 0.998s, 1026.36/s  (1.013s, 1011.25/s)  LR: 7.297e-04  Data: 0.012 (0.016)
Train: 105 [ 350/1251 ( 28%)]  Loss: 3.608 (3.64)  Time: 0.999s, 1025.53/s  (1.011s, 1013.22/s)  LR: 7.297e-04  Data: 0.011 (0.016)
Train: 105 [ 400/1251 ( 32%)]  Loss: 3.356 (3.61)  Time: 1.037s,  987.27/s  (1.009s, 1014.65/s)  LR: 7.297e-04  Data: 0.012 (0.015)
Train: 105 [ 450/1251 ( 36%)]  Loss: 3.955 (3.64)  Time: 1.048s,  977.38/s  (1.011s, 1013.33/s)  LR: 7.297e-04  Data: 0.011 (0.015)
Train: 105 [ 500/1251 ( 40%)]  Loss: 3.733 (3.65)  Time: 0.996s, 1028.02/s  (1.011s, 1013.14/s)  LR: 7.297e-04  Data: 0.011 (0.014)
Train: 105 [ 550/1251 ( 44%)]  Loss: 3.792 (3.66)  Time: 1.002s, 1021.46/s  (1.010s, 1014.00/s)  LR: 7.297e-04  Data: 0.012 (0.014)
Train: 105 [ 600/1251 ( 48%)]  Loss: 3.985 (3.69)  Time: 1.005s, 1019.23/s  (1.009s, 1014.90/s)  LR: 7.297e-04  Data: 0.012 (0.014)
Train: 105 [ 650/1251 ( 52%)]  Loss: 3.973 (3.71)  Time: 0.999s, 1025.22/s  (1.008s, 1015.38/s)  LR: 7.297e-04  Data: 0.011 (0.014)
Train: 105 [ 700/1251 ( 56%)]  Loss: 3.771 (3.71)  Time: 0.995s, 1029.09/s  (1.008s, 1015.95/s)  LR: 7.297e-04  Data: 0.010 (0.013)
Train: 105 [ 750/1251 ( 60%)]  Loss: 3.582 (3.70)  Time: 1.031s,  993.27/s  (1.008s, 1015.79/s)  LR: 7.297e-04  Data: 0.011 (0.013)
Train: 105 [ 800/1251 ( 64%)]  Loss: 3.949 (3.72)  Time: 0.998s, 1026.10/s  (1.008s, 1016.19/s)  LR: 7.297e-04  Data: 0.012 (0.013)
Train: 105 [ 850/1251 ( 68%)]  Loss: 3.447 (3.70)  Time: 0.994s, 1029.82/s  (1.007s, 1016.66/s)  LR: 7.297e-04  Data: 0.011 (0.013)
Train: 105 [ 900/1251 ( 72%)]  Loss: 3.747 (3.71)  Time: 1.031s,  993.02/s  (1.007s, 1016.91/s)  LR: 7.297e-04  Data: 0.012 (0.013)
Train: 105 [ 950/1251 ( 76%)]  Loss: 3.771 (3.71)  Time: 0.994s, 1030.15/s  (1.008s, 1015.38/s)  LR: 7.297e-04  Data: 0.011 (0.013)
Train: 105 [1000/1251 ( 80%)]  Loss: 3.495 (3.70)  Time: 1.005s, 1018.85/s  (1.008s, 1015.82/s)  LR: 7.297e-04  Data: 0.011 (0.013)
Train: 105 [1050/1251 ( 84%)]  Loss: 3.494 (3.69)  Time: 0.997s, 1027.15/s  (1.008s, 1016.07/s)  LR: 7.297e-04  Data: 0.012 (0.013)
Train: 105 [1100/1251 ( 88%)]  Loss: 3.338 (3.67)  Time: 0.994s, 1030.05/s  (1.007s, 1016.45/s)  LR: 7.297e-04  Data: 0.010 (0.013)
Train: 105 [1150/1251 ( 92%)]  Loss: 3.891 (3.68)  Time: 0.993s, 1031.13/s  (1.007s, 1016.83/s)  LR: 7.297e-04  Data: 0.011 (0.013)
Train: 105 [1200/1251 ( 96%)]  Loss: 3.997 (3.70)  Time: 0.998s, 1026.41/s  (1.007s, 1017.12/s)  LR: 7.297e-04  Data: 0.011 (0.012)
Train: 105 [1250/1251 (100%)]  Loss: 4.080 (3.71)  Time: 1.039s,  985.18/s  (1.008s, 1016.29/s)  LR: 7.297e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.612 (1.612)  Loss:  0.7996 (0.7996)  Acc@1: 87.6953 (87.6953)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.245 (0.571)  Loss:  0.9323 (1.3635)  Acc@1: 83.1368 (72.5680)  Acc@5: 94.8113 (91.3900)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-103.pth.tar', 72.67000009765626)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-105.pth.tar', 72.56800004394532)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-97.pth.tar', 72.47400012695313)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-102.pth.tar', 72.35400001708985)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-104.pth.tar', 72.2220000415039)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-94.pth.tar', 72.16800004394531)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-92.pth.tar', 72.15199997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-95.pth.tar', 72.13600001220703)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-96.pth.tar', 72.08600017089844)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-88.pth.tar', 72.02400007080078)

Train: 106 [   0/1251 (  0%)]  Loss: 3.703 (3.70)  Time: 2.442s,  419.30/s  (2.442s,  419.30/s)  LR: 7.251e-04  Data: 1.482 (1.482)
Train: 106 [  50/1251 (  4%)]  Loss: 3.555 (3.63)  Time: 0.996s, 1028.27/s  (1.045s,  979.73/s)  LR: 7.251e-04  Data: 0.011 (0.046)
Train: 106 [ 100/1251 (  8%)]  Loss: 3.760 (3.67)  Time: 1.000s, 1023.52/s  (1.026s,  997.69/s)  LR: 7.251e-04  Data: 0.010 (0.029)
Train: 106 [ 150/1251 ( 12%)]  Loss: 3.588 (3.65)  Time: 0.990s, 1034.54/s  (1.019s, 1004.83/s)  LR: 7.251e-04  Data: 0.010 (0.023)
Train: 106 [ 200/1251 ( 16%)]  Loss: 3.835 (3.69)  Time: 0.997s, 1027.29/s  (1.013s, 1010.61/s)  LR: 7.251e-04  Data: 0.011 (0.020)
Train: 106 [ 250/1251 ( 20%)]  Loss: 3.973 (3.74)  Time: 0.996s, 1028.12/s  (1.015s, 1008.66/s)  LR: 7.251e-04  Data: 0.012 (0.018)
Train: 106 [ 300/1251 ( 24%)]  Loss: 3.609 (3.72)  Time: 1.000s, 1023.53/s  (1.014s, 1010.14/s)  LR: 7.251e-04  Data: 0.011 (0.017)
Train: 106 [ 350/1251 ( 28%)]  Loss: 3.870 (3.74)  Time: 0.995s, 1029.44/s  (1.013s, 1011.31/s)  LR: 7.251e-04  Data: 0.011 (0.016)
Train: 106 [ 400/1251 ( 32%)]  Loss: 3.572 (3.72)  Time: 0.998s, 1026.44/s  (1.011s, 1012.52/s)  LR: 7.251e-04  Data: 0.012 (0.016)
Train: 106 [ 450/1251 ( 36%)]  Loss: 3.978 (3.74)  Time: 0.994s, 1030.41/s  (1.010s, 1013.85/s)  LR: 7.251e-04  Data: 0.011 (0.015)
Train: 106 [ 500/1251 ( 40%)]  Loss: 3.954 (3.76)  Time: 0.999s, 1025.21/s  (1.009s, 1014.89/s)  LR: 7.251e-04  Data: 0.011 (0.015)
Train: 106 [ 550/1251 ( 44%)]  Loss: 3.746 (3.76)  Time: 1.001s, 1022.62/s  (1.009s, 1015.21/s)  LR: 7.251e-04  Data: 0.011 (0.015)
Train: 106 [ 600/1251 ( 48%)]  Loss: 3.321 (3.73)  Time: 1.033s,  991.07/s  (1.009s, 1015.04/s)  LR: 7.251e-04  Data: 0.011 (0.014)
Train: 106 [ 650/1251 ( 52%)]  Loss: 3.791 (3.73)  Time: 0.997s, 1026.60/s  (1.009s, 1014.67/s)  LR: 7.251e-04  Data: 0.011 (0.014)
Train: 106 [ 700/1251 ( 56%)]  Loss: 3.595 (3.72)  Time: 1.020s, 1004.04/s  (1.009s, 1014.77/s)  LR: 7.251e-04  Data: 0.011 (0.014)
Train: 106 [ 750/1251 ( 60%)]  Loss: 3.655 (3.72)  Time: 0.993s, 1030.73/s  (1.009s, 1015.13/s)  LR: 7.251e-04  Data: 0.011 (0.014)
Train: 106 [ 800/1251 ( 64%)]  Loss: 3.828 (3.73)  Time: 0.995s, 1029.54/s  (1.009s, 1014.99/s)  LR: 7.251e-04  Data: 0.011 (0.013)
Train: 106 [ 850/1251 ( 68%)]  Loss: 3.942 (3.74)  Time: 0.996s, 1028.53/s  (1.009s, 1015.13/s)  LR: 7.251e-04  Data: 0.012 (0.013)
Train: 106 [ 900/1251 ( 72%)]  Loss: 3.918 (3.75)  Time: 0.998s, 1025.92/s  (1.009s, 1015.35/s)  LR: 7.251e-04  Data: 0.011 (0.013)
Train: 106 [ 950/1251 ( 76%)]  Loss: 4.012 (3.76)  Time: 0.999s, 1024.85/s  (1.009s, 1015.36/s)  LR: 7.251e-04  Data: 0.010 (0.013)
Train: 106 [1000/1251 ( 80%)]  Loss: 3.396 (3.74)  Time: 1.033s,  991.03/s  (1.009s, 1015.02/s)  LR: 7.251e-04  Data: 0.012 (0.013)
Train: 106 [1050/1251 ( 84%)]  Loss: 3.543 (3.73)  Time: 0.996s, 1028.48/s  (1.009s, 1015.14/s)  LR: 7.251e-04  Data: 0.012 (0.013)
Train: 106 [1100/1251 ( 88%)]  Loss: 3.822 (3.74)  Time: 0.995s, 1029.28/s  (1.009s, 1014.77/s)  LR: 7.251e-04  Data: 0.011 (0.013)
Train: 106 [1150/1251 ( 92%)]  Loss: 3.857 (3.74)  Time: 0.994s, 1029.97/s  (1.010s, 1014.08/s)  LR: 7.251e-04  Data: 0.011 (0.013)
Train: 106 [1200/1251 ( 96%)]  Loss: 3.901 (3.75)  Time: 0.995s, 1028.95/s  (1.009s, 1014.37/s)  LR: 7.251e-04  Data: 0.010 (0.013)
Train: 106 [1250/1251 (100%)]  Loss: 3.747 (3.75)  Time: 0.977s, 1047.62/s  (1.009s, 1014.58/s)  LR: 7.251e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.609 (1.609)  Loss:  0.9583 (0.9583)  Acc@1: 87.3047 (87.3047)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.245 (0.567)  Loss:  1.0344 (1.4505)  Acc@1: 82.3113 (72.5440)  Acc@5: 95.8726 (91.3260)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-103.pth.tar', 72.67000009765626)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-105.pth.tar', 72.56800004394532)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-106.pth.tar', 72.54399994384765)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-97.pth.tar', 72.47400012695313)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-102.pth.tar', 72.35400001708985)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-104.pth.tar', 72.2220000415039)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-94.pth.tar', 72.16800004394531)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-92.pth.tar', 72.15199997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-95.pth.tar', 72.13600001220703)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-96.pth.tar', 72.08600017089844)

Train: 107 [   0/1251 (  0%)]  Loss: 3.380 (3.38)  Time: 2.483s,  412.38/s  (2.483s,  412.38/s)  LR: 7.204e-04  Data: 1.522 (1.522)
Train: 107 [  50/1251 (  4%)]  Loss: 3.636 (3.51)  Time: 1.023s, 1000.68/s  (1.065s,  961.39/s)  LR: 7.204e-04  Data: 0.011 (0.041)
Train: 107 [ 100/1251 (  8%)]  Loss: 3.616 (3.54)  Time: 1.028s,  995.87/s  (1.039s,  985.93/s)  LR: 7.204e-04  Data: 0.010 (0.026)
Train: 107 [ 150/1251 ( 12%)]  Loss: 4.203 (3.71)  Time: 1.033s,  991.67/s  (1.028s,  995.97/s)  LR: 7.204e-04  Data: 0.011 (0.021)
Train: 107 [ 200/1251 ( 16%)]  Loss: 3.758 (3.72)  Time: 1.007s, 1017.02/s  (1.023s, 1001.09/s)  LR: 7.204e-04  Data: 0.012 (0.019)
Train: 107 [ 250/1251 ( 20%)]  Loss: 3.749 (3.72)  Time: 1.046s,  978.99/s  (1.019s, 1004.89/s)  LR: 7.204e-04  Data: 0.011 (0.017)
Train: 107 [ 300/1251 ( 24%)]  Loss: 3.510 (3.69)  Time: 0.997s, 1026.69/s  (1.017s, 1007.34/s)  LR: 7.204e-04  Data: 0.011 (0.016)
Train: 107 [ 350/1251 ( 28%)]  Loss: 3.730 (3.70)  Time: 1.006s, 1018.36/s  (1.019s, 1005.00/s)  LR: 7.204e-04  Data: 0.013 (0.015)
Train: 107 [ 400/1251 ( 32%)]  Loss: 3.580 (3.68)  Time: 1.009s, 1014.76/s  (1.017s, 1006.71/s)  LR: 7.204e-04  Data: 0.012 (0.015)
Train: 107 [ 450/1251 ( 36%)]  Loss: 3.803 (3.70)  Time: 0.998s, 1026.25/s  (1.016s, 1007.94/s)  LR: 7.204e-04  Data: 0.011 (0.014)
Train: 107 [ 500/1251 ( 40%)]  Loss: 3.813 (3.71)  Time: 0.997s, 1027.11/s  (1.016s, 1008.30/s)  LR: 7.204e-04  Data: 0.012 (0.014)
Train: 107 [ 550/1251 ( 44%)]  Loss: 3.563 (3.70)  Time: 1.039s,  985.87/s  (1.015s, 1008.60/s)  LR: 7.204e-04  Data: 0.011 (0.014)
Train: 107 [ 600/1251 ( 48%)]  Loss: 3.954 (3.71)  Time: 0.995s, 1029.29/s  (1.014s, 1009.48/s)  LR: 7.204e-04  Data: 0.012 (0.014)
Train: 107 [ 650/1251 ( 52%)]  Loss: 3.358 (3.69)  Time: 1.003s, 1020.98/s  (1.014s, 1010.07/s)  LR: 7.204e-04  Data: 0.011 (0.013)
Train: 107 [ 700/1251 ( 56%)]  Loss: 3.645 (3.69)  Time: 0.994s, 1030.01/s  (1.013s, 1010.60/s)  LR: 7.204e-04  Data: 0.010 (0.013)
Train: 107 [ 750/1251 ( 60%)]  Loss: 3.522 (3.68)  Time: 0.994s, 1029.99/s  (1.013s, 1010.55/s)  LR: 7.204e-04  Data: 0.011 (0.013)
Train: 107 [ 800/1251 ( 64%)]  Loss: 3.615 (3.67)  Time: 0.999s, 1025.37/s  (1.013s, 1011.30/s)  LR: 7.204e-04  Data: 0.012 (0.013)
Train: 107 [ 850/1251 ( 68%)]  Loss: 3.632 (3.67)  Time: 1.036s,  988.59/s  (1.012s, 1011.40/s)  LR: 7.204e-04  Data: 0.011 (0.013)
Train: 107 [ 900/1251 ( 72%)]  Loss: 3.674 (3.67)  Time: 1.035s,  989.05/s  (1.013s, 1011.09/s)  LR: 7.204e-04  Data: 0.012 (0.013)
Train: 107 [ 950/1251 ( 76%)]  Loss: 3.561 (3.67)  Time: 0.995s, 1029.22/s  (1.013s, 1011.21/s)  LR: 7.204e-04  Data: 0.011 (0.013)
Train: 107 [1000/1251 ( 80%)]  Loss: 3.694 (3.67)  Time: 1.064s,  962.82/s  (1.013s, 1011.02/s)  LR: 7.204e-04  Data: 0.012 (0.013)
Train: 107 [1050/1251 ( 84%)]  Loss: 3.399 (3.65)  Time: 1.008s, 1016.01/s  (1.013s, 1011.12/s)  LR: 7.204e-04  Data: 0.011 (0.013)
Train: 107 [1100/1251 ( 88%)]  Loss: 3.441 (3.65)  Time: 1.030s,  994.55/s  (1.013s, 1010.95/s)  LR: 7.204e-04  Data: 0.010 (0.012)
Train: 107 [1150/1251 ( 92%)]  Loss: 3.756 (3.65)  Time: 0.998s, 1025.56/s  (1.013s, 1010.72/s)  LR: 7.204e-04  Data: 0.011 (0.012)
Train: 107 [1200/1251 ( 96%)]  Loss: 3.626 (3.65)  Time: 0.997s, 1027.46/s  (1.014s, 1010.25/s)  LR: 7.204e-04  Data: 0.010 (0.012)
Train: 107 [1250/1251 (100%)]  Loss: 3.573 (3.65)  Time: 0.983s, 1041.57/s  (1.014s, 1010.24/s)  LR: 7.204e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.818 (1.818)  Loss:  0.9022 (0.9022)  Acc@1: 87.9883 (87.9883)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.245 (0.564)  Loss:  0.9705 (1.5010)  Acc@1: 84.0802 (72.5940)  Acc@5: 95.8726 (91.3060)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-103.pth.tar', 72.67000009765626)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-107.pth.tar', 72.59400006591797)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-105.pth.tar', 72.56800004394532)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-106.pth.tar', 72.54399994384765)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-97.pth.tar', 72.47400012695313)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-102.pth.tar', 72.35400001708985)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-104.pth.tar', 72.2220000415039)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-94.pth.tar', 72.16800004394531)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-92.pth.tar', 72.15199997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-95.pth.tar', 72.13600001220703)

Train: 108 [   0/1251 (  0%)]  Loss: 3.887 (3.89)  Time: 2.434s,  420.73/s  (2.434s,  420.73/s)  LR: 7.158e-04  Data: 1.473 (1.473)
Train: 108 [  50/1251 (  4%)]  Loss: 3.462 (3.67)  Time: 1.036s,  988.30/s  (1.048s,  976.97/s)  LR: 7.158e-04  Data: 0.012 (0.040)
Train: 108 [ 100/1251 (  8%)]  Loss: 3.225 (3.52)  Time: 0.995s, 1028.74/s  (1.025s,  998.70/s)  LR: 7.158e-04  Data: 0.011 (0.026)
Train: 108 [ 150/1251 ( 12%)]  Loss: 3.755 (3.58)  Time: 0.994s, 1029.92/s  (1.018s, 1005.52/s)  LR: 7.158e-04  Data: 0.011 (0.021)
Train: 108 [ 200/1251 ( 16%)]  Loss: 3.719 (3.61)  Time: 1.035s,  989.05/s  (1.015s, 1008.97/s)  LR: 7.158e-04  Data: 0.010 (0.019)
Train: 108 [ 250/1251 ( 20%)]  Loss: 3.734 (3.63)  Time: 0.996s, 1027.63/s  (1.012s, 1012.22/s)  LR: 7.158e-04  Data: 0.011 (0.017)
Train: 108 [ 300/1251 ( 24%)]  Loss: 3.923 (3.67)  Time: 1.002s, 1021.53/s  (1.010s, 1013.74/s)  LR: 7.158e-04  Data: 0.011 (0.016)
Train: 108 [ 350/1251 ( 28%)]  Loss: 3.614 (3.66)  Time: 0.995s, 1029.28/s  (1.009s, 1015.13/s)  LR: 7.158e-04  Data: 0.010 (0.015)
Train: 108 [ 400/1251 ( 32%)]  Loss: 3.777 (3.68)  Time: 0.996s, 1028.15/s  (1.007s, 1016.69/s)  LR: 7.158e-04  Data: 0.012 (0.015)
Train: 108 [ 450/1251 ( 36%)]  Loss: 3.965 (3.71)  Time: 1.043s,  981.91/s  (1.007s, 1016.66/s)  LR: 7.158e-04  Data: 0.011 (0.014)
Train: 108 [ 500/1251 ( 40%)]  Loss: 3.427 (3.68)  Time: 1.004s, 1019.49/s  (1.008s, 1016.35/s)  LR: 7.158e-04  Data: 0.011 (0.014)
Train: 108 [ 550/1251 ( 44%)]  Loss: 3.696 (3.68)  Time: 1.003s, 1020.56/s  (1.007s, 1016.97/s)  LR: 7.158e-04  Data: 0.011 (0.014)
Train: 108 [ 600/1251 ( 48%)]  Loss: 3.737 (3.69)  Time: 1.035s,  989.15/s  (1.007s, 1017.36/s)  LR: 7.158e-04  Data: 0.011 (0.014)
Train: 108 [ 650/1251 ( 52%)]  Loss: 3.738 (3.69)  Time: 0.992s, 1031.75/s  (1.007s, 1016.65/s)  LR: 7.158e-04  Data: 0.011 (0.014)
Train: 108 [ 700/1251 ( 56%)]  Loss: 3.822 (3.70)  Time: 0.995s, 1029.51/s  (1.007s, 1017.37/s)  LR: 7.158e-04  Data: 0.012 (0.013)
Train: 108 [ 750/1251 ( 60%)]  Loss: 3.872 (3.71)  Time: 1.031s,  993.36/s  (1.006s, 1017.41/s)  LR: 7.158e-04  Data: 0.011 (0.013)
Train: 108 [ 800/1251 ( 64%)]  Loss: 3.830 (3.72)  Time: 1.005s, 1019.05/s  (1.006s, 1017.47/s)  LR: 7.158e-04  Data: 0.011 (0.013)
Train: 108 [ 850/1251 ( 68%)]  Loss: 3.532 (3.71)  Time: 1.036s,  988.28/s  (1.008s, 1016.19/s)  LR: 7.158e-04  Data: 0.012 (0.013)
Train: 108 [ 900/1251 ( 72%)]  Loss: 3.666 (3.70)  Time: 0.996s, 1027.62/s  (1.007s, 1016.40/s)  LR: 7.158e-04  Data: 0.012 (0.013)
Train: 108 [ 950/1251 ( 76%)]  Loss: 3.509 (3.69)  Time: 1.004s, 1020.29/s  (1.007s, 1016.66/s)  LR: 7.158e-04  Data: 0.012 (0.013)
Train: 108 [1000/1251 ( 80%)]  Loss: 3.478 (3.68)  Time: 0.994s, 1030.65/s  (1.007s, 1016.56/s)  LR: 7.158e-04  Data: 0.011 (0.013)
Train: 108 [1050/1251 ( 84%)]  Loss: 3.581 (3.68)  Time: 0.999s, 1024.84/s  (1.007s, 1016.79/s)  LR: 7.158e-04  Data: 0.010 (0.013)
Train: 108 [1100/1251 ( 88%)]  Loss: 3.632 (3.68)  Time: 0.998s, 1025.86/s  (1.007s, 1017.08/s)  LR: 7.158e-04  Data: 0.010 (0.013)
Train: 108 [1150/1251 ( 92%)]  Loss: 4.018 (3.69)  Time: 0.999s, 1025.07/s  (1.007s, 1016.51/s)  LR: 7.158e-04  Data: 0.011 (0.013)
Train: 108 [1200/1251 ( 96%)]  Loss: 3.781 (3.70)  Time: 1.063s,  963.32/s  (1.007s, 1016.40/s)  LR: 7.158e-04  Data: 0.011 (0.012)
Train: 108 [1250/1251 (100%)]  Loss: 3.870 (3.70)  Time: 0.983s, 1041.21/s  (1.007s, 1016.68/s)  LR: 7.158e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.696 (1.696)  Loss:  0.8577 (0.8577)  Acc@1: 87.7930 (87.7930)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.245 (0.577)  Loss:  0.8895 (1.3689)  Acc@1: 84.6698 (72.5740)  Acc@5: 96.2264 (91.4100)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-103.pth.tar', 72.67000009765626)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-107.pth.tar', 72.59400006591797)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-108.pth.tar', 72.57399993408202)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-105.pth.tar', 72.56800004394532)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-106.pth.tar', 72.54399994384765)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-97.pth.tar', 72.47400012695313)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-102.pth.tar', 72.35400001708985)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-104.pth.tar', 72.2220000415039)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-94.pth.tar', 72.16800004394531)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-92.pth.tar', 72.15199997558594)

Train: 109 [   0/1251 (  0%)]  Loss: 3.729 (3.73)  Time: 2.534s,  404.13/s  (2.534s,  404.13/s)  LR: 7.111e-04  Data: 1.530 (1.530)
Train: 109 [  50/1251 (  4%)]  Loss: 3.720 (3.72)  Time: 0.995s, 1029.20/s  (1.033s,  990.94/s)  LR: 7.111e-04  Data: 0.012 (0.041)
Train: 109 [ 100/1251 (  8%)]  Loss: 3.945 (3.80)  Time: 1.001s, 1022.74/s  (1.021s, 1003.03/s)  LR: 7.111e-04  Data: 0.010 (0.026)
Train: 109 [ 150/1251 ( 12%)]  Loss: 3.196 (3.65)  Time: 0.998s, 1026.26/s  (1.026s,  998.36/s)  LR: 7.111e-04  Data: 0.014 (0.021)
Train: 109 [ 200/1251 ( 16%)]  Loss: 3.650 (3.65)  Time: 0.996s, 1028.41/s  (1.020s, 1004.02/s)  LR: 7.111e-04  Data: 0.011 (0.019)
Train: 109 [ 250/1251 ( 20%)]  Loss: 3.948 (3.70)  Time: 0.999s, 1024.96/s  (1.016s, 1007.92/s)  LR: 7.111e-04  Data: 0.013 (0.017)
Train: 109 [ 300/1251 ( 24%)]  Loss: 3.888 (3.73)  Time: 1.006s, 1017.39/s  (1.013s, 1010.38/s)  LR: 7.111e-04  Data: 0.011 (0.016)
Train: 109 [ 350/1251 ( 28%)]  Loss: 3.706 (3.72)  Time: 1.004s, 1019.98/s  (1.012s, 1012.25/s)  LR: 7.111e-04  Data: 0.011 (0.016)
Train: 109 [ 400/1251 ( 32%)]  Loss: 3.872 (3.74)  Time: 1.002s, 1021.66/s  (1.011s, 1013.07/s)  LR: 7.111e-04  Data: 0.012 (0.015)
Train: 109 [ 450/1251 ( 36%)]  Loss: 3.852 (3.75)  Time: 0.996s, 1027.64/s  (1.011s, 1012.51/s)  LR: 7.111e-04  Data: 0.010 (0.015)
Train: 109 [ 500/1251 ( 40%)]  Loss: 3.958 (3.77)  Time: 0.995s, 1029.14/s  (1.010s, 1013.53/s)  LR: 7.111e-04  Data: 0.012 (0.014)
Train: 109 [ 550/1251 ( 44%)]  Loss: 3.818 (3.77)  Time: 1.001s, 1023.37/s  (1.010s, 1013.55/s)  LR: 7.111e-04  Data: 0.011 (0.014)
Train: 109 [ 600/1251 ( 48%)]  Loss: 3.690 (3.77)  Time: 1.056s,  969.77/s  (1.011s, 1012.57/s)  LR: 7.111e-04  Data: 0.011 (0.014)
Train: 109 [ 650/1251 ( 52%)]  Loss: 3.632 (3.76)  Time: 0.999s, 1024.51/s  (1.010s, 1013.56/s)  LR: 7.111e-04  Data: 0.012 (0.014)
Train: 109 [ 700/1251 ( 56%)]  Loss: 3.426 (3.74)  Time: 0.996s, 1027.95/s  (1.010s, 1014.31/s)  LR: 7.111e-04  Data: 0.011 (0.013)
Train: 109 [ 750/1251 ( 60%)]  Loss: 3.779 (3.74)  Time: 1.032s,  992.09/s  (1.010s, 1013.93/s)  LR: 7.111e-04  Data: 0.010 (0.013)
Train: 109 [ 800/1251 ( 64%)]  Loss: 4.275 (3.77)  Time: 0.992s, 1031.83/s  (1.009s, 1014.56/s)  LR: 7.111e-04  Data: 0.011 (0.013)
Train: 109 [ 850/1251 ( 68%)]  Loss: 3.660 (3.76)  Time: 0.997s, 1027.48/s  (1.009s, 1015.33/s)  LR: 7.111e-04  Data: 0.012 (0.013)
Train: 109 [ 900/1251 ( 72%)]  Loss: 3.951 (3.77)  Time: 0.995s, 1029.51/s  (1.008s, 1015.96/s)  LR: 7.111e-04  Data: 0.011 (0.013)
Train: 109 [ 950/1251 ( 76%)]  Loss: 3.460 (3.76)  Time: 0.993s, 1030.87/s  (1.009s, 1015.33/s)  LR: 7.111e-04  Data: 0.011 (0.013)
Train: 109 [1000/1251 ( 80%)]  Loss: 3.354 (3.74)  Time: 0.994s, 1030.32/s  (1.009s, 1014.91/s)  LR: 7.111e-04  Data: 0.010 (0.013)
Train: 109 [1050/1251 ( 84%)]  Loss: 4.072 (3.75)  Time: 0.995s, 1029.45/s  (1.009s, 1015.37/s)  LR: 7.111e-04  Data: 0.011 (0.013)
Train: 109 [1100/1251 ( 88%)]  Loss: 3.723 (3.75)  Time: 0.992s, 1032.40/s  (1.008s, 1015.59/s)  LR: 7.111e-04  Data: 0.011 (0.013)
Train: 109 [1150/1251 ( 92%)]  Loss: 3.786 (3.75)  Time: 0.997s, 1027.37/s  (1.008s, 1015.88/s)  LR: 7.111e-04  Data: 0.011 (0.013)
Train: 109 [1200/1251 ( 96%)]  Loss: 3.369 (3.74)  Time: 0.995s, 1028.94/s  (1.008s, 1016.21/s)  LR: 7.111e-04  Data: 0.011 (0.012)
Train: 109 [1250/1251 (100%)]  Loss: 3.843 (3.74)  Time: 0.984s, 1041.04/s  (1.007s, 1016.51/s)  LR: 7.111e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.629 (1.629)  Loss:  0.8703 (0.8703)  Acc@1: 88.5742 (88.5742)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.245 (0.569)  Loss:  0.8927 (1.4781)  Acc@1: 83.7264 (72.4560)  Acc@5: 96.6981 (91.3560)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-103.pth.tar', 72.67000009765626)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-107.pth.tar', 72.59400006591797)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-108.pth.tar', 72.57399993408202)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-105.pth.tar', 72.56800004394532)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-106.pth.tar', 72.54399994384765)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-97.pth.tar', 72.47400012695313)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-109.pth.tar', 72.45600004150391)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-102.pth.tar', 72.35400001708985)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-104.pth.tar', 72.2220000415039)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-94.pth.tar', 72.16800004394531)

Train: 110 [   0/1251 (  0%)]  Loss: 3.697 (3.70)  Time: 2.506s,  408.66/s  (2.506s,  408.66/s)  LR: 7.063e-04  Data: 1.547 (1.547)
Train: 110 [  50/1251 (  4%)]  Loss: 3.654 (3.68)  Time: 1.032s,  992.47/s  (1.058s,  968.25/s)  LR: 7.063e-04  Data: 0.011 (0.042)
Train: 110 [ 100/1251 (  8%)]  Loss: 3.825 (3.73)  Time: 1.002s, 1022.26/s  (1.032s,  991.82/s)  LR: 7.063e-04  Data: 0.011 (0.026)
Train: 110 [ 150/1251 ( 12%)]  Loss: 3.695 (3.72)  Time: 0.997s, 1026.74/s  (1.025s,  998.73/s)  LR: 7.063e-04  Data: 0.010 (0.021)
Train: 110 [ 200/1251 ( 16%)]  Loss: 3.761 (3.73)  Time: 0.998s, 1025.96/s  (1.019s, 1005.28/s)  LR: 7.063e-04  Data: 0.012 (0.019)
Train: 110 [ 250/1251 ( 20%)]  Loss: 3.757 (3.73)  Time: 0.996s, 1028.02/s  (1.015s, 1008.99/s)  LR: 7.063e-04  Data: 0.012 (0.017)
Train: 110 [ 300/1251 ( 24%)]  Loss: 3.715 (3.73)  Time: 0.996s, 1027.84/s  (1.013s, 1011.18/s)  LR: 7.063e-04  Data: 0.011 (0.016)
Train: 110 [ 350/1251 ( 28%)]  Loss: 3.619 (3.72)  Time: 1.023s, 1001.21/s  (1.013s, 1010.61/s)  LR: 7.063e-04  Data: 0.011 (0.016)
Train: 110 [ 400/1251 ( 32%)]  Loss: 3.819 (3.73)  Time: 0.999s, 1025.51/s  (1.012s, 1011.81/s)  LR: 7.063e-04  Data: 0.012 (0.015)
Train: 110 [ 450/1251 ( 36%)]  Loss: 3.659 (3.72)  Time: 1.001s, 1022.77/s  (1.011s, 1013.13/s)  LR: 7.063e-04  Data: 0.011 (0.015)
Train: 110 [ 500/1251 ( 40%)]  Loss: 3.539 (3.70)  Time: 1.058s,  967.57/s  (1.010s, 1013.44/s)  LR: 7.063e-04  Data: 0.010 (0.014)
Train: 110 [ 550/1251 ( 44%)]  Loss: 3.860 (3.72)  Time: 0.993s, 1030.98/s  (1.010s, 1013.87/s)  LR: 7.063e-04  Data: 0.010 (0.014)
Train: 110 [ 600/1251 ( 48%)]  Loss: 3.551 (3.70)  Time: 1.000s, 1024.08/s  (1.010s, 1014.15/s)  LR: 7.063e-04  Data: 0.011 (0.014)
Train: 110 [ 650/1251 ( 52%)]  Loss: 3.852 (3.71)  Time: 0.995s, 1029.65/s  (1.010s, 1013.76/s)  LR: 7.063e-04  Data: 0.011 (0.013)
Train: 110 [ 700/1251 ( 56%)]  Loss: 3.554 (3.70)  Time: 0.997s, 1027.23/s  (1.009s, 1014.66/s)  LR: 7.063e-04  Data: 0.011 (0.013)
Train: 110 [ 750/1251 ( 60%)]  Loss: 3.713 (3.70)  Time: 0.995s, 1029.39/s  (1.009s, 1015.23/s)  LR: 7.063e-04  Data: 0.010 (0.013)
Train: 110 [ 800/1251 ( 64%)]  Loss: 3.947 (3.72)  Time: 0.991s, 1032.89/s  (1.008s, 1015.85/s)  LR: 7.063e-04  Data: 0.011 (0.013)
Train: 110 [ 850/1251 ( 68%)]  Loss: 3.846 (3.73)  Time: 1.042s,  983.15/s  (1.008s, 1015.61/s)  LR: 7.063e-04  Data: 0.011 (0.013)
Train: 110 [ 900/1251 ( 72%)]  Loss: 3.849 (3.73)  Time: 0.996s, 1028.56/s  (1.008s, 1015.88/s)  LR: 7.063e-04  Data: 0.011 (0.013)
Train: 110 [ 950/1251 ( 76%)]  Loss: 3.862 (3.74)  Time: 0.995s, 1028.71/s  (1.007s, 1016.47/s)  LR: 7.063e-04  Data: 0.011 (0.013)
Train: 110 [1000/1251 ( 80%)]  Loss: 3.428 (3.72)  Time: 0.997s, 1027.15/s  (1.007s, 1016.98/s)  LR: 7.063e-04  Data: 0.010 (0.013)
Train: 110 [1050/1251 ( 84%)]  Loss: 3.663 (3.72)  Time: 1.052s,  973.18/s  (1.007s, 1016.40/s)  LR: 7.063e-04  Data: 0.012 (0.013)
Train: 110 [1100/1251 ( 88%)]  Loss: 3.114 (3.69)  Time: 0.998s, 1026.41/s  (1.007s, 1016.44/s)  LR: 7.063e-04  Data: 0.012 (0.013)
Train: 110 [1150/1251 ( 92%)]  Loss: 3.903 (3.70)  Time: 0.995s, 1029.18/s  (1.007s, 1016.81/s)  LR: 7.063e-04  Data: 0.011 (0.012)
Train: 110 [1200/1251 ( 96%)]  Loss: 3.754 (3.71)  Time: 0.997s, 1027.19/s  (1.007s, 1017.05/s)  LR: 7.063e-04  Data: 0.011 (0.012)
Train: 110 [1250/1251 (100%)]  Loss: 3.528 (3.70)  Time: 0.984s, 1040.41/s  (1.006s, 1017.44/s)  LR: 7.063e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.595 (1.595)  Loss:  0.8293 (0.8293)  Acc@1: 87.4023 (87.4023)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.245 (0.569)  Loss:  0.9309 (1.4007)  Acc@1: 83.3726 (72.2920)  Acc@5: 95.5189 (91.3000)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-103.pth.tar', 72.67000009765626)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-107.pth.tar', 72.59400006591797)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-108.pth.tar', 72.57399993408202)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-105.pth.tar', 72.56800004394532)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-106.pth.tar', 72.54399994384765)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-97.pth.tar', 72.47400012695313)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-109.pth.tar', 72.45600004150391)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-102.pth.tar', 72.35400001708985)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-110.pth.tar', 72.29200001708985)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-104.pth.tar', 72.2220000415039)

Train: 111 [   0/1251 (  0%)]  Loss: 3.363 (3.36)  Time: 4.312s,  237.46/s  (4.312s,  237.46/s)  LR: 7.016e-04  Data: 3.112 (3.112)
Train: 111 [  50/1251 (  4%)]  Loss: 3.638 (3.50)  Time: 1.047s,  977.98/s  (1.077s,  950.73/s)  LR: 7.016e-04  Data: 0.011 (0.072)
Train: 111 [ 100/1251 (  8%)]  Loss: 3.604 (3.53)  Time: 0.995s, 1029.65/s  (1.039s,  985.93/s)  LR: 7.016e-04  Data: 0.011 (0.042)
Train: 111 [ 150/1251 ( 12%)]  Loss: 4.046 (3.66)  Time: 1.002s, 1022.14/s  (1.026s,  997.63/s)  LR: 7.016e-04  Data: 0.014 (0.032)
Train: 111 [ 200/1251 ( 16%)]  Loss: 3.179 (3.57)  Time: 0.998s, 1025.76/s  (1.024s, 1000.00/s)  LR: 7.016e-04  Data: 0.011 (0.027)
Train: 111 [ 250/1251 ( 20%)]  Loss: 3.916 (3.62)  Time: 0.991s, 1033.67/s  (1.019s, 1004.62/s)  LR: 7.016e-04  Data: 0.010 (0.024)
Train: 111 [ 300/1251 ( 24%)]  Loss: 3.800 (3.65)  Time: 1.008s, 1016.36/s  (1.016s, 1007.63/s)  LR: 7.016e-04  Data: 0.011 (0.021)
Train: 111 [ 350/1251 ( 28%)]  Loss: 4.123 (3.71)  Time: 0.995s, 1028.72/s  (1.014s, 1009.53/s)  LR: 7.016e-04  Data: 0.011 (0.020)
Train: 111 [ 400/1251 ( 32%)]  Loss: 3.298 (3.66)  Time: 1.008s, 1016.01/s  (1.012s, 1011.56/s)  LR: 7.016e-04  Data: 0.011 (0.019)
Train: 111 [ 450/1251 ( 36%)]  Loss: 3.874 (3.68)  Time: 0.998s, 1025.65/s  (1.011s, 1012.66/s)  LR: 7.016e-04  Data: 0.011 (0.018)
Train: 111 [ 500/1251 ( 40%)]  Loss: 3.129 (3.63)  Time: 0.996s, 1028.12/s  (1.010s, 1013.61/s)  LR: 7.016e-04  Data: 0.012 (0.017)
Train: 111 [ 550/1251 ( 44%)]  Loss: 3.625 (3.63)  Time: 0.993s, 1030.71/s  (1.009s, 1014.47/s)  LR: 7.016e-04  Data: 0.011 (0.017)
Train: 111 [ 600/1251 ( 48%)]  Loss: 3.544 (3.63)  Time: 0.995s, 1029.54/s  (1.010s, 1013.94/s)  LR: 7.016e-04  Data: 0.011 (0.016)
Train: 111 [ 650/1251 ( 52%)]  Loss: 3.863 (3.64)  Time: 0.995s, 1029.01/s  (1.009s, 1014.74/s)  LR: 7.016e-04  Data: 0.011 (0.016)
Train: 111 [ 700/1251 ( 56%)]  Loss: 3.874 (3.66)  Time: 0.994s, 1029.72/s  (1.009s, 1015.11/s)  LR: 7.016e-04  Data: 0.011 (0.016)
Train: 111 [ 750/1251 ( 60%)]  Loss: 3.797 (3.67)  Time: 0.996s, 1028.20/s  (1.008s, 1015.82/s)  LR: 7.016e-04  Data: 0.011 (0.015)
Train: 111 [ 800/1251 ( 64%)]  Loss: 3.476 (3.66)  Time: 0.996s, 1027.76/s  (1.007s, 1016.46/s)  LR: 7.016e-04  Data: 0.012 (0.015)
Train: 111 [ 850/1251 ( 68%)]  Loss: 3.814 (3.66)  Time: 0.998s, 1026.29/s  (1.007s, 1016.96/s)  LR: 7.016e-04  Data: 0.012 (0.015)
Train: 111 [ 900/1251 ( 72%)]  Loss: 3.773 (3.67)  Time: 0.994s, 1029.84/s  (1.007s, 1017.27/s)  LR: 7.016e-04  Data: 0.012 (0.015)
Train: 111 [ 950/1251 ( 76%)]  Loss: 3.777 (3.68)  Time: 1.029s,  995.56/s  (1.007s, 1017.27/s)  LR: 7.016e-04  Data: 0.012 (0.014)
Train: 111 [1000/1251 ( 80%)]  Loss: 3.763 (3.68)  Time: 0.996s, 1028.38/s  (1.006s, 1017.57/s)  LR: 7.016e-04  Data: 0.011 (0.014)
Train: 111 [1050/1251 ( 84%)]  Loss: 3.505 (3.67)  Time: 0.996s, 1028.23/s  (1.006s, 1017.87/s)  LR: 7.016e-04  Data: 0.012 (0.014)
Train: 111 [1100/1251 ( 88%)]  Loss: 3.950 (3.68)  Time: 1.018s, 1006.25/s  (1.006s, 1017.53/s)  LR: 7.016e-04  Data: 0.010 (0.014)
Train: 111 [1150/1251 ( 92%)]  Loss: 3.803 (3.69)  Time: 0.999s, 1024.80/s  (1.006s, 1017.82/s)  LR: 7.016e-04  Data: 0.011 (0.014)
Train: 111 [1200/1251 ( 96%)]  Loss: 4.024 (3.70)  Time: 1.000s, 1023.96/s  (1.006s, 1017.87/s)  LR: 7.016e-04  Data: 0.011 (0.014)
Train: 111 [1250/1251 (100%)]  Loss: 3.962 (3.71)  Time: 1.026s,  997.70/s  (1.006s, 1017.78/s)  LR: 7.016e-04  Data: 0.000 (0.014)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.702 (1.702)  Loss:  0.8507 (0.8507)  Acc@1: 87.7930 (87.7930)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.245 (0.573)  Loss:  0.8953 (1.3737)  Acc@1: 83.0189 (72.7960)  Acc@5: 96.5802 (91.6060)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-111.pth.tar', 72.79600012207031)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-103.pth.tar', 72.67000009765626)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-107.pth.tar', 72.59400006591797)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-108.pth.tar', 72.57399993408202)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-105.pth.tar', 72.56800004394532)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-106.pth.tar', 72.54399994384765)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-97.pth.tar', 72.47400012695313)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-109.pth.tar', 72.45600004150391)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-102.pth.tar', 72.35400001708985)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-110.pth.tar', 72.29200001708985)

Train: 112 [   0/1251 (  0%)]  Loss: 3.814 (3.81)  Time: 2.511s,  407.73/s  (2.511s,  407.73/s)  LR: 6.968e-04  Data: 1.548 (1.548)
Train: 112 [  50/1251 (  4%)]  Loss: 3.416 (3.62)  Time: 0.997s, 1027.56/s  (1.036s,  988.87/s)  LR: 6.968e-04  Data: 0.012 (0.042)
Train: 112 [ 100/1251 (  8%)]  Loss: 3.762 (3.66)  Time: 0.998s, 1026.04/s  (1.032s,  991.96/s)  LR: 6.968e-04  Data: 0.012 (0.027)
Train: 112 [ 150/1251 ( 12%)]  Loss: 3.538 (3.63)  Time: 0.996s, 1028.08/s  (1.022s, 1002.13/s)  LR: 6.968e-04  Data: 0.011 (0.022)
Train: 112 [ 200/1251 ( 16%)]  Loss: 3.846 (3.68)  Time: 1.004s, 1019.52/s  (1.019s, 1004.48/s)  LR: 6.968e-04  Data: 0.012 (0.019)
Train: 112 [ 250/1251 ( 20%)]  Loss: 3.698 (3.68)  Time: 0.998s, 1026.19/s  (1.016s, 1007.87/s)  LR: 6.968e-04  Data: 0.012 (0.017)
Train: 112 [ 300/1251 ( 24%)]  Loss: 3.405 (3.64)  Time: 0.996s, 1027.63/s  (1.014s, 1010.20/s)  LR: 6.968e-04  Data: 0.011 (0.016)
Train: 112 [ 350/1251 ( 28%)]  Loss: 3.806 (3.66)  Time: 1.000s, 1023.76/s  (1.014s, 1009.45/s)  LR: 6.968e-04  Data: 0.011 (0.016)
Train: 112 [ 400/1251 ( 32%)]  Loss: 3.681 (3.66)  Time: 1.005s, 1019.12/s  (1.013s, 1010.95/s)  LR: 6.968e-04  Data: 0.010 (0.015)
Train: 112 [ 450/1251 ( 36%)]  Loss: 3.496 (3.65)  Time: 1.062s,  964.38/s  (1.012s, 1011.77/s)  LR: 6.968e-04  Data: 0.012 (0.015)
Train: 112 [ 500/1251 ( 40%)]  Loss: 3.712 (3.65)  Time: 1.004s, 1020.42/s  (1.012s, 1012.31/s)  LR: 6.968e-04  Data: 0.012 (0.014)
Train: 112 [ 550/1251 ( 44%)]  Loss: 3.686 (3.65)  Time: 1.061s,  964.94/s  (1.012s, 1011.68/s)  LR: 6.968e-04  Data: 0.012 (0.014)
Train: 112 [ 600/1251 ( 48%)]  Loss: 3.843 (3.67)  Time: 0.993s, 1030.89/s  (1.012s, 1011.84/s)  LR: 6.968e-04  Data: 0.011 (0.014)
Train: 112 [ 650/1251 ( 52%)]  Loss: 3.661 (3.67)  Time: 0.993s, 1031.72/s  (1.011s, 1012.81/s)  LR: 6.968e-04  Data: 0.010 (0.014)
Train: 112 [ 700/1251 ( 56%)]  Loss: 3.559 (3.66)  Time: 1.004s, 1019.56/s  (1.010s, 1013.59/s)  LR: 6.968e-04  Data: 0.010 (0.013)
Train: 112 [ 750/1251 ( 60%)]  Loss: 3.710 (3.66)  Time: 0.995s, 1028.77/s  (1.009s, 1014.49/s)  LR: 6.968e-04  Data: 0.011 (0.013)
Train: 112 [ 800/1251 ( 64%)]  Loss: 3.801 (3.67)  Time: 1.066s,  960.69/s  (1.011s, 1013.07/s)  LR: 6.968e-04  Data: 0.014 (0.013)
Train: 112 [ 850/1251 ( 68%)]  Loss: 3.620 (3.67)  Time: 1.002s, 1021.82/s  (1.012s, 1012.26/s)  LR: 6.968e-04  Data: 0.012 (0.013)
Train: 112 [ 900/1251 ( 72%)]  Loss: 3.766 (3.67)  Time: 0.995s, 1029.57/s  (1.011s, 1012.75/s)  LR: 6.968e-04  Data: 0.011 (0.013)
Train: 112 [ 950/1251 ( 76%)]  Loss: 3.884 (3.69)  Time: 0.996s, 1028.50/s  (1.010s, 1013.42/s)  LR: 6.968e-04  Data: 0.012 (0.013)
Train: 112 [1000/1251 ( 80%)]  Loss: 3.729 (3.69)  Time: 1.001s, 1023.29/s  (1.010s, 1014.12/s)  LR: 6.968e-04  Data: 0.012 (0.013)
Train: 112 [1050/1251 ( 84%)]  Loss: 3.586 (3.68)  Time: 0.997s, 1027.40/s  (1.009s, 1014.61/s)  LR: 6.968e-04  Data: 0.010 (0.013)
Train: 112 [1100/1251 ( 88%)]  Loss: 3.806 (3.69)  Time: 0.996s, 1028.59/s  (1.009s, 1015.03/s)  LR: 6.968e-04  Data: 0.011 (0.013)
Train: 112 [1150/1251 ( 92%)]  Loss: 3.895 (3.70)  Time: 1.005s, 1019.17/s  (1.008s, 1015.38/s)  LR: 6.968e-04  Data: 0.011 (0.013)
Train: 112 [1200/1251 ( 96%)]  Loss: 3.419 (3.69)  Time: 0.995s, 1028.86/s  (1.009s, 1015.12/s)  LR: 6.968e-04  Data: 0.010 (0.013)
Train: 112 [1250/1251 (100%)]  Loss: 3.515 (3.68)  Time: 0.983s, 1041.66/s  (1.009s, 1015.16/s)  LR: 6.968e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.631 (1.631)  Loss:  0.7921 (0.7921)  Acc@1: 88.3789 (88.3789)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.245 (0.570)  Loss:  0.9237 (1.4355)  Acc@1: 84.6698 (72.4420)  Acc@5: 96.5802 (91.4200)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-111.pth.tar', 72.79600012207031)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-103.pth.tar', 72.67000009765626)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-107.pth.tar', 72.59400006591797)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-108.pth.tar', 72.57399993408202)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-105.pth.tar', 72.56800004394532)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-106.pth.tar', 72.54399994384765)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-97.pth.tar', 72.47400012695313)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-109.pth.tar', 72.45600004150391)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-112.pth.tar', 72.44199993408203)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-102.pth.tar', 72.35400001708985)

Train: 113 [   0/1251 (  0%)]  Loss: 3.949 (3.95)  Time: 2.593s,  394.98/s  (2.593s,  394.98/s)  LR: 6.920e-04  Data: 1.632 (1.632)
Train: 113 [  50/1251 (  4%)]  Loss: 3.711 (3.83)  Time: 0.995s, 1029.65/s  (1.032s,  992.11/s)  LR: 6.920e-04  Data: 0.011 (0.043)
Train: 113 [ 100/1251 (  8%)]  Loss: 3.897 (3.85)  Time: 0.995s, 1029.26/s  (1.020s, 1003.58/s)  LR: 6.920e-04  Data: 0.011 (0.027)
Train: 113 [ 150/1251 ( 12%)]  Loss: 3.950 (3.88)  Time: 0.994s, 1030.19/s  (1.013s, 1010.90/s)  LR: 6.920e-04  Data: 0.011 (0.022)
Train: 113 [ 200/1251 ( 16%)]  Loss: 3.523 (3.81)  Time: 0.994s, 1030.02/s  (1.012s, 1011.64/s)  LR: 6.920e-04  Data: 0.011 (0.019)
Train: 113 [ 250/1251 ( 20%)]  Loss: 3.760 (3.80)  Time: 0.998s, 1025.84/s  (1.011s, 1013.28/s)  LR: 6.920e-04  Data: 0.010 (0.018)
Train: 113 [ 300/1251 ( 24%)]  Loss: 3.412 (3.74)  Time: 1.058s,  968.06/s  (1.015s, 1008.49/s)  LR: 6.920e-04  Data: 0.011 (0.017)
Train: 113 [ 350/1251 ( 28%)]  Loss: 3.478 (3.71)  Time: 0.994s, 1029.77/s  (1.013s, 1010.61/s)  LR: 6.920e-04  Data: 0.010 (0.016)
Train: 113 [ 400/1251 ( 32%)]  Loss: 3.788 (3.72)  Time: 1.035s,  989.43/s  (1.012s, 1011.67/s)  LR: 6.920e-04  Data: 0.012 (0.015)
Train: 113 [ 450/1251 ( 36%)]  Loss: 3.608 (3.71)  Time: 0.990s, 1033.96/s  (1.011s, 1013.08/s)  LR: 6.920e-04  Data: 0.012 (0.015)
Train: 113 [ 500/1251 ( 40%)]  Loss: 3.495 (3.69)  Time: 1.037s,  987.47/s  (1.010s, 1013.59/s)  LR: 6.920e-04  Data: 0.012 (0.014)
Train: 113 [ 550/1251 ( 44%)]  Loss: 3.929 (3.71)  Time: 0.995s, 1028.97/s  (1.010s, 1013.51/s)  LR: 6.920e-04  Data: 0.012 (0.014)
Train: 113 [ 600/1251 ( 48%)]  Loss: 3.602 (3.70)  Time: 0.996s, 1027.79/s  (1.010s, 1013.48/s)  LR: 6.920e-04  Data: 0.011 (0.014)
Train: 113 [ 650/1251 ( 52%)]  Loss: 3.736 (3.70)  Time: 0.996s, 1028.19/s  (1.010s, 1014.31/s)  LR: 6.920e-04  Data: 0.012 (0.014)
Train: 113 [ 700/1251 ( 56%)]  Loss: 3.600 (3.70)  Time: 1.001s, 1022.66/s  (1.010s, 1014.26/s)  LR: 6.920e-04  Data: 0.012 (0.013)
Train: 113 [ 750/1251 ( 60%)]  Loss: 3.714 (3.70)  Time: 0.996s, 1027.92/s  (1.009s, 1014.95/s)  LR: 6.920e-04  Data: 0.012 (0.013)
Train: 113 [ 800/1251 ( 64%)]  Loss: 3.557 (3.69)  Time: 0.996s, 1028.32/s  (1.008s, 1015.46/s)  LR: 6.920e-04  Data: 0.011 (0.013)
Train: 113 [ 850/1251 ( 68%)]  Loss: 3.671 (3.69)  Time: 1.063s,  963.49/s  (1.009s, 1014.79/s)  LR: 6.920e-04  Data: 0.011 (0.013)
Train: 113 [ 900/1251 ( 72%)]  Loss: 3.733 (3.69)  Time: 1.044s,  980.63/s  (1.009s, 1014.54/s)  LR: 6.920e-04  Data: 0.010 (0.013)
Train: 113 [ 950/1251 ( 76%)]  Loss: 3.245 (3.67)  Time: 0.994s, 1030.19/s  (1.009s, 1015.03/s)  LR: 6.920e-04  Data: 0.011 (0.013)
Train: 113 [1000/1251 ( 80%)]  Loss: 3.840 (3.68)  Time: 1.006s, 1018.28/s  (1.008s, 1015.59/s)  LR: 6.920e-04  Data: 0.011 (0.013)
Train: 113 [1050/1251 ( 84%)]  Loss: 3.441 (3.67)  Time: 1.000s, 1023.91/s  (1.008s, 1016.15/s)  LR: 6.920e-04  Data: 0.012 (0.013)
Train: 113 [1100/1251 ( 88%)]  Loss: 3.507 (3.66)  Time: 1.063s,  963.52/s  (1.008s, 1015.86/s)  LR: 6.920e-04  Data: 0.011 (0.013)
Train: 113 [1150/1251 ( 92%)]  Loss: 3.812 (3.66)  Time: 0.995s, 1028.64/s  (1.009s, 1015.35/s)  LR: 6.920e-04  Data: 0.012 (0.013)
Train: 113 [1200/1251 ( 96%)]  Loss: 3.562 (3.66)  Time: 1.025s,  999.39/s  (1.009s, 1014.57/s)  LR: 6.920e-04  Data: 0.011 (0.012)
Train: 113 [1250/1251 (100%)]  Loss: 3.395 (3.65)  Time: 1.021s, 1003.15/s  (1.009s, 1014.40/s)  LR: 6.920e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.727 (1.727)  Loss:  0.8361 (0.8361)  Acc@1: 88.0859 (88.0859)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.245 (0.567)  Loss:  1.0434 (1.4167)  Acc@1: 81.3679 (73.2920)  Acc@5: 96.3443 (91.7620)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-113.pth.tar', 73.291999921875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-111.pth.tar', 72.79600012207031)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-103.pth.tar', 72.67000009765626)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-107.pth.tar', 72.59400006591797)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-108.pth.tar', 72.57399993408202)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-105.pth.tar', 72.56800004394532)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-106.pth.tar', 72.54399994384765)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-97.pth.tar', 72.47400012695313)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-109.pth.tar', 72.45600004150391)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-112.pth.tar', 72.44199993408203)

Train: 114 [   0/1251 (  0%)]  Loss: 3.439 (3.44)  Time: 2.556s,  400.67/s  (2.556s,  400.67/s)  LR: 6.872e-04  Data: 1.596 (1.596)
Train: 114 [  50/1251 (  4%)]  Loss: 3.576 (3.51)  Time: 1.042s,  982.67/s  (1.042s,  982.28/s)  LR: 6.872e-04  Data: 0.011 (0.042)
Train: 114 [ 100/1251 (  8%)]  Loss: 3.430 (3.48)  Time: 0.998s, 1026.07/s  (1.022s, 1002.25/s)  LR: 6.872e-04  Data: 0.012 (0.027)
Train: 114 [ 150/1251 ( 12%)]  Loss: 3.759 (3.55)  Time: 1.000s, 1024.07/s  (1.014s, 1009.77/s)  LR: 6.872e-04  Data: 0.012 (0.022)
Train: 114 [ 200/1251 ( 16%)]  Loss: 3.745 (3.59)  Time: 0.996s, 1028.52/s  (1.014s, 1009.83/s)  LR: 6.872e-04  Data: 0.011 (0.019)
Train: 114 [ 250/1251 ( 20%)]  Loss: 3.519 (3.58)  Time: 1.055s,  970.28/s  (1.014s, 1010.13/s)  LR: 6.872e-04  Data: 0.011 (0.017)
Train: 114 [ 300/1251 ( 24%)]  Loss: 3.667 (3.59)  Time: 0.997s, 1027.56/s  (1.012s, 1012.32/s)  LR: 6.872e-04  Data: 0.011 (0.016)
Train: 114 [ 350/1251 ( 28%)]  Loss: 3.418 (3.57)  Time: 1.045s,  980.22/s  (1.011s, 1012.85/s)  LR: 6.872e-04  Data: 0.011 (0.016)
Train: 114 [ 400/1251 ( 32%)]  Loss: 3.662 (3.58)  Time: 1.001s, 1023.34/s  (1.010s, 1013.44/s)  LR: 6.872e-04  Data: 0.011 (0.015)
Train: 114 [ 450/1251 ( 36%)]  Loss: 3.790 (3.60)  Time: 0.996s, 1028.14/s  (1.010s, 1013.75/s)  LR: 6.872e-04  Data: 0.011 (0.015)
Train: 114 [ 500/1251 ( 40%)]  Loss: 3.519 (3.59)  Time: 0.993s, 1030.90/s  (1.009s, 1014.54/s)  LR: 6.872e-04  Data: 0.011 (0.014)
Train: 114 [ 550/1251 ( 44%)]  Loss: 3.665 (3.60)  Time: 0.996s, 1028.35/s  (1.008s, 1015.56/s)  LR: 6.872e-04  Data: 0.012 (0.014)
Train: 114 [ 600/1251 ( 48%)]  Loss: 3.284 (3.57)  Time: 0.996s, 1028.30/s  (1.008s, 1016.37/s)  LR: 6.872e-04  Data: 0.012 (0.014)
Train: 114 [ 650/1251 ( 52%)]  Loss: 4.002 (3.61)  Time: 0.993s, 1031.31/s  (1.008s, 1016.17/s)  LR: 6.872e-04  Data: 0.010 (0.014)
Train: 114 [ 700/1251 ( 56%)]  Loss: 3.802 (3.62)  Time: 0.996s, 1028.36/s  (1.007s, 1016.76/s)  LR: 6.872e-04  Data: 0.011 (0.013)
Train: 114 [ 750/1251 ( 60%)]  Loss: 3.966 (3.64)  Time: 0.999s, 1025.31/s  (1.007s, 1017.02/s)  LR: 6.872e-04  Data: 0.011 (0.013)
Train: 114 [ 800/1251 ( 64%)]  Loss: 4.052 (3.66)  Time: 1.038s,  986.67/s  (1.007s, 1017.23/s)  LR: 6.872e-04  Data: 0.012 (0.013)
Train: 114 [ 850/1251 ( 68%)]  Loss: 3.369 (3.65)  Time: 0.996s, 1027.67/s  (1.008s, 1016.14/s)  LR: 6.872e-04  Data: 0.011 (0.013)
Train: 114 [ 900/1251 ( 72%)]  Loss: 4.051 (3.67)  Time: 0.992s, 1032.15/s  (1.009s, 1015.10/s)  LR: 6.872e-04  Data: 0.010 (0.013)
Train: 114 [ 950/1251 ( 76%)]  Loss: 3.701 (3.67)  Time: 0.990s, 1034.02/s  (1.009s, 1015.24/s)  LR: 6.872e-04  Data: 0.010 (0.013)
Train: 114 [1000/1251 ( 80%)]  Loss: 3.724 (3.67)  Time: 0.997s, 1027.56/s  (1.008s, 1015.64/s)  LR: 6.872e-04  Data: 0.010 (0.013)
Train: 114 [1050/1251 ( 84%)]  Loss: 3.947 (3.69)  Time: 0.998s, 1025.96/s  (1.008s, 1015.72/s)  LR: 6.872e-04  Data: 0.011 (0.013)
Train: 114 [1100/1251 ( 88%)]  Loss: 3.733 (3.69)  Time: 0.995s, 1029.34/s  (1.008s, 1016.16/s)  LR: 6.872e-04  Data: 0.011 (0.013)
Train: 114 [1150/1251 ( 92%)]  Loss: 3.607 (3.68)  Time: 1.001s, 1022.69/s  (1.007s, 1016.57/s)  LR: 6.872e-04  Data: 0.010 (0.013)
Train: 114 [1200/1251 ( 96%)]  Loss: 3.825 (3.69)  Time: 0.994s, 1029.88/s  (1.007s, 1016.74/s)  LR: 6.872e-04  Data: 0.011 (0.012)
Train: 114 [1250/1251 (100%)]  Loss: 3.467 (3.68)  Time: 0.983s, 1041.61/s  (1.007s, 1016.98/s)  LR: 6.872e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.647 (1.647)  Loss:  0.8347 (0.8347)  Acc@1: 89.1602 (89.1602)  Acc@5: 96.1914 (96.1914)
Test: [  48/48]  Time: 0.245 (0.577)  Loss:  0.9028 (1.4179)  Acc@1: 83.6085 (73.1120)  Acc@5: 95.9906 (91.6060)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-113.pth.tar', 73.291999921875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-114.pth.tar', 73.11199999023438)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-111.pth.tar', 72.79600012207031)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-103.pth.tar', 72.67000009765626)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-107.pth.tar', 72.59400006591797)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-108.pth.tar', 72.57399993408202)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-105.pth.tar', 72.56800004394532)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-106.pth.tar', 72.54399994384765)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-97.pth.tar', 72.47400012695313)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-109.pth.tar', 72.45600004150391)

Train: 115 [   0/1251 (  0%)]  Loss: 3.446 (3.45)  Time: 2.484s,  412.24/s  (2.484s,  412.24/s)  LR: 6.824e-04  Data: 1.522 (1.522)
Train: 115 [  50/1251 (  4%)]  Loss: 3.994 (3.72)  Time: 1.028s,  996.46/s  (1.042s,  982.94/s)  LR: 6.824e-04  Data: 0.011 (0.041)
Train: 115 [ 100/1251 (  8%)]  Loss: 3.532 (3.66)  Time: 0.997s, 1027.03/s  (1.020s, 1003.69/s)  LR: 6.824e-04  Data: 0.012 (0.026)
Train: 115 [ 150/1251 ( 12%)]  Loss: 3.541 (3.63)  Time: 0.997s, 1026.98/s  (1.014s, 1010.09/s)  LR: 6.824e-04  Data: 0.012 (0.021)
Train: 115 [ 200/1251 ( 16%)]  Loss: 3.650 (3.63)  Time: 1.063s,  962.91/s  (1.017s, 1006.87/s)  LR: 6.824e-04  Data: 0.011 (0.019)
Train: 115 [ 250/1251 ( 20%)]  Loss: 3.408 (3.60)  Time: 1.007s, 1016.83/s  (1.017s, 1006.61/s)  LR: 6.824e-04  Data: 0.011 (0.017)
Train: 115 [ 300/1251 ( 24%)]  Loss: 3.570 (3.59)  Time: 0.995s, 1029.53/s  (1.015s, 1008.48/s)  LR: 6.824e-04  Data: 0.010 (0.016)
Train: 115 [ 350/1251 ( 28%)]  Loss: 3.762 (3.61)  Time: 0.995s, 1029.29/s  (1.014s, 1009.96/s)  LR: 6.824e-04  Data: 0.011 (0.015)
Train: 115 [ 400/1251 ( 32%)]  Loss: 3.253 (3.57)  Time: 0.994s, 1030.48/s  (1.012s, 1011.94/s)  LR: 6.824e-04  Data: 0.011 (0.015)
Train: 115 [ 450/1251 ( 36%)]  Loss: 3.727 (3.59)  Time: 1.004s, 1019.53/s  (1.011s, 1013.29/s)  LR: 6.824e-04  Data: 0.011 (0.014)
Train: 115 [ 500/1251 ( 40%)]  Loss: 3.676 (3.60)  Time: 0.994s, 1030.29/s  (1.010s, 1014.02/s)  LR: 6.824e-04  Data: 0.011 (0.014)
Train: 115 [ 550/1251 ( 44%)]  Loss: 3.603 (3.60)  Time: 0.996s, 1027.78/s  (1.010s, 1014.04/s)  LR: 6.824e-04  Data: 0.012 (0.014)
Train: 115 [ 600/1251 ( 48%)]  Loss: 3.803 (3.61)  Time: 0.996s, 1028.25/s  (1.009s, 1015.18/s)  LR: 6.824e-04  Data: 0.011 (0.014)
Train: 115 [ 650/1251 ( 52%)]  Loss: 3.709 (3.62)  Time: 0.996s, 1028.14/s  (1.008s, 1015.98/s)  LR: 6.824e-04  Data: 0.011 (0.013)
Train: 115 [ 700/1251 ( 56%)]  Loss: 3.626 (3.62)  Time: 0.997s, 1027.41/s  (1.007s, 1016.43/s)  LR: 6.824e-04  Data: 0.011 (0.013)
Train: 115 [ 750/1251 ( 60%)]  Loss: 3.739 (3.63)  Time: 1.002s, 1022.02/s  (1.007s, 1016.97/s)  LR: 6.824e-04  Data: 0.012 (0.013)
Train: 115 [ 800/1251 ( 64%)]  Loss: 3.706 (3.63)  Time: 0.995s, 1029.31/s  (1.006s, 1017.51/s)  LR: 6.824e-04  Data: 0.010 (0.013)
Train: 115 [ 850/1251 ( 68%)]  Loss: 3.651 (3.63)  Time: 0.994s, 1030.26/s  (1.008s, 1016.03/s)  LR: 6.824e-04  Data: 0.011 (0.013)
Train: 115 [ 900/1251 ( 72%)]  Loss: 3.412 (3.62)  Time: 0.995s, 1029.38/s  (1.007s, 1016.48/s)  LR: 6.824e-04  Data: 0.011 (0.013)
Train: 115 [ 950/1251 ( 76%)]  Loss: 3.724 (3.63)  Time: 0.996s, 1027.65/s  (1.007s, 1016.83/s)  LR: 6.824e-04  Data: 0.010 (0.013)
Train: 115 [1000/1251 ( 80%)]  Loss: 3.773 (3.63)  Time: 0.997s, 1027.59/s  (1.007s, 1017.07/s)  LR: 6.824e-04  Data: 0.011 (0.013)
Train: 115 [1050/1251 ( 84%)]  Loss: 3.383 (3.62)  Time: 1.037s,  987.21/s  (1.007s, 1016.85/s)  LR: 6.824e-04  Data: 0.011 (0.013)
Train: 115 [1100/1251 ( 88%)]  Loss: 3.639 (3.62)  Time: 0.994s, 1030.56/s  (1.007s, 1016.83/s)  LR: 6.824e-04  Data: 0.011 (0.012)
Train: 115 [1150/1251 ( 92%)]  Loss: 3.391 (3.61)  Time: 0.997s, 1026.60/s  (1.007s, 1017.18/s)  LR: 6.824e-04  Data: 0.011 (0.012)
Train: 115 [1200/1251 ( 96%)]  Loss: 3.920 (3.63)  Time: 0.997s, 1027.53/s  (1.006s, 1017.43/s)  LR: 6.824e-04  Data: 0.011 (0.012)
Train: 115 [1250/1251 (100%)]  Loss: 3.741 (3.63)  Time: 0.990s, 1034.06/s  (1.006s, 1017.75/s)  LR: 6.824e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.646 (1.646)  Loss:  0.8227 (0.8227)  Acc@1: 88.2812 (88.2812)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.245 (0.568)  Loss:  0.9377 (1.4332)  Acc@1: 83.1368 (72.6920)  Acc@5: 95.6368 (91.4600)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-113.pth.tar', 73.291999921875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-114.pth.tar', 73.11199999023438)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-111.pth.tar', 72.79600012207031)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-115.pth.tar', 72.69200004394531)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-103.pth.tar', 72.67000009765626)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-107.pth.tar', 72.59400006591797)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-108.pth.tar', 72.57399993408202)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-105.pth.tar', 72.56800004394532)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-106.pth.tar', 72.54399994384765)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-97.pth.tar', 72.47400012695313)

Train: 116 [   0/1251 (  0%)]  Loss: 3.767 (3.77)  Time: 2.473s,  414.04/s  (2.473s,  414.04/s)  LR: 6.775e-04  Data: 1.502 (1.502)
Train: 116 [  50/1251 (  4%)]  Loss: 3.964 (3.87)  Time: 1.012s, 1012.13/s  (1.058s,  967.72/s)  LR: 6.775e-04  Data: 0.012 (0.040)
Train: 116 [ 100/1251 (  8%)]  Loss: 3.644 (3.79)  Time: 0.992s, 1032.14/s  (1.032s,  992.08/s)  LR: 6.775e-04  Data: 0.012 (0.026)
Train: 116 [ 150/1251 ( 12%)]  Loss: 3.617 (3.75)  Time: 1.039s,  985.73/s  (1.025s,  998.77/s)  LR: 6.775e-04  Data: 0.011 (0.021)
Train: 116 [ 200/1251 ( 16%)]  Loss: 3.870 (3.77)  Time: 0.994s, 1030.24/s  (1.022s, 1001.92/s)  LR: 6.775e-04  Data: 0.011 (0.018)
Train: 116 [ 250/1251 ( 20%)]  Loss: 3.880 (3.79)  Time: 0.995s, 1029.44/s  (1.018s, 1006.32/s)  LR: 6.775e-04  Data: 0.011 (0.017)
Train: 116 [ 300/1251 ( 24%)]  Loss: 3.831 (3.80)  Time: 1.002s, 1021.62/s  (1.016s, 1008.29/s)  LR: 6.775e-04  Data: 0.011 (0.016)
Train: 116 [ 350/1251 ( 28%)]  Loss: 4.088 (3.83)  Time: 1.051s,  974.04/s  (1.014s, 1010.31/s)  LR: 6.775e-04  Data: 0.010 (0.015)
Train: 116 [ 400/1251 ( 32%)]  Loss: 3.836 (3.83)  Time: 1.032s,  992.29/s  (1.013s, 1010.54/s)  LR: 6.775e-04  Data: 0.012 (0.015)
Train: 116 [ 450/1251 ( 36%)]  Loss: 3.580 (3.81)  Time: 0.998s, 1026.22/s  (1.012s, 1011.84/s)  LR: 6.775e-04  Data: 0.012 (0.014)
Train: 116 [ 500/1251 ( 40%)]  Loss: 3.832 (3.81)  Time: 0.993s, 1031.39/s  (1.011s, 1013.11/s)  LR: 6.775e-04  Data: 0.010 (0.014)
Train: 116 [ 550/1251 ( 44%)]  Loss: 3.708 (3.80)  Time: 1.021s, 1003.40/s  (1.010s, 1014.09/s)  LR: 6.775e-04  Data: 0.012 (0.014)
Train: 116 [ 600/1251 ( 48%)]  Loss: 3.815 (3.80)  Time: 0.995s, 1028.63/s  (1.010s, 1014.05/s)  LR: 6.775e-04  Data: 0.011 (0.014)
Train: 116 [ 650/1251 ( 52%)]  Loss: 3.738 (3.80)  Time: 0.996s, 1027.65/s  (1.010s, 1014.33/s)  LR: 6.775e-04  Data: 0.012 (0.013)
Train: 116 [ 700/1251 ( 56%)]  Loss: 3.560 (3.78)  Time: 0.996s, 1028.56/s  (1.009s, 1015.24/s)  LR: 6.775e-04  Data: 0.011 (0.013)
Train: 116 [ 750/1251 ( 60%)]  Loss: 3.830 (3.78)  Time: 1.064s,  962.15/s  (1.008s, 1015.43/s)  LR: 6.775e-04  Data: 0.012 (0.013)
Train: 116 [ 800/1251 ( 64%)]  Loss: 4.010 (3.80)  Time: 1.002s, 1021.85/s  (1.008s, 1016.04/s)  LR: 6.775e-04  Data: 0.011 (0.013)
Train: 116 [ 850/1251 ( 68%)]  Loss: 4.219 (3.82)  Time: 0.995s, 1029.34/s  (1.007s, 1016.45/s)  LR: 6.775e-04  Data: 0.011 (0.013)
Train: 116 [ 900/1251 ( 72%)]  Loss: 3.751 (3.82)  Time: 0.994s, 1030.19/s  (1.008s, 1015.75/s)  LR: 6.775e-04  Data: 0.012 (0.013)
Train: 116 [ 950/1251 ( 76%)]  Loss: 3.853 (3.82)  Time: 1.004s, 1020.03/s  (1.008s, 1016.12/s)  LR: 6.775e-04  Data: 0.011 (0.013)
Train: 116 [1000/1251 ( 80%)]  Loss: 3.404 (3.80)  Time: 1.014s, 1010.14/s  (1.008s, 1016.17/s)  LR: 6.775e-04  Data: 0.013 (0.013)
Train: 116 [1050/1251 ( 84%)]  Loss: 3.708 (3.80)  Time: 0.993s, 1031.10/s  (1.007s, 1016.52/s)  LR: 6.775e-04  Data: 0.011 (0.013)
Train: 116 [1100/1251 ( 88%)]  Loss: 3.992 (3.80)  Time: 0.993s, 1031.59/s  (1.007s, 1016.94/s)  LR: 6.775e-04  Data: 0.011 (0.012)
Train: 116 [1150/1251 ( 92%)]  Loss: 3.730 (3.80)  Time: 0.993s, 1031.49/s  (1.007s, 1017.31/s)  LR: 6.775e-04  Data: 0.011 (0.012)
Train: 116 [1200/1251 ( 96%)]  Loss: 3.943 (3.81)  Time: 0.999s, 1024.83/s  (1.006s, 1017.69/s)  LR: 6.775e-04  Data: 0.011 (0.012)
Train: 116 [1250/1251 (100%)]  Loss: 3.826 (3.81)  Time: 0.983s, 1041.42/s  (1.006s, 1018.08/s)  LR: 6.775e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.652 (1.652)  Loss:  0.9216 (0.9216)  Acc@1: 88.4766 (88.4766)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.245 (0.573)  Loss:  0.9988 (1.4461)  Acc@1: 82.9009 (72.9160)  Acc@5: 95.4009 (91.5600)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-113.pth.tar', 73.291999921875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-114.pth.tar', 73.11199999023438)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-116.pth.tar', 72.91600007080078)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-111.pth.tar', 72.79600012207031)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-115.pth.tar', 72.69200004394531)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-103.pth.tar', 72.67000009765626)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-107.pth.tar', 72.59400006591797)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-108.pth.tar', 72.57399993408202)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-105.pth.tar', 72.56800004394532)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-106.pth.tar', 72.54399994384765)

Train: 117 [   0/1251 (  0%)]  Loss: 3.565 (3.56)  Time: 2.492s,  410.90/s  (2.492s,  410.90/s)  LR: 6.727e-04  Data: 1.534 (1.534)
Train: 117 [  50/1251 (  4%)]  Loss: 3.779 (3.67)  Time: 0.994s, 1029.80/s  (1.047s,  978.32/s)  LR: 6.727e-04  Data: 0.011 (0.041)
Train: 117 [ 100/1251 (  8%)]  Loss: 3.412 (3.59)  Time: 0.997s, 1027.05/s  (1.025s,  999.26/s)  LR: 6.727e-04  Data: 0.012 (0.026)
Train: 117 [ 150/1251 ( 12%)]  Loss: 3.600 (3.59)  Time: 0.995s, 1028.75/s  (1.020s, 1003.43/s)  LR: 6.727e-04  Data: 0.010 (0.021)
Train: 117 [ 200/1251 ( 16%)]  Loss: 3.379 (3.55)  Time: 0.997s, 1027.03/s  (1.016s, 1007.48/s)  LR: 6.727e-04  Data: 0.011 (0.019)
Train: 117 [ 250/1251 ( 20%)]  Loss: 3.882 (3.60)  Time: 0.996s, 1027.89/s  (1.013s, 1010.41/s)  LR: 6.727e-04  Data: 0.011 (0.017)
Train: 117 [ 300/1251 ( 24%)]  Loss: 3.190 (3.54)  Time: 1.032s,  992.25/s  (1.012s, 1011.69/s)  LR: 6.727e-04  Data: 0.011 (0.016)
Train: 117 [ 350/1251 ( 28%)]  Loss: 3.517 (3.54)  Time: 0.997s, 1027.23/s  (1.010s, 1013.89/s)  LR: 6.727e-04  Data: 0.011 (0.015)
Train: 117 [ 400/1251 ( 32%)]  Loss: 3.793 (3.57)  Time: 0.997s, 1027.53/s  (1.009s, 1015.28/s)  LR: 6.727e-04  Data: 0.012 (0.015)
Train: 117 [ 450/1251 ( 36%)]  Loss: 3.933 (3.60)  Time: 0.995s, 1029.40/s  (1.008s, 1015.59/s)  LR: 6.727e-04  Data: 0.011 (0.014)
Train: 117 [ 500/1251 ( 40%)]  Loss: 3.769 (3.62)  Time: 0.995s, 1029.23/s  (1.008s, 1016.12/s)  LR: 6.727e-04  Data: 0.012 (0.014)
Train: 117 [ 550/1251 ( 44%)]  Loss: 3.498 (3.61)  Time: 1.000s, 1024.14/s  (1.007s, 1017.05/s)  LR: 6.727e-04  Data: 0.011 (0.014)
Train: 117 [ 600/1251 ( 48%)]  Loss: 3.840 (3.63)  Time: 0.996s, 1028.06/s  (1.006s, 1017.71/s)  LR: 6.727e-04  Data: 0.011 (0.014)
Train: 117 [ 650/1251 ( 52%)]  Loss: 3.678 (3.63)  Time: 0.998s, 1025.85/s  (1.006s, 1017.40/s)  LR: 6.727e-04  Data: 0.012 (0.013)
Train: 117 [ 700/1251 ( 56%)]  Loss: 3.826 (3.64)  Time: 0.990s, 1034.25/s  (1.006s, 1017.99/s)  LR: 6.727e-04  Data: 0.011 (0.013)
Train: 117 [ 750/1251 ( 60%)]  Loss: 3.520 (3.64)  Time: 0.997s, 1027.48/s  (1.005s, 1018.54/s)  LR: 6.727e-04  Data: 0.012 (0.013)
Train: 117 [ 800/1251 ( 64%)]  Loss: 3.589 (3.63)  Time: 0.994s, 1030.13/s  (1.005s, 1019.07/s)  LR: 6.727e-04  Data: 0.011 (0.013)
Train: 117 [ 850/1251 ( 68%)]  Loss: 3.539 (3.63)  Time: 1.056s,  969.70/s  (1.005s, 1019.25/s)  LR: 6.727e-04  Data: 0.012 (0.013)
Train: 117 [ 900/1251 ( 72%)]  Loss: 3.701 (3.63)  Time: 0.995s, 1028.89/s  (1.004s, 1019.61/s)  LR: 6.727e-04  Data: 0.012 (0.013)
Train: 117 [ 950/1251 ( 76%)]  Loss: 3.867 (3.64)  Time: 1.053s,  972.83/s  (1.005s, 1019.29/s)  LR: 6.727e-04  Data: 0.011 (0.013)
Train: 117 [1000/1251 ( 80%)]  Loss: 3.531 (3.64)  Time: 0.996s, 1027.64/s  (1.005s, 1018.98/s)  LR: 6.727e-04  Data: 0.012 (0.013)
Train: 117 [1050/1251 ( 84%)]  Loss: 3.690 (3.64)  Time: 0.998s, 1026.35/s  (1.005s, 1019.32/s)  LR: 6.727e-04  Data: 0.011 (0.013)
Train: 117 [1100/1251 ( 88%)]  Loss: 4.060 (3.66)  Time: 0.998s, 1025.96/s  (1.004s, 1019.45/s)  LR: 6.727e-04  Data: 0.012 (0.013)
Train: 117 [1150/1251 ( 92%)]  Loss: 3.469 (3.65)  Time: 0.996s, 1028.40/s  (1.004s, 1019.46/s)  LR: 6.727e-04  Data: 0.011 (0.013)
Train: 117 [1200/1251 ( 96%)]  Loss: 3.823 (3.66)  Time: 1.054s,  971.46/s  (1.005s, 1018.97/s)  LR: 6.727e-04  Data: 0.010 (0.012)
Train: 117 [1250/1251 (100%)]  Loss: 3.350 (3.65)  Time: 0.982s, 1042.55/s  (1.005s, 1018.48/s)  LR: 6.727e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.665 (1.665)  Loss:  0.6812 (0.6812)  Acc@1: 88.4766 (88.4766)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.245 (0.569)  Loss:  0.8487 (1.3419)  Acc@1: 83.7264 (72.8260)  Acc@5: 95.7547 (91.5140)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-113.pth.tar', 73.291999921875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-114.pth.tar', 73.11199999023438)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-116.pth.tar', 72.91600007080078)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-117.pth.tar', 72.82599991210938)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-111.pth.tar', 72.79600012207031)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-115.pth.tar', 72.69200004394531)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-103.pth.tar', 72.67000009765626)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-107.pth.tar', 72.59400006591797)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-108.pth.tar', 72.57399993408202)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-105.pth.tar', 72.56800004394532)

Train: 118 [   0/1251 (  0%)]  Loss: 3.055 (3.05)  Time: 2.455s,  417.16/s  (2.455s,  417.16/s)  LR: 6.678e-04  Data: 1.488 (1.488)
Train: 118 [  50/1251 (  4%)]  Loss: 3.868 (3.46)  Time: 1.038s,  986.63/s  (1.054s,  971.29/s)  LR: 6.678e-04  Data: 0.011 (0.046)
Train: 118 [ 100/1251 (  8%)]  Loss: 3.762 (3.56)  Time: 0.996s, 1027.93/s  (1.034s,  989.97/s)  LR: 6.678e-04  Data: 0.011 (0.028)
Train: 118 [ 150/1251 ( 12%)]  Loss: 3.555 (3.56)  Time: 0.995s, 1029.47/s  (1.030s,  994.50/s)  LR: 6.678e-04  Data: 0.011 (0.023)
Train: 118 [ 200/1251 ( 16%)]  Loss: 3.987 (3.65)  Time: 0.996s, 1027.88/s  (1.022s, 1001.99/s)  LR: 6.678e-04  Data: 0.011 (0.020)
Train: 118 [ 250/1251 ( 20%)]  Loss: 3.818 (3.67)  Time: 1.005s, 1019.06/s  (1.018s, 1005.82/s)  LR: 6.678e-04  Data: 0.011 (0.018)
Train: 118 [ 300/1251 ( 24%)]  Loss: 3.561 (3.66)  Time: 0.993s, 1031.09/s  (1.015s, 1008.85/s)  LR: 6.678e-04  Data: 0.012 (0.017)
Train: 118 [ 350/1251 ( 28%)]  Loss: 3.536 (3.64)  Time: 0.995s, 1028.66/s  (1.014s, 1009.71/s)  LR: 6.678e-04  Data: 0.011 (0.016)
Train: 118 [ 400/1251 ( 32%)]  Loss: 3.643 (3.64)  Time: 1.053s,  972.77/s  (1.012s, 1011.65/s)  LR: 6.678e-04  Data: 0.012 (0.015)
Train: 118 [ 450/1251 ( 36%)]  Loss: 3.551 (3.63)  Time: 1.044s,  980.45/s  (1.012s, 1011.62/s)  LR: 6.678e-04  Data: 0.011 (0.015)
Train: 118 [ 500/1251 ( 40%)]  Loss: 3.571 (3.63)  Time: 1.036s,  988.49/s  (1.013s, 1010.54/s)  LR: 6.678e-04  Data: 0.011 (0.015)
Train: 118 [ 550/1251 ( 44%)]  Loss: 3.498 (3.62)  Time: 0.993s, 1030.77/s  (1.013s, 1010.61/s)  LR: 6.678e-04  Data: 0.011 (0.014)
Train: 118 [ 600/1251 ( 48%)]  Loss: 3.778 (3.63)  Time: 1.001s, 1022.58/s  (1.013s, 1010.96/s)  LR: 6.678e-04  Data: 0.015 (0.014)
Train: 118 [ 650/1251 ( 52%)]  Loss: 3.728 (3.64)  Time: 1.054s,  971.87/s  (1.014s, 1009.99/s)  LR: 6.678e-04  Data: 0.011 (0.014)
Train: 118 [ 700/1251 ( 56%)]  Loss: 3.553 (3.63)  Time: 0.997s, 1026.64/s  (1.013s, 1010.74/s)  LR: 6.678e-04  Data: 0.011 (0.014)
Train: 118 [ 750/1251 ( 60%)]  Loss: 3.148 (3.60)  Time: 0.995s, 1029.52/s  (1.012s, 1011.64/s)  LR: 6.678e-04  Data: 0.011 (0.013)
Train: 118 [ 800/1251 ( 64%)]  Loss: 3.643 (3.60)  Time: 0.995s, 1029.06/s  (1.012s, 1011.65/s)  LR: 6.678e-04  Data: 0.011 (0.013)
Train: 118 [ 850/1251 ( 68%)]  Loss: 3.372 (3.59)  Time: 1.047s,  978.31/s  (1.012s, 1012.21/s)  LR: 6.678e-04  Data: 0.011 (0.013)
Train: 118 [ 900/1251 ( 72%)]  Loss: 3.681 (3.60)  Time: 0.996s, 1027.79/s  (1.012s, 1011.63/s)  LR: 6.678e-04  Data: 0.010 (0.013)
Train: 118 [ 950/1251 ( 76%)]  Loss: 3.582 (3.59)  Time: 0.995s, 1028.87/s  (1.011s, 1012.36/s)  LR: 6.678e-04  Data: 0.010 (0.013)
Train: 118 [1000/1251 ( 80%)]  Loss: 3.448 (3.59)  Time: 1.042s,  983.07/s  (1.011s, 1012.80/s)  LR: 6.678e-04  Data: 0.011 (0.013)
Train: 118 [1050/1251 ( 84%)]  Loss: 3.680 (3.59)  Time: 0.999s, 1025.27/s  (1.011s, 1013.09/s)  LR: 6.678e-04  Data: 0.011 (0.013)
Train: 118 [1100/1251 ( 88%)]  Loss: 3.527 (3.59)  Time: 0.999s, 1024.64/s  (1.010s, 1013.47/s)  LR: 6.678e-04  Data: 0.012 (0.013)
Train: 118 [1150/1251 ( 92%)]  Loss: 3.984 (3.61)  Time: 0.995s, 1028.76/s  (1.011s, 1013.34/s)  LR: 6.678e-04  Data: 0.011 (0.013)
Train: 118 [1200/1251 ( 96%)]  Loss: 3.923 (3.62)  Time: 1.064s,  962.09/s  (1.011s, 1012.78/s)  LR: 6.678e-04  Data: 0.011 (0.013)
Train: 118 [1250/1251 (100%)]  Loss: 3.799 (3.63)  Time: 0.983s, 1041.73/s  (1.011s, 1013.11/s)  LR: 6.678e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.613 (1.613)  Loss:  0.8341 (0.8341)  Acc@1: 89.2578 (89.2578)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.245 (0.576)  Loss:  0.9317 (1.4089)  Acc@1: 85.1415 (73.0840)  Acc@5: 96.3444 (91.7080)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-113.pth.tar', 73.291999921875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-114.pth.tar', 73.11199999023438)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-118.pth.tar', 73.08400000976563)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-116.pth.tar', 72.91600007080078)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-117.pth.tar', 72.82599991210938)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-111.pth.tar', 72.79600012207031)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-115.pth.tar', 72.69200004394531)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-103.pth.tar', 72.67000009765626)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-107.pth.tar', 72.59400006591797)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-108.pth.tar', 72.57399993408202)

Train: 119 [   0/1251 (  0%)]  Loss: 3.892 (3.89)  Time: 2.380s,  430.23/s  (2.380s,  430.23/s)  LR: 6.629e-04  Data: 1.429 (1.429)
Train: 119 [  50/1251 (  4%)]  Loss: 3.429 (3.66)  Time: 0.998s, 1026.41/s  (1.048s,  976.68/s)  LR: 6.629e-04  Data: 0.011 (0.039)
Train: 119 [ 100/1251 (  8%)]  Loss: 3.463 (3.59)  Time: 0.994s, 1030.45/s  (1.025s,  999.36/s)  LR: 6.629e-04  Data: 0.011 (0.025)
Train: 119 [ 150/1251 ( 12%)]  Loss: 3.448 (3.56)  Time: 0.994s, 1029.79/s  (1.019s, 1004.77/s)  LR: 6.629e-04  Data: 0.011 (0.021)
Train: 119 [ 200/1251 ( 16%)]  Loss: 3.514 (3.55)  Time: 1.056s,  970.10/s  (1.024s, 1000.21/s)  LR: 6.629e-04  Data: 0.011 (0.018)
Train: 119 [ 250/1251 ( 20%)]  Loss: 3.471 (3.54)  Time: 0.996s, 1028.16/s  (1.020s, 1004.04/s)  LR: 6.629e-04  Data: 0.011 (0.017)
Train: 119 [ 300/1251 ( 24%)]  Loss: 3.394 (3.52)  Time: 1.034s,  990.11/s  (1.017s, 1006.78/s)  LR: 6.629e-04  Data: 0.011 (0.016)
Train: 119 [ 350/1251 ( 28%)]  Loss: 3.819 (3.55)  Time: 0.992s, 1032.09/s  (1.018s, 1005.41/s)  LR: 6.629e-04  Data: 0.011 (0.015)
Train: 119 [ 400/1251 ( 32%)]  Loss: 3.815 (3.58)  Time: 1.002s, 1022.28/s  (1.016s, 1007.66/s)  LR: 6.629e-04  Data: 0.011 (0.015)
Train: 119 [ 450/1251 ( 36%)]  Loss: 3.510 (3.58)  Time: 1.006s, 1018.25/s  (1.015s, 1009.06/s)  LR: 6.629e-04  Data: 0.011 (0.014)
Train: 119 [ 500/1251 ( 40%)]  Loss: 3.192 (3.54)  Time: 0.994s, 1029.76/s  (1.014s, 1009.42/s)  LR: 6.629e-04  Data: 0.010 (0.014)
Train: 119 [ 550/1251 ( 44%)]  Loss: 3.602 (3.55)  Time: 0.996s, 1027.93/s  (1.013s, 1010.71/s)  LR: 6.629e-04  Data: 0.012 (0.014)
Train: 119 [ 600/1251 ( 48%)]  Loss: 3.515 (3.54)  Time: 0.998s, 1026.23/s  (1.012s, 1011.83/s)  LR: 6.629e-04  Data: 0.012 (0.014)
Train: 119 [ 650/1251 ( 52%)]  Loss: 3.613 (3.55)  Time: 0.995s, 1029.47/s  (1.011s, 1012.82/s)  LR: 6.629e-04  Data: 0.011 (0.014)
Train: 119 [ 700/1251 ( 56%)]  Loss: 3.386 (3.54)  Time: 1.002s, 1021.66/s  (1.010s, 1013.39/s)  LR: 6.629e-04  Data: 0.012 (0.013)
Train: 119 [ 750/1251 ( 60%)]  Loss: 3.635 (3.54)  Time: 0.990s, 1033.83/s  (1.010s, 1013.68/s)  LR: 6.629e-04  Data: 0.010 (0.013)
Train: 119 [ 800/1251 ( 64%)]  Loss: 3.910 (3.57)  Time: 0.998s, 1025.71/s  (1.010s, 1014.27/s)  LR: 6.629e-04  Data: 0.012 (0.013)
Train: 119 [ 850/1251 ( 68%)]  Loss: 3.858 (3.58)  Time: 0.991s, 1033.55/s  (1.009s, 1015.00/s)  LR: 6.629e-04  Data: 0.010 (0.013)
Train: 119 [ 900/1251 ( 72%)]  Loss: 3.589 (3.58)  Time: 0.993s, 1030.84/s  (1.008s, 1015.45/s)  LR: 6.629e-04  Data: 0.011 (0.013)
Train: 119 [ 950/1251 ( 76%)]  Loss: 3.653 (3.59)  Time: 0.995s, 1029.50/s  (1.008s, 1015.94/s)  LR: 6.629e-04  Data: 0.012 (0.013)
Train: 119 [1000/1251 ( 80%)]  Loss: 3.520 (3.58)  Time: 0.996s, 1028.06/s  (1.008s, 1016.35/s)  LR: 6.629e-04  Data: 0.011 (0.013)
Train: 119 [1050/1251 ( 84%)]  Loss: 3.512 (3.58)  Time: 1.031s,  993.18/s  (1.007s, 1016.43/s)  LR: 6.629e-04  Data: 0.011 (0.013)
Train: 119 [1100/1251 ( 88%)]  Loss: 4.095 (3.60)  Time: 0.995s, 1028.84/s  (1.008s, 1015.72/s)  LR: 6.629e-04  Data: 0.011 (0.013)
Train: 119 [1150/1251 ( 92%)]  Loss: 3.649 (3.60)  Time: 0.996s, 1028.56/s  (1.008s, 1016.12/s)  LR: 6.629e-04  Data: 0.011 (0.013)
Train: 119 [1200/1251 ( 96%)]  Loss: 3.445 (3.60)  Time: 0.995s, 1029.62/s  (1.008s, 1016.31/s)  LR: 6.629e-04  Data: 0.011 (0.012)
Train: 119 [1250/1251 (100%)]  Loss: 3.845 (3.61)  Time: 0.983s, 1041.68/s  (1.007s, 1016.62/s)  LR: 6.629e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.641 (1.641)  Loss:  0.8378 (0.8378)  Acc@1: 88.4766 (88.4766)  Acc@5: 96.1914 (96.1914)
Test: [  48/48]  Time: 0.245 (0.570)  Loss:  0.9396 (1.3702)  Acc@1: 82.6651 (73.3140)  Acc@5: 95.5189 (91.8880)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-119.pth.tar', 73.31400009765625)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-113.pth.tar', 73.291999921875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-114.pth.tar', 73.11199999023438)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-118.pth.tar', 73.08400000976563)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-116.pth.tar', 72.91600007080078)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-117.pth.tar', 72.82599991210938)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-111.pth.tar', 72.79600012207031)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-115.pth.tar', 72.69200004394531)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-103.pth.tar', 72.67000009765626)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-107.pth.tar', 72.59400006591797)

Train: 120 [   0/1251 (  0%)]  Loss: 4.018 (4.02)  Time: 4.388s,  233.36/s  (4.388s,  233.36/s)  LR: 6.580e-04  Data: 3.163 (3.163)
Train: 120 [  50/1251 (  4%)]  Loss: 3.879 (3.95)  Time: 0.995s, 1028.77/s  (1.073s,  954.64/s)  LR: 6.580e-04  Data: 0.011 (0.073)
Train: 120 [ 100/1251 (  8%)]  Loss: 3.668 (3.85)  Time: 1.006s, 1018.26/s  (1.045s,  980.08/s)  LR: 6.580e-04  Data: 0.011 (0.042)
Train: 120 [ 150/1251 ( 12%)]  Loss: 3.648 (3.80)  Time: 0.995s, 1028.63/s  (1.029s,  994.73/s)  LR: 6.580e-04  Data: 0.012 (0.032)
Train: 120 [ 200/1251 ( 16%)]  Loss: 3.495 (3.74)  Time: 1.058s,  967.92/s  (1.026s,  998.45/s)  LR: 6.580e-04  Data: 0.011 (0.027)
Train: 120 [ 250/1251 ( 20%)]  Loss: 3.797 (3.75)  Time: 1.001s, 1023.42/s  (1.021s, 1002.61/s)  LR: 6.580e-04  Data: 0.011 (0.024)
Train: 120 [ 300/1251 ( 24%)]  Loss: 3.705 (3.74)  Time: 1.048s,  977.51/s  (1.018s, 1005.93/s)  LR: 6.580e-04  Data: 0.011 (0.022)
Train: 120 [ 350/1251 ( 28%)]  Loss: 3.548 (3.72)  Time: 0.998s, 1025.98/s  (1.019s, 1005.18/s)  LR: 6.580e-04  Data: 0.011 (0.020)
Train: 120 [ 400/1251 ( 32%)]  Loss: 3.872 (3.74)  Time: 0.997s, 1027.51/s  (1.016s, 1007.78/s)  LR: 6.580e-04  Data: 0.011 (0.019)
Train: 120 [ 450/1251 ( 36%)]  Loss: 3.247 (3.69)  Time: 0.996s, 1028.33/s  (1.014s, 1010.01/s)  LR: 6.580e-04  Data: 0.011 (0.018)
Train: 120 [ 500/1251 ( 40%)]  Loss: 3.885 (3.71)  Time: 1.002s, 1022.14/s  (1.013s, 1011.31/s)  LR: 6.580e-04  Data: 0.011 (0.018)
Train: 120 [ 550/1251 ( 44%)]  Loss: 3.324 (3.67)  Time: 0.998s, 1025.68/s  (1.012s, 1011.93/s)  LR: 6.580e-04  Data: 0.010 (0.017)
Train: 120 [ 600/1251 ( 48%)]  Loss: 3.649 (3.67)  Time: 0.994s, 1029.98/s  (1.011s, 1012.88/s)  LR: 6.580e-04  Data: 0.011 (0.016)
Train: 120 [ 650/1251 ( 52%)]  Loss: 3.858 (3.69)  Time: 0.997s, 1027.20/s  (1.010s, 1013.79/s)  LR: 6.580e-04  Data: 0.011 (0.016)
Train: 120 [ 700/1251 ( 56%)]  Loss: 4.035 (3.71)  Time: 1.007s, 1017.02/s  (1.010s, 1013.56/s)  LR: 6.580e-04  Data: 0.011 (0.016)
Train: 120 [ 750/1251 ( 60%)]  Loss: 3.524 (3.70)  Time: 1.035s,  989.16/s  (1.010s, 1014.15/s)  LR: 6.580e-04  Data: 0.011 (0.015)
Train: 120 [ 800/1251 ( 64%)]  Loss: 3.573 (3.69)  Time: 0.993s, 1030.96/s  (1.010s, 1014.17/s)  LR: 6.580e-04  Data: 0.011 (0.015)
Train: 120 [ 850/1251 ( 68%)]  Loss: 3.652 (3.69)  Time: 0.997s, 1026.86/s  (1.009s, 1014.88/s)  LR: 6.580e-04  Data: 0.011 (0.015)
Train: 120 [ 900/1251 ( 72%)]  Loss: 3.602 (3.68)  Time: 0.997s, 1026.72/s  (1.009s, 1015.27/s)  LR: 6.580e-04  Data: 0.011 (0.015)
Train: 120 [ 950/1251 ( 76%)]  Loss: 3.253 (3.66)  Time: 0.995s, 1029.52/s  (1.009s, 1015.20/s)  LR: 6.580e-04  Data: 0.011 (0.015)
Train: 120 [1000/1251 ( 80%)]  Loss: 3.765 (3.67)  Time: 0.996s, 1028.06/s  (1.008s, 1015.82/s)  LR: 6.580e-04  Data: 0.012 (0.014)
Train: 120 [1050/1251 ( 84%)]  Loss: 4.078 (3.69)  Time: 1.000s, 1024.07/s  (1.008s, 1016.35/s)  LR: 6.580e-04  Data: 0.010 (0.014)
Train: 120 [1100/1251 ( 88%)]  Loss: 3.550 (3.68)  Time: 0.994s, 1030.04/s  (1.007s, 1016.77/s)  LR: 6.580e-04  Data: 0.012 (0.014)
Train: 120 [1150/1251 ( 92%)]  Loss: 3.728 (3.68)  Time: 0.994s, 1030.28/s  (1.007s, 1016.84/s)  LR: 6.580e-04  Data: 0.010 (0.014)
Train: 120 [1200/1251 ( 96%)]  Loss: 3.822 (3.69)  Time: 0.999s, 1025.48/s  (1.007s, 1017.12/s)  LR: 6.580e-04  Data: 0.010 (0.014)
Train: 120 [1250/1251 (100%)]  Loss: 3.815 (3.69)  Time: 0.983s, 1041.98/s  (1.007s, 1017.14/s)  LR: 6.580e-04  Data: 0.000 (0.014)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.651 (1.651)  Loss:  0.8265 (0.8265)  Acc@1: 87.5977 (87.5977)  Acc@5: 96.1914 (96.1914)
Test: [  48/48]  Time: 0.245 (0.574)  Loss:  0.9102 (1.3647)  Acc@1: 84.3160 (72.9460)  Acc@5: 95.4009 (91.6480)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-119.pth.tar', 73.31400009765625)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-113.pth.tar', 73.291999921875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-114.pth.tar', 73.11199999023438)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-118.pth.tar', 73.08400000976563)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-120.pth.tar', 72.94599990966798)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-116.pth.tar', 72.91600007080078)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-117.pth.tar', 72.82599991210938)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-111.pth.tar', 72.79600012207031)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-115.pth.tar', 72.69200004394531)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-103.pth.tar', 72.67000009765626)

Train: 121 [   0/1251 (  0%)]  Loss: 3.633 (3.63)  Time: 2.606s,  392.92/s  (2.606s,  392.92/s)  LR: 6.530e-04  Data: 1.649 (1.649)
Train: 121 [  50/1251 (  4%)]  Loss: 3.518 (3.58)  Time: 1.061s,  965.10/s  (1.050s,  975.31/s)  LR: 6.530e-04  Data: 0.011 (0.049)
Train: 121 [ 100/1251 (  8%)]  Loss: 3.438 (3.53)  Time: 1.000s, 1023.55/s  (1.038s,  986.80/s)  LR: 6.530e-04  Data: 0.012 (0.030)
Train: 121 [ 150/1251 ( 12%)]  Loss: 3.555 (3.54)  Time: 0.996s, 1027.95/s  (1.031s,  992.86/s)  LR: 6.530e-04  Data: 0.010 (0.024)
Train: 121 [ 200/1251 ( 16%)]  Loss: 3.659 (3.56)  Time: 1.055s,  970.62/s  (1.023s, 1000.63/s)  LR: 6.530e-04  Data: 0.011 (0.021)
Train: 121 [ 250/1251 ( 20%)]  Loss: 3.528 (3.56)  Time: 1.045s,  979.74/s  (1.019s, 1005.28/s)  LR: 6.530e-04  Data: 0.011 (0.019)
Train: 121 [ 300/1251 ( 24%)]  Loss: 3.669 (3.57)  Time: 0.994s, 1029.67/s  (1.015s, 1008.76/s)  LR: 6.530e-04  Data: 0.011 (0.018)
Train: 121 [ 350/1251 ( 28%)]  Loss: 3.667 (3.58)  Time: 1.054s,  971.13/s  (1.018s, 1006.29/s)  LR: 6.530e-04  Data: 0.011 (0.017)
Train: 121 [ 400/1251 ( 32%)]  Loss: 3.575 (3.58)  Time: 1.029s,  995.01/s  (1.016s, 1007.66/s)  LR: 6.530e-04  Data: 0.012 (0.016)
Train: 121 [ 450/1251 ( 36%)]  Loss: 3.709 (3.60)  Time: 0.995s, 1028.87/s  (1.014s, 1009.60/s)  LR: 6.530e-04  Data: 0.011 (0.015)
Train: 121 [ 500/1251 ( 40%)]  Loss: 4.085 (3.64)  Time: 0.994s, 1030.24/s  (1.013s, 1011.09/s)  LR: 6.530e-04  Data: 0.011 (0.015)
Train: 121 [ 550/1251 ( 44%)]  Loss: 3.356 (3.62)  Time: 0.997s, 1027.56/s  (1.011s, 1012.37/s)  LR: 6.530e-04  Data: 0.011 (0.015)
Train: 121 [ 600/1251 ( 48%)]  Loss: 3.595 (3.61)  Time: 0.997s, 1027.40/s  (1.012s, 1012.00/s)  LR: 6.530e-04  Data: 0.011 (0.014)
Train: 121 [ 650/1251 ( 52%)]  Loss: 4.022 (3.64)  Time: 1.053s,  972.37/s  (1.012s, 1011.44/s)  LR: 6.530e-04  Data: 0.011 (0.014)
Train: 121 [ 700/1251 ( 56%)]  Loss: 3.570 (3.64)  Time: 0.995s, 1029.44/s  (1.012s, 1011.56/s)  LR: 6.530e-04  Data: 0.011 (0.014)
Train: 121 [ 750/1251 ( 60%)]  Loss: 3.234 (3.61)  Time: 0.991s, 1033.56/s  (1.011s, 1012.38/s)  LR: 6.530e-04  Data: 0.010 (0.014)
Train: 121 [ 800/1251 ( 64%)]  Loss: 3.912 (3.63)  Time: 0.995s, 1028.80/s  (1.011s, 1013.26/s)  LR: 6.530e-04  Data: 0.012 (0.014)
Train: 121 [ 850/1251 ( 68%)]  Loss: 3.801 (3.64)  Time: 0.997s, 1027.58/s  (1.010s, 1014.00/s)  LR: 6.530e-04  Data: 0.012 (0.013)
Train: 121 [ 900/1251 ( 72%)]  Loss: 3.371 (3.63)  Time: 0.993s, 1030.91/s  (1.009s, 1014.81/s)  LR: 6.530e-04  Data: 0.011 (0.013)
Train: 121 [ 950/1251 ( 76%)]  Loss: 3.604 (3.63)  Time: 0.995s, 1028.83/s  (1.008s, 1015.38/s)  LR: 6.530e-04  Data: 0.012 (0.013)
Train: 121 [1000/1251 ( 80%)]  Loss: 3.500 (3.62)  Time: 0.996s, 1028.52/s  (1.009s, 1015.35/s)  LR: 6.530e-04  Data: 0.012 (0.013)
Train: 121 [1050/1251 ( 84%)]  Loss: 3.575 (3.62)  Time: 0.995s, 1029.09/s  (1.008s, 1015.82/s)  LR: 6.530e-04  Data: 0.011 (0.013)
Train: 121 [1100/1251 ( 88%)]  Loss: 3.505 (3.61)  Time: 0.996s, 1028.58/s  (1.009s, 1015.25/s)  LR: 6.530e-04  Data: 0.012 (0.013)
Train: 121 [1150/1251 ( 92%)]  Loss: 3.725 (3.62)  Time: 1.065s,  961.92/s  (1.009s, 1014.99/s)  LR: 6.530e-04  Data: 0.011 (0.013)
Train: 121 [1200/1251 ( 96%)]  Loss: 3.705 (3.62)  Time: 0.995s, 1029.18/s  (1.009s, 1015.34/s)  LR: 6.530e-04  Data: 0.011 (0.013)
Train: 121 [1250/1251 (100%)]  Loss: 3.884 (3.63)  Time: 0.983s, 1041.77/s  (1.008s, 1015.78/s)  LR: 6.530e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.648 (1.648)  Loss:  0.7746 (0.7746)  Acc@1: 88.3789 (88.3789)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.245 (0.571)  Loss:  0.7692 (1.3184)  Acc@1: 84.7877 (72.9400)  Acc@5: 95.8727 (91.7100)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-119.pth.tar', 73.31400009765625)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-113.pth.tar', 73.291999921875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-114.pth.tar', 73.11199999023438)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-118.pth.tar', 73.08400000976563)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-120.pth.tar', 72.94599990966798)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-121.pth.tar', 72.9400001147461)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-116.pth.tar', 72.91600007080078)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-117.pth.tar', 72.82599991210938)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-111.pth.tar', 72.79600012207031)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-115.pth.tar', 72.69200004394531)

Train: 122 [   0/1251 (  0%)]  Loss: 3.347 (3.35)  Time: 2.589s,  395.47/s  (2.589s,  395.47/s)  LR: 6.481e-04  Data: 1.627 (1.627)
Train: 122 [  50/1251 (  4%)]  Loss: 3.468 (3.41)  Time: 0.999s, 1025.13/s  (1.057s,  969.19/s)  LR: 6.481e-04  Data: 0.010 (0.045)
Train: 122 [ 100/1251 (  8%)]  Loss: 3.734 (3.52)  Time: 1.034s,  990.79/s  (1.033s,  990.90/s)  LR: 6.481e-04  Data: 0.010 (0.028)
Train: 122 [ 150/1251 ( 12%)]  Loss: 3.834 (3.60)  Time: 1.014s, 1009.84/s  (1.026s,  998.40/s)  LR: 6.481e-04  Data: 0.014 (0.023)
Train: 122 [ 200/1251 ( 16%)]  Loss: 3.401 (3.56)  Time: 1.053s,  972.43/s  (1.026s,  997.69/s)  LR: 6.481e-04  Data: 0.012 (0.020)
Train: 122 [ 250/1251 ( 20%)]  Loss: 3.808 (3.60)  Time: 0.996s, 1027.82/s  (1.022s, 1002.17/s)  LR: 6.481e-04  Data: 0.011 (0.018)
Train: 122 [ 300/1251 ( 24%)]  Loss: 3.646 (3.61)  Time: 0.997s, 1026.82/s  (1.018s, 1006.11/s)  LR: 6.481e-04  Data: 0.010 (0.017)
Train: 122 [ 350/1251 ( 28%)]  Loss: 3.510 (3.59)  Time: 0.995s, 1029.14/s  (1.015s, 1008.87/s)  LR: 6.481e-04  Data: 0.011 (0.016)
Train: 122 [ 400/1251 ( 32%)]  Loss: 4.040 (3.64)  Time: 1.049s,  976.19/s  (1.014s, 1010.11/s)  LR: 6.481e-04  Data: 0.010 (0.015)
Train: 122 [ 450/1251 ( 36%)]  Loss: 3.207 (3.60)  Time: 1.045s,  980.26/s  (1.013s, 1010.93/s)  LR: 6.481e-04  Data: 0.011 (0.015)
Train: 122 [ 500/1251 ( 40%)]  Loss: 3.657 (3.60)  Time: 1.037s,  987.39/s  (1.014s, 1010.00/s)  LR: 6.481e-04  Data: 0.011 (0.015)
Train: 122 [ 550/1251 ( 44%)]  Loss: 3.491 (3.60)  Time: 0.997s, 1026.81/s  (1.014s, 1009.67/s)  LR: 6.481e-04  Data: 0.012 (0.014)
Train: 122 [ 600/1251 ( 48%)]  Loss: 3.662 (3.60)  Time: 1.039s,  985.49/s  (1.015s, 1009.36/s)  LR: 6.481e-04  Data: 0.011 (0.014)
Train: 122 [ 650/1251 ( 52%)]  Loss: 3.534 (3.60)  Time: 0.994s, 1029.68/s  (1.015s, 1009.29/s)  LR: 6.481e-04  Data: 0.011 (0.014)
Train: 122 [ 700/1251 ( 56%)]  Loss: 3.746 (3.61)  Time: 1.001s, 1023.49/s  (1.014s, 1010.33/s)  LR: 6.481e-04  Data: 0.012 (0.014)
Train: 122 [ 750/1251 ( 60%)]  Loss: 3.325 (3.59)  Time: 0.996s, 1028.45/s  (1.013s, 1011.11/s)  LR: 6.481e-04  Data: 0.012 (0.013)
Train: 122 [ 800/1251 ( 64%)]  Loss: 3.317 (3.57)  Time: 0.995s, 1029.19/s  (1.012s, 1011.87/s)  LR: 6.481e-04  Data: 0.010 (0.013)
Train: 122 [ 850/1251 ( 68%)]  Loss: 3.484 (3.57)  Time: 1.002s, 1022.31/s  (1.012s, 1012.00/s)  LR: 6.481e-04  Data: 0.011 (0.013)
Train: 122 [ 900/1251 ( 72%)]  Loss: 3.222 (3.55)  Time: 0.998s, 1026.32/s  (1.011s, 1012.60/s)  LR: 6.481e-04  Data: 0.012 (0.013)
Train: 122 [ 950/1251 ( 76%)]  Loss: 3.875 (3.57)  Time: 0.996s, 1028.42/s  (1.011s, 1013.29/s)  LR: 6.481e-04  Data: 0.012 (0.013)
Train: 122 [1000/1251 ( 80%)]  Loss: 3.549 (3.56)  Time: 1.003s, 1021.24/s  (1.010s, 1013.91/s)  LR: 6.481e-04  Data: 0.011 (0.013)
Train: 122 [1050/1251 ( 84%)]  Loss: 3.620 (3.57)  Time: 1.012s, 1011.70/s  (1.010s, 1014.29/s)  LR: 6.481e-04  Data: 0.014 (0.013)
Train: 122 [1100/1251 ( 88%)]  Loss: 3.562 (3.57)  Time: 0.995s, 1029.30/s  (1.009s, 1014.47/s)  LR: 6.481e-04  Data: 0.011 (0.013)
Train: 122 [1150/1251 ( 92%)]  Loss: 3.678 (3.57)  Time: 1.044s,  980.62/s  (1.010s, 1014.23/s)  LR: 6.481e-04  Data: 0.011 (0.013)
Train: 122 [1200/1251 ( 96%)]  Loss: 3.481 (3.57)  Time: 0.996s, 1028.19/s  (1.009s, 1014.54/s)  LR: 6.481e-04  Data: 0.011 (0.013)
Train: 122 [1250/1251 (100%)]  Loss: 3.797 (3.58)  Time: 0.983s, 1041.83/s  (1.009s, 1014.87/s)  LR: 6.481e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.612 (1.612)  Loss:  0.7917 (0.7917)  Acc@1: 89.3555 (89.3555)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.245 (0.572)  Loss:  0.8462 (1.3557)  Acc@1: 84.1981 (73.5220)  Acc@5: 96.1085 (91.8420)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-122.pth.tar', 73.5220001171875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-119.pth.tar', 73.31400009765625)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-113.pth.tar', 73.291999921875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-114.pth.tar', 73.11199999023438)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-118.pth.tar', 73.08400000976563)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-120.pth.tar', 72.94599990966798)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-121.pth.tar', 72.9400001147461)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-116.pth.tar', 72.91600007080078)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-117.pth.tar', 72.82599991210938)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-111.pth.tar', 72.79600012207031)

Train: 123 [   0/1251 (  0%)]  Loss: 3.279 (3.28)  Time: 2.453s,  417.51/s  (2.453s,  417.51/s)  LR: 6.431e-04  Data: 1.495 (1.495)
Train: 123 [  50/1251 (  4%)]  Loss: 3.727 (3.50)  Time: 0.997s, 1027.08/s  (1.040s,  984.67/s)  LR: 6.431e-04  Data: 0.011 (0.040)
Train: 123 [ 100/1251 (  8%)]  Loss: 3.427 (3.48)  Time: 1.038s,  986.12/s  (1.022s, 1002.27/s)  LR: 6.431e-04  Data: 0.010 (0.025)
Train: 123 [ 150/1251 ( 12%)]  Loss: 3.994 (3.61)  Time: 0.994s, 1029.81/s  (1.014s, 1010.08/s)  LR: 6.431e-04  Data: 0.011 (0.021)
Train: 123 [ 200/1251 ( 16%)]  Loss: 3.482 (3.58)  Time: 0.999s, 1025.03/s  (1.010s, 1014.21/s)  LR: 6.431e-04  Data: 0.011 (0.018)
Train: 123 [ 250/1251 ( 20%)]  Loss: 3.866 (3.63)  Time: 1.000s, 1023.57/s  (1.008s, 1015.83/s)  LR: 6.431e-04  Data: 0.010 (0.017)
Train: 123 [ 300/1251 ( 24%)]  Loss: 3.762 (3.65)  Time: 0.996s, 1027.73/s  (1.008s, 1016.32/s)  LR: 6.431e-04  Data: 0.011 (0.016)
Train: 123 [ 350/1251 ( 28%)]  Loss: 3.286 (3.60)  Time: 1.065s,  961.88/s  (1.008s, 1016.18/s)  LR: 6.431e-04  Data: 0.011 (0.015)
Train: 123 [ 400/1251 ( 32%)]  Loss: 3.468 (3.59)  Time: 0.998s, 1026.51/s  (1.008s, 1015.74/s)  LR: 6.431e-04  Data: 0.011 (0.015)
Train: 123 [ 450/1251 ( 36%)]  Loss: 3.628 (3.59)  Time: 0.995s, 1028.63/s  (1.009s, 1014.69/s)  LR: 6.431e-04  Data: 0.012 (0.014)
Train: 123 [ 500/1251 ( 40%)]  Loss: 3.482 (3.58)  Time: 0.996s, 1027.79/s  (1.008s, 1015.79/s)  LR: 6.431e-04  Data: 0.011 (0.014)
Train: 123 [ 550/1251 ( 44%)]  Loss: 3.715 (3.59)  Time: 1.045s,  980.13/s  (1.008s, 1016.22/s)  LR: 6.431e-04  Data: 0.011 (0.014)
Train: 123 [ 600/1251 ( 48%)]  Loss: 3.661 (3.60)  Time: 0.994s, 1029.91/s  (1.007s, 1016.98/s)  LR: 6.431e-04  Data: 0.011 (0.013)
Train: 123 [ 650/1251 ( 52%)]  Loss: 3.747 (3.61)  Time: 0.998s, 1026.21/s  (1.007s, 1017.34/s)  LR: 6.431e-04  Data: 0.011 (0.013)
Train: 123 [ 700/1251 ( 56%)]  Loss: 3.761 (3.62)  Time: 0.998s, 1025.61/s  (1.006s, 1017.79/s)  LR: 6.431e-04  Data: 0.012 (0.013)
Train: 123 [ 750/1251 ( 60%)]  Loss: 3.336 (3.60)  Time: 0.991s, 1033.73/s  (1.006s, 1018.10/s)  LR: 6.431e-04  Data: 0.011 (0.013)
Train: 123 [ 800/1251 ( 64%)]  Loss: 3.764 (3.61)  Time: 0.994s, 1029.70/s  (1.006s, 1018.03/s)  LR: 6.431e-04  Data: 0.011 (0.013)
Train: 123 [ 850/1251 ( 68%)]  Loss: 3.939 (3.63)  Time: 1.006s, 1018.04/s  (1.006s, 1018.26/s)  LR: 6.431e-04  Data: 0.012 (0.013)
Train: 123 [ 900/1251 ( 72%)]  Loss: 3.708 (3.63)  Time: 1.008s, 1015.58/s  (1.005s, 1018.49/s)  LR: 6.431e-04  Data: 0.011 (0.013)
Train: 123 [ 950/1251 ( 76%)]  Loss: 3.377 (3.62)  Time: 1.056s,  969.50/s  (1.006s, 1018.00/s)  LR: 6.431e-04  Data: 0.011 (0.013)
Train: 123 [1000/1251 ( 80%)]  Loss: 3.440 (3.61)  Time: 0.991s, 1033.73/s  (1.007s, 1017.14/s)  LR: 6.431e-04  Data: 0.011 (0.013)
Train: 123 [1050/1251 ( 84%)]  Loss: 3.629 (3.61)  Time: 1.000s, 1023.93/s  (1.006s, 1017.44/s)  LR: 6.431e-04  Data: 0.012 (0.013)
Train: 123 [1100/1251 ( 88%)]  Loss: 3.378 (3.60)  Time: 0.995s, 1028.73/s  (1.006s, 1017.85/s)  LR: 6.431e-04  Data: 0.011 (0.012)
Train: 123 [1150/1251 ( 92%)]  Loss: 3.591 (3.60)  Time: 0.996s, 1027.90/s  (1.006s, 1018.16/s)  LR: 6.431e-04  Data: 0.011 (0.012)
Train: 123 [1200/1251 ( 96%)]  Loss: 3.346 (3.59)  Time: 0.995s, 1029.60/s  (1.006s, 1018.19/s)  LR: 6.431e-04  Data: 0.011 (0.012)
Train: 123 [1250/1251 (100%)]  Loss: 3.965 (3.61)  Time: 0.983s, 1041.44/s  (1.005s, 1018.46/s)  LR: 6.431e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.662 (1.662)  Loss:  0.8872 (0.8872)  Acc@1: 87.4023 (87.4023)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.246 (0.569)  Loss:  0.9750 (1.4534)  Acc@1: 83.2547 (72.7020)  Acc@5: 94.8113 (91.3940)
Train: 124 [   0/1251 (  0%)]  Loss: 3.586 (3.59)  Time: 2.510s,  407.92/s  (2.510s,  407.92/s)  LR: 6.381e-04  Data: 1.547 (1.547)
Train: 124 [  50/1251 (  4%)]  Loss: 3.445 (3.52)  Time: 1.029s,  995.28/s  (1.034s,  990.25/s)  LR: 6.381e-04  Data: 0.011 (0.046)
Train: 124 [ 100/1251 (  8%)]  Loss: 3.506 (3.51)  Time: 0.995s, 1029.24/s  (1.025s,  999.06/s)  LR: 6.381e-04  Data: 0.011 (0.029)
Train: 124 [ 150/1251 ( 12%)]  Loss: 3.735 (3.57)  Time: 1.046s,  979.39/s  (1.018s, 1005.59/s)  LR: 6.381e-04  Data: 0.011 (0.023)
Train: 124 [ 200/1251 ( 16%)]  Loss: 3.332 (3.52)  Time: 0.994s, 1030.05/s  (1.015s, 1009.35/s)  LR: 6.381e-04  Data: 0.011 (0.020)
Train: 124 [ 250/1251 ( 20%)]  Loss: 3.631 (3.54)  Time: 1.008s, 1015.99/s  (1.013s, 1010.72/s)  LR: 6.381e-04  Data: 0.011 (0.018)
Train: 124 [ 300/1251 ( 24%)]  Loss: 3.750 (3.57)  Time: 0.997s, 1027.20/s  (1.012s, 1012.24/s)  LR: 6.381e-04  Data: 0.011 (0.017)
Train: 124 [ 350/1251 ( 28%)]  Loss: 3.958 (3.62)  Time: 0.996s, 1027.65/s  (1.012s, 1011.99/s)  LR: 6.381e-04  Data: 0.010 (0.016)
Train: 124 [ 400/1251 ( 32%)]  Loss: 3.967 (3.66)  Time: 0.998s, 1025.71/s  (1.012s, 1012.36/s)  LR: 6.381e-04  Data: 0.011 (0.016)
Train: 124 [ 450/1251 ( 36%)]  Loss: 3.811 (3.67)  Time: 0.993s, 1031.61/s  (1.010s, 1013.48/s)  LR: 6.381e-04  Data: 0.010 (0.015)
Train: 124 [ 500/1251 ( 40%)]  Loss: 3.475 (3.65)  Time: 1.048s,  976.87/s  (1.012s, 1012.02/s)  LR: 6.381e-04  Data: 0.011 (0.015)
Train: 124 [ 550/1251 ( 44%)]  Loss: 3.496 (3.64)  Time: 1.034s,  990.25/s  (1.012s, 1011.48/s)  LR: 6.381e-04  Data: 0.011 (0.014)
Train: 124 [ 600/1251 ( 48%)]  Loss: 3.285 (3.61)  Time: 1.062s,  964.01/s  (1.013s, 1010.91/s)  LR: 6.381e-04  Data: 0.015 (0.014)
Train: 124 [ 650/1251 ( 52%)]  Loss: 3.589 (3.61)  Time: 0.998s, 1026.55/s  (1.012s, 1011.83/s)  LR: 6.381e-04  Data: 0.011 (0.014)
Train: 124 [ 700/1251 ( 56%)]  Loss: 3.739 (3.62)  Time: 0.999s, 1025.36/s  (1.011s, 1012.45/s)  LR: 6.381e-04  Data: 0.012 (0.014)
Train: 124 [ 750/1251 ( 60%)]  Loss: 3.781 (3.63)  Time: 0.996s, 1028.33/s  (1.011s, 1013.06/s)  LR: 6.381e-04  Data: 0.011 (0.014)
Train: 124 [ 800/1251 ( 64%)]  Loss: 3.667 (3.63)  Time: 0.995s, 1029.37/s  (1.010s, 1013.65/s)  LR: 6.381e-04  Data: 0.011 (0.013)
Train: 124 [ 850/1251 ( 68%)]  Loss: 3.715 (3.64)  Time: 1.003s, 1020.80/s  (1.010s, 1014.10/s)  LR: 6.381e-04  Data: 0.011 (0.013)
Train: 124 [ 900/1251 ( 72%)]  Loss: 3.514 (3.63)  Time: 0.995s, 1029.40/s  (1.010s, 1013.93/s)  LR: 6.381e-04  Data: 0.011 (0.013)
Train: 124 [ 950/1251 ( 76%)]  Loss: 3.716 (3.63)  Time: 0.996s, 1028.51/s  (1.010s, 1014.12/s)  LR: 6.381e-04  Data: 0.012 (0.013)
Train: 124 [1000/1251 ( 80%)]  Loss: 3.392 (3.62)  Time: 0.996s, 1028.44/s  (1.011s, 1012.88/s)  LR: 6.381e-04  Data: 0.011 (0.013)
Train: 124 [1050/1251 ( 84%)]  Loss: 3.971 (3.64)  Time: 0.994s, 1030.44/s  (1.011s, 1013.03/s)  LR: 6.381e-04  Data: 0.011 (0.013)
Train: 124 [1100/1251 ( 88%)]  Loss: 3.990 (3.65)  Time: 1.037s,  987.78/s  (1.011s, 1013.26/s)  LR: 6.381e-04  Data: 0.012 (0.013)
Train: 124 [1150/1251 ( 92%)]  Loss: 3.889 (3.66)  Time: 0.998s, 1026.48/s  (1.010s, 1013.65/s)  LR: 6.381e-04  Data: 0.011 (0.013)
Train: 124 [1200/1251 ( 96%)]  Loss: 4.105 (3.68)  Time: 1.004s, 1019.52/s  (1.010s, 1013.57/s)  LR: 6.381e-04  Data: 0.011 (0.013)
Train: 124 [1250/1251 (100%)]  Loss: 3.626 (3.68)  Time: 0.983s, 1042.17/s  (1.010s, 1013.55/s)  LR: 6.381e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.671 (1.671)  Loss:  0.8135 (0.8135)  Acc@1: 88.7695 (88.7695)  Acc@5: 96.2891 (96.2891)
Test: [  48/48]  Time: 0.245 (0.572)  Loss:  0.8588 (1.3735)  Acc@1: 86.2028 (73.4880)  Acc@5: 96.8160 (91.9160)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-122.pth.tar', 73.5220001171875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-124.pth.tar', 73.48799995361328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-119.pth.tar', 73.31400009765625)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-113.pth.tar', 73.291999921875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-114.pth.tar', 73.11199999023438)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-118.pth.tar', 73.08400000976563)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-120.pth.tar', 72.94599990966798)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-121.pth.tar', 72.9400001147461)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-116.pth.tar', 72.91600007080078)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-117.pth.tar', 72.82599991210938)

Train: 125 [   0/1251 (  0%)]  Loss: 3.836 (3.84)  Time: 2.471s,  414.36/s  (2.471s,  414.36/s)  LR: 6.331e-04  Data: 1.511 (1.511)
Train: 125 [  50/1251 (  4%)]  Loss: 3.762 (3.80)  Time: 1.043s,  981.71/s  (1.038s,  986.75/s)  LR: 6.331e-04  Data: 0.011 (0.042)
Train: 125 [ 100/1251 (  8%)]  Loss: 3.623 (3.74)  Time: 0.996s, 1028.56/s  (1.028s,  996.23/s)  LR: 6.331e-04  Data: 0.011 (0.027)
Train: 125 [ 150/1251 ( 12%)]  Loss: 3.807 (3.76)  Time: 0.995s, 1029.47/s  (1.019s, 1005.00/s)  LR: 6.331e-04  Data: 0.011 (0.022)
Train: 125 [ 200/1251 ( 16%)]  Loss: 3.688 (3.74)  Time: 0.997s, 1027.06/s  (1.015s, 1009.07/s)  LR: 6.331e-04  Data: 0.011 (0.019)
Train: 125 [ 250/1251 ( 20%)]  Loss: 3.576 (3.72)  Time: 0.995s, 1029.17/s  (1.011s, 1012.41/s)  LR: 6.331e-04  Data: 0.011 (0.017)
Train: 125 [ 300/1251 ( 24%)]  Loss: 3.658 (3.71)  Time: 0.996s, 1027.71/s  (1.010s, 1014.01/s)  LR: 6.331e-04  Data: 0.011 (0.016)
Train: 125 [ 350/1251 ( 28%)]  Loss: 3.580 (3.69)  Time: 1.000s, 1024.26/s  (1.010s, 1013.50/s)  LR: 6.331e-04  Data: 0.012 (0.016)
Train: 125 [ 400/1251 ( 32%)]  Loss: 3.479 (3.67)  Time: 1.015s, 1008.75/s  (1.010s, 1014.33/s)  LR: 6.331e-04  Data: 0.010 (0.015)
Train: 125 [ 450/1251 ( 36%)]  Loss: 3.615 (3.66)  Time: 0.988s, 1036.29/s  (1.009s, 1015.31/s)  LR: 6.331e-04  Data: 0.010 (0.015)
Train: 125 [ 500/1251 ( 40%)]  Loss: 3.666 (3.66)  Time: 0.997s, 1026.94/s  (1.008s, 1016.36/s)  LR: 6.331e-04  Data: 0.012 (0.014)
Train: 125 [ 550/1251 ( 44%)]  Loss: 3.571 (3.65)  Time: 0.994s, 1029.74/s  (1.008s, 1015.78/s)  LR: 6.331e-04  Data: 0.010 (0.014)
Train: 125 [ 600/1251 ( 48%)]  Loss: 3.387 (3.63)  Time: 0.999s, 1025.04/s  (1.008s, 1016.37/s)  LR: 6.331e-04  Data: 0.011 (0.014)
Train: 125 [ 650/1251 ( 52%)]  Loss: 3.537 (3.63)  Time: 0.997s, 1027.36/s  (1.007s, 1016.66/s)  LR: 6.331e-04  Data: 0.011 (0.014)
Train: 125 [ 700/1251 ( 56%)]  Loss: 3.700 (3.63)  Time: 0.996s, 1027.71/s  (1.008s, 1016.03/s)  LR: 6.331e-04  Data: 0.012 (0.013)
Train: 125 [ 750/1251 ( 60%)]  Loss: 3.693 (3.64)  Time: 0.998s, 1025.62/s  (1.007s, 1016.59/s)  LR: 6.331e-04  Data: 0.014 (0.013)
Train: 125 [ 800/1251 ( 64%)]  Loss: 3.574 (3.63)  Time: 1.025s,  999.50/s  (1.008s, 1015.98/s)  LR: 6.331e-04  Data: 0.012 (0.013)
Train: 125 [ 850/1251 ( 68%)]  Loss: 3.563 (3.63)  Time: 0.997s, 1026.93/s  (1.008s, 1016.10/s)  LR: 6.331e-04  Data: 0.012 (0.013)
Train: 125 [ 900/1251 ( 72%)]  Loss: 3.565 (3.63)  Time: 0.996s, 1028.59/s  (1.008s, 1015.98/s)  LR: 6.331e-04  Data: 0.012 (0.013)
Train: 125 [ 950/1251 ( 76%)]  Loss: 3.656 (3.63)  Time: 1.004s, 1020.40/s  (1.007s, 1016.47/s)  LR: 6.331e-04  Data: 0.011 (0.013)
Train: 125 [1000/1251 ( 80%)]  Loss: 3.378 (3.61)  Time: 0.996s, 1028.30/s  (1.008s, 1015.88/s)  LR: 6.331e-04  Data: 0.011 (0.013)
Train: 125 [1050/1251 ( 84%)]  Loss: 3.463 (3.61)  Time: 0.995s, 1029.07/s  (1.008s, 1016.36/s)  LR: 6.331e-04  Data: 0.012 (0.013)
Train: 125 [1100/1251 ( 88%)]  Loss: 3.868 (3.62)  Time: 0.993s, 1031.64/s  (1.008s, 1016.35/s)  LR: 6.331e-04  Data: 0.010 (0.013)
Train: 125 [1150/1251 ( 92%)]  Loss: 3.241 (3.60)  Time: 1.053s,  972.50/s  (1.007s, 1016.70/s)  LR: 6.331e-04  Data: 0.011 (0.013)
Train: 125 [1200/1251 ( 96%)]  Loss: 3.487 (3.60)  Time: 0.990s, 1034.84/s  (1.007s, 1017.07/s)  LR: 6.331e-04  Data: 0.011 (0.013)
Train: 125 [1250/1251 (100%)]  Loss: 3.383 (3.59)  Time: 0.983s, 1041.49/s  (1.007s, 1017.28/s)  LR: 6.331e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.615 (1.615)  Loss:  0.7925 (0.7925)  Acc@1: 89.5508 (89.5508)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.245 (0.566)  Loss:  0.9482 (1.3643)  Acc@1: 83.7264 (73.2280)  Acc@5: 95.5189 (91.7180)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-122.pth.tar', 73.5220001171875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-124.pth.tar', 73.48799995361328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-119.pth.tar', 73.31400009765625)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-113.pth.tar', 73.291999921875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-125.pth.tar', 73.2280000415039)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-114.pth.tar', 73.11199999023438)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-118.pth.tar', 73.08400000976563)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-120.pth.tar', 72.94599990966798)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-121.pth.tar', 72.9400001147461)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-116.pth.tar', 72.91600007080078)

Train: 126 [   0/1251 (  0%)]  Loss: 3.665 (3.67)  Time: 2.425s,  422.22/s  (2.425s,  422.22/s)  LR: 6.281e-04  Data: 1.459 (1.459)
Train: 126 [  50/1251 (  4%)]  Loss: 3.727 (3.70)  Time: 0.995s, 1028.90/s  (1.026s,  997.89/s)  LR: 6.281e-04  Data: 0.013 (0.040)
Train: 126 [ 100/1251 (  8%)]  Loss: 3.427 (3.61)  Time: 1.055s,  970.46/s  (1.022s, 1002.11/s)  LR: 6.281e-04  Data: 0.011 (0.025)
Train: 126 [ 150/1251 ( 12%)]  Loss: 3.431 (3.56)  Time: 0.994s, 1030.46/s  (1.016s, 1007.72/s)  LR: 6.281e-04  Data: 0.010 (0.021)
Train: 126 [ 200/1251 ( 16%)]  Loss: 3.633 (3.58)  Time: 1.000s, 1023.63/s  (1.013s, 1010.54/s)  LR: 6.281e-04  Data: 0.011 (0.018)
Train: 126 [ 250/1251 ( 20%)]  Loss: 3.896 (3.63)  Time: 0.997s, 1027.38/s  (1.011s, 1012.64/s)  LR: 6.281e-04  Data: 0.011 (0.017)
Train: 126 [ 300/1251 ( 24%)]  Loss: 3.404 (3.60)  Time: 0.996s, 1028.05/s  (1.009s, 1014.69/s)  LR: 6.281e-04  Data: 0.012 (0.016)
Train: 126 [ 350/1251 ( 28%)]  Loss: 3.371 (3.57)  Time: 0.996s, 1028.42/s  (1.007s, 1016.47/s)  LR: 6.281e-04  Data: 0.010 (0.015)
Train: 126 [ 400/1251 ( 32%)]  Loss: 3.964 (3.61)  Time: 1.053s,  972.77/s  (1.006s, 1017.58/s)  LR: 6.281e-04  Data: 0.011 (0.015)
Train: 126 [ 450/1251 ( 36%)]  Loss: 3.575 (3.61)  Time: 0.996s, 1027.74/s  (1.008s, 1016.17/s)  LR: 6.281e-04  Data: 0.011 (0.014)
Train: 126 [ 500/1251 ( 40%)]  Loss: 3.506 (3.60)  Time: 1.010s, 1014.13/s  (1.008s, 1016.09/s)  LR: 6.281e-04  Data: 0.011 (0.014)
Train: 126 [ 550/1251 ( 44%)]  Loss: 3.702 (3.61)  Time: 0.996s, 1028.13/s  (1.007s, 1016.89/s)  LR: 6.281e-04  Data: 0.011 (0.014)
Train: 126 [ 600/1251 ( 48%)]  Loss: 3.922 (3.63)  Time: 1.002s, 1022.28/s  (1.007s, 1016.79/s)  LR: 6.281e-04  Data: 0.011 (0.014)
Train: 126 [ 650/1251 ( 52%)]  Loss: 3.629 (3.63)  Time: 0.998s, 1026.06/s  (1.006s, 1017.48/s)  LR: 6.281e-04  Data: 0.011 (0.013)
Train: 126 [ 700/1251 ( 56%)]  Loss: 3.565 (3.63)  Time: 0.996s, 1027.79/s  (1.006s, 1017.45/s)  LR: 6.281e-04  Data: 0.011 (0.013)
Train: 126 [ 750/1251 ( 60%)]  Loss: 3.685 (3.63)  Time: 0.999s, 1025.49/s  (1.006s, 1017.92/s)  LR: 6.281e-04  Data: 0.011 (0.013)
Train: 126 [ 800/1251 ( 64%)]  Loss: 3.597 (3.63)  Time: 0.996s, 1028.14/s  (1.005s, 1018.50/s)  LR: 6.281e-04  Data: 0.012 (0.013)
Train: 126 [ 850/1251 ( 68%)]  Loss: 3.926 (3.65)  Time: 1.051s,  973.89/s  (1.007s, 1017.33/s)  LR: 6.281e-04  Data: 0.011 (0.013)
Train: 126 [ 900/1251 ( 72%)]  Loss: 3.557 (3.64)  Time: 1.047s,  977.80/s  (1.007s, 1016.70/s)  LR: 6.281e-04  Data: 0.012 (0.013)
Train: 126 [ 950/1251 ( 76%)]  Loss: 3.710 (3.64)  Time: 1.005s, 1018.98/s  (1.008s, 1015.82/s)  LR: 6.281e-04  Data: 0.011 (0.013)
Train: 126 [1000/1251 ( 80%)]  Loss: 3.421 (3.63)  Time: 0.996s, 1027.63/s  (1.008s, 1016.23/s)  LR: 6.281e-04  Data: 0.012 (0.013)
Train: 126 [1050/1251 ( 84%)]  Loss: 3.579 (3.63)  Time: 0.997s, 1027.09/s  (1.007s, 1016.76/s)  LR: 6.281e-04  Data: 0.012 (0.012)
Train: 126 [1100/1251 ( 88%)]  Loss: 3.305 (3.62)  Time: 0.995s, 1029.06/s  (1.007s, 1017.14/s)  LR: 6.281e-04  Data: 0.010 (0.012)
Train: 126 [1150/1251 ( 92%)]  Loss: 3.471 (3.61)  Time: 1.003s, 1021.11/s  (1.006s, 1017.57/s)  LR: 6.281e-04  Data: 0.011 (0.012)
Train: 126 [1200/1251 ( 96%)]  Loss: 3.438 (3.60)  Time: 0.994s, 1029.70/s  (1.006s, 1017.92/s)  LR: 6.281e-04  Data: 0.011 (0.012)
Train: 126 [1250/1251 (100%)]  Loss: 3.241 (3.59)  Time: 0.981s, 1043.91/s  (1.006s, 1018.24/s)  LR: 6.281e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.758 (1.758)  Loss:  0.8989 (0.8989)  Acc@1: 87.9883 (87.9883)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.245 (0.575)  Loss:  0.8458 (1.3530)  Acc@1: 83.0189 (73.1000)  Acc@5: 96.2264 (91.5720)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-122.pth.tar', 73.5220001171875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-124.pth.tar', 73.48799995361328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-119.pth.tar', 73.31400009765625)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-113.pth.tar', 73.291999921875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-125.pth.tar', 73.2280000415039)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-114.pth.tar', 73.11199999023438)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-126.pth.tar', 73.09999999267578)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-118.pth.tar', 73.08400000976563)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-120.pth.tar', 72.94599990966798)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-121.pth.tar', 72.9400001147461)

Train: 127 [   0/1251 (  0%)]  Loss: 3.596 (3.60)  Time: 2.509s,  408.18/s  (2.509s,  408.18/s)  LR: 6.231e-04  Data: 1.550 (1.550)
Train: 127 [  50/1251 (  4%)]  Loss: 3.836 (3.72)  Time: 0.998s, 1026.24/s  (1.035s,  989.06/s)  LR: 6.231e-04  Data: 0.011 (0.045)
Train: 127 [ 100/1251 (  8%)]  Loss: 3.713 (3.71)  Time: 0.995s, 1029.63/s  (1.017s, 1007.02/s)  LR: 6.231e-04  Data: 0.011 (0.028)
Train: 127 [ 150/1251 ( 12%)]  Loss: 3.774 (3.73)  Time: 0.999s, 1024.53/s  (1.011s, 1012.85/s)  LR: 6.231e-04  Data: 0.011 (0.022)
Train: 127 [ 200/1251 ( 16%)]  Loss: 3.504 (3.68)  Time: 0.995s, 1029.65/s  (1.007s, 1016.62/s)  LR: 6.231e-04  Data: 0.011 (0.020)
Train: 127 [ 250/1251 ( 20%)]  Loss: 3.723 (3.69)  Time: 0.998s, 1026.30/s  (1.006s, 1018.17/s)  LR: 6.231e-04  Data: 0.012 (0.018)
Train: 127 [ 300/1251 ( 24%)]  Loss: 3.505 (3.66)  Time: 1.012s, 1012.02/s  (1.005s, 1019.12/s)  LR: 6.231e-04  Data: 0.010 (0.017)
Train: 127 [ 350/1251 ( 28%)]  Loss: 3.238 (3.61)  Time: 0.998s, 1025.87/s  (1.005s, 1019.06/s)  LR: 6.231e-04  Data: 0.011 (0.016)
Train: 127 [ 400/1251 ( 32%)]  Loss: 3.928 (3.65)  Time: 0.997s, 1026.75/s  (1.005s, 1019.12/s)  LR: 6.231e-04  Data: 0.012 (0.015)
Train: 127 [ 450/1251 ( 36%)]  Loss: 3.677 (3.65)  Time: 0.998s, 1026.00/s  (1.004s, 1019.60/s)  LR: 6.231e-04  Data: 0.011 (0.015)
Train: 127 [ 500/1251 ( 40%)]  Loss: 3.988 (3.68)  Time: 0.994s, 1030.07/s  (1.005s, 1018.49/s)  LR: 6.231e-04  Data: 0.011 (0.015)
Train: 127 [ 550/1251 ( 44%)]  Loss: 3.813 (3.69)  Time: 0.990s, 1034.69/s  (1.005s, 1018.81/s)  LR: 6.231e-04  Data: 0.011 (0.014)
Train: 127 [ 600/1251 ( 48%)]  Loss: 3.721 (3.69)  Time: 0.994s, 1029.93/s  (1.004s, 1019.45/s)  LR: 6.231e-04  Data: 0.010 (0.014)
Train: 127 [ 650/1251 ( 52%)]  Loss: 3.913 (3.71)  Time: 0.993s, 1030.77/s  (1.005s, 1018.52/s)  LR: 6.231e-04  Data: 0.011 (0.014)
Train: 127 [ 700/1251 ( 56%)]  Loss: 3.651 (3.71)  Time: 1.020s, 1003.56/s  (1.005s, 1019.11/s)  LR: 6.231e-04  Data: 0.010 (0.014)
Train: 127 [ 750/1251 ( 60%)]  Loss: 3.461 (3.69)  Time: 0.995s, 1028.92/s  (1.004s, 1019.52/s)  LR: 6.231e-04  Data: 0.011 (0.013)
Train: 127 [ 800/1251 ( 64%)]  Loss: 3.696 (3.69)  Time: 0.997s, 1026.73/s  (1.004s, 1019.96/s)  LR: 6.231e-04  Data: 0.011 (0.013)
Train: 127 [ 850/1251 ( 68%)]  Loss: 3.561 (3.68)  Time: 0.994s, 1030.28/s  (1.004s, 1020.18/s)  LR: 6.231e-04  Data: 0.011 (0.013)
Train: 127 [ 900/1251 ( 72%)]  Loss: 3.615 (3.68)  Time: 1.037s,  987.02/s  (1.005s, 1019.02/s)  LR: 6.231e-04  Data: 0.011 (0.013)
Train: 127 [ 950/1251 ( 76%)]  Loss: 3.477 (3.67)  Time: 0.991s, 1033.22/s  (1.005s, 1019.13/s)  LR: 6.231e-04  Data: 0.010 (0.013)
Train: 127 [1000/1251 ( 80%)]  Loss: 3.714 (3.67)  Time: 1.002s, 1022.45/s  (1.005s, 1019.33/s)  LR: 6.231e-04  Data: 0.011 (0.013)
Train: 127 [1050/1251 ( 84%)]  Loss: 3.775 (3.68)  Time: 0.997s, 1026.95/s  (1.005s, 1019.38/s)  LR: 6.231e-04  Data: 0.012 (0.013)
Train: 127 [1100/1251 ( 88%)]  Loss: 3.258 (3.66)  Time: 0.999s, 1025.21/s  (1.005s, 1019.35/s)  LR: 6.231e-04  Data: 0.011 (0.013)
Train: 127 [1150/1251 ( 92%)]  Loss: 3.609 (3.66)  Time: 1.036s,  988.26/s  (1.004s, 1019.54/s)  LR: 6.231e-04  Data: 0.011 (0.013)
Train: 127 [1200/1251 ( 96%)]  Loss: 3.678 (3.66)  Time: 0.997s, 1027.50/s  (1.005s, 1019.33/s)  LR: 6.231e-04  Data: 0.011 (0.013)
Train: 127 [1250/1251 (100%)]  Loss: 3.498 (3.65)  Time: 0.981s, 1043.64/s  (1.004s, 1019.45/s)  LR: 6.231e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.633 (1.633)  Loss:  0.8207 (0.8207)  Acc@1: 87.0117 (87.0117)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.245 (0.567)  Loss:  0.9487 (1.3860)  Acc@1: 82.9010 (73.3220)  Acc@5: 95.5189 (91.7820)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-122.pth.tar', 73.5220001171875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-124.pth.tar', 73.48799995361328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-127.pth.tar', 73.32200020019532)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-119.pth.tar', 73.31400009765625)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-113.pth.tar', 73.291999921875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-125.pth.tar', 73.2280000415039)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-114.pth.tar', 73.11199999023438)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-126.pth.tar', 73.09999999267578)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-118.pth.tar', 73.08400000976563)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-120.pth.tar', 72.94599990966798)

Train: 128 [   0/1251 (  0%)]  Loss: 3.911 (3.91)  Time: 2.487s,  411.78/s  (2.487s,  411.78/s)  LR: 6.180e-04  Data: 1.520 (1.520)
Train: 128 [  50/1251 (  4%)]  Loss: 3.490 (3.70)  Time: 1.036s,  988.01/s  (1.036s,  988.68/s)  LR: 6.180e-04  Data: 0.011 (0.041)
Train: 128 [ 100/1251 (  8%)]  Loss: 3.561 (3.65)  Time: 0.993s, 1030.78/s  (1.024s,  999.54/s)  LR: 6.180e-04  Data: 0.011 (0.026)
Train: 128 [ 150/1251 ( 12%)]  Loss: 3.863 (3.71)  Time: 0.998s, 1026.19/s  (1.016s, 1008.08/s)  LR: 6.180e-04  Data: 0.012 (0.021)
Train: 128 [ 200/1251 ( 16%)]  Loss: 3.516 (3.67)  Time: 0.998s, 1026.46/s  (1.012s, 1012.08/s)  LR: 6.180e-04  Data: 0.012 (0.019)
Train: 128 [ 250/1251 ( 20%)]  Loss: 3.911 (3.71)  Time: 0.993s, 1031.33/s  (1.009s, 1014.61/s)  LR: 6.180e-04  Data: 0.011 (0.017)
Train: 128 [ 300/1251 ( 24%)]  Loss: 3.666 (3.70)  Time: 0.997s, 1026.90/s  (1.008s, 1016.26/s)  LR: 6.180e-04  Data: 0.011 (0.016)
Train: 128 [ 350/1251 ( 28%)]  Loss: 3.245 (3.65)  Time: 0.995s, 1028.95/s  (1.006s, 1017.73/s)  LR: 6.180e-04  Data: 0.011 (0.016)
Train: 128 [ 400/1251 ( 32%)]  Loss: 3.727 (3.65)  Time: 1.005s, 1018.66/s  (1.005s, 1018.70/s)  LR: 6.180e-04  Data: 0.014 (0.015)
Train: 128 [ 450/1251 ( 36%)]  Loss: 3.522 (3.64)  Time: 1.034s,  990.40/s  (1.006s, 1018.06/s)  LR: 6.180e-04  Data: 0.011 (0.015)
Train: 128 [ 500/1251 ( 40%)]  Loss: 3.458 (3.62)  Time: 1.048s,  977.08/s  (1.006s, 1017.44/s)  LR: 6.180e-04  Data: 0.012 (0.014)
Train: 128 [ 550/1251 ( 44%)]  Loss: 4.082 (3.66)  Time: 0.998s, 1026.07/s  (1.008s, 1016.05/s)  LR: 6.180e-04  Data: 0.011 (0.014)
Train: 128 [ 600/1251 ( 48%)]  Loss: 3.776 (3.67)  Time: 0.997s, 1026.79/s  (1.008s, 1016.27/s)  LR: 6.180e-04  Data: 0.012 (0.014)
Train: 128 [ 650/1251 ( 52%)]  Loss: 3.511 (3.66)  Time: 1.052s,  973.56/s  (1.008s, 1016.05/s)  LR: 6.180e-04  Data: 0.010 (0.014)
Train: 128 [ 700/1251 ( 56%)]  Loss: 3.310 (3.64)  Time: 0.993s, 1030.81/s  (1.007s, 1016.76/s)  LR: 6.180e-04  Data: 0.011 (0.013)
Train: 128 [ 750/1251 ( 60%)]  Loss: 3.382 (3.62)  Time: 0.998s, 1025.57/s  (1.007s, 1017.31/s)  LR: 6.180e-04  Data: 0.012 (0.013)
Train: 128 [ 800/1251 ( 64%)]  Loss: 3.594 (3.62)  Time: 0.994s, 1029.87/s  (1.006s, 1017.75/s)  LR: 6.180e-04  Data: 0.010 (0.013)
Train: 128 [ 850/1251 ( 68%)]  Loss: 3.741 (3.63)  Time: 1.002s, 1021.90/s  (1.006s, 1018.35/s)  LR: 6.180e-04  Data: 0.011 (0.013)
Train: 128 [ 900/1251 ( 72%)]  Loss: 3.518 (3.62)  Time: 0.995s, 1029.19/s  (1.005s, 1018.67/s)  LR: 6.180e-04  Data: 0.012 (0.013)
Train: 128 [ 950/1251 ( 76%)]  Loss: 3.733 (3.63)  Time: 0.995s, 1029.02/s  (1.005s, 1018.83/s)  LR: 6.180e-04  Data: 0.010 (0.013)
Train: 128 [1000/1251 ( 80%)]  Loss: 3.754 (3.63)  Time: 1.023s, 1000.97/s  (1.006s, 1018.13/s)  LR: 6.180e-04  Data: 0.011 (0.013)
Train: 128 [1050/1251 ( 84%)]  Loss: 3.743 (3.64)  Time: 0.994s, 1029.96/s  (1.006s, 1018.35/s)  LR: 6.180e-04  Data: 0.012 (0.013)
Train: 128 [1100/1251 ( 88%)]  Loss: 3.966 (3.65)  Time: 1.045s,  979.46/s  (1.006s, 1018.23/s)  LR: 6.180e-04  Data: 0.011 (0.013)
Train: 128 [1150/1251 ( 92%)]  Loss: 3.667 (3.65)  Time: 0.992s, 1031.83/s  (1.006s, 1017.61/s)  LR: 6.180e-04  Data: 0.010 (0.013)
Train: 128 [1200/1251 ( 96%)]  Loss: 3.467 (3.64)  Time: 1.083s,  945.59/s  (1.006s, 1017.73/s)  LR: 6.180e-04  Data: 0.011 (0.012)
Train: 128 [1250/1251 (100%)]  Loss: 3.506 (3.64)  Time: 0.983s, 1041.93/s  (1.007s, 1017.37/s)  LR: 6.180e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.674 (1.674)  Loss:  0.8844 (0.8844)  Acc@1: 87.0117 (87.0117)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.245 (0.571)  Loss:  0.8867 (1.3685)  Acc@1: 84.6698 (73.4920)  Acc@5: 95.9906 (91.9660)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-122.pth.tar', 73.5220001171875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-128.pth.tar', 73.49200006347657)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-124.pth.tar', 73.48799995361328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-127.pth.tar', 73.32200020019532)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-119.pth.tar', 73.31400009765625)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-113.pth.tar', 73.291999921875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-125.pth.tar', 73.2280000415039)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-114.pth.tar', 73.11199999023438)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-126.pth.tar', 73.09999999267578)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-118.pth.tar', 73.08400000976563)

Train: 129 [   0/1251 (  0%)]  Loss: 3.840 (3.84)  Time: 4.180s,  244.96/s  (4.180s,  244.96/s)  LR: 6.130e-04  Data: 3.045 (3.045)
Train: 129 [  50/1251 (  4%)]  Loss: 3.576 (3.71)  Time: 1.000s, 1024.32/s  (1.069s,  958.31/s)  LR: 6.130e-04  Data: 0.011 (0.071)
Train: 129 [ 100/1251 (  8%)]  Loss: 3.620 (3.68)  Time: 0.995s, 1029.18/s  (1.036s,  988.27/s)  LR: 6.130e-04  Data: 0.011 (0.041)
Train: 129 [ 150/1251 ( 12%)]  Loss: 3.576 (3.65)  Time: 0.995s, 1028.69/s  (1.024s,  999.92/s)  LR: 6.130e-04  Data: 0.011 (0.031)
Train: 129 [ 200/1251 ( 16%)]  Loss: 3.456 (3.61)  Time: 1.003s, 1021.28/s  (1.020s, 1003.82/s)  LR: 6.130e-04  Data: 0.011 (0.026)
Train: 129 [ 250/1251 ( 20%)]  Loss: 3.643 (3.62)  Time: 0.997s, 1026.88/s  (1.017s, 1006.91/s)  LR: 6.130e-04  Data: 0.012 (0.023)
Train: 129 [ 300/1251 ( 24%)]  Loss: 4.077 (3.68)  Time: 1.001s, 1022.74/s  (1.014s, 1009.82/s)  LR: 6.130e-04  Data: 0.011 (0.021)
Train: 129 [ 350/1251 ( 28%)]  Loss: 3.508 (3.66)  Time: 0.997s, 1026.91/s  (1.012s, 1012.22/s)  LR: 6.130e-04  Data: 0.012 (0.020)
Train: 129 [ 400/1251 ( 32%)]  Loss: 3.577 (3.65)  Time: 1.000s, 1024.03/s  (1.011s, 1012.61/s)  LR: 6.130e-04  Data: 0.010 (0.019)
Train: 129 [ 450/1251 ( 36%)]  Loss: 3.936 (3.68)  Time: 1.058s,  967.41/s  (1.011s, 1012.99/s)  LR: 6.130e-04  Data: 0.011 (0.018)
Train: 129 [ 500/1251 ( 40%)]  Loss: 3.660 (3.68)  Time: 0.995s, 1029.23/s  (1.010s, 1014.14/s)  LR: 6.130e-04  Data: 0.011 (0.017)
Train: 129 [ 550/1251 ( 44%)]  Loss: 4.004 (3.71)  Time: 1.031s,  992.91/s  (1.009s, 1014.93/s)  LR: 6.130e-04  Data: 0.011 (0.017)
Train: 129 [ 600/1251 ( 48%)]  Loss: 3.392 (3.68)  Time: 0.993s, 1031.05/s  (1.009s, 1015.07/s)  LR: 6.130e-04  Data: 0.011 (0.016)
Train: 129 [ 650/1251 ( 52%)]  Loss: 3.653 (3.68)  Time: 0.995s, 1029.49/s  (1.008s, 1015.56/s)  LR: 6.130e-04  Data: 0.011 (0.016)
Train: 129 [ 700/1251 ( 56%)]  Loss: 3.645 (3.68)  Time: 1.006s, 1018.11/s  (1.009s, 1015.30/s)  LR: 6.130e-04  Data: 0.012 (0.015)
Train: 129 [ 750/1251 ( 60%)]  Loss: 3.774 (3.68)  Time: 1.002s, 1022.37/s  (1.008s, 1015.78/s)  LR: 6.130e-04  Data: 0.011 (0.015)
Train: 129 [ 800/1251 ( 64%)]  Loss: 3.616 (3.68)  Time: 0.996s, 1027.98/s  (1.008s, 1016.38/s)  LR: 6.130e-04  Data: 0.011 (0.015)
Train: 129 [ 850/1251 ( 68%)]  Loss: 3.548 (3.67)  Time: 0.995s, 1028.63/s  (1.007s, 1016.71/s)  LR: 6.130e-04  Data: 0.011 (0.015)
Train: 129 [ 900/1251 ( 72%)]  Loss: 3.263 (3.65)  Time: 0.996s, 1028.39/s  (1.007s, 1017.11/s)  LR: 6.130e-04  Data: 0.011 (0.015)
Train: 129 [ 950/1251 ( 76%)]  Loss: 3.669 (3.65)  Time: 0.994s, 1029.68/s  (1.006s, 1017.49/s)  LR: 6.130e-04  Data: 0.011 (0.014)
Train: 129 [1000/1251 ( 80%)]  Loss: 3.849 (3.66)  Time: 0.994s, 1029.67/s  (1.006s, 1017.58/s)  LR: 6.130e-04  Data: 0.011 (0.014)
Train: 129 [1050/1251 ( 84%)]  Loss: 3.626 (3.66)  Time: 0.991s, 1033.76/s  (1.006s, 1017.78/s)  LR: 6.130e-04  Data: 0.010 (0.014)
Train: 129 [1100/1251 ( 88%)]  Loss: 3.516 (3.65)  Time: 0.996s, 1028.50/s  (1.006s, 1018.04/s)  LR: 6.130e-04  Data: 0.011 (0.014)
Train: 129 [1150/1251 ( 92%)]  Loss: 3.485 (3.65)  Time: 0.994s, 1030.66/s  (1.006s, 1018.29/s)  LR: 6.130e-04  Data: 0.011 (0.014)
Train: 129 [1200/1251 ( 96%)]  Loss: 3.846 (3.65)  Time: 0.993s, 1031.03/s  (1.005s, 1018.60/s)  LR: 6.130e-04  Data: 0.010 (0.014)
Train: 129 [1250/1251 (100%)]  Loss: 3.639 (3.65)  Time: 0.983s, 1041.25/s  (1.005s, 1018.85/s)  LR: 6.130e-04  Data: 0.000 (0.014)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.669 (1.669)  Loss:  0.9803 (0.9803)  Acc@1: 89.1602 (89.1602)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.245 (0.575)  Loss:  0.9783 (1.4991)  Acc@1: 84.9057 (73.0980)  Acc@5: 96.1085 (91.4580)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-122.pth.tar', 73.5220001171875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-128.pth.tar', 73.49200006347657)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-124.pth.tar', 73.48799995361328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-127.pth.tar', 73.32200020019532)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-119.pth.tar', 73.31400009765625)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-113.pth.tar', 73.291999921875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-125.pth.tar', 73.2280000415039)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-114.pth.tar', 73.11199999023438)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-126.pth.tar', 73.09999999267578)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-129.pth.tar', 73.09800003662109)

Train: 130 [   0/1251 (  0%)]  Loss: 3.650 (3.65)  Time: 2.431s,  421.15/s  (2.431s,  421.15/s)  LR: 6.079e-04  Data: 1.478 (1.478)
Train: 130 [  50/1251 (  4%)]  Loss: 3.725 (3.69)  Time: 1.030s,  994.01/s  (1.043s,  981.81/s)  LR: 6.079e-04  Data: 0.011 (0.042)
Train: 130 [ 100/1251 (  8%)]  Loss: 3.591 (3.66)  Time: 0.996s, 1028.00/s  (1.032s,  992.50/s)  LR: 6.079e-04  Data: 0.011 (0.027)
Train: 130 [ 150/1251 ( 12%)]  Loss: 3.600 (3.64)  Time: 0.997s, 1027.45/s  (1.021s, 1002.60/s)  LR: 6.079e-04  Data: 0.011 (0.022)
Train: 130 [ 200/1251 ( 16%)]  Loss: 3.480 (3.61)  Time: 0.997s, 1027.17/s  (1.015s, 1008.59/s)  LR: 6.079e-04  Data: 0.011 (0.019)
Train: 130 [ 250/1251 ( 20%)]  Loss: 3.483 (3.59)  Time: 0.994s, 1029.73/s  (1.015s, 1009.06/s)  LR: 6.079e-04  Data: 0.011 (0.017)
Train: 130 [ 300/1251 ( 24%)]  Loss: 3.906 (3.63)  Time: 0.995s, 1029.46/s  (1.012s, 1011.64/s)  LR: 6.079e-04  Data: 0.011 (0.016)
Train: 130 [ 350/1251 ( 28%)]  Loss: 3.633 (3.63)  Time: 0.993s, 1031.51/s  (1.010s, 1014.02/s)  LR: 6.079e-04  Data: 0.010 (0.016)
Train: 130 [ 400/1251 ( 32%)]  Loss: 3.523 (3.62)  Time: 0.997s, 1027.10/s  (1.008s, 1015.73/s)  LR: 6.079e-04  Data: 0.011 (0.015)
Train: 130 [ 450/1251 ( 36%)]  Loss: 3.086 (3.57)  Time: 1.001s, 1022.81/s  (1.008s, 1016.34/s)  LR: 6.079e-04  Data: 0.011 (0.015)
Train: 130 [ 500/1251 ( 40%)]  Loss: 3.734 (3.58)  Time: 0.996s, 1027.78/s  (1.008s, 1016.35/s)  LR: 6.079e-04  Data: 0.011 (0.014)
Train: 130 [ 550/1251 ( 44%)]  Loss: 3.687 (3.59)  Time: 1.064s,  962.75/s  (1.008s, 1015.56/s)  LR: 6.079e-04  Data: 0.011 (0.014)
Train: 130 [ 600/1251 ( 48%)]  Loss: 4.024 (3.62)  Time: 0.995s, 1029.40/s  (1.009s, 1015.08/s)  LR: 6.079e-04  Data: 0.011 (0.014)
Train: 130 [ 650/1251 ( 52%)]  Loss: 3.505 (3.62)  Time: 0.996s, 1027.96/s  (1.008s, 1015.86/s)  LR: 6.079e-04  Data: 0.011 (0.013)
Train: 130 [ 700/1251 ( 56%)]  Loss: 3.843 (3.63)  Time: 0.993s, 1030.72/s  (1.008s, 1015.40/s)  LR: 6.079e-04  Data: 0.011 (0.013)
Train: 130 [ 750/1251 ( 60%)]  Loss: 3.493 (3.62)  Time: 1.008s, 1015.37/s  (1.008s, 1015.68/s)  LR: 6.079e-04  Data: 0.011 (0.013)
Train: 130 [ 800/1251 ( 64%)]  Loss: 3.562 (3.62)  Time: 0.996s, 1028.38/s  (1.008s, 1015.60/s)  LR: 6.079e-04  Data: 0.011 (0.013)
Train: 130 [ 850/1251 ( 68%)]  Loss: 3.683 (3.62)  Time: 1.031s,  993.24/s  (1.009s, 1015.30/s)  LR: 6.079e-04  Data: 0.011 (0.013)
Train: 130 [ 900/1251 ( 72%)]  Loss: 3.504 (3.62)  Time: 1.000s, 1023.65/s  (1.009s, 1015.29/s)  LR: 6.079e-04  Data: 0.010 (0.013)
Train: 130 [ 950/1251 ( 76%)]  Loss: 3.644 (3.62)  Time: 0.995s, 1029.14/s  (1.008s, 1015.52/s)  LR: 6.079e-04  Data: 0.011 (0.013)
Train: 130 [1000/1251 ( 80%)]  Loss: 3.239 (3.60)  Time: 0.996s, 1027.91/s  (1.008s, 1016.02/s)  LR: 6.079e-04  Data: 0.011 (0.013)
Train: 130 [1050/1251 ( 84%)]  Loss: 3.557 (3.60)  Time: 0.997s, 1027.18/s  (1.008s, 1015.88/s)  LR: 6.079e-04  Data: 0.011 (0.012)
Train: 130 [1100/1251 ( 88%)]  Loss: 3.078 (3.58)  Time: 1.002s, 1022.00/s  (1.008s, 1016.05/s)  LR: 6.079e-04  Data: 0.011 (0.012)
Train: 130 [1150/1251 ( 92%)]  Loss: 3.727 (3.58)  Time: 0.996s, 1027.82/s  (1.008s, 1016.26/s)  LR: 6.079e-04  Data: 0.012 (0.012)
Train: 130 [1200/1251 ( 96%)]  Loss: 3.438 (3.58)  Time: 0.995s, 1029.62/s  (1.007s, 1016.48/s)  LR: 6.079e-04  Data: 0.011 (0.012)
Train: 130 [1250/1251 (100%)]  Loss: 3.574 (3.58)  Time: 0.984s, 1040.52/s  (1.007s, 1016.82/s)  LR: 6.079e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.618 (1.618)  Loss:  0.7983 (0.7983)  Acc@1: 89.2578 (89.2578)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.245 (0.569)  Loss:  0.8927 (1.3090)  Acc@1: 83.2547 (73.5660)  Acc@5: 95.8726 (92.0100)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-130.pth.tar', 73.56600009521485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-122.pth.tar', 73.5220001171875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-128.pth.tar', 73.49200006347657)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-124.pth.tar', 73.48799995361328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-127.pth.tar', 73.32200020019532)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-119.pth.tar', 73.31400009765625)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-113.pth.tar', 73.291999921875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-125.pth.tar', 73.2280000415039)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-114.pth.tar', 73.11199999023438)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-126.pth.tar', 73.09999999267578)

Train: 131 [   0/1251 (  0%)]  Loss: 3.619 (3.62)  Time: 2.528s,  404.99/s  (2.528s,  404.99/s)  LR: 6.028e-04  Data: 1.557 (1.557)
Train: 131 [  50/1251 (  4%)]  Loss: 4.057 (3.84)  Time: 0.996s, 1028.24/s  (1.034s,  990.40/s)  LR: 6.028e-04  Data: 0.011 (0.042)
Train: 131 [ 100/1251 (  8%)]  Loss: 3.606 (3.76)  Time: 1.029s,  994.74/s  (1.019s, 1004.47/s)  LR: 6.028e-04  Data: 0.012 (0.027)
Train: 131 [ 150/1251 ( 12%)]  Loss: 3.689 (3.74)  Time: 0.995s, 1029.42/s  (1.013s, 1011.30/s)  LR: 6.028e-04  Data: 0.010 (0.021)
Train: 131 [ 200/1251 ( 16%)]  Loss: 3.464 (3.69)  Time: 1.035s,  989.64/s  (1.014s, 1010.14/s)  LR: 6.028e-04  Data: 0.012 (0.019)
Train: 131 [ 250/1251 ( 20%)]  Loss: 3.684 (3.69)  Time: 1.001s, 1022.90/s  (1.015s, 1009.30/s)  LR: 6.028e-04  Data: 0.011 (0.017)
Train: 131 [ 300/1251 ( 24%)]  Loss: 3.851 (3.71)  Time: 1.009s, 1015.19/s  (1.014s, 1009.39/s)  LR: 6.028e-04  Data: 0.010 (0.016)
Train: 131 [ 350/1251 ( 28%)]  Loss: 3.491 (3.68)  Time: 0.994s, 1030.03/s  (1.014s, 1010.13/s)  LR: 6.028e-04  Data: 0.011 (0.016)
Train: 131 [ 400/1251 ( 32%)]  Loss: 3.959 (3.71)  Time: 0.997s, 1026.96/s  (1.012s, 1011.70/s)  LR: 6.028e-04  Data: 0.011 (0.015)
Train: 131 [ 450/1251 ( 36%)]  Loss: 3.609 (3.70)  Time: 0.990s, 1034.23/s  (1.011s, 1013.32/s)  LR: 6.028e-04  Data: 0.011 (0.015)
Train: 131 [ 500/1251 ( 40%)]  Loss: 3.390 (3.67)  Time: 1.048s,  977.15/s  (1.011s, 1013.14/s)  LR: 6.028e-04  Data: 0.010 (0.014)
Train: 131 [ 550/1251 ( 44%)]  Loss: 3.737 (3.68)  Time: 0.993s, 1030.72/s  (1.010s, 1014.05/s)  LR: 6.028e-04  Data: 0.011 (0.014)
Train: 131 [ 600/1251 ( 48%)]  Loss: 3.236 (3.65)  Time: 0.995s, 1029.38/s  (1.009s, 1015.05/s)  LR: 6.028e-04  Data: 0.011 (0.014)
Train: 131 [ 650/1251 ( 52%)]  Loss: 3.836 (3.66)  Time: 0.993s, 1030.88/s  (1.008s, 1015.85/s)  LR: 6.028e-04  Data: 0.011 (0.013)
Train: 131 [ 700/1251 ( 56%)]  Loss: 3.664 (3.66)  Time: 0.990s, 1034.15/s  (1.008s, 1016.17/s)  LR: 6.028e-04  Data: 0.011 (0.013)
Train: 131 [ 750/1251 ( 60%)]  Loss: 2.875 (3.61)  Time: 0.995s, 1029.08/s  (1.008s, 1016.34/s)  LR: 6.028e-04  Data: 0.010 (0.013)
Train: 131 [ 800/1251 ( 64%)]  Loss: 3.783 (3.62)  Time: 0.997s, 1027.16/s  (1.008s, 1016.03/s)  LR: 6.028e-04  Data: 0.010 (0.013)
Train: 131 [ 850/1251 ( 68%)]  Loss: 3.590 (3.62)  Time: 0.995s, 1028.67/s  (1.007s, 1016.40/s)  LR: 6.028e-04  Data: 0.011 (0.013)
Train: 131 [ 900/1251 ( 72%)]  Loss: 4.005 (3.64)  Time: 0.997s, 1027.30/s  (1.008s, 1016.13/s)  LR: 6.028e-04  Data: 0.011 (0.013)
Train: 131 [ 950/1251 ( 76%)]  Loss: 4.079 (3.66)  Time: 0.996s, 1028.12/s  (1.007s, 1016.76/s)  LR: 6.028e-04  Data: 0.012 (0.013)
Train: 131 [1000/1251 ( 80%)]  Loss: 3.395 (3.65)  Time: 0.997s, 1027.53/s  (1.007s, 1016.95/s)  LR: 6.028e-04  Data: 0.011 (0.013)
Train: 131 [1050/1251 ( 84%)]  Loss: 3.769 (3.65)  Time: 1.004s, 1019.63/s  (1.007s, 1017.08/s)  LR: 6.028e-04  Data: 0.015 (0.013)
Train: 131 [1100/1251 ( 88%)]  Loss: 3.500 (3.65)  Time: 0.994s, 1030.38/s  (1.007s, 1017.23/s)  LR: 6.028e-04  Data: 0.011 (0.012)
Train: 131 [1150/1251 ( 92%)]  Loss: 3.846 (3.66)  Time: 1.004s, 1019.51/s  (1.006s, 1017.63/s)  LR: 6.028e-04  Data: 0.011 (0.012)
Train: 131 [1200/1251 ( 96%)]  Loss: 3.660 (3.66)  Time: 1.041s,  983.54/s  (1.006s, 1017.69/s)  LR: 6.028e-04  Data: 0.011 (0.012)
Train: 131 [1250/1251 (100%)]  Loss: 3.870 (3.66)  Time: 1.020s, 1004.08/s  (1.006s, 1017.85/s)  LR: 6.028e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.642 (1.642)  Loss:  0.7145 (0.7145)  Acc@1: 89.0625 (89.0625)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.245 (0.568)  Loss:  0.7704 (1.2822)  Acc@1: 84.9057 (73.8780)  Acc@5: 96.9340 (91.9940)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-131.pth.tar', 73.87800016601562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-130.pth.tar', 73.56600009521485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-122.pth.tar', 73.5220001171875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-128.pth.tar', 73.49200006347657)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-124.pth.tar', 73.48799995361328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-127.pth.tar', 73.32200020019532)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-119.pth.tar', 73.31400009765625)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-113.pth.tar', 73.291999921875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-125.pth.tar', 73.2280000415039)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-114.pth.tar', 73.11199999023438)

Train: 132 [   0/1251 (  0%)]  Loss: 3.615 (3.62)  Time: 2.503s,  409.05/s  (2.503s,  409.05/s)  LR: 5.978e-04  Data: 1.542 (1.542)
Train: 132 [  50/1251 (  4%)]  Loss: 3.603 (3.61)  Time: 0.996s, 1027.60/s  (1.028s,  995.85/s)  LR: 5.978e-04  Data: 0.012 (0.041)
Train: 132 [ 100/1251 (  8%)]  Loss: 3.661 (3.63)  Time: 1.002s, 1021.86/s  (1.027s,  996.72/s)  LR: 5.978e-04  Data: 0.010 (0.026)
Train: 132 [ 150/1251 ( 12%)]  Loss: 3.165 (3.51)  Time: 0.995s, 1029.16/s  (1.020s, 1003.73/s)  LR: 5.978e-04  Data: 0.010 (0.021)
Train: 132 [ 200/1251 ( 16%)]  Loss: 3.799 (3.57)  Time: 1.036s,  988.84/s  (1.017s, 1007.31/s)  LR: 5.978e-04  Data: 0.011 (0.019)
Train: 132 [ 250/1251 ( 20%)]  Loss: 3.775 (3.60)  Time: 0.999s, 1025.53/s  (1.017s, 1007.06/s)  LR: 5.978e-04  Data: 0.011 (0.017)
Train: 132 [ 300/1251 ( 24%)]  Loss: 3.832 (3.64)  Time: 0.997s, 1026.80/s  (1.017s, 1007.23/s)  LR: 5.978e-04  Data: 0.010 (0.016)
Train: 132 [ 350/1251 ( 28%)]  Loss: 3.663 (3.64)  Time: 0.995s, 1029.56/s  (1.014s, 1009.70/s)  LR: 5.978e-04  Data: 0.010 (0.015)
Train: 132 [ 400/1251 ( 32%)]  Loss: 3.934 (3.67)  Time: 0.998s, 1026.34/s  (1.013s, 1010.97/s)  LR: 5.978e-04  Data: 0.011 (0.015)
Train: 132 [ 450/1251 ( 36%)]  Loss: 3.683 (3.67)  Time: 1.038s,  986.32/s  (1.011s, 1012.42/s)  LR: 5.978e-04  Data: 0.011 (0.014)
Train: 132 [ 500/1251 ( 40%)]  Loss: 3.396 (3.65)  Time: 1.004s, 1019.44/s  (1.010s, 1013.69/s)  LR: 5.978e-04  Data: 0.011 (0.014)
Train: 132 [ 550/1251 ( 44%)]  Loss: 3.694 (3.65)  Time: 0.999s, 1025.50/s  (1.009s, 1014.45/s)  LR: 5.978e-04  Data: 0.011 (0.014)
Train: 132 [ 600/1251 ( 48%)]  Loss: 3.523 (3.64)  Time: 0.994s, 1030.15/s  (1.009s, 1014.85/s)  LR: 5.978e-04  Data: 0.011 (0.014)
Train: 132 [ 650/1251 ( 52%)]  Loss: 3.386 (3.62)  Time: 0.996s, 1027.96/s  (1.009s, 1015.24/s)  LR: 5.978e-04  Data: 0.011 (0.013)
Train: 132 [ 700/1251 ( 56%)]  Loss: 3.459 (3.61)  Time: 0.994s, 1029.98/s  (1.008s, 1015.90/s)  LR: 5.978e-04  Data: 0.010 (0.013)
Train: 132 [ 750/1251 ( 60%)]  Loss: 3.910 (3.63)  Time: 1.049s,  976.26/s  (1.008s, 1016.12/s)  LR: 5.978e-04  Data: 0.012 (0.013)
Train: 132 [ 800/1251 ( 64%)]  Loss: 3.353 (3.61)  Time: 0.996s, 1028.27/s  (1.009s, 1014.38/s)  LR: 5.978e-04  Data: 0.010 (0.013)
Train: 132 [ 850/1251 ( 68%)]  Loss: 3.665 (3.62)  Time: 0.996s, 1028.15/s  (1.009s, 1014.70/s)  LR: 5.978e-04  Data: 0.012 (0.013)
Train: 132 [ 900/1251 ( 72%)]  Loss: 3.584 (3.62)  Time: 1.062s,  964.52/s  (1.010s, 1014.17/s)  LR: 5.978e-04  Data: 0.010 (0.013)
Train: 132 [ 950/1251 ( 76%)]  Loss: 3.712 (3.62)  Time: 1.053s,  972.81/s  (1.010s, 1014.06/s)  LR: 5.978e-04  Data: 0.011 (0.013)
Train: 132 [1000/1251 ( 80%)]  Loss: 3.559 (3.62)  Time: 1.007s, 1017.25/s  (1.010s, 1014.32/s)  LR: 5.978e-04  Data: 0.011 (0.013)
Train: 132 [1050/1251 ( 84%)]  Loss: 3.492 (3.61)  Time: 1.003s, 1021.15/s  (1.009s, 1014.71/s)  LR: 5.978e-04  Data: 0.010 (0.013)
Train: 132 [1100/1251 ( 88%)]  Loss: 3.630 (3.61)  Time: 0.995s, 1028.95/s  (1.009s, 1014.90/s)  LR: 5.978e-04  Data: 0.010 (0.012)
Train: 132 [1150/1251 ( 92%)]  Loss: 3.739 (3.62)  Time: 0.996s, 1027.91/s  (1.009s, 1014.88/s)  LR: 5.978e-04  Data: 0.010 (0.012)
Train: 132 [1200/1251 ( 96%)]  Loss: 3.456 (3.61)  Time: 1.004s, 1020.22/s  (1.009s, 1014.82/s)  LR: 5.978e-04  Data: 0.012 (0.012)
Train: 132 [1250/1251 (100%)]  Loss: 3.378 (3.60)  Time: 1.023s, 1000.84/s  (1.009s, 1014.91/s)  LR: 5.978e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.654 (1.654)  Loss:  0.7197 (0.7197)  Acc@1: 89.4531 (89.4531)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.245 (0.566)  Loss:  0.8394 (1.3103)  Acc@1: 85.1415 (74.0940)  Acc@5: 96.4623 (92.0960)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-132.pth.tar', 74.09400000976562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-131.pth.tar', 73.87800016601562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-130.pth.tar', 73.56600009521485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-122.pth.tar', 73.5220001171875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-128.pth.tar', 73.49200006347657)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-124.pth.tar', 73.48799995361328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-127.pth.tar', 73.32200020019532)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-119.pth.tar', 73.31400009765625)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-113.pth.tar', 73.291999921875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-125.pth.tar', 73.2280000415039)

Train: 133 [   0/1251 (  0%)]  Loss: 3.513 (3.51)  Time: 2.385s,  429.30/s  (2.385s,  429.30/s)  LR: 5.927e-04  Data: 1.435 (1.435)
Train: 133 [  50/1251 (  4%)]  Loss: 3.889 (3.70)  Time: 0.996s, 1028.49/s  (1.044s,  980.97/s)  LR: 5.927e-04  Data: 0.011 (0.039)
Train: 133 [ 100/1251 (  8%)]  Loss: 3.550 (3.65)  Time: 0.994s, 1030.11/s  (1.024s,  999.65/s)  LR: 5.927e-04  Data: 0.011 (0.025)
Train: 133 [ 150/1251 ( 12%)]  Loss: 3.273 (3.56)  Time: 0.995s, 1029.23/s  (1.018s, 1006.20/s)  LR: 5.927e-04  Data: 0.011 (0.021)
Train: 133 [ 200/1251 ( 16%)]  Loss: 3.719 (3.59)  Time: 0.997s, 1027.13/s  (1.013s, 1010.63/s)  LR: 5.927e-04  Data: 0.011 (0.018)
Train: 133 [ 250/1251 ( 20%)]  Loss: 3.496 (3.57)  Time: 1.035s,  989.79/s  (1.012s, 1012.09/s)  LR: 5.927e-04  Data: 0.011 (0.017)
Train: 133 [ 300/1251 ( 24%)]  Loss: 3.594 (3.58)  Time: 0.996s, 1028.29/s  (1.013s, 1011.09/s)  LR: 5.927e-04  Data: 0.011 (0.016)
Train: 133 [ 350/1251 ( 28%)]  Loss: 3.611 (3.58)  Time: 1.000s, 1024.36/s  (1.014s, 1009.41/s)  LR: 5.927e-04  Data: 0.010 (0.015)
Train: 133 [ 400/1251 ( 32%)]  Loss: 3.450 (3.57)  Time: 1.032s,  991.80/s  (1.016s, 1008.08/s)  LR: 5.927e-04  Data: 0.011 (0.015)
Train: 133 [ 450/1251 ( 36%)]  Loss: 3.869 (3.60)  Time: 1.002s, 1022.13/s  (1.015s, 1009.12/s)  LR: 5.927e-04  Data: 0.010 (0.014)
Train: 133 [ 500/1251 ( 40%)]  Loss: 3.796 (3.61)  Time: 0.995s, 1028.91/s  (1.013s, 1010.77/s)  LR: 5.927e-04  Data: 0.010 (0.014)
Train: 133 [ 550/1251 ( 44%)]  Loss: 3.439 (3.60)  Time: 1.013s, 1011.15/s  (1.012s, 1011.94/s)  LR: 5.927e-04  Data: 0.012 (0.014)
Train: 133 [ 600/1251 ( 48%)]  Loss: 3.613 (3.60)  Time: 0.995s, 1028.72/s  (1.011s, 1012.57/s)  LR: 5.927e-04  Data: 0.012 (0.014)
Train: 133 [ 650/1251 ( 52%)]  Loss: 3.544 (3.60)  Time: 1.043s,  982.07/s  (1.011s, 1013.34/s)  LR: 5.927e-04  Data: 0.010 (0.013)
Train: 133 [ 700/1251 ( 56%)]  Loss: 3.689 (3.60)  Time: 1.065s,  961.72/s  (1.011s, 1012.77/s)  LR: 5.927e-04  Data: 0.011 (0.013)
Train: 133 [ 750/1251 ( 60%)]  Loss: 3.257 (3.58)  Time: 1.060s,  965.91/s  (1.012s, 1011.99/s)  LR: 5.927e-04  Data: 0.010 (0.013)
Train: 133 [ 800/1251 ( 64%)]  Loss: 3.451 (3.57)  Time: 1.011s, 1013.31/s  (1.013s, 1010.41/s)  LR: 5.927e-04  Data: 0.011 (0.013)
Train: 133 [ 850/1251 ( 68%)]  Loss: 3.639 (3.58)  Time: 1.031s,  993.50/s  (1.013s, 1010.72/s)  LR: 5.927e-04  Data: 0.011 (0.013)
Train: 133 [ 900/1251 ( 72%)]  Loss: 3.416 (3.57)  Time: 1.008s, 1016.10/s  (1.013s, 1010.83/s)  LR: 5.927e-04  Data: 0.011 (0.013)
Train: 133 [ 950/1251 ( 76%)]  Loss: 3.966 (3.59)  Time: 0.996s, 1027.83/s  (1.013s, 1011.17/s)  LR: 5.927e-04  Data: 0.012 (0.013)
Train: 133 [1000/1251 ( 80%)]  Loss: 3.873 (3.60)  Time: 1.000s, 1023.60/s  (1.012s, 1011.64/s)  LR: 5.927e-04  Data: 0.011 (0.013)
Train: 133 [1050/1251 ( 84%)]  Loss: 3.208 (3.58)  Time: 0.995s, 1029.50/s  (1.011s, 1012.37/s)  LR: 5.927e-04  Data: 0.011 (0.012)
Train: 133 [1100/1251 ( 88%)]  Loss: 3.510 (3.58)  Time: 1.021s, 1003.15/s  (1.011s, 1012.89/s)  LR: 5.927e-04  Data: 0.011 (0.012)
Train: 133 [1150/1251 ( 92%)]  Loss: 3.599 (3.58)  Time: 0.994s, 1030.56/s  (1.011s, 1012.82/s)  LR: 5.927e-04  Data: 0.010 (0.012)
Train: 133 [1200/1251 ( 96%)]  Loss: 3.973 (3.60)  Time: 0.996s, 1028.59/s  (1.010s, 1013.43/s)  LR: 5.927e-04  Data: 0.011 (0.012)
Train: 133 [1250/1251 (100%)]  Loss: 3.812 (3.61)  Time: 0.985s, 1040.03/s  (1.010s, 1013.78/s)  LR: 5.927e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.631 (1.631)  Loss:  0.8648 (0.8648)  Acc@1: 87.7930 (87.7930)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.245 (0.564)  Loss:  0.9120 (1.3359)  Acc@1: 83.9623 (73.9340)  Acc@5: 96.1085 (92.0340)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-132.pth.tar', 74.09400000976562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-133.pth.tar', 73.93400001464843)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-131.pth.tar', 73.87800016601562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-130.pth.tar', 73.56600009521485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-122.pth.tar', 73.5220001171875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-128.pth.tar', 73.49200006347657)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-124.pth.tar', 73.48799995361328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-127.pth.tar', 73.32200020019532)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-119.pth.tar', 73.31400009765625)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-113.pth.tar', 73.291999921875)

Train: 134 [   0/1251 (  0%)]  Loss: 3.733 (3.73)  Time: 2.571s,  398.28/s  (2.571s,  398.28/s)  LR: 5.876e-04  Data: 1.610 (1.610)
Train: 134 [  50/1251 (  4%)]  Loss: 3.739 (3.74)  Time: 1.005s, 1019.26/s  (1.033s,  991.61/s)  LR: 5.876e-04  Data: 0.011 (0.043)
Train: 134 [ 100/1251 (  8%)]  Loss: 3.252 (3.57)  Time: 0.995s, 1029.05/s  (1.021s, 1003.41/s)  LR: 5.876e-04  Data: 0.010 (0.027)
Train: 134 [ 150/1251 ( 12%)]  Loss: 3.811 (3.63)  Time: 0.999s, 1025.26/s  (1.013s, 1011.19/s)  LR: 5.876e-04  Data: 0.011 (0.022)
Train: 134 [ 200/1251 ( 16%)]  Loss: 3.714 (3.65)  Time: 1.065s,  961.23/s  (1.017s, 1006.56/s)  LR: 5.876e-04  Data: 0.015 (0.019)
Train: 134 [ 250/1251 ( 20%)]  Loss: 3.487 (3.62)  Time: 1.026s,  998.19/s  (1.014s, 1010.01/s)  LR: 5.876e-04  Data: 0.011 (0.018)
Train: 134 [ 300/1251 ( 24%)]  Loss: 3.325 (3.58)  Time: 0.994s, 1029.85/s  (1.011s, 1012.61/s)  LR: 5.876e-04  Data: 0.011 (0.017)
Train: 134 [ 350/1251 ( 28%)]  Loss: 3.365 (3.55)  Time: 0.995s, 1029.21/s  (1.010s, 1014.35/s)  LR: 5.876e-04  Data: 0.011 (0.016)
Train: 134 [ 400/1251 ( 32%)]  Loss: 3.634 (3.56)  Time: 0.996s, 1027.97/s  (1.009s, 1015.12/s)  LR: 5.876e-04  Data: 0.010 (0.015)
Train: 134 [ 450/1251 ( 36%)]  Loss: 3.816 (3.59)  Time: 0.996s, 1027.91/s  (1.008s, 1016.36/s)  LR: 5.876e-04  Data: 0.012 (0.015)
Train: 134 [ 500/1251 ( 40%)]  Loss: 3.511 (3.58)  Time: 0.994s, 1029.70/s  (1.007s, 1017.25/s)  LR: 5.876e-04  Data: 0.011 (0.014)
Train: 134 [ 550/1251 ( 44%)]  Loss: 3.557 (3.58)  Time: 1.051s,  974.63/s  (1.006s, 1017.87/s)  LR: 5.876e-04  Data: 0.010 (0.014)
Train: 134 [ 600/1251 ( 48%)]  Loss: 3.390 (3.56)  Time: 0.994s, 1030.08/s  (1.006s, 1018.39/s)  LR: 5.876e-04  Data: 0.011 (0.014)
Train: 134 [ 650/1251 ( 52%)]  Loss: 3.620 (3.57)  Time: 0.997s, 1027.42/s  (1.005s, 1018.69/s)  LR: 5.876e-04  Data: 0.012 (0.014)
Train: 134 [ 700/1251 ( 56%)]  Loss: 3.210 (3.54)  Time: 0.994s, 1030.25/s  (1.005s, 1019.26/s)  LR: 5.876e-04  Data: 0.010 (0.013)
Train: 134 [ 750/1251 ( 60%)]  Loss: 3.404 (3.54)  Time: 0.996s, 1028.10/s  (1.004s, 1019.65/s)  LR: 5.876e-04  Data: 0.011 (0.013)
Train: 134 [ 800/1251 ( 64%)]  Loss: 3.652 (3.54)  Time: 0.995s, 1028.80/s  (1.005s, 1018.98/s)  LR: 5.876e-04  Data: 0.010 (0.013)
Train: 134 [ 850/1251 ( 68%)]  Loss: 3.658 (3.55)  Time: 0.996s, 1027.80/s  (1.006s, 1017.85/s)  LR: 5.876e-04  Data: 0.010 (0.013)
Train: 134 [ 900/1251 ( 72%)]  Loss: 3.525 (3.55)  Time: 1.061s,  965.18/s  (1.006s, 1017.98/s)  LR: 5.876e-04  Data: 0.011 (0.013)
Train: 134 [ 950/1251 ( 76%)]  Loss: 3.651 (3.55)  Time: 0.996s, 1028.59/s  (1.006s, 1018.28/s)  LR: 5.876e-04  Data: 0.011 (0.013)
Train: 134 [1000/1251 ( 80%)]  Loss: 3.494 (3.55)  Time: 0.995s, 1028.65/s  (1.005s, 1018.72/s)  LR: 5.876e-04  Data: 0.011 (0.013)
Train: 134 [1050/1251 ( 84%)]  Loss: 3.728 (3.56)  Time: 0.993s, 1030.82/s  (1.005s, 1018.59/s)  LR: 5.876e-04  Data: 0.011 (0.013)
Train: 134 [1100/1251 ( 88%)]  Loss: 3.166 (3.54)  Time: 0.996s, 1027.64/s  (1.005s, 1018.67/s)  LR: 5.876e-04  Data: 0.011 (0.013)
Train: 134 [1150/1251 ( 92%)]  Loss: 3.349 (3.53)  Time: 0.995s, 1028.74/s  (1.005s, 1018.85/s)  LR: 5.876e-04  Data: 0.011 (0.012)
Train: 134 [1200/1251 ( 96%)]  Loss: 3.611 (3.54)  Time: 0.997s, 1026.77/s  (1.005s, 1018.90/s)  LR: 5.876e-04  Data: 0.011 (0.012)
Train: 134 [1250/1251 (100%)]  Loss: 3.686 (3.54)  Time: 0.983s, 1041.88/s  (1.006s, 1018.01/s)  LR: 5.876e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.631 (1.631)  Loss:  0.7042 (0.7042)  Acc@1: 88.6719 (88.6719)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.245 (0.572)  Loss:  0.9097 (1.2585)  Acc@1: 83.3726 (73.5540)  Acc@5: 95.8727 (91.8260)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-132.pth.tar', 74.09400000976562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-133.pth.tar', 73.93400001464843)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-131.pth.tar', 73.87800016601562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-130.pth.tar', 73.56600009521485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-134.pth.tar', 73.55400001708985)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-122.pth.tar', 73.5220001171875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-128.pth.tar', 73.49200006347657)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-124.pth.tar', 73.48799995361328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-127.pth.tar', 73.32200020019532)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-119.pth.tar', 73.31400009765625)

Train: 135 [   0/1251 (  0%)]  Loss: 3.448 (3.45)  Time: 2.387s,  429.04/s  (2.387s,  429.04/s)  LR: 5.824e-04  Data: 1.436 (1.436)
Train: 135 [  50/1251 (  4%)]  Loss: 3.682 (3.57)  Time: 0.997s, 1026.94/s  (1.038s,  986.30/s)  LR: 5.824e-04  Data: 0.011 (0.045)
Train: 135 [ 100/1251 (  8%)]  Loss: 3.739 (3.62)  Time: 0.995s, 1029.07/s  (1.025s,  999.22/s)  LR: 5.824e-04  Data: 0.011 (0.028)
Train: 135 [ 150/1251 ( 12%)]  Loss: 3.156 (3.51)  Time: 0.995s, 1029.12/s  (1.026s,  998.10/s)  LR: 5.824e-04  Data: 0.011 (0.022)
Train: 135 [ 200/1251 ( 16%)]  Loss: 3.761 (3.56)  Time: 1.000s, 1024.40/s  (1.020s, 1003.66/s)  LR: 5.824e-04  Data: 0.012 (0.020)
Train: 135 [ 250/1251 ( 20%)]  Loss: 3.840 (3.60)  Time: 0.994s, 1029.83/s  (1.020s, 1004.23/s)  LR: 5.824e-04  Data: 0.011 (0.018)
Train: 135 [ 300/1251 ( 24%)]  Loss: 3.464 (3.58)  Time: 0.996s, 1027.85/s  (1.018s, 1006.35/s)  LR: 5.824e-04  Data: 0.011 (0.017)
Train: 135 [ 350/1251 ( 28%)]  Loss: 3.584 (3.58)  Time: 1.043s,  981.88/s  (1.015s, 1008.44/s)  LR: 5.824e-04  Data: 0.011 (0.016)
Train: 135 [ 400/1251 ( 32%)]  Loss: 3.564 (3.58)  Time: 0.992s, 1032.18/s  (1.014s, 1009.48/s)  LR: 5.824e-04  Data: 0.010 (0.015)
Train: 135 [ 450/1251 ( 36%)]  Loss: 3.598 (3.58)  Time: 1.046s,  978.59/s  (1.015s, 1008.90/s)  LR: 5.824e-04  Data: 0.011 (0.015)
Train: 135 [ 500/1251 ( 40%)]  Loss: 3.555 (3.58)  Time: 0.997s, 1027.30/s  (1.016s, 1007.75/s)  LR: 5.824e-04  Data: 0.011 (0.015)
Train: 135 [ 550/1251 ( 44%)]  Loss: 3.618 (3.58)  Time: 1.037s,  987.91/s  (1.015s, 1008.78/s)  LR: 5.824e-04  Data: 0.011 (0.014)
Train: 135 [ 600/1251 ( 48%)]  Loss: 3.619 (3.59)  Time: 0.994s, 1030.50/s  (1.014s, 1010.02/s)  LR: 5.824e-04  Data: 0.011 (0.014)
Train: 135 [ 650/1251 ( 52%)]  Loss: 3.730 (3.60)  Time: 1.045s,  979.90/s  (1.013s, 1011.17/s)  LR: 5.824e-04  Data: 0.011 (0.014)
Train: 135 [ 700/1251 ( 56%)]  Loss: 3.389 (3.58)  Time: 0.997s, 1026.71/s  (1.013s, 1011.35/s)  LR: 5.824e-04  Data: 0.011 (0.014)
Train: 135 [ 750/1251 ( 60%)]  Loss: 3.416 (3.57)  Time: 1.066s,  960.41/s  (1.013s, 1010.89/s)  LR: 5.824e-04  Data: 0.012 (0.014)
Train: 135 [ 800/1251 ( 64%)]  Loss: 3.382 (3.56)  Time: 1.004s, 1019.83/s  (1.013s, 1010.96/s)  LR: 5.824e-04  Data: 0.011 (0.013)
Train: 135 [ 850/1251 ( 68%)]  Loss: 3.894 (3.58)  Time: 1.039s,  985.85/s  (1.012s, 1011.65/s)  LR: 5.824e-04  Data: 0.011 (0.013)
Train: 135 [ 900/1251 ( 72%)]  Loss: 3.430 (3.57)  Time: 1.003s, 1020.96/s  (1.012s, 1012.12/s)  LR: 5.824e-04  Data: 0.011 (0.013)
Train: 135 [ 950/1251 ( 76%)]  Loss: 3.609 (3.57)  Time: 0.994s, 1030.69/s  (1.011s, 1012.82/s)  LR: 5.824e-04  Data: 0.012 (0.013)
Train: 135 [1000/1251 ( 80%)]  Loss: 3.640 (3.58)  Time: 0.996s, 1028.12/s  (1.010s, 1013.56/s)  LR: 5.824e-04  Data: 0.011 (0.013)
Train: 135 [1050/1251 ( 84%)]  Loss: 3.487 (3.57)  Time: 1.001s, 1022.99/s  (1.010s, 1014.15/s)  LR: 5.824e-04  Data: 0.011 (0.013)
Train: 135 [1100/1251 ( 88%)]  Loss: 3.711 (3.58)  Time: 0.994s, 1030.30/s  (1.009s, 1014.48/s)  LR: 5.824e-04  Data: 0.011 (0.013)
Train: 135 [1150/1251 ( 92%)]  Loss: 3.680 (3.58)  Time: 1.006s, 1017.69/s  (1.009s, 1014.67/s)  LR: 5.824e-04  Data: 0.012 (0.013)
Train: 135 [1200/1251 ( 96%)]  Loss: 3.746 (3.59)  Time: 0.997s, 1027.05/s  (1.009s, 1015.14/s)  LR: 5.824e-04  Data: 0.012 (0.013)
Train: 135 [1250/1251 (100%)]  Loss: 3.597 (3.59)  Time: 0.982s, 1042.64/s  (1.009s, 1015.36/s)  LR: 5.824e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.615 (1.615)  Loss:  0.7650 (0.7650)  Acc@1: 87.8906 (87.8906)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.245 (0.573)  Loss:  0.9096 (1.2807)  Acc@1: 83.8443 (73.9820)  Acc@5: 95.5189 (92.1160)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-132.pth.tar', 74.09400000976562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-135.pth.tar', 73.98199996337891)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-133.pth.tar', 73.93400001464843)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-131.pth.tar', 73.87800016601562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-130.pth.tar', 73.56600009521485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-134.pth.tar', 73.55400001708985)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-122.pth.tar', 73.5220001171875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-128.pth.tar', 73.49200006347657)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-124.pth.tar', 73.48799995361328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-127.pth.tar', 73.32200020019532)

Train: 136 [   0/1251 (  0%)]  Loss: 3.662 (3.66)  Time: 2.490s,  411.24/s  (2.490s,  411.24/s)  LR: 5.773e-04  Data: 1.526 (1.526)
Train: 136 [  50/1251 (  4%)]  Loss: 3.583 (3.62)  Time: 0.995s, 1029.01/s  (1.044s,  980.65/s)  LR: 5.773e-04  Data: 0.011 (0.041)
Train: 136 [ 100/1251 (  8%)]  Loss: 3.281 (3.51)  Time: 1.023s, 1001.32/s  (1.032s,  991.89/s)  LR: 5.773e-04  Data: 0.011 (0.026)
Train: 136 [ 150/1251 ( 12%)]  Loss: 3.594 (3.53)  Time: 1.006s, 1018.11/s  (1.028s,  996.41/s)  LR: 5.773e-04  Data: 0.011 (0.021)
Train: 136 [ 200/1251 ( 16%)]  Loss: 3.871 (3.60)  Time: 0.996s, 1028.16/s  (1.021s, 1002.91/s)  LR: 5.773e-04  Data: 0.011 (0.019)
Train: 136 [ 250/1251 ( 20%)]  Loss: 3.940 (3.66)  Time: 0.995s, 1028.91/s  (1.017s, 1006.85/s)  LR: 5.773e-04  Data: 0.011 (0.017)
Train: 136 [ 300/1251 ( 24%)]  Loss: 3.874 (3.69)  Time: 1.030s,  993.85/s  (1.016s, 1007.83/s)  LR: 5.773e-04  Data: 0.011 (0.016)
Train: 136 [ 350/1251 ( 28%)]  Loss: 3.741 (3.69)  Time: 1.052s,  973.34/s  (1.016s, 1007.73/s)  LR: 5.773e-04  Data: 0.013 (0.016)
Train: 136 [ 400/1251 ( 32%)]  Loss: 3.668 (3.69)  Time: 0.995s, 1028.83/s  (1.017s, 1006.50/s)  LR: 5.773e-04  Data: 0.011 (0.015)
Train: 136 [ 450/1251 ( 36%)]  Loss: 3.729 (3.69)  Time: 0.997s, 1027.27/s  (1.016s, 1007.40/s)  LR: 5.773e-04  Data: 0.011 (0.015)
Train: 136 [ 500/1251 ( 40%)]  Loss: 3.362 (3.66)  Time: 1.060s,  966.02/s  (1.016s, 1007.59/s)  LR: 5.773e-04  Data: 0.011 (0.014)
Train: 136 [ 550/1251 ( 44%)]  Loss: 3.807 (3.68)  Time: 0.995s, 1029.40/s  (1.017s, 1006.77/s)  LR: 5.773e-04  Data: 0.012 (0.014)
Train: 136 [ 600/1251 ( 48%)]  Loss: 3.059 (3.63)  Time: 0.995s, 1028.80/s  (1.016s, 1008.29/s)  LR: 5.773e-04  Data: 0.011 (0.014)
Train: 136 [ 650/1251 ( 52%)]  Loss: 3.685 (3.63)  Time: 1.002s, 1022.34/s  (1.014s, 1009.54/s)  LR: 5.773e-04  Data: 0.012 (0.014)
Train: 136 [ 700/1251 ( 56%)]  Loss: 3.724 (3.64)  Time: 1.020s, 1003.83/s  (1.015s, 1009.34/s)  LR: 5.773e-04  Data: 0.012 (0.013)
Train: 136 [ 750/1251 ( 60%)]  Loss: 3.610 (3.64)  Time: 1.003s, 1020.93/s  (1.013s, 1010.40/s)  LR: 5.773e-04  Data: 0.012 (0.013)
Train: 136 [ 800/1251 ( 64%)]  Loss: 3.747 (3.64)  Time: 0.993s, 1030.79/s  (1.013s, 1011.07/s)  LR: 5.773e-04  Data: 0.010 (0.013)
Train: 136 [ 850/1251 ( 68%)]  Loss: 3.688 (3.65)  Time: 0.994s, 1029.91/s  (1.012s, 1011.46/s)  LR: 5.773e-04  Data: 0.011 (0.013)
Train: 136 [ 900/1251 ( 72%)]  Loss: 3.187 (3.62)  Time: 1.033s,  991.63/s  (1.012s, 1011.47/s)  LR: 5.773e-04  Data: 0.011 (0.013)
Train: 136 [ 950/1251 ( 76%)]  Loss: 3.183 (3.60)  Time: 0.990s, 1034.44/s  (1.013s, 1011.35/s)  LR: 5.773e-04  Data: 0.011 (0.013)
Train: 136 [1000/1251 ( 80%)]  Loss: 3.640 (3.60)  Time: 1.057s,  968.40/s  (1.013s, 1011.29/s)  LR: 5.773e-04  Data: 0.012 (0.013)
Train: 136 [1050/1251 ( 84%)]  Loss: 3.419 (3.59)  Time: 0.995s, 1029.52/s  (1.014s, 1010.34/s)  LR: 5.773e-04  Data: 0.011 (0.013)
Train: 136 [1100/1251 ( 88%)]  Loss: 3.887 (3.61)  Time: 0.999s, 1024.87/s  (1.013s, 1010.89/s)  LR: 5.773e-04  Data: 0.014 (0.013)
Train: 136 [1150/1251 ( 92%)]  Loss: 3.843 (3.62)  Time: 0.996s, 1028.00/s  (1.013s, 1011.27/s)  LR: 5.773e-04  Data: 0.012 (0.013)
Train: 136 [1200/1251 ( 96%)]  Loss: 3.511 (3.61)  Time: 1.063s,  963.76/s  (1.012s, 1011.58/s)  LR: 5.773e-04  Data: 0.012 (0.012)
Train: 136 [1250/1251 (100%)]  Loss: 3.441 (3.61)  Time: 0.983s, 1041.57/s  (1.012s, 1011.88/s)  LR: 5.773e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.667 (1.667)  Loss:  0.7791 (0.7791)  Acc@1: 87.3047 (87.3047)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.245 (0.572)  Loss:  0.8606 (1.3294)  Acc@1: 82.6651 (73.4620)  Acc@5: 95.4009 (91.9680)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-132.pth.tar', 74.09400000976562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-135.pth.tar', 73.98199996337891)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-133.pth.tar', 73.93400001464843)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-131.pth.tar', 73.87800016601562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-130.pth.tar', 73.56600009521485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-134.pth.tar', 73.55400001708985)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-122.pth.tar', 73.5220001171875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-128.pth.tar', 73.49200006347657)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-124.pth.tar', 73.48799995361328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-136.pth.tar', 73.46199996826172)

Train: 137 [   0/1251 (  0%)]  Loss: 3.647 (3.65)  Time: 2.509s,  408.19/s  (2.509s,  408.19/s)  LR: 5.722e-04  Data: 1.554 (1.554)
Train: 137 [  50/1251 (  4%)]  Loss: 3.817 (3.73)  Time: 0.997s, 1027.50/s  (1.033s,  991.27/s)  LR: 5.722e-04  Data: 0.012 (0.041)
Train: 137 [ 100/1251 (  8%)]  Loss: 3.620 (3.69)  Time: 0.995s, 1029.54/s  (1.021s, 1003.03/s)  LR: 5.722e-04  Data: 0.010 (0.026)
Train: 137 [ 150/1251 ( 12%)]  Loss: 3.039 (3.53)  Time: 0.999s, 1025.16/s  (1.016s, 1007.83/s)  LR: 5.722e-04  Data: 0.012 (0.021)
Train: 137 [ 200/1251 ( 16%)]  Loss: 3.725 (3.57)  Time: 0.993s, 1030.84/s  (1.017s, 1006.42/s)  LR: 5.722e-04  Data: 0.010 (0.019)
Train: 137 [ 250/1251 ( 20%)]  Loss: 3.938 (3.63)  Time: 0.996s, 1028.13/s  (1.014s, 1009.91/s)  LR: 5.722e-04  Data: 0.011 (0.017)
Train: 137 [ 300/1251 ( 24%)]  Loss: 3.828 (3.66)  Time: 0.996s, 1028.54/s  (1.018s, 1006.10/s)  LR: 5.722e-04  Data: 0.011 (0.016)
Train: 137 [ 350/1251 ( 28%)]  Loss: 3.731 (3.67)  Time: 0.996s, 1028.35/s  (1.016s, 1008.28/s)  LR: 5.722e-04  Data: 0.011 (0.016)
Train: 137 [ 400/1251 ( 32%)]  Loss: 3.658 (3.67)  Time: 0.997s, 1027.49/s  (1.015s, 1008.54/s)  LR: 5.722e-04  Data: 0.011 (0.015)
Train: 137 [ 450/1251 ( 36%)]  Loss: 3.530 (3.65)  Time: 0.994s, 1030.46/s  (1.014s, 1010.14/s)  LR: 5.722e-04  Data: 0.010 (0.015)
Train: 137 [ 500/1251 ( 40%)]  Loss: 3.535 (3.64)  Time: 1.004s, 1019.72/s  (1.012s, 1011.52/s)  LR: 5.722e-04  Data: 0.011 (0.014)
Train: 137 [ 550/1251 ( 44%)]  Loss: 3.495 (3.63)  Time: 0.994s, 1029.88/s  (1.012s, 1012.29/s)  LR: 5.722e-04  Data: 0.011 (0.014)
Train: 137 [ 600/1251 ( 48%)]  Loss: 3.596 (3.63)  Time: 0.998s, 1026.08/s  (1.011s, 1013.25/s)  LR: 5.722e-04  Data: 0.011 (0.014)
Train: 137 [ 650/1251 ( 52%)]  Loss: 3.567 (3.62)  Time: 1.062s,  964.46/s  (1.011s, 1013.34/s)  LR: 5.722e-04  Data: 0.011 (0.013)
Train: 137 [ 700/1251 ( 56%)]  Loss: 3.751 (3.63)  Time: 0.996s, 1027.66/s  (1.010s, 1013.92/s)  LR: 5.722e-04  Data: 0.011 (0.013)
Train: 137 [ 750/1251 ( 60%)]  Loss: 3.534 (3.63)  Time: 1.003s, 1021.23/s  (1.010s, 1013.59/s)  LR: 5.722e-04  Data: 0.014 (0.013)
Train: 137 [ 800/1251 ( 64%)]  Loss: 3.576 (3.62)  Time: 0.995s, 1028.80/s  (1.010s, 1014.24/s)  LR: 5.722e-04  Data: 0.010 (0.013)
Train: 137 [ 850/1251 ( 68%)]  Loss: 3.734 (3.63)  Time: 0.994s, 1030.23/s  (1.009s, 1014.57/s)  LR: 5.722e-04  Data: 0.011 (0.013)
Train: 137 [ 900/1251 ( 72%)]  Loss: 3.480 (3.62)  Time: 0.994s, 1030.15/s  (1.009s, 1014.61/s)  LR: 5.722e-04  Data: 0.010 (0.013)
Train: 137 [ 950/1251 ( 76%)]  Loss: 3.411 (3.61)  Time: 1.046s,  978.58/s  (1.010s, 1014.32/s)  LR: 5.722e-04  Data: 0.011 (0.013)
Train: 137 [1000/1251 ( 80%)]  Loss: 3.549 (3.61)  Time: 0.998s, 1025.76/s  (1.009s, 1014.94/s)  LR: 5.722e-04  Data: 0.011 (0.013)
Train: 137 [1050/1251 ( 84%)]  Loss: 3.545 (3.60)  Time: 0.992s, 1032.64/s  (1.009s, 1015.29/s)  LR: 5.722e-04  Data: 0.010 (0.013)
Train: 137 [1100/1251 ( 88%)]  Loss: 3.244 (3.59)  Time: 1.026s,  998.12/s  (1.009s, 1015.10/s)  LR: 5.722e-04  Data: 0.010 (0.012)
Train: 137 [1150/1251 ( 92%)]  Loss: 3.515 (3.59)  Time: 0.996s, 1028.25/s  (1.009s, 1015.06/s)  LR: 5.722e-04  Data: 0.011 (0.012)
Train: 137 [1200/1251 ( 96%)]  Loss: 3.452 (3.58)  Time: 0.998s, 1026.52/s  (1.008s, 1015.39/s)  LR: 5.722e-04  Data: 0.011 (0.012)
Train: 137 [1250/1251 (100%)]  Loss: 3.777 (3.59)  Time: 0.983s, 1041.47/s  (1.009s, 1015.24/s)  LR: 5.722e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.601 (1.601)  Loss:  0.8814 (0.8814)  Acc@1: 89.3555 (89.3555)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.245 (0.577)  Loss:  0.9491 (1.4234)  Acc@1: 83.0189 (73.5380)  Acc@5: 95.4009 (91.8020)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-132.pth.tar', 74.09400000976562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-135.pth.tar', 73.98199996337891)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-133.pth.tar', 73.93400001464843)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-131.pth.tar', 73.87800016601562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-130.pth.tar', 73.56600009521485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-134.pth.tar', 73.55400001708985)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-137.pth.tar', 73.53800012207032)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-122.pth.tar', 73.5220001171875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-128.pth.tar', 73.49200006347657)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-124.pth.tar', 73.48799995361328)

Train: 138 [   0/1251 (  0%)]  Loss: 3.726 (3.73)  Time: 4.243s,  241.34/s  (4.243s,  241.34/s)  LR: 5.670e-04  Data: 3.044 (3.044)
Train: 138 [  50/1251 (  4%)]  Loss: 3.402 (3.56)  Time: 0.997s, 1026.62/s  (1.062s,  964.62/s)  LR: 5.670e-04  Data: 0.012 (0.070)
Train: 138 [ 100/1251 (  8%)]  Loss: 4.046 (3.72)  Time: 0.996s, 1028.44/s  (1.037s,  987.35/s)  LR: 5.670e-04  Data: 0.010 (0.041)
Train: 138 [ 150/1251 ( 12%)]  Loss: 3.894 (3.77)  Time: 0.997s, 1026.74/s  (1.025s,  998.85/s)  LR: 5.670e-04  Data: 0.011 (0.031)
Train: 138 [ 200/1251 ( 16%)]  Loss: 3.620 (3.74)  Time: 1.000s, 1024.40/s  (1.019s, 1004.54/s)  LR: 5.670e-04  Data: 0.011 (0.026)
Train: 138 [ 250/1251 ( 20%)]  Loss: 3.394 (3.68)  Time: 0.998s, 1026.32/s  (1.015s, 1008.54/s)  LR: 5.670e-04  Data: 0.011 (0.023)
Train: 138 [ 300/1251 ( 24%)]  Loss: 3.777 (3.69)  Time: 0.992s, 1032.26/s  (1.014s, 1009.87/s)  LR: 5.670e-04  Data: 0.010 (0.021)
Train: 138 [ 350/1251 ( 28%)]  Loss: 3.880 (3.72)  Time: 0.993s, 1031.11/s  (1.013s, 1010.90/s)  LR: 5.670e-04  Data: 0.011 (0.020)
Train: 138 [ 400/1251 ( 32%)]  Loss: 3.734 (3.72)  Time: 0.994s, 1030.43/s  (1.015s, 1008.73/s)  LR: 5.670e-04  Data: 0.011 (0.019)
Train: 138 [ 450/1251 ( 36%)]  Loss: 3.755 (3.72)  Time: 0.996s, 1027.75/s  (1.014s, 1010.10/s)  LR: 5.670e-04  Data: 0.012 (0.018)
Train: 138 [ 500/1251 ( 40%)]  Loss: 3.658 (3.72)  Time: 0.995s, 1029.09/s  (1.012s, 1011.59/s)  LR: 5.670e-04  Data: 0.011 (0.017)
Train: 138 [ 550/1251 ( 44%)]  Loss: 3.666 (3.71)  Time: 0.995s, 1028.66/s  (1.011s, 1012.65/s)  LR: 5.670e-04  Data: 0.010 (0.017)
Train: 138 [ 600/1251 ( 48%)]  Loss: 3.699 (3.71)  Time: 1.034s,  990.47/s  (1.011s, 1013.23/s)  LR: 5.670e-04  Data: 0.012 (0.016)
Train: 138 [ 650/1251 ( 52%)]  Loss: 3.773 (3.72)  Time: 1.002s, 1022.17/s  (1.010s, 1013.44/s)  LR: 5.670e-04  Data: 0.010 (0.016)
Train: 138 [ 700/1251 ( 56%)]  Loss: 3.655 (3.71)  Time: 0.998s, 1025.83/s  (1.011s, 1013.11/s)  LR: 5.670e-04  Data: 0.012 (0.015)
Train: 138 [ 750/1251 ( 60%)]  Loss: 3.623 (3.71)  Time: 1.036s,  988.65/s  (1.012s, 1012.36/s)  LR: 5.670e-04  Data: 0.011 (0.015)
Train: 138 [ 800/1251 ( 64%)]  Loss: 3.894 (3.72)  Time: 1.059s,  967.01/s  (1.012s, 1011.84/s)  LR: 5.670e-04  Data: 0.010 (0.015)
Train: 138 [ 850/1251 ( 68%)]  Loss: 3.622 (3.71)  Time: 0.994s, 1029.74/s  (1.012s, 1011.77/s)  LR: 5.670e-04  Data: 0.011 (0.015)
Train: 138 [ 900/1251 ( 72%)]  Loss: 3.420 (3.70)  Time: 0.997s, 1027.30/s  (1.012s, 1012.32/s)  LR: 5.670e-04  Data: 0.011 (0.014)
Train: 138 [ 950/1251 ( 76%)]  Loss: 3.409 (3.68)  Time: 1.047s,  977.83/s  (1.011s, 1012.61/s)  LR: 5.670e-04  Data: 0.011 (0.014)
Train: 138 [1000/1251 ( 80%)]  Loss: 3.838 (3.69)  Time: 0.997s, 1027.13/s  (1.012s, 1012.21/s)  LR: 5.670e-04  Data: 0.011 (0.014)
Train: 138 [1050/1251 ( 84%)]  Loss: 3.902 (3.70)  Time: 1.035s,  989.05/s  (1.012s, 1012.29/s)  LR: 5.670e-04  Data: 0.011 (0.014)
Train: 138 [1100/1251 ( 88%)]  Loss: 3.629 (3.70)  Time: 0.995s, 1028.63/s  (1.012s, 1012.33/s)  LR: 5.670e-04  Data: 0.012 (0.014)
Train: 138 [1150/1251 ( 92%)]  Loss: 3.660 (3.69)  Time: 0.996s, 1028.16/s  (1.012s, 1011.76/s)  LR: 5.670e-04  Data: 0.011 (0.014)
Train: 138 [1200/1251 ( 96%)]  Loss: 3.767 (3.70)  Time: 0.995s, 1028.85/s  (1.012s, 1012.05/s)  LR: 5.670e-04  Data: 0.010 (0.014)
Train: 138 [1250/1251 (100%)]  Loss: 3.543 (3.69)  Time: 1.019s, 1004.49/s  (1.012s, 1011.77/s)  LR: 5.670e-04  Data: 0.000 (0.014)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.612 (1.612)  Loss:  0.7166 (0.7166)  Acc@1: 89.0625 (89.0625)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.245 (0.575)  Loss:  0.7806 (1.2842)  Acc@1: 84.9057 (74.2500)  Acc@5: 97.4057 (92.2400)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-138.pth.tar', 74.24999990722657)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-132.pth.tar', 74.09400000976562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-135.pth.tar', 73.98199996337891)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-133.pth.tar', 73.93400001464843)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-131.pth.tar', 73.87800016601562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-130.pth.tar', 73.56600009521485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-134.pth.tar', 73.55400001708985)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-137.pth.tar', 73.53800012207032)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-122.pth.tar', 73.5220001171875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-128.pth.tar', 73.49200006347657)

Train: 139 [   0/1251 (  0%)]  Loss: 3.203 (3.20)  Time: 2.544s,  402.53/s  (2.544s,  402.53/s)  LR: 5.619e-04  Data: 1.584 (1.584)
Train: 139 [  50/1251 (  4%)]  Loss: 3.600 (3.40)  Time: 1.058s,  968.30/s  (1.053s,  972.90/s)  LR: 5.619e-04  Data: 0.010 (0.042)
Train: 139 [ 100/1251 (  8%)]  Loss: 3.517 (3.44)  Time: 0.996s, 1027.92/s  (1.029s,  994.91/s)  LR: 5.619e-04  Data: 0.011 (0.027)
Train: 139 [ 150/1251 ( 12%)]  Loss: 3.463 (3.45)  Time: 0.994s, 1029.69/s  (1.020s, 1003.44/s)  LR: 5.619e-04  Data: 0.011 (0.022)
Train: 139 [ 200/1251 ( 16%)]  Loss: 3.609 (3.48)  Time: 0.994s, 1030.46/s  (1.015s, 1008.51/s)  LR: 5.619e-04  Data: 0.011 (0.019)
Train: 139 [ 250/1251 ( 20%)]  Loss: 3.817 (3.53)  Time: 0.994s, 1029.74/s  (1.015s, 1008.41/s)  LR: 5.619e-04  Data: 0.012 (0.017)
Train: 139 [ 300/1251 ( 24%)]  Loss: 3.590 (3.54)  Time: 1.007s, 1016.98/s  (1.013s, 1010.44/s)  LR: 5.619e-04  Data: 0.011 (0.016)
Train: 139 [ 350/1251 ( 28%)]  Loss: 3.725 (3.57)  Time: 0.997s, 1027.39/s  (1.013s, 1010.92/s)  LR: 5.619e-04  Data: 0.012 (0.016)
Train: 139 [ 400/1251 ( 32%)]  Loss: 3.303 (3.54)  Time: 0.997s, 1027.38/s  (1.012s, 1011.39/s)  LR: 5.619e-04  Data: 0.012 (0.015)
Train: 139 [ 450/1251 ( 36%)]  Loss: 3.473 (3.53)  Time: 1.055s,  970.32/s  (1.011s, 1012.82/s)  LR: 5.619e-04  Data: 0.011 (0.015)
Train: 139 [ 500/1251 ( 40%)]  Loss: 3.859 (3.56)  Time: 1.029s,  994.82/s  (1.011s, 1013.03/s)  LR: 5.619e-04  Data: 0.010 (0.014)
Train: 139 [ 550/1251 ( 44%)]  Loss: 3.319 (3.54)  Time: 0.995s, 1029.22/s  (1.011s, 1012.84/s)  LR: 5.619e-04  Data: 0.010 (0.014)
Train: 139 [ 600/1251 ( 48%)]  Loss: 3.629 (3.55)  Time: 0.994s, 1029.74/s  (1.011s, 1012.41/s)  LR: 5.619e-04  Data: 0.011 (0.014)
Train: 139 [ 650/1251 ( 52%)]  Loss: 3.460 (3.54)  Time: 0.995s, 1028.80/s  (1.011s, 1013.28/s)  LR: 5.619e-04  Data: 0.012 (0.014)
Train: 139 [ 700/1251 ( 56%)]  Loss: 3.437 (3.53)  Time: 1.000s, 1023.57/s  (1.010s, 1013.46/s)  LR: 5.619e-04  Data: 0.011 (0.013)
Train: 139 [ 750/1251 ( 60%)]  Loss: 3.665 (3.54)  Time: 1.028s,  996.54/s  (1.011s, 1012.96/s)  LR: 5.619e-04  Data: 0.010 (0.013)
Train: 139 [ 800/1251 ( 64%)]  Loss: 3.334 (3.53)  Time: 0.996s, 1027.70/s  (1.010s, 1013.38/s)  LR: 5.619e-04  Data: 0.010 (0.013)
Train: 139 [ 850/1251 ( 68%)]  Loss: 3.558 (3.53)  Time: 0.995s, 1029.44/s  (1.010s, 1013.83/s)  LR: 5.619e-04  Data: 0.010 (0.013)
Train: 139 [ 900/1251 ( 72%)]  Loss: 4.033 (3.56)  Time: 0.997s, 1026.86/s  (1.012s, 1012.23/s)  LR: 5.619e-04  Data: 0.011 (0.013)
Train: 139 [ 950/1251 ( 76%)]  Loss: 3.551 (3.56)  Time: 0.992s, 1031.90/s  (1.011s, 1012.98/s)  LR: 5.619e-04  Data: 0.011 (0.013)
Train: 139 [1000/1251 ( 80%)]  Loss: 3.625 (3.56)  Time: 1.063s,  963.32/s  (1.011s, 1013.03/s)  LR: 5.619e-04  Data: 0.011 (0.013)
Train: 139 [1050/1251 ( 84%)]  Loss: 3.093 (3.54)  Time: 0.996s, 1027.89/s  (1.011s, 1012.91/s)  LR: 5.619e-04  Data: 0.011 (0.013)
Train: 139 [1100/1251 ( 88%)]  Loss: 3.616 (3.54)  Time: 0.996s, 1028.09/s  (1.011s, 1013.13/s)  LR: 5.619e-04  Data: 0.011 (0.013)
Train: 139 [1150/1251 ( 92%)]  Loss: 3.798 (3.55)  Time: 1.060s,  966.27/s  (1.010s, 1013.50/s)  LR: 5.619e-04  Data: 0.011 (0.012)
Train: 139 [1200/1251 ( 96%)]  Loss: 3.828 (3.56)  Time: 0.994s, 1029.72/s  (1.010s, 1013.63/s)  LR: 5.619e-04  Data: 0.011 (0.012)
Train: 139 [1250/1251 (100%)]  Loss: 3.860 (3.58)  Time: 1.019s, 1005.16/s  (1.010s, 1013.37/s)  LR: 5.619e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.650 (1.650)  Loss:  0.7295 (0.7295)  Acc@1: 88.5742 (88.5742)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.245 (0.566)  Loss:  0.9129 (1.2878)  Acc@1: 83.1368 (74.2540)  Acc@5: 95.6368 (92.2500)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-139.pth.tar', 74.25400017333985)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-138.pth.tar', 74.24999990722657)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-132.pth.tar', 74.09400000976562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-135.pth.tar', 73.98199996337891)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-133.pth.tar', 73.93400001464843)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-131.pth.tar', 73.87800016601562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-130.pth.tar', 73.56600009521485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-134.pth.tar', 73.55400001708985)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-137.pth.tar', 73.53800012207032)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-122.pth.tar', 73.5220001171875)

Train: 140 [   0/1251 (  0%)]  Loss: 3.707 (3.71)  Time: 2.607s,  392.74/s  (2.607s,  392.74/s)  LR: 5.567e-04  Data: 1.639 (1.639)
Train: 140 [  50/1251 (  4%)]  Loss: 3.604 (3.66)  Time: 1.002s, 1021.69/s  (1.050s,  975.15/s)  LR: 5.567e-04  Data: 0.012 (0.045)
Train: 140 [ 100/1251 (  8%)]  Loss: 3.107 (3.47)  Time: 1.006s, 1017.73/s  (1.025s,  998.79/s)  LR: 5.567e-04  Data: 0.012 (0.028)
Train: 140 [ 150/1251 ( 12%)]  Loss: 3.421 (3.46)  Time: 0.993s, 1030.80/s  (1.018s, 1006.05/s)  LR: 5.567e-04  Data: 0.011 (0.022)
Train: 140 [ 200/1251 ( 16%)]  Loss: 3.269 (3.42)  Time: 0.997s, 1027.17/s  (1.015s, 1008.59/s)  LR: 5.567e-04  Data: 0.012 (0.020)
Train: 140 [ 250/1251 ( 20%)]  Loss: 3.404 (3.42)  Time: 1.001s, 1022.80/s  (1.016s, 1008.07/s)  LR: 5.567e-04  Data: 0.011 (0.018)
Train: 140 [ 300/1251 ( 24%)]  Loss: 3.610 (3.45)  Time: 0.994s, 1029.82/s  (1.013s, 1010.89/s)  LR: 5.567e-04  Data: 0.011 (0.017)
Train: 140 [ 350/1251 ( 28%)]  Loss: 3.400 (3.44)  Time: 0.994s, 1029.75/s  (1.014s, 1009.83/s)  LR: 5.567e-04  Data: 0.011 (0.016)
Train: 140 [ 400/1251 ( 32%)]  Loss: 3.965 (3.50)  Time: 0.996s, 1028.39/s  (1.012s, 1011.96/s)  LR: 5.567e-04  Data: 0.011 (0.015)
Train: 140 [ 450/1251 ( 36%)]  Loss: 3.853 (3.53)  Time: 1.038s,  986.44/s  (1.012s, 1011.37/s)  LR: 5.567e-04  Data: 0.011 (0.015)
Train: 140 [ 500/1251 ( 40%)]  Loss: 3.406 (3.52)  Time: 1.001s, 1023.47/s  (1.013s, 1011.21/s)  LR: 5.567e-04  Data: 0.011 (0.015)
Train: 140 [ 550/1251 ( 44%)]  Loss: 3.384 (3.51)  Time: 0.999s, 1025.22/s  (1.011s, 1012.46/s)  LR: 5.567e-04  Data: 0.011 (0.014)
Train: 140 [ 600/1251 ( 48%)]  Loss: 3.574 (3.52)  Time: 0.997s, 1027.04/s  (1.011s, 1012.94/s)  LR: 5.567e-04  Data: 0.011 (0.014)
Train: 140 [ 650/1251 ( 52%)]  Loss: 3.358 (3.50)  Time: 0.996s, 1028.20/s  (1.010s, 1013.82/s)  LR: 5.567e-04  Data: 0.011 (0.014)
Train: 140 [ 700/1251 ( 56%)]  Loss: 3.550 (3.51)  Time: 1.003s, 1021.28/s  (1.009s, 1014.51/s)  LR: 5.567e-04  Data: 0.012 (0.014)
Train: 140 [ 750/1251 ( 60%)]  Loss: 3.633 (3.52)  Time: 0.994s, 1030.27/s  (1.009s, 1014.92/s)  LR: 5.567e-04  Data: 0.010 (0.014)
Train: 140 [ 800/1251 ( 64%)]  Loss: 3.773 (3.53)  Time: 0.995s, 1029.19/s  (1.009s, 1015.34/s)  LR: 5.567e-04  Data: 0.011 (0.013)
Train: 140 [ 850/1251 ( 68%)]  Loss: 3.305 (3.52)  Time: 1.000s, 1023.64/s  (1.008s, 1015.75/s)  LR: 5.567e-04  Data: 0.010 (0.013)
Train: 140 [ 900/1251 ( 72%)]  Loss: 3.298 (3.51)  Time: 0.998s, 1026.23/s  (1.008s, 1016.35/s)  LR: 5.567e-04  Data: 0.012 (0.013)
Train: 140 [ 950/1251 ( 76%)]  Loss: 3.249 (3.49)  Time: 0.996s, 1028.04/s  (1.008s, 1015.67/s)  LR: 5.567e-04  Data: 0.011 (0.013)
Train: 140 [1000/1251 ( 80%)]  Loss: 3.555 (3.50)  Time: 1.032s,  992.23/s  (1.008s, 1015.52/s)  LR: 5.567e-04  Data: 0.011 (0.013)
Train: 140 [1050/1251 ( 84%)]  Loss: 3.641 (3.50)  Time: 1.039s,  985.59/s  (1.009s, 1015.14/s)  LR: 5.567e-04  Data: 0.012 (0.013)
Train: 140 [1100/1251 ( 88%)]  Loss: 3.923 (3.52)  Time: 0.996s, 1028.28/s  (1.008s, 1015.46/s)  LR: 5.567e-04  Data: 0.012 (0.013)
Train: 140 [1150/1251 ( 92%)]  Loss: 3.407 (3.52)  Time: 0.990s, 1034.40/s  (1.008s, 1015.53/s)  LR: 5.567e-04  Data: 0.011 (0.013)
Train: 140 [1200/1251 ( 96%)]  Loss: 3.556 (3.52)  Time: 0.994s, 1029.80/s  (1.008s, 1015.75/s)  LR: 5.567e-04  Data: 0.011 (0.013)
Train: 140 [1250/1251 (100%)]  Loss: 3.307 (3.51)  Time: 0.981s, 1044.21/s  (1.008s, 1016.07/s)  LR: 5.567e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.602 (1.602)  Loss:  0.8247 (0.8247)  Acc@1: 89.5508 (89.5508)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.245 (0.574)  Loss:  0.9678 (1.3680)  Acc@1: 82.3113 (73.5900)  Acc@5: 95.0472 (91.9800)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-139.pth.tar', 74.25400017333985)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-138.pth.tar', 74.24999990722657)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-132.pth.tar', 74.09400000976562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-135.pth.tar', 73.98199996337891)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-133.pth.tar', 73.93400001464843)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-131.pth.tar', 73.87800016601562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-140.pth.tar', 73.58999994384766)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-130.pth.tar', 73.56600009521485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-134.pth.tar', 73.55400001708985)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-137.pth.tar', 73.53800012207032)

Train: 141 [   0/1251 (  0%)]  Loss: 3.738 (3.74)  Time: 2.442s,  419.40/s  (2.442s,  419.40/s)  LR: 5.516e-04  Data: 1.489 (1.489)
Train: 141 [  50/1251 (  4%)]  Loss: 3.638 (3.69)  Time: 1.001s, 1022.67/s  (1.042s,  983.17/s)  LR: 5.516e-04  Data: 0.011 (0.042)
Train: 141 [ 100/1251 (  8%)]  Loss: 3.758 (3.71)  Time: 0.994s, 1029.89/s  (1.032s,  992.36/s)  LR: 5.516e-04  Data: 0.011 (0.026)
Train: 141 [ 150/1251 ( 12%)]  Loss: 3.677 (3.70)  Time: 0.995s, 1029.22/s  (1.021s, 1002.56/s)  LR: 5.516e-04  Data: 0.011 (0.021)
Train: 141 [ 200/1251 ( 16%)]  Loss: 3.446 (3.65)  Time: 1.025s,  998.89/s  (1.015s, 1008.76/s)  LR: 5.516e-04  Data: 0.012 (0.019)
Train: 141 [ 250/1251 ( 20%)]  Loss: 3.599 (3.64)  Time: 0.995s, 1029.08/s  (1.012s, 1011.79/s)  LR: 5.516e-04  Data: 0.011 (0.017)
Train: 141 [ 300/1251 ( 24%)]  Loss: 3.674 (3.65)  Time: 0.995s, 1029.03/s  (1.015s, 1008.71/s)  LR: 5.516e-04  Data: 0.012 (0.016)
Train: 141 [ 350/1251 ( 28%)]  Loss: 3.500 (3.63)  Time: 0.995s, 1029.11/s  (1.015s, 1009.19/s)  LR: 5.516e-04  Data: 0.011 (0.015)
Train: 141 [ 400/1251 ( 32%)]  Loss: 3.270 (3.59)  Time: 0.998s, 1026.17/s  (1.013s, 1011.31/s)  LR: 5.516e-04  Data: 0.011 (0.015)
Train: 141 [ 450/1251 ( 36%)]  Loss: 3.655 (3.60)  Time: 1.036s,  988.62/s  (1.015s, 1009.11/s)  LR: 5.516e-04  Data: 0.011 (0.015)
Train: 141 [ 500/1251 ( 40%)]  Loss: 3.351 (3.57)  Time: 0.995s, 1029.01/s  (1.013s, 1010.48/s)  LR: 5.516e-04  Data: 0.011 (0.014)
Train: 141 [ 550/1251 ( 44%)]  Loss: 3.594 (3.58)  Time: 1.067s,  959.61/s  (1.013s, 1011.08/s)  LR: 5.516e-04  Data: 0.012 (0.014)
Train: 141 [ 600/1251 ( 48%)]  Loss: 3.632 (3.58)  Time: 0.997s, 1027.47/s  (1.013s, 1010.97/s)  LR: 5.516e-04  Data: 0.011 (0.014)
Train: 141 [ 650/1251 ( 52%)]  Loss: 3.704 (3.59)  Time: 0.993s, 1031.29/s  (1.012s, 1011.48/s)  LR: 5.516e-04  Data: 0.012 (0.014)
Train: 141 [ 700/1251 ( 56%)]  Loss: 3.571 (3.59)  Time: 1.035s,  989.07/s  (1.012s, 1012.16/s)  LR: 5.516e-04  Data: 0.012 (0.013)
Train: 141 [ 750/1251 ( 60%)]  Loss: 3.706 (3.59)  Time: 0.995s, 1028.88/s  (1.012s, 1011.81/s)  LR: 5.516e-04  Data: 0.012 (0.013)
Train: 141 [ 800/1251 ( 64%)]  Loss: 3.638 (3.60)  Time: 0.999s, 1025.10/s  (1.011s, 1012.77/s)  LR: 5.516e-04  Data: 0.012 (0.013)
Train: 141 [ 850/1251 ( 68%)]  Loss: 3.334 (3.58)  Time: 0.998s, 1026.54/s  (1.011s, 1013.29/s)  LR: 5.516e-04  Data: 0.011 (0.013)
Train: 141 [ 900/1251 ( 72%)]  Loss: 3.626 (3.58)  Time: 1.001s, 1022.84/s  (1.010s, 1013.89/s)  LR: 5.516e-04  Data: 0.012 (0.013)
Train: 141 [ 950/1251 ( 76%)]  Loss: 3.799 (3.60)  Time: 1.033s,  991.34/s  (1.010s, 1013.46/s)  LR: 5.516e-04  Data: 0.010 (0.013)
Train: 141 [1000/1251 ( 80%)]  Loss: 3.891 (3.61)  Time: 0.992s, 1032.18/s  (1.010s, 1013.50/s)  LR: 5.516e-04  Data: 0.010 (0.013)
Train: 141 [1050/1251 ( 84%)]  Loss: 3.644 (3.61)  Time: 0.997s, 1027.29/s  (1.010s, 1014.15/s)  LR: 5.516e-04  Data: 0.011 (0.013)
Train: 141 [1100/1251 ( 88%)]  Loss: 3.774 (3.62)  Time: 1.037s,  987.12/s  (1.010s, 1014.20/s)  LR: 5.516e-04  Data: 0.011 (0.013)
Train: 141 [1150/1251 ( 92%)]  Loss: 3.492 (3.61)  Time: 0.995s, 1029.23/s  (1.010s, 1014.02/s)  LR: 5.516e-04  Data: 0.010 (0.013)
Train: 141 [1200/1251 ( 96%)]  Loss: 3.182 (3.60)  Time: 0.994s, 1030.12/s  (1.009s, 1014.53/s)  LR: 5.516e-04  Data: 0.011 (0.012)
Train: 141 [1250/1251 (100%)]  Loss: 3.560 (3.59)  Time: 0.983s, 1041.51/s  (1.009s, 1014.97/s)  LR: 5.516e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.714 (1.714)  Loss:  0.8081 (0.8081)  Acc@1: 88.9648 (88.9648)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.245 (0.571)  Loss:  0.8205 (1.3047)  Acc@1: 83.0189 (73.7620)  Acc@5: 96.9340 (92.0100)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-139.pth.tar', 74.25400017333985)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-138.pth.tar', 74.24999990722657)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-132.pth.tar', 74.09400000976562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-135.pth.tar', 73.98199996337891)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-133.pth.tar', 73.93400001464843)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-131.pth.tar', 73.87800016601562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-141.pth.tar', 73.76199999267578)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-140.pth.tar', 73.58999994384766)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-130.pth.tar', 73.56600009521485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-134.pth.tar', 73.55400001708985)

Train: 142 [   0/1251 (  0%)]  Loss: 3.350 (3.35)  Time: 2.351s,  435.47/s  (2.351s,  435.47/s)  LR: 5.464e-04  Data: 1.402 (1.402)
Train: 142 [  50/1251 (  4%)]  Loss: 3.615 (3.48)  Time: 0.996s, 1028.13/s  (1.045s,  979.88/s)  LR: 5.464e-04  Data: 0.012 (0.043)
Train: 142 [ 100/1251 (  8%)]  Loss: 3.844 (3.60)  Time: 1.004s, 1019.42/s  (1.022s, 1001.48/s)  LR: 5.464e-04  Data: 0.011 (0.028)
Train: 142 [ 150/1251 ( 12%)]  Loss: 3.546 (3.59)  Time: 1.061s,  964.69/s  (1.022s, 1001.68/s)  LR: 5.464e-04  Data: 0.011 (0.022)
Train: 142 [ 200/1251 ( 16%)]  Loss: 3.749 (3.62)  Time: 0.999s, 1024.60/s  (1.019s, 1004.70/s)  LR: 5.464e-04  Data: 0.010 (0.019)
Train: 142 [ 250/1251 ( 20%)]  Loss: 3.424 (3.59)  Time: 1.003s, 1021.37/s  (1.016s, 1008.31/s)  LR: 5.464e-04  Data: 0.011 (0.018)
Train: 142 [ 300/1251 ( 24%)]  Loss: 3.556 (3.58)  Time: 0.994s, 1029.94/s  (1.013s, 1011.24/s)  LR: 5.464e-04  Data: 0.010 (0.017)
Train: 142 [ 350/1251 ( 28%)]  Loss: 3.839 (3.62)  Time: 1.010s, 1013.58/s  (1.011s, 1013.25/s)  LR: 5.464e-04  Data: 0.011 (0.016)
Train: 142 [ 400/1251 ( 32%)]  Loss: 3.807 (3.64)  Time: 0.995s, 1029.58/s  (1.009s, 1014.73/s)  LR: 5.464e-04  Data: 0.011 (0.015)
Train: 142 [ 450/1251 ( 36%)]  Loss: 3.353 (3.61)  Time: 0.996s, 1027.83/s  (1.008s, 1015.58/s)  LR: 5.464e-04  Data: 0.011 (0.015)
Train: 142 [ 500/1251 ( 40%)]  Loss: 3.598 (3.61)  Time: 0.995s, 1028.90/s  (1.008s, 1015.80/s)  LR: 5.464e-04  Data: 0.011 (0.015)
Train: 142 [ 550/1251 ( 44%)]  Loss: 3.490 (3.60)  Time: 1.009s, 1014.42/s  (1.007s, 1016.49/s)  LR: 5.464e-04  Data: 0.010 (0.014)
Train: 142 [ 600/1251 ( 48%)]  Loss: 3.154 (3.56)  Time: 0.995s, 1029.25/s  (1.008s, 1015.86/s)  LR: 5.464e-04  Data: 0.010 (0.014)
Train: 142 [ 650/1251 ( 52%)]  Loss: 3.604 (3.57)  Time: 1.037s,  987.36/s  (1.009s, 1015.15/s)  LR: 5.464e-04  Data: 0.011 (0.014)
Train: 142 [ 700/1251 ( 56%)]  Loss: 3.544 (3.56)  Time: 0.997s, 1027.10/s  (1.008s, 1015.51/s)  LR: 5.464e-04  Data: 0.011 (0.014)
Train: 142 [ 750/1251 ( 60%)]  Loss: 3.255 (3.55)  Time: 0.995s, 1028.76/s  (1.008s, 1015.56/s)  LR: 5.464e-04  Data: 0.011 (0.013)
Train: 142 [ 800/1251 ( 64%)]  Loss: 3.536 (3.54)  Time: 0.997s, 1027.32/s  (1.008s, 1016.14/s)  LR: 5.464e-04  Data: 0.012 (0.013)
Train: 142 [ 850/1251 ( 68%)]  Loss: 3.261 (3.53)  Time: 0.995s, 1029.46/s  (1.007s, 1016.38/s)  LR: 5.464e-04  Data: 0.011 (0.013)
Train: 142 [ 900/1251 ( 72%)]  Loss: 3.436 (3.52)  Time: 1.064s,  962.63/s  (1.008s, 1015.98/s)  LR: 5.464e-04  Data: 0.012 (0.013)
Train: 142 [ 950/1251 ( 76%)]  Loss: 3.583 (3.53)  Time: 1.052s,  973.47/s  (1.009s, 1014.95/s)  LR: 5.464e-04  Data: 0.010 (0.013)
Train: 142 [1000/1251 ( 80%)]  Loss: 3.592 (3.53)  Time: 1.040s,  984.24/s  (1.009s, 1014.61/s)  LR: 5.464e-04  Data: 0.011 (0.013)
Train: 142 [1050/1251 ( 84%)]  Loss: 3.368 (3.52)  Time: 1.000s, 1023.82/s  (1.009s, 1014.81/s)  LR: 5.464e-04  Data: 0.011 (0.013)
Train: 142 [1100/1251 ( 88%)]  Loss: 3.568 (3.52)  Time: 0.995s, 1028.86/s  (1.009s, 1015.18/s)  LR: 5.464e-04  Data: 0.012 (0.013)
Train: 142 [1150/1251 ( 92%)]  Loss: 3.392 (3.52)  Time: 1.063s,  963.59/s  (1.009s, 1015.06/s)  LR: 5.464e-04  Data: 0.010 (0.013)
Train: 142 [1200/1251 ( 96%)]  Loss: 3.147 (3.50)  Time: 0.998s, 1026.32/s  (1.009s, 1015.14/s)  LR: 5.464e-04  Data: 0.010 (0.012)
Train: 142 [1250/1251 (100%)]  Loss: 3.703 (3.51)  Time: 1.030s,  994.13/s  (1.009s, 1014.68/s)  LR: 5.464e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.625 (1.625)  Loss:  0.7827 (0.7827)  Acc@1: 89.4531 (89.4531)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.245 (0.567)  Loss:  0.8111 (1.3279)  Acc@1: 84.5519 (74.5260)  Acc@5: 96.6981 (92.3340)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-142.pth.tar', 74.52600001220704)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-139.pth.tar', 74.25400017333985)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-138.pth.tar', 74.24999990722657)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-132.pth.tar', 74.09400000976562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-135.pth.tar', 73.98199996337891)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-133.pth.tar', 73.93400001464843)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-131.pth.tar', 73.87800016601562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-141.pth.tar', 73.76199999267578)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-140.pth.tar', 73.58999994384766)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-130.pth.tar', 73.56600009521485)

Train: 143 [   0/1251 (  0%)]  Loss: 3.492 (3.49)  Time: 2.450s,  418.00/s  (2.450s,  418.00/s)  LR: 5.413e-04  Data: 1.486 (1.486)
Train: 143 [  50/1251 (  4%)]  Loss: 3.561 (3.53)  Time: 1.003s, 1021.39/s  (1.039s,  985.98/s)  LR: 5.413e-04  Data: 0.013 (0.040)
Train: 143 [ 100/1251 (  8%)]  Loss: 3.260 (3.44)  Time: 0.994s, 1029.74/s  (1.033s,  991.10/s)  LR: 5.413e-04  Data: 0.011 (0.026)
Train: 143 [ 150/1251 ( 12%)]  Loss: 3.781 (3.52)  Time: 0.995s, 1028.69/s  (1.021s, 1002.85/s)  LR: 5.413e-04  Data: 0.012 (0.021)
Train: 143 [ 200/1251 ( 16%)]  Loss: 3.554 (3.53)  Time: 0.996s, 1027.91/s  (1.018s, 1005.85/s)  LR: 5.413e-04  Data: 0.011 (0.018)
Train: 143 [ 250/1251 ( 20%)]  Loss: 3.709 (3.56)  Time: 0.993s, 1031.15/s  (1.015s, 1009.29/s)  LR: 5.413e-04  Data: 0.010 (0.017)
Train: 143 [ 300/1251 ( 24%)]  Loss: 3.636 (3.57)  Time: 1.001s, 1022.51/s  (1.012s, 1012.05/s)  LR: 5.413e-04  Data: 0.013 (0.016)
Train: 143 [ 350/1251 ( 28%)]  Loss: 4.023 (3.63)  Time: 0.994s, 1030.34/s  (1.010s, 1014.05/s)  LR: 5.413e-04  Data: 0.010 (0.015)
Train: 143 [ 400/1251 ( 32%)]  Loss: 3.586 (3.62)  Time: 0.997s, 1026.97/s  (1.008s, 1015.73/s)  LR: 5.413e-04  Data: 0.011 (0.015)
Train: 143 [ 450/1251 ( 36%)]  Loss: 3.547 (3.62)  Time: 0.998s, 1025.80/s  (1.009s, 1014.58/s)  LR: 5.413e-04  Data: 0.011 (0.014)
Train: 143 [ 500/1251 ( 40%)]  Loss: 3.362 (3.59)  Time: 0.993s, 1030.90/s  (1.010s, 1013.94/s)  LR: 5.413e-04  Data: 0.010 (0.014)
Train: 143 [ 550/1251 ( 44%)]  Loss: 4.012 (3.63)  Time: 1.036s,  988.51/s  (1.009s, 1014.75/s)  LR: 5.413e-04  Data: 0.011 (0.014)
Train: 143 [ 600/1251 ( 48%)]  Loss: 3.346 (3.61)  Time: 1.050s,  975.69/s  (1.010s, 1014.07/s)  LR: 5.413e-04  Data: 0.011 (0.014)
Train: 143 [ 650/1251 ( 52%)]  Loss: 3.705 (3.61)  Time: 0.997s, 1027.13/s  (1.010s, 1013.74/s)  LR: 5.413e-04  Data: 0.010 (0.013)
Train: 143 [ 700/1251 ( 56%)]  Loss: 3.751 (3.62)  Time: 0.994s, 1030.24/s  (1.009s, 1014.54/s)  LR: 5.413e-04  Data: 0.012 (0.013)
Train: 143 [ 750/1251 ( 60%)]  Loss: 3.775 (3.63)  Time: 0.994s, 1030.25/s  (1.010s, 1013.81/s)  LR: 5.413e-04  Data: 0.011 (0.013)
Train: 143 [ 800/1251 ( 64%)]  Loss: 3.801 (3.64)  Time: 1.006s, 1017.65/s  (1.010s, 1014.09/s)  LR: 5.413e-04  Data: 0.011 (0.013)
Train: 143 [ 850/1251 ( 68%)]  Loss: 3.537 (3.64)  Time: 0.995s, 1029.47/s  (1.010s, 1014.15/s)  LR: 5.413e-04  Data: 0.011 (0.013)
Train: 143 [ 900/1251 ( 72%)]  Loss: 4.081 (3.66)  Time: 1.065s,  961.70/s  (1.009s, 1014.61/s)  LR: 5.413e-04  Data: 0.012 (0.013)
Train: 143 [ 950/1251 ( 76%)]  Loss: 3.844 (3.67)  Time: 0.991s, 1033.08/s  (1.010s, 1013.89/s)  LR: 5.413e-04  Data: 0.010 (0.013)
Train: 143 [1000/1251 ( 80%)]  Loss: 3.507 (3.66)  Time: 0.994s, 1030.14/s  (1.010s, 1013.93/s)  LR: 5.413e-04  Data: 0.011 (0.013)
Train: 143 [1050/1251 ( 84%)]  Loss: 3.475 (3.65)  Time: 0.997s, 1027.26/s  (1.009s, 1014.39/s)  LR: 5.413e-04  Data: 0.011 (0.012)
Train: 143 [1100/1251 ( 88%)]  Loss: 3.527 (3.65)  Time: 0.995s, 1029.20/s  (1.009s, 1014.76/s)  LR: 5.413e-04  Data: 0.010 (0.012)
Train: 143 [1150/1251 ( 92%)]  Loss: 3.594 (3.64)  Time: 1.052s,  973.24/s  (1.010s, 1013.86/s)  LR: 5.413e-04  Data: 0.011 (0.012)
Train: 143 [1200/1251 ( 96%)]  Loss: 3.453 (3.64)  Time: 1.064s,  962.34/s  (1.010s, 1013.83/s)  LR: 5.413e-04  Data: 0.011 (0.012)
Train: 143 [1250/1251 (100%)]  Loss: 3.626 (3.64)  Time: 0.982s, 1042.61/s  (1.010s, 1013.41/s)  LR: 5.413e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.618 (1.618)  Loss:  0.8665 (0.8665)  Acc@1: 89.0625 (89.0625)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.245 (0.565)  Loss:  0.8591 (1.3178)  Acc@1: 84.3160 (74.3080)  Acc@5: 96.6981 (92.2360)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-142.pth.tar', 74.52600001220704)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-143.pth.tar', 74.3080000390625)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-139.pth.tar', 74.25400017333985)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-138.pth.tar', 74.24999990722657)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-132.pth.tar', 74.09400000976562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-135.pth.tar', 73.98199996337891)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-133.pth.tar', 73.93400001464843)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-131.pth.tar', 73.87800016601562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-141.pth.tar', 73.76199999267578)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-140.pth.tar', 73.58999994384766)

Train: 144 [   0/1251 (  0%)]  Loss: 3.539 (3.54)  Time: 2.728s,  375.38/s  (2.728s,  375.38/s)  LR: 5.361e-04  Data: 1.769 (1.769)
Train: 144 [  50/1251 (  4%)]  Loss: 3.917 (3.73)  Time: 1.029s,  995.50/s  (1.047s,  977.91/s)  LR: 5.361e-04  Data: 0.011 (0.046)
Train: 144 [ 100/1251 (  8%)]  Loss: 3.480 (3.65)  Time: 1.003s, 1021.42/s  (1.023s, 1000.90/s)  LR: 5.361e-04  Data: 0.011 (0.029)
Train: 144 [ 150/1251 ( 12%)]  Loss: 3.538 (3.62)  Time: 1.006s, 1018.21/s  (1.018s, 1006.35/s)  LR: 5.361e-04  Data: 0.011 (0.023)
Train: 144 [ 200/1251 ( 16%)]  Loss: 3.903 (3.68)  Time: 1.024s, 1000.13/s  (1.018s, 1005.59/s)  LR: 5.361e-04  Data: 0.011 (0.020)
Train: 144 [ 250/1251 ( 20%)]  Loss: 3.295 (3.61)  Time: 0.999s, 1024.97/s  (1.015s, 1009.18/s)  LR: 5.361e-04  Data: 0.011 (0.018)
Train: 144 [ 300/1251 ( 24%)]  Loss: 3.490 (3.59)  Time: 0.995s, 1029.16/s  (1.012s, 1011.84/s)  LR: 5.361e-04  Data: 0.011 (0.017)
Train: 144 [ 350/1251 ( 28%)]  Loss: 3.503 (3.58)  Time: 0.997s, 1027.20/s  (1.013s, 1010.72/s)  LR: 5.361e-04  Data: 0.011 (0.016)
Train: 144 [ 400/1251 ( 32%)]  Loss: 3.147 (3.53)  Time: 0.995s, 1029.17/s  (1.012s, 1012.28/s)  LR: 5.361e-04  Data: 0.010 (0.015)
Train: 144 [ 450/1251 ( 36%)]  Loss: 3.482 (3.53)  Time: 0.996s, 1028.46/s  (1.010s, 1013.84/s)  LR: 5.361e-04  Data: 0.012 (0.015)
Train: 144 [ 500/1251 ( 40%)]  Loss: 3.770 (3.55)  Time: 0.997s, 1026.93/s  (1.011s, 1012.69/s)  LR: 5.361e-04  Data: 0.010 (0.015)
Train: 144 [ 550/1251 ( 44%)]  Loss: 3.563 (3.55)  Time: 0.997s, 1027.22/s  (1.010s, 1013.48/s)  LR: 5.361e-04  Data: 0.011 (0.014)
Train: 144 [ 600/1251 ( 48%)]  Loss: 3.322 (3.53)  Time: 0.995s, 1028.93/s  (1.009s, 1014.49/s)  LR: 5.361e-04  Data: 0.011 (0.014)
Train: 144 [ 650/1251 ( 52%)]  Loss: 3.675 (3.54)  Time: 0.996s, 1028.30/s  (1.009s, 1015.27/s)  LR: 5.361e-04  Data: 0.011 (0.014)
Train: 144 [ 700/1251 ( 56%)]  Loss: 3.461 (3.54)  Time: 0.995s, 1029.27/s  (1.008s, 1015.72/s)  LR: 5.361e-04  Data: 0.010 (0.014)
Train: 144 [ 750/1251 ( 60%)]  Loss: 3.356 (3.53)  Time: 1.036s,  988.40/s  (1.008s, 1015.39/s)  LR: 5.361e-04  Data: 0.012 (0.013)
Train: 144 [ 800/1251 ( 64%)]  Loss: 3.546 (3.53)  Time: 0.998s, 1026.33/s  (1.008s, 1015.46/s)  LR: 5.361e-04  Data: 0.012 (0.013)
Train: 144 [ 850/1251 ( 68%)]  Loss: 3.722 (3.54)  Time: 0.996s, 1028.13/s  (1.010s, 1014.30/s)  LR: 5.361e-04  Data: 0.011 (0.013)
Train: 144 [ 900/1251 ( 72%)]  Loss: 3.577 (3.54)  Time: 1.032s,  991.82/s  (1.009s, 1014.57/s)  LR: 5.361e-04  Data: 0.010 (0.013)
Train: 144 [ 950/1251 ( 76%)]  Loss: 3.463 (3.54)  Time: 0.994s, 1030.59/s  (1.009s, 1014.89/s)  LR: 5.361e-04  Data: 0.011 (0.013)
Train: 144 [1000/1251 ( 80%)]  Loss: 3.370 (3.53)  Time: 1.035s,  989.45/s  (1.010s, 1013.58/s)  LR: 5.361e-04  Data: 0.011 (0.013)
Train: 144 [1050/1251 ( 84%)]  Loss: 3.924 (3.55)  Time: 0.993s, 1031.12/s  (1.010s, 1013.89/s)  LR: 5.361e-04  Data: 0.011 (0.013)
Train: 144 [1100/1251 ( 88%)]  Loss: 3.461 (3.54)  Time: 1.053s,  972.76/s  (1.010s, 1014.03/s)  LR: 5.361e-04  Data: 0.012 (0.013)
Train: 144 [1150/1251 ( 92%)]  Loss: 3.270 (3.53)  Time: 1.065s,  961.87/s  (1.011s, 1012.77/s)  LR: 5.361e-04  Data: 0.011 (0.013)
Train: 144 [1200/1251 ( 96%)]  Loss: 3.578 (3.53)  Time: 1.057s,  968.67/s  (1.011s, 1012.89/s)  LR: 5.361e-04  Data: 0.011 (0.013)
Train: 144 [1250/1251 (100%)]  Loss: 3.587 (3.54)  Time: 0.983s, 1041.51/s  (1.010s, 1013.41/s)  LR: 5.361e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.608 (1.608)  Loss:  0.8070 (0.8070)  Acc@1: 88.1836 (88.1836)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.245 (0.578)  Loss:  0.9064 (1.3066)  Acc@1: 83.2547 (74.4380)  Acc@5: 95.9906 (92.4560)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-142.pth.tar', 74.52600001220704)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-144.pth.tar', 74.43800009521485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-143.pth.tar', 74.3080000390625)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-139.pth.tar', 74.25400017333985)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-138.pth.tar', 74.24999990722657)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-132.pth.tar', 74.09400000976562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-135.pth.tar', 73.98199996337891)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-133.pth.tar', 73.93400001464843)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-131.pth.tar', 73.87800016601562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-141.pth.tar', 73.76199999267578)

Train: 145 [   0/1251 (  0%)]  Loss: 3.780 (3.78)  Time: 2.771s,  369.52/s  (2.771s,  369.52/s)  LR: 5.309e-04  Data: 1.764 (1.764)
Train: 145 [  50/1251 (  4%)]  Loss: 3.727 (3.75)  Time: 1.000s, 1023.53/s  (1.050s,  975.39/s)  LR: 5.309e-04  Data: 0.011 (0.046)
Train: 145 [ 100/1251 (  8%)]  Loss: 3.437 (3.65)  Time: 0.997s, 1026.78/s  (1.032s,  992.43/s)  LR: 5.309e-04  Data: 0.012 (0.029)
Train: 145 [ 150/1251 ( 12%)]  Loss: 3.482 (3.61)  Time: 0.995s, 1028.70/s  (1.021s, 1002.67/s)  LR: 5.309e-04  Data: 0.012 (0.023)
Train: 145 [ 200/1251 ( 16%)]  Loss: 3.439 (3.57)  Time: 1.033s,  991.52/s  (1.018s, 1006.37/s)  LR: 5.309e-04  Data: 0.010 (0.020)
Train: 145 [ 250/1251 ( 20%)]  Loss: 2.990 (3.48)  Time: 0.997s, 1027.34/s  (1.016s, 1008.25/s)  LR: 5.309e-04  Data: 0.011 (0.018)
Train: 145 [ 300/1251 ( 24%)]  Loss: 3.637 (3.50)  Time: 0.995s, 1029.47/s  (1.013s, 1011.26/s)  LR: 5.309e-04  Data: 0.011 (0.017)
Train: 145 [ 350/1251 ( 28%)]  Loss: 3.641 (3.52)  Time: 1.024s, 1000.05/s  (1.011s, 1013.00/s)  LR: 5.309e-04  Data: 0.011 (0.016)
Train: 145 [ 400/1251 ( 32%)]  Loss: 3.347 (3.50)  Time: 0.995s, 1029.16/s  (1.012s, 1012.11/s)  LR: 5.309e-04  Data: 0.010 (0.016)
Train: 145 [ 450/1251 ( 36%)]  Loss: 3.454 (3.49)  Time: 0.994s, 1030.61/s  (1.013s, 1011.34/s)  LR: 5.309e-04  Data: 0.010 (0.015)
Train: 145 [ 500/1251 ( 40%)]  Loss: 3.656 (3.51)  Time: 0.996s, 1027.88/s  (1.011s, 1012.80/s)  LR: 5.309e-04  Data: 0.011 (0.015)
Train: 145 [ 550/1251 ( 44%)]  Loss: 3.481 (3.51)  Time: 0.995s, 1029.49/s  (1.012s, 1012.04/s)  LR: 5.309e-04  Data: 0.011 (0.014)
Train: 145 [ 600/1251 ( 48%)]  Loss: 3.148 (3.48)  Time: 1.030s,  993.75/s  (1.013s, 1011.05/s)  LR: 5.309e-04  Data: 0.010 (0.014)
Train: 145 [ 650/1251 ( 52%)]  Loss: 3.527 (3.48)  Time: 1.012s, 1011.67/s  (1.012s, 1011.68/s)  LR: 5.309e-04  Data: 0.011 (0.014)
Train: 145 [ 700/1251 ( 56%)]  Loss: 3.639 (3.49)  Time: 1.001s, 1023.06/s  (1.011s, 1012.52/s)  LR: 5.309e-04  Data: 0.012 (0.014)
Train: 145 [ 750/1251 ( 60%)]  Loss: 3.919 (3.52)  Time: 0.996s, 1028.25/s  (1.010s, 1013.45/s)  LR: 5.309e-04  Data: 0.011 (0.013)
Train: 145 [ 800/1251 ( 64%)]  Loss: 3.617 (3.52)  Time: 1.035s,  989.57/s  (1.010s, 1013.39/s)  LR: 5.309e-04  Data: 0.011 (0.013)
Train: 145 [ 850/1251 ( 68%)]  Loss: 3.814 (3.54)  Time: 0.995s, 1028.92/s  (1.010s, 1013.51/s)  LR: 5.309e-04  Data: 0.011 (0.013)
Train: 145 [ 900/1251 ( 72%)]  Loss: 3.498 (3.54)  Time: 0.997s, 1027.46/s  (1.010s, 1014.16/s)  LR: 5.309e-04  Data: 0.014 (0.013)
Train: 145 [ 950/1251 ( 76%)]  Loss: 3.222 (3.52)  Time: 1.034s,  990.00/s  (1.009s, 1014.69/s)  LR: 5.309e-04  Data: 0.011 (0.013)
Train: 145 [1000/1251 ( 80%)]  Loss: 3.337 (3.51)  Time: 0.996s, 1028.17/s  (1.009s, 1015.05/s)  LR: 5.309e-04  Data: 0.011 (0.013)
Train: 145 [1050/1251 ( 84%)]  Loss: 3.477 (3.51)  Time: 0.996s, 1027.98/s  (1.008s, 1015.54/s)  LR: 5.309e-04  Data: 0.011 (0.013)
Train: 145 [1100/1251 ( 88%)]  Loss: 3.387 (3.51)  Time: 0.998s, 1026.53/s  (1.008s, 1016.07/s)  LR: 5.309e-04  Data: 0.011 (0.013)
Train: 145 [1150/1251 ( 92%)]  Loss: 3.224 (3.50)  Time: 1.002s, 1022.27/s  (1.007s, 1016.43/s)  LR: 5.309e-04  Data: 0.010 (0.013)
Train: 145 [1200/1251 ( 96%)]  Loss: 3.751 (3.51)  Time: 0.996s, 1028.57/s  (1.007s, 1016.76/s)  LR: 5.309e-04  Data: 0.010 (0.013)
Train: 145 [1250/1251 (100%)]  Loss: 3.782 (3.52)  Time: 0.981s, 1043.74/s  (1.007s, 1017.11/s)  LR: 5.309e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.716 (1.716)  Loss:  0.7823 (0.7823)  Acc@1: 89.0625 (89.0625)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.245 (0.568)  Loss:  0.8617 (1.3124)  Acc@1: 84.5519 (74.2240)  Acc@5: 96.4623 (92.3560)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-142.pth.tar', 74.52600001220704)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-144.pth.tar', 74.43800009521485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-143.pth.tar', 74.3080000390625)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-139.pth.tar', 74.25400017333985)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-138.pth.tar', 74.24999990722657)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-145.pth.tar', 74.22400014160156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-132.pth.tar', 74.09400000976562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-135.pth.tar', 73.98199996337891)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-133.pth.tar', 73.93400001464843)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-131.pth.tar', 73.87800016601562)

Train: 146 [   0/1251 (  0%)]  Loss: 3.523 (3.52)  Time: 2.456s,  416.87/s  (2.456s,  416.87/s)  LR: 5.257e-04  Data: 1.494 (1.494)
Train: 146 [  50/1251 (  4%)]  Loss: 3.381 (3.45)  Time: 0.995s, 1029.04/s  (1.041s,  983.51/s)  LR: 5.257e-04  Data: 0.011 (0.040)
Train: 146 [ 100/1251 (  8%)]  Loss: 3.631 (3.51)  Time: 0.998s, 1025.77/s  (1.021s, 1002.65/s)  LR: 5.257e-04  Data: 0.012 (0.026)
Train: 146 [ 150/1251 ( 12%)]  Loss: 3.700 (3.56)  Time: 1.063s,  963.60/s  (1.025s,  999.09/s)  LR: 5.257e-04  Data: 0.012 (0.021)
Train: 146 [ 200/1251 ( 16%)]  Loss: 3.436 (3.53)  Time: 1.067s,  959.45/s  (1.024s, 1000.31/s)  LR: 5.257e-04  Data: 0.011 (0.018)
Train: 146 [ 250/1251 ( 20%)]  Loss: 3.299 (3.50)  Time: 0.996s, 1028.38/s  (1.023s, 1000.79/s)  LR: 5.257e-04  Data: 0.011 (0.017)
Train: 146 [ 300/1251 ( 24%)]  Loss: 3.364 (3.48)  Time: 1.001s, 1022.79/s  (1.019s, 1004.96/s)  LR: 5.257e-04  Data: 0.010 (0.016)
Train: 146 [ 350/1251 ( 28%)]  Loss: 3.551 (3.49)  Time: 1.041s,  983.68/s  (1.017s, 1006.50/s)  LR: 5.257e-04  Data: 0.011 (0.015)
Train: 146 [ 400/1251 ( 32%)]  Loss: 3.444 (3.48)  Time: 0.998s, 1025.68/s  (1.016s, 1007.81/s)  LR: 5.257e-04  Data: 0.011 (0.015)
Train: 146 [ 450/1251 ( 36%)]  Loss: 3.587 (3.49)  Time: 0.994s, 1030.07/s  (1.015s, 1008.61/s)  LR: 5.257e-04  Data: 0.011 (0.014)
Train: 146 [ 500/1251 ( 40%)]  Loss: 3.590 (3.50)  Time: 1.004s, 1019.63/s  (1.015s, 1009.00/s)  LR: 5.257e-04  Data: 0.014 (0.014)
Train: 146 [ 550/1251 ( 44%)]  Loss: 3.883 (3.53)  Time: 0.995s, 1029.32/s  (1.013s, 1010.47/s)  LR: 5.257e-04  Data: 0.011 (0.014)
Train: 146 [ 600/1251 ( 48%)]  Loss: 3.587 (3.54)  Time: 1.013s, 1010.91/s  (1.013s, 1010.94/s)  LR: 5.257e-04  Data: 0.011 (0.013)
Train: 146 [ 650/1251 ( 52%)]  Loss: 3.409 (3.53)  Time: 0.993s, 1031.44/s  (1.012s, 1011.93/s)  LR: 5.257e-04  Data: 0.011 (0.013)
Train: 146 [ 700/1251 ( 56%)]  Loss: 4.021 (3.56)  Time: 1.002s, 1022.26/s  (1.011s, 1012.60/s)  LR: 5.257e-04  Data: 0.015 (0.013)
Train: 146 [ 750/1251 ( 60%)]  Loss: 3.621 (3.56)  Time: 1.002s, 1022.34/s  (1.011s, 1012.77/s)  LR: 5.257e-04  Data: 0.011 (0.013)
Train: 146 [ 800/1251 ( 64%)]  Loss: 3.729 (3.57)  Time: 0.994s, 1029.89/s  (1.010s, 1013.55/s)  LR: 5.257e-04  Data: 0.010 (0.013)
Train: 146 [ 850/1251 ( 68%)]  Loss: 3.682 (3.58)  Time: 0.997s, 1027.16/s  (1.010s, 1014.34/s)  LR: 5.257e-04  Data: 0.011 (0.013)
Train: 146 [ 900/1251 ( 72%)]  Loss: 3.472 (3.57)  Time: 0.994s, 1029.89/s  (1.009s, 1014.96/s)  LR: 5.257e-04  Data: 0.011 (0.013)
Train: 146 [ 950/1251 ( 76%)]  Loss: 4.065 (3.60)  Time: 1.035s,  989.22/s  (1.009s, 1015.25/s)  LR: 5.257e-04  Data: 0.011 (0.013)
Train: 146 [1000/1251 ( 80%)]  Loss: 3.397 (3.59)  Time: 1.002s, 1021.87/s  (1.009s, 1014.66/s)  LR: 5.257e-04  Data: 0.010 (0.013)
Train: 146 [1050/1251 ( 84%)]  Loss: 3.500 (3.59)  Time: 0.992s, 1031.84/s  (1.009s, 1015.03/s)  LR: 5.257e-04  Data: 0.010 (0.012)
Train: 146 [1100/1251 ( 88%)]  Loss: 3.890 (3.60)  Time: 1.002s, 1021.97/s  (1.008s, 1015.44/s)  LR: 5.257e-04  Data: 0.010 (0.012)
Train: 146 [1150/1251 ( 92%)]  Loss: 3.375 (3.59)  Time: 1.004s, 1019.79/s  (1.009s, 1014.93/s)  LR: 5.257e-04  Data: 0.010 (0.012)
Train: 146 [1200/1251 ( 96%)]  Loss: 3.247 (3.58)  Time: 1.001s, 1023.15/s  (1.009s, 1014.91/s)  LR: 5.257e-04  Data: 0.011 (0.012)
Train: 146 [1250/1251 (100%)]  Loss: 3.760 (3.58)  Time: 1.018s, 1005.71/s  (1.009s, 1015.10/s)  LR: 5.257e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.606 (1.606)  Loss:  0.7765 (0.7765)  Acc@1: 89.2578 (89.2578)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.245 (0.568)  Loss:  0.9628 (1.3498)  Acc@1: 84.7877 (74.1440)  Acc@5: 95.0472 (92.0660)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-142.pth.tar', 74.52600001220704)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-144.pth.tar', 74.43800009521485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-143.pth.tar', 74.3080000390625)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-139.pth.tar', 74.25400017333985)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-138.pth.tar', 74.24999990722657)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-145.pth.tar', 74.22400014160156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-146.pth.tar', 74.14399998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-132.pth.tar', 74.09400000976562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-135.pth.tar', 73.98199996337891)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-133.pth.tar', 73.93400001464843)

Train: 147 [   0/1251 (  0%)]  Loss: 3.529 (3.53)  Time: 4.055s,  252.52/s  (4.055s,  252.52/s)  LR: 5.205e-04  Data: 2.928 (2.928)
Train: 147 [  50/1251 (  4%)]  Loss: 3.872 (3.70)  Time: 1.037s,  987.49/s  (1.087s,  942.08/s)  LR: 5.205e-04  Data: 0.012 (0.068)
Train: 147 [ 100/1251 (  8%)]  Loss: 3.296 (3.57)  Time: 1.002s, 1021.99/s  (1.048s,  977.42/s)  LR: 5.205e-04  Data: 0.011 (0.040)
Train: 147 [ 150/1251 ( 12%)]  Loss: 3.616 (3.58)  Time: 1.036s,  988.35/s  (1.032s,  992.22/s)  LR: 5.205e-04  Data: 0.012 (0.030)
Train: 147 [ 200/1251 ( 16%)]  Loss: 3.444 (3.55)  Time: 0.994s, 1030.04/s  (1.024s, 1000.19/s)  LR: 5.205e-04  Data: 0.011 (0.026)
Train: 147 [ 250/1251 ( 20%)]  Loss: 3.364 (3.52)  Time: 1.009s, 1014.97/s  (1.020s, 1004.22/s)  LR: 5.205e-04  Data: 0.011 (0.023)
Train: 147 [ 300/1251 ( 24%)]  Loss: 3.402 (3.50)  Time: 0.995s, 1029.13/s  (1.016s, 1007.74/s)  LR: 5.205e-04  Data: 0.012 (0.021)
Train: 147 [ 350/1251 ( 28%)]  Loss: 3.306 (3.48)  Time: 0.994s, 1029.76/s  (1.014s, 1009.72/s)  LR: 5.205e-04  Data: 0.011 (0.019)
Train: 147 [ 400/1251 ( 32%)]  Loss: 3.388 (3.47)  Time: 0.993s, 1030.78/s  (1.013s, 1010.48/s)  LR: 5.205e-04  Data: 0.011 (0.018)
Train: 147 [ 450/1251 ( 36%)]  Loss: 3.331 (3.45)  Time: 1.000s, 1024.00/s  (1.013s, 1010.87/s)  LR: 5.205e-04  Data: 0.012 (0.018)
Train: 147 [ 500/1251 ( 40%)]  Loss: 3.445 (3.45)  Time: 0.997s, 1027.54/s  (1.012s, 1012.24/s)  LR: 5.205e-04  Data: 0.011 (0.017)
Train: 147 [ 550/1251 ( 44%)]  Loss: 3.594 (3.47)  Time: 1.054s,  971.61/s  (1.011s, 1013.17/s)  LR: 5.205e-04  Data: 0.011 (0.017)
Train: 147 [ 600/1251 ( 48%)]  Loss: 3.307 (3.45)  Time: 1.005s, 1018.48/s  (1.011s, 1012.55/s)  LR: 5.205e-04  Data: 0.014 (0.016)
Train: 147 [ 650/1251 ( 52%)]  Loss: 3.722 (3.47)  Time: 0.996s, 1027.61/s  (1.010s, 1013.44/s)  LR: 5.205e-04  Data: 0.012 (0.016)
Train: 147 [ 700/1251 ( 56%)]  Loss: 3.466 (3.47)  Time: 1.000s, 1023.85/s  (1.010s, 1014.09/s)  LR: 5.205e-04  Data: 0.012 (0.015)
Train: 147 [ 750/1251 ( 60%)]  Loss: 3.493 (3.47)  Time: 1.054s,  971.27/s  (1.010s, 1013.63/s)  LR: 5.205e-04  Data: 0.011 (0.015)
Train: 147 [ 800/1251 ( 64%)]  Loss: 3.273 (3.46)  Time: 1.030s,  994.22/s  (1.012s, 1012.16/s)  LR: 5.205e-04  Data: 0.011 (0.015)
Train: 147 [ 850/1251 ( 68%)]  Loss: 3.212 (3.45)  Time: 0.996s, 1028.59/s  (1.011s, 1012.83/s)  LR: 5.205e-04  Data: 0.012 (0.015)
Train: 147 [ 900/1251 ( 72%)]  Loss: 4.137 (3.48)  Time: 0.995s, 1028.79/s  (1.011s, 1013.28/s)  LR: 5.205e-04  Data: 0.011 (0.015)
Train: 147 [ 950/1251 ( 76%)]  Loss: 3.884 (3.50)  Time: 0.997s, 1026.73/s  (1.010s, 1013.83/s)  LR: 5.205e-04  Data: 0.010 (0.014)
Train: 147 [1000/1251 ( 80%)]  Loss: 3.683 (3.51)  Time: 0.998s, 1026.24/s  (1.010s, 1014.15/s)  LR: 5.205e-04  Data: 0.012 (0.014)
Train: 147 [1050/1251 ( 84%)]  Loss: 3.338 (3.50)  Time: 0.997s, 1026.98/s  (1.009s, 1014.38/s)  LR: 5.205e-04  Data: 0.012 (0.014)
Train: 147 [1100/1251 ( 88%)]  Loss: 3.581 (3.51)  Time: 0.998s, 1026.30/s  (1.010s, 1014.18/s)  LR: 5.205e-04  Data: 0.012 (0.014)
Train: 147 [1150/1251 ( 92%)]  Loss: 3.441 (3.51)  Time: 0.992s, 1031.78/s  (1.009s, 1014.51/s)  LR: 5.205e-04  Data: 0.010 (0.014)
Train: 147 [1200/1251 ( 96%)]  Loss: 3.656 (3.51)  Time: 1.047s,  978.10/s  (1.009s, 1015.00/s)  LR: 5.205e-04  Data: 0.011 (0.014)
Train: 147 [1250/1251 (100%)]  Loss: 3.680 (3.52)  Time: 0.984s, 1040.80/s  (1.009s, 1015.21/s)  LR: 5.205e-04  Data: 0.000 (0.014)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.656 (1.656)  Loss:  0.8269 (0.8269)  Acc@1: 88.4766 (88.4766)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.245 (0.564)  Loss:  0.8380 (1.3411)  Acc@1: 85.3774 (74.3020)  Acc@5: 96.5802 (92.3860)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-142.pth.tar', 74.52600001220704)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-144.pth.tar', 74.43800009521485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-143.pth.tar', 74.3080000390625)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-147.pth.tar', 74.30199998291016)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-139.pth.tar', 74.25400017333985)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-138.pth.tar', 74.24999990722657)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-145.pth.tar', 74.22400014160156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-146.pth.tar', 74.14399998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-132.pth.tar', 74.09400000976562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-135.pth.tar', 73.98199996337891)

Train: 148 [   0/1251 (  0%)]  Loss: 3.570 (3.57)  Time: 2.595s,  394.60/s  (2.595s,  394.60/s)  LR: 5.154e-04  Data: 1.635 (1.635)
Train: 148 [  50/1251 (  4%)]  Loss: 3.708 (3.64)  Time: 0.996s, 1028.05/s  (1.038s,  986.05/s)  LR: 5.154e-04  Data: 0.012 (0.043)
Train: 148 [ 100/1251 (  8%)]  Loss: 3.622 (3.63)  Time: 0.996s, 1027.87/s  (1.035s,  989.53/s)  LR: 5.154e-04  Data: 0.011 (0.027)
Train: 148 [ 150/1251 ( 12%)]  Loss: 3.243 (3.54)  Time: 1.034s,  990.23/s  (1.024s, 1000.39/s)  LR: 5.154e-04  Data: 0.012 (0.022)
Train: 148 [ 200/1251 ( 16%)]  Loss: 3.754 (3.58)  Time: 0.993s, 1031.68/s  (1.018s, 1006.24/s)  LR: 5.154e-04  Data: 0.010 (0.019)
Train: 148 [ 250/1251 ( 20%)]  Loss: 3.280 (3.53)  Time: 1.032s,  992.36/s  (1.016s, 1008.12/s)  LR: 5.154e-04  Data: 0.011 (0.018)
Train: 148 [ 300/1251 ( 24%)]  Loss: 3.379 (3.51)  Time: 0.998s, 1026.39/s  (1.014s, 1009.92/s)  LR: 5.154e-04  Data: 0.012 (0.017)
Train: 148 [ 350/1251 ( 28%)]  Loss: 3.640 (3.52)  Time: 0.997s, 1026.62/s  (1.016s, 1007.81/s)  LR: 5.154e-04  Data: 0.011 (0.016)
Train: 148 [ 400/1251 ( 32%)]  Loss: 3.600 (3.53)  Time: 1.008s, 1016.08/s  (1.016s, 1007.80/s)  LR: 5.154e-04  Data: 0.011 (0.015)
Train: 148 [ 450/1251 ( 36%)]  Loss: 3.530 (3.53)  Time: 0.996s, 1028.37/s  (1.014s, 1009.80/s)  LR: 5.154e-04  Data: 0.011 (0.015)
Train: 148 [ 500/1251 ( 40%)]  Loss: 3.575 (3.54)  Time: 0.995s, 1028.81/s  (1.013s, 1011.34/s)  LR: 5.154e-04  Data: 0.011 (0.014)
Train: 148 [ 550/1251 ( 44%)]  Loss: 3.597 (3.54)  Time: 1.042s,  982.60/s  (1.012s, 1012.06/s)  LR: 5.154e-04  Data: 0.012 (0.014)
Train: 148 [ 600/1251 ( 48%)]  Loss: 3.782 (3.56)  Time: 0.995s, 1029.26/s  (1.012s, 1011.74/s)  LR: 5.154e-04  Data: 0.011 (0.014)
Train: 148 [ 650/1251 ( 52%)]  Loss: 3.894 (3.58)  Time: 0.995s, 1028.68/s  (1.013s, 1011.28/s)  LR: 5.154e-04  Data: 0.011 (0.014)
Train: 148 [ 700/1251 ( 56%)]  Loss: 3.564 (3.58)  Time: 1.066s,  960.19/s  (1.012s, 1011.67/s)  LR: 5.154e-04  Data: 0.011 (0.013)
Train: 148 [ 750/1251 ( 60%)]  Loss: 3.833 (3.60)  Time: 0.997s, 1027.04/s  (1.012s, 1011.93/s)  LR: 5.154e-04  Data: 0.012 (0.013)
Train: 148 [ 800/1251 ( 64%)]  Loss: 3.640 (3.60)  Time: 1.009s, 1015.04/s  (1.012s, 1011.74/s)  LR: 5.154e-04  Data: 0.010 (0.013)
Train: 148 [ 850/1251 ( 68%)]  Loss: 3.571 (3.60)  Time: 1.035s,  989.25/s  (1.012s, 1011.37/s)  LR: 5.154e-04  Data: 0.012 (0.013)
Train: 148 [ 900/1251 ( 72%)]  Loss: 3.499 (3.59)  Time: 0.995s, 1028.97/s  (1.012s, 1012.08/s)  LR: 5.154e-04  Data: 0.011 (0.013)
Train: 148 [ 950/1251 ( 76%)]  Loss: 3.782 (3.60)  Time: 0.995s, 1029.36/s  (1.011s, 1012.69/s)  LR: 5.154e-04  Data: 0.011 (0.013)
Train: 148 [1000/1251 ( 80%)]  Loss: 3.516 (3.60)  Time: 0.994s, 1029.99/s  (1.011s, 1013.24/s)  LR: 5.154e-04  Data: 0.011 (0.013)
Train: 148 [1050/1251 ( 84%)]  Loss: 3.686 (3.60)  Time: 0.999s, 1024.78/s  (1.010s, 1013.85/s)  LR: 5.154e-04  Data: 0.011 (0.013)
Train: 148 [1100/1251 ( 88%)]  Loss: 3.224 (3.59)  Time: 1.032s,  991.87/s  (1.010s, 1014.07/s)  LR: 5.154e-04  Data: 0.011 (0.013)
Train: 148 [1150/1251 ( 92%)]  Loss: 3.620 (3.59)  Time: 0.995s, 1029.03/s  (1.010s, 1014.25/s)  LR: 5.154e-04  Data: 0.010 (0.012)
Train: 148 [1200/1251 ( 96%)]  Loss: 3.725 (3.59)  Time: 1.001s, 1023.12/s  (1.009s, 1014.69/s)  LR: 5.154e-04  Data: 0.010 (0.012)
Train: 148 [1250/1251 (100%)]  Loss: 3.261 (3.58)  Time: 0.984s, 1041.10/s  (1.009s, 1014.78/s)  LR: 5.154e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.626 (1.626)  Loss:  0.8762 (0.8762)  Acc@1: 90.2344 (90.2344)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.245 (0.571)  Loss:  0.9729 (1.4184)  Acc@1: 83.1368 (74.1960)  Acc@5: 96.3443 (92.1520)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-142.pth.tar', 74.52600001220704)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-144.pth.tar', 74.43800009521485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-143.pth.tar', 74.3080000390625)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-147.pth.tar', 74.30199998291016)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-139.pth.tar', 74.25400017333985)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-138.pth.tar', 74.24999990722657)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-145.pth.tar', 74.22400014160156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-148.pth.tar', 74.19599991455078)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-146.pth.tar', 74.14399998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-132.pth.tar', 74.09400000976562)

Train: 149 [   0/1251 (  0%)]  Loss: 3.725 (3.72)  Time: 2.429s,  421.56/s  (2.429s,  421.56/s)  LR: 5.102e-04  Data: 1.469 (1.469)
Train: 149 [  50/1251 (  4%)]  Loss: 3.334 (3.53)  Time: 1.032s,  992.55/s  (1.056s,  969.88/s)  LR: 5.102e-04  Data: 0.011 (0.041)
Train: 149 [ 100/1251 (  8%)]  Loss: 3.622 (3.56)  Time: 0.995s, 1028.96/s  (1.030s,  994.22/s)  LR: 5.102e-04  Data: 0.011 (0.026)
Train: 149 [ 150/1251 ( 12%)]  Loss: 3.341 (3.51)  Time: 1.021s, 1002.83/s  (1.020s, 1003.94/s)  LR: 5.102e-04  Data: 0.012 (0.021)
Train: 149 [ 200/1251 ( 16%)]  Loss: 3.091 (3.42)  Time: 1.065s,  961.51/s  (1.017s, 1006.91/s)  LR: 5.102e-04  Data: 0.012 (0.019)
Train: 149 [ 250/1251 ( 20%)]  Loss: 3.392 (3.42)  Time: 0.996s, 1028.44/s  (1.016s, 1007.83/s)  LR: 5.102e-04  Data: 0.012 (0.017)
Train: 149 [ 300/1251 ( 24%)]  Loss: 3.649 (3.45)  Time: 1.004s, 1019.90/s  (1.015s, 1008.60/s)  LR: 5.102e-04  Data: 0.012 (0.016)
Train: 149 [ 350/1251 ( 28%)]  Loss: 3.358 (3.44)  Time: 0.997s, 1026.88/s  (1.013s, 1010.63/s)  LR: 5.102e-04  Data: 0.012 (0.016)
Train: 149 [ 400/1251 ( 32%)]  Loss: 3.680 (3.47)  Time: 0.996s, 1027.67/s  (1.012s, 1011.48/s)  LR: 5.102e-04  Data: 0.011 (0.015)
Train: 149 [ 450/1251 ( 36%)]  Loss: 3.921 (3.51)  Time: 0.997s, 1027.57/s  (1.011s, 1012.50/s)  LR: 5.102e-04  Data: 0.011 (0.015)
Train: 149 [ 500/1251 ( 40%)]  Loss: 3.673 (3.53)  Time: 0.993s, 1030.93/s  (1.010s, 1013.54/s)  LR: 5.102e-04  Data: 0.011 (0.014)
Train: 149 [ 550/1251 ( 44%)]  Loss: 3.435 (3.52)  Time: 0.996s, 1028.47/s  (1.010s, 1013.99/s)  LR: 5.102e-04  Data: 0.011 (0.014)
Train: 149 [ 600/1251 ( 48%)]  Loss: 3.263 (3.50)  Time: 0.995s, 1028.83/s  (1.009s, 1014.79/s)  LR: 5.102e-04  Data: 0.011 (0.014)
Train: 149 [ 650/1251 ( 52%)]  Loss: 3.687 (3.51)  Time: 1.007s, 1017.38/s  (1.009s, 1014.62/s)  LR: 5.102e-04  Data: 0.012 (0.014)
Train: 149 [ 700/1251 ( 56%)]  Loss: 3.679 (3.52)  Time: 1.036s,  988.07/s  (1.009s, 1014.82/s)  LR: 5.102e-04  Data: 0.012 (0.013)
Train: 149 [ 750/1251 ( 60%)]  Loss: 3.303 (3.51)  Time: 1.034s,  990.11/s  (1.011s, 1012.94/s)  LR: 5.102e-04  Data: 0.011 (0.013)
Train: 149 [ 800/1251 ( 64%)]  Loss: 3.523 (3.51)  Time: 0.998s, 1025.83/s  (1.011s, 1012.93/s)  LR: 5.102e-04  Data: 0.014 (0.013)
Train: 149 [ 850/1251 ( 68%)]  Loss: 2.875 (3.48)  Time: 0.993s, 1031.58/s  (1.010s, 1013.48/s)  LR: 5.102e-04  Data: 0.010 (0.013)
Train: 149 [ 900/1251 ( 72%)]  Loss: 3.405 (3.47)  Time: 1.035s,  989.76/s  (1.010s, 1013.72/s)  LR: 5.102e-04  Data: 0.011 (0.013)
Train: 149 [ 950/1251 ( 76%)]  Loss: 3.782 (3.49)  Time: 1.000s, 1023.79/s  (1.010s, 1014.27/s)  LR: 5.102e-04  Data: 0.011 (0.013)
Train: 149 [1000/1251 ( 80%)]  Loss: 3.065 (3.47)  Time: 0.997s, 1027.47/s  (1.009s, 1014.85/s)  LR: 5.102e-04  Data: 0.011 (0.013)
Train: 149 [1050/1251 ( 84%)]  Loss: 3.865 (3.48)  Time: 1.037s,  987.80/s  (1.009s, 1015.01/s)  LR: 5.102e-04  Data: 0.012 (0.013)
Train: 149 [1100/1251 ( 88%)]  Loss: 3.458 (3.48)  Time: 1.047s,  978.39/s  (1.010s, 1013.54/s)  LR: 5.102e-04  Data: 0.011 (0.013)
Train: 149 [1150/1251 ( 92%)]  Loss: 3.649 (3.49)  Time: 0.994s, 1029.81/s  (1.010s, 1013.98/s)  LR: 5.102e-04  Data: 0.011 (0.013)
Train: 149 [1200/1251 ( 96%)]  Loss: 3.437 (3.49)  Time: 0.994s, 1029.98/s  (1.009s, 1014.56/s)  LR: 5.102e-04  Data: 0.011 (0.012)
Train: 149 [1250/1251 (100%)]  Loss: 3.252 (3.48)  Time: 0.982s, 1042.25/s  (1.009s, 1014.99/s)  LR: 5.102e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.636 (1.636)  Loss:  0.8764 (0.8764)  Acc@1: 89.1602 (89.1602)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.245 (0.570)  Loss:  0.8719 (1.3462)  Acc@1: 84.3160 (74.1840)  Acc@5: 95.9906 (92.3360)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-142.pth.tar', 74.52600001220704)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-144.pth.tar', 74.43800009521485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-143.pth.tar', 74.3080000390625)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-147.pth.tar', 74.30199998291016)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-139.pth.tar', 74.25400017333985)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-138.pth.tar', 74.24999990722657)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-145.pth.tar', 74.22400014160156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-148.pth.tar', 74.19599991455078)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-149.pth.tar', 74.18399990966797)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-146.pth.tar', 74.14399998535156)

Train: 150 [   0/1251 (  0%)]  Loss: 3.641 (3.64)  Time: 2.507s,  408.43/s  (2.507s,  408.43/s)  LR: 5.050e-04  Data: 1.550 (1.550)
Train: 150 [  50/1251 (  4%)]  Loss: 3.597 (3.62)  Time: 0.997s, 1027.14/s  (1.038s,  986.23/s)  LR: 5.050e-04  Data: 0.011 (0.044)
Train: 150 [ 100/1251 (  8%)]  Loss: 3.726 (3.65)  Time: 1.005s, 1019.23/s  (1.020s, 1003.95/s)  LR: 5.050e-04  Data: 0.011 (0.028)
Train: 150 [ 150/1251 ( 12%)]  Loss: 3.523 (3.62)  Time: 1.001s, 1022.56/s  (1.013s, 1010.83/s)  LR: 5.050e-04  Data: 0.014 (0.022)
Train: 150 [ 200/1251 ( 16%)]  Loss: 3.518 (3.60)  Time: 0.995s, 1029.60/s  (1.010s, 1013.62/s)  LR: 5.050e-04  Data: 0.011 (0.020)
Train: 150 [ 250/1251 ( 20%)]  Loss: 3.801 (3.63)  Time: 1.001s, 1023.42/s  (1.009s, 1015.32/s)  LR: 5.050e-04  Data: 0.011 (0.018)
Train: 150 [ 300/1251 ( 24%)]  Loss: 3.816 (3.66)  Time: 1.032s,  992.21/s  (1.007s, 1016.55/s)  LR: 5.050e-04  Data: 0.011 (0.017)
Train: 150 [ 350/1251 ( 28%)]  Loss: 3.340 (3.62)  Time: 1.017s, 1006.83/s  (1.009s, 1015.36/s)  LR: 5.050e-04  Data: 0.010 (0.016)
Train: 150 [ 400/1251 ( 32%)]  Loss: 3.637 (3.62)  Time: 0.995s, 1029.56/s  (1.007s, 1016.75/s)  LR: 5.050e-04  Data: 0.011 (0.015)
Train: 150 [ 450/1251 ( 36%)]  Loss: 3.587 (3.62)  Time: 0.991s, 1033.29/s  (1.006s, 1017.55/s)  LR: 5.050e-04  Data: 0.011 (0.015)
Train: 150 [ 500/1251 ( 40%)]  Loss: 3.887 (3.64)  Time: 0.995s, 1029.50/s  (1.006s, 1018.22/s)  LR: 5.050e-04  Data: 0.011 (0.015)
Train: 150 [ 550/1251 ( 44%)]  Loss: 3.449 (3.63)  Time: 0.998s, 1025.87/s  (1.006s, 1018.28/s)  LR: 5.050e-04  Data: 0.010 (0.014)
Train: 150 [ 600/1251 ( 48%)]  Loss: 3.865 (3.65)  Time: 1.031s,  993.45/s  (1.006s, 1018.22/s)  LR: 5.050e-04  Data: 0.011 (0.014)
Train: 150 [ 650/1251 ( 52%)]  Loss: 3.706 (3.65)  Time: 1.050s,  975.22/s  (1.005s, 1018.53/s)  LR: 5.050e-04  Data: 0.011 (0.014)
Train: 150 [ 700/1251 ( 56%)]  Loss: 3.485 (3.64)  Time: 0.995s, 1028.85/s  (1.006s, 1018.00/s)  LR: 5.050e-04  Data: 0.012 (0.014)
Train: 150 [ 750/1251 ( 60%)]  Loss: 3.201 (3.61)  Time: 1.003s, 1020.88/s  (1.005s, 1018.42/s)  LR: 5.050e-04  Data: 0.011 (0.013)
Train: 150 [ 800/1251 ( 64%)]  Loss: 3.429 (3.60)  Time: 0.998s, 1025.75/s  (1.006s, 1017.45/s)  LR: 5.050e-04  Data: 0.012 (0.013)
Train: 150 [ 850/1251 ( 68%)]  Loss: 3.267 (3.58)  Time: 0.995s, 1028.72/s  (1.007s, 1016.97/s)  LR: 5.050e-04  Data: 0.011 (0.013)
Train: 150 [ 900/1251 ( 72%)]  Loss: 3.717 (3.59)  Time: 1.002s, 1022.09/s  (1.007s, 1016.95/s)  LR: 5.050e-04  Data: 0.015 (0.013)
Train: 150 [ 950/1251 ( 76%)]  Loss: 3.526 (3.59)  Time: 0.999s, 1025.31/s  (1.007s, 1017.18/s)  LR: 5.050e-04  Data: 0.012 (0.013)
Train: 150 [1000/1251 ( 80%)]  Loss: 3.668 (3.59)  Time: 0.998s, 1026.29/s  (1.007s, 1017.01/s)  LR: 5.050e-04  Data: 0.012 (0.013)
Train: 150 [1050/1251 ( 84%)]  Loss: 3.702 (3.59)  Time: 0.995s, 1029.63/s  (1.007s, 1017.22/s)  LR: 5.050e-04  Data: 0.011 (0.013)
Train: 150 [1100/1251 ( 88%)]  Loss: 3.632 (3.60)  Time: 1.076s,  951.45/s  (1.007s, 1017.03/s)  LR: 5.050e-04  Data: 0.011 (0.013)
Train: 150 [1150/1251 ( 92%)]  Loss: 3.457 (3.59)  Time: 0.995s, 1029.34/s  (1.007s, 1017.06/s)  LR: 5.050e-04  Data: 0.012 (0.013)
Train: 150 [1200/1251 ( 96%)]  Loss: 3.555 (3.59)  Time: 1.002s, 1022.24/s  (1.007s, 1017.34/s)  LR: 5.050e-04  Data: 0.012 (0.013)
Train: 150 [1250/1251 (100%)]  Loss: 3.571 (3.59)  Time: 0.983s, 1042.15/s  (1.006s, 1017.68/s)  LR: 5.050e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.619 (1.619)  Loss:  0.8066 (0.8066)  Acc@1: 89.0625 (89.0625)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.245 (0.577)  Loss:  0.8390 (1.3255)  Acc@1: 86.6745 (74.6320)  Acc@5: 97.1698 (92.5300)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-150.pth.tar', 74.63200002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-142.pth.tar', 74.52600001220704)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-144.pth.tar', 74.43800009521485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-143.pth.tar', 74.3080000390625)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-147.pth.tar', 74.30199998291016)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-139.pth.tar', 74.25400017333985)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-138.pth.tar', 74.24999990722657)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-145.pth.tar', 74.22400014160156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-148.pth.tar', 74.19599991455078)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-149.pth.tar', 74.18399990966797)

Train: 151 [   0/1251 (  0%)]  Loss: 3.889 (3.89)  Time: 2.432s,  421.03/s  (2.432s,  421.03/s)  LR: 4.998e-04  Data: 1.443 (1.443)
Train: 151 [  50/1251 (  4%)]  Loss: 3.324 (3.61)  Time: 0.998s, 1026.28/s  (1.048s,  977.37/s)  LR: 4.998e-04  Data: 0.011 (0.040)
Train: 151 [ 100/1251 (  8%)]  Loss: 3.474 (3.56)  Time: 0.994s, 1029.77/s  (1.025s,  998.55/s)  LR: 4.998e-04  Data: 0.011 (0.026)
Train: 151 [ 150/1251 ( 12%)]  Loss: 3.187 (3.47)  Time: 1.033s,  991.06/s  (1.022s, 1001.61/s)  LR: 4.998e-04  Data: 0.011 (0.021)
Train: 151 [ 200/1251 ( 16%)]  Loss: 3.760 (3.53)  Time: 0.998s, 1025.98/s  (1.017s, 1006.88/s)  LR: 4.998e-04  Data: 0.011 (0.018)
Train: 151 [ 250/1251 ( 20%)]  Loss: 3.717 (3.56)  Time: 0.995s, 1029.61/s  (1.014s, 1009.80/s)  LR: 4.998e-04  Data: 0.011 (0.017)
Train: 151 [ 300/1251 ( 24%)]  Loss: 3.758 (3.59)  Time: 0.995s, 1028.98/s  (1.012s, 1011.89/s)  LR: 4.998e-04  Data: 0.012 (0.016)
Train: 151 [ 350/1251 ( 28%)]  Loss: 3.800 (3.61)  Time: 0.996s, 1027.91/s  (1.013s, 1011.35/s)  LR: 4.998e-04  Data: 0.010 (0.015)
Train: 151 [ 400/1251 ( 32%)]  Loss: 3.505 (3.60)  Time: 1.032s,  992.19/s  (1.011s, 1012.59/s)  LR: 4.998e-04  Data: 0.010 (0.015)
Train: 151 [ 450/1251 ( 36%)]  Loss: 3.818 (3.62)  Time: 0.996s, 1028.03/s  (1.010s, 1013.52/s)  LR: 4.998e-04  Data: 0.011 (0.014)
Train: 151 [ 500/1251 ( 40%)]  Loss: 3.282 (3.59)  Time: 1.032s,  992.60/s  (1.010s, 1014.13/s)  LR: 4.998e-04  Data: 0.011 (0.014)
Train: 151 [ 550/1251 ( 44%)]  Loss: 3.693 (3.60)  Time: 1.049s,  975.71/s  (1.009s, 1014.66/s)  LR: 4.998e-04  Data: 0.011 (0.014)
Train: 151 [ 600/1251 ( 48%)]  Loss: 3.630 (3.60)  Time: 0.996s, 1027.90/s  (1.008s, 1015.77/s)  LR: 4.998e-04  Data: 0.011 (0.014)
Train: 151 [ 650/1251 ( 52%)]  Loss: 3.675 (3.61)  Time: 0.996s, 1027.89/s  (1.008s, 1016.31/s)  LR: 4.998e-04  Data: 0.010 (0.013)
Train: 151 [ 700/1251 ( 56%)]  Loss: 3.726 (3.62)  Time: 0.996s, 1028.52/s  (1.007s, 1017.08/s)  LR: 4.998e-04  Data: 0.011 (0.013)
Train: 151 [ 750/1251 ( 60%)]  Loss: 3.432 (3.60)  Time: 0.996s, 1028.03/s  (1.007s, 1017.36/s)  LR: 4.998e-04  Data: 0.011 (0.013)
Train: 151 [ 800/1251 ( 64%)]  Loss: 3.085 (3.57)  Time: 0.996s, 1027.73/s  (1.006s, 1017.93/s)  LR: 4.998e-04  Data: 0.011 (0.013)
Train: 151 [ 850/1251 ( 68%)]  Loss: 3.394 (3.56)  Time: 0.996s, 1027.88/s  (1.006s, 1017.74/s)  LR: 4.998e-04  Data: 0.011 (0.013)
Train: 151 [ 900/1251 ( 72%)]  Loss: 3.348 (3.55)  Time: 0.994s, 1030.55/s  (1.006s, 1018.19/s)  LR: 4.998e-04  Data: 0.010 (0.013)
Train: 151 [ 950/1251 ( 76%)]  Loss: 3.536 (3.55)  Time: 0.996s, 1027.86/s  (1.006s, 1018.07/s)  LR: 4.998e-04  Data: 0.011 (0.013)
Train: 151 [1000/1251 ( 80%)]  Loss: 3.690 (3.56)  Time: 0.996s, 1028.48/s  (1.006s, 1017.83/s)  LR: 4.998e-04  Data: 0.011 (0.013)
Train: 151 [1050/1251 ( 84%)]  Loss: 3.658 (3.56)  Time: 0.998s, 1026.11/s  (1.006s, 1018.11/s)  LR: 4.998e-04  Data: 0.011 (0.012)
Train: 151 [1100/1251 ( 88%)]  Loss: 3.186 (3.55)  Time: 0.996s, 1028.21/s  (1.006s, 1017.55/s)  LR: 4.998e-04  Data: 0.010 (0.012)
Train: 151 [1150/1251 ( 92%)]  Loss: 4.052 (3.57)  Time: 0.996s, 1027.99/s  (1.006s, 1017.47/s)  LR: 4.998e-04  Data: 0.010 (0.012)
Train: 151 [1200/1251 ( 96%)]  Loss: 3.313 (3.56)  Time: 1.052s,  972.96/s  (1.008s, 1016.19/s)  LR: 4.998e-04  Data: 0.010 (0.012)
Train: 151 [1250/1251 (100%)]  Loss: 3.765 (3.57)  Time: 0.983s, 1041.38/s  (1.008s, 1016.03/s)  LR: 4.998e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.648 (1.648)  Loss:  0.6315 (0.6315)  Acc@1: 88.1836 (88.1836)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.245 (0.566)  Loss:  0.7716 (1.1657)  Acc@1: 84.7877 (75.2080)  Acc@5: 96.4623 (92.6120)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-151.pth.tar', 75.20799998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-150.pth.tar', 74.63200002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-142.pth.tar', 74.52600001220704)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-144.pth.tar', 74.43800009521485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-143.pth.tar', 74.3080000390625)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-147.pth.tar', 74.30199998291016)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-139.pth.tar', 74.25400017333985)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-138.pth.tar', 74.24999990722657)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-145.pth.tar', 74.22400014160156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-148.pth.tar', 74.19599991455078)

Train: 152 [   0/1251 (  0%)]  Loss: 3.625 (3.63)  Time: 2.611s,  392.25/s  (2.611s,  392.25/s)  LR: 4.946e-04  Data: 1.644 (1.644)
Train: 152 [  50/1251 (  4%)]  Loss: 3.528 (3.58)  Time: 1.001s, 1022.78/s  (1.035s,  989.36/s)  LR: 4.946e-04  Data: 0.010 (0.043)
Train: 152 [ 100/1251 (  8%)]  Loss: 3.256 (3.47)  Time: 0.996s, 1028.61/s  (1.016s, 1007.78/s)  LR: 4.946e-04  Data: 0.013 (0.027)
Train: 152 [ 150/1251 ( 12%)]  Loss: 3.556 (3.49)  Time: 1.064s,  962.20/s  (1.027s,  997.43/s)  LR: 4.946e-04  Data: 0.011 (0.022)
Train: 152 [ 200/1251 ( 16%)]  Loss: 3.554 (3.50)  Time: 0.997s, 1027.12/s  (1.024s, 1000.28/s)  LR: 4.946e-04  Data: 0.011 (0.019)
Train: 152 [ 250/1251 ( 20%)]  Loss: 3.420 (3.49)  Time: 1.033s,  991.12/s  (1.021s, 1002.57/s)  LR: 4.946e-04  Data: 0.011 (0.018)
Train: 152 [ 300/1251 ( 24%)]  Loss: 3.497 (3.49)  Time: 0.995s, 1028.88/s  (1.018s, 1005.64/s)  LR: 4.946e-04  Data: 0.010 (0.016)
Train: 152 [ 350/1251 ( 28%)]  Loss: 3.904 (3.54)  Time: 0.996s, 1028.44/s  (1.015s, 1008.53/s)  LR: 4.946e-04  Data: 0.011 (0.016)
Train: 152 [ 400/1251 ( 32%)]  Loss: 3.694 (3.56)  Time: 0.998s, 1026.40/s  (1.013s, 1010.42/s)  LR: 4.946e-04  Data: 0.010 (0.015)
Train: 152 [ 450/1251 ( 36%)]  Loss: 3.145 (3.52)  Time: 0.996s, 1028.10/s  (1.014s, 1010.33/s)  LR: 4.946e-04  Data: 0.011 (0.015)
Train: 152 [ 500/1251 ( 40%)]  Loss: 3.651 (3.53)  Time: 0.997s, 1026.57/s  (1.012s, 1011.46/s)  LR: 4.946e-04  Data: 0.010 (0.014)
Train: 152 [ 550/1251 ( 44%)]  Loss: 3.873 (3.56)  Time: 0.996s, 1027.81/s  (1.012s, 1011.36/s)  LR: 4.946e-04  Data: 0.011 (0.014)
Train: 152 [ 600/1251 ( 48%)]  Loss: 3.789 (3.58)  Time: 0.996s, 1027.67/s  (1.012s, 1012.28/s)  LR: 4.946e-04  Data: 0.011 (0.014)
Train: 152 [ 650/1251 ( 52%)]  Loss: 3.781 (3.59)  Time: 1.056s,  970.14/s  (1.011s, 1012.80/s)  LR: 4.946e-04  Data: 0.011 (0.014)
Train: 152 [ 700/1251 ( 56%)]  Loss: 3.710 (3.60)  Time: 0.995s, 1029.19/s  (1.010s, 1013.75/s)  LR: 4.946e-04  Data: 0.011 (0.013)
Train: 152 [ 750/1251 ( 60%)]  Loss: 3.599 (3.60)  Time: 1.021s, 1003.10/s  (1.010s, 1014.21/s)  LR: 4.946e-04  Data: 0.011 (0.013)
Train: 152 [ 800/1251 ( 64%)]  Loss: 3.368 (3.59)  Time: 1.033s,  991.60/s  (1.010s, 1013.97/s)  LR: 4.946e-04  Data: 0.010 (0.013)
Train: 152 [ 850/1251 ( 68%)]  Loss: 3.614 (3.59)  Time: 1.002s, 1022.16/s  (1.010s, 1014.30/s)  LR: 4.946e-04  Data: 0.010 (0.013)
Train: 152 [ 900/1251 ( 72%)]  Loss: 3.569 (3.59)  Time: 1.002s, 1021.70/s  (1.009s, 1014.68/s)  LR: 4.946e-04  Data: 0.012 (0.013)
Train: 152 [ 950/1251 ( 76%)]  Loss: 3.568 (3.59)  Time: 0.998s, 1026.38/s  (1.009s, 1014.45/s)  LR: 4.946e-04  Data: 0.011 (0.013)
Train: 152 [1000/1251 ( 80%)]  Loss: 3.362 (3.57)  Time: 0.994s, 1029.71/s  (1.009s, 1014.88/s)  LR: 4.946e-04  Data: 0.011 (0.013)
Train: 152 [1050/1251 ( 84%)]  Loss: 3.338 (3.56)  Time: 0.997s, 1027.43/s  (1.009s, 1014.88/s)  LR: 4.946e-04  Data: 0.011 (0.013)
Train: 152 [1100/1251 ( 88%)]  Loss: 3.415 (3.56)  Time: 1.052s,  973.67/s  (1.009s, 1015.08/s)  LR: 4.946e-04  Data: 0.011 (0.013)
Train: 152 [1150/1251 ( 92%)]  Loss: 3.682 (3.56)  Time: 0.997s, 1026.72/s  (1.009s, 1015.13/s)  LR: 4.946e-04  Data: 0.011 (0.012)
Train: 152 [1200/1251 ( 96%)]  Loss: 3.648 (3.57)  Time: 0.997s, 1026.67/s  (1.008s, 1015.59/s)  LR: 4.946e-04  Data: 0.012 (0.012)
Train: 152 [1250/1251 (100%)]  Loss: 3.543 (3.56)  Time: 0.985s, 1039.74/s  (1.008s, 1015.88/s)  LR: 4.946e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.629 (1.629)  Loss:  0.7981 (0.7981)  Acc@1: 88.4766 (88.4766)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.245 (0.574)  Loss:  0.8571 (1.2835)  Acc@1: 84.3160 (74.3720)  Acc@5: 96.5802 (92.2700)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-151.pth.tar', 75.20799998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-150.pth.tar', 74.63200002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-142.pth.tar', 74.52600001220704)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-144.pth.tar', 74.43800009521485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-152.pth.tar', 74.37199990966796)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-143.pth.tar', 74.3080000390625)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-147.pth.tar', 74.30199998291016)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-139.pth.tar', 74.25400017333985)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-138.pth.tar', 74.24999990722657)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-145.pth.tar', 74.22400014160156)

Train: 153 [   0/1251 (  0%)]  Loss: 3.427 (3.43)  Time: 2.534s,  404.11/s  (2.534s,  404.11/s)  LR: 4.895e-04  Data: 1.579 (1.579)
Train: 153 [  50/1251 (  4%)]  Loss: 4.061 (3.74)  Time: 0.995s, 1029.38/s  (1.029s,  994.93/s)  LR: 4.895e-04  Data: 0.012 (0.042)
Train: 153 [ 100/1251 (  8%)]  Loss: 3.681 (3.72)  Time: 0.995s, 1028.93/s  (1.027s,  996.75/s)  LR: 4.895e-04  Data: 0.010 (0.027)
Train: 153 [ 150/1251 ( 12%)]  Loss: 4.090 (3.81)  Time: 0.996s, 1027.77/s  (1.018s, 1005.42/s)  LR: 4.895e-04  Data: 0.012 (0.022)
Train: 153 [ 200/1251 ( 16%)]  Loss: 3.453 (3.74)  Time: 0.996s, 1028.35/s  (1.013s, 1010.41/s)  LR: 4.895e-04  Data: 0.011 (0.019)
Train: 153 [ 250/1251 ( 20%)]  Loss: 3.333 (3.67)  Time: 0.996s, 1027.83/s  (1.012s, 1011.62/s)  LR: 4.895e-04  Data: 0.011 (0.017)
Train: 153 [ 300/1251 ( 24%)]  Loss: 3.407 (3.64)  Time: 0.997s, 1027.21/s  (1.010s, 1014.35/s)  LR: 4.895e-04  Data: 0.011 (0.016)
Train: 153 [ 350/1251 ( 28%)]  Loss: 3.309 (3.60)  Time: 0.998s, 1026.23/s  (1.008s, 1015.84/s)  LR: 4.895e-04  Data: 0.012 (0.016)
Train: 153 [ 400/1251 ( 32%)]  Loss: 3.358 (3.57)  Time: 0.999s, 1024.63/s  (1.008s, 1016.33/s)  LR: 4.895e-04  Data: 0.012 (0.015)
Train: 153 [ 450/1251 ( 36%)]  Loss: 3.587 (3.57)  Time: 0.995s, 1028.92/s  (1.007s, 1017.34/s)  LR: 4.895e-04  Data: 0.011 (0.015)
Train: 153 [ 500/1251 ( 40%)]  Loss: 3.585 (3.57)  Time: 1.040s,  984.48/s  (1.008s, 1016.13/s)  LR: 4.895e-04  Data: 0.011 (0.014)
Train: 153 [ 550/1251 ( 44%)]  Loss: 3.250 (3.54)  Time: 0.996s, 1027.98/s  (1.007s, 1016.41/s)  LR: 4.895e-04  Data: 0.011 (0.014)
Train: 153 [ 600/1251 ( 48%)]  Loss: 3.405 (3.53)  Time: 0.996s, 1028.08/s  (1.007s, 1016.93/s)  LR: 4.895e-04  Data: 0.012 (0.014)
Train: 153 [ 650/1251 ( 52%)]  Loss: 3.334 (3.52)  Time: 0.997s, 1027.19/s  (1.006s, 1017.79/s)  LR: 4.895e-04  Data: 0.012 (0.014)
Train: 153 [ 700/1251 ( 56%)]  Loss: 4.011 (3.55)  Time: 0.999s, 1025.31/s  (1.005s, 1018.54/s)  LR: 4.895e-04  Data: 0.012 (0.014)
Train: 153 [ 750/1251 ( 60%)]  Loss: 3.861 (3.57)  Time: 1.032s,  992.30/s  (1.005s, 1018.97/s)  LR: 4.895e-04  Data: 0.011 (0.013)
Train: 153 [ 800/1251 ( 64%)]  Loss: 3.363 (3.56)  Time: 0.993s, 1031.50/s  (1.005s, 1019.27/s)  LR: 4.895e-04  Data: 0.011 (0.013)
Train: 153 [ 850/1251 ( 68%)]  Loss: 3.625 (3.56)  Time: 0.999s, 1025.53/s  (1.004s, 1019.78/s)  LR: 4.895e-04  Data: 0.011 (0.013)
Train: 153 [ 900/1251 ( 72%)]  Loss: 3.596 (3.56)  Time: 0.994s, 1030.28/s  (1.004s, 1019.46/s)  LR: 4.895e-04  Data: 0.012 (0.013)
Train: 153 [ 950/1251 ( 76%)]  Loss: 3.588 (3.57)  Time: 1.013s, 1010.87/s  (1.004s, 1019.98/s)  LR: 4.895e-04  Data: 0.012 (0.013)
Train: 153 [1000/1251 ( 80%)]  Loss: 3.507 (3.56)  Time: 1.034s,  990.45/s  (1.004s, 1020.12/s)  LR: 4.895e-04  Data: 0.012 (0.013)
Train: 153 [1050/1251 ( 84%)]  Loss: 3.491 (3.56)  Time: 1.061s,  965.08/s  (1.005s, 1018.46/s)  LR: 4.895e-04  Data: 0.011 (0.013)
Train: 153 [1100/1251 ( 88%)]  Loss: 3.381 (3.55)  Time: 0.994s, 1029.85/s  (1.005s, 1018.62/s)  LR: 4.895e-04  Data: 0.012 (0.013)
Train: 153 [1150/1251 ( 92%)]  Loss: 3.832 (3.56)  Time: 1.008s, 1015.92/s  (1.005s, 1018.71/s)  LR: 4.895e-04  Data: 0.011 (0.013)
Train: 153 [1200/1251 ( 96%)]  Loss: 3.828 (3.57)  Time: 1.052s,  973.19/s  (1.005s, 1018.43/s)  LR: 4.895e-04  Data: 0.011 (0.013)
Train: 153 [1250/1251 (100%)]  Loss: 3.632 (3.58)  Time: 0.978s, 1046.68/s  (1.006s, 1018.33/s)  LR: 4.895e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.578 (1.578)  Loss:  0.9441 (0.9441)  Acc@1: 88.8672 (88.8672)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.245 (0.559)  Loss:  0.9936 (1.4175)  Acc@1: 83.3726 (74.2800)  Acc@5: 96.1085 (92.2480)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-151.pth.tar', 75.20799998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-150.pth.tar', 74.63200002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-142.pth.tar', 74.52600001220704)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-144.pth.tar', 74.43800009521485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-152.pth.tar', 74.37199990966796)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-143.pth.tar', 74.3080000390625)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-147.pth.tar', 74.30199998291016)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-153.pth.tar', 74.28000001708985)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-139.pth.tar', 74.25400017333985)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-138.pth.tar', 74.24999990722657)

Train: 154 [   0/1251 (  0%)]  Loss: 3.078 (3.08)  Time: 2.501s,  409.49/s  (2.501s,  409.49/s)  LR: 4.843e-04  Data: 1.544 (1.544)
Train: 154 [  50/1251 (  4%)]  Loss: 3.917 (3.50)  Time: 1.061s,  965.37/s  (1.045s,  979.63/s)  LR: 4.843e-04  Data: 0.011 (0.041)
Train: 154 [ 100/1251 (  8%)]  Loss: 3.641 (3.55)  Time: 1.003s, 1021.34/s  (1.028s,  996.35/s)  LR: 4.843e-04  Data: 0.011 (0.026)
Train: 154 [ 150/1251 ( 12%)]  Loss: 3.565 (3.55)  Time: 1.002s, 1022.20/s  (1.020s, 1003.92/s)  LR: 4.843e-04  Data: 0.011 (0.021)
Train: 154 [ 200/1251 ( 16%)]  Loss: 3.838 (3.61)  Time: 1.038s,  986.74/s  (1.017s, 1007.34/s)  LR: 4.843e-04  Data: 0.012 (0.019)
Train: 154 [ 250/1251 ( 20%)]  Loss: 3.853 (3.65)  Time: 1.035s,  989.23/s  (1.015s, 1009.08/s)  LR: 4.843e-04  Data: 0.012 (0.017)
Train: 154 [ 300/1251 ( 24%)]  Loss: 3.123 (3.57)  Time: 1.036s,  988.07/s  (1.014s, 1010.06/s)  LR: 4.843e-04  Data: 0.011 (0.016)
Train: 154 [ 350/1251 ( 28%)]  Loss: 3.500 (3.56)  Time: 0.997s, 1026.74/s  (1.012s, 1011.73/s)  LR: 4.843e-04  Data: 0.011 (0.016)
Train: 154 [ 400/1251 ( 32%)]  Loss: 3.691 (3.58)  Time: 0.996s, 1028.40/s  (1.011s, 1012.59/s)  LR: 4.843e-04  Data: 0.012 (0.015)
Train: 154 [ 450/1251 ( 36%)]  Loss: 2.853 (3.51)  Time: 1.035s,  989.61/s  (1.013s, 1011.30/s)  LR: 4.843e-04  Data: 0.011 (0.015)
Train: 154 [ 500/1251 ( 40%)]  Loss: 4.040 (3.55)  Time: 1.038s,  986.68/s  (1.013s, 1010.75/s)  LR: 4.843e-04  Data: 0.011 (0.014)
Train: 154 [ 550/1251 ( 44%)]  Loss: 3.116 (3.52)  Time: 1.009s, 1014.40/s  (1.012s, 1011.41/s)  LR: 4.843e-04  Data: 0.011 (0.014)
Train: 154 [ 600/1251 ( 48%)]  Loss: 3.832 (3.54)  Time: 0.994s, 1030.21/s  (1.012s, 1011.76/s)  LR: 4.843e-04  Data: 0.011 (0.014)
Train: 154 [ 650/1251 ( 52%)]  Loss: 3.604 (3.55)  Time: 0.996s, 1027.75/s  (1.012s, 1011.94/s)  LR: 4.843e-04  Data: 0.011 (0.014)
Train: 154 [ 700/1251 ( 56%)]  Loss: 3.649 (3.55)  Time: 0.999s, 1025.49/s  (1.011s, 1012.42/s)  LR: 4.843e-04  Data: 0.011 (0.014)
Train: 154 [ 750/1251 ( 60%)]  Loss: 3.685 (3.56)  Time: 1.000s, 1023.91/s  (1.011s, 1012.49/s)  LR: 4.843e-04  Data: 0.012 (0.013)
Train: 154 [ 800/1251 ( 64%)]  Loss: 3.234 (3.54)  Time: 0.996s, 1027.86/s  (1.011s, 1012.84/s)  LR: 4.843e-04  Data: 0.012 (0.013)
Train: 154 [ 850/1251 ( 68%)]  Loss: 3.784 (3.56)  Time: 0.997s, 1027.46/s  (1.010s, 1013.55/s)  LR: 4.843e-04  Data: 0.012 (0.013)
Train: 154 [ 900/1251 ( 72%)]  Loss: 3.980 (3.58)  Time: 0.993s, 1031.53/s  (1.010s, 1013.52/s)  LR: 4.843e-04  Data: 0.011 (0.013)
Train: 154 [ 950/1251 ( 76%)]  Loss: 3.564 (3.58)  Time: 1.005s, 1018.73/s  (1.010s, 1014.03/s)  LR: 4.843e-04  Data: 0.011 (0.013)
Train: 154 [1000/1251 ( 80%)]  Loss: 3.630 (3.58)  Time: 0.998s, 1025.70/s  (1.010s, 1014.01/s)  LR: 4.843e-04  Data: 0.012 (0.013)
Train: 154 [1050/1251 ( 84%)]  Loss: 3.393 (3.57)  Time: 0.996s, 1027.73/s  (1.010s, 1014.20/s)  LR: 4.843e-04  Data: 0.012 (0.013)
Train: 154 [1100/1251 ( 88%)]  Loss: 3.672 (3.58)  Time: 0.996s, 1028.35/s  (1.010s, 1014.15/s)  LR: 4.843e-04  Data: 0.011 (0.013)
Train: 154 [1150/1251 ( 92%)]  Loss: 3.655 (3.58)  Time: 1.032s,  992.43/s  (1.010s, 1014.16/s)  LR: 4.843e-04  Data: 0.011 (0.013)
Train: 154 [1200/1251 ( 96%)]  Loss: 3.840 (3.59)  Time: 0.997s, 1027.32/s  (1.011s, 1013.35/s)  LR: 4.843e-04  Data: 0.012 (0.013)
Train: 154 [1250/1251 (100%)]  Loss: 3.578 (3.59)  Time: 0.980s, 1044.85/s  (1.010s, 1013.58/s)  LR: 4.843e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.584 (1.584)  Loss:  0.6396 (0.6396)  Acc@1: 89.5508 (89.5508)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.245 (0.560)  Loss:  0.8518 (1.2826)  Acc@1: 83.3727 (74.6680)  Acc@5: 96.2264 (92.4680)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-151.pth.tar', 75.20799998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-154.pth.tar', 74.66800014648437)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-150.pth.tar', 74.63200002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-142.pth.tar', 74.52600001220704)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-144.pth.tar', 74.43800009521485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-152.pth.tar', 74.37199990966796)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-143.pth.tar', 74.3080000390625)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-147.pth.tar', 74.30199998291016)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-153.pth.tar', 74.28000001708985)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-139.pth.tar', 74.25400017333985)

Train: 155 [   0/1251 (  0%)]  Loss: 3.585 (3.58)  Time: 2.440s,  419.67/s  (2.440s,  419.67/s)  LR: 4.791e-04  Data: 1.477 (1.477)
Train: 155 [  50/1251 (  4%)]  Loss: 3.527 (3.56)  Time: 1.030s,  994.19/s  (1.056s,  970.00/s)  LR: 4.791e-04  Data: 0.011 (0.041)
Train: 155 [ 100/1251 (  8%)]  Loss: 3.692 (3.60)  Time: 1.040s,  984.71/s  (1.040s,  984.60/s)  LR: 4.791e-04  Data: 0.012 (0.026)
Train: 155 [ 150/1251 ( 12%)]  Loss: 3.485 (3.57)  Time: 0.997s, 1026.60/s  (1.029s,  995.43/s)  LR: 4.791e-04  Data: 0.012 (0.021)
Train: 155 [ 200/1251 ( 16%)]  Loss: 3.593 (3.58)  Time: 1.035s,  989.32/s  (1.026s,  998.35/s)  LR: 4.791e-04  Data: 0.011 (0.019)
Train: 155 [ 250/1251 ( 20%)]  Loss: 3.701 (3.60)  Time: 1.025s,  999.35/s  (1.023s, 1001.02/s)  LR: 4.791e-04  Data: 0.011 (0.017)
Train: 155 [ 300/1251 ( 24%)]  Loss: 3.202 (3.54)  Time: 0.996s, 1028.36/s  (1.021s, 1002.76/s)  LR: 4.791e-04  Data: 0.011 (0.016)
Train: 155 [ 350/1251 ( 28%)]  Loss: 3.924 (3.59)  Time: 1.033s,  991.02/s  (1.021s, 1002.60/s)  LR: 4.791e-04  Data: 0.011 (0.016)
Train: 155 [ 400/1251 ( 32%)]  Loss: 3.621 (3.59)  Time: 0.997s, 1027.36/s  (1.019s, 1005.03/s)  LR: 4.791e-04  Data: 0.011 (0.015)
Train: 155 [ 450/1251 ( 36%)]  Loss: 3.817 (3.61)  Time: 0.993s, 1031.30/s  (1.016s, 1007.71/s)  LR: 4.791e-04  Data: 0.011 (0.015)
Train: 155 [ 500/1251 ( 40%)]  Loss: 3.035 (3.56)  Time: 0.998s, 1025.70/s  (1.014s, 1009.88/s)  LR: 4.791e-04  Data: 0.011 (0.014)
Train: 155 [ 550/1251 ( 44%)]  Loss: 3.613 (3.57)  Time: 0.997s, 1027.27/s  (1.012s, 1011.50/s)  LR: 4.791e-04  Data: 0.011 (0.014)
Train: 155 [ 600/1251 ( 48%)]  Loss: 3.570 (3.57)  Time: 0.999s, 1025.31/s  (1.011s, 1012.86/s)  LR: 4.791e-04  Data: 0.011 (0.014)
Train: 155 [ 650/1251 ( 52%)]  Loss: 3.375 (3.55)  Time: 0.993s, 1031.42/s  (1.010s, 1014.02/s)  LR: 4.791e-04  Data: 0.011 (0.014)
Train: 155 [ 700/1251 ( 56%)]  Loss: 3.531 (3.55)  Time: 0.998s, 1026.10/s  (1.009s, 1015.05/s)  LR: 4.791e-04  Data: 0.011 (0.013)
Train: 155 [ 750/1251 ( 60%)]  Loss: 3.550 (3.55)  Time: 1.000s, 1024.15/s  (1.008s, 1015.73/s)  LR: 4.791e-04  Data: 0.012 (0.013)
Train: 155 [ 800/1251 ( 64%)]  Loss: 3.094 (3.52)  Time: 1.001s, 1022.92/s  (1.007s, 1016.47/s)  LR: 4.791e-04  Data: 0.015 (0.013)
Train: 155 [ 850/1251 ( 68%)]  Loss: 3.391 (3.52)  Time: 0.994s, 1030.33/s  (1.007s, 1017.20/s)  LR: 4.791e-04  Data: 0.012 (0.013)
Train: 155 [ 900/1251 ( 72%)]  Loss: 3.187 (3.50)  Time: 0.995s, 1028.68/s  (1.006s, 1017.84/s)  LR: 4.791e-04  Data: 0.012 (0.013)
Train: 155 [ 950/1251 ( 76%)]  Loss: 3.545 (3.50)  Time: 0.995s, 1029.54/s  (1.005s, 1018.47/s)  LR: 4.791e-04  Data: 0.011 (0.013)
Train: 155 [1000/1251 ( 80%)]  Loss: 3.567 (3.50)  Time: 0.991s, 1033.44/s  (1.005s, 1019.04/s)  LR: 4.791e-04  Data: 0.011 (0.013)
Train: 155 [1050/1251 ( 84%)]  Loss: 3.581 (3.51)  Time: 0.993s, 1030.79/s  (1.004s, 1019.58/s)  LR: 4.791e-04  Data: 0.011 (0.013)
Train: 155 [1100/1251 ( 88%)]  Loss: 3.809 (3.52)  Time: 0.993s, 1031.35/s  (1.004s, 1020.05/s)  LR: 4.791e-04  Data: 0.011 (0.013)
Train: 155 [1150/1251 ( 92%)]  Loss: 3.635 (3.53)  Time: 0.994s, 1029.82/s  (1.003s, 1020.51/s)  LR: 4.791e-04  Data: 0.011 (0.013)
Train: 155 [1200/1251 ( 96%)]  Loss: 3.534 (3.53)  Time: 0.992s, 1032.64/s  (1.003s, 1020.92/s)  LR: 4.791e-04  Data: 0.011 (0.013)
Train: 155 [1250/1251 (100%)]  Loss: 3.678 (3.53)  Time: 0.978s, 1046.78/s  (1.003s, 1021.28/s)  LR: 4.791e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.586 (1.586)  Loss:  0.7972 (0.7972)  Acc@1: 89.4531 (89.4531)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.245 (0.563)  Loss:  0.9107 (1.3128)  Acc@1: 84.7877 (74.7360)  Acc@5: 96.6981 (92.4860)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-151.pth.tar', 75.20799998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-155.pth.tar', 74.73599998535157)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-154.pth.tar', 74.66800014648437)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-150.pth.tar', 74.63200002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-142.pth.tar', 74.52600001220704)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-144.pth.tar', 74.43800009521485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-152.pth.tar', 74.37199990966796)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-143.pth.tar', 74.3080000390625)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-147.pth.tar', 74.30199998291016)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-153.pth.tar', 74.28000001708985)

Train: 156 [   0/1251 (  0%)]  Loss: 3.504 (3.50)  Time: 4.007s,  255.58/s  (4.007s,  255.58/s)  LR: 4.739e-04  Data: 2.820 (2.820)
Train: 156 [  50/1251 (  4%)]  Loss: 3.480 (3.49)  Time: 0.992s, 1032.20/s  (1.057s,  968.36/s)  LR: 4.739e-04  Data: 0.011 (0.066)
Train: 156 [ 100/1251 (  8%)]  Loss: 3.854 (3.61)  Time: 0.995s, 1028.66/s  (1.027s,  997.10/s)  LR: 4.739e-04  Data: 0.011 (0.039)
Train: 156 [ 150/1251 ( 12%)]  Loss: 3.366 (3.55)  Time: 0.992s, 1032.48/s  (1.017s, 1006.92/s)  LR: 4.739e-04  Data: 0.011 (0.030)
Train: 156 [ 200/1251 ( 16%)]  Loss: 3.659 (3.57)  Time: 0.996s, 1028.01/s  (1.012s, 1012.28/s)  LR: 4.739e-04  Data: 0.011 (0.025)
Train: 156 [ 250/1251 ( 20%)]  Loss: 3.318 (3.53)  Time: 0.995s, 1028.93/s  (1.008s, 1015.60/s)  LR: 4.739e-04  Data: 0.012 (0.022)
Train: 156 [ 300/1251 ( 24%)]  Loss: 3.106 (3.47)  Time: 1.000s, 1023.87/s  (1.006s, 1017.95/s)  LR: 4.739e-04  Data: 0.011 (0.020)
Train: 156 [ 350/1251 ( 28%)]  Loss: 3.681 (3.50)  Time: 0.993s, 1031.11/s  (1.004s, 1019.58/s)  LR: 4.739e-04  Data: 0.011 (0.019)
Train: 156 [ 400/1251 ( 32%)]  Loss: 3.669 (3.52)  Time: 1.000s, 1023.78/s  (1.003s, 1020.83/s)  LR: 4.739e-04  Data: 0.011 (0.018)
Train: 156 [ 450/1251 ( 36%)]  Loss: 3.475 (3.51)  Time: 0.994s, 1030.55/s  (1.002s, 1021.85/s)  LR: 4.739e-04  Data: 0.011 (0.017)
Train: 156 [ 500/1251 ( 40%)]  Loss: 3.521 (3.51)  Time: 0.991s, 1032.84/s  (1.001s, 1022.61/s)  LR: 4.739e-04  Data: 0.011 (0.017)
Train: 156 [ 550/1251 ( 44%)]  Loss: 3.313 (3.50)  Time: 0.998s, 1026.04/s  (1.001s, 1023.27/s)  LR: 4.739e-04  Data: 0.011 (0.016)
Train: 156 [ 600/1251 ( 48%)]  Loss: 3.710 (3.51)  Time: 0.998s, 1026.28/s  (1.000s, 1023.84/s)  LR: 4.739e-04  Data: 0.011 (0.016)
Train: 156 [ 650/1251 ( 52%)]  Loss: 3.767 (3.53)  Time: 0.997s, 1027.24/s  (1.000s, 1024.33/s)  LR: 4.739e-04  Data: 0.012 (0.015)
Train: 156 [ 700/1251 ( 56%)]  Loss: 3.321 (3.52)  Time: 0.992s, 1032.27/s  (0.999s, 1024.72/s)  LR: 4.739e-04  Data: 0.011 (0.015)
Train: 156 [ 750/1251 ( 60%)]  Loss: 3.504 (3.52)  Time: 0.995s, 1029.41/s  (0.999s, 1025.05/s)  LR: 4.739e-04  Data: 0.011 (0.015)
Train: 156 [ 800/1251 ( 64%)]  Loss: 3.239 (3.50)  Time: 0.998s, 1026.53/s  (0.999s, 1025.37/s)  LR: 4.739e-04  Data: 0.011 (0.015)
Train: 156 [ 850/1251 ( 68%)]  Loss: 3.616 (3.51)  Time: 0.993s, 1031.40/s  (0.998s, 1025.62/s)  LR: 4.739e-04  Data: 0.011 (0.014)
Train: 156 [ 900/1251 ( 72%)]  Loss: 3.390 (3.50)  Time: 0.995s, 1028.63/s  (0.998s, 1025.86/s)  LR: 4.739e-04  Data: 0.012 (0.014)
Train: 156 [ 950/1251 ( 76%)]  Loss: 3.445 (3.50)  Time: 0.992s, 1032.06/s  (0.998s, 1026.08/s)  LR: 4.739e-04  Data: 0.011 (0.014)
Train: 156 [1000/1251 ( 80%)]  Loss: 3.487 (3.50)  Time: 0.994s, 1029.70/s  (0.998s, 1026.28/s)  LR: 4.739e-04  Data: 0.011 (0.014)
Train: 156 [1050/1251 ( 84%)]  Loss: 3.525 (3.50)  Time: 0.992s, 1031.77/s  (0.998s, 1026.50/s)  LR: 4.739e-04  Data: 0.011 (0.014)
Train: 156 [1100/1251 ( 88%)]  Loss: 3.679 (3.51)  Time: 0.993s, 1030.91/s  (0.997s, 1026.68/s)  LR: 4.739e-04  Data: 0.011 (0.014)
Train: 156 [1150/1251 ( 92%)]  Loss: 3.407 (3.50)  Time: 0.995s, 1029.32/s  (0.997s, 1026.85/s)  LR: 4.739e-04  Data: 0.011 (0.014)
Train: 156 [1200/1251 ( 96%)]  Loss: 3.653 (3.51)  Time: 0.991s, 1033.00/s  (0.997s, 1026.89/s)  LR: 4.739e-04  Data: 0.011 (0.013)
Train: 156 [1250/1251 (100%)]  Loss: 3.278 (3.50)  Time: 0.983s, 1042.18/s  (0.997s, 1027.01/s)  LR: 4.739e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.600 (1.600)  Loss:  0.7679 (0.7679)  Acc@1: 89.5508 (89.5508)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.245 (0.557)  Loss:  0.8500 (1.2545)  Acc@1: 83.4906 (75.1800)  Acc@5: 95.9906 (92.8240)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-151.pth.tar', 75.20799998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-156.pth.tar', 75.18000006835938)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-155.pth.tar', 74.73599998535157)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-154.pth.tar', 74.66800014648437)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-150.pth.tar', 74.63200002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-142.pth.tar', 74.52600001220704)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-144.pth.tar', 74.43800009521485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-152.pth.tar', 74.37199990966796)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-143.pth.tar', 74.3080000390625)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-147.pth.tar', 74.30199998291016)

Train: 157 [   0/1251 (  0%)]  Loss: 3.755 (3.75)  Time: 2.572s,  398.08/s  (2.572s,  398.08/s)  LR: 4.687e-04  Data: 1.614 (1.614)
Train: 157 [  50/1251 (  4%)]  Loss: 3.547 (3.65)  Time: 0.994s, 1029.88/s  (1.028s,  996.52/s)  LR: 4.687e-04  Data: 0.011 (0.043)
Train: 157 [ 100/1251 (  8%)]  Loss: 3.685 (3.66)  Time: 0.994s, 1029.77/s  (1.012s, 1012.24/s)  LR: 4.687e-04  Data: 0.011 (0.027)
Train: 157 [ 150/1251 ( 12%)]  Loss: 3.876 (3.72)  Time: 0.999s, 1024.86/s  (1.006s, 1018.04/s)  LR: 4.687e-04  Data: 0.012 (0.022)
Train: 157 [ 200/1251 ( 16%)]  Loss: 3.186 (3.61)  Time: 0.988s, 1036.06/s  (1.003s, 1020.85/s)  LR: 4.687e-04  Data: 0.010 (0.019)
Train: 157 [ 250/1251 ( 20%)]  Loss: 3.300 (3.56)  Time: 0.991s, 1033.63/s  (1.002s, 1022.25/s)  LR: 4.687e-04  Data: 0.011 (0.018)
Train: 157 [ 300/1251 ( 24%)]  Loss: 3.062 (3.49)  Time: 0.994s, 1030.64/s  (1.001s, 1022.97/s)  LR: 4.687e-04  Data: 0.011 (0.016)
Train: 157 [ 350/1251 ( 28%)]  Loss: 3.402 (3.48)  Time: 0.991s, 1033.76/s  (1.000s, 1023.85/s)  LR: 4.687e-04  Data: 0.011 (0.016)
Train: 157 [ 400/1251 ( 32%)]  Loss: 3.540 (3.48)  Time: 0.990s, 1034.73/s  (0.999s, 1024.53/s)  LR: 4.687e-04  Data: 0.011 (0.015)
Train: 157 [ 450/1251 ( 36%)]  Loss: 3.612 (3.50)  Time: 0.992s, 1031.81/s  (0.999s, 1025.10/s)  LR: 4.687e-04  Data: 0.011 (0.015)
Train: 157 [ 500/1251 ( 40%)]  Loss: 3.663 (3.51)  Time: 0.992s, 1032.35/s  (0.998s, 1025.55/s)  LR: 4.687e-04  Data: 0.011 (0.014)
Train: 157 [ 550/1251 ( 44%)]  Loss: 3.846 (3.54)  Time: 0.999s, 1024.93/s  (0.998s, 1025.87/s)  LR: 4.687e-04  Data: 0.011 (0.014)
Train: 157 [ 600/1251 ( 48%)]  Loss: 3.226 (3.52)  Time: 0.994s, 1030.28/s  (0.998s, 1026.20/s)  LR: 4.687e-04  Data: 0.011 (0.014)
Train: 157 [ 650/1251 ( 52%)]  Loss: 3.821 (3.54)  Time: 1.002s, 1022.16/s  (0.998s, 1026.42/s)  LR: 4.687e-04  Data: 0.014 (0.014)
Train: 157 [ 700/1251 ( 56%)]  Loss: 3.420 (3.53)  Time: 1.006s, 1017.81/s  (0.998s, 1026.39/s)  LR: 4.687e-04  Data: 0.011 (0.013)
Train: 157 [ 750/1251 ( 60%)]  Loss: 3.780 (3.55)  Time: 0.993s, 1031.24/s  (0.998s, 1026.37/s)  LR: 4.687e-04  Data: 0.012 (0.013)
Train: 157 [ 800/1251 ( 64%)]  Loss: 3.679 (3.55)  Time: 0.994s, 1029.67/s  (0.998s, 1026.47/s)  LR: 4.687e-04  Data: 0.012 (0.013)
Train: 157 [ 850/1251 ( 68%)]  Loss: 3.247 (3.54)  Time: 0.996s, 1028.47/s  (0.997s, 1026.63/s)  LR: 4.687e-04  Data: 0.011 (0.013)
Train: 157 [ 900/1251 ( 72%)]  Loss: 3.666 (3.54)  Time: 0.994s, 1030.17/s  (0.998s, 1026.31/s)  LR: 4.687e-04  Data: 0.012 (0.013)
Train: 157 [ 950/1251 ( 76%)]  Loss: 3.323 (3.53)  Time: 0.995s, 1029.29/s  (0.998s, 1026.43/s)  LR: 4.687e-04  Data: 0.011 (0.013)
Train: 157 [1000/1251 ( 80%)]  Loss: 3.647 (3.54)  Time: 0.994s, 1029.70/s  (0.998s, 1026.53/s)  LR: 4.687e-04  Data: 0.011 (0.013)
Train: 157 [1050/1251 ( 84%)]  Loss: 3.629 (3.54)  Time: 0.993s, 1031.64/s  (0.997s, 1026.58/s)  LR: 4.687e-04  Data: 0.011 (0.013)
Train: 157 [1100/1251 ( 88%)]  Loss: 3.430 (3.54)  Time: 0.992s, 1032.67/s  (0.997s, 1026.66/s)  LR: 4.687e-04  Data: 0.011 (0.013)
Train: 157 [1150/1251 ( 92%)]  Loss: 3.411 (3.53)  Time: 0.993s, 1031.45/s  (0.997s, 1026.71/s)  LR: 4.687e-04  Data: 0.011 (0.013)
Train: 157 [1200/1251 ( 96%)]  Loss: 3.494 (3.53)  Time: 0.994s, 1030.44/s  (0.997s, 1026.76/s)  LR: 4.687e-04  Data: 0.011 (0.013)
Train: 157 [1250/1251 (100%)]  Loss: 3.553 (3.53)  Time: 0.984s, 1040.93/s  (0.997s, 1026.84/s)  LR: 4.687e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.593 (1.593)  Loss:  0.8147 (0.8147)  Acc@1: 89.3555 (89.3555)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.245 (0.561)  Loss:  0.8690 (1.3167)  Acc@1: 85.0236 (75.2340)  Acc@5: 96.2264 (92.6780)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-157.pth.tar', 75.23400008789062)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-151.pth.tar', 75.20799998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-156.pth.tar', 75.18000006835938)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-155.pth.tar', 74.73599998535157)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-154.pth.tar', 74.66800014648437)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-150.pth.tar', 74.63200002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-142.pth.tar', 74.52600001220704)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-144.pth.tar', 74.43800009521485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-152.pth.tar', 74.37199990966796)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-143.pth.tar', 74.3080000390625)

Train: 158 [   0/1251 (  0%)]  Loss: 3.667 (3.67)  Time: 2.563s,  399.49/s  (2.563s,  399.49/s)  LR: 4.636e-04  Data: 1.603 (1.603)
Train: 158 [  50/1251 (  4%)]  Loss: 3.921 (3.79)  Time: 0.998s, 1025.57/s  (1.027s,  997.29/s)  LR: 4.636e-04  Data: 0.012 (0.043)
Train: 158 [ 100/1251 (  8%)]  Loss: 3.338 (3.64)  Time: 0.996s, 1028.23/s  (1.011s, 1013.25/s)  LR: 4.636e-04  Data: 0.011 (0.027)
Train: 158 [ 150/1251 ( 12%)]  Loss: 3.566 (3.62)  Time: 0.997s, 1026.91/s  (1.005s, 1018.57/s)  LR: 4.636e-04  Data: 0.011 (0.022)
Train: 158 [ 200/1251 ( 16%)]  Loss: 3.574 (3.61)  Time: 0.993s, 1030.93/s  (1.003s, 1021.29/s)  LR: 4.636e-04  Data: 0.011 (0.019)
Train: 158 [ 250/1251 ( 20%)]  Loss: 3.427 (3.58)  Time: 0.993s, 1031.04/s  (1.001s, 1023.03/s)  LR: 4.636e-04  Data: 0.011 (0.018)
Train: 158 [ 300/1251 ( 24%)]  Loss: 3.646 (3.59)  Time: 0.996s, 1028.47/s  (1.000s, 1024.12/s)  LR: 4.636e-04  Data: 0.012 (0.016)
Train: 158 [ 350/1251 ( 28%)]  Loss: 3.596 (3.59)  Time: 0.994s, 1030.06/s  (0.999s, 1024.89/s)  LR: 4.636e-04  Data: 0.011 (0.016)
Train: 158 [ 400/1251 ( 32%)]  Loss: 3.666 (3.60)  Time: 0.992s, 1032.55/s  (0.998s, 1025.55/s)  LR: 4.636e-04  Data: 0.011 (0.015)
Train: 158 [ 450/1251 ( 36%)]  Loss: 3.303 (3.57)  Time: 0.996s, 1027.81/s  (0.998s, 1026.01/s)  LR: 4.636e-04  Data: 0.011 (0.015)
Train: 158 [ 500/1251 ( 40%)]  Loss: 3.709 (3.58)  Time: 0.993s, 1031.55/s  (0.998s, 1026.35/s)  LR: 4.636e-04  Data: 0.011 (0.014)
Train: 158 [ 550/1251 ( 44%)]  Loss: 3.620 (3.59)  Time: 0.992s, 1032.53/s  (0.998s, 1026.44/s)  LR: 4.636e-04  Data: 0.011 (0.014)
Train: 158 [ 600/1251 ( 48%)]  Loss: 3.654 (3.59)  Time: 0.998s, 1026.40/s  (0.997s, 1026.72/s)  LR: 4.636e-04  Data: 0.011 (0.014)
Train: 158 [ 650/1251 ( 52%)]  Loss: 3.425 (3.58)  Time: 0.995s, 1028.95/s  (0.997s, 1026.89/s)  LR: 4.636e-04  Data: 0.011 (0.014)
Train: 158 [ 700/1251 ( 56%)]  Loss: 3.713 (3.59)  Time: 0.992s, 1031.98/s  (0.997s, 1027.01/s)  LR: 4.636e-04  Data: 0.011 (0.013)
Train: 158 [ 750/1251 ( 60%)]  Loss: 3.218 (3.57)  Time: 0.993s, 1031.57/s  (0.997s, 1027.17/s)  LR: 4.636e-04  Data: 0.011 (0.013)
Train: 158 [ 800/1251 ( 64%)]  Loss: 3.072 (3.54)  Time: 0.994s, 1030.52/s  (0.997s, 1027.33/s)  LR: 4.636e-04  Data: 0.011 (0.013)
Train: 158 [ 850/1251 ( 68%)]  Loss: 3.426 (3.53)  Time: 1.005s, 1018.82/s  (0.997s, 1027.45/s)  LR: 4.636e-04  Data: 0.011 (0.013)
Train: 158 [ 900/1251 ( 72%)]  Loss: 3.683 (3.54)  Time: 0.994s, 1030.02/s  (0.996s, 1027.62/s)  LR: 4.636e-04  Data: 0.011 (0.013)
Train: 158 [ 950/1251 ( 76%)]  Loss: 3.476 (3.53)  Time: 0.993s, 1030.77/s  (0.996s, 1027.73/s)  LR: 4.636e-04  Data: 0.011 (0.013)
Train: 158 [1000/1251 ( 80%)]  Loss: 3.316 (3.52)  Time: 0.992s, 1032.22/s  (0.996s, 1027.83/s)  LR: 4.636e-04  Data: 0.011 (0.013)
Train: 158 [1050/1251 ( 84%)]  Loss: 3.584 (3.53)  Time: 0.994s, 1030.67/s  (0.996s, 1027.87/s)  LR: 4.636e-04  Data: 0.012 (0.013)
Train: 158 [1100/1251 ( 88%)]  Loss: 3.438 (3.52)  Time: 0.995s, 1029.60/s  (0.996s, 1027.96/s)  LR: 4.636e-04  Data: 0.011 (0.013)
Train: 158 [1150/1251 ( 92%)]  Loss: 3.664 (3.53)  Time: 1.036s,  988.66/s  (0.996s, 1027.94/s)  LR: 4.636e-04  Data: 0.012 (0.013)
Train: 158 [1200/1251 ( 96%)]  Loss: 3.447 (3.53)  Time: 0.993s, 1031.61/s  (0.996s, 1027.99/s)  LR: 4.636e-04  Data: 0.012 (0.013)
Train: 158 [1250/1251 (100%)]  Loss: 3.710 (3.53)  Time: 0.979s, 1045.76/s  (0.996s, 1028.09/s)  LR: 4.636e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.634 (1.634)  Loss:  0.7555 (0.7555)  Acc@1: 89.4531 (89.4531)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.245 (0.562)  Loss:  0.9368 (1.3462)  Acc@1: 84.4340 (74.6360)  Acc@5: 96.4623 (92.4700)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-157.pth.tar', 75.23400008789062)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-151.pth.tar', 75.20799998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-156.pth.tar', 75.18000006835938)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-155.pth.tar', 74.73599998535157)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-154.pth.tar', 74.66800014648437)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-158.pth.tar', 74.6359999609375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-150.pth.tar', 74.63200002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-142.pth.tar', 74.52600001220704)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-144.pth.tar', 74.43800009521485)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-152.pth.tar', 74.37199990966796)

Train: 159 [   0/1251 (  0%)]  Loss: 3.384 (3.38)  Time: 2.422s,  422.76/s  (2.422s,  422.76/s)  LR: 4.584e-04  Data: 1.472 (1.472)
Train: 159 [  50/1251 (  4%)]  Loss: 3.608 (3.50)  Time: 1.033s,  991.21/s  (1.043s,  981.89/s)  LR: 4.584e-04  Data: 0.011 (0.040)
Train: 159 [ 100/1251 (  8%)]  Loss: 3.679 (3.56)  Time: 0.995s, 1029.62/s  (1.030s,  994.60/s)  LR: 4.584e-04  Data: 0.011 (0.026)
Train: 159 [ 150/1251 ( 12%)]  Loss: 3.388 (3.52)  Time: 0.993s, 1030.85/s  (1.018s, 1005.62/s)  LR: 4.584e-04  Data: 0.011 (0.021)
Train: 159 [ 200/1251 ( 16%)]  Loss: 3.213 (3.45)  Time: 0.994s, 1029.69/s  (1.012s, 1011.51/s)  LR: 4.584e-04  Data: 0.011 (0.018)
Train: 159 [ 250/1251 ( 20%)]  Loss: 3.413 (3.45)  Time: 0.992s, 1032.07/s  (1.009s, 1015.08/s)  LR: 4.584e-04  Data: 0.011 (0.017)
Train: 159 [ 300/1251 ( 24%)]  Loss: 3.308 (3.43)  Time: 0.992s, 1032.47/s  (1.006s, 1017.59/s)  LR: 4.584e-04  Data: 0.011 (0.016)
Train: 159 [ 350/1251 ( 28%)]  Loss: 3.771 (3.47)  Time: 0.996s, 1027.84/s  (1.005s, 1019.29/s)  LR: 4.584e-04  Data: 0.012 (0.015)
Train: 159 [ 400/1251 ( 32%)]  Loss: 3.308 (3.45)  Time: 0.995s, 1028.75/s  (1.003s, 1020.56/s)  LR: 4.584e-04  Data: 0.011 (0.015)
Train: 159 [ 450/1251 ( 36%)]  Loss: 3.288 (3.44)  Time: 0.995s, 1029.45/s  (1.002s, 1021.54/s)  LR: 4.584e-04  Data: 0.011 (0.014)
Train: 159 [ 500/1251 ( 40%)]  Loss: 3.383 (3.43)  Time: 0.995s, 1028.93/s  (1.002s, 1022.40/s)  LR: 4.584e-04  Data: 0.011 (0.014)
Train: 159 [ 550/1251 ( 44%)]  Loss: 3.547 (3.44)  Time: 0.995s, 1029.46/s  (1.001s, 1023.07/s)  LR: 4.584e-04  Data: 0.011 (0.014)
Train: 159 [ 600/1251 ( 48%)]  Loss: 3.428 (3.44)  Time: 0.992s, 1031.88/s  (1.000s, 1023.60/s)  LR: 4.584e-04  Data: 0.011 (0.013)
Train: 159 [ 650/1251 ( 52%)]  Loss: 3.519 (3.45)  Time: 0.994s, 1030.11/s  (1.000s, 1024.00/s)  LR: 4.584e-04  Data: 0.011 (0.013)
Train: 159 [ 700/1251 ( 56%)]  Loss: 3.493 (3.45)  Time: 0.997s, 1027.37/s  (1.000s, 1024.42/s)  LR: 4.584e-04  Data: 0.012 (0.013)
Train: 159 [ 750/1251 ( 60%)]  Loss: 3.701 (3.46)  Time: 0.996s, 1027.83/s  (0.999s, 1024.75/s)  LR: 4.584e-04  Data: 0.011 (0.013)
Train: 159 [ 800/1251 ( 64%)]  Loss: 3.771 (3.48)  Time: 0.993s, 1031.38/s  (0.999s, 1025.06/s)  LR: 4.584e-04  Data: 0.011 (0.013)
Train: 159 [ 850/1251 ( 68%)]  Loss: 3.306 (3.47)  Time: 0.998s, 1026.56/s  (0.999s, 1025.33/s)  LR: 4.584e-04  Data: 0.011 (0.013)
Train: 159 [ 900/1251 ( 72%)]  Loss: 3.395 (3.47)  Time: 0.995s, 1028.94/s  (0.998s, 1025.60/s)  LR: 4.584e-04  Data: 0.012 (0.013)
Train: 159 [ 950/1251 ( 76%)]  Loss: 3.560 (3.47)  Time: 0.994s, 1030.12/s  (0.998s, 1025.79/s)  LR: 4.584e-04  Data: 0.011 (0.013)
Train: 159 [1000/1251 ( 80%)]  Loss: 3.256 (3.46)  Time: 0.996s, 1028.54/s  (0.998s, 1025.93/s)  LR: 4.584e-04  Data: 0.011 (0.013)
Train: 159 [1050/1251 ( 84%)]  Loss: 3.498 (3.46)  Time: 0.992s, 1032.17/s  (0.998s, 1026.09/s)  LR: 4.584e-04  Data: 0.011 (0.013)
Train: 159 [1100/1251 ( 88%)]  Loss: 3.429 (3.46)  Time: 1.001s, 1022.60/s  (0.998s, 1026.23/s)  LR: 4.584e-04  Data: 0.011 (0.012)
Train: 159 [1150/1251 ( 92%)]  Loss: 3.539 (3.47)  Time: 1.002s, 1021.83/s  (0.998s, 1026.37/s)  LR: 4.584e-04  Data: 0.012 (0.012)
Train: 159 [1200/1251 ( 96%)]  Loss: 3.378 (3.46)  Time: 0.994s, 1030.20/s  (0.998s, 1026.48/s)  LR: 4.584e-04  Data: 0.011 (0.012)
Train: 159 [1250/1251 (100%)]  Loss: 3.374 (3.46)  Time: 0.981s, 1044.09/s  (0.997s, 1026.60/s)  LR: 4.584e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.606 (1.606)  Loss:  0.7042 (0.7042)  Acc@1: 89.2578 (89.2578)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.245 (0.559)  Loss:  0.8295 (1.2433)  Acc@1: 84.1981 (75.1240)  Acc@5: 96.5802 (92.7780)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-157.pth.tar', 75.23400008789062)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-151.pth.tar', 75.20799998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-156.pth.tar', 75.18000006835938)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-159.pth.tar', 75.12399998779297)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-155.pth.tar', 74.73599998535157)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-154.pth.tar', 74.66800014648437)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-158.pth.tar', 74.6359999609375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-150.pth.tar', 74.63200002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-142.pth.tar', 74.52600001220704)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-144.pth.tar', 74.43800009521485)

Train: 160 [   0/1251 (  0%)]  Loss: 3.889 (3.89)  Time: 2.390s,  428.51/s  (2.390s,  428.51/s)  LR: 4.533e-04  Data: 1.429 (1.429)
Train: 160 [  50/1251 (  4%)]  Loss: 3.877 (3.88)  Time: 0.997s, 1027.27/s  (1.023s, 1001.06/s)  LR: 4.533e-04  Data: 0.011 (0.039)
Train: 160 [ 100/1251 (  8%)]  Loss: 3.375 (3.71)  Time: 0.996s, 1028.18/s  (1.009s, 1014.97/s)  LR: 4.533e-04  Data: 0.011 (0.025)
Train: 160 [ 150/1251 ( 12%)]  Loss: 3.562 (3.68)  Time: 0.997s, 1027.44/s  (1.005s, 1018.84/s)  LR: 4.533e-04  Data: 0.011 (0.021)
Train: 160 [ 200/1251 ( 16%)]  Loss: 3.741 (3.69)  Time: 0.991s, 1033.21/s  (1.002s, 1021.53/s)  LR: 4.533e-04  Data: 0.011 (0.018)
Train: 160 [ 250/1251 ( 20%)]  Loss: 3.466 (3.65)  Time: 0.997s, 1027.57/s  (1.001s, 1023.16/s)  LR: 4.533e-04  Data: 0.011 (0.017)
Train: 160 [ 300/1251 ( 24%)]  Loss: 3.505 (3.63)  Time: 0.993s, 1031.69/s  (1.000s, 1024.22/s)  LR: 4.533e-04  Data: 0.011 (0.016)
Train: 160 [ 350/1251 ( 28%)]  Loss: 3.844 (3.66)  Time: 0.997s, 1027.54/s  (0.999s, 1024.94/s)  LR: 4.533e-04  Data: 0.012 (0.015)
Train: 160 [ 400/1251 ( 32%)]  Loss: 3.172 (3.60)  Time: 0.995s, 1029.54/s  (0.999s, 1024.71/s)  LR: 4.533e-04  Data: 0.011 (0.015)
Train: 160 [ 450/1251 ( 36%)]  Loss: 3.884 (3.63)  Time: 0.996s, 1028.34/s  (0.999s, 1025.10/s)  LR: 4.533e-04  Data: 0.012 (0.014)
Train: 160 [ 500/1251 ( 40%)]  Loss: 3.428 (3.61)  Time: 0.996s, 1027.81/s  (0.999s, 1025.37/s)  LR: 4.533e-04  Data: 0.012 (0.014)
Train: 160 [ 550/1251 ( 44%)]  Loss: 3.617 (3.61)  Time: 0.995s, 1029.23/s  (0.998s, 1025.81/s)  LR: 4.533e-04  Data: 0.011 (0.014)
Train: 160 [ 600/1251 ( 48%)]  Loss: 3.452 (3.60)  Time: 1.000s, 1023.84/s  (0.998s, 1026.15/s)  LR: 4.533e-04  Data: 0.011 (0.014)
Train: 160 [ 650/1251 ( 52%)]  Loss: 3.310 (3.58)  Time: 0.997s, 1027.39/s  (0.998s, 1026.43/s)  LR: 4.533e-04  Data: 0.011 (0.013)
Train: 160 [ 700/1251 ( 56%)]  Loss: 3.601 (3.58)  Time: 0.992s, 1032.39/s  (0.997s, 1026.62/s)  LR: 4.533e-04  Data: 0.011 (0.013)
Train: 160 [ 750/1251 ( 60%)]  Loss: 3.875 (3.60)  Time: 0.994s, 1030.27/s  (0.997s, 1026.77/s)  LR: 4.533e-04  Data: 0.011 (0.013)
Train: 160 [ 800/1251 ( 64%)]  Loss: 3.442 (3.59)  Time: 0.990s, 1034.45/s  (0.997s, 1026.97/s)  LR: 4.533e-04  Data: 0.011 (0.013)
Train: 160 [ 850/1251 ( 68%)]  Loss: 3.415 (3.58)  Time: 0.994s, 1030.33/s  (0.997s, 1027.16/s)  LR: 4.533e-04  Data: 0.011 (0.013)
Train: 160 [ 900/1251 ( 72%)]  Loss: 3.530 (3.58)  Time: 0.995s, 1029.17/s  (0.997s, 1027.29/s)  LR: 4.533e-04  Data: 0.011 (0.013)
Train: 160 [ 950/1251 ( 76%)]  Loss: 3.599 (3.58)  Time: 1.001s, 1023.13/s  (0.997s, 1027.42/s)  LR: 4.533e-04  Data: 0.011 (0.013)
Train: 160 [1000/1251 ( 80%)]  Loss: 3.517 (3.58)  Time: 0.999s, 1024.99/s  (0.997s, 1027.53/s)  LR: 4.533e-04  Data: 0.011 (0.013)
Train: 160 [1050/1251 ( 84%)]  Loss: 3.596 (3.58)  Time: 1.000s, 1023.50/s  (0.996s, 1027.65/s)  LR: 4.533e-04  Data: 0.011 (0.012)
Train: 160 [1100/1251 ( 88%)]  Loss: 3.397 (3.57)  Time: 1.001s, 1022.56/s  (0.996s, 1027.70/s)  LR: 4.533e-04  Data: 0.012 (0.012)
Train: 160 [1150/1251 ( 92%)]  Loss: 3.757 (3.58)  Time: 1.000s, 1023.56/s  (0.996s, 1027.77/s)  LR: 4.533e-04  Data: 0.011 (0.012)
Train: 160 [1200/1251 ( 96%)]  Loss: 3.101 (3.56)  Time: 0.994s, 1029.99/s  (0.996s, 1027.82/s)  LR: 4.533e-04  Data: 0.011 (0.012)
Train: 160 [1250/1251 (100%)]  Loss: 3.071 (3.54)  Time: 0.982s, 1042.29/s  (0.996s, 1027.87/s)  LR: 4.533e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.622 (1.622)  Loss:  0.6582 (0.6582)  Acc@1: 89.5508 (89.5508)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.245 (0.558)  Loss:  0.7547 (1.2072)  Acc@1: 83.6085 (75.2540)  Acc@5: 96.9340 (92.8520)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-160.pth.tar', 75.25399999023438)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-157.pth.tar', 75.23400008789062)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-151.pth.tar', 75.20799998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-156.pth.tar', 75.18000006835938)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-159.pth.tar', 75.12399998779297)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-155.pth.tar', 74.73599998535157)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-154.pth.tar', 74.66800014648437)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-158.pth.tar', 74.6359999609375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-150.pth.tar', 74.63200002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-142.pth.tar', 74.52600001220704)

Train: 161 [   0/1251 (  0%)]  Loss: 3.762 (3.76)  Time: 2.442s,  419.29/s  (2.442s,  419.29/s)  LR: 4.481e-04  Data: 1.480 (1.480)
Train: 161 [  50/1251 (  4%)]  Loss: 3.833 (3.80)  Time: 0.993s, 1031.64/s  (1.029s,  995.36/s)  LR: 4.481e-04  Data: 0.011 (0.041)
Train: 161 [ 100/1251 (  8%)]  Loss: 3.680 (3.76)  Time: 0.998s, 1025.90/s  (1.022s, 1001.87/s)  LR: 4.481e-04  Data: 0.011 (0.026)
Train: 161 [ 150/1251 ( 12%)]  Loss: 3.504 (3.69)  Time: 0.994s, 1030.54/s  (1.013s, 1011.12/s)  LR: 4.481e-04  Data: 0.011 (0.021)
Train: 161 [ 200/1251 ( 16%)]  Loss: 3.243 (3.60)  Time: 0.992s, 1031.75/s  (1.008s, 1015.66/s)  LR: 4.481e-04  Data: 0.011 (0.019)
Train: 161 [ 250/1251 ( 20%)]  Loss: 3.374 (3.57)  Time: 0.994s, 1030.00/s  (1.005s, 1018.46/s)  LR: 4.481e-04  Data: 0.011 (0.017)
Train: 161 [ 300/1251 ( 24%)]  Loss: 3.603 (3.57)  Time: 0.995s, 1029.45/s  (1.004s, 1020.36/s)  LR: 4.481e-04  Data: 0.011 (0.016)
Train: 161 [ 350/1251 ( 28%)]  Loss: 3.428 (3.55)  Time: 0.994s, 1030.44/s  (1.002s, 1021.78/s)  LR: 4.481e-04  Data: 0.011 (0.015)
Train: 161 [ 400/1251 ( 32%)]  Loss: 3.744 (3.57)  Time: 0.992s, 1032.11/s  (1.001s, 1022.84/s)  LR: 4.481e-04  Data: 0.011 (0.015)
Train: 161 [ 450/1251 ( 36%)]  Loss: 3.576 (3.57)  Time: 0.996s, 1027.70/s  (1.000s, 1023.60/s)  LR: 4.481e-04  Data: 0.011 (0.015)
Train: 161 [ 500/1251 ( 40%)]  Loss: 3.812 (3.60)  Time: 0.998s, 1025.77/s  (1.000s, 1024.23/s)  LR: 4.481e-04  Data: 0.011 (0.014)
Train: 161 [ 550/1251 ( 44%)]  Loss: 3.962 (3.63)  Time: 0.994s, 1030.67/s  (0.999s, 1024.61/s)  LR: 4.481e-04  Data: 0.011 (0.014)
Train: 161 [ 600/1251 ( 48%)]  Loss: 3.384 (3.61)  Time: 0.993s, 1030.93/s  (0.999s, 1025.08/s)  LR: 4.481e-04  Data: 0.011 (0.014)
Train: 161 [ 650/1251 ( 52%)]  Loss: 3.767 (3.62)  Time: 0.994s, 1029.75/s  (0.999s, 1025.46/s)  LR: 4.481e-04  Data: 0.011 (0.014)
Train: 161 [ 700/1251 ( 56%)]  Loss: 3.578 (3.62)  Time: 0.992s, 1032.50/s  (0.998s, 1025.78/s)  LR: 4.481e-04  Data: 0.011 (0.013)
Train: 161 [ 750/1251 ( 60%)]  Loss: 3.224 (3.59)  Time: 0.994s, 1030.48/s  (0.998s, 1026.07/s)  LR: 4.481e-04  Data: 0.012 (0.013)
Train: 161 [ 800/1251 ( 64%)]  Loss: 3.474 (3.59)  Time: 0.992s, 1032.29/s  (0.998s, 1026.28/s)  LR: 4.481e-04  Data: 0.011 (0.013)
Train: 161 [ 850/1251 ( 68%)]  Loss: 3.344 (3.57)  Time: 0.994s, 1029.95/s  (0.998s, 1026.50/s)  LR: 4.481e-04  Data: 0.011 (0.013)
Train: 161 [ 900/1251 ( 72%)]  Loss: 3.317 (3.56)  Time: 0.997s, 1027.18/s  (0.997s, 1026.60/s)  LR: 4.481e-04  Data: 0.011 (0.013)
Train: 161 [ 950/1251 ( 76%)]  Loss: 3.246 (3.54)  Time: 0.996s, 1027.82/s  (0.997s, 1026.77/s)  LR: 4.481e-04  Data: 0.011 (0.013)
Train: 161 [1000/1251 ( 80%)]  Loss: 3.535 (3.54)  Time: 0.996s, 1027.60/s  (0.997s, 1026.91/s)  LR: 4.481e-04  Data: 0.011 (0.013)
Train: 161 [1050/1251 ( 84%)]  Loss: 3.641 (3.55)  Time: 0.994s, 1029.84/s  (0.997s, 1027.04/s)  LR: 4.481e-04  Data: 0.011 (0.013)
Train: 161 [1100/1251 ( 88%)]  Loss: 3.591 (3.55)  Time: 0.993s, 1030.89/s  (0.997s, 1027.19/s)  LR: 4.481e-04  Data: 0.011 (0.013)
Train: 161 [1150/1251 ( 92%)]  Loss: 3.897 (3.56)  Time: 0.993s, 1031.64/s  (0.997s, 1027.33/s)  LR: 4.481e-04  Data: 0.011 (0.012)
Train: 161 [1200/1251 ( 96%)]  Loss: 3.898 (3.58)  Time: 0.994s, 1030.05/s  (0.997s, 1027.45/s)  LR: 4.481e-04  Data: 0.011 (0.012)
Train: 161 [1250/1251 (100%)]  Loss: 4.019 (3.59)  Time: 1.025s,  998.71/s  (0.997s, 1027.47/s)  LR: 4.481e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.581 (1.581)  Loss:  0.7191 (0.7191)  Acc@1: 89.8438 (89.8438)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.245 (0.562)  Loss:  0.8480 (1.2799)  Acc@1: 82.9009 (74.7700)  Acc@5: 96.5802 (92.3560)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-160.pth.tar', 75.25399999023438)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-157.pth.tar', 75.23400008789062)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-151.pth.tar', 75.20799998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-156.pth.tar', 75.18000006835938)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-159.pth.tar', 75.12399998779297)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-161.pth.tar', 74.76999994140625)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-155.pth.tar', 74.73599998535157)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-154.pth.tar', 74.66800014648437)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-158.pth.tar', 74.6359999609375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-150.pth.tar', 74.63200002929688)

Train: 162 [   0/1251 (  0%)]  Loss: 3.930 (3.93)  Time: 2.569s,  398.53/s  (2.569s,  398.53/s)  LR: 4.430e-04  Data: 1.610 (1.610)
Train: 162 [  50/1251 (  4%)]  Loss: 3.608 (3.77)  Time: 0.994s, 1030.33/s  (1.030s,  994.48/s)  LR: 4.430e-04  Data: 0.011 (0.043)
Train: 162 [ 100/1251 (  8%)]  Loss: 3.032 (3.52)  Time: 0.994s, 1030.50/s  (1.013s, 1011.34/s)  LR: 4.430e-04  Data: 0.011 (0.027)
Train: 162 [ 150/1251 ( 12%)]  Loss: 3.061 (3.41)  Time: 0.993s, 1031.07/s  (1.007s, 1017.31/s)  LR: 4.430e-04  Data: 0.011 (0.022)
Train: 162 [ 200/1251 ( 16%)]  Loss: 3.259 (3.38)  Time: 0.993s, 1031.43/s  (1.004s, 1020.29/s)  LR: 4.430e-04  Data: 0.011 (0.019)
Train: 162 [ 250/1251 ( 20%)]  Loss: 3.437 (3.39)  Time: 0.990s, 1034.00/s  (1.002s, 1022.17/s)  LR: 4.430e-04  Data: 0.010 (0.018)
Train: 162 [ 300/1251 ( 24%)]  Loss: 3.557 (3.41)  Time: 0.995s, 1029.55/s  (1.001s, 1023.48/s)  LR: 4.430e-04  Data: 0.011 (0.017)
Train: 162 [ 350/1251 ( 28%)]  Loss: 3.405 (3.41)  Time: 0.993s, 1030.72/s  (1.000s, 1024.33/s)  LR: 4.430e-04  Data: 0.011 (0.016)
Train: 162 [ 400/1251 ( 32%)]  Loss: 3.498 (3.42)  Time: 0.996s, 1028.60/s  (0.999s, 1024.91/s)  LR: 4.430e-04  Data: 0.011 (0.015)
Train: 162 [ 450/1251 ( 36%)]  Loss: 3.720 (3.45)  Time: 0.995s, 1029.17/s  (0.998s, 1025.54/s)  LR: 4.430e-04  Data: 0.011 (0.015)
Train: 162 [ 500/1251 ( 40%)]  Loss: 3.708 (3.47)  Time: 0.993s, 1031.27/s  (0.998s, 1026.04/s)  LR: 4.430e-04  Data: 0.011 (0.014)
Train: 162 [ 550/1251 ( 44%)]  Loss: 3.882 (3.51)  Time: 0.992s, 1032.61/s  (0.998s, 1026.42/s)  LR: 4.430e-04  Data: 0.011 (0.014)
Train: 162 [ 600/1251 ( 48%)]  Loss: 3.745 (3.53)  Time: 0.992s, 1032.71/s  (0.997s, 1026.70/s)  LR: 4.430e-04  Data: 0.011 (0.014)
Train: 162 [ 650/1251 ( 52%)]  Loss: 3.681 (3.54)  Time: 0.995s, 1029.13/s  (0.997s, 1026.91/s)  LR: 4.430e-04  Data: 0.010 (0.014)
Train: 162 [ 700/1251 ( 56%)]  Loss: 3.576 (3.54)  Time: 1.034s,  990.72/s  (0.998s, 1026.24/s)  LR: 4.430e-04  Data: 0.012 (0.013)
Train: 162 [ 750/1251 ( 60%)]  Loss: 3.580 (3.54)  Time: 0.992s, 1032.63/s  (0.998s, 1026.49/s)  LR: 4.430e-04  Data: 0.011 (0.013)
Train: 162 [ 800/1251 ( 64%)]  Loss: 3.810 (3.56)  Time: 1.000s, 1024.30/s  (0.997s, 1026.66/s)  LR: 4.430e-04  Data: 0.011 (0.013)
Train: 162 [ 850/1251 ( 68%)]  Loss: 3.505 (3.56)  Time: 1.001s, 1022.88/s  (0.997s, 1026.83/s)  LR: 4.430e-04  Data: 0.011 (0.013)
Train: 162 [ 900/1251 ( 72%)]  Loss: 3.763 (3.57)  Time: 0.994s, 1030.42/s  (0.997s, 1026.72/s)  LR: 4.430e-04  Data: 0.012 (0.013)
Train: 162 [ 950/1251 ( 76%)]  Loss: 3.276 (3.55)  Time: 0.993s, 1031.25/s  (0.997s, 1026.79/s)  LR: 4.430e-04  Data: 0.011 (0.013)
Train: 162 [1000/1251 ( 80%)]  Loss: 3.229 (3.54)  Time: 0.996s, 1028.20/s  (0.997s, 1026.90/s)  LR: 4.430e-04  Data: 0.012 (0.013)
Train: 162 [1050/1251 ( 84%)]  Loss: 3.469 (3.53)  Time: 0.996s, 1027.90/s  (0.997s, 1027.02/s)  LR: 4.430e-04  Data: 0.011 (0.013)
Train: 162 [1100/1251 ( 88%)]  Loss: 3.408 (3.53)  Time: 0.992s, 1032.46/s  (0.997s, 1027.16/s)  LR: 4.430e-04  Data: 0.011 (0.013)
Train: 162 [1150/1251 ( 92%)]  Loss: 3.623 (3.53)  Time: 0.994s, 1030.48/s  (0.997s, 1027.27/s)  LR: 4.430e-04  Data: 0.011 (0.013)
Train: 162 [1200/1251 ( 96%)]  Loss: 3.322 (3.52)  Time: 0.993s, 1030.82/s  (0.997s, 1027.35/s)  LR: 4.430e-04  Data: 0.011 (0.013)
Train: 162 [1250/1251 (100%)]  Loss: 3.865 (3.54)  Time: 0.980s, 1044.94/s  (0.997s, 1027.44/s)  LR: 4.430e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.620 (1.620)  Loss:  0.7499 (0.7499)  Acc@1: 89.3555 (89.3555)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.245 (0.576)  Loss:  0.7806 (1.2464)  Acc@1: 84.7877 (75.2840)  Acc@5: 96.6981 (92.7020)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-162.pth.tar', 75.28399998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-160.pth.tar', 75.25399999023438)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-157.pth.tar', 75.23400008789062)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-151.pth.tar', 75.20799998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-156.pth.tar', 75.18000006835938)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-159.pth.tar', 75.12399998779297)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-161.pth.tar', 74.76999994140625)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-155.pth.tar', 74.73599998535157)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-154.pth.tar', 74.66800014648437)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-158.pth.tar', 74.6359999609375)

Train: 163 [   0/1251 (  0%)]  Loss: 3.358 (3.36)  Time: 2.387s,  429.07/s  (2.387s,  429.07/s)  LR: 4.378e-04  Data: 1.426 (1.426)
Train: 163 [  50/1251 (  4%)]  Loss: 3.401 (3.38)  Time: 0.994s, 1030.42/s  (1.029s,  995.54/s)  LR: 4.378e-04  Data: 0.011 (0.039)
Train: 163 [ 100/1251 (  8%)]  Loss: 3.029 (3.26)  Time: 0.994s, 1030.04/s  (1.012s, 1011.95/s)  LR: 4.378e-04  Data: 0.011 (0.025)
Train: 163 [ 150/1251 ( 12%)]  Loss: 3.274 (3.27)  Time: 0.993s, 1031.15/s  (1.007s, 1017.24/s)  LR: 4.378e-04  Data: 0.011 (0.020)
Train: 163 [ 200/1251 ( 16%)]  Loss: 3.550 (3.32)  Time: 0.997s, 1026.78/s  (1.004s, 1019.98/s)  LR: 4.378e-04  Data: 0.011 (0.018)
Train: 163 [ 250/1251 ( 20%)]  Loss: 3.080 (3.28)  Time: 0.996s, 1027.81/s  (1.003s, 1021.39/s)  LR: 4.378e-04  Data: 0.011 (0.017)
Train: 163 [ 300/1251 ( 24%)]  Loss: 3.359 (3.29)  Time: 0.994s, 1030.08/s  (1.001s, 1022.77/s)  LR: 4.378e-04  Data: 0.011 (0.016)
Train: 163 [ 350/1251 ( 28%)]  Loss: 3.618 (3.33)  Time: 0.994s, 1030.37/s  (1.000s, 1023.90/s)  LR: 4.378e-04  Data: 0.011 (0.015)
Train: 163 [ 400/1251 ( 32%)]  Loss: 3.453 (3.35)  Time: 0.992s, 1032.35/s  (0.999s, 1024.64/s)  LR: 4.378e-04  Data: 0.011 (0.015)
Train: 163 [ 450/1251 ( 36%)]  Loss: 3.430 (3.36)  Time: 0.995s, 1028.84/s  (0.999s, 1025.26/s)  LR: 4.378e-04  Data: 0.011 (0.014)
Train: 163 [ 500/1251 ( 40%)]  Loss: 3.219 (3.34)  Time: 0.993s, 1030.82/s  (0.998s, 1025.67/s)  LR: 4.378e-04  Data: 0.012 (0.014)
Train: 163 [ 550/1251 ( 44%)]  Loss: 3.797 (3.38)  Time: 1.001s, 1023.21/s  (0.998s, 1026.01/s)  LR: 4.378e-04  Data: 0.012 (0.014)
Train: 163 [ 600/1251 ( 48%)]  Loss: 3.528 (3.39)  Time: 1.034s,  990.31/s  (1.001s, 1022.89/s)  LR: 4.378e-04  Data: 0.012 (0.014)
Train: 163 [ 650/1251 ( 52%)]  Loss: 3.218 (3.38)  Time: 1.035s,  989.46/s  (1.004s, 1020.20/s)  LR: 4.378e-04  Data: 0.011 (0.013)
Train: 163 [ 700/1251 ( 56%)]  Loss: 3.420 (3.38)  Time: 1.036s,  988.51/s  (1.006s, 1017.93/s)  LR: 4.378e-04  Data: 0.012 (0.013)
Train: 163 [ 750/1251 ( 60%)]  Loss: 3.428 (3.39)  Time: 0.993s, 1031.65/s  (1.005s, 1018.72/s)  LR: 4.378e-04  Data: 0.011 (0.013)
Train: 163 [ 800/1251 ( 64%)]  Loss: 3.649 (3.40)  Time: 0.992s, 1032.69/s  (1.004s, 1019.45/s)  LR: 4.378e-04  Data: 0.011 (0.013)
Train: 163 [ 850/1251 ( 68%)]  Loss: 3.439 (3.40)  Time: 0.993s, 1030.96/s  (1.004s, 1020.04/s)  LR: 4.378e-04  Data: 0.011 (0.013)
Train: 163 [ 900/1251 ( 72%)]  Loss: 3.473 (3.41)  Time: 0.995s, 1028.68/s  (1.003s, 1020.60/s)  LR: 4.378e-04  Data: 0.012 (0.013)
Train: 163 [ 950/1251 ( 76%)]  Loss: 3.967 (3.43)  Time: 0.994s, 1030.50/s  (1.003s, 1021.07/s)  LR: 4.378e-04  Data: 0.011 (0.013)
Train: 163 [1000/1251 ( 80%)]  Loss: 3.665 (3.45)  Time: 1.002s, 1021.98/s  (1.002s, 1021.50/s)  LR: 4.378e-04  Data: 0.011 (0.013)
Train: 163 [1050/1251 ( 84%)]  Loss: 3.546 (3.45)  Time: 0.995s, 1029.14/s  (1.002s, 1021.91/s)  LR: 4.378e-04  Data: 0.011 (0.013)
Train: 163 [1100/1251 ( 88%)]  Loss: 3.499 (3.45)  Time: 0.994s, 1030.32/s  (1.002s, 1022.24/s)  LR: 4.378e-04  Data: 0.011 (0.012)
Train: 163 [1150/1251 ( 92%)]  Loss: 3.581 (3.46)  Time: 0.993s, 1031.71/s  (1.001s, 1022.56/s)  LR: 4.378e-04  Data: 0.011 (0.012)
Train: 163 [1200/1251 ( 96%)]  Loss: 3.687 (3.47)  Time: 0.993s, 1031.03/s  (1.001s, 1022.82/s)  LR: 4.378e-04  Data: 0.011 (0.012)
Train: 163 [1250/1251 (100%)]  Loss: 3.431 (3.47)  Time: 0.984s, 1041.12/s  (1.001s, 1023.08/s)  LR: 4.378e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.636 (1.636)  Loss:  0.7832 (0.7832)  Acc@1: 89.3555 (89.3555)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.245 (0.579)  Loss:  0.9389 (1.3137)  Acc@1: 84.5519 (75.3300)  Acc@5: 96.2264 (92.7580)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-163.pth.tar', 75.33000014160156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-162.pth.tar', 75.28399998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-160.pth.tar', 75.25399999023438)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-157.pth.tar', 75.23400008789062)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-151.pth.tar', 75.20799998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-156.pth.tar', 75.18000006835938)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-159.pth.tar', 75.12399998779297)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-161.pth.tar', 74.76999994140625)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-155.pth.tar', 74.73599998535157)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-154.pth.tar', 74.66800014648437)

Train: 164 [   0/1251 (  0%)]  Loss: 3.476 (3.48)  Time: 2.606s,  393.01/s  (2.606s,  393.01/s)  LR: 4.327e-04  Data: 1.640 (1.640)
Train: 164 [  50/1251 (  4%)]  Loss: 3.934 (3.70)  Time: 1.031s,  992.82/s  (1.036s,  988.54/s)  LR: 4.327e-04  Data: 0.011 (0.047)
Train: 164 [ 100/1251 (  8%)]  Loss: 3.245 (3.55)  Time: 0.996s, 1028.25/s  (1.019s, 1005.23/s)  LR: 4.327e-04  Data: 0.012 (0.029)
Train: 164 [ 150/1251 ( 12%)]  Loss: 3.734 (3.60)  Time: 0.995s, 1028.65/s  (1.011s, 1012.67/s)  LR: 4.327e-04  Data: 0.011 (0.023)
Train: 164 [ 200/1251 ( 16%)]  Loss: 3.765 (3.63)  Time: 0.994s, 1030.03/s  (1.007s, 1016.83/s)  LR: 4.327e-04  Data: 0.012 (0.020)
Train: 164 [ 250/1251 ( 20%)]  Loss: 3.189 (3.56)  Time: 0.994s, 1029.79/s  (1.005s, 1019.28/s)  LR: 4.327e-04  Data: 0.011 (0.019)
Train: 164 [ 300/1251 ( 24%)]  Loss: 3.207 (3.51)  Time: 0.993s, 1030.73/s  (1.003s, 1020.88/s)  LR: 4.327e-04  Data: 0.011 (0.017)
Train: 164 [ 350/1251 ( 28%)]  Loss: 3.639 (3.52)  Time: 0.992s, 1032.15/s  (1.002s, 1022.09/s)  LR: 4.327e-04  Data: 0.011 (0.016)
Train: 164 [ 400/1251 ( 32%)]  Loss: 3.360 (3.51)  Time: 0.994s, 1029.75/s  (1.001s, 1022.90/s)  LR: 4.327e-04  Data: 0.011 (0.016)
Train: 164 [ 450/1251 ( 36%)]  Loss: 3.356 (3.49)  Time: 0.998s, 1025.70/s  (1.000s, 1023.53/s)  LR: 4.327e-04  Data: 0.011 (0.015)
Train: 164 [ 500/1251 ( 40%)]  Loss: 3.439 (3.49)  Time: 0.995s, 1029.58/s  (1.000s, 1024.11/s)  LR: 4.327e-04  Data: 0.013 (0.015)
Train: 164 [ 550/1251 ( 44%)]  Loss: 3.962 (3.53)  Time: 0.994s, 1030.28/s  (0.999s, 1024.60/s)  LR: 4.327e-04  Data: 0.011 (0.015)
Train: 164 [ 600/1251 ( 48%)]  Loss: 3.810 (3.55)  Time: 0.990s, 1034.61/s  (0.999s, 1024.99/s)  LR: 4.327e-04  Data: 0.011 (0.014)
Train: 164 [ 650/1251 ( 52%)]  Loss: 3.643 (3.55)  Time: 1.002s, 1022.07/s  (0.999s, 1025.27/s)  LR: 4.327e-04  Data: 0.012 (0.014)
Train: 164 [ 700/1251 ( 56%)]  Loss: 3.661 (3.56)  Time: 0.993s, 1031.50/s  (0.998s, 1025.56/s)  LR: 4.327e-04  Data: 0.011 (0.014)
Train: 164 [ 750/1251 ( 60%)]  Loss: 3.352 (3.55)  Time: 0.990s, 1033.90/s  (0.998s, 1025.77/s)  LR: 4.327e-04  Data: 0.010 (0.014)
Train: 164 [ 800/1251 ( 64%)]  Loss: 3.458 (3.54)  Time: 0.996s, 1027.72/s  (0.998s, 1025.97/s)  LR: 4.327e-04  Data: 0.012 (0.014)
Train: 164 [ 850/1251 ( 68%)]  Loss: 3.433 (3.54)  Time: 0.993s, 1030.78/s  (0.998s, 1026.18/s)  LR: 4.327e-04  Data: 0.011 (0.013)
Train: 164 [ 900/1251 ( 72%)]  Loss: 3.422 (3.53)  Time: 0.992s, 1032.05/s  (0.998s, 1026.34/s)  LR: 4.327e-04  Data: 0.011 (0.013)
Train: 164 [ 950/1251 ( 76%)]  Loss: 3.207 (3.51)  Time: 0.995s, 1029.44/s  (0.998s, 1026.48/s)  LR: 4.327e-04  Data: 0.011 (0.013)
Train: 164 [1000/1251 ( 80%)]  Loss: 3.735 (3.53)  Time: 0.994s, 1030.50/s  (0.997s, 1026.58/s)  LR: 4.327e-04  Data: 0.011 (0.013)
Train: 164 [1050/1251 ( 84%)]  Loss: 3.604 (3.53)  Time: 0.991s, 1033.05/s  (0.997s, 1026.66/s)  LR: 4.327e-04  Data: 0.011 (0.013)
Train: 164 [1100/1251 ( 88%)]  Loss: 3.384 (3.52)  Time: 0.993s, 1031.66/s  (0.997s, 1026.77/s)  LR: 4.327e-04  Data: 0.011 (0.013)
Train: 164 [1150/1251 ( 92%)]  Loss: 3.632 (3.53)  Time: 0.996s, 1027.65/s  (0.997s, 1026.83/s)  LR: 4.327e-04  Data: 0.012 (0.013)
Train: 164 [1200/1251 ( 96%)]  Loss: 3.452 (3.52)  Time: 0.993s, 1031.33/s  (0.997s, 1026.76/s)  LR: 4.327e-04  Data: 0.011 (0.013)
Train: 164 [1250/1251 (100%)]  Loss: 3.480 (3.52)  Time: 0.984s, 1040.12/s  (0.997s, 1026.83/s)  LR: 4.327e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.708 (1.708)  Loss:  0.7328 (0.7328)  Acc@1: 89.4531 (89.4531)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.245 (0.564)  Loss:  0.8174 (1.2951)  Acc@1: 84.7877 (74.8300)  Acc@5: 96.8160 (92.6260)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-163.pth.tar', 75.33000014160156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-162.pth.tar', 75.28399998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-160.pth.tar', 75.25399999023438)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-157.pth.tar', 75.23400008789062)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-151.pth.tar', 75.20799998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-156.pth.tar', 75.18000006835938)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-159.pth.tar', 75.12399998779297)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-164.pth.tar', 74.8300001147461)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-161.pth.tar', 74.76999994140625)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-155.pth.tar', 74.73599998535157)

Train: 165 [   0/1251 (  0%)]  Loss: 4.010 (4.01)  Time: 4.285s,  239.00/s  (4.285s,  239.00/s)  LR: 4.276e-04  Data: 3.027 (3.027)
Train: 165 [  50/1251 (  4%)]  Loss: 3.630 (3.82)  Time: 0.995s, 1029.10/s  (1.061s,  965.58/s)  LR: 4.276e-04  Data: 0.011 (0.070)
Train: 165 [ 100/1251 (  8%)]  Loss: 3.666 (3.77)  Time: 0.995s, 1028.73/s  (1.028s,  996.17/s)  LR: 4.276e-04  Data: 0.011 (0.041)
Train: 165 [ 150/1251 ( 12%)]  Loss: 3.572 (3.72)  Time: 0.993s, 1030.81/s  (1.017s, 1007.02/s)  LR: 4.276e-04  Data: 0.011 (0.031)
Train: 165 [ 200/1251 ( 16%)]  Loss: 3.449 (3.67)  Time: 1.000s, 1024.37/s  (1.011s, 1012.48/s)  LR: 4.276e-04  Data: 0.011 (0.026)
Train: 165 [ 250/1251 ( 20%)]  Loss: 3.456 (3.63)  Time: 0.993s, 1031.43/s  (1.008s, 1015.85/s)  LR: 4.276e-04  Data: 0.011 (0.023)
Train: 165 [ 300/1251 ( 24%)]  Loss: 3.442 (3.60)  Time: 0.993s, 1031.73/s  (1.006s, 1018.23/s)  LR: 4.276e-04  Data: 0.011 (0.021)
Train: 165 [ 350/1251 ( 28%)]  Loss: 3.579 (3.60)  Time: 0.993s, 1031.44/s  (1.004s, 1019.89/s)  LR: 4.276e-04  Data: 0.011 (0.020)
Train: 165 [ 400/1251 ( 32%)]  Loss: 3.685 (3.61)  Time: 0.992s, 1032.59/s  (1.003s, 1021.03/s)  LR: 4.276e-04  Data: 0.011 (0.019)
Train: 165 [ 450/1251 ( 36%)]  Loss: 3.638 (3.61)  Time: 0.998s, 1026.55/s  (1.002s, 1021.90/s)  LR: 4.276e-04  Data: 0.011 (0.018)
Train: 165 [ 500/1251 ( 40%)]  Loss: 3.482 (3.60)  Time: 0.994s, 1030.16/s  (1.001s, 1022.55/s)  LR: 4.276e-04  Data: 0.011 (0.017)
Train: 165 [ 550/1251 ( 44%)]  Loss: 3.429 (3.59)  Time: 0.996s, 1027.97/s  (1.001s, 1022.98/s)  LR: 4.276e-04  Data: 0.011 (0.017)
Train: 165 [ 600/1251 ( 48%)]  Loss: 3.490 (3.58)  Time: 0.994s, 1029.67/s  (1.001s, 1023.39/s)  LR: 4.276e-04  Data: 0.011 (0.016)
Train: 165 [ 650/1251 ( 52%)]  Loss: 3.427 (3.57)  Time: 0.996s, 1028.13/s  (1.000s, 1023.70/s)  LR: 4.276e-04  Data: 0.012 (0.016)
Train: 165 [ 700/1251 ( 56%)]  Loss: 3.320 (3.55)  Time: 0.994s, 1029.73/s  (1.000s, 1024.01/s)  LR: 4.276e-04  Data: 0.011 (0.016)
Train: 165 [ 750/1251 ( 60%)]  Loss: 3.734 (3.56)  Time: 0.995s, 1029.44/s  (1.000s, 1024.25/s)  LR: 4.276e-04  Data: 0.011 (0.015)
Train: 165 [ 800/1251 ( 64%)]  Loss: 3.593 (3.56)  Time: 0.993s, 1031.41/s  (0.999s, 1024.52/s)  LR: 4.276e-04  Data: 0.011 (0.015)
Train: 165 [ 850/1251 ( 68%)]  Loss: 3.709 (3.57)  Time: 0.998s, 1025.95/s  (0.999s, 1024.84/s)  LR: 4.276e-04  Data: 0.012 (0.015)
Train: 165 [ 900/1251 ( 72%)]  Loss: 3.589 (3.57)  Time: 0.999s, 1024.84/s  (0.999s, 1025.07/s)  LR: 4.276e-04  Data: 0.012 (0.015)
Train: 165 [ 950/1251 ( 76%)]  Loss: 3.638 (3.58)  Time: 0.993s, 1031.50/s  (0.999s, 1025.29/s)  LR: 4.276e-04  Data: 0.011 (0.014)
Train: 165 [1000/1251 ( 80%)]  Loss: 3.299 (3.56)  Time: 0.995s, 1029.64/s  (0.999s, 1025.52/s)  LR: 4.276e-04  Data: 0.011 (0.014)
Train: 165 [1050/1251 ( 84%)]  Loss: 2.873 (3.53)  Time: 0.995s, 1029.55/s  (0.998s, 1025.67/s)  LR: 4.276e-04  Data: 0.015 (0.014)
Train: 165 [1100/1251 ( 88%)]  Loss: 3.616 (3.54)  Time: 0.996s, 1027.82/s  (0.998s, 1025.83/s)  LR: 4.276e-04  Data: 0.011 (0.014)
Train: 165 [1150/1251 ( 92%)]  Loss: 3.058 (3.52)  Time: 0.996s, 1028.07/s  (0.998s, 1025.97/s)  LR: 4.276e-04  Data: 0.015 (0.014)
Train: 165 [1200/1251 ( 96%)]  Loss: 3.807 (3.53)  Time: 0.999s, 1024.57/s  (0.998s, 1026.12/s)  LR: 4.276e-04  Data: 0.014 (0.014)
Train: 165 [1250/1251 (100%)]  Loss: 3.688 (3.53)  Time: 0.984s, 1040.21/s  (0.998s, 1026.28/s)  LR: 4.276e-04  Data: 0.000 (0.014)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.618 (1.618)  Loss:  0.7121 (0.7121)  Acc@1: 89.8438 (89.8438)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.245 (0.561)  Loss:  0.8935 (1.2614)  Acc@1: 83.8443 (75.4340)  Acc@5: 96.4623 (92.7780)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-165.pth.tar', 75.43400009277343)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-163.pth.tar', 75.33000014160156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-162.pth.tar', 75.28399998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-160.pth.tar', 75.25399999023438)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-157.pth.tar', 75.23400008789062)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-151.pth.tar', 75.20799998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-156.pth.tar', 75.18000006835938)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-159.pth.tar', 75.12399998779297)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-164.pth.tar', 74.8300001147461)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-161.pth.tar', 74.76999994140625)

Train: 166 [   0/1251 (  0%)]  Loss: 3.422 (3.42)  Time: 2.395s,  427.47/s  (2.395s,  427.47/s)  LR: 4.224e-04  Data: 1.446 (1.446)
Train: 166 [  50/1251 (  4%)]  Loss: 3.323 (3.37)  Time: 0.992s, 1031.76/s  (1.023s, 1000.57/s)  LR: 4.224e-04  Data: 0.011 (0.040)
Train: 166 [ 100/1251 (  8%)]  Loss: 3.511 (3.42)  Time: 0.994s, 1030.45/s  (1.015s, 1008.77/s)  LR: 4.224e-04  Data: 0.011 (0.026)
Train: 166 [ 150/1251 ( 12%)]  Loss: 3.704 (3.49)  Time: 0.999s, 1025.03/s  (1.009s, 1015.00/s)  LR: 4.224e-04  Data: 0.011 (0.021)
Train: 166 [ 200/1251 ( 16%)]  Loss: 3.713 (3.53)  Time: 0.998s, 1026.15/s  (1.006s, 1018.29/s)  LR: 4.224e-04  Data: 0.011 (0.018)
Train: 166 [ 250/1251 ( 20%)]  Loss: 3.619 (3.55)  Time: 0.997s, 1026.65/s  (1.004s, 1020.30/s)  LR: 4.224e-04  Data: 0.011 (0.017)
Train: 166 [ 300/1251 ( 24%)]  Loss: 3.307 (3.51)  Time: 1.000s, 1023.91/s  (1.002s, 1021.86/s)  LR: 4.224e-04  Data: 0.011 (0.016)
Train: 166 [ 350/1251 ( 28%)]  Loss: 3.332 (3.49)  Time: 0.996s, 1028.59/s  (1.001s, 1022.84/s)  LR: 4.224e-04  Data: 0.011 (0.015)
Train: 166 [ 400/1251 ( 32%)]  Loss: 3.153 (3.45)  Time: 0.994s, 1029.80/s  (1.000s, 1023.63/s)  LR: 4.224e-04  Data: 0.011 (0.015)
Train: 166 [ 450/1251 ( 36%)]  Loss: 3.022 (3.41)  Time: 0.993s, 1031.29/s  (1.000s, 1024.25/s)  LR: 4.224e-04  Data: 0.011 (0.014)
Train: 166 [ 500/1251 ( 40%)]  Loss: 3.603 (3.43)  Time: 0.997s, 1027.35/s  (0.999s, 1024.80/s)  LR: 4.224e-04  Data: 0.011 (0.014)
Train: 166 [ 550/1251 ( 44%)]  Loss: 3.364 (3.42)  Time: 1.011s, 1012.55/s  (0.999s, 1025.22/s)  LR: 4.224e-04  Data: 0.011 (0.014)
Train: 166 [ 600/1251 ( 48%)]  Loss: 3.380 (3.42)  Time: 0.993s, 1031.66/s  (0.998s, 1025.56/s)  LR: 4.224e-04  Data: 0.011 (0.014)
Train: 166 [ 650/1251 ( 52%)]  Loss: 3.520 (3.43)  Time: 0.995s, 1029.04/s  (0.998s, 1025.81/s)  LR: 4.224e-04  Data: 0.011 (0.013)
Train: 166 [ 700/1251 ( 56%)]  Loss: 3.693 (3.44)  Time: 0.993s, 1031.06/s  (0.998s, 1026.09/s)  LR: 4.224e-04  Data: 0.011 (0.013)
Train: 166 [ 750/1251 ( 60%)]  Loss: 3.275 (3.43)  Time: 0.992s, 1031.98/s  (0.998s, 1026.34/s)  LR: 4.224e-04  Data: 0.011 (0.013)
Train: 166 [ 800/1251 ( 64%)]  Loss: 3.616 (3.44)  Time: 0.996s, 1027.78/s  (0.998s, 1026.56/s)  LR: 4.224e-04  Data: 0.011 (0.013)
Train: 166 [ 850/1251 ( 68%)]  Loss: 3.444 (3.44)  Time: 0.992s, 1032.34/s  (0.997s, 1026.71/s)  LR: 4.224e-04  Data: 0.011 (0.013)
Train: 166 [ 900/1251 ( 72%)]  Loss: 3.327 (3.44)  Time: 1.001s, 1023.23/s  (0.997s, 1026.80/s)  LR: 4.224e-04  Data: 0.012 (0.013)
Train: 166 [ 950/1251 ( 76%)]  Loss: 3.636 (3.45)  Time: 0.992s, 1032.37/s  (0.997s, 1026.93/s)  LR: 4.224e-04  Data: 0.011 (0.013)
Train: 166 [1000/1251 ( 80%)]  Loss: 3.521 (3.45)  Time: 0.995s, 1029.24/s  (0.997s, 1027.10/s)  LR: 4.224e-04  Data: 0.011 (0.013)
Train: 166 [1050/1251 ( 84%)]  Loss: 3.405 (3.45)  Time: 0.992s, 1031.76/s  (0.997s, 1027.25/s)  LR: 4.224e-04  Data: 0.011 (0.012)
Train: 166 [1100/1251 ( 88%)]  Loss: 3.666 (3.46)  Time: 0.994s, 1030.23/s  (0.997s, 1027.38/s)  LR: 4.224e-04  Data: 0.011 (0.012)
Train: 166 [1150/1251 ( 92%)]  Loss: 3.538 (3.46)  Time: 0.994s, 1030.16/s  (0.997s, 1027.47/s)  LR: 4.224e-04  Data: 0.011 (0.012)
Train: 166 [1200/1251 ( 96%)]  Loss: 3.489 (3.46)  Time: 0.993s, 1031.22/s  (0.997s, 1027.59/s)  LR: 4.224e-04  Data: 0.011 (0.012)
Train: 166 [1250/1251 (100%)]  Loss: 3.518 (3.47)  Time: 0.979s, 1045.75/s  (0.996s, 1027.68/s)  LR: 4.224e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.686 (1.686)  Loss:  0.7454 (0.7454)  Acc@1: 89.5508 (89.5508)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.245 (0.561)  Loss:  0.8451 (1.2632)  Acc@1: 85.3774 (75.4980)  Acc@5: 96.6981 (92.9760)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-166.pth.tar', 75.49799998291016)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-165.pth.tar', 75.43400009277343)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-163.pth.tar', 75.33000014160156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-162.pth.tar', 75.28399998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-160.pth.tar', 75.25399999023438)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-157.pth.tar', 75.23400008789062)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-151.pth.tar', 75.20799998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-156.pth.tar', 75.18000006835938)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-159.pth.tar', 75.12399998779297)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-164.pth.tar', 74.8300001147461)

Train: 167 [   0/1251 (  0%)]  Loss: 3.613 (3.61)  Time: 2.727s,  375.50/s  (2.727s,  375.50/s)  LR: 4.173e-04  Data: 1.763 (1.763)
Train: 167 [  50/1251 (  4%)]  Loss: 3.387 (3.50)  Time: 1.003s, 1020.58/s  (1.029s,  995.31/s)  LR: 4.173e-04  Data: 0.011 (0.046)
Train: 167 [ 100/1251 (  8%)]  Loss: 3.425 (3.47)  Time: 0.994s, 1030.43/s  (1.012s, 1011.92/s)  LR: 4.173e-04  Data: 0.011 (0.029)
Train: 167 [ 150/1251 ( 12%)]  Loss: 3.722 (3.54)  Time: 0.993s, 1030.71/s  (1.006s, 1017.91/s)  LR: 4.173e-04  Data: 0.012 (0.023)
Train: 167 [ 200/1251 ( 16%)]  Loss: 3.373 (3.50)  Time: 0.991s, 1032.99/s  (1.003s, 1020.96/s)  LR: 4.173e-04  Data: 0.011 (0.020)
Train: 167 [ 250/1251 ( 20%)]  Loss: 3.392 (3.49)  Time: 0.995s, 1029.16/s  (1.001s, 1022.70/s)  LR: 4.173e-04  Data: 0.012 (0.018)
Train: 167 [ 300/1251 ( 24%)]  Loss: 3.455 (3.48)  Time: 0.998s, 1026.50/s  (1.000s, 1023.96/s)  LR: 4.173e-04  Data: 0.012 (0.017)
Train: 167 [ 350/1251 ( 28%)]  Loss: 3.241 (3.45)  Time: 0.993s, 1031.35/s  (0.999s, 1024.73/s)  LR: 4.173e-04  Data: 0.011 (0.016)
Train: 167 [ 400/1251 ( 32%)]  Loss: 3.296 (3.43)  Time: 0.993s, 1031.61/s  (0.999s, 1025.25/s)  LR: 4.173e-04  Data: 0.012 (0.016)
Train: 167 [ 450/1251 ( 36%)]  Loss: 3.303 (3.42)  Time: 0.995s, 1029.35/s  (0.998s, 1025.79/s)  LR: 4.173e-04  Data: 0.012 (0.015)
Train: 167 [ 500/1251 ( 40%)]  Loss: 3.478 (3.43)  Time: 0.993s, 1030.94/s  (0.998s, 1026.18/s)  LR: 4.173e-04  Data: 0.011 (0.015)
Train: 167 [ 550/1251 ( 44%)]  Loss: 3.329 (3.42)  Time: 0.995s, 1029.19/s  (0.998s, 1026.44/s)  LR: 4.173e-04  Data: 0.012 (0.014)
Train: 167 [ 600/1251 ( 48%)]  Loss: 3.506 (3.42)  Time: 0.996s, 1028.46/s  (0.997s, 1026.75/s)  LR: 4.173e-04  Data: 0.012 (0.014)
Train: 167 [ 650/1251 ( 52%)]  Loss: 3.665 (3.44)  Time: 0.993s, 1031.39/s  (0.997s, 1027.00/s)  LR: 4.173e-04  Data: 0.011 (0.014)
Train: 167 [ 700/1251 ( 56%)]  Loss: 3.606 (3.45)  Time: 0.994s, 1030.23/s  (0.997s, 1027.17/s)  LR: 4.173e-04  Data: 0.010 (0.014)
Train: 167 [ 750/1251 ( 60%)]  Loss: 3.698 (3.47)  Time: 0.992s, 1032.58/s  (0.997s, 1027.30/s)  LR: 4.173e-04  Data: 0.011 (0.014)
Train: 167 [ 800/1251 ( 64%)]  Loss: 3.500 (3.47)  Time: 0.996s, 1028.59/s  (0.997s, 1027.47/s)  LR: 4.173e-04  Data: 0.012 (0.013)
Train: 167 [ 850/1251 ( 68%)]  Loss: 3.746 (3.49)  Time: 0.993s, 1031.53/s  (0.996s, 1027.62/s)  LR: 4.173e-04  Data: 0.011 (0.013)
Train: 167 [ 900/1251 ( 72%)]  Loss: 3.479 (3.49)  Time: 0.995s, 1028.82/s  (0.996s, 1027.74/s)  LR: 4.173e-04  Data: 0.011 (0.013)
Train: 167 [ 950/1251 ( 76%)]  Loss: 3.505 (3.49)  Time: 0.997s, 1027.37/s  (0.996s, 1027.84/s)  LR: 4.173e-04  Data: 0.012 (0.013)
Train: 167 [1000/1251 ( 80%)]  Loss: 3.330 (3.48)  Time: 0.993s, 1031.65/s  (0.996s, 1027.96/s)  LR: 4.173e-04  Data: 0.011 (0.013)
Train: 167 [1050/1251 ( 84%)]  Loss: 3.632 (3.49)  Time: 0.993s, 1031.21/s  (0.996s, 1028.01/s)  LR: 4.173e-04  Data: 0.012 (0.013)
Train: 167 [1100/1251 ( 88%)]  Loss: 3.463 (3.48)  Time: 0.993s, 1031.11/s  (0.996s, 1028.09/s)  LR: 4.173e-04  Data: 0.012 (0.013)
Train: 167 [1150/1251 ( 92%)]  Loss: 3.208 (3.47)  Time: 0.996s, 1028.15/s  (0.996s, 1028.19/s)  LR: 4.173e-04  Data: 0.011 (0.013)
Train: 167 [1200/1251 ( 96%)]  Loss: 3.183 (3.46)  Time: 0.998s, 1025.60/s  (0.996s, 1028.18/s)  LR: 4.173e-04  Data: 0.011 (0.013)
Train: 167 [1250/1251 (100%)]  Loss: 3.521 (3.46)  Time: 1.022s, 1002.04/s  (0.996s, 1028.15/s)  LR: 4.173e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.617 (1.617)  Loss:  0.7449 (0.7449)  Acc@1: 89.1602 (89.1602)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.245 (0.563)  Loss:  0.8234 (1.3016)  Acc@1: 84.1981 (74.6880)  Acc@5: 96.6981 (92.5600)
Train: 168 [   0/1251 (  0%)]  Loss: 3.547 (3.55)  Time: 2.414s,  424.25/s  (2.414s,  424.25/s)  LR: 4.122e-04  Data: 1.452 (1.452)
Train: 168 [  50/1251 (  4%)]  Loss: 3.001 (3.27)  Time: 0.992s, 1032.45/s  (1.022s, 1001.66/s)  LR: 4.122e-04  Data: 0.011 (0.040)
Train: 168 [ 100/1251 (  8%)]  Loss: 3.245 (3.26)  Time: 0.993s, 1031.43/s  (1.008s, 1015.41/s)  LR: 4.122e-04  Data: 0.011 (0.026)
Train: 168 [ 150/1251 ( 12%)]  Loss: 4.052 (3.46)  Time: 0.995s, 1029.24/s  (1.004s, 1019.47/s)  LR: 4.122e-04  Data: 0.011 (0.021)
Train: 168 [ 200/1251 ( 16%)]  Loss: 3.535 (3.48)  Time: 0.999s, 1024.80/s  (1.002s, 1021.48/s)  LR: 4.122e-04  Data: 0.011 (0.018)
Train: 168 [ 250/1251 ( 20%)]  Loss: 3.290 (3.44)  Time: 0.996s, 1028.29/s  (1.001s, 1022.77/s)  LR: 4.122e-04  Data: 0.012 (0.017)
Train: 168 [ 300/1251 ( 24%)]  Loss: 3.387 (3.44)  Time: 0.998s, 1026.28/s  (1.000s, 1023.73/s)  LR: 4.122e-04  Data: 0.010 (0.016)
Train: 168 [ 350/1251 ( 28%)]  Loss: 3.713 (3.47)  Time: 0.999s, 1024.81/s  (1.000s, 1024.36/s)  LR: 4.122e-04  Data: 0.011 (0.015)
Train: 168 [ 400/1251 ( 32%)]  Loss: 3.080 (3.43)  Time: 1.001s, 1022.84/s  (0.999s, 1024.84/s)  LR: 4.122e-04  Data: 0.012 (0.015)
Train: 168 [ 450/1251 ( 36%)]  Loss: 3.623 (3.45)  Time: 1.000s, 1024.14/s  (0.999s, 1025.18/s)  LR: 4.122e-04  Data: 0.011 (0.014)
Train: 168 [ 500/1251 ( 40%)]  Loss: 3.763 (3.48)  Time: 0.994s, 1030.59/s  (0.999s, 1025.52/s)  LR: 4.122e-04  Data: 0.012 (0.014)
Train: 168 [ 550/1251 ( 44%)]  Loss: 3.374 (3.47)  Time: 0.993s, 1031.02/s  (0.998s, 1025.93/s)  LR: 4.122e-04  Data: 0.012 (0.014)
Train: 168 [ 600/1251 ( 48%)]  Loss: 3.550 (3.47)  Time: 0.992s, 1031.82/s  (0.998s, 1026.28/s)  LR: 4.122e-04  Data: 0.011 (0.014)
Train: 168 [ 650/1251 ( 52%)]  Loss: 3.309 (3.46)  Time: 0.998s, 1026.03/s  (0.998s, 1026.56/s)  LR: 4.122e-04  Data: 0.011 (0.013)
Train: 168 [ 700/1251 ( 56%)]  Loss: 3.478 (3.46)  Time: 0.996s, 1027.83/s  (0.997s, 1026.75/s)  LR: 4.122e-04  Data: 0.011 (0.013)
Train: 168 [ 750/1251 ( 60%)]  Loss: 3.394 (3.46)  Time: 0.998s, 1026.23/s  (0.997s, 1026.94/s)  LR: 4.122e-04  Data: 0.011 (0.013)
Train: 168 [ 800/1251 ( 64%)]  Loss: 3.692 (3.47)  Time: 0.995s, 1029.18/s  (0.997s, 1027.08/s)  LR: 4.122e-04  Data: 0.011 (0.013)
Train: 168 [ 850/1251 ( 68%)]  Loss: 3.826 (3.49)  Time: 0.996s, 1028.29/s  (0.997s, 1027.21/s)  LR: 4.122e-04  Data: 0.011 (0.013)
Train: 168 [ 900/1251 ( 72%)]  Loss: 3.401 (3.49)  Time: 0.994s, 1030.34/s  (0.997s, 1027.31/s)  LR: 4.122e-04  Data: 0.011 (0.013)
Train: 168 [ 950/1251 ( 76%)]  Loss: 3.169 (3.47)  Time: 0.994s, 1030.58/s  (0.997s, 1027.44/s)  LR: 4.122e-04  Data: 0.011 (0.013)
Train: 168 [1000/1251 ( 80%)]  Loss: 3.692 (3.48)  Time: 0.996s, 1027.92/s  (0.997s, 1027.56/s)  LR: 4.122e-04  Data: 0.011 (0.013)
Train: 168 [1050/1251 ( 84%)]  Loss: 3.218 (3.47)  Time: 0.997s, 1027.21/s  (0.997s, 1027.57/s)  LR: 4.122e-04  Data: 0.011 (0.013)
Train: 168 [1100/1251 ( 88%)]  Loss: 3.505 (3.47)  Time: 0.994s, 1030.65/s  (0.996s, 1027.67/s)  LR: 4.122e-04  Data: 0.011 (0.013)
Train: 168 [1150/1251 ( 92%)]  Loss: 4.140 (3.50)  Time: 1.001s, 1022.81/s  (0.996s, 1027.75/s)  LR: 4.122e-04  Data: 0.011 (0.012)
Train: 168 [1200/1251 ( 96%)]  Loss: 3.161 (3.49)  Time: 0.993s, 1030.80/s  (0.996s, 1027.81/s)  LR: 4.122e-04  Data: 0.011 (0.012)
Train: 168 [1250/1251 (100%)]  Loss: 3.774 (3.50)  Time: 0.981s, 1043.51/s  (0.996s, 1027.89/s)  LR: 4.122e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.578 (1.578)  Loss:  0.8920 (0.8920)  Acc@1: 89.4531 (89.4531)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.245 (0.564)  Loss:  0.8072 (1.3110)  Acc@1: 84.9057 (75.3800)  Acc@5: 95.7547 (92.8220)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-166.pth.tar', 75.49799998291016)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-165.pth.tar', 75.43400009277343)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-168.pth.tar', 75.3800000366211)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-163.pth.tar', 75.33000014160156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-162.pth.tar', 75.28399998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-160.pth.tar', 75.25399999023438)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-157.pth.tar', 75.23400008789062)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-151.pth.tar', 75.20799998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-156.pth.tar', 75.18000006835938)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-159.pth.tar', 75.12399998779297)

Train: 169 [   0/1251 (  0%)]  Loss: 3.292 (3.29)  Time: 2.435s,  420.47/s  (2.435s,  420.47/s)  LR: 4.072e-04  Data: 1.477 (1.477)
Train: 169 [  50/1251 (  4%)]  Loss: 3.409 (3.35)  Time: 0.999s, 1025.43/s  (1.047s,  978.45/s)  LR: 4.072e-04  Data: 0.011 (0.040)
Train: 169 [ 100/1251 (  8%)]  Loss: 3.116 (3.27)  Time: 0.992s, 1032.39/s  (1.021s, 1003.07/s)  LR: 4.072e-04  Data: 0.011 (0.026)
Train: 169 [ 150/1251 ( 12%)]  Loss: 3.506 (3.33)  Time: 0.995s, 1029.29/s  (1.012s, 1011.55/s)  LR: 4.072e-04  Data: 0.011 (0.021)
Train: 169 [ 200/1251 ( 16%)]  Loss: 3.434 (3.35)  Time: 0.992s, 1031.97/s  (1.008s, 1015.83/s)  LR: 4.072e-04  Data: 0.011 (0.019)
Train: 169 [ 250/1251 ( 20%)]  Loss: 3.082 (3.31)  Time: 0.999s, 1025.02/s  (1.005s, 1018.58/s)  LR: 4.072e-04  Data: 0.012 (0.017)
Train: 169 [ 300/1251 ( 24%)]  Loss: 3.665 (3.36)  Time: 0.992s, 1032.29/s  (1.004s, 1020.22/s)  LR: 4.072e-04  Data: 0.011 (0.016)
Train: 169 [ 350/1251 ( 28%)]  Loss: 3.239 (3.34)  Time: 0.995s, 1029.12/s  (1.002s, 1021.56/s)  LR: 4.072e-04  Data: 0.011 (0.015)
Train: 169 [ 400/1251 ( 32%)]  Loss: 3.574 (3.37)  Time: 0.995s, 1029.33/s  (1.001s, 1022.60/s)  LR: 4.072e-04  Data: 0.011 (0.015)
Train: 169 [ 450/1251 ( 36%)]  Loss: 3.152 (3.35)  Time: 0.992s, 1032.62/s  (1.001s, 1023.43/s)  LR: 4.072e-04  Data: 0.011 (0.015)
Train: 169 [ 500/1251 ( 40%)]  Loss: 3.282 (3.34)  Time: 0.993s, 1030.80/s  (1.000s, 1024.05/s)  LR: 4.072e-04  Data: 0.011 (0.014)
Train: 169 [ 550/1251 ( 44%)]  Loss: 3.728 (3.37)  Time: 0.993s, 1031.27/s  (1.000s, 1024.37/s)  LR: 4.072e-04  Data: 0.011 (0.014)
Train: 169 [ 600/1251 ( 48%)]  Loss: 3.704 (3.40)  Time: 0.993s, 1030.98/s  (0.999s, 1024.85/s)  LR: 4.072e-04  Data: 0.011 (0.014)
Train: 169 [ 650/1251 ( 52%)]  Loss: 3.208 (3.39)  Time: 0.994s, 1030.04/s  (0.999s, 1025.19/s)  LR: 4.072e-04  Data: 0.011 (0.013)
Train: 169 [ 700/1251 ( 56%)]  Loss: 3.540 (3.40)  Time: 0.994s, 1030.25/s  (0.999s, 1025.53/s)  LR: 4.072e-04  Data: 0.011 (0.013)
Train: 169 [ 750/1251 ( 60%)]  Loss: 3.318 (3.39)  Time: 0.995s, 1028.99/s  (0.998s, 1025.77/s)  LR: 4.072e-04  Data: 0.011 (0.013)
Train: 169 [ 800/1251 ( 64%)]  Loss: 3.589 (3.40)  Time: 0.995s, 1028.97/s  (0.998s, 1026.06/s)  LR: 4.072e-04  Data: 0.011 (0.013)
Train: 169 [ 850/1251 ( 68%)]  Loss: 3.677 (3.42)  Time: 1.063s,  963.27/s  (0.998s, 1025.98/s)  LR: 4.072e-04  Data: 0.012 (0.013)
Train: 169 [ 900/1251 ( 72%)]  Loss: 3.832 (3.44)  Time: 0.993s, 1030.94/s  (0.999s, 1025.10/s)  LR: 4.072e-04  Data: 0.011 (0.013)
Train: 169 [ 950/1251 ( 76%)]  Loss: 3.565 (3.45)  Time: 0.996s, 1027.83/s  (0.999s, 1025.35/s)  LR: 4.072e-04  Data: 0.012 (0.013)
Train: 169 [1000/1251 ( 80%)]  Loss: 3.582 (3.45)  Time: 0.995s, 1029.63/s  (0.998s, 1025.56/s)  LR: 4.072e-04  Data: 0.011 (0.013)
Train: 169 [1050/1251 ( 84%)]  Loss: 3.577 (3.46)  Time: 0.992s, 1031.80/s  (0.998s, 1025.72/s)  LR: 4.072e-04  Data: 0.010 (0.013)
Train: 169 [1100/1251 ( 88%)]  Loss: 3.649 (3.47)  Time: 0.993s, 1031.06/s  (0.998s, 1025.87/s)  LR: 4.072e-04  Data: 0.012 (0.013)
Train: 169 [1150/1251 ( 92%)]  Loss: 3.330 (3.46)  Time: 0.994s, 1030.44/s  (0.998s, 1026.00/s)  LR: 4.072e-04  Data: 0.011 (0.012)
Train: 169 [1200/1251 ( 96%)]  Loss: 3.496 (3.46)  Time: 0.994s, 1029.72/s  (0.998s, 1026.14/s)  LR: 4.072e-04  Data: 0.012 (0.012)
Train: 169 [1250/1251 (100%)]  Loss: 3.500 (3.46)  Time: 0.984s, 1040.86/s  (0.998s, 1026.29/s)  LR: 4.072e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.611 (1.611)  Loss:  0.8353 (0.8353)  Acc@1: 91.2109 (91.2109)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.245 (0.554)  Loss:  1.0091 (1.3759)  Acc@1: 83.9623 (75.2200)  Acc@5: 95.0472 (92.8020)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-166.pth.tar', 75.49799998291016)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-165.pth.tar', 75.43400009277343)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-168.pth.tar', 75.3800000366211)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-163.pth.tar', 75.33000014160156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-162.pth.tar', 75.28399998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-160.pth.tar', 75.25399999023438)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-157.pth.tar', 75.23400008789062)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-169.pth.tar', 75.22000001464843)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-151.pth.tar', 75.20799998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-156.pth.tar', 75.18000006835938)

Train: 170 [   0/1251 (  0%)]  Loss: 3.470 (3.47)  Time: 2.331s,  439.27/s  (2.331s,  439.27/s)  LR: 4.021e-04  Data: 1.382 (1.382)
Train: 170 [  50/1251 (  4%)]  Loss: 3.690 (3.58)  Time: 0.993s, 1031.34/s  (1.021s, 1002.94/s)  LR: 4.021e-04  Data: 0.011 (0.038)
Train: 170 [ 100/1251 (  8%)]  Loss: 3.359 (3.51)  Time: 0.993s, 1031.19/s  (1.008s, 1015.92/s)  LR: 4.021e-04  Data: 0.011 (0.025)
Train: 170 [ 150/1251 ( 12%)]  Loss: 3.416 (3.48)  Time: 0.993s, 1031.26/s  (1.004s, 1020.28/s)  LR: 4.021e-04  Data: 0.011 (0.020)
Train: 170 [ 200/1251 ( 16%)]  Loss: 3.449 (3.48)  Time: 0.995s, 1029.62/s  (1.001s, 1022.70/s)  LR: 4.021e-04  Data: 0.012 (0.018)
Train: 170 [ 250/1251 ( 20%)]  Loss: 3.282 (3.44)  Time: 0.995s, 1029.05/s  (1.000s, 1024.22/s)  LR: 4.021e-04  Data: 0.011 (0.017)
Train: 170 [ 300/1251 ( 24%)]  Loss: 3.506 (3.45)  Time: 0.992s, 1032.20/s  (0.999s, 1025.21/s)  LR: 4.021e-04  Data: 0.011 (0.016)
Train: 170 [ 350/1251 ( 28%)]  Loss: 3.755 (3.49)  Time: 0.993s, 1031.40/s  (0.998s, 1025.85/s)  LR: 4.021e-04  Data: 0.011 (0.015)
Train: 170 [ 400/1251 ( 32%)]  Loss: 3.475 (3.49)  Time: 0.998s, 1026.08/s  (0.998s, 1026.35/s)  LR: 4.021e-04  Data: 0.011 (0.015)
Train: 170 [ 450/1251 ( 36%)]  Loss: 3.761 (3.52)  Time: 0.993s, 1031.50/s  (0.997s, 1026.75/s)  LR: 4.021e-04  Data: 0.011 (0.014)
Train: 170 [ 500/1251 ( 40%)]  Loss: 3.364 (3.50)  Time: 0.993s, 1031.22/s  (0.997s, 1027.11/s)  LR: 4.021e-04  Data: 0.011 (0.014)
Train: 170 [ 550/1251 ( 44%)]  Loss: 3.155 (3.47)  Time: 0.992s, 1032.00/s  (0.997s, 1027.36/s)  LR: 4.021e-04  Data: 0.011 (0.014)
Train: 170 [ 600/1251 ( 48%)]  Loss: 3.851 (3.50)  Time: 0.999s, 1025.27/s  (0.997s, 1027.52/s)  LR: 4.021e-04  Data: 0.011 (0.014)
Train: 170 [ 650/1251 ( 52%)]  Loss: 3.270 (3.49)  Time: 0.992s, 1031.92/s  (0.996s, 1027.74/s)  LR: 4.021e-04  Data: 0.011 (0.013)
Train: 170 [ 700/1251 ( 56%)]  Loss: 3.427 (3.48)  Time: 0.993s, 1030.75/s  (0.996s, 1027.91/s)  LR: 4.021e-04  Data: 0.011 (0.013)
Train: 170 [ 750/1251 ( 60%)]  Loss: 3.255 (3.47)  Time: 0.998s, 1025.69/s  (1.000s, 1024.34/s)  LR: 4.021e-04  Data: 0.010 (0.013)
Train: 170 [ 800/1251 ( 64%)]  Loss: 3.422 (3.47)  Time: 0.996s, 1028.24/s  (0.999s, 1024.66/s)  LR: 4.021e-04  Data: 0.012 (0.013)
Train: 170 [ 850/1251 ( 68%)]  Loss: 3.543 (3.47)  Time: 0.992s, 1032.47/s  (0.999s, 1024.96/s)  LR: 4.021e-04  Data: 0.010 (0.013)
Train: 170 [ 900/1251 ( 72%)]  Loss: 3.792 (3.49)  Time: 0.989s, 1035.21/s  (0.999s, 1025.19/s)  LR: 4.021e-04  Data: 0.011 (0.013)
Train: 170 [ 950/1251 ( 76%)]  Loss: 3.245 (3.47)  Time: 0.995s, 1028.90/s  (0.999s, 1025.43/s)  LR: 4.021e-04  Data: 0.012 (0.013)
Train: 170 [1000/1251 ( 80%)]  Loss: 3.287 (3.47)  Time: 0.989s, 1035.84/s  (0.998s, 1025.63/s)  LR: 4.021e-04  Data: 0.010 (0.013)
Train: 170 [1050/1251 ( 84%)]  Loss: 3.388 (3.46)  Time: 0.995s, 1029.61/s  (0.998s, 1025.79/s)  LR: 4.021e-04  Data: 0.011 (0.013)
Train: 170 [1100/1251 ( 88%)]  Loss: 3.470 (3.46)  Time: 0.995s, 1028.98/s  (0.998s, 1026.00/s)  LR: 4.021e-04  Data: 0.011 (0.012)
Train: 170 [1150/1251 ( 92%)]  Loss: 3.289 (3.45)  Time: 0.996s, 1028.20/s  (0.998s, 1026.16/s)  LR: 4.021e-04  Data: 0.011 (0.012)
Train: 170 [1200/1251 ( 96%)]  Loss: 3.451 (3.45)  Time: 0.993s, 1031.49/s  (0.998s, 1026.32/s)  LR: 4.021e-04  Data: 0.011 (0.012)
Train: 170 [1250/1251 (100%)]  Loss: 3.444 (3.45)  Time: 0.985s, 1039.49/s  (0.998s, 1026.47/s)  LR: 4.021e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.639 (1.639)  Loss:  0.7874 (0.7874)  Acc@1: 89.7461 (89.7461)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.245 (0.560)  Loss:  0.9128 (1.3305)  Acc@1: 85.7311 (75.4320)  Acc@5: 96.8160 (92.9740)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-166.pth.tar', 75.49799998291016)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-165.pth.tar', 75.43400009277343)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-170.pth.tar', 75.43200000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-168.pth.tar', 75.3800000366211)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-163.pth.tar', 75.33000014160156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-162.pth.tar', 75.28399998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-160.pth.tar', 75.25399999023438)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-157.pth.tar', 75.23400008789062)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-169.pth.tar', 75.22000001464843)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-151.pth.tar', 75.20799998535156)

Train: 171 [   0/1251 (  0%)]  Loss: 3.200 (3.20)  Time: 2.527s,  405.28/s  (2.527s,  405.28/s)  LR: 3.970e-04  Data: 1.571 (1.571)
Train: 171 [  50/1251 (  4%)]  Loss: 3.462 (3.33)  Time: 0.994s, 1030.63/s  (1.030s,  994.63/s)  LR: 3.970e-04  Data: 0.011 (0.042)
Train: 171 [ 100/1251 (  8%)]  Loss: 3.457 (3.37)  Time: 1.031s,  993.58/s  (1.017s, 1006.62/s)  LR: 3.970e-04  Data: 0.011 (0.027)
Train: 171 [ 150/1251 ( 12%)]  Loss: 3.068 (3.30)  Time: 0.993s, 1031.03/s  (1.012s, 1011.90/s)  LR: 3.970e-04  Data: 0.011 (0.021)
Train: 171 [ 200/1251 ( 16%)]  Loss: 3.592 (3.36)  Time: 1.002s, 1022.44/s  (1.008s, 1016.33/s)  LR: 3.970e-04  Data: 0.011 (0.019)
Train: 171 [ 250/1251 ( 20%)]  Loss: 3.461 (3.37)  Time: 0.993s, 1031.23/s  (1.005s, 1018.88/s)  LR: 3.970e-04  Data: 0.012 (0.017)
Train: 171 [ 300/1251 ( 24%)]  Loss: 3.425 (3.38)  Time: 0.995s, 1029.58/s  (1.003s, 1020.71/s)  LR: 3.970e-04  Data: 0.011 (0.016)
Train: 171 [ 350/1251 ( 28%)]  Loss: 3.393 (3.38)  Time: 0.994s, 1029.85/s  (1.002s, 1022.07/s)  LR: 3.970e-04  Data: 0.012 (0.016)
Train: 171 [ 400/1251 ( 32%)]  Loss: 3.384 (3.38)  Time: 0.997s, 1027.41/s  (1.002s, 1022.09/s)  LR: 3.970e-04  Data: 0.012 (0.015)
Train: 171 [ 450/1251 ( 36%)]  Loss: 3.630 (3.41)  Time: 0.993s, 1031.53/s  (1.001s, 1023.03/s)  LR: 3.970e-04  Data: 0.011 (0.015)
Train: 171 [ 500/1251 ( 40%)]  Loss: 3.762 (3.44)  Time: 0.992s, 1031.86/s  (1.000s, 1023.67/s)  LR: 3.970e-04  Data: 0.011 (0.014)
Train: 171 [ 550/1251 ( 44%)]  Loss: 3.754 (3.47)  Time: 0.996s, 1027.67/s  (1.000s, 1024.27/s)  LR: 3.970e-04  Data: 0.011 (0.014)
Train: 171 [ 600/1251 ( 48%)]  Loss: 3.465 (3.47)  Time: 0.995s, 1029.26/s  (0.999s, 1024.74/s)  LR: 3.970e-04  Data: 0.011 (0.014)
Train: 171 [ 650/1251 ( 52%)]  Loss: 3.753 (3.49)  Time: 1.007s, 1017.03/s  (0.999s, 1025.05/s)  LR: 3.970e-04  Data: 0.011 (0.014)
Train: 171 [ 700/1251 ( 56%)]  Loss: 3.614 (3.49)  Time: 0.999s, 1024.59/s  (0.999s, 1025.29/s)  LR: 3.970e-04  Data: 0.011 (0.013)
Train: 171 [ 750/1251 ( 60%)]  Loss: 3.549 (3.50)  Time: 0.993s, 1031.56/s  (0.999s, 1025.54/s)  LR: 3.970e-04  Data: 0.011 (0.013)
Train: 171 [ 800/1251 ( 64%)]  Loss: 3.235 (3.48)  Time: 0.996s, 1028.09/s  (0.998s, 1025.74/s)  LR: 3.970e-04  Data: 0.012 (0.013)
Train: 171 [ 850/1251 ( 68%)]  Loss: 3.297 (3.47)  Time: 0.994s, 1030.25/s  (0.998s, 1025.89/s)  LR: 3.970e-04  Data: 0.012 (0.013)
Train: 171 [ 900/1251 ( 72%)]  Loss: 3.670 (3.48)  Time: 0.995s, 1029.09/s  (0.998s, 1026.12/s)  LR: 3.970e-04  Data: 0.011 (0.013)
Train: 171 [ 950/1251 ( 76%)]  Loss: 2.995 (3.46)  Time: 0.997s, 1027.33/s  (0.998s, 1026.33/s)  LR: 3.970e-04  Data: 0.012 (0.013)
Train: 171 [1000/1251 ( 80%)]  Loss: 3.466 (3.46)  Time: 0.993s, 1030.74/s  (0.998s, 1026.49/s)  LR: 3.970e-04  Data: 0.012 (0.013)
Train: 171 [1050/1251 ( 84%)]  Loss: 3.554 (3.46)  Time: 0.993s, 1031.17/s  (0.997s, 1026.61/s)  LR: 3.970e-04  Data: 0.011 (0.013)
Train: 171 [1100/1251 ( 88%)]  Loss: 3.609 (3.47)  Time: 0.994s, 1030.63/s  (0.997s, 1026.71/s)  LR: 3.970e-04  Data: 0.012 (0.013)
Train: 171 [1150/1251 ( 92%)]  Loss: 3.319 (3.46)  Time: 0.995s, 1029.34/s  (0.997s, 1026.81/s)  LR: 3.970e-04  Data: 0.011 (0.013)
Train: 171 [1200/1251 ( 96%)]  Loss: 3.570 (3.47)  Time: 0.993s, 1031.69/s  (0.997s, 1026.85/s)  LR: 3.970e-04  Data: 0.011 (0.013)
Train: 171 [1250/1251 (100%)]  Loss: 3.279 (3.46)  Time: 0.984s, 1040.42/s  (0.997s, 1026.90/s)  LR: 3.970e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.631 (1.631)  Loss:  0.7565 (0.7565)  Acc@1: 89.9414 (89.9414)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.245 (0.564)  Loss:  0.8803 (1.2705)  Acc@1: 85.0236 (75.8080)  Acc@5: 97.1698 (93.0340)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-171.pth.tar', 75.8079999584961)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-166.pth.tar', 75.49799998291016)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-165.pth.tar', 75.43400009277343)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-170.pth.tar', 75.43200000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-168.pth.tar', 75.3800000366211)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-163.pth.tar', 75.33000014160156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-162.pth.tar', 75.28399998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-160.pth.tar', 75.25399999023438)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-157.pth.tar', 75.23400008789062)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-169.pth.tar', 75.22000001464843)

Train: 172 [   0/1251 (  0%)]  Loss: 3.106 (3.11)  Time: 2.376s,  430.97/s  (2.376s,  430.97/s)  LR: 3.920e-04  Data: 1.428 (1.428)
Train: 172 [  50/1251 (  4%)]  Loss: 3.655 (3.38)  Time: 0.997s, 1027.16/s  (1.023s, 1000.58/s)  LR: 3.920e-04  Data: 0.011 (0.039)
Train: 172 [ 100/1251 (  8%)]  Loss: 3.618 (3.46)  Time: 1.000s, 1024.42/s  (1.010s, 1014.34/s)  LR: 3.920e-04  Data: 0.011 (0.025)
Train: 172 [ 150/1251 ( 12%)]  Loss: 3.388 (3.44)  Time: 0.992s, 1032.65/s  (1.005s, 1019.31/s)  LR: 3.920e-04  Data: 0.011 (0.021)
Train: 172 [ 200/1251 ( 16%)]  Loss: 3.181 (3.39)  Time: 1.001s, 1022.87/s  (1.002s, 1021.50/s)  LR: 3.920e-04  Data: 0.012 (0.018)
Train: 172 [ 250/1251 ( 20%)]  Loss: 3.720 (3.44)  Time: 0.995s, 1029.50/s  (1.001s, 1022.68/s)  LR: 3.920e-04  Data: 0.011 (0.017)
Train: 172 [ 300/1251 ( 24%)]  Loss: 3.159 (3.40)  Time: 0.994s, 1030.41/s  (1.000s, 1023.70/s)  LR: 3.920e-04  Data: 0.011 (0.016)
Train: 172 [ 350/1251 ( 28%)]  Loss: 3.485 (3.41)  Time: 0.993s, 1031.70/s  (1.000s, 1024.27/s)  LR: 3.920e-04  Data: 0.010 (0.015)
Train: 172 [ 400/1251 ( 32%)]  Loss: 3.551 (3.43)  Time: 0.994s, 1029.77/s  (0.999s, 1024.72/s)  LR: 3.920e-04  Data: 0.011 (0.015)
Train: 172 [ 450/1251 ( 36%)]  Loss: 3.635 (3.45)  Time: 0.993s, 1031.57/s  (0.999s, 1025.05/s)  LR: 3.920e-04  Data: 0.011 (0.014)
Train: 172 [ 500/1251 ( 40%)]  Loss: 3.324 (3.44)  Time: 0.995s, 1029.17/s  (0.999s, 1025.41/s)  LR: 3.920e-04  Data: 0.011 (0.014)
Train: 172 [ 550/1251 ( 44%)]  Loss: 3.655 (3.46)  Time: 0.994s, 1030.34/s  (0.998s, 1025.64/s)  LR: 3.920e-04  Data: 0.011 (0.014)
Train: 172 [ 600/1251 ( 48%)]  Loss: 3.471 (3.46)  Time: 0.994s, 1030.48/s  (0.998s, 1025.77/s)  LR: 3.920e-04  Data: 0.011 (0.014)
Train: 172 [ 650/1251 ( 52%)]  Loss: 3.510 (3.46)  Time: 0.996s, 1027.98/s  (0.998s, 1025.96/s)  LR: 3.920e-04  Data: 0.011 (0.013)
Train: 172 [ 700/1251 ( 56%)]  Loss: 3.631 (3.47)  Time: 0.993s, 1030.95/s  (0.998s, 1026.11/s)  LR: 3.920e-04  Data: 0.011 (0.013)
Train: 172 [ 750/1251 ( 60%)]  Loss: 3.601 (3.48)  Time: 1.001s, 1022.87/s  (0.998s, 1026.21/s)  LR: 3.920e-04  Data: 0.011 (0.013)
Train: 172 [ 800/1251 ( 64%)]  Loss: 3.610 (3.49)  Time: 0.996s, 1028.36/s  (0.998s, 1026.43/s)  LR: 3.920e-04  Data: 0.011 (0.013)
Train: 172 [ 850/1251 ( 68%)]  Loss: 3.647 (3.50)  Time: 0.995s, 1029.57/s  (0.997s, 1026.63/s)  LR: 3.920e-04  Data: 0.011 (0.013)
Train: 172 [ 900/1251 ( 72%)]  Loss: 3.339 (3.49)  Time: 0.990s, 1034.56/s  (0.997s, 1026.81/s)  LR: 3.920e-04  Data: 0.010 (0.013)
Train: 172 [ 950/1251 ( 76%)]  Loss: 3.502 (3.49)  Time: 0.995s, 1028.76/s  (0.997s, 1026.96/s)  LR: 3.920e-04  Data: 0.011 (0.013)
Train: 172 [1000/1251 ( 80%)]  Loss: 3.552 (3.49)  Time: 1.004s, 1020.26/s  (0.997s, 1027.05/s)  LR: 3.920e-04  Data: 0.011 (0.013)
Train: 172 [1050/1251 ( 84%)]  Loss: 3.593 (3.50)  Time: 0.998s, 1026.27/s  (0.997s, 1027.15/s)  LR: 3.920e-04  Data: 0.011 (0.013)
Train: 172 [1100/1251 ( 88%)]  Loss: 3.392 (3.49)  Time: 0.995s, 1028.86/s  (0.997s, 1027.18/s)  LR: 3.920e-04  Data: 0.011 (0.013)
Train: 172 [1150/1251 ( 92%)]  Loss: 3.327 (3.49)  Time: 0.993s, 1031.19/s  (0.997s, 1027.28/s)  LR: 3.920e-04  Data: 0.011 (0.012)
Train: 172 [1200/1251 ( 96%)]  Loss: 3.737 (3.50)  Time: 0.998s, 1025.56/s  (0.997s, 1027.34/s)  LR: 3.920e-04  Data: 0.012 (0.012)
Train: 172 [1250/1251 (100%)]  Loss: 3.354 (3.49)  Time: 0.980s, 1044.62/s  (0.997s, 1027.42/s)  LR: 3.920e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.756 (1.756)  Loss:  0.7391 (0.7391)  Acc@1: 89.7461 (89.7461)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.245 (0.564)  Loss:  0.7983 (1.2522)  Acc@1: 86.2028 (75.7120)  Acc@5: 95.9906 (93.1480)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-171.pth.tar', 75.8079999584961)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-172.pth.tar', 75.71200008300781)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-166.pth.tar', 75.49799998291016)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-165.pth.tar', 75.43400009277343)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-170.pth.tar', 75.43200000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-168.pth.tar', 75.3800000366211)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-163.pth.tar', 75.33000014160156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-162.pth.tar', 75.28399998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-160.pth.tar', 75.25399999023438)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-157.pth.tar', 75.23400008789062)

Train: 173 [   0/1251 (  0%)]  Loss: 3.725 (3.72)  Time: 2.460s,  416.24/s  (2.460s,  416.24/s)  LR: 3.869e-04  Data: 1.492 (1.492)
Train: 173 [  50/1251 (  4%)]  Loss: 3.734 (3.73)  Time: 0.995s, 1029.57/s  (1.040s,  984.81/s)  LR: 3.869e-04  Data: 0.012 (0.042)
Train: 173 [ 100/1251 (  8%)]  Loss: 3.092 (3.52)  Time: 1.120s,  914.48/s  (1.032s,  992.01/s)  LR: 3.869e-04  Data: 0.012 (0.027)
Train: 173 [ 150/1251 ( 12%)]  Loss: 3.554 (3.53)  Time: 0.998s, 1026.32/s  (1.022s, 1002.10/s)  LR: 3.869e-04  Data: 0.011 (0.022)
Train: 173 [ 200/1251 ( 16%)]  Loss: 3.560 (3.53)  Time: 1.009s, 1014.95/s  (1.017s, 1007.24/s)  LR: 3.869e-04  Data: 0.016 (0.019)
Train: 173 [ 250/1251 ( 20%)]  Loss: 3.481 (3.52)  Time: 0.997s, 1027.17/s  (1.014s, 1009.66/s)  LR: 3.869e-04  Data: 0.011 (0.018)
Train: 173 [ 300/1251 ( 24%)]  Loss: 3.349 (3.50)  Time: 1.006s, 1017.51/s  (1.012s, 1011.85/s)  LR: 3.869e-04  Data: 0.011 (0.017)
Train: 173 [ 350/1251 ( 28%)]  Loss: 3.479 (3.50)  Time: 1.005s, 1018.92/s  (1.011s, 1013.19/s)  LR: 3.869e-04  Data: 0.015 (0.016)
Train: 173 [ 400/1251 ( 32%)]  Loss: 3.291 (3.47)  Time: 0.995s, 1029.06/s  (1.009s, 1014.39/s)  LR: 3.869e-04  Data: 0.011 (0.015)
Train: 173 [ 450/1251 ( 36%)]  Loss: 3.732 (3.50)  Time: 0.996s, 1027.65/s  (1.008s, 1015.75/s)  LR: 3.869e-04  Data: 0.012 (0.015)
Train: 173 [ 500/1251 ( 40%)]  Loss: 3.486 (3.50)  Time: 0.995s, 1029.42/s  (1.007s, 1016.44/s)  LR: 3.869e-04  Data: 0.011 (0.015)
Train: 173 [ 550/1251 ( 44%)]  Loss: 3.501 (3.50)  Time: 0.999s, 1024.86/s  (1.007s, 1017.24/s)  LR: 3.869e-04  Data: 0.012 (0.014)
Train: 173 [ 600/1251 ( 48%)]  Loss: 3.457 (3.50)  Time: 1.001s, 1023.45/s  (1.006s, 1017.87/s)  LR: 3.869e-04  Data: 0.011 (0.014)
Train: 173 [ 650/1251 ( 52%)]  Loss: 3.552 (3.50)  Time: 0.996s, 1027.62/s  (1.005s, 1018.49/s)  LR: 3.869e-04  Data: 0.011 (0.014)
Train: 173 [ 700/1251 ( 56%)]  Loss: 3.557 (3.50)  Time: 0.994s, 1030.17/s  (1.005s, 1018.95/s)  LR: 3.869e-04  Data: 0.011 (0.014)
Train: 173 [ 750/1251 ( 60%)]  Loss: 3.815 (3.52)  Time: 0.996s, 1028.28/s  (1.005s, 1019.16/s)  LR: 3.869e-04  Data: 0.012 (0.013)
Train: 173 [ 800/1251 ( 64%)]  Loss: 3.434 (3.52)  Time: 0.996s, 1028.22/s  (1.005s, 1019.24/s)  LR: 3.869e-04  Data: 0.011 (0.013)
Train: 173 [ 850/1251 ( 68%)]  Loss: 3.436 (3.51)  Time: 1.005s, 1018.65/s  (1.005s, 1019.26/s)  LR: 3.869e-04  Data: 0.012 (0.013)
Train: 173 [ 900/1251 ( 72%)]  Loss: 3.413 (3.51)  Time: 1.000s, 1023.86/s  (1.005s, 1018.89/s)  LR: 3.869e-04  Data: 0.011 (0.013)
Train: 173 [ 950/1251 ( 76%)]  Loss: 3.142 (3.49)  Time: 0.995s, 1028.78/s  (1.005s, 1019.29/s)  LR: 3.869e-04  Data: 0.011 (0.013)
Train: 173 [1000/1251 ( 80%)]  Loss: 3.460 (3.49)  Time: 1.004s, 1020.06/s  (1.004s, 1019.55/s)  LR: 3.869e-04  Data: 0.011 (0.013)
Train: 173 [1050/1251 ( 84%)]  Loss: 3.829 (3.50)  Time: 0.993s, 1030.82/s  (1.004s, 1019.76/s)  LR: 3.869e-04  Data: 0.011 (0.013)
Train: 173 [1100/1251 ( 88%)]  Loss: 3.354 (3.50)  Time: 0.994s, 1029.68/s  (1.005s, 1019.37/s)  LR: 3.869e-04  Data: 0.011 (0.013)
Train: 173 [1150/1251 ( 92%)]  Loss: 3.622 (3.50)  Time: 1.006s, 1017.75/s  (1.004s, 1019.56/s)  LR: 3.869e-04  Data: 0.011 (0.013)
Train: 173 [1200/1251 ( 96%)]  Loss: 3.058 (3.48)  Time: 0.994s, 1029.75/s  (1.004s, 1019.79/s)  LR: 3.869e-04  Data: 0.011 (0.013)
Train: 173 [1250/1251 (100%)]  Loss: 3.791 (3.50)  Time: 0.988s, 1036.53/s  (1.004s, 1020.08/s)  LR: 3.869e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.614 (1.614)  Loss:  0.7278 (0.7278)  Acc@1: 90.5273 (90.5273)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.246 (0.574)  Loss:  0.7989 (1.2347)  Acc@1: 85.6132 (75.5920)  Acc@5: 96.8160 (92.9460)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-171.pth.tar', 75.8079999584961)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-172.pth.tar', 75.71200008300781)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-173.pth.tar', 75.59200008544921)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-166.pth.tar', 75.49799998291016)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-165.pth.tar', 75.43400009277343)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-170.pth.tar', 75.43200000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-168.pth.tar', 75.3800000366211)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-163.pth.tar', 75.33000014160156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-162.pth.tar', 75.28399998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-160.pth.tar', 75.25399999023438)

Train: 174 [   0/1251 (  0%)]  Loss: 3.489 (3.49)  Time: 4.532s,  225.97/s  (4.532s,  225.97/s)  LR: 3.819e-04  Data: 3.246 (3.246)
Train: 174 [  50/1251 (  4%)]  Loss: 3.879 (3.68)  Time: 1.002s, 1022.35/s  (1.073s,  954.33/s)  LR: 3.819e-04  Data: 0.012 (0.075)
Train: 174 [ 100/1251 (  8%)]  Loss: 3.385 (3.58)  Time: 1.002s, 1021.95/s  (1.039s,  985.90/s)  LR: 3.819e-04  Data: 0.012 (0.044)
Train: 174 [ 150/1251 ( 12%)]  Loss: 3.874 (3.66)  Time: 0.994s, 1030.29/s  (1.030s,  994.56/s)  LR: 3.819e-04  Data: 0.011 (0.033)
Train: 174 [ 200/1251 ( 16%)]  Loss: 3.386 (3.60)  Time: 1.037s,  987.61/s  (1.024s, 1000.24/s)  LR: 3.819e-04  Data: 0.011 (0.028)
Train: 174 [ 250/1251 ( 20%)]  Loss: 3.515 (3.59)  Time: 0.995s, 1028.95/s  (1.021s, 1003.34/s)  LR: 3.819e-04  Data: 0.010 (0.024)
Train: 174 [ 300/1251 ( 24%)]  Loss: 3.697 (3.60)  Time: 1.040s,  984.73/s  (1.020s, 1004.05/s)  LR: 3.819e-04  Data: 0.011 (0.022)
Train: 174 [ 350/1251 ( 28%)]  Loss: 3.588 (3.60)  Time: 0.995s, 1028.65/s  (1.019s, 1004.68/s)  LR: 3.819e-04  Data: 0.011 (0.021)
Train: 174 [ 400/1251 ( 32%)]  Loss: 3.301 (3.57)  Time: 1.067s,  960.06/s  (1.019s, 1004.89/s)  LR: 3.819e-04  Data: 0.011 (0.020)
Train: 174 [ 450/1251 ( 36%)]  Loss: 3.329 (3.54)  Time: 1.031s,  993.33/s  (1.020s, 1003.45/s)  LR: 3.819e-04  Data: 0.011 (0.019)
Train: 174 [ 500/1251 ( 40%)]  Loss: 3.620 (3.55)  Time: 1.008s, 1015.84/s  (1.019s, 1004.47/s)  LR: 3.819e-04  Data: 0.012 (0.018)
Train: 174 [ 550/1251 ( 44%)]  Loss: 3.359 (3.54)  Time: 0.991s, 1032.94/s  (1.018s, 1006.08/s)  LR: 3.819e-04  Data: 0.010 (0.017)
Train: 174 [ 600/1251 ( 48%)]  Loss: 3.460 (3.53)  Time: 0.999s, 1025.07/s  (1.017s, 1007.33/s)  LR: 3.819e-04  Data: 0.012 (0.017)
Train: 174 [ 650/1251 ( 52%)]  Loss: 3.419 (3.52)  Time: 1.032s,  992.38/s  (1.016s, 1008.31/s)  LR: 3.819e-04  Data: 0.010 (0.016)
Train: 174 [ 700/1251 ( 56%)]  Loss: 3.220 (3.50)  Time: 0.993s, 1031.36/s  (1.014s, 1009.41/s)  LR: 3.819e-04  Data: 0.011 (0.016)
Train: 174 [ 750/1251 ( 60%)]  Loss: 3.638 (3.51)  Time: 1.001s, 1023.17/s  (1.014s, 1010.17/s)  LR: 3.819e-04  Data: 0.011 (0.016)
Train: 174 [ 800/1251 ( 64%)]  Loss: 3.172 (3.49)  Time: 0.999s, 1024.78/s  (1.014s, 1010.20/s)  LR: 3.819e-04  Data: 0.011 (0.015)
Train: 174 [ 850/1251 ( 68%)]  Loss: 3.299 (3.48)  Time: 0.996s, 1028.06/s  (1.013s, 1010.94/s)  LR: 3.819e-04  Data: 0.012 (0.015)
Train: 174 [ 900/1251 ( 72%)]  Loss: 2.896 (3.45)  Time: 1.029s,  994.67/s  (1.012s, 1011.60/s)  LR: 3.819e-04  Data: 0.010 (0.015)
Train: 174 [ 950/1251 ( 76%)]  Loss: 3.733 (3.46)  Time: 0.998s, 1025.66/s  (1.012s, 1012.23/s)  LR: 3.819e-04  Data: 0.012 (0.015)
Train: 174 [1000/1251 ( 80%)]  Loss: 3.434 (3.46)  Time: 0.995s, 1029.35/s  (1.012s, 1012.07/s)  LR: 3.819e-04  Data: 0.011 (0.015)
Train: 174 [1050/1251 ( 84%)]  Loss: 3.407 (3.46)  Time: 1.000s, 1024.34/s  (1.012s, 1011.88/s)  LR: 3.819e-04  Data: 0.011 (0.014)
Train: 174 [1100/1251 ( 88%)]  Loss: 3.514 (3.46)  Time: 1.026s,  998.53/s  (1.011s, 1012.37/s)  LR: 3.819e-04  Data: 0.011 (0.014)
Train: 174 [1150/1251 ( 92%)]  Loss: 3.452 (3.46)  Time: 1.003s, 1021.32/s  (1.012s, 1012.23/s)  LR: 3.819e-04  Data: 0.013 (0.014)
Train: 174 [1200/1251 ( 96%)]  Loss: 3.745 (3.47)  Time: 1.000s, 1023.95/s  (1.012s, 1012.34/s)  LR: 3.819e-04  Data: 0.011 (0.014)
Train: 174 [1250/1251 (100%)]  Loss: 3.524 (3.47)  Time: 0.986s, 1038.60/s  (1.011s, 1012.80/s)  LR: 3.819e-04  Data: 0.000 (0.014)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.681 (1.681)  Loss:  0.7743 (0.7743)  Acc@1: 89.3555 (89.3555)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.245 (0.570)  Loss:  0.8279 (1.2582)  Acc@1: 84.7877 (76.0860)  Acc@5: 96.2264 (93.1240)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-174.pth.tar', 76.08599998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-171.pth.tar', 75.8079999584961)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-172.pth.tar', 75.71200008300781)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-173.pth.tar', 75.59200008544921)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-166.pth.tar', 75.49799998291016)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-165.pth.tar', 75.43400009277343)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-170.pth.tar', 75.43200000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-168.pth.tar', 75.3800000366211)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-163.pth.tar', 75.33000014160156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-162.pth.tar', 75.28399998535156)

Train: 175 [   0/1251 (  0%)]  Loss: 3.382 (3.38)  Time: 2.520s,  406.30/s  (2.520s,  406.30/s)  LR: 3.769e-04  Data: 1.558 (1.558)
Train: 175 [  50/1251 (  4%)]  Loss: 3.370 (3.38)  Time: 1.003s, 1020.62/s  (1.037s,  987.62/s)  LR: 3.769e-04  Data: 0.016 (0.042)
Train: 175 [ 100/1251 (  8%)]  Loss: 3.257 (3.34)  Time: 0.996s, 1027.92/s  (1.019s, 1004.64/s)  LR: 3.769e-04  Data: 0.012 (0.027)
Train: 175 [ 150/1251 ( 12%)]  Loss: 3.414 (3.36)  Time: 1.000s, 1024.21/s  (1.017s, 1007.34/s)  LR: 3.769e-04  Data: 0.013 (0.022)
Train: 175 [ 200/1251 ( 16%)]  Loss: 3.485 (3.38)  Time: 0.996s, 1028.15/s  (1.015s, 1009.32/s)  LR: 3.769e-04  Data: 0.012 (0.019)
Train: 175 [ 250/1251 ( 20%)]  Loss: 3.605 (3.42)  Time: 0.998s, 1026.15/s  (1.012s, 1011.38/s)  LR: 3.769e-04  Data: 0.011 (0.018)
Train: 175 [ 300/1251 ( 24%)]  Loss: 3.086 (3.37)  Time: 1.001s, 1022.88/s  (1.010s, 1013.76/s)  LR: 3.769e-04  Data: 0.011 (0.017)
Train: 175 [ 350/1251 ( 28%)]  Loss: 3.874 (3.43)  Time: 0.995s, 1029.37/s  (1.011s, 1012.80/s)  LR: 3.769e-04  Data: 0.011 (0.016)
Train: 175 [ 400/1251 ( 32%)]  Loss: 3.366 (3.43)  Time: 0.993s, 1030.73/s  (1.010s, 1014.09/s)  LR: 3.769e-04  Data: 0.011 (0.015)
Train: 175 [ 450/1251 ( 36%)]  Loss: 3.523 (3.44)  Time: 1.001s, 1023.47/s  (1.009s, 1015.26/s)  LR: 3.769e-04  Data: 0.012 (0.015)
Train: 175 [ 500/1251 ( 40%)]  Loss: 2.956 (3.39)  Time: 1.003s, 1020.49/s  (1.008s, 1015.56/s)  LR: 3.769e-04  Data: 0.013 (0.015)
Train: 175 [ 550/1251 ( 44%)]  Loss: 3.391 (3.39)  Time: 1.022s, 1002.24/s  (1.008s, 1016.15/s)  LR: 3.769e-04  Data: 0.011 (0.014)
Train: 175 [ 600/1251 ( 48%)]  Loss: 3.151 (3.37)  Time: 1.000s, 1023.74/s  (1.007s, 1016.73/s)  LR: 3.769e-04  Data: 0.012 (0.014)
Train: 175 [ 650/1251 ( 52%)]  Loss: 3.565 (3.39)  Time: 0.999s, 1024.61/s  (1.008s, 1015.58/s)  LR: 3.769e-04  Data: 0.011 (0.014)
Train: 175 [ 700/1251 ( 56%)]  Loss: 3.574 (3.40)  Time: 0.996s, 1028.00/s  (1.008s, 1016.01/s)  LR: 3.769e-04  Data: 0.010 (0.014)
Train: 175 [ 750/1251 ( 60%)]  Loss: 3.524 (3.41)  Time: 1.000s, 1023.73/s  (1.007s, 1016.55/s)  LR: 3.769e-04  Data: 0.012 (0.014)
Train: 175 [ 800/1251 ( 64%)]  Loss: 3.670 (3.42)  Time: 0.997s, 1027.17/s  (1.007s, 1017.02/s)  LR: 3.769e-04  Data: 0.012 (0.013)
Train: 175 [ 850/1251 ( 68%)]  Loss: 3.315 (3.42)  Time: 0.998s, 1026.17/s  (1.007s, 1017.25/s)  LR: 3.769e-04  Data: 0.011 (0.013)
Train: 175 [ 900/1251 ( 72%)]  Loss: 3.275 (3.41)  Time: 0.995s, 1028.72/s  (1.006s, 1017.49/s)  LR: 3.769e-04  Data: 0.011 (0.013)
Train: 175 [ 950/1251 ( 76%)]  Loss: 3.552 (3.42)  Time: 0.995s, 1029.17/s  (1.006s, 1017.55/s)  LR: 3.769e-04  Data: 0.011 (0.013)
Train: 175 [1000/1251 ( 80%)]  Loss: 3.357 (3.41)  Time: 0.999s, 1025.11/s  (1.006s, 1017.92/s)  LR: 3.769e-04  Data: 0.012 (0.013)
Train: 175 [1050/1251 ( 84%)]  Loss: 3.670 (3.43)  Time: 0.998s, 1026.29/s  (1.006s, 1018.26/s)  LR: 3.769e-04  Data: 0.012 (0.013)
Train: 175 [1100/1251 ( 88%)]  Loss: 3.313 (3.42)  Time: 0.994s, 1030.30/s  (1.005s, 1018.41/s)  LR: 3.769e-04  Data: 0.012 (0.013)
Train: 175 [1150/1251 ( 92%)]  Loss: 3.353 (3.42)  Time: 0.997s, 1026.71/s  (1.005s, 1018.66/s)  LR: 3.769e-04  Data: 0.012 (0.013)
Train: 175 [1200/1251 ( 96%)]  Loss: 3.275 (3.41)  Time: 0.995s, 1029.12/s  (1.005s, 1018.96/s)  LR: 3.769e-04  Data: 0.011 (0.013)
Train: 175 [1250/1251 (100%)]  Loss: 3.174 (3.40)  Time: 0.983s, 1041.89/s  (1.005s, 1018.41/s)  LR: 3.769e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.634 (1.634)  Loss:  0.7026 (0.7026)  Acc@1: 89.1602 (89.1602)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.245 (0.578)  Loss:  0.8408 (1.2436)  Acc@1: 84.4340 (75.9340)  Acc@5: 97.2877 (93.0280)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-174.pth.tar', 76.08599998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-175.pth.tar', 75.9339999609375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-171.pth.tar', 75.8079999584961)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-172.pth.tar', 75.71200008300781)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-173.pth.tar', 75.59200008544921)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-166.pth.tar', 75.49799998291016)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-165.pth.tar', 75.43400009277343)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-170.pth.tar', 75.43200000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-168.pth.tar', 75.3800000366211)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-163.pth.tar', 75.33000014160156)

Train: 176 [   0/1251 (  0%)]  Loss: 3.388 (3.39)  Time: 2.682s,  381.77/s  (2.682s,  381.77/s)  LR: 3.719e-04  Data: 1.649 (1.649)
Train: 176 [  50/1251 (  4%)]  Loss: 3.397 (3.39)  Time: 0.994s, 1030.40/s  (1.037s,  987.67/s)  LR: 3.719e-04  Data: 0.011 (0.044)
Train: 176 [ 100/1251 (  8%)]  Loss: 3.526 (3.44)  Time: 0.994s, 1029.85/s  (1.017s, 1006.47/s)  LR: 3.719e-04  Data: 0.011 (0.028)
Train: 176 [ 150/1251 ( 12%)]  Loss: 3.318 (3.41)  Time: 1.054s,  971.94/s  (1.018s, 1005.81/s)  LR: 3.719e-04  Data: 0.011 (0.022)
Train: 176 [ 200/1251 ( 16%)]  Loss: 3.774 (3.48)  Time: 0.999s, 1025.14/s  (1.014s, 1009.95/s)  LR: 3.719e-04  Data: 0.012 (0.020)
Train: 176 [ 250/1251 ( 20%)]  Loss: 3.633 (3.51)  Time: 1.032s,  992.71/s  (1.013s, 1011.10/s)  LR: 3.719e-04  Data: 0.011 (0.018)
Train: 176 [ 300/1251 ( 24%)]  Loss: 3.113 (3.45)  Time: 0.998s, 1025.97/s  (1.012s, 1011.46/s)  LR: 3.719e-04  Data: 0.012 (0.017)
Train: 176 [ 350/1251 ( 28%)]  Loss: 3.547 (3.46)  Time: 1.033s,  991.50/s  (1.011s, 1013.32/s)  LR: 3.719e-04  Data: 0.011 (0.016)
Train: 176 [ 400/1251 ( 32%)]  Loss: 3.493 (3.47)  Time: 0.999s, 1024.63/s  (1.014s, 1009.97/s)  LR: 3.719e-04  Data: 0.011 (0.016)
Train: 176 [ 450/1251 ( 36%)]  Loss: 3.253 (3.44)  Time: 1.011s, 1012.90/s  (1.012s, 1011.45/s)  LR: 3.719e-04  Data: 0.011 (0.015)
Train: 176 [ 500/1251 ( 40%)]  Loss: 3.193 (3.42)  Time: 0.998s, 1025.75/s  (1.011s, 1012.48/s)  LR: 3.719e-04  Data: 0.011 (0.015)
Train: 176 [ 550/1251 ( 44%)]  Loss: 3.434 (3.42)  Time: 0.996s, 1027.79/s  (1.010s, 1013.73/s)  LR: 3.719e-04  Data: 0.011 (0.014)
Train: 176 [ 600/1251 ( 48%)]  Loss: 3.401 (3.42)  Time: 0.996s, 1028.28/s  (1.009s, 1014.59/s)  LR: 3.719e-04  Data: 0.011 (0.014)
Train: 176 [ 650/1251 ( 52%)]  Loss: 3.067 (3.40)  Time: 1.051s,  974.27/s  (1.009s, 1014.81/s)  LR: 3.719e-04  Data: 0.012 (0.014)
Train: 176 [ 700/1251 ( 56%)]  Loss: 3.556 (3.41)  Time: 1.064s,  962.54/s  (1.010s, 1014.13/s)  LR: 3.719e-04  Data: 0.011 (0.014)
Train: 176 [ 750/1251 ( 60%)]  Loss: 3.175 (3.39)  Time: 0.998s, 1026.53/s  (1.010s, 1014.22/s)  LR: 3.719e-04  Data: 0.011 (0.014)
Train: 176 [ 800/1251 ( 64%)]  Loss: 3.391 (3.39)  Time: 0.999s, 1024.91/s  (1.009s, 1014.84/s)  LR: 3.719e-04  Data: 0.011 (0.013)
Train: 176 [ 850/1251 ( 68%)]  Loss: 3.609 (3.40)  Time: 0.995s, 1029.38/s  (1.008s, 1015.54/s)  LR: 3.719e-04  Data: 0.011 (0.013)
Train: 176 [ 900/1251 ( 72%)]  Loss: 3.618 (3.42)  Time: 0.995s, 1029.62/s  (1.008s, 1015.93/s)  LR: 3.719e-04  Data: 0.011 (0.013)
Train: 176 [ 950/1251 ( 76%)]  Loss: 3.588 (3.42)  Time: 0.995s, 1028.97/s  (1.007s, 1016.41/s)  LR: 3.719e-04  Data: 0.011 (0.013)
Train: 176 [1000/1251 ( 80%)]  Loss: 3.387 (3.42)  Time: 1.004s, 1020.02/s  (1.008s, 1015.83/s)  LR: 3.719e-04  Data: 0.011 (0.013)
Train: 176 [1050/1251 ( 84%)]  Loss: 3.611 (3.43)  Time: 1.050s,  975.03/s  (1.008s, 1016.14/s)  LR: 3.719e-04  Data: 0.011 (0.013)
Train: 176 [1100/1251 ( 88%)]  Loss: 3.649 (3.44)  Time: 0.996s, 1027.81/s  (1.007s, 1016.62/s)  LR: 3.719e-04  Data: 0.011 (0.013)
Train: 176 [1150/1251 ( 92%)]  Loss: 3.312 (3.43)  Time: 0.998s, 1026.02/s  (1.008s, 1015.79/s)  LR: 3.719e-04  Data: 0.012 (0.013)
Train: 176 [1200/1251 ( 96%)]  Loss: 3.534 (3.44)  Time: 0.994s, 1029.83/s  (1.008s, 1016.17/s)  LR: 3.719e-04  Data: 0.011 (0.013)
Train: 176 [1250/1251 (100%)]  Loss: 3.336 (3.43)  Time: 0.989s, 1035.09/s  (1.007s, 1016.39/s)  LR: 3.719e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.668 (1.668)  Loss:  0.7053 (0.7053)  Acc@1: 90.4297 (90.4297)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.246 (0.573)  Loss:  0.7900 (1.2078)  Acc@1: 86.4387 (75.9220)  Acc@5: 97.0519 (93.1160)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-174.pth.tar', 76.08599998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-175.pth.tar', 75.9339999609375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-176.pth.tar', 75.9219999267578)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-171.pth.tar', 75.8079999584961)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-172.pth.tar', 75.71200008300781)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-173.pth.tar', 75.59200008544921)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-166.pth.tar', 75.49799998291016)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-165.pth.tar', 75.43400009277343)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-170.pth.tar', 75.43200000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-168.pth.tar', 75.3800000366211)

Train: 177 [   0/1251 (  0%)]  Loss: 3.597 (3.60)  Time: 2.502s,  409.30/s  (2.502s,  409.30/s)  LR: 3.669e-04  Data: 1.537 (1.537)
Train: 177 [  50/1251 (  4%)]  Loss: 3.706 (3.65)  Time: 0.996s, 1027.74/s  (1.032s,  992.46/s)  LR: 3.669e-04  Data: 0.012 (0.042)
Train: 177 [ 100/1251 (  8%)]  Loss: 3.739 (3.68)  Time: 0.996s, 1027.95/s  (1.018s, 1005.51/s)  LR: 3.669e-04  Data: 0.012 (0.027)
Train: 177 [ 150/1251 ( 12%)]  Loss: 3.352 (3.60)  Time: 0.993s, 1030.73/s  (1.013s, 1010.96/s)  LR: 3.669e-04  Data: 0.011 (0.022)
Train: 177 [ 200/1251 ( 16%)]  Loss: 3.824 (3.64)  Time: 0.995s, 1029.00/s  (1.010s, 1013.69/s)  LR: 3.669e-04  Data: 0.011 (0.019)
Train: 177 [ 250/1251 ( 20%)]  Loss: 3.161 (3.56)  Time: 0.997s, 1027.11/s  (1.008s, 1015.62/s)  LR: 3.669e-04  Data: 0.011 (0.018)
Train: 177 [ 300/1251 ( 24%)]  Loss: 3.509 (3.56)  Time: 0.999s, 1024.82/s  (1.008s, 1015.57/s)  LR: 3.669e-04  Data: 0.012 (0.017)
Train: 177 [ 350/1251 ( 28%)]  Loss: 3.618 (3.56)  Time: 0.999s, 1025.24/s  (1.007s, 1016.67/s)  LR: 3.669e-04  Data: 0.010 (0.016)
Train: 177 [ 400/1251 ( 32%)]  Loss: 3.624 (3.57)  Time: 1.001s, 1022.75/s  (1.007s, 1017.16/s)  LR: 3.669e-04  Data: 0.012 (0.015)
Train: 177 [ 450/1251 ( 36%)]  Loss: 3.736 (3.59)  Time: 0.991s, 1033.49/s  (1.006s, 1017.93/s)  LR: 3.669e-04  Data: 0.010 (0.015)
Train: 177 [ 500/1251 ( 40%)]  Loss: 3.652 (3.59)  Time: 0.997s, 1027.52/s  (1.006s, 1018.33/s)  LR: 3.669e-04  Data: 0.011 (0.015)
Train: 177 [ 550/1251 ( 44%)]  Loss: 3.768 (3.61)  Time: 0.995s, 1028.69/s  (1.005s, 1018.45/s)  LR: 3.669e-04  Data: 0.011 (0.014)
Train: 177 [ 600/1251 ( 48%)]  Loss: 3.497 (3.60)  Time: 0.996s, 1027.77/s  (1.005s, 1018.93/s)  LR: 3.669e-04  Data: 0.011 (0.014)
Train: 177 [ 650/1251 ( 52%)]  Loss: 3.329 (3.58)  Time: 0.994s, 1030.43/s  (1.005s, 1019.34/s)  LR: 3.669e-04  Data: 0.011 (0.014)
Train: 177 [ 700/1251 ( 56%)]  Loss: 3.762 (3.59)  Time: 0.993s, 1031.26/s  (1.006s, 1018.20/s)  LR: 3.669e-04  Data: 0.011 (0.014)
Train: 177 [ 750/1251 ( 60%)]  Loss: 3.195 (3.57)  Time: 1.006s, 1017.75/s  (1.006s, 1018.33/s)  LR: 3.669e-04  Data: 0.013 (0.014)
Train: 177 [ 800/1251 ( 64%)]  Loss: 3.650 (3.57)  Time: 1.003s, 1020.50/s  (1.006s, 1018.06/s)  LR: 3.669e-04  Data: 0.012 (0.013)
Train: 177 [ 850/1251 ( 68%)]  Loss: 3.274 (3.56)  Time: 1.023s, 1000.94/s  (1.006s, 1018.33/s)  LR: 3.669e-04  Data: 0.011 (0.013)
Train: 177 [ 900/1251 ( 72%)]  Loss: 3.493 (3.55)  Time: 1.003s, 1020.59/s  (1.007s, 1017.34/s)  LR: 3.669e-04  Data: 0.011 (0.013)
Train: 177 [ 950/1251 ( 76%)]  Loss: 3.028 (3.53)  Time: 0.996s, 1027.94/s  (1.006s, 1017.57/s)  LR: 3.669e-04  Data: 0.011 (0.013)
Train: 177 [1000/1251 ( 80%)]  Loss: 3.631 (3.53)  Time: 1.062s,  963.96/s  (1.006s, 1017.58/s)  LR: 3.669e-04  Data: 0.011 (0.013)
Train: 177 [1050/1251 ( 84%)]  Loss: 3.420 (3.53)  Time: 1.006s, 1017.61/s  (1.006s, 1017.53/s)  LR: 3.669e-04  Data: 0.010 (0.013)
Train: 177 [1100/1251 ( 88%)]  Loss: 3.208 (3.51)  Time: 0.997s, 1027.54/s  (1.006s, 1017.93/s)  LR: 3.669e-04  Data: 0.011 (0.013)
Train: 177 [1150/1251 ( 92%)]  Loss: 3.859 (3.53)  Time: 1.015s, 1008.49/s  (1.006s, 1018.00/s)  LR: 3.669e-04  Data: 0.011 (0.013)
Train: 177 [1200/1251 ( 96%)]  Loss: 3.400 (3.52)  Time: 0.996s, 1028.54/s  (1.006s, 1018.25/s)  LR: 3.669e-04  Data: 0.012 (0.013)
Train: 177 [1250/1251 (100%)]  Loss: 3.461 (3.52)  Time: 0.981s, 1044.00/s  (1.005s, 1018.49/s)  LR: 3.669e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.615 (1.615)  Loss:  0.7835 (0.7835)  Acc@1: 90.0391 (90.0391)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.245 (0.570)  Loss:  0.8964 (1.2750)  Acc@1: 84.4340 (75.4820)  Acc@5: 96.6981 (93.1080)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-174.pth.tar', 76.08599998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-175.pth.tar', 75.9339999609375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-176.pth.tar', 75.9219999267578)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-171.pth.tar', 75.8079999584961)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-172.pth.tar', 75.71200008300781)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-173.pth.tar', 75.59200008544921)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-166.pth.tar', 75.49799998291016)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-177.pth.tar', 75.48200009033204)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-165.pth.tar', 75.43400009277343)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-170.pth.tar', 75.43200000732422)

Train: 178 [   0/1251 (  0%)]  Loss: 3.644 (3.64)  Time: 2.583s,  396.41/s  (2.583s,  396.41/s)  LR: 3.619e-04  Data: 1.622 (1.622)
Train: 178 [  50/1251 (  4%)]  Loss: 3.443 (3.54)  Time: 1.031s,  992.73/s  (1.059s,  966.83/s)  LR: 3.619e-04  Data: 0.011 (0.043)
Train: 178 [ 100/1251 (  8%)]  Loss: 3.568 (3.55)  Time: 0.996s, 1028.50/s  (1.032s,  992.00/s)  LR: 3.619e-04  Data: 0.012 (0.027)
Train: 178 [ 150/1251 ( 12%)]  Loss: 3.341 (3.50)  Time: 0.998s, 1025.93/s  (1.021s, 1002.64/s)  LR: 3.619e-04  Data: 0.010 (0.022)
Train: 178 [ 200/1251 ( 16%)]  Loss: 3.585 (3.52)  Time: 0.996s, 1028.26/s  (1.019s, 1005.07/s)  LR: 3.619e-04  Data: 0.011 (0.019)
Train: 178 [ 250/1251 ( 20%)]  Loss: 3.377 (3.49)  Time: 0.999s, 1025.26/s  (1.015s, 1009.05/s)  LR: 3.619e-04  Data: 0.012 (0.018)
Train: 178 [ 300/1251 ( 24%)]  Loss: 3.651 (3.52)  Time: 0.999s, 1025.00/s  (1.012s, 1011.76/s)  LR: 3.619e-04  Data: 0.011 (0.017)
Train: 178 [ 350/1251 ( 28%)]  Loss: 3.660 (3.53)  Time: 1.006s, 1018.08/s  (1.011s, 1013.22/s)  LR: 3.619e-04  Data: 0.013 (0.016)
Train: 178 [ 400/1251 ( 32%)]  Loss: 3.053 (3.48)  Time: 0.996s, 1028.12/s  (1.009s, 1014.50/s)  LR: 3.619e-04  Data: 0.011 (0.015)
Train: 178 [ 450/1251 ( 36%)]  Loss: 3.485 (3.48)  Time: 0.995s, 1028.67/s  (1.009s, 1015.19/s)  LR: 3.619e-04  Data: 0.011 (0.015)
Train: 178 [ 500/1251 ( 40%)]  Loss: 3.580 (3.49)  Time: 0.996s, 1028.50/s  (1.008s, 1015.55/s)  LR: 3.619e-04  Data: 0.012 (0.014)
Train: 178 [ 550/1251 ( 44%)]  Loss: 3.687 (3.51)  Time: 1.018s, 1005.93/s  (1.008s, 1015.82/s)  LR: 3.619e-04  Data: 0.011 (0.014)
Train: 178 [ 600/1251 ( 48%)]  Loss: 3.046 (3.47)  Time: 0.993s, 1031.03/s  (1.008s, 1016.07/s)  LR: 3.619e-04  Data: 0.010 (0.014)
Train: 178 [ 650/1251 ( 52%)]  Loss: 3.267 (3.46)  Time: 1.002s, 1021.66/s  (1.008s, 1016.32/s)  LR: 3.619e-04  Data: 0.010 (0.014)
Train: 178 [ 700/1251 ( 56%)]  Loss: 3.502 (3.46)  Time: 0.995s, 1029.18/s  (1.007s, 1016.75/s)  LR: 3.619e-04  Data: 0.011 (0.013)
Train: 178 [ 750/1251 ( 60%)]  Loss: 3.335 (3.45)  Time: 0.993s, 1031.49/s  (1.007s, 1017.22/s)  LR: 3.619e-04  Data: 0.010 (0.013)
Train: 178 [ 800/1251 ( 64%)]  Loss: 3.564 (3.46)  Time: 0.996s, 1027.91/s  (1.006s, 1017.72/s)  LR: 3.619e-04  Data: 0.011 (0.013)
Train: 178 [ 850/1251 ( 68%)]  Loss: 3.361 (3.45)  Time: 1.049s,  975.74/s  (1.006s, 1018.05/s)  LR: 3.619e-04  Data: 0.012 (0.013)
Train: 178 [ 900/1251 ( 72%)]  Loss: 3.199 (3.44)  Time: 0.996s, 1027.99/s  (1.006s, 1018.32/s)  LR: 3.619e-04  Data: 0.011 (0.013)
Train: 178 [ 950/1251 ( 76%)]  Loss: 3.014 (3.42)  Time: 1.009s, 1015.16/s  (1.005s, 1018.56/s)  LR: 3.619e-04  Data: 0.010 (0.013)
Train: 178 [1000/1251 ( 80%)]  Loss: 3.614 (3.43)  Time: 1.000s, 1023.99/s  (1.005s, 1018.87/s)  LR: 3.619e-04  Data: 0.010 (0.013)
Train: 178 [1050/1251 ( 84%)]  Loss: 3.391 (3.43)  Time: 1.061s,  965.04/s  (1.006s, 1018.10/s)  LR: 3.619e-04  Data: 0.011 (0.013)
Train: 178 [1100/1251 ( 88%)]  Loss: 3.719 (3.44)  Time: 0.997s, 1027.23/s  (1.006s, 1017.79/s)  LR: 3.619e-04  Data: 0.012 (0.012)
Train: 178 [1150/1251 ( 92%)]  Loss: 3.494 (3.44)  Time: 0.998s, 1025.92/s  (1.006s, 1017.97/s)  LR: 3.619e-04  Data: 0.011 (0.012)
Train: 178 [1200/1251 ( 96%)]  Loss: 3.333 (3.44)  Time: 0.994s, 1029.98/s  (1.006s, 1018.37/s)  LR: 3.619e-04  Data: 0.011 (0.012)
Train: 178 [1250/1251 (100%)]  Loss: 3.688 (3.45)  Time: 0.984s, 1041.16/s  (1.006s, 1018.31/s)  LR: 3.619e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.604 (1.604)  Loss:  0.7540 (0.7540)  Acc@1: 90.1367 (90.1367)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.245 (0.575)  Loss:  0.8368 (1.2855)  Acc@1: 85.2594 (75.6620)  Acc@5: 96.8160 (93.1120)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-174.pth.tar', 76.08599998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-175.pth.tar', 75.9339999609375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-176.pth.tar', 75.9219999267578)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-171.pth.tar', 75.8079999584961)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-172.pth.tar', 75.71200008300781)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-178.pth.tar', 75.66200006103516)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-173.pth.tar', 75.59200008544921)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-166.pth.tar', 75.49799998291016)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-177.pth.tar', 75.48200009033204)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-165.pth.tar', 75.43400009277343)

Train: 179 [   0/1251 (  0%)]  Loss: 3.446 (3.45)  Time: 2.439s,  419.79/s  (2.439s,  419.79/s)  LR: 3.570e-04  Data: 1.462 (1.462)
Train: 179 [  50/1251 (  4%)]  Loss: 3.333 (3.39)  Time: 0.995s, 1028.79/s  (1.028s,  996.38/s)  LR: 3.570e-04  Data: 0.011 (0.040)
Train: 179 [ 100/1251 (  8%)]  Loss: 3.418 (3.40)  Time: 0.994s, 1030.21/s  (1.019s, 1005.09/s)  LR: 3.570e-04  Data: 0.011 (0.026)
Train: 179 [ 150/1251 ( 12%)]  Loss: 3.152 (3.34)  Time: 0.997s, 1027.27/s  (1.022s, 1002.40/s)  LR: 3.570e-04  Data: 0.011 (0.021)
Train: 179 [ 200/1251 ( 16%)]  Loss: 3.490 (3.37)  Time: 0.994s, 1029.85/s  (1.017s, 1006.69/s)  LR: 3.570e-04  Data: 0.011 (0.019)
Train: 179 [ 250/1251 ( 20%)]  Loss: 3.297 (3.36)  Time: 0.993s, 1030.74/s  (1.013s, 1010.56/s)  LR: 3.570e-04  Data: 0.011 (0.017)
Train: 179 [ 300/1251 ( 24%)]  Loss: 3.727 (3.41)  Time: 0.995s, 1028.86/s  (1.011s, 1012.86/s)  LR: 3.570e-04  Data: 0.011 (0.016)
Train: 179 [ 350/1251 ( 28%)]  Loss: 3.539 (3.43)  Time: 1.003s, 1021.05/s  (1.010s, 1014.04/s)  LR: 3.570e-04  Data: 0.011 (0.016)
Train: 179 [ 400/1251 ( 32%)]  Loss: 3.386 (3.42)  Time: 0.994s, 1029.99/s  (1.009s, 1015.08/s)  LR: 3.570e-04  Data: 0.011 (0.015)
Train: 179 [ 450/1251 ( 36%)]  Loss: 3.499 (3.43)  Time: 1.056s,  969.70/s  (1.008s, 1015.57/s)  LR: 3.570e-04  Data: 0.014 (0.015)
Train: 179 [ 500/1251 ( 40%)]  Loss: 3.401 (3.43)  Time: 1.004s, 1019.47/s  (1.008s, 1016.13/s)  LR: 3.570e-04  Data: 0.011 (0.014)
Train: 179 [ 550/1251 ( 44%)]  Loss: 3.523 (3.43)  Time: 0.994s, 1030.20/s  (1.007s, 1016.53/s)  LR: 3.570e-04  Data: 0.010 (0.014)
Train: 179 [ 600/1251 ( 48%)]  Loss: 3.374 (3.43)  Time: 1.000s, 1023.78/s  (1.007s, 1017.14/s)  LR: 3.570e-04  Data: 0.010 (0.014)
Train: 179 [ 650/1251 ( 52%)]  Loss: 3.269 (3.42)  Time: 0.996s, 1028.13/s  (1.007s, 1017.26/s)  LR: 3.570e-04  Data: 0.011 (0.014)
Train: 179 [ 700/1251 ( 56%)]  Loss: 3.556 (3.43)  Time: 0.994s, 1029.92/s  (1.006s, 1017.99/s)  LR: 3.570e-04  Data: 0.011 (0.013)
Train: 179 [ 750/1251 ( 60%)]  Loss: 3.494 (3.43)  Time: 0.993s, 1030.72/s  (1.005s, 1018.53/s)  LR: 3.570e-04  Data: 0.011 (0.013)
Train: 179 [ 800/1251 ( 64%)]  Loss: 3.477 (3.43)  Time: 1.003s, 1020.83/s  (1.005s, 1019.05/s)  LR: 3.570e-04  Data: 0.011 (0.013)
Train: 179 [ 850/1251 ( 68%)]  Loss: 3.544 (3.44)  Time: 0.998s, 1026.07/s  (1.005s, 1019.20/s)  LR: 3.570e-04  Data: 0.011 (0.013)
Train: 179 [ 900/1251 ( 72%)]  Loss: 3.703 (3.45)  Time: 1.006s, 1018.07/s  (1.004s, 1019.46/s)  LR: 3.570e-04  Data: 0.012 (0.013)
Train: 179 [ 950/1251 ( 76%)]  Loss: 3.697 (3.47)  Time: 1.002s, 1022.08/s  (1.004s, 1019.62/s)  LR: 3.570e-04  Data: 0.013 (0.013)
Train: 179 [1000/1251 ( 80%)]  Loss: 3.397 (3.46)  Time: 0.991s, 1033.27/s  (1.004s, 1019.72/s)  LR: 3.570e-04  Data: 0.010 (0.013)
Train: 179 [1050/1251 ( 84%)]  Loss: 3.236 (3.45)  Time: 0.994s, 1030.31/s  (1.004s, 1019.76/s)  LR: 3.570e-04  Data: 0.010 (0.013)
Train: 179 [1100/1251 ( 88%)]  Loss: 3.516 (3.46)  Time: 0.998s, 1025.55/s  (1.004s, 1019.57/s)  LR: 3.570e-04  Data: 0.011 (0.013)
Train: 179 [1150/1251 ( 92%)]  Loss: 3.554 (3.46)  Time: 1.001s, 1022.94/s  (1.004s, 1019.73/s)  LR: 3.570e-04  Data: 0.011 (0.012)
Train: 179 [1200/1251 ( 96%)]  Loss: 3.167 (3.45)  Time: 0.995s, 1029.53/s  (1.004s, 1019.92/s)  LR: 3.570e-04  Data: 0.011 (0.012)
Train: 179 [1250/1251 (100%)]  Loss: 3.593 (3.45)  Time: 1.018s, 1005.72/s  (1.004s, 1019.63/s)  LR: 3.570e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.669 (1.669)  Loss:  0.7268 (0.7268)  Acc@1: 91.1133 (91.1133)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.245 (0.569)  Loss:  0.8179 (1.2446)  Acc@1: 85.4953 (76.3320)  Acc@5: 96.6981 (93.2380)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-179.pth.tar', 76.33200016357422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-174.pth.tar', 76.08599998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-175.pth.tar', 75.9339999609375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-176.pth.tar', 75.9219999267578)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-171.pth.tar', 75.8079999584961)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-172.pth.tar', 75.71200008300781)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-178.pth.tar', 75.66200006103516)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-173.pth.tar', 75.59200008544921)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-166.pth.tar', 75.49799998291016)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-177.pth.tar', 75.48200009033204)

Train: 180 [   0/1251 (  0%)]  Loss: 3.360 (3.36)  Time: 2.442s,  419.30/s  (2.442s,  419.30/s)  LR: 3.520e-04  Data: 1.454 (1.454)
Train: 180 [  50/1251 (  4%)]  Loss: 3.534 (3.45)  Time: 0.999s, 1024.60/s  (1.032s,  992.41/s)  LR: 3.520e-04  Data: 0.012 (0.040)
Train: 180 [ 100/1251 (  8%)]  Loss: 3.708 (3.53)  Time: 1.031s,  992.82/s  (1.024s, 1000.44/s)  LR: 3.520e-04  Data: 0.011 (0.026)
Train: 180 [ 150/1251 ( 12%)]  Loss: 3.320 (3.48)  Time: 0.998s, 1025.65/s  (1.020s, 1003.81/s)  LR: 3.520e-04  Data: 0.011 (0.021)
Train: 180 [ 200/1251 ( 16%)]  Loss: 3.756 (3.54)  Time: 0.994s, 1029.71/s  (1.016s, 1007.81/s)  LR: 3.520e-04  Data: 0.011 (0.019)
Train: 180 [ 250/1251 ( 20%)]  Loss: 3.582 (3.54)  Time: 0.994s, 1029.69/s  (1.012s, 1011.48/s)  LR: 3.520e-04  Data: 0.010 (0.017)
Train: 180 [ 300/1251 ( 24%)]  Loss: 3.523 (3.54)  Time: 1.005s, 1019.38/s  (1.011s, 1013.26/s)  LR: 3.520e-04  Data: 0.010 (0.016)
Train: 180 [ 350/1251 ( 28%)]  Loss: 3.180 (3.50)  Time: 0.998s, 1025.71/s  (1.010s, 1014.32/s)  LR: 3.520e-04  Data: 0.011 (0.015)
Train: 180 [ 400/1251 ( 32%)]  Loss: 3.577 (3.50)  Time: 0.996s, 1027.80/s  (1.009s, 1014.89/s)  LR: 3.520e-04  Data: 0.011 (0.015)
Train: 180 [ 450/1251 ( 36%)]  Loss: 3.664 (3.52)  Time: 0.996s, 1027.62/s  (1.008s, 1015.95/s)  LR: 3.520e-04  Data: 0.011 (0.014)
Train: 180 [ 500/1251 ( 40%)]  Loss: 3.348 (3.50)  Time: 0.995s, 1029.15/s  (1.007s, 1016.98/s)  LR: 3.520e-04  Data: 0.012 (0.014)
Train: 180 [ 550/1251 ( 44%)]  Loss: 3.353 (3.49)  Time: 0.994s, 1030.29/s  (1.006s, 1017.56/s)  LR: 3.520e-04  Data: 0.010 (0.014)
Train: 180 [ 600/1251 ( 48%)]  Loss: 3.565 (3.50)  Time: 0.995s, 1028.79/s  (1.006s, 1017.81/s)  LR: 3.520e-04  Data: 0.011 (0.014)
Train: 180 [ 650/1251 ( 52%)]  Loss: 3.597 (3.50)  Time: 0.998s, 1025.78/s  (1.006s, 1018.13/s)  LR: 3.520e-04  Data: 0.011 (0.013)
Train: 180 [ 700/1251 ( 56%)]  Loss: 3.405 (3.50)  Time: 0.994s, 1030.60/s  (1.005s, 1018.43/s)  LR: 3.520e-04  Data: 0.014 (0.013)
Train: 180 [ 750/1251 ( 60%)]  Loss: 3.065 (3.47)  Time: 0.995s, 1028.86/s  (1.006s, 1017.71/s)  LR: 3.520e-04  Data: 0.011 (0.013)
Train: 180 [ 800/1251 ( 64%)]  Loss: 3.399 (3.47)  Time: 0.995s, 1029.09/s  (1.006s, 1018.05/s)  LR: 3.520e-04  Data: 0.010 (0.013)
Train: 180 [ 850/1251 ( 68%)]  Loss: 3.139 (3.45)  Time: 0.993s, 1030.92/s  (1.005s, 1018.49/s)  LR: 3.520e-04  Data: 0.011 (0.013)
Train: 180 [ 900/1251 ( 72%)]  Loss: 3.552 (3.45)  Time: 1.014s, 1010.23/s  (1.005s, 1018.70/s)  LR: 3.520e-04  Data: 0.010 (0.013)
Train: 180 [ 950/1251 ( 76%)]  Loss: 3.537 (3.46)  Time: 1.001s, 1023.04/s  (1.005s, 1018.58/s)  LR: 3.520e-04  Data: 0.011 (0.013)
Train: 180 [1000/1251 ( 80%)]  Loss: 3.267 (3.45)  Time: 0.996s, 1027.72/s  (1.005s, 1018.96/s)  LR: 3.520e-04  Data: 0.010 (0.013)
Train: 180 [1050/1251 ( 84%)]  Loss: 3.735 (3.46)  Time: 1.002s, 1021.74/s  (1.006s, 1018.33/s)  LR: 3.520e-04  Data: 0.011 (0.013)
Train: 180 [1100/1251 ( 88%)]  Loss: 3.661 (3.47)  Time: 0.998s, 1026.43/s  (1.006s, 1017.59/s)  LR: 3.520e-04  Data: 0.011 (0.013)
Train: 180 [1150/1251 ( 92%)]  Loss: 3.659 (3.48)  Time: 0.996s, 1027.79/s  (1.006s, 1017.52/s)  LR: 3.520e-04  Data: 0.011 (0.012)
Train: 180 [1200/1251 ( 96%)]  Loss: 3.589 (3.48)  Time: 1.065s,  961.26/s  (1.006s, 1017.63/s)  LR: 3.520e-04  Data: 0.011 (0.012)
Train: 180 [1250/1251 (100%)]  Loss: 3.246 (3.47)  Time: 0.985s, 1039.91/s  (1.007s, 1017.05/s)  LR: 3.520e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.659 (1.659)  Loss:  0.7587 (0.7587)  Acc@1: 89.8438 (89.8438)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.245 (0.573)  Loss:  0.8298 (1.2424)  Acc@1: 85.6132 (76.1540)  Acc@5: 96.5802 (93.2480)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-179.pth.tar', 76.33200016357422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-180.pth.tar', 76.15399995605469)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-174.pth.tar', 76.08599998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-175.pth.tar', 75.9339999609375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-176.pth.tar', 75.9219999267578)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-171.pth.tar', 75.8079999584961)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-172.pth.tar', 75.71200008300781)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-178.pth.tar', 75.66200006103516)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-173.pth.tar', 75.59200008544921)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-166.pth.tar', 75.49799998291016)

Train: 181 [   0/1251 (  0%)]  Loss: 3.395 (3.39)  Time: 2.498s,  409.96/s  (2.498s,  409.96/s)  LR: 3.471e-04  Data: 1.533 (1.533)
Train: 181 [  50/1251 (  4%)]  Loss: 3.569 (3.48)  Time: 1.063s,  963.05/s  (1.057s,  968.91/s)  LR: 3.471e-04  Data: 0.011 (0.041)
Train: 181 [ 100/1251 (  8%)]  Loss: 3.366 (3.44)  Time: 0.999s, 1024.87/s  (1.033s,  990.95/s)  LR: 3.471e-04  Data: 0.010 (0.027)
Train: 181 [ 150/1251 ( 12%)]  Loss: 3.694 (3.51)  Time: 0.996s, 1027.61/s  (1.023s, 1000.97/s)  LR: 3.471e-04  Data: 0.011 (0.021)
Train: 181 [ 200/1251 ( 16%)]  Loss: 3.394 (3.48)  Time: 1.051s,  973.94/s  (1.020s, 1003.64/s)  LR: 3.471e-04  Data: 0.010 (0.019)
Train: 181 [ 250/1251 ( 20%)]  Loss: 3.456 (3.48)  Time: 0.999s, 1025.39/s  (1.018s, 1006.34/s)  LR: 3.471e-04  Data: 0.011 (0.017)
Train: 181 [ 300/1251 ( 24%)]  Loss: 3.449 (3.47)  Time: 0.995s, 1028.88/s  (1.015s, 1008.49/s)  LR: 3.471e-04  Data: 0.011 (0.016)
Train: 181 [ 350/1251 ( 28%)]  Loss: 3.546 (3.48)  Time: 0.996s, 1027.88/s  (1.014s, 1010.13/s)  LR: 3.471e-04  Data: 0.011 (0.016)
Train: 181 [ 400/1251 ( 32%)]  Loss: 3.563 (3.49)  Time: 1.005s, 1019.17/s  (1.012s, 1011.91/s)  LR: 3.471e-04  Data: 0.015 (0.015)
Train: 181 [ 450/1251 ( 36%)]  Loss: 3.342 (3.48)  Time: 0.997s, 1026.71/s  (1.011s, 1013.01/s)  LR: 3.471e-04  Data: 0.011 (0.015)
Train: 181 [ 500/1251 ( 40%)]  Loss: 3.698 (3.50)  Time: 0.994s, 1030.21/s  (1.010s, 1014.30/s)  LR: 3.471e-04  Data: 0.010 (0.014)
Train: 181 [ 550/1251 ( 44%)]  Loss: 3.565 (3.50)  Time: 0.998s, 1025.58/s  (1.009s, 1015.28/s)  LR: 3.471e-04  Data: 0.011 (0.014)
Train: 181 [ 600/1251 ( 48%)]  Loss: 3.233 (3.48)  Time: 0.996s, 1028.14/s  (1.008s, 1016.10/s)  LR: 3.471e-04  Data: 0.012 (0.014)
Train: 181 [ 650/1251 ( 52%)]  Loss: 3.616 (3.49)  Time: 0.999s, 1025.52/s  (1.007s, 1016.69/s)  LR: 3.471e-04  Data: 0.011 (0.013)
Train: 181 [ 700/1251 ( 56%)]  Loss: 3.305 (3.48)  Time: 0.992s, 1032.45/s  (1.007s, 1017.15/s)  LR: 3.471e-04  Data: 0.010 (0.013)
Train: 181 [ 750/1251 ( 60%)]  Loss: 3.267 (3.47)  Time: 1.017s, 1006.44/s  (1.007s, 1017.38/s)  LR: 3.471e-04  Data: 0.012 (0.013)
Train: 181 [ 800/1251 ( 64%)]  Loss: 3.826 (3.49)  Time: 1.031s,  993.46/s  (1.006s, 1017.79/s)  LR: 3.471e-04  Data: 0.010 (0.013)
Train: 181 [ 850/1251 ( 68%)]  Loss: 3.521 (3.49)  Time: 0.994s, 1030.34/s  (1.006s, 1018.06/s)  LR: 3.471e-04  Data: 0.011 (0.013)
Train: 181 [ 900/1251 ( 72%)]  Loss: 3.383 (3.48)  Time: 0.997s, 1027.25/s  (1.006s, 1018.39/s)  LR: 3.471e-04  Data: 0.011 (0.013)
Train: 181 [ 950/1251 ( 76%)]  Loss: 3.373 (3.48)  Time: 1.003s, 1020.92/s  (1.005s, 1018.73/s)  LR: 3.471e-04  Data: 0.011 (0.013)
Train: 181 [1000/1251 ( 80%)]  Loss: 3.383 (3.47)  Time: 0.994s, 1030.05/s  (1.005s, 1019.09/s)  LR: 3.471e-04  Data: 0.011 (0.013)
Train: 181 [1050/1251 ( 84%)]  Loss: 3.314 (3.47)  Time: 0.995s, 1029.06/s  (1.004s, 1019.51/s)  LR: 3.471e-04  Data: 0.011 (0.013)
Train: 181 [1100/1251 ( 88%)]  Loss: 3.448 (3.47)  Time: 0.995s, 1028.97/s  (1.004s, 1019.77/s)  LR: 3.471e-04  Data: 0.010 (0.012)
Train: 181 [1150/1251 ( 92%)]  Loss: 3.751 (3.48)  Time: 0.996s, 1027.68/s  (1.004s, 1019.95/s)  LR: 3.471e-04  Data: 0.011 (0.012)
Train: 181 [1200/1251 ( 96%)]  Loss: 3.191 (3.47)  Time: 0.999s, 1024.70/s  (1.004s, 1020.16/s)  LR: 3.471e-04  Data: 0.011 (0.012)
Train: 181 [1250/1251 (100%)]  Loss: 3.392 (3.46)  Time: 0.984s, 1040.65/s  (1.004s, 1020.29/s)  LR: 3.471e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.650 (1.650)  Loss:  0.7362 (0.7362)  Acc@1: 90.5273 (90.5273)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.245 (0.566)  Loss:  0.7972 (1.2091)  Acc@1: 84.6698 (76.0640)  Acc@5: 96.8160 (93.3180)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-179.pth.tar', 76.33200016357422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-180.pth.tar', 76.15399995605469)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-174.pth.tar', 76.08599998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-181.pth.tar', 76.06400019287109)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-175.pth.tar', 75.9339999609375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-176.pth.tar', 75.9219999267578)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-171.pth.tar', 75.8079999584961)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-172.pth.tar', 75.71200008300781)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-178.pth.tar', 75.66200006103516)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-173.pth.tar', 75.59200008544921)

Train: 182 [   0/1251 (  0%)]  Loss: 3.095 (3.09)  Time: 2.602s,  393.49/s  (2.602s,  393.49/s)  LR: 3.422e-04  Data: 1.586 (1.586)
Train: 182 [  50/1251 (  4%)]  Loss: 3.756 (3.43)  Time: 1.032s,  991.97/s  (1.034s,  990.44/s)  LR: 3.422e-04  Data: 0.011 (0.043)
Train: 182 [ 100/1251 (  8%)]  Loss: 3.194 (3.35)  Time: 0.994s, 1030.55/s  (1.017s, 1007.07/s)  LR: 3.422e-04  Data: 0.011 (0.027)
Train: 182 [ 150/1251 ( 12%)]  Loss: 3.409 (3.36)  Time: 1.019s, 1004.52/s  (1.012s, 1012.31/s)  LR: 3.422e-04  Data: 0.012 (0.022)
Train: 182 [ 200/1251 ( 16%)]  Loss: 3.312 (3.35)  Time: 0.991s, 1033.27/s  (1.008s, 1015.73/s)  LR: 3.422e-04  Data: 0.010 (0.019)
Train: 182 [ 250/1251 ( 20%)]  Loss: 3.818 (3.43)  Time: 0.998s, 1026.30/s  (1.006s, 1017.65/s)  LR: 3.422e-04  Data: 0.012 (0.018)
Train: 182 [ 300/1251 ( 24%)]  Loss: 3.579 (3.45)  Time: 0.997s, 1027.23/s  (1.005s, 1018.95/s)  LR: 3.422e-04  Data: 0.012 (0.017)
Train: 182 [ 350/1251 ( 28%)]  Loss: 3.726 (3.49)  Time: 0.994s, 1030.35/s  (1.004s, 1019.89/s)  LR: 3.422e-04  Data: 0.011 (0.016)
Train: 182 [ 400/1251 ( 32%)]  Loss: 3.349 (3.47)  Time: 0.999s, 1025.07/s  (1.003s, 1020.51/s)  LR: 3.422e-04  Data: 0.011 (0.015)
Train: 182 [ 450/1251 ( 36%)]  Loss: 2.860 (3.41)  Time: 0.999s, 1024.61/s  (1.003s, 1021.14/s)  LR: 3.422e-04  Data: 0.011 (0.015)
Train: 182 [ 500/1251 ( 40%)]  Loss: 3.312 (3.40)  Time: 1.059s,  967.39/s  (1.004s, 1020.16/s)  LR: 3.422e-04  Data: 0.011 (0.014)
Train: 182 [ 550/1251 ( 44%)]  Loss: 3.851 (3.44)  Time: 0.996s, 1028.17/s  (1.004s, 1019.47/s)  LR: 3.422e-04  Data: 0.011 (0.014)
Train: 182 [ 600/1251 ( 48%)]  Loss: 3.456 (3.44)  Time: 1.000s, 1023.61/s  (1.005s, 1019.39/s)  LR: 3.422e-04  Data: 0.013 (0.014)
Train: 182 [ 650/1251 ( 52%)]  Loss: 3.283 (3.43)  Time: 0.998s, 1025.88/s  (1.006s, 1017.40/s)  LR: 3.422e-04  Data: 0.011 (0.014)
Train: 182 [ 700/1251 ( 56%)]  Loss: 3.249 (3.42)  Time: 1.006s, 1017.56/s  (1.006s, 1017.76/s)  LR: 3.422e-04  Data: 0.011 (0.014)
Train: 182 [ 750/1251 ( 60%)]  Loss: 3.739 (3.44)  Time: 0.999s, 1025.13/s  (1.006s, 1017.83/s)  LR: 3.422e-04  Data: 0.011 (0.013)
Train: 182 [ 800/1251 ( 64%)]  Loss: 3.277 (3.43)  Time: 0.998s, 1026.09/s  (1.006s, 1018.02/s)  LR: 3.422e-04  Data: 0.011 (0.013)
Train: 182 [ 850/1251 ( 68%)]  Loss: 3.490 (3.43)  Time: 0.998s, 1026.50/s  (1.006s, 1018.18/s)  LR: 3.422e-04  Data: 0.010 (0.013)
Train: 182 [ 900/1251 ( 72%)]  Loss: 3.451 (3.43)  Time: 1.006s, 1017.53/s  (1.005s, 1018.51/s)  LR: 3.422e-04  Data: 0.011 (0.013)
Train: 182 [ 950/1251 ( 76%)]  Loss: 3.658 (3.44)  Time: 0.995s, 1029.27/s  (1.005s, 1018.88/s)  LR: 3.422e-04  Data: 0.012 (0.013)
Train: 182 [1000/1251 ( 80%)]  Loss: 3.642 (3.45)  Time: 0.997s, 1027.17/s  (1.006s, 1018.39/s)  LR: 3.422e-04  Data: 0.012 (0.013)
Train: 182 [1050/1251 ( 84%)]  Loss: 3.353 (3.45)  Time: 1.047s,  978.35/s  (1.006s, 1017.89/s)  LR: 3.422e-04  Data: 0.010 (0.013)
Train: 182 [1100/1251 ( 88%)]  Loss: 3.343 (3.44)  Time: 1.025s,  998.84/s  (1.006s, 1018.11/s)  LR: 3.422e-04  Data: 0.011 (0.013)
Train: 182 [1150/1251 ( 92%)]  Loss: 3.125 (3.43)  Time: 0.995s, 1029.59/s  (1.006s, 1018.26/s)  LR: 3.422e-04  Data: 0.011 (0.013)
Train: 182 [1200/1251 ( 96%)]  Loss: 3.276 (3.42)  Time: 0.997s, 1027.15/s  (1.005s, 1018.50/s)  LR: 3.422e-04  Data: 0.011 (0.012)
Train: 182 [1250/1251 (100%)]  Loss: 3.248 (3.42)  Time: 0.985s, 1039.15/s  (1.005s, 1018.73/s)  LR: 3.422e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.658 (1.658)  Loss:  0.7305 (0.7305)  Acc@1: 90.9180 (90.9180)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.245 (0.565)  Loss:  0.8497 (1.2531)  Acc@1: 85.4953 (76.0940)  Acc@5: 96.3443 (93.3460)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-179.pth.tar', 76.33200016357422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-180.pth.tar', 76.15399995605469)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-182.pth.tar', 76.09400003417969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-174.pth.tar', 76.08599998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-181.pth.tar', 76.06400019287109)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-175.pth.tar', 75.9339999609375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-176.pth.tar', 75.9219999267578)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-171.pth.tar', 75.8079999584961)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-172.pth.tar', 75.71200008300781)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-178.pth.tar', 75.66200006103516)

Train: 183 [   0/1251 (  0%)]  Loss: 3.365 (3.36)  Time: 4.318s,  237.14/s  (4.318s,  237.14/s)  LR: 3.373e-04  Data: 3.291 (3.291)
Train: 183 [  50/1251 (  4%)]  Loss: 3.277 (3.32)  Time: 0.994s, 1030.61/s  (1.068s,  958.77/s)  LR: 3.373e-04  Data: 0.011 (0.076)
Train: 183 [ 100/1251 (  8%)]  Loss: 3.597 (3.41)  Time: 1.022s, 1001.65/s  (1.037s,  987.24/s)  LR: 3.373e-04  Data: 0.011 (0.044)
Train: 183 [ 150/1251 ( 12%)]  Loss: 3.645 (3.47)  Time: 0.997s, 1026.79/s  (1.025s,  998.56/s)  LR: 3.373e-04  Data: 0.011 (0.033)
Train: 183 [ 200/1251 ( 16%)]  Loss: 3.414 (3.46)  Time: 0.996s, 1028.39/s  (1.019s, 1004.47/s)  LR: 3.373e-04  Data: 0.011 (0.028)
Train: 183 [ 250/1251 ( 20%)]  Loss: 3.498 (3.47)  Time: 1.031s,  993.57/s  (1.019s, 1004.66/s)  LR: 3.373e-04  Data: 0.011 (0.024)
Train: 183 [ 300/1251 ( 24%)]  Loss: 3.420 (3.46)  Time: 1.004s, 1020.05/s  (1.017s, 1007.23/s)  LR: 3.373e-04  Data: 0.011 (0.022)
Train: 183 [ 350/1251 ( 28%)]  Loss: 3.687 (3.49)  Time: 0.997s, 1027.55/s  (1.017s, 1006.54/s)  LR: 3.373e-04  Data: 0.011 (0.021)
Train: 183 [ 400/1251 ( 32%)]  Loss: 3.075 (3.44)  Time: 0.997s, 1027.08/s  (1.015s, 1008.75/s)  LR: 3.373e-04  Data: 0.012 (0.019)
Train: 183 [ 450/1251 ( 36%)]  Loss: 3.538 (3.45)  Time: 0.997s, 1026.81/s  (1.014s, 1010.01/s)  LR: 3.373e-04  Data: 0.011 (0.019)
Train: 183 [ 500/1251 ( 40%)]  Loss: 3.401 (3.45)  Time: 1.000s, 1023.64/s  (1.012s, 1011.57/s)  LR: 3.373e-04  Data: 0.011 (0.018)
Train: 183 [ 550/1251 ( 44%)]  Loss: 3.580 (3.46)  Time: 0.990s, 1034.24/s  (1.011s, 1012.75/s)  LR: 3.373e-04  Data: 0.010 (0.017)
Train: 183 [ 600/1251 ( 48%)]  Loss: 3.037 (3.43)  Time: 0.998s, 1025.58/s  (1.011s, 1012.42/s)  LR: 3.373e-04  Data: 0.011 (0.017)
Train: 183 [ 650/1251 ( 52%)]  Loss: 3.487 (3.43)  Time: 0.999s, 1024.73/s  (1.011s, 1013.08/s)  LR: 3.373e-04  Data: 0.011 (0.016)
Train: 183 [ 700/1251 ( 56%)]  Loss: 3.669 (3.45)  Time: 1.001s, 1022.75/s  (1.010s, 1013.90/s)  LR: 3.373e-04  Data: 0.010 (0.016)
Train: 183 [ 750/1251 ( 60%)]  Loss: 3.531 (3.45)  Time: 0.995s, 1028.85/s  (1.009s, 1014.60/s)  LR: 3.373e-04  Data: 0.011 (0.016)
Train: 183 [ 800/1251 ( 64%)]  Loss: 2.953 (3.42)  Time: 0.996s, 1027.79/s  (1.009s, 1015.06/s)  LR: 3.373e-04  Data: 0.011 (0.015)
Train: 183 [ 850/1251 ( 68%)]  Loss: 3.761 (3.44)  Time: 0.997s, 1026.96/s  (1.008s, 1015.68/s)  LR: 3.373e-04  Data: 0.011 (0.015)
Train: 183 [ 900/1251 ( 72%)]  Loss: 3.598 (3.45)  Time: 0.994s, 1029.88/s  (1.008s, 1016.10/s)  LR: 3.373e-04  Data: 0.011 (0.015)
Train: 183 [ 950/1251 ( 76%)]  Loss: 3.626 (3.46)  Time: 1.001s, 1022.58/s  (1.008s, 1016.34/s)  LR: 3.373e-04  Data: 0.010 (0.015)
Train: 183 [1000/1251 ( 80%)]  Loss: 2.953 (3.43)  Time: 0.998s, 1026.39/s  (1.007s, 1016.51/s)  LR: 3.373e-04  Data: 0.011 (0.014)
Train: 183 [1050/1251 ( 84%)]  Loss: 3.406 (3.43)  Time: 1.000s, 1024.37/s  (1.008s, 1016.29/s)  LR: 3.373e-04  Data: 0.011 (0.014)
Train: 183 [1100/1251 ( 88%)]  Loss: 3.612 (3.44)  Time: 1.002s, 1021.55/s  (1.007s, 1016.67/s)  LR: 3.373e-04  Data: 0.011 (0.014)
Train: 183 [1150/1251 ( 92%)]  Loss: 3.318 (3.44)  Time: 0.998s, 1026.49/s  (1.007s, 1016.89/s)  LR: 3.373e-04  Data: 0.011 (0.014)
Train: 183 [1200/1251 ( 96%)]  Loss: 3.384 (3.43)  Time: 0.994s, 1030.11/s  (1.007s, 1017.22/s)  LR: 3.373e-04  Data: 0.011 (0.014)
Train: 183 [1250/1251 (100%)]  Loss: 3.139 (3.42)  Time: 0.985s, 1039.80/s  (1.006s, 1017.46/s)  LR: 3.373e-04  Data: 0.000 (0.014)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.780 (1.780)  Loss:  0.6509 (0.6509)  Acc@1: 89.1602 (89.1602)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.245 (0.568)  Loss:  0.7411 (1.1876)  Acc@1: 85.6132 (76.2400)  Acc@5: 96.5802 (93.1780)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-179.pth.tar', 76.33200016357422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-183.pth.tar', 76.24000008544922)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-180.pth.tar', 76.15399995605469)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-182.pth.tar', 76.09400003417969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-174.pth.tar', 76.08599998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-181.pth.tar', 76.06400019287109)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-175.pth.tar', 75.9339999609375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-176.pth.tar', 75.9219999267578)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-171.pth.tar', 75.8079999584961)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-172.pth.tar', 75.71200008300781)

Train: 184 [   0/1251 (  0%)]  Loss: 3.422 (3.42)  Time: 2.461s,  416.09/s  (2.461s,  416.09/s)  LR: 3.325e-04  Data: 1.499 (1.499)
Train: 184 [  50/1251 (  4%)]  Loss: 3.482 (3.45)  Time: 0.992s, 1032.41/s  (1.031s,  993.44/s)  LR: 3.325e-04  Data: 0.010 (0.040)
Train: 184 [ 100/1251 (  8%)]  Loss: 3.375 (3.43)  Time: 1.006s, 1017.99/s  (1.019s, 1004.97/s)  LR: 3.325e-04  Data: 0.011 (0.026)
Train: 184 [ 150/1251 ( 12%)]  Loss: 3.479 (3.44)  Time: 0.996s, 1028.41/s  (1.013s, 1011.07/s)  LR: 3.325e-04  Data: 0.011 (0.021)
Train: 184 [ 200/1251 ( 16%)]  Loss: 3.445 (3.44)  Time: 0.994s, 1030.08/s  (1.009s, 1014.91/s)  LR: 3.325e-04  Data: 0.010 (0.019)
Train: 184 [ 250/1251 ( 20%)]  Loss: 3.281 (3.41)  Time: 1.002s, 1022.36/s  (1.008s, 1015.40/s)  LR: 3.325e-04  Data: 0.011 (0.017)
Train: 184 [ 300/1251 ( 24%)]  Loss: 3.066 (3.36)  Time: 0.994s, 1030.67/s  (1.007s, 1016.56/s)  LR: 3.325e-04  Data: 0.011 (0.016)
Train: 184 [ 350/1251 ( 28%)]  Loss: 3.173 (3.34)  Time: 1.010s, 1014.13/s  (1.006s, 1017.41/s)  LR: 3.325e-04  Data: 0.011 (0.015)
Train: 184 [ 400/1251 ( 32%)]  Loss: 3.558 (3.36)  Time: 1.021s, 1002.69/s  (1.006s, 1018.31/s)  LR: 3.325e-04  Data: 0.011 (0.015)
Train: 184 [ 450/1251 ( 36%)]  Loss: 3.258 (3.35)  Time: 0.997s, 1027.09/s  (1.005s, 1019.22/s)  LR: 3.325e-04  Data: 0.011 (0.014)
Train: 184 [ 500/1251 ( 40%)]  Loss: 3.303 (3.35)  Time: 0.994s, 1030.10/s  (1.004s, 1019.43/s)  LR: 3.325e-04  Data: 0.011 (0.014)
Train: 184 [ 550/1251 ( 44%)]  Loss: 3.303 (3.35)  Time: 1.061s,  965.31/s  (1.006s, 1017.88/s)  LR: 3.325e-04  Data: 0.011 (0.014)
Train: 184 [ 600/1251 ( 48%)]  Loss: 3.620 (3.37)  Time: 0.994s, 1030.01/s  (1.007s, 1016.88/s)  LR: 3.325e-04  Data: 0.010 (0.014)
Train: 184 [ 650/1251 ( 52%)]  Loss: 3.201 (3.35)  Time: 0.996s, 1027.88/s  (1.006s, 1017.47/s)  LR: 3.325e-04  Data: 0.011 (0.013)
Train: 184 [ 700/1251 ( 56%)]  Loss: 3.209 (3.35)  Time: 0.998s, 1026.43/s  (1.006s, 1017.95/s)  LR: 3.325e-04  Data: 0.011 (0.013)
Train: 184 [ 750/1251 ( 60%)]  Loss: 3.808 (3.37)  Time: 1.032s,  991.91/s  (1.006s, 1017.57/s)  LR: 3.325e-04  Data: 0.011 (0.013)
Train: 184 [ 800/1251 ( 64%)]  Loss: 3.279 (3.37)  Time: 0.994s, 1030.46/s  (1.007s, 1016.74/s)  LR: 3.325e-04  Data: 0.010 (0.013)
Train: 184 [ 850/1251 ( 68%)]  Loss: 3.579 (3.38)  Time: 1.057s,  968.68/s  (1.007s, 1016.47/s)  LR: 3.325e-04  Data: 0.011 (0.013)
Train: 184 [ 900/1251 ( 72%)]  Loss: 3.501 (3.39)  Time: 1.007s, 1017.21/s  (1.008s, 1015.91/s)  LR: 3.325e-04  Data: 0.010 (0.013)
Train: 184 [ 950/1251 ( 76%)]  Loss: 3.570 (3.40)  Time: 0.994s, 1030.32/s  (1.008s, 1015.64/s)  LR: 3.325e-04  Data: 0.011 (0.013)
Train: 184 [1000/1251 ( 80%)]  Loss: 3.490 (3.40)  Time: 0.999s, 1025.08/s  (1.008s, 1015.64/s)  LR: 3.325e-04  Data: 0.012 (0.013)
Train: 184 [1050/1251 ( 84%)]  Loss: 3.331 (3.40)  Time: 1.035s,  989.23/s  (1.008s, 1015.82/s)  LR: 3.325e-04  Data: 0.012 (0.013)
Train: 184 [1100/1251 ( 88%)]  Loss: 3.290 (3.39)  Time: 0.997s, 1027.06/s  (1.008s, 1016.16/s)  LR: 3.325e-04  Data: 0.011 (0.012)
Train: 184 [1150/1251 ( 92%)]  Loss: 3.488 (3.40)  Time: 1.012s, 1011.95/s  (1.007s, 1016.50/s)  LR: 3.325e-04  Data: 0.012 (0.012)
Train: 184 [1200/1251 ( 96%)]  Loss: 3.197 (3.39)  Time: 0.989s, 1034.88/s  (1.007s, 1016.81/s)  LR: 3.325e-04  Data: 0.010 (0.012)
Train: 184 [1250/1251 (100%)]  Loss: 3.560 (3.39)  Time: 0.981s, 1044.33/s  (1.007s, 1016.95/s)  LR: 3.325e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.676 (1.676)  Loss:  0.8243 (0.8243)  Acc@1: 90.5273 (90.5273)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.245 (0.577)  Loss:  0.9277 (1.3169)  Acc@1: 86.4387 (76.2300)  Acc@5: 96.4623 (93.1040)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-179.pth.tar', 76.33200016357422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-183.pth.tar', 76.24000008544922)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-184.pth.tar', 76.22999992675781)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-180.pth.tar', 76.15399995605469)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-182.pth.tar', 76.09400003417969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-174.pth.tar', 76.08599998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-181.pth.tar', 76.06400019287109)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-175.pth.tar', 75.9339999609375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-176.pth.tar', 75.9219999267578)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-171.pth.tar', 75.8079999584961)

Train: 185 [   0/1251 (  0%)]  Loss: 3.259 (3.26)  Time: 2.424s,  422.36/s  (2.424s,  422.36/s)  LR: 3.276e-04  Data: 1.459 (1.459)
Train: 185 [  50/1251 (  4%)]  Loss: 3.528 (3.39)  Time: 1.035s,  989.82/s  (1.047s,  978.20/s)  LR: 3.276e-04  Data: 0.011 (0.040)
Train: 185 [ 100/1251 (  8%)]  Loss: 2.885 (3.22)  Time: 1.013s, 1010.83/s  (1.024s, 1000.49/s)  LR: 3.276e-04  Data: 0.012 (0.026)
Train: 185 [ 150/1251 ( 12%)]  Loss: 3.803 (3.37)  Time: 0.991s, 1033.57/s  (1.016s, 1007.55/s)  LR: 3.276e-04  Data: 0.011 (0.021)
Train: 185 [ 200/1251 ( 16%)]  Loss: 3.160 (3.33)  Time: 0.993s, 1031.31/s  (1.013s, 1010.83/s)  LR: 3.276e-04  Data: 0.011 (0.019)
Train: 185 [ 250/1251 ( 20%)]  Loss: 3.498 (3.36)  Time: 1.001s, 1022.53/s  (1.010s, 1013.51/s)  LR: 3.276e-04  Data: 0.012 (0.017)
Train: 185 [ 300/1251 ( 24%)]  Loss: 3.603 (3.39)  Time: 0.998s, 1026.07/s  (1.009s, 1014.98/s)  LR: 3.276e-04  Data: 0.012 (0.016)
Train: 185 [ 350/1251 ( 28%)]  Loss: 3.271 (3.38)  Time: 1.028s,  995.78/s  (1.011s, 1012.89/s)  LR: 3.276e-04  Data: 0.011 (0.016)
Train: 185 [ 400/1251 ( 32%)]  Loss: 3.161 (3.35)  Time: 0.996s, 1028.48/s  (1.009s, 1014.61/s)  LR: 3.276e-04  Data: 0.011 (0.015)
Train: 185 [ 450/1251 ( 36%)]  Loss: 3.222 (3.34)  Time: 0.995s, 1029.16/s  (1.008s, 1015.77/s)  LR: 3.276e-04  Data: 0.012 (0.015)
Train: 185 [ 500/1251 ( 40%)]  Loss: 3.240 (3.33)  Time: 1.003s, 1020.51/s  (1.007s, 1016.46/s)  LR: 3.276e-04  Data: 0.013 (0.014)
Train: 185 [ 550/1251 ( 44%)]  Loss: 3.373 (3.33)  Time: 0.997s, 1027.15/s  (1.007s, 1017.01/s)  LR: 3.276e-04  Data: 0.012 (0.014)
Train: 185 [ 600/1251 ( 48%)]  Loss: 3.477 (3.34)  Time: 0.995s, 1029.47/s  (1.006s, 1017.81/s)  LR: 3.276e-04  Data: 0.011 (0.014)
Train: 185 [ 650/1251 ( 52%)]  Loss: 3.335 (3.34)  Time: 1.001s, 1023.45/s  (1.005s, 1018.45/s)  LR: 3.276e-04  Data: 0.011 (0.014)
Train: 185 [ 700/1251 ( 56%)]  Loss: 3.562 (3.36)  Time: 0.995s, 1028.73/s  (1.005s, 1019.07/s)  LR: 3.276e-04  Data: 0.012 (0.013)
Train: 185 [ 750/1251 ( 60%)]  Loss: 3.199 (3.35)  Time: 1.021s, 1003.18/s  (1.005s, 1018.83/s)  LR: 3.276e-04  Data: 0.010 (0.013)
Train: 185 [ 800/1251 ( 64%)]  Loss: 3.065 (3.33)  Time: 0.995s, 1028.80/s  (1.005s, 1018.85/s)  LR: 3.276e-04  Data: 0.011 (0.013)
Train: 185 [ 850/1251 ( 68%)]  Loss: 3.485 (3.34)  Time: 1.015s, 1008.49/s  (1.005s, 1019.26/s)  LR: 3.276e-04  Data: 0.011 (0.013)
Train: 185 [ 900/1251 ( 72%)]  Loss: 3.320 (3.34)  Time: 1.003s, 1020.53/s  (1.004s, 1019.61/s)  LR: 3.276e-04  Data: 0.012 (0.013)
Train: 185 [ 950/1251 ( 76%)]  Loss: 3.102 (3.33)  Time: 0.999s, 1025.17/s  (1.004s, 1019.66/s)  LR: 3.276e-04  Data: 0.012 (0.013)
Train: 185 [1000/1251 ( 80%)]  Loss: 3.354 (3.33)  Time: 0.997s, 1027.23/s  (1.004s, 1020.03/s)  LR: 3.276e-04  Data: 0.012 (0.013)
Train: 185 [1050/1251 ( 84%)]  Loss: 3.740 (3.35)  Time: 0.997s, 1027.55/s  (1.004s, 1020.26/s)  LR: 3.276e-04  Data: 0.011 (0.013)
Train: 185 [1100/1251 ( 88%)]  Loss: 3.723 (3.36)  Time: 0.997s, 1027.21/s  (1.004s, 1020.35/s)  LR: 3.276e-04  Data: 0.010 (0.013)
Train: 185 [1150/1251 ( 92%)]  Loss: 3.442 (3.37)  Time: 0.997s, 1027.32/s  (1.003s, 1020.64/s)  LR: 3.276e-04  Data: 0.012 (0.013)
Train: 185 [1200/1251 ( 96%)]  Loss: 3.690 (3.38)  Time: 0.995s, 1028.78/s  (1.003s, 1020.71/s)  LR: 3.276e-04  Data: 0.011 (0.013)
Train: 185 [1250/1251 (100%)]  Loss: 3.438 (3.38)  Time: 0.985s, 1039.51/s  (1.003s, 1020.84/s)  LR: 3.276e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.658 (1.658)  Loss:  0.7605 (0.7605)  Acc@1: 90.0391 (90.0391)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.245 (0.573)  Loss:  0.9147 (1.2715)  Acc@1: 83.8443 (76.5060)  Acc@5: 96.4623 (93.3640)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-185.pth.tar', 76.50599996337891)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-179.pth.tar', 76.33200016357422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-183.pth.tar', 76.24000008544922)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-184.pth.tar', 76.22999992675781)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-180.pth.tar', 76.15399995605469)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-182.pth.tar', 76.09400003417969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-174.pth.tar', 76.08599998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-181.pth.tar', 76.06400019287109)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-175.pth.tar', 75.9339999609375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-176.pth.tar', 75.9219999267578)

Train: 186 [   0/1251 (  0%)]  Loss: 3.404 (3.40)  Time: 2.697s,  379.64/s  (2.697s,  379.64/s)  LR: 3.228e-04  Data: 1.728 (1.728)
Train: 186 [  50/1251 (  4%)]  Loss: 3.074 (3.24)  Time: 0.999s, 1024.57/s  (1.038s,  986.17/s)  LR: 3.228e-04  Data: 0.012 (0.045)
Train: 186 [ 100/1251 (  8%)]  Loss: 3.674 (3.38)  Time: 1.034s,  990.67/s  (1.022s, 1001.72/s)  LR: 3.228e-04  Data: 0.012 (0.029)
Train: 186 [ 150/1251 ( 12%)]  Loss: 3.602 (3.44)  Time: 0.993s, 1030.95/s  (1.022s, 1002.42/s)  LR: 3.228e-04  Data: 0.011 (0.023)
Train: 186 [ 200/1251 ( 16%)]  Loss: 3.525 (3.46)  Time: 0.997s, 1027.00/s  (1.017s, 1006.49/s)  LR: 3.228e-04  Data: 0.011 (0.020)
Train: 186 [ 250/1251 ( 20%)]  Loss: 3.414 (3.45)  Time: 1.002s, 1021.91/s  (1.014s, 1009.82/s)  LR: 3.228e-04  Data: 0.011 (0.018)
Train: 186 [ 300/1251 ( 24%)]  Loss: 3.482 (3.45)  Time: 1.011s, 1012.42/s  (1.012s, 1011.82/s)  LR: 3.228e-04  Data: 0.012 (0.017)
Train: 186 [ 350/1251 ( 28%)]  Loss: 3.372 (3.44)  Time: 0.996s, 1028.33/s  (1.011s, 1013.14/s)  LR: 3.228e-04  Data: 0.011 (0.016)
Train: 186 [ 400/1251 ( 32%)]  Loss: 3.528 (3.45)  Time: 0.998s, 1026.21/s  (1.010s, 1014.17/s)  LR: 3.228e-04  Data: 0.011 (0.016)
Train: 186 [ 450/1251 ( 36%)]  Loss: 3.009 (3.41)  Time: 0.997s, 1026.98/s  (1.009s, 1014.56/s)  LR: 3.228e-04  Data: 0.012 (0.015)
Train: 186 [ 500/1251 ( 40%)]  Loss: 3.259 (3.39)  Time: 1.003s, 1020.96/s  (1.008s, 1015.55/s)  LR: 3.228e-04  Data: 0.011 (0.015)
Train: 186 [ 550/1251 ( 44%)]  Loss: 3.504 (3.40)  Time: 0.992s, 1032.58/s  (1.008s, 1016.24/s)  LR: 3.228e-04  Data: 0.011 (0.015)
Train: 186 [ 600/1251 ( 48%)]  Loss: 3.135 (3.38)  Time: 1.037s,  987.86/s  (1.008s, 1015.61/s)  LR: 3.228e-04  Data: 0.011 (0.014)
Train: 186 [ 650/1251 ( 52%)]  Loss: 3.378 (3.38)  Time: 0.995s, 1029.56/s  (1.008s, 1015.47/s)  LR: 3.228e-04  Data: 0.010 (0.014)
Train: 186 [ 700/1251 ( 56%)]  Loss: 3.686 (3.40)  Time: 1.007s, 1017.38/s  (1.008s, 1016.28/s)  LR: 3.228e-04  Data: 0.012 (0.014)
Train: 186 [ 750/1251 ( 60%)]  Loss: 3.513 (3.41)  Time: 0.998s, 1025.59/s  (1.007s, 1016.73/s)  LR: 3.228e-04  Data: 0.011 (0.014)
Train: 186 [ 800/1251 ( 64%)]  Loss: 3.369 (3.41)  Time: 1.057s,  968.76/s  (1.007s, 1017.19/s)  LR: 3.228e-04  Data: 0.011 (0.014)
Train: 186 [ 850/1251 ( 68%)]  Loss: 3.286 (3.40)  Time: 1.010s, 1013.56/s  (1.007s, 1016.61/s)  LR: 3.228e-04  Data: 0.011 (0.013)
Train: 186 [ 900/1251 ( 72%)]  Loss: 3.439 (3.40)  Time: 0.998s, 1026.19/s  (1.007s, 1017.11/s)  LR: 3.228e-04  Data: 0.011 (0.013)
Train: 186 [ 950/1251 ( 76%)]  Loss: 3.322 (3.40)  Time: 0.998s, 1026.34/s  (1.006s, 1017.49/s)  LR: 3.228e-04  Data: 0.011 (0.013)
Train: 186 [1000/1251 ( 80%)]  Loss: 3.552 (3.41)  Time: 0.991s, 1033.70/s  (1.006s, 1017.85/s)  LR: 3.228e-04  Data: 0.010 (0.013)
Train: 186 [1050/1251 ( 84%)]  Loss: 3.052 (3.39)  Time: 1.004s, 1019.90/s  (1.006s, 1017.94/s)  LR: 3.228e-04  Data: 0.012 (0.013)
Train: 186 [1100/1251 ( 88%)]  Loss: 3.396 (3.39)  Time: 0.995s, 1029.08/s  (1.006s, 1018.00/s)  LR: 3.228e-04  Data: 0.011 (0.013)
Train: 186 [1150/1251 ( 92%)]  Loss: 3.478 (3.39)  Time: 1.005s, 1019.05/s  (1.006s, 1018.21/s)  LR: 3.228e-04  Data: 0.011 (0.013)
Train: 186 [1200/1251 ( 96%)]  Loss: 3.558 (3.40)  Time: 1.005s, 1018.79/s  (1.007s, 1016.85/s)  LR: 3.228e-04  Data: 0.012 (0.013)
Train: 186 [1250/1251 (100%)]  Loss: 3.095 (3.39)  Time: 1.027s,  997.28/s  (1.007s, 1017.12/s)  LR: 3.228e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.694 (1.694)  Loss:  0.6737 (0.6737)  Acc@1: 90.8203 (90.8203)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.245 (0.586)  Loss:  0.7359 (1.1777)  Acc@1: 84.9057 (76.1960)  Acc@5: 97.0519 (93.3720)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-185.pth.tar', 76.50599996337891)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-179.pth.tar', 76.33200016357422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-183.pth.tar', 76.24000008544922)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-184.pth.tar', 76.22999992675781)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-186.pth.tar', 76.1960000366211)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-180.pth.tar', 76.15399995605469)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-182.pth.tar', 76.09400003417969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-174.pth.tar', 76.08599998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-181.pth.tar', 76.06400019287109)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-175.pth.tar', 75.9339999609375)

Train: 187 [   0/1251 (  0%)]  Loss: 3.503 (3.50)  Time: 2.574s,  397.87/s  (2.574s,  397.87/s)  LR: 3.180e-04  Data: 1.613 (1.613)
Train: 187 [  50/1251 (  4%)]  Loss: 3.280 (3.39)  Time: 0.998s, 1025.66/s  (1.036s,  988.37/s)  LR: 3.180e-04  Data: 0.011 (0.044)
Train: 187 [ 100/1251 (  8%)]  Loss: 3.371 (3.38)  Time: 1.007s, 1016.65/s  (1.022s, 1002.06/s)  LR: 3.180e-04  Data: 0.012 (0.028)
Train: 187 [ 150/1251 ( 12%)]  Loss: 3.826 (3.50)  Time: 1.071s,  955.77/s  (1.016s, 1007.81/s)  LR: 3.180e-04  Data: 0.012 (0.023)
Train: 187 [ 200/1251 ( 16%)]  Loss: 3.660 (3.53)  Time: 0.998s, 1025.80/s  (1.012s, 1011.66/s)  LR: 3.180e-04  Data: 0.012 (0.020)
Train: 187 [ 250/1251 ( 20%)]  Loss: 3.644 (3.55)  Time: 0.996s, 1028.00/s  (1.010s, 1013.40/s)  LR: 3.180e-04  Data: 0.011 (0.018)
Train: 187 [ 300/1251 ( 24%)]  Loss: 3.766 (3.58)  Time: 1.006s, 1017.74/s  (1.013s, 1010.65/s)  LR: 3.180e-04  Data: 0.013 (0.017)
Train: 187 [ 350/1251 ( 28%)]  Loss: 3.047 (3.51)  Time: 0.999s, 1024.92/s  (1.012s, 1012.06/s)  LR: 3.180e-04  Data: 0.011 (0.016)
Train: 187 [ 400/1251 ( 32%)]  Loss: 3.626 (3.52)  Time: 1.032s,  991.79/s  (1.012s, 1011.48/s)  LR: 3.180e-04  Data: 0.011 (0.016)
Train: 187 [ 450/1251 ( 36%)]  Loss: 3.406 (3.51)  Time: 1.051s,  974.64/s  (1.013s, 1011.21/s)  LR: 3.180e-04  Data: 0.011 (0.015)
Train: 187 [ 500/1251 ( 40%)]  Loss: 3.387 (3.50)  Time: 0.998s, 1026.55/s  (1.012s, 1012.01/s)  LR: 3.180e-04  Data: 0.012 (0.015)
Train: 187 [ 550/1251 ( 44%)]  Loss: 3.290 (3.48)  Time: 1.061s,  964.92/s  (1.012s, 1011.64/s)  LR: 3.180e-04  Data: 0.011 (0.014)
Train: 187 [ 600/1251 ( 48%)]  Loss: 3.511 (3.49)  Time: 1.000s, 1024.04/s  (1.012s, 1012.05/s)  LR: 3.180e-04  Data: 0.011 (0.014)
Train: 187 [ 650/1251 ( 52%)]  Loss: 3.346 (3.48)  Time: 0.997s, 1026.98/s  (1.011s, 1012.79/s)  LR: 3.180e-04  Data: 0.010 (0.014)
Train: 187 [ 700/1251 ( 56%)]  Loss: 3.269 (3.46)  Time: 1.014s, 1010.14/s  (1.011s, 1013.12/s)  LR: 3.180e-04  Data: 0.010 (0.014)
Train: 187 [ 750/1251 ( 60%)]  Loss: 3.384 (3.46)  Time: 0.992s, 1032.21/s  (1.010s, 1013.69/s)  LR: 3.180e-04  Data: 0.011 (0.014)
Train: 187 [ 800/1251 ( 64%)]  Loss: 3.498 (3.46)  Time: 0.998s, 1026.24/s  (1.011s, 1012.85/s)  LR: 3.180e-04  Data: 0.012 (0.013)
Train: 187 [ 850/1251 ( 68%)]  Loss: 3.268 (3.45)  Time: 1.028s,  995.82/s  (1.010s, 1013.58/s)  LR: 3.180e-04  Data: 0.011 (0.013)
Train: 187 [ 900/1251 ( 72%)]  Loss: 3.495 (3.45)  Time: 0.997s, 1027.56/s  (1.010s, 1014.04/s)  LR: 3.180e-04  Data: 0.012 (0.013)
Train: 187 [ 950/1251 ( 76%)]  Loss: 3.560 (3.46)  Time: 0.998s, 1025.87/s  (1.009s, 1014.39/s)  LR: 3.180e-04  Data: 0.011 (0.013)
Train: 187 [1000/1251 ( 80%)]  Loss: 3.420 (3.46)  Time: 1.000s, 1024.12/s  (1.009s, 1014.65/s)  LR: 3.180e-04  Data: 0.011 (0.013)
Train: 187 [1050/1251 ( 84%)]  Loss: 3.413 (3.45)  Time: 0.996s, 1027.71/s  (1.009s, 1014.62/s)  LR: 3.180e-04  Data: 0.011 (0.013)
Train: 187 [1100/1251 ( 88%)]  Loss: 2.974 (3.43)  Time: 1.009s, 1014.47/s  (1.009s, 1015.08/s)  LR: 3.180e-04  Data: 0.011 (0.013)
Train: 187 [1150/1251 ( 92%)]  Loss: 3.316 (3.43)  Time: 0.997s, 1026.71/s  (1.009s, 1014.78/s)  LR: 3.180e-04  Data: 0.012 (0.013)
Train: 187 [1200/1251 ( 96%)]  Loss: 3.259 (3.42)  Time: 0.994s, 1030.19/s  (1.009s, 1015.21/s)  LR: 3.180e-04  Data: 0.011 (0.013)
Train: 187 [1250/1251 (100%)]  Loss: 3.827 (3.44)  Time: 0.983s, 1042.08/s  (1.009s, 1015.11/s)  LR: 3.180e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.596 (1.596)  Loss:  0.7516 (0.7516)  Acc@1: 91.1133 (91.1133)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.245 (0.569)  Loss:  0.8398 (1.2564)  Acc@1: 85.7311 (76.7920)  Acc@5: 96.5802 (93.5620)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-187.pth.tar', 76.79200000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-185.pth.tar', 76.50599996337891)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-179.pth.tar', 76.33200016357422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-183.pth.tar', 76.24000008544922)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-184.pth.tar', 76.22999992675781)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-186.pth.tar', 76.1960000366211)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-180.pth.tar', 76.15399995605469)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-182.pth.tar', 76.09400003417969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-174.pth.tar', 76.08599998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-181.pth.tar', 76.06400019287109)

Train: 188 [   0/1251 (  0%)]  Loss: 3.763 (3.76)  Time: 2.529s,  404.86/s  (2.529s,  404.86/s)  LR: 3.132e-04  Data: 1.545 (1.545)
Train: 188 [  50/1251 (  4%)]  Loss: 3.377 (3.57)  Time: 1.001s, 1023.35/s  (1.031s,  993.64/s)  LR: 3.132e-04  Data: 0.011 (0.042)
Train: 188 [ 100/1251 (  8%)]  Loss: 3.351 (3.50)  Time: 0.998s, 1026.51/s  (1.015s, 1009.01/s)  LR: 3.132e-04  Data: 0.012 (0.026)
Train: 188 [ 150/1251 ( 12%)]  Loss: 3.611 (3.53)  Time: 0.995s, 1029.14/s  (1.015s, 1009.05/s)  LR: 3.132e-04  Data: 0.011 (0.021)
Train: 188 [ 200/1251 ( 16%)]  Loss: 3.638 (3.55)  Time: 1.009s, 1015.30/s  (1.012s, 1011.80/s)  LR: 3.132e-04  Data: 0.011 (0.019)
Train: 188 [ 250/1251 ( 20%)]  Loss: 3.258 (3.50)  Time: 0.994s, 1030.55/s  (1.010s, 1014.11/s)  LR: 3.132e-04  Data: 0.011 (0.017)
Train: 188 [ 300/1251 ( 24%)]  Loss: 3.495 (3.50)  Time: 0.992s, 1031.88/s  (1.007s, 1016.38/s)  LR: 3.132e-04  Data: 0.011 (0.016)
Train: 188 [ 350/1251 ( 28%)]  Loss: 3.564 (3.51)  Time: 1.060s,  966.09/s  (1.010s, 1013.80/s)  LR: 3.132e-04  Data: 0.014 (0.016)
Train: 188 [ 400/1251 ( 32%)]  Loss: 3.436 (3.50)  Time: 0.998s, 1026.36/s  (1.010s, 1013.49/s)  LR: 3.132e-04  Data: 0.013 (0.015)
Train: 188 [ 450/1251 ( 36%)]  Loss: 3.264 (3.48)  Time: 0.994s, 1030.32/s  (1.010s, 1014.20/s)  LR: 3.132e-04  Data: 0.011 (0.015)
Train: 188 [ 500/1251 ( 40%)]  Loss: 3.517 (3.48)  Time: 0.996s, 1028.45/s  (1.008s, 1015.41/s)  LR: 3.132e-04  Data: 0.010 (0.014)
Train: 188 [ 550/1251 ( 44%)]  Loss: 3.104 (3.45)  Time: 0.997s, 1027.51/s  (1.008s, 1015.96/s)  LR: 3.132e-04  Data: 0.012 (0.014)
Train: 188 [ 600/1251 ( 48%)]  Loss: 3.508 (3.45)  Time: 0.996s, 1027.84/s  (1.007s, 1016.41/s)  LR: 3.132e-04  Data: 0.011 (0.014)
Train: 188 [ 650/1251 ( 52%)]  Loss: 3.694 (3.47)  Time: 1.034s,  990.48/s  (1.008s, 1015.75/s)  LR: 3.132e-04  Data: 0.011 (0.014)
Train: 188 [ 700/1251 ( 56%)]  Loss: 3.354 (3.46)  Time: 1.005s, 1019.10/s  (1.008s, 1015.47/s)  LR: 3.132e-04  Data: 0.012 (0.013)
Train: 188 [ 750/1251 ( 60%)]  Loss: 3.654 (3.47)  Time: 0.998s, 1025.95/s  (1.008s, 1015.91/s)  LR: 3.132e-04  Data: 0.012 (0.013)
Train: 188 [ 800/1251 ( 64%)]  Loss: 3.290 (3.46)  Time: 0.994s, 1029.71/s  (1.008s, 1015.46/s)  LR: 3.132e-04  Data: 0.010 (0.013)
Train: 188 [ 850/1251 ( 68%)]  Loss: 3.654 (3.47)  Time: 0.997s, 1027.14/s  (1.008s, 1016.12/s)  LR: 3.132e-04  Data: 0.012 (0.013)
Train: 188 [ 900/1251 ( 72%)]  Loss: 3.256 (3.46)  Time: 0.994s, 1029.80/s  (1.007s, 1016.41/s)  LR: 3.132e-04  Data: 0.010 (0.013)
Train: 188 [ 950/1251 ( 76%)]  Loss: 3.391 (3.46)  Time: 0.998s, 1026.15/s  (1.007s, 1016.60/s)  LR: 3.132e-04  Data: 0.011 (0.013)
Train: 188 [1000/1251 ( 80%)]  Loss: 3.287 (3.45)  Time: 1.014s, 1009.98/s  (1.007s, 1017.00/s)  LR: 3.132e-04  Data: 0.011 (0.013)
Train: 188 [1050/1251 ( 84%)]  Loss: 3.085 (3.43)  Time: 0.989s, 1035.07/s  (1.007s, 1017.18/s)  LR: 3.132e-04  Data: 0.011 (0.013)
Train: 188 [1100/1251 ( 88%)]  Loss: 3.664 (3.44)  Time: 0.999s, 1024.69/s  (1.006s, 1017.59/s)  LR: 3.132e-04  Data: 0.010 (0.013)
Train: 188 [1150/1251 ( 92%)]  Loss: 3.418 (3.44)  Time: 0.996s, 1027.76/s  (1.006s, 1017.96/s)  LR: 3.132e-04  Data: 0.011 (0.013)
Train: 188 [1200/1251 ( 96%)]  Loss: 3.416 (3.44)  Time: 1.064s,  962.83/s  (1.006s, 1017.92/s)  LR: 3.132e-04  Data: 0.011 (0.012)
Train: 188 [1250/1251 (100%)]  Loss: 3.046 (3.43)  Time: 0.984s, 1040.51/s  (1.006s, 1018.04/s)  LR: 3.132e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.666 (1.666)  Loss:  0.5820 (0.5820)  Acc@1: 91.3086 (91.3086)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.245 (0.574)  Loss:  0.6844 (1.1437)  Acc@1: 85.7311 (76.4320)  Acc@5: 97.2877 (93.3940)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-187.pth.tar', 76.79200000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-185.pth.tar', 76.50599996337891)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-188.pth.tar', 76.43200013671876)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-179.pth.tar', 76.33200016357422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-183.pth.tar', 76.24000008544922)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-184.pth.tar', 76.22999992675781)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-186.pth.tar', 76.1960000366211)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-180.pth.tar', 76.15399995605469)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-182.pth.tar', 76.09400003417969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-174.pth.tar', 76.08599998535156)

Train: 189 [   0/1251 (  0%)]  Loss: 3.167 (3.17)  Time: 2.745s,  373.04/s  (2.745s,  373.04/s)  LR: 3.084e-04  Data: 1.734 (1.734)
Train: 189 [  50/1251 (  4%)]  Loss: 3.565 (3.37)  Time: 1.004s, 1020.39/s  (1.047s,  977.81/s)  LR: 3.084e-04  Data: 0.010 (0.045)
Train: 189 [ 100/1251 (  8%)]  Loss: 3.554 (3.43)  Time: 1.032s,  991.96/s  (1.035s,  988.91/s)  LR: 3.084e-04  Data: 0.010 (0.028)
Train: 189 [ 150/1251 ( 12%)]  Loss: 3.698 (3.50)  Time: 1.006s, 1018.23/s  (1.024s, 1000.34/s)  LR: 3.084e-04  Data: 0.012 (0.022)
Train: 189 [ 200/1251 ( 16%)]  Loss: 3.672 (3.53)  Time: 0.995s, 1029.65/s  (1.018s, 1006.28/s)  LR: 3.084e-04  Data: 0.011 (0.020)
Train: 189 [ 250/1251 ( 20%)]  Loss: 2.957 (3.44)  Time: 1.003s, 1020.55/s  (1.014s, 1009.66/s)  LR: 3.084e-04  Data: 0.011 (0.018)
Train: 189 [ 300/1251 ( 24%)]  Loss: 3.192 (3.40)  Time: 0.995s, 1028.67/s  (1.012s, 1011.97/s)  LR: 3.084e-04  Data: 0.012 (0.017)
Train: 189 [ 350/1251 ( 28%)]  Loss: 3.453 (3.41)  Time: 1.058s,  968.24/s  (1.011s, 1012.95/s)  LR: 3.084e-04  Data: 0.010 (0.016)
Train: 189 [ 400/1251 ( 32%)]  Loss: 3.724 (3.44)  Time: 0.995s, 1028.84/s  (1.010s, 1013.71/s)  LR: 3.084e-04  Data: 0.011 (0.015)
Train: 189 [ 450/1251 ( 36%)]  Loss: 3.346 (3.43)  Time: 0.997s, 1026.91/s  (1.010s, 1014.13/s)  LR: 3.084e-04  Data: 0.011 (0.015)
Train: 189 [ 500/1251 ( 40%)]  Loss: 3.302 (3.42)  Time: 1.004s, 1020.39/s  (1.012s, 1012.05/s)  LR: 3.084e-04  Data: 0.011 (0.015)
Train: 189 [ 550/1251 ( 44%)]  Loss: 3.599 (3.44)  Time: 0.994s, 1030.05/s  (1.011s, 1013.11/s)  LR: 3.084e-04  Data: 0.011 (0.014)
Train: 189 [ 600/1251 ( 48%)]  Loss: 3.280 (3.42)  Time: 0.994s, 1030.62/s  (1.010s, 1013.88/s)  LR: 3.084e-04  Data: 0.011 (0.014)
Train: 189 [ 650/1251 ( 52%)]  Loss: 3.130 (3.40)  Time: 0.994s, 1029.92/s  (1.009s, 1014.83/s)  LR: 3.084e-04  Data: 0.010 (0.014)
Train: 189 [ 700/1251 ( 56%)]  Loss: 3.111 (3.38)  Time: 0.998s, 1026.27/s  (1.009s, 1015.21/s)  LR: 3.084e-04  Data: 0.011 (0.014)
Train: 189 [ 750/1251 ( 60%)]  Loss: 3.231 (3.37)  Time: 0.995s, 1029.42/s  (1.008s, 1015.68/s)  LR: 3.084e-04  Data: 0.011 (0.014)
Train: 189 [ 800/1251 ( 64%)]  Loss: 3.554 (3.38)  Time: 0.994s, 1030.36/s  (1.008s, 1016.06/s)  LR: 3.084e-04  Data: 0.011 (0.013)
Train: 189 [ 850/1251 ( 68%)]  Loss: 3.492 (3.39)  Time: 1.006s, 1017.85/s  (1.007s, 1016.44/s)  LR: 3.084e-04  Data: 0.011 (0.013)
Train: 189 [ 900/1251 ( 72%)]  Loss: 3.663 (3.40)  Time: 1.006s, 1017.40/s  (1.007s, 1016.80/s)  LR: 3.084e-04  Data: 0.011 (0.013)
Train: 189 [ 950/1251 ( 76%)]  Loss: 3.471 (3.41)  Time: 0.998s, 1026.46/s  (1.007s, 1017.17/s)  LR: 3.084e-04  Data: 0.011 (0.013)
Train: 189 [1000/1251 ( 80%)]  Loss: 3.333 (3.40)  Time: 0.994s, 1030.52/s  (1.006s, 1017.55/s)  LR: 3.084e-04  Data: 0.010 (0.013)
Train: 189 [1050/1251 ( 84%)]  Loss: 3.270 (3.40)  Time: 0.992s, 1032.77/s  (1.006s, 1017.78/s)  LR: 3.084e-04  Data: 0.011 (0.013)
Train: 189 [1100/1251 ( 88%)]  Loss: 2.999 (3.38)  Time: 0.998s, 1026.26/s  (1.006s, 1017.96/s)  LR: 3.084e-04  Data: 0.010 (0.013)
Train: 189 [1150/1251 ( 92%)]  Loss: 3.687 (3.39)  Time: 0.996s, 1027.71/s  (1.006s, 1017.92/s)  LR: 3.084e-04  Data: 0.010 (0.013)
Train: 189 [1200/1251 ( 96%)]  Loss: 3.758 (3.41)  Time: 1.048s,  976.84/s  (1.006s, 1017.97/s)  LR: 3.084e-04  Data: 0.010 (0.013)
Train: 189 [1250/1251 (100%)]  Loss: 3.285 (3.40)  Time: 0.982s, 1042.47/s  (1.006s, 1018.37/s)  LR: 3.084e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.607 (1.607)  Loss:  0.8704 (0.8704)  Acc@1: 90.5273 (90.5273)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.245 (0.566)  Loss:  0.8840 (1.3206)  Acc@1: 84.4340 (76.4020)  Acc@5: 96.8160 (93.3340)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-187.pth.tar', 76.79200000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-185.pth.tar', 76.50599996337891)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-188.pth.tar', 76.43200013671876)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-189.pth.tar', 76.40200009033204)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-179.pth.tar', 76.33200016357422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-183.pth.tar', 76.24000008544922)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-184.pth.tar', 76.22999992675781)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-186.pth.tar', 76.1960000366211)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-180.pth.tar', 76.15399995605469)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-182.pth.tar', 76.09400003417969)

Train: 190 [   0/1251 (  0%)]  Loss: 3.764 (3.76)  Time: 2.531s,  404.60/s  (2.531s,  404.60/s)  LR: 3.037e-04  Data: 1.568 (1.568)
Train: 190 [  50/1251 (  4%)]  Loss: 3.315 (3.54)  Time: 1.032s,  992.14/s  (1.039s,  985.13/s)  LR: 3.037e-04  Data: 0.011 (0.041)
Train: 190 [ 100/1251 (  8%)]  Loss: 3.569 (3.55)  Time: 0.995s, 1029.55/s  (1.031s,  993.14/s)  LR: 3.037e-04  Data: 0.011 (0.026)
Train: 190 [ 150/1251 ( 12%)]  Loss: 3.391 (3.51)  Time: 1.008s, 1016.12/s  (1.021s, 1003.29/s)  LR: 3.037e-04  Data: 0.011 (0.021)
Train: 190 [ 200/1251 ( 16%)]  Loss: 3.634 (3.53)  Time: 0.995s, 1029.37/s  (1.015s, 1008.58/s)  LR: 3.037e-04  Data: 0.011 (0.019)
Train: 190 [ 250/1251 ( 20%)]  Loss: 3.640 (3.55)  Time: 0.998s, 1026.48/s  (1.012s, 1011.93/s)  LR: 3.037e-04  Data: 0.010 (0.017)
Train: 190 [ 300/1251 ( 24%)]  Loss: 3.254 (3.51)  Time: 0.996s, 1027.73/s  (1.010s, 1014.08/s)  LR: 3.037e-04  Data: 0.011 (0.016)
Train: 190 [ 350/1251 ( 28%)]  Loss: 3.063 (3.45)  Time: 1.004s, 1019.44/s  (1.008s, 1015.58/s)  LR: 3.037e-04  Data: 0.011 (0.015)
Train: 190 [ 400/1251 ( 32%)]  Loss: 3.519 (3.46)  Time: 1.058s,  968.21/s  (1.009s, 1014.94/s)  LR: 3.037e-04  Data: 0.011 (0.015)
Train: 190 [ 450/1251 ( 36%)]  Loss: 3.180 (3.43)  Time: 1.061s,  965.13/s  (1.010s, 1014.09/s)  LR: 3.037e-04  Data: 0.014 (0.015)
Train: 190 [ 500/1251 ( 40%)]  Loss: 3.448 (3.43)  Time: 0.996s, 1027.64/s  (1.009s, 1014.84/s)  LR: 3.037e-04  Data: 0.012 (0.014)
Train: 190 [ 550/1251 ( 44%)]  Loss: 3.403 (3.43)  Time: 1.012s, 1011.51/s  (1.008s, 1015.70/s)  LR: 3.037e-04  Data: 0.012 (0.014)
Train: 190 [ 600/1251 ( 48%)]  Loss: 3.352 (3.43)  Time: 0.997s, 1027.01/s  (1.008s, 1015.83/s)  LR: 3.037e-04  Data: 0.011 (0.014)
Train: 190 [ 650/1251 ( 52%)]  Loss: 3.253 (3.41)  Time: 1.021s, 1002.82/s  (1.007s, 1016.69/s)  LR: 3.037e-04  Data: 0.012 (0.013)
Train: 190 [ 700/1251 ( 56%)]  Loss: 3.435 (3.41)  Time: 0.996s, 1028.21/s  (1.007s, 1017.20/s)  LR: 3.037e-04  Data: 0.011 (0.013)
Train: 190 [ 750/1251 ( 60%)]  Loss: 3.469 (3.42)  Time: 0.996s, 1027.67/s  (1.006s, 1017.59/s)  LR: 3.037e-04  Data: 0.011 (0.013)
Train: 190 [ 800/1251 ( 64%)]  Loss: 3.609 (3.43)  Time: 0.996s, 1028.37/s  (1.006s, 1017.53/s)  LR: 3.037e-04  Data: 0.011 (0.013)
Train: 190 [ 850/1251 ( 68%)]  Loss: 3.264 (3.42)  Time: 1.000s, 1023.61/s  (1.006s, 1017.50/s)  LR: 3.037e-04  Data: 0.012 (0.013)
Train: 190 [ 900/1251 ( 72%)]  Loss: 3.024 (3.40)  Time: 0.995s, 1029.53/s  (1.006s, 1017.80/s)  LR: 3.037e-04  Data: 0.011 (0.013)
Train: 190 [ 950/1251 ( 76%)]  Loss: 3.273 (3.39)  Time: 1.000s, 1024.45/s  (1.006s, 1018.04/s)  LR: 3.037e-04  Data: 0.011 (0.013)
Train: 190 [1000/1251 ( 80%)]  Loss: 3.672 (3.41)  Time: 1.005s, 1019.10/s  (1.006s, 1017.94/s)  LR: 3.037e-04  Data: 0.011 (0.013)
Train: 190 [1050/1251 ( 84%)]  Loss: 2.987 (3.39)  Time: 0.995s, 1029.44/s  (1.006s, 1018.24/s)  LR: 3.037e-04  Data: 0.011 (0.013)
Train: 190 [1100/1251 ( 88%)]  Loss: 3.283 (3.38)  Time: 0.992s, 1031.92/s  (1.005s, 1018.47/s)  LR: 3.037e-04  Data: 0.011 (0.012)
Train: 190 [1150/1251 ( 92%)]  Loss: 3.360 (3.38)  Time: 0.998s, 1025.71/s  (1.005s, 1018.73/s)  LR: 3.037e-04  Data: 0.012 (0.012)
Train: 190 [1200/1251 ( 96%)]  Loss: 3.593 (3.39)  Time: 0.996s, 1028.06/s  (1.005s, 1018.98/s)  LR: 3.037e-04  Data: 0.011 (0.012)
Train: 190 [1250/1251 (100%)]  Loss: 3.904 (3.41)  Time: 0.985s, 1039.63/s  (1.005s, 1019.01/s)  LR: 3.037e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.615 (1.615)  Loss:  0.7806 (0.7806)  Acc@1: 91.2109 (91.2109)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.245 (0.570)  Loss:  0.8388 (1.2226)  Acc@1: 85.8491 (76.7240)  Acc@5: 96.1085 (93.5640)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-187.pth.tar', 76.79200000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-190.pth.tar', 76.72400005859375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-185.pth.tar', 76.50599996337891)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-188.pth.tar', 76.43200013671876)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-189.pth.tar', 76.40200009033204)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-179.pth.tar', 76.33200016357422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-183.pth.tar', 76.24000008544922)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-184.pth.tar', 76.22999992675781)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-186.pth.tar', 76.1960000366211)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-180.pth.tar', 76.15399995605469)

Train: 191 [   0/1251 (  0%)]  Loss: 3.406 (3.41)  Time: 2.561s,  399.81/s  (2.561s,  399.81/s)  LR: 2.989e-04  Data: 1.603 (1.603)
Train: 191 [  50/1251 (  4%)]  Loss: 3.459 (3.43)  Time: 0.995s, 1029.63/s  (1.030s,  993.94/s)  LR: 2.989e-04  Data: 0.010 (0.042)
Train: 191 [ 100/1251 (  8%)]  Loss: 3.219 (3.36)  Time: 1.017s, 1006.88/s  (1.018s, 1006.32/s)  LR: 2.989e-04  Data: 0.011 (0.027)
Train: 191 [ 150/1251 ( 12%)]  Loss: 3.457 (3.39)  Time: 0.995s, 1029.03/s  (1.012s, 1011.71/s)  LR: 2.989e-04  Data: 0.011 (0.022)
Train: 191 [ 200/1251 ( 16%)]  Loss: 3.181 (3.34)  Time: 1.017s, 1007.35/s  (1.010s, 1013.51/s)  LR: 2.989e-04  Data: 0.015 (0.019)
Train: 191 [ 250/1251 ( 20%)]  Loss: 3.302 (3.34)  Time: 1.047s,  977.72/s  (1.011s, 1013.06/s)  LR: 2.989e-04  Data: 0.012 (0.017)
Train: 191 [ 300/1251 ( 24%)]  Loss: 3.256 (3.33)  Time: 0.996s, 1028.24/s  (1.012s, 1012.12/s)  LR: 2.989e-04  Data: 0.012 (0.016)
Train: 191 [ 350/1251 ( 28%)]  Loss: 3.325 (3.33)  Time: 0.997s, 1027.58/s  (1.011s, 1013.34/s)  LR: 2.989e-04  Data: 0.012 (0.016)
Train: 191 [ 400/1251 ( 32%)]  Loss: 3.398 (3.33)  Time: 1.000s, 1024.03/s  (1.009s, 1014.62/s)  LR: 2.989e-04  Data: 0.011 (0.015)
Train: 191 [ 450/1251 ( 36%)]  Loss: 3.342 (3.33)  Time: 1.002s, 1021.64/s  (1.008s, 1015.38/s)  LR: 2.989e-04  Data: 0.011 (0.015)
Train: 191 [ 500/1251 ( 40%)]  Loss: 3.804 (3.38)  Time: 0.999s, 1024.97/s  (1.007s, 1016.51/s)  LR: 2.989e-04  Data: 0.011 (0.014)
Train: 191 [ 550/1251 ( 44%)]  Loss: 3.005 (3.35)  Time: 0.993s, 1031.52/s  (1.007s, 1017.22/s)  LR: 2.989e-04  Data: 0.011 (0.014)
Train: 191 [ 600/1251 ( 48%)]  Loss: 3.118 (3.33)  Time: 1.003s, 1020.44/s  (1.006s, 1017.97/s)  LR: 2.989e-04  Data: 0.011 (0.014)
Train: 191 [ 650/1251 ( 52%)]  Loss: 3.396 (3.33)  Time: 0.993s, 1031.24/s  (1.006s, 1018.24/s)  LR: 2.989e-04  Data: 0.011 (0.014)
Train: 191 [ 700/1251 ( 56%)]  Loss: 3.389 (3.34)  Time: 1.057s,  968.75/s  (1.008s, 1016.37/s)  LR: 2.989e-04  Data: 0.010 (0.013)
Train: 191 [ 750/1251 ( 60%)]  Loss: 3.618 (3.35)  Time: 0.993s, 1031.60/s  (1.008s, 1015.88/s)  LR: 2.989e-04  Data: 0.011 (0.013)
Train: 191 [ 800/1251 ( 64%)]  Loss: 3.175 (3.34)  Time: 1.003s, 1020.50/s  (1.008s, 1016.22/s)  LR: 2.989e-04  Data: 0.010 (0.013)
Train: 191 [ 850/1251 ( 68%)]  Loss: 3.591 (3.36)  Time: 0.993s, 1031.38/s  (1.007s, 1016.61/s)  LR: 2.989e-04  Data: 0.011 (0.013)
Train: 191 [ 900/1251 ( 72%)]  Loss: 3.747 (3.38)  Time: 1.002s, 1021.60/s  (1.007s, 1017.00/s)  LR: 2.989e-04  Data: 0.014 (0.013)
Train: 191 [ 950/1251 ( 76%)]  Loss: 3.379 (3.38)  Time: 1.036s,  988.83/s  (1.007s, 1016.51/s)  LR: 2.989e-04  Data: 0.012 (0.013)
Train: 191 [1000/1251 ( 80%)]  Loss: 3.364 (3.38)  Time: 0.995s, 1029.10/s  (1.008s, 1015.65/s)  LR: 2.989e-04  Data: 0.010 (0.013)
Train: 191 [1050/1251 ( 84%)]  Loss: 3.649 (3.39)  Time: 1.033s,  991.07/s  (1.008s, 1015.87/s)  LR: 2.989e-04  Data: 0.011 (0.013)
Train: 191 [1100/1251 ( 88%)]  Loss: 3.071 (3.38)  Time: 1.036s,  988.58/s  (1.009s, 1014.40/s)  LR: 2.989e-04  Data: 0.012 (0.013)
Train: 191 [1150/1251 ( 92%)]  Loss: 3.591 (3.39)  Time: 1.009s, 1014.53/s  (1.010s, 1014.05/s)  LR: 2.989e-04  Data: 0.012 (0.013)
Train: 191 [1200/1251 ( 96%)]  Loss: 3.090 (3.37)  Time: 0.998s, 1025.82/s  (1.009s, 1014.40/s)  LR: 2.989e-04  Data: 0.011 (0.013)
Train: 191 [1250/1251 (100%)]  Loss: 3.730 (3.39)  Time: 0.986s, 1038.20/s  (1.009s, 1014.81/s)  LR: 2.989e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.629 (1.629)  Loss:  0.7942 (0.7942)  Acc@1: 89.4531 (89.4531)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.245 (0.568)  Loss:  0.8681 (1.2630)  Acc@1: 84.9057 (76.1300)  Acc@5: 96.9340 (93.2760)
Train: 192 [   0/1251 (  0%)]  Loss: 3.293 (3.29)  Time: 4.568s,  224.19/s  (4.568s,  224.19/s)  LR: 2.942e-04  Data: 3.429 (3.429)
Train: 192 [  50/1251 (  4%)]  Loss: 3.207 (3.25)  Time: 0.996s, 1028.27/s  (1.080s,  948.55/s)  LR: 2.942e-04  Data: 0.011 (0.078)
Train: 192 [ 100/1251 (  8%)]  Loss: 3.630 (3.38)  Time: 0.996s, 1028.62/s  (1.041s,  983.95/s)  LR: 2.942e-04  Data: 0.012 (0.045)
Train: 192 [ 150/1251 ( 12%)]  Loss: 3.395 (3.38)  Time: 1.062s,  964.31/s  (1.038s,  986.69/s)  LR: 2.942e-04  Data: 0.012 (0.034)
Train: 192 [ 200/1251 ( 16%)]  Loss: 3.772 (3.46)  Time: 0.999s, 1025.05/s  (1.030s,  993.98/s)  LR: 2.942e-04  Data: 0.012 (0.028)
Train: 192 [ 250/1251 ( 20%)]  Loss: 3.346 (3.44)  Time: 0.995s, 1029.65/s  (1.024s,  999.83/s)  LR: 2.942e-04  Data: 0.010 (0.025)
Train: 192 [ 300/1251 ( 24%)]  Loss: 3.010 (3.38)  Time: 0.995s, 1029.19/s  (1.020s, 1003.54/s)  LR: 2.942e-04  Data: 0.011 (0.023)
Train: 192 [ 350/1251 ( 28%)]  Loss: 3.276 (3.37)  Time: 1.007s, 1017.01/s  (1.018s, 1006.26/s)  LR: 2.942e-04  Data: 0.012 (0.021)
Train: 192 [ 400/1251 ( 32%)]  Loss: 3.433 (3.37)  Time: 0.997s, 1026.86/s  (1.015s, 1008.45/s)  LR: 2.942e-04  Data: 0.011 (0.020)
Train: 192 [ 450/1251 ( 36%)]  Loss: 3.041 (3.34)  Time: 0.998s, 1025.68/s  (1.014s, 1009.90/s)  LR: 2.942e-04  Data: 0.011 (0.019)
Train: 192 [ 500/1251 ( 40%)]  Loss: 3.723 (3.38)  Time: 1.000s, 1023.51/s  (1.013s, 1010.76/s)  LR: 2.942e-04  Data: 0.011 (0.018)
Train: 192 [ 550/1251 ( 44%)]  Loss: 3.610 (3.39)  Time: 1.006s, 1017.47/s  (1.012s, 1011.91/s)  LR: 2.942e-04  Data: 0.012 (0.017)
Train: 192 [ 600/1251 ( 48%)]  Loss: 3.479 (3.40)  Time: 1.009s, 1014.51/s  (1.011s, 1013.16/s)  LR: 2.942e-04  Data: 0.011 (0.017)
Train: 192 [ 650/1251 ( 52%)]  Loss: 3.393 (3.40)  Time: 1.024s, 1000.01/s  (1.011s, 1012.40/s)  LR: 2.942e-04  Data: 0.011 (0.016)
Train: 192 [ 700/1251 ( 56%)]  Loss: 3.531 (3.41)  Time: 0.996s, 1028.31/s  (1.011s, 1012.68/s)  LR: 2.942e-04  Data: 0.011 (0.016)
Train: 192 [ 750/1251 ( 60%)]  Loss: 3.186 (3.40)  Time: 1.000s, 1024.11/s  (1.011s, 1013.33/s)  LR: 2.942e-04  Data: 0.012 (0.016)
Train: 192 [ 800/1251 ( 64%)]  Loss: 3.050 (3.37)  Time: 0.998s, 1026.46/s  (1.010s, 1014.06/s)  LR: 2.942e-04  Data: 0.012 (0.015)
Train: 192 [ 850/1251 ( 68%)]  Loss: 3.571 (3.39)  Time: 1.001s, 1023.45/s  (1.009s, 1014.69/s)  LR: 2.942e-04  Data: 0.011 (0.015)
Train: 192 [ 900/1251 ( 72%)]  Loss: 3.375 (3.39)  Time: 0.996s, 1028.05/s  (1.009s, 1015.10/s)  LR: 2.942e-04  Data: 0.011 (0.015)
Train: 192 [ 950/1251 ( 76%)]  Loss: 3.269 (3.38)  Time: 0.997s, 1026.65/s  (1.008s, 1015.58/s)  LR: 2.942e-04  Data: 0.011 (0.015)
Train: 192 [1000/1251 ( 80%)]  Loss: 3.548 (3.39)  Time: 0.999s, 1025.09/s  (1.008s, 1015.99/s)  LR: 2.942e-04  Data: 0.011 (0.015)
Train: 192 [1050/1251 ( 84%)]  Loss: 3.566 (3.40)  Time: 0.996s, 1028.34/s  (1.007s, 1016.47/s)  LR: 2.942e-04  Data: 0.011 (0.014)
Train: 192 [1100/1251 ( 88%)]  Loss: 3.450 (3.40)  Time: 1.003s, 1020.72/s  (1.007s, 1016.45/s)  LR: 2.942e-04  Data: 0.012 (0.014)
Train: 192 [1150/1251 ( 92%)]  Loss: 3.323 (3.39)  Time: 0.994s, 1030.42/s  (1.007s, 1016.43/s)  LR: 2.942e-04  Data: 0.011 (0.014)
Train: 192 [1200/1251 ( 96%)]  Loss: 3.105 (3.38)  Time: 0.996s, 1028.07/s  (1.007s, 1016.64/s)  LR: 2.942e-04  Data: 0.012 (0.014)
Train: 192 [1250/1251 (100%)]  Loss: 3.450 (3.39)  Time: 0.992s, 1032.54/s  (1.007s, 1016.93/s)  LR: 2.942e-04  Data: 0.000 (0.014)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.684 (1.684)  Loss:  0.7766 (0.7766)  Acc@1: 90.1367 (90.1367)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.245 (0.577)  Loss:  0.8573 (1.2728)  Acc@1: 85.1415 (76.7160)  Acc@5: 97.0519 (93.4700)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-187.pth.tar', 76.79200000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-190.pth.tar', 76.72400005859375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-192.pth.tar', 76.71600000976562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-185.pth.tar', 76.50599996337891)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-188.pth.tar', 76.43200013671876)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-189.pth.tar', 76.40200009033204)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-179.pth.tar', 76.33200016357422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-183.pth.tar', 76.24000008544922)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-184.pth.tar', 76.22999992675781)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-186.pth.tar', 76.1960000366211)

Train: 193 [   0/1251 (  0%)]  Loss: 3.335 (3.34)  Time: 2.571s,  398.26/s  (2.571s,  398.26/s)  LR: 2.896e-04  Data: 1.613 (1.613)
Train: 193 [  50/1251 (  4%)]  Loss: 3.427 (3.38)  Time: 0.997s, 1026.96/s  (1.034s,  989.86/s)  LR: 2.896e-04  Data: 0.011 (0.043)
Train: 193 [ 100/1251 (  8%)]  Loss: 3.230 (3.33)  Time: 1.000s, 1024.12/s  (1.021s, 1003.00/s)  LR: 2.896e-04  Data: 0.011 (0.028)
Train: 193 [ 150/1251 ( 12%)]  Loss: 3.309 (3.33)  Time: 0.997s, 1027.02/s  (1.014s, 1009.89/s)  LR: 2.896e-04  Data: 0.011 (0.022)
Train: 193 [ 200/1251 ( 16%)]  Loss: 3.426 (3.35)  Time: 1.004s, 1019.89/s  (1.010s, 1013.78/s)  LR: 2.896e-04  Data: 0.012 (0.020)
Train: 193 [ 250/1251 ( 20%)]  Loss: 3.259 (3.33)  Time: 1.034s,  990.54/s  (1.012s, 1012.00/s)  LR: 2.896e-04  Data: 0.010 (0.018)
Train: 193 [ 300/1251 ( 24%)]  Loss: 3.426 (3.34)  Time: 1.063s,  963.20/s  (1.018s, 1006.25/s)  LR: 2.896e-04  Data: 0.011 (0.017)
Train: 193 [ 350/1251 ( 28%)]  Loss: 3.630 (3.38)  Time: 1.034s,  990.25/s  (1.018s, 1005.55/s)  LR: 2.896e-04  Data: 0.011 (0.016)
Train: 193 [ 400/1251 ( 32%)]  Loss: 3.921 (3.44)  Time: 0.997s, 1027.18/s  (1.017s, 1006.78/s)  LR: 2.896e-04  Data: 0.011 (0.015)
Train: 193 [ 450/1251 ( 36%)]  Loss: 3.245 (3.42)  Time: 0.996s, 1028.37/s  (1.016s, 1008.17/s)  LR: 2.896e-04  Data: 0.011 (0.015)
Train: 193 [ 500/1251 ( 40%)]  Loss: 3.879 (3.46)  Time: 0.996s, 1027.90/s  (1.014s, 1009.43/s)  LR: 2.896e-04  Data: 0.011 (0.015)
Train: 193 [ 550/1251 ( 44%)]  Loss: 3.518 (3.47)  Time: 0.999s, 1025.49/s  (1.013s, 1010.46/s)  LR: 2.896e-04  Data: 0.011 (0.014)
Train: 193 [ 600/1251 ( 48%)]  Loss: 3.411 (3.46)  Time: 1.032s,  991.78/s  (1.013s, 1011.35/s)  LR: 2.896e-04  Data: 0.011 (0.014)
Train: 193 [ 650/1251 ( 52%)]  Loss: 3.266 (3.45)  Time: 0.994s, 1030.65/s  (1.012s, 1011.88/s)  LR: 2.896e-04  Data: 0.011 (0.014)
Train: 193 [ 700/1251 ( 56%)]  Loss: 3.403 (3.45)  Time: 1.001s, 1023.15/s  (1.011s, 1012.59/s)  LR: 2.896e-04  Data: 0.011 (0.014)
Train: 193 [ 750/1251 ( 60%)]  Loss: 3.773 (3.47)  Time: 0.995s, 1029.51/s  (1.011s, 1013.22/s)  LR: 2.896e-04  Data: 0.011 (0.013)
Train: 193 [ 800/1251 ( 64%)]  Loss: 3.211 (3.45)  Time: 0.995s, 1028.76/s  (1.010s, 1013.75/s)  LR: 2.896e-04  Data: 0.012 (0.013)
Train: 193 [ 850/1251 ( 68%)]  Loss: 3.307 (3.44)  Time: 0.994s, 1030.19/s  (1.009s, 1014.41/s)  LR: 2.896e-04  Data: 0.011 (0.013)
Train: 193 [ 900/1251 ( 72%)]  Loss: 3.904 (3.47)  Time: 1.005s, 1018.91/s  (1.009s, 1014.90/s)  LR: 2.896e-04  Data: 0.011 (0.013)
Train: 193 [ 950/1251 ( 76%)]  Loss: 3.623 (3.48)  Time: 0.995s, 1029.05/s  (1.008s, 1015.54/s)  LR: 2.896e-04  Data: 0.011 (0.013)
Train: 193 [1000/1251 ( 80%)]  Loss: 3.611 (3.48)  Time: 1.038s,  986.16/s  (1.010s, 1013.83/s)  LR: 2.896e-04  Data: 0.011 (0.013)
Train: 193 [1050/1251 ( 84%)]  Loss: 3.620 (3.49)  Time: 0.992s, 1032.63/s  (1.010s, 1013.37/s)  LR: 2.896e-04  Data: 0.011 (0.013)
Train: 193 [1100/1251 ( 88%)]  Loss: 3.526 (3.49)  Time: 0.996s, 1028.24/s  (1.010s, 1013.74/s)  LR: 2.896e-04  Data: 0.011 (0.013)
Train: 193 [1150/1251 ( 92%)]  Loss: 3.694 (3.50)  Time: 1.002s, 1022.27/s  (1.010s, 1014.30/s)  LR: 2.896e-04  Data: 0.011 (0.013)
Train: 193 [1200/1251 ( 96%)]  Loss: 3.463 (3.50)  Time: 0.998s, 1026.45/s  (1.009s, 1014.75/s)  LR: 2.896e-04  Data: 0.010 (0.012)
Train: 193 [1250/1251 (100%)]  Loss: 3.362 (3.49)  Time: 1.010s, 1014.35/s  (1.009s, 1015.20/s)  LR: 2.896e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.621 (1.621)  Loss:  0.7519 (0.7519)  Acc@1: 89.5508 (89.5508)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.245 (0.579)  Loss:  0.8426 (1.2363)  Acc@1: 85.0236 (76.8280)  Acc@5: 96.9340 (93.4680)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-193.pth.tar', 76.8279999584961)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-187.pth.tar', 76.79200000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-190.pth.tar', 76.72400005859375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-192.pth.tar', 76.71600000976562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-185.pth.tar', 76.50599996337891)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-188.pth.tar', 76.43200013671876)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-189.pth.tar', 76.40200009033204)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-179.pth.tar', 76.33200016357422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-183.pth.tar', 76.24000008544922)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-184.pth.tar', 76.22999992675781)

Train: 194 [   0/1251 (  0%)]  Loss: 3.152 (3.15)  Time: 2.622s,  390.53/s  (2.622s,  390.53/s)  LR: 2.849e-04  Data: 1.652 (1.652)
Train: 194 [  50/1251 (  4%)]  Loss: 3.332 (3.24)  Time: 0.996s, 1027.74/s  (1.034s,  989.95/s)  LR: 2.849e-04  Data: 0.012 (0.044)
Train: 194 [ 100/1251 (  8%)]  Loss: 3.041 (3.18)  Time: 0.996s, 1028.42/s  (1.018s, 1005.60/s)  LR: 2.849e-04  Data: 0.012 (0.028)
Train: 194 [ 150/1251 ( 12%)]  Loss: 3.175 (3.18)  Time: 0.998s, 1025.71/s  (1.014s, 1009.57/s)  LR: 2.849e-04  Data: 0.011 (0.022)
Train: 194 [ 200/1251 ( 16%)]  Loss: 3.615 (3.26)  Time: 1.001s, 1023.30/s  (1.012s, 1011.95/s)  LR: 2.849e-04  Data: 0.013 (0.020)
Train: 194 [ 250/1251 ( 20%)]  Loss: 3.345 (3.28)  Time: 0.994s, 1030.06/s  (1.014s, 1010.03/s)  LR: 2.849e-04  Data: 0.012 (0.018)
Train: 194 [ 300/1251 ( 24%)]  Loss: 3.073 (3.25)  Time: 0.993s, 1030.95/s  (1.012s, 1012.10/s)  LR: 2.849e-04  Data: 0.011 (0.017)
Train: 194 [ 350/1251 ( 28%)]  Loss: 3.527 (3.28)  Time: 1.003s, 1020.64/s  (1.012s, 1011.93/s)  LR: 2.849e-04  Data: 0.011 (0.016)
Train: 194 [ 400/1251 ( 32%)]  Loss: 3.363 (3.29)  Time: 0.995s, 1028.79/s  (1.011s, 1013.17/s)  LR: 2.849e-04  Data: 0.011 (0.015)
Train: 194 [ 450/1251 ( 36%)]  Loss: 3.336 (3.30)  Time: 0.994s, 1030.04/s  (1.012s, 1012.32/s)  LR: 2.849e-04  Data: 0.012 (0.015)
Train: 194 [ 500/1251 ( 40%)]  Loss: 3.354 (3.30)  Time: 0.995s, 1028.72/s  (1.011s, 1013.33/s)  LR: 2.849e-04  Data: 0.012 (0.015)
Train: 194 [ 550/1251 ( 44%)]  Loss: 3.359 (3.31)  Time: 1.000s, 1024.13/s  (1.010s, 1013.57/s)  LR: 2.849e-04  Data: 0.010 (0.014)
Train: 194 [ 600/1251 ( 48%)]  Loss: 3.071 (3.29)  Time: 0.995s, 1028.83/s  (1.010s, 1013.73/s)  LR: 2.849e-04  Data: 0.011 (0.014)
Train: 194 [ 650/1251 ( 52%)]  Loss: 2.895 (3.26)  Time: 0.996s, 1028.24/s  (1.009s, 1014.58/s)  LR: 2.849e-04  Data: 0.012 (0.014)
Train: 194 [ 700/1251 ( 56%)]  Loss: 3.632 (3.28)  Time: 0.996s, 1027.62/s  (1.009s, 1015.28/s)  LR: 2.849e-04  Data: 0.012 (0.014)
Train: 194 [ 750/1251 ( 60%)]  Loss: 3.551 (3.30)  Time: 0.992s, 1032.18/s  (1.008s, 1015.80/s)  LR: 2.849e-04  Data: 0.011 (0.014)
Train: 194 [ 800/1251 ( 64%)]  Loss: 3.511 (3.31)  Time: 0.997s, 1027.15/s  (1.007s, 1016.38/s)  LR: 2.849e-04  Data: 0.012 (0.013)
Train: 194 [ 850/1251 ( 68%)]  Loss: 3.742 (3.34)  Time: 1.000s, 1024.48/s  (1.008s, 1016.33/s)  LR: 2.849e-04  Data: 0.012 (0.013)
Train: 194 [ 900/1251 ( 72%)]  Loss: 3.503 (3.35)  Time: 1.048s,  976.66/s  (1.007s, 1016.68/s)  LR: 2.849e-04  Data: 0.012 (0.013)
Train: 194 [ 950/1251 ( 76%)]  Loss: 3.459 (3.35)  Time: 1.012s, 1011.51/s  (1.007s, 1017.02/s)  LR: 2.849e-04  Data: 0.011 (0.013)
Train: 194 [1000/1251 ( 80%)]  Loss: 3.551 (3.36)  Time: 1.009s, 1014.71/s  (1.006s, 1017.41/s)  LR: 2.849e-04  Data: 0.010 (0.013)
Train: 194 [1050/1251 ( 84%)]  Loss: 3.513 (3.37)  Time: 1.040s,  984.47/s  (1.007s, 1017.33/s)  LR: 2.849e-04  Data: 0.011 (0.013)
Train: 194 [1100/1251 ( 88%)]  Loss: 3.736 (3.38)  Time: 1.005s, 1019.37/s  (1.006s, 1017.57/s)  LR: 2.849e-04  Data: 0.011 (0.013)
Train: 194 [1150/1251 ( 92%)]  Loss: 3.341 (3.38)  Time: 0.995s, 1029.48/s  (1.006s, 1017.50/s)  LR: 2.849e-04  Data: 0.011 (0.013)
Train: 194 [1200/1251 ( 96%)]  Loss: 3.236 (3.38)  Time: 1.058s,  968.05/s  (1.006s, 1017.73/s)  LR: 2.849e-04  Data: 0.011 (0.013)
Train: 194 [1250/1251 (100%)]  Loss: 3.366 (3.38)  Time: 0.986s, 1038.96/s  (1.007s, 1017.18/s)  LR: 2.849e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.636 (1.636)  Loss:  0.8479 (0.8479)  Acc@1: 90.7227 (90.7227)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.245 (0.569)  Loss:  0.8700 (1.2658)  Acc@1: 85.2594 (76.6060)  Acc@5: 97.0519 (93.5680)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-193.pth.tar', 76.8279999584961)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-187.pth.tar', 76.79200000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-190.pth.tar', 76.72400005859375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-192.pth.tar', 76.71600000976562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-194.pth.tar', 76.60600006103516)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-185.pth.tar', 76.50599996337891)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-188.pth.tar', 76.43200013671876)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-189.pth.tar', 76.40200009033204)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-179.pth.tar', 76.33200016357422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-183.pth.tar', 76.24000008544922)

Train: 195 [   0/1251 (  0%)]  Loss: 3.437 (3.44)  Time: 2.481s,  412.66/s  (2.481s,  412.66/s)  LR: 2.803e-04  Data: 1.516 (1.516)
Train: 195 [  50/1251 (  4%)]  Loss: 3.390 (3.41)  Time: 0.998s, 1026.37/s  (1.036s,  988.44/s)  LR: 2.803e-04  Data: 0.011 (0.041)
Train: 195 [ 100/1251 (  8%)]  Loss: 3.505 (3.44)  Time: 0.998s, 1026.46/s  (1.020s, 1004.30/s)  LR: 2.803e-04  Data: 0.011 (0.026)
Train: 195 [ 150/1251 ( 12%)]  Loss: 3.309 (3.41)  Time: 0.995s, 1029.61/s  (1.014s, 1010.10/s)  LR: 2.803e-04  Data: 0.012 (0.021)
Train: 195 [ 200/1251 ( 16%)]  Loss: 2.975 (3.32)  Time: 0.996s, 1028.24/s  (1.010s, 1013.42/s)  LR: 2.803e-04  Data: 0.011 (0.019)
Train: 195 [ 250/1251 ( 20%)]  Loss: 3.200 (3.30)  Time: 0.997s, 1026.76/s  (1.008s, 1015.42/s)  LR: 2.803e-04  Data: 0.011 (0.017)
Train: 195 [ 300/1251 ( 24%)]  Loss: 2.983 (3.26)  Time: 0.996s, 1028.41/s  (1.007s, 1016.53/s)  LR: 2.803e-04  Data: 0.010 (0.016)
Train: 195 [ 350/1251 ( 28%)]  Loss: 3.040 (3.23)  Time: 1.003s, 1021.21/s  (1.007s, 1016.95/s)  LR: 2.803e-04  Data: 0.010 (0.015)
Train: 195 [ 400/1251 ( 32%)]  Loss: 3.316 (3.24)  Time: 0.995s, 1029.30/s  (1.006s, 1017.83/s)  LR: 2.803e-04  Data: 0.010 (0.015)
Train: 195 [ 450/1251 ( 36%)]  Loss: 3.241 (3.24)  Time: 0.994s, 1029.78/s  (1.005s, 1018.61/s)  LR: 2.803e-04  Data: 0.012 (0.015)
Train: 195 [ 500/1251 ( 40%)]  Loss: 3.463 (3.26)  Time: 1.001s, 1022.59/s  (1.005s, 1019.40/s)  LR: 2.803e-04  Data: 0.011 (0.014)
Train: 195 [ 550/1251 ( 44%)]  Loss: 3.220 (3.26)  Time: 0.998s, 1025.79/s  (1.004s, 1020.10/s)  LR: 2.803e-04  Data: 0.012 (0.014)
Train: 195 [ 600/1251 ( 48%)]  Loss: 3.316 (3.26)  Time: 0.995s, 1028.96/s  (1.003s, 1020.63/s)  LR: 2.803e-04  Data: 0.012 (0.014)
Train: 195 [ 650/1251 ( 52%)]  Loss: 3.263 (3.26)  Time: 1.056s,  969.88/s  (1.004s, 1020.27/s)  LR: 2.803e-04  Data: 0.011 (0.014)
Train: 195 [ 700/1251 ( 56%)]  Loss: 3.430 (3.27)  Time: 0.997s, 1027.57/s  (1.004s, 1019.85/s)  LR: 2.803e-04  Data: 0.011 (0.013)
Train: 195 [ 750/1251 ( 60%)]  Loss: 3.269 (3.27)  Time: 0.998s, 1026.22/s  (1.004s, 1020.25/s)  LR: 2.803e-04  Data: 0.011 (0.013)
Train: 195 [ 800/1251 ( 64%)]  Loss: 3.363 (3.28)  Time: 1.038s,  986.76/s  (1.005s, 1019.33/s)  LR: 2.803e-04  Data: 0.011 (0.013)
Train: 195 [ 850/1251 ( 68%)]  Loss: 3.325 (3.28)  Time: 0.990s, 1033.87/s  (1.004s, 1019.50/s)  LR: 2.803e-04  Data: 0.011 (0.013)
Train: 195 [ 900/1251 ( 72%)]  Loss: 3.038 (3.27)  Time: 1.032s,  992.72/s  (1.006s, 1018.02/s)  LR: 2.803e-04  Data: 0.010 (0.013)
Train: 195 [ 950/1251 ( 76%)]  Loss: 3.125 (3.26)  Time: 1.046s,  979.22/s  (1.007s, 1016.86/s)  LR: 2.803e-04  Data: 0.011 (0.013)
Train: 195 [1000/1251 ( 80%)]  Loss: 3.422 (3.27)  Time: 0.996s, 1027.70/s  (1.007s, 1017.02/s)  LR: 2.803e-04  Data: 0.012 (0.013)
Train: 195 [1050/1251 ( 84%)]  Loss: 3.640 (3.28)  Time: 1.032s,  991.77/s  (1.007s, 1017.18/s)  LR: 2.803e-04  Data: 0.010 (0.013)
Train: 195 [1100/1251 ( 88%)]  Loss: 3.614 (3.30)  Time: 0.999s, 1024.94/s  (1.006s, 1017.42/s)  LR: 2.803e-04  Data: 0.011 (0.013)
Train: 195 [1150/1251 ( 92%)]  Loss: 3.406 (3.30)  Time: 0.995s, 1029.53/s  (1.006s, 1017.70/s)  LR: 2.803e-04  Data: 0.012 (0.013)
Train: 195 [1200/1251 ( 96%)]  Loss: 3.618 (3.32)  Time: 0.997s, 1027.24/s  (1.006s, 1017.85/s)  LR: 2.803e-04  Data: 0.010 (0.012)
Train: 195 [1250/1251 (100%)]  Loss: 3.312 (3.32)  Time: 0.987s, 1037.41/s  (1.006s, 1018.11/s)  LR: 2.803e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.636 (1.636)  Loss:  0.7110 (0.7110)  Acc@1: 90.1367 (90.1367)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.245 (0.575)  Loss:  0.7885 (1.2112)  Acc@1: 86.9104 (76.4920)  Acc@5: 96.8160 (93.4100)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-193.pth.tar', 76.8279999584961)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-187.pth.tar', 76.79200000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-190.pth.tar', 76.72400005859375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-192.pth.tar', 76.71600000976562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-194.pth.tar', 76.60600006103516)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-185.pth.tar', 76.50599996337891)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-195.pth.tar', 76.4920000024414)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-188.pth.tar', 76.43200013671876)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-189.pth.tar', 76.40200009033204)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-179.pth.tar', 76.33200016357422)

Train: 196 [   0/1251 (  0%)]  Loss: 3.705 (3.71)  Time: 2.572s,  398.09/s  (2.572s,  398.09/s)  LR: 2.757e-04  Data: 1.562 (1.562)
Train: 196 [  50/1251 (  4%)]  Loss: 3.320 (3.51)  Time: 0.997s, 1026.89/s  (1.040s,  984.52/s)  LR: 2.757e-04  Data: 0.012 (0.042)
Train: 196 [ 100/1251 (  8%)]  Loss: 3.395 (3.47)  Time: 0.997s, 1026.68/s  (1.021s, 1003.34/s)  LR: 2.757e-04  Data: 0.011 (0.027)
Train: 196 [ 150/1251 ( 12%)]  Loss: 3.513 (3.48)  Time: 0.997s, 1026.65/s  (1.013s, 1010.43/s)  LR: 2.757e-04  Data: 0.011 (0.021)
Train: 196 [ 200/1251 ( 16%)]  Loss: 3.590 (3.50)  Time: 0.996s, 1028.60/s  (1.009s, 1014.38/s)  LR: 2.757e-04  Data: 0.010 (0.019)
Train: 196 [ 250/1251 ( 20%)]  Loss: 3.243 (3.46)  Time: 0.997s, 1026.73/s  (1.007s, 1016.77/s)  LR: 2.757e-04  Data: 0.011 (0.017)
Train: 196 [ 300/1251 ( 24%)]  Loss: 3.441 (3.46)  Time: 0.994s, 1030.67/s  (1.006s, 1017.78/s)  LR: 2.757e-04  Data: 0.011 (0.016)
Train: 196 [ 350/1251 ( 28%)]  Loss: 3.225 (3.43)  Time: 0.993s, 1031.54/s  (1.006s, 1018.27/s)  LR: 2.757e-04  Data: 0.011 (0.015)
Train: 196 [ 400/1251 ( 32%)]  Loss: 3.400 (3.43)  Time: 0.998s, 1026.54/s  (1.006s, 1018.29/s)  LR: 2.757e-04  Data: 0.011 (0.015)
Train: 196 [ 450/1251 ( 36%)]  Loss: 3.232 (3.41)  Time: 0.996s, 1027.76/s  (1.005s, 1019.12/s)  LR: 2.757e-04  Data: 0.012 (0.015)
Train: 196 [ 500/1251 ( 40%)]  Loss: 3.312 (3.40)  Time: 0.995s, 1028.75/s  (1.004s, 1019.72/s)  LR: 2.757e-04  Data: 0.011 (0.014)
Train: 196 [ 550/1251 ( 44%)]  Loss: 3.587 (3.41)  Time: 1.006s, 1017.78/s  (1.004s, 1019.86/s)  LR: 2.757e-04  Data: 0.013 (0.014)
Train: 196 [ 600/1251 ( 48%)]  Loss: 3.415 (3.41)  Time: 1.000s, 1024.45/s  (1.004s, 1020.19/s)  LR: 2.757e-04  Data: 0.012 (0.014)
Train: 196 [ 650/1251 ( 52%)]  Loss: 3.448 (3.42)  Time: 0.996s, 1028.07/s  (1.004s, 1020.41/s)  LR: 2.757e-04  Data: 0.012 (0.014)
Train: 196 [ 700/1251 ( 56%)]  Loss: 3.141 (3.40)  Time: 0.996s, 1028.06/s  (1.003s, 1020.50/s)  LR: 2.757e-04  Data: 0.012 (0.013)
Train: 196 [ 750/1251 ( 60%)]  Loss: 3.382 (3.40)  Time: 1.007s, 1016.84/s  (1.004s, 1019.63/s)  LR: 2.757e-04  Data: 0.011 (0.013)
Train: 196 [ 800/1251 ( 64%)]  Loss: 3.484 (3.40)  Time: 0.993s, 1030.92/s  (1.004s, 1019.80/s)  LR: 2.757e-04  Data: 0.011 (0.013)
Train: 196 [ 850/1251 ( 68%)]  Loss: 3.767 (3.42)  Time: 0.998s, 1026.45/s  (1.005s, 1018.82/s)  LR: 2.757e-04  Data: 0.012 (0.013)
Train: 196 [ 900/1251 ( 72%)]  Loss: 3.295 (3.42)  Time: 0.997s, 1027.49/s  (1.005s, 1019.14/s)  LR: 2.757e-04  Data: 0.011 (0.013)
Train: 196 [ 950/1251 ( 76%)]  Loss: 3.595 (3.42)  Time: 0.995s, 1028.69/s  (1.006s, 1018.23/s)  LR: 2.757e-04  Data: 0.011 (0.013)
Train: 196 [1000/1251 ( 80%)]  Loss: 3.130 (3.41)  Time: 1.032s,  992.21/s  (1.006s, 1018.36/s)  LR: 2.757e-04  Data: 0.011 (0.013)
Train: 196 [1050/1251 ( 84%)]  Loss: 3.354 (3.41)  Time: 0.999s, 1025.44/s  (1.006s, 1018.12/s)  LR: 2.757e-04  Data: 0.011 (0.013)
Train: 196 [1100/1251 ( 88%)]  Loss: 3.132 (3.40)  Time: 0.995s, 1029.65/s  (1.005s, 1018.44/s)  LR: 2.757e-04  Data: 0.012 (0.013)
Train: 196 [1150/1251 ( 92%)]  Loss: 3.393 (3.40)  Time: 1.045s,  980.04/s  (1.006s, 1018.12/s)  LR: 2.757e-04  Data: 0.011 (0.013)
Train: 196 [1200/1251 ( 96%)]  Loss: 3.588 (3.40)  Time: 0.994s, 1030.54/s  (1.006s, 1018.20/s)  LR: 2.757e-04  Data: 0.011 (0.013)
Train: 196 [1250/1251 (100%)]  Loss: 3.478 (3.41)  Time: 1.012s, 1011.94/s  (1.005s, 1018.44/s)  LR: 2.757e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.700 (1.700)  Loss:  0.8268 (0.8268)  Acc@1: 89.9414 (89.9414)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.245 (0.570)  Loss:  0.8954 (1.2724)  Acc@1: 85.9670 (76.7720)  Acc@5: 96.3443 (93.6220)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-193.pth.tar', 76.8279999584961)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-187.pth.tar', 76.79200000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-196.pth.tar', 76.77200010986328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-190.pth.tar', 76.72400005859375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-192.pth.tar', 76.71600000976562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-194.pth.tar', 76.60600006103516)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-185.pth.tar', 76.50599996337891)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-195.pth.tar', 76.4920000024414)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-188.pth.tar', 76.43200013671876)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-189.pth.tar', 76.40200009033204)

Train: 197 [   0/1251 (  0%)]  Loss: 3.331 (3.33)  Time: 2.459s,  416.51/s  (2.459s,  416.51/s)  LR: 2.711e-04  Data: 1.492 (1.492)
Train: 197 [  50/1251 (  4%)]  Loss: 3.544 (3.44)  Time: 1.003s, 1020.77/s  (1.032s,  991.78/s)  LR: 2.711e-04  Data: 0.012 (0.041)
Train: 197 [ 100/1251 (  8%)]  Loss: 3.451 (3.44)  Time: 1.000s, 1023.50/s  (1.018s, 1005.46/s)  LR: 2.711e-04  Data: 0.013 (0.026)
Train: 197 [ 150/1251 ( 12%)]  Loss: 3.280 (3.40)  Time: 1.040s,  985.03/s  (1.017s, 1006.52/s)  LR: 2.711e-04  Data: 0.011 (0.021)
Train: 197 [ 200/1251 ( 16%)]  Loss: 3.153 (3.35)  Time: 0.993s, 1030.81/s  (1.013s, 1010.37/s)  LR: 2.711e-04  Data: 0.011 (0.019)
Train: 197 [ 250/1251 ( 20%)]  Loss: 3.375 (3.36)  Time: 0.995s, 1028.63/s  (1.011s, 1012.74/s)  LR: 2.711e-04  Data: 0.011 (0.017)
Train: 197 [ 300/1251 ( 24%)]  Loss: 3.200 (3.33)  Time: 0.997s, 1027.16/s  (1.012s, 1012.32/s)  LR: 2.711e-04  Data: 0.012 (0.016)
Train: 197 [ 350/1251 ( 28%)]  Loss: 3.159 (3.31)  Time: 0.997s, 1027.47/s  (1.013s, 1011.22/s)  LR: 2.711e-04  Data: 0.012 (0.016)
Train: 197 [ 400/1251 ( 32%)]  Loss: 3.349 (3.32)  Time: 0.995s, 1029.10/s  (1.011s, 1012.92/s)  LR: 2.711e-04  Data: 0.012 (0.015)
Train: 197 [ 450/1251 ( 36%)]  Loss: 3.270 (3.31)  Time: 0.997s, 1026.97/s  (1.010s, 1014.30/s)  LR: 2.711e-04  Data: 0.011 (0.015)
Train: 197 [ 500/1251 ( 40%)]  Loss: 3.251 (3.31)  Time: 0.995s, 1029.07/s  (1.009s, 1015.04/s)  LR: 2.711e-04  Data: 0.011 (0.014)
Train: 197 [ 550/1251 ( 44%)]  Loss: 3.302 (3.31)  Time: 1.011s, 1012.51/s  (1.009s, 1015.17/s)  LR: 2.711e-04  Data: 0.011 (0.014)
Train: 197 [ 600/1251 ( 48%)]  Loss: 3.310 (3.31)  Time: 0.994s, 1030.22/s  (1.008s, 1015.98/s)  LR: 2.711e-04  Data: 0.012 (0.014)
Train: 197 [ 650/1251 ( 52%)]  Loss: 3.404 (3.31)  Time: 1.007s, 1017.19/s  (1.007s, 1016.53/s)  LR: 2.711e-04  Data: 0.010 (0.014)
Train: 197 [ 700/1251 ( 56%)]  Loss: 3.633 (3.33)  Time: 0.999s, 1025.47/s  (1.007s, 1016.88/s)  LR: 2.711e-04  Data: 0.011 (0.013)
Train: 197 [ 750/1251 ( 60%)]  Loss: 3.394 (3.34)  Time: 0.999s, 1025.00/s  (1.007s, 1017.28/s)  LR: 2.711e-04  Data: 0.012 (0.013)
Train: 197 [ 800/1251 ( 64%)]  Loss: 3.548 (3.35)  Time: 0.998s, 1026.05/s  (1.006s, 1017.67/s)  LR: 2.711e-04  Data: 0.011 (0.013)
Train: 197 [ 850/1251 ( 68%)]  Loss: 3.619 (3.37)  Time: 0.996s, 1028.17/s  (1.006s, 1017.98/s)  LR: 2.711e-04  Data: 0.012 (0.013)
Train: 197 [ 900/1251 ( 72%)]  Loss: 3.259 (3.36)  Time: 1.013s, 1011.07/s  (1.006s, 1018.13/s)  LR: 2.711e-04  Data: 0.012 (0.013)
Train: 197 [ 950/1251 ( 76%)]  Loss: 3.339 (3.36)  Time: 0.996s, 1028.24/s  (1.005s, 1018.55/s)  LR: 2.711e-04  Data: 0.011 (0.013)
Train: 197 [1000/1251 ( 80%)]  Loss: 3.401 (3.36)  Time: 1.002s, 1022.02/s  (1.005s, 1018.42/s)  LR: 2.711e-04  Data: 0.011 (0.013)
Train: 197 [1050/1251 ( 84%)]  Loss: 3.459 (3.36)  Time: 0.996s, 1028.38/s  (1.005s, 1018.75/s)  LR: 2.711e-04  Data: 0.012 (0.013)
Train: 197 [1100/1251 ( 88%)]  Loss: 3.119 (3.35)  Time: 1.001s, 1023.15/s  (1.005s, 1019.04/s)  LR: 2.711e-04  Data: 0.011 (0.013)
Train: 197 [1150/1251 ( 92%)]  Loss: 3.535 (3.36)  Time: 0.995s, 1028.75/s  (1.005s, 1019.19/s)  LR: 2.711e-04  Data: 0.011 (0.013)
Train: 197 [1200/1251 ( 96%)]  Loss: 3.718 (3.38)  Time: 0.997s, 1027.14/s  (1.005s, 1018.77/s)  LR: 2.711e-04  Data: 0.011 (0.012)
Train: 197 [1250/1251 (100%)]  Loss: 3.569 (3.38)  Time: 0.996s, 1027.65/s  (1.006s, 1018.09/s)  LR: 2.711e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.617 (1.617)  Loss:  0.6757 (0.6757)  Acc@1: 89.4531 (89.4531)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.245 (0.572)  Loss:  0.7452 (1.1806)  Acc@1: 85.4953 (76.6460)  Acc@5: 97.1698 (93.5620)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-193.pth.tar', 76.8279999584961)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-187.pth.tar', 76.79200000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-196.pth.tar', 76.77200010986328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-190.pth.tar', 76.72400005859375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-192.pth.tar', 76.71600000976562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-197.pth.tar', 76.64600003417969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-194.pth.tar', 76.60600006103516)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-185.pth.tar', 76.50599996337891)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-195.pth.tar', 76.4920000024414)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-188.pth.tar', 76.43200013671876)

Train: 198 [   0/1251 (  0%)]  Loss: 3.174 (3.17)  Time: 2.552s,  401.31/s  (2.552s,  401.31/s)  LR: 2.665e-04  Data: 1.584 (1.584)
Train: 198 [  50/1251 (  4%)]  Loss: 3.459 (3.32)  Time: 0.999s, 1024.61/s  (1.057s,  968.52/s)  LR: 2.665e-04  Data: 0.011 (0.042)
Train: 198 [ 100/1251 (  8%)]  Loss: 3.398 (3.34)  Time: 0.995s, 1029.37/s  (1.030s,  994.31/s)  LR: 2.665e-04  Data: 0.010 (0.027)
Train: 198 [ 150/1251 ( 12%)]  Loss: 3.424 (3.36)  Time: 1.004s, 1019.84/s  (1.020s, 1004.00/s)  LR: 2.665e-04  Data: 0.012 (0.022)
Train: 198 [ 200/1251 ( 16%)]  Loss: 3.469 (3.38)  Time: 0.998s, 1026.44/s  (1.015s, 1008.63/s)  LR: 2.665e-04  Data: 0.011 (0.019)
Train: 198 [ 250/1251 ( 20%)]  Loss: 3.497 (3.40)  Time: 1.003s, 1020.99/s  (1.012s, 1011.65/s)  LR: 2.665e-04  Data: 0.011 (0.017)
Train: 198 [ 300/1251 ( 24%)]  Loss: 3.691 (3.44)  Time: 1.003s, 1020.92/s  (1.010s, 1013.91/s)  LR: 2.665e-04  Data: 0.011 (0.016)
Train: 198 [ 350/1251 ( 28%)]  Loss: 3.489 (3.45)  Time: 0.995s, 1029.07/s  (1.009s, 1015.31/s)  LR: 2.665e-04  Data: 0.011 (0.016)
Train: 198 [ 400/1251 ( 32%)]  Loss: 3.300 (3.43)  Time: 0.999s, 1024.59/s  (1.007s, 1016.50/s)  LR: 2.665e-04  Data: 0.011 (0.015)
Train: 198 [ 450/1251 ( 36%)]  Loss: 3.324 (3.42)  Time: 1.001s, 1022.68/s  (1.007s, 1017.33/s)  LR: 2.665e-04  Data: 0.010 (0.015)
Train: 198 [ 500/1251 ( 40%)]  Loss: 3.455 (3.43)  Time: 1.002s, 1022.43/s  (1.006s, 1017.70/s)  LR: 2.665e-04  Data: 0.011 (0.014)
Train: 198 [ 550/1251 ( 44%)]  Loss: 3.283 (3.41)  Time: 1.003s, 1020.62/s  (1.006s, 1018.16/s)  LR: 2.665e-04  Data: 0.011 (0.014)
Train: 198 [ 600/1251 ( 48%)]  Loss: 3.656 (3.43)  Time: 1.001s, 1023.22/s  (1.005s, 1018.75/s)  LR: 2.665e-04  Data: 0.011 (0.014)
Train: 198 [ 650/1251 ( 52%)]  Loss: 3.486 (3.44)  Time: 0.994s, 1030.59/s  (1.005s, 1019.15/s)  LR: 2.665e-04  Data: 0.010 (0.014)
Train: 198 [ 700/1251 ( 56%)]  Loss: 3.313 (3.43)  Time: 1.005s, 1018.71/s  (1.005s, 1019.39/s)  LR: 2.665e-04  Data: 0.011 (0.013)
Train: 198 [ 750/1251 ( 60%)]  Loss: 3.561 (3.44)  Time: 0.999s, 1024.96/s  (1.004s, 1019.76/s)  LR: 2.665e-04  Data: 0.011 (0.013)
Train: 198 [ 800/1251 ( 64%)]  Loss: 3.263 (3.43)  Time: 0.997s, 1026.92/s  (1.004s, 1020.08/s)  LR: 2.665e-04  Data: 0.011 (0.013)
Train: 198 [ 850/1251 ( 68%)]  Loss: 3.568 (3.43)  Time: 0.997s, 1026.92/s  (1.004s, 1020.23/s)  LR: 2.665e-04  Data: 0.011 (0.013)
Train: 198 [ 900/1251 ( 72%)]  Loss: 3.225 (3.42)  Time: 0.995s, 1028.69/s  (1.004s, 1020.20/s)  LR: 2.665e-04  Data: 0.011 (0.013)
Train: 198 [ 950/1251 ( 76%)]  Loss: 3.199 (3.41)  Time: 0.996s, 1027.97/s  (1.003s, 1020.52/s)  LR: 2.665e-04  Data: 0.012 (0.013)
Train: 198 [1000/1251 ( 80%)]  Loss: 3.216 (3.40)  Time: 0.998s, 1025.71/s  (1.003s, 1020.76/s)  LR: 2.665e-04  Data: 0.011 (0.013)
Train: 198 [1050/1251 ( 84%)]  Loss: 2.966 (3.38)  Time: 0.995s, 1029.28/s  (1.003s, 1020.87/s)  LR: 2.665e-04  Data: 0.010 (0.013)
Train: 198 [1100/1251 ( 88%)]  Loss: 3.389 (3.38)  Time: 0.995s, 1029.54/s  (1.003s, 1021.01/s)  LR: 2.665e-04  Data: 0.011 (0.013)
Train: 198 [1150/1251 ( 92%)]  Loss: 3.442 (3.39)  Time: 0.998s, 1025.65/s  (1.003s, 1021.13/s)  LR: 2.665e-04  Data: 0.011 (0.013)
Train: 198 [1200/1251 ( 96%)]  Loss: 3.390 (3.39)  Time: 0.999s, 1024.80/s  (1.004s, 1020.14/s)  LR: 2.665e-04  Data: 0.011 (0.012)
Train: 198 [1250/1251 (100%)]  Loss: 3.582 (3.39)  Time: 0.985s, 1039.72/s  (1.004s, 1020.38/s)  LR: 2.665e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.668 (1.668)  Loss:  0.7531 (0.7531)  Acc@1: 90.1367 (90.1367)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.245 (0.570)  Loss:  0.8691 (1.2365)  Acc@1: 85.4953 (76.8240)  Acc@5: 97.0519 (93.6240)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-193.pth.tar', 76.8279999584961)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-198.pth.tar', 76.82400003417969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-187.pth.tar', 76.79200000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-196.pth.tar', 76.77200010986328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-190.pth.tar', 76.72400005859375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-192.pth.tar', 76.71600000976562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-197.pth.tar', 76.64600003417969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-194.pth.tar', 76.60600006103516)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-185.pth.tar', 76.50599996337891)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-195.pth.tar', 76.4920000024414)

Train: 199 [   0/1251 (  0%)]  Loss: 3.468 (3.47)  Time: 2.457s,  416.72/s  (2.457s,  416.72/s)  LR: 2.620e-04  Data: 1.496 (1.496)
Train: 199 [  50/1251 (  4%)]  Loss: 3.391 (3.43)  Time: 0.999s, 1024.93/s  (1.032s,  992.35/s)  LR: 2.620e-04  Data: 0.011 (0.040)
Train: 199 [ 100/1251 (  8%)]  Loss: 3.531 (3.46)  Time: 0.993s, 1031.46/s  (1.024s, 1000.02/s)  LR: 2.620e-04  Data: 0.012 (0.026)
Train: 199 [ 150/1251 ( 12%)]  Loss: 3.414 (3.45)  Time: 1.012s, 1012.26/s  (1.017s, 1007.27/s)  LR: 2.620e-04  Data: 0.011 (0.021)
Train: 199 [ 200/1251 ( 16%)]  Loss: 3.578 (3.48)  Time: 0.999s, 1024.61/s  (1.013s, 1010.55/s)  LR: 2.620e-04  Data: 0.011 (0.018)
Train: 199 [ 250/1251 ( 20%)]  Loss: 3.495 (3.48)  Time: 0.999s, 1025.14/s  (1.011s, 1012.80/s)  LR: 2.620e-04  Data: 0.011 (0.017)
Train: 199 [ 300/1251 ( 24%)]  Loss: 3.399 (3.47)  Time: 0.996s, 1027.91/s  (1.010s, 1014.18/s)  LR: 2.620e-04  Data: 0.012 (0.016)
Train: 199 [ 350/1251 ( 28%)]  Loss: 3.489 (3.47)  Time: 0.994s, 1030.00/s  (1.009s, 1015.16/s)  LR: 2.620e-04  Data: 0.010 (0.015)
Train: 199 [ 400/1251 ( 32%)]  Loss: 3.365 (3.46)  Time: 0.997s, 1027.49/s  (1.008s, 1015.46/s)  LR: 2.620e-04  Data: 0.012 (0.015)
Train: 199 [ 450/1251 ( 36%)]  Loss: 3.696 (3.48)  Time: 0.996s, 1027.80/s  (1.008s, 1016.34/s)  LR: 2.620e-04  Data: 0.011 (0.014)
Train: 199 [ 500/1251 ( 40%)]  Loss: 3.474 (3.48)  Time: 0.995s, 1029.59/s  (1.007s, 1017.05/s)  LR: 2.620e-04  Data: 0.010 (0.014)
Train: 199 [ 550/1251 ( 44%)]  Loss: 3.067 (3.45)  Time: 0.995s, 1029.58/s  (1.006s, 1017.56/s)  LR: 2.620e-04  Data: 0.012 (0.014)
Train: 199 [ 600/1251 ( 48%)]  Loss: 3.457 (3.45)  Time: 0.997s, 1027.30/s  (1.006s, 1018.16/s)  LR: 2.620e-04  Data: 0.011 (0.014)
Train: 199 [ 650/1251 ( 52%)]  Loss: 3.132 (3.43)  Time: 0.990s, 1033.99/s  (1.006s, 1018.30/s)  LR: 2.620e-04  Data: 0.010 (0.013)
Train: 199 [ 700/1251 ( 56%)]  Loss: 3.206 (3.41)  Time: 0.999s, 1024.54/s  (1.005s, 1018.72/s)  LR: 2.620e-04  Data: 0.011 (0.013)
Train: 199 [ 750/1251 ( 60%)]  Loss: 3.611 (3.42)  Time: 1.003s, 1020.69/s  (1.005s, 1018.77/s)  LR: 2.620e-04  Data: 0.011 (0.013)
Train: 199 [ 800/1251 ( 64%)]  Loss: 3.490 (3.43)  Time: 0.996s, 1028.47/s  (1.005s, 1018.88/s)  LR: 2.620e-04  Data: 0.011 (0.013)
Train: 199 [ 850/1251 ( 68%)]  Loss: 3.694 (3.44)  Time: 0.997s, 1027.53/s  (1.005s, 1018.83/s)  LR: 2.620e-04  Data: 0.011 (0.013)
Train: 199 [ 900/1251 ( 72%)]  Loss: 3.598 (3.45)  Time: 1.063s,  962.92/s  (1.005s, 1018.65/s)  LR: 2.620e-04  Data: 0.011 (0.013)
Train: 199 [ 950/1251 ( 76%)]  Loss: 3.140 (3.43)  Time: 0.991s, 1033.01/s  (1.005s, 1018.78/s)  LR: 2.620e-04  Data: 0.010 (0.013)
Train: 199 [1000/1251 ( 80%)]  Loss: 3.444 (3.44)  Time: 0.998s, 1025.68/s  (1.005s, 1019.03/s)  LR: 2.620e-04  Data: 0.010 (0.013)
Train: 199 [1050/1251 ( 84%)]  Loss: 3.649 (3.44)  Time: 0.998s, 1026.41/s  (1.005s, 1019.40/s)  LR: 2.620e-04  Data: 0.011 (0.013)
Train: 199 [1100/1251 ( 88%)]  Loss: 3.534 (3.45)  Time: 0.997s, 1027.02/s  (1.004s, 1019.67/s)  LR: 2.620e-04  Data: 0.011 (0.013)
Train: 199 [1150/1251 ( 92%)]  Loss: 3.090 (3.43)  Time: 1.010s, 1014.09/s  (1.004s, 1019.84/s)  LR: 2.620e-04  Data: 0.012 (0.012)
Train: 199 [1200/1251 ( 96%)]  Loss: 3.158 (3.42)  Time: 0.997s, 1027.43/s  (1.004s, 1019.92/s)  LR: 2.620e-04  Data: 0.010 (0.012)
Train: 199 [1250/1251 (100%)]  Loss: 3.260 (3.42)  Time: 0.984s, 1041.11/s  (1.005s, 1019.17/s)  LR: 2.620e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.652 (1.652)  Loss:  0.7481 (0.7481)  Acc@1: 90.6250 (90.6250)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.245 (0.568)  Loss:  0.7904 (1.1903)  Acc@1: 85.4953 (76.9120)  Acc@5: 96.6981 (93.5780)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-199.pth.tar', 76.91200003417968)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-193.pth.tar', 76.8279999584961)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-198.pth.tar', 76.82400003417969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-187.pth.tar', 76.79200000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-196.pth.tar', 76.77200010986328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-190.pth.tar', 76.72400005859375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-192.pth.tar', 76.71600000976562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-197.pth.tar', 76.64600003417969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-194.pth.tar', 76.60600006103516)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-185.pth.tar', 76.50599996337891)

Train: 200 [   0/1251 (  0%)]  Loss: 3.529 (3.53)  Time: 2.704s,  378.67/s  (2.704s,  378.67/s)  LR: 2.575e-04  Data: 1.738 (1.738)
Train: 200 [  50/1251 (  4%)]  Loss: 3.063 (3.30)  Time: 1.051s,  974.37/s  (1.036s,  988.11/s)  LR: 2.575e-04  Data: 0.013 (0.045)
Train: 200 [ 100/1251 (  8%)]  Loss: 3.521 (3.37)  Time: 0.997s, 1027.49/s  (1.020s, 1003.62/s)  LR: 2.575e-04  Data: 0.011 (0.028)
Train: 200 [ 150/1251 ( 12%)]  Loss: 3.593 (3.43)  Time: 1.008s, 1015.45/s  (1.013s, 1010.44/s)  LR: 2.575e-04  Data: 0.012 (0.022)
Train: 200 [ 200/1251 ( 16%)]  Loss: 3.328 (3.41)  Time: 1.009s, 1014.97/s  (1.019s, 1004.49/s)  LR: 2.575e-04  Data: 0.016 (0.020)
Train: 200 [ 250/1251 ( 20%)]  Loss: 3.398 (3.41)  Time: 0.996s, 1027.88/s  (1.016s, 1008.13/s)  LR: 2.575e-04  Data: 0.012 (0.018)
Train: 200 [ 300/1251 ( 24%)]  Loss: 3.426 (3.41)  Time: 1.000s, 1023.96/s  (1.013s, 1010.70/s)  LR: 2.575e-04  Data: 0.013 (0.017)
Train: 200 [ 350/1251 ( 28%)]  Loss: 3.255 (3.39)  Time: 0.993s, 1030.75/s  (1.011s, 1012.53/s)  LR: 2.575e-04  Data: 0.011 (0.016)
Train: 200 [ 400/1251 ( 32%)]  Loss: 3.578 (3.41)  Time: 0.998s, 1026.11/s  (1.010s, 1013.95/s)  LR: 2.575e-04  Data: 0.012 (0.015)
Train: 200 [ 450/1251 ( 36%)]  Loss: 3.439 (3.41)  Time: 0.999s, 1025.40/s  (1.009s, 1014.95/s)  LR: 2.575e-04  Data: 0.011 (0.015)
Train: 200 [ 500/1251 ( 40%)]  Loss: 3.317 (3.40)  Time: 0.992s, 1032.55/s  (1.008s, 1015.47/s)  LR: 2.575e-04  Data: 0.010 (0.014)
Train: 200 [ 550/1251 ( 44%)]  Loss: 3.038 (3.37)  Time: 1.062s,  964.46/s  (1.008s, 1015.58/s)  LR: 2.575e-04  Data: 0.012 (0.014)
Train: 200 [ 600/1251 ( 48%)]  Loss: 3.560 (3.39)  Time: 0.996s, 1028.06/s  (1.010s, 1014.32/s)  LR: 2.575e-04  Data: 0.012 (0.014)
Train: 200 [ 650/1251 ( 52%)]  Loss: 3.416 (3.39)  Time: 1.035s,  989.29/s  (1.011s, 1012.79/s)  LR: 2.575e-04  Data: 0.011 (0.014)
Train: 200 [ 700/1251 ( 56%)]  Loss: 3.392 (3.39)  Time: 1.004s, 1019.47/s  (1.011s, 1012.96/s)  LR: 2.575e-04  Data: 0.011 (0.014)
Train: 200 [ 750/1251 ( 60%)]  Loss: 3.390 (3.39)  Time: 1.000s, 1024.06/s  (1.010s, 1013.63/s)  LR: 2.575e-04  Data: 0.011 (0.013)
Train: 200 [ 800/1251 ( 64%)]  Loss: 3.590 (3.40)  Time: 1.000s, 1023.62/s  (1.010s, 1013.64/s)  LR: 2.575e-04  Data: 0.010 (0.013)
Train: 200 [ 850/1251 ( 68%)]  Loss: 3.399 (3.40)  Time: 0.993s, 1031.73/s  (1.010s, 1014.32/s)  LR: 2.575e-04  Data: 0.010 (0.013)
Train: 200 [ 900/1251 ( 72%)]  Loss: 3.491 (3.41)  Time: 0.998s, 1026.30/s  (1.009s, 1014.92/s)  LR: 2.575e-04  Data: 0.011 (0.013)
Train: 200 [ 950/1251 ( 76%)]  Loss: 3.150 (3.39)  Time: 0.997s, 1027.29/s  (1.009s, 1015.22/s)  LR: 2.575e-04  Data: 0.012 (0.013)
Train: 200 [1000/1251 ( 80%)]  Loss: 3.590 (3.40)  Time: 0.999s, 1025.28/s  (1.008s, 1015.67/s)  LR: 2.575e-04  Data: 0.011 (0.013)
Train: 200 [1050/1251 ( 84%)]  Loss: 3.187 (3.39)  Time: 1.000s, 1024.42/s  (1.008s, 1015.85/s)  LR: 2.575e-04  Data: 0.012 (0.013)
Train: 200 [1100/1251 ( 88%)]  Loss: 3.077 (3.38)  Time: 0.994s, 1030.20/s  (1.008s, 1016.31/s)  LR: 2.575e-04  Data: 0.011 (0.013)
Train: 200 [1150/1251 ( 92%)]  Loss: 3.452 (3.38)  Time: 0.998s, 1026.57/s  (1.007s, 1016.45/s)  LR: 2.575e-04  Data: 0.011 (0.013)
Train: 200 [1200/1251 ( 96%)]  Loss: 3.366 (3.38)  Time: 0.997s, 1027.33/s  (1.008s, 1015.91/s)  LR: 2.575e-04  Data: 0.012 (0.013)
Train: 200 [1250/1251 (100%)]  Loss: 3.144 (3.37)  Time: 0.981s, 1043.96/s  (1.008s, 1016.37/s)  LR: 2.575e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.649 (1.649)  Loss:  0.7335 (0.7335)  Acc@1: 90.9180 (90.9180)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.245 (0.575)  Loss:  0.8212 (1.1700)  Acc@1: 85.1415 (77.1520)  Acc@5: 97.0519 (93.7400)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-200.pth.tar', 77.15200000976563)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-199.pth.tar', 76.91200003417968)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-193.pth.tar', 76.8279999584961)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-198.pth.tar', 76.82400003417969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-187.pth.tar', 76.79200000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-196.pth.tar', 76.77200010986328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-190.pth.tar', 76.72400005859375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-192.pth.tar', 76.71600000976562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-197.pth.tar', 76.64600003417969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-194.pth.tar', 76.60600006103516)

Train: 201 [   0/1251 (  0%)]  Loss: 3.454 (3.45)  Time: 4.299s,  238.21/s  (4.299s,  238.21/s)  LR: 2.530e-04  Data: 3.039 (3.039)
Train: 201 [  50/1251 (  4%)]  Loss: 3.385 (3.42)  Time: 0.996s, 1027.67/s  (1.069s,  957.76/s)  LR: 2.530e-04  Data: 0.012 (0.071)
Train: 201 [ 100/1251 (  8%)]  Loss: 3.260 (3.37)  Time: 0.998s, 1026.23/s  (1.043s,  981.38/s)  LR: 2.530e-04  Data: 0.011 (0.041)
Train: 201 [ 150/1251 ( 12%)]  Loss: 3.534 (3.41)  Time: 0.995s, 1029.10/s  (1.030s,  994.19/s)  LR: 2.530e-04  Data: 0.011 (0.031)
Train: 201 [ 200/1251 ( 16%)]  Loss: 3.073 (3.34)  Time: 0.997s, 1026.80/s  (1.024s, 1000.33/s)  LR: 2.530e-04  Data: 0.012 (0.026)
Train: 201 [ 250/1251 ( 20%)]  Loss: 3.639 (3.39)  Time: 1.026s,  998.00/s  (1.019s, 1004.46/s)  LR: 2.530e-04  Data: 0.010 (0.023)
Train: 201 [ 300/1251 ( 24%)]  Loss: 3.182 (3.36)  Time: 1.003s, 1021.39/s  (1.016s, 1007.42/s)  LR: 2.530e-04  Data: 0.011 (0.021)
Train: 201 [ 350/1251 ( 28%)]  Loss: 3.298 (3.35)  Time: 1.004s, 1019.87/s  (1.014s, 1009.72/s)  LR: 2.530e-04  Data: 0.011 (0.020)
Train: 201 [ 400/1251 ( 32%)]  Loss: 3.583 (3.38)  Time: 1.002s, 1022.21/s  (1.012s, 1011.66/s)  LR: 2.530e-04  Data: 0.012 (0.019)
Train: 201 [ 450/1251 ( 36%)]  Loss: 3.382 (3.38)  Time: 1.006s, 1017.56/s  (1.011s, 1012.79/s)  LR: 2.530e-04  Data: 0.012 (0.018)
Train: 201 [ 500/1251 ( 40%)]  Loss: 3.372 (3.38)  Time: 0.999s, 1024.96/s  (1.011s, 1012.78/s)  LR: 2.530e-04  Data: 0.011 (0.017)
Train: 201 [ 550/1251 ( 44%)]  Loss: 3.204 (3.36)  Time: 0.995s, 1029.12/s  (1.010s, 1013.73/s)  LR: 2.530e-04  Data: 0.011 (0.017)
Train: 201 [ 600/1251 ( 48%)]  Loss: 3.340 (3.36)  Time: 0.999s, 1025.25/s  (1.009s, 1014.39/s)  LR: 2.530e-04  Data: 0.011 (0.016)
Train: 201 [ 650/1251 ( 52%)]  Loss: 3.170 (3.35)  Time: 0.995s, 1029.05/s  (1.009s, 1015.07/s)  LR: 2.530e-04  Data: 0.011 (0.016)
Train: 201 [ 700/1251 ( 56%)]  Loss: 3.262 (3.34)  Time: 1.004s, 1020.39/s  (1.009s, 1015.31/s)  LR: 2.530e-04  Data: 0.017 (0.016)
Train: 201 [ 750/1251 ( 60%)]  Loss: 3.500 (3.35)  Time: 1.043s,  981.71/s  (1.008s, 1015.62/s)  LR: 2.530e-04  Data: 0.012 (0.015)
Train: 201 [ 800/1251 ( 64%)]  Loss: 3.373 (3.35)  Time: 0.997s, 1026.59/s  (1.008s, 1016.05/s)  LR: 2.530e-04  Data: 0.011 (0.015)
Train: 201 [ 850/1251 ( 68%)]  Loss: 2.899 (3.33)  Time: 1.058s,  968.27/s  (1.008s, 1016.32/s)  LR: 2.530e-04  Data: 0.011 (0.015)
Train: 201 [ 900/1251 ( 72%)]  Loss: 3.249 (3.32)  Time: 1.048s,  977.31/s  (1.008s, 1015.78/s)  LR: 2.530e-04  Data: 0.011 (0.015)
Train: 201 [ 950/1251 ( 76%)]  Loss: 3.303 (3.32)  Time: 1.003s, 1020.73/s  (1.008s, 1015.90/s)  LR: 2.530e-04  Data: 0.014 (0.015)
Train: 201 [1000/1251 ( 80%)]  Loss: 3.337 (3.32)  Time: 0.996s, 1027.72/s  (1.008s, 1016.24/s)  LR: 2.530e-04  Data: 0.012 (0.014)
Train: 201 [1050/1251 ( 84%)]  Loss: 3.522 (3.33)  Time: 0.999s, 1024.77/s  (1.007s, 1016.70/s)  LR: 2.530e-04  Data: 0.011 (0.014)
Train: 201 [1100/1251 ( 88%)]  Loss: 3.453 (3.34)  Time: 0.994s, 1030.56/s  (1.007s, 1016.92/s)  LR: 2.530e-04  Data: 0.011 (0.014)
Train: 201 [1150/1251 ( 92%)]  Loss: 3.357 (3.34)  Time: 0.995s, 1028.74/s  (1.007s, 1017.16/s)  LR: 2.530e-04  Data: 0.011 (0.014)
Train: 201 [1200/1251 ( 96%)]  Loss: 3.349 (3.34)  Time: 1.002s, 1022.06/s  (1.006s, 1017.41/s)  LR: 2.530e-04  Data: 0.011 (0.014)
Train: 201 [1250/1251 (100%)]  Loss: 3.294 (3.34)  Time: 1.017s, 1007.06/s  (1.008s, 1016.00/s)  LR: 2.530e-04  Data: 0.000 (0.014)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.688 (1.688)  Loss:  0.6669 (0.6669)  Acc@1: 89.8438 (89.8438)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.245 (0.572)  Loss:  0.7536 (1.1734)  Acc@1: 85.7311 (77.1040)  Acc@5: 96.9340 (93.8020)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-200.pth.tar', 77.15200000976563)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-201.pth.tar', 77.10400000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-199.pth.tar', 76.91200003417968)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-193.pth.tar', 76.8279999584961)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-198.pth.tar', 76.82400003417969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-187.pth.tar', 76.79200000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-196.pth.tar', 76.77200010986328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-190.pth.tar', 76.72400005859375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-192.pth.tar', 76.71600000976562)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-197.pth.tar', 76.64600003417969)

Train: 202 [   0/1251 (  0%)]  Loss: 3.407 (3.41)  Time: 2.528s,  405.02/s  (2.528s,  405.02/s)  LR: 2.486e-04  Data: 1.569 (1.569)
Train: 202 [  50/1251 (  4%)]  Loss: 2.914 (3.16)  Time: 1.005s, 1018.55/s  (1.032s,  992.62/s)  LR: 2.486e-04  Data: 0.011 (0.045)
Train: 202 [ 100/1251 (  8%)]  Loss: 3.231 (3.18)  Time: 0.997s, 1027.02/s  (1.018s, 1006.05/s)  LR: 2.486e-04  Data: 0.011 (0.028)
Train: 202 [ 150/1251 ( 12%)]  Loss: 3.035 (3.15)  Time: 0.991s, 1033.39/s  (1.012s, 1011.92/s)  LR: 2.486e-04  Data: 0.010 (0.023)
Train: 202 [ 200/1251 ( 16%)]  Loss: 3.341 (3.19)  Time: 0.998s, 1025.83/s  (1.009s, 1014.60/s)  LR: 2.486e-04  Data: 0.012 (0.020)
Train: 202 [ 250/1251 ( 20%)]  Loss: 3.305 (3.21)  Time: 1.045s,  979.90/s  (1.009s, 1014.40/s)  LR: 2.486e-04  Data: 0.010 (0.018)
Train: 202 [ 300/1251 ( 24%)]  Loss: 3.629 (3.27)  Time: 0.999s, 1025.25/s  (1.010s, 1014.04/s)  LR: 2.486e-04  Data: 0.011 (0.017)
Train: 202 [ 350/1251 ( 28%)]  Loss: 3.462 (3.29)  Time: 0.996s, 1028.13/s  (1.008s, 1015.43/s)  LR: 2.486e-04  Data: 0.011 (0.016)
Train: 202 [ 400/1251 ( 32%)]  Loss: 3.695 (3.34)  Time: 0.994s, 1030.61/s  (1.007s, 1016.79/s)  LR: 2.486e-04  Data: 0.011 (0.015)
Train: 202 [ 450/1251 ( 36%)]  Loss: 3.236 (3.33)  Time: 0.999s, 1025.38/s  (1.006s, 1017.54/s)  LR: 2.486e-04  Data: 0.011 (0.015)
Train: 202 [ 500/1251 ( 40%)]  Loss: 3.467 (3.34)  Time: 0.999s, 1025.53/s  (1.007s, 1017.26/s)  LR: 2.486e-04  Data: 0.010 (0.015)
Train: 202 [ 550/1251 ( 44%)]  Loss: 3.543 (3.36)  Time: 0.996s, 1028.34/s  (1.006s, 1017.56/s)  LR: 2.486e-04  Data: 0.010 (0.014)
Train: 202 [ 600/1251 ( 48%)]  Loss: 2.709 (3.31)  Time: 0.997s, 1027.28/s  (1.008s, 1015.85/s)  LR: 2.486e-04  Data: 0.011 (0.014)
Train: 202 [ 650/1251 ( 52%)]  Loss: 3.320 (3.31)  Time: 0.992s, 1031.96/s  (1.008s, 1015.73/s)  LR: 2.486e-04  Data: 0.011 (0.014)
Train: 202 [ 700/1251 ( 56%)]  Loss: 3.479 (3.32)  Time: 0.994s, 1030.68/s  (1.008s, 1016.30/s)  LR: 2.486e-04  Data: 0.011 (0.014)
Train: 202 [ 750/1251 ( 60%)]  Loss: 2.801 (3.29)  Time: 0.993s, 1031.63/s  (1.007s, 1016.90/s)  LR: 2.486e-04  Data: 0.010 (0.013)
Train: 202 [ 800/1251 ( 64%)]  Loss: 3.282 (3.29)  Time: 1.014s, 1010.08/s  (1.008s, 1015.39/s)  LR: 2.486e-04  Data: 0.011 (0.013)
Train: 202 [ 850/1251 ( 68%)]  Loss: 3.448 (3.29)  Time: 1.013s, 1011.11/s  (1.008s, 1015.89/s)  LR: 2.486e-04  Data: 0.011 (0.013)
Train: 202 [ 900/1251 ( 72%)]  Loss: 3.643 (3.31)  Time: 1.032s,  992.15/s  (1.008s, 1015.68/s)  LR: 2.486e-04  Data: 0.011 (0.013)
Train: 202 [ 950/1251 ( 76%)]  Loss: 3.427 (3.32)  Time: 0.999s, 1025.40/s  (1.008s, 1015.56/s)  LR: 2.486e-04  Data: 0.011 (0.013)
Train: 202 [1000/1251 ( 80%)]  Loss: 3.454 (3.33)  Time: 0.996s, 1028.47/s  (1.008s, 1015.94/s)  LR: 2.486e-04  Data: 0.011 (0.013)
Train: 202 [1050/1251 ( 84%)]  Loss: 3.245 (3.32)  Time: 1.058s,  967.57/s  (1.008s, 1015.65/s)  LR: 2.486e-04  Data: 0.011 (0.013)
Train: 202 [1100/1251 ( 88%)]  Loss: 3.301 (3.32)  Time: 0.995s, 1029.45/s  (1.009s, 1014.67/s)  LR: 2.486e-04  Data: 0.012 (0.013)
Train: 202 [1150/1251 ( 92%)]  Loss: 3.465 (3.33)  Time: 0.997s, 1027.36/s  (1.009s, 1015.01/s)  LR: 2.486e-04  Data: 0.011 (0.013)
Train: 202 [1200/1251 ( 96%)]  Loss: 3.235 (3.32)  Time: 1.053s,  972.12/s  (1.009s, 1015.23/s)  LR: 2.486e-04  Data: 0.011 (0.013)
Train: 202 [1250/1251 (100%)]  Loss: 3.403 (3.33)  Time: 0.982s, 1043.15/s  (1.009s, 1015.33/s)  LR: 2.486e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.764 (1.764)  Loss:  0.7919 (0.7919)  Acc@1: 91.5039 (91.5039)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.245 (0.571)  Loss:  0.8405 (1.2563)  Acc@1: 86.0849 (77.0080)  Acc@5: 96.9340 (93.4700)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-200.pth.tar', 77.15200000976563)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-201.pth.tar', 77.10400000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-202.pth.tar', 77.00800003173828)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-199.pth.tar', 76.91200003417968)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-193.pth.tar', 76.8279999584961)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-198.pth.tar', 76.82400003417969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-187.pth.tar', 76.79200000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-196.pth.tar', 76.77200010986328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-190.pth.tar', 76.72400005859375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-192.pth.tar', 76.71600000976562)

Train: 203 [   0/1251 (  0%)]  Loss: 3.255 (3.26)  Time: 2.674s,  383.00/s  (2.674s,  383.00/s)  LR: 2.442e-04  Data: 1.696 (1.696)
Train: 203 [  50/1251 (  4%)]  Loss: 3.365 (3.31)  Time: 1.028s,  996.24/s  (1.034s,  990.43/s)  LR: 2.442e-04  Data: 0.010 (0.044)
Train: 203 [ 100/1251 (  8%)]  Loss: 3.107 (3.24)  Time: 1.007s, 1016.81/s  (1.018s, 1006.32/s)  LR: 2.442e-04  Data: 0.011 (0.028)
Train: 203 [ 150/1251 ( 12%)]  Loss: 3.062 (3.20)  Time: 0.992s, 1031.78/s  (1.012s, 1011.60/s)  LR: 2.442e-04  Data: 0.010 (0.022)
Train: 203 [ 200/1251 ( 16%)]  Loss: 3.519 (3.26)  Time: 1.038s,  986.78/s  (1.013s, 1010.72/s)  LR: 2.442e-04  Data: 0.010 (0.020)
Train: 203 [ 250/1251 ( 20%)]  Loss: 3.193 (3.25)  Time: 1.000s, 1024.51/s  (1.013s, 1010.70/s)  LR: 2.442e-04  Data: 0.011 (0.018)
Train: 203 [ 300/1251 ( 24%)]  Loss: 3.622 (3.30)  Time: 0.996s, 1028.40/s  (1.012s, 1012.22/s)  LR: 2.442e-04  Data: 0.012 (0.017)
Train: 203 [ 350/1251 ( 28%)]  Loss: 3.508 (3.33)  Time: 1.006s, 1017.69/s  (1.013s, 1011.22/s)  LR: 2.442e-04  Data: 0.012 (0.016)
Train: 203 [ 400/1251 ( 32%)]  Loss: 3.325 (3.33)  Time: 1.014s, 1010.17/s  (1.011s, 1012.72/s)  LR: 2.442e-04  Data: 0.011 (0.015)
Train: 203 [ 450/1251 ( 36%)]  Loss: 3.316 (3.33)  Time: 0.998s, 1026.26/s  (1.010s, 1013.55/s)  LR: 2.442e-04  Data: 0.012 (0.015)
Train: 203 [ 500/1251 ( 40%)]  Loss: 3.442 (3.34)  Time: 1.026s,  998.42/s  (1.010s, 1013.79/s)  LR: 2.442e-04  Data: 0.011 (0.015)
Train: 203 [ 550/1251 ( 44%)]  Loss: 3.489 (3.35)  Time: 1.001s, 1023.19/s  (1.010s, 1014.24/s)  LR: 2.442e-04  Data: 0.010 (0.014)
Train: 203 [ 600/1251 ( 48%)]  Loss: 3.243 (3.34)  Time: 1.060s,  965.88/s  (1.009s, 1014.48/s)  LR: 2.442e-04  Data: 0.012 (0.014)
Train: 203 [ 650/1251 ( 52%)]  Loss: 3.650 (3.36)  Time: 0.999s, 1024.81/s  (1.010s, 1013.45/s)  LR: 2.442e-04  Data: 0.011 (0.014)
Train: 203 [ 700/1251 ( 56%)]  Loss: 2.925 (3.33)  Time: 1.034s,  990.23/s  (1.010s, 1014.10/s)  LR: 2.442e-04  Data: 0.012 (0.014)
Train: 203 [ 750/1251 ( 60%)]  Loss: 3.359 (3.34)  Time: 1.002s, 1021.92/s  (1.010s, 1014.05/s)  LR: 2.442e-04  Data: 0.011 (0.013)
Train: 203 [ 800/1251 ( 64%)]  Loss: 3.488 (3.35)  Time: 0.998s, 1026.04/s  (1.009s, 1014.37/s)  LR: 2.442e-04  Data: 0.011 (0.013)
Train: 203 [ 850/1251 ( 68%)]  Loss: 3.441 (3.35)  Time: 0.999s, 1024.58/s  (1.009s, 1014.98/s)  LR: 2.442e-04  Data: 0.012 (0.013)
Train: 203 [ 900/1251 ( 72%)]  Loss: 3.529 (3.36)  Time: 0.995s, 1028.91/s  (1.009s, 1014.82/s)  LR: 2.442e-04  Data: 0.011 (0.013)
Train: 203 [ 950/1251 ( 76%)]  Loss: 3.736 (3.38)  Time: 0.994s, 1030.17/s  (1.009s, 1015.07/s)  LR: 2.442e-04  Data: 0.011 (0.013)
Train: 203 [1000/1251 ( 80%)]  Loss: 2.982 (3.36)  Time: 1.010s, 1013.61/s  (1.008s, 1015.46/s)  LR: 2.442e-04  Data: 0.012 (0.013)
Train: 203 [1050/1251 ( 84%)]  Loss: 3.696 (3.38)  Time: 0.998s, 1025.92/s  (1.009s, 1015.30/s)  LR: 2.442e-04  Data: 0.011 (0.013)
Train: 203 [1100/1251 ( 88%)]  Loss: 3.589 (3.38)  Time: 0.990s, 1033.88/s  (1.008s, 1015.62/s)  LR: 2.442e-04  Data: 0.010 (0.013)
Train: 203 [1150/1251 ( 92%)]  Loss: 3.471 (3.39)  Time: 0.996s, 1027.75/s  (1.008s, 1015.94/s)  LR: 2.442e-04  Data: 0.010 (0.013)
Train: 203 [1200/1251 ( 96%)]  Loss: 3.239 (3.38)  Time: 0.991s, 1033.22/s  (1.008s, 1016.23/s)  LR: 2.442e-04  Data: 0.011 (0.013)
Train: 203 [1250/1251 (100%)]  Loss: 3.067 (3.37)  Time: 0.987s, 1037.58/s  (1.007s, 1016.63/s)  LR: 2.442e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.666 (1.666)  Loss:  0.7137 (0.7137)  Acc@1: 90.7227 (90.7227)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.245 (0.576)  Loss:  0.8132 (1.1740)  Acc@1: 85.7311 (77.2360)  Acc@5: 96.6981 (93.9380)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-203.pth.tar', 77.23599987792969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-200.pth.tar', 77.15200000976563)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-201.pth.tar', 77.10400000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-202.pth.tar', 77.00800003173828)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-199.pth.tar', 76.91200003417968)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-193.pth.tar', 76.8279999584961)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-198.pth.tar', 76.82400003417969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-187.pth.tar', 76.79200000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-196.pth.tar', 76.77200010986328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-190.pth.tar', 76.72400005859375)

Train: 204 [   0/1251 (  0%)]  Loss: 3.108 (3.11)  Time: 2.522s,  405.99/s  (2.522s,  405.99/s)  LR: 2.398e-04  Data: 1.564 (1.564)
Train: 204 [  50/1251 (  4%)]  Loss: 3.381 (3.24)  Time: 0.996s, 1027.84/s  (1.034s,  990.36/s)  LR: 2.398e-04  Data: 0.011 (0.046)
Train: 204 [ 100/1251 (  8%)]  Loss: 3.154 (3.21)  Time: 0.998s, 1026.42/s  (1.016s, 1007.51/s)  LR: 2.398e-04  Data: 0.010 (0.028)
Train: 204 [ 150/1251 ( 12%)]  Loss: 3.532 (3.29)  Time: 1.001s, 1023.18/s  (1.011s, 1012.92/s)  LR: 2.398e-04  Data: 0.010 (0.023)
Train: 204 [ 200/1251 ( 16%)]  Loss: 3.285 (3.29)  Time: 0.994s, 1030.70/s  (1.008s, 1015.94/s)  LR: 2.398e-04  Data: 0.012 (0.020)
Train: 204 [ 250/1251 ( 20%)]  Loss: 3.149 (3.27)  Time: 1.001s, 1022.72/s  (1.009s, 1014.76/s)  LR: 2.398e-04  Data: 0.012 (0.018)
Train: 204 [ 300/1251 ( 24%)]  Loss: 3.301 (3.27)  Time: 0.999s, 1025.14/s  (1.009s, 1015.30/s)  LR: 2.398e-04  Data: 0.011 (0.017)
Train: 204 [ 350/1251 ( 28%)]  Loss: 3.195 (3.26)  Time: 0.992s, 1032.09/s  (1.007s, 1016.58/s)  LR: 2.398e-04  Data: 0.011 (0.016)
Train: 204 [ 400/1251 ( 32%)]  Loss: 3.617 (3.30)  Time: 1.064s,  962.59/s  (1.007s, 1017.30/s)  LR: 2.398e-04  Data: 0.011 (0.015)
Train: 204 [ 450/1251 ( 36%)]  Loss: 3.387 (3.31)  Time: 1.000s, 1024.10/s  (1.006s, 1017.78/s)  LR: 2.398e-04  Data: 0.011 (0.015)
Train: 204 [ 500/1251 ( 40%)]  Loss: 3.240 (3.30)  Time: 0.998s, 1026.48/s  (1.005s, 1018.47/s)  LR: 2.398e-04  Data: 0.011 (0.015)
Train: 204 [ 550/1251 ( 44%)]  Loss: 3.412 (3.31)  Time: 0.995s, 1028.88/s  (1.005s, 1018.66/s)  LR: 2.398e-04  Data: 0.011 (0.014)
Train: 204 [ 600/1251 ( 48%)]  Loss: 3.342 (3.32)  Time: 0.990s, 1034.35/s  (1.005s, 1018.85/s)  LR: 2.398e-04  Data: 0.010 (0.014)
Train: 204 [ 650/1251 ( 52%)]  Loss: 3.375 (3.32)  Time: 0.994s, 1030.47/s  (1.005s, 1019.21/s)  LR: 2.398e-04  Data: 0.010 (0.014)
Train: 204 [ 700/1251 ( 56%)]  Loss: 3.669 (3.34)  Time: 0.993s, 1030.71/s  (1.004s, 1019.46/s)  LR: 2.398e-04  Data: 0.011 (0.014)
Train: 204 [ 750/1251 ( 60%)]  Loss: 3.423 (3.35)  Time: 0.999s, 1025.40/s  (1.004s, 1019.87/s)  LR: 2.398e-04  Data: 0.011 (0.013)
Train: 204 [ 800/1251 ( 64%)]  Loss: 3.320 (3.35)  Time: 0.996s, 1027.97/s  (1.004s, 1019.86/s)  LR: 2.398e-04  Data: 0.012 (0.013)
Train: 204 [ 850/1251 ( 68%)]  Loss: 3.083 (3.33)  Time: 1.037s,  987.01/s  (1.005s, 1018.82/s)  LR: 2.398e-04  Data: 0.011 (0.013)
Train: 204 [ 900/1251 ( 72%)]  Loss: 3.545 (3.34)  Time: 0.998s, 1026.38/s  (1.006s, 1017.76/s)  LR: 2.398e-04  Data: 0.011 (0.013)
Train: 204 [ 950/1251 ( 76%)]  Loss: 2.882 (3.32)  Time: 0.995s, 1029.60/s  (1.006s, 1018.01/s)  LR: 2.398e-04  Data: 0.011 (0.013)
Train: 204 [1000/1251 ( 80%)]  Loss: 3.513 (3.33)  Time: 0.995s, 1029.14/s  (1.005s, 1018.40/s)  LR: 2.398e-04  Data: 0.011 (0.013)
Train: 204 [1050/1251 ( 84%)]  Loss: 3.261 (3.33)  Time: 0.997s, 1027.34/s  (1.005s, 1018.64/s)  LR: 2.398e-04  Data: 0.012 (0.013)
Train: 204 [1100/1251 ( 88%)]  Loss: 3.110 (3.32)  Time: 0.995s, 1028.72/s  (1.005s, 1018.99/s)  LR: 2.398e-04  Data: 0.012 (0.013)
Train: 204 [1150/1251 ( 92%)]  Loss: 3.300 (3.32)  Time: 0.995s, 1029.31/s  (1.005s, 1019.33/s)  LR: 2.398e-04  Data: 0.012 (0.013)
Train: 204 [1200/1251 ( 96%)]  Loss: 2.989 (3.30)  Time: 0.993s, 1030.96/s  (1.005s, 1019.33/s)  LR: 2.398e-04  Data: 0.011 (0.013)
Train: 204 [1250/1251 (100%)]  Loss: 3.407 (3.31)  Time: 0.984s, 1040.48/s  (1.004s, 1019.52/s)  LR: 2.398e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.667 (1.667)  Loss:  0.6930 (0.6930)  Acc@1: 90.8203 (90.8203)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.245 (0.572)  Loss:  0.8996 (1.1830)  Acc@1: 84.1981 (77.3700)  Acc@5: 96.5802 (93.7360)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-204.pth.tar', 77.3700001171875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-203.pth.tar', 77.23599987792969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-200.pth.tar', 77.15200000976563)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-201.pth.tar', 77.10400000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-202.pth.tar', 77.00800003173828)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-199.pth.tar', 76.91200003417968)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-193.pth.tar', 76.8279999584961)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-198.pth.tar', 76.82400003417969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-187.pth.tar', 76.79200000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-196.pth.tar', 76.77200010986328)

Train: 205 [   0/1251 (  0%)]  Loss: 2.806 (2.81)  Time: 2.519s,  406.45/s  (2.519s,  406.45/s)  LR: 2.354e-04  Data: 1.513 (1.513)
Train: 205 [  50/1251 (  4%)]  Loss: 3.269 (3.04)  Time: 0.995s, 1029.62/s  (1.036s,  987.99/s)  LR: 2.354e-04  Data: 0.010 (0.041)
Train: 205 [ 100/1251 (  8%)]  Loss: 3.411 (3.16)  Time: 0.993s, 1030.99/s  (1.018s, 1006.11/s)  LR: 2.354e-04  Data: 0.010 (0.026)
Train: 205 [ 150/1251 ( 12%)]  Loss: 3.079 (3.14)  Time: 1.065s,  961.55/s  (1.023s, 1000.87/s)  LR: 2.354e-04  Data: 0.012 (0.021)
Train: 205 [ 200/1251 ( 16%)]  Loss: 3.300 (3.17)  Time: 0.994s, 1029.73/s  (1.021s, 1002.45/s)  LR: 2.354e-04  Data: 0.011 (0.019)
Train: 205 [ 250/1251 ( 20%)]  Loss: 3.438 (3.22)  Time: 0.999s, 1024.94/s  (1.022s, 1002.09/s)  LR: 2.354e-04  Data: 0.012 (0.017)
Train: 205 [ 300/1251 ( 24%)]  Loss: 3.178 (3.21)  Time: 0.996s, 1028.45/s  (1.022s, 1002.22/s)  LR: 2.354e-04  Data: 0.011 (0.016)
Train: 205 [ 350/1251 ( 28%)]  Loss: 3.417 (3.24)  Time: 0.996s, 1028.14/s  (1.019s, 1004.64/s)  LR: 2.354e-04  Data: 0.012 (0.015)
Train: 205 [ 400/1251 ( 32%)]  Loss: 3.553 (3.27)  Time: 0.995s, 1029.10/s  (1.017s, 1006.99/s)  LR: 2.354e-04  Data: 0.010 (0.015)
Train: 205 [ 450/1251 ( 36%)]  Loss: 3.679 (3.31)  Time: 0.997s, 1026.84/s  (1.015s, 1009.04/s)  LR: 2.354e-04  Data: 0.011 (0.014)
Train: 205 [ 500/1251 ( 40%)]  Loss: 3.633 (3.34)  Time: 1.043s,  982.19/s  (1.013s, 1010.37/s)  LR: 2.354e-04  Data: 0.011 (0.014)
Train: 205 [ 550/1251 ( 44%)]  Loss: 3.430 (3.35)  Time: 0.996s, 1027.71/s  (1.013s, 1010.47/s)  LR: 2.354e-04  Data: 0.011 (0.014)
Train: 205 [ 600/1251 ( 48%)]  Loss: 2.965 (3.32)  Time: 1.005s, 1019.12/s  (1.013s, 1011.15/s)  LR: 2.354e-04  Data: 0.011 (0.014)
Train: 205 [ 650/1251 ( 52%)]  Loss: 3.408 (3.33)  Time: 0.997s, 1027.10/s  (1.012s, 1012.01/s)  LR: 2.354e-04  Data: 0.011 (0.013)
Train: 205 [ 700/1251 ( 56%)]  Loss: 3.290 (3.32)  Time: 0.996s, 1028.62/s  (1.012s, 1011.37/s)  LR: 2.354e-04  Data: 0.012 (0.013)
Train: 205 [ 750/1251 ( 60%)]  Loss: 3.640 (3.34)  Time: 0.995s, 1028.64/s  (1.012s, 1011.53/s)  LR: 2.354e-04  Data: 0.011 (0.013)
Train: 205 [ 800/1251 ( 64%)]  Loss: 3.719 (3.37)  Time: 1.000s, 1024.37/s  (1.011s, 1012.41/s)  LR: 2.354e-04  Data: 0.012 (0.013)
Train: 205 [ 850/1251 ( 68%)]  Loss: 3.333 (3.36)  Time: 0.992s, 1031.74/s  (1.011s, 1013.11/s)  LR: 2.354e-04  Data: 0.011 (0.013)
Train: 205 [ 900/1251 ( 72%)]  Loss: 3.519 (3.37)  Time: 0.996s, 1028.39/s  (1.010s, 1013.68/s)  LR: 2.354e-04  Data: 0.011 (0.013)
Train: 205 [ 950/1251 ( 76%)]  Loss: 3.340 (3.37)  Time: 0.994s, 1030.23/s  (1.010s, 1013.96/s)  LR: 2.354e-04  Data: 0.011 (0.013)
Train: 205 [1000/1251 ( 80%)]  Loss: 3.363 (3.37)  Time: 0.993s, 1031.08/s  (1.010s, 1013.91/s)  LR: 2.354e-04  Data: 0.010 (0.013)
Train: 205 [1050/1251 ( 84%)]  Loss: 3.324 (3.37)  Time: 1.050s,  975.13/s  (1.010s, 1014.19/s)  LR: 2.354e-04  Data: 0.011 (0.013)
Train: 205 [1100/1251 ( 88%)]  Loss: 3.451 (3.37)  Time: 1.004s, 1020.02/s  (1.009s, 1014.38/s)  LR: 2.354e-04  Data: 0.011 (0.012)
Train: 205 [1150/1251 ( 92%)]  Loss: 3.604 (3.38)  Time: 0.995s, 1028.81/s  (1.009s, 1014.69/s)  LR: 2.354e-04  Data: 0.011 (0.012)
Train: 205 [1200/1251 ( 96%)]  Loss: 3.288 (3.38)  Time: 1.003s, 1020.64/s  (1.009s, 1014.84/s)  LR: 2.354e-04  Data: 0.011 (0.012)
Train: 205 [1250/1251 (100%)]  Loss: 3.248 (3.37)  Time: 1.022s, 1001.68/s  (1.009s, 1015.12/s)  LR: 2.354e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.621 (1.621)  Loss:  0.6672 (0.6672)  Acc@1: 90.7227 (90.7227)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.245 (0.571)  Loss:  0.7690 (1.1255)  Acc@1: 85.4953 (77.3180)  Acc@5: 96.5802 (93.8180)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-204.pth.tar', 77.3700001171875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-205.pth.tar', 77.31800003417969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-203.pth.tar', 77.23599987792969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-200.pth.tar', 77.15200000976563)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-201.pth.tar', 77.10400000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-202.pth.tar', 77.00800003173828)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-199.pth.tar', 76.91200003417968)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-193.pth.tar', 76.8279999584961)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-198.pth.tar', 76.82400003417969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-187.pth.tar', 76.79200000732422)

Train: 206 [   0/1251 (  0%)]  Loss: 3.246 (3.25)  Time: 2.589s,  395.45/s  (2.589s,  395.45/s)  LR: 2.311e-04  Data: 1.618 (1.618)
Train: 206 [  50/1251 (  4%)]  Loss: 3.359 (3.30)  Time: 1.002s, 1022.00/s  (1.034s,  989.92/s)  LR: 2.311e-04  Data: 0.011 (0.044)
Train: 206 [ 100/1251 (  8%)]  Loss: 3.722 (3.44)  Time: 0.995s, 1029.42/s  (1.016s, 1008.10/s)  LR: 2.311e-04  Data: 0.011 (0.028)
Train: 206 [ 150/1251 ( 12%)]  Loss: 2.972 (3.32)  Time: 0.996s, 1028.43/s  (1.010s, 1013.77/s)  LR: 2.311e-04  Data: 0.011 (0.022)
Train: 206 [ 200/1251 ( 16%)]  Loss: 3.144 (3.29)  Time: 0.998s, 1026.49/s  (1.009s, 1015.25/s)  LR: 2.311e-04  Data: 0.011 (0.020)
Train: 206 [ 250/1251 ( 20%)]  Loss: 2.804 (3.21)  Time: 0.997s, 1027.37/s  (1.008s, 1015.98/s)  LR: 2.311e-04  Data: 0.011 (0.018)
Train: 206 [ 300/1251 ( 24%)]  Loss: 3.112 (3.19)  Time: 1.049s,  975.80/s  (1.007s, 1017.26/s)  LR: 2.311e-04  Data: 0.011 (0.017)
Train: 206 [ 350/1251 ( 28%)]  Loss: 3.186 (3.19)  Time: 0.995s, 1029.55/s  (1.006s, 1017.79/s)  LR: 2.311e-04  Data: 0.011 (0.016)
Train: 206 [ 400/1251 ( 32%)]  Loss: 3.183 (3.19)  Time: 1.012s, 1012.35/s  (1.005s, 1018.45/s)  LR: 2.311e-04  Data: 0.012 (0.015)
Train: 206 [ 450/1251 ( 36%)]  Loss: 3.423 (3.22)  Time: 0.995s, 1029.44/s  (1.005s, 1018.67/s)  LR: 2.311e-04  Data: 0.010 (0.015)
Train: 206 [ 500/1251 ( 40%)]  Loss: 3.324 (3.23)  Time: 1.057s,  968.71/s  (1.006s, 1018.09/s)  LR: 2.311e-04  Data: 0.011 (0.015)
Train: 206 [ 550/1251 ( 44%)]  Loss: 3.375 (3.24)  Time: 1.050s,  974.91/s  (1.006s, 1017.56/s)  LR: 2.311e-04  Data: 0.011 (0.014)
Train: 206 [ 600/1251 ( 48%)]  Loss: 3.216 (3.24)  Time: 0.994s, 1030.40/s  (1.006s, 1018.01/s)  LR: 2.311e-04  Data: 0.011 (0.014)
Train: 206 [ 650/1251 ( 52%)]  Loss: 3.390 (3.25)  Time: 0.993s, 1031.61/s  (1.006s, 1018.31/s)  LR: 2.311e-04  Data: 0.011 (0.014)
Train: 206 [ 700/1251 ( 56%)]  Loss: 3.408 (3.26)  Time: 0.995s, 1029.33/s  (1.005s, 1018.78/s)  LR: 2.311e-04  Data: 0.011 (0.014)
Train: 206 [ 750/1251 ( 60%)]  Loss: 3.468 (3.27)  Time: 1.002s, 1022.29/s  (1.005s, 1018.44/s)  LR: 2.311e-04  Data: 0.011 (0.014)
Train: 206 [ 800/1251 ( 64%)]  Loss: 3.171 (3.26)  Time: 0.994s, 1030.56/s  (1.005s, 1018.90/s)  LR: 2.311e-04  Data: 0.010 (0.013)
Train: 206 [ 850/1251 ( 68%)]  Loss: 3.292 (3.27)  Time: 1.024s,  999.89/s  (1.005s, 1019.25/s)  LR: 2.311e-04  Data: 0.012 (0.013)
Train: 206 [ 900/1251 ( 72%)]  Loss: 3.678 (3.29)  Time: 0.998s, 1026.34/s  (1.004s, 1019.51/s)  LR: 2.311e-04  Data: 0.011 (0.013)
Train: 206 [ 950/1251 ( 76%)]  Loss: 3.376 (3.29)  Time: 1.057s,  968.62/s  (1.005s, 1019.23/s)  LR: 2.311e-04  Data: 0.012 (0.013)
Train: 206 [1000/1251 ( 80%)]  Loss: 3.470 (3.30)  Time: 0.993s, 1031.25/s  (1.004s, 1019.56/s)  LR: 2.311e-04  Data: 0.010 (0.013)
Train: 206 [1050/1251 ( 84%)]  Loss: 2.820 (3.28)  Time: 0.994s, 1030.21/s  (1.004s, 1019.76/s)  LR: 2.311e-04  Data: 0.011 (0.013)
Train: 206 [1100/1251 ( 88%)]  Loss: 3.292 (3.28)  Time: 0.996s, 1028.42/s  (1.004s, 1019.93/s)  LR: 2.311e-04  Data: 0.012 (0.013)
Train: 206 [1150/1251 ( 92%)]  Loss: 3.165 (3.27)  Time: 0.997s, 1027.46/s  (1.004s, 1020.21/s)  LR: 2.311e-04  Data: 0.010 (0.013)
Train: 206 [1200/1251 ( 96%)]  Loss: 3.389 (3.28)  Time: 0.994s, 1029.70/s  (1.004s, 1020.38/s)  LR: 2.311e-04  Data: 0.011 (0.013)
Train: 206 [1250/1251 (100%)]  Loss: 3.392 (3.28)  Time: 0.988s, 1036.35/s  (1.004s, 1020.29/s)  LR: 2.311e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.651 (1.651)  Loss:  0.6888 (0.6888)  Acc@1: 90.2344 (90.2344)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.245 (0.568)  Loss:  0.6966 (1.1669)  Acc@1: 86.3208 (77.3280)  Acc@5: 97.8774 (93.9060)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-204.pth.tar', 77.3700001171875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-206.pth.tar', 77.32800000488281)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-205.pth.tar', 77.31800003417969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-203.pth.tar', 77.23599987792969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-200.pth.tar', 77.15200000976563)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-201.pth.tar', 77.10400000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-202.pth.tar', 77.00800003173828)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-199.pth.tar', 76.91200003417968)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-193.pth.tar', 76.8279999584961)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-198.pth.tar', 76.82400003417969)

Train: 207 [   0/1251 (  0%)]  Loss: 3.363 (3.36)  Time: 2.408s,  425.23/s  (2.408s,  425.23/s)  LR: 2.268e-04  Data: 1.444 (1.444)
Train: 207 [  50/1251 (  4%)]  Loss: 3.413 (3.39)  Time: 1.050s,  975.11/s  (1.043s,  981.42/s)  LR: 2.268e-04  Data: 0.010 (0.040)
Train: 207 [ 100/1251 (  8%)]  Loss: 3.282 (3.35)  Time: 0.996s, 1027.80/s  (1.021s, 1003.13/s)  LR: 2.268e-04  Data: 0.011 (0.026)
Train: 207 [ 150/1251 ( 12%)]  Loss: 3.216 (3.32)  Time: 0.995s, 1029.54/s  (1.016s, 1007.82/s)  LR: 2.268e-04  Data: 0.012 (0.021)
Train: 207 [ 200/1251 ( 16%)]  Loss: 3.290 (3.31)  Time: 0.996s, 1027.81/s  (1.012s, 1011.64/s)  LR: 2.268e-04  Data: 0.012 (0.019)
Train: 207 [ 250/1251 ( 20%)]  Loss: 3.393 (3.33)  Time: 0.995s, 1029.31/s  (1.010s, 1014.20/s)  LR: 2.268e-04  Data: 0.011 (0.017)
Train: 207 [ 300/1251 ( 24%)]  Loss: 3.208 (3.31)  Time: 1.033s,  991.39/s  (1.009s, 1015.11/s)  LR: 2.268e-04  Data: 0.010 (0.016)
Train: 207 [ 350/1251 ( 28%)]  Loss: 3.336 (3.31)  Time: 1.057s,  968.70/s  (1.015s, 1009.35/s)  LR: 2.268e-04  Data: 0.011 (0.015)
Train: 207 [ 400/1251 ( 32%)]  Loss: 2.801 (3.26)  Time: 0.997s, 1026.91/s  (1.013s, 1010.51/s)  LR: 2.268e-04  Data: 0.010 (0.015)
Train: 207 [ 450/1251 ( 36%)]  Loss: 2.979 (3.23)  Time: 0.993s, 1030.83/s  (1.012s, 1011.93/s)  LR: 2.268e-04  Data: 0.010 (0.014)
Train: 207 [ 500/1251 ( 40%)]  Loss: 3.293 (3.23)  Time: 1.061s,  964.73/s  (1.011s, 1012.37/s)  LR: 2.268e-04  Data: 0.010 (0.014)
Train: 207 [ 550/1251 ( 44%)]  Loss: 3.011 (3.22)  Time: 0.996s, 1028.12/s  (1.011s, 1012.80/s)  LR: 2.268e-04  Data: 0.011 (0.014)
Train: 207 [ 600/1251 ( 48%)]  Loss: 3.400 (3.23)  Time: 1.000s, 1024.32/s  (1.010s, 1013.51/s)  LR: 2.268e-04  Data: 0.011 (0.014)
Train: 207 [ 650/1251 ( 52%)]  Loss: 3.737 (3.27)  Time: 0.996s, 1028.23/s  (1.010s, 1014.29/s)  LR: 2.268e-04  Data: 0.011 (0.013)
Train: 207 [ 700/1251 ( 56%)]  Loss: 3.343 (3.27)  Time: 0.995s, 1029.65/s  (1.009s, 1015.04/s)  LR: 2.268e-04  Data: 0.010 (0.013)
Train: 207 [ 750/1251 ( 60%)]  Loss: 3.345 (3.28)  Time: 1.006s, 1018.07/s  (1.008s, 1015.47/s)  LR: 2.268e-04  Data: 0.012 (0.013)
Train: 207 [ 800/1251 ( 64%)]  Loss: 3.308 (3.28)  Time: 0.996s, 1028.61/s  (1.008s, 1015.98/s)  LR: 2.268e-04  Data: 0.011 (0.013)
Train: 207 [ 850/1251 ( 68%)]  Loss: 3.364 (3.28)  Time: 1.033s,  990.85/s  (1.008s, 1016.18/s)  LR: 2.268e-04  Data: 0.011 (0.013)
Train: 207 [ 900/1251 ( 72%)]  Loss: 3.750 (3.31)  Time: 1.005s, 1018.53/s  (1.008s, 1016.34/s)  LR: 2.268e-04  Data: 0.011 (0.013)
Train: 207 [ 950/1251 ( 76%)]  Loss: 3.341 (3.31)  Time: 0.993s, 1031.06/s  (1.007s, 1016.61/s)  LR: 2.268e-04  Data: 0.011 (0.013)
Train: 207 [1000/1251 ( 80%)]  Loss: 3.335 (3.31)  Time: 1.000s, 1024.40/s  (1.007s, 1017.05/s)  LR: 2.268e-04  Data: 0.011 (0.013)
Train: 207 [1050/1251 ( 84%)]  Loss: 3.367 (3.31)  Time: 1.004s, 1019.56/s  (1.006s, 1017.43/s)  LR: 2.268e-04  Data: 0.015 (0.013)
Train: 207 [1100/1251 ( 88%)]  Loss: 3.467 (3.32)  Time: 0.996s, 1028.43/s  (1.006s, 1017.76/s)  LR: 2.268e-04  Data: 0.011 (0.013)
Train: 207 [1150/1251 ( 92%)]  Loss: 3.803 (3.34)  Time: 0.997s, 1027.47/s  (1.006s, 1017.99/s)  LR: 2.268e-04  Data: 0.011 (0.012)
Train: 207 [1200/1251 ( 96%)]  Loss: 3.007 (3.33)  Time: 1.000s, 1023.51/s  (1.006s, 1018.32/s)  LR: 2.268e-04  Data: 0.011 (0.012)
Train: 207 [1250/1251 (100%)]  Loss: 3.431 (3.33)  Time: 0.982s, 1042.71/s  (1.006s, 1018.34/s)  LR: 2.268e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.663 (1.663)  Loss:  0.6773 (0.6773)  Acc@1: 91.1133 (91.1133)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.245 (0.568)  Loss:  0.9511 (1.1989)  Acc@1: 83.4906 (77.2800)  Acc@5: 96.2264 (93.9260)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-204.pth.tar', 77.3700001171875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-206.pth.tar', 77.32800000488281)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-205.pth.tar', 77.31800003417969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-207.pth.tar', 77.28000006835937)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-203.pth.tar', 77.23599987792969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-200.pth.tar', 77.15200000976563)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-201.pth.tar', 77.10400000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-202.pth.tar', 77.00800003173828)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-199.pth.tar', 76.91200003417968)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-193.pth.tar', 76.8279999584961)

Train: 208 [   0/1251 (  0%)]  Loss: 3.509 (3.51)  Time: 2.567s,  398.89/s  (2.567s,  398.89/s)  LR: 2.225e-04  Data: 1.609 (1.609)
Train: 208 [  50/1251 (  4%)]  Loss: 3.647 (3.58)  Time: 1.006s, 1018.31/s  (1.036s,  987.95/s)  LR: 2.225e-04  Data: 0.011 (0.043)
Train: 208 [ 100/1251 (  8%)]  Loss: 3.528 (3.56)  Time: 0.999s, 1025.47/s  (1.021s, 1002.89/s)  LR: 2.225e-04  Data: 0.012 (0.027)
Train: 208 [ 150/1251 ( 12%)]  Loss: 3.403 (3.52)  Time: 0.994s, 1029.71/s  (1.014s, 1009.61/s)  LR: 2.225e-04  Data: 0.011 (0.022)
Train: 208 [ 200/1251 ( 16%)]  Loss: 3.290 (3.48)  Time: 1.057s,  968.44/s  (1.019s, 1005.18/s)  LR: 2.225e-04  Data: 0.010 (0.019)
Train: 208 [ 250/1251 ( 20%)]  Loss: 3.449 (3.47)  Time: 0.995s, 1028.91/s  (1.016s, 1008.08/s)  LR: 2.225e-04  Data: 0.011 (0.018)
Train: 208 [ 300/1251 ( 24%)]  Loss: 3.228 (3.44)  Time: 0.994s, 1030.33/s  (1.013s, 1010.61/s)  LR: 2.225e-04  Data: 0.010 (0.016)
Train: 208 [ 350/1251 ( 28%)]  Loss: 3.177 (3.40)  Time: 0.996s, 1028.51/s  (1.011s, 1012.51/s)  LR: 2.225e-04  Data: 0.012 (0.016)
Train: 208 [ 400/1251 ( 32%)]  Loss: 3.552 (3.42)  Time: 1.045s,  979.53/s  (1.010s, 1014.00/s)  LR: 2.225e-04  Data: 0.012 (0.015)
Train: 208 [ 450/1251 ( 36%)]  Loss: 3.301 (3.41)  Time: 1.002s, 1021.77/s  (1.009s, 1014.90/s)  LR: 2.225e-04  Data: 0.012 (0.015)
Train: 208 [ 500/1251 ( 40%)]  Loss: 3.211 (3.39)  Time: 0.997s, 1027.13/s  (1.008s, 1015.44/s)  LR: 2.225e-04  Data: 0.011 (0.014)
Train: 208 [ 550/1251 ( 44%)]  Loss: 3.620 (3.41)  Time: 0.999s, 1024.94/s  (1.009s, 1014.67/s)  LR: 2.225e-04  Data: 0.011 (0.014)
Train: 208 [ 600/1251 ( 48%)]  Loss: 3.381 (3.41)  Time: 0.998s, 1025.75/s  (1.008s, 1015.45/s)  LR: 2.225e-04  Data: 0.012 (0.014)
Train: 208 [ 650/1251 ( 52%)]  Loss: 3.551 (3.42)  Time: 0.994s, 1030.42/s  (1.008s, 1016.00/s)  LR: 2.225e-04  Data: 0.011 (0.014)
Train: 208 [ 700/1251 ( 56%)]  Loss: 3.196 (3.40)  Time: 1.007s, 1016.97/s  (1.010s, 1013.99/s)  LR: 2.225e-04  Data: 0.011 (0.013)
Train: 208 [ 750/1251 ( 60%)]  Loss: 3.242 (3.39)  Time: 1.027s,  997.42/s  (1.009s, 1014.66/s)  LR: 2.225e-04  Data: 0.012 (0.013)
Train: 208 [ 800/1251 ( 64%)]  Loss: 3.068 (3.37)  Time: 0.995s, 1029.58/s  (1.009s, 1015.29/s)  LR: 2.225e-04  Data: 0.011 (0.013)
Train: 208 [ 850/1251 ( 68%)]  Loss: 3.187 (3.36)  Time: 1.033s,  991.50/s  (1.008s, 1015.63/s)  LR: 2.225e-04  Data: 0.011 (0.013)
Train: 208 [ 900/1251 ( 72%)]  Loss: 3.370 (3.36)  Time: 1.017s, 1006.69/s  (1.008s, 1015.93/s)  LR: 2.225e-04  Data: 0.011 (0.013)
Train: 208 [ 950/1251 ( 76%)]  Loss: 3.223 (3.36)  Time: 0.997s, 1026.76/s  (1.007s, 1016.46/s)  LR: 2.225e-04  Data: 0.011 (0.013)
Train: 208 [1000/1251 ( 80%)]  Loss: 3.533 (3.36)  Time: 1.004s, 1020.23/s  (1.007s, 1016.91/s)  LR: 2.225e-04  Data: 0.011 (0.013)
Train: 208 [1050/1251 ( 84%)]  Loss: 3.438 (3.37)  Time: 0.996s, 1027.73/s  (1.007s, 1017.29/s)  LR: 2.225e-04  Data: 0.012 (0.013)
Train: 208 [1100/1251 ( 88%)]  Loss: 3.314 (3.37)  Time: 1.004s, 1020.07/s  (1.006s, 1017.52/s)  LR: 2.225e-04  Data: 0.012 (0.013)
Train: 208 [1150/1251 ( 92%)]  Loss: 3.355 (3.37)  Time: 1.009s, 1015.15/s  (1.007s, 1016.88/s)  LR: 2.225e-04  Data: 0.010 (0.013)
Train: 208 [1200/1251 ( 96%)]  Loss: 3.565 (3.37)  Time: 0.998s, 1026.53/s  (1.007s, 1017.23/s)  LR: 2.225e-04  Data: 0.012 (0.013)
Train: 208 [1250/1251 (100%)]  Loss: 3.492 (3.38)  Time: 0.982s, 1043.09/s  (1.007s, 1017.27/s)  LR: 2.225e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.648 (1.648)  Loss:  0.6828 (0.6828)  Acc@1: 90.8203 (90.8203)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.245 (0.567)  Loss:  0.8030 (1.1621)  Acc@1: 84.9057 (77.4900)  Acc@5: 97.0519 (93.8440)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-208.pth.tar', 77.4900000366211)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-204.pth.tar', 77.3700001171875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-206.pth.tar', 77.32800000488281)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-205.pth.tar', 77.31800003417969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-207.pth.tar', 77.28000006835937)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-203.pth.tar', 77.23599987792969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-200.pth.tar', 77.15200000976563)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-201.pth.tar', 77.10400000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-202.pth.tar', 77.00800003173828)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-199.pth.tar', 76.91200003417968)

Train: 209 [   0/1251 (  0%)]  Loss: 3.138 (3.14)  Time: 2.583s,  396.44/s  (2.583s,  396.44/s)  LR: 2.183e-04  Data: 1.620 (1.620)
Train: 209 [  50/1251 (  4%)]  Loss: 3.406 (3.27)  Time: 0.996s, 1028.25/s  (1.034s,  990.41/s)  LR: 2.183e-04  Data: 0.011 (0.043)
Train: 209 [ 100/1251 (  8%)]  Loss: 3.141 (3.23)  Time: 1.001s, 1023.01/s  (1.036s,  988.89/s)  LR: 2.183e-04  Data: 0.011 (0.027)
Train: 209 [ 150/1251 ( 12%)]  Loss: 3.257 (3.24)  Time: 1.043s,  981.41/s  (1.025s,  999.18/s)  LR: 2.183e-04  Data: 0.010 (0.022)
Train: 209 [ 200/1251 ( 16%)]  Loss: 3.220 (3.23)  Time: 0.997s, 1026.94/s  (1.019s, 1004.79/s)  LR: 2.183e-04  Data: 0.011 (0.019)
Train: 209 [ 250/1251 ( 20%)]  Loss: 3.554 (3.29)  Time: 1.037s,  987.04/s  (1.016s, 1008.36/s)  LR: 2.183e-04  Data: 0.011 (0.018)
Train: 209 [ 300/1251 ( 24%)]  Loss: 3.451 (3.31)  Time: 0.995s, 1029.53/s  (1.013s, 1010.52/s)  LR: 2.183e-04  Data: 0.011 (0.016)
Train: 209 [ 350/1251 ( 28%)]  Loss: 3.164 (3.29)  Time: 0.999s, 1024.89/s  (1.013s, 1011.10/s)  LR: 2.183e-04  Data: 0.011 (0.016)
Train: 209 [ 400/1251 ( 32%)]  Loss: 2.965 (3.26)  Time: 0.996s, 1027.96/s  (1.011s, 1012.88/s)  LR: 2.183e-04  Data: 0.011 (0.015)
Train: 209 [ 450/1251 ( 36%)]  Loss: 3.604 (3.29)  Time: 1.002s, 1021.95/s  (1.010s, 1014.08/s)  LR: 2.183e-04  Data: 0.011 (0.015)
Train: 209 [ 500/1251 ( 40%)]  Loss: 3.607 (3.32)  Time: 0.994s, 1029.82/s  (1.009s, 1015.23/s)  LR: 2.183e-04  Data: 0.010 (0.014)
Train: 209 [ 550/1251 ( 44%)]  Loss: 3.377 (3.32)  Time: 0.998s, 1025.94/s  (1.008s, 1016.18/s)  LR: 2.183e-04  Data: 0.011 (0.014)
Train: 209 [ 600/1251 ( 48%)]  Loss: 3.437 (3.33)  Time: 0.992s, 1031.77/s  (1.007s, 1016.53/s)  LR: 2.183e-04  Data: 0.010 (0.014)
Train: 209 [ 650/1251 ( 52%)]  Loss: 3.335 (3.33)  Time: 0.997s, 1026.98/s  (1.007s, 1016.74/s)  LR: 2.183e-04  Data: 0.012 (0.014)
Train: 209 [ 700/1251 ( 56%)]  Loss: 3.541 (3.35)  Time: 0.998s, 1025.97/s  (1.007s, 1017.12/s)  LR: 2.183e-04  Data: 0.011 (0.013)
Train: 209 [ 750/1251 ( 60%)]  Loss: 3.307 (3.34)  Time: 0.997s, 1026.67/s  (1.006s, 1017.44/s)  LR: 2.183e-04  Data: 0.010 (0.013)
Train: 209 [ 800/1251 ( 64%)]  Loss: 3.594 (3.36)  Time: 0.995s, 1028.78/s  (1.006s, 1017.47/s)  LR: 2.183e-04  Data: 0.012 (0.013)
Train: 209 [ 850/1251 ( 68%)]  Loss: 3.174 (3.35)  Time: 0.995s, 1029.25/s  (1.006s, 1017.87/s)  LR: 2.183e-04  Data: 0.011 (0.013)
Train: 209 [ 900/1251 ( 72%)]  Loss: 3.321 (3.35)  Time: 1.058s,  968.04/s  (1.006s, 1017.91/s)  LR: 2.183e-04  Data: 0.011 (0.013)
Train: 209 [ 950/1251 ( 76%)]  Loss: 3.607 (3.36)  Time: 0.994s, 1029.84/s  (1.006s, 1017.56/s)  LR: 2.183e-04  Data: 0.011 (0.013)
Train: 209 [1000/1251 ( 80%)]  Loss: 3.831 (3.38)  Time: 0.993s, 1031.05/s  (1.006s, 1017.94/s)  LR: 2.183e-04  Data: 0.011 (0.013)
Train: 209 [1050/1251 ( 84%)]  Loss: 3.290 (3.38)  Time: 0.993s, 1030.80/s  (1.006s, 1018.19/s)  LR: 2.183e-04  Data: 0.011 (0.013)
Train: 209 [1100/1251 ( 88%)]  Loss: 3.336 (3.38)  Time: 1.037s,  987.25/s  (1.006s, 1018.21/s)  LR: 2.183e-04  Data: 0.011 (0.013)
Train: 209 [1150/1251 ( 92%)]  Loss: 3.268 (3.37)  Time: 1.007s, 1016.61/s  (1.005s, 1018.51/s)  LR: 2.183e-04  Data: 0.010 (0.013)
Train: 209 [1200/1251 ( 96%)]  Loss: 3.667 (3.38)  Time: 1.054s,  971.74/s  (1.005s, 1018.65/s)  LR: 2.183e-04  Data: 0.012 (0.012)
Train: 209 [1250/1251 (100%)]  Loss: 3.275 (3.38)  Time: 0.983s, 1042.04/s  (1.005s, 1018.91/s)  LR: 2.183e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.626 (1.626)  Loss:  0.7148 (0.7148)  Acc@1: 90.6250 (90.6250)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.245 (0.573)  Loss:  0.8419 (1.1968)  Acc@1: 84.1981 (77.4300)  Acc@5: 96.9340 (93.9440)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-208.pth.tar', 77.4900000366211)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-209.pth.tar', 77.42999998779297)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-204.pth.tar', 77.3700001171875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-206.pth.tar', 77.32800000488281)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-205.pth.tar', 77.31800003417969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-207.pth.tar', 77.28000006835937)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-203.pth.tar', 77.23599987792969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-200.pth.tar', 77.15200000976563)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-201.pth.tar', 77.10400000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-202.pth.tar', 77.00800003173828)

Train: 210 [   0/1251 (  0%)]  Loss: 3.338 (3.34)  Time: 4.274s,  239.59/s  (4.274s,  239.59/s)  LR: 2.140e-04  Data: 3.040 (3.040)
Train: 210 [  50/1251 (  4%)]  Loss: 3.192 (3.27)  Time: 1.035s,  989.30/s  (1.076s,  951.37/s)  LR: 2.140e-04  Data: 0.011 (0.071)
Train: 210 [ 100/1251 (  8%)]  Loss: 3.263 (3.26)  Time: 1.045s,  979.68/s  (1.046s,  979.35/s)  LR: 2.140e-04  Data: 0.010 (0.041)
Train: 210 [ 150/1251 ( 12%)]  Loss: 3.695 (3.37)  Time: 0.998s, 1026.52/s  (1.031s,  993.55/s)  LR: 2.140e-04  Data: 0.011 (0.031)
Train: 210 [ 200/1251 ( 16%)]  Loss: 3.249 (3.35)  Time: 0.997s, 1027.13/s  (1.023s, 1001.15/s)  LR: 2.140e-04  Data: 0.012 (0.026)
Train: 210 [ 250/1251 ( 20%)]  Loss: 3.200 (3.32)  Time: 1.007s, 1016.52/s  (1.019s, 1004.51/s)  LR: 2.140e-04  Data: 0.011 (0.023)
Train: 210 [ 300/1251 ( 24%)]  Loss: 3.166 (3.30)  Time: 1.004s, 1019.54/s  (1.016s, 1007.81/s)  LR: 2.140e-04  Data: 0.011 (0.021)
Train: 210 [ 350/1251 ( 28%)]  Loss: 3.657 (3.34)  Time: 1.009s, 1015.25/s  (1.014s, 1009.62/s)  LR: 2.140e-04  Data: 0.011 (0.020)
Train: 210 [ 400/1251 ( 32%)]  Loss: 3.366 (3.35)  Time: 1.003s, 1020.58/s  (1.014s, 1009.46/s)  LR: 2.140e-04  Data: 0.010 (0.019)
Train: 210 [ 450/1251 ( 36%)]  Loss: 3.349 (3.35)  Time: 0.997s, 1027.11/s  (1.013s, 1010.92/s)  LR: 2.140e-04  Data: 0.011 (0.018)
Train: 210 [ 500/1251 ( 40%)]  Loss: 3.587 (3.37)  Time: 1.004s, 1020.25/s  (1.012s, 1011.88/s)  LR: 2.140e-04  Data: 0.012 (0.017)
Train: 210 [ 550/1251 ( 44%)]  Loss: 3.375 (3.37)  Time: 0.998s, 1026.31/s  (1.011s, 1012.69/s)  LR: 2.140e-04  Data: 0.012 (0.017)
Train: 210 [ 600/1251 ( 48%)]  Loss: 3.222 (3.36)  Time: 0.996s, 1027.72/s  (1.010s, 1013.76/s)  LR: 2.140e-04  Data: 0.011 (0.016)
Train: 210 [ 650/1251 ( 52%)]  Loss: 3.675 (3.38)  Time: 0.997s, 1027.43/s  (1.010s, 1014.22/s)  LR: 2.140e-04  Data: 0.011 (0.016)
Train: 210 [ 700/1251 ( 56%)]  Loss: 3.126 (3.36)  Time: 1.003s, 1020.46/s  (1.009s, 1015.03/s)  LR: 2.140e-04  Data: 0.011 (0.015)
Train: 210 [ 750/1251 ( 60%)]  Loss: 3.222 (3.36)  Time: 1.033s,  991.57/s  (1.008s, 1015.39/s)  LR: 2.140e-04  Data: 0.010 (0.015)
Train: 210 [ 800/1251 ( 64%)]  Loss: 3.287 (3.35)  Time: 1.009s, 1014.90/s  (1.008s, 1015.95/s)  LR: 2.140e-04  Data: 0.012 (0.015)
Train: 210 [ 850/1251 ( 68%)]  Loss: 3.166 (3.34)  Time: 0.998s, 1026.19/s  (1.008s, 1016.31/s)  LR: 2.140e-04  Data: 0.012 (0.015)
Train: 210 [ 900/1251 ( 72%)]  Loss: 3.032 (3.32)  Time: 1.056s,  969.48/s  (1.007s, 1016.50/s)  LR: 2.140e-04  Data: 0.010 (0.015)
Train: 210 [ 950/1251 ( 76%)]  Loss: 3.155 (3.32)  Time: 0.998s, 1025.94/s  (1.007s, 1016.92/s)  LR: 2.140e-04  Data: 0.011 (0.014)
Train: 210 [1000/1251 ( 80%)]  Loss: 3.414 (3.32)  Time: 1.040s,  984.22/s  (1.007s, 1016.49/s)  LR: 2.140e-04  Data: 0.011 (0.014)
Train: 210 [1050/1251 ( 84%)]  Loss: 3.102 (3.31)  Time: 0.994s, 1029.73/s  (1.008s, 1016.27/s)  LR: 2.140e-04  Data: 0.011 (0.014)
Train: 210 [1100/1251 ( 88%)]  Loss: 3.395 (3.31)  Time: 0.994s, 1029.72/s  (1.007s, 1016.65/s)  LR: 2.140e-04  Data: 0.015 (0.014)
Train: 210 [1150/1251 ( 92%)]  Loss: 3.257 (3.31)  Time: 1.011s, 1013.23/s  (1.007s, 1016.94/s)  LR: 2.140e-04  Data: 0.013 (0.014)
Train: 210 [1200/1251 ( 96%)]  Loss: 3.182 (3.31)  Time: 1.002s, 1022.00/s  (1.007s, 1017.26/s)  LR: 2.140e-04  Data: 0.012 (0.014)
Train: 210 [1250/1251 (100%)]  Loss: 2.998 (3.29)  Time: 1.028s,  996.17/s  (1.006s, 1017.42/s)  LR: 2.140e-04  Data: 0.000 (0.014)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.711 (1.711)  Loss:  0.8312 (0.8312)  Acc@1: 90.9180 (90.9180)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.245 (0.570)  Loss:  0.8503 (1.2620)  Acc@1: 86.4387 (77.2880)  Acc@5: 96.9340 (93.8860)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-208.pth.tar', 77.4900000366211)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-209.pth.tar', 77.42999998779297)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-204.pth.tar', 77.3700001171875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-206.pth.tar', 77.32800000488281)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-205.pth.tar', 77.31800003417969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-210.pth.tar', 77.28800005615234)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-207.pth.tar', 77.28000006835937)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-203.pth.tar', 77.23599987792969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-200.pth.tar', 77.15200000976563)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-201.pth.tar', 77.10400000732422)

Train: 211 [   0/1251 (  0%)]  Loss: 3.217 (3.22)  Time: 2.694s,  380.11/s  (2.694s,  380.11/s)  LR: 2.099e-04  Data: 1.741 (1.741)
Train: 211 [  50/1251 (  4%)]  Loss: 3.258 (3.24)  Time: 1.000s, 1023.52/s  (1.040s,  984.86/s)  LR: 2.099e-04  Data: 0.013 (0.049)
Train: 211 [ 100/1251 (  8%)]  Loss: 3.192 (3.22)  Time: 0.996s, 1027.81/s  (1.021s, 1002.98/s)  LR: 2.099e-04  Data: 0.013 (0.030)
Train: 211 [ 150/1251 ( 12%)]  Loss: 3.350 (3.25)  Time: 0.996s, 1028.41/s  (1.015s, 1008.87/s)  LR: 2.099e-04  Data: 0.011 (0.024)
Train: 211 [ 200/1251 ( 16%)]  Loss: 3.262 (3.26)  Time: 0.995s, 1029.09/s  (1.011s, 1012.66/s)  LR: 2.099e-04  Data: 0.011 (0.021)
Train: 211 [ 250/1251 ( 20%)]  Loss: 3.536 (3.30)  Time: 1.055s,  970.99/s  (1.009s, 1015.02/s)  LR: 2.099e-04  Data: 0.010 (0.019)
Train: 211 [ 300/1251 ( 24%)]  Loss: 3.196 (3.29)  Time: 0.994s, 1030.22/s  (1.010s, 1013.96/s)  LR: 2.099e-04  Data: 0.011 (0.018)
Train: 211 [ 350/1251 ( 28%)]  Loss: 3.208 (3.28)  Time: 0.995s, 1028.86/s  (1.009s, 1014.98/s)  LR: 2.099e-04  Data: 0.011 (0.017)
Train: 211 [ 400/1251 ( 32%)]  Loss: 3.284 (3.28)  Time: 1.000s, 1024.06/s  (1.009s, 1015.21/s)  LR: 2.099e-04  Data: 0.011 (0.016)
Train: 211 [ 450/1251 ( 36%)]  Loss: 3.180 (3.27)  Time: 1.065s,  961.06/s  (1.010s, 1013.96/s)  LR: 2.099e-04  Data: 0.012 (0.015)
Train: 211 [ 500/1251 ( 40%)]  Loss: 3.669 (3.30)  Time: 1.011s, 1013.07/s  (1.009s, 1014.62/s)  LR: 2.099e-04  Data: 0.012 (0.015)
Train: 211 [ 550/1251 ( 44%)]  Loss: 3.797 (3.35)  Time: 0.998s, 1026.28/s  (1.008s, 1015.41/s)  LR: 2.099e-04  Data: 0.011 (0.015)
Train: 211 [ 600/1251 ( 48%)]  Loss: 3.687 (3.37)  Time: 1.001s, 1023.45/s  (1.008s, 1016.18/s)  LR: 2.099e-04  Data: 0.011 (0.014)
Train: 211 [ 650/1251 ( 52%)]  Loss: 3.258 (3.36)  Time: 0.998s, 1025.78/s  (1.007s, 1016.46/s)  LR: 2.099e-04  Data: 0.011 (0.014)
Train: 211 [ 700/1251 ( 56%)]  Loss: 3.501 (3.37)  Time: 0.997s, 1027.16/s  (1.007s, 1017.08/s)  LR: 2.099e-04  Data: 0.011 (0.014)
Train: 211 [ 750/1251 ( 60%)]  Loss: 3.302 (3.37)  Time: 0.995s, 1029.32/s  (1.006s, 1017.49/s)  LR: 2.099e-04  Data: 0.012 (0.014)
Train: 211 [ 800/1251 ( 64%)]  Loss: 3.228 (3.36)  Time: 1.007s, 1016.87/s  (1.007s, 1017.32/s)  LR: 2.099e-04  Data: 0.011 (0.014)
Train: 211 [ 850/1251 ( 68%)]  Loss: 3.345 (3.36)  Time: 0.993s, 1030.82/s  (1.006s, 1017.73/s)  LR: 2.099e-04  Data: 0.011 (0.013)
Train: 211 [ 900/1251 ( 72%)]  Loss: 3.483 (3.37)  Time: 1.004s, 1020.31/s  (1.006s, 1017.86/s)  LR: 2.099e-04  Data: 0.014 (0.013)
Train: 211 [ 950/1251 ( 76%)]  Loss: 3.303 (3.36)  Time: 0.995s, 1029.24/s  (1.006s, 1018.22/s)  LR: 2.099e-04  Data: 0.011 (0.013)
Train: 211 [1000/1251 ( 80%)]  Loss: 3.196 (3.35)  Time: 0.997s, 1026.98/s  (1.005s, 1018.49/s)  LR: 2.099e-04  Data: 0.012 (0.013)
Train: 211 [1050/1251 ( 84%)]  Loss: 3.472 (3.36)  Time: 0.999s, 1025.23/s  (1.006s, 1018.29/s)  LR: 2.099e-04  Data: 0.011 (0.013)
Train: 211 [1100/1251 ( 88%)]  Loss: 3.417 (3.36)  Time: 0.997s, 1026.64/s  (1.005s, 1018.47/s)  LR: 2.099e-04  Data: 0.011 (0.013)
Train: 211 [1150/1251 ( 92%)]  Loss: 3.308 (3.36)  Time: 0.995s, 1028.66/s  (1.005s, 1018.63/s)  LR: 2.099e-04  Data: 0.010 (0.013)
Train: 211 [1200/1251 ( 96%)]  Loss: 3.208 (3.35)  Time: 1.001s, 1022.58/s  (1.005s, 1018.78/s)  LR: 2.099e-04  Data: 0.010 (0.013)
Train: 211 [1250/1251 (100%)]  Loss: 3.150 (3.35)  Time: 1.019s, 1004.80/s  (1.006s, 1018.18/s)  LR: 2.099e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.603 (1.603)  Loss:  0.7265 (0.7265)  Acc@1: 91.1133 (91.1133)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.245 (0.568)  Loss:  0.7720 (1.1856)  Acc@1: 86.0849 (77.6140)  Acc@5: 97.1698 (94.0720)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-211.pth.tar', 77.61400003173829)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-208.pth.tar', 77.4900000366211)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-209.pth.tar', 77.42999998779297)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-204.pth.tar', 77.3700001171875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-206.pth.tar', 77.32800000488281)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-205.pth.tar', 77.31800003417969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-210.pth.tar', 77.28800005615234)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-207.pth.tar', 77.28000006835937)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-203.pth.tar', 77.23599987792969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-200.pth.tar', 77.15200000976563)

Train: 212 [   0/1251 (  0%)]  Loss: 3.531 (3.53)  Time: 2.587s,  395.75/s  (2.587s,  395.75/s)  LR: 2.057e-04  Data: 1.622 (1.622)
Train: 212 [  50/1251 (  4%)]  Loss: 3.450 (3.49)  Time: 0.997s, 1027.44/s  (1.039s,  985.33/s)  LR: 2.057e-04  Data: 0.011 (0.045)
Train: 212 [ 100/1251 (  8%)]  Loss: 3.330 (3.44)  Time: 0.999s, 1025.24/s  (1.021s, 1003.16/s)  LR: 2.057e-04  Data: 0.012 (0.028)
Train: 212 [ 150/1251 ( 12%)]  Loss: 3.445 (3.44)  Time: 1.044s,  980.63/s  (1.015s, 1008.61/s)  LR: 2.057e-04  Data: 0.012 (0.023)
Train: 212 [ 200/1251 ( 16%)]  Loss: 3.318 (3.42)  Time: 0.997s, 1027.21/s  (1.013s, 1010.37/s)  LR: 2.057e-04  Data: 0.012 (0.020)
Train: 212 [ 250/1251 ( 20%)]  Loss: 3.312 (3.40)  Time: 0.993s, 1031.17/s  (1.011s, 1013.36/s)  LR: 2.057e-04  Data: 0.011 (0.018)
Train: 212 [ 300/1251 ( 24%)]  Loss: 3.441 (3.40)  Time: 0.993s, 1031.23/s  (1.009s, 1014.73/s)  LR: 2.057e-04  Data: 0.011 (0.017)
Train: 212 [ 350/1251 ( 28%)]  Loss: 3.638 (3.43)  Time: 1.002s, 1022.36/s  (1.009s, 1014.44/s)  LR: 2.057e-04  Data: 0.010 (0.016)
Train: 212 [ 400/1251 ( 32%)]  Loss: 3.454 (3.44)  Time: 0.996s, 1027.87/s  (1.008s, 1015.92/s)  LR: 2.057e-04  Data: 0.010 (0.016)
Train: 212 [ 450/1251 ( 36%)]  Loss: 3.193 (3.41)  Time: 0.997s, 1027.54/s  (1.007s, 1016.83/s)  LR: 2.057e-04  Data: 0.011 (0.015)
Train: 212 [ 500/1251 ( 40%)]  Loss: 3.489 (3.42)  Time: 1.003s, 1020.44/s  (1.007s, 1017.22/s)  LR: 2.057e-04  Data: 0.013 (0.015)
Train: 212 [ 550/1251 ( 44%)]  Loss: 3.194 (3.40)  Time: 0.999s, 1025.07/s  (1.006s, 1017.98/s)  LR: 2.057e-04  Data: 0.011 (0.014)
Train: 212 [ 600/1251 ( 48%)]  Loss: 3.102 (3.38)  Time: 1.005s, 1018.66/s  (1.006s, 1018.19/s)  LR: 2.057e-04  Data: 0.012 (0.014)
Train: 212 [ 650/1251 ( 52%)]  Loss: 3.445 (3.38)  Time: 0.994s, 1029.67/s  (1.006s, 1017.80/s)  LR: 2.057e-04  Data: 0.011 (0.014)
Train: 212 [ 700/1251 ( 56%)]  Loss: 3.128 (3.36)  Time: 0.999s, 1025.02/s  (1.006s, 1018.24/s)  LR: 2.057e-04  Data: 0.011 (0.014)
Train: 212 [ 750/1251 ( 60%)]  Loss: 3.138 (3.35)  Time: 1.009s, 1014.78/s  (1.005s, 1018.62/s)  LR: 2.057e-04  Data: 0.011 (0.014)
Train: 212 [ 800/1251 ( 64%)]  Loss: 3.191 (3.34)  Time: 0.996s, 1028.02/s  (1.005s, 1019.03/s)  LR: 2.057e-04  Data: 0.011 (0.013)
Train: 212 [ 850/1251 ( 68%)]  Loss: 3.146 (3.33)  Time: 0.992s, 1032.34/s  (1.005s, 1019.22/s)  LR: 2.057e-04  Data: 0.011 (0.013)
Train: 212 [ 900/1251 ( 72%)]  Loss: 2.950 (3.31)  Time: 0.996s, 1028.46/s  (1.004s, 1019.69/s)  LR: 2.057e-04  Data: 0.011 (0.013)
Train: 212 [ 950/1251 ( 76%)]  Loss: 3.406 (3.32)  Time: 1.000s, 1024.47/s  (1.004s, 1019.60/s)  LR: 2.057e-04  Data: 0.011 (0.013)
Train: 212 [1000/1251 ( 80%)]  Loss: 3.170 (3.31)  Time: 1.062s,  963.88/s  (1.005s, 1018.84/s)  LR: 2.057e-04  Data: 0.011 (0.013)
Train: 212 [1050/1251 ( 84%)]  Loss: 3.332 (3.31)  Time: 0.998s, 1026.50/s  (1.005s, 1018.61/s)  LR: 2.057e-04  Data: 0.012 (0.013)
Train: 212 [1100/1251 ( 88%)]  Loss: 3.308 (3.31)  Time: 1.000s, 1024.03/s  (1.006s, 1018.33/s)  LR: 2.057e-04  Data: 0.010 (0.013)
Train: 212 [1150/1251 ( 92%)]  Loss: 3.103 (3.30)  Time: 0.997s, 1026.89/s  (1.005s, 1018.46/s)  LR: 2.057e-04  Data: 0.016 (0.013)
Train: 212 [1200/1251 ( 96%)]  Loss: 3.509 (3.31)  Time: 0.992s, 1032.33/s  (1.005s, 1018.77/s)  LR: 2.057e-04  Data: 0.011 (0.013)
Train: 212 [1250/1251 (100%)]  Loss: 3.591 (3.32)  Time: 0.984s, 1040.87/s  (1.005s, 1019.09/s)  LR: 2.057e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.677 (1.677)  Loss:  0.7161 (0.7161)  Acc@1: 91.2109 (91.2109)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.245 (0.567)  Loss:  0.7606 (1.1659)  Acc@1: 85.3774 (77.4660)  Acc@5: 97.2877 (94.0680)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-211.pth.tar', 77.61400003173829)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-208.pth.tar', 77.4900000366211)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-212.pth.tar', 77.46599998291016)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-209.pth.tar', 77.42999998779297)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-204.pth.tar', 77.3700001171875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-206.pth.tar', 77.32800000488281)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-205.pth.tar', 77.31800003417969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-210.pth.tar', 77.28800005615234)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-207.pth.tar', 77.28000006835937)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-203.pth.tar', 77.23599987792969)

Train: 213 [   0/1251 (  0%)]  Loss: 3.555 (3.56)  Time: 2.569s,  398.64/s  (2.569s,  398.64/s)  LR: 2.016e-04  Data: 1.572 (1.572)
Train: 213 [  50/1251 (  4%)]  Loss: 3.577 (3.57)  Time: 0.998s, 1026.45/s  (1.041s,  984.03/s)  LR: 2.016e-04  Data: 0.012 (0.042)
Train: 213 [ 100/1251 (  8%)]  Loss: 3.297 (3.48)  Time: 1.046s,  978.67/s  (1.024s,  999.73/s)  LR: 2.016e-04  Data: 0.011 (0.027)
Train: 213 [ 150/1251 ( 12%)]  Loss: 3.116 (3.39)  Time: 1.037s,  987.25/s  (1.021s, 1002.93/s)  LR: 2.016e-04  Data: 0.010 (0.022)
Train: 213 [ 200/1251 ( 16%)]  Loss: 3.597 (3.43)  Time: 1.038s,  986.47/s  (1.018s, 1006.38/s)  LR: 2.016e-04  Data: 0.011 (0.019)
Train: 213 [ 250/1251 ( 20%)]  Loss: 3.260 (3.40)  Time: 1.064s,  962.08/s  (1.020s, 1003.94/s)  LR: 2.016e-04  Data: 0.011 (0.017)
Train: 213 [ 300/1251 ( 24%)]  Loss: 3.269 (3.38)  Time: 1.002s, 1022.24/s  (1.018s, 1005.94/s)  LR: 2.016e-04  Data: 0.013 (0.016)
Train: 213 [ 350/1251 ( 28%)]  Loss: 3.090 (3.35)  Time: 1.018s, 1006.20/s  (1.015s, 1008.51/s)  LR: 2.016e-04  Data: 0.011 (0.016)
Train: 213 [ 400/1251 ( 32%)]  Loss: 2.959 (3.30)  Time: 1.001s, 1023.12/s  (1.014s, 1010.13/s)  LR: 2.016e-04  Data: 0.013 (0.015)
Train: 213 [ 450/1251 ( 36%)]  Loss: 3.292 (3.30)  Time: 0.996s, 1028.00/s  (1.012s, 1011.94/s)  LR: 2.016e-04  Data: 0.012 (0.015)
Train: 213 [ 500/1251 ( 40%)]  Loss: 3.371 (3.31)  Time: 0.994s, 1030.21/s  (1.011s, 1012.50/s)  LR: 2.016e-04  Data: 0.011 (0.014)
Train: 213 [ 550/1251 ( 44%)]  Loss: 3.267 (3.30)  Time: 0.994s, 1029.86/s  (1.011s, 1013.16/s)  LR: 2.016e-04  Data: 0.010 (0.014)
Train: 213 [ 600/1251 ( 48%)]  Loss: 2.949 (3.28)  Time: 0.994s, 1029.81/s  (1.010s, 1013.94/s)  LR: 2.016e-04  Data: 0.010 (0.014)
Train: 213 [ 650/1251 ( 52%)]  Loss: 3.044 (3.26)  Time: 1.063s,  963.09/s  (1.010s, 1013.69/s)  LR: 2.016e-04  Data: 0.012 (0.014)
Train: 213 [ 700/1251 ( 56%)]  Loss: 3.597 (3.28)  Time: 1.000s, 1023.71/s  (1.009s, 1014.42/s)  LR: 2.016e-04  Data: 0.011 (0.013)
Train: 213 [ 750/1251 ( 60%)]  Loss: 3.690 (3.31)  Time: 0.997s, 1027.09/s  (1.009s, 1014.55/s)  LR: 2.016e-04  Data: 0.012 (0.013)
Train: 213 [ 800/1251 ( 64%)]  Loss: 3.484 (3.32)  Time: 0.997s, 1026.90/s  (1.009s, 1014.99/s)  LR: 2.016e-04  Data: 0.011 (0.013)
Train: 213 [ 850/1251 ( 68%)]  Loss: 3.313 (3.32)  Time: 0.998s, 1026.25/s  (1.009s, 1015.05/s)  LR: 2.016e-04  Data: 0.012 (0.013)
Train: 213 [ 900/1251 ( 72%)]  Loss: 3.390 (3.32)  Time: 1.018s, 1005.72/s  (1.009s, 1015.25/s)  LR: 2.016e-04  Data: 0.014 (0.013)
Train: 213 [ 950/1251 ( 76%)]  Loss: 3.414 (3.33)  Time: 0.996s, 1028.41/s  (1.008s, 1015.64/s)  LR: 2.016e-04  Data: 0.011 (0.013)
Train: 213 [1000/1251 ( 80%)]  Loss: 3.035 (3.31)  Time: 0.997s, 1026.96/s  (1.008s, 1015.47/s)  LR: 2.016e-04  Data: 0.012 (0.013)
Train: 213 [1050/1251 ( 84%)]  Loss: 3.418 (3.32)  Time: 0.999s, 1024.99/s  (1.008s, 1015.86/s)  LR: 2.016e-04  Data: 0.011 (0.013)
Train: 213 [1100/1251 ( 88%)]  Loss: 3.540 (3.33)  Time: 0.995s, 1028.86/s  (1.008s, 1016.32/s)  LR: 2.016e-04  Data: 0.011 (0.013)
Train: 213 [1150/1251 ( 92%)]  Loss: 3.163 (3.32)  Time: 1.034s,  990.26/s  (1.008s, 1016.11/s)  LR: 2.016e-04  Data: 0.014 (0.012)
Train: 213 [1200/1251 ( 96%)]  Loss: 3.648 (3.33)  Time: 1.032s,  992.32/s  (1.008s, 1016.10/s)  LR: 2.016e-04  Data: 0.011 (0.012)
Train: 213 [1250/1251 (100%)]  Loss: 3.276 (3.33)  Time: 0.983s, 1041.21/s  (1.008s, 1015.40/s)  LR: 2.016e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.656 (1.656)  Loss:  0.7186 (0.7186)  Acc@1: 90.9180 (90.9180)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.245 (0.568)  Loss:  0.7599 (1.1481)  Acc@1: 86.9104 (77.9020)  Acc@5: 97.5236 (94.1820)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-213.pth.tar', 77.90200013183593)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-211.pth.tar', 77.61400003173829)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-208.pth.tar', 77.4900000366211)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-212.pth.tar', 77.46599998291016)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-209.pth.tar', 77.42999998779297)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-204.pth.tar', 77.3700001171875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-206.pth.tar', 77.32800000488281)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-205.pth.tar', 77.31800003417969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-210.pth.tar', 77.28800005615234)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-207.pth.tar', 77.28000006835937)

Train: 214 [   0/1251 (  0%)]  Loss: 3.384 (3.38)  Time: 2.513s,  407.48/s  (2.513s,  407.48/s)  LR: 1.975e-04  Data: 1.549 (1.549)
Train: 214 [  50/1251 (  4%)]  Loss: 3.459 (3.42)  Time: 0.999s, 1024.55/s  (1.030s,  994.36/s)  LR: 1.975e-04  Data: 0.012 (0.042)
Train: 214 [ 100/1251 (  8%)]  Loss: 3.371 (3.40)  Time: 0.999s, 1025.28/s  (1.016s, 1008.05/s)  LR: 1.975e-04  Data: 0.011 (0.027)
Train: 214 [ 150/1251 ( 12%)]  Loss: 3.511 (3.43)  Time: 1.003s, 1021.02/s  (1.013s, 1011.17/s)  LR: 1.975e-04  Data: 0.012 (0.022)
Train: 214 [ 200/1251 ( 16%)]  Loss: 3.570 (3.46)  Time: 0.994s, 1030.50/s  (1.015s, 1008.91/s)  LR: 1.975e-04  Data: 0.011 (0.019)
Train: 214 [ 250/1251 ( 20%)]  Loss: 3.447 (3.46)  Time: 1.012s, 1012.26/s  (1.012s, 1011.47/s)  LR: 1.975e-04  Data: 0.011 (0.017)
Train: 214 [ 300/1251 ( 24%)]  Loss: 2.963 (3.39)  Time: 0.995s, 1029.27/s  (1.011s, 1012.70/s)  LR: 1.975e-04  Data: 0.011 (0.016)
Train: 214 [ 350/1251 ( 28%)]  Loss: 3.126 (3.35)  Time: 0.994s, 1029.75/s  (1.014s, 1009.79/s)  LR: 1.975e-04  Data: 0.011 (0.016)
Train: 214 [ 400/1251 ( 32%)]  Loss: 3.291 (3.35)  Time: 0.997s, 1027.28/s  (1.012s, 1011.96/s)  LR: 1.975e-04  Data: 0.012 (0.015)
Train: 214 [ 450/1251 ( 36%)]  Loss: 3.257 (3.34)  Time: 1.021s, 1002.55/s  (1.013s, 1010.70/s)  LR: 1.975e-04  Data: 0.011 (0.015)
Train: 214 [ 500/1251 ( 40%)]  Loss: 3.324 (3.34)  Time: 1.052s,  973.14/s  (1.016s, 1007.78/s)  LR: 1.975e-04  Data: 0.011 (0.014)
Train: 214 [ 550/1251 ( 44%)]  Loss: 3.445 (3.35)  Time: 1.002s, 1021.47/s  (1.016s, 1007.43/s)  LR: 1.975e-04  Data: 0.011 (0.014)
Train: 214 [ 600/1251 ( 48%)]  Loss: 3.296 (3.34)  Time: 1.005s, 1018.55/s  (1.016s, 1008.22/s)  LR: 1.975e-04  Data: 0.011 (0.014)
Train: 214 [ 650/1251 ( 52%)]  Loss: 3.266 (3.34)  Time: 1.057s,  969.10/s  (1.015s, 1008.63/s)  LR: 1.975e-04  Data: 0.011 (0.013)
Train: 214 [ 700/1251 ( 56%)]  Loss: 3.353 (3.34)  Time: 0.997s, 1026.65/s  (1.015s, 1008.44/s)  LR: 1.975e-04  Data: 0.012 (0.013)
Train: 214 [ 750/1251 ( 60%)]  Loss: 3.694 (3.36)  Time: 0.995s, 1029.64/s  (1.015s, 1008.79/s)  LR: 1.975e-04  Data: 0.010 (0.013)
Train: 214 [ 800/1251 ( 64%)]  Loss: 3.127 (3.35)  Time: 1.004s, 1020.32/s  (1.015s, 1009.07/s)  LR: 1.975e-04  Data: 0.014 (0.013)
Train: 214 [ 850/1251 ( 68%)]  Loss: 3.705 (3.37)  Time: 1.026s,  997.82/s  (1.015s, 1009.08/s)  LR: 1.975e-04  Data: 0.012 (0.013)
Train: 214 [ 900/1251 ( 72%)]  Loss: 2.999 (3.35)  Time: 0.990s, 1033.94/s  (1.014s, 1009.83/s)  LR: 1.975e-04  Data: 0.010 (0.013)
Train: 214 [ 950/1251 ( 76%)]  Loss: 3.244 (3.34)  Time: 0.999s, 1024.72/s  (1.014s, 1010.10/s)  LR: 1.975e-04  Data: 0.010 (0.013)
Train: 214 [1000/1251 ( 80%)]  Loss: 3.386 (3.34)  Time: 0.997s, 1026.69/s  (1.014s, 1009.84/s)  LR: 1.975e-04  Data: 0.011 (0.013)
Train: 214 [1050/1251 ( 84%)]  Loss: 3.423 (3.35)  Time: 1.012s, 1012.24/s  (1.013s, 1010.39/s)  LR: 1.975e-04  Data: 0.011 (0.013)
Train: 214 [1100/1251 ( 88%)]  Loss: 3.467 (3.35)  Time: 1.003s, 1021.12/s  (1.013s, 1010.97/s)  LR: 1.975e-04  Data: 0.014 (0.013)
Train: 214 [1150/1251 ( 92%)]  Loss: 3.415 (3.36)  Time: 1.000s, 1023.58/s  (1.013s, 1010.96/s)  LR: 1.975e-04  Data: 0.014 (0.012)
Train: 214 [1200/1251 ( 96%)]  Loss: 3.221 (3.35)  Time: 1.003s, 1021.41/s  (1.012s, 1011.51/s)  LR: 1.975e-04  Data: 0.011 (0.012)
Train: 214 [1250/1251 (100%)]  Loss: 3.401 (3.35)  Time: 0.984s, 1040.77/s  (1.012s, 1011.98/s)  LR: 1.975e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.602 (1.602)  Loss:  0.6967 (0.6967)  Acc@1: 91.1133 (91.1133)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.245 (0.574)  Loss:  0.8182 (1.1758)  Acc@1: 85.8491 (77.7560)  Acc@5: 97.1698 (94.1440)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-213.pth.tar', 77.90200013183593)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-214.pth.tar', 77.75600005859376)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-211.pth.tar', 77.61400003173829)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-208.pth.tar', 77.4900000366211)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-212.pth.tar', 77.46599998291016)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-209.pth.tar', 77.42999998779297)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-204.pth.tar', 77.3700001171875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-206.pth.tar', 77.32800000488281)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-205.pth.tar', 77.31800003417969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-210.pth.tar', 77.28800005615234)

Train: 215 [   0/1251 (  0%)]  Loss: 3.498 (3.50)  Time: 2.599s,  393.98/s  (2.599s,  393.98/s)  LR: 1.935e-04  Data: 1.633 (1.633)
Train: 215 [  50/1251 (  4%)]  Loss: 3.415 (3.46)  Time: 1.030s,  994.41/s  (1.039s,  985.88/s)  LR: 1.935e-04  Data: 0.012 (0.043)
Train: 215 [ 100/1251 (  8%)]  Loss: 3.074 (3.33)  Time: 1.032s,  992.09/s  (1.025s,  999.35/s)  LR: 1.935e-04  Data: 0.011 (0.027)
Train: 215 [ 150/1251 ( 12%)]  Loss: 3.106 (3.27)  Time: 1.001s, 1022.73/s  (1.018s, 1005.63/s)  LR: 1.935e-04  Data: 0.013 (0.022)
Train: 215 [ 200/1251 ( 16%)]  Loss: 3.422 (3.30)  Time: 0.998s, 1025.97/s  (1.016s, 1008.09/s)  LR: 1.935e-04  Data: 0.011 (0.019)
Train: 215 [ 250/1251 ( 20%)]  Loss: 3.104 (3.27)  Time: 0.999s, 1024.92/s  (1.013s, 1011.13/s)  LR: 1.935e-04  Data: 0.011 (0.018)
Train: 215 [ 300/1251 ( 24%)]  Loss: 3.199 (3.26)  Time: 0.998s, 1025.84/s  (1.011s, 1013.06/s)  LR: 1.935e-04  Data: 0.012 (0.017)
Train: 215 [ 350/1251 ( 28%)]  Loss: 3.541 (3.29)  Time: 0.999s, 1025.10/s  (1.010s, 1014.04/s)  LR: 1.935e-04  Data: 0.010 (0.016)
Train: 215 [ 400/1251 ( 32%)]  Loss: 3.521 (3.32)  Time: 1.011s, 1013.18/s  (1.008s, 1015.54/s)  LR: 1.935e-04  Data: 0.010 (0.015)
Train: 215 [ 450/1251 ( 36%)]  Loss: 3.153 (3.30)  Time: 0.996s, 1027.86/s  (1.007s, 1016.38/s)  LR: 1.935e-04  Data: 0.011 (0.015)
Train: 215 [ 500/1251 ( 40%)]  Loss: 3.177 (3.29)  Time: 1.003s, 1020.47/s  (1.007s, 1016.67/s)  LR: 1.935e-04  Data: 0.011 (0.014)
Train: 215 [ 550/1251 ( 44%)]  Loss: 3.740 (3.33)  Time: 0.995s, 1028.81/s  (1.006s, 1017.58/s)  LR: 1.935e-04  Data: 0.012 (0.014)
Train: 215 [ 600/1251 ( 48%)]  Loss: 3.600 (3.35)  Time: 0.997s, 1026.57/s  (1.006s, 1018.15/s)  LR: 1.935e-04  Data: 0.011 (0.014)
Train: 215 [ 650/1251 ( 52%)]  Loss: 3.161 (3.34)  Time: 0.994s, 1030.43/s  (1.005s, 1018.63/s)  LR: 1.935e-04  Data: 0.011 (0.014)
Train: 215 [ 700/1251 ( 56%)]  Loss: 3.430 (3.34)  Time: 0.995s, 1028.86/s  (1.005s, 1018.86/s)  LR: 1.935e-04  Data: 0.011 (0.013)
Train: 215 [ 750/1251 ( 60%)]  Loss: 3.461 (3.35)  Time: 1.003s, 1020.47/s  (1.005s, 1019.27/s)  LR: 1.935e-04  Data: 0.012 (0.013)
Train: 215 [ 800/1251 ( 64%)]  Loss: 3.262 (3.34)  Time: 0.997s, 1026.74/s  (1.004s, 1019.61/s)  LR: 1.935e-04  Data: 0.012 (0.013)
Train: 215 [ 850/1251 ( 68%)]  Loss: 3.023 (3.33)  Time: 1.051s,  974.51/s  (1.005s, 1018.49/s)  LR: 1.935e-04  Data: 0.012 (0.013)
Train: 215 [ 900/1251 ( 72%)]  Loss: 3.027 (3.31)  Time: 1.000s, 1024.27/s  (1.006s, 1017.87/s)  LR: 1.935e-04  Data: 0.010 (0.013)
Train: 215 [ 950/1251 ( 76%)]  Loss: 3.459 (3.32)  Time: 0.995s, 1028.67/s  (1.006s, 1018.24/s)  LR: 1.935e-04  Data: 0.010 (0.013)
Train: 215 [1000/1251 ( 80%)]  Loss: 3.432 (3.32)  Time: 1.020s, 1003.56/s  (1.006s, 1018.38/s)  LR: 1.935e-04  Data: 0.012 (0.013)
Train: 215 [1050/1251 ( 84%)]  Loss: 3.215 (3.32)  Time: 0.995s, 1029.60/s  (1.005s, 1018.69/s)  LR: 1.935e-04  Data: 0.012 (0.013)
Train: 215 [1100/1251 ( 88%)]  Loss: 3.541 (3.33)  Time: 0.995s, 1028.98/s  (1.005s, 1019.01/s)  LR: 1.935e-04  Data: 0.011 (0.013)
Train: 215 [1150/1251 ( 92%)]  Loss: 3.166 (3.32)  Time: 0.992s, 1031.96/s  (1.005s, 1019.16/s)  LR: 1.935e-04  Data: 0.011 (0.013)
Train: 215 [1200/1251 ( 96%)]  Loss: 3.176 (3.32)  Time: 1.020s, 1003.97/s  (1.005s, 1019.27/s)  LR: 1.935e-04  Data: 0.011 (0.013)
Train: 215 [1250/1251 (100%)]  Loss: 3.489 (3.32)  Time: 0.983s, 1042.19/s  (1.004s, 1019.52/s)  LR: 1.935e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.668 (1.668)  Loss:  0.6841 (0.6841)  Acc@1: 90.4297 (90.4297)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.245 (0.574)  Loss:  0.7716 (1.1549)  Acc@1: 86.3208 (77.8800)  Acc@5: 97.0519 (94.2120)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-213.pth.tar', 77.90200013183593)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-215.pth.tar', 77.88000000488282)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-214.pth.tar', 77.75600005859376)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-211.pth.tar', 77.61400003173829)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-208.pth.tar', 77.4900000366211)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-212.pth.tar', 77.46599998291016)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-209.pth.tar', 77.42999998779297)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-204.pth.tar', 77.3700001171875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-206.pth.tar', 77.32800000488281)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-205.pth.tar', 77.31800003417969)

Train: 216 [   0/1251 (  0%)]  Loss: 3.540 (3.54)  Time: 2.502s,  409.25/s  (2.502s,  409.25/s)  LR: 1.895e-04  Data: 1.540 (1.540)
Train: 216 [  50/1251 (  4%)]  Loss: 3.496 (3.52)  Time: 0.993s, 1031.64/s  (1.036s,  988.32/s)  LR: 1.895e-04  Data: 0.011 (0.047)
Train: 216 [ 100/1251 (  8%)]  Loss: 3.363 (3.47)  Time: 1.010s, 1013.67/s  (1.021s, 1002.79/s)  LR: 1.895e-04  Data: 0.011 (0.029)
Train: 216 [ 150/1251 ( 12%)]  Loss: 3.334 (3.43)  Time: 0.997s, 1027.32/s  (1.016s, 1007.90/s)  LR: 1.895e-04  Data: 0.011 (0.023)
Train: 216 [ 200/1251 ( 16%)]  Loss: 3.021 (3.35)  Time: 1.002s, 1022.09/s  (1.012s, 1012.23/s)  LR: 1.895e-04  Data: 0.010 (0.020)
Train: 216 [ 250/1251 ( 20%)]  Loss: 3.469 (3.37)  Time: 1.032s,  992.59/s  (1.010s, 1014.03/s)  LR: 1.895e-04  Data: 0.011 (0.018)
Train: 216 [ 300/1251 ( 24%)]  Loss: 3.374 (3.37)  Time: 1.004s, 1020.26/s  (1.014s, 1009.83/s)  LR: 1.895e-04  Data: 0.010 (0.017)
Train: 216 [ 350/1251 ( 28%)]  Loss: 3.229 (3.35)  Time: 0.995s, 1029.49/s  (1.012s, 1011.46/s)  LR: 1.895e-04  Data: 0.011 (0.016)
Train: 216 [ 400/1251 ( 32%)]  Loss: 3.324 (3.35)  Time: 0.997s, 1026.77/s  (1.011s, 1012.91/s)  LR: 1.895e-04  Data: 0.011 (0.016)
Train: 216 [ 450/1251 ( 36%)]  Loss: 3.254 (3.34)  Time: 0.996s, 1028.38/s  (1.010s, 1013.92/s)  LR: 1.895e-04  Data: 0.010 (0.015)
Train: 216 [ 500/1251 ( 40%)]  Loss: 3.316 (3.34)  Time: 0.996s, 1028.19/s  (1.009s, 1014.81/s)  LR: 1.895e-04  Data: 0.011 (0.015)
Train: 216 [ 550/1251 ( 44%)]  Loss: 3.329 (3.34)  Time: 1.004s, 1020.16/s  (1.009s, 1015.17/s)  LR: 1.895e-04  Data: 0.011 (0.014)
Train: 216 [ 600/1251 ( 48%)]  Loss: 3.038 (3.31)  Time: 1.006s, 1017.91/s  (1.008s, 1016.05/s)  LR: 1.895e-04  Data: 0.011 (0.014)
Train: 216 [ 650/1251 ( 52%)]  Loss: 3.124 (3.30)  Time: 0.994s, 1030.30/s  (1.007s, 1016.41/s)  LR: 1.895e-04  Data: 0.011 (0.014)
Train: 216 [ 700/1251 ( 56%)]  Loss: 3.456 (3.31)  Time: 0.994s, 1029.69/s  (1.007s, 1016.49/s)  LR: 1.895e-04  Data: 0.011 (0.014)
Train: 216 [ 750/1251 ( 60%)]  Loss: 3.350 (3.31)  Time: 1.042s,  982.49/s  (1.008s, 1016.05/s)  LR: 1.895e-04  Data: 0.011 (0.014)
Train: 216 [ 800/1251 ( 64%)]  Loss: 3.237 (3.31)  Time: 0.995s, 1029.54/s  (1.007s, 1016.49/s)  LR: 1.895e-04  Data: 0.011 (0.013)
Train: 216 [ 850/1251 ( 68%)]  Loss: 3.509 (3.32)  Time: 1.002s, 1022.11/s  (1.007s, 1016.85/s)  LR: 1.895e-04  Data: 0.011 (0.013)
Train: 216 [ 900/1251 ( 72%)]  Loss: 3.470 (3.33)  Time: 0.995s, 1028.95/s  (1.007s, 1017.20/s)  LR: 1.895e-04  Data: 0.011 (0.013)
Train: 216 [ 950/1251 ( 76%)]  Loss: 3.211 (3.32)  Time: 0.995s, 1029.65/s  (1.006s, 1017.51/s)  LR: 1.895e-04  Data: 0.010 (0.013)
Train: 216 [1000/1251 ( 80%)]  Loss: 3.240 (3.32)  Time: 1.073s,  954.01/s  (1.006s, 1017.61/s)  LR: 1.895e-04  Data: 0.011 (0.013)
Train: 216 [1050/1251 ( 84%)]  Loss: 3.168 (3.31)  Time: 1.030s,  993.75/s  (1.008s, 1015.99/s)  LR: 1.895e-04  Data: 0.011 (0.013)
Train: 216 [1100/1251 ( 88%)]  Loss: 3.250 (3.31)  Time: 0.995s, 1028.93/s  (1.008s, 1015.38/s)  LR: 1.895e-04  Data: 0.010 (0.013)
Train: 216 [1150/1251 ( 92%)]  Loss: 3.305 (3.31)  Time: 1.007s, 1016.59/s  (1.008s, 1015.75/s)  LR: 1.895e-04  Data: 0.014 (0.013)
Train: 216 [1200/1251 ( 96%)]  Loss: 3.044 (3.30)  Time: 0.995s, 1029.17/s  (1.009s, 1015.08/s)  LR: 1.895e-04  Data: 0.012 (0.013)
Train: 216 [1250/1251 (100%)]  Loss: 2.787 (3.28)  Time: 0.986s, 1038.94/s  (1.009s, 1015.32/s)  LR: 1.895e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.671 (1.671)  Loss:  0.7412 (0.7412)  Acc@1: 91.0156 (91.0156)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.245 (0.580)  Loss:  0.8276 (1.1853)  Acc@1: 85.0236 (77.4000)  Acc@5: 96.9340 (93.9540)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-213.pth.tar', 77.90200013183593)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-215.pth.tar', 77.88000000488282)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-214.pth.tar', 77.75600005859376)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-211.pth.tar', 77.61400003173829)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-208.pth.tar', 77.4900000366211)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-212.pth.tar', 77.46599998291016)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-209.pth.tar', 77.42999998779297)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-216.pth.tar', 77.3999999584961)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-204.pth.tar', 77.3700001171875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-206.pth.tar', 77.32800000488281)

Train: 217 [   0/1251 (  0%)]  Loss: 3.423 (3.42)  Time: 2.496s,  410.17/s  (2.496s,  410.17/s)  LR: 1.855e-04  Data: 1.532 (1.532)
Train: 217 [  50/1251 (  4%)]  Loss: 3.463 (3.44)  Time: 1.003s, 1020.92/s  (1.048s,  976.98/s)  LR: 1.855e-04  Data: 0.012 (0.043)
Train: 217 [ 100/1251 (  8%)]  Loss: 3.495 (3.46)  Time: 0.995s, 1029.26/s  (1.031s,  992.87/s)  LR: 1.855e-04  Data: 0.011 (0.027)
Train: 217 [ 150/1251 ( 12%)]  Loss: 3.341 (3.43)  Time: 0.996s, 1028.40/s  (1.021s, 1003.35/s)  LR: 1.855e-04  Data: 0.011 (0.022)
Train: 217 [ 200/1251 ( 16%)]  Loss: 3.618 (3.47)  Time: 1.047s,  978.31/s  (1.016s, 1008.27/s)  LR: 1.855e-04  Data: 0.011 (0.019)
Train: 217 [ 250/1251 ( 20%)]  Loss: 3.268 (3.43)  Time: 0.995s, 1029.28/s  (1.012s, 1011.51/s)  LR: 1.855e-04  Data: 0.011 (0.018)
Train: 217 [ 300/1251 ( 24%)]  Loss: 3.304 (3.42)  Time: 0.998s, 1025.69/s  (1.010s, 1013.78/s)  LR: 1.855e-04  Data: 0.011 (0.017)
Train: 217 [ 350/1251 ( 28%)]  Loss: 3.221 (3.39)  Time: 0.998s, 1026.07/s  (1.009s, 1015.08/s)  LR: 1.855e-04  Data: 0.011 (0.016)
Train: 217 [ 400/1251 ( 32%)]  Loss: 3.061 (3.35)  Time: 1.003s, 1020.87/s  (1.009s, 1015.31/s)  LR: 1.855e-04  Data: 0.011 (0.015)
Train: 217 [ 450/1251 ( 36%)]  Loss: 3.427 (3.36)  Time: 1.002s, 1021.46/s  (1.008s, 1016.07/s)  LR: 1.855e-04  Data: 0.011 (0.015)
Train: 217 [ 500/1251 ( 40%)]  Loss: 3.307 (3.36)  Time: 0.990s, 1034.45/s  (1.008s, 1015.77/s)  LR: 1.855e-04  Data: 0.010 (0.014)
Train: 217 [ 550/1251 ( 44%)]  Loss: 3.274 (3.35)  Time: 0.998s, 1025.75/s  (1.007s, 1016.46/s)  LR: 1.855e-04  Data: 0.011 (0.014)
Train: 217 [ 600/1251 ( 48%)]  Loss: 3.546 (3.37)  Time: 0.996s, 1027.87/s  (1.007s, 1016.86/s)  LR: 1.855e-04  Data: 0.011 (0.014)
Train: 217 [ 650/1251 ( 52%)]  Loss: 3.022 (3.34)  Time: 0.997s, 1027.42/s  (1.006s, 1017.43/s)  LR: 1.855e-04  Data: 0.012 (0.014)
Train: 217 [ 700/1251 ( 56%)]  Loss: 3.278 (3.34)  Time: 0.995s, 1028.91/s  (1.006s, 1017.92/s)  LR: 1.855e-04  Data: 0.011 (0.014)
Train: 217 [ 750/1251 ( 60%)]  Loss: 3.181 (3.33)  Time: 0.995s, 1029.42/s  (1.006s, 1018.32/s)  LR: 1.855e-04  Data: 0.010 (0.013)
Train: 217 [ 800/1251 ( 64%)]  Loss: 3.315 (3.33)  Time: 1.030s,  994.30/s  (1.005s, 1018.47/s)  LR: 1.855e-04  Data: 0.011 (0.013)
Train: 217 [ 850/1251 ( 68%)]  Loss: 3.419 (3.33)  Time: 0.997s, 1026.81/s  (1.005s, 1018.78/s)  LR: 1.855e-04  Data: 0.012 (0.013)
Train: 217 [ 900/1251 ( 72%)]  Loss: 3.157 (3.32)  Time: 0.997s, 1027.24/s  (1.005s, 1019.18/s)  LR: 1.855e-04  Data: 0.011 (0.013)
Train: 217 [ 950/1251 ( 76%)]  Loss: 3.428 (3.33)  Time: 1.011s, 1012.55/s  (1.005s, 1019.29/s)  LR: 1.855e-04  Data: 0.011 (0.013)
Train: 217 [1000/1251 ( 80%)]  Loss: 3.734 (3.35)  Time: 0.997s, 1026.72/s  (1.004s, 1019.57/s)  LR: 1.855e-04  Data: 0.011 (0.013)
Train: 217 [1050/1251 ( 84%)]  Loss: 3.100 (3.34)  Time: 1.063s,  962.94/s  (1.005s, 1018.92/s)  LR: 1.855e-04  Data: 0.012 (0.013)
Train: 217 [1100/1251 ( 88%)]  Loss: 3.268 (3.33)  Time: 0.996s, 1028.56/s  (1.005s, 1018.81/s)  LR: 1.855e-04  Data: 0.012 (0.013)
Train: 217 [1150/1251 ( 92%)]  Loss: 3.126 (3.32)  Time: 0.997s, 1026.93/s  (1.005s, 1019.09/s)  LR: 1.855e-04  Data: 0.012 (0.013)
Train: 217 [1200/1251 ( 96%)]  Loss: 3.211 (3.32)  Time: 0.999s, 1025.48/s  (1.005s, 1019.23/s)  LR: 1.855e-04  Data: 0.012 (0.013)
Train: 217 [1250/1251 (100%)]  Loss: 3.247 (3.32)  Time: 0.983s, 1041.95/s  (1.005s, 1019.26/s)  LR: 1.855e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.661 (1.661)  Loss:  0.6806 (0.6806)  Acc@1: 90.8203 (90.8203)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.245 (0.575)  Loss:  0.7649 (1.1388)  Acc@1: 86.4387 (77.9160)  Acc@5: 97.0519 (94.1780)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-217.pth.tar', 77.91600005615234)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-213.pth.tar', 77.90200013183593)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-215.pth.tar', 77.88000000488282)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-214.pth.tar', 77.75600005859376)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-211.pth.tar', 77.61400003173829)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-208.pth.tar', 77.4900000366211)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-212.pth.tar', 77.46599998291016)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-209.pth.tar', 77.42999998779297)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-216.pth.tar', 77.3999999584961)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-204.pth.tar', 77.3700001171875)

Train: 218 [   0/1251 (  0%)]  Loss: 3.565 (3.57)  Time: 2.429s,  421.60/s  (2.429s,  421.60/s)  LR: 1.816e-04  Data: 1.419 (1.419)
Train: 218 [  50/1251 (  4%)]  Loss: 3.552 (3.56)  Time: 1.047s,  978.00/s  (1.065s,  961.58/s)  LR: 1.816e-04  Data: 0.012 (0.039)
Train: 218 [ 100/1251 (  8%)]  Loss: 3.131 (3.42)  Time: 0.993s, 1031.05/s  (1.032s,  991.85/s)  LR: 1.816e-04  Data: 0.011 (0.025)
Train: 218 [ 150/1251 ( 12%)]  Loss: 3.082 (3.33)  Time: 1.027s,  997.37/s  (1.025s,  998.72/s)  LR: 1.816e-04  Data: 0.010 (0.021)
Train: 218 [ 200/1251 ( 16%)]  Loss: 3.081 (3.28)  Time: 0.996s, 1028.56/s  (1.027s,  997.21/s)  LR: 1.816e-04  Data: 0.012 (0.018)
Train: 218 [ 250/1251 ( 20%)]  Loss: 3.234 (3.27)  Time: 0.995s, 1029.11/s  (1.026s,  997.75/s)  LR: 1.816e-04  Data: 0.012 (0.017)
Train: 218 [ 300/1251 ( 24%)]  Loss: 3.751 (3.34)  Time: 0.996s, 1028.60/s  (1.023s, 1000.72/s)  LR: 1.816e-04  Data: 0.011 (0.016)
Train: 218 [ 350/1251 ( 28%)]  Loss: 3.269 (3.33)  Time: 0.996s, 1027.68/s  (1.020s, 1004.22/s)  LR: 1.816e-04  Data: 0.012 (0.015)
Train: 218 [ 400/1251 ( 32%)]  Loss: 3.293 (3.33)  Time: 0.998s, 1026.19/s  (1.017s, 1006.55/s)  LR: 1.816e-04  Data: 0.010 (0.015)
Train: 218 [ 450/1251 ( 36%)]  Loss: 3.552 (3.35)  Time: 0.995s, 1028.88/s  (1.016s, 1008.22/s)  LR: 1.816e-04  Data: 0.011 (0.014)
Train: 218 [ 500/1251 ( 40%)]  Loss: 3.212 (3.34)  Time: 0.997s, 1027.55/s  (1.014s, 1009.56/s)  LR: 1.816e-04  Data: 0.011 (0.014)
Train: 218 [ 550/1251 ( 44%)]  Loss: 3.588 (3.36)  Time: 0.997s, 1027.27/s  (1.013s, 1010.74/s)  LR: 1.816e-04  Data: 0.012 (0.014)
Train: 218 [ 600/1251 ( 48%)]  Loss: 2.946 (3.33)  Time: 1.008s, 1016.27/s  (1.012s, 1011.41/s)  LR: 1.816e-04  Data: 0.014 (0.014)
Train: 218 [ 650/1251 ( 52%)]  Loss: 3.496 (3.34)  Time: 1.001s, 1023.24/s  (1.012s, 1012.26/s)  LR: 1.816e-04  Data: 0.011 (0.013)
Train: 218 [ 700/1251 ( 56%)]  Loss: 3.347 (3.34)  Time: 1.000s, 1024.49/s  (1.011s, 1013.27/s)  LR: 1.816e-04  Data: 0.011 (0.013)
Train: 218 [ 750/1251 ( 60%)]  Loss: 3.483 (3.35)  Time: 0.994s, 1029.71/s  (1.010s, 1014.07/s)  LR: 1.816e-04  Data: 0.011 (0.013)
Train: 218 [ 800/1251 ( 64%)]  Loss: 3.425 (3.35)  Time: 0.994s, 1029.95/s  (1.009s, 1014.84/s)  LR: 1.816e-04  Data: 0.010 (0.013)
Train: 218 [ 850/1251 ( 68%)]  Loss: 3.553 (3.36)  Time: 1.009s, 1014.71/s  (1.009s, 1015.13/s)  LR: 1.816e-04  Data: 0.010 (0.013)
Train: 218 [ 900/1251 ( 72%)]  Loss: 3.567 (3.38)  Time: 0.997s, 1026.86/s  (1.008s, 1015.51/s)  LR: 1.816e-04  Data: 0.011 (0.013)
Train: 218 [ 950/1251 ( 76%)]  Loss: 3.076 (3.36)  Time: 1.006s, 1017.46/s  (1.008s, 1015.83/s)  LR: 1.816e-04  Data: 0.010 (0.013)
Train: 218 [1000/1251 ( 80%)]  Loss: 3.274 (3.36)  Time: 0.998s, 1026.11/s  (1.008s, 1016.25/s)  LR: 1.816e-04  Data: 0.012 (0.013)
Train: 218 [1050/1251 ( 84%)]  Loss: 3.153 (3.35)  Time: 1.002s, 1021.79/s  (1.008s, 1015.74/s)  LR: 1.816e-04  Data: 0.011 (0.013)
Train: 218 [1100/1251 ( 88%)]  Loss: 3.272 (3.34)  Time: 0.995s, 1028.84/s  (1.008s, 1015.88/s)  LR: 1.816e-04  Data: 0.011 (0.013)
Train: 218 [1150/1251 ( 92%)]  Loss: 3.520 (3.35)  Time: 0.994s, 1030.21/s  (1.008s, 1016.24/s)  LR: 1.816e-04  Data: 0.011 (0.012)
Train: 218 [1200/1251 ( 96%)]  Loss: 3.239 (3.35)  Time: 0.996s, 1028.47/s  (1.007s, 1016.49/s)  LR: 1.816e-04  Data: 0.011 (0.012)
Train: 218 [1250/1251 (100%)]  Loss: 3.123 (3.34)  Time: 0.982s, 1042.29/s  (1.007s, 1016.83/s)  LR: 1.816e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.641 (1.641)  Loss:  0.7603 (0.7603)  Acc@1: 90.7227 (90.7227)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.245 (0.573)  Loss:  0.7975 (1.2066)  Acc@1: 86.0849 (77.9680)  Acc@5: 97.9953 (94.1600)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-218.pth.tar', 77.9680001611328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-217.pth.tar', 77.91600005615234)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-213.pth.tar', 77.90200013183593)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-215.pth.tar', 77.88000000488282)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-214.pth.tar', 77.75600005859376)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-211.pth.tar', 77.61400003173829)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-208.pth.tar', 77.4900000366211)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-212.pth.tar', 77.46599998291016)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-209.pth.tar', 77.42999998779297)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-216.pth.tar', 77.3999999584961)

Train: 219 [   0/1251 (  0%)]  Loss: 3.487 (3.49)  Time: 4.305s,  237.84/s  (4.305s,  237.84/s)  LR: 1.777e-04  Data: 3.053 (3.053)
Train: 219 [  50/1251 (  4%)]  Loss: 2.653 (3.07)  Time: 1.003s, 1020.99/s  (1.070s,  957.35/s)  LR: 1.777e-04  Data: 0.011 (0.071)
Train: 219 [ 100/1251 (  8%)]  Loss: 3.554 (3.23)  Time: 1.007s, 1016.51/s  (1.035s,  989.55/s)  LR: 1.777e-04  Data: 0.012 (0.041)
Train: 219 [ 150/1251 ( 12%)]  Loss: 3.254 (3.24)  Time: 1.011s, 1012.53/s  (1.025s,  999.40/s)  LR: 1.777e-04  Data: 0.011 (0.031)
Train: 219 [ 200/1251 ( 16%)]  Loss: 3.402 (3.27)  Time: 0.999s, 1025.44/s  (1.019s, 1005.33/s)  LR: 1.777e-04  Data: 0.011 (0.026)
Train: 219 [ 250/1251 ( 20%)]  Loss: 3.328 (3.28)  Time: 1.046s,  978.74/s  (1.015s, 1008.80/s)  LR: 1.777e-04  Data: 0.011 (0.023)
Train: 219 [ 300/1251 ( 24%)]  Loss: 3.128 (3.26)  Time: 1.050s,  975.02/s  (1.017s, 1006.48/s)  LR: 1.777e-04  Data: 0.011 (0.021)
Train: 219 [ 350/1251 ( 28%)]  Loss: 3.593 (3.30)  Time: 1.059s,  966.78/s  (1.017s, 1006.46/s)  LR: 1.777e-04  Data: 0.012 (0.020)
Train: 219 [ 400/1251 ( 32%)]  Loss: 3.273 (3.30)  Time: 0.995s, 1029.12/s  (1.016s, 1007.80/s)  LR: 1.777e-04  Data: 0.010 (0.019)
Train: 219 [ 450/1251 ( 36%)]  Loss: 3.123 (3.28)  Time: 0.996s, 1027.84/s  (1.015s, 1008.87/s)  LR: 1.777e-04  Data: 0.010 (0.018)
Train: 219 [ 500/1251 ( 40%)]  Loss: 3.370 (3.29)  Time: 1.000s, 1023.98/s  (1.013s, 1010.55/s)  LR: 1.777e-04  Data: 0.012 (0.017)
Train: 219 [ 550/1251 ( 44%)]  Loss: 3.212 (3.28)  Time: 0.991s, 1033.72/s  (1.012s, 1011.72/s)  LR: 1.777e-04  Data: 0.010 (0.017)
Train: 219 [ 600/1251 ( 48%)]  Loss: 3.213 (3.28)  Time: 1.056s,  969.25/s  (1.012s, 1012.34/s)  LR: 1.777e-04  Data: 0.011 (0.016)
Train: 219 [ 650/1251 ( 52%)]  Loss: 3.013 (3.26)  Time: 0.996s, 1028.52/s  (1.011s, 1012.98/s)  LR: 1.777e-04  Data: 0.011 (0.016)
Train: 219 [ 700/1251 ( 56%)]  Loss: 3.269 (3.26)  Time: 0.997s, 1027.50/s  (1.010s, 1013.84/s)  LR: 1.777e-04  Data: 0.010 (0.016)
Train: 219 [ 750/1251 ( 60%)]  Loss: 3.472 (3.27)  Time: 0.999s, 1025.39/s  (1.010s, 1014.19/s)  LR: 1.777e-04  Data: 0.011 (0.015)
Train: 219 [ 800/1251 ( 64%)]  Loss: 3.298 (3.27)  Time: 0.994s, 1030.41/s  (1.010s, 1014.20/s)  LR: 1.777e-04  Data: 0.010 (0.015)
Train: 219 [ 850/1251 ( 68%)]  Loss: 3.528 (3.29)  Time: 0.995s, 1029.49/s  (1.009s, 1014.83/s)  LR: 1.777e-04  Data: 0.011 (0.015)
Train: 219 [ 900/1251 ( 72%)]  Loss: 3.307 (3.29)  Time: 0.995s, 1029.43/s  (1.008s, 1015.49/s)  LR: 1.777e-04  Data: 0.011 (0.015)
Train: 219 [ 950/1251 ( 76%)]  Loss: 3.387 (3.29)  Time: 0.996s, 1028.14/s  (1.008s, 1016.04/s)  LR: 1.777e-04  Data: 0.011 (0.014)
Train: 219 [1000/1251 ( 80%)]  Loss: 3.237 (3.29)  Time: 0.999s, 1025.51/s  (1.008s, 1016.36/s)  LR: 1.777e-04  Data: 0.010 (0.014)
Train: 219 [1050/1251 ( 84%)]  Loss: 3.176 (3.29)  Time: 0.998s, 1026.49/s  (1.008s, 1015.88/s)  LR: 1.777e-04  Data: 0.011 (0.014)
Train: 219 [1100/1251 ( 88%)]  Loss: 3.284 (3.29)  Time: 0.998s, 1025.64/s  (1.008s, 1016.29/s)  LR: 1.777e-04  Data: 0.011 (0.014)
Train: 219 [1150/1251 ( 92%)]  Loss: 3.424 (3.29)  Time: 0.996s, 1028.36/s  (1.007s, 1016.49/s)  LR: 1.777e-04  Data: 0.012 (0.014)
Train: 219 [1200/1251 ( 96%)]  Loss: 3.528 (3.30)  Time: 0.998s, 1026.50/s  (1.007s, 1016.72/s)  LR: 1.777e-04  Data: 0.011 (0.014)
Train: 219 [1250/1251 (100%)]  Loss: 3.447 (3.31)  Time: 0.994s, 1030.49/s  (1.007s, 1016.62/s)  LR: 1.777e-04  Data: 0.000 (0.014)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.684 (1.684)  Loss:  0.7086 (0.7086)  Acc@1: 90.5273 (90.5273)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.245 (0.568)  Loss:  0.8247 (1.1890)  Acc@1: 85.9670 (78.1360)  Acc@5: 97.2877 (94.3620)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-219.pth.tar', 78.13599998046875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-218.pth.tar', 77.9680001611328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-217.pth.tar', 77.91600005615234)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-213.pth.tar', 77.90200013183593)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-215.pth.tar', 77.88000000488282)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-214.pth.tar', 77.75600005859376)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-211.pth.tar', 77.61400003173829)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-208.pth.tar', 77.4900000366211)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-212.pth.tar', 77.46599998291016)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-209.pth.tar', 77.42999998779297)

Train: 220 [   0/1251 (  0%)]  Loss: 3.156 (3.16)  Time: 2.425s,  422.33/s  (2.425s,  422.33/s)  LR: 1.738e-04  Data: 1.466 (1.466)
Train: 220 [  50/1251 (  4%)]  Loss: 3.503 (3.33)  Time: 0.997s, 1027.56/s  (1.028s,  996.22/s)  LR: 1.738e-04  Data: 0.012 (0.040)
Train: 220 [ 100/1251 (  8%)]  Loss: 3.135 (3.26)  Time: 1.009s, 1015.01/s  (1.016s, 1008.00/s)  LR: 1.738e-04  Data: 0.012 (0.026)
Train: 220 [ 150/1251 ( 12%)]  Loss: 3.017 (3.20)  Time: 0.997s, 1027.54/s  (1.018s, 1006.15/s)  LR: 1.738e-04  Data: 0.012 (0.021)
Train: 220 [ 200/1251 ( 16%)]  Loss: 3.226 (3.21)  Time: 0.995s, 1028.88/s  (1.015s, 1008.70/s)  LR: 1.738e-04  Data: 0.011 (0.018)
Train: 220 [ 250/1251 ( 20%)]  Loss: 3.477 (3.25)  Time: 0.995s, 1029.38/s  (1.014s, 1009.86/s)  LR: 1.738e-04  Data: 0.010 (0.017)
Train: 220 [ 300/1251 ( 24%)]  Loss: 3.434 (3.28)  Time: 0.996s, 1028.50/s  (1.012s, 1011.53/s)  LR: 1.738e-04  Data: 0.011 (0.016)
Train: 220 [ 350/1251 ( 28%)]  Loss: 3.083 (3.25)  Time: 1.006s, 1017.66/s  (1.010s, 1013.44/s)  LR: 1.738e-04  Data: 0.011 (0.015)
Train: 220 [ 400/1251 ( 32%)]  Loss: 3.568 (3.29)  Time: 1.054s,  971.87/s  (1.010s, 1014.20/s)  LR: 1.738e-04  Data: 0.012 (0.015)
Train: 220 [ 450/1251 ( 36%)]  Loss: 2.928 (3.25)  Time: 0.997s, 1027.00/s  (1.011s, 1013.15/s)  LR: 1.738e-04  Data: 0.011 (0.015)
Train: 220 [ 500/1251 ( 40%)]  Loss: 3.416 (3.27)  Time: 0.996s, 1028.61/s  (1.010s, 1014.19/s)  LR: 1.738e-04  Data: 0.012 (0.014)
Train: 220 [ 550/1251 ( 44%)]  Loss: 3.406 (3.28)  Time: 0.994s, 1030.59/s  (1.009s, 1015.17/s)  LR: 1.738e-04  Data: 0.011 (0.014)
Train: 220 [ 600/1251 ( 48%)]  Loss: 3.656 (3.31)  Time: 1.003s, 1021.25/s  (1.008s, 1015.83/s)  LR: 1.738e-04  Data: 0.011 (0.014)
Train: 220 [ 650/1251 ( 52%)]  Loss: 3.337 (3.31)  Time: 1.001s, 1023.05/s  (1.007s, 1016.47/s)  LR: 1.738e-04  Data: 0.014 (0.014)
Train: 220 [ 700/1251 ( 56%)]  Loss: 3.120 (3.30)  Time: 1.013s, 1010.87/s  (1.008s, 1016.10/s)  LR: 1.738e-04  Data: 0.012 (0.013)
Train: 220 [ 750/1251 ( 60%)]  Loss: 3.106 (3.29)  Time: 1.001s, 1023.16/s  (1.007s, 1016.57/s)  LR: 1.738e-04  Data: 0.011 (0.013)
Train: 220 [ 800/1251 ( 64%)]  Loss: 2.918 (3.26)  Time: 0.998s, 1026.24/s  (1.007s, 1016.95/s)  LR: 1.738e-04  Data: 0.012 (0.013)
Train: 220 [ 850/1251 ( 68%)]  Loss: 3.218 (3.26)  Time: 0.995s, 1029.01/s  (1.007s, 1017.35/s)  LR: 1.738e-04  Data: 0.011 (0.013)
Train: 220 [ 900/1251 ( 72%)]  Loss: 3.042 (3.25)  Time: 0.995s, 1029.31/s  (1.006s, 1017.87/s)  LR: 1.738e-04  Data: 0.011 (0.013)
Train: 220 [ 950/1251 ( 76%)]  Loss: 3.540 (3.26)  Time: 0.995s, 1029.37/s  (1.006s, 1018.31/s)  LR: 1.738e-04  Data: 0.011 (0.013)
Train: 220 [1000/1251 ( 80%)]  Loss: 3.436 (3.27)  Time: 1.017s, 1007.27/s  (1.005s, 1018.55/s)  LR: 1.738e-04  Data: 0.010 (0.013)
Train: 220 [1050/1251 ( 84%)]  Loss: 3.058 (3.26)  Time: 1.048s,  976.95/s  (1.005s, 1018.69/s)  LR: 1.738e-04  Data: 0.011 (0.013)
Train: 220 [1100/1251 ( 88%)]  Loss: 3.311 (3.26)  Time: 1.036s,  988.18/s  (1.005s, 1018.43/s)  LR: 1.738e-04  Data: 0.011 (0.013)
Train: 220 [1150/1251 ( 92%)]  Loss: 3.075 (3.26)  Time: 0.993s, 1031.04/s  (1.007s, 1017.36/s)  LR: 1.738e-04  Data: 0.011 (0.013)
Train: 220 [1200/1251 ( 96%)]  Loss: 3.614 (3.27)  Time: 1.003s, 1021.11/s  (1.006s, 1017.54/s)  LR: 1.738e-04  Data: 0.011 (0.012)
Train: 220 [1250/1251 (100%)]  Loss: 3.096 (3.26)  Time: 0.987s, 1037.47/s  (1.007s, 1017.25/s)  LR: 1.738e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.686 (1.686)  Loss:  0.6543 (0.6543)  Acc@1: 90.5273 (90.5273)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.245 (0.570)  Loss:  0.7436 (1.1287)  Acc@1: 85.7311 (78.0260)  Acc@5: 97.5236 (94.1880)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-219.pth.tar', 78.13599998046875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-220.pth.tar', 78.02600000732421)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-218.pth.tar', 77.9680001611328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-217.pth.tar', 77.91600005615234)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-213.pth.tar', 77.90200013183593)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-215.pth.tar', 77.88000000488282)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-214.pth.tar', 77.75600005859376)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-211.pth.tar', 77.61400003173829)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-208.pth.tar', 77.4900000366211)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-212.pth.tar', 77.46599998291016)

Train: 221 [   0/1251 (  0%)]  Loss: 3.320 (3.32)  Time: 2.684s,  381.49/s  (2.684s,  381.49/s)  LR: 1.699e-04  Data: 1.726 (1.726)
Train: 221 [  50/1251 (  4%)]  Loss: 3.224 (3.27)  Time: 1.000s, 1023.94/s  (1.047s,  978.45/s)  LR: 1.699e-04  Data: 0.011 (0.045)
Train: 221 [ 100/1251 (  8%)]  Loss: 3.305 (3.28)  Time: 0.993s, 1030.82/s  (1.026s,  998.26/s)  LR: 1.699e-04  Data: 0.011 (0.028)
Train: 221 [ 150/1251 ( 12%)]  Loss: 3.138 (3.25)  Time: 1.004s, 1019.97/s  (1.019s, 1004.92/s)  LR: 1.699e-04  Data: 0.011 (0.023)
Train: 221 [ 200/1251 ( 16%)]  Loss: 3.574 (3.31)  Time: 1.000s, 1024.10/s  (1.014s, 1009.82/s)  LR: 1.699e-04  Data: 0.012 (0.020)
Train: 221 [ 250/1251 ( 20%)]  Loss: 2.999 (3.26)  Time: 1.034s,  990.00/s  (1.015s, 1008.90/s)  LR: 1.699e-04  Data: 0.011 (0.018)
Train: 221 [ 300/1251 ( 24%)]  Loss: 3.006 (3.22)  Time: 1.009s, 1015.09/s  (1.014s, 1009.97/s)  LR: 1.699e-04  Data: 0.012 (0.017)
Train: 221 [ 350/1251 ( 28%)]  Loss: 3.286 (3.23)  Time: 0.996s, 1028.48/s  (1.012s, 1011.65/s)  LR: 1.699e-04  Data: 0.011 (0.016)
Train: 221 [ 400/1251 ( 32%)]  Loss: 3.121 (3.22)  Time: 0.998s, 1026.53/s  (1.011s, 1013.00/s)  LR: 1.699e-04  Data: 0.011 (0.016)
Train: 221 [ 450/1251 ( 36%)]  Loss: 3.325 (3.23)  Time: 1.045s,  980.15/s  (1.013s, 1010.78/s)  LR: 1.699e-04  Data: 0.010 (0.015)
Train: 221 [ 500/1251 ( 40%)]  Loss: 3.235 (3.23)  Time: 1.030s,  994.24/s  (1.012s, 1012.06/s)  LR: 1.699e-04  Data: 0.010 (0.015)
Train: 221 [ 550/1251 ( 44%)]  Loss: 3.426 (3.25)  Time: 0.997s, 1027.33/s  (1.011s, 1013.17/s)  LR: 1.699e-04  Data: 0.011 (0.014)
Train: 221 [ 600/1251 ( 48%)]  Loss: 3.507 (3.27)  Time: 0.996s, 1028.06/s  (1.010s, 1014.19/s)  LR: 1.699e-04  Data: 0.012 (0.014)
Train: 221 [ 650/1251 ( 52%)]  Loss: 3.007 (3.25)  Time: 0.998s, 1026.19/s  (1.009s, 1015.02/s)  LR: 1.699e-04  Data: 0.012 (0.014)
Train: 221 [ 700/1251 ( 56%)]  Loss: 3.104 (3.24)  Time: 0.995s, 1028.63/s  (1.008s, 1015.58/s)  LR: 1.699e-04  Data: 0.011 (0.014)
Train: 221 [ 750/1251 ( 60%)]  Loss: 3.345 (3.25)  Time: 0.996s, 1028.21/s  (1.008s, 1016.19/s)  LR: 1.699e-04  Data: 0.011 (0.013)
Train: 221 [ 800/1251 ( 64%)]  Loss: 3.662 (3.27)  Time: 1.001s, 1023.08/s  (1.007s, 1016.59/s)  LR: 1.699e-04  Data: 0.012 (0.013)
Train: 221 [ 850/1251 ( 68%)]  Loss: 3.109 (3.26)  Time: 0.995s, 1029.11/s  (1.007s, 1016.97/s)  LR: 1.699e-04  Data: 0.011 (0.013)
Train: 221 [ 900/1251 ( 72%)]  Loss: 3.363 (3.27)  Time: 0.998s, 1026.25/s  (1.007s, 1017.33/s)  LR: 1.699e-04  Data: 0.011 (0.013)
Train: 221 [ 950/1251 ( 76%)]  Loss: 3.417 (3.27)  Time: 1.009s, 1014.78/s  (1.007s, 1017.27/s)  LR: 1.699e-04  Data: 0.012 (0.013)
Train: 221 [1000/1251 ( 80%)]  Loss: 3.299 (3.27)  Time: 0.994s, 1030.13/s  (1.006s, 1017.73/s)  LR: 1.699e-04  Data: 0.010 (0.013)
Train: 221 [1050/1251 ( 84%)]  Loss: 3.201 (3.27)  Time: 0.997s, 1027.15/s  (1.006s, 1018.05/s)  LR: 1.699e-04  Data: 0.011 (0.013)
Train: 221 [1100/1251 ( 88%)]  Loss: 3.473 (3.28)  Time: 1.025s,  999.43/s  (1.006s, 1018.09/s)  LR: 1.699e-04  Data: 0.011 (0.013)
Train: 221 [1150/1251 ( 92%)]  Loss: 3.462 (3.29)  Time: 0.996s, 1028.02/s  (1.007s, 1017.37/s)  LR: 1.699e-04  Data: 0.011 (0.013)
Train: 221 [1200/1251 ( 96%)]  Loss: 3.241 (3.29)  Time: 1.046s,  979.20/s  (1.007s, 1017.08/s)  LR: 1.699e-04  Data: 0.010 (0.013)
Train: 221 [1250/1251 (100%)]  Loss: 3.246 (3.28)  Time: 0.984s, 1040.98/s  (1.008s, 1015.47/s)  LR: 1.699e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.747 (1.747)  Loss:  0.7109 (0.7109)  Acc@1: 90.9180 (90.9180)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.245 (0.567)  Loss:  0.7740 (1.1423)  Acc@1: 85.4953 (78.1680)  Acc@5: 96.9340 (94.2300)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-221.pth.tar', 78.16800003417968)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-219.pth.tar', 78.13599998046875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-220.pth.tar', 78.02600000732421)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-218.pth.tar', 77.9680001611328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-217.pth.tar', 77.91600005615234)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-213.pth.tar', 77.90200013183593)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-215.pth.tar', 77.88000000488282)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-214.pth.tar', 77.75600005859376)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-211.pth.tar', 77.61400003173829)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-208.pth.tar', 77.4900000366211)

Train: 222 [   0/1251 (  0%)]  Loss: 3.236 (3.24)  Time: 2.422s,  422.78/s  (2.422s,  422.78/s)  LR: 1.661e-04  Data: 1.459 (1.459)
Train: 222 [  50/1251 (  4%)]  Loss: 3.325 (3.28)  Time: 1.004s, 1019.55/s  (1.037s,  987.91/s)  LR: 1.661e-04  Data: 0.011 (0.043)
Train: 222 [ 100/1251 (  8%)]  Loss: 3.080 (3.21)  Time: 0.994s, 1030.44/s  (1.019s, 1005.12/s)  LR: 1.661e-04  Data: 0.011 (0.027)
Train: 222 [ 150/1251 ( 12%)]  Loss: 3.399 (3.26)  Time: 0.999s, 1025.33/s  (1.012s, 1011.46/s)  LR: 1.661e-04  Data: 0.011 (0.022)
Train: 222 [ 200/1251 ( 16%)]  Loss: 3.482 (3.30)  Time: 1.002s, 1021.66/s  (1.009s, 1014.85/s)  LR: 1.661e-04  Data: 0.011 (0.019)
Train: 222 [ 250/1251 ( 20%)]  Loss: 3.009 (3.25)  Time: 1.002s, 1021.92/s  (1.007s, 1016.62/s)  LR: 1.661e-04  Data: 0.011 (0.018)
Train: 222 [ 300/1251 ( 24%)]  Loss: 3.443 (3.28)  Time: 0.998s, 1026.31/s  (1.006s, 1017.74/s)  LR: 1.661e-04  Data: 0.012 (0.017)
Train: 222 [ 350/1251 ( 28%)]  Loss: 3.059 (3.25)  Time: 1.002s, 1021.93/s  (1.005s, 1019.00/s)  LR: 1.661e-04  Data: 0.013 (0.016)
Train: 222 [ 400/1251 ( 32%)]  Loss: 3.336 (3.26)  Time: 0.997s, 1026.69/s  (1.006s, 1017.94/s)  LR: 1.661e-04  Data: 0.011 (0.015)
Train: 222 [ 450/1251 ( 36%)]  Loss: 3.108 (3.25)  Time: 0.995s, 1029.51/s  (1.005s, 1018.66/s)  LR: 1.661e-04  Data: 0.010 (0.015)
Train: 222 [ 500/1251 ( 40%)]  Loss: 3.461 (3.27)  Time: 1.003s, 1021.02/s  (1.006s, 1018.36/s)  LR: 1.661e-04  Data: 0.012 (0.015)
Train: 222 [ 550/1251 ( 44%)]  Loss: 3.485 (3.29)  Time: 1.037s,  987.82/s  (1.005s, 1018.94/s)  LR: 1.661e-04  Data: 0.010 (0.014)
Train: 222 [ 600/1251 ( 48%)]  Loss: 3.233 (3.28)  Time: 1.002s, 1021.80/s  (1.005s, 1019.23/s)  LR: 1.661e-04  Data: 0.011 (0.014)
Train: 222 [ 650/1251 ( 52%)]  Loss: 3.640 (3.31)  Time: 1.001s, 1022.50/s  (1.004s, 1019.60/s)  LR: 1.661e-04  Data: 0.011 (0.014)
Train: 222 [ 700/1251 ( 56%)]  Loss: 3.580 (3.32)  Time: 1.006s, 1018.13/s  (1.005s, 1018.97/s)  LR: 1.661e-04  Data: 0.011 (0.014)
Train: 222 [ 750/1251 ( 60%)]  Loss: 3.283 (3.32)  Time: 0.995s, 1029.49/s  (1.004s, 1019.52/s)  LR: 1.661e-04  Data: 0.010 (0.013)
Train: 222 [ 800/1251 ( 64%)]  Loss: 3.362 (3.32)  Time: 1.014s, 1009.37/s  (1.004s, 1019.87/s)  LR: 1.661e-04  Data: 0.012 (0.013)
Train: 222 [ 850/1251 ( 68%)]  Loss: 3.102 (3.31)  Time: 0.995s, 1028.63/s  (1.004s, 1020.12/s)  LR: 1.661e-04  Data: 0.011 (0.013)
Train: 222 [ 900/1251 ( 72%)]  Loss: 3.171 (3.30)  Time: 1.000s, 1024.47/s  (1.004s, 1020.37/s)  LR: 1.661e-04  Data: 0.011 (0.013)
Train: 222 [ 950/1251 ( 76%)]  Loss: 3.154 (3.30)  Time: 1.007s, 1016.77/s  (1.004s, 1020.34/s)  LR: 1.661e-04  Data: 0.012 (0.013)
Train: 222 [1000/1251 ( 80%)]  Loss: 3.357 (3.30)  Time: 0.998s, 1026.40/s  (1.003s, 1020.65/s)  LR: 1.661e-04  Data: 0.011 (0.013)
Train: 222 [1050/1251 ( 84%)]  Loss: 3.227 (3.30)  Time: 0.995s, 1029.10/s  (1.004s, 1019.84/s)  LR: 1.661e-04  Data: 0.010 (0.013)
Train: 222 [1100/1251 ( 88%)]  Loss: 3.596 (3.31)  Time: 0.997s, 1026.94/s  (1.004s, 1019.45/s)  LR: 1.661e-04  Data: 0.011 (0.013)
Train: 222 [1150/1251 ( 92%)]  Loss: 3.379 (3.31)  Time: 1.002s, 1021.79/s  (1.004s, 1019.66/s)  LR: 1.661e-04  Data: 0.011 (0.013)
Train: 222 [1200/1251 ( 96%)]  Loss: 2.976 (3.30)  Time: 1.008s, 1015.41/s  (1.004s, 1019.66/s)  LR: 1.661e-04  Data: 0.012 (0.013)
Train: 222 [1250/1251 (100%)]  Loss: 3.542 (3.31)  Time: 0.981s, 1043.60/s  (1.004s, 1019.75/s)  LR: 1.661e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.611 (1.611)  Loss:  0.6252 (0.6252)  Acc@1: 90.8203 (90.8203)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.245 (0.566)  Loss:  0.7258 (1.0870)  Acc@1: 85.2594 (78.1320)  Acc@5: 97.4057 (94.2840)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-221.pth.tar', 78.16800003417968)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-219.pth.tar', 78.13599998046875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-222.pth.tar', 78.13200006103516)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-220.pth.tar', 78.02600000732421)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-218.pth.tar', 77.9680001611328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-217.pth.tar', 77.91600005615234)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-213.pth.tar', 77.90200013183593)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-215.pth.tar', 77.88000000488282)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-214.pth.tar', 77.75600005859376)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-211.pth.tar', 77.61400003173829)

Train: 223 [   0/1251 (  0%)]  Loss: 3.236 (3.24)  Time: 2.506s,  408.66/s  (2.506s,  408.66/s)  LR: 1.624e-04  Data: 1.538 (1.538)
Train: 223 [  50/1251 (  4%)]  Loss: 3.031 (3.13)  Time: 0.995s, 1028.86/s  (1.029s,  995.30/s)  LR: 1.624e-04  Data: 0.011 (0.041)
Train: 223 [ 100/1251 (  8%)]  Loss: 3.465 (3.24)  Time: 1.006s, 1017.85/s  (1.014s, 1009.37/s)  LR: 1.624e-04  Data: 0.012 (0.026)
Train: 223 [ 150/1251 ( 12%)]  Loss: 3.468 (3.30)  Time: 1.002s, 1021.57/s  (1.011s, 1012.48/s)  LR: 1.624e-04  Data: 0.011 (0.021)
Train: 223 [ 200/1251 ( 16%)]  Loss: 3.378 (3.32)  Time: 0.998s, 1026.40/s  (1.009s, 1014.57/s)  LR: 1.624e-04  Data: 0.011 (0.019)
Train: 223 [ 250/1251 ( 20%)]  Loss: 3.264 (3.31)  Time: 1.063s,  963.52/s  (1.014s, 1010.29/s)  LR: 1.624e-04  Data: 0.011 (0.017)
Train: 223 [ 300/1251 ( 24%)]  Loss: 3.629 (3.35)  Time: 0.998s, 1025.56/s  (1.012s, 1011.94/s)  LR: 1.624e-04  Data: 0.011 (0.016)
Train: 223 [ 350/1251 ( 28%)]  Loss: 3.538 (3.38)  Time: 1.000s, 1024.25/s  (1.011s, 1012.66/s)  LR: 1.624e-04  Data: 0.011 (0.016)
Train: 223 [ 400/1251 ( 32%)]  Loss: 3.288 (3.37)  Time: 0.994s, 1029.86/s  (1.010s, 1013.90/s)  LR: 1.624e-04  Data: 0.011 (0.015)
Train: 223 [ 450/1251 ( 36%)]  Loss: 3.487 (3.38)  Time: 1.002s, 1021.79/s  (1.009s, 1015.08/s)  LR: 1.624e-04  Data: 0.012 (0.015)
Train: 223 [ 500/1251 ( 40%)]  Loss: 3.117 (3.35)  Time: 0.996s, 1028.62/s  (1.009s, 1015.27/s)  LR: 1.624e-04  Data: 0.011 (0.014)
Train: 223 [ 550/1251 ( 44%)]  Loss: 3.328 (3.35)  Time: 1.003s, 1020.46/s  (1.008s, 1015.74/s)  LR: 1.624e-04  Data: 0.011 (0.014)
Train: 223 [ 600/1251 ( 48%)]  Loss: 3.270 (3.35)  Time: 0.996s, 1028.44/s  (1.008s, 1016.36/s)  LR: 1.624e-04  Data: 0.011 (0.014)
Train: 223 [ 650/1251 ( 52%)]  Loss: 3.045 (3.32)  Time: 1.031s,  993.63/s  (1.010s, 1013.88/s)  LR: 1.624e-04  Data: 0.010 (0.014)
Train: 223 [ 700/1251 ( 56%)]  Loss: 3.237 (3.32)  Time: 0.996s, 1028.20/s  (1.009s, 1014.58/s)  LR: 1.624e-04  Data: 0.011 (0.013)
Train: 223 [ 750/1251 ( 60%)]  Loss: 3.491 (3.33)  Time: 0.999s, 1025.27/s  (1.009s, 1015.26/s)  LR: 1.624e-04  Data: 0.012 (0.013)
Train: 223 [ 800/1251 ( 64%)]  Loss: 3.552 (3.34)  Time: 0.997s, 1026.59/s  (1.008s, 1015.61/s)  LR: 1.624e-04  Data: 0.012 (0.013)
Train: 223 [ 850/1251 ( 68%)]  Loss: 3.179 (3.33)  Time: 0.994s, 1030.03/s  (1.008s, 1015.83/s)  LR: 1.624e-04  Data: 0.011 (0.013)
Train: 223 [ 900/1251 ( 72%)]  Loss: 3.411 (3.34)  Time: 0.991s, 1033.74/s  (1.008s, 1016.09/s)  LR: 1.624e-04  Data: 0.011 (0.013)
Train: 223 [ 950/1251 ( 76%)]  Loss: 3.273 (3.33)  Time: 1.008s, 1015.75/s  (1.008s, 1015.74/s)  LR: 1.624e-04  Data: 0.011 (0.013)
Train: 223 [1000/1251 ( 80%)]  Loss: 3.442 (3.34)  Time: 0.993s, 1030.76/s  (1.009s, 1014.93/s)  LR: 1.624e-04  Data: 0.011 (0.013)
Train: 223 [1050/1251 ( 84%)]  Loss: 3.502 (3.35)  Time: 1.005s, 1018.58/s  (1.009s, 1014.99/s)  LR: 1.624e-04  Data: 0.012 (0.013)
Train: 223 [1100/1251 ( 88%)]  Loss: 3.354 (3.35)  Time: 1.000s, 1023.77/s  (1.009s, 1015.09/s)  LR: 1.624e-04  Data: 0.016 (0.013)
Train: 223 [1150/1251 ( 92%)]  Loss: 3.253 (3.34)  Time: 0.997s, 1027.53/s  (1.009s, 1015.22/s)  LR: 1.624e-04  Data: 0.010 (0.013)
Train: 223 [1200/1251 ( 96%)]  Loss: 3.304 (3.34)  Time: 0.996s, 1027.91/s  (1.008s, 1015.40/s)  LR: 1.624e-04  Data: 0.012 (0.013)
Train: 223 [1250/1251 (100%)]  Loss: 3.088 (3.33)  Time: 1.010s, 1013.45/s  (1.008s, 1015.79/s)  LR: 1.624e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.642 (1.642)  Loss:  0.6603 (0.6603)  Acc@1: 91.2109 (91.2109)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.245 (0.571)  Loss:  0.7734 (1.1227)  Acc@1: 86.4387 (78.1020)  Acc@5: 96.8160 (94.2900)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-221.pth.tar', 78.16800003417968)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-219.pth.tar', 78.13599998046875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-222.pth.tar', 78.13200006103516)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-223.pth.tar', 78.10200005615235)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-220.pth.tar', 78.02600000732421)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-218.pth.tar', 77.9680001611328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-217.pth.tar', 77.91600005615234)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-213.pth.tar', 77.90200013183593)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-215.pth.tar', 77.88000000488282)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-214.pth.tar', 77.75600005859376)

Train: 224 [   0/1251 (  0%)]  Loss: 3.031 (3.03)  Time: 2.636s,  388.42/s  (2.636s,  388.42/s)  LR: 1.587e-04  Data: 1.669 (1.669)
Train: 224 [  50/1251 (  4%)]  Loss: 3.033 (3.03)  Time: 0.995s, 1028.99/s  (1.035s,  989.19/s)  LR: 1.587e-04  Data: 0.011 (0.045)
Train: 224 [ 100/1251 (  8%)]  Loss: 3.218 (3.09)  Time: 1.058s,  967.46/s  (1.026s,  998.22/s)  LR: 1.587e-04  Data: 0.011 (0.028)
Train: 224 [ 150/1251 ( 12%)]  Loss: 3.234 (3.13)  Time: 0.993s, 1031.66/s  (1.022s, 1001.89/s)  LR: 1.587e-04  Data: 0.011 (0.023)
Train: 224 [ 200/1251 ( 16%)]  Loss: 3.307 (3.16)  Time: 0.996s, 1028.02/s  (1.017s, 1007.32/s)  LR: 1.587e-04  Data: 0.011 (0.020)
Train: 224 [ 250/1251 ( 20%)]  Loss: 3.350 (3.20)  Time: 0.993s, 1030.91/s  (1.013s, 1010.80/s)  LR: 1.587e-04  Data: 0.011 (0.018)
Train: 224 [ 300/1251 ( 24%)]  Loss: 3.064 (3.18)  Time: 1.008s, 1016.15/s  (1.011s, 1012.85/s)  LR: 1.587e-04  Data: 0.011 (0.017)
Train: 224 [ 350/1251 ( 28%)]  Loss: 3.729 (3.25)  Time: 0.992s, 1032.10/s  (1.013s, 1011.32/s)  LR: 1.587e-04  Data: 0.010 (0.016)
Train: 224 [ 400/1251 ( 32%)]  Loss: 3.451 (3.27)  Time: 0.995s, 1029.19/s  (1.011s, 1012.96/s)  LR: 1.587e-04  Data: 0.011 (0.015)
Train: 224 [ 450/1251 ( 36%)]  Loss: 3.360 (3.28)  Time: 0.999s, 1025.20/s  (1.010s, 1013.99/s)  LR: 1.587e-04  Data: 0.011 (0.015)
Train: 224 [ 500/1251 ( 40%)]  Loss: 3.358 (3.29)  Time: 1.001s, 1023.03/s  (1.009s, 1015.07/s)  LR: 1.587e-04  Data: 0.014 (0.015)
Train: 224 [ 550/1251 ( 44%)]  Loss: 3.001 (3.26)  Time: 0.996s, 1027.94/s  (1.008s, 1015.85/s)  LR: 1.587e-04  Data: 0.012 (0.014)
Train: 224 [ 600/1251 ( 48%)]  Loss: 3.326 (3.27)  Time: 0.994s, 1030.53/s  (1.007s, 1016.53/s)  LR: 1.587e-04  Data: 0.011 (0.014)
Train: 224 [ 650/1251 ( 52%)]  Loss: 3.194 (3.26)  Time: 0.999s, 1025.22/s  (1.007s, 1017.10/s)  LR: 1.587e-04  Data: 0.012 (0.014)
Train: 224 [ 700/1251 ( 56%)]  Loss: 3.322 (3.27)  Time: 1.084s,  945.06/s  (1.008s, 1015.77/s)  LR: 1.587e-04  Data: 0.014 (0.014)
Train: 224 [ 750/1251 ( 60%)]  Loss: 3.121 (3.26)  Time: 1.002s, 1022.06/s  (1.008s, 1015.61/s)  LR: 1.587e-04  Data: 0.011 (0.014)
Train: 224 [ 800/1251 ( 64%)]  Loss: 3.134 (3.25)  Time: 1.002s, 1021.95/s  (1.008s, 1016.12/s)  LR: 1.587e-04  Data: 0.010 (0.013)
Train: 224 [ 850/1251 ( 68%)]  Loss: 3.264 (3.25)  Time: 0.996s, 1028.47/s  (1.007s, 1016.43/s)  LR: 1.587e-04  Data: 0.011 (0.013)
Train: 224 [ 900/1251 ( 72%)]  Loss: 3.284 (3.25)  Time: 1.034s,  990.33/s  (1.007s, 1016.46/s)  LR: 1.587e-04  Data: 0.011 (0.013)
Train: 224 [ 950/1251 ( 76%)]  Loss: 3.200 (3.25)  Time: 1.034s,  990.15/s  (1.008s, 1015.97/s)  LR: 1.587e-04  Data: 0.011 (0.013)
Train: 224 [1000/1251 ( 80%)]  Loss: 3.128 (3.24)  Time: 0.995s, 1028.73/s  (1.009s, 1015.24/s)  LR: 1.587e-04  Data: 0.011 (0.013)
Train: 224 [1050/1251 ( 84%)]  Loss: 3.282 (3.25)  Time: 0.996s, 1028.28/s  (1.008s, 1015.61/s)  LR: 1.587e-04  Data: 0.011 (0.013)
Train: 224 [1100/1251 ( 88%)]  Loss: 3.571 (3.26)  Time: 0.997s, 1026.70/s  (1.009s, 1015.20/s)  LR: 1.587e-04  Data: 0.015 (0.013)
Train: 224 [1150/1251 ( 92%)]  Loss: 3.114 (3.25)  Time: 1.011s, 1012.91/s  (1.008s, 1015.48/s)  LR: 1.587e-04  Data: 0.011 (0.013)
Train: 224 [1200/1251 ( 96%)]  Loss: 3.460 (3.26)  Time: 1.003s, 1021.09/s  (1.008s, 1015.74/s)  LR: 1.587e-04  Data: 0.011 (0.013)
Train: 224 [1250/1251 (100%)]  Loss: 3.282 (3.26)  Time: 0.983s, 1041.51/s  (1.008s, 1016.09/s)  LR: 1.587e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.638 (1.638)  Loss:  0.6344 (0.6344)  Acc@1: 91.1133 (91.1133)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.245 (0.567)  Loss:  0.7240 (1.0957)  Acc@1: 86.4387 (78.4720)  Acc@5: 97.0519 (94.4700)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-224.pth.tar', 78.47199992675782)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-221.pth.tar', 78.16800003417968)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-219.pth.tar', 78.13599998046875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-222.pth.tar', 78.13200006103516)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-223.pth.tar', 78.10200005615235)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-220.pth.tar', 78.02600000732421)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-218.pth.tar', 77.9680001611328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-217.pth.tar', 77.91600005615234)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-213.pth.tar', 77.90200013183593)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-215.pth.tar', 77.88000000488282)

Train: 225 [   0/1251 (  0%)]  Loss: 3.163 (3.16)  Time: 2.723s,  376.05/s  (2.723s,  376.05/s)  LR: 1.550e-04  Data: 1.769 (1.769)
Train: 225 [  50/1251 (  4%)]  Loss: 2.972 (3.07)  Time: 0.993s, 1031.31/s  (1.038s,  986.50/s)  LR: 1.550e-04  Data: 0.011 (0.046)
Train: 225 [ 100/1251 (  8%)]  Loss: 3.147 (3.09)  Time: 1.000s, 1024.23/s  (1.021s, 1003.04/s)  LR: 1.550e-04  Data: 0.012 (0.029)
Train: 225 [ 150/1251 ( 12%)]  Loss: 3.034 (3.08)  Time: 0.999s, 1025.03/s  (1.014s, 1010.01/s)  LR: 1.550e-04  Data: 0.012 (0.023)
Train: 225 [ 200/1251 ( 16%)]  Loss: 2.977 (3.06)  Time: 1.008s, 1015.70/s  (1.011s, 1013.34/s)  LR: 1.550e-04  Data: 0.011 (0.020)
Train: 225 [ 250/1251 ( 20%)]  Loss: 3.494 (3.13)  Time: 1.000s, 1024.32/s  (1.008s, 1015.58/s)  LR: 1.550e-04  Data: 0.012 (0.019)
Train: 225 [ 300/1251 ( 24%)]  Loss: 3.017 (3.11)  Time: 0.997s, 1027.33/s  (1.007s, 1017.28/s)  LR: 1.550e-04  Data: 0.012 (0.017)
Train: 225 [ 350/1251 ( 28%)]  Loss: 3.308 (3.14)  Time: 1.021s, 1002.77/s  (1.006s, 1018.36/s)  LR: 1.550e-04  Data: 0.012 (0.017)
Train: 225 [ 400/1251 ( 32%)]  Loss: 3.283 (3.15)  Time: 1.067s,  959.95/s  (1.007s, 1016.75/s)  LR: 1.550e-04  Data: 0.011 (0.016)
Train: 225 [ 450/1251 ( 36%)]  Loss: 3.121 (3.15)  Time: 1.004s, 1020.33/s  (1.008s, 1016.14/s)  LR: 1.550e-04  Data: 0.011 (0.015)
Train: 225 [ 500/1251 ( 40%)]  Loss: 2.945 (3.13)  Time: 0.996s, 1028.29/s  (1.007s, 1016.81/s)  LR: 1.550e-04  Data: 0.011 (0.015)
Train: 225 [ 550/1251 ( 44%)]  Loss: 3.416 (3.16)  Time: 1.002s, 1021.53/s  (1.006s, 1017.53/s)  LR: 1.550e-04  Data: 0.014 (0.015)
Train: 225 [ 600/1251 ( 48%)]  Loss: 3.126 (3.15)  Time: 1.022s, 1001.73/s  (1.006s, 1017.83/s)  LR: 1.550e-04  Data: 0.011 (0.014)
Train: 225 [ 650/1251 ( 52%)]  Loss: 3.536 (3.18)  Time: 1.003s, 1020.93/s  (1.006s, 1018.15/s)  LR: 1.550e-04  Data: 0.015 (0.014)
Train: 225 [ 700/1251 ( 56%)]  Loss: 3.290 (3.19)  Time: 0.993s, 1030.98/s  (1.006s, 1018.33/s)  LR: 1.550e-04  Data: 0.012 (0.014)
Train: 225 [ 750/1251 ( 60%)]  Loss: 3.052 (3.18)  Time: 1.017s, 1006.73/s  (1.006s, 1017.63/s)  LR: 1.550e-04  Data: 0.011 (0.014)
Train: 225 [ 800/1251 ( 64%)]  Loss: 3.082 (3.17)  Time: 0.996s, 1028.28/s  (1.006s, 1018.12/s)  LR: 1.550e-04  Data: 0.011 (0.014)
Train: 225 [ 850/1251 ( 68%)]  Loss: 3.347 (3.18)  Time: 1.000s, 1024.07/s  (1.005s, 1018.49/s)  LR: 1.550e-04  Data: 0.011 (0.013)
Train: 225 [ 900/1251 ( 72%)]  Loss: 2.668 (3.16)  Time: 0.993s, 1031.68/s  (1.005s, 1018.75/s)  LR: 1.550e-04  Data: 0.011 (0.013)
Train: 225 [ 950/1251 ( 76%)]  Loss: 3.248 (3.16)  Time: 1.002s, 1021.77/s  (1.005s, 1019.03/s)  LR: 1.550e-04  Data: 0.011 (0.013)
Train: 225 [1000/1251 ( 80%)]  Loss: 3.257 (3.17)  Time: 0.993s, 1030.73/s  (1.005s, 1019.15/s)  LR: 1.550e-04  Data: 0.011 (0.013)
Train: 225 [1050/1251 ( 84%)]  Loss: 3.153 (3.17)  Time: 0.995s, 1029.36/s  (1.005s, 1018.73/s)  LR: 1.550e-04  Data: 0.010 (0.013)
Train: 225 [1100/1251 ( 88%)]  Loss: 3.323 (3.17)  Time: 0.999s, 1025.45/s  (1.005s, 1018.85/s)  LR: 1.550e-04  Data: 0.012 (0.013)
Train: 225 [1150/1251 ( 92%)]  Loss: 2.908 (3.16)  Time: 1.013s, 1010.78/s  (1.005s, 1018.92/s)  LR: 1.550e-04  Data: 0.014 (0.013)
Train: 225 [1200/1251 ( 96%)]  Loss: 3.335 (3.17)  Time: 1.004s, 1020.01/s  (1.005s, 1019.00/s)  LR: 1.550e-04  Data: 0.015 (0.013)
Train: 225 [1250/1251 (100%)]  Loss: 3.009 (3.16)  Time: 0.988s, 1036.91/s  (1.005s, 1019.24/s)  LR: 1.550e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.760 (1.760)  Loss:  0.6699 (0.6699)  Acc@1: 91.1133 (91.1133)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.245 (0.572)  Loss:  0.8109 (1.1599)  Acc@1: 85.9670 (78.2440)  Acc@5: 97.4057 (94.2680)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-224.pth.tar', 78.47199992675782)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-225.pth.tar', 78.24399985107422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-221.pth.tar', 78.16800003417968)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-219.pth.tar', 78.13599998046875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-222.pth.tar', 78.13200006103516)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-223.pth.tar', 78.10200005615235)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-220.pth.tar', 78.02600000732421)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-218.pth.tar', 77.9680001611328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-217.pth.tar', 77.91600005615234)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-213.pth.tar', 77.90200013183593)

Train: 226 [   0/1251 (  0%)]  Loss: 3.606 (3.61)  Time: 2.681s,  381.98/s  (2.681s,  381.98/s)  LR: 1.513e-04  Data: 1.716 (1.716)
Train: 226 [  50/1251 (  4%)]  Loss: 3.391 (3.50)  Time: 1.000s, 1024.42/s  (1.038s,  986.95/s)  LR: 1.513e-04  Data: 0.011 (0.045)
Train: 226 [ 100/1251 (  8%)]  Loss: 3.213 (3.40)  Time: 0.996s, 1027.73/s  (1.019s, 1004.69/s)  LR: 1.513e-04  Data: 0.011 (0.028)
Train: 226 [ 150/1251 ( 12%)]  Loss: 3.258 (3.37)  Time: 1.001s, 1023.43/s  (1.016s, 1008.12/s)  LR: 1.513e-04  Data: 0.011 (0.023)
Train: 226 [ 200/1251 ( 16%)]  Loss: 3.079 (3.31)  Time: 0.996s, 1027.81/s  (1.012s, 1012.12/s)  LR: 1.513e-04  Data: 0.012 (0.020)
Train: 226 [ 250/1251 ( 20%)]  Loss: 3.251 (3.30)  Time: 0.997s, 1027.24/s  (1.010s, 1014.18/s)  LR: 1.513e-04  Data: 0.012 (0.018)
Train: 226 [ 300/1251 ( 24%)]  Loss: 3.204 (3.29)  Time: 1.062s,  964.10/s  (1.009s, 1015.14/s)  LR: 1.513e-04  Data: 0.011 (0.017)
Train: 226 [ 350/1251 ( 28%)]  Loss: 3.098 (3.26)  Time: 0.996s, 1028.46/s  (1.010s, 1013.46/s)  LR: 1.513e-04  Data: 0.012 (0.016)
Train: 226 [ 400/1251 ( 32%)]  Loss: 3.289 (3.27)  Time: 1.009s, 1014.66/s  (1.010s, 1014.05/s)  LR: 1.513e-04  Data: 0.011 (0.015)
Train: 226 [ 450/1251 ( 36%)]  Loss: 3.173 (3.26)  Time: 1.003s, 1020.60/s  (1.009s, 1015.33/s)  LR: 1.513e-04  Data: 0.011 (0.015)
Train: 226 [ 500/1251 ( 40%)]  Loss: 3.088 (3.24)  Time: 0.995s, 1029.14/s  (1.008s, 1015.89/s)  LR: 1.513e-04  Data: 0.011 (0.015)
Train: 226 [ 550/1251 ( 44%)]  Loss: 3.461 (3.26)  Time: 0.998s, 1026.09/s  (1.007s, 1016.66/s)  LR: 1.513e-04  Data: 0.011 (0.014)
Train: 226 [ 600/1251 ( 48%)]  Loss: 3.176 (3.25)  Time: 1.001s, 1022.53/s  (1.007s, 1016.91/s)  LR: 1.513e-04  Data: 0.011 (0.014)
Train: 226 [ 650/1251 ( 52%)]  Loss: 3.640 (3.28)  Time: 0.996s, 1027.85/s  (1.006s, 1017.46/s)  LR: 1.513e-04  Data: 0.012 (0.014)
Train: 226 [ 700/1251 ( 56%)]  Loss: 2.807 (3.25)  Time: 1.000s, 1023.95/s  (1.006s, 1017.79/s)  LR: 1.513e-04  Data: 0.010 (0.014)
Train: 226 [ 750/1251 ( 60%)]  Loss: 3.333 (3.25)  Time: 0.999s, 1025.38/s  (1.006s, 1018.31/s)  LR: 1.513e-04  Data: 0.011 (0.013)
Train: 226 [ 800/1251 ( 64%)]  Loss: 3.513 (3.27)  Time: 0.995s, 1028.94/s  (1.005s, 1018.77/s)  LR: 1.513e-04  Data: 0.010 (0.013)
Train: 226 [ 850/1251 ( 68%)]  Loss: 3.035 (3.26)  Time: 1.000s, 1023.68/s  (1.005s, 1018.69/s)  LR: 1.513e-04  Data: 0.011 (0.013)
Train: 226 [ 900/1251 ( 72%)]  Loss: 3.122 (3.25)  Time: 0.997s, 1027.40/s  (1.005s, 1019.14/s)  LR: 1.513e-04  Data: 0.011 (0.013)
Train: 226 [ 950/1251 ( 76%)]  Loss: 3.279 (3.25)  Time: 1.063s,  963.14/s  (1.005s, 1019.15/s)  LR: 1.513e-04  Data: 0.012 (0.013)
Train: 226 [1000/1251 ( 80%)]  Loss: 2.892 (3.23)  Time: 1.020s, 1003.77/s  (1.005s, 1018.93/s)  LR: 1.513e-04  Data: 0.011 (0.013)
Train: 226 [1050/1251 ( 84%)]  Loss: 3.394 (3.24)  Time: 0.996s, 1027.73/s  (1.005s, 1018.72/s)  LR: 1.513e-04  Data: 0.011 (0.013)
Train: 226 [1100/1251 ( 88%)]  Loss: 3.441 (3.25)  Time: 0.996s, 1028.41/s  (1.005s, 1018.84/s)  LR: 1.513e-04  Data: 0.013 (0.013)
Train: 226 [1150/1251 ( 92%)]  Loss: 2.949 (3.24)  Time: 0.995s, 1029.22/s  (1.005s, 1019.03/s)  LR: 1.513e-04  Data: 0.011 (0.013)
Train: 226 [1200/1251 ( 96%)]  Loss: 3.194 (3.24)  Time: 0.995s, 1029.28/s  (1.005s, 1019.21/s)  LR: 1.513e-04  Data: 0.012 (0.013)
Train: 226 [1250/1251 (100%)]  Loss: 3.450 (3.24)  Time: 0.983s, 1041.96/s  (1.005s, 1019.36/s)  LR: 1.513e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.656 (1.656)  Loss:  0.7376 (0.7376)  Acc@1: 91.8945 (91.8945)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.245 (0.570)  Loss:  0.8213 (1.2255)  Acc@1: 86.5566 (78.3400)  Acc@5: 97.7594 (94.2260)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-224.pth.tar', 78.47199992675782)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-226.pth.tar', 78.33999997802735)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-225.pth.tar', 78.24399985107422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-221.pth.tar', 78.16800003417968)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-219.pth.tar', 78.13599998046875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-222.pth.tar', 78.13200006103516)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-223.pth.tar', 78.10200005615235)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-220.pth.tar', 78.02600000732421)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-218.pth.tar', 77.9680001611328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-217.pth.tar', 77.91600005615234)

Train: 227 [   0/1251 (  0%)]  Loss: 2.803 (2.80)  Time: 2.536s,  403.84/s  (2.536s,  403.84/s)  LR: 1.477e-04  Data: 1.575 (1.575)
Train: 227 [  50/1251 (  4%)]  Loss: 3.648 (3.23)  Time: 1.002s, 1021.65/s  (1.033s,  991.29/s)  LR: 1.477e-04  Data: 0.012 (0.042)
Train: 227 [ 100/1251 (  8%)]  Loss: 3.404 (3.29)  Time: 0.995s, 1029.36/s  (1.023s, 1001.28/s)  LR: 1.477e-04  Data: 0.012 (0.027)
Train: 227 [ 150/1251 ( 12%)]  Loss: 3.245 (3.28)  Time: 0.998s, 1026.14/s  (1.016s, 1007.59/s)  LR: 1.477e-04  Data: 0.011 (0.022)
Train: 227 [ 200/1251 ( 16%)]  Loss: 3.427 (3.31)  Time: 0.998s, 1025.73/s  (1.013s, 1011.23/s)  LR: 1.477e-04  Data: 0.010 (0.019)
Train: 227 [ 250/1251 ( 20%)]  Loss: 3.083 (3.27)  Time: 0.999s, 1025.48/s  (1.010s, 1013.84/s)  LR: 1.477e-04  Data: 0.011 (0.017)
Train: 227 [ 300/1251 ( 24%)]  Loss: 3.303 (3.27)  Time: 1.001s, 1022.58/s  (1.009s, 1015.24/s)  LR: 1.477e-04  Data: 0.011 (0.016)
Train: 227 [ 350/1251 ( 28%)]  Loss: 3.226 (3.27)  Time: 0.999s, 1025.42/s  (1.007s, 1016.78/s)  LR: 1.477e-04  Data: 0.012 (0.016)
Train: 227 [ 400/1251 ( 32%)]  Loss: 3.496 (3.29)  Time: 0.998s, 1026.55/s  (1.006s, 1017.46/s)  LR: 1.477e-04  Data: 0.010 (0.015)
Train: 227 [ 450/1251 ( 36%)]  Loss: 3.233 (3.29)  Time: 1.002s, 1021.66/s  (1.006s, 1018.08/s)  LR: 1.477e-04  Data: 0.010 (0.015)
Train: 227 [ 500/1251 ( 40%)]  Loss: 3.511 (3.31)  Time: 1.006s, 1017.64/s  (1.006s, 1018.40/s)  LR: 1.477e-04  Data: 0.012 (0.014)
Train: 227 [ 550/1251 ( 44%)]  Loss: 3.438 (3.32)  Time: 1.000s, 1023.50/s  (1.005s, 1018.67/s)  LR: 1.477e-04  Data: 0.014 (0.014)
Train: 227 [ 600/1251 ( 48%)]  Loss: 3.263 (3.31)  Time: 0.994s, 1030.13/s  (1.005s, 1019.00/s)  LR: 1.477e-04  Data: 0.011 (0.014)
Train: 227 [ 650/1251 ( 52%)]  Loss: 3.409 (3.32)  Time: 1.000s, 1024.23/s  (1.004s, 1019.48/s)  LR: 1.477e-04  Data: 0.014 (0.014)
Train: 227 [ 700/1251 ( 56%)]  Loss: 3.292 (3.32)  Time: 0.994s, 1030.08/s  (1.004s, 1019.71/s)  LR: 1.477e-04  Data: 0.010 (0.013)
Train: 227 [ 750/1251 ( 60%)]  Loss: 3.334 (3.32)  Time: 0.996s, 1028.38/s  (1.004s, 1019.80/s)  LR: 1.477e-04  Data: 0.011 (0.013)
Train: 227 [ 800/1251 ( 64%)]  Loss: 3.257 (3.32)  Time: 0.993s, 1030.71/s  (1.004s, 1020.03/s)  LR: 1.477e-04  Data: 0.011 (0.013)
Train: 227 [ 850/1251 ( 68%)]  Loss: 3.332 (3.32)  Time: 0.996s, 1027.89/s  (1.004s, 1020.38/s)  LR: 1.477e-04  Data: 0.011 (0.013)
Train: 227 [ 900/1251 ( 72%)]  Loss: 2.894 (3.29)  Time: 0.997s, 1027.47/s  (1.004s, 1020.16/s)  LR: 1.477e-04  Data: 0.011 (0.013)
Train: 227 [ 950/1251 ( 76%)]  Loss: 3.353 (3.30)  Time: 0.997s, 1027.32/s  (1.003s, 1020.43/s)  LR: 1.477e-04  Data: 0.011 (0.013)
Train: 227 [1000/1251 ( 80%)]  Loss: 3.310 (3.30)  Time: 1.000s, 1024.29/s  (1.005s, 1019.18/s)  LR: 1.477e-04  Data: 0.011 (0.013)
Train: 227 [1050/1251 ( 84%)]  Loss: 3.179 (3.29)  Time: 1.001s, 1022.79/s  (1.005s, 1019.39/s)  LR: 1.477e-04  Data: 0.012 (0.013)
Train: 227 [1100/1251 ( 88%)]  Loss: 3.339 (3.29)  Time: 0.995s, 1029.30/s  (1.004s, 1019.69/s)  LR: 1.477e-04  Data: 0.012 (0.012)
Train: 227 [1150/1251 ( 92%)]  Loss: 3.082 (3.29)  Time: 0.996s, 1027.86/s  (1.004s, 1019.89/s)  LR: 1.477e-04  Data: 0.011 (0.012)
Train: 227 [1200/1251 ( 96%)]  Loss: 2.989 (3.27)  Time: 1.005s, 1018.73/s  (1.004s, 1020.13/s)  LR: 1.477e-04  Data: 0.012 (0.012)
Train: 227 [1250/1251 (100%)]  Loss: 3.294 (3.27)  Time: 0.985s, 1039.73/s  (1.004s, 1020.30/s)  LR: 1.477e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.625 (1.625)  Loss:  0.7211 (0.7211)  Acc@1: 90.9180 (90.9180)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.245 (0.573)  Loss:  0.9061 (1.2033)  Acc@1: 86.6745 (78.3140)  Acc@5: 97.2877 (94.3040)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-224.pth.tar', 78.47199992675782)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-226.pth.tar', 78.33999997802735)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-227.pth.tar', 78.31400002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-225.pth.tar', 78.24399985107422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-221.pth.tar', 78.16800003417968)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-219.pth.tar', 78.13599998046875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-222.pth.tar', 78.13200006103516)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-223.pth.tar', 78.10200005615235)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-220.pth.tar', 78.02600000732421)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-218.pth.tar', 77.9680001611328)

Train: 228 [   0/1251 (  0%)]  Loss: 3.166 (3.17)  Time: 4.401s,  232.67/s  (4.401s,  232.67/s)  LR: 1.442e-04  Data: 3.070 (3.070)
Train: 228 [  50/1251 (  4%)]  Loss: 3.515 (3.34)  Time: 1.005s, 1018.86/s  (1.065s,  961.13/s)  LR: 1.442e-04  Data: 0.012 (0.071)
Train: 228 [ 100/1251 (  8%)]  Loss: 3.214 (3.30)  Time: 0.995s, 1029.56/s  (1.032s,  992.25/s)  LR: 1.442e-04  Data: 0.011 (0.041)
Train: 228 [ 150/1251 ( 12%)]  Loss: 3.380 (3.32)  Time: 1.047s,  977.99/s  (1.022s, 1001.95/s)  LR: 1.442e-04  Data: 0.011 (0.031)
Train: 228 [ 200/1251 ( 16%)]  Loss: 3.628 (3.38)  Time: 0.998s, 1026.42/s  (1.017s, 1007.02/s)  LR: 1.442e-04  Data: 0.012 (0.026)
Train: 228 [ 250/1251 ( 20%)]  Loss: 3.303 (3.37)  Time: 1.014s, 1009.58/s  (1.017s, 1006.87/s)  LR: 1.442e-04  Data: 0.011 (0.023)
Train: 228 [ 300/1251 ( 24%)]  Loss: 2.979 (3.31)  Time: 0.993s, 1031.46/s  (1.014s, 1010.01/s)  LR: 1.442e-04  Data: 0.010 (0.021)
Train: 228 [ 350/1251 ( 28%)]  Loss: 3.017 (3.28)  Time: 0.997s, 1027.22/s  (1.012s, 1011.79/s)  LR: 1.442e-04  Data: 0.011 (0.020)
Train: 228 [ 400/1251 ( 32%)]  Loss: 3.169 (3.26)  Time: 0.996s, 1028.29/s  (1.011s, 1013.28/s)  LR: 1.442e-04  Data: 0.011 (0.019)
Train: 228 [ 450/1251 ( 36%)]  Loss: 3.293 (3.27)  Time: 0.995s, 1028.77/s  (1.009s, 1014.63/s)  LR: 1.442e-04  Data: 0.011 (0.018)
Train: 228 [ 500/1251 ( 40%)]  Loss: 3.018 (3.24)  Time: 0.996s, 1028.21/s  (1.009s, 1015.32/s)  LR: 1.442e-04  Data: 0.012 (0.017)
Train: 228 [ 550/1251 ( 44%)]  Loss: 3.533 (3.27)  Time: 0.994s, 1030.01/s  (1.008s, 1015.92/s)  LR: 1.442e-04  Data: 0.011 (0.017)
Train: 228 [ 600/1251 ( 48%)]  Loss: 3.036 (3.25)  Time: 0.994s, 1029.77/s  (1.008s, 1016.10/s)  LR: 1.442e-04  Data: 0.011 (0.016)
Train: 228 [ 650/1251 ( 52%)]  Loss: 3.409 (3.26)  Time: 0.995s, 1029.27/s  (1.008s, 1016.32/s)  LR: 1.442e-04  Data: 0.011 (0.016)
Train: 228 [ 700/1251 ( 56%)]  Loss: 2.967 (3.24)  Time: 1.004s, 1019.67/s  (1.008s, 1015.77/s)  LR: 1.442e-04  Data: 0.011 (0.016)
Train: 228 [ 750/1251 ( 60%)]  Loss: 2.925 (3.22)  Time: 1.001s, 1022.73/s  (1.008s, 1016.18/s)  LR: 1.442e-04  Data: 0.012 (0.015)
Train: 228 [ 800/1251 ( 64%)]  Loss: 3.404 (3.23)  Time: 0.995s, 1028.90/s  (1.007s, 1016.41/s)  LR: 1.442e-04  Data: 0.011 (0.015)
Train: 228 [ 850/1251 ( 68%)]  Loss: 3.310 (3.24)  Time: 0.997s, 1027.47/s  (1.007s, 1016.60/s)  LR: 1.442e-04  Data: 0.011 (0.015)
Train: 228 [ 900/1251 ( 72%)]  Loss: 3.429 (3.25)  Time: 0.997s, 1027.14/s  (1.007s, 1016.91/s)  LR: 1.442e-04  Data: 0.014 (0.015)
Train: 228 [ 950/1251 ( 76%)]  Loss: 3.037 (3.24)  Time: 0.996s, 1028.36/s  (1.007s, 1017.21/s)  LR: 1.442e-04  Data: 0.012 (0.014)
Train: 228 [1000/1251 ( 80%)]  Loss: 3.019 (3.23)  Time: 1.002s, 1021.96/s  (1.006s, 1017.50/s)  LR: 1.442e-04  Data: 0.011 (0.014)
Train: 228 [1050/1251 ( 84%)]  Loss: 3.127 (3.22)  Time: 0.997s, 1027.15/s  (1.006s, 1017.73/s)  LR: 1.442e-04  Data: 0.011 (0.014)
Train: 228 [1100/1251 ( 88%)]  Loss: 3.311 (3.23)  Time: 0.992s, 1032.20/s  (1.006s, 1017.94/s)  LR: 1.442e-04  Data: 0.011 (0.014)
Train: 228 [1150/1251 ( 92%)]  Loss: 3.236 (3.23)  Time: 0.995s, 1028.76/s  (1.006s, 1018.20/s)  LR: 1.442e-04  Data: 0.011 (0.014)
Train: 228 [1200/1251 ( 96%)]  Loss: 3.070 (3.22)  Time: 0.996s, 1027.79/s  (1.006s, 1018.36/s)  LR: 1.442e-04  Data: 0.011 (0.014)
Train: 228 [1250/1251 (100%)]  Loss: 3.077 (3.21)  Time: 1.029s,  995.43/s  (1.005s, 1018.58/s)  LR: 1.442e-04  Data: 0.000 (0.014)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.744 (1.744)  Loss:  0.7077 (0.7077)  Acc@1: 91.7969 (91.7969)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.246 (0.582)  Loss:  0.7249 (1.1277)  Acc@1: 87.1462 (78.7060)  Acc@5: 97.7594 (94.3720)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-228.pth.tar', 78.70600010498048)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-224.pth.tar', 78.47199992675782)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-226.pth.tar', 78.33999997802735)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-227.pth.tar', 78.31400002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-225.pth.tar', 78.24399985107422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-221.pth.tar', 78.16800003417968)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-219.pth.tar', 78.13599998046875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-222.pth.tar', 78.13200006103516)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-223.pth.tar', 78.10200005615235)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-220.pth.tar', 78.02600000732421)

Train: 229 [   0/1251 (  0%)]  Loss: 3.507 (3.51)  Time: 2.420s,  423.15/s  (2.420s,  423.15/s)  LR: 1.406e-04  Data: 1.462 (1.462)
Train: 229 [  50/1251 (  4%)]  Loss: 3.559 (3.53)  Time: 0.997s, 1026.67/s  (1.031s,  993.34/s)  LR: 1.406e-04  Data: 0.012 (0.040)
Train: 229 [ 100/1251 (  8%)]  Loss: 3.202 (3.42)  Time: 1.003s, 1021.41/s  (1.025s,  998.76/s)  LR: 1.406e-04  Data: 0.012 (0.026)
Train: 229 [ 150/1251 ( 12%)]  Loss: 3.338 (3.40)  Time: 1.006s, 1017.62/s  (1.018s, 1006.24/s)  LR: 1.406e-04  Data: 0.011 (0.021)
Train: 229 [ 200/1251 ( 16%)]  Loss: 3.546 (3.43)  Time: 1.016s, 1007.40/s  (1.014s, 1009.83/s)  LR: 1.406e-04  Data: 0.011 (0.018)
Train: 229 [ 250/1251 ( 20%)]  Loss: 3.125 (3.38)  Time: 0.995s, 1028.84/s  (1.012s, 1012.17/s)  LR: 1.406e-04  Data: 0.011 (0.017)
Train: 229 [ 300/1251 ( 24%)]  Loss: 3.029 (3.33)  Time: 1.022s, 1002.37/s  (1.010s, 1013.49/s)  LR: 1.406e-04  Data: 0.011 (0.016)
Train: 229 [ 350/1251 ( 28%)]  Loss: 3.387 (3.34)  Time: 0.995s, 1029.19/s  (1.009s, 1014.56/s)  LR: 1.406e-04  Data: 0.010 (0.015)
Train: 229 [ 400/1251 ( 32%)]  Loss: 3.224 (3.32)  Time: 0.996s, 1028.06/s  (1.010s, 1014.29/s)  LR: 1.406e-04  Data: 0.011 (0.015)
Train: 229 [ 450/1251 ( 36%)]  Loss: 3.613 (3.35)  Time: 0.996s, 1027.69/s  (1.009s, 1015.09/s)  LR: 1.406e-04  Data: 0.011 (0.014)
Train: 229 [ 500/1251 ( 40%)]  Loss: 3.143 (3.33)  Time: 0.999s, 1024.71/s  (1.008s, 1015.72/s)  LR: 1.406e-04  Data: 0.011 (0.014)
Train: 229 [ 550/1251 ( 44%)]  Loss: 3.224 (3.32)  Time: 0.999s, 1025.17/s  (1.008s, 1016.36/s)  LR: 1.406e-04  Data: 0.011 (0.014)
Train: 229 [ 600/1251 ( 48%)]  Loss: 3.251 (3.32)  Time: 0.992s, 1032.37/s  (1.007s, 1016.95/s)  LR: 1.406e-04  Data: 0.011 (0.013)
Train: 229 [ 650/1251 ( 52%)]  Loss: 3.439 (3.33)  Time: 0.996s, 1028.06/s  (1.006s, 1017.40/s)  LR: 1.406e-04  Data: 0.011 (0.013)
Train: 229 [ 700/1251 ( 56%)]  Loss: 3.452 (3.34)  Time: 0.994s, 1030.45/s  (1.006s, 1017.77/s)  LR: 1.406e-04  Data: 0.012 (0.013)
Train: 229 [ 750/1251 ( 60%)]  Loss: 3.280 (3.33)  Time: 1.005s, 1018.69/s  (1.006s, 1018.16/s)  LR: 1.406e-04  Data: 0.011 (0.013)
Train: 229 [ 800/1251 ( 64%)]  Loss: 3.186 (3.32)  Time: 1.034s,  990.38/s  (1.006s, 1017.78/s)  LR: 1.406e-04  Data: 0.011 (0.013)
Train: 229 [ 850/1251 ( 68%)]  Loss: 3.180 (3.32)  Time: 0.995s, 1029.22/s  (1.007s, 1016.41/s)  LR: 1.406e-04  Data: 0.011 (0.013)
Train: 229 [ 900/1251 ( 72%)]  Loss: 3.380 (3.32)  Time: 0.995s, 1029.64/s  (1.007s, 1016.81/s)  LR: 1.406e-04  Data: 0.010 (0.013)
Train: 229 [ 950/1251 ( 76%)]  Loss: 2.944 (3.30)  Time: 0.994s, 1029.90/s  (1.007s, 1017.12/s)  LR: 1.406e-04  Data: 0.012 (0.013)
Train: 229 [1000/1251 ( 80%)]  Loss: 3.414 (3.31)  Time: 0.995s, 1029.37/s  (1.006s, 1017.54/s)  LR: 1.406e-04  Data: 0.012 (0.013)
Train: 229 [1050/1251 ( 84%)]  Loss: 3.184 (3.30)  Time: 1.061s,  965.41/s  (1.008s, 1015.53/s)  LR: 1.406e-04  Data: 0.012 (0.012)
Train: 229 [1100/1251 ( 88%)]  Loss: 3.361 (3.30)  Time: 0.997s, 1026.91/s  (1.009s, 1015.33/s)  LR: 1.406e-04  Data: 0.012 (0.012)
Train: 229 [1150/1251 ( 92%)]  Loss: 3.451 (3.31)  Time: 0.999s, 1025.07/s  (1.008s, 1015.71/s)  LR: 1.406e-04  Data: 0.013 (0.012)
Train: 229 [1200/1251 ( 96%)]  Loss: 3.041 (3.30)  Time: 0.996s, 1028.50/s  (1.008s, 1016.09/s)  LR: 1.406e-04  Data: 0.012 (0.012)
Train: 229 [1250/1251 (100%)]  Loss: 3.246 (3.30)  Time: 0.983s, 1041.77/s  (1.007s, 1016.50/s)  LR: 1.406e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.583 (1.583)  Loss:  0.7479 (0.7479)  Acc@1: 91.4062 (91.4062)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.245 (0.573)  Loss:  0.7628 (1.1726)  Acc@1: 87.3821 (78.5160)  Acc@5: 97.4057 (94.4060)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-228.pth.tar', 78.70600010498048)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-229.pth.tar', 78.51599994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-224.pth.tar', 78.47199992675782)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-226.pth.tar', 78.33999997802735)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-227.pth.tar', 78.31400002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-225.pth.tar', 78.24399985107422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-221.pth.tar', 78.16800003417968)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-219.pth.tar', 78.13599998046875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-222.pth.tar', 78.13200006103516)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-223.pth.tar', 78.10200005615235)

Train: 230 [   0/1251 (  0%)]  Loss: 3.321 (3.32)  Time: 2.459s,  416.46/s  (2.459s,  416.46/s)  LR: 1.371e-04  Data: 1.492 (1.492)
Train: 230 [  50/1251 (  4%)]  Loss: 3.170 (3.25)  Time: 1.000s, 1024.07/s  (1.032s,  991.97/s)  LR: 1.371e-04  Data: 0.011 (0.041)
Train: 230 [ 100/1251 (  8%)]  Loss: 3.284 (3.26)  Time: 1.008s, 1015.40/s  (1.019s, 1005.37/s)  LR: 1.371e-04  Data: 0.011 (0.026)
Train: 230 [ 150/1251 ( 12%)]  Loss: 3.342 (3.28)  Time: 1.034s,  989.94/s  (1.012s, 1011.42/s)  LR: 1.371e-04  Data: 0.011 (0.021)
Train: 230 [ 200/1251 ( 16%)]  Loss: 3.229 (3.27)  Time: 1.016s, 1007.38/s  (1.011s, 1012.47/s)  LR: 1.371e-04  Data: 0.011 (0.019)
Train: 230 [ 250/1251 ( 20%)]  Loss: 3.132 (3.25)  Time: 1.027s,  997.30/s  (1.011s, 1013.07/s)  LR: 1.371e-04  Data: 0.011 (0.017)
Train: 230 [ 300/1251 ( 24%)]  Loss: 3.415 (3.27)  Time: 1.005s, 1018.88/s  (1.010s, 1014.34/s)  LR: 1.371e-04  Data: 0.011 (0.016)
Train: 230 [ 350/1251 ( 28%)]  Loss: 3.377 (3.28)  Time: 0.993s, 1031.70/s  (1.009s, 1015.29/s)  LR: 1.371e-04  Data: 0.011 (0.016)
Train: 230 [ 400/1251 ( 32%)]  Loss: 3.191 (3.27)  Time: 0.995s, 1028.88/s  (1.008s, 1016.17/s)  LR: 1.371e-04  Data: 0.012 (0.015)
Train: 230 [ 450/1251 ( 36%)]  Loss: 3.394 (3.29)  Time: 0.997s, 1027.18/s  (1.007s, 1016.90/s)  LR: 1.371e-04  Data: 0.011 (0.015)
Train: 230 [ 500/1251 ( 40%)]  Loss: 3.108 (3.27)  Time: 1.032s,  992.72/s  (1.007s, 1017.38/s)  LR: 1.371e-04  Data: 0.011 (0.014)
Train: 230 [ 550/1251 ( 44%)]  Loss: 3.174 (3.26)  Time: 1.024s,  999.73/s  (1.006s, 1017.79/s)  LR: 1.371e-04  Data: 0.011 (0.014)
Train: 230 [ 600/1251 ( 48%)]  Loss: 3.119 (3.25)  Time: 1.004s, 1019.73/s  (1.006s, 1018.37/s)  LR: 1.371e-04  Data: 0.011 (0.014)
Train: 230 [ 650/1251 ( 52%)]  Loss: 3.059 (3.24)  Time: 1.003s, 1020.97/s  (1.005s, 1018.52/s)  LR: 1.371e-04  Data: 0.011 (0.014)
Train: 230 [ 700/1251 ( 56%)]  Loss: 3.502 (3.25)  Time: 1.000s, 1024.34/s  (1.005s, 1018.55/s)  LR: 1.371e-04  Data: 0.012 (0.013)
Train: 230 [ 750/1251 ( 60%)]  Loss: 3.413 (3.26)  Time: 1.007s, 1016.61/s  (1.005s, 1018.61/s)  LR: 1.371e-04  Data: 0.011 (0.013)
Train: 230 [ 800/1251 ( 64%)]  Loss: 3.455 (3.28)  Time: 0.997s, 1027.47/s  (1.005s, 1018.84/s)  LR: 1.371e-04  Data: 0.012 (0.013)
Train: 230 [ 850/1251 ( 68%)]  Loss: 3.232 (3.27)  Time: 0.997s, 1027.22/s  (1.005s, 1018.57/s)  LR: 1.371e-04  Data: 0.012 (0.013)
Train: 230 [ 900/1251 ( 72%)]  Loss: 3.179 (3.27)  Time: 0.996s, 1028.41/s  (1.005s, 1018.87/s)  LR: 1.371e-04  Data: 0.012 (0.013)
Train: 230 [ 950/1251 ( 76%)]  Loss: 3.036 (3.26)  Time: 0.996s, 1028.19/s  (1.005s, 1019.25/s)  LR: 1.371e-04  Data: 0.011 (0.013)
Train: 230 [1000/1251 ( 80%)]  Loss: 3.397 (3.26)  Time: 0.999s, 1025.08/s  (1.004s, 1019.52/s)  LR: 1.371e-04  Data: 0.011 (0.013)
Train: 230 [1050/1251 ( 84%)]  Loss: 3.126 (3.26)  Time: 1.000s, 1024.40/s  (1.004s, 1019.49/s)  LR: 1.371e-04  Data: 0.011 (0.013)
Train: 230 [1100/1251 ( 88%)]  Loss: 3.282 (3.26)  Time: 1.000s, 1023.69/s  (1.006s, 1017.96/s)  LR: 1.371e-04  Data: 0.011 (0.013)
Train: 230 [1150/1251 ( 92%)]  Loss: 3.303 (3.26)  Time: 0.994s, 1030.43/s  (1.006s, 1017.71/s)  LR: 1.371e-04  Data: 0.012 (0.013)
Train: 230 [1200/1251 ( 96%)]  Loss: 3.294 (3.26)  Time: 0.998s, 1025.74/s  (1.006s, 1017.88/s)  LR: 1.371e-04  Data: 0.011 (0.012)
Train: 230 [1250/1251 (100%)]  Loss: 3.229 (3.26)  Time: 0.981s, 1043.74/s  (1.006s, 1017.98/s)  LR: 1.371e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.691 (1.691)  Loss:  0.6412 (0.6412)  Acc@1: 91.3086 (91.3086)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.245 (0.567)  Loss:  0.7893 (1.1339)  Acc@1: 85.7311 (78.3280)  Acc@5: 97.6415 (94.3860)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-228.pth.tar', 78.70600010498048)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-229.pth.tar', 78.51599994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-224.pth.tar', 78.47199992675782)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-226.pth.tar', 78.33999997802735)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-230.pth.tar', 78.32800013671876)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-227.pth.tar', 78.31400002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-225.pth.tar', 78.24399985107422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-221.pth.tar', 78.16800003417968)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-219.pth.tar', 78.13599998046875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-222.pth.tar', 78.13200006103516)

Train: 231 [   0/1251 (  0%)]  Loss: 3.244 (3.24)  Time: 2.387s,  428.99/s  (2.387s,  428.99/s)  LR: 1.337e-04  Data: 1.419 (1.419)
Train: 231 [  50/1251 (  4%)]  Loss: 3.345 (3.29)  Time: 1.030s,  993.91/s  (1.048s,  977.04/s)  LR: 1.337e-04  Data: 0.011 (0.041)
Train: 231 [ 100/1251 (  8%)]  Loss: 3.119 (3.24)  Time: 1.001s, 1022.52/s  (1.026s,  997.76/s)  LR: 1.337e-04  Data: 0.012 (0.026)
Train: 231 [ 150/1251 ( 12%)]  Loss: 3.422 (3.28)  Time: 1.001s, 1023.11/s  (1.017s, 1006.45/s)  LR: 1.337e-04  Data: 0.012 (0.021)
Train: 231 [ 200/1251 ( 16%)]  Loss: 3.231 (3.27)  Time: 0.992s, 1031.95/s  (1.013s, 1010.73/s)  LR: 1.337e-04  Data: 0.011 (0.019)
Train: 231 [ 250/1251 ( 20%)]  Loss: 3.238 (3.27)  Time: 0.994s, 1030.06/s  (1.011s, 1013.14/s)  LR: 1.337e-04  Data: 0.011 (0.017)
Train: 231 [ 300/1251 ( 24%)]  Loss: 3.371 (3.28)  Time: 0.993s, 1031.60/s  (1.010s, 1013.42/s)  LR: 1.337e-04  Data: 0.011 (0.016)
Train: 231 [ 350/1251 ( 28%)]  Loss: 3.121 (3.26)  Time: 1.005s, 1018.60/s  (1.009s, 1014.95/s)  LR: 1.337e-04  Data: 0.011 (0.016)
Train: 231 [ 400/1251 ( 32%)]  Loss: 3.244 (3.26)  Time: 1.002s, 1021.85/s  (1.008s, 1015.95/s)  LR: 1.337e-04  Data: 0.011 (0.015)
Train: 231 [ 450/1251 ( 36%)]  Loss: 3.195 (3.25)  Time: 0.995s, 1029.14/s  (1.007s, 1016.90/s)  LR: 1.337e-04  Data: 0.012 (0.015)
Train: 231 [ 500/1251 ( 40%)]  Loss: 3.553 (3.28)  Time: 1.037s,  987.06/s  (1.009s, 1015.12/s)  LR: 1.337e-04  Data: 0.011 (0.014)
Train: 231 [ 550/1251 ( 44%)]  Loss: 3.187 (3.27)  Time: 1.035s,  989.17/s  (1.009s, 1015.32/s)  LR: 1.337e-04  Data: 0.012 (0.014)
Train: 231 [ 600/1251 ( 48%)]  Loss: 3.356 (3.28)  Time: 1.000s, 1023.71/s  (1.009s, 1014.42/s)  LR: 1.337e-04  Data: 0.011 (0.014)
Train: 231 [ 650/1251 ( 52%)]  Loss: 3.435 (3.29)  Time: 0.995s, 1029.37/s  (1.009s, 1015.16/s)  LR: 1.337e-04  Data: 0.011 (0.014)
Train: 231 [ 700/1251 ( 56%)]  Loss: 3.281 (3.29)  Time: 1.001s, 1023.23/s  (1.008s, 1015.86/s)  LR: 1.337e-04  Data: 0.011 (0.013)
Train: 231 [ 750/1251 ( 60%)]  Loss: 3.388 (3.30)  Time: 1.007s, 1016.93/s  (1.007s, 1016.39/s)  LR: 1.337e-04  Data: 0.011 (0.013)
Train: 231 [ 800/1251 ( 64%)]  Loss: 3.400 (3.30)  Time: 1.003s, 1020.95/s  (1.007s, 1016.87/s)  LR: 1.337e-04  Data: 0.014 (0.013)
Train: 231 [ 850/1251 ( 68%)]  Loss: 2.935 (3.28)  Time: 1.033s,  991.40/s  (1.007s, 1016.75/s)  LR: 1.337e-04  Data: 0.011 (0.013)
Train: 231 [ 900/1251 ( 72%)]  Loss: 3.245 (3.28)  Time: 0.998s, 1026.27/s  (1.007s, 1017.11/s)  LR: 1.337e-04  Data: 0.011 (0.013)
Train: 231 [ 950/1251 ( 76%)]  Loss: 3.536 (3.29)  Time: 1.018s, 1005.40/s  (1.007s, 1017.35/s)  LR: 1.337e-04  Data: 0.015 (0.013)
Train: 231 [1000/1251 ( 80%)]  Loss: 3.195 (3.29)  Time: 1.002s, 1022.13/s  (1.006s, 1017.66/s)  LR: 1.337e-04  Data: 0.013 (0.013)
Train: 231 [1050/1251 ( 84%)]  Loss: 3.421 (3.29)  Time: 0.996s, 1028.54/s  (1.006s, 1017.89/s)  LR: 1.337e-04  Data: 0.010 (0.013)
Train: 231 [1100/1251 ( 88%)]  Loss: 3.299 (3.29)  Time: 0.994s, 1029.79/s  (1.006s, 1018.13/s)  LR: 1.337e-04  Data: 0.011 (0.013)
Train: 231 [1150/1251 ( 92%)]  Loss: 3.478 (3.30)  Time: 1.000s, 1024.02/s  (1.006s, 1018.40/s)  LR: 1.337e-04  Data: 0.012 (0.012)
Train: 231 [1200/1251 ( 96%)]  Loss: 3.262 (3.30)  Time: 0.999s, 1025.30/s  (1.005s, 1018.53/s)  LR: 1.337e-04  Data: 0.011 (0.012)
Train: 231 [1250/1251 (100%)]  Loss: 3.429 (3.31)  Time: 0.986s, 1038.30/s  (1.005s, 1018.71/s)  LR: 1.337e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.656 (1.656)  Loss:  0.6780 (0.6780)  Acc@1: 91.4062 (91.4062)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.245 (0.569)  Loss:  0.7079 (1.1235)  Acc@1: 85.9670 (78.5300)  Acc@5: 97.2877 (94.4460)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-228.pth.tar', 78.70600010498048)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-231.pth.tar', 78.52999998046874)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-229.pth.tar', 78.51599994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-224.pth.tar', 78.47199992675782)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-226.pth.tar', 78.33999997802735)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-230.pth.tar', 78.32800013671876)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-227.pth.tar', 78.31400002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-225.pth.tar', 78.24399985107422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-221.pth.tar', 78.16800003417968)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-219.pth.tar', 78.13599998046875)

Train: 232 [   0/1251 (  0%)]  Loss: 3.114 (3.11)  Time: 2.496s,  410.28/s  (2.496s,  410.28/s)  LR: 1.303e-04  Data: 1.542 (1.542)
Train: 232 [  50/1251 (  4%)]  Loss: 3.055 (3.08)  Time: 0.995s, 1029.29/s  (1.050s,  975.50/s)  LR: 1.303e-04  Data: 0.011 (0.041)
Train: 232 [ 100/1251 (  8%)]  Loss: 2.999 (3.06)  Time: 1.043s,  981.52/s  (1.036s,  988.03/s)  LR: 1.303e-04  Data: 0.010 (0.026)
Train: 232 [ 150/1251 ( 12%)]  Loss: 3.115 (3.07)  Time: 0.994s, 1030.58/s  (1.025s,  999.16/s)  LR: 1.303e-04  Data: 0.010 (0.021)
Train: 232 [ 200/1251 ( 16%)]  Loss: 2.981 (3.05)  Time: 0.996s, 1028.48/s  (1.018s, 1005.48/s)  LR: 1.303e-04  Data: 0.010 (0.019)
Train: 232 [ 250/1251 ( 20%)]  Loss: 3.087 (3.06)  Time: 1.005s, 1018.79/s  (1.014s, 1009.57/s)  LR: 1.303e-04  Data: 0.011 (0.017)
Train: 232 [ 300/1251 ( 24%)]  Loss: 3.572 (3.13)  Time: 1.030s,  993.80/s  (1.013s, 1010.77/s)  LR: 1.303e-04  Data: 0.011 (0.016)
Train: 232 [ 350/1251 ( 28%)]  Loss: 2.912 (3.10)  Time: 0.993s, 1030.79/s  (1.013s, 1010.97/s)  LR: 1.303e-04  Data: 0.011 (0.015)
Train: 232 [ 400/1251 ( 32%)]  Loss: 3.044 (3.10)  Time: 1.047s,  978.25/s  (1.011s, 1012.41/s)  LR: 1.303e-04  Data: 0.011 (0.015)
Train: 232 [ 450/1251 ( 36%)]  Loss: 3.230 (3.11)  Time: 1.003s, 1020.80/s  (1.010s, 1013.59/s)  LR: 1.303e-04  Data: 0.012 (0.014)
Train: 232 [ 500/1251 ( 40%)]  Loss: 3.331 (3.13)  Time: 0.997s, 1026.86/s  (1.011s, 1013.30/s)  LR: 1.303e-04  Data: 0.010 (0.014)
Train: 232 [ 550/1251 ( 44%)]  Loss: 3.317 (3.15)  Time: 0.996s, 1028.48/s  (1.009s, 1014.53/s)  LR: 1.303e-04  Data: 0.011 (0.014)
Train: 232 [ 600/1251 ( 48%)]  Loss: 3.050 (3.14)  Time: 0.996s, 1027.64/s  (1.009s, 1014.85/s)  LR: 1.303e-04  Data: 0.011 (0.014)
Train: 232 [ 650/1251 ( 52%)]  Loss: 3.390 (3.16)  Time: 1.004s, 1020.21/s  (1.009s, 1014.98/s)  LR: 1.303e-04  Data: 0.010 (0.013)
Train: 232 [ 700/1251 ( 56%)]  Loss: 3.364 (3.17)  Time: 0.997s, 1027.42/s  (1.008s, 1015.46/s)  LR: 1.303e-04  Data: 0.010 (0.013)
Train: 232 [ 750/1251 ( 60%)]  Loss: 3.164 (3.17)  Time: 0.993s, 1031.14/s  (1.008s, 1016.10/s)  LR: 1.303e-04  Data: 0.011 (0.013)
Train: 232 [ 800/1251 ( 64%)]  Loss: 2.890 (3.15)  Time: 0.994s, 1029.75/s  (1.007s, 1016.50/s)  LR: 1.303e-04  Data: 0.011 (0.013)
Train: 232 [ 850/1251 ( 68%)]  Loss: 3.116 (3.15)  Time: 0.996s, 1028.00/s  (1.007s, 1016.88/s)  LR: 1.303e-04  Data: 0.010 (0.013)
Train: 232 [ 900/1251 ( 72%)]  Loss: 3.308 (3.16)  Time: 1.037s,  987.86/s  (1.008s, 1015.66/s)  LR: 1.303e-04  Data: 0.011 (0.013)
Train: 232 [ 950/1251 ( 76%)]  Loss: 3.042 (3.15)  Time: 1.006s, 1017.51/s  (1.008s, 1015.64/s)  LR: 1.303e-04  Data: 0.011 (0.013)
Train: 232 [1000/1251 ( 80%)]  Loss: 3.111 (3.15)  Time: 1.001s, 1023.11/s  (1.008s, 1015.85/s)  LR: 1.303e-04  Data: 0.013 (0.013)
Train: 232 [1050/1251 ( 84%)]  Loss: 3.093 (3.15)  Time: 1.007s, 1017.34/s  (1.008s, 1016.13/s)  LR: 1.303e-04  Data: 0.011 (0.013)
Train: 232 [1100/1251 ( 88%)]  Loss: 3.242 (3.15)  Time: 0.994s, 1030.38/s  (1.007s, 1016.48/s)  LR: 1.303e-04  Data: 0.011 (0.012)
Train: 232 [1150/1251 ( 92%)]  Loss: 3.527 (3.17)  Time: 1.010s, 1014.30/s  (1.007s, 1016.61/s)  LR: 1.303e-04  Data: 0.011 (0.012)
Train: 232 [1200/1251 ( 96%)]  Loss: 3.534 (3.18)  Time: 0.996s, 1028.57/s  (1.007s, 1016.89/s)  LR: 1.303e-04  Data: 0.011 (0.012)
Train: 232 [1250/1251 (100%)]  Loss: 3.197 (3.18)  Time: 0.986s, 1038.79/s  (1.007s, 1017.21/s)  LR: 1.303e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.725 (1.725)  Loss:  0.6484 (0.6484)  Acc@1: 91.3086 (91.3086)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.245 (0.567)  Loss:  0.7563 (1.1121)  Acc@1: 84.7877 (78.5980)  Acc@5: 97.2877 (94.4480)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-228.pth.tar', 78.70600010498048)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-232.pth.tar', 78.59799998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-231.pth.tar', 78.52999998046874)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-229.pth.tar', 78.51599994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-224.pth.tar', 78.47199992675782)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-226.pth.tar', 78.33999997802735)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-230.pth.tar', 78.32800013671876)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-227.pth.tar', 78.31400002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-225.pth.tar', 78.24399985107422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-221.pth.tar', 78.16800003417968)

Train: 233 [   0/1251 (  0%)]  Loss: 3.103 (3.10)  Time: 2.428s,  421.74/s  (2.428s,  421.74/s)  LR: 1.269e-04  Data: 1.470 (1.470)
Train: 233 [  50/1251 (  4%)]  Loss: 3.266 (3.18)  Time: 1.000s, 1024.10/s  (1.037s,  987.38/s)  LR: 1.269e-04  Data: 0.011 (0.041)
Train: 233 [ 100/1251 (  8%)]  Loss: 2.934 (3.10)  Time: 1.013s, 1010.54/s  (1.023s, 1000.94/s)  LR: 1.269e-04  Data: 0.012 (0.026)
Train: 233 [ 150/1251 ( 12%)]  Loss: 3.276 (3.14)  Time: 0.997s, 1027.44/s  (1.016s, 1007.53/s)  LR: 1.269e-04  Data: 0.010 (0.021)
Train: 233 [ 200/1251 ( 16%)]  Loss: 3.247 (3.17)  Time: 0.996s, 1028.02/s  (1.017s, 1007.08/s)  LR: 1.269e-04  Data: 0.010 (0.019)
Train: 233 [ 250/1251 ( 20%)]  Loss: 3.383 (3.20)  Time: 0.996s, 1028.51/s  (1.013s, 1010.61/s)  LR: 1.269e-04  Data: 0.012 (0.017)
Train: 233 [ 300/1251 ( 24%)]  Loss: 3.475 (3.24)  Time: 0.994s, 1030.17/s  (1.011s, 1012.50/s)  LR: 1.269e-04  Data: 0.011 (0.016)
Train: 233 [ 350/1251 ( 28%)]  Loss: 3.175 (3.23)  Time: 0.998s, 1025.87/s  (1.012s, 1012.08/s)  LR: 1.269e-04  Data: 0.011 (0.015)
Train: 233 [ 400/1251 ( 32%)]  Loss: 3.253 (3.23)  Time: 0.997s, 1027.01/s  (1.014s, 1009.90/s)  LR: 1.269e-04  Data: 0.011 (0.015)
Train: 233 [ 450/1251 ( 36%)]  Loss: 3.187 (3.23)  Time: 1.038s,  986.13/s  (1.013s, 1010.93/s)  LR: 1.269e-04  Data: 0.011 (0.014)
Train: 233 [ 500/1251 ( 40%)]  Loss: 3.541 (3.26)  Time: 0.997s, 1026.93/s  (1.012s, 1011.99/s)  LR: 1.269e-04  Data: 0.011 (0.014)
Train: 233 [ 550/1251 ( 44%)]  Loss: 3.596 (3.29)  Time: 1.014s, 1010.11/s  (1.011s, 1012.48/s)  LR: 1.269e-04  Data: 0.011 (0.014)
Train: 233 [ 600/1251 ( 48%)]  Loss: 3.524 (3.30)  Time: 0.994s, 1030.04/s  (1.011s, 1012.96/s)  LR: 1.269e-04  Data: 0.011 (0.014)
Train: 233 [ 650/1251 ( 52%)]  Loss: 2.939 (3.28)  Time: 0.994s, 1029.90/s  (1.010s, 1013.80/s)  LR: 1.269e-04  Data: 0.011 (0.013)
Train: 233 [ 700/1251 ( 56%)]  Loss: 3.216 (3.27)  Time: 0.992s, 1032.03/s  (1.009s, 1014.53/s)  LR: 1.269e-04  Data: 0.010 (0.013)
Train: 233 [ 750/1251 ( 60%)]  Loss: 3.391 (3.28)  Time: 0.994s, 1029.91/s  (1.009s, 1015.17/s)  LR: 1.269e-04  Data: 0.011 (0.013)
Train: 233 [ 800/1251 ( 64%)]  Loss: 3.212 (3.28)  Time: 1.001s, 1023.12/s  (1.008s, 1015.61/s)  LR: 1.269e-04  Data: 0.012 (0.013)
Train: 233 [ 850/1251 ( 68%)]  Loss: 3.509 (3.29)  Time: 1.023s, 1001.08/s  (1.008s, 1016.11/s)  LR: 1.269e-04  Data: 0.010 (0.013)
Train: 233 [ 900/1251 ( 72%)]  Loss: 3.124 (3.28)  Time: 1.061s,  965.39/s  (1.009s, 1014.84/s)  LR: 1.269e-04  Data: 0.012 (0.013)
Train: 233 [ 950/1251 ( 76%)]  Loss: 3.287 (3.28)  Time: 1.004s, 1019.68/s  (1.009s, 1014.68/s)  LR: 1.269e-04  Data: 0.011 (0.013)
Train: 233 [1000/1251 ( 80%)]  Loss: 3.357 (3.29)  Time: 1.026s,  998.01/s  (1.009s, 1014.98/s)  LR: 1.269e-04  Data: 0.011 (0.013)
Train: 233 [1050/1251 ( 84%)]  Loss: 3.288 (3.29)  Time: 0.993s, 1030.96/s  (1.008s, 1015.43/s)  LR: 1.269e-04  Data: 0.011 (0.013)
Train: 233 [1100/1251 ( 88%)]  Loss: 3.166 (3.28)  Time: 0.997s, 1026.93/s  (1.008s, 1015.72/s)  LR: 1.269e-04  Data: 0.011 (0.012)
Train: 233 [1150/1251 ( 92%)]  Loss: 3.028 (3.27)  Time: 0.996s, 1027.84/s  (1.009s, 1015.25/s)  LR: 1.269e-04  Data: 0.011 (0.012)
Train: 233 [1200/1251 ( 96%)]  Loss: 3.670 (3.29)  Time: 0.995s, 1028.85/s  (1.008s, 1015.65/s)  LR: 1.269e-04  Data: 0.012 (0.012)
Train: 233 [1250/1251 (100%)]  Loss: 3.195 (3.28)  Time: 1.029s,  995.31/s  (1.008s, 1015.77/s)  LR: 1.269e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.694 (1.694)  Loss:  0.6472 (0.6472)  Acc@1: 90.3320 (90.3320)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.245 (0.565)  Loss:  0.7809 (1.1225)  Acc@1: 86.5566 (78.7100)  Acc@5: 97.0519 (94.5220)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-233.pth.tar', 78.71000010742188)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-228.pth.tar', 78.70600010498048)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-232.pth.tar', 78.59799998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-231.pth.tar', 78.52999998046874)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-229.pth.tar', 78.51599994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-224.pth.tar', 78.47199992675782)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-226.pth.tar', 78.33999997802735)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-230.pth.tar', 78.32800013671876)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-227.pth.tar', 78.31400002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-225.pth.tar', 78.24399985107422)

Train: 234 [   0/1251 (  0%)]  Loss: 3.608 (3.61)  Time: 2.593s,  394.97/s  (2.593s,  394.97/s)  LR: 1.236e-04  Data: 1.632 (1.632)
Train: 234 [  50/1251 (  4%)]  Loss: 3.060 (3.33)  Time: 1.057s,  968.41/s  (1.078s,  949.67/s)  LR: 1.236e-04  Data: 0.015 (0.044)
Train: 234 [ 100/1251 (  8%)]  Loss: 3.195 (3.29)  Time: 0.999s, 1025.14/s  (1.054s,  971.44/s)  LR: 1.236e-04  Data: 0.011 (0.028)
Train: 234 [ 150/1251 ( 12%)]  Loss: 3.602 (3.37)  Time: 1.006s, 1017.73/s  (1.038s,  986.42/s)  LR: 1.236e-04  Data: 0.011 (0.022)
Train: 234 [ 200/1251 ( 16%)]  Loss: 3.567 (3.41)  Time: 1.018s, 1005.49/s  (1.036s,  988.35/s)  LR: 1.236e-04  Data: 0.011 (0.020)
Train: 234 [ 250/1251 ( 20%)]  Loss: 2.836 (3.31)  Time: 1.046s,  979.11/s  (1.030s,  993.72/s)  LR: 1.236e-04  Data: 0.011 (0.018)
Train: 234 [ 300/1251 ( 24%)]  Loss: 3.184 (3.29)  Time: 1.031s,  993.01/s  (1.027s,  997.53/s)  LR: 1.236e-04  Data: 0.011 (0.017)
Train: 234 [ 350/1251 ( 28%)]  Loss: 3.430 (3.31)  Time: 0.998s, 1025.67/s  (1.026s,  998.47/s)  LR: 1.236e-04  Data: 0.011 (0.016)
Train: 234 [ 400/1251 ( 32%)]  Loss: 3.313 (3.31)  Time: 0.995s, 1028.98/s  (1.023s, 1000.91/s)  LR: 1.236e-04  Data: 0.012 (0.015)
Train: 234 [ 450/1251 ( 36%)]  Loss: 3.200 (3.30)  Time: 0.999s, 1025.34/s  (1.020s, 1003.45/s)  LR: 1.236e-04  Data: 0.012 (0.015)
Train: 234 [ 500/1251 ( 40%)]  Loss: 2.964 (3.27)  Time: 0.998s, 1026.54/s  (1.018s, 1005.46/s)  LR: 1.236e-04  Data: 0.011 (0.014)
Train: 234 [ 550/1251 ( 44%)]  Loss: 2.767 (3.23)  Time: 0.995s, 1028.99/s  (1.017s, 1007.30/s)  LR: 1.236e-04  Data: 0.011 (0.014)
Train: 234 [ 600/1251 ( 48%)]  Loss: 3.132 (3.22)  Time: 0.999s, 1025.29/s  (1.015s, 1008.69/s)  LR: 1.236e-04  Data: 0.011 (0.014)
Train: 234 [ 650/1251 ( 52%)]  Loss: 3.266 (3.22)  Time: 0.994s, 1030.43/s  (1.014s, 1009.89/s)  LR: 1.236e-04  Data: 0.010 (0.014)
Train: 234 [ 700/1251 ( 56%)]  Loss: 3.085 (3.21)  Time: 0.998s, 1026.52/s  (1.015s, 1009.33/s)  LR: 1.236e-04  Data: 0.012 (0.013)
Train: 234 [ 750/1251 ( 60%)]  Loss: 3.146 (3.21)  Time: 0.992s, 1031.90/s  (1.014s, 1010.13/s)  LR: 1.236e-04  Data: 0.011 (0.013)
Train: 234 [ 800/1251 ( 64%)]  Loss: 3.219 (3.21)  Time: 0.995s, 1029.16/s  (1.013s, 1010.95/s)  LR: 1.236e-04  Data: 0.011 (0.013)
Train: 234 [ 850/1251 ( 68%)]  Loss: 3.394 (3.22)  Time: 0.995s, 1028.71/s  (1.012s, 1011.68/s)  LR: 1.236e-04  Data: 0.011 (0.013)
Train: 234 [ 900/1251 ( 72%)]  Loss: 3.438 (3.23)  Time: 1.045s,  979.53/s  (1.012s, 1011.76/s)  LR: 1.236e-04  Data: 0.011 (0.013)
Train: 234 [ 950/1251 ( 76%)]  Loss: 2.992 (3.22)  Time: 0.998s, 1025.66/s  (1.012s, 1011.94/s)  LR: 1.236e-04  Data: 0.011 (0.013)
Train: 234 [1000/1251 ( 80%)]  Loss: 3.240 (3.22)  Time: 1.049s,  976.12/s  (1.012s, 1012.27/s)  LR: 1.236e-04  Data: 0.011 (0.013)
Train: 234 [1050/1251 ( 84%)]  Loss: 3.074 (3.21)  Time: 0.995s, 1029.22/s  (1.013s, 1011.09/s)  LR: 1.236e-04  Data: 0.011 (0.013)
Train: 234 [1100/1251 ( 88%)]  Loss: 3.289 (3.22)  Time: 1.053s,  972.40/s  (1.012s, 1011.56/s)  LR: 1.236e-04  Data: 0.011 (0.013)
Train: 234 [1150/1251 ( 92%)]  Loss: 3.035 (3.21)  Time: 0.999s, 1025.53/s  (1.012s, 1011.90/s)  LR: 1.236e-04  Data: 0.011 (0.013)
Train: 234 [1200/1251 ( 96%)]  Loss: 2.979 (3.20)  Time: 0.997s, 1026.64/s  (1.012s, 1012.35/s)  LR: 1.236e-04  Data: 0.010 (0.013)
Train: 234 [1250/1251 (100%)]  Loss: 3.176 (3.20)  Time: 0.988s, 1036.50/s  (1.012s, 1012.12/s)  LR: 1.236e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.690 (1.690)  Loss:  0.7318 (0.7318)  Acc@1: 91.6992 (91.6992)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.245 (0.577)  Loss:  0.8209 (1.2072)  Acc@1: 86.0849 (78.4720)  Acc@5: 97.6415 (94.4100)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-233.pth.tar', 78.71000010742188)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-228.pth.tar', 78.70600010498048)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-232.pth.tar', 78.59799998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-231.pth.tar', 78.52999998046874)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-229.pth.tar', 78.51599994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-224.pth.tar', 78.47199992675782)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-234.pth.tar', 78.47199990234375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-226.pth.tar', 78.33999997802735)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-230.pth.tar', 78.32800013671876)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-227.pth.tar', 78.31400002929688)

Train: 235 [   0/1251 (  0%)]  Loss: 3.183 (3.18)  Time: 2.478s,  413.27/s  (2.478s,  413.27/s)  LR: 1.203e-04  Data: 1.472 (1.472)
Train: 235 [  50/1251 (  4%)]  Loss: 3.531 (3.36)  Time: 0.995s, 1029.66/s  (1.029s,  994.84/s)  LR: 1.203e-04  Data: 0.011 (0.040)
Train: 235 [ 100/1251 (  8%)]  Loss: 3.100 (3.27)  Time: 1.033s,  991.19/s  (1.017s, 1006.84/s)  LR: 1.203e-04  Data: 0.011 (0.026)
Train: 235 [ 150/1251 ( 12%)]  Loss: 3.298 (3.28)  Time: 0.995s, 1028.93/s  (1.011s, 1012.39/s)  LR: 1.203e-04  Data: 0.011 (0.021)
Train: 235 [ 200/1251 ( 16%)]  Loss: 3.136 (3.25)  Time: 1.002s, 1021.81/s  (1.010s, 1014.29/s)  LR: 1.203e-04  Data: 0.011 (0.018)
Train: 235 [ 250/1251 ( 20%)]  Loss: 3.102 (3.23)  Time: 0.996s, 1028.25/s  (1.007s, 1016.48/s)  LR: 1.203e-04  Data: 0.011 (0.017)
Train: 235 [ 300/1251 ( 24%)]  Loss: 3.251 (3.23)  Time: 0.996s, 1027.96/s  (1.006s, 1017.98/s)  LR: 1.203e-04  Data: 0.011 (0.016)
Train: 235 [ 350/1251 ( 28%)]  Loss: 3.295 (3.24)  Time: 0.998s, 1025.55/s  (1.005s, 1018.80/s)  LR: 1.203e-04  Data: 0.012 (0.015)
Train: 235 [ 400/1251 ( 32%)]  Loss: 3.338 (3.25)  Time: 1.023s, 1000.97/s  (1.006s, 1018.01/s)  LR: 1.203e-04  Data: 0.011 (0.015)
Train: 235 [ 450/1251 ( 36%)]  Loss: 3.431 (3.27)  Time: 1.061s,  965.26/s  (1.008s, 1016.28/s)  LR: 1.203e-04  Data: 0.011 (0.014)
Train: 235 [ 500/1251 ( 40%)]  Loss: 3.432 (3.28)  Time: 0.994s, 1029.83/s  (1.007s, 1017.09/s)  LR: 1.203e-04  Data: 0.012 (0.014)
Train: 235 [ 550/1251 ( 44%)]  Loss: 3.517 (3.30)  Time: 1.000s, 1023.97/s  (1.006s, 1017.59/s)  LR: 1.203e-04  Data: 0.011 (0.014)
Train: 235 [ 600/1251 ( 48%)]  Loss: 2.901 (3.27)  Time: 0.997s, 1026.95/s  (1.006s, 1018.10/s)  LR: 1.203e-04  Data: 0.012 (0.014)
Train: 235 [ 650/1251 ( 52%)]  Loss: 3.240 (3.27)  Time: 0.997s, 1027.17/s  (1.006s, 1018.23/s)  LR: 1.203e-04  Data: 0.011 (0.013)
Train: 235 [ 700/1251 ( 56%)]  Loss: 3.512 (3.28)  Time: 0.998s, 1026.53/s  (1.006s, 1018.35/s)  LR: 1.203e-04  Data: 0.011 (0.013)
Train: 235 [ 750/1251 ( 60%)]  Loss: 3.298 (3.29)  Time: 1.008s, 1015.90/s  (1.005s, 1018.76/s)  LR: 1.203e-04  Data: 0.010 (0.013)
Train: 235 [ 800/1251 ( 64%)]  Loss: 3.175 (3.28)  Time: 0.999s, 1024.74/s  (1.005s, 1019.08/s)  LR: 1.203e-04  Data: 0.011 (0.013)
Train: 235 [ 850/1251 ( 68%)]  Loss: 3.223 (3.28)  Time: 0.997s, 1026.87/s  (1.005s, 1019.39/s)  LR: 1.203e-04  Data: 0.011 (0.013)
Train: 235 [ 900/1251 ( 72%)]  Loss: 3.380 (3.28)  Time: 0.995s, 1028.83/s  (1.004s, 1019.78/s)  LR: 1.203e-04  Data: 0.011 (0.013)
Train: 235 [ 950/1251 ( 76%)]  Loss: 2.753 (3.25)  Time: 1.004s, 1019.97/s  (1.004s, 1020.06/s)  LR: 1.203e-04  Data: 0.011 (0.013)
Train: 235 [1000/1251 ( 80%)]  Loss: 3.114 (3.25)  Time: 0.996s, 1028.11/s  (1.004s, 1020.30/s)  LR: 1.203e-04  Data: 0.011 (0.013)
Train: 235 [1050/1251 ( 84%)]  Loss: 2.947 (3.23)  Time: 1.009s, 1014.49/s  (1.003s, 1020.51/s)  LR: 1.203e-04  Data: 0.012 (0.012)
Train: 235 [1100/1251 ( 88%)]  Loss: 2.818 (3.22)  Time: 0.997s, 1027.58/s  (1.004s, 1020.05/s)  LR: 1.203e-04  Data: 0.011 (0.012)
Train: 235 [1150/1251 ( 92%)]  Loss: 3.303 (3.22)  Time: 0.995s, 1029.33/s  (1.004s, 1020.17/s)  LR: 1.203e-04  Data: 0.011 (0.012)
Train: 235 [1200/1251 ( 96%)]  Loss: 3.009 (3.21)  Time: 0.995s, 1028.73/s  (1.005s, 1018.51/s)  LR: 1.203e-04  Data: 0.011 (0.012)
Train: 235 [1250/1251 (100%)]  Loss: 2.901 (3.20)  Time: 0.983s, 1041.71/s  (1.005s, 1018.58/s)  LR: 1.203e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.659 (1.659)  Loss:  0.5922 (0.5922)  Acc@1: 91.3086 (91.3086)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.245 (0.571)  Loss:  0.6820 (1.0699)  Acc@1: 86.0849 (78.8380)  Acc@5: 97.4057 (94.5180)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-235.pth.tar', 78.83800003173828)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-233.pth.tar', 78.71000010742188)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-228.pth.tar', 78.70600010498048)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-232.pth.tar', 78.59799998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-231.pth.tar', 78.52999998046874)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-229.pth.tar', 78.51599994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-224.pth.tar', 78.47199992675782)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-234.pth.tar', 78.47199990234375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-226.pth.tar', 78.33999997802735)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-230.pth.tar', 78.32800013671876)

Train: 236 [   0/1251 (  0%)]  Loss: 3.540 (3.54)  Time: 2.498s,  409.94/s  (2.498s,  409.94/s)  LR: 1.171e-04  Data: 1.533 (1.533)
Train: 236 [  50/1251 (  4%)]  Loss: 2.965 (3.25)  Time: 0.996s, 1027.66/s  (1.045s,  980.16/s)  LR: 1.171e-04  Data: 0.012 (0.042)
Train: 236 [ 100/1251 (  8%)]  Loss: 3.201 (3.24)  Time: 0.995s, 1028.84/s  (1.022s, 1001.54/s)  LR: 1.171e-04  Data: 0.011 (0.027)
Train: 236 [ 150/1251 ( 12%)]  Loss: 2.784 (3.12)  Time: 0.994s, 1029.83/s  (1.016s, 1007.41/s)  LR: 1.171e-04  Data: 0.010 (0.022)
Train: 236 [ 200/1251 ( 16%)]  Loss: 3.139 (3.13)  Time: 1.055s,  970.33/s  (1.013s, 1010.90/s)  LR: 1.171e-04  Data: 0.010 (0.019)
Train: 236 [ 250/1251 ( 20%)]  Loss: 2.546 (3.03)  Time: 1.064s,  962.81/s  (1.011s, 1012.63/s)  LR: 1.171e-04  Data: 0.011 (0.017)
Train: 236 [ 300/1251 ( 24%)]  Loss: 2.955 (3.02)  Time: 1.006s, 1018.39/s  (1.011s, 1013.15/s)  LR: 1.171e-04  Data: 0.011 (0.016)
Train: 236 [ 350/1251 ( 28%)]  Loss: 3.254 (3.05)  Time: 0.999s, 1024.85/s  (1.009s, 1014.89/s)  LR: 1.171e-04  Data: 0.010 (0.016)
Train: 236 [ 400/1251 ( 32%)]  Loss: 2.920 (3.03)  Time: 1.033s,  991.47/s  (1.010s, 1014.18/s)  LR: 1.171e-04  Data: 0.012 (0.015)
Train: 236 [ 450/1251 ( 36%)]  Loss: 3.243 (3.05)  Time: 1.036s,  988.03/s  (1.014s, 1009.50/s)  LR: 1.171e-04  Data: 0.012 (0.015)
Train: 236 [ 500/1251 ( 40%)]  Loss: 3.372 (3.08)  Time: 1.003s, 1020.51/s  (1.015s, 1008.74/s)  LR: 1.171e-04  Data: 0.010 (0.014)
Train: 236 [ 550/1251 ( 44%)]  Loss: 3.175 (3.09)  Time: 0.993s, 1030.78/s  (1.014s, 1010.09/s)  LR: 1.171e-04  Data: 0.011 (0.014)
Train: 236 [ 600/1251 ( 48%)]  Loss: 3.376 (3.11)  Time: 0.999s, 1025.35/s  (1.012s, 1011.38/s)  LR: 1.171e-04  Data: 0.011 (0.014)
Train: 236 [ 650/1251 ( 52%)]  Loss: 3.444 (3.14)  Time: 0.996s, 1028.03/s  (1.011s, 1012.51/s)  LR: 1.171e-04  Data: 0.011 (0.013)
Train: 236 [ 700/1251 ( 56%)]  Loss: 3.394 (3.15)  Time: 0.997s, 1027.30/s  (1.012s, 1012.15/s)  LR: 1.171e-04  Data: 0.011 (0.013)
Train: 236 [ 750/1251 ( 60%)]  Loss: 3.110 (3.15)  Time: 0.995s, 1028.88/s  (1.011s, 1013.13/s)  LR: 1.171e-04  Data: 0.010 (0.013)
Train: 236 [ 800/1251 ( 64%)]  Loss: 3.007 (3.14)  Time: 0.995s, 1028.91/s  (1.010s, 1013.98/s)  LR: 1.171e-04  Data: 0.010 (0.013)
Train: 236 [ 850/1251 ( 68%)]  Loss: 3.400 (3.16)  Time: 0.995s, 1029.52/s  (1.009s, 1014.69/s)  LR: 1.171e-04  Data: 0.011 (0.013)
Train: 236 [ 900/1251 ( 72%)]  Loss: 3.559 (3.18)  Time: 1.000s, 1024.08/s  (1.009s, 1015.30/s)  LR: 1.171e-04  Data: 0.011 (0.013)
Train: 236 [ 950/1251 ( 76%)]  Loss: 3.416 (3.19)  Time: 0.998s, 1026.18/s  (1.008s, 1015.82/s)  LR: 1.171e-04  Data: 0.011 (0.013)
Train: 236 [1000/1251 ( 80%)]  Loss: 3.089 (3.19)  Time: 0.996s, 1028.34/s  (1.008s, 1016.37/s)  LR: 1.171e-04  Data: 0.011 (0.013)
Train: 236 [1050/1251 ( 84%)]  Loss: 3.139 (3.18)  Time: 0.995s, 1029.14/s  (1.007s, 1016.89/s)  LR: 1.171e-04  Data: 0.011 (0.012)
Train: 236 [1100/1251 ( 88%)]  Loss: 3.342 (3.19)  Time: 0.997s, 1027.02/s  (1.007s, 1017.31/s)  LR: 1.171e-04  Data: 0.012 (0.012)
Train: 236 [1150/1251 ( 92%)]  Loss: 3.315 (3.20)  Time: 0.996s, 1027.82/s  (1.007s, 1017.33/s)  LR: 1.171e-04  Data: 0.012 (0.012)
Train: 236 [1200/1251 ( 96%)]  Loss: 3.287 (3.20)  Time: 0.995s, 1029.19/s  (1.006s, 1017.55/s)  LR: 1.171e-04  Data: 0.011 (0.012)
Train: 236 [1250/1251 (100%)]  Loss: 3.207 (3.20)  Time: 0.988s, 1036.13/s  (1.006s, 1017.73/s)  LR: 1.171e-04  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.649 (1.649)  Loss:  0.6555 (0.6555)  Acc@1: 91.0156 (91.0156)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.245 (0.566)  Loss:  0.7950 (1.1129)  Acc@1: 85.7311 (78.9140)  Acc@5: 97.5236 (94.5340)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-236.pth.tar', 78.91400000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-235.pth.tar', 78.83800003173828)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-233.pth.tar', 78.71000010742188)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-228.pth.tar', 78.70600010498048)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-232.pth.tar', 78.59799998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-231.pth.tar', 78.52999998046874)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-229.pth.tar', 78.51599994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-224.pth.tar', 78.47199992675782)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-234.pth.tar', 78.47199990234375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-226.pth.tar', 78.33999997802735)

Train: 237 [   0/1251 (  0%)]  Loss: 3.307 (3.31)  Time: 4.183s,  244.78/s  (4.183s,  244.78/s)  LR: 1.139e-04  Data: 2.994 (2.994)
Train: 237 [  50/1251 (  4%)]  Loss: 3.083 (3.19)  Time: 1.029s,  994.82/s  (1.080s,  948.14/s)  LR: 1.139e-04  Data: 0.011 (0.070)
Train: 237 [ 100/1251 (  8%)]  Loss: 2.955 (3.11)  Time: 1.002s, 1021.46/s  (1.044s,  981.20/s)  LR: 1.139e-04  Data: 0.011 (0.041)
Train: 237 [ 150/1251 ( 12%)]  Loss: 3.551 (3.22)  Time: 0.992s, 1032.11/s  (1.031s,  992.82/s)  LR: 1.139e-04  Data: 0.010 (0.031)
Train: 237 [ 200/1251 ( 16%)]  Loss: 3.556 (3.29)  Time: 0.992s, 1032.25/s  (1.023s, 1000.49/s)  LR: 1.139e-04  Data: 0.011 (0.026)
Train: 237 [ 250/1251 ( 20%)]  Loss: 3.143 (3.27)  Time: 0.998s, 1026.48/s  (1.019s, 1005.06/s)  LR: 1.139e-04  Data: 0.012 (0.023)
Train: 237 [ 300/1251 ( 24%)]  Loss: 3.186 (3.25)  Time: 0.994s, 1029.66/s  (1.016s, 1007.82/s)  LR: 1.139e-04  Data: 0.011 (0.021)
Train: 237 [ 350/1251 ( 28%)]  Loss: 3.133 (3.24)  Time: 1.035s,  989.64/s  (1.014s, 1009.75/s)  LR: 1.139e-04  Data: 0.011 (0.020)
Train: 237 [ 400/1251 ( 32%)]  Loss: 3.278 (3.24)  Time: 0.996s, 1027.75/s  (1.012s, 1011.56/s)  LR: 1.139e-04  Data: 0.011 (0.019)
Train: 237 [ 450/1251 ( 36%)]  Loss: 2.972 (3.22)  Time: 1.005s, 1019.06/s  (1.011s, 1013.02/s)  LR: 1.139e-04  Data: 0.011 (0.018)
Train: 237 [ 500/1251 ( 40%)]  Loss: 3.314 (3.23)  Time: 0.996s, 1027.82/s  (1.010s, 1014.03/s)  LR: 1.139e-04  Data: 0.011 (0.017)
Train: 237 [ 550/1251 ( 44%)]  Loss: 3.261 (3.23)  Time: 0.997s, 1026.72/s  (1.009s, 1014.71/s)  LR: 1.139e-04  Data: 0.011 (0.017)
Train: 237 [ 600/1251 ( 48%)]  Loss: 2.914 (3.20)  Time: 0.993s, 1030.73/s  (1.008s, 1015.47/s)  LR: 1.139e-04  Data: 0.011 (0.016)
Train: 237 [ 650/1251 ( 52%)]  Loss: 3.261 (3.21)  Time: 1.063s,  963.63/s  (1.009s, 1015.02/s)  LR: 1.139e-04  Data: 0.011 (0.016)
Train: 237 [ 700/1251 ( 56%)]  Loss: 3.244 (3.21)  Time: 0.997s, 1026.68/s  (1.009s, 1015.32/s)  LR: 1.139e-04  Data: 0.010 (0.015)
Train: 237 [ 750/1251 ( 60%)]  Loss: 3.371 (3.22)  Time: 0.996s, 1027.91/s  (1.008s, 1015.65/s)  LR: 1.139e-04  Data: 0.011 (0.015)
Train: 237 [ 800/1251 ( 64%)]  Loss: 3.122 (3.21)  Time: 1.004s, 1020.27/s  (1.008s, 1015.61/s)  LR: 1.139e-04  Data: 0.012 (0.015)
Train: 237 [ 850/1251 ( 68%)]  Loss: 3.363 (3.22)  Time: 0.994s, 1029.76/s  (1.009s, 1015.17/s)  LR: 1.139e-04  Data: 0.011 (0.015)
Train: 237 [ 900/1251 ( 72%)]  Loss: 3.496 (3.24)  Time: 0.996s, 1027.60/s  (1.008s, 1015.63/s)  LR: 1.139e-04  Data: 0.012 (0.014)
Train: 237 [ 950/1251 ( 76%)]  Loss: 3.399 (3.25)  Time: 1.003s, 1020.58/s  (1.008s, 1016.04/s)  LR: 1.139e-04  Data: 0.010 (0.014)
Train: 237 [1000/1251 ( 80%)]  Loss: 2.938 (3.23)  Time: 1.002s, 1022.14/s  (1.007s, 1016.50/s)  LR: 1.139e-04  Data: 0.014 (0.014)
Train: 237 [1050/1251 ( 84%)]  Loss: 3.259 (3.23)  Time: 0.997s, 1027.35/s  (1.007s, 1016.79/s)  LR: 1.139e-04  Data: 0.012 (0.014)
Train: 237 [1100/1251 ( 88%)]  Loss: 3.143 (3.23)  Time: 0.994s, 1030.38/s  (1.007s, 1016.98/s)  LR: 1.139e-04  Data: 0.011 (0.014)
Train: 237 [1150/1251 ( 92%)]  Loss: 3.151 (3.23)  Time: 0.994s, 1030.34/s  (1.007s, 1017.01/s)  LR: 1.139e-04  Data: 0.011 (0.014)
Train: 237 [1200/1251 ( 96%)]  Loss: 3.355 (3.23)  Time: 1.001s, 1022.99/s  (1.007s, 1017.27/s)  LR: 1.139e-04  Data: 0.012 (0.014)
Train: 237 [1250/1251 (100%)]  Loss: 3.378 (3.24)  Time: 0.986s, 1038.57/s  (1.006s, 1017.69/s)  LR: 1.139e-04  Data: 0.000 (0.014)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.666 (1.666)  Loss:  0.6575 (0.6575)  Acc@1: 92.1875 (92.1875)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.245 (0.577)  Loss:  0.7752 (1.1102)  Acc@1: 85.7311 (78.8900)  Acc@5: 97.1698 (94.6120)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-236.pth.tar', 78.91400000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-237.pth.tar', 78.88999987792968)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-235.pth.tar', 78.83800003173828)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-233.pth.tar', 78.71000010742188)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-228.pth.tar', 78.70600010498048)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-232.pth.tar', 78.59799998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-231.pth.tar', 78.52999998046874)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-229.pth.tar', 78.51599994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-224.pth.tar', 78.47199992675782)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-234.pth.tar', 78.47199990234375)

Train: 238 [   0/1251 (  0%)]  Loss: 3.208 (3.21)  Time: 2.655s,  385.67/s  (2.655s,  385.67/s)  LR: 1.107e-04  Data: 1.690 (1.690)
Train: 238 [  50/1251 (  4%)]  Loss: 3.416 (3.31)  Time: 1.003s, 1020.93/s  (1.039s,  985.51/s)  LR: 1.107e-04  Data: 0.011 (0.045)
Train: 238 [ 100/1251 (  8%)]  Loss: 3.270 (3.30)  Time: 0.996s, 1028.55/s  (1.022s, 1001.60/s)  LR: 1.107e-04  Data: 0.010 (0.028)
Train: 238 [ 150/1251 ( 12%)]  Loss: 3.272 (3.29)  Time: 0.997s, 1027.11/s  (1.017s, 1007.21/s)  LR: 1.107e-04  Data: 0.011 (0.023)
Train: 238 [ 200/1251 ( 16%)]  Loss: 3.146 (3.26)  Time: 0.995s, 1029.60/s  (1.019s, 1004.71/s)  LR: 1.107e-04  Data: 0.011 (0.020)
Train: 238 [ 250/1251 ( 20%)]  Loss: 3.308 (3.27)  Time: 0.996s, 1027.85/s  (1.016s, 1007.91/s)  LR: 1.107e-04  Data: 0.012 (0.018)
Train: 238 [ 300/1251 ( 24%)]  Loss: 3.304 (3.27)  Time: 0.995s, 1028.73/s  (1.014s, 1010.24/s)  LR: 1.107e-04  Data: 0.011 (0.017)
Train: 238 [ 350/1251 ( 28%)]  Loss: 3.361 (3.29)  Time: 1.053s,  972.46/s  (1.015s, 1009.24/s)  LR: 1.107e-04  Data: 0.010 (0.016)
Train: 238 [ 400/1251 ( 32%)]  Loss: 2.890 (3.24)  Time: 0.999s, 1025.24/s  (1.015s, 1009.05/s)  LR: 1.107e-04  Data: 0.011 (0.016)
Train: 238 [ 450/1251 ( 36%)]  Loss: 3.456 (3.26)  Time: 0.997s, 1027.30/s  (1.013s, 1010.91/s)  LR: 1.107e-04  Data: 0.011 (0.015)
Train: 238 [ 500/1251 ( 40%)]  Loss: 3.224 (3.26)  Time: 1.044s,  980.62/s  (1.012s, 1011.98/s)  LR: 1.107e-04  Data: 0.012 (0.015)
Train: 238 [ 550/1251 ( 44%)]  Loss: 3.155 (3.25)  Time: 1.064s,  962.71/s  (1.013s, 1010.47/s)  LR: 1.107e-04  Data: 0.012 (0.014)
Train: 238 [ 600/1251 ( 48%)]  Loss: 3.108 (3.24)  Time: 0.994s, 1030.59/s  (1.012s, 1011.55/s)  LR: 1.107e-04  Data: 0.011 (0.014)
Train: 238 [ 650/1251 ( 52%)]  Loss: 3.358 (3.25)  Time: 0.997s, 1026.85/s  (1.011s, 1012.62/s)  LR: 1.107e-04  Data: 0.011 (0.014)
Train: 238 [ 700/1251 ( 56%)]  Loss: 2.871 (3.22)  Time: 0.996s, 1027.66/s  (1.010s, 1013.43/s)  LR: 1.107e-04  Data: 0.012 (0.014)
Train: 238 [ 750/1251 ( 60%)]  Loss: 3.163 (3.22)  Time: 0.994s, 1030.13/s  (1.011s, 1013.22/s)  LR: 1.107e-04  Data: 0.011 (0.013)
Train: 238 [ 800/1251 ( 64%)]  Loss: 2.870 (3.20)  Time: 0.996s, 1028.02/s  (1.010s, 1013.98/s)  LR: 1.107e-04  Data: 0.011 (0.013)
Train: 238 [ 850/1251 ( 68%)]  Loss: 3.263 (3.20)  Time: 0.994s, 1029.86/s  (1.009s, 1014.40/s)  LR: 1.107e-04  Data: 0.012 (0.013)
Train: 238 [ 900/1251 ( 72%)]  Loss: 3.006 (3.19)  Time: 0.998s, 1025.76/s  (1.009s, 1014.84/s)  LR: 1.107e-04  Data: 0.011 (0.013)
Train: 238 [ 950/1251 ( 76%)]  Loss: 3.385 (3.20)  Time: 0.993s, 1030.98/s  (1.008s, 1015.38/s)  LR: 1.107e-04  Data: 0.011 (0.013)
Train: 238 [1000/1251 ( 80%)]  Loss: 3.248 (3.20)  Time: 1.057s,  969.08/s  (1.008s, 1015.74/s)  LR: 1.107e-04  Data: 0.010 (0.013)
Train: 238 [1050/1251 ( 84%)]  Loss: 3.033 (3.20)  Time: 0.994s, 1030.17/s  (1.008s, 1015.98/s)  LR: 1.107e-04  Data: 0.011 (0.013)
Train: 238 [1100/1251 ( 88%)]  Loss: 3.271 (3.20)  Time: 1.002s, 1021.93/s  (1.008s, 1015.61/s)  LR: 1.107e-04  Data: 0.013 (0.013)
Train: 238 [1150/1251 ( 92%)]  Loss: 3.361 (3.21)  Time: 0.996s, 1028.21/s  (1.008s, 1016.00/s)  LR: 1.107e-04  Data: 0.012 (0.013)
Train: 238 [1200/1251 ( 96%)]  Loss: 3.175 (3.20)  Time: 1.045s,  979.86/s  (1.008s, 1016.19/s)  LR: 1.107e-04  Data: 0.011 (0.013)
Train: 238 [1250/1251 (100%)]  Loss: 3.266 (3.21)  Time: 1.017s, 1007.03/s  (1.008s, 1016.25/s)  LR: 1.107e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.666 (1.666)  Loss:  0.7039 (0.7039)  Acc@1: 91.6016 (91.6016)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.245 (0.563)  Loss:  0.8245 (1.1533)  Acc@1: 86.4387 (78.7660)  Acc@5: 97.1698 (94.5400)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-236.pth.tar', 78.91400000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-237.pth.tar', 78.88999987792968)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-235.pth.tar', 78.83800003173828)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-238.pth.tar', 78.76600005615235)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-233.pth.tar', 78.71000010742188)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-228.pth.tar', 78.70600010498048)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-232.pth.tar', 78.59799998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-231.pth.tar', 78.52999998046874)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-229.pth.tar', 78.51599994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-224.pth.tar', 78.47199992675782)

Train: 239 [   0/1251 (  0%)]  Loss: 3.068 (3.07)  Time: 2.468s,  414.87/s  (2.468s,  414.87/s)  LR: 1.076e-04  Data: 1.498 (1.498)
Train: 239 [  50/1251 (  4%)]  Loss: 3.085 (3.08)  Time: 0.994s, 1030.37/s  (1.030s,  993.80/s)  LR: 1.076e-04  Data: 0.010 (0.042)
Train: 239 [ 100/1251 (  8%)]  Loss: 3.359 (3.17)  Time: 0.994s, 1030.39/s  (1.015s, 1008.47/s)  LR: 1.076e-04  Data: 0.011 (0.027)
Train: 239 [ 150/1251 ( 12%)]  Loss: 3.007 (3.13)  Time: 1.062s,  963.95/s  (1.019s, 1004.89/s)  LR: 1.076e-04  Data: 0.011 (0.022)
Train: 239 [ 200/1251 ( 16%)]  Loss: 3.048 (3.11)  Time: 0.998s, 1026.11/s  (1.017s, 1006.91/s)  LR: 1.076e-04  Data: 0.011 (0.019)
Train: 239 [ 250/1251 ( 20%)]  Loss: 3.226 (3.13)  Time: 0.995s, 1028.71/s  (1.013s, 1010.53/s)  LR: 1.076e-04  Data: 0.012 (0.018)
Train: 239 [ 300/1251 ( 24%)]  Loss: 3.112 (3.13)  Time: 0.993s, 1030.72/s  (1.011s, 1012.85/s)  LR: 1.076e-04  Data: 0.011 (0.016)
Train: 239 [ 350/1251 ( 28%)]  Loss: 3.237 (3.14)  Time: 1.002s, 1021.45/s  (1.009s, 1014.43/s)  LR: 1.076e-04  Data: 0.012 (0.016)
Train: 239 [ 400/1251 ( 32%)]  Loss: 3.032 (3.13)  Time: 0.992s, 1032.18/s  (1.008s, 1016.11/s)  LR: 1.076e-04  Data: 0.010 (0.015)
Train: 239 [ 450/1251 ( 36%)]  Loss: 3.227 (3.14)  Time: 0.999s, 1024.99/s  (1.007s, 1017.16/s)  LR: 1.076e-04  Data: 0.011 (0.015)
Train: 239 [ 500/1251 ( 40%)]  Loss: 3.340 (3.16)  Time: 1.003s, 1020.71/s  (1.007s, 1017.33/s)  LR: 1.076e-04  Data: 0.015 (0.014)
Train: 239 [ 550/1251 ( 44%)]  Loss: 3.562 (3.19)  Time: 1.000s, 1023.77/s  (1.006s, 1017.46/s)  LR: 1.076e-04  Data: 0.011 (0.014)
Train: 239 [ 600/1251 ( 48%)]  Loss: 3.350 (3.20)  Time: 0.996s, 1027.70/s  (1.006s, 1017.82/s)  LR: 1.076e-04  Data: 0.011 (0.014)
Train: 239 [ 650/1251 ( 52%)]  Loss: 3.185 (3.20)  Time: 0.995s, 1029.00/s  (1.006s, 1017.87/s)  LR: 1.076e-04  Data: 0.012 (0.014)
Train: 239 [ 700/1251 ( 56%)]  Loss: 3.318 (3.21)  Time: 0.997s, 1027.24/s  (1.006s, 1018.24/s)  LR: 1.076e-04  Data: 0.012 (0.014)
Train: 239 [ 750/1251 ( 60%)]  Loss: 3.328 (3.22)  Time: 0.995s, 1028.67/s  (1.005s, 1018.57/s)  LR: 1.076e-04  Data: 0.012 (0.013)
Train: 239 [ 800/1251 ( 64%)]  Loss: 3.318 (3.22)  Time: 1.059s,  966.77/s  (1.006s, 1018.35/s)  LR: 1.076e-04  Data: 0.011 (0.013)
Train: 239 [ 850/1251 ( 68%)]  Loss: 3.359 (3.23)  Time: 1.061s,  964.82/s  (1.008s, 1016.22/s)  LR: 1.076e-04  Data: 0.012 (0.013)
Train: 239 [ 900/1251 ( 72%)]  Loss: 3.423 (3.24)  Time: 1.003s, 1020.88/s  (1.007s, 1016.65/s)  LR: 1.076e-04  Data: 0.010 (0.013)
Train: 239 [ 950/1251 ( 76%)]  Loss: 3.080 (3.23)  Time: 1.000s, 1024.22/s  (1.008s, 1015.96/s)  LR: 1.076e-04  Data: 0.011 (0.013)
Train: 239 [1000/1251 ( 80%)]  Loss: 3.127 (3.23)  Time: 0.998s, 1025.80/s  (1.008s, 1016.37/s)  LR: 1.076e-04  Data: 0.011 (0.013)
Train: 239 [1050/1251 ( 84%)]  Loss: 3.264 (3.23)  Time: 1.029s,  994.96/s  (1.007s, 1016.56/s)  LR: 1.076e-04  Data: 0.011 (0.013)
Train: 239 [1100/1251 ( 88%)]  Loss: 2.660 (3.20)  Time: 0.996s, 1028.43/s  (1.007s, 1016.99/s)  LR: 1.076e-04  Data: 0.011 (0.013)
Train: 239 [1150/1251 ( 92%)]  Loss: 3.441 (3.21)  Time: 1.001s, 1022.96/s  (1.007s, 1017.17/s)  LR: 1.076e-04  Data: 0.012 (0.013)
Train: 239 [1200/1251 ( 96%)]  Loss: 3.348 (3.22)  Time: 0.994s, 1029.95/s  (1.007s, 1017.19/s)  LR: 1.076e-04  Data: 0.012 (0.013)
Train: 239 [1250/1251 (100%)]  Loss: 3.266 (3.22)  Time: 0.984s, 1040.56/s  (1.006s, 1017.44/s)  LR: 1.076e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.628 (1.628)  Loss:  0.7700 (0.7700)  Acc@1: 91.5039 (91.5039)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.245 (0.567)  Loss:  0.8749 (1.1988)  Acc@1: 85.7311 (78.9420)  Acc@5: 96.8160 (94.5880)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-239.pth.tar', 78.94200000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-236.pth.tar', 78.91400000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-237.pth.tar', 78.88999987792968)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-235.pth.tar', 78.83800003173828)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-238.pth.tar', 78.76600005615235)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-233.pth.tar', 78.71000010742188)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-228.pth.tar', 78.70600010498048)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-232.pth.tar', 78.59799998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-231.pth.tar', 78.52999998046874)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-229.pth.tar', 78.51599994873047)

Train: 240 [   0/1251 (  0%)]  Loss: 3.143 (3.14)  Time: 2.617s,  391.36/s  (2.617s,  391.36/s)  LR: 1.045e-04  Data: 1.662 (1.662)
Train: 240 [  50/1251 (  4%)]  Loss: 3.374 (3.26)  Time: 0.996s, 1028.28/s  (1.033s,  991.31/s)  LR: 1.045e-04  Data: 0.011 (0.044)
Train: 240 [ 100/1251 (  8%)]  Loss: 3.258 (3.26)  Time: 1.009s, 1014.42/s  (1.015s, 1008.91/s)  LR: 1.045e-04  Data: 0.011 (0.028)
Train: 240 [ 150/1251 ( 12%)]  Loss: 3.102 (3.22)  Time: 0.996s, 1027.84/s  (1.010s, 1014.19/s)  LR: 1.045e-04  Data: 0.012 (0.022)
Train: 240 [ 200/1251 ( 16%)]  Loss: 2.850 (3.15)  Time: 0.995s, 1028.69/s  (1.014s, 1010.25/s)  LR: 1.045e-04  Data: 0.012 (0.020)
Train: 240 [ 250/1251 ( 20%)]  Loss: 3.046 (3.13)  Time: 0.998s, 1026.41/s  (1.013s, 1010.98/s)  LR: 1.045e-04  Data: 0.011 (0.018)
Train: 240 [ 300/1251 ( 24%)]  Loss: 3.543 (3.19)  Time: 0.997s, 1027.02/s  (1.011s, 1012.92/s)  LR: 1.045e-04  Data: 0.011 (0.017)
Train: 240 [ 350/1251 ( 28%)]  Loss: 3.005 (3.17)  Time: 1.007s, 1016.97/s  (1.010s, 1014.02/s)  LR: 1.045e-04  Data: 0.011 (0.016)
Train: 240 [ 400/1251 ( 32%)]  Loss: 3.420 (3.19)  Time: 0.994s, 1029.94/s  (1.009s, 1014.96/s)  LR: 1.045e-04  Data: 0.011 (0.015)
Train: 240 [ 450/1251 ( 36%)]  Loss: 2.801 (3.15)  Time: 0.995s, 1029.06/s  (1.008s, 1016.20/s)  LR: 1.045e-04  Data: 0.012 (0.015)
Train: 240 [ 500/1251 ( 40%)]  Loss: 3.188 (3.16)  Time: 0.994s, 1030.39/s  (1.007s, 1017.33/s)  LR: 1.045e-04  Data: 0.011 (0.015)
Train: 240 [ 550/1251 ( 44%)]  Loss: 3.393 (3.18)  Time: 0.999s, 1025.16/s  (1.007s, 1017.19/s)  LR: 1.045e-04  Data: 0.010 (0.014)
Train: 240 [ 600/1251 ( 48%)]  Loss: 3.148 (3.17)  Time: 1.062s,  963.95/s  (1.010s, 1013.46/s)  LR: 1.045e-04  Data: 0.011 (0.014)
Train: 240 [ 650/1251 ( 52%)]  Loss: 3.195 (3.18)  Time: 1.031s,  993.55/s  (1.011s, 1012.52/s)  LR: 1.045e-04  Data: 0.011 (0.014)
Train: 240 [ 700/1251 ( 56%)]  Loss: 3.170 (3.18)  Time: 0.996s, 1028.16/s  (1.011s, 1012.98/s)  LR: 1.045e-04  Data: 0.012 (0.014)
Train: 240 [ 750/1251 ( 60%)]  Loss: 2.715 (3.15)  Time: 0.996s, 1028.41/s  (1.010s, 1013.41/s)  LR: 1.045e-04  Data: 0.011 (0.014)
Train: 240 [ 800/1251 ( 64%)]  Loss: 3.128 (3.15)  Time: 1.063s,  963.67/s  (1.010s, 1013.64/s)  LR: 1.045e-04  Data: 0.013 (0.013)
Train: 240 [ 850/1251 ( 68%)]  Loss: 3.515 (3.17)  Time: 0.996s, 1027.84/s  (1.010s, 1013.94/s)  LR: 1.045e-04  Data: 0.011 (0.013)
Train: 240 [ 900/1251 ( 72%)]  Loss: 3.179 (3.17)  Time: 0.996s, 1027.95/s  (1.010s, 1014.33/s)  LR: 1.045e-04  Data: 0.012 (0.013)
Train: 240 [ 950/1251 ( 76%)]  Loss: 3.013 (3.16)  Time: 1.044s,  981.05/s  (1.009s, 1014.44/s)  LR: 1.045e-04  Data: 0.011 (0.013)
Train: 240 [1000/1251 ( 80%)]  Loss: 3.247 (3.16)  Time: 0.994s, 1029.86/s  (1.009s, 1014.94/s)  LR: 1.045e-04  Data: 0.010 (0.013)
Train: 240 [1050/1251 ( 84%)]  Loss: 3.388 (3.17)  Time: 0.996s, 1027.65/s  (1.008s, 1015.37/s)  LR: 1.045e-04  Data: 0.012 (0.013)
Train: 240 [1100/1251 ( 88%)]  Loss: 3.194 (3.17)  Time: 0.994s, 1029.90/s  (1.008s, 1015.77/s)  LR: 1.045e-04  Data: 0.011 (0.013)
Train: 240 [1150/1251 ( 92%)]  Loss: 3.335 (3.18)  Time: 1.012s, 1012.23/s  (1.008s, 1015.75/s)  LR: 1.045e-04  Data: 0.012 (0.013)
Train: 240 [1200/1251 ( 96%)]  Loss: 2.786 (3.17)  Time: 0.992s, 1031.78/s  (1.008s, 1015.76/s)  LR: 1.045e-04  Data: 0.010 (0.013)
Train: 240 [1250/1251 (100%)]  Loss: 3.269 (3.17)  Time: 0.986s, 1038.95/s  (1.008s, 1015.69/s)  LR: 1.045e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.742 (1.742)  Loss:  0.6919 (0.6919)  Acc@1: 91.8945 (91.8945)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.245 (0.569)  Loss:  0.7736 (1.1495)  Acc@1: 86.6745 (78.9780)  Acc@5: 97.6415 (94.6600)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-240.pth.tar', 78.9780001586914)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-239.pth.tar', 78.94200000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-236.pth.tar', 78.91400000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-237.pth.tar', 78.88999987792968)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-235.pth.tar', 78.83800003173828)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-238.pth.tar', 78.76600005615235)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-233.pth.tar', 78.71000010742188)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-228.pth.tar', 78.70600010498048)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-232.pth.tar', 78.59799998535156)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-231.pth.tar', 78.52999998046874)

Train: 241 [   0/1251 (  0%)]  Loss: 3.267 (3.27)  Time: 2.867s,  357.20/s  (2.867s,  357.20/s)  LR: 1.015e-04  Data: 1.899 (1.899)
Train: 241 [  50/1251 (  4%)]  Loss: 3.142 (3.20)  Time: 1.021s, 1002.80/s  (1.074s,  953.79/s)  LR: 1.015e-04  Data: 0.011 (0.048)
Train: 241 [ 100/1251 (  8%)]  Loss: 3.365 (3.26)  Time: 1.001s, 1022.79/s  (1.040s,  984.22/s)  LR: 1.015e-04  Data: 0.011 (0.030)
Train: 241 [ 150/1251 ( 12%)]  Loss: 3.093 (3.22)  Time: 0.997s, 1027.42/s  (1.026s,  998.16/s)  LR: 1.015e-04  Data: 0.011 (0.024)
Train: 241 [ 200/1251 ( 16%)]  Loss: 3.651 (3.30)  Time: 0.997s, 1026.84/s  (1.030s,  994.47/s)  LR: 1.015e-04  Data: 0.011 (0.021)
Train: 241 [ 250/1251 ( 20%)]  Loss: 3.409 (3.32)  Time: 1.061s,  964.69/s  (1.026s,  998.36/s)  LR: 1.015e-04  Data: 0.011 (0.019)
Train: 241 [ 300/1251 ( 24%)]  Loss: 3.333 (3.32)  Time: 0.993s, 1031.27/s  (1.025s,  999.11/s)  LR: 1.015e-04  Data: 0.011 (0.017)
Train: 241 [ 350/1251 ( 28%)]  Loss: 3.295 (3.32)  Time: 1.001s, 1022.91/s  (1.021s, 1002.62/s)  LR: 1.015e-04  Data: 0.012 (0.017)
Train: 241 [ 400/1251 ( 32%)]  Loss: 3.023 (3.29)  Time: 0.996s, 1027.76/s  (1.019s, 1005.27/s)  LR: 1.015e-04  Data: 0.011 (0.016)
Train: 241 [ 450/1251 ( 36%)]  Loss: 3.284 (3.29)  Time: 0.998s, 1025.59/s  (1.016s, 1007.48/s)  LR: 1.015e-04  Data: 0.011 (0.015)
Train: 241 [ 500/1251 ( 40%)]  Loss: 3.649 (3.32)  Time: 0.995s, 1028.64/s  (1.015s, 1009.35/s)  LR: 1.015e-04  Data: 0.010 (0.015)
Train: 241 [ 550/1251 ( 44%)]  Loss: 3.296 (3.32)  Time: 0.994s, 1030.27/s  (1.013s, 1010.63/s)  LR: 1.015e-04  Data: 0.012 (0.015)
Train: 241 [ 600/1251 ( 48%)]  Loss: 3.134 (3.30)  Time: 0.996s, 1028.06/s  (1.012s, 1011.80/s)  LR: 1.015e-04  Data: 0.011 (0.014)
Train: 241 [ 650/1251 ( 52%)]  Loss: 2.988 (3.28)  Time: 0.997s, 1027.12/s  (1.012s, 1012.32/s)  LR: 1.015e-04  Data: 0.011 (0.014)
Train: 241 [ 700/1251 ( 56%)]  Loss: 3.427 (3.29)  Time: 0.995s, 1029.44/s  (1.011s, 1012.98/s)  LR: 1.015e-04  Data: 0.011 (0.014)
Train: 241 [ 750/1251 ( 60%)]  Loss: 3.389 (3.30)  Time: 1.000s, 1024.35/s  (1.010s, 1013.66/s)  LR: 1.015e-04  Data: 0.011 (0.014)
Train: 241 [ 800/1251 ( 64%)]  Loss: 3.401 (3.30)  Time: 0.993s, 1031.19/s  (1.009s, 1014.38/s)  LR: 1.015e-04  Data: 0.011 (0.014)
Train: 241 [ 850/1251 ( 68%)]  Loss: 2.989 (3.29)  Time: 0.996s, 1028.43/s  (1.009s, 1015.06/s)  LR: 1.015e-04  Data: 0.012 (0.013)
Train: 241 [ 900/1251 ( 72%)]  Loss: 3.269 (3.28)  Time: 1.003s, 1020.87/s  (1.008s, 1015.61/s)  LR: 1.015e-04  Data: 0.013 (0.013)
Train: 241 [ 950/1251 ( 76%)]  Loss: 3.425 (3.29)  Time: 1.003s, 1020.46/s  (1.008s, 1016.02/s)  LR: 1.015e-04  Data: 0.012 (0.013)
Train: 241 [1000/1251 ( 80%)]  Loss: 3.530 (3.30)  Time: 1.001s, 1022.74/s  (1.007s, 1016.53/s)  LR: 1.015e-04  Data: 0.014 (0.013)
Train: 241 [1050/1251 ( 84%)]  Loss: 3.265 (3.30)  Time: 0.995s, 1029.17/s  (1.007s, 1017.01/s)  LR: 1.015e-04  Data: 0.010 (0.013)
Train: 241 [1100/1251 ( 88%)]  Loss: 3.362 (3.30)  Time: 0.991s, 1033.70/s  (1.007s, 1017.38/s)  LR: 1.015e-04  Data: 0.011 (0.013)
Train: 241 [1150/1251 ( 92%)]  Loss: 2.895 (3.29)  Time: 0.997s, 1027.15/s  (1.006s, 1017.76/s)  LR: 1.015e-04  Data: 0.012 (0.013)
Train: 241 [1200/1251 ( 96%)]  Loss: 3.004 (3.28)  Time: 0.994s, 1030.07/s  (1.006s, 1018.03/s)  LR: 1.015e-04  Data: 0.011 (0.013)
Train: 241 [1250/1251 (100%)]  Loss: 2.979 (3.26)  Time: 0.984s, 1040.28/s  (1.006s, 1018.26/s)  LR: 1.015e-04  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.675 (1.675)  Loss:  0.6424 (0.6424)  Acc@1: 91.1133 (91.1133)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.245 (0.579)  Loss:  0.7551 (1.0869)  Acc@1: 86.2028 (79.1040)  Acc@5: 97.5236 (94.7220)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-241.pth.tar', 79.10399995361328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-240.pth.tar', 78.9780001586914)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-239.pth.tar', 78.94200000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-236.pth.tar', 78.91400000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-237.pth.tar', 78.88999987792968)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-235.pth.tar', 78.83800003173828)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-238.pth.tar', 78.76600005615235)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-233.pth.tar', 78.71000010742188)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-228.pth.tar', 78.70600010498048)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-232.pth.tar', 78.59799998535156)

Train: 242 [   0/1251 (  0%)]  Loss: 3.516 (3.52)  Time: 2.440s,  419.61/s  (2.440s,  419.61/s)  LR: 9.853e-05  Data: 1.480 (1.480)
Train: 242 [  50/1251 (  4%)]  Loss: 2.966 (3.24)  Time: 1.000s, 1023.72/s  (1.038s,  986.96/s)  LR: 9.853e-05  Data: 0.011 (0.041)
Train: 242 [ 100/1251 (  8%)]  Loss: 3.208 (3.23)  Time: 1.035s,  989.51/s  (1.025s,  998.63/s)  LR: 9.853e-05  Data: 0.011 (0.026)
Train: 242 [ 150/1251 ( 12%)]  Loss: 2.993 (3.17)  Time: 0.995s, 1029.03/s  (1.021s, 1002.65/s)  LR: 9.853e-05  Data: 0.011 (0.021)
Train: 242 [ 200/1251 ( 16%)]  Loss: 3.179 (3.17)  Time: 0.994s, 1030.69/s  (1.015s, 1008.72/s)  LR: 9.853e-05  Data: 0.012 (0.019)
Train: 242 [ 250/1251 ( 20%)]  Loss: 3.085 (3.16)  Time: 1.001s, 1023.22/s  (1.011s, 1012.37/s)  LR: 9.853e-05  Data: 0.011 (0.017)
Train: 242 [ 300/1251 ( 24%)]  Loss: 3.490 (3.21)  Time: 1.012s, 1012.01/s  (1.009s, 1014.77/s)  LR: 9.853e-05  Data: 0.012 (0.016)
Train: 242 [ 350/1251 ( 28%)]  Loss: 3.114 (3.19)  Time: 1.007s, 1016.94/s  (1.010s, 1013.69/s)  LR: 9.853e-05  Data: 0.012 (0.015)
Train: 242 [ 400/1251 ( 32%)]  Loss: 3.155 (3.19)  Time: 1.051s,  974.17/s  (1.009s, 1015.15/s)  LR: 9.853e-05  Data: 0.010 (0.015)
Train: 242 [ 450/1251 ( 36%)]  Loss: 3.062 (3.18)  Time: 0.994s, 1030.21/s  (1.008s, 1015.63/s)  LR: 9.853e-05  Data: 0.012 (0.015)
Train: 242 [ 500/1251 ( 40%)]  Loss: 3.467 (3.20)  Time: 0.997s, 1027.06/s  (1.007s, 1016.91/s)  LR: 9.853e-05  Data: 0.011 (0.014)
Train: 242 [ 550/1251 ( 44%)]  Loss: 3.288 (3.21)  Time: 1.051s,  974.59/s  (1.006s, 1017.40/s)  LR: 9.853e-05  Data: 0.011 (0.014)
Train: 242 [ 600/1251 ( 48%)]  Loss: 3.191 (3.21)  Time: 1.002s, 1021.47/s  (1.006s, 1017.82/s)  LR: 9.853e-05  Data: 0.011 (0.014)
Train: 242 [ 650/1251 ( 52%)]  Loss: 3.337 (3.22)  Time: 1.004s, 1019.99/s  (1.005s, 1018.41/s)  LR: 9.853e-05  Data: 0.011 (0.014)
Train: 242 [ 700/1251 ( 56%)]  Loss: 3.176 (3.22)  Time: 1.004s, 1020.22/s  (1.006s, 1017.55/s)  LR: 9.853e-05  Data: 0.011 (0.013)
Train: 242 [ 750/1251 ( 60%)]  Loss: 3.156 (3.21)  Time: 0.994s, 1030.40/s  (1.006s, 1017.95/s)  LR: 9.853e-05  Data: 0.011 (0.013)
Train: 242 [ 800/1251 ( 64%)]  Loss: 2.743 (3.18)  Time: 1.005s, 1018.50/s  (1.006s, 1018.11/s)  LR: 9.853e-05  Data: 0.011 (0.013)
Train: 242 [ 850/1251 ( 68%)]  Loss: 3.102 (3.18)  Time: 1.005s, 1019.30/s  (1.006s, 1018.14/s)  LR: 9.853e-05  Data: 0.012 (0.013)
Train: 242 [ 900/1251 ( 72%)]  Loss: 3.340 (3.19)  Time: 1.001s, 1022.71/s  (1.005s, 1018.42/s)  LR: 9.853e-05  Data: 0.011 (0.013)
Train: 242 [ 950/1251 ( 76%)]  Loss: 2.805 (3.17)  Time: 0.992s, 1032.64/s  (1.005s, 1018.65/s)  LR: 9.853e-05  Data: 0.010 (0.013)
Train: 242 [1000/1251 ( 80%)]  Loss: 3.233 (3.17)  Time: 0.995s, 1029.23/s  (1.005s, 1018.75/s)  LR: 9.853e-05  Data: 0.011 (0.013)
Train: 242 [1050/1251 ( 84%)]  Loss: 3.241 (3.17)  Time: 0.992s, 1032.08/s  (1.005s, 1018.94/s)  LR: 9.853e-05  Data: 0.011 (0.013)
Train: 242 [1100/1251 ( 88%)]  Loss: 3.301 (3.18)  Time: 0.997s, 1026.85/s  (1.005s, 1019.12/s)  LR: 9.853e-05  Data: 0.011 (0.013)
Train: 242 [1150/1251 ( 92%)]  Loss: 3.095 (3.18)  Time: 0.996s, 1028.31/s  (1.005s, 1019.07/s)  LR: 9.853e-05  Data: 0.011 (0.013)
Train: 242 [1200/1251 ( 96%)]  Loss: 3.067 (3.17)  Time: 1.001s, 1023.47/s  (1.005s, 1019.27/s)  LR: 9.853e-05  Data: 0.012 (0.012)
Train: 242 [1250/1251 (100%)]  Loss: 3.297 (3.18)  Time: 0.987s, 1037.67/s  (1.004s, 1019.52/s)  LR: 9.853e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.716 (1.716)  Loss:  0.6690 (0.6690)  Acc@1: 91.3086 (91.3086)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.245 (0.568)  Loss:  0.7460 (1.1233)  Acc@1: 85.4953 (79.0560)  Acc@5: 97.6415 (94.6880)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-241.pth.tar', 79.10399995361328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-242.pth.tar', 79.05599990478515)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-240.pth.tar', 78.9780001586914)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-239.pth.tar', 78.94200000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-236.pth.tar', 78.91400000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-237.pth.tar', 78.88999987792968)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-235.pth.tar', 78.83800003173828)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-238.pth.tar', 78.76600005615235)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-233.pth.tar', 78.71000010742188)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-228.pth.tar', 78.70600010498048)

Train: 243 [   0/1251 (  0%)]  Loss: 3.285 (3.29)  Time: 2.444s,  419.05/s  (2.444s,  419.05/s)  LR: 9.560e-05  Data: 1.483 (1.483)
Train: 243 [  50/1251 (  4%)]  Loss: 3.289 (3.29)  Time: 1.001s, 1022.65/s  (1.031s,  993.45/s)  LR: 9.560e-05  Data: 0.012 (0.040)
Train: 243 [ 100/1251 (  8%)]  Loss: 3.327 (3.30)  Time: 0.996s, 1028.33/s  (1.014s, 1009.76/s)  LR: 9.560e-05  Data: 0.011 (0.026)
Train: 243 [ 150/1251 ( 12%)]  Loss: 3.092 (3.25)  Time: 0.996s, 1027.77/s  (1.009s, 1014.76/s)  LR: 9.560e-05  Data: 0.011 (0.021)
Train: 243 [ 200/1251 ( 16%)]  Loss: 3.143 (3.23)  Time: 1.056s,  969.42/s  (1.011s, 1012.68/s)  LR: 9.560e-05  Data: 0.011 (0.019)
Train: 243 [ 250/1251 ( 20%)]  Loss: 2.916 (3.18)  Time: 1.011s, 1012.88/s  (1.011s, 1013.27/s)  LR: 9.560e-05  Data: 0.011 (0.017)
Train: 243 [ 300/1251 ( 24%)]  Loss: 3.391 (3.21)  Time: 1.005s, 1018.69/s  (1.009s, 1015.10/s)  LR: 9.560e-05  Data: 0.011 (0.016)
Train: 243 [ 350/1251 ( 28%)]  Loss: 3.328 (3.22)  Time: 1.001s, 1022.94/s  (1.007s, 1016.49/s)  LR: 9.560e-05  Data: 0.010 (0.015)
Train: 243 [ 400/1251 ( 32%)]  Loss: 3.213 (3.22)  Time: 0.996s, 1027.83/s  (1.007s, 1017.28/s)  LR: 9.560e-05  Data: 0.011 (0.015)
Train: 243 [ 450/1251 ( 36%)]  Loss: 3.497 (3.25)  Time: 1.009s, 1015.01/s  (1.006s, 1017.50/s)  LR: 9.560e-05  Data: 0.011 (0.014)
Train: 243 [ 500/1251 ( 40%)]  Loss: 3.205 (3.24)  Time: 0.996s, 1027.69/s  (1.006s, 1018.08/s)  LR: 9.560e-05  Data: 0.012 (0.014)
Train: 243 [ 550/1251 ( 44%)]  Loss: 3.284 (3.25)  Time: 0.996s, 1027.76/s  (1.006s, 1017.97/s)  LR: 9.560e-05  Data: 0.011 (0.014)
Train: 243 [ 600/1251 ( 48%)]  Loss: 3.253 (3.25)  Time: 0.998s, 1026.02/s  (1.005s, 1018.49/s)  LR: 9.560e-05  Data: 0.011 (0.014)
Train: 243 [ 650/1251 ( 52%)]  Loss: 3.192 (3.24)  Time: 0.999s, 1025.32/s  (1.005s, 1018.90/s)  LR: 9.560e-05  Data: 0.010 (0.013)
Train: 243 [ 700/1251 ( 56%)]  Loss: 2.800 (3.21)  Time: 0.999s, 1025.13/s  (1.005s, 1019.36/s)  LR: 9.560e-05  Data: 0.012 (0.013)
Train: 243 [ 750/1251 ( 60%)]  Loss: 3.322 (3.22)  Time: 0.994s, 1029.91/s  (1.005s, 1019.22/s)  LR: 9.560e-05  Data: 0.011 (0.013)
Train: 243 [ 800/1251 ( 64%)]  Loss: 3.120 (3.22)  Time: 0.993s, 1031.05/s  (1.004s, 1019.57/s)  LR: 9.560e-05  Data: 0.011 (0.013)
Train: 243 [ 850/1251 ( 68%)]  Loss: 3.231 (3.22)  Time: 0.998s, 1025.80/s  (1.004s, 1019.57/s)  LR: 9.560e-05  Data: 0.011 (0.013)
Train: 243 [ 900/1251 ( 72%)]  Loss: 2.892 (3.20)  Time: 0.996s, 1027.89/s  (1.005s, 1018.98/s)  LR: 9.560e-05  Data: 0.011 (0.013)
Train: 243 [ 950/1251 ( 76%)]  Loss: 3.156 (3.20)  Time: 0.996s, 1028.00/s  (1.005s, 1019.24/s)  LR: 9.560e-05  Data: 0.011 (0.013)
Train: 243 [1000/1251 ( 80%)]  Loss: 3.470 (3.21)  Time: 0.996s, 1027.96/s  (1.004s, 1019.69/s)  LR: 9.560e-05  Data: 0.010 (0.013)
Train: 243 [1050/1251 ( 84%)]  Loss: 3.045 (3.20)  Time: 0.998s, 1025.99/s  (1.004s, 1019.93/s)  LR: 9.560e-05  Data: 0.011 (0.013)
Train: 243 [1100/1251 ( 88%)]  Loss: 3.073 (3.20)  Time: 1.028s,  995.63/s  (1.004s, 1019.88/s)  LR: 9.560e-05  Data: 0.010 (0.013)
Train: 243 [1150/1251 ( 92%)]  Loss: 3.575 (3.21)  Time: 0.994s, 1030.53/s  (1.004s, 1019.93/s)  LR: 9.560e-05  Data: 0.011 (0.012)
Train: 243 [1200/1251 ( 96%)]  Loss: 3.326 (3.22)  Time: 0.998s, 1026.30/s  (1.005s, 1019.25/s)  LR: 9.560e-05  Data: 0.011 (0.012)
Train: 243 [1250/1251 (100%)]  Loss: 2.884 (3.20)  Time: 0.983s, 1041.71/s  (1.005s, 1019.23/s)  LR: 9.560e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.643 (1.643)  Loss:  0.6805 (0.6805)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.245 (0.567)  Loss:  0.7667 (1.1260)  Acc@1: 86.0849 (79.0540)  Acc@5: 97.2877 (94.6720)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-241.pth.tar', 79.10399995361328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-242.pth.tar', 79.05599990478515)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-243.pth.tar', 79.05400003173828)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-240.pth.tar', 78.9780001586914)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-239.pth.tar', 78.94200000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-236.pth.tar', 78.91400000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-237.pth.tar', 78.88999987792968)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-235.pth.tar', 78.83800003173828)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-238.pth.tar', 78.76600005615235)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-233.pth.tar', 78.71000010742188)

Train: 244 [   0/1251 (  0%)]  Loss: 3.118 (3.12)  Time: 2.532s,  404.44/s  (2.532s,  404.44/s)  LR: 9.270e-05  Data: 1.572 (1.572)
Train: 244 [  50/1251 (  4%)]  Loss: 2.998 (3.06)  Time: 0.995s, 1029.16/s  (1.029s,  994.69/s)  LR: 9.270e-05  Data: 0.012 (0.042)
Train: 244 [ 100/1251 (  8%)]  Loss: 2.879 (3.00)  Time: 1.085s,  944.14/s  (1.027s,  997.20/s)  LR: 9.270e-05  Data: 0.011 (0.027)
Train: 244 [ 150/1251 ( 12%)]  Loss: 2.988 (3.00)  Time: 0.996s, 1028.08/s  (1.018s, 1005.67/s)  LR: 9.270e-05  Data: 0.011 (0.021)
Train: 244 [ 200/1251 ( 16%)]  Loss: 2.983 (2.99)  Time: 0.997s, 1026.59/s  (1.014s, 1010.02/s)  LR: 9.270e-05  Data: 0.011 (0.019)
Train: 244 [ 250/1251 ( 20%)]  Loss: 3.375 (3.06)  Time: 1.000s, 1024.45/s  (1.011s, 1012.92/s)  LR: 9.270e-05  Data: 0.011 (0.017)
Train: 244 [ 300/1251 ( 24%)]  Loss: 2.860 (3.03)  Time: 1.000s, 1023.89/s  (1.013s, 1011.29/s)  LR: 9.270e-05  Data: 0.011 (0.016)
Train: 244 [ 350/1251 ( 28%)]  Loss: 3.275 (3.06)  Time: 1.000s, 1023.79/s  (1.011s, 1013.22/s)  LR: 9.270e-05  Data: 0.011 (0.016)
Train: 244 [ 400/1251 ( 32%)]  Loss: 3.112 (3.07)  Time: 1.004s, 1019.85/s  (1.009s, 1014.57/s)  LR: 9.270e-05  Data: 0.012 (0.015)
Train: 244 [ 450/1251 ( 36%)]  Loss: 3.196 (3.08)  Time: 0.998s, 1026.22/s  (1.010s, 1013.94/s)  LR: 9.270e-05  Data: 0.011 (0.015)
Train: 244 [ 500/1251 ( 40%)]  Loss: 3.338 (3.10)  Time: 1.002s, 1021.74/s  (1.009s, 1014.85/s)  LR: 9.270e-05  Data: 0.012 (0.014)
Train: 244 [ 550/1251 ( 44%)]  Loss: 3.404 (3.13)  Time: 0.997s, 1026.59/s  (1.008s, 1015.70/s)  LR: 9.270e-05  Data: 0.011 (0.014)
Train: 244 [ 600/1251 ( 48%)]  Loss: 3.026 (3.12)  Time: 1.032s,  991.77/s  (1.008s, 1015.74/s)  LR: 9.270e-05  Data: 0.011 (0.014)
Train: 244 [ 650/1251 ( 52%)]  Loss: 3.079 (3.12)  Time: 1.000s, 1024.43/s  (1.008s, 1015.76/s)  LR: 9.270e-05  Data: 0.010 (0.014)
Train: 244 [ 700/1251 ( 56%)]  Loss: 3.100 (3.12)  Time: 0.995s, 1029.26/s  (1.007s, 1016.42/s)  LR: 9.270e-05  Data: 0.011 (0.013)
Train: 244 [ 750/1251 ( 60%)]  Loss: 2.838 (3.10)  Time: 0.994s, 1030.17/s  (1.007s, 1017.10/s)  LR: 9.270e-05  Data: 0.010 (0.013)
Train: 244 [ 800/1251 ( 64%)]  Loss: 3.336 (3.11)  Time: 1.002s, 1021.92/s  (1.006s, 1017.61/s)  LR: 9.270e-05  Data: 0.011 (0.013)
Train: 244 [ 850/1251 ( 68%)]  Loss: 3.064 (3.11)  Time: 1.002s, 1022.45/s  (1.006s, 1018.01/s)  LR: 9.270e-05  Data: 0.010 (0.013)
Train: 244 [ 900/1251 ( 72%)]  Loss: 3.106 (3.11)  Time: 1.015s, 1009.09/s  (1.006s, 1018.04/s)  LR: 9.270e-05  Data: 0.011 (0.013)
Train: 244 [ 950/1251 ( 76%)]  Loss: 3.082 (3.11)  Time: 0.992s, 1031.82/s  (1.005s, 1018.46/s)  LR: 9.270e-05  Data: 0.010 (0.013)
Train: 244 [1000/1251 ( 80%)]  Loss: 3.303 (3.12)  Time: 0.997s, 1027.40/s  (1.007s, 1017.38/s)  LR: 9.270e-05  Data: 0.011 (0.013)
Train: 244 [1050/1251 ( 84%)]  Loss: 3.270 (3.12)  Time: 1.056s,  969.43/s  (1.007s, 1017.08/s)  LR: 9.270e-05  Data: 0.011 (0.013)
Train: 244 [1100/1251 ( 88%)]  Loss: 3.269 (3.13)  Time: 0.998s, 1026.09/s  (1.007s, 1016.56/s)  LR: 9.270e-05  Data: 0.011 (0.013)
Train: 244 [1150/1251 ( 92%)]  Loss: 3.065 (3.13)  Time: 0.995s, 1029.60/s  (1.007s, 1016.78/s)  LR: 9.270e-05  Data: 0.011 (0.013)
Train: 244 [1200/1251 ( 96%)]  Loss: 2.957 (3.12)  Time: 0.998s, 1025.59/s  (1.007s, 1017.07/s)  LR: 9.270e-05  Data: 0.012 (0.012)
Train: 244 [1250/1251 (100%)]  Loss: 3.003 (3.12)  Time: 0.985s, 1039.96/s  (1.007s, 1017.21/s)  LR: 9.270e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.642 (1.642)  Loss:  0.5777 (0.5777)  Acc@1: 91.6016 (91.6016)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.245 (0.569)  Loss:  0.7038 (1.0456)  Acc@1: 86.4387 (79.3680)  Acc@5: 97.2877 (94.8160)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-244.pth.tar', 79.36799992675782)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-241.pth.tar', 79.10399995361328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-242.pth.tar', 79.05599990478515)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-243.pth.tar', 79.05400003173828)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-240.pth.tar', 78.9780001586914)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-239.pth.tar', 78.94200000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-236.pth.tar', 78.91400000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-237.pth.tar', 78.88999987792968)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-235.pth.tar', 78.83800003173828)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-238.pth.tar', 78.76600005615235)

Train: 245 [   0/1251 (  0%)]  Loss: 3.302 (3.30)  Time: 2.582s,  396.64/s  (2.582s,  396.64/s)  LR: 8.986e-05  Data: 1.623 (1.623)
Train: 245 [  50/1251 (  4%)]  Loss: 3.296 (3.30)  Time: 0.996s, 1028.52/s  (1.033s,  991.77/s)  LR: 8.986e-05  Data: 0.011 (0.043)
Train: 245 [ 100/1251 (  8%)]  Loss: 2.849 (3.15)  Time: 0.995s, 1029.53/s  (1.016s, 1008.11/s)  LR: 8.986e-05  Data: 0.011 (0.027)
Train: 245 [ 150/1251 ( 12%)]  Loss: 3.419 (3.22)  Time: 1.002s, 1021.55/s  (1.015s, 1009.08/s)  LR: 8.986e-05  Data: 0.013 (0.022)
Train: 245 [ 200/1251 ( 16%)]  Loss: 3.438 (3.26)  Time: 1.001s, 1022.89/s  (1.012s, 1011.38/s)  LR: 8.986e-05  Data: 0.011 (0.019)
Train: 245 [ 250/1251 ( 20%)]  Loss: 3.584 (3.31)  Time: 1.000s, 1023.60/s  (1.014s, 1009.95/s)  LR: 8.986e-05  Data: 0.011 (0.018)
Train: 245 [ 300/1251 ( 24%)]  Loss: 3.158 (3.29)  Time: 0.992s, 1032.38/s  (1.012s, 1012.35/s)  LR: 8.986e-05  Data: 0.011 (0.017)
Train: 245 [ 350/1251 ( 28%)]  Loss: 3.278 (3.29)  Time: 1.000s, 1024.21/s  (1.011s, 1012.43/s)  LR: 8.986e-05  Data: 0.011 (0.016)
Train: 245 [ 400/1251 ( 32%)]  Loss: 3.457 (3.31)  Time: 1.001s, 1023.38/s  (1.010s, 1013.90/s)  LR: 8.986e-05  Data: 0.012 (0.015)
Train: 245 [ 450/1251 ( 36%)]  Loss: 3.434 (3.32)  Time: 0.995s, 1028.68/s  (1.009s, 1015.26/s)  LR: 8.986e-05  Data: 0.012 (0.015)
Train: 245 [ 500/1251 ( 40%)]  Loss: 3.351 (3.32)  Time: 1.039s,  985.86/s  (1.008s, 1015.57/s)  LR: 8.986e-05  Data: 0.012 (0.014)
Train: 245 [ 550/1251 ( 44%)]  Loss: 3.230 (3.32)  Time: 1.001s, 1023.07/s  (1.009s, 1014.74/s)  LR: 8.986e-05  Data: 0.012 (0.014)
Train: 245 [ 600/1251 ( 48%)]  Loss: 3.067 (3.30)  Time: 0.997s, 1026.72/s  (1.008s, 1015.66/s)  LR: 8.986e-05  Data: 0.011 (0.014)
Train: 245 [ 650/1251 ( 52%)]  Loss: 3.180 (3.29)  Time: 0.994s, 1030.39/s  (1.008s, 1016.32/s)  LR: 8.986e-05  Data: 0.012 (0.014)
Train: 245 [ 700/1251 ( 56%)]  Loss: 3.292 (3.29)  Time: 0.996s, 1028.31/s  (1.007s, 1016.95/s)  LR: 8.986e-05  Data: 0.011 (0.013)
Train: 245 [ 750/1251 ( 60%)]  Loss: 3.243 (3.29)  Time: 0.998s, 1026.02/s  (1.007s, 1016.60/s)  LR: 8.986e-05  Data: 0.011 (0.013)
Train: 245 [ 800/1251 ( 64%)]  Loss: 2.944 (3.27)  Time: 0.996s, 1027.96/s  (1.007s, 1016.55/s)  LR: 8.986e-05  Data: 0.010 (0.013)
Train: 245 [ 850/1251 ( 68%)]  Loss: 3.171 (3.26)  Time: 1.009s, 1014.44/s  (1.007s, 1016.68/s)  LR: 8.986e-05  Data: 0.012 (0.013)
Train: 245 [ 900/1251 ( 72%)]  Loss: 3.238 (3.26)  Time: 0.998s, 1026.17/s  (1.007s, 1017.00/s)  LR: 8.986e-05  Data: 0.011 (0.013)
Train: 245 [ 950/1251 ( 76%)]  Loss: 3.363 (3.26)  Time: 1.005s, 1018.46/s  (1.007s, 1016.83/s)  LR: 8.986e-05  Data: 0.010 (0.013)
Train: 245 [1000/1251 ( 80%)]  Loss: 2.710 (3.24)  Time: 1.004s, 1020.39/s  (1.007s, 1017.35/s)  LR: 8.986e-05  Data: 0.013 (0.013)
Train: 245 [1050/1251 ( 84%)]  Loss: 2.995 (3.23)  Time: 0.988s, 1036.87/s  (1.006s, 1017.71/s)  LR: 8.986e-05  Data: 0.010 (0.013)
Train: 245 [1100/1251 ( 88%)]  Loss: 2.999 (3.22)  Time: 0.998s, 1025.83/s  (1.006s, 1017.91/s)  LR: 8.986e-05  Data: 0.012 (0.013)
Train: 245 [1150/1251 ( 92%)]  Loss: 3.251 (3.22)  Time: 0.999s, 1024.95/s  (1.007s, 1017.12/s)  LR: 8.986e-05  Data: 0.013 (0.013)
Train: 245 [1200/1251 ( 96%)]  Loss: 3.118 (3.21)  Time: 0.993s, 1030.91/s  (1.006s, 1017.41/s)  LR: 8.986e-05  Data: 0.010 (0.012)
Train: 245 [1250/1251 (100%)]  Loss: 3.473 (3.22)  Time: 0.985s, 1039.93/s  (1.006s, 1017.71/s)  LR: 8.986e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.597 (1.597)  Loss:  0.6873 (0.6873)  Acc@1: 91.6992 (91.6992)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.245 (0.573)  Loss:  0.7785 (1.1740)  Acc@1: 86.6745 (79.1200)  Acc@5: 97.2877 (94.6440)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-244.pth.tar', 79.36799992675782)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-245.pth.tar', 79.12000002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-241.pth.tar', 79.10399995361328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-242.pth.tar', 79.05599990478515)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-243.pth.tar', 79.05400003173828)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-240.pth.tar', 78.9780001586914)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-239.pth.tar', 78.94200000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-236.pth.tar', 78.91400000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-237.pth.tar', 78.88999987792968)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-235.pth.tar', 78.83800003173828)

Train: 246 [   0/1251 (  0%)]  Loss: 3.159 (3.16)  Time: 4.154s,  246.48/s  (4.154s,  246.48/s)  LR: 8.706e-05  Data: 2.977 (2.977)
Train: 246 [  50/1251 (  4%)]  Loss: 3.266 (3.21)  Time: 0.998s, 1025.76/s  (1.065s,  961.33/s)  LR: 8.706e-05  Data: 0.011 (0.070)
Train: 246 [ 100/1251 (  8%)]  Loss: 3.281 (3.24)  Time: 1.056s,  969.26/s  (1.035s,  989.07/s)  LR: 8.706e-05  Data: 0.011 (0.041)
Train: 246 [ 150/1251 ( 12%)]  Loss: 3.077 (3.20)  Time: 1.001s, 1023.05/s  (1.026s,  998.34/s)  LR: 8.706e-05  Data: 0.011 (0.031)
Train: 246 [ 200/1251 ( 16%)]  Loss: 3.492 (3.26)  Time: 0.995s, 1028.99/s  (1.025s,  998.81/s)  LR: 8.706e-05  Data: 0.010 (0.026)
Train: 246 [ 250/1251 ( 20%)]  Loss: 3.114 (3.23)  Time: 1.004s, 1020.02/s  (1.020s, 1003.43/s)  LR: 8.706e-05  Data: 0.012 (0.023)
Train: 246 [ 300/1251 ( 24%)]  Loss: 3.147 (3.22)  Time: 1.001s, 1022.51/s  (1.017s, 1006.53/s)  LR: 8.706e-05  Data: 0.012 (0.021)
Train: 246 [ 350/1251 ( 28%)]  Loss: 3.424 (3.25)  Time: 1.003s, 1020.94/s  (1.015s, 1009.15/s)  LR: 8.706e-05  Data: 0.010 (0.020)
Train: 246 [ 400/1251 ( 32%)]  Loss: 3.232 (3.24)  Time: 1.000s, 1024.33/s  (1.013s, 1010.90/s)  LR: 8.706e-05  Data: 0.011 (0.019)
Train: 246 [ 450/1251 ( 36%)]  Loss: 3.361 (3.26)  Time: 0.997s, 1027.43/s  (1.012s, 1011.86/s)  LR: 8.706e-05  Data: 0.010 (0.018)
Train: 246 [ 500/1251 ( 40%)]  Loss: 3.112 (3.24)  Time: 0.997s, 1026.76/s  (1.011s, 1013.03/s)  LR: 8.706e-05  Data: 0.011 (0.017)
Train: 246 [ 550/1251 ( 44%)]  Loss: 3.337 (3.25)  Time: 1.000s, 1024.12/s  (1.010s, 1013.50/s)  LR: 8.706e-05  Data: 0.010 (0.017)
Train: 246 [ 600/1251 ( 48%)]  Loss: 3.053 (3.23)  Time: 1.056s,  970.13/s  (1.010s, 1014.07/s)  LR: 8.706e-05  Data: 0.012 (0.016)
Train: 246 [ 650/1251 ( 52%)]  Loss: 3.350 (3.24)  Time: 0.997s, 1026.74/s  (1.009s, 1014.40/s)  LR: 8.706e-05  Data: 0.011 (0.016)
Train: 246 [ 700/1251 ( 56%)]  Loss: 3.068 (3.23)  Time: 1.002s, 1022.42/s  (1.009s, 1015.29/s)  LR: 8.706e-05  Data: 0.011 (0.015)
Train: 246 [ 750/1251 ( 60%)]  Loss: 3.143 (3.23)  Time: 0.997s, 1027.38/s  (1.008s, 1015.39/s)  LR: 8.706e-05  Data: 0.011 (0.015)
Train: 246 [ 800/1251 ( 64%)]  Loss: 3.181 (3.22)  Time: 1.037s,  987.23/s  (1.008s, 1015.92/s)  LR: 8.706e-05  Data: 0.011 (0.015)
Train: 246 [ 850/1251 ( 68%)]  Loss: 2.902 (3.21)  Time: 1.001s, 1022.86/s  (1.008s, 1016.17/s)  LR: 8.706e-05  Data: 0.012 (0.015)
Train: 246 [ 900/1251 ( 72%)]  Loss: 3.136 (3.20)  Time: 1.001s, 1022.51/s  (1.007s, 1016.50/s)  LR: 8.706e-05  Data: 0.011 (0.014)
Train: 246 [ 950/1251 ( 76%)]  Loss: 2.843 (3.18)  Time: 1.005s, 1019.18/s  (1.007s, 1016.93/s)  LR: 8.706e-05  Data: 0.011 (0.014)
Train: 246 [1000/1251 ( 80%)]  Loss: 3.319 (3.19)  Time: 1.066s,  960.82/s  (1.007s, 1016.40/s)  LR: 8.706e-05  Data: 0.011 (0.014)
Train: 246 [1050/1251 ( 84%)]  Loss: 3.340 (3.20)  Time: 0.999s, 1025.17/s  (1.007s, 1016.71/s)  LR: 8.706e-05  Data: 0.012 (0.014)
Train: 246 [1100/1251 ( 88%)]  Loss: 2.966 (3.19)  Time: 0.994s, 1029.70/s  (1.007s, 1017.12/s)  LR: 8.706e-05  Data: 0.010 (0.014)
Train: 246 [1150/1251 ( 92%)]  Loss: 3.041 (3.18)  Time: 1.051s,  974.60/s  (1.007s, 1017.34/s)  LR: 8.706e-05  Data: 0.011 (0.014)
Train: 246 [1200/1251 ( 96%)]  Loss: 3.537 (3.20)  Time: 0.996s, 1028.21/s  (1.007s, 1016.95/s)  LR: 8.706e-05  Data: 0.011 (0.014)
Train: 246 [1250/1251 (100%)]  Loss: 3.159 (3.19)  Time: 0.983s, 1042.08/s  (1.007s, 1016.77/s)  LR: 8.706e-05  Data: 0.000 (0.014)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.624 (1.624)  Loss:  0.6947 (0.6947)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.245 (0.571)  Loss:  0.8052 (1.1247)  Acc@1: 86.4387 (79.1400)  Acc@5: 97.1698 (94.7840)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-244.pth.tar', 79.36799992675782)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-246.pth.tar', 79.13999992675781)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-245.pth.tar', 79.12000002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-241.pth.tar', 79.10399995361328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-242.pth.tar', 79.05599990478515)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-243.pth.tar', 79.05400003173828)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-240.pth.tar', 78.9780001586914)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-239.pth.tar', 78.94200000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-236.pth.tar', 78.91400000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-237.pth.tar', 78.88999987792968)

Train: 247 [   0/1251 (  0%)]  Loss: 3.342 (3.34)  Time: 2.432s,  421.13/s  (2.432s,  421.13/s)  LR: 8.430e-05  Data: 1.472 (1.472)
Train: 247 [  50/1251 (  4%)]  Loss: 3.255 (3.30)  Time: 0.997s, 1026.93/s  (1.033s,  991.72/s)  LR: 8.430e-05  Data: 0.011 (0.040)
Train: 247 [ 100/1251 (  8%)]  Loss: 2.975 (3.19)  Time: 1.015s, 1008.93/s  (1.017s, 1007.10/s)  LR: 8.430e-05  Data: 0.011 (0.026)
Train: 247 [ 150/1251 ( 12%)]  Loss: 3.170 (3.19)  Time: 0.994s, 1030.62/s  (1.013s, 1011.28/s)  LR: 8.430e-05  Data: 0.010 (0.021)
Train: 247 [ 200/1251 ( 16%)]  Loss: 3.276 (3.20)  Time: 1.002s, 1021.66/s  (1.010s, 1013.87/s)  LR: 8.430e-05  Data: 0.015 (0.019)
Train: 247 [ 250/1251 ( 20%)]  Loss: 3.034 (3.18)  Time: 0.999s, 1024.98/s  (1.016s, 1008.06/s)  LR: 8.430e-05  Data: 0.011 (0.017)
Train: 247 [ 300/1251 ( 24%)]  Loss: 3.410 (3.21)  Time: 1.031s,  993.46/s  (1.015s, 1009.19/s)  LR: 8.430e-05  Data: 0.011 (0.016)
Train: 247 [ 350/1251 ( 28%)]  Loss: 2.866 (3.17)  Time: 1.057s,  969.16/s  (1.016s, 1007.49/s)  LR: 8.430e-05  Data: 0.010 (0.015)
Train: 247 [ 400/1251 ( 32%)]  Loss: 3.017 (3.15)  Time: 0.995s, 1028.64/s  (1.018s, 1006.37/s)  LR: 8.430e-05  Data: 0.012 (0.015)
Train: 247 [ 450/1251 ( 36%)]  Loss: 3.200 (3.15)  Time: 0.995s, 1029.59/s  (1.016s, 1008.02/s)  LR: 8.430e-05  Data: 0.011 (0.014)
Train: 247 [ 500/1251 ( 40%)]  Loss: 3.069 (3.15)  Time: 0.999s, 1025.45/s  (1.014s, 1009.62/s)  LR: 8.430e-05  Data: 0.011 (0.014)
Train: 247 [ 550/1251 ( 44%)]  Loss: 3.098 (3.14)  Time: 0.996s, 1028.04/s  (1.013s, 1011.13/s)  LR: 8.430e-05  Data: 0.011 (0.014)
Train: 247 [ 600/1251 ( 48%)]  Loss: 3.036 (3.13)  Time: 1.000s, 1023.68/s  (1.012s, 1012.22/s)  LR: 8.430e-05  Data: 0.012 (0.014)
Train: 247 [ 650/1251 ( 52%)]  Loss: 3.156 (3.14)  Time: 1.002s, 1021.80/s  (1.011s, 1013.06/s)  LR: 8.430e-05  Data: 0.011 (0.013)
Train: 247 [ 700/1251 ( 56%)]  Loss: 3.123 (3.13)  Time: 0.995s, 1029.00/s  (1.010s, 1013.87/s)  LR: 8.430e-05  Data: 0.011 (0.013)
Train: 247 [ 750/1251 ( 60%)]  Loss: 3.275 (3.14)  Time: 1.000s, 1023.94/s  (1.010s, 1014.28/s)  LR: 8.430e-05  Data: 0.010 (0.013)
Train: 247 [ 800/1251 ( 64%)]  Loss: 3.059 (3.14)  Time: 1.002s, 1021.94/s  (1.009s, 1014.73/s)  LR: 8.430e-05  Data: 0.011 (0.013)
Train: 247 [ 850/1251 ( 68%)]  Loss: 3.142 (3.14)  Time: 1.002s, 1021.86/s  (1.009s, 1015.29/s)  LR: 8.430e-05  Data: 0.012 (0.013)
Train: 247 [ 900/1251 ( 72%)]  Loss: 3.430 (3.15)  Time: 1.005s, 1018.75/s  (1.008s, 1015.87/s)  LR: 8.430e-05  Data: 0.012 (0.013)
Train: 247 [ 950/1251 ( 76%)]  Loss: 3.143 (3.15)  Time: 1.001s, 1022.90/s  (1.008s, 1016.24/s)  LR: 8.430e-05  Data: 0.011 (0.013)
Train: 247 [1000/1251 ( 80%)]  Loss: 3.246 (3.16)  Time: 1.034s,  990.72/s  (1.008s, 1016.05/s)  LR: 8.430e-05  Data: 0.011 (0.013)
Train: 247 [1050/1251 ( 84%)]  Loss: 3.015 (3.15)  Time: 0.995s, 1029.45/s  (1.008s, 1015.41/s)  LR: 8.430e-05  Data: 0.011 (0.013)
Train: 247 [1100/1251 ( 88%)]  Loss: 3.008 (3.15)  Time: 0.995s, 1028.81/s  (1.008s, 1015.62/s)  LR: 8.430e-05  Data: 0.011 (0.013)
Train: 247 [1150/1251 ( 92%)]  Loss: 3.346 (3.15)  Time: 1.000s, 1023.56/s  (1.008s, 1016.06/s)  LR: 8.430e-05  Data: 0.011 (0.012)
Train: 247 [1200/1251 ( 96%)]  Loss: 2.898 (3.14)  Time: 0.992s, 1032.50/s  (1.007s, 1016.43/s)  LR: 8.430e-05  Data: 0.011 (0.012)
Train: 247 [1250/1251 (100%)]  Loss: 3.270 (3.15)  Time: 0.983s, 1042.07/s  (1.007s, 1016.42/s)  LR: 8.430e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.647 (1.647)  Loss:  0.6662 (0.6662)  Acc@1: 92.2852 (92.2852)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.245 (0.584)  Loss:  0.7985 (1.1017)  Acc@1: 85.4953 (79.3680)  Acc@5: 97.4057 (94.8040)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-247.pth.tar', 79.36800003417969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-244.pth.tar', 79.36799992675782)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-246.pth.tar', 79.13999992675781)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-245.pth.tar', 79.12000002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-241.pth.tar', 79.10399995361328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-242.pth.tar', 79.05599990478515)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-243.pth.tar', 79.05400003173828)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-240.pth.tar', 78.9780001586914)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-239.pth.tar', 78.94200000732422)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-236.pth.tar', 78.91400000732422)

Train: 248 [   0/1251 (  0%)]  Loss: 2.962 (2.96)  Time: 2.554s,  401.02/s  (2.554s,  401.02/s)  LR: 8.159e-05  Data: 1.589 (1.589)
Train: 248 [  50/1251 (  4%)]  Loss: 3.174 (3.07)  Time: 1.033s,  991.72/s  (1.051s,  974.47/s)  LR: 8.159e-05  Data: 0.011 (0.045)
Train: 248 [ 100/1251 (  8%)]  Loss: 3.109 (3.08)  Time: 0.997s, 1026.69/s  (1.028s,  996.09/s)  LR: 8.159e-05  Data: 0.011 (0.028)
Train: 248 [ 150/1251 ( 12%)]  Loss: 3.576 (3.21)  Time: 1.004s, 1020.15/s  (1.019s, 1004.87/s)  LR: 8.159e-05  Data: 0.011 (0.022)
Train: 248 [ 200/1251 ( 16%)]  Loss: 2.847 (3.13)  Time: 1.063s,  963.57/s  (1.023s, 1001.31/s)  LR: 8.159e-05  Data: 0.011 (0.020)
Train: 248 [ 250/1251 ( 20%)]  Loss: 3.268 (3.16)  Time: 1.001s, 1023.08/s  (1.021s, 1003.14/s)  LR: 8.159e-05  Data: 0.012 (0.018)
Train: 248 [ 300/1251 ( 24%)]  Loss: 3.107 (3.15)  Time: 0.996s, 1027.75/s  (1.018s, 1006.00/s)  LR: 8.159e-05  Data: 0.011 (0.017)
Train: 248 [ 350/1251 ( 28%)]  Loss: 3.268 (3.16)  Time: 0.995s, 1029.48/s  (1.015s, 1008.57/s)  LR: 8.159e-05  Data: 0.011 (0.016)
Train: 248 [ 400/1251 ( 32%)]  Loss: 2.969 (3.14)  Time: 1.009s, 1014.89/s  (1.013s, 1010.54/s)  LR: 8.159e-05  Data: 0.011 (0.015)
Train: 248 [ 450/1251 ( 36%)]  Loss: 3.417 (3.17)  Time: 0.994s, 1029.96/s  (1.012s, 1012.28/s)  LR: 8.159e-05  Data: 0.011 (0.015)
Train: 248 [ 500/1251 ( 40%)]  Loss: 3.294 (3.18)  Time: 1.039s,  985.82/s  (1.011s, 1013.00/s)  LR: 8.159e-05  Data: 0.012 (0.015)
Train: 248 [ 550/1251 ( 44%)]  Loss: 3.299 (3.19)  Time: 0.997s, 1026.86/s  (1.012s, 1011.95/s)  LR: 8.159e-05  Data: 0.013 (0.014)
Train: 248 [ 600/1251 ( 48%)]  Loss: 3.089 (3.18)  Time: 1.005s, 1019.34/s  (1.011s, 1012.93/s)  LR: 8.159e-05  Data: 0.012 (0.014)
Train: 248 [ 650/1251 ( 52%)]  Loss: 3.333 (3.19)  Time: 1.010s, 1014.35/s  (1.011s, 1012.80/s)  LR: 8.159e-05  Data: 0.011 (0.014)
Train: 248 [ 700/1251 ( 56%)]  Loss: 2.910 (3.17)  Time: 0.992s, 1032.26/s  (1.010s, 1013.57/s)  LR: 8.159e-05  Data: 0.010 (0.014)
Train: 248 [ 750/1251 ( 60%)]  Loss: 3.118 (3.17)  Time: 0.996s, 1028.22/s  (1.010s, 1014.05/s)  LR: 8.159e-05  Data: 0.011 (0.013)
Train: 248 [ 800/1251 ( 64%)]  Loss: 3.311 (3.18)  Time: 0.994s, 1029.71/s  (1.009s, 1014.66/s)  LR: 8.159e-05  Data: 0.011 (0.013)
Train: 248 [ 850/1251 ( 68%)]  Loss: 3.343 (3.19)  Time: 1.006s, 1017.40/s  (1.009s, 1015.31/s)  LR: 8.159e-05  Data: 0.011 (0.013)
Train: 248 [ 900/1251 ( 72%)]  Loss: 3.129 (3.19)  Time: 1.000s, 1023.69/s  (1.008s, 1015.83/s)  LR: 8.159e-05  Data: 0.010 (0.013)
Train: 248 [ 950/1251 ( 76%)]  Loss: 3.036 (3.18)  Time: 0.998s, 1026.35/s  (1.008s, 1016.19/s)  LR: 8.159e-05  Data: 0.011 (0.013)
Train: 248 [1000/1251 ( 80%)]  Loss: 3.135 (3.18)  Time: 0.998s, 1026.03/s  (1.007s, 1016.49/s)  LR: 8.159e-05  Data: 0.011 (0.013)
Train: 248 [1050/1251 ( 84%)]  Loss: 2.851 (3.16)  Time: 0.998s, 1026.30/s  (1.007s, 1016.45/s)  LR: 8.159e-05  Data: 0.011 (0.013)
Train: 248 [1100/1251 ( 88%)]  Loss: 2.545 (3.13)  Time: 1.009s, 1014.97/s  (1.007s, 1016.79/s)  LR: 8.159e-05  Data: 0.011 (0.013)
Train: 248 [1150/1251 ( 92%)]  Loss: 3.134 (3.13)  Time: 1.057s,  969.14/s  (1.007s, 1017.02/s)  LR: 8.159e-05  Data: 0.011 (0.013)
Train: 248 [1200/1251 ( 96%)]  Loss: 3.037 (3.13)  Time: 0.994s, 1029.76/s  (1.007s, 1017.34/s)  LR: 8.159e-05  Data: 0.011 (0.013)
Train: 248 [1250/1251 (100%)]  Loss: 3.396 (3.14)  Time: 1.032s,  991.78/s  (1.006s, 1017.67/s)  LR: 8.159e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.619 (1.619)  Loss:  0.6398 (0.6398)  Acc@1: 91.5039 (91.5039)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.245 (0.584)  Loss:  0.7162 (1.0676)  Acc@1: 86.9104 (79.5040)  Acc@5: 97.2877 (94.8800)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-248.pth.tar', 79.50399987304688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-247.pth.tar', 79.36800003417969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-244.pth.tar', 79.36799992675782)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-246.pth.tar', 79.13999992675781)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-245.pth.tar', 79.12000002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-241.pth.tar', 79.10399995361328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-242.pth.tar', 79.05599990478515)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-243.pth.tar', 79.05400003173828)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-240.pth.tar', 78.9780001586914)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-239.pth.tar', 78.94200000732422)

Train: 249 [   0/1251 (  0%)]  Loss: 3.314 (3.31)  Time: 2.401s,  426.46/s  (2.401s,  426.46/s)  LR: 7.893e-05  Data: 1.436 (1.436)
Train: 249 [  50/1251 (  4%)]  Loss: 3.175 (3.24)  Time: 0.997s, 1027.42/s  (1.031s,  993.53/s)  LR: 7.893e-05  Data: 0.011 (0.040)
Train: 249 [ 100/1251 (  8%)]  Loss: 3.181 (3.22)  Time: 1.065s,  961.52/s  (1.035s,  989.30/s)  LR: 7.893e-05  Data: 0.011 (0.026)
Train: 249 [ 150/1251 ( 12%)]  Loss: 3.031 (3.18)  Time: 1.007s, 1016.63/s  (1.029s,  994.91/s)  LR: 7.893e-05  Data: 0.011 (0.021)
Train: 249 [ 200/1251 ( 16%)]  Loss: 3.324 (3.20)  Time: 0.998s, 1025.80/s  (1.022s, 1001.49/s)  LR: 7.893e-05  Data: 0.012 (0.019)
Train: 249 [ 250/1251 ( 20%)]  Loss: 3.164 (3.20)  Time: 1.006s, 1017.74/s  (1.018s, 1005.65/s)  LR: 7.893e-05  Data: 0.011 (0.017)
Train: 249 [ 300/1251 ( 24%)]  Loss: 3.390 (3.23)  Time: 0.992s, 1032.46/s  (1.015s, 1008.85/s)  LR: 7.893e-05  Data: 0.011 (0.016)
Train: 249 [ 350/1251 ( 28%)]  Loss: 3.177 (3.22)  Time: 0.999s, 1024.72/s  (1.013s, 1011.00/s)  LR: 7.893e-05  Data: 0.011 (0.015)
Train: 249 [ 400/1251 ( 32%)]  Loss: 2.952 (3.19)  Time: 0.997s, 1027.57/s  (1.011s, 1012.53/s)  LR: 7.893e-05  Data: 0.011 (0.015)
Train: 249 [ 450/1251 ( 36%)]  Loss: 3.139 (3.18)  Time: 1.000s, 1023.95/s  (1.010s, 1013.59/s)  LR: 7.893e-05  Data: 0.011 (0.015)
Train: 249 [ 500/1251 ( 40%)]  Loss: 3.094 (3.18)  Time: 0.996s, 1028.41/s  (1.009s, 1014.36/s)  LR: 7.893e-05  Data: 0.011 (0.014)
Train: 249 [ 550/1251 ( 44%)]  Loss: 3.241 (3.18)  Time: 1.000s, 1023.65/s  (1.009s, 1015.31/s)  LR: 7.893e-05  Data: 0.011 (0.014)
Train: 249 [ 600/1251 ( 48%)]  Loss: 3.495 (3.21)  Time: 0.998s, 1025.68/s  (1.008s, 1016.16/s)  LR: 7.893e-05  Data: 0.010 (0.014)
Train: 249 [ 650/1251 ( 52%)]  Loss: 3.147 (3.20)  Time: 0.999s, 1024.56/s  (1.008s, 1015.82/s)  LR: 7.893e-05  Data: 0.011 (0.013)
Train: 249 [ 700/1251 ( 56%)]  Loss: 3.264 (3.21)  Time: 0.990s, 1034.45/s  (1.007s, 1016.45/s)  LR: 7.893e-05  Data: 0.010 (0.013)
Train: 249 [ 750/1251 ( 60%)]  Loss: 2.918 (3.19)  Time: 1.007s, 1017.36/s  (1.007s, 1016.98/s)  LR: 7.893e-05  Data: 0.011 (0.013)
Train: 249 [ 800/1251 ( 64%)]  Loss: 3.174 (3.19)  Time: 0.994s, 1030.18/s  (1.006s, 1017.42/s)  LR: 7.893e-05  Data: 0.011 (0.013)
Train: 249 [ 850/1251 ( 68%)]  Loss: 3.331 (3.19)  Time: 0.996s, 1028.24/s  (1.006s, 1017.60/s)  LR: 7.893e-05  Data: 0.011 (0.013)
Train: 249 [ 900/1251 ( 72%)]  Loss: 3.423 (3.21)  Time: 1.001s, 1022.47/s  (1.006s, 1017.86/s)  LR: 7.893e-05  Data: 0.011 (0.013)
Train: 249 [ 950/1251 ( 76%)]  Loss: 3.152 (3.20)  Time: 0.997s, 1027.03/s  (1.006s, 1018.08/s)  LR: 7.893e-05  Data: 0.011 (0.013)
Train: 249 [1000/1251 ( 80%)]  Loss: 3.081 (3.20)  Time: 0.995s, 1028.75/s  (1.006s, 1018.33/s)  LR: 7.893e-05  Data: 0.011 (0.013)
Train: 249 [1050/1251 ( 84%)]  Loss: 2.967 (3.19)  Time: 0.996s, 1028.30/s  (1.005s, 1018.49/s)  LR: 7.893e-05  Data: 0.011 (0.013)
Train: 249 [1100/1251 ( 88%)]  Loss: 2.939 (3.18)  Time: 0.997s, 1026.59/s  (1.005s, 1018.53/s)  LR: 7.893e-05  Data: 0.011 (0.013)
Train: 249 [1150/1251 ( 92%)]  Loss: 3.252 (3.18)  Time: 1.000s, 1023.85/s  (1.006s, 1018.17/s)  LR: 7.893e-05  Data: 0.011 (0.012)
Train: 249 [1200/1251 ( 96%)]  Loss: 3.252 (3.18)  Time: 1.071s,  955.80/s  (1.006s, 1017.81/s)  LR: 7.893e-05  Data: 0.011 (0.012)
Train: 249 [1250/1251 (100%)]  Loss: 3.425 (3.19)  Time: 1.024s,  999.60/s  (1.006s, 1017.85/s)  LR: 7.893e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.611 (1.611)  Loss:  0.6082 (0.6082)  Acc@1: 91.6016 (91.6016)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.245 (0.575)  Loss:  0.7278 (1.0691)  Acc@1: 86.0849 (79.3080)  Acc@5: 97.6415 (94.8140)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-248.pth.tar', 79.50399987304688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-247.pth.tar', 79.36800003417969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-244.pth.tar', 79.36799992675782)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-249.pth.tar', 79.30800003173829)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-246.pth.tar', 79.13999992675781)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-245.pth.tar', 79.12000002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-241.pth.tar', 79.10399995361328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-242.pth.tar', 79.05599990478515)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-243.pth.tar', 79.05400003173828)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-240.pth.tar', 78.9780001586914)

Train: 250 [   0/1251 (  0%)]  Loss: 3.282 (3.28)  Time: 2.626s,  389.96/s  (2.626s,  389.96/s)  LR: 7.632e-05  Data: 1.665 (1.665)
Train: 250 [  50/1251 (  4%)]  Loss: 3.332 (3.31)  Time: 1.031s,  993.18/s  (1.041s,  984.10/s)  LR: 7.632e-05  Data: 0.011 (0.044)
Train: 250 [ 100/1251 (  8%)]  Loss: 3.367 (3.33)  Time: 0.996s, 1028.47/s  (1.024s, 1000.05/s)  LR: 7.632e-05  Data: 0.011 (0.028)
Train: 250 [ 150/1251 ( 12%)]  Loss: 3.273 (3.31)  Time: 1.028s,  995.69/s  (1.018s, 1006.38/s)  LR: 7.632e-05  Data: 0.011 (0.022)
Train: 250 [ 200/1251 ( 16%)]  Loss: 3.110 (3.27)  Time: 0.996s, 1027.81/s  (1.016s, 1007.96/s)  LR: 7.632e-05  Data: 0.012 (0.020)
Train: 250 [ 250/1251 ( 20%)]  Loss: 3.193 (3.26)  Time: 1.002s, 1021.82/s  (1.013s, 1010.59/s)  LR: 7.632e-05  Data: 0.012 (0.018)
Train: 250 [ 300/1251 ( 24%)]  Loss: 3.083 (3.23)  Time: 0.995s, 1029.26/s  (1.011s, 1012.38/s)  LR: 7.632e-05  Data: 0.010 (0.017)
Train: 250 [ 350/1251 ( 28%)]  Loss: 3.081 (3.22)  Time: 1.000s, 1024.05/s  (1.012s, 1012.14/s)  LR: 7.632e-05  Data: 0.011 (0.016)
Train: 250 [ 400/1251 ( 32%)]  Loss: 3.058 (3.20)  Time: 1.002s, 1022.05/s  (1.010s, 1013.43/s)  LR: 7.632e-05  Data: 0.011 (0.015)
Train: 250 [ 450/1251 ( 36%)]  Loss: 3.239 (3.20)  Time: 0.998s, 1026.15/s  (1.010s, 1014.35/s)  LR: 7.632e-05  Data: 0.011 (0.015)
Train: 250 [ 500/1251 ( 40%)]  Loss: 3.071 (3.19)  Time: 0.995s, 1029.64/s  (1.008s, 1015.51/s)  LR: 7.632e-05  Data: 0.012 (0.015)
Train: 250 [ 550/1251 ( 44%)]  Loss: 2.832 (3.16)  Time: 0.998s, 1025.74/s  (1.008s, 1016.34/s)  LR: 7.632e-05  Data: 0.011 (0.014)
Train: 250 [ 600/1251 ( 48%)]  Loss: 3.337 (3.17)  Time: 0.994s, 1030.26/s  (1.007s, 1017.04/s)  LR: 7.632e-05  Data: 0.011 (0.014)
Train: 250 [ 650/1251 ( 52%)]  Loss: 3.364 (3.19)  Time: 0.994s, 1030.68/s  (1.006s, 1017.58/s)  LR: 7.632e-05  Data: 0.011 (0.014)
Train: 250 [ 700/1251 ( 56%)]  Loss: 3.056 (3.18)  Time: 0.995s, 1029.13/s  (1.006s, 1017.91/s)  LR: 7.632e-05  Data: 0.011 (0.014)
Train: 250 [ 750/1251 ( 60%)]  Loss: 3.087 (3.17)  Time: 0.996s, 1027.69/s  (1.006s, 1018.32/s)  LR: 7.632e-05  Data: 0.011 (0.013)
Train: 250 [ 800/1251 ( 64%)]  Loss: 3.470 (3.19)  Time: 0.996s, 1027.70/s  (1.005s, 1018.64/s)  LR: 7.632e-05  Data: 0.011 (0.013)
Train: 250 [ 850/1251 ( 68%)]  Loss: 2.998 (3.18)  Time: 0.994s, 1030.50/s  (1.005s, 1018.59/s)  LR: 7.632e-05  Data: 0.012 (0.013)
Train: 250 [ 900/1251 ( 72%)]  Loss: 3.103 (3.18)  Time: 0.999s, 1025.20/s  (1.005s, 1018.54/s)  LR: 7.632e-05  Data: 0.012 (0.013)
Train: 250 [ 950/1251 ( 76%)]  Loss: 3.290 (3.18)  Time: 1.028s,  996.14/s  (1.006s, 1017.91/s)  LR: 7.632e-05  Data: 0.011 (0.013)
Train: 250 [1000/1251 ( 80%)]  Loss: 3.114 (3.18)  Time: 0.995s, 1029.41/s  (1.006s, 1018.35/s)  LR: 7.632e-05  Data: 0.012 (0.013)
Train: 250 [1050/1251 ( 84%)]  Loss: 3.238 (3.18)  Time: 0.994s, 1030.03/s  (1.005s, 1018.67/s)  LR: 7.632e-05  Data: 0.011 (0.013)
Train: 250 [1100/1251 ( 88%)]  Loss: 3.020 (3.17)  Time: 1.006s, 1018.27/s  (1.005s, 1019.00/s)  LR: 7.632e-05  Data: 0.010 (0.013)
Train: 250 [1150/1251 ( 92%)]  Loss: 3.242 (3.18)  Time: 0.998s, 1026.34/s  (1.005s, 1019.16/s)  LR: 7.632e-05  Data: 0.012 (0.013)
Train: 250 [1200/1251 ( 96%)]  Loss: 3.154 (3.18)  Time: 0.997s, 1027.03/s  (1.006s, 1018.30/s)  LR: 7.632e-05  Data: 0.012 (0.013)
Train: 250 [1250/1251 (100%)]  Loss: 2.923 (3.17)  Time: 1.014s, 1009.69/s  (1.006s, 1018.17/s)  LR: 7.632e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.636 (1.636)  Loss:  0.6461 (0.6461)  Acc@1: 91.6016 (91.6016)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.245 (0.578)  Loss:  0.7590 (1.1073)  Acc@1: 86.9104 (79.3980)  Acc@5: 97.1698 (94.8060)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-248.pth.tar', 79.50399987304688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-250.pth.tar', 79.39800013183594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-247.pth.tar', 79.36800003417969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-244.pth.tar', 79.36799992675782)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-249.pth.tar', 79.30800003173829)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-246.pth.tar', 79.13999992675781)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-245.pth.tar', 79.12000002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-241.pth.tar', 79.10399995361328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-242.pth.tar', 79.05599990478515)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-243.pth.tar', 79.05400003173828)

Train: 251 [   0/1251 (  0%)]  Loss: 3.019 (3.02)  Time: 2.362s,  433.49/s  (2.362s,  433.49/s)  LR: 7.375e-05  Data: 1.413 (1.413)
Train: 251 [  50/1251 (  4%)]  Loss: 3.468 (3.24)  Time: 0.993s, 1031.59/s  (1.028s,  995.87/s)  LR: 7.375e-05  Data: 0.011 (0.041)
Train: 251 [ 100/1251 (  8%)]  Loss: 3.278 (3.25)  Time: 1.005s, 1019.34/s  (1.014s, 1010.06/s)  LR: 7.375e-05  Data: 0.013 (0.026)
Train: 251 [ 150/1251 ( 12%)]  Loss: 2.992 (3.19)  Time: 1.036s,  988.23/s  (1.012s, 1012.36/s)  LR: 7.375e-05  Data: 0.011 (0.021)
Train: 251 [ 200/1251 ( 16%)]  Loss: 3.054 (3.16)  Time: 0.996s, 1028.08/s  (1.015s, 1008.59/s)  LR: 7.375e-05  Data: 0.011 (0.019)
Train: 251 [ 250/1251 ( 20%)]  Loss: 3.091 (3.15)  Time: 0.994s, 1030.51/s  (1.013s, 1010.80/s)  LR: 7.375e-05  Data: 0.010 (0.017)
Train: 251 [ 300/1251 ( 24%)]  Loss: 3.090 (3.14)  Time: 0.994s, 1030.16/s  (1.011s, 1013.13/s)  LR: 7.375e-05  Data: 0.012 (0.016)
Train: 251 [ 350/1251 ( 28%)]  Loss: 3.175 (3.15)  Time: 0.993s, 1030.91/s  (1.009s, 1015.01/s)  LR: 7.375e-05  Data: 0.010 (0.016)
Train: 251 [ 400/1251 ( 32%)]  Loss: 3.110 (3.14)  Time: 1.001s, 1023.46/s  (1.009s, 1014.66/s)  LR: 7.375e-05  Data: 0.011 (0.015)
Train: 251 [ 450/1251 ( 36%)]  Loss: 3.134 (3.14)  Time: 0.998s, 1025.89/s  (1.008s, 1015.83/s)  LR: 7.375e-05  Data: 0.012 (0.015)
Train: 251 [ 500/1251 ( 40%)]  Loss: 2.836 (3.11)  Time: 0.994s, 1030.57/s  (1.007s, 1016.44/s)  LR: 7.375e-05  Data: 0.010 (0.014)
Train: 251 [ 550/1251 ( 44%)]  Loss: 3.394 (3.14)  Time: 1.002s, 1021.58/s  (1.007s, 1016.88/s)  LR: 7.375e-05  Data: 0.013 (0.014)
Train: 251 [ 600/1251 ( 48%)]  Loss: 2.974 (3.12)  Time: 0.997s, 1027.22/s  (1.007s, 1016.71/s)  LR: 7.375e-05  Data: 0.011 (0.014)
Train: 251 [ 650/1251 ( 52%)]  Loss: 3.311 (3.14)  Time: 0.993s, 1031.11/s  (1.007s, 1017.08/s)  LR: 7.375e-05  Data: 0.010 (0.014)
Train: 251 [ 700/1251 ( 56%)]  Loss: 2.932 (3.12)  Time: 0.996s, 1027.94/s  (1.006s, 1017.43/s)  LR: 7.375e-05  Data: 0.011 (0.013)
Train: 251 [ 750/1251 ( 60%)]  Loss: 3.237 (3.13)  Time: 0.996s, 1028.18/s  (1.007s, 1016.85/s)  LR: 7.375e-05  Data: 0.012 (0.013)
Train: 251 [ 800/1251 ( 64%)]  Loss: 2.846 (3.11)  Time: 0.995s, 1028.77/s  (1.007s, 1017.22/s)  LR: 7.375e-05  Data: 0.011 (0.013)
Train: 251 [ 850/1251 ( 68%)]  Loss: 3.194 (3.12)  Time: 1.032s,  991.85/s  (1.006s, 1017.39/s)  LR: 7.375e-05  Data: 0.012 (0.013)
Train: 251 [ 900/1251 ( 72%)]  Loss: 3.565 (3.14)  Time: 0.998s, 1026.19/s  (1.006s, 1017.86/s)  LR: 7.375e-05  Data: 0.012 (0.013)
Train: 251 [ 950/1251 ( 76%)]  Loss: 2.963 (3.13)  Time: 1.000s, 1024.19/s  (1.006s, 1017.86/s)  LR: 7.375e-05  Data: 0.011 (0.013)
Train: 251 [1000/1251 ( 80%)]  Loss: 3.373 (3.14)  Time: 0.994s, 1029.78/s  (1.006s, 1018.28/s)  LR: 7.375e-05  Data: 0.011 (0.013)
Train: 251 [1050/1251 ( 84%)]  Loss: 3.482 (3.16)  Time: 0.997s, 1027.33/s  (1.005s, 1018.45/s)  LR: 7.375e-05  Data: 0.011 (0.013)
Train: 251 [1100/1251 ( 88%)]  Loss: 3.531 (3.18)  Time: 0.997s, 1027.59/s  (1.006s, 1018.36/s)  LR: 7.375e-05  Data: 0.011 (0.013)
Train: 251 [1150/1251 ( 92%)]  Loss: 2.980 (3.17)  Time: 0.997s, 1027.49/s  (1.005s, 1018.49/s)  LR: 7.375e-05  Data: 0.011 (0.013)
Train: 251 [1200/1251 ( 96%)]  Loss: 3.031 (3.16)  Time: 1.046s,  978.91/s  (1.005s, 1018.59/s)  LR: 7.375e-05  Data: 0.011 (0.012)
Train: 251 [1250/1251 (100%)]  Loss: 3.083 (3.16)  Time: 1.025s,  999.37/s  (1.005s, 1018.74/s)  LR: 7.375e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.608 (1.608)  Loss:  0.7004 (0.7004)  Acc@1: 91.6992 (91.6992)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.245 (0.568)  Loss:  0.8488 (1.1774)  Acc@1: 86.6745 (79.3200)  Acc@5: 97.7594 (94.8220)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-248.pth.tar', 79.50399987304688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-250.pth.tar', 79.39800013183594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-247.pth.tar', 79.36800003417969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-244.pth.tar', 79.36799992675782)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-251.pth.tar', 79.32000002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-249.pth.tar', 79.30800003173829)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-246.pth.tar', 79.13999992675781)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-245.pth.tar', 79.12000002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-241.pth.tar', 79.10399995361328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-242.pth.tar', 79.05599990478515)

Train: 252 [   0/1251 (  0%)]  Loss: 2.987 (2.99)  Time: 2.529s,  404.92/s  (2.529s,  404.92/s)  LR: 7.123e-05  Data: 1.567 (1.567)
Train: 252 [  50/1251 (  4%)]  Loss: 2.785 (2.89)  Time: 1.039s,  985.97/s  (1.036s,  988.75/s)  LR: 7.123e-05  Data: 0.011 (0.044)
Train: 252 [ 100/1251 (  8%)]  Loss: 3.468 (3.08)  Time: 1.054s,  971.17/s  (1.022s, 1001.50/s)  LR: 7.123e-05  Data: 0.010 (0.028)
Train: 252 [ 150/1251 ( 12%)]  Loss: 3.446 (3.17)  Time: 0.998s, 1026.40/s  (1.021s, 1002.82/s)  LR: 7.123e-05  Data: 0.010 (0.022)
Train: 252 [ 200/1251 ( 16%)]  Loss: 3.166 (3.17)  Time: 0.995s, 1028.65/s  (1.017s, 1006.86/s)  LR: 7.123e-05  Data: 0.011 (0.019)
Train: 252 [ 250/1251 ( 20%)]  Loss: 3.040 (3.15)  Time: 0.995s, 1028.75/s  (1.015s, 1008.99/s)  LR: 7.123e-05  Data: 0.012 (0.018)
Train: 252 [ 300/1251 ( 24%)]  Loss: 2.956 (3.12)  Time: 1.010s, 1014.27/s  (1.013s, 1010.78/s)  LR: 7.123e-05  Data: 0.012 (0.017)
Train: 252 [ 350/1251 ( 28%)]  Loss: 3.300 (3.14)  Time: 0.993s, 1031.73/s  (1.011s, 1012.71/s)  LR: 7.123e-05  Data: 0.011 (0.016)
Train: 252 [ 400/1251 ( 32%)]  Loss: 3.672 (3.20)  Time: 0.996s, 1028.39/s  (1.010s, 1014.00/s)  LR: 7.123e-05  Data: 0.012 (0.015)
Train: 252 [ 450/1251 ( 36%)]  Loss: 3.068 (3.19)  Time: 0.992s, 1032.46/s  (1.009s, 1014.95/s)  LR: 7.123e-05  Data: 0.011 (0.015)
Train: 252 [ 500/1251 ( 40%)]  Loss: 3.362 (3.20)  Time: 0.997s, 1027.35/s  (1.008s, 1015.77/s)  LR: 7.123e-05  Data: 0.011 (0.014)
Train: 252 [ 550/1251 ( 44%)]  Loss: 3.130 (3.20)  Time: 0.996s, 1028.14/s  (1.007s, 1016.73/s)  LR: 7.123e-05  Data: 0.011 (0.014)
Train: 252 [ 600/1251 ( 48%)]  Loss: 3.418 (3.22)  Time: 1.000s, 1024.32/s  (1.007s, 1016.77/s)  LR: 7.123e-05  Data: 0.011 (0.014)
Train: 252 [ 650/1251 ( 52%)]  Loss: 3.010 (3.20)  Time: 0.997s, 1027.42/s  (1.007s, 1017.36/s)  LR: 7.123e-05  Data: 0.012 (0.014)
Train: 252 [ 700/1251 ( 56%)]  Loss: 3.187 (3.20)  Time: 0.998s, 1025.76/s  (1.006s, 1017.70/s)  LR: 7.123e-05  Data: 0.011 (0.014)
Train: 252 [ 750/1251 ( 60%)]  Loss: 3.264 (3.20)  Time: 1.002s, 1022.31/s  (1.006s, 1018.11/s)  LR: 7.123e-05  Data: 0.013 (0.013)
Train: 252 [ 800/1251 ( 64%)]  Loss: 2.815 (3.18)  Time: 1.004s, 1020.23/s  (1.006s, 1018.39/s)  LR: 7.123e-05  Data: 0.014 (0.013)
Train: 252 [ 850/1251 ( 68%)]  Loss: 3.321 (3.19)  Time: 0.998s, 1025.93/s  (1.005s, 1018.62/s)  LR: 7.123e-05  Data: 0.011 (0.013)
Train: 252 [ 900/1251 ( 72%)]  Loss: 3.418 (3.20)  Time: 0.998s, 1025.77/s  (1.005s, 1018.92/s)  LR: 7.123e-05  Data: 0.012 (0.013)
Train: 252 [ 950/1251 ( 76%)]  Loss: 3.294 (3.21)  Time: 0.997s, 1027.57/s  (1.005s, 1019.06/s)  LR: 7.123e-05  Data: 0.012 (0.013)
Train: 252 [1000/1251 ( 80%)]  Loss: 2.938 (3.19)  Time: 1.019s, 1004.92/s  (1.005s, 1018.91/s)  LR: 7.123e-05  Data: 0.012 (0.013)
Train: 252 [1050/1251 ( 84%)]  Loss: 3.231 (3.19)  Time: 0.996s, 1028.26/s  (1.005s, 1019.17/s)  LR: 7.123e-05  Data: 0.012 (0.013)
Train: 252 [1100/1251 ( 88%)]  Loss: 3.204 (3.19)  Time: 1.003s, 1020.99/s  (1.005s, 1019.39/s)  LR: 7.123e-05  Data: 0.010 (0.013)
Train: 252 [1150/1251 ( 92%)]  Loss: 2.895 (3.18)  Time: 0.996s, 1028.58/s  (1.004s, 1019.60/s)  LR: 7.123e-05  Data: 0.011 (0.013)
Train: 252 [1200/1251 ( 96%)]  Loss: 3.021 (3.18)  Time: 0.995s, 1029.60/s  (1.004s, 1019.47/s)  LR: 7.123e-05  Data: 0.011 (0.013)
Train: 252 [1250/1251 (100%)]  Loss: 3.224 (3.18)  Time: 0.996s, 1028.14/s  (1.004s, 1019.68/s)  LR: 7.123e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.677 (1.677)  Loss:  0.6705 (0.6705)  Acc@1: 91.6016 (91.6016)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.246 (0.569)  Loss:  0.7909 (1.1174)  Acc@1: 85.9670 (79.5820)  Acc@5: 96.8160 (94.8660)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-252.pth.tar', 79.58199998046875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-248.pth.tar', 79.50399987304688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-250.pth.tar', 79.39800013183594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-247.pth.tar', 79.36800003417969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-244.pth.tar', 79.36799992675782)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-251.pth.tar', 79.32000002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-249.pth.tar', 79.30800003173829)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-246.pth.tar', 79.13999992675781)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-245.pth.tar', 79.12000002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-241.pth.tar', 79.10399995361328)

Train: 253 [   0/1251 (  0%)]  Loss: 3.319 (3.32)  Time: 2.525s,  405.55/s  (2.525s,  405.55/s)  LR: 6.875e-05  Data: 1.566 (1.566)
Train: 253 [  50/1251 (  4%)]  Loss: 3.239 (3.28)  Time: 1.001s, 1023.33/s  (1.036s,  988.55/s)  LR: 6.875e-05  Data: 0.011 (0.042)
Train: 253 [ 100/1251 (  8%)]  Loss: 2.706 (3.09)  Time: 0.997s, 1027.51/s  (1.025s,  999.16/s)  LR: 6.875e-05  Data: 0.011 (0.027)
Train: 253 [ 150/1251 ( 12%)]  Loss: 3.092 (3.09)  Time: 1.033s,  991.27/s  (1.019s, 1004.84/s)  LR: 6.875e-05  Data: 0.011 (0.022)
Train: 253 [ 200/1251 ( 16%)]  Loss: 3.197 (3.11)  Time: 0.993s, 1031.22/s  (1.016s, 1007.81/s)  LR: 6.875e-05  Data: 0.011 (0.019)
Train: 253 [ 250/1251 ( 20%)]  Loss: 3.295 (3.14)  Time: 1.000s, 1024.16/s  (1.013s, 1011.14/s)  LR: 6.875e-05  Data: 0.011 (0.017)
Train: 253 [ 300/1251 ( 24%)]  Loss: 3.315 (3.17)  Time: 0.996s, 1027.95/s  (1.010s, 1013.63/s)  LR: 6.875e-05  Data: 0.012 (0.016)
Train: 253 [ 350/1251 ( 28%)]  Loss: 3.221 (3.17)  Time: 1.000s, 1024.10/s  (1.009s, 1014.82/s)  LR: 6.875e-05  Data: 0.011 (0.016)
Train: 253 [ 400/1251 ( 32%)]  Loss: 3.111 (3.17)  Time: 0.993s, 1030.71/s  (1.008s, 1016.01/s)  LR: 6.875e-05  Data: 0.012 (0.015)
Train: 253 [ 450/1251 ( 36%)]  Loss: 3.263 (3.18)  Time: 0.995s, 1028.72/s  (1.007s, 1017.00/s)  LR: 6.875e-05  Data: 0.011 (0.015)
Train: 253 [ 500/1251 ( 40%)]  Loss: 3.131 (3.17)  Time: 1.006s, 1018.17/s  (1.007s, 1017.12/s)  LR: 6.875e-05  Data: 0.011 (0.014)
Train: 253 [ 550/1251 ( 44%)]  Loss: 3.053 (3.16)  Time: 1.060s,  966.48/s  (1.006s, 1017.66/s)  LR: 6.875e-05  Data: 0.011 (0.014)
Train: 253 [ 600/1251 ( 48%)]  Loss: 3.043 (3.15)  Time: 0.997s, 1026.65/s  (1.006s, 1017.92/s)  LR: 6.875e-05  Data: 0.012 (0.014)
Train: 253 [ 650/1251 ( 52%)]  Loss: 3.364 (3.17)  Time: 0.994s, 1030.58/s  (1.005s, 1018.58/s)  LR: 6.875e-05  Data: 0.011 (0.014)
Train: 253 [ 700/1251 ( 56%)]  Loss: 3.052 (3.16)  Time: 0.994s, 1030.51/s  (1.005s, 1018.71/s)  LR: 6.875e-05  Data: 0.011 (0.013)
Train: 253 [ 750/1251 ( 60%)]  Loss: 3.094 (3.16)  Time: 1.012s, 1012.33/s  (1.006s, 1018.25/s)  LR: 6.875e-05  Data: 0.011 (0.013)
Train: 253 [ 800/1251 ( 64%)]  Loss: 3.106 (3.15)  Time: 0.999s, 1024.58/s  (1.005s, 1018.52/s)  LR: 6.875e-05  Data: 0.012 (0.013)
Train: 253 [ 850/1251 ( 68%)]  Loss: 2.992 (3.14)  Time: 0.997s, 1026.94/s  (1.005s, 1018.88/s)  LR: 6.875e-05  Data: 0.011 (0.013)
Train: 253 [ 900/1251 ( 72%)]  Loss: 3.270 (3.15)  Time: 0.998s, 1026.30/s  (1.005s, 1019.30/s)  LR: 6.875e-05  Data: 0.011 (0.013)
Train: 253 [ 950/1251 ( 76%)]  Loss: 3.022 (3.14)  Time: 0.994s, 1030.27/s  (1.005s, 1019.38/s)  LR: 6.875e-05  Data: 0.011 (0.013)
Train: 253 [1000/1251 ( 80%)]  Loss: 2.917 (3.13)  Time: 0.994s, 1029.88/s  (1.004s, 1019.58/s)  LR: 6.875e-05  Data: 0.011 (0.013)
Train: 253 [1050/1251 ( 84%)]  Loss: 3.330 (3.14)  Time: 1.006s, 1018.39/s  (1.004s, 1019.64/s)  LR: 6.875e-05  Data: 0.011 (0.013)
Train: 253 [1100/1251 ( 88%)]  Loss: 3.395 (3.15)  Time: 0.995s, 1029.17/s  (1.004s, 1019.85/s)  LR: 6.875e-05  Data: 0.012 (0.013)
Train: 253 [1150/1251 ( 92%)]  Loss: 2.841 (3.14)  Time: 1.000s, 1024.00/s  (1.004s, 1019.86/s)  LR: 6.875e-05  Data: 0.012 (0.013)
Train: 253 [1200/1251 ( 96%)]  Loss: 2.941 (3.13)  Time: 0.997s, 1027.23/s  (1.004s, 1020.03/s)  LR: 6.875e-05  Data: 0.011 (0.012)
Train: 253 [1250/1251 (100%)]  Loss: 3.185 (3.13)  Time: 0.986s, 1038.49/s  (1.004s, 1020.11/s)  LR: 6.875e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.671 (1.671)  Loss:  0.6970 (0.6970)  Acc@1: 91.3086 (91.3086)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.245 (0.568)  Loss:  0.7886 (1.1405)  Acc@1: 86.5566 (79.5040)  Acc@5: 97.7594 (94.8660)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-252.pth.tar', 79.58199998046875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-253.pth.tar', 79.50399997802734)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-248.pth.tar', 79.50399987304688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-250.pth.tar', 79.39800013183594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-247.pth.tar', 79.36800003417969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-244.pth.tar', 79.36799992675782)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-251.pth.tar', 79.32000002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-249.pth.tar', 79.30800003173829)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-246.pth.tar', 79.13999992675781)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-245.pth.tar', 79.12000002929688)

Train: 254 [   0/1251 (  0%)]  Loss: 3.163 (3.16)  Time: 2.511s,  407.79/s  (2.511s,  407.79/s)  LR: 6.633e-05  Data: 1.557 (1.557)
Train: 254 [  50/1251 (  4%)]  Loss: 2.997 (3.08)  Time: 0.996s, 1028.58/s  (1.029s,  995.57/s)  LR: 6.633e-05  Data: 0.011 (0.042)
Train: 254 [ 100/1251 (  8%)]  Loss: 3.491 (3.22)  Time: 0.992s, 1032.10/s  (1.014s, 1009.97/s)  LR: 6.633e-05  Data: 0.010 (0.026)
Train: 254 [ 150/1251 ( 12%)]  Loss: 3.244 (3.22)  Time: 0.999s, 1025.09/s  (1.011s, 1012.53/s)  LR: 6.633e-05  Data: 0.011 (0.022)
Train: 254 [ 200/1251 ( 16%)]  Loss: 2.965 (3.17)  Time: 0.996s, 1027.84/s  (1.016s, 1008.35/s)  LR: 6.633e-05  Data: 0.012 (0.019)
Train: 254 [ 250/1251 ( 20%)]  Loss: 3.428 (3.21)  Time: 1.034s,  989.94/s  (1.014s, 1010.15/s)  LR: 6.633e-05  Data: 0.011 (0.017)
Train: 254 [ 300/1251 ( 24%)]  Loss: 3.133 (3.20)  Time: 1.012s, 1011.81/s  (1.012s, 1011.68/s)  LR: 6.633e-05  Data: 0.010 (0.016)
Train: 254 [ 350/1251 ( 28%)]  Loss: 2.814 (3.15)  Time: 1.003s, 1021.32/s  (1.010s, 1013.39/s)  LR: 6.633e-05  Data: 0.012 (0.016)
Train: 254 [ 400/1251 ( 32%)]  Loss: 3.409 (3.18)  Time: 1.000s, 1023.78/s  (1.009s, 1014.63/s)  LR: 6.633e-05  Data: 0.013 (0.015)
Train: 254 [ 450/1251 ( 36%)]  Loss: 3.067 (3.17)  Time: 0.998s, 1025.61/s  (1.008s, 1015.69/s)  LR: 6.633e-05  Data: 0.011 (0.015)
Train: 254 [ 500/1251 ( 40%)]  Loss: 2.908 (3.15)  Time: 0.996s, 1028.22/s  (1.007s, 1016.79/s)  LR: 6.633e-05  Data: 0.011 (0.014)
Train: 254 [ 550/1251 ( 44%)]  Loss: 2.785 (3.12)  Time: 0.995s, 1029.25/s  (1.006s, 1017.55/s)  LR: 6.633e-05  Data: 0.011 (0.014)
Train: 254 [ 600/1251 ( 48%)]  Loss: 3.123 (3.12)  Time: 0.997s, 1026.58/s  (1.006s, 1017.97/s)  LR: 6.633e-05  Data: 0.011 (0.014)
Train: 254 [ 650/1251 ( 52%)]  Loss: 3.134 (3.12)  Time: 0.995s, 1029.03/s  (1.005s, 1018.48/s)  LR: 6.633e-05  Data: 0.012 (0.014)
Train: 254 [ 700/1251 ( 56%)]  Loss: 2.966 (3.11)  Time: 0.997s, 1026.77/s  (1.005s, 1018.79/s)  LR: 6.633e-05  Data: 0.011 (0.013)
Train: 254 [ 750/1251 ( 60%)]  Loss: 3.124 (3.11)  Time: 0.999s, 1025.33/s  (1.005s, 1018.68/s)  LR: 6.633e-05  Data: 0.011 (0.013)
Train: 254 [ 800/1251 ( 64%)]  Loss: 3.375 (3.13)  Time: 1.003s, 1021.00/s  (1.005s, 1018.89/s)  LR: 6.633e-05  Data: 0.014 (0.013)
Train: 254 [ 850/1251 ( 68%)]  Loss: 3.057 (3.12)  Time: 0.996s, 1028.32/s  (1.005s, 1019.25/s)  LR: 6.633e-05  Data: 0.011 (0.013)
Train: 254 [ 900/1251 ( 72%)]  Loss: 3.306 (3.13)  Time: 0.996s, 1028.27/s  (1.004s, 1019.42/s)  LR: 6.633e-05  Data: 0.011 (0.013)
Train: 254 [ 950/1251 ( 76%)]  Loss: 3.121 (3.13)  Time: 0.995s, 1029.63/s  (1.004s, 1019.65/s)  LR: 6.633e-05  Data: 0.012 (0.013)
Train: 254 [1000/1251 ( 80%)]  Loss: 3.097 (3.13)  Time: 0.994s, 1030.15/s  (1.004s, 1019.80/s)  LR: 6.633e-05  Data: 0.011 (0.013)
Train: 254 [1050/1251 ( 84%)]  Loss: 3.282 (3.14)  Time: 0.997s, 1026.92/s  (1.004s, 1019.99/s)  LR: 6.633e-05  Data: 0.011 (0.013)
Train: 254 [1100/1251 ( 88%)]  Loss: 2.822 (3.12)  Time: 0.997s, 1026.88/s  (1.004s, 1019.88/s)  LR: 6.633e-05  Data: 0.011 (0.013)
Train: 254 [1150/1251 ( 92%)]  Loss: 3.328 (3.13)  Time: 0.994s, 1029.71/s  (1.004s, 1020.00/s)  LR: 6.633e-05  Data: 0.011 (0.013)
Train: 254 [1200/1251 ( 96%)]  Loss: 2.928 (3.12)  Time: 0.995s, 1029.50/s  (1.004s, 1020.18/s)  LR: 6.633e-05  Data: 0.011 (0.012)
Train: 254 [1250/1251 (100%)]  Loss: 2.823 (3.11)  Time: 1.022s, 1001.67/s  (1.004s, 1020.22/s)  LR: 6.633e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.600 (1.600)  Loss:  0.6304 (0.6304)  Acc@1: 91.3086 (91.3086)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.245 (0.569)  Loss:  0.6839 (1.0742)  Acc@1: 87.1462 (79.5420)  Acc@5: 97.7594 (94.8280)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-252.pth.tar', 79.58199998046875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-254.pth.tar', 79.54199997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-253.pth.tar', 79.50399997802734)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-248.pth.tar', 79.50399987304688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-250.pth.tar', 79.39800013183594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-247.pth.tar', 79.36800003417969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-244.pth.tar', 79.36799992675782)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-251.pth.tar', 79.32000002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-249.pth.tar', 79.30800003173829)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-246.pth.tar', 79.13999992675781)

Train: 255 [   0/1251 (  0%)]  Loss: 3.123 (3.12)  Time: 4.372s,  234.24/s  (4.372s,  234.24/s)  LR: 6.395e-05  Data: 3.057 (3.057)
Train: 255 [  50/1251 (  4%)]  Loss: 3.045 (3.08)  Time: 1.003s, 1021.26/s  (1.068s,  959.22/s)  LR: 6.395e-05  Data: 0.015 (0.071)
Train: 255 [ 100/1251 (  8%)]  Loss: 3.444 (3.20)  Time: 1.001s, 1023.29/s  (1.036s,  988.48/s)  LR: 6.395e-05  Data: 0.011 (0.041)
Train: 255 [ 150/1251 ( 12%)]  Loss: 3.477 (3.27)  Time: 1.003s, 1021.43/s  (1.028s,  995.63/s)  LR: 6.395e-05  Data: 0.011 (0.031)
Train: 255 [ 200/1251 ( 16%)]  Loss: 3.119 (3.24)  Time: 0.996s, 1028.42/s  (1.025s,  999.44/s)  LR: 6.395e-05  Data: 0.012 (0.026)
Train: 255 [ 250/1251 ( 20%)]  Loss: 3.190 (3.23)  Time: 0.994s, 1029.74/s  (1.023s, 1000.94/s)  LR: 6.395e-05  Data: 0.010 (0.023)
Train: 255 [ 300/1251 ( 24%)]  Loss: 2.871 (3.18)  Time: 0.995s, 1028.75/s  (1.021s, 1002.74/s)  LR: 6.395e-05  Data: 0.011 (0.021)
Train: 255 [ 350/1251 ( 28%)]  Loss: 3.094 (3.17)  Time: 1.032s,  991.87/s  (1.021s, 1003.07/s)  LR: 6.395e-05  Data: 0.011 (0.020)
Train: 255 [ 400/1251 ( 32%)]  Loss: 3.181 (3.17)  Time: 0.996s, 1027.95/s  (1.019s, 1004.78/s)  LR: 6.395e-05  Data: 0.012 (0.019)
Train: 255 [ 450/1251 ( 36%)]  Loss: 3.028 (3.16)  Time: 0.996s, 1027.77/s  (1.018s, 1006.11/s)  LR: 6.395e-05  Data: 0.011 (0.018)
Train: 255 [ 500/1251 ( 40%)]  Loss: 2.846 (3.13)  Time: 0.990s, 1033.85/s  (1.016s, 1008.13/s)  LR: 6.395e-05  Data: 0.010 (0.017)
Train: 255 [ 550/1251 ( 44%)]  Loss: 3.204 (3.14)  Time: 0.992s, 1031.96/s  (1.014s, 1009.64/s)  LR: 6.395e-05  Data: 0.010 (0.017)
Train: 255 [ 600/1251 ( 48%)]  Loss: 2.786 (3.11)  Time: 1.056s,  969.26/s  (1.013s, 1010.88/s)  LR: 6.395e-05  Data: 0.010 (0.016)
Train: 255 [ 650/1251 ( 52%)]  Loss: 3.103 (3.11)  Time: 1.003s, 1020.94/s  (1.013s, 1011.20/s)  LR: 6.395e-05  Data: 0.011 (0.016)
Train: 255 [ 700/1251 ( 56%)]  Loss: 2.866 (3.09)  Time: 1.022s, 1002.02/s  (1.012s, 1011.75/s)  LR: 6.395e-05  Data: 0.011 (0.015)
Train: 255 [ 750/1251 ( 60%)]  Loss: 3.053 (3.09)  Time: 1.060s,  966.02/s  (1.012s, 1011.83/s)  LR: 6.395e-05  Data: 0.011 (0.015)
Train: 255 [ 800/1251 ( 64%)]  Loss: 3.218 (3.10)  Time: 0.990s, 1034.10/s  (1.011s, 1012.49/s)  LR: 6.395e-05  Data: 0.011 (0.015)
Train: 255 [ 850/1251 ( 68%)]  Loss: 3.047 (3.09)  Time: 0.998s, 1026.13/s  (1.011s, 1012.90/s)  LR: 6.395e-05  Data: 0.012 (0.015)
Train: 255 [ 900/1251 ( 72%)]  Loss: 3.091 (3.09)  Time: 0.995s, 1029.10/s  (1.010s, 1013.39/s)  LR: 6.395e-05  Data: 0.011 (0.014)
Train: 255 [ 950/1251 ( 76%)]  Loss: 3.275 (3.10)  Time: 0.996s, 1027.65/s  (1.010s, 1013.77/s)  LR: 6.395e-05  Data: 0.012 (0.014)
Train: 255 [1000/1251 ( 80%)]  Loss: 3.246 (3.11)  Time: 1.001s, 1022.64/s  (1.010s, 1013.59/s)  LR: 6.395e-05  Data: 0.010 (0.014)
Train: 255 [1050/1251 ( 84%)]  Loss: 2.889 (3.10)  Time: 0.994s, 1030.02/s  (1.010s, 1014.03/s)  LR: 6.395e-05  Data: 0.011 (0.014)
Train: 255 [1100/1251 ( 88%)]  Loss: 3.114 (3.10)  Time: 0.999s, 1024.63/s  (1.009s, 1014.46/s)  LR: 6.395e-05  Data: 0.013 (0.014)
Train: 255 [1150/1251 ( 92%)]  Loss: 2.906 (3.09)  Time: 0.993s, 1031.00/s  (1.009s, 1014.88/s)  LR: 6.395e-05  Data: 0.010 (0.014)
Train: 255 [1200/1251 ( 96%)]  Loss: 3.326 (3.10)  Time: 0.994s, 1029.67/s  (1.009s, 1015.29/s)  LR: 6.395e-05  Data: 0.011 (0.014)
Train: 255 [1250/1251 (100%)]  Loss: 3.006 (3.10)  Time: 0.988s, 1036.19/s  (1.008s, 1015.64/s)  LR: 6.395e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.613 (1.613)  Loss:  0.6703 (0.6703)  Acc@1: 91.3086 (91.3086)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.245 (0.573)  Loss:  0.7833 (1.1294)  Acc@1: 87.1462 (79.6500)  Acc@5: 97.9953 (94.8980)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-255.pth.tar', 79.64999997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-252.pth.tar', 79.58199998046875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-254.pth.tar', 79.54199997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-253.pth.tar', 79.50399997802734)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-248.pth.tar', 79.50399987304688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-250.pth.tar', 79.39800013183594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-247.pth.tar', 79.36800003417969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-244.pth.tar', 79.36799992675782)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-251.pth.tar', 79.32000002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-249.pth.tar', 79.30800003173829)

Train: 256 [   0/1251 (  0%)]  Loss: 2.982 (2.98)  Time: 2.495s,  410.36/s  (2.495s,  410.36/s)  LR: 6.162e-05  Data: 1.493 (1.493)
Train: 256 [  50/1251 (  4%)]  Loss: 2.976 (2.98)  Time: 0.998s, 1026.34/s  (1.046s,  979.43/s)  LR: 6.162e-05  Data: 0.011 (0.040)
Train: 256 [ 100/1251 (  8%)]  Loss: 3.009 (2.99)  Time: 0.998s, 1026.28/s  (1.024s, 1000.47/s)  LR: 6.162e-05  Data: 0.011 (0.026)
Train: 256 [ 150/1251 ( 12%)]  Loss: 2.888 (2.96)  Time: 0.997s, 1026.57/s  (1.020s, 1003.87/s)  LR: 6.162e-05  Data: 0.011 (0.021)
Train: 256 [ 200/1251 ( 16%)]  Loss: 3.340 (3.04)  Time: 0.995s, 1029.58/s  (1.015s, 1008.39/s)  LR: 6.162e-05  Data: 0.011 (0.019)
Train: 256 [ 250/1251 ( 20%)]  Loss: 2.937 (3.02)  Time: 0.997s, 1027.20/s  (1.012s, 1011.62/s)  LR: 6.162e-05  Data: 0.012 (0.017)
Train: 256 [ 300/1251 ( 24%)]  Loss: 2.994 (3.02)  Time: 0.996s, 1028.43/s  (1.013s, 1011.18/s)  LR: 6.162e-05  Data: 0.011 (0.016)
Train: 256 [ 350/1251 ( 28%)]  Loss: 3.351 (3.06)  Time: 0.998s, 1026.04/s  (1.011s, 1012.80/s)  LR: 6.162e-05  Data: 0.011 (0.016)
Train: 256 [ 400/1251 ( 32%)]  Loss: 3.202 (3.08)  Time: 1.000s, 1023.54/s  (1.010s, 1014.11/s)  LR: 6.162e-05  Data: 0.011 (0.015)
Train: 256 [ 450/1251 ( 36%)]  Loss: 3.266 (3.09)  Time: 1.045s,  979.89/s  (1.010s, 1014.03/s)  LR: 6.162e-05  Data: 0.011 (0.015)
Train: 256 [ 500/1251 ( 40%)]  Loss: 3.122 (3.10)  Time: 0.993s, 1030.99/s  (1.010s, 1013.46/s)  LR: 6.162e-05  Data: 0.011 (0.014)
Train: 256 [ 550/1251 ( 44%)]  Loss: 3.358 (3.12)  Time: 1.013s, 1010.63/s  (1.009s, 1014.62/s)  LR: 6.162e-05  Data: 0.011 (0.014)
Train: 256 [ 600/1251 ( 48%)]  Loss: 3.394 (3.14)  Time: 0.994s, 1029.92/s  (1.009s, 1015.10/s)  LR: 6.162e-05  Data: 0.011 (0.014)
Train: 256 [ 650/1251 ( 52%)]  Loss: 3.233 (3.15)  Time: 0.998s, 1025.69/s  (1.008s, 1015.78/s)  LR: 6.162e-05  Data: 0.011 (0.014)
Train: 256 [ 700/1251 ( 56%)]  Loss: 3.393 (3.16)  Time: 0.994s, 1029.89/s  (1.008s, 1016.37/s)  LR: 6.162e-05  Data: 0.010 (0.013)
Train: 256 [ 750/1251 ( 60%)]  Loss: 3.145 (3.16)  Time: 0.998s, 1025.57/s  (1.007s, 1016.93/s)  LR: 6.162e-05  Data: 0.012 (0.013)
Train: 256 [ 800/1251 ( 64%)]  Loss: 2.746 (3.14)  Time: 0.994s, 1030.42/s  (1.006s, 1017.51/s)  LR: 6.162e-05  Data: 0.010 (0.013)
Train: 256 [ 850/1251 ( 68%)]  Loss: 3.353 (3.15)  Time: 0.997s, 1027.09/s  (1.006s, 1017.92/s)  LR: 6.162e-05  Data: 0.011 (0.013)
Train: 256 [ 900/1251 ( 72%)]  Loss: 2.969 (3.14)  Time: 0.998s, 1026.40/s  (1.006s, 1018.36/s)  LR: 6.162e-05  Data: 0.010 (0.013)
Train: 256 [ 950/1251 ( 76%)]  Loss: 2.668 (3.12)  Time: 0.996s, 1027.92/s  (1.005s, 1018.85/s)  LR: 6.162e-05  Data: 0.010 (0.013)
Train: 256 [1000/1251 ( 80%)]  Loss: 3.344 (3.13)  Time: 1.052s,  973.42/s  (1.006s, 1018.36/s)  LR: 6.162e-05  Data: 0.011 (0.013)
Train: 256 [1050/1251 ( 84%)]  Loss: 3.324 (3.14)  Time: 0.998s, 1026.22/s  (1.006s, 1018.37/s)  LR: 6.162e-05  Data: 0.012 (0.013)
Train: 256 [1100/1251 ( 88%)]  Loss: 3.061 (3.13)  Time: 1.005s, 1019.30/s  (1.006s, 1017.93/s)  LR: 6.162e-05  Data: 0.012 (0.012)
Train: 256 [1150/1251 ( 92%)]  Loss: 3.078 (3.13)  Time: 1.018s, 1005.94/s  (1.006s, 1018.11/s)  LR: 6.162e-05  Data: 0.011 (0.012)
Train: 256 [1200/1251 ( 96%)]  Loss: 2.961 (3.12)  Time: 0.994s, 1030.25/s  (1.005s, 1018.42/s)  LR: 6.162e-05  Data: 0.010 (0.012)
Train: 256 [1250/1251 (100%)]  Loss: 3.330 (3.13)  Time: 0.985s, 1039.73/s  (1.005s, 1018.52/s)  LR: 6.162e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.616 (1.616)  Loss:  0.6234 (0.6234)  Acc@1: 91.0156 (91.0156)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.245 (0.575)  Loss:  0.7570 (1.0532)  Acc@1: 86.3208 (79.7560)  Acc@5: 97.2877 (94.8560)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-256.pth.tar', 79.75600000488281)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-255.pth.tar', 79.64999997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-252.pth.tar', 79.58199998046875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-254.pth.tar', 79.54199997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-253.pth.tar', 79.50399997802734)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-248.pth.tar', 79.50399987304688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-250.pth.tar', 79.39800013183594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-247.pth.tar', 79.36800003417969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-244.pth.tar', 79.36799992675782)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-251.pth.tar', 79.32000002929688)

Train: 257 [   0/1251 (  0%)]  Loss: 2.784 (2.78)  Time: 2.494s,  410.51/s  (2.494s,  410.51/s)  LR: 5.934e-05  Data: 1.533 (1.533)
Train: 257 [  50/1251 (  4%)]  Loss: 3.167 (2.98)  Time: 0.996s, 1027.81/s  (1.032s,  992.04/s)  LR: 5.934e-05  Data: 0.011 (0.041)
Train: 257 [ 100/1251 (  8%)]  Loss: 2.779 (2.91)  Time: 0.995s, 1029.51/s  (1.018s, 1006.01/s)  LR: 5.934e-05  Data: 0.011 (0.026)
Train: 257 [ 150/1251 ( 12%)]  Loss: 3.282 (3.00)  Time: 0.996s, 1027.73/s  (1.012s, 1011.45/s)  LR: 5.934e-05  Data: 0.011 (0.021)
Train: 257 [ 200/1251 ( 16%)]  Loss: 2.934 (2.99)  Time: 0.994s, 1030.14/s  (1.012s, 1011.59/s)  LR: 5.934e-05  Data: 0.012 (0.019)
Train: 257 [ 250/1251 ( 20%)]  Loss: 3.396 (3.06)  Time: 1.042s,  982.42/s  (1.011s, 1012.53/s)  LR: 5.934e-05  Data: 0.012 (0.017)
Train: 257 [ 300/1251 ( 24%)]  Loss: 3.158 (3.07)  Time: 0.998s, 1026.28/s  (1.010s, 1014.33/s)  LR: 5.934e-05  Data: 0.012 (0.016)
Train: 257 [ 350/1251 ( 28%)]  Loss: 3.147 (3.08)  Time: 1.055s,  970.23/s  (1.010s, 1013.62/s)  LR: 5.934e-05  Data: 0.011 (0.016)
Train: 257 [ 400/1251 ( 32%)]  Loss: 2.934 (3.06)  Time: 1.001s, 1023.02/s  (1.009s, 1015.08/s)  LR: 5.934e-05  Data: 0.010 (0.015)
Train: 257 [ 450/1251 ( 36%)]  Loss: 3.176 (3.08)  Time: 1.032s,  992.27/s  (1.008s, 1016.17/s)  LR: 5.934e-05  Data: 0.012 (0.015)
Train: 257 [ 500/1251 ( 40%)]  Loss: 3.043 (3.07)  Time: 1.047s,  978.28/s  (1.007s, 1017.08/s)  LR: 5.934e-05  Data: 0.011 (0.014)
Train: 257 [ 550/1251 ( 44%)]  Loss: 3.090 (3.07)  Time: 1.027s,  996.90/s  (1.007s, 1017.32/s)  LR: 5.934e-05  Data: 0.012 (0.014)
Train: 257 [ 600/1251 ( 48%)]  Loss: 3.361 (3.10)  Time: 1.004s, 1019.87/s  (1.006s, 1017.76/s)  LR: 5.934e-05  Data: 0.016 (0.014)
Train: 257 [ 650/1251 ( 52%)]  Loss: 3.304 (3.11)  Time: 1.006s, 1017.74/s  (1.006s, 1018.06/s)  LR: 5.934e-05  Data: 0.012 (0.014)
Train: 257 [ 700/1251 ( 56%)]  Loss: 3.274 (3.12)  Time: 0.994s, 1030.30/s  (1.005s, 1018.58/s)  LR: 5.934e-05  Data: 0.011 (0.013)
Train: 257 [ 750/1251 ( 60%)]  Loss: 3.566 (3.15)  Time: 0.998s, 1026.47/s  (1.005s, 1018.91/s)  LR: 5.934e-05  Data: 0.012 (0.013)
Train: 257 [ 800/1251 ( 64%)]  Loss: 3.078 (3.15)  Time: 0.993s, 1030.78/s  (1.005s, 1019.16/s)  LR: 5.934e-05  Data: 0.011 (0.013)
Train: 257 [ 850/1251 ( 68%)]  Loss: 2.876 (3.13)  Time: 0.992s, 1031.78/s  (1.005s, 1019.40/s)  LR: 5.934e-05  Data: 0.011 (0.013)
Train: 257 [ 900/1251 ( 72%)]  Loss: 3.265 (3.14)  Time: 1.000s, 1023.94/s  (1.004s, 1019.56/s)  LR: 5.934e-05  Data: 0.011 (0.013)
Train: 257 [ 950/1251 ( 76%)]  Loss: 2.925 (3.13)  Time: 1.002s, 1021.52/s  (1.005s, 1019.31/s)  LR: 5.934e-05  Data: 0.013 (0.013)
Train: 257 [1000/1251 ( 80%)]  Loss: 3.161 (3.13)  Time: 1.060s,  966.16/s  (1.005s, 1019.06/s)  LR: 5.934e-05  Data: 0.011 (0.013)
Train: 257 [1050/1251 ( 84%)]  Loss: 2.939 (3.12)  Time: 1.001s, 1022.77/s  (1.006s, 1018.07/s)  LR: 5.934e-05  Data: 0.010 (0.013)
Train: 257 [1100/1251 ( 88%)]  Loss: 3.259 (3.13)  Time: 0.996s, 1028.54/s  (1.006s, 1017.51/s)  LR: 5.934e-05  Data: 0.011 (0.013)
Train: 257 [1150/1251 ( 92%)]  Loss: 3.433 (3.14)  Time: 0.994s, 1029.83/s  (1.006s, 1017.61/s)  LR: 5.934e-05  Data: 0.010 (0.013)
Train: 257 [1200/1251 ( 96%)]  Loss: 2.910 (3.13)  Time: 1.035s,  989.03/s  (1.006s, 1017.43/s)  LR: 5.934e-05  Data: 0.011 (0.012)
Train: 257 [1250/1251 (100%)]  Loss: 3.303 (3.14)  Time: 0.989s, 1035.82/s  (1.006s, 1017.76/s)  LR: 5.934e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.641 (1.641)  Loss:  0.5877 (0.5877)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.245 (0.573)  Loss:  0.7288 (1.0595)  Acc@1: 86.9104 (79.8140)  Acc@5: 97.2877 (94.9300)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-257.pth.tar', 79.8140000024414)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-256.pth.tar', 79.75600000488281)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-255.pth.tar', 79.64999997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-252.pth.tar', 79.58199998046875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-254.pth.tar', 79.54199997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-253.pth.tar', 79.50399997802734)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-248.pth.tar', 79.50399987304688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-250.pth.tar', 79.39800013183594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-247.pth.tar', 79.36800003417969)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-244.pth.tar', 79.36799992675782)

Train: 258 [   0/1251 (  0%)]  Loss: 3.148 (3.15)  Time: 2.555s,  400.78/s  (2.555s,  400.78/s)  LR: 5.711e-05  Data: 1.588 (1.588)
Train: 258 [  50/1251 (  4%)]  Loss: 3.085 (3.12)  Time: 1.000s, 1023.55/s  (1.029s,  995.44/s)  LR: 5.711e-05  Data: 0.010 (0.042)
Train: 258 [ 100/1251 (  8%)]  Loss: 3.457 (3.23)  Time: 1.020s, 1004.32/s  (1.025s,  998.94/s)  LR: 5.711e-05  Data: 0.012 (0.027)
Train: 258 [ 150/1251 ( 12%)]  Loss: 3.274 (3.24)  Time: 0.996s, 1028.31/s  (1.017s, 1006.41/s)  LR: 5.711e-05  Data: 0.012 (0.022)
Train: 258 [ 200/1251 ( 16%)]  Loss: 3.300 (3.25)  Time: 1.006s, 1017.52/s  (1.014s, 1009.87/s)  LR: 5.711e-05  Data: 0.011 (0.019)
Train: 258 [ 250/1251 ( 20%)]  Loss: 3.124 (3.23)  Time: 0.998s, 1026.46/s  (1.012s, 1012.23/s)  LR: 5.711e-05  Data: 0.012 (0.018)
Train: 258 [ 300/1251 ( 24%)]  Loss: 3.457 (3.26)  Time: 1.004s, 1019.79/s  (1.010s, 1014.05/s)  LR: 5.711e-05  Data: 0.012 (0.017)
Train: 258 [ 350/1251 ( 28%)]  Loss: 3.207 (3.26)  Time: 0.996s, 1027.77/s  (1.009s, 1014.81/s)  LR: 5.711e-05  Data: 0.010 (0.016)
Train: 258 [ 400/1251 ( 32%)]  Loss: 3.066 (3.24)  Time: 1.023s, 1000.87/s  (1.010s, 1014.27/s)  LR: 5.711e-05  Data: 0.012 (0.015)
Train: 258 [ 450/1251 ( 36%)]  Loss: 2.940 (3.21)  Time: 0.998s, 1025.86/s  (1.009s, 1015.14/s)  LR: 5.711e-05  Data: 0.012 (0.015)
Train: 258 [ 500/1251 ( 40%)]  Loss: 3.285 (3.21)  Time: 0.998s, 1025.69/s  (1.008s, 1015.49/s)  LR: 5.711e-05  Data: 0.011 (0.014)
Train: 258 [ 550/1251 ( 44%)]  Loss: 2.989 (3.19)  Time: 1.001s, 1023.31/s  (1.008s, 1015.52/s)  LR: 5.711e-05  Data: 0.013 (0.014)
Train: 258 [ 600/1251 ( 48%)]  Loss: 2.931 (3.17)  Time: 0.995s, 1029.55/s  (1.009s, 1014.39/s)  LR: 5.711e-05  Data: 0.011 (0.014)
Train: 258 [ 650/1251 ( 52%)]  Loss: 3.396 (3.19)  Time: 0.994s, 1029.89/s  (1.010s, 1013.87/s)  LR: 5.711e-05  Data: 0.010 (0.014)
Train: 258 [ 700/1251 ( 56%)]  Loss: 3.091 (3.18)  Time: 0.997s, 1026.75/s  (1.010s, 1014.20/s)  LR: 5.711e-05  Data: 0.012 (0.013)
Train: 258 [ 750/1251 ( 60%)]  Loss: 3.132 (3.18)  Time: 0.996s, 1027.84/s  (1.009s, 1015.00/s)  LR: 5.711e-05  Data: 0.012 (0.013)
Train: 258 [ 800/1251 ( 64%)]  Loss: 3.040 (3.17)  Time: 1.066s,  960.24/s  (1.009s, 1015.37/s)  LR: 5.711e-05  Data: 0.011 (0.013)
Train: 258 [ 850/1251 ( 68%)]  Loss: 3.247 (3.18)  Time: 1.012s, 1012.14/s  (1.008s, 1016.00/s)  LR: 5.711e-05  Data: 0.012 (0.013)
Train: 258 [ 900/1251 ( 72%)]  Loss: 2.980 (3.17)  Time: 1.011s, 1012.95/s  (1.007s, 1016.39/s)  LR: 5.711e-05  Data: 0.011 (0.013)
Train: 258 [ 950/1251 ( 76%)]  Loss: 3.351 (3.18)  Time: 0.996s, 1027.90/s  (1.007s, 1016.78/s)  LR: 5.711e-05  Data: 0.012 (0.013)
Train: 258 [1000/1251 ( 80%)]  Loss: 2.706 (3.15)  Time: 1.000s, 1023.98/s  (1.007s, 1017.10/s)  LR: 5.711e-05  Data: 0.014 (0.013)
Train: 258 [1050/1251 ( 84%)]  Loss: 3.166 (3.15)  Time: 0.995s, 1028.80/s  (1.007s, 1016.60/s)  LR: 5.711e-05  Data: 0.011 (0.013)
Train: 258 [1100/1251 ( 88%)]  Loss: 3.284 (3.16)  Time: 0.995s, 1028.86/s  (1.007s, 1016.84/s)  LR: 5.711e-05  Data: 0.012 (0.013)
Train: 258 [1150/1251 ( 92%)]  Loss: 3.371 (3.17)  Time: 0.998s, 1026.07/s  (1.007s, 1017.02/s)  LR: 5.711e-05  Data: 0.011 (0.013)
Train: 258 [1200/1251 ( 96%)]  Loss: 2.828 (3.15)  Time: 1.006s, 1018.23/s  (1.007s, 1017.32/s)  LR: 5.711e-05  Data: 0.011 (0.013)
Train: 258 [1250/1251 (100%)]  Loss: 3.276 (3.16)  Time: 0.984s, 1040.59/s  (1.006s, 1017.53/s)  LR: 5.711e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.679 (1.679)  Loss:  0.7037 (0.7037)  Acc@1: 92.1875 (92.1875)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.245 (0.571)  Loss:  0.8266 (1.1501)  Acc@1: 85.8491 (79.6860)  Acc@5: 97.2877 (94.8620)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-257.pth.tar', 79.8140000024414)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-256.pth.tar', 79.75600000488281)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-258.pth.tar', 79.68600005859375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-255.pth.tar', 79.64999997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-252.pth.tar', 79.58199998046875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-254.pth.tar', 79.54199997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-253.pth.tar', 79.50399997802734)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-248.pth.tar', 79.50399987304688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-250.pth.tar', 79.39800013183594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-247.pth.tar', 79.36800003417969)

Train: 259 [   0/1251 (  0%)]  Loss: 3.164 (3.16)  Time: 2.533s,  404.21/s  (2.533s,  404.21/s)  LR: 5.493e-05  Data: 1.575 (1.575)
Train: 259 [  50/1251 (  4%)]  Loss: 3.347 (3.26)  Time: 1.041s,  983.36/s  (1.055s,  970.41/s)  LR: 5.493e-05  Data: 0.015 (0.042)
Train: 259 [ 100/1251 (  8%)]  Loss: 3.284 (3.26)  Time: 1.060s,  965.92/s  (1.039s,  985.23/s)  LR: 5.493e-05  Data: 0.011 (0.027)
Train: 259 [ 150/1251 ( 12%)]  Loss: 3.170 (3.24)  Time: 1.056s,  969.54/s  (1.040s,  984.53/s)  LR: 5.493e-05  Data: 0.011 (0.022)
Train: 259 [ 200/1251 ( 16%)]  Loss: 3.134 (3.22)  Time: 0.997s, 1026.93/s  (1.032s,  992.04/s)  LR: 5.493e-05  Data: 0.010 (0.019)
Train: 259 [ 250/1251 ( 20%)]  Loss: 3.406 (3.25)  Time: 1.006s, 1017.46/s  (1.027s,  997.45/s)  LR: 5.493e-05  Data: 0.011 (0.018)
Train: 259 [ 300/1251 ( 24%)]  Loss: 3.255 (3.25)  Time: 1.007s, 1017.30/s  (1.022s, 1001.49/s)  LR: 5.493e-05  Data: 0.012 (0.017)
Train: 259 [ 350/1251 ( 28%)]  Loss: 3.186 (3.24)  Time: 0.996s, 1027.83/s  (1.019s, 1004.65/s)  LR: 5.493e-05  Data: 0.011 (0.016)
Train: 259 [ 400/1251 ( 32%)]  Loss: 3.123 (3.23)  Time: 0.997s, 1027.49/s  (1.017s, 1007.06/s)  LR: 5.493e-05  Data: 0.011 (0.015)
Train: 259 [ 450/1251 ( 36%)]  Loss: 2.927 (3.20)  Time: 0.996s, 1027.64/s  (1.015s, 1008.90/s)  LR: 5.493e-05  Data: 0.012 (0.015)
Train: 259 [ 500/1251 ( 40%)]  Loss: 3.280 (3.21)  Time: 1.038s,  986.46/s  (1.014s, 1010.25/s)  LR: 5.493e-05  Data: 0.011 (0.014)
Train: 259 [ 550/1251 ( 44%)]  Loss: 3.278 (3.21)  Time: 1.053s,  972.41/s  (1.014s, 1010.06/s)  LR: 5.493e-05  Data: 0.010 (0.014)
Train: 259 [ 600/1251 ( 48%)]  Loss: 3.037 (3.20)  Time: 0.999s, 1025.34/s  (1.015s, 1009.21/s)  LR: 5.493e-05  Data: 0.011 (0.014)
Train: 259 [ 650/1251 ( 52%)]  Loss: 3.318 (3.21)  Time: 0.992s, 1032.46/s  (1.014s, 1010.13/s)  LR: 5.493e-05  Data: 0.011 (0.014)
Train: 259 [ 700/1251 ( 56%)]  Loss: 3.210 (3.21)  Time: 0.991s, 1033.50/s  (1.013s, 1011.00/s)  LR: 5.493e-05  Data: 0.010 (0.013)
Train: 259 [ 750/1251 ( 60%)]  Loss: 3.453 (3.22)  Time: 1.058s,  967.84/s  (1.012s, 1011.96/s)  LR: 5.493e-05  Data: 0.011 (0.013)
Train: 259 [ 800/1251 ( 64%)]  Loss: 3.240 (3.22)  Time: 0.994s, 1029.74/s  (1.011s, 1012.72/s)  LR: 5.493e-05  Data: 0.011 (0.013)
Train: 259 [ 850/1251 ( 68%)]  Loss: 3.082 (3.22)  Time: 0.997s, 1027.36/s  (1.011s, 1013.30/s)  LR: 5.493e-05  Data: 0.011 (0.013)
Train: 259 [ 900/1251 ( 72%)]  Loss: 3.336 (3.22)  Time: 1.008s, 1015.88/s  (1.011s, 1012.99/s)  LR: 5.493e-05  Data: 0.011 (0.013)
Train: 259 [ 950/1251 ( 76%)]  Loss: 3.210 (3.22)  Time: 0.996s, 1028.39/s  (1.010s, 1013.55/s)  LR: 5.493e-05  Data: 0.011 (0.013)
Train: 259 [1000/1251 ( 80%)]  Loss: 3.301 (3.23)  Time: 1.033s,  990.90/s  (1.011s, 1013.14/s)  LR: 5.493e-05  Data: 0.011 (0.013)
Train: 259 [1050/1251 ( 84%)]  Loss: 2.941 (3.21)  Time: 0.993s, 1030.83/s  (1.010s, 1013.37/s)  LR: 5.493e-05  Data: 0.010 (0.013)
Train: 259 [1100/1251 ( 88%)]  Loss: 2.820 (3.20)  Time: 0.992s, 1031.88/s  (1.010s, 1013.87/s)  LR: 5.493e-05  Data: 0.011 (0.013)
Train: 259 [1150/1251 ( 92%)]  Loss: 3.300 (3.20)  Time: 0.997s, 1026.68/s  (1.010s, 1014.23/s)  LR: 5.493e-05  Data: 0.012 (0.013)
Train: 259 [1200/1251 ( 96%)]  Loss: 3.235 (3.20)  Time: 0.995s, 1029.53/s  (1.009s, 1014.54/s)  LR: 5.493e-05  Data: 0.011 (0.013)
Train: 259 [1250/1251 (100%)]  Loss: 3.231 (3.20)  Time: 0.993s, 1031.32/s  (1.009s, 1014.89/s)  LR: 5.493e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.737 (1.737)  Loss:  0.6810 (0.6810)  Acc@1: 91.6016 (91.6016)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.245 (0.571)  Loss:  0.7755 (1.1312)  Acc@1: 86.9104 (79.6860)  Acc@5: 97.6415 (94.8500)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-257.pth.tar', 79.8140000024414)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-256.pth.tar', 79.75600000488281)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-258.pth.tar', 79.68600005859375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-259.pth.tar', 79.6860000024414)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-255.pth.tar', 79.64999997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-252.pth.tar', 79.58199998046875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-254.pth.tar', 79.54199997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-253.pth.tar', 79.50399997802734)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-248.pth.tar', 79.50399987304688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-250.pth.tar', 79.39800013183594)

Train: 260 [   0/1251 (  0%)]  Loss: 3.162 (3.16)  Time: 2.460s,  416.32/s  (2.460s,  416.32/s)  LR: 5.279e-05  Data: 1.492 (1.492)
Train: 260 [  50/1251 (  4%)]  Loss: 3.150 (3.16)  Time: 0.996s, 1027.91/s  (1.034s,  990.27/s)  LR: 5.279e-05  Data: 0.011 (0.040)
Train: 260 [ 100/1251 (  8%)]  Loss: 3.206 (3.17)  Time: 1.058s,  968.09/s  (1.022s, 1001.55/s)  LR: 5.279e-05  Data: 0.011 (0.026)
Train: 260 [ 150/1251 ( 12%)]  Loss: 2.650 (3.04)  Time: 1.062s,  964.33/s  (1.019s, 1004.80/s)  LR: 5.279e-05  Data: 0.011 (0.021)
Train: 260 [ 200/1251 ( 16%)]  Loss: 3.186 (3.07)  Time: 0.999s, 1025.33/s  (1.026s,  998.39/s)  LR: 5.279e-05  Data: 0.011 (0.018)
Train: 260 [ 250/1251 ( 20%)]  Loss: 3.480 (3.14)  Time: 1.004s, 1020.21/s  (1.020s, 1003.45/s)  LR: 5.279e-05  Data: 0.011 (0.017)
Train: 260 [ 300/1251 ( 24%)]  Loss: 3.155 (3.14)  Time: 0.995s, 1028.63/s  (1.018s, 1005.74/s)  LR: 5.279e-05  Data: 0.011 (0.016)
Train: 260 [ 350/1251 ( 28%)]  Loss: 3.382 (3.17)  Time: 1.039s,  985.61/s  (1.016s, 1007.94/s)  LR: 5.279e-05  Data: 0.011 (0.015)
Train: 260 [ 400/1251 ( 32%)]  Loss: 3.319 (3.19)  Time: 0.999s, 1025.20/s  (1.014s, 1009.91/s)  LR: 5.279e-05  Data: 0.011 (0.015)
Train: 260 [ 450/1251 ( 36%)]  Loss: 3.356 (3.20)  Time: 0.993s, 1030.80/s  (1.013s, 1010.89/s)  LR: 5.279e-05  Data: 0.011 (0.014)
Train: 260 [ 500/1251 ( 40%)]  Loss: 2.917 (3.18)  Time: 0.999s, 1024.62/s  (1.012s, 1012.09/s)  LR: 5.279e-05  Data: 0.011 (0.014)
Train: 260 [ 550/1251 ( 44%)]  Loss: 3.123 (3.17)  Time: 1.004s, 1019.77/s  (1.011s, 1013.08/s)  LR: 5.279e-05  Data: 0.013 (0.014)
Train: 260 [ 600/1251 ( 48%)]  Loss: 3.318 (3.19)  Time: 0.997s, 1027.47/s  (1.010s, 1013.82/s)  LR: 5.279e-05  Data: 0.011 (0.014)
Train: 260 [ 650/1251 ( 52%)]  Loss: 3.361 (3.20)  Time: 1.000s, 1024.22/s  (1.009s, 1014.54/s)  LR: 5.279e-05  Data: 0.012 (0.013)
Train: 260 [ 700/1251 ( 56%)]  Loss: 3.382 (3.21)  Time: 0.995s, 1029.34/s  (1.009s, 1015.13/s)  LR: 5.279e-05  Data: 0.011 (0.013)
Train: 260 [ 750/1251 ( 60%)]  Loss: 3.045 (3.20)  Time: 1.001s, 1023.44/s  (1.009s, 1014.71/s)  LR: 5.279e-05  Data: 0.010 (0.013)
Train: 260 [ 800/1251 ( 64%)]  Loss: 3.346 (3.21)  Time: 0.993s, 1031.58/s  (1.008s, 1015.39/s)  LR: 5.279e-05  Data: 0.010 (0.013)
Train: 260 [ 850/1251 ( 68%)]  Loss: 3.155 (3.21)  Time: 1.001s, 1023.08/s  (1.009s, 1014.72/s)  LR: 5.279e-05  Data: 0.012 (0.013)
Train: 260 [ 900/1251 ( 72%)]  Loss: 2.902 (3.19)  Time: 0.994s, 1029.69/s  (1.009s, 1015.22/s)  LR: 5.279e-05  Data: 0.010 (0.013)
Train: 260 [ 950/1251 ( 76%)]  Loss: 2.771 (3.17)  Time: 1.057s,  968.93/s  (1.008s, 1015.59/s)  LR: 5.279e-05  Data: 0.011 (0.013)
Train: 260 [1000/1251 ( 80%)]  Loss: 3.337 (3.18)  Time: 0.997s, 1027.30/s  (1.008s, 1015.47/s)  LR: 5.279e-05  Data: 0.011 (0.013)
Train: 260 [1050/1251 ( 84%)]  Loss: 2.932 (3.17)  Time: 0.996s, 1027.84/s  (1.008s, 1015.59/s)  LR: 5.279e-05  Data: 0.011 (0.013)
Train: 260 [1100/1251 ( 88%)]  Loss: 2.979 (3.16)  Time: 1.005s, 1018.73/s  (1.008s, 1015.90/s)  LR: 5.279e-05  Data: 0.011 (0.012)
Train: 260 [1150/1251 ( 92%)]  Loss: 3.279 (3.16)  Time: 0.997s, 1026.75/s  (1.008s, 1016.03/s)  LR: 5.279e-05  Data: 0.011 (0.012)
Train: 260 [1200/1251 ( 96%)]  Loss: 3.331 (3.17)  Time: 1.002s, 1022.14/s  (1.008s, 1016.21/s)  LR: 5.279e-05  Data: 0.012 (0.012)
Train: 260 [1250/1251 (100%)]  Loss: 2.939 (3.16)  Time: 0.984s, 1040.29/s  (1.007s, 1016.51/s)  LR: 5.279e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.669 (1.669)  Loss:  0.6069 (0.6069)  Acc@1: 91.2109 (91.2109)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.245 (0.568)  Loss:  0.6908 (1.0187)  Acc@1: 85.7311 (79.8560)  Acc@5: 97.4057 (94.9920)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-260.pth.tar', 79.85600000732421)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-257.pth.tar', 79.8140000024414)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-256.pth.tar', 79.75600000488281)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-258.pth.tar', 79.68600005859375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-259.pth.tar', 79.6860000024414)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-255.pth.tar', 79.64999997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-252.pth.tar', 79.58199998046875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-254.pth.tar', 79.54199997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-253.pth.tar', 79.50399997802734)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-248.pth.tar', 79.50399987304688)

Train: 261 [   0/1251 (  0%)]  Loss: 2.727 (2.73)  Time: 2.470s,  414.52/s  (2.470s,  414.52/s)  LR: 5.071e-05  Data: 1.509 (1.509)
Train: 261 [  50/1251 (  4%)]  Loss: 3.265 (3.00)  Time: 0.995s, 1029.02/s  (1.043s,  981.69/s)  LR: 5.071e-05  Data: 0.012 (0.043)
Train: 261 [ 100/1251 (  8%)]  Loss: 3.105 (3.03)  Time: 1.005s, 1019.13/s  (1.024s,  999.88/s)  LR: 5.071e-05  Data: 0.017 (0.027)
Train: 261 [ 150/1251 ( 12%)]  Loss: 2.817 (2.98)  Time: 0.997s, 1027.37/s  (1.016s, 1007.74/s)  LR: 5.071e-05  Data: 0.011 (0.022)
Train: 261 [ 200/1251 ( 16%)]  Loss: 3.057 (2.99)  Time: 1.014s, 1009.83/s  (1.013s, 1010.93/s)  LR: 5.071e-05  Data: 0.011 (0.019)
Train: 261 [ 250/1251 ( 20%)]  Loss: 3.252 (3.04)  Time: 0.993s, 1031.18/s  (1.011s, 1013.14/s)  LR: 5.071e-05  Data: 0.011 (0.018)
Train: 261 [ 300/1251 ( 24%)]  Loss: 3.199 (3.06)  Time: 0.994s, 1030.41/s  (1.015s, 1009.28/s)  LR: 5.071e-05  Data: 0.011 (0.017)
Train: 261 [ 350/1251 ( 28%)]  Loss: 3.372 (3.10)  Time: 0.996s, 1028.35/s  (1.012s, 1011.51/s)  LR: 5.071e-05  Data: 0.011 (0.016)
Train: 261 [ 400/1251 ( 32%)]  Loss: 3.077 (3.10)  Time: 1.063s,  963.29/s  (1.014s, 1009.87/s)  LR: 5.071e-05  Data: 0.011 (0.015)
Train: 261 [ 450/1251 ( 36%)]  Loss: 3.158 (3.10)  Time: 1.038s,  986.33/s  (1.014s, 1010.12/s)  LR: 5.071e-05  Data: 0.010 (0.015)
Train: 261 [ 500/1251 ( 40%)]  Loss: 3.248 (3.12)  Time: 1.040s,  985.07/s  (1.012s, 1011.50/s)  LR: 5.071e-05  Data: 0.012 (0.014)
Train: 261 [ 550/1251 ( 44%)]  Loss: 3.052 (3.11)  Time: 0.995s, 1029.02/s  (1.011s, 1012.66/s)  LR: 5.071e-05  Data: 0.012 (0.014)
Train: 261 [ 600/1251 ( 48%)]  Loss: 3.442 (3.14)  Time: 0.995s, 1028.74/s  (1.010s, 1013.54/s)  LR: 5.071e-05  Data: 0.011 (0.014)
Train: 261 [ 650/1251 ( 52%)]  Loss: 3.413 (3.16)  Time: 1.004s, 1020.18/s  (1.010s, 1014.33/s)  LR: 5.071e-05  Data: 0.011 (0.014)
Train: 261 [ 700/1251 ( 56%)]  Loss: 3.260 (3.16)  Time: 0.997s, 1027.31/s  (1.009s, 1014.58/s)  LR: 5.071e-05  Data: 0.012 (0.013)
Train: 261 [ 750/1251 ( 60%)]  Loss: 3.264 (3.17)  Time: 0.993s, 1030.70/s  (1.009s, 1015.06/s)  LR: 5.071e-05  Data: 0.011 (0.013)
Train: 261 [ 800/1251 ( 64%)]  Loss: 3.124 (3.17)  Time: 0.996s, 1027.74/s  (1.008s, 1015.64/s)  LR: 5.071e-05  Data: 0.011 (0.013)
Train: 261 [ 850/1251 ( 68%)]  Loss: 3.045 (3.16)  Time: 1.003s, 1020.96/s  (1.008s, 1015.64/s)  LR: 5.071e-05  Data: 0.011 (0.013)
Train: 261 [ 900/1251 ( 72%)]  Loss: 3.027 (3.15)  Time: 0.997s, 1026.85/s  (1.009s, 1015.06/s)  LR: 5.071e-05  Data: 0.015 (0.013)
Train: 261 [ 950/1251 ( 76%)]  Loss: 3.091 (3.15)  Time: 1.004s, 1020.09/s  (1.008s, 1015.41/s)  LR: 5.071e-05  Data: 0.011 (0.013)
Train: 261 [1000/1251 ( 80%)]  Loss: 2.883 (3.14)  Time: 0.994s, 1030.31/s  (1.008s, 1015.68/s)  LR: 5.071e-05  Data: 0.011 (0.013)
Train: 261 [1050/1251 ( 84%)]  Loss: 2.902 (3.13)  Time: 0.993s, 1031.20/s  (1.008s, 1016.16/s)  LR: 5.071e-05  Data: 0.011 (0.013)
Train: 261 [1100/1251 ( 88%)]  Loss: 3.192 (3.13)  Time: 0.994s, 1030.67/s  (1.007s, 1016.55/s)  LR: 5.071e-05  Data: 0.012 (0.013)
Train: 261 [1150/1251 ( 92%)]  Loss: 3.367 (3.14)  Time: 0.994s, 1030.30/s  (1.007s, 1016.50/s)  LR: 5.071e-05  Data: 0.011 (0.013)
Train: 261 [1200/1251 ( 96%)]  Loss: 3.138 (3.14)  Time: 0.996s, 1028.10/s  (1.008s, 1016.06/s)  LR: 5.071e-05  Data: 0.011 (0.013)
Train: 261 [1250/1251 (100%)]  Loss: 3.147 (3.14)  Time: 1.028s,  996.22/s  (1.008s, 1016.25/s)  LR: 5.071e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.649 (1.649)  Loss:  0.6279 (0.6279)  Acc@1: 92.2852 (92.2852)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.245 (0.571)  Loss:  0.7579 (1.0854)  Acc@1: 86.2028 (79.8680)  Acc@5: 97.1698 (94.9980)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-261.pth.tar', 79.86799995361328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-260.pth.tar', 79.85600000732421)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-257.pth.tar', 79.8140000024414)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-256.pth.tar', 79.75600000488281)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-258.pth.tar', 79.68600005859375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-259.pth.tar', 79.6860000024414)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-255.pth.tar', 79.64999997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-252.pth.tar', 79.58199998046875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-254.pth.tar', 79.54199997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-253.pth.tar', 79.50399997802734)

Train: 262 [   0/1251 (  0%)]  Loss: 3.506 (3.51)  Time: 2.518s,  406.74/s  (2.518s,  406.74/s)  LR: 4.868e-05  Data: 1.565 (1.565)
Train: 262 [  50/1251 (  4%)]  Loss: 3.290 (3.40)  Time: 0.996s, 1027.97/s  (1.040s,  984.60/s)  LR: 4.868e-05  Data: 0.012 (0.042)
Train: 262 [ 100/1251 (  8%)]  Loss: 3.193 (3.33)  Time: 0.992s, 1031.96/s  (1.019s, 1004.82/s)  LR: 4.868e-05  Data: 0.011 (0.027)
Train: 262 [ 150/1251 ( 12%)]  Loss: 3.279 (3.32)  Time: 1.019s, 1005.22/s  (1.014s, 1009.74/s)  LR: 4.868e-05  Data: 0.010 (0.022)
Train: 262 [ 200/1251 ( 16%)]  Loss: 3.380 (3.33)  Time: 1.005s, 1018.99/s  (1.012s, 1011.60/s)  LR: 4.868e-05  Data: 0.011 (0.019)
Train: 262 [ 250/1251 ( 20%)]  Loss: 2.909 (3.26)  Time: 0.998s, 1026.31/s  (1.010s, 1014.02/s)  LR: 4.868e-05  Data: 0.011 (0.018)
Train: 262 [ 300/1251 ( 24%)]  Loss: 3.266 (3.26)  Time: 0.998s, 1025.73/s  (1.008s, 1015.84/s)  LR: 4.868e-05  Data: 0.011 (0.017)
Train: 262 [ 350/1251 ( 28%)]  Loss: 3.157 (3.25)  Time: 0.990s, 1033.96/s  (1.007s, 1017.08/s)  LR: 4.868e-05  Data: 0.010 (0.016)
Train: 262 [ 400/1251 ( 32%)]  Loss: 3.088 (3.23)  Time: 0.999s, 1025.17/s  (1.007s, 1016.91/s)  LR: 4.868e-05  Data: 0.012 (0.015)
Train: 262 [ 450/1251 ( 36%)]  Loss: 3.189 (3.23)  Time: 0.996s, 1028.34/s  (1.006s, 1017.70/s)  LR: 4.868e-05  Data: 0.011 (0.015)
Train: 262 [ 500/1251 ( 40%)]  Loss: 2.652 (3.17)  Time: 1.003s, 1021.09/s  (1.006s, 1017.89/s)  LR: 4.868e-05  Data: 0.016 (0.014)
Train: 262 [ 550/1251 ( 44%)]  Loss: 2.966 (3.16)  Time: 1.032s,  992.39/s  (1.006s, 1018.31/s)  LR: 4.868e-05  Data: 0.011 (0.014)
Train: 262 [ 600/1251 ( 48%)]  Loss: 3.282 (3.17)  Time: 0.994s, 1030.43/s  (1.006s, 1017.66/s)  LR: 4.868e-05  Data: 0.011 (0.014)
Train: 262 [ 650/1251 ( 52%)]  Loss: 3.045 (3.16)  Time: 0.996s, 1028.58/s  (1.007s, 1016.50/s)  LR: 4.868e-05  Data: 0.011 (0.014)
Train: 262 [ 700/1251 ( 56%)]  Loss: 3.158 (3.16)  Time: 0.999s, 1025.10/s  (1.007s, 1017.03/s)  LR: 4.868e-05  Data: 0.012 (0.014)
Train: 262 [ 750/1251 ( 60%)]  Loss: 3.319 (3.17)  Time: 1.036s,  988.65/s  (1.008s, 1015.45/s)  LR: 4.868e-05  Data: 0.012 (0.013)
Train: 262 [ 800/1251 ( 64%)]  Loss: 3.211 (3.17)  Time: 1.011s, 1012.86/s  (1.009s, 1015.13/s)  LR: 4.868e-05  Data: 0.011 (0.013)
Train: 262 [ 850/1251 ( 68%)]  Loss: 3.070 (3.16)  Time: 0.995s, 1028.80/s  (1.009s, 1015.35/s)  LR: 4.868e-05  Data: 0.011 (0.013)
Train: 262 [ 900/1251 ( 72%)]  Loss: 2.857 (3.15)  Time: 1.003s, 1021.22/s  (1.008s, 1015.44/s)  LR: 4.868e-05  Data: 0.011 (0.013)
Train: 262 [ 950/1251 ( 76%)]  Loss: 3.121 (3.15)  Time: 0.997s, 1027.17/s  (1.008s, 1015.87/s)  LR: 4.868e-05  Data: 0.011 (0.013)
Train: 262 [1000/1251 ( 80%)]  Loss: 3.142 (3.15)  Time: 0.996s, 1028.47/s  (1.007s, 1016.42/s)  LR: 4.868e-05  Data: 0.011 (0.013)
Train: 262 [1050/1251 ( 84%)]  Loss: 3.276 (3.15)  Time: 1.004s, 1020.05/s  (1.007s, 1016.70/s)  LR: 4.868e-05  Data: 0.011 (0.013)
Train: 262 [1100/1251 ( 88%)]  Loss: 3.156 (3.15)  Time: 0.997s, 1027.23/s  (1.008s, 1016.22/s)  LR: 4.868e-05  Data: 0.011 (0.013)
Train: 262 [1150/1251 ( 92%)]  Loss: 3.103 (3.15)  Time: 0.993s, 1031.08/s  (1.007s, 1016.58/s)  LR: 4.868e-05  Data: 0.011 (0.013)
Train: 262 [1200/1251 ( 96%)]  Loss: 3.015 (3.15)  Time: 0.996s, 1028.39/s  (1.007s, 1016.56/s)  LR: 4.868e-05  Data: 0.011 (0.013)
Train: 262 [1250/1251 (100%)]  Loss: 3.176 (3.15)  Time: 0.987s, 1037.56/s  (1.007s, 1016.91/s)  LR: 4.868e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.609 (1.609)  Loss:  0.6851 (0.6851)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.245 (0.569)  Loss:  0.7635 (1.1230)  Acc@1: 87.0283 (79.6800)  Acc@5: 97.7594 (94.9420)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-261.pth.tar', 79.86799995361328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-260.pth.tar', 79.85600000732421)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-257.pth.tar', 79.8140000024414)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-256.pth.tar', 79.75600000488281)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-258.pth.tar', 79.68600005859375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-259.pth.tar', 79.6860000024414)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-262.pth.tar', 79.68000005371094)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-255.pth.tar', 79.64999997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-252.pth.tar', 79.58199998046875)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-254.pth.tar', 79.54199997558594)

Train: 263 [   0/1251 (  0%)]  Loss: 3.242 (3.24)  Time: 2.549s,  401.80/s  (2.549s,  401.80/s)  LR: 4.669e-05  Data: 1.529 (1.529)
Train: 263 [  50/1251 (  4%)]  Loss: 3.070 (3.16)  Time: 1.033s,  990.96/s  (1.040s,  984.65/s)  LR: 4.669e-05  Data: 0.012 (0.041)
Train: 263 [ 100/1251 (  8%)]  Loss: 3.121 (3.14)  Time: 1.036s,  988.17/s  (1.029s,  995.10/s)  LR: 4.669e-05  Data: 0.011 (0.027)
Train: 263 [ 150/1251 ( 12%)]  Loss: 3.348 (3.20)  Time: 1.001s, 1022.82/s  (1.022s, 1002.13/s)  LR: 4.669e-05  Data: 0.011 (0.022)
Train: 263 [ 200/1251 ( 16%)]  Loss: 2.951 (3.15)  Time: 1.008s, 1015.82/s  (1.018s, 1006.13/s)  LR: 4.669e-05  Data: 0.011 (0.019)
Train: 263 [ 250/1251 ( 20%)]  Loss: 3.120 (3.14)  Time: 1.019s, 1004.83/s  (1.015s, 1009.22/s)  LR: 4.669e-05  Data: 0.012 (0.017)
Train: 263 [ 300/1251 ( 24%)]  Loss: 2.917 (3.11)  Time: 0.993s, 1030.82/s  (1.012s, 1011.40/s)  LR: 4.669e-05  Data: 0.011 (0.016)
Train: 263 [ 350/1251 ( 28%)]  Loss: 3.295 (3.13)  Time: 1.015s, 1008.99/s  (1.010s, 1013.37/s)  LR: 4.669e-05  Data: 0.011 (0.016)
Train: 263 [ 400/1251 ( 32%)]  Loss: 3.087 (3.13)  Time: 1.007s, 1017.38/s  (1.009s, 1014.89/s)  LR: 4.669e-05  Data: 0.011 (0.015)
Train: 263 [ 450/1251 ( 36%)]  Loss: 3.097 (3.12)  Time: 0.996s, 1027.74/s  (1.008s, 1016.17/s)  LR: 4.669e-05  Data: 0.012 (0.015)
Train: 263 [ 500/1251 ( 40%)]  Loss: 2.589 (3.08)  Time: 0.997s, 1027.33/s  (1.007s, 1016.92/s)  LR: 4.669e-05  Data: 0.011 (0.014)
Train: 263 [ 550/1251 ( 44%)]  Loss: 3.299 (3.09)  Time: 0.994s, 1029.74/s  (1.006s, 1017.61/s)  LR: 4.669e-05  Data: 0.012 (0.014)
Train: 263 [ 600/1251 ( 48%)]  Loss: 3.141 (3.10)  Time: 0.993s, 1031.58/s  (1.006s, 1018.23/s)  LR: 4.669e-05  Data: 0.011 (0.014)
Train: 263 [ 650/1251 ( 52%)]  Loss: 3.023 (3.09)  Time: 1.002s, 1022.03/s  (1.005s, 1018.72/s)  LR: 4.669e-05  Data: 0.012 (0.014)
Train: 263 [ 700/1251 ( 56%)]  Loss: 3.080 (3.09)  Time: 0.991s, 1033.01/s  (1.005s, 1019.06/s)  LR: 4.669e-05  Data: 0.010 (0.014)
Train: 263 [ 750/1251 ( 60%)]  Loss: 3.086 (3.09)  Time: 0.997s, 1027.09/s  (1.005s, 1019.26/s)  LR: 4.669e-05  Data: 0.011 (0.013)
Train: 263 [ 800/1251 ( 64%)]  Loss: 3.275 (3.10)  Time: 0.994s, 1030.11/s  (1.004s, 1019.51/s)  LR: 4.669e-05  Data: 0.011 (0.013)
Train: 263 [ 850/1251 ( 68%)]  Loss: 3.160 (3.11)  Time: 0.993s, 1031.32/s  (1.004s, 1019.84/s)  LR: 4.669e-05  Data: 0.011 (0.013)
Train: 263 [ 900/1251 ( 72%)]  Loss: 3.098 (3.11)  Time: 0.995s, 1028.93/s  (1.004s, 1020.05/s)  LR: 4.669e-05  Data: 0.011 (0.013)
Train: 263 [ 950/1251 ( 76%)]  Loss: 3.332 (3.12)  Time: 0.994s, 1029.84/s  (1.004s, 1019.94/s)  LR: 4.669e-05  Data: 0.011 (0.013)
Train: 263 [1000/1251 ( 80%)]  Loss: 3.212 (3.12)  Time: 0.998s, 1026.29/s  (1.004s, 1019.98/s)  LR: 4.669e-05  Data: 0.011 (0.013)
Train: 263 [1050/1251 ( 84%)]  Loss: 2.915 (3.11)  Time: 1.000s, 1024.34/s  (1.005s, 1019.36/s)  LR: 4.669e-05  Data: 0.011 (0.013)
Train: 263 [1100/1251 ( 88%)]  Loss: 2.960 (3.11)  Time: 0.997s, 1026.94/s  (1.005s, 1019.29/s)  LR: 4.669e-05  Data: 0.012 (0.013)
Train: 263 [1150/1251 ( 92%)]  Loss: 3.326 (3.11)  Time: 0.996s, 1028.53/s  (1.005s, 1019.19/s)  LR: 4.669e-05  Data: 0.011 (0.013)
Train: 263 [1200/1251 ( 96%)]  Loss: 2.910 (3.11)  Time: 0.997s, 1026.81/s  (1.005s, 1018.92/s)  LR: 4.669e-05  Data: 0.011 (0.013)
Train: 263 [1250/1251 (100%)]  Loss: 3.067 (3.10)  Time: 1.008s, 1016.24/s  (1.005s, 1018.87/s)  LR: 4.669e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.669 (1.669)  Loss:  0.6076 (0.6076)  Acc@1: 91.6016 (91.6016)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.245 (0.570)  Loss:  0.7166 (1.0498)  Acc@1: 86.4387 (79.8980)  Acc@5: 97.7594 (94.9740)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-263.pth.tar', 79.89800005615234)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-261.pth.tar', 79.86799995361328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-260.pth.tar', 79.85600000732421)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-257.pth.tar', 79.8140000024414)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-256.pth.tar', 79.75600000488281)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-258.pth.tar', 79.68600005859375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-259.pth.tar', 79.6860000024414)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-262.pth.tar', 79.68000005371094)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-255.pth.tar', 79.64999997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-252.pth.tar', 79.58199998046875)

Train: 264 [   0/1251 (  0%)]  Loss: 3.146 (3.15)  Time: 4.332s,  236.40/s  (4.332s,  236.40/s)  LR: 4.476e-05  Data: 3.023 (3.023)
Train: 264 [  50/1251 (  4%)]  Loss: 3.479 (3.31)  Time: 1.000s, 1023.51/s  (1.069s,  957.58/s)  LR: 4.476e-05  Data: 0.011 (0.070)
Train: 264 [ 100/1251 (  8%)]  Loss: 3.293 (3.31)  Time: 1.058s,  967.83/s  (1.040s,  984.84/s)  LR: 4.476e-05  Data: 0.011 (0.041)
Train: 264 [ 150/1251 ( 12%)]  Loss: 3.278 (3.30)  Time: 0.997s, 1027.22/s  (1.029s,  995.33/s)  LR: 4.476e-05  Data: 0.011 (0.031)
Train: 264 [ 200/1251 ( 16%)]  Loss: 3.102 (3.26)  Time: 1.002s, 1022.41/s  (1.022s, 1001.88/s)  LR: 4.476e-05  Data: 0.011 (0.026)
Train: 264 [ 250/1251 ( 20%)]  Loss: 3.345 (3.27)  Time: 1.013s, 1011.21/s  (1.018s, 1006.27/s)  LR: 4.476e-05  Data: 0.010 (0.023)
Train: 264 [ 300/1251 ( 24%)]  Loss: 3.676 (3.33)  Time: 0.997s, 1026.73/s  (1.015s, 1008.74/s)  LR: 4.476e-05  Data: 0.012 (0.021)
Train: 264 [ 350/1251 ( 28%)]  Loss: 3.328 (3.33)  Time: 0.999s, 1024.53/s  (1.015s, 1009.16/s)  LR: 4.476e-05  Data: 0.012 (0.020)
Train: 264 [ 400/1251 ( 32%)]  Loss: 3.331 (3.33)  Time: 1.006s, 1018.22/s  (1.013s, 1010.56/s)  LR: 4.476e-05  Data: 0.010 (0.019)
Train: 264 [ 450/1251 ( 36%)]  Loss: 3.053 (3.30)  Time: 0.997s, 1027.26/s  (1.012s, 1011.72/s)  LR: 4.476e-05  Data: 0.011 (0.018)
Train: 264 [ 500/1251 ( 40%)]  Loss: 2.851 (3.26)  Time: 0.994s, 1029.69/s  (1.011s, 1012.80/s)  LR: 4.476e-05  Data: 0.011 (0.017)
Train: 264 [ 550/1251 ( 44%)]  Loss: 3.096 (3.25)  Time: 0.995s, 1029.39/s  (1.010s, 1014.05/s)  LR: 4.476e-05  Data: 0.011 (0.017)
Train: 264 [ 600/1251 ( 48%)]  Loss: 3.214 (3.25)  Time: 1.001s, 1022.99/s  (1.009s, 1015.13/s)  LR: 4.476e-05  Data: 0.011 (0.016)
Train: 264 [ 650/1251 ( 52%)]  Loss: 3.071 (3.23)  Time: 0.997s, 1026.66/s  (1.009s, 1014.90/s)  LR: 4.476e-05  Data: 0.012 (0.016)
Train: 264 [ 700/1251 ( 56%)]  Loss: 3.198 (3.23)  Time: 0.998s, 1026.42/s  (1.008s, 1015.60/s)  LR: 4.476e-05  Data: 0.011 (0.016)
Train: 264 [ 750/1251 ( 60%)]  Loss: 3.151 (3.23)  Time: 0.992s, 1032.26/s  (1.008s, 1016.03/s)  LR: 4.476e-05  Data: 0.011 (0.015)
Train: 264 [ 800/1251 ( 64%)]  Loss: 3.086 (3.22)  Time: 0.996s, 1028.14/s  (1.008s, 1016.29/s)  LR: 4.476e-05  Data: 0.010 (0.015)
Train: 264 [ 850/1251 ( 68%)]  Loss: 2.850 (3.20)  Time: 1.005s, 1018.89/s  (1.007s, 1016.74/s)  LR: 4.476e-05  Data: 0.012 (0.015)
Train: 264 [ 900/1251 ( 72%)]  Loss: 3.028 (3.19)  Time: 0.995s, 1028.72/s  (1.007s, 1017.00/s)  LR: 4.476e-05  Data: 0.010 (0.015)
Train: 264 [ 950/1251 ( 76%)]  Loss: 2.719 (3.16)  Time: 0.993s, 1031.40/s  (1.006s, 1017.40/s)  LR: 4.476e-05  Data: 0.011 (0.014)
Train: 264 [1000/1251 ( 80%)]  Loss: 2.904 (3.15)  Time: 1.019s, 1005.31/s  (1.006s, 1017.52/s)  LR: 4.476e-05  Data: 0.011 (0.014)
Train: 264 [1050/1251 ( 84%)]  Loss: 3.058 (3.15)  Time: 0.998s, 1026.46/s  (1.006s, 1017.71/s)  LR: 4.476e-05  Data: 0.011 (0.014)
Train: 264 [1100/1251 ( 88%)]  Loss: 3.448 (3.16)  Time: 1.000s, 1024.14/s  (1.006s, 1017.98/s)  LR: 4.476e-05  Data: 0.011 (0.014)
Train: 264 [1150/1251 ( 92%)]  Loss: 3.452 (3.17)  Time: 1.006s, 1018.21/s  (1.006s, 1018.05/s)  LR: 4.476e-05  Data: 0.011 (0.014)
Train: 264 [1200/1251 ( 96%)]  Loss: 3.231 (3.18)  Time: 0.997s, 1027.21/s  (1.006s, 1018.31/s)  LR: 4.476e-05  Data: 0.012 (0.014)
Train: 264 [1250/1251 (100%)]  Loss: 3.396 (3.18)  Time: 0.980s, 1044.73/s  (1.006s, 1018.36/s)  LR: 4.476e-05  Data: 0.000 (0.014)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.637 (1.637)  Loss:  0.5686 (0.5686)  Acc@1: 91.7969 (91.7969)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.245 (0.571)  Loss:  0.7111 (1.0217)  Acc@1: 86.6745 (80.0220)  Acc@5: 97.6415 (95.1260)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-264.pth.tar', 80.02200002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-263.pth.tar', 79.89800005615234)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-261.pth.tar', 79.86799995361328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-260.pth.tar', 79.85600000732421)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-257.pth.tar', 79.8140000024414)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-256.pth.tar', 79.75600000488281)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-258.pth.tar', 79.68600005859375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-259.pth.tar', 79.6860000024414)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-262.pth.tar', 79.68000005371094)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-255.pth.tar', 79.64999997558594)

Train: 265 [   0/1251 (  0%)]  Loss: 3.354 (3.35)  Time: 2.556s,  400.62/s  (2.556s,  400.62/s)  LR: 4.288e-05  Data: 1.588 (1.588)
Train: 265 [  50/1251 (  4%)]  Loss: 3.261 (3.31)  Time: 1.046s,  978.62/s  (1.034s,  990.12/s)  LR: 4.288e-05  Data: 0.010 (0.042)
Train: 265 [ 100/1251 (  8%)]  Loss: 3.059 (3.22)  Time: 0.996s, 1028.57/s  (1.017s, 1006.87/s)  LR: 4.288e-05  Data: 0.012 (0.027)
Train: 265 [ 150/1251 ( 12%)]  Loss: 3.293 (3.24)  Time: 1.002s, 1021.87/s  (1.013s, 1010.84/s)  LR: 4.288e-05  Data: 0.012 (0.022)
Train: 265 [ 200/1251 ( 16%)]  Loss: 3.201 (3.23)  Time: 0.996s, 1027.60/s  (1.013s, 1010.40/s)  LR: 4.288e-05  Data: 0.011 (0.019)
Train: 265 [ 250/1251 ( 20%)]  Loss: 3.097 (3.21)  Time: 0.995s, 1029.00/s  (1.012s, 1011.71/s)  LR: 4.288e-05  Data: 0.011 (0.017)
Train: 265 [ 300/1251 ( 24%)]  Loss: 3.157 (3.20)  Time: 0.995s, 1029.48/s  (1.011s, 1012.89/s)  LR: 4.288e-05  Data: 0.011 (0.016)
Train: 265 [ 350/1251 ( 28%)]  Loss: 3.095 (3.19)  Time: 0.996s, 1028.18/s  (1.009s, 1014.45/s)  LR: 4.288e-05  Data: 0.012 (0.016)
Train: 265 [ 400/1251 ( 32%)]  Loss: 3.321 (3.20)  Time: 0.993s, 1030.97/s  (1.008s, 1015.78/s)  LR: 4.288e-05  Data: 0.012 (0.015)
Train: 265 [ 450/1251 ( 36%)]  Loss: 2.894 (3.17)  Time: 1.053s,  972.43/s  (1.009s, 1014.81/s)  LR: 4.288e-05  Data: 0.015 (0.015)
Train: 265 [ 500/1251 ( 40%)]  Loss: 3.158 (3.17)  Time: 1.003s, 1021.22/s  (1.009s, 1015.15/s)  LR: 4.288e-05  Data: 0.011 (0.014)
Train: 265 [ 550/1251 ( 44%)]  Loss: 3.466 (3.20)  Time: 1.034s,  990.69/s  (1.010s, 1014.17/s)  LR: 4.288e-05  Data: 0.018 (0.014)
Train: 265 [ 600/1251 ( 48%)]  Loss: 3.024 (3.18)  Time: 1.008s, 1015.89/s  (1.009s, 1015.20/s)  LR: 4.288e-05  Data: 0.011 (0.014)
Train: 265 [ 650/1251 ( 52%)]  Loss: 3.181 (3.18)  Time: 0.999s, 1025.42/s  (1.008s, 1015.60/s)  LR: 4.288e-05  Data: 0.012 (0.014)
Train: 265 [ 700/1251 ( 56%)]  Loss: 3.203 (3.18)  Time: 0.997s, 1027.45/s  (1.008s, 1016.28/s)  LR: 4.288e-05  Data: 0.011 (0.013)
Train: 265 [ 750/1251 ( 60%)]  Loss: 3.515 (3.20)  Time: 1.005s, 1018.54/s  (1.007s, 1017.01/s)  LR: 4.288e-05  Data: 0.011 (0.013)
Train: 265 [ 800/1251 ( 64%)]  Loss: 3.312 (3.21)  Time: 1.000s, 1024.14/s  (1.007s, 1017.37/s)  LR: 4.288e-05  Data: 0.011 (0.013)
Train: 265 [ 850/1251 ( 68%)]  Loss: 3.327 (3.22)  Time: 0.997s, 1027.46/s  (1.006s, 1017.66/s)  LR: 4.288e-05  Data: 0.010 (0.013)
Train: 265 [ 900/1251 ( 72%)]  Loss: 3.317 (3.22)  Time: 1.000s, 1023.70/s  (1.006s, 1018.04/s)  LR: 4.288e-05  Data: 0.011 (0.013)
Train: 265 [ 950/1251 ( 76%)]  Loss: 3.104 (3.22)  Time: 1.004s, 1019.60/s  (1.005s, 1018.40/s)  LR: 4.288e-05  Data: 0.012 (0.013)
Train: 265 [1000/1251 ( 80%)]  Loss: 3.212 (3.22)  Time: 0.997s, 1027.39/s  (1.005s, 1018.62/s)  LR: 4.288e-05  Data: 0.011 (0.013)
Train: 265 [1050/1251 ( 84%)]  Loss: 2.717 (3.19)  Time: 0.994s, 1029.82/s  (1.005s, 1018.84/s)  LR: 4.288e-05  Data: 0.011 (0.013)
Train: 265 [1100/1251 ( 88%)]  Loss: 2.635 (3.17)  Time: 0.996s, 1027.70/s  (1.005s, 1019.12/s)  LR: 4.288e-05  Data: 0.011 (0.013)
Train: 265 [1150/1251 ( 92%)]  Loss: 2.983 (3.16)  Time: 1.004s, 1019.48/s  (1.005s, 1019.05/s)  LR: 4.288e-05  Data: 0.012 (0.013)
Train: 265 [1200/1251 ( 96%)]  Loss: 3.044 (3.16)  Time: 1.058s,  967.55/s  (1.005s, 1019.22/s)  LR: 4.288e-05  Data: 0.011 (0.012)
Train: 265 [1250/1251 (100%)]  Loss: 3.100 (3.15)  Time: 0.987s, 1037.29/s  (1.005s, 1019.38/s)  LR: 4.288e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.685 (1.685)  Loss:  0.6606 (0.6606)  Acc@1: 91.8945 (91.8945)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.245 (0.567)  Loss:  0.8175 (1.1200)  Acc@1: 86.6745 (79.9000)  Acc@5: 97.4057 (95.0660)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-264.pth.tar', 80.02200002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-265.pth.tar', 79.90000002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-263.pth.tar', 79.89800005615234)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-261.pth.tar', 79.86799995361328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-260.pth.tar', 79.85600000732421)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-257.pth.tar', 79.8140000024414)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-256.pth.tar', 79.75600000488281)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-258.pth.tar', 79.68600005859375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-259.pth.tar', 79.6860000024414)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-262.pth.tar', 79.68000005371094)

Train: 266 [   0/1251 (  0%)]  Loss: 3.280 (3.28)  Time: 2.453s,  417.41/s  (2.453s,  417.41/s)  LR: 4.105e-05  Data: 1.446 (1.446)
Train: 266 [  50/1251 (  4%)]  Loss: 3.219 (3.25)  Time: 0.997s, 1027.45/s  (1.027s,  996.62/s)  LR: 4.105e-05  Data: 0.013 (0.040)
Train: 266 [ 100/1251 (  8%)]  Loss: 2.835 (3.11)  Time: 0.998s, 1026.05/s  (1.022s, 1001.70/s)  LR: 4.105e-05  Data: 0.010 (0.026)
Train: 266 [ 150/1251 ( 12%)]  Loss: 3.346 (3.17)  Time: 1.005s, 1019.38/s  (1.016s, 1007.47/s)  LR: 4.105e-05  Data: 0.011 (0.021)
Train: 266 [ 200/1251 ( 16%)]  Loss: 3.084 (3.15)  Time: 0.997s, 1026.91/s  (1.014s, 1009.81/s)  LR: 4.105e-05  Data: 0.011 (0.018)
Train: 266 [ 250/1251 ( 20%)]  Loss: 2.827 (3.10)  Time: 0.999s, 1025.39/s  (1.013s, 1010.45/s)  LR: 4.105e-05  Data: 0.011 (0.017)
Train: 266 [ 300/1251 ( 24%)]  Loss: 3.359 (3.14)  Time: 1.031s,  993.01/s  (1.011s, 1012.65/s)  LR: 4.105e-05  Data: 0.011 (0.016)
Train: 266 [ 350/1251 ( 28%)]  Loss: 2.941 (3.11)  Time: 1.002s, 1022.08/s  (1.010s, 1013.59/s)  LR: 4.105e-05  Data: 0.012 (0.015)
Train: 266 [ 400/1251 ( 32%)]  Loss: 3.498 (3.15)  Time: 0.998s, 1026.13/s  (1.009s, 1014.87/s)  LR: 4.105e-05  Data: 0.011 (0.015)
Train: 266 [ 450/1251 ( 36%)]  Loss: 3.109 (3.15)  Time: 0.994s, 1030.18/s  (1.008s, 1015.88/s)  LR: 4.105e-05  Data: 0.010 (0.014)
Train: 266 [ 500/1251 ( 40%)]  Loss: 3.320 (3.17)  Time: 0.994s, 1029.67/s  (1.007s, 1016.38/s)  LR: 4.105e-05  Data: 0.011 (0.014)
Train: 266 [ 550/1251 ( 44%)]  Loss: 2.872 (3.14)  Time: 0.996s, 1028.06/s  (1.007s, 1017.02/s)  LR: 4.105e-05  Data: 0.011 (0.014)
Train: 266 [ 600/1251 ( 48%)]  Loss: 3.265 (3.15)  Time: 1.002s, 1022.26/s  (1.007s, 1017.36/s)  LR: 4.105e-05  Data: 0.012 (0.014)
Train: 266 [ 650/1251 ( 52%)]  Loss: 3.281 (3.16)  Time: 1.002s, 1021.76/s  (1.006s, 1017.93/s)  LR: 4.105e-05  Data: 0.011 (0.013)
Train: 266 [ 700/1251 ( 56%)]  Loss: 3.273 (3.17)  Time: 0.992s, 1031.75/s  (1.006s, 1018.36/s)  LR: 4.105e-05  Data: 0.010 (0.013)
Train: 266 [ 750/1251 ( 60%)]  Loss: 3.030 (3.16)  Time: 0.998s, 1026.36/s  (1.005s, 1018.47/s)  LR: 4.105e-05  Data: 0.011 (0.013)
Train: 266 [ 800/1251 ( 64%)]  Loss: 3.164 (3.16)  Time: 1.035s,  989.44/s  (1.005s, 1018.56/s)  LR: 4.105e-05  Data: 0.011 (0.013)
Train: 266 [ 850/1251 ( 68%)]  Loss: 3.387 (3.17)  Time: 0.997s, 1026.72/s  (1.005s, 1018.76/s)  LR: 4.105e-05  Data: 0.011 (0.013)
Train: 266 [ 900/1251 ( 72%)]  Loss: 3.196 (3.17)  Time: 1.001s, 1022.57/s  (1.006s, 1018.23/s)  LR: 4.105e-05  Data: 0.011 (0.013)
Train: 266 [ 950/1251 ( 76%)]  Loss: 3.000 (3.16)  Time: 0.995s, 1029.36/s  (1.006s, 1018.19/s)  LR: 4.105e-05  Data: 0.010 (0.013)
Train: 266 [1000/1251 ( 80%)]  Loss: 2.766 (3.15)  Time: 0.993s, 1031.52/s  (1.006s, 1018.39/s)  LR: 4.105e-05  Data: 0.011 (0.013)
Train: 266 [1050/1251 ( 84%)]  Loss: 3.058 (3.14)  Time: 0.995s, 1029.53/s  (1.005s, 1018.70/s)  LR: 4.105e-05  Data: 0.011 (0.013)
Train: 266 [1100/1251 ( 88%)]  Loss: 3.231 (3.15)  Time: 0.993s, 1030.85/s  (1.005s, 1018.84/s)  LR: 4.105e-05  Data: 0.011 (0.013)
Train: 266 [1150/1251 ( 92%)]  Loss: 3.009 (3.14)  Time: 1.038s,  986.60/s  (1.005s, 1018.94/s)  LR: 4.105e-05  Data: 0.012 (0.012)
Train: 266 [1200/1251 ( 96%)]  Loss: 2.813 (3.13)  Time: 0.999s, 1025.16/s  (1.005s, 1019.18/s)  LR: 4.105e-05  Data: 0.011 (0.012)
Train: 266 [1250/1251 (100%)]  Loss: 3.341 (3.13)  Time: 1.022s, 1001.76/s  (1.005s, 1018.47/s)  LR: 4.105e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.643 (1.643)  Loss:  0.6933 (0.6933)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.245 (0.572)  Loss:  0.8243 (1.1394)  Acc@1: 87.2642 (79.8400)  Acc@5: 97.6415 (95.0280)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-264.pth.tar', 80.02200002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-265.pth.tar', 79.90000002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-263.pth.tar', 79.89800005615234)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-261.pth.tar', 79.86799995361328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-260.pth.tar', 79.85600000732421)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-266.pth.tar', 79.84000015625)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-257.pth.tar', 79.8140000024414)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-256.pth.tar', 79.75600000488281)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-258.pth.tar', 79.68600005859375)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-259.pth.tar', 79.6860000024414)

Train: 267 [   0/1251 (  0%)]  Loss: 3.095 (3.10)  Time: 2.706s,  378.41/s  (2.706s,  378.41/s)  LR: 3.926e-05  Data: 1.742 (1.742)
Train: 267 [  50/1251 (  4%)]  Loss: 2.989 (3.04)  Time: 0.997s, 1026.66/s  (1.038s,  986.27/s)  LR: 3.926e-05  Data: 0.012 (0.046)
Train: 267 [ 100/1251 (  8%)]  Loss: 2.978 (3.02)  Time: 0.996s, 1028.26/s  (1.024s, 1000.29/s)  LR: 3.926e-05  Data: 0.011 (0.029)
Train: 267 [ 150/1251 ( 12%)]  Loss: 3.013 (3.02)  Time: 1.046s,  978.53/s  (1.015s, 1008.39/s)  LR: 3.926e-05  Data: 0.010 (0.023)
Train: 267 [ 200/1251 ( 16%)]  Loss: 3.106 (3.04)  Time: 0.996s, 1028.28/s  (1.012s, 1012.34/s)  LR: 3.926e-05  Data: 0.011 (0.020)
Train: 267 [ 250/1251 ( 20%)]  Loss: 2.942 (3.02)  Time: 1.061s,  965.48/s  (1.010s, 1014.05/s)  LR: 3.926e-05  Data: 0.011 (0.018)
Train: 267 [ 300/1251 ( 24%)]  Loss: 2.892 (3.00)  Time: 1.051s,  974.57/s  (1.009s, 1014.63/s)  LR: 3.926e-05  Data: 0.011 (0.017)
Train: 267 [ 350/1251 ( 28%)]  Loss: 3.151 (3.02)  Time: 0.994s, 1030.31/s  (1.009s, 1014.51/s)  LR: 3.926e-05  Data: 0.010 (0.016)
Train: 267 [ 400/1251 ( 32%)]  Loss: 3.115 (3.03)  Time: 0.996s, 1027.61/s  (1.008s, 1015.74/s)  LR: 3.926e-05  Data: 0.011 (0.016)
Train: 267 [ 450/1251 ( 36%)]  Loss: 3.130 (3.04)  Time: 0.997s, 1027.05/s  (1.008s, 1016.36/s)  LR: 3.926e-05  Data: 0.012 (0.015)
Train: 267 [ 500/1251 ( 40%)]  Loss: 3.071 (3.04)  Time: 1.028s,  996.36/s  (1.007s, 1016.71/s)  LR: 3.926e-05  Data: 0.011 (0.015)
Train: 267 [ 550/1251 ( 44%)]  Loss: 3.385 (3.07)  Time: 0.995s, 1029.01/s  (1.007s, 1017.25/s)  LR: 3.926e-05  Data: 0.012 (0.014)
Train: 267 [ 600/1251 ( 48%)]  Loss: 2.697 (3.04)  Time: 1.000s, 1023.85/s  (1.007s, 1017.20/s)  LR: 3.926e-05  Data: 0.011 (0.014)
Train: 267 [ 650/1251 ( 52%)]  Loss: 2.815 (3.03)  Time: 0.993s, 1031.27/s  (1.006s, 1017.75/s)  LR: 3.926e-05  Data: 0.011 (0.014)
Train: 267 [ 700/1251 ( 56%)]  Loss: 2.891 (3.02)  Time: 1.006s, 1017.90/s  (1.006s, 1017.79/s)  LR: 3.926e-05  Data: 0.011 (0.014)
Train: 267 [ 750/1251 ( 60%)]  Loss: 3.159 (3.03)  Time: 0.999s, 1024.95/s  (1.006s, 1017.96/s)  LR: 3.926e-05  Data: 0.012 (0.014)
Train: 267 [ 800/1251 ( 64%)]  Loss: 3.025 (3.03)  Time: 1.000s, 1023.81/s  (1.006s, 1018.26/s)  LR: 3.926e-05  Data: 0.010 (0.013)
Train: 267 [ 850/1251 ( 68%)]  Loss: 3.174 (3.03)  Time: 0.992s, 1032.63/s  (1.005s, 1018.66/s)  LR: 3.926e-05  Data: 0.010 (0.013)
Train: 267 [ 900/1251 ( 72%)]  Loss: 2.798 (3.02)  Time: 0.996s, 1027.79/s  (1.005s, 1018.96/s)  LR: 3.926e-05  Data: 0.011 (0.013)
Train: 267 [ 950/1251 ( 76%)]  Loss: 3.370 (3.04)  Time: 0.994s, 1029.71/s  (1.005s, 1019.28/s)  LR: 3.926e-05  Data: 0.011 (0.013)
Train: 267 [1000/1251 ( 80%)]  Loss: 3.309 (3.05)  Time: 0.994s, 1030.06/s  (1.004s, 1019.55/s)  LR: 3.926e-05  Data: 0.011 (0.013)
Train: 267 [1050/1251 ( 84%)]  Loss: 3.285 (3.06)  Time: 0.993s, 1031.07/s  (1.004s, 1019.93/s)  LR: 3.926e-05  Data: 0.011 (0.013)
Train: 267 [1100/1251 ( 88%)]  Loss: 3.151 (3.07)  Time: 0.992s, 1032.07/s  (1.004s, 1020.20/s)  LR: 3.926e-05  Data: 0.010 (0.013)
Train: 267 [1150/1251 ( 92%)]  Loss: 3.352 (3.08)  Time: 1.043s,  981.75/s  (1.004s, 1020.34/s)  LR: 3.926e-05  Data: 0.010 (0.013)
Train: 267 [1200/1251 ( 96%)]  Loss: 2.752 (3.07)  Time: 1.051s,  974.37/s  (1.004s, 1019.92/s)  LR: 3.926e-05  Data: 0.012 (0.013)
Train: 267 [1250/1251 (100%)]  Loss: 3.268 (3.07)  Time: 0.980s, 1044.71/s  (1.005s, 1019.29/s)  LR: 3.926e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.652 (1.652)  Loss:  0.6643 (0.6643)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.246 (0.571)  Loss:  0.7728 (1.1083)  Acc@1: 86.4387 (80.0020)  Acc@5: 97.6415 (94.9680)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-264.pth.tar', 80.02200002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-267.pth.tar', 80.00200005615234)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-265.pth.tar', 79.90000002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-263.pth.tar', 79.89800005615234)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-261.pth.tar', 79.86799995361328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-260.pth.tar', 79.85600000732421)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-266.pth.tar', 79.84000015625)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-257.pth.tar', 79.8140000024414)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-256.pth.tar', 79.75600000488281)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-258.pth.tar', 79.68600005859375)

Train: 268 [   0/1251 (  0%)]  Loss: 3.177 (3.18)  Time: 2.548s,  401.82/s  (2.548s,  401.82/s)  LR: 3.753e-05  Data: 1.583 (1.583)
Train: 268 [  50/1251 (  4%)]  Loss: 2.728 (2.95)  Time: 1.000s, 1023.49/s  (1.033s,  990.96/s)  LR: 3.753e-05  Data: 0.012 (0.042)
Train: 268 [ 100/1251 (  8%)]  Loss: 3.328 (3.08)  Time: 0.998s, 1025.83/s  (1.018s, 1005.84/s)  LR: 3.753e-05  Data: 0.011 (0.027)
Train: 268 [ 150/1251 ( 12%)]  Loss: 3.171 (3.10)  Time: 0.994s, 1029.94/s  (1.013s, 1011.31/s)  LR: 3.753e-05  Data: 0.011 (0.022)
Train: 268 [ 200/1251 ( 16%)]  Loss: 2.715 (3.02)  Time: 1.000s, 1024.34/s  (1.009s, 1014.63/s)  LR: 3.753e-05  Data: 0.012 (0.019)
Train: 268 [ 250/1251 ( 20%)]  Loss: 2.810 (2.99)  Time: 1.000s, 1023.63/s  (1.007s, 1016.40/s)  LR: 3.753e-05  Data: 0.012 (0.018)
Train: 268 [ 300/1251 ( 24%)]  Loss: 2.963 (2.98)  Time: 1.061s,  964.71/s  (1.009s, 1014.70/s)  LR: 3.753e-05  Data: 0.011 (0.017)
Train: 268 [ 350/1251 ( 28%)]  Loss: 3.223 (3.01)  Time: 0.998s, 1026.42/s  (1.011s, 1012.97/s)  LR: 3.753e-05  Data: 0.012 (0.016)
Train: 268 [ 400/1251 ( 32%)]  Loss: 3.476 (3.07)  Time: 1.004s, 1019.63/s  (1.011s, 1012.56/s)  LR: 3.753e-05  Data: 0.014 (0.015)
Train: 268 [ 450/1251 ( 36%)]  Loss: 3.019 (3.06)  Time: 1.001s, 1023.32/s  (1.010s, 1013.85/s)  LR: 3.753e-05  Data: 0.012 (0.015)
Train: 268 [ 500/1251 ( 40%)]  Loss: 3.181 (3.07)  Time: 1.054s,  971.18/s  (1.009s, 1014.63/s)  LR: 3.753e-05  Data: 0.012 (0.015)
Train: 268 [ 550/1251 ( 44%)]  Loss: 3.531 (3.11)  Time: 0.998s, 1026.07/s  (1.008s, 1015.45/s)  LR: 3.753e-05  Data: 0.011 (0.014)
Train: 268 [ 600/1251 ( 48%)]  Loss: 2.880 (3.09)  Time: 1.034s,  990.20/s  (1.008s, 1016.24/s)  LR: 3.753e-05  Data: 0.011 (0.014)
Train: 268 [ 650/1251 ( 52%)]  Loss: 3.151 (3.10)  Time: 0.996s, 1027.97/s  (1.007s, 1016.44/s)  LR: 3.753e-05  Data: 0.012 (0.014)
Train: 268 [ 700/1251 ( 56%)]  Loss: 3.142 (3.10)  Time: 0.997s, 1027.45/s  (1.007s, 1017.11/s)  LR: 3.753e-05  Data: 0.011 (0.014)
Train: 268 [ 750/1251 ( 60%)]  Loss: 3.489 (3.12)  Time: 0.999s, 1024.61/s  (1.006s, 1017.50/s)  LR: 3.753e-05  Data: 0.010 (0.013)
Train: 268 [ 800/1251 ( 64%)]  Loss: 3.192 (3.13)  Time: 0.997s, 1027.23/s  (1.007s, 1016.92/s)  LR: 3.753e-05  Data: 0.011 (0.013)
Train: 268 [ 850/1251 ( 68%)]  Loss: 3.375 (3.14)  Time: 1.061s,  965.32/s  (1.007s, 1016.99/s)  LR: 3.753e-05  Data: 0.011 (0.013)
Train: 268 [ 900/1251 ( 72%)]  Loss: 3.094 (3.14)  Time: 0.999s, 1024.72/s  (1.007s, 1016.56/s)  LR: 3.753e-05  Data: 0.010 (0.013)
Train: 268 [ 950/1251 ( 76%)]  Loss: 3.029 (3.13)  Time: 0.991s, 1032.79/s  (1.007s, 1016.96/s)  LR: 3.753e-05  Data: 0.010 (0.013)
Train: 268 [1000/1251 ( 80%)]  Loss: 3.161 (3.13)  Time: 0.996s, 1027.85/s  (1.007s, 1017.16/s)  LR: 3.753e-05  Data: 0.011 (0.013)
Train: 268 [1050/1251 ( 84%)]  Loss: 3.170 (3.14)  Time: 0.996s, 1027.88/s  (1.006s, 1017.55/s)  LR: 3.753e-05  Data: 0.012 (0.013)
Train: 268 [1100/1251 ( 88%)]  Loss: 3.034 (3.13)  Time: 1.009s, 1014.42/s  (1.006s, 1017.45/s)  LR: 3.753e-05  Data: 0.011 (0.013)
Train: 268 [1150/1251 ( 92%)]  Loss: 3.065 (3.13)  Time: 0.998s, 1026.32/s  (1.006s, 1017.59/s)  LR: 3.753e-05  Data: 0.012 (0.013)
Train: 268 [1200/1251 ( 96%)]  Loss: 2.662 (3.11)  Time: 0.999s, 1024.77/s  (1.006s, 1017.79/s)  LR: 3.753e-05  Data: 0.011 (0.013)
Train: 268 [1250/1251 (100%)]  Loss: 3.128 (3.11)  Time: 1.000s, 1023.56/s  (1.007s, 1016.57/s)  LR: 3.753e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.625 (1.625)  Loss:  0.6202 (0.6202)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.245 (0.570)  Loss:  0.7489 (1.0666)  Acc@1: 86.9104 (80.0280)  Acc@5: 97.9953 (95.1000)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-268.pth.tar', 80.0280000024414)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-264.pth.tar', 80.02200002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-267.pth.tar', 80.00200005615234)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-265.pth.tar', 79.90000002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-263.pth.tar', 79.89800005615234)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-261.pth.tar', 79.86799995361328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-260.pth.tar', 79.85600000732421)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-266.pth.tar', 79.84000015625)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-257.pth.tar', 79.8140000024414)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-256.pth.tar', 79.75600000488281)

Train: 269 [   0/1251 (  0%)]  Loss: 2.849 (2.85)  Time: 2.417s,  423.62/s  (2.417s,  423.62/s)  LR: 3.585e-05  Data: 1.468 (1.468)
Train: 269 [  50/1251 (  4%)]  Loss: 3.185 (3.02)  Time: 1.064s,  962.17/s  (1.038s,  986.73/s)  LR: 3.585e-05  Data: 0.012 (0.040)
Train: 269 [ 100/1251 (  8%)]  Loss: 3.639 (3.22)  Time: 1.039s,  985.12/s  (1.034s,  990.35/s)  LR: 3.585e-05  Data: 0.011 (0.026)
Train: 269 [ 150/1251 ( 12%)]  Loss: 3.176 (3.21)  Time: 0.995s, 1028.82/s  (1.023s, 1000.67/s)  LR: 3.585e-05  Data: 0.010 (0.021)
Train: 269 [ 200/1251 ( 16%)]  Loss: 3.183 (3.21)  Time: 0.995s, 1029.17/s  (1.018s, 1005.60/s)  LR: 3.585e-05  Data: 0.011 (0.018)
Train: 269 [ 250/1251 ( 20%)]  Loss: 3.219 (3.21)  Time: 0.999s, 1024.71/s  (1.015s, 1008.95/s)  LR: 3.585e-05  Data: 0.011 (0.017)
Train: 269 [ 300/1251 ( 24%)]  Loss: 3.230 (3.21)  Time: 0.999s, 1025.30/s  (1.013s, 1011.03/s)  LR: 3.585e-05  Data: 0.012 (0.016)
Train: 269 [ 350/1251 ( 28%)]  Loss: 2.556 (3.13)  Time: 0.997s, 1027.59/s  (1.012s, 1012.22/s)  LR: 3.585e-05  Data: 0.011 (0.015)
Train: 269 [ 400/1251 ( 32%)]  Loss: 3.041 (3.12)  Time: 1.020s, 1004.18/s  (1.010s, 1013.92/s)  LR: 3.585e-05  Data: 0.010 (0.015)
Train: 269 [ 450/1251 ( 36%)]  Loss: 3.379 (3.15)  Time: 0.991s, 1032.94/s  (1.009s, 1014.58/s)  LR: 3.585e-05  Data: 0.010 (0.014)
Train: 269 [ 500/1251 ( 40%)]  Loss: 2.882 (3.12)  Time: 0.992s, 1032.21/s  (1.009s, 1015.19/s)  LR: 3.585e-05  Data: 0.011 (0.014)
Train: 269 [ 550/1251 ( 44%)]  Loss: 3.542 (3.16)  Time: 0.995s, 1028.70/s  (1.008s, 1015.67/s)  LR: 3.585e-05  Data: 0.011 (0.014)
Train: 269 [ 600/1251 ( 48%)]  Loss: 3.187 (3.16)  Time: 1.057s,  968.99/s  (1.009s, 1015.18/s)  LR: 3.585e-05  Data: 0.015 (0.014)
Train: 269 [ 650/1251 ( 52%)]  Loss: 3.159 (3.16)  Time: 1.002s, 1022.09/s  (1.008s, 1015.71/s)  LR: 3.585e-05  Data: 0.011 (0.013)
Train: 269 [ 700/1251 ( 56%)]  Loss: 3.209 (3.16)  Time: 0.995s, 1029.30/s  (1.007s, 1016.40/s)  LR: 3.585e-05  Data: 0.011 (0.013)
Train: 269 [ 750/1251 ( 60%)]  Loss: 3.376 (3.18)  Time: 0.999s, 1024.62/s  (1.008s, 1016.25/s)  LR: 3.585e-05  Data: 0.011 (0.013)
Train: 269 [ 800/1251 ( 64%)]  Loss: 2.920 (3.16)  Time: 1.033s,  991.53/s  (1.007s, 1016.49/s)  LR: 3.585e-05  Data: 0.011 (0.013)
Train: 269 [ 850/1251 ( 68%)]  Loss: 2.967 (3.15)  Time: 0.995s, 1028.90/s  (1.007s, 1016.76/s)  LR: 3.585e-05  Data: 0.010 (0.013)
Train: 269 [ 900/1251 ( 72%)]  Loss: 3.402 (3.16)  Time: 0.995s, 1028.91/s  (1.007s, 1016.99/s)  LR: 3.585e-05  Data: 0.012 (0.013)
Train: 269 [ 950/1251 ( 76%)]  Loss: 3.320 (3.17)  Time: 0.997s, 1027.04/s  (1.007s, 1017.33/s)  LR: 3.585e-05  Data: 0.011 (0.013)
Train: 269 [1000/1251 ( 80%)]  Loss: 3.456 (3.18)  Time: 1.013s, 1011.19/s  (1.006s, 1017.49/s)  LR: 3.585e-05  Data: 0.011 (0.013)
Train: 269 [1050/1251 ( 84%)]  Loss: 3.074 (3.18)  Time: 0.998s, 1026.41/s  (1.006s, 1017.64/s)  LR: 3.585e-05  Data: 0.011 (0.013)
Train: 269 [1100/1251 ( 88%)]  Loss: 2.997 (3.17)  Time: 0.997s, 1026.60/s  (1.006s, 1017.95/s)  LR: 3.585e-05  Data: 0.011 (0.012)
Train: 269 [1150/1251 ( 92%)]  Loss: 3.000 (3.16)  Time: 1.049s,  976.62/s  (1.006s, 1018.22/s)  LR: 3.585e-05  Data: 0.010 (0.012)
Train: 269 [1200/1251 ( 96%)]  Loss: 2.823 (3.15)  Time: 0.996s, 1028.22/s  (1.005s, 1018.49/s)  LR: 3.585e-05  Data: 0.012 (0.012)
Train: 269 [1250/1251 (100%)]  Loss: 3.164 (3.15)  Time: 0.987s, 1037.65/s  (1.005s, 1018.68/s)  LR: 3.585e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.615 (1.615)  Loss:  0.7159 (0.7159)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.245 (0.566)  Loss:  0.8580 (1.1768)  Acc@1: 87.0283 (79.9300)  Acc@5: 97.8774 (95.0280)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-268.pth.tar', 80.0280000024414)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-264.pth.tar', 80.02200002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-267.pth.tar', 80.00200005615234)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-269.pth.tar', 79.9299999243164)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-265.pth.tar', 79.90000002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-263.pth.tar', 79.89800005615234)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-261.pth.tar', 79.86799995361328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-260.pth.tar', 79.85600000732421)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-266.pth.tar', 79.84000015625)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-257.pth.tar', 79.8140000024414)

Train: 270 [   0/1251 (  0%)]  Loss: 3.176 (3.18)  Time: 2.480s,  412.88/s  (2.480s,  412.88/s)  LR: 3.423e-05  Data: 1.518 (1.518)
Train: 270 [  50/1251 (  4%)]  Loss: 3.260 (3.22)  Time: 1.037s,  987.70/s  (1.056s,  969.97/s)  LR: 3.423e-05  Data: 0.010 (0.042)
Train: 270 [ 100/1251 (  8%)]  Loss: 3.037 (3.16)  Time: 0.997s, 1026.59/s  (1.033s,  991.69/s)  LR: 3.423e-05  Data: 0.011 (0.027)
Train: 270 [ 150/1251 ( 12%)]  Loss: 3.261 (3.18)  Time: 0.995s, 1029.58/s  (1.022s, 1001.55/s)  LR: 3.423e-05  Data: 0.011 (0.021)
Train: 270 [ 200/1251 ( 16%)]  Loss: 3.112 (3.17)  Time: 0.998s, 1026.16/s  (1.018s, 1006.14/s)  LR: 3.423e-05  Data: 0.011 (0.019)
Train: 270 [ 250/1251 ( 20%)]  Loss: 3.256 (3.18)  Time: 1.006s, 1018.07/s  (1.015s, 1009.00/s)  LR: 3.423e-05  Data: 0.011 (0.017)
Train: 270 [ 300/1251 ( 24%)]  Loss: 3.175 (3.18)  Time: 0.997s, 1027.17/s  (1.012s, 1011.56/s)  LR: 3.423e-05  Data: 0.011 (0.016)
Train: 270 [ 350/1251 ( 28%)]  Loss: 3.031 (3.16)  Time: 0.995s, 1029.00/s  (1.012s, 1011.97/s)  LR: 3.423e-05  Data: 0.010 (0.015)
Train: 270 [ 400/1251 ( 32%)]  Loss: 3.026 (3.15)  Time: 0.997s, 1027.38/s  (1.011s, 1013.19/s)  LR: 3.423e-05  Data: 0.011 (0.015)
Train: 270 [ 450/1251 ( 36%)]  Loss: 3.094 (3.14)  Time: 0.995s, 1029.02/s  (1.009s, 1014.49/s)  LR: 3.423e-05  Data: 0.011 (0.014)
Train: 270 [ 500/1251 ( 40%)]  Loss: 3.244 (3.15)  Time: 1.045s,  980.14/s  (1.009s, 1015.24/s)  LR: 3.423e-05  Data: 0.010 (0.014)
Train: 270 [ 550/1251 ( 44%)]  Loss: 3.051 (3.14)  Time: 0.995s, 1029.15/s  (1.008s, 1015.77/s)  LR: 3.423e-05  Data: 0.011 (0.014)
Train: 270 [ 600/1251 ( 48%)]  Loss: 3.128 (3.14)  Time: 0.995s, 1029.29/s  (1.008s, 1016.04/s)  LR: 3.423e-05  Data: 0.012 (0.014)
Train: 270 [ 650/1251 ( 52%)]  Loss: 2.845 (3.12)  Time: 0.995s, 1028.65/s  (1.007s, 1016.48/s)  LR: 3.423e-05  Data: 0.011 (0.013)
Train: 270 [ 700/1251 ( 56%)]  Loss: 3.128 (3.12)  Time: 1.032s,  992.27/s  (1.007s, 1016.63/s)  LR: 3.423e-05  Data: 0.011 (0.013)
Train: 270 [ 750/1251 ( 60%)]  Loss: 3.080 (3.12)  Time: 0.994s, 1030.16/s  (1.007s, 1016.62/s)  LR: 3.423e-05  Data: 0.011 (0.013)
Train: 270 [ 800/1251 ( 64%)]  Loss: 3.185 (3.12)  Time: 0.994s, 1029.67/s  (1.007s, 1017.26/s)  LR: 3.423e-05  Data: 0.010 (0.013)
Train: 270 [ 850/1251 ( 68%)]  Loss: 3.371 (3.14)  Time: 0.998s, 1026.17/s  (1.006s, 1017.65/s)  LR: 3.423e-05  Data: 0.012 (0.013)
Train: 270 [ 900/1251 ( 72%)]  Loss: 2.876 (3.12)  Time: 0.998s, 1025.70/s  (1.006s, 1017.84/s)  LR: 3.423e-05  Data: 0.012 (0.013)
Train: 270 [ 950/1251 ( 76%)]  Loss: 2.842 (3.11)  Time: 0.996s, 1028.37/s  (1.007s, 1017.19/s)  LR: 3.423e-05  Data: 0.011 (0.013)
Train: 270 [1000/1251 ( 80%)]  Loss: 3.052 (3.11)  Time: 0.995s, 1029.66/s  (1.006s, 1017.49/s)  LR: 3.423e-05  Data: 0.011 (0.013)
Train: 270 [1050/1251 ( 84%)]  Loss: 3.214 (3.11)  Time: 0.994s, 1030.42/s  (1.006s, 1017.48/s)  LR: 3.423e-05  Data: 0.011 (0.012)
Train: 270 [1100/1251 ( 88%)]  Loss: 3.047 (3.11)  Time: 0.998s, 1026.23/s  (1.006s, 1017.84/s)  LR: 3.423e-05  Data: 0.011 (0.012)
Train: 270 [1150/1251 ( 92%)]  Loss: 3.125 (3.11)  Time: 1.004s, 1020.01/s  (1.006s, 1017.53/s)  LR: 3.423e-05  Data: 0.012 (0.012)
Train: 270 [1200/1251 ( 96%)]  Loss: 3.032 (3.11)  Time: 0.995s, 1028.80/s  (1.006s, 1017.72/s)  LR: 3.423e-05  Data: 0.011 (0.012)
Train: 270 [1250/1251 (100%)]  Loss: 3.221 (3.11)  Time: 0.986s, 1038.83/s  (1.006s, 1017.71/s)  LR: 3.423e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.623 (1.623)  Loss:  0.6527 (0.6527)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.245 (0.576)  Loss:  0.7540 (1.0979)  Acc@1: 87.2642 (80.0220)  Acc@5: 97.7594 (95.1100)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-268.pth.tar', 80.0280000024414)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-264.pth.tar', 80.02200002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-270.pth.tar', 80.02200002685547)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-267.pth.tar', 80.00200005615234)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-269.pth.tar', 79.9299999243164)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-265.pth.tar', 79.90000002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-263.pth.tar', 79.89800005615234)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-261.pth.tar', 79.86799995361328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-260.pth.tar', 79.85600000732421)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-266.pth.tar', 79.84000015625)

Train: 271 [   0/1251 (  0%)]  Loss: 2.900 (2.90)  Time: 2.478s,  413.17/s  (2.478s,  413.17/s)  LR: 3.265e-05  Data: 1.521 (1.521)
Train: 271 [  50/1251 (  4%)]  Loss: 3.308 (3.10)  Time: 0.996s, 1027.85/s  (1.033s,  991.39/s)  LR: 3.265e-05  Data: 0.011 (0.041)
Train: 271 [ 100/1251 (  8%)]  Loss: 2.964 (3.06)  Time: 1.062s,  964.34/s  (1.026s,  997.57/s)  LR: 3.265e-05  Data: 0.011 (0.026)
Train: 271 [ 150/1251 ( 12%)]  Loss: 2.993 (3.04)  Time: 0.997s, 1027.15/s  (1.024s,  999.71/s)  LR: 3.265e-05  Data: 0.012 (0.021)
Train: 271 [ 200/1251 ( 16%)]  Loss: 3.131 (3.06)  Time: 1.047s,  978.24/s  (1.018s, 1005.48/s)  LR: 3.265e-05  Data: 0.012 (0.019)
Train: 271 [ 250/1251 ( 20%)]  Loss: 3.163 (3.08)  Time: 0.997s, 1027.39/s  (1.015s, 1009.12/s)  LR: 3.265e-05  Data: 0.011 (0.017)
Train: 271 [ 300/1251 ( 24%)]  Loss: 3.181 (3.09)  Time: 0.996s, 1028.18/s  (1.013s, 1011.21/s)  LR: 3.265e-05  Data: 0.012 (0.016)
Train: 271 [ 350/1251 ( 28%)]  Loss: 3.195 (3.10)  Time: 0.994s, 1030.57/s  (1.011s, 1012.92/s)  LR: 3.265e-05  Data: 0.011 (0.015)
Train: 271 [ 400/1251 ( 32%)]  Loss: 3.405 (3.14)  Time: 1.005s, 1018.65/s  (1.010s, 1014.09/s)  LR: 3.265e-05  Data: 0.010 (0.015)
Train: 271 [ 450/1251 ( 36%)]  Loss: 3.170 (3.14)  Time: 0.991s, 1033.68/s  (1.010s, 1014.35/s)  LR: 3.265e-05  Data: 0.011 (0.014)
Train: 271 [ 500/1251 ( 40%)]  Loss: 3.181 (3.14)  Time: 0.998s, 1025.71/s  (1.009s, 1015.16/s)  LR: 3.265e-05  Data: 0.012 (0.014)
Train: 271 [ 550/1251 ( 44%)]  Loss: 3.326 (3.16)  Time: 0.997s, 1027.36/s  (1.008s, 1015.48/s)  LR: 3.265e-05  Data: 0.011 (0.014)
Train: 271 [ 600/1251 ( 48%)]  Loss: 2.894 (3.14)  Time: 1.004s, 1019.49/s  (1.008s, 1015.95/s)  LR: 3.265e-05  Data: 0.011 (0.014)
Train: 271 [ 650/1251 ( 52%)]  Loss: 3.012 (3.13)  Time: 1.003s, 1021.36/s  (1.007s, 1016.47/s)  LR: 3.265e-05  Data: 0.012 (0.013)
Train: 271 [ 700/1251 ( 56%)]  Loss: 3.005 (3.12)  Time: 1.030s,  994.09/s  (1.007s, 1016.54/s)  LR: 3.265e-05  Data: 0.011 (0.013)
Train: 271 [ 750/1251 ( 60%)]  Loss: 3.237 (3.13)  Time: 0.997s, 1027.21/s  (1.009s, 1015.05/s)  LR: 3.265e-05  Data: 0.011 (0.013)
Train: 271 [ 800/1251 ( 64%)]  Loss: 3.243 (3.14)  Time: 0.996s, 1027.62/s  (1.008s, 1015.78/s)  LR: 3.265e-05  Data: 0.010 (0.013)
Train: 271 [ 850/1251 ( 68%)]  Loss: 3.260 (3.14)  Time: 0.994s, 1030.16/s  (1.007s, 1016.42/s)  LR: 3.265e-05  Data: 0.011 (0.013)
Train: 271 [ 900/1251 ( 72%)]  Loss: 3.135 (3.14)  Time: 1.004s, 1020.36/s  (1.009s, 1014.89/s)  LR: 3.265e-05  Data: 0.011 (0.013)
Train: 271 [ 950/1251 ( 76%)]  Loss: 3.157 (3.14)  Time: 0.994s, 1029.72/s  (1.008s, 1015.54/s)  LR: 3.265e-05  Data: 0.011 (0.013)
Train: 271 [1000/1251 ( 80%)]  Loss: 3.126 (3.14)  Time: 0.996s, 1027.67/s  (1.008s, 1015.89/s)  LR: 3.265e-05  Data: 0.011 (0.013)
Train: 271 [1050/1251 ( 84%)]  Loss: 3.387 (3.15)  Time: 0.997s, 1026.94/s  (1.008s, 1016.26/s)  LR: 3.265e-05  Data: 0.011 (0.013)
Train: 271 [1100/1251 ( 88%)]  Loss: 3.263 (3.16)  Time: 1.001s, 1023.17/s  (1.007s, 1016.48/s)  LR: 3.265e-05  Data: 0.010 (0.012)
Train: 271 [1150/1251 ( 92%)]  Loss: 3.010 (3.15)  Time: 0.996s, 1028.04/s  (1.007s, 1016.53/s)  LR: 3.265e-05  Data: 0.011 (0.012)
Train: 271 [1200/1251 ( 96%)]  Loss: 3.104 (3.15)  Time: 1.017s, 1006.68/s  (1.007s, 1016.78/s)  LR: 3.265e-05  Data: 0.011 (0.012)
Train: 271 [1250/1251 (100%)]  Loss: 2.673 (3.13)  Time: 1.028s,  996.52/s  (1.007s, 1016.74/s)  LR: 3.265e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.606 (1.606)  Loss:  0.6435 (0.6435)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.245 (0.567)  Loss:  0.7470 (1.0801)  Acc@1: 87.3821 (80.0740)  Acc@5: 97.8774 (95.0960)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-271.pth.tar', 80.07399994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-268.pth.tar', 80.0280000024414)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-264.pth.tar', 80.02200002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-270.pth.tar', 80.02200002685547)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-267.pth.tar', 80.00200005615234)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-269.pth.tar', 79.9299999243164)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-265.pth.tar', 79.90000002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-263.pth.tar', 79.89800005615234)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-261.pth.tar', 79.86799995361328)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-260.pth.tar', 79.85600000732421)

Train: 272 [   0/1251 (  0%)]  Loss: 3.074 (3.07)  Time: 2.493s,  410.71/s  (2.493s,  410.71/s)  LR: 3.113e-05  Data: 1.486 (1.486)
Train: 272 [  50/1251 (  4%)]  Loss: 3.043 (3.06)  Time: 0.995s, 1029.16/s  (1.031s,  993.55/s)  LR: 3.113e-05  Data: 0.012 (0.040)
Train: 272 [ 100/1251 (  8%)]  Loss: 3.227 (3.11)  Time: 1.033s,  991.24/s  (1.018s, 1005.48/s)  LR: 3.113e-05  Data: 0.011 (0.026)
Train: 272 [ 150/1251 ( 12%)]  Loss: 2.777 (3.03)  Time: 1.009s, 1015.27/s  (1.018s, 1005.59/s)  LR: 3.113e-05  Data: 0.010 (0.021)
Train: 272 [ 200/1251 ( 16%)]  Loss: 3.384 (3.10)  Time: 0.994s, 1029.68/s  (1.014s, 1009.85/s)  LR: 3.113e-05  Data: 0.012 (0.018)
Train: 272 [ 250/1251 ( 20%)]  Loss: 3.216 (3.12)  Time: 1.047s,  977.87/s  (1.013s, 1011.23/s)  LR: 3.113e-05  Data: 0.011 (0.017)
Train: 272 [ 300/1251 ( 24%)]  Loss: 3.099 (3.12)  Time: 0.994s, 1029.95/s  (1.011s, 1012.74/s)  LR: 3.113e-05  Data: 0.011 (0.016)
Train: 272 [ 350/1251 ( 28%)]  Loss: 2.984 (3.10)  Time: 0.998s, 1025.55/s  (1.009s, 1014.45/s)  LR: 3.113e-05  Data: 0.011 (0.015)
Train: 272 [ 400/1251 ( 32%)]  Loss: 2.963 (3.09)  Time: 1.008s, 1015.62/s  (1.008s, 1015.43/s)  LR: 3.113e-05  Data: 0.013 (0.015)
Train: 272 [ 450/1251 ( 36%)]  Loss: 3.298 (3.11)  Time: 1.001s, 1023.38/s  (1.008s, 1015.84/s)  LR: 3.113e-05  Data: 0.011 (0.014)
Train: 272 [ 500/1251 ( 40%)]  Loss: 3.359 (3.13)  Time: 0.996s, 1028.17/s  (1.008s, 1015.58/s)  LR: 3.113e-05  Data: 0.012 (0.014)
Train: 272 [ 550/1251 ( 44%)]  Loss: 3.011 (3.12)  Time: 1.000s, 1024.26/s  (1.008s, 1016.30/s)  LR: 3.113e-05  Data: 0.011 (0.014)
Train: 272 [ 600/1251 ( 48%)]  Loss: 3.294 (3.13)  Time: 1.065s,  961.88/s  (1.009s, 1014.56/s)  LR: 3.113e-05  Data: 0.011 (0.014)
Train: 272 [ 650/1251 ( 52%)]  Loss: 3.413 (3.15)  Time: 1.008s, 1015.82/s  (1.010s, 1013.99/s)  LR: 3.113e-05  Data: 0.011 (0.013)
Train: 272 [ 700/1251 ( 56%)]  Loss: 3.378 (3.17)  Time: 0.995s, 1028.73/s  (1.009s, 1014.68/s)  LR: 3.113e-05  Data: 0.010 (0.013)
Train: 272 [ 750/1251 ( 60%)]  Loss: 3.345 (3.18)  Time: 0.995s, 1029.54/s  (1.009s, 1015.34/s)  LR: 3.113e-05  Data: 0.010 (0.013)
Train: 272 [ 800/1251 ( 64%)]  Loss: 2.912 (3.16)  Time: 0.998s, 1026.27/s  (1.008s, 1015.93/s)  LR: 3.113e-05  Data: 0.012 (0.013)
Train: 272 [ 850/1251 ( 68%)]  Loss: 2.961 (3.15)  Time: 0.997s, 1026.61/s  (1.008s, 1016.03/s)  LR: 3.113e-05  Data: 0.011 (0.013)
Train: 272 [ 900/1251 ( 72%)]  Loss: 3.304 (3.16)  Time: 0.996s, 1027.67/s  (1.008s, 1016.36/s)  LR: 3.113e-05  Data: 0.012 (0.013)
Train: 272 [ 950/1251 ( 76%)]  Loss: 3.319 (3.17)  Time: 1.035s,  989.00/s  (1.008s, 1016.10/s)  LR: 3.113e-05  Data: 0.011 (0.013)
Train: 272 [1000/1251 ( 80%)]  Loss: 3.103 (3.16)  Time: 0.994s, 1030.01/s  (1.008s, 1015.63/s)  LR: 3.113e-05  Data: 0.011 (0.013)
Train: 272 [1050/1251 ( 84%)]  Loss: 3.406 (3.18)  Time: 0.993s, 1031.18/s  (1.008s, 1016.02/s)  LR: 3.113e-05  Data: 0.011 (0.012)
Train: 272 [1100/1251 ( 88%)]  Loss: 3.056 (3.17)  Time: 1.032s,  991.94/s  (1.008s, 1015.98/s)  LR: 3.113e-05  Data: 0.011 (0.012)
Train: 272 [1150/1251 ( 92%)]  Loss: 3.065 (3.17)  Time: 1.003s, 1021.43/s  (1.009s, 1014.96/s)  LR: 3.113e-05  Data: 0.011 (0.012)
Train: 272 [1200/1251 ( 96%)]  Loss: 2.469 (3.14)  Time: 0.994s, 1030.54/s  (1.009s, 1015.37/s)  LR: 3.113e-05  Data: 0.011 (0.012)
Train: 272 [1250/1251 (100%)]  Loss: 3.246 (3.14)  Time: 0.983s, 1041.64/s  (1.008s, 1015.73/s)  LR: 3.113e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.610 (1.610)  Loss:  0.6254 (0.6254)  Acc@1: 92.3828 (92.3828)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.245 (0.567)  Loss:  0.7685 (1.0807)  Acc@1: 86.7925 (80.0940)  Acc@5: 97.9953 (95.1640)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-272.pth.tar', 80.0940000805664)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-271.pth.tar', 80.07399994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-268.pth.tar', 80.0280000024414)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-264.pth.tar', 80.02200002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-270.pth.tar', 80.02200002685547)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-267.pth.tar', 80.00200005615234)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-269.pth.tar', 79.9299999243164)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-265.pth.tar', 79.90000002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-263.pth.tar', 79.89800005615234)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-261.pth.tar', 79.86799995361328)

Train: 273 [   0/1251 (  0%)]  Loss: 3.032 (3.03)  Time: 4.107s,  249.35/s  (4.107s,  249.35/s)  LR: 2.965e-05  Data: 2.886 (2.886)
Train: 273 [  50/1251 (  4%)]  Loss: 3.367 (3.20)  Time: 1.031s,  992.96/s  (1.068s,  958.55/s)  LR: 2.965e-05  Data: 0.011 (0.068)
Train: 273 [ 100/1251 (  8%)]  Loss: 3.182 (3.19)  Time: 0.996s, 1028.26/s  (1.036s,  988.62/s)  LR: 2.965e-05  Data: 0.012 (0.040)
Train: 273 [ 150/1251 ( 12%)]  Loss: 3.224 (3.20)  Time: 1.014s, 1010.29/s  (1.028s,  996.35/s)  LR: 2.965e-05  Data: 0.012 (0.031)
Train: 273 [ 200/1251 ( 16%)]  Loss: 3.153 (3.19)  Time: 1.015s, 1008.64/s  (1.026s,  997.57/s)  LR: 2.965e-05  Data: 0.010 (0.026)
Train: 273 [ 250/1251 ( 20%)]  Loss: 3.347 (3.22)  Time: 0.998s, 1025.62/s  (1.023s, 1000.80/s)  LR: 2.965e-05  Data: 0.010 (0.023)
Train: 273 [ 300/1251 ( 24%)]  Loss: 3.280 (3.23)  Time: 0.996s, 1028.01/s  (1.020s, 1003.78/s)  LR: 2.965e-05  Data: 0.011 (0.021)
Train: 273 [ 350/1251 ( 28%)]  Loss: 3.147 (3.22)  Time: 0.993s, 1031.10/s  (1.017s, 1006.61/s)  LR: 2.965e-05  Data: 0.010 (0.019)
Train: 273 [ 400/1251 ( 32%)]  Loss: 3.066 (3.20)  Time: 1.000s, 1023.70/s  (1.016s, 1008.15/s)  LR: 2.965e-05  Data: 0.012 (0.018)
Train: 273 [ 450/1251 ( 36%)]  Loss: 3.257 (3.21)  Time: 0.994s, 1030.29/s  (1.014s, 1010.09/s)  LR: 2.965e-05  Data: 0.011 (0.018)
Train: 273 [ 500/1251 ( 40%)]  Loss: 3.125 (3.20)  Time: 0.991s, 1033.47/s  (1.012s, 1011.58/s)  LR: 2.965e-05  Data: 0.011 (0.017)
Train: 273 [ 550/1251 ( 44%)]  Loss: 3.258 (3.20)  Time: 0.996s, 1027.61/s  (1.011s, 1012.63/s)  LR: 2.965e-05  Data: 0.011 (0.016)
Train: 273 [ 600/1251 ( 48%)]  Loss: 3.233 (3.21)  Time: 0.994s, 1030.26/s  (1.010s, 1013.54/s)  LR: 2.965e-05  Data: 0.012 (0.016)
Train: 273 [ 650/1251 ( 52%)]  Loss: 3.130 (3.20)  Time: 0.996s, 1027.83/s  (1.010s, 1014.24/s)  LR: 2.965e-05  Data: 0.011 (0.016)
Train: 273 [ 700/1251 ( 56%)]  Loss: 2.988 (3.19)  Time: 0.993s, 1030.88/s  (1.009s, 1015.12/s)  LR: 2.965e-05  Data: 0.011 (0.015)
Train: 273 [ 750/1251 ( 60%)]  Loss: 3.358 (3.20)  Time: 0.994s, 1030.16/s  (1.008s, 1015.91/s)  LR: 2.965e-05  Data: 0.011 (0.015)
Train: 273 [ 800/1251 ( 64%)]  Loss: 3.424 (3.21)  Time: 0.995s, 1028.95/s  (1.007s, 1016.60/s)  LR: 2.965e-05  Data: 0.011 (0.015)
Train: 273 [ 850/1251 ( 68%)]  Loss: 3.125 (3.21)  Time: 1.001s, 1022.79/s  (1.007s, 1016.81/s)  LR: 2.965e-05  Data: 0.013 (0.015)
Train: 273 [ 900/1251 ( 72%)]  Loss: 3.079 (3.20)  Time: 1.000s, 1024.45/s  (1.007s, 1017.22/s)  LR: 2.965e-05  Data: 0.015 (0.014)
Train: 273 [ 950/1251 ( 76%)]  Loss: 3.023 (3.19)  Time: 0.993s, 1031.21/s  (1.006s, 1017.62/s)  LR: 2.965e-05  Data: 0.011 (0.014)
Train: 273 [1000/1251 ( 80%)]  Loss: 3.240 (3.19)  Time: 0.993s, 1031.39/s  (1.006s, 1017.39/s)  LR: 2.965e-05  Data: 0.012 (0.014)
Train: 273 [1050/1251 ( 84%)]  Loss: 2.821 (3.18)  Time: 0.996s, 1028.26/s  (1.006s, 1017.57/s)  LR: 2.965e-05  Data: 0.012 (0.014)
Train: 273 [1100/1251 ( 88%)]  Loss: 3.040 (3.17)  Time: 1.007s, 1017.28/s  (1.006s, 1017.94/s)  LR: 2.965e-05  Data: 0.014 (0.014)
Train: 273 [1150/1251 ( 92%)]  Loss: 2.813 (3.15)  Time: 0.995s, 1028.78/s  (1.006s, 1018.16/s)  LR: 2.965e-05  Data: 0.012 (0.014)
Train: 273 [1200/1251 ( 96%)]  Loss: 3.190 (3.16)  Time: 0.996s, 1027.75/s  (1.005s, 1018.53/s)  LR: 2.965e-05  Data: 0.011 (0.014)
Train: 273 [1250/1251 (100%)]  Loss: 3.291 (3.16)  Time: 0.981s, 1043.67/s  (1.005s, 1018.90/s)  LR: 2.965e-05  Data: 0.000 (0.014)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.599 (1.599)  Loss:  0.6075 (0.6075)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.245 (0.569)  Loss:  0.7516 (1.0652)  Acc@1: 87.0283 (80.1880)  Acc@5: 98.1132 (95.0900)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-273.pth.tar', 80.1879999243164)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-272.pth.tar', 80.0940000805664)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-271.pth.tar', 80.07399994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-268.pth.tar', 80.0280000024414)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-264.pth.tar', 80.02200002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-270.pth.tar', 80.02200002685547)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-267.pth.tar', 80.00200005615234)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-269.pth.tar', 79.9299999243164)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-265.pth.tar', 79.90000002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-263.pth.tar', 79.89800005615234)

Train: 274 [   0/1251 (  0%)]  Loss: 3.149 (3.15)  Time: 2.466s,  415.32/s  (2.466s,  415.32/s)  LR: 2.823e-05  Data: 1.502 (1.502)
Train: 274 [  50/1251 (  4%)]  Loss: 2.839 (2.99)  Time: 0.997s, 1026.59/s  (1.027s,  996.70/s)  LR: 2.823e-05  Data: 0.011 (0.040)
Train: 274 [ 100/1251 (  8%)]  Loss: 2.818 (2.94)  Time: 0.997s, 1027.10/s  (1.013s, 1011.04/s)  LR: 2.823e-05  Data: 0.010 (0.026)
Train: 274 [ 150/1251 ( 12%)]  Loss: 2.875 (2.92)  Time: 1.062s,  964.37/s  (1.016s, 1008.23/s)  LR: 2.823e-05  Data: 0.011 (0.021)
Train: 274 [ 200/1251 ( 16%)]  Loss: 3.090 (2.95)  Time: 0.997s, 1027.48/s  (1.015s, 1008.45/s)  LR: 2.823e-05  Data: 0.011 (0.019)
Train: 274 [ 250/1251 ( 20%)]  Loss: 3.096 (2.98)  Time: 0.992s, 1032.01/s  (1.012s, 1011.64/s)  LR: 2.823e-05  Data: 0.011 (0.017)
Train: 274 [ 300/1251 ( 24%)]  Loss: 3.095 (2.99)  Time: 0.998s, 1026.34/s  (1.012s, 1011.61/s)  LR: 2.823e-05  Data: 0.011 (0.016)
Train: 274 [ 350/1251 ( 28%)]  Loss: 2.694 (2.96)  Time: 1.003s, 1020.58/s  (1.011s, 1012.44/s)  LR: 2.823e-05  Data: 0.011 (0.015)
Train: 274 [ 400/1251 ( 32%)]  Loss: 3.076 (2.97)  Time: 0.995s, 1029.26/s  (1.010s, 1013.96/s)  LR: 2.823e-05  Data: 0.011 (0.015)
Train: 274 [ 450/1251 ( 36%)]  Loss: 3.305 (3.00)  Time: 0.995s, 1028.95/s  (1.009s, 1015.03/s)  LR: 2.823e-05  Data: 0.010 (0.014)
Train: 274 [ 500/1251 ( 40%)]  Loss: 3.269 (3.03)  Time: 0.996s, 1028.19/s  (1.008s, 1015.90/s)  LR: 2.823e-05  Data: 0.011 (0.014)
Train: 274 [ 550/1251 ( 44%)]  Loss: 3.011 (3.03)  Time: 1.047s,  978.36/s  (1.007s, 1016.70/s)  LR: 2.823e-05  Data: 0.011 (0.014)
Train: 274 [ 600/1251 ( 48%)]  Loss: 3.024 (3.03)  Time: 1.001s, 1023.31/s  (1.007s, 1017.22/s)  LR: 2.823e-05  Data: 0.011 (0.014)
Train: 274 [ 650/1251 ( 52%)]  Loss: 2.975 (3.02)  Time: 1.043s,  981.34/s  (1.006s, 1017.71/s)  LR: 2.823e-05  Data: 0.012 (0.013)
Train: 274 [ 700/1251 ( 56%)]  Loss: 3.262 (3.04)  Time: 0.994s, 1029.80/s  (1.006s, 1018.15/s)  LR: 2.823e-05  Data: 0.011 (0.013)
Train: 274 [ 750/1251 ( 60%)]  Loss: 2.925 (3.03)  Time: 0.996s, 1028.26/s  (1.006s, 1017.73/s)  LR: 2.823e-05  Data: 0.011 (0.013)
Train: 274 [ 800/1251 ( 64%)]  Loss: 3.088 (3.03)  Time: 1.003s, 1020.61/s  (1.006s, 1018.11/s)  LR: 2.823e-05  Data: 0.011 (0.013)
Train: 274 [ 850/1251 ( 68%)]  Loss: 3.427 (3.06)  Time: 0.995s, 1028.88/s  (1.005s, 1018.48/s)  LR: 2.823e-05  Data: 0.012 (0.013)
Train: 274 [ 900/1251 ( 72%)]  Loss: 3.285 (3.07)  Time: 1.000s, 1024.41/s  (1.005s, 1018.73/s)  LR: 2.823e-05  Data: 0.012 (0.013)
Train: 274 [ 950/1251 ( 76%)]  Loss: 2.839 (3.06)  Time: 1.000s, 1024.50/s  (1.005s, 1019.19/s)  LR: 2.823e-05  Data: 0.012 (0.013)
Train: 274 [1000/1251 ( 80%)]  Loss: 2.697 (3.04)  Time: 1.006s, 1018.00/s  (1.005s, 1019.35/s)  LR: 2.823e-05  Data: 0.013 (0.013)
Train: 274 [1050/1251 ( 84%)]  Loss: 3.216 (3.05)  Time: 0.993s, 1031.70/s  (1.004s, 1019.60/s)  LR: 2.823e-05  Data: 0.010 (0.013)
Train: 274 [1100/1251 ( 88%)]  Loss: 2.756 (3.04)  Time: 0.996s, 1027.88/s  (1.004s, 1019.94/s)  LR: 2.823e-05  Data: 0.011 (0.012)
Train: 274 [1150/1251 ( 92%)]  Loss: 3.098 (3.04)  Time: 1.044s,  980.97/s  (1.005s, 1019.38/s)  LR: 2.823e-05  Data: 0.012 (0.012)
Train: 274 [1200/1251 ( 96%)]  Loss: 2.961 (3.03)  Time: 1.003s, 1021.32/s  (1.004s, 1019.69/s)  LR: 2.823e-05  Data: 0.012 (0.012)
Train: 274 [1250/1251 (100%)]  Loss: 3.001 (3.03)  Time: 0.985s, 1039.56/s  (1.004s, 1020.00/s)  LR: 2.823e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.668 (1.668)  Loss:  0.5947 (0.5947)  Acc@1: 92.2852 (92.2852)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.245 (0.567)  Loss:  0.7268 (1.0461)  Acc@1: 87.0283 (80.0600)  Acc@5: 97.9953 (95.1480)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-273.pth.tar', 80.1879999243164)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-272.pth.tar', 80.0940000805664)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-271.pth.tar', 80.07399994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-274.pth.tar', 80.0599999243164)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-268.pth.tar', 80.0280000024414)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-264.pth.tar', 80.02200002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-270.pth.tar', 80.02200002685547)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-267.pth.tar', 80.00200005615234)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-269.pth.tar', 79.9299999243164)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-265.pth.tar', 79.90000002929688)

Train: 275 [   0/1251 (  0%)]  Loss: 3.348 (3.35)  Time: 2.485s,  412.02/s  (2.485s,  412.02/s)  LR: 2.687e-05  Data: 1.524 (1.524)
Train: 275 [  50/1251 (  4%)]  Loss: 3.050 (3.20)  Time: 1.016s, 1007.64/s  (1.062s,  964.34/s)  LR: 2.687e-05  Data: 0.011 (0.041)
Train: 275 [ 100/1251 (  8%)]  Loss: 3.285 (3.23)  Time: 1.007s, 1017.20/s  (1.034s,  990.52/s)  LR: 2.687e-05  Data: 0.012 (0.026)
Train: 275 [ 150/1251 ( 12%)]  Loss: 3.109 (3.20)  Time: 0.996s, 1028.24/s  (1.023s, 1000.55/s)  LR: 2.687e-05  Data: 0.014 (0.021)
Train: 275 [ 200/1251 ( 16%)]  Loss: 3.186 (3.20)  Time: 0.994s, 1029.95/s  (1.018s, 1005.66/s)  LR: 2.687e-05  Data: 0.012 (0.019)
Train: 275 [ 250/1251 ( 20%)]  Loss: 3.118 (3.18)  Time: 1.002s, 1022.33/s  (1.015s, 1008.88/s)  LR: 2.687e-05  Data: 0.012 (0.017)
Train: 275 [ 300/1251 ( 24%)]  Loss: 2.908 (3.14)  Time: 0.998s, 1025.82/s  (1.013s, 1010.39/s)  LR: 2.687e-05  Data: 0.012 (0.016)
Train: 275 [ 350/1251 ( 28%)]  Loss: 3.086 (3.14)  Time: 0.997s, 1027.18/s  (1.012s, 1012.24/s)  LR: 2.687e-05  Data: 0.011 (0.016)
Train: 275 [ 400/1251 ( 32%)]  Loss: 2.618 (3.08)  Time: 0.996s, 1027.73/s  (1.010s, 1013.94/s)  LR: 2.687e-05  Data: 0.011 (0.015)
Train: 275 [ 450/1251 ( 36%)]  Loss: 3.234 (3.09)  Time: 0.995s, 1029.15/s  (1.009s, 1015.03/s)  LR: 2.687e-05  Data: 0.011 (0.015)
Train: 275 [ 500/1251 ( 40%)]  Loss: 2.936 (3.08)  Time: 0.999s, 1024.71/s  (1.008s, 1016.03/s)  LR: 2.687e-05  Data: 0.012 (0.014)
Train: 275 [ 550/1251 ( 44%)]  Loss: 2.934 (3.07)  Time: 0.995s, 1028.83/s  (1.007s, 1016.76/s)  LR: 2.687e-05  Data: 0.011 (0.014)
Train: 275 [ 600/1251 ( 48%)]  Loss: 3.503 (3.10)  Time: 0.997s, 1027.23/s  (1.007s, 1017.00/s)  LR: 2.687e-05  Data: 0.011 (0.014)
Train: 275 [ 650/1251 ( 52%)]  Loss: 2.897 (3.09)  Time: 0.995s, 1029.39/s  (1.006s, 1017.42/s)  LR: 2.687e-05  Data: 0.011 (0.014)
Train: 275 [ 700/1251 ( 56%)]  Loss: 2.974 (3.08)  Time: 0.994s, 1030.51/s  (1.006s, 1017.88/s)  LR: 2.687e-05  Data: 0.012 (0.013)
Train: 275 [ 750/1251 ( 60%)]  Loss: 3.025 (3.08)  Time: 0.994s, 1030.21/s  (1.005s, 1018.45/s)  LR: 2.687e-05  Data: 0.011 (0.013)
Train: 275 [ 800/1251 ( 64%)]  Loss: 2.960 (3.07)  Time: 1.003s, 1021.00/s  (1.005s, 1018.89/s)  LR: 2.687e-05  Data: 0.011 (0.013)
Train: 275 [ 850/1251 ( 68%)]  Loss: 3.065 (3.07)  Time: 0.999s, 1024.79/s  (1.007s, 1016.67/s)  LR: 2.687e-05  Data: 0.013 (0.013)
Train: 275 [ 900/1251 ( 72%)]  Loss: 2.797 (3.05)  Time: 1.006s, 1017.77/s  (1.007s, 1016.94/s)  LR: 2.687e-05  Data: 0.012 (0.013)
Train: 275 [ 950/1251 ( 76%)]  Loss: 3.191 (3.06)  Time: 0.997s, 1027.08/s  (1.007s, 1017.22/s)  LR: 2.687e-05  Data: 0.011 (0.013)
Train: 275 [1000/1251 ( 80%)]  Loss: 2.964 (3.06)  Time: 0.996s, 1028.21/s  (1.006s, 1017.53/s)  LR: 2.687e-05  Data: 0.012 (0.013)
Train: 275 [1050/1251 ( 84%)]  Loss: 2.576 (3.03)  Time: 0.995s, 1029.62/s  (1.006s, 1017.85/s)  LR: 2.687e-05  Data: 0.010 (0.013)
Train: 275 [1100/1251 ( 88%)]  Loss: 3.289 (3.05)  Time: 0.996s, 1027.89/s  (1.006s, 1018.26/s)  LR: 2.687e-05  Data: 0.011 (0.013)
Train: 275 [1150/1251 ( 92%)]  Loss: 3.088 (3.05)  Time: 0.995s, 1028.77/s  (1.005s, 1018.60/s)  LR: 2.687e-05  Data: 0.011 (0.013)
Train: 275 [1200/1251 ( 96%)]  Loss: 3.161 (3.05)  Time: 0.996s, 1027.84/s  (1.005s, 1018.73/s)  LR: 2.687e-05  Data: 0.011 (0.013)
Train: 275 [1250/1251 (100%)]  Loss: 3.093 (3.05)  Time: 0.983s, 1041.72/s  (1.005s, 1018.51/s)  LR: 2.687e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.651 (1.651)  Loss:  0.6807 (0.6807)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.245 (0.568)  Loss:  0.7776 (1.1137)  Acc@1: 86.2028 (80.0780)  Acc@5: 97.7594 (95.0960)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-273.pth.tar', 80.1879999243164)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-272.pth.tar', 80.0940000805664)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-275.pth.tar', 80.07799995361329)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-271.pth.tar', 80.07399994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-274.pth.tar', 80.0599999243164)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-268.pth.tar', 80.0280000024414)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-264.pth.tar', 80.02200002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-270.pth.tar', 80.02200002685547)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-267.pth.tar', 80.00200005615234)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-269.pth.tar', 79.9299999243164)

Train: 276 [   0/1251 (  0%)]  Loss: 3.303 (3.30)  Time: 2.463s,  415.68/s  (2.463s,  415.68/s)  LR: 2.555e-05  Data: 1.506 (1.506)
Train: 276 [  50/1251 (  4%)]  Loss: 2.819 (3.06)  Time: 1.011s, 1013.08/s  (1.036s,  988.46/s)  LR: 2.555e-05  Data: 0.012 (0.046)
Train: 276 [ 100/1251 (  8%)]  Loss: 2.917 (3.01)  Time: 0.997s, 1027.27/s  (1.022s, 1002.41/s)  LR: 2.555e-05  Data: 0.012 (0.029)
Train: 276 [ 150/1251 ( 12%)]  Loss: 3.158 (3.05)  Time: 0.999s, 1024.56/s  (1.015s, 1008.43/s)  LR: 2.555e-05  Data: 0.011 (0.023)
Train: 276 [ 200/1251 ( 16%)]  Loss: 2.851 (3.01)  Time: 0.995s, 1028.84/s  (1.015s, 1008.88/s)  LR: 2.555e-05  Data: 0.011 (0.020)
Train: 276 [ 250/1251 ( 20%)]  Loss: 2.818 (2.98)  Time: 0.999s, 1025.46/s  (1.012s, 1011.90/s)  LR: 2.555e-05  Data: 0.011 (0.018)
Train: 276 [ 300/1251 ( 24%)]  Loss: 2.913 (2.97)  Time: 1.037s,  987.58/s  (1.011s, 1012.77/s)  LR: 2.555e-05  Data: 0.010 (0.017)
Train: 276 [ 350/1251 ( 28%)]  Loss: 3.456 (3.03)  Time: 0.995s, 1029.09/s  (1.011s, 1012.85/s)  LR: 2.555e-05  Data: 0.011 (0.016)
Train: 276 [ 400/1251 ( 32%)]  Loss: 3.007 (3.03)  Time: 0.995s, 1028.78/s  (1.010s, 1014.24/s)  LR: 2.555e-05  Data: 0.011 (0.016)
Train: 276 [ 450/1251 ( 36%)]  Loss: 3.078 (3.03)  Time: 1.005s, 1019.39/s  (1.009s, 1015.15/s)  LR: 2.555e-05  Data: 0.011 (0.015)
Train: 276 [ 500/1251 ( 40%)]  Loss: 3.101 (3.04)  Time: 1.028s,  996.30/s  (1.008s, 1016.05/s)  LR: 2.555e-05  Data: 0.011 (0.015)
Train: 276 [ 550/1251 ( 44%)]  Loss: 3.271 (3.06)  Time: 0.996s, 1027.96/s  (1.008s, 1016.05/s)  LR: 2.555e-05  Data: 0.011 (0.014)
Train: 276 [ 600/1251 ( 48%)]  Loss: 3.187 (3.07)  Time: 1.010s, 1013.71/s  (1.007s, 1016.77/s)  LR: 2.555e-05  Data: 0.012 (0.014)
Train: 276 [ 650/1251 ( 52%)]  Loss: 3.361 (3.09)  Time: 0.997s, 1027.11/s  (1.006s, 1017.44/s)  LR: 2.555e-05  Data: 0.011 (0.014)
Train: 276 [ 700/1251 ( 56%)]  Loss: 3.187 (3.10)  Time: 1.003s, 1020.81/s  (1.006s, 1018.14/s)  LR: 2.555e-05  Data: 0.011 (0.014)
Train: 276 [ 750/1251 ( 60%)]  Loss: 3.157 (3.10)  Time: 0.999s, 1025.33/s  (1.005s, 1018.47/s)  LR: 2.555e-05  Data: 0.011 (0.013)
Train: 276 [ 800/1251 ( 64%)]  Loss: 3.033 (3.10)  Time: 0.995s, 1028.80/s  (1.005s, 1019.02/s)  LR: 2.555e-05  Data: 0.010 (0.013)
Train: 276 [ 850/1251 ( 68%)]  Loss: 3.283 (3.11)  Time: 0.996s, 1027.75/s  (1.005s, 1019.28/s)  LR: 2.555e-05  Data: 0.010 (0.013)
Train: 276 [ 900/1251 ( 72%)]  Loss: 2.882 (3.09)  Time: 1.001s, 1022.87/s  (1.004s, 1019.59/s)  LR: 2.555e-05  Data: 0.011 (0.013)
Train: 276 [ 950/1251 ( 76%)]  Loss: 2.948 (3.09)  Time: 1.039s,  985.86/s  (1.004s, 1019.55/s)  LR: 2.555e-05  Data: 0.011 (0.013)
Train: 276 [1000/1251 ( 80%)]  Loss: 3.479 (3.11)  Time: 0.999s, 1025.34/s  (1.004s, 1019.69/s)  LR: 2.555e-05  Data: 0.010 (0.013)
Train: 276 [1050/1251 ( 84%)]  Loss: 3.340 (3.12)  Time: 1.016s, 1008.00/s  (1.004s, 1019.82/s)  LR: 2.555e-05  Data: 0.027 (0.013)
Train: 276 [1100/1251 ( 88%)]  Loss: 3.156 (3.12)  Time: 1.003s, 1020.96/s  (1.004s, 1019.93/s)  LR: 2.555e-05  Data: 0.011 (0.013)
Train: 276 [1150/1251 ( 92%)]  Loss: 3.054 (3.11)  Time: 1.001s, 1023.47/s  (1.004s, 1019.85/s)  LR: 2.555e-05  Data: 0.012 (0.013)
Train: 276 [1200/1251 ( 96%)]  Loss: 2.965 (3.11)  Time: 1.001s, 1023.16/s  (1.004s, 1020.10/s)  LR: 2.555e-05  Data: 0.011 (0.013)
Train: 276 [1250/1251 (100%)]  Loss: 3.267 (3.12)  Time: 0.983s, 1041.93/s  (1.004s, 1020.35/s)  LR: 2.555e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.626 (1.626)  Loss:  0.6079 (0.6079)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.245 (0.566)  Loss:  0.7514 (1.0541)  Acc@1: 86.6745 (80.2640)  Acc@5: 97.9953 (95.1500)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-276.pth.tar', 80.26400002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-273.pth.tar', 80.1879999243164)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-272.pth.tar', 80.0940000805664)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-275.pth.tar', 80.07799995361329)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-271.pth.tar', 80.07399994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-274.pth.tar', 80.0599999243164)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-268.pth.tar', 80.0280000024414)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-264.pth.tar', 80.02200002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-270.pth.tar', 80.02200002685547)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-267.pth.tar', 80.00200005615234)

Train: 277 [   0/1251 (  0%)]  Loss: 3.066 (3.07)  Time: 2.448s,  418.30/s  (2.448s,  418.30/s)  LR: 2.429e-05  Data: 1.488 (1.488)
Train: 277 [  50/1251 (  4%)]  Loss: 2.803 (2.93)  Time: 1.000s, 1023.94/s  (1.031s,  993.16/s)  LR: 2.429e-05  Data: 0.011 (0.040)
Train: 277 [ 100/1251 (  8%)]  Loss: 3.312 (3.06)  Time: 0.998s, 1025.71/s  (1.016s, 1007.64/s)  LR: 2.429e-05  Data: 0.011 (0.026)
Train: 277 [ 150/1251 ( 12%)]  Loss: 2.840 (3.01)  Time: 0.999s, 1024.77/s  (1.010s, 1013.57/s)  LR: 2.429e-05  Data: 0.011 (0.021)
Train: 277 [ 200/1251 ( 16%)]  Loss: 2.658 (2.94)  Time: 0.994s, 1030.35/s  (1.008s, 1016.23/s)  LR: 2.429e-05  Data: 0.011 (0.019)
Train: 277 [ 250/1251 ( 20%)]  Loss: 3.119 (2.97)  Time: 1.000s, 1024.35/s  (1.008s, 1016.06/s)  LR: 2.429e-05  Data: 0.012 (0.017)
Train: 277 [ 300/1251 ( 24%)]  Loss: 3.172 (3.00)  Time: 1.004s, 1020.07/s  (1.007s, 1016.82/s)  LR: 2.429e-05  Data: 0.010 (0.016)
Train: 277 [ 350/1251 ( 28%)]  Loss: 3.018 (3.00)  Time: 0.995s, 1028.85/s  (1.006s, 1017.60/s)  LR: 2.429e-05  Data: 0.011 (0.015)
Train: 277 [ 400/1251 ( 32%)]  Loss: 3.058 (3.01)  Time: 0.996s, 1028.05/s  (1.006s, 1018.19/s)  LR: 2.429e-05  Data: 0.012 (0.015)
Train: 277 [ 450/1251 ( 36%)]  Loss: 3.107 (3.02)  Time: 0.997s, 1026.78/s  (1.005s, 1018.88/s)  LR: 2.429e-05  Data: 0.012 (0.014)
Train: 277 [ 500/1251 ( 40%)]  Loss: 3.118 (3.02)  Time: 0.997s, 1027.28/s  (1.005s, 1019.16/s)  LR: 2.429e-05  Data: 0.011 (0.014)
Train: 277 [ 550/1251 ( 44%)]  Loss: 2.873 (3.01)  Time: 1.004s, 1019.75/s  (1.004s, 1019.60/s)  LR: 2.429e-05  Data: 0.011 (0.014)
Train: 277 [ 600/1251 ( 48%)]  Loss: 3.088 (3.02)  Time: 0.996s, 1027.95/s  (1.004s, 1019.90/s)  LR: 2.429e-05  Data: 0.011 (0.014)
Train: 277 [ 650/1251 ( 52%)]  Loss: 2.839 (3.01)  Time: 0.994s, 1030.46/s  (1.004s, 1019.96/s)  LR: 2.429e-05  Data: 0.010 (0.013)
Train: 277 [ 700/1251 ( 56%)]  Loss: 3.163 (3.02)  Time: 0.997s, 1027.11/s  (1.004s, 1020.04/s)  LR: 2.429e-05  Data: 0.012 (0.013)
Train: 277 [ 750/1251 ( 60%)]  Loss: 2.923 (3.01)  Time: 0.997s, 1027.52/s  (1.004s, 1020.32/s)  LR: 2.429e-05  Data: 0.011 (0.013)
Train: 277 [ 800/1251 ( 64%)]  Loss: 3.165 (3.02)  Time: 0.995s, 1029.09/s  (1.003s, 1020.47/s)  LR: 2.429e-05  Data: 0.011 (0.013)
Train: 277 [ 850/1251 ( 68%)]  Loss: 2.889 (3.01)  Time: 0.994s, 1030.49/s  (1.003s, 1020.65/s)  LR: 2.429e-05  Data: 0.011 (0.013)
Train: 277 [ 900/1251 ( 72%)]  Loss: 3.280 (3.03)  Time: 1.062s,  963.92/s  (1.003s, 1020.43/s)  LR: 2.429e-05  Data: 0.011 (0.013)
Train: 277 [ 950/1251 ( 76%)]  Loss: 3.050 (3.03)  Time: 1.041s,  984.01/s  (1.003s, 1020.54/s)  LR: 2.429e-05  Data: 0.017 (0.013)
Train: 277 [1000/1251 ( 80%)]  Loss: 2.941 (3.02)  Time: 1.005s, 1018.40/s  (1.003s, 1020.60/s)  LR: 2.429e-05  Data: 0.011 (0.013)
Train: 277 [1050/1251 ( 84%)]  Loss: 3.408 (3.04)  Time: 1.032s,  992.12/s  (1.004s, 1020.17/s)  LR: 2.429e-05  Data: 0.011 (0.013)
Train: 277 [1100/1251 ( 88%)]  Loss: 2.983 (3.04)  Time: 1.006s, 1018.34/s  (1.004s, 1020.17/s)  LR: 2.429e-05  Data: 0.011 (0.013)
Train: 277 [1150/1251 ( 92%)]  Loss: 2.826 (3.03)  Time: 0.998s, 1026.05/s  (1.004s, 1020.23/s)  LR: 2.429e-05  Data: 0.012 (0.013)
Train: 277 [1200/1251 ( 96%)]  Loss: 2.943 (3.03)  Time: 1.005s, 1018.44/s  (1.004s, 1020.07/s)  LR: 2.429e-05  Data: 0.011 (0.013)
Train: 277 [1250/1251 (100%)]  Loss: 3.253 (3.03)  Time: 0.983s, 1041.74/s  (1.004s, 1020.15/s)  LR: 2.429e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.612 (1.612)  Loss:  0.6161 (0.6161)  Acc@1: 92.2852 (92.2852)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.245 (0.567)  Loss:  0.7354 (1.0583)  Acc@1: 86.7924 (80.2240)  Acc@5: 97.7594 (95.1580)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-276.pth.tar', 80.26400002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-277.pth.tar', 80.22399995117188)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-273.pth.tar', 80.1879999243164)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-272.pth.tar', 80.0940000805664)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-275.pth.tar', 80.07799995361329)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-271.pth.tar', 80.07399994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-274.pth.tar', 80.0599999243164)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-268.pth.tar', 80.0280000024414)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-264.pth.tar', 80.02200002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-270.pth.tar', 80.02200002685547)

Train: 278 [   0/1251 (  0%)]  Loss: 3.164 (3.16)  Time: 2.621s,  390.62/s  (2.621s,  390.62/s)  LR: 2.308e-05  Data: 1.610 (1.610)
Train: 278 [  50/1251 (  4%)]  Loss: 3.049 (3.11)  Time: 0.996s, 1028.60/s  (1.034s,  990.74/s)  LR: 2.308e-05  Data: 0.011 (0.042)
Train: 278 [ 100/1251 (  8%)]  Loss: 2.930 (3.05)  Time: 0.997s, 1026.57/s  (1.021s, 1002.91/s)  LR: 2.308e-05  Data: 0.010 (0.027)
Train: 278 [ 150/1251 ( 12%)]  Loss: 3.344 (3.12)  Time: 1.000s, 1023.55/s  (1.015s, 1008.54/s)  LR: 2.308e-05  Data: 0.010 (0.022)
Train: 278 [ 200/1251 ( 16%)]  Loss: 3.310 (3.16)  Time: 1.009s, 1014.44/s  (1.014s, 1010.31/s)  LR: 2.308e-05  Data: 0.010 (0.019)
Train: 278 [ 250/1251 ( 20%)]  Loss: 2.771 (3.09)  Time: 1.000s, 1024.31/s  (1.011s, 1012.64/s)  LR: 2.308e-05  Data: 0.011 (0.018)
Train: 278 [ 300/1251 ( 24%)]  Loss: 3.087 (3.09)  Time: 0.996s, 1028.48/s  (1.009s, 1014.66/s)  LR: 2.308e-05  Data: 0.010 (0.016)
Train: 278 [ 350/1251 ( 28%)]  Loss: 3.326 (3.12)  Time: 1.001s, 1022.59/s  (1.009s, 1015.03/s)  LR: 2.308e-05  Data: 0.012 (0.016)
Train: 278 [ 400/1251 ( 32%)]  Loss: 3.179 (3.13)  Time: 1.001s, 1022.97/s  (1.008s, 1016.24/s)  LR: 2.308e-05  Data: 0.011 (0.015)
Train: 278 [ 450/1251 ( 36%)]  Loss: 3.210 (3.14)  Time: 1.002s, 1022.14/s  (1.007s, 1016.65/s)  LR: 2.308e-05  Data: 0.012 (0.015)
Train: 278 [ 500/1251 ( 40%)]  Loss: 3.247 (3.15)  Time: 0.997s, 1027.23/s  (1.007s, 1017.31/s)  LR: 2.308e-05  Data: 0.011 (0.014)
Train: 278 [ 550/1251 ( 44%)]  Loss: 3.132 (3.15)  Time: 0.994s, 1030.66/s  (1.008s, 1015.45/s)  LR: 2.308e-05  Data: 0.011 (0.014)
Train: 278 [ 600/1251 ( 48%)]  Loss: 2.828 (3.12)  Time: 0.995s, 1029.46/s  (1.008s, 1016.02/s)  LR: 2.308e-05  Data: 0.010 (0.014)
Train: 278 [ 650/1251 ( 52%)]  Loss: 3.272 (3.13)  Time: 0.997s, 1027.09/s  (1.009s, 1015.25/s)  LR: 2.308e-05  Data: 0.012 (0.014)
Train: 278 [ 700/1251 ( 56%)]  Loss: 3.001 (3.12)  Time: 1.001s, 1023.35/s  (1.008s, 1015.90/s)  LR: 2.308e-05  Data: 0.010 (0.013)
Train: 278 [ 750/1251 ( 60%)]  Loss: 3.194 (3.13)  Time: 0.996s, 1028.14/s  (1.008s, 1015.98/s)  LR: 2.308e-05  Data: 0.011 (0.013)
Train: 278 [ 800/1251 ( 64%)]  Loss: 2.949 (3.12)  Time: 1.009s, 1015.14/s  (1.007s, 1016.55/s)  LR: 2.308e-05  Data: 0.010 (0.013)
Train: 278 [ 850/1251 ( 68%)]  Loss: 2.996 (3.11)  Time: 0.993s, 1031.01/s  (1.007s, 1016.83/s)  LR: 2.308e-05  Data: 0.011 (0.013)
Train: 278 [ 900/1251 ( 72%)]  Loss: 2.874 (3.10)  Time: 1.001s, 1023.47/s  (1.007s, 1017.16/s)  LR: 2.308e-05  Data: 0.013 (0.013)
Train: 278 [ 950/1251 ( 76%)]  Loss: 3.215 (3.10)  Time: 1.030s,  993.78/s  (1.007s, 1017.35/s)  LR: 2.308e-05  Data: 0.010 (0.013)
Train: 278 [1000/1251 ( 80%)]  Loss: 3.245 (3.11)  Time: 0.995s, 1028.94/s  (1.006s, 1017.61/s)  LR: 2.308e-05  Data: 0.012 (0.013)
Train: 278 [1050/1251 ( 84%)]  Loss: 3.186 (3.11)  Time: 0.990s, 1033.98/s  (1.006s, 1017.63/s)  LR: 2.308e-05  Data: 0.010 (0.013)
Train: 278 [1100/1251 ( 88%)]  Loss: 3.196 (3.12)  Time: 0.998s, 1025.71/s  (1.006s, 1017.94/s)  LR: 2.308e-05  Data: 0.011 (0.013)
Train: 278 [1150/1251 ( 92%)]  Loss: 2.908 (3.11)  Time: 0.993s, 1030.80/s  (1.006s, 1018.13/s)  LR: 2.308e-05  Data: 0.010 (0.013)
Train: 278 [1200/1251 ( 96%)]  Loss: 2.889 (3.10)  Time: 0.996s, 1027.94/s  (1.006s, 1018.37/s)  LR: 2.308e-05  Data: 0.010 (0.012)
Train: 278 [1250/1251 (100%)]  Loss: 3.279 (3.11)  Time: 0.983s, 1042.17/s  (1.005s, 1018.71/s)  LR: 2.308e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.731 (1.731)  Loss:  0.6585 (0.6585)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.245 (0.566)  Loss:  0.7824 (1.1100)  Acc@1: 87.0283 (80.1980)  Acc@5: 98.1132 (95.1500)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-276.pth.tar', 80.26400002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-277.pth.tar', 80.22399995117188)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-278.pth.tar', 80.19800018310546)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-273.pth.tar', 80.1879999243164)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-272.pth.tar', 80.0940000805664)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-275.pth.tar', 80.07799995361329)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-271.pth.tar', 80.07399994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-274.pth.tar', 80.0599999243164)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-268.pth.tar', 80.0280000024414)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-264.pth.tar', 80.02200002929688)

Train: 279 [   0/1251 (  0%)]  Loss: 2.860 (2.86)  Time: 2.499s,  409.73/s  (2.499s,  409.73/s)  LR: 2.192e-05  Data: 1.535 (1.535)
Train: 279 [  50/1251 (  4%)]  Loss: 3.165 (3.01)  Time: 0.997s, 1027.07/s  (1.038s,  986.59/s)  LR: 2.192e-05  Data: 0.010 (0.042)
Train: 279 [ 100/1251 (  8%)]  Loss: 2.618 (2.88)  Time: 0.998s, 1025.67/s  (1.019s, 1004.44/s)  LR: 2.192e-05  Data: 0.011 (0.027)
Train: 279 [ 150/1251 ( 12%)]  Loss: 3.179 (2.96)  Time: 0.998s, 1026.16/s  (1.014s, 1010.33/s)  LR: 2.192e-05  Data: 0.010 (0.021)
Train: 279 [ 200/1251 ( 16%)]  Loss: 3.097 (2.98)  Time: 0.998s, 1026.27/s  (1.010s, 1013.85/s)  LR: 2.192e-05  Data: 0.012 (0.019)
Train: 279 [ 250/1251 ( 20%)]  Loss: 3.217 (3.02)  Time: 0.993s, 1031.36/s  (1.009s, 1014.69/s)  LR: 2.192e-05  Data: 0.010 (0.017)
Train: 279 [ 300/1251 ( 24%)]  Loss: 2.947 (3.01)  Time: 0.997s, 1027.23/s  (1.008s, 1016.22/s)  LR: 2.192e-05  Data: 0.012 (0.016)
Train: 279 [ 350/1251 ( 28%)]  Loss: 2.974 (3.01)  Time: 0.994s, 1029.98/s  (1.009s, 1014.63/s)  LR: 2.192e-05  Data: 0.011 (0.016)
Train: 279 [ 400/1251 ( 32%)]  Loss: 2.806 (2.98)  Time: 1.004s, 1019.90/s  (1.009s, 1014.46/s)  LR: 2.192e-05  Data: 0.011 (0.015)
Train: 279 [ 450/1251 ( 36%)]  Loss: 3.372 (3.02)  Time: 1.008s, 1016.12/s  (1.011s, 1012.42/s)  LR: 2.192e-05  Data: 0.012 (0.015)
Train: 279 [ 500/1251 ( 40%)]  Loss: 3.020 (3.02)  Time: 0.997s, 1027.57/s  (1.010s, 1013.69/s)  LR: 2.192e-05  Data: 0.011 (0.014)
Train: 279 [ 550/1251 ( 44%)]  Loss: 3.369 (3.05)  Time: 0.994s, 1030.16/s  (1.009s, 1014.72/s)  LR: 2.192e-05  Data: 0.011 (0.014)
Train: 279 [ 600/1251 ( 48%)]  Loss: 2.874 (3.04)  Time: 0.994s, 1029.78/s  (1.008s, 1015.40/s)  LR: 2.192e-05  Data: 0.012 (0.014)
Train: 279 [ 650/1251 ( 52%)]  Loss: 2.856 (3.03)  Time: 1.004s, 1020.12/s  (1.009s, 1014.89/s)  LR: 2.192e-05  Data: 0.011 (0.014)
Train: 279 [ 700/1251 ( 56%)]  Loss: 3.005 (3.02)  Time: 0.996s, 1028.61/s  (1.008s, 1015.66/s)  LR: 2.192e-05  Data: 0.011 (0.013)
Train: 279 [ 750/1251 ( 60%)]  Loss: 2.986 (3.02)  Time: 0.997s, 1027.44/s  (1.008s, 1016.35/s)  LR: 2.192e-05  Data: 0.013 (0.013)
Train: 279 [ 800/1251 ( 64%)]  Loss: 2.538 (2.99)  Time: 0.994s, 1029.78/s  (1.007s, 1016.97/s)  LR: 2.192e-05  Data: 0.011 (0.013)
Train: 279 [ 850/1251 ( 68%)]  Loss: 3.068 (3.00)  Time: 1.005s, 1018.96/s  (1.006s, 1017.41/s)  LR: 2.192e-05  Data: 0.011 (0.013)
Train: 279 [ 900/1251 ( 72%)]  Loss: 3.266 (3.01)  Time: 1.000s, 1023.57/s  (1.007s, 1016.42/s)  LR: 2.192e-05  Data: 0.012 (0.013)
Train: 279 [ 950/1251 ( 76%)]  Loss: 3.237 (3.02)  Time: 0.995s, 1029.21/s  (1.007s, 1016.95/s)  LR: 2.192e-05  Data: 0.012 (0.013)
Train: 279 [1000/1251 ( 80%)]  Loss: 3.484 (3.04)  Time: 0.996s, 1027.72/s  (1.007s, 1017.36/s)  LR: 2.192e-05  Data: 0.012 (0.013)
Train: 279 [1050/1251 ( 84%)]  Loss: 2.947 (3.04)  Time: 0.993s, 1031.56/s  (1.006s, 1017.61/s)  LR: 2.192e-05  Data: 0.011 (0.013)
Train: 279 [1100/1251 ( 88%)]  Loss: 2.993 (3.04)  Time: 0.996s, 1028.26/s  (1.006s, 1017.93/s)  LR: 2.192e-05  Data: 0.012 (0.013)
Train: 279 [1150/1251 ( 92%)]  Loss: 3.265 (3.05)  Time: 1.010s, 1014.29/s  (1.006s, 1018.22/s)  LR: 2.192e-05  Data: 0.013 (0.013)
Train: 279 [1200/1251 ( 96%)]  Loss: 2.944 (3.04)  Time: 0.994s, 1029.98/s  (1.005s, 1018.42/s)  LR: 2.192e-05  Data: 0.012 (0.013)
Train: 279 [1250/1251 (100%)]  Loss: 3.258 (3.05)  Time: 0.987s, 1037.48/s  (1.005s, 1018.63/s)  LR: 2.192e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.602 (1.602)  Loss:  0.6550 (0.6550)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.245 (0.569)  Loss:  0.7541 (1.0870)  Acc@1: 87.0283 (80.1220)  Acc@5: 97.7594 (95.1120)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-276.pth.tar', 80.26400002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-277.pth.tar', 80.22399995117188)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-278.pth.tar', 80.19800018310546)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-273.pth.tar', 80.1879999243164)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-279.pth.tar', 80.12200005371093)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-272.pth.tar', 80.0940000805664)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-275.pth.tar', 80.07799995361329)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-271.pth.tar', 80.07399994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-274.pth.tar', 80.0599999243164)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-268.pth.tar', 80.0280000024414)

Train: 280 [   0/1251 (  0%)]  Loss: 3.297 (3.30)  Time: 2.535s,  404.01/s  (2.535s,  404.01/s)  LR: 2.082e-05  Data: 1.568 (1.568)
Train: 280 [  50/1251 (  4%)]  Loss: 3.166 (3.23)  Time: 0.995s, 1028.85/s  (1.040s,  984.87/s)  LR: 2.082e-05  Data: 0.011 (0.042)
Train: 280 [ 100/1251 (  8%)]  Loss: 2.884 (3.12)  Time: 1.001s, 1023.41/s  (1.024s, 1000.44/s)  LR: 2.082e-05  Data: 0.011 (0.027)
Train: 280 [ 150/1251 ( 12%)]  Loss: 3.086 (3.11)  Time: 0.996s, 1027.65/s  (1.017s, 1007.02/s)  LR: 2.082e-05  Data: 0.012 (0.022)
Train: 280 [ 200/1251 ( 16%)]  Loss: 3.327 (3.15)  Time: 0.990s, 1033.98/s  (1.013s, 1011.18/s)  LR: 2.082e-05  Data: 0.010 (0.019)
Train: 280 [ 250/1251 ( 20%)]  Loss: 2.958 (3.12)  Time: 1.006s, 1017.93/s  (1.010s, 1013.82/s)  LR: 2.082e-05  Data: 0.010 (0.018)
Train: 280 [ 300/1251 ( 24%)]  Loss: 3.079 (3.11)  Time: 1.004s, 1020.01/s  (1.010s, 1013.50/s)  LR: 2.082e-05  Data: 0.011 (0.016)
Train: 280 [ 350/1251 ( 28%)]  Loss: 3.248 (3.13)  Time: 0.995s, 1029.27/s  (1.010s, 1014.31/s)  LR: 2.082e-05  Data: 0.011 (0.016)
Train: 280 [ 400/1251 ( 32%)]  Loss: 2.978 (3.11)  Time: 0.996s, 1028.17/s  (1.008s, 1015.60/s)  LR: 2.082e-05  Data: 0.012 (0.015)
Train: 280 [ 450/1251 ( 36%)]  Loss: 3.198 (3.12)  Time: 1.060s,  965.96/s  (1.008s, 1015.92/s)  LR: 2.082e-05  Data: 0.012 (0.015)
Train: 280 [ 500/1251 ( 40%)]  Loss: 3.063 (3.12)  Time: 0.997s, 1027.37/s  (1.007s, 1016.51/s)  LR: 2.082e-05  Data: 0.012 (0.014)
Train: 280 [ 550/1251 ( 44%)]  Loss: 2.881 (3.10)  Time: 1.042s,  982.81/s  (1.007s, 1017.01/s)  LR: 2.082e-05  Data: 0.012 (0.014)
Train: 280 [ 600/1251 ( 48%)]  Loss: 3.373 (3.12)  Time: 0.998s, 1026.33/s  (1.006s, 1017.65/s)  LR: 2.082e-05  Data: 0.011 (0.014)
Train: 280 [ 650/1251 ( 52%)]  Loss: 3.242 (3.13)  Time: 0.994s, 1029.82/s  (1.006s, 1017.64/s)  LR: 2.082e-05  Data: 0.011 (0.014)
Train: 280 [ 700/1251 ( 56%)]  Loss: 2.992 (3.12)  Time: 1.012s, 1012.25/s  (1.006s, 1017.93/s)  LR: 2.082e-05  Data: 0.010 (0.013)
Train: 280 [ 750/1251 ( 60%)]  Loss: 2.870 (3.10)  Time: 0.993s, 1031.55/s  (1.006s, 1018.36/s)  LR: 2.082e-05  Data: 0.011 (0.013)
Train: 280 [ 800/1251 ( 64%)]  Loss: 3.170 (3.11)  Time: 0.997s, 1026.84/s  (1.006s, 1018.10/s)  LR: 2.082e-05  Data: 0.012 (0.013)
Train: 280 [ 850/1251 ( 68%)]  Loss: 3.020 (3.10)  Time: 1.061s,  965.25/s  (1.006s, 1017.98/s)  LR: 2.082e-05  Data: 0.015 (0.013)
Train: 280 [ 900/1251 ( 72%)]  Loss: 3.274 (3.11)  Time: 0.998s, 1025.73/s  (1.007s, 1016.91/s)  LR: 2.082e-05  Data: 0.012 (0.013)
Train: 280 [ 950/1251 ( 76%)]  Loss: 2.902 (3.10)  Time: 0.994s, 1030.62/s  (1.007s, 1017.21/s)  LR: 2.082e-05  Data: 0.011 (0.013)
Train: 280 [1000/1251 ( 80%)]  Loss: 2.977 (3.09)  Time: 0.995s, 1029.55/s  (1.006s, 1017.52/s)  LR: 2.082e-05  Data: 0.011 (0.013)
Train: 280 [1050/1251 ( 84%)]  Loss: 2.991 (3.09)  Time: 0.996s, 1028.43/s  (1.006s, 1017.86/s)  LR: 2.082e-05  Data: 0.011 (0.013)
Train: 280 [1100/1251 ( 88%)]  Loss: 2.993 (3.09)  Time: 1.061s,  964.77/s  (1.006s, 1017.88/s)  LR: 2.082e-05  Data: 0.012 (0.013)
Train: 280 [1150/1251 ( 92%)]  Loss: 2.656 (3.07)  Time: 1.060s,  965.78/s  (1.006s, 1017.70/s)  LR: 2.082e-05  Data: 0.011 (0.013)
Train: 280 [1200/1251 ( 96%)]  Loss: 3.026 (3.07)  Time: 1.003s, 1020.95/s  (1.006s, 1017.90/s)  LR: 2.082e-05  Data: 0.013 (0.013)
Train: 280 [1250/1251 (100%)]  Loss: 3.040 (3.07)  Time: 0.987s, 1037.75/s  (1.006s, 1017.43/s)  LR: 2.082e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.591 (1.591)  Loss:  0.6226 (0.6226)  Acc@1: 92.3828 (92.3828)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.245 (0.576)  Loss:  0.7278 (1.0643)  Acc@1: 87.6179 (80.2960)  Acc@5: 98.1132 (95.1540)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-280.pth.tar', 80.29600005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-276.pth.tar', 80.26400002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-277.pth.tar', 80.22399995117188)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-278.pth.tar', 80.19800018310546)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-273.pth.tar', 80.1879999243164)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-279.pth.tar', 80.12200005371093)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-272.pth.tar', 80.0940000805664)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-275.pth.tar', 80.07799995361329)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-271.pth.tar', 80.07399994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-274.pth.tar', 80.0599999243164)

Train: 281 [   0/1251 (  0%)]  Loss: 3.246 (3.25)  Time: 2.480s,  412.98/s  (2.480s,  412.98/s)  LR: 1.977e-05  Data: 1.516 (1.516)
Train: 281 [  50/1251 (  4%)]  Loss: 3.350 (3.30)  Time: 0.997s, 1027.34/s  (1.041s,  983.91/s)  LR: 1.977e-05  Data: 0.011 (0.043)
Train: 281 [ 100/1251 (  8%)]  Loss: 3.149 (3.25)  Time: 0.992s, 1032.13/s  (1.022s, 1002.28/s)  LR: 1.977e-05  Data: 0.010 (0.027)
Train: 281 [ 150/1251 ( 12%)]  Loss: 2.984 (3.18)  Time: 0.998s, 1025.63/s  (1.015s, 1009.03/s)  LR: 1.977e-05  Data: 0.011 (0.022)
Train: 281 [ 200/1251 ( 16%)]  Loss: 3.183 (3.18)  Time: 0.989s, 1035.12/s  (1.019s, 1004.95/s)  LR: 1.977e-05  Data: 0.011 (0.019)
Train: 281 [ 250/1251 ( 20%)]  Loss: 2.894 (3.13)  Time: 0.995s, 1029.38/s  (1.015s, 1008.68/s)  LR: 1.977e-05  Data: 0.011 (0.018)
Train: 281 [ 300/1251 ( 24%)]  Loss: 3.173 (3.14)  Time: 0.995s, 1028.92/s  (1.013s, 1011.18/s)  LR: 1.977e-05  Data: 0.012 (0.016)
Train: 281 [ 350/1251 ( 28%)]  Loss: 3.082 (3.13)  Time: 0.996s, 1028.53/s  (1.011s, 1012.71/s)  LR: 1.977e-05  Data: 0.012 (0.016)
Train: 281 [ 400/1251 ( 32%)]  Loss: 3.105 (3.13)  Time: 0.997s, 1027.20/s  (1.010s, 1013.77/s)  LR: 1.977e-05  Data: 0.012 (0.015)
Train: 281 [ 450/1251 ( 36%)]  Loss: 3.346 (3.15)  Time: 1.051s,  974.40/s  (1.009s, 1014.75/s)  LR: 1.977e-05  Data: 0.012 (0.015)
Train: 281 [ 500/1251 ( 40%)]  Loss: 3.093 (3.15)  Time: 0.996s, 1028.18/s  (1.008s, 1015.60/s)  LR: 1.977e-05  Data: 0.011 (0.014)
Train: 281 [ 550/1251 ( 44%)]  Loss: 3.121 (3.14)  Time: 0.997s, 1026.57/s  (1.008s, 1015.98/s)  LR: 1.977e-05  Data: 0.011 (0.014)
Train: 281 [ 600/1251 ( 48%)]  Loss: 2.796 (3.12)  Time: 0.999s, 1024.81/s  (1.008s, 1016.27/s)  LR: 1.977e-05  Data: 0.011 (0.014)
Train: 281 [ 650/1251 ( 52%)]  Loss: 3.215 (3.12)  Time: 1.004s, 1020.07/s  (1.007s, 1016.54/s)  LR: 1.977e-05  Data: 0.010 (0.014)
Train: 281 [ 700/1251 ( 56%)]  Loss: 2.933 (3.11)  Time: 0.997s, 1026.60/s  (1.007s, 1016.94/s)  LR: 1.977e-05  Data: 0.012 (0.013)
Train: 281 [ 750/1251 ( 60%)]  Loss: 3.015 (3.11)  Time: 0.992s, 1031.91/s  (1.006s, 1017.53/s)  LR: 1.977e-05  Data: 0.010 (0.013)
Train: 281 [ 800/1251 ( 64%)]  Loss: 3.314 (3.12)  Time: 1.006s, 1017.71/s  (1.006s, 1018.04/s)  LR: 1.977e-05  Data: 0.013 (0.013)
Train: 281 [ 850/1251 ( 68%)]  Loss: 2.967 (3.11)  Time: 1.060s,  966.48/s  (1.008s, 1015.87/s)  LR: 1.977e-05  Data: 0.010 (0.013)
Train: 281 [ 900/1251 ( 72%)]  Loss: 3.264 (3.12)  Time: 1.002s, 1021.81/s  (1.008s, 1015.96/s)  LR: 1.977e-05  Data: 0.012 (0.013)
Train: 281 [ 950/1251 ( 76%)]  Loss: 2.974 (3.11)  Time: 0.997s, 1027.32/s  (1.008s, 1016.29/s)  LR: 1.977e-05  Data: 0.011 (0.013)
Train: 281 [1000/1251 ( 80%)]  Loss: 3.158 (3.11)  Time: 1.066s,  960.99/s  (1.008s, 1016.34/s)  LR: 1.977e-05  Data: 0.011 (0.013)
Train: 281 [1050/1251 ( 84%)]  Loss: 3.023 (3.11)  Time: 0.999s, 1024.85/s  (1.008s, 1015.59/s)  LR: 1.977e-05  Data: 0.012 (0.013)
Train: 281 [1100/1251 ( 88%)]  Loss: 2.676 (3.09)  Time: 1.002s, 1021.93/s  (1.009s, 1014.90/s)  LR: 1.977e-05  Data: 0.011 (0.013)
Train: 281 [1150/1251 ( 92%)]  Loss: 2.792 (3.08)  Time: 0.993s, 1030.78/s  (1.009s, 1015.30/s)  LR: 1.977e-05  Data: 0.011 (0.013)
Train: 281 [1200/1251 ( 96%)]  Loss: 2.815 (3.07)  Time: 0.995s, 1028.66/s  (1.008s, 1015.64/s)  LR: 1.977e-05  Data: 0.012 (0.012)
Train: 281 [1250/1251 (100%)]  Loss: 3.091 (3.07)  Time: 0.983s, 1041.76/s  (1.008s, 1016.10/s)  LR: 1.977e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.653 (1.653)  Loss:  0.6491 (0.6491)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.245 (0.576)  Loss:  0.7710 (1.0746)  Acc@1: 86.6745 (80.2440)  Acc@5: 97.8774 (95.1460)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-280.pth.tar', 80.29600005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-276.pth.tar', 80.26400002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-281.pth.tar', 80.24400002929687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-277.pth.tar', 80.22399995117188)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-278.pth.tar', 80.19800018310546)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-273.pth.tar', 80.1879999243164)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-279.pth.tar', 80.12200005371093)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-272.pth.tar', 80.0940000805664)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-275.pth.tar', 80.07799995361329)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-271.pth.tar', 80.07399994873047)

Train: 282 [   0/1251 (  0%)]  Loss: 2.985 (2.98)  Time: 4.158s,  246.28/s  (4.158s,  246.28/s)  LR: 1.877e-05  Data: 2.986 (2.986)
Train: 282 [  50/1251 (  4%)]  Loss: 3.063 (3.02)  Time: 1.039s,  985.69/s  (1.079s,  948.80/s)  LR: 1.877e-05  Data: 0.011 (0.070)
Train: 282 [ 100/1251 (  8%)]  Loss: 2.824 (2.96)  Time: 1.038s,  986.84/s  (1.054s,  971.87/s)  LR: 1.877e-05  Data: 0.013 (0.041)
Train: 282 [ 150/1251 ( 12%)]  Loss: 3.098 (2.99)  Time: 1.065s,  961.10/s  (1.043s,  982.00/s)  LR: 1.877e-05  Data: 0.012 (0.031)
Train: 282 [ 200/1251 ( 16%)]  Loss: 2.965 (2.99)  Time: 1.036s,  988.34/s  (1.035s,  989.82/s)  LR: 1.877e-05  Data: 0.010 (0.026)
Train: 282 [ 250/1251 ( 20%)]  Loss: 2.899 (2.97)  Time: 0.993s, 1031.61/s  (1.028s,  996.23/s)  LR: 1.877e-05  Data: 0.010 (0.023)
Train: 282 [ 300/1251 ( 24%)]  Loss: 3.284 (3.02)  Time: 0.995s, 1028.96/s  (1.023s, 1000.66/s)  LR: 1.877e-05  Data: 0.011 (0.021)
Train: 282 [ 350/1251 ( 28%)]  Loss: 2.919 (3.00)  Time: 0.994s, 1030.41/s  (1.020s, 1003.92/s)  LR: 1.877e-05  Data: 0.011 (0.020)
Train: 282 [ 400/1251 ( 32%)]  Loss: 2.935 (3.00)  Time: 0.996s, 1028.53/s  (1.017s, 1006.67/s)  LR: 1.877e-05  Data: 0.012 (0.019)
Train: 282 [ 450/1251 ( 36%)]  Loss: 3.029 (3.00)  Time: 1.007s, 1017.36/s  (1.016s, 1007.40/s)  LR: 1.877e-05  Data: 0.012 (0.018)
Train: 282 [ 500/1251 ( 40%)]  Loss: 3.059 (3.01)  Time: 0.996s, 1027.85/s  (1.015s, 1009.14/s)  LR: 1.877e-05  Data: 0.011 (0.017)
Train: 282 [ 550/1251 ( 44%)]  Loss: 2.816 (2.99)  Time: 1.059s,  966.82/s  (1.013s, 1010.50/s)  LR: 1.877e-05  Data: 0.011 (0.017)
Train: 282 [ 600/1251 ( 48%)]  Loss: 3.168 (3.00)  Time: 1.009s, 1014.61/s  (1.013s, 1011.34/s)  LR: 1.877e-05  Data: 0.012 (0.016)
Train: 282 [ 650/1251 ( 52%)]  Loss: 2.970 (3.00)  Time: 0.995s, 1029.02/s  (1.012s, 1012.26/s)  LR: 1.877e-05  Data: 0.011 (0.016)
Train: 282 [ 700/1251 ( 56%)]  Loss: 2.982 (3.00)  Time: 1.007s, 1016.97/s  (1.011s, 1013.02/s)  LR: 1.877e-05  Data: 0.013 (0.015)
Train: 282 [ 750/1251 ( 60%)]  Loss: 3.354 (3.02)  Time: 0.994s, 1029.68/s  (1.010s, 1013.79/s)  LR: 1.877e-05  Data: 0.011 (0.015)
Train: 282 [ 800/1251 ( 64%)]  Loss: 2.954 (3.02)  Time: 1.002s, 1021.74/s  (1.009s, 1014.52/s)  LR: 1.877e-05  Data: 0.011 (0.015)
Train: 282 [ 850/1251 ( 68%)]  Loss: 2.962 (3.01)  Time: 0.994s, 1030.06/s  (1.009s, 1015.13/s)  LR: 1.877e-05  Data: 0.012 (0.015)
Train: 282 [ 900/1251 ( 72%)]  Loss: 3.024 (3.02)  Time: 1.014s, 1010.29/s  (1.008s, 1015.57/s)  LR: 1.877e-05  Data: 0.011 (0.015)
Train: 282 [ 950/1251 ( 76%)]  Loss: 2.973 (3.01)  Time: 0.999s, 1024.69/s  (1.008s, 1015.92/s)  LR: 1.877e-05  Data: 0.012 (0.014)
Train: 282 [1000/1251 ( 80%)]  Loss: 2.916 (3.01)  Time: 0.995s, 1028.71/s  (1.008s, 1016.17/s)  LR: 1.877e-05  Data: 0.011 (0.014)
Train: 282 [1050/1251 ( 84%)]  Loss: 3.062 (3.01)  Time: 0.998s, 1026.44/s  (1.007s, 1016.49/s)  LR: 1.877e-05  Data: 0.011 (0.014)
Train: 282 [1100/1251 ( 88%)]  Loss: 3.049 (3.01)  Time: 1.037s,  987.75/s  (1.007s, 1016.91/s)  LR: 1.877e-05  Data: 0.014 (0.014)
Train: 282 [1150/1251 ( 92%)]  Loss: 3.347 (3.03)  Time: 0.998s, 1026.43/s  (1.007s, 1017.18/s)  LR: 1.877e-05  Data: 0.012 (0.014)
Train: 282 [1200/1251 ( 96%)]  Loss: 3.292 (3.04)  Time: 1.059s,  966.95/s  (1.008s, 1015.40/s)  LR: 1.877e-05  Data: 0.011 (0.014)
Train: 282 [1250/1251 (100%)]  Loss: 2.828 (3.03)  Time: 0.985s, 1039.50/s  (1.009s, 1015.14/s)  LR: 1.877e-05  Data: 0.000 (0.014)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.665 (1.665)  Loss:  0.6369 (0.6369)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.245 (0.570)  Loss:  0.7329 (1.0700)  Acc@1: 86.9104 (80.2720)  Acc@5: 97.9953 (95.1820)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-280.pth.tar', 80.29600005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-282.pth.tar', 80.2720000024414)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-276.pth.tar', 80.26400002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-281.pth.tar', 80.24400002929687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-277.pth.tar', 80.22399995117188)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-278.pth.tar', 80.19800018310546)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-273.pth.tar', 80.1879999243164)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-279.pth.tar', 80.12200005371093)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-272.pth.tar', 80.0940000805664)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-275.pth.tar', 80.07799995361329)

Train: 283 [   0/1251 (  0%)]  Loss: 2.812 (2.81)  Time: 2.683s,  381.69/s  (2.683s,  381.69/s)  LR: 1.782e-05  Data: 1.716 (1.716)
Train: 283 [  50/1251 (  4%)]  Loss: 3.026 (2.92)  Time: 0.997s, 1026.80/s  (1.036s,  988.81/s)  LR: 1.782e-05  Data: 0.011 (0.045)
Train: 283 [ 100/1251 (  8%)]  Loss: 3.120 (2.99)  Time: 0.996s, 1028.04/s  (1.023s, 1000.93/s)  LR: 1.782e-05  Data: 0.011 (0.029)
Train: 283 [ 150/1251 ( 12%)]  Loss: 3.044 (3.00)  Time: 0.995s, 1029.26/s  (1.016s, 1007.94/s)  LR: 1.782e-05  Data: 0.012 (0.023)
Train: 283 [ 200/1251 ( 16%)]  Loss: 3.155 (3.03)  Time: 0.996s, 1027.76/s  (1.011s, 1012.71/s)  LR: 1.782e-05  Data: 0.010 (0.020)
Train: 283 [ 250/1251 ( 20%)]  Loss: 3.348 (3.08)  Time: 1.061s,  965.05/s  (1.008s, 1015.52/s)  LR: 1.782e-05  Data: 0.012 (0.018)
Train: 283 [ 300/1251 ( 24%)]  Loss: 3.377 (3.13)  Time: 1.028s,  996.42/s  (1.008s, 1016.03/s)  LR: 1.782e-05  Data: 0.011 (0.017)
Train: 283 [ 350/1251 ( 28%)]  Loss: 3.055 (3.12)  Time: 0.993s, 1030.83/s  (1.007s, 1016.40/s)  LR: 1.782e-05  Data: 0.011 (0.016)
Train: 283 [ 400/1251 ( 32%)]  Loss: 2.884 (3.09)  Time: 0.997s, 1026.77/s  (1.007s, 1017.26/s)  LR: 1.782e-05  Data: 0.011 (0.016)
Train: 283 [ 450/1251 ( 36%)]  Loss: 3.042 (3.09)  Time: 0.996s, 1027.74/s  (1.006s, 1017.82/s)  LR: 1.782e-05  Data: 0.011 (0.015)
Train: 283 [ 500/1251 ( 40%)]  Loss: 3.141 (3.09)  Time: 0.996s, 1027.95/s  (1.006s, 1017.83/s)  LR: 1.782e-05  Data: 0.012 (0.015)
Train: 283 [ 550/1251 ( 44%)]  Loss: 3.248 (3.10)  Time: 0.997s, 1027.26/s  (1.005s, 1018.43/s)  LR: 1.782e-05  Data: 0.011 (0.014)
Train: 283 [ 600/1251 ( 48%)]  Loss: 3.272 (3.12)  Time: 1.001s, 1022.94/s  (1.006s, 1018.05/s)  LR: 1.782e-05  Data: 0.010 (0.014)
Train: 283 [ 650/1251 ( 52%)]  Loss: 3.107 (3.12)  Time: 0.999s, 1024.62/s  (1.005s, 1018.55/s)  LR: 1.782e-05  Data: 0.012 (0.014)
Train: 283 [ 700/1251 ( 56%)]  Loss: 3.276 (3.13)  Time: 0.995s, 1029.19/s  (1.005s, 1018.62/s)  LR: 1.782e-05  Data: 0.011 (0.014)
Train: 283 [ 750/1251 ( 60%)]  Loss: 2.985 (3.12)  Time: 0.999s, 1024.83/s  (1.005s, 1018.55/s)  LR: 1.782e-05  Data: 0.011 (0.014)
Train: 283 [ 800/1251 ( 64%)]  Loss: 2.987 (3.11)  Time: 0.994s, 1030.45/s  (1.005s, 1018.95/s)  LR: 1.782e-05  Data: 0.011 (0.013)
Train: 283 [ 850/1251 ( 68%)]  Loss: 3.380 (3.13)  Time: 0.996s, 1028.33/s  (1.005s, 1019.33/s)  LR: 1.782e-05  Data: 0.011 (0.013)
Train: 283 [ 900/1251 ( 72%)]  Loss: 3.100 (3.12)  Time: 0.998s, 1026.50/s  (1.005s, 1019.28/s)  LR: 1.782e-05  Data: 0.011 (0.013)
Train: 283 [ 950/1251 ( 76%)]  Loss: 3.031 (3.12)  Time: 0.995s, 1029.26/s  (1.004s, 1019.46/s)  LR: 1.782e-05  Data: 0.012 (0.013)
Train: 283 [1000/1251 ( 80%)]  Loss: 3.183 (3.12)  Time: 0.999s, 1025.15/s  (1.004s, 1019.71/s)  LR: 1.782e-05  Data: 0.011 (0.013)
Train: 283 [1050/1251 ( 84%)]  Loss: 2.835 (3.11)  Time: 0.992s, 1032.48/s  (1.004s, 1019.79/s)  LR: 1.782e-05  Data: 0.011 (0.013)
Train: 283 [1100/1251 ( 88%)]  Loss: 3.074 (3.11)  Time: 0.994s, 1030.26/s  (1.004s, 1020.07/s)  LR: 1.782e-05  Data: 0.012 (0.013)
Train: 283 [1150/1251 ( 92%)]  Loss: 3.269 (3.11)  Time: 0.994s, 1030.25/s  (1.004s, 1020.37/s)  LR: 1.782e-05  Data: 0.011 (0.013)
Train: 283 [1200/1251 ( 96%)]  Loss: 3.437 (3.13)  Time: 0.997s, 1027.09/s  (1.003s, 1020.63/s)  LR: 1.782e-05  Data: 0.011 (0.013)
Train: 283 [1250/1251 (100%)]  Loss: 3.149 (3.13)  Time: 0.983s, 1041.75/s  (1.003s, 1020.54/s)  LR: 1.782e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.654 (1.654)  Loss:  0.6429 (0.6429)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.245 (0.569)  Loss:  0.7459 (1.0754)  Acc@1: 87.1462 (80.2340)  Acc@5: 97.8774 (95.1280)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-280.pth.tar', 80.29600005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-282.pth.tar', 80.2720000024414)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-276.pth.tar', 80.26400002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-281.pth.tar', 80.24400002929687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-283.pth.tar', 80.23399997558593)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-277.pth.tar', 80.22399995117188)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-278.pth.tar', 80.19800018310546)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-273.pth.tar', 80.1879999243164)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-279.pth.tar', 80.12200005371093)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-272.pth.tar', 80.0940000805664)

Train: 284 [   0/1251 (  0%)]  Loss: 2.831 (2.83)  Time: 2.421s,  423.04/s  (2.421s,  423.04/s)  LR: 1.693e-05  Data: 1.459 (1.459)
Train: 284 [  50/1251 (  4%)]  Loss: 3.043 (2.94)  Time: 1.036s,  988.64/s  (1.032s,  992.42/s)  LR: 1.693e-05  Data: 0.011 (0.041)
Train: 284 [ 100/1251 (  8%)]  Loss: 3.088 (2.99)  Time: 0.994s, 1029.93/s  (1.024s, 1000.07/s)  LR: 1.693e-05  Data: 0.011 (0.026)
Train: 284 [ 150/1251 ( 12%)]  Loss: 3.072 (3.01)  Time: 1.001s, 1022.78/s  (1.016s, 1007.73/s)  LR: 1.693e-05  Data: 0.011 (0.021)
Train: 284 [ 200/1251 ( 16%)]  Loss: 3.014 (3.01)  Time: 1.010s, 1013.64/s  (1.016s, 1008.25/s)  LR: 1.693e-05  Data: 0.011 (0.019)
Train: 284 [ 250/1251 ( 20%)]  Loss: 3.005 (3.01)  Time: 0.995s, 1028.88/s  (1.013s, 1011.30/s)  LR: 1.693e-05  Data: 0.011 (0.017)
Train: 284 [ 300/1251 ( 24%)]  Loss: 3.204 (3.04)  Time: 0.995s, 1028.98/s  (1.011s, 1013.27/s)  LR: 1.693e-05  Data: 0.011 (0.016)
Train: 284 [ 350/1251 ( 28%)]  Loss: 2.843 (3.01)  Time: 0.996s, 1027.83/s  (1.009s, 1014.42/s)  LR: 1.693e-05  Data: 0.011 (0.016)
Train: 284 [ 400/1251 ( 32%)]  Loss: 3.234 (3.04)  Time: 1.019s, 1005.33/s  (1.009s, 1014.99/s)  LR: 1.693e-05  Data: 0.016 (0.015)
Train: 284 [ 450/1251 ( 36%)]  Loss: 2.966 (3.03)  Time: 0.998s, 1026.37/s  (1.008s, 1016.09/s)  LR: 1.693e-05  Data: 0.012 (0.015)
Train: 284 [ 500/1251 ( 40%)]  Loss: 3.132 (3.04)  Time: 0.998s, 1025.85/s  (1.007s, 1016.65/s)  LR: 1.693e-05  Data: 0.011 (0.014)
Train: 284 [ 550/1251 ( 44%)]  Loss: 3.037 (3.04)  Time: 0.997s, 1027.34/s  (1.007s, 1017.22/s)  LR: 1.693e-05  Data: 0.011 (0.014)
Train: 284 [ 600/1251 ( 48%)]  Loss: 3.262 (3.06)  Time: 0.999s, 1024.81/s  (1.006s, 1017.86/s)  LR: 1.693e-05  Data: 0.011 (0.014)
Train: 284 [ 650/1251 ( 52%)]  Loss: 3.040 (3.06)  Time: 0.994s, 1030.32/s  (1.005s, 1018.54/s)  LR: 1.693e-05  Data: 0.011 (0.014)
Train: 284 [ 700/1251 ( 56%)]  Loss: 3.192 (3.06)  Time: 0.998s, 1026.03/s  (1.005s, 1019.04/s)  LR: 1.693e-05  Data: 0.012 (0.013)
Train: 284 [ 750/1251 ( 60%)]  Loss: 3.014 (3.06)  Time: 0.997s, 1026.58/s  (1.005s, 1019.33/s)  LR: 1.693e-05  Data: 0.012 (0.013)
Train: 284 [ 800/1251 ( 64%)]  Loss: 2.904 (3.05)  Time: 0.995s, 1029.46/s  (1.004s, 1019.50/s)  LR: 1.693e-05  Data: 0.011 (0.013)
Train: 284 [ 850/1251 ( 68%)]  Loss: 2.990 (3.05)  Time: 0.996s, 1027.69/s  (1.004s, 1019.65/s)  LR: 1.693e-05  Data: 0.012 (0.013)
Train: 284 [ 900/1251 ( 72%)]  Loss: 3.218 (3.06)  Time: 0.997s, 1026.85/s  (1.004s, 1019.93/s)  LR: 1.693e-05  Data: 0.011 (0.013)
Train: 284 [ 950/1251 ( 76%)]  Loss: 3.075 (3.06)  Time: 0.992s, 1032.19/s  (1.004s, 1020.24/s)  LR: 1.693e-05  Data: 0.011 (0.013)
Train: 284 [1000/1251 ( 80%)]  Loss: 3.106 (3.06)  Time: 0.995s, 1029.49/s  (1.003s, 1020.60/s)  LR: 1.693e-05  Data: 0.011 (0.013)
Train: 284 [1050/1251 ( 84%)]  Loss: 2.910 (3.05)  Time: 1.007s, 1016.39/s  (1.003s, 1020.78/s)  LR: 1.693e-05  Data: 0.012 (0.013)
Train: 284 [1100/1251 ( 88%)]  Loss: 3.475 (3.07)  Time: 0.995s, 1028.70/s  (1.003s, 1021.05/s)  LR: 1.693e-05  Data: 0.011 (0.013)
Train: 284 [1150/1251 ( 92%)]  Loss: 3.090 (3.07)  Time: 0.994s, 1030.69/s  (1.003s, 1021.33/s)  LR: 1.693e-05  Data: 0.010 (0.013)
Train: 284 [1200/1251 ( 96%)]  Loss: 3.292 (3.08)  Time: 1.004s, 1019.51/s  (1.003s, 1021.31/s)  LR: 1.693e-05  Data: 0.011 (0.013)
Train: 284 [1250/1251 (100%)]  Loss: 2.829 (3.07)  Time: 0.987s, 1037.78/s  (1.003s, 1021.41/s)  LR: 1.693e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.632 (1.632)  Loss:  0.6640 (0.6640)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.245 (0.568)  Loss:  0.7725 (1.1088)  Acc@1: 87.2641 (80.1640)  Acc@5: 97.6415 (95.1840)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-280.pth.tar', 80.29600005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-282.pth.tar', 80.2720000024414)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-276.pth.tar', 80.26400002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-281.pth.tar', 80.24400002929687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-283.pth.tar', 80.23399997558593)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-277.pth.tar', 80.22399995117188)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-278.pth.tar', 80.19800018310546)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-273.pth.tar', 80.1879999243164)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-284.pth.tar', 80.16399989746094)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-279.pth.tar', 80.12200005371093)

Train: 285 [   0/1251 (  0%)]  Loss: 2.972 (2.97)  Time: 2.397s,  427.14/s  (2.397s,  427.14/s)  LR: 1.609e-05  Data: 1.434 (1.434)
Train: 285 [  50/1251 (  4%)]  Loss: 3.077 (3.02)  Time: 1.037s,  987.80/s  (1.043s,  981.74/s)  LR: 1.609e-05  Data: 0.011 (0.040)
Train: 285 [ 100/1251 (  8%)]  Loss: 2.758 (2.94)  Time: 0.999s, 1025.01/s  (1.022s, 1001.90/s)  LR: 1.609e-05  Data: 0.012 (0.026)
Train: 285 [ 150/1251 ( 12%)]  Loss: 3.230 (3.01)  Time: 0.995s, 1029.25/s  (1.015s, 1008.91/s)  LR: 1.609e-05  Data: 0.012 (0.021)
Train: 285 [ 200/1251 ( 16%)]  Loss: 3.078 (3.02)  Time: 0.995s, 1029.50/s  (1.011s, 1012.94/s)  LR: 1.609e-05  Data: 0.011 (0.019)
Train: 285 [ 250/1251 ( 20%)]  Loss: 3.004 (3.02)  Time: 0.991s, 1033.07/s  (1.008s, 1015.54/s)  LR: 1.609e-05  Data: 0.011 (0.017)
Train: 285 [ 300/1251 ( 24%)]  Loss: 3.055 (3.02)  Time: 0.997s, 1026.90/s  (1.007s, 1016.95/s)  LR: 1.609e-05  Data: 0.012 (0.016)
Train: 285 [ 350/1251 ( 28%)]  Loss: 3.379 (3.07)  Time: 0.994s, 1030.11/s  (1.006s, 1017.98/s)  LR: 1.609e-05  Data: 0.011 (0.015)
Train: 285 [ 400/1251 ( 32%)]  Loss: 3.389 (3.10)  Time: 1.005s, 1018.68/s  (1.006s, 1018.26/s)  LR: 1.609e-05  Data: 0.011 (0.015)
Train: 285 [ 450/1251 ( 36%)]  Loss: 3.180 (3.11)  Time: 1.037s,  987.05/s  (1.005s, 1018.84/s)  LR: 1.609e-05  Data: 0.011 (0.014)
Train: 285 [ 500/1251 ( 40%)]  Loss: 3.054 (3.11)  Time: 0.998s, 1025.56/s  (1.005s, 1019.41/s)  LR: 1.609e-05  Data: 0.011 (0.014)
Train: 285 [ 550/1251 ( 44%)]  Loss: 3.303 (3.12)  Time: 1.043s,  981.37/s  (1.004s, 1019.58/s)  LR: 1.609e-05  Data: 0.010 (0.014)
Train: 285 [ 600/1251 ( 48%)]  Loss: 3.219 (3.13)  Time: 0.992s, 1032.08/s  (1.005s, 1019.35/s)  LR: 1.609e-05  Data: 0.011 (0.014)
Train: 285 [ 650/1251 ( 52%)]  Loss: 3.359 (3.15)  Time: 1.025s,  998.77/s  (1.005s, 1019.41/s)  LR: 1.609e-05  Data: 0.011 (0.013)
Train: 285 [ 700/1251 ( 56%)]  Loss: 3.019 (3.14)  Time: 0.996s, 1028.32/s  (1.004s, 1019.45/s)  LR: 1.609e-05  Data: 0.011 (0.013)
Train: 285 [ 750/1251 ( 60%)]  Loss: 3.123 (3.14)  Time: 1.049s,  975.98/s  (1.005s, 1018.69/s)  LR: 1.609e-05  Data: 0.011 (0.013)
Train: 285 [ 800/1251 ( 64%)]  Loss: 2.957 (3.13)  Time: 1.001s, 1023.47/s  (1.007s, 1016.94/s)  LR: 1.609e-05  Data: 0.012 (0.013)
Train: 285 [ 850/1251 ( 68%)]  Loss: 2.798 (3.11)  Time: 0.997s, 1026.83/s  (1.007s, 1016.51/s)  LR: 1.609e-05  Data: 0.012 (0.013)
Train: 285 [ 900/1251 ( 72%)]  Loss: 3.019 (3.10)  Time: 1.060s,  966.28/s  (1.008s, 1016.09/s)  LR: 1.609e-05  Data: 0.011 (0.013)
Train: 285 [ 950/1251 ( 76%)]  Loss: 3.253 (3.11)  Time: 1.060s,  966.35/s  (1.010s, 1013.85/s)  LR: 1.609e-05  Data: 0.011 (0.013)
Train: 285 [1000/1251 ( 80%)]  Loss: 3.010 (3.11)  Time: 0.994s, 1030.09/s  (1.010s, 1013.37/s)  LR: 1.609e-05  Data: 0.011 (0.013)
Train: 285 [1050/1251 ( 84%)]  Loss: 3.144 (3.11)  Time: 0.994s, 1029.72/s  (1.010s, 1013.78/s)  LR: 1.609e-05  Data: 0.011 (0.013)
Train: 285 [1100/1251 ( 88%)]  Loss: 3.220 (3.11)  Time: 0.994s, 1029.68/s  (1.010s, 1014.18/s)  LR: 1.609e-05  Data: 0.012 (0.013)
Train: 285 [1150/1251 ( 92%)]  Loss: 3.314 (3.12)  Time: 0.996s, 1027.87/s  (1.009s, 1014.58/s)  LR: 1.609e-05  Data: 0.011 (0.012)
Train: 285 [1200/1251 ( 96%)]  Loss: 3.267 (3.13)  Time: 0.996s, 1028.03/s  (1.009s, 1014.78/s)  LR: 1.609e-05  Data: 0.012 (0.012)
Train: 285 [1250/1251 (100%)]  Loss: 2.945 (3.12)  Time: 1.026s,  997.62/s  (1.009s, 1015.10/s)  LR: 1.609e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.606 (1.606)  Loss:  0.6098 (0.6098)  Acc@1: 92.2852 (92.2852)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.245 (0.579)  Loss:  0.7264 (1.0560)  Acc@1: 87.5000 (80.3440)  Acc@5: 97.9953 (95.2280)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-285.pth.tar', 80.344)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-280.pth.tar', 80.29600005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-282.pth.tar', 80.2720000024414)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-276.pth.tar', 80.26400002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-281.pth.tar', 80.24400002929687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-283.pth.tar', 80.23399997558593)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-277.pth.tar', 80.22399995117188)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-278.pth.tar', 80.19800018310546)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-273.pth.tar', 80.1879999243164)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-284.pth.tar', 80.16399989746094)

Train: 286 [   0/1251 (  0%)]  Loss: 2.891 (2.89)  Time: 2.438s,  419.96/s  (2.438s,  419.96/s)  LR: 1.531e-05  Data: 1.477 (1.477)
Train: 286 [  50/1251 (  4%)]  Loss: 3.088 (2.99)  Time: 1.023s, 1001.20/s  (1.034s,  990.24/s)  LR: 1.531e-05  Data: 0.011 (0.041)
Train: 286 [ 100/1251 (  8%)]  Loss: 2.700 (2.89)  Time: 1.033s,  991.32/s  (1.032s,  992.02/s)  LR: 1.531e-05  Data: 0.011 (0.026)
Train: 286 [ 150/1251 ( 12%)]  Loss: 3.253 (2.98)  Time: 1.057s,  969.01/s  (1.037s,  987.78/s)  LR: 1.531e-05  Data: 0.011 (0.021)
Train: 286 [ 200/1251 ( 16%)]  Loss: 2.746 (2.94)  Time: 0.999s, 1025.34/s  (1.030s,  994.12/s)  LR: 1.531e-05  Data: 0.012 (0.019)
Train: 286 [ 250/1251 ( 20%)]  Loss: 3.081 (2.96)  Time: 0.994s, 1029.84/s  (1.025s,  999.28/s)  LR: 1.531e-05  Data: 0.010 (0.017)
Train: 286 [ 300/1251 ( 24%)]  Loss: 3.038 (2.97)  Time: 1.002s, 1022.04/s  (1.021s, 1002.89/s)  LR: 1.531e-05  Data: 0.011 (0.016)
Train: 286 [ 350/1251 ( 28%)]  Loss: 3.118 (2.99)  Time: 1.003s, 1021.28/s  (1.019s, 1004.60/s)  LR: 1.531e-05  Data: 0.013 (0.016)
Train: 286 [ 400/1251 ( 32%)]  Loss: 3.245 (3.02)  Time: 1.008s, 1015.51/s  (1.017s, 1006.87/s)  LR: 1.531e-05  Data: 0.012 (0.015)
Train: 286 [ 450/1251 ( 36%)]  Loss: 3.092 (3.03)  Time: 1.005s, 1018.44/s  (1.015s, 1008.69/s)  LR: 1.531e-05  Data: 0.011 (0.015)
Train: 286 [ 500/1251 ( 40%)]  Loss: 3.141 (3.04)  Time: 0.993s, 1030.71/s  (1.013s, 1010.53/s)  LR: 1.531e-05  Data: 0.010 (0.014)
Train: 286 [ 550/1251 ( 44%)]  Loss: 2.880 (3.02)  Time: 0.993s, 1031.01/s  (1.012s, 1011.93/s)  LR: 1.531e-05  Data: 0.011 (0.014)
Train: 286 [ 600/1251 ( 48%)]  Loss: 2.949 (3.02)  Time: 0.997s, 1026.96/s  (1.011s, 1012.64/s)  LR: 1.531e-05  Data: 0.011 (0.014)
Train: 286 [ 650/1251 ( 52%)]  Loss: 2.922 (3.01)  Time: 0.992s, 1031.99/s  (1.011s, 1013.22/s)  LR: 1.531e-05  Data: 0.011 (0.014)
Train: 286 [ 700/1251 ( 56%)]  Loss: 3.117 (3.02)  Time: 1.001s, 1022.98/s  (1.010s, 1014.08/s)  LR: 1.531e-05  Data: 0.011 (0.013)
Train: 286 [ 750/1251 ( 60%)]  Loss: 3.042 (3.02)  Time: 1.004s, 1019.48/s  (1.010s, 1014.15/s)  LR: 1.531e-05  Data: 0.011 (0.013)
Train: 286 [ 800/1251 ( 64%)]  Loss: 2.968 (3.02)  Time: 0.998s, 1025.90/s  (1.009s, 1014.71/s)  LR: 1.531e-05  Data: 0.011 (0.013)
Train: 286 [ 850/1251 ( 68%)]  Loss: 2.990 (3.01)  Time: 0.995s, 1028.71/s  (1.009s, 1015.32/s)  LR: 1.531e-05  Data: 0.011 (0.013)
Train: 286 [ 900/1251 ( 72%)]  Loss: 2.909 (3.01)  Time: 1.002s, 1021.96/s  (1.008s, 1015.84/s)  LR: 1.531e-05  Data: 0.012 (0.013)
Train: 286 [ 950/1251 ( 76%)]  Loss: 3.035 (3.01)  Time: 0.998s, 1026.33/s  (1.008s, 1016.19/s)  LR: 1.531e-05  Data: 0.011 (0.013)
Train: 286 [1000/1251 ( 80%)]  Loss: 3.205 (3.02)  Time: 0.995s, 1028.74/s  (1.009s, 1014.70/s)  LR: 1.531e-05  Data: 0.012 (0.013)
Train: 286 [1050/1251 ( 84%)]  Loss: 3.144 (3.03)  Time: 1.001s, 1023.41/s  (1.009s, 1015.07/s)  LR: 1.531e-05  Data: 0.012 (0.013)
Train: 286 [1100/1251 ( 88%)]  Loss: 3.072 (3.03)  Time: 1.000s, 1024.47/s  (1.009s, 1015.27/s)  LR: 1.531e-05  Data: 0.011 (0.013)
Train: 286 [1150/1251 ( 92%)]  Loss: 3.368 (3.04)  Time: 0.995s, 1028.63/s  (1.008s, 1015.79/s)  LR: 1.531e-05  Data: 0.011 (0.013)
Train: 286 [1200/1251 ( 96%)]  Loss: 3.240 (3.05)  Time: 0.997s, 1027.44/s  (1.008s, 1016.06/s)  LR: 1.531e-05  Data: 0.011 (0.012)
Train: 286 [1250/1251 (100%)]  Loss: 2.989 (3.05)  Time: 0.986s, 1038.09/s  (1.008s, 1015.43/s)  LR: 1.531e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.642 (1.642)  Loss:  0.6178 (0.6178)  Acc@1: 92.2852 (92.2852)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.245 (0.568)  Loss:  0.7257 (1.0493)  Acc@1: 86.9104 (80.2340)  Acc@5: 97.9953 (95.2000)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-285.pth.tar', 80.344)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-280.pth.tar', 80.29600005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-282.pth.tar', 80.2720000024414)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-276.pth.tar', 80.26400002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-281.pth.tar', 80.24400002929687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-283.pth.tar', 80.23399997558593)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-286.pth.tar', 80.23399987304687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-277.pth.tar', 80.22399995117188)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-278.pth.tar', 80.19800018310546)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-273.pth.tar', 80.1879999243164)

Train: 287 [   0/1251 (  0%)]  Loss: 3.002 (3.00)  Time: 2.634s,  388.76/s  (2.634s,  388.76/s)  LR: 1.458e-05  Data: 1.668 (1.668)
Train: 287 [  50/1251 (  4%)]  Loss: 3.056 (3.03)  Time: 1.004s, 1020.36/s  (1.036s,  988.43/s)  LR: 1.458e-05  Data: 0.014 (0.048)
Train: 287 [ 100/1251 (  8%)]  Loss: 2.927 (3.00)  Time: 1.000s, 1023.89/s  (1.018s, 1005.74/s)  LR: 1.458e-05  Data: 0.011 (0.030)
Train: 287 [ 150/1251 ( 12%)]  Loss: 2.965 (2.99)  Time: 0.998s, 1025.67/s  (1.012s, 1011.78/s)  LR: 1.458e-05  Data: 0.011 (0.023)
Train: 287 [ 200/1251 ( 16%)]  Loss: 2.866 (2.96)  Time: 1.004s, 1019.80/s  (1.009s, 1014.37/s)  LR: 1.458e-05  Data: 0.012 (0.020)
Train: 287 [ 250/1251 ( 20%)]  Loss: 3.242 (3.01)  Time: 1.042s,  982.58/s  (1.011s, 1013.10/s)  LR: 1.458e-05  Data: 0.010 (0.019)
Train: 287 [ 300/1251 ( 24%)]  Loss: 3.296 (3.05)  Time: 0.995s, 1029.27/s  (1.010s, 1013.66/s)  LR: 1.458e-05  Data: 0.011 (0.017)
Train: 287 [ 350/1251 ( 28%)]  Loss: 3.249 (3.08)  Time: 0.992s, 1031.91/s  (1.008s, 1015.39/s)  LR: 1.458e-05  Data: 0.010 (0.016)
Train: 287 [ 400/1251 ( 32%)]  Loss: 3.080 (3.08)  Time: 0.994s, 1029.83/s  (1.008s, 1015.97/s)  LR: 1.458e-05  Data: 0.011 (0.016)
Train: 287 [ 450/1251 ( 36%)]  Loss: 3.140 (3.08)  Time: 1.000s, 1023.49/s  (1.007s, 1016.72/s)  LR: 1.458e-05  Data: 0.012 (0.015)
Train: 287 [ 500/1251 ( 40%)]  Loss: 2.751 (3.05)  Time: 0.999s, 1025.50/s  (1.006s, 1017.43/s)  LR: 1.458e-05  Data: 0.012 (0.015)
Train: 287 [ 550/1251 ( 44%)]  Loss: 3.138 (3.06)  Time: 1.033s,  991.24/s  (1.008s, 1015.47/s)  LR: 1.458e-05  Data: 0.011 (0.015)
Train: 287 [ 600/1251 ( 48%)]  Loss: 2.730 (3.03)  Time: 1.012s, 1011.57/s  (1.009s, 1015.28/s)  LR: 1.458e-05  Data: 0.012 (0.014)
Train: 287 [ 650/1251 ( 52%)]  Loss: 3.419 (3.06)  Time: 0.998s, 1026.40/s  (1.008s, 1015.58/s)  LR: 1.458e-05  Data: 0.011 (0.014)
Train: 287 [ 700/1251 ( 56%)]  Loss: 3.005 (3.06)  Time: 0.994s, 1029.77/s  (1.008s, 1016.36/s)  LR: 1.458e-05  Data: 0.011 (0.014)
Train: 287 [ 750/1251 ( 60%)]  Loss: 2.913 (3.05)  Time: 1.004s, 1019.96/s  (1.007s, 1016.83/s)  LR: 1.458e-05  Data: 0.012 (0.014)
Train: 287 [ 800/1251 ( 64%)]  Loss: 3.192 (3.06)  Time: 0.998s, 1026.08/s  (1.007s, 1017.20/s)  LR: 1.458e-05  Data: 0.011 (0.013)
Train: 287 [ 850/1251 ( 68%)]  Loss: 2.971 (3.05)  Time: 0.997s, 1027.01/s  (1.007s, 1017.24/s)  LR: 1.458e-05  Data: 0.010 (0.013)
Train: 287 [ 900/1251 ( 72%)]  Loss: 2.759 (3.04)  Time: 0.994s, 1030.55/s  (1.006s, 1017.70/s)  LR: 1.458e-05  Data: 0.011 (0.013)
Train: 287 [ 950/1251 ( 76%)]  Loss: 3.194 (3.04)  Time: 1.000s, 1024.19/s  (1.006s, 1018.09/s)  LR: 1.458e-05  Data: 0.011 (0.013)
Train: 287 [1000/1251 ( 80%)]  Loss: 3.161 (3.05)  Time: 0.996s, 1028.28/s  (1.006s, 1018.05/s)  LR: 1.458e-05  Data: 0.011 (0.013)
Train: 287 [1050/1251 ( 84%)]  Loss: 2.731 (3.04)  Time: 1.003s, 1020.84/s  (1.006s, 1018.10/s)  LR: 1.458e-05  Data: 0.011 (0.013)
Train: 287 [1100/1251 ( 88%)]  Loss: 2.892 (3.03)  Time: 0.999s, 1025.18/s  (1.006s, 1018.30/s)  LR: 1.458e-05  Data: 0.011 (0.013)
Train: 287 [1150/1251 ( 92%)]  Loss: 3.064 (3.03)  Time: 0.997s, 1026.73/s  (1.006s, 1017.85/s)  LR: 1.458e-05  Data: 0.012 (0.013)
Train: 287 [1200/1251 ( 96%)]  Loss: 2.833 (3.02)  Time: 0.994s, 1030.51/s  (1.006s, 1018.07/s)  LR: 1.458e-05  Data: 0.011 (0.013)
Train: 287 [1250/1251 (100%)]  Loss: 2.821 (3.02)  Time: 1.041s,  983.97/s  (1.006s, 1018.11/s)  LR: 1.458e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.643 (1.643)  Loss:  0.5980 (0.5980)  Acc@1: 91.6992 (91.6992)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.245 (0.576)  Loss:  0.7052 (1.0374)  Acc@1: 86.9104 (80.3240)  Acc@5: 98.1132 (95.2140)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-285.pth.tar', 80.344)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-287.pth.tar', 80.32400000244141)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-280.pth.tar', 80.29600005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-282.pth.tar', 80.2720000024414)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-276.pth.tar', 80.26400002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-281.pth.tar', 80.24400002929687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-283.pth.tar', 80.23399997558593)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-286.pth.tar', 80.23399987304687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-277.pth.tar', 80.22399995117188)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-278.pth.tar', 80.19800018310546)

Train: 288 [   0/1251 (  0%)]  Loss: 3.083 (3.08)  Time: 2.465s,  415.50/s  (2.465s,  415.50/s)  LR: 1.390e-05  Data: 1.511 (1.511)
Train: 288 [  50/1251 (  4%)]  Loss: 2.999 (3.04)  Time: 1.011s, 1012.48/s  (1.031s,  993.29/s)  LR: 1.390e-05  Data: 0.011 (0.043)
Train: 288 [ 100/1251 (  8%)]  Loss: 3.113 (3.07)  Time: 1.006s, 1018.31/s  (1.016s, 1007.60/s)  LR: 1.390e-05  Data: 0.011 (0.027)
Train: 288 [ 150/1251 ( 12%)]  Loss: 3.238 (3.11)  Time: 1.001s, 1023.38/s  (1.012s, 1012.08/s)  LR: 1.390e-05  Data: 0.012 (0.022)
Train: 288 [ 200/1251 ( 16%)]  Loss: 2.973 (3.08)  Time: 1.001s, 1022.93/s  (1.009s, 1015.11/s)  LR: 1.390e-05  Data: 0.012 (0.019)
Train: 288 [ 250/1251 ( 20%)]  Loss: 3.481 (3.15)  Time: 1.053s,  972.24/s  (1.007s, 1016.75/s)  LR: 1.390e-05  Data: 0.012 (0.018)
Train: 288 [ 300/1251 ( 24%)]  Loss: 3.123 (3.14)  Time: 0.997s, 1027.46/s  (1.007s, 1017.33/s)  LR: 1.390e-05  Data: 0.011 (0.017)
Train: 288 [ 350/1251 ( 28%)]  Loss: 3.080 (3.14)  Time: 0.995s, 1029.12/s  (1.006s, 1017.84/s)  LR: 1.390e-05  Data: 0.012 (0.016)
Train: 288 [ 400/1251 ( 32%)]  Loss: 2.915 (3.11)  Time: 0.997s, 1026.58/s  (1.005s, 1018.72/s)  LR: 1.390e-05  Data: 0.012 (0.015)
Train: 288 [ 450/1251 ( 36%)]  Loss: 3.064 (3.11)  Time: 0.996s, 1027.71/s  (1.004s, 1019.42/s)  LR: 1.390e-05  Data: 0.010 (0.015)
Train: 288 [ 500/1251 ( 40%)]  Loss: 3.095 (3.11)  Time: 1.006s, 1017.40/s  (1.005s, 1019.21/s)  LR: 1.390e-05  Data: 0.012 (0.015)
Train: 288 [ 550/1251 ( 44%)]  Loss: 3.057 (3.10)  Time: 0.995s, 1028.85/s  (1.004s, 1019.92/s)  LR: 1.390e-05  Data: 0.011 (0.014)
Train: 288 [ 600/1251 ( 48%)]  Loss: 2.836 (3.08)  Time: 0.999s, 1025.13/s  (1.003s, 1020.44/s)  LR: 1.390e-05  Data: 0.011 (0.014)
Train: 288 [ 650/1251 ( 52%)]  Loss: 3.055 (3.08)  Time: 0.998s, 1025.97/s  (1.003s, 1020.74/s)  LR: 1.390e-05  Data: 0.012 (0.014)
Train: 288 [ 700/1251 ( 56%)]  Loss: 3.032 (3.08)  Time: 0.996s, 1027.72/s  (1.003s, 1021.05/s)  LR: 1.390e-05  Data: 0.011 (0.014)
Train: 288 [ 750/1251 ( 60%)]  Loss: 3.232 (3.09)  Time: 1.001s, 1023.01/s  (1.003s, 1020.63/s)  LR: 1.390e-05  Data: 0.012 (0.013)
Train: 288 [ 800/1251 ( 64%)]  Loss: 3.082 (3.09)  Time: 0.997s, 1027.16/s  (1.003s, 1020.94/s)  LR: 1.390e-05  Data: 0.011 (0.013)
Train: 288 [ 850/1251 ( 68%)]  Loss: 2.932 (3.08)  Time: 0.995s, 1028.74/s  (1.003s, 1021.25/s)  LR: 1.390e-05  Data: 0.011 (0.013)
Train: 288 [ 900/1251 ( 72%)]  Loss: 2.947 (3.07)  Time: 1.002s, 1022.39/s  (1.003s, 1021.26/s)  LR: 1.390e-05  Data: 0.013 (0.013)
Train: 288 [ 950/1251 ( 76%)]  Loss: 3.352 (3.08)  Time: 0.995s, 1028.91/s  (1.003s, 1021.36/s)  LR: 1.390e-05  Data: 0.011 (0.013)
Train: 288 [1000/1251 ( 80%)]  Loss: 3.200 (3.09)  Time: 0.996s, 1028.18/s  (1.003s, 1021.28/s)  LR: 1.390e-05  Data: 0.012 (0.013)
Train: 288 [1050/1251 ( 84%)]  Loss: 3.298 (3.10)  Time: 0.994s, 1029.68/s  (1.002s, 1021.51/s)  LR: 1.390e-05  Data: 0.012 (0.013)
Train: 288 [1100/1251 ( 88%)]  Loss: 3.055 (3.10)  Time: 0.999s, 1024.94/s  (1.003s, 1021.25/s)  LR: 1.390e-05  Data: 0.012 (0.013)
Train: 288 [1150/1251 ( 92%)]  Loss: 3.325 (3.11)  Time: 0.996s, 1028.28/s  (1.003s, 1021.17/s)  LR: 1.390e-05  Data: 0.012 (0.013)
Train: 288 [1200/1251 ( 96%)]  Loss: 3.064 (3.11)  Time: 1.048s,  976.77/s  (1.003s, 1020.90/s)  LR: 1.390e-05  Data: 0.011 (0.013)
Train: 288 [1250/1251 (100%)]  Loss: 3.349 (3.11)  Time: 0.983s, 1041.67/s  (1.003s, 1020.98/s)  LR: 1.390e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.730 (1.730)  Loss:  0.6478 (0.6478)  Acc@1: 91.7969 (91.7969)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.246 (0.574)  Loss:  0.7759 (1.0897)  Acc@1: 86.6745 (80.2820)  Acc@5: 97.8774 (95.1860)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-285.pth.tar', 80.344)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-287.pth.tar', 80.32400000244141)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-280.pth.tar', 80.29600005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-288.pth.tar', 80.28200002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-282.pth.tar', 80.2720000024414)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-276.pth.tar', 80.26400002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-281.pth.tar', 80.24400002929687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-283.pth.tar', 80.23399997558593)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-286.pth.tar', 80.23399987304687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-277.pth.tar', 80.22399995117188)

Train: 289 [   0/1251 (  0%)]  Loss: 2.924 (2.92)  Time: 2.461s,  416.04/s  (2.461s,  416.04/s)  LR: 1.328e-05  Data: 1.511 (1.511)
Train: 289 [  50/1251 (  4%)]  Loss: 2.971 (2.95)  Time: 1.033s,  991.06/s  (1.061s,  965.54/s)  LR: 1.328e-05  Data: 0.012 (0.041)
Train: 289 [ 100/1251 (  8%)]  Loss: 3.329 (3.07)  Time: 0.995s, 1028.75/s  (1.043s,  981.89/s)  LR: 1.328e-05  Data: 0.011 (0.026)
Train: 289 [ 150/1251 ( 12%)]  Loss: 3.087 (3.08)  Time: 0.998s, 1026.37/s  (1.029s,  995.06/s)  LR: 1.328e-05  Data: 0.010 (0.021)
Train: 289 [ 200/1251 ( 16%)]  Loss: 3.088 (3.08)  Time: 0.995s, 1029.20/s  (1.025s,  999.46/s)  LR: 1.328e-05  Data: 0.012 (0.019)
Train: 289 [ 250/1251 ( 20%)]  Loss: 3.192 (3.10)  Time: 1.058s,  967.56/s  (1.022s, 1002.44/s)  LR: 1.328e-05  Data: 0.013 (0.017)
Train: 289 [ 300/1251 ( 24%)]  Loss: 3.129 (3.10)  Time: 0.995s, 1028.85/s  (1.018s, 1005.64/s)  LR: 1.328e-05  Data: 0.011 (0.016)
Train: 289 [ 350/1251 ( 28%)]  Loss: 3.208 (3.12)  Time: 0.989s, 1035.39/s  (1.016s, 1008.23/s)  LR: 1.328e-05  Data: 0.010 (0.015)
Train: 289 [ 400/1251 ( 32%)]  Loss: 3.362 (3.14)  Time: 0.993s, 1030.94/s  (1.014s, 1010.14/s)  LR: 1.328e-05  Data: 0.010 (0.015)
Train: 289 [ 450/1251 ( 36%)]  Loss: 3.279 (3.16)  Time: 0.990s, 1034.09/s  (1.013s, 1010.92/s)  LR: 1.328e-05  Data: 0.010 (0.014)
Train: 289 [ 500/1251 ( 40%)]  Loss: 2.830 (3.13)  Time: 0.994s, 1029.72/s  (1.011s, 1012.41/s)  LR: 1.328e-05  Data: 0.010 (0.014)
Train: 289 [ 550/1251 ( 44%)]  Loss: 3.157 (3.13)  Time: 0.994s, 1030.24/s  (1.010s, 1013.42/s)  LR: 1.328e-05  Data: 0.010 (0.014)
Train: 289 [ 600/1251 ( 48%)]  Loss: 2.972 (3.12)  Time: 1.051s,  974.13/s  (1.010s, 1014.14/s)  LR: 1.328e-05  Data: 0.010 (0.014)
Train: 289 [ 650/1251 ( 52%)]  Loss: 3.072 (3.11)  Time: 0.998s, 1026.14/s  (1.009s, 1014.88/s)  LR: 1.328e-05  Data: 0.012 (0.013)
Train: 289 [ 700/1251 ( 56%)]  Loss: 3.055 (3.11)  Time: 0.996s, 1028.42/s  (1.008s, 1015.60/s)  LR: 1.328e-05  Data: 0.012 (0.013)
Train: 289 [ 750/1251 ( 60%)]  Loss: 2.891 (3.10)  Time: 1.060s,  966.34/s  (1.008s, 1015.54/s)  LR: 1.328e-05  Data: 0.011 (0.013)
Train: 289 [ 800/1251 ( 64%)]  Loss: 3.156 (3.10)  Time: 1.000s, 1023.91/s  (1.010s, 1014.21/s)  LR: 1.328e-05  Data: 0.011 (0.013)
Train: 289 [ 850/1251 ( 68%)]  Loss: 2.953 (3.09)  Time: 1.000s, 1024.19/s  (1.009s, 1014.78/s)  LR: 1.328e-05  Data: 0.011 (0.013)
Train: 289 [ 900/1251 ( 72%)]  Loss: 3.392 (3.11)  Time: 0.999s, 1024.96/s  (1.009s, 1014.65/s)  LR: 1.328e-05  Data: 0.011 (0.013)
Train: 289 [ 950/1251 ( 76%)]  Loss: 2.902 (3.10)  Time: 0.994s, 1029.80/s  (1.009s, 1015.01/s)  LR: 1.328e-05  Data: 0.011 (0.013)
Train: 289 [1000/1251 ( 80%)]  Loss: 3.184 (3.10)  Time: 0.994s, 1030.56/s  (1.008s, 1015.42/s)  LR: 1.328e-05  Data: 0.011 (0.013)
Train: 289 [1050/1251 ( 84%)]  Loss: 3.082 (3.10)  Time: 0.991s, 1033.38/s  (1.008s, 1015.77/s)  LR: 1.328e-05  Data: 0.010 (0.013)
Train: 289 [1100/1251 ( 88%)]  Loss: 3.082 (3.10)  Time: 1.032s,  992.00/s  (1.008s, 1015.95/s)  LR: 1.328e-05  Data: 0.011 (0.012)
Train: 289 [1150/1251 ( 92%)]  Loss: 3.064 (3.10)  Time: 1.008s, 1016.21/s  (1.008s, 1016.01/s)  LR: 1.328e-05  Data: 0.011 (0.012)
Train: 289 [1200/1251 ( 96%)]  Loss: 2.959 (3.09)  Time: 0.997s, 1027.50/s  (1.007s, 1016.46/s)  LR: 1.328e-05  Data: 0.012 (0.012)
Train: 289 [1250/1251 (100%)]  Loss: 3.013 (3.09)  Time: 1.060s,  966.05/s  (1.007s, 1016.49/s)  LR: 1.328e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.639 (1.639)  Loss:  0.6333 (0.6333)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.245 (0.573)  Loss:  0.7346 (1.0593)  Acc@1: 87.0283 (80.3760)  Acc@5: 98.1132 (95.2400)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-289.pth.tar', 80.37600005371094)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-285.pth.tar', 80.344)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-287.pth.tar', 80.32400000244141)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-280.pth.tar', 80.29600005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-288.pth.tar', 80.28200002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-282.pth.tar', 80.2720000024414)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-276.pth.tar', 80.26400002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-281.pth.tar', 80.24400002929687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-283.pth.tar', 80.23399997558593)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-286.pth.tar', 80.23399987304687)

Train: 290 [   0/1251 (  0%)]  Loss: 3.011 (3.01)  Time: 2.438s,  420.01/s  (2.438s,  420.01/s)  LR: 1.271e-05  Data: 1.460 (1.460)
Train: 290 [  50/1251 (  4%)]  Loss: 2.796 (2.90)  Time: 1.008s, 1016.28/s  (1.032s,  992.34/s)  LR: 1.271e-05  Data: 0.010 (0.039)
Train: 290 [ 100/1251 (  8%)]  Loss: 3.147 (2.98)  Time: 0.998s, 1026.55/s  (1.018s, 1005.55/s)  LR: 1.271e-05  Data: 0.011 (0.025)
Train: 290 [ 150/1251 ( 12%)]  Loss: 2.804 (2.94)  Time: 0.999s, 1024.52/s  (1.012s, 1012.29/s)  LR: 1.271e-05  Data: 0.010 (0.021)
Train: 290 [ 200/1251 ( 16%)]  Loss: 3.132 (2.98)  Time: 0.995s, 1029.20/s  (1.008s, 1015.64/s)  LR: 1.271e-05  Data: 0.012 (0.018)
Train: 290 [ 250/1251 ( 20%)]  Loss: 2.753 (2.94)  Time: 0.997s, 1027.44/s  (1.007s, 1017.03/s)  LR: 1.271e-05  Data: 0.012 (0.017)
Train: 290 [ 300/1251 ( 24%)]  Loss: 3.109 (2.96)  Time: 0.994s, 1030.43/s  (1.006s, 1018.30/s)  LR: 1.271e-05  Data: 0.011 (0.016)
Train: 290 [ 350/1251 ( 28%)]  Loss: 3.154 (2.99)  Time: 0.996s, 1027.82/s  (1.004s, 1019.60/s)  LR: 1.271e-05  Data: 0.011 (0.015)
Train: 290 [ 400/1251 ( 32%)]  Loss: 3.036 (2.99)  Time: 0.995s, 1029.45/s  (1.003s, 1020.43/s)  LR: 1.271e-05  Data: 0.012 (0.015)
Train: 290 [ 450/1251 ( 36%)]  Loss: 2.990 (2.99)  Time: 0.996s, 1028.47/s  (1.003s, 1020.70/s)  LR: 1.271e-05  Data: 0.011 (0.014)
Train: 290 [ 500/1251 ( 40%)]  Loss: 3.138 (3.01)  Time: 0.995s, 1029.44/s  (1.003s, 1020.86/s)  LR: 1.271e-05  Data: 0.010 (0.014)
Train: 290 [ 550/1251 ( 44%)]  Loss: 2.947 (3.00)  Time: 0.999s, 1024.89/s  (1.003s, 1020.96/s)  LR: 1.271e-05  Data: 0.011 (0.014)
Train: 290 [ 600/1251 ( 48%)]  Loss: 3.093 (3.01)  Time: 1.058s,  967.56/s  (1.003s, 1020.82/s)  LR: 1.271e-05  Data: 0.011 (0.013)
Train: 290 [ 650/1251 ( 52%)]  Loss: 2.735 (2.99)  Time: 0.996s, 1028.09/s  (1.005s, 1018.98/s)  LR: 1.271e-05  Data: 0.011 (0.013)
Train: 290 [ 700/1251 ( 56%)]  Loss: 2.858 (2.98)  Time: 0.993s, 1031.38/s  (1.005s, 1019.35/s)  LR: 1.271e-05  Data: 0.010 (0.013)
Train: 290 [ 750/1251 ( 60%)]  Loss: 2.852 (2.97)  Time: 1.002s, 1021.88/s  (1.004s, 1019.54/s)  LR: 1.271e-05  Data: 0.011 (0.013)
Train: 290 [ 800/1251 ( 64%)]  Loss: 3.172 (2.98)  Time: 1.055s,  970.72/s  (1.006s, 1017.87/s)  LR: 1.271e-05  Data: 0.011 (0.013)
Train: 290 [ 850/1251 ( 68%)]  Loss: 2.761 (2.97)  Time: 0.994s, 1030.65/s  (1.006s, 1017.46/s)  LR: 1.271e-05  Data: 0.011 (0.013)
Train: 290 [ 900/1251 ( 72%)]  Loss: 3.345 (2.99)  Time: 0.992s, 1032.02/s  (1.007s, 1016.71/s)  LR: 1.271e-05  Data: 0.010 (0.013)
Train: 290 [ 950/1251 ( 76%)]  Loss: 3.130 (3.00)  Time: 1.004s, 1019.52/s  (1.007s, 1016.97/s)  LR: 1.271e-05  Data: 0.011 (0.013)
Train: 290 [1000/1251 ( 80%)]  Loss: 3.021 (3.00)  Time: 0.998s, 1026.47/s  (1.007s, 1017.20/s)  LR: 1.271e-05  Data: 0.012 (0.013)
Train: 290 [1050/1251 ( 84%)]  Loss: 3.243 (3.01)  Time: 0.996s, 1028.24/s  (1.006s, 1017.45/s)  LR: 1.271e-05  Data: 0.011 (0.013)
Train: 290 [1100/1251 ( 88%)]  Loss: 2.888 (3.01)  Time: 0.997s, 1026.87/s  (1.006s, 1017.63/s)  LR: 1.271e-05  Data: 0.011 (0.012)
Train: 290 [1150/1251 ( 92%)]  Loss: 3.000 (3.00)  Time: 0.996s, 1028.16/s  (1.006s, 1017.79/s)  LR: 1.271e-05  Data: 0.012 (0.012)
Train: 290 [1200/1251 ( 96%)]  Loss: 2.989 (3.00)  Time: 0.995s, 1029.22/s  (1.006s, 1017.67/s)  LR: 1.271e-05  Data: 0.011 (0.012)
Train: 290 [1250/1251 (100%)]  Loss: 3.030 (3.01)  Time: 0.985s, 1039.30/s  (1.006s, 1017.89/s)  LR: 1.271e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.649 (1.649)  Loss:  0.5955 (0.5955)  Acc@1: 91.8945 (91.8945)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.245 (0.572)  Loss:  0.7202 (1.0379)  Acc@1: 87.3821 (80.3660)  Acc@5: 97.9953 (95.2160)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-289.pth.tar', 80.37600005371094)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-290.pth.tar', 80.36599994873046)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-285.pth.tar', 80.344)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-287.pth.tar', 80.32400000244141)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-280.pth.tar', 80.29600005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-288.pth.tar', 80.28200002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-282.pth.tar', 80.2720000024414)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-276.pth.tar', 80.26400002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-281.pth.tar', 80.24400002929687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-283.pth.tar', 80.23399997558593)

Train: 291 [   0/1251 (  0%)]  Loss: 2.833 (2.83)  Time: 4.466s,  229.30/s  (4.466s,  229.30/s)  LR: 1.220e-05  Data: 3.126 (3.126)
Train: 291 [  50/1251 (  4%)]  Loss: 3.260 (3.05)  Time: 1.006s, 1018.27/s  (1.069s,  957.50/s)  LR: 1.220e-05  Data: 0.011 (0.072)
Train: 291 [ 100/1251 (  8%)]  Loss: 3.025 (3.04)  Time: 0.994s, 1030.66/s  (1.044s,  980.82/s)  LR: 1.220e-05  Data: 0.011 (0.042)
Train: 291 [ 150/1251 ( 12%)]  Loss: 3.063 (3.05)  Time: 0.995s, 1029.49/s  (1.029s,  994.81/s)  LR: 1.220e-05  Data: 0.011 (0.032)
Train: 291 [ 200/1251 ( 16%)]  Loss: 3.183 (3.07)  Time: 0.992s, 1031.75/s  (1.023s, 1001.46/s)  LR: 1.220e-05  Data: 0.011 (0.027)
Train: 291 [ 250/1251 ( 20%)]  Loss: 2.899 (3.04)  Time: 0.994s, 1030.68/s  (1.018s, 1005.53/s)  LR: 1.220e-05  Data: 0.010 (0.024)
Train: 291 [ 300/1251 ( 24%)]  Loss: 3.045 (3.04)  Time: 0.996s, 1028.02/s  (1.015s, 1008.41/s)  LR: 1.220e-05  Data: 0.011 (0.022)
Train: 291 [ 350/1251 ( 28%)]  Loss: 2.754 (3.01)  Time: 0.994s, 1030.30/s  (1.014s, 1010.22/s)  LR: 1.220e-05  Data: 0.011 (0.020)
Train: 291 [ 400/1251 ( 32%)]  Loss: 3.273 (3.04)  Time: 1.005s, 1019.19/s  (1.013s, 1010.86/s)  LR: 1.220e-05  Data: 0.014 (0.019)
Train: 291 [ 450/1251 ( 36%)]  Loss: 3.366 (3.07)  Time: 0.996s, 1028.50/s  (1.013s, 1011.21/s)  LR: 1.220e-05  Data: 0.011 (0.018)
Train: 291 [ 500/1251 ( 40%)]  Loss: 3.018 (3.07)  Time: 1.031s,  993.29/s  (1.012s, 1012.21/s)  LR: 1.220e-05  Data: 0.011 (0.017)
Train: 291 [ 550/1251 ( 44%)]  Loss: 2.753 (3.04)  Time: 1.002s, 1021.63/s  (1.011s, 1013.14/s)  LR: 1.220e-05  Data: 0.010 (0.017)
Train: 291 [ 600/1251 ( 48%)]  Loss: 3.155 (3.05)  Time: 1.006s, 1018.14/s  (1.010s, 1013.94/s)  LR: 1.220e-05  Data: 0.012 (0.016)
Train: 291 [ 650/1251 ( 52%)]  Loss: 2.828 (3.03)  Time: 0.999s, 1024.98/s  (1.009s, 1014.74/s)  LR: 1.220e-05  Data: 0.011 (0.016)
Train: 291 [ 700/1251 ( 56%)]  Loss: 3.169 (3.04)  Time: 0.996s, 1028.04/s  (1.008s, 1015.41/s)  LR: 1.220e-05  Data: 0.011 (0.016)
Train: 291 [ 750/1251 ( 60%)]  Loss: 3.075 (3.04)  Time: 1.033s,  991.74/s  (1.008s, 1015.64/s)  LR: 1.220e-05  Data: 0.010 (0.015)
Train: 291 [ 800/1251 ( 64%)]  Loss: 2.906 (3.04)  Time: 1.058s,  968.02/s  (1.011s, 1013.11/s)  LR: 1.220e-05  Data: 0.011 (0.015)
Train: 291 [ 850/1251 ( 68%)]  Loss: 3.390 (3.06)  Time: 0.995s, 1029.29/s  (1.011s, 1013.19/s)  LR: 1.220e-05  Data: 0.011 (0.015)
Train: 291 [ 900/1251 ( 72%)]  Loss: 3.258 (3.07)  Time: 1.016s, 1007.97/s  (1.010s, 1013.83/s)  LR: 1.220e-05  Data: 0.011 (0.015)
Train: 291 [ 950/1251 ( 76%)]  Loss: 3.207 (3.07)  Time: 1.029s,  995.51/s  (1.011s, 1013.13/s)  LR: 1.220e-05  Data: 0.012 (0.014)
Train: 291 [1000/1251 ( 80%)]  Loss: 2.954 (3.07)  Time: 1.019s, 1005.28/s  (1.010s, 1013.56/s)  LR: 1.220e-05  Data: 0.011 (0.014)
Train: 291 [1050/1251 ( 84%)]  Loss: 2.933 (3.06)  Time: 1.002s, 1021.60/s  (1.010s, 1013.95/s)  LR: 1.220e-05  Data: 0.012 (0.014)
Train: 291 [1100/1251 ( 88%)]  Loss: 3.315 (3.07)  Time: 0.995s, 1029.59/s  (1.009s, 1014.46/s)  LR: 1.220e-05  Data: 0.011 (0.014)
Train: 291 [1150/1251 ( 92%)]  Loss: 3.153 (3.08)  Time: 1.035s,  989.61/s  (1.009s, 1014.84/s)  LR: 1.220e-05  Data: 0.011 (0.014)
Train: 291 [1200/1251 ( 96%)]  Loss: 3.160 (3.08)  Time: 1.011s, 1012.59/s  (1.009s, 1015.27/s)  LR: 1.220e-05  Data: 0.011 (0.014)
Train: 291 [1250/1251 (100%)]  Loss: 2.820 (3.07)  Time: 0.986s, 1039.02/s  (1.008s, 1015.49/s)  LR: 1.220e-05  Data: 0.000 (0.014)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.628 (1.628)  Loss:  0.6079 (0.6079)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.245 (0.568)  Loss:  0.7201 (1.0472)  Acc@1: 86.9104 (80.3840)  Acc@5: 97.8774 (95.1840)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-291.pth.tar', 80.38399987304687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-289.pth.tar', 80.37600005371094)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-290.pth.tar', 80.36599994873046)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-285.pth.tar', 80.344)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-287.pth.tar', 80.32400000244141)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-280.pth.tar', 80.29600005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-288.pth.tar', 80.28200002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-282.pth.tar', 80.2720000024414)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-276.pth.tar', 80.26400002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-281.pth.tar', 80.24400002929687)

Train: 292 [   0/1251 (  0%)]  Loss: 2.764 (2.76)  Time: 3.007s,  340.58/s  (3.007s,  340.58/s)  LR: 1.174e-05  Data: 2.042 (2.042)
Train: 292 [  50/1251 (  4%)]  Loss: 3.331 (3.05)  Time: 0.997s, 1026.90/s  (1.041s,  983.66/s)  LR: 1.174e-05  Data: 0.012 (0.052)
Train: 292 [ 100/1251 (  8%)]  Loss: 3.117 (3.07)  Time: 0.996s, 1027.70/s  (1.020s, 1004.00/s)  LR: 1.174e-05  Data: 0.012 (0.032)
Train: 292 [ 150/1251 ( 12%)]  Loss: 2.870 (3.02)  Time: 0.995s, 1029.39/s  (1.021s, 1003.18/s)  LR: 1.174e-05  Data: 0.011 (0.025)
Train: 292 [ 200/1251 ( 16%)]  Loss: 2.856 (2.99)  Time: 1.070s,  957.20/s  (1.019s, 1004.43/s)  LR: 1.174e-05  Data: 0.011 (0.022)
Train: 292 [ 250/1251 ( 20%)]  Loss: 3.315 (3.04)  Time: 0.998s, 1025.83/s  (1.018s, 1005.93/s)  LR: 1.174e-05  Data: 0.011 (0.020)
Train: 292 [ 300/1251 ( 24%)]  Loss: 3.018 (3.04)  Time: 0.996s, 1027.96/s  (1.015s, 1008.89/s)  LR: 1.174e-05  Data: 0.012 (0.018)
Train: 292 [ 350/1251 ( 28%)]  Loss: 3.021 (3.04)  Time: 1.009s, 1014.78/s  (1.013s, 1010.95/s)  LR: 1.174e-05  Data: 0.011 (0.017)
Train: 292 [ 400/1251 ( 32%)]  Loss: 2.996 (3.03)  Time: 0.997s, 1027.01/s  (1.012s, 1012.32/s)  LR: 1.174e-05  Data: 0.013 (0.017)
Train: 292 [ 450/1251 ( 36%)]  Loss: 2.963 (3.03)  Time: 1.001s, 1022.59/s  (1.010s, 1013.75/s)  LR: 1.174e-05  Data: 0.012 (0.016)
Train: 292 [ 500/1251 ( 40%)]  Loss: 3.068 (3.03)  Time: 0.993s, 1030.83/s  (1.010s, 1013.62/s)  LR: 1.174e-05  Data: 0.010 (0.016)
Train: 292 [ 550/1251 ( 44%)]  Loss: 3.100 (3.03)  Time: 0.994s, 1029.72/s  (1.009s, 1014.49/s)  LR: 1.174e-05  Data: 0.011 (0.015)
Train: 292 [ 600/1251 ( 48%)]  Loss: 3.175 (3.05)  Time: 0.999s, 1025.38/s  (1.009s, 1015.22/s)  LR: 1.174e-05  Data: 0.011 (0.015)
Train: 292 [ 650/1251 ( 52%)]  Loss: 3.129 (3.05)  Time: 1.021s, 1002.76/s  (1.008s, 1015.59/s)  LR: 1.174e-05  Data: 0.011 (0.014)
Train: 292 [ 700/1251 ( 56%)]  Loss: 2.898 (3.04)  Time: 0.995s, 1028.70/s  (1.008s, 1015.95/s)  LR: 1.174e-05  Data: 0.011 (0.014)
Train: 292 [ 750/1251 ( 60%)]  Loss: 3.135 (3.05)  Time: 0.994s, 1030.18/s  (1.007s, 1016.60/s)  LR: 1.174e-05  Data: 0.011 (0.014)
Train: 292 [ 800/1251 ( 64%)]  Loss: 3.192 (3.06)  Time: 0.993s, 1031.35/s  (1.007s, 1016.76/s)  LR: 1.174e-05  Data: 0.011 (0.014)
Train: 292 [ 850/1251 ( 68%)]  Loss: 3.110 (3.06)  Time: 1.000s, 1023.56/s  (1.007s, 1017.16/s)  LR: 1.174e-05  Data: 0.010 (0.014)
Train: 292 [ 900/1251 ( 72%)]  Loss: 3.092 (3.06)  Time: 0.995s, 1028.76/s  (1.006s, 1017.49/s)  LR: 1.174e-05  Data: 0.014 (0.014)
Train: 292 [ 950/1251 ( 76%)]  Loss: 2.786 (3.05)  Time: 0.996s, 1027.93/s  (1.006s, 1017.74/s)  LR: 1.174e-05  Data: 0.011 (0.013)
Train: 292 [1000/1251 ( 80%)]  Loss: 3.120 (3.05)  Time: 0.997s, 1027.57/s  (1.006s, 1018.08/s)  LR: 1.174e-05  Data: 0.011 (0.013)
Train: 292 [1050/1251 ( 84%)]  Loss: 3.011 (3.05)  Time: 1.034s,  990.27/s  (1.006s, 1018.26/s)  LR: 1.174e-05  Data: 0.011 (0.013)
Train: 292 [1100/1251 ( 88%)]  Loss: 2.987 (3.05)  Time: 0.996s, 1028.60/s  (1.005s, 1018.47/s)  LR: 1.174e-05  Data: 0.011 (0.013)
Train: 292 [1150/1251 ( 92%)]  Loss: 3.261 (3.05)  Time: 0.996s, 1028.07/s  (1.005s, 1018.58/s)  LR: 1.174e-05  Data: 0.012 (0.013)
Train: 292 [1200/1251 ( 96%)]  Loss: 3.098 (3.06)  Time: 1.016s, 1008.30/s  (1.005s, 1018.51/s)  LR: 1.174e-05  Data: 0.011 (0.013)
Train: 292 [1250/1251 (100%)]  Loss: 3.310 (3.07)  Time: 0.983s, 1042.04/s  (1.005s, 1018.70/s)  LR: 1.174e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.672 (1.672)  Loss:  0.6590 (0.6590)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.245 (0.569)  Loss:  0.7796 (1.0911)  Acc@1: 87.1462 (80.3500)  Acc@5: 97.7594 (95.1600)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-291.pth.tar', 80.38399987304687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-289.pth.tar', 80.37600005371094)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-290.pth.tar', 80.36599994873046)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-292.pth.tar', 80.35000010498047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-285.pth.tar', 80.344)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-287.pth.tar', 80.32400000244141)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-280.pth.tar', 80.29600005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-288.pth.tar', 80.28200002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-282.pth.tar', 80.2720000024414)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-276.pth.tar', 80.26400002929688)

Train: 293 [   0/1251 (  0%)]  Loss: 3.232 (3.23)  Time: 2.475s,  413.71/s  (2.475s,  413.71/s)  LR: 1.133e-05  Data: 1.510 (1.510)
Train: 293 [  50/1251 (  4%)]  Loss: 2.928 (3.08)  Time: 0.998s, 1026.55/s  (1.028s,  996.31/s)  LR: 1.133e-05  Data: 0.011 (0.040)
Train: 293 [ 100/1251 (  8%)]  Loss: 2.944 (3.03)  Time: 0.995s, 1028.85/s  (1.013s, 1010.67/s)  LR: 1.133e-05  Data: 0.012 (0.026)
Train: 293 [ 150/1251 ( 12%)]  Loss: 3.149 (3.06)  Time: 1.011s, 1013.04/s  (1.010s, 1013.69/s)  LR: 1.133e-05  Data: 0.011 (0.021)
Train: 293 [ 200/1251 ( 16%)]  Loss: 2.857 (3.02)  Time: 0.995s, 1028.91/s  (1.007s, 1016.47/s)  LR: 1.133e-05  Data: 0.011 (0.019)
Train: 293 [ 250/1251 ( 20%)]  Loss: 2.838 (2.99)  Time: 0.993s, 1031.55/s  (1.006s, 1017.80/s)  LR: 1.133e-05  Data: 0.010 (0.017)
Train: 293 [ 300/1251 ( 24%)]  Loss: 2.855 (2.97)  Time: 1.030s,  994.46/s  (1.005s, 1019.10/s)  LR: 1.133e-05  Data: 0.011 (0.016)
Train: 293 [ 350/1251 ( 28%)]  Loss: 2.564 (2.92)  Time: 0.999s, 1025.13/s  (1.004s, 1019.86/s)  LR: 1.133e-05  Data: 0.011 (0.015)
Train: 293 [ 400/1251 ( 32%)]  Loss: 2.945 (2.92)  Time: 1.000s, 1023.82/s  (1.003s, 1020.54/s)  LR: 1.133e-05  Data: 0.011 (0.015)
Train: 293 [ 450/1251 ( 36%)]  Loss: 3.137 (2.94)  Time: 0.995s, 1029.30/s  (1.003s, 1020.69/s)  LR: 1.133e-05  Data: 0.010 (0.014)
Train: 293 [ 500/1251 ( 40%)]  Loss: 3.038 (2.95)  Time: 1.071s,  956.03/s  (1.004s, 1019.86/s)  LR: 1.133e-05  Data: 0.012 (0.014)
Train: 293 [ 550/1251 ( 44%)]  Loss: 3.052 (2.96)  Time: 1.009s, 1015.24/s  (1.008s, 1015.80/s)  LR: 1.133e-05  Data: 0.015 (0.014)
Train: 293 [ 600/1251 ( 48%)]  Loss: 2.973 (2.96)  Time: 1.001s, 1023.18/s  (1.009s, 1014.84/s)  LR: 1.133e-05  Data: 0.011 (0.014)
Train: 293 [ 650/1251 ( 52%)]  Loss: 2.819 (2.95)  Time: 1.033s,  991.06/s  (1.008s, 1015.40/s)  LR: 1.133e-05  Data: 0.011 (0.013)
Train: 293 [ 700/1251 ( 56%)]  Loss: 3.132 (2.96)  Time: 1.002s, 1022.06/s  (1.008s, 1015.91/s)  LR: 1.133e-05  Data: 0.012 (0.013)
Train: 293 [ 750/1251 ( 60%)]  Loss: 3.374 (2.99)  Time: 0.997s, 1027.31/s  (1.009s, 1015.07/s)  LR: 1.133e-05  Data: 0.010 (0.013)
Train: 293 [ 800/1251 ( 64%)]  Loss: 3.011 (2.99)  Time: 0.996s, 1027.86/s  (1.008s, 1015.46/s)  LR: 1.133e-05  Data: 0.011 (0.013)
Train: 293 [ 850/1251 ( 68%)]  Loss: 2.415 (2.96)  Time: 1.001s, 1023.15/s  (1.008s, 1015.96/s)  LR: 1.133e-05  Data: 0.010 (0.013)
Train: 293 [ 900/1251 ( 72%)]  Loss: 3.029 (2.96)  Time: 1.009s, 1014.50/s  (1.007s, 1016.43/s)  LR: 1.133e-05  Data: 0.011 (0.013)
Train: 293 [ 950/1251 ( 76%)]  Loss: 2.818 (2.96)  Time: 1.002s, 1022.19/s  (1.007s, 1016.61/s)  LR: 1.133e-05  Data: 0.012 (0.013)
Train: 293 [1000/1251 ( 80%)]  Loss: 3.227 (2.97)  Time: 0.996s, 1027.72/s  (1.007s, 1016.85/s)  LR: 1.133e-05  Data: 0.010 (0.013)
Train: 293 [1050/1251 ( 84%)]  Loss: 3.175 (2.98)  Time: 1.016s, 1008.10/s  (1.007s, 1017.14/s)  LR: 1.133e-05  Data: 0.011 (0.013)
Train: 293 [1100/1251 ( 88%)]  Loss: 3.260 (2.99)  Time: 1.007s, 1017.14/s  (1.006s, 1017.46/s)  LR: 1.133e-05  Data: 0.011 (0.012)
Train: 293 [1150/1251 ( 92%)]  Loss: 2.877 (2.99)  Time: 0.994s, 1029.83/s  (1.006s, 1017.85/s)  LR: 1.133e-05  Data: 0.011 (0.012)
Train: 293 [1200/1251 ( 96%)]  Loss: 2.925 (2.98)  Time: 1.007s, 1016.63/s  (1.006s, 1018.08/s)  LR: 1.133e-05  Data: 0.011 (0.012)
Train: 293 [1250/1251 (100%)]  Loss: 3.103 (2.99)  Time: 0.990s, 1034.13/s  (1.006s, 1018.40/s)  LR: 1.133e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.599 (1.599)  Loss:  0.6439 (0.6439)  Acc@1: 92.3828 (92.3828)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.245 (0.568)  Loss:  0.7496 (1.0816)  Acc@1: 87.0283 (80.4220)  Acc@5: 97.9953 (95.2520)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-293.pth.tar', 80.42199992431641)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-291.pth.tar', 80.38399987304687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-289.pth.tar', 80.37600005371094)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-290.pth.tar', 80.36599994873046)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-292.pth.tar', 80.35000010498047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-285.pth.tar', 80.344)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-287.pth.tar', 80.32400000244141)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-280.pth.tar', 80.29600005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-288.pth.tar', 80.28200002929688)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-282.pth.tar', 80.2720000024414)

Train: 294 [   0/1251 (  0%)]  Loss: 3.036 (3.04)  Time: 2.396s,  427.38/s  (2.396s,  427.38/s)  LR: 1.098e-05  Data: 1.430 (1.430)
Train: 294 [  50/1251 (  4%)]  Loss: 3.063 (3.05)  Time: 0.992s, 1032.48/s  (1.044s,  981.06/s)  LR: 1.098e-05  Data: 0.011 (0.041)
Train: 294 [ 100/1251 (  8%)]  Loss: 3.009 (3.04)  Time: 1.003s, 1021.12/s  (1.022s, 1001.57/s)  LR: 1.098e-05  Data: 0.013 (0.026)
Train: 294 [ 150/1251 ( 12%)]  Loss: 3.234 (3.09)  Time: 0.994s, 1030.53/s  (1.018s, 1006.16/s)  LR: 1.098e-05  Data: 0.011 (0.021)
Train: 294 [ 200/1251 ( 16%)]  Loss: 3.062 (3.08)  Time: 1.031s,  993.38/s  (1.015s, 1009.11/s)  LR: 1.098e-05  Data: 0.014 (0.019)
Train: 294 [ 250/1251 ( 20%)]  Loss: 2.792 (3.03)  Time: 1.005s, 1018.61/s  (1.016s, 1007.45/s)  LR: 1.098e-05  Data: 0.011 (0.017)
Train: 294 [ 300/1251 ( 24%)]  Loss: 3.291 (3.07)  Time: 0.996s, 1028.37/s  (1.013s, 1010.40/s)  LR: 1.098e-05  Data: 0.010 (0.016)
Train: 294 [ 350/1251 ( 28%)]  Loss: 3.075 (3.07)  Time: 0.998s, 1026.47/s  (1.012s, 1012.27/s)  LR: 1.098e-05  Data: 0.010 (0.015)
Train: 294 [ 400/1251 ( 32%)]  Loss: 3.131 (3.08)  Time: 0.997s, 1026.66/s  (1.010s, 1013.82/s)  LR: 1.098e-05  Data: 0.010 (0.015)
Train: 294 [ 450/1251 ( 36%)]  Loss: 3.155 (3.08)  Time: 0.998s, 1026.09/s  (1.009s, 1014.59/s)  LR: 1.098e-05  Data: 0.010 (0.014)
Train: 294 [ 500/1251 ( 40%)]  Loss: 3.117 (3.09)  Time: 1.046s,  979.17/s  (1.008s, 1015.67/s)  LR: 1.098e-05  Data: 0.011 (0.014)
Train: 294 [ 550/1251 ( 44%)]  Loss: 3.004 (3.08)  Time: 1.054s,  971.18/s  (1.008s, 1015.82/s)  LR: 1.098e-05  Data: 0.011 (0.014)
Train: 294 [ 600/1251 ( 48%)]  Loss: 2.788 (3.06)  Time: 0.996s, 1028.24/s  (1.007s, 1016.54/s)  LR: 1.098e-05  Data: 0.010 (0.014)
Train: 294 [ 650/1251 ( 52%)]  Loss: 3.078 (3.06)  Time: 1.000s, 1024.43/s  (1.007s, 1017.06/s)  LR: 1.098e-05  Data: 0.012 (0.013)
Train: 294 [ 700/1251 ( 56%)]  Loss: 2.501 (3.02)  Time: 0.995s, 1028.98/s  (1.007s, 1017.29/s)  LR: 1.098e-05  Data: 0.012 (0.013)
Train: 294 [ 750/1251 ( 60%)]  Loss: 2.854 (3.01)  Time: 0.993s, 1031.42/s  (1.006s, 1017.50/s)  LR: 1.098e-05  Data: 0.011 (0.013)
Train: 294 [ 800/1251 ( 64%)]  Loss: 3.156 (3.02)  Time: 0.990s, 1034.18/s  (1.006s, 1017.73/s)  LR: 1.098e-05  Data: 0.011 (0.013)
Train: 294 [ 850/1251 ( 68%)]  Loss: 3.148 (3.03)  Time: 0.996s, 1028.28/s  (1.006s, 1018.09/s)  LR: 1.098e-05  Data: 0.012 (0.013)
Train: 294 [ 900/1251 ( 72%)]  Loss: 3.334 (3.04)  Time: 1.027s,  996.70/s  (1.005s, 1018.47/s)  LR: 1.098e-05  Data: 0.011 (0.013)
Train: 294 [ 950/1251 ( 76%)]  Loss: 3.107 (3.05)  Time: 1.044s,  980.59/s  (1.006s, 1017.48/s)  LR: 1.098e-05  Data: 0.011 (0.013)
Train: 294 [1000/1251 ( 80%)]  Loss: 2.908 (3.04)  Time: 1.000s, 1023.95/s  (1.007s, 1017.18/s)  LR: 1.098e-05  Data: 0.011 (0.013)
Train: 294 [1050/1251 ( 84%)]  Loss: 2.960 (3.04)  Time: 0.997s, 1026.77/s  (1.006s, 1017.47/s)  LR: 1.098e-05  Data: 0.012 (0.013)
Train: 294 [1100/1251 ( 88%)]  Loss: 2.964 (3.03)  Time: 0.994s, 1030.38/s  (1.006s, 1017.63/s)  LR: 1.098e-05  Data: 0.010 (0.012)
Train: 294 [1150/1251 ( 92%)]  Loss: 2.979 (3.03)  Time: 0.997s, 1027.16/s  (1.006s, 1017.83/s)  LR: 1.098e-05  Data: 0.014 (0.012)
Train: 294 [1200/1251 ( 96%)]  Loss: 3.236 (3.04)  Time: 0.997s, 1027.09/s  (1.006s, 1018.02/s)  LR: 1.098e-05  Data: 0.014 (0.012)
Train: 294 [1250/1251 (100%)]  Loss: 2.950 (3.04)  Time: 1.040s,  984.72/s  (1.006s, 1018.30/s)  LR: 1.098e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.637 (1.637)  Loss:  0.6842 (0.6842)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.245 (0.577)  Loss:  0.7918 (1.1244)  Acc@1: 87.1462 (80.2860)  Acc@5: 97.8774 (95.1880)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-293.pth.tar', 80.42199992431641)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-291.pth.tar', 80.38399987304687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-289.pth.tar', 80.37600005371094)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-290.pth.tar', 80.36599994873046)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-292.pth.tar', 80.35000010498047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-285.pth.tar', 80.344)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-287.pth.tar', 80.32400000244141)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-280.pth.tar', 80.29600005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-294.pth.tar', 80.28599997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-288.pth.tar', 80.28200002929688)

Train: 295 [   0/1251 (  0%)]  Loss: 3.351 (3.35)  Time: 2.450s,  417.98/s  (2.450s,  417.98/s)  LR: 1.068e-05  Data: 1.494 (1.494)
Train: 295 [  50/1251 (  4%)]  Loss: 3.179 (3.27)  Time: 1.009s, 1014.40/s  (1.033s,  991.73/s)  LR: 1.068e-05  Data: 0.011 (0.040)
Train: 295 [ 100/1251 (  8%)]  Loss: 2.853 (3.13)  Time: 1.003s, 1021.10/s  (1.024s,  999.71/s)  LR: 1.068e-05  Data: 0.011 (0.026)
Train: 295 [ 150/1251 ( 12%)]  Loss: 3.304 (3.17)  Time: 0.994s, 1029.84/s  (1.017s, 1006.64/s)  LR: 1.068e-05  Data: 0.011 (0.021)
Train: 295 [ 200/1251 ( 16%)]  Loss: 2.780 (3.09)  Time: 1.002s, 1022.08/s  (1.014s, 1010.31/s)  LR: 1.068e-05  Data: 0.011 (0.019)
Train: 295 [ 250/1251 ( 20%)]  Loss: 2.915 (3.06)  Time: 0.998s, 1025.81/s  (1.011s, 1012.78/s)  LR: 1.068e-05  Data: 0.011 (0.017)
Train: 295 [ 300/1251 ( 24%)]  Loss: 3.164 (3.08)  Time: 1.008s, 1015.85/s  (1.012s, 1011.60/s)  LR: 1.068e-05  Data: 0.012 (0.016)
Train: 295 [ 350/1251 ( 28%)]  Loss: 3.037 (3.07)  Time: 0.993s, 1030.87/s  (1.013s, 1010.61/s)  LR: 1.068e-05  Data: 0.010 (0.015)
Train: 295 [ 400/1251 ( 32%)]  Loss: 2.867 (3.05)  Time: 1.003s, 1021.03/s  (1.011s, 1012.40/s)  LR: 1.068e-05  Data: 0.014 (0.015)
Train: 295 [ 450/1251 ( 36%)]  Loss: 3.205 (3.07)  Time: 0.995s, 1029.11/s  (1.010s, 1013.65/s)  LR: 1.068e-05  Data: 0.011 (0.014)
Train: 295 [ 500/1251 ( 40%)]  Loss: 3.163 (3.07)  Time: 1.035s,  988.90/s  (1.009s, 1014.80/s)  LR: 1.068e-05  Data: 0.010 (0.014)
Train: 295 [ 550/1251 ( 44%)]  Loss: 2.806 (3.05)  Time: 0.997s, 1027.11/s  (1.008s, 1015.41/s)  LR: 1.068e-05  Data: 0.012 (0.014)
Train: 295 [ 600/1251 ( 48%)]  Loss: 2.806 (3.03)  Time: 0.998s, 1026.34/s  (1.008s, 1016.29/s)  LR: 1.068e-05  Data: 0.011 (0.014)
Train: 295 [ 650/1251 ( 52%)]  Loss: 2.953 (3.03)  Time: 1.005s, 1018.92/s  (1.009s, 1014.98/s)  LR: 1.068e-05  Data: 0.010 (0.013)
Train: 295 [ 700/1251 ( 56%)]  Loss: 2.991 (3.02)  Time: 1.063s,  962.95/s  (1.009s, 1015.32/s)  LR: 1.068e-05  Data: 0.011 (0.013)
Train: 295 [ 750/1251 ( 60%)]  Loss: 3.324 (3.04)  Time: 0.996s, 1028.30/s  (1.008s, 1015.81/s)  LR: 1.068e-05  Data: 0.011 (0.013)
Train: 295 [ 800/1251 ( 64%)]  Loss: 3.212 (3.05)  Time: 0.995s, 1029.36/s  (1.008s, 1016.36/s)  LR: 1.068e-05  Data: 0.011 (0.013)
Train: 295 [ 850/1251 ( 68%)]  Loss: 2.982 (3.05)  Time: 1.056s,  969.98/s  (1.008s, 1015.41/s)  LR: 1.068e-05  Data: 0.010 (0.013)
Train: 295 [ 900/1251 ( 72%)]  Loss: 3.165 (3.06)  Time: 1.058s,  968.22/s  (1.008s, 1015.57/s)  LR: 1.068e-05  Data: 0.010 (0.013)
Train: 295 [ 950/1251 ( 76%)]  Loss: 3.285 (3.07)  Time: 0.998s, 1026.45/s  (1.010s, 1013.82/s)  LR: 1.068e-05  Data: 0.011 (0.013)
Train: 295 [1000/1251 ( 80%)]  Loss: 2.834 (3.06)  Time: 0.995s, 1029.42/s  (1.010s, 1014.36/s)  LR: 1.068e-05  Data: 0.011 (0.013)
Train: 295 [1050/1251 ( 84%)]  Loss: 3.474 (3.08)  Time: 0.993s, 1031.59/s  (1.009s, 1014.70/s)  LR: 1.068e-05  Data: 0.011 (0.012)
Train: 295 [1100/1251 ( 88%)]  Loss: 2.710 (3.06)  Time: 0.995s, 1028.63/s  (1.009s, 1015.03/s)  LR: 1.068e-05  Data: 0.012 (0.012)
Train: 295 [1150/1251 ( 92%)]  Loss: 3.186 (3.06)  Time: 1.065s,  961.22/s  (1.009s, 1014.90/s)  LR: 1.068e-05  Data: 0.014 (0.012)
Train: 295 [1200/1251 ( 96%)]  Loss: 2.857 (3.06)  Time: 0.999s, 1025.50/s  (1.010s, 1014.12/s)  LR: 1.068e-05  Data: 0.011 (0.012)
Train: 295 [1250/1251 (100%)]  Loss: 2.943 (3.05)  Time: 0.989s, 1035.17/s  (1.009s, 1014.41/s)  LR: 1.068e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.653 (1.653)  Loss:  0.5897 (0.5897)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.245 (0.576)  Loss:  0.7186 (1.0308)  Acc@1: 87.3821 (80.4920)  Acc@5: 97.5236 (95.2060)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-295.pth.tar', 80.49199994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-293.pth.tar', 80.42199992431641)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-291.pth.tar', 80.38399987304687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-289.pth.tar', 80.37600005371094)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-290.pth.tar', 80.36599994873046)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-292.pth.tar', 80.35000010498047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-285.pth.tar', 80.344)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-287.pth.tar', 80.32400000244141)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-280.pth.tar', 80.29600005126953)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-294.pth.tar', 80.28599997558594)

Train: 296 [   0/1251 (  0%)]  Loss: 3.080 (3.08)  Time: 2.700s,  379.29/s  (2.700s,  379.29/s)  LR: 1.043e-05  Data: 1.672 (1.672)
Train: 296 [  50/1251 (  4%)]  Loss: 2.868 (2.97)  Time: 1.049s,  976.37/s  (1.043s,  981.58/s)  LR: 1.043e-05  Data: 0.011 (0.045)
Train: 296 [ 100/1251 (  8%)]  Loss: 2.770 (2.91)  Time: 1.036s,  988.83/s  (1.025s,  998.54/s)  LR: 1.043e-05  Data: 0.012 (0.028)
Train: 296 [ 150/1251 ( 12%)]  Loss: 2.794 (2.88)  Time: 0.995s, 1028.90/s  (1.016s, 1007.55/s)  LR: 1.043e-05  Data: 0.010 (0.023)
Train: 296 [ 200/1251 ( 16%)]  Loss: 3.052 (2.91)  Time: 0.996s, 1027.80/s  (1.013s, 1010.87/s)  LR: 1.043e-05  Data: 0.011 (0.020)
Train: 296 [ 250/1251 ( 20%)]  Loss: 2.940 (2.92)  Time: 1.058s,  968.00/s  (1.011s, 1012.74/s)  LR: 1.043e-05  Data: 0.011 (0.018)
Train: 296 [ 300/1251 ( 24%)]  Loss: 3.158 (2.95)  Time: 1.028s,  996.44/s  (1.011s, 1013.30/s)  LR: 1.043e-05  Data: 0.011 (0.017)
Train: 296 [ 350/1251 ( 28%)]  Loss: 3.199 (2.98)  Time: 1.011s, 1013.35/s  (1.011s, 1013.32/s)  LR: 1.043e-05  Data: 0.011 (0.016)
Train: 296 [ 400/1251 ( 32%)]  Loss: 2.925 (2.98)  Time: 0.995s, 1028.66/s  (1.009s, 1014.71/s)  LR: 1.043e-05  Data: 0.012 (0.015)
Train: 296 [ 450/1251 ( 36%)]  Loss: 2.844 (2.96)  Time: 1.001s, 1023.40/s  (1.009s, 1015.33/s)  LR: 1.043e-05  Data: 0.011 (0.015)
Train: 296 [ 500/1251 ( 40%)]  Loss: 3.193 (2.98)  Time: 0.996s, 1027.83/s  (1.008s, 1016.19/s)  LR: 1.043e-05  Data: 0.012 (0.015)
Train: 296 [ 550/1251 ( 44%)]  Loss: 3.114 (2.99)  Time: 1.055s,  970.30/s  (1.007s, 1016.55/s)  LR: 1.043e-05  Data: 0.011 (0.014)
Train: 296 [ 600/1251 ( 48%)]  Loss: 3.004 (3.00)  Time: 0.994s, 1030.13/s  (1.008s, 1016.22/s)  LR: 1.043e-05  Data: 0.010 (0.014)
Train: 296 [ 650/1251 ( 52%)]  Loss: 2.705 (2.97)  Time: 1.005s, 1019.08/s  (1.007s, 1016.91/s)  LR: 1.043e-05  Data: 0.011 (0.014)
Train: 296 [ 700/1251 ( 56%)]  Loss: 3.098 (2.98)  Time: 0.993s, 1030.89/s  (1.007s, 1017.33/s)  LR: 1.043e-05  Data: 0.010 (0.014)
Train: 296 [ 750/1251 ( 60%)]  Loss: 3.003 (2.98)  Time: 1.001s, 1023.04/s  (1.006s, 1017.82/s)  LR: 1.043e-05  Data: 0.010 (0.013)
Train: 296 [ 800/1251 ( 64%)]  Loss: 2.799 (2.97)  Time: 0.999s, 1025.16/s  (1.006s, 1017.82/s)  LR: 1.043e-05  Data: 0.011 (0.013)
Train: 296 [ 850/1251 ( 68%)]  Loss: 3.013 (2.98)  Time: 0.998s, 1026.01/s  (1.006s, 1018.13/s)  LR: 1.043e-05  Data: 0.011 (0.013)
Train: 296 [ 900/1251 ( 72%)]  Loss: 3.137 (2.98)  Time: 1.026s,  998.40/s  (1.005s, 1018.43/s)  LR: 1.043e-05  Data: 0.011 (0.013)
Train: 296 [ 950/1251 ( 76%)]  Loss: 2.811 (2.98)  Time: 1.001s, 1022.82/s  (1.005s, 1018.76/s)  LR: 1.043e-05  Data: 0.011 (0.013)
Train: 296 [1000/1251 ( 80%)]  Loss: 3.258 (2.99)  Time: 0.995s, 1028.90/s  (1.005s, 1019.08/s)  LR: 1.043e-05  Data: 0.010 (0.013)
Train: 296 [1050/1251 ( 84%)]  Loss: 3.025 (2.99)  Time: 0.995s, 1029.19/s  (1.005s, 1018.66/s)  LR: 1.043e-05  Data: 0.011 (0.013)
Train: 296 [1100/1251 ( 88%)]  Loss: 3.191 (3.00)  Time: 0.995s, 1029.58/s  (1.005s, 1019.01/s)  LR: 1.043e-05  Data: 0.011 (0.013)
Train: 296 [1150/1251 ( 92%)]  Loss: 3.210 (3.01)  Time: 0.998s, 1026.27/s  (1.005s, 1019.18/s)  LR: 1.043e-05  Data: 0.011 (0.013)
Train: 296 [1200/1251 ( 96%)]  Loss: 3.014 (3.01)  Time: 1.036s,  988.26/s  (1.005s, 1018.43/s)  LR: 1.043e-05  Data: 0.011 (0.013)
Train: 296 [1250/1251 (100%)]  Loss: 3.243 (3.02)  Time: 0.986s, 1038.90/s  (1.005s, 1018.64/s)  LR: 1.043e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.629 (1.629)  Loss:  0.6257 (0.6257)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.245 (0.583)  Loss:  0.7345 (1.0607)  Acc@1: 87.3821 (80.3440)  Acc@5: 97.8774 (95.2040)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-295.pth.tar', 80.49199994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-293.pth.tar', 80.42199992431641)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-291.pth.tar', 80.38399987304687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-289.pth.tar', 80.37600005371094)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-290.pth.tar', 80.36599994873046)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-292.pth.tar', 80.35000010498047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-296.pth.tar', 80.344000078125)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-285.pth.tar', 80.344)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-287.pth.tar', 80.32400000244141)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-280.pth.tar', 80.29600005126953)

Train: 297 [   0/1251 (  0%)]  Loss: 3.057 (3.06)  Time: 2.547s,  402.06/s  (2.547s,  402.06/s)  LR: 1.024e-05  Data: 1.548 (1.548)
Train: 297 [  50/1251 (  4%)]  Loss: 3.320 (3.19)  Time: 0.995s, 1028.96/s  (1.048s,  976.64/s)  LR: 1.024e-05  Data: 0.011 (0.041)
Train: 297 [ 100/1251 (  8%)]  Loss: 2.730 (3.04)  Time: 0.997s, 1027.18/s  (1.023s, 1000.67/s)  LR: 1.024e-05  Data: 0.011 (0.027)
Train: 297 [ 150/1251 ( 12%)]  Loss: 3.449 (3.14)  Time: 0.995s, 1029.00/s  (1.016s, 1008.17/s)  LR: 1.024e-05  Data: 0.011 (0.022)
Train: 297 [ 200/1251 ( 16%)]  Loss: 3.017 (3.11)  Time: 0.997s, 1027.57/s  (1.012s, 1011.86/s)  LR: 1.024e-05  Data: 0.012 (0.019)
Train: 297 [ 250/1251 ( 20%)]  Loss: 2.794 (3.06)  Time: 0.999s, 1024.69/s  (1.010s, 1013.65/s)  LR: 1.024e-05  Data: 0.010 (0.018)
Train: 297 [ 300/1251 ( 24%)]  Loss: 3.381 (3.11)  Time: 1.061s,  964.71/s  (1.008s, 1015.38/s)  LR: 1.024e-05  Data: 0.011 (0.016)
Train: 297 [ 350/1251 ( 28%)]  Loss: 3.119 (3.11)  Time: 0.995s, 1029.25/s  (1.009s, 1014.69/s)  LR: 1.024e-05  Data: 0.011 (0.016)
Train: 297 [ 400/1251 ( 32%)]  Loss: 3.227 (3.12)  Time: 0.994s, 1030.12/s  (1.008s, 1015.65/s)  LR: 1.024e-05  Data: 0.011 (0.015)
Train: 297 [ 450/1251 ( 36%)]  Loss: 2.717 (3.08)  Time: 0.996s, 1028.56/s  (1.008s, 1016.32/s)  LR: 1.024e-05  Data: 0.012 (0.015)
Train: 297 [ 500/1251 ( 40%)]  Loss: 2.726 (3.05)  Time: 1.050s,  975.57/s  (1.009s, 1014.91/s)  LR: 1.024e-05  Data: 0.011 (0.014)
Train: 297 [ 550/1251 ( 44%)]  Loss: 3.064 (3.05)  Time: 1.019s, 1005.23/s  (1.009s, 1015.21/s)  LR: 1.024e-05  Data: 0.011 (0.014)
Train: 297 [ 600/1251 ( 48%)]  Loss: 3.030 (3.05)  Time: 1.009s, 1014.40/s  (1.008s, 1015.67/s)  LR: 1.024e-05  Data: 0.010 (0.014)
Train: 297 [ 650/1251 ( 52%)]  Loss: 3.153 (3.06)  Time: 0.998s, 1026.38/s  (1.008s, 1016.23/s)  LR: 1.024e-05  Data: 0.011 (0.014)
Train: 297 [ 700/1251 ( 56%)]  Loss: 3.294 (3.07)  Time: 0.999s, 1024.61/s  (1.007s, 1016.69/s)  LR: 1.024e-05  Data: 0.012 (0.013)
Train: 297 [ 750/1251 ( 60%)]  Loss: 3.082 (3.07)  Time: 0.996s, 1027.64/s  (1.007s, 1017.16/s)  LR: 1.024e-05  Data: 0.012 (0.013)
Train: 297 [ 800/1251 ( 64%)]  Loss: 2.916 (3.06)  Time: 1.067s,  959.94/s  (1.007s, 1017.10/s)  LR: 1.024e-05  Data: 0.010 (0.013)
Train: 297 [ 850/1251 ( 68%)]  Loss: 3.060 (3.06)  Time: 0.994s, 1030.45/s  (1.008s, 1016.16/s)  LR: 1.024e-05  Data: 0.011 (0.013)
Train: 297 [ 900/1251 ( 72%)]  Loss: 3.114 (3.07)  Time: 1.013s, 1011.30/s  (1.007s, 1016.69/s)  LR: 1.024e-05  Data: 0.011 (0.013)
Train: 297 [ 950/1251 ( 76%)]  Loss: 3.272 (3.08)  Time: 1.001s, 1023.33/s  (1.007s, 1017.06/s)  LR: 1.024e-05  Data: 0.011 (0.013)
Train: 297 [1000/1251 ( 80%)]  Loss: 2.962 (3.07)  Time: 0.999s, 1025.01/s  (1.006s, 1017.43/s)  LR: 1.024e-05  Data: 0.011 (0.013)
Train: 297 [1050/1251 ( 84%)]  Loss: 2.953 (3.07)  Time: 1.000s, 1023.92/s  (1.006s, 1017.83/s)  LR: 1.024e-05  Data: 0.011 (0.013)
Train: 297 [1100/1251 ( 88%)]  Loss: 3.189 (3.07)  Time: 0.998s, 1026.50/s  (1.006s, 1018.10/s)  LR: 1.024e-05  Data: 0.012 (0.013)
Train: 297 [1150/1251 ( 92%)]  Loss: 3.012 (3.07)  Time: 0.995s, 1028.98/s  (1.006s, 1018.10/s)  LR: 1.024e-05  Data: 0.011 (0.013)
Train: 297 [1200/1251 ( 96%)]  Loss: 3.148 (3.07)  Time: 1.006s, 1017.39/s  (1.006s, 1017.98/s)  LR: 1.024e-05  Data: 0.012 (0.012)
Train: 297 [1250/1251 (100%)]  Loss: 2.512 (3.05)  Time: 0.989s, 1035.20/s  (1.006s, 1018.21/s)  LR: 1.024e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.713 (1.713)  Loss:  0.5908 (0.5908)  Acc@1: 92.3828 (92.3828)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.245 (0.570)  Loss:  0.6979 (1.0297)  Acc@1: 87.3821 (80.4400)  Acc@5: 97.9953 (95.2960)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-295.pth.tar', 80.49199994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-297.pth.tar', 80.43999994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-293.pth.tar', 80.42199992431641)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-291.pth.tar', 80.38399987304687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-289.pth.tar', 80.37600005371094)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-290.pth.tar', 80.36599994873046)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-292.pth.tar', 80.35000010498047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-296.pth.tar', 80.344000078125)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-285.pth.tar', 80.344)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-287.pth.tar', 80.32400000244141)

Train: 298 [   0/1251 (  0%)]  Loss: 3.318 (3.32)  Time: 2.486s,  411.87/s  (2.486s,  411.87/s)  LR: 1.011e-05  Data: 1.521 (1.521)
Train: 298 [  50/1251 (  4%)]  Loss: 2.902 (3.11)  Time: 0.989s, 1035.50/s  (1.033s,  991.48/s)  LR: 1.011e-05  Data: 0.010 (0.041)
Train: 298 [ 100/1251 (  8%)]  Loss: 3.288 (3.17)  Time: 0.993s, 1031.02/s  (1.017s, 1007.25/s)  LR: 1.011e-05  Data: 0.011 (0.026)
Train: 298 [ 150/1251 ( 12%)]  Loss: 3.066 (3.14)  Time: 0.997s, 1027.29/s  (1.012s, 1012.28/s)  LR: 1.011e-05  Data: 0.011 (0.021)
Train: 298 [ 200/1251 ( 16%)]  Loss: 2.767 (3.07)  Time: 1.003s, 1020.97/s  (1.011s, 1013.35/s)  LR: 1.011e-05  Data: 0.011 (0.019)
Train: 298 [ 250/1251 ( 20%)]  Loss: 3.030 (3.06)  Time: 0.995s, 1029.59/s  (1.009s, 1015.26/s)  LR: 1.011e-05  Data: 0.011 (0.017)
Train: 298 [ 300/1251 ( 24%)]  Loss: 3.127 (3.07)  Time: 0.996s, 1027.64/s  (1.008s, 1016.03/s)  LR: 1.011e-05  Data: 0.010 (0.016)
Train: 298 [ 350/1251 ( 28%)]  Loss: 3.236 (3.09)  Time: 0.996s, 1028.28/s  (1.007s, 1017.34/s)  LR: 1.011e-05  Data: 0.012 (0.015)
Train: 298 [ 400/1251 ( 32%)]  Loss: 2.842 (3.06)  Time: 0.994s, 1030.60/s  (1.006s, 1017.76/s)  LR: 1.011e-05  Data: 0.011 (0.015)
Train: 298 [ 450/1251 ( 36%)]  Loss: 2.899 (3.05)  Time: 0.998s, 1025.77/s  (1.006s, 1018.29/s)  LR: 1.011e-05  Data: 0.011 (0.015)
Train: 298 [ 500/1251 ( 40%)]  Loss: 3.192 (3.06)  Time: 0.998s, 1026.24/s  (1.005s, 1018.56/s)  LR: 1.011e-05  Data: 0.011 (0.014)
Train: 298 [ 550/1251 ( 44%)]  Loss: 2.697 (3.03)  Time: 0.996s, 1028.24/s  (1.005s, 1019.10/s)  LR: 1.011e-05  Data: 0.011 (0.014)
Train: 298 [ 600/1251 ( 48%)]  Loss: 3.325 (3.05)  Time: 0.996s, 1028.54/s  (1.005s, 1019.35/s)  LR: 1.011e-05  Data: 0.011 (0.014)
Train: 298 [ 650/1251 ( 52%)]  Loss: 2.845 (3.04)  Time: 1.006s, 1018.13/s  (1.004s, 1019.70/s)  LR: 1.011e-05  Data: 0.012 (0.013)
Train: 298 [ 700/1251 ( 56%)]  Loss: 3.108 (3.04)  Time: 1.004s, 1019.71/s  (1.004s, 1019.54/s)  LR: 1.011e-05  Data: 0.010 (0.013)
Train: 298 [ 750/1251 ( 60%)]  Loss: 2.855 (3.03)  Time: 0.994s, 1030.06/s  (1.005s, 1018.43/s)  LR: 1.011e-05  Data: 0.010 (0.013)
Train: 298 [ 800/1251 ( 64%)]  Loss: 3.359 (3.05)  Time: 0.993s, 1030.82/s  (1.005s, 1018.57/s)  LR: 1.011e-05  Data: 0.011 (0.013)
Train: 298 [ 850/1251 ( 68%)]  Loss: 3.116 (3.05)  Time: 1.002s, 1021.65/s  (1.006s, 1018.33/s)  LR: 1.011e-05  Data: 0.014 (0.013)
Train: 298 [ 900/1251 ( 72%)]  Loss: 3.241 (3.06)  Time: 0.997s, 1027.40/s  (1.006s, 1017.50/s)  LR: 1.011e-05  Data: 0.011 (0.013)
Train: 298 [ 950/1251 ( 76%)]  Loss: 2.914 (3.06)  Time: 1.007s, 1017.24/s  (1.006s, 1017.72/s)  LR: 1.011e-05  Data: 0.011 (0.013)
Train: 298 [1000/1251 ( 80%)]  Loss: 2.917 (3.05)  Time: 0.996s, 1028.46/s  (1.006s, 1017.63/s)  LR: 1.011e-05  Data: 0.011 (0.013)
Train: 298 [1050/1251 ( 84%)]  Loss: 2.704 (3.03)  Time: 0.990s, 1033.85/s  (1.006s, 1017.85/s)  LR: 1.011e-05  Data: 0.010 (0.013)
Train: 298 [1100/1251 ( 88%)]  Loss: 3.184 (3.04)  Time: 0.995s, 1028.65/s  (1.006s, 1018.29/s)  LR: 1.011e-05  Data: 0.011 (0.013)
Train: 298 [1150/1251 ( 92%)]  Loss: 3.327 (3.05)  Time: 0.998s, 1025.94/s  (1.005s, 1018.65/s)  LR: 1.011e-05  Data: 0.011 (0.012)
Train: 298 [1200/1251 ( 96%)]  Loss: 3.083 (3.05)  Time: 0.996s, 1027.65/s  (1.005s, 1018.91/s)  LR: 1.011e-05  Data: 0.012 (0.012)
Train: 298 [1250/1251 (100%)]  Loss: 2.885 (3.05)  Time: 0.985s, 1039.28/s  (1.005s, 1019.19/s)  LR: 1.011e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.621 (1.621)  Loss:  0.6587 (0.6587)  Acc@1: 92.3828 (92.3828)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.245 (0.564)  Loss:  0.7648 (1.0910)  Acc@1: 87.0283 (80.3320)  Acc@5: 98.1132 (95.2180)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-295.pth.tar', 80.49199994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-297.pth.tar', 80.43999994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-293.pth.tar', 80.42199992431641)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-291.pth.tar', 80.38399987304687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-289.pth.tar', 80.37600005371094)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-290.pth.tar', 80.36599994873046)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-292.pth.tar', 80.35000010498047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-296.pth.tar', 80.344000078125)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-285.pth.tar', 80.344)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-298.pth.tar', 80.3319999243164)

Train: 299 [   0/1251 (  0%)]  Loss: 3.249 (3.25)  Time: 2.506s,  408.66/s  (2.506s,  408.66/s)  LR: 1.003e-05  Data: 1.552 (1.552)
Train: 299 [  50/1251 (  4%)]  Loss: 3.300 (3.27)  Time: 1.012s, 1012.08/s  (1.031s,  993.08/s)  LR: 1.003e-05  Data: 0.011 (0.041)
Train: 299 [ 100/1251 (  8%)]  Loss: 3.015 (3.19)  Time: 0.997s, 1026.97/s  (1.019s, 1005.16/s)  LR: 1.003e-05  Data: 0.012 (0.026)
Train: 299 [ 150/1251 ( 12%)]  Loss: 3.461 (3.26)  Time: 0.992s, 1032.51/s  (1.014s, 1010.08/s)  LR: 1.003e-05  Data: 0.010 (0.021)
Train: 299 [ 200/1251 ( 16%)]  Loss: 2.530 (3.11)  Time: 0.995s, 1029.15/s  (1.012s, 1011.61/s)  LR: 1.003e-05  Data: 0.011 (0.019)
Train: 299 [ 250/1251 ( 20%)]  Loss: 3.070 (3.10)  Time: 0.992s, 1031.93/s  (1.010s, 1013.63/s)  LR: 1.003e-05  Data: 0.011 (0.017)
Train: 299 [ 300/1251 ( 24%)]  Loss: 3.367 (3.14)  Time: 1.013s, 1010.41/s  (1.009s, 1014.53/s)  LR: 1.003e-05  Data: 0.011 (0.016)
Train: 299 [ 350/1251 ( 28%)]  Loss: 2.901 (3.11)  Time: 1.000s, 1023.60/s  (1.009s, 1015.35/s)  LR: 1.003e-05  Data: 0.011 (0.015)
Train: 299 [ 400/1251 ( 32%)]  Loss: 3.227 (3.12)  Time: 0.997s, 1027.32/s  (1.008s, 1016.36/s)  LR: 1.003e-05  Data: 0.011 (0.015)
Train: 299 [ 450/1251 ( 36%)]  Loss: 3.099 (3.12)  Time: 0.997s, 1026.83/s  (1.007s, 1017.33/s)  LR: 1.003e-05  Data: 0.011 (0.015)
Train: 299 [ 500/1251 ( 40%)]  Loss: 3.086 (3.12)  Time: 0.997s, 1027.30/s  (1.006s, 1018.18/s)  LR: 1.003e-05  Data: 0.010 (0.014)
Train: 299 [ 550/1251 ( 44%)]  Loss: 2.993 (3.11)  Time: 1.003s, 1020.48/s  (1.005s, 1018.53/s)  LR: 1.003e-05  Data: 0.012 (0.014)
Train: 299 [ 600/1251 ( 48%)]  Loss: 3.247 (3.12)  Time: 0.993s, 1030.95/s  (1.005s, 1018.92/s)  LR: 1.003e-05  Data: 0.011 (0.014)
Train: 299 [ 650/1251 ( 52%)]  Loss: 3.336 (3.13)  Time: 1.002s, 1022.11/s  (1.005s, 1019.20/s)  LR: 1.003e-05  Data: 0.011 (0.013)
Train: 299 [ 700/1251 ( 56%)]  Loss: 3.171 (3.14)  Time: 1.004s, 1019.87/s  (1.005s, 1019.34/s)  LR: 1.003e-05  Data: 0.012 (0.013)
Train: 299 [ 750/1251 ( 60%)]  Loss: 3.236 (3.14)  Time: 1.030s,  994.03/s  (1.004s, 1019.59/s)  LR: 1.003e-05  Data: 0.010 (0.013)
Train: 299 [ 800/1251 ( 64%)]  Loss: 2.955 (3.13)  Time: 0.995s, 1029.15/s  (1.004s, 1020.01/s)  LR: 1.003e-05  Data: 0.011 (0.013)
Train: 299 [ 850/1251 ( 68%)]  Loss: 2.836 (3.12)  Time: 0.995s, 1029.58/s  (1.004s, 1020.20/s)  LR: 1.003e-05  Data: 0.011 (0.013)
Train: 299 [ 900/1251 ( 72%)]  Loss: 3.080 (3.11)  Time: 0.997s, 1026.93/s  (1.004s, 1020.19/s)  LR: 1.003e-05  Data: 0.011 (0.013)
Train: 299 [ 950/1251 ( 76%)]  Loss: 2.878 (3.10)  Time: 1.033s,  991.38/s  (1.004s, 1019.42/s)  LR: 1.003e-05  Data: 0.010 (0.013)
Train: 299 [1000/1251 ( 80%)]  Loss: 2.779 (3.09)  Time: 1.001s, 1022.78/s  (1.005s, 1019.39/s)  LR: 1.003e-05  Data: 0.011 (0.013)
Train: 299 [1050/1251 ( 84%)]  Loss: 3.082 (3.09)  Time: 0.996s, 1028.16/s  (1.005s, 1019.36/s)  LR: 1.003e-05  Data: 0.010 (0.013)
Train: 299 [1100/1251 ( 88%)]  Loss: 2.890 (3.08)  Time: 0.999s, 1025.35/s  (1.004s, 1019.48/s)  LR: 1.003e-05  Data: 0.011 (0.013)
Train: 299 [1150/1251 ( 92%)]  Loss: 3.305 (3.09)  Time: 1.003s, 1020.80/s  (1.004s, 1019.63/s)  LR: 1.003e-05  Data: 0.010 (0.012)
Train: 299 [1200/1251 ( 96%)]  Loss: 2.982 (3.08)  Time: 0.994s, 1029.69/s  (1.004s, 1019.99/s)  LR: 1.003e-05  Data: 0.011 (0.012)
Train: 299 [1250/1251 (100%)]  Loss: 3.303 (3.09)  Time: 0.982s, 1042.44/s  (1.004s, 1020.06/s)  LR: 1.003e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.697 (1.697)  Loss:  0.6501 (0.6501)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.245 (0.572)  Loss:  0.7572 (1.0821)  Acc@1: 87.0283 (80.3540)  Acc@5: 97.8774 (95.2160)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-295.pth.tar', 80.49199994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-297.pth.tar', 80.43999994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-293.pth.tar', 80.42199992431641)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-291.pth.tar', 80.38399987304687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-289.pth.tar', 80.37600005371094)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-290.pth.tar', 80.36599994873046)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-299.pth.tar', 80.35399992431641)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-292.pth.tar', 80.35000010498047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-296.pth.tar', 80.344000078125)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-285.pth.tar', 80.344)

Train: 300 [   0/1251 (  0%)]  Loss: 2.881 (2.88)  Time: 4.231s,  242.04/s  (4.231s,  242.04/s)  LR: 1.000e-05  Data: 2.987 (2.987)
Train: 300 [  50/1251 (  4%)]  Loss: 3.031 (2.96)  Time: 1.005s, 1018.75/s  (1.063s,  963.31/s)  LR: 1.000e-05  Data: 0.011 (0.070)
Train: 300 [ 100/1251 (  8%)]  Loss: 3.019 (2.98)  Time: 0.999s, 1025.37/s  (1.042s,  982.72/s)  LR: 1.000e-05  Data: 0.010 (0.041)
Train: 300 [ 150/1251 ( 12%)]  Loss: 2.790 (2.93)  Time: 0.999s, 1024.68/s  (1.029s,  995.10/s)  LR: 1.000e-05  Data: 0.011 (0.031)
Train: 300 [ 200/1251 ( 16%)]  Loss: 3.122 (2.97)  Time: 1.021s, 1003.35/s  (1.023s, 1000.51/s)  LR: 1.000e-05  Data: 0.016 (0.026)
Train: 300 [ 250/1251 ( 20%)]  Loss: 3.238 (3.01)  Time: 0.996s, 1028.19/s  (1.019s, 1004.76/s)  LR: 1.000e-05  Data: 0.011 (0.023)
Train: 300 [ 300/1251 ( 24%)]  Loss: 3.185 (3.04)  Time: 0.996s, 1027.93/s  (1.019s, 1004.67/s)  LR: 1.000e-05  Data: 0.011 (0.021)
Train: 300 [ 350/1251 ( 28%)]  Loss: 3.014 (3.03)  Time: 0.997s, 1026.59/s  (1.017s, 1007.18/s)  LR: 1.000e-05  Data: 0.010 (0.020)
Train: 300 [ 400/1251 ( 32%)]  Loss: 2.944 (3.02)  Time: 1.008s, 1015.77/s  (1.015s, 1008.88/s)  LR: 1.000e-05  Data: 0.011 (0.019)
Train: 300 [ 450/1251 ( 36%)]  Loss: 2.997 (3.02)  Time: 1.008s, 1015.82/s  (1.013s, 1010.40/s)  LR: 1.000e-05  Data: 0.011 (0.018)
Train: 300 [ 500/1251 ( 40%)]  Loss: 3.085 (3.03)  Time: 0.997s, 1027.25/s  (1.012s, 1011.43/s)  LR: 1.000e-05  Data: 0.011 (0.017)
Train: 300 [ 550/1251 ( 44%)]  Loss: 3.167 (3.04)  Time: 0.994s, 1029.91/s  (1.011s, 1012.70/s)  LR: 1.000e-05  Data: 0.010 (0.017)
Train: 300 [ 600/1251 ( 48%)]  Loss: 2.825 (3.02)  Time: 1.001s, 1023.01/s  (1.010s, 1013.83/s)  LR: 1.000e-05  Data: 0.012 (0.016)
Train: 300 [ 650/1251 ( 52%)]  Loss: 2.958 (3.02)  Time: 0.998s, 1025.81/s  (1.010s, 1013.86/s)  LR: 1.000e-05  Data: 0.011 (0.016)
Train: 300 [ 700/1251 ( 56%)]  Loss: 2.426 (2.98)  Time: 1.059s,  966.57/s  (1.011s, 1012.48/s)  LR: 1.000e-05  Data: 0.011 (0.015)
Train: 300 [ 750/1251 ( 60%)]  Loss: 3.068 (2.98)  Time: 1.003s, 1020.64/s  (1.011s, 1013.22/s)  LR: 1.000e-05  Data: 0.011 (0.015)
Train: 300 [ 800/1251 ( 64%)]  Loss: 2.998 (2.99)  Time: 0.996s, 1028.58/s  (1.010s, 1013.83/s)  LR: 1.000e-05  Data: 0.011 (0.015)
Train: 300 [ 850/1251 ( 68%)]  Loss: 3.208 (3.00)  Time: 0.996s, 1028.62/s  (1.010s, 1014.31/s)  LR: 1.000e-05  Data: 0.012 (0.015)
Train: 300 [ 900/1251 ( 72%)]  Loss: 3.062 (3.00)  Time: 1.003s, 1021.22/s  (1.009s, 1014.85/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 300 [ 950/1251 ( 76%)]  Loss: 3.434 (3.02)  Time: 0.994s, 1029.77/s  (1.009s, 1015.24/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 300 [1000/1251 ( 80%)]  Loss: 3.161 (3.03)  Time: 0.995s, 1029.04/s  (1.008s, 1015.68/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 300 [1050/1251 ( 84%)]  Loss: 2.915 (3.02)  Time: 0.995s, 1029.60/s  (1.008s, 1015.93/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 300 [1100/1251 ( 88%)]  Loss: 2.719 (3.01)  Time: 0.997s, 1026.78/s  (1.008s, 1016.14/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 300 [1150/1251 ( 92%)]  Loss: 2.893 (3.01)  Time: 0.995s, 1029.27/s  (1.008s, 1016.25/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 300 [1200/1251 ( 96%)]  Loss: 3.097 (3.01)  Time: 1.003s, 1021.06/s  (1.007s, 1016.54/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 300 [1250/1251 (100%)]  Loss: 3.156 (3.02)  Time: 0.990s, 1034.46/s  (1.007s, 1016.88/s)  LR: 1.000e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.633 (1.633)  Loss:  0.6505 (0.6505)  Acc@1: 92.2852 (92.2852)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.245 (0.566)  Loss:  0.7588 (1.0758)  Acc@1: 87.1462 (80.3640)  Acc@5: 98.1132 (95.2300)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-295.pth.tar', 80.49199994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-297.pth.tar', 80.43999994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-293.pth.tar', 80.42199992431641)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-291.pth.tar', 80.38399987304687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-289.pth.tar', 80.37600005371094)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-290.pth.tar', 80.36599994873046)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-300.pth.tar', 80.36399997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-299.pth.tar', 80.35399992431641)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-292.pth.tar', 80.35000010498047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-296.pth.tar', 80.344000078125)

Train: 301 [   0/1251 (  0%)]  Loss: 3.059 (3.06)  Time: 2.492s,  410.83/s  (2.492s,  410.83/s)  LR: 1.000e-05  Data: 1.531 (1.531)
Train: 301 [  50/1251 (  4%)]  Loss: 3.080 (3.07)  Time: 0.995s, 1029.18/s  (1.033s,  991.41/s)  LR: 1.000e-05  Data: 0.011 (0.041)
Train: 301 [ 100/1251 (  8%)]  Loss: 3.227 (3.12)  Time: 1.034s,  990.45/s  (1.020s, 1004.07/s)  LR: 1.000e-05  Data: 0.010 (0.026)
Train: 301 [ 150/1251 ( 12%)]  Loss: 3.409 (3.19)  Time: 0.997s, 1027.37/s  (1.014s, 1009.76/s)  LR: 1.000e-05  Data: 0.012 (0.021)
Train: 301 [ 200/1251 ( 16%)]  Loss: 3.074 (3.17)  Time: 0.994s, 1030.20/s  (1.013s, 1010.90/s)  LR: 1.000e-05  Data: 0.011 (0.019)
Train: 301 [ 250/1251 ( 20%)]  Loss: 2.958 (3.13)  Time: 0.997s, 1026.63/s  (1.011s, 1013.19/s)  LR: 1.000e-05  Data: 0.011 (0.017)
Train: 301 [ 300/1251 ( 24%)]  Loss: 2.794 (3.09)  Time: 0.996s, 1027.74/s  (1.009s, 1014.69/s)  LR: 1.000e-05  Data: 0.011 (0.016)
Train: 301 [ 350/1251 ( 28%)]  Loss: 2.983 (3.07)  Time: 0.998s, 1026.29/s  (1.008s, 1016.15/s)  LR: 1.000e-05  Data: 0.011 (0.016)
Train: 301 [ 400/1251 ( 32%)]  Loss: 3.161 (3.08)  Time: 1.015s, 1008.71/s  (1.007s, 1016.95/s)  LR: 1.000e-05  Data: 0.012 (0.015)
Train: 301 [ 450/1251 ( 36%)]  Loss: 3.164 (3.09)  Time: 0.994s, 1030.04/s  (1.006s, 1017.67/s)  LR: 1.000e-05  Data: 0.011 (0.015)
Train: 301 [ 500/1251 ( 40%)]  Loss: 3.199 (3.10)  Time: 1.012s, 1011.39/s  (1.006s, 1018.09/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 301 [ 550/1251 ( 44%)]  Loss: 3.043 (3.10)  Time: 1.010s, 1014.14/s  (1.005s, 1018.53/s)  LR: 1.000e-05  Data: 0.012 (0.014)
Train: 301 [ 600/1251 ( 48%)]  Loss: 3.224 (3.11)  Time: 1.000s, 1024.28/s  (1.005s, 1018.72/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 301 [ 650/1251 ( 52%)]  Loss: 2.874 (3.09)  Time: 1.049s,  975.94/s  (1.005s, 1019.08/s)  LR: 1.000e-05  Data: 0.012 (0.014)
Train: 301 [ 700/1251 ( 56%)]  Loss: 2.809 (3.07)  Time: 0.996s, 1027.63/s  (1.005s, 1019.35/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 301 [ 750/1251 ( 60%)]  Loss: 3.216 (3.08)  Time: 1.042s,  983.15/s  (1.004s, 1019.60/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 301 [ 800/1251 ( 64%)]  Loss: 3.080 (3.08)  Time: 1.025s,  998.85/s  (1.004s, 1019.97/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 301 [ 850/1251 ( 68%)]  Loss: 3.291 (3.09)  Time: 0.993s, 1030.76/s  (1.004s, 1020.12/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 301 [ 900/1251 ( 72%)]  Loss: 2.913 (3.08)  Time: 1.002s, 1021.67/s  (1.004s, 1020.20/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 301 [ 950/1251 ( 76%)]  Loss: 3.103 (3.08)  Time: 1.003s, 1020.51/s  (1.004s, 1020.29/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 301 [1000/1251 ( 80%)]  Loss: 3.181 (3.09)  Time: 0.996s, 1028.18/s  (1.003s, 1020.46/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 301 [1050/1251 ( 84%)]  Loss: 3.182 (3.09)  Time: 1.056s,  970.11/s  (1.003s, 1020.44/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 301 [1100/1251 ( 88%)]  Loss: 3.089 (3.09)  Time: 0.996s, 1028.43/s  (1.004s, 1019.94/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 301 [1150/1251 ( 92%)]  Loss: 2.960 (3.09)  Time: 0.994s, 1029.78/s  (1.004s, 1020.20/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 301 [1200/1251 ( 96%)]  Loss: 2.952 (3.08)  Time: 1.002s, 1022.13/s  (1.004s, 1020.31/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 301 [1250/1251 (100%)]  Loss: 2.965 (3.08)  Time: 0.984s, 1040.17/s  (1.004s, 1020.39/s)  LR: 1.000e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.679 (1.679)  Loss:  0.6945 (0.6945)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.245 (0.572)  Loss:  0.8068 (1.1263)  Acc@1: 86.5566 (80.3020)  Acc@5: 97.8774 (95.2040)
Train: 302 [   0/1251 (  0%)]  Loss: 3.083 (3.08)  Time: 2.556s,  400.56/s  (2.556s,  400.56/s)  LR: 1.000e-05  Data: 1.589 (1.589)
Train: 302 [  50/1251 (  4%)]  Loss: 2.951 (3.02)  Time: 0.996s, 1028.50/s  (1.030s,  994.47/s)  LR: 1.000e-05  Data: 0.011 (0.042)
Train: 302 [ 100/1251 (  8%)]  Loss: 2.996 (3.01)  Time: 0.994s, 1030.47/s  (1.015s, 1009.28/s)  LR: 1.000e-05  Data: 0.012 (0.027)
Train: 302 [ 150/1251 ( 12%)]  Loss: 3.166 (3.05)  Time: 0.996s, 1027.61/s  (1.012s, 1011.72/s)  LR: 1.000e-05  Data: 0.011 (0.022)
Train: 302 [ 200/1251 ( 16%)]  Loss: 2.705 (2.98)  Time: 1.000s, 1024.45/s  (1.009s, 1014.47/s)  LR: 1.000e-05  Data: 0.011 (0.019)
Train: 302 [ 250/1251 ( 20%)]  Loss: 3.129 (3.01)  Time: 0.995s, 1029.20/s  (1.009s, 1014.76/s)  LR: 1.000e-05  Data: 0.011 (0.017)
Train: 302 [ 300/1251 ( 24%)]  Loss: 2.821 (2.98)  Time: 0.997s, 1026.71/s  (1.008s, 1016.03/s)  LR: 1.000e-05  Data: 0.010 (0.016)
Train: 302 [ 350/1251 ( 28%)]  Loss: 3.049 (2.99)  Time: 1.005s, 1018.87/s  (1.007s, 1016.78/s)  LR: 1.000e-05  Data: 0.011 (0.016)
Train: 302 [ 400/1251 ( 32%)]  Loss: 3.150 (3.01)  Time: 0.994s, 1030.23/s  (1.006s, 1017.47/s)  LR: 1.000e-05  Data: 0.011 (0.015)
Train: 302 [ 450/1251 ( 36%)]  Loss: 3.161 (3.02)  Time: 0.993s, 1030.79/s  (1.006s, 1018.28/s)  LR: 1.000e-05  Data: 0.010 (0.015)
Train: 302 [ 500/1251 ( 40%)]  Loss: 3.056 (3.02)  Time: 0.999s, 1025.10/s  (1.006s, 1018.38/s)  LR: 1.000e-05  Data: 0.012 (0.014)
Train: 302 [ 550/1251 ( 44%)]  Loss: 3.055 (3.03)  Time: 0.995s, 1028.89/s  (1.005s, 1019.08/s)  LR: 1.000e-05  Data: 0.012 (0.014)
Train: 302 [ 600/1251 ( 48%)]  Loss: 3.120 (3.03)  Time: 0.995s, 1029.39/s  (1.005s, 1019.18/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 302 [ 650/1251 ( 52%)]  Loss: 3.112 (3.04)  Time: 0.998s, 1025.60/s  (1.005s, 1019.14/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 302 [ 700/1251 ( 56%)]  Loss: 3.084 (3.04)  Time: 0.998s, 1026.12/s  (1.005s, 1019.16/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 302 [ 750/1251 ( 60%)]  Loss: 3.177 (3.05)  Time: 0.998s, 1025.91/s  (1.004s, 1019.50/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 302 [ 800/1251 ( 64%)]  Loss: 2.976 (3.05)  Time: 0.995s, 1029.52/s  (1.004s, 1019.83/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 302 [ 850/1251 ( 68%)]  Loss: 3.130 (3.05)  Time: 1.008s, 1015.42/s  (1.004s, 1020.16/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 302 [ 900/1251 ( 72%)]  Loss: 3.171 (3.06)  Time: 1.008s, 1016.05/s  (1.004s, 1020.32/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 302 [ 950/1251 ( 76%)]  Loss: 3.365 (3.07)  Time: 0.996s, 1028.05/s  (1.004s, 1020.29/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 302 [1000/1251 ( 80%)]  Loss: 2.980 (3.07)  Time: 1.009s, 1014.43/s  (1.004s, 1020.27/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 302 [1050/1251 ( 84%)]  Loss: 3.031 (3.07)  Time: 1.007s, 1016.49/s  (1.003s, 1020.44/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 302 [1100/1251 ( 88%)]  Loss: 2.940 (3.06)  Time: 1.008s, 1015.54/s  (1.004s, 1020.43/s)  LR: 1.000e-05  Data: 0.015 (0.013)
Train: 302 [1150/1251 ( 92%)]  Loss: 3.380 (3.07)  Time: 1.004s, 1019.93/s  (1.003s, 1020.61/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 302 [1200/1251 ( 96%)]  Loss: 2.978 (3.07)  Time: 1.005s, 1019.07/s  (1.003s, 1020.75/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 302 [1250/1251 (100%)]  Loss: 2.996 (3.07)  Time: 0.988s, 1036.64/s  (1.003s, 1020.75/s)  LR: 1.000e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.664 (1.664)  Loss:  0.6360 (0.6360)  Acc@1: 92.6758 (92.6758)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.245 (0.572)  Loss:  0.7402 (1.0732)  Acc@1: 87.3821 (80.4980)  Acc@5: 98.1132 (95.2580)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-302.pth.tar', 80.49799994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-295.pth.tar', 80.49199994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-297.pth.tar', 80.43999994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-293.pth.tar', 80.42199992431641)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-291.pth.tar', 80.38399987304687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-289.pth.tar', 80.37600005371094)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-290.pth.tar', 80.36599994873046)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-300.pth.tar', 80.36399997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-299.pth.tar', 80.35399992431641)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-292.pth.tar', 80.35000010498047)

Train: 303 [   0/1251 (  0%)]  Loss: 2.975 (2.97)  Time: 2.519s,  406.48/s  (2.519s,  406.48/s)  LR: 1.000e-05  Data: 1.560 (1.560)
Train: 303 [  50/1251 (  4%)]  Loss: 2.888 (2.93)  Time: 0.998s, 1026.24/s  (1.031s,  992.98/s)  LR: 1.000e-05  Data: 0.012 (0.042)
Train: 303 [ 100/1251 (  8%)]  Loss: 2.967 (2.94)  Time: 0.997s, 1026.59/s  (1.023s, 1000.54/s)  LR: 1.000e-05  Data: 0.011 (0.027)
Train: 303 [ 150/1251 ( 12%)]  Loss: 3.200 (3.01)  Time: 0.993s, 1031.62/s  (1.016s, 1007.61/s)  LR: 1.000e-05  Data: 0.011 (0.022)
Train: 303 [ 200/1251 ( 16%)]  Loss: 2.845 (2.97)  Time: 1.005s, 1018.64/s  (1.014s, 1009.96/s)  LR: 1.000e-05  Data: 0.014 (0.019)
Train: 303 [ 250/1251 ( 20%)]  Loss: 2.841 (2.95)  Time: 0.996s, 1028.49/s  (1.014s, 1009.83/s)  LR: 1.000e-05  Data: 0.012 (0.018)
Train: 303 [ 300/1251 ( 24%)]  Loss: 3.021 (2.96)  Time: 1.066s,  960.66/s  (1.012s, 1012.08/s)  LR: 1.000e-05  Data: 0.013 (0.017)
Train: 303 [ 350/1251 ( 28%)]  Loss: 3.392 (3.02)  Time: 1.009s, 1014.84/s  (1.010s, 1013.69/s)  LR: 1.000e-05  Data: 0.011 (0.016)
Train: 303 [ 400/1251 ( 32%)]  Loss: 3.102 (3.03)  Time: 1.043s,  981.31/s  (1.009s, 1014.69/s)  LR: 1.000e-05  Data: 0.011 (0.016)
Train: 303 [ 450/1251 ( 36%)]  Loss: 3.071 (3.03)  Time: 0.997s, 1026.79/s  (1.008s, 1015.64/s)  LR: 1.000e-05  Data: 0.012 (0.015)
Train: 303 [ 500/1251 ( 40%)]  Loss: 3.248 (3.05)  Time: 0.996s, 1028.31/s  (1.007s, 1016.73/s)  LR: 1.000e-05  Data: 0.011 (0.015)
Train: 303 [ 550/1251 ( 44%)]  Loss: 3.241 (3.07)  Time: 1.046s,  979.04/s  (1.008s, 1016.36/s)  LR: 1.000e-05  Data: 0.012 (0.014)
Train: 303 [ 600/1251 ( 48%)]  Loss: 2.740 (3.04)  Time: 1.013s, 1011.29/s  (1.007s, 1016.81/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 303 [ 650/1251 ( 52%)]  Loss: 2.596 (3.01)  Time: 1.009s, 1014.38/s  (1.007s, 1017.38/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 303 [ 700/1251 ( 56%)]  Loss: 3.097 (3.01)  Time: 0.996s, 1028.39/s  (1.006s, 1017.76/s)  LR: 1.000e-05  Data: 0.012 (0.014)
Train: 303 [ 750/1251 ( 60%)]  Loss: 2.609 (2.99)  Time: 0.989s, 1035.19/s  (1.006s, 1018.19/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 303 [ 800/1251 ( 64%)]  Loss: 3.277 (3.01)  Time: 0.997s, 1026.72/s  (1.006s, 1018.38/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 303 [ 850/1251 ( 68%)]  Loss: 3.290 (3.02)  Time: 0.995s, 1029.21/s  (1.005s, 1018.88/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 303 [ 900/1251 ( 72%)]  Loss: 2.985 (3.02)  Time: 1.036s,  987.96/s  (1.005s, 1018.96/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 303 [ 950/1251 ( 76%)]  Loss: 3.098 (3.02)  Time: 1.060s,  966.24/s  (1.005s, 1018.88/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 303 [1000/1251 ( 80%)]  Loss: 3.305 (3.04)  Time: 1.002s, 1022.17/s  (1.005s, 1018.73/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 303 [1050/1251 ( 84%)]  Loss: 3.249 (3.05)  Time: 1.005s, 1018.98/s  (1.005s, 1019.04/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 303 [1100/1251 ( 88%)]  Loss: 3.270 (3.06)  Time: 1.027s,  997.45/s  (1.005s, 1019.27/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 303 [1150/1251 ( 92%)]  Loss: 3.184 (3.06)  Time: 0.999s, 1025.06/s  (1.005s, 1019.41/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 303 [1200/1251 ( 96%)]  Loss: 3.120 (3.06)  Time: 0.994s, 1030.26/s  (1.004s, 1019.49/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 303 [1250/1251 (100%)]  Loss: 2.999 (3.06)  Time: 1.024s,  999.51/s  (1.004s, 1019.55/s)  LR: 1.000e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.640 (1.640)  Loss:  0.6730 (0.6730)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.245 (0.569)  Loss:  0.7694 (1.1064)  Acc@1: 87.3821 (80.4440)  Acc@5: 97.9953 (95.1920)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-302.pth.tar', 80.49799994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-295.pth.tar', 80.49199994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-303.pth.tar', 80.44399994873046)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-297.pth.tar', 80.43999994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-293.pth.tar', 80.42199992431641)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-291.pth.tar', 80.38399987304687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-289.pth.tar', 80.37600005371094)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-290.pth.tar', 80.36599994873046)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-300.pth.tar', 80.36399997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-299.pth.tar', 80.35399992431641)

Train: 304 [   0/1251 (  0%)]  Loss: 3.141 (3.14)  Time: 2.508s,  408.37/s  (2.508s,  408.37/s)  LR: 1.000e-05  Data: 1.549 (1.549)
Train: 304 [  50/1251 (  4%)]  Loss: 3.077 (3.11)  Time: 1.014s, 1009.81/s  (1.031s,  992.91/s)  LR: 1.000e-05  Data: 0.011 (0.042)
Train: 304 [ 100/1251 (  8%)]  Loss: 2.847 (3.02)  Time: 1.001s, 1022.81/s  (1.021s, 1003.35/s)  LR: 1.000e-05  Data: 0.015 (0.027)
Train: 304 [ 150/1251 ( 12%)]  Loss: 2.989 (3.01)  Time: 1.051s,  974.70/s  (1.016s, 1007.57/s)  LR: 1.000e-05  Data: 0.010 (0.022)
Train: 304 [ 200/1251 ( 16%)]  Loss: 3.024 (3.02)  Time: 0.995s, 1029.49/s  (1.016s, 1007.81/s)  LR: 1.000e-05  Data: 0.011 (0.019)
Train: 304 [ 250/1251 ( 20%)]  Loss: 2.984 (3.01)  Time: 0.996s, 1027.74/s  (1.013s, 1010.87/s)  LR: 1.000e-05  Data: 0.012 (0.017)
Train: 304 [ 300/1251 ( 24%)]  Loss: 2.886 (2.99)  Time: 0.996s, 1028.14/s  (1.011s, 1012.60/s)  LR: 1.000e-05  Data: 0.012 (0.016)
Train: 304 [ 350/1251 ( 28%)]  Loss: 2.998 (2.99)  Time: 1.002s, 1021.89/s  (1.010s, 1013.90/s)  LR: 1.000e-05  Data: 0.012 (0.016)
Train: 304 [ 400/1251 ( 32%)]  Loss: 2.678 (2.96)  Time: 1.002s, 1022.37/s  (1.009s, 1014.50/s)  LR: 1.000e-05  Data: 0.011 (0.015)
Train: 304 [ 450/1251 ( 36%)]  Loss: 3.339 (3.00)  Time: 0.995s, 1029.45/s  (1.009s, 1015.36/s)  LR: 1.000e-05  Data: 0.010 (0.015)
Train: 304 [ 500/1251 ( 40%)]  Loss: 3.235 (3.02)  Time: 1.045s,  979.69/s  (1.008s, 1016.15/s)  LR: 1.000e-05  Data: 0.012 (0.014)
Train: 304 [ 550/1251 ( 44%)]  Loss: 3.085 (3.02)  Time: 1.001s, 1022.74/s  (1.007s, 1016.42/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 304 [ 600/1251 ( 48%)]  Loss: 3.306 (3.05)  Time: 0.995s, 1029.01/s  (1.008s, 1016.38/s)  LR: 1.000e-05  Data: 0.012 (0.014)
Train: 304 [ 650/1251 ( 52%)]  Loss: 3.136 (3.05)  Time: 0.997s, 1026.89/s  (1.007s, 1017.09/s)  LR: 1.000e-05  Data: 0.012 (0.014)
Train: 304 [ 700/1251 ( 56%)]  Loss: 2.930 (3.04)  Time: 0.994s, 1030.63/s  (1.006s, 1017.56/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 304 [ 750/1251 ( 60%)]  Loss: 3.062 (3.04)  Time: 0.992s, 1032.55/s  (1.006s, 1017.90/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 304 [ 800/1251 ( 64%)]  Loss: 2.989 (3.04)  Time: 1.000s, 1023.70/s  (1.006s, 1018.16/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 304 [ 850/1251 ( 68%)]  Loss: 2.823 (3.03)  Time: 0.995s, 1028.72/s  (1.005s, 1018.57/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 304 [ 900/1251 ( 72%)]  Loss: 2.908 (3.02)  Time: 0.998s, 1025.54/s  (1.005s, 1019.03/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 304 [ 950/1251 ( 76%)]  Loss: 3.052 (3.02)  Time: 0.997s, 1026.71/s  (1.005s, 1019.37/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 304 [1000/1251 ( 80%)]  Loss: 3.143 (3.03)  Time: 1.010s, 1014.21/s  (1.004s, 1019.57/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 304 [1050/1251 ( 84%)]  Loss: 3.008 (3.03)  Time: 0.996s, 1028.34/s  (1.004s, 1019.68/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 304 [1100/1251 ( 88%)]  Loss: 3.213 (3.04)  Time: 0.997s, 1027.32/s  (1.004s, 1019.82/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 304 [1150/1251 ( 92%)]  Loss: 3.227 (3.04)  Time: 1.037s,  987.14/s  (1.004s, 1019.63/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 304 [1200/1251 ( 96%)]  Loss: 3.085 (3.05)  Time: 1.036s,  988.48/s  (1.005s, 1019.09/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 304 [1250/1251 (100%)]  Loss: 3.006 (3.05)  Time: 0.991s, 1033.47/s  (1.005s, 1019.09/s)  LR: 1.000e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.673 (1.673)  Loss:  0.5815 (0.5815)  Acc@1: 92.2852 (92.2852)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.245 (0.565)  Loss:  0.6857 (1.0193)  Acc@1: 86.5566 (80.4740)  Acc@5: 97.6415 (95.2640)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-302.pth.tar', 80.49799994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-295.pth.tar', 80.49199994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-304.pth.tar', 80.47399997802735)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-303.pth.tar', 80.44399994873046)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-297.pth.tar', 80.43999994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-293.pth.tar', 80.42199992431641)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-291.pth.tar', 80.38399987304687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-289.pth.tar', 80.37600005371094)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-290.pth.tar', 80.36599994873046)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-300.pth.tar', 80.36399997558594)

Train: 305 [   0/1251 (  0%)]  Loss: 3.153 (3.15)  Time: 2.504s,  408.89/s  (2.504s,  408.89/s)  LR: 1.000e-05  Data: 1.487 (1.487)
Train: 305 [  50/1251 (  4%)]  Loss: 3.203 (3.18)  Time: 1.006s, 1018.17/s  (1.030s,  993.92/s)  LR: 1.000e-05  Data: 0.011 (0.040)
Train: 305 [ 100/1251 (  8%)]  Loss: 2.922 (3.09)  Time: 0.995s, 1028.97/s  (1.015s, 1008.53/s)  LR: 1.000e-05  Data: 0.011 (0.026)
Train: 305 [ 150/1251 ( 12%)]  Loss: 3.125 (3.10)  Time: 1.008s, 1015.95/s  (1.012s, 1011.94/s)  LR: 1.000e-05  Data: 0.012 (0.021)
Train: 305 [ 200/1251 ( 16%)]  Loss: 3.298 (3.14)  Time: 1.064s,  961.96/s  (1.010s, 1013.55/s)  LR: 1.000e-05  Data: 0.012 (0.019)
Train: 305 [ 250/1251 ( 20%)]  Loss: 3.098 (3.13)  Time: 1.063s,  963.56/s  (1.016s, 1007.43/s)  LR: 1.000e-05  Data: 0.011 (0.017)
Train: 305 [ 300/1251 ( 24%)]  Loss: 3.308 (3.16)  Time: 1.001s, 1023.02/s  (1.015s, 1008.42/s)  LR: 1.000e-05  Data: 0.011 (0.016)
Train: 305 [ 350/1251 ( 28%)]  Loss: 2.980 (3.14)  Time: 0.998s, 1026.27/s  (1.013s, 1010.73/s)  LR: 1.000e-05  Data: 0.012 (0.016)
Train: 305 [ 400/1251 ( 32%)]  Loss: 3.089 (3.13)  Time: 0.993s, 1031.45/s  (1.011s, 1012.37/s)  LR: 1.000e-05  Data: 0.011 (0.015)
Train: 305 [ 450/1251 ( 36%)]  Loss: 3.181 (3.14)  Time: 1.053s,  972.01/s  (1.011s, 1012.76/s)  LR: 1.000e-05  Data: 0.012 (0.015)
Train: 305 [ 500/1251 ( 40%)]  Loss: 3.158 (3.14)  Time: 0.997s, 1027.45/s  (1.012s, 1011.67/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 305 [ 550/1251 ( 44%)]  Loss: 3.244 (3.15)  Time: 0.997s, 1026.68/s  (1.012s, 1011.83/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 305 [ 600/1251 ( 48%)]  Loss: 3.175 (3.15)  Time: 1.004s, 1020.04/s  (1.011s, 1012.93/s)  LR: 1.000e-05  Data: 0.012 (0.014)
Train: 305 [ 650/1251 ( 52%)]  Loss: 3.258 (3.16)  Time: 0.990s, 1034.78/s  (1.010s, 1013.79/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 305 [ 700/1251 ( 56%)]  Loss: 2.533 (3.12)  Time: 0.995s, 1028.77/s  (1.009s, 1014.41/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 305 [ 750/1251 ( 60%)]  Loss: 2.801 (3.10)  Time: 1.042s,  983.00/s  (1.011s, 1012.99/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 305 [ 800/1251 ( 64%)]  Loss: 2.823 (3.08)  Time: 1.032s,  992.72/s  (1.011s, 1013.06/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 305 [ 850/1251 ( 68%)]  Loss: 3.155 (3.08)  Time: 1.014s, 1009.41/s  (1.011s, 1013.28/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 305 [ 900/1251 ( 72%)]  Loss: 3.081 (3.08)  Time: 0.995s, 1029.11/s  (1.010s, 1013.50/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 305 [ 950/1251 ( 76%)]  Loss: 3.066 (3.08)  Time: 0.994s, 1029.71/s  (1.011s, 1012.91/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 305 [1000/1251 ( 80%)]  Loss: 2.921 (3.07)  Time: 1.002s, 1021.86/s  (1.010s, 1013.41/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 305 [1050/1251 ( 84%)]  Loss: 2.910 (3.07)  Time: 0.995s, 1029.38/s  (1.010s, 1013.88/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 305 [1100/1251 ( 88%)]  Loss: 3.233 (3.07)  Time: 0.999s, 1024.97/s  (1.010s, 1014.21/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 305 [1150/1251 ( 92%)]  Loss: 3.235 (3.08)  Time: 1.000s, 1024.22/s  (1.009s, 1014.54/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 305 [1200/1251 ( 96%)]  Loss: 3.135 (3.08)  Time: 0.998s, 1026.47/s  (1.009s, 1014.77/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 305 [1250/1251 (100%)]  Loss: 3.006 (3.08)  Time: 0.986s, 1038.88/s  (1.009s, 1015.10/s)  LR: 1.000e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.659 (1.659)  Loss:  0.5954 (0.5954)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.245 (0.572)  Loss:  0.7122 (1.0366)  Acc@1: 86.7924 (80.4740)  Acc@5: 97.9953 (95.2220)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-302.pth.tar', 80.49799994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-295.pth.tar', 80.49199994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-304.pth.tar', 80.47399997802735)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-305.pth.tar', 80.47399995117188)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-303.pth.tar', 80.44399994873046)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-297.pth.tar', 80.43999994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-293.pth.tar', 80.42199992431641)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-291.pth.tar', 80.38399987304687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-289.pth.tar', 80.37600005371094)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-290.pth.tar', 80.36599994873046)

Train: 306 [   0/1251 (  0%)]  Loss: 3.043 (3.04)  Time: 2.611s,  392.12/s  (2.611s,  392.12/s)  LR: 1.000e-05  Data: 1.646 (1.646)
Train: 306 [  50/1251 (  4%)]  Loss: 3.111 (3.08)  Time: 1.020s, 1003.71/s  (1.036s,  988.44/s)  LR: 1.000e-05  Data: 0.011 (0.044)
Train: 306 [ 100/1251 (  8%)]  Loss: 2.793 (2.98)  Time: 1.048s,  976.90/s  (1.023s, 1001.21/s)  LR: 1.000e-05  Data: 0.011 (0.028)
Train: 306 [ 150/1251 ( 12%)]  Loss: 2.463 (2.85)  Time: 0.993s, 1030.83/s  (1.021s, 1002.58/s)  LR: 1.000e-05  Data: 0.011 (0.022)
Train: 306 [ 200/1251 ( 16%)]  Loss: 3.132 (2.91)  Time: 0.998s, 1026.35/s  (1.017s, 1007.34/s)  LR: 1.000e-05  Data: 0.011 (0.020)
Train: 306 [ 250/1251 ( 20%)]  Loss: 3.044 (2.93)  Time: 0.996s, 1028.42/s  (1.014s, 1009.45/s)  LR: 1.000e-05  Data: 0.012 (0.018)
Train: 306 [ 300/1251 ( 24%)]  Loss: 3.068 (2.95)  Time: 0.996s, 1028.63/s  (1.012s, 1012.31/s)  LR: 1.000e-05  Data: 0.011 (0.017)
Train: 306 [ 350/1251 ( 28%)]  Loss: 3.095 (2.97)  Time: 1.012s, 1011.52/s  (1.010s, 1013.68/s)  LR: 1.000e-05  Data: 0.011 (0.016)
Train: 306 [ 400/1251 ( 32%)]  Loss: 3.337 (3.01)  Time: 0.993s, 1031.53/s  (1.009s, 1014.75/s)  LR: 1.000e-05  Data: 0.011 (0.015)
Train: 306 [ 450/1251 ( 36%)]  Loss: 3.002 (3.01)  Time: 0.991s, 1033.63/s  (1.009s, 1015.25/s)  LR: 1.000e-05  Data: 0.010 (0.015)
Train: 306 [ 500/1251 ( 40%)]  Loss: 2.807 (2.99)  Time: 0.992s, 1032.03/s  (1.008s, 1016.04/s)  LR: 1.000e-05  Data: 0.010 (0.015)
Train: 306 [ 550/1251 ( 44%)]  Loss: 2.952 (2.99)  Time: 1.063s,  963.76/s  (1.007s, 1016.70/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 306 [ 600/1251 ( 48%)]  Loss: 3.141 (3.00)  Time: 0.996s, 1028.63/s  (1.008s, 1015.52/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 306 [ 650/1251 ( 52%)]  Loss: 3.168 (3.01)  Time: 0.993s, 1031.14/s  (1.008s, 1016.24/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 306 [ 700/1251 ( 56%)]  Loss: 3.183 (3.02)  Time: 1.061s,  964.79/s  (1.007s, 1016.72/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 306 [ 750/1251 ( 60%)]  Loss: 3.133 (3.03)  Time: 0.995s, 1029.59/s  (1.007s, 1016.94/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 306 [ 800/1251 ( 64%)]  Loss: 3.291 (3.04)  Time: 0.995s, 1029.55/s  (1.007s, 1017.28/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 306 [ 850/1251 ( 68%)]  Loss: 3.001 (3.04)  Time: 1.063s,  962.94/s  (1.006s, 1017.53/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 306 [ 900/1251 ( 72%)]  Loss: 2.622 (3.02)  Time: 1.038s,  986.96/s  (1.006s, 1017.66/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 306 [ 950/1251 ( 76%)]  Loss: 2.753 (3.01)  Time: 0.998s, 1025.66/s  (1.006s, 1017.98/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 306 [1000/1251 ( 80%)]  Loss: 3.060 (3.01)  Time: 1.049s,  976.24/s  (1.006s, 1017.44/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 306 [1050/1251 ( 84%)]  Loss: 2.969 (3.01)  Time: 0.996s, 1028.21/s  (1.008s, 1015.46/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 306 [1100/1251 ( 88%)]  Loss: 3.041 (3.01)  Time: 0.996s, 1028.36/s  (1.008s, 1015.89/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 306 [1150/1251 ( 92%)]  Loss: 3.230 (3.02)  Time: 1.001s, 1022.66/s  (1.008s, 1015.72/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 306 [1200/1251 ( 96%)]  Loss: 3.150 (3.02)  Time: 0.994s, 1030.51/s  (1.008s, 1015.86/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 306 [1250/1251 (100%)]  Loss: 3.072 (3.03)  Time: 0.985s, 1039.09/s  (1.008s, 1016.22/s)  LR: 1.000e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.665 (1.665)  Loss:  0.6007 (0.6007)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.245 (0.570)  Loss:  0.7095 (1.0389)  Acc@1: 86.9104 (80.4040)  Acc@5: 97.9953 (95.2200)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-302.pth.tar', 80.49799994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-295.pth.tar', 80.49199994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-304.pth.tar', 80.47399997802735)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-305.pth.tar', 80.47399995117188)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-303.pth.tar', 80.44399994873046)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-297.pth.tar', 80.43999994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-293.pth.tar', 80.42199992431641)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-306.pth.tar', 80.40399987304687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-291.pth.tar', 80.38399987304687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-289.pth.tar', 80.37600005371094)

Train: 307 [   0/1251 (  0%)]  Loss: 3.171 (3.17)  Time: 2.608s,  392.69/s  (2.608s,  392.69/s)  LR: 1.000e-05  Data: 1.647 (1.647)
Train: 307 [  50/1251 (  4%)]  Loss: 3.051 (3.11)  Time: 1.005s, 1018.47/s  (1.041s,  983.69/s)  LR: 1.000e-05  Data: 0.010 (0.043)
Train: 307 [ 100/1251 (  8%)]  Loss: 3.244 (3.16)  Time: 0.998s, 1025.72/s  (1.023s, 1000.93/s)  LR: 1.000e-05  Data: 0.011 (0.027)
Train: 307 [ 150/1251 ( 12%)]  Loss: 2.999 (3.12)  Time: 1.005s, 1018.93/s  (1.018s, 1006.18/s)  LR: 1.000e-05  Data: 0.010 (0.022)
Train: 307 [ 200/1251 ( 16%)]  Loss: 3.111 (3.12)  Time: 1.040s,  984.40/s  (1.014s, 1009.68/s)  LR: 1.000e-05  Data: 0.011 (0.019)
Train: 307 [ 250/1251 ( 20%)]  Loss: 3.245 (3.14)  Time: 1.019s, 1004.76/s  (1.013s, 1011.07/s)  LR: 1.000e-05  Data: 0.011 (0.018)
Train: 307 [ 300/1251 ( 24%)]  Loss: 3.263 (3.15)  Time: 0.999s, 1025.38/s  (1.010s, 1013.39/s)  LR: 1.000e-05  Data: 0.011 (0.017)
Train: 307 [ 350/1251 ( 28%)]  Loss: 3.200 (3.16)  Time: 0.993s, 1031.13/s  (1.009s, 1014.79/s)  LR: 1.000e-05  Data: 0.012 (0.016)
Train: 307 [ 400/1251 ( 32%)]  Loss: 3.155 (3.16)  Time: 1.002s, 1022.12/s  (1.008s, 1016.19/s)  LR: 1.000e-05  Data: 0.012 (0.015)
Train: 307 [ 450/1251 ( 36%)]  Loss: 3.146 (3.16)  Time: 1.006s, 1018.30/s  (1.007s, 1016.66/s)  LR: 1.000e-05  Data: 0.012 (0.015)
Train: 307 [ 500/1251 ( 40%)]  Loss: 3.106 (3.15)  Time: 1.002s, 1021.93/s  (1.007s, 1016.64/s)  LR: 1.000e-05  Data: 0.011 (0.015)
Train: 307 [ 550/1251 ( 44%)]  Loss: 3.060 (3.15)  Time: 0.994s, 1030.18/s  (1.006s, 1017.39/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 307 [ 600/1251 ( 48%)]  Loss: 2.747 (3.12)  Time: 1.032s,  991.89/s  (1.008s, 1016.17/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 307 [ 650/1251 ( 52%)]  Loss: 2.953 (3.10)  Time: 0.996s, 1028.57/s  (1.007s, 1016.46/s)  LR: 1.000e-05  Data: 0.012 (0.014)
Train: 307 [ 700/1251 ( 56%)]  Loss: 3.185 (3.11)  Time: 0.994s, 1030.04/s  (1.008s, 1016.02/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 307 [ 750/1251 ( 60%)]  Loss: 2.980 (3.10)  Time: 0.996s, 1027.66/s  (1.007s, 1016.61/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 307 [ 800/1251 ( 64%)]  Loss: 2.953 (3.09)  Time: 0.996s, 1027.62/s  (1.007s, 1016.79/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 307 [ 850/1251 ( 68%)]  Loss: 3.326 (3.11)  Time: 0.995s, 1029.11/s  (1.008s, 1016.26/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 307 [ 900/1251 ( 72%)]  Loss: 2.681 (3.08)  Time: 0.996s, 1028.35/s  (1.007s, 1016.73/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 307 [ 950/1251 ( 76%)]  Loss: 3.299 (3.09)  Time: 0.996s, 1027.93/s  (1.007s, 1017.07/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 307 [1000/1251 ( 80%)]  Loss: 2.918 (3.09)  Time: 0.995s, 1028.93/s  (1.006s, 1017.57/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 307 [1050/1251 ( 84%)]  Loss: 3.066 (3.08)  Time: 0.997s, 1027.15/s  (1.006s, 1017.82/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 307 [1100/1251 ( 88%)]  Loss: 3.211 (3.09)  Time: 0.999s, 1024.95/s  (1.006s, 1018.22/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 307 [1150/1251 ( 92%)]  Loss: 3.183 (3.09)  Time: 0.997s, 1027.56/s  (1.006s, 1018.24/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 307 [1200/1251 ( 96%)]  Loss: 3.310 (3.10)  Time: 0.995s, 1029.49/s  (1.005s, 1018.49/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 307 [1250/1251 (100%)]  Loss: 2.804 (3.09)  Time: 1.016s, 1007.83/s  (1.005s, 1018.52/s)  LR: 1.000e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.641 (1.641)  Loss:  0.6064 (0.6064)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.245 (0.569)  Loss:  0.7111 (1.0370)  Acc@1: 87.1462 (80.4680)  Acc@5: 98.1132 (95.2560)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-302.pth.tar', 80.49799994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-295.pth.tar', 80.49199994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-304.pth.tar', 80.47399997802735)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-305.pth.tar', 80.47399995117188)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-307.pth.tar', 80.46799997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-303.pth.tar', 80.44399994873046)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-297.pth.tar', 80.43999994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-293.pth.tar', 80.42199992431641)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-306.pth.tar', 80.40399987304687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-291.pth.tar', 80.38399987304687)

Train: 308 [   0/1251 (  0%)]  Loss: 3.406 (3.41)  Time: 2.424s,  422.46/s  (2.424s,  422.46/s)  LR: 1.000e-05  Data: 1.459 (1.459)
Train: 308 [  50/1251 (  4%)]  Loss: 3.153 (3.28)  Time: 1.000s, 1023.57/s  (1.038s,  986.37/s)  LR: 1.000e-05  Data: 0.011 (0.039)
Train: 308 [ 100/1251 (  8%)]  Loss: 2.703 (3.09)  Time: 1.015s, 1008.51/s  (1.024s,  999.82/s)  LR: 1.000e-05  Data: 0.011 (0.025)
Train: 308 [ 150/1251 ( 12%)]  Loss: 3.299 (3.14)  Time: 1.008s, 1015.89/s  (1.019s, 1004.72/s)  LR: 1.000e-05  Data: 0.012 (0.021)
Train: 308 [ 200/1251 ( 16%)]  Loss: 2.837 (3.08)  Time: 1.060s,  965.67/s  (1.017s, 1006.97/s)  LR: 1.000e-05  Data: 0.011 (0.018)
Train: 308 [ 250/1251 ( 20%)]  Loss: 2.997 (3.07)  Time: 1.003s, 1021.09/s  (1.018s, 1006.14/s)  LR: 1.000e-05  Data: 0.011 (0.017)
Train: 308 [ 300/1251 ( 24%)]  Loss: 2.889 (3.04)  Time: 0.998s, 1026.20/s  (1.015s, 1008.54/s)  LR: 1.000e-05  Data: 0.011 (0.016)
Train: 308 [ 350/1251 ( 28%)]  Loss: 2.945 (3.03)  Time: 0.994s, 1029.96/s  (1.014s, 1009.93/s)  LR: 1.000e-05  Data: 0.010 (0.015)
Train: 308 [ 400/1251 ( 32%)]  Loss: 2.936 (3.02)  Time: 0.995s, 1028.93/s  (1.015s, 1009.31/s)  LR: 1.000e-05  Data: 0.010 (0.015)
Train: 308 [ 450/1251 ( 36%)]  Loss: 3.197 (3.04)  Time: 0.995s, 1029.61/s  (1.013s, 1011.14/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 308 [ 500/1251 ( 40%)]  Loss: 3.288 (3.06)  Time: 1.008s, 1016.08/s  (1.011s, 1012.64/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 308 [ 550/1251 ( 44%)]  Loss: 3.245 (3.07)  Time: 0.999s, 1025.15/s  (1.010s, 1013.69/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 308 [ 600/1251 ( 48%)]  Loss: 3.178 (3.08)  Time: 0.998s, 1026.49/s  (1.010s, 1014.32/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 308 [ 650/1251 ( 52%)]  Loss: 2.947 (3.07)  Time: 0.996s, 1028.38/s  (1.009s, 1014.94/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 308 [ 700/1251 ( 56%)]  Loss: 3.178 (3.08)  Time: 0.993s, 1031.16/s  (1.008s, 1015.52/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 308 [ 750/1251 ( 60%)]  Loss: 3.181 (3.09)  Time: 0.994s, 1030.64/s  (1.009s, 1014.84/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 308 [ 800/1251 ( 64%)]  Loss: 3.215 (3.09)  Time: 1.033s,  991.25/s  (1.009s, 1015.14/s)  LR: 1.000e-05  Data: 0.013 (0.013)
Train: 308 [ 850/1251 ( 68%)]  Loss: 3.190 (3.10)  Time: 0.995s, 1029.09/s  (1.008s, 1015.71/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 308 [ 900/1251 ( 72%)]  Loss: 3.238 (3.11)  Time: 1.043s,  982.05/s  (1.008s, 1015.94/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 308 [ 950/1251 ( 76%)]  Loss: 3.173 (3.11)  Time: 0.999s, 1025.22/s  (1.008s, 1015.91/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 308 [1000/1251 ( 80%)]  Loss: 3.068 (3.11)  Time: 0.996s, 1028.37/s  (1.008s, 1016.32/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 308 [1050/1251 ( 84%)]  Loss: 3.218 (3.11)  Time: 0.992s, 1032.66/s  (1.007s, 1016.76/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 308 [1100/1251 ( 88%)]  Loss: 2.855 (3.10)  Time: 1.004s, 1020.09/s  (1.007s, 1016.93/s)  LR: 1.000e-05  Data: 0.013 (0.012)
Train: 308 [1150/1251 ( 92%)]  Loss: 3.141 (3.10)  Time: 0.994s, 1029.84/s  (1.007s, 1016.38/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 308 [1200/1251 ( 96%)]  Loss: 3.118 (3.10)  Time: 0.993s, 1030.87/s  (1.007s, 1016.49/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 308 [1250/1251 (100%)]  Loss: 3.233 (3.11)  Time: 0.989s, 1035.30/s  (1.007s, 1016.72/s)  LR: 1.000e-05  Data: 0.000 (0.012)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.697 (1.697)  Loss:  0.6031 (0.6031)  Acc@1: 92.2852 (92.2852)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.245 (0.567)  Loss:  0.7078 (1.0319)  Acc@1: 87.0283 (80.3960)  Acc@5: 98.1132 (95.2320)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-302.pth.tar', 80.49799994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-295.pth.tar', 80.49199994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-304.pth.tar', 80.47399997802735)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-305.pth.tar', 80.47399995117188)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-307.pth.tar', 80.46799997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-303.pth.tar', 80.44399994873046)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-297.pth.tar', 80.43999994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-293.pth.tar', 80.42199992431641)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-306.pth.tar', 80.40399987304687)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-308.pth.tar', 80.3959999243164)

Train: 309 [   0/1251 (  0%)]  Loss: 3.023 (3.02)  Time: 4.014s,  255.10/s  (4.014s,  255.10/s)  LR: 1.000e-05  Data: 2.785 (2.785)
Train: 309 [  50/1251 (  4%)]  Loss: 3.411 (3.22)  Time: 1.004s, 1020.33/s  (1.063s,  963.50/s)  LR: 1.000e-05  Data: 0.016 (0.066)
Train: 309 [ 100/1251 (  8%)]  Loss: 3.213 (3.22)  Time: 0.992s, 1032.26/s  (1.039s,  985.18/s)  LR: 1.000e-05  Data: 0.010 (0.039)
Train: 309 [ 150/1251 ( 12%)]  Loss: 3.189 (3.21)  Time: 0.996s, 1028.49/s  (1.028s,  996.25/s)  LR: 1.000e-05  Data: 0.011 (0.030)
Train: 309 [ 200/1251 ( 16%)]  Loss: 3.053 (3.18)  Time: 0.994s, 1030.06/s  (1.022s, 1002.33/s)  LR: 1.000e-05  Data: 0.012 (0.025)
Train: 309 [ 250/1251 ( 20%)]  Loss: 3.032 (3.15)  Time: 0.999s, 1025.02/s  (1.018s, 1006.10/s)  LR: 1.000e-05  Data: 0.011 (0.022)
Train: 309 [ 300/1251 ( 24%)]  Loss: 3.328 (3.18)  Time: 1.004s, 1019.85/s  (1.015s, 1008.78/s)  LR: 1.000e-05  Data: 0.011 (0.021)
Train: 309 [ 350/1251 ( 28%)]  Loss: 3.033 (3.16)  Time: 0.994s, 1029.94/s  (1.013s, 1010.95/s)  LR: 1.000e-05  Data: 0.010 (0.019)
Train: 309 [ 400/1251 ( 32%)]  Loss: 2.841 (3.12)  Time: 0.995s, 1028.84/s  (1.012s, 1011.92/s)  LR: 1.000e-05  Data: 0.011 (0.018)
Train: 309 [ 450/1251 ( 36%)]  Loss: 3.298 (3.14)  Time: 0.994s, 1030.05/s  (1.011s, 1013.24/s)  LR: 1.000e-05  Data: 0.011 (0.017)
Train: 309 [ 500/1251 ( 40%)]  Loss: 2.895 (3.12)  Time: 1.030s,  994.21/s  (1.010s, 1014.19/s)  LR: 1.000e-05  Data: 0.012 (0.017)
Train: 309 [ 550/1251 ( 44%)]  Loss: 3.227 (3.13)  Time: 0.996s, 1027.66/s  (1.009s, 1015.02/s)  LR: 1.000e-05  Data: 0.011 (0.016)
Train: 309 [ 600/1251 ( 48%)]  Loss: 2.839 (3.11)  Time: 0.993s, 1030.93/s  (1.008s, 1015.67/s)  LR: 1.000e-05  Data: 0.011 (0.016)
Train: 309 [ 650/1251 ( 52%)]  Loss: 3.370 (3.13)  Time: 0.998s, 1026.46/s  (1.007s, 1016.41/s)  LR: 1.000e-05  Data: 0.010 (0.016)
Train: 309 [ 700/1251 ( 56%)]  Loss: 3.121 (3.12)  Time: 0.996s, 1027.62/s  (1.007s, 1017.02/s)  LR: 1.000e-05  Data: 0.011 (0.015)
Train: 309 [ 750/1251 ( 60%)]  Loss: 2.789 (3.10)  Time: 1.062s,  964.65/s  (1.007s, 1016.97/s)  LR: 1.000e-05  Data: 0.011 (0.015)
Train: 309 [ 800/1251 ( 64%)]  Loss: 3.194 (3.11)  Time: 0.994s, 1030.16/s  (1.007s, 1017.26/s)  LR: 1.000e-05  Data: 0.010 (0.015)
Train: 309 [ 850/1251 ( 68%)]  Loss: 3.250 (3.12)  Time: 1.010s, 1013.74/s  (1.006s, 1017.68/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 309 [ 900/1251 ( 72%)]  Loss: 3.308 (3.13)  Time: 0.995s, 1029.48/s  (1.006s, 1017.98/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 309 [ 950/1251 ( 76%)]  Loss: 3.072 (3.12)  Time: 0.991s, 1033.10/s  (1.006s, 1018.22/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 309 [1000/1251 ( 80%)]  Loss: 3.020 (3.12)  Time: 0.996s, 1027.88/s  (1.005s, 1018.45/s)  LR: 1.000e-05  Data: 0.012 (0.014)
Train: 309 [1050/1251 ( 84%)]  Loss: 3.079 (3.12)  Time: 1.040s,  984.25/s  (1.005s, 1018.67/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 309 [1100/1251 ( 88%)]  Loss: 2.910 (3.11)  Time: 1.000s, 1023.62/s  (1.005s, 1018.88/s)  LR: 1.000e-05  Data: 0.011 (0.014)
Train: 309 [1150/1251 ( 92%)]  Loss: 2.832 (3.10)  Time: 1.011s, 1013.06/s  (1.005s, 1018.83/s)  LR: 1.000e-05  Data: 0.012 (0.014)
Train: 309 [1200/1251 ( 96%)]  Loss: 3.200 (3.10)  Time: 0.998s, 1025.94/s  (1.005s, 1018.93/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 309 [1250/1251 (100%)]  Loss: 3.269 (3.11)  Time: 0.986s, 1038.99/s  (1.005s, 1019.00/s)  LR: 1.000e-05  Data: 0.000 (0.013)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.659 (1.659)  Loss:  0.6394 (0.6394)  Acc@1: 92.3828 (92.3828)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.245 (0.567)  Loss:  0.7427 (1.0674)  Acc@1: 87.5000 (80.4080)  Acc@5: 98.1132 (95.2020)
Current checkpoints:
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-302.pth.tar', 80.49799994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-295.pth.tar', 80.49199994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-304.pth.tar', 80.47399997802735)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-305.pth.tar', 80.47399995117188)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-307.pth.tar', 80.46799997558594)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-303.pth.tar', 80.44399994873046)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-297.pth.tar', 80.43999994873047)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-293.pth.tar', 80.42199992431641)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-309.pth.tar', 80.408)
 ('./output/train/20220127-204945-hrnet32-224/checkpoint-306.pth.tar', 80.40399987304687)

*** Best metric: 80.49799994873047 (epoch 302)
